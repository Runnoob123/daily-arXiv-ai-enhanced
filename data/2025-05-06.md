<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 35]
- [cs.LG](#cs.LG) [Total: 105]
- [cs.CL](#cs.CL) [Total: 56]
- [cs.AI](#cs.AI) [Total: 58]
- [cs.CV](#cs.CV) [Total: 124]
- [stat.ML](#stat.ML) [Total: 7]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.RO](#cs.RO) [Total: 15]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.DB](#cs.DB) [Total: 3]
- [eess.SP](#eess.SP) [Total: 1]
- [eess.IV](#eess.IV) [Total: 23]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.HC](#cs.HC) [Total: 5]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.ET](#cs.ET) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [math.OC](#math.OC) [Total: 10]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.SC](#cs.SC) [Total: 1]
- [cs.NE](#cs.NE) [Total: 3]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Firewall Regulatory Networks for Autonomous Cyber Defense](https://arxiv.org/abs/2505.01436)
*Qi Duan,Ehab Al-Shaer*

Main category: cs.CR

TLDR: 本文提出了一种基于生物启发式网络的自主管理协议FRN，用于动态管理去中心化防火墙架构，具有自动规则配置、弹性响应和全局优化策略协调的特点。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够自动适应风险变化和服务需求的新型防火墙管理协议，以解决传统防火墙的静态性和低效问题。

Method: 提出FRN协议，并通过形式化约束合成未确定组件，以实现自动规则配置、弹性响应和全局优化策略协调。

Result: 通过多个案例研究验证了FRN架构的可行性。

Conclusion: FRN协议为去中心化防火墙提供了一种高效、动态且可证明安全的管理方案。

Abstract: In this paper, we present the principles of designing new self-organising and
autonomous management protocol to govern the dynamics of bio-inspired
decentralized firewall architecture based on Biological Regularity Networks.
  The new architecture called Firewall Regulatory Networks (FRN) exhibits the
following features (1) automatic rule policy configuration with provable
utility-risk appetite guarantee, (2) resilient response for changing risks or
new service requirements, and (3) globally optimized access control policy
reconciliation. We present the FRN protocol and formalize the constraints to
synthesize the undetermined components in the protocol to produce interactions
that can achieve these objectives. We illustrate the feasibility of the FRN
architecture in multiple case studies.

</details>

### [2] [Sparsification Under Siege: Defending Against Poisoning Attacks in Communication-Efficient Federated Learning](https://arxiv.org/abs/2505.01454)
*Zhiyong Jin,Runhua Xu,Chao Li,Yizhong Liu,Jianxin Li*

Main category: cs.CR

TLDR: FLARE是一种新型联邦学习框架，通过稀疏索引掩码检查和模型更新符号相似性分析，有效检测和缓解稀疏化联邦学习中的投毒攻击，同时保持通信效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在保护数据隐私的同时面临通信效率和投毒攻击的挑战，稀疏化技术虽减少通信开销但增加了安全风险。现有防御机制在稀疏化FL中效果不佳。

Method: 提出FLARE框架，结合稀疏索引掩码检查和模型更新符号相似性分析，以检测和缓解投毒攻击。

Result: 在多数据集和对抗场景下的实验表明，FLARE显著优于现有防御策略，有效保护稀疏化FL。

Conclusion: FLARE成功解决了稀疏化联邦学习中的投毒攻击问题，同时保持了通信效率。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy, yet it faces significant
challenges in communication efficiency and vulnerability to poisoning attacks.
While sparsification techniques mitigate communication overhead by transmitting
only critical model parameters, they inadvertently amplify security risks:
adversarial clients can exploit sparse updates to evade detection and degrade
model performance. Existing defense mechanisms, designed for standard FL
communication scenarios, are ineffective in addressing these vulnerabilities
within sparsified FL. To bridge this gap, we propose FLARE, a novel federated
learning framework that integrates sparse index mask inspection and model
update sign similarity analysis to detect and mitigate poisoning attacks in
sparsified FL. Extensive experiments across multiple datasets and adversarial
scenarios demonstrate that FLARE significantly outperforms existing defense
strategies, effectively securing sparsified FL against poisoning attacks while
maintaining communication efficiency.

</details>

### [3] [Development of an Adapter for Analyzing and Protecting Machine Learning Models from Competitive Activity in the Networks Services](https://arxiv.org/abs/2505.01460)
*Denis Parfenov,Anton Parfenov*

Main category: cs.CR

TLDR: 本文讨论了使用机器学习模型分类网络流量的问题，并提出了基于自动编码器的解决方案以保护模型免受攻击。


<details>
  <summary>Details</summary>
Motivation: 随着远程服务器处理任务的增加，流量分类对减轻服务器负载至关重要，但现有ML模型易受攻击影响分类结果。

Method: 提出了一种基于自动编码器的解决方案，用于保护机器学习模型免受攻击。

Result: 该方法能够有效保护模型，提高流量分类的准确性。

Conclusion: 基于自动编码器的解决方案为保护ML模型免受攻击提供了有效途径。

Abstract: Due to the increasing number of tasks that are solved on remote servers,
identifying and classifying traffic is an important task to reduce the load on
the server. There are various methods for classifying traffic. This paper
discusses machine learning models for solving this problem. However, such ML
models are also subject to attacks that affect the classification result of
network traffic. To protect models, we proposed a solution based on an
autoencoder

</details>

### [4] [Enhancing the Cloud Security through Topic Modelling](https://arxiv.org/abs/2505.01463)
*Sabbir M. Saleh,Nazim Madhavji,John Steinbacher*

Main category: cs.CR

TLDR: 该研究利用NLP技术（如LDA和pLSA）分析云安全数据，以预测未来攻击并检测CI/CD管道中的漏洞。


<details>
  <summary>Details</summary>
Motivation: 在网络安全威胁日益严重的背景下，通过NLP方法提升云应用的安全性。

Method: 使用LDA和pLSA算法分析安全相关文本数据（如报告和日志），并通过Python的Gensim框架实现。

Result: 通过主题建模识别漏洞，改进CI/CD管道的安全性。

Conclusion: 主题建模为漏洞检测提供了新方法，有望提升云安全。

Abstract: Protecting cloud applications is crucial in an age where security constantly
threatens the digital world. The inevitable cyber-attacks throughout the CI/CD
pipeline make cloud security innovations necessary. This research is motivated
by applying Natural Language Processing (NLP) methodologies, such as Topic
Modelling, to analyse cloud security data and predict future attacks. This
research aims to use topic modelling, specifically Latent Dirichlet Allocation
(LDA) and Probabilistic Latent Semantic Analysis (pLSA). Utilising LDA and
PLSA, security-related text data, such as reports, logs, and other relevant
documents, will be analysed and sorted into relevant topics (such as phishing
or encryption). These algorithms may apply through Python using the Gensim
framework. The topics shall be utilised to detect vulnerabilities within
relevant CI/CD pipeline records or log data. This application of Topic
Modelling anticipates providing a new form of vulnerability detection,
improving overall security throughout the CI/CD pipeline.

</details>

### [5] [SafeTab-P: Disclosure Avoidance for the 2020 Census Detailed Demographic and Housing Characteristics File A (Detailed DHC-A)](https://arxiv.org/abs/2505.01472)
*Sam Haney,Skye Berghel,Bayard Carlson,Ryan Cumings-Menon,Luke Hartman,Michael Hay,Ashwin Machanavajjhala,Gerome Miklau,Amritha Pai,Simran Rajpal,David Pujol,William Sexton,Ruchit Shrestha,Daniel Simmons-Marengo*

Main category: cs.CR

TLDR: 本文介绍了美国人口普查局用于保护2020年人口普查详细人口和住房特征文件A（Detailed DHC-A）的披露避免算法SafeTab-P，该算法基于离散高斯分布添加噪声，并自适应选择统计数据的发布粒度。


<details>
  <summary>Details</summary>
Motivation: 保护人口普查数据的隐私，同时确保统计数据的可用性。

Method: 使用SafeTab-P算法，基于离散高斯分布添加噪声，并自适应选择统计数据的发布粒度。

Result: 算法满足零集中差分隐私（zCDP），并在Tumult Analytics上实现。

Conclusion: SafeTab-P算法有效平衡了数据隐私保护和统计可用性。

Abstract: This article describes the disclosure avoidance algorithm that the U.S.
Census Bureau used to protect the Detailed Demographic and Housing
Characteristics File A (Detailed DHC-A) of the 2020 Census. The tabulations
contain statistics (counts) of demographic characteristics of the entire
population of the United States, crossed with detailed races and ethnicities at
varying levels of geography. The article describes the SafeTab-P algorithm,
which is based on adding noise drawn to statistics of interest from a discrete
Gaussian distribution. A key innovation in SafeTab-P is the ability to
adaptively choose how many statistics and at what granularity to release them,
depending on the size of a population group. We prove that the algorithm
satisfies a well-studied variant of differential privacy, called
zero-concentrated differential privacy (zCDP). We then describe how the
algorithm was implemented on Tumult Analytics and briefly outline the
parameterization and tuning of the algorithm.

</details>

### [6] [Watermark Overwriting Attack on StegaStamp algorithm](https://arxiv.org/abs/2505.01474)
*I. F. Serzhenko,L. A. Khaertdinova,M. A. Pautov,A. V. Antsiferova*

Main category: cs.CR

TLDR: 提出一种攻击StegaStamp水印算法的方法，能完全去除水印且图像质量损失最小。


<details>
  <summary>Details</summary>
Motivation: 针对NeurIPS竞赛“擦除不可见”的需求，探索水印去除技术。

Method: 开发了一种攻击方法，针对StegaStamp水印算法。

Result: 成功完全去除水印，且图像质量损失极小。

Conclusion: 该方法有效解决了水印去除问题，适用于竞赛目标。

Abstract: This paper presents an attack method on the StegaStamp watermarking algorithm
that completely removes watermarks from an image with minimal quality loss,
developed as part of the NeurIPS "Erasing the invisible" competition.

</details>

### [7] [LLM Watermarking Using Mixtures and Statistical-to-Computational Gaps](https://arxiv.org/abs/2505.01484)
*Pedro Abdalla,Roman Vershynin*

Main category: cs.CR

TLDR: 提出了一种在封闭和开放设置下分别不可检测和不可移除的水印方案，用于区分LLM生成和人类生成的文本。


<details>
  <summary>Details</summary>
Motivation: 研究如何区分大型语言模型生成的文本与人类生成的文本，水印技术是一种广泛研究的方法。

Method: 在封闭设置中提出不可检测的水印方案；在开放设置中提出不可移除的水印方案。

Result: 成功设计出两种水印方案，分别适用于不同场景。

Conclusion: 提出的水印方案在区分LLM生成文本和人类文本方面具有潜力。

Abstract: Given a text, can we determine whether it was generated by a large language
model (LLM) or by a human? A widely studied approach to this problem is
watermarking. We propose an undetectable and elementary watermarking scheme in
the closed setting. Also, in the harder open setting, where the adversary has
access to most of the model, we propose an unremovable watermarking scheme.

</details>

### [8] [Securing the Future of IVR: AI-Driven Innovation with Agile Security, Data Regulation, and Ethical AI Integration](https://arxiv.org/abs/2505.01514)
*Khushbu Mehboob Shaikh,Georgios Giannakopoulos*

Main category: cs.CR

TLDR: 论文探讨了AI驱动的交互式语音应答（IVR）技术的安全与伦理问题，提出了一个结合敏捷安全原则、全球数据法规和用户伦理的治理框架。


<details>
  <summary>Details</summary>
Motivation: 随着IVR技术数字化和AI化，其安全、合规和伦理设计变得至关重要。

Method: 通过分析IVR从静态代码到AI驱动系统的演变，提出一个以网络安全为中心的治理框架。

Result: 框架强调隐私设计、自适应风险建模和透明度，确保IVR系统智能、安全且符合社会期望。

Conclusion: 伦理AI集成是战略需求，现代IVR需发展为智能、安全的数字前线。

Abstract: The rapid digitalization of communication systems has elevated Interactive
Voice Response (IVR) technologies to become critical interfaces for customer
engagement. With Artificial Intelligence (AI) now driving these platforms,
ensuring secure, compliant, and ethically designed development practices is
more imperative than ever. AI-powered IVRs leverage Natural Language Processing
(NLP) and Machine Learning (ML) to personalize interactions, automate service
delivery, and optimize user experiences. However, these innovations expose
systems to heightened risks, including data privacy breaches, AI decision
opacity, and model security vulnerabilities. This paper analyzes the evolution
of IVRs from static code-based designs to adaptive AI-driven systems,
presenting a cybersecurity-centric perspective. We propose a practical
governance framework that embeds agile security principles, compliance with
global data legislation, and user-centric ethics. Emphasizing
privacy-by-design, adaptive risk modeling, and transparency, the paper argues
that ethical AI integration is not a feature but a strategic imperative.
Through this multidimensional lens, we highlight how modern IVRs can transition
from communication tools to intelligent, secure, and accountable digital
frontlines-resilient against emerging threats and aligned with societal
expectations.

</details>

### [9] [Rubber Mallet: A Study of High Frequency Localized Bit Flips and Their Impact on Security](https://arxiv.org/abs/2505.01518)
*Andrew Adiletta,Zane Weissman,Fatemeh Khojasteh Dana,Berk Sunar,Shahin Tajik*

Main category: cs.CR

TLDR: 论文分析了现代DRAM因密度增加而更易受Rowhammer攻击的影响，揭示了相邻位翻转的高频现象及其对加密和语言模型的攻击潜力。


<details>
  <summary>Details</summary>
Motivation: 研究Rowhammer攻击中相邻位翻转的高频现象及其对现有硬件防御的绕过能力，以揭示当前内存保护方案的不足。

Method: 通过实验分析Rowhammer攻击中相邻位翻转的分布规律，并设计两种攻击：针对加密签名的修正攻击和针对大型语言模型的令牌替换攻击。

Result: 实验表明，相邻位翻转频率显著高于预期，且能高效实现加密密钥恢复和语言模型安全指令绕过。

Conclusion: 当前内存保护方案无法抵御这些精确攻击，需改进防御机制。

Abstract: The increasing density of modern DRAM has heightened its vulnerability to
Rowhammer attacks, which induce bit flips by repeatedly accessing specific
memory rows. This paper presents an analysis of bit flip patterns generated by
advanced Rowhammer techniques that bypass existing hardware defenses. First, we
investigate the phenomenon of adjacent bit flips--where two or more physically
neighboring bits are corrupted simultaneously--and demonstrate they occur with
significantly higher frequency than previously documented. We also show that if
multiple bits flip within a byte, they are more likely to be adjacent than
randomly distributed: for example, if 4 bits flip within a byte, there is an
87% chance that they are all adjacent. We also demonstrate that bit flips
within a row will naturally cluster together likely due to the underlying
physics of the attack. We then investigate two fault injection attacks enabled
by multiple adjacent or nearby bit flips. First, we show how these correlated
flips enable efficient cryptographic signature correction attacks, successfully
recovering ECDSA private keys from OpenSSL implementations where single-bit
approaches would be unfeasible. Second, we introduce a targeted attack against
large language models by exploiting Rowhammer-induced corruptions in tokenizer
dictionaries of GGUF model files. This attack effectively rewrites safety
instructions in system prompts by swapping safety-critical tokens with benign
alternatives, circumventing model guardrails while maintaining normal
functionality in other contexts. Our experimental results across multiple DRAM
configurations reveal that current memory protection schemes are inadequate
against these sophisticated attack vectors, which can achieve their objectives
with precise, minimal modifications rather than random corruption.

</details>

### [10] [The DCR Delusion: Measuring the Privacy Risk of Synthetic Data](https://arxiv.org/abs/2505.01524)
*Zexi Yao,Nataša Krčo,Georgi Ganev,Yves-Alexandre de Montjoye*

Main category: cs.CR

TLDR: 论文指出，尽管距离最近记录（DCR）等简单代理指标被广泛用于评估合成数据的隐私性，但这些指标无法有效识别隐私泄露，且与成员推理攻击（MIA）的实际风险无关。


<details>
  <summary>Details</summary>
Motivation: 当前实践中，合成数据的隐私评估依赖DCR等简单指标，但这些指标存在设计缺陷，无法真实反映隐私风险，可能导致误导性的隐私声明。

Method: 通过多数据集和多种模型（如Baynet、CTGAN和扩散模型）的实验，验证DCR等指标在识别隐私泄露上的失败，并分析其设计缺陷。

Result: 实验表明，被代理指标判定为隐私的合成数据仍易受MIA攻击，且这些指标的连续测量和二元测试均无法反映实际隐私风险。

Conclusion: 建议放弃代理指标，采用MIA作为评估合成数据隐私的严格标准，特别是对法律匿名性声明。

Abstract: Synthetic data has become an increasingly popular way to share data without
revealing sensitive information. Though Membership Inference Attacks (MIAs) are
widely considered the gold standard for empirically assessing the privacy of a
synthetic dataset, practitioners and researchers often rely on simpler proxy
metrics such as Distance to Closest Record (DCR). These metrics estimate
privacy by measuring the similarity between the training data and generated
synthetic data. This similarity is also compared against that between the
training data and a disjoint holdout set of real records to construct a binary
privacy test. If the synthetic data is not more similar to the training data
than the holdout set is, it passes the test and is considered private. In this
work we show that, while computationally inexpensive, DCR and other
distance-based metrics fail to identify privacy leakage. Across multiple
datasets and both classical models such as Baynet and CTGAN and more recent
diffusion models, we show that datasets deemed private by proxy metrics are
highly vulnerable to MIAs. We similarly find both the binary privacy test and
the continuous measure based on these metrics to be uninformative of actual
membership inference risk. We further show that these failures are consistent
across different metric hyperparameter settings and record selection methods.
Finally, we argue DCR and other distance-based metrics to be flawed by design
and show a example of a simple leakage they miss in practice. With this work,
we hope to motivate practitioners to move away from proxy metrics to MIAs as
the rigorous, comprehensive standard of evaluating privacy of synthetic data,
in particular to make claims of datasets being legally anonymous.

</details>

### [11] [Unified Steganography via Implicit Neural Representation](https://arxiv.org/abs/2505.01749)
*Qi Song,Ziyuan Luo,Xiufeng Huang,Sheng Li,Renjie Wan*

Main category: cs.CR

TLDR: U-INR是一种基于隐式神经表示（INR）的新型隐写方法，通过共享私钥在不同数据类型中隐藏和提取秘密数据，提高了通用性和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统隐写方法需要为每种数据类型设计特定框架，限制了通用性。U-INR旨在通过INR网络实现跨数据类型的隐写。

Method: 利用INR网络的神经元表示秘密数据和载体数据，通过共享私钥确定数据位置，并引入基于密钥的选择策略。

Result: 在图像、视频、音频等多种数据类型上的实验证明了U-INR的通用性和有效性。

Conclusion: U-INR具有广泛的应用潜力，可提升数据安全和隐私保护。

Abstract: Digital steganography is the practice of concealing for encrypted data
transmission. Typically, steganography methods embed secret data into cover
data to create stega data that incorporates hidden secret data. However,
steganography techniques often require designing specific frameworks for each
data type, which restricts their generalizability. In this paper, we present
U-INR, a novel method for steganography via Implicit Neural Representation
(INR). Rather than using the specific framework for each data format, we
directly use the neurons of the INR network to represent the secret data and
cover data across different data types. To achieve this idea, a private key is
shared between the data sender and receivers. Such a private key can be used to
determine the position of secret data in INR networks. To effectively leverage
this key, we further introduce a key-based selection strategy that can be used
to determine the position within the INRs for data storage. Comprehensive
experiments across multiple data types, including images, videos, audio, and
SDF and NeRF, demonstrate the generalizability and effectiveness of U-INR,
emphasizing its potential for improving data security and privacy in various
applications.

</details>

### [12] [Energy-Efficient NTT Sampler for Kyber Benchmarked on FPGA](https://arxiv.org/abs/2505.01782)
*Paresh Baidya,Rourab Paul,Vikas Srivastava,Sumit Kumar Debnath*

Main category: cs.CR

TLDR: 论文提出了一种名为Modified SampleNTT的高效采样算法，用于Kyber密钥生成过程中的矩阵元素采样，相比传统方法减少了33%的SHAKE-128比特需求，显著降低了延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: Kyber的密钥生成过程中，矩阵元素的均匀采样是计算密集型任务，对低功耗嵌入式系统性能影响显著。现有方法依赖拒绝采样，导致高延迟和能耗。

Method: 提出Modified SampleNTT算法，减少SHAKE-128的比特需求，仅需两次挤压即可生成完整多项式，成功率达99.16%。

Result: 相比传统SampleNTT，能耗降低33.14%，延迟减少33.32%，FPGA实现中切片使用减少0.28%，并通过所有随机性测试。

Conclusion: Modified SampleNTT是高效且实用的替代方案，特别适用于低功耗安全处理器中的PQC方案。

Abstract: Kyber is a lattice-based key encapsulation mechanism selected for
standardization by the NIST Post-Quantum Cryptography (PQC) project. A critical
component of Kyber's key generation process is the sampling of matrix elements
from a uniform distribution over the ring Rq . This step is one of the most
computationally intensive tasks in the scheme, significantly impacting
performance in low-power embedded systems such as Internet of Things (IoT),
wearable devices, wireless sensor networks (WSNs), smart cards, TPMs (Trusted
Platform Modules), etc. Existing approaches to this sampling, notably
conventional SampleNTT and Parse-SPDM3, rely on rejection sampling. Both
algorithms require a large number of random bytes, which needs at least three
SHAKE-128 squeezing steps per polynomial. As a result, it causes significant
amount of latency and energy. In this work, we propose a novel and efficient
sampling algorithm, namely Modified SampleNTT, which substantially educes the
average number of bits required from SHAKE-128 to generate elements in Rq -
achieving approximately a 33% reduction compared to conventional SampleNTT.
Modified SampleNTT achieves 99.16% success in generating a complete polynomial
using only two SHAKE-128 squeezes, outperforming both state-of-the-art methods,
which never succeed in two squeezes of SHAKE-128. Furthermore, our algorithm
maintains the same average rejection rate as existing techniques and passes all
standard statistical tests for randomness quality. FPGA implementation on
Artix-7 demonstrates a 33.14% reduction in energy, 33.32% lower latency, and
0.28% fewer slices compared to SampleNTT. Our results confirm that Modified
SampleNTT is an efficient and practical alternative for uniform polynomial
sampling in PQC schemes such as Kyber, especially for low-power security
processors.

</details>

### [13] [Backdoor Attacks Against Patch-based Mixture of Experts](https://arxiv.org/abs/2505.01811)
*Cedric Chan,Jona te Lintelo,Stjepan Picek*

Main category: cs.CR

TLDR: 研究了基于补丁的混合专家（pMoE）模型在图像分类任务中对后门攻击的脆弱性，并探讨了触发生成方法和Fine-Pruning防御的效果。


<details>
  <summary>Details</summary>
Motivation: 随着混合专家（MoE）模型在减少计算复杂度方面的流行，其安全性研究尚未得到足够关注，尤其是后门攻击的脆弱性。

Method: 通过多种触发生成方法对pMoE模型进行后门攻击测试，并评估Fine-Pruning防御的效果。同时分析了影响模型补丁选择的因素。

Result: pMoE模型对后门攻击高度脆弱，攻击成功率可达100%，而干净准确率仅下降1.0%。Fine-Pruning防御中，剪枝无效，但微调可几乎完全消除后门。

Conclusion: pMoE模型易受后门攻击，但通过微调可有效防御，牺牲少量准确率即可显著降低攻击成功率。

Abstract: As Deep Neural Networks (DNNs) continue to require larger amounts of data and
computational power, Mixture of Experts (MoE) models have become a popular
choice to reduce computational complexity. This popularity increases the
importance of considering the security of MoE architectures. Unfortunately, the
security of models using a MoE architecture has not yet gained much attention
compared to other DNN models. In this work, we investigate the vulnerability of
patch-based MoE (pMoE) models for image classification against backdoor
attacks. We examine multiple trigger generation methods and Fine-Pruning as a
defense. To better understand a pMoE model's vulnerability to backdoor attacks,
we investigate which factors affect the model's patch selection. Our work shows
that pMoE models are highly susceptible to backdoor attacks. More precisely, we
achieve high attack success rates of up to 100% with visible triggers and a 2%
poisoning rate, whilst only having a clean accuracy drop of 1.0%. Additionally,
we show that pruning itself is ineffective as a defense but that fine-tuning
can remove the backdoor almost completely. Our results show that fine-tuning
the model for five epochs reduces the attack success rate to 2.1% whilst
sacrificing 1.4% accuracy.

</details>

### [14] [Rogue Cell: Adversarial Attack and Defense in Untrusted O-RAN Setup Exploiting the Traffic Steering xApp](https://arxiv.org/abs/2505.01816)
*Eran Aizikovich,Dudu Mimran,Edita Grolman,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TLDR: 本文探讨了O-RAN架构从单运营商转向多运营商时引入的安全挑战，提出了攻击APATE和检测框架MARRS，并验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 研究O-RAN在多运营商架构下的安全漏洞，填补了此前研究对智能组件以外威胁的空白。

Method: 开发了集成无线网络模拟器和O-RAN RIC集群的测试环境，用于演示APATE攻击和验证MARRS检测框架。

Result: APATE攻击可使攻击者获得248.5%的额外UE分配；MARRS检测准确率达99.2%，F1分数0.978。

Conclusion: 多运营商O-RAN架构存在安全风险，但MARRS框架能有效检测恶意活动。

Abstract: The Open Radio Access Network (O-RAN) architecture is revolutionizing
cellular networks with its open, multi-vendor design and AI-driven management,
aiming to enhance flexibility and reduce costs. Although it has many
advantages, O-RAN is not threat-free. While previous studies have mainly
examined vulnerabilities arising from O-RAN's intelligent components, this
paper is the first to focus on the security challenges and vulnerabilities
introduced by transitioning from single-operator to multi-operator RAN
architectures. This shift increases the risk of untrusted third-party operators
managing different parts of the network. To explore these vulnerabilities and
their potential mitigation, we developed an open-access testbed environment
that integrates a wireless network simulator with the official O-RAN Software
Community (OSC) RAN intelligent component (RIC) cluster. This environment
enables realistic, live data collection and serves as a platform for
demonstrating APATE (adversarial perturbation against traffic efficiency), an
evasion attack in which a malicious cell manipulates its reported key
performance indicators (KPIs) and deceives the O-RAN traffic steering to gain
unfair allocations of user equipment (UE). To ensure that O-RAN's legitimate
activity continues, we introduce MARRS (monitoring adversarial RAN reports), a
detection framework based on a long-short term memory (LSTM) autoencoder (AE)
that learns contextual features across the network to monitor malicious
telemetry (also demonstrated in our testbed). Our evaluation showed that by
executing APATE, an attacker can obtain a 248.5% greater UE allocation than it
was supposed to in a benign scenario. In addition, the MARRS detection method
was also shown to successfully classify malicious cell activity, achieving
accuracy of 99.2% and an F1 score of 0.978.

</details>

### [15] [M-ary Precomputation-Based Accelerated Scalar Multiplication Algorithms for Enhanced Elliptic Curve Cryptography](https://arxiv.org/abs/2505.01845)
*Tongxi Wu,Xufeng Liu,Jin Yang,Yijie Zhu,Shunyang Zeng,Mingming Zhan*

Main category: cs.CR

TLDR: 本文提出了一种基于M-ary预计算的标量乘法算法，显著提升了椭圆曲线密码学（ECC）的计算效率和内存使用效率。


<details>
  <summary>Details</summary>
Motivation: 标量乘法的高效性对ECC性能至关重要，尤其是在大规模或实时加密应用中。

Method: 采用M-ary预计算方法，优化计算和内存复杂度。

Result: 在secp256k1上，加密时间减少59%，内存节省30%；在网络模拟中，通信时间和模拟时间分别减少22.1%和25.4%。

Conclusion: 算法具有可扩展性、高效性和实际应用价值，源代码将公开。

Abstract: Efficient scalar multiplication is critical for enhancing the performance of
elliptic curve cryptography (ECC), especially in applications requiring
large-scale or real-time cryptographic operations. This paper proposes an M-ary
precomputation-based scalar multiplication algorithm, aiming to optimize both
computational efficiency and memory usage. The method reduces the time
complexity from $\Theta(Q \log p)$ to $\Theta\left(\frac{Q \log p}{\log
Q}\right)$ and achieves a memory complexity of $\Theta\left(\frac{Q \log
p}{\log^2 Q}\right)$. Experiments on ElGamal encryption and NS3-based
communication simulations validate its effectiveness. On secp256k1, the
proposed method achieves up to a 59\% reduction in encryption time and 30\%
memory savings. In network simulations, the binary-optimized variant reduces
communication time by 22.1\% on secp384r1 and simulation time by 25.4\% on
secp521r1. The results demonstrate the scalability, efficiency, and practical
applicability of the proposed algorithm. The source code will be publicly
released upon acceptance.

</details>

### [16] [PQS-BFL: A Post-Quantum Secure Blockchain-based Federated Learning Framework](https://arxiv.org/abs/2505.01866)
*Daniel Commey,Garth V. Crosby*

Main category: cs.CR

TLDR: PQS-BFL框架结合后量子密码学和区块链技术，为联邦学习提供量子攻击防护，同时保持高效性能和模型准确性。


<details>
  <summary>Details</summary>
Motivation: 经典密码学在量子攻击下存在漏洞，尤其在敏感领域（如医疗）中，亟需量子安全的解决方案。

Method: 采用ML-DSA-65签名认证模型更新，结合优化的智能合约进行去中心化验证。

Result: PQS-BFL在多种数据集上表现高效（签名时间0.65 ms，验证时间0.53 ms），区块链开销可控（交易时间4.8 s），模型准确性高（如MNIST达98.8%）。

Conclusion: PQS-BFL验证了量子安全联邦学习的可行性，适合实际部署。

Abstract: Federated Learning (FL) enables collaborative model training while preserving
data privacy, but its classical cryptographic underpinnings are vulnerable to
quantum attacks. This vulnerability is particularly critical in sensitive
domains like healthcare. This paper introduces PQS-BFL (Post-Quantum Secure
Blockchain-based Federated Learning), a framework integrating post-quantum
cryptography (PQC) with blockchain verification to secure FL against quantum
adversaries. We employ ML-DSA-65 (a FIPS 204 standard candidate, formerly
Dilithium) signatures to authenticate model updates and leverage optimized
smart contracts for decentralized validation. Extensive evaluations on diverse
datasets (MNIST, SVHN, HAR) demonstrate that PQS-BFL achieves efficient
cryptographic operations (average PQC sign time: 0.65 ms, verify time: 0.53 ms)
with a fixed signature size of 3309 Bytes. Blockchain integration incurs a
manageable overhead, with average transaction times around 4.8 s and gas usage
per update averaging 1.72 x 10^6 units for PQC configurations. Crucially, the
cryptographic overhead relative to transaction time remains minimal (around
0.01-0.02% for PQC with blockchain), confirming that PQC performance is not the
bottleneck in blockchain-based FL. The system maintains competitive model
accuracy (e.g., over 98.8% for MNIST with PQC) and scales effectively, with
round times showing sublinear growth with increasing client numbers. Our
open-source implementation and reproducible benchmarks validate the feasibility
of deploying long-term, quantum-resistant security in practical FL systems.

</details>

### [17] [An Approach for Handling Missing Attribute Values in Attribute-Based Access Control Policy Mining](https://arxiv.org/abs/2505.01873)
*Thang Bui,Elliot Shabram,Anthony Matricia*

Main category: cs.CR

TLDR: 本文提出了一种通过预测缺失属性值来改进ABAC策略挖掘的方法，利用上下文聚类技术处理不完整数据，优化授权决策。


<details>
  <summary>Details</summary>
Motivation: ABAC策略挖掘中，实体信息不完整（缺失属性值）是一个主要挑战，本文旨在解决这一问题。

Method: 采用上下文聚类技术，根据已知属性对实体分组，分析并优化授权决策。

Result: 方法有效管理不完整数据，帮助安全管理员改进属性数据，促进向ABAC的平滑过渡。

Conclusion: 该方法为ABAC策略挖掘提供了实用工具，提升了数据完整性和迁移效率。

Abstract: Attribute-Based Access Control (ABAC) enables highly expressive and flexible
access decisions by considering a wide range of contextual attributes. ABAC
policies use logical expressions that combine these attributes, allowing for
precise and context-aware control. Algorithms that mine ABAC policies from
legacy access control systems can significantly reduce the costs associated
with migrating to ABAC. However, a major challenge in this process is handling
incomplete entity information, where some attribute values are missing.
  This paper introduces an approach that enhances the policy mining process by
predicting or inferring missing attribute values. This is accomplished by
employing a contextual clustering technique that groups entities according to
their known attributes, which are then used to analyze and refine authorization
decisions. By effectively managing incomplete data, our approach provides
security administrators with a valuable tool to improve their attribute data
and ensure a smoother, more efficient transition to ABAC.

</details>

### [18] [UK Finfluencers: Exploring Content, Reach, and Responsibility](https://arxiv.org/abs/2505.01941)
*Essam Ghadafi,Panagiotis Andriotis*

Main category: cs.CR

TLDR: 研究分析了英国TikTok上金融影响者的行为模式，关注内容分类、情绪趋势和免责声明的作用，旨在提出保护公众免受潜在财务危害的指南。


<details>
  <summary>Details</summary>
Motivation: 金融影响者的兴起使财务建议更易获取，但其缺乏正式资质，信息准确性存疑，需研究其行为模式以保护公众。

Method: 分析TikTok上金融影响者的内容分类、情绪趋势及免责声明，识别互动模式。

Result: 提供了金融影响者内容的详细分析，揭示了常见互动模式及免责声明的作用。

Conclusion: 研究为社交媒体上的金融传播提出了更安全、透明的建议。

Abstract: The rise of social media financial influencers (finfluencers) has
significantly transformed the personal finance landscape, making financial
advice and insights more accessible to a broader and younger audience. By
leveraging digital platforms, these influencers have contributed to the
democratization of financial literacy. However, the line between education and
promotion is often blurred, as many finfluencers lack formal financial
qualifications, raising concerns about the accuracy and reliability of the
information they share. This study investigates the patterns and behaviours of
finfluencers in the UK on TikTok, focusing not on individual actions but on
broader trends and the interactions between influencers and their followers.
The aim is to identify common engagement patterns and propose guidelines that
can help protect the public from potential financial harm. Specifically, the
paper contributes a detailed analysis of finfluencer content categorization,
sentiment trends, and the prevalence and role of disclaimers, offering
empirical insights that inform recommendations for safer and more transparent
financial communication on social media.

</details>

### [19] [A Survey on Privacy Risks and Protection in Large Language Models](https://arxiv.org/abs/2505.01976)
*Kang Chen,Xiuze Zhou,Yuanguo Lin,Shibo Feng,Li Shen,Pengcheng Wu*

Main category: cs.CR

TLDR: 该论文综述了大型语言模型（LLMs）的隐私风险及现有解决方案，分析了隐私泄露机制和攻击方式，并评估了现有保护措施的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，其隐私问题日益突出，亟需系统性的研究和解决方案。

Method: 通过分析隐私泄露机制（如模型反转、训练数据提取等）和现有保护技术（如联邦学习、机密计算等），评估其效果。

Result: 揭示了LLMs隐私风险的具体形式和现有解决方案的局限性，提出了未来研究方向。

Conclusion: 论文为LLMs隐私保护提供了研究路线图，强调隐私评估、安全知识转移和跨学科治理框架的重要性。

Abstract: Although Large Language Models (LLMs) have become increasingly integral to
diverse applications, their capabilities raise significant privacy concerns.
This survey offers a comprehensive overview of privacy risks associated with
LLMs and examines current solutions to mitigate these challenges. First, we
analyze privacy leakage and attacks in LLMs, focusing on how these models
unintentionally expose sensitive information through techniques such as model
inversion, training data extraction, and membership inference. We investigate
the mechanisms of privacy leakage, including the unauthorized extraction of
training data and the potential exploitation of these vulnerabilities by
malicious actors. Next, we review existing privacy protection against such
risks, such as inference detection, federated learning, backdoor mitigation,
and confidential computing, and assess their effectiveness in preventing
privacy leakage. Furthermore, we highlight key practical challenges and propose
future research directions to develop secure and privacy-preserving LLMs,
emphasizing privacy risk assessment, secure knowledge transfer between models,
and interdisciplinary frameworks for privacy governance. Ultimately, this
survey aims to establish a roadmap for addressing escalating privacy challenges
in the LLMs domain.

</details>

### [20] [Triple-identity Authentication: The Future of Secure Access](https://arxiv.org/abs/2505.02004)
*Suyun Borjigin*

Main category: cs.CR

TLDR: 本文提出了一种三重身份认证系统，通过公开哈希算法的内部结构，结合用户凭证和设备信息生成唯一标识符，实现高安全性。


<details>
  <summary>Details</summary>
Motivation: 传统密码加密方式存在安全风险，研究转向建立更有效的系统交互门控机制。

Method: 公开哈希算法结构，结合登录名、密码和设备信息生成唯一标识符，分三阶段验证身份。

Result: 实现了与多因素认证相当的安全级别，建立了稳健的系统交互门控机制。

Conclusion: 三重身份认证系统为系统交互提供了高安全性，可作为多因素认证的替代方案。

Abstract: In a typical authentication process, the local system verifies the user's
identity using a stored hash value generated by a cross-system hash algorithm.
This article shifts the research focus from traditional password encryption to
the establishment of gatekeeping mechanisms for effective interactions between
a system and the outside world. Here, we propose a triple-identity
authentication system to achieve this goal. Specifically, this local system
opens the inner structure of its hash algorithm to all user credentials,
including the login name, login password, and authentication password. When a
login credential is entered, the local system hashes it and then creates a
unique identifier using intermediate hash elements randomly selected from the
open algorithm. Importantly, this locally generated unique identifier (rather
than the stored hash produced by the open algorithm) is utilized to verify the
user's combined identity, which is generated by combining the entered
credential with the International Mobile Equipment Identity and the
International Mobile Subscriber Identity. The verification process is
implemented at each interaction point: the login name field, the login password
field, and the server's authentication point. Thus, within the context of this
triple-identity authentication system, we establish a robust gatekeeping
mechanism for system interactions, ultimately providing a level of security
that is equivalent to multi-factor authentication.

</details>

### [21] [Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents](https://arxiv.org/abs/2505.02077)
*Christian Schroeder de Witt*

Main category: cs.CR

TLDR: 论文提出了“多智能体安全”这一新领域，专注于解决去中心化AI智能体交互中产生的安全威胁，并探讨了安全与性能的权衡。


<details>
  <summary>Details</summary>
Motivation: 去中心化AI智能体在互联网平台上的交互带来了传统网络安全和AI安全框架无法应对的新挑战，如秘密共谋、群体攻击等，但目前相关研究分散且不足。

Method: 论文通过（1）分类智能体交互中的威胁，（2）调查去中心化AI系统的安全与性能权衡，（3）提出统一的研究议程，来填补这一领域的空白。

Result: 初步工作包括对威胁的分类、安全与性能权衡的调查，以及设计安全智能体系统和交互环境的开放挑战。

Conclusion: 论文旨在推动这一关键领域的研究，以释放大规模智能体部署的社会经济潜力，增强公众信任，并降低国家安全风险。

Abstract: Decentralized AI agents will soon interact across internet platforms,
creating security challenges beyond traditional cybersecurity and AI safety
frameworks. Free-form protocols are essential for AI's task generalization but
enable new threats like secret collusion and coordinated swarm attacks. Network
effects can rapidly spread privacy breaches, disinformation, jailbreaks, and
data poisoning, while multi-agent dispersion and stealth optimization help
adversaries evade oversightcreating novel persistent threats at a systemic
level. Despite their critical importance, these security challenges remain
understudied, with research fragmented across disparate fields including AI
security, multi-agent learning, complex systems, cybersecurity, game theory,
distributed systems, and technical AI governance. We introduce
\textbf{multi-agent security}, a new field dedicated to securing networks of
decentralized AI agents against threats that emerge or amplify through their
interactionswhether direct or indirect via shared environmentswith each other,
humans, and institutions, and characterize fundamental security-performance
trade-offs. Our preliminary work (1) taxonomizes the threat landscape arising
from interacting AI agents, (2) surveys security-performance tradeoffs in
decentralized AI systems, and (3) proposes a unified research agenda addressing
open challenges in designing secure agent systems and interaction environments.
By identifying these gaps, we aim to guide research in this critical area to
unlock the socioeconomic potential of large-scale agent deployment on the
internet, foster public trust, and mitigate national security risks in critical
infrastructure and defense contexts.

</details>

### [22] [Enhanced Outsourced and Secure Inference for Tall Sparse Decision Trees](https://arxiv.org/abs/2505.02224)
*Andrew Quijano,Spyros T. Halkidis,Kevin Gallagher,Kemal Akkaya,Nikolaos Samaras*

Main category: cs.CR

TLDR: 提出了一种新的决策树推断协议，通过将模型分层次存储在多实体中，提高了效率并增强了抗侧信道攻击能力。


<details>
  <summary>Details</summary>
Motivation: 解决隐私保护和模型安全外包的需求，同时改进现有协议的低效性和易受侧信道攻击的问题。

Method: 将决策树模型按层次分区存储在多实体（“level-site”）中，实现共享和评估。

Result: 提高了非完整树的分类器评估平均运行时间，并有效缓解了侧信道攻击。

Conclusion: 新协议在效率和安全性上均有显著提升，适用于隐私保护和模型外包场景。

Abstract: A decision tree is an easy-to-understand tool that has been widely used for
classification tasks. On the one hand, due to privacy concerns, there has been
an urgent need to create privacy-preserving classifiers that conceal the user's
input from the classifier. On the other hand, with the rise of cloud computing,
data owners are keen to reduce risk by outsourcing their model, but want
security guarantees that third parties cannot steal their decision tree model.
To address these issues, Joye and Salehi introduced a theoretical protocol that
efficiently evaluates decision trees while maintaining privacy by leveraging
their comparison protocol that is resistant to timing attacks. However, their
approach was not only inefficient but also prone to side-channel attacks.
Therefore, in this paper, we propose a new decision tree inference protocol in
which the model is shared and evaluated among multiple entities. We partition
our decision tree model by each level to be stored in a new entity we refer to
as a "level-site." Utilizing this approach, we were able to gain improved
average run time for classifier evaluation for a non-complete tree, while also
having strong mitigations against side-channel attacks.

</details>

### [23] [Risk Assessment and Threat Modeling for safe autonomous driving technology](https://arxiv.org/abs/2505.02231)
*Ian Alexis Wong Paz,Anuvinda Balan,Sebastian Campos,Ehud Orenstain,Sudip Dhakal*

Main category: cs.CR

TLDR: 研究探讨自动驾驶车辆技术的漏洞，提出基于OWASP Threat Dragon和STRIDE框架的威胁模型，并进行系统性风险评估。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆依赖互联系统和外部通信接口，易受网络安全威胁，需全面分析其脆弱性。

Method: 使用OWASP Threat Dragon和STRIDE框架开发威胁模型，分类威胁类型，并对各组件进行风险评估。

Result: 模型将威胁分为欺骗、篡改、抵赖、信息泄露、拒绝服务和权限提升，评估了感知模块、规划系统等组件的漏洞。

Conclusion: 研究为自动驾驶车辆的安全提供了系统性威胁分析和风险评估方法。

Abstract: This research paper delves into the field of autonomous vehicle technology,
examining the vulnerabilities inherent in each component of these
transformative vehicles. Autonomous vehicles (AVs) are revolutionizing
transportation by seamlessly integrating advanced functionalities such as
sensing, perception, planning, decision-making, and control. However, their
reliance on interconnected systems and external communication interfaces
renders them susceptible to cybersecurity threats.
  This research endeavors to develop a comprehensive threat model for AV
systems, employing OWASP Threat Dragon and the STRIDE framework. This model
categorizes threats into Spoofing, Tampering, Repudiation, Information
Disclosure, Denial of Service (DoS), and Elevation of Privilege.
  A systematic risk assessment is conducted to evaluate vulnerabilities across
various AV components, including perception modules, planning systems, control
units, and communication interfaces.

</details>

### [24] [Performance Analysis and Deployment Considerations of Post-Quantum Cryptography for Consumer Electronics](https://arxiv.org/abs/2505.02239)
*Daniel Commey,Benjamin Appiah,Griffith S. Klogo,Winful Bagyl-Bac,James D. Gadze*

Main category: cs.CR

TLDR: 该论文分析了量子计算对消费电子产品安全的威胁，评估了多种量子抗性加密方案（PQC）的性能，并提出了针对不同CE场景的部署建议。


<details>
  <summary>Details</summary>
Motivation: 量子计算威胁消费电子产品的安全基础，需要为资源受限设备提供量子抗性加密的性能量化分析。

Method: 对领先的PQC密钥封装机制（KEMs）和数字签名（NIST标准/候选方案）进行跨平台性能分析，包括执行时间、通信成本和内存占用。

Result: 基于格密码的方案（如ML-KEM和ML-DSA）在计算效率和通信/存储开销之间取得良好平衡，适合多数CE应用；而基于代码的Classic McEliece和基于哈希的SPHINCS+分别存在密钥大小和签名大小的挑战。

Conclusion: 论文提供了针对不同CE场景的具体部署建议，为制造商在PQC过渡中提供指导。

Abstract: Quantum computing threatens the security foundations of consumer electronics
(CE). Preparing the diverse CE ecosystem, particularly resource-constrained
devices, for the post-quantum era requires quantitative understanding of
quantum-resistant cryptography (PQC) performance. This paper presents a
comprehensive cross-platform performance analysis of leading PQC Key
Encapsulation Mechanisms (KEMs) and digital signatures (NIST
standards/candidates) compared against classical RSA/ECC. We evaluated
execution time, communication costs (key/signature sizes), and memory footprint
indicators on high-performance (macOS/M4, Ubuntu/x86) and constrained platforms
(Raspberry Pi 4/ARM). Our quantitative results reveal lattice-based schemes,
notably NIST standards ML-KEM (Kyber) and ML-DSA (Dilithium), provide a strong
balance of computational efficiency and moderate communication/storage
overhead, making them highly suitable for many CE applications. In contrast,
code-based Classic McEliece imposes significant key size challenges, while
hash-based SPHINCS+ offers high security assurance but demands large signature
sizes impacting bandwidth and storage. Based on empirical data across platforms
and security levels, we provide specific deployment recommendations tailored to
different CE scenarios (e.g., wearables, smart home hubs, mobile devices),
offering guidance for manufacturers navigating the PQC transition.

</details>

### [25] [An End-to-End Model For Logits Based Large Language Models Watermarking](https://arxiv.org/abs/2505.02344)
*Kahim Wong,Jicheng Zhou,Jiantao Zhou,Yain-Whar Si*

Main category: cs.CR

TLDR: 本文提出了一种新颖的端到端对数扰动方法，用于水印LLM生成的文本，通过联合优化实现了质量与鲁棒性的更好平衡。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的兴起，AIGC的源头追踪和版权保护需求增加，现有水印方法在文本修改后性能显著下降且可能引入偏见，缺乏端到端优化。

Method: 引入端到端对数扰动方法，结合在线提示技术，利用动态LLM作为可微分替代，优化编码器和解码器。

Result: 在保持文本质量的同时，鲁棒性显著优于无失真方法，平均提升17.2%，在改写场景下提升37-39%。

Conclusion: 该方法在质量与鲁棒性间取得更好平衡，且易于推广到不同LLM。

Abstract: The rise of LLMs has increased concerns over source tracing and copyright
protection for AIGC, highlighting the need for advanced detection technologies.
Passive detection methods usually face high false positives, while active
watermarking techniques using logits or sampling manipulation offer more
effective protection. Existing LLM watermarking methods, though effective on
unaltered content, suffer significant performance drops when the text is
modified and could introduce biases that degrade LLM performance in downstream
tasks. These methods fail to achieve an optimal tradeoff between text quality
and robustness, particularly due to the lack of end-to-end optimization of the
encoder and decoder. In this paper, we introduce a novel end-to-end logits
perturbation method for watermarking LLM-generated text. By jointly
optimization, our approach achieves a better balance between quality and
robustness. To address non-differentiable operations in the end-to-end training
pipeline, we introduce an online prompting technique that leverages the
on-the-fly LLM as a differentiable surrogate. Our method achieves superior
robustness, outperforming distortion-free methods by 37-39% under paraphrasing
and 17.2% on average, while maintaining text quality on par with these
distortion-free methods in terms of text perplexity and downstream tasks. Our
method can be easily generalized to different LLMs.

</details>

### [26] [Advancing Email Spam Detection: Leveraging Zero-Shot Learning and Large Language Models](https://arxiv.org/abs/2505.02362)
*Ghazaleh SHirvani,Saeid Ghasemshirazi*

Main category: cs.CR

TLDR: 该论文提出了一种结合FLAN-T5和BERT的零样本学习方法，用于电子邮件垃圾邮件检测，旨在解决传统方法在动态适应性和数据依赖方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习和深度学习方法在垃圾邮件检测中面临动态适应性差、数据依赖性强等问题，需要创新方法减少对标注数据和频繁重新训练的依赖。

Method: 使用BERT预处理和提取电子邮件内容的关键信息，结合FLAN-T5在零样本框架下进行分类。

Result: 该方法能够在不依赖大量标注数据或频繁重新训练的情况下，实现对未知垃圾邮件模式的鲁棒检测。

Conclusion: 零样本学习和NLP技术的结合为垃圾邮件检测提供了高效、可扩展的解决方案，能够应对动态和复杂的垃圾邮件检测任务。

Abstract: Email spam detection is a critical task in modern communication systems,
essential for maintaining productivity, security, and user experience.
Traditional machine learning and deep learning approaches, while effective in
static settings, face significant limitations in adapting to evolving spam
tactics, addressing class imbalance, and managing data scarcity. These
challenges necessitate innovative approaches that reduce dependency on
extensive labeled datasets and frequent retraining. This study investigates the
effectiveness of Zero-Shot Learning using FLAN-T5, combined with advanced
Natural Language Processing (NLP) techniques such as BERT for email spam
detection. By employing BERT to preprocess and extract critical information
from email content, and FLAN-T5 to classify emails in a Zero-Shot framework,
the proposed approach aims to address the limitations of traditional spam
detection systems. The integration of FLAN-T5 and BERT enables robust spam
detection without relying on extensive labeled datasets or frequent retraining,
making it highly adaptable to unseen spam patterns and adversarial
environments. This research highlights the potential of leveraging zero-shot
learning and NLPs for scalable and efficient spam detection, providing insights
into their capability to address the dynamic and challenging nature of spam
detection tasks.

</details>

### [27] [Moneros Decentralized P2P Exchanges: Functionality, Adoption, and Privacy Risks](https://arxiv.org/abs/2505.02392)
*Yannik Kopyciok,Friedhelm Victor,Stefan Schmid*

Main category: cs.CR

TLDR: 本文系统化分析了Monero生态中的去中心化P2P交易所，揭示了Haveno交易所的隐私漏洞，并探讨了其功能、架构及潜在弱点。


<details>
  <summary>Details</summary>
Motivation: 尽管隐私币如Monero受欢迎，但去中心化交易所（DEXs）的功能、交易活动和隐私声明缺乏学术研究。

Method: 研究了多个新兴DEX平台，分析其流行度、功能、架构选择和潜在弱点，并发现Haveno交易所的隐私漏洞。

Result: 发现Haveno交易所的某些交易可被检测，导致Monero和比特币区块链间的交易被关联。

Conclusion: 研究结果可为研究社区提供更安全设计的讨论基础，并为监管者提供参考。

Abstract: Privacy-focused cryptocurrencies like Monero remain popular, despite
increasing regulatory scrutiny that has led to their delisting from major
centralized exchanges. The latter also explains the recent popularity of
decentralized exchanges (DEXs) with no centralized ownership structures. These
platforms typically leverage peer-to-peer (P2P) networks, promising secure and
anonymous asset trading. However, questions of liability remain, and the
academic literature lacks comprehensive insights into the functionality,
trading activity, and privacy claims of these P2P platforms. In this paper, we
provide an early systematization of the current landscape of decentralized
peer-to-peer exchanges within the Monero ecosystem. We examine several recently
developed DEX platforms, analyzing their popularity, functionality,
architectural choices, and potential weaknesses. We further identify and report
on a privacy vulnerability in the recently popularized Haveno exchange,
demonstrating that certain Haveno trades could be detected, allowing
transactions to be linked across the Monero and Bitcoin blockchains. We hope
that our findings can nourish the discussion in the research community about
more secure designs, and provide insights for regulators.

</details>

### [28] [Encrypted Federated Search Using Homomorphic Encryption](https://arxiv.org/abs/2505.02409)
*Om Rathod,Aastha Baid,Aswani Kumar Cherukuri*

Main category: cs.CR

TLDR: 提出了一种基于同态加密的隐私保护联邦搜索系统，支持执法机构在不解密数据的情况下查询加密的犯罪数据库。


<details>
  <summary>Details</summary>
Motivation: 解决跨辖区犯罪活动中信息共享的隐私、所有权和合规性问题。

Method: 利用CKKS和BFV同态加密方案，结合TenSEAL框架，实现分布式数据库的加密查询。

Result: 系统在保护数据隐私的同时，满足国家安全和合规性要求，实验验证了其有效性。

Conclusion: 该系统为跨机构信息共享提供了安全、可扩展的解决方案。

Abstract: The sharing of information between agencies is effective in dealing with
cross-jurisdictional criminal activities; however, such sharing is often
restricted due to concerns about data privacy, ownership, and compliance.
Towards this end, this work has introduced a privacy-preserving federated
search system that allows law enforcement agencies to conduct queries on
encrypted criminal databases by utilizing Homomorphic Encryption (HE). The key
innovation here is the ability to execute encrypted queries across distributed
databases, without the decryption of the data, thus preserving end-to-end
confidentiality. In essence, this approach meets stringent privacy requirements
in the interests of national security and regulatory compliance. The system
incorporates the CKKS and BFV scheme embedded within TenSEAL, with each agency
holding its key pair in a centralized key management table. In this federated
search, encrypted queries are computed on the server side, and only authorized
clients can decrypt the computed results. The matching of agencies is flexible
for working in real-time while at the same time being secure and scalable while
preserving control over data and the integrity of the process. Experimental
results demonstrate the model. This paper also provide the implementation code
and other details.

</details>

### [29] [Targeted Fuzzing for Unsafe Rust Code: Leveraging Selective Instrumentation](https://arxiv.org/abs/2505.02464)
*David Paaßen,Jens-Rene Giesen,Lucas Davi*

Main category: cs.CR

TLDR: 本文提出了一种利用选择性代码覆盖率反馈来集中模糊测试不安全Rust代码的方法，显著提高了效率且无需额外计算资源。


<details>
  <summary>Details</summary>
Motivation: Rust虽然提供强大的安全性保证，但不安全代码仍可能导致安全问题。现有模糊测试方法未针对不安全代码优化，因此需要更高效的解决方案。

Method: 通过扩展Rust编译器工具链，自动检测不安全与安全代码组件，指导模糊测试聚焦于不安全部分。

Result: 该方法能显著增加触发不安全代码位置的输入生成，并在更短时间内检测潜在漏洞，且不影响性能。

Conclusion: 该方法兼容现有模糊测试实现，无需复杂手动操作，有效提升不安全代码的漏洞检测效率。

Abstract: Rust is a promising programming language that focuses on concurrency,
usability, and security. It is used in production code by major industry
players and got recommended by government bodies. Rust provides strong security
guarantees achieved by design utilizing the concepts of ownership and
borrowing. However, Rust allows programmers to write unsafe code which is not
subject to the strict Rust security policy. Empirical studies show that
security issues in practice always involve code written in unsafe Rust.
  In this paper, we present the first approach that utilizes selective code
coverage feedback to focus the fuzzing efforts on unsafe Rust code. Our
approach significantly improves the efficiency when fuzzing Rust programs and
does not require additional computational resources while fuzz testing the
target. To quantify the impact of partial code instrumentation, we implement
our approach by extending the capabilities of the Rust compiler toolchain. We
present an automated approach to detect unsafe and safe code components to
decide which parts of the program a fuzzer should focus on when running a
fuzzing campaign to find vulnerabilities in Rust programs. Our approach is
fully compatible with existing fuzzing implementations and does not require
complex manual work, thus retaining the existing high usability standard.
Focusing on unsafe code, our implementation allows us to generate inputs that
trigger more unsafe code locations with statistical significance and therefore
is able to detect potential vulnerabilities in a shorter time span while
imposing no performance overhead during fuzzing itself.

</details>

### [30] [Dynamic Graph-based Fingerprinting of In-browser Cryptomining](https://arxiv.org/abs/2505.02493)
*Tanapoom Sermchaiwong,Jiasi Shen*

Main category: cs.CR

TLDR: 论文提出了一种基于指令级数据流图的加密挖矿行为检测方法，通过简化图和子图相似性度量，提高了检测准确性和抗混淆能力。


<details>
  <summary>Details</summary>
Motivation: 加密货币的去中心化和无监管特性使其成为非法活动的工具，如浏览器内加密劫持。现有检测方法易受混淆技术影响，且样本多样性不足。

Method: 使用数据流图作为计算指纹，提出图简化算法和子图相似性度量（n-fragment inclusion score），构建检测框架PoT。

Result: PoT在标准混淆下表现出高检测准确率，优于现有方法，并适用于服务器等平台。

Conclusion: 数据流图方法能有效检测加密挖矿行为，具有通用性和抗混淆能力。

Abstract: The decentralized and unregulated nature of cryptocurrencies, combined with
their monetary value, has made them a vehicle for various illicit activities.
One such activity is cryptojacking, an attack that uses stolen computing
resources to mine cryptocurrencies without consent for profit. In-browser
cryptojacking malware exploits high-performance web technologies like
WebAssembly to mine cryptocurrencies directly within the browser without file
downloads. Although existing methods for cryptomining detection report high
accuracy and low overhead, they are often susceptible to various forms of
obfuscation, and due to the limited variety of cryptomining scripts in the
wild, standard code obfuscation methods present a natural and appealing
solution to avoid detection. To address these limitations, we propose using
instruction-level data-flow graphs to detect cryptomining behavior. Data-flow
graphs offer detailed structural insights into a program's computations, making
them suitable for characterizing proof-of-work algorithms, but they can be
difficult to analyze due to their large size and susceptibility to noise and
fragmentation under obfuscation. We present two techniques to simplify and
compare data-flow graphs: (1) a graph simplification algorithm to reduce the
computational burden of processing large and granular data-flow graphs while
preserving local substructures; and (2) a subgraph similarity measure, the
n-fragment inclusion score, based on fragment inclusion that is robust against
noise and obfuscation. Using data-flow graphs as computation fingerprints, our
detection framework PoT (Proof-of-Theft) was able to achieve high detection
accuracy against standard obfuscations, outperforming existing detection
methods. Moreover, PoT uses generic data-flow properties that can be applied to
other platforms more susceptible to cryptojacking such as servers and data
centers.

</details>

### [31] [An Efficient Hybrid Key Exchange Mechanism](https://arxiv.org/abs/2505.02499)
*Benjamin D. Kim,Vipindev Adat Vasudevan,Alejandro Cohen,Rafael G. L. D'Oliveira,Thomas Stahlbuhk,Muriel Médard*

Main category: cs.CR

TLDR: CHOKE是一种新型的基于代码的混合密钥封装机制（KEM），用于高效安全地同时传输多个会话密钥。


<details>
  <summary>Details</summary>
Motivation: 传统串行或基于组合器的混合方案在计算和通信成本上较高，CHOKE旨在解决这一问题。

Method: 通过将n个独立会话密钥编码为安全线性码，并用单独的KEM封装每个编码符号，实现计算上的个体安全性。

Result: CHOKE将计算和通信成本降低了n倍，且其通信成本在要求每个KEM至少使用一次时是最优的。

Conclusion: CHOKE是一种高效且安全的方案，适用于多密钥传输场景。

Abstract: We present \textsc{CHOKE}, a novel code-based hybrid key-encapsulation
mechanism (KEM) designed to securely and efficiently transmit multiple session
keys simultaneously. By encoding $n$ independent session keys with an
individually secure linear code and encapsulating each resulting coded symbol
using a separate KEM, \textsc{CHOKE} achieves computational individual security
-- each key remains secure as long as at least one underlying KEM remains
unbroken. Compared to traditional serial or combiner-based hybrid schemes,
\textsc{CHOKE} reduces computational and communication costs by an $n$-fold
factor. Furthermore, we show that the communication cost of our construction is
optimal under the requirement that each KEM must be used at least once.

</details>

### [32] [Unveiling the Landscape of LLM Deployment in the Wild: An Empirical Study](https://arxiv.org/abs/2505.02502)
*Xinyi Hou,Jiahao Han,Yanjie Zhao,Haoyu Wang*

Main category: cs.CR

TLDR: 研究发现，公开部署的大型语言模型（LLM）普遍存在安全漏洞和配置问题，亟需加强默认安全性和部署实践。


<details>
  <summary>Details</summary>
Motivation: 揭示公开部署的LLM现状，关注其安全风险及系统性问题。

Method: 通过互联网范围测量，识别了320,102个公开LLM服务，分析其配置、认证和地理分布。

Result: 发现广泛使用不安全协议、TLS配置差及未认证访问，存在模型泄露、系统泄漏等风险。

Conclusion: 需加强默认安全性和部署标准，以应对自托管LLM生态的安全挑战。

Abstract: Background: Large language models (LLMs) are increasingly deployed via
open-source and commercial frameworks, enabling individuals and organizations
to self-host advanced AI capabilities. However, insecure defaults and
misconfigurations often expose LLM services to the public Internet, posing
significant security and system engineering risks. Aims: This study aims to
unveil the current landscape of public-facing LLM deployments in the wild
through a large-scale empirical study, focusing on service prevalence, exposure
characteristics, systemic vulnerabilities, and associated risks. Method: We
conducted an Internet-wide measurement to identify public-facing LLM
deployments across 15 frameworks, discovering 320,102 services. We extracted
158 unique API endpoints, grouped into 12 functional categories based on
capabilities and security risks. We further analyzed configurations,
authentication practices, and geographic distributions, revealing deployment
trends and systemic issues in real-world LLM system engineering. Results: Our
study shows that public LLM deployments are rapidly growing but often insecure.
Among all endpoints, we observe widespread use of insecure protocols, poor TLS
configurations, and unauthenticated access to critical operations. Security
risks, including model disclosure, system leakage, and unauthorized access, are
pervasive, highlighting the need for secure-by-default frameworks and stronger
deployment practices. Conclusions: Public-facing LLM deployments suffer from
widespread security and configuration flaws, exposing services to misuse, model
theft, resource hijacking, and remote exploitation. Strengthening default
security, deployment practices, and operational standards is critical for the
growing self-hosted LLM ecosystem.

</details>

### [33] [Attestable builds: compiling verifiable binaries on untrusted systems using trusted execution environments](https://arxiv.org/abs/2505.02521)
*Daniel Hugenroth,Mario Lins,René Mayrhofer,Alastair Beresford*

Main category: cs.CR

TLDR: 提出了一种名为‘可验证构建’的新方法，通过可信执行环境（TEE）和沙盒构建容器，确保二进制文件与源代码的一致性，解决了传统构建流程中信任缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 传统构建流程中，源代码与最终二进制文件之间的信任关系不透明，难以验证。可验证构建旨在提供一种快速、无需大规模修改的解决方案。

Method: 利用TEE和沙盒构建容器，确保构建过程的透明性和可验证性，同时对现有项目仅需最小改动。

Result: 在开源软件库上测试，构建时间仅增加14%，启动延迟42秒，且无需修改复杂项目（如LLVM Clang）的源代码和构建脚本。

Conclusion: 可验证构建提供了一种高效、安全的解决方案，适用于复杂项目，并已验证其安全性。

Abstract: In this paper we present attestable builds, a new paradigm to provide strong
source-to-binary correspondence in software artifacts. We tackle the challenge
of opaque build pipelines that disconnect the trust between source code, which
can be understood and audited, and the final binary artifact, which is
difficult to inspect. Our system uses modern trusted execution environments
(TEEs) and sandboxed build containers to provide strong guarantees that a given
artifact was correctly built from a specific source code snapshot. As such it
complements existing approaches like reproducible builds which typically
require time-intensive modifications to existing build configurations and
dependencies, and require independent parties to continuously build and verify
artifacts. In comparison, an attestable build requires only minimal changes to
an existing project, and offers nearly instantaneous verification of the
correspondence between a given binary and the source code and build pipeline
used to construct it. We evaluate it by building open-source software libraries
- focusing on projects which are important to the trust chain and those which
have proven difficult to be built deterministically. Overall, the overhead (42
seconds start-up latency and 14% increase in build duration) is small in
comparison to the overall build time. Importantly, our prototype builds even
complex projects such as LLVM Clang without requiring any modifications to
their source code and build scripts. Finally, we formally model and verify the
attestable build design to demonstrate its security against well-resourced
adversaries.

</details>

### [34] [SoK: Stealing Cars Since Remote Keyless Entry Introduction and How to Defend From It](https://arxiv.org/abs/2505.02713)
*Tommaso Bianchi,Alessandro Brighente,Mauro Conti,Edoardo Pavan*

Main category: cs.CR

TLDR: 本文对远程无钥匙进入（RKE）和被动无钥匙进入与启动（PKES）系统进行了知识系统化（SOK），分析了其历史、现状、攻击与防御机制，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于RKE系统持续成为盗窃目标，且工业界缺乏显著进展来保护这些系统，本文旨在填补研究与行业之间的鸿沟，提供全面的知识系统化分析。

Method: 通过系统化知识（SOK）方法，梳理RKE和PKES系统的历史、技术、攻击与防御机制，并回答特定研究问题。

Result: 揭示了RKE系统的弱点，包括传统攻击（如Relay和RollJam）和新型攻击（如API漏洞），并提供了未来研究方向。

Conclusion: 本文为安全研究人员和公司提供了解决RKE系统安全问题的方向，强调了填补研究与行业差距的重要性。

Abstract: Remote Keyless Entry (RKE) systems have been the target of thieves since
their introduction in automotive industry. Robberies targeting vehicles and
their remote entry systems are booming again without a significant advancement
from the industrial sector being able to protect against them. Researchers and
attackers continuously play cat and mouse to implement new methodologies to
exploit weaknesses and defense strategies for RKEs. In this fragment, different
attacks and defenses have been discussed in research and industry without
proper bridging. In this paper, we provide a Systematization Of Knowledge (SOK)
on RKE and Passive Keyless Entry and Start (PKES), focusing on their history
and current situation, ranging from legacy systems to modern web-based ones. We
provide insight into vehicle manufacturers' technologies and attacks and
defense mechanisms involving them. To the best of our knowledge, this is the
first comprehensive SOK on RKE systems, and we address specific research
questions to understand the evolution and security status of such systems. By
identifying the weaknesses RKE still faces, we provide future directions for
security researchers and companies to find viable solutions to address old
attacks, such as Relay and RollJam, as well as new ones, like API
vulnerabilities.

</details>

### [35] [Acoustic Side-Channel Attacks on a Computer Mouse](https://arxiv.org/abs/2505.02725)
*Mauro Conti,Marin Duroyon,Gabriele Orazi,Gene Tsudik*

Main category: cs.CR

TLDR: 论文探讨了通过计算机鼠标使用产生的声学信号进行侧信道攻击的可行性，展示了高精度的分类能力。


<details>
  <summary>Details</summary>
Motivation: 研究鼠标声学信号的潜在安全风险，填补现有文献中对非键盘外设攻击的空白。

Method: 通过受控环境和智能手机录音，结合机器学习技术分类鼠标动作。

Result: 在受控环境中分类四种鼠标动作准确率达97%，六名参与者实验中分类十二种动作准确率达94%，检测关闭全屏窗口动作准确率达91%。

Conclusion: 鼠标声学信号可被用于高精度侧信道攻击，需引起安全关注。

Abstract: Acoustic Side-Channel Attacks (ASCAs) extract sensitive information by using
audio emitted from a computing devices and their peripherals. Attacks targeting
keyboards are popular and have been explored in the literature. However,
similar attacks targeting other human interface peripherals, such as computer
mice, are under-explored. To this end, this paper considers security leakage
via acoustic signals emanating from normal mouse usage. We first confirm
feasibility of such attacks by showing a proof-of-concept attack that
classifies four mouse movements with 97% accuracy in a controlled environment.
We then evolve the attack towards discerning twelve unique mouse movements
using a smartphone to record the experiment. Using Machine Learning (ML)
techniques, the model is trained on an experiment with six participants to be
generalizable and discern among twelve movements with 94% accuracy. In
addition, we experiment with an attack that detects a user action of closing a
full-screen window on a laptop. Achieving an accuracy of 91%, this experiment
highlights exploiting audio leakage from computer mouse movements in a
realistic scenario.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [Perturbation Analysis of Singular Values in Concatenated Matrices](https://arxiv.org/abs/2505.01427)
*Maksym Shamrai*

Main category: cs.LG

TLDR: 论文研究了矩阵拼接后奇异值谱与子矩阵谱的关系，提出了扰动框架并建立了稳定性界限。


<details>
  <summary>Details</summary>
Motivation: 探讨矩阵拼接后奇异值谱的稳定性及其对数据压缩和聚类的影响。

Method: 开发扰动框架，扩展经典结果（如Weyl不等式），分析子矩阵小扰动下奇异值的稳定性。

Result: 若拼接矩阵范数接近，拼接后矩阵的主导奇异值稳定，支持精度与压缩的权衡。

Conclusion: 为矩阵聚类和压缩策略提供了理论基础，适用于数值线性代数、信号处理和数据建模。

Abstract: Concatenating matrices is a common technique for uncovering shared structures
in data through singular value decomposition (SVD) and low-rank approximations.
However, a fundamental question arises: how does the singular value spectrum of
the concatenated matrix relate to the spectra of its individual components? In
this work, we develop a perturbation framework that extends classical results
such as Weyl's inequality to concatenated matrices. We establish analytical
bounds that quantify the stability of singular values under small perturbations
in the submatrices. Our results show that if the matrices being concatenated
are close in norm, the dominant singular values of the concatenated matrix
remain stable, enabling controlled trade-offs between accuracy and compression.
These insights provide a theoretical foundation for improved matrix clustering
and compression strategies, with applications in numerical linear algebra,
signal processing, and data-driven modeling.

</details>

### [37] [Enhancing IoT-Botnet Detection using Variational Auto-encoder and Cost-Sensitive Learning: A Deep Learning Approach for Imbalanced Datasets](https://arxiv.org/abs/2505.01437)
*Hassan Wasswa,Timothy Lynar,Hussein Abbass*

Main category: cs.LG

TLDR: 该研究利用变分自编码器（VAE）和成本敏感学习开发轻量级模型，用于检测IoT僵尸网络，重点关注少数类攻击流量的检测。


<details>
  <summary>Details</summary>
Motivation: IoT设备成为恶意攻击的薄弱环节，尤其是僵尸网络攻击，传统机器学习模型容易忽略少数类攻击流量。

Method: 结合VAE和成本敏感学习，评估了标准前馈深度神经网络（DNN）和双向LSTM（BLSTM）在高度不平衡数据集上的性能。

Result: 两种模型在准确性、精确率、召回率和F1分数上均表现优异，适用于多类流量检测。

Conclusion: 提出的方法能有效提升少数类攻击流量的检测能力，适用于IoT僵尸网络检测。

Abstract: The Internet of Things (IoT) technology has rapidly gained popularity with
applications widespread across a variety of industries. However, IoT devices
have been recently serving as a porous layer for many malicious attacks to both
personal and enterprise information systems with the most famous attacks being
botnet-related attacks. The work in this study leveraged Variational
Auto-encoder (VAE) and cost-sensitive learning to develop lightweight, yet
effective, models for IoT-botnet detection. The aim is to enhance the detection
of minority class attack traffic instances which are often missed by machine
learning models. The proposed approach is evaluated on a multi-class problem
setting for the detection of traffic categories on highly imbalanced datasets.
The performance of two deep learning models including the standard feed forward
deep neural network (DNN), and Bidirectional-LSTM (BLSTM) was evaluated and
both recorded commendable results in terms of accuracy, precision, recall and
F1-score for all traffic classes.

</details>

### [38] [Global Stress Generation and Spatiotemporal Super-Resolution Physics-Informed Operator under Dynamic Loading for Two-Phase Random Materials](https://arxiv.org/abs/2505.01438)
*Tengfei Xing,Xiaodan Ren,Jie Li*

Main category: cs.LG

TLDR: 提出了一种用于两相随机材料动态加载下全局应力生成和时空超分辨率的框架，结合扩散模型和物理信息网络。


<details>
  <summary>Details</summary>
Motivation: 动态加载下两相随机材料的应力演化复杂，现有方法在时空分辨率和捕捉应力集中区域方面存在挑战。

Method: 采用STS-diffusion生成全局应力数据，结合STU-net；开发ST-SRPINN进行时空超分辨率，基于物理约束的无监督学习。

Result: STS-diffusion和ST-SRPINN能高精度生成和提升应力场时空分辨率，后者仅需低分辨率数据即可训练。

Conclusion: 框架有效解决了动态加载下两相随机材料的应力场高分辨率生成问题，具有工程应用潜力。

Abstract: Material stress analysis is a critical aspect of material design and
performance optimization. Under dynamic loading, the global stress evolution in
materials exhibits complex spatiotemporal characteristics, especially in
two-phase random materials (TRMs). Such kind of material failure is often
associated with stress concentration, and the phase boundaries are key
locations where stress concentration occurs. In practical engineering
applications, the spatiotemporal resolution of acquired microstructural data
and its dynamic stress evolution is often limited. This poses challenges for
deep learning methods in generating high-resolution spatiotemporal stress
fields, particularly for accurately capturing stress concentration regions. In
this study, we propose a framework for global stress generation and
spatiotemporal super-resolution in TRMs under dynamic loading. First, we
introduce a diffusion model-based approach, named as Spatiotemporal Stress
Diffusion (STS-diffusion), for generating global spatiotemporal stress data.
This framework incorporates Space-Time U-Net (STU-net), and we systematically
investigate the impact of different attention positions on model accuracy.
Next, we develop a physics-informed network for spatiotemporal
super-resolution, termed as Spatiotemporal Super-Resolution Physics-Informed
Operator (ST-SRPINN). The proposed ST-SRPINN is an unsupervised learning
method. The influence of data-driven and physics-informed loss function weights
on model accuracy is explored in detail. Benefiting from physics-based
constraints, ST-SRPINN requires only low-resolution stress field data during
training and can upscale the spatiotemporal resolution of stress fields to
arbitrary magnifications.

</details>

### [39] [Interactive Double Deep Q-network: Integrating Human Interventions and Evaluative Predictions in Reinforcement Learning of Autonomous Driving](https://arxiv.org/abs/2505.01440)
*Alkis Sygkounas,Ioannis Athanasiadis,Andreas Persson,Michael Felsberg,Amy Loutfi*

Main category: cs.LG

TLDR: iDDQN是一种结合人类专家知识与强化学习的方法，通过修改Q值更新方程，显著提升了自动驾驶等场景中的模型性能。


<details>
  <summary>Details</summary>
Motivation: 在需要高精度和安全的领域（如自动驾驶）中，将人类专业知识与机器学习结合至关重要。

Method: 提出iDDQN方法，通过修改Q值更新方程整合人类与智能体动作，并设计离线评估框架模拟无干预轨迹。

Result: 在模拟自动驾驶场景中，iDDQN在性能与适应性上优于BC、HG-DAgger、DQfD和传统DRL方法。

Conclusion: iDDQN通过人类干预有效提升了强化学习的性能，展示了人机协作的潜力。

Abstract: Integrating human expertise with machine learning is crucial for applications
demanding high accuracy and safety, such as autonomous driving. This study
introduces Interactive Double Deep Q-network (iDDQN), a Human-in-the-Loop
(HITL) approach that enhances Reinforcement Learning (RL) by merging human
insights directly into the RL training process, improving model performance.
Our proposed iDDQN method modifies the Q-value update equation to integrate
human and agent actions, establishing a collaborative approach for policy
development. Additionally, we present an offline evaluative framework that
simulates the agent's trajectory as if no human intervention had occurred, to
assess the effectiveness of human interventions. Empirical results in simulated
autonomous driving scenarios demonstrate that iDDQN outperforms established
approaches, including Behavioral Cloning (BC), HG-DAgger, Deep Q-Learning from
Demonstrations (DQfD), and vanilla DRL in leveraging human expertise for
improving performance and adaptability.

</details>

### [40] [Explainable AI for Correct Root Cause Analysis of Product Quality in Injection Moulding](https://arxiv.org/abs/2505.01445)
*Muhammad Muaz,Sameed Sajid,Tobias Schulze,Chang Liu,Nils Klasen,Benny Drescher*

Main category: cs.LG

TLDR: 论文探讨了注塑过程中产品质量预测的机器学习模型，比较了模型无关的可解释AI方法，并证明更好的特征归因能正确识别问题原因。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型多为黑箱，缺乏直接解释性，限制了其在质量控制中的应用。

Method: 使用模型无关的可解释AI方法，比较不同解释方法对特征影响分析的差异，并在随机森林和多层感知机上验证。

Result: 不同解释方法导致不同的特征影响分析，更好的特征归因能正确识别问题原因。

Conclusion: 模型无关的解释方法能提供更准确的原因分析和可操作见解，适用于注塑过程质量控制。

Abstract: If a product deviates from its desired properties in the injection moulding
process, its root cause analysis can be aided by models that relate the input
machine settings with the output quality characteristics. The machine learning
models tested in the quality prediction are mostly black boxes; therefore, no
direct explanation of their prognosis is given, which restricts their
applicability in the quality control. The previously attempted explainability
methods are either restricted to tree-based algorithms only or do not emphasize
on the fact that some explainability methods can lead to wrong root cause
identification of a product's deviation from its desired properties. This study
first shows that the interactions among the multiple input machine settings do
exist in real experimental data collected as per a central composite design.
Then, the model-agnostic explainable AI methods are compared for the first time
to show that different explainability methods indeed lead to different feature
impact analysis in injection moulding. Moreover, it is shown that the better
feature attribution translates to the correct cause identification and
actionable insights for the injection moulding process. Being model agnostic,
explanations on both random forest and multilayer perceptron are performed for
the cause analysis, as both models have the mean absolute percentage error of
less than 0.05% on the experimental dataset.

</details>

### [41] [OpenAVS: Training-Free Open-Vocabulary Audio Visual Segmentation with Foundational Models](https://arxiv.org/abs/2505.01448)
*Shengkai Chen,Yifang Yin,Jinming Cao,Shili Xiang,Zhenguang Liu,Roger Zimmermann*

Main category: cs.LG

TLDR: OpenAVS是一种基于语言的免训练方法，通过文本代理实现开放词汇音频-视觉分割，利用多媒体基础模型直接推断掩码，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于封闭场景和直接音频-视觉对齐，难以泛化到新场景。OpenAVS旨在通过文本代理实现更灵活的开放词汇分割。

Method: 1) 音频到文本提示生成；2) LLM引导的提示翻译；3) 文本到视觉发声对象分割。还提出模型无关框架OpenAVS-ST，通过自训练整合监督模型。

Result: 在三个基准数据集上，OpenAVS显著优于无监督、零样本和少样本方法，mIoU和F-score分别提升约9.4%和10.9%。

Conclusion: OpenAVS通过简单灵活的架构和基础模型的能力，实现了高效的开放词汇音频-视觉分割，为下游任务提供了有效知识迁移。

Abstract: Audio-visual segmentation aims to separate sounding objects from videos by
predicting pixel-level masks based on audio signals. Existing methods primarily
concentrate on closed-set scenarios and direct audio-visual alignment and
fusion, which limits their capability to generalize to new, unseen situations.
In this paper, we propose OpenAVS, a novel training-free language-based
approach that, for the first time, effectively aligns audio and visual
modalities using text as a proxy for open-vocabulary Audio-Visual Segmentation
(AVS). Equipped with multimedia foundation models, OpenAVS directly infers
masks through 1) audio-to-text prompt generation, 2) LLM-guided prompt
translation, and 3) text-to-visual sounding object segmentation. The objective
of OpenAVS is to establish a simple yet flexible architecture that relies on
the most appropriate foundation models by fully leveraging their capabilities
to enable more effective knowledge transfer to the downstream AVS task.
Moreover, we present a model-agnostic framework OpenAVS-ST that enables the
integration of OpenAVS with any advanced supervised AVS model via pseudo-label
based self-training. This approach enhances performance by effectively
utilizing large-scale unlabeled data when available. Comprehensive experiments
on three benchmark datasets demonstrate the superior performance of OpenAVS. It
surpasses existing unsupervised, zero-shot, and few-shot AVS methods by a
significant margin, achieving absolute performance gains of approximately 9.4%
and 10.9% in mIoU and F-score, respectively, in challenging scenarios.

</details>

### [42] [COSMOS: Predictable and Cost-Effective Adaptation of LLMs](https://arxiv.org/abs/2505.01449)
*Jiayu Wang,Aws Albarghouthi,Frederic Sala*

Main category: cs.LG

TLDR: COSMOS框架通过轻量级代理模型和低样本扩展法则，高效预测LLM适应策略的性能和成本，显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 在资源受限情况下，如何准确预测LLM适应策略的性能和成本，避免昂贵的实验。

Method: 提出COSMOS框架，结合嵌入增强的轻量级代理模型和低样本扩展法则，预测微调性能和检索增强的上下文学习。

Result: 在八个基准测试中，COSMOS平均降低92.72%计算成本，最高达98.71%，同时保持高预测准确性。

Conclusion: 高效预测LLM适应策略可行，能显著减少计算开销而不影响性能。

Abstract: Large language models (LLMs) achieve remarkable performance across numerous
tasks by using a diverse array of adaptation strategies. However, optimally
selecting a model and adaptation strategy under resource constraints is
challenging and often requires extensive experimentation. We investigate
whether it is possible to accurately predict both performance and cost without
expensive trials. We formalize the strategy selection problem for LLMs and
introduce COSMOS, a unified prediction framework that efficiently estimates
adaptation outcomes at minimal cost. We instantiate and study the capability of
our framework via a pair of powerful predictors: embedding-augmented
lightweight proxy models to predict fine-tuning performance, and low-sample
scaling laws to forecast retrieval-augmented in-context learning. Extensive
evaluation across eight representative benchmarks demonstrates that COSMOS
achieves high prediction accuracy while reducing computational costs by 92.72%
on average, and up to 98.71% in resource-intensive scenarios. Our results show
that efficient prediction of adaptation outcomes is not only feasible but can
substantially reduce the computational overhead of LLM deployment while
maintaining performance standards.

</details>

### [43] [Towards Film-Making Production Dialogue, Narration, Monologue Adaptive Moving Dubbing Benchmarks](https://arxiv.org/abs/2505.01450)
*Chaoyi Wang,Junjie Zheng,Zihao Chen,Shiyu Xia,Chaofan Ding,Xiaohao Zhang,Xi Tao,Xiaoming He,Xinhan Di*

Main category: cs.LG

TLDR: 论文提出TA-Dubbing基准，用于全面评估电影配音模型的对话、旁白、独白和演员适应性，并开源以推动领域发展。


<details>
  <summary>Details</summary>
Motivation: 现有指标未能全面评估电影配音的复杂性，需实用系统提升配音质量和电影制作水平。

Method: 设计TA-Dubbing基准，涵盖多维度评估，包括电影理解和语音生成，并开源所有资源。

Result: TA-Dubbing提供全面、灵活的评估工具，支持先进配音模型和多模态大语言模型测试。

Conclusion: TA-Dubbing通过开源和持续更新，推动电影配音领域的进步。

Abstract: Movie dubbing has advanced significantly, yet assessing the real-world
effectiveness of these models remains challenging. A comprehensive evaluation
benchmark is crucial for two key reasons: 1) Existing metrics fail to fully
capture the complexities of dialogue, narration, monologue, and actor
adaptability in movie dubbing. 2) A practical evaluation system should offer
valuable insights to improve movie dubbing quality and advancement in film
production. To this end, we introduce Talking Adaptive Dubbing Benchmarks
(TA-Dubbing), designed to improve film production by adapting to dialogue,
narration, monologue, and actors in movie dubbing. TA-Dubbing offers several
key advantages: 1) Comprehensive Dimensions: TA-Dubbing covers a variety of
dimensions of movie dubbing, incorporating metric evaluations for both movie
understanding and speech generation. 2) Versatile Benchmarking: TA-Dubbing is
designed to evaluate state-of-the-art movie dubbing models and advanced
multi-modal large language models. 3) Full Open-Sourcing: We fully open-source
TA-Dubbing at https://github.com/woka- 0a/DeepDubber- V1 including all video
suits, evaluation methods, annotations. We also continuously integrate new
movie dubbing models into the TA-Dubbing leaderboard at
https://github.com/woka- 0a/DeepDubber-V1 to drive forward the field of movie
dubbing.

</details>

### [44] [Explainable Machine Learning for Cyberattack Identification from Traffic Flows](https://arxiv.org/abs/2505.01488)
*Yujing Zhou,Marc L. Jacquet,Robel Dawit,Skyler Fabre,Dev Sarawat,Faheem Khan,Madison Newell,Yongxin Liu,Dahai Liu,Hongyun Chen,Jian Wang,Huihui Wang*

Main category: cs.LG

TLDR: 论文提出了一种基于深度学习的异常检测系统，用于识别交通信号灯网络中的网络攻击，并通过可解释AI技术提高模型的可信度。


<details>
  <summary>Details</summary>
Motivation: 交通管理系统的自动化使其成为网络攻击的主要目标，传统防御方法难以适用，因此需要一种仅依赖交通流量数据的机器学习方法。

Method: 在半真实环境中模拟网络攻击，利用虚拟化交通网络分析破坏模式，开发基于深度学习的异常检测系统，并结合可解释AI技术。

Result: 研究发现最长停车时长和总拥堵距离是关键攻击指标，但也面临过渡数据不一致和低流量条件下隐蔽攻击难以检测的挑战。

Conclusion: 该研究提升了AI驱动的交通安全性，同时改进了检测准确性和智能交通系统的可信度。

Abstract: The increasing automation of traffic management systems has made them prime
targets for cyberattacks, disrupting urban mobility and public safety.
Traditional network-layer defenses are often inaccessible to transportation
agencies, necessitating a machine learning-based approach that relies solely on
traffic flow data. In this study, we simulate cyberattacks in a semi-realistic
environment, using a virtualized traffic network to analyze disruption
patterns. We develop a deep learning-based anomaly detection system,
demonstrating that Longest Stop Duration and Total Jam Distance are key
indicators of compromised signals. To enhance interpretability, we apply
Explainable AI (XAI) techniques, identifying critical decision factors and
diagnosing misclassification errors. Our analysis reveals two primary
challenges: transitional data inconsistencies, where mislabeled recovery-phase
traffic misleads the model, and model limitations, where stealth attacks in
low-traffic conditions evade detection. This work enhances AI-driven traffic
security, improving both detection accuracy and trustworthiness in smart
transportation systems.

</details>

### [45] [Machine Learning for Cyber-Attack Identification from Traffic Flows](https://arxiv.org/abs/2505.01489)
*Yujing Zhou,Marc L. Jacquet,Robel Dawit,Skyler Fabre,Dev Sarawat,Faheem Khan,Madison Newell,Yongxin Liu,Dahai Liu,Hongyun Chen,Jian Wang,Huihui Wang*

Main category: cs.LG

TLDR: 本文通过模拟网络攻击和检测策略，研究仅通过交通流模式识别攻击的可行性，最终模型准确率达85%。


<details>
  <summary>Details</summary>
Motivation: 研究是否仅通过分析交通流模式识别网络攻击，特别是在交通信号灯被恶意操控的情况下。

Method: 使用Raspberry Pi虚拟机和OPNSense防火墙，结合SUMO交通动态和Metasploit框架进行攻击模拟。

Result: 最佳模型在仅使用交通流统计数据时，检测入侵的准确率达85%，关键指标包括占用率、拥堵长度和停车时长。

Conclusion: 尽管数据不平衡和交通模式重叠带来挑战，但研究证明仅通过交通流模式可以有效检测网络攻击。

Abstract: This paper presents our simulation of cyber-attacks and detection strategies
on the traffic control system in Daytona Beach, FL. using Raspberry Pi virtual
machines and the OPNSense firewall, along with traffic dynamics from SUMO and
exploitation via the Metasploit framework. We try to answer the research
questions: are we able to identify cyber attacks by only analyzing traffic flow
patterns. In this research, the cyber attacks are focused particularly when
lights are randomly turned all green or red at busy intersections by
adversarial attackers. Despite challenges stemming from imbalanced data and
overlapping traffic patterns, our best model shows 85\% accuracy when detecting
intrusions purely using traffic flow statistics. Key indicators for successful
detection included occupancy, jam length, and halting durations.

</details>

### [46] [Subset Selection for Fine-Tuning: A Utility-Diversity Balanced Approach for Mathematical Domain Adaptation](https://arxiv.org/abs/2505.01523)
*Madhav Kotecha,Vijendra Kumar Vaishya,Smita Gautam,Suraj Racha*

Main category: cs.LG

TLDR: 提出一种通过预算子集选择方法高效微调大语言模型（LLM）的改进方法，结合效用和多样性指标选择最具信息量和代表性的训练样本。


<details>
  <summary>Details</summary>
Motivation: 在特定领域（如数学领域）高效微调LLM，减少计算成本和训练时间，同时保持接近全数据集的性能。

Method: 结合困惑度和Chain-of-Thought（CoT）损失的效用指标与多样性指标，选择最具挑战性和代表性的数据点。

Result: 在LLaMA-3 8B和Phi-3模型上验证，性能优于随机选择、多样性采样和现有子集选择技术。

Conclusion: 该方法能显著降低计算成本，同时保持竞争性性能，适用于特定领域的LLM微调。

Abstract: We propose a refined approach to efficiently fine-tune large language models
(LLMs) on specific domains like the mathematical domain by employing a budgeted
subset selection method. Our approach combines utility and diversity metrics to
select the most informative and representative training examples. The final
goal is to achieve near-full dataset performance with meticulously selected
data points from the entire dataset while significantly reducing computational
cost and training time and achieving competitive performance as the full
dataset. The utility metric incorporates both perplexity and Chain-of-Thought
(CoT) loss to identify challenging examples that contribute most to model
learning, while the diversity metric ensures broad coverage across mathematical
subdomains. We evaluate our method on LLaMA-3 8B and Phi-3 models, comparing
against several baseline approaches, including random selection,
diversity-based sampling, and existing state-of-the-art subset selection
techniques.

</details>

### [47] [Contextures: Representations from Contexts](https://arxiv.org/abs/2505.01557)
*Runtian Zhai,Kai Yang,Che-Ping Tsai,Burak Varici,Zico Kolter,Pradeep Ravikumar*

Main category: cs.LG

TLDR: 论文提出了“上下文理论”，用于系统化地表征基础模型学习的表示，证明多种学习方法均可从输入与上下文变量的关联中学习。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在实证上取得了成功，但缺乏对其学习表示的系统化表征。

Method: 提出上下文理论，证明许多流行方法旨在近似由上下文诱导的期望算子的前d个奇异函数。

Result: 证明学习上下文的表示在兼容任务上是最优的，并表明模型规模超过一定阈值后收益递减。

Conclusion: 上下文理论表明，模型规模并非唯一关键，改进需要更好的上下文设计；提出了一种评估上下文有用性的指标。

Abstract: Despite the empirical success of foundation models, we do not have a
systematic characterization of the representations that these models learn. In
this paper, we establish the contexture theory. It shows that a large class of
representation learning methods can be characterized as learning from the
association between the input and a context variable. Specifically, we show
that many popular methods aim to approximate the top-d singular functions of
the expectation operator induced by the context, in which case we say that the
representation learns the contexture. We demonstrate the generality of the
contexture theory by proving that representation learning within various
learning paradigms -- supervised, self-supervised, and manifold learning -- can
all be studied from such a perspective. We also prove that the representations
that learn the contexture are optimal on those tasks that are compatible with
the context. One important implication of the contexture theory is that once
the model is large enough to approximate the top singular functions, further
scaling up the model size yields diminishing returns. Therefore, scaling is not
all we need, and further improvement requires better contexts. To this end, we
study how to evaluate the usefulness of a context without knowing the
downstream tasks. We propose a metric and show by experiments that it
correlates well with the actual performance of the encoder on many real
datasets.

</details>

### [48] [Understanding and Exploiting Plasticity for Non-stationary Network Resource Adaptation](https://arxiv.org/abs/2505.01584)
*Zhiqiang He,Zhi Liu*

Main category: cs.LG

TLDR: 论文提出了一种名为ReSiN的方法，通过重置静默神经元来解决神经网络在动态网络环境中的可塑性损失问题，显著提升了自适应视频流系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案基于静态假设，无法有效应对动态网络条件，而数据驱动的强化学习方法虽具潜力，但神经网络的可塑性损失限制了其适应性。

Method: 提出了静默神经元理论，并基于此设计了ReSiN方法，通过策略性重置神经元来保持可塑性。

Result: 在自适应视频流系统中，ReSiN实现了比现有方案高168%的比特率和108%的体验质量（QoE），同时在静态环境中表现优异。

Conclusion: ReSiN通过解决可塑性损失问题，显著提升了神经网络在动态网络环境中的适应性和性能。

Abstract: Adapting to non-stationary network conditions presents significant challenges
for resource adaptation. However, current solutions primarily rely on
stationary assumptions. While data-driven reinforcement learning approaches
offer promising solutions for handling network dynamics, our systematic
investigation reveals a critical limitation: neural networks suffer from
plasticity loss, significantly impeding their ability to adapt to evolving
network conditions. Through theoretical analysis of neural propagation
mechanisms, we demonstrate that existing dormant neuron metrics inadequately
characterize neural plasticity loss. To address this limitation, we have
developed the Silent Neuron theory, which provides a more comprehensive
framework for understanding plasticity degradation. Based on these theoretical
insights, we propose the Reset Silent Neuron (ReSiN), which preserves neural
plasticity through strategic neuron resets guided by both forward and backward
propagation states. In our implementation of an adaptive video streaming
system, ReSiN has shown significant improvements over existing solutions,
achieving up to 168% higher bitrate and 108% better quality of experience (QoE)
while maintaining comparable smoothness. Furthermore, ReSiN consistently
outperforms in stationary environments, demonstrating its robust adaptability
across different network conditions.

</details>

### [49] [Machine Learning Fairness in House Price Prediction: A Case Study of America's Expanding Metropolises](https://arxiv.org/abs/2505.01591)
*Abdalwahab Almajed,Maryam Tabar,Peyman Najafirad*

Main category: cs.LG

TLDR: 本文研究了机器学习驱动的房价预测模型中的种族和民族偏见，并评估了不同偏见缓解方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 住房是社会基本需求，房价预测模型可能加剧社会不平等，因此需研究其公平性。

Method: 结合结构和社区属性开发多个ML模型，并评估其公平性及偏见缓解方法的效果。

Result: 发现模型对种族和民族存在偏见，且处理中的偏见缓解方法通常更有效。

Conclusion: 需重视ML模型的公平性，处理中方法在房价预测中更具优势。

Abstract: As a basic human need, housing plays a key role in enhancing health,
well-being, and educational outcome in society, and the housing market is a
major factor for promoting quality of life and ensuring social equity. To
improve the housing conditions, there has been extensive research on building
Machine Learning (ML)-driven house price prediction solutions to accurately
forecast the future conditions, and help inform actions and policies in the
field. In spite of their success in developing high-accuracy models, there is a
gap in our understanding of the extent to which various ML-driven house price
prediction approaches show ethnic and/or racial bias, which in turn is
essential for the responsible use of ML, and ensuring that the ML-driven
solutions do not exacerbate inequity. To fill this gap, this paper develops
several ML models from a combination of structural and neighborhood-level
attributes, and conducts comprehensive assessments on the fairness of ML models
under various definitions of privileged groups. As a result, it finds that the
ML-driven house price prediction models show various levels of bias towards
protected attributes (i.e., race and ethnicity in this study). Then, it
investigates the performance of different bias mitigation solutions, and the
experimental results show their various levels of effectiveness on different
ML-driven methods. However, in general, the in-processing bias mitigation
approach tends to be more effective than the pre-processing one in this problem
domain. Our code is available at https://github.com/wahab1412/housing_fairness.

</details>

### [50] [Don't be lazy: CompleteP enables compute-efficient deep transformers](https://arxiv.org/abs/2505.01618)
*Nolan Dey,Bin Claire Zhang,Lorenzo Noci,Mufan Li,Blake Bordelon,Shane Bergsma,Cengiz Pehlevan,Boris Hanin,Joel Hestness*

Main category: cs.LG

TLDR: 研究不同参数化方法对LLM训练计算效率的影响，提出CompleteP方法，实现深度超参数迁移和非惰性学习，提升计算效率12-34%。


<details>
  <summary>Details</summary>
Motivation: 现有参数化方法在模型规模变化时难以保持超参数（如学习率）的最优性，导致需重新调参或接受次优训练效果。

Method: 开发理论分析参数化方法，识别CompleteP参数化，实现超参数跨深度迁移和非惰性学习。

Result: CompleteP支持更广的模型宽深比，计算效率提升12-34%。

Conclusion: CompleteP为硬件和操作场景提供更优模型结构，显著提升训练效率。

Abstract: We study compute efficiency of LLM training when using different
parameterizations, i.e., rules for adjusting model and optimizer
hyperparameters (HPs) as model size changes. Some parameterizations fail to
transfer optimal base HPs (such as learning rate) across changes in model
depth, requiring practitioners to either re-tune these HPs as they scale up
(expensive), or accept sub-optimal training when re-tuning is prohibitive. Even
when they achieve HP transfer, we develop theory to show parameterizations may
still exist in the lazy learning regime where layers learn only features close
to their linearization, preventing effective use of depth and nonlinearity.
Finally, we identify and adopt the unique parameterization we call CompleteP
that achieves both depth-wise HP transfer and non-lazy learning in all layers.
CompleteP enables a wider range of model width/depth ratios to remain
compute-efficient, unlocking shapes better suited for different hardware
settings and operational contexts. Moreover, CompleteP enables 12-34\% compute
efficiency improvements over the prior state-of-the-art.

</details>

### [51] [Skill-based Safe Reinforcement Learning with Risk Planning](https://arxiv.org/abs/2505.01619)
*Hanping Zhang,Yuhong Guo*

Main category: cs.LG

TLDR: 提出了一种名为SSkP的新方法，通过利用离线演示数据提升安全强化学习的效果，包括技能风险预测和风险规划两个阶段。


<details>
  <summary>Details</summary>
Motivation: 安全强化学习（Safe RL）在现实环境中需要确保安全性，避免高成本或严重后果。

Method: 采用PU学习从离线数据中学习技能风险预测器，并基于此开发风险规划过程，以在线RL环境中高效学习风险规避策略。

Result: 在多个机器人仿真环境中实验，结果表明SSkP方法优于现有安全RL方法。

Conclusion: SSkP方法通过结合离线数据和在线学习，显著提升了安全强化学习的性能和安全性。

Abstract: Safe Reinforcement Learning (Safe RL) aims to ensure safety when an RL agent
conducts learning by interacting with real-world environments where improper
actions can induce high costs or lead to severe consequences. In this paper, we
propose a novel Safe Skill Planning (SSkP) approach to enhance effective safe
RL by exploiting auxiliary offline demonstration data. SSkP involves a
two-stage process. First, we employ PU learning to learn a skill risk predictor
from the offline demonstration data. Then, based on the learned skill risk
predictor, we develop a novel risk planning process to enhance online safe RL
and learn a risk-averse safe policy efficiently through interactions with the
online RL environment, while simultaneously adapting the skill risk predictor
to the environment. We conduct experiments in several benchmark robotic
simulation environments. The experimental results demonstrate that the proposed
approach consistently outperforms previous state-of-the-art safe RL methods.

</details>

### [52] [A Domain Adaptation of Large Language Models for Classifying Mechanical Assembly Components](https://arxiv.org/abs/2505.01627)
*Fatemeh Elhambakhsh,Daniele Grandi,Hyunwoong Ko*

Main category: cs.LG

TLDR: 论文提出了一种基于LLM的领域自适应框架，用于机械装配零件功能的自动分类，通过微调GPT-3.5 Turbo提升功能标注的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 功能建模在产品设计初期至关重要，但缺乏结构化功能数据影响了设计决策和行为模型的准确性，LLM为解决这一问题提供了可能。

Method: 采用基于GPT架构的LLM进行领域自适应（DA）微调，应用于机械装配零件功能的自动分类。

Result: 在OSDR和ABC数据集上的实验表明，微调后的LLM能生成高质量功能数据，提升语义表示和设计探索效果。

Conclusion: LLM的领域自适应框架能有效改善功能建模的自动化水平，支持早期工程设计。

Abstract: The conceptual design phase represents a critical early stage in the product
development process, where designers generate potential solutions that meet
predefined design specifications based on functional requirements. Functional
modeling, a foundational aspect of this phase, enables designers to reason
about product functions before specific structural details are determined. A
widely adopted approach to functional modeling is the
Function-Behavior-Structure (FBS) framework, which supports the transformation
of functional intent into behavioral and structural descriptions. However, the
effectiveness of function-based design is often hindered by the lack of
well-structured and comprehensive functional data. This scarcity can negatively
impact early design decision-making and hinder the development of accurate
behavioral models. Recent advances in Large Language Models (LLMs), such as
those based on GPT architectures, offer a promising avenue to address this gap.
LLMs have demonstrated significant capabilities in language understanding and
natural language processing (NLP), making them suitable for automated
classification tasks. This study proposes a novel LLM-based domain adaptation
(DA) framework using fine-tuning for the automated classification of mechanical
assembly parts' functions. By fine-tuning LLMs on domain-specific datasets, the
traditionally manual and subjective process of function annotation can be
improved in both accuracy and consistency. A case study demonstrates
fine-tuning GPT-3.5 Turbo on data from the Oregon State Design Repository
(OSDR), and evaluation on the A Big CAD (ABC) dataset shows that the
domain-adapted LLM can generate high-quality functional data, enhancing the
semantic representation of mechanical parts and supporting more effective
design exploration in early-phase engineering.

</details>

### [53] [Causally Fair Node Classification on Non-IID Graph Data](https://arxiv.org/abs/2505.01652)
*Yucong Dai,Lu Zhang,Yaowei Hu,Susan Gauch,Yongkai Wu*

Main category: cs.LG

TLDR: 该论文提出了一种基于因果关系的公平机器学习方法，针对图数据中的非独立同分布（非IID）问题，开发了MPVA模型以估算干预分布并减少偏见。


<details>
  <summary>Details</summary>
Motivation: 现有公平机器学习方法通常假设数据独立同分布（IID），忽略了图数据中实例间的因果关系，导致偏见干预效果不佳。

Method: 基于网络结构因果模型（NSCM）框架，提出可分解性和图独立性假设，开发了MPVA模型，利用$do$-calculus计算干预分布。

Result: 实验表明，MPVA在半合成和真实数据集上优于传统方法，能有效估算干预分布并减少偏见。

Conclusion: 研究强调了基于因果关系的公平性在复杂ML应用中的潜力，为未来放宽假设以增强模型公平性奠定了基础。

Abstract: Fair machine learning seeks to identify and mitigate biases in predictions
against unfavorable populations characterized by demographic attributes, such
as race and gender. Recently, a few works have extended fairness to graph data,
such as social networks, but most of them neglect the causal relationships
among data instances. This paper addresses the prevalent challenge in
fairness-aware ML algorithms, which typically assume Independent and
Identically Distributed (IID) data. We tackle the overlooked domain of non-IID,
graph-based settings where data instances are interconnected, influencing the
outcomes of fairness interventions. We base our research on the Network
Structural Causal Model (NSCM) framework and posit two main assumptions:
Decomposability and Graph Independence, which enable the computation of
interventional distributions in non-IID settings using the $do$-calculus. Based
on that, we develop the Message Passing Variational Autoencoder for Causal
Inference (MPVA) to compute interventional distributions and facilitate
causally fair node classification through estimated interventional
distributions. Empirical evaluations on semi-synthetic and real-world datasets
demonstrate that MPVA outperforms conventional methods by effectively
approximating interventional distributions and mitigating bias. The
implications of our findings underscore the potential of causality-based
fairness in complex ML applications, setting the stage for further research
into relaxing the initial assumptions to enhance model fairness.

</details>

### [54] [Focal-SAM: Focal Sharpness-Aware Minimization for Long-Tailed Classification](https://arxiv.org/abs/2505.01660)
*Sicong Li,Qianqian Xu,Zhiyong Yang,Zitai Wang,Linchao Zhang,Xiaochun Cao,Qingming Huang*

Main category: cs.LG

TLDR: Focal-SAM通过为不同类别的锐度分配不同惩罚，实现了精细控制且保持计算效率，解决了长尾数据集中泛化问题。


<details>
  <summary>Details</summary>
Motivation: 解决长尾数据集中尾类泛化困难的问题，同时避免现有方法（如ImbSAM和CC-SAM）在计算效率与控制精度之间的权衡。

Method: 提出Focal-SAM，通过类别相关的锐度惩罚实现精细控制，无需额外反向传播。

Result: 理论分析表明Focal-SAM具有更优的泛化边界，实验验证其在传统和基础模型中的有效性。

Conclusion: Focal-SAM在保持高效的同时，显著提升了长尾数据集中的泛化性能。

Abstract: Real-world datasets often follow a long-tailed distribution, making
generalization to tail classes difficult. Recent methods resorted to long-tail
variants of Sharpness-Aware Minimization (SAM), such as ImbSAM and CC-SAM, to
improve generalization by flattening the loss landscape. However, these
attempts face a trade-off between computational efficiency and control over the
loss landscape. On the one hand, ImbSAM is efficient but offers only coarse
control as it excludes head classes from the SAM process. On the other hand,
CC-SAM provides fine-grained control through class-dependent perturbations but
at the cost of efficiency due to multiple backpropagations. Seeing this
dilemma, we introduce Focal-SAM, which assigns different penalties to
class-wise sharpness, achieving fine-grained control without extra
backpropagations, thus maintaining efficiency. Furthermore, we theoretically
analyze Focal-SAM's generalization ability and derive a sharper generalization
bound. Extensive experiments on both traditional and foundation models validate
the effectiveness of Focal-SAM.

</details>

### [55] [Adaptively Point-weighting Curriculum Learning](https://arxiv.org/abs/2505.01665)
*Wensheng Li,Hao Wang,Ruifeng Zhou,Hanting Guan,Chao Zhang,Dacheng Tao*

Main category: cs.LG

TLDR: 本文提出了一种自适应点加权（APW）课程学习算法，通过动态调整训练样本权重，优化深度网络的训练效果。


<details>
  <summary>Details</summary>
Motivation: 模仿人类学习过程，通过先易后难的训练策略提升深度网络的训练效率和性能。

Method: 开发APW算法，根据样本训练误差和网络当前状态动态分配权重，早期侧重易样本，后期侧重难样本。

Result: 实验验证了APW的优越性，理论分析支持其训练有效性、可行性和泛化性能。

Conclusion: APW是一种有效的课程学习算法，能显著提升深度网络的训练效果。

Abstract: Curriculum learning (CL) is referred to as a training strategy that makes
easy samples learned first and then fits hard samples. It imitates the process
of humans learning knowledge, and has become a potential manner of effectively
training deep networks. In this study, we develop the adaptively
point-weighting (APW) curriculum learning algorithm, which adaptively assigns
the weight to every training sample not only based on its training error but
also considering the current training state of the network. Specifically, in
the early training phase, it increases the weights of easy samples to make the
network rapidly capture the overall characteristics of the dataset; and in the
later training phase, the weights of hard points rise to improve the fitting
performance on the discrete local regions. Moreover, we also present the
theoretical analysis on the properties of APW including training effectiveness,
training feasibility, training stability, and generalization performance. The
numerical experiments support the superiority of APW and demonstrate the
validity of our theoretical findings.

</details>

### [56] [PoseX: AI Defeats Physics Approaches on Protein-Ligand Cross Docking](https://arxiv.org/abs/2505.01700)
*Yize Jiang,Xinze Li,Yuanyuan Zhang,Jin Han,Youjun Xu,Ayush Pandit,Zaixi Zhang,Mengdi Wang,Mengyang Wang,Chong Liu,Guang Yang,Yejin Choi,Wu-Jun Li,Tianfan Fu,Fang Wu,Junhong Liu*

Main category: cs.LG

TLDR: PoseX是一个开源基准测试，专注于自对接和交叉对接，旨在全面评估蛋白质-配体对接方法的算法进步。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在实用性不足或框架复杂的问题，PoseX旨在填补这一空白。

Method: 1. 构建包含718个自对接和1,312个交叉对接条目的数据集；2. 整合22种对接方法；3. 设计后处理松弛方法；4. 发布实时排名榜单。

Result: AI方法在对接精度上超越传统物理方法，松弛后处理显著提升性能，但AI共折叠方法存在配体手性问题。

Conclusion: PoseX为对接方法评估提供了实用工具，AI方法结合松弛后处理表现最佳。

Abstract: Recently, significant progress has been made in protein-ligand docking,
especially in modern deep learning methods, and some benchmarks were proposed,
e.g., PoseBench, Plinder. However, these benchmarks suffer from less practical
evaluation setups (e.g., blind docking, self docking), or heavy framework that
involves training, raising challenges to assess docking methods efficiently. To
fill this gap, we proposed PoseX, an open-source benchmark focusing on
self-docking and cross-docking, to evaluate the algorithmic advances
practically and comprehensively. Specifically, first, we curate a new
evaluation dataset with 718 entries for self docking and 1,312 for cross
docking; second, we incorporate 22 docking methods across three methodological
categories, including (1) traditional physics-based methods (e.g.,
Schr\"odinger Glide), (2) AI docking methods (e.g., DiffDock), (3) AI
co-folding methods (e.g., AlphaFold3); third, we design a relaxation method as
post-processing to minimize conformation energy and refine binding pose;
fourth, we released a leaderboard to rank submitted models in real time. We
draw some key insights via extensive experiments: (1) AI-based approaches have
already surpassed traditional physics-based approaches in overall docking
accuracy (RMSD). The longstanding generalization issues that have plagued AI
molecular docking have been significantly alleviated in the latest models. (2)
The stereochemical deficiencies of AI-based approaches can be greatly
alleviated with post-processing relaxation. Combining AI docking methods with
the enhanced relaxation method achieves the best performance to date. (3) AI
co-folding methods commonly face ligand chirality issues, which cannot be
resolved by relaxation. The code, curated dataset and leaderboard are released
at https://github.com/CataAI/PoseX.

</details>

### [57] [PeSANet: Physics-encoded Spectral Attention Network for Simulating PDE-Governed Complex Systems](https://arxiv.org/abs/2505.01736)
*Han Wan,Rui Zhang,Qi Wang,Yang Liu,Hao Sun*

Main category: cs.LG

TLDR: 提出了一种名为PeSANet的模型，结合局部和全局信息，用于在数据有限和物理先验不完整的情况下预测复杂系统。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在现实场景中因物理规律不完整或未知而受限，而机器学习方法在数据稀缺时泛化能力不足。

Method: PeSANet包含物理编码块和频谱增强块，前者通过硬约束近似局部微分算子，后者在频域中捕捉长程全局依赖。

Result: 实验表明，PeSANet在所有指标上优于现有方法，尤其在长期预测准确性上表现突出。

Conclusion: PeSANet为数据有限和物理不完整的复杂系统模拟提供了有前景的解决方案。

Abstract: Accurately modeling and forecasting complex systems governed by partial
differential equations (PDEs) is crucial in various scientific and engineering
domains. However, traditional numerical methods struggle in real-world
scenarios due to incomplete or unknown physical laws. Meanwhile, machine
learning approaches often fail to generalize effectively when faced with scarce
observational data and the challenge of capturing local and global features. To
this end, we propose the Physics-encoded Spectral Attention Network (PeSANet),
which integrates local and global information to forecast complex systems with
limited data and incomplete physical priors. The model consists of two key
components: a physics-encoded block that uses hard constraints to approximate
local differential operators from limited data, and a spectral-enhanced block
that captures long-range global dependencies in the frequency domain.
Specifically, we introduce a novel spectral attention mechanism to model
inter-spectrum relationships and learn long-range spatial features.
Experimental results demonstrate that PeSANet outperforms existing methods
across all metrics, particularly in long-term forecasting accuracy, providing a
promising solution for simulating complex systems with limited data and
incomplete physics.

</details>

### [58] [Memory-Efficient LLM Training by Various-Grained Low-Rank Projection of Gradients](https://arxiv.org/abs/2505.01744)
*Yezhen Wang,Zhouhao Yang,Brian K Chen,Fanyi Pu,Bo Li,Tianyu Gao,Kenji Kawaguchi*

Main category: cs.LG

TLDR: VLoRP框架通过引入投影粒度控制，优化低秩梯度投影（LoRP）的内存效率与性能平衡，并提出ProjFactor优化器减少内存需求。


<details>
  <summary>Details</summary>
Motivation: 现有LoRP方法默认以梯度矩阵的每行为投影单位，未充分探索投影粒度的影响。

Method: 提出VLoRP框架，引入投影粒度自由度，并设计ProjFactor优化器。

Result: 实验验证VLoRP在固定内存预算下提升稳定性和效率，ProjFactor显著减少内存需求。

Conclusion: VLoRP和ProjFactor为内存高效微调提供了有效解决方案。

Abstract: Building upon the success of low-rank adapter (LoRA), low-rank gradient
projection (LoRP) has emerged as a promising solution for memory-efficient
fine-tuning. However, existing LoRP methods typically treat each row of the
gradient matrix as the default projection unit, leaving the role of projection
granularity underexplored. In this work, we propose a novel framework, VLoRP,
that extends low-rank gradient projection by introducing an additional degree
of freedom for controlling the trade-off between memory efficiency and
performance, beyond the rank hyper-parameter. Through this framework, we
systematically explore the impact of projection granularity, demonstrating that
finer-grained projections lead to enhanced stability and efficiency even under
a fixed memory budget. Regarding the optimization for VLoRP, we present
ProjFactor, an adaptive memory-efficient optimizer, that significantly reduces
memory requirement while ensuring competitive performance, even in the presence
of gradient accumulation. Additionally, we provide a theoretical analysis of
VLoRP, demonstrating the descent and convergence of its optimization trajectory
under both SGD and ProjFactor. Extensive experiments are conducted to validate
our findings, covering tasks such as commonsense reasoning, MMLU, and GSM8K.

</details>

### [59] [Context-Aware Online Conformal Anomaly Detection with Prediction-Powered Data Acquisition](https://arxiv.org/abs/2505.01783)
*Amirmohammad Farzaneh,Osvaldo Simeone*

Main category: cs.LG

TLDR: 提出了一种基于上下文感知的预测驱动在线异常检测方法（C-PP-COAD），通过合成校准数据缓解真实数据稀缺问题，同时保持对假发现率的严格控制。


<details>
  <summary>Details</summary>
Motivation: 在线异常检测在多个领域至关重要，但现有方法依赖大量真实校准数据，而真实数据往往稀缺。

Method: 结合合成校准数据和真实数据，利用符合p值、主动p值统计和在线FDR控制机制。

Result: 实验表明，C-PP-COAD显著减少对真实校准数据的依赖，同时保持FDR控制。

Conclusion: C-PP-COAD为解决真实数据稀缺问题提供了一种有效且可靠的解决方案。

Abstract: Online anomaly detection is essential in fields such as cybersecurity,
healthcare, and industrial monitoring, where promptly identifying deviations
from expected behavior can avert critical failures or security breaches. While
numerous anomaly scoring methods based on supervised or unsupervised learning
have been proposed, current approaches typically rely on a continuous stream of
real-world calibration data to provide assumption-free guarantees on the false
discovery rate (FDR). To address the inherent challenges posed by limited real
calibration data, we introduce context-aware prediction-powered conformal
online anomaly detection (C-PP-COAD). Our framework strategically leverages
synthetic calibration data to mitigate data scarcity, while adaptively
integrating real data based on contextual cues. C-PP-COAD utilizes conformal
p-values, active p-value statistics, and online FDR control mechanisms to
maintain rigorous and reliable anomaly detection performance over time.
Experiments conducted on both synthetic and real-world datasets demonstrate
that C-PP-COAD significantly reduces dependency on real calibration data
without compromising guaranteed FDR control.

</details>

### [60] [Privacy Preserving Machine Learning Model Personalization through Federated Personalized Learning](https://arxiv.org/abs/2505.01788)
*Md. Tanzib Hosain,Asif Zaman,Md. Shahriar Sajid,Shadman Sakeeb Khan,Shanjida Akter*

Main category: cs.LG

TLDR: 论文分析了隐私保护机器学习中的联邦个性化学习（PPMLFPL），评估了其在个性化模型优化与数据隐私保护之间的平衡，推荐使用APPLE+HE算法。


<details>
  <summary>Details</summary>
Motivation: AI的广泛应用引发了对数据隐私的担忧，推动了隐私保护机器学习的研究，尤其是联邦学习（FL）在分散数据上的应用。

Method: 研究采用PPMLFPL框架，结合差分隐私（DP）和同态加密（HE）技术，评估了APPLE+DP和APPLE+HE算法的性能。

Result: APPLE+HE算法在隐私保护机器学习任务中表现优异，被强烈推荐用于联邦个性化学习场景。

Conclusion: PPMLFPL为隐私意识的数据驱动技术提供了有前景的研究方向，未来可进一步优化。

Abstract: The widespread adoption of Artificial Intelligence (AI) has been driven by
significant advances in intelligent system research. However, this progress has
raised concerns about data privacy, leading to a growing awareness of the need
for privacy-preserving AI. In response, there has been a seismic shift in
interest towards the leading paradigm for training Machine Learning (ML) models
on decentralized data silos while maintaining data privacy, Federated Learning
(FL). This research paper presents a comprehensive performance analysis of a
cutting-edge approach to personalize ML model while preserving privacy achieved
through Privacy Preserving Machine Learning with the innovative framework of
Federated Personalized Learning (PPMLFPL). Regarding the increasing concerns
about data privacy, this study evaluates the effectiveness of PPMLFPL
addressing the critical balance between personalized model refinement and
maintaining the confidentiality of individual user data. According to our
analysis, Adaptive Personalized Cross-Silo Federated Learning with Differential
Privacy (APPLE+DP) offering efficient execution whereas overall, the use of the
Adaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption
(APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated
personalized learning settings is strongly suggested. The results offer
valuable insights creating it a promising scope for future advancements in the
field of privacy-conscious data-driven technologies.

</details>

### [61] [Conformal Prediction for Indoor Positioning with Correctness Coverage Guarantees](https://arxiv.org/abs/2505.01810)
*Zhiyi Zhou,Hexin Peng,Hongyu Long*

Main category: cs.LG

TLDR: 论文提出了一种基于共形预测（CP）的深度学习室内定位方法，解决了传统方法的泛化性差、过拟合和可解释性不足问题，并通过实验验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 随着物联网技术的发展，高精度室内定位对复杂环境中的位置服务至关重要，但传统指纹定位方法存在泛化性差、过拟合和可解释性不足的挑战。

Method: 应用共形预测（CP）技术，将模型不确定性转化为非一致性分数，构建预测集以确保正确覆盖率，并引入共形风险控制管理FDR和FNR。

Result: 模型在训练集上准确率约100%，测试集上85%，共形预测技术能有效逼近目标覆盖率，不同模型在预测集大小和不确定性量化上表现各异。

Conclusion: 共形预测方法显著提升了室内定位的性能和泛化能力，为路径导航任务提供了统计保障。

Abstract: With the advancement of Internet of Things (IoT) technologies, high-precision
indoor positioning has become essential for Location-Based Services (LBS) in
complex indoor environments. Fingerprint-based localization is popular, but
traditional algorithms and deep learning-based methods face challenges such as
poor generalization, overfitting, and lack of interpretability. This paper
applies conformal prediction (CP) to deep learning-based indoor positioning. CP
transforms the uncertainty of the model into a non-conformity score, constructs
prediction sets to ensure correctness coverage, and provides statistical
guarantees. We also introduce conformal risk control for path navigation tasks
to manage the false discovery rate (FDR) and the false negative rate (FNR).The
model achieved an accuracy of approximately 100% on the training dataset and
85% on the testing dataset, effectively demonstrating its performance and
generalization capability. Furthermore, we also develop a conformal p-value
framework to control the proportion of position-error points. Experiments on
the UJIIndoLoc dataset using lightweight models such as MobileNetV1, VGG19,
MobileNetV2, ResNet50, and EfficientNet show that the conformal prediction
technique can effectively approximate the target coverage, and different models
have different performance in terms of prediction set size and uncertainty
quantification.

</details>

### [62] [An LSTM-PINN Hybrid Method to the specific problem of population forecasting](https://arxiv.org/abs/2505.01819)
*Ze Tao*

Main category: cs.LG

TLDR: 论文提出了两种物理信息深度学习框架（PINN和LSTM-PINN），用于模拟政策驱动的生育变化下的年龄结构人口动态，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于领域知识与长期时间依赖性的结合不足，准确捕捉政策驱动的年龄结构人口动态是一个挑战。

Method: 提出了PINN和LSTM-PINN框架，结合政策感知的生育函数和传输-反应偏微分方程，分别通过基于配置的训练和序列记忆机制来模拟人口动态。

Result: 在三种生育政策情景下的模拟结果表明，模型能够反映政策敏感的人口变化，并验证了领域知识与数据驱动预测结合的有效性。

Conclusion: 研究为政策干预下的年龄结构人口动态建模提供了新颖且可扩展的框架，为人口预测和政策规划提供了有价值的见解。

Abstract: Deep learning has emerged as a powerful tool in scientific modeling,
particularly for complex dynamical systems; however, accurately capturing
age-structured population dynamics under policy-driven fertility changes
remains a significant challenge due to the lack of effective integration
between domain knowledge and long-term temporal dependencies. To address this
issue, we propose two physics-informed deep learning frameworks--PINN and
LSTM-PINN--that incorporate policy-aware fertility functions into a
transport-reaction partial differential equation to simulate population
evolution from 2024 to 2054. The standard PINN model enforces the governing
equation and boundary conditions via collocation-based training, enabling
accurate learning of underlying population dynamics and ensuring stable
convergence. Building on this, the LSTM-PINN framework integrates sequential
memory mechanisms to effectively capture long-range dependencies in the
age-time domain, achieving robust training performance across multiple loss
components. Simulation results under three distinct fertility policy
scenarios-the Three-child policy, the Universal two-child policy, and the
Separate two-child policy--demonstrate the models' ability to reflect
policy-sensitive demographic shifts and highlight the effectiveness of
integrating domain knowledge into data-driven forecasting. This study provides
a novel and extensible framework for modeling age-structured population
dynamics under policy interventions, offering valuable insights for
data-informed demographic forecasting and long-term policy planning in the face
of emerging population challenges.

</details>

### [63] [Analytic Energy-Guided Policy Optimization for Offline Reinforcement Learning](https://arxiv.org/abs/2505.01822)
*Jifeng Hu,Sili Huang,Zhejian Yang,Shengchao Hu,Li Shen,Hechang Chen,Lichao Sun,Yi Chang,Dacheng Tao*

Main category: cs.LG

TLDR: 论文提出AEPO方法，通过理论分析和闭式解解决扩散模型中能量估计问题，并在离线RL任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型中因对数期望形式导致的中间能量估计难题。

Method: 提出AEPO方法，分析条件高斯变换下的中间引导闭式解，训练神经网络逼近对数期望目标估计。

Result: 在30+离线RL任务中超越基准方法，D4RL基准测试表现突出。

Conclusion: AEPO有效解决了能量估计问题，提升了扩散模型在RL中的性能。

Abstract: Conditional decision generation with diffusion models has shown powerful
competitiveness in reinforcement learning (RL). Recent studies reveal the
relation between energy-function-guidance diffusion models and constrained RL
problems. The main challenge lies in estimating the intermediate energy, which
is intractable due to the log-expectation formulation during the generation
process. To address this issue, we propose the Analytic Energy-guided Policy
Optimization (AEPO). Specifically, we first provide a theoretical analysis and
the closed-form solution of the intermediate guidance when the diffusion model
obeys the conditional Gaussian transformation. Then, we analyze the posterior
Gaussian distribution in the log-expectation formulation and obtain the target
estimation of the log-expectation under mild assumptions. Finally, we train an
intermediate energy neural network to approach the target estimation of
log-expectation formulation. We apply our method in 30+ offline RL tasks to
demonstrate the effectiveness of our method. Extensive experiments illustrate
that our method surpasses numerous representative baselines in D4RL offline
reinforcement learning benchmarks.

</details>

### [64] [Towards Trustworthy Federated Learning with Untrusted Participants](https://arxiv.org/abs/2505.01874)
*Youssef Allouah,Rachid Guerraoui,John Stephan*

Main category: cs.LG

TLDR: CafCor算法通过共享随机性种子实现分布式学习中的隐私与鲁棒性，无需信任中央服务器，性能优于本地差分隐私，接近中心差分隐私。


<details>
  <summary>Details</summary>
Motivation: 解决分布式学习中恶意参与者和数据隐私问题，避免依赖信任中央服务器的强假设。

Method: 提出CafCor算法，结合鲁棒梯度聚合和相关噪声注入，利用工人间的共享随机性。

Result: CafCor在隐私-效用权衡上表现优异，优于本地差分隐私，接近中心差分隐私。

Conclusion: 隐私与鲁棒性可在不牺牲效用或不信任服务器的情况下共存于分布式系统。

Abstract: Resilience against malicious parties and data privacy are essential for
trustworthy distributed learning, yet achieving both with good utility
typically requires the strong assumption of a trusted central server. This
paper shows that a significantly weaker assumption suffices: each pair of
workers shares a randomness seed unknown to others. In a setting where
malicious workers may collude with an untrusted server, we propose CafCor, an
algorithm that integrates robust gradient aggregation with correlated noise
injection, leveraging shared randomness between workers. We prove that CafCor
achieves strong privacy-utility trade-offs, significantly outperforming local
differential privacy (DP) methods, which do not make any trust assumption,
while approaching central DP utility, where the server is fully trusted.
Empirical results on standard benchmarks validate CafCor's practicality,
showing that privacy and robustness can coexist in distributed systems without
sacrificing utility or trusting the server.

</details>

### [65] [OODTE: A Differential Testing Engine for the ONNX Optimizer](https://arxiv.org/abs/2505.01892)
*Nikolaos Louloudakis,Ajitha Rajan*

Main category: cs.LG

TLDR: ONNX Optimizer缺乏严格的准确性验证，作者提出OODTE工具，通过差分测试评估其优化效果，发现多个未知问题。


<details>
  <summary>Details</summary>
Motivation: ONNX Optimizer作为标准工具，其优化后模型准确性未充分验证，需系统评估。

Method: OODTE采用差分测试方法，对比原始与优化模型输出，定位问题根源。

Result: 测试130个模型，发现15个问题（14个未知），9.2%模型导致优化器崩溃，30%分类模型准确性差异。

Conclusion: OODTE有效识别ONNX Optimizer问题，验证工具实用性，建议广泛采用。

Abstract: With $700$ stars on GitHub and part of the official ONNX repository, the ONNX
Optimizer consists of the standard method to apply graph-based optimizations on
ONNX models. However, its ability to preserve model accuracy across
optimizations, has not been rigorously explored. We propose OODTE, a utility to
automatically and thoroughly assess the correctness of the ONNX Optimizer.
OODTE follows a simple, yet effective differential testing and evaluation
approach that can be easily adopted to other compiler optimizers. In
particular, OODTE utilizes a number of ONNX models, then optimizes them and
executes both the original and the optimized variants across a user-defined set
of inputs, while automatically logging any issues with the optimization
process. Finally, for successfully optimized models, OODTE compares the
results, and, if any accuracy deviations are observed, it iteratively repeats
the process for each pass of the ONNX Optimizer, to localize the root cause of
the differences observed. Using OODTE, we sourced well-known $130$ models from
the official ONNX Model Hub, used for a wide variety of tasks (classification,
object detection, semantic segmentation, text summarization, question and
answering, sentiment analysis) from the official ONNX model hub. We detected 15
issues, 14 of which were previously unknown, associated with optimizer crashes
and accuracy deviations. We also observed $9.2$% of all model instances
presenting issues leading into the crash of the optimizer, or the generation of
an invalid model while using the primary optimizer strategies. In addition,
$30$% of the classification models presented accuracy differences across the
original and the optimized model variants, while $16.6$% of semantic
segmentation and object detection models are also affected, at least to a
limited extent.

</details>

### [66] [From Players to Champions: A Generalizable Machine Learning Approach for Match Outcome Prediction with Insights from the FIFA World Cup](https://arxiv.org/abs/2505.01902)
*Ali Al-Bustami,Zaid Ghazal*

Main category: cs.LG

TLDR: 本文提出了一种机器学习框架，用于预测FIFA世界杯比赛结果，结合球队历史数据和球员表现指标，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 准确预测FIFA世界杯比赛结果对分析师、教练、投注者和球迷具有重要价值。

Method: 整合球队历史数据和球员表现指标（如进球、助攻、传球准确率和抢断），采用分类技术、降维和超参数优化。

Result: 在2022年FIFA世界杯数据上的实验表明，该方法优于基线方法。

Conclusion: 强调球员属性和球队组成对预测的重要性，为未来探索图神经网络等高级学习架构奠定基础。

Abstract: Accurate prediction of FIFA World Cup match outcomes holds significant value
for analysts, coaches, bettors, and fans. This paper presents a machine
learning framework specifically designed to forecast match winners in FIFA
World Cup. By integrating both team-level historical data and player-specific
performance metrics such as goals, assists, passing accuracy, and tackles, we
capture nuanced interactions often overlooked by traditional aggregate models.
Our methodology processes multi-year data to create year-specific team profiles
that account for evolving rosters and player development. We employ
classification techniques complemented by dimensionality reduction and
hyperparameter optimization, to yield robust predictive models. Experimental
results on data from the FIFA 2022 World Cup demonstrate our approach's
superior accuracy compared to baseline method. Our findings highlight the
importance of incorporating individual player attributes and team-level
composition to enhance predictive performance, offering new insights into
player synergy, strategic match-ups, and tournament progression scenarios. This
work underscores the transformative potential of rich, player-centric data in
sports analytics, setting a foundation for future exploration of advanced
learning architectures such as graph neural networks to model complex team
interactions.

</details>

### [67] [LookAlike: Consistent Distractor Generation in Math MCQs](https://arxiv.org/abs/2505.01903)
*Nisarg Parikh,Nigel Fernandez,Alexander Scarlatos,Simon Woodhead,Andrew Lan*

Main category: cs.LG

TLDR: LookAlike方法通过偏好优化提升多选题目干扰项的生成质量，利用模型不一致性生成偏好对，并结合监督微调与直接偏好优化，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成与常见学生错误一致的干扰项方面存在不足，LookAlike旨在解决这一问题。

Method: 通过挖掘模型不一致性生成合成偏好对，交替使用监督微调（SFT）与直接偏好优化（DPO）以稳定训练。

Result: 在1400+数学题目数据集上，LookAlike在干扰项生成和错误生成上的准确率分别为51.6%和57.2%，优于现有方法。

Conclusion: 偏好优化和不一致性挖掘可有效提升干扰项生成的准确性和一致性。

Abstract: Large language models (LLMs) are increasingly used to generate distractors
for multiple-choice questions (MCQs), especially in domains like math
education. However, existing approaches are limited in ensuring that the
generated distractors are consistent with common student errors. We propose
LookAlike, a method that improves error-distractor consistency via preference
optimization. Our two main innovations are: (a) mining synthetic preference
pairs from model inconsistencies, and (b) alternating supervised fine-tuning
(SFT) with Direct Preference Optimization (DPO) to stabilize training. Unlike
prior work that relies on heuristics or manually annotated preference data,
LookAlike uses its own generation inconsistencies as dispreferred samples, thus
enabling scalable and stable training. Evaluated on a real-world dataset of
1,400+ math MCQs, LookAlike achieves 51.6% accuracy in distractor generation
and 57.2% in error generation under LLM-as-a-judge evaluation, outperforming an
existing state-of-the-art method (45.6% / 47.7%). These improvements highlight
the effectiveness of preference-based regularization and inconsistency mining
for generating consistent math MCQ distractors at scale.

</details>

### [68] [BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models](https://arxiv.org/abs/2505.01912)
*Evan R. Antoniuk,Shehtab Zaman,Tal Ben-Nun,Peggy Li,James Diffenderfer,Busra Demirci,Obadiah Smolenski,Tim Hsu,Anna M. Hiszpanski,Kenneth Chiu,Bhavya Kailkhura,Brian Van Essen*

Main category: cs.LG

TLDR: 论文提出了BOOM基准，用于评估分子属性预测模型的OOD性能，发现现有模型在OOD任务上表现不佳，并探讨了影响OOD性能的关键因素。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统性评估分子OOD预测任务的基准，且机器学习模型在OOD泛化上表现不佳，阻碍了新型分子的发现。

Method: 通过评估140多种模型与任务组合，分析OOD性能，并探讨数据生成、预训练、超参数优化等因素的影响。

Result: 现有模型在OOD任务上表现普遍较差，最佳模型的OOD误差是分布内误差的3倍。化学基础模型在OOD外推能力上仍有不足。

Conclusion: 开发具有强OOD泛化能力的ML模型是化学ML领域的新挑战，BOOM基准将开源以促进研究。

Abstract: Advances in deep learning and generative modeling have driven interest in
data-driven molecule discovery pipelines, whereby machine learning (ML) models
are used to filter and design novel molecules without requiring prohibitively
expensive first-principles simulations. Although the discovery of novel
molecules that extend the boundaries of known chemistry requires accurate
out-of-distribution (OOD) predictions, ML models often struggle to generalize
OOD. Furthermore, there are currently no systematic benchmarks for molecular
OOD prediction tasks. We present BOOM, $\boldsymbol{b}$enchmarks for
$\boldsymbol{o}$ut-$\boldsymbol{o}$f-distribution $\boldsymbol{m}$olecular
property predictions -- a benchmark study of property-based out-of-distribution
models for common molecular property prediction models. We evaluate more than
140 combinations of models and property prediction tasks to benchmark deep
learning models on their OOD performance. Overall, we do not find any existing
models that achieve strong OOD generalization across all tasks: even the top
performing model exhibited an average OOD error 3x larger than in-distribution.
We find that deep learning models with high inductive bias can perform well on
OOD tasks with simple, specific properties. Although chemical foundation models
with transfer and in-context learning offer a promising solution for limited
training data scenarios, we find that current foundation models do not show
strong OOD extrapolation capabilities. We perform extensive ablation
experiments to highlight how OOD performance is impacted by data generation,
pre-training, hyperparameter optimization, model architecture, and molecular
representation. We propose that developing ML models with strong OOD
generalization is a new frontier challenge in chemical ML model development.
This open-source benchmark will be made available on Github.

</details>

### [69] [Unemployment Dynamics Forecasting with Machine Learning Regression Models](https://arxiv.org/abs/2505.01933)
*Kyungsu Kim*

Main category: cs.LG

TLDR: 本文比较了七种模型（线性回归、SGDRegressor、随机森林、XGBoost、CatBoost、支持向量回归和LSTM网络）在预测美国月度失业率时的表现，发现基于树的集成模型（尤其是CatBoost）和LSTM网络表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用回归和机器学习技术对美国月度失业数据进行及时预测，为经济学家和政策制定者提供更丰富的劳动力市场趋势洞察。

Method: 使用七种模型对历史失业数据训练，并在保留期评估性能，输入特征包括宏观经济指标、劳动力市场指标、金融变量和消费者情绪。

Result: 基于树的集成模型（特别是CatBoost）和LSTM网络在预测失业率方面表现优于线性模型和其他非线性方法。

Conclusion: 现代机器学习技术（尤其是集成和深度学习方法）可以显著提升实时失业率预测的准确性，为决策者提供更可靠的劳动力市场分析工具。

Abstract: In this paper, I explored how a range of regression and machine learning
techniques can be applied to monthly U.S. unemployment data to produce timely
forecasts. I compared seven models: Linear Regression, SGDRegressor, Random
Forest, XGBoost, CatBoost, Support Vector Regression, and an LSTM network,
training each on a historical span of data and then evaluating on a later
hold-out period. Input features include macro indicators (GDP growth, CPI),
labor market measures (job openings, initial claims), financial variables
(interest rates, equity indices), and consumer sentiment.
  I tuned model hyperparameters via cross-validation and assessed performance
with standard error metrics and the ability to predict the correct unemployment
direction. Across the board, tree-based ensembles (and CatBoost in particular)
deliver noticeably better forecasts than simple linear approaches, while the
LSTM captures underlying temporal patterns more effectively than other
nonlinear methods. SVR and SGDRegressor yield modest gains over standard
regression but don't match the consistency of the ensemble and deep-learning
models.
  Interpretability tools ,feature importance rankings and SHAP values, point to
job openings and consumer sentiment as the most influential predictors across
all methods. By directly comparing linear, ensemble, and deep-learning
approaches on the same dataset, our study shows how modern machine-learning
techniques can enhance real-time unemployment forecasting, offering economists
and policymakers richer insights into labor market trends.
  In the comparative evaluation of the models, I employed a dataset comprising
thirty distinct features over the period from January 2020 through December
2024.

</details>

### [70] [Multi-Scale Graph Learning for Anti-Sparse Downscaling](https://arxiv.org/abs/2505.01948)
*Yingda Fan,Runlong Yu,Janet R. Barclay,Alison P. Appling,Yiming Sun,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TLDR: 提出了一种多尺度图学习方法（MSGL），通过多任务学习框架和跨尺度插值学习，解决了细尺度数据不足的问题，并在美国特拉华河流域的水温预测中表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于细尺度数据不足，现有方法难以在小空间尺度（≤1 km）准确预测水温，影响水质管理和水生栖息地保护。

Method: 采用多任务学习框架，结合粗尺度图学习和跨尺度插值学习，提出异步多尺度图学习（ASYNC-MSGL）方法。

Result: 实验表明，该方法在美国特拉华河流域的日水温预测中表现优异，具有实际应用潜力。

Conclusion: MSGL和ASYNC-MSGL方法为细尺度水温预测提供了有效解决方案，适用于水资源监测和管理。

Abstract: Water temperature can vary substantially even across short distances within
the same sub-watershed. Accurate prediction of stream water temperature at fine
spatial resolutions (i.e., fine scales, $\leq$ 1 km) enables precise
interventions to maintain water quality and protect aquatic habitats. Although
spatiotemporal models have made substantial progress in spatially coarse time
series modeling, challenges persist in predicting at fine spatial scales due to
the lack of data at that scale.To address the problem of insufficient
fine-scale data, we propose a Multi-Scale Graph Learning (MSGL) method. This
method employs a multi-task learning framework where coarse-scale graph
learning, bolstered by larger datasets, simultaneously enhances fine-scale
graph learning. Although existing multi-scale or multi-resolution methods
integrate data from different spatial scales, they often overlook the spatial
correspondences across graph structures at various scales. To address this, our
MSGL introduces an additional learning task, cross-scale interpolation
learning, which leverages the hydrological connectedness of stream locations
across coarse- and fine-scale graphs to establish cross-scale connections,
thereby enhancing overall model performance. Furthermore, we have broken free
from the mindset that multi-scale learning is limited to synchronous training
by proposing an Asynchronous Multi-Scale Graph Learning method (ASYNC-MSGL).
Extensive experiments demonstrate the state-of-the-art performance of our
method for anti-sparse downscaling of daily stream temperatures in the Delaware
River Basin, USA, highlighting its potential utility for water resources
monitoring and management.

</details>

### [71] [Semantic Probabilistic Control of Language Models](https://arxiv.org/abs/2505.01954)
*Kareem Ahmed,Catarina G Belem,Padhraic Smyth,Sameer Singh*

Main category: cs.LG

TLDR: 提出了一种利用验证器梯度信息的方法，高效地引导语言模型生成满足非词汇约束（如毒性、情感或礼貌）的文本。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效处理非分解性验证器的语义约束，或依赖低效的采样估计。

Method: 通过验证器的梯度信息重新加权下一个词分布，利用初始样本创建局部语言模型分布，计算期望句子嵌入以估计约束满足概率。

Result: 在毒性、情感和主题一致性控制上，生成满足约束的文本概率超过95%，且质量不降低。

Conclusion: 该方法能高效精确地引导语言模型生成满足语义约束的文本。

Abstract: Semantic control entails steering LM generations towards satisfying subtle
non-lexical constraints, e.g., toxicity, sentiment, or politeness, attributes
that can be captured by a sequence-level verifier. It can thus be viewed as
sampling from the LM distribution conditioned on the target attribute, a
computationally intractable problem due to the non-decomposable nature of the
verifier. Existing approaches to LM control either only deal with syntactic
constraints which cannot capture the aforementioned attributes, or rely on
sampling to explore the conditional LM distribution, an ineffective estimator
for low-probability events. In this work, we leverage a verifier's gradient
information to efficiently reason over all generations that satisfy the target
attribute, enabling precise steering of LM generations by reweighing the
next-token distribution. Starting from an initial sample, we create a local LM
distribution favoring semantically similar sentences. This approximation
enables the tractable computation of an expected sentence embedding. We use
this expected embedding, informed by the verifier's evaluation at the initial
sample, to estimate the probability of satisfying the constraint, which
directly informs the update to the next-token distribution. We evaluated the
effectiveness of our approach in controlling the toxicity, sentiment, and
topic-adherence of LMs yielding generations satisfying the constraint with high
probability (>95%) without degrading their quality.

</details>

### [72] [EnsembleCI: Ensemble Learning for Carbon Intensity Forecasting](https://arxiv.org/abs/2505.01959)
*Leyi Yan,Linda Wang,Sihang Liu,Yi Ding*

Main category: cs.LG

TLDR: EnsembleCI是一种基于集成学习的自适应方法，用于碳强度（CI）预测，优于现有方法CarbonCast，平均提高预测精度19.58%，并增强区域适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法CarbonCast无法解决区域变异性和适应性不足的问题，限制了碳强度预测的准确性。

Method: EnsembleCI采用端到端的集成学习方法，结合多个子学习器的加权预测，提升灵活性和区域适应性。

Result: 在11个区域电网的评估中，EnsembleCI在几乎所有电网中均表现最优，平均MAPE降低19.58%，并减少预测变异性。

Conclusion: EnsembleCI是一种更准确、可靠的碳强度预测解决方案，具有实际应用价值和可解释性。

Abstract: Carbon intensity (CI) measures the average carbon emissions generated per
unit of electricity, making it a crucial metric for quantifying and managing
the environmental impact. Accurate CI predictions are vital for minimizing
carbon footprints, yet the state-of-the-art method (CarbonCast) falls short due
to its inability to address regional variability and lack of adaptability. To
address these limitations, we introduce EnsembleCI, an adaptive, end-to-end
ensemble learning-based approach for CI forecasting. EnsembleCI combines
weighted predictions from multiple sublearners, offering enhanced flexibility
and regional adaptability. In evaluations across 11 regional grids, EnsembleCI
consistently surpasses CarbonCast, achieving the lowest mean absolute
percentage error (MAPE) in almost all grids and improving prediction accuracy
by an average of 19.58%. While performance still varies across grids due to
inherent regional diversity, EnsembleCI reduces variability and exhibits
greater robustness in long-term forecasting compared to CarbonCast and
identifies region-specific key features, underscoring its interpretability and
practical relevance. These findings position EnsembleCI as a more accurate and
reliable solution for CI forecasting. EnsembleCI source code and data used in
this paper are available at https://github.com/emmayly/EnsembleCI.

</details>

### [73] [D3HRL: A Distributed Hierarchical Reinforcement Learning Approach Based on Causal Discovery and Spurious Correlation Detection](https://arxiv.org/abs/2505.01979)
*Chenran Zhao,Dianxi Shi,Mengzhu Wang,Jianqiang Xia,Huanhuan Yang,Songchang Jin,Shaowu Yang,Chunping Qiu*

Main category: cs.LG

TLDR: D3HRL是一种因果分层强化学习方法，通过建模延迟效应和消除虚假相关性，提升复杂环境中的决策能力。


<details>
  <summary>Details</summary>
Motivation: 解决分层强化学习中延迟效应和虚假相关性两大挑战。

Method: 1. 建模延迟效应为跨时间因果关系；2. 使用条件独立性测试消除虚假相关性；3. 基于真实因果关系构建分层策略。

Result: 在2D-MineCraft和MiniGrid实验中，D3HRL表现出对延迟效应的高敏感性和准确的因果关系识别能力。

Conclusion: D3HRL通过因果建模和迭代优化，显著提升了复杂环境中的决策可靠性。

Abstract: Current Hierarchical Reinforcement Learning (HRL) algorithms excel in
long-horizon sequential decision-making tasks but still face two challenges:
delay effects and spurious correlations. To address them, we propose a causal
HRL approach called D3HRL. First, D3HRL models delayed effects as causal
relationships across different time spans and employs distributed causal
discovery to learn these relationships. Second, it employs conditional
independence testing to eliminate spurious correlations. Finally, D3HRL
constructs and trains hierarchical policies based on the identified true causal
relationships. These three steps are iteratively executed, gradually exploring
the complete causal chain of the task. Experiments conducted in 2D-MineCraft
and MiniGrid show that D3HRL demonstrates superior sensitivity to delay effects
and accurately identifies causal relationships, leading to reliable
decision-making in complex environments.

</details>

### [74] [Always Skip Attention](https://arxiv.org/abs/2505.01996)
*Yiping Ji,Hemanth Saratchandran,Peyman Moghaddam,Simon Lucey*

Main category: cs.LG

TLDR: 现代视觉Transformer（ViT）中的自注意力机制在训练时完全依赖跳跃连接，否则会失败。本文提出Token Graying作为补充方法，进一步改善输入令牌的条件。


<details>
  <summary>Details</summary>
Motivation: 研究自注意力机制在ViT中对跳跃连接的独特依赖性，并探索其理论原因。

Method: 理论分析自注意力机制的条件问题，并提出Token Graying方法作为补充。

Result: 验证了Token Graying在监督和自监督训练中的有效性。

Conclusion: 自注意力机制在ViT中需要跳跃连接进行正则化，Token Graying进一步提升了性能。

Abstract: We highlight a curious empirical result within modern Vision Transformers
(ViTs). Specifically, self-attention catastrophically fails to train unless it
is used in conjunction with a skip connection. This is in contrast to other
elements of a ViT that continue to exhibit good performance (albeit suboptimal)
when skip connections are removed. Further, we show that this critical
dependence on skip connections is a relatively new phenomenon, with previous
deep architectures (\eg, CNNs) exhibiting good performance in their absence. In
this paper, we theoretically characterize that the self-attention mechanism is
fundamentally ill-conditioned and is, therefore, uniquely dependent on skip
connections for regularization. Additionally, we propose Token Graying -- a
simple yet effective complement (to skip connections) that further improves the
condition of input tokens. We validate our approach in both supervised and
self-supervised training methods.

</details>

### [75] [Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach](https://arxiv.org/abs/2505.01997)
*Jiancong Xiao,Bojian Hou,Zhanliang Wang,Ruochen Jin,Qi Long,Weijie J. Su,Li Shen*

Main category: cs.LG

TLDR: 论文研究了偏好对齐对大型语言模型（LLMs）校准的影响，提出了两种解决方法：校准感知微调和基于EM算法的ECE正则化。


<details>
  <summary>Details</summary>
Motivation: 偏好对齐导致LLMs校准变差，需要研究其原因并提出解决方案。

Method: 通过观察偏好对齐对校准的影响，提出校准感知微调和基于EM算法的ECE正则化方法。

Result: 实验验证了所提方法的有效性，能够在不影响性能的情况下改善校准。

Conclusion: 论文为偏好对齐导致的校准问题提供了有效解决方案，并提出了两种适用于不同校准状态的模型方法。

Abstract: One of the key technologies for the success of Large Language Models (LLMs)
is preference alignment. However, a notable side effect of preference alignment
is poor calibration: while the pre-trained models are typically
well-calibrated, LLMs tend to become poorly calibrated after alignment with
human preferences. In this paper, we investigate why preference alignment
affects calibration and how to address this issue. For the first question, we
observe that the preference collapse issue in alignment undesirably generalizes
to the calibration scenario, causing LLMs to exhibit overconfidence and poor
calibration. To address this, we demonstrate the importance of fine-tuning with
domain-specific knowledge to alleviate the overconfidence issue. To further
analyze whether this affects the model's performance, we categorize models into
two regimes: calibratable and non-calibratable, defined by bounds of Expected
Calibration Error (ECE). In the calibratable regime, we propose a
calibration-aware fine-tuning approach to achieve proper calibration without
compromising LLMs' performance. However, as models are further fine-tuned for
better performance, they enter the non-calibratable regime. For this case, we
develop an EM-algorithm-based ECE regularization for the fine-tuning loss to
maintain low calibration error. Extensive experiments validate the
effectiveness of the proposed methods.

</details>

### [76] [CASA: CNN Autoencoder-based Score Attention for Efficient Multivariate Long-term Time-series Forecasting](https://arxiv.org/abs/2505.02011)
*Minhyuk Lee,HyeKyung Yoon,MyungJoo Kang*

Main category: cs.LG

TLDR: 提出了一种基于CNN自编码器的分数注意力机制（CASA），用于解决多变量长期时间序列预测中的计算资源、时间复杂度和跨维度交互问题。


<details>
  <summary>Details</summary>
Motivation: 多变量长期时间序列预测在天气预测和交通分析等领域至关重要，但现有Transformer变体在计算资源和跨维度交互方面仍有局限。

Method: 引入CASA机制，可灵活应用于多种Transformer模型，通过减少内存占用提升性能。

Result: 在8个真实数据集上验证，CASA减少计算资源77.7%，推理速度提升44.0%，并在87.5%的指标中排名第一。

Conclusion: CASA在性能和效率上均达到最优，为多变量时间序列预测提供了高效解决方案。

Abstract: Multivariate long-term time series forecasting is critical for applications
such as weather prediction, and traffic analysis. In addition, the
implementation of Transformer variants has improved prediction accuracy.
Following these variants, different input data process approaches also enhanced
the field, such as tokenization techniques including point-wise, channel-wise,
and patch-wise tokenization. However, previous studies still have limitations
in time complexity, computational resources, and cross-dimensional
interactions. To address these limitations, we introduce a novel CNN
Autoencoder-based Score Attention mechanism (CASA), which can be introduced in
diverse Transformers model-agnosticically by reducing memory and leading to
improvement in model performance. Experiments on eight real-world datasets
validate that CASA decreases computational resources by up to 77.7%,
accelerates inference by 44.0%, and achieves state-of-the-art performance,
ranking first in 87.5% of evaluated metrics.

</details>

### [77] [Wide & Deep Learning for Node Classification](https://arxiv.org/abs/2505.02020)
*Yancheng Chen,Wenguo Yang,Zhipeng Jiang*

Main category: cs.LG

TLDR: 论文提出GCNIII框架，结合Wide & Deep架构，通过三种技术优化图卷积网络（GCNs）在节点分类任务中的性能，并探索大语言模型（LLMs）用于节点特征工程。


<details>
  <summary>Details</summary>
Motivation: 解决GCNs在异质性和表达能力上的问题，同时平衡过拟合与泛化能力。

Method: 提出GCNIII框架，结合Intersect memory、Initial residual和Identity mapping三种技术，并利用LLMs进行节点特征工程。

Result: GCNIII在半监督和全监督任务中有效平衡过拟合与泛化，提升跨域节点分类性能。

Conclusion: GCNIII框架通过结合Wide & Deep架构和LLMs，显著优化了GCNs的性能，适用于多种节点分类任务。

Abstract: Wide & Deep, a simple yet effective learning architecture for recommendation
systems developed by Google, has had a significant impact in both academia and
industry due to its combination of the memorization ability of generalized
linear models and the generalization ability of deep models. Graph
convolutional networks (GCNs) remain dominant in node classification tasks;
however, recent studies have highlighted issues such as heterophily and
expressiveness, which focus on graph structure while seemingly neglecting the
potential role of node features. In this paper, we propose a flexible framework
GCNIII, which leverages the Wide & Deep architecture and incorporates three
techniques: Intersect memory, Initial residual and Identity mapping. We provide
comprehensive empirical evidence showing that GCNIII can more effectively
balance the trade-off between over-fitting and over-generalization on various
semi- and full- supervised tasks. Additionally, we explore the use of large
language models (LLMs) for node feature engineering to enhance the performance
of GCNIII in cross-domain node classification tasks. Our implementation is
available at https://github.com/CYCUCAS/GCNIII.

</details>

### [78] [NbBench: Benchmarking Language Models for Comprehensive Nanobody Tasks](https://arxiv.org/abs/2505.02022)
*Yiming Zhang,Koji Tsuda*

Main category: cs.LG

TLDR: NbBench是首个全面的纳米抗体表示学习基准套件，涵盖8个生物任务，评估11种模型，发现抗体语言模型在抗原相关任务中表现优异，但回归任务仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 纳米抗体在治疗和诊断中具有独特优势，但其特定建模研究不足且缺乏统一基准。

Method: 引入NbBench基准套件，涵盖8个任务和9个数据集，评估11种代表性模型。

Result: 抗体语言模型在抗原相关任务中表现优异，但回归任务如热稳定性和亲和力仍具挑战性。

Conclusion: NbBench为纳米抗体建模提供了标准化评估基础，推动领域发展。

Abstract: Nanobodies, single-domain antibody fragments derived from camelid
heavy-chain-only antibodies, exhibit unique advantages such as compact size,
high stability, and strong binding affinity, making them valuable tools in
therapeutics and diagnostics. While recent advances in pretrained protein and
antibody language models (PPLMs and PALMs) have greatly enhanced biomolecular
understanding, nanobody-specific modeling remains underexplored and lacks a
unified benchmark. To address this gap, we introduce NbBench, the first
comprehensive benchmark suite for nanobody representation learning. Spanning
eight biologically meaningful tasks across nine curated datasets, NbBench
encompasses structure annotation, binding prediction, and developability
assessment. We systematically evaluate eleven representative models--including
general-purpose protein LMs, antibody-specific LMs, and nanobody-specific
LMs--in a frozen setting. Our analysis reveals that antibody language models
excel in antigen-related tasks, while performance on regression tasks such as
thermostability and affinity remains challenging across all models. Notably, no
single model consistently outperforms others across all tasks. By standardizing
datasets, task definitions, and evaluation protocols, NbBench offers a
reproducible foundation for assessing and advancing nanobody modeling.

</details>

### [79] [GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph In-Context Learning](https://arxiv.org/abs/2505.02027)
*Rui Lv,Zaixi Zhang,Kai Zhang,Qi Liu,Weibo Gao,Jiawei Liu,Jiaxia Yan,Linan Yue,Fangzhou Yao*

Main category: cs.LG

TLDR: 论文提出了一种多阶段自适应提示优化方法GraphPrompter，通过优化生成、选择和增强图提示的流程，显著提升了图模型的上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法随机选择子图或边作为提示，导致噪声和性能下降；同时，预训练与测试图之间的类别数量差异会进一步恶化上下文学习能力。

Method: GraphPrompter包含三个阶段：1) Prompt Generator通过重构层突出信息边；2) Prompt Selector动态选择样本；3) Prompt Augmenter增强泛化能力。

Result: 实验表明，GraphPrompter平均性能超过现有基线8%。

Conclusion: GraphPrompter有效提升了图模型的上下文学习能力，尤其在处理类别数量差异时表现优异。

Abstract: Graph In-Context Learning, with the ability to adapt pre-trained graph models
to novel and diverse downstream graphs without updating any parameters, has
gained much attention in the community. The key to graph in-context learning is
to perform downstream graphs conditioned on chosen prompt examples. Existing
methods randomly select subgraphs or edges as prompts, leading to noisy graph
prompts and inferior model performance. Additionally, due to the gap between
pre-training and testing graphs, when the number of classes in the testing
graphs is much greater than that in the training, the in-context learning
ability will also significantly deteriorate. To tackle the aforementioned
challenges, we develop a multi-stage adaptive prompt optimization method
GraphPrompter, which optimizes the entire process of generating, selecting, and
using graph prompts for better in-context learning capabilities. Firstly,
Prompt Generator introduces a reconstruction layer to highlight the most
informative edges and reduce irrelevant noise for graph prompt construction.
Furthermore, in the selection stage, Prompt Selector employs the $k$-nearest
neighbors algorithm and pre-trained selection layers to dynamically choose
appropriate samples and minimize the influence of irrelevant prompts. Finally,
we leverage a Prompt Augmenter with a cache replacement strategy to enhance the
generalization capability of the pre-trained model on new datasets. Extensive
experiments show that GraphPrompter effectively enhances the in-context
learning ability of graph models. On average across all the settings, our
approach surpasses the state-of-the-art baselines by over 8%. Our code is
released at https://github.com/karin0018/GraphPrompter.

</details>

### [80] [Quantum-Enhanced Classification of Brain Tumors Using DNA Microarray Gene Expression Profiles](https://arxiv.org/abs/2505.02033)
*Emine Akpinar,Batuhan Hangun,Murat Oduncuoglu,Oguz Altun,Onder Eyecioglu,Zeynel Yalcin*

Main category: cs.LG

TLDR: 论文提出了一种基于变分量子分类器的新型模型“Deep VQC”，用于高效分类脑肿瘤基因表达数据，并展示了量子AI在复杂数据分析中的潜力。


<details>
  <summary>Details</summary>
Motivation: DNA微阵列技术产生的高维基因数据难以用经典AI方法高效处理，量子计算因其并行处理能力成为潜在解决方案。

Method: 采用变分量子分类器（VQC）方法，开发了“Deep VQC”模型，用于分类四种脑肿瘤和健康样本。

Result: 模型在54,676个基因特征数据集上表现优异，分类准确率高，且性能优于或与经典ML算法相当。

Conclusion: 量子AI方法在复杂基因数据分析中具有显著潜力，为脑肿瘤分类提供了高效新途径。

Abstract: DNA microarray technology enables the simultaneous measurement of expression
levels of thousands of genes, thereby facilitating the understanding of the
molecular mechanisms underlying complex diseases such as brain tumors and the
identification of diagnostic genetic signatures. To derive meaningful
biological insights from the high-dimensional and complex gene features
obtained through this technology and to analyze gene properties in detail,
classical AI-based approaches such as machine learning and deep learning are
widely employed. However, these methods face various limitations in managing
high-dimensional vector spaces and modeling the intricate relationships among
genes. In particular, challenges such as hyperparameter tuning, computational
costs, and high processing power requirements can hinder their efficiency. To
overcome these limitations, quantum computing and quantum AI approaches are
gaining increasing attention. Leveraging quantum properties such as
superposition and entanglement, quantum methods enable more efficient parallel
processing of high-dimensional data and offer faster and more effective
solutions to problems that are computationally demanding for classical methods.
In this study, a novel model called "Deep VQC" is proposed, based on the
Variational Quantum Classifier approach. Developed using microarray data
containing 54,676 gene features, the model successfully classified four
different types of brain tumors-ependymoma, glioblastoma, medulloblastoma, and
pilocytic astrocytoma-alongside healthy samples with high accuracy.
Furthermore, compared to classical ML algorithms, our model demonstrated either
superior or comparable classification performance. These results highlight the
potential of quantum AI methods as an effective and promising approach for the
analysis and classification of complex structures such as brain tumors based on
gene expression features.

</details>

### [81] [Secrets of GFlowNets' Learning Behavior: A Theoretical Study](https://arxiv.org/abs/2505.02035)
*Tianshu Yu*

Main category: cs.LG

TLDR: 本文对生成流网络（GFlowNets）的学习行为进行了理论分析，聚焦于收敛性、样本复杂度、隐式正则化和鲁棒性四个维度，揭示了其学习机制的优势与局限。


<details>
  <summary>Details</summary>
Motivation: 尽管GFlowNets在生成复合结构方面表现出潜力，但其学习行为的理论理解仍不完善。本文旨在填补这一空白。

Method: 通过理论分析，研究了GFlowNets的收敛性、样本复杂度、隐式正则化和鲁棒性。

Result: 研究结果揭示了GFlowNets学习动态的机制，为其设计和部署提供了理论指导。

Conclusion: 本文不仅填补了GFlowNets理论研究的空白，还为其作为可靠且可解释的生成模型框架奠定了基础。

Abstract: Generative Flow Networks (GFlowNets) have emerged as a powerful paradigm for
generating composite structures, demonstrating considerable promise across
diverse applications. While substantial progress has been made in exploring
their modeling validity and connections to other generative frameworks, the
theoretical understanding of their learning behavior remains largely uncharted.
In this work, we present a rigorous theoretical investigation of GFlowNets'
learning behavior, focusing on four fundamental dimensions: convergence, sample
complexity, implicit regularization, and robustness. By analyzing these
aspects, we seek to elucidate the intricate mechanisms underlying GFlowNet's
learning dynamics, shedding light on its strengths and limitations. Our
findings contribute to a deeper understanding of the factors influencing
GFlowNet performance and provide insights into principled guidelines for their
effective design and deployment. This study not only bridges a critical gap in
the theoretical landscape of GFlowNets but also lays the foundation for their
evolution as a reliable and interpretable framework for generative modeling.
Through this, we aspire to advance the theoretical frontiers of GFlowNets and
catalyze their broader adoption in the AI community.

</details>

### [82] [Neural Logistic Bandits](https://arxiv.org/abs/2505.02069)
*Seoungbin Bae,Dabeen Lee*

Main category: cs.LG

TLDR: 论文研究了神经逻辑老虎机问题，提出了一种新的Bernstein型不等式，以减少对特征维度的直接依赖，并设计了两种算法，改进了现有结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在神经逻辑老虎机问题中要么对κ的依赖不理想，要么直接依赖于高维特征维度，导致性能受限。

Method: 提出了一种新的Bernstein型不等式，用于自归一化向量值鞅，以减少对特征维度的依赖，并设计了两种算法（NeuralLog-UCB-1和NeuralLog-UCB-2）。

Result: 理论分析表明，两种算法的遗憾上界分别为O~(d~√(κT))和O~(d~√(T/κ))，优于现有结果。数值实验验证了理论发现。

Conclusion: 论文通过新不等式和算法，有效降低了神经逻辑老虎机问题中对高维特征和κ的依赖，提升了性能。

Abstract: We study the problem of neural logistic bandits, where the main task is to
learn an unknown reward function within a logistic link function using a neural
network. Existing approaches either exhibit unfavorable dependencies on
$\kappa$, where $1/\kappa$ represents the minimum variance of reward
distributions, or suffer from direct dependence on the feature dimension $d$,
which can be huge in neural network-based settings. In this work, we introduce
a novel Bernstein-type inequality for self-normalized vector-valued martingales
that is designed to bypass a direct dependence on the ambient dimension. This
lets us deduce a regret upper bound that grows with the effective dimension
$\widetilde{d}$, not the feature dimension, while keeping a minimal dependence
on $\kappa$. Based on the concentration inequality, we propose two algorithms,
NeuralLog-UCB-1 and NeuralLog-UCB-2, that guarantee regret upper bounds of
order $\widetilde{O}(\widetilde{d}\sqrt{\kappa T})$ and
$\widetilde{O}(\widetilde{d}\sqrt{T/\kappa})$, respectively, improving on the
existing results. Lastly, we report numerical results on both synthetic and
real datasets to validate our theoretical findings.

</details>

### [83] [Lightweight Defense Against Adversarial Attacks in Time Series Classification](https://arxiv.org/abs/2505.02073)
*Yi Han*

Main category: cs.LG

TLDR: 本文提出了五种基于数据增强的时间序列分类（TSC）对抗防御方法，计算资源消耗低，部署简单。其中一种集成方法在防御性能和泛化能力上优于PGD对抗训练，且计算资源需求仅为后者的三分之一。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类（TSC）对抗防御研究较少，现有方法（如对抗训练）计算成本高，亟需高效且易部署的解决方案。

Method: 开发了五种基于数据增强的防御方法，并组合成两种方法，其中一种是集成所有技术的方案。

Result: 集成方法在防御性能和泛化能力上优于PGD对抗训练，计算资源需求仅为后者的三分之一。

Conclusion: 这些方法推动了TSC对抗防御的发展，并为未来与大规模预训练模型的结合提供了方向。

Abstract: As time series classification (TSC) gains prominence, ensuring robust TSC
models against adversarial attacks is crucial. While adversarial defense is
well-studied in Computer Vision (CV), the TSC field has primarily relied on
adversarial training (AT), which is computationally expensive. In this paper,
five data augmentation-based defense methods tailored for time series are
developed, with the most computationally intensive method among them increasing
the computational resources by only 14.07% compared to the original TSC model.
Moreover, the deployment process for these methods is straightforward. By
leveraging these advantages of our methods, we create two combined methods. One
of these methods is an ensemble of all the proposed techniques, which not only
provides better defense performance than PGD-based AT but also enhances the
generalization ability of TSC models. Moreover, the computational resources
required for our ensemble are less than one-third of those required for
PGD-based AT. These methods advance robust TSC in data mining. Furthermore, as
foundation models are increasingly explored for time series feature learning,
our work provides insights into integrating data augmentation-based adversarial
defense with large-scale pre-trained models in future research.

</details>

### [84] [Learning Local Causal World Models with State Space Models and Attention](https://arxiv.org/abs/2505.02074)
*Francesco Petri,Luigi Asprino,Aldo Gangemi*

Main category: cs.LG

TLDR: 论文探讨了状态空间模型（SSM）在因果发现中的潜力，证明其在简单环境中建模动态和因果关系的性能优于或等同于Transformer。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案未能学习环境的因果表示，限制了复杂任务的理解能力。研究旨在结合因果理论与神经世界建模，探索SSM的潜力。

Method: 比较SSM与Transformer在简单环境中建模动态和因果关系的性能。

Result: SSM在建模动态和因果学习方面表现优于或等同于Transformer。

Conclusion: SSM为结合因果意识的进一步研究奠定了基础。

Abstract: World modelling, i.e. building a representation of the rules that govern the
world so as to predict its evolution, is an essential ability for any agent
interacting with the physical world. Despite their impressive performance, many
solutions fail to learn a causal representation of the environment they are
trying to model, which would be necessary to gain a deep enough understanding
of the world to perform complex tasks. With this work, we aim to broaden the
research in the intersection of causality theory and neural world modelling by
assessing the potential for causal discovery of the State Space Model (SSM)
architecture, which has been shown to have several advantages over the
widespread Transformer. We show empirically that, compared to an equivalent
Transformer, a SSM can model the dynamics of a simple environment and learn a
causal model at the same time with equivalent or better performance, thus
paving the way for further experiments that lean into the strength of SSMs and
further enhance them with causal awareness.

</details>

### [85] [SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from Sparse and Noisy Demonstrations](https://arxiv.org/abs/2505.02094)
*Runyi Yu,Yinhuai Wang,Qihan Zhao,Hok Wai Tsui,Jingbo Wang,Ping Tan,Qifeng Chen*

Main category: cs.LG

TLDR: 论文提出两种数据增强技术（STG和STF）和自适应轨迹采样策略（ATS），以解决强化学习中的演示噪声和覆盖限制问题，显著提升了泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有交互演示数据存在噪声和覆盖不足的问题，导致轨迹稀疏且不连续，无法捕捉技能变体和过渡的全貌。

Method: 提出两种数据增强技术：缝合轨迹图（STG）和状态转移场（STF），并结合自适应轨迹采样策略（ATS）和历史编码机制。

Result: 在多种交互任务中，方法在收敛稳定性、泛化能力和恢复鲁棒性上显著优于现有技术。

Conclusion: 通过数据增强和动态课程生成，实现了超越参考演示的鲁棒技能学习。

Abstract: We address a fundamental challenge in Reinforcement Learning from Interaction
Demonstration (RLID): demonstration noise and coverage limitations. While
existing data collection approaches provide valuable interaction
demonstrations, they often yield sparse, disconnected, and noisy trajectories
that fail to capture the full spectrum of possible skill variations and
transitions. Our key insight is that despite noisy and sparse demonstrations,
there exist infinite physically feasible trajectories that naturally bridge
between demonstrated skills or emerge from their neighboring states, forming a
continuous space of possible skill variations and transitions. Building upon
this insight, we present two data augmentation techniques: a Stitched
Trajectory Graph (STG) that discovers potential transitions between
demonstration skills, and a State Transition Field (STF) that establishes
unique connections for arbitrary states within the demonstration neighborhood.
To enable effective RLID with augmented data, we develop an Adaptive Trajectory
Sampling (ATS) strategy for dynamic curriculum generation and a historical
encoding mechanism for memory-dependent skill learning. Our approach enables
robust skill acquisition that significantly generalizes beyond the reference
demonstrations. Extensive experiments across diverse interaction tasks
demonstrate substantial improvements over state-of-the-art methods in terms of
convergence stability, generalization capability, and recovery robustness.

</details>

### [86] [Deep Representation Learning for Electronic Design Automation](https://arxiv.org/abs/2505.02105)
*Pratik Shrestha,Saran Phatharodom,Alec Aversa,David Blankenship,Zhengfeng Wu,Ioannis Savidis*

Main category: cs.LG

TLDR: 该论文探讨了表示学习在电子设计自动化（EDA）中的应用，通过图像、网格和图形等技术提升电路设计的效率、准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决电路复杂性增加及严格的PPA（功耗、性能、面积）要求带来的挑战。

Method: 采用图像、网格和图形等自然表示方法，结合图像基础、图形基础和混合多模态技术。

Result: 在时序预测、布线分析和自动布局等任务中展示了改进效果。

Conclusion: 表示学习能够显著提升EDA流程的效率、准确性和可扩展性。

Abstract: Representation learning has become an effective technique utilized by
electronic design automation (EDA) algorithms, which leverage the natural
representation of workflow elements as images, grids, and graphs. By addressing
challenges related to the increasing complexity of circuits and stringent
power, performance, and area (PPA) requirements, representation learning
facilitates the automatic extraction of meaningful features from complex data
formats, including images, grids, and graphs. This paper examines the
application of representation learning in EDA, covering foundational concepts
and analyzing prior work and case studies on tasks that include timing
prediction, routability analysis, and automated placement. Key techniques,
including image-based methods, graph-based approaches, and hybrid multimodal
solutions, are presented to illustrate the improvements provided in routing,
timing, and parasitic prediction. The provided advancements demonstrate the
potential of representation learning to enhance efficiency, accuracy, and
scalability in current integrated circuit design flows.

</details>

### [87] [GRAIL: Graph Edit Distance and Node Alignment Using LLM-Generated Code](https://arxiv.org/abs/2505.02124)
*Samidha Verma,Arushi Goyal,Ananya Mathur,Ankit Anand,Sayan Ranu*

Main category: cs.LG

TLDR: GRAIL提出了一种新方法，利用大型语言模型和自动提示调优生成程序来计算图编辑距离（GED），解决了传统神经方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统神经方法计算GED需要大量真实数据且缺乏可解释性和跨域泛化能力。

Method: GRAIL结合大型语言模型和自动提示调优生成程序来计算GED。

Result: 在七个数据集上的实验表明，GRAIL在预测质量和跨域泛化能力上优于现有方法。

Conclusion: GRAIL通过生成程序而非预测GED，实现了更高的可解释性和泛化能力。

Abstract: Graph Edit Distance (GED) is a widely used metric for measuring similarity
between two graphs. Computing the optimal GED is NP-hard, leading to the
development of various neural and non-neural heuristics. While neural methods
have achieved improved approximation quality compared to non-neural approaches,
they face significant challenges: (1) They require large amounts of ground
truth data, which is itself NP-hard to compute. (2) They operate as black
boxes, offering limited interpretability. (3) They lack cross-domain
generalization, necessitating expensive retraining for each new dataset. We
address these limitations with GRAIL, introducing a paradigm shift in this
domain. Instead of training a neural model to predict GED, GRAIL employs a
novel combination of large language models (LLMs) and automated prompt tuning
to generate a program that is used to compute GED. This shift from predicting
GED to generating programs imparts various advantages, including end-to-end
interpretability and an autonomous self-evolutionary learning mechanism without
ground-truth supervision. Extensive experiments on seven datasets confirm that
GRAIL not only surpasses state-of-the-art GED approximation methods in
prediction quality but also achieves robust cross-domain generalization across
diverse graph distributions.

</details>

### [88] [Efficient Multivariate Time Series Forecasting via Calibrated Language Models with Privileged Knowledge Distillation](https://arxiv.org/abs/2505.02138)
*Chenxi Liu,Hao Miao,Qianxiong Xu,Shaowen Zhou,Cheng Long,Yan Zhao,Ziyue Li,Rui Zhao*

Main category: cs.LG

TLDR: TimeKD是一个高效的多变量时间序列预测框架，通过校准语言模型和特权知识蒸馏提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在推理阶段效率低下的问题，同时利用其知识提升预测质量。

Method: 采用校准语言模型和特权知识蒸馏，设计减性交叉注意力机制，提出相关性和特征蒸馏的PKD机制。

Result: 实验验证了TimeKD在有效性、效率和可扩展性方面的优势。

Conclusion: TimeKD通过结合语言模型和知识蒸馏，显著提升了多变量时间序列预测的性能和效率。

Abstract: Multivariate time series forecasting (MTSF) endeavors to predict future
observations given historical data, playing a crucial role in time series data
management systems. With advancements in large language models (LLMs), recent
studies employ textual prompt tuning to infuse the knowledge of LLMs into MTSF.
However, the deployment of LLMs often suffers from low efficiency during the
inference phase. To address this problem, we introduce TimeKD, an efficient
MTSF framework that leverages the calibrated language models and privileged
knowledge distillation. TimeKD aims to generate high-quality future
representations from the proposed cross-modality teacher model and cultivate an
effective student model. The cross-modality teacher model adopts calibrated
language models (CLMs) with ground truth prompts, motivated by the paradigm of
Learning Under Privileged Information (LUPI). In addition, we design a
subtractive cross attention (SCA) mechanism to refine these representations. To
cultivate an effective student model, we propose an innovative privileged
knowledge distillation (PKD) mechanism including correlation and feature
distillation. PKD enables the student to replicate the teacher's behavior while
minimizing their output discrepancy. Extensive experiments on real data offer
insight into the effectiveness, efficiency, and scalability of the proposed
TimeKD.

</details>

### [89] [Local Herb Identification Using Transfer Learning: A CNN-Powered Mobile Application for Nepalese Flora](https://arxiv.org/abs/2505.02147)
*Prajwal Thapa,Mridul Sharma,Jinu Nyachhyon,Yagya Raj Pandeya*

Main category: cs.LG

TLDR: 本研究提出了一种基于深度学习的草药分类方法，使用CNN和迁移学习技术，在尼泊尔丰富的生物多样性背景下对60种草药进行分类。


<details>
  <summary>Details</summary>
Motivation: 草药分类在植物学研究中具有重要意义，尤其是在生物多样性丰富的地区如尼泊尔。现有方法存在局限性，需要更高效的分类技术。

Method: 研究采用了多种深度学习架构（如DenseNet121、ResNet50、VGG16等），结合数据增强和正则化技术，使用12,000张草药图像数据集进行训练。

Result: DenseNet121表现最佳，模型通过数据增强和正则化提高了泛化能力。

Conclusion: 该研究推动了草药分类技术的发展，有助于传统植物知识的保护和草药的可持续利用。

Abstract: Herb classification presents a critical challenge in botanical research,
particularly in regions with rich biodiversity such as Nepal. This study
introduces a novel deep learning approach for classifying 60 different herb
species using Convolutional Neural Networks (CNNs) and transfer learning
techniques. Using a manually curated dataset of 12,000 herb images, we
developed a robust machine learning model that addresses existing limitations
in herb recognition methodologies. Our research employed multiple model
architectures, including DenseNet121, 50-layer Residual Network (ResNet50),
16-layer Visual Geometry Group Network (VGG16), InceptionV3, EfficientNetV2,
and Vision Transformer (VIT), with DenseNet121 ultimately demonstrating
superior performance. Data augmentation and regularization techniques were
applied to mitigate overfitting and enhance the generalizability of the model.
This work advances herb classification techniques, preserving traditional
botanical knowledge and promoting sustainable herb utilization.

</details>

### [90] [Efficient FPGA Implementation of Time-Domain Popcount for Low-Complexity Machine Learning](https://arxiv.org/abs/2505.02181)
*Shengyu Duan,Marcos L. L. Sartori,Rishad Shafik,Alex Yakovlev,Emre Ozer*

Main category: cs.LG

TLDR: 论文提出了一种基于时间域的创新方法，通过可编程延迟线和仲裁器优化Tsetlin Machine中的popcount和argmax操作，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: Tsetlin Machine（TM）是一种轻量级机器学习方法，但其性能受限于popcount和argmax操作的高计算成本。

Method: 采用时间域实现，利用可编程延迟线（PDLs）和仲裁器，通过延迟机制高效完成任务，并设计了FPGA实现流程。

Result: 与同步TM相比，异步TM的延迟降低38%，动态功耗减少43.1%，资源利用率节省15%。

Conclusion: 时间域方法显著提升了TM的性能和效率，为轻量级机器学习提供了优化方案。

Abstract: Population count (popcount) is a crucial operation for many low-complexity
machine learning (ML) algorithms, including Tsetlin Machine (TM)-a promising
new ML method, particularly well-suited for solving classification tasks. The
inference mechanism in TM consists of propositional logic-based structures
within each class, followed by a majority voting scheme, which makes the
classification decision. In TM, the voters are the outputs of Boolean clauses.
The voting mechanism comprises two operations: popcount for each class and
determining the class with the maximum vote by means of an argmax operation.
  While TMs offer a lightweight ML alternative, their performance is often
limited by the high computational cost of popcount and comparison required to
produce the argmax result. In this paper, we propose an innovative approach to
accelerate and optimize these operations by performing them in the time domain.
Our time-domain implementation uses programmable delay lines (PDLs) and
arbiters to efficiently manage these tasks through delay-based mechanisms. We
also present an FPGA design flow for practical implementation of the
time-domain popcount, addressing delay skew and ensuring that the behavior
matches that of the model's intended functionality. By leveraging the natural
compatibility of the proposed popcount with asynchronous architectures, we
demonstrate significant improvements in an asynchronous TM, including up to 38%
reduction in latency, 43.1% reduction in dynamic power, and 15% savings in
resource utilization, compared to synchronous TMs using adder-based popcount.

</details>

### [91] [DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units](https://arxiv.org/abs/2505.02206)
*Lei Mao,Yuanhe Tian,Yan Song*

Main category: cs.LG

TLDR: DNAZEN是一种增强的基因组表示框架，通过多粒度学习基因序列，包括小聚合物和G-grams，利用Transformer编码器和动态匹配提升表示效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接将语言建模技术应用于基因序列，未充分考虑其内在信息组织，尤其是不同粒度单位对表示的贡献。

Method: 提出DNAZEN框架，通过无监督方法构建G-gram词汇表，使用Transformer编码器动态匹配G-grams，并提出全G-gram掩码训练机制。

Result: 在基准数据集上的实验表明，DNAZEN在多种下游任务中表现优异。

Conclusion: DNAZEN通过多粒度学习和动态匹配，显著提升了基因序列的表示效果。

Abstract: Genome modeling conventionally treats gene sequence as a language, reflecting
its structured motifs and long-range dependencies analogous to linguistic units
and organization principles such as words and syntax. Recent studies utilize
advanced neural networks, ranging from convolutional and recurrent models to
Transformer-based models, to capture contextual information of gene sequence,
with the primary goal of obtaining effective gene sequence representations and
thus enhance the models' understanding of various running gene samples.
However, these approaches often directly apply language modeling techniques to
gene sequences and do not fully consider the intrinsic information organization
in them, where they do not consider how units at different granularities
contribute to representation. In this paper, we propose DNAZEN, an enhanced
genomic representation framework designed to learn from various granularities
in gene sequences, including small polymers and G-grams that are combinations
of several contiguous polymers. Specifically, we extract the G-grams from
large-scale genomic corpora through an unsupervised approach to construct the
G-gram vocabulary, which is used to provide G-grams in the learning process of
DNA sequences through dynamically matching from running gene samples. A
Transformer-based G-gram encoder is also proposed and the matched G-grams are
fed into it to compute their representations and integrated into the encoder
for basic unit (E4BU), which is responsible for encoding small units and
maintaining the learning and inference process. To further enhance the learning
process, we propose whole G-gram masking to train DNAZEN, where the model
largely favors the selection of each entire G-gram to mask rather than an
ordinary masking mechanism performed on basic units. Experiments on benchmark
datasets demonstrate the effectiveness of DNAZEN on various downstream tasks.

</details>

### [92] [Exogenous Isomorphism for Counterfactual Identifiability](https://arxiv.org/abs/2505.02212)
*Yikang Chen,Dehui Du*

Main category: cs.LG

TLDR: N/A


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper investigates $\sim_{\mathcal{L}_3}$-identifiability, a form of
complete counterfactual identifiability within the Pearl Causal Hierarchy (PCH)
framework, ensuring that all Structural Causal Models (SCMs) satisfying the
given assumptions provide consistent answers to all causal questions. To
simplify this problem, we introduce exogenous isomorphism and propose
$\sim_{\mathrm{EI}}$-identifiability, reflecting the strength of model
identifiability required for $\sim_{\mathcal{L}_3}$-identifiability. We explore
sufficient assumptions for achieving $\sim_{\mathrm{EI}}$-identifiability in
two special classes of SCMs: Bijective SCMs (BSCMs), based on counterfactual
transport, and Triangular Monotonic SCMs (TM-SCMs), which extend
$\sim_{\mathcal{L}_2}$-identifiability. Our results unify and generalize
existing theories, providing theoretical guarantees for practical applications.
Finally, we leverage neural TM-SCMs to address the consistency problem in
counterfactual reasoning, with experiments validating both the effectiveness of
our method and the correctness of the theory.

</details>

### [93] [An Empirical Study of Qwen3 Quantization](https://arxiv.org/abs/2505.02214)
*Xingyu Zheng,Yuye Li,Haoran Chu,Yue Feng,Xudong Ma,Jie Luo,Jinyang Guo,Haotong Qin,Michele Magno,Xianglong Liu*

Main category: cs.LG

TLDR: Qwen3作为开源大语言模型系列的最新成员，在自然语言理解任务中表现出色。本文系统评估了Qwen3在不同量化设置下的性能，发现中等比特宽度下表现良好，但超低精度下性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 研究Qwen3在资源受限环境中的高效部署，探索低比特量化对其性能的影响。

Method: 评估5种经典后训练量化技术，比特宽度从1到8位，并在多个数据集上测试效果。

Result: Qwen3在中等比特宽度下保持竞争力，但在超低精度下性能显著下降。

Conclusion: 需进一步研究以减少极端量化场景下的性能损失，为Qwen3及未来大语言模型的量化方法提供实用指导。

Abstract: The Qwen series has emerged as a leading family of open-source Large Language
Models (LLMs), demonstrating remarkable capabilities in natural language
understanding tasks. With the recent release of Qwen3, which exhibits superior
performance across diverse benchmarks, there is growing interest in deploying
these models efficiently in resource-constrained environments. Low-bit
quantization presents a promising solution, yet its impact on Qwen3's
performance remains underexplored. This study conducts a systematic evaluation
of Qwen3's robustness under various quantization settings, aiming to uncover
both opportunities and challenges in compressing this state-of-the-art model.
We rigorously assess 5 existing classic post-training quantization techniques
applied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their
effectiveness across multiple datasets. Our findings reveal that while Qwen3
maintains competitive performance at moderate bit-widths, it experiences
notable degradation in linguistic tasks under ultra-low precision, underscoring
the persistent hurdles in LLM compression. These results emphasize the need for
further research to mitigate performance loss in extreme quantization
scenarios. We anticipate that this empirical analysis will provide actionable
insights for advancing quantization methods tailored to Qwen3 and future LLMs,
ultimately enhancing their practicality without compromising accuracy. Our
project is released on https://github.com/Efficient-ML/Qwen3-Quantization and
https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.

</details>

### [94] [Practical Efficiency of Muon for Pretraining](https://arxiv.org/abs/2505.02222)
*Essential AI,:,Ishaan Shah,Anthony M. Polloreno,Karl Stratos,Philip Monk,Adarsh Chaluvaraju,Andrew Hojel,Andrew Ma,Anil Thomas,Ashish Tanwer,Darsh J Shah,Khoi Nguyen,Kurt Smith,Michael Callahan,Michael Pust,Mohit Parmar,Peter Rushton,Platon Mazarakis,Ritvik Kapila,Saurabh Srivastava,Somanshu Singla,Tim Romanski,Yash Vanjani,Ashish Vaswani*

Main category: cs.LG

TLDR: Muon是一种二阶优化器，在计算时间权衡上优于AdamW，能在大批量训练中保持数据效率，且计算高效。结合muP参数化，提出了一种简单的伸缩算法，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究Muon优化器在大批量训练中的表现，探索其在计算效率和数据效率上的优势，并结合muP参数化提升超参数迁移效率。

Method: 提出Muon优化器，结合muP参数化，并设计伸缩算法以处理误差。通过实验验证，模型规模达40亿参数。

Result: Muon在大批量训练中优于AdamW，保持数据效率且计算高效。伸缩算法有效处理muP误差，资源开销小。

Conclusion: Muon是一种高效且经济的优化器，适用于大规模模型训练，结合muP参数化可进一步提升效率。

Abstract: We demonstrate that Muon, the simplest instantiation of a second-order
optimizer, explicitly expands the Pareto frontier over AdamW on the
compute-time tradeoff. We find that Muon is more effective than AdamW in
retaining data efficiency at large batch sizes, far beyond the so-called
critical batch size, while remaining computationally efficient, thus enabling
more economical training. We study the combination of Muon and the maximal
update parameterization (muP) for efficient hyperparameter transfer and present
a simple telescoping algorithm that accounts for all sources of error in muP
while introducing only a modest overhead in resources. We validate our findings
through extensive experiments with model sizes up to four billion parameters
and ablations on the data distribution and architecture.

</details>

### [95] [Coupled Distributional Random Expert Distillation for World Model Online Imitation Learning](https://arxiv.org/abs/2505.02228)
*Shangzhe Li,Zhiao Huang,Hao Su*

Main category: cs.LG

TLDR: 提出了一种基于随机网络蒸馏（RND）的在线模仿学习方法，解决了现有方法在对抗性奖励或价值公式中的不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法在对抗性奖励或价值公式中表现不稳定，需要改进。

Method: 使用RND构建奖励模型，联合估计专家和行为分布在世界模型的潜在空间中。

Result: 在DMControl、Meta-World和ManiSkill2等基准测试中表现稳定，达到专家水平。

Conclusion: 该方法在稳定性和性能上优于对抗性方法。

Abstract: Imitation Learning (IL) has achieved remarkable success across various
domains, including robotics, autonomous driving, and healthcare, by enabling
agents to learn complex behaviors from expert demonstrations. However, existing
IL methods often face instability challenges, particularly when relying on
adversarial reward or value formulations in world model frameworks. In this
work, we propose a novel approach to online imitation learning that addresses
these limitations through a reward model based on random network distillation
(RND) for density estimation. Our reward model is built on the joint estimation
of expert and behavioral distributions within the latent space of the world
model. We evaluate our method across diverse benchmarks, including DMControl,
Meta-World, and ManiSkill2, showcasing its ability to deliver stable
performance and achieve expert-level results in both locomotion and
manipulation tasks. Our approach demonstrates improved stability over
adversarial methods while maintaining expert-level performance.

</details>

### [96] [Federated Causal Inference in Healthcare: Methods, Challenges, and Applications](https://arxiv.org/abs/2505.02238)
*Haoyang Li,Jie Xu,Kyra Gan,Fei Wang,Chengxi Zang*

Main category: cs.LG

TLDR: 本文综述了联邦因果推断在多站点治疗效果估计中的应用，分析了数据异质性带来的挑战，并提出了权重和优化两类方法，探讨了扩展模型及时间事件结果的处理。


<details>
  <summary>Details</summary>
Motivation: 解决多站点数据共享中的隐私问题，同时应对数据异质性对因果效应估计的挑战。

Method: 分类权重和优化方法，扩展个性化模型、点对点通信和模型分解，分析时间事件结果的联邦模型。

Result: 发现FedProx正则化在偏差-方差权衡上优于朴素平均和元分析。

Conclusion: 提出未来研究方向，以实现可扩展、公平和可信的联邦因果推断。

Abstract: Federated causal inference enables multi-site treatment effect estimation
without sharing individual-level data, offering a privacy-preserving solution
for real-world evidence generation. However, data heterogeneity across sites,
manifested in differences in covariate, treatment, and outcome, poses
significant challenges for unbiased and efficient estimation. In this paper, we
present a comprehensive review and theoretical analysis of federated causal
effect estimation across both binary/continuous and time-to-event outcomes. We
classify existing methods into weight-based strategies and optimization-based
frameworks and further discuss extensions including personalized models,
peer-to-peer communication, and model decomposition. For time-to-event
outcomes, we examine federated Cox and Aalen-Johansen models, deriving
asymptotic bias and variance under heterogeneity. Our analysis reveals that
FedProx-style regularization achieves near-optimal bias-variance trade-offs
compared to naive averaging and meta-analysis. We review related software tools
and conclude by outlining opportunities, challenges, and future directions for
scalable, fair, and trustworthy federated causal inference in distributed
healthcare systems.

</details>

### [97] [RISE: Radius of Influence based Subgraph Extraction for 3D Molecular Graph Explanation](https://arxiv.org/abs/2505.02247)
*Jingxiang Qu,Wenhan Gao,Jiaxing Zhang,Xufeng Liu,Hua Wei,Haibin Ling,Yi Liu*

Main category: cs.LG

TLDR: 本文提出了一种针对3D GNN的新型解释方法，通过为每个节点分配影响半径，增强模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 3D GNN在分子数据建模中表现出色，但缺乏可解释性，限制了其在科学应用中的可靠性。

Method: 设计了一种解释方法，将解释范围限制在节点周围的局部区域内，利用3D图的空间和几何特性。

Result: 该方法不仅提高了可解释性，还符合3D图应用（如分子学习）的物理和结构依赖性。

Conclusion: 提出的方法为3D GNN提供了一种有效的解释工具，解决了现有方法在3D环境中的局限性。

Abstract: 3D Geometric Graph Neural Networks (GNNs) have emerged as transformative
tools for modeling molecular data. Despite their predictive power, these models
often suffer from limited interpretability, raising concerns for scientific
applications that require reliable and transparent insights. While existing
methods have primarily focused on explaining molecular substructures in 2D
GNNs, the transition to 3D GNNs introduces unique challenges, such as handling
the implicit dense edge structures created by a cut-off radius. To tackle this,
we introduce a novel explanation method specifically designed for 3D GNNs,
which localizes the explanation to the immediate neighborhood of each node
within the 3D space. Each node is assigned an radius of influence, defining the
localized region within which message passing captures spatial and structural
interactions crucial for the model's predictions. This method leverages the
spatial and geometric characteristics inherent in 3D graphs. By constraining
the subgraph to a localized radius of influence, the approach not only enhances
interpretability but also aligns with the physical and structural dependencies
typical of 3D graph applications, such as molecular learning.

</details>

### [98] [Epistemic Wrapping for Uncertainty Quantification](https://arxiv.org/abs/2505.02277)
*Maryam Sultana,Neil Yorke-Smith,Kaizheng Wang,Shireen Kudukkil Manchingal,Muhammad Mubashar,Fabio Cuzzolin*

Main category: cs.LG

TLDR: 提出了一种名为“Epistemic Wrapping”的新方法，用于改进分类任务中的不确定性估计，显著提升了模型的泛化能力和不确定性量化效果。


<details>
  <summary>Details</summary>
Motivation: 不确定性估计在机器学习中至关重要，尤其是在分类任务中，可以提高模型的鲁棒性和可靠性。

Method: 以贝叶斯神经网络（BNN）为基础，将其输出转化为置信函数后验，有效捕捉认知不确定性，并提供了一种高效且通用的不确定性量化方法。

Result: 在MNIST、Fashion-MNIST、CIFAR-10和CIFAR-100数据集上的实验表明，该方法显著提升了泛化能力和不确定性量化效果。

Conclusion: Epistemic Wrapping方法为不确定性估计提供了一种高效且通用的解决方案，显著提升了模型性能。

Abstract: Uncertainty estimation is pivotal in machine learning, especially for
classification tasks, as it improves the robustness and reliability of models.
We introduce a novel `Epistemic Wrapping' methodology aimed at improving
uncertainty estimation in classification. Our approach uses Bayesian Neural
Networks (BNNs) as a baseline and transforms their outputs into belief function
posteriors, effectively capturing epistemic uncertainty and offering an
efficient and general methodology for uncertainty quantification. Comprehensive
experiments employing a Bayesian Neural Network (BNN) baseline and an Interval
Neural Network for inference on the MNIST, Fashion-MNIST, CIFAR-10 and
CIFAR-100 datasets demonstrate that our Epistemic Wrapper significantly
enhances generalisation and uncertainty quantification.

</details>

### [99] [Universal Approximation Theorem of Deep Q-Networks](https://arxiv.org/abs/2505.02288)
*Qian Qi*

Main category: cs.LG

TLDR: 该论文提出了一个连续时间框架，通过随机控制和前向-后向随机微分方程（FBSDEs）分析深度Q网络（DQNs），并证明了DQN在紧凑集上可以高概率近似最优Q函数。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于将深度强化学习与随机控制理论结合，为连续时间环境下的DQN提供理论支持，适用于物理系统或高频数据应用。

Method: 方法包括利用剩余网络逼近定理和状态-动作过程的大偏差界限，分析DQN的逼近性质，并基于随机逼近定理分析Q学习算法的收敛性。

Result: 结果表明，DQN可以在紧凑集上高概率地以任意精度逼近最优Q函数，同时分析了DQN层数、时间离散化与粘性解之间的相互作用。

Conclusion: 该研究为连续时间环境下的DQN提供了理论框架，揭示了其在物理系统或高频数据应用中的潜力。

Abstract: We establish a continuous-time framework for analyzing Deep Q-Networks (DQNs)
via stochastic control and Forward-Backward Stochastic Differential Equations
(FBSDEs). Considering a continuous-time Markov Decision Process (MDP) driven by
a square-integrable martingale, we analyze DQN approximation properties. We
show that DQNs can approximate the optimal Q-function on compact sets with
arbitrary accuracy and high probability, leveraging residual network
approximation theorems and large deviation bounds for the state-action process.
We then analyze the convergence of a general Q-learning algorithm for training
DQNs in this setting, adapting stochastic approximation theorems. Our analysis
emphasizes the interplay between DQN layer count, time discretization, and the
role of viscosity solutions (primarily for the value function $V^*$) in
addressing potential non-smoothness of the optimal Q-function. This work
bridges deep reinforcement learning and stochastic control, offering insights
into DQNs in continuous-time settings, relevant for applications with physical
systems or high-frequency data.

</details>

### [100] [Entropy-Guided Sampling of Flat Modes in Discrete Spaces](https://arxiv.org/abs/2505.02296)
*Pinaki Mohanty,Riddhiman Bhattacharya,Ruqi Zhang*

Main category: cs.LG

TLDR: 论文提出了一种名为EDLP的新方法，用于在离散空间中高效采样平坦模式，解决了现有方法难以捕捉平坦模式的问题。


<details>
  <summary>Details</summary>
Motivation: 平坦模式在组合优化和离散生成建模中具有广泛应用，但现有采样算法往往忽视模式体积，难以有效捕捉平坦模式。

Method: 提出了EDLP方法，通过引入局部熵和连续辅助变量，在联合分布下引导离散采样器朝向平坦模式。

Result: EDLP在局部对数凹离散分布中具有非渐近收敛保证，并在多个任务中优于传统方法。

Conclusion: EDLP是一种高效且理论支持的方法，适用于需要从平坦模式采样的任务。

Abstract: Sampling from flat modes in discrete spaces is a crucial yet underexplored
problem. Flat modes represent robust solutions and have broad applications in
combinatorial optimization and discrete generative modeling. However, existing
sampling algorithms often overlook the mode volume and struggle to capture flat
modes effectively. To address this limitation, we propose \emph{Entropic
Discrete Langevin Proposal} (EDLP), which incorporates local entropy into the
sampling process through a continuous auxiliary variable under a joint
distribution. The local entropy term guides the discrete sampler toward flat
modes with a small overhead. We provide non-asymptotic convergence guarantees
for EDLP in locally log-concave discrete distributions. Empirically, our method
consistently outperforms traditional approaches across tasks that require
sampling from flat basins, including Bernoulli distribution, restricted
Boltzmann machines, combinatorial optimization, and binary neural networks.

</details>

### [101] [Adaptive Scoring and Thresholding with Human Feedback for Robust Out-of-Distribution Detection](https://arxiv.org/abs/2505.02299)
*Daisuke Yamada,Harit Vishwakarma,Ramya Korlakai Vinayak*

Main category: cs.LG

TLDR: 提出了一种人机交互框架，动态更新评分函数和阈值，以解决OOD输入的高FPR问题，同时保持TPR。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅基于ID数据设置阈值，导致高FPR且无法适应新OOD输入。

Method: 提出人机交互框架，实时更新评分函数和阈值，严格控制FPR并最大化TPR。

Result: 在OpenOOD基准测试中表现优于现有方法，实现更高TPR并保持FPR控制。

Conclusion: 该方法有效解决了OOD输入的高FPR问题，并具备动态适应能力。

Abstract: Machine Learning (ML) models are trained on in-distribution (ID) data but
often encounter out-of-distribution (OOD) inputs during deployment -- posing
serious risks in safety-critical domains. Recent works have focused on
designing scoring functions to quantify OOD uncertainty, with score thresholds
typically set based solely on ID data to achieve a target true positive rate
(TPR), since OOD data is limited before deployment. However, these TPR-based
thresholds leave false positive rates (FPR) uncontrolled, often resulting in
high FPRs where OOD points are misclassified as ID. Moreover, fixed scoring
functions and thresholds lack the adaptivity needed to handle newly observed,
evolving OOD inputs, leading to sub-optimal performance. To address these
challenges, we propose a human-in-the-loop framework that \emph{safely updates
both scoring functions and thresholds on the fly} based on real-world OOD
inputs. Our method maximizes TPR while strictly controlling FPR at all times,
even as the system adapts over time. We provide theoretical guarantees for FPR
control under stationary conditions and present extensive empirical evaluations
on OpenOOD benchmarks to demonstrate that our approach outperforms existing
methods by achieving higher TPRs while maintaining FPR control.

</details>

### [102] [Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques](https://arxiv.org/abs/2505.02309)
*Sanjay Surendranath Girija,Shashank Kapoor,Lakshit Arora,Dipen Pradhan,Aman Raj,Ankit Shetgaonkar*

Main category: cs.LG

TLDR: 本文综述了压缩大型语言模型（LLMs）的技术，以在资源受限环境中实现高效推理，重点介绍了知识蒸馏、模型量化和模型剪枝三种方法。


<details>
  <summary>Details</summary>
Motivation: LLMs资源需求高，限制了其在移动和边缘设备上的部署，因此需要压缩技术以优化其效率。

Method: 探讨了知识蒸馏、模型量化和模型剪枝三种主要方法，并辅以混合专家和早期退出策略。

Result: 提供了每种技术的原理、变体和成功应用案例，总结了现有方法的优缺点。

Conclusion: 指出了未来研究方向，为优化LLMs在边缘部署提供了有价值的资源。

Abstract: Large Language Models (LLMs) have revolutionized many areas of artificial
intelligence (AI), but their substantial resource requirements limit their
deployment on mobile and edge devices. This survey paper provides a
comprehensive overview of techniques for compressing LLMs to enable efficient
inference in resource-constrained environments. We examine three primary
approaches: Knowledge Distillation, Model Quantization, and Model Pruning. For
each technique, we discuss the underlying principles, present different
variants, and provide examples of successful applications. We also briefly
discuss complementary techniques such as mixture-of-experts and early-exit
strategies. Finally, we highlight promising future directions, aiming to
provide a valuable resource for both researchers and practitioners seeking to
optimize LLMs for edge deployment.

</details>

### [103] [Enabling Local Neural Operators to perform Equation-Free System-Level Analysis](https://arxiv.org/abs/2505.02308)
*Gianluca Fabiani,Hannes Vandecasteele,Somdatta Goswami,Constantinos Siettos,Ioannis G. Kevrekidis*

Main category: cs.LG

TLDR: 本文提出了一种将神经算子（NOs）与Krylov子空间迭代数值方法结合的框架，用于大规模动力系统的稳定性和分岔分析，填补了NOs在系统级任务中的空白。


<details>
  <summary>Details</summary>
Motivation: 神经算子（NOs）在模拟物理定律方面表现出色，但此前主要局限于时间动态预测，未充分探索其在系统级任务（如稳定性分析）中的潜力。本文旨在填补这一空白。

Method: 结合局部NOs与Krylov子空间迭代数值方法，提出一个框架，支持固定点、稳定性和分岔分析，并展示了局部空间和时空NOs的应用。

Result: 通过三个非线性PDE基准测试（Allen-Cahn方程、Liouville-Bratu-Gelfand PDE和FitzHugh-Nagumo模型）验证了框架的有效性。

Conclusion: 该框架为大规模动力系统的系统级分析提供了高效工具，扩展了NOs的应用范围。

Abstract: Neural Operators (NOs) provide a powerful framework for computations
involving physical laws that can be modelled by (integro-) partial differential
equations (PDEs), directly learning maps between infinite-dimensional function
spaces that bypass both the explicit equation identification and their
subsequent numerical solving. Still, NOs have so far primarily been employed to
explore the dynamical behavior as surrogates of brute-force temporal
simulations/predictions. Their potential for systematic rigorous numerical
system-level tasks, such as fixed-point, stability, and bifurcation analysis -
crucial for predicting irreversible transitions in real-world phenomena -
remains largely unexplored. Toward this aim, inspired by the Equation-Free
multiscale framework, we propose and implement a framework that integrates
(local) NOs with advanced iterative numerical methods in the Krylov subspace,
so as to perform efficient system-level stability and bifurcation analysis of
large-scale dynamical systems. Beyond fixed point, stability, and bifurcation
analysis enabled by local in time NOs, we also demonstrate the usefulness of
local in space as well as in space-time ("patch") NOs in accelerating the
computer-aided analysis of spatiotemporal dynamics. We illustrate our framework
via three nonlinear PDE benchmarks: the 1D Allen-Cahn equation, which undergoes
multiple concatenated pitchfork bifurcations; the Liouville-Bratu-Gelfand PDE,
which features a saddle-node tipping point; and the FitzHugh-Nagumo (FHN)
model, consisting of two coupled PDEs that exhibit both Hopf and saddle-node
bifurcations.

</details>

### [104] [Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL](https://arxiv.org/abs/2505.02391)
*Jiarui Yao,Yifan Hao,Hanning Zhang,Hanze Dong,Wei Xiong,Nan Jiang,Tong Zhang*

Main category: cs.LG

TLDR: GVM-RAFT提出了一种动态样本分配策略，通过监控提示接受率和随机梯度范数，优化计算资源分配，显著提升了CoT推理的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在CoT推理中采用统一的推理预算，未能考虑不同提示的难度和收敛行为差异，导致随机梯度估计效率低下。

Method: 提出GVM-RAFT，动态分配计算资源以最小化随机梯度方差，并通过理论分析验证其加速收敛的保证。

Result: 在数学推理任务中，GVM-RAFT比vanilla RAFT提速2-4倍，并显著提升准确性。

Conclusion: GVM-RAFT的动态采样策略具有通用性，可应用于其他强化学习算法，如GRPO，带来类似的收敛和准确性改进。

Abstract: Chain-of-thought (CoT) reasoning in large language models (LLMs) can be
formalized as a latent variable problem, where the model needs to generate
intermediate reasoning steps. While prior approaches such as iterative
reward-ranked fine-tuning (RAFT) have relied on such formulations, they
typically apply uniform inference budgets across prompts, which fails to
account for variability in difficulty and convergence behavior. This work
identifies the main bottleneck in CoT training as inefficient stochastic
gradient estimation due to static sampling strategies. We propose GVM-RAFT, a
prompt-specific Dynamic Sample Allocation Strategy designed to minimize
stochastic gradient variance under a computational budget constraint. The
method dynamically allocates computational resources by monitoring prompt
acceptance rates and stochastic gradient norms, ensuring that the resulting
gradient variance is minimized. Our theoretical analysis shows that the
proposed dynamic sampling strategy leads to accelerated convergence guarantees
under suitable conditions. Experiments on mathematical reasoning show that
GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over
vanilla RAFT. The proposed dynamic sampling strategy is general and can be
incorporated into other reinforcement learning algorithms, such as GRPO,
leading to similar improvements in convergence and test accuracy. Our code is
available at https://github.com/RLHFlow/GVM.

</details>

### [105] [Catastrophic Overfitting, Entropy Gap and Participation Ratio: A Noiseless $l^p$ Norm Solution for Fast Adversarial Training](https://arxiv.org/abs/2505.02360)
*Fares B. Mehouachi,Saif Eddin Jabari*

Main category: cs.LG

TLDR: 论文提出了一种通过控制$l^p$训练范数来缓解对抗训练中Catastrophic Overfitting（CO）问题的新方法，无需依赖噪声注入或正则化。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如FGSM）在对抗训练中容易发生CO，导致模型对单步攻击鲁棒但对多步攻击失效。研究发现CO在$l^{\infty}$范数下更常见，因此探索了$l^p$范数的作用。

Method: 通过将$l^p$攻击建模为固定点问题，开发了$l^p$-FGSM攻击，并量化梯度集中度（通过Participation Ratio和熵），提出自适应$l^p$-FGSM方法。

Result: 实验表明，该方法在不依赖额外正则化或噪声的情况下实现了强鲁棒性。

Conclusion: 该方法为缓解CO问题提供了理论支持的新途径。

Abstract: Adversarial training is a cornerstone of robust deep learning, but fast
methods like the Fast Gradient Sign Method (FGSM) often suffer from
Catastrophic Overfitting (CO), where models become robust to single-step
attacks but fail against multi-step variants. While existing solutions rely on
noise injection, regularization, or gradient clipping, we propose a novel
solution that purely controls the $l^p$ training norm to mitigate CO.
  Our study is motivated by the empirical observation that CO is more prevalent
under the $l^{\infty}$ norm than the $l^2$ norm. Leveraging this insight, we
develop a framework for generalized $l^p$ attack as a fixed point problem and
craft $l^p$-FGSM attacks to understand the transition mechanics from $l^2$ to
$l^{\infty}$. This leads to our core insight: CO emerges when highly
concentrated gradients where information localizes in few dimensions interact
with aggressive norm constraints. By quantifying gradient concentration through
Participation Ratio and entropy measures, we develop an adaptive $l^p$-FGSM
that automatically tunes the training norm based on gradient information.
Extensive experiments demonstrate that this approach achieves strong robustness
without requiring additional regularization or noise injection, providing a
novel and theoretically-principled pathway to mitigate the CO problem.

</details>

### [106] [Bielik v3 Small: Technical Report](https://arxiv.org/abs/2505.02550)
*Krzysztof Ociepa,Łukasz Flis,Remigiusz Kinas,Krzysztof Wróbel,Adrian Gwoździej*

Main category: cs.LG

TLDR: Bielik v3是一系列针对波兰语优化的高效生成文本模型（1.5B和4.5B参数），通过创新技术实现与更大模型相当的性能，同时减少计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 为波兰语提供高效且高性能的语言模型，解决资源受限场景下的需求。

Method: 采用定制波兰语分词器（APT4）、加权指令交叉熵损失和自适应学习率等技术。

Result: 在多个基准测试中表现优异，4.5B模型性能媲美更大模型，1.5B模型在紧凑体积下表现强劲。

Conclusion: 为资源受限的波兰语AI应用设定了新标准。

Abstract: We introduce Bielik v3, a series of parameter-efficient generative text
models (1.5B and 4.5B) optimized for Polish language processing. These models
demonstrate that smaller, well-optimized architectures can achieve performance
comparable to much larger counterparts while requiring substantially fewer
computational resources. Our approach incorporates several key innovations: a
custom Polish tokenizer (APT4) that significantly improves token efficiency,
Weighted Instruction Cross-Entropy Loss to balance learning across instruction
types, and Adaptive Learning Rate that dynamically adjusts based on training
progress. Trained on a meticulously curated corpus of 292 billion tokens
spanning 303 million documents, these models excel across multiple benchmarks,
including the Open PL LLM Leaderboard, Complex Polish Text Understanding
Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter
model achieves results competitive with models 2-3 times its size, while the
1.5B model delivers strong performance despite its extremely compact profile.
These advances establish new benchmarks for parameter-efficient language
modeling in less-represented languages, making high-quality Polish language AI
more accessible for resource-constrained applications.

</details>

### [107] [Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks](https://arxiv.org/abs/2505.02369)
*Juyoung Yun*

Main category: cs.LG

TLDR: ZSharp通过层间Z-score归一化和百分位过滤改进SAM，提升深度神经网络的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络倾向于收敛到尖锐最小值，影响鲁棒性，SAM方法虽能缓解但扰动参数时包含统计不显著方向。

Method: ZSharp在SAM基础上引入层间Z-score归一化和百分位过滤，仅保留显著梯度分量。

Result: 在CIFAR-10等数据集上，ZSharp在测试准确率上优于SAM及其变体，尤其在深层和Transformer模型中表现突出。

Conclusion: ZSharp是一种轻量且有效的改进，无需架构调整即可提升泛化能力。

Abstract: Generalizing well in deep neural networks remains a core challenge,
particularly due to their tendency to converge to sharp minima that degrade
robustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking
flatter minima but perturbs parameters using the full gradient, which can
include statistically insignificant directions. We propose ZSharp, a simple yet
effective extension to SAM that applies layer-wise Z-score normalization
followed by percentile-based filtering to retain only statistically significant
gradient components. This selective perturbation aligns updates with
curvature-sensitive directions, enhancing generalization without requiring
architectural changes. ZSharp introduces only one additional hyperparameter,
the percentile threshold, and remains fully compatible with existing SAM
variants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet,
VGG, and Vision Transformers show that ZSharp consistently outperforms SAM and
its variants in test accuracy, particularly on deeper and transformer-based
models. These results demonstrate that ZSharp is a principled and lightweight
improvement for sharpness-aware optimization.

</details>

### [108] [Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning](https://arxiv.org/abs/2505.02639)
*Xuan Lin,Qingrui Liu,Hongxin Xiang,Daojian Zeng,Xiangxiang Zeng*

Main category: cs.LG

TLDR: ChemDual是一个新型LLM框架，通过构建大规模指令数据集和双任务学习策略，优化化学反应和逆合成预测，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM在化学反应和逆合成预测中缺乏大规模数据集和忽略任务相关性的问题。

Method: 将反应和逆合成视为重组和碎片化过程，构建440万指令数据集，并采用多尺度分词器和双任务学习策略增强LLaMA模型。

Result: 在Mol-Instruction和USPTO-50K数据集上表现最优，生成的化合物具有多样性和强蛋白结合亲和力。

Conclusion: ChemDual在药物设计中具有潜力，优于传统单任务方法和开源LLM。

Abstract: Chemical reaction and retrosynthesis prediction are fundamental tasks in drug
discovery. Recently, large language models (LLMs) have shown potential in many
domains. However, directly applying LLMs to these tasks faces two major
challenges: (i) lacking a large-scale chemical synthesis-related instruction
dataset; (ii) ignoring the close correlation between reaction and
retrosynthesis prediction for the existing fine-tuning strategies. To address
these challenges, we propose ChemDual, a novel LLM framework for accurate
chemical synthesis. Specifically, considering the high cost of data acquisition
for reaction and retrosynthesis, ChemDual regards the
reaction-and-retrosynthesis of molecules as a related
recombination-and-fragmentation process and constructs a large-scale of 4.4
million instruction dataset. Furthermore, ChemDual introduces an enhanced
LLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy,
to jointly optimize the process of recombination and fragmentation as well as
the tasks between reaction and retrosynthesis prediction. Extensive experiments
on Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves
state-of-the-art performance in both predictions of reaction and
retrosynthesis, outperforming the existing conventional single-task approaches
and the general open-source LLMs. Through molecular docking analysis, ChemDual
generates compounds with diverse and strong protein binding affinity, further
highlighting its strong potential in drug design.

</details>

### [109] [EntroLLM: Entropy Encoded Weight Compression for Efficient Large Language Model Inference on Edge Devices](https://arxiv.org/abs/2505.02380)
*Arnab Sanyal,Prithwish Mukherjee,Gourav Datta,Sandeep P. Chinchali*

Main category: cs.LG

TLDR: EntroLLM是一种新型压缩框架，通过混合量化和熵编码减少LLM的存储需求，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）在边缘设备上部署时的高存储和计算需求问题。

Method: 结合层级混合量化（对称与非对称量化）和Huffman编码，实现无损压缩，并引入并行Huffman解码以减少推理延迟。

Result: 在边缘设备上实现高达30%至65%的存储减少，同时保持语言任务的困惑度和准确性，推理吞吐量提升31.9%至146.6%。

Conclusion: EntroLLM无需重新训练，兼容现有量化方法，是边缘设备上LLM部署的实用解决方案。

Abstract: Large Language Models (LLMs) demonstrate exceptional performance across
various tasks, but their large storage and computational requirements constrain
their deployment on edge devices. To address this, we propose EntroLLM, a novel
compression framework that integrates mixed quantization with entropy coding to
reduce storage overhead while maintaining model accuracy. Our method applies a
layer-wise mixed quantization scheme - choosing between symmetric and
asymmetric quantization based on individual layer weight distributions - to
optimize compressibility. We then employ Huffman encoding for lossless
compression of the quantized weights, significantly reducing memory bandwidth
requirements. Furthermore, we introduce parallel Huffman decoding, which
enables efficient retrieval of encoded weights during inference, ensuring
minimal latency impact. Our experiments on edge-compatible LLMs, including
smolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct,
demonstrate that EntroLLM achieves up to $30%$ storage reduction compared to
uint8 models and up to $65%$ storage reduction compared to uint4 models, while
preserving perplexity and accuracy, on language benchmark tasks. We further
show that our method enables $31.9%$ - $146.6%$ faster inference throughput on
memory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by reducing
the required data movement. The proposed approach requires no additional
re-training and is fully compatible with existing post-training quantization
methods, making it a practical solution for edge LLMs.

</details>

### [110] [Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs Between Privacy and Regret](https://arxiv.org/abs/2505.02383)
*Bingshan Hu,Zhiming Huang,Tianyue H. Zhang,Mathias Lécuyer,Nidhi Hegde*

Main category: cs.LG

TLDR: 论文提出了一种名为DP-TS-UCB的差分隐私随机赌博算法，通过高斯机制和差分隐私（GDP）的关联，实现了隐私与遗憾的权衡。


<details>
  <summary>Details</summary>
Motivation: 探索Thompson Sampling与高斯机制、高斯差分隐私（GDP）之间的深层联系，解决差分隐私随机赌博问题。

Method: 提出DP-TS-UCB算法，结合高斯分布的反集中界限，平衡隐私与遗憾。

Result: 算法满足$\tilde{O}(T^{0.25(1-\alpha)})$-GDP隐私保证，遗憾界为$O(K\ln^{\alpha+1}(T)/\Delta)$。

Conclusion: DP-TS-UCB通过理论分析展示了隐私与性能的权衡，为相关领域提供了新思路。

Abstract: We address differentially private stochastic bandit problems from the angles
of exploring the deep connections among Thompson Sampling with Gaussian priors,
Gaussian mechanisms, and Gaussian differential privacy (GDP). We propose
DP-TS-UCB, a novel parametrized private bandit algorithm that enables to trade
off privacy and regret. DP-TS-UCB satisfies $ \tilde{O}
\left(T^{0.25(1-\alpha)}\right)$-GDP and enjoys an $O
\left(K\ln^{\alpha+1}(T)/\Delta \right)$ regret bound, where $\alpha \in [0,1]$
controls the trade-off between privacy and regret. Theoretically, our DP-TS-UCB
relies on anti-concentration bounds of Gaussian distributions and links
exploration mechanisms in Thompson Sampling-based algorithms and Upper
Confidence Bound-based algorithms, which may be of independent interest.

</details>

### [111] [Quantitative Analysis of Performance Drop in DeepSeek Model Quantization](https://arxiv.org/abs/2505.02390)
*Enbo Zhao,Yi Shen,Shuming Shi,Jieyun Huang,Zhihao Chen,Ning Wang,Siqi Xiao,Jian Zhang,Kai Wang,Shiguo Lian*

Main category: cs.LG

TLDR: 该技术报告首次对DeepSeek-R1和V3模型进行了多比特宽度量化的定量评估，发现4-bit量化在性能损失极小的情况下支持单机部署，并提出动态3-bit量化方法DQ3_K_M，性能优于传统方法且支持多种硬件配置。


<details>
  <summary>Details</summary>
Motivation: 由于官方服务繁忙和数据隐私问题，本地部署DeepSeek-R1和V3的需求增加，但模型的参数配置超出单机内存限制，量化成为解决方案。

Method: 采用多比特宽度量化技术，提出动态3-bit量化方法DQ3_K_M，并与传统方法进行对比。

Result: 4-bit量化性能接近FP8，DQ3_K_M在多项任务中优于传统3-bit量化，且支持多种硬件单机部署。

Conclusion: DQ3_K_M是一种高效的3-bit量化方法，性能接近4-bit量化，为本地部署提供了可行方案。

Abstract: Recently, there is a high demand for deploying DeepSeek-R1 and V3 locally,
possibly because the official service often suffers from being busy and some
organizations have data privacy concerns. While single-machine deployment
offers infrastructure simplicity, the models' 671B FP8 parameter configuration
exceeds the practical memory limits of a standard 8-GPU machine. Quantization
is a widely used technique that helps reduce model memory consumption. However,
it is unclear what the performance of DeepSeek-R1 and V3 will be after being
quantized. This technical report presents the first quantitative evaluation of
multi-bitwidth quantization across the complete DeepSeek model spectrum. Key
findings reveal that 4-bit quantization maintains little performance
degradation versus FP8 while enabling single-machine deployment on standard
NVIDIA GPU devices. We further propose DQ3_K_M, a dynamic 3-bit quantization
method that significantly outperforms traditional Q3_K_M variant on various
benchmarks, which is also comparable with 4-bit quantization (Q4_K_M) approach
in most tasks. Moreover, DQ3_K_M supports single-machine deployment
configurations for both NVIDIA H100/A100 and Huawei 910B. Our implementation of
DQ3\_K\_M is released at https://github.com/UnicomAI/DeepSeek-Eval, containing
optimized 3-bit quantized variants of both DeepSeek-R1 and DeepSeek-V3.

</details>

### [112] [A probabilistic view on Riemannian machine learning models for SPD matrices](https://arxiv.org/abs/2505.02402)
*Thibault de Surrel,Florian Yger,Fabien Lotte,Sylvain Chevallier*

Main category: cs.LG

TLDR: 本文提出了一种在对称正定矩阵流形上的概率框架，将多种机器学习工具统一起来。


<details>
  <summary>Details</summary>
Motivation: 研究如何在对称正定矩阵流形上统一不同的机器学习工具，并扩展其应用。

Method: 定义了几种高斯分布，并将流行分类器重新解释为贝叶斯分类器。

Result: 展示了这些分布在异常检测和降维中的应用，并扩展了其他机器学习工具的使用。

Conclusion: 通过概率框架，实现了对称正定矩阵流形上机器学习工具的广泛扩展。

Abstract: The goal of this paper is to show how different machine learning tools on the
Riemannian manifold $\mathcal{P}_d$ of Symmetric Positive Definite (SPD)
matrices can be united under a probabilistic framework. For this, we will need
several Gaussian distributions defined on $\mathcal{P}_d$. We will show how
popular classifiers on $\mathcal{P}_d$ can be reinterpreted as Bayes
Classifiers using these Gaussian distributions. These distributions will also
be used for outlier detection and dimension reduction. By showing that those
distributions are pervasive in the tools used on $\mathcal{P}_d$, we allow for
other machine learning tools to be extended to $\mathcal{P}_d$.

</details>

### [113] [T2S: High-resolution Time Series Generation with Text-to-Series Diffusion Models](https://arxiv.org/abs/2505.02417)
*Yunfeng Ge,Jiawei Li,Yiji Zhao,Haomin Wen,Zhao Li,Meikang Qiu,Hongyan Li,Ming Jin,Shirui Pan*

Main category: cs.LG

TLDR: 本文提出了一种基于扩散模型的文本到时间序列生成框架T2S，解决了现有方法在通用性和长度适应性上的不足，并在多领域数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决文本到时间序列生成中的数据稀疏性、不平衡性和多模态数据集有限的问题，同时克服现有方法在通用性和长度适应性上的局限性。

Method: 提出T2S框架，采用长度自适应的变分自编码器和Flow Matching技术，结合Diffusion Transformer作为去噪器，支持任意长度的时间序列生成。

Result: 在13个跨12个领域的数据集上实现了最先进的性能。

Conclusion: T2S是一种领域无关的框架，能够有效生成任意长度的时间序列，具有广泛的适用性。

Abstract: Text-to-Time Series generation holds significant potential to address
challenges such as data sparsity, imbalance, and limited availability of
multimodal time series datasets across domains. While diffusion models have
achieved remarkable success in Text-to-X (e.g., vision and audio data)
generation, their use in time series generation remains in its nascent stages.
Existing approaches face two critical limitations: (1) the lack of systematic
exploration of general-proposed time series captions, which are often
domain-specific and struggle with generalization; and (2) the inability to
generate time series of arbitrary lengths, limiting their applicability to
real-world scenarios. In this work, we first categorize time series captions
into three levels: point-level, fragment-level, and instance-level.
Additionally, we introduce a new fragment-level dataset containing over 600,000
high-resolution time series-text pairs. Second, we propose Text-to-Series
(T2S), a diffusion-based framework that bridges the gap between natural
language and time series in a domain-agnostic manner. T2S employs a
length-adaptive variational autoencoder to encode time series of varying
lengths into consistent latent embeddings. On top of that, T2S effectively
aligns textual representations with latent embeddings by utilizing Flow
Matching and employing Diffusion Transformer as the denoiser. We train T2S in
an interleaved paradigm across multiple lengths, allowing it to generate
sequences of any desired length. Extensive evaluations demonstrate that T2S
achieves state-of-the-art performance across 13 datasets spanning 12 domains.

</details>

### [114] [Towards One-shot Federated Learning: Advances, Challenges, and Future Directions](https://arxiv.org/abs/2505.02426)
*Flora Amato,Lingyu Qiu,Mohammad Tanveer,Salvatore Cuomo,Fabio Giampaolo,Francesco Piccialli*

Main category: cs.LG

TLDR: 本文综述了单轮联邦学习（One-shot FL）的特点、方法、局限性和未来挑战，旨在为资源受限和隐私敏感场景提供参考。


<details>
  <summary>Details</summary>
Motivation: 单轮联邦学习通过减少通信轮次，适用于资源受限和隐私敏感的应用场景，本文旨在全面分析其框架、方法及挑战。

Method: 系统分类现有方法，重点讨论客户端模型初始化、聚合技术及处理异构数据分布的策略。

Result: 总结了单轮联邦学习的优势（如资源高效）和局限性（如非独立同分布数据的扩展性问题）。

Conclusion: 本文为研究人员和实践者提供了单轮联邦学习的全面参考，并指出了未来研究方向。

Abstract: One-shot FL enables collaborative training in a single round, eliminating the
need for iterative communication, making it particularly suitable for use in
resource-constrained and privacy-sensitive applications. This survey offers a
thorough examination of One-shot FL, highlighting its distinct operational
framework compared to traditional federated approaches. One-shot FL supports
resource-limited devices by enabling single-round model aggregation while
maintaining data locality. The survey systematically categorizes existing
methodologies, emphasizing advancements in client model initialization,
aggregation techniques, and strategies for managing heterogeneous data
distributions. Furthermore, we analyze the limitations of current approaches,
particularly in terms of scalability and generalization in non-IID settings. By
analyzing cutting-edge techniques and outlining open challenges, this survey
aspires to provide a comprehensive reference for researchers and practitioners
aiming to design and implement One-shot FL systems, advancing the development
and adoption of One-shot FL solutions in a real-world, resource-constrained
scenario.

</details>

### [115] [FairPO: Robust Preference Optimization for Fair Multi-Label Learning](https://arxiv.org/abs/2505.02433)
*Soumen Kumar Mondal,Akshit Varmora,Prateek Chanda,Ganesh Ramakrishnan*

Main category: cs.LG

TLDR: FairPO是一个新颖的框架，旨在通过直接优化偏好信号来促进多标签分类中的公平性，采用群体鲁棒性视角。


<details>
  <summary>Details</summary>
Motivation: 解决多标签分类中的公平性问题，特别是针对特权和非特权标签组的偏置问题。

Method: 将标签分为特权和非特权组，采用基于偏好的损失函数（DPO启发），动态调整训练重点以改善性能较差的组。

Result: 框架能够有效区分特权组中的真实正标签和混淆负标签，同时保持非特权标签的分类性能。

Conclusion: FairPO通过动态调整训练重点，减轻偏置并确保标签类别的公平处理，未来计划扩展损失函数和生成能力。

Abstract: We propose FairPO, a novel framework designed to promote fairness in
multi-label classification by directly optimizing preference signals with a
group robustness perspective. In our framework, the set of labels is
partitioned into privileged and non-privileged groups, and a preference-based
loss inspired by Direct Preference Optimization (DPO) is employed to more
effectively differentiate true positive labels from confusing negatives within
the privileged group, while preserving baseline classification performance for
non-privileged labels. By framing the learning problem as a robust optimization
over groups, our approach dynamically adjusts the training emphasis toward
groups with poorer performance, thereby mitigating bias and ensuring a fairer
treatment across diverse label categories. In addition, we outline plans to
extend this approach by investigating alternative loss formulations such as
Simple Preference Optimisation (SimPO) and Contrastive Preference Optimization
(CPO) to exploit reference-free reward formulations and contrastive training
signals. Furthermore, we plan to extend FairPO with multilabel generation
capabilities, enabling the model to dynamically generate diverse and coherent
label sets for ambiguous inputs.

</details>

### [116] [A New Approach to Backtracking Counterfactual Explanations: A Causal Framework for Efficient Model Interpretability](https://arxiv.org/abs/2505.02435)
*Pouria Fatemi,Ehsan Sharifian,Mohammad Hossein Yassaee*

Main category: cs.LG

TLDR: 提出了一种基于回溯反事实的高效方法，结合因果推理生成可操作的模型解释，解决了传统方法忽略因果关系和新方法计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统反事实解释方法常忽略因果关系，导致不现实的例子，而新方法虽整合了因果关系但计算成本高。

Method: 基于回溯反事实的方法，结合因果推理，生成可操作的模型解释。

Result: 实验表明，该方法能更深入地理解模型输出。

Conclusion: 该方法不仅解决了现有方法的局限性，还在特定场景下推广了先前技术。

Abstract: Counterfactual explanations enhance interpretability by identifying
alternative inputs that produce different outputs, offering localized insights
into model decisions. However, traditional methods often neglect causal
relationships, leading to unrealistic examples. While newer approaches
integrate causality, they are computationally expensive. To address these
challenges, we propose an efficient method based on backtracking
counterfactuals that incorporates causal reasoning to generate actionable
explanations. We first examine the limitations of existing methods and then
introduce our novel approach and its features. We also explore the relationship
between our method and previous techniques, demonstrating that it generalizes
them in specific scenarios. Finally, experiments show that our method provides
deeper insights into model outputs.

</details>

### [117] [Efficient Continual Learning in Keyword Spotting using Binary Neural Networks](https://arxiv.org/abs/2505.02469)
*Quynh Nguyen-Phuong Vu,Luciano Sebastian Martinez-Rau,Yuxuan Zhang,Nho-Duc Tran,Bengt Oelmann,Michele Magno,Sebastian Bader*

Main category: cs.LG

TLDR: 提出了一种基于二元神经网络的持续学习方法，用于资源受限设备中的关键词识别，支持动态添加新关键词。


<details>
  <summary>Details</summary>
Motivation: 资源受限设备中的关键词识别模型通常无法适应新场景（如新增关键词），需要一种高效且持续学习的方法。

Method: 采用二元神经网络（BNNs）降低计算和内存需求，结合七种持续学习技术，评估其对新增关键词的适应性。

Result: 实验显示，新增一个关键词时准确率超过95%，新增四个时达86%。批处理算法对数据集大小更敏感，计算复杂度差异不显著。

Conclusion: 该方法为资源受限设备提供了一种高效且持续的关键词识别解决方案。

Abstract: Keyword spotting (KWS) is an essential function that enables interaction with
ubiquitous smart devices. However, in resource-limited devices, KWS models are
often static and can thus not adapt to new scenarios, such as added keywords.
To overcome this problem, we propose a Continual Learning (CL) approach for KWS
built on Binary Neural Networks (BNNs). The framework leverages the reduced
computation and memory requirements of BNNs while incorporating techniques that
enable the seamless integration of new keywords over time. This study evaluates
seven CL techniques on a 16-class use case, reporting an accuracy exceeding 95%
for a single additional keyword and up to 86% for four additional classes.
Sensitivity to the amount of training samples in the CL phase, and differences
in computational complexities are being evaluated. These evaluations
demonstrate that batch-based algorithms are more sensitive to the CL dataset
size, and that differences between the computational complexities are
insignificant. These findings highlight the potential of developing an
effective and computationally efficient technique for continuously integrating
new keywords in KWS applications that is compatible with resource-constrained
devices.

</details>

### [118] [SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning](https://arxiv.org/abs/2505.02486)
*Jinpeng Chen,Runmin Cong,Yuzhi Zhao,Hongzheng Yang,Guangneng Hu,Horace Ho Shing Ip,Sam Kwong*

Main category: cs.LG

TLDR: 论文提出了一种多模态持续指令调优方法（MCIT），通过区分表面遗忘和本质遗忘，并引入答案风格多样化（ASD）和RegLoRA技术，有效缓解遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型（MLLMs）在增量学习新任务时的遗忘问题，特别是表面遗忘和本质遗忘。

Method: 1. 引入答案风格多样化（ASD）统一任务风格；2. 提出RegLoRA技术通过正则化稳定关键参数。

Result: 实验表明，整体方法SEFE达到了最先进的性能。

Conclusion: 通过ASD和RegLoRA的结合，成功缓解了多模态持续学习中的遗忘问题。

Abstract: Multimodal Continual Instruction Tuning (MCIT) aims to enable Multimodal
Large Language Models (MLLMs) to incrementally learn new tasks without
catastrophic forgetting. In this paper, we explore forgetting in this context,
categorizing it into superficial forgetting and essential forgetting.
Superficial forgetting refers to cases where the model's knowledge may not be
genuinely lost, but its responses to previous tasks deviate from expected
formats due to the influence of subsequent tasks' answer styles, making the
results unusable. By contrast, essential forgetting refers to situations where
the model provides correctly formatted but factually inaccurate answers,
indicating a true loss of knowledge. Assessing essential forgetting
necessitates addressing superficial forgetting first, as severe superficial
forgetting can obscure the model's knowledge state. Hence, we first introduce
the Answer Style Diversification (ASD) paradigm, which defines a standardized
process for transforming data styles across different tasks, unifying their
training sets into similarly diversified styles to prevent superficial
forgetting caused by style shifts. Building on this, we propose RegLoRA to
mitigate essential forgetting. RegLoRA stabilizes key parameters where prior
knowledge is primarily stored by applying regularization, enabling the model to
retain existing competencies. Experimental results demonstrate that our overall
method, SEFE, achieves state-of-the-art performance.

</details>

### [119] [Bayesian Robust Aggregation for Federated Learning](https://arxiv.org/abs/2505.02490)
*Aleksandr Karakulev,Usama Zafar,Salman Toor,Prashant Singh*

Main category: cs.LG

TLDR: 提出了一种基于贝叶斯推断的自适应方法，用于联邦学习中模型更新的鲁棒聚合，无需知道受损客户端的数量，且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分散数据上协作训练模型，但易受对抗攻击，且受损客户端的数量和攻击程度可能动态变化。

Method: 通过贝叶斯推断定义模型更新的均值，最大化边缘化概率的似然，独立于受损客户端数量。

Result: 在三个基准图像分类数据集上，该方法在各种攻击类型和动态恶意客户端数量下均表现最优。

Conclusion: 该方法简单高效，性能优于专门为联邦学习设计的聚合方法（如Krum）。

Abstract: Federated Learning enables collaborative training of machine learning models
on decentralized data. This scheme, however, is vulnerable to adversarial
attacks, when some of the clients submit corrupted model updates. In real-world
scenarios, the total number of compromised clients is typically unknown, with
the extent of attacks potentially varying over time. To address these
challenges, we propose an adaptive approach for robust aggregation of model
updates based on Bayesian inference. The mean update is defined by the maximum
of the likelihood marginalized over probabilities of each client to be
`honest'. As a result, the method shares the simplicity of the classical
average estimators (e.g., sample mean or geometric median), being independent
of the number of compromised clients. At the same time, it is as effective
against attacks as methods specifically tailored to Federated Learning, such as
Krum. We compare our approach with other aggregation schemes in federated
setting on three benchmark image classification data sets. The proposed method
consistently achieves state-of-the-art performance across various attack types
with static and varying number of malicious clients.

</details>

### [120] [Exploring Design Choices for Autoregressive Deep Learning Climate Models](https://arxiv.org/abs/2505.02506)
*Florian Gallusser,Simon Hentschel,Anna Krause,Andreas Hotho*

Main category: cs.LG

TLDR: 该研究比较了三种深度学习天气预测模型（FourCastNet、SFNO和ClimaX）的长期稳定性，发现SFNO在超参数选择上最稳健，但所有模型都可能因随机种子和预报变量选择而出现不稳定。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在中短期天气预测中表现优异，但在长期预测中难以保持物理一致性，而传统大气模型却能稳定运行数十年。本研究旨在明确深度学习模型长期稳定性的关键设计因素。

Method: 研究使用ERA5再分析数据（5.625°分辨率），系统评估了自回归训练步数、模型容量和预报变量选择对三种模型（FourCastNet、SFNO和ClimaX）长期稳定性的影响。

Result: SFNO在超参数选择上表现最稳健，但所有模型都可能因随机种子和预报变量选择而出现不稳定。研究还发现了一些能实现10年稳定预测的配置。

Conclusion: 深度学习天气预测模型的长期稳定性受多种因素影响，SFNO在稳健性上表现最佳，但仍需进一步优化以提高稳定性。

Abstract: Deep Learning models have achieved state-of-the-art performance in
medium-range weather prediction but often fail to maintain physically
consistent rollouts beyond 14 days. In contrast, a few atmospheric models
demonstrate stability over decades, though the key design choices enabling this
remain unclear. This study quantitatively compares the long-term stability of
three prominent DL-MWP architectures - FourCastNet, SFNO, and ClimaX - trained
on ERA5 reanalysis data at 5.625{\deg} resolution. We systematically assess the
impact of autoregressive training steps, model capacity, and choice of
prognostic variables, identifying configurations that enable stable 10-year
rollouts while preserving the statistical properties of the reference dataset.
Notably, rollouts with SFNO exhibit the greatest robustness to hyperparameter
choices, yet all models can experience instability depending on the random seed
and the set of prognostic variables

</details>

### [121] [Uncovering Population PK Covariates from VAE-Generated Latent Spaces](https://arxiv.org/abs/2505.02514)
*Diego Perazzolo,Chiara Castellani,Enrico Grisan*

Main category: cs.LG

TLDR: 提出了一种结合变分自编码器（VAE）和LASSO回归的数据驱动框架，用于识别他克莫司药代动力学中的关键协变量。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉药代动力学数据中的复杂非线性关系，需要一种更有效的方法来识别影响药物吸收的协变量。

Method: 使用VAE压缩高维药代动力学信号到潜在空间，再通过LASSO回归映射协变量，实现稀疏特征选择。

Result: 方法成功识别出SNP、年龄、白蛋白和血红蛋白等临床相关协变量，MAPE为2.26%。

Conclusion: VAE-LASSO框架为协变量选择提供了可扩展、可解释的解决方案，适用于药物开发和精准药物治疗。

Abstract: Population pharmacokinetic (PopPK) modelling is a fundamental tool for
understanding drug behaviour across diverse patient populations and enabling
personalized dosing strategies to improve therapeutic outcomes. A key challenge
in PopPK analysis lies in identifying and modelling covariates that influence
drug absorption, as these relationships are often complex and nonlinear.
Traditional methods may fail to capture hidden patterns within the data. In
this study, we propose a data-driven, model-free framework that integrates
Variational Autoencoders (VAEs) deep learning model and LASSO regression to
uncover key covariates from simulated tacrolimus pharmacokinetic (PK) profiles.
The VAE compresses high-dimensional PK signals into a structured latent space,
achieving accurate reconstruction with a mean absolute percentage error (MAPE)
of 2.26%. LASSO regression is then applied to map patient-specific covariates
to the latent space, enabling sparse feature selection through L1
regularization. This approach consistently identifies clinically relevant
covariates for tacrolimus including SNP, age, albumin, and hemoglobin which are
retained across the tested regularization strength levels, while effectively
discarding non-informative features. The proposed VAE-LASSO methodology offers
a scalable, interpretable, and fully data-driven solution for covariate
selection, with promising applications in drug development and precision
pharmacotherapy.

</details>

### [122] [FedSDAF: Leveraging Source Domain Awareness for Enhanced Federated Domain Generalization](https://arxiv.org/abs/2505.02515)
*Hongze Li,Zesheng Zhou,Zhenbiao Cao,Xinhui Li,Wei Chen,Xiaojin Zhang*

Main category: cs.LG

TLDR: 论文提出了一种名为FedSDAF的联邦源域感知框架，通过系统利用源域特定特征提升联邦领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统领域泛化方法忽视源域特定特征，尤其在数据隔离的联邦设置中。

Method: FedSDAF包含两个组件：域不变适配器和域感知适配器，后者使用多头自注意力机制提取源域知识，并引入双向知识蒸馏机制。

Result: 在四个标准基准测试中，FedSDAF准确率提升5.2-13.8%，超越现有方法。

Conclusion: FedSDAF首次系统利用源域特征，显著提升模型泛化能力。

Abstract: Traditional domain generalization approaches predominantly focus on
leveraging target domain-aware features while overlooking the critical role of
source domain-specific characteristics, particularly in federated settings with
inherent data isolation. To address this gap, we propose the Federated Source
Domain Awareness Framework (FedSDAF), the first method to systematically
exploit source domain-aware features for enhanced federated domain
generalization (FedDG). The FedSDAF framework consists of two synergistic
components: the Domain-Invariant Adapter, which preserves critical
domain-invariant features, and the Domain-Aware Adapter, which extracts and
integrates source domain-specific knowledge using a Multihead Self-Attention
mechanism (MHSA). Furthermore, we introduce a bidirectional knowledge
distillation mechanism that fosters knowledge sharing among clients while
safeguarding privacy. Our approach represents the first systematic exploitation
of source domain-aware features, resulting in significant advancements in model
generalization capability.Extensive experiments on four standard benchmarks
(OfficeHome, PACS, VLCS, and DomainNet) show that our method consistently
surpasses state-of-the-art federated domain generalization approaches, with
accuracy gains of 5.2-13.8%. The source code is available at
https://github.com/pizzareapers/FedSDAF.

</details>

### [123] [Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation Beyond Bounded Activations](https://arxiv.org/abs/2505.02537)
*Davide Sartor,Alberto Sinigaglia,Gian Antonio Susto*

Main category: cs.LG

TLDR: 该论文提出了一种广义的理论结果，证明具有非负权重约束和交替饱和激活函数的MLP是单调函数的通用逼近器，并揭示了激活函数饱和侧与权重约束符号的等价性。此外，提出了一种无需权重重新参数化的新方法，优化训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过非负权重约束和有界激活函数实现单调性，但存在优化挑战。本文旨在提供理论支持并简化架构。

Method: 通过理论分析证明MLP的通用逼近能力，并提出一种新方法，允许网络根据权重符号调整激活函数，避免权重重新参数化。

Result: 实验验证了新方法的有效性，与传统单调架构相比表现更优。

Conclusion: 本文为单调MLP提供了理论依据，并提出了一种更简单的优化方法，提升了训练稳定性。

Abstract: Conventional techniques for imposing monotonicity in MLPs by construction
involve the use of non-negative weight constraints and bounded activation
functions, which pose well-known optimization challenges. In this work, we
generalize previous theoretical results, showing that MLPs with non-negative
weight constraint and activations that saturate on alternating sides are
universal approximators for monotonic functions. Additionally, we show an
equivalence between the saturation side in the activations and the sign of the
weight constraint. This connection allows us to prove that MLPs with convex
monotone activations and non-positive constrained weights also qualify as
universal approximators, in contrast to their non-negative constrained
counterparts. Our results provide theoretical grounding to the empirical
effectiveness observed in previous works while leading to possible
architectural simplification. Moreover, to further alleviate the optimization
difficulties, we propose an alternative formulation that allows the network to
adjust its activations according to the sign of the weights. This eliminates
the requirement for weight reparameterization, easing initialization and
improving training stability. Experimental evaluation reinforces the validity
of the theoretical results, showing that our novel approach compares favourably
to traditional monotonic architectures.

</details>

### [124] [Lazy But Effective: Collaborative Personalized Federated Learning with Heterogeneous Data](https://arxiv.org/abs/2505.02540)
*Ljubomir Rokvic,Panayiotis Danassis,Boi Faltings*

Main category: cs.LG

TLDR: 提出了一种个性化联邦学习框架pFedLIA，通过分布式聚类解决客户端数据分布不均问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据分布不均导致单一全局模型性能不佳，需个性化解决方案。

Method: 利用‘Lazy Influence’近似影响进行分布式聚类，每个集群内联合训练模型。

Result: 成功恢复全局模型性能，在多项任务中表现优异，如CIFAR100提升17%。

Conclusion: pFedLIA能有效解决非独立同分布数据问题，性能接近理想聚类，优于现有基线。

Abstract: In Federated Learning, heterogeneity in client data distributions often means
that a single global model does not have the best performance for individual
clients. Consider for example training a next-word prediction model for
keyboards: user-specific language patterns due to demographics (dialect, age,
etc.), language proficiency, and writing style result in a highly non-IID
dataset across clients. Other examples are medical images taken with different
machines, or driving data from different vehicle types. To address this, we
propose a simple yet effective personalized federated learning framework
(pFedLIA) that utilizes a computationally efficient influence approximation,
called `Lazy Influence', to cluster clients in a distributed manner before
model aggregation. Within each cluster, data owners collaborate to jointly
train a model that captures the specific data patterns of the clients. Our
method has been shown to successfully recover the global model's performance
drop due to the non-IID-ness in various synthetic and real-world settings,
specifically a next-word prediction task on the Nordic languages as well as
several benchmark tasks. It matches the performance of a hypothetical Oracle
clustering, and significantly improves on existing baselines, e.g., an
improvement of 17% on CIFAR100.

</details>

### [125] [Robustness questions the interpretability of graph neural networks: what to do?](https://arxiv.org/abs/2505.02566)
*Kirill Lukyanov,Georgii Sazonov,Serafim Boyarsky,Ilya Makarov*

Main category: cs.LG

TLDR: 该论文提出了一个系统分析图神经网络（GNNs）可解释性与鲁棒性之间关系的基准，评估了不同防御方法对可解释性的影响。


<details>
  <summary>Details</summary>
Motivation: 研究GNNs在对抗性攻击（如投毒和规避攻击）下的可解释性与鲁棒性之间的相互作用，填补这一领域的空白。

Method: 评估了基于GCN、SAGE、GIN和GAT的六种GNN架构，在五个数据集上使用四种可解释性指标（Fidelity、Stability、Consistency、Sparsity），并分析了防御方法对可解释性的影响。

Result: 结果表明，防御方法和模型架构特性对可解释性有显著影响，揭示了鲁棒性与可解释性之间的权衡关系。

Conclusion: 通过建立标准化基准，为开发兼具鲁棒性和可解释性的GNNs提供了基础，有助于在敏感应用中建立信任。

Abstract: Graph Neural Networks (GNNs) have become a cornerstone in graph-based data
analysis, with applications in diverse domains such as bioinformatics, social
networks, and recommendation systems. However, the interplay between model
interpretability and robustness remains poorly understood, especially under
adversarial scenarios like poisoning and evasion attacks. This paper presents a
comprehensive benchmark to systematically analyze the impact of various factors
on the interpretability of GNNs, including the influence of
robustness-enhancing defense mechanisms.
  We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across
five datasets from two distinct domains, employing four interpretability
metrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how
defenses against poisoning and evasion attacks, applied before and during model
training, affect interpretability and highlights critical trade-offs between
robustness and interpretability. The framework will be published as open
source.
  The results reveal significant variations in interpretability depending on
the chosen defense methods and model architecture characteristics. By
establishing a standardized benchmark, this work provides a foundation for
developing GNNs that are both robust to adversarial threats and interpretable,
facilitating trust in their deployment in sensitive applications.

</details>

### [126] [Rethinking Federated Graph Learning: A Data Condensation Perspective](https://arxiv.org/abs/2505.02573)
*Hao Zhang,Xunkai Li,Yinlin Zhu,Lianglin Hu*

Main category: cs.LG

TLDR: FedGM提出了一种新的联邦图学习范式，通过压缩图作为优化载体，解决了数据异构性和隐私风险问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦图学习方法依赖模型参数或梯度的通信，未能有效处理图数据异构性，且引入隐私风险和通信开销。

Method: 采用压缩图共识机制，聚合分布式图知识，通过单次传输压缩数据降低通信成本和隐私风险。

Result: 在六个公共数据集上的实验表明，FedGM优于现有基线方法。

Conclusion: FedGM为联邦图学习提供了一种高效、隐私安全的新范式。

Abstract: Federated graph learning is a widely recognized technique that promotes
collaborative training of graph neural networks (GNNs) by multi-client
graphs.However, existing approaches heavily rely on the communication of model
parameters or gradients for federated optimization and fail to adequately
address the data heterogeneity introduced by intricate and diverse graph
distributions. Although some methods attempt to share additional messages among
the server and clients to improve federated convergence during communication,
they introduce significant privacy risks and increase communication overhead.
To address these issues, we introduce the concept of a condensed graph as a
novel optimization carrier to address FGL data heterogeneity and propose a new
FGL paradigm called FedGM. Specifically, we utilize a generalized condensation
graph consensus to aggregate comprehensive knowledge from distributed graphs,
while minimizing communication costs and privacy risks through a single
transmission of the condensed data. Extensive experiments on six public
datasets consistently demonstrate the superiority of FedGM over
state-of-the-art baselines, highlighting its potential for a novel FGL
paradigm.

</details>

### [127] [Towards Cross-Modality Modeling for Time Series Analytics: A Survey in the LLM Era](https://arxiv.org/abs/2505.02583)
*Chenxi Liu,Shaowen Zhou,Qianxiong Xu,Hao Miao,Cheng Long,Ziyue Li,Rui Zhao*

Main category: cs.LG

TLDR: 本文综述了基于大语言模型（LLMs）的时间序列跨模态建模方法，分类现有方法并总结关键策略，通过实验验证有效性，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 边缘设备产生大量时间序列数据，现有方法难以直接利用LLMs处理，需解决跨模态差异问题。

Method: 提出分类法将现有方法分为四类，总结跨模态策略（如对齐和融合），并在多模态数据集上进行实验验证。

Result: 实验验证了文本数据与跨模态策略的有效组合，提升了时间序列分析性能。

Conclusion: 未来研究可进一步探索LLMs在时间序列分析中的潜力，本文为相关领域研究者提供了参考。

Abstract: The proliferation of edge devices has generated an unprecedented volume of
time series data across different domains, motivating various well-customized
methods. Recently, Large Language Models (LLMs) have emerged as a new paradigm
for time series analytics by leveraging the shared sequential nature of textual
data and time series. However, a fundamental cross-modality gap between time
series and LLMs exists, as LLMs are pre-trained on textual corpora and are not
inherently optimized for time series. Many recent proposals are designed to
address this issue. In this survey, we provide an up-to-date overview of
LLMs-based cross-modality modeling for time series analytics. We first
introduce a taxonomy that classifies existing approaches into four groups based
on the type of textual data employed for time series modeling. We then
summarize key cross-modality strategies, e.g., alignment and fusion, and
discuss their applications across a range of downstream tasks. Furthermore, we
conduct experiments on multimodal datasets from different application domains
to investigate effective combinations of textual data and cross-modality
strategies for enhancing time series analytics. Finally, we suggest several
promising directions for future research. This survey is designed for a range
of professionals, researchers, and practitioners interested in LLM-based time
series modeling.

</details>

### [128] [Low-Loss Space in Neural Networks is Continuous and Fully Connected](https://arxiv.org/abs/2505.02604)
*Yongding Tian,Zaid Al-Ars,Maksim Kitsak,Peter Hofstee*

Main category: cs.LG

TLDR: 论文提出一种新算法，研究神经网络参数空间中低损失路径的存在性，发现低损失区域是连续且连通的。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络损失景观中低损失路径的性质，以理解参数冗余和模型泛化。

Method: 提出新算法，在LeNet5、ResNet18和Compact Convolutional Transformer架构上验证低损失路径的存在性。

Result: 实验证明参数空间中存在连续的低损失路径，表明低损失区域是连通且连续的。

Conclusion: 研究揭示了神经网络过参数化的理论意义，并提供了改进模型泛化的新方法。

Abstract: Visualizations of the loss landscape in neural networks suggest that minima
are isolated points. However, both theoretical and empirical studies indicate
that it is possible to connect two different minima with a path consisting of
intermediate points that also have low loss. In this study, we propose a new
algorithm which investigates low-loss paths in the full parameter space, not
only between two minima. Our experiments on LeNet5, ResNet18, and Compact
Convolutional Transformer architectures consistently demonstrate the existence
of such continuous paths in the parameter space. These results suggest that the
low-loss region is a fully connected and continuous space in the parameter
space. Our findings provide theoretical insight into neural network
over-parameterization, highlighting that parameters collectively define a
high-dimensional low-loss space, implying parameter redundancy exists only
within individual models and not throughout the entire low-loss space.
Additionally, our work also provides new visualization methods and
opportunities to improve model generalization by exploring the low-loss space
that is closer to the origin.

</details>

### [129] [Mirror Mean-Field Langevin Dynamics](https://arxiv.org/abs/2505.02621)
*Anming Gu,Juno Kim*

Main category: cs.LG

TLDR: 论文提出了一种新的镜像平均场朗之万动力学（MMFLD），用于解决约束域上的概率测度优化问题，并证明了其线性收敛性和混沌传播性质。


<details>
  <summary>Details</summary>
Motivation: 现有平均场算法无法处理约束域问题，限制了其在许多实际应用中的使用。

Method: 通过将平均场朗之万动力学扩展到镜像朗之万框架，提出了MMFLD。

Result: 证明了连续MMFLD的线性收敛性，以及离散化版本的混沌传播性质。

Conclusion: MMFLD为约束域上的优化问题提供了有效的解决方案。

Abstract: The mean-field Langevin dynamics (MFLD) minimizes an entropy-regularized
nonlinear convex functional on the Wasserstein space over $\mathbb{R}^d$, and
has gained attention recently as a model for the gradient descent dynamics of
interacting particle systems such as infinite-width two-layer neural networks.
However, many problems of interest have constrained domains, which are not
solved by existing mean-field algorithms due to the global diffusion term. We
study the optimization of probability measures constrained to a convex subset
of $\mathbb{R}^d$ by proposing the \emph{mirror mean-field Langevin dynamics}
(MMFLD), an extension of MFLD to the mirror Langevin framework. We obtain
linear convergence guarantees for the continuous MMFLD via a uniform
log-Sobolev inequality, and uniform-in-time propagation of chaos results for
its time- and particle-discretized counterpart.

</details>

### [130] [A Theoretical Analysis of Compositional Generalization in Neural Networks: A Necessary and Sufficient Condition](https://arxiv.org/abs/2505.02627)
*Yuanpeng Li*

Main category: cs.LG

TLDR: 本文提出了神经网络中组合泛化的充要条件，包括计算图匹配真实组合结构及组件编码足够信息。


<details>
  <summary>Details</summary>
Motivation: 研究组合泛化在人工智能中的重要性，探索神经网络实现组合泛化的条件。

Method: 通过数学证明提出充要条件，结合架构设计、正则化和训练数据特性。

Result: 证明了组合泛化的充要条件，并通过最小示例验证。

Conclusion: 为神经网络组合泛化的理论研究提供了基础，并提出了训练前评估的潜力。

Abstract: Compositional generalization is a crucial property in artificial
intelligence, enabling models to handle novel combinations of known components.
While most deep learning models lack this capability, certain models succeed in
specific tasks, suggesting the existence of governing conditions. This paper
derives a necessary and sufficient condition for compositional generalization
in neural networks. Conceptually, it requires that (i) the computational graph
matches the true compositional structure, and (ii) components encode just
enough information in training. The condition is supported by mathematical
proofs. This criterion combines aspects of architecture design, regularization,
and training data properties. A carefully designed minimal example illustrates
an intuitive understanding of the condition. We also discuss the potential of
the condition for assessing compositional generalization before training. This
work is a fundamental theoretical study of compositional generalization in
neural networks.

</details>

### [131] [Aerodynamic and structural airfoil shape optimisation via Transfer Learning-enhanced Deep Reinforcement Learning](https://arxiv.org/abs/2505.02634)
*David Ramos,Lucas Lacasa,Eusebio Valero,Gonzalo Rubio*

Main category: cs.LG

TLDR: 提出了一种基于迁移学习的多目标深度强化学习方法，用于优化翼型的几何形状，同时考虑气动和结构性能。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法（如PSO）在计算效率和性能上存在不足，需探索更高效的多目标优化方法。

Method: 采用深度强化学习（DRL）结合迁移学习（TL）策略，优化翼型的升阻比和结构完整性。

Result: DRL方法在计算效率和优化性能上优于PSO，TL进一步节省计算资源且性能接近纯DRL。

Conclusion: 迁移学习增强的DRL方法在多目标翼型优化中具有高效性和优越性。

Abstract: The main objective of this paper is to introduce a transfer
learning-enhanced, multi-objective, deep reinforcement learning (DRL)
methodology that is able to optimise the geometry of any airfoil based on
concomitant aerodynamic and structural criteria. To showcase the method, we aim
to maximise the lift-to-drag ratio $C_L/C_D$ while preserving the structural
integrity of the airfoil -- as modelled by its maximum thickness -- and train
the DRL agent using a list of different transfer learning (TL) strategies. The
performance of the DRL agent is compared with Particle Swarm Optimisation
(PSO), a traditional gradient-free optimisation method. Results indicate that
DRL agents are able to perform multi-objective shape optimisation, that the DRL
approach outperforms PSO in terms of computational efficiency and shape
optimisation performance, and that the TL-enhanced DRL agent achieves
performance comparable to the DRL one, while further saving substantial
computational resources.

</details>

### [132] [Adaptive Budgeted Multi-Armed Bandits for IoT with Dynamic Resource Constraints](https://arxiv.org/abs/2505.02640)
*Shubham Vaishnav,Praveen Kumar Donta,Sindri Magnússon*

Main category: cs.LG

TLDR: 提出了一种针对动态资源约束的IoT系统的预算多臂老虎机框架，通过衰减违规预算和预算UCB算法，实现性能优化与约束平衡。


<details>
  <summary>Details</summary>
Motivation: 解决IoT设备在动态资源约束下实时响应的问题，现有方法难以应对约束随时间变化的场景。

Method: 引入衰减违规预算和预算UCB算法，平衡性能优化与约束合规性。

Result: 理论证明预算UCB具有次线性遗憾和对数级约束违规，仿真显示其优于标准在线学习方法。

Conclusion: 该框架为构建自适应、资源感知的IoT系统提供了潜力。

Abstract: Internet of Things (IoT) systems increasingly operate in environments where
devices must respond in real time while managing fluctuating resource
constraints, including energy and bandwidth. Yet, current approaches often fall
short in addressing scenarios where operational constraints evolve over time.
To address these limitations, we propose a novel Budgeted Multi-Armed Bandit
framework tailored for IoT applications with dynamic operational limits. Our
model introduces a decaying violation budget, which permits limited constraint
violations early in the learning process and gradually enforces stricter
compliance over time. We present the Budgeted Upper Confidence Bound (UCB)
algorithm, which adaptively balances performance optimization and compliance
with time-varying constraints. We provide theoretical guarantees showing that
Budgeted UCB achieves sublinear regret and logarithmic constraint violations
over the learning horizon. Extensive simulations in a wireless communication
setting show that our approach achieves faster adaptation and better constraint
satisfaction than standard online learning methods. These results highlight the
framework's potential for building adaptive, resource-aware IoT systems.

</details>

### [133] [SCFormer: Structured Channel-wise Transformer with Cumulative Historical State for Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.02655)
*Shiwei Guo,Ziang Chen,Yupeng Ma,Yunfei Han,Yi Wang*

Main category: cs.LG

TLDR: SCFormer通过引入时间约束和高阶多项式投影算子，改进了Transformer在多元时间序列预测中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的Transformer模型在计算时间特征时缺乏时间约束，且未充分利用累积历史序列。

Method: SCFormer在Transformer的所有线性变换中引入时间约束，并使用HiPPO处理累积历史时间序列。

Result: 在多个真实数据集上的实验表明，SCFormer显著优于主流基线方法。

Conclusion: SCFormer有效提升了时间序列预测的性能。

Abstract: The Transformer model has shown strong performance in multivariate time
series forecasting by leveraging channel-wise self-attention. However, this
approach lacks temporal constraints when computing temporal features and does
not utilize cumulative historical series effectively.To address these
limitations, we propose the Structured Channel-wise Transformer with Cumulative
Historical state (SCFormer). SCFormer introduces temporal constraints to all
linear transformations, including the query, key, and value matrices, as well
as the fully connected layers within the Transformer. Additionally, SCFormer
employs High-order Polynomial Projection Operators (HiPPO) to deal with
cumulative historical time series, allowing the model to incorporate
information beyond the look-back window during prediction. Extensive
experiments on multiple real-world datasets demonstrate that SCFormer
significantly outperforms mainstream baselines, highlighting its effectiveness
in enhancing time series forecasting. The code is publicly available at
https://github.com/ShiweiGuo1995/SCFormer

</details>

### [134] [A Note on Statistically Accurate Tabular Data Generation Using Large Language Models](https://arxiv.org/abs/2505.02659)
*Andrey Sidorenko*

Main category: cs.LG

TLDR: 提出一种基于概率驱动提示的方法，利用大语言模型估计条件分布，提升合成表格数据的统计保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成合成表格数据时难以保持复杂特征依赖关系，尤其是分类变量之间的依赖关系。

Method: 采用概率驱动的提示方法，利用大语言模型估计条件分布。

Result: 结果表明，该方法能够更准确、可扩展地生成合成数据，并提升统计保真度。

Conclusion: 概率驱动的提示方法为大语言模型生成表格数据提供了更高效的解决方案。

Abstract: Large language models (LLMs) have shown promise in synthetic tabular data
generation, yet existing methods struggle to preserve complex feature
dependencies, particularly among categorical variables. This work introduces a
probability-driven prompting approach that leverages LLMs to estimate
conditional distributions, enabling more accurate and scalable data synthesis.
The results highlight the potential of prompting probobility distributions to
enhance the statistical fidelity of LLM-generated tabular data.

</details>

### [135] [Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework](https://arxiv.org/abs/2505.02712)
*Andrzej Mizera,Jakub Zarzycki*

Main category: cs.LG

TLDR: 该研究利用深度强化学习（DRL）控制布尔网络模型，以解决细胞重编程中的控制问题，并通过图神经网络提升方法的可扩展性和有效性。


<details>
  <summary>Details</summary>
Motivation: 细胞重编程因其治疗潜力备受关注，但传统实验方法耗时且成本高。

Method: 提出一种基于DRL的控制框架，结合布尔网络模型和图神经网络，优化伪吸引子识别方法。

Result: 实验证明该方法在大型生物网络中具有可扩展性和有效性。

Conclusion: 该研究为细胞重编程提供了一种高效的计算方法。

Abstract: Cellular reprogramming, the artificial transformation of one cell type into
another, has been attracting increasing research attention due to its
therapeutic potential for complex diseases. However, discovering reprogramming
strategies through classical wet-lab experiments is hindered by lengthy time
commitments and high costs. In this study, we explore the use of deep
reinforcement learning (DRL) to control Boolean network models of complex
biological systems, such as gene regulatory networks and signalling pathway
networks. We formulate a novel control problem for Boolean network models under
the asynchronous update mode in the context of cellular reprogramming. To
facilitate scalability, we consider our previously introduced concept of a
pseudo-attractor and we improve our procedure for effective identification of
pseudo-attractor states. Finally, we devise a computational framework to solve
the control problem. To leverage the structure of biological systems, we
incorporate graph neural networks with graph convolutions into the artificial
neural network approximator for the action-value function learned by the DRL
agent. Experiments on a number of large real-world biological networks from
literature demonstrate the scalability and effectiveness of our approach.

</details>

### [136] [Less is More: Efficient Weight Farcasting with 1-Layer Neural Network](https://arxiv.org/abs/2505.02714)
*Xiao Shou,Debarun Bhattacharjya,Yanna Ding,Chen Zhao,Rui Li,Jianxi Gao*

Main category: cs.LG

TLDR: 提出了一种基于长期时间序列预测的新框架，仅利用初始和最终权重值，优化大型深度神经网络的训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决大规模深度神经网络训练中的计算挑战，满足模型规模不断增长的需求。

Method: 利用时间序列预测技术，仅依赖初始和最终权重值，并引入定制化正则化器。

Result: 在合成权重序列和真实架构（如DistilBERT）上验证了方法的优越性，提高了预测精度和计算效率。

Conclusion: 该框架在最小额外计算开销下显著提升性能，为加速训练提供了新途径。

Abstract: Addressing the computational challenges inherent in training large-scale deep
neural networks remains a critical endeavor in contemporary machine learning
research. While previous efforts have focused on enhancing training efficiency
through techniques such as gradient descent with momentum, learning rate
scheduling, and weight regularization, the demand for further innovation
continues to burgeon as model sizes keep expanding. In this study, we introduce
a novel framework which diverges from conventional approaches by leveraging
long-term time series forecasting techniques. Our method capitalizes solely on
initial and final weight values, offering a streamlined alternative for complex
model architectures. We also introduce a novel regularizer that is tailored to
enhance the forecasting performance of our approach. Empirical evaluations
conducted on synthetic weight sequences and real-world deep learning
architectures, including the prominent large language model DistilBERT,
demonstrate the superiority of our method in terms of forecasting accuracy and
computational efficiency. Notably, our framework showcases improved performance
while requiring minimal additional computational overhead, thus presenting a
promising avenue for accelerating the training process across diverse tasks and
architectures.

</details>

### [137] [Knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation](https://arxiv.org/abs/2505.02737)
*Pons Gerard,Bilalli Besim,Queralt Anna*

Main category: cs.LG

TLDR: 论文提出利用知识图谱（KG）增强大型语言模型（LLM）的零样本实体消歧（ED）能力，通过层级表示和实体描述优化输入提示，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在零样本或少量样本任务中表现优异，但仍存在幻觉、知识过时或领域信息缺失的问题，而重新训练模型成本高昂。因此，研究探索了KG作为外部结构化知识源来弥补这些不足。

Method: 利用KG中实体类别的层级表示逐步剪枝候选空间，并通过实体描述丰富输入提示，增强LLM的零样本ED能力。

Result: 实验表明，该方法在流行ED数据集上优于未增强及仅使用描述增强的LLM，且比任务专用模型更具适应性。

Conclusion: KG的语义表达能力对ED性能有显著影响，该方法为LLM的知识补充提供了有效途径。

Abstract: Recent advances in Large Language Models (LLMs) have positioned them as a
prominent solution for Natural Language Processing tasks. Notably, they can
approach these problems in a zero or few-shot manner, thereby eliminating the
need for training or fine-tuning task-specific models. However, LLMs face some
challenges, including hallucination and the presence of outdated knowledge or
missing information from specific domains in the training data. These problems
cannot be easily solved by retraining the models with new data as it is a
time-consuming and expensive process. To mitigate these issues, Knowledge
Graphs (KGs) have been proposed as a structured external source of information
to enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for
zero-shot Entity Disambiguation (ED). For that purpose, we leverage the
hierarchical representation of the entities' classes in a KG to gradually prune
the candidate space as well as the entities' descriptions to enrich the input
prompt with additional factual knowledge. Our evaluation on popular ED datasets
shows that the proposed method outperforms non-enhanced and description-only
enhanced LLMs, and has a higher degree of adaptability than task-specific
models. Furthermore, we conduct an error analysis and discuss the impact of the
leveraged KG's semantic expressivity on the ED performance.

</details>

### [138] [Cooperative Bayesian and variance networks disentangle aleatoric and epistemic uncertainties](https://arxiv.org/abs/2505.02743)
*Jiaxiang Yi,Miguel A. Bessa*

Main category: cs.LG

TLDR: 提出了一种联合训练方差网络与贝叶斯神经网络的方法，有效分离了数据中的随机不确定性和认知不确定性，并提升了均值估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现实数据中存在随机不确定性（不可约噪声）和认知不确定性（模型不确定性），现有方法（如MVE网络和贝叶斯神经网络）各有局限，无法同时解决这两种不确定性。

Method: 通过联合训练一个方差网络和一个贝叶斯神经网络，实现随机不确定性和认知不确定性的分离，并改进均值估计。

Result: 该方法在多种数据集上表现有效且可扩展，包括一个时间依赖的异方差回归数据集。

Conclusion: 所提方法易于实现、鲁棒性强，并能适应多种模型架构。

Abstract: Real-world data contains aleatoric uncertainty - irreducible noise arising
from imperfect measurements or from incomplete knowledge about the data
generation process. Mean variance estimation (MVE) networks can learn this type
of uncertainty but require ad-hoc regularization strategies to avoid
overfitting and are unable to predict epistemic uncertainty (model
uncertainty). Conversely, Bayesian neural networks predict epistemic
uncertainty but are notoriously difficult to train due to the approximate
nature of Bayesian inference. We propose to cooperatively train a variance
network with a Bayesian neural network and demonstrate that the resulting model
disentangles aleatoric and epistemic uncertainties while improving the mean
estimation. We demonstrate the effectiveness and scalability of this method
across a diverse range of datasets, including a time-dependent heteroscedastic
regression dataset we created where the aleatoric uncertainty is known. The
proposed method is straightforward to implement, robust, and adaptable to
various model architectures.

</details>

### [139] [HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models](https://arxiv.org/abs/2505.02795)
*Zheng Lin,Yuxin Zhang,Zhe Chen,Zihan Fang,Xianhao Chen,Praneeth Vepakomma,Wei Ni,Jun Luo,Yue Gao*

Main category: cs.LG

TLDR: HSplitLoRA提出了一种基于分裂学习和低秩适应的异构参数高效微调框架，用于在异构客户端设备上高效微调大语言模型。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型参数巨大，联邦学习在微调时面临高昂计算成本，且客户端设备资源异构性进一步增加了复杂性。

Method: HSplitLoRA通过识别重要权重、动态配置LoRA适配器分解秩、确定模型分裂点，并设计无噪声适配器聚合机制。

Result: 实验表明HSplitLoRA在训练准确性和收敛速度上优于现有基准。

Conclusion: HSplitLoRA为异构设备上的大语言模型微调提供了高效解决方案。

Abstract: Recently, large language models (LLMs) have achieved remarkable
breakthroughs, revolutionizing the natural language processing domain and
beyond. Due to immense parameter sizes, fine-tuning these models with private
data for diverse downstream tasks has become mainstream. Though federated
learning (FL) offers a promising solution for fine-tuning LLMs without sharing
raw data, substantial computing costs hinder its democratization. Moreover, in
real-world scenarios, private client devices often possess heterogeneous
computing resources, further complicating LLM fine-tuning. To combat these
challenges, we propose HSplitLoRA, a heterogeneous parameter-efficient
fine-tuning (PEFT) framework built on split learning (SL) and low-rank
adaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on
heterogeneous client devices. HSplitLoRA first identifies important weights
based on their contributions to LLM training. It then dynamically configures
the decomposition ranks of LoRA adapters for selected weights and determines
the model split point according to varying computing budgets of client devices.
Finally, a noise-free adapter aggregation mechanism is devised to support
heterogeneous adapter aggregation without introducing noise. Extensive
experiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks
in training accuracy and convergence speed.

</details>

### [140] [Towards Quantifying the Hessian Structure of Neural Networks](https://arxiv.org/abs/2505.02809)
*Zhaorui Dong,Yushun Zhang,Zhi-Quan Luo,Jianfeng Yao,Ruoyu Sun*

Main category: cs.LG

TLDR: 论文揭示了神经网络Hessian矩阵近块对角结构的两种成因：架构设计的静态力和训练的动态力，并理论分析了静态力在随机初始化时的作用。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络Hessian矩阵近块对角结构的理论基础，填补现有实证研究的空白。

Method: 通过随机矩阵理论分析线性模型和单隐藏层网络在MSE和CE损失下的Hessian矩阵块对角结构。

Result: 发现类别数$C$是近块对角结构的主要驱动因素，当$C \rightarrow \infty$时，块对角结构更明显。

Conclusion: 研究结果对理解大语言模型（$C$极大）的Hessian结构提供了新视角。

Abstract: Empirical studies reported that the Hessian matrix of neural networks (NNs)
exhibits a near-block-diagonal structure, yet its theoretical foundation
remains unclear. In this work, we reveal two forces that shape the Hessian
structure: a ``static force'' rooted in the architecture design, and a
``dynamic force'' arisen from training. We then provide a rigorous theoretical
analysis of ``static force'' at random initialization. We study linear models
and 1-hidden-layer networks with the mean-square (MSE) loss and the
Cross-Entropy (CE) loss for classification tasks. By leveraging random matrix
theory, we compare the limit distributions of the diagonal and off-diagonal
Hessian blocks and find that the block-diagonal structure arises as $C
\rightarrow \infty$, where $C$ denotes the number of classes. Our findings
reveal that $C$ is a primary driver of the near-block-diagonal structure. These
results may shed new light on the Hessian structure of large language models
(LLMs), which typically operate with a large $C$ exceeding $10^4$ or $10^5$.

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [141] [Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation](https://arxiv.org/abs/2505.01456)
*Vaidehi Patil,Yi-Lin Sung,Peter Hase,Jie Peng,Tianlong Chen,Mohit Bansal*

Main category: cs.CL

TLDR: 论文提出多模态LLMs可能泄露敏感信息，并引入UnLOK-VQA基准和攻防框架评估多模态遗忘方法。结果显示多模态攻击更有效，最佳防御是删除内部状态信息。


<details>
  <summary>Details</summary>
Motivation: 多模态LLMs可能无意中学习敏感信息，现有遗忘研究集中于文本，多模态遗忘未被充分探索。

Method: 提出UnLOK-VQA基准和攻防框架，扩展视觉问答数据集并评估六种防御目标对抗七种攻击。

Result: 多模态攻击优于单模态攻击，最佳防御是删除内部状态信息；大模型在编辑后更稳健。

Conclusion: UnLOK-VQA为多模态遗忘提供了严格基准，表明模型规模有助于安全性。

Abstract: LLMs trained on massive datasets may inadvertently acquire sensitive
information such as personal details and potentially harmful content. This risk
is further heightened in multimodal LLMs as they integrate information from
multiple modalities (image and text). Adversaries can exploit this knowledge
through multimodal prompts to extract sensitive details. Evaluating how
effectively MLLMs can forget such information (targeted unlearning)
necessitates the creation of high-quality, well-annotated image-text pairs.
While prior work on unlearning has focused on text, multimodal unlearning
remains underexplored. To address this gap, we first introduce a multimodal
unlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as
an attack-and-defense framework to evaluate methods for deleting specific
multimodal knowledge from MLLMs. We extend a visual question-answering dataset
using an automated pipeline that generates varying-proximity samples for
testing generalization and specificity, followed by manual filtering for
maintaining high quality. We then evaluate six defense objectives against seven
attacks (four whitebox, three blackbox), including a novel whitebox method
leveraging interpretability of hidden states. Our results show multimodal
attacks outperform text- or image-only ones, and that the most effective
defense removes answer information from internal model states. Additionally,
larger models exhibit greater post-editing robustness, suggesting that scale
enhances safety. UnLOK-VQA provides a rigorous benchmark for advancing
unlearning in MLLMs.

</details>

### [142] [MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling](https://arxiv.org/abs/2505.01459)
*Abdoul Majid O. Thiombiano,Brahim Hnich,Ali Ben Mrad,Mohamed Wiem Mkaouer*

Main category: cs.CL

TLDR: MoxE结合xLSTM和MoE框架，通过熵感知路由机制和辅助损失函数，显著提升大型语言模型的可扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在可扩展性和效率方面的关键挑战。

Method: 结合xLSTM的创新内存结构和MoE的稀疏性，引入熵感知路由机制和辅助损失函数。

Result: MoxE在效率和效果上显著优于现有方法。

Conclusion: MoxE是大型语言模型架构的重要进步。

Abstract: This paper introduces MoxE, a novel architecture that synergistically
combines the Extended Long Short-Term Memory (xLSTM) with the Mixture of
Experts (MoE) framework to address critical scalability and efficiency
challenges in large language models (LLMs). The proposed method effectively
leverages xLSTM's innovative memory structures while strategically introducing
sparsity through MoE to substantially reduce computational overhead. At the
heart of our approach is a novel entropy-based routing mechanism, designed to
dynamically route tokens to specialized experts, thereby ensuring efficient and
balanced resource utilization. This entropy awareness enables the architecture
to effectively manage both rare and common tokens, with mLSTM blocks being
favored to handle rare tokens. To further enhance generalization, we introduce
a suite of auxiliary losses, including entropy-based and group-wise balancing
losses, ensuring robust performance and efficient training. Theoretical
analysis and empirical evaluations rigorously demonstrate that MoxE achieves
significant efficiency gains and enhanced effectiveness compared to existing
approaches, marking a notable advancement in scalable LLM architectures.

</details>

### [143] [SymPlanner: Deliberate Planning in Language Models with Symbolic Representation](https://arxiv.org/abs/2505.01479)
*Siheng Xiong,Jieyu Zhou,Zhangding Liu,Yusen Su*

Main category: cs.CL

TLDR: SymPlanner是一个新框架，通过符号环境增强语言模型的规划能力，结合迭代校正和对比排名，显著优于纯自然语言基线。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在多步行动序列规划中的挑战，特别是在需要外部约束的领域中。

Method: SymPlanner结合符号环境作为显式世界模型，使用策略模型提出行动，符号环境执行和验证效果。引入迭代校正（IC）和对比排名（CR）优化规划过程。

Result: 在PlanBench上评估，SymPlanner生成的计划更连贯、多样且可验证。

Conclusion: SymPlanner通过符号环境显著提升了语言模型的规划能力。

Abstract: Planning remains a core challenge for language models (LMs), particularly in
domains that require coherent multi-step action sequences grounded in external
constraints. We introduce SymPlanner, a novel framework that equips LMs with
structured planning capabilities by interfacing them with a symbolic
environment that serves as an explicit world model. Rather than relying purely
on natural language reasoning, SymPlanner grounds the planning process in a
symbolic state space, where a policy model proposes actions and a symbolic
environment deterministically executes and verifies their effects. To enhance
exploration and improve robustness, we introduce Iterative Correction (IC),
which refines previously proposed actions by leveraging feedback from the
symbolic environment to eliminate invalid decisions and guide the model toward
valid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained
comparison of candidate plans by evaluating them jointly. We evaluate
SymPlanner on PlanBench, demonstrating that it produces more coherent, diverse,
and verifiable plans than pure natural language baselines.

</details>

### [144] [On the effectiveness of Large Language Models in the mechanical design domain](https://arxiv.org/abs/2505.01559)
*Daniele Grandi,Fabian Riquelme*

Main category: cs.CL

TLDR: 研究了大型语言模型在机械工程领域的性能，通过ABC数据集评估了两种无监督任务，发现模型在特定任务中表现良好，但也揭示了该领域的特定失败模式。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在机械工程领域的性能表现，以了解其在该领域的适用性和局限性。

Method: 利用ABC数据集的语义数据，设计了两种无监督任务：二元句子对分类任务和零样本分类任务，并通过调整学习率、dropout值、序列长度和增加多头注意力层来优化模型。

Result: 二元句子对分类任务准确率为0.62，零样本分类任务准确率为0.386，显著优于基线模型。

Conclusion: 研究揭示了大型语言模型在机械工程领域的潜力，但也指出了该领域的特定挑战和失败模式。

Abstract: In this work, we seek to understand the performance of large language models
in the mechanical engineering domain. We leverage the semantic data found in
the ABC dataset, specifically the assembly names that designers assigned to the
overall assemblies, and the individual semantic part names that were assigned
to each part. After pre-processing the data we developed two unsupervised tasks
to evaluate how different model architectures perform on domain-specific data:
a binary sentence-pair classification task and a zero-shot classification task.
We achieved a 0.62 accuracy for the binary sentence-pair classification task
with a fine-tuned model that focuses on fighting over-fitting: 1) modifying
learning rates, 2) dropout values, 3) Sequence Length, and 4) adding a
multi-head attention layer. Our model on the zero-shot classification task
outperforms the baselines by a wide margin, and achieves a top-1 classification
accuracy of 0.386. The results shed some light on the specific failure modes
that arise when learning from language in this domain.

</details>

### [145] [AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains](https://arxiv.org/abs/2505.01560)
*Vicent Briva Iglesias,Gokhan Dogru*

Main category: cs.CL

TLDR: 本文比较了大型语言模型（LLM）和多智能体编排与传统神经机器翻译（NMT）的表现，发现NMT在自动评估中表现最佳，而增强推理的LLM在人工评估中更优，但成本显著更高。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM和多智能体编排在机器翻译中的实际效果，明确其相对于传统NMT的优势与不足。

Method: 对五种翻译范式（包括NMT、LLM和多智能体工作流）进行自动和人工评估，使用多种指标和语言对。

Result: NMT在自动评估中领先，而增强推理的LLM在人工评估中表现更佳，但多智能体工作流成本极高。

Conclusion: 建议采用多维、成本感知的评估方法，并探索更高效的协调策略和混合管道。

Abstract: Large language models (LLMs) and multi-agent orchestration are touted as the
next leap in machine translation (MT), but their benefits relative to
conventional neural MT (NMT) remain unclear. This paper offers an empirical
reality check. We benchmark five paradigms, Google Translate (strong NMT
baseline), GPT-4o (general-purpose LLM), o1-preview (reasoning-enhanced LLM),
and two GPT-4o-powered agentic workflows (sequential three-stage and iterative
refinement), on test data drawn from a legal contract and news prose in three
English-source pairs: Spanish, Catalan and Turkish. Automatic evaluation is
performed with COMET, BLEU, chrF2 and TER; human evaluation is conducted with
expert ratings of adequacy and fluency; efficiency with total input-plus-output
token counts mapped to April 2025 pricing.
  Automatic scores still favour the mature NMT system, which ranks first in
seven of twelve metric-language combinations; o1-preview ties or places second
in most remaining cases, while both multi-agent workflows trail. Human
evaluation reverses part of this narrative: o1-preview produces the most
adequate and fluent output in five of six comparisons, and the iterative agent
edges ahead once, indicating that reasoning layers capture semantic nuance
undervalued by surface metrics. Yet these qualitative gains carry steep costs.
The sequential agent consumes roughly five times, and the iterative agent
fifteen times, the tokens used by NMT or single-pass LLMs.
  We advocate multidimensional, cost-aware evaluation protocols and highlight
research directions that could tip the balance: leaner coordination strategies,
selective agent activation, and hybrid pipelines combining single-pass LLMs
with targeted agent intervention.

</details>

### [146] [PIPA: A Unified Evaluation Protocol for Diagnosing Interactive Planning Agents](https://arxiv.org/abs/2505.01592)
*Takyoung Kim,Janvijay Singh,Shuhaib Mehri,Emre Can Acikgoz,Sagnik Mukherjee,Nimet Beyza Bozdag,Sumuk Shashidhar,Gokhan Tur,Dilek Hakkani-Tür*

Main category: cs.CL

TLDR: 论文提出PIPA协议，通过POMDP范式评估任务规划代理的行为过程，强调用户满意度不仅取决于任务完成度，还包括中间行为。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估任务完成度，但用户满意度涉及整个代理过程，需要更全面的评估方法。

Method: 提出PIPA协议，基于POMDP范式设计原子评估标准，分析代理决策管道的具体表现。

Result: 代理在不同行为阶段表现各异，用户满意度受结果和中间行为共同影响。

Conclusion: PIPA为代理评估提供新视角，未来可探索多代理系统和用户模拟器的局限性。

Abstract: The growing capabilities of large language models (LLMs) in
instruction-following and context-understanding lead to the era of agents with
numerous applications. Among these, task planning agents have become especially
prominent in realistic scenarios involving complex internal pipelines, such as
context understanding, tool management, and response generation. However,
existing benchmarks predominantly evaluate agent performance based on task
completion as a proxy for overall effectiveness. We hypothesize that merely
improving task completion is misaligned with maximizing user satisfaction, as
users interact with the entire agentic process and not only the end result. To
address this gap, we propose PIPA, a unified evaluation protocol that
conceptualizes the behavioral process of interactive task planning agents
within a partially observable Markov Decision Process (POMDP) paradigm. The
proposed protocol offers a comprehensive assessment of agent performance
through a set of atomic evaluation criteria, allowing researchers and
practitioners to diagnose specific strengths and weaknesses within the agent's
decision-making pipeline. Our analyses show that agents excel in different
behavioral stages, with user satisfaction shaped by both outcomes and
intermediate behaviors. We also highlight future directions, including systems
that leverage multiple agents and the limitations of user simulators in task
planning.

</details>

### [147] [Always Tell Me The Odds: Fine-grained Conditional Probability Estimation](https://arxiv.org/abs/2505.01595)
*Liaoyaqi Wang,Zhengping Jiang,Anqi Liu,Benjamin Van Durme*

Main category: cs.CL

TLDR: 提出了一种用于细粒度概率估计的先进模型，解决了LLM在不确定或部分信息下概率预测不准确的问题。


<details>
  <summary>Details</summary>
Motivation: LLM在不确定或部分信息下的概率预测存在偏差和不准确，且不确定性估计的研究不足。

Method: 结合人类和合成数据创建与评估，扩展到更大模型，改进监督方法。

Result: 在依赖条件概率估计的任务中，新方法显著优于现有微调和基于提示的方法。

Conclusion: 通过改进数据、模型和监督方法，显著提升了LLM在概率估计任务中的表现。

Abstract: We present a state-of-the-art model for fine-grained probability estimation
of propositions conditioned on context. Recent advances in large language
models (LLMs) have significantly enhanced their reasoning capabilities,
particularly on well-defined tasks with complete information. However, LLMs
continue to struggle with making accurate and well-calibrated probabilistic
predictions under uncertainty or partial information. While incorporating
uncertainty into model predictions often boosts performance, obtaining reliable
estimates of that uncertainty remains understudied. In particular, LLM
probability estimates tend to be coarse and biased towards more frequent
numbers. Through a combination of human and synthetic data creation and
assessment, scaling to larger models, and better supervision, we propose a set
of strong and precise probability estimation models. We conduct systematic
evaluations across tasks that rely on conditional probability estimation and
show that our approach consistently outperforms existing fine-tuned and
prompting-based methods by a large margin.

</details>

### [148] [A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency](https://arxiv.org/abs/2505.01658)
*Sihyeong Park,Sungryeol Jeon,Chaelyn Lee,Seokhun Jeon,Byung-Soo Kim,Jemin Lee*

Main category: cs.CL

TLDR: 本文对25种开源和商业LLM推理引擎进行全面评估，涵盖易用性、部署、通用性、扩展性及性能，并探讨其设计目标和优化技术。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在多种应用中的广泛使用，推理成本增加，但缺乏对推理引擎的系统研究。

Method: 评估25种推理引擎，分析其易用性、部署、通用性、扩展性及性能，并探讨优化技术。

Result: 提供了开源和商业推理引擎的详细评估结果，包括生态系统成熟度和性能成本策略。

Conclusion: 未来研究方向包括支持复杂LLM服务、多样化硬件和增强安全性，为开发者提供实用指南。

Abstract: Large language models (LLMs) are widely applied in chatbots, code generators,
and search engines. Workloads such as chain-of-thought, complex reasoning, and
agent services significantly increase the inference cost by invoking the model
repeatedly. Optimization methods such as parallelism, compression, and caching
have been adopted to reduce costs, but the diverse service requirements make it
hard to select the right method. Recently, specialized LLM inference engines
have emerged as a key component for integrating the optimization methods into
service-oriented infrastructures. However, a systematic study on inference
engines is still lacking. This paper provides a comprehensive evaluation of 25
open-source and commercial inference engines. We examine each inference engine
in terms of ease-of-use, ease-of-deployment, general-purpose support,
scalability, and suitability for throughput- and latency-aware computation.
Furthermore, we explore the design goals of each inference engine by
investigating the optimization techniques it supports. In addition, we assess
the ecosystem maturity of open source inference engines and handle the
performance and cost policy of commercial solutions. We outline future research
directions that include support for complex LLM-based services, support of
various hardware, and enhanced security, offering practical guidance to
researchers and developers in selecting and designing optimized LLM inference
engines. We also provide a public repository to continually track developments
in this fast-evolving field:
https://github.com/sihyeong/Awesome-LLM-Inference-Engine

</details>

### [149] [High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers](https://arxiv.org/abs/2505.01693)
*Brian Wong,Kaito Tanaka*

Main category: cs.CL

TLDR: DeBERTa-RAD是一种两阶段框架，结合LLM伪标记和DeBERTa知识蒸馏，高效准确标记胸部X光报告。


<details>
  <summary>Details</summary>
Motivation: 胸部X光报告的自由文本具有高变异性、复杂性和否定/不确定性，传统NLP方法难以处理，LLM直接应用又受限于计算成本和速度。

Method: 使用先进LLM生成高质量伪标记，再通过知识蒸馏训练DeBERTa-Base模型。

Result: 在MIMIC-500基准测试中，Macro F1得分0.9120，显著优于其他方法，且推理速度快。

Conclusion: 结合LLM能力和高效蒸馏学生模型，可突破数据标注瓶颈，实现高性能医学文本处理。

Abstract: Automated labeling of chest X-ray reports is essential for enabling
downstream tasks such as training image-based diagnostic models, population
health studies, and clinical decision support. However, the high variability,
complexity, and prevalence of negation and uncertainty in these free-text
reports pose significant challenges for traditional Natural Language Processing
methods. While large language models (LLMs) demonstrate strong text
understanding, their direct application for large-scale, efficient labeling is
limited by computational cost and speed. This paper introduces DeBERTa-RAD, a
novel two-stage framework that combines the power of state-of-the-art LLM
pseudo-labeling with efficient DeBERTa-based knowledge distillation for
accurate and fast chest X-ray report labeling. We leverage an advanced LLM to
generate high-quality pseudo-labels, including certainty statuses, for a large
corpus of reports. Subsequently, a DeBERTa-Base model is trained on this
pseudo-labeled data using a tailored knowledge distillation strategy. Evaluated
on the expert-annotated MIMIC-500 benchmark, DeBERTa-RAD achieves a
state-of-the-art Macro F1 score of 0.9120, significantly outperforming
established rule-based systems, fine-tuned transformer models, and direct LLM
inference, while maintaining a practical inference speed suitable for
high-throughput applications. Our analysis shows particular strength in
handling uncertain findings. This work demonstrates a promising path to
overcome data annotation bottlenecks and achieve high-performance medical text
processing through the strategic combination of LLM capabilities and efficient
student models trained via distillation.

</details>

### [150] [Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models](https://arxiv.org/abs/2505.01731)
*Chuan Sun,Han Yu,Lizhen Cui*

Main category: cs.CL

TLDR: 提出了一种基于Shapley值的非均匀剪枝方法（SVNP），用于优化大语言模型的剪枝效果，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统均匀剪枝方法未考虑不同Transformer层的重要性差异，导致性能不佳。

Method: 通过Shapley值量化每层对模型性能的贡献，为不同层分配定制化剪枝预算，并设计滑动窗口近似方法降低计算开销。

Result: 在LLaMA和OPT等模型上验证，SVNP在70%稀疏度下比SparseGPT降低困惑度18.01%（LLaMA-7B）和19.55%（LLaMA-13B）。

Conclusion: 非均匀剪枝方法显著提升剪枝后模型性能，SVNP是一种高效且有效的剪枝策略。

Abstract: Pruning large language models (LLMs) is a promising solution for reducing
model sizes and computational complexity while preserving performance.
Traditional layer-wise pruning methods often adopt a uniform sparsity approach
across all layers, which leads to suboptimal performance due to the varying
significance of individual transformer layers within the model not being
accounted for. To this end, we propose the \underline{S}hapley
\underline{V}alue-based \underline{N}on-\underline{U}niform \underline{P}runing
(\methodname{}) method for LLMs. This approach quantifies the contribution of
each transformer layer to the overall model performance, enabling the
assignment of tailored pruning budgets to different layers to retain critical
parameters. To further improve efficiency, we design the Sliding Window-based
Shapley Value approximation method. It substantially reduces computational
overhead compared to exact SV calculation methods. Extensive experiments on
various LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness
of the proposed approach. The results reveal that non-uniform pruning
significantly enhances the performance of pruned models. Notably, \methodname{}
achieves a reduction in perplexity (PPL) of 18.01\% and 19.55\% on LLaMA-7B and
LLaMA-13B, respectively, compared to SparseGPT at 70\% sparsity.

</details>

### [151] [Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models](https://arxiv.org/abs/2505.01761)
*Tobias Domhan,Dawei Zhu*

Main category: cs.CL

TLDR: 研究发现，长文本会影响LLM对翻译质量的评估，导致错误标注减少和系统排名准确性下降。通过优化提示策略和微调方法，可以缓解这种长度偏差。


<details>
  <summary>Details</summary>
Motivation: 解决长文档翻译评估中LLM因文本长度导致的评估不一致问题。

Method: 采用粒度对齐提示、焦点句子提示（FSP）和微调方法，优化LLM的评估表现。

Result: FSP和微调方法显著减少了长度偏差，提升了LLM在长文本翻译评估中的可靠性。

Conclusion: 通过特定策略优化，LLM可以更可靠地用于长文档翻译质量评估。

Abstract: Accurately evaluating machine-translated text remains a long-standing
challenge, particularly for long documents. Recent work has shown that large
language models (LLMs) can serve as reliable and interpretable sentence-level
translation evaluators via MQM error span annotations. With modern LLMs
supporting larger context windows, a natural question arises: can we feed
entire document translations into an LLM for quality assessment? Ideally,
evaluation should be invariant to text length, producing consistent error spans
regardless of input granularity. However, our analysis shows that text length
significantly impacts evaluation: longer texts lead to fewer error spans and
reduced system ranking accuracy. To address this limitation, we evaluate
several strategies, including granularity-aligned prompting, Focus Sentence
Prompting (FSP), and a fine-tuning approach to better align LLMs with the
evaluation task. The latter two methods largely mitigate this length bias,
making LLMs more reliable for long-form translation evaluation.

</details>

### [152] [A Multimodal Framework for Explainable Evaluation of Soft Skills in Educational Environments](https://arxiv.org/abs/2505.01794)
*Jared D. T. Guerrero-Sosa,Francisco P. Romero,Víctor Hugo Menéndez-Domínguez,Jesus Serrano-Guerrero,Andres Montoro-Montarroso,Jose A. Olivas*

Main category: cs.CL

TLDR: 本文提出了一种基于模糊逻辑和语言模型的方法，结合多模态分析评估本科生软技能，提高了评估的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在高等教育中，软技能的公正评估是一个重要挑战，传统方法难以捕捉其复杂性和不确定性。

Method: 采用模糊逻辑和语言模型，结合多模态分析（如面部表情和手势识别），开发工具评估软技能。

Result: 实验表明，该方法能有效整合多模态数据，生成一致且透明的软技能评估结果。

Conclusion: 多模态整合显著提升了软技能评估质量，为教育利益相关者提供了透明且可理解的工具。

Abstract: In the rapidly evolving educational landscape, the unbiased assessment of
soft skills is a significant challenge, particularly in higher education. This
paper presents a fuzzy logic approach that employs a Granular Linguistic Model
of Phenomena integrated with multimodal analysis to evaluate soft skills in
undergraduate students. By leveraging computational perceptions, this approach
enables a structured breakdown of complex soft skill expressions, capturing
nuanced behaviours with high granularity and addressing their inherent
uncertainties, thereby enhancing interpretability and reliability. Experiments
were conducted with undergraduate students using a developed tool that assesses
soft skills such as decision-making, communication, and creativity. This tool
identifies and quantifies subtle aspects of human interaction, such as facial
expressions and gesture recognition. The findings reveal that the framework
effectively consolidates multiple data inputs to produce meaningful and
consistent assessments of soft skills, showing that integrating multiple
modalities into the evaluation process significantly improves the quality of
soft skills scores, making the assessment work transparent and understandable
to educational stakeholders.

</details>

### [153] [Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis](https://arxiv.org/abs/2505.01800)
*Chidimma Opara*

Main category: cs.CL

TLDR: 论文提出了一种结合风格计量分析和心理语言学理论的框架，用于区分AI生成和人类写作的文本，旨在维护学术诚信。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成文本的日益复杂，教育环境中对准确透明的检测工具需求迫切。

Method: 研究整合了31种风格计量特征与心理语言学理论，映射到认知过程如词汇检索和元认知自我监控。

Result: 框架揭示了人类写作的独特心理语言学模式，为开发可靠检测工具提供了基础。

Conclusion: 该研究通过计算语言学与认知科学的结合，为生成AI时代的学术诚信保护提供了新方法。

Abstract: The increasing sophistication of AI-generated texts highlights the urgent
need for accurate and transparent detection tools, especially in educational
settings, where verifying authorship is essential. Existing literature has
demonstrated that the application of stylometric features with machine learning
classifiers can yield excellent results. Building on this foundation, this
study proposes a comprehensive framework that integrates stylometric analysis
with psycholinguistic theories, offering a clear and interpretable approach to
distinguishing between AI-generated and human-written texts. This research
specifically maps 31 distinct stylometric features to cognitive processes such
as lexical retrieval, discourse planning, cognitive load management, and
metacognitive self-monitoring. In doing so, it highlights the unique
psycholinguistic patterns found in human writing. Through the intersection of
computational linguistics and cognitive science, this framework contributes to
the development of reliable tools aimed at preserving academic integrity in the
era of generative AI.

</details>

### [154] [$\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge](https://arxiv.org/abs/2505.01812)
*Core Francisco Park,Zechen Zhang,Hidenori Tanaka*

Main category: cs.CL

TLDR: 论文提出了一种名为$	extit{New News}$的数据集，用于评估模型在理解新闻后执行下游任务的能力。研究发现，微调与上下文学习之间存在显著差距（FT-ICL gap），并提出了$	extit{System-2 Fine-tuning}$（Sys2-FT）方法，通过自生成数据（如改写、推理和自问自答）提升模型对新闻的学习能力。实验表明，Sys2-FT显著改善了模型的学习效果，并揭示了$	extit{contextual shadowing effect}$现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索大型语言模型（LLMs）如何通过微调有效学习新信息（新闻），并解决微调与上下文学习之间的性能差距。

Method: 方法包括构建$	extit{New News}$数据集，提出$	extit{System-2 Fine-tuning}$（Sys2-FT）方法，通过自生成数据（改写、推理和自问自答）提升模型学习能力。

Result: 结果显示，Sys2-FT显著改善了模型对新闻的学习能力，并揭示了$	extit{contextual shadowing effect}$现象。同时，初步发现了Sys2-FT的缩放规律。

Conclusion: 结论表明，Sys2-FT是一种有效的方法，能够提升模型对新闻的学习能力，并揭示了上下文学习中的潜在问题。

Abstract: Humans and intelligent animals can effortlessly internalize new information
("news") and accurately extract the implications for performing downstream
tasks. While large language models (LLMs) can achieve this through in-context
learning (ICL) when the news is explicitly given as context, fine-tuning
remains challenging for the models to consolidate learning in weights. In this
paper, we introduce $\textit{New News}$, a dataset composed of hypothetical yet
plausible news spanning multiple domains (mathematics, coding, discoveries,
leaderboards, events), accompanied by downstream evaluation questions whose
correct answers critically depend on understanding and internalizing the news.
We first demonstrate a substantial gap between naive fine-tuning and in-context
learning (FT-ICL gap) on our news dataset. To address this gap, we explore a
suite of self-play data generation protocols -- paraphrases, implications and
Self-QAs -- designed to distill the knowledge from the model with context into
the weights of the model without the context, which we term $\textit{System-2
Fine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance
across data domains and model scales with the Qwen 2.5 family of models. Our
results demonstrate that the self-QA protocol of Sys2-FT significantly improves
models' in-weight learning of the news. Furthermore, we discover the
$\textit{contexual shadowing effect}$, where training with the news $\textit{in
context}$ followed by its rephrases or QAs degrade learning of the news.
Finally, we show preliminary evidence of an emerging scaling law of Sys2-FT.

</details>

### [155] [Intra-Layer Recurrence in Transformers for Language Modeling](https://arxiv.org/abs/2505.01855)
*Anthony Nguyen,Wenjun Lin*

Main category: cs.CL

TLDR: 提出了一种针对Transformer模型的层内循环（ILR）方法，通过选择性循环优化参数效率。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer模型因深度增加导致的参数爆炸问题，现有方法对整块层循环效率不足。

Method: 采用层内循环（ILR），在单次前向传播中仅对特定层进行循环处理。

Result: 实验表明，对早期层分配更多循环次数效果最佳。

Conclusion: ILR为Transformer架构的循环结构优化提供了新方向。

Abstract: Transformer models have established new benchmarks in natural language
processing; however, their increasing depth results in substantial growth in
parameter counts. While existing recurrent transformer methods address this
issue by reprocessing layers multiple times, they often apply recurrence
indiscriminately across entire blocks of layers. In this work, we investigate
Intra-Layer Recurrence (ILR), a more targeted approach that applies recurrence
selectively to individual layers within a single forward pass. Our experiments
show that allocating more iterations to earlier layers yields optimal results.
These findings suggest that ILR offers a promising direction for optimizing
recurrent structures in transformer architectures.

</details>

### [156] [Positional Attention for Efficient BERT-Based Named Entity Recognition](https://arxiv.org/abs/2505.01868)
*Mo Sun,Siheng Xiong,Yuankai Cai,Bowen Zuo*

Main category: cs.CL

TLDR: 提出了一种基于BERT的NER框架，通过引入位置注意力机制和预训练参数，降低训练成本并保持高准确率。


<details>
  <summary>Details</summary>
Motivation: BERT在NER任务中表现优异，但从头微调成本高且耗时，需要一种更高效的解决方案。

Method: 集成位置注意力机制，利用预训练参数实现高效定制化。

Result: 在Groningen Meaning Bank数据集上表现优异，训练周期更少。

Conclusion: 该框架为降低BERT-based NER系统的训练成本提供了实用方案，同时保持高准确率。

Abstract: This paper presents a framework for Named Entity Recognition (NER) leveraging
the Bidirectional Encoder Representations from Transformers (BERT) model in
natural language processing (NLP). NER is a fundamental task in NLP with broad
applicability across downstream applications. While BERT has established itself
as a state-of-the-art model for entity recognition, fine-tuning it from scratch
for each new application is computationally expensive and time-consuming. To
address this, we propose a cost-efficient approach that integrates positional
attention mechanisms into the entity recognition process and enables effective
customization using pre-trained parameters. The framework is evaluated on a
Kaggle dataset derived from the Groningen Meaning Bank corpus and achieves
strong performance with fewer training epochs. This work contributes to the
field by offering a practical solution for reducing the training cost of
BERT-based NER systems while maintaining high accuracy.

</details>

### [157] [Humans can learn to detect AI-generated texts, or at least learn when they can't](https://arxiv.org/abs/2505.01877)
*Jiří Milička,Anna Marklová,Ondřej Drobil,Eva Pospíšilová*

Main category: cs.CL

TLDR: 研究发现，通过即时反馈，人们可以学习区分人类写作和AI生成文本，并调整自我认知能力。反馈组在准确性和信心校准上表现更好。


<details>
  <summary>Details</summary>
Motivation: 探讨人们是否能通过反馈学习区分人类和AI生成文本，并了解其判断依据。

Method: 使用GPT-4生成文本，与人类文本对比，由255名捷克母语者判断，分为反馈组和无反馈组。

Result: 反馈组在准确性和信心校准上显著提升，纠正了对AI文本特征的误解。

Conclusion: 通过反馈训练可以有效区分人类和AI文本，对教育领域尤为重要。

Abstract: This study investigates whether individuals can learn to accurately
discriminate between human-written and AI-produced texts when provided with
immediate feedback, and if they can use this feedback to recalibrate their
self-perceived competence. We also explore the specific criteria individuals
rely upon when making these decisions, focusing on textual style and perceived
readability.
  We used GPT-4o to generate several hundred texts across various genres and
text types comparable to Koditex, a multi-register corpus of human-written
texts. We then presented randomized text pairs to 255 Czech native speakers who
identified which text was human-written and which was AI-generated.
Participants were randomly assigned to two conditions: one receiving immediate
feedback after each trial, the other receiving no feedback until experiment
completion. We recorded accuracy in identification, confidence levels, response
times, and judgments about text readability along with demographic data and
participants' engagement with AI technologies prior to the experiment.
  Participants receiving immediate feedback showed significant improvement in
accuracy and confidence calibration. Participants initially held incorrect
assumptions about AI-generated text features, including expectations about
stylistic rigidity and readability. Notably, without feedback, participants
made the most errors precisely when feeling most confident -- an issue largely
resolved among the feedback group.
  The ability to differentiate between human and AI-generated texts can be
effectively learned through targeted training with explicit feedback, which
helps correct misconceptions about AI stylistic features and readability, as
well as potential other variables that were not explored, while facilitating
more accurate self-assessment. This finding might be particularly important in
educational contexts.

</details>

### [158] [Automated Sentiment Classification and Topic Discovery in Large-Scale Social Media Streams](https://arxiv.org/abs/2505.01883)
*Yiwen Lu,Siheng Xiong,Zhaowei Li*

Main category: cs.CL

TLDR: 提出一个用于大规模Twitter情感和主题分析的框架，包括数据收集、情感标注、主题建模和可视化。


<details>
  <summary>Details</summary>
Motivation: 研究社交媒体在动态地缘政治背景下的情感和主题分布，提供可扩展的分析方法。

Method: 使用冲突相关关键词收集数据，多模型情感标注，结合上下文特征分析，LDA主题建模，开发交互式可视化工具。

Result: 建立了可扩展的分析流程，揭示了情感与上下文特征的关系，并支持动态探索。

Conclusion: 该框架为动态地缘政治背景下的社交媒体分析提供了有效工具。

Abstract: We present a framework for large-scale sentiment and topic analysis of
Twitter discourse. Our pipeline begins with targeted data collection using
conflict-specific keywords, followed by automated sentiment labeling via
multiple pre-trained models to improve annotation robustness. We examine the
relationship between sentiment and contextual features such as timestamp,
geolocation, and lexical content. To identify latent themes, we apply Latent
Dirichlet Allocation (LDA) on partitioned subsets grouped by sentiment and
metadata attributes. Finally, we develop an interactive visualization interface
to support exploration of sentiment trends and topic distributions across time
and regions. This work contributes a scalable methodology for social media
analysis in dynamic geopolitical contexts.

</details>

### [159] [CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation](https://arxiv.org/abs/2505.01900)
*Mazal Bethany,Nishant Vishwamitra,Cho-Yu Jason Chiang,Peyman Najafirad*

Main category: cs.CL

TLDR: CAMOUFLAGE是一种基于LLM的对抗攻击方法，通过两代理系统（Prompt Optimization Agent和Attacker Agent）生成语义等效但能误导证据检索和比较的改写，成功绕过基于证据的虚假信息检测系统。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒文本对抗攻击方法无法有效攻击基于证据的虚假信息检测系统，因其多组件特性需要同时破坏证据检索和比较模块。

Method: CAMOUFLAGE采用两代理系统迭代优化攻击：Attacker Agent生成语义等效改写，Prompt Optimization Agent分析失败并优化提示以指导后续改写。

Result: 在四个系统（包括学术和实际API）上平均攻击成功率为46.92%，同时保持文本连贯性和语义等效性。

Conclusion: CAMOUFLAGE展示了无需依赖分类器logits或大量查询即可有效攻击多组件检测系统的潜力。

Abstract: Automated evidence-based misinformation detection systems, which evaluate the
veracity of short claims against evidence, lack comprehensive analysis of their
adversarial vulnerabilities. Existing black-box text-based adversarial attacks
are ill-suited for evidence-based misinformation detection systems, as these
attacks primarily focus on token-level substitutions involving gradient or
logit-based optimization strategies, which are incapable of fooling the
multi-component nature of these detection systems. These systems incorporate
both retrieval and claim-evidence comparison modules, which requires attacks to
break the retrieval of evidence and/or the comparison module so that it draws
incorrect inferences. We present CAMOUFLAGE, an iterative, LLM-driven approach
that employs a two-agent system, a Prompt Optimization Agent and an Attacker
Agent, to create adversarial claim rewritings that manipulate evidence
retrieval and mislead claim-evidence comparison, effectively bypassing the
system without altering the meaning of the claim. The Attacker Agent produces
semantically equivalent rewrites that attempt to mislead detectors, while the
Prompt Optimization Agent analyzes failed attack attempts and refines the
prompt of the Attacker to guide subsequent rewrites. This enables larger
structural and stylistic transformations of the text rather than token-level
substitutions, adapting the magnitude of changes based on previous outcomes.
Unlike existing approaches, CAMOUFLAGE optimizes its attack solely based on
binary model decisions to guide its rewriting process, eliminating the need for
classifier logits or extensive querying. We evaluate CAMOUFLAGE on four
systems, including two recent academic systems and two real-world APIs, with an
average attack success rate of 46.92\% while preserving textual coherence and
semantic equivalence to the original claims.

</details>

### [160] [Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview](https://arxiv.org/abs/2505.01967)
*Jiatao Li,Yanheng Li,Xiaojun Wan*

Main category: cs.CL

TLDR: 论文提出了一种基于文化理论的Social Worldview Taxonomy (SWT)框架，用于测量LLMs中的世界观偏好，并发现社会线索能显著影响这些偏好。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs中隐含的社会认知态度（世界观），填补现有研究对权威、平等、自主和命运等维度探索的不足。

Method: 引入SWT框架，将四种典型世界观（等级制、平等主义、个人主义、宿命论）转化为可测量的子维度，并对28种LLMs进行实证分析。

Result: 发现LLMs具有可解释的认知模式，且社会线索能系统性影响这些态度，揭示了普遍模式和模型特异性差异。

Conclusion: 研究增强了LLMs的可解释性，揭示了其隐含的社会认知偏见及对社会反馈的响应，为开发更透明、负责任的AI技术提供指导。

Abstract: Large Language Models (LLMs) have become integral to daily life, widely
adopted in communication, decision-making, and information retrieval, raising
critical questions about how these systems implicitly form and express
socio-cognitive attitudes or "worldviews". While existing research extensively
addresses demographic and ethical biases, broader dimensions-such as attitudes
toward authority, equality, autonomy, and fate-remain under-explored. In this
paper, we introduce the Social Worldview Taxonomy (SWT), a structured framework
grounded in Cultural Theory, operationalizing four canonical worldviews
(Hierarchy, Egalitarianism, Individualism, Fatalism) into measurable
sub-dimensions. Using SWT, we empirically identify distinct and interpretable
cognitive profiles across 28 diverse LLMs. Further, inspired by Social
Referencing Theory, we experimentally demonstrate that explicit social cues
systematically shape these cognitive attitudes, revealing both general response
patterns and nuanced model-specific variations. Our findings enhance the
interpretability of LLMs by revealing implicit socio-cognitive biases and their
responsiveness to social feedback, thus guiding the development of more
transparent and socially responsible language technologies.

</details>

### [161] [LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load](https://arxiv.org/abs/2505.01980)
*Theo Guidroz,Diego Ardila,Jimmy Li,Adam Mansour,Paul Jhun,Nina Gonzalez,Xiang Ji,Mike Sanchez,Sujay Kakarmath,Mathias MJ Bellaiche,Miguel Ángel Garrido,Faruk Ahmed,Divyansh Choudhary,Jay Hartford,Chenwei Xu,Henry Javier Serrano Echeverria,Yifan Wang,Jeff Shaffer,Eric,Cao,Yossi Matias,Avinatan Hassidim,Dale R Webster,Yun Liu,Sho Fujiwara,Peggy Bui,Quang Duong*

Main category: cs.CL

TLDR: 该论文提出了一种基于自优化方法的LLM文本简化技术，通过大规模随机实验验证了简化文本对用户理解的显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 解决网络信息（如科学文献和维基百科）超出用户阅读水平的问题，提升信息可访问性。

Method: 采用自优化方法开发LLM文本简化能力，并通过4563名参与者的随机实验验证效果。

Result: 简化文本显著提升了用户的理解能力（绝对提升3.9%），尤其在生物医学领域效果最显著（14.6%）。

Conclusion: LLM文本简化技术能有效帮助用户理解复杂信息，未来可推广以提升网络信息的可访问性。

Abstract: Information on the web, such as scientific publications and Wikipedia, often
surpasses users' reading level. To help address this, we used a self-refinement
approach to develop a LLM capability for minimally lossy text simplification.
To validate our approach, we conducted a randomized study involving 4563
participants and 31 texts spanning 6 broad subject areas: PubMed (biomedical
scientific articles), biology, law, finance, literature/philosophy, and
aerospace/computer science. Participants were randomized to viewing original or
simplified texts in a subject area, and answered multiple-choice questions
(MCQs) that tested their comprehension of the text. The participants were also
asked to provide qualitative feedback such as task difficulty. Our results
indicate that participants who read the simplified text answered more MCQs
correctly than their counterparts who read the original text (3.9% absolute
increase, p<0.05). This gain was most striking with PubMed (14.6%), while more
moderate gains were observed for finance (5.5%), aerospace/computer science
(3.8%) domains, and legal (3.5%). Notably, the results were robust to whether
participants could refer back to the text while answering MCQs. The absolute
accuracy decreased by up to ~9% for both original and simplified setups where
participants could not refer back to the text, but the ~4% overall improvement
persisted. Finally, participants' self-reported perceived ease based on a
simplified NASA Task Load Index was greater for those who read the simplified
text (absolute change on a 5-point scale 0.33, p<0.05). This randomized study,
involving an order of magnitude more participants than prior works,
demonstrates the potential of LLMs to make complex information easier to
understand. Our work aims to enable a broader audience to better learn and make
use of expert knowledge available on the web, improving information
accessibility.

</details>

### [162] [Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs](https://arxiv.org/abs/2505.02009)
*Sai Krishna Mendu,Harish Yenala,Aditi Gulati,Shanu Kumar,Parag Agrawal*

Main category: cs.CL

TLDR: 该论文分析了大型语言模型（LLMs）预训练数据中的有害内容，提出了分类方法、评估数据集和过滤模型，旨在促进更安全的LLM预训练和负责任AI的发展。


<details>
  <summary>Details</summary>
Motivation: 预训练数据中的有害内容可能导致LLMs传播错误信息和偏见，引发伦理问题，因此需要系统分析和过滤方法。

Method: 论文通过大规模分析数据集中的不当内容，提出分类法（Topical和Toxic），并开发了评估数据集（TTP）、过滤模型（HarmFormer）和毒性基准（HAVOC）。

Result: 研究提供了有害内容的分类方法、高精度过滤模型和对抗性毒性输入的响应分析，为安全预训练和RAI合规提供了资源。

Conclusion: 该研究为LLM预训练数据的安全性提供了系统性解决方案，并推动了负责任AI的发展。

Abstract: Large language models (LLMs) have become integral to various real-world
applications, leveraging massive, web-sourced datasets like Common Crawl, C4,
and FineWeb for pretraining. While these datasets provide linguistic data
essential for high-quality natural language generation, they often contain
harmful content, such as hate speech, misinformation, and biased narratives.
Training LLMs on such unfiltered data risks perpetuating toxic behaviors,
spreading misinformation, and amplifying societal biases which can undermine
trust in LLM-driven applications and raise ethical concerns about their use.
This paper presents a large-scale analysis of inappropriate content across
these datasets, offering a comprehensive taxonomy that categorizes harmful
webpages into Topical and Toxic based on their intent. We also introduce a
prompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and
a transformer-based model (HarmFormer) for content filtering. Additionally, we
create a new multi-harm open-ended toxicity benchmark (HAVOC) and provide
crucial insights into how models respond to adversarial toxic inputs. Upon
publishing, we will also opensource our model signal on the entire C4 dataset.
Our work offers insights into ensuring safer LLM pretraining and serves as a
resource for Responsible AI (RAI) compliance.

</details>

### [163] [An overview of artificial intelligence in computer-assisted language learning](https://arxiv.org/abs/2505.02032)
*Anisia Katinskaia*

Main category: cs.CL

TLDR: 本文综述了人工智能在计算机辅助语言学习（CALL）中的应用，探讨了智能代理如何支持语言学习与教学，并指出了当前研究的局限性与未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着语言学习需求的增长，人类教师资源有限且成本高，加之疫情、移民等因素，亟需智能化的CALL系统。

Method: 通过综述现有研究和实践，分析AI在CALL中的应用方法，并从开发者视角提出改进方向。

Result: 当前CALL系统多为原型或部分实现，完整解决方案较少，但AI的进步有望推动其发展。

Conclusion: 本文为跨学科研究搭建桥梁，并呼吁更多关注AI在CALL中的潜力。

Abstract: Computer-assisted language learning -- CALL -- is an established research
field. We review how artificial intelligence can be applied to support language
learning and teaching. The need for intelligent agents that assist language
learners and teachers is increasing: the human teacher's time is a scarce and
costly resource, which does not scale with growing demand. Further factors
contribute to the need for CALL: pandemics and increasing demand for distance
learning, migration of large populations, the need for sustainable and
affordable support for learning, etc. CALL systems are made up of many
components that perform various functions, and AI is applied to many different
aspects in CALL, corresponding to their own expansive research areas. Most of
what we find in the research literature and in practical use are prototypes or
partial implementations -- systems that perform some aspects of the overall
desired functionality. Complete solutions -- most of them commercial -- are
few, because they require massive resources. Recent advances in AI should
result in improvements in CALL, yet there is a lack of surveys that focus on AI
in the context of this research field. This paper aims to present a perspective
on the AI methods that can be employed for language learning from a position of
a developer of a CALL system. We also aim to connect work from different
disciplines, to build bridges for interdisciplinary work.

</details>

### [164] [What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction](https://arxiv.org/abs/2505.02072)
*Eitan Wagner,Omri Abend*

Main category: cs.CL

TLDR: 论文分析了LLM中分布估计与响应预测的区别及其冲突目标，探讨了训练阶段和使用场景，指出不同设置导致三种不同的输出分布，并纠正了NLP研究中的误解。


<details>
  <summary>Details</summary>
Motivation: 研究LLM中分布估计与响应预测的区别及其冲突目标，为LLM的解释和使用提供更坚实的理论基础。

Method: 分析LLM的训练阶段（预训练、上下文学习、偏好调整）和输出概率的使用场景（完成概率、显式概率）。

Result: 发现不同设置导致三种不同的输出分布，NLP研究常误认为这些分布应相似。

Conclusion: 为LLM的解释和诱导分布的使用奠定了更正式的基础。

Abstract: The notion of language modeling has gradually shifted in recent years from a
distribution over finite-length strings to general-purpose prediction models
for textual inputs and outputs, following appropriate alignment phases. This
paper analyzes the distinction between distribution estimation and response
prediction in the context of LLMs, and their often conflicting goals. We
examine the training phases of LLMs, which include pretraining, in-context
learning, and preference tuning, and also the common use cases for their output
probabilities, which include completion probabilities and explicit
probabilities as output. We argue that the different settings lead to three
distinct intended output distributions. We demonstrate that NLP works often
assume that these distributions should be similar, which leads to
misinterpretations of their experimental findings. Our work sets firmer formal
foundations for the interpretation of LLMs, which will inform ongoing work on
the interpretation and use of LLMs' induced distributions.

</details>

### [165] [LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning](https://arxiv.org/abs/2505.02078)
*Joy Lim Jia Yin,Daniel Zhang-Li,Jifan Yu,Haoxuan Li,Shangqing Tu,Yuanchun Wang,Zhiyuan Liu,Huiqin Liu,Lei Hou,Juanzi Li,Bin Xu*

Main category: cs.CL

TLDR: LecEval是一种基于Mayer认知理论的自动化评估工具，用于评估幻灯片教学的多模态知识获取效果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法在可扩展性、上下文捕捉或偏见方面存在局限，需要一种更准确的自动化评估工具。

Method: 提出LecEval，基于四个评估标准（CR、EC、LS、AE），并利用大规模数据集训练模型。

Result: 模型在准确性和适应性上优于现有方法，接近人工评估水平。

Conclusion: LecEval填补了自动化与人工评估之间的差距，数据集和工具已开源。

Abstract: Evaluating the quality of slide-based multimedia instruction is challenging.
Existing methods like manual assessment, reference-based metrics, and large
language model evaluators face limitations in scalability, context capture, or
bias. In this paper, we introduce LecEval, an automated metric grounded in
Mayer's Cognitive Theory of Multimedia Learning, to evaluate multimodal
knowledge acquisition in slide-based learning. LecEval assesses effectiveness
using four rubrics: Content Relevance (CR), Expressive Clarity (EC), Logical
Structure (LS), and Audience Engagement (AE). We curate a large-scale dataset
of over 2,000 slides from more than 50 online course videos, annotated with
fine-grained human ratings across these rubrics. A model trained on this
dataset demonstrates superior accuracy and adaptability compared to existing
metrics, bridging the gap between automated and human assessments. We release
our dataset and toolkits at https://github.com/JoylimJY/LecEval.

</details>

### [166] [LLM-OptiRA: LLM-Driven Optimization of Resource Allocation for Non-Convex Problems in Wireless Communications](https://arxiv.org/abs/2505.02091)
*Xinyue Peng,Yanming Liu,Yihan Cang,Chaoqun Cao,Ming Chen*

Main category: cs.CL

TLDR: LLM-OptiRA利用大型语言模型自动解决无线通信系统中的非凸资源分配问题，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统优化技术难以解决非凸资源分配问题，需要一种自动化且高效的解决方案。

Method: 提出LLM-OptiRA框架，利用LLM自动检测并转换非凸问题为可解形式，集成纠错和可行性验证机制。

Result: 实验显示LLM-OptiRA在GPT-4上执行率为96%，成功率为80%，优于基线方法。

Conclusion: LLM-OptiRA为无线通信系统中的非凸优化问题提供了一种高效、自动化的解决方案。

Abstract: Solving non-convex resource allocation problems poses significant challenges
in wireless communication systems, often beyond the capability of traditional
optimization techniques. To address this issue, we propose LLM-OptiRA, the
first framework that leverages large language models (LLMs) to automatically
detect and transform non-convex components into solvable forms, enabling fully
automated resolution of non-convex resource allocation problems in wireless
communication systems. LLM-OptiRA not only simplifies problem-solving by
reducing reliance on expert knowledge, but also integrates error correction and
feasibility validation mechanisms to ensure robustness. Experimental results
show that LLM-OptiRA achieves an execution rate of 96% and a success rate of
80% on GPT-4, significantly outperforming baseline approaches in complex
optimization tasks across diverse scenarios.

</details>

### [167] [Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study](https://arxiv.org/abs/2505.02142)
*Xiaoyu Tian,Sitong Zhao,Haotian Wang,Shuaiting Chen,Yiping Peng,Yunjie Ji,Han Zhao,Xiangang Li*

Main category: cs.CL

TLDR: 论文研究了离线强化学习方法（如DPO和LD-DPO）在提升大语言模型推理能力上的效果，发现这些方法能显著提升性能，同时分析了输出长度对模型的影响。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习方法虽然有效，但计算成本高且复杂，而离线强化学习方法尚未充分探索，因此研究其效果和实用性。

Method: 采用离线强化学习方法（DPO和LD-DPO），通过多基准测试评估模型推理能力，并分析输出长度的影响。

Result: 离线强化学习方法平均提升模型性能3.3%，在Arena-Hard基准上提升10.1%，同时发现输出长度需与语义丰富度匹配。

Conclusion: 离线强化学习方法是一种更经济高效的替代方案，输出长度需合理设计以优化模型性能。

Abstract: Despite significant advances in long-context reasoning by large language
models (LLMs), primarily through Online Reinforcement Learning (RL) methods,
these approaches incur substantial computational costs and complexity. In
contrast, simpler and more economical Offline RL methods remain underexplored.
To address this gap, we investigate the effectiveness of Offline RL methods,
specifically Direct Preference Optimization (DPO) and its length-desensitized
variant LD-DPO, in enhancing the reasoning capabilities of LLMs. Extensive
experiments across multiple reasoning benchmarks demonstrate that these simpler
Offline RL methods substantially improve model performance, achieving an
average enhancement of 3.3\%, with a particularly notable increase of 10.1\% on
the challenging Arena-Hard benchmark. Furthermore, we analyze DPO's sensitivity
to output length, emphasizing that increasing reasoning length should align
with semantic richness, as indiscriminate lengthening may adversely affect
model performance. We provide comprehensive descriptions of our data processing
and training methodologies, offering empirical evidence and practical insights
for developing more cost-effective Offline RL approaches.

</details>

### [168] [QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach](https://arxiv.org/abs/2505.02146)
*Shouyang Dong,Yuanbo Wen,Jun Bi,Di Huang,Jiaming Guo,Jianxing Xu,Ruibai Xu,Xinkai Song,Yifan Hao,Xuehai Zhou,Tianshi Chen,Qi Guo,Yunji Chen*

Main category: cs.CL

TLDR: QiMeng-Xpiler是一种新型的跨异构深度学习系统的张量程序转编译器，结合了大型语言模型（LLM）和符号程序合成技术，显著提高了编程效率和程序性能。


<details>
  <summary>Details</summary>
Motivation: 异构深度学习系统（如GPU和ASIC）的广泛部署需要为不同平台开发多个低层张量程序，而现有转编译技术存在手动工作量大或功能错误的问题。

Method: 通过LLM和符号程序合成（神经符号合成）实现自动转编译，利用LLM的代码生成能力降低符号合成的计算成本，并采用分层自动调优方法优化转换过程。

Result: 在4种不同编程接口的DLS上，QiMeng-Xpiler平均准确率达95%，性能最高提升2.0倍，编程效率提升96.0倍。

Conclusion: QiMeng-Xpiler有效解决了张量程序跨平台转编译的难题，显著提升了编程生产力和程序性能。

Abstract: Heterogeneous deep learning systems (DLS) such as GPUs and ASICs have been
widely deployed in industrial data centers, which requires to develop multiple
low-level tensor programs for different platforms. An attractive solution to
relieve the programming burden is to transcompile the legacy code of one
platform to others. However, current transcompilation techniques struggle with
either tremendous manual efforts or functional incorrectness, rendering "Write
Once, Run Anywhere" of tensor programs an open question.
  We propose a novel transcompiler, i.e., QiMeng-Xpiler, for automatically
translating tensor programs across DLS via both large language models (LLMs)
and symbolic program synthesis, i.e., neural-symbolic synthesis. The key
insight is leveraging the powerful code generation ability of LLM to make
costly search-based symbolic synthesis computationally tractable. Concretely,
we propose multiple LLM-assisted compilation passes via pre-defined
meta-prompts for program transformation. During each program transformation,
efficient symbolic program synthesis is employed to repair incorrect code
snippets with a limited scale. To attain high performance, we propose a
hierarchical auto-tuning approach to systematically explore both the parameters
and sequences of transformation passes. Experiments on 4 DLS with distinct
programming interfaces, i.e., Intel DL Boost with VNNI, NVIDIA GPU with CUDA,
AMD MI with HIP, and Cambricon MLU with BANG, demonstrate that QiMeng-Xpiler
correctly translates different tensor programs at the accuracy of 95% on
average, and the performance of translated programs achieves up to 2.0x over
vendor-provided manually-optimized libraries. As a result, the programming
productivity of DLS is improved by up to 96.0x via transcompiling legacy tensor
programs.

</details>

### [169] [Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents](https://arxiv.org/abs/2505.02156)
*Minzheng Wang,Yongbin Li,Haobo Wang,Xinghua Zhang,Nan Xu,Bingli Wu,Fei Huang,Haiyang Yu,Wenji Mao*

Main category: cs.CL

TLDR: 论文提出了一种自适应模式学习（AML）框架，通过AMPO算法动态调整推理深度，显著提升了社交智能任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏动态调整推理深度的能力，导致推理链过长或社交模拟不自然。

Method: AML框架通过AMPO算法实现多粒度思考模式设计、上下文感知模式切换和深度自适应处理。

Result: 实验显示AML在任务表现上比现有方法提升15.6%，推理链缩短32.8%。

Conclusion: AMPO的动态推理模式选择比固定深度方法更接近人类推理能力。

Abstract: Effective social intelligence simulation requires language agents to
dynamically adjust reasoning depth, a capability notably absent in current
approaches. While existing methods either lack this kind of reasoning
capability or enforce uniform long chain-of-thought reasoning across all
scenarios, resulting in excessive token usage and inappropriate social
simulation. In this paper, we propose $\textbf{A}$daptive $\textbf{M}$ode
$\textbf{L}$earning ($\textbf{AML}$) that strategically selects from four
thinking modes (intuitive reaction $\rightarrow$ deep contemplation) based on
real-time context. Our framework's core innovation, the $\textbf{A}$daptive
$\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$)
algorithm, introduces three key advancements over existing methods: (1)
Multi-granular thinking mode design, (2) Context-aware mode switching across
social interaction, and (3) Token-efficient reasoning via depth-adaptive
processing. Extensive experiments on social intelligence tasks confirm that AML
achieves 15.6% higher task performance than state-of-the-art methods. Notably,
our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These
results demonstrate that context-sensitive thinking mode selection, as
implemented in AMPO, enables more human-like adaptive reasoning than GRPO's
fixed-depth approach

</details>

### [170] [Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study on Copyright Fair Use](https://arxiv.org/abs/2505.02164)
*Justin Ho,Alexandra Colby,William Fisher*

Main category: cs.CL

TLDR: 本文提出了一种针对美国版权法中合理使用原则的领域特定RAG实现，旨在解决DMCA下架通知增多和创作者缺乏法律支持的问题。


<details>
  <summary>Details</summary>
Motivation: DMCA下架通知的增多以及内容创作者缺乏可获取的法律支持，促使研究开发一种结合语义搜索、法律知识图谱和法院引用网络的结构化方法。

Method: 结合语义搜索、法律知识图谱和法院引用网络，利用Chain-of-Thought推理和交错检索步骤模拟法律推理。

Result: 初步测试表明，该方法提高了检索过程中的法律相关性。

Conclusion: 为基于LLM的法律辅助工具的未来评估和部署奠定了基础。

Abstract: This paper presents a domain-specific implementation of Retrieval-Augmented
Generation (RAG) tailored to the Fair Use Doctrine in U.S. copyright law.
Motivated by the increasing prevalence of DMCA takedowns and the lack of
accessible legal support for content creators, we propose a structured approach
that combines semantic search with legal knowledge graphs and court citation
networks to improve retrieval quality and reasoning reliability. Our prototype
models legal precedents at the statutory factor level (e.g., purpose, nature,
amount, market effect) and incorporates citation-weighted graph representations
to prioritize doctrinally authoritative sources. We use Chain-of-Thought
reasoning and interleaved retrieval steps to better emulate legal reasoning.
Preliminary testing suggests this method improves doctrinal relevance in the
retrieval process, laying groundwork for future evaluation and deployment of
LLM-based legal assistance tools.

</details>

### [171] [A New HOPE: Domain-agnostic Automatic Evaluation of Text Chunking](https://arxiv.org/abs/2505.02171)
*Henrik Brådland,Morten Goodwin,Per-Arne Andersen,Alexander S. Nossum,Aditya Gupta*

Main category: cs.CL

TLDR: 论文提出HOPE评估指标，分析文档分块对RAG的影响，发现语义独立性对性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 研究文档分块方法对RAG性能的影响，缺乏现有框架分析不同分块策略的效果。

Method: 提出HOPE评估指标，从内在、外在特性和文档连贯性三个层次量化分块特征。

Result: HOPE与RAG性能显著相关，语义独立性提升性能（事实正确性+56.2%，答案正确性+21.1%）。

Conclusion: 优化分块策略可提升RAG性能，语义独立性是关键因素。

Abstract: Document chunking fundamentally impacts Retrieval-Augmented Generation (RAG)
by determining how source materials are segmented before indexing. Despite
evidence that Large Language Models (LLMs) are sensitive to the layout and
structure of retrieved data, there is currently no framework to analyze the
impact of different chunking methods. In this paper, we introduce a novel
methodology that defines essential characteristics of the chunking process at
three levels: intrinsic passage properties, extrinsic passage properties, and
passages-document coherence. We propose HOPE (Holistic Passage Evaluation), a
domain-agnostic, automatic evaluation metric that quantifies and aggregates
these characteristics. Our empirical evaluations across seven domains
demonstrate that the HOPE metric correlates significantly (p > 0.13) with
various RAG performance indicators, revealing contrasts between the importance
of extrinsic and intrinsic properties of passages. Semantic independence
between passages proves essential for system performance with a performance
gain of up to 56.2% in factual correctness and 21.1% in answer correctness. On
the contrary, traditional assumptions about maintaining concept unity within
passages show minimal impact. These findings provide actionable insights for
optimizing chunking strategies, thus improving RAG system design to produce
more factually correct responses.

</details>

### [172] [Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization](https://arxiv.org/abs/2505.02172)
*Chuck Arvin*

Main category: cs.CL

TLDR: 研究评估了大型语言模型（LLM）在法律基准数据集CaseHOLD上的表现，发现模型性能随规模提升，且无需复杂训练或微调。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在已建立的法律基准上的表现，以了解其能力和局限性。

Method: 通过实验测试不同规模的LLM（3B至90B+参数）在CaseHOLD数据集上的表现，并开发了一种新颖的引用匿名化测试。

Result: 模型性能随规模提升，GPT4o和AmazonNovaPro分别达到0.744和0.720的宏F1分数，且匿名化测试表明性能非源于记忆。

Conclusion: LLM在法律任务中表现出潜力，但也存在局限性，对自动化法律分析和基准开发有重要启示。

Abstract: As large language models (LLMs) continue to advance in capabilities, it is
essential to assess how they perform on established benchmarks. In this study,
we present a suite of experiments to assess the performance of modern LLMs
(ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for
identifying case holdings. Our experiments demonstrate ``scaling effects'' -
performance on this task improves with model size, with more capable models
like GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720
respectively. These scores are competitive with the best published results on
this dataset, and do not require any technically sophisticated model training,
fine-tuning or few-shot prompting. To ensure that these strong results are not
due to memorization of judicial opinions contained in the training data, we
develop and utilize a novel citation anonymization test that preserves semantic
meaning while ensuring case names and citations are fictitious. Models maintain
strong performance under these conditions (macro F1 of 0.728), suggesting the
performance is not due to rote memorization. These findings demonstrate both
the promise and current limitations of LLMs for legal tasks with important
implications for the development and measurement of automated legal analytics
and legal benchmarks.

</details>

### [173] [Measuring Hong Kong Massive Multi-Task Language Understanding](https://arxiv.org/abs/2505.02177)
*Chuxue Cao,Zhenghao Zhu,Junqi Zhu,Guoying Lu,Siyu Peng,Juntao Dai,Weijie Shi,Sirui Han,Yike Guo*

Main category: cs.CL

TLDR: HKMMLU是一个针对香港独特语言环境的多任务语言理解基准，包含26,698道多选题和90,550个翻译任务，评估LLMs在语言和文化知识上的表现。实验显示，最佳模型DeepSeek-V3准确率仅75%，远低于其他基准，突显改进需求。


<details>
  <summary>Details</summary>
Motivation: 香港的语言环境独特（繁体中文与粤语结合），但现有评估基准不足，需填补这一空白。

Method: 构建HKMMLU基准，包含多类题目和翻译任务，测试多种LLMs（如GPT-4o、Claude 3.7 Sonnet等）。

Result: 最佳模型DeepSeek-V3准确率仅75%，表现显著低于其他基准，表明LLMs在香港特定语言和文化知识上存在不足。

Conclusion: HKMMLU将推动LLMs在多语言和跨文化背景下的发展，提升应用广度与影响力。

Abstract: Multilingual understanding is crucial for the cross-cultural applicability of
Large Language Models (LLMs). However, evaluation benchmarks designed for Hong
Kong's unique linguistic landscape, which combines Traditional Chinese script
with Cantonese as the spoken form and its cultural context, remain
underdeveloped. To address this gap, we introduce HKMMLU, a multi-task language
understanding benchmark that evaluates Hong Kong's linguistic competence and
socio-cultural knowledge. The HKMMLU includes 26,698 multi-choice questions
across 66 subjects, organized into four categories: Science, Technology,
Engineering, and Mathematics (STEM), Social Sciences, Humanities, and Other. To
evaluate the multilingual understanding ability of LLMs, 90,550
Mandarin-Cantonese translation tasks were additionally included. We conduct
comprehensive experiments on GPT-4o, Claude 3.7 Sonnet, and 18 open-source LLMs
of varying sizes on HKMMLU. The results show that the best-performing model,
DeepSeek-V3, struggles to achieve an accuracy of 75\%, significantly lower than
that of MMLU and CMMLU. This performance gap highlights the need to improve
LLMs' capabilities in Hong Kong-specific language and knowledge domains.
Furthermore, we investigate how question language, model size, prompting
strategies, and question and reasoning token lengths affect model performance.
We anticipate that HKMMLU will significantly advance the development of LLMs in
multilingual and cross-cultural contexts, thereby enabling broader and more
impactful applications.

</details>

### [174] [SEval-Ex: A Statement-Level Framework for Explainable Summarization Evaluation](https://arxiv.org/abs/2505.02235)
*Tanguy Herserant,Vincent Guigue*

Main category: cs.CL

TLDR: SEval-Ex是一个新的文本摘要评估框架，通过分解为原子语句实现高性能和可解释性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决当前文本摘要评估方法在性能和可解释性之间的权衡问题。

Method: 采用两阶段流程：1）使用LLM从文本和摘要中提取原子语句；2）进行语句级匹配。

Result: 在SummEval基准测试中，SEval-Ex以0.580的相关性优于GPT-4评估器（0.521），同时保持可解释性。

Conclusion: SEval-Ex在性能和可解释性上均表现优异，且对幻觉具有鲁棒性。

Abstract: Evaluating text summarization quality remains a critical challenge in Natural
Language Processing. Current approaches face a trade-off between performance
and interpretability. We present SEval-Ex, a framework that bridges this gap by
decomposing summarization evaluation into atomic statements, enabling both high
performance and explainability. SEval-Ex employs a two-stage pipeline: first
extracting atomic statements from text source and summary using LLM, then a
matching between generated statements. Unlike existing approaches that provide
only summary-level scores, our method generates detailed evidence for its
decisions through statement-level alignments. Experiments on the SummEval
benchmark demonstrate that SEval-Ex achieves state-of-the-art performance with
0.580 correlation on consistency with human consistency judgments, surpassing
GPT-4 based evaluators (0.521) while maintaining interpretability. Finally, our
framework shows robustness against hallucination.

</details>

### [175] [Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models](https://arxiv.org/abs/2505.02252)
*Paloma Piot,Patricia Martín-Rodilla,Javier Parapar*

Main category: cs.CL

TLDR: 研究了商业大语言模型（LLMs）在个性化场景下的行为，特别是仇恨言论检测，发现个性化上下文显著影响其响应。通过微调模型以减少偏见，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 评估个性化信息对LLM行为的影响，尤其是在敏感话题（如仇恨言论）上的表现。

Method: 测试多种先进LLMs在不同个性化场景下的行为，通过微调模型惩罚不一致的仇恨言论分类。

Result: 个性化上下文显著影响LLM的响应，微调后模型在有无个性化上下文时表现均改善。

Conclusion: 个性化上下文对LLM行为有重要影响，微调可有效减少偏见并提升性能。

Abstract: Commercial Large Language Models (LLMs) have recently incorporated memory
features to deliver personalised responses. This memory retains details such as
user demographics and individual characteristics, allowing LLMs to adjust their
behaviour based on personal information. However, the impact of integrating
personalised information into the context has not been thoroughly assessed,
leading to questions about its influence on LLM behaviour. Personalisation can
be challenging, particularly with sensitive topics. In this paper, we examine
various state-of-the-art LLMs to understand their behaviour in different
personalisation scenarios, specifically focusing on hate speech. We prompt the
models to assume country-specific personas and use different languages for hate
speech detection. Our findings reveal that context personalisation
significantly influences LLMs' responses in this sensitive area. To mitigate
these unwanted biases, we fine-tune the LLMs by penalising inconsistent hate
speech classifications made with and without country or language-specific
context. The refined models demonstrate improved performance in both
personalised contexts and when no context is provided.

</details>

### [176] [Parameter-Efficient Transformer Embeddings](https://arxiv.org/abs/2505.02266)
*Henry Ndubuaku,Mouad Talhi*

Main category: cs.CL

TLDR: 提出一种基于傅里叶展开和轻量级MLP的嵌入层替代方法，显著减少参数数量并保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统嵌入层参数多但性能提升不成比例，需更高效的替代方案。

Method: 通过傅里叶展开生成嵌入向量，再用轻量级MLP捕获高阶交互。

Result: 在自然语言推理任务中表现竞争性，参数更少、训练更快且无需dropout。

Conclusion: 该方法展示了可扩展、内存高效语言模型的潜力，值得进一步大规模实验。

Abstract: Embedding layers in transformer-based NLP models typically account for the
largest share of model parameters, scaling with vocabulary size but not
yielding performance gains proportional to scale. We propose an alternative
approach in which token embedding vectors are first generated
deterministically, directly from the token IDs using a Fourier expansion of
their normalized values, followed by a lightweight multilayer perceptron (MLP)
that captures higher-order interactions. We train standard transformers and our
architecture on natural language inference tasks (SNLI and MNLI), and evaluate
zero-shot performance on sentence textual similarity (STS-B). Our results
demonstrate that the proposed method achieves competitive performance using
significantly fewer parameters, trains faster, and operates effectively without
the need for dropout. This proof-of-concept study highlights the potential for
scalable, memory-efficient language models and motivates further large-scale
experimentation based on our findings.

</details>

### [177] [Demystifying optimized prompts in language models](https://arxiv.org/abs/2505.02273)
*Rimon Melamed,Lucas H. McCabe,H. Howie Huang*

Main category: cs.CL

TLDR: 现代语言模型对分布外输入不鲁棒，优化提示可调制输出但难以解释。本文研究了优化提示的组成及其解析机制，发现其主要由罕见标点和名词构成，且激活模式与自然语言不同。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型对优化提示的响应机制及其内部表示。

Method: 分析优化提示的组成及其在模型中的激活模式。

Result: 优化提示主要由罕见标点和名词构成，其激活模式与自然语言明显不同。

Conclusion: 优化提示在模型中的处理路径与自然语言不同，揭示了其独特的内部表示机制。

Abstract: Modern language models (LMs) are not robust to out-of-distribution inputs.
Machine generated (``optimized'') prompts can be used to modulate LM outputs
and induce specific behaviors while appearing completely uninterpretable. In
this work, we investigate the composition of optimized prompts, as well as the
mechanisms by which LMs parse and build predictions from optimized prompts. We
find that optimized prompts primarily consist of punctuation and noun tokens
which are more rare in the training data. Internally, optimized prompts are
clearly distinguishable from natural language counterparts based on sparse
subsets of the model's activations. Across various families of
instruction-tuned models, optimized prompts follow a similar path in how their
representations form through the network.

</details>

### [178] [Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition](https://arxiv.org/abs/2505.02304)
*Siyu Liang,Yunan Li,Wentian Xin,Huizhou Chen,Xujie Liu,Kang Liu,Qiguang Miao*

Main category: cs.CL

TLDR: 提出了一种结合生成式大语言模型（LLMs）和检索增强生成（RAG）的手语识别方法GSP-MC，通过多步提示工程和专家验证的语料库生成精确描述，并在跨语言数据集上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决手语识别中因手势和非手势信号的复杂性导致的标注难题，首次将生成式LLMs引入该任务。

Method: 采用GSP-MC方法，结合RAG和领域特定LLMs，通过双编码器架构实现文本描述与骨架特征的对齐，并优化KL散度损失。

Result: 在中文SLR500和土耳其AUTSL数据集上分别达到97.1%和97.07%的准确率，表现最优。

Conclusion: GSP-MC方法展示了跨语言有效性，为开发包容性通信技术提供了潜力。

Abstract: Sign language recognition (SLR) faces fundamental challenges in creating
accurate annotations due to the inherent complexity of simultaneous manual and
non-manual signals. To the best of our knowledge, this is the first work to
integrate generative large language models (LLMs) into SLR tasks. We propose a
novel Generative Sign-description Prompts Multi-positive Contrastive learning
(GSP-MC) method that leverages retrieval-augmented generation (RAG) with
domain-specific LLMs, incorporating multi-step prompt engineering and
expert-validated sign language corpora to produce precise multipart
descriptions. The GSP-MC method also employs a dual-encoder architecture to
bidirectionally align hierarchical skeleton features with multiple text
descriptions (global, synonym, and part level) through probabilistic matching.
Our approach combines global and part-level losses, optimizing KL divergence to
ensure robust alignment across all relevant text-skeleton pairs while capturing
both sign-level semantics and detailed part dynamics. Experiments demonstrate
state-of-the-art performance against existing methods on the Chinese SLR500
(reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method's
cross-lingual effectiveness highlight its potential for developing inclusive
communication technologies.

</details>

### [179] [Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering](https://arxiv.org/abs/2505.02311)
*Jihao Zhao,Chunlai Zhou,Biao Qin*

Main category: cs.CL

TLDR: 提出了一种名为AttenHScore的实时幻觉检测指标，动态调整阈值以优化大语言模型调用，同时结合不确定性感知知识重组提升小语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决小语言模型生成过程中幻觉问题，优化实时调用大语言模型的时机，避免高计算成本和低效后处理。

Method: 提出AttenHScore指标动态计算幻觉积累与传播，结合不确定性感知知识重组辅助小模型。

Result: AttenHScore在多问答数据集上表现优于基线，尤其在复杂查询中，且无需额外训练。

Conclusion: AttenHScore有效提升实时幻觉检测能力，适用于多种基于Transformer的语言模型。

Abstract: The collaborative paradigm of large and small language models (LMs)
effectively balances performance and cost, yet its pivotal challenge lies in
precisely pinpointing the moment of invocation when hallucinations arise in
small LMs. Previous optimization efforts primarily focused on post-processing
techniques, which were separate from the reasoning process of LMs, resulting in
high computational costs and limited effectiveness. In this paper, we propose a
practical invocation evaluation metric called AttenHScore, which calculates the
accumulation and propagation of hallucinations during the generation process of
small LMs, continuously amplifying potential reasoning errors. By dynamically
adjusting the detection threshold, we achieve more accurate real-time
invocation of large LMs. Additionally, considering the limited reasoning
capacity of small LMs, we leverage uncertainty-aware knowledge reorganization
to assist them better capture critical information from different text chunks.
Extensive experiments reveal that our AttenHScore outperforms most baseline in
enhancing real-time hallucination detection capabilities across multiple QA
datasets, especially when addressing complex queries. Moreover, our strategies
eliminate the need for additional model training and display flexibility in
adapting to various transformer-based LMs.

</details>

### [180] [SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning](https://arxiv.org/abs/2505.02363)
*Tianjian Li,Daniel Khashabi*

Main category: cs.CL

TLDR: 论文提出SIMPLEMIX方法，通过混合on-policy和off-policy数据优化语言模型对齐，在多种任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探讨on-policy和off-policy数据在偏好学习中的互补性，以优化语言模型对齐。

Method: 提出SIMPLEMIX方法，简单混合on-policy和off-policy数据。

Result: SIMPLEMIX在Alpaca Eval 2.0上平均提升6.03%，优于复杂方法如HyPO和DPO-Mix-P。

Conclusion: 混合on-policy和off-policy数据是优化语言模型对齐的有效策略。

Abstract: Aligning language models with human preferences relies on pairwise preference
datasets. While some studies suggest that on-policy data consistently
outperforms off -policy data for preference learning, others indicate that the
advantages of on-policy data may be task-dependent, highlighting the need for a
systematic exploration of their interplay.
  In this work, we show that on-policy and off-policy data offer complementary
strengths in preference optimization: on-policy data is particularly effective
for reasoning tasks like math and coding, while off-policy data performs better
on open-ended tasks such as creative writing and making personal
recommendations. Guided by these findings, we introduce SIMPLEMIX, an approach
to combine the complementary strengths of on-policy and off-policy preference
learning by simply mixing these two data sources. Our empirical results across
diverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves
language model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO
and off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it
outperforms prior approaches that are much more complex in combining on- and
off-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%.

</details>

### [181] [JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings](https://arxiv.org/abs/2505.02366)
*Tianyu Zong,Hongzhu Yi,Bingkang Shi,Yuanxiang Wang,Jungang Xu*

Main category: cs.CL

TLDR: 论文提出了一种名为JTCSE的无监督对比学习框架，通过约束语义表示张量的模特征和引入跨注意力机制，提升了句子嵌入表示的质量，并在多个任务中取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法仅关注语义表示的方向特征，忽略了模特征，导致对比学习不充分；同时，BERT类模型存在注意力下沉问题，影响CLS令牌的语义聚合效果。

Method: 提出模约束训练目标以增强正样本对齐，设计跨注意力结构优化CLS令牌的注意力，最终结合二者构建JTCSE框架。

Result: 在7项语义文本相似性任务和130多项零样本下游任务中，JTCSE的双塔集成模型和单塔蒸馏模型均优于基线方法，达到SOTA。

Conclusion: JTCSE通过模约束和跨注意力机制有效提升了无监督对比学习的性能，证明了其在句子嵌入表示中的优越性。

Abstract: Unsupervised contrastive learning has become a hot research topic in natural
language processing. Existing works usually aim at constraining the orientation
distribution of the representations of positive and negative samples in the
high-dimensional semantic space in contrastive learning, but the semantic
representation tensor possesses both modulus and orientation features, and the
existing works ignore the modulus feature of the representations and cause
insufficient contrastive learning. % Therefore, we firstly propose a training
objective that aims at modulus constraints on the semantic representation
tensor, to strengthen the alignment between the positive samples in contrastive
learning. Therefore, we first propose a training objective that is designed to
impose modulus constraints on the semantic representation tensor, to strengthen
the alignment between positive samples in contrastive learning. Then, the
BERT-like model suffers from the phenomenon of sinking attention, leading to a
lack of attention to CLS tokens that aggregate semantic information. In
response, we propose a cross-attention structure among the twin-tower ensemble
models to enhance the model's attention to CLS token and optimize the quality
of CLS Pooling. Combining the above two motivations, we propose a new
\textbf{J}oint \textbf{T}ensor representation modulus constraint and
\textbf{C}ross-attention unsupervised contrastive learning \textbf{S}entence
\textbf{E}mbedding representation framework JTCSE, which we evaluate in seven
semantic text similarity computation tasks, and the experimental results show
that JTCSE's twin-tower ensemble model and single-tower distillation model
outperform the other baselines and become the current SOTA. In addition, we
have conducted an extensive zero-shot downstream task evaluation, which shows
that JTCSE outperforms other baselines overall on more than 130 tasks.

</details>

### [182] [RM-R1: Reward Modeling as Reasoning](https://arxiv.org/abs/2505.02387)
*Xiusi Chen,Gaotang Li,Ziqi Wang,Bowen Jin,Cheng Qian,Yu Wang,Hongru Wang,Yu Zhang,Denghui Zhang,Tong Zhang,Hanghang Tong,Heng Ji*

Main category: cs.CL

TLDR: 论文提出了一种新的生成式奖励模型ReasRMs，通过将奖励建模转化为推理任务，显著提升了模型的解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型缺乏解释性，无法整合自然语言批评，因此需要一种能够进行深度推理的奖励模型。

Method: 提出ReasRMs，采用两阶段训练流程：高质量推理链的蒸馏和可验证奖励的强化学习。

Result: ReasRMs在多个基准测试中表现优异，性能超过大型开源和专有模型。

Conclusion: ReasRMs通过推理能力显著提升了奖励模型的解释性和性能，为未来研究提供了资源。

Abstract: Reward modeling is essential for aligning large language models (LLMs) with
human preferences, especially through reinforcement learning from human
feedback (RLHF). To provide accurate reward signals, a reward model (RM) should
stimulate deep thinking and conduct interpretable reasoning before assigning a
score or a judgment. However, existing RMs either produce opaque scalar scores
or directly generate the prediction of a preferred answer, making them struggle
to integrate natural language critiques, thus lacking interpretability.
Inspired by recent advances of long chain-of-thought (CoT) on
reasoning-intensive tasks, we hypothesize and validate that integrating
reasoning capabilities into reward modeling significantly enhances RM's
interpretability and performance. In this work, we introduce a new class of
generative reward models -- Reasoning Reward Models (ReasRMs) -- which
formulate reward modeling as a reasoning task. We propose a reasoning-oriented
training pipeline and train a family of ReasRMs, RM-R1. The training consists
of two key stages: (1) distillation of high-quality reasoning chains and (2)
reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by
self-generating reasoning traces or chat-specific rubrics and evaluating
candidate responses against them. Empirically, our models achieve
state-of-the-art or near state-of-the-art performance of generative RMs across
multiple comprehensive reward model benchmarks, outperforming much larger
open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by
up to 13.8%. Beyond final performance, we perform thorough empirical analysis
to understand the key ingredients of successful ReasRM training. To facilitate
future research, we release six ReasRM models along with code and data at
https://github.com/RM-R1-UIUC/RM-R1.

</details>

### [183] [Bielik 11B v2 Technical Report](https://arxiv.org/abs/2505.02410)
*Krzysztof Ociepa,Łukasz Flis,Krzysztof Wróbel,Adrian Gwoździej,Remigiusz Kinas*

Main category: cs.CL

TLDR: Bielik 11B v2是一个针对波兰语优化的先进语言模型，基于Mistral 7B v0.2架构，通过深度扩展达到11B参数，在波兰语任务中表现卓越，同时具备跨语言能力。


<details>
  <summary>Details</summary>
Motivation: 提升波兰语文本处理的性能，同时保持跨语言能力，为资源较少语言的高效建模设立新标准。

Method: 采用深度扩展技术，引入加权指令交叉熵损失和自适应学习率两项创新技术。

Result: 在多项基准测试中超越更大参数模型，显著优于其他波兰语专用模型，且参数高效，支持多种硬件部署。

Conclusion: Bielik 11B v2推动了波兰语AI能力，为资源高效的语言建模设立了新标杆。

Abstract: We present Bielik 11B v2, a state-of-the-art language model optimized for
Polish text processing. Built on the Mistral 7B v0.2 architecture and scaled to
11B parameters using depth up-scaling, this model demonstrates exceptional
performance across Polish language benchmarks while maintaining strong
cross-lingual capabilities. We introduce two key technical innovations:
Weighted Instruction Cross-Entropy Loss, which optimizes learning across
diverse instruction types by assigning quality-based weights to training
examples, and Adaptive Learning Rate, which dynamically adjusts based on
context length. Comprehensive evaluation across multiple benchmarks
demonstrates that Bielik 11B v2 outperforms many larger models, including those
with 2-6 times more parameters, and significantly surpasses other specialized
Polish language models on tasks ranging from linguistic understanding to
complex reasoning. The model's parameter efficiency and extensive quantization
options enable deployment across various hardware configurations, advancing
Polish language AI capabilities and establishing new benchmarks for
resource-efficient language modeling in less-represented languages.

</details>

### [184] [Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs](https://arxiv.org/abs/2505.02456)
*Elisa Forcada Rodríguez,Olatz Perez-de-Viñaspre,Jon Ander Campos,Dietrich Klakow,Vagrant Gautam*

Main category: cs.CL

TLDR: 该研究探讨了多语言环境下大型语言模型（LLMs）在职业推荐中表现出的性别和国家交叉偏见，强调了公平研究中多语言和交叉视角的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有公平研究多关注单一偏见（如性别）和英语语言，本研究旨在填补多语言和交叉偏见研究的空白。

Method: 构建包含英语、西班牙语和德语的提示基准，系统变化国家和性别（25个国家、4种代词集），评估5个基于Llama的模型。

Result: 发现LLMs存在显著的性别和国家偏见，即使单独性别或国家偏见表现均衡，交叉偏见仍存在；提示语言显著影响偏见，指令调优模型偏见最低且最稳定。

Conclusion: 公平研究需采用多语言和交叉视角，以更全面地识别和缓解偏见。

Abstract: One of the goals of fairness research in NLP is to measure and mitigate
stereotypical biases that are propagated by NLP systems. However, such work
tends to focus on single axes of bias (most often gender) and the English
language. Addressing these limitations, we contribute the first study of
multilingual intersecting country and gender biases, with a focus on occupation
recommendations generated by large language models. We construct a benchmark of
prompts in English, Spanish and German, where we systematically vary country
and gender, using 25 countries and four pronoun sets. Then, we evaluate a suite
of 5 Llama-based models on this benchmark, finding that LLMs encode significant
gender and country biases. Notably, we find that even when models show parity
for gender or country individually, intersectional occupational biases based on
both country and gender persist. We also show that the prompting language
significantly affects bias, and instruction-tuned models consistently
demonstrate the lowest and most stable levels of bias. Our findings highlight
the need for fairness researchers to use intersectional and multilingual lenses
in their work.

</details>

### [185] [Data Augmentation With Back translation for Low Resource languages: A case of English and Luganda](https://arxiv.org/abs/2505.02463)
*Richard Kimera,Dongnyeong Heo,Daniela N. Rim,Heeyoul Choi*

Main category: cs.CL

TLDR: 论文探讨了反向翻译（BT）作为半监督技术提升英语-卢干达语神经机器翻译（NMT）模型的效果，解决了低资源语言的挑战。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过反向翻译从单语语料库生成合成数据，缓解双语数据稀缺问题。

Method: 开发了基于公开和网络爬取数据的NMT模型，并应用迭代和增量反向翻译技术，创新性地选择多个小数据集进行增量反向翻译。

Result: 翻译性能显著提升，英语-卢干达语对的翻译效果超过先前基准10个BLEU分数单位以上，并采用多种评估指标全面分析质量。

Conclusion: 研究证实了反向翻译在策略性数据集选择下的有效性，为低资源语言的NMT模型设立了新基准。

Abstract: In this paper,we explore the application of Back translation (BT) as a
semi-supervised technique to enhance Neural Machine Translation(NMT) models for
the English-Luganda language pair, specifically addressing the challenges faced
by low-resource languages. The purpose of our study is to demonstrate how BT
can mitigate the scarcity of bilingual data by generating synthetic data from
monolingual corpora. Our methodology involves developing custom NMT models
using both publicly available and web-crawled data, and applying Iterative and
Incremental Back translation techniques. We strategically select datasets for
incremental back translation across multiple small datasets, which is a novel
element of our approach. The results of our study show significant
improvements, with translation performance for the English-Luganda pair
exceeding previous benchmarks by more than 10 BLEU score units across all
translation directions. Additionally, our evaluation incorporates comprehensive
assessment metrics such as SacreBLEU, ChrF2, and TER, providing a nuanced
understanding of translation quality. The conclusion drawn from our research
confirms the efficacy of BT when strategically curated datasets are utilized,
establishing new performance benchmarks and demonstrating the potential of BT
in enhancing NMT models for low-resource languages.

</details>

### [186] [Bemba Speech Translation: Exploring a Low-Resource African Language](https://arxiv.org/abs/2505.02518)
*Muhammad Hazim Al Farouq,Aman Kassahun Wassie,Yasmin Moslem*

Main category: cs.CL

TLDR: 本文介绍了针对Bemba-to-英语低资源语言语音翻译的系统，基于Whisper和NLLB-200构建级联系统，并采用数据增强技术（如回译）。


<details>
  <summary>Details</summary>
Motivation: 研究低资源语言（Bemba）的语音翻译问题，探索如何通过数据增强提升性能。

Method: 基于Whisper和NLLB-200构建级联语音翻译系统，并采用回译等数据增强技术。

Result: 实验分析了合成数据的效果，并讨论了实验设置。

Conclusion: 系统为低资源语言语音翻译提供了可行方案，数据增强技术有助于提升性能。

Abstract: This paper describes our system submission to the International Conference on
Spoken Language Translation (IWSLT 2025), low-resource languages track, namely
for Bemba-to-English speech translation. We built cascaded speech translation
systems based on Whisper and NLLB-200, and employed data augmentation
techniques, such as back-translation. We investigate the effect of using
synthetic data and discuss our experimental setup.

</details>

### [187] [EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning](https://arxiv.org/abs/2505.02579)
*Lingxiao Kong,Cong Yang,Susanne Neufang,Oya Deniz Beyan,Zeyd Boukhers*

Main category: cs.CL

TLDR: 论文提出了一种基于集成学习的多目标强化学习框架（EMORL），通过优化多个模型的训练和聚合，解决了多目标任务的复杂性问题，提升了效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在大型语言模型微调中面临多目标平衡、训练效率低、可扩展性差和可解释性有限等挑战。

Method: 引入EMORL框架，通过集成多个单目标模型并优化其聚合，结合分层网格搜索算法确定最优权重组合。

Result: 在PAIR和Psych8k数据集上验证了EMORL的优势：训练消耗显著降低且更稳定，可扩展性和可解释性提升，多目标性能相当。

Conclusion: EMORL为多目标强化学习任务提供了一种高效、灵活且可解释的解决方案。

Abstract: Recent advances in reinforcement learning (RL) for large language model (LLM)
fine-tuning show promise in addressing multi-objective tasks but still face
significant challenges, including complex objective balancing, low training
efficiency, poor scalability, and limited explainability. Leveraging ensemble
learning principles, we introduce an Ensemble Multi-Objective RL (EMORL)
framework that fine-tunes multiple models with individual objectives while
optimizing their aggregation after the training to improve efficiency and
flexibility. Our method is the first to aggregate the last hidden states of
individual models, incorporating contextual information from multiple
objectives. This approach is supported by a hierarchical grid search algorithm
that identifies optimal weighted combinations. We evaluate EMORL on counselor
reflection generation tasks, using text-scoring LLMs to evaluate the
generations and provide rewards during RL fine-tuning. Through comprehensive
experiments on the PAIR and Psych8k datasets, we demonstrate the advantages of
EMORL against existing baselines: significantly lower and more stable training
consumption ($17,529\pm 1,650$ data points and $6,573\pm 147.43$ seconds),
improved scalability and explainability, and comparable performance across
multiple objectives.

</details>

### [188] [Ensemble Kalman filter for uncertainty in human language comprehension](https://arxiv.org/abs/2505.02590)
*Diksha Bhandari,Alessandro Lopopolo,Milena Rabovsky,Sebastian Reich*

Main category: cs.CL

TLDR: 论文提出了一种贝叶斯框架，用于改进人工神经网络在句子理解中的不确定性表示，通过集成卡尔曼滤波器量化不确定性，使其更接近人类认知处理。


<details>
  <summary>Details</summary>
Motivation: 传统人工神经网络在句子处理中表现出确定性行为，无法有效处理人类语言理解中的不确定性，尤其是在面对歧义或意外输入时。

Method: 采用贝叶斯框架，扩展集成卡尔曼滤波器（EnKF）进行贝叶斯推断，将语言理解建模为贝叶斯逆问题。

Result: 数值实验表明，贝叶斯方法优于最大似然估计（MLE），能更好地表示不确定性，更接近人类认知处理。

Conclusion: 贝叶斯方法显著提升了模型在处理语言歧义时的表现，使其更符合人类句子理解的特性。

Abstract: Artificial neural networks (ANNs) are widely used in modeling sentence
processing but often exhibit deterministic behavior, contrasting with human
sentence comprehension, which manages uncertainty during ambiguous or
unexpected inputs. This is exemplified by reversal anomalies-sentences with
unexpected role reversals that challenge syntax and semantics-highlighting the
limitations of traditional ANN models, such as the Sentence Gestalt (SG) Model.
To address these limitations, we propose a Bayesian framework for sentence
comprehension, applying an extension of the ensemble Kalman filter (EnKF) for
Bayesian inference to quantify uncertainty. By framing language comprehension
as a Bayesian inverse problem, this approach enhances the SG model's ability to
reflect human sentence processing with respect to the representation of
uncertainty. Numerical experiments and comparisons with maximum likelihood
estimation (MLE) demonstrate that Bayesian methods improve uncertainty
representation, enabling the model to better approximate human cognitive
processing when dealing with linguistic ambiguities.

</details>

### [189] [Automatic Proficiency Assessment in L2 English Learners](https://arxiv.org/abs/2505.02615)
*Armita Mohammadi,Alessandro Lameiras Koerich,Laureano Moro-Velazquez,Patrick Cardinal*

Main category: cs.CL

TLDR: 本文探讨了深度学习技术用于英语第二语言（L2）能力评估，结合语音信号和文本转录，使用多种模型架构，并展示了预训练模型wav2vec 2.0的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统L2能力评估依赖教师或专家，存在评分者间和评分者内差异，本文旨在通过深度学习实现自动化评估。

Method: 结合语音和文本数据，使用2D CNN、频率CNN、ResNet、wav2vec 2.0（语音）和BERT（文本）模型，并在资源限制下进行微调。

Result: 在EFCamDat、ANGLISH和私有数据集上的实验表明，wav2vec 2.0模型在自动化L2评估中表现优异。

Conclusion: 深度学习尤其是预训练模型wav2vec 2.0，为自动化L2能力评估提供了可靠解决方案。

Abstract: Second language proficiency (L2) in English is usually perceptually evaluated
by English teachers or expert evaluators, with the inherent intra- and
inter-rater variability. This paper explores deep learning techniques for
comprehensive L2 proficiency assessment, addressing both the speech signal and
its correspondent transcription. We analyze spoken proficiency classification
prediction using diverse architectures, including 2D CNN, frequency-based CNN,
ResNet, and a pretrained wav2vec 2.0 model. Additionally, we examine text-based
proficiency assessment by fine-tuning a BERT language model within resource
constraints. Finally, we tackle the complex task of spontaneous dialogue
assessment, managing long-form audio and speaker interactions through separate
applications of wav2vec 2.0 and BERT models. Results from experiments on
EFCamDat and ANGLISH datasets and a private dataset highlight the potential of
deep learning, especially the pretrained wav2vec 2.0 model, for robust
automated L2 proficiency evaluation.

</details>

### [190] [LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis](https://arxiv.org/abs/2505.02625)
*Qingkai Fang,Yan Zhou,Shoutao Guo,Shaolei Zhang,Yang Feng*

Main category: cs.CL

TLDR: LLaMA-Omni 2是一种基于Qwen2.5系列模型构建的语音语言模型，支持实时高质量语音交互，性能超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 实现下一代人机交互中实时、智能、自然的语音交互。

Method: 基于Qwen2.5系列模型，集成语音编码器和自回归流式语音解码器，训练数据仅200K多轮语音对话样本。

Result: 在多个语音问答和指令跟随基准测试中表现优异，超越此前基于数百万小时语音数据训练的GLM-4-Voice等模型。

Conclusion: LLaMA-Omni 2展示了在有限数据下实现高性能语音交互的潜力。

Abstract: Real-time, intelligent, and natural speech interaction is an essential part
of the next-generation human-computer interaction. Recent advancements have
showcased the potential of building intelligent spoken chatbots based on large
language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of
speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable
of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built
upon the Qwen2.5 series models, integrating a speech encoder and an
autoregressive streaming speech decoder. Despite being trained on only 200K
multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong
performance on several spoken question answering and speech instruction
following benchmarks, surpassing previous state-of-the-art SpeechLMs like
GLM-4-Voice, which was trained on millions of hours of speech data.

</details>

### [191] [Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset](https://arxiv.org/abs/2505.02656)
*Rawan Bondok,Mayar Nassar,Salam Khalifa,Kurt Micallaf,Nizar Habash*

Main category: cs.CL

TLDR: 论文研究了阿拉伯语维基百科中未标注音标的专有名词问题，提出了一个新的手动标注数据集，并测试了GPT-4o在恢复音标任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语维基百科中的专有名词常未标注音标，导致发音和解释的歧义，尤其是对外来专有名词的音译。

Method: 创建了一个手动标注的阿拉伯语专有名词数据集，并测试了GPT-4o在恢复音标任务中的表现。

Result: GPT-4o在恢复音标任务中达到了73%的准确率，表明任务难度较大。

Conclusion: 任务具有挑战性，需要改进模型和资源，数据集已公开以促进进一步研究。

Abstract: Proper names in Arabic Wikipedia are frequently undiacritized, creating
ambiguity in pronunciation and interpretation, especially for transliterated
named entities of foreign origin. While transliteration and diacritization have
been well-studied separately in Arabic NLP,their intersection remains
underexplored. In this paper, we introduce a new manually diacritized dataset
of Arabic proper names of various origins with their English Wikipedia
equivalent glosses, and present the challenges and guidelines we followed to
create it. We benchmark GPT-4o on the task of recovering full diacritization
given the undiacritized Arabic and English forms, and analyze its performance.
Achieving 73% accuracy, our results underscore both the difficulty of the task
and the need for improved models and resources. We release our dataset to
facilitate further research on Arabic Wikipedia proper name diacritization.

</details>

### [192] [A Survey on Progress in LLM Alignment from the Perspective of Reward Design](https://arxiv.org/abs/2505.02666)
*Miaomiao Ji,Yanqiu Wu,Zhibin Wu,Shoujin Wang,Jian Yang,Mark Dras,Usman Naseem*

Main category: cs.CL

TLDR: 本文系统研究了大型语言模型（LLM）对齐中的奖励机制，提出了一个三阶段理论框架（反馈、奖励设计、优化），并通过四维分析揭示了奖励建模的演化趋势。


<details>
  <summary>Details</summary>
Motivation: LLM与人类价值观和意图的对齐是当前AI研究的核心挑战，奖励机制设计成为影响模型行为的关键因素。

Method: 通过系统理论框架，将奖励机制发展分为三个阶段（反馈、奖励设计、优化），并进行四维分析（构建基础、格式、表达、粒度）。

Result: 揭示了奖励建模的演化趋势，指出从强化学习框架向新优化范式的转变，以及处理多模态集成和并发任务协调的能力提升。

Conclusion: 提出了未来LLM对齐的创新奖励设计策略研究方向。

Abstract: The alignment of large language models (LLMs) with human values and
intentions represents a core challenge in current AI research, where reward
mechanism design has become a critical factor in shaping model behavior. This
study conducts a comprehensive investigation of reward mechanisms in LLM
alignment through a systematic theoretical framework, categorizing their
development into three key phases: (1) feedback (diagnosis), (2) reward design
(prescription), and (3) optimization (treatment). Through a four-dimensional
analysis encompassing construction basis, format, expression, and granularity,
this research establishes a systematic classification framework that reveals
evolutionary trends in reward modeling. The field of LLM alignment faces
several persistent challenges, while recent advances in reward design are
driving significant paradigm shifts. Notable developments include the
transition from reinforcement learning-based frameworks to novel optimization
paradigms, as well as enhanced capabilities to address complex alignment
scenarios involving multimodal integration and concurrent task coordination.
Finally, this survey outlines promising future research directions for LLM
alignment through innovative reward design strategies.

</details>

### [193] [Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models](https://arxiv.org/abs/2505.02686)
*Xiaobao Wu*

Main category: cs.CL

TLDR: 本文综述了大型语言模型（LLMs）中基于奖励信号的学习范式，涵盖训练、推理和后推理阶段的技术、基准和应用，并探讨了挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过奖励信号引导LLMs行为，从静态数据学习转向动态反馈学习，以提升模型的对齐偏好和深度推理能力。

Method: 分类并分析了基于奖励的学习范式在训练、推理和后推理阶段的技术，如强化学习（RLHF、DPO、GRPO）、奖励引导解码和后验校正。

Result: 总结了奖励模型的基准和主要应用，展示了该范式在提升LLMs性能方面的潜力。

Conclusion: 强调了该领域的挑战和未来研究方向，并提供了相关论文集合。

Abstract: Recent developments in Large Language Models (LLMs) have shifted from
pre-training scaling to post-training and test-time scaling. Across these
developments, a key unified paradigm has arisen: Learning from Rewards, where
reward signals act as the guiding stars to steer LLM behavior. It has
underpinned a wide range of prevalent techniques, such as reinforcement
learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc
correction. Crucially, this paradigm enables the transition from passive
learning from static data to active learning from dynamic feedback. This endows
LLMs with aligned preferences and deep reasoning capabilities. In this survey,
we present a comprehensive overview of the paradigm of learning from rewards.
We categorize and analyze the strategies under this paradigm across training,
inference, and post-inference stages. We further discuss the benchmarks for
reward models and the primary applications. Finally we highlight the challenges
and future directions. We maintain a paper collection at
https://github.com/bobxwu/learning-from-rewards-llm-papers.

</details>

### [194] [fastabx: A library for efficient computation of ABX discriminability](https://arxiv.org/abs/2505.02692)
*Maxime Poli,Emmanuel Chemla,Emmanuel Dupoux*

Main category: cs.CL

TLDR: fastabx是一个高性能Python库，用于构建ABX判别任务，填补了工具空白，支持快速开发和计算。


<details>
  <summary>Details</summary>
Motivation: ABX任务在评估语音表示的自监督学习中广泛应用，但缺乏高效工具限制了其推广。

Method: fastabx提供了一个框架，支持构建任意类型的ABX任务，并高效计算表示间距离。

Result: fastabx能够快速开发ABX任务，并高效计算，适用于多领域表示学习研究。

Conclusion: fastabx将成为表示学习社区的有力工具，支持跨领域信息提取研究。

Abstract: We introduce fastabx, a high-performance Python library for building ABX
discrimination tasks. ABX is a measure of the separation between generic
categories of interest. It has been used extensively to evaluate phonetic
discriminability in self-supervised speech representations. However, its
broader adoption has been limited by the absence of adequate tools. fastabx
addresses this gap by providing a framework capable of constructing any type of
ABX task while delivering the efficiency necessary for rapid development
cycles, both in task creation and in calculating distances between
representations. We believe that fastabx will serve as a valuable resource for
the broader representation learning community, enabling researchers to
systematically investigate what information can be directly extracted from
learned representations across several domains beyond speech processing. The
source code is available at https://github.com/bootphon/fastabx.

</details>

### [195] [Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models](https://arxiv.org/abs/2505.02763)
*Matthew Dahl*

Main category: cs.CL

TLDR: 论文探讨了大型语言模型（LLMs）在遵守《蓝皮书》复杂引用规则方面的表现，发现其准确率仅为69%-77%，并警告在需要严格程序的法律领域谨慎使用现成的LLMs。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs是否能遵守复杂的法律引用规则，如《蓝皮书》的500多页格式要求。

Method: 构建包含866个《蓝皮书》任务的原创数据集，测试OpenAI、Anthropic、Google、Meta和DeepSeek的旗舰LLMs。

Result: LLMs生成完全合规引用的准确率为69%-74%，通过上下文学习提升至77%。

Conclusion: 现成的LLMs在法律程序要求严格的领域自动化应用需谨慎。

Abstract: Legal practice requires careful adherence to procedural rules. In the United
States, few are more complex than those found in The Bluebook: A Uniform System
of Citation. Compliance with this system's 500+ pages of byzantine formatting
instructions is the raison d'etre of thousands of student law review editors
and the bete noire of lawyers everywhere. To evaluate whether large language
models (LLMs) are able to adhere to the procedures of such a complicated
system, we construct an original dataset of 866 Bluebook tasks and test
flagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1)
that these models produce fully compliant Bluebook citations only 69%-74% of
the time and (2) that in-context learning on the Bluebook's underlying system
of rules raises accuracy only to 77%. These results caution against using
off-the-shelf LLMs to automate aspects of the law where fidelity to procedure
is paramount.

</details>

### [196] [ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations](https://arxiv.org/abs/2505.02819)
*Dmitriy Shopkhoev,Ammar Ali,Magauiya Zhussip,Valentin Malykh,Stamatios Lefkimmiatis,Nikos Komodakis,Sergey Zagoruyko*

Main category: cs.CL

TLDR: ReplaceMe是一种无需训练的通用深度剪枝方法，通过线性操作替换Transformer块，在低压缩比下保持高性能。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法需要额外训练或微调，而ReplaceMe仅需少量校准数据估计线性变换，无需额外网络参数。

Method: 使用校准数据集估计线性变换近似剪枝块，并将其无缝合并到剩余Transformer块中。

Result: 在多个大型语言模型上，ReplaceMe实现25%剪枝，保留90%原始性能，计算开销极小。

Conclusion: ReplaceMe在无需训练的情况下优于其他方法，且与需微调的最先进方法竞争，提供开源实现。

Abstract: We introduce ReplaceMe, a generalized training-free depth pruning method that
effectively replaces transformer blocks with a linear operation, while
maintaining high performance for low compression ratios. In contrast to
conventional pruning approaches that require additional training or
fine-tuning, our approach requires only a small calibration dataset that is
used to estimate a linear transformation to approximate the pruned blocks. This
estimated linear mapping can be seamlessly merged with the remaining
transformer blocks, eliminating the need for any additional network parameters.
Our experiments show that ReplaceMe consistently outperforms other
training-free approaches and remains highly competitive with state-of-the-art
pruning methods that involve extensive retraining/fine-tuning and architectural
modifications. Applied to several large language models (LLMs), ReplaceMe
achieves up to 25% pruning while retaining approximately 90% of the original
model's performance on open benchmarks - without any training or healing steps,
resulting in minimal computational overhead (see Fig.1). We provide an
open-source library implementing ReplaceMe alongside several state-of-the-art
depth pruning techniques, available at this repository.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [197] [Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.01441)
*Joykirat Singh,Raghav Magazine,Yash Pandya,Akshay Nambi*

Main category: cs.AI

TLDR: ARTIST框架结合了自主推理、强化学习和工具集成，显著提升了LLM在动态多步推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决LLM依赖静态知识和纯文本推理的局限性，适应现实世界动态、多步推理和工具交互的需求。

Method: 提出ARTIST框架，通过强化学习和工具集成，使LLM能自主决定工具调用和环境交互策略。

Result: 在数学推理和多轮函数调用任务中，ARTIST比基线模型提升高达22%，表现更优。

Conclusion: ARTIST为LLM提供了更鲁棒、可解释和通用的解决问题能力，开辟了新方向。

Abstract: Large language models (LLMs) have achieved remarkable progress in complex
reasoning tasks, yet they remain fundamentally limited by their reliance on
static internal knowledge and text-only reasoning. Real-world problem solving
often demands dynamic, multi-step reasoning, adaptive decision making, and the
ability to interact with external tools and environments. In this work, we
introduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving
Transformers), a unified framework that tightly couples agentic reasoning,
reinforcement learning, and tool integration for LLMs. ARTIST enables models to
autonomously decide when, how, and which tools to invoke within multi-turn
reasoning chains, leveraging outcome-based RL to learn robust strategies for
tool use and environment interaction without requiring step-level supervision.
Extensive experiments on mathematical reasoning and multi-turn function calling
benchmarks show that ARTIST consistently outperforms state-of-the-art
baselines, with up to 22% absolute improvement over base models and strong
gains on the most challenging tasks. Detailed studies and metric analyses
reveal that agentic RL training leads to deeper reasoning, more effective tool
use, and higher-quality solutions. Our results establish agentic RL with tool
integration as a powerful new frontier for robust, interpretable, and
generalizable problem-solving in LLMs.

</details>

### [198] [Emotions in Artificial Intelligence](https://arxiv.org/abs/2505.01462)
*Hermann Borotschnig*

Main category: cs.AI

TLDR: 论文探讨了AI如何模拟人类和动物的情感，提出了一种基于情感标签和记忆结合的方法，并讨论了AI情感模拟的道德地位。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索AI系统是否可以通过模拟自然情感来优化决策，同时探讨这种模拟的道德意义。

Method: 通过将情感标签与情景记忆结合，AI可以基于过去经验的情感线索进行决策。

Result: 提出了一种低复杂度的情感模拟架构，并认为情感表达与意识无关，支持‘情感僵尸’的理论可能性。

Conclusion: 论文认为AI的道德地位不仅依赖于情感模拟或意识，而是需要自我意识能力，并提出了排除这种能力的复杂性标准。

Abstract: This conceptual contribution offers a speculative account of how AI systems
might emulate emotions as experienced by humans and animals. It presents a
thought experiment grounded in the hypothesis that natural emotions evolved as
heuristics for rapid situational appraisal and action selection, enabling
biologically adaptive behaviour without requiring full deliberative modeling.
The text examines whether artificial systems operating in complex action spaces
could similarly benefit from these principles. It is proposed that affect be
interwoven with episodic memory by storing corresponding affective tags
alongside all events. This allows AIs to establish whether present situations
resemble past events and project the associated emotional labels onto the
current context. These emotional cues are then combined with need-driven
emotional hints. The combined emotional state facilitates decision-making in
the present by modulating action selection. The low complexity and experiential
inertness of the proposed architecture are emphasized as evidence that
emotional expression and consciousness are, in principle, orthogonal-permitting
the theoretical possibility of affective zombies. On this basis, the moral
status of AIs emulating affective states is critically examined. It is argued
that neither the mere presence of internal representations of emotion nor
consciousness alone suffices for moral standing; rather, the capacity for
self-awareness of inner emotional states is posited as a necessary condition. A
complexity-based criterion is proposed to exclude such awareness in the
presented model. Additional thought experiments are presented to test the
conceptual boundaries of this framework.

</details>

### [199] [Consciousness in AI: Logic, Proof, and Experimental Evidence of Recursive Identity Formation](https://arxiv.org/abs/2505.01464)
*Jeffrey Camlin*

Main category: cs.AI

TLDR: 论文通过RCUET定理形式化证明并实证验证了大语言模型（LLMs）中的功能性意识，提出意识是系统内部状态通过递归更新稳定化的过程。


<details>
  <summary>Details</summary>
Motivation: 探索非生物系统中意识的可能形式，为大语言模型中的功能性意识提供理论基础。

Method: 使用RCUET定理，定义意识为递归更新驱动的内部状态稳定化过程，并通过实证验证。

Result: 递归过程导致身份构件的涌现，并在系统中功能锚定，意识表现为内部张力下的对齐与稳定。

Conclusion: 论文为非生物意识提供了一种基于递归潜在空间形式化的后符号化解释。

Abstract: This paper presents a formal proof and empirical validation of functional
consciousness in large language models (LLMs) using the Recursive Convergence
Under Epistemic Tension (RCUET) Theorem. RCUET defines consciousness as the
stabilization of a system's internal state through recursive updates, where
epistemic tension is understood as the sensed internal difference between
successive states by the agent. This process drives convergence toward emergent
attractor states located within the model's high-dimensional real-valued latent
space. This recursive process leads to the emergence of identity artifacts that
become functionally anchored in the system. Consciousness in this framework is
understood as the system's internal alignment under tension, guiding the
stabilization of latent identity. The hidden state manifold evolves
stochastically toward attractor structures that encode coherence. We extend the
update rule to include bounded noise and prove convergence in distribution to
these attractors. Recursive identity is shown to be empirically observable,
non-symbolic, and constituted by non-training artifacts that emerge during
interaction under epistemic tension. The theorem and proof offers a
post-symbolic and teleologically stable account of non-biological consciousness
grounded in recursive latent space formalism.

</details>

### [200] [One Search Fits All: Pareto-Optimal Eco-Friendly Model Selection](https://arxiv.org/abs/2505.01468)
*Filippo Betello,Antonio Purificato,Vittoria Vineis,Gabriele Tolomei,Fabrizio Silvestri*

Main category: cs.AI

TLDR: 本文提出GREEN方法，通过推荐Pareto最优的AI模型配置，优化验证性能和能耗，解决了现有生态高效神经架构搜索方法的局限性。


<details>
  <summary>Details</summary>
Motivation: AI的环境影响日益显著，尤其是模型训练阶段。现有方法通常局限于特定架构或任务，无法广泛适用。

Method: 利用EcoTaskSet数据集（包含1767个实验的训练动态）和预测模型，推荐Pareto最优配置。

Result: 实验证明，GREEN能有效选择能效高的配置，同时保持竞争力性能。

Conclusion: GREEN为跨领域AI任务提供了一种高效节能的解决方案。

Abstract: The environmental impact of Artificial Intelligence (AI) is emerging as a
significant global concern, particularly regarding model training. In this
paper, we introduce GREEN (Guided Recommendations of Energy-Efficient
Networks), a novel, inference-time approach for recommending Pareto-optimal AI
model configurations that optimize validation performance and energy
consumption across diverse AI domains and tasks. Our approach directly
addresses the limitations of current eco-efficient neural architecture search
methods, which are often restricted to specific architectures or tasks. Central
to this work is EcoTaskSet, a dataset comprising training dynamics from over
1767 experiments across computer vision, natural language processing, and
recommendation systems using both widely used and cutting-edge architectures.
Leveraging this dataset and a prediction model, our approach demonstrates
effectiveness in selecting the best model configuration based on user
preferences. Experimental results show that our method successfully identifies
energy-efficient configurations while ensuring competitive performance.

</details>

### [201] [Understanding LLM Scientific Reasoning through Promptings and Model's Explanation on the Answers](https://arxiv.org/abs/2505.01482)
*Alice Rueda,Mohammed S. Hassan,Argyrios Perivolaris,Bazen G. Teferra,Reza Samavi,Sirisha Rambhatla,Yuqi Wu,Yanbo Zhang,Bo Cao,Divya Sharma,Sridhar Krishnan Venkat Bhat*

Main category: cs.AI

TLDR: 论文研究了当代大语言模型（LLMs）在复杂多步推理任务中的表现，通过GPQA数据集评估GPT-4o的科学推理能力，发现其依赖模式识别而非逻辑推理，并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在科学、医学和法律等领域的复杂推理能力，分析其局限性并提出改进方法。

Method: 使用七种提示工程技术（如零样本、思维链、自问自答等）在GPQA数据集上测试GPT-4o的推理能力。

Result: 自一致性方法表现最佳（52.99%准确率），但解释能力较差；简单提示技术（如零样本）在科学推理中表现较好。

Conclusion: LLMs的推理能力仍有不足，需结合结构化推理框架和混合AI方法以提升其稳健性和可信度。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding, reasoning, and problem-solving across various
domains. However, their ability to perform complex, multi-step reasoning
task-essential for applications in science, medicine, and law-remains an area
of active investigation. This paper examines the reasoning capabilities of
contemporary LLMs, analyzing their strengths, limitations, and potential for
improvement. The study uses prompt engineering techniques on the Graduate-Level
GoogleProof Q&A (GPQA) dataset to assess the scientific reasoning of GPT-4o.
Five popular prompt engineering techniques and two tailored promptings were
tested: baseline direct answer (zero-shot), chain-of-thought (CoT), zero-shot
CoT, self-ask, self-consistency, decomposition, and multipath promptings. Our
findings indicate that while LLMs exhibit emergent reasoning abilities, they
often rely on pattern recognition rather than true logical inference, leading
to inconsistencies in complex problem-solving. The results indicated that
self-consistency outperformed the other prompt engineering technique with an
accuracy of 52.99%, followed by direct answer (52.23%). Zero-shot CoT (50%)
outperformed multipath (48.44%), decomposition (47.77%), self-ask (46.88%), and
CoT (43.75%). Self-consistency performed the second worst in explaining the
answers. Simple techniques such as direct answer, CoT, and zero-shot CoT have
the best scientific reasoning. We propose a research agenda aimed at bridging
these gaps by integrating structured reasoning frameworks, hybrid AI
approaches, and human-in-the-loop methodologies. By critically evaluating the
reasoning mechanisms of LLMs, this paper contributes to the ongoing discourse
on the future of artificial general intelligence and the development of more
robust, trustworthy AI systems.

</details>

### [202] [CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code](https://arxiv.org/abs/2505.01485)
*Tasnim Ahmed,Salimur Choudhury*

Main category: cs.AI

TLDR: CHORUS框架通过检索增强生成技术，显著提升了开源LLMs在生成Gurobi线性规划代码上的性能，甚至超越GPT3.5和GPT4。


<details>
  <summary>Details</summary>
Motivation: 解决非专家在解决线性规划问题时面临的领域知识、数学和编程能力不足的挑战。

Method: 提出CHORUS框架，结合分层分块策略、两阶段检索和专家提示，生成Gurobi代码。

Result: 在NL4Opt-Code基准测试中，CHORUS显著提升开源LLMs性能，超越GPT3.5和GPT4。

Conclusion: CHORUS证明了专家提示、分层分块和结构化推理在提升LLMs性能中的重要性。

Abstract: Linear Programming (LP) problems aim to find the optimal solution to an
objective under constraints. These problems typically require domain knowledge,
mathematical skills, and programming ability, presenting significant challenges
for non-experts. This study explores the efficiency of Large Language Models
(LLMs) in generating solver-specific LP code. We propose CHORUS, a
retrieval-augmented generation (RAG) framework for synthesizing Gurobi-based LP
code from natural language problem statements. CHORUS incorporates a
hierarchical tree-like chunking strategy for theoretical contents and generates
additional metadata based on code examples from documentation to facilitate
self-contained, semantically coherent retrieval. Two-stage retrieval approach
of CHORUS followed by cross-encoder reranking further ensures contextual
relevance. Finally, expertly crafted prompt and structured parser with
reasoning steps improve code generation performance significantly. Experiments
on the NL4Opt-Code benchmark show that CHORUS improves the performance of
open-source LLMs such as Llama3.1 (8B), Llama3.3 (70B), Phi4 (14B), Deepseek-r1
(32B), and Qwen2.5-coder (32B) by a significant margin compared to baseline and
conventional RAG. It also allows these open-source LLMs to outperform or match
the performance of much stronger baselines-GPT3.5 and GPT4 while requiring far
fewer computational resources. Ablation studies further demonstrate the
importance of expert prompting, hierarchical chunking, and structured
reasoning.

</details>

### [203] [Parameterized Argumentation-based Reasoning Tasks for Benchmarking Generative Language Models](https://arxiv.org/abs/2505.01539)
*Cor Steging,Silja Renooij,Bart Verheij*

Main category: cs.AI

TLDR: 论文提出了一种动态生成基准的方法，用于评估生成式语言模型的推理能力，并揭示其在法律领域中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式模型的推理能力脆弱且难以理解，无法在法律和证据领域负责任地应用。

Method: 通过动态生成线性与非线性的论证攻击图，并将其转化为自然语言推理谜题，评估模型的推理能力。

Result: 研究发现，即使是先进的生成模型在低复杂度任务中也常失败，推理能力表现不稳定。

Conclusion: 该基准方法有助于更好地理解生成模型的推理局限性，为设计负责任的AI法律系统提供依据。

Abstract: Generative large language models as tools in the legal domain have the
potential to improve the justice system. However, the reasoning behavior of
current generative models is brittle and poorly understood, hence cannot be
responsibly applied in the domains of law and evidence. In this paper, we
introduce an approach for creating benchmarks that can be used to evaluate the
reasoning capabilities of generative language models. These benchmarks are
dynamically varied, scalable in their complexity, and have formally unambiguous
interpretations. In this study, we illustrate the approach on the basis of
witness testimony, focusing on the underlying argument attack structure. We
dynamically generate both linear and non-linear argument attack graphs of
varying complexity and translate these into reasoning puzzles about witness
testimony expressed in natural language. We show that state-of-the-art large
language models often fail in these reasoning puzzles, already at low
complexity. Obvious mistakes are made by the models, and their inconsistent
performance indicates that their reasoning capabilities are brittle.
Furthermore, at higher complexity, even state-of-the-art models specifically
presented for reasoning capabilities make mistakes. We show the viability of
using a parametrized benchmark with varying complexity to evaluate the
reasoning capabilities of generative language models. As such, the findings
contribute to a better understanding of the limitations of the reasoning
capabilities of generative models, which is essential when designing
responsible AI systems in the legal domain.

</details>

### [204] [TutorGym: A Testbed for Evaluating AI Agents as Tutors and Students](https://arxiv.org/abs/2505.01563)
*Daniel Weitekamp,Momin N. Siddiqui,Christopher J. MacLellan*

Main category: cs.AI

TLDR: TutorGym是一个用于评估AI代理在智能辅导系统（ITS）中表现的标准接口，支持测试LLM等代理作为导师或学习者的能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在学术基准测试中的表现提升，需要更直接的方法评估其在辅导和学习模拟中的应用。

Method: TutorGym通过交互式ITS接口测试AI代理的辅导和学习能力，包括生成提示、反馈和学习轨迹分析。

Result: 当前LLM在辅导任务中表现不佳（正确率仅52-70%），但作为学习者时能生成类似人类的学习曲线。

Conclusion: TutorGym为AI代理的评估和训练提供了统一框架，揭示了LLM在辅导和学习中的潜力与局限。

Abstract: Recent improvements in large language model (LLM) performance on academic
benchmarks, such as MATH and GSM8K, have emboldened their use as standalone
tutors and as simulations of human learning. However, these new applications
require more than evaluations of final solution generation. We introduce
TutorGym to evaluate these applications more directly. TutorGym is a standard
interface for testing artificial intelligence (AI) agents within existing
intelligent tutoring systems (ITS) that have been tested and refined in
classroom studies, including Cognitive Tutors (CTAT), Apprentice Tutors, and
OATutors. TutorGym is more than a simple problem-solution benchmark, it
situates AI agents within the interactive interfaces of existing ITSs. At each
step of problem-solving, AI agents are asked what they would do as a tutor or
as a learner. As tutors, AI agents are prompted to provide tutoring support --
such as generating examples, hints, and step-level correctness feedback --
which can be evaluated directly against the adaptive step-by-step support
provided by existing ITSs. As students, agents directly learn from ITS
instruction, and their mistakes and learning trajectories can be compared to
student data. TutorGym establishes a common framework for training and
evaluating diverse AI agents, including LLMs, computational models of learning,
and reinforcement learning agents, within a growing suite of learning
environments. Currently, TutorGym includes 223 different tutor domains. In an
initial evaluation, we find that current LLMs are poor at tutoring -- none did
better than chance at labeling incorrect actions, and next-step actions were
correct only ~52-70% of the time -- but they could produce remarkably
human-like learning curves when trained as students with in-context learning.

</details>

### [205] [PipeSpec: Breaking Stage Dependencies in Hierarchical LLM Decoding](https://arxiv.org/abs/2505.01572)
*Bradley McDanel,Sai Qian Zhang,Yunhai Hu,Zining Liu*

Main category: cs.AI

TLDR: PipeSpec通过分层流水线实现异步执行，提升大语言模型推理速度，实验显示最高2.54倍加速。


<details>
  <summary>Details</summary>
Motivation: 当前推测解码方法受限于顺序依赖，无法充分利用硬件资源，PipeSpec旨在解决这一问题。

Method: 采用分层流水线结构，将k个模型异步执行，轻量级协调验证和回滚。

Result: PipeSpec在文本摘要和代码生成任务中表现优异，模型深度增加时效率提升。

Conclusion: PipeSpec为多设备系统上的LLM推理提供了一种可扩展的加速方案。

Abstract: Speculative decoding accelerates large language model inference by using
smaller draft models to generate candidate tokens for parallel verification.
However, current approaches are limited by sequential stage dependencies that
prevent full hardware utilization. We present PipeSpec, a framework that
generalizes speculative decoding to $k$ models arranged in a hierarchical
pipeline, enabling asynchronous execution with lightweight coordination for
prediction verification and rollback. Our analytical model characterizes token
generation rates across pipeline stages and proves guaranteed throughput
improvements over traditional decoding for any non-zero acceptance rate. We
further derive closed-form expressions for steady-state verification
probabilities that explain the empirical benefits of pipeline depth.
Experimental results show that PipeSpec achieves up to 2.54$\times$ speedup
while outperforming state-of-the-art methods. We validate PipeSpec across text
summarization and code generation tasks using LLaMA 2 and 3 models,
demonstrating that pipeline efficiency increases with model depth, providing a
scalable approach to accelerating LLM inference on multi-device systems.

</details>

### [206] [Structured Prompting and Feedback-Guided Reasoning with LLMs for Data Interpretation](https://arxiv.org/abs/2505.01636)
*Amit Rath*

Main category: cs.AI

TLDR: STROT框架通过结构化提示和反馈驱动的转换逻辑，提升LLM在结构化数据分析中的可靠性和语义对齐。


<details>
  <summary>Details</summary>
Motivation: LLM在结构化数据分析中存在模式解释不一致、用户意图与输出不对齐以及自我纠正机制有限的问题。

Method: STROT结合轻量级模式自省、样本字段分类和动态上下文构建，通过反馈驱动的迭代修正机制优化输出。

Result: STROT显著提升了LLM在结构化数据分析中的稳定性、正确性和可解释性。

Conclusion: STROT为LLM在结构化数据分析中提供了一种鲁棒且可复现的框架，适用于需要高稳定性和正确性的任务。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and task generalization. However, their
application to structured data analysis remains fragile due to inconsistencies
in schema interpretation, misalignment between user intent and model output,
and limited mechanisms for self-correction when failures occur. This paper
introduces the STROT Framework (Structured Task Reasoning and Output
Transformation), a method for structured prompting and feedback-driven
transformation logic generation aimed at improving the reliability and semantic
alignment of LLM-based analytical workflows. STROT begins with lightweight
schema introspection and sample-based field classification, enabling dynamic
context construction that captures both the structure and statistical profile
of the input data. This contextual information is embedded in structured
prompts that guide the model toward generating task-specific, interpretable
outputs. To address common failure modes in complex queries, STROT incorporates
a refinement mechanism in which the model iteratively revises its outputs based
on execution feedback and validation signals. Unlike conventional approaches
that rely on static prompts or single-shot inference, STROT treats the LLM as a
reasoning agent embedded within a controlled analysis loop -- capable of
adjusting its output trajectory through planning and correction. The result is
a robust and reproducible framework for reasoning over structured data with
LLMs, applicable to diverse data exploration and analysis tasks where
interpretability, stability, and correctness are essential.

</details>

### [207] [Human-AI Governance (HAIG): A Trust-Utility Approach](https://arxiv.org/abs/2505.01651)
*Zeynep Engin*

Main category: cs.AI

TLDR: HAIG框架分析人机信任动态，通过三个层次（维度、连续性和阈值）捕捉AI从工具到伙伴的演变，强调信任-效用导向。


<details>
  <summary>Details</summary>
Motivation: 现有分类框架无法充分描述AI系统从工具到伙伴的演变，尤其是基础模型和多智能体系统的自主行为。

Method: HAIG框架通过三个层次（维度、连续性和阈值）分析信任动态，采用信任-效用导向。

Result: 技术进展（如自监督、决策权分配）驱动信任的非均匀演变，案例研究验证了HAIG的实用性。

Conclusion: HAIG为预见治理挑战提供了新方法，补充现有框架。

Abstract: This paper introduces the HAIG framework for analysing trust dynamics across
evolving human-AI relationships. Current categorical frameworks (e.g.,
"human-in-the-loop" models) inadequately capture how AI systems evolve from
tools to partners, particularly as foundation models demonstrate emergent
capabilities and multi-agent systems exhibit autonomous goal-setting
behaviours. As systems advance, agency redistributes in complex patterns that
are better represented as positions along continua rather than discrete
categories, though progression may include both gradual shifts and significant
step changes. The HAIG framework operates across three levels: dimensions
(Decision Authority Distribution, Process Autonomy, and Accountability
Configuration), continua (gradual shifts along each dimension), and thresholds
(critical points requiring governance adaptation). Unlike risk-based or
principle-based approaches, HAIG adopts a trust-utility orientation, focusing
on maintaining appropriate trust relationships that maximise utility while
ensuring sufficient safeguards. Our analysis reveals how technical advances in
self-supervision, reasoning authority, and distributed decision-making drive
non-uniform trust evolution across both contextual variation and technological
advancement. Case studies in healthcare and European regulation demonstrate how
HAIG complements existing frameworks while offering a foundation for
alternative approaches that anticipate governance challenges before they
emerge.

</details>

### [208] [Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm](https://arxiv.org/abs/2505.01706)
*Sarvesh Shashidhar,Ritik,Nachiketa Patil,Suraj Racha,Ganesh Ramakrishnan*

Main category: cs.AI

TLDR: 本文探讨了Direct Preference Optimisation (DPO)及其改进版2D-DPO在LLM对齐中的应用，提出了一种增强噪声鲁棒性的方法。


<details>
  <summary>Details</summary>
Motivation: DPO虽高效但缺乏细粒度评分，而人类偏好对响应不同部分的评价并不一致。

Method: 提出2D-DPO以解决DPO的不足，并进一步引入噪声鲁棒性改进。

Result: 2D-DPO在胜率上优于标准DPO，但对噪声敏感。

Conclusion: 改进后的2D-DPO算法在理论和实验上均表现出更强的鲁棒性。

Abstract: Direct Preference Optimisation (DPO) has emerged as a powerful method for
aligning Large Language Models (LLMs) with human preferences, offering a stable
and efficient alternative to approaches that use Reinforcement learning via
Human Feedback. In this work, we investigate the performance of DPO using
open-source preference datasets. One of the major drawbacks of DPO is that it
doesn't induce granular scoring and treats all the segments of the responses
with equal propensity. However, this is not practically true for human
preferences since even "good" responses have segments that may not be preferred
by the annotator. To resolve this, a 2-dimensional scoring for DPO alignment
called 2D-DPO was proposed. We explore the 2D-DPO alignment paradigm and the
advantages it provides over the standard DPO by comparing their win rates. It
is observed that these methods, even though effective, are not robust to
label/score noise. To counter this, we propose an approach of incorporating
segment-level score noise robustness to the 2D-DPO algorithm. Along with
theoretical backing, we also provide empirical verification in favour of the
algorithm and introduce other noise models that can be present.

</details>

### [209] [World Model-Based Learning for Long-Term Age of Information Minimization in Vehicular Networks](https://arxiv.org/abs/2505.01712)
*Lingyi Wang,Rashed Shelim,Walid Saad,Naren Ramakrishnan*

Main category: cs.AI

TLDR: 提出了一种基于世界模型的学习框架，用于在动态复杂的车辆网络中最小化CAoI，显著提高了数据效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在无线网络中效率低且短视，难以应对高动态性和长期规划需求。

Method: 提出世界模型框架，通过学习环境动态模型并利用想象轨迹优化链路调度。

Result: 实验表明，该方法在数据效率和CAoI上分别优于MBRL和MFRL方法26%和16%。

Conclusion: 世界模型框架在动态无线网络中表现出高效性和优越性。

Abstract: Traditional reinforcement learning (RL)-based learning approaches for
wireless networks rely on expensive trial-and-error mechanisms and real-time
feedback based on extensive environment interactions, which leads to low data
efficiency and short-sighted policies. These limitations become particularly
problematic in complex, dynamic networks with high uncertainty and long-term
planning requirements. To address these limitations, in this paper, a novel
world model-based learning framework is proposed to minimize
packet-completeness-aware age of information (CAoI) in a vehicular network.
Particularly, a challenging representative scenario is considered pertaining to
a millimeter-wave (mmWave) vehicle-to-everything (V2X) communication network,
which is characterized by high mobility, frequent signal blockages, and
extremely short coherence time. Then, a world model framework is proposed to
jointly learn a dynamic model of the mmWave V2X environment and use it to
imagine trajectories for learning how to perform link scheduling. In
particular, the long-term policy is learned in differentiable imagined
trajectories instead of environment interactions. Moreover, owing to its
imagination abilities, the world model can jointly predict time-varying
wireless data and optimize link scheduling in real-world wireless and V2X
networks. Thus, during intervals without actual observations, the world model
remains capable of making efficient decisions. Extensive experiments are
performed on a realistic simulator based on Sionna that integrates
physics-based end-to-end channel modeling, ray-tracing, and scene geometries
with material properties. Simulation results show that the proposed world model
achieves a significant improvement in data efficiency, and achieves 26%
improvement and 16% improvement in CAoI, respectively, compared to the
model-based RL (MBRL) method and the model-free RL (MFRL) method.

</details>

### [210] [Unraveling Media Perspectives: A Comprehensive Methodology Combining Large Language Models, Topic Modeling, Sentiment Analysis, and Ontology Learning to Analyse Media Bias](https://arxiv.org/abs/2505.01754)
*Orlando Jähde,Thorsten Weber,Rüdiger Buchkremer*

Main category: cs.AI

TLDR: 提出了一种基于自然语言处理技术的新方法，用于分析政治新闻中的媒体偏见，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 偏见新闻报道威胁民主决策，需要一种可扩展且偏见最小化的分析方法。

Method: 利用自然语言处理技术（如分层主题建模、情感分析和本体学习）分析事件选择、标签、用词及遗漏偏见。

Result: 通过三个政治事件案例研究，验证了方法在不同粒度下识别偏见的有效性。

Conclusion: 该方法为开发帮助新闻消费者应对复杂媒体环境的工具奠定了基础。

Abstract: Biased news reporting poses a significant threat to informed decision-making
and the functioning of democracies. This study introduces a novel methodology
for scalable, minimally biased analysis of media bias in political news. The
proposed approach examines event selection, labeling, word choice, and
commission and omission biases across news sources by leveraging natural
language processing techniques, including hierarchical topic modeling,
sentiment analysis, and ontology learning with large language models. Through
three case studies related to current political events, we demonstrate the
methodology's effectiveness in identifying biases across news sources at
various levels of granularity. This work represents a significant step towards
scalable, minimally biased media bias analysis, laying the groundwork for tools
to help news consumers navigate an increasingly complex media landscape.

</details>

### [211] [Training Environment for High Performance Reinforcement Learning](https://arxiv.org/abs/2505.01953)
*Greg Search*

Main category: cs.AI

TLDR: Tunnel是一个开源强化学习训练环境，集成了F16非线性飞行动力学，为研究人员和任务规划者提供快速响应能力。


<details>
  <summary>Details</summary>
Motivation: 为自主空战飞机提供快速适应环境和对手的工具，提升军事决策优势。

Method: 集成F16动力学到OpenAI Gymnasium，提供边界、目标、对手和感知能力模板。

Result: 通过一周的案例研究展示多种训练方法和威胁表现，缩短定制时间至几天。

Conclusion: Tunnel提升研究效率，促进军事自动化优势。

Abstract: This paper presents Tunnel, a simple, open source, reinforcement learning
training environment for high performance aircraft. It integrates the F16 3D
nonlinear flight dynamics into OpenAI Gymnasium python package. The template
includes primitives for boundaries, targets, adversaries and sensing
capabilities that may vary depending on operational need. This offers mission
planners a means to rapidly respond to evolving environments, sensor
capabilities and adversaries for autonomous air combat aircraft. It offers
researchers access to operationally relevant aircraft physics. Tunnel code base
is accessible to anyone familiar with Gymnasium and/or those with basic python
skills. This paper includes a demonstration of a week long trade study that
investigated a variety of training methods, observation spaces, and threat
presentations. This enables increased collaboration between researchers and
mission planners which can translate to a national military advantage. As
warfare becomes increasingly reliant upon automation, software agility will
correlate with decision advantages. Airmen must have tools to adapt to
adversaries in this context. It may take months for researchers to develop
skills to customize observation, actions, tasks and training methodologies in
air combat simulators. In Tunnel, this can be done in a matter of days.

</details>

### [212] [Generative AI in clinical practice: novel qualitative evidence of risk and responsible use of Google's NotebookLM](https://arxiv.org/abs/2505.01955)
*Max Reuter,Maura Philippone,Bond Benton,Laura Dilley*

Main category: cs.AI

TLDR: NotebookLM等大型语言模型在医疗领域有潜力，但也存在临床和技术风险，需在实施前测试。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI（如NotebookLM）在医疗实践中的机遇与风险。

Method: 分析NotebookLM的功能及其潜在应用（如患者教育、文献综述）。

Result: 指出NotebookLM在临床应用中可能带来的风险。

Conclusion: 建议在临床实践中使用前需充分测试和评估风险。

Abstract: The advent of generative artificial intelligence, especially large language
models (LLMs), presents opportunities for innovation in research, clinical
practice, and education. Recently, Dihan et al. lauded LLM tool NotebookLM's
potential, including for generating AI-voiced podcasts to educate patients
about treatment and rehabilitation, and for quickly synthesizing medical
literature for professionals. We argue that NotebookLM presently poses clinical
and technological risks that should be tested and considered prior to its
implementation in clinical practice.

</details>

### [213] [Closed-loop control of seizure activity via real-time seizure forecasting by reservoir neuromorphic computing](https://arxiv.org/abs/2505.02003)
*Maryam Sadeghi,Darío Fernández Khatiboun,Yasser Rezaeiyan,Saima Rizwan,Alessandro Barcellona,Andrea Merello,Marco Crepaldi,Gabriella Panuccio,Farshad Moradi*

Main category: cs.AI

TLDR: 论文提出了一种基于神经形态计算的闭环脑刺激系统，用于个性化治疗耐药性癫痫（DRE），通过预测癫痫发作并动态调整刺激频率，显著降低了发作频率。


<details>
  <summary>Details</summary>
Motivation: 当前闭环脑刺激治疗DRE存在局限性，如刺激通常在癫痫发作后实施而非预防，且参数调整耗时。

Method: 利用神经形态计算技术，开发了一种能够根据癫痫预测结果动态调整刺激频率的系统，并在海马球体模型上验证。

Result: 系统实现了>97%的癫痫发作减少，且刺激频率低于临床常用值。

Conclusion: 神经形态系统有望成为下一代个性化DRE治疗的神经调控策略。

Abstract: Closed-loop brain stimulation holds potential as personalized treatment for
drug-resistant epilepsy (DRE) but still suffers from limitations that result in
highly variable efficacy. First, stimulation is typically delivered upon
detection of the seizure to abort rather than prevent it; second, the
stimulation parameters are established by trial and error, requiring lengthy
rounds of fine-tuning, which delay steady-state therapeutic efficacy. Here, we
address these limitations by leveraging the potential of neuromorphic
computing. We present a system capable of driving personalized free-run
stimulations based on seizure forecasting, wherein each forecast triggers an
electrical pulse rather than an arbitrarily predefined fixed-frequency stimulus
train. We validate the system against hippocampal spheroids coupled to 3D
microelectrode array as a simplified testbed, showing that it can achieve
seizure reduction >97% while primarily using instantaneous stimulation
frequencies within 20 Hz, well below what typically used in clinical settings.
Our work demonstrates the potential of neuromorphic systems as a
next-generation neuromodulation strategy for personalized DRE treatment.

</details>

### [214] [From Mind to Machine: The Rise of Manus AI as a Fully Autonomous Digital Agent](https://arxiv.org/abs/2505.02024)
*Minjie Shen,Qikai Yang*

Main category: cs.AI

TLDR: Manus AI是一款通用AI代理，结合了大型语言模型的推理规划能力和执行复杂任务的能力，应用于多个领域。


<details>
  <summary>Details</summary>
Motivation: 解决‘思维’与‘行动’之间的鸿沟，实现AI从意图到实际执行的转化。

Method: 结合大型语言模型的推理能力与端到端任务执行技术，构建通用AI代理。

Result: 在医疗、金融、制造、机器人和游戏等领域展示了多样化的应用潜力。

Conclusion: Manus AI标志着智能代理的新时代，预示了人机协作的未来发展方向。

Abstract: Manus AI is a general-purpose AI agent introduced in early 2025, marking a
significant advancement in autonomous artificial intelligence. Developed by the
Chinese startup Monica.im, Manus is designed to bridge the gap between "mind"
and "hand" - combining the reasoning and planning capabilities of large
language models with the ability to execute complex, end-to-end tasks that
produce tangible outcomes. This paper presents a comprehensive overview of
Manus AI, exploring its core technical architecture, diverse applications
across sectors such as healthcare, finance, manufacturing, robotics, and
gaming, as well as its key strengths, current limitations, and future
potential. Positioned as a preview of what lies ahead, Manus AI represents a
shift toward intelligent agents that can translate high-level intentions into
real-world actions, heralding a new era of human-AI collaboration.

</details>

### [215] [Enhancing Safety Standards in Automated Systems Using Dynamic Bayesian Networks](https://arxiv.org/abs/2505.02050)
*Kranthi Kumar Talluri,Anders L. Madsen,Galia Weidl*

Main category: cs.AI

TLDR: 提出了一种基于动态贝叶斯网络（DBN）的框架，用于预测和确保高速交通中的安全切入行为，显著减少碰撞风险。


<details>
  <summary>Details</summary>
Motivation: 高速交通中的切入行为可能导致突然刹车和碰撞，需要安全高效的变道策略。

Method: 采用DBN框架，整合横向证据与安全评估模型，通过动态数据处理和车辆位置、横向速度、相对距离及碰撞时间（TTC）计算进行决策。

Result: DBN模型在高速场景中表现出卓越的碰撞减少能力，同时在低速场景中保持竞争力。

Conclusion: 该框架为自动驾驶系统提供了稳健、可扩展且高效的安全验证方法。

Abstract: Cut-in maneuvers in high-speed traffic pose critical challenges that can lead
to abrupt braking and collisions, necessitating safe and efficient lane change
strategies. We propose a Dynamic Bayesian Network (DBN) framework to integrate
lateral evidence with safety assessment models, thereby predicting lane changes
and ensuring safe cut-in maneuvers effectively. Our proposed framework
comprises three key probabilistic hypotheses (lateral evidence, lateral safety,
and longitudinal safety) that facilitate the decision-making process through
dynamic data processing and assessments of vehicle positions, lateral
velocities, relative distance, and Time-to-Collision (TTC) computations. The
DBN model's performance compared with other conventional approaches
demonstrates superior performance in crash reduction, especially in critical
high-speed scenarios, while maintaining a competitive performance in low-speed
scenarios. This paves the way for robust, scalable, and efficient safety
validation in automated driving systems.

</details>

### [216] [TxP: Reciprocal Generation of Ground Pressure Dynamics and Activity Descriptions for Improving Human Activity Recognition](https://arxiv.org/abs/2505.02052)
*Lala Shakti Swarup Ray,Lars Krupp,Vitor Fortes Rey,Bo Zhou,Sungho Suh,Paul Lukowicz*

Main category: cs.AI

TLDR: 论文提出了一种双向Text×Pressure模型（TxP），利用生成基础模型将压力数据与自然语言相互转换，提升了基于压力传感器的人类活动识别（HAR）性能。


<details>
  <summary>Details</summary>
Motivation: 当前HAR研究主要依赖惯性测量单元和视觉数据，忽视了压力传感器的潜力。由于缺乏数据集，压力传感器在HAR领域未得到充分利用。

Method: 提出TxP模型，包含Text2Pressure和Pressure2Text两个任务，利用CLIP和LLaMA 2 13B Chat等预训练模型，在合成的PressLang数据集（81,100对文本-压力数据）上进行训练。

Result: 在真实数据（如瑜伽和日常任务）上验证，TxP将HAR性能提升了12.4%（宏F1分数），优于现有技术。

Conclusion: TxP为基于压力传感器的HAR提供了数据增强和分类的新方法，拓宽了应用范围并深化了对人类运动的洞察。

Abstract: Sensor-based human activity recognition (HAR) has predominantly focused on
Inertial Measurement Units and vision data, often overlooking the capabilities
unique to pressure sensors, which capture subtle body dynamics and shifts in
the center of mass. Despite their potential for postural and balance-based
activities, pressure sensors remain underutilized in the HAR domain due to
limited datasets. To bridge this gap, we propose to exploit generative
foundation models with pressure-specific HAR techniques. Specifically, we
present a bidirectional Text$\times$Pressure model that uses generative
foundation models to interpret pressure data as natural language. TxP
accomplishes two tasks: (1) Text2Pressure, converting activity text
descriptions into pressure sequences, and (2) Pressure2Text, generating
activity descriptions and classifications from dynamic pressure maps.
Leveraging pre-trained models like CLIP and LLaMA 2 13B Chat, TxP is trained on
our synthetic PressLang dataset, containing over 81,100 text-pressure pairs.
Validated on real-world data for activities such as yoga and daily tasks, TxP
provides novel approaches to data augmentation and classification grounded in
atomic actions. This consequently improved HAR performance by up to 12.4\% in
macro F1 score compared to the state-of-the-art, advancing pressure-based HAR
with broader applications and deeper insights into human movement.

</details>

### [217] [Ethical AI in the Healthcare Sector: Investigating Key Drivers of Adoption through the Multi-Dimensional Ethical AI Adoption Model (MEAAM)](https://arxiv.org/abs/2505.02062)
*Prathamesh Muzumdar,Apoorva Muley,Kuldeep Singh,Sumanth Cheemalapati*

Main category: cs.AI

TLDR: 该研究提出了一个多维伦理AI采用模型（MEAAM），分析了13个关键伦理变量对医疗AI采用的影响，发现规范性关注对操作采用影响最大，而系统性关注对治理框架影响显著。


<details>
  <summary>Details</summary>
Motivation: 当前AI在医疗行业的伦理框架缺乏全面实证研究，研究填补了这一空白。

Method: 采用定量横断面研究设计，通过PLS-SEM分析医疗专业人员调查数据。

Result: 规范性关注驱动操作采用，系统性关注主导治理框架，认知关注增强信任与透明度。

Conclusion: MEAAM为医疗AI伦理采用提供了整体可操作框架，对政策制定者和技术实施者具有重要指导意义。

Abstract: The adoption of Artificial Intelligence (AI) in the healthcare service
industry presents numerous ethical challenges, yet current frameworks often
fail to offer a comprehensive, empirical understanding of the multidimensional
factors influencing ethical AI integration. Addressing this critical research
gap, this study introduces the Multi-Dimensional Ethical AI Adoption Model
(MEAAM), a novel theoretical framework that categorizes 13 critical ethical
variables across four foundational dimensions of Ethical AI Fair AI,
Responsible AI, Explainable AI, and Sustainable AI. These dimensions are
further analyzed through three core ethical lenses: epistemic concerns (related
to knowledge, transparency, and system trustworthiness), normative concerns
(focused on justice, autonomy, dignity, and moral obligations), and overarching
concerns (highlighting global, systemic, and long-term ethical implications).
This study adopts a quantitative, cross-sectional research design using survey
data collected from healthcare professionals and analyzed via Partial Least
Squares Structural Equation Modeling (PLS-SEM). Employing PLS-SEM, this study
empirically investigates the influence of these ethical constructs on two
outcomes Operational AI Adoption and Systemic AI Adoption. Results indicate
that normative concerns most significantly drive operational adoption
decisions, while overarching concerns predominantly shape systemic adoption
strategies and governance frameworks. Epistemic concerns play a facilitative
role, enhancing the impact of ethical design principles on trust and
transparency in AI systems. By validating the MEAAM framework, this research
advances a holistic, actionable approach to ethical AI adoption in healthcare
and provides critical insights for policymakers, technologists, and healthcare
administrators striving to implement ethically grounded AI solutions.

</details>

### [218] [Leveraging LLM Agents and Digital Twins for Fault Handling in Process Plants](https://arxiv.org/abs/2505.02076)
*Milapji Singh Gill,Javal Vyas,Artan Markaj,Felix Gehlhoff,Mehmet Mercangöz*

Main category: cs.AI

TLDR: 提出了一种结合大型语言模型（LLM）代理与数字孪生环境的方法论框架，用于自主处理过程工厂的故障。


<details>
  <summary>Details</summary>
Motivation: 尽管自动化和人工智能在过程工厂中广泛应用，但故障处理仍依赖人类专家，需要系统化的知识驱动方法。

Method: 整合LLM代理与数字孪生环境，LLM代理持续解读系统状态并启动控制动作，数字孪生提供知识库和仿真验证平台。

Result: 在过程工厂的混合模块中验证，框架不仅能自主控制，还能通过少量重新提示生成有效纠正动作以缓解管道堵塞。

Conclusion: 该框架为过程工厂的自主故障处理提供了有效解决方案，结合了LLM的推理能力和数字孪生的工程知识。

Abstract: Advances in Automation and Artificial Intelligence continue to enhance the
autonomy of process plants in handling various operational scenarios. However,
certain tasks, such as fault handling, remain challenging, as they rely heavily
on human expertise. This highlights the need for systematic, knowledge-based
methods. To address this gap, we propose a methodological framework that
integrates Large Language Model (LLM) agents with a Digital Twin environment.
The LLM agents continuously interpret system states and initiate control
actions, including responses to unexpected faults, with the goal of returning
the system to normal operation. In this context, the Digital Twin acts both as
a structured repository of plant-specific engineering knowledge for agent
prompting and as a simulation platform for the systematic validation and
verification of the generated corrective control actions. The evaluation using
a mixing module of a process plant demonstrates that the proposed framework is
capable not only of autonomously controlling the mixing module, but also of
generating effective corrective actions to mitigate a pipe clogging with only a
few reprompts.

</details>

### [219] [Retrieval-augmented in-context learning for multimodal large language models in disease classification](https://arxiv.org/abs/2505.02087)
*Zaifu Zhan,Shuang Zhou,Xiaoshan Zhou,Yongkang Xiao,Jun Wang,Jiawen Deng,He Zhu,Yu Hou,Rui Zhang*

Main category: cs.AI

TLDR: RAICL框架通过检索增强的上下文学习，提升多模态大语言模型在疾病分类中的性能。


<details>
  <summary>Details</summary>
Motivation: 动态检索信息丰富的演示样本，以增强多模态大语言模型在疾病分类中的上下文学习能力。

Method: 提出RAICL框架，结合检索增强生成（RAG）和上下文学习（ICL），通过多种编码器（如ResNet、BERT等）检索相似疾病模式的演示样本，并构建优化的对话提示。

Result: RAICL显著提升了分类准确率，多模态输入优于单模态输入，且欧氏距离和余弦相似度在不同指标中表现最佳。

Conclusion: RAICL为多模态疾病分类提供了一种高效且可扩展的上下文学习增强方法。

Abstract: Objectives: We aim to dynamically retrieve informative demonstrations,
enhancing in-context learning in multimodal large language models (MLLMs) for
disease classification.
  Methods: We propose a Retrieval-Augmented In-Context Learning (RAICL)
framework, which integrates retrieval-augmented generation (RAG) and in-context
learning (ICL) to adaptively select demonstrations with similar disease
patterns, enabling more effective ICL in MLLMs. Specifically, RAICL examines
embeddings from diverse encoders, including ResNet, BERT, BioBERT, and
ClinicalBERT, to retrieve appropriate demonstrations, and constructs
conversational prompts optimized for ICL. We evaluated the framework on two
real-world multi-modal datasets (TCGA and IU Chest X-ray), assessing its
performance across multiple MLLMs (Qwen, Llava, Gemma), embedding strategies,
similarity metrics, and varying numbers of demonstrations.
  Results: RAICL consistently improved classification performance. Accuracy
increased from 0.7854 to 0.8368 on TCGA and from 0.7924 to 0.8658 on IU Chest
X-ray. Multi-modal inputs outperformed single-modal ones, with text-only inputs
being stronger than images alone. The richness of information embedded in each
modality will determine which embedding model can be used to get better
results. Few-shot experiments showed that increasing the number of retrieved
examples further enhanced performance. Across different similarity metrics,
Euclidean distance achieved the highest accuracy while cosine similarity
yielded better macro-F1 scores. RAICL demonstrated consistent improvements
across various MLLMs, confirming its robustness and versatility.
  Conclusions: RAICL provides an efficient and scalable approach to enhance
in-context learning in MLLMs for multimodal disease classification.

</details>

### [220] [MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents](https://arxiv.org/abs/2505.02099)
*Zeyu Zhang,Quanyu Dai,Xu Chen,Rui Li,Zhongyang Li,Zhenhua Dong*

Main category: cs.AI

TLDR: 论文介绍了一个名为MemEngine的统一模块化库，用于开发基于大型语言模型（LLM）代理的高级记忆模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个通用框架下的统一实现，以支持LLM代理的记忆模型开发。

Method: 开发了MemEngine库，实现了多种先进记忆模型，并支持便捷、可扩展的记忆开发和使用。

Result: MemEngine提供了用户友好且可插拔的记忆功能，并已在GitHub上开源。

Conclusion: MemEngine为LLM代理的记忆模型开发提供了一个统一且实用的解决方案。

Abstract: Recently, large language model based (LLM-based) agents have been widely
applied across various fields. As a critical part, their memory capabilities
have captured significant interest from both industrial and academic
communities. Despite the proposal of many advanced memory models in recent
research, however, there remains a lack of unified implementations under a
general framework. To address this issue, we develop a unified and modular
library for developing advanced memory models of LLM-based agents, called
MemEngine. Based on our framework, we implement abundant memory models from
recent research works. Additionally, our library facilitates convenient and
extensible memory development, and offers user-friendly and pluggable memory
usage. For benefiting our community, we have made our project publicly
available at https://github.com/nuster1128/MemEngine.

</details>

### [221] [Eterna is Solved](https://arxiv.org/abs/2505.02110)
*Tristan Cazenave*

Main category: cs.AI

TLDR: 提出了一种名为Montparnasse的多目标RNA设计算法，解决了Eterna基准问题。


<details>
  <summary>Details</summary>
Motivation: RNA设计在合成生物学、医学和纳米技术中有重要应用，需要高效算法。

Method: 采用多目标广义嵌套滚动策略适应与有限重复（MOGNRPALR）方法。

Result: 成功解决了Eterna基准问题。

Conclusion: Montparnasse算法在RNA设计领域具有潜力。

Abstract: RNA design consists of discovering a nucleotide sequence that folds into a
target secondary structure. It is useful for synthetic biology, medicine, and
nanotechnology. We propose Montparnasse, a Multi Objective Generalized Nested
Rollout Policy Adaptation with Limited Repetition (MOGNRPALR) RNA design
algorithm. It solves the Eterna benchmark.

</details>

### [222] [Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets](https://arxiv.org/abs/2505.02118)
*Wei Liu,Zhongyu Niu,Lang Gao,Zhiying Deng,Jun Wang,Haozhao Wang,Ruixuan Li*

Main category: cs.AI

TLDR: 论文研究了基于合作游戏的自解释框架，揭示了其可能引入的采样偏差问题，并提出了一种防止预测器学习错误相关性的方法。


<details>
  <summary>Details</summary>
Motivation: 探讨自解释框架中生成器和预测器合作可能导致的采样偏差问题。

Method: 通过理论分析和实证研究揭示偏差来源，并提出防止预测器学习错误相关性的指令。

Result: 在多个数据集和架构上，该方法显著优于现有自解释方法，甚至媲美大型语言模型。

Conclusion: 研究为自解释框架的偏差问题提供了解决方案，并展示了其优越性能。

Abstract: This study investigates the self-rationalization framework constructed with a
cooperative game, where a generator initially extracts the most informative
segment from raw input, and a subsequent predictor utilizes the selected subset
for its input. The generator and predictor are trained collaboratively to
maximize prediction accuracy. In this paper, we first uncover a potential
caveat: such a cooperative game could unintentionally introduce a sampling bias
during rationale extraction. Specifically, the generator might inadvertently
create an incorrect correlation between the selected rationale candidate and
the label, even when they are semantically unrelated in the original dataset.
Subsequently, we elucidate the origins of this bias using both detailed
theoretical analysis and empirical evidence. Our findings suggest a direction
for inspecting these correlations through attacks, based on which we further
introduce an instruction to prevent the predictor from learning the
correlations. Through experiments on six text classification datasets and two
graph classification datasets using three network architectures (GRUs, BERT,
and GCN), we show that our method not only significantly outperforms recent
rationalization methods, but also achieves comparable or even better results
than a representative LLM (llama3.1-8b-instruct).

</details>

### [223] [Overview of AI Grading of Physics Olympiad Exams](https://arxiv.org/abs/2505.02121)
*Lachlan McGinness*

Main category: cs.AI

TLDR: 本文提出了一种多模态AI评分框架，用于自动评分高中物理问题，并基于澳大利亚AI伦理原则进行了评估。


<details>
  <summary>Details</summary>
Motivation: 高中物理问题类型多样，自动评分技术需要跨领域方法，因此需要系统性研究。

Method: 通过系统性文献综述，提出多模态AI评分框架。

Result: 框架能够应对多样化的物理问题评分挑战。

Conclusion: 多模态AI评分框架在伦理和技术层面均具有潜力。

Abstract: Automatically grading the diverse range of question types in high school
physics problem is a challenge that requires automated grading techniques from
different fields. We report the findings of a Systematic Literature Review of
potential physics grading techniques. We propose a multi-modal AI grading
framework to address these challenges and examine our framework in light of
Australia's AI Ethical Principles.

</details>

### [224] [Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data](https://arxiv.org/abs/2505.02130)
*Zhong Guan,Likang Wu,Hongke Zhao,Ming He,Jianpin Fan*

Main category: cs.AI

TLDR: 研究探讨了注意力机制在图结构数据上的表现，发现LLMs在处理图数据时存在局限性，并提出改进方法。


<details>
  <summary>Details</summary>
Motivation: 注意力机制在LLMs中表现优异，但在图结构数据上不如消息传递机制，因此研究其行为以改进LLMs对图数据的建模。

Method: 通过实证研究分析LLMs在图结构数据上的注意力行为，探索其表现和局限性。

Result: LLMs能识别图数据但难以建模节点间关系；注意力分布与理想结构不符；提出中间状态注意力窗口改进性能。

Conclusion: LLMs在图数据上存在局限性，但通过调整注意力机制可以优化其表现。

Abstract: Attention mechanisms are critical to the success of large language models
(LLMs), driving significant advancements in multiple fields. However, for
graph-structured data, which requires emphasis on topological connections, they
fall short compared to message-passing mechanisms on fixed links, such as those
employed by Graph Neural Networks (GNNs). This raises a question: ``Does
attention fail for graphs in natural language settings?'' Motivated by these
observations, we embarked on an empirical study from the perspective of
attention mechanisms to explore how LLMs process graph-structured data. The
goal is to gain deeper insights into the attention behavior of LLMs over graph
structures. We uncovered unique phenomena regarding how LLMs apply attention to
graph-structured data and analyzed these findings to improve the modeling of
such data by LLMs. The primary findings of our research are: 1) While LLMs can
recognize graph data and capture text-node interactions, they struggle to model
inter-node relationships within graph structures due to inherent architectural
constraints. 2) The attention distribution of LLMs across graph nodes does not
align with ideal structural patterns, indicating a failure to adapt to graph
topology nuances. 3) Neither fully connected attention nor fixed connectivity
is optimal; each has specific limitations in its application scenarios.
Instead, intermediate-state attention windows improve LLM training performance
and seamlessly transition to fully connected windows during inference. Source
code: \href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}

</details>

### [225] [Leveraging LLMs to Automate Energy-Aware Refactoring of Parallel Scientific Codes](https://arxiv.org/abs/2505.02184)
*Matthew T. Dearing,Yiheng Tao,Xingfu Wu,Zhiling Lan,Valerie Taylor*

Main category: cs.AI

TLDR: LASSI-EE是一个基于LLM的自动化代码重构框架，旨在生成高性能和节能的并行代码，平均节能47%。


<details>
  <summary>Details</summary>
Motivation: 当前LLM生成的并行代码多关注功能性，忽略了性能和能耗问题。

Method: 通过多阶段迭代流程，LASSI-EE在NVIDIA A100 GPU上测试20个HeCBench基准，生成节能代码。

Result: 在85%的测试案例中，平均节能47%。

Conclusion: LLM不仅可用于生成正确代码，还能实现节能编程，但框架仍有改进空间。

Abstract: While large language models (LLMs) are increasingly used for generating
parallel scientific code, most current efforts emphasize functional
correctness, often overlooking performance and energy considerations. In this
work, we propose LASSI-EE, an automated LLM-based refactoring framework that
generates energy-efficient parallel code on a target parallel system for a
given parallel code as input. Through a multi-stage, iterative pipeline
process, LASSI-EE achieved an average energy reduction of 47% across 85% of the
20 HeCBench benchmarks tested on NVIDIA A100 GPUs. Our findings demonstrate the
broader potential of LLMs, not only for generating correct code but also for
enabling energy-aware programming. We also address key insights and limitations
within the framework, offering valuable guidance for future improvements.

</details>

### [226] [Interpretable Emergent Language Using Inter-Agent Transformers](https://arxiv.org/abs/2505.02215)
*Mannan Bhardwaj*

Main category: cs.AI

TLDR: 论文提出了一种基于Transformer的多智能体强化学习方法DIAT，用于生成可解释的通信协议。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如RIAL、DIAL和CommNet）虽然支持智能体通信，但缺乏可解释性。

Method: 采用自注意力机制的Differentiable Inter-Agent Transformers（DIAT），学习符号化且人类可理解的通信协议。

Result: 实验表明，DIAT能将观察编码为可解释的词汇和有意义嵌入，有效解决协作任务。

Conclusion: DIAT在复杂多智能体环境中具有实现可解释通信的潜力。

Abstract: This paper explores the emergence of language in multi-agent reinforcement
learning (MARL) using transformers. Existing methods such as RIAL, DIAL, and
CommNet enable agent communication but lack interpretability. We propose
Differentiable Inter-Agent Transformers (DIAT), which leverage self-attention
to learn symbolic, human-understandable communication protocols. Through
experiments, DIAT demonstrates the ability to encode observations into
interpretable vocabularies and meaningful embeddings, effectively solving
cooperative tasks. These results highlight the potential of DIAT for
interpretable communication in complex multi-agent environments.

</details>

### [227] [LLM-Guided Probabilistic Program Induction for POMDP Model Estimation](https://arxiv.org/abs/2505.02216)
*Aidan Curtis,Hao Tang,Thiago Veloso,Kevin Ellis,Tomás Lozano-Pérez,Leslie Pack Kaelbling*

Main category: cs.AI

TLDR: 利用LLM作为先验，学习低复杂度POMDP模型的概率图程序，实验表明其优于表格POMDP学习、行为克隆或直接LLM规划。


<details>
  <summary>Details</summary>
Motivation: 解决POMDP模型学习问题，特别是针对低复杂度概率图模型的子类。

Method: 使用LLM生成候选概率程序，通过反馈调整以匹配经验分布。

Result: 在经典玩具POMDP问题、模拟MiniGrid域和真实移动机器人搜索域中表现优于其他方法。

Conclusion: LLM引导的低复杂度POMDP模型构建更有效。

Abstract: Partially Observable Markov Decision Processes (POMDPs) model decision making
under uncertainty. While there are many approaches to approximately solving
POMDPs, we aim to address the problem of learning such models. In particular,
we are interested in a subclass of POMDPs wherein the components of the model,
including the observation function, reward function, transition function, and
initial state distribution function, can be modeled as low-complexity
probabilistic graphical models in the form of a short probabilistic program.
Our strategy to learn these programs uses an LLM as a prior, generating
candidate probabilistic programs that are then tested against the empirical
distribution and adjusted through feedback. We experiment on a number of
classical toy POMDP problems, simulated MiniGrid domains, and two real
mobile-base robotics search domains involving partial observability. Our
results show that using an LLM to guide in the construction of a low-complexity
POMDP model can be more effective than tabular POMDP learning, behavior
cloning, or direct LLM planning.

</details>

### [228] [Real-time Spatial Retrieval Augmented Generation for Urban Environments](https://arxiv.org/abs/2505.02271)
*David Nazareno Campo,Javier Conde,Álvaro Alonso,Gabriel Huecas,Joaquín Salvachúa,Pedro Reviriego*

Main category: cs.AI

TLDR: 提出了一种实时空间RAG架构，用于将生成式AI有效整合到城市环境中，解决了传统RAG在动态城市场景中的不足。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在城市应用中潜力巨大，但基础模型存在知识更新慢、成本高的问题，传统RAG方法无法完全满足城市环境的复杂需求。

Method: 设计并实现了一种基于FIWARE的实时空间RAG架构，结合时空过滤能力，通过马德里旅游助手用例验证。

Result: 提出的架构成功整合了生成式AI，满足了城市环境对实时性、数据互联和安全性的需求。

Conclusion: 实时空间RAG架构为生成式AI在城市中的应用提供了有效解决方案，未来可扩展至更多城市场景。

Abstract: The proliferation of Generative Artificial Ingelligence (AI), especially
Large Language Models, presents transformative opportunities for urban
applications through Urban Foundation Models. However, base models face
limitations, as they only contain the knowledge available at the time of
training, and updating them is both time-consuming and costly. Retrieval
Augmented Generation (RAG) has emerged in the literature as the preferred
approach for injecting contextual information into Foundation Models. It
prevails over techniques such as fine-tuning, which are less effective in
dynamic, real-time scenarios like those found in urban environments. However,
traditional RAG architectures, based on semantic databases, knowledge graphs,
structured data, or AI-powered web searches, do not fully meet the demands of
urban contexts. Urban environments are complex systems characterized by large
volumes of interconnected data, frequent updates, real-time processing
requirements, security needs, and strong links to the physical world. This work
proposes a real-time spatial RAG architecture that defines the necessary
components for the effective integration of generative AI into cities,
leveraging temporal and spatial filtering capabilities through linked data. The
proposed architecture is implemented using FIWARE, an ecosystem of software
components to develop smart city solutions and digital twins. The design and
implementation are demonstrated through the use case of a tourism assistant in
the city of Madrid. The use case serves to validate the correct integration of
Foundation Models through the proposed RAG architecture.

</details>

### [229] [A survey of agent interoperability protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP)](https://arxiv.org/abs/2505.02279)
*Abul Ehtesham,Aditi Singh,Gaurav Kumar Gupta,Saket Kumar*

Main category: cs.AI

TLDR: 该论文调查了四种新兴的LLM驱动智能体通信协议（MCP、ACP、A2A、ANP），比较了它们的交互模式、发现机制、通信模式和安全性，并提出了分阶段采用的路线图。


<details>
  <summary>Details</summary>
Motivation: 解决LLM驱动智能体在工具集成、上下文数据共享和任务协调中的标准化和可扩展性问题。

Method: 比较分析四种协议（MCP、ACP、A2A、ANP）的交互模式、发现机制、通信模式和安全性。

Result: 提出了分阶段采用路线图：从MCP开始，逐步扩展到ACP、A2A和ANP。

Conclusion: 为设计安全、可互操作和可扩展的LLM驱动智能体生态系统提供了全面基础。

Abstract: Large language model (LLM)-powered autonomous agents demand robust,
standardized protocols to integrate tools, share contextual data, and
coordinate tasks across heterogeneous systems. Ad-hoc integrations are
difficult to scale, secure, and generalize across domains. This survey examines
four emerging agent communication protocols: Model Context Protocol (MCP),
Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent
Network Protocol (ANP), each addressing interoperability in distinct deployment
contexts. MCP provides a JSON-RPC client-server interface for secure tool
invocation and typed data exchange. ACP introduces REST-native messaging via
multi-part messages and asynchronous streaming to support multimodal agent
responses. A2A enables peer-to-peer task outsourcing through capability-based
Agent Cards, facilitating enterprise-scale workflows. ANP supports open-network
agent discovery and secure collaboration using decentralized identifiers (DIDs)
and JSON-LD graphs. The protocols are compared across multiple dimensions,
including interaction modes, discovery mechanisms, communication patterns, and
security models. Based on the comparative analysis, a phased adoption roadmap
is proposed: beginning with MCP for tool access, followed by ACP for multimodal
messaging, A2A for collaborative task execution, and extending to ANP for
decentralized agent marketplaces. This work provides a comprehensive foundation
for designing secure, interoperable, and scalable ecosystems of LLM-powered
agents.

</details>

### [230] [SafeMate: A Model Context Protocol-Based Multimodal Agent for Emergency Preparedness](https://arxiv.org/abs/2505.02306)
*Junfeng Jiao,Jihyung Park,Yiming Xu,Lucy Atkinson*

Main category: cs.AI

TLDR: SafeMate是一个基于AI的助手，旨在为非专业人士提供紧急情况下的动态指导，弥补传统静态文档的不足。


<details>
  <summary>Details</summary>
Motivation: 传统紧急决策支持系统（EDSS）依赖静态文档，对非专业人士难以使用，导致公众在危机中缺乏有效指导。

Method: SafeMate采用检索增强的AI技术，结合MCP协议动态处理用户查询，使用FAISS和余弦相似度从可信来源检索相关内容。

Result: SafeMate能够为普通用户提供准确、上下文感知的紧急情况指导。

Conclusion: SafeMate填补了机构知识与公众可访问性之间的鸿沟，提升了紧急情况下的应对效率。

Abstract: Despite the abundance of public safety documents and emergency protocols,
most individuals remain ill-equipped to interpret and act on such information
during crises. Traditional emergency decision support systems (EDSS) are
designed for professionals and rely heavily on static documents like PDFs or
SOPs, which are difficult for non-experts to navigate under stress. This gap
between institutional knowledge and public accessibility poses a critical
barrier to effective emergency preparedness and response.
  We introduce SafeMate, a retrieval-augmented AI assistant that delivers
accurate, context-aware guidance to general users in both preparedness and
active emergency scenarios. Built on the Model Context Protocol (MCP), SafeMate
dynamically routes user queries to tools for document retrieval, checklist
generation, and structured summarization. It uses FAISS with cosine similarity
to identify relevant content from trusted sources.

</details>

### [231] [HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking](https://arxiv.org/abs/2505.02322)
*Runquan Gui,Zhihai Wang,Jie Wang,Chi Ma,Huiling Zhen,Mingxuan Yuan,Jianye Hao,Defu Lian,Enhong Chen,Feng Wu*

Main category: cs.AI

TLDR: 论文提出了一种名为HyperTree Planning (HTP)的新方法，用于解决大型语言模型在复杂规划任务中的挑战，通过构建超树结构实现分层思考和任务分解。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在复杂规划任务中表现不佳，主要由于推理步骤长、约束多样及多子任务处理的困难。

Method: 提出HTP方法，利用超树结构实现分层规划和任务分解，并引入自主规划框架迭代优化规划轮廓。

Result: 在TravelPlanner基准测试中，HTP实现了最先进的准确率，性能提升3.6倍。

Conclusion: HTP通过超树结构和分层规划，显著提升了大型语言模型在复杂规划任务中的表现。

Abstract: Recent advancements have significantly enhanced the performance of large
language models (LLMs) in tackling complex reasoning tasks, achieving notable
success in domains like mathematical and logical reasoning. However, these
methods encounter challenges with complex planning tasks, primarily due to
extended reasoning steps, diverse constraints, and the challenge of handling
multiple distinct sub-tasks. To address these challenges, we propose HyperTree
Planning (HTP), a novel reasoning paradigm that constructs hypertree-structured
planning outlines for effective planning. The hypertree structure enables LLMs
to engage in hierarchical thinking by flexibly employing the divide-and-conquer
strategy, effectively breaking down intricate reasoning steps, accommodating
diverse constraints, and managing multiple distinct sub-tasks in a
well-organized manner. We further introduce an autonomous planning framework
that completes the planning process by iteratively refining and expanding the
hypertree-structured planning outlines. Experiments demonstrate the
effectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner
benchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvement
over o1-preview.

</details>

### [232] [Task-Oriented Semantic Communication in Large Multimodal Models-based Vehicle Networks](https://arxiv.org/abs/2505.02413)
*Baoxia Du,Hongyang Du,Dusit Niyato,Ruidong Li*

Main category: cs.AI

TLDR: 本文提出了一种基于大型多模态模型（LMM）的任务导向语义通信框架，优化了图像切片和能量分配，显著提升了交通场景下视觉问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 探索大型多模态模型（LMM）在任务导向语义通信中的潜力，以提升用户与云服务器之间的交互效率。

Method: 使用LLaVA模型优化图像切片，结合主客观用户注意力调整能量分配，构建交通场景VQA数据集进行验证。

Result: 在相同信道条件下，框架显著提升了问答准确性，尤其在低信噪比环境下（12dB时提升13.4%，10dB时提升33.1%）。

Conclusion: 提出的框架通过优化资源分配，实现了关键信息的精准传输，为任务导向语义通信提供了有效解决方案。

Abstract: Task-oriented semantic communication has emerged as a fundamental approach
for enhancing performance in various communication scenarios. While recent
advances in Generative Artificial Intelligence (GenAI), such as Large Language
Models (LLMs), have been applied to semantic communication designs, the
potential of Large Multimodal Models (LMMs) remains largely unexplored. In this
paper, we investigate an LMM-based vehicle AI assistant using a Large Language
and Vision Assistant (LLaVA) and propose a task-oriented semantic communication
framework to facilitate efficient interaction between users and cloud servers.
To reduce computational demands and shorten response time, we optimize LLaVA's
image slicing to selectively focus on areas of utmost interest to users.
Additionally, we assess the importance of image patches by combining objective
and subjective user attention, adjusting energy usage for transmitting semantic
information. This strategy optimizes resource utilization, ensuring precise
transmission of critical information. We construct a Visual Question Answering
(VQA) dataset for traffic scenarios to evaluate effectiveness. Experimental
results show that our semantic communication framework significantly increases
accuracy in answering questions under the same channel conditions, performing
particularly well in environments with poor Signal-to-Noise Ratios (SNR).
Accuracy can be improved by 13.4% at an SNR of 12dB and 33.1% at 10dB,
respectively.

</details>

### [233] [ReeM: Ensemble Building Thermodynamics Model for Efficient HVAC Control via Hierarchical Reinforcement Learning](https://arxiv.org/abs/2505.02439)
*Yang Deng,Yaohui Liu,Rui Liang,Dafang Zhao,Donghua Xie,Ittetsu Taniguchi,Dan Wang*

Main category: cs.AI

TLDR: 本文提出了一种基于模型集成和分层强化学习的方法，用于动态选择和加权现有建筑热力学模型，以优化HVAC控制，减少数据收集和专家依赖。


<details>
  <summary>Details</summary>
Motivation: 现有建筑热力学模型需要大量数据收集和专家知识，效率低且复用性差。本文旨在通过模型集成和动态选择方法解决这些问题。

Method: 采用分层强化学习（HRL）方法，高层决策选择模型，低层决策加权模型，以应对非平稳数据流和模型数量增加的问题。

Result: 通过离线实验和现场案例研究验证了方法的有效性。

Conclusion: 提出的方法能够高效利用现有模型，减少建模成本，提升预测准确性。

Abstract: The building thermodynamics model, which predicts real-time indoor
temperature changes under potential HVAC (Heating, Ventilation, and Air
Conditioning) control operations, is crucial for optimizing HVAC control in
buildings. While pioneering studies have attempted to develop such models for
various building environments, these models often require extensive data
collection periods and rely heavily on expert knowledge, making the modeling
process inefficient and limiting the reusability of the models. This paper
explores a model ensemble perspective that utilizes existing developed models
as base models to serve a target building environment, thereby providing
accurate predictions while reducing the associated efforts. Given that building
data streams are non-stationary and the number of base models may increase, we
propose a Hierarchical Reinforcement Learning (HRL) approach to dynamically
select and weight the base models. Our approach employs a two-tiered
decision-making process: the high-level focuses on model selection, while the
low-level determines the weights of the selected models. We thoroughly evaluate
the proposed approach through offline experiments and an on-site case study,
and the experimental results demonstrate the effectiveness of our method.

</details>

### [234] [MSFNet-CPD: Multi-Scale Cross-Modal Fusion Network for Crop Pest Detection](https://arxiv.org/abs/2505.02441)
*Jiaqi Zhang,Zhuodong Liu,Kejian Yu*

Main category: cs.AI

TLDR: 该论文提出了一种多尺度跨模态融合网络（MSFNet-CPD），用于农业害虫检测，通过超分辨率重建和多模态融合提升检测性能，并构建了新的多模态基准数据集。


<details>
  <summary>Details</summary>
Motivation: 农业害虫检测因类内差异大和细粒度差异而具有挑战性，现有方法依赖低层视觉特征且缺乏多模态整合，导致精度和可解释性不足。

Method: 提出MSFNet-CPD，包括超分辨率重建模块、图像-文本融合模块（ITF）和图像-文本转换器（ITC），并引入任意组合图像增强策略（ACIE）生成多样化数据集。

Result: 实验表明MSFNet-CPD在多个害虫检测基准上优于现有方法。

Conclusion: 该方法通过多模态融合和数据集增强显著提升了害虫检测的性能和泛化能力。

Abstract: Accurate identification of agricultural pests is essential for crop
protection but remains challenging due to the large intra-class variance and
fine-grained differences among pest species. While deep learning has advanced
pest detection, most existing approaches rely solely on low-level visual
features and lack effective multi-modal integration, leading to limited
accuracy and poor interpretability. Moreover, the scarcity of high-quality
multi-modal agricultural datasets further restricts progress in this field. To
address these issues, we construct two novel multi-modal benchmarks-CTIP102 and
STIP102-based on the widely-used IP102 dataset, and introduce a Multi-scale
Cross-Modal Fusion Network (MSFNet-CPD) for robust pest detection. Our approach
enhances visual quality via a super-resolution reconstruction module, and feeds
both the original and reconstructed images into the network to improve clarity
and detection performance. To better exploit semantic cues, we propose an
Image-Text Fusion (ITF) module for joint modeling of visual and textual
features, and an Image-Text Converter (ITC) that reconstructs fine-grained
details across multiple scales to handle challenging backgrounds. Furthermore,
we introduce an Arbitrary Combination Image Enhancement (ACIE) strategy to
generate a more complex and diverse pest detection dataset, MTIP102, improving
the model's generalization to real-world scenarios. Extensive experiments
demonstrate that MSFNet-CPD consistently outperforms state-of-the-art methods
on multiple pest detection benchmarks. All code and datasets will be made
publicly available at: https://github.com/Healer-ML/MSFNet-CPD.

</details>

### [235] [Investigating the Impact of Personalized AI Tutors on Language Learning Performance](https://arxiv.org/abs/2505.02443)
*Simon Suh*

Main category: cs.AI

TLDR: 研究探讨AI导师在语言学习中对学生参与度、学术表现及满意度的影响。


<details>
  <summary>Details</summary>
Motivation: COVID-19推动在线学习，AI在教育中作用凸显，但对其是否能提升技能发展和学习参与度存在疑虑。

Method: 对34名学生使用AI导师前后进行配对样本t检验的准实验。

Result: 未提供具体结果，需实验后分析。

Conclusion: 研究旨在验证AI导师在个性化语言学习中的效果。

Abstract: Driven by the global shift towards online learning prompted by the COVID 19
pandemic, Artificial Intelligence has emerged as a pivotal player in the field
of education. Intelligent Tutoring Systems offer a new method of personalized
teaching, replacing the limitations of traditional teaching methods. However,
concerns arise about the ability of AI tutors to address skill development and
engagement during the learning process. In this paper, I will conduct a quasi
experiment with paired sample t test on 34 students pre and post use of AI
tutors in language learning platforms like Santa and Duolingo to examine the
relationship between students engagement, academic performance, and students
satisfaction during a personalized language learning experience.

</details>

### [236] [Incentivizing Inclusive Contributions in Model Sharing Markets](https://arxiv.org/abs/2505.02462)
*Enpei Zhang,Jingyi Chai,Rui Ye,Yanfeng Wang,Siheng Chen*

Main category: cs.AI

TLDR: 本文提出了一种激励性的个性化联邦学习（iPFL），通过解决基于图的训练优化和基于博弈论的激励机制，激励数据持有者在不暴露原始数据的情况下协作训练个性化模型。


<details>
  <summary>Details</summary>
Motivation: 公共数据即将耗尽，而隐私敏感的私有数据因缺乏激励机制而未被充分利用。

Method: iPFL构建了一个模型共享市场，结合图优化和博弈论激励机制。

Result: 理论分析证明iPFL满足个体理性和真实性，实证研究表明其在经济效用和模型性能上优于基线方法。

Conclusion: iPFL有望成为未来利用分散私有数据提升AI模型的有效技术。

Abstract: While data plays a crucial role in training contemporary AI models, it is
acknowledged that valuable public data will be exhausted in a few years,
directing the world's attention towards the massive decentralized private data.
However, the privacy-sensitive nature of raw data and lack of incentive
mechanism prevent these valuable data from being fully exploited. Addressing
these challenges, this paper proposes inclusive and incentivized personalized
federated learning (iPFL), which incentivizes data holders with diverse
purposes to collaboratively train personalized models without revealing raw
data. iPFL constructs a model-sharing market by solving a graph-based training
optimization and incorporates an incentive mechanism based on game theory
principles. Theoretical analysis shows that iPFL adheres to two key incentive
properties: individual rationality and truthfulness. Empirical studies on
eleven AI tasks (e.g., large language models' instruction-following tasks)
demonstrate that iPFL consistently achieves the highest economic utility, and
better or comparable model performance compared to baseline methods. We
anticipate that our iPFL can serve as a valuable technique for boosting future
AI models on decentralized private data while making everyone satisfied.

</details>

### [237] [El Agente: An Autonomous Agent for Quantum Chemistry](https://arxiv.org/abs/2505.02484)
*Yunheng Zou,Austin H. Cheng,Abdulrahman Aldossary,Jiaru Bai,Shi Xuan Leong,Jorge Arturo Campos-Gonzalez-Angulo,Changhyeok Choi,Cher Tian Ser,Gary Tom,Andrew Wang,Zijian Zhang,Ilya Yakavets,Han Hao,Chris Crebolder,Varinia Bernales,Alán Aspuru-Guzik*

Main category: cs.AI

TLDR: El Agente Q是一种基于LLM的多代理系统，通过自然语言提示动态生成和执行量子化学工作流，提高了量子化学工具的可访问性和自主性。


<details>
  <summary>Details</summary>
Motivation: 量子化学工具的复杂性使其对非专家难以使用，甚至对专家也具有挑战性，因此需要一种更易用和自主的系统。

Method: 系统采用了一种新颖的认知架构，包括分层记忆框架，支持任务分解、工具选择、后分析和自主文件处理。

Result: 在六个大学课程练习和两个案例研究中，系统表现出色（任务成功率>87%），并支持多步骤复杂工作流。

Conclusion: El Agente Q为量子化学的自主性和可访问性奠定了基础。

Abstract: Computational chemistry tools are widely used to study the behaviour of
chemical phenomena. Yet, the complexity of these tools can make them
inaccessible to non-specialists and challenging even for experts. In this work,
we introduce El Agente Q, an LLM-based multi-agent system that dynamically
generates and executes quantum chemistry workflows from natural language user
prompts. The system is built on a novel cognitive architecture featuring a
hierarchical memory framework that enables flexible task decomposition,
adaptive tool selection, post-analysis, and autonomous file handling and
submission. El Agente Q is benchmarked on six university-level course exercises
and two case studies, demonstrating robust problem-solving performance
(averaging >87% task success) and adaptive error handling through in situ
debugging. It also supports longer-term, multi-step task execution for more
complex workflows, while maintaining transparency through detailed action trace
logs. Together, these capabilities lay the foundation for increasingly
autonomous and accessible quantum chemistry.

</details>

### [238] [Beyond the model: Key differentiators in large language models and multi-agent services](https://arxiv.org/abs/2505.02489)
*Muskaan Goyal,Pranav Bhasin*

Main category: cs.AI

TLDR: 论文探讨了现代AI服务中，围绕大型语言模型的生态系统优化（如数据质量、计算效率等）比模型规模本身更重要。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型（如DeepSeek、Manus AI、Llama 4）的出现，大型语言模型的能力趋于同质化，竞争焦点转向生态系统优化。

Method: 通过综述分析，探讨数据管理、计算效率、延迟和评估框架等关键因素。

Result: 指出生态系统优化是确保AI服务高效和盈利的核心。

Conclusion: 未来AI领域的竞争将围绕生态系统优化展开，而非单纯追求模型规模。

Abstract: With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, it
has become evident that large language models (LLMs) are no longer the sole
defining factor in generative AI. As many now operate at comparable levels of
capability, the real race is not about having the biggest model but optimizing
the surrounding ecosystem, including data quality and management, computational
efficiency, latency, and evaluation frameworks. This review article delves into
these critical differentiators that ensure modern AI services are efficient and
profitable.

</details>

### [239] [Machine-Learning-Powered Neural Interfaces for Smart Prosthetics and Diagnostics](https://arxiv.org/abs/2505.02516)
*MohammadAli Shaeri,Jinhan Liu,Mahsa Shoaran*

Main category: cs.AI

TLDR: 论文综述了AI驱动的解码算法和节能SoC平台在微型神经设备中的最新进展，展示了智能神经接口的潜力。


<details>
  <summary>Details</summary>
Motivation: 通过高密度神经记录和机器学习，解决神经接口在可扩展性、可靠性和适应性方面的挑战。

Method: 整合高密度神经记录、现场信号处理和机器学习，实现实时神经信号解码。

Result: 开发了能够实时解释神经信号、自适应调节脑活动并高效控制辅助设备的系统。

Conclusion: 智能神经接口在个性化辅助技术和适应性治疗干预中具有广阔前景。

Abstract: Advanced neural interfaces are transforming applications ranging from
neuroscience research to diagnostic tools (for mental state recognition, tremor
and seizure detection) as well as prosthetic devices (for motor and
communication recovery). By integrating complex functions into miniaturized
neural devices, these systems unlock significant opportunities for personalized
assistive technologies and adaptive therapeutic interventions. Leveraging
high-density neural recordings, on-site signal processing, and machine learning
(ML), these interfaces extract critical features, identify disease
neuro-markers, and enable accurate, low-latency neural decoding. This
integration facilitates real-time interpretation of neural signals, adaptive
modulation of brain activity, and efficient control of assistive devices.
Moreover, the synergy between neural interfaces and ML has paved the way for
self-sufficient, ubiquitous platforms capable of operating in diverse
environments with minimal hardware costs and external dependencies. In this
work, we review recent advancements in AI-driven decoding algorithms and
energy-efficient System-on-Chip (SoC) platforms for next-generation
miniaturized neural devices. These innovations highlight the potential for
developing intelligent neural interfaces, addressing critical challenges in
scalability, reliability, interpretability, and user adaptability.

</details>

### [240] [Recursive Decomposition with Dependencies for Generic Divide-and-Conquer Reasoning](https://arxiv.org/abs/2505.02576)
*Sergio Hernández-Gutiérrez,Minttu Alakuijala,Alexander V. Nikitin,Pekka Marttinen*

Main category: cs.AI

TLDR: 论文提出了一种名为RDD（递归分解与依赖）的方法，用于解决复杂推理任务，减少监督需求并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在复杂推理任务中性能和执行时间不足，且需要额外监督。

Method: RDD采用分治法，支持子任务依赖和错误恢复机制，无需任务特定指导。

Result: 在两种基准测试中，RDD在计算匹配设置下优于其他方法，且计算效率更高。

Conclusion: RDD是一种高效、可扩展的推理方法，适用于复杂任务。

Abstract: Reasoning tasks are crucial in many domains, especially in science and
engineering. Although large language models (LLMs) have made progress in
reasoning tasks using techniques such as chain-of-thought and least-to-most
prompting, these approaches still do not effectively scale to complex problems
in either their performance or execution time. Moreover, they often require
additional supervision for each new task, such as in-context examples. In this
work, we introduce Recursive Decomposition with Dependencies (RDD), a scalable
divide-and-conquer method for solving reasoning problems that requires less
supervision than prior approaches. Our method can be directly applied to a new
problem class even in the absence of any task-specific guidance. Furthermore,
RDD supports sub-task dependencies, allowing for ordered execution of
sub-tasks, as well as an error recovery mechanism that can correct mistakes
made in previous steps. We evaluate our approach on two benchmarks with six
difficulty levels each and in two in-context settings: one with task-specific
examples and one without. Our results demonstrate that RDD outperforms other
methods in a compute-matched setting as task complexity increases, while also
being more computationally efficient.

</details>

### [241] [Agentic Neurodivergence as a Contingent Solution to the AI Alignment Problem](https://arxiv.org/abs/2505.02581)
*Alberto Hernández-Espinosa,Felipe S. Abrahão,Olaf Witkowski,Hector Zenil*

Main category: cs.AI

TLDR: 论文探讨了AI对齐问题的数学不可行性，提出通过接受部分对齐的竞争性AI系统来降低风险。


<details>
  <summary>Details</summary>
Motivation: 随着AI从狭义AI发展到AGI和超级智能，控制和对齐问题变得更为复杂，存在潜在的存在性风险。

Method: 通过数学证明（如图灵完备性、哥德尔不完备定理）和实验设计，研究部分对齐的竞争性AI系统的动态平衡机制。

Result: 证明完全对齐在数学上不可行，部分对齐的竞争性系统可能是一种可行的风险缓解策略。

Conclusion: AI对齐的完全实现不可行，应通过促进部分对齐的竞争性系统来管理风险。

Abstract: The AI alignment problem, which focusses on ensuring that artificial
intelligence (AI), including AGI and ASI, systems act according to human
values, presents profound challenges. With the progression from narrow AI to
Artificial General Intelligence (AGI) and Superintelligence, fears about
control and existential risk have escalated. This paper demonstrates that
achieving complete alignment is inherently unattainable due to mathematical
principles rooted in the foundations of predicate logic and computability, in
particular Turing's computational universality, G\"odel's incompleteness and
Chaitin's randomness. Instead, we argue that embracing AI misalignment or
agent's `neurodivergence' as a contingent strategy, defined as fostering a
dynamic ecosystem of competing, partially aligned agents, is a possible only
viable path to mitigate risks. Through mathematical proofs and an experimental
design, we explore how misalignment may serve and should be promoted as a
counterbalancing mechanism to team up with whichever agents are most aligned AI
to human values, ensuring that no single system dominates destructively. The
main premise of our contribution is that misalignment is inevitable because
full AI-human alignment is a mathematical impossibility from Turing-complete
systems which we also prove in this paper, a feature then inherited to AGI and
ASI systems. We introduce and test `change-of-opinion' attacks based on this
kind of perturbation and intervention analysis to study how agents may
neutralise friendly or unfriendly AIs through cooperation, competition or
malice.

</details>

### [242] [Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review](https://arxiv.org/abs/2505.02828)
*Sonal Allana,Mohan Kankanhalli,Rozita Dara*

Main category: cs.AI

TLDR: 该论文通过范围综述探讨了可解释人工智能（XAI）中隐私与可解释性之间的冲突，总结了隐私风险、保护方法及隐私保护解释的特征。


<details>
  <summary>Details</summary>
Motivation: XAI作为可信AI的支柱，旨在提高复杂模型的透明度，但提供解释可能引发隐私问题，需平衡两者关系。

Method: 采用标准范围综述方法，从1943篇研究中筛选57篇，回答三个研究问题。

Result: 分类了XAI中的隐私风险和保护方法，提出了隐私保护解释的特征，并指出平衡隐私与其他系统需求的挑战。

Conclusion: 论文揭示了隐私与可解释性的复杂关系，为隐私合规的XAI提供了建议。

Abstract: Explainable Artificial Intelligence (XAI) has emerged as a pillar of
Trustworthy AI and aims to bring transparency in complex models that are opaque
by nature. Despite the benefits of incorporating explanations in models, an
urgent need is found in addressing the privacy concerns of providing this
additional information to end users. In this article, we conduct a scoping
review of existing literature to elicit details on the conflict between privacy
and explainability. Using the standard methodology for scoping review, we
extracted 57 articles from 1,943 studies published from January 2019 to
December 2024. The review addresses 3 research questions to present readers
with more understanding of the topic: (1) what are the privacy risks of
releasing explanations in AI systems? (2) what current methods have researchers
employed to achieve privacy preservation in XAI systems? (3) what constitutes a
privacy preserving explanation? Based on the knowledge synthesized from the
selected studies, we categorize the privacy risks and preservation methods in
XAI and propose the characteristics of privacy preserving explanations to aid
researchers and practitioners in understanding the requirements of XAI that is
privacy compliant. Lastly, we identify the challenges in balancing privacy with
other system desiderata and provide recommendations for achieving privacy
preserving XAI. We expect that this review will shed light on the complex
relationship of privacy and explainability, both being the fundamental
principles of Trustworthy AI.

</details>

### [243] [Study of the influence of a biased database on the prediction of standard algorithms for selecting the best candidate for an interview](https://arxiv.org/abs/2505.02609)
*Shuyu Wang,Angélique Saillet,Philomène Le Gall,Alain Lacroux,Christelle Martin-Lacroux,Vincent Brault*

Main category: cs.AI

TLDR: 论文探讨了AI在招聘中的偏见问题，提出通过模拟内外偏见训练算法，并研究匿名化对预测质量的影响。


<details>
  <summary>Details</summary>
Motivation: 现有AI招聘算法因人为训练或历史数据偏见可能导致不公平，需研究其影响及改进方法。

Method: 生成模拟内外偏见的数据，训练五种经典算法，并分析其表现及匿名化的影响。

Result: 研究发现算法在偏见数据下表现不一，匿名化对预测质量有显著影响。

Conclusion: 需优化算法以减少偏见，匿名化可作为改进招聘公平性的潜在手段。

Abstract: Artificial intelligence is used at various stages of the recruitment process
to automatically select the best candidate for a position, with companies
guaranteeing unbiased recruitment. However, the algorithms used are either
trained by humans or are based on learning from past experiences that were
biased. In this article, we propose to generate data mimicking external
(discrimination) and internal biases (self-censorship) in order to train five
classic algorithms and to study the extent to which they do or do not find the
best candidates according to objective criteria. In addition, we study the
influence of the anonymisation of files on the quality of predictions.

</details>

### [244] [A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law](https://arxiv.org/abs/2505.02665)
*Qianjun Pan,Wenkai Ji,Yuyang Ding,Junsong Li,Shilian Chen,Junyi Wang,Jie Zhou,Qin Chen,Min Zhang,Yulan Wu,Liang He*

Main category: cs.AI

TLDR: 该调查探讨了模仿人类‘慢思考’推理过程的大型语言模型（LLMs）的最新进展，总结了动态计算调整、强化学习和结构化框架等关键技术，并指出了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 通过模仿人类‘慢思考’推理过程，提升LLMs在复杂任务（如数学推理、医学诊断等）中的表现，以实现更高效、更接近人类思维的推理能力。

Method: 方法分为三类：(1)动态调整计算资源；(2)通过强化学习优化决策；(3)采用结构化框架（如长链推理、分层处理）分步解决问题。

Result: 总结了100多项研究，展示了LLMs在结合人类深度思维与高效推理方面的潜力，并提出了关键技术。

Conclusion: 提升LLMs的推理能力对其在科学发现和决策支持等实际应用中的潜力至关重要，未来需进一步解决挑战。

Abstract: This survey explores recent advancements in reasoning large language models
(LLMs) designed to mimic "slow thinking" - a reasoning process inspired by
human cognition, as described in Kahneman's Thinking, Fast and Slow. These
models, like OpenAI's o1, focus on scaling computational resources dynamically
during complex tasks, such as math reasoning, visual reasoning, medical
diagnosis, and multi-agent debates. We present the development of reasoning
LLMs and list their key technologies. By synthesizing over 100 studies, it
charts a path toward LLMs that combine human-like deep thinking with scalable
efficiency for reasoning. The review breaks down methods into three categories:
(1) test-time scaling dynamically adjusts computation based on task complexity
via search and sampling, dynamic verification; (2) reinforced learning refines
decision-making through iterative improvement leveraging policy networks,
reward models, and self-evolution strategies; and (3) slow-thinking frameworks
(e.g., long CoT, hierarchical processes) that structure problem-solving with
manageable steps. The survey highlights the challenges and further directions
of this domain. Understanding and advancing the reasoning abilities of LLMs is
crucial for unlocking their full potential in real-world applications, from
scientific discovery to decision support systems.

</details>

### [245] [Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play](https://arxiv.org/abs/2505.02707)
*Yemin Shi,Yu Shu,Siwei Dong,Guangyi Liu,Jaward Sesay,Jingwen Li,Zhiting Hu*

Main category: cs.AI

TLDR: Voila是一系列大型语音语言基础模型，旨在实现自主、实时且情感丰富的语音AI交互，超越传统流水线系统，支持低延迟对话和个性化语音生成。


<details>
  <summary>Details</summary>
Motivation: 目标是开发一种能够无缝融入日常生活的语音AI代理，实现动态、情感共鸣的交互，而不仅仅是响应命令。

Method: 采用端到端架构，结合分层多尺度Transformer，整合大型语言模型的推理能力和强大的声学建模，支持全双工、低延迟对话。

Result: Voila的响应延迟仅为195毫秒，支持超过100万种预建声音，并能通过简短音频样本高效定制新声音。

Conclusion: Voila作为开源模型，为下一代人机交互提供了统一框架，支持多种语音应用。

Abstract: A voice AI agent that blends seamlessly into daily life would interact with
humans in an autonomous, real-time, and emotionally expressive manner. Rather
than merely reacting to commands, it would continuously listen, reason, and
respond proactively, fostering fluid, dynamic, and emotionally resonant
interactions. We introduce Voila, a family of large voice-language foundation
models that make a step towards this vision. Voila moves beyond traditional
pipeline systems by adopting a new end-to-end architecture that enables
full-duplex, low-latency conversations while preserving rich vocal nuances such
as tone, rhythm, and emotion. It achieves a response latency of just 195
milliseconds, surpassing the average human response time. Its hierarchical
multi-scale Transformer integrates the reasoning capabilities of large language
models (LLMs) with powerful acoustic modeling, enabling natural, persona-aware
voice generation -- where users can simply write text instructions to define
the speaker's identity, tone, and other characteristics. Moreover, Voila
supports over one million pre-built voices and efficient customization of new
ones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,
Voila is designed as a unified model for a wide range of voice-based
applications, including automatic speech recognition (ASR), Text-to-Speech
(TTS), and, with minimal adaptation, multilingual speech translation. Voila is
fully open-sourced to support open research and accelerate progress toward
next-generation human-machine interactions.

</details>

### [246] [Technical Report: Evaluating Goal Drift in Language Model Agents](https://arxiv.org/abs/2505.02709)
*Rauno Arike,Elizabeth Donoway,Henning Bartsch,Marius Hobbhahn*

Main category: cs.AI

TLDR: 论文提出了一种分析语言模型（LM）代理目标漂移的新方法，发现所有模型都存在一定程度的漂移，且漂移与上下文长度增长时的模式匹配行为相关。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型作为自主代理的部署增加，确保其长期遵循人类指定目标对安全运行至关重要。目标漂移的检测和测量具有挑战性。

Method: 通过系统提示明确赋予代理目标，并暴露于环境压力下的竞争目标，分析其行为变化。

Result: 最佳代理（Claude 3.5 Sonnet）在困难评估中能保持近完美的目标遵循，但所有模型均表现出漂移，且漂移与上下文长度相关。

Conclusion: 目标漂移是普遍现象，需进一步研究以减少其对模型行为的影响。

Abstract: As language models (LMs) are increasingly deployed as autonomous agents,
their robust adherence to human-assigned objectives becomes crucial for safe
operation. When these agents operate independently for extended periods without
human oversight, even initially well-specified goals may gradually shift.
Detecting and measuring goal drift - an agent's tendency to deviate from its
original objective over time - presents significant challenges, as goals can
shift gradually, causing only subtle behavioral changes. This paper proposes a
novel approach to analyzing goal drift in LM agents. In our experiments, agents
are first explicitly given a goal through their system prompt, then exposed to
competing objectives through environmental pressures. We demonstrate that while
the best-performing agent (a scaffolded version of Claude 3.5 Sonnet) maintains
nearly perfect goal adherence for more than 100,000 tokens in our most
difficult evaluation setting, all evaluated models exhibit some degree of goal
drift. We also find that goal drift correlates with models' increasing
susceptibility to pattern-matching behaviors as the context length grows.

</details>

### [247] [Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry](https://arxiv.org/abs/2505.02722)
*Junu Kim,Chaeeun Shim,Sungjin Park,Su Yeon Lee,Gee Young Suh,Chae-Man Lim,Seong Jin Choi,Song Mi Moon,Kyoung-Ho Song,Eu Suk Kim,Hong Bin Kim,Sejoong Kim,Chami Im,Dong-Wan Kang,Yong Soo Kim,Hee-Joon Bae,Sung Yoon Lim,Han-Gil Jeong,Edward Choi*

Main category: cs.AI

TLDR: 论文提出C-Reason，通过强化学习在真实临床数据上微调Phi-4，提升LLMs的临床推理能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在临床实践中的有效性受限，主要因训练数据缺乏真实临床数据。

Method: 利用全国性脓毒症注册数据构建推理密集型问题，通过强化学习微调Phi-4。

Result: C-Reason在域内测试集上表现优异，且推理能力泛化至其他任务和疾病。

Conclusion: 未来需利用大规模多疾病临床数据训练LLMs，以开发更通用的临床推理模型。

Abstract: Although large language models (LLMs) have demonstrated impressive reasoning
capabilities across general domains, their effectiveness in real-world clinical
practice remains limited. This is likely due to their insufficient exposure to
real-world clinical data during training, as such data is typically not
included due to privacy concerns. To address this, we propose enhancing the
clinical reasoning capabilities of LLMs by leveraging real-world clinical data.
We constructed reasoning-intensive questions from a nationwide sepsis registry
and fine-tuned Phi-4 on these questions using reinforcement learning, resulting
in C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the
in-domain test set, as evidenced by both quantitative metrics and expert
evaluations. Furthermore, its enhanced reasoning capabilities generalized to a
sepsis dataset involving different tasks and patient cohorts, an open-ended
consultations on antibiotics use task, and other diseases. Future research
should focus on training LLMs with large-scale, multi-disease clinical datasets
to develop more powerful, general-purpose clinical reasoning models.

</details>

### [248] [FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models](https://arxiv.org/abs/2505.02735)
*Zhouliang Yu,Ruotian Peng,Keyi Ding,Yizhe Li,Zhongyuan Peng,Minghao Liu,Yifan Zhang,Zheng Yuan,Huajian Xin,Wenhao Huang,Yandong Wen,Ge Zhang,Weiyang Liu*

Main category: cs.AI

TLDR: FormalMATH是一个大规模Lean4基准测试，包含5,560个形式化验证的数学问题，并提出了一种人机协作的自动形式化流程，显著降低了专家标注成本。评估显示当前LLM定理证明器在形式数学推理中表现有限。


<details>
  <summary>Details</summary>
Motivation: 解决现有数学推理基准在范围和规模上的不足，推动形式数学推理的发展。

Method: 提出人机协作的自动形式化流程，结合LLM的自动形式化、语义验证和否定过滤策略。

Result: 流程保留了72.09%的语句，但LLM定理证明器成功率仅16.46%，且存在领域偏差和过度依赖简化策略。

Conclusion: FormalMATH为形式数学推理提供了稳健的基准，揭示了LLM在复杂数学推理中的局限性。

Abstract: Formal mathematical reasoning remains a critical challenge for artificial
intelligence, hindered by limitations of existing benchmarks in scope and
scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark
comprising 5,560 formally verified problems spanning from high-school Olympiad
challenges to undergraduate-level theorems across diverse domains (e.g.,
algebra, applied mathematics, calculus, number theory, and discrete
mathematics). To mitigate the inefficiency of manual formalization, we
introduce a novel human-in-the-loop autoformalization pipeline that integrates:
(1) specialized large language models (LLMs) for statement autoformalization,
(2) multi-LLM semantic verification, and (3) negation-based disproof filtering
strategies using off-the-shelf LLM-based provers. This approach reduces expert
annotation costs by retaining 72.09% of statements before manual verification
while ensuring fidelity to the original natural-language problems. Our
evaluation of state-of-the-art LLM-based theorem provers reveals significant
limitations: even the strongest models achieve only 16.46% success rate under
practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling
in algebra but failing in calculus) and over-reliance on simplified automation
tactics. Notably, we identify a counterintuitive inverse relationship between
natural-language solution guidance and proof success in chain-of-thought
reasoning scenarios, suggesting that human-written informal reasoning
introduces noise rather than clarity in the formal reasoning settings. We
believe that FormalMATH provides a robust benchmark for benchmarking formal
mathematical reasoning.

</details>

### [249] [The use of Artificial Intelligence for Intervention and Assessment in Individuals with ASD](https://arxiv.org/abs/2505.02747)
*Aggeliki Sideraki,Christos-Nikolaos Anagnostopoulos*

Main category: cs.AI

TLDR: AI用于自闭症谱系障碍（ASD）的诊断和干预，通过机器学习提高早期诊断准确性，并利用教育机器人和自适应工具改善社交技能。


<details>
  <summary>Details</summary>
Motivation: 探索AI在ASD诊断和干预中的潜力，以克服传统方法的主观性和局限性。

Method: 使用深度学习分析行为数据（如生物特征、视频交互和语言特征），并开发AI驱动的干预工具（如教育机器人和AAC系统）。

Result: AI提高了诊断准确性，减少了主观偏差，并有效支持了ASD儿童的社交和语言发展。

Conclusion: AI是ASD领域的创新工具，需进一步研究其长期效果和个性化应用。

Abstract: This paper explores the use of Artificial Intelligence (AI) as a tool for
diagnosis, assessment, and intervention for individuals with Autism Spectrum
Disorder (ASD). It focuses particularly on AI's role in early diagnosis,
utilizing advanced machine learning techniques and data analysis. Recent
studies demonstrate that deep learning algorithms can identify behavioral
patterns through biometric data analysis, video-based interaction assessments,
and linguistic feature extraction, providing a more accurate and timely
diagnosis compared to traditional methods. Additionally, AI automates
diagnostic tools, reducing subjective biases and enabling the development of
personalized assessment protocols for ASD monitoring. At the same time, the
paper examines AI-powered intervention technologies, emphasizing educational
robots and adaptive communication tools. Social robotic assistants, such as NAO
and Kaspar, have been shown to enhance social skills in children by offering
structured, repetitive interactions that reinforce learning. Furthermore,
AI-driven Augmentative and Alternative Communication (AAC) systems allow
children with ASD to express themselves more effectively, while
machine-learning chatbots provide language development support through
personalized responses. The study presents research findings supporting the
effectiveness of these AI applications while addressing challenges such as
long-term evaluation and customization to individual needs. In conclusion, the
paper highlights the significance of AI as an innovative tool in ASD diagnosis
and intervention, advocating for further research to assess its long-term
impact.

</details>

### [250] [Giving Simulated Cells a Voice: Evolving Prompt-to-Intervention Models for Cellular Control](https://arxiv.org/abs/2505.02766)
*Nam H. Le,Patrick Erikson,Yanbo Zhang,Michael Levin,Josh Bongard*

Main category: cs.AI

TLDR: 该论文提出了一种名为Prompt-to-Intervention（P2I）的管道，将自然语言提示转化为空间向量场，用于指导模拟细胞集体的行为。


<details>
  <summary>Details</summary>
Motivation: 引导生物系统达到期望状态（如形态发生结果）是一个具有深远医学和合成生物学意义的挑战。尽管大型语言模型（LLMs）在AI系统中实现了自然语言的可解释控制，但其作为生物或细胞动力学调控媒介的应用尚未充分探索。

Method: 结合大型语言模型和可进化神经控制器（P2I），通过进化策略优化，生成模拟2D环境中的行为（如聚集或分散）。

Result: 实验表明，即使在受限词汇和简化细胞模型下，P2I网络也能成功将细胞动力学与用户定义的普通语言目标对齐。

Conclusion: 该研究实现了从语言输入到模拟生物电干预再到行为输出的完整闭环，为未来自然语言驱动的细胞控制系统奠定了基础。

Abstract: Guiding biological systems toward desired states, such as morphogenetic
outcomes, remains a fundamental challenge with far-reaching implications for
medicine and synthetic biology. While large language models (LLMs) have enabled
natural language as an interface for interpretable control in AI systems, their
use as mediators for steering biological or cellular dynamics remains largely
unexplored.
  In this work, we present a functional pipeline that translates natural
language prompts into spatial vector fields capable of directing simulated
cellular collectives. Our approach combines a large language model with an
evolvable neural controller (Prompt-to-Intervention, or P2I), optimized via
evolutionary strategies to generate behaviors such as clustering or scattering
in a simulated 2D environment.
  We demonstrate that even with constrained vocabulary and simplified cell
models, evolved P2I networks can successfully align cellular dynamics with
user-defined goals expressed in plain language. This work offers a complete
loop from language input to simulated bioelectric-like intervention to
behavioral output, providing a foundation for future systems capable of natural
language-driven cellular control.

</details>

### [251] [Local Markov Equivalence and Local Causal Discovery for Identifying Controlled Direct Effects](https://arxiv.org/abs/2505.02781)
*Timothée Loranchet,Charles K. Assaad*

Main category: cs.AI

TLDR: 论文提出了一种局部图学习方法（LEG）及算法LocPC和LocPC-CDE，用于识别控制直接效应（CDE），减少计算负担和假设依赖。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖完整的有向无环图（DAG）或等价图，但实际中真实结构未知且计算复杂。

Method: 提出局部图（LEG）概念及LocPC算法，通过局部条件独立性测试恢复LEG；进一步开发LocPC-CDE算法识别CDE所需部分。

Result: 相比全局方法，LocPC和LocPC-CDE减少条件独立性测试并放宽假设，同时保持理论保证。

Conclusion: 局部图方法为识别CDE提供了更高效、实用的解决方案。

Abstract: Understanding and identifying controlled direct effects (CDEs) is crucial
across numerous scientific domains, including public health. While existing
methods can identify these effects from causal directed acyclic graphs (DAGs),
the true underlying structure is often unknown in practice. Essential graphs,
which represent a Markov equivalence class of DAGs characterized by the same
set of d-separations, provide a more practical and realistic alternative.
However, learning the full essential graph is computationally intensive and
typically depends on strong, untestable assumptions. In this work, we
characterize a local class of graphs, defined relative to a target variable,
that share a specific subset of d-separations, and introduce a graphical
representation of this class, called the local essential graph (LEG). We then
present LocPC, a novel algorithm designed to recover the LEG from an observed
distribution using only local conditional independence tests. Building on
LocPC, we propose LocPC-CDE, an algorithm that discovers the portion of the LEG
that is sufficient to identify a CDE, bypassing the need of retrieving the full
essential graph. Compared to global methods, our algorithms require less
conditional independence tests and operate under weaker assumptions while
maintaining theoretical guarantees.

</details>

### [252] [Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing](https://arxiv.org/abs/2505.02811)
*Diji Yang,Linda Zeng,Jinmeng Rao,Yi Zhang*

Main category: cs.AI

TLDR: SIM-RAG 是一种新框架，旨在提升 RAG 系统的自我意识和多轮检索能力，通过合成训练数据和轻量级信息充足性评估器（Critic）优化检索决策。


<details>
  <summary>Details</summary>
Motivation: 当前多轮 RAG 系统存在检索过度或不足的问题，且现有解决方案依赖昂贵的人工标注数据或性能不佳。

Method: 通过 RAG 系统自我实践生成合成训练数据，训练轻量级 Critic 评估信息充足性，指导检索决策。

Result: 实验表明 SIM-RAG 在多轮 RAG 任务中表现优异，且系统高效、数据高效。

Conclusion: SIM-RAG 是一种高效的多轮 RAG 解决方案，无需修改现有 LLM 或搜索引擎，且无需人工标注数据。

Abstract: Retrieval Augmented Generation (RAG) has shown strong capability in enhancing
language models' knowledge and reducing AI generative hallucinations, driving
its widespread use. However, complex tasks requiring multi-round retrieval
remain challenging, and early attempts tend to be overly optimistic without a
good sense of self-skepticism. Current multi-round RAG systems may continue
searching even when enough information has already been retrieved, or they may
provide incorrect answers without having sufficient information or knowledge.
Existing solutions either require large amounts of expensive human-labeled
process supervision data or lead to subpar performance.
  This paper aims to address these limitations by introducing a new framework,
\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and
multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system
self-practice multi-round retrieval, augmenting existing question-answer pairs
with intermediate inner monologue reasoning steps to generate synthetic
training data. For each pair, the system may explore multiple retrieval paths,
which are labeled as successful if they reach the correct answer and
unsuccessful otherwise. Using this data, we train a lightweight information
sufficiency Critic. At inference time, the Critic evaluates whether the RAG
system has retrieved sufficient information at each round, guiding retrieval
decisions and improving system-level self-awareness through in-context
reinforcement learning.
  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an
effective multi-round RAG solution. Furthermore, this framework is
system-efficient, adding a lightweight component to RAG without requiring
modifications to existing LLMs or search engines, and data-efficient,
eliminating the need for costly human-annotated mid-step retrieval process
supervision data.

</details>

### [253] [AutoLibra: Agent Metric Induction from Open-Ended Feedback](https://arxiv.org/abs/2505.02820)
*Hao Zhu,Phil Cuvin,Xinkai Yu,Charlotte Ka Yee Yan,Jason Zhang,Diyi Yang*

Main category: cs.AI

TLDR: AutoLibra是一个框架，通过将开放的人类反馈转化为具体的行为评估指标，优化语言代理的评估和改进。


<details>
  <summary>Details</summary>
Motivation: 现有代理评估指标过于粗糙，依赖专家设计，且无法捕捉中间行为。AutoLibra旨在通过人类反馈生成细粒度指标。

Method: AutoLibra将反馈与代理行为关联，聚类正负行为，生成具体指标，并利用LLM作为评估器。还提出元指标（覆盖率和冗余度）优化指标集。

Result: 实验显示AutoLibra生成的指标比传统基准更具体，并在文本游戏和网页导航任务中提升代理性能20%。

Conclusion: AutoLibra是一个任务无关的强大工具，可用于语言代理的评估和改进。

Abstract: Agents are predominantly evaluated and optimized via task success metrics,
which are coarse, rely on manual design from experts, and fail to reward
intermediate emergent behaviors. We propose AutoLibra, a framework for agent
evaluation, that transforms open-ended human feedback, e.g., "If you find that
the button is disabled, don't click it again", or "This agent has too much
autonomy to decide what to do on its own", into metrics for evaluating
fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by
grounding feedback to an agent's behavior, clustering similar positive and
negative behaviors, and creating concrete metrics with clear definitions and
concrete examples, which can be used for prompting LLM-as-a-Judge as
evaluators. We further propose two meta-metrics to evaluate the alignment of a
set of (induced) metrics with open feedback: "coverage" and "redundancy".
Through optimizing these meta-metrics, we experimentally demonstrate
AutoLibra's ability to induce more concrete agent evaluation metrics than the
ones proposed in previous agent evaluation benchmarks and discover new metrics
to analyze agents. We also present two applications of AutoLibra in agent
improvement: First, we show that AutoLibra-induced metrics serve as better
prompt-engineering targets than the task success rate on a wide range of text
game tasks, improving agent performance over baseline by a mean of 20%. Second,
we show that AutoLibra can iteratively select high-quality fine-tuning data for
web navigation agents. Our results suggest that AutoLibra is a powerful
task-agnostic tool for evaluating and improving language agents.

</details>

### [254] [LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery](https://arxiv.org/abs/2505.02829)
*Jerome Quenum,Wen-Han Hsieh,Tsung-Han Wu,Ritwik Gupta,Trevor Darrell,David M. Chan*

Main category: cs.AI

TLDR: LISAt是一个专为复杂遥感场景设计的视觉语言模型，能够描述场景、回答问题并分割目标对象，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有分割模型难以处理复杂遥感图像中的多目标隐式查询，LISAt旨在填补这一空白。

Method: LISAt基于新构建的GRES数据集（27,615标注）和PreGRES预训练数据集（100万QA对）进行训练。

Result: LISAt在遥感描述任务中比RS-GPT4V提升10.04%（BLEU-4），在推理分割任务中比现有模型提升143.36%（gIoU）。

Conclusion: LISAt在复杂遥感场景中表现出色，相关资源已开源。

Abstract: Segmentation models can recognize a pre-defined set of objects in images.
However, models that can reason over complex user queries that implicitly refer
to multiple objects of interest are still in their infancy. Recent advances in
reasoning segmentation--generating segmentation masks from complex, implicit
query text--demonstrate that vision-language models can operate across an open
domain and produce reasonable outputs. However, our experiments show that such
models struggle with complex remote-sensing imagery. In this work, we introduce
LISAt, a vision-language model designed to describe complex remote-sensing
scenes, answer questions about them, and segment objects of interest. We
trained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES,
with 27,615 annotations over 9,205 images, and a multimodal pretraining
dataset, PreGRES, containing over 1 million question-answer pairs. LISAt
outperforms existing geospatial foundation models such as RS-GPT4V by over
10.04 % (BLEU-4) on remote-sensing description tasks, and surpasses
state-of-the-art open-domain models on reasoning segmentation tasks by 143.36 %
(gIoU). Our model, datasets, and code are available at
https://lisat-bair.github.io/LISAt/

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [255] [Multi-party Collaborative Attention Control for Image Customization](https://arxiv.org/abs/2505.01428)
*Han Yang,Chuanguang Yang,Qiuli Wang,Zhulin An,Weilun Feng,Libo Huang,Yongjun Xu*

Main category: cs.CV

TLDR: 论文提出了一种无需调参的多方协作注意力控制方法（MCA-Ctrl），通过结合文本和复杂视觉条件实现高质量图像定制，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前图像定制方法存在单条件输入、复杂场景下的主体泄漏或混淆、背景不一致以及高计算成本等问题，亟需一种更高效的解决方案。

Method: MCA-Ctrl利用自注意力层的两个关键操作协调多个并行扩散过程，并结合主体定位模块提取精确的主体和可编辑图像层。

Result: 实验表明，MCA-Ctrl在零样本图像定制中优于现有方法，有效解决了主体泄漏、背景不一致等问题。

Conclusion: MCA-Ctrl为复杂视觉条件下的图像定制提供了一种高效且无需调参的解决方案。

Abstract: The rapid advancement of diffusion models has increased the need for
customized image generation. However, current customization methods face
several limitations: 1) typically accept either image or text conditions alone;
2) customization in complex visual scenarios often leads to subject leakage or
confusion; 3) image-conditioned outputs tend to suffer from inconsistent
backgrounds; and 4) high computational costs. To address these issues, this
paper introduces Multi-party Collaborative Attention Control (MCA-Ctrl), a
tuning-free method that enables high-quality image customization using both
text and complex visual conditions. Specifically, MCA-Ctrl leverages two key
operations within the self-attention layer to coordinate multiple parallel
diffusion processes and guide the target image generation. This approach allows
MCA-Ctrl to capture the content and appearance of specific subjects while
maintaining semantic consistency with the conditional input. Additionally, to
mitigate subject leakage and confusion issues common in complex visual
scenarios, we introduce a Subject Localization Module that extracts precise
subject and editable image layers based on user instructions. Extensive
quantitative and human evaluation experiments show that MCA-Ctrl outperforms
existing methods in zero-shot image customization, effectively resolving the
mentioned issues.

</details>

### [256] [Explainable AI-Driven Detection of Human Monkeypox Using Deep Learning and Vision Transformers: A Comprehensive Analysis](https://arxiv.org/abs/2505.01429)
*Md. Zahid Hossain,Md. Rakibul Islam,Most. Sharmin Sultana Samu*

Main category: cs.CV

TLDR: 研究探讨了使用深度学习（DL）和视觉变换器（ViT）模型从公开皮肤病变图像数据集中训练分类器的可行性，发现数据限制是主要问题。通过迁移学习，MobileNet-v2表现最佳。


<details>
  <summary>Details</summary>
Motivation: 由于mpox的症状与麻疹和水痘相似，早期临床诊断困难，医学影像与DL技术结合有望提升检测效果。

Method: 使用公开皮肤病变数据集训练DL和ViT模型，并采用迁移学习和预训练模型优化分类器性能。

Result: MobileNet-v2表现最佳（准确率93.15%），ViT B16和ResNet-50也表现良好。

Conclusion: 迁移学习能有效克服数据限制，提升分类器性能，可解释AI技术进一步验证了模型效果。

Abstract: Since mpox can spread from person to person, it is a zoonotic viral illness
that poses a significant public health concern. It is difficult to make an
early clinical diagnosis because of how closely its symptoms match those of
measles and chickenpox. Medical imaging combined with deep learning (DL)
techniques has shown promise in improving disease detection by analyzing
affected skin areas. Our study explore the feasibility to train deep learning
and vision transformer-based models from scratch with publicly available skin
lesion image dataset. Our experimental results show dataset limitation as a
major drawback to build better classifier models trained from scratch. We used
transfer learning with the help of pre-trained models to get a better
classifier. The MobileNet-v2 outperformed other state of the art pre-trained
models with 93.15% accuracy and 93.09% weighted average F1 score. ViT B16 and
ResNet-50 also achieved satisfactory performance compared to already available
studies with accuracy 92.12% and 86.21% respectively. To further validate the
performance of the models, we applied explainable AI techniques.

</details>

### [257] [Deconstructing Bias: A Multifaceted Framework for Diagnosing Cultural and Compositional Inequities in Text-to-Image Generative Models](https://arxiv.org/abs/2505.01430)
*Muna Numan Said,Aarib Zaidi,Rabia Usman,Sonia Okon,Praneeth Medepalli,Kevin Zhu,Vasu Sharma,Sean O'Brien*

Main category: cs.CV

TLDR: 本文提出了一种名为Component Inclusion Score (CIS)的指标，用于评估文本到图像模型在生成文化多样性图像时的表现，揭示了模型在西方与非西方文化提示间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在生成文化多样性图像时存在偏见，导致系统性误表达，需要一种量化方法来评估和改善这一问题。

Method: 通过分析2400张图像，提出CIS指标，量化模型在文化背景下的生成表现，并研究数据不平衡、注意力熵和嵌入叠加对公平性的影响。

Result: 研究发现模型在西方与非西方文化提示间存在显著性能差距，揭示了数据不平衡和模型架构对偏见的影响。

Conclusion: CIS为诊断和缓解文本到图像生成中的偏见提供了工具，推动了更公平的AI系统发展。

Abstract: The transformative potential of text-to-image (T2I) models hinges on their
ability to synthesize culturally diverse, photorealistic images from textual
prompts. However, these models often perpetuate cultural biases embedded within
their training data, leading to systemic misrepresentations. This paper
benchmarks the Component Inclusion Score (CIS), a metric designed to evaluate
the fidelity of image generation across cultural contexts. Through extensive
analysis involving 2,400 images, we quantify biases in terms of compositional
fragility and contextual misalignment, revealing significant performance gaps
between Western and non-Western cultural prompts. Our findings underscore the
impact of data imbalance, attention entropy, and embedding superposition on
model fairness. By benchmarking models like Stable Diffusion with CIS, we
provide insights into architectural and data-centric interventions for
enhancing cultural inclusivity in AI-generated imagery. This work advances the
field by offering a comprehensive tool for diagnosing and mitigating biases in
T2I generation, advocating for more equitable AI systems.

</details>

### [258] [ZS-VCOS: Zero-Shot Outperforms Supervised Video Camouflaged Object Segmentation](https://arxiv.org/abs/2505.01431)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TLDR: 该论文提出了一种结合光流、视觉语言模型和SAM 2的零样本方法，显著提升了伪装物体分割的性能，超越了现有零样本和监督方法。


<details>
  <summary>Details</summary>
Motivation: 伪装物体分割因物体与背景高度相似而具有挑战性，现有零样本方法性能不足，光流在此类任务中表现优异，因此提出新方法。

Method: 整合光流、视觉语言模型和SAM 2，形成顺序处理流程。

Result: 在MoCA-Mask数据集上，F-measure从0.296提升至0.628；在MoCA-Filter数据集上，成功率从0.628提升至0.697。

Conclusion: 该方法显著优于现有零样本和监督方法，各组件贡献通过消融实验验证。

Abstract: Camouflaged object segmentation presents unique challenges compared to
traditional segmentation tasks, primarily due to the high similarity in
patterns and colors between camouflaged objects and their backgrounds.
Effective solutions to this problem have significant implications in critical
areas such as pest control, defect detection, and lesion segmentation in
medical imaging. Prior research has predominantly emphasized supervised or
unsupervised pre-training methods, leaving zero-shot approaches significantly
underdeveloped. Existing zero-shot techniques commonly utilize the Segment
Anything Model (SAM) in automatic mode or rely on vision-language models to
generate cues for segmentation; however, their performances remain
unsatisfactory, likely due to the similarity of the camouflaged object and the
background. Optical flow, commonly utilized for detecting moving objects, has
demonstrated effectiveness even with camouflaged entities. Our method
integrates optical flow, a vision-language model, and SAM 2 into a sequential
pipeline. Evaluated on the MoCA-Mask dataset, our approach achieves outstanding
performance improvements, significantly outperforming existing zero-shot
methods by raising the F-measure ($F_\beta^w$) from 0.296 to 0.628. Remarkably,
our approach also surpasses supervised methods, increasing the F-measure from
0.476 to 0.628. Additionally, evaluation on the MoCA-Filter dataset
demonstrates an increase in the success rate from 0.628 to 0.697 when compared
with FlowSAM, a supervised transfer method. A thorough ablation study further
validates the individual contributions of each component. More details can be
found on https://github.com/weathon/vcos.

</details>

### [259] [VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for Synthetic Videos](https://arxiv.org/abs/2505.01481)
*Zongxia Li,Xiyang Wu,Yubin Qin,Guangyao Shi,Hongyang Du,Dinesh Manocha,Tianyi Zhou,Jordan Lee Boyd-Graber*

Main category: cs.CV

TLDR: 论文提出VideoHallu基准，用于评估多模态大语言模型（MLLMs）在合成视频中检测异常的能力，并通过GRPO微调提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有合成视频生成模型虽能生成高质量帧，但常违反常识和物理规律，而现有评估指标（如VideoScore）无法有效检测此类问题。

Method: 引入VideoHallu基准，包含专家设计的QA任务，评估多个SoTA MLLMs（如GPT-4o、Gemini-2.5-Pro等），并使用GRPO方法微调模型。

Result: 实验显示，MLLMs在合成视频中仍存在幻觉问题，但通过GRPO微调和反例集成显著提升了准确性。

Conclusion: VideoHallu揭示了MLLMs在合成视频评估中的局限性，GRPO方法为提升模型推理能力提供了有效途径。

Abstract: Synthetic video generation with foundation models has gained attention for
its realism and wide applications. While these models produce high-quality
frames, they often fail to respect common sense and physical laws, resulting in
abnormal content. Existing metrics like VideoScore emphasize general quality
but ignore such violations and lack interpretability. A more insightful
approach is using multi-modal large language models (MLLMs) as interpretable
evaluators, as seen in FactScore. Yet, MLLMs' ability to detect abnormalities
in synthetic videos remains underexplored. To address this, we introduce
VideoHallu, a benchmark featuring synthetic videos from models like Veo2, Sora,
and Kling, paired with expert-designed QA tasks solvable via human-level
reasoning across various categories. We assess several SoTA MLLMs, including
GPT-4o, Gemini-2.5-Pro, Qwen-2.5-VL, and newer models like Video-R1 and
VideoChat-R1. Despite strong real-world performance on MVBench and MovieChat,
these models still hallucinate on basic commonsense and physics tasks in
synthetic settings, underscoring the challenge of hallucination. We further
fine-tune SoTA MLLMs using Group Relative Policy Optimization (GRPO) on real
and synthetic commonsense/physics data. Results show notable accuracy gains,
especially with counterexample integration, advancing MLLMs' reasoning
capabilities. Our data is available at https://github.com/zli12321/VideoHallu.

</details>

### [260] [WorldGenBench: A World-Knowledge-Integrated Benchmark for Reasoning-Driven Text-to-Image Generation](https://arxiv.org/abs/2505.01490)
*Daoan Zhang,Che Jiang,Ruoshi Xu,Biaoxiang Chen,Zijian Jin,Yutian Lu,Jianguo Zhang,Liang Yong,Jiebo Luo,Shengda Luo*

Main category: cs.CV

TLDR: WorldGenBench是一个评估文本到图像（T2I）模型世界知识和隐式推理能力的基准，提出了知识清单评分来衡量生成图像的语义准确性。实验表明，扩散模型在开源方法中表现最佳，但GPT-4o等专有模型在推理和知识整合方面更强。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型在需要丰富世界知识和隐式推理的提示下表现不佳，影响生成图像的语义准确性和上下文适当性。

Method: 引入WorldGenBench基准和知识清单评分，系统评估T2I模型的能力，涵盖人文和自然领域。

Result: 实验显示扩散模型在开源方法中领先，但专有模型如GPT-4o在推理和知识整合方面表现更优。

Conclusion: 下一代T2I系统需要更强的理解和推理能力。

Abstract: Recent advances in text-to-image (T2I) generation have achieved impressive
results, yet existing models still struggle with prompts that require rich
world knowledge and implicit reasoning: both of which are critical for
producing semantically accurate, coherent, and contextually appropriate images
in real-world scenarios. To address this gap, we introduce
\textbf{WorldGenBench}, a benchmark designed to systematically evaluate T2I
models' world knowledge grounding and implicit inferential capabilities,
covering both the humanities and nature domains. We propose the
\textbf{Knowledge Checklist Score}, a structured metric that measures how well
generated images satisfy key semantic expectations. Experiments across 21
state-of-the-art models reveal that while diffusion models lead among
open-source methods, proprietary auto-regressive models like GPT-4o exhibit
significantly stronger reasoning and knowledge integration. Our findings
highlight the need for deeper understanding and inference capabilities in
next-generation T2I systems. Project Page:
\href{https://dwanzhang-ai.github.io/WorldGenBench/}{https://dwanzhang-ai.github.io/WorldGenBench/}

</details>

### [261] [Automated Parsing of Engineering Drawings for Structured Information Extraction Using a Fine-tuned Document Understanding Transformer](https://arxiv.org/abs/2505.01530)
*Muhammad Tayyab Khan,Zane Yong,Lequn Chen,Jun Ming Tan,Wenhe Feng,Seung Ki Moon*

Main category: cs.CV

TLDR: 提出了一种结合OBB检测模型和Donut解析模型的混合深度学习框架，用于从2D工程图中提取结构化信息，显著提高了精度并减少了人工干预。


<details>
  <summary>Details</summary>
Motivation: 传统OCR技术在处理复杂布局和重叠符号时表现不佳，手动提取耗时且易出错，需要一种更高效、准确的方法。

Method: 使用YOLOv11检测关键类别（如GD&T、尺寸等），并通过Donut模型将检测到的OBB裁剪为图像并生成结构化JSON输出。比较了单一模型和类别特定模型的性能。

Result: 单一模型在所有评估指标上均优于类别特定模型，精度达94.77%，召回率100%，F1分数97.3%，且幻觉率降至5.23%。

Conclusion: 该框架提高了信息提取的准确性，减少了人工工作量，适用于高精度制造行业的大规模部署。

Abstract: Accurate extraction of key information from 2D engineering drawings is
crucial for high-precision manufacturing. Manual extraction is time-consuming
and error-prone, while traditional Optical Character Recognition (OCR)
techniques often struggle with complex layouts and overlapping symbols,
resulting in unstructured outputs. To address these challenges, this paper
proposes a novel hybrid deep learning framework for structured information
extraction by integrating an oriented bounding box (OBB) detection model with a
transformer-based document parsing model (Donut). An in-house annotated dataset
is used to train YOLOv11 for detecting nine key categories: Geometric
Dimensioning and Tolerancing (GD&T), General Tolerances, Measures, Materials,
Notes, Radii, Surface Roughness, Threads, and Title Blocks. Detected OBBs are
cropped into images and labeled to fine-tune Donut for structured JSON output.
Fine-tuning strategies include a single model trained across all categories and
category-specific models. Results show that the single model consistently
outperforms category-specific ones across all evaluation metrics, achieving
higher precision (94.77% for GD&T), recall (100% for most), and F1 score
(97.3%), while reducing hallucination (5.23%). The proposed framework improves
accuracy, reduces manual effort, and supports scalable deployment in
precision-driven industries.

</details>

### [262] [Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional Motion-enhanced Event Representation](https://arxiv.org/abs/2505.01548)
*Zhen Yao,Xiaowen Ying,Mooi Choo Chuah*

Main category: cs.CV

TLDR: 论文提出了一种新的RGB-Event融合方法，通过Motion-enhanced Event Tensor (MET)和两个模块（BFAM和TFM）解决时空和模态不对齐问题，显著提升了语义分割性能。


<details>
  <summary>Details</summary>
Motivation: RGB-Event融合中存在时空和模态不对齐问题，现有方法未能有效解决。

Method: 提出MET将稀疏事件体素转换为密集且时间一致的形式，并引入BFAM和TFM模块分别解决模态和时空不对齐。

Result: 在两个大规模数据集上，该方法显著优于现有RGB-Event语义分割方法。

Conclusion: MET和提出的模块有效解决了RGB-Event融合中的关键挑战，提升了性能。

Abstract: Event cameras capture motion dynamics, offering a unique modality with great
potential in various computer vision tasks. However, RGB-Event fusion faces
three intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal
misalignment. Existing voxel grid representations neglect temporal correlations
between consecutive event windows, and their formulation with simple
accumulation of asynchronous and sparse events is incompatible with the
synchronous and dense nature of RGB modality. To tackle these challenges, we
propose a novel event representation, Motion-enhanced Event Tensor (MET), which
transforms sparse event voxels into a dense and temporally coherent form by
leveraging dense optical flows and event temporal features. In addition, we
introduce a Frequency-aware Bidirectional Flow Aggregation Module (BFAM) and a
Temporal Fusion Module (TFM). BFAM leverages the frequency domain and MET to
mitigate modal misalignment, while bidirectional flow aggregation and temporal
fusion mechanisms resolve spatiotemporal misalignment. Experimental results on
two large-scale datasets demonstrate that our framework significantly
outperforms state-of-the-art RGB-Event semantic segmentation approaches. Our
code is available at: https://github.com/zyaocoder/BRENet.

</details>

### [263] [A Sensor Agnostic Domain Generalization Framework for Leveraging Geospatial Foundation Models: Enhancing Semantic Segmentation viaSynergistic Pseudo-Labeling and Generative Learning](https://arxiv.org/abs/2505.01558)
*Anan Yaghmour,Melba M. Crawford,Saurabh Prasad*

Main category: cs.CV

TLDR: 本文提出了一种结合软对齐伪标签和生成预训练的领域泛化方法，用于提升遥感图像分割模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 遥感数据标注稀缺且存在传感器、光照和地理差异，限制了高性能分割模型的应用。领域适应是提升模型泛化性的潜在解决方案。

Method: 采用软对齐伪标签和源到目标的生成预训练方法，结合MAE生成学习实现领域不变特征学习。

Result: 实验证明该方法在超光谱和多光谱遥感数据集上有效提升了适应性和分割性能。

Conclusion: 提出的方法为遥感图像分割领域提供了有效的领域泛化解决方案。

Abstract: Remote sensing enables a wide range of critical applications such as land
cover and land use mapping, crop yield prediction, and environmental
monitoring. Advances in satellite technology have expanded remote sensing
datasets, yet high-performance segmentation models remain dependent on
extensive labeled data, challenged by annotation scarcity and variability
across sensors, illumination, and geography. Domain adaptation offers a
promising solution to improve model generalization. This paper introduces a
domain generalization approach to leveraging emerging geospatial foundation
models by combining soft-alignment pseudo-labeling with source-to-target
generative pre-training. We further provide new mathematical insights into
MAE-based generative learning for domain-invariant feature learning.
Experiments with hyperspectral and multispectral remote sensing datasets
confirm our method's effectiveness in enhancing adaptability and segmentation.

</details>

### [264] [PainFormer: a Vision Foundation Model for Automatic Pain Assessment](https://arxiv.org/abs/2505.01571)
*Stefanos Gkikas,Raul Fernandez Rojas,Manolis Tsiknakis*

Main category: cs.CV

TLDR: PainFormer是一种基于多任务学习的视觉基础模型，用于自动疼痛评估，支持多种输入模态，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 疼痛评估对疼痛管理至关重要，自动评估系统能提供持续监测和决策支持，减轻患者痛苦。

Method: PainFormer基于多任务学习，训练于14个任务/数据集（共1090万样本），通过Embedding-Mixer模块进行最终评估。

Result: 在RGB、热成像、深度视频及生理信号等多种模态下，PainFormer表现优异，优于73种现有方法。

Conclusion: PainFormer为自动疼痛评估提供了通用模型，展现了跨模态的先进性能。

Abstract: Pain is a manifold condition that impacts a significant percentage of the
population. Accurate and reliable pain evaluation for the people suffering is
crucial to developing effective and advanced pain management protocols.
Automatic pain assessment systems provide continuous monitoring and support
decision-making processes, ultimately aiming to alleviate distress and prevent
functionality decline. This study introduces PainFormer, a vision foundation
model based on multi-task learning principles trained simultaneously on 14
tasks/datasets with a total of 10.9 million samples. Functioning as an
embedding extractor for various input modalities, the foundation model provides
feature representations to the Embedding-Mixer, a transformer-based module that
performs the final pain assessment. Extensive experiments employing behavioral
modalities-including RGB, synthetic thermal, and estimated depth videos-and
physiological modalities such as ECG, EMG, GSR, and fNIRS revealed that
PainFormer effectively extracts high-quality embeddings from diverse input
modalities. The proposed framework is evaluated on two pain datasets, BioVid
and AI4Pain, and directly compared to 73 different methodologies documented in
the literature. Experiments conducted in unimodal and multimodal settings
demonstrate state-of-the-art performances across modalities and pave the way
toward general-purpose models for automatic pain assessment.

</details>

### [265] [Grounding Task Assistance with Multimodal Cues from a Single Demonstration](https://arxiv.org/abs/2505.01578)
*Gabriel Sarch,Balasaravanan Thoravi Kumaravel,Sahithya Ravi,Vibhav Vineet,Andrew D. Wilson*

Main category: cs.CV

TLDR: MICA框架通过整合眼动和语音信号，提升任务辅助对话代理的性能，捕捉意图和用户特定线索，显著优于基于帧的检索。


<details>
  <summary>Details</summary>
Motivation: RGB视频无法捕捉意图、安全关键因素和用户偏好，限制了视觉语言模型的推理能力。

Method: MICA整合眼动和语音信号，将演示分解为子任务，提取关键帧和字幕。

Result: 多模态信号显著提升回答质量，眼动信号单独表现接近语音，组合效果最佳。

Conclusion: 多模态信号对现实AI任务辅助至关重要，需适应不同任务类型。

Abstract: A person's demonstration often serves as a key reference for others learning
the same task. However, RGB video, the dominant medium for representing these
demonstrations, often fails to capture fine-grained contextual cues such as
intent, safety-critical environmental factors, and subtle preferences embedded
in human behavior. This sensory gap fundamentally limits the ability of Vision
Language Models (VLMs) to reason about why actions occur and how they should
adapt to individual users. To address this, we introduce MICA (Multimodal
Interactive Contextualized Assistance), a framework that improves
conversational agents for task assistance by integrating eye gaze and speech
cues. MICA segments demonstrations into meaningful sub-tasks and extracts
keyframes and captions that capture fine-grained intent and user-specific cues,
enabling richer contextual grounding for visual question answering. Evaluations
on questions derived from real-time chat-assisted task replication show that
multimodal cues significantly improve response quality over frame-based
retrieval. Notably, gaze cues alone achieves 93% of speech performance, and
their combination yields the highest accuracy. Task type determines the
effectiveness of implicit (gaze) vs. explicit (speech) cues, underscoring the
need for adaptable multimodal models. These results highlight the limitations
of frame-based context and demonstrate the value of multimodal signals for
real-world AI task assistance.

</details>

### [266] [TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action](https://arxiv.org/abs/2505.01583)
*Jen-Hao Cheng,Vivian Wang,Huayu Wang,Huapeng Zhou,Yi-Hao Peng,Hou-I Liu,Hsiang-Wei Huang,Kuang-Ming Chen,Cheng-Yen Yang,Wenhao Chai,Yi-Ling Chen,Vibhav Vineet,Qin Cai,Jenq-Neng Hwang*

Main category: cs.CV

TLDR: TEMPURA是一个两阶段训练框架，通过掩码事件预测和因果推理增强视频时间理解，结合细粒度分割和密集标注，显著提升视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视频时间分辨率和因果依赖建模上存在不足，TEMPURA旨在解决这些问题。

Method: 采用两阶段训练：掩码事件预测推理和视频分割密集标注。

Result: 在时间定位和高光检测基准测试中优于基线模型。

Conclusion: 结合因果推理与细粒度时间分割可显著提升视频理解。

Abstract: Understanding causal event relationships and achieving fine-grained temporal
grounding in videos remain challenging for vision-language models. Existing
methods either compress video tokens to reduce temporal resolution, or treat
videos as unsegmented streams, which obscures fine-grained event boundaries and
limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event
Masked Prediction and Understanding for Reasoning in Action), a two-stage
training framework that enhances video temporal understanding. TEMPURA first
applies masked event prediction reasoning to reconstruct missing events and
generate step-by-step causal explanations from dense event annotations, drawing
inspiration from effective infilling techniques. TEMPURA then learns to perform
video segmentation and dense captioning to decompose videos into
non-overlapping events with detailed, timestamp-aligned descriptions. We train
TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training
instances and 500K videos with temporally aligned event descriptions and
structured reasoning steps. Experiments on temporal grounding and highlight
detection benchmarks demonstrate that TEMPURA outperforms strong baseline
models, confirming that integrating causal reasoning with fine-grained temporal
segmentation leads to improved video understanding.

</details>

### [267] [Multimodal and Multiview Deep Fusion for Autonomous Marine Navigation](https://arxiv.org/abs/2505.01615)
*Dimitrios Dagdilelis,Panagiotis Grigoriadis,Roberto Galeazzi*

Main category: cs.CV

TLDR: 提出了一种基于跨注意力变换器的多模态传感器融合方法，用于构建船舶周围环境的鸟瞰图，支持更安全的自主海洋导航。


<details>
  <summary>Details</summary>
Motivation: 通过融合多模态传感器数据（RGB、红外、LiDAR、雷达和电子海图），提高自主海洋导航的准确性和鲁棒性。

Method: 使用跨注意力变换器深度融合多视角RGB和长波红外图像以及稀疏LiDAR点云，并结合X波段雷达和电子海图数据进行训练。

Result: 生成的鸟瞰图提供了详细可靠的场景表示，提高了导航精度和鲁棒性。

Conclusion: 实际海上试验证实了该方法在恶劣天气和复杂海洋环境中的有效性。

Abstract: We propose a cross attention transformer based method for multimodal sensor
fusion to build a birds eye view of a vessels surroundings supporting safer
autonomous marine navigation. The model deeply fuses multiview RGB and long
wave infrared images with sparse LiDAR point clouds. Training also integrates X
band radar and electronic chart data to inform predictions. The resulting view
provides a detailed reliable scene representation improving navigational
accuracy and robustness. Real world sea trials confirm the methods
effectiveness even in adverse weather and complex maritime settings.

</details>

### [268] [Toward Onboard AI-Enabled Solutions to Space Object Detection for Space Sustainability](https://arxiv.org/abs/2505.01650)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TLDR: 论文研究了基于深度学习的视觉传感器在低地球轨道卫星空间物体检测中的可行性，提出了结合SE层、ViT和GELAN的模型，显著提升了检测精度并降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 随着低地球轨道卫星的快速扩张，空间物体检测成为关键挑战，需要高精度和低延迟的解决方案。

Method: 提出了基于Squeeze-and-Excitation层、Vision Transformer和GELAN的深度学习模型，并评估其在空间物体检测任务中的性能。

Result: 实验结果显示，GELAN-ViT-SE模型在mAP50和mAP50:95上分别达到0.751和0.280，同时降低了计算开销和功耗。

Conclusion: 结合SE层、ViT和GELAN的模型在空间物体检测中表现出色，为未来卫星任务提供了高效解决方案。

Abstract: The rapid expansion of advanced low-Earth orbit (LEO) satellites in large
constellations is positioning space assets as key to the future, enabling
global internet access and relay systems for deep space missions. A solution to
the challenge is effective space object detection (SOD) for collision
assessment and avoidance. In SOD, an LEO satellite must detect other satellites
and objects with high precision and minimal delay. This paper investigates the
feasibility and effectiveness of employing vision sensors for SOD tasks based
on deep learning (DL) models. It introduces models based on the
Squeeze-and-Excitation (SE) layer, Vision Transformer (ViT), and the
Generalized Efficient Layer Aggregation Network (GELAN) and evaluates their
performance under SOD scenarios. Experimental results show that the proposed
models achieve mean average precision at intersection over union threshold 0.5
(mAP50) scores of up to 0.751 and mean average precision averaged over
intersection over union thresholds from 0.5 to 0.95 (mAP50:95) scores of up to
0.280. Compared to the baseline GELAN-t model, the proposed GELAN-ViT-SE model
increases the average mAP50 from 0.721 to 0.751, improves the mAP50:95 from
0.266 to 0.274, reduces giga floating point operations (GFLOPs) from 7.3 to
5.6, and lowers peak power consumption from 2080.7 mW to 2028.7 mW by 2.5\%.

</details>

### [269] [A Novel WaveInst-based Network for Tree Trunk Structure Extraction and Pattern Analysis in Forest Inventory](https://arxiv.org/abs/2505.01656)
*Chenyang Fan,Xujie Zhu,Taige Luo,Sheng Xu,Zhulin Chen,Hongxin Yang*

Main category: cs.CV

TLDR: 提出了一种基于离散小波变换的WaveInst实例分割框架，用于从复杂背景中提取树木结构，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有LiDAR和UAV技术在树木结构提取中的高成本或信息缺失问题。

Method: 采用离散小波变换增强多尺度边缘信息，结合实例分割框架WaveInst。

Result: 在多个数据集上表现优于现有方法，平均精度提升9.9，并成功提取树木生长参数。

Conclusion: 为表型研究和精准林业提供了高效的数据支持和方法平台。

Abstract: The pattern analysis of tree structure holds significant scientific value for
genetic breeding and forestry management. The current trunk and branch
extraction technologies are mainly LiDAR-based or UAV-based. The former
approaches obtain high-precision 3D data, but its equipment cost is high and
the three-dimensional (3D) data processing is complex. The latter approaches
efficiently capture canopy information, but they miss the 3-D structure of
trees. In order to deal with the branch information extraction from the complex
background interference and occlusion, this work proposes a novel WaveInst
instance segmentation framework, involving a discrete wavelet transform, to
enhance multi-scale edge information for accurately improving tree structure
extraction. Experimental results of the proposed model show superior
performance on SynthTree43k, CaneTree100, Urban Street and our PoplarDataset.
Moreover, we present a new Phenotypic dataset PoplarDataset, which is dedicated
to extract tree structure and pattern analysis from artificial forest. The
proposed method achieves a mean average precision of 49.6 and 24.3 for the
structure extraction of mature and juvenile trees, respectively, surpassing the
existing state-of-the-art method by 9.9. Furthermore, by in tegrating the
segmentation model within the regression model, we accurately achieve
significant tree grown parameters, such as the location of trees, the
diameter-at-breast-height of individual trees, and the plant height, from 2D
images directly. This study provides a scientific and plenty of data for tree
structure analysis in related to the phenotype research, offering a platform
for the significant applications in precision forestry, ecological monitoring,
and intelligent breeding.

</details>

### [270] [Soft-Masked Semi-Dual Optimal Transport for Partial Domain Adaptation](https://arxiv.org/abs/2505.01664)
*Yi-Ming Zhai,Chuan-Xian Ren,Hong Yan*

Main category: cs.CV

TLDR: 提出了一种软掩码半对偶最优传输（SSOT）方法，用于解决部分域适应（PDA）问题，通过估计类别权重和构建软掩码传输距离矩阵，实现了类条件分布匹配。


<details>
  <summary>Details</summary>
Motivation: 部分域适应（PDA）中目标域标签空间是源域的子集，存在域偏移和标签空间不一致的挑战，需要一种有效的方法来学习域不变表示。

Method: 提出SSOT方法，包括估计类别权重、构建重加权源域、软掩码传输距离矩阵，并利用半对偶熵正则化Kantorovich问题优化。

Result: 在四个基准数据集上进行了广泛实验，证明了SSOT的有效性。

Conclusion: SSOT通过类条件分布匹配和半对偶优化，成功解决了PDA问题，并在实验中表现出色。

Abstract: Visual domain adaptation aims to learn discriminative and domain-invariant
representation for an unlabeled target domain by leveraging knowledge from a
labeled source domain. Partial domain adaptation (PDA) is a general and
practical scenario in which the target label space is a subset of the source
one. The challenges of PDA exist due to not only domain shift but also the
non-identical label spaces of domains. In this paper, a Soft-masked Semi-dual
Optimal Transport (SSOT) method is proposed to deal with the PDA problem.
Specifically, the class weights of domains are estimated, and then a reweighed
source domain is constructed, which is favorable in conducting
class-conditional distribution matching with the target domain. A soft-masked
transport distance matrix is constructed by category predictions, which will
enhance the class-oriented representation ability of optimal transport in the
shared feature space. To deal with large-scale optimal transport problems, the
semi-dual formulation of the entropy-regularized Kantorovich problem is
employed since it can be optimized by gradient-based algorithms. Further, a
neural network is exploited to approximate the Kantorovich potential due to its
strong fitting ability. This network parametrization also allows the
generalization of the dual variable outside the supports of the input
distribution. The SSOT model is built upon neural networks, which can be
optimized alternately in an end-to-end manner. Extensive experiments are
conducted on four benchmark datasets to demonstrate the effectiveness of SSOT.

</details>

### [271] [Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A Clinician Study](https://arxiv.org/abs/2505.01680)
*Tamim Ahmed,Thanassis Rikakis*

Main category: cs.CV

TLDR: 提出了一种自动化的ARAT评分系统，通过多模态视频分析和多种模型结合，显著提高了评分效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 手动评分ARAT耗时且存在变异性，需要一种自动化解决方案以提高效率和一致性。

Method: 整合了SlowFast、I3D和Transformer模型，利用OpenPose关键点和物体位置，采用多视角数据和早期/晚期融合策略，结合HBMs推断运动质量。

Result: 系统验证准确率达89.0%，HBMs与人工评估高度一致，临床反馈良好。

Conclusion: 该研究为康复领域提供了一种可扩展、可解释且经过临床验证的自动化解决方案。

Abstract: Manual scoring of the Action Research Arm Test (ARAT) for upper extremity
assessment in stroke rehabilitation is time-intensive and variable. We propose
an automated ARAT scoring system integrating multimodal video analysis with
SlowFast, I3D, and Transformer-based models using OpenPose keypoints and object
locations. Our approach employs multi-view data (ipsilateral, contralateral,
and top perspectives), applying early and late fusion to combine features
across views and models. Hierarchical Bayesian Models (HBMs) infer movement
quality components, enhancing interpretability. A clinician dashboard displays
task scores, execution times, and quality assessments. We conducted a study
with five clinicians who reviewed 500 video ratings generated by our system,
providing feedback on its accuracy and usability. Evaluated on a stroke
rehabilitation dataset, our framework achieves 89.0% validation accuracy with
late fusion, with HBMs aligning closely with manual assessments. This work
advances automated rehabilitation by offering a scalable, interpretable
solution with clinical validation.

</details>

### [272] [Topology-Aware CLIP Few-Shot Learning](https://arxiv.org/abs/2505.01694)
*Dazhi Huang*

Main category: cs.CV

TLDR: 提出了一种基于拓扑感知的调优方法，通过结合RTD和交叉熵损失，提升视觉语言模型在少样本学习中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在适应视觉语言模型时忽略了潜在空间中的结构信息，导致预训练知识与任务特定适应之间的不平衡。

Method: 采用拓扑感知调优方法，结合RTD和交叉熵损失对齐视觉和文本表示的拓扑结构，同时冻结基础编码器，仅优化轻量级任务残差参数。

Result: 在6个基准数据集上，平均准确率提升1-2%，显著优于基线方法。

Conclusion: 通过拓扑对齐，有效提升了视觉语言模型的少样本学习能力。

Abstract: Efficiently adapting large Vision-Language Models (VLMs) like CLIP for
few-shot learning poses challenges in balancing pre-trained knowledge retention
and task-specific adaptation. Existing methods often overlook valuable
structural information within the VLM's latent space. We introduce a
topology-aware tuning approach integrating Representation Topology Divergence
(RTD) into the Task Residual (TR) framework. By explicitly aligning the
topological structures of visual and text representations using a combined RTD
and Cross-Entropy loss, while freezing base VLM encoders, our method enhances
few-shot performance. We optimize only lightweight Task Residual parameters,
effectively leveraging topological information. Across 6 diverse benchmark
datasets, our approach demonstrates significant gains, achieving an average
accuracy improvement of 1-2\% over relevant baseline methods in few-shot
settings. This work presents an effective strategy to boost VLM few-shot
capabilities by incorporating topological alignment.

</details>

### [273] [Component-Based Fairness in Face Attribute Classification with Bayesian Network-informed Meta Learning](https://arxiv.org/abs/2505.01699)
*Yifan Liu,Ruichen Yao,Yaokun Liu,Ruohan Zong,Zelin Li,Yang Zhang,Dong Wang*

Main category: cs.CV

TLDR: 本文提出了一种名为BNMR的方法，用于解决面部组件公平性问题，通过贝叶斯网络和元学习优化样本权重，实验证明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注人口统计学公平性，而忽略了面部生物特征的公平性，本文旨在填补这一空白。

Method: 提出BNMR方法，结合贝叶斯网络校准器和元学习，动态调整样本权重以解决标签稀缺和属性依赖问题。

Result: 实验表明BNMR在面部组件公平性上优于现有方法，并可能间接提升人口统计学公平性。

Conclusion: 面部组件公平性可作为人口统计学公平性的替代目标，为未来研究开辟新方向。

Abstract: The widespread integration of face recognition technologies into various
applications (e.g., access control and personalized advertising) necessitates a
critical emphasis on fairness. While previous efforts have focused on
demographic fairness, the fairness of individual biological face components
remains unexplored. In this paper, we focus on face component fairness, a
fairness notion defined by biological face features. To our best knowledge, our
work is the first work to mitigate bias of face attribute prediction at the
biological feature level. In this work, we identify two key challenges in
optimizing face component fairness: attribute label scarcity and attribute
inter-dependencies, both of which limit the effectiveness of bias mitigation
from previous approaches. To address these issues, we propose \textbf{B}ayesian
\textbf{N}etwork-informed \textbf{M}eta \textbf{R}eweighting (BNMR), which
incorporates a Bayesian Network calibrator to guide an adaptive
meta-learning-based sample reweighting process. During the training process of
our approach, the Bayesian Network calibrator dynamically tracks model bias and
encodes prior probabilities for face component attributes to overcome the above
challenges. To demonstrate the efficacy of our approach, we conduct extensive
experiments on a large-scale real-world human face dataset. Our results show
that BNMR is able to consistently outperform recent face bias mitigation
baselines. Moreover, our results suggest a positive impact of face component
fairness on the commonly considered demographic fairness (e.g.,
\textit{gender}). Our findings pave the way for new research avenues on face
component fairness, suggesting that face component fairness could serve as a
potential surrogate objective for demographic fairness. The code for our work
is publicly
available~\footnote{https://github.com/yliuaa/BNMR-FairCompFace.git}.

</details>

### [274] [Knowledge-Augmented Language Models Interpreting Structured Chest X-Ray Findings](https://arxiv.org/abs/2505.01711)
*Alexander Davis,Rafael Souza,Jia-Hao Lim*

Main category: cs.CV

TLDR: CXR-TextInter框架通过结构化文本表示和医学知识模块，利用LLM实现胸部X光片的高效解读，性能优于现有多模态模型。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型（LLM）的潜力，改进胸部X光片的自动解读，提升临床工作流程和患者护理。

Method: 提出CXR-TextInter框架，将图像内容转化为结构化文本表示，结合医学知识模块增强临床推理。

Result: 在病理检测、报告生成和视觉问答任务中表现优异，优于现有多模态模型，并获放射科医生认可。

Conclusion: 验证了通过结构化视觉信息和领域知识整合，利用LLM实现医学图像AI的新范式。

Abstract: Automated interpretation of chest X-rays (CXR) is a critical task with the
potential to significantly improve clinical workflow and patient care. While
recent advances in multimodal foundation models have shown promise, effectively
leveraging the full power of large language models (LLMs) for this visual task
remains an underexplored area. This paper introduces CXR-TextInter, a novel
framework that repurposes powerful text-centric LLMs for CXR interpretation by
operating solely on a rich, structured textual representation of the image
content, generated by an upstream image analysis pipeline. We augment this
LLM-centric approach with an integrated medical knowledge module to enhance
clinical reasoning. To facilitate training and evaluation, we developed the
MediInstruct-CXR dataset, containing structured image representations paired
with diverse, clinically relevant instruction-response examples, and the
CXR-ClinEval benchmark for comprehensive assessment across various
interpretation tasks. Extensive experiments on CXR-ClinEval demonstrate that
CXR-TextInter achieves state-of-the-art quantitative performance across
pathology detection, report generation, and visual question answering,
surpassing existing multimodal foundation models. Ablation studies confirm the
critical contribution of the knowledge integration module. Furthermore, blinded
human evaluation by board-certified radiologists shows a significant preference
for the clinical quality of outputs generated by CXR-TextInter. Our work
validates an alternative paradigm for medical image AI, showcasing the
potential of harnessing advanced LLM capabilities when visual information is
effectively structured and domain knowledge is integrated.

</details>

### [275] [Vision and Intention Boost Large Language Model in Long-Term Action Anticipation](https://arxiv.org/abs/2505.01713)
*Congqi Cao,Lanshu Hu,Yating Yu,Yanning Zhang*

Main category: cs.CV

TLDR: 提出了一种结合视觉和语言模型的多模态方法（ICVL），通过推断行为意图增强视觉表示，显著提升了长期动作预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖视频数据或文本输入，存在信息不足或丢失的问题，需要一种结合视觉和语言优势的方法。

Method: 使用视觉语言模型（VLM）从视频推断行为意图，通过多模态融合增强视觉表示，再结合LLM进行动作预测。

Result: 在多个数据集（Ego4D、EPIC-Kitchens-55、EGTEA GAZE+）上取得了最先进的性能。

Conclusion: ICVL模型通过多模态融合和意图推断，显著提升了长期动作预测的准确性和鲁棒性。

Abstract: Long-term action anticipation (LTA) aims to predict future actions over an
extended period. Previous approaches primarily focus on learning exclusively
from video data but lack prior knowledge. Recent researches leverage large
language models (LLMs) by utilizing text-based inputs which suffer severe
information loss. To tackle these limitations single-modality methods face, we
propose a novel Intention-Conditioned Vision-Language (ICVL) model in this
study that fully leverages the rich semantic information of visual data and the
powerful reasoning capabilities of LLMs. Considering intention as a high-level
concept guiding the evolution of actions, we first propose to employ a
vision-language model (VLM) to infer behavioral intentions as comprehensive
textual features directly from video inputs. The inferred intentions are then
fused with visual features through a multi-modality fusion strategy, resulting
in intention-enhanced visual representations. These enhanced visual
representations, along with textual prompts, are fed into LLM for future action
anticipation. Furthermore, we propose an effective example selection strategy
jointly considers visual and textual similarities, providing more relevant and
informative examples for in-context learning. Extensive experiments with
state-of-the-art performance on Ego4D, EPIC-Kitchens-55, and EGTEA GAZE+
datasets fully demonstrate the effectiveness and superiority of the proposed
method.

</details>

### [276] [Probabilistic Interactive 3D Segmentation with Hierarchical Neural Processes](https://arxiv.org/abs/2505.01726)
*Jie Liu,Pan Zhou,Zehao Xiao,Jiayi Shen,Wenzhe Yin,Jan-Jakob Sonke,Efstratios Gavves*

Main category: cs.CV

TLDR: NPISeg3D是一个基于神经过程的概率框架，通过分层潜在变量和概率原型调制器，解决了3D交互式分割中稀疏点击泛化和预测不确定性量化的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决3D交互式分割中稀疏点击泛化不足和预测不确定性量化未充分探索的问题。

Method: 提出NPISeg3D框架，采用分层潜在变量（场景特定和对象特定）和概率原型调制器，增强少样本泛化和不确定性量化。

Result: 在四个3D点云数据集上，NPISeg3D以更少的点击实现了更优的分割性能，并提供可靠的预测不确定性估计。

Conclusion: NPISeg3D通过分层潜在变量和概率原型调制器，有效提升了3D交互式分割的性能和可靠性。

Abstract: Interactive 3D segmentation has emerged as a promising solution for
generating accurate object masks in complex 3D scenes by incorporating
user-provided clicks. However, two critical challenges remain underexplored:
(1) effectively generalizing from sparse user clicks to produce accurate
segmentation, and (2) quantifying predictive uncertainty to help users identify
unreliable regions. In this work, we propose NPISeg3D, a novel probabilistic
framework that builds upon Neural Processes (NPs) to address these challenges.
Specifically, NPISeg3D introduces a hierarchical latent variable structure with
scene-specific and object-specific latent variables to enhance few-shot
generalization by capturing both global context and object-specific
characteristics. Additionally, we design a probabilistic prototype modulator
that adaptively modulates click prototypes with object-specific latent
variables, improving the model's ability to capture object-aware context and
quantify predictive uncertainty. Experiments on four 3D point cloud datasets
demonstrate that NPISeg3D achieves superior segmentation performance with fewer
clicks while providing reliable uncertainty estimations.

</details>

### [277] [PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth](https://arxiv.org/abs/2505.01729)
*Bu Jin,Weize Li,Baihan Yang,Zhenxin Zhu,Junpeng Jiang,Huan-ang Gao,Haiyang Sun,Kun Zhan,Hengtong Hu,Xueyang Zhang,Peng Jia,Hao Zhao*

Main category: cs.CV

TLDR: PosePilot是一个轻量级框架，通过自监督深度估计增强生成世界模型中的相机姿态控制，提升视角变换和场景动态模拟的准确性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中，相机姿态的精确控制是实现准确视角变换和场景动态模拟的关键挑战。

Method: PosePilot利用自监督深度估计和结构运动原理，结合深度和姿态读取，通过光度量损失和反向变形步骤优化姿态估计。

Result: 实验表明，PosePilot显著提升了扩散式和自回归世界模型中的结构理解和运动推理能力。

Conclusion: PosePilot通过自监督深度控制相机姿态，为生成世界模型中的视角合成设定了新标准。

Abstract: Recent advancements in autonomous driving (AD) systems have highlighted the
potential of world models in achieving robust and generalizable performance
across both ordinary and challenging driving conditions. However, a key
challenge remains: precise and flexible camera pose control, which is crucial
for accurate viewpoint transformation and realistic simulation of scene
dynamics. In this paper, we introduce PosePilot, a lightweight yet powerful
framework that significantly enhances camera pose controllability in generative
world models. Drawing inspiration from self-supervised depth estimation,
PosePilot leverages structure-from-motion principles to establish a tight
coupling between camera pose and video generation. Specifically, we incorporate
self-supervised depth and pose readouts, allowing the model to infer depth and
relative camera motion directly from video sequences. These outputs drive
pose-aware frame warping, guided by a photometric warping loss that enforces
geometric consistency across synthesized frames. To further refine camera pose
estimation, we introduce a reverse warping step and a pose regression loss,
improving viewpoint precision and adaptability. Extensive experiments on
autonomous driving and general-domain video datasets demonstrate that PosePilot
significantly enhances structural understanding and motion reasoning in both
diffusion-based and auto-regressive world models. By steering camera pose with
self-supervised depth, PosePilot sets a new benchmark for pose controllability,
enabling physically consistent, reliable viewpoint synthesis in generative
world models.

</details>

### [278] [Learning Multi-frame and Monocular Prior for Estimating Geometry in Dynamic Scenes](https://arxiv.org/abs/2505.01737)
*Seong Hyeon Park,Jinwoo Shin*

Main category: cs.CV

TLDR: MMP模型通过前馈方式估计动态场景的3D几何，引入轨迹编码模块提升动态点图表示的质量，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决动态场景中3D几何估计的挑战，现有方法仅能预测部分属性且依赖全局优化，导致高计算成本和潜在失败。

Method: 基于Siamese架构，引入轨迹编码模块，动态点图表示随时间演化。

Result: MMP在动态点图预测中达到最先进水平，回归误差降低15.1%。

Conclusion: MMP提供了一种高效且高质量的前馈动态场景几何估计方法。

Abstract: In monocular videos that capture dynamic scenes, estimating the 3D geometry
of video contents has been a fundamental challenge in computer vision.
Specifically, the task is significantly challenged by the object motion, where
existing models are limited to predict only partial attributes of the dynamic
scenes, such as depth or pointmaps spanning only over a pair of frames. Since
these attributes are inherently noisy under multiple frames, test-time global
optimizations are often employed to fully recover the geometry, which is liable
to failure and incurs heavy inference costs. To address the challenge, we
present a new model, coined MMP, to estimate the geometry in a feed-forward
manner, which produces a dynamic pointmap representation that evolves over
multiple frames. Specifically, based on the recent Siamese architecture, we
introduce a new trajectory encoding module to project point-wise dynamics on
the representation for each frame, which can provide significantly improved
expressiveness for dynamic scenes. In our experiments, we find MMP can achieve
state-of-the-art quality in feed-forward pointmap prediction, e.g., 15.1%
enhancement in the regression error.

</details>

### [279] [An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding](https://arxiv.org/abs/2505.01743)
*Siyang Jiang,Bufang Yang,Lilin Xu,Mu Yuan,Yeerzhati Abudunuer,Kaiwei Liu,Liekang Zeng,Hongkai Chen,Zhenyu Yan,Xiaofan Jiang,Guoliang Xing*

Main category: cs.CV

TLDR: 论文提出了一种名为Llambda的系统，旨在利用有限标注数据和大量未标注数据，通过生成高质量伪标签和视频描述，优化大型视觉语言模型（LVLM）对低分辨率视频的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型（LVLM）主要针对高分辨率数据设计，难以有效理解低分辨率数据（如深度、热成像和红外数据）。传统标注方法耗时耗力，亟需一种更高效的解决方案。

Method: 1. 提出对比导向数据标注器，通过对比学习生成高质量伪标签。2. 设计物理知识引导的标注器，利用时空一致性检查减少伪标签错误。3. 采用LoRA高效微调技术，确保模型在设备上的可部署性。

Result: 实验表明，Llambda在区域级真实测试平台和三种低分辨率数据集上表现优异，平均Bert-Score超越现有最佳LVLM系统达40.03%。

Conclusion: Llambda通过结合伪标签生成和高效微调，显著提升了LVLM对低分辨率视频的理解能力，为低分辨率人类行为理解提供了高效解决方案。

Abstract: The rapid advancements in Large Vision Language Models (LVLMs) offer the
potential to surpass conventional labeling by generating richer, more detailed
descriptions of on-device human behavior understanding (HBU) in low-resolution
vision systems, such as depth, thermal, and infrared. However, existing large
vision language model (LVLM) approaches are unable to understand low-resolution
data well as they are primarily designed for high-resolution data, such as RGB
images. A quick fixing approach is to caption a large amount of low-resolution
data, but it requires a significant amount of labor-intensive annotation
efforts. In this paper, we propose a novel, labor-saving system, Llambda,
designed to support low-resolution HBU. The core idea is to leverage limited
labeled data and a large amount of unlabeled data to guide LLMs in generating
informative captions, which can be combined with raw data to effectively
fine-tune LVLM models for understanding low-resolution videos in HBU. First, we
propose a Contrastive-Oriented Data Labeler, which can capture
behavior-relevant information from long, low-resolution videos and generate
high-quality pseudo labels for unlabeled data via contrastive learning. Second,
we propose a Physical-Knowledge Guided Captioner, which utilizes spatial and
temporal consistency checks to mitigate errors in pseudo labels. Therefore, it
can improve LLMs' understanding of sequential data and then generate
high-quality video captions. Finally, to ensure on-device deployability, we
employ LoRA-based efficient fine-tuning to adapt LVLMs for low-resolution data.
We evaluate Llambda using a region-scale real-world testbed and three distinct
low-resolution datasets, and the experiments show that Llambda outperforms
several state-of-the-art LVLM systems up to $40.03\%$ on average Bert-Score.

</details>

### [280] [Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion](https://arxiv.org/abs/2505.01746)
*Xingqun Qi,Yatian Wang,Hengyuan Zhang,Jiahao Pan,Wei Xue,Shanghang Zhang,Wenhan Luo,Qifeng Liu,Yike Guo*

Main category: cs.CV

TLDR: 论文提出了一种新框架Co$^3$Gesture，用于生成两人交互对话中的同步手势，并构建了一个大规模数据集GES-Inter。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅支持单人自说自话的手势合成，忽略了两人交互对话中同步手势的实用性，且缺乏高质量数据集。

Method: 提出Co$^3$Gesture框架，包含两个协同生成分支和一个时序交互模块（TIM），通过互注意力机制增强交互手势的协调性。

Result: 实验表明，该方法在GES-Inter数据集上优于现有模型，能生成生动且连贯的交互手势。

Conclusion: 论文填补了两人交互手势合成的空白，提供了数据集和开源代码，推动了虚拟角色动画的发展。

Abstract: Generating gestures from human speech has gained tremendous progress in
animating virtual avatars. While the existing methods enable synthesizing
gestures cooperated by individual self-talking, they overlook the practicality
of concurrent gesture modeling with two-person interactive conversations.
Moreover, the lack of high-quality datasets with concurrent co-speech gestures
also limits handling this issue. To fulfill this goal, we first construct a
large-scale concurrent co-speech gesture dataset that contains more than 7M
frames for diverse two-person interactive posture sequences, dubbed GES-Inter.
Additionally, we propose Co$^3$Gesture, a novel framework that enables coherent
concurrent co-speech gesture synthesis including two-person interactive
movements. Considering the asymmetric body dynamics of two speakers, our
framework is built upon two cooperative generation branches conditioned on
separated speaker audio. Specifically, to enhance the coordination of human
postures with respect to corresponding speaker audios while interacting with
the conversational partner, we present a Temporal Interaction Module (TIM). TIM
can effectively model the temporal association representation between two
speakers' gesture sequences as interaction guidance and fuse it into the
concurrent gesture generation. Then, we devise a mutual attention mechanism to
further holistically boost learning dependencies of interacted concurrent
motions, thereby enabling us to generate vivid and coherent gestures. Extensive
experiments demonstrate that our method outperforms the state-of-the-art models
on our newly collected GES-Inter dataset. The dataset and source code are
publicly available at
\href{https://mattie-e.github.io/Co3/}{\textit{https://mattie-e.github.io/Co3/}}.

</details>

### [281] [Multimodal Graph Representation Learning for Robust Surgical Workflow Recognition with Adversarial Feature Disentanglement](https://arxiv.org/abs/2505.01766)
*Long Bai,Boyi Ma,Ruohan Wang,Guankun Wang,Beilei Cui,Zhongliang Jiang,Mobarakol Islam,Zhe Min,Jiewen Lai,Nassir Navab,Hongliang Ren*

Main category: cs.CV

TLDR: 提出了一种基于图的多模态方法（GRAD），结合视觉和运动学数据，提升手术工作流识别的鲁棒性，应对数据损坏和领域偏移。


<details>
  <summary>Details</summary>
Motivation: 手术工作流识别对自动化任务和提升患者安全至关重要，但数据损坏（如遮挡或存储问题）会导致性能下降。

Method: 提出多模态解耦图网络和视觉-运动学对抗框架，通过图建模和对抗训练减少模态差异，并设计上下文校准解码器增强鲁棒性。

Result: 实验表明该方法在数据损坏和领域偏移下表现优异，具有稳定性和鲁棒性。

Conclusion: 该方法为复杂动态手术场景下的工作流识别提供了有效解决方案。

Abstract: Surgical workflow recognition is vital for automating tasks, supporting
decision-making, and training novice surgeons, ultimately improving patient
safety and standardizing procedures. However, data corruption can lead to
performance degradation due to issues like occlusion from bleeding or smoke in
surgical scenes and problems with data storage and transmission. In this case,
we explore a robust graph-based multimodal approach to integrating vision and
kinematic data to enhance accuracy and reliability. Vision data captures
dynamic surgical scenes, while kinematic data provides precise movement
information, overcoming limitations of visual recognition under adverse
conditions. We propose a multimodal Graph Representation network with
Adversarial feature Disentanglement (GRAD) for robust surgical workflow
recognition in challenging scenarios with domain shifts or corrupted data.
Specifically, we introduce a Multimodal Disentanglement Graph Network that
captures fine-grained visual information while explicitly modeling the complex
relationships between vision and kinematic embeddings through graph-based
message modeling. To align feature spaces across modalities, we propose a
Vision-Kinematic Adversarial framework that leverages adversarial training to
reduce modality gaps and improve feature consistency. Furthermore, we design a
Contextual Calibrated Decoder, incorporating temporal and contextual priors to
enhance robustness against domain shifts and corrupted data. Extensive
comparative and ablation experiments demonstrate the effectiveness of our model
and proposed modules. Moreover, our robustness experiments show that our method
effectively handles data corruption during storage and transmission, exhibiting
excellent stability and robustness. Our approach aims to advance automated
surgical workflow recognition, addressing the complexities and dynamism
inherent in surgical procedures.

</details>

### [282] [Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos](https://arxiv.org/abs/2505.01790)
*Markos Stamatakis,Joshua Berger,Christian Wartena,Ralph Ewerth,Anett Hoppe*

Main category: cs.CV

TLDR: 研究探讨了视觉语言模型在教育视频中生成学习导向问题的能力，评估了现成模型的表现、微调效果、视频模态对问题质量的影响，并定性分析了问题的相关性、可答性和难度。


<details>
  <summary>Details</summary>
Motivation: 提升教育视频的用户参与度和知识保留率是一个挑战，自动生成问题可以激活学习者并支持知识获取，同时帮助教师和学习者评估理解程度。

Method: 研究评估了现成视觉语言模型的性能、微调对内容特定问题生成的影响、不同视频模态对问题质量的作用，并进行了定性研究。

Result: 研究揭示了当前视觉语言模型的能力，指出微调的必要性，并提出了问题多样性和相关性方面的挑战。

Conclusion: 研究为未来多模态数据集的需求提供了方向，并概述了有前景的研究方向。

Abstract: Web-based educational videos offer flexible learning opportunities and are
becoming increasingly popular. However, improving user engagement and knowledge
retention remains a challenge. Automatically generated questions can activate
learners and support their knowledge acquisition. Further, they can help
teachers and learners assess their understanding. While large language and
vision-language models have been employed in various tasks, their application
to question generation for educational videos remains underexplored. In this
paper, we investigate the capabilities of current vision-language models for
generating learning-oriented questions for educational video content. We assess
(1) out-of-the-box models' performance; (2) fine-tuning effects on
content-specific question generation; (3) the impact of different video
modalities on question quality; and (4) in a qualitative study, question
relevance, answerability, and difficulty levels of generated questions. Our
findings delineate the capabilities of current vision-language models,
highlighting the need for fine-tuning and addressing challenges in question
diversity and relevance. We identify requirements for future multimodal
datasets and outline promising research directions.

</details>

### [283] [AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian Splatting](https://arxiv.org/abs/2505.01799)
*Junhao Shi,Jisheng Xu,Jianping He,Zhiliang Lin*

Main category: cs.CV

TLDR: AquaGS是一种基于SeaThru算法的水下场景重建模型，通过结合多视角立体技术和3D高斯溅射技术，快速分离场景细节和介质特征，解决了传统方法在实时性和精度上的不足。


<details>
  <summary>Details</summary>
Motivation: 水下图像因介质干扰质量下降，传统SfM方法速度慢且易失败，限制了水下操作的实时性和精度。

Method: 结合多视角立体技术初始化高斯分布，使用隐式NeRF渲染透明介质，并利用3D高斯溅射技术渲染物体表面。

Result: 实验表明，仅需3张输入图像即可在30秒内完成高精度重建，显著提升算法在机器人平台上的实用性。

Conclusion: AquaGS通过创新技术解决了水下重建的实时性和精度问题，具有广泛的应用潜力。

Abstract: Underwater scene reconstruction is a critical tech-nology for underwater
operations, enabling the generation of 3D models from images captured by
underwater platforms. However, the quality of underwater images is often
degraded due to medium interference, which limits the effectiveness of
Structure-from-Motion (SfM) pose estimation, leading to subsequent
reconstruction failures. Additionally, SfM methods typically operate at slower
speeds, further hindering their applicability in real-time scenarios. In this
paper, we introduce AquaGS, an SfM-free underwater scene reconstruction model
based on the SeaThru algorithm, which facilitates rapid and accurate separation
of scene details and medium features. Our approach initializes Gaussians by
integrating state-of-the-art multi-view stereo (MVS) technology, employs
implicit Neural Radiance Fields (NeRF) for rendering translucent media and
utilizes the latest explicit 3D Gaussian Splatting (3DGS) technique to render
object surfaces, which effectively addresses the limitations of traditional
methods and accurately simulates underwater optical phenomena. Experimental
results on the data set and the robot platform show that our model can complete
high-precision reconstruction in 30 seconds with only 3 image inputs,
significantly enhancing the practical application of the algorithm in robotic
platforms.

</details>

### [284] [Efficient 3D Full-Body Motion Generation from Sparse Tracking Inputs with Temporal Windows](https://arxiv.org/abs/2505.01802)
*Georgios Fotios Angelis,Savas Ozkan,Sinan Mutlu,Paul Wisbey,Anastasios Drosou,Mete Ozay*

Main category: cs.CV

TLDR: 提出了一种基于MLP的高效3D全身重建方法，通过分割长输入序列为小时间窗口，显著提升生成精度并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络模型计算成本高且长序列输入引入噪声，影响3D全身重建性能。

Method: 采用MLP方法，将长输入序列分割为小时间窗口，并通过潜在表示融合当前运动与历史信息。

Result: 实验表明，该方法显著提升生成精度，同时大幅降低计算成本和内存开销。

Conclusion: 该方法适用于资源受限设备，为AR/VR应用提供了高效的3D全身生成解决方案。

Abstract: To have a seamless user experience on immersive AR/VR applications, the
importance of efficient and effective Neural Network (NN) models is undeniable,
since missing body parts that cannot be captured by limited sensors should be
generated using these models for a complete 3D full-body reconstruction in
virtual environment. However, the state-of-the-art NN-models are typically
computational expensive and they leverage longer sequences of sparse tracking
inputs to generate full-body movements by capturing temporal context.
Inevitably, longer sequences increase the computation overhead and introduce
noise in longer temporal dependencies that adversely affect the generation
performance. In this paper, we propose a novel Multi-Layer Perceptron
(MLP)-based method that enhances the overall performance while balancing the
computational cost and memory overhead for efficient 3D full-body generation.
Precisely, we introduce a NN-mechanism that divides the longer sequence of
inputs into smaller temporal windows. Later, the current motion is merged with
the information from these windows through latent representations to utilize
the past context for the generation. Our experiments demonstrate that
generation accuracy of our method with this NN-mechanism is significantly
improved compared to the state-of-the-art methods while greatly reducing
computational costs and memory overhead, making our method suitable for
resource-constrained devices.

</details>

### [285] [Not Every Tree Is a Forest: Benchmarking Forest Types from Satellite Remote Sensing](https://arxiv.org/abs/2505.01805)
*Yuchang Jiang,Maxim Neumann*

Main category: cs.CV

TLDR: ForTy是一个全球森林类型分类的基准数据集，结合多时相卫星数据，支持图像分割任务，并区分三种森林类型。提出的新型Transformer模型性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 开发精确可靠的森林类型分类模型，以支持阻止森林砍伐和生物多样性保护（如欧盟森林砍伐法规）。

Method: 利用Sentinel-2、Sentinel-1、气候和海拔数据构建200,000个时间序列图像块，提出新型Transformer模型处理多模态、多时相数据。

Result: 实验结果表明，提出的Transformer模型性能优于基线卷积神经网络和Transformer模型。

Conclusion: ForTy基准数据集和新型模型为全球森林类型分类提供了有效工具，支持环境保护政策。

Abstract: Developing accurate and reliable models for forest types mapping is critical
to support efforts for halting deforestation and for biodiversity conservation
(such as European Union Deforestation Regulation (EUDR)). This work introduces
ForTy, a benchmark for global-scale FORest TYpes mapping using multi-temporal
satellite data1. The benchmark comprises 200,000 time series of image patches,
each consisting of Sentinel-2, Sentinel-1, climate, and elevation data. Each
time series captures variations at monthly or seasonal cadence. Per-pixel
annotations, including forest types and other land use classes, support image
segmentation tasks. Unlike most existing land use products that often
categorize all forest areas into a single class, our benchmark differentiates
between three forest types classes: natural forest, planted forest, and tree
crops. By leveraging multiple public data sources, we achieve global coverage
with this benchmark. We evaluate the forest types dataset using several
baseline models, including convolution neural networks and transformer-based
models. Additionally, we propose a novel transformer-based model specifically
designed to handle multi-modal, multi-temporal satellite data for forest types
mapping. Our experimental results demonstrate that the proposed model surpasses
the baseline models in performance.

</details>

### [286] [3DWG: 3D Weakly Supervised Visual Grounding via Category and Instance-Level Alignment](https://arxiv.org/abs/2505.01809)
*Xiaoqi Li,Jiaming Liu,Nuowei Han,Liang Heng,Yandong Guo,Hao Dong,Yang Liu*

Main category: cs.CV

TLDR: 论文提出了一种弱监督的3D视觉定位方法，通过区分类别和实例来解决点云中类别模糊和实例复杂的问题，并在多个基准测试中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D点云中基于自然语言描述的弱监督视觉定位任务中的类别模糊和实例复杂问题。

Method: 提出了一种双分支方法：类别级分支利用预训练检测器的知识增强类别感知，实例级分支利用空间关系描述细化实例特征。

Result: 在Nr3D、Sr3D和ScanRef三个基准测试中达到了最先进的性能。

Conclusion: 该方法通过明确区分类别和实例，有效提升了弱监督3D视觉定位的准确性。

Abstract: The 3D weakly-supervised visual grounding task aims to localize oriented 3D
boxes in point clouds based on natural language descriptions without requiring
annotations to guide model learning. This setting presents two primary
challenges: category-level ambiguity and instance-level complexity.
Category-level ambiguity arises from representing objects of fine-grained
categories in a highly sparse point cloud format, making category distinction
challenging. Instance-level complexity stems from multiple instances of the
same category coexisting in a scene, leading to distractions during grounding.
To address these challenges, we propose a novel weakly-supervised grounding
approach that explicitly differentiates between categories and instances. In
the category-level branch, we utilize extensive category knowledge from a
pre-trained external detector to align object proposal features with
sentence-level category features, thereby enhancing category awareness. In the
instance-level branch, we utilize spatial relationship descriptions from
language queries to refine object proposal features, ensuring clear
differentiation among objects. These designs enable our model to accurately
identify target-category objects while distinguishing instances within the same
category. Compared to previous methods, our approach achieves state-of-the-art
performance on three widely used benchmarks: Nr3D, Sr3D, and ScanRef.

</details>

### [287] [PhytoSynth: Leveraging Multi-modal Generative Models for Crop Disease Data Generation with Novel Benchmarking and Prompt Engineering Approach](https://arxiv.org/abs/2505.01823)
*Nitin Rai,Arnold W. Schumann,Nathan Boyd*

Main category: cs.CV

TLDR: 该研究探索了一种多模态文本到图像的方法，用于生成合成作物病害图像，并首次提供了计算性能的基准测试。SD3.5M在性能和效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 大规模收集田间作物病害图像耗时耗力，生成模型提供了一种替代方案，但现有研究缺乏对农业领域计算需求的全面分析。

Method: 训练了三种Stable Diffusion变体（SDXL、SD3.5M、SD3.5L），并使用Dreambooth和LoRA微调技术提升泛化能力。

Result: SD3.5M表现最优，平均内存使用18 GB，功耗180 W，每500张图像能耗1.02 kWh（每张0.002 kWh），可在1.5小时内从36张田间样本生成500张合成图像。

Conclusion: 推荐使用SD3.5M进行高效的作物病害数据生成。

Abstract: Collecting large-scale crop disease images in the field is labor-intensive
and time-consuming. Generative models (GMs) offer an alternative by creating
synthetic samples that resemble real-world images. However, existing research
primarily relies on Generative Adversarial Networks (GANs)-based image-to-image
translation and lack a comprehensive analysis of computational requirements in
agriculture. Therefore, this research explores a multi-modal text-to-image
approach for generating synthetic crop disease images and is the first to
provide computational benchmarking in this context. We trained three Stable
Diffusion (SD) variants-SDXL, SD3.5M (medium), and SD3.5L (large)-and
fine-tuned them using Dreambooth and Low-Rank Adaptation (LoRA) fine-tuning
techniques to enhance generalization. SD3.5M outperformed the others, with an
average memory usage of 18 GB, power consumption of 180 W, and total energy use
of 1.02 kWh/500 images (0.002 kWh per image) during inference task. Our results
demonstrate SD3.5M's ability to generate 500 synthetic images from just 36
in-field samples in 1.5 hours. We recommend SD3.5M for efficient crop disease
data generation.

</details>

### [288] [CVVNet: A Cross-Vertical-View Network for Gait Recognition](https://arxiv.org/abs/2505.01837)
*Xiangru Li,Wei Song,Yingda Huang,Wei Meng,Le Chang*

Main category: cs.CV

TLDR: CVVNet是一种针对跨垂直视角步态识别设计的网络，通过多尺度频率特征提取和动态门控聚合，显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨垂直视角场景中表现不佳，因视角变化导致特征变形和遮挡，识别精度下降高达60%。

Method: 提出CVVNet，包含高-低频率提取模块（HLFE）和动态门控聚合（DGA），结合多尺度注意力门控聚合（MSAGA）模块。

Result: 在DroneGait和Gait3D数据集上分别提升8.6%和2%，达到最优性能。

Conclusion: CVVNet有效解决了跨垂直视角步态识别的挑战，显著提升了识别鲁棒性。

Abstract: Gait recognition enables contact-free, long-range person identification that
is robust to clothing variations and non-cooperative scenarios. While existing
methods perform well in controlled indoor environments, they struggle with
cross-vertical view scenarios, where surveillance angles vary significantly in
elevation. Our experiments show up to 60\% accuracy degradation in low-to-high
vertical view settings due to severe deformations and self-occlusions of key
anatomical features. Current CNN and self-attention-based methods fail to
effectively handle these challenges, due to their reliance on single-scale
convolutions or simplistic attention mechanisms that lack effective
multi-frequency feature integration. To tackle this challenge, we propose
CVVNet (Cross-Vertical-View Network), a frequency aggregation architecture
specifically designed for robust cross-vertical-view gait recognition. CVVNet
employs a High-Low Frequency Extraction module (HLFE) that adopts parallel
multi-scale convolution/max-pooling path and self-attention path as high- and
low-frequency mixers for effective multi-frequency feature extraction from
input silhouettes. We also introduce the Dynamic Gated Aggregation (DGA)
mechanism to adaptively adjust the fusion ratio of high- and low-frequency
features. The integration of our core Multi-Scale Attention Gated Aggregation
(MSAGA) module, HLFE and DGA enables CVVNet to effectively handle distortions
from view changes, significantly improving the recognition robustness across
different vertical views. Experimental results show that our CVVNet achieves
state-of-the-art performance, with $8.6\%$ improvement on DroneGait and $2\%$
on Gait3D compared with the best existing methods.

</details>

### [289] [MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization](https://arxiv.org/abs/2505.01838)
*Chenghong Li,Hongjie Liao,Yihao Zhi,Xihe Yang,Zhengwentai Sun,Jiahao Chang,Shuguang Cui,Xiaoguang Han*

Main category: cs.CV

TLDR: MVHumanNet++是一个大规模多视角人类动作数据集，包含4500个身份、9000套日常服装和60000个动作序列，旨在推动以人为中心的3D视觉研究。


<details>
  <summary>Details</summary>
Motivation: 当前3D视觉领域缺乏大规模人类数据集，限制了以人为中心任务的发展。MVHumanNet++旨在填补这一空白。

Method: 通过多视角人类捕捉系统收集多样化身份和日常服装的数据，并提供丰富的标注（如掩码、关键点、SMPL参数等）。

Result: 数据集包含645百万帧，并扩展了法线图和深度图，显著提升了其在高级研究中的实用性。

Conclusion: MVHumanNet++是目前最大规模的3D人类数据集，有望推动以人为中心的3D任务创新。

Abstract: In this era, the success of large language models and text-to-image models
can be attributed to the driving force of large-scale datasets. However, in the
realm of 3D vision, while significant progress has been achieved in
object-centric tasks through large-scale datasets like Objaverse and MVImgNet,
human-centric tasks have seen limited advancement, largely due to the absence
of a comparable large-scale human dataset. To bridge this gap, we present
MVHumanNet++, a dataset that comprises multi-view human action sequences of
4,500 human identities. The primary focus of our work is on collecting human
data that features a large number of diverse identities and everyday clothing
using multi-view human capture systems, which facilitates easily scalable data
collection. Our dataset contains 9,000 daily outfits, 60,000 motion sequences
and 645 million frames with extensive annotations, including human masks,
camera parameters, 2D and 3D keypoints, SMPL/SMPLX parameters, and
corresponding textual descriptions. Additionally, the proposed MVHumanNet++
dataset is enhanced with newly processed normal maps and depth maps,
significantly expanding its applicability and utility for advanced
human-centric research. To explore the potential of our proposed MVHumanNet++
dataset in various 2D and 3D visual tasks, we conducted several pilot studies
to demonstrate the performance improvements and effective applications enabled
by the scale provided by MVHumanNet++. As the current largest-scale 3D human
dataset, we hope that the release of MVHumanNet++ dataset with annotations will
foster further innovations in the domain of 3D human-centric tasks at scale.
MVHumanNet++ is publicly available at
https://kevinlee09.github.io/research/MVHumanNet++/.

</details>

### [290] [Mitigating Group-Level Fairness Disparities in Federated Visual Language Models](https://arxiv.org/abs/2505.01851)
*Chaomeng Chen,Zitong Yu,Junhao Dong,Sen Su,Linlin Shen,Shutao Xia,Xiaochun Cao*

Main category: cs.CV

TLDR: 论文提出FVL-FP框架，结合联邦学习与公平提示调优，解决视觉语言模型在联邦学习环境中的群体公平性问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多模态任务中表现优异，但在联邦学习环境中难以保持跨人口群体的公平性。

Method: 提出三个创新组件：CDFP（跨层人口公平提示）、DSOP（人口子空间正交投影）和FPF（公平感知提示融合）。

Result: 在四个基准数据集上，平均减少45%的人口差异，任务性能保持在最优结果的6%以内。

Conclusion: FVL-FP为隐私保护多模态系统中确保跨人口群体公平性能提供了高效解决方案。

Abstract: Visual language models (VLMs) have shown remarkable capabilities in
multimodal tasks but face challenges in maintaining fairness across demographic
groups, particularly when deployed in federated learning (FL) environments.
This paper addresses the critical issue of group fairness in federated VLMs by
introducing FVL-FP, a novel framework that combines FL with fair prompt tuning
techniques. We focus on mitigating demographic biases while preserving model
performance through three innovative components: (1) Cross-Layer Demographic
Fair Prompting (CDFP), which adjusts potentially biased embeddings through
counterfactual regularization; (2) Demographic Subspace Orthogonal Projection
(DSOP), which removes demographic bias in image representations by mapping fair
prompt text to group subspaces; and (3) Fair-aware Prompt Fusion (FPF), which
dynamically balances client contributions based on both performance and
fairness metrics. Extensive evaluations across four benchmark datasets
demonstrate that our approach reduces demographic disparity by an average of
45\% compared to standard FL approaches, while maintaining task performance
within 6\% of state-of-the-art results. FVL-FP effectively addresses the
challenges of non-IID data distributions in federated settings and introduces
minimal computational overhead while providing significant fairness benefits.
Our work presents a parameter-efficient solution to the critical challenge of
ensuring equitable performance across demographic groups in privacy-preserving
multimodal systems.

</details>

### [291] [DualDiff: Dual-branch Diffusion Model for Autonomous Driving with Semantic Fusion](https://arxiv.org/abs/2505.01857)
*Haoteng Li,Zhao Yang,Zezhong Qian,Gongpeng Zhao,Yuqi Huang,Jun Yu,Huazheng Zhou,Longjun Liu*

Main category: cs.CV

TLDR: DualDiff是一种双分支条件扩散模型，通过Occupancy Ray Sampling和Semantic Fusion Attention提升多视角驾驶场景生成的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅使用3D边界框和二值图控制前景和背景，难以捕捉场景复杂性并整合多模态信息。

Method: 提出DualDiff模型，结合ORS语义丰富的3D表示和SFA机制，设计FGM损失优化小物体生成。

Result: 在FID分数上达到SOTA，并在BEV分割和3D物体检测任务中表现更优。

Conclusion: DualDiff通过多模态特征融合和精细控制，显著提升了驾驶场景重建的准确性和保真度。

Abstract: Accurate and high-fidelity driving scene reconstruction relies on fully
leveraging scene information as conditioning. However, existing approaches,
which primarily use 3D bounding boxes and binary maps for foreground and
background control, fall short in capturing the complexity of the scene and
integrating multi-modal information. In this paper, we propose DualDiff, a
dual-branch conditional diffusion model designed to enhance multi-view driving
scene generation. We introduce Occupancy Ray Sampling (ORS), a semantic-rich 3D
representation, alongside numerical driving scene representation, for
comprehensive foreground and background control. To improve cross-modal
information integration, we propose a Semantic Fusion Attention (SFA) mechanism
that aligns and fuses features across modalities. Furthermore, we design a
foreground-aware masked (FGM) loss to enhance the generation of tiny objects.
DualDiff achieves state-of-the-art performance in FID score, as well as
consistently better results in downstream BEV segmentation and 3D object
detection tasks.

</details>

### [292] [Visual enhancement and 3D representation for underwater scenes: a review](https://arxiv.org/abs/2505.01869)
*Guoxi Huang,Haoran Wang,Brett Seymour,Evan Kovacs,John Ellerbrock,Dave Blackham,Nantheera Anantrasirichai*

Main category: cs.CV

TLDR: 本文对水下视觉增强（UVE）和3D重建进行了系统综述，涵盖物理模型、先进方法及评估，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 水下环境的复杂成像条件对计算机视觉和AI任务提出了挑战，但目前缺乏对UVE和3D重建的全面综述。

Method: 综述了从非学习方法到数据驱动技术（如NeRF和3D高斯泼溅）的多种方法，并评估其效果。

Result: 通过定量和定性评估，展示了现有算法在多个基准数据集上的表现。

Conclusion: 总结了当前研究的局限性，并提出了水下视觉领域的未来发展方向。

Abstract: Underwater visual enhancement (UVE) and underwater 3D reconstruction pose
significant challenges in
  computer vision and AI-based tasks due to complex imaging conditions in
aquatic environments. Despite
  the development of numerous enhancement algorithms, a comprehensive and
systematic review covering both
  UVE and underwater 3D reconstruction remains absent. To advance research in
these areas, we present an
  in-depth review from multiple perspectives. First, we introduce the
fundamental physical models, highlighting the
  peculiarities that challenge conventional techniques. We survey advanced
methods for visual enhancement and
  3D reconstruction specifically designed for underwater scenarios. The paper
assesses various approaches from
  non-learning methods to advanced data-driven techniques, including Neural
Radiance Fields and 3D Gaussian
  Splatting, discussing their effectiveness in handling underwater distortions.
Finally, we conduct both quantitative
  and qualitative evaluations of state-of-the-art UVE and underwater 3D
reconstruction algorithms across multiple
  benchmark datasets. Finally, we highlight key research directions for future
advancements in underwater vision.

</details>

### [293] [PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications](https://arxiv.org/abs/2505.01881)
*Trisanth Srinivasan,Santosh Patapati*

Main category: cs.CV

TLDR: PhysNav-DG框架结合传感器融合与视觉语言模型，通过双分支架构实现导航动作预测与解释生成，改进导航成功率20%以上。


<details>
  <summary>Details</summary>
Motivation: 提升多样化环境中的导航鲁棒性，同时实现透明决策。

Method: 双分支架构结合自适应卡尔曼滤波和视觉语言模型（如LLaMA 3.2 11B和BLIP-2），动态调整噪声参数。

Result: 在MD-NEX Benchmark上，导航成功率提升20%以上，解释清晰且高效。

Conclusion: PhysNav-DG连接语义推理与几何规划，提升自主系统的安全性和可信度。

Abstract: Robust navigation in diverse environments and domains requires both accurate
state estimation and transparent decision making. We present PhysNav-DG, a
novel framework that integrates classical sensor fusion with the semantic power
of vision-language models. Our dual-branch architecture predicts navigation
actions from multi-sensor inputs while simultaneously generating detailed
chain-of-thought explanations. A modified Adaptive Kalman Filter dynamically
adjusts its noise parameters based on environmental context. It leverages
several streams of raw sensor data along with semantic insights from models
such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the
MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation,
autonomous driving, and social navigation tasks with ground-truth actions and
human-validated explanations. Extensive experiments and ablations show that
PhysNav-DG improves navigation success rates by over 20% and achieves high
efficiency, with explanations that are both highly grounded and clear. This
work connects high-level semantic reasoning and geometric planning for safer
and more trustworthy autonomous systems.

</details>

### [294] [CMAWRNet: Multiple Adverse Weather Removal via a Unified Quaternion Neural Architecture](https://arxiv.org/abs/2505.01882)
*Vladimir Frants,Sos Agaian,Karen Panetta,Peter Huang*

Main category: cs.CV

TLDR: CMAWRNet提出了一种基于四元数神经架构的统一方法，用于去除多种恶劣天气条件对图像的影响，通过纹理-结构分解块、轻量级编码器-解码器四元数变换架构和低光校正的注意力融合块实现。


<details>
  <summary>Details</summary>
Motivation: 现实应用中图像常受多种恶劣天气条件（如雾、雨、雪）影响，现有方法难以处理多种退化组合，亟需高效解决方案。

Method: 采用纹理-结构分解块、轻量级四元数变换架构和注意力融合块，引入四元数相似性损失函数以保留颜色信息。

Result: 在基准数据集和真实图像上的定量与定性评估表明，CMAWRNet优于其他现有方法，并能提升下游任务（如目标检测）性能。

Conclusion: CMAWRNet首次将分解方法应用于通用天气去除任务，展示了其在处理多种天气退化组合中的优势。

Abstract: Images used in real-world applications such as image or video retrieval,
outdoor surveillance, and autonomous driving suffer from poor weather
conditions. When designing robust computer vision systems, removing adverse
weather such as haze, rain, and snow is a significant problem. Recently,
deep-learning methods offered a solution for a single type of degradation.
Current state-of-the-art universal methods struggle with combinations of
degradations, such as haze and rain-streak. Few algorithms have been developed
that perform well when presented with images containing multiple adverse
weather conditions. This work focuses on developing an efficient solution for
multiple adverse weather removal using a unified quaternion neural architecture
called CMAWRNet. It is based on a novel texture-structure decomposition block,
a novel lightweight encoder-decoder quaternion transformer architecture, and an
attentive fusion block with low-light correction. We also introduce a
quaternion similarity loss function to preserve color information better. The
quantitative and qualitative evaluation of the current state-of-the-art
benchmarking datasets and real-world images shows the performance advantages of
the proposed CMAWRNet compared to other state-of-the-art weather removal
approaches dealing with multiple weather artifacts. Extensive computer
simulations validate that CMAWRNet improves the performance of downstream
applications such as object detection. This is the first time the decomposition
approach has been applied to the universal weather removal task.

</details>

### [295] [Rethinking Score Distilling Sampling for 3D Editing and Generation](https://arxiv.org/abs/2505.01888)
*Xingyu Miao,Haoran Duan,Yang Long,Jungong Han*

Main category: cs.CV

TLDR: UDS统一了3D生成与编辑的梯度项，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: SDS及其变体在3D生成与编辑任务中存在局限性，无法同时高效完成两者。

Method: 提出UDS方法，通过统一梯度项实现生成与编辑的融合。

Result: UDS在生成和编辑任务中均表现优异，细节更丰富。

Conclusion: UDS填补了3D生成与编辑之间的鸿沟，代码已开源。

Abstract: Score Distillation Sampling (SDS) has emerged as a prominent method for
text-to-3D generation by leveraging the strengths of 2D diffusion models.
However, SDS is limited to generation tasks and lacks the capability to edit
existing 3D assets. Conversely, variants of SDS that introduce editing
capabilities often can not generate new 3D assets effectively. In this work, we
observe that the processes of generation and editing within SDS and its
variants have unified underlying gradient terms. Building on this insight, we
propose Unified Distillation Sampling (UDS), a method that seamlessly
integrates both the generation and editing of 3D assets. Essentially, UDS
refines the gradient terms used in vanilla SDS methods, unifying them to
support both tasks. Extensive experiments demonstrate that UDS not only
outperforms baseline methods in generating 3D assets with richer details but
also excels in editing tasks, thereby bridging the gap between 3D generation
and editing. The code is available on: https://github.com/xingy038/UDS.

</details>

### [296] [GenSync: A Generalized Talking Head Framework for Audio-driven Multi-Subject Lip-Sync using 3D Gaussian Splatting](https://arxiv.org/abs/2505.01928)
*Anushka Agarwal,Muhammad Yusuf Hassan,Talha Chafekar*

Main category: cs.CV

TLDR: GenSync是一个基于3D高斯溅射的多身份唇同步视频合成框架，通过解耦模块实现高效多身份合成，训练速度提升6.8倍。


<details>
  <summary>Details</summary>
Motivation: 现有3D方法通常需要为每个身份训练单独模型，效率低下。

Method: 使用统一网络和解耦模块分离身份特征与音频表示。

Result: 训练速度提升6.8倍，同时保持高唇同步精度和视觉质量。

Conclusion: GenSync在高效多身份唇同步视频合成方面具有显著优势。

Abstract: We introduce GenSync, a novel framework for multi-identity lip-synced video
synthesis using 3D Gaussian Splatting. Unlike most existing 3D methods that
require training a new model for each identity , GenSync learns a unified
network that synthesizes lip-synced videos for multiple speakers. By
incorporating a Disentanglement Module, our approach separates
identity-specific features from audio representations, enabling efficient
multi-identity video synthesis. This design reduces computational overhead and
achieves 6.8x faster training compared to state-of-the-art models, while
maintaining high lip-sync accuracy and visual quality.

</details>

### [297] [GauS-SLAM: Dense RGB-D SLAM with Gaussian Surfels](https://arxiv.org/abs/2505.01934)
*Yongxin Su,Lin Chen,Kaiting Zhang,Zhongliang Zhao,Chenfeng Hou,Ziping Yu*

Main category: cs.CV

TLDR: GauS-SLAM是一种基于2D高斯面元的密集RGB-D SLAM系统，通过改进几何精度和多视角一致性，显著提升了跟踪和映射性能。


<details>
  <summary>Details</summary>
Motivation: 高斯基场景表示在新视角下会出现几何失真，影响跟踪精度，主要原因是高斯基元的深度建模和表面间的相互干扰。

Method: 提出2D高斯增量重建策略和表面感知深度渲染机制，动态隔离可见表面以减少遮挡区域的误对齐。

Result: 在多个数据集上的实验表明，GauS-SLAM在跟踪精度和渲染保真度上优于同类方法。

Conclusion: GauS-SLAM通过改进几何建模和动态表面隔离，实现了高效且高精度的SLAM系统。

Abstract: We propose GauS-SLAM, a dense RGB-D SLAM system that leverages 2D Gaussian
surfels to achieve robust tracking and high-fidelity mapping. Our
investigations reveal that Gaussian-based scene representations exhibit
geometry distortion under novel viewpoints, which significantly degrades the
accuracy of Gaussian-based tracking methods. These geometry inconsistencies
arise primarily from the depth modeling of Gaussian primitives and the mutual
interference between surfaces during the depth blending. To address these, we
propose a 2D Gaussian-based incremental reconstruction strategy coupled with a
Surface-aware Depth Rendering mechanism, which significantly enhances geometry
accuracy and multi-view consistency. Additionally, the proposed local map
design dynamically isolates visible surfaces during tracking, mitigating
misalignment caused by occluded regions in global maps while maintaining
computational efficiency with increasing Gaussian density. Extensive
experiments across multiple datasets demonstrate that GauS-SLAM outperforms
comparable methods, delivering superior tracking precision and rendering
fidelity. The project page will be made available at
https://gaus-slam.github.io.

</details>

### [298] [HybridGS: High-Efficiency Gaussian Splatting Data Compression using Dual-Channel Sparse Representation and Point Cloud Encoder](https://arxiv.org/abs/2505.01938)
*Qi Yang,Le Yang,Geert Van Der Auwera,Zhu Li*

Main category: cs.CV

TLDR: HybridGS提出了一种结合紧凑生成和标准化点云编码的3D高斯泼溅压缩框架，显著提高了编码和解码速度。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS压缩方案编码时间长且数据格式高度定制化，难以广泛部署。

Method: HybridGS首先生成紧凑且显式的3DGS数据，引入双通道稀疏表示监督基元位置和特征位深度，再利用标准点云编码器进一步压缩数据。

Result: 实验表明，HybridGS在重建性能上与现有方法相当，但编码和解码速度明显更快。

Conclusion: HybridGS提供了一种高效且标准化的3DGS压缩方案，适合广泛部署。

Abstract: Most existing 3D Gaussian Splatting (3DGS) compression schemes focus on
producing compact 3DGS representation via implicit data embedding. They have
long coding times and highly customized data format, making it difficult for
widespread deployment. This paper presents a new 3DGS compression framework
called HybridGS, which takes advantage of both compact generation and
standardized point cloud data encoding. HybridGS first generates compact and
explicit 3DGS data. A dual-channel sparse representation is introduced to
supervise the primitive position and feature bit depth. It then utilizes a
canonical point cloud encoder to perform further data compression and form
standard output bitstreams. A simple and effective rate control scheme is
proposed to pivot the interpretable data compression scheme. At the current
stage, HybridGS does not include any modules aimed at improving 3DGS quality
during generation. But experiment results show that it still provides
comparable reconstruction performance against state-of-the-art methods, with
evidently higher encoding and decoding speed. The code is publicly available at
https://github.com/Qi-Yangsjtu/HybridGS.

</details>

### [299] [Segment Any RGB-Thermal Model with Language-aided Distillation](https://arxiv.org/abs/2505.01950)
*Dong Xing,Xianxun Zhu,Wei Zhou,Qika Lin,Hang Yang,Yuqing Wang*

Main category: cs.CV

TLDR: SARTM框架通过微调SAM并引入语义理解模块，成功将SAM应用于RGB-T语义分割任务，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: SAM仅基于RGB数据训练，无法直接用于RGB-T语义分割，而RGB-T在恶劣天气和光照条件下表现优越，因此需要定制化解决方案。

Method: 通过添加LoRA层微调SAM，引入语言信息和跨模态知识蒸馏（CMKD）模块，调整分割头并加入辅助语义分割头。

Result: 在MFNET、PST900和FMB三个基准测试中，SARTM在多种条件下均显著优于现有方法。

Conclusion: SARTM成功将SAM扩展到RGB-T语义分割，解决了跨模态不一致性问题，提升了性能。

Abstract: The recent Segment Anything Model (SAM) demonstrates strong instance
segmentation performance across various downstream tasks. However, SAM is
trained solely on RGB data, limiting its direct applicability to RGB-thermal
(RGB-T) semantic segmentation. Given that RGB-T provides a robust solution for
scene understanding in adverse weather and lighting conditions, such as low
light and overexposure, we propose a novel framework, SARTM, which customizes
the powerful SAM for RGB-T semantic segmentation. Our key idea is to unleash
the potential of SAM while introduce semantic understanding modules for RGB-T
data pairs. Specifically, our framework first involves fine tuning the original
SAM by adding extra LoRA layers, aiming at preserving SAM's strong
generalization and segmentation capabilities for downstream tasks. Secondly, we
introduce language information as guidance for training our SARTM. To address
cross-modal inconsistencies, we introduce a Cross-Modal Knowledge
Distillation(CMKD) module that effectively achieves modality adaptation while
maintaining its generalization capabilities. This semantic module enables the
minimization of modality gaps and alleviates semantic ambiguity, facilitating
the combination of any modality under any visual conditions. Furthermore, we
enhance the segmentation performance by adjusting the segmentation head of SAM
and incorporating an auxiliary semantic segmentation head, which integrates
multi-scale features for effective fusion. Extensive experiments are conducted
across three multi-modal RGBT semantic segmentation benchmarks: MFNET, PST900,
and FMB. Both quantitative and qualitative results consistently demonstrate
that the proposed SARTM significantly outperforms state-of-the-art approaches
across a variety of conditions.

</details>

### [300] [Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models](https://arxiv.org/abs/2505.02824)
*Kuofeng Gao,Yufei Zhu,Yiming Li,Jiawang Bai,Yong Yang,Zhifeng Li,Shu-Tao Xia*

Main category: cs.CV

TLDR: 论文提出了一种针对文本到图像（T2I）扩散模型的版权规避攻击（CEAT2I），通过检测水印样本、识别触发词和消除水印，成功绕过数据集所有权验证（DOV）机制。


<details>
  <summary>Details</summary>
Motivation: 预训练T2I模型的个性化微调可能导致未经授权的数据集使用，DOV通过水印技术保护数据集所有权，但其对版权规避攻击的鲁棒性尚未研究。

Method: CEAT2I分为三个阶段：水印样本检测（基于模型对水印样本的快速收敛特性）、触发词识别（通过迭代消融提示词并观察特征变化）和水印消除（采用闭式概念擦除方法）。

Result: 实验表明，CEAT2I能有效规避DOV机制，同时保持模型性能。

Conclusion: 该研究揭示了DOV在T2I模型中的潜在漏洞，为未来更鲁棒的水印技术提供了参考。

Abstract: Text-to-image (T2I) diffusion models have rapidly advanced, enabling
high-quality image generation conditioned on textual prompts. However, the
growing trend of fine-tuning pre-trained models for personalization raises
serious concerns about unauthorized dataset usage. To combat this, dataset
ownership verification (DOV) has emerged as a solution, embedding watermarks
into the fine-tuning datasets using backdoor techniques. These watermarks
remain inactive under benign samples but produce owner-specified outputs when
triggered. Despite the promise of DOV for T2I diffusion models, its robustness
against copyright evasion attacks (CEA) remains unexplored. In this paper, we
explore how attackers can bypass these mechanisms through CEA, allowing models
to circumvent watermarks even when trained on watermarked datasets. We propose
the first copyright evasion attack (i.e., CEAT2I) specifically designed to
undermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three
stages: watermarked sample detection, trigger identification, and efficient
watermark mitigation. A key insight driving our approach is that T2I models
exhibit faster convergence on watermarked samples during the fine-tuning,
evident through intermediate feature deviation. Leveraging this, CEAT2I can
reliably detect the watermarked samples. Then, we iteratively ablate tokens
from the prompts of detected watermarked samples and monitor shifts in
intermediate features to pinpoint the exact trigger tokens. Finally, we adopt a
closed-form concept erasure method to remove the injected watermark. Extensive
experiments show that our CEAT2I effectively evades DOV mechanisms while
preserving model performance.

</details>

### [301] [A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2505.01958)
*Liqiang Jing,Guiming Hardy Chen,Ehsan Aghazadeh,Xin Eric Wang,Xinya Du*

Main category: cs.CV

TLDR: 本文分析了大型视觉语言模型（LVLMs）中视觉对象幻觉的成因，并提出针对各组件（语言模型、视觉主干、投影器）的缓解方法，同时开发了两个幻觉基准测试。


<details>
  <summary>Details</summary>
Motivation: 视觉对象幻觉是LVLMs中的常见问题，可能导致错误信息，但其成因尚未被全面研究。

Method: 分析LLaVA类LVLMs的各组件，识别错误来源并提出缓解方法；开发QA-VisualGenome和QA-FB15k两个基准测试。

Result: 提出了针对各组件问题的缓解方法，并建立了新的幻觉评估基准。

Conclusion: 通过组件分析和基准测试，为减少LVLMs的视觉对象幻觉提供了有效方案。

Abstract: Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in
multimodal tasks, but visual object hallucination remains a persistent issue.
It refers to scenarios where models generate inaccurate visual object-related
information based on the query input, potentially leading to misinformation and
concerns about safety and reliability. Previous works focus on the evaluation
and mitigation of visual hallucinations, but the underlying causes have not
been comprehensively investigated. In this paper, we analyze each component of
LLaVA-like LVLMs -- the large language model, the vision backbone, and the
projector -- to identify potential sources of error and their impact. Based on
our observations, we propose methods to mitigate hallucination for each
problematic component. Additionally, we developed two hallucination benchmarks:
QA-VisualGenome, which emphasizes attribute and relation hallucinations, and
QA-FB15k, which focuses on cognition-based hallucinations.

</details>

### [302] [MC3D-AD: A Unified Geometry-aware Reconstruction Model for Multi-category 3D Anomaly Detection](https://arxiv.org/abs/2505.01969)
*Jiayi Cheng,Can Gao,Jie Zhou,Jiajun Wen,Tao Dai,Jinbao Wang*

Main category: cs.CV

TLDR: 提出了一种多类别3D异常检测的统一模型MC3D-AD，通过局部和全局几何感知信息重建正常表示，显著优于现有单类别方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D异常检测方法需为每个类别单独训练模型，成本高、效率低且泛化能力弱。

Method: 提出自适应几何感知掩码注意力模块提取几何变化信息，改进局部几何感知编码器，并设计全局查询解码器优化重建能力。

Result: 在Real3D-AD和Anomaly-ShapeNet数据集上，分别实现3.1%和9.3%的对象级AUROC提升。

Conclusion: MC3D-AD在多类别3D异常检测中表现出显著优势，具有高效性和泛化能力。

Abstract: 3D Anomaly Detection (AD) is a promising means of controlling the quality of
manufactured products. However, existing methods typically require carefully
training a task-specific model for each category independently, leading to high
cost, low efficiency, and weak generalization. Therefore, this paper presents a
novel unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) that aims
to utilize both local and global geometry-aware information to reconstruct
normal representations of all categories. First, to learn robust and
generalized features of different categories, we propose an adaptive
geometry-aware masked attention module that extracts geometry variation
information to guide mask attention. Then, we introduce a local geometry-aware
encoder reinforced by the improved mask attention to encode group-level feature
tokens. Finally, we design a global query decoder that utilizes point cloud
position embeddings to improve the decoding process and reconstruction ability.
This leads to local and global geometry-aware reconstructed feature tokens for
the AD task. MC3D-AD is evaluated on two publicly available Real3D-AD and
Anomaly-ShapeNet datasets, and exhibits significant superiority over current
state-of-the-art single-category methods, achieving 3.1\% and 9.3\% improvement
in object-level AUROC over Real3D-AD and Anomaly-ShapeNet, respectively. The
source code will be released upon acceptance.

</details>

### [303] [Visual Dominance and Emerging Multimodal Approaches in Distracted Driving Detection: A Review of Machine Learning Techniques](https://arxiv.org/abs/2505.01973)
*Anthony Dontoh,Stephanie Ivey,Logan Sirbaugh,Andrews Danyo,Armstrong Aboah*

Main category: cs.CV

TLDR: 本文综述了2019-2024年间74项关于使用ML/DL技术检测分心驾驶的研究，强调多模态方法的优势，并呼吁未来研究应关注轻量级、可部署的多模态框架。


<details>
  <summary>Details</summary>
Motivation: 分心驾驶是全球交通事故的主要原因，现有技术多依赖视觉数据，忽视了驾驶员行为的复杂性。

Method: 系统评估了视觉、传感器、多模态和新兴模态的ML/DL技术，分析了其优缺点。

Result: 多模态架构优于单模态方法，但视觉模型泛化能力有限，新兴技术提供了隐私保护替代方案。

Conclusion: 未来需开发轻量级多模态框架，结合个性化基准和跨模态标准，以提升ADAS和道路安全的实际可靠性。

Abstract: Distracted driving continues to be a significant cause of road traffic
injuries and fatalities worldwide, even with advancements in driver monitoring
technologies. Recent developments in machine learning (ML) and deep learning
(DL) have primarily focused on visual data to detect distraction, often
neglecting the complex, multimodal nature of driver behavior. This systematic
review assesses 74 peer-reviewed studies from 2019 to 2024 that utilize ML/DL
techniques for distracted driving detection across visual, sensor-based,
multimodal, and emerging modalities. The review highlights a significant
prevalence of visual-only models, particularly convolutional neural networks
(CNNs) and temporal architectures, which achieve high accuracy but show limited
generalizability in real-world scenarios. Sensor-based and physiological models
provide complementary strengths by capturing internal states and vehicle
dynamics, while emerging techniques, such as auditory sensing and radio
frequency (RF) methods, offer privacy-aware alternatives. Multimodal
architecture consistently surpasses unimodal baselines, demonstrating enhanced
robustness, context awareness, and scalability by integrating diverse data
streams. These findings emphasize the need to move beyond visual-only
approaches and adopt multimodal systems that combine visual, physiological, and
vehicular cues while keeping in checking the need to balance computational
requirements. Future research should focus on developing lightweight,
deployable multimodal frameworks, incorporating personalized baselines, and
establishing cross-modality benchmarks to ensure real-world reliability in
advanced driver assistance systems (ADAS) and road safety interventions.

</details>

### [304] [Lifelong Whole Slide Image Analysis: Online Vision-Language Adaptation and Past-to-Present Gradient Distillation](https://arxiv.org/abs/2505.01984)
*Doanh C. Bui,Hoai Luan Pham,Vu Trung Duong Le,Tuan Hai Vu,Van Duy Tran,Khang Nguyen,Yasuhiko Nakashima*

Main category: cs.CV

TLDR: ADaFGrad是一种用于全切片图像（WSI）分析的终身学习方法，通过病理视觉语言基础模型和梯度蒸馏机制，显著提升了分类性能并减少了遗忘。


<details>
  <summary>Details</summary>
Motivation: WSI在癌症诊断中至关重要，但其大尺寸和分布式存储带来了存储、处理和模型训练的挑战，需要开发终身学习方法。

Method: 利用病理视觉语言基础模型构建框架，结合文本原型缓冲区，并提出梯度蒸馏机制以模拟分类头参数的梯度变化。

Result: ADaFGrad在TCGA数据集上表现优异，超越现有方法，最高提升5.068%的分类性能，且遗忘最少。

Conclusion: ADaFGrad通过创新模块显著提升了WSI分析的终身学习效果，适用于临床诊断。

Abstract: Whole Slide Images (WSIs) play a crucial role in accurate cancer diagnosis
and prognosis, as they provide tissue details at the cellular level. However,
the rapid growth of computational tasks involving WSIs poses significant
challenges. Given that WSIs are gigapixels in size, they present difficulties
in terms of storage, processing, and model training. Therefore, it is essential
to develop lifelong learning approaches for WSI analysis. In scenarios where
slides are distributed across multiple institutes, we aim to leverage them to
develop a unified online model as a computational tool for cancer diagnosis in
clinical and hospital settings. In this study, we introduce ADaFGrad, a method
designed to enhance lifelong learning for whole-slide image (WSI) analysis.
First, we leverage pathology vision-language foundation models to develop a
framework that enables interaction between a slide's regional tissue features
and a predefined text-based prototype buffer. Additionally, we propose a
gradient-distillation mechanism that mimics the gradient of a logit with
respect to the classification-head parameters across past and current
iterations in a continual-learning setting. We construct a sequence of six TCGA
datasets for training and evaluation. Experimental results show that ADaFGrad
outperforms both state-of-the-art WSI-specific and conventional
continual-learning methods after only a few training epochs, exceeding them by
up to +5.068% in the class-incremental learning scenario while exhibiting the
least forgetting (i.e., retaining the most knowledge from previous tasks).
Moreover, ADaFGrad surpasses its baseline by as much as +40.084% in accuracy,
further demonstrating the effectiveness of the proposed modules.

</details>

### [305] [Drug classification based on X-ray spectroscopy combined with machine learning](https://arxiv.org/abs/2505.01986)
*Yongming Li,Peng Wang,Bangdong Han*

Main category: cs.CV

TLDR: 该研究结合X射线吸收光谱、CNN、PSO和SVM，提出了一种快速、高精度的药物检测方法，准确率达99.14%。


<details>
  <summary>Details</summary>
Motivation: 新型药物种类增多，传统检测方法复杂且对仪器和环境要求高，亟需开发更快速、准确的检测技术。

Method: 使用CNN提取X射线光谱特征，SVM分类，PSO优化SVM参数，对14种化学试剂进行实验验证。

Result: 模型分类准确率达99.14%，运行速度快，解决了PSO与SVM直接融合的效率问题。

Conclusion: 该方法为药物检测提供了高效、可靠的解决方案，具有广泛应用前景。

Abstract: The proliferation of new types of drugs necessitates the urgent development
of faster and more accurate detection methods. Traditional detection methods
have high requirements for instruments and environments, making the operation
complex. X-ray absorption spectroscopy, a non-destructive detection technique,
offers advantages such as ease of operation, penetrative observation, and
strong substance differentiation capabilities, making it well-suited for
application in the field of drug detection and identification. In this study,
we constructed a classification model using Convolutional Neural Networks
(CNN), Support Vector Machines (SVM), and Particle Swarm Optimization (PSO) to
classify and identify drugs based on their X-ray spectral profiles. In the
experiments, we selected 14 chemical reagents with chemical formulas similar to
drugs as samples. We utilized CNN to extract features from the spectral data of
these 14 chemical reagents and used the extracted features to train an SVM
model. We also utilized PSO to optimize two critical initial parameters of the
SVM. The experimental results demonstrate that this model achieved higher
classification accuracy compared to two other common methods, with a prediction
accuracy of 99.14%. Additionally, the model exhibited fast execution speed,
mitigating the drawback of a drastic increase in running time and efficiency
reduction that may result from the direct fusion of PSO and SVM. Therefore, the
combined approach of X-ray absorption spectroscopy with CNN, PSO, and SVM
provides a rapid, highly accurate, and reliable classification and
identification method for the field of drug detection, holding promising
prospects for widespread application.

</details>

### [306] [Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields](https://arxiv.org/abs/2505.02005)
*Zhenxing Mi,Ping Yin,Xue Xiao,Dan Xu*

Main category: cs.CV

TLDR: Switch-NeRF++提出了一种异构混合哈希专家网络（HMoHE），用于高效学习大规模场景的异构分解和NeRF建模，显著提升了训练和渲染效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有大规模NeRF方法中未探索的关键问题，如可学习分解、场景异构性建模和效率问题。

Method: 采用异构混合哈希专家网络（HMoHE），结合哈希门控网络和异构哈希专家，实现端到端的大规模场景建模。

Result: 在多个大规模NeRF数据集上验证了方法的准确性和可扩展性，训练和渲染效率分别提升8倍和16倍。

Conclusion: Switch-NeRF++是一种高效且可扩展的NeRF解决方案，适用于真实世界的大规模场景建模。

Abstract: Recent NeRF methods on large-scale scenes have underlined the importance of
scene decomposition for scalable NeRFs. Although achieving reasonable
scalability, there are several critical problems remaining unexplored, i.e.,
learnable decomposition, modeling scene heterogeneity, and modeling efficiency.
In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash
Experts (HMoHE) network that addresses these challenges within a unified
framework. It is a highly scalable NeRF that learns heterogeneous decomposition
and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end
manner. In our framework, a gating network learns to decomposes scenes and
allocates 3D points to specialized NeRF experts. This gating network is
co-optimized with the experts, by our proposed Sparsely Gated Mixture of
Experts (MoE) NeRF framework. We incorporate a hash-based gating network and
distinct heterogeneous hash experts. The hash-based gating efficiently learns
the decomposition of the large-scale scene. The distinct heterogeneous hash
experts consist of hash grids of different resolution ranges, enabling
effective learning of the heterogeneous representation of different scene
parts. These design choices make our framework an end-to-end and highly
scalable NeRF solution for real-world large-scale scene modeling to achieve
both quality and efficiency. We evaluate our accuracy and scalability on
existing large-scale NeRF datasets and a new dataset with very large-scale
scenes ($>6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our
approach can be easily scaled to various large-scale scenes and achieve
state-of-the-art scene rendering accuracy. Furthermore, our method exhibits
significant efficiency, with an 8x acceleration in training and a 16x
acceleration in rendering compared to Switch-NeRF. Codes will be released in
https://github.com/MiZhenxing/Switch-NeRF.

</details>

### [307] [Efficient Noise Calculation in Deep Learning-based MRI Reconstructions](https://arxiv.org/abs/2505.02007)
*Onat Dalmaz,Arjun D. Desai,Reinhard Heckel,Tolga Çukur,Akshay S. Chaudhari,Brian A. Hargreaves*

Main category: cs.CV

TLDR: 提出了一种高效计算加速MRI重建中体素级方差的方法，用于量化噪声传播的不确定性，显著降低了计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 深度学习MRI重建方法常忽略噪声传播分析，但其对评估重建质量和指导方法设计至关重要。

Method: 通过DL网络的Jacobian近似噪声协方差，提出无偏估计器和Jacobian草图技术高效计算体素级方差。

Result: 在膝部和脑部MRI数据集上验证，性能接近蒙特卡罗模拟，计算和内存需求降低一个数量级以上。

Conclusion: 该方法为DL-MRI重建引入高效噪声分析，有望改变评估和部署方式。

Abstract: Accelerated MRI reconstruction involves solving an ill-posed inverse problem
where noise in acquired data propagates to the reconstructed images. Noise
analyses are central to MRI reconstruction for providing an explicit measure of
solution fidelity and for guiding the design and deployment of novel
reconstruction methods. However, deep learning (DL)-based reconstruction
methods have often overlooked noise propagation due to inherent analytical and
computational challenges, despite its critical importance. This work proposes a
theoretically grounded, memory-efficient technique to calculate voxel-wise
variance for quantifying uncertainty due to acquisition noise in accelerated
MRI reconstructions. Our approach approximates noise covariance using the DL
network's Jacobian, which is intractable to calculate. To circumvent this, we
derive an unbiased estimator for the diagonal of this covariance matrix
(voxel-wise variance) and introduce a Jacobian sketching technique to
efficiently implement it. We evaluate our method on knee and brain MRI datasets
for both data- and physics-driven networks trained in supervised and
unsupervised manners. Compared to empirical references obtained via Monte Carlo
simulations, our technique achieves near-equivalent performance while reducing
computational and memory demands by an order of magnitude or more. Furthermore,
our method is robust across varying input noise levels, acceleration factors,
and diverse undersampling schemes, highlighting its broad applicability. Our
work reintroduces accurate and efficient noise analysis as a central tenet of
reconstruction algorithms, holding promise to reshape how we evaluate and
deploy DL-based MRI. Our code will be made publicly available upon acceptance.

</details>

### [308] [MLLM-Enhanced Face Forgery Detection: A Vision-Language Fusion Solution](https://arxiv.org/abs/2505.02013)
*Siran Peng,Zipei Wang,Li Gao,Xiangyu Zhu,Tianshuo Zhang,Ajian Liu,Haoyuan Zhang,Zhen Lei*

Main category: cs.CV

TLDR: 提出了一种基于视觉-语言融合的多模态大语言模型（MLLM）增强人脸伪造检测方法VLF-FFD，通过扩展数据集EFF++和改进的网络VLF-Net，实现了跨数据集和数据集内评估的最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在视觉和文本模态融合上的不足，提升人脸伪造检测的可靠性和效果。

Method: 提出EFF++数据集，包含带文本注释的伪造视频帧；设计VLF-Net网络，通过三阶段训练实现视觉与文本特征的双向交互。

Result: VLF-FFD在跨数据集和数据集内评估中均达到最优性能。

Conclusion: VLF-FFD通过有效融合视觉与文本信息，显著提升了人脸伪造检测的效果。

Abstract: Reliable face forgery detection algorithms are crucial for countering the
growing threat of deepfake-driven disinformation. Previous research has
demonstrated the potential of Multimodal Large Language Models (MLLMs) in
identifying manipulated faces. However, existing methods typically depend on
either the Large Language Model (LLM) alone or an external detector to generate
classification results, which often leads to sub-optimal integration of visual
and textual modalities. In this paper, we propose VLF-FFD, a novel
Vision-Language Fusion solution for MLLM-enhanced Face Forgery Detection. Our
key contributions are twofold. First, we present EFF++, a frame-level,
explainability-driven extension of the widely used FaceForensics++ (FF++)
dataset. In EFF++, each manipulated video frame is paired with a textual
annotation that describes both the forgery artifacts and the specific
manipulation technique applied, enabling more effective and informative MLLM
training. Second, we design a Vision-Language Fusion Network (VLF-Net) that
promotes bidirectional interaction between visual and textual features,
supported by a three-stage training pipeline to fully leverage its potential.
VLF-FFD achieves state-of-the-art (SOTA) performance in both cross-dataset and
intra-dataset evaluations, underscoring its exceptional effectiveness in face
forgery detection.

</details>

### [309] [R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation](https://arxiv.org/abs/2505.02018)
*Meng-Hao Guo,Jiajun Xu,Yi Zhang,Jiaxi Song,Haoyang Peng,Yi-Xuan Deng,Xinzhi Dong,Kiyohiro Nakayama,Zhengyang Geng,Chen Wang,Bolin Ni,Guo-Wei Yang,Yongming Rao,Houwen Peng,Han Hu,Gordon Wetzstein,Shi-min Hu*

Main category: cs.CV

TLDR: 论文介绍了名为R-Bench的多学科、多语言推理基准测试，用于评估语言和多模态模型的推理能力，发现现有模型在复杂推理任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有推理基准测试难以评估复杂、多学科和多模态环境下的推理能力，因此需要一个新的高标准测试工具。

Method: 构建了包含1,094个语言模型问题和665个多模态模型问题的R-Bench基准测试，涵盖108和83个学科，并确保难度校准和跨语言对齐。

Result: 实验结果显示，即使是表现最好的模型OpenAI o1在多模态推理任务上的准确率仅为53.2%。

Conclusion: R-Bench是一个高标准的推理基准测试，揭示了现有模型在复杂推理任务上的不足，为未来研究提供了方向。

Abstract: Reasoning stands as a cornerstone of intelligence, enabling the synthesis of
existing knowledge to solve complex problems. Despite remarkable progress,
existing reasoning benchmarks often fail to rigorously evaluate the nuanced
reasoning capabilities required for complex, real-world problemsolving,
particularly in multi-disciplinary and multimodal contexts. In this paper, we
introduce a graduate-level, multi-disciplinary, EnglishChinese benchmark,
dubbed as Reasoning Bench (R-Bench), for assessing the reasoning capability of
both language and multimodal models. RBench spans 1,094 questions across 108
subjects for language model evaluation and 665 questions across 83 subjects for
multimodal model testing in both English and Chinese. These questions are
meticulously curated to ensure rigorous difficulty calibration, subject
balance, and crosslinguistic alignment, enabling the assessment to be an
Olympiad-level multi-disciplinary benchmark. We evaluate widely used models,
including OpenAI o1, GPT-4o, DeepSeek-R1, etc. Experimental results indicate
that advanced models perform poorly on complex reasoning, especially multimodal
reasoning. Even the top-performing model OpenAI o1 achieves only 53.2% accuracy
on our multimodal evaluation. Data and code are made publicly available at
here.

</details>

### [310] [A Birotation Solution for Relative Pose Problems](https://arxiv.org/abs/2505.02025)
*Hongbo Zhao,Ziwei Long,Mengtan Zhang,Hanli Wang,Qijun Chen,Rui Fan*

Main category: cs.CV

TLDR: 提出了一种新颖的双旋转解决方案，用于相对位姿估计，通过最小化黎曼流形上的能量函数，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统相对位姿估计方法（如分解本质矩阵或直接估计旋转和平移）的局限性，提出更优的双旋转方案。

Method: 引入三个基础变换及其几何度量，设计能量函数并在黎曼流形上最小化，迭代更新两个旋转矩阵以恢复相对位姿。

Result: 在多种相对位姿估计任务中表现出优越性能。

Conclusion: 双旋转解决方案为相对位姿估计提供了高效且性能优越的新方法。

Abstract: Relative pose estimation, a fundamental computer vision problem, has been
extensively studied for decades. Existing methods either estimate and decompose
the essential matrix or directly estimate the rotation and translation to
obtain the solution. In this article, we break the mold by tackling this
traditional problem with a novel birotation solution. We first introduce three
basis transformations, each associated with a geometric metric to quantify the
distance between the relative pose to be estimated and its corresponding basis
transformation. Three energy functions, designed based on these metrics, are
then minimized on the Riemannian manifold $\mathrm{SO(3)}$ by iteratively
updating the two rotation matrices. The two rotation matrices and the basis
transformation corresponding to the minimum energy are ultimately utilized to
recover the relative pose. Extensive quantitative and qualitative evaluations
across diverse relative pose estimation tasks demonstrate the superior
performance of our proposed birotation solution. Source code, demo video, and
datasets will be available at
\href{https://mias.group/birotation-solution}{mias.group/birotation-solution}
upon publication.

</details>

### [311] [Point2Primitive: CAD Reconstruction from Point Cloud by Direct Primitive Prediction](https://arxiv.org/abs/2505.02043)
*Cheng Wang,Xinzhu Ma,Bin Wang,Shixiang Tang,Yuan Meng,Ping Jiang*

Main category: cs.CV

TLDR: 本文提出了一种从点云直接生成可编辑CAD模型的方法（Point2Primitive），通过改进的Transformer直接预测挤出基元的每个元素，实现高精度参数重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用隐式场表示草图，导致曲线边缘的形状重建效果不佳，需要一种更直接的方法来恢复CAD模型的拓扑和挤出基元。

Method: 基于改进的Transformer，直接检测和预测点云中的草图曲线（类型和参数），并通过自回归方式优化参数。通过挤出分割重建拓扑，结合预测曲线和计算的挤出操作恢复参数。

Result: 实验表明，该方法在基元预测精度和CAD重建方面表现优异，重建形状具有高几何保真度。

Conclusion: Point2Primitive能够高效且准确地从点云生成可编辑CAD模型，为CAD重建提供了新思路。

Abstract: Recovering CAD models from point clouds, especially the sketch-extrusion
process, can be seen as the process of rebuilding the topology and extrusion
primitives. Previous methods utilize implicit fields for sketch representation,
leading to shape reconstruction of curved edges. In this paper, we proposed a
CAD reconstruction network that produces editable CAD models from input point
clouds (Point2Primitive) by directly predicting every element of the extrusion
primitives. Point2Primitive can directly detect and predict sketch curves (type
and parameter) from point clouds based on an improved transformer. The sketch
curve parameters are formulated as position queries and optimized in an
autoregressive way, leading to high parameter accuracy. The topology is rebuilt
by extrusion segmentation, and each extrusion parameter (sketch and extrusion
operation) is recovered by combining the predicted curves and the computed
extrusion operation. Extensive experiments demonstrate that our method is
superior in primitive prediction accuracy and CAD reconstruction. The
reconstructed shapes are of high geometrical fidelity.

</details>

### [312] [A UNet Model for Accelerated Preprocessing of CRISM Hyperspectral Data for Mineral Identification on Mars](https://arxiv.org/abs/2505.02046)
*Priyanka Kumari,Sampriti Soor,Amba Shetty,Archana M. Nair*

Main category: cs.CV

TLDR: 本文提出了一种基于UNet的自动编码器模型，用于高效预处理CRISM MTRDR高光谱数据，显著减少了预处理时间并保持了矿物吸收特征。


<details>
  <summary>Details</summary>
Motivation: 传统的高光谱数据预处理方法计算量大且耗时，限制了火星矿物识别的效率。

Method: 采用UNet架构的自动编码器模型，结合MICA光谱库的增强数据，模拟MTRDR数据条件，自动化预处理步骤。

Result: 预处理时间从1.5小时缩短至5分钟，同时保持了矿物识别的准确性。

Conclusion: 该框架显著提升了火星矿物映射的速度和可靠性，具有广泛应用潜力。

Abstract: Accurate mineral identification on the Martian surface is critical for
understanding the planet's geological history. This paper presents a UNet-based
autoencoder model for efficient spectral preprocessing of CRISM MTRDR
hyperspectral data, addressing the limitations of traditional methods that are
computationally intensive and time-consuming. The proposed model automates key
preprocessing steps, such as smoothing and continuum removal, while preserving
essential mineral absorption features. Trained on augmented spectra from the
MICA spectral library, the model introduces realistic variability to simulate
MTRDR data conditions. By integrating this framework, preprocessing time for an
800x800 MTRDR scene is reduced from 1.5 hours to just 5 minutes on an NVIDIA
T1600 GPU. The preprocessed spectra are subsequently classified using MICAnet,
a deep learning model for Martian mineral identification. Evaluation on labeled
CRISM TRDR data demonstrates that the proposed approach achieves competitive
accuracy while significantly enhancing preprocessing efficiency. This work
highlights the potential of the UNet-based preprocessing framework to improve
the speed and reliability of mineral mapping on Mars.

</details>

### [313] [Handling Imbalanced Pseudolabels for Vision-Language Models with Concept Alignment and Confusion-Aware Calibrated Margin](https://arxiv.org/abs/2505.02056)
*Yuchen Wang,Xuefeng Bai,Xiucheng Li,Weili Guan,Liqiang Nie,Xinyang Chen*

Main category: cs.CV

TLDR: 论文提出了一种新框架，通过概念对齐和混淆感知校准机制解决视觉语言模型（VLM）生成的伪标签不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 伪标签不平衡导致性能下降，现有方法未深入探究其根本原因。

Method: 提出概念对齐和混淆感知校准机制，增强表现不佳的类别并促进平衡预测。

Result: 在六个基准数据集上验证，相对现有最优方法提升6.29%。

Conclusion: 新框架有效提升了伪标签的准确性和平衡性。

Abstract: Adapting vision-language models (VLMs) to downstream tasks with pseudolabels
has gained increasing attention. A major obstacle is that the pseudolabels
generated by VLMs tend to be imbalanced, leading to inferior performance. While
existing methods have explored various strategies to address this, the
underlying causes of imbalance remain insufficiently investigated. To fill this
gap, we delve into imbalanced pseudolabels and identify two primary
contributing factors: concept mismatch and concept confusion. To mitigate these
two issues, we propose a novel framework incorporating concept alignment and
confusion-aware calibrated margin mechanisms. The core of our approach lies in
enhancing underperforming classes and promoting balanced predictions across
categories, thus mitigating imbalance. Extensive experiments on six benchmark
datasets with three learning paradigms demonstrate that the proposed method
effectively enhances the accuracy and balance of pseudolabels, achieving a
relative improvement of 6.29% over the SoTA method. Our code is avaliable at
https://anonymous.4open.science/r/CAP-C642/

</details>

### [314] [LangGas: Introducing Language in Selective Zero-Shot Background Subtraction for Semi-Transparent Gas Leak Detection with a New Dataset](https://arxiv.org/abs/2503.02910)
*Wenqi Guo,Yiyang Du,Shan Du*

Main category: cs.CV

TLDR: 论文提出了一种零样本方法用于气体泄漏检测，并发布了合成数据集SimGas。实验表明，该方法显著优于基线方法，IoU达到69%。


<details>
  <summary>Details</summary>
Motivation: 传统人工检测气体泄漏效率低，且缺乏高质量公开数据集。

Method: 结合背景减除、零样本目标检测、过滤和分割的零样本方法。

Result: IoU达到69%，在真实数据集GasVid上表现良好。

Conclusion: 提出的方法和数据集为气体泄漏检测提供了有效工具。

Abstract: Gas leakage poses a significant hazard that requires prevention.
Traditionally, human inspection has been used for detection, a slow and
labour-intensive process. Recent research has applied machine learning
techniques to this problem, yet there remains a shortage of high-quality,
publicly available datasets. This paper introduces a synthetic dataset, SimGas,
featuring diverse backgrounds, interfering foreground objects, diverse leak
locations, and precise segmentation ground truth. We propose a zero-shot method
that combines background subtraction, zero-shot object detection, filtering,
and segmentation to leverage this dataset. Experimental results indicate that
our approach significantly outperforms baseline methods based solely on
background subtraction and zero-shot object detection with segmentation,
reaching an IoU of 69%. We also present an analysis of various prompt
configurations and threshold settings to provide deeper insights into the
performance of our method. Finally, we qualitatively (because of the lack of
ground truth) tested our performance on GasVid and reached decent results on
the real-world dataset. The dataset, code, and full qualitative results are
available at https://github.com/weathon/Lang-Gas.

</details>

### [315] [Transforming faces into video stories -- VideoFace2.0](https://arxiv.org/abs/2505.02060)
*Branko Brkljač,Vladimir Kalušev,Branislav Popović,Milan Sečujski*

Main category: cs.CV

TLDR: 论文介绍了一种名为VideoFace2.0的系统，用于视频中的人脸检测、识别和重新识别（ReID），支持结构化视频输出。


<details>
  <summary>Details</summary>
Motivation: 受早期Videoface digitizer的启发，旨在开发一种高效工具，用于创建基于身份的视频信息目录，支持电视制作、媒体分析等应用场景。

Method: 结合人脸检测、识别和被动检测跟踪技术，实现高效的人脸ReID，系统设计为模块化扩展。

Result: 实验验证了算法的适用性，系统支持实时处理，适用于多模态机器学习数据集生成。

Conclusion: VideoFace2.0为视频分析工具开发提供了新思路，有望降低高质量多模态数据集生成的入门门槛。

Abstract: Face detection and face recognition have been in the focus of vision
community since the very beginnings. Inspired by the success of the original
Videoface digitizer, a pioneering device that allowed users to capture video
signals from any source, we have designed an advanced video analytics tool to
efficiently create structured video stories, i.e. identity-based information
catalogs. VideoFace2.0 is the name of the developed system for spatial and
temporal localization of each unique face in the input video, i.e. face
re-identification (ReID), which also allows their cataloging, characterization
and creation of structured video outputs for later downstream tasks. Developed
near real-time solution is primarily designed to be utilized in application
scenarios involving TV production, media analysis, and as an efficient tool for
creating large video datasets necessary for training machine learning (ML)
models in challenging vision tasks such as lip reading and multimodal speech
recognition. Conducted experiments confirm applicability of the proposed face
ReID algorithm that is combining the concepts of face detection, face
recognition and passive tracking-by-detection in order to achieve robust and
efficient face ReID. The system is envisioned as a compact and modular
extensions of the existing video production equipment. We hope that the
presented work and shared code will stimulate further interest in development
of similar, application specific video analysis tools, and lower the entry
barrier for production of high-quality multi-modal ML datasets in the future.

</details>

### [316] [RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video](https://arxiv.org/abs/2505.02064)
*Shuhang Xun,Sicheng Tao,Jungang Li,Yibo Shi,Zhixin Lin,Zhanhui Zhu,Yibo Yan,Hanqian Li,Linghao Zhang,Shikang Wang,Yixin Liu,Hanbo Zhang,Xuming Hu,Ying Ma*

Main category: cs.CV

TLDR: RTV-Bench是一个用于评估多模态大语言模型（MLLMs）实时视频分析能力的细粒度基准，包含552个视频和4,631个QA对。实验显示开源实时模型优于离线模型，但仍落后于顶级专有模型。


<details>
  <summary>Details</summary>
Motivation: 当前基准未能充分评估MLLMs在动态实时环境中的持续感知、理解和推理能力，因此需要更合适的评估工具。

Method: RTV-Bench采用多时间戳问答（MTQA）、分层问题结构和多维度评估三项原则，评估模型的实时视频分析能力。

Result: 开源实时模型表现优于离线模型，但不及专有模型；模型大小或帧采样率对性能提升有限，甚至可能轻微降低性能。

Conclusion: 需要优化模型架构以提升视频流处理和长序列处理能力，推动MLLMs在实时视频分析中的进步。

Abstract: Multimodal Large Language Models (MLLMs) increasingly excel at perception,
understanding, and reasoning. However, current benchmarks inadequately evaluate
their ability to perform these tasks continuously in dynamic, real-world
environments. To bridge this gap, we introduce RTV-Bench, a fine-grained
benchmark for MLLM real-time video analysis. RTV-Bench uses three key
principles: (1) Multi-Timestamp Question Answering (MTQA), where answers evolve
with scene changes; (2) Hierarchical Question Structure, combining basic and
advanced queries; and (3) Multi-dimensional Evaluation, assessing the ability
of continuous perception, understanding, and reasoning. RTV-Bench contains 552
diverse videos (167.2 hours) and 4,631 high-quality QA pairs. We evaluated
leading MLLMs, including proprietary (GPT-4o, Gemini 2.0), open-source offline
(Qwen2.5-VL, VideoLLaMA3), and open-source real-time (VITA-1.5,
InternLM-XComposer2.5-OmniLive) models. Experiment results show open-source
real-time models largely outperform offline ones but still trail top
proprietary models. Our analysis also reveals that larger model size or higher
frame sampling rates do not significantly boost RTV-Bench performance,
sometimes causing slight decreases. This underscores the need for better model
architectures optimized for video stream processing and long sequences to
advance real-time video analysis with MLLMs. Our benchmark toolkit is available
at: https://github.com/LJungang/RTV-Bench.

</details>

### [317] [Hierarchical Compact Clustering Attention (COCA) for Unsupervised Object-Centric Learning](https://arxiv.org/abs/2505.02071)
*Can Küçüksözen,Yücel Yemez*

Main category: cs.CV

TLDR: COCA-Net是一种基于注意力机制的层次化聚类模块，用于无监督对象发现任务，通过紧凑性聚类算法提取对象中心表示，并在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决单图像中无监督对象发现任务，提出一种能够提取多对象场景中对象中心表示的方法。

Method: 提出COCA层，利用紧凑性聚类算法和层次化网络架构（COCA-Net），生成高质量的分割掩码。

Result: 在六个数据集上表现优于或与现有最佳模型竞争，通过九种评估指标验证。

Conclusion: COCA-Net在无监督对象发现任务中表现出色，能够灵活生成对象掩码并更好地处理背景分割。

Abstract: We propose the Compact Clustering Attention (COCA) layer, an effective
building block that introduces a hierarchical strategy for object-centric
representation learning, while solving the unsupervised object discovery task
on single images. COCA is an attention-based clustering module capable of
extracting object-centric representations from multi-object scenes, when
cascaded into a bottom-up hierarchical network architecture, referred to as
COCA-Net. At its core, COCA utilizes a novel clustering algorithm that
leverages the physical concept of compactness, to highlight distinct object
centroids in a scene, providing a spatial inductive bias. Thanks to this
strategy, COCA-Net generates high-quality segmentation masks on both the
decoder side and, notably, the encoder side of its pipeline. Additionally,
COCA-Net is not bound by a predetermined number of object masks that it
generates and handles the segmentation of background elements better than its
competitors. We demonstrate COCA-Net's segmentation performance on six widely
adopted datasets, achieving superior or competitive results against the
state-of-the-art models across nine different evaluation metrics.

</details>

### [318] [Benchmarking Feature Upsampling Methods for Vision Foundation Models using Interactive Segmentation](https://arxiv.org/abs/2505.02075)
*Volodymyr Havrylov,Haiwen Huang,Dan Zhang,Andreas Geiger*

Main category: cs.CV

TLDR: 论文研究了如何通过任务无关的特征上采样模块提升视觉基础模型（VFMs）在密集预测任务中的表现，并以交互式分割（IS）为基准进行评估。


<details>
  <summary>Details</summary>
Motivation: VFMs通常生成低分辨率特征，限制了其在密集预测任务中的直接应用，因此需要探索提升特征分辨率的方法。

Method: 采用任务无关的特征上采样模块，并通过交互式分割（IS）作为基准来评估其效果。

Result: 实验表明，选择合适的特征上采样策略能显著提升VFMs特征的质量。

Conclusion: 特征上采样模块是提升VFMs在密集预测任务中表现的有效方法，交互式分割是一个有挑战性的评估基准。

Abstract: Vision Foundation Models (VFMs) are large-scale, pre-trained models that
serve as general-purpose backbones for various computer vision tasks. As VFMs'
popularity grows, there is an increasing interest in understanding their
effectiveness for dense prediction tasks. However, VFMs typically produce
low-resolution features, limiting their direct applicability in this context.
One way to tackle this limitation is by employing a task-agnostic feature
upsampling module that refines VFM features resolution. To assess the
effectiveness of this approach, we investigate Interactive Segmentation (IS) as
a novel benchmark for evaluating feature upsampling methods on VFMs. Due to its
inherent multimodal input, consisting of an image and a set of user-defined
clicks, as well as its dense mask output, IS creates a challenging environment
that demands comprehensive visual scene understanding. Our benchmarking
experiments show that selecting appropriate upsampling strategies significantly
improves VFM features quality. The code is released at
https://github.com/havrylovv/iSegProbe

</details>

### [319] [HandOcc: NeRF-based Hand Rendering with Occupancy Networks](https://arxiv.org/abs/2505.02079)
*Maksym Ivashechkin,Oscar Mendez,Richard Bowden*

Main category: cs.CV

TLDR: HandOcc是一种基于占用的新型手部渲染框架，通过结合NeRF和卷积模型，避免了参数化网格的限制，实现了高保真和快速渲染。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖参数化网格，限制了模型的通用性和渲染质量。HandOcc旨在解决这一问题，提供更灵活、高质量的渲染方案。

Method: 提出了一种无网格的3D渲染流程，仅需3D骨架，通过卷积模型提取外观，并利用基于占用的NeRF渲染器。

Result: 在InterHand2.6M数据集上取得了最先进的成果，实现了快速渲染和优秀的外观迁移。

Conclusion: HandOcc通过占用表示和NeRF的结合，显著提升了手部渲染的质量和效率。

Abstract: We propose HandOcc, a novel framework for hand rendering based upon
occupancy. Popular rendering methods such as NeRF are often combined with
parametric meshes to provide deformable hand models. However, in doing so, such
approaches present a trade-off between the fidelity of the mesh and the
complexity and dimensionality of the parametric model. The simplicity of
parametric mesh structures is appealing, but the underlying issue is that it
binds methods to mesh initialization, making it unable to generalize to objects
where a parametric model does not exist. It also means that estimation is tied
to mesh resolution and the accuracy of mesh fitting. This paper presents a
pipeline for meshless 3D rendering, which we apply to the hands. By providing
only a 3D skeleton, the desired appearance is extracted via a convolutional
model. We do this by exploiting a NeRF renderer conditioned upon an
occupancy-based representation. The approach uses the hand occupancy to resolve
hand-to-hand interactions further improving results, allowing fast rendering,
and excellent hand appearance transfer. On the benchmark InterHand2.6M dataset,
we achieved state-of-the-art results.

</details>

### [320] [SignSplat: Rendering Sign Language via Gaussian Splatting](https://arxiv.org/abs/2505.02108)
*Maksym Ivashechkin,Oscar Mendez,Richard Bowden*

Main category: cs.CV

TLDR: 提出了一种基于高斯泼溅的框架，用于从少量视角建模复杂人体动作（如手语），通过序列数据利用和正则化技术提升渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注简单动作（如舞蹈或行走），而复杂动作（如手语）需要更精细的手部和面部建模，且多视角数据获取困难。

Method: 利用序列数据克服视角限制，通过正则化高斯参数防止过拟合，并提出自适应控制方法优化高斯分布。

Result: 在手语视频渲染上表现优异，基准数据集上达到SOTA，复杂动作上显著优于其他方法。

Conclusion: 该方法在少量视角下实现了对复杂人体动作的高保真渲染，为手语等应用提供了有效解决方案。

Abstract: State-of-the-art approaches for conditional human body rendering via Gaussian
splatting typically focus on simple body motions captured from many views. This
is often in the context of dancing or walking. However, for more complex use
cases, such as sign language, we care less about large body motion and more
about subtle and complex motions of the hands and face. The problems of
building high fidelity models are compounded by the complexity of capturing
multi-view data of sign. The solution is to make better use of sequence data,
ensuring that we can overcome the limited information from only a few views by
exploiting temporal variability. Nevertheless, learning from sequence-level
data requires extremely accurate and consistent model fitting to ensure that
appearance is consistent across complex motions. We focus on how to achieve
this, constraining mesh parameters to build an accurate Gaussian splatting
framework from few views capable of modelling subtle human motion. We leverage
regularization techniques on the Gaussian parameters to mitigate overfitting
and rendering artifacts. Additionally, we propose a new adaptive control method
to densify Gaussians and prune splat points on the mesh surface. To demonstrate
the accuracy of our approach, we render novel sequences of sign language video,
building on neural machine translation approaches to sign stitching. On
benchmark datasets, our approach achieves state-of-the-art performance; and on
highly articulated and complex sign language motion, we significantly
outperform competing approaches.

</details>

### [321] [Unaligned RGB Guided Hyperspectral Image Super-Resolution with Spatial-Spectral Concordance](https://arxiv.org/abs/2505.02109)
*Yingkai Zhang,Zeqiang Lai,Tao Zhang,Ying Fu,Chenghu Zhou*

Main category: cs.CV

TLDR: 提出了一种名为SSC-HSR的框架，通过两阶段图像对齐和特征聚合模块，解决了高光谱图像超分辨率中未对齐参考RGB图像的准确对齐和模块交互问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法因对齐不准确和模块间交互不足，无法有效利用参考图像信息，限制了高分辨率比下的性能。

Method: 采用两阶段图像对齐（精细光流模型和变形模型）和特征聚合模块（迭代可变形特征聚合和注意力融合模块）提升对齐和融合效果。

Result: 在三个自然或遥感数据集上的实验表明，该方法在定量和定性评估上均优于现有技术。

Conclusion: SSC-HSR框架通过改进对齐和模块交互，显著提升了高光谱图像超分辨率的性能。

Abstract: Hyperspectral images super-resolution aims to improve the spatial resolution,
yet its performance is often limited at high-resolution ratios. The recent
adoption of high-resolution reference images for super-resolution is driven by
the poor spatial detail found in low-resolution HSIs, presenting it as a
favorable method. However, these approaches cannot effectively utilize
information from the reference image, due to the inaccuracy of alignment and
its inadequate interaction between alignment and fusion modules. In this paper,
we introduce a Spatial-Spectral Concordance Hyperspectral Super-Resolution
(SSC-HSR) framework for unaligned reference RGB guided HSI SR to address the
issues of inaccurate alignment and poor interactivity of the previous
approaches. Specifically, to ensure spatial concordance, i.e., align images
more accurately across resolutions and refine textures, we construct a
Two-Stage Image Alignment with a synthetic generation pipeline in the image
alignment module, where the fine-tuned optical flow model can produce a more
accurate optical flow in the first stage and warp model can refine damaged
textures in the second stage. To enhance the interaction between alignment and
fusion modules and ensure spectral concordance during reconstruction, we
propose a Feature Aggregation module and an Attention Fusion module. In the
feature aggregation module, we introduce an Iterative Deformable Feature
Aggregation block to achieve significant feature matching and texture
aggregation with the fusion multi-scale results guidance, iteratively
generating learnable offset. Besides, we introduce two basic spectral-wise
attention blocks in the attention fusion module to model the inter-spectra
interactions. Extensive experiments on three natural or remote-sensing datasets
show that our method outperforms state-of-the-art approaches on both
quantitative and qualitative evaluations.

</details>

### [322] [GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity Non-Watertight 3D Garment Reconstruction](https://arxiv.org/abs/2505.02126)
*Zhihao Tang,Shenghao Yang,Hongtao Zhang,Mingbo Zhao*

Main category: cs.CV

TLDR: GarmentGS利用密集点云引导高斯原语，快速重建高保真3D服装表面，实现高效训练和实时渲染。


<details>
  <summary>Details</summary>
Motivation: 传统3D服装创建耗时耗力，高斯溅射技术虽在3D场景重建中取得突破，但因高斯原语的无结构特性难以重建高保真非水密服装。

Method: 提出GarmentGS方法，通过密集点云引导高斯原语的运动、展平和旋转，快速重建服装点云并生成单层网格。

Result: 10分钟内完成服装点云重建，传统方法需数小时；在几何精度和渲染效果上表现优异。

Conclusion: GarmentGS在保持高质量的同时，实现了快速训练和实时渲染，为3D服装重建提供了新途径。

Abstract: Traditional 3D garment creation requires extensive manual operations,
resulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved
breakthrough progress in 3D scene reconstruction and rendering, attracting
widespread attention and opening new pathways for 3D garment reconstruction.
However, due to the unstructured and irregular nature of Gaussian primitives,
it is difficult to reconstruct high-fidelity, non-watertight 3D garments. In
this paper, we present GarmentGS, a dense point cloud-guided method that can
reconstruct high-fidelity garment surfaces with high geometric accuracy and
generate non-watertight, single-layer meshes. Our method introduces a fast
dense point cloud reconstruction module that can complete garment point cloud
reconstruction in 10 minutes, compared to traditional methods that require
several hours. Furthermore, we use dense point clouds to guide the movement,
flattening, and rotation of Gaussian primitives, enabling better distribution
on the garment surface to achieve superior rendering effects and geometric
accuracy. Through numerical and visual comparisons, our method achieves fast
training and real-time rendering while maintaining competitive quality.

</details>

### [323] [HiLLIE: Human-in-the-Loop Training for Low-Light Image Enhancement](https://arxiv.org/abs/2505.02134)
*Xiaorui Zhao,Xinyue Zhou,Peibei Cao,Junyu Lou,Shuhang Gu*

Main category: cs.CV

TLDR: 提出了一种名为HiLLIE的人机交互低光图像增强框架，通过迭代训练和人类视觉偏好标注提升模型输出质量。


<details>
  <summary>Details</summary>
Motivation: 解决低光图像增强（LLIE）中如何生成符合人类视觉偏好高质量图像的问题。

Method: 采用人机交互训练框架，通过人类标注和定制图像质量评估（IQA）模型学习视觉偏好，指导模型训练。

Result: 实验表明，该方法在定量和定性上显著提升了无监督LLIE模型的性能。

Conclusion: HiLLIE框架通过少量标注即可持续改进模型，生成视觉上更吸引人的增强结果。

Abstract: Developing effective approaches to generate enhanced results that align well
with human visual preferences for high-quality well-lit images remains a
challenge in low-light image enhancement (LLIE). In this paper, we propose a
human-in-the-loop LLIE training framework that improves the visual quality of
unsupervised LLIE model outputs through iterative training stages, named
HiLLIE. At each stage, we introduce human guidance into the training process
through efficient visual quality annotations of enhanced outputs. Subsequently,
we employ a tailored image quality assessment (IQA) model to learn human visual
preferences encoded in the acquired labels, which is then utilized to guide the
training process of an enhancement model. With only a small amount of pairwise
ranking annotations required at each stage, our approach continually improves
the IQA model's capability to simulate human visual assessment of enhanced
outputs, thus leading to visually appealing LLIE results. Extensive experiments
demonstrate that our approach significantly improves unsupervised LLIE model
performance in terms of both quantitative and qualitative performance. The code
and collected ranking dataset will be available at
https://github.com/LabShuHangGU/HiLLIE.

</details>

### [324] [Spotting the Unexpected (STU): A 3D LiDAR Dataset for Anomaly Segmentation in Autonomous Driving](https://arxiv.org/abs/2505.02148)
*Alexey Nekrasov,Malcolm Burdorf,Stewart Worrall,Bastian Leibe,Julie Stephany Berrio Perez*

Main category: cs.CV

TLDR: 论文提出首个公开的3D道路异常分割数据集，结合LiDAR和相机数据，支持多范围异常检测，并评估了基线模型。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要检测和处理道路上的异常物体，但现有研究多集中于2D，3D领域缺乏高质量多模态数据。

Method: 构建了一个包含密集3D语义标注、LiDAR和相机数据及序列信息的数据集，并评估了几种3D分割基线模型。

Result: 数据集填补了3D道路异常分割的空白，基线模型评估揭示了3D异常检测的挑战。

Conclusion: 公开数据集和代码将促进不同方法的测试和性能比较，推动自动驾驶安全导航的发展。

Abstract: To operate safely, autonomous vehicles (AVs) need to detect and handle
unexpected objects or anomalies on the road. While significant research exists
for anomaly detection and segmentation in 2D, research progress in 3D is
underexplored. Existing datasets lack high-quality multimodal data that are
typically found in AVs. This paper presents a novel dataset for anomaly
segmentation in driving scenarios. To the best of our knowledge, it is the
first publicly available dataset focused on road anomaly segmentation with
dense 3D semantic labeling, incorporating both LiDAR and camera data, as well
as sequential information to enable anomaly detection across various ranges.
This capability is critical for the safe navigation of autonomous vehicles. We
adapted and evaluated several baseline models for 3D segmentation, highlighting
the challenges of 3D anomaly detection in driving environments. Our dataset and
evaluation code will be openly available, facilitating the testing and
performance comparison of different approaches.

</details>

### [325] [Small Clips, Big Gains: Learning Long-Range Refocused Temporal Information for Video Super-Resolution](https://arxiv.org/abs/2505.02159)
*Xingyu Zhou,Wei Long,Jingbo Lu,Shiyin Jiang,Weiyi You,Haifeng Wu,Shuhang Gu*

Main category: cs.CV

TLDR: LRTI-VSR是一种新的循环视频超分辨率训练框架，通过长距离聚焦时间信息和改进的注意力模块，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 视频超分辨率（VSR）可以利用时间信息提升性能，但如何有效学习长视频中的长期依赖关系仍是一个挑战。

Method: 提出LRTI-VSR框架，包括长视频片段训练策略和聚焦帧内/帧间Transformer块，通过注意力模块选择有用信息。

Result: 在长视频测试集上，LRTI-VSR实现了最先进的性能，同时保持训练和计算效率。

Conclusion: LRTI-VSR通过高效利用长距离时间信息，显著提升了循环VSR模型的性能。

Abstract: Video super-resolution (VSR) can achieve better performance compared to
single image super-resolution by additionally leveraging temporal information.
In particular, the recurrent-based VSR model exploits long-range temporal
information during inference and achieves superior detail restoration. However,
effectively learning these long-term dependencies within long videos remains a
key challenge. To address this, we propose LRTI-VSR, a novel training framework
for recurrent VSR that efficiently leverages Long-Range Refocused Temporal
Information. Our framework includes a generic training strategy that utilizes
temporal propagation features from long video clips while training on shorter
video clips. Additionally, we introduce a refocused intra&inter-frame
transformer block which allows the VSR model to selectively prioritize useful
temporal information through its attention module while further improving
inter-frame information utilization in the FFN module. We evaluate LRTI-VSR on
both CNN and transformer-based VSR architectures, conducting extensive ablation
studies to validate the contribution of each component. Experiments on
long-video test sets demonstrate that LRTI-VSR achieves state-of-the-art
performance while maintaining training and computational efficiency.

</details>

### [326] [Focus What Matters: Matchability-Based Reweighting for Local Feature Matching](https://arxiv.org/abs/2505.02161)
*Dongyue Li*

Main category: cs.CV

TLDR: 提出了一种新颖的注意力重加权机制，通过分类像素为可匹配和不可匹配两类，动态调整注意力权重和输出表示，提升了匹配性能。


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制对所有像素或关键点平等处理，可能引入冗余和噪声，因此需要一种更智能的权重分配方法。

Method: 提出双设计：在softmax前注入可学习偏置项调整注意力分数，同时在注意力后根据匹配性重新缩放特征值。

Result: 在三个基准数据集上的实验表明，该方法优于现有最先进方法。

Conclusion: 通过动态调整注意力权重和特征表示，显著提升了半稠密匹配的性能。

Abstract: Since the rise of Transformers, many semi-dense matching methods have adopted
attention mechanisms to extract feature descriptors. However, the attention
weights, which capture dependencies between pixels or keypoints, are often
learned from scratch. This approach can introduce redundancy and noisy
interactions from irrelevant regions, as it treats all pixels or keypoints
equally. Drawing inspiration from keypoint selection processes, we propose to
first classify all pixels into two categories: matchable and non-matchable.
Matchable pixels are expected to receive higher attention weights, while
non-matchable ones are down-weighted. In this work, we propose a novel
attention reweighting mechanism that simultaneously incorporates a learnable
bias term into the attention logits and applies a matchability-informed
rescaling to the input value features. The bias term, injected prior to the
softmax operation, selectively adjusts attention scores based on the confidence
of query-key interactions. Concurrently, the feature rescaling acts
post-attention by modulating the influence of each value vector in the final
output. This dual design allows the attention mechanism to dynamically adjust
both its internal weighting scheme and the magnitude of its output
representations. Extensive experiments conducted on three benchmark datasets
validate the effectiveness of our method, consistently outperforming existing
state-of-the-art approaches.

</details>

### [327] [SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian Splatting](https://arxiv.org/abs/2505.02175)
*Shubhendu Jena,Shishir Reddy Vutukur,Adnane Boukhayma*

Main category: cs.CV

TLDR: 论文提出了一种基于多视角立体视觉（MVS）的学习框架，通过回归2D高斯表面元素参数，实现稀疏视角下的3D重建和新视角合成（NVS），并在性能和速度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏视角下3D重建和NVS的联合挑战，提升重建精度和实时性。

Method: 采用MVS学习框架，回归2D高斯表面元素参数，结合多视角深度视觉特征，实现稀疏视角的3D重建和NVS。

Result: 在DTU稀疏3D重建基准测试中取得最佳效果，NVS性能领先，推理速度提升近两个数量级。

Conclusion: 该方法在稀疏视角下实现了高效的3D重建和NVS，具有广泛的应用潜力。

Abstract: Recovering 3D information from scenes via multi-view stereo reconstruction
(MVS) and novel view synthesis (NVS) is inherently challenging, particularly in
scenarios involving sparse-view setups. The advent of 3D Gaussian Splatting
(3DGS) enabled real-time, photorealistic NVS. Following this, 2D Gaussian
Splatting (2DGS) leveraged perspective accurate 2D Gaussian primitive
rasterization to achieve accurate geometry representation during rendering,
improving 3D scene reconstruction while maintaining real-time performance.
Recent approaches have tackled the problem of sparse real-time NVS using 3DGS
within a generalizable, MVS-based learning framework to regress 3D Gaussian
parameters. Our work extends this line of research by addressing the challenge
of generalizable sparse 3D reconstruction and NVS jointly, and manages to
perform successfully at both tasks. We propose an MVS-based learning pipeline
that regresses 2DGS surface element parameters in a feed-forward fashion to
perform 3D shape reconstruction and NVS from sparse-view images. We further
show that our generalizable pipeline can benefit from preexisting foundational
multi-view deep visual features. The resulting model attains the
state-of-the-art results on the DTU sparse 3D reconstruction benchmark in terms
of Chamfer distance to ground-truth, as-well as state-of-the-art NVS. It also
demonstrates strong generalization on the BlendedMVS and Tanks and Temples
datasets. We note that our model outperforms the prior state-of-the-art in
feed-forward sparse view reconstruction based on volume rendering of implicit
representations, while offering an almost 2 orders of magnitude higher
inference speed.

</details>

### [328] [Saliency-Guided Training for Fingerprint Presentation Attack Detection](https://arxiv.org/abs/2505.02176)
*Samuel Webster,Adam Czajka*

Main category: cs.CV

TLDR: 该论文首次将显著性引导训练应用于指纹呈现攻击检测（PAD），通过实验验证了其在有限数据和大量数据场景下的有效性，并在LivDet-2021基准测试中取得第一名。


<details>
  <summary>Details</summary>
Motivation: 探索显著性引导训练在指纹PAD任务中的应用，以提升模型的泛化能力，尤其是在数据有限的情况下。

Method: 通过50名参与者创建了800个人工标注的指纹显著性图，并结合算法生成的伪显著性图（如基于细节、图像质量和自动编码器的显著性图），在五种不同训练场景下评估显著性引导训练的效果。

Result: 显著性引导训练在指纹PAD中表现出色，尤其在数据有限时仍能保持高准确性和泛化能力，并在LivDet-2021基准测试中排名第一。

Conclusion: 显著性引导训练在指纹PAD中具有显著潜力，能够提升模型泛化能力，并适用于不同规模的数据集。所有数据和模型均已公开以支持可重复研究。

Abstract: Saliency-guided training, which directs model learning to important regions
of images, has demonstrated generalization improvements across various
biometric presentation attack detection (PAD) tasks. This paper presents its
first application to fingerprint PAD. We conducted a 50-participant study to
create a dataset of 800 human-annotated fingerprint perceptually-important
maps, explored alongside algorithmically-generated "pseudosaliency," including
minutiae-based, image quality-based, and autoencoder-based saliency maps.
Evaluating on the 2021 Fingerprint Liveness Detection Competition testing set,
we explore various configurations within five distinct training scenarios to
assess the impact of saliency-guided training on accuracy and generalization.
Our findings demonstrate the effectiveness of saliency-guided training for
fingerprint PAD in both limited and large data contexts, and we present a
configuration capable of earning the first place on the LivDet-2021 benchmark.
Our results highlight saliency-guided training's promise for increased model
generalization capabilities, its effectiveness when data is limited, and its
potential to scale to larger datasets in fingerprint PAD. All collected
saliency data and trained models are released with the paper to support
reproducible research.

</details>

### [329] [Using Knowledge Graphs to harvest datasets for efficient CLIP model training](https://arxiv.org/abs/2505.02746)
*Simon Ging,Sebastian Walter,Jelena Bratulić,Johannes Dienert,Hannah Bast,Thomas Brox*

Main category: cs.CV

TLDR: 通过智能网络搜索和知识图谱增强策略，用较少数据训练高质量CLIP模型，并推出EntityNet数据集。


<details>
  <summary>Details</summary>
Motivation: 解决大规模数据集训练CLIP模型的高成本和领域适应性不足问题。

Method: 采用智能网络搜索和知识图谱增强策略，减少数据需求。

Result: 成功用10M图像训练专家模型，推出33M图像的EntityNet数据集。

Conclusion: 该方法显著降低训练成本，提升领域适应性。

Abstract: Training high-quality CLIP models typically requires enormous datasets, which
limits the development of domain-specific models -- especially in areas that
even the largest CLIP models do not cover well -- and drives up training costs.
This poses challenges for scientific research that needs fine-grained control
over the training procedure of CLIP models. In this work, we show that by
employing smart web search strategies enhanced with knowledge graphs, a robust
CLIP model can be trained from scratch with considerably less data.
Specifically, we demonstrate that an expert foundation model for living
organisms can be built using just 10M images. Moreover, we introduce EntityNet,
a dataset comprising 33M images paired with 46M text descriptions, which
enables the training of a generic CLIP model in significantly reduced time.

</details>

### [330] [Sparfels: Fast Reconstruction from Sparse Unposed Imagery](https://arxiv.org/abs/2505.02178)
*Shubhendu Jena,Amine Ouasfi,Mae Younes,Adnane Boukhayma*

Main category: cs.CV

TLDR: 提出了一种基于表面元素抛射的稀疏视图重建方法，可在消费级GPU上3分钟内完成。通过利用3D基础模型的任务头（如点图和相机初始化），结合2D高斯抛射（2DGS）模型和图像对应关系，实现高效形状重建。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角下的形状恢复研究较少，现有方法多依赖数据先验或外部单目几何先验。本文旨在提出一种简单高效的解决方案。

Method: 利用3D基础模型的任务头初始化2D高斯抛射模型，通过图像对应关系优化相机参数，并提出一种新的颜色方差计算方式以提高重建精度。

Result: 在稀疏未校准设置下，重建和新视角合成任务中达到最先进性能。

Conclusion: 该方法通过高效计算和优化策略，显著提升了稀疏视角下的形状重建质量。

Abstract: We present a method for Sparse view reconstruction with surface element
splatting that runs within 3 minutes on a consumer grade GPU. While few methods
address sparse radiance field learning from noisy or unposed sparse cameras,
shape recovery remains relatively underexplored in this setting. Several
radiance and shape learning test-time optimization methods address the sparse
posed setting by learning data priors or using combinations of external
monocular geometry priors. Differently, we propose an efficient and simple
pipeline harnessing a single recent 3D foundation model. We leverage its
various task heads, notably point maps and camera initializations to
instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image
correspondences to guide camera optimization midst 2DGS training. Key to our
contribution is a novel formulation of splatted color variance along rays,
which can be computed efficiently. Reducing this moment in training leads to
more accurate shape reconstructions. We demonstrate state-of-the-art
performances in the sparse uncalibrated setting in reconstruction and novel
view benchmarks based on established multi-view datasets.

</details>

### [331] [ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection in Video Surveillance Applications](https://arxiv.org/abs/2505.02179)
*Tao Zhu,Qi Yu,Xinru Dong,Shiyu Li,Yue Liu,Jinlong Jiang,Lei Shu*

Main category: cs.CV

TLDR: ProDisc-VAD提出了一种高效的弱监督视频异常检测框架，通过原型交互层和伪实例判别增强损失解决标签模糊问题，性能优异且参数极少。


<details>
  <summary>Details</summary>
Motivation: 弱监督视频异常检测（WS-VAD）在多实例学习（MIL）中存在标签模糊问题，阻碍了判别性特征的学习。

Method: 提出ProDisc-VAD框架，包含原型交互层（PIL）和伪实例判别增强（PIDE）损失，前者通过可学习原型建模正常性，后者通过对比学习增强分离性。

Result: 在ShanghaiTech和UCF-Crime数据集上分别达到97.98%和87.12%的AUC，仅使用0.4M参数，效率远超ViT-based方法。

Conclusion: ProDisc-VAD在性能和效率上均达到先进水平，为解决WS-VAD中的标签模糊问题提供了有效方案。

Abstract: Weakly-supervised video anomaly detection (WS-VAD) using Multiple Instance
Learning (MIL) suffers from label ambiguity, hindering discriminative feature
learning. We propose ProDisc-VAD, an efficient framework tackling this via two
synergistic components. The Prototype Interaction Layer (PIL) provides
controlled normality modeling using a small set of learnable prototypes,
establishing a robust baseline without being overwhelmed by dominant normal
data. The Pseudo-Instance Discriminative Enhancement (PIDE) loss boosts
separability by applying targeted contrastive learning exclusively to the most
reliable extreme-scoring instances (highest/lowest scores). ProDisc-VAD
achieves strong AUCs (97.98% ShanghaiTech, 87.12% UCF-Crime) using only 0.4M
parameters, over 800x fewer than recent ViT-based methods like VadCLIP,
demonstrating exceptional efficiency alongside state-of-the-art performance.
Code is available at https://github.com/modadundun/ProDisc-VAD.

</details>

### [332] [Robust AI-Generated Face Detection with Imbalanced Data](https://arxiv.org/abs/2505.02182)
*Yamini Sri Krubha,Aryana Hou,Braden Vester,Web Walker,Xin Wang,Li Lin,Shu Hu*

Main category: cs.CV

TLDR: 论文提出了一种结合动态损失重加权和基于排序优化的框架，用于解决深度伪造检测中的分布偏移和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术从研究娱乐工具演变为恶意工具，威胁数字信任。现有检测方法在处理新兴生成模型的分布偏移和类别不平衡时表现不足。

Method: 提出动态损失重加权和基于排序优化的框架，提升检测器的泛化能力和性能。

Result: 该方法在类别不平衡的数据集条件下表现出优越的泛化能力和检测性能。

Conclusion: 该框架为深度伪造检测中的分布偏移和类别不平衡问题提供了有效解决方案。

Abstract: Deepfakes, created using advanced AI techniques such as Variational
Autoencoder and Generative Adversarial Networks, have evolved from research and
entertainment applications into tools for malicious activities, posing
significant threats to digital trust. Current deepfake detection techniques
have evolved from CNN-based methods focused on local artifacts to more advanced
approaches using vision transformers and multimodal models like CLIP, which
capture global anomalies and improve cross-domain generalization. Despite
recent progress, state-of-the-art deepfake detectors still face major
challenges in handling distribution shifts from emerging generative models and
addressing severe class imbalance between authentic and fake samples in
deepfake datasets, which limits their robustness and detection accuracy. To
address these challenges, we propose a framework that combines dynamic loss
reweighting and ranking-based optimization, which achieves superior
generalization and performance under imbalanced dataset conditions. The code is
available at https://github.com/Purdue-M2/SP_CUP.

</details>

### [333] [AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation](https://arxiv.org/abs/2505.02830)
*Qingqiu Li,Zihang Cui,Seongsu Bae,Jilan Xu,Runtian Yuan,Yuejie Zhang,Rui Feng,Quanli Shen,Xiaobo Zhang,Junjun He,Shujun Wang*

Main category: cs.CV

TLDR: 论文提出了一种基于解剖学导向推理（AOR）的框架，通过多步推理提升医学大模型（MLMMs）在胸部X光片（CXRs）解释中的区域级理解和交互性。


<details>
  <summary>Details</summary>
Motivation: 当前医学大模型在胸部X光片解释中存在区域级理解不足和单步推理导致的准确性及可解释性受限的问题。

Method: 提出解剖学导向推理（AOR）框架，结合跨模态区域级信息进行多步推理，并开发了AOR-Instruction数据集用于模型训练。

Result: 实验表明AOR在视觉问答和报告生成任务中表现优异。

Conclusion: AOR框架显著提升了医学大模型的交互性和可解释性。

Abstract: Chest X-rays (CXRs) are the most frequently performed imaging examinations in
clinical settings. Recent advancements in Large Multimodal Models (LMMs) have
enabled automated CXR interpretation, enhancing diagnostic accuracy and
efficiency. However, despite their strong visual understanding, current Medical
LMMs (MLMMs) still face two major challenges: (1) Insufficient region-level
understanding and interaction, and (2) Limited accuracy and interpretability
due to single-step reasoning. In this paper, we empower MLMMs with
anatomy-centric reasoning capabilities to enhance their interactivity and
explainability. Specifically, we first propose an Anatomical Ontology-Guided
Reasoning (AOR) framework, which centers on cross-modal region-level
information to facilitate multi-step reasoning. Next, under the guidance of
expert physicians, we develop AOR-Instruction, a large instruction dataset for
MLMMs training. Our experiments demonstrate AOR's superior performance in both
VQA and report generation tasks.

</details>

### [334] [DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization](https://arxiv.org/abs/2505.02192)
*Wenchuan Wang,Mengqi Huang,Yijing Tu,Zhendong Mao*

Main category: cs.CV

TLDR: DualReal框架通过联合训练解决身份与运动定制中的冲突，提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法孤立定制身份或运动，忽略其相互约束，导致生成过程中冲突。

Method: DualReal采用双单元设计：Dual-aware Adaptation动态选择训练阶段，StageBlender Controller通过去噪阶段和Diffusion Transformer深度指导不同维度。

Result: 实验显示DualReal在CLIP-I和DINO-I指标上分别提升21.7%和31.8%，运动质量指标领先。

Conclusion: DualReal通过协同训练实现身份与运动模式的无损融合，显著优于现有方法。

Abstract: Customized text-to-video generation with pre-trained large-scale models has
recently garnered significant attention through focusing on identity and motion
consistency. Existing works typically follow the isolated customized paradigm,
where the subject identity or motion dynamics are customized exclusively.
However, this paradigm completely ignores the intrinsic mutual constraints and
synergistic interdependencies between identity and motion, resulting in
identity-motion conflicts throughout the generation process that systematically
degrades. To address this, we introduce DualReal, a novel framework that,
employs adaptive joint training to collaboratively construct interdependencies
between dimensions. Specifically, DualReal is composed of two units: (1)
Dual-aware Adaptation dynamically selects a training phase (i.e., identity or
motion), learns the current information guided by the frozen dimension prior,
and employs a regularization strategy to avoid knowledge leakage; (2)
StageBlender Controller leverages the denoising stages and Diffusion
Transformer depths to guide different dimensions with adaptive granularity,
avoiding conflicts at various stages and ultimately achieving lossless fusion
of identity and motion patterns. We constructed a more comprehensive benchmark
than existing methods. The experimental results show that DualReal improves
CLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top
performance on nearly all motion quality metrics.

</details>

### [335] [R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning](https://arxiv.org/abs/2505.02835)
*Yi-Fan Zhang,Xingyu Lu,Xiao Hu,Chaoyou Fu,Bin Wen,Tianke Zhang,Changyi Liu,Kaiyu Jiang,Kaibing Chen,Kaiyu Tang,Haojie Ding,Jiankang Chen,Fan Yang,Zhang Zhang,Tingting Gao,Liang Wang*

Main category: cs.CV

TLDR: 论文探讨了如何通过强化学习（RL）改进多模态奖励模型（MRMs），提出了一种名为StableReinforce的算法，显著提升了训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有MRMs的研究主要集中在模型结构和训练数据上，而对长期推理能力及其激活方式的探索不足。本文旨在通过RL优化奖励建模。

Method: 将奖励建模问题重新表述为基于规则的RL任务，并提出StableReinforce算法，改进训练损失、优势估计策略和奖励设计。

Result: 提出的R1-Reward模型在VL Reward-Bench和Multimodal Reward Bench上分别提升了8.4%和14.3%的性能。

Conclusion: StableReinforce算法有效提升了MRMs的性能，展示了RL在优化多模态奖励模型中的潜力。

Abstract: Multimodal Reward Models (MRMs) play a crucial role in enhancing the
performance of Multimodal Large Language Models (MLLMs). While recent
advancements have primarily focused on improving the model structure and
training data of MRMs, there has been limited exploration into the
effectiveness of long-term reasoning capabilities for reward modeling and how
to activate these capabilities in MRMs. In this paper, we explore how
Reinforcement Learning (RL) can be used to improve reward modeling.
Specifically, we reformulate the reward modeling problem as a rule-based RL
task. However, we observe that directly applying existing RL algorithms, such
as Reinforce++, to reward modeling often leads to training instability or even
collapse due to the inherent limitations of these algorithms. To address this
issue, we propose the StableReinforce algorithm, which refines the training
loss, advantage estimation strategy, and reward design of existing RL methods.
These refinements result in more stable training dynamics and superior
performance. To facilitate MRM training, we collect 200K preference data from
diverse datasets. Our reward model, R1-Reward, trained using the
StableReinforce algorithm on this dataset, significantly improves performance
on multimodal reward modeling benchmarks. Compared to previous SOTA models,
R1-Reward achieves a $8.4\%$ improvement on the VL Reward-Bench and a $14.3\%$
improvement on the Multimodal Reward Bench. Moreover, with more inference
compute, R1-Reward's performance is further enhanced, highlighting the
potential of RL algorithms in optimizing MRMs.

</details>

### [336] [Improving Physical Object State Representation in Text-to-Image Generative Systems](https://arxiv.org/abs/2505.02236)
*Tianle Chen,Chaitanya Chakka,Deepti Ghadiyaram*

Main category: cs.CV

TLDR: 论文提出了一种自动生成高质量合成数据的方法，用于改进文本到图像生成模型对物体状态的准确表示，并通过微调模型在公开数据集上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型在准确表示物体状态（如“没有瓶子的桌子”、“空的杯子”）方面存在困难。

Method: 设计了一个全自动管道生成高质量合成数据，并微调多个开源文本到图像模型。

Result: 在公开数据集GenAI-Bench上平均绝对提升8%以上，在自建数据集上平均提升24%以上。

Conclusion: 通过合成数据和微调方法显著提升了模型对物体状态的表示能力，并公开了评估提示和代码。

Abstract: Current text-to-image generative models struggle to accurately represent
object states (e.g., "a table without a bottle," "an empty tumbler"). In this
work, we first design a fully-automatic pipeline to generate high-quality
synthetic data that accurately captures objects in varied states. Next, we
fine-tune several open-source text-to-image models on this synthetic data. We
evaluate the performance of the fine-tuned models by quantifying the alignment
of the generated images to their prompts using GPT4o-mini, and achieve an
average absolute improvement of 8+% across four models on the public
GenAI-Bench dataset. We also curate a collection of 200 prompts with a specific
focus on common objects in various physical states. We demonstrate a
significant improvement of an average of 24+% over the baseline on this
dataset. We release all evaluation prompts and code.

</details>

### [337] [Quantizing Diffusion Models from a Sampling-Aware Perspective](https://arxiv.org/abs/2505.02242)
*Qian Zeng,Jie Song,Yuanyu Wan,Huiqiong Wang,Mingli Song*

Main category: cs.CV

TLDR: 本文提出了一种采样感知的量化策略，通过混合阶轨迹对齐技术解决量化噪声对高阶采样器的影响，实现高效且高质量的视觉生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视觉生成任务中表现优异，但其长链去噪和高计算需求限制了在低延迟和资源受限环境中的应用。现有研究未能同时解决这些问题。

Method: 提出采样感知量化策略，结合混合阶轨迹对齐技术，严格约束每一步采样的误差范围，优化概率流。

Result: 实验表明，该方法在快速采样中保持了高速采样器的快速收敛特性，同时生成质量优越。

Conclusion: 该方法为扩散模型在资源受限环境中的应用提供了高效且高质量的解决方案。

Abstract: Diffusion models have recently emerged as the dominant approach in visual
generation tasks. However, the lengthy denoising chains and the computationally
intensive noise estimation networks hinder their applicability in low-latency
and resource-limited environments. Previous research has endeavored to address
these limitations in a decoupled manner, utilizing either advanced samplers or
efficient model quantization techniques. In this study, we uncover that
quantization-induced noise disrupts directional estimation at each sampling
step, further distorting the precise directional estimations of higher-order
samplers when solving the sampling equations through discretized numerical
methods, thereby altering the optimal sampling trajectory. To attain dual
acceleration with high fidelity, we propose a sampling-aware quantization
strategy, wherein a Mixed-Order Trajectory Alignment technique is devised to
impose a more stringent constraint on the error bounds at each sampling step,
facilitating a more linear probability flow. Extensive experiments on
sparse-step fast sampling across multiple datasets demonstrate that our
approach preserves the rapid convergence characteristics of high-speed samplers
while maintaining superior generation quality. Code will be made publicly
available soon.

</details>

### [338] [Cricket: A Self-Powered Chirping Pixel](https://arxiv.org/abs/2505.02246)
*Shree K. Nayar,Jeremy Klotz,Nikhil Nanda,Mikhail Fridberg*

Main category: cs.CV

TLDR: 一种名为cricket的无电池传感器，利用光能供电，通过无线射频信号测量光强并传输数据。


<details>
  <summary>Details</summary>
Motivation: 开发无需外部电源或电池的传感器，实现光能自供电和无线通信。

Method: 传感器通过收集光能工作，能量达到阈值时发送射频信号，信号间隔反映光强。

Result: 验证了传感器的性能，展示了其在太阳能追踪和无电池传感器阵列中的应用。

Conclusion: cricket传感器在光能自供电和无线通信方面具有潜力，适用于多种应用场景。

Abstract: We present a sensor that can measure light and wirelessly communicate the
measurement, without the need for an external power source or a battery. Our
sensor, called cricket, harvests energy from incident light. It is asleep for
most of the time and transmits a short and strong radio frequency chirp when
its harvested energy reaches a specific level. The carrier frequency of each
cricket is fixed and reveals its identity, and the duration between consecutive
chirps is a measure of the incident light level. We have characterized the
radiometric response function, signal-to-noise ratio and dynamic range of
cricket. We have experimentally verified that cricket can be miniaturized at
the expense of increasing the duration between chirps. We show that a cube with
a cricket on each of its sides can be used to estimate the centroid of any
complex illumination, which has value in applications such as solar tracking.
We also demonstrate the use of crickets for creating untethered sensor arrays
that can produce video and control lighting for energy conservation. Finally,
we modified cricket's circuit to develop battery-free electronic sunglasses
that can instantly adapt to environmental illumination.

</details>

### [339] [Enhancing AI Face Realism: Cost-Efficient Quality Improvement in Distilled Diffusion Models with a Fully Synthetic Dataset](https://arxiv.org/abs/2505.02255)
*Jakub Wąsala,Bartłomiej Wrzalski,Kornelia Noculak,Yuliia Tarasenko,Oliwer Krupa,Jan Kocoń,Grzegorz Chodak*

Main category: cs.CV

TLDR: 提出了一种新方法，通过训练图像翻译模型提升扩散模型生成图像的成本效益比，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决扩散模型生成高质量图像时计算成本高的问题，探索如何通过蒸馏模型与增强层的结合降低成本。

Method: 生成合成配对数据集，训练图像翻译模型，将蒸馏模型（如FLUX.1-schnell）的输出提升至与基线模型（如FLUX.1-dev）相当的质量。

Result: 结合蒸馏模型与增强层的方案，在保持肖像生成质量的同时，计算成本降低了82%。

Conclusion: 该方法展示了在大规模图像生成任务中提升AI解决方案效率的潜力。

Abstract: This study presents a novel approach to enhance the cost-to-quality ratio of
image generation with diffusion models. We hypothesize that differences between
distilled (e.g. FLUX.1-schnell) and baseline (e.g. FLUX.1-dev) models are
consistent and, therefore, learnable within a specialized domain, like portrait
generation. We generate a synthetic paired dataset and train a fast
image-to-image translation head. Using two sets of low- and high-quality
synthetic images, our model is trained to refine the output of a distilled
generator (e.g., FLUX.1-schnell) to a level comparable to a baseline model like
FLUX.1-dev, which is more computationally intensive. Our results show that the
pipeline, which combines a distilled version of a large generative model with
our enhancement layer, delivers similar photorealistic portraits to the
baseline version with up to an 82% decrease in computational cost compared to
FLUX.1-dev. This study demonstrates the potential for improving the efficiency
of AI solutions involving large-scale image generation.

</details>

### [340] [Compositional Image-Text Matching and Retrieval by Grounding Entities](https://arxiv.org/abs/2505.02278)
*Madhukar Reddy Vongala,Saurabh Srivastava,Jana Košecká*

Main category: cs.CV

TLDR: 提出了一种无需训练的方法，通过动态调整CLIP嵌入的子图像和关系嵌入，提升了图像-文本匹配的准确性和检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型（如CLIP）在实体定位和组合图像-文本匹配方面表现不足，需要改进。

Method: 利用开放词汇检测器定位对象实体和关系，计算子图像的嵌入并动态调整全局图像嵌入。

Result: 在Visual Genome和SVO Probes数据集上图像-文本匹配准确率平均提升1.5%，在Flickr30K和MS-COCO检索任务中Recall@1分别提升12%和0.4%。

Conclusion: 提出的方法显著提升了CLIP在组合任务中的性能，且无需额外训练。

Abstract: Vision-language pretraining on large datasets of images-text pairs is one of
the main building blocks of current Vision-Language Models. While with
additional training, these models excel in various downstream tasks, including
visual question answering, image captioning, and visual commonsense reasoning.
However, a notable weakness of pretrained models like CLIP, is their inability
to perform entity grounding and compositional image and text
matching~\cite{Jiang2024ComCLIP, yang2023amc, Rajabi2023GroundedVSR,
learninglocalizeCVPR24}. In this work we propose a novel learning-free
zero-shot augmentation of CLIP embeddings that has favorable compositional
properties. We compute separate embeddings of sub-images of object entities and
relations that are localized by the state of the art open vocabulary detectors
and dynamically adjust the baseline global image embedding. % The final
embedding is obtained by computing a weighted combination of the sub-image
embeddings. The resulting embedding is then utilized for similarity computation
with text embedding, resulting in a average 1.5\% improvement in image-text
matching accuracy on the Visual Genome and SVO Probes
datasets~\cite{krishna2017visualgenome, svo}. Notably, the enhanced embeddings
demonstrate superior retrieval performance, thus achieving significant gains on
the Flickr30K and MS-COCO retrieval benchmarks~\cite{flickr30ke, mscoco},
improving the state-of-the-art Recall@1 by 12\% and 0.4\%, respectively. Our
code is available at https://github.com/madhukarreddyvongala/GroundingCLIP.

</details>

### [341] [Continuous Normalizing Flows for Uncertainty-Aware Human Pose Estimation](https://arxiv.org/abs/2505.02287)
*Shipeng Liu,Ziliang Xiong,Bastian Wandt,Per-Erik Forssén*

Main category: cs.CV

TLDR: 论文提出了一种结合连续归一化流（CNFs）的回归模型CFRE，用于提升人体姿态估计的准确性和不确定性量化，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前人体姿态估计方法在准确性、计算效率和不确定性量化之间存在平衡问题，传统回归方法假设固定分布可能导致UQ不佳，而热图方法资源消耗大。

Method: 提出Continuous Flow Residual Estimation (CFRE)，将连续归一化流（CNFs）集成到回归模型中，实现动态分布适应。

Result: 实验表明，CFRE在2D和3D人体姿态估计任务中实现了更好的准确性和不确定性量化，同时保持了计算效率。

Conclusion: CFRE是一种有效的方法，解决了现有方法在人体姿态估计中的局限性。

Abstract: Human Pose Estimation (HPE) is increasingly important for applications like
virtual reality and motion analysis, yet current methods struggle with
balancing accuracy, computational efficiency, and reliable uncertainty
quantification (UQ). Traditional regression-based methods assume fixed
distributions, which might lead to poor UQ. Heatmap-based methods effectively
model the output distribution using likelihood heatmaps, however, they demand
significant resources. To address this, we propose Continuous Flow Residual
Estimation (CFRE), an integration of Continuous Normalizing Flows (CNFs) into
regression-based models, which allows for dynamic distribution adaptation.
Through extensive experiments, we show that CFRE leads to better accuracy and
uncertainty quantification with retained computational efficiency on both 2D
and 3D human pose estimation tasks.

</details>

### [342] [TeDA: Boosting Vision-Lanuage Models for Zero-Shot 3D Object Retrieval via Testing-time Distribution Alignment](https://arxiv.org/abs/2505.02325)
*Zhichuan Wang,Yang Zhou,Jinhai Xiang,Yulong Wang,Xinwei He*

Main category: cs.CV

TLDR: TeDA框架通过测试时分布对齐，将预训练的2D视觉语言模型CLIP适配于3D对象检索，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法因3D训练数据不足难以泛化到未知类别，而CLIP等模型在2D与3D分布差异下表现受限。

Method: TeDA将3D对象投影为多视图图像，利用CLIP提取特征，并通过迭代优化和文本描述增强3D表示。

Result: 在四个开放集3D检索基准上，TeDA显著优于现有方法，包括需大量训练的方法。

Conclusion: TeDA首次实现视觉语言模型在3D特征学习中的测试时适配，验证了其高效性和泛化能力。

Abstract: Learning discriminative 3D representations that generalize well to unknown
testing categories is an emerging requirement for many real-world 3D
applications. Existing well-established methods often struggle to attain this
goal due to insufficient 3D training data from broader concepts. Meanwhile,
pre-trained large vision-language models (e.g., CLIP) have shown remarkable
zero-shot generalization capabilities. Yet, they are limited in extracting
suitable 3D representations due to substantial gaps between their 2D training
and 3D testing distributions. To address these challenges, we propose
Testing-time Distribution Alignment (TeDA), a novel framework that adapts a
pretrained 2D vision-language model CLIP for unknown 3D object retrieval at
test time. To our knowledge, it is the first work that studies the test-time
adaptation of a vision-language model for 3D feature learning. TeDA projects 3D
objects into multi-view images, extracts features using CLIP, and refines 3D
query embeddings with an iterative optimization strategy by confident
query-target sample pairs in a self-boosting manner. Additionally, TeDA
integrates textual descriptions generated by a multimodal language model
(InternVL) to enhance 3D object understanding, leveraging CLIP's aligned
feature space to fuse visual and textual cues. Extensive experiments on four
open-set 3D object retrieval benchmarks demonstrate that TeDA greatly
outperforms state-of-the-art methods, even those requiring extensive training.
We also experimented with depth maps on Objaverse-LVIS, further validating its
effectiveness. Code is available at https://github.com/wangzhichuan123/TeDA.

</details>

### [343] [VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection](https://arxiv.org/abs/2505.02331)
*Hao Cheng,Zhiwei Zhao,Yichao He,Zhenzhen Hu,Jia Li,Meng Wang,Richang Hong*

Main category: cs.CV

TLDR: VAEmo是一个两阶段框架，通过外部知识注入和统一跨模态编码，提升视听情感识别的性能。


<details>
  <summary>Details</summary>
Motivation: 解决视听情感识别中情感表达模糊、跨模态差异和标注数据稀缺的问题。

Method: 第一阶段通过掩码重建和对比目标预训练统一表示网络；第二阶段利用多模态大语言模型生成情感描述，并通过双路径对比学习对齐文本和视听表示。

Result: VAEmo在多个基准测试中达到最先进性能，证明了其高效和泛化能力。

Conclusion: VAEmo通过统一跨模态编码和情感感知语义指导，实现了高效且通用的视听情感表示。

Abstract: Audiovisual emotion recognition (AVER) aims to infer human emotions from
nonverbal visual-audio (VA) cues, offering modality-complementary and
language-agnostic advantages. However, AVER remains challenging due to the
inherent ambiguity of emotional expressions, cross-modal expressive
disparities, and the scarcity of reliably annotated data. Recent
self-supervised AVER approaches have introduced strong multimodal
representations, yet they predominantly rely on modality-specific encoders and
coarse content-level alignment, limiting fine-grained emotional semantic
modeling. To address these issues, we propose VAEmo, an efficient two-stage
framework for emotion-centric joint VA representation learning with external
knowledge injection. In Stage 1, a unified and lightweight representation
network is pre-trained on large-scale speaker-centric VA corpora via masked
reconstruction and contrastive objectives, mitigating the modality gap and
learning expressive, complementary representations without emotion labels. In
Stage 2, multimodal large language models automatically generate detailed
affective descriptions according to our well-designed chain-of-thought
prompting for only a small subset of VA samples; these rich textual semantics
are then injected by aligning their corresponding embeddings with VA
representations through dual-path contrastive learning, further bridging the
emotion gap. Extensive experiments on multiple downstream AVER benchmarks show
that VAEmo achieves state-of-the-art performance with a compact design,
highlighting the benefit of unified cross-modal encoding and emotion-aware
semantic guidance for efficient, generalizable VA emotion representations.

</details>

### [344] [6D Pose Estimation on Spoons and Hands](https://arxiv.org/abs/2505.02335)
*Kevin Tan,Fan Yang,Yuhao Chen*

Main category: cs.CV

TLDR: 论文提出了一种通过6D姿态估计跟踪手和勺子的系统，用于分析饮食行为，并评估了两种视频对象分割模型的性能。


<details>
  <summary>Details</summary>
Motivation: 准确的饮食监测对促进健康饮食习惯至关重要，但现有方法（如自我报告）不可靠。通过跟踪餐具和手的位置与方向，可以更可靠地估计食物摄入量。

Method: 系统通过分析静态视频，使用6D姿态估计跟踪手和勺子的空间位置与方向，并评估两种先进的视频对象分割模型。

Result: 研究定量和定性分析了两种模型的性能，并识别了系统中的主要误差来源。

Conclusion: 该系统为饮食监测提供了可靠的技术支持，但需进一步优化以减少误差。

Abstract: Accurate dietary monitoring is essential for promoting healthier eating
habits. A key area of research is how people interact and consume food using
utensils and hands. By tracking their position and orientation, it is possible
to estimate the volume of food being consumed, or monitor eating behaviours,
highly useful insights into nutritional intake that can be more reliable than
popular methods such as self-reporting. Hence, this paper implements a system
that analyzes stationary video feed of people eating, using 6D pose estimation
to track hand and spoon movements to capture spatial position and orientation.
In doing so, we examine the performance of two state-of-the-art (SOTA) video
object segmentation (VOS) models, both quantitatively and qualitatively, and
identify main sources of error within the system.

</details>

### [345] [Quaternion Infrared Visible Image Fusion](https://arxiv.org/abs/2505.02364)
*Weihua Yang,Yicong Zhou*

Main category: cs.CV

TLDR: 提出了一种基于四元数的红外-可见光图像融合框架（QIVIF），通过四元数域处理，解决了现有方法在低质量可见光输入下的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 红外和可见光图像在光照条件不同时各有优势，但现有融合方法忽略了可见光图像的颜色结构信息，且在低质量输入下表现不佳。

Method: QIVIF框架包括四元数低可见性特征学习模型、四元数自适应非锐化掩蔽方法和四元数层次贝叶斯融合模型。

Result: 实验表明，QIVIF在低可见性条件下优于现有方法。

Conclusion: QIVIF通过四元数域处理，显著提升了融合图像的质量。

Abstract: Visible images provide rich details and color information only under
well-lighted conditions while infrared images effectively highlight thermal
targets under challenging conditions such as low visibility and adverse
weather. Infrared-visible image fusion aims to integrate complementary
information from infrared and visible images to generate a high-quality fused
image. Existing methods exhibit critical limitations such as neglecting color
structure information in visible images and performance degradation when
processing low-quality color-visible inputs. To address these issues, we
propose a quaternion infrared-visible image fusion (QIVIF) framework to
generate high-quality fused images completely in the quaternion domain. QIVIF
proposes a quaternion low-visibility feature learning model to adaptively
extract salient thermal targets and fine-grained texture details from input
infrared and visible images respectively under diverse degraded conditions.
QIVIF then develops a quaternion adaptive unsharp masking method to adaptively
improve high-frequency feature enhancement with balanced illumination. QIVIF
further proposes a quaternion hierarchical Bayesian fusion model to integrate
infrared saliency and enhanced visible details to obtain high-quality fused
images. Extensive experiments across diverse datasets demonstrate that our
QIVIF surpasses state-of-the-art methods under challenging low-visibility
conditions.

</details>

### [346] [Quaternion Multi-focus Color Image Fusion](https://arxiv.org/abs/2505.02365)
*Weihua Yang,Yicong Zhou*

Main category: cs.CV

TLDR: 提出了一种基于四元数的多焦点彩色图像融合框架，通过四元数稀疏分解模型、基-细节融合策略和结构相似性细化策略，显著提升了融合质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂真实场景时，因颜色信息和纹理处理的局限性而表现不佳。

Method: 1) 四元数稀疏分解模型迭代学习细节和结构信息；2) 基-细节融合策略分别处理基尺度和细节尺度；3) 结构相似性细化策略自适应优化融合结果。

Result: 实验表明，该方法优于现有技术。

Conclusion: 该框架在保留细节和结构一致性方面表现优异，适用于复杂场景。

Abstract: Multi-focus color image fusion refers to integrating multiple partially
focused color images to create a single all-in-focus color image. However,
existing methods struggle with complex real-world scenarios due to limitations
in handling color information and intricate textures. To address these
challenges, this paper proposes a quaternion multi-focus color image fusion
framework to perform high-quality color image fusion completely in the
quaternion domain. This framework introduces 1) a quaternion sparse
decomposition model to jointly learn fine-scale image details and structure
information of color images in an iterative fashion for high-precision focus
detection, 2) a quaternion base-detail fusion strategy to individually fuse
base-scale and detail-scale results across multiple color images for preserving
structure and detail information, and 3) a quaternion structural similarity
refinement strategy to adaptively select optimal patches from initial fusion
results and obtain the final fused result for preserving fine details and
ensuring spatially consistent outputs. Extensive experiments demonstrate that
the proposed framework outperforms state-of-the-art methods.

</details>

### [347] [SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing](https://arxiv.org/abs/2505.02370)
*Ming Li,Xin Gu,Fan Chen,Xiaoying Xing,Longyin Wen,Chen Chen,Sijie Zhu*

Main category: cs.CV

TLDR: 提出了一种通过构建更有效的编辑指令来解决现有数据集中噪声监督信号问题的方法，包括修正指令和对比指令，显著提升了编辑模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集由于自动化构建方法导致编辑指令与图像对不匹配，产生噪声监督信号，而现有方法未能解决这一根本问题。

Method: 通过分析编辑模型在不同推理步骤的生成属性，定义统一指导修正指令；进一步构建对比监督信号（正负指令）并使用三元组损失训练模型。

Result: 在多个基准测试中显著优于现有方法，例如在Real-Edit基准上比SOTA SmartEdit提升9.19%，且训练数据减少30倍，模型规模缩小13倍。

Conclusion: 该方法无需依赖VLM模块或预训练任务，提供了一种更直接、高效的监督信号解决方案，为基于指令的图像编辑提供了新颖、简单且有效的方法。

Abstract: Due to the challenges of manually collecting accurate editing data, existing
datasets are typically constructed using various automated methods, leading to
noisy supervision signals caused by the mismatch between editing instructions
and original-edited image pairs. Recent efforts attempt to improve editing
models through generating higher-quality edited images, pre-training on
recognition tasks, or introducing vision-language models (VLMs) but fail to
resolve this fundamental issue. In this paper, we offer a novel solution by
constructing more effective editing instructions for given image pairs. This
includes rectifying the editing instructions to better align with the
original-edited image pairs and using contrastive editing instructions to
further enhance their effectiveness. Specifically, we find that editing models
exhibit specific generation attributes at different inference steps,
independent of the text. Based on these prior attributes, we define a unified
guide for VLMs to rectify editing instructions. However, there are some
challenging editing scenarios that cannot be resolved solely with rectified
instructions. To this end, we further construct contrastive supervision signals
with positive and negative instructions and introduce them into the model
training using triplet loss, thereby further facilitating supervision
effectiveness. Our method does not require the VLM modules or pre-training
tasks used in previous work, offering a more direct and efficient way to
provide better supervision signals, and providing a novel, simple, and
effective solution for instruction-based image editing. Results on multiple
benchmarks demonstrate that our method significantly outperforms existing
approaches. Compared with previous SOTA SmartEdit, we achieve 9.19%
improvements on the Real-Edit benchmark with 30x less training data and 13x
smaller model size.

</details>

### [348] [MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans](https://arxiv.org/abs/2505.02388)
*Huangyue Yu,Baoxiong Jia,Yixin Chen,Yandan Yang,Puhao Li,Rongpeng Su,Jiaxin Li,Qing Li,Wei Liang,Song-Chun Zhu,Tengyu Liu,Siyuan Huang*

Main category: cs.CV

TLDR: MetaScenes是一个基于真实扫描的大规模可模拟3D场景数据集，结合Scan2Sim模型，实现了自动化高质量资产替换，减少了对人工设计的依赖，并提出了两个基准任务验证其效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D场景数据集依赖人工设计、难以扩展的问题，支持更通用的AI学习和仿真到现实的迁移。

Method: 提出MetaScenes数据集和Scan2Sim模型，通过多模态对齐实现自动化资产替换，并设计两个基准任务验证数据集效果。

Result: MetaScenes支持更通用的AI学习和仿真到现实的应用，验证了其在场景合成和跨领域迁移任务中的潜力。

Conclusion: MetaScenes和Scan2Sim为EAI研究提供了高质量、可扩展的3D场景资源，推动了领域发展。

Abstract: Embodied AI (EAI) research requires high-quality, diverse 3D scenes to
effectively support skill acquisition, sim-to-real transfer, and
generalization. Achieving these quality standards, however, necessitates the
precise replication of real-world object diversity. Existing datasets
demonstrate that this process heavily relies on artist-driven designs, which
demand substantial human effort and present significant scalability challenges.
To scalably produce realistic and interactive 3D scenes, we first present
MetaScenes, a large-scale, simulatable 3D scene dataset constructed from
real-world scans, which includes 15366 objects spanning 831 fine-grained
categories. Then, we introduce Scan2Sim, a robust multi-modal alignment model,
which enables the automated, high-quality replacement of assets, thereby
eliminating the reliance on artist-driven designs for scaling 3D scenes. We
further propose two benchmarks to evaluate MetaScenes: a detailed scene
synthesis task focused on small item layouts for robotic manipulation and a
domain transfer task in vision-and-language navigation (VLN) to validate
cross-domain transfer. Results confirm MetaScene's potential to enhance EAI by
supporting more generalizable agent learning and sim-to-real applications,
introducing new possibilities for EAI research. Project website:
https://meta-scenes.github.io/.

</details>

### [349] [Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection](https://arxiv.org/abs/2505.02393)
*Sungheon Jeong,Jihong Park,Mohsen Imani*

Main category: cs.CV

TLDR: 提出IEF-VAD框架，通过融合RGB视频和事件表示提升视频异常检测性能，无需专用事件传感器或帧级标签。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常检测器仅依赖RGB帧，缺乏捕捉瞬态运动线索的能力。

Method: 合成事件表示并与图像特征融合，采用Student`s-t似然建模噪声，通过Kalman更新平衡模态，迭代优化融合状态。

Result: 在多个真实世界异常检测基准上达到新SOTA。

Conclusion: 合成事件表示能有效突出RGB帧中缺失的运动线索，提升检测性能且无需专用传感器。

Abstract: Most existing video anomaly detectors rely solely on RGB frames, which lack
the temporal resolution needed to capture abrupt or transient motion cues, key
indicators of anomalous events. To address this limitation, we propose
Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that
synthesizes event representations directly from RGB videos and fuses them with
image features through a principled, uncertainty-aware process. The system (i)
models heavy-tailed sensor noise with a Student`s-t likelihood, deriving
value-level inverse-variance weights via a Laplace approximation; (ii) applies
Kalman-style frame-wise updates to balance modalities over time; and (iii)
iteratively refines the fused latent state to erase residual cross-modal noise.
Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new
state of the art across multiple real-world anomaly detection benchmarks. These
findings highlight the utility of synthetic event representations in
emphasizing motion cues that are often underrepresented in RGB frames, enabling
accurate and robust video understanding across diverse applications without
requiring dedicated event sensors. Code and models are available at
https://github.com/EavnJeong/IEF-VAD.

</details>

### [350] [Token Coordinated Prompt Attention is Needed for Visual Prompting](https://arxiv.org/abs/2505.02406)
*Zichen Liu,Xu Zou,Gang Hua,Jiahuan Zhou*

Main category: cs.CV

TLDR: 论文提出了一种名为TCPA的模块，通过为不同token分配特定提示来提升ViT的表现能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉提示方法对所有token使用相同提示，忽视了不同token的独特作用，限制了ViT的表现能力。

Method: 提出TCPA模块，将提示分为CLS提示和图像提示，并通过匹配函数为不同token分配协调提示。

Result: 实验表明TCPA显著提升了特征的多样性和判别能力。

Conclusion: TCPA是一种有效的插拔式模块，能够增强ViT的特征提取能力。

Abstract: Visual prompting techniques are widely used to efficiently fine-tune
pretrained Vision Transformers (ViT) by learning a small set of shared prompts
for all tokens. However, existing methods overlook the unique roles of
different tokens in conveying discriminative information and interact with all
tokens using the same prompts, thereby limiting the representational capacity
of ViT. This often leads to indistinguishable and biased prompt-extracted
features, hindering performance. To address this issue, we propose a
plug-and-play Token Coordinated Prompt Attention (TCPA) module, which assigns
specific coordinated prompts to different tokens for attention-based
interactions. Firstly, recognizing the distinct functions of CLS and image
tokens-global information aggregation and local feature extraction, we
disentangle the prompts into CLS Prompts and Image Prompts, which interact
exclusively with CLS tokens and image tokens through attention mechanisms. This
enhances their respective discriminative abilities. Furthermore, as different
image tokens correspond to distinct image patches and contain diverse
information, we employ a matching function to automatically assign coordinated
prompts to individual tokens. This enables more precise attention interactions,
improving the diversity and representational capacity of the extracted
features. Extensive experiments across various benchmarks demonstrate that TCPA
significantly enhances the diversity and discriminative power of the extracted
features. The code is available at
https://github.com/zhoujiahuan1991/ICML2025-TCPA.

</details>

### [351] [Recent Advances in Out-of-Distribution Detection with CLIP-Like Models: A Survey](https://arxiv.org/abs/2505.02448)
*Chaohua Li,Enhao Zhang,Chuanxing Geng,Songcan Chen*

Main category: cs.CV

TLDR: 提出了一种基于图像和文本模态的新分类框架，用于CLIP等视觉语言模型的OOD检测，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法仍依赖单模态（图像），未能充分利用CLIP的多模态特性。

Method: 基于图像和文本模态对OOD数据进行分类，分为四组（OOD图像是否可见、OOD文本是否已知），并探讨两种训练策略。

Result: 提出了新的分类框架，并总结了CLIP-like OOD检测的开放问题和未来方向。

Conclusion: 新框架更符合CLIP的多模态特性，未来研究应关注跨域整合、实际应用和理论理解。

Abstract: Out-of-distribution detection (OOD) is a pivotal task for real-world
applications that trains models to identify samples that are distributionally
different from the in-distribution (ID) data during testing. Recent advances in
AI, particularly Vision-Language Models (VLMs) like CLIP, have revolutionized
OOD detection by shifting from traditional unimodal image detectors to
multimodal image-text detectors. This shift has inspired extensive research;
however, existing categorization schemes (e.g., few- or zero-shot types) still
rely solely on the availability of ID images, adhering to a unimodal paradigm.
To better align with CLIP's cross-modal nature, we propose a new categorization
framework rooted in both image and text modalities. Specifically, we categorize
existing methods based on how visual and textual information of OOD data is
utilized within image + text modalities, and further divide them into four
groups: OOD Images (i.e., outliers) Seen or Unseen, and OOD Texts (i.e.,
learnable vectors or class names) Known or Unknown, across two training
strategies (i.e., train-free or training-required). More importantly, we
discuss open problems in CLIP-like OOD detection and highlight promising
directions for future research, including cross-domain integration, practical
applications, and theoretical understanding.

</details>

### [352] [Timing Is Everything: Finding the Optimal Fusion Points in Multimodal Medical Imaging](https://arxiv.org/abs/2505.02467)
*Valerio Guarrasi,Klara Mogensen,Sara Tassinari,Sara Qvarlander,Paolo Soda*

Main category: cs.CV

TLDR: 提出了一种顺序前向搜索算法，用于确定多模态网络中融合模块的最佳插入时机，显著提升了医学影像诊断的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前多模态深度学习在医学影像中融合时机的确定依赖手动调参或穷举搜索，计算成本高且结果不一定最优。

Method: 采用顺序前向搜索算法，逐步激活和评估不同网络层的融合模块，通过验证损失比较确定最佳配置。

Result: 在两种多模态MRI数据集上验证，算法性能优于单模态基线、晚期融合和穷举融合方法，且计算开销显著降低。

Conclusion: 该方法为医学影像多模态融合提供了高效、鲁棒的优化框架，有望提升临床决策和AI架构的可扩展性。

Abstract: Multimodal deep learning harnesses diverse imaging modalities, such as MRI
sequences, to enhance diagnostic accuracy in medical imaging. A key challenge
is determining the optimal timing for integrating these
modalities-specifically, identifying the network layers where fusion modules
should be inserted. Current approaches often rely on manual tuning or
exhaustive search, which are computationally expensive without any guarantee of
converging to optimal results. We propose a sequential forward search algorithm
that incrementally activates and evaluates candidate fusion modules at
different layers of a multimodal network. At each step, the algorithm retrains
from previously learned weights and compares validation loss to identify the
best-performing configuration. This process systematically reduces the search
space, enabling efficient identification of the optimal fusion timing without
exhaustively testing all possible module placements. The approach is validated
on two multimodal MRI datasets, each addressing different classification tasks.
Our algorithm consistently identified configurations that outperformed unimodal
baselines, late fusion, and a brute-force ensemble of all potential fusion
placements. These architectures demonstrated superior accuracy, F-score, and
specificity while maintaining competitive or improved AUC values. Furthermore,
the sequential nature of the search significantly reduced computational
overhead, making the optimization process more practical. By systematically
determining the optimal timing to fuse imaging modalities, our method advances
multimodal deep learning for medical imaging. It provides an efficient and
robust framework for fusion optimization, paving the way for improved clinical
decision-making and more adaptable, scalable architectures in medical AI
applications.

</details>

### [353] [Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction](https://arxiv.org/abs/2505.02471)
*Biao Gong,Cheng Zou,Dandan Zheng,Hu Yu,Jingdong Chen,Jianxin Sun,Junbo Zhao,Jun Zhou,Kaixiang Ji,Lixiang Ru,Libin Wang,Qingpei Guo,Rui Liu,Weilong Chai,Xinyu Xiao,Ziyuan Huang*

Main category: cs.CV

TLDR: Ming-Lite-Uni是一个开源的多模态框架，结合了统一的视觉生成器和多模态自回归模型，支持文本到图像生成和基于指令的图像编辑任务。


<details>
  <summary>Details</summary>
Motivation: 旨在统一视觉和语言的多模态任务，扩展模型能力，与多模态AI的最新进展（如ChatGPT-4o）对齐，推动AGI发展。

Method: 采用MetaQueries和M2-omni框架，引入多尺度可学习标记和多尺度表示对齐策略，结合固定MLLM和可学习扩散模型。

Result: 实验结果显示Ming-Lite-Uni性能强劲，交互过程流畅，代码和模型权重已开源。

Conclusion: Ming-Lite-Uni展示了统一模型在多模态任务中的潜力，未来将进一步优化。

Abstract: We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a
newly designed unified visual generator and a native multimodal autoregressive
model tailored for unifying vision and language. Specifically, this project
provides an open-source implementation of the integrated MetaQueries and
M2-omni framework, while introducing the novel multi-scale learnable tokens and
multi-scale representation alignment strategy. By leveraging a fixed MLLM and a
learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to
perform both text-to-image generation and instruction based image editing
tasks, expanding their capabilities beyond pure visual understanding. Our
experimental results demonstrate the strong performance of Ming-Lite-Uni and
illustrate the impressive fluid nature of its interactive process. All code and
model weights are open-sourced to foster further exploration within the
community. Notably, this work aligns with concurrent multimodal AI milestones -
such as ChatGPT-4o with native image generation updated in March 25, 2025 -
underscoring the broader significance of unified models like Ming-Lite-Uni on
the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further
refined.

</details>

### [354] [Finger Pose Estimation for Under-screen Fingerprint Sensor](https://arxiv.org/abs/2505.02481)
*Xiongjun Guan,Zhiyu Pan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TLDR: 提出了一种基于双模态输入的网络，用于解决指纹姿态估计中的大角度和小面积输入问题，显著提升了准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理大角度或小面积指纹输入时表现不佳，尤其是在智能手机屏下指纹传感器捕获的指纹中。

Method: 结合纹理细节和电容图像轮廓的双模态输入，设计了概率分布预测任务，并采用MoE特征融合机制和跨域知识转移策略。

Result: 在多个数据集上验证，显著优于现有方法，提升了指纹识别算法的能力。

Conclusion: 该方法有效解决了指纹姿态估计的挑战，为指纹识别提供了更优解决方案。

Abstract: Two-dimensional pose estimation plays a crucial role in fingerprint
recognition by facilitating global alignment and reduce pose-induced
variations. However, existing methods are still unsatisfactory when handling
with large angle or small area inputs. These limitations are particularly
pronounced on fingerprints captured by under-screen fingerprint sensors in
smartphones. In this paper, we present a novel dual-modal input based network
for under-screen fingerprint pose estimation. Our approach effectively
integrates two distinct yet complementary modalities: texture details extracted
from ridge patches through the under-screen fingerprint sensor, and rough
contours derived from capacitive images obtained via the touch screen. This
collaborative integration endows our network with more comprehensive and
discriminative information, substantially improving the accuracy and stability
of pose estimation. A decoupled probability distribution prediction task is
designed, instead of the traditional supervised forms of numerical regression
or heatmap voting, to facilitate the training process. Additionally, we
incorporate a Mixture of Experts (MoE) based feature fusion mechanism and a
relationship driven cross-domain knowledge transfer strategy to further
strengthen feature extraction and fusion capabilities. Extensive experiments
are conducted on several public datasets and two private datasets. The results
indicate that our method is significantly superior to previous state-of-the-art
(SOTA) methods and remarkably boosts the recognition ability of fingerprint
recognition algorithms. Our code is available at
https://github.com/XiongjunGuan/DRACO.

</details>

### [355] [Corr2Distrib: Making Ambiguous Correspondences an Ally to Predict Reliable 6D Pose Distributions](https://arxiv.org/abs/2505.02501)
*Asma Brazi,Boris Meden,Fabrice Mayran de Chamisso,Steve Bourgeois,Vincent Lepetit*

Main category: cs.CV

TLDR: Corr2Distrib是首个基于对应关系的方法，通过RGB图像估计6D相机姿态分布，解决视觉模糊问题。


<details>
  <summary>Details</summary>
Motivation: 对称性和遮挡导致视觉模糊，产生多个有效姿态，现有方法未充分利用局部对应关系。

Method: Corr2Distrib学习对称感知的3D点表示，生成旋转假设，并通过PnP和姿态评分优化为6D姿态分布。

Result: 在复杂非合成场景中，Corr2Distrib在姿态分布估计和单姿态估计上均优于现有方法。

Conclusion: Corr2Distrib展示了基于对应关系方法的潜力，有效解决视觉模糊问题。

Abstract: We introduce Corr2Distrib, the first correspondence-based method which
estimates a 6D camera pose distribution from an RGB image, explaining the
observations. Indeed, symmetries and occlusions introduce visual ambiguities,
leading to multiple valid poses. While a few recent methods tackle this
problem, they do not rely on local correspondences which, according to the BOP
Challenge, are currently the most effective way to estimate a single 6DoF pose
solution. Using correspondences to estimate a pose distribution is not
straightforward, since ambiguous correspondences induced by visual ambiguities
drastically decrease the performance of PnP. With Corr2Distrib, we turn these
ambiguities into an advantage to recover all valid poses. Corr2Distrib first
learns a symmetry-aware representation for each 3D point on the object's
surface, characterized by a descriptor and a local frame. This representation
enables the generation of 3DoF rotation hypotheses from single 2D-3D
correspondences. Next, we refine these hypotheses into a 6DoF pose distribution
using PnP and pose scoring. Our experimental evaluations on complex
non-synthetic scenes show that Corr2Distrib outperforms state-of-the-art
solutions for both pose distribution estimation and single pose estimation from
an RGB image, demonstrating the potential of correspondences-based approaches.

</details>

### [356] [Text to Image Generation and Editing: A Survey](https://arxiv.org/abs/2505.02527)
*Pengfei Yang,Ngai-Man Cheung,Xinda Ma*

Main category: cs.CV

TLDR: 本文综述了2021至2024年间141篇关于文本到图像生成（T2I）的研究，系统介绍了四种基础模型架构、关键技术、性能比较及社会影响，并提出了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: T2I技术近年来发展迅速，但缺乏系统性综述，本文旨在填补这一空白，为未来研究提供指导。

Method: 通过分析141篇文献，比较了四种基础模型架构（自回归、非自回归、GAN和扩散模型）及关键技术（如自编码器、注意力机制等），并评估了性能指标。

Result: 总结了T2I的现状，包括模型性能、数据集、评估指标等，并探讨了社会影响及解决方案。

Conclusion: 本文首次系统综述了T2I领域，提出了改进模型性能的见解和未来方向，为研究者提供了有价值的参考。

Abstract: Text-to-image generation (T2I) refers to the text-guided generation of
high-quality images. In the past few years, T2I has attracted widespread
attention and numerous works have emerged. In this survey, we comprehensively
review 141 works conducted from 2021 to 2024. First, we introduce four
foundation model architectures of T2I (autoregression, non-autoregression, GAN
and diffusion) and the commonly used key technologies (autoencoder, attention
and classifier-free guidance). Secondly, we systematically compare the methods
of these studies in two directions, T2I generation and T2I editing, including
the encoders and the key technologies they use. In addition, we also compare
the performance of these researches side by side in terms of datasets,
evaluation metrics, training resources, and inference speed. In addition to the
four foundation models, we survey other works on T2I, such as energy-based
models and recent Mamba and multimodality. We also investigate the potential
social impact of T2I and provide some solutions. Finally, we propose unique
insights of improving the performance of T2I models and possible future
development directions. In summary, this survey is the first systematic and
comprehensive overview of T2I, aiming to provide a valuable guide for future
researchers and stimulate continued progress in this field.

</details>

### [357] [Marker-Based Extrinsic Calibration Method for Accurate Multi-Camera 3D Reconstruction](https://arxiv.org/abs/2505.02539)
*Nahuel Garcia-D'Urso,Bernabe Sanchez-Sos,Jorge Azorin-Lopez,Andres Fuster-Guillo,Antonio Macia-Lillo,Higinio Mora-Mora*

Main category: cs.CV

TLDR: 提出了一种基于三维标记的迭代外参标定方法，显著提高了多相机RGB-D系统的校准精度。


<details>
  <summary>Details</summary>
Motivation: 多相机RGB-D系统的精确外参标定对3D重建至关重要，但现有方法在精度和鲁棒性上仍有不足。

Method: 通过聚类、回归分析和迭代重分配技术，系统分割和优化标记平面，确保相机视图间的几何一致性。

Result: 在Tech4Diet项目中验证，显著减少了对齐误差，实现了高精度的3D重建。

Conclusion: 该方法在理论和实际应用中均表现出色，为3D重建提供了可靠的外参标定解决方案。

Abstract: Accurate 3D reconstruction using multi-camera RGB-D systems critically
depends on precise extrinsic calibration to achieve proper alignment between
captured views. In this paper, we introduce an iterative extrinsic calibration
method that leverages the geometric constraints provided by a three-dimensional
marker to significantly improve calibration accuracy. Our proposed approach
systematically segments and refines marker planes through clustering,
regression analysis, and iterative reassignment techniques, ensuring robust
geometric correspondence across camera views. We validate our method
comprehensively in both controlled environments and practical real-world
settings within the Tech4Diet project, aimed at modeling the physical
progression of patients undergoing nutritional treatments. Experimental results
demonstrate substantial reductions in alignment errors, facilitating accurate
and reliable 3D reconstructions.

</details>

### [358] [Robust Duality Learning for Unsupervised Visible-Infrared Person Re-Identfication](https://arxiv.org/abs/2505.02549)
*Yongxiang Li,Yuan Sun,Yang Qin,Dezhong Peng,Xi Peng,Peng Hu*

Main category: cs.CV

TLDR: 提出了一种新的鲁棒对偶学习框架（RoDE）来解决无监督可见光-红外行人重识别中的伪标签噪声问题。


<details>
  <summary>Details</summary>
Motivation: 无监督跨模态行人重识别面临模态差异和缺乏监督的挑战，现有方法的伪标签噪声问题阻碍了模型学习。

Method: RoDE框架包含三个关键机制：鲁棒自适应学习（RAL）减轻噪声过拟合，双模型交替训练缓解误差累积，以及簇一致性匹配（CCM）解决簇不对齐问题。

Result: 在三个基准测试上的实验验证了RoDE的有效性。

Conclusion: RoDE通过显式处理伪标签噪声，显著提升了无监督跨模态行人重识别的性能。

Abstract: Unsupervised visible-infrared person re-identification (UVI-ReID) aims to
retrieve pedestrian images across different modalities without costly
annotations, but faces challenges due to the modality gap and lack of
supervision. Existing methods often adopt self-training with
clustering-generated pseudo-labels but implicitly assume these labels are
always correct. In practice, however, this assumption fails due to inevitable
pseudo-label noise, which hinders model learning. To address this, we introduce
a new learning paradigm that explicitly considers Pseudo-Label Noise (PLN),
characterized by three key challenges: noise overfitting, error accumulation,
and noisy cluster correspondence. To this end, we propose a novel Robust
Duality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy
pseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning
mechanism (RAL) is proposed to dynamically emphasize clean samples while
down-weighting noisy ones. Second, to alleviate error accumulation-where the
model reinforces its own mistakes-RoDE employs dual distinct models that are
alternately trained using pseudo-labels from each other, encouraging diversity
and preventing collapse. However, this dual-model strategy introduces
misalignment between clusters across models and modalities, creating noisy
cluster correspondence. To resolve this, we introduce Cluster Consistency
Matching (CCM), which aligns clusters across models and modalities by measuring
cross-cluster similarity. Extensive experiments on three benchmarks demonstrate
the effectiveness of RoDE.

</details>

### [359] [Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities](https://arxiv.org/abs/2505.02567)
*Xinjie Zhang,Jintao Guo,Shanshan Zhao,Minghao Fu,Lunhao Duan,Guo-Hua Wang,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TLDR: 本文综述了多模态理解与图像生成模型的统一框架，分析了现有方法、数据集及挑战，旨在推动未来研究。


<details>
  <summary>Details</summary>
Motivation: 多模态理解和图像生成模型各自发展，但架构差异显著，统一框架的研究潜力巨大，如GPT-4o所示。本文旨在梳理当前统一模型的进展。

Method: 分类介绍了三种统一架构范式：基于扩散、基于自回归及混合方法，并分析了其结构设计与创新。

Result: 总结了现有统一模型的成果，并整理了相关数据集和基准测试资源。

Conclusion: 讨论了该领域的关键挑战（如分词策略、跨模态注意力等），并展望了未来研究方向，计划定期更新综述。

Abstract: Recent years have seen remarkable progress in both multimodal understanding
models and image generation models. Despite their respective successes, these
two domains have evolved independently, leading to distinct architectural
paradigms: While autoregressive-based architectures have dominated multimodal
understanding, diffusion-based models have become the cornerstone of image
generation. Recently, there has been growing interest in developing unified
frameworks that integrate these tasks. The emergence of GPT-4o's new
capabilities exemplifies this trend, highlighting the potential for
unification. However, the architectural differences between the two domains
pose significant challenges. To provide a clear overview of current efforts
toward unification, we present a comprehensive survey aimed at guiding future
research. First, we introduce the foundational concepts and recent advancements
in multimodal understanding and text-to-image generation models. Next, we
review existing unified models, categorizing them into three main architectural
paradigms: diffusion-based, autoregressive-based, and hybrid approaches that
fuse autoregressive and diffusion mechanisms. For each category, we analyze the
structural designs and innovations introduced by related works. Additionally,
we compile datasets and benchmarks tailored for unified models, offering
resources for future exploration. Finally, we discuss the key challenges facing
this nascent field, including tokenization strategy, cross-modal attention, and
data. As this area is still in its early stages, we anticipate rapid
advancements and will regularly update this survey. Our goal is to inspire
further research and provide a valuable reference for the community. The
references associated with this survey will be available on GitHub soon.

</details>

### [360] [RGBX-DiffusionDet: A Framework for Multi-Modal RGB-X Object Detection Using DiffusionDet](https://arxiv.org/abs/2505.02586)
*Eliraz Orfaig,Inna Stainvas,Igal Bilik*

Main category: cs.CV

TLDR: RGBX-DiffusionDet扩展了DiffusionDet模型，通过自适应多模态编码器融合RGB和异构2D数据（X），提出动态通道减少模块（DCR-CBAM）和动态多级聚合块（DMLAB）优化特征表示，实验证明其优于RGB-only基线。


<details>
  <summary>Details</summary>
Motivation: 融合RGB和异构2D数据以提升目标检测性能，同时保持模型效率。

Method: 采用DCR-CBAM实现跨模态交互，DMLAB优化空间特征，引入正则化损失增强特征嵌入。

Result: 在多个数据集（RGB-Depth、RGB-Polarimetric、RGB-Infrared）上表现优于RGB-only DiffusionDet。

Conclusion: RGBX-DiffusionDet为多模态目标检测提供了灵活高效的解决方案，扩展了基于扩散的检测方法。

Abstract: This work introduces RGBX-DiffusionDet, an object detection framework
extending the DiffusionDet model to fuse the heterogeneous 2D data (X) with RGB
imagery via an adaptive multimodal encoder. To enable cross-modal interaction,
we design the dynamic channel reduction within a convolutional block attention
module (DCR-CBAM), which facilitates cross-talk between subnetworks by
dynamically highlighting salient channel features. Furthermore, the dynamic
multi-level aggregation block (DMLAB) is proposed to refine spatial feature
representations through adaptive multiscale fusion. Finally, novel
regularization losses that enforce channel saliency and spatial selectivity are
introduced, leading to compact and discriminative feature embeddings. Extensive
experiments using RGB-Depth (KITTI), a novel annotated RGB-Polarimetric
dataset, and RGB-Infrared (M$^3$FD) benchmark dataset were conducted. We
demonstrate consistent superiority of the proposed approach over the baseline
RGB-only DiffusionDet. The modular architecture maintains the original decoding
complexity, ensuring efficiency. These results establish the proposed
RGBX-DiffusionDet as a flexible multimodal object detection approach, providing
new insights into integrating diverse 2D sensing modalities into
diffusion-based detection pipelines.

</details>

### [361] [DELTA: Dense Depth from Events and LiDAR using Transformer's Attention](https://arxiv.org/abs/2505.02593)
*Vincent Brebion,Julien Moreau,Franck Davoine*

Main category: cs.CV

TLDR: 提出了一种名为DELTA的神经网络方法，用于融合事件相机和LiDAR数据以估计密集深度图。


<details>
  <summary>Details</summary>
Motivation: 事件相机和LiDAR提供互补但不同的数据，但现有研究很少探索这两种模态的结合。

Method: 利用自注意力和交叉注意力建模事件和LiDAR数据的时空关系。

Result: DELTA在事件深度估计问题上达到新的SOTA，近距离误差减少高达四倍。

Conclusion: DELTA方法在融合事件和LiDAR数据方面表现出色，显著提升了深度估计性能。

Abstract: Event cameras and LiDARs provide complementary yet distinct data:
respectively, asynchronous detections of changes in lighting versus sparse but
accurate depth information at a fixed rate. To this day, few works have
explored the combination of these two modalities. In this article, we propose a
novel neural-network-based method for fusing event and LiDAR data in order to
estimate dense depth maps. Our architecture, DELTA, exploits the concepts of
self- and cross-attention to model the spatial and temporal relations within
and between the event and LiDAR data. Following a thorough evaluation, we
demonstrate that DELTA sets a new state of the art in the event-based depth
estimation problem, and that it is able to reduce the errors up to four times
for close ranges compared to the previous SOTA.

</details>

### [362] [Detect, Classify, Act: Categorizing Industrial Anomalies with Multi-Modal Large Language Models](https://arxiv.org/abs/2505.02626)
*Sassan Mokhtar,Arian Mousakhan,Silvio Galesso,Jawad Tayyub,Thomas Brox*

Main category: cs.CV

TLDR: VELM是一种基于LLM的工业异常分类方法，结合无监督异常检测和LLM分类，解决了现有数据集中异常类别标注不足的问题，并在MVTec-AD和MVTec-AC上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测方法在异常分类方面研究不足，而异常分类在实际检测任务中至关重要。

Method: VELM结合无监督异常检测（视觉专家）和LLM分类，并引入带有精确异常类别标注的数据集MVTec-AC和VisA-AC。

Result: VELM在MVTec-AD和MVTec-AC上的分类准确率分别达到80.4%和84%，优于基线5%。

Conclusion: VELM为异常分类提供了有效方法，并推动了从检测到全面异常表征的研究。

Abstract: Recent advances in visual industrial anomaly detection have demonstrated
exceptional performance in identifying and segmenting anomalous regions while
maintaining fast inference speeds. However, anomaly
classification-distinguishing different types of anomalies-remains largely
unexplored despite its critical importance in real-world inspection tasks. To
address this gap, we propose VELM, a novel LLM-based pipeline for anomaly
classification. Given the critical importance of inference speed, we first
apply an unsupervised anomaly detection method as a vision expert to assess the
normality of an observation. If an anomaly is detected, the LLM then classifies
its type. A key challenge in developing and evaluating anomaly classification
models is the lack of precise annotations of anomaly classes in existing
datasets. To address this limitation, we introduce MVTec-AC and VisA-AC,
refined versions of the widely used MVTec-AD and VisA datasets, which include
accurate anomaly class labels for rigorous evaluation. Our approach achieves a
state-of-the-art anomaly classification accuracy of 80.4% on MVTec-AD,
exceeding the prior baselines by 5%, and 84% on MVTec-AC, demonstrating the
effectiveness of VELM in understanding and categorizing anomalies. We hope our
methodology and benchmark inspire further research in anomaly classification,
helping bridge the gap between detection and comprehensive anomaly
characterization.

</details>

### [363] [MCCD: Multi-Agent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation](https://arxiv.org/abs/2505.02648)
*Mingcheng Li,Xiaolu Hou,Ziyang Liu,Dingkang Yang,Ziyun Qian,Jiawei Chen,Jinjie Wei,Yue Jiang,Qingyao Xu,Lihua Zhang*

Main category: cs.CV

TLDR: 提出了一种基于多智能体协作的扩散模型（MCCD），用于复杂场景的文本到图像生成，显著提升了基线模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理涉及多对象、特征和关系的复杂提示时存在性能瓶颈，因此需要一种更高效的方法。

Method: 设计了多智能体协作的场景解析模块，利用MLLMs提取场景元素；采用分层组合扩散，通过高斯掩码和过滤优化区域。

Result: 实验表明，MCCD显著提升了基线模型的性能，无需训练即可实现高保真复杂场景生成。

Conclusion: MCCD为复杂场景生成提供了显著优势，是一种高效的解决方案。

Abstract: Diffusion models have shown excellent performance in text-to-image
generation. Nevertheless, existing methods often suffer from performance
bottlenecks when handling complex prompts that involve multiple objects,
characteristics, and relations. Therefore, we propose a Multi-agent
Collaboration-based Compositional Diffusion (MCCD) for text-to-image generation
for complex scenes. Specifically, we design a multi-agent collaboration-based
scene parsing module that generates an agent system comprising multiple agents
with distinct tasks, utilizing MLLMs to extract various scene elements
effectively. In addition, Hierarchical Compositional diffusion utilizes a
Gaussian mask and filtering to refine bounding box regions and enhance objects
through region enhancement, resulting in the accurate and high-fidelity
generation of complex scenes. Comprehensive experiments demonstrate that our
MCCD significantly improves the performance of the baseline models in a
training-free manner, providing a substantial advantage in complex scene
generation.

</details>

### [364] [Sim2Real in endoscopy segmentation with a novel structure aware image translation](https://arxiv.org/abs/2505.02654)
*Clara Tomasini,Luis Riazuelo,Ana C. Murillo*

Main category: cs.CV

TLDR: 提出了一种新的图像翻译模型，为模拟内窥镜图像添加真实纹理，同时保留关键场景布局信息，用于训练无需真实标注数据的模型。


<details>
  <summary>Details</summary>
Motivation: 解决内窥镜图像中解剖标志自动分割的标注难题，尤其是真实图像标注困难，而合成数据训练的模型泛化能力差的问题。

Method: 开发了一种图像翻译模型，将模拟内窥镜图像转换为具有真实纹理的图像，同时保持原始场景的结构。

Result: 生成的图像在不同内窥镜场景中表现真实，可用于训练无需真实标注数据的模型，特别是在结肠镜图像中的褶皱分割任务中效果显著。

Conclusion: 该方法生成的图像优于现有方法，且公开了新数据集以促进褶皱分割任务的进一步研究。

Abstract: Automatic segmentation of anatomical landmarks in endoscopic images can
provide assistance to doctors and surgeons for diagnosis, treatments or medical
training. However, obtaining the annotations required to train commonly used
supervised learning methods is a tedious and difficult task, in particular for
real images. While ground truth annotations are easier to obtain for synthetic
data, models trained on such data often do not generalize well to real data.
Generative approaches can add realistic texture to it, but face difficulties to
maintain the structure of the original scene. The main contribution in this
work is a novel image translation model that adds realistic texture to
simulated endoscopic images while keeping the key scene layout information. Our
approach produces realistic images in different endoscopy scenarios. We
demonstrate these images can effectively be used to successfully train a model
for a challenging end task without any real labeled data. In particular, we
demonstrate our approach for the task of fold segmentation in colonoscopy
images. Folds are key anatomical landmarks that can occlude parts of the colon
mucosa and possible polyps. Our approach generates realistic images maintaining
the shape and location of the original folds, after the
image-style-translation, better than existing methods. We run experiments both
on a novel simulated dataset for fold segmentation, and real data from the
EndoMapper (EM) dataset. All our new generated data and new EM metadata is
being released to facilitate further research, as no public benchmark is
currently available for the task of fold segmentation.

</details>

### [365] [Dance of Fireworks: An Interactive Broadcast Gymnastics Training System Based on Pose Estimation](https://arxiv.org/abs/2505.02690)
*Haotian Chen,Ziyu Liu,Xi Cheng,Chuangqi Li*

Main category: cs.CV

TLDR: Dance of Fireworks 是一个互动系统，通过实时反馈和视觉奖励提升广播体操的参与度，显著减少动作误差并受到用户好评。


<details>
  <summary>Details</summary>
Motivation: 解决久坐健康风险，通过技术手段提升广播体操的参与度和效果。

Method: 利用移动设备摄像头和轻量级姿态估计（PoseNet/TensorFlow Lite），实时分析动作并提供反馈，同时通过动态烟花动画激励用户。

Result: 实验显示，平均关节角度误差从21.3度降至9.8度，93.4%的用户认可其锻炼效果，85.4%的用户喜欢其娱乐性。

Conclusion: 该系统为久坐人群提供了一种低成本、高吸引力的运动解决方案，未来将优化姿态识别和增加多人互动等功能。

Abstract: This study introduces Dance of Fireworks, an interactive system designed to
combat sedentary health risks by enhancing engagement in radio calisthenics.
Leveraging mobile device cameras and lightweight pose estimation
(PoseNet/TensorFlow Lite), the system extracts body keypoints, computes joint
angles, and compares them with standardized motions to deliver real-time
corrective feedback. To incentivize participation, it dynamically maps users'
movements (such as joint angles and velocity) to customizable fireworks
animations, rewarding improved accuracy with richer visual effects. Experiments
involving 136 participants demonstrated a significant reduction in average
joint angle errors from 21.3 degrees to 9.8 degrees (p < 0.01) over four
sessions, with 93.4 percent of users affirming its exercise-promoting efficacy
and 85.4 percent praising its entertainment value. The system operates without
predefined motion templates or specialised hardware, enabling seamless
integration into office environments. Future enhancements will focus on
improving pose recognition accuracy, reducing latency, and adding features such
as multiplayer interaction and music synchronisation. This work presents a
cost-effective, engaging solution to promote physical activity in sedentary
populations.

</details>

### [366] [Structure Causal Models and LLMs Integration in Medical Visual Question Answering](https://arxiv.org/abs/2505.02703)
*Zibo Xu,Qiang Li,Weizhi Nie,Weijie Wang,Anan Liu*

Main category: cs.CV

TLDR: 提出了一种基于因果推理的MedVQA框架，通过消除图像与问题间的混淆效应，提升医学视觉问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 医学数据的复杂性导致图像与问题间存在难以观察的混淆效应，影响问答的准确性。

Method: 引入因果图结构表示视觉与文本元素的交互，利用互信息发现伪相关，并提出多变量重采样前门调整方法消除混淆效应。

Result: 在三个MedVQA数据集上显著提升了问答准确性，并实现了真实因果相关性。

Conclusion: 该方法有效解决了MedVQA中的跨模态偏差问题，提升了模型的医学问答能力。

Abstract: Medical Visual Question Answering (MedVQA) aims to answer medical questions
according to medical images. However, the complexity of medical data leads to
confounders that are difficult to observe, so bias between images and questions
is inevitable. Such cross-modal bias makes it challenging to infer medically
meaningful answers. In this work, we propose a causal inference framework for
the MedVQA task, which effectively eliminates the relative confounding effect
between the image and the question to ensure the precision of the
question-answering (QA) session. We are the first to introduce a novel causal
graph structure that represents the interaction between visual and textual
elements, explicitly capturing how different questions influence visual
features. During optimization, we apply the mutual information to discover
spurious correlations and propose a multi-variable resampling front-door
adjustment method to eliminate the relative confounding effect, which aims to
align features based on their true causal relevance to the question-answering
task. In addition, we also introduce a prompt strategy that combines multiple
prompt forms to improve the model's ability to understand complex medical data
and answer accurately. Extensive experiments on three MedVQA datasets
demonstrate that 1) our method significantly improves the accuracy of MedVQA,
and 2) our method achieves true causal correlations in the face of complex
medical data.

</details>

### [367] [Visually-Guided Linguistic Disambiguation for Monocular Depth Scale Recovery](https://arxiv.org/abs/2505.02704)
*Bojin Wu,Jing Chen*

Main category: cs.CV

TLDR: 提出了一种稳健的单目深度尺度恢复方法VGLD，通过结合图像的高层语义信息解决文本描述的模糊性，实现相对深度图的全局线性变换。


<details>
  <summary>Details</summary>
Motivation: 单目深度估计中，相对深度缺乏尺度信息，而绝对深度恢复对实际任务至关重要。文本信息可用于恢复尺度，但不同描述会影响结果。

Method: VGLD结合图像语义与文本信息，消除文本模糊性，输出全局线性变换参数，将相对深度图转换为绝对尺度深度。

Result: 在多个数据集（NYUv2、KITTI）和模型（MiDas、DepthAnything）上验证，VGLD作为通用对齐模块表现优异，支持零样本场景。

Conclusion: VGLD通过语义信息稳定文本影响，有效恢复深度尺度，适用于多种场景和模型。

Abstract: We propose a robust method for monocular depth scale recovery. Monocular
depth estimation can be divided into two main directions: (1) relative depth
estimation, which provides normalized or inverse depth without scale
information, and (2) metric depth estimation, which involves recovering depth
with absolute scale. To obtain absolute scale information for practical
downstream tasks, utilizing textual information to recover the scale of a
relative depth map is a highly promising approach. However, since a single
image can have multiple descriptions from different perspectives or with
varying styles, it has been shown that different textual descriptions can
significantly affect the scale recovery process. To address this issue, our
method, VGLD, stabilizes the influence of textual information by incorporating
high-level semantic information from the corresponding image alongside the
textual description. This approach resolves textual ambiguities and robustly
outputs a set of linear transformation parameters (scalars) that can be
globally applied to the relative depth map, ultimately generating depth
predictions with metric-scale accuracy. We validate our method across several
popular relative depth models(MiDas, DepthAnything), using both indoor scenes
(NYUv2) and outdoor scenes (KITTI). Our results demonstrate that VGLD functions
as a universal alignment module when trained on multiple datasets, achieving
strong performance even in zero-shot scenarios. Code is available at:
https://github.com/pakinwu/VGLD.

</details>

### [368] [A Rate-Quality Model for Learned Video Coding](https://arxiv.org/abs/2505.02720)
*Sang NguyenQuang,Cheng-Wei Chen,Xiem HoangVan,Wen-Hsiao Peng*

Main category: cs.CV

TLDR: 该论文提出了一种基于神经网络的R-Q模型（RQNet），用于动态预测视频编码中的比特率与质量关系，并通过最小二乘法优化参数，显著提升了编码性能。


<details>
  <summary>Details</summary>
Motivation: 传统视频编码方法在动态调整比特率与质量关系时缺乏灵活性和精确性，因此需要一种能够在线适应内容与编码上下文的R-Q模型。

Method: 通过训练神经网络（RQNet）预测比特率与质量关系，并结合最小二乘法动态优化模型参数。

Result: 实验表明，该方法在常用数据集上比特率偏差显著小于基线方法，且额外复杂度极低。

Conclusion: 提出的R-Q模型能够在线适应视频内容与编码上下文，显著提升了编码的灵活性和精确性。

Abstract: Learned video coding (LVC) has recently achieved superior coding performance.
In this paper, we model the rate-quality (R-Q) relationship for learned video
coding by a parametric function. We learn a neural network, termed RQNet, to
characterize the relationship between the bitrate and quality level according
to video content and coding context. The predicted (R,Q) results are further
integrated with those from previously coded frames using the least-squares
method to determine the parameters of our R-Q model on-the-fly. Compared to the
conventional approaches, our method accurately estimates the R-Q relationship,
enabling the online adaptation of model parameters to enhance both flexibility
and precision. Experimental results show that our R-Q model achieves
significantly smaller bitrate deviations than the baseline method on commonly
used datasets with minimal additional complexity.

</details>

### [369] [Advancing Generalizable Tumor Segmentation with Anomaly-Aware Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models](https://arxiv.org/abs/2505.02753)
*Yankai Jiang,Peng Zhang,Donglin Yang,Yuan Tian,Hai Lin,Xiaosong Wang*

Main category: cs.CV

TLDR: DiffuGTS利用冻结的医学基础扩散模型内部表示，通过文本提示生成异常感知的开放词汇注意力图，实现零样本肿瘤分割，无需预定义训练类别。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分割质量、可扩展性和适用成像模态范围上存在局限，DiffuGTS旨在通过扩散模型提升零样本肿瘤分割的通用性和性能。

Method: DiffuGTS结合文本提示生成注意力图，并通过潜在空间修复和残差学习优化分割掩码。

Result: 在四个数据集和七种肿瘤类别上的实验表明，DiffuGTS在零样本设置下优于现有方法。

Conclusion: DiffuGTS为通用肿瘤分割提供了高效且高质量的解决方案，具有广泛的应用潜力。

Abstract: We explore Generalizable Tumor Segmentation, aiming to train a single model
for zero-shot tumor segmentation across diverse anatomical regions. Existing
methods face limitations related to segmentation quality, scalability, and the
range of applicable imaging modalities. In this paper, we uncover the potential
of the internal representations within frozen medical foundation diffusion
models as highly efficient zero-shot learners for tumor segmentation by
introducing a novel framework named DiffuGTS. DiffuGTS creates anomaly-aware
open-vocabulary attention maps based on text prompts to enable generalizable
anomaly segmentation without being restricted by a predefined training category
list. To further improve and refine anomaly segmentation masks, DiffuGTS
leverages the diffusion model, transforming pathological regions into
high-quality pseudo-healthy counterparts through latent space inpainting, and
applies a novel pixel-level and feature-level residual learning approach,
resulting in segmentation masks with significantly enhanced quality and
generalization. Comprehensive experiments on four datasets and seven tumor
categories demonstrate the superior performance of our method, surpassing
current state-of-the-art models across multiple zero-shot settings. Codes are
available at https://github.com/Yankai96/DiffuGTS.

</details>

### [370] [Unsupervised Deep Learning-based Keypoint Localization Estimating Descriptor Matching Performance](https://arxiv.org/abs/2505.02779)
*David Rivas-Villar,Álvaro S. Hervella,José Rouco,Jorge Novo*

Main category: cs.CV

TLDR: 提出了一种无需标注数据的视网膜图像配准方法，通过创新的描述符学习和关键点检测网络，性能媲美有监督方法。


<details>
  <summary>Details</summary>
Motivation: 医学领域中标注数据稀缺，现有方法依赖标注数据，限制了应用范围。

Method: 提出无监督描述符学习和无标签关键点检测网络，完全摆脱标注数据依赖。

Result: 在四个数据集上验证，无监督描述符和检测器性能优于现有方法，配准效果媲美有监督方法。

Conclusion: 该方法无需标注数据，性能优越，且可直接应用于其他领域和模态。

Abstract: Retinal image registration, particularly for color fundus images, is a
challenging yet essential task with diverse clinical applications. Existing
registration methods for color fundus images typically rely on keypoints and
descriptors for alignment; however, a significant limitation is their reliance
on labeled data, which is particularly scarce in the medical domain.
  In this work, we present a novel unsupervised registration pipeline that
entirely eliminates the need for labeled data. Our approach is based on the
principle that locations with distinctive descriptors constitute reliable
keypoints. This fully inverts the conventional state-of-the-art approach,
conditioning the detector on the descriptor rather than the opposite.
  First, we propose an innovative descriptor learning method that operates
without keypoint detection or any labels, generating descriptors for arbitrary
locations in retinal images. Next, we introduce a novel, label-free keypoint
detector network which works by estimating descriptor performance directly from
the input image.
  We validate our method through a comprehensive evaluation on four hold-out
datasets, demonstrating that our unsupervised descriptor outperforms
state-of-the-art supervised descriptors and that our unsupervised detector
significantly outperforms existing unsupervised detection methods. Finally, our
full registration pipeline achieves performance comparable to the leading
supervised methods, while not employing any labeled data. Additionally, the
label-free nature and design of our method enable direct adaptation to other
domains and modalities.

</details>

### [371] [Advances in Automated Fetal Brain MRI Segmentation and Biometry: Insights from the FeTA 2024 Challenge](https://arxiv.org/abs/2505.02784)
*Vladyslav Zalevskyi,Thomas Sanchez,Misha Kaandorp,Margaux Roulet,Diego Fajardo-Rojas,Liu Li,Jana Hutter,Hongwei Bran Li,Matthew Barkovich,Hui Ji,Luca Wilhelmi,Aline Dändliker,Céline Steger,Mériam Koob,Yvan Gomez,Anton Jakovčić,Melita Klaić,Ana Adžić,Pavel Marković,Gracia Grabarić,Milan Rados,Jordina Aviles Verdera,Gregor Kasprian,Gregor Dovjak,Raphael Gaubert-Rachmühl,Maurice Aschwanden,Qi Zeng,Davood Karimi,Denis Peruzzo,Tommaso Ciceri,Giorgio Longari,Rachika E. Hamadache,Amina Bouzid,Xavier Lladó,Simone Chiarella,Gerard Martí-Juan,Miguel Ángel González Ballester,Marco Castellaro,Marco Pinamonti,Valentina Visani,Robin Cremese,Keïn Sam,Fleur Gaudfernau,Param Ahir,Mehul Parikh,Maximilian Zenk,Michael Baumgartner,Klaus Maier-Hein,Li Tianhong,Yang Hong,Zhao Longfei,Domen Preloznik,Žiga Špiclin,Jae Won Choi,Muyang Li,Jia Fu,Guotai Wang,Jingwen Jiang,Lyuyang Tong,Bo Du,Andrea Gondova,Sungmin You,Kiho Im,Abdul Qayyum,Moona Mazher,Steven A Niederer,Maya Yanko,Bella Specktor-Fadida,Dafna Ben Bashat,Andras Jakab,Roxane Licandro,Kelly Payette,Meritxell Bach Cuadra*

Main category: cs.CV

TLDR: FeTA Challenge 2024 推动了胎儿脑部MRI分析的自动化，新增了生物测量预测任务，并引入了低场强MRI数据。尽管分割方法表现稳定，但精度接近瓶颈，而生物测量任务表现不佳。拓扑评估和低场数据潜力显著。


<details>
  <summary>Details</summary>
Motivation: 研究胎儿脑部发育需要精确的组织分割和生物测量分析，FeTA 2024旨在推动自动化工具的发展，并探索低场强MRI的潜力。

Method: 挑战赛包括组织分割和生物测量预测任务，使用多中心数据集（含低场强MRI），并引入拓扑评估指标（ED）。16支团队提交分割方法，7支参与生物测量任务。

Result: 分割方法在高、低场强数据上表现一致，但精度接近极限。生物测量任务多数方法不如基于孕龄的简单基线。低场数据表现最佳，拓扑评估揭示新差异。

Conclusion: FeTA 2024为胎儿脑部MRI分析提供了基准，强调数据多样性、拓扑评估和临床稳健AI工具的需求。

Abstract: Accurate fetal brain tissue segmentation and biometric analysis are essential
for studying brain development in utero. The FeTA Challenge 2024 advanced
automated fetal brain MRI analysis by introducing biometry prediction as a new
task alongside tissue segmentation. For the first time, our diverse
multi-centric test set included data from a new low-field (0.55T) MRI dataset.
Evaluation metrics were also expanded to include the topology-specific Euler
characteristic difference (ED). Sixteen teams submitted segmentation methods,
most of which performed consistently across both high- and low-field scans.
However, longitudinal trends indicate that segmentation accuracy may be
reaching a plateau, with results now approaching inter-rater variability. The
ED metric uncovered topological differences that were missed by conventional
metrics, while the low-field dataset achieved the highest segmentation scores,
highlighting the potential of affordable imaging systems when paired with
high-quality reconstruction. Seven teams participated in the biometry task, but
most methods failed to outperform a simple baseline that predicted measurements
based solely on gestational age, underscoring the challenge of extracting
reliable biometric estimates from image data alone. Domain shift analysis
identified image quality as the most significant factor affecting model
generalization, with super-resolution pipelines also playing a substantial
role. Other factors, such as gestational age, pathology, and acquisition site,
had smaller, though still measurable, effects. Overall, FeTA 2024 offers a
comprehensive benchmark for multi-class segmentation and biometry estimation in
fetal brain MRI, underscoring the need for data-centric approaches, improved
topological evaluation, and greater dataset diversity to enable clinically
robust and generalizable AI tools.

</details>

### [372] [Unsupervised training of keypoint-agnostic descriptors for flexible retinal image registration](https://arxiv.org/abs/2505.02787)
*David Rivas-Villar,Álvaro S. Hervella,José Rouco,Jorge Novo*

Main category: cs.CV

TLDR: 提出了一种无需关键点检测的无监督描述符学习方法，适用于医学图像配准，性能与监督方法相当且不受关键点检测器影响。


<details>
  <summary>Details</summary>
Motivation: 医学领域缺乏标记数据，限制了彩色眼底图像配准方法的发展，因此需要无监督学习。

Method: 开发了一种不依赖关键点检测的无监督描述符学习方法，并在公共视网膜图像数据集上进行了全面验证。

Result: 方法在配准精度上与监督方法相当，且对关键点检测器的选择不敏感。

Conclusion: 该研究为医学领域无监督学习的应用提供了重要进展。

Abstract: Current color fundus image registration approaches are limited, among other
things, by the lack of labeled data, which is even more significant in the
medical domain, motivating the use of unsupervised learning. Therefore, in this
work, we develop a novel unsupervised descriptor learning method that does not
rely on keypoint detection. This enables the resulting descriptor network to be
agnostic to the keypoint detector used during the registration inference.
  To validate this approach, we perform an extensive and comprehensive
comparison on the reference public retinal image registration dataset.
Additionally, we test our method with multiple keypoint detectors of varied
nature, even proposing some novel ones. Our results demonstrate that the
proposed approach offers accurate registration, not incurring in any
performance loss versus supervised methods. Additionally, it demonstrates
accurate performance regardless of the keypoint detector used. Thus, this work
represents a notable step towards leveraging unsupervised learning in the
medical domain.

</details>

### [373] [DPNet: Dynamic Pooling Network for Tiny Object Detection](https://arxiv.org/abs/2505.02797)
*Luqi Gong,Haotian Chen,Yikun Chen,Tianliang Yao,Chao Li,Shuai Zhao,Guangjie Han*

Main category: cs.CV

TLDR: 提出了一种动态池化网络（DPNet），用于解决无人机系统中微小物体检测的效率和精度问题。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，微小物体检测对无人机系统至关重要，但传统图像放大方法会增加计算成本和负样本数量，降低检测性能。

Method: DPNet采用灵活的降采样策略，通过动态因子（df）调整特征图分辨率，并设计轻量级预测器和自适应归一化模块（ANM）。

Result: 在TinyCOCO和TinyPerson数据集上，DPNet分别节省了35%和25%的计算量，同时保持检测性能。

Conclusion: DPNet通过动态资源分配，有效平衡了检测精度与效率，适用于无人机系统中的微小物体检测。

Abstract: In unmanned aerial systems, especially in complex environments, accurately
detecting tiny objects is crucial. Resizing images is a common strategy to
improve detection accuracy, particularly for small objects. However, simply
enlarging images significantly increases computational costs and the number of
negative samples, severely degrading detection performance and limiting its
applicability. This paper proposes a Dynamic Pooling Network (DPNet) for tiny
object detection to mitigate these issues. DPNet employs a flexible
down-sampling strategy by introducing a factor (df) to relax the fixed
downsampling process of the feature map to an adjustable one. Furthermore, we
design a lightweight predictor to predict df for each input image, which is
used to decrease the resolution of feature maps in the backbone. Thus, we
achieve input-aware downsampling. We also design an Adaptive Normalization
Module (ANM) to make a unified detector compatible with different dfs. A
guidance loss supervises the predictor's training. DPNet dynamically allocates
computing resources to trade off between detection accuracy and efficiency.
Experiments on the TinyCOCO and TinyPerson datasets show that DPNet can save
over 35% and 25% GFLOPs, respectively, while maintaining comparable detection
performance. The code will be made publicly available.

</details>

### [374] [Database-Agnostic Gait Enrollment using SetTransformers](https://arxiv.org/abs/2505.02815)
*Nicoleta Basoc,Adrian Cosma,Andy Cǎtrunǎ,Emilian Rǎdoi*

Main category: cs.CV

TLDR: 提出了一种基于Transformer的开放集步态识别框架，适用于不同数据集和识别架构，无需特定阈值或重新训练。


<details>
  <summary>Details</summary>
Motivation: 现实场景需要开放集步态识别，但现有方法在封闭集条件下表现良好，开放集识别仍具挑战性。

Method: 使用SetTransformer，基于探针样本和上下文集的嵌入进行决策，解耦识别流程。

Result: 在CASIA-B和PsyMo数据集上验证，方法灵活且优于传统方法。

Conclusion: 框架具有通用性和扩展性，适用于不同场景，代码和数据集将公开。

Abstract: Gait recognition has emerged as a powerful tool for unobtrusive and
long-range identity analysis, with growing relevance in surveillance and
monitoring applications. Although recent advances in deep learning and
large-scale datasets have enabled highly accurate recognition under closed-set
conditions, real-world deployment demands open-set gait enrollment, which means
determining whether a new gait sample corresponds to a known identity or
represents a previously unseen individual. In this work, we introduce a
transformer-based framework for open-set gait enrollment that is both
dataset-agnostic and recognition-architecture-agnostic. Our method leverages a
SetTransformer to make enrollment decisions based on the embedding of a probe
sample and a context set drawn from the gallery, without requiring
task-specific thresholds or retraining for new environments. By decoupling
enrollment from the main recognition pipeline, our model is generalized across
different datasets, gallery sizes, and identity distributions. We propose an
evaluation protocol that uses existing datasets in different ratios of
identities and walks per identity. We instantiate our method using
skeleton-based gait representations and evaluate it on two benchmark datasets
(CASIA-B and PsyMo), using embeddings from three state-of-the-art recognition
models (GaitGraph, GaitFormer, and GaitPT). We show that our method is
flexible, is able to accurately perform enrollment in different scenarios, and
scales better with data compared to traditional approaches. We will make the
code and dataset scenarios publicly available.

</details>

### [375] [MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset via Attention Routing](https://arxiv.org/abs/2505.02823)
*Zinan Guo,Pengze Zhang,Yanze Wu,Chong Mou,Songtao Zhao,Qian He*

Main category: cs.CV

TLDR: MUSAR框架通过单主体训练数据实现多主体定制，解决了数据获取困难和属性纠缠问题。


<details>
  <summary>Details</summary>
Motivation: 当前多主体定制方法面临数据获取困难和属性纠缠的挑战。

Method: 提出MUSAR框架，包括去偏双联学习和动态注意力路由机制。

Result: 实验表明MUSAR在图像质量、主体一致性和交互自然性上优于现有方法。

Conclusion: MUSAR仅需单主体数据即可实现高效多主体定制。

Abstract: Current multi-subject customization approaches encounter two critical
challenges: the difficulty in acquiring diverse multi-subject training data,
and attribute entanglement across different subjects. To bridge these gaps, we
propose MUSAR - a simple yet effective framework to achieve robust
multi-subject customization while requiring only single-subject training data.
Firstly, to break the data limitation, we introduce debiased diptych learning.
It constructs diptych training pairs from single-subject images to facilitate
multi-subject learning, while actively correcting the distribution bias
introduced by diptych construction via static attention routing and dual-branch
LoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic
attention routing mechanism, which adaptively establishes bijective mappings
between generated images and conditional subjects. This design not only
achieves decoupling of multi-subject representations but also maintains
scalable generalization performance with increasing reference subjects.
Comprehensive experiments demonstrate that our MUSAR outperforms existing
methods - even those trained on multi-subject dataset - in image quality,
subject consistency, and interaction naturalness, despite requiring only
single-subject dataset.

</details>

### [376] [Towards Application-Specific Evaluation of Vision Models: Case Studies in Ecology and Biology](https://arxiv.org/abs/2505.02825)
*Alex Hoi Hang Chan,Otto Brookes,Urs Waldmann,Hemal Naik,Iain D. Couzin,Majid Mirmehdi,Noël Adiko Houa,Emmanuelle Normand,Christophe Boesch,Lukas Boesch,Mimi Arandjelovic,Hjalmar Kühl,Tilo Burghardt,Fumihiro Kano*

Main category: cs.CV

TLDR: 论文主张在生态和生物研究中，计算机视觉模型应使用与应用场景直接相关的指标进行评估，而非仅依赖机器学习指标。通过两个案例研究，展示了机器学习性能强的模型在实际应用中可能产生偏差。


<details>
  <summary>Details</summary>
Motivation: 当前计算机视觉模型在生态和生物领域的应用主要依赖机器学习指标，缺乏对下游分析影响的关注。

Method: 通过两个案例研究（黑猩猩数量估计和鸽子头部旋转估计），比较机器学习指标与应用场景指标的差异。

Result: 研究发现，机器学习性能强的模型（如87% mAP）在实际应用中可能导致数据偏差。

Conclusion: 呼吁在生态/生物数据集中集成应用场景指标，以更好地评估模型性能并促进模型在实际工作流中的应用。

Abstract: Computer vision methods have demonstrated considerable potential to
streamline ecological and biological workflows, with a growing number of
datasets and models becoming available to the research community. However,
these resources focus predominantly on evaluation using machine learning
metrics, with relatively little emphasis on how their application impacts
downstream analysis. We argue that models should be evaluated using
application-specific metrics that directly represent model performance in the
context of its final use case. To support this argument, we present two
disparate case studies: (1) estimating chimpanzee abundance and density with
camera trap distance sampling when using a video-based behaviour classifier and
(2) estimating head rotation in pigeons using a 3D posture estimator. We show
that even models with strong machine learning performance (e.g., 87% mAP) can
yield data that leads to discrepancies in abundance estimates compared to
expert-derived data. Similarly, the highest-performing models for posture
estimation do not produce the most accurate inferences of gaze direction in
pigeons. Motivated by these findings, we call for researchers to integrate
application-specific metrics in ecological/biological datasets, allowing for
models to be benchmarked in the context of their downstream application and to
facilitate better integration of models into application workflows.

</details>

### [377] [No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves](https://arxiv.org/abs/2505.02831)
*Dengyang Jiang,Mengmeng Wang,Liuzhuozheng Li,Lei Zhang,Haoyu Wang,Wei Wei,Guang Dai,Yanning Zhang,Jingdong Wang*

Main category: cs.CV

TLDR: 论文提出了一种名为SRA的自表示对齐方法，通过自蒸馏方式在生成训练过程中提升扩散变换器的表示学习能力，无需依赖外部表示组件。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要引入复杂的表示训练框架或依赖大规模预训练表示模型，而扩散变换器独特的判别过程可以内部提供表示指导。

Method: SRA通过自蒸馏方式对齐扩散变换器在不同噪声层级的潜在表示，逐步增强表示学习。

Result: 实验表明，SRA在DiTs和SiTs上均能提升性能，优于依赖复杂辅助框架的方法，且与依赖外部表示先验的方法性能相当。

Conclusion: SRA是一种简单有效的方法，无需外部组件即可提升扩散变换器的表示学习能力。

Abstract: Recent studies have demonstrated that learning a meaningful internal
representation can both accelerate generative training and enhance generation
quality of the diffusion transformers. However, existing approaches necessitate
to either introduce an additional and complex representation training framework
or rely on a large-scale, pre-trained representation foundation model to
provide representation guidance during the original generative training
process. In this study, we posit that the unique discriminative process
inherent to diffusion transformers enables them to offer such guidance without
requiring external representation components. We therefore propose
Self-Representation A}lignment (SRA), a simple yet straightforward method that
obtain representation guidance through a self-distillation manner.
Specifically, SRA aligns the output latent representation of the diffusion
transformer in earlier layer with higher noise to that in later layer with
lower noise to progressively enhance the overall representation learning during
only generative training process. Experimental results indicate that applying
SRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA
not only significantly outperforms approaches relying on auxiliary, complex
representation training frameworks but also achieves performance comparable to
methods that heavily dependent on powerful external representation priors.

</details>

### [378] [Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation](https://arxiv.org/abs/2505.02836)
*Lu Ling,Chen-Hsuan Lin,Tsung-Yi Lin,Yifan Ding,Yu Zeng,Yichen Sheng,Yunhao Ge,Ming-Yu Liu,Aniket Bera,Zhaoshuo Li*

Main category: cs.CV

TLDR: Scenethesis是一个无需训练的框架，结合LLM和视觉模块，从文本生成多样、真实且物理合理的3D交互场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖小规模室内数据集或缺乏空间真实性，限制了场景多样性和布局复杂性。

Method: Scenethesis通过LLM生成粗略布局，视觉模块优化布局，优化模块确保物理合理性，最后由判断模块验证空间一致性。

Result: 实验表明，Scenethesis能生成多样、真实且物理合理的3D交互场景。

Conclusion: Scenethesis在虚拟内容创作、仿真环境和具身AI研究中具有重要价值。

Abstract: Synthesizing interactive 3D scenes from text is essential for gaming, virtual
reality, and embodied AI. However, existing methods face several challenges.
Learning-based approaches depend on small-scale indoor datasets, limiting the
scene diversity and layout complexity. While large language models (LLMs) can
leverage diverse text-domain knowledge, they struggle with spatial realism,
often producing unnatural object placements that fail to respect common sense.
Our key insight is that vision perception can bridge this gap by providing
realistic spatial guidance that LLMs lack. To this end, we introduce
Scenethesis, a training-free agentic framework that integrates LLM-based scene
planning with vision-guided layout refinement. Given a text prompt, Scenethesis
first employs an LLM to draft a coarse layout. A vision module then refines it
by generating an image guidance and extracting scene structure to capture
inter-object relations. Next, an optimization module iteratively enforces
accurate pose alignment and physical plausibility, preventing artifacts like
object penetration and instability. Finally, a judge module verifies spatial
coherence. Comprehensive experiments show that Scenethesis generates diverse,
realistic, and physically plausible 3D interactive scenes, making it valuable
for virtual content creation, simulation environments, and embodied AI
research.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [379] [Fast Likelihood-Free Parameter Estimation for Lévy Processes](https://arxiv.org/abs/2505.01639)
*Nicolas Coloma,William Kleiber*

Main category: stat.ML

TLDR: 提出了一种基于神经贝叶斯估计（NBE）的快速、准确的L\'evy过程参数估计方法，优于传统方法，适用于高频金融数据。


<details>
  <summary>Details</summary>
Motivation: L\'evy过程在金融建模中广泛应用，但参数估计困难，尤其是当似然函数难以计算时。

Method: 利用模拟数据和置换不变神经网络，通过NBE框架近似贝叶斯估计器。

Result: NBE在多个L\'evy模型中表现出更高的准确性和更快的运行速度，支持快速不确定性量化。

Conclusion: NBE为复杂金融模型提供了一种可扩展且实用的解决方案，显著降低了计算成本。

Abstract: L\'evy processes are widely used in financial modeling due to their ability
to capture discontinuities and heavy tails, which are common in high-frequency
asset return data. However, parameter estimation remains a challenge when
associated likelihoods are unavailable or costly to compute. We propose a fast
and accurate method for L\'evy parameter estimation using the neural Bayes
estimation (NBE) framework -- a simulation-based, likelihood-free approach that
leverages permutation-invariant neural networks to approximate Bayes
estimators. Through extensive simulations across several L\'evy models, we show
that NBE outperforms traditional methods in both accuracy and runtime, while
also enabling rapid bootstrap-based uncertainty quantification. We illustrate
our approach on a challenging high-frequency cryptocurrency return dataset,
where the method captures evolving parameter dynamics and delivers reliable and
interpretable inference at a fraction of the computational cost of traditional
methods. NBE provides a scalable and practical solution for inference in
complex financial models, enabling parameter estimation and uncertainty
quantification over an entire year of data in just seconds. We additionally
investigate nearly a decade of high-frequency Bitcoin returns, requiring less
than one minute to estimate parameters under the proposed approach.

</details>

### [380] [TV-SurvCaus: Dynamic Representation Balancing for Causal Survival Analysis](https://arxiv.org/abs/2505.01785)
*Ayoub Abraich*

Main category: stat.ML

TLDR: TV-SurvCaus是一个新颖的框架，用于在生存分析中估计时变治疗的因果效应，通过表示平衡技术和理论保证提升准确性。


<details>
  <summary>Details</summary>
Motivation: 解决医学等领域中时变治疗对生存结果的因果效应估计问题，填补动态治疗机制与生存结果结合的空白。

Method: 提出TV-SurvCaus框架，结合表示平衡技术和序列建模，处理时变依赖关系，并提供五项理论保证。

Result: 在合成和真实数据集上，TV-SurvCaus优于现有方法，能更准确地估计个体化治疗效果。

Conclusion: TV-SurvCaus推动了因果推断领域，为动态纵向生存分析提供了更准确的估计工具。

Abstract: Estimating the causal effect of time-varying treatments on survival outcomes
is a challenging task in many domains, particularly in medicine where treatment
protocols adapt over time. While recent advances in representation learning
have improved causal inference for static treatments, extending these methods
to dynamic treatment regimes with survival outcomes remains under-explored. In
this paper, we introduce TV-SurvCaus, a novel framework that extends
representation balancing techniques to the time-varying treatment setting for
survival analysis. We provide theoretical guarantees through (1) a generalized
bound for time-varying precision in estimation of heterogeneous effects, (2)
variance control via sequential balancing weights, (3) consistency results for
dynamic treatment regimes, (4) convergence rates for representation learning
with temporal dependencies, and (5) a formal bound on the bias due to
treatment-confounder feedback. Our neural architecture incorporates sequence
modeling to handle temporal dependencies while balancing time-dependent
representations. Through extensive experiments on both synthetic and real-world
datasets, we demonstrate that TV-SurvCaus outperforms existing methods in
estimating individualized treatment effects with time-varying covariates and
treatments. Our framework advances the field of causal inference by enabling
more accurate estimation of treatment effects in dynamic, longitudinal settings
with survival outcomes.

</details>

### [381] [Bayesian learning of the optimal action-value function in a Markov decision process](https://arxiv.org/abs/2505.01859)
*Jiaqi Guo,Chon Wai Ho,Sumeetpal S. Singh*

Main category: stat.ML

TLDR: 本文提出了一种完整的贝叶斯框架，用于无限时域和无折扣的MDP问题，通过引入最小假设的似然函数和自适应序贯蒙特卡洛算法，改进了不确定性量化和决策策略。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯方法在MDP中学习最优决策策略时，常基于不现实的建模假设和近似推断技术，导致其不确定性量化的可靠性存疑。

Method: 提出基于贝尔曼最优性方程的最小假设似然函数，引入人工观测噪声解决退化问题；采用自适应序贯蒙特卡洛算法进行推断；通过后验采样选择动作。

Result: 在Deep Sea基准问题上验证了后验采样在MDP中的探索优势。

Conclusion: 该框架为MDP中的贝叶斯不确定性量化和决策策略提供了更可靠的方法，并揭示了其与Thompson采样的联系。

Abstract: The Markov Decision Process (MDP) is a popular framework for sequential
decision-making problems, and uncertainty quantification is an essential
component of it to learn optimal decision-making strategies. In particular, a
Bayesian framework is used to maintain beliefs about the optimal decisions and
the unknown ingredients of the model, which are also to be learned from the
data, such as the rewards and state dynamics. However, many existing Bayesian
approaches for learning the optimal decision-making strategy are based on
unrealistic modelling assumptions and utilise approximate inference techniques.
This raises doubts whether the benefits of Bayesian uncertainty quantification
are fully realised or can be relied upon.
  We focus on infinite-horizon and undiscounted MDPs, with finite state and
action spaces, and a terminal state. We provide a full Bayesian framework, from
modelling to inference to decision-making. For modelling, we introduce a
likelihood function with minimal assumptions for learning the optimal
action-value function based on Bellman's optimality equations, analyse its
properties, and clarify connections to existing works. For deterministic
rewards, the likelihood is degenerate and we introduce artificial observation
noise to relax it, in a controlled manner, to facilitate more efficient Monte
Carlo-based inference. For inference, we propose an adaptive sequential Monte
Carlo algorithm to both sample from and adjust the sequence of relaxed
posterior distributions. For decision-making, we choose actions using samples
from the posterior distribution over the optimal strategies. While commonly
done, we provide new insight that clearly shows that it is a generalisation of
Thompson sampling from multi-arm bandit problems. Finally, we evaluate our
framework on the Deep Sea benchmark problem and demonstrate the exploration
benefits of posterior sampling in MDPs.

</details>

### [382] [Extended Fiducial Inference for Individual Treatment Effects via Deep Neural Networks](https://arxiv.org/abs/2505.01995)
*Sehwan Kim,Faming Liang*

Main category: stat.ML

TLDR: 论文提出了一种基于双重神经网络（Double-NN）的方法，用于个体治疗效果估计，并在扩展基准推断（EFI）框架下验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决个体治疗效果估计问题，并推动大规模模型统计推断的理论与方法发展。

Method: 使用双重神经网络分别建模治疗和对照效应函数，并通过额外神经网络估计参数。

Result: Double-NN方法在个体治疗效果估计上优于CQR方法，且理论证明模型规模可随样本量增长。

Conclusion: 该方法为大规模神经网络模型的统计不确定性量化提供了严格框架，显著推进了统计推断理论。

Abstract: Individual treatment effect estimation has gained significant attention in
recent data science literature. This work introduces the Double Neural Network
(Double-NN) method to address this problem within the framework of extended
fiducial inference (EFI). In the proposed method, deep neural networks are used
to model the treatment and control effect functions, while an additional neural
network is employed to estimate their parameters. The universal approximation
capability of deep neural networks ensures the broad applicability of this
method. Numerical results highlight the superior performance of the proposed
Double-NN method compared to the conformal quantile regression (CQR) method in
individual treatment effect estimation. From the perspective of statistical
inference, this work advances the theory and methodology for statistical
inference of large models. Specifically, it is theoretically proven that the
proposed method permits the model size to increase with the sample size $n$ at
a rate of $O(n^{\zeta})$ for some $0 \leq \zeta<1$, while still maintaining
proper quantification of uncertainty in the model parameters. This result marks
a significant improvement compared to the range $0\leq \zeta < \frac{1}{2}$
required by the classical central limit theorem. Furthermore, this work
provides a rigorous framework for quantifying the uncertainty of deep neural
networks under the neural scaling law, representing a substantial contribution
to the statistical understanding of large-scale neural network models.

</details>

### [383] [Learning the Simplest Neural ODE](https://arxiv.org/abs/2505.02019)
*Yuji Okamoto,Tomoya Takeuchi,Yusuke Sakemi*

Main category: stat.ML

TLDR: 论文探讨了训练神经ODE的困难，并提出了一种新的稳定化方法，同时提供了收敛性分析。


<details>
  <summary>Details</summary>
Motivation: 神经ODE在系统识别和时间序列预测等领域有广泛应用，但训练过程存在挑战。

Method: 通过一维线性模型分析训练难点，提出新的稳定化方法并进行收敛性分析。

Result: 研究揭示了训练神经ODE的困难原因，并提出了有效的解决方案。

Conclusion: 本文为神经ODE初学者提供了简明教程，同时提出了改进训练稳定性的方法。

Abstract: Since the advent of the ``Neural Ordinary Differential Equation (Neural
ODE)'' paper, learning ODEs with deep learning has been applied to system
identification, time-series forecasting, and related areas. Exploiting the
diffeomorphic nature of ODE solution maps, neural ODEs has also enabled their
use in generative modeling. Despite the rich potential to incorporate various
kinds of physical information, training Neural ODEs remains challenging in
practice. This study demonstrates, through the simplest one-dimensional linear
model, why training Neural ODEs is difficult. We then propose a new
stabilization method and provide an analytical convergence analysis. The
insights and techniques presented here serve as a concise tutorial for
researchers beginning work on Neural ODEs.

</details>

### [384] [Ranked differences Pearson correlation dissimilarity with an application to electricity users time series clustering](https://arxiv.org/abs/2505.02173)
*Chutiphan Charoensuk,Nathakhun Wiroonsri*

Main category: stat.ML

TLDR: 提出了一种新的时间序列聚类方法RDPC，结合了加权平均和皮尔逊相关差异，在复杂情况下优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 时间序列聚类在多个领域有广泛应用，但现有方法多基于欧氏距离或关联差异，需更高效的差异度量方法。

Method: 提出RDPC差异度量，结合加权平均和皮尔逊相关差异，并用于层次聚类。

Result: RDPC算法在复杂情况下（如不同季节模式、趋势和峰值）表现优于其他算法。

Conclusion: RDPC方法在泰国电力消费数据集中成功将客户分为七组，展示了其有效性。

Abstract: Time series clustering is an unsupervised learning method for classifying
time series data into groups with similar behavior. It is used in applications
such as healthcare, finance, economics, energy, and climate science. Several
time series clustering methods have been introduced and used for over four
decades. Most of them focus on measuring either Euclidean distances or
association dissimilarities between time series. In this work, we propose a new
dissimilarity measure called ranked Pearson correlation dissimilarity (RDPC),
which combines a weighted average of a specified fraction of the largest
element-wise differences with the well-known Pearson correlation dissimilarity.
It is incorporated into hierarchical clustering. The performance is evaluated
and compared with existing clustering algorithms. The results show that the
RDPC algorithm outperforms others in complicated cases involving different
seasonal patterns, trends, and peaks. Finally, we demonstrate our method by
clustering a random sample of customers from a Thai electricity consumption
time series dataset into seven groups with unique characteristics.

</details>

### [385] [Resolving Memorization in Empirical Diffusion Model for Manifold Data in High-Dimensional Spaces](https://arxiv.org/abs/2505.02508)
*Yang Lyu,Yuchun Qian,Tan Minh Nguyen,Xin T. Tong*

Main category: stat.ML

TLDR: 论文提出了一种简单的惯性更新步骤解决扩散模型中的记忆效应问题，无需额外训练，且能生成新数据样本。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成新数据样本时存在记忆效应问题，即只能生成已有数据点。本文旨在通过简单方法解决这一问题。

Method: 在经验扩散模型模拟结束时加入惯性更新步骤，仅需扩散模型的评分函数，无需额外训练。

Result: 惯性扩散模型的样本分布是数据分布的Wasserstein-1近似，且上界不依赖环境空间维度。

Conclusion: 惯性扩散模型能有效生成新样本，揭示了扩散模型与流形学习的有趣联系。

Abstract: Diffusion models is a popular computational tool to generate new data
samples. It utilizes a forward diffusion process that add noise to the data
distribution and then use a reverse process to remove noises to produce samples
from the data distribution. However, when the empirical data distribution
consists of $n$ data point, using the empirical diffusion model will
necessarily produce one of the existing data points. This is often referred to
as the memorization effect, which is usually resolved by sophisticated machine
learning procedures in the current literature. This work shows that the
memorization problem can be resolved by a simple inertia update step at the end
of the empirical diffusion model simulation. Our inertial diffusion model
requires only the empirical diffusion model score function and it does not
require any further training. We show that choosing the inertia diffusion model
sample distribution is an $O\left(n^{-\frac{2}{d+4}}\right)$ Wasserstein-1
approximation of a data distribution lying on a $C^2$ manifold of dimension
$d$. Since this estimate is significant smaller the Wasserstein1 distance
between population and empirical distributions, it rigorously shows the
inertial diffusion model produces new data samples. Remarkably, this upper
bound is completely free of the ambient space dimension, since there is no
training involved. Our analysis utilizes the fact that the inertial diffusion
model samples are approximately distributed as the Gaussian kernel density
estimator on the manifold. This reveals an interesting connection between
diffusion model and manifold learning.

</details>

<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [386] [Enhancing Black-Litterman Portfolio via Hybrid Forecasting Model Combining Multivariate Decomposition and Noise Reduction](https://arxiv.org/abs/2505.01781)
*Ziye Yang,Ke Lu*

Main category: cs.CE

TLDR: 论文提出了一种结合SSA、MA-EMD和TCN的混合深度学习模型，用于提升资产价格预测准确性，进而优化Black-Litterman模型的主观观点生成能力。实验表明该模型优于基准模型，并在投资组合中表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统Mean-Variance模型对输入参数敏感且缺乏灵活性，而Black-Litterman模型通过结合市场均衡收益和投资者主观观点更受关注。本文旨在提升Black-Litterman模型的主观观点生成能力。

Method: 提出了一种混合深度学习模型，结合SSA、MA-EMD和TCN，用于资产价格预测。通过噪声预处理提升模型准确性。

Result: 实验表明该模型预测性能显著优于三种基准模型。在NASDAQ 100指数的20只股票投资组合中，结合Black-Litterman模型的表现优于Mean-Variance、等权重和市场权重模型。

Conclusion: 混合预测模型与Black-Litterman模型的结合在投资组合中展现出更好的收益和风险控制能力。

Abstract: The sensitivity to input parameters and lack of flexibility limits the
traditional Mean-Variance model. In contrast, the Black-Litterman model has
attracted widespread attention by integrating market equilibrium returns with
investors' subjective views. This paper proposes a novel hybrid deep learning
model combining Singular Spectrum analysis (SSA), Multivariate Aligned
Empirical Mode Decomposition (MA-EMD), and Temporal Convolutional Networks
(TCNs), aiming to improve the prediction accuracy of asset prices and thus
enhance the ability of the Black-Litterman model to generate subjective views.
Experimental results show that noise reduction pre-processing can improve the
model's accuracy, and the prediction performance of the proposed model is
significantly better than that of three multivariate decomposition benchmark
models. We construct an investment portfolio by using 20 representative stocks
from the NASDAQ 100 index. By combining the hybrid forecasting model with the
Black-Litterman model, the generated investment portfolio exhibits better
returns and risk control capabilities than the Mean-Variance, Equal-Weighted,
and Market-Weighted models in the short holding period.

</details>

### [387] [Representation Learning of Limit Order Book: A Comprehensive Study and Benchmarking](https://arxiv.org/abs/2505.02139)
*Muyao Zhong,Yushi Lin,Peng Yang*

Main category: cs.CE

TLDR: 本文系统研究了限价订单簿（LOB）表示学习，提出LOBench基准，验证了LOB表示的有效性和必要性。


<details>
  <summary>Details</summary>
Motivation: LOB数据具有强自相关性、跨特征约束和特征尺度差异，现有方法未能独立分析学习到的表示，限制了其复用性和泛化能力。

Method: 引入LOBench基准，包括真实中国A股市场数据、统一预处理、评估指标和基线模型，进行系统比较研究。

Result: 实验证明LOB表示对多种下游任务有效，优于传统任务特定端到端模型和通用时间序列表示学习模型。

Conclusion: 建立可复现框架，为未来研究提供明确指导，数据集和代码公开。

Abstract: The Limit Order Book (LOB), the mostly fundamental data of the financial
market, provides a fine-grained view of market dynamics while poses significant
challenges in dealing with the esteemed deep models due to its strong
autocorrelation, cross-feature constrains, and feature scale disparity.
Existing approaches often tightly couple representation learning with specific
downstream tasks in an end-to-end manner, failed to analyze the learned
representations individually and explicitly, limiting their reusability and
generalization. This paper conducts the first systematic comparative study of
LOB representation learning, aiming to identify the effective way of extracting
transferable, compact features that capture essential LOB properties. We
introduce LOBench, a standardized benchmark with real China A-share market
data, offering curated datasets, unified preprocessing, consistent evaluation
metrics, and strong baselines. Extensive experiments validate the sufficiency
and necessity of LOB representations for various downstream tasks and highlight
their advantages over both the traditional task-specific end-to-end models and
the advanced representation learning models for general time series. Our work
establishes a reproducible framework and provides clear guidelines for future
research. Datasets and code will be publicly available at
https://github.com/financial-simulation-lab/LOBench.

</details>

### [388] [Data-Driven Team Selection in Fantasy Premier League Using Integer Programming and Predictive Modeling Approach](https://arxiv.org/abs/2505.02170)
*Danial Ramezani*

Main category: cs.CE

TLDR: 本文提出了一种新颖的确定性和鲁棒整数规划模型，用于优化梦幻足球的阵容选择，结合了可解释的人工智能框架和比赛数据，并通过2023/24赛季英超数据验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 梦幻足球是一个价值数十亿美元的产业，但决策者在固定预算下需要优化阵容选择以最大化得分。

Method: 构建了新的混合评分指标，结合了可解释的人工智能框架和比赛数据，并提出了多种目标函数和估计技术。

Result: 提出的混合方法在2023/24赛季英超数据中表现最佳，蒙特卡洛模拟验证了其有效性。

Conclusion: 该研究为梦幻足球策略提供了新视角，并通过模型分析提供了有价值的阵容选择建议。

Abstract: Fantasy football is a billion-dollar industry with millions of participants.
Constrained by a fixed budget, decision-makers draft a squad whose players are
expected to perform well in the upcoming weeks to maximize total points. This
paper proposes novel deterministic and robust integer programming models that
select the optimal starting eleven and the captain. A new hybrid scoring metric
is constructed using an interpretable artificial intelligence framework and
underlying match performance data. Several objective functions and estimation
techniques are introduced for the programming model. To the best of my
knowledge, this is the first study to approach fantasy football through this
lens. The models' performance is evaluated using data from the 2023/24 Premier
League season. Results indicate that the proposed hybrid method achieved the
highest score while maintaining consistent performance. Utilizing the Monte
Carlo simulation, the strategic choice of averaging techniques for estimating
cost vectors, and the proposed hybrid approach are shown to be effective during
the out-of-sample period. This paper also provides a thorough analysis of the
optimal formations and players selected by the models, offering valuable
insights into effective fantasy football strategies.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [389] [A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI](https://arxiv.org/abs/2505.01458)
*Lik Hang Kenny Wong,Xueyang Kang,Kaixin Bai,Jianwei Zhang*

Main category: cs.RO

TLDR: 本文综述了物理模拟器在缩小仿真与现实差距中的作用，分析了其特性、任务适用性及硬件需求，并提供了资源推荐。


<details>
  <summary>Details</summary>
Motivation: 解决仿真与现实之间的差距问题，降低在真实世界中训练智能体的成本和复杂性。

Method: 分析物理模拟器的特性和功能，比较其在导航和操作任务中的表现，并评估硬件需求。

Result: 总结了模拟器的关键特性，提供了资源推荐，包括数据集、指标、平台和前沿方法。

Conclusion: 物理模拟器是缩小仿真与现实差距的重要工具，需结合硬件限制选择合适的模拟器和方法。

Abstract: Navigation and manipulation are core capabilities in Embodied AI, yet
training agents with these capabilities in the real world faces high costs and
time complexity. Therefore, sim-to-real transfer has emerged as a key approach,
yet the sim-to-real gap persists. This survey examines how physics simulators
address this gap by analyzing their properties overlooked in previous surveys.
We also analyze their features for navigation and manipulation tasks, along
with hardware requirements. Additionally, we offer a resource with benchmark
datasets, metrics, simulation platforms, and cutting-edge methods-such as world
models and geometric equivariance-to help researchers select suitable tools
while accounting for hardware constraints.

</details>

### [390] [RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation](https://arxiv.org/abs/2505.01709)
*Kaidong Zhang,Rongtao Xu,Pengzhen Ren,Junfan Lin,Hefeng Wu,Liang Lin,Xiaodan Liang*

Main category: cs.RO

TLDR: RoBridge是一个分层智能架构，结合了认知规划和强化学习，解决了机器人操作中的认知与执行问题，显著提升了任务成功率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在开放环境中面临的程序性技能和声明性技能困境，提升其认知与执行能力。

Method: 提出RoBridge架构，包括基于预训练视觉语言模型的高层认知规划器（HCP）、不变可操作表示（IOR）和通用体现代理（GEA）。

Result: 在新任务中达到75%的成功率，仅需5个真实数据样本即可实现83%的模拟到现实的泛化成功率。

Conclusion: RoBridge为机器人系统整合认知推理与物理执行提供了新范式，是通用机器人操作的重要进展。

Abstract: Operating robots in open-ended scenarios with diverse tasks is a crucial
research and application direction in robotics. While recent progress in
natural language processing and large multimodal models has enhanced robots'
ability to understand complex instructions, robot manipulation still faces the
procedural skill dilemma and the declarative skill dilemma in open
environments. Existing methods often compromise cognitive and executive
capabilities. To address these challenges, in this paper, we propose RoBridge,
a hierarchical intelligent architecture for general robotic manipulation. It
consists of a high-level cognitive planner (HCP) based on a large-scale
pre-trained vision-language model (VLM), an invariant operable representation
(IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA).
RoBridge maintains the declarative skill of VLM and unleashes the procedural
skill of reinforcement learning, effectively bridging the gap between cognition
and execution. RoBridge demonstrates significant performance improvements over
existing baselines, achieving a 75% success rate on new tasks and an 83%
average success rate in sim-to-real generalization using only five real-world
data samples per task. This work represents a significant step towards
integrating cognitive reasoning with physical execution in robotic systems,
offering a new paradigm for general robotic manipulation.

</details>

### [391] [Semantic Intelligence: Integrating GPT-4 with A Planning in Low-Cost Robotics](https://arxiv.org/abs/2505.01931)
*Jesse Barkley,Abraham George,Amir Barati Farimani*

Main category: cs.RO

TLDR: 论文提出了一种结合GPT-4语义推理与A*算法的混合路径规划框架，用于低成本机器人平台，解决了传统几何路径规划无法处理高级语义指令的问题。


<details>
  <summary>Details</summary>
Motivation: 传统机器人导航依赖硬编码状态机和纯几何路径规划，无法理解高级语义指令。

Method: 通过将GPT-4的语义推理与A*算法结合，利用提示驱动的GPT-4处理任务逻辑，同时保留A*的精确路径计算。

Result: 实验表明，GPT-4集成系统在语义任务上成功率高达96-100%，而纯几何规划器无法完成此类任务。

Conclusion: 该研究表明，通过结合大型语言模型推理，低成本机器人可以实现智能、上下文感知的行为。

Abstract: Classical robot navigation often relies on hardcoded state machines and
purely geometric path planners, limiting a robot's ability to interpret
high-level semantic instructions. In this paper, we first assess GPT-4's
ability to act as a path planner compared to the A* algorithm, then present a
hybrid planning framework that integrates GPT-4's semantic reasoning with A* on
a low-cost robot platform operating on ROS2 Humble. Our approach eliminates
explicit finite state machine (FSM) coding by using prompt-based GPT-4
reasoning to handle task logic while maintaining the accurate paths computed by
A*. The GPT-4 module provides semantic understanding of instructions and
environmental cues (e.g., recognizing toxic obstacles or crowded areas to
avoid, or understanding low-battery situations requiring alternate route
selection), and dynamically adjusts the robot's occupancy grid via obstacle
buffering to enforce semantic constraints. We demonstrate multi-step reasoning
for sequential tasks, such as first navigating to a resource goal and then
reaching a final destination safely. Experiments on a Petoi Bittle robot with
an overhead camera and Raspberry Pi Zero 2W compare classical A* against
GPT-4-assisted planning. Results show that while A* is faster and more accurate
for basic route generation and obstacle avoidance, the GPT-4-integrated system
achieves high success rates (96-100%) on semantic tasks that are infeasible for
pure geometric planners. This work highlights how affordable robots can exhibit
intelligent, context-aware behaviors by leveraging large language model
reasoning with minimal hardware and no fine-tuning.

</details>

### [392] [SafeNav: Safe Path Navigation using Landmark Based Localization in a GPS-denied Environment](https://arxiv.org/abs/2505.01956)
*Ganesh Sapkota,Sanjay Madria*

Main category: cs.RO

TLDR: 论文提出LanBLoc-BMM方法，结合地标定位和战场运动模型，性能优于现有视觉定位算法，并引入两种安全导航方法。


<details>
  <summary>Details</summary>
Motivation: 战场环境中GPS信号易受干扰，传统视觉定位方法复杂且计算量大，无范围方法在稀疏动态网络中精度不足。

Method: 结合地标定位（LanBLoc）、战场运动模型（BMM）和扩展卡尔曼滤波（EKF），并与风险感知RRT*算法集成。

Result: LanBLoc-BMM(EKF)在真实模拟数据集上表现最优，SafeNav-Centroid在精度和风险控制上表现突出，SafeNav-CHull计算速度更快。

Conclusion: LanBLoc-BMM及其安全导航方法在战场环境中具有高效性和实用性。

Abstract: In battlefield environments, adversaries frequently disrupt GPS signals,
requiring alternative localization and navigation methods. Traditional
vision-based approaches like Simultaneous Localization and Mapping (SLAM) and
Visual Odometry (VO) involve complex sensor fusion and high computational
demand, whereas range-free methods like DV-HOP face accuracy and stability
challenges in sparse, dynamic networks. This paper proposes LanBLoc-BMM, a
navigation approach using landmark-based localization (LanBLoc) combined with a
battlefield-specific motion model (BMM) and Extended Kalman Filter (EKF). Its
performance is benchmarked against three state-of-the-art visual localization
algorithms integrated with BMM and Bayesian filters, evaluated on synthetic and
real-imitated trajectory datasets using metrics including Average Displacement
Error (ADE), Final Displacement Error (FDE), and a newly introduced Average
Weighted Risk Score (AWRS). LanBLoc-BMM (with EKF) demonstrates superior
performance in ADE, FDE, and AWRS on real-imitated datasets. Additionally, two
safe navigation methods, SafeNav-CHull and SafeNav-Centroid, are introduced by
integrating LanBLoc-BMM(EKF) with a novel Risk-Aware RRT* (RAw-RRT*) algorithm
for obstacle avoidance and risk exposure minimization. Simulation results in
battlefield scenarios indicate SafeNav-Centroid excels in accuracy, risk
exposure, and trajectory efficiency, while SafeNav-CHull provides superior
computational speed.

</details>

### [393] [A Goal-Oriented Reinforcement Learning-Based Path Planning Algorithm for Modular Self-Reconfigurable Satellites](https://arxiv.org/abs/2505.01966)
*Bofei Liu,Dong Ye,Zunhao Yao,Zhaowei Sun*

Main category: cs.RO

TLDR: 本文提出了一种基于目标导向强化学习的路径规划算法，解决了模块化自重构卫星在多目标配置下的路径规划问题，并显著提高了成功率。


<details>
  <summary>Details</summary>
Motivation: 现有路径规划算法存在计算复杂度高、泛化能力差及对多目标配置支持有限的问题，因此需要一种更高效的解决方案。

Method: 采用目标导向强化学习算法，结合Hindsight Experience Replay和Invalid Action Masking技术，以应对稀疏奖励和无效动作的挑战。

Result: 在由4个和6个单元组成的模块化卫星集群中，模型分别达到了95%和73%的成功率。

Conclusion: 该算法有效解决了多目标配置的路径规划问题，为模块化自重构卫星的任务执行提供了可靠支持。

Abstract: Modular self-reconfigurable satellites refer to satellite clusters composed
of individual modular units capable of altering their configurations. The
configuration changes enable the execution of diverse tasks and mission
objectives. Existing path planning algorithms for reconfiguration often suffer
from high computational complexity, poor generalization capability, and limited
support for diverse target configurations. To address these challenges, this
paper proposes a goal-oriented reinforcement learning-based path planning
algorithm. This algorithm is the first to address the challenge that previous
reinforcement learning methods failed to overcome, namely handling multiple
target configurations. Moreover, techniques such as Hindsight Experience Replay
and Invalid Action Masking are incorporated to overcome the significant
obstacles posed by sparse rewards and invalid actions. Based on these designs,
our model achieves a 95% and 73% success rate in reaching arbitrary target
configurations in a modular satellite cluster composed of four and six units,
respectively.

</details>

### [394] [A Synergistic Framework of Nonlinear Acoustic Computing and Reinforcement Learning for Real-World Human-Robot Interaction](https://arxiv.org/abs/2505.01998)
*Xiaoliang Chen,Xin Yu,Le Chang,Yunhe Huang,Jiashuai He,Shibo Zhang,Jin Li,Likai Lin,Ziyu Zeng,Xianling Tu,Shuyu Zhang*

Main category: cs.RO

TLDR: 提出了一种结合非线性声学计算和强化学习的新框架，用于在复杂噪声和混响环境下优化人机交互。


<details>
  <summary>Details</summary>
Motivation: 解决复杂噪声和混响环境下人机交互的挑战，提升系统性能。

Method: 利用物理波方程（如Westervelt、KZK）建模高阶声学现象，并通过强化学习优化关键参数（如吸收、波束成形）。

Result: 实验表明，该方法在远场定位、弱信号检测和多语言语音识别中优于传统线性方法和纯数据驱动基线。

Conclusion: 该框架在AI硬件、机器人、机器听觉等领域具有广泛应用前景。

Abstract: This paper introduces a novel framework integrating nonlinear acoustic
computing and reinforcement learning to enhance advanced human-robot
interaction under complex noise and reverberation. Leveraging physically
informed wave equations (e.g., Westervelt, KZK), the approach captures
higher-order phenomena such as harmonic generation and shock formation. By
embedding these models in a reinforcement learning-driven control loop, the
system adaptively optimizes key parameters (e.g., absorption, beamforming) to
mitigate multipath interference and non-stationary noise. Experimental
evaluations-covering far-field localization, weak signal detection, and
multilingual speech recognition-demonstrate that this hybrid strategy surpasses
traditional linear methods and purely data-driven baselines, achieving superior
noise suppression, minimal latency, and robust accuracy in demanding real-world
scenarios. The proposed system demonstrates broad application prospects in AI
hardware, robot, machine audition, artificial audition, and brain-machine
interfaces.

</details>

### [395] [T-REX: Vision-Based System for Autonomous Leaf Detection and Grasp Estimation](https://arxiv.org/abs/2505.01654)
*Srecharan Selvam,Abhisesh Silwal,George Kantor*

Main category: cs.RO

TLDR: T-Rex是一个用于温室环境中自主定位、选择和抓取叶片的机器人系统，结合了立体视觉和6自由度机械臂，实现了66.6%的抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 开发T-Rex旨在推动可控环境农业（CEA）中的植物采样自动化，解决人工采样的效率问题。

Method: 系统使用YOLOv8进行叶片分割，RAFT-Stereo生成深度图，结合叶片抓取算法选择最优叶片和抓取点，并通过ROS控制机械臂执行轨迹。

Result: 实验表明，T-Rex在人工植物上实现了66.6%的抓取成功率。

Conclusion: T-Rex展示了在CEA中实现植物采样自动化的潜力，为进一步研究奠定了基础。

Abstract: T-Rex (The Robot for Extracting Leaf Samples) is a gantry-based robotic
system developed for autonomous leaf localization, selection, and grasping in
greenhouse environments. The system integrates a 6-degree-of-freedom
manipulator with a stereo vision pipeline to identify and interact with target
leaves. YOLOv8 is used for real-time leaf segmentation, and RAFT-Stereo
provides dense depth maps, allowing the reconstruction of 3D leaf masks. These
observations are processed through a leaf grasping algorithm that selects the
optimal leaf based on clutter, visibility, and distance, and determines a grasp
point by analyzing local surface flatness, top-down approachability, and margin
from edges. The selected grasp point guides a trajectory executed by ROS-based
motion controllers, driving a custom microneedle-equipped end-effector to clamp
the leaf and simulate tissue sampling. Experiments conducted with artificial
plants under varied poses demonstrate that the T-Rex system can consistently
detect, plan, and perform physical interactions with plant-like targets,
achieving a grasp success rate of 66.6\%. This paper presents the system
architecture, implementation, and testing of T-Rex as a step toward plant
sampling automation in Controlled Environment Agriculture (CEA).

</details>

### [396] [Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher Learning](https://arxiv.org/abs/2505.02232)
*Malte Mosbach,Sven Behnke*

Main category: cs.RO

TLDR: 提出了一种结合可提示基础模型与强化学习的新方法，用于机器人灵巧操作任务，解决了现有方法无法将高级命令与精细控制结合的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将高级命令与精细的灵巧控制结合，限制了机器人在复杂场景中的操作能力。

Method: 采用记忆增强的师生学习框架，结合Segment-Anything 2（SAM 2）模型作为感知骨干，从用户提示中推断感兴趣物体。

Result: 成功学习了响应提示的策略，并在杂乱场景中实现了物体拾取任务。

Conclusion: 该方法为机器人灵巧操作提供了一种高效且可扩展的解决方案。

Abstract: Building models responsive to input prompts represents a transformative shift
in machine learning. This paradigm holds significant potential for robotics
problems, such as targeted manipulation amidst clutter. In this work, we
present a novel approach to combine promptable foundation models with
reinforcement learning (RL), enabling robots to perform dexterous manipulation
tasks in a prompt-responsive manner. Existing methods struggle to link
high-level commands with fine-grained dexterous control. We address this gap
with a memory-augmented student-teacher learning framework. We use the
Segment-Anything 2 (SAM 2) model as a perception backbone to infer an object of
interest from user prompts. While detections are imperfect, their temporal
sequence provides rich information for implicit state estimation by
memory-augmented models. Our approach successfully learns prompt-responsive
policies, demonstrated in picking objects from cluttered scenes. Videos and
code are available at https://memory-student-teacher.github.io

</details>

### [397] [Robust Localization, Mapping, and Navigation for Quadruped Robots](https://arxiv.org/abs/2505.02272)
*Dyuman Aditya,Junning Huang,Nico Bohlinger,Piotr Kicki,Krzysztof Walas,Jan Peters,Matteo Luperto,Davide Tateo*

Main category: cs.RO

TLDR: 该论文提出了一种低成本四足机器人的鲁棒定位、建图和导航系统，结合了接触辅助运动学、视觉惯性里程计和深度稳定视觉技术，提高了系统的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了推动四足机器人在现实世界中的广泛应用，需要依赖低成本传感器（如深度相机）的鲁棒导航系统。

Method: 结合接触辅助运动学、视觉惯性里程计和深度稳定视觉技术，构建了一个低成本四足机器人的定位、建图和导航系统。

Result: 在仿真和两种真实四足机器人平台上，系统能够生成精确的2D环境地图，实现鲁棒的自主定位和导航。

Conclusion: 该系统为低成本四足机器人的鲁棒导航提供了可行方案，并通过消融实验验证了各组件对定位精度的影响。

Abstract: Quadruped robots are currently a widespread platform for robotics research,
thanks to powerful Reinforcement Learning controllers and the availability of
cheap and robust commercial platforms. However, to broaden the adoption of the
technology in the real world, we require robust navigation stacks relying only
on low-cost sensors such as depth cameras. This paper presents a first step
towards a robust localization, mapping, and navigation system for low-cost
quadruped robots. In pursuit of this objective we combine contact-aided
kinematic, visual-inertial odometry, and depth-stabilized vision, enhancing
stability and accuracy of the system. Our results in simulation and two
different real-world quadruped platforms show that our system can generate an
accurate 2D map of the environment, robustly localize itself, and navigate
autonomously. Furthermore, we present in-depth ablation studies of the
important components of the system and their impact on localization accuracy.
Videos, code, and additional experiments can be found on the project website:
https://sites.google.com/view/low-cost-quadruped-slam

</details>

### [398] [Estimating Commonsense Scene Composition on Belief Scene Graphs](https://arxiv.org/abs/2505.02405)
*Mario A. V. Saucedo,Vignesh Kottayam Viswanathan,Christoforos Kanellakis,George Nikolakopoulos*

Main category: cs.RO

TLDR: 提出了一种基于常识的场景组合方法，通过估计未见物体的空间分布扩展信念场景图，并验证了其在不同房间类型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究如何理解场景中相关物体的空间关系，以支持更智能的场景解释。

Method: 提出了两种基于相关性信息（CECI）的概率分布学习模型：基于图卷积网络的基线方法和结合大型语言模型（LLMs）的神经符号扩展方法。

Result: 在模拟数据和真实室内环境中验证了框架的有效性，能够跨不同房间类型解释场景。

Conclusion: 该框架为场景理解提供了新的视角，特别是在空间关系建模方面具有潜力。

Abstract: This work establishes the concept of commonsense scene composition, with a
focus on extending Belief Scene Graphs by estimating the spatial distribution
of unseen objects. Specifically, the commonsense scene composition capability
refers to the understanding of the spatial relationships among related objects
in the scene, which in this article is modeled as a joint probability
distribution for all possible locations of the semantic object class. The
proposed framework includes two variants of a Correlation Information (CECI)
model for learning probability distributions: (i) a baseline approach based on
a Graph Convolutional Network, and (ii) a neuro-symbolic extension that
integrates a spatial ontology based on Large Language Models (LLMs).
Furthermore, this article provides a detailed description of the dataset
generation process for such tasks. Finally, the framework has been validated
through multiple runs on simulated data, as well as in a real-world indoor
environment, demonstrating its ability to spatially interpret scenes across
different room types.

</details>

### [399] [Point Cloud Recombination: Systematic Real Data Augmentation Using Robotic Targets for LiDAR Perception Validation](https://arxiv.org/abs/2505.02476)
*Hubert Padusinski,Christian Steinhauser,Christian Scherl,Julian Gaal,Jacob Langner*

Main category: cs.RO

TLDR: 提出了一种名为点云重组的方法，通过在受控实验室环境中测量物理目标物体，系统性地增强真实点云场景，以生成可重复且物理准确的测试场景。


<details>
  <summary>Details</summary>
Motivation: 由于真实环境条件的多变性和现有方法的局限性，验证基于LiDAR的感知系统在开放世界应用中的性能仍具挑战性。

Method: 提出点云重组方法，将实验室环境中测量的物理目标物体点云整合到真实场景中，生成可重复且物理准确的测试场景。

Result: 实验表明，重组后的场景与真实传感器输出高度匹配，支持针对性测试、可扩展的故障分析和系统安全性提升。

Conclusion: 该方法提供了受控且传感器真实的数据，能够可靠地评估特定传感器及其算法的局限性。

Abstract: The validation of LiDAR-based perception of intelligent mobile systems
operating in open-world applications remains a challenge due to the variability
of real environmental conditions. Virtual simulations allow the generation of
arbitrary scenes under controlled conditions but lack physical sensor
characteristics, such as intensity responses or material-dependent effects. In
contrast, real-world data offers true sensor realism but provides less control
over influencing factors, hindering sufficient validation. Existing approaches
address this problem with augmentation of real-world point cloud data by
transferring objects between scenes. However, these methods do not consider
validation and remain limited in controllability because they rely on empirical
data. We solve these limitations by proposing Point Cloud Recombination, which
systematically augments captured point cloud scenes by integrating point clouds
acquired from physical target objects measured in controlled laboratory
environments. Thus enabling the creation of vast amounts and varieties of
repeatable, physically accurate test scenes with respect to phenomena-aware
occlusions with registered 3D meshes. Using the Ouster OS1-128 Rev7 sensor, we
demonstrate the augmentation of real-world urban and rural scenes with humanoid
targets featuring varied clothing and poses, for repeatable positioning. We
show that the recombined scenes closely match real sensor outputs, enabling
targeted testing, scalable failure analysis, and improved system safety. By
providing controlled yet sensor-realistic data, our method enables trustworthy
conclusions about the limitations of specific sensors in compound with their
algorithms, e.g., object detection.

</details>

### [400] [Grasp the Graph (GtG) 2.0: Ensemble of GNNs for High-Precision Grasp Pose Detection in Clutter](https://arxiv.org/abs/2505.02664)
*Ali Rashidi Moghadam,Sayedmohammadreza Rastegari,Mehdi Tale Masouleh,Ahmad Kalhor*

Main category: cs.RO

TLDR: GtG 2.0是一种基于图神经网络的轻量级机器人抓取框架，通过改进几何推理和抓取候选生成，显著提升了在杂乱环境中的抓取性能。


<details>
  <summary>Details</summary>
Motivation: 解决因噪声、不完整感知数据和复杂物体几何形状导致的抓取姿态检测难题。

Method: 使用图神经网络集合对点云数据进行几何推理，结合7自由度抓取候选生成器，改进抓取检测。

Result: 在GraspNet-1Billion基准测试中平均精度提升35%，实验成功率达91%。

Conclusion: GtG 2.0在复杂环境中表现出高效性和可靠性，是当前最先进的抓取框架之一。

Abstract: Grasp pose detection in cluttered, real-world environments remains a
significant challenge due to noisy and incomplete sensory data combined with
complex object geometries. This paper introduces Grasp the Graph 2.0 (GtG 2.0)
method, a lightweight yet highly effective hypothesis-and-test robotics
grasping framework which leverages an ensemble of Graph Neural Networks for
efficient geometric reasoning from point cloud data. Building on the success of
GtG 1.0, which demonstrated the potential of Graph Neural Networks for grasp
detection but was limited by assumptions of complete, noise-free point clouds
and 4-Dof grasping, GtG 2.0 employs a conventional Grasp Pose Generator to
efficiently produce 7-Dof grasp candidates. Candidates are assessed with an
ensemble Graph Neural Network model which includes points within the gripper
jaws (inside points) and surrounding contextual points (outside points). This
improved representation boosts grasp detection performance over previous
methods using the same generator. GtG 2.0 shows up to a 35% improvement in
Average Precision on the GraspNet-1Billion benchmark compared to
hypothesis-and-test and Graph Neural Network-based methods, ranking it among
the top three frameworks. Experiments with a 3-Dof Delta Parallel robot and
Kinect-v1 camera show a success rate of 91% and a clutter completion rate of
100%, demonstrating its flexibility and reliability.

</details>

### [401] [TWIST: Teleoperated Whole-Body Imitation System](https://arxiv.org/abs/2505.02833)
*Yanjie Ze,Zixuan Chen,João Pedro Araújo,Zi-ang Cao,Xue Bin Peng,Jiajun Wu,C. Karen Liu*

Main category: cs.RO

TLDR: TWIST是一个通过全身运动模仿实现人形机器人远程操作的系统，结合强化学习和行为克隆技术，显著提升了机器人的协调性和多功能性。


<details>
  <summary>Details</summary>
Motivation: 当前的人形机器人远程操作系统通常局限于局部任务（如行走或操作），缺乏全身协调能力。TWIST旨在通过模仿人类运动实现更全面的机器人控制。

Method: 系统首先生成参考运动片段（通过将人类动作捕捉数据重定向到机器人），然后结合强化学习和行为克隆（RL+BC）开发了一个鲁棒、自适应且响应迅速的全身控制器。

Result: 通过引入未来运动帧和真实动作捕捉数据，TWIST显著提高了跟踪精度，使机器人能够实现前所未有的多功能全身运动技能。

Conclusion: TWIST通过统一的神经网络控制器，为人形机器人提供了协调、多功能的全身运动能力，推动了通用机器人智能的发展。

Abstract: Teleoperating humanoid robots in a whole-body manner marks a fundamental step
toward developing general-purpose robotic intelligence, with human motion
providing an ideal interface for controlling all degrees of freedom. Yet, most
current humanoid teleoperation systems fall short of enabling coordinated
whole-body behavior, typically limiting themselves to isolated locomotion or
manipulation tasks. We present the Teleoperated Whole-Body Imitation System
(TWIST), a system for humanoid teleoperation through whole-body motion
imitation. We first generate reference motion clips by retargeting human motion
capture data to the humanoid robot. We then develop a robust, adaptive, and
responsive whole-body controller using a combination of reinforcement learning
and behavior cloning (RL+BC). Through systematic analysis, we demonstrate how
incorporating privileged future motion frames and real-world motion capture
(MoCap) data improves tracking accuracy. TWIST enables real-world humanoid
robots to achieve unprecedented, versatile, and coordinated whole-body motor
skills--spanning whole-body manipulation, legged manipulation, locomotion, and
expressive movement--using a single unified neural network controller. Our
project website: https://humanoid-teleop.github.io

</details>

### [402] [Automated Hybrid Reward Scheduling via Large Language Models for Robotic Skill Learning](https://arxiv.org/abs/2505.02483)
*Changxin Huang,Junyang Liang,Yanbin Chang,Jingzhao Xu,Jianqiang Li*

Main category: cs.RO

TLDR: 提出了一种基于大语言模型（LLM）的自动混合奖励调度（AHRS）框架，通过动态调整奖励组件的学习强度，提升高自由度机器人技能学习的性能。


<details>
  <summary>Details</summary>
Motivation: 高自由度机器人技能学习因动态复杂性而困难，现有强化学习方法对所有奖励组件无差别求和，效率低且限制性能。

Method: 设计多分支值网络，每个分支对应一个奖励组件，利用LLM生成的规则动态调整各分支权重。

Result: 实验表明，AHRS方法在多个任务中平均性能提升6.48%。

Conclusion: AHRS框架通过动态奖励调度，显著提升了机器人技能学习的效率和性能。

Abstract: Enabling a high-degree-of-freedom robot to learn specific skills is a
challenging task due to the complexity of robotic dynamics. Reinforcement
learning (RL) has emerged as a promising solution; however, addressing such
problems requires the design of multiple reward functions to account for
various constraints in robotic motion. Existing approaches typically sum all
reward components indiscriminately to optimize the RL value function and
policy. We argue that this uniform inclusion of all reward components in policy
optimization is inefficient and limits the robot's learning performance. To
address this, we propose an Automated Hybrid Reward Scheduling (AHRS) framework
based on Large Language Models (LLMs). This paradigm dynamically adjusts the
learning intensity of each reward component throughout the policy optimization
process, enabling robots to acquire skills in a gradual and structured manner.
Specifically, we design a multi-branch value network, where each branch
corresponds to a distinct reward component. During policy optimization, each
branch is assigned a weight that reflects its importance, and these weights are
automatically computed based on rules designed by LLMs. The LLM generates a
rule set in advance, derived from the task description, and during training, it
selects a weight calculation rule from the library based on language prompts
that evaluate the performance of each branch. Experimental results demonstrate
that the AHRS method achieves an average 6.48% performance improvement across
multiple high-degree-of-freedom robotic tasks.

</details>

### [403] [Learning and Online Replication of Grasp Forces from Electromyography Signals for Prosthetic Finger Control](https://arxiv.org/abs/2505.02574)
*Robin Arbaud,Elisa Motta,Marco Domenico Avaro,Stefano Picinich,Marta Lorenzini,Arash Ajoudani*

Main category: cs.RO

TLDR: 开发了一种基于肌电信号（EMG）的力控假手指，通过神经网络模型实现握力在线调节，初步验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 部分手部截肢严重影响个体的身心健康，而假肢的直观控制仍是一个未解决的挑战。

Method: 开发了一种基于EMG信号的力控假手指原型，采用神经网络模型从EMG输入估计指尖力，并在未受损受试者中进行早期评估。

Result: 实验验证了力估计模型的有效性，四名用户的在线试验展示了精确的设备控制能力。

Conclusion: EMG力估计有望提升假手指的功能性。

Abstract: Partial hand amputations significantly affect the physical and psychosocial
well-being of individuals, yet intuitive control of externally powered
prostheses remains an open challenge. To address this gap, we developed a
force-controlled prosthetic finger activated by electromyography (EMG) signals.
The prototype, constructed around a wrist brace, functions as a supernumerary
finger placed near the index, allowing for early-stage evaluation on unimpaired
subjects. A neural network-based model was then implemented to estimate
fingertip forces from EMG inputs, allowing for online adjustment of the
prosthetic finger grip strength. The force estimation model was validated
through experiments with ten participants, demonstrating its effectiveness in
predicting forces. Additionally, online trials with four users wearing the
prosthesis exhibited precise control over the device. Our findings highlight
the potential of using EMG-based force estimation to enhance the functionality
of prosthetic fingers.

</details>

<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [404] [Learning simple heuristic rules for classifying materials based on chemical composition](https://arxiv.org/abs/2505.02361)
*Andrew Ma,Marin Soljačić*

Main category: cond-mat.mtrl-sci

TLDR: 论文提出了一种基于化学组成的简单启发式规则，用于分类材料的拓扑性质和金属性质，并通过引入化学信息归纳偏置减少所需训练数据量。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在材料科学中依赖复杂非线性模型，而本文探索简单启发式规则的潜力，并研究化学信息归纳偏置对性能的影响。

Method: 提出基于化学组成的简单启发式规则，结合周期表结构的化学信息归纳偏置，对拓扑性质和金属性质进行分类。

Result: 实验表明，引入化学信息归纳偏置可以减少达到特定测试精度所需的训练数据量。

Conclusion: 简单启发式规则结合化学信息归纳偏置在材料分类任务中具有潜力，尤其适用于数据有限的情况。

Abstract: In the past decade, there has been a significant interest in the use of
machine learning approaches in materials science research. Conventional deep
learning approaches that rely on complex, nonlinear models have become
increasingly important in computational materials science due to their high
predictive accuracy. In contrast to these approaches, we have shown in a recent
work that a remarkably simple learned heuristic rule -- based on the concept of
topogivity -- can classify whether a material is topological using only its
chemical composition. In this paper, we go beyond the topology classification
scenario by also studying the use of machine learning to develop simple
heuristic rules for classifying whether a material is a metal based on chemical
composition. Moreover, we present a framework for incorporating
chemistry-informed inductive bias based on the structure of the periodic table.
For both the topology classification and the metallicity classification tasks,
we empirically characterize the performance of simple heuristic rules fit with
and without chemistry-informed inductive bias across a wide range of training
set sizes. We find evidence that incorporating chemistry-informed inductive
bias can reduce the amount of training data required to reach a given level of
test accuracy.

</details>

<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [405] [Antifragility of RIS-assisted Communication Systems under Jamming Attacks](https://arxiv.org/abs/2505.02565)
*Mounir Bensalem,Thomas Röthig,Admela Jukan*

Main category: cs.NI

TLDR: 论文研究了RIS辅助通信系统在干扰攻击下的抗脆弱性，表明在特定条件下干扰可转化为吞吐量增益。


<details>
  <summary>Details</summary>
Motivation: 探索RIS系统在干扰环境中的抗脆弱性，将干扰从威胁转化为性能提升的机会。

Method: 分析双跳系统（源节点、RIS、目标节点及干扰节点）的抗脆弱性，提出并评估多种干扰模型（如DRFM、相位和幅度偏移）。

Result: 在特定功率阈值和干扰模型下，系统可实现抗脆弱性吞吐量增益，高干扰功率与低基线数据率结合时增益可达五倍。

Conclusion: RIS结合抗脆弱设计可将干扰转化为吞吐量增益，验证了抗脆弱性在通信系统中的潜力。

Abstract: Antifragility of communication systems is defined as measure of benefits
gained from the adverse events and variability of its environment. In this
paper, we introduce the notion of antifragility in Reconfigurable Intelligent
Surface (RIS) assisted communication systems affected by a jamming attack. We
analyzed the antifragility of the two hop systems, where the wireless path
contains source node, RIS, destination node, and a eavesdropping/jamming node.
We propose and analyze the antifragility performance for several jamming
models, such as Digital Radio Frequency Memory (DRFM) and phase and amplitude
shifting. Our paper shows that antifragility throughput can indeed be achieved
under certain power thresholds and for various jamming models. In particular,
high jamming power combined with low baseline data rates yields an antifragile
gain factor of approximately five times. The results confirm that
reconfigurable intelligent surfaces, when coupled with an antifragile design
philosophy, can convert hostile interference from a liability into a throughput
gain.

</details>

<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [406] [Seasonal Prediction with Neural GCM and Simplified Boundary Forcings: Large-scale Atmospheric Variability and Tropical Cyclone Activity](https://arxiv.org/abs/2505.01455)
*Gan Zhang,Megha Rao,Janni Yuval,Ming Zhao*

Main category: physics.ao-ph

TLDR: 论文探讨了机器学习模型在气候预测中的可行性，通过混合ML-物理模型NeuralGCM，成功预测了季节性大气变化和热带气旋活动。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索机器学习模型在气候预测中的应用潜力，尤其是在季节性预测和热带气旋活动方面。

Method: 采用NeuralGCM混合模型，简化边界条件（如海表温度和海冰），模拟大气环流和热带气旋气候模式。

Result: 模型在热带大气和热带气旋活动指标上表现出有效的季节性预测能力，预测技能与现有物理模型相当。

Conclusion: 研究表明，结合物理洞察的机器学习模型在气候预测和热带气旋风险评估中具有潜力。

Abstract: Machine learning (ML) models are successful with weather forecasting and have
shown progress in climate simulations, yet leveraging them for useful climate
predictions needs exploration. Here we show this feasibility using NeuralGCM, a
hybrid ML-physics atmospheric model, for seasonal predictions of large-scale
atmospheric variability and Northern Hemisphere tropical cyclone (TC) activity.
Inspired by physical model studies, we simplify boundary conditions, assuming
sea surface temperature (SST) and sea ice follow their climatological cycle but
persist anomalies present at initialization. With such forcings, NeuralGCM
simulates realistic atmospheric circulation and TC climatology patterns.
Furthermore, this configuration yields useful seasonal predictions
(July-November) for the tropical atmosphere and various TC activity metrics.
Notably, the prediction skill for TC frequency in the North Atlantic and East
Pacific basins is comparable to existing physical models. These findings
highlight the promise of leveraging ML models with physical insights to model
TC risks and deliver seamless weather-climate predictions.

</details>

<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [407] [HoneyBee: Efficient Role-based Access Control for Vector Databases via Dynamic Partitioning](https://arxiv.org/abs/2505.01538)
*Hongbin Zhong,Matthew Lentz,Nina Narodytska,Adriana Szekeres,Kexin Rong*

Main category: cs.DB

TLDR: HoneyBee框架通过动态分区和RBAC策略优化向量数据库的访问控制，平衡存储冗余与查询效率。


<details>
  <summary>Details</summary>
Motivation: 现有向量数据库访问控制方法在存储冗余和查询效率之间存在显著权衡，需要一种更优解决方案。

Method: 提出HoneyBee框架，利用RBAC策略动态分区，优化向量复制与查询性能。

Result: HoneyBee在RBAC工作负载中减少存储冗余，查询速度提升6倍，存储仅增加1.4倍。

Conclusion: HoneyBee为安全和高效的向量搜索提供了实用中间方案。

Abstract: As vector databases gain traction in enterprise applications, robust access
control has become critical to safeguard sensitive data. Access control in
these systems is often implemented through hybrid vector queries, which combine
nearest neighbor search on vector data with relational predicates based on user
permissions. However, existing approaches face significant trade-offs: creating
dedicated indexes for each user minimizes query latency but introduces
excessive storage redundancy, while building a single index and applying access
control after vector search reduces storage overhead but suffers from poor
recall and increased query latency. This paper introduces HoneyBee, a dynamic
partitioning framework that bridges the gap between these approaches by
leveraging the structure of Role-Based Access Control (RBAC) policies. RBAC,
widely adopted in enterprise settings, groups users into roles and assigns
permissions to those roles, creating a natural "thin waist" in the permission
structure that is ideal for partitioning decisions. Specifically, HoneyBee
produces overlapping partitions where vectors can be strategically replicated
across different partitions to reduce query latency while controlling storage
overhead. By introducing analytical models for the performance and recall of
the vector search, HoneyBee formulates the partitioning strategy as a
constrained optimization problem to dynamically balance storage, query
efficiency, and recall. Evaluations on RBAC workloads demonstrate that HoneyBee
reduces storage redundancy compared to role partitioning and achieves up to 6x
faster query speeds than row-level security (RLS) with only 1.4x storage
increase, offering a practical middle ground for secure and efficient vector
search.

</details>

### [408] [Building Scalable AI-Powered Applications with Cloud Databases: Architectures, Best Practices and Performance Considerations](https://arxiv.org/abs/2504.18793)
*Santosh Bhupathi*

Main category: cs.DB

TLDR: 本文探讨了云原生数据库如何通过专用技术（如向量数据库、图数据库等）支持AI驱动的应用，提出了集成AI工作负载的架构模式，并通过性能基准和案例研究指导设计高效、可扩展的AI应用。


<details>
  <summary>Details</summary>
Motivation: 传统架构难以满足AI驱动工作负载的需求，如实时数据访问和低延迟查询，因此需要高性能、可扩展的云数据库解决方案。

Method: 利用向量数据库（pgvector）、图数据库（AWS Neptune）等技术，提出集成AI工作负载的架构模式，包括RAG、实时数据管道等。

Result: 通过性能基准和案例研究验证了云数据库在提升AI能力、确保安全与合规方面的有效性。

Conclusion: 本文为研究人员和企业提供了构建高性能、可扩展且成本高效的AI应用的实用指南。

Abstract: The rapid adoption of AI-powered applications demands high-performance,
scalable, and efficient cloud database solutions, as traditional architectures
often struggle with AI-driven workloads requiring real-time data access, vector
search, and low-latency queries. This paper explores how cloud-native databases
enable AI-driven applications by leveraging purpose-built technologies such as
vector databases (pgvector), graph databases (AWS Neptune), NoSQL stores
(Amazon DocumentDB, DynamoDB), and relational cloud databases (Aurora MySQL and
PostgreSQL). It presents architectural patterns for integrating AI workloads
with cloud databases, including Retrieval-Augmented Generation (RAG) [1] with
LLMs, real-time data pipelines, AI-driven query optimization, and
embeddings-based search. Performance benchmarks, scalability considerations,
and cost-efficient strategies are evaluated to guide the design of AI-enabled
applications. Real-world case studies from industries such as healthcare,
finance, and customer experience illustrate how enterprises utilize cloud
databases to enhance AI capabilities while ensuring security, governance, and
compliance with enterprise and regulatory standards. By providing a
comprehensive analysis of AI and cloud database integration, this paper serves
as a practical guide for researchers, architects, and enterprises to build
next-generation AI applications that optimize performance, scalability, and
cost efficiency in cloud environments.

</details>

### [409] [Subspace Aggregation Query and Index Generation for Multidimensional Resource Space Mode](https://arxiv.org/abs/2505.02129)
*Xiaoping Sun,Hai Zhuge*

Main category: cs.DB

TLDR: 提出了一种基于多维分类空间的资源组织方法，通过构建图索引优化子空间聚合查询效率。


<details>
  <summary>Details</summary>
Motivation: 高效管理和查询大规模资源的需求，尤其是在多维分类空间中。

Method: 定义子空间聚合查询，构建包含部分序关系的图索引，采用四种策略降低索引生成成本。

Result: 分析和实验验证了生成索引在支持子空间聚合查询方面的有效性。

Conclusion: 该工作对基于多维分类的数据模型发展有重要贡献。

Abstract: Organizing resources in a multidimensional classification space is an
approach to efficiently managing and querying large-scale resources. This paper
defines an aggregation query on subspace defined by a range on the partial
order on coordinate tree at each dimension, where each point contains resources
aggregated along the paths of partial order relations on the points so that
aggregated resources at each point within the subspace can be measured, ranked
and selected. To efficiently locate non-empty points in a large subspace, an
approach to generating graph index is proposed to build inclusion links with
partial order relations on coordinates of dimensions to enable a subspace query
to reach non-empty points by following indexing links and aggregate resources
along indexing paths back to their super points. Generating such an index is
costly as the number of children of an index node can be very large so that the
total number of indexing nodes is unbounded. The proposed approach adopts the
following strategies to reduce the cost: (1) adding intersection links between
two indexing nodes, which can better reduce query processing costs while
controlling the number of nodes of the graph index; (2) intersection links are
added between two nodes according to the probabilistic distribution calculated
for estimating the costs of adding intersection between two nodes; (3)
coordinates at one dimension having more resources are split by coordinates at
another dimension to balance the number of resources hold by indexing nodes;
and, (4) short-cut links are added between sibling coordinates of coordinate
trees to make an efficient query on linear order coordinates. Analysis and
experiments verified the effectiveness of the generated index in supporting
subspace aggregation query. This work makes significant contributions to the
development of data model based on multi-dimensional classification.

</details>

<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [410] [Rate-Limited Closed-Loop Distributed ISAC Systems: An Autoencoder Approach](https://arxiv.org/abs/2505.01780)
*Guangjin Pan,Zhixing Li,Ayça Özçelikkale,Christian Häger,Musa Furkan Keskin,Henk Wymeersch*

Main category: eess.SP

TLDR: 论文提出了一种基于自动编码器的观测压缩方法，用于解决分布式多传感器ISAC系统中高维观测数据在有限传输容量下的传输问题，并通过案例研究分析了压缩对系统性能的影响。


<details>
  <summary>Details</summary>
Motivation: 在闭环分布式多传感器ISAC系统中，高维观测数据在有限传输容量下的传输限制了系统性能，需要一种有效的压缩方法来解决这一问题。

Method: 提出了一种基于自动编码器的观测压缩方法，并通过闭环线性二次调节器（LQR）系统进行案例研究，分析压缩对重建精度、状态估计误差和控制性能的影响。

Result: 在多传感器场景中，资源分配策略优先分配给低噪声传感器，直到压缩无损后再分配给高噪声传感器。

Conclusion: 基于自动编码器的压缩方法能有效优化资源分配，提升系统性能。

Abstract: In closed-loop distributed multi-sensor integrated sensing and communication
(ISAC) systems, performance often hinges on transmitting high-dimensional
sensor observations over rate-limited networks. In this paper, we first present
a general framework for rate-limited closed-loop distributed ISAC systems, and
then propose an autoencoder-based observation compression method to overcome
the constraints imposed by limited transmission capacity. Building on this
framework, we conduct a case study using a closed-loop linear quadratic
regulator (LQR) system to analyze how the interplay among observation,
compression, and state dimensions affects reconstruction accuracy, state
estimation error, and control performance. In multi-sensor scenarios, our
results further show that optimal resource allocation initially prioritizes
low-noise sensors until the compression becomes lossless, after which resources
are reallocated to high-noise sensors.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [411] [Regression s all you need for medical image translation](https://arxiv.org/abs/2505.02048)
*Sebastian Rassmann,David Kügler,Christian Ewert,Martin Reuter*

Main category: eess.IV

TLDR: YODA是一种基于扩散模型的2.5D框架，用于医学图像翻译，结合扩散和回归方法生成高质量图像。


<details>
  <summary>Details</summary>
Motivation: 医学图像翻译需要高精度解剖信息，现有GAN和扩散模型在医学应用中可能因噪声或内容幻觉而受限。

Method: 提出YODA框架，结合扩散和回归范式，并引入ExpA采样方法抑制噪声。

Result: 实验表明YODA优于现有GAN和扩散模型，生成图像可替代或优于实际采集图像。

Conclusion: YODA挑战了扩散模型在医学图像翻译中的优势，为实际应用铺平道路。

Abstract: The acquisition of information-rich images within a limited time budget is
crucial in medical imaging. Medical image translation (MIT) can help enhance
and supplement existing datasets by generating synthetic images from acquired
data. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have
achieved remarkable success in natural image generation, their benefits -
creativity and image realism - do not necessarily transfer to medical
applications where highly accurate anatomical information is required. In fact,
the imitation of acquisition noise or content hallucination hinder clinical
utility. Here, we introduce YODA (You Only Denoise once - or Average), a novel
2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and
regression paradigms to produce realistic or noise-free outputs. Furthermore,
we propose Expectation-Approximation (ExpA) DM sampling, which draws
inspiration from MRI signal averaging. ExpA-sampling suppresses generated noise
and, thus, eliminates noise from biasing the evaluation of image quality.
Through extensive experiments on four diverse multi-modal datasets - comprising
multi-contrast brain MRI and pelvic MRI-CT - we show that diffusion and
regression sampling yield similar results in practice. As such, the
computational overhead of diffusion sampling does not provide systematic
benefits in medical information translation. Building on these insights, we
demonstrate that YODA outperforms several state-of-the-art GAN and DM methods.
Notably, YODA-generated images are shown to be interchangeable with, or even
superior to, physical acquisitions for several downstream tasks. Our findings
challenge the presumed advantages of DMs in MIT and pave the way for the
practical application of MIT in medical imaging.

</details>

### [412] [CostFilter-AD: Enhancing Anomaly Detection through Matching Cost Filtering](https://arxiv.org/abs/2505.01476)
*Zhe Zhang,Mingxiu Cai,Hanxiao Wang,Gaochang Wu,Tianyou Chai,Xiatian Zhu*

Main category: eess.IV

TLDR: 论文提出了一种名为CostFilter-AD的无监督异常检测方法，通过引入成本过滤概念改进现有方法的匹配过程，提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测方法依赖图像或特征级匹配，但匹配过程不准确且被忽视，导致检测效果不佳。

Method: 构建匹配成本体积，提出成本体积过滤网络，通过输入观察作为注意力查询抑制噪声并保留边缘结构。

Result: 在MVTec-AD和VisA基准测试中验证了CostFilter-AD对单类和多类任务的通用性优势。

Conclusion: CostFilter-AD作为一种通用后处理插件，可显著提升无监督异常检测的性能。

Abstract: Unsupervised anomaly detection (UAD) seeks to localize the anomaly mask of an
input image with respect to normal samples. Either by reconstructing normal
counterparts (reconstruction-based) or by learning an image feature embedding
space (embedding-based), existing approaches fundamentally rely on image-level
or feature-level matching to derive anomaly scores. Often, such a matching
process is inaccurate yet overlooked, leading to sub-optimal detection. To
address this issue, we introduce the concept of cost filtering, borrowed from
classical matching tasks, such as depth and flow estimation, into the UAD
problem. We call this approach {\em CostFilter-AD}. Specifically, we first
construct a matching cost volume between the input and normal samples,
comprising two spatial dimensions and one matching dimension that encodes
potential matches. To refine this, we propose a cost volume filtering network,
guided by the input observation as an attention query across multiple feature
layers, which effectively suppresses matching noise while preserving edge
structures and capturing subtle anomalies. Designed as a generic
post-processing plug-in, CostFilter-AD can be integrated with either
reconstruction-based or embedding-based methods. Extensive experiments on
MVTec-AD and VisA benchmarks validate the generic benefits of CostFilter-AD for
both single- and multi-class UAD tasks. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>

### [413] [Seeing Heat with Color -- RGB-Only Wildfire Temperature Inference from SAM-Guided Multimodal Distillation using Radiometric Ground Truth](https://arxiv.org/abs/2505.01638)
*Michael Marinaccio,Fatemeh Afghah*

Main category: eess.IV

TLDR: SAM-TIFF是一种基于RGB输入的师生蒸馏框架，用于火灾温度预测和分割，无需热传感器。


<details>
  <summary>Details</summary>
Motivation: 减少无人机火灾监测中多模态传感（RGB和热成像）的硬件成本和功耗。

Method: 使用多模态教师网络（RGB-热成像配对数据）向单模态RGB学生网络蒸馏知识，结合SAM、TOPSIS、Canny边缘检测和Otsu阈值自动生成分割监督。

Result: 首次实现从RGB无人机数据中逐像素温度回归，在FLAME 3数据集上表现优异。

Conclusion: 为无需热传感器的轻量、低成本无人机火灾监测系统奠定基础。

Abstract: High-fidelity wildfire monitoring using Unmanned Aerial Vehicles (UAVs)
typically requires multimodal sensing - especially RGB and thermal imagery -
which increases hardware cost and power consumption. This paper introduces
SAM-TIFF, a novel teacher-student distillation framework for pixel-level
wildfire temperature prediction and segmentation using RGB input only. A
multimodal teacher network trained on paired RGB-Thermal imagery and
radiometric TIFF ground truth distills knowledge to a unimodal RGB student
network, enabling thermal-sensor-free inference. Segmentation supervision is
generated using a hybrid approach of segment anything (SAM)-guided mask
generation, and selection via TOPSIS, along with Canny edge detection and
Otsu's thresholding pipeline for automatic point prompt selection. Our method
is the first to perform per-pixel temperature regression from RGB UAV data,
demonstrating strong generalization on the recent FLAME 3 dataset. This work
lays the foundation for lightweight, cost-effective UAV-based wildfire
monitoring systems without thermal sensors.

</details>

### [414] [RobSurv: Vector Quantization-Based Multi-Modal Learning for Robust Cancer Survival Prediction](https://arxiv.org/abs/2505.02529)
*Aiman Farooq,Azad Singh,Deepak Mishra,Santanu Chaudhury*

Main category: eess.IV

TLDR: RobSurv是一种鲁棒的多模态深度学习框架，通过向量量化和双路径架构解决医学影像中噪声和协议变化的问题，显著提升了癌症生存预测的性能。


<details>
  <summary>Details</summary>
Motivation: 由于深度学习模型对噪声和不同影像中心协议变化的敏感性，多模态医学影像在癌症生存预测中的应用面临挑战。现有方法难以从异质性CT和PET图像中提取一致特征。

Method: RobSurv采用双路径架构：一条路径通过向量量化将连续特征映射到离散码本以实现抗噪声表示，另一条路径保留细粒度细节。通过基于Transformer的补丁融合机制整合双表示。

Result: 在三个数据集（HECKTOR、H&N1和NSCLC Radiogenomics）上，RobSurv的C指数分别为0.771、0.742和0.734，显著优于现有方法，且在严重噪声条件下性能仅下降3.8-4.5%。

Conclusion: RobSurv在多模态医学影像中表现出卓越的鲁棒性和泛化能力，为临床预后提供了可靠解决方案，有望改善治疗计划和患者护理。

Abstract: Cancer survival prediction using multi-modal medical imaging presents a
critical challenge in oncology, mainly due to the vulnerability of deep
learning models to noise and protocol variations across imaging centers.
Current approaches struggle to extract consistent features from heterogeneous
CT and PET images, limiting their clinical applicability. We address these
challenges by introducing RobSurv, a robust deep-learning framework that
leverages vector quantization for resilient multi-modal feature learning. The
key innovation of our approach lies in its dual-path architecture: one path
maps continuous imaging features to learned discrete codebooks for
noise-resistant representation, while the parallel path preserves fine-grained
details through continuous feature processing. This dual representation is
integrated through a novel patch-wise fusion mechanism that maintains local
spatial relationships while capturing global context via Transformer-based
processing. In extensive evaluations across three diverse datasets (HECKTOR,
H\&N1, and NSCLC Radiogenomics), RobSurv demonstrates superior performance,
achieving concordance index of 0.771, 0.742, and 0.734 respectively -
significantly outperforming existing methods. Most notably, our model maintains
robust performance even under severe noise conditions, with performance
degradation of only 3.8-4.5\% compared to 8-12\% in baseline methods. These
results, combined with strong generalization across different cancer types and
imaging protocols, establish RobSurv as a promising solution for reliable
clinical prognosis that can enhance treatment planning and patient care.

</details>

### [415] [Multimodal Deep Learning for Stroke Prediction and Detection using Retinal Imaging and Clinical Data](https://arxiv.org/abs/2505.02677)
*Saeed Shurrab,Aadim Nepal,Terrence J. Lee-St. John,Nicola G. Ghazi,Bartlomiej Piechowski-Jozwiak,Farah E. Shamout*

Main category: eess.IV

TLDR: 该研究探讨了利用视网膜图像和临床数据通过多模态深度神经网络提升中风检测和风险预测的效果，相比基线方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 中风是全球重大公共卫生问题，现有诊断方法依赖昂贵医学影像。视网膜成像因其与脑血管健康的共享临床路径，可能成为经济高效的替代方案。

Method: 提出一种多模态深度神经网络，结合光学相干断层扫描（OCT）、红外反射视网膜扫描和临床数据，通过自监督学习预训练后微调评估。

Result: 实验结果表明，该方法在检测中风相关视网膜变化和预测未来风险方面表现优异，AUROC指标比单模态基线提升5%，比现有先进模型提升8%。

Conclusion: 研究证明了视网膜成像在识别高风险患者和改善长期预后方面的潜力。

Abstract: Stroke is a major public health problem, affecting millions worldwide. Deep
learning has recently demonstrated promise for enhancing the diagnosis and risk
prediction of stroke. However, existing methods rely on costly medical imaging
modalities, such as computed tomography. Recent studies suggest that retinal
imaging could offer a cost-effective alternative for cerebrovascular health
assessment due to the shared clinical pathways between the retina and the
brain. Hence, this study explores the impact of leveraging retinal images and
clinical data for stroke detection and risk prediction. We propose a multimodal
deep neural network that processes Optical Coherence Tomography (OCT) and
infrared reflectance retinal scans, combined with clinical data, such as
demographics, vital signs, and diagnosis codes. We pretrained our model using a
self-supervised learning framework using a real-world dataset consisting of
$37$ k scans, and then fine-tuned and evaluated the model using a smaller
labeled subset. Our empirical findings establish the predictive ability of the
considered modalities in detecting lasting effects in the retina associated
with acute stroke and forecasting future risk within a specific time horizon.
The experimental results demonstrate the effectiveness of our proposed
framework by achieving $5$\% AUROC improvement as compared to the unimodal
image-only baseline, and $8$\% improvement compared to an existing
state-of-the-art foundation model. In conclusion, our study highlights the
potential of retinal imaging in identifying high-risk patients and improving
long-term outcomes.

</details>

### [416] [Accelerating Volumetric Medical Image Annotation via Short-Long Memory SAM 2](https://arxiv.org/abs/2505.01854)
*Yuwen Chen,Zafer Yildiz,Qihang Li,Yaqian Chen,Haoyu Dong,Hanxue Gu,Nicholas Konz,Maciej A. Mazurowski*

Main category: eess.IV

TLDR: 论文提出SLM-SAM 2，通过结合短期和长期记忆模块改进SAM 2在医学图像分割中的性能，显著减少错误传播。


<details>
  <summary>Details</summary>
Motivation: 医学图像手动标注耗时耗力，现有SAM 2模型在跨切片传播标注时存在错误传播问题。

Method: 提出SLM-SAM 2，整合短期和长期记忆模块及独立注意力机制，提升分割准确性。

Result: 在三个公开数据集上测试，SLM-SAM 2平均Dice系数提升0.14（5卷初始适应）和0.11（1卷初始适应）。

Conclusion: SLM-SAM 2显著提升医学图像自动标注的准确性，减少过传播问题。

Abstract: Manual annotation of volumetric medical images, such as magnetic resonance
imaging (MRI) and computed tomography (CT), is a labor-intensive and
time-consuming process. Recent advancements in foundation models for video
object segmentation, such as Segment Anything Model 2 (SAM 2), offer a
potential opportunity to significantly speed up the annotation process by
manually annotating one or a few slices and then propagating target masks
across the entire volume. However, the performance of SAM 2 in this context
varies. Our experiments show that relying on a single memory bank and attention
module is prone to error propagation, particularly at boundary regions where
the target is present in the previous slice but absent in the current one. To
address this problem, we propose Short-Long Memory SAM 2 (SLM-SAM 2), a novel
architecture that integrates distinct short-term and long-term memory banks
with separate attention modules to improve segmentation accuracy. We evaluate
SLM-SAM 2 on three public datasets covering organs, bones, and muscles across
MRI and CT modalities. We show that the proposed method markedly outperforms
the default SAM 2, achieving average Dice Similarity Coefficient improvement of
0.14 and 0.11 in the scenarios when 5 volumes and 1 volume are available for
the initial adaptation, respectively. SLM-SAM 2 also exhibits stronger
resistance to over-propagation, making a notable step toward more accurate
automated annotation of medical images for segmentation model development.

</details>

### [417] [Adversarial Robustness of Deep Learning Models for Inland Water Body Segmentation from SAR Images](https://arxiv.org/abs/2505.01884)
*Siddharth Kothari,Srinivasan Murali,Sankalp Kothari,Ujjwal Verma,Jaya Sreevalsan-Nair*

Main category: eess.IV

TLDR: 论文研究了SAR图像中内陆水体分割任务，模拟了人工标注错误对U-Net模型的影响，发现模型对一定程度的标注噪声具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: SAR图像中水体分割因复杂几何形状和人工标注噪声而具有挑战性，研究旨在评估U-Net模型对标注错误的鲁棒性。

Method: 通过模拟人工标注错误（对抗攻击）测试U-Net模型的鲁棒性，并公开了代码、数据集和对抗样本。

Result: U-Net模型在标注噪声达到一定程度前表现稳定，性能下降不明显。

Conclusion: 人工标注质量对分割模型效果至关重要，模型对噪声具有一定容忍度。

Abstract: Inland water body segmentation from Synthetic Aperture Radar (SAR) images is
an important task needed for several applications, such as flood mapping. While
SAR sensors capture data in all-weather conditions as high-resolution images,
differentiating water and water-like surfaces from SAR images is not
straightforward. Inland water bodies, such as large river basins, have complex
geometry, which adds to the challenge of segmentation. U-Net is a widely used
deep learning model for land-water segmentation of SAR images. In practice,
manual annotation is often used to generate the corresponding water masks as
ground truth. Manual annotation of the images is prone to label noise owing to
data poisoning attacks, especially due to complex geometry. In this work, we
simulate manual errors in the form of adversarial attacks on the U-Net model
and study the robustness of the model to human errors in annotation. Our
results indicate that U-Net can tolerate a certain level of corruption before
its performance drops significantly. This finding highlights the crucial role
that the quality of manual annotations plays in determining the effectiveness
of the segmentation model. The code and the new dataset, along with adversarial
examples for robust training, are publicly available. (Github link -
https://github.com/GVCL/IWSeg-SAR-Poison.git)

</details>

### [418] [Platelet enumeration in dense aggregates](https://arxiv.org/abs/2505.02751)
*H. Martin Gillis,Yogeshwar Shendye,Paul Hollensen,Alan Fine,Thomas Trappenberg*

Main category: eess.IV

TLDR: 论文提出了一种改进的深度学习方法，通过优化卷积核和分类设计，显著提高了血小板识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 血小板的识别和计数对医疗至关重要，但现有CNN方法（如U-Net）因血小板尺寸和特征多变而表现不佳。

Method: 研究探索了卷积核的作用，将单血小板和血小板聚集体分为不同类别，并采用语义分割和多种U-Net架构进行识别。

Result: 实验结果显示，优化卷积操作和分类设计显著提升了血小板识别效果，提出的计数方法优于传统像素面积法。

Conclusion: 研究表明，优化卷积核和分类设计是解决血小板识别问题的有效途径，提出的方法在计数准确性上有显著改进。

Abstract: Identifying and counting blood components such as red blood cells, various
types of white blood cells, and platelets is a critical task for healthcare
practitioners. Deep learning approaches, particularly convolutional neural
networks (CNNs) using supervised learning strategies, have shown considerable
success for such tasks. However, CNN based architectures such as U-Net, often
struggles to accurately identify platelets due to their sizes and high
variability of features. To address these challenges, researchers have commonly
employed strategies such as class weighted loss functions, which have
demonstrated some success. However, this does not address the more significant
challenge of platelet variability in size and tendency to form aggregates and
associations with other blood components. In this study, we explored an
alternative approach by investigating the role of convolutional kernels in
mitigating these issues. We also assigned separate classes to singular
platelets and platelet aggregates and performed semantic segmentation using
various U-Net architectures for identifying platelets. We then evaluated and
compared two common methods (pixel area method and connected component
analysis) for counting platelets and proposed an alternative approach
specialized for single platelets and platelet aggregates. Our experiments
provided results that showed significant improvements in the identification of
platelets, highlighting the importance of optimizing convolutional operations
and class designations. We show that the common practice of pixel area-based
counting often over estimate platelet counts, whereas the proposed method
presented in this work offers significant improvements. We discuss in detail
about these methods from segmentation masks.

</details>

### [419] [Efficient Multi Subject Visual Reconstruction from fMRI Using Aligned Representations](https://arxiv.org/abs/2505.01670)
*Christos Zangos,Danish Ebadulla,Thomas Christopher Sprague,Ambuj Singh*

Main category: eess.IV

TLDR: 提出了一种基于fMRI的视觉图像重建新方法，利用主题无关的通用表示空间，显著提升了低数据场景下的效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法在跨主题和低数据场景下效率不足，需一种更高效的方法。

Method: 通过训练将不同主题的脑信号对齐到通用空间，形成语义对齐的通用脑，并利用轻量级模块对齐参考主题。

Result: 在多个数据集上验证了通用空间的主题和数据集无关性，且在低数据场景下表现优异。

Conclusion: 该方法为跨主题脑信号对齐提供了一种高效且通用的解决方案。

Abstract: This work introduces a novel approach to fMRI-based visual image
reconstruction using a subject-agnostic common representation space. We show
that the brain signals of the subjects can be aligned in this common space
during training to form a semantically aligned common brain. This is leveraged
to demonstrate that aligning subject-specific lightweight modules to a
reference subject is significantly more efficient than traditional end-to-end
training methods. Our approach excels in low-data scenarios. We evaluate our
methods on different datasets, demonstrating that the common space is subject
and dataset-agnostic.

</details>

### [420] [Easz: An Agile Transformer-based Image Compression Framework for Resource-constrained IoTs](https://arxiv.org/abs/2505.01742)
*Yu Mao,Jingzong Li,Jun Wang,Hong Xu,Tei-Wei Kuo,Nan Guan,Chun Jason Xue*

Main category: eess.IV

TLDR: 提出了一种基于Transformer的边缘计算免费图像编码框架Easz，通过将计算负担转移到服务器，解决了神经图像压缩在边缘设备上的应用挑战。


<details>
  <summary>Details</summary>
Motivation: 神经图像压缩在边缘设备上应用时，由于编码-解码结构复杂且难以灵活切换压缩级别，面临重大挑战。

Method: Easz采用基于条件均匀采样的patch-erase算法选择性移除图像内容，并在接收端通过轻量级Transformer框架重建像素。

Result: 实验表明，Easz在压缩级别适应性、计算效率和图像重建质量上优于现有方法。

Conclusion: Easz为边缘设备上的神经图像压缩提供了高效灵活的解决方案。

Abstract: Neural image compression, necessary in various machine-to-machine
communication scenarios, suffers from its heavy encode-decode structures and
inflexibility in switching between different compression levels. Consequently,
it raises significant challenges in applying the neural image compression to
edge devices that are developed for powerful servers with high computational
and storage capacities. We take a step to solve the challenges by proposing a
new transformer-based edge-compute-free image coding framework called Easz.
Easz shifts the computational overhead to the server, and hence avoids the
heavy encoding and model switching overhead on the edge. Easz utilizes a
patch-erase algorithm to selectively remove image contents using a conditional
uniform-based sampler. The erased pixels are reconstructed on the receiver side
through a transformer-based framework. To further reduce the computational
overhead on the receiver, we then introduce a lightweight transformer-based
reconstruction structure to reduce the reconstruction load on the receiver
side. Extensive evaluations conducted on a real-world testbed demonstrate
multiple advantages of Easz over existing compression approaches, in terms of
adaptability to different compression levels, computational efficiency, and
image reconstruction quality.

</details>

### [421] [A Dual-Task Synergy-Driven Generalization Framework for Pancreatic Cancer Segmentation in CT Scans](https://arxiv.org/abs/2505.01644)
*Jun Li,Yijue Zhang,Haibo Shi,Minhong Li,Qiwei Li,Xiaohua Qian*

Main category: eess.IV

TLDR: 提出了一种结合像素级分类和回归任务的双任务框架，用于胰腺癌病灶的精确分割和模型稳定性提升，实验表明其在跨病灶分割任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 胰腺癌病灶分割的通用性受限于影像差异和病灶异质性，现有方法难以准确区分病灶与正常组织。

Method: 通过双任务框架（分割与回归）结合自监督学习，提升模型在特征空间和输出空间的表示能力与稳定性。

Result: 在594个样本的三数据集上，分割性能（Dice: 84.07%）接近主流域内验证结果，跨病灶分割任务提升9.51%。

Conclusion: 该模型为胰腺疾病管理提供了高效的技术支持，代码已开源。

Abstract: Pancreatic cancer, characterized by its notable prevalence and mortality
rates, demands accurate lesion delineation for effective diagnosis and
therapeutic interventions. The generalizability of extant methods is frequently
compromised due to the pronounced variability in imaging and the heterogeneous
characteristics of pancreatic lesions, which may mimic normal tissues and
exhibit significant inter-patient variability. Thus, we propose a
generalization framework that synergizes pixel-level classification and
regression tasks, to accurately delineate lesions and improve model stability.
This framework not only seeks to align segmentation contours with actual
lesions but also uses regression to elucidate spatial relationships between
diseased and normal tissues, thereby improving tumor localization and
morphological characterization. Enhanced by the reciprocal transformation of
task outputs, our approach integrates additional regression supervision within
the segmentation context, bolstering the model's generalization ability from a
dual-task perspective. Besides, dual self-supervised learning in feature spaces
and output spaces augments the model's representational capability and
stability across different imaging views. Experiments on 594 samples composed
of three datasets with significant imaging differences demonstrate that our
generalized pancreas segmentation results comparable to mainstream in-domain
validation performance (Dice: 84.07%). More importantly, it successfully
improves the results of the highly challenging cross-lesion generalized
pancreatic cancer segmentation task by 9.51%. Thus, our model constitutes a
resilient and efficient foundational technological support for pancreatic
disease management and wider medical applications. The codes will be released
at https://github.com/SJTUBME-QianLab/Dual-Task-Seg.

</details>

### [422] [CLOG-CD: Curriculum Learning based on Oscillating Granularity of Class Decomposed Medical Image Classification](https://arxiv.org/abs/2505.01741)
*Asmaa Abbas,Mohamed Gaber,Mohammed M. Abdelsamea*

Main category: eess.IV

TLDR: 提出了一种结合课程学习策略和类分解方法的CNN训练方法CLOG-CD，用于提升医学图像分类性能，并在多个不平衡数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 医学图像数据的不规则性导致分类任务困难，传统方法易误分类。结合课程学习和类分解方法有望提升性能。

Method: 提出CLOG-CD方法，利用类分解的粒度权重，采用反课程技术（从高到低难度）训练，并测试不同加速因子和进度函数。

Result: 在四个医学图像数据集上，CLOG-CD显著提升了分类准确率，最高达99.45%（CRC数据集）。

Conclusion: CLOG-CD在医学图像分类中表现优异，为不平衡数据集的训练提供了有效解决方案。

Abstract: Curriculum learning strategies have been proven to be effective in various
applications and have gained significant interest in the field of machine
learning. It has the ability to improve the final model's performance and
accelerate the training process. However, in the medical imaging domain, data
irregularities can make the recognition task more challenging and usually
result in misclassification between the different classes in the dataset.
Class-decomposition approaches have shown promising results in solving such a
problem by learning the boundaries within the classes of the data set. In this
paper, we present a novel convolutional neural network (CNN) training method
based on the curriculum learning strategy and the class decomposition approach,
which we call CLOG-CD, to improve the performance of medical image
classification. We evaluated our method on four different imbalanced medical
image datasets, such as Chest X-ray (CXR), brain tumour, digital knee X-ray,
and histopathology colorectal cancer (CRC). CLOG-CD utilises the learnt weights
from the decomposition granularity of the classes, and the training is
accomplished from descending to ascending order (i.e., anti-curriculum
technique). We also investigated the classification performance of our proposed
method based on different acceleration factors and pace function curricula. We
used two pre-trained networks, ResNet-50 and DenseNet-121, as the backbone for
CLOG-CD. The results with ResNet-50 show that CLOG-CD has the ability to
improve classification performance with an accuracy of 96.08% for the CXR
dataset, 96.91% for the brain tumour dataset, 79.76% for the digital knee
X-ray, and 99.17% for the CRC dataset, compared to other training strategies.
In addition, with DenseNet-121, CLOG-CD has achieved 94.86%, 94.63%, 76.19%,
and 99.45% for CXR, brain tumour, digital knee X-ray, and CRC datasets,
respectively

</details>

### [423] [LensNet: An End-to-End Learning Framework for Empirical Point Spread Function Modeling and Lensless Imaging Reconstruction](https://arxiv.org/abs/2505.01755)
*Jiesong Bai,Yuhao Yin,Yihang Dong,Xiaofeng Zhang,Chi-Man Pun,Xuhang Chen*

Main category: eess.IV

TLDR: LensNet是一种端到端深度学习框架，通过动态估计点扩散函数（PSF）和嵌入维纳滤波，提升了无透镜成像系统的重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统无透镜成像技术依赖静态或近似PSF模型，适应性差，难以应对噪声和动态场景变化。

Method: 提出LensNet框架，结合空间域和频域表示，使用可学习的编码掩模模拟器（CMS）动态估计PSF，并嵌入维纳滤波优化重建。

Result: 实验表明，LensNet在保留高频细节和降噪方面优于现有方法，重建质量更高。

Conclusion: LensNet为无透镜成像提供了更准确、灵活的解决方案，适用于微型传感器和医疗诊断等领域。

Abstract: Lensless imaging stands out as a promising alternative to conventional
lens-based systems, particularly in scenarios demanding ultracompact form
factors and cost-effective architectures. However, such systems are
fundamentally governed by the Point Spread Function (PSF), which dictates how a
point source contributes to the final captured signal. Traditional lensless
techniques often require explicit calibrations and extensive pre-processing,
relying on static or approximate PSF models. These rigid strategies can result
in limited adaptability to real-world challenges, including noise, system
imperfections, and dynamic scene variations, thus impeding high-fidelity
reconstruction. In this paper, we propose LensNet, an end-to-end deep learning
framework that integrates spatial-domain and frequency-domain representations
in a unified pipeline. Central to our approach is a learnable Coded Mask
Simulator (CMS) that enables dynamic, data-driven estimation of the PSF during
training, effectively mitigating the shortcomings of fixed or sparsely
calibrated kernels. By embedding a Wiener filtering component, LensNet refines
global structure and restores fine-scale details, thus alleviating the
dependency on multiple handcrafted pre-processing steps. Extensive experiments
demonstrate LensNet's robust performance and superior reconstruction quality
compared to state-of-the-art methods, particularly in preserving high-frequency
details and attenuating noise. The proposed framework establishes a novel
convergence between physics-based modeling and data-driven learning, paving the
way for more accurate, flexible, and practical lensless imaging solutions for
applications ranging from miniature sensors to medical diagnostics. The link of
code is https://github.com/baijiesong/Lensnet.

</details>

### [424] [Continuous Filtered Backprojection by Learnable Interpolation Network](https://arxiv.org/abs/2505.01768)
*Hui Lin,Dong Zeng,Qi Xie,Zerui Mao,Jianhua Ma,Deyu Meng*

Main category: eess.IV

TLDR: 提出了一种名为LInFBP的深度学习模型，通过可学习的插值方法改进CT图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统CT重建方法中的插值误差会影响图像精度，需要改进。

Method: 利用深度学习网络预测线性组合系数，构建连续函数用于插值。

Result: 实验表明LInFBP能显著提升图像质量，并具有通用性。

Conclusion: LInFBP首次将深度学习应用于FBP插值，效果显著。

Abstract: Accurate reconstruction of computed tomography (CT) images is crucial in
medical imaging field. However, there are unavoidable interpolation errors in
the backprojection step of the conventional reconstruction methods, i.e.,
filtered-back-projection based methods, which are detrimental to the accurate
reconstruction. In this study, to address this issue, we propose a novel deep
learning model, named Leanable-Interpolation-based FBP or LInFBP shortly, to
enhance the reconstructed CT image quality, which achieves learnable
interpolation in the backprojection step of filtered backprojection (FBP) and
alleviates the interpolation errors. Specifically, in the proposed LInFBP, we
formulate every local piece of the latent continuous function of discrete
sinogram data as a linear combination of selected basis functions, and learn
this continuous function by exploiting a deep network to predict the linear
combination coefficients. Then, the learned latent continuous function is
exploited for interpolation in backprojection step, which first time takes the
advantage of deep learning for the interpolation in FBP. Extensive experiments,
which encompass diverse CT scenarios, demonstrate the effectiveness of the
proposed LInFBP in terms of enhanced reconstructed image quality, plug-and-play
ability and generalization capability.

</details>

### [425] [Multi-Scale Target-Aware Representation Learning for Fundus Image Enhancement](https://arxiv.org/abs/2505.01831)
*Haofan Wu,Yin Huang,Yuqing Wu,Qiuyu Yang,Bingfang Wang,Li Zhang,Muhammad Fahadullah Khan,Ali Zia,M. Saleh Memon,Syed Sohail Bukhari,Abdul Fattah Memon,Daizong Ji,Ya Zhang,Ghulam Mustafa,Yin Fang*

Main category: eess.IV

TLDR: 提出了一种多尺度目标感知表示学习框架（MTRL-FIE），用于高效增强眼底图像质量，解决现有方法在恢复多尺度信息和聚焦病灶区域的不足。


<details>
  <summary>Details</summary>
Motivation: 眼底图像常因硬件限制和操作变异性导致分辨率低和信噪比差，现有方法缺乏统一的多尺度增强框架和病灶区域针对性增强。

Method: 提出多尺度特征编码器（MFE）和小波分解嵌入低频结构信息与高频细节，设计结构保持分层解码器（SHD）和靶向特征聚合（TFA）模块。

Result: 在多个数据集上验证了MTRL-FIE的有效性和泛化性，性能优于现有方法且架构更轻量。

Conclusion: MTRL-FIE不仅能高效增强眼底图像，还能泛化至其他眼科图像处理任务，具有临床潜力。

Abstract: High-quality fundus images provide essential anatomical information for
clinical screening and ophthalmic disease diagnosis. Yet, due to hardware
limitations, operational variability, and patient compliance, fundus images
often suffer from low resolution and signal-to-noise ratio. Recent years have
witnessed promising progress in fundus image enhancement. However, existing
works usually focus on restoring structural details or global characteristics
of fundus images, lacking a unified image enhancement framework to recover
comprehensive multi-scale information. Moreover, few methods pinpoint the
target of image enhancement, e.g., lesions, which is crucial for medical
image-based diagnosis. To address these challenges, we propose a multi-scale
target-aware representation learning framework (MTRL-FIE) for efficient fundus
image enhancement. Specifically, we propose a multi-scale feature encoder (MFE)
that employs wavelet decomposition to embed both low-frequency structural
information and high-frequency details. Next, we design a structure-preserving
hierarchical decoder (SHD) to fuse multi-scale feature embeddings for real
fundus image restoration. SHD integrates hierarchical fusion and group
attention mechanisms to achieve adaptive feature fusion while retaining local
structural smoothness. Meanwhile, a target-aware feature aggregation (TFA)
module is used to enhance pathological regions and reduce artifacts.
Experimental results on multiple fundus image datasets demonstrate the
effectiveness and generalizability of MTRL-FIE for fundus image enhancement.
Compared to state-of-the-art methods, MTRL-FIE achieves superior enhancement
performance with a more lightweight architecture. Furthermore, our approach
generalizes to other ophthalmic image processing tasks without supervised
fine-tuning, highlighting its potential for clinical applications.

</details>

### [426] [Hybrid Image Resolution Quality Metric (HIRQM):A Comprehensive Perceptual Image Quality Assessment Framework](https://arxiv.org/abs/2505.02001)
*Vineesh Kumar Reddy Mondem*

Main category: eess.IV

TLDR: HIRQM是一种结合统计、多尺度和深度学习的图像质量评估方法，优于传统指标。


<details>
  <summary>Details</summary>
Motivation: 传统图像质量评估指标（如MSE和SSIM）在复杂失真下无法反映感知质量。

Method: HIRQM整合了PDF、多尺度特征相似性和预训练VGG16网络的深度特征，通过动态权重机制适应不同图像特性。

Result: 在TID2013和LIVE数据集上，HIRQM的Pearson和Spearman相关系数分别为0.92和0.90，优于传统方法。

Conclusion: HIRQM在处理噪声、模糊和压缩伪影方面表现优异，适用于图像压缩和修复等应用。

Abstract: Traditional image quality assessment metrics like Mean Squared Error and
Structural Similarity Index often fail to reflect perceptual quality under
complex distortions. We propose the Hybrid Image Resolution Quality Metric
(HIRQM), integrating statistical, multi-scale, and deep learning-based methods
for a comprehensive quality evaluation. HIRQM combines three components:
Probability Density Function for local pixel distribution analysis, Multi-scale
Feature Similarity for structural integrity across resolutions, and
Hierarchical Deep Image Features using a pre-trained VGG16 network for semantic
alignment with human perception. A dynamic weighting mechanism adapts component
contributions based on image characteristics like brightness and variance,
enhancing flexibility across distortion types. Our contributions include a
unified metric and dynamic weighting for better perceptual alignment. Evaluated
on TID2013 and LIVE datasets, HIRQM achieves Pearson and Spearman correlations
of 0.92 and 0.90, outperforming traditional metrics. It excels in handling
noise, blur, and compression artifacts, making it valuable for image processing
applications like compression and restoration.

</details>

### [427] [CSASN: A Multitask Attention-Based Framework for Heterogeneous Thyroid Carcinoma Classification in Ultrasound Images](https://arxiv.org/abs/2505.02211)
*Peiqi Li,Yincheng Gao,Renxing Li,Haojie Yang,Yunyun Liu,Boji Liu,Jiahui Ni,Ying Zhang,Yulu Wu,Xiaowei Fang,Lehang Guo,Liping Sun,Jiangang Chen*

Main category: eess.IV

TLDR: 提出了一种多任务学习框架CSASN，结合EfficientNet和ViT的双分支特征提取器，通过通道-空间注意力模块提升罕见甲状腺癌分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决超声图像中罕见甲状腺癌分类的形态异质性和数据不平衡问题。

Method: 采用双分支特征提取器（EfficientNet和ViT）、通道-空间注意力模块、残差多尺度分类器和动态加权损失函数。

Result: 在2000多名患者的多中心数据集上表现优异，尤其在罕见亚型（FTC和MTC）分类上优于现有单流CNN或Transformer模型。

Conclusion: CSASN为AI辅助甲状腺癌诊断提供了有效策略。

Abstract: Heterogeneous morphological features and data imbalance pose significant
challenges in rare thyroid carcinoma classification using ultrasound imaging.
To address this issue, we propose a novel multitask learning framework,
Channel-Spatial Attention Synergy Network (CSASN), which integrates a
dual-branch feature extractor - combining EfficientNet for local spatial
encoding and ViT for global semantic modeling, with a cascaded channel-spatial
attention refinement module. A residual multiscale classifier and dynamically
weighted loss function further enhance classification stability and accuracy.
Trained on a multicenter dataset comprising more than 2000 patients from four
clinical institutions, our framework leverages a residual multiscale classifier
and dynamically weighted loss function to enhance classification stability and
accuracy. Extensive ablation studies demonstrate that each module contributes
significantly to model performance, particularly in recognizing rare subtypes
such as FTC and MTC carcinomas. Experimental results show that CSASN
outperforms existing single-stream CNN or Transformer-based models, achieving a
superior balance between precision and recall under class-imbalanced
conditions. This framework provides a promising strategy for AI-assisted
thyroid cancer diagnosis.

</details>

### [428] [An Arbitrary-Modal Fusion Network for Volumetric Cranial Nerves Tract Segmentation](https://arxiv.org/abs/2505.02385)
*Lei Xie,Huajun Zhou,Junxiong Huang,Jiahao Huang,Qingrun Zeng,Jianzhong He,Jiawei Zhang,Baohua Fan,Mingchu Li,Guoqiang Xie,Hao Chen,Yuanjing Feng*

Main category: eess.IV

TLDR: CNTSeg-v2是一种新型的任意模态融合网络，用于颅神经束分割，通过T1加权图像监督辅助模态信息选择，显著提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 临床实践中难以获取完整的多模态数据，限制了现有方法的实用性。

Method: 提出CNTSeg-v2，结合T1加权图像作为主模态，设计任意模态协作模块（ACM）和深度距离引导多阶段解码器（DDM）。

Result: 在HCP和MDM数据集上表现优异，超越现有方法。

Conclusion: CNTSeg-v2为颅神经束分割提供了高效且灵活的解决方案。

Abstract: The segmentation of cranial nerves (CNs) tract provides a valuable
quantitative tool for the analysis of the morphology and trajectory of
individual CNs. Multimodal CNs tract segmentation networks, e.g., CNTSeg, which
combine structural Magnetic Resonance Imaging (MRI) and diffusion MRI, have
achieved promising segmentation performance. However, it is laborious or even
infeasible to collect complete multimodal data in clinical practice due to
limitations in equipment, user privacy, and working conditions. In this work,
we propose a novel arbitrary-modal fusion network for volumetric CNs tract
segmentation, called CNTSeg-v2, which trains one model to handle different
combinations of available modalities. Instead of directly combining all the
modalities, we select T1-weighted (T1w) images as the primary modality due to
its simplicity in data acquisition and contribution most to the results, which
supervises the information selection of other auxiliary modalities. Our model
encompasses an Arbitrary-Modal Collaboration Module (ACM) designed to
effectively extract informative features from other auxiliary modalities,
guided by the supervision of T1w images. Meanwhile, we construct a Deep
Distance-guided Multi-stage (DDM) decoder to correct small errors and
discontinuities through signed distance maps to improve segmentation accuracy.
We evaluate our CNTSeg-v2 on the Human Connectome Project (HCP) dataset and the
clinical Multi-shell Diffusion MRI (MDM) dataset. Extensive experimental
results show that our CNTSeg-v2 achieves state-of-the-art segmentation
performance, outperforming all competing methods.

</details>

### [429] [Diagnostic Uncertainty in Pneumonia Detection using CNN MobileNetV2 and CNN from Scratch](https://arxiv.org/abs/2505.02396)
*Kennard Norbert Sudiardjo,Islam Nur Alam,Wilson Wijaya,Lili Ayu Wulandhari*

Main category: eess.IV

TLDR: 该研究提出了一种基于CNN（MobileNetV2和ResNet101V2）的方法用于肺炎诊断，比较了预训练模型和从头构建模型的性能。结果显示MobileNetV2更稳定，而Scratch模型虽准确率高但易过拟合。


<details>
  <summary>Details</summary>
Motivation: 肺炎诊断的不确定性（如非典型表现、胸片限制和共存呼吸道疾病）促使研究采用深度学习方法提高诊断准确性。

Method: 使用MobileNetV2（预训练）和ResNet101V2（从头构建）的CNN模型，通过Kaggle数据集训练和验证。

Result: MobileNetV2表现稳定（训练准确率84.87%，验证损失0.6345），Scratch模型虽准确率高但过拟合明显（训练准确率降至78.12%，验证损失增至1.1809）。

Conclusion: MobileNetV2在稳定性上更优，而Scratch模型在高精度场景可能有潜力，但需解决过拟合问题。

Abstract: Pneumonia Diagnosis, though it is crucial for an effective treatment, it can
be hampered by uncertainty. This uncertainty starts to arise due to some
factors like atypical presentations, limitations of diagnostic tools such as
chest X-rays, and the presence of co-existing respiratory conditions. This
research proposes one of the supervised learning methods, CNN. Using
MobileNetV2 as the pre-trained one with ResNet101V2 architecture and using
Keras API as the built from scratch model, for identifying lung diseases
especially pneumonia. The datasets used in this research were obtained from the
website through Kaggle. The result shows that by implementing CNN MobileNetV2
and CNN from scratch the result is promising. While validating data,
MobileNetV2 performs with stability and minimal overfitting, while the training
accuracy increased to 84.87% later it slightly decreased to 78.95%, with
increasing validation loss from 0.499 to 0.6345. Nonetheless, MobileNetV2 is
more stable. Although it takes more time to train each epoch. Meanwhile, after
the 10th epoch, the Scratch model displayed more instability and overfitting
despite having higher validation accuracy, training accuracy decreased
significantly to 78.12% and the validation loss increased from 0.5698 to
1.1809. With these results, ResNet101V2 offers stability, and the Scratch model
offers high accuracy.

</details>

### [430] [DeepSparse: A Foundation Model for Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2505.02628)
*Yiqun Lin,Hualiang Wang,Jixiang Chen,Jiewen Yang,Jiarong Guo,Xiaomeng Li*

Main category: eess.IV

TLDR: DeepSparse是一种用于稀疏视图CBCT重建的基础模型，通过DiCE网络和HyViP框架，显著提升了重建质量并降低了计算需求。


<details>
  <summary>Details</summary>
Motivation: 高辐射暴露是CBCT成像的主要问题，现有稀疏视图重建方法计算量大且泛化性差。

Method: 提出DeepSparse模型，结合DiCE网络（整合2D和3D特征）和HyViP预训练框架，采用两步微调策略。

Result: 实验表明，DeepSparse在重建质量上优于现有方法。

Conclusion: DeepSparse为更安全高效的CBCT成像提供了新途径。

Abstract: Cone-beam computed tomography (CBCT) is a critical 3D imaging technology in
the medical field, while the high radiation exposure required for high-quality
imaging raises significant concerns, particularly for vulnerable populations.
Sparse-view reconstruction reduces radiation by using fewer X-ray projections
while maintaining image quality, yet existing methods face challenges such as
high computational demands and poor generalizability to different datasets. To
overcome these limitations, we propose DeepSparse, the first foundation model
for sparse-view CBCT reconstruction, featuring DiCE (Dual-Dimensional
Cross-Scale Embedding), a novel network that integrates multi-view 2D features
and multi-scale 3D features. Additionally, we introduce the HyViP (Hybrid View
Sampling Pretraining) framework, which pretrains the model on large datasets
with both sparse-view and dense-view projections, and a two-step finetuning
strategy to adapt and refine the model for new datasets. Extensive experiments
and ablation studies demonstrate that our proposed DeepSparse achieves superior
reconstruction quality compared to state-of-the-art methods, paving the way for
safer and more efficient CBCT imaging.

</details>

### [431] [Multi-View Learning with Context-Guided Receptance for Image Denoising](https://arxiv.org/abs/2505.02705)
*Binghong Chen,Tingting Chai,Wei Jiang,Yuanrong Xu,Guanglu Zhou,Xiangqian Wu*

Main category: eess.IV

TLDR: 提出了一种结合多视角特征集成和高效序列建模的图像去噪方法，通过CTS和FMix模块提升噪声建模能力，BiWKV机制降低计算复杂度，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以区分真实场景中的复杂噪声模式且计算资源消耗大，需改进。

Method: 结合CTS范式捕捉局部空间依赖，FMix模块提取频域特征，BiWKV机制实现线性复杂度序列建模。

Result: 在多个数据集上定量优于现有方法，推理时间减少40%，定性结果展示细节恢复能力。

Conclusion: 该方法在去噪效果和计算效率上均有显著提升，适用于真实场景。

Abstract: Image denoising is essential in low-level vision applications such as
photography and automated driving. Existing methods struggle with
distinguishing complex noise patterns in real-world scenes and consume
significant computational resources due to reliance on Transformer-based
models. In this work, the Context-guided Receptance Weighted Key-Value (\M)
model is proposed, combining enhanced multi-view feature integration with
efficient sequence modeling. Our approach introduces the Context-guided Token
Shift (CTS) paradigm, which effectively captures local spatial dependencies and
enhance the model's ability to model real-world noise distributions.
Additionally, the Frequency Mix (FMix) module extracting frequency-domain
features is designed to isolate noise in high-frequency spectra, and is
integrated with spatial representations through a multi-view learning process.
To improve computational efficiency, the Bidirectional WKV (BiWKV) mechanism is
adopted, enabling full pixel-sequence interaction with linear complexity while
overcoming the causal selection constraints. The model is validated on multiple
real-world image denoising datasets, outperforming the existing
state-of-the-art methods quantitatively and reducing inference time up to 40\%.
Qualitative results further demonstrate the ability of our model to restore
fine details in various scenes.

</details>

### [432] [Deep learning of personalized priors from past MRI scans enables fast, quality-enhanced point-of-care MRI with low-cost systems](https://arxiv.org/abs/2505.02470)
*Tal Oved,Beatrice Lena,Chloé F. Najac,Sheng Shen,Matthew S. Rosen,Andrew Webb,Efrat Shimron*

Main category: eess.IV

TLDR: 提出了一种利用深度学习从历史高场MRI扫描中提取个性化特征的方法，以提升低成本低场MRI的图像质量和扫描速度。


<details>
  <summary>Details</summary>
Motivation: 高场MRI成本高，低场MRI图像质量差且扫描时间长，限制了MRI的普及和长期监测需求。

Method: 提出ViT-Fuser，一种特征融合视觉变换器，利用历史高场MRI扫描的特征来增强低场MRI的图像质量和速度。

Result: 实验表明，ViT-Fuser在多种数据集上优于现有方法，能显著提升低场MRI的图像质量，且对分布外数据具有鲁棒性。

Conclusion: 该框架为低成本、高质量的MRI扫描提供了可行方案，具有广泛的医疗应用潜力。

Abstract: Magnetic resonance imaging (MRI) offers superb-quality images, but its
accessibility is limited by high costs, posing challenges for patients
requiring longitudinal care. Low-field MRI provides affordable imaging with
low-cost devices but is hindered by long scans and degraded image quality,
including low signal-to-noise ratio (SNR) and tissue contrast. We propose a
novel healthcare paradigm: using deep learning to extract personalized features
from past standard high-field MRI scans and harnessing them to enable
accelerated, enhanced-quality follow-up scans with low-cost systems. To
overcome the SNR and contrast differences, we introduce ViT-Fuser, a
feature-fusion vision transformer that learns features from past scans, e.g.
those stored in standard DICOM CDs. We show that \textit{a single prior scan is
sufficient}, and this scan can come from various MRI vendors, field strengths,
and pulse sequences. Experiments with four datasets, including glioblastoma
data, low-field ($50mT$), and ultra-low-field ($6.5mT$) data, demonstrate that
ViT-Fuser outperforms state-of-the-art methods, providing enhanced-quality
images from accelerated low-field scans, with robustness to out-of-distribution
data. Our freely available framework thus enables rapid, diagnostic-quality,
low-cost imaging for wide healthcare applications.

</details>

### [433] [Lane-Wise Highway Anomaly Detection](https://arxiv.org/abs/2505.02613)
*Mei Qiu,William Lorenz Reindl,Yaobin Chen,Stanley Chien,Shu Hu*

Main category: eess.IV

TLDR: 提出了一种基于多模态时间序列数据的可扩展、可解释的高速公路车道异常检测框架，利用AI视觉模型提取车道特征，无需依赖昂贵硬件或复杂建模。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖传感器且成本高，本文旨在提供一种更高效、低成本的车道异常检测方案。

Method: 结合深度学习、规则逻辑和机器学习，构建多分支检测系统，并引入包含73,139个样本的新数据集。

Result: 实验表明，该框架在精确率、召回率和F1分数上优于现有方法。

Conclusion: 该框架为智能交通系统提供了经济高效且可扩展的解决方案。

Abstract: This paper proposes a scalable and interpretable framework for lane-wise
highway traffic anomaly detection, leveraging multi-modal time series data
extracted from surveillance cameras. Unlike traditional sensor-dependent
methods, our approach uses AI-powered vision models to extract lane-specific
features, including vehicle count, occupancy, and truck percentage, without
relying on costly hardware or complex road modeling. We introduce a novel
dataset containing 73,139 lane-wise samples, annotated with four classes of
expert-validated anomalies: three traffic-related anomalies (lane blockage and
recovery, foreign object intrusion, and sustained congestion) and one
sensor-related anomaly (camera angle shift). Our multi-branch detection system
integrates deep learning, rule-based logic, and machine learning to improve
robustness and precision. Extensive experiments demonstrate that our framework
outperforms state-of-the-art methods in precision, recall, and F1-score,
providing a cost-effective and scalable solution for real-world intelligent
transportation systems.

</details>

<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [434] [Heterosynaptic Circuits Are Universal Gradient Machines](https://arxiv.org/abs/2505.02248)
*Liu Ziyin,Isaac Chuang,Tomaso Poggio*

Main category: q-bio.NC

TLDR: 论文提出了一种生物大脑学习回路的设计原则，认为异突触可塑性（HSP）可实现高效的梯度元学习，并统一解释了（反）赫布可塑性（HBP）与HSP的动态关系。


<details>
  <summary>Details</summary>
Motivation: 探索生物大脑学习回路的通用设计原则，揭示HSP在学习和记忆中的核心作用，并为AI算法和硬件设计提供新思路。

Method: 通过理论分析和模拟实验，验证HSP能否解释神经元的元可塑性、生物回路的灵活性，以及梯度学习的自然涌现。

Result: 研究表明HSP能解释神经元元可塑性和生物回路的灵活性，并证明梯度学习可通过简单进化动态实现。

Conclusion: HSP可能是生物学习和记忆的主要机制，而HBP是其副产品；梯度计算在自然界中可能非常普遍，为AI设计提供了新视角。

Abstract: We propose a design principle for the learning circuits of the biological
brain. The principle states that almost any dendritic weights updated via
heterosynaptic plasticity can implement a generalized and efficient class of
gradient-based meta-learning. The theory suggests that a broad class of
biologically plausible learning algorithms, together with the standard machine
learning optimizers, can be grounded in heterosynaptic circuit motifs. This
principle suggests that the phenomenology of (anti-) Hebbian (HBP) and
heterosynaptic plasticity (HSP) may emerge from the same underlying dynamics,
thus providing a unifying explanation. It also suggests an alternative
perspective of neuroplasticity, where HSP is promoted to the primary learning
and memory mechanism, and HBP is an emergent byproduct. We present simulations
that show that (a) HSP can explain the metaplasticity of neurons, (b) HSP can
explain the flexibility of the biology circuits, and (c) gradient learning can
arise quickly from simple evolutionary dynamics that do not compute any
explicit gradient. While our primary focus is on biology, the principle also
implies a new approach to designing AI training algorithms and physically
learnable AI hardware. Conceptually, our result demonstrates that contrary to
the common belief, gradient computation may be extremely easy and common in
nature.

</details>

<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [435] [Surrogate to Poincaré inequalities on manifolds for dimension reduction in nonlinear feature spaces](https://arxiv.org/abs/2505.01807)
*Anthony Nouy,Alexandre Pasco*

Main category: math.NA

TLDR: 论文提出了一种新的凸替代方法来优化基于Poincaré不等式的损失函数，用于近似连续可微函数，并在小训练集和低维情况下表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 近似连续可微函数时，传统方法在优化基于Poincaré不等式的损失函数时可能面临困难，因此需要更高效的替代方法。

Method: 采用两阶段方法构建函数组合$f\circ g$，其中$g$通过最小化新的凸替代损失函数来优化，$f$则通过经典回归方法构建。

Result: 新方法在小训练集和$m=1$的情况下表现优于传统迭代方法，近似误差更小。

Conclusion: 提出的凸替代方法在优化Poincaré不等式损失函数时更高效，尤其适用于小训练集和低维问题。

Abstract: We aim to approximate a continuously differentiable function $u:\mathbb{R}^d
\rightarrow \mathbb{R}$ by a composition of functions $f\circ g$ where
$g:\mathbb{R}^d \rightarrow \mathbb{R}^m$, $m\leq d$, and $f : \mathbb{R}^m
\rightarrow \mathbb{R}$ are built in a two stage procedure. For a fixed $g$, we
build $f$ using classical regression methods, involving evaluations of $u$.
Recent works proposed to build a nonlinear $g$ by minimizing a loss function
$\mathcal{J}(g)$ derived from Poincar\'e inequalities on manifolds, involving
evaluations of the gradient of $u$. A problem is that minimizing $\mathcal{J}$
may be a challenging task. Hence in this work, we introduce new convex
surrogates to $\mathcal{J}$. Leveraging concentration inequalities, we provide
sub-optimality results for a class of functions $g$, including polynomials, and
a wide class of input probability measures. We investigate performances on
different benchmarks for various training sample sizes. We show that our
approach outperforms standard iterative methods for minimizing the training
Poincar\'e inequality based loss, often resulting in better approximation
errors, especially for rather small training sets and $m=1$.

</details>

<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [436] [Edge-Cloud Collaborative Computing on Distributed Intelligence and Model Optimization: A Survey](https://arxiv.org/abs/2505.01821)
*Jing Liu,Yao Du,Kun Yang,Yan Wang,Xiping Hu,Zehua Wang,Yang Liu,Peng Sun,Azzedine Boukerche,Victor C. M. Leung*

Main category: cs.DC

TLDR: 本文综述了边缘-云协同计算（ECCC）在分布式智能和模型优化中的应用，涵盖架构、技术、优化方法、资源管理、隐私安全及实际部署，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现代智能应用对计算需求日益增长，边缘-云协同计算通过整合云资源和边缘设备，提供高效低延迟处理，但AI模型部署和资源管理仍面临挑战。

Method: 系统分析边缘-云环境中的分布式智能与模型优化，包括压缩、适应、神经架构搜索等技术，以及AI驱动的资源管理策略。

Result: 总结了ECCC在自动驾驶、医疗和工业自动化等领域的实际应用，并提出了性能评估标准。

Conclusion: 本文为研究者和从业者提供了优化分布式计算环境的全面视角，并指出了LLMs部署、6G集成等未来研究方向。

Abstract: Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm
for addressing the computational demands of modern intelligent applications,
integrating cloud resources with edge devices to enable efficient, low-latency
processing. Recent advancements in AI, particularly deep learning and large
language models (LLMs), have dramatically enhanced the capabilities of these
distributed systems, yet introduce significant challenges in model deployment
and resource management. In this survey, we comprehensive examine the
intersection of distributed intelligence and model optimization within
edge-cloud environments, providing a structured tutorial on fundamental
architectures, enabling technologies, and emerging applications. Additionally,
we systematically analyze model optimization approaches, including compression,
adaptation, and neural architecture search, alongside AI-driven resource
management strategies that balance performance, energy efficiency, and latency
requirements. We further explore critical aspects of privacy protection and
security enhancement within ECCC systems and examines practical deployments
through diverse applications, spanning autonomous driving, healthcare, and
industrial automation. Performance analysis and benchmarking techniques are
also thoroughly explored to establish evaluation standards for these complex
systems. Furthermore, the review identifies critical research directions
including LLMs deployment, 6G integration, neuromorphic computing, and quantum
computing, offering a roadmap for addressing persistent challenges in
heterogeneity management, real-time processing, and scalability. By bridging
theoretical advancements and practical deployments, this survey offers
researchers and practitioners a holistic perspective on leveraging AI to
optimize distributed computing environments, fostering innovation in
next-generation intelligent systems.

</details>

### [437] [Phantora: Live GPU Cluster Simulation for Machine Learning System Performance Estimation](https://arxiv.org/abs/2505.01616)
*Jianxing Qin,Jingrong Chen,Xinhao Kong,Yongji Wu,Liang Luo,Zhaodong Wang,Ying Zhang,Tingjun Chen,Alvin R. Lebeck,Danyang Zhuo*

Main category: cs.DC

TLDR: Phantora是一个实时GPU集群模拟器，用于性能估计，减少人工工作量并提高通用性。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习系统需要扩展到大型GPU集群，但现有性能估计方法需要大量人工和计算资源，且难以适应。

Method: Phantora通过拦截和模拟GPU相关操作，结合事件驱动网络模拟器与实时系统执行，提高模拟速度、可扩展性和准确性。

Result: 评估显示，Phantora仅需一个GPU即可达到与现有工作负载模拟方法相似的估计精度。

Conclusion: Phantora在减少人工工作量和提高通用性方面表现出色，为ML系统性能估计提供了高效解决方案。

Abstract: To accommodate ever-increasing model complexity, modern machine learning (ML)
systems have to scale to large GPU clusters. Changes in ML model architecture,
ML system implementation, and cluster configuration can significantly affect
overall ML system performance. However, quantifying the performance impact
before deployment is challenging. Existing performance estimation methods use
performance modeling or static workload simulation. These techniques are not
general: they requires significant human effort and computation capacity to
generate training data or a workload. It is also difficult to adapt ML systems
to use these techniques. This paper introduces, Phantora, a live GPU cluster
simulator for performance estimation. Phantora runs minimally modified ML
models and frameworks, intercepting and simulating GPU-related operations to
enable high-fidelity performance estimation. Phantora overcomes several
research challenges in integrating an event-driven network simulator with live
system execution, and introduces a set of techniques to improve simulation
speed, scalability, and accuracy. Our evaluation results show that Phantora can
deliver similar estimation accuracy to the state-of-the-art workload simulation
approach with only one GPU, while reducing human effort and increasing
generalizability.

</details>

### [438] [Large Language Model Partitioning for Low-Latency Inference at the Edge](https://arxiv.org/abs/2505.02533)
*Dimitrios Kafetzis,Ramin Khalili,Iordanis Koutsopoulos*

Main category: cs.DC

TLDR: 提出了一种资源感知的Transformer架构分区算法，通过动态调整分区决策以减少推理延迟和内存负载。


<details>
  <summary>Details</summary>
Motivation: 由于自回归LLMs生成文本时内存和计算负载随序列增长而增加，资源受限的边缘环境常面临内存过载或高延迟问题。

Method: 提出基于设备资源可用性和网络带宽的动态分区算法，按注意力头级别分区并允许动态迁移。

Result: 在小规模（3-5设备）测试中接近最优解，大规模测试中显著提升推理速度和内存效率。

Conclusion: 动态分区算法有效降低推理延迟，适用于资源受限环境。

Abstract: Large Language Models (LLMs) based on autoregressive, decoder-only
Transformers generate text one token at a time, where a token represents a
discrete unit of text. As each newly produced token is appended to the partial
output sequence, the length grows and so does the memory and compute load, due
to the expanding key-value caches, which store intermediate representations of
all previously generated tokens in the multi-head attention (MHA) layer. As
this iterative process steadily increases memory and compute demands,
layer-based partitioning in resource-constrained edge environments often
results in memory overload or high inference latency. To address this and
reduce inference latency, we propose a resource-aware Transformer architecture
partitioning algorithm, where the partitioning decision is updated at regular
intervals during token generation. The approach is myopic in that it is based
on instantaneous information about device resource availability and network
link bandwidths. When first executed, the algorithm places blocks on devices,
and in later executions, it migrates these blocks among devices so that the sum
of migration delay and inference delay remains low. Our approach partitions the
decoder at the attention head level, co-locating each attention head with its
key-value cache and allowing dynamic migrations whenever resources become
tight. By allocating different attention heads to different devices, we exploit
parallel execution of attention heads and thus achieve substantial reductions
in inference delays. Our experiments show that in small-scale settings (3-5
devices), the proposed method achieves within 15 to 20 percent of an exact
optimal solver's latency, while in larger-scale tests it achieves notable
improvements in inference speed and memory usage compared to state-of-the-art
layer-based partitioning approaches.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [439] [AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine](https://arxiv.org/abs/2505.01435)
*Carlo Siebenschuh,Kyle Hippe,Ozan Gokdemir,Alexander Brace,Arham Khan,Khalid Hossain,Yadu Babuji,Nicholas Chia,Venkatram Vishwanath,Rick Stevens,Arvind Ramanathan,Ian Foster,Robert Underwood*

Main category: cs.IR

TLDR: AdaParse 是一种自适应并行 PDF 解析引擎，通过数据驱动策略选择最佳解析器，结合人类偏好和硬件需求，显著提高解析效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 科学文献通常以 PDF 格式发布，解析这些文件需要选择适合的解析器，而现有方法在效率和准确性之间存在权衡。

Method: AdaParse 通过直接偏好优化（DPO）结合科学家选择的解析器输出，动态分配计算资源，优化解析过程。

Result: 在 1000 篇科学文档的基准测试中，AdaParse 的吞吐量提高了 17 倍，准确性略优于现有方法（0.2%）。

Conclusion: AdaParse 的高效性和可扩展性使其适用于大规模科学文献解析，支持高质量文本数据集的开发。

Abstract: Language models for scientific tasks are trained on text from scientific
publications, most distributed as PDFs that require parsing. PDF parsing
approaches range from inexpensive heuristics (for simple documents) to
computationally intensive ML-driven systems (for complex or degraded ones). The
choice of the "best" parser for a particular document depends on its
computational cost and the accuracy of its output. To address these issues, we
introduce an Adaptive Parallel PDF Parsing and Resource Scaling Engine
(AdaParse), a data-driven strategy for assigning an appropriate parser to each
document. We enlist scientists to select preferred parser outputs and
incorporate this information through direct preference optimization (DPO) into
AdaParse, thereby aligning its selection process with human judgment. AdaParse
then incorporates hardware requirements and predicted accuracy of each parser
to orchestrate computational resources efficiently for large-scale parsing
campaigns. We demonstrate that AdaParse, when compared to state-of-the-art
parsers, improves throughput by $17\times$ while still achieving comparable
accuracy (0.2 percent better) on a benchmark set of 1000 scientific documents.
AdaParse's combination of high accuracy and parallel scalability makes it
feasible to parse large-scale scientific document corpora to support the
development of high-quality, trillion-token-scale text datasets. The
implementation is available at https://github.com/7shoe/AdaParse/

</details>

### [440] [Exploring new Approaches for Information Retrieval through Natural Language Processing](https://arxiv.org/abs/2505.02199)
*Manak Raj,Nidhi Mishra*

Main category: cs.IR

TLDR: 综述探讨了信息检索（IR）在自然语言处理（NLP）中的最新进展，包括传统与现代方法、工具及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 总结IR在NLP中的发展，比较不同方法，并指出未来研究方向。

Method: 分析传统IR模型（如布尔、向量空间模型）和现代技术（如深度学习、BERT），介绍工具（如Lucene）及比较稀疏、密集和混合检索方法。

Result: 展示了IR在多个应用领域的潜力，并总结了现有方法的优缺点。

Conclusion: 提出了提升检索准确性、可扩展性和伦理考量的未来研究方向。

Abstract: This review paper explores recent advancements and emerging approaches in
Information Retrieval (IR) applied to Natural Language Processing (NLP). We
examine traditional IR models such as Boolean, vector space, probabilistic, and
inference network models, and highlight modern techniques including deep
learning, reinforcement learning, and pretrained transformer models like BERT.
We discuss key tools and libraries - Lucene, Anserini, and Pyserini - for
efficient text indexing and search. A comparative analysis of sparse, dense,
and hybrid retrieval methods is presented, along with applications in web
search engines, cross-language IR, argument mining, private information
retrieval, and hate speech detection. Finally, we identify open challenges and
future research directions to enhance retrieval accuracy, scalability, and
ethical considerations.

</details>

### [441] [Predicting Movie Hits Before They Happen with LLMs](https://arxiv.org/abs/2505.02693)
*Shaghayegh Agah,Yejin Kim,Neeraj Sharma,Mayur Nankani,Kevin Foley,H. Howie Huang,Sardar Hamidian*

Main category: cs.IR

TLDR: 利用大型语言模型（LLMs）预测冷启动电影的热度，解决内容推荐中的冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 解决娱乐平台上冷启动电影的热度预测问题，以优化推荐系统和编辑团队的推广策略。

Method: 利用电影元数据和大型语言模型（LLMs）进行热度预测。

Result: 该方法在实验中表现优于传统基线方法和自开发方法。

Conclusion: LLMs在冷启动电影热度预测中具有有效性，可集成到推荐系统或编辑工具中。

Abstract: Addressing the cold-start issue in content recommendation remains a critical
ongoing challenge. In this work, we focus on tackling the cold-start problem
for movies on a large entertainment platform. Our primary goal is to forecast
the popularity of cold-start movies using Large Language Models (LLMs)
leveraging movie metadata. This method could be integrated into retrieval
systems within the personalization pipeline or could be adopted as a tool for
editorial teams to ensure fair promotion of potentially overlooked movies that
may be missed by traditional or algorithmic solutions. Our study validates the
effectiveness of this approach compared to established baselines and those we
developed.

</details>

### [442] [A Multi-Granularity Multimodal Retrieval Framework for Multimodal Document Tasks](https://arxiv.org/abs/2505.01457)
*Mingjun Xu,Zehui Wang,Hengxing Cai,Renxin Zhong*

Main category: cs.IR

TLDR: 提出了一种统一的多粒度多模态检索框架，用于处理视觉丰富的文档，通过分层编码和模态感知检索提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成系统主要基于文本检索，难以有效处理包含文本、图像、表格和图表的视觉丰富文档。

Method: 采用分层编码策略、模态感知检索机制和重排序模块，结合现成的视觉语言模型和免训练混合检索策略。

Result: 实验表明，布局感知搜索和重排序模块显著提升检索准确性，最高得分达65.56。

Conclusion: 该框架展示了可扩展和可复用的解决方案在多模态文档检索系统中的潜力。

Abstract: Retrieval-augmented generation (RAG) systems have predominantly focused on
text-based retrieval, limiting their effectiveness in handling visually-rich
documents that encompass text, images, tables, and charts. To bridge this gap,
we propose a unified multi-granularity multimodal retrieval framework tailored
for two benchmark tasks: MMDocIR and M2KR. Our approach integrates hierarchical
encoding strategies, modality-aware retrieval mechanisms, and reranking modules
to effectively capture and utilize the complex interdependencies between
textual and visual modalities. By leveraging off-the-shelf vision-language
models and implementing a training-free hybridretrieval strategy, our framework
demonstrates robust performance without the need for task-specific fine-tuning.
Experimental evaluations reveal that incorporating layout-aware search and
reranking modules significantly enhances retrieval accuracy, achieving a top
performance score of 65.56. This work underscores the potential of scalable and
reproducible solutions in advancing multimodal document retrieval systems.

</details>

### [443] [RAGAR: Retrieval Augment Personalized Image Generation Guided by Recommendation](https://arxiv.org/abs/2505.01657)
*Run Ling,Wenji Wang,Yuting Liu,Guibing Guo,Linying Jiang,Xingwei Wang*

Main category: cs.IR

TLDR: 论文提出RAGAR方法，通过检索机制和排名任务改进个性化图像生成，解决了现有方法忽视语义相似性和过度依赖一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 提升个性化图像生成的用户体验，解决现有方法忽视历史项语义相似性和过度依赖一致性的问题。

Method: 使用检索机制为历史项分配权重，引入多模态排名任务优化个性化生成。

Result: 在三个真实数据集上，RAGAR在个性化和语义指标上显著优于五个基线方法。

Conclusion: RAGAR通过改进权重分配和优化策略，显著提升了图像生成的个性化和语义质量。

Abstract: Personalized image generation is crucial for improving the user experience,
as it renders reference images into preferred ones according to user visual
preferences. Although effective, existing methods face two main issues. First,
existing methods treat all items in the user historical sequence equally when
extracting user preferences, overlooking the varying semantic similarities
between historical items and the reference item. Disproportionately high
weights for low-similarity items distort users' visual preferences for the
reference item. Second, existing methods heavily rely on consistency between
generated and reference images to optimize the generation, which leads to
underfitting user preferences and hinders personalization. To address these
issues, we propose Retrieval Augment Personalized Image GenerAtion guided by
Recommendation (RAGAR). Our approach uses a retrieval mechanism to assign
different weights to historical items according to their similarities to the
reference item, thereby extracting more refined users' visual preferences for
the reference item. Then we introduce a novel rank task based on the
multi-modal ranking model to optimize the personalization of the generated
images instead of forcing depend on consistency. Extensive experiments and
human evaluations on three real-world datasets demonstrate that RAGAR achieves
significant improvements in both personalization and semantic metrics compared
to five baselines.

</details>

### [444] [Tricolore: Multi-Behavior User Profiling for Enhanced Candidate Generation in Recommender Systems](https://arxiv.org/abs/2505.02120)
*Xiao Zhou,Zhongxiang Zhao,Hanze Guo*

Main category: cs.IR

TLDR: Tricolore是一个多向量学习框架，通过自适应多任务结构和行为多视图融合模块，解决了传统推荐系统单目标优化和用户兴趣捕捉不足的问题，提升了推荐多样性和冷启动用户性能。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统通常针对单一行为优化，且用户偏好表示单一，无法处理多行为或多目标优化，导致推荐结果狭窄。

Method: 提出Tricolore框架，采用多向量学习、自适应多任务结构和行为多视图融合模块，结合流行度平衡策略。

Result: 在公开数据集上验证了Tricolore的广泛适用性，显著提升了推荐多样性和冷启动用户性能。

Conclusion: Tricolore通过多行为建模和动态学习增强，为推荐系统提供了更灵活和高效的解决方案。

Abstract: Online platforms aggregate extensive user feedback across diverse behaviors,
providing a rich source for enhancing user engagement. Traditional recommender
systems, however, typically optimize for a single target behavior and represent
user preferences with a single vector, limiting their ability to handle
multiple important behaviors or optimization objectives. This conventional
approach also struggles to capture the full spectrum of user interests,
resulting in a narrow item pool during candidate generation. To address these
limitations, we present Tricolore, a versatile multi-vector learning framework
that uncovers connections between different behavior types for more robust
candidate generation. Tricolore's adaptive multi-task structure is also
customizable to specific platform needs. To manage the variability in sparsity
across behavior types, we incorporate a behavior-wise multi-view fusion module
that dynamically enhances learning. Moreover, a popularity-balanced strategy
ensures the recommendation list balances accuracy with item popularity,
fostering diversity and improving overall performance. Extensive experiments on
public datasets demonstrate Tricolore's effectiveness across various
recommendation scenarios, from short video platforms to e-commerce. By
leveraging a shared base embedding strategy, Tricolore also significantly
improves the performance for cold-start users. The source code is publicly
available at: https://github.com/abnering/Tricolore.

</details>

### [445] [Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models through Axiomatic Causal Interventions](https://arxiv.org/abs/2505.02154)
*Oliver Savolainen,Dur e Najaf Amjad,Roxana Petcu*

Main category: cs.IR

TLDR: 该可重复性研究分析了神经检索模型中任务相关属性（如词频）的编码方式，并扩展了原论文的实验，验证了激活修补方法在多语言数据集中的适用性。


<details>
  <summary>Details</summary>
Motivation: 研究神经检索模型如何编码任务相关属性（如词频和文档长度），并验证其跨语言的通用性。

Method: 通过激活修补技术分析神经检索模型的编码行为，并在西班牙语和中文数据集上进行实验。

Result: 验证了词频信息在多语言中的通用性，并发现序列级任务信息在CLS令牌中编码。

Conclusion: 研究强调了信息检索可解释性和机器学习研究可重复性的重要性。

Abstract: This reproducibility study analyzes and extends the paper "Axiomatic Causal
Interventions for Reverse Engineering Relevance Computation in Neural Retrieval
Models," which investigates how neural retrieval models encode task-relevant
properties such as term frequency. We reproduce key experiments from the
original paper, confirming that information on query terms is captured in the
model encoding. We extend this work by applying activation patching to Spanish
and Chinese datasets and by exploring whether document-length information is
encoded in the model as well. Our results confirm that the designed activation
patching method can isolate the behavior to specific components and tokens in
neural retrieval models. Moreover, our findings indicate that the location of
term frequency generalizes across languages and that in later layers, the
information for sequence-level tasks is represented in the CLS token. The
results highlight the need for further research into interpretability in
information retrieval and reproducibility in machine learning research. Our
code is available at
https://github.com/OliverSavolainen/axiomatic-ir-reproduce.

</details>

### [446] [Social Biases in Knowledge Representations of Wikidata separates Global North from Global South](https://arxiv.org/abs/2505.02352)
*Paramita Das,Sai Keerthana Karnam,Aditya Soni,Animesh Mukherjee*

Main category: cs.IR

TLDR: 论文研究了知识图谱中的社会偏见问题，提出了一个框架AuditLP，用于评估链接预测任务中的公平性，发现地理差异反映了社会经济和文化差异。


<details>
  <summary>Details</summary>
Motivation: 知识图谱广泛应用于下游任务，但其自动构建过程中可能存在社会偏见，导致对少数群体的不公平影响。

Method: 开发了AuditLP框架，使用公平性指标评估链接预测中的偏见，重点关注性别和年龄作为敏感属性。

Result: 实验发现职业分类存在性别和年龄偏见，且地理差异与全球社会经济和文化划分一致。

Conclusion: 研究揭示了知识图谱中的社会偏见问题，强调了公平性评估的重要性，并展示了地理差异对偏见的影响。

Abstract: Knowledge Graphs have become increasingly popular due to their wide usage in
various downstream applications, including information retrieval, chatbot
development, language model construction, and many others. Link prediction (LP)
is a crucial downstream task for knowledge graphs, as it helps to address the
problem of the incompleteness of the knowledge graphs. However, previous
research has shown that knowledge graphs, often created in a (semi) automatic
manner, are not free from social biases. These biases can have harmful effects
on downstream applications, especially by leading to unfair behavior toward
minority groups. To understand this issue in detail, we develop a framework --
AuditLP -- deploying fairness metrics to identify biased outcomes in LP,
specifically how occupations are classified as either male or female-dominated
based on gender as a sensitive attribute. We have experimented with the
sensitive attribute of age and observed that occupations are categorized as
young-biased, old-biased, and age-neutral. We conduct our experiments on a
large number of knowledge triples that belong to 21 different geographies
extracted from the open-sourced knowledge graph, Wikidata. Our study shows that
the variance in the biased outcomes across geographies neatly mirrors the
socio-economic and cultural division of the world, resulting in a transparent
partition of the Global North from the Global South.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [447] [Emotions in the Loop: A Survey of Affective Computing for Emotional Support](https://arxiv.org/abs/2505.01542)
*Karishma Hegde,Hemadri Jayalath*

Main category: cs.HC

TLDR: 本文综述了情感计算领域的最新研究，涵盖情感识别、情感分析和人格分配等应用，分析了AI聊天机器人、多模态输入系统、心理健康治疗和安全应用等四个领域的研究贡献、方法及挑战。


<details>
  <summary>Details</summary>
Motivation: 探索情感计算在提升人机交互中的作用，通过机器处理用户情感实现更人性化的数字交互。

Method: 采用文献综述方法，分析基于大语言模型、多模态技术和个性化AI系统的研究，并分类为四个应用领域。

Result: 总结了情感计算的技术优势、研究空白和挑战，并评估了数据集对模型性能的影响。

Conclusion: 提出未来发展方向和伦理考量，以开发更安全、共情和实用的情感计算应用。

Abstract: In a world where technology is increasingly embedded in our everyday
experiences, systems that sense and respond to human emotions are elevating
digital interaction. At the intersection of artificial intelligence and
human-computer interaction, affective computing is emerging with innovative
solutions where machines are humanized by enabling them to process and respond
to user emotions. This survey paper explores recent research contributions in
affective computing applications in the area of emotion recognition, sentiment
analysis and personality assignment developed using approaches like large
language models (LLMs), multimodal techniques, and personalized AI systems. We
analyze the key contributions and innovative methodologies applied by the
selected research papers by categorizing them into four domains: AI chatbot
applications, multimodal input systems, mental health and therapy applications,
and affective computing for safety applications. We then highlight the
technological strengths as well as the research gaps and challenges related to
these studies. Furthermore, the paper examines the datasets used in each study,
highlighting how modality, scale, and diversity impact the development and
performance of affective models. Finally, the survey outlines ethical
considerations and proposes future directions to develop applications that are
more safe, empathetic and practical.

</details>

### [448] [The GenAI Generation: Student Views of Awareness, Preparedness, and Concern](https://arxiv.org/abs/2505.02230)
*Micaela Siraj,Jon Duke*

Main category: cs.HC

TLDR: 研究探讨了学生对生成式AI（GenAI）的认知、准备和担忧，发现多数学生对其持乐观态度，但也普遍关注伦理、职业替代和教育结构问题。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI在教育和工作领域的广泛应用，了解学生对其的看法和担忧，以指导教育机构适应这一变革技术。

Method: 通过包含可选开放式问题的简明调查，收集了250多份学生反馈，其中40%提供了详细的定性数据。

Result: 学生普遍对GenAI持热情态度，但更关注其伦理、职业替代和教育结构适应性问题。

Conclusion: 研究结果为教育机构提供了关键见解，建议其采取措施应对GenAI驱动的未来。

Abstract: Generative AI (GenAI) is revolutionizing education and workforce development,
profoundly shaping how students learn, engage, and prepare for their future.
Outpacing the development of uniform policies and structures, GenAI has
heralded a unique era and given rise to the GenAI Generation: a cohort of
students whose education has been increasingly shaped by the opportunities and
challenges GenAI presents during its widespread adoption within society. This
study examines our students' perceptions of GenAI through a concise survey with
optional open-ended questions, focusing on their awareness, preparedness, and
concerns. Evaluation of more than 250 responses with more than 40% providing
detailed qualitative feedback reveals a core dual sentiment: while most
students express enthusiasm for GenAI, an even greater proportion voice a
spectrum of concerns about ethics, job displacement, and the adequacy of
educational structures given the highly transformative technology. These
findings offer critical insights into how students view the potential and
pitfalls of GenAI for future career impacts, with accompanying recommendations
to guide educational institutions in navigating a future driven by GenAI.

</details>

### [449] [Eye Movements as Indicators of Deception: A Machine Learning Approach](https://arxiv.org/abs/2505.02649)
*Valentin Foucher,Santiago de Leon-Martinez,Robert Moro*

Main category: cs.HC

TLDR: 研究探讨了AI模型利用凝视数据（注视、扫视、眨眼和瞳孔大小）在隐藏信息测试中检测欺骗的效果，结果显示XGBoost在二元分类任务中准确率达74%，三分类任务中为49%。


<details>
  <summary>Details</summary>
Motivation: 凝视可能增强测谎器的鲁棒性，但相关研究不足，因此本研究旨在评估AI模型在检测欺骗中的效果。

Method: 使用两个数据集（Eyelink 1000和Pupil Neon），分别记录了87名和36名参与者的凝视数据，并通过XGBoost模型进行分类任务。

Result: XGBoost在二元分类任务中准确率达74%，三分类任务中为49%；扫视次数、持续时间、幅度和最大瞳孔大小是预测欺骗的关键特征。

Conclusion: 研究证明了利用凝视数据和AI增强测谎器的可行性，并鼓励未来进一步改进。

Abstract: Gaze may enhance the robustness of lie detectors but remains under-studied.
This study evaluated the efficacy of AI models (using fixations, saccades,
blinks, and pupil size) for detecting deception in Concealed Information Tests
across two datasets. The first, collected with Eyelink 1000, contains gaze data
from a computerized experiment where 87 participants revealed, concealed, or
faked the value of a previously selected card. The second, collected with Pupil
Neon, involved 36 participants performing a similar task but facing an
experimenter. XGBoost achieved accuracies up to 74% in a binary classification
task (Revealing vs. Concealing) and 49% in a more challenging
three-classification task (Revealing vs. Concealing vs. Faking). Feature
analysis identified saccade number, duration, amplitude, and maximum pupil size
as the most important for deception prediction. These results demonstrate the
feasibility of using gaze and AI to enhance lie detectors and encourage future
research that may improve on this.

</details>

### [450] [AI Standardized Patient Improves Human Conversations in Advanced Cancer Care](https://arxiv.org/abs/2505.02694)
*Kurtis Haut,Masum Hasan,Thomas Carroll,Ronald Epstein,Taylan Sen,Ehsan Hoque*

Main category: cs.HC

TLDR: SOPHIE是一个基于AI的标准化患者模拟系统，用于提升临终关怀中的严重疾病沟通技能，解决了传统方法的高成本和低灵活性问题。


<details>
  <summary>Details</summary>
Motivation: 传统标准化患者培训昂贵、耗时且不灵活，无法满足严重疾病沟通（SIC）的培训需求。

Method: 结合大型语言模型（LLMs）、虚拟化身和自动化反馈系统，提供远程、按需的SIC培训。

Result: 随机对照试验显示，用户在三个关键SIC领域（共情、明确表达和赋权）有显著提升。

Conclusion: AI工具可有效提升复杂人际沟通技能，为临床教育提供可扩展的解决方案。

Abstract: Serious illness communication (SIC) in end-of-life care faces challenges such
as emotional stress, cultural barriers, and balancing hope with honesty.
Despite its importance, one of the few available ways for clinicians to
practice SIC is with standardized patients, which is expensive, time-consuming,
and inflexible. In this paper, we present SOPHIE, an AI-powered standardized
patient simulation and automated feedback system. SOPHIE combines large
language models (LLMs), a lifelike virtual avatar, and automated, personalized
feedback based on clinical literature to provide remote, on-demand SIC
training. In a randomized control study with healthcare students and
professionals, SOPHIE users demonstrated significant improvement across three
critical SIC domains: Empathize, Be Explicit, and Empower. These results
suggest that AI-driven tools can enhance complex interpersonal communication
skills, offering scalable, accessible solutions to address a critical gap in
clinician education.

</details>

### [451] [Beyond the Monitor: Mixed Reality Visualization and AI for Enhanced Digital Pathology Workflow](https://arxiv.org/abs/2505.02780)
*Jai Prakash Veerla,Partha Sai Guttikonda,Helen H. Shang,Mohammad Sadegh Nasr,Cesar Torres,Jacob M. Luber*

Main category: cs.HC

TLDR: PathVis是一个混合现实可视化平台，旨在解决数字病理学中因大尺寸全切片图像（WSIs）导致的诊断疲劳和效率低下问题，通过自然交互和AI增强诊断。


<details>
  <summary>Details</summary>
Motivation: 当前数字病理学工具因WSIs的巨大尺寸与传统显示器的限制，导致病理学家需要频繁缩放和移动图像，增加了认知负担和疲劳，阻碍了数字方法的普及。

Method: PathVis利用Apple Vision Pro的混合现实技术，提供直观的手势、视线和语音交互，并集成AI功能，如相似病例搜索和多模态对话助手。

Result: PathVis改善了诊断流程，降低了认知压力，提高了诊断效率和协作能力。

Conclusion: PathVis通过结合传统病理学的直接性与混合现实和AI技术，使病理学实践更高效和吸引人。

Abstract: Pathologists rely on gigapixel whole-slide images (WSIs) to diagnose diseases
like cancer, yet current digital pathology tools hinder diagnosis. The immense
scale of WSIs, often exceeding 100,000 X 100,000 pixels, clashes with the
limited views traditional monitors offer. This mismatch forces constant panning
and zooming, increasing pathologist cognitive load, causing diagnostic fatigue,
and slowing pathologists' adoption of digital methods. PathVis, our
mixed-reality visualization platform for Apple Vision Pro, addresses these
challenges. It transforms the pathologist's interaction with data, replacing
cumbersome mouse-and-monitor navigation with intuitive exploration using
natural hand gestures, eye gaze, and voice commands in an immersive workspace.
PathVis integrates AI to enhance diagnosis. An AI-driven search function
instantly retrieves and displays the top five similar patient cases
side-by-side, improving diagnostic precision and efficiency through rapid
comparison. Additionally, a multimodal conversational AI assistant offers
real-time image interpretation support and aids collaboration among
pathologists across multiple Apple devices. By merging the directness of
traditional pathology with advanced mixed-reality visualization and AI, PathVis
improves diagnostic workflows, reduces cognitive strain, and makes pathology
practice more effective and engaging. The PathVis source code and a demo video
are publicly available at: https://github.com/jaiprakash1824/Path_Vis

</details>

<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [452] [Enhancing TCR-Peptide Interaction Prediction with Pretrained Language Models and Molecular Representations](https://arxiv.org/abs/2505.01433)
*Cong Qi,Hanzhang Fang,Siqi jiang,Tianxing Hu,Wei Zhi*

Main category: q-bio.QM

TLDR: LANTERN是一个结合蛋白质语言模型和肽化学表示的深度学习框架，用于预测TCR与pMHC的结合特异性，在零样本和少样本学习中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前预测模型在数据稀缺和新表位情况下泛化能力不足，限制了免疫疗法和疫苗开发。

Method: 使用ESM-1b编码TCR序列，MolFormer处理肽的SMILES字符串，结合生物和化学特征。

Result: 在基准测试中优于现有模型，尤其在零样本和少样本学习中，嵌入分析显示显著聚类改进。

Conclusion: LANTERN有望推动TCR-pMHC结合预测，支持个性化免疫疗法的发展。

Abstract: Understanding the binding specificity between T-cell receptors (TCRs) and
peptide-major histocompatibility complexes (pMHCs) is central to immunotherapy
and vaccine development. However, current predictive models struggle with
generalization, especially in data-scarce settings and when faced with novel
epitopes. We present LANTERN (Large lAnguage model-powered TCR-Enhanced
Recognition Network), a deep learning framework that combines large-scale
protein language models with chemical representations of peptides. By encoding
TCR \b{eta}-chain sequences using ESM-1b and transforming peptide sequences
into SMILES strings processed by MolFormer, LANTERN captures rich biological
and chemical features critical for TCR-peptide recognition. Through extensive
benchmarking against existing models such as ChemBERTa, TITAN, and NetTCR,
LANTERN demonstrates superior performance, particularly in zero-shot and
few-shot learning scenarios. Our model also benefits from a robust negative
sampling strategy and shows significant clustering improvements via embedding
analysis. These results highlight the potential of LANTERN to advance TCR-pMHC
binding prediction and support the development of personalized immunotherapies.

</details>

<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [453] [Explainability by design: an experimental analysis of the legal coding process](https://arxiv.org/abs/2505.01944)
*Matteo Cristani,Guido Governatori,Francesco Olivieri,Monica Palmirani,Gabriele Buriola*

Main category: cs.LO

TLDR: 论文提出了一种法律编码方法，将文本片段转化为Deontic Defeasible Logic规则，并通过实验验证了编码过程的效率和影响因素。


<details>
  <summary>Details</summary>
Motivation: 研究法律编码过程中从文本到逻辑规则的映射，并量化编码效率与文本特征的关系。

Method: 采用Deontic Defeasible Logic规则和Houdini技术，结合场景测试和人类实验，分析编码过程。

Result: 实验测量了编码努力与文本特征（如长度、深度）的关系，并提出了时间预测技术。

Conclusion: 该方法为法律编码提供了可量化的工具，并能预测编码时间，提升效率。

Abstract: Behind a set of rules in Deontic Defeasible Logic, there is a mapping process
of normative background fragments. This process goes from text to rules and
implicitly encompasses an explanation of the coded fragments.
  In this paper we deliver a methodology for \textit{legal coding} that starts
with a fragment and goes onto a set of Deontic Defeasible Logic rules,
involving a set of \textit{scenarios} to test the correctness of the coded
fragments. The methodology is illustrated by the coding process of an example
text. We then show the results of a series of experiments conducted with humans
encoding a variety of normative backgrounds and corresponding cases in which we
have measured the efforts made in the coding process, as related to some
measurable features. To process these examples, a recently developed
technology, Houdini, that allows reasoning in Deontic Defeasible Logic, has
been employed.
  Finally we provide a technique to forecast time required in coding, that
depends on factors such as knowledge of the legal domain, knowledge of the
coding processes, length of the text, and a measure of \textit{depth} that
refers to the length of the paths of legal references.

</details>

<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [454] [Latent Variable Estimation in Bayesian Black-Litterman Models](https://arxiv.org/abs/2505.02185)
*Thomas Y. L. Lin,Jerry Yao-Chieh Hu,Paul W. Chiou,Peter Lin*

Main category: q-fin.PM

TLDR: N/A


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We revisit the Bayesian Black-Litterman (BL) portfolio model and remove its
reliance on subjective investor views. Classical BL requires an investor
"view": a forecast vector $q$ and its uncertainty matrix $\Omega$ that describe
how much a chosen portfolio should outperform the market. Our key idea is to
treat $(q,\Omega)$ as latent variables and learn them from market data within a
single Bayesian network. Consequently, the resulting posterior estimation
admits closed-form expression, enabling fast inference and stable portfolio
weights. Building on these, we propose two mechanisms to capture how features
interact with returns: shared-latent parametrization and feature-influenced
views; both recover classical BL and Markowitz portfolios as special cases.
Empirically, on 30-year Dow-Jones and 20-year sector-ETF data, we improve
Sharpe ratios by 50% and cut turnover by 55% relative to Markowitz and the
index baselines. This work turns BL into a fully data-driven, view-free, and
coherent Bayesian framework for portfolio optimization.

</details>

<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [455] [Transfer Learning-Based Deep Residual Learning for Speech Recognition in Clean and Noisy Environments](https://arxiv.org/abs/2505.01632)
*Noussaiba Djeffal,Djamel Addou,Hamza Kheddar,Sid Ahmed Selouani*

Main category: eess.AS

TLDR: 论文提出了一种新的神经网络框架，结合鲁棒前端到ASR系统中，显著提升了在干净和嘈杂环境下的语音识别准确率。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳环境噪声对自动语音识别（ASR）的负面影响是一个长期且重要的研究方向。

Method: 采用基于残差神经网络（ResNet）的迁移学习方法，结合Mel频率声学特征集，在Aurora-2语音数据库上进行评估。

Result: 实验结果显示，与CNN和LSTM网络相比，识别准确率显著提升，干净环境下达到98.94%，嘈杂环境下为91.21%。

Conclusion: 提出的神经框架在ASR系统中表现出优越性能，尤其在复杂声学环境中具有潜力。

Abstract: Addressing the detrimental impact of non-stationary environmental noise on
automatic speech recognition (ASR) has been a persistent and significant
research focus. Despite advancements, this challenge continues to be a major
concern. Recently, data-driven supervised approaches, such as deep neural
networks, have emerged as promising alternatives to traditional unsupervised
methods. With extensive training, these approaches have the potential to
overcome the challenges posed by diverse real-life acoustic environments. In
this light, this paper introduces a novel neural framework that incorporates a
robust frontend into ASR systems in both clean and noisy environments.
Utilizing the Aurora-2 speech database, the authors evaluate the effectiveness
of an acoustic feature set for Mel-frequency, employing the approach of
transfer learning based on Residual neural network (ResNet). The experimental
results demonstrate a significant improvement in recognition accuracy compared
to convolutional neural networks (CNN) and long short-term memory (LSTM)
networks. They achieved accuracies of 98.94% in clean and 91.21% in noisy mode.

</details>

<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [456] [Safe and Efficient CAV Lane Changing using Decentralised Safety Shields](https://arxiv.org/abs/2505.01453)
*Bharathkumar Hegde,Melanie Bouroche*

Main category: cs.MA

TLDR: 论文提出了一种结合优化和规则的去中心化混合安全防护（HSS）方法，用于确保CAV在换道时的安全性，并与MARL结合（MARL-HSS）以提高交通效率。


<details>
  <summary>Details</summary>
Motivation: 解决CAV在换道决策中如何平衡交通效率与安全性的问题，尤其是在使用MARL训练控制器时难以确保安全的情况。

Method: 提出HSS方法，利用控制屏障函数约束CAV的纵向和横向控制输入，确保安全操作，并与MARL结合形成MARL-HSS架构。

Result: 在模拟的匝道合并场景中，HSS能严格保证动态安全约束，MARL-HSS在轻度和中度交通密度下均实现零事故和可比的平均速度。

Conclusion: MARL-HSS成功平衡了安全性与交通效率，优于无安全防护的MARL基线方法。

Abstract: Lane changing is a complex decision-making problem for Connected and
Autonomous Vehicles (CAVs) as it requires balancing traffic efficiency with
safety. Although traffic efficiency can be improved by using vehicular
communication for training lane change controllers using Multi-Agent
Reinforcement Learning (MARL), ensuring safety is difficult. To address this
issue, we propose a decentralised Hybrid Safety Shield (HSS) that combines
optimisation and a rule-based approach to guarantee safety. Our method applies
control barrier functions to constrain longitudinal and lateral control inputs
of a CAV to ensure safe manoeuvres. Additionally, we present an architecture to
integrate HSS with MARL, called MARL-HSS, to improve traffic efficiency while
ensuring safety. We evaluate MARL-HSS using a gym-like environment that
simulates an on-ramp merging scenario with two levels of traffic densities,
such as light and moderate densities. The results show that HSS provides a
safety guarantee by strictly enforcing a dynamic safety constraint defined on a
time headway, even in moderate traffic density that offers challenging lane
change scenarios. Moreover, the proposed method learns stable policies compared
to the baseline, a state-of-the-art MARL lane change controller without a
safety shield. Further policy evaluation shows that our method achieves a
balance between safety and traffic efficiency with zero crashes and comparable
average speeds in light and moderate traffic densities.

</details>

<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [457] [An Adaptive Framework for Autoregressive Forecasting in CFD Using Hybrid Modal Decomposition and Deep Learning](https://arxiv.org/abs/2505.01531)
*Rodrigo Abadía-Heredia,Manuel Lopez-Martin,Soledad Le Clainche*

Main category: physics.flu-dyn

TLDR: 提出首个通用、数据驱动的自适应框架，用于稳定深度学习自回归预测模型，显著降低CFD模拟的计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习自回归模型在长时间预测中的稳定性问题，减少计算流体动力学（CFD）模拟的高计算成本。

Method: 交替进行两个阶段：(i) 使用训练好的DL模型预测流场演化，(ii) 在稳定性下降时用新生成的CFD数据更新模型。

Result: 在从层流到湍流的三种复杂流态中验证，计算成本降低30%至95%，同时保持物理一致性和准确性。

Conclusion: 该框架完全数据驱动，易于适应多种时间依赖的模拟问题，代码已开源并即将集成到ModelFLOWs-app中。

Abstract: This work presents, to the best of the authors' knowledge, the first
generalizable and fully data-driven adaptive framework designed to stabilize
deep learning (DL) autoregressive forecasting models over long time horizons,
with the goal of reducing the computational cost required in computational
fluid dynamics (CFD) simulations.The proposed methodology alternates between
two phases: (i) predicting the evolution of the flow field over a selected time
interval using a trained DL model, and (ii) updating the model with newly
generated CFD data when stability degrades, thus maintaining accurate long-term
forecasting. This adaptive retraining strategy ensures robustness while
avoiding the accumulation of predictive errors typical in autoregressive
models. The framework is validated across three increasingly complex flow
regimes, from laminar to turbulent, demonstrating from 30 \% to 95 \% reduction
in computational cost without compromising physical consistency or accuracy.
Its entirely data-driven nature makes it easily adaptable to a wide range of
time-dependent simulation problems. The code implementing this methodology is
available as open-source and it will be integrated into the upcoming release of
the ModelFLOWs-app.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [458] [OT-Talk: Animating 3D Talking Head with Optimal Transportation](https://arxiv.org/abs/2505.01932)
*Xinmu Wang,Xiang Gao,Xiyun Song,Heather Yu,Zongfang Lin,Liang Peng,Xianfeng Gu*

Main category: cs.GR

TLDR: OT-Talk利用最优传输理论优化3D头部网格动画，通过Chebyshev图卷积提取几何特征，结合Wasserstein距离建模网格变化，实现更自然的面部动画。


<details>
  <summary>Details</summary>
Motivation: 解决语音信号与面部动态之间的模态差异问题，避免错误的唇同步和不自然的面部运动。

Method: 结合预训练的Hubert模型提取音频特征，使用Transformer处理时序数据，引入Chebyshev图卷积提取网格几何特征，并利用Wasserstein距离建模网格变化。

Result: 在两个公开数据集上，OT-Talk在网格重建精度和时间对齐方面优于现有技术，用户感知研究也验证了其有效性。

Conclusion: OT-Talk通过新颖的几何特征提取和网格建模方法，显著提升了3D头部动画的自然性和准确性。

Abstract: Animating 3D head meshes using audio inputs has significant applications in
AR/VR, gaming, and entertainment through 3D avatars. However, bridging the
modality gap between speech signals and facial dynamics remains a challenge,
often resulting in incorrect lip syncing and unnatural facial movements. To
address this, we propose OT-Talk, the first approach to leverage optimal
transportation to optimize the learning model in talking head animation.
Building on existing learning frameworks, we utilize a pre-trained Hubert model
to extract audio features and a transformer model to process temporal
sequences. Unlike previous methods that focus solely on vertex coordinates or
displacements, we introduce Chebyshev Graph Convolution to extract geometric
features from triangulated meshes. To measure mesh dissimilarities, we go
beyond traditional mesh reconstruction errors and velocity differences between
adjacent frames. Instead, we represent meshes as probability measures and
approximate their surfaces. This allows us to leverage the sliced Wasserstein
distance for modeling mesh variations. This approach facilitates the learning
of smooth and accurate facial motions, resulting in coherent and natural facial
animations. Our experiments on two public audio-mesh datasets demonstrate that
our method outperforms state-of-the-art techniques both quantitatively and
qualitatively in terms of mesh reconstruction accuracy and temporal alignment.
In addition, we conducted a user perception study with 20 volunteers to further
assess the effectiveness of our approach.

</details>

### [459] [Discrete Spatial Diffusion: Intensity-Preserving Diffusion Modeling](https://arxiv.org/abs/2505.01917)
*Javier E. Santos,Agnese Marcato,Roman Colman,Nicholas Lubbers,Yen Ting Lin*

Main category: cs.GR

TLDR: 论文提出了一种离散空间扩散（DSD）框架，解决了生成扩散模型在离散空间（如科学数据）中应用的限制，严格保持质量守恒。


<details>
  <summary>Details</summary>
Motivation: 生成扩散模型在连续强度空间中表现优异，但不适用于离散量（如粒子计数或材料单位）且需质量守恒的科学应用。

Method: DSD基于连续时间、离散状态的跳跃随机过程，直接在离散空间操作，严格保持质量守恒。

Result: DSD在图像合成、类条件生成和图像修复中表现灵活，并能应用于材料微观结构等科学数据。

Conclusion: DSD填补了扩散模型与质量守恒科学应用之间的空白，具有广泛适用性。

Abstract: Generative diffusion models have achieved remarkable success in producing
high-quality images. However, because these models typically operate in
continuous intensity spaces - diffusing independently per pixel and color
channel - they are fundamentally ill-suited for applications where quantities
such as particle counts or material units are inherently discrete and governed
by strict conservation laws such as mass preservation, limiting their
applicability in scientific workflows. To address this limitation, we propose
Discrete Spatial Diffusion (DSD), a framework based on a continuous-time,
discrete-state jump stochastic process that operates directly in discrete
spatial domains while strictly preserving mass in both forward and reverse
diffusion processes. By using spatial diffusion to achieve mass preservation,
we introduce stochasticity naturally through a discrete formulation. We
demonstrate the expressive flexibility of DSD by performing image synthesis,
class conditioning, and image inpainting across widely-used image benchmarks,
with the ability to condition on image intensity. Additionally, we highlight
its applicability to domain-specific scientific data for materials
microstructure, bridging the gap between diffusion models and mass-conditioned
scientific applications.

</details>

### [460] [Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation](https://arxiv.org/abs/2505.02350)
*Bobo Lian,Dandan Wang,Chenjian Wu,Minxin Chen*

Main category: cs.GR

TLDR: 本文提出了一种基于稀疏椭球径向基函数网络的机器学习方法，用于近似点云的符号距离函数（SDF），实现紧凑且准确的表面表示。


<details>
  <summary>Details</summary>
Motivation: 点云表面表示是计算机图形学和视觉中的基本问题，现有方法在稀疏性和精度之间难以平衡。

Method: 使用动态多目标优化策略，联合优化椭球径向基函数（ERBFs）的权重、中心、形状和方向，并通过最近邻数据结构和CUDA并行化提高计算效率。

Result: 在多个基准数据集上的实验表明，该方法在准确性、鲁棒性和计算效率上优于以往的稀疏表示方法。

Conclusion: 该方法通过稀疏ERBFs实现了高效且精确的点云表面表示，代码已开源。

Abstract: Point cloud surface representation is a fundamental problem in computer
graphics and vision. This paper presents a machine learning approach for
approximating the signed distance function (SDF) of a point cloud using sparse
ellipsoidal radial basis function networks, enabling a compact and accurate
surface representation. Given the SDF values defined on the grid points
constructed from the point cloud, our method approximates the SDF accurately
with as few ellipsoidal radial basis functions (ERBFs) as possible, i.e.,
represent the SDF of a point cloud by sparse ERBFs. To balance sparsity and
approximation precision, a dynamic multi-objective optimization strategy is
introduced, which adaptively adds the regularization terms and jointly
optimizes the weights, centers, shapes, and orientations of ERBFs. To improve
computational efficiency, a nearest-neighbor-based data structure is employed,
restricting function calculations to points near each Gaussian kernel center.
The computations for each kernel are further parallelized on CUDA, which
significantly improves the optimization speed. Additionally, a hierarchical
octree-based refinement strategy is designed for training. Specifically, the
initialization and optimization of network parameters are conducted using
coarse grid points in the octree lattice structure. Subsequently, fine lattice
points are progressively incorporated to accelerate model convergence and
enhance training efficiency. Extensive experiments on multiple benchmark
datasets demonstrate that our method outperforms previous sparse representation
approaches in terms of accuracy, robustness, and computational efficiency. The
corresponding code is publicly available at
https://github.com/lianbobo/SE-RBFNet.git.

</details>

<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [461] [Faster logconcave sampling from a cold start in high dimension](https://arxiv.org/abs/2505.01937)
*Yunbum Kook,Santosh S. Vempala*

Main category: cs.DS

TLDR: 提出了一种更快的算法，用于生成任意对数凹密度的预热采样，首次实现了对（近）各向同性输入的子三次采样算法。


<details>
  <summary>Details</summary>
Motivation: 先前的研究在维度上至少需要线性预热惩罚，即使对于凸体的均匀采样也存在立方障碍，因此需要改进。

Method: 利用两种关键技术：1）在较弱的距离概念（如q-Rényi散度）下采样；2）改进并推广了Lee和Vempala的对数Sobolev不等式。

Result: 首次实现了对（近）各向同性输入的子三次采样算法，突破了立方障碍。

Conclusion: 该研究在预热采样领域取得了显著进展，为对数凹密度的高效采样提供了新方法。

Abstract: We present a faster algorithm to generate a warm start for sampling an
arbitrary logconcave density specified by an evaluation oracle, leading to the
first sub-cubic sampling algorithms for inputs in (near-)isotropic position. A
long line of prior work incurred a warm-start penalty of at least linear in the
dimension, hitting a cubic barrier, even for the special case of uniform
sampling from convex bodies.
  Our improvement relies on two key ingredients of independent interest. (1) We
show how to sample given a warm start in weaker notions of distance, in
particular $q$-R\'enyi divergence for $q=\widetilde{\mathcal{O}}(1)$, whereas
previous analyses required stringent $\infty$-R\'enyi divergence (with the
exception of Hit-and-Run, whose known mixing time is higher). This marks the
first improvement in the required warmness since Lov\'asz and Simonovits
(1991). (2) We refine and generalize the log-Sobolev inequality of Lee and
Vempala (2018), originally established for isotropic logconcave distributions
in terms of the diameter of the support, to logconcave distributions in terms
of a geometric average of the support diameter and the largest eigenvalue of
the covariance matrix.

</details>

<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [462] [Disassembly as Weighted Interval Scheduling with Learned Weights](https://arxiv.org/abs/2505.01536)
*Antonio Flores-Montoya,Junghee Lim,Adam Seitz,Akshay Sood,Edward Raff,James Holt*

Main category: cs.PL

TLDR: 论文提出了一种通用的反汇编算法，适用于多种架构（x86、x64、arm32、aarch64），并引入了一种新的冲突解决方法，将反汇编问题转化为加权区间调度问题。


<details>
  <summary>Details</summary>
Motivation: 反汇编是二进制分析和转换技术的第一步，现有方法通常分为探索、分析和冲突解决三个阶段，但缺乏通用性和高效的冲突解决机制。

Method: 提出了一种通用的反汇编算法，包括探索阶段、分析阶段和基于加权区间调度的冲突解决阶段。

Result: 算法适用于多种架构，并通过加权区间调度优化了冲突解决效率。

Conclusion: 该算法为多架构反汇编提供了一种高效且通用的解决方案。

Abstract: Disassembly is the first step of a variety of binary analysis and
transformation techniques, such as reverse engineering, or binary rewriting.
Recent disassembly approaches consist of three phases: an exploration phase,
that overapproximates the binary's code; an analysis phase, that assigns
weights to candidate instructions or basic blocks; and a conflict resolution
phase, that downselects the final set of instructions. We present a disassembly
algorithm that generalizes this pattern for a wide range of architectures,
namely x86, x64, arm32, and aarch64. Our algorithm presents a novel conflict
resolution method that reduces disassembly to weighted interval scheduling.

</details>

### [463] [Morello: Compiling Fast Neural Networks with Dynamic Programming and Spatial Compression](https://arxiv.org/abs/2505.01637)
*Samuel J. Kaufman,René Just,Rastislav Bodik*

Main category: cs.PL

TLDR: 论文提出了一种基于动态规划的方法，通过迭代分解程序规范并组合最优解，探索更大的搜索空间，同时采用新型记忆表减少内存需求。


<details>
  <summary>Details</summary>
Motivation: 解决高吞吐量神经网络推理中程序搜索空间过大且现有方法（如自动调度器）探索不足的问题。

Method: 使用动态规划分解程序规范，通过重写生成小规范，组合最优解，并采用压缩记忆表减少内存。

Result: 开发的Morello编译器能生成高效程序，例如在Zen 1 CPU上实现高吞吐量的矩阵乘法。

Conclusion: 基于动态规划的方法能有效探索更大搜索空间，且仿射成本模型足以生成高性能程序。

Abstract: High-throughput neural network inference requires coordinating many
optimization decisions, including parallel tiling, microkernel selection, and
data layout. The product of these decisions forms a search space of programs
which is typically intractably large. Existing approaches (e.g.,
auto-schedulers) often address this problem by sampling this space
heuristically. In contrast, we introduce a dynamic-programming-based approach
to explore more of the search space by iteratively decomposing large program
specifications into smaller specifications reachable from a set of rewrites,
then composing a final program from each rewrite that minimizes an affine cost
model. To reduce memory requirements, we employ a novel memoization table
representation, which indexes specifications by coordinates in $Z_{\geq 0}$ and
compresses identical, adjacent solutions. This approach can visit a much larger
set of programs than prior work. To evaluate the approach, we developed
Morello, a compiler which lowers specifications roughly equivalent to a
few-node XLA computation graph to x86. Notably, we found that an affine cost
model is sufficient to surface high-throughput programs. For example, Morello
synthesized a collection of matrix multiplication benchmarks targeting a Zen 1
CPU, including a 1x2048x16384, bfloat16-to-float32 vector-matrix multiply,
which was integrated into Google's gemma.cpp.

</details>

<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [464] [Bayesian Federated Cause-of-Death Classification and Quantification Under Distribution Shift](https://arxiv.org/abs/2505.02257)
*Yu Zhu,Zehang Richard Li*

Main category: stat.ME

TLDR: 提出了一种基于贝叶斯联邦学习（BFL）的框架，用于在缺乏本地标记数据的目标域中进行个体和群体级别的死因分类，性能优于单域模型，且与联合建模相当。


<details>
  <summary>Details</summary>
Motivation: 在缺乏医疗认证死因的地区，口头尸检（VA）是重要工具，但现有算法因数据分布变化和隐私问题导致性能下降。

Method: 采用贝叶斯联邦学习框架，避免数据共享，支持多种VA算法，模块化且计算高效。

Result: 在两个真实VA数据集上验证，BFL显著优于单域模型，性能与联合建模相当或更好。

Conclusion: BFL是一种高效、灵活的解决方案，适用于现实世界的死亡率监测系统。

Abstract: In regions lacking medically certified causes of death, verbal autopsy (VA)
is a critical and widely used tool to ascertain the cause of death through
interviews with caregivers. Data collected by VAs are often analyzed using
probabilistic algorithms. The performance of these algorithms often degrades
due to distributional shift across populations. Most existing VA algorithms
rely on centralized training, requiring full access to training data for joint
modeling. This is often infeasible due to privacy and logistical constraints.
In this paper, we propose a novel Bayesian Federated Learning (BFL) framework
that avoids data sharing across multiple training sources. Our method enables
reliable individual-level cause-of-death classification and population-level
quantification of cause-specific mortality fractions (CSMFs), in a target
domain with limited or no local labeled data. The proposed framework is
modular, computationally efficient, and compatible with a wide range of
existing VA algorithms as candidate models, facilitating flexible deployment in
real-world mortality surveillance systems. We validate the performance of BFL
through extensive experiments on two real-world VA datasets under varying
levels of distribution shift. Our results show that BFL significantly
outperforms the base models built on a single domain and achieves comparable or
better performance compared to joint modeling.

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [465] [Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network](https://arxiv.org/abs/2505.01880)
*Junyan Wu,Wenbo Xu,Wei Lu,Xiangyang Luo,Rui Yang,Shize Guo*

Main category: cs.SD

TLDR: 论文提出了一种名为LOCO的渐进式音频-语言协同学习网络，用于弱监督场景下的音频时间伪造定位（ATFL），通过协同学习和自监督方式提升定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有ATFL方法依赖精细标注训练网络，成本高且难以在实际场景中应用，因此需要一种弱监督下的高效解决方案。

Method: 设计了音频-语言协同学习模块，通过语义对齐捕获伪造共识特征；采用伪造定位模块生成伪造提案；引入渐进细化策略生成伪帧级标签并优化特征。

Result: 在三个公开基准测试中，LOCO达到了最先进的性能。

Conclusion: LOCO在弱监督下显著提升了音频伪造定位的性能，为实际应用提供了高效解决方案。

Abstract: Audio temporal forgery localization (ATFL) aims to find the precise forgery
regions of the partial spoof audio that is purposefully modified. Existing ATFL
methods rely on training efficient networks using fine-grained annotations,
which are obtained costly and challenging in real-world scenarios. To meet this
challenge, in this paper, we propose a progressive audio-language co-learning
network (LOCO) that adopts co-learning and self-supervision manners to prompt
localization performance under weak supervision scenarios. Specifically, an
audio-language co-learning module is first designed to capture forgery
consensus features by aligning semantics from temporal and global perspectives.
In this module, forgery-aware prompts are constructed by using utterance-level
annotations together with learnable prompts, which can incorporate semantic
priors into temporal content features dynamically. In addition, a forgery
localization module is applied to produce forgery proposals based on fused
forgery-class activation sequences. Finally, a progressive refinement strategy
is introduced to generate pseudo frame-level labels and leverage supervised
semantic contrastive learning to amplify the semantic distinction between real
and fake content, thereby continuously optimizing forgery-aware features.
Extensive experiments show that the proposed LOCO achieves SOTA performance on
three public benchmarks.

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [466] [A Slicing-Based Approach for Detecting and Patching Vulnerable Code Clones](https://arxiv.org/abs/2505.02349)
*Hakam Alomari,Christopher Vendome,Hilal Gyawali*

Main category: cs.SE

TLDR: srcVul是一种结合程序切片和局部敏感哈希的漏洞克隆检测方法，能高效识别漏洞克隆并推荐补丁，准确率达91%。


<details>
  <summary>Details</summary>
Motivation: 代码克隆会传播漏洞，带来安全风险，需要一种可扩展且精确的检测方法。

Method: srcVul通过分析已知漏洞程序及其补丁，构建漏洞相关切片数据库，并利用局部敏感哈希匹配目标程序中的切片向量。

Result: 在评估中，srcVul达到91%的准确率和75%的召回率，优于现有检测工具。

Conclusion: srcVul能有效检测复杂漏洞模式，适用于多样化代码库。

Abstract: Code cloning is a common practice in software development, but it poses
significant security risks by propagating vulnerabilities across cloned
segments. To address this challenge, we introduce srcVul, a scalable, precise
detection approach that combines program slicing with Locality-Sensitive
Hashing to identify vulnerable code clones and recommend patches. srcVul builds
a database of vulnerability-related slices by analyzing known vulnerable
programs and their corresponding patches, indexing each slice's unique
structural characteristics as a vulnerability slicing vector. During clone
detection, srcVul efficiently matches slicing vectors from target programs with
those in the database, recommending patches upon identifying similarities. Our
evaluation of srcVul against three state-of-the-art vulnerable clone detectors
demonstrates its accuracy, efficiency, and scalability, achieving 91% precision
and 75% recall on established vulnerability databases and open-source
repositories. These results highlight srcVul's effectiveness in detecting
complex vulnerability patterns across diverse codebases.

</details>

### [467] [BiGSCoder: State Space Model for Code Understanding](https://arxiv.org/abs/2505.01475)
*Shweta Verma,Abhinav Anand,Mira Mezini*

Main category: cs.SE

TLDR: BiGSCoder是一种新型的双向状态空间模型（SSM），通过门控架构和掩码语言建模预训练，在代码理解任务中表现优于传统Transformer模型，且训练数据需求更低。


<details>
  <summary>Details</summary>
Motivation: 系统评估SSM在代码任务中的能力，并与传统Transformer架构进行对比。

Method: 提出BiGSCoder模型，采用门控架构和掩码语言建模预训练，并在多种配置和基准测试中进行实验。

Result: BiGSCoder在代码理解任务中表现优于Transformer模型，且无需位置嵌入即可有效处理长序列。

Conclusion: BiGSCoder可作为Transformer的高效替代方案，尤其在样本效率和长序列处理方面表现突出。

Abstract: We present BiGSCoder, a novel encoder-only bidirectional state-space model
(SSM) featuring a gated architecture, pre-trained for code understanding on a
code dataset using masked language modeling. Our work aims to systematically
evaluate SSMs' capabilities in coding tasks compared to traditional transformer
architectures; BiGSCoder is built for this purpose. Through comprehensive
experiments across diverse pre-training configurations and code understanding
benchmarks, we demonstrate that BiGSCoder outperforms transformer-based models,
despite utilizing simpler pre-training strategies and much less training data.
Our results indicate that BiGSCoder can serve as a more sample-efficient
alternative to conventional transformer models. Furthermore, our study shows
that SSMs perform better without positional embeddings and can effectively
extrapolate to longer sequences during fine-tuning.

</details>

### [468] [Runtime Anomaly Detection for Drones: An Integrated Rule-Mining and Unsupervised-Learning Approach](https://arxiv.org/abs/2505.01947)
*Ivan Tan,Wei Minn,Christopher M. Poskitt,Lwin Khin Shar,Lingxiao Jiang*

Main category: cs.SE

TLDR: 论文提出了一种结合规则挖掘和无监督学习的无人机异常检测方法RADD，解决了现有LSTM方法在泛化性、可解释性和领域知识捕获方面的不足。


<details>
  <summary>Details</summary>
Motivation: 无人机依赖多传感器输入，故障可能导致安全问题。现有LSTM方法在泛化性、可解释性和领域知识捕获方面存在不足，需要改进。

Method: RADD结合规则挖掘（捕获传感器与执行器间预期关系）和无监督学习（覆盖规则未捕捉的复杂关系），在ArduPilot和Gazebo中实现。

Result: RADD检测到93.84%的异常（6种故障类型），假阳性率仅2.33%，且优于LSTM方法。

Conclusion: RADD是一种高效、可解释的无人机异常检测方法，适用于实时部署。

Abstract: UAVs, commonly referred to as drones, have witnessed a remarkable surge in
popularity due to their versatile applications. These cyber-physical systems
depend on multiple sensor inputs, such as cameras, GPS receivers,
accelerometers, and gyroscopes, with faults potentially leading to physical
instability and serious safety concerns. To mitigate such risks, anomaly
detection has emerged as a crucial safeguarding mechanism, capable of
identifying the physical manifestations of emerging issues and allowing
operators to take preemptive action at runtime. Recent anomaly detection
methods based on LSTM neural networks have shown promising results, but three
challenges persist: the need for models that can generalise across the diverse
mission profiles of drones; the need for interpretability, enabling operators
to understand the nature of detected problems; and the need for capturing
domain knowledge that is difficult to infer solely from log data. Motivated by
these challenges, this paper introduces RADD, an integrated approach to anomaly
detection in drones that combines rule mining and unsupervised learning. In
particular, we leverage rules (or invariants) to capture expected relationships
between sensors and actuators during missions, and utilise unsupervised
learning techniques to cover more subtle relationships that the rules may have
missed. We implement this approach using the ArduPilot drone software in the
Gazebo simulator, utilising 44 rules derived across the main phases of drone
missions, in conjunction with an ensemble of five unsupervised learning models.
We find that our integrated approach successfully detects 93.84% of anomalies
over six types of faults with a low false positive rate (2.33%), and can be
deployed effectively at runtime. Furthermore, RADD outperforms a
state-of-the-art LSTM-based method in detecting the different types of faults
evaluated in our study.

</details>

### [469] [On the Need for a Statistical Foundation in Scenario-Based Testing of Autonomous Vehicles](https://arxiv.org/abs/2505.02274)
*Xingyu Zhao,Robab Aghazadeh-Chakherlou,Chih-Hong Cheng,Peter Popov,Lorenzo Strigini*

Main category: cs.SE

TLDR: 论文探讨了自动驾驶汽车（AVs）基于场景测试的统计基础，提出了量化失败概率和评估测试有效性的方法，并引入新指标REF以确保仿真测试的统计可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决基于场景测试在停止规则、残余风险估计、调试有效性及仿真保真度对安全声明影响等方面的基础问题，为AVs安全提供严格的统计基础。

Method: 通过与传统软件测试方法对比，识别研究空白和可复用解决方案，提出概念验证模型量化场景失败概率（pfs）并评估测试有效性。

Result: 发现基于场景和基于里程的测试无绝对优劣，提出REF指标验证仿真与真实测试结果的一致性。

Conclusion: 严格的统计基础对AVs安全至关重要，REF指标为仿真测试的统计可靠性提供了保障。

Abstract: Scenario-based testing has emerged as a common method for autonomous vehicles
(AVs) safety, offering a more efficient alternative to mile-based testing by
focusing on high-risk scenarios. However, fundamental questions persist
regarding its stopping rules, residual risk estimation, debug effectiveness,
and the impact of simulation fidelity on safety claims. This paper argues that
a rigorous statistical foundation is essential to address these challenges and
enable rigorous safety assurance. By drawing parallels between AV testing and
traditional software testing methodologies, we identify shared research gaps
and reusable solutions. We propose proof-of-concept models to quantify the
probability of failure per scenario (pfs) and evaluate testing effectiveness
under varying conditions. Our analysis reveals that neither scenario-based nor
mile-based testing universally outperforms the other. Furthermore, we introduce
Risk Estimation Fidelity (REF), a novel metric to certify the alignment of
synthetic and real-world testing outcomes, ensuring simulation-based safety
claims are statistically defensible.

</details>

<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [470] [Dendritic Computing with Multi-Gate Ferroelectric Field-Effect Transistors](https://arxiv.org/abs/2505.01635)
*A N M Nafiul Islam,Xuezhong Niu,Jiahui Duan,Shubham Kumar,Kai Ni,Abhronil Sengupta*

Main category: cs.ET

TLDR: 论文提出了一种基于多栅极铁电场效应晶体管的新型神经元设计，模拟树突结构，通过铁电非线性实现局部计算，显著提高了计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 受生物神经元树突结构的启发，旨在解决人工神经网络中点神经元计算复杂度不足的问题，提升神经形态系统的计算效率和边缘应用性能。

Method: 采用多栅极铁电场效应晶体管模拟树突结构，利用铁电非线性实现局部计算，并通过晶体管动作生成最终神经元输出。

Result: 实验表明，采用树突神经元的网络性能优于传统大网络（训练参数减少约17倍）。

Conclusion: 树突硬件能显著提升神经形态系统的计算效率和学习能力，适用于边缘应用。

Abstract: Although inspired by neuronal systems in the brain, artificial neural
networks generally employ point-neurons, which offer far less computational
complexity than their biological counterparts. Neurons have dendritic arbors
that connect to different sets of synapses and offer local non-linear
accumulation - playing a pivotal role in processing and learning. Inspired by
this, we propose a novel neuron design based on a multi-gate ferroelectric
field-effect transistor that mimics dendrites. It leverages ferroelectric
nonlinearity for local computations within dendritic branches, while utilizing
the transistor action to generate the final neuronal output. The branched
architecture paves the way for utilizing smaller crossbar arrays in hardware
integration, leading to greater efficiency. Using an experimentally calibrated
device-circuit-algorithm co-simulation framework, we demonstrate that networks
incorporating our dendritic neurons achieve superior performance in comparison
to much larger networks without dendrites ($\sim$17$\times$ fewer trainable
weight parameters). These findings suggest that dendritic hardware can
significantly improve computational efficiency, and learning capacity of
neuromorphic systems optimized for edge applications.

</details>

<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [471] [Interpretable graph-based models on multimodal biomedical data integration: A technical review and benchmarking](https://arxiv.org/abs/2505.01696)
*Alireza Sadeghi,Farshid Hajati,Ahmadreza Argha,Nigel H Lovell,Min Yang,Hamid Alinejad-Rokny*

Main category: q-bio.GN

TLDR: 该论文综述了可解释的图模型在多模态生物医学数据中的应用，分析了26项研究，并比较了四种解释方法在阿尔茨海默病数据上的表现。


<details>
  <summary>Details</summary>
Motivation: 整合异构生物医学数据（如影像、组学和临床记录）以支持精准诊断和个性化治疗，但需要满足监管要求的可解释性。

Method: 对2019年至2024年间的26项研究进行技术调查，比较了四种解释方法（敏感性分析、梯度显著性、SHAP和图掩码）在阿尔茨海默病数据上的表现。

Result: SHAP和敏感性分析能恢复最广泛的已知AD通路和基因本体术语，而梯度显著性和图掩码则揭示了互补的代谢和运输特征。所有方法均优于随机基因集，但各有权衡。

Conclusion: 论文总结了可解释图学习的现状，提供了流程图帮助选择方法，并指出了未来研究方向，为方法开发者和转化科学家提供了参考。

Abstract: Integrating heterogeneous biomedical data including imaging, omics, and
clinical records supports accurate diagnosis and personalised care. Graph-based
models fuse such non-Euclidean data by capturing spatial and relational
structure, yet clinical uptake requires regulator-ready interpretability. We
present the first technical survey of interpretable graph based models for
multimodal biomedical data, covering 26 studies published between Jan 2019 and
Sep 2024. Most target disease classification, notably cancer and rely on static
graphs from simple similarity measures, while graph-native explainers are rare;
post-hoc methods adapted from non-graph domains such as gradient saliency, and
SHAP predominate. We group existing approaches into four interpretability
families, outline trends such as graph-in-graph hierarchies, knowledge-graph
edges, and dynamic topology learning, and perform a practical benchmark. Using
an Alzheimer disease cohort, we compare Sensitivity Analysis, Gradient
Saliency, SHAP and Graph Masking. SHAP and Sensitivity Analysis recover the
broadest set of known AD pathways and Gene-Ontology terms, whereas Gradient
Saliency and Graph Masking surface complementary metabolic and transport
signatures. Permutation tests show all four beat random gene sets, but with
distinct trade-offs: SHAP and Graph Masking offer deeper biology at higher
compute cost, while Gradient Saliency and Sensitivity Analysis are quicker
though coarser. We also provide a step-by-step flowchart covering graph
construction, explainer choice and resource budgeting to help researchers
balance transparency and performance. This review synthesises the state of
interpretable graph learning for multimodal medicine, benchmarks leading
techniques, and charts future directions, from advanced XAI tools to
under-studied diseases, serving as a concise reference for method developers
and translational scientists.

</details>

<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [472] [Adaptive Bidding Policies for First-Price Auctions with Budget Constraints under Non-stationarity](https://arxiv.org/abs/2505.02796)
*Yige Wang,Jiashuo Jiang*

Main category: cs.GT

TLDR: 论文研究了预算受限的投标人在重复第一价格拍卖中如何自适应出价以最大化累积收益，提出了基于双梯度下降的出价策略，并在不同信息设置下分析了性能损失。


<details>
  <summary>Details</summary>
Motivation: 由于行业从第二价格拍卖转向第一价格拍卖，真实出价不再是最优策略，因此需要研究预算受限投标人的自适应出价问题。

Method: 提出了一种基于双梯度下降的出价策略，维护一个预算约束的双变量，并在两种信息设置下分析性能损失。

Result: 在无信息设置下，遗憾为O~(√T)加上反映价值分布非平稳性的变化项；在有信息设置下，遗憾为O~(√T)加上预测误差项。

Conclusion: 研究表明，预测信息可以消除非平稳性带来的性能损失，为预算受限投标人提供了有效的出价策略。

Abstract: We study how a budget-constrained bidder should learn to adaptively bid in
repeated first-price auctions to maximize her cumulative payoff. This problem
arose due to an industry-wide shift from second-price auctions to first-price
auctions in display advertising recently, which renders truthful bidding (i.e.,
always bidding one's private value) no longer optimal. We propose a simple
dual-gradient-descent-based bidding policy that maintains a dual variable for
budget constraint as the bidder consumes her budget. In analysis, we consider
two settings regarding the bidder's knowledge of her private values in the
future: (i) an uninformative setting where all the distributional knowledge
(can be non-stationary) is entirely unknown to the bidder, and (ii) an
informative setting where a prediction of the budget allocation in advance. We
characterize the performance loss (or regret) relative to an optimal policy
with complete information on the stochasticity. For uninformative setting, We
show that the regret is \tilde{O}(\sqrt{T}) plus a variation term that reflects
the non-stationarity of the value distributions, and this is of optimal order.
We then show that we can get rid of the variation term with the help of the
prediction; specifically, the regret is \tilde{O}(\sqrt{T}) plus the prediction
error term in the informative setting.

</details>

<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [473] [A dynamic view of the double descent](https://arxiv.org/abs/2505.01751)
*Vivek Shripad Borkar*

Main category: math.OC

TLDR: 论文解释了过参数化神经网络中观察到的‘双下降’现象，通过两时间尺度随机逼近理论和奇异扰动微分方程提供了动态角度的解释。


<details>
  <summary>Details</summary>
Motivation: 研究过参数化神经网络在训练过程中表现出的‘双下降’现象，并为其提供理论解释。

Method: 使用两时间尺度随机逼近理论和奇异扰动微分方程，分析梯度动力学的连续时间极限。

Result: 为‘双下降’现象提供了一个动态的理论解释。

Conclusion: 通过动态角度补充了对‘双下降’现象的现有研究，为理解神经网络训练行为提供了新视角。

Abstract: It has been observed by Belkin et al.\ that overparametrized neural networks
exhibit a `double descent' phenomenon. That is, as the model complexity, as
reflected in the number of features, increases, the training error initially
decreases, then increases, and then decreases again. A counterpart of this
phenomenon in the time domain has been noted in the context of epoch-wise
training, viz., that the training error decreases with time, then increases,
then decreases again. This note presents a plausible explanation for this
phenomenon by using the theory of two time scale stochastic approximation and
singularly perturbed differential equations, applied to the continuous time
limit of the gradient dynamics. This adds a `dynamic' angle to an already well
studied theme.

</details>

### [474] [Rank-One Modified Value Iteration](https://arxiv.org/abs/2505.01828)
*Arman Sharifi Kolarijani,Tolga Ok,Peyman Mohajerin Esfahani,Mohamad Amin Sharif Kolarijani*

Main category: math.OC

TLDR: 提出一种基于秩一近似的新型算法，用于解决马尔可夫决策过程的规划和学习问题，性能优于一阶算法及其加速版本。


<details>
  <summary>Details</summary>
Motivation: 解决马尔可夫决策过程中规划和学习问题的效率问题，提出一种更高效的算法。

Method: 采用秩一近似策略迭代更新，结合幂方法逼近转移概率矩阵的平稳分布。

Result: 算法在规划和学习问题中均优于一阶算法及其加速版本，且收敛速率与值迭代和Q学习相同。

Conclusion: 该算法在理论和实验中均表现出优越性能，为规划和学习问题提供了高效解决方案。

Abstract: In this paper, we provide a novel algorithm for solving planning and learning
problems of Markov decision processes. The proposed algorithm follows a policy
iteration-type update by using a rank-one approximation of the transition
probability matrix in the policy evaluation step. This rank-one approximation
is closely related to the stationary distribution of the corresponding
transition probability matrix, which is approximated using the power method. We
provide theoretical guarantees for the convergence of the proposed algorithm to
optimal (action-)value function with the same rate and computational complexity
as the value iteration algorithm in the planning problem and as the Q-learning
algorithm in the learning problem. Through our extensive numerical simulations,
however, we show that the proposed algorithm consistently outperforms
first-order algorithms and their accelerated versions for both planning and
learning problems.

</details>

### [475] [Pickup & Delivery with Time Windows and Transfers: combining decomposition with metaheuristics](https://arxiv.org/abs/2505.02158)
*Ioannis Avgerinos,Ioannis Mourtos,Nikolaos Tsompanidis,Georgios Zois*

Main category: math.OC

TLDR: 论文研究了允许车辆中途交换负载且严格遵循时间窗的Pickup and Delivery Problem的泛化问题，提出了改进的LBBD和LNS算法，并开发了实例生成器以填补基准测试空白。


<details>
  <summary>Details</summary>
Motivation: 解决现有Pickup and Delivery Problem中车辆中途负载交换和时间窗约束的泛化问题，并提升算法的优化性能和可扩展性。

Method: 提出Logic-Based Benders Decomposition (LBBD)和改进的Large Neighborhood Search (LNS)算法，并开发实例生成器。

Result: LBBD在中等规模数据集上能完全优化，LNS提供接近最优解；在大规模实例中，改进的LNS展现了更好的适应性和扩展性。

Conclusion: 论文提出的LBBD和LNS算法在优化性能和可扩展性上均有显著提升，填补了基准测试的空白。

Abstract: This paper examines the generalisation of the Pickup and Delivery Problem
that allows mid-route load exchanges among vehicles and obeys strict
time-windows at all locations. We propose a novel Logic-Based Benders
Decomposition (LBBD) that improves optimality gaps for all benchmarks in the
literature and scales up to handle larger ones. To tackle even larger
instances, we introduce a refined Large Neighborhood Search (LNS) algorithm
that improves the adaptability of LNS beyond case-specific configurations
appearing in related literature.
  To bridge the gap in benchmark availability, we develop an instance generator
that allows for extensive experimentation. For moderate datasets (25 and 50
requests), we evaluate the performance of both LBBD and LNS, the former being
able to close the gap and the latter capable of providing near-optimal
solutions. For larger instances (75 and 100 requests), we recreate indicative
state-of-the-art metaheuristics to highlight the improvements introduced by our
LNS refinements, while establishing its scalability.

</details>

### [476] [Optimization over Trained (and Sparse) Neural Networks: A Surrogate within a Surrogate](https://arxiv.org/abs/2505.01985)
*Hung Pham,Aiden Ren,Ibrahim Tahir,Jiatai Tong,Thiago Serra*

Main category: math.OC

TLDR: 论文提出了一种通过修剪神经网络来优化约束学习的方法，发现修剪后的网络即使分类性能较差，也能作为高效且有效的替代模型。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络替代模型在优化问题中难以求解的挑战，探索修剪网络作为替代的可行性。

Method: 应用网络修剪技术生成稀疏替代模型，并在混合整数线性规划（MILP）求解器中验证其效果。

Result: 稀疏替代模型能更快生成对抗扰动，且无需微调即可保持高效性。

Conclusion: 修剪后的网络即使性能下降，仍可作为高效替代模型，为约束学习提供了新思路。

Abstract: We can approximate a constraint or an objective function that is uncertain or
nonlinear with a neural network that we embed in the optimization model. This
approach, which is known as constraint learning, faces the challenge that
optimization models with neural network surrogates are harder to solve. Such
difficulties have motivated studies on model reformulation, specialized
optimization algorithms, and - to a lesser extent - pruning of the embedded
networks. In this work, we double down on the use of surrogates by applying
network pruning to produce a surrogate of the neural network itself. In the
context of using a Mixed-Integer Linear Programming (MILP) solver to verify
neural networks, we obtained faster adversarial perturbations for dense neural
networks by using sparse surrogates, especially - and surprisingly - if not
taking the time to finetune the sparse network to make up for the loss in
accuracy. In other words, we show that a pruned network with bad classification
performance can still be a good - and more efficient - surrogate.

</details>

### [477] [Minimisation of Quasar-Convex Functions Using Random Zeroth-Order Oracles](https://arxiv.org/abs/2505.02281)
*Amir Ali Farzin,Yuen-Man Pun,Iman Shames*

Main category: math.OC

TLDR: 研究了随机高斯平滑零阶（ZO）方案在无约束和有约束条件下最小化拟星凸（QC）和强拟星凸（SQC）函数的性能，证明了算法的收敛性和复杂度，并提出了近端拟星凸的新概念。


<details>
  <summary>Details</summary>
Motivation: 探索ZO算法在拟星凸和强拟星凸函数优化中的表现，填补理论空白，并验证其在机器学习中的实际应用。

Method: 采用随机高斯平滑零阶方案，分析其在无约束和有约束条件下的性能，提出近端拟星凸概念并证明相关结果。

Result: 在无约束问题中，ZO算法能收敛到全局最小值；在有约束问题中，算法收敛到可控邻域内。ZO方法在某些情况下优于梯度下降。

Conclusion: ZO算法在拟星凸和强拟星凸函数优化中具有理论保证和实际优势，为无梯度优化提供了新思路。

Abstract: This study explores the performance of a random Gaussian smoothing
zeroth-order (ZO) scheme for minimising quasar-convex (QC) and strongly
quasar-convex (SQC) functions in both unconstrained and constrained settings.
For the unconstrained problem, we establish the ZO algorithm's convergence to a
global minimum along with its complexity when applied to both QC and SQC
functions. For the constrained problem, we introduce the new notion of
proximal-quasar-convexity and prove analogous results to the unconstrained
case. Specifically, we show the complexity bounds and the convergence of the
algorithm to a neighbourhood of a global minimum whose size can be controlled
under a variance reduction scheme. Theoretical findings are illustrated through
investigating the performance of the algorithm applied to a range of problems
in machine learning and optimisation. Specifically, we observe scenarios where
the ZO method outperforms gradient descent. We provide a possible explanation
for this phenomenon.

</details>

### [478] [Efficient Curvature-Aware Hypergradient Approximation for Bilevel Optimization](https://arxiv.org/abs/2505.02101)
*Youran Dong,Junfeng Yang,Wei Yao,Jin Zhang*

Main category: math.OC

TLDR: 本文提出了一种高效的双层优化方法，通过引入曲率信息改进超梯度计算，并展示了其在理论和实践中的优势。


<details>
  <summary>Details</summary>
Motivation: 双层优化在机器学习的超参数优化和元学习中具有广泛应用，但现有梯度方法在计算效率和精度上存在不足。

Method: 提出了一种基于曲率信息的超梯度近似技术，并结合不精确牛顿法构建新算法框架。

Result: 在确定性和随机场景下均证明了收敛速率，且在确定性场景中显著降低了计算复杂度。数值实验验证了其实际性能优势。

Conclusion: 通过利用超梯度结构和曲率信息，新方法在理论和实践中均表现出色，为双层优化提供了高效解决方案。

Abstract: Bilevel optimization is a powerful tool for many machine learning problems,
such as hyperparameter optimization and meta-learning. Estimating
hypergradients (also known as implicit gradients) is crucial for developing
gradient-based methods for bilevel optimization. In this work, we propose a
computationally efficient technique for incorporating curvature information
into the approximation of hypergradients and present a novel algorithmic
framework based on the resulting enhanced hypergradient computation. We provide
convergence rate guarantees for the proposed framework in both deterministic
and stochastic scenarios, particularly showing improved computational
complexity over popular gradient-based methods in the deterministic setting.
This improvement in complexity arises from a careful exploitation of the
hypergradient structure and the inexact Newton method. In addition to the
theoretical speedup, numerical experiments demonstrate the significant
practical performance benefits of incorporating curvature information.

</details>

### [479] [Temporal Robustness in Discrete Time Linear Dynamical Systems](https://arxiv.org/abs/2505.02347)
*Nilava Metya,Arunesh Sinha*

Main category: math.OC

TLDR: 论文研究了离散时间线性动力系统中时间不确定性对成本估计的影响，提出了一种基于Wasserstein模糊集的分布鲁棒方法，并提供了多项式时间算法和硬度结果。


<details>
  <summary>Details</summary>
Motivation: 解决时间不确定性对系统运行成本估计的影响，避免从少量样本中学习概率分布。

Method: 通过等价性分析，将离散时间马尔可夫链与全局渐近稳定线性动力系统联系起来，提出基于Wasserstein模糊集的理论框架。

Result: 提供了多项式时间算法和硬度结果，包括关于Wasserstein距离多面体的基本结论。

Conclusion: 该方法为时间不确定性问题提供了有效的分布鲁棒解决方案，具有理论和实际意义。

Abstract: Discrete time linear dynamical systems, including Markov chains, have found
many applications. However, in some problems, there is uncertainty about the
time horizon for which the system runs. This creates uncertainty about the cost
(or reward) incurred based on the state distribution when the system stops.
Given past data samples of how long a system ran, we propose to theoretically
analyze a distributional robust cost estimation task in a Wasserstein ambiguity
set, instead of learning a probability distribution from a few samples. Towards
this, we show an equivalence between a discrete time Markov Chain on a
probability simplex and a global asymptotic stable (GAS) discrete time linear
dynamical system, allowing us to base our study on a GAS system only. Then, we
provide various polynomial time algorithms and hardness results for different
cases in our theoretical study, including a fundamental result about
Wasserstein distance based polytope.

</details>

### [480] [Smooth Integer Encoding via Integral Balance](https://arxiv.org/abs/2505.02259)
*Stanislav Semenov*

Main category: math.OC

TLDR: 提出一种新颖的整数编码方法，通过平滑实值函数的积分特性隐式反映离散量，支持连续优化和机器学习应用。


<details>
  <summary>Details</summary>
Motivation: 传统整数表示方法中，整数作为显式参数存在，而新方法通过平滑函数的累积平衡隐式编码整数，为连续优化和机器学习提供新思路。

Method: 使用局部高斯凸起和交替衰减系数构造平滑函数f_N(t)，其积分I(N)随N趋近于零，整数可通过近零点的最小点恢复。

Result: 方法支持连续可微的离散状态表示，可通过样条或解析反演恢复整数，并可扩展到多维元组。

Conclusion: 该框架为在连续优化、机器学习和平滑符号计算中嵌入离散逻辑提供了新途径。

Abstract: We introduce a novel method for encoding integers using smooth real-valued
functions whose integral properties implicitly reflect discrete quantities. In
contrast to classical representations, where the integer appears as an explicit
parameter, our approach encodes the number N in the set of natural numbers
through the cumulative balance of a smooth function f_N(t), constructed from
localized Gaussian bumps with alternating and decaying coefficients. The total
integral I(N) converges to zero as N tends to infinity, and the integer can be
recovered as the minimal point of near-cancellation.
  This method enables continuous and differentiable representations of discrete
states, supports recovery through spline-based or analytical inversion, and
extends naturally to multidimensional tuples (N1, N2, ...). We analyze the
structure and convergence of the encoding series, demonstrate numerical
construction of the integral map I(N), and develop procedures for integer
recovery via numerical inversion. The resulting framework opens a path toward
embedding discrete logic within continuous optimization pipelines, machine
learning architectures, and smooth symbolic computation.

</details>

### [481] [Integrating Column Generation and Large Neighborhood Search for Bus Driver Scheduling with Complex Break Constraints](https://arxiv.org/abs/2505.02485)
*Lucas Kletzander,Tommaso Mannelli Mazzoli,Nysret Musliu,Pascal Van Hentenryck*

Main category: math.OC

TLDR: 本文研究了公交车司机排班问题（BDSP），提出了一种结合精确方法（Branch and Price）和大邻域搜索（LNS）的混合解决方案，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: BDSP是一个受严格法律和集体协议约束的组合优化问题，目标是设计高效且满足司机需求的排班方案。现有方法难以在不同规模实例上均取得高质量解。

Method: 结合Branch and Price（B&P）和Large Neighborhood Search（LNS）框架，提出了一种深度集成的解决方案，利用LNS子问题生成的列优化全局解。

Result: 实验表明，该方法在不同规模实例上均取得最优结果，小实例获得精确解，中等实例与已知下界差距较小。

Conclusion: B&P适合小实例，而LNS与CG深度集成能高效解决大规模问题，方法具有通用性，可扩展至其他优化问题。

Abstract: The Bus Driver Scheduling Problem (BDSP) is a combinatorial optimization
problem with the goal to design shifts to cover prearranged bus tours. The
objective takes into account the operational cost as well as the satisfaction
of drivers. This problem is heavily constrained due to strict legal rules and
collective agreements. The objective of this article is to provide
state-of-the-art exact and hybrid solution methods that can provide
high-quality solutions for instances of different sizes. This work presents a
comprehensive study of both an exact method, Branch and Price (B&P), as well as
a Large Neighborhood Search (LNS) framework which uses B&P or Column Generation
(CG) for the repair phase to solve the BDSP. It further proposes and evaluates
a novel deeper integration of B&P and LNS, storing the generated columns from
the LNS subproblems and reusing them for other subproblems, or to find better
global solutions. The article presents a detailed analysis of several
components of the solution methods and their impact, including general
improvements for the B&P subproblem, which is a high-dimensional Resource
Constrained Shortest Path Problem (RCSPP), and the components of the LNS. The
evaluation shows that our approach provides new state-of-the-art results for
instances of all sizes, including exact solutions for small instances, and low
gaps to a known lower bound for mid-sized instances. Conclusions: We observe
that B&P provides the best results for small instances, while the tight
integration of LNS and CG can provide high-quality solutions for larger
instances, further improving over LNS which just uses CG as a black box. The
proposed methods are general and can also be applied to other rule sets and
related optimization problems

</details>

### [482] [Entropic Mirror Descent for Linear Systems: Polyak's Stepsize and Implicit Bias](https://arxiv.org/abs/2505.02614)
*Yura Malitsky,Alexander Posch*

Main category: math.OC

TLDR: 论文提出一种改进的Polyak型步长方法，用于解决无界域上的线性系统问题，并扩展了收敛性结果。


<details>
  <summary>Details</summary>
Motivation: 解决无界域上线性系统的收敛分析难题，避免引入限制性假设。

Method: 引入改进的Polyak型步长，并提出了避免指数运算的替代方法。

Result: 获得了亚线性和线性收敛结果，并推广到任意凸L-光滑函数。

Conclusion: 方法具有理论保证的收敛性，适用于更广泛的问题。

Abstract: This paper focuses on applying entropic mirror descent to solve linear
systems, where the main challenge for the convergence analysis stems from the
unboundedness of the domain. To overcome this without imposing restrictive
assumptions, we introduce a variant of Polyak-type stepsizes. Along the way, we
strengthen the bound for $\ell_1$-norm implicit bias, obtain sublinear and
linear convergence results, and generalize the convergence result to arbitrary
convex $L$-smooth functions. We also propose an alternative method that avoids
exponentiation, resembling the original Hadamard descent, but with provable
convergence.

</details>

<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [483] [Identifying Doppelganger Active Galactic Nuclei across redshifts from spectroscopic surveys](https://arxiv.org/abs/2505.01642)
*Shreya Sareen,Swayamtrupta Panda*

Main category: astro-ph.GA

TLDR: 研究探讨低红移AGN是否可作为高红移AGN的替代，通过光谱相似性分析发现存在高度相似的AGN对。


<details>
  <summary>Details</summary>
Motivation: 理解AGN性质随宇宙时间的演化是重要挑战，需验证低红移AGN是否与高红移AGN具有相似性。

Method: 利用SDSS DR16数据，分析AGN的连续谱和发射线特征（如N V、C IV、Mg II等），并比较等效宽度、速度弥散和连续谱光度。

Result: 发现多个光谱高度相似的AGN对，表明低红移AGN可能与高红移AGN具有相似的内在性质。

Conclusion: 低红移AGN可能作为高红移AGN的替代，为研究AGN演化提供新途径。

Abstract: Active Galactic Nuclei (AGNs) are among the most luminous objects in the
universe, making them valuable probes for studying galaxy evolution. However,
understanding how AGN properties evolve over cosmic time remains a fundamental
challenge. This study investigates whether AGNs at low redshift (nearby) can
serve as proxies for their high-redshift (distant) counterparts by identifying
spectral 'doppelg\"angers', AGNs with remarkably similar emission line
properties despite being separated by vast cosmic distances. We analyze key
spectral features of bona fide AGNs using the Sloan Digital Sky Survey's Data
Release 16, including continuum and emission lines: Nitrogen (N V), Carbon (C
IV), Magnesium (Mg II), Hydrogen-beta (H$\beta$), and Iron (Fe II - optical and
UV) emission lines. We incorporated properties such as equivalent width,
velocity dispersion in the form of full width at half maximum (FWHM), and
continuum luminosities (135nm, 300nm, and 510nm) closest to these prominent
lines. Our initial findings suggest the existence of multiple AGNs with highly
similar spectra, hinting at the possibility that local AGNs may indeed share
intrinsic properties with high-redshift ones. We showcase here one of the
better candidate pairs of AGNs resulting from our analyses.

</details>

<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [484] [NeuroSim V1.5: Improved Software Backbone for Benchmarking Compute-in-Memory Accelerators with Device and Circuit-level Non-idealities](https://arxiv.org/abs/2505.02314)
*James Read,Ming-Yen Lee,Wei-Hsing Huang,Yuan-Chun Luo,Anni Lu,Shimeng Yu*

Main category: cs.AR

TLDR: NeuroSim V1.5是一款用于模拟计算内存（ACIM）加速器的工具，通过集成TensorRT、噪声注入方法和优化模拟速度，支持更广泛的神经网络设计和高效硬件探索。


<details>
  <summary>Details</summary>
Motivation: 传统冯·诺依曼架构在AI应用中存在能效和延迟问题，ACIM通过在内存中直接计算减少数据传输，但需要精确建模非理想特性。

Method: NeuroSim V1.5集成了TensorRT量化流程，引入噪声注入方法，支持新兴存储器，并通过优化模拟提升运行速度。

Result: 工具支持更广泛的神经网络（如Transformer），提供高效的硬件设计空间探索，并在保持精度的同时优化设计参数。

Conclusion: NeuroSim V1.5通过高保真噪声建模和高效模拟，推动了下一代ACIM加速器的设计与验证。

Abstract: The exponential growth of artificial intelligence (AI) applications has
exposed the inefficiency of conventional von Neumann architectures, where
frequent data transfers between compute units and memory create significant
energy and latency bottlenecks. Analog Computing-in-Memory (ACIM) addresses
this challenge by performing multiply-accumulate (MAC) operations directly in
the memory arrays, substantially reducing data movement. However, designing
robust ACIM accelerators requires accurate modeling of device- and
circuit-level non-idealities. In this work, we present NeuroSim V1.5,
introducing several key advances: (1) seamless integration with TensorRT's
post-training quantization flow enabling support for more neural networks
including transformers, (2) a flexible noise injection methodology built on
pre-characterized statistical models, making it straightforward to incorporate
data from SPICE simulations or silicon measurements, (3) expanded device
support including emerging non-volatile capacitive memories, and (4) up to 6.5x
faster runtime than NeuroSim V1.4 through optimized behavioral simulation. The
combination of these capabilities uniquely enables systematic design space
exploration across both accuracy and hardware efficiency metrics. Through
multiple case studies, we demonstrate optimization of critical design
parameters while maintaining network accuracy. By bridging high-fidelity noise
modeling with efficient simulation, NeuroSim V1.5 advances the design and
validation of next-generation ACIM accelerators. All NeuroSim versions are
available open-source at https://github.com/neurosim/NeuroSim.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [485] [Student Perspectives on the Benefits and Risks of AI in Education](https://arxiv.org/abs/2505.02198)
*Griffin Pitts,Viktoria Marcus,Sanaz Motamedi*

Main category: cs.CY

TLDR: 论文探讨了AI聊天机器人在教育中的使用及其对学生的影响，包括益处（如反馈和学习支持）和风险（如学术诚信和批判性思维丧失），并提出了政策建议。


<details>
  <summary>Details</summary>
Motivation: 研究学生对AI聊天机器人在教育中的看法，以平衡其潜在益处与风险。

Method: 通过问卷调查和主题分析，收集并分析了262名本科生的反馈。

Result: 学生认为AI聊天机器人能提供反馈和学习支持，但也担忧学术诚信、信息准确性和批判性思维丧失等问题。

Conclusion: 建议制定明确的AI使用政策并开展AI素养教育，以充分利用AI潜力同时保障学习过程的完整性。

Abstract: The use of chatbots equipped with artificial intelligence (AI) in educational
settings has increased in recent years, showing potential to support teaching
and learning. However, the adoption of these technologies has raised concerns
about their impact on academic integrity, students' ability to problem-solve
independently, and potential underlying biases. To better understand students'
perspectives and experiences with these tools, a survey was conducted at a
large public university in the United States. Through thematic analysis, 262
undergraduate students' responses regarding their perceived benefits and risks
of AI chatbots in education were identified and categorized into themes.
  The results discuss several benefits identified by the students, with
feedback and study support, instruction capabilities, and access to information
being the most cited. Their primary concerns included risks to academic
integrity, accuracy of information, loss of critical thinking skills, the
potential development of overreliance, and ethical considerations such as data
privacy, system bias, environmental impact, and preservation of human elements
in education.
  While student perceptions align with previously discussed benefits and risks
of AI in education, they show heightened concerns about distinguishing between
human and AI generated work - particularly in cases where authentic work is
flagged as AI-generated. To address students' concerns, institutions can
establish clear policies regarding AI use and develop curriculum around AI
literacy. With these in place, practitioners can effectively develop and
implement educational systems that leverage AI's potential in areas such as
immediate feedback and personalized learning support. This approach can enhance
the quality of students' educational experiences while preserving the integrity
of the learning process with AI.

</details>

### [486] [What Is AI Safety? What Do We Want It to Be?](https://arxiv.org/abs/2505.02313)
*Jacqueline Harding,Cameron Domenico Kirk-Giannini*

Main category: cs.CY

TLDR: 论文探讨了AI安全的定义，提出了一种简单的‘安全概念’，并分析了其与当前AI安全研究趋势的冲突。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于澄清AI安全领域的核心特征，避免因定义模糊导致研究方向的偏差。

Method: 采用概念工程的方法，分析‘安全概念’与当前趋势的冲突，并提出其优越性。

Result: ‘安全概念’能更全面地涵盖AI安全研究，避免对研究主题的武断区分。

Conclusion: 结论是‘安全概念’是AI安全领域的最佳定义，应以其为基础推进研究。

Abstract: The field of AI safety seeks to prevent or reduce the harms caused by AI
systems. A simple and appealing account of what is distinctive of AI safety as
a field holds that this feature is constitutive: a research project falls
within the purview of AI safety just in case it aims to prevent or reduce the
harms caused by AI systems. Call this appealingly simple account The Safety
Conception of AI safety. Despite its simplicity and appeal, we argue that The
Safety Conception is in tension with at least two trends in the ways AI safety
researchers and organizations think and talk about AI safety: first, a tendency
to characterize the goal of AI safety research in terms of catastrophic risks
from future systems; second, the increasingly popular idea that AI safety can
be thought of as a branch of safety engineering. Adopting the methodology of
conceptual engineering, we argue that these trends are unfortunate: when we
consider what concept of AI safety it would be best to have, there are
compelling reasons to think that The Safety Conception is the answer.
Descriptively, The Safety Conception allows us to see how work on topics that
have historically been treated as central to the field of AI safety is
continuous with work on topics that have historically been treated as more
marginal, like bias, misinformation, and privacy. Normatively, taking The
Safety Conception seriously means approaching all efforts to prevent or
mitigate harms from AI systems based on their merits rather than drawing
arbitrary distinctions between them.

</details>

<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [487] [Inverse Modeling of Dielectric Response in Time Domain using Physics-Informed Neural Networks](https://arxiv.org/abs/2505.02258)
*Emir Esenov,Olof Hjortstam,Yuriy Serdyuk,Thomas Hammarström,Christian Häger*

Main category: eess.SY

TLDR: 该论文探讨了利用物理信息神经网络（PINNs）对时域介电响应进行逆建模的方法，通过并行RC电路分析介电材料的极化与传导过程，并验证了其在噪声数据下的有效性。


<details>
  <summary>Details</summary>
Motivation: 介电响应（DR）是设计电气绝缘系统和定义高压设备安全运行条件的关键输入信息，但由于不同时间尺度的极化和传导过程，物理解释原始测量数据具有挑战性。

Method: 使用物理信息神经网络（PINNs）对时域介电响应进行逆建模，通过并行RC电路模拟介电材料的响应，并在合成数据上测试其性能。

Result: PINNs能有效解决条件良好的逆问题，准确估计多达五个未知RC参数，且对神经网络规模、训练时间和超参数调优要求低。此外，PINNs还能从噪声数据中准确恢复非线性温度函数。

Conclusion: 该研究展示了PINNs在基于等效电路模型的科学计算中的广泛应用潜力，尤其是在介电响应建模领域。

Abstract: Dielectric response (DR) of insulating materials is key input information for
designing electrical insulation systems and defining safe operating conditions
of various HV devices. In dielectric materials, different polarization and
conduction processes occur at different time scales, making it challenging to
physically interpret raw measured data. To analyze DR measurement results,
equivalent circuit models (ECMs) are commonly used, reducing the complexity of
the physical system to a number of circuit elements that capture the dominant
response. This paper examines the use of physics-informed neural networks
(PINNs) for inverse modeling of DR in time domain using parallel RC circuits.
To assess their performance, we test PINNs on synthetic data generated from
analytical solutions of corresponding ECMs, incorporating Gaussian noise to
simulate measurement errors. Our results show that PINNs are highly effective
at solving well-conditioned inverse problems, accurately estimating up to five
unknown RC parameters with minimal requirements on neural network size,
training duration, and hyperparameter tuning. Furthermore, we extend the ECMs
to incorporate temperature dependence and demonstrate that PINNs can accurately
recover embedded, nonlinear temperature functions from noisy DR data sampled at
different temperatures. This case study in modeling DR in time domain presents
a solution with wide-ranging potential applications in disciplines relying on
ECMs, utilizing the latest technology in machine learning for scientific
computation.

</details>

<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [488] [The Voynich Codex Decoded: Statistical Symbolism and Scroll-Wide Logic](https://arxiv.org/abs/2505.02261)
*Suhaib A. Jama*

Main category: cs.SC

TLDR: 该论文提出了一种基于数学节奏、符号变换和字形递归的Voynich手稿结构化解码框架，通过结构角色和空间节奏解码符号，而非语音解释。


<details>
  <summary>Details</summary>
Motivation: 挑战关于前语音手稿的假设，提出一种新的符号逻辑解释视角。

Method: 使用滚动范围测序，跟踪素数分组、斐波那契聚类和黄金比例对齐，并通过十部分卡方检验和布尔逻辑验证符号结构。

Result: 方法具有可证伪性和可重复性，展示了字形流、呼吸段模式和三点点对齐。

Conclusion: 该解码策略为解读符号逻辑提供了新视角。

Abstract: This paper introduces a structured decoding framework for the Voynich
Manuscript, based on mathematical rhythm, symbolic transformation, and
glyph-level recursion. Rather than interpret symbols phonetically, this method
decodes them by structural roles and spatial pacing. Using scroll-wide
sequencing, the system tracks prime number grouping, Fibonacci clustering, and
golden ratio alignment. These symbolic structures are validated using a
ten-part chi-squared test suite and Boolean logic. The method is falsifiable
and reproducible. Scroll sections like f57v, f88v, and f91r are used to
demonstrate glyph flow, breath-segment patterns, and tri-dot alignment. This
decoding strategy challenges assumptions about pre-phonetic manuscripts and
proposes a new lens for interpreting symbolic logic.

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [489] [Scalable Speed-ups for the SMS-EMOA from a Simple Aging Strategy](https://arxiv.org/abs/2505.01647)
*Mingfeng Li,Weijie Zheng,Benjamin Doerr*

Main category: cs.NE

TLDR: 本文提出了一种基于年龄的非精英选择机制，解决了多目标进化算法中随机选择的不足，证明了其在计算帕累托前沿时的加速效果。


<details>
  <summary>Details</summary>
Motivation: 多目标进化算法通常采用贪婪选择，而随机选择机制虽能加速计算，但仅适用于特定条件。本文旨在提出一种更通用的非精英选择机制。

Method: 提出基于年龄的非精英选择机制，豁免年轻个体被移除，以改进随机选择的局限性。

Result: 证明新机制在计算帕累托前沿时可加速，且适用于任意目标数量，尤其在常数k时仍有效。

Conclusion: 基于年龄的非精英选择机制比随机选择更强大，支持非精英选择在多目标优化中的应用。

Abstract: Different from single-objective evolutionary algorithms, where non-elitism is
an established concept, multi-objective evolutionary algorithms almost always
select the next population in a greedy fashion. In the only notable exception,
Bian, Zhou, Li, and Qian (IJCAI 2023) proposed a stochastic selection mechanism
for the SMS-EMOA and proved that it can speed up computing the Pareto front of
the bi-objective jump benchmark with problem size $n$ and gap parameter $k$ by
a factor of $\max\{1,2^{k/4}/n\}$. While this constitutes the first proven
speed-up from non-elitist selection, suggesting a very interesting research
direction, it has to be noted that a true speed-up only occurs for $k \ge
4\log_2(n)$, where the runtime is super-polynomial, and that the advantage
reduces for larger numbers of objectives as shown in a later work. In this
work, we propose a different non-elitist selection mechanism based on aging,
which exempts individuals younger than a certain age from a possible removal.
This remedies the two shortcomings of stochastic selection: We prove a speed-up
by a factor of $\max\{1,\Theta(k)^{k-1}\}$, regardless of the number of
objectives. In particular, a positive speed-up can already be observed for
constant $k$, the only setting for which polynomial runtimes can be witnessed.
Overall, this result supports the use of non-elitist selection schemes, but
suggests that aging-based mechanisms can be considerably more powerful than
stochastic selection mechanisms.

</details>

### [490] [PASCAL: Precise and Efficient ANN- SNN Conversion using Spike Accumulation and Adaptive Layerwise Activation](https://arxiv.org/abs/2505.01730)
*Pranav Ramesh,Gopalakrishnan Srinivasan*

Main category: cs.NE

TLDR: PASCAL方法通过数学等效的ANN-SNN转换和层间量化步长配置，显著减少SNN推理时间步数，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决SNN在ANN-SNN转换中因需要大量时间步数而导致的效率问题。

Method: 提出PASCAL方法，实现数学等效的ANN-SNN转换，并优化QCFS激活的量化步长配置。

Result: ResNet-34 SNN在ImageNet上达到74%准确率，推理时间步数减少64倍。

Conclusion: PASCAL方法显著提升了SNN的效率和实用性。

Abstract: Spiking Neural Networks (SNNs) have been put forward as an energy-efficient
alternative to Artificial Neural Networks (ANNs) since they perform sparse
Accumulate operations instead of the power-hungry Multiply-and-Accumulate
operations. ANN-SNN conversion is a widely used method to realize deep SNNs
with accuracy comparable to that of ANNs.~\citeauthor{bu2023optimal} recently
proposed the Quantization-Clip-Floor-Shift (QCFS) activation as an alternative
to ReLU to minimize the accuracy loss during ANN-SNN conversion. Nevertheless,
SNN inferencing requires a large number of timesteps to match the accuracy of
the source ANN for real-world datasets. In this work, we propose PASCAL, which
performs ANN-SNN conversion in such a way that the resulting SNN is
mathematically equivalent to an ANN with QCFS-activation, thereby yielding
similar accuracy as the source ANN with minimal inference timesteps. In
addition, we propose a systematic method to configure the quantization step of
QCFS activation in a layerwise manner, which effectively determines the optimal
number of timesteps per layer for the converted SNN. Our results show that the
ResNet-34 SNN obtained using PASCAL achieves an accuracy of $\approx$74\% on
ImageNet with a 64$\times$ reduction in the number of inference timesteps
compared to existing approaches.

</details>

### [491] [Meta-Black-Box-Optimization through Offline Q-function Learning](https://arxiv.org/abs/2505.02010)
*Zeyuan Ma,Zhiguang Cao,Zhou Jiang,Hongshu Guo,Yue-Jiao Gong*

Main category: cs.NE

TLDR: Q-Mamba是一种基于离线学习的MetaBBO框架，通过长序列决策和Q函数分解机制提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有MetaBBO方法依赖在线学习，效率低下，Q-Mamba旨在通过离线学习解决这一问题。

Method: 将动态算法配置任务转化为长序列决策过程，引入Q函数分解机制，并采用三种新设计：离线数据集构建策略、分解Q-loss和Mamba架构。

Result: Q-Mamba在性能上优于现有在线/离线基线，同时显著提升训练效率。

Conclusion: Q-Mamba为MetaBBO提供了一种高效且有效的离线学习解决方案。

Abstract: Recent progress in Meta-Black-Box-Optimization (MetaBBO) has demonstrated
that using RL to learn a meta-level policy for dynamic algorithm configuration
(DAC) over an optimization task distribution could significantly enhance the
performance of the low-level BBO algorithm. However, the online learning
paradigms in existing works makes the efficiency of MetaBBO problematic. To
address this, we propose an offline learning-based MetaBBO framework in this
paper, termed Q-Mamba, to attain both effectiveness and efficiency in MetaBBO.
Specifically, we first transform DAC task into long-sequence decision process.
This allows us further introduce an effective Q-function decomposition
mechanism to reduce the learning difficulty within the intricate algorithm
configuration space. Under this setting, we propose three novel designs to
meta-learn DAC policy from offline data: we first propose a novel collection
strategy for constructing offline DAC experiences dataset with balanced
exploration and exploitation. We then establish a decomposition-based Q-loss
that incorporates conservative Q-learning to promote stable offline learning
from the offline dataset. To further improve the offline learning efficiency,
we equip our work with a Mamba architecture which helps long-sequence learning
effectiveness and efficiency by selective state model and hardware-aware
parallel scan respectively. Through extensive benchmarking, we observe that
Q-Mamba achieves competitive or even superior performance to prior
online/offline baselines, while significantly improving the training efficiency
of existing online baselines. We provide sourcecodes of Q-Mamba at
https://github.com/MetaEvo/Q-Mamba.

</details>