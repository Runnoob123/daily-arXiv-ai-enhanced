<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 28]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.CL](#cs.CL) [Total: 35]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.CV](#cs.CV) [Total: 66]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 6]
- [cs.IR](#cs.IR) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [eess.IV](#eess.IV) [Total: 11]
- [cs.NI](#cs.NI) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.RO](#cs.RO) [Total: 8]
- [quant-ph](#quant-ph) [Total: 5]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [econ.TH](#econ.TH) [Total: 1]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent](https://arxiv.org/abs/2504.13192)
*Liang-bo Ning,Shijie Wang,Wenqi Fan,Qing Li,Xin Xu,Hao Chen,Feiran Huang*

Main category: cs.CR

TLDR: 提出了一种名为CheatAgent的攻击框架，利用LLM的能力攻击LLM赋能的推荐系统，通过最小输入修改和迭代优化实现高效攻击。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM赋能的推荐系统在个性化用户体验方面取得了显著进展，但其安全漏洞研究仍不足。考虑到安全和隐私问题，研究黑盒推荐系统的攻击更具实际意义。

Method: 提出CheatAgent框架，利用LLM生成对抗性扰动，并通过提示调优技术迭代优化攻击策略。

Result: 在三个真实数据集上的实验证明了该攻击方法的有效性。

Conclusion: LLM可以作为攻击代理，为推荐系统的安全研究提供了新视角。

Abstract: Recently, Large Language Model (LLM)-empowered recommender systems (RecSys)
have brought significant advances in personalized user experience and have
attracted considerable attention. Despite the impressive progress, the research
question regarding the safety vulnerability of LLM-empowered RecSys still
remains largely under-investigated. Given the security and privacy concerns, it
is more practical to focus on attacking the black-box RecSys, where attackers
can only observe the system's inputs and outputs. However, traditional attack
approaches employing reinforcement learning (RL) agents are not effective for
attacking LLM-empowered RecSys due to the limited capabilities in processing
complex textual inputs, planning, and reasoning. On the other hand, LLMs
provide unprecedented opportunities to serve as attack agents to attack RecSys
because of their impressive capability in simulating human-like decision-making
processes. Therefore, in this paper, we propose a novel attack framework called
CheatAgent by harnessing the human-like capabilities of LLMs, where an
LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our
method first identifies the insertion position for maximum impact with minimal
input modification. After that, the LLM agent is designed to generate
adversarial perturbations to insert at target positions. To further improve the
quality of generated perturbations, we utilize the prompt tuning technique to
improve attacking strategies via feedback from the victim RecSys iteratively.
Extensive experiments across three real-world datasets demonstrate the
effectiveness of our proposed attacking method.

</details>

### [2] [Investigating cybersecurity incidents using large language models in latest-generation wireless networks](https://arxiv.org/abs/2504.13196)
*Leonid Legashev,Arthur Zhigalov*

Main category: cs.CR

TLDR: 研究基于现代生成模型检测网络安全事件，分析决策支持及评估应对信息安全威胁措施的有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用生成模型提升网络安全事件的检测与分析能力，特别是在最新一代无线网络中。

Method: 包括MIMO系统信号传播数据仿真、对抗样本合成、对抗攻击执行、大语言模型微调及基于提示技术的决策解释。

Result: Gemma-7b模型在对抗攻击检测中表现最佳（Precision=0.89, Recall=0.89, F1-Score=0.89），并能解释决策和提供缓解建议。

Conclusion: 大语言模型与网络威胁二元分类器结合，在网络安全事件调查和决策支持中具有显著应用潜力。

Abstract: The purpose of research: Detection of cybersecurity incidents and analysis of
decision support and assessment of the effectiveness of measures to counter
information security threats based on modern generative models. The methods of
research: Emulation of signal propagation data in MIMO systems, synthesis of
adversarial examples, execution of adversarial attacks on machine learning
models, fine tuning of large language models for detecting adversarial attacks,
explainability of decisions on detecting cybersecurity incidents based on the
prompts technique. Scientific novelty: A binary classification of data
poisoning attacks was performed using large language models, and the
possibility of using large language models for investigating cybersecurity
incidents in the latest generation wireless networks was investigated. The
result of research: Fine-tuning of large language models was performed on the
prepared data of the emulated wireless network segment. Six large language
models were compared for detecting adversarial attacks, and the capabilities of
explaining decisions made by a large language model were investigated. The
Gemma-7b model showed the best results according to the metrics Precision =
0.89, Recall = 0.89 and F1-Score = 0.89. Based on various explainability
prompts, the Gemma-7b model notes inconsistencies in the compromised data under
study, performs feature importance analysis and provides various
recommendations for mitigating the consequences of adversarial attacks. Large
language models integrated with binary classifiers of network threats have
significant potential for practical application in the field of cybersecurity
incident investigation, decision support and assessing the effectiveness of
measures to counter information security threats.

</details>

### [3] [Overcoming Bottlenecks in Homomorphic Encryption for the 2024 Mexican Federal Election](https://arxiv.org/abs/2504.13198)
*Eric Landquist,Nimit Sawhney,Simer Sawhney*

Main category: cs.CR

TLDR: 墨西哥2024年联邦选举首次允许海外公民通过在线方式投票，文章介绍了支持该选举的技术和加密工具。


<details>
  <summary>Details</summary>
Motivation: 为海外墨西哥公民提供便捷的投票方式，并确保选举的安全性和快速结果生成。

Method: 使用网络应用程序和电子投票亭，结合加密技术保障投票安全。

Result: 144,734名海外公民参与投票，其中122,496人通过个人设备，22,238人通过投票亭。

Conclusion: 该技术方案可扩展至全国范围的选举，展示了在线投票的可行性和效率。

Abstract: On June 2, 2024, Mexico held its federal elections. The majority of Mexican
citizens voted in person at the polls in this historic election. For the first
time though, Mexican citizens living outside their country were able to vote
online via a web app, either on a personal device or using an electronic voting
kiosk at one of 23 embassies and consulates in the U.S., Canada, and Europe. In
total, 144,734 people voted outside of Mexico: 122,496 on a personal device and
22,238 in-person at a kiosk. Voting was open for remote voting from 8PM, May
18, 2024 to 6PM, June 2, 2024 and was open for in-person voting from 8AM-6PM on
June 2, 2024. This article describes the technical and cryptographic tools
applied to secure the ex-patriate component of the election and to enable INE
(Mexico's National Electoral Institute) to generate provable election results
within minutes of the close of the election. This article will also describe
how the solutions we present scale to elections on a national level.

</details>

### [4] [Building Trustworthy Multimodal AI: A Review of Fairness, Transparency, and Ethics in Vision-Language Tasks](https://arxiv.org/abs/2504.13199)
*Mohammad Saleha,Azadeh Tabatabaeib*

Main category: cs.CR

TLDR: 本文综述了多模态AI系统（特别是视觉-语言任务）的可信度问题，探讨了公平性、透明度和伦理挑战，并比较了VQA、图像描述和视觉对话等任务。


<details>
  <summary>Details</summary>
Motivation: 多模态模型通过整合视觉和文本数据模拟人类学习，但其可信度问题（如公平性、透明度和伦理）亟待解决。

Method: 综述了2017-2024年的相关研究，采用比较分析方法，从可信度角度评估任务，并总结趋势与挑战。

Result: 关键发现包括：透明度（注意力图等方法提升解释性）、公平性（需减少VQA和视觉对话中的偏见）和伦理（多语言模型的偏见与数据伦理）。

Conclusion: 强调在开发视觉-语言模型时，需将公平性、透明度和伦理纳入统一框架。

Abstract: Objective: This review explores the trustworthiness of multimodal artificial
intelligence (AI) systems, specifically focusing on vision-language tasks. It
addresses critical challenges related to fairness, transparency, and ethical
implications in these systems, providing a comparative analysis of key tasks
such as Visual Question Answering (VQA), image captioning, and visual dialogue.
Background: Multimodal models, particularly vision-language models, enhance
artificial intelligence (AI) capabilities by integrating visual and textual
data, mimicking human learning processes. Despite significant advancements, the
trustworthiness of these models remains a crucial concern, particularly as AI
systems increasingly confront issues regarding fairness, transparency, and
ethics. Methods: This review examines research conducted from 2017 to 2024
focusing on forenamed core vision-language tasks. It employs a comparative
approach to analyze these tasks through the lens of trustworthiness,
underlining fairness, explainability, and ethics. This study synthesizes
findings from recent literature to identify trends, challenges, and
state-of-the-art solutions. Results: Several key findings were highlighted.
Transparency: Explainability of vision language tasks is important for user
trust. Techniques, such as attention maps and gradient-based methods, have
successfully addressed this issue. Fairness: Bias mitigation in VQA and visual
dialogue systems is essential for ensuring unbiased outcomes across diverse
demographic groups. Ethical Implications: Addressing biases in multilingual
models and ensuring ethical data handling is critical for the responsible
deployment of vision-language systems. Conclusion: This study underscores the
importance of integrating fairness, transparency, and ethical considerations in
developing vision-language models within a unified framework.

</details>

### [5] [Concept Enhancement Engineering: A Lightweight and Efficient Robust Defense Against Jailbreak Attacks in Embodied AI](https://arxiv.org/abs/2504.13201)
*Jirui Yang,Zheyu Lin,Shuhan Yang,Zhihui Lu,Xin Du*

Main category: cs.CR

TLDR: 论文提出了一种名为概念增强工程（CEE）的新防御框架，通过动态引导大型语言模型（LLM）的内部激活来增强其安全性，有效应对越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御策略（如输入过滤和输出监控）在实时场景中计算开销高或影响任务性能，亟需一种高效且不影响性能的安全机制。

Method: CEE通过提取多语言安全模式、构建安全对齐的概念子空间控制方向，并应用子空间概念旋转来强化推理时的安全行为。

Result: 实验表明，CEE能有效减轻越狱攻击，同时保持任务性能，在鲁棒性和效率上优于现有防御方法。

Conclusion: CEE为具身AI提供了一种可扩展且可解释的安全机制，填补了理论表示工程与实际安全应用之间的空白。

Abstract: Embodied Intelligence (EI) systems integrated with large language models
(LLMs) face significant security risks, particularly from jailbreak attacks
that manipulate models into generating harmful outputs or executing unsafe
physical actions. Traditional defense strategies, such as input filtering and
output monitoring, often introduce high computational overhead or interfere
with task performance in real-time embodied scenarios. To address these
challenges, we propose Concept Enhancement Engineering (CEE), a novel defense
framework that leverages representation engineering to enhance the safety of
embodied LLMs by dynamically steering their internal activations. CEE operates
by (1) extracting multilingual safety patterns from model activations, (2)
constructing control directions based on safety-aligned concept subspaces, and
(3) applying subspace concept rotation to reinforce safe behavior during
inference. Our experiments demonstrate that CEE effectively mitigates jailbreak
attacks while maintaining task performance, outperforming existing defense
methods in both robustness and efficiency. This work contributes a scalable and
interpretable safety mechanism for embodied AI, bridging the gap between
theoretical representation engineering and practical security applications. Our
findings highlight the potential of latent-space interventions as a viable
defense paradigm against emerging adversarial threats in physically grounded AI
systems.

</details>

### [6] [X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents](https://arxiv.org/abs/2504.13203)
*Salman Rahman,Liwei Jiang,James Shiffer,Genglin Liu,Sheriff Issaka,Md Rizwan Parvez,Hamid Palangi,Kai-Wei Chang,Yejin Choi,Saadia Gabriel*

Main category: cs.CR

TLDR: X-Teaming是一个可扩展的框架，用于探索多轮对话中的安全风险，并生成攻击场景，成功率达到98.1%。


<details>
  <summary>Details</summary>
Motivation: 多轮对话中的安全风险尚未充分研究，现有方法主要关注单轮攻击。

Method: X-Teaming采用协作代理进行规划、攻击优化和验证，生成多样化的攻击场景。

Result: X-Teaming在多轮攻击中表现优异，成功率达98.1%，并针对Claude 3.7 Sonnet模型达到96.2%。

Conclusion: X-Teaming和XGuard-Train为提升语言模型的多轮安全性提供了重要工具和数据支持。

Abstract: Multi-turn interactions with language models (LMs) pose critical safety
risks, as harmful intent can be strategically spread across exchanges. Yet, the
vast majority of prior work has focused on single-turn safety, while
adaptability and diversity remain among the key challenges of multi-turn
red-teaming. To address these challenges, we present X-Teaming, a scalable
framework that systematically explores how seemingly harmless interactions
escalate into harmful outcomes and generates corresponding attack scenarios.
X-Teaming employs collaborative agents for planning, attack optimization, and
verification, achieving state-of-the-art multi-turn jailbreak effectiveness and
diversity with success rates up to 98.1% across representative leading
open-weight and closed-source models. In particular, X-Teaming achieves a 96.2%
attack success rate against the latest Claude 3.7 Sonnet model, which has been
considered nearly immune to single-turn attacks. Building on X-Teaming, we
introduce XGuard-Train, an open-source multi-turn safety training dataset that
is 20x larger than the previous best resource, comprising 30K interactive
jailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our
work offers essential tools and insights for mitigating sophisticated
conversational attacks, advancing the multi-turn safety of LMs.

</details>

### [7] [On-Device Watermarking: A Socio-Technical Imperative For Authenticity In The Age of Generative AI](https://arxiv.org/abs/2504.13205)
*Houssam Kherraz*

Main category: cs.CR

TLDR: 论文主张通过硬件层加密签名标记可信内容，而非AI生成内容，认为这是更可行的方向。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI输出越来越逼真，检测AI生成内容的需求增加，但现有水印技术存在严重局限。

Method: 提出通过硬件传感器捕获的真实内容进行加密签名，借鉴HTTPS认证和蓝光验证协议。

Result: 硬件认证为政策层面提供了更可行的路径，尤其在视听内容领域。

Conclusion: 建议研究重点转向文本和LLM领域的水印技术，因其无法追溯到物理传感器。

Abstract: As generative AI models produce increasingly realistic output, both academia
and industry are focusing on the ability to detect whether an output was
generated by an AI model or not. Many of the research efforts and policy
discourse are centered around robust watermarking of AI outputs. While plenty
of progress has been made, all watermarking and AI detection techniques face
severe limitations. In this position paper, we argue that we are adopting the
wrong approach, and should instead focus on watermarking via cryptographic
signatures trustworthy content rather than AI generated ones. For audio-visual
content, in particular, all real content is grounded in the physical world and
captured via hardware sensors. This presents a unique opportunity to watermark
at the hardware layer, and we lay out a socio-technical framework and draw
parallels with HTTPS certification and Blu-Ray verification protocols. While
acknowledging implementation challenges, we contend that hardware-based
authentication offers a more tractable path forward, particularly from a policy
perspective. As generative models approach perceptual indistinguishability, the
research community should be wary of being overly optimistic with AI
watermarking, and we argue that AI watermarking research efforts are better
spent in the text and LLM space, which are ultimately not traceable to a
physical sensor.

</details>

### [8] [On the Feasibility of Using MultiModal LLMs to Execute AR Social Engineering Attacks](https://arxiv.org/abs/2504.13209)
*Ting Bi,Chenghang Ye,Zheyu Yang,Ziyi Zhou,Cui Tang,Jun Zhang,Zui Tao,Kailong Wang,Liting Zhou,Yang Yang,Tianlong Yu*

Main category: cs.CR

TLDR: 本文提出SEAR框架，首次系统研究AR与多模态LLM结合的社会工程攻击可行性，通过三个阶段实现高效攻击，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: AR与多模态LLM的快速发展为攻击者提供了新的攻击面，本文旨在探索其结合用于社会工程攻击的可行性。

Method: 提出SEAR框架，包括AR社交上下文合成、角色多模态RAG和ReInteract攻击代理三阶段。

Result: 实验显示SEAR高效诱导高风险行为（如93.3%参与者易受钓鱼邮件攻击），85%目标愿意接受攻击者电话。

Conclusion: SEAR为AR-LLM驱动的社会工程攻击提供了概念验证，并为防御下一代AR威胁提供了见解。

Abstract: Augmented Reality (AR) and Multimodal Large Language Models (LLMs) are
rapidly evolving, providing unprecedented capabilities for human-computer
interaction. However, their integration introduces a new attack surface for
social engineering. In this paper, we systematically investigate the
feasibility of orchestrating AR-driven Social Engineering attacks using
Multimodal LLM for the first time, via our proposed SEAR framework, which
operates through three key phases: (1) AR-based social context synthesis, which
fuses Multimodal inputs (visual, auditory and environmental cues); (2)
role-based Multimodal RAG (Retrieval-Augmented Generation), which dynamically
retrieves and integrates contextual data while preserving character
differentiation; and (3) ReInteract social engineering agents, which execute
adaptive multiphase attack strategies through inference interaction loops. To
verify SEAR, we conducted an IRB-approved study with 60 participants in three
experimental configurations (unassisted, AR+LLM, and full SEAR pipeline)
compiling a new dataset of 180 annotated conversations in simulated social
scenarios. Our results show that SEAR is highly effective at eliciting
high-risk behaviors (e.g., 93.3% of participants susceptible to email
phishing). The framework was particularly effective in building trust, with 85%
of targets willing to accept an attacker's call after an interaction. Also, we
identified notable limitations such as ``occasionally artificial'' due to
perceived authenticity gaps. This work provides proof-of-concept for AR-LLM
driven social engineering attacks and insights for developing defensive
countermeasures against next-generation augmented reality threats.

</details>

### [9] [I Know What You Bought Last Summer: Investigating User Data Leakage in E-Commerce Platforms](https://arxiv.org/abs/2504.13212)
*Ioannis Vlachogiannakis,Emmanouil Papadogiannakis,Panagiotis Papadopoulos,Nicolas Kourtellis,Evangelos Markatos*

Main category: cs.CR

TLDR: 研究发现，30%的电商平台存在隐私泄露问题，用户数据被共享给第三方如Facebook。


<details>
  <summary>Details</summary>
Motivation: 探讨电商平台中用户隐私泄露的风险及其对消费者的影响。

Method: 采用半自动数据收集方法，分析多个热门电商平台的数据共享行为。

Result: 近30%的电商平台将用户个人信息泄露给第三方，尤其是Facebook。

Conclusion: 电商平台需加强隐私保护措施，防止用户数据被滥用。

Abstract: In the digital age, e-commerce has transformed the way consumers shop,
offering convenience and accessibility. Nevertheless, concerns about the
privacy and security of personal information shared on these platforms have
risen. In this work, we investigate user privacy violations, noting the risks
of data leakage to third-party entities. Utilizing a semi-automated data
collection approach, we examine a selection of popular online e-shops,
revealing that nearly 30% of them violate user privacy by disclosing personal
information to third parties. We unveil how minimal user interaction across
multiple e-commerce websites can result in a comprehensive privacy breach. We
observe significant data-sharing patterns with platforms like Facebook, which
use personal information to build user profiles and link them to social media
accounts.

</details>

### [10] [Leveraging Functional Encryption and Deep Learning for Privacy-Preserving Traffic Forecasting](https://arxiv.org/abs/2504.13267)
*Isaac Adom,Mohammmad Iqbal Hossain,Hassan Mahmoud,Ahmad Alsharif,Mahmoud Nabil Mahmoud,Yang Xiao*

Main category: cs.CR

TLDR: 提出了一种基于k-匿名和功能加密的隐私保护交通预测系统，结合Conv-LSTM和Bi-LSTM模型，实现了高精度预测和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 交通拥堵带来诸多负面影响，智能交通系统需在提升效率的同时保护驾驶员隐私。

Method: 采用k-匿名和功能加密保护位置数据，结合Conv-LSTM和Bi-LSTM模型进行时空特征提取和预测。

Result: 在真实数据集上验证，60分钟预测的平均绝对误差低于10%，同时保护隐私。

Conclusion: 系统在保证隐私的前提下，实现了高精度的交通预测。

Abstract: Over the past few years, traffic congestion has continuously plagued the
nation's transportation system creating several negative impacts including
longer travel times, increased pollution rates, and higher collision risks. To
overcome these challenges, Intelligent Transportation Systems (ITS) aim to
improve mobility and vehicular systems, ensuring higher levels of safety by
utilizing cutting-edge technologies, sophisticated sensing capabilities, and
innovative algorithms. Drivers' participatory sensing, current/future location
reporting, and machine learning algorithms have considerably improved real-time
congestion monitoring and future traffic management. However, each driver's
sensitive spatiotemporal location information can create serious privacy
concerns. To address these challenges, we propose in this paper a secure,
privacy-preserving location reporting and traffic forecasting system that
guarantees privacy protection of driver data while maintaining high traffic
forecasting accuracy. Our novel k-anonymity scheme utilizes functional
encryption to aggregate encrypted location information submitted by drivers
while ensuring the privacy of driver location data. Additionally, using the
aggregated encrypted location information as input, this research proposes a
deep learning model that incorporates a Convolutional-Long Short-Term Memory
(Conv-LSTM) module to capture spatial and short-term temporal features and a
Bidirectional Long Short-Term Memory (Bi-LSTM) module to recover long-term
periodic patterns for traffic forecasting. With extensive evaluation on real
datasets, we demonstrate the effectiveness of the proposed scheme with less
than 10% mean absolute error for a 60-minute forecasting horizon, all while
protecting driver privacy.

</details>

### [11] [DYNAMITE: Dynamic Defense Selection for Enhancing Machine Learning-based Intrusion Detection Against Adversarial Attacks](https://arxiv.org/abs/2504.13301)
*Jing Chen,Onat Gungor,Zhengli Shang,Elvin Li,Tajana Rosing*

Main category: cs.CR

TLDR: Dynamite是一种动态防御选择框架，通过机器学习驱动的选择机制，显著提升ML-IDS的防御能力，减少计算时间并提高性能。


<details>
  <summary>Details</summary>
Motivation: 物联网（IoT）的快速发展带来了安全漏洞，而现有的ML-IDS容易受到对抗攻击，缺乏针对特定攻击选择最有效防御的系统性方法。

Method: 提出Dynamite框架，利用机器学习智能选择和部署最适合的防御机制。

Result: Dynamite减少了96.2%的计算时间，F1分数比随机防御提高了76.7%，比最佳静态防御提高了65.8%。

Conclusion: Dynamite为ML-IDS提供了一种高效、动态的防御选择方案，显著提升了对抗攻击的防御能力。

Abstract: The rapid proliferation of the Internet of Things (IoT) has introduced
substantial security vulnerabilities, highlighting the need for robust
Intrusion Detection Systems (IDS). Machine learning-based intrusion detection
systems (ML-IDS) have significantly improved threat detection capabilities;
however, they remain highly susceptible to adversarial attacks. While numerous
defense mechanisms have been proposed to enhance ML-IDS resilience, a
systematic approach for selecting the most effective defense against a specific
adversarial attack remains absent. To address this challenge, we propose
Dynamite, a dynamic defense selection framework that enhances ML-IDS by
intelligently identifying and deploying the most suitable defense using a
machine learning-driven selection mechanism. Our results demonstrate that
Dynamite achieves a 96.2% reduction in computational time compared to the
Oracle, significantly decreasing computational overhead while preserving strong
prediction performance. Dynamite also demonstrates an average F1-score
improvement of 76.7% over random defense and 65.8% over the best static
state-of-the-art defense.

</details>

### [12] [GraphQLer: Enhancing GraphQL Security with Context-Aware API Testing](https://arxiv.org/abs/2504.13358)
*Omar Tsai,Jianing Li,Tsz Tung Cheung,Lejing Huang,Hao Zhu,Jianrui Xiao,Iman Sharafaldin,Mohammad A. Tayebi*

Main category: cs.CR

TLDR: GraphQLer是一个针对GraphQL API的安全测试框架，通过上下文感知和依赖图分析，显著提升测试覆盖率和漏洞检测能力。


<details>
  <summary>Details</summary>
Motivation: GraphQL的动态执行模型和缺乏内置安全机制导致其易受攻击，现有测试工具多关注功能正确性，忽视安全风险。

Method: GraphQLer构建依赖图分析查询和对象间关系，链式测试揭示认证、授权和资源滥用问题，并跟踪资源使用情况。

Result: 测试覆盖率平均提升35%，最高达84%，且耗时更少，成功检测已知CVE和潜在漏洞。

Conclusion: GraphQLer通过自动化、上下文感知的漏洞检测，有效提升GraphQL API的安全性。

Abstract: GraphQL is an open-source data query and manipulation language for web
applications, offering a flexible alternative to RESTful APIs. However, its
dynamic execution model and lack of built-in security mechanisms expose it to
vulnerabilities such as unauthorized data access, denial-of-service (DoS)
attacks, and injections. Existing testing tools focus on functional
correctness, often overlooking security risks stemming from query
interdependencies and execution context. This paper presents GraphQLer, the
first context-aware security testing framework for GraphQL APIs. GraphQLer
constructs a dependency graph to analyze relationships among mutations,
queries, and objects, capturing critical interdependencies. It chains related
queries and mutations to reveal authentication and authorization flaws, access
control bypasses, and resource misuse. Additionally, GraphQLer tracks internal
resource usage to uncover data leakage, privilege escalation, and replay attack
vectors. We assess GraphQLer on various GraphQL APIs, demonstrating improved
testing coverage - averaging a 35% increase, with up to 84% in some cases -
compared to top-performing baselines. Remarkably, this is achieved in less
time, making GraphQLer suitable for time-sensitive contexts. GraphQLer also
successfully detects a known CVE and potential vulnerabilities in large-scale
production APIs. These results underline GraphQLer's utility in proactively
securing GraphQL APIs through automated, context-aware vulnerability detection.

</details>

### [13] [The Impact of AI on the Cyber Offense-Defense Balance and the Character of Cyber Conflict](https://arxiv.org/abs/2504.13371)
*Andrew J. Lohn*

Main category: cs.CR

TLDR: 论文探讨了AI对网络冲突中攻防平衡的影响，总结了44种AI可能改变网络冲突特性的方式。


<details>
  <summary>Details</summary>
Motivation: 网络领域因其数字化特性和AI训练与应用的紧密反馈循环，可能成为AI影响最早且最大的领域之一，因此需要理解AI如何改变网络冲突的特性。

Method: 通过文献综述，收集了9种支持进攻优势的论点、9种支持防御优势的论点，以及48种关于网络冲突特性的论点，并分析AI对这些论点的影响。

Result: 网络领域过于复杂，AI对攻防平衡的影响因具体方面而异，可能增强、阻碍或保持不变。

Conclusion: AI对网络冲突的影响是多方面的，没有单一的答案，但总结了44种具体影响方式。

Abstract: Unlike other domains of conflict, and unlike other fields with high
anticipated risk from AI, the cyber domain is intrinsically digital with a
tight feedback loop between AI training and cyber application. Cyber may have
some of the largest and earliest impacts from AI, so it is important to
understand how the cyber domain may change as AI continues to advance. Our
approach reviewed the literature, collecting nine arguments that have been
proposed for offensive advantage in cyber conflict and nine proposed arguments
for defensive advantage. We include an additional forty-eight arguments that
have been proposed to give cyber conflict and competition its character as
collected separately by Healey, Jervis, and Nandrajog. We then consider how
each of those arguments and propositions might change with varying degrees of
AI advancement. We find that the cyber domain is too multifaceted for a single
answer to whether AI will enhance offense or defense broadly. AI will improve
some aspects, hinder others, and leave some aspects unchanged. We collect and
present forty-four ways that we expect AI to impact the cyber offense-defense
balance and the character of cyber conflict and competition.

</details>

### [14] [EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for Enhanced Cache Occupancy Attacks](https://arxiv.org/abs/2504.13385)
*Tianhong Xu,Aidong Adam Ding,Yunsi Fei*

Main category: cs.CR

TLDR: 论文提出针对Apple M系列SoC的SLC缓存占用攻击，首次实现通过SLC监控GPU和其他CPU集群活动，包括网站指纹攻击、像素窃取攻击和屏幕捕获攻击。


<details>
  <summary>Details</summary>
Motivation: 研究Apple M系列SoC中SLC缓存的独占性特性，探索其安全漏洞，填补现有缓存攻击研究的空白。

Method: 通过逆向工程分析SLC结构及访问策略，设计三种攻击：网站指纹攻击、像素窃取攻击和屏幕捕获攻击。

Result: 成功实现通过SLC监控GPU活动，攻击精度可达57像素行，揭示M系列SoC的重大安全漏洞。

Conclusion: Apple M系列SoC存在严重安全风险，需针对缓存占用攻击设计有效防御措施。

Abstract: Cache occupancy attacks exploit the shared nature of cache hierarchies to
infer a victim's activities by monitoring overall cache usage, unlike
access-driven cache attacks that focus on specific cache lines or sets. There
exists some prior work that target the last-level cache (LLC) of Intel
processors, which is inclusive of higher-level caches, and L2 caches of ARM
systems. In this paper, we target the System-Level Cache (SLC) of Apple
M-series SoCs, which is exclusive to higher-level CPU caches. We address the
challenges of the exclusiveness and propose a suite of SLC-cache occupancy
attacks, the first of its kind, where an adversary can monitor GPU and other
CPU cluster activities from their own CPU cluster. We first discover the
structure of SLC in Apple M1 SOC and various policies pertaining to access and
sharing through reverse engineering. We propose two attacks against websites.
One is a coarse-grained fingerprinting attack, recognizing which website is
accessed based on their different GPU memory access patterns monitored through
the SLC occupancy channel. The other attack is a fine-grained pixel stealing
attack, which precisely monitors the GPU memory usage for rendering different
pixels, through the SLC occupancy channel. Third, we introduce a novel screen
capturing attack which works beyond webpages, with the monitoring granularity
of 57 rows of pixels (there are 1600 rows for the screen). This significantly
expands the attack surface, allowing the adversary to retrieve any screen
display, posing a substantial new threat to system security. Our findings
reveal critical vulnerabilities in Apple's M-series SoCs and emphasize the
urgent need for effective countermeasures against cache occupancy attacks in
heterogeneous computing environments.

</details>

### [15] [Insecurity Through Obscurity: Veiled Vulnerabilities in Closed-Source Contracts](https://arxiv.org/abs/2504.13398)
*Sen Yang,Kaihua Qin,Aviv Yaish,Fan Zhang*

Main category: cs.CR

TLDR: 论文探讨了区块链中智能合约的闭源和混淆问题，提出了一种新工具SKANF，用于分析闭源和混淆合约的风险，并成功检测和利用了大量漏洞。


<details>
  <summary>Details</summary>
Motivation: 智能合约的闭源和混淆可能掩盖关键漏洞而非增强安全性，即“通过混淆导致的不安全性”。

Method: 开发了SKANF工具，结合控制流去混淆、符号执行和基于历史交易的混合执行，以识别和利用资产管理漏洞。

Result: 在真实MEV机器人中检测到1,028个合约漏洞，成功生成373个漏洞利用，潜在损失超900万美元；还发现40起实际攻击，损失达90万美元。

Conclusion: 闭源和混淆可能增加安全风险，SKANF工具能有效识别和利用漏洞，揭示了智能合约安全性的重要问题。

Abstract: Most blockchains cannot hide the binary code of programs (i.e., smart
contracts) running on them. To conceal proprietary business logic and to
potentially deter attacks, many smart contracts are closed-source and employ
layers of obfuscation. However, we demonstrate that such obfuscation can
obscure critical vulnerabilities rather than enhance security, a phenomenon we
term insecurity through obscurity. To systematically analyze these risks on a
large scale, we present SKANF, a novel EVM bytecode analysis tool tailored for
closed-source and obfuscated contracts. SKANF combines control-flow
deobfuscation, symbolic execution, and concolic execution based on historical
transactions to identify and exploit asset management vulnerabilities. Our
evaluation on real-world Maximal Extractable Value (MEV) bots reveals that
SKANF detects vulnerabilities in 1,028 contracts and successfully generates
exploits for 373 of them, with potential losses exceeding \$9.0M. Additionally,
we uncover 40 real-world MEV bot attacks that collectively resulted in \$900K
in losses.

</details>

### [16] [OpCode-Based Malware Classification Using Machine Learning and Deep Learning Techniques](https://arxiv.org/abs/2504.13408)
*Varij Saini,Rudraksh Gupta,Neel Soni*

Main category: cs.CR

TLDR: 论文比较了传统机器学习（SVM、KNN、决策树）和深度学习（CNN）在恶意软件分类中的表现，CNN在自动特征提取方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究不同方法在恶意软件分类中的效果，特别是传统机器学习与深度学习的对比。

Method: 传统方法使用n-gram分析和SVM、KNN、决策树；深度学习方法使用CNN自动提取特征。

Result: SVM在传统方法中表现最佳，但CNN在自动特征提取方面表现竞争性。

Conclusion: CNN在恶意软件分类中具有潜力，尤其是其自动特征提取的优势。

Abstract: This technical report presents a comprehensive analysis of malware
classification using OpCode sequences. Two distinct approaches are evaluated:
traditional machine learning using n-gram analysis with Support Vector Machine
(SVM), K-Nearest Neighbors (KNN), and Decision Tree classifiers; and a deep
learning approach employing a Convolutional Neural Network (CNN). The
traditional machine learning approach establishes a baseline using handcrafted
1-gram and 2-gram features from disassembled malware samples. The deep learning
methodology builds upon the work proposed in "Deep Android Malware Detection"
by McLaughlin et al. and evaluates the performance of a CNN model trained to
automatically extract features from raw OpCode data. Empirical results are
compared using standard performance metrics (accuracy, precision, recall, and
F1-score). While the SVM classifier outperforms other traditional techniques,
the CNN model demonstrates competitive performance with the added benefit of
automated feature extraction.

</details>

### [17] [Everything You Wanted to Know About LLM-based Vulnerability Detection But Were Afraid to Ask](https://arxiv.org/abs/2504.13474)
*Yue Li,Xiao Li,Hao Wu,Minghui Xu,Yue Zhang,Xiuzhen Cheng,Fengyuan Xu,Sheng Zhong*

Main category: cs.CR

TLDR: 本文挑战了关于大语言模型（LLM）在漏洞检测中的三个常见误解，提出了一种新的评估框架CORRECT，通过引入上下文信息显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法忽略了漏洞检测所需的执行和数据流上下文，导致误导性结论和不可靠的评估结果。

Method: 提出CORRECT框架，构建包含2000个漏洞-补丁对的上下文丰富数据集，评估13个LLM，并结合自然语言推理验证。

Result: 在足够上下文支持下，LLM性能显著提升（如关键CWE的F1分数达0.7），但存在推理错误和泛化不足等新问题。

Conclusion: 上下文信息对LLM漏洞检测至关重要，现有系统需改进以解决推理错误和泛化问题。

Abstract: Large Language Models are a promising tool for automated vulnerability
detection, thanks to their success in code generation and repair. However,
despite widespread adoption, a critical question remains: Are LLMs truly
effective at detecting real-world vulnerabilities? Current evaluations, which
often assess models on isolated functions or files, ignore the broader
execution and data-flow context essential for understanding vulnerabilities.
This oversight leads to two types of misleading outcomes: incorrect conclusions
and flawed rationales, collectively undermining the reliability of prior
assessments. Therefore, in this paper, we challenge three widely held community
beliefs: that LLMs are (i) unreliable, (ii) insensitive to code patches, and
(iii) performance-plateaued across model scales. We argue that these beliefs
are artifacts of context-deprived evaluations. To address this, we propose
CORRECT (Context-Rich Reasoning Evaluation of Code with Trust), a new
evaluation framework that systematically incorporates contextual information
into LLM-based vulnerability detection. We construct a context-rich dataset of
2,000 vulnerable-patched program pairs spanning 99 CWEs and evaluate 13 LLMs
across four model families. Our framework elicits both binary predictions and
natural-language rationales, which are further validated using LLM-as-a-judge
techniques. Our findings overturn existing misconceptions. When provided with
sufficient context, SOTA LLMs achieve significantly improved performance (e.g.,
0.7 F1-score on key CWEs), with 0.8 precision. We show that most false
positives stem from reasoning errors rather than misclassification, and that
while model and test-time scaling improve performance, they introduce
diminishing returns and trade-offs in recall. Finally, we uncover new flaws in
current LLM-based detection systems, such as limited generalization and
overthinking biases.

</details>

### [18] [Multi-class Item Mining under Local Differential Privacy](https://arxiv.org/abs/2504.13526)
*Yulian Mao,Qingqing Ye,Rong Du,Qi Wang,Kai Huang,Haibo Hu*

Main category: cs.CR

TLDR: 该论文提出了一种基于局部差分隐私（LDP）的多类别项目挖掘框架，通过有效性扰动和相关性扰动解决标签扰动带来的问题，并在频率估计和top-k项目挖掘任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的LDP项目挖掘机制主要关注全局统计，无法满足个性化推荐等用户定制任务的需求。同时，类别标签的引入带来了新的挑战，如标签扰动可能导致无效数据。

Method: 提出了多类别项目挖掘框架，包括有效性扰动和相关性扰动两种机制，分别用于减少无效数据的影响和保持标签与项目之间的关系。

Result: 通过理论分析和实验验证，证明了这些方法在频率估计和top-k项目挖掘任务中的有效性和优越性。

Conclusion: 该研究为多类别项目挖掘提供了一种隐私保护且高效的解决方案，尤其适用于需要细粒度信息的任务。

Abstract: Item mining, a fundamental task for collecting statistical data from users,
has raised increasing privacy concerns. To address these concerns, local
differential privacy (LDP) was proposed as a privacy-preserving technique.
Existing LDP item mining mechanisms primarily concentrate on global statistics,
i.e., those from the entire dataset. Nevertheless, they fall short of
user-tailored tasks such as personalized recommendations, whereas classwise
statistics can improve task accuracy with fine-grained information. Meanwhile,
the introduction of class labels brings new challenges. Label perturbation may
result in invalid items for aggregation. To this end, we propose frameworks for
multi-class item mining, along with two mechanisms: validity perturbation to
reduce the impact of invalid data, and correlated perturbation to preserve the
relationship between labels and items. We also apply these optimized methods to
two multi-class item mining queries: frequency estimation and top-$k$ item
mining. Through theoretical analysis and extensive experiments, we verify the
effectiveness and superiority of these methods.

</details>

### [19] [Designing a reliable lateral movement detector using a graph foundation model](https://arxiv.org/abs/2504.13527)
*Corentin Larroche*

Main category: cs.CR

TLDR: 本文探讨了图基础模型（GFMs）在网络安全中的应用潜力，特别是通过横向移动检测的案例展示了其无需领域数据训练即可达到先进性能的能力。


<details>
  <summary>Details</summary>
Motivation: 基础模型在机器学习中表现出色，但在网络安全领域的应用受到数据处理效率的限制。图基础模型（GFMs）因其适合表示网络数据而可能成为解决方案。

Method: 通过预训练的图基础模型（GFM），构建了一个横向移动检测器，无需领域数据训练。

Result: 该检测器在横向移动检测任务中达到了最先进的性能。

Conclusion: 图基础模型（GFMs）在网络安全中具有显著潜力，能够高效处理复杂数据并实现高性能。

Abstract: Foundation models have recently emerged as a new paradigm in machine learning
(ML). These models are pre-trained on large and diverse datasets and can
subsequently be applied to various downstream tasks with little or no
retraining. This allows people without advanced ML expertise to build ML
applications, accelerating innovation across many fields. However, the adoption
of foundation models in cybersecurity is hindered by their inability to
efficiently process data such as network traffic captures or binary
executables. The recent introduction of graph foundation models (GFMs) could
make a significant difference, as graphs are well-suited to representing these
types of data. We study the usability of GFMs in cybersecurity through the lens
of one specific use case, namely lateral movement detection. Using a
pre-trained GFM, we build a detector that reaches state-of-the-art performance
without requiring any training on domain-specific data. This case study thus
provides compelling evidence of the potential of GFMs for cybersecurity.

</details>

### [20] [Complexity of Post-Quantum Cryptography in Embedded Systems and Its Optimization Strategies](https://arxiv.org/abs/2504.13537)
*Omar Alnaseri,Yassine Himeur,Shadi Atalla,Wathiq Mansoor*

Main category: cs.CR

TLDR: 本文分析了后量子密码（PQC）在嵌入式系统中的硬件复杂性，分类了基于不同数学问题的PQC算法，并探讨了优化策略。最后对比了CRYSTALS-Kyber和McEliece的复杂性。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的快速发展，传统密码方案（如RSA和ECC）变得脆弱，需要开发量子抗性算法。NIST已启动PQC标准化进程。

Method: 分类PQC算法为基于格、基于码、基于哈希和基于多变量/同源问题的家族，分析其计算、内存和能耗特性，并提出优化策略（如流水线、并行化和高级综合）。

Result: 详细比较了CRYSTALS-Kyber和McEliece在密钥生成、加密和解密过程中的计算复杂性。

Conclusion: PQC算法的硬件复杂性因家族而异，优化策略可提升性能和能效，为嵌入式系统选择合适的PQC算法提供了指导。

Abstract: With the rapid advancements in quantum computing, traditional cryptographic
schemes like Rivest-Shamir-Adleman (RSA) and elliptic curve cryptography (ECC)
are becoming vulnerable, necessitating the development of quantum-resistant
algorithms. The National Institute of Standards and Technology (NIST) has
initiated a standardization process for PQC algorithms, and several candidates,
including CRYSTALS-Kyber and McEliece, have reached the final stages. This
paper first provides a comprehensive analysis of the hardware complexity of
post-quantum cryptography (PQC) in embedded systems, categorizing PQC
algorithms into families based on their underlying mathematical problems:
lattice-based, code-based, hash-based and multivariate / isogeny-based schemes.
Each family presents distinct computational, memory, and energy profiles,
making them suitable for different use cases. To address these challenges, this
paper discusses optimization strategies such as pipelining, parallelization,
and high-level synthesis (HLS), which can improve the performance and energy
efficiency of PQC implementations. Finally, a detailed complexity analysis of
CRYSTALS-Kyber and McEliece, comparing their key generation, encryption, and
decryption processes in terms of computational complexity, has been conducted.

</details>

### [21] [Version-level Third-Party Library Detection in Android Applications via Class Structural Similarity](https://arxiv.org/abs/2504.13547)
*Bolin Zhou,Jingzheng Wu,Xiang Ling,Tianyue Luo,Jingkun Zhang*

Main category: cs.CR

TLDR: SAD是一种高效的第三方库（TPL）检测工具，专注于解决版本级检测的局限性，显著降低误报率并提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有工具在版本级TPL检测上表现不佳，误报率高，增加了安全分析的手动工作量，因此需要一种更精确的检测方法。

Method: SAD通过类依赖图（CDG）特征生成候选类列表，基于类摘要相似度进行匹配，并通过子图结构相似性识别TPL版本。

Result: SAD在模糊化应用上的库级和版本级检测F1分数分别为97.64%和84.82%，优于现有工具，误报率显著降低。

Conclusion: SAD在TPL检测中表现出色，尤其在版本级检测上解决了现有工具的不足，为安全分析提供了更可靠的解决方案。

Abstract: Android applications (apps) integrate reusable and well-tested third-party
libraries (TPLs) to enhance functionality and shorten development cycles.
However, recent research reveals that TPLs have become the largest attack
surface for Android apps, where the use of insecure TPLs can compromise both
developer and user interests. To mitigate such threats, researchers have
proposed various tools to detect TPLs used by apps, supporting further security
analyses such as vulnerable TPLs identification. Although existing tools
achieve notable library-level TPL detection performance in the presence of
obfuscation, they struggle with version-level TPL detection due to a lack of
sensitivity to differences between versions. This limitation results in a high
version-level false positive rate, significantly increasing the manual workload
for security analysts. To resolve this issue, we propose SAD, a TPL detection
tool with high version-level detection performance. SAD generates a candidate
app class list for each TPL class based on the feature of nodes in class
dependency graphs (CDGs). It then identifies the unique corresponding app class
for each TPL class by performing class matching based on the similarity of
their class summaries. Finally, SAD identifies TPL versions by evaluating the
structural similarity of the sub-graph formed by matched classes within the
CDGs of the TPL and the app. Extensive evaluation on three datasets
demonstrates the effectiveness of SAD and its components. SAD achieves F1
scores of 97.64% and 84.82% for library-level and version-level detection on
obfuscated apps, respectively, surpassing existing state-of-the-art tools. The
version-level false positives reported by the best tool is 1.61 times that of
SAD. We further evaluate the degree to which TPLs identified by detection tools
correspond to actual TPL classes.

</details>

### [22] [Q-FAKER: Query-free Hard Black-box Attack via Controlled Generation](https://arxiv.org/abs/2504.13551)
*CheolWon Na,YunSeok Choi,Jee-Hyong Lee*

Main category: cs.CR

TLDR: 提出了一种无需查询目标模型的高效对抗攻击方法Q-faker，适用于硬黑盒场景。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法需要大量查询或目标模型信息，不适用于硬黑盒场景。

Method: 使用替代模型生成对抗句子，结合可控生成技术，实现目标无关攻击。

Result: 在八个数据集上验证了方法的高效性、高迁移性和生成对抗样本的高质量。

Conclusion: Q-faker在硬黑盒场景中具有实际应用价值。

Abstract: Many adversarial attack approaches are proposed to verify the vulnerability
of language models. However, they require numerous queries and the information
on the target model. Even black-box attack methods also require the target
model's output information. They are not applicable in real-world scenarios, as
in hard black-box settings where the target model is closed and inaccessible.
Even the recently proposed hard black-box attacks still require many queries
and demand extremely high costs for training adversarial generators. To address
these challenges, we propose Q-faker (Query-free Hard Black-box Attacker), a
novel and efficient method that generates adversarial examples without
accessing the target model. To avoid accessing the target model, we use a
surrogate model instead. The surrogate model generates adversarial sentences
for a target-agnostic attack. During this process, we leverage controlled
generation techniques. We evaluate our proposed method on eight datasets.
Experimental results demonstrate our method's effectiveness including high
transferability and the high quality of the generated adversarial examples, and
prove its practical in hard black-box settings.

</details>

### [23] [Cybersquatting in Web3: The Case of NFT](https://arxiv.org/abs/2504.13573)
*Kai Ma,Ningyu He,Jintao Huang,Bosi Zhang,Ping Wu,Haoyu Wang*

Main category: cs.CR

TLDR: 本文首次深入研究了NFT领域的域名抢注行为，分析了超过22万个NFT集合和1.5亿个NFT代币，发现了针对654个热门NFT项目的8,019个抢注案例，揭示了七种抢注手法及其重大财务影响。


<details>
  <summary>Details</summary>
Motivation: 随着NFT生态系统的增长，域名抢注行为从传统域名扩展到NFT领域，亟需研究其影响和防范措施。

Method: 通过分析大量NFT集合和代币，系统性地识别和分类抢注行为，并研究其元数据、数字资产内容和社交媒体状态。

Result: 研究发现抢注活动导致67万受害者，财务损失达5926万美元，揭示了七种抢注手法。

Conclusion: 研究强调了识别和预防NFT抢注行为的紧迫性。

Abstract: Cybersquatting refers to the practice where attackers register a domain name
similar to a legitimate one to confuse users for illegal gains. With the growth
of the Non-Fungible Token (NFT) ecosystem, there are indications that
cybersquatting tactics have evolved from targeting domain names to NFTs. This
paper presents the first in-depth measurement study of NFT cybersquatting. By
analyzing over 220K NFT collections with over 150M NFT tokens, we have
identified 8,019 cybersquatting NFT collections targeting 654 popular NFT
projects. Through systematic analysis, we discover and characterize seven
distinct squatting tactics employed by scammers. We further conduct a
comprehensive measurement study of these cybersquatting NFT collections,
examining their metadata, associated digital asset content, and social media
status. Our analysis reveals that these NFT cybersquatting activities have
resulted in a significant financial impact, with over 670K victims affected by
these scams, leading to a total financial exploitation of $59.26 million. Our
findings demonstrate the urgency to identify and prevent NFT squatting abuses.

</details>

### [24] [Trace Gadgets: Minimizing Code Context for Machine Learning-Based Vulnerability Prediction](https://arxiv.org/abs/2504.13676)
*Felix Mächtle,Nils Loose,Tim Schulz,Florian Sieck,Jan-Niclas Serr,Ralf Möller,Thomas Eisenbarth*

Main category: cs.CR

TLDR: 论文提出了一种名为Trace Gadgets的代码表示方法，通过最小化代码上下文提高机器学习模型的漏洞检测性能，并在真实数据集上验证了其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着网络应用和API端点数量的增加，漏洞数量也随之增长，手动检测效率低且静态扫描器误报率高。机器学习方法在训练和测试数据相关时表现良好，但代码上下文过长会影响模型性能。

Method: 引入Trace Gadgets，一种去除无关代码的表示方法，精确捕捉漏洞路径的语句，为ML模型提供最小但完整的上下文。同时收集大规模真实数据集以提升性能。

Result: 实验表明，使用Trace Gadgets的ML模型优于现有代码表示方法，在完全未见数据集上比GitHub的CodeQL至少提升4%的检测能力。

Conclusion: Trace Gadgets显著提升了漏洞检测性能，并在实际应用中发现了广泛部署软件中的未知漏洞。

Abstract: As the number of web applications and API endpoints exposed to the Internet
continues to grow, so does the number of exploitable vulnerabilities. Manually
identifying such vulnerabilities is tedious. Meanwhile, static security
scanners tend to produce many false positives. While machine learning-based
approaches are promising, they typically perform well only in scenarios where
training and test data are closely related. A key challenge for ML-based
vulnerability detection is providing suitable and concise code context, as
excessively long contexts negatively affect the code comprehension capabilities
of machine learning models, particularly smaller ones.
  This work introduces Trace Gadgets, a novel code representation that
minimizes code context by removing non-related code. Trace Gadgets precisely
capture the statements that cover the path to the vulnerability. As input for
ML models, Trace Gadgets provide a minimal but complete context, thereby
improving the detection performance. Moreover, we collect a large-scale dataset
generated from real-world applications with manually curated labels to further
improve the performance of ML-based vulnerability detectors. Our results show
that state-of-the-art machine learning models perform best when using Trace
Gadgets compared to previous code representations, surpassing the detection
capabilities of industry-standard static scanners such as GitHub's CodeQL by at
least 4% on a fully unseen dataset. By applying our framework to real-world
applications, we identify and report previously unknown vulnerabilities in
widely deployed software.

</details>

### [25] [Breaking ECDSA with Two Affinely Related Nonces](https://arxiv.org/abs/2504.13737)
*Jamie Gilchrist,William J. Buchanan,Keir Finlow-Bates*

Main category: cs.CR

TLDR: 论文展示了ECDSA中即使非ce存在仿射关系，也能通过代数方法恢复私钥，仅需两个签名。


<details>
  <summary>Details</summary>
Motivation: 研究ECDSA中非ce的仿射关系对私钥安全性的影响，填补了已知攻击方法的空白。

Method: 通过代数推导，利用两个签名（即使消息相同）和已知的非ce仿射关系，无需格约减或暴力搜索。

Result: 成功从两个签名中恢复私钥，首次实现了在已知非ce仿射关系下的闭式推导。

Conclusion: ECDSA的安全性不仅依赖于非ce的唯一性，还需避免非ce间的仿射关系，否则私钥可能泄露。

Abstract: The security of the Elliptic Curve Digital Signature Algorithm (ECDSA)
depends on the uniqueness and secrecy of the nonce, which is used in each
signature. While it is well understood that nonce $k$ reuse across two distinct
messages can leak the private key, we show that even if a distinct value is
used for $k_2$, where an affine relationship exists in the form of: \(k_m = a
\cdot k_n + b\), we can also recover the private key. Our method requires only
two signatures (even over the same message) and relies purely on algebra, with
no need for lattice reduction or brute-force search(if the relationship, or
offset, is known). To our knowledge, this is the first closed-form derivation
of the ECDSA private key from only two signatures over the same message, under
a known affine relationship between nonces.

</details>

### [26] [Scoring Azure permissions with metric spaces](https://arxiv.org/abs/2504.13747)
*Christophe Parisel*

Main category: cs.CR

TLDR: 论文提出了两种互补的指标（WAR距离和爆炸半径距离）来量化Microsoft Azure中的特权风险，分别用于控制平面和数据平面，为IAM分析提供了统一框架。


<details>
  <summary>Details</summary>
Motivation: 为了主动分析和量化Microsoft Azure中的特权风险，需要一种系统性的方法来评估权限配置和数据泄露的潜在影响。

Method: 在控制平面中，定义了WAR距离（基于Write、Action、Read权限的超递增距离）；在数据平面中，提出了爆炸半径距离（利用Azure租户的聚类层次结构）。

Result: 两种指标能够对主体进行排序，并量化数据泄露和伪造的最大范围。

Conclusion: 这些指标为IAM分析、生命周期监控和最小权限实施提供了统一的框架。

Abstract: In this work, we introduce two complementary metrics for quantifying and
scoring privilege risk in Microsoft Azure. In the Control Plane, we define the
WAR distance, a superincreasing distance over Write, Action, and Read control
permissions, which yields a total ordering of principals by their configuration
power.
  In the Data Plane, we present a blast radius distance for measuring the
maximum breadth of data exfiltration and forgery, leveraging the natural
ultrametry of Azure Tenants clustering hierarchy
  Together, these metrics offer a unified framework for proactive IAM analysis,
ranking, lifecycle monitoring, and least privilege enforcement.

</details>

### [27] [Access control for Data Spaces](https://arxiv.org/abs/2504.13767)
*Nikos Fotiou,Vasilios A. Siris,George C. Polyzos*

Main category: cs.CR

TLDR: 本文提出了一种数据空间中的细粒度访问控制机制，支持持续策略评估、数据语义感知和数据事件订阅。


<details>
  <summary>Details</summary>
Motivation: 数据空间中实现细粒度访问控制而不影响功能是一个重要挑战。

Method: 设计并实现了一种访问控制机制，策略集中存储，并扩展支持数据所有者管理自己的策略管理点，基于W3C可验证凭证。

Result: 机制支持持续策略评估、数据语义感知和数据事件订阅。

Conclusion: 该机制为数据空间提供了安全、灵活的访问控制解决方案。

Abstract: Data spaces represent an emerging paradigm that facilitates secure and
trusted data exchange through foundational elements of data interoperability,
sovereignty, and trust. Within a data space, data items, potentially owned by
different entities, can be interconnected. Concurrently, data consumers can
execute advanced data lookup operations and subscribe to data-driven events.
Achieving fine-grained access control without compromising functionality
presents a significant challenge. In this paper, we design and implement an
access control mechanism that ensures continuous evaluation of access control
policies, is data semantics aware, and supports subscriptions to data events.
We present a construction where access control policies are stored in a
centralized location, which we extend to allow data owners to maintain their
own Policy Administration Points. This extension builds upon W3C Verifiable
Credentials.

</details>

### [28] [Can LLMs handle WebShell detection? Overcoming Detection Challenges with Behavioral Function-Aware Framework](https://arxiv.org/abs/2504.13811)
*Feijiang Han,Jiaming Zhang,Chuyi Deng,Jianheng Tang,Yunhuai Liu*

Main category: cs.CR

TLDR: 论文评估了七种大语言模型（LLMs）在WebShell检测中的表现，并提出了BFAD框架以提升性能。结果表明，较大LLMs精度高但召回率低，而BFAD显著提升了所有模型的F1分数。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在WebShell检测中存在数据需求大、泛化能力差等问题，LLMs的潜力尚未充分探索。

Method: 提出了BFAD框架，包括关键功能过滤器、上下文感知代码提取和加权行为功能分析。

Result: 较大LLMs（如GPT-4）性能优于传统方法，较小模型也表现接近。BFAD平均提升F1分数13.82%。

Conclusion: LLMs在WebShell检测中具有潜力，BFAD框架有效解决了相关挑战。

Abstract: WebShell attacks, in which malicious scripts are injected into web servers,
are a major cybersecurity threat. Traditional machine learning and deep
learning methods are hampered by issues such as the need for extensive training
data, catastrophic forgetting, and poor generalization. Recently, Large
Language Models (LLMs) have gained attention for code-related tasks, but their
potential in WebShell detection remains underexplored. In this paper, we make
two major contributions: (1) a comprehensive evaluation of seven LLMs,
including GPT-4, LLaMA 3.1 70B, and Qwen 2.5 variants, benchmarked against
traditional sequence- and graph-based methods using a dataset of 26.59K PHP
scripts, and (2) the Behavioral Function-Aware Detection (BFAD) framework,
designed to address the specific challenges of applying LLMs to this domain.
Our framework integrates three components: a Critical Function Filter that
isolates malicious PHP function calls, a Context-Aware Code Extraction strategy
that captures the most behaviorally indicative code segments, and Weighted
Behavioral Function Profiling (WBFP) that enhances in-context learning by
prioritizing the most relevant demonstrations based on discriminative
function-level profiles. Our results show that larger LLMs achieve near-perfect
precision but lower recall, while smaller models exhibit the opposite
trade-off. However, all models lag behind previous State-Of-The-Art (SOTA)
methods. With BFAD, the performance of all LLMs improved, with an average F1
score increase of 13.82%. Larger models such as GPT-4, LLaMA 3.1 70B, and Qwen
2.5 14B outperform SOTA methods, while smaller models such as Qwen 2.5 3B
achieve performance competitive with traditional methods. This work is the
first to explore the feasibility and limitations of LLMs for WebShell
detection, and provides solutions to address the challenges in this task.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [Harmony: A Unified Framework for Modality Incremental Learning](https://arxiv.org/abs/2504.13218)
*Yaguang Song,Xiaoshan Yang,Dongmei Jiang,Yaowei Wang,Changsheng Xu*

Main category: cs.LG

TLDR: 本文提出了一种新的模态增量学习（MIL）范式，旨在解决模型在连续演变的模态序列中进行增量学习的挑战。通过Harmony框架实现模态对齐和知识保留，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中数据常来自全新模态，而现有研究多关注单模态或模态一致的增量学习。本文探索如何在模态不断变化的情况下实现统一模型的增量学习。

Method: 提出Harmony框架，包含自适应兼容特征调制和累积模态桥接，通过构建历史模态特征和模态知识积累与对齐，减少模态差异。

Result: 在MIL任务上的实验表明，该方法显著优于现有增量学习方法。

Conclusion: Harmony框架有效解决了模态增量学习中的挑战，为多模态统一模型的增量学习提供了可行方案。

Abstract: Incremental learning aims to enable models to continuously acquire knowledge
from evolving data streams while preserving previously learned capabilities.
While current research predominantly focuses on unimodal incremental learning
and multimodal incremental learning where the modalities are consistent,
real-world scenarios often present data from entirely new modalities, posing
additional challenges. This paper investigates the feasibility of developing a
unified model capable of incremental learning across continuously evolving
modal sequences. To this end, we introduce a novel paradigm called Modality
Incremental Learning (MIL), where each learning stage involves data from
distinct modalities. To address this task, we propose a novel framework named
Harmony, designed to achieve modal alignment and knowledge retention, enabling
the model to reduce the modal discrepancy and learn from a sequence of distinct
modalities, ultimately completing tasks across multiple modalities within a
unified framework. Our approach introduces the adaptive compatible feature
modulation and cumulative modal bridging. Through constructing historical modal
features and performing modal knowledge accumulation and alignment, the
proposed components collaboratively bridge modal differences and maintain
knowledge retention, even with solely unimodal data available at each learning
phase.These components work in concert to establish effective modality
connections and maintain knowledge retention, even when only unimodal data is
available at each learning stage. Extensive experiments on the MIL task
demonstrate that our proposed method significantly outperforms existing
incremental learning methods, validating its effectiveness in MIL scenarios.

</details>

### [30] [Scaling Laws for Data-Efficient Visual Transfer Learning](https://arxiv.org/abs/2504.13219)
*Wenxuan Yang,Qingqu Wei,Chenxi Ma,Weimin Tan,Bo Yan*

Main category: cs.LG

TLDR: 本文提出了首个视觉迁移学习中数据高效扩展的实用框架，解决了数据受限下游任务的扩展行为及知识蒸馏效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉AI模型的扩展定律主要关注大规模预训练，而忽略了数据受限下游任务的性能扩展问题。

Method: 通过系统分析不同数据规模（1K-1M样本）的视觉任务，提出蒸馏边界理论，研究知识蒸馏在数据稀缺和充足条件下的效率变化。

Result: 发现蒸馏模型在数据稀缺时表现更优，但随着预训练数据增加，非蒸馏模型逐渐超越蒸馏模型。

Conclusion: 本研究重新定义了数据受限条件下的扩展定律，填补了大规模预训练与下游任务适应之间的知识空白。

Abstract: Current scaling laws for visual AI models focus predominantly on large-scale
pretraining, leaving a critical gap in understanding how performance scales for
data-constrained downstream tasks. To address this limitation, this paper
establishes the first practical framework for data-efficient scaling laws in
visual transfer learning, addressing two fundamental questions: 1) How do
scaling behaviors shift when downstream tasks operate with limited data? 2)
What governs the efficacy of knowledge distillation under such constraints?
Through systematic analysis of vision tasks across data regimes (1K-1M
samples), we propose the distillation boundary theory, revealing a critical
turning point in distillation efficiency: 1) Distillation superiority: In
data-scarce conditions, distilled models significantly outperform their
non-distillation counterparts, efficiently leveraging inherited knowledge to
compensate for limited training samples. 2) Pre-training dominance: As
pre-training data increases beyond a critical threshold, non-distilled models
gradually surpass distilled versions, suggesting diminishing returns from
knowledge inheritance when sufficient task-specific data becomes available.
Empirical validation across various model scales (2.5M to 38M parameters) and
data volumes demonstrate these performance inflection points, with error
difference curves transitioning from positive to negative values at critical
data thresholds, confirming our theoretical predictions. This work redefines
scaling laws for data-limited regimes, bridging the knowledge gap between
large-scale pretraining and practical downstream adaptation, addressing a
critical barrier to understanding vision model scaling behaviors and optimizing
computational resource allocation.

</details>

### [31] [Modelling Mean-Field Games with Neural Ordinary Differential Equations](https://arxiv.org/abs/2504.13228)
*Anna C. M. Thöni,Yoram Bachrach,Tal Kachman*

Main category: cs.LG

TLDR: 结合均值场博弈论与深度学习（神经ODE），提出一种数据驱动、轻量级的模型，减少对模型的依赖，并能学习复杂策略交互。


<details>
  <summary>Details</summary>
Motivation: 传统均值场博弈论依赖解析解，存在模型偏差和解的唯一性问题，需减少模型与博弈的依赖。

Method: 结合均值场博弈论与神经ODE，利用自动微分提高鲁棒性。

Result: 模型灵活轻量，仅需少量观测即可学习数据分布，成功解决三种复杂度不同的均值场博弈问题。

Conclusion: 该模型在复杂、噪声和可观测性不同的场景中表现高效灵活，优于传统方法。

Abstract: Mean-field game theory relies on approximating games that would otherwise
have been intractable to model. While the games can be solved analytically via
the associated system of partial derivatives, this approach is not model-free,
can lead to the loss of the existence or uniqueness of solutions and may suffer
from modelling bias. To reduce the dependency between the model and the game,
we combine mean-field game theory with deep learning in the form of neural
ordinary differential equations. The resulting model is data-driven,
lightweight and can learn extensive strategic interactions that are hard to
capture using mean-field theory alone. In addition, the model is based on
automatic differentiation, making it more robust and objective than approaches
based on finite differences. We highlight the efficiency and flexibility of our
approach by solving three mean-field games that vary in their complexity,
observability and the presence of noise. Using these results, we show that the
model is flexible, lightweight and requires few observations to learn the
distribution underlying the data.

</details>

### [32] [PSG-MAE: Robust Multitask Sleep Event Monitoring using Multichannel PSG Reconstruction and Inter-channel Contrastive Learning](https://arxiv.org/abs/2504.13229)
*Yifei Wang,Qi Liu,Fuli Min,Honghao Wang*

Main category: cs.LG

TLDR: PSG-MAE是一种基于掩码自编码器的预训练框架，通过自监督学习从大量未标记的PSG数据中提取稳健特征，适用于多种睡眠事件监测任务。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度神经网络在睡眠监测中因数据集有限导致的泛化能力不足和鲁棒性差的问题。

Method: 提出PSG-MAE框架，包括多通道互补掩码生成、多通道信号重建方法和自监督的通道间对比学习策略。

Result: PSG-MAE能有效捕获PSG信号的时序细节和通道间信息，睡眠分期准确率达83.7%，阻塞性睡眠呼吸暂停检测准确率达90.45%。

Conclusion: PSG-MAE框架具有鲁棒性和广泛适用性，为睡眠监测任务提供了有效的解决方案。

Abstract: Polysomnography (PSG) signals are essential for studying sleep processes and
diagnosing sleep disorders. Analyzing PSG data through deep neural networks
(DNNs) for automated sleep monitoring has become increasingly feasible.
However, the limited availability of datasets for certain sleep events often
leads to DNNs focusing on a single task with a single-sourced training dataset.
As a result, these models struggle to transfer to new sleep events and lack
robustness when applied to new datasets. To address these challenges, we
propose PSG-MAE, a mask autoencoder (MAE) based pre-training framework. By
performing self-supervised learning on a large volume of unlabeled PSG data,
PSG-MAE develops a robust feature extraction network that can be broadly
applied to various sleep event monitoring tasks. Unlike conventional MAEs,
PSG-MAE generates complementary masks across PSG channels, integrates a
multichannel signal reconstruction method, and employs a self-supervised
inter-channel contrastive learning (ICCL) strategy. This approach enables the
encoder to capture temporal features from each channel while simultaneously
learning latent relationships between channels, thereby enhancing the
utilization of multichannel information. Experimental results show that PSG-MAE
effectively captures both temporal details and inter-channel information from
PSG signals. When the encoder pre-trained through PSG-MAE is fine-tuned with
downstream feature decomposition networks, it achieves an accuracy of 83.7% for
sleep staging and 90.45% for detecting obstructive sleep apnea, which
highlights the framework's robustness and broad applicability.

</details>

### [33] [Auto-FEDUS: Autoregressive Generative Modeling of Doppler Ultrasound Signals from Fetal Electrocardiograms](https://arxiv.org/abs/2504.13233)
*Alireza Rafiei,Gari D. Clifford,Nasim Katebi*

Main category: cs.LG

TLDR: 提出了一种新型自回归生成模型Auto-FEDUS，将胎儿心电图信号映射到多普勒超声波形，解决了数据稀缺问题，并提升了信号生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于多普勒超声信号的胎儿健康监测技术因数据稀缺和分布不均而受限，需要一种有效的数据生成方法。

Method: 采用基于扩张因果卷积的神经时序网络，直接在波形级别操作，捕捉信号的短长期依赖关系。

Result: Auto-FEDUS在时域和频域评估中优于传统生成模型，生成信号质量高，心率估计结果与真实数据接近。

Conclusion: Auto-FEDUS为解决数据稀缺问题提供了有效方案，提升了多普勒超声信号模型的训练效果和泛化能力。

Abstract: Fetal health monitoring through one-dimensional Doppler ultrasound (DUS)
signals offers a cost-effective and accessible approach that is increasingly
gaining interest. Despite its potential, the development of machine learning
based techniques to assess the health condition of mothers and fetuses using
DUS signals remains limited. This scarcity is primarily due to the lack of
extensive DUS datasets with a reliable reference for interpretation and data
imbalance across different gestational ages. In response, we introduce a novel
autoregressive generative model designed to map fetal electrocardiogram (FECG)
signals to corresponding DUS waveforms (Auto-FEDUS). By leveraging a neural
temporal network based on dilated causal convolutions that operate directly on
the waveform level, the model effectively captures both short and long-range
dependencies within the signals, preserving the integrity of generated data.
Cross-subject experiments demonstrate that Auto-FEDUS outperforms conventional
generative architectures across both time and frequency domain evaluations,
producing DUS signals that closely resemble the morphology of their real
counterparts. The realism of these synthesized signals was further gauged using
a quality assessment model, which classified all as good quality, and a heart
rate estimation model, which produced comparable results for generated and real
data, with a Bland-Altman limit of 4.5 beats per minute. This advancement
offers a promising solution for mitigating limited data availability and
enhancing the training of DUS-based fetal models, making them more effective
and generalizable.

</details>

### [34] [Non-Uniform Class-Wise Coreset Selection: Characterizing Category Difficulty for Data-Efficient Transfer Learning](https://arxiv.org/abs/2504.13234)
*Hanyu Zhang,Zhen Xing,Wenxuan Yang,Chenxi Ma,Weimin Tan,Bo Yan*

Main category: cs.LG

TLDR: NUCS提出了一种新的核心集选择框架，结合类别级和实例级标准，优化数据选择预算，显著提升少数类别的代表性。


<details>
  <summary>Details</summary>
Motivation: 随着迁移学习模型和数据集的增大，高效适应和存储优化成为关键需求。现有方法主要依赖实例级难度评估，忽略了类别级特性，导致少数类别代表性不足。

Method: NUCS框架通过自动分配类别预算和自适应选择样本，结合类别难度和实例难度，实现更平衡的核心集构建。

Result: 在14个数据集和模型架构上的实验表明，NUCS优于现有方法，仅保留30%样本即可匹配全数据训练精度，计算时间减少60%。

Conclusion: NUCS强调了类别难度在核心集选择中的重要性，为迁移学习提供了高效且鲁棒的解决方案。

Abstract: As transfer learning models and datasets grow larger, efficient adaptation
and storage optimization have become critical needs. Coreset selection
addresses these challenges by identifying and retaining the most informative
samples, constructing a compact subset for target domain training. However,
current methods primarily rely on instance-level difficulty assessments,
overlooking crucial category-level characteristics and consequently
under-representing minority classes. To overcome this limitation, we propose
Non-Uniform Class-Wise Coreset Selection (NUCS), a novel framework that
integrates both class-level and instance-level criteria. NUCS automatically
allocates data selection budgets for each class based on intrinsic category
difficulty and adaptively selects samples within optimal difficulty ranges. By
explicitly incorporating category-specific insights, our approach achieves a
more balanced and representative coreset, addressing key shortcomings of prior
methods. Comprehensive theoretical analysis validates the rationale behind
adaptive budget allocation and sample selection, while extensive experiments
across 14 diverse datasets and model architectures demonstrate NUCS's
consistent improvements over state-of-the-art methods, achieving superior
accuracy and computational efficiency. Notably, on CIFAR100 and Food101, NUCS
matches full-data training accuracy while retaining just 30% of samples and
reducing computation time by 60%. Our work highlights the importance of
characterizing category difficulty in coreset selection, offering a robust and
data-efficient solution for transfer learning.

</details>

### [35] [NNTile: a machine learning framework capable of training extremely large GPT language models on a single node](https://arxiv.org/abs/2504.13236)
*Aleksandr Mikhalev,Aleksandr Katrutsa,Konstantin Sozykin,Ivan Oseledets*

Main category: cs.LG

TLDR: NNTile框架利用StarPU库实现异构集群中大型深度神经网络的训练，通过任务并行和自动调度优化计算资源分配。


<details>
  <summary>Details</summary>
Motivation: 解决大型神经网络训练中计算资源分配和调度的复杂性，减轻人工决策负担。

Method: 基于StarPU库的任务并行和自动调度机制，支持CPU和GPU的异构计算。

Result: 在大型语言模型训练中展示了良好的性能表现。

Conclusion: NNTile框架通过自动调度显著提升了大型神经网络训练的效率和灵活性。

Abstract: This study presents an NNTile framework for training large deep neural
networks in heterogeneous clusters. The NNTile is based on a StarPU library,
which implements task-based parallelism and schedules all provided tasks onto
all available processing units (CPUs and GPUs). It means that a particular
operation, necessary to train a large neural network, can be performed on any
of the CPU cores or GPU devices, depending on automatic scheduling decisions.
Such an approach shifts the burden of deciding where to compute and when to
communicate from a human being to an automatic decision maker, whether a simple
greedy heuristic or a complex AI-based software. The performance of the
presented tool for training large language models is demonstrated in extensive
numerical experiments.

</details>

### [36] [Recursive Deep Inverse Reinforcement Learning](https://arxiv.org/abs/2504.13241)
*Paul Ghanem,Michael Potter,Owen Howell,Pau Closas,Alireza Ramezani,Deniz Erdogmus,Robert Platt,Tales Imbiriba*

Main category: cs.LG

TLDR: 提出了一种在线递归深度逆强化学习方法（RDIRL），用于实时推断对手的目标和成本函数，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度逆强化学习方法多为离线且计算量大，难以应用于实时场景，需改进以适应实际需求。

Method: 采用递归深度逆强化学习，通过二阶牛顿更新最小化目标函数，类似扩展卡尔曼滤波，实现快速收敛。

Result: 在标准及对抗性基准任务中，RDIRL能有效恢复专家代理的成本和奖励函数，性能优于其他领先算法。

Conclusion: RDIRL方法在实时性和性能上优于现有逆强化学习方法，适用于实际场景。

Abstract: Inferring an adversary's goals from exhibited behavior is crucial for
counterplanning and non-cooperative multi-agent systems in domains like
cybersecurity, military, and strategy games. Deep Inverse Reinforcement
Learning (IRL) methods based on maximum entropy principles show promise in
recovering adversaries' goals but are typically offline, require large batch
sizes with gradient descent, and rely on first-order updates, limiting their
applicability in real-time scenarios. We propose an online Recursive Deep
Inverse Reinforcement Learning (RDIRL) approach to recover the cost function
governing the adversary actions and goals. Specifically, we minimize an upper
bound on the standard Guided Cost Learning (GCL) objective using sequential
second-order Newton updates, akin to the Extended Kalman Filter (EKF), leading
to a fast (in terms of convergence) learning algorithm. We demonstrate that
RDIRL is able to recover cost and reward functions of expert agents in standard
and adversarial benchmark tasks. Experiments on benchmark tasks show that our
proposed approach outperforms several leading IRL algorithms.

</details>

### [37] [Graph Learning at Scale: Characterizing and Optimizing Pre-Propagation GNNs](https://arxiv.org/abs/2504.13266)
*Zichao Yue,Chenhui Deng,Zhiru Zhang*

Main category: cs.LG

TLDR: PP-GNNs通过预处理解耦特征传播与训练，解决了GNNs的邻居爆炸问题，但数据加载和输入扩展仍是瓶颈。本文优化了数据加载和训练方法，显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: GNNs的消息传递机制导致邻居爆炸问题，计算和内存需求随层数指数增长。PP-GNNs理论上解决了这一问题，但实际优势和系统优化尚未充分探索。

Method: 本文全面分析了PP-GNNs，并与基于图采样的方法在训练效率、可扩展性和准确性上进行比较。提出了优化的数据加载方案和训练方法。

Result: PP-GNNs在准确性上表现相当，但数据加载是训练效率的瓶颈。优化后，训练吞吐量平均提升15倍，最大提速达2个数量级。

Conclusion: PP-GNNs通过系统优化显著提升了训练效率，为大规模图学习提供了实用解决方案。

Abstract: Graph neural networks (GNNs) are widely used for learning node embeddings in
graphs, typically adopting a message-passing scheme. This approach, however,
leads to the neighbor explosion problem, with exponentially growing
computational and memory demands as layers increase. Graph sampling has become
the predominant method for scaling GNNs to large graphs, mitigating but not
fully solving the issue. Pre-propagation GNNs (PP-GNNs) represent a new class
of models that decouple feature propagation from training through
pre-processing, addressing neighbor explosion in theory. Yet, their practical
advantages and system-level optimizations remain underexplored. This paper
provides a comprehensive characterization of PP-GNNs, comparing them with
graph-sampling-based methods in training efficiency, scalability, and accuracy.
While PP-GNNs achieve comparable accuracy, we identify data loading as the key
bottleneck for training efficiency and input expansion as a major scalability
challenge. To address these issues, we propose optimized data loading schemes
and tailored training methods that improve PP-GNN training throughput by an
average of 15$\times$ over the PP-GNN baselines, with speedup of up to 2 orders
of magnitude compared to sampling-based GNNs on large graph benchmarks. Our
implementation is publicly available at
https://github.com/cornell-zhang/preprop-gnn.

</details>

### [38] [Let Me Grok for You: Accelerating Grokking via Embedding Transfer from a Weaker Model](https://arxiv.org/abs/2504.13292)
*Zhiwei Xu,Zhiyu Ni,Yixin Wang,Wei Hu*

Main category: cs.LG

TLDR: GrokTransfer是一种加速神经网络训练中“顿悟”现象的方法，通过提取小模型的嵌入初始化大模型，避免延迟泛化。


<details>
  <summary>Details</summary>
Motivation: “顿悟”现象导致神经网络在训练初期表现不佳，随后突然泛化，影响效率和可预测性。本文旨在解决这一问题。

Method: 提出GrokTransfer方法，先训练小模型提取嵌入，再用于初始化大模型。

Result: 在合成任务和多种实际任务中，GrokTransfer成功消除延迟泛化现象。

Conclusion: GrokTransfer是一种简单有效的方法，可显著改善神经网络的训练动态和泛化性能。

Abstract: ''Grokking'' is a phenomenon where a neural network first memorizes training
data and generalizes poorly, but then suddenly transitions to near-perfect
generalization after prolonged training. While intriguing, this delayed
generalization phenomenon compromises predictability and efficiency. Ideally,
models should generalize directly without delay. To this end, this paper
proposes GrokTransfer, a simple and principled method for accelerating grokking
in training neural networks, based on the key observation that data embedding
plays a crucial role in determining whether generalization is delayed.
GrokTransfer first trains a smaller, weaker model to reach a nontrivial (but
far from optimal) test performance. Then, the learned input embedding from this
weaker model is extracted and used to initialize the embedding in the target,
stronger model. We rigorously prove that, on a synthetic XOR task where delayed
generalization always occurs in normal training, GrokTransfer enables the
target model to generalize directly without delay. Moreover, we demonstrate
that, across empirical studies of different tasks, GrokTransfer effectively
reshapes the training dynamics and eliminates delayed generalization, for both
fully-connected neural networks and Transformers.

</details>

### [39] [Enhanced Pruning Strategy for Multi-Component Neural Architectures Using Component-Aware Graph Analysis](https://arxiv.org/abs/2504.13296)
*Ganesh Sundaram,Jonas Ulmen,Daniel Görges*

Main category: cs.LG

TLDR: 论文提出了一种组件感知的剪枝策略，通过扩展依赖图来隔离多组件神经架构中的单个组件和组件间流，从而在保持功能完整性的同时实现更高的稀疏性和更低的性能下降。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNNs）在资源受限的环境中部署时，其复杂性成为障碍。现有的结构化剪枝框架在多组件神经架构（MCNAs）中可能破坏网络完整性。

Method: 通过扩展依赖图，隔离单个组件和组件间流，形成更小、更有针对性的剪枝组。

Result: 在控制任务中验证了方法的有效性，实现了更高的稀疏性和更低的性能下降。

Conclusion: 该方法为优化复杂多组件DNNs提供了高效路径。

Abstract: Deep neural networks (DNNs) deliver outstanding performance, but their
complexity often prohibits deployment in resource-constrained settings.
Comprehensive structured pruning frameworks based on parameter dependency
analysis reduce model size with specific regard to computational performance.
When applying them to Multi-Component Neural Architectures (MCNAs), they risk
network integrity by removing large parameter groups. We introduce a
component-aware pruning strategy, extending dependency graphs to isolate
individual components and inter-component flows. This creates smaller, targeted
pruning groups that conserve functional integrity. Demonstrated effectively on
a control task, our approach achieves greater sparsity and reduced performance
degradation, opening a path for optimizing complex, multi-component DNNs
efficiently.

</details>

### [40] [Training Autoencoders Using Stochastic Hessian-Free Optimization with LSMR](https://arxiv.org/abs/2504.13302)
*Ibrahim Emirahmetoglu,David E. Stewart*

Main category: cs.LG

TLDR: 本文提出了一种基于LSMR方法和新样本选择算法的随机Hessian-free优化方法，用于加速深度自编码器的训练，并提高泛化性能。


<details>
  <summary>Details</summary>
Motivation: Hessian-free优化在训练深度自编码器时有效但计算量大，本文旨在通过减少训练数据量和改进优化方法加速训练。

Method: 采用LSMR方法替代共轭梯度算法，结合改进的预处理器，并引入新的小批量选择算法以减少过拟合。

Result: 实验表明，该方法能快速训练深度自编码器，并显著降低泛化误差。

Conclusion: 提出的随机Hessian-free优化方法在训练效率和泛化性能上均优于传统方法。

Abstract: Hessian-free (HF) optimization has been shown to effectively train deep
autoencoders (Martens, 2010). In this paper, we aim to accelerate HF training
of autoencoders by reducing the amount of data used in training. HF utilizes
the conjugate gradient algorithm to estimate update directions. Instead, we
propose using the LSMR method, which is known for effectively solving large
sparse linear systems. We also incorporate Chapelle & Erhan (2011)'s improved
preconditioner for HF optimization. In addition, we introduce a new mini-batch
selection algorithm to mitigate overfitting. Our algorithm starts with a small
subset of the training data and gradually increases the mini-batch size based
on (i) variance estimates obtained during the computation of a mini-batch
gradient (Byrd et al., 2012) and (ii) the relative decrease in objective value
for the validation data. Our experimental results demonstrate that our
stochastic Hessian-free optimization, using the LSMR method and the new sample
selection algorithm, leads to rapid training of deep autoencoders with improved
generalization error.

</details>

### [41] [Wearable-Derived Behavioral and Physiological Biomarkers for Classifying Unipolar and Bipolar Depression Severity](https://arxiv.org/abs/2504.13331)
*Yassine Ouzar,Clémence Nineuil,Fouad Boutaleb,Emery Pierson,Ali Amad,Mohamed Daoudi*

Main category: cs.LG

TLDR: 利用可穿戴设备预测抑郁症亚型（单极和双极抑郁），通过生理和行为信号识别生物标志物，提升诊断精度。


<details>
  <summary>Details</summary>
Motivation: 现有研究多采用二元分类（健康与抑郁），无法捕捉抑郁症的异质性，需更精确的亚型分类方法。

Method: 引入CALYPSO数据集，结合生理和行为信号（如脉搏、皮肤电活动、体温、加速度），使用标准机器学习方法建立基准。

Result: 加速度数据提取的体力活动特征区分单极和双极抑郁效果最佳（准确率96.77%），体温特征次之（93.55%）。

Conclusion: 生理和行为监测有望改善抑郁症亚型分类，推动个性化临床干预。

Abstract: Depression is a complex mental disorder characterized by a diverse range of
observable and measurable indicators that go beyond traditional subjective
assessments. Recent research has increasingly focused on objective, passive,
and continuous monitoring using wearable devices to gain more precise insights
into the physiological and behavioral aspects of depression. However, most
existing studies primarily distinguish between healthy and depressed
individuals, adopting a binary classification that fails to capture the
heterogeneity of depressive disorders. In this study, we leverage wearable
devices to predict depression subtypes-specifically unipolar and bipolar
depression-aiming to identify distinctive biomarkers that could enhance
diagnostic precision and support personalized treatment strategies. To this
end, we introduce the CALYPSO dataset, designed for non-invasive detection of
depression subtypes and symptomatology through physiological and behavioral
signals, including blood volume pulse, electrodermal activity, body
temperature, and three-axis acceleration. Additionally, we establish a
benchmark on the dataset using well-known features and standard machine
learning methods. Preliminary results indicate that features related to
physical activity, extracted from accelerometer data, are the most effective in
distinguishing between unipolar and bipolar depression, achieving an accuracy
of $96.77\%$. Temperature-based features also showed high discriminative power,
reaching an accuracy of $93.55\%$. These findings highlight the potential of
physiological and behavioral monitoring for improving the classification of
depressive subtypes, paving the way for more tailored clinical interventions.

</details>

### [42] [Denoising and Reconstruction of Nonlinear Dynamics using Truncated Reservoir Computing](https://arxiv.org/abs/2504.13355)
*Omid Sedehi,Manish Yadav,Merten Stender,Sebastian Oberst*

Main category: cs.LG

TLDR: 本文提出了一种基于储层计算（RC）的新方法，用于噪声过滤和非线性动态重建，并通过优化超参数提升性能。


<details>
  <summary>Details</summary>
Motivation: 分布式物理系统的测量数据通常稀疏且噪声大，而传统方法难以在未知动态方程的情况下有效处理。RC技术虽有潜力，但其在噪声环境中的应用尚未充分探索。

Method: 提出了一种新的RC方法，结合超参数优化（如泄漏率、谱半径等），并通过截断冗余节点和边来提升性能。

Result: 在Lorenz吸引子和AdEx系统的实验中，该方法在低信噪比和高频范围内表现出色，优于扩展卡尔曼滤波器（EKF）。

Conclusion: 该RC框架在噪声过滤和动态重建方面具有竞争力，且能泛化到未见过的动态系统。

Abstract: Measurements acquired from distributed physical systems are often sparse and
noisy. Therefore, signal processing and system identification tools are
required to mitigate noise effects and reconstruct unobserved dynamics from
limited sensor data. However, this process is particularly challenging because
the fundamental equations governing the dynamics are largely unavailable in
practice. Reservoir Computing (RC) techniques have shown promise in efficiently
simulating dynamical systems through an unstructured and efficient computation
graph comprising a set of neurons with random connectivity. However, the
potential of RC to operate in noisy regimes and distinguish noise from the
primary dynamics of the system has not been fully explored. This paper presents
a novel RC method for noise filtering and reconstructing nonlinear dynamics,
offering a novel learning protocol associated with hyperparameter optimization.
The performance of the RC in terms of noise intensity, noise frequency content,
and drastic shifts in dynamical parameters are studied in two illustrative
examples involving the nonlinear dynamics of the Lorenz attractor and adaptive
exponential integrate-and-fire system (AdEx). It is shown that the denoising
performance improves via truncating redundant nodes and edges of the computing
reservoir, as well as properly optimizing the hyperparameters, e.g., the
leakage rate, the spectral radius, the input connectivity, and the ridge
regression parameter. Furthermore, the presented framework shows good
generalization behavior when tested for reconstructing unseen attractors from
the bifurcation diagram. Compared to the Extended Kalman Filter (EKF), the
presented RC framework yields competitive accuracy at low signal-to-noise
ratios (SNRs) and high-frequency ranges.

</details>

### [43] [An Optimal Discriminator Weighted Imitation Perspective for Reinforcement Learning](https://arxiv.org/abs/2504.13368)
*Haoran Xu,Shuozhe Li,Harshit Sikchi,Scott Niekum,Amy Zhang*

Main category: cs.LG

TLDR: IDRL是一种新的强化学习方法，通过迭代优化访问分布比率，显著提升了离线数据集上的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 发现当前Dual-RL方法未能正确估计最优访问分布比率，提出IDRL以迭代优化该比率。

Method: IDRL通过迭代移除次优转换并运行Dual-RL，逐步逼近最优访问分布比率。

Result: IDRL在多种离线数据集上表现优于Primal-RL和Dual-RL基线，包括D4RL数据集和真实演示数据。

Conclusion: IDRL通过迭代优化访问分布比率，显著提升了离线强化学习的性能和稳定性。

Abstract: We introduce Iterative Dual Reinforcement Learning (IDRL), a new method that
takes an optimal discriminator-weighted imitation view of solving RL. Our
method is motivated by a simple experiment in which we find training a
discriminator using the offline dataset plus an additional expert dataset and
then performing discriminator-weighted behavior cloning gives strong results on
various types of datasets. That optimal discriminator weight is quite similar
to the learned visitation distribution ratio in Dual-RL, however, we find that
current Dual-RL methods do not correctly estimate that ratio. In IDRL, we
propose a correction method to iteratively approach the optimal visitation
distribution ratio in the offline dataset given no addtional expert dataset.
During each iteration, IDRL removes zero-weight suboptimal transitions using
the learned ratio from the previous iteration and runs Dual-RL on the remaining
subdataset. This can be seen as replacing the behavior visitation distribution
with the optimized visitation distribution from the previous iteration, which
theoretically gives a curriculum of improved visitation distribution ratios
that are closer to the optimal discriminator weight. We verify the
effectiveness of IDRL on various kinds of offline datasets, including D4RL
datasets and more realistic corrupted demonstrations. IDRL beats strong
Primal-RL and Dual-RL baselines in terms of both performance and stability, on
all datasets.

</details>

### [44] [A mean teacher algorithm for unlearning of language models](https://arxiv.org/abs/2504.13388)
*Yegor Klochkov*

Main category: cs.LG

TLDR: 本文研究了使用均值教师算法和负对数非似然损失（NLUL）来减少语言模型对特定文本实例的记忆，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 减少语言模型对特定文本实例的记忆，同时不显著降低模型性能，是一个具有挑战性的问题。

Method: 采用均值教师算法和负对数非似然损失（NLUL）来优化模型。

Result: 在MUSE基准测试中，该方法在某些指标上表现更好。

Conclusion: 均值教师算法与NLUL结合可以有效减少记忆问题，同时保持模型性能。

Abstract: One of the goals of language model unlearning is to reduce memorization of
selected text instances while retaining the model's general abilities. Despite
various proposed methods, reducing memorization of large datasets without
noticeable degradation in model utility remains challenging. In this paper, we
investigate the mean teacher algorithm (Tarvainen & Valpola, 2017), a simple
proximal optimization method from continual learning literature that gradually
modifies the teacher model. We show that the mean teacher can approximate a
trajectory of a slow natural gradient descent (NGD), which inherently seeks
low-curvature updates that are less likely to degrade the model utility. While
slow NGD can suffer from vanishing gradients, we introduce a new unlearning
loss called "negative log-unlikelihood" (NLUL) that avoids this problem. We
show that the combination of mean teacher and NLUL improves some metrics on the
MUSE benchmarks (Shi et al., 2024).

</details>

### [45] [A Model-Based Approach to Imitation Learning through Multi-Step Predictions](https://arxiv.org/abs/2504.13413)
*Haldun Balim,Yang Hu,Yuyang Zhang,Na Li*

Main category: cs.LG

TLDR: 提出了一种基于模型预测控制的模仿学习框架，通过多步状态预测解决传统方法中的误差累积和泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法在复杂决策任务中常因误差累积和训练与部署间的分布偏移而表现不佳。

Method: 结合模型预测控制，通过多步状态预测进行建模，提升误差纠正能力和泛化性。

Result: 在数值基准测试中优于传统行为克隆方法，对分布偏移和测量噪声表现出更强的鲁棒性。

Conclusion: 提供了理论保证，证明了方法的样本复杂性和误差界限，验证了其收敛性。

Abstract: Imitation learning is a widely used approach for training agents to replicate
expert behavior in complex decision-making tasks. However, existing methods
often struggle with compounding errors and limited generalization, due to the
inherent challenge of error correction and the distribution shift between
training and deployment. In this paper, we present a novel model-based
imitation learning framework inspired by model predictive control, which
addresses these limitations by integrating predictive modeling through
multi-step state predictions. Our method outperforms traditional behavior
cloning numerical benchmarks, demonstrating superior robustness to distribution
shift and measurement noise both in available data and during execution.
Furthermore, we provide theoretical guarantees on the sample complexity and
error bounds of our method, offering insights into its convergence properties.

</details>

### [46] [STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings](https://arxiv.org/abs/2504.13416)
*Saksham Rastogi,Pratyush Maini,Danish Pruthi*

Main category: cs.LG

TLDR: STAMP是一个用于检测数据集成员资格的框架，通过生成带有水印的重述版本，比较模型对公开和私有版本的可能性，以证明数据是否被用于训练大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 数据创作者和基准测试管理者担心其专有数据被未经许可用于训练大型语言模型，STAMP旨在解决这一问题。

Method: 生成多个带有唯一密钥水印的重述版本，公开一个版本，保留其他版本，通过统计测试比较模型对公开和私有版本的可能性。

Result: STAMP能成功检测到训练数据中的污染，优于其他基准方法，并保持原始数据的语义和实用性。

Conclusion: STAMP在真实场景中验证了论文摘要和博客文章是否被用于预训练语料库，为数据成员资格检测提供了有效工具。

Abstract: Given how large parts of publicly available text are crawled to pretrain
large language models (LLMs), data creators increasingly worry about the
inclusion of their proprietary data for model training without attribution or
licensing. Their concerns are also shared by benchmark curators whose test-sets
might be compromised. In this paper, we present STAMP, a framework for
detecting dataset membership-i.e., determining the inclusion of a dataset in
the pretraining corpora of LLMs. Given an original piece of content, our
proposal involves first generating multiple rephrases, each embedding a
watermark with a unique secret key. One version is to be released publicly,
while others are to be kept private. Subsequently, creators can compare model
likelihoods between public and private versions using paired statistical tests
to prove membership. We show that our framework can successfully detect
contamination across four benchmarks which appear only once in the training
data and constitute less than 0.001% of the total tokens, outperforming several
contamination detection and dataset inference baselines. We verify that STAMP
preserves both the semantic meaning and the utility of the original data in
comparing different models. We apply STAMP to two real-world scenarios to
confirm the inclusion of paper abstracts and blog articles in the pretraining
corpora.

</details>

### [47] [Equilibrium Conserving Neural Operators for Super-Resolution Learning](https://arxiv.org/abs/2504.13422)
*Vivek Oommen,Andreas E. Robertson,Daniel Diaz,Coleman Alleman,Zhen Zhang,Anthony D. Rollett,George E. Karniadakis,Rémi Dingreville*

Main category: cs.LG

TLDR: 本文提出了一种名为ECO的框架，通过低分辨率数据训练高分辨率神经网络，嵌入已知物理规律以减少对高分辨率数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统神经代理求解器需要大量高分辨率训练数据，限制了其效率。本文旨在打破这一限制。

Method: 采用ECO架构，将已知物理规律直接嵌入网络，弥补训练中缺失的高分辨率信息。

Result: ECO框架在固体力学问题中显著减少了对高保真数据的依赖，数据收集成本降低两个数量级。

Conclusion: ECO为材料建模中的资源高效代理建模提供了稳健途径，并可推广到其他基于物理的问题。

Abstract: Neural surrogate solvers can estimate solutions to partial differential
equations in physical problems more efficiently than standard numerical
methods, but require extensive high-resolution training data. In this paper, we
break this limitation; we introduce a framework for super-resolution learning
in solid mechanics problems. Our approach allows one to train a high-resolution
neural network using only low-resolution data. Our Equilibrium Conserving
Operator (ECO) architecture embeds known physics directly into the network to
make up for missing high-resolution information during training. We evaluate
this ECO-based super-resolution framework that strongly enforces
conservation-laws in the predicted solutions on two working examples: embedded
pores in a homogenized matrix and randomly textured polycrystalline materials.
ECO eliminates the reliance on high-fidelity data and reduces the upfront cost
of data collection by two orders of magnitude, offering a robust pathway for
resource-efficient surrogate modeling in materials modeling. ECO is readily
generalizable to other physics-based problems.

</details>

### [48] [Simplifying Graph Convolutional Networks with Redundancy-Free Neighbors](https://arxiv.org/abs/2504.13426)
*Jielong LuZhihao Wu,Zhiling Cai,Yueyang Pi,Shiping Wang*

Main category: cs.LG

TLDR: 论文分析了图卷积网络（GCNs）中消息传递机制的问题，提出过度聚合是导致过平滑现象的根本原因。


<details>
  <summary>Details</summary>
Motivation: 现有GCN方法因过平滑现象通常采用浅层架构，现有改进方法效果有限，需深入分析其内在机制。

Method: 分析GCN的消息传递机制，识别高阶邻居信息需通过低阶邻居传递的问题，提出过度聚合概念。

Result: 研究表明过度聚合不仅引入冗余信息，还是GCN中过平滑现象的根本原因。

Conclusion: 过度聚合是GCN过平滑的核心问题，为未来改进GCN架构提供了新方向。

Abstract: In recent years, Graph Convolutional Networks (GCNs) have gained popularity
for their exceptional ability to process graph-structured data. Existing
GCN-based approaches typically employ a shallow model architecture due to the
over-smoothing phenomenon. Current approaches to mitigating over-smoothing
primarily involve adding supplementary components to GCN architectures, such as
residual connections and random edge-dropping strategies. However, these
improvements toward deep GCNs have achieved only limited success. In this work,
we analyze the intrinsic message passing mechanism of GCNs and identify a
critical issue: messages originating from high-order neighbors must traverse
through low-order neighbors to reach the target node. This repeated reliance on
low-order neighbors leads to redundant information aggregation, a phenomenon we
term over-aggregation. Our analysis demonstrates that over-aggregation not only
introduces significant redundancy but also serves as the fundamental cause of
over-smoothing in GCNs.

</details>

### [49] [Bounded and Uniform Energy-based Out-of-distribution Detection for Graphs](https://arxiv.org/abs/2504.13429)
*Shenzhi Yang,Bin Liang,An Liu,Lin Gui,Xingkai Yao,Xiaofang Zhang*

Main category: cs.LG

TLDR: 论文提出NODESAFE方法，通过优化负能量分数和缓解logit偏移，显著提升GNN在节点级别检测OOD数据的能力。


<details>
  <summary>Details</summary>
Motivation: 图在现实应用中具有重要性和高安全性需求，但现有GNNSAFE框架在节点级OOD检测中因分数极端值和logit偏移而受限。

Method: 提出NODESAFE，通过添加两个优化项使负能量分数有界并缓解logit偏移。

Result: 实验显示，NODESAFE显著提升OOD检测能力，FPR95指标在无（有）OOD数据暴露场景下分别降低28.4%（22.7%）。

Conclusion: NODESAFE有效解决了极端分数问题，提升了GNN在节点级OOD检测中的准确性。

Abstract: Given the critical role of graphs in real-world applications and their
high-security requirements, improving the ability of graph neural networks
(GNNs) to detect out-of-distribution (OOD) data is an urgent research problem.
The recent work GNNSAFE proposes a framework based on the aggregation of
negative energy scores that significantly improves the performance of GNNs to
detect node-level OOD data. However, our study finds that score aggregation
among nodes is susceptible to extreme values due to the unboundedness of the
negative energy scores and logit shifts, which severely limits the accuracy of
GNNs in detecting node-level OOD data. In this paper, we propose NODESAFE:
reducing the generation of extreme scores of nodes by adding two optimization
terms that make the negative energy scores bounded and mitigate the logit
shift. Experimental results show that our approach dramatically improves the
ability of GNNs to detect OOD data at the node level, e.g., in detecting OOD
data induced by Structure Manipulation, the metric of FPR95 (lower is better)
in scenarios without (with) OOD data exposure are reduced from the current SOTA
by 28.4% (22.7%).

</details>

### [50] [Using Machine Learning and Neural Networks to Analyze and Predict Chaos in Multi-Pendulum and Chaotic Systems](https://arxiv.org/abs/2504.13453)
*Vasista Ramachandruni,Sai Hruday Reddy Nara,Geo Lalu,Sabrina Yang,Mohit Ramesh Kumar,Aarjav Jain,Pratham Mehta,Hankyu Koo,Jason Damonte,Marx Akl*

Main category: cs.LG

TLDR: 该研究评估了10种机器学习模型和神经网络在预测多摆混沌系统中的表现，发现LSTM和VRNN在不同场景下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 混沌系统广泛存在于自然界和社会中，预测这些系统对社会有重要意义。

Method: 生成合成数据并使用滑动窗口和时间步长方法评估模型，同时用Lyapunov指数分析稳定性。

Result: LSTM在双摆中表现最佳，VRNN和GRU在三摆中表现优异。

Conclusion: LSTM和VRNN是预测多摆混沌系统的有效模型。

Abstract: A chaotic system is a highly volatile system characterized by its sensitive
dependence on initial conditions and outside factors. Chaotic systems are
prevalent throughout the world today: in weather patterns, disease outbreaks,
and even financial markets. Chaotic systems are seen in every field of science
and humanities, so being able to predict these systems is greatly beneficial to
society. In this study, we evaluate 10 different machine learning models and
neural networks [1] based on Root Mean Squared Error (RMSE) and R^2 values for
their ability to predict one of these systems, the multi-pendulum. We begin by
generating synthetic data representing the angles of the pendulum over time
using the Runge Kutta Method for solving 4th Order Differential Equations
(ODE-RK4) [2]. At first, we used the single-step sliding window approach,
predicting the 50st step after training for steps 0-49 and so forth. However,
to more accurately cover chaotic motion and behavior in these systems, we
transitioned to a time-step based approach. Here, we trained the model/network
on many initial angles and tested it on a completely new set of initial angles,
or 'in-between' to capture chaotic motion to its fullest extent. We also
evaluated the stability of the system using Lyapunov exponents. We concluded
that for a double pendulum, the best model was the Long Short Term Memory
Network (LSTM)[3] for the sliding window and time step approaches in both
friction and frictionless scenarios. For triple pendulum, the Vanilla Recurrent
Neural Network (VRNN)[4] was the best for the sliding window and Gated
Recurrent Network (GRU) [5] was the best for the time step approach, but for
friction, LSTM was the best.

</details>

### [51] [Stratify: Rethinking Federated Learning for Non-IID Data through Balanced Sampling](https://arxiv.org/abs/2504.13462)
*Hui Yeok Wong,Chee Kau Lim,Chee Seng Chan*

Main category: cs.LG

TLDR: Stratify是一种新型联邦学习框架，通过分层标签调度和标签感知客户端选择策略，系统性解决非独立同分布数据中的异质性问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在非独立同分布数据上表现不佳，仅通过增量调整解决表面问题，未能从根本上解决数据异质性。

Method: 提出分层标签调度（SLS）平衡标签暴露，结合标签感知客户端选择和高频更新方案，同时使用同态加密保护隐私。

Result: 在多个数据集上验证，Stratify性能接近独立同分布基线，加速收敛并减少客户端计算。

Conclusion: Stratify有效解决了非独立同分布数据中的根本问题，具有实际应用价值。

Abstract: Federated Learning (FL) on non-independently and identically distributed
(non-IID) data remains a critical challenge, as existing approaches struggle
with severe data heterogeneity. Current methods primarily address symptoms of
non-IID by applying incremental adjustments to Federated Averaging (FedAvg),
rather than directly resolving its inherent design limitations. Consequently,
performance significantly deteriorates under highly heterogeneous conditions,
as the fundamental issue of imbalanced exposure to diverse class and feature
distributions remains unresolved. This paper introduces Stratify, a novel FL
framework designed to systematically manage class and feature distributions
throughout training, effectively tackling the root cause of non-IID challenges.
Inspired by classical stratified sampling, our approach employs a Stratified
Label Schedule (SLS) to ensure balanced exposure across labels, significantly
reducing bias and variance in aggregated gradients. Complementing SLS, we
propose a label-aware client selection strategy, restricting participation
exclusively to clients possessing data relevant to scheduled labels.
Additionally, Stratify incorporates a fine-grained, high-frequency update
scheme, accelerating convergence and further mitigating data heterogeneity. To
uphold privacy, we implement a secure client selection protocol leveraging
homomorphic encryption, enabling precise global label statistics without
disclosing sensitive client information. Extensive evaluations on MNIST,
CIFAR-10, CIFAR-100, Tiny-ImageNet, COVTYPE, PACS, and Digits-DG demonstrate
that Stratify attains performance comparable to IID baselines, accelerates
convergence, and reduces client-side computation compared to state-of-the-art
methods, underscoring its practical effectiveness in realistic federated
learning scenarios.

</details>

### [52] [Are you SURE? Enhancing Multimodal Pretraining with Missing Modalities through Uncertainty Estimation](https://arxiv.org/abs/2504.13465)
*Duy A. Nguyen,Quan Huu Do,Khoa D. Doan,Minh N. Do*

Main category: cs.LG

TLDR: SURE框架通过潜在空间重建和不确定性估计，提升了预训练多模态模型在不完整数据下的性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中多模态数据常缺失，现有方法忽略重建的不可靠性，影响输出质量。

Method: 引入潜在空间重建和不确定性估计，使用Pearson相关性损失和统计误差传播。

Result: 在情感分析、类型分类和动作识别等任务中表现优异。

Conclusion: SURE在不完整数据下提供可靠预测，提升了模型实用性和解释性。

Abstract: Multimodal learning has demonstrated incredible successes by integrating
diverse data sources, yet it often relies on the availability of all modalities
- an assumption that rarely holds in real-world applications. Pretrained
multimodal models, while effective, struggle when confronted with small-scale
and incomplete datasets (i.e., missing modalities), limiting their practical
applicability. Previous studies on reconstructing missing modalities have
overlooked the reconstruction's potential unreliability, which could compromise
the quality of the final outputs. We present SURE (Scalable Uncertainty and
Reconstruction Estimation), a novel framework that extends the capabilities of
pretrained multimodal models by introducing latent space reconstruction and
uncertainty estimation for both reconstructed modalities and downstream tasks.
Our method is architecture-agnostic, reconstructs missing modalities, and
delivers reliable uncertainty estimates, improving both interpretability and
performance. SURE introduces a unique Pearson Correlation-based loss and
applies statistical error propagation in deep networks for the first time,
allowing precise quantification of uncertainties from missing data and model
predictions. Extensive experiments across tasks such as sentiment analysis,
genre classification, and action recognition show that SURE consistently
achieves state-of-the-art performance, ensuring robust predictions even in the
presence of incomplete data.

</details>

### [53] [Variational Autoencoder Framework for Hyperspectral Retrievals (Hyper-VAE) of Phytoplankton Absorption and Chlorophyll a in Coastal Waters for NASA's EMIT and PACE Missions](https://arxiv.org/abs/2504.13476)
*Jiadong Lou,Bingqing Liu,Yuanheng Xiong,Xiaodong Zhang,Xu Yuan*

Main category: cs.LG

TLDR: 该研究利用变分自编码器（VAE）改进高光谱遥感数据，提升浮游植物吸收系数和叶绿素a的提取精度，尤其在复杂水域表现优异。


<details>
  <summary>Details</summary>
Motivation: 浮游植物对水色的影响微小但可被高光谱传感器捕捉，传统方法在复杂水域效果有限，需新技术提升精度。

Method: 采用变分自编码器（VAE）处理高光谱遥感数据，创新设计模型以解决多分布预测问题。

Result: VAE模型在实验验证中表现优异，精度高且偏差低，优于混合密度网络（MDN）。

Conclusion: 结合AI技术的高光谱数据（如EMIT、PACE）将推动对浮游植物群落动态的深入研究。

Abstract: Phytoplankton absorb and scatter light in unique ways, subtly altering the
color of water, changes that are often minor for human eyes to detect but can
be captured by sensitive ocean color instruments onboard satellites from space.
Hyperspectral sensors, paired with advanced algorithms, are expected to
significantly enhance the characterization of phytoplankton community
composition, especially in coastal waters where ocean color remote sensing
applications have historically encountered significant challenges. This study
presents novel machine learning-based solutions for NASA's hyperspectral
missions, including EMIT and PACE, tackling high-fidelity retrievals of
phytoplankton absorption coefficient and chlorophyll a from their hyperspectral
remote sensing reflectance. Given that a single Rrs spectrum may correspond to
varied combinations of inherent optical properties and associated
concentrations, the Variational Autoencoder (VAE) is used as a backbone in this
study to handle such multi-distribution prediction problems. We first time
tailor the VAE model with innovative designs to achieve hyperspectral
retrievals of aphy and of Chl-a from hyperspectral Rrs in optically complex
estuarine-coastal waters. Validation with extensive experimental observation
demonstrates superior performance of the VAE models with high precision and low
bias. The in-depth analysis of VAE's advanced model structures and learning
designs highlights the improvement and advantages of VAE-based solutions over
the mixture density network (MDN) approach, particularly on high-dimensional
data, such as PACE. Our study provides strong evidence that current EMIT and
PACE hyperspectral data as well as the upcoming Surface Biology Geology mission
will open new pathways toward a better understanding of phytoplankton community
dynamics in aquatic ecosystems when integrated with AI technologies.

</details>

### [54] [Safety Monitoring for Learning-Enabled Cyber-Physical Systems in Out-of-Distribution Scenarios](https://arxiv.org/abs/2504.13478)
*Vivian Lin,Ramneet Kaur,Yahan Yang,Souradeep Dutta,Yiannis Kantaros,Anirban Roy,Susmit Jha,Oleg Sokolsky,Insup Lee*

Main category: cs.LG

TLDR: 论文提出了一种直接监控学习型信息物理系统安全性的方法，通过预测信号时序逻辑安全规范的违反情况，并结合自适应共形预测和增量学习，提高了对分布外数据的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过检测分布外数据来监控安全性，但分布外输入并不一定违反安全属性，因此需要一种更直接且鲁棒的方法。

Method: 提出基于预测未来轨迹的安全监控方法，结合自适应共形预测和增量学习，前者提供概率预测保证，后者避免过于保守的预测。

Result: 在两个案例研究中（F1Tenth赛车与静态障碍物碰撞、赛车与动态障碍物碰撞），该方法在分布外数据下仍能保持高召回率和及时性，同时减少精度损失。

Conclusion: 该方法在分布外数据下优于其他方法，并提供了理论保证，为学习型信息物理系统的安全性监控提供了有效解决方案。

Abstract: The safety of learning-enabled cyber-physical systems is compromised by the
well-known vulnerabilities of deep neural networks to out-of-distribution (OOD)
inputs. Existing literature has sought to monitor the safety of such systems by
detecting OOD data. However, such approaches have limited utility, as the
presence of an OOD input does not necessarily imply the violation of a desired
safety property. We instead propose to directly monitor safety in a manner that
is itself robust to OOD data. To this end, we predict violations of signal
temporal logic safety specifications based on predicted future trajectories.
Our safety monitor additionally uses a novel combination of adaptive conformal
prediction and incremental learning. The former obtains probabilistic
prediction guarantees even on OOD data, and the latter prevents overly
conservative predictions. We evaluate the efficacy of the proposed approach in
two case studies on safety monitoring: 1) predicting collisions of an F1Tenth
car with static obstacles, and 2) predicting collisions of a race car with
multiple dynamic obstacles. We find that adaptive conformal prediction obtains
theoretical guarantees where other uncertainty quantification methods fail to
do so. Additionally, combining adaptive conformal prediction and incremental
learning for safety monitoring achieves high recall and timeliness while
reducing loss in precision. We achieve these results even in OOD settings and
outperform alternative methods.

</details>

### [55] [Monitor and Recover: A Paradigm for Future Research on Distribution Shift in Learning-Enabled Cyber-Physical Systems](https://arxiv.org/abs/2504.13484)
*Vivian Lin,Insup Lee*

Main category: cs.LG

TLDR: 论文提出了一种新的“监控与恢复”范式，以替代传统的“检测与回避”方法，用于提高学习型网络在分布偏移下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 神经网络对分布偏移的脆弱性导致学习型网络在物理系统中的可靠性问题，现有方法（检测与回避）在实际应用中效果有限。

Method: 提出“监控与恢复”范式，强调安全监控和分布偏移恢复，而非检测与回避。

Result: 通过两个实例展示了该范式的可行性。

Conclusion: “监控与恢复”是未来研究的有前景方向，能更有效地应对分布偏移问题。

Abstract: With the known vulnerability of neural networks to distribution shift,
maintaining reliability in learning-enabled cyber-physical systems poses a
salient challenge. In response, many existing methods adopt a detect and
abstain methodology, aiming to detect distribution shift at inference time so
that the learning-enabled component can abstain from decision-making. This
approach, however, has limited use in real-world applications. We instead
propose a monitor and recover paradigm as a promising direction for future
research. This philosophy emphasizes 1) robust safety monitoring instead of
distribution shift detection and 2) distribution shift recovery instead of
abstention. We discuss two examples from our recent work.

</details>

### [56] [Integrating Locality-Aware Attention with Transformers for General Geometry PDEs](https://arxiv.org/abs/2504.13480)
*Minsu Koh,Beom-Chul Park,Heejo Kong,Seong-Whan Lee*

Main category: cs.LG

TLDR: 论文提出了一种名为LA2Former的神经网络算子，通过结合全局和局部注意力机制，显著提升了在复杂几何和不规则网格上求解偏微分方程（PDE）的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的神经算子在处理PDE时过于依赖全局特征聚合，忽略了局部动态行为，限制了其在复杂几何和不规则网格上的表现。

Method: 提出LA2Former，利用K近邻动态分块，结合全局线性注意力和局部成对注意力机制，平衡计算效率和预测精度。

Result: 在六个基准数据集上，LA2Former比现有线性注意力方法预测精度提升50%以上，并在最优条件下优于全成对注意力方法。

Conclusion: 局部特征学习对提升基于Transformer的神经算子在复杂和不规则域上求解PDE的性能至关重要。

Abstract: Neural operators have emerged as promising frameworks for learning mappings
governed by partial differential equations (PDEs), serving as data-driven
alternatives to traditional numerical methods. While methods such as the
Fourier neural operator (FNO) have demonstrated notable performance, their
reliance on uniform grids restricts their applicability to complex geometries
and irregular meshes. Recently, Transformer-based neural operators with linear
attention mechanisms have shown potential in overcoming these limitations for
large-scale PDE simulations. However, these approaches predominantly emphasize
global feature aggregation, often overlooking fine-scale dynamics and localized
PDE behaviors essential for accurate solutions. To address these challenges, we
propose the Locality-Aware Attention Transformer (LA2Former), which leverages
K-nearest neighbors for dynamic patchifying and integrates global-local
attention for enhanced PDE modeling. By combining linear attention for
efficient global context encoding with pairwise attention for capturing
intricate local interactions, LA2Former achieves an optimal balance between
computational efficiency and predictive accuracy. Extensive evaluations across
six benchmark datasets demonstrate that LA2Former improves predictive accuracy
by over 50% relative to existing linear attention methods, while also
outperforming full pairwise attention under optimal conditions. This work
underscores the critical importance of localized feature learning in advancing
Transformer-based neural operators for solving PDEs on complex and irregular
domains.

</details>

### [57] [Bitcoin's Edge: Embedded Sentiment in Blockchain Transactional Data](https://arxiv.org/abs/2504.13598)
*Charalampos Kleitsikas,Nikolaos Korfiatis,Stefanos Leonardos,Carmine Ventre*

Main category: cs.LG

TLDR: 该论文利用自然语言处理和机器学习技术分析区块链交易数据中的隐含情绪，首次展示了其在预测比特币和以太坊价格变动中的潜力。


<details>
  <summary>Details</summary>
Motivation: 区块链不仅用于支付，还存储了大量非金融内容，这些内容可能通过传递私人信息、塑造情绪和影响舆论来影响价格变动。然而，现有分析方法在范围和可扩展性上受限。

Method: 采用自然语言处理技术分析区块链交易数据中的情绪模式，并结合多种机器学习方法进行预测。

Result: 研究发现区块链嵌入的情绪数据可以预测加密货币价格变动，比特币在信息优势上优于以太坊。

Conclusion: 区块链情绪分析为加密货币市场的金融预测提供了新颖且稳健的框架，揭示了比特币与以太坊之间的不对称性。

Abstract: Cryptocurrency blockchains, beyond their primary role as distributed payment
systems, are increasingly used to store and share arbitrary content, such as
text messages and files. Although often non-financial, this hidden content can
impact price movements by conveying private information, shaping sentiment, and
influencing public opinion. However, current analyses of such data are limited
in scope and scalability, primarily relying on manual classification or
hand-crafted heuristics. In this work, we address these limitations by
employing Natural Language Processing techniques to analyze, detect patterns,
and extract public sentiment encoded within blockchain transactional data.
Using a variety of Machine Learning techniques, we showcase for the first time
the predictive power of blockchain-embedded sentiment in forecasting
cryptocurrency price movements on the Bitcoin and Ethereum blockchains. Our
findings shed light on a previously underexplored source of freely available,
transparent, and immutable data and introduce blockchain sentiment analysis as
a novel and robust framework for enhancing financial predictions in
cryptocurrency markets. Incidentally, we discover an asymmetry between
cryptocurrencies; Bitcoin has an informational advantage over Ethereum in that
the sentiment embedded into transactional data is sufficient to predict its
price movement.

</details>

### [58] [Latent Tensor Factorization with Nonlinear PID Control for Missing Data Recovery in Non-Intrusive Load Monitoring](https://arxiv.org/abs/2504.13483)
*Yiran Wang,Tangtang Xie,Hao Wu*

Main category: cs.LG

TLDR: 本文提出了一种非线性PID结合的张量分解模型（NPIL），用于解决NILM数据缺失问题，通过结合历史更新信息和PSO算法优化参数，显著提高了收敛速度和预测精度。


<details>
  <summary>Details</summary>
Motivation: NILM数据因传感器故障等原因存在缺失值，传统SGD方法收敛慢且未利用历史信息。

Method: 提出NPIL模型，结合非线性PID控制器原理和PSO算法优化增益参数。

Result: 实验证明NPIL在收敛速度和预测精度上优于现有模型。

Conclusion: NPIL模型有效解决了NILM数据缺失问题，提升了性能。

Abstract: Non-Intrusive Load Monitoring (NILM) has emerged as a key smart grid
technology, identifying electrical device and providing detailed energy
consumption data for precise demand response management. Nevertheless, NILM
data suffers from missing values due to inescapable factors like sensor
failure, leading to inaccuracies in non-intrusive load monitoring. A stochastic
gradient descent (SGD)-based latent factorization of tensors model has proven
to be effective in estimating missing data, however, it updates a latent factor
solely based on the current stochastic gradient, without considering past
information, which leads to slow convergence of anLFT model. To address this
issue, this paper proposes a Nonlinear Proportional-integral-derivative
(PID)-Incorporated Latent factorization of tensors (NPIL) model with two-fold
ideas: a) rebuilding the instant learning error according to the principle of a
nonlinear PID controller, thus, the past update information is efficiently
incorporated into the learning scheme, and b) implementing gain parameter
adaptation by utilizing particle swarm optimization (PSO) algorithm, hence, the
model computational efficiency is effectively improved. Experimental results on
real-world NILM datasets demonstrate that the proposed NPIL model surpasses
state-of-the-art models in convergence rate and accuracy when predicting the
missing NILM data.

</details>

### [59] [Deep Learning Models Meet Financial Data Modalities](https://arxiv.org/abs/2504.13521)
*Kasymkhan Khubiev,Michail Semenov*

Main category: cs.LG

TLDR: 该研究探讨了深度学习在金融数据中的应用，提出了一种新颖的限价订单簿分析方法，提升了高频交易算法的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在非结构化数据处理中表现出色，但其在结构化金融数据中的应用仍具挑战性，研究旨在填补这一空白。

Method: 开发了嵌入技术，将限价订单簿快照作为图像通道输入，结合深度学习模型进行分析。

Result: 该方法在高频交易算法中实现了最先进的性能。

Conclusion: 深度学习在金融应用中具有显著潜力，尤其是在限价订单簿分析方面。

Abstract: Algorithmic trading relies on extracting meaningful signals from diverse
financial data sources, including candlestick charts, order statistics on put
and canceled orders, traded volume data, limit order books, and news flow.
While deep learning has demonstrated remarkable success in processing
unstructured data and has significantly advanced natural language processing,
its application to structured financial data remains an ongoing challenge. This
study investigates the integration of deep learning models with financial data
modalities, aiming to enhance predictive performance in trading strategies and
portfolio optimization. We present a novel approach to incorporating limit
order book analysis into algorithmic trading by developing embedding techniques
and treating sequential limit order book snapshots as distinct input channels
in an image-based representation. Our methodology for processing limit order
book data achieves state-of-the-art performance in high-frequency trading
algorithms, underscoring the effectiveness of deep learning in financial
applications.

</details>

### [60] [Cross-Modal Temporal Fusion for Financial Market Forecasting](https://arxiv.org/abs/2504.13522)
*Yunhua Pei,John Cartlidge,Anandadeep Mandal,Daniel Gold,Enrique Marcilio,Riccardo Mazzon*

Main category: cs.LG

TLDR: 提出了一种基于Transformer的跨模态时序融合框架（CMTF），用于整合异构金融数据以提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常独立处理不同模态数据或未能有效建模其交互，导致预测精度受限。

Method: 采用注意力机制动态加权不同模态的贡献，并结合专门的张量解释模块进行特征提取，同时引入自动化训练方案以优化模型迭代。

Result: 在真实金融数据集上，CMTF在股票价格预测方面优于基线模型。

Conclusion: CMTF为金融市场预测中的跨模态整合提供了一种可扩展且有效的解决方案。

Abstract: Accurate financial market forecasting requires diverse data sources,
including historical price trends, macroeconomic indicators, and financial
news, each contributing unique predictive signals. However, existing methods
often process these modalities independently or fail to effectively model their
interactions. In this paper, we introduce Cross-Modal Temporal Fusion (CMTF), a
novel transformer-based framework that integrates heterogeneous financial data
to improve predictive accuracy. Our approach employs attention mechanisms to
dynamically weight the contribution of different modalities, along with a
specialized tensor interpretation module for feature extraction. To facilitate
rapid model iteration in industry applications, we incorporate a mature
auto-training scheme that streamlines optimization. When applied to real-world
financial datasets, CMTF demonstrates improvements over baseline models in
forecasting stock price movements and provides a scalable and effective
solution for cross-modal integration in financial market prediction.

</details>

### [61] [Risk-aware black-box portfolio construction using Bayesian optimization with adaptive weighted Lagrangian estimator](https://arxiv.org/abs/2504.13529)
*Zinuo You,John Cartlidge,Karen Elliott,Menghan Ge,Daniel Gold*

Main category: cs.LG

TLDR: 提出了一种基于贝叶斯优化的新框架，用于在有限观测下优化黑盒投资组合管理模型，通过自适应权重拉格朗日估计器平衡性能与风险。


<details>
  <summary>Details</summary>
Motivation: 现有投资组合管理模型因行业安全和商业问题多为黑盒模型，性能波动大且评估成本高，优化性能同时控制风险成为关键挑战。

Method: 采用贝叶斯优化框架，提出自适应权重拉格朗日估计器，兼顾最大化模型性能和最小化观测方差。

Result: 在五种回测设置和三种黑盒股票投资组合管理模型上验证了方法的优越性，消融实验进一步证实了估计器的有效性。

Conclusion: 该方法有效解决了黑盒模型优化中的性能与风险平衡问题，为投资组合管理提供了实用工具。

Abstract: Existing portfolio management approaches are often black-box models due to
safety and commercial issues in the industry. However, their performance can
vary considerably whenever market conditions or internal trading strategies
change. Furthermore, evaluating these non-transparent systems is expensive,
where certain budgets limit observations of the systems. Therefore, optimizing
performance while controlling the potential risk of these financial systems has
become a critical challenge. This work presents a novel Bayesian optimization
framework to optimize black-box portfolio management models under limited
observations. In conventional Bayesian optimization settings, the objective
function is to maximize the expectation of performance metrics. However, simply
maximizing performance expectations leads to erratic optimization trajectories,
which exacerbate risk accumulation in portfolio management. Meanwhile, this can
lead to misalignment between the target distribution and the actual
distribution of the black-box model. To mitigate this problem, we propose an
adaptive weight Lagrangian estimator considering dual objective, which
incorporates maximizing model performance and minimizing variance of model
observations. Extensive experiments demonstrate the superiority of our approach
over five backtest settings with three black-box stock portfolio management
models. Ablation studies further verify the effectiveness of the proposed
estimator.

</details>

### [62] [Can Local Representation Alignment RNNs Solve Temporal Tasks?](https://arxiv.org/abs/2504.13531)
*Nikolay Manchev,Luis C. Garcia-Peraza-Herrera*

Main category: cs.LG

TLDR: 本文提出了一种基于目标传播的RNN训练方法，使用局部表示对齐（LRA）以减少梯度爆炸和消失问题，并通过梯度正则化改进模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的BPTT算法存在梯度不稳定和生物不现实的问题，限制了RNN的实际应用。本文旨在通过局部更新方法解决这些问题。

Method: 采用局部表示对齐（LRA）方法，分析其性能，实验不同归一化和局部误差函数，并引入梯度正则化以改善梯度流动。

Result: 尽管LRA方法仍存在梯度消失问题，但梯度正则化显著提升了模型性能，在多个任务中表现优于未正则化版本。

Conclusion: 梯度正则化的LRA RNN在解决梯度消失问题上表现优异，为RNN的稳定训练提供了新思路。

Abstract: Recurrent Neural Networks (RNNs) are commonly used for real-time processing,
streaming data, and cases where the amount of training samples is limited.
Backpropagation Through Time (BPTT) is the predominant algorithm for training
RNNs; however, it is frequently criticized for being prone to exploding and
vanishing gradients and being biologically implausible. In this paper, we
present and evaluate a target propagation-based method for RNNs, which uses
local updates and seeks to reduce the said instabilities. Having stable RNN
models increases their practical use in a wide range of fields such as natural
language processing, time-series forecasting, anomaly detection, control
systems, and robotics.
  The proposed solution uses local representation alignment (LRA). We
thoroughly analyze the performance of this method, experiment with
normalization and different local error functions, and invalidate certain
assumptions about the behavior of this type of learning. Namely, we demonstrate
that despite the decomposition of the network into sub-graphs, the model still
suffers from vanishing gradients. We also show that gradient clipping as
proposed in LRA has little to no effect on network performance. This results in
an LRA RNN model that is very difficult to train due to vanishing gradients. We
address this by introducing gradient regularization in the direction of the
update and demonstrate that this modification promotes gradient flow and
meaningfully impacts convergence. We compare and discuss the performance of the
algorithm, and we show that the regularized LRA RNN considerably outperforms
the unregularized version on three landmark tasks: temporal order, 3-bit
temporal order, and random permutation.

</details>

### [63] [Irregular Sampling of High-Dimensional Functions in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2504.13543)
*Armin Iske,Lennart Ohlsen*

Main category: cs.LG

TLDR: 提出了一种在高维再生核希尔伯特空间中的采样方法，通过低维不规则样本组合成高维样本，显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决高维函数采样中的计算复杂度问题。

Method: 利用张量积核和低维不规则样本组合成高维样本。

Result: 显著降低了高维函数采样的计算复杂度。

Conclusion: 该方法为高维函数采样提供了一种高效的计算方案。

Abstract: We develop sampling formulas for high-dimensional functions in reproducing
kernel Hilbert spaces, where we rely on irregular samples that are taken at
determining sequences of data points. We place particular emphasis on sampling
formulas for tensor product kernels, where we show that determining irregular
samples in lower dimensions can be composed to obtain a tensor of determining
irregular samples in higher dimensions. This in turn reduces the computational
complexity of sampling formulas for high-dimensional functions quite
significantly.

</details>

### [64] [Transformers Can Overcome the Curse of Dimensionality: A Theoretical Study from an Approximation Perspective](https://arxiv.org/abs/2504.13558)
*Yuling Jiao,Yanming Lai,Yang Wang,Bokai Yan*

Main category: cs.LG

TLDR: 本文研究了Transformer模型对Hölder连续函数类的逼近能力，并构建了几种能克服维度灾难的Transformer结构。结果表明，仅需少量层和宽度即可实现高精度逼近，证明了Transformer的强大表达能力。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型在逼近高维连续函数时的能力，以克服维度灾难问题，并简化现有逼近方法的复杂性。

Method: 构建了一种由单头自注意力层和若干前馈层组成的Transformer，利用ReLU和floor等激活函数，基于Kolmogorov-Arnold表示定理进行设计。

Result: 仅需对数级层数和多项式级宽度即可实现高精度逼近，且使用其他激活函数可进一步减少宽度至常数。

Conclusion: Transformer在逼近高维函数时具有强大表达能力，且本文方法简化了证明过程，为后续研究提供了新思路。

Abstract: The Transformer model is widely used in various application areas of machine
learning, such as natural language processing. This paper investigates the
approximation of the H\"older continuous function class
$\mathcal{H}_{Q}^{\beta}\left([0,1]^{d\times n},\mathbb{R}^{d\times n}\right)$
by Transformers and constructs several Transformers that can overcome the curse
of dimensionality. These Transformers consist of one self-attention layer with
one head and the softmax function as the activation function, along with
several feedforward layers. For example, to achieve an approximation accuracy
of $\epsilon$, if the activation functions of the feedforward layers in the
Transformer are ReLU and floor, only
$\mathcal{O}\left(\log\frac{1}{\epsilon}\right)$ layers of feedforward layers
are needed, with widths of these layers not exceeding
$\mathcal{O}\left(\frac{1}{\epsilon^{2/\beta}}\log\frac{1}{\epsilon}\right)$.
If other activation functions are allowed in the feedforward layers, the width
of the feedforward layers can be further reduced to a constant. These results
demonstrate that Transformers have a strong expressive capability. The
construction in this paper is based on the Kolmogorov-Arnold Representation
Theorem and does not require the concept of contextual mapping, hence our proof
is more intuitively clear compared to previous Transformer approximation works.
Additionally, the translation technique proposed in this paper helps to apply
the previous approximation results of feedforward neural networks to
Transformer research.

</details>

### [65] [Bayesian continual learning and forgetting in neural networks](https://arxiv.org/abs/2504.13569)
*Djohan Bonnet,Kellian Cottart,Tifenn Hirtzlin,Tarcisius Januel,Thomas Dalgaty,Elisa Vianello,Damien Querlioz*

Main category: cs.LG

TLDR: MESU是一种基于贝叶斯框架的方法，通过参数不确定性更新网络参数，平衡学习与遗忘，避免灾难性遗忘或记忆问题。


<details>
  <summary>Details</summary>
Motivation: 解决人工神经网络在记忆保留与灵活性之间的极端问题（灾难性遗忘或记忆）。

Method: 引入Metaplasticity from Synaptic Uncertainty (MESU)，一种贝叶斯框架，根据参数不确定性更新网络参数。

Result: 在图像分类基准测试中，MESU有效缓解灾难性遗忘，保持对新任务的可塑性，并在200个连续MNIST任务中表现优于现有持续学习方法。

Conclusion: MESU结合元可塑性、贝叶斯推断和基于Hessian的正则化，为稳健的持续学习提供了生物启发的途径。

Abstract: Biological synapses effortlessly balance memory retention and flexibility,
yet artificial neural networks still struggle with the extremes of catastrophic
forgetting and catastrophic remembering. Here, we introduce Metaplasticity from
Synaptic Uncertainty (MESU), a Bayesian framework that updates network
parameters according their uncertainty. This approach allows a principled
combination of learning and forgetting that ensures that critical knowledge is
preserved while unused or outdated information is gradually released. Unlike
standard Bayesian approaches -- which risk becoming overly constrained, and
popular continual-learning methods that rely on explicit task boundaries, MESU
seamlessly adapts to streaming data. It further provides reliable epistemic
uncertainty estimates, allowing out-of-distribution detection, the only
computational cost being to sample the weights multiple times to provide proper
output statistics. Experiments on image-classification benchmarks demonstrate
that MESU mitigates catastrophic forgetting, while maintaining plasticity for
new tasks. When training 200 sequential permuted MNIST tasks, MESU outperforms
established continual learning techniques in terms of accuracy, capability to
learn additional tasks, and out-of-distribution data detection. Additionally,
due to its non-reliance on task boundaries, MESU outperforms conventional
learning techniques on the incremental training of CIFAR-100 tasks consistently
in a wide range of scenarios. Our results unify ideas from metaplasticity,
Bayesian inference, and Hessian-based regularization, offering a
biologically-inspired pathway to robust, perpetual learning.

</details>

### [66] [MAAM: A Lightweight Multi-Agent Aggregation Module for Efficient Image Classification Based on the MindSpore Framework](https://arxiv.org/abs/2504.13574)
*Zhenkai Qin,Feng Zhu,Huan Zeng,Xunyi Nong*

Main category: cs.LG

TLDR: 提出了一种轻量级注意力架构MAAM，用于资源受限环境下的图像分类任务，显著提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制计算复杂且结构僵化，难以在资源受限环境中应用，需要一种轻量且高效的替代方案。

Method: MAAM通过三个并行代理分支提取异构特征，自适应融合并通过卷积压缩层优化，结合MindSpore框架实现高效计算。

Result: 在CIFAR-10数据集上达到87.0%准确率，显著优于传统CNN和MLP模型，训练效率提升30%。

Conclusion: MAAM在保持高准确性的同时，具备硬件加速能力和低内存占用，适用于资源受限场景。

Abstract: The demand for lightweight models in image classification tasks under
resource-constrained environments necessitates a balance between computational
efficiency and robust feature representation. Traditional attention mechanisms,
despite their strong feature modeling capability, often struggle with high
computational complexity and structural rigidity, limiting their applicability
in scenarios with limited computational resources (e.g., edge devices or
real-time systems). To address this, we propose the Multi-Agent Aggregation
Module (MAAM), a lightweight attention architecture integrated with the
MindSpore framework. MAAM employs three parallel agent branches with
independently parameterized operations to extract heterogeneous features,
adaptively fused via learnable scalar weights, and refined through a
convolutional compression layer. Leveraging MindSpore's dynamic computational
graph and operator fusion, MAAM achieves 87.0% accuracy on the CIFAR-10
dataset, significantly outperforming conventional CNN (58.3%) and MLP (49.6%)
models, while improving training efficiency by 30%. Ablation studies confirm
the critical role of agent attention (accuracy drops to 32.0% if removed) and
compression modules (25.5% if omitted), validating their necessity for
maintaining discriminative feature learning. The framework's hardware
acceleration capabilities and minimal memory footprint further demonstrate its
practicality, offering a deployable solution for image classification in
resource-constrained scenarios without compromising accuracy.

</details>

### [67] [MSTIM: A MindSpore-Based Model for Traffic Flow Prediction](https://arxiv.org/abs/2504.13576)
*Weiqi Qin,Yuxin Liu,Dongze Wu,Zhenkai Qin,Qining Luo*

Main category: cs.LG

TLDR: 提出了一种基于Mindspore框架的多尺度时间序列信息建模模型MSTIM，结合LSTM、CNN和注意力机制，显著提升了交通流量预测的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统交通流量预测模型在处理多尺度时间特征和动态变化模式时存在准确性低、误差波动大的问题。

Method: 提出MSTIM模型，整合LSTM、CNN和注意力机制，利用MITV数据集进行实验，并与典型模型对比分析。

Result: MSTIM模型在MAE、MSE和RMSE指标上表现更优，显著提高了预测准确性和稳定性。

Conclusion: MSTIM模型能有效解决多尺度时间特征和动态变化模式的交通流量预测问题。

Abstract: Aiming at the problems of low accuracy and large error fluctuation of
traditional traffic flow predictionmodels when dealing with multi-scale
temporal features and dynamic change patterns. this paperproposes a multi-scale
time series information modelling model MSTIM based on the Mindspore framework,
which integrates long and short-term memory networks (LSTMs), convolutional
neural networks (CNN), and the attention mechanism to improve the modelling
accuracy and stability. The Metropolitan Interstate Traffic Volume (MITV)
dataset was used for the experiments and compared and analysed with typical
LSTM-attention models, CNN-attention models and LSTM-CNN models. The
experimental results show that the MSTIM model achieves better results in the
metrics of Mean Absolute Error (MAE), Mean Square Error (MSE), and Root Mean
Square Error (RMSE), which significantly improves the accuracy and stability of
the traffic volume prediction.

</details>

### [68] [How to Achieve Higher Accuracy with Less Training Points?](https://arxiv.org/abs/2504.13586)
*Jinghan Yang,Anupam Pani,Yunchao Zhang*

Main category: cs.LG

TLDR: 提出一种基于影响函数的方法，从大规模数据中筛选出信息量高的子集，仅用10%数据即可达到全数据训练效果，60%数据时效果更优。


<details>
  <summary>Details</summary>
Motivation: 大规模模型训练中，数据使用效率低下，需寻找能高效替代全数据训练的子集。

Method: 利用影响函数筛选训练样本，基于逻辑回归模型在二分类任务上验证。

Result: 仅用10%数据即可达到全数据效果，60%数据时准确率更高。

Conclusion: 该方法能显著提升数据效率，适用于大规模模型训练。

Abstract: In the era of large-scale model training, the extensive use of available
datasets has resulted in significant computational inefficiencies. To tackle
this issue, we explore methods for identifying informative subsets of training
data that can achieve comparable or even superior model performance. We propose
a technique based on influence functions to determine which training samples
should be included in the training set. We conducted empirical evaluations of
our method on binary classification tasks utilizing logistic regression models.
Our approach demonstrates performance comparable to that of training on the
entire dataset while using only 10% of the data. Furthermore, we found that our
method achieved even higher accuracy when trained with just 60% of the data.

</details>

### [69] [Fairness and Robustness in Machine Unlearning](https://arxiv.org/abs/2504.13610)
*Khoa Tran,Simon S. Woo*

Main category: cs.LG

TLDR: 该论文探讨了机器遗忘中的公平性和鲁棒性问题，提出了基于方差-偏差权衡特性的公平性猜想，并通过实验验证了公平性与鲁棒性的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有近似遗忘方法在隐私保护中未能实现精确遗忘，且忽视了公平性和鲁棒性。论文首次关注这一问题，并提出相关猜想。

Method: 通过实验在ResNet和ViT模型上验证公平性与鲁棒性的相关性，并评估现有遗忘算法对对抗攻击的脆弱性。

Result: 实验表明，公平性差距越大，模型越敏感脆弱；现有近似遗忘算法在对抗攻击下准确性显著下降。

Conclusion: 提出公平性差距和鲁棒性指标应作为遗忘算法的评估标准，并证明中间层和最后一层的遗忘在时间和内存复杂度上是高效且足够的。

Abstract: Machine unlearning poses the challenge of ``how to eliminate the influence of
specific data from a pretrained model'' in regard to privacy concerns. While
prior research on approximated unlearning has demonstrated accuracy and
efficiency in time complexity, we claim that it falls short of achieving exact
unlearning, and we are the first to focus on fairness and robustness in machine
unlearning algorithms. Our study presents fairness Conjectures for a
well-trained model, based on the variance-bias trade-off characteristic, and
considers their relevance to robustness. Our Conjectures are supported by
experiments conducted on the two most widely used model architectures, ResNet
and ViT, demonstrating the correlation between fairness and robustness:
\textit{the higher fairness-gap is, the more the model is sensitive and
vulnerable}. In addition, our experiments demonstrate the vulnerability of
current state-of-the-art approximated unlearning algorithms to adversarial
attacks, where their unlearned models suffer a significant drop in accuracy
compared to the exact-unlearned models. We claim that our fairness-gap
measurement and robustness metric should be used to evaluate the unlearning
algorithm. Furthermore, we demonstrate that unlearning in the intermediate and
last layers is sufficient and cost-effective for time and memory complexity.

</details>

### [70] [Entropic Time Schedulers for Generative Diffusion Models](https://arxiv.org/abs/2504.13612)
*Dejan Stancevic,Luca Ambrogioni*

Main category: cs.LG

TLDR: 本文提出了一种基于熵的时间调度器，用于生成扩散模型，通过优化采样点选择提升生成性能。


<details>
  <summary>Details</summary>
Motivation: 传统噪声调度函数的选择对生成扩散模型的性能至关重要，但均匀时间间隔可能效率不高。

Method: 提出基于熵的时间重参数化方法，确保每个采样点贡献等量信息，并提供精确计算熵时间的公式。

Result: 实验表明，该方法显著提升了预训练模型的图像质量（如FID和FD-DINO评分），尤其在低NFE情况下。

Conclusion: 基于熵的时间重参数化是一种高效且无需额外计算开销的优化方法。

Abstract: The practical performance of generative diffusion models depends on the
appropriate choice of the noise scheduling function, which can also be
equivalently expressed as a time reparameterization. In this paper, we present
a time scheduler that selects sampling points based on entropy rather than
uniform time spacing, ensuring that each point contributes an equal amount of
information to the final generation. We prove that this time reparameterization
does not depend on the initial choice of time. Furthermore, we provide a
tractable exact formula to estimate this \emph{entropic time} for a trained
model using the training loss without substantial overhead. Alongside the
entropic time, inspired by the optimality results, we introduce a rescaled
entropic time. In our experiments with mixtures of Gaussian distributions and
ImageNet, we show that using the (rescaled) entropic times greatly improves the
inference performance of trained models. In particular, we found that the image
quality in pretrained EDM2 models, as evaluated by FID and FD-DINO scores, can
be substantially increased by the rescaled entropic time reparameterization
without increasing the number of function evaluations, with greater
improvements in the few NFEs regime.

</details>

### [71] [Efficient algorithms for the Hadamard decomposition](https://arxiv.org/abs/2504.13633)
*Samuel Wertz,Arnaud Vandaele,Nicolas Gillis*

Main category: cs.LG

TLDR: 本文提出了一种高效的Hadamard分解算法，通过交替优化和动量加速技术，支持多低秩矩阵分解，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: Hadamard分解是一种强大的数据分析和矩阵压缩技术，但现有方法效率有限，需要更高效的算法。

Method: 采用交替优化将非凸问题分解为凸子问题，结合SVD初始化策略和动量加速技术，并扩展支持多低秩矩阵分解。

Result: 实验表明，该方法在多样数据集上优于现有梯度下降法和传统低秩近似技术。

Conclusion: 所提算法高效且灵活，适用于高有效秩的矩阵近似，具有广泛应用潜力。

Abstract: The Hadamard decomposition is a powerful technique for data analysis and
matrix compression, which decomposes a given matrix into the element-wise
product of two or more low-rank matrices. In this paper, we develop an
efficient algorithm to solve this problem, leveraging an alternating
optimization approach that decomposes the global non-convex problem into a
series of convex sub-problems. To improve performance, we explore advanced
initialization strategies inspired by the singular value decomposition (SVD)
and incorporate acceleration techniques by introducing momentum-based updates.
Beyond optimizing the two-matrix case, we also extend the Hadamard
decomposition framework to support more than two low-rank matrices, enabling
approximations with higher effective ranks while preserving computational
efficiency. Finally, we conduct extensive experiments to compare our method
with the existing gradient descent-based approaches for the Hadamard
decomposition and with traditional low-rank approximation techniques. The
results highlight the effectiveness of our proposed method across diverse
datasets.

</details>

### [72] [MEGA: Second-Order Gradient Alignment for Catastrophic Forgetting Mitigation in GFSCIL](https://arxiv.org/abs/2504.13691)
*Jinhui Pang,Changqing Lin,Hao Lin,Jinglin He,Zhengjun Li,Zhihui Zhang,Xiaoshuai Hao*

Main category: cs.LG

TLDR: 论文提出了一种新的图少样本类增量学习（GFSCIL）方法MEGA，通过元训练阶段的二阶梯度计算，有效缓解灾难性遗忘，并在实验中取得最优效果。


<details>
  <summary>Details</summary>
Motivation: 现有GFSCIL方法基于原型网络（PNs），简化了学习过程且未整合图持续学习（GCL）技术，导致性能受限。

Method: 提出MEGA方法，通过元训练阶段的二阶梯度计算，学习高质量先验，对齐元训练和增量学习阶段的行为。

Result: 在四个主流图数据集上的实验表明，MEGA取得最优效果，并提升了多种GCL方法在GFSCIL中的有效性。

Conclusion: MEGA作为一种模型无关的GFSCIL范式，为未来研究提供了新方向。

Abstract: Graph Few-Shot Class-Incremental Learning (GFSCIL) enables models to
continually learn from limited samples of novel tasks after initial training on
a large base dataset. Existing GFSCIL approaches typically utilize Prototypical
Networks (PNs) for metric-based class representations and fine-tune the model
during the incremental learning stage. However, these PN-based methods
oversimplify learning via novel query set fine-tuning and fail to integrate
Graph Continual Learning (GCL) techniques due to architectural constraints. To
address these challenges, we propose a more rigorous and practical setting for
GFSCIL that excludes query sets during the incremental training phase. Building
on this foundation, we introduce Model-Agnostic Meta Graph Continual Learning
(MEGA), aimed at effectively alleviating catastrophic forgetting for GFSCIL.
Specifically, by calculating the incremental second-order gradient during the
meta-training stage, we endow the model to learn high-quality priors that
enhance incremental learning by aligning its behaviors across both the
meta-training and incremental learning stages. Extensive experiments on four
mainstream graph datasets demonstrate that MEGA achieves state-of-the-art
results and enhances the effectiveness of various GCL methods in GFSCIL. We
believe that our proposed MEGA serves as a model-agnostic GFSCIL paradigm,
paving the way for future research.

</details>

### [73] [Dynamic Regularized CBDT: Variance-Calibrated Causal Boosting for Interpretable Heterogeneous Treatment Effects](https://arxiv.org/abs/2504.13733)
*Yichen Liu*

Main category: cs.LG

TLDR: 提出了一种动态正则化因果提升决策树（CBDT）框架，通过动态调整正则化参数，显著提高了处理效应估计的准确性和模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中，现有树基因果推断方法难以捕捉复杂交互且依赖静态正则化，导致估计误差高。

Method: 将方差正则化和平均处理效应校准集成到梯度提升决策树的损失函数中，动态更新正则化参数。

Result: 在基准数据集和真实临床数据上显著提高了估计准确性，并在ICU患者分诊研究中识别出可操作的临床规则。

Conclusion: 动态正则化能有效缩小误差范围，提升预测性能和模型可解释性。

Abstract: Heterogeneous treatment effect estimation in high-stakes applications demands
models that simultaneously optimize precision, interpretability, and
calibration. Many existing tree-based causal inference techniques, however,
exhibit high estimation errors when applied to observational data because they
struggle to capture complex interactions among factors and rely on static
regularization schemes. In this work, we propose Dynamic Regularized Causal
Boosted Decision Trees (CBDT), a novel framework that integrates variance
regularization and average treatment effect calibration into the loss function
of gradient boosted decision trees. Our approach dynamically updates the
regularization parameters using gradient statistics to better balance the
bias-variance tradeoff. Extensive experiments on standard benchmark datasets
and real-world clinical data demonstrate that the proposed method significantly
improves estimation accuracy while maintaining reliable coverage of true
treatment effects. In an intensive care unit patient triage study, the method
successfully identified clinically actionable rules and achieved high accuracy
in treatment effect estimation. The results validate that dynamic
regularization can effectively tighten error bounds and enhance both predictive
performance and model interpretability.

</details>

### [74] [Learning to Attribute with Attention](https://arxiv.org/abs/2504.13752)
*Benjamin Cohen-Wang,Yung-Sung Chuang,Aleksander Madry*

Main category: cs.LG

TLDR: 论文提出了一种基于注意力权重的令牌归因方法（AT2），通过将注意力权重作为特征学习，显著提高了效率，同时性能与高成本的消融方法相当。


<details>
  <summary>Details</summary>
Motivation: 现有令牌归因方法（如消融）计算成本高，而传统的注意力权重方法不可靠，因此需要一种高效且可靠的归因方法。

Method: 将不同注意力头的注意力权重作为特征，通过学习如何有效利用这些权重进行归因，提出AT2方法。

Result: AT2在性能上与消融方法相当，但效率显著更高，并在问答任务中成功用于上下文修剪。

Conclusion: AT2是一种高效且可靠的令牌归因方法，适用于实际应用，如上下文优化。

Abstract: Given a sequence of tokens generated by a language model, we may want to
identify the preceding tokens that influence the model to generate this
sequence. Performing such token attribution is expensive; a common approach is
to ablate preceding tokens and directly measure their effects. To reduce the
cost of token attribution, we revisit attention weights as a heuristic for how
a language model uses previous tokens. Naive approaches to attribute model
behavior with attention (e.g., averaging attention weights across attention
heads to estimate a token's influence) have been found to be unreliable. To
attain faithful attributions, we propose treating the attention weights of
different attention heads as features. This way, we can learn how to
effectively leverage attention weights for attribution (using signal from
ablations). Our resulting method, Attribution with Attention (AT2), reliably
performs on par with approaches that involve many ablations, while being
significantly more efficient. To showcase the utility of AT2, we use it to
prune less important parts of a provided context in a question answering
setting, improving answer quality. We provide code for AT2 at
https://github.com/MadryLab/AT2 .

</details>

### [75] [Predictors of Childhood Vaccination Uptake in England: An Explainable Machine Learning Analysis of Longitudinal Regional Data (2021-2024)](https://arxiv.org/abs/2504.13755)
*Amin Noroozi,Sidratul Muntaha Esha,Mansoureh Ghari*

Main category: cs.LG

TLDR: 论文通过机器学习分析了英格兰150个地区2021-2024年的儿童疫苗接种覆盖率，发现地理、文化和人口因素是主要预测因素，而社会经济因素影响较小。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决疫苗接种覆盖率差异问题，传统方法难以捕捉多变量动态关系。

Method: 使用分层聚类和CatBoost分类器预测疫苗接种集群，并通过SHAP方法解释变量重要性。

Result: 分类器预测准确率高（92.1%-86.3%），地理、文化和人口因素（如农村化、英语熟练度）影响最大。

Conclusion: 疫苗接种差异主要由非社会经济因素驱动，农村地区接种率更高。

Abstract: Childhood vaccination is a cornerstone of public health, yet disparities in
vaccination coverage persist across England. These disparities are shaped by
complex interactions among various factors, including geographic, demographic,
socioeconomic, and cultural (GDSC) factors. Previous studies mostly rely on
cross-sectional data and traditional statistical approaches that assess
individual or limited sets of variables in isolation. Such methods may fall
short in capturing the dynamic and multivariate nature of vaccine uptake. In
this paper, we conducted a longitudinal machine learning analysis of childhood
vaccination coverage across 150 districts in England from 2021 to 2024. Using
vaccination data from NHS records, we applied hierarchical clustering to group
districts by vaccination coverage into low- and high-coverage clusters. A
CatBoost classifier was then trained to predict districts' vaccination clusters
using their GDSC data. Finally, the SHapley Additive exPlanations (SHAP) method
was used to interpret the predictors' importance. The classifier achieved high
accuracies of 92.1, 90.6, and 86.3 in predicting districts' vaccination
clusters for the years 2021-2022, 2022-2023, and 2023-2024, respectively. SHAP
revealed that geographic, cultural, and demographic variables, particularly
rurality, English language proficiency, the percentage of foreign-born
residents, and ethnic composition, were the most influential predictors of
vaccination coverage, whereas socioeconomic variables, such as deprivation and
employment, consistently showed lower importance, especially in 2023-2024.
Surprisingly, rural districts were significantly more likely to have higher
vaccination rates. Additionally, districts with lower vaccination coverage had
higher populations whose first language was not English, who were born outside
the UK, or who were from ethnic minority groups.

</details>

### [76] [Scaling sparse feature circuit finding for in-context learning](https://arxiv.org/abs/2504.13756)
*Dmitrii Kharlapenko,Stepan Shabalin,Fazl Barez,Arthur Conmy,Neel Nanda*

Main category: cs.LG

TLDR: 稀疏自编码器（SAEs）用于解释大语言模型激活，但其在解决可解释性开放问题中的效用尚不明确。本文通过SAEs加深对上下文学习（ICL）机制的理解，发现抽象SAE特征能编码任务信息并因果诱导任务执行。


<details>
  <summary>Details</summary>
Motivation: 探讨SAEs在解释大语言模型激活中的实际效用，尤其是对ICL机制的理解。

Method: 使用SAEs识别任务执行特征，并通过稀疏特征电路方法分析Gemma-1 2B模型中的ICL机制。

Result: 发现任务检测特征及其因果关联的任务执行特征，验证了任务向量由稀疏SAE潜在特征近似。

Conclusion: SAEs能有效揭示ICL机制，任务向量可通过稀疏潜在特征表示，为模型可解释性提供新视角。

Abstract: Sparse autoencoders (SAEs) are a popular tool for interpreting large language
model activations, but their utility in addressing open questions in
interpretability remains unclear. In this work, we demonstrate their
effectiveness by using SAEs to deepen our understanding of the mechanism behind
in-context learning (ICL). We identify abstract SAE features that (i) encode
the model's knowledge of which task to execute and (ii) whose latent vectors
causally induce the task zero-shot. This aligns with prior work showing that
ICL is mediated by task vectors. We further demonstrate that these task vectors
are well approximated by a sparse sum of SAE latents, including these
task-execution features. To explore the ICL mechanism, we adapt the sparse
feature circuits methodology of Marks et al. (2024) to work for the much larger
Gemma-1 2B model, with 30 times as many parameters, and to the more complex
task of ICL. Through circuit finding, we discover task-detecting features with
corresponding SAE latents that activate earlier in the prompt, that detect when
tasks have been performed. They are causally linked with task-execution
features through the attention and MLP sublayers.

</details>

### [77] [Equi-Euler GraphNet: An Equivariant, Temporal-Dynamics Informed Graph Neural Network for Dual Force and Trajectory Prediction in Multi-Body Systems](https://arxiv.org/abs/2504.13768)
*Vinay Sharma,Rémi Tanguy Oddon,Pietro Tesini,Jens Ravesloot,Cees Taal,Olga Fink*

Main category: cs.LG

TLDR: 提出了一种名为Equi-Euler GraphNet的物理信息图神经网络，用于同时预测多体系统中的内部力和全局轨迹，适用于数字孪生应用。


<details>
  <summary>Details</summary>
Motivation: 多体动力学系统的实时建模对数字孪生应用至关重要，但现有方法难以同时预测内部载荷和系统轨迹，而这对故障检测和预测性维护尤为重要。

Method: 采用物理信息图神经网络，通过节点表示系统组件，边编码相互作用，引入等变消息传递和时间感知迭代节点更新机制。

Result: 在圆柱滚子轴承上验证，模型能泛化到未见过的速度、载荷和配置，预测精度高且误差累积小，速度比传统求解器快200倍。

Conclusion: Equi-Euler GraphNet是一种高效的数字孪生降阶模型，适用于设计、维护和实时应用。

Abstract: Accurate real-time modeling of multi-body dynamical systems is essential for
enabling digital twin applications across industries. While many data-driven
approaches aim to learn system dynamics, jointly predicting internal loads and
system trajectories remains a key challenge. This dual prediction is especially
important for fault detection and predictive maintenance, where internal
loads-such as contact forces-act as early indicators of faults, reflecting wear
or misalignment before affecting motion. These forces also serve as inputs to
degradation models (e.g., crack growth), enabling damage prediction and
remaining useful life estimation. We propose Equi-Euler GraphNet, a
physics-informed graph neural network (GNN) that simultaneously predicts
internal forces and global trajectories in multi-body systems. In this
mesh-free framework, nodes represent system components and edges encode
interactions. Equi-Euler GraphNet introduces two inductive biases: (1) an
equivariant message-passing scheme, interpreting edge messages as interaction
forces consistent under Euclidean transformations; and (2) a temporal-aware
iterative node update mechanism, based on Euler integration, to capture
influence of distant interactions over time. Tailored for cylindrical roller
bearings, it decouples ring dynamics from constrained motion of rolling
elements. Trained on high-fidelity multiphysics simulations, Equi-Euler
GraphNet generalizes beyond the training distribution, accurately predicting
loads and trajectories under unseen speeds, loads, and configurations. It
outperforms state-of-the-art GNNs focused on trajectory prediction, delivering
stable rollouts over thousands of time steps with minimal error accumulation.
Achieving up to a 200x speedup over conventional solvers while maintaining
comparable accuracy, it serves as an efficient reduced-order model for digital
twins, design, and maintenance.

</details>

### [78] [DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs](https://arxiv.org/abs/2504.13774)
*Tamim Al Mahmud,Najeeb Jebreel,Josep Domingo-Ferrer,David Sanchez*

Main category: cs.LG

TLDR: 本文提出了一种名为DP2Unlearning的新框架，用于高效实现大语言模型（LLM）的遗忘，同时提供正式的遗忘保证，成本显著低于从头训练。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在处理语言任务时可能记忆并泄露训练数据中的隐私或受版权保护信息，现有解决方案（如从头训练或近似遗忘）成本高或缺乏保证。

Method: DP2Unlearning框架通过在训练时使用ε-差分隐私（DP）保护数据，后续实现高效遗忘，并提供与所选ε相关的遗忘保证。

Result: 实验表明，DP2Unlearning在遗忘后模型性能接近从头训练的黄金标准，成本仅为后者的一半，且在保留模型效用和有效遗忘目标信息方面优于近似遗忘方法。

Conclusion: DP2Unlearning是一种高效且具有正式遗忘保证的LLM遗忘方法，优于现有解决方案。

Abstract: Large language models (LLMs) have recently revolutionized language processing
tasks but have also brought ethical and legal issues. LLMs have a tendency to
memorize potentially private or copyrighted information present in the training
data, which might then be delivered to end users at inference time. When this
happens, a naive solution is to retrain the model from scratch after excluding
the undesired data. Although this guarantees that the target data have been
forgotten, it is also prohibitively expensive for LLMs. Approximate unlearning
offers a more efficient alternative, as it consists of ex post modifications of
the trained model itself to prevent undesirable results, but it lacks
forgetting guarantees because it relies solely on empirical evidence. In this
work, we present DP2Unlearning, a novel LLM unlearning framework that offers
formal forgetting guarantees at a significantly lower cost than retraining from
scratch on the data to be retained. DP2Unlearning involves training LLMs on
textual data protected using {\epsilon}-differential privacy (DP), which later
enables efficient unlearning with the guarantees against disclosure associated
with the chosen {\epsilon}. Our experiments demonstrate that DP2Unlearning
achieves similar model performance post-unlearning, compared to an LLM
retraining from scratch on retained data -- the gold standard exact unlearning
-- but at approximately half the unlearning cost. In addition, with a
reasonable computational cost, it outperforms approximate unlearning methods at
both preserving the utility of the model post-unlearning and effectively
forgetting the targeted information.

</details>

### [79] [On the Relationship Between Robustness and Expressivity of Graph Neural Networks](https://arxiv.org/abs/2504.13786)
*Lorenz Kummer,Wilfried N. Gansterer,Nils M. Kriege*

Main category: cs.LG

TLDR: 该论文研究了图神经网络（GNNs）对位翻转攻击（BFAs）的脆弱性，分析了架构特征、图属性及其交互对GNN表达能力的影响。


<details>
  <summary>Details</summary>
Motivation: GNNs的表达能力依赖于节点邻域的编码，但位翻转攻击可能导致其失去表达能力。研究旨在量化这种脆弱性，并分析影响因素。

Method: 提出了一个分析框架，研究GNNs对BFAs的脆弱性，包括理论界限推导和实证验证。

Result: 理论分析表明，ReLU激活的GNNs在高度同质的图上使用低维或独热编码特征时更易受攻击。实证结果验证了理论发现。

Conclusion: 研究为减少GNNs在关键应用中的BFA风险提供了理论依据和实用建议。

Abstract: We investigate the vulnerability of Graph Neural Networks (GNNs) to bit-flip
attacks (BFAs) by introducing an analytical framework to study the influence of
architectural features, graph properties, and their interaction.
  The expressivity of GNNs refers to their ability to distinguish
non-isomorphic graphs and depends on the encoding of node neighborhoods. We
examine the vulnerability of neural multiset functions commonly used for this
purpose and establish formal criteria to characterize a GNN's susceptibility to
losing expressivity due to BFAs. This enables an analysis of the impact of
homophily, graph structural variety, feature encoding, and activation functions
on GNN robustness. We derive theoretical bounds for the number of bit flips
required to degrade GNN expressivity on a dataset, identifying ReLU-activated
GNNs operating on highly homophilous graphs with low-dimensional or one-hot
encoded features as particularly susceptible. Empirical results using ten
real-world datasets confirm the statistical significance of our key theoretical
insights and offer actionable results to mitigate BFA risks in
expressivity-critical applications.

</details>

### [80] [Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning](https://arxiv.org/abs/2504.13818)
*Yixuan Even Xu,Yash Savani,Fei Fang,Zico Kolter*

Main category: cs.LG

TLDR: PODS框架通过并行生成大量rollout但仅更新信息丰富的子集，解决了RL中计算与内存需求的不对称问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习在语言模型中面临计算与内存需求不对称的问题，推理并行且内存占用低，而策略更新需同步且内存密集。

Method: 提出PODS框架，采用max-variance down-sampling方法选择奖励信号多样化的rollout子集进行更新。

Result: PODS框架在GSM8K基准测试中优于标准GRPO。

Conclusion: PODS通过解耦并行生成与策略更新，有效提升了RL性能。

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing
reasoning capabilities in large language models, but faces a fundamental
asymmetry in computation and memory requirements: inference is embarrassingly
parallel with a minimal memory footprint, while policy updates require
extensive synchronization and are memory-intensive. To address this asymmetry,
we introduce PODS (Policy Optimization with Down-Sampling), a framework that
strategically decouples these phases by generating numerous rollouts in
parallel but updating only on an informative subset. Within this framework, we
develop max-variance down-sampling, a theoretically motivated method that
selects rollouts with maximally diverse reward signals. We prove that this
approach has an efficient algorithmic solution, and empirically demonstrate
that GRPO with PODS using max-variance down-sampling achieves superior
performance over standard GRPO on the GSM8K benchmark.

</details>

### [81] [Probabilistic Stability Guarantees for Feature Attributions](https://arxiv.org/abs/2504.13787)
*Helen Jin,Anton Xue,Weiqiu You,Surbhi Goel,Eric Wong*

Main category: cs.LG

TLDR: 论文提出了一种名为SCA的模型无关、样本高效的稳定性认证算法，通过软稳定性概念解决了现有方法的保守性问题，并在视觉和语言任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有特征归因稳定性认证方法依赖平滑分类器且结果保守，限制了其实际应用。

Method: 引入软稳定性概念，提出SCA算法，结合布尔函数分析对平滑下的稳定性进行新表征。

Result: SCA提供了非平凡且可解释的稳定性保证，并在实验中验证了其有效性。

Conclusion: 软稳定性在平衡准确性和稳定性方面优于现有方法，为解释方法的鲁棒性评估提供了新工具。

Abstract: Stability guarantees are an emerging tool for evaluating feature
attributions, but existing certification methods rely on smoothed classifiers
and often yield conservative guarantees. To address these limitations, we
introduce soft stability and propose a simple, model-agnostic, and
sample-efficient stability certification algorithm (SCA) that provides
non-trivial and interpretable guarantees for any attribution. Moreover, we show
that mild smoothing enables a graceful tradeoff between accuracy and stability,
in contrast to prior certification methods that require a more aggressive
compromise. Using Boolean function analysis, we give a novel characterization
of stability under smoothing. We evaluate SCA on vision and language tasks, and
demonstrate the effectiveness of soft stability in measuring the robustness of
explanation methods.

</details>

### [82] [The Binary and Ternary Quantization Can Improve Feature Discrimination](https://arxiv.org/abs/2504.13792)
*Weizhi Lu,Mingrui Chen,Weiyu Li*

Main category: cs.LG

TLDR: 论文探讨了量化对分类性能的影响，提出直接研究量化数据的特征判别性而非量化误差，发现某些低比特量化方法能提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 量化在机器学习中广泛应用，但当前研究主要关注量化误差，缺乏理论支持且与实证结果矛盾。

Method: 提出直接分析量化数据的特征判别性，而非量化误差，验证了二进制和三进制量化方法的有效性。

Result: 实验表明，某些低比特量化方法（如二进制和三进制）能提升特征判别性，分类性能优于或接近原始数据。

Conclusion: 量化数据的特征判别性是评估分类性能的更准确指标，某些低比特量化方法具有实际优势。

Abstract: In machine learning, quantization is widely used to simplify data
representation and facilitate algorithm deployment on hardware. Given the
fundamental role of classification in machine learning, it is crucial to
investigate the impact of quantization on classification. Current research
primarily focuses on quantization errors, operating under the premise that
higher quantization errors generally result in lower classification
performance. However, this premise lacks a solid theoretical foundation and
often contradicts empirical findings. For instance, certain extremely low
bit-width quantization methods, such as $\{0,1\}$-binary quantization and $\{0,
\pm1\}$-ternary quantization, can achieve comparable or even superior
classification accuracy compared to the original non-quantized data, despite
exhibiting high quantization errors. To more accurately evaluate classification
performance, we propose to directly investigate the feature discrimination of
quantized data, instead of analyzing its quantization error. Interestingly, it
is found that both binary and ternary quantization methods can improve, rather
than degrade, the feature discrimination of the original data. This remarkable
performance is validated through classification experiments across various data
types, including images, speech, and texts.

</details>

### [83] [Meta-Learning and Knowledge Discovery based Physics-Informed Neural Network for Remaining Useful Life Prediction](https://arxiv.org/abs/2504.13797)
*Yu Wang,Shujie Liu,Shuai Lv,Gengshuo Liu*

Main category: cs.LG

TLDR: 提出了一种基于元学习和知识发现的物理信息神经网络（MKDPINN），用于解决旋转机械剩余使用寿命（RUL）预测中的数据稀缺和退化动态不明确问题。


<details>
  <summary>Details</summary>
Motivation: 工业安全和维护需要准确的RUL预测，但现有方法在目标域数据稀缺和退化动态不明确的情况下表现不佳。

Method: MKDPINN通过隐藏状态映射器（HSM）将噪声传感器数据映射到低维隐藏状态空间，再通过物理引导调节器（PGR）学习未知的非线性偏微分方程，并将其嵌入PINN框架中，结合数据驱动和物理方法。利用元学习优化源域元任务，实现对新目标任务的少样本适应。

Result: 在工业数据和C-MAPSS基准测试中，MKDPINN在泛化性和准确性上优于基线方法。

Conclusion: MKDPINN在数据稀缺情况下有效提升了RUL预测的性能。

Abstract: Predicting the remaining useful life (RUL) of rotating machinery is critical
for industrial safety and maintenance, but existing methods struggle with
scarce target-domain data and unclear degradation dynamics. We propose a
Meta-Learning and Knowledge Discovery-based Physics-Informed Neural Network
(MKDPINN) to address these challenges. The method first maps noisy sensor data
to a low-dimensional hidden state space via a Hidden State Mapper (HSM). A
Physics-Guided Regulator (PGR) then learns unknown nonlinear PDEs governing
degradation evolution, embedding these physical constraints into the PINN
framework. This integrates data-driven and physics-based approaches. The
framework uses meta-learning, optimizing across source-domain meta-tasks to
enable few-shot adaptation to new target tasks. Experiments on industrial data
and the C-MAPSS benchmark show MKDPINN outperforms baselines in generalization
and accuracy, proving its effectiveness for RUL prediction under data scarcity

</details>

### [84] [Transformer Encoder and Multi-features Time2Vec for Financial Prediction](https://arxiv.org/abs/2504.13801)
*Nguyen Kim Hai Bui,Nguyen Duy Chien,Péter Kovács,Gergő Bognár*

Main category: cs.LG

TLDR: 本文提出了一种结合Time2Vec和Transformer编码器的新型神经网络架构，用于金融预测，通过相关性特征选择和多股票价格预测提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 金融预测需同时捕捉短期波动和长期依赖关系，而现有研究多关注单一特征和预测，限制了模型对市场趋势的理解。

Method: 集成Time2Vec与Transformer编码器，提出相关性特征选择方法，并通过超参数调优进行对比分析。

Result: 该方法优于其他编码方法（如位置编码），且相关性特征选择显著提高了多股票价格预测的准确性。

Conclusion: 结合Time2Vec和Transformer编码器的方法在金融预测中表现优异，相关性特征选择对多股票预测有重要贡献。

Abstract: Financial prediction is a complex and challenging task of time series
analysis and signal processing, expected to model both short-term fluctuations
and long-term temporal dependencies. Transformers have remarkable success
mostly in natural language processing using attention mechanism, which also
influenced the time series community. The ability to capture both short and
long-range dependencies helps to understand the financial market and to
recognize price patterns, leading to successful applications of Transformers in
stock prediction. Although, the previous research predominantly focuses on
individual features and singular predictions, that limits the model's ability
to understand broader market trends. In reality, within sectors such as finance
and technology, companies belonging to the same industry often exhibit
correlated stock price movements.
  In this paper, we develop a novel neural network architecture by integrating
Time2Vec with the Encoder of the Transformer model. Based on the study of
different markets, we propose a novel correlation feature selection method.
Through a comprehensive fine-tuning of multiple hyperparameters, we conduct a
comparative analysis of our results against benchmark models. We conclude that
our method outperforms other state-of-the-art encoding methods such as
positional encoding, and we also conclude that selecting correlation features
enhance the accuracy of predicting multiple stock prices.

</details>

### [85] [Parameter-Efficient Continual Fine-Tuning: A Survey](https://arxiv.org/abs/2504.13822)
*Eric Nuertey Coleman,Luigi Quarantiello,Ziyue Liu,Qinwen Yang,Samrat Mukherjee,Julio Hurtado,Vincenzo Lomonaco*

Main category: cs.LG

TLDR: 该论文探讨了大规模预训练网络在动态学习场景中的局限性，提出了参数高效微调（PEFT）作为解决方案，并综述了参数高效持续微调（PECFT）的最新进展。


<details>
  <summary>Details</summary>
Motivation: 解决大规模预训练网络对i.i.d.假设的依赖问题，推动AI在动态环境中的持续学习能力。

Method: 综述了持续学习（CL）算法和PEFT方法，并分析了PECFT的最新研究。

Result: 总结了现有方法的优缺点，提出了未来研究方向。

Conclusion: 强调了CL与PEFT的协同作用，为未来研究提供了指导。

Abstract: The emergence of large pre-trained networks has revolutionized the AI field,
unlocking new possibilities and achieving unprecedented performance. However,
these models inherit a fundamental limitation from traditional Machine Learning
approaches: their strong dependence on the \textit{i.i.d.} assumption hinders
their adaptability to dynamic learning scenarios. We believe the next
breakthrough in AI lies in enabling efficient adaptation to evolving
environments -- such as the real world -- where new data and tasks arrive
sequentially. This challenge defines the field of Continual Learning (CL), a
Machine Learning paradigm focused on developing lifelong learning neural
models. One alternative to efficiently adapt these large-scale models is known
Parameter-Efficient Fine-Tuning (PEFT). These methods tackle the issue of
adapting the model to a particular data or scenario by performing small and
efficient modifications, achieving similar performance to full fine-tuning.
However, these techniques still lack the ability to adjust the model to
multiple tasks continually, as they suffer from the issue of Catastrophic
Forgetting. In this survey, we first provide an overview of CL algorithms and
PEFT methods before reviewing the state-of-the-art on Parameter-Efficient
Continual Fine-Tuning (PECFT). We examine various approaches, discuss
evaluation metrics, and explore potential future research directions. Our goal
is to highlight the synergy between CL and Parameter-Efficient Fine-Tuning,
guide researchers in this field, and pave the way for novel future research
directions.

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [86] [Benchmarking Large Language Models for Calculus Problem-Solving: A Comparative Analysis](https://arxiv.org/abs/2504.13187)
*In Hak Moon*

Main category: cs.CL

TLDR: 本研究评估了五种大型语言模型（LLMs）在解决微积分微分问题上的表现，发现Chat GPT 4o表现最佳，但所有模型在概念理解和代数操作上存在局限性。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在微积分学习中的潜力与限制，为教育应用提供参考。

Method: 采用系统性交叉评估框架，测试五种模型在13类基础问题上的表现。

Result: Chat GPT 4o成功率最高（94.71%），但所有模型在概念性问题上表现不佳。

Conclusion: LLMs在程序性任务上表现优异，但概念理解仍需人类教学辅助。

Abstract: This study presents a comprehensive evaluation of five leading large language
models (LLMs) - Chat GPT 4o, Copilot Pro, Gemini Advanced, Claude Pro, and Meta
AI - on their performance in solving calculus differentiation problems. The
investigation assessed these models across 13 fundamental problem types,
employing a systematic cross-evaluation framework where each model solved
problems generated by all models. Results revealed significant performance
disparities, with Chat GPT 4o achieving the highest success rate (94.71%),
followed by Claude Pro (85.74%), Gemini Advanced (84.42%), Copilot Pro
(76.30%), and Meta AI (56.75%). All models excelled at procedural
differentiation tasks but showed varying limitations with conceptual
understanding and algebraic manipulation. Notably, problems involving
increasing/decreasing intervals and optimization word problems proved most
challenging across all models. The cross-evaluation matrix revealed that Claude
Pro generated the most difficult problems, suggesting distinct capabilities
between problem generation and problem-solving. These findings have significant
implications for educational applications, highlighting both the potential and
limitations of LLMs as calculus learning tools. While they demonstrate
impressive procedural capabilities, their conceptual understanding remains
limited compared to human mathematical reasoning, emphasizing the continued
importance of human instruction for developing deeper mathematical
comprehension.

</details>

### [87] [BASIR: Budget-Assisted Sectoral Impact Ranking -- A Dataset for Sector Identification and Performance Prediction Using Language Models](https://arxiv.org/abs/2504.13189)
*Sohom Ghosh,Sudip Kumar Naskar*

Main category: cs.CL

TLDR: 提出了一种框架BASIR，用于分析印度联邦预算对特定行业股票表现的影响，包括行业分类和绩效排名。


<details>
  <summary>Details</summary>
Motivation: 实时分析预算对行业股票表现的影响方法上具有挑战性且研究不足。

Method: 使用多标签分类和语言模型对预算摘录进行分类和排名，基于1947-2025年的预算数据。

Result: 行业分类F1得分为0.605，绩效排名NDCG得分为0.997。

Conclusion: 该方法为投资者和政策制定者提供了数据驱动的见解，填补了手动分析的空白。

Abstract: Government fiscal policies, particularly annual union budgets, exert
significant influence on financial markets. However, real-time analysis of
budgetary impacts on sector-specific equity performance remains
methodologically challenging and largely unexplored. This study proposes a
framework to systematically identify and rank sectors poised to benefit from
India's Union Budget announcements. The framework addresses two core tasks: (1)
multi-label classification of excerpts from budget transcripts into 81
predefined economic sectors, and (2) performance ranking of these sectors.
Leveraging a comprehensive corpus of Indian Union Budget transcripts from 1947
to 2025, we introduce BASIR (Budget-Assisted Sectoral Impact Ranking), an
annotated dataset mapping excerpts from budgetary transcripts to sectoral
impacts. Our architecture incorporates fine-tuned embeddings for sector
identification, coupled with language models that rank sectors based on their
predicted performances. Our results demonstrate 0.605 F1-score in sector
classification, and 0.997 NDCG score in predicting ranks of sectors based on
post-budget performances. The methodology enables investors and policymakers to
quantify fiscal policy impacts through structured, data-driven insights,
addressing critical gaps in manual analysis. The annotated dataset has been
released under CC-BY-NC-SA-4.0 license to advance computational economics
research.

</details>

### [88] [KFinEval-Pilot: A Comprehensive Benchmark Suite for Korean Financial Language Understanding](https://arxiv.org/abs/2504.13216)
*Bokwang Hwang,Seonkyu Lim,Taewoong Kim,Yongjae Geun,Sunghyun Bang,Sohyun Park,Jihyun Park,Myeonggyu Lee,Jinwoo Lee,Yerin Kim,Jinsun Yoo,Jingyeong Hong,Jina Park,Yongchan Kim,Suhyun Kim,Younggyun Hahm,Yiseul Lee,Yejee Kang,Chanhyuk Yoon,Chansu Lee,Heeyewon Jeong,Jiyeon Lee,Seonhye Gu,Hyebin Kang,Yousang Cho,Hangyeol Yoo,KyungTae Lim*

Main category: cs.CL

TLDR: KFinEval-Pilot是一个专为评估韩语金融领域大型语言模型（LLMs）设计的基准套件，包含1000多个问题，覆盖金融知识、法律推理和金融毒性三个领域。


<details>
  <summary>Details</summary>
Motivation: 解决现有以英语为中心的基准在韩语金融领域的局限性，提供更贴合韩国监管和语言环境的评估工具。

Method: 采用半自动化流程，结合GPT-4生成的提示和专家验证，确保问题的领域相关性和事实准确性。

Result: 评估了多种LLMs，发现模型间性能差异显著，任务准确性与输出安全性之间存在权衡。

Conclusion: KFinEval-Pilot为开发更安全可靠的金融AI系统提供了早期诊断工具，突显了LLMs在高风险金融应用中推理和安全性的挑战。

Abstract: We introduce KFinEval-Pilot, a benchmark suite specifically designed to
evaluate large language models (LLMs) in the Korean financial domain.
Addressing the limitations of existing English-centric benchmarks,
KFinEval-Pilot comprises over 1,000 curated questions across three critical
areas: financial knowledge, legal reasoning, and financial toxicity. The
benchmark is constructed through a semi-automated pipeline that combines
GPT-4-generated prompts with expert validation to ensure domain relevance and
factual accuracy. We evaluate a range of representative LLMs and observe
notable performance differences across models, with trade-offs between task
accuracy and output safety across different model families. These results
highlight persistent challenges in applying LLMs to high-stakes financial
applications, particularly in reasoning and safety. Grounded in real-world
financial use cases and aligned with the Korean regulatory and linguistic
context, KFinEval-Pilot serves as an early diagnostic tool for developing safer
and more reliable financial AI systems.

</details>

### [89] [Sustainability via LLM Right-sizing](https://arxiv.org/abs/2504.13217)
*Jennifer Haase,Finn Klessascheck,Jan Mendling,Sebastian Pokutta*

Main category: cs.CL

TLDR: 研究评估了11种LLM在10种日常任务中的表现，发现GPT-4o性能最优但成本高，而小型模型如Gemma-3和Phi-4在多数任务中表现可靠，适合成本敏感或隐私需求场景。


<details>
  <summary>Details</summary>
Motivation: 探讨在实际应用中，何时小型本地部署模型足够好，以解决LLM的能耗、成本和数据主权问题。

Method: 使用双LLM评估框架，自动化任务执行并标准化评估10项标准，涵盖输出质量、事实准确性和伦理责任。

Result: GPT-4o表现最优但成本高，小型模型在多数任务中表现可靠。任务类型影响模型效果，概念性任务更具挑战性。

Conclusion: 建议从性能最大化转向任务和情境感知的评估，以更好地反映组织需求，为可持续和负责任LLM部署提供指导。

Abstract: Large language models (LLMs) have become increasingly embedded in
organizational workflows. This has raised concerns over their energy
consumption, financial costs, and data sovereignty. While performance
benchmarks often celebrate cutting-edge models, real-world deployment decisions
require a broader perspective: when is a smaller, locally deployable model
"good enough"? This study offers an empirical answer by evaluating eleven
proprietary and open-weight LLMs across ten everyday occupational tasks,
including summarizing texts, generating schedules, and drafting emails and
proposals. Using a dual-LLM-based evaluation framework, we automated task
execution and standardized evaluation across ten criteria related to output
quality, factual accuracy, and ethical responsibility. Results show that GPT-4o
delivers consistently superior performance but at a significantly higher cost
and environmental footprint. Notably, smaller models like Gemma-3 and Phi-4
achieved strong and reliable results on most tasks, suggesting their viability
in contexts requiring cost-efficiency, local deployment, or privacy. A cluster
analysis revealed three model groups -- premium all-rounders, competent
generalists, and limited but safe performers -- highlighting trade-offs between
quality, control, and sustainability. Significantly, task type influenced model
effectiveness: conceptual tasks challenged most models, while aggregation and
transformation tasks yielded better performances. We argue for a shift from
performance-maximizing benchmarks to task- and context-aware sufficiency
assessments that better reflect organizational priorities. Our approach
contributes a scalable method to evaluate AI models through a sustainability
lens and offers actionable guidance for responsible LLM deployment in practice.

</details>

### [90] [DIDS: Domain Impact-aware Data Sampling for Large Language Model Training](https://arxiv.org/abs/2504.13227)
*Weijie Shi,Jipeng Zhang,Yaguang Wu,Jingzhi Fang,Ruiyuan Zhang,Jiajie Xu,Jia Zhu,Hao Chen,Yao Zhao,Sirui Han,Xiaofang Zhou*

Main category: cs.CL

TLDR: 本文提出了一种名为DIDS的领域感知数据采样方法，通过梯度聚类和FIM度量优化领域采样策略，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 多领域数据集中领域采样策略对模型性能影响显著，现有方法难以保持领域内一致性和准确衡量领域影响。

Method: 提出梯度聚类算法确保领域内一致性，使用FIM度量量化领域影响，并结合损失学习轨迹确定最优采样比例。

Result: 实验表明DIDS平均性能提升3.4%，同时保持训练效率。

Conclusion: DIDS通过优化领域采样策略，显著提升模型性能，具有理论和实践价值。

Abstract: Large language models (LLMs) are commonly trained on multi-domain datasets,
where domain sampling strategies significantly impact model performance due to
varying domain importance across downstream tasks. Existing approaches for
optimizing domain-level sampling strategies struggle with maintaining
intra-domain consistency and accurately measuring domain impact. In this paper,
we present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain
consistency, a gradient clustering algorithm is proposed to group training data
based on their learning effects, where a proxy language model and
dimensionality reduction are employed to reduce computational overhead. To
accurately measure domain impact, we develop a Fisher Information Matrix (FIM)
guided metric that quantifies how domain-specific parameter updates affect the
model's output distributions on downstream tasks, with theoretical guarantees.
Furthermore, to determine optimal sampling ratios, DIDS combines both the
FIM-guided domain impact assessment and loss learning trajectories that
indicate domain-specific potential, while accounting for diminishing marginal
returns. Extensive experiments demonstrate that DIDS achieves 3.4% higher
average performance while maintaining comparable training efficiency.

</details>

### [91] [ImPart: Importance-Aware Delta-Sparsification for Improved Model Compression and Merging in LLMs](https://arxiv.org/abs/2504.13237)
*Yan Yang,Yixia Li,Hongru Wang,Xuetao Wei,Jianqiao Yu,Yun Chen,Guanhua Chen*

Main category: cs.CL

TLDR: ImPart是一种新型重要性感知的delta稀疏化方法，通过动态调整奇异向量的稀疏比，在高压缩比下保留关键任务知识，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有delta稀疏化方法忽视参数重要性或评估粒度粗糙的问题。

Method: 利用SVD动态调整不同奇异向量的稀疏比，基于其重要性。

Result: 实验显示ImPart在相同性能水平下压缩比提高2倍，并在delta量化和模型合并中达到新SOTA。

Conclusion: ImPart通过重要性感知的稀疏化方法，显著提升了delta压缩的性能和应用效果。

Abstract: With the proliferation of task-specific large language models, delta
compression has emerged as a method to mitigate the resource challenges of
deploying numerous such models by effectively compressing the delta model
parameters. Previous delta-sparsification methods either remove parameters
randomly or truncate singular vectors directly after singular value
decomposition (SVD). However, these methods either disregard parameter
importance entirely or evaluate it with too coarse a granularity. In this work,
we introduce ImPart, a novel importance-aware delta sparsification approach.
Leveraging SVD, it dynamically adjusts sparsity ratios of different singular
vectors based on their importance, effectively retaining crucial task-specific
knowledge even at high sparsity ratios. Experiments show that ImPart achieves
state-of-the-art delta sparsification performance, demonstrating $2\times$
higher compression ratio than baselines at the same performance level. When
integrated with existing methods, ImPart sets a new state-of-the-art on delta
quantization and model merging.

</details>

### [92] [CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models](https://arxiv.org/abs/2504.13261)
*Dong Wang*

Main category: cs.CL

TLDR: CPG-EVAL是首个专门评估大语言模型在对外汉语教学中语法能力的基准测试，包含五项任务，发现小模型在单任务表现尚可，但多任务和干扰下表现不佳，大模型抗干扰能力较强但仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在对外汉语教学中的语法能力，填补现有研究的空白。

Method: 设计五项任务评估语法识别、细粒度区分、分类辨别和抗干扰能力。

Result: 小模型在单任务中表现较好，但在多任务和干扰下表现不佳；大模型抗干扰能力较强，但准确性仍需提升。

Conclusion: 研究为教育者、政策制定者和开发者提供了评估AI能力的工具，并为未来改进模型与教育需求的匹配奠定了基础。

Abstract: Purpose: The rapid emergence of large language models (LLMs) such as ChatGPT
has significantly impacted foreign language education, yet their pedagogical
grammar competence remains under-assessed. This paper introduces CPG-EVAL, the
first dedicated benchmark specifically designed to evaluate LLMs' knowledge of
pedagogical grammar within the context of foreign language instruction.
Methodology: The benchmark comprises five tasks designed to assess grammar
recognition, fine-grained grammatical distinction, categorical discrimination,
and resistance to linguistic interference. Findings: Smaller-scale models can
succeed in single language instance tasks, but struggle with multiple instance
tasks and interference from confusing instances. Larger-scale models show
better resistance to interference but still have significant room for accuracy
improvement. The evaluation indicates the need for better instructional
alignment and more rigorous benchmarks, to effectively guide the deployment of
LLMs in educational contexts. Value: This study offers the first specialized,
theory-driven, multi-tiered benchmark framework for systematically evaluating
LLMs' pedagogical grammar competence in Chinese language teaching contexts.
CPG-EVAL not only provides empirical insights for educators, policymakers, and
model developers to better gauge AI's current abilities in educational
settings, but also lays the groundwork for future research on improving model
alignment, enhancing educational suitability, and ensuring informed
decision-making concerning LLM integration in foreign language instruction.

</details>

### [93] [Sentiment Analysis on the young people's perception about the mobile Internet costs in Senegal](https://arxiv.org/abs/2504.13284)
*Derguene Mbaye,Madoune Robert Seye,Moussa Diallo,Mamadou Lamine Ndiaye,Djiby Sow,Dimitri Samuel Adjanohoun,Tatiana Mbengue,Cheikh Samba Wade,De Roulet Pablo,Jean-Claude Baraka Munyaka,Jerome Chenal*

Main category: cs.CL

TLDR: 研究探讨了塞内加尔年轻人对移动互联网价格与服务质量感知的态度，通过社交媒体评论的情感分析得出结论。


<details>
  <summary>Details</summary>
Motivation: 随着非洲互联网普及率上升，年轻人对移动互联网价格与服务质量的关系关注增加，但市场运营商有限，缺乏性价比选择。

Method: 通过扫描Twitter和Facebook上与主题相关的评论，并应用情感分析模型收集用户的普遍感受。

Result: 情感分析揭示了年轻人对移动互联网价格与服务质量的普遍态度。

Conclusion: 研究强调了市场运营商需关注价格与服务质量的平衡，以满足年轻人的需求。

Abstract: Internet penetration rates in Africa are rising steadily, and mobile Internet
is getting an even bigger boost with the availability of smartphones. Young
people are increasingly using the Internet, especially social networks, and
Senegal is no exception to this revolution. Social networks have become the
main means of expression for young people. Despite this evolution in Internet
access, there are few operators on the market, which limits the alternatives
available in terms of value for money. In this paper, we will look at how young
people feel about the price of mobile Internet in Senegal, in relation to the
perceived quality of the service, through their comments on social networks. We
scanned a set of Twitter and Facebook comments related to the subject and
applied a sentiment analysis model to gather their general feelings.

</details>

### [94] [THOUGHTTERMINATOR: Benchmarking, Calibrating, and Mitigating Overthinking in Reasoning Models](https://arxiv.org/abs/2504.13367)
*Xiao Pu,Michael Saxon,Wenyue Hua,William Yang Wang*

Main category: cs.CL

TLDR: 论文研究了推理模型在任务中生成过多不必要标记的问题，提出了问题难度与最优标记使用量的关系，并评估了模型的校准能力。通过DUMB500数据集和THOUGHTTERMINATOR技术，改进了模型校准。


<details>
  <summary>Details</summary>
Motivation: 推理模型在任务中表现出色，但存在过度生成不必要标记的问题，影响了效率。

Method: 提出问题难度与最优标记使用量的关系，评估模型校准能力，并引入DUMB500数据集和THOUGHTTERMINATOR技术。

Result: 发现推理模型校准较差，特别是在简单问题上；THOUGHTTERMINATOR显著改善了校准。

Conclusion: 通过问题难度与标记使用量的关系及新技术，可以有效提升推理模型的校准效率。

Abstract: Reasoning models have demonstrated impressive performance on difficult tasks
that traditional language models struggle at. However, many are plagued with
the problem of overthinking--generating large amounts of unnecessary tokens
which don't improve accuracy on a question. We introduce approximate measures
of problem-level difficulty and demonstrate that a clear relationship between
problem difficulty and optimal token spend exists, and evaluate how well
calibrated a variety of reasoning models are in terms of efficiently allocating
the optimal token count. We find that in general, reasoning models are poorly
calibrated, particularly on easy problems. To evaluate calibration on easy
questions we introduce DUMB500, a dataset of extremely easy math, reasoning,
code, and task problems, and jointly evaluate reasoning model on these simple
examples and extremely difficult examples from existing frontier benchmarks on
the same task domain. Finally, we introduce THOUGHTTERMINATOR, a training-free
black box decoding technique that significantly improves reasoning model
calibration.

</details>

### [95] [Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with Security Filtering](https://arxiv.org/abs/2504.13425)
*Grace Byun,Shinsun Lee,Nayoung Choi,Jinho Choi*

Main category: cs.CL

TLDR: SecMulti-RAG框架通过多源检索和本地开源生成器解决企业RAG系统的检索范围有限和数据安全问题，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在企业环境中因检索范围有限和数据安全风险表现不佳，需改进。

Method: 提出SecMulti-RAG框架，结合内部文档、预生成专家知识和外部LLM生成知识，使用本地开源生成器和安全过滤机制。

Result: 在汽车行业报告生成任务中，SecMulti-RAG在LLM和人工评估中均显著优于传统RAG。

Conclusion: SecMulti-RAG是企业RAG的实用且安全的解决方案。

Abstract: Existing Retrieval-Augmented Generation (RAG) systems face challenges in
enterprise settings due to limited retrieval scope and data security risks.
When relevant internal documents are unavailable, the system struggles to
generate accurate and complete responses. Additionally, using closed-source
Large Language Models (LLMs) raises concerns about exposing proprietary
information. To address these issues, we propose the Secure Multifaceted-RAG
(SecMulti-RAG) framework, which retrieves not only from internal documents but
also from two supplementary sources: pre-generated expert knowledge for
anticipated queries and on-demand external LLM-generated knowledge. To mitigate
security risks, we adopt a local open-source generator and selectively utilize
external LLMs only when prompts are deemed safe by a filtering mechanism. This
approach enhances completeness, prevents data leakage, and reduces costs. In
our evaluation on a report generation task in the automotive industry,
SecMulti-RAG significantly outperforms traditional RAG - achieving 79.3 to 91.9
percent win rates across correctness, richness, and helpfulness in LLM-based
evaluation, and 56.3 to 70.4 percent in human evaluation. This highlights
SecMulti-RAG as a practical and secure solution for enterprise RAG.

</details>

### [96] [D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Model](https://arxiv.org/abs/2504.13439)
*Grace Byun,Jinho Choi*

Main category: cs.CL

TLDR: D-GEN是一个开源干扰项生成模型，将开放式数据转换为多选题格式，并通过排名对齐和熵分析评估干扰项质量。


<details>
  <summary>Details</summary>
Motivation: 解决开放式生成模型评估中响应格式不一致的问题，同时减少高质量干扰项生成的时间和人力成本。

Method: 提出D-GEN模型，并采用排名对齐和熵分析两种新方法评估干扰项质量。

Result: D-GEN在排名一致性和熵分布上与真实干扰项高度匹配，人类评估也验证了其干扰性、流畅性和一致性。

Conclusion: D-GEN为多选题评估提供了高效、自动化的干扰项生成方法，设定了新标准。

Abstract: Evaluating generative models with open-ended generation is challenging due to
inconsistencies in response formats. Multiple-choice (MC) evaluation mitigates
this issue, but generating high-quality distractors is time-consuming and
labor-intensive. We introduce D-GEN, the first open-source distractor generator
model that transforms open-ended data into an MC format. To evaluate distractor
quality, we propose two novel methods: (1) ranking alignment, ensuring
generated distractors retain the discriminatory power of ground-truth
distractors, and (2) entropy analysis, comparing model confidence
distributions. Our results show that D-GEN preserves ranking consistency
(Spearman's rho 0.99, Kendall's tau 0.94) and closely matches the entropy
distribution of ground-truth distractors. Human evaluation further confirms the
fluency, coherence, distractiveness, and incorrectness. Our work advances
robust and efficient distractor generation with automated evaluation, setting a
new standard for MC evaluation.

</details>

### [97] [From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs](https://arxiv.org/abs/2504.13471)
*Jiliang Ni,Jiachen Pu,Zhongyi Yang,Kun Zhou,Hui Wang,Xiaoliang Xiao,Dakui Wang,Xin Li,Jingfeng Luo,Conggang Hu*

Main category: cs.CL

TLDR: 本文提出了一种三阶段的高效LLM部署流程，通过原型设计、知识转移和模型压缩，解决了LLM框架中的成本与性能矛盾。


<details>
  <summary>Details</summary>
Motivation: 传统的一阶段LLM部署方法虽然有效，但成本高且延迟大，因此需要一种更高效的解决方案。

Method: 采用三阶段流程：1）构建原型系统生成高质量数据；2）通过知识蒸馏等技术将知识转移到小模型；3）通过量化和剪枝进一步压缩模型。

Result: 最终得到一个0.4B的超小模型，实现了超低延迟和成本，同时保持了高性能。

Conclusion: 该框架的模块化设计和跨领域能力表明其可广泛应用于其他NLP领域。

Abstract: In recent years, Large Language Models (LLMs) have significantly advanced
artificial intelligence by optimizing traditional Natural Language Processing
(NLP) pipelines, improving performance and generalization. This has spurred
their integration into various systems. Many NLP systems, including ours,
employ a "one-stage" pipeline directly incorporating LLMs. While effective,
this approach incurs substantial costs and latency due to the need for large
model parameters to achieve satisfactory outcomes. This paper introduces a
three-stage cost-efficient end-to-end LLM deployment pipeline-including
prototyping, knowledge transfer, and model compression-to tackle the
cost-performance dilemma in LLM-based frameworks. Our approach yields a super
tiny model optimized for cost and performance in online systems, simplifying
the system architecture. Initially, by transforming complex tasks into a
function call-based LLM-driven pipeline, an optimal performance prototype
system is constructed to produce high-quality data as a teacher model. The
second stage combine techniques like rejection fine-tuning, reinforcement
learning and knowledge distillation to transfer knowledge to a smaller 0.5B
student model, delivering effective performance at minimal cost. The final
stage applies quantization and pruning to extremely compress model to 0.4B,
achieving ultra-low latency and cost. The framework's modular design and
cross-domain capabilities suggest potential applicability in other NLP areas.

</details>

### [98] [LLM Sensitivity Evaluation Framework for Clinical Diagnosis](https://arxiv.org/abs/2504.13475)
*Chenwei Yan,Xiangling Fu,Yuxuan Xiong,Tianyi Wang,Siu Cheung Hui,Ji Wu,Xien Liu*

Main category: cs.CL

TLDR: 论文研究了大型语言模型（LLMs）在临床诊断中对关键医学信息的敏感性，发现现有模型存在不足，需改进可靠性和敏感性。


<details>
  <summary>Details</summary>
Motivation: 临床诊断对LLMs的可靠性和敏感性要求更高，但目前研究多关注无关上下文，忽略了关键信息的重要性。

Method: 通过引入不同扰动策略，评估GPT-3.5、GPT-4、Gemini、Claude3和LLaMA2-7b对关键医学信息的敏感性。

Result: 当前LLMs在诊断决策中对关键医学信息的敏感性存在局限。

Conclusion: LLMs需改进可靠性和关键信息敏感性，以增强人类信任并促进实际应用。

Abstract: Large language models (LLMs) have demonstrated impressive performance across
various domains. However, for clinical diagnosis, higher expectations are
required for LLM's reliability and sensitivity: thinking like physicians and
remaining sensitive to key medical information that affects diagnostic
reasoning, as subtle variations can lead to different diagnosis results. Yet,
existing works focus mainly on investigating the sensitivity of LLMs to
irrelevant context and overlook the importance of key information. In this
paper, we investigate the sensitivity of LLMs, i.e. GPT-3.5, GPT-4, Gemini,
Claude3 and LLaMA2-7b, to key medical information by introducing different
perturbation strategies. The evaluation results highlight the limitations of
current LLMs in remaining sensitive to key medical information for diagnostic
decision-making. The evolution of LLMs must focus on improving their
reliability, enhancing their ability to be sensitive to key information, and
effectively utilizing this information. These improvements will enhance human
trust in LLMs and facilitate their practical application in real-world
scenarios. Our code and dataset are available at
https://github.com/chenwei23333/DiagnosisQA.

</details>

### [99] [Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning](https://arxiv.org/abs/2504.13500)
*Jianing Wang,Jin Jiang,Yang Liu,Mengdi Zhang,Xunliang Cai*

Main category: cs.CL

TLDR: 论文提出了一种名为“过程预判”的新策略，用于提升LLM的推理能力，通过预判错误并避免依赖试错法。


<details>
  <summary>Details</summary>
Motivation: 传统LLM推理依赖试错法，缺乏类似人类预判错误的机制，导致效率低下。

Method: 定义了“预判节点”并开发了动态树搜索框架，结合监督微调和强化学习训练机制。

Result: 实验表明该方法显著提升了LLM在复杂推理任务中的表现。

Conclusion: “过程预判”策略有效增强了LLM的推理能力，代码和数据已开源。

Abstract: In this paper, we introduce a new \emph{process prejudge} strategy in LLM
reasoning to demonstrate that bootstrapping with process prejudge allows the
LLM to adaptively anticipate the errors encountered when advancing the
subsequent reasoning steps, similar to people sometimes pausing to think about
what mistakes may occur and how to avoid them, rather than relying solely on
trial and error. Specifically, we define a prejudge node in the rationale,
which represents a reasoning step, with at least one step that follows the
prejudge node that has no paths toward the correct answer. To synthesize the
prejudge reasoning process, we present an automated reasoning framework with a
dynamic tree-searching strategy. This framework requires only one LLM to
perform answer judging, response critiquing, prejudge generation, and thought
completion. Furthermore, we develop a two-phase training mechanism with
supervised fine-tuning (SFT) and reinforcement learning (RL) to further enhance
the reasoning capabilities of LLMs. Experimental results from competition-level
complex reasoning demonstrate that our method can teach the model to prejudge
before thinking and significantly enhance the reasoning ability of LLMs. Code
and data is released at https://github.com/wjn1996/Prejudge-Before-Think.

</details>

### [100] [CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models](https://arxiv.org/abs/2504.13534)
*Feiyang Li,Peng Fang,Zhan Shi,Arijit Khan,Fang Wang,Dan Feng,Weihao Wang,Xin Zhang,Yongjian Cui*

Main category: cs.CL

TLDR: CoT-RAG是一种新型推理框架，通过知识图谱驱动、可学习的检索增强生成和伪程序提示执行，显著提升了大型语言模型在复杂任务中的推理准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决链式思维推理中依赖大型语言模型生成推理链的低可靠性问题，以及自然语言推理链对模型推理逻辑的干扰。

Method: 结合知识图谱驱动的推理链生成、可学习的检索增强生成和伪程序提示执行，提升推理的可信度和逻辑严谨性。

Result: 在九个公共数据集上，CoT-RAG的准确率比现有方法提高了4.0%至23.0%，并在领域特定数据集上表现出高效性和实用性。

Conclusion: CoT-RAG在提升推理任务准确性和可靠性方面具有显著优势，并展示了强大的实际应用潜力。

Abstract: While chain-of-thought (CoT) reasoning improves the performance of large
language models (LLMs) in complex tasks, it still has two main challenges: the
low reliability of relying solely on LLMs to generate reasoning chains and the
interference of natural language reasoning chains on the inference logic of
LLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework
with three key designs: (i) Knowledge Graph-driven CoT Generation, featuring
knowledge graphs to modulate reasoning chain generation of LLMs, thereby
enhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which
incorporates retrieval-augmented generation (RAG) into knowledge graphs to
retrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable
information; (iii) Pseudo-Program Prompting Execution, which encourages LLMs to
execute reasoning tasks in pseudo-programs with greater logical rigor. We
conduct a comprehensive evaluation on nine public datasets, covering three
reasoning problems. Compared with the-state-of-the-art methods, CoT-RAG
exhibits a significant accuracy improvement, ranging from 4.0% to 23.0%.
Furthermore, testing on four domain-specific datasets, CoT-RAG shows remarkable
accuracy and efficient execution, highlighting its strong practical
applicability and scalability.

</details>

### [101] [Enhancing Multilingual Sentiment Analysis with Explainability for Sinhala, English, and Code-Mixed Content](https://arxiv.org/abs/2504.13545)
*Azmarah Rizvi,Navojith Thamindu,A. M. N. H. Adhikari,W. P. U. Senevirathna,Dharshana Kasthurirathna,Lakmini Abeywardhana*

Main category: cs.CL

TLDR: 该研究开发了一种混合框架，用于银行客户评论的多语言情感分析，结合了XLM-RoBERTa和BERT-base-uncased模型，并利用SHAP和LIME提高解释性，取得了高准确率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 银行客户反馈涉及多种语言（英语、僧伽罗语、混合语），现有模型在低资源语言（如僧伽罗语）上表现不佳且缺乏解释性，因此需要开发一种更强大且透明的解决方案。

Method: 研究使用XLM-RoBERTa处理僧伽罗语和混合语文本，BERT-base-uncased处理英语，结合领域特定词典校正，并通过SHAP和LIME提供实时解释。

Result: 模型在英语中达到92.3%的准确率和0.89的F1分数，在僧伽罗语和混合语中达到88.4%的准确率，解释性分析揭示了关键情感驱动因素。

Conclusion: 该研究填补了多语言低资源NLP和解释性方面的空白，为金融应用提供了透明且强大的情感分析工具。

Abstract: Sentiment analysis is crucial for brand reputation management in the banking
sector, where customer feedback spans English, Sinhala, Singlish, and
code-mixed text. Existing models struggle with low-resource languages like
Sinhala and lack interpretability for practical use. This research develops a
hybrid aspect-based sentiment analysis framework that enhances multilingual
capabilities with explainable outputs. Using cleaned banking customer reviews,
we fine-tune XLM-RoBERTa for Sinhala and code-mixed text, integrate
domain-specific lexicon correction, and employ BERT-base-uncased for English.
The system classifies sentiment (positive, neutral, negative) with confidence
scores, while SHAP and LIME improve interpretability by providing real-time
sentiment explanations. Experimental results show that our approaches
outperform traditional transformer-based classifiers, achieving 92.3 percent
accuracy and an F1-score of 0.89 in English and 88.4 percent in Sinhala and
code-mixed content. An explainability analysis reveals key sentiment drivers,
improving trust and transparency. A user-friendly interface delivers
aspect-wise sentiment insights, ensuring accessibility for businesses. This
research contributes to robust, transparent sentiment analysis for financial
applications by bridging gaps in multilingual, low-resource NLP and
explainability.

</details>

### [102] [DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification](https://arxiv.org/abs/2504.13562)
*Yu Li,Han Jiang,Zhihua Wei*

Main category: cs.CL

TLDR: DETAM是一种无需微调的防御方法，通过修改注意力机制来提升LLMs对越狱攻击的防御能力，实验证明其优于现有基线方法，并具有强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，越狱攻击成为安全威胁。现有防御方法（如微调或输入修改）泛化能力有限且可能降低模型实用性。

Method: 通过分析成功与失败防御的注意力分数差异，识别对越狱攻击敏感的注意力头，并在推理时重新分配注意力以强调用户核心意图。

Result: DETAM在越狱防御中表现优于基线方法，泛化能力强，且对实际攻击数据有效。

Conclusion: DETAM提供了一种高效且实用的防御越狱攻击的方法，同时保持了模型实用性。

Abstract: With the widespread adoption of Large Language Models (LLMs), jailbreak
attacks have become an increasingly pressing safety concern. While
safety-aligned LLMs can effectively defend against normal harmful queries, they
remain vulnerable to such attacks. Existing defense methods primarily rely on
fine-tuning or input modification, which often suffer from limited
generalization and reduced utility. To address this, we introduce DETAM, a
finetuning-free defense approach that improves the defensive capabilities
against jailbreak attacks of LLMs via targeted attention modification.
Specifically, we analyze the differences in attention scores between successful
and unsuccessful defenses to identify the attention heads sensitive to
jailbreak attacks. During inference, we reallocate attention to emphasize the
user's core intention, minimizing interference from attack tokens. Our
experimental results demonstrate that DETAM outperforms various baselines in
jailbreak defense and exhibits robust generalization across different attacks
and models, maintaining its effectiveness even on in-the-wild jailbreak data.
Furthermore, in evaluating the model's utility, we incorporated over-defense
datasets, which further validate the superior performance of our approach. The
code will be released immediately upon acceptance.

</details>

### [103] [Improving Generalization in Intent Detection: GRPO with Reward-Based Curriculum Sampling](https://arxiv.org/abs/2504.13592)
*Zihao Feng,Xiaoxue Wang,Ziwei Bai,Donghang Su,Bowen Wu,Qun Yu,Baoxun Wang*

Main category: cs.CL

TLDR: 论文提出了一种结合强化学习（RL）和奖励课程采样（RCS）的方法，用于提升任务导向对话系统中意图检测的泛化能力，显著优于传统监督微调方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在遇到未见意图时性能下降，导致任务路由错误，亟需提升模型在未见任务上的泛化能力。

Method: 采用强化学习（RL）结合奖励课程采样（RCS）在GRPO训练中优化意图检测，并引入链式思考（COT）提升复杂意图检测。

Result: 实验显示RL模型在泛化性能上显著优于监督微调基线，RCS和COT进一步提升了RL的效果。

Conclusion: 该方法显著提升了意图检测的泛化能力，为部署适应性强的对话系统提供了实用见解。

Abstract: Intent detection, a critical component in task-oriented dialogue (TOD)
systems, faces significant challenges in adapting to the rapid influx of
integrable tools with complex interrelationships. Existing approaches, such as
zero-shot reformulations and LLM-based dynamic recognition, struggle with
performance degradation when encountering unseen intents, leading to erroneous
task routing. To enhance the model's generalization performance on unseen
tasks, we employ Reinforcement Learning (RL) combined with a Reward-based
Curriculum Sampling (RCS) during Group Relative Policy Optimization (GRPO)
training in intent detection tasks. Experiments demonstrate that RL-trained
models substantially outperform supervised fine-tuning (SFT) baselines in
generalization. Besides, the introduction of the RCS, significantly bolsters
the effectiveness of RL in intent detection by focusing the model on
challenging cases during training. Moreover, incorporating Chain-of-Thought
(COT) processes in RL notably improves generalization in complex intent
detection tasks, underscoring the importance of thought in challenging
scenarios. This work advances the generalization of intent detection tasks,
offering practical insights for deploying adaptable dialogue systems.

</details>

### [104] [Continual Pre-Training is (not) What You Need in Domain Adaption](https://arxiv.org/abs/2504.13603)
*Pin-Er Chen,Da-Chen Lian,Shu-Kai Hsieh,Sieh-Chuen Huang,Hsuan-Lei Shao,Jun-Wei Chiu,Yang-Hsien Lin,Zih-Ching Chen,Cheng-Kuang,Eddie TC Huang,Simon See*

Main category: cs.CL

TLDR: 本文探讨了领域自适应持续预训练（DACP）对法律大语言模型（LLMs）在法律推理任务中的效果，发现其虽能增强领域知识，但并非对所有任务均有效。


<details>
  <summary>Details</summary>
Motivation: 法律LLMs在自动化任务和提升研究精度方面有显著进展，但适应法律领域的复杂性仍具挑战性。

Method: 通过在中国台湾法律框架下的实验，评估DACP对法律推理任务的影响。

Result: DACP提升了领域知识，但对所有法律任务的性能提升不一致。

Conclusion: 需进一步研究优化法律AI的领域适应策略，权衡DACP对模型泛化和提示任务的影响。

Abstract: The recent advances in Legal Large Language Models (LLMs) have transformed
the landscape of legal research and practice by automating tasks, enhancing
research precision, and supporting complex decision-making processes. However,
effectively adapting LLMs to the legal domain remains challenging due to the
complexity of legal reasoning, the need for precise interpretation of
specialized language, and the potential for hallucinations. This paper examines
the efficacy of Domain-Adaptive Continual Pre-Training (DACP) in improving the
legal reasoning capabilities of LLMs. Through a series of experiments on legal
reasoning tasks within the Taiwanese legal framework, we demonstrate that while
DACP enhances domain-specific knowledge, it does not uniformly improve
performance across all legal tasks. We discuss the trade-offs involved in DACP,
particularly its impact on model generalization and performance in prompt-based
tasks, and propose directions for future research to optimize domain adaptation
strategies in legal AI.

</details>

### [105] [Long-context Non-factoid Question Answering in Indic Languages](https://arxiv.org/abs/2504.13615)
*Ritwik Mishra,Rajiv Ratn Shah,Ponnurangam Kumaraguru*

Main category: cs.CL

TLDR: 研究探讨了通过上下文缩短技术（如OIE、共指消解和APS）提升低资源印地语系语言在QA任务中的表现，结果显示语义和词级评分均有提升，同时减少了计算开销。


<details>
  <summary>Details</summary>
Motivation: 长上下文对LLMs的QA任务构成挑战，尤其在低资源语言中，研究旨在通过上下文缩短技术提升效率和效果。

Method: 采用Open Information Extraction (OIE)、共指消解和Answer Paragraph Selection (APS)等技术组合缩短上下文。

Result: 在四种印地语系语言中，上下文缩短技术平均提升语义评分4%、词级评分47%，微调后进一步提升2%。

Conclusion: 上下文缩短技术能显著提升LLMs在低资源语言QA任务中的表现，但对非事实性问题仍有限制。

Abstract: Question Answering (QA) tasks, which involve extracting answers from a given
context, are relatively straightforward for modern Large Language Models (LLMs)
when the context is short. However, long contexts pose challenges due to the
quadratic complexity of the self-attention mechanism. This challenge is
compounded in Indic languages, which are often low-resource. This study
explores context-shortening techniques, including Open Information Extraction
(OIE), coreference resolution, Answer Paragraph Selection (APS), and their
combinations, to improve QA performance. Compared to the baseline of
unshortened (long) contexts, our experiments on four Indic languages (Hindi,
Tamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield
an average improvement of 4\% in semantic scores and 47\% in token-level scores
when evaluated on three popular LLMs without fine-tuning. Furthermore, with
fine-tuning, we achieve an average increase of 2\% in both semantic and
token-level scores. Additionally, context-shortening reduces computational
overhead. Explainability techniques like LIME and SHAP reveal that when the APS
model confidently identifies the paragraph containing the answer, nearly all
tokens within the selected text receive high relevance scores. However, the
study also highlights the limitations of LLM-based QA systems in addressing
non-factoid questions, particularly those requiring reasoning or debate.
Moreover, verbalizing OIE-generated triples does not enhance system
performance. These findings emphasize the potential of context-shortening
techniques to improve the efficiency and effectiveness of LLM-based QA systems,
especially for low-resource languages. The source code and resources are
available at https://github.com/ritwikmishra/IndicGenQA.

</details>

### [106] [Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models](https://arxiv.org/abs/2504.13626)
*Yule Liu,Jingyi Zheng,Zhen Sun,Zifan Peng,Wenhan Dong,Zeyang Sha,Shiwen Cui,Weiqiang Wang,Xinlei He*

Main category: cs.CL

TLDR: 论文提出了一种名为ThoughtMani的方法，通过在小模型生成的外部CoTs（思维链）中插入标记，有效减少大推理模型（LRMs）的冗余推理步骤，从而降低计算成本并保持性能。


<details>
  <summary>Details</summary>
Motivation: 大推理模型（LRMs）存在“过度思考”问题，即生成冗余推理步骤但性能提升有限。现有方法依赖微调，但存在数据需求高、泛化性差等问题。

Method: 提出ThoughtMani方法，通过在小模型生成的外部CoTs中插入标记（<think>和</think>），操纵LRMs减少冗余推理步骤。

Result: 在QwQ-32B模型上，ThoughtMani保持性能同时减少约30%的输出标记，且安全对齐性平均提升10%。

Conclusion: ThoughtMani为构建高效、可访问的LRMs提供了一种简单有效的方法，适用于实际应用。

Abstract: Recent advancements in large reasoning models (LRMs) have demonstrated the
effectiveness of scaling test-time computation to enhance reasoning
capabilities in multiple tasks. However, LRMs typically suffer from
"overthinking" problems, where models generate significantly redundant
reasoning steps while bringing limited performance gains. Existing work relies
on fine-tuning to mitigate overthinking, which requires additional data,
unconventional training setups, risky safety misalignment, and poor
generalization.
  Through empirical analysis, we reveal an important characteristic of LRM
behaviors that placing external CoTs generated by smaller models between the
thinking token ($\texttt{<think>}$ and $\texttt{</think>)}$ can effectively
manipulate the model to generate fewer thoughts. Building on these insights, we
propose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass
unnecessary intermediate steps and reduce computational costs significantly. We
conduct extensive experiments to validate the utility and efficiency of
ThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code
dataset, ThoughtMani keeps the original performance and reduces output token
counts by approximately 30%, with little overhead from the CoT generator.
Furthermore, we find that ThoughtMani enhances safety alignment by an average
of 10%. Since model vendors typically serve models of different sizes
simultaneously, ThoughtMani provides an effective way to construct more
efficient and accessible LRMs for real-world applications.

</details>

### [107] [Divergent LLM Adoption and Heterogeneous Convergence Paths in Research Writing](https://arxiv.org/abs/2504.13629)
*Cong William Lin,Wu Zhu*

Main category: cs.CL

TLDR: 研究探讨了AI辅助生成修订对学术论文的影响，揭示了不同学科、性别、母语状态和职业阶段在LLM采用上的差异，以及LLM使用对写作风格的影响。


<details>
  <summary>Details</summary>
Motivation: 研究AI辅助生成修订对学术写作的影响，特别是异质性采用模式及其对写作趋同的影响。

Method: 利用arXiv上的627,000篇论文数据集，通过微调特定提示和学科的大型语言模型，开发分类框架检测ChatGPT修订文本的风格。

Result: 发现LLM采用在不同群体中存在显著差异，且LLM使用提高了写作的清晰度、简洁性和正式性。早期采用者、男性、非母语者和初级学者风格变化最明显。

Conclusion: LLM推动了学术写作的趋同，但不同群体的采用和影响存在差异，早期采用者和特定群体风格变化最为显著。

Abstract: Large Language Models (LLMs), such as ChatGPT, are reshaping content creation
and academic writing. This study investigates the impact of AI-assisted
generative revisions on research manuscripts, focusing on heterogeneous
adoption patterns and their influence on writing convergence. Leveraging a
dataset of over 627,000 academic papers from arXiv, we develop a novel
classification framework by fine-tuning prompt- and discipline-specific large
language models to detect the style of ChatGPT-revised texts. Our findings
reveal substantial disparities in LLM adoption across academic disciplines,
gender, native language status, and career stage, alongside a rapid evolution
in scholarly writing styles. Moreover, LLM usage enhances clarity, conciseness,
and adherence to formal writing conventions, with improvements varying by
revision type. Finally, a difference-in-differences analysis shows that while
LLMs drive convergence in academic writing, early adopters, male researchers,
non-native speakers, and junior scholars exhibit the most pronounced stylistic
shifts, aligning their writing more closely with that of established
researchers.

</details>

### [108] [Remedy: Learning Machine Translation Evaluation from Human Preferences with Reward Modeling](https://arxiv.org/abs/2504.13630)
*Shaomu Tan,Christof Monz*

Main category: cs.CL

TLDR: ReMedy是一种新的机器翻译评估框架，通过将翻译评估重新定义为奖励建模任务，利用成对偏好数据学习相对翻译质量，显著提升了评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统基于回归的神经指标难以处理人类评分的噪声和不一致性，而基于提示的大语言模型在系统级评估表现良好但在片段级表现不佳。

Method: ReMedy通过奖励建模任务学习相对翻译质量，使用成对偏好数据而非直接回归人类评分。

Result: 在WMT22-24共享任务（39种语言对，111个MT系统）中，ReMedy-9B在片段级和系统级评估中均达到最先进水平，超越多个大型模型。

Conclusion: ReMedy在检测翻译错误和评估低质量翻译方面表现出色，为MT评估提供了更可靠的解决方案。

Abstract: A key challenge in MT evaluation is the inherent noise and inconsistency of
human ratings. Regression-based neural metrics struggle with this noise, while
prompting LLMs shows promise at system-level evaluation but performs poorly at
segment level. In this work, we propose ReMedy, a novel MT metric framework
that reformulates translation evaluation as a reward modeling task. Instead of
regressing on imperfect human ratings directly, ReMedy learns relative
translation quality using pairwise preference data, resulting in a more
reliable evaluation. In extensive experiments across WMT22-24 shared tasks (39
language pairs, 111 MT systems), ReMedy achieves state-of-the-art performance
at both segment- and system-level evaluation. Specifically, ReMedy-9B surpasses
larger WMT winners and massive closed LLMs such as MetricX-13B,
XCOMET-Ensemble, GEMBA-GPT-4, PaLM-540B, and finetuned PaLM2. Further analyses
demonstrate that ReMedy delivers superior capability in detecting translation
errors and evaluating low-quality translations.

</details>

### [109] [Simulating Before Planning: Constructing Intrinsic User World Model for User-Tailored Dialogue Policy Planning](https://arxiv.org/abs/2504.13643)
*Tao He,Lizi Liao,Ming Liu,Bing Qin*

Main category: cs.CL

TLDR: 论文提出了一种用户定制的对话策略规划框架（UDP），通过建模用户特征和反馈来优化对话策略，解决了现有方法忽视用户特性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有对话策略规划方法常忽略用户特性，而实际场景中用户个性、偏好和目标对交互至关重要。

Method: UDP框架分三阶段：用户画像动态推断、用户反馈预测和用户定制策略规划，并结合主动学习优化训练。

Result: 实验表明UDP在协作与非协作场景中均能有效学习用户特定策略，验证了其鲁棒性和适应性。

Conclusion: UDP框架为推进用户为中心的对话系统提供了有效解决方案。

Abstract: Recent advancements in dialogue policy planning have emphasized optimizing
system agent policies to achieve predefined goals, focusing on strategy design,
trajectory acquisition, and efficient training paradigms. However, these
approaches often overlook the critical role of user characteristics, which are
essential in real-world scenarios like conversational search and
recommendation, where interactions must adapt to individual user traits such as
personality, preferences, and goals. To address this gap, we first conduct a
comprehensive study utilizing task-specific user personas to systematically
assess dialogue policy planning under diverse user behaviors. By leveraging
realistic user profiles for different tasks, our study reveals significant
limitations in existing approaches, highlighting the need for user-tailored
dialogue policy planning. Building on this foundation, we present the
User-Tailored Dialogue Policy Planning (UDP) framework, which incorporates an
Intrinsic User World Model to model user traits and feedback. UDP operates in
three stages: (1) User Persona Portraying, using a diffusion model to
dynamically infer user profiles; (2) User Feedback Anticipating, leveraging a
Brownian Bridge-inspired anticipator to predict user reactions; and (3)
User-Tailored Policy Planning, integrating these insights to optimize response
strategies. To ensure robust performance, we further propose an active learning
approach that prioritizes challenging user personas during training.
Comprehensive experiments on benchmarks, including collaborative and
non-collaborative settings, demonstrate the effectiveness of UDP in learning
user-specific dialogue strategies. Results validate the protocol's utility and
highlight UDP's robustness, adaptability, and potential to advance user-centric
dialogue systems.

</details>

### [110] [Word Embedding Techniques for Classification of Star Ratings](https://arxiv.org/abs/2504.13653)
*Hesham Abdelmotaleb,Craig McNeile,Malgorzata Wojtys*

Main category: cs.CL

TLDR: 该研究探讨了不同词嵌入算法对电信客户评论文本分类的影响，发现BERT结合PCA在性能上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 电信服务对现代社会至关重要，通过分析客户反馈可以改进服务。NLP工具能处理这些文本数据，但不同词嵌入算法对分类效果的影响尚需研究。

Method: 使用BERT、Word2Vec和Doc2Vec等词嵌入技术，结合多种分类算法，并探索PCA降维方法。还研究了不同词嵌入的能耗。

Result: 某些词嵌入模型（如BERT结合PCA）在精确度、召回率和F1分数上表现更优，尤其在复杂分类任务中。

Conclusion: BERT结合PCA的词嵌入方法在电信客户评论分类中表现最佳，且提出的PCA组合方法优于传统平均值法。

Abstract: Telecom services are at the core of today's societies' everyday needs. The
availability of numerous online forums and discussion platforms enables telecom
providers to improve their services by exploring the views of their customers
to learn about common issues that the customers face. Natural Language
Processing (NLP) tools can be used to process the free text collected.
  One way of working with such data is to represent text as numerical vectors
using one of many word embedding models based on neural networks. This research
uses a novel dataset of telecom customers' reviews to perform an extensive
study showing how different word embedding algorithms can affect the text
classification process. Several state-of-the-art word embedding techniques are
considered, including BERT, Word2Vec and Doc2Vec, coupled with several
classification algorithms. The important issue of feature engineering and
dimensionality reduction is addressed and several PCA-based approaches are
explored. Moreover, the energy consumption used by the different word
embeddings is investigated. The findings show that some word embedding models
can lead to consistently better text classifiers in terms of precision, recall
and F1-Score. In particular, for the more challenging classification tasks,
BERT combined with PCA stood out with the highest performance metrics.
Moreover, our proposed PCA approach of combining word vectors using the first
principal component shows clear advantages in performance over the traditional
approach of taking the average.

</details>

### [111] [Multi-Type Context-Aware Conversational Recommender Systems via Mixture-of-Experts](https://arxiv.org/abs/2504.13655)
*Jie Zou,Cheng Lin,Weikang Guo,Zheng Wang,Jiwei Wei,Yang Yang,Hengtao Shen*

Main category: cs.CL

TLDR: 提出了一种多类型上下文感知的对话推荐系统MCCRS，通过混合专家模型融合多种上下文信息，显著提升了推荐效果。


<details>
  <summary>Details</summary>
Motivation: 对话推荐系统通常缺乏丰富的上下文信息，现有方法难以有效结合不同类型的上下文信息。

Method: MCCRS结合结构化知识图谱、非结构化对话历史和物品评论，通过多个专家模型和ChairBot协调生成推荐结果。

Result: 实验表明MCCRS性能显著优于现有基线方法。

Conclusion: MCCRS通过融合多类型上下文信息和专家协作，突破了单一上下文信息的限制，提升了推荐效果。

Abstract: Conversational recommender systems enable natural language conversations and
thus lead to a more engaging and effective recommendation scenario. As the
conversations for recommender systems usually contain limited contextual
information, many existing conversational recommender systems incorporate
external sources to enrich the contextual information. However, how to combine
different types of contextual information is still a challenge. In this paper,
we propose a multi-type context-aware conversational recommender system, called
MCCRS, effectively fusing multi-type contextual information via
mixture-of-experts to improve conversational recommender systems. MCCRS
incorporates both structured information and unstructured information,
including the structured knowledge graph, unstructured conversation history,
and unstructured item reviews. It consists of several experts, with each expert
specialized in a particular domain (i.e., one specific contextual information).
Multiple experts are then coordinated by a ChairBot to generate the final
results. Our proposed MCCRS model takes advantage of different contextual
information and the specialization of different experts followed by a ChairBot
breaks the model bottleneck on a single contextual information. Experimental
results demonstrate that our proposed MCCRS method achieves significantly
higher performance compared to existing baselines.

</details>

### [112] [Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results](https://arxiv.org/abs/2504.13677)
*Andrea Santilli,Adam Golinski,Michael Kirchhof,Federico Danieli,Arno Blaas,Miao Xiong,Luca Zappella,Sinead Williamson*

Main category: cs.CL

TLDR: 论文指出语言模型不确定性量化（UQ）评估中常用的正确性函数存在长度偏差，扭曲了UQ方法的评估结果，并提出LLM-as-a-judge方法作为潜在解决方案。


<details>
  <summary>Details</summary>
Motivation: 语言模型的不确定性量化（UQ）对提升其安全性和可靠性至关重要，但现有评估方法因正确性函数的偏差而失真。

Method: 评估了7种正确性函数（包括基于词汇、嵌入和LLM-as-a-judge方法）在4个数据集、4个模型和6种UQ方法上的表现。

Result: 发现正确性函数的长度偏差与UQ方法的长度偏差相互作用，扭曲了评估结果，LLM-as-a-judge方法偏差最小。

Conclusion: LLM-as-a-judge方法可能成为减少评估偏差的有效解决方案。

Abstract: Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for
improving their safety and reliability. Evaluations often use performance
metrics like AUROC to assess how well UQ methods (e.g., negative sequence
probabilities) correlate with task correctness functions (e.g., ROUGE-L). In
this paper, we show that commonly used correctness functions bias UQ
evaluations by inflating the performance of certain UQ methods. We evaluate 7
correctness functions -- from lexical-based and embedding-based metrics to
LLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our
analysis reveals that length biases in the errors of these correctness
functions distort UQ assessments by interacting with length biases in UQ
methods. We identify LLM-as-a-judge approaches as among the least length-biased
choices and hence a potential solution to mitigate these biases.

</details>

### [113] [Deep literature reviews: an application of fine-tuned language models to migration research](https://arxiv.org/abs/2504.13685)
*Stefano M. Iacus,Haodong Qi,Jiyoung Han*

Main category: cs.CL

TLDR: 本文提出了一种结合传统文献计量方法和大型语言模型（LLM）的混合框架，用于高效、一致且深入的文献综述。


<details>
  <summary>Details</summary>
Motivation: 传统文献综述方法在处理大规模研究内容时效率低且一致性差，需要一种更高效的工具来增强知识合成的广度和深度。

Method: 通过微调开源LLM，结合人工验证，实现大规模文献的定性分析，并应用于20000多篇关于人类迁移的科学文章。

Result: 结果表明，领域适应的LLM能准确筛选相关研究、检测趋势并识别研究空白，尤其揭示了气候引发迁移的研究兴趣增长，但存在环境危害研究不均衡的问题。

Conclusion: 该框架展示了微调LLM在多学科文献综述中的潜力，可加速知识合成和科学发现。

Abstract: This paper presents a hybrid framework for literature reviews that augments
traditional bibliometric methods with large language models (LLMs). By
fine-tuning open-source LLMs, our approach enables scalable extraction of
qualitative insights from large volumes of research content, enhancing both the
breadth and depth of knowledge synthesis. To improve annotation efficiency and
consistency, we introduce an error-focused validation process in which LLMs
generate initial labels and human reviewers correct misclassifications.
Applying this framework to over 20000 scientific articles about human
migration, we demonstrate that a domain-adapted LLM can serve as a "specialist"
model - capable of accurately selecting relevant studies, detecting emerging
trends, and identifying critical research gaps. Notably, the LLM-assisted
review reveals a growing scholarly interest in climate-induced migration.
However, existing literature disproportionately centers on a narrow set of
environmental hazards (e.g., floods, droughts, sea-level rise, and land
degradation), while overlooking others that more directly affect human health
and well-being, such as air and water pollution or infectious diseases. This
imbalance highlights the need for more comprehensive research that goes beyond
physical environmental changes to examine their ecological and societal
consequences, particularly in shaping migration as an adaptive response.
Overall, our proposed framework demonstrates the potential of fine-tuned LLMs
to conduct more efficient, consistent, and insightful literature reviews across
disciplines, ultimately accelerating knowledge synthesis and scientific
discovery.

</details>

### [114] [BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models](https://arxiv.org/abs/2504.13775)
*Zhengxian Wu,Juan Wen,Wanli Peng,Ziwei Zhang,Yinghan Zhou,Yiming Xue*

Main category: cs.CL

TLDR: 论文提出了一种基于黑盒大语言模型的自适应优化机制的新型后门攻击方法（BadApex），通过迭代优化提示生成高质量、语义一致的毒化文本，显著提升了攻击效果和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击方法在文本质量和语义一致性上存在不足，且依赖专家经验的手工提示，难以适应防御措施。

Method: 设计了自适应优化机制，通过生成和修改代理迭代优化初始提示，生成高质量毒化文本。

Result: 实验表明，BadApex在攻击成功率（ASR）上显著优于现有方法，即使在防御下仍保持96.75%的成功率。

Conclusion: BadApex通过自适应优化机制有效提升了后门攻击的适应性、语义一致性和文本质量。

Abstract: Previous insertion-based and paraphrase-based backdoors have achieved great
success in attack efficacy, but they ignore the text quality and semantic
consistency between poisoned and clean texts. Although recent studies introduce
LLMs to generate poisoned texts and improve the stealthiness, semantic
consistency, and text quality, their hand-crafted prompts rely on expert
experiences, facing significant challenges in prompt adaptability and attack
performance after defenses. In this paper, we propose a novel backdoor attack
based on adaptive optimization mechanism of black-box large language models
(BadApex), which leverages a black-box LLM to generate poisoned text through a
refined prompt. Specifically, an Adaptive Optimization Mechanism is designed to
refine an initial prompt iteratively using the generation and modification
agents. The generation agent generates the poisoned text based on the initial
prompt. Then the modification agent evaluates the quality of the poisoned text
and refines a new prompt. After several iterations of the above process, the
refined prompt is used to generate poisoned texts through LLMs. We conduct
extensive experiments on three dataset with six backdoor attacks and two
defenses. Extensive experimental results demonstrate that BadApex significantly
outperforms state-of-the-art attacks. It improves prompt adaptability, semantic
consistency, and text quality. Furthermore, when two defense methods are
applied, the average attack success rate (ASR) still up to 96.75%.

</details>

### [115] [Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping Occupied Territory from Open Source Intelligence](https://arxiv.org/abs/2504.13730)
*Paul K. Mandal,Cole Leo,Connor Hurley*

Main category: cs.CL

TLDR: CONTACT是一个基于大语言模型（LLMs）和少量监督的领土控制预测框架，通过两种方法（SetFit和BLOOMZ-560m的提示调优）在低资源环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 利用开源情报（OSINT）的非结构化文本数据预测领土控制情况，减少标注负担并支持结构化推理。

Method: 使用少量标注的新闻文章数据集（涉及ISIS在叙利亚和伊拉克的活动），结合提示调优方法提取控制相关信号（如军事行动、伤亡和地点）。

Result: 基于BLOOMZ的模型优于SetFit基线，提示调优方法在低资源环境下提升了泛化能力。

Conclusion: CONTACT证明少量标注数据结合LLMs可以有效减少标注负担，并从开放OSINT流中提取结构化信息。

Abstract: Open-source intelligence provides a stream of unstructured textual data that
can inform assessments of territorial control. We present CONTACT, a framework
for territorial control prediction using large language models (LLMs) and
minimal supervision. We evaluate two approaches: SetFit, an embedding-based
few-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a
multilingual generative LLM. Our model is trained on a small hand-labeled
dataset of news articles covering ISIS activity in Syria and Iraq, using
prompt-conditioned extraction of control-relevant signals such as military
operations, casualties, and location references. We show that the BLOOMZ-based
model outperforms the SetFit baseline, and that prompt-based supervision
improves generalization in low-resource settings. CONTACT demonstrates that
LLMs fine-tuned using few-shot methods can reduce annotation burdens and
support structured inference from open-ended OSINT streams. Our code is
available at https://github.com/PaulKMandal/CONTACT/.

</details>

### [116] [Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations](https://arxiv.org/abs/2504.13816)
*Chenghao Xiao,Hou Pong Chan,Hao Zhang,Mahani Aljunied,Lidong Bing,Noura Al Moubayed,Yu Rong*

Main category: cs.CL

TLDR: 本文研究了LLMs在不同语言中如何识别知识边界，揭示了其内部表征的规律，并提出了一种无需训练的对齐方法以减少低资源语言中的幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs的知识边界对防止幻觉至关重要，但现有研究主要集中在英语上，缺乏跨语言分析。

Method: 通过分析LLMs处理多语言已知和未知问题时的内部表征，提出了一种训练自由的对齐方法，并构建了多语言评估套件。

Result: 发现LLMs的知识边界感知编码在中上层，语言差异呈线性结构，双语微调可增强跨语言识别能力。

Conclusion: 研究填补了跨语言知识边界分析的空白，提出的方法有效减少了低资源语言的幻觉风险。

Abstract: While understanding the knowledge boundaries of LLMs is crucial to prevent
hallucination, research on knowledge boundaries of LLMs has predominantly
focused on English. In this work, we present the first study to analyze how
LLMs recognize knowledge boundaries across different languages by probing their
internal representations when processing known and unknown questions in
multiple languages. Our empirical studies reveal three key findings: 1) LLMs'
perceptions of knowledge boundaries are encoded in the middle to middle-upper
layers across different languages. 2) Language differences in knowledge
boundary perception follow a linear structure, which motivates our proposal of
a training-free alignment method that effectively transfers knowledge boundary
perception ability across languages, thereby helping reduce hallucination risk
in low-resource languages; 3) Fine-tuning on bilingual question pair
translation further enhances LLMs' recognition of knowledge boundaries across
languages. Given the absence of standard testbeds for cross-lingual knowledge
boundary analysis, we construct a multilingual evaluation suite comprising
three representative types of knowledge boundary data. Our code and datasets
are publicly available at
https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries.

</details>

### [117] [Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models](https://arxiv.org/abs/2504.13825)
*Junjie Yang,Junhao Song,Xudong Han,Ziqian Bi,Tianyang Wang,Chia Xin Liang,Xinyuan Song,Yichao Zhang,Qian Niu,Benji Peng,Keyu Chen,Ming Liu*

Main category: cs.CL

TLDR: 知识蒸馏（KD）通过从复杂教师模型向简单学生模型转移知识，显著提升模型效率和准确性，广泛应用于图像分类、目标检测等领域。


<details>
  <summary>Details</summary>
Motivation: 探讨知识蒸馏在提升模型效率和准确性方面的潜力，以及其在压缩大型语言模型中的作用。

Method: 采用注意力机制、块级逻辑蒸馏和解耦蒸馏等创新方法优化知识转移。

Result: 知识蒸馏显著提升学生模型性能，同时减少计算开销并提高推理速度。

Conclusion: 知识蒸馏在人工智能和机器学习中具有重要价值，未来研究方向包括进一步优化知识转移方法。

Abstract: Knowledge distillation (KD) is a technique for transferring knowledge from
complex teacher models to simpler student models, significantly enhancing model
efficiency and accuracy. It has demonstrated substantial advancements in
various applications including image classification, object detection, language
modeling, text classification, and sentiment analysis. Recent innovations in KD
methods, such as attention-based approaches, block-wise logit distillation, and
decoupling distillation, have notably improved student model performance. These
techniques focus on stimulus complexity, attention mechanisms, and global
information capture to optimize knowledge transfer. In addition, KD has proven
effective in compressing large language models while preserving accuracy,
reducing computational overhead, and improving inference speed. This survey
synthesizes the latest literature, highlighting key findings, contributions,
and future directions in knowledge distillation to provide insights for
researchers and practitioners on its evolving role in artificial intelligence
and machine learning.

</details>

### [118] [Generative AI Act II: Test Time Scaling Drives Cognition Engineering](https://arxiv.org/abs/2504.13828)
*Shijie Xia,Yiwei Qin,Xuefeng Li,Yan Ma,Run-Ze Fan,Steffi Chern,Haoyang Zou,Fan Zhou,Xiangkun Hu,Jiahe Jin,Yanheng He,Yixin Ye,Yixiu Liu,Pengfei Liu*

Main category: cs.CL

TLDR: 论文分析了生成式AI从第一代（2020-2023）到第二代（2024至今）的演变，强调了认知工程的重要性，并提供了相关教程和实现。


<details>
  <summary>Details</summary>
Motivation: 第一代大语言模型在参数和数据规模上取得了成功，但在知识延迟、浅层推理和认知过程受限方面存在不足，因此需要转向更高级的认知工程。

Method: 通过测试时扩展技术，将模型从知识检索系统转变为思维构建引擎，并通过教程和优化实现普及认知工程。

Result: 论文提供了认知工程的概念基础和实现方法，并通过GitHub资源库持续更新相关内容。

Conclusion: 认知工程是AI发展的关键，第二代模型通过思维构建引擎实现了更深层次的AI交互。

Abstract: The first generation of Large Language Models - what might be called "Act I"
of generative AI (2020-2023) - achieved remarkable success through massive
parameter and data scaling, yet exhibited fundamental limitations in knowledge
latency, shallow reasoning, and constrained cognitive processes. During this
era, prompt engineering emerged as our primary interface with AI, enabling
dialogue-level communication through natural language. We now witness the
emergence of "Act II" (2024-present), where models are transitioning from
knowledge-retrieval systems (in latent space) to thought-construction engines
through test-time scaling techniques. This new paradigm establishes a
mind-level connection with AI through language-based thoughts. In this paper,
we clarify the conceptual foundations of cognition engineering and explain why
this moment is critical for its development. We systematically break down these
advanced approaches through comprehensive tutorials and optimized
implementations, democratizing access to cognition engineering and enabling
every practitioner to participate in AI's second act. We provide a regularly
updated collection of papers on test-time scaling in the GitHub Repository:
https://github.com/GAIR-NLP/cognition-engineering

</details>

### [119] [Science Hierarchography: Hierarchical Organization of Science Literature](https://arxiv.org/abs/2504.13834)
*Muhan Gao,Jash Shah,Weiqi Wang,Daniel Khashabi*

Main category: cs.CL

TLDR: 论文提出了一种名为SCIENCE HIERARCHOGRAPHY的方法，通过分层结构组织科学文献，结合嵌入聚类和LLM提示，提升文献探索效率。


<details>
  <summary>Details</summary>
Motivation: 科学知识快速增长，现有工具缺乏抽象能力，难以跟踪跨学科的进展和高层次概念联系。

Method: 结合快速嵌入聚类和LLM提示，构建多层次分类的层次结构。

Result: 该方法在质量和速度上优于依赖LLM的方法，提升文献探索的效率和可解释性。

Conclusion: SCIENCE HIERARCHOGRAPHY为科学文献探索提供了结构化途径，支持趋势发现和跨学科研究。

Abstract: Scientific knowledge is growing rapidly, making it challenging to track
progress and high-level conceptual links across broad disciplines. While
existing tools like citation networks and search engines make it easy to access
a few related papers, they fundamentally lack the flexible abstraction needed
to represent the density of activity in various scientific subfields. We
motivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature
into a high-quality hierarchical structure that allows for the categorization
of scientific work across varying levels of abstraction, from very broad fields
to very specific studies. Such a representation can provide insights into which
fields are well-explored and which are under-explored. To achieve the goals of
SCIENCE HIERARCHOGRAPHY, we develop a range of algorithms. Our primary approach
combines fast embedding-based clustering with LLM-based prompting to balance
the computational efficiency of embedding methods with the semantic precision
offered by LLM prompting. We demonstrate that this approach offers the best
trade-off between quality and speed compared to methods that heavily rely on
LLM prompting, such as iterative tree construction with LLMs. To better reflect
the interdisciplinary and multifaceted nature of research papers, our hierarchy
captures multiple dimensions of categorization beyond simple topic labels. We
evaluate the utility of our framework by assessing how effectively an LLM-based
agent can locate target papers using the hierarchy. Results show that this
structured approach enhances interpretability, supports trend discovery, and
offers an alternative pathway for exploring scientific literature beyond
traditional search methods. Code, data and demo:
$\href{https://github.com/JHU-CLSP/science-hierarchography}{https://github.com/JHU-CLSP/science-hierarchography}$

</details>

### [120] [MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space](https://arxiv.org/abs/2504.13835)
*Yicheng Chen,Yining Li,Kai Hu,Zerun Ma,Haochen Ye,Kai Chen*

Main category: cs.CL

TLDR: 提出了一种基于语义空间信息增益的数据选择方法（MIG），用于高效构建高质量的指令调优数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法在数据质量和多样性选择上存在不足，缺乏对语义空间的全面建模。

Method: 通过构建标签图建模语义空间，并基于信息增益迭代选择数据样本。

Result: MIG方法在多个数据集和基准模型上表现优异，仅用5%数据即可达到全数据集训练的效果。

Conclusion: MIG方法显著提升了指令调优数据集的构建效率和质量。

Abstract: Data quality and diversity are key to the construction of effective
instruction-tuning datasets. % With the increasing availability of open-source
instruction-tuning datasets, it is advantageous to automatically select
high-quality and diverse subsets from a vast amount of data. % Existing methods
typically prioritize instance quality and use heuristic rules to maintain
diversity. % However, this absence of a comprehensive view of the entire
collection often leads to suboptimal results. % Moreover, heuristic rules
generally focus on distance or clustering within the embedding space, which
fails to accurately capture the intent of complex instructions in the semantic
space. % To bridge this gap, we propose a unified method for quantifying the
information content of datasets. This method models the semantic space by
constructing a label graph and quantifies diversity based on the distribution
of information within the graph. % Based on such a measurement, we further
introduce an efficient sampling method that selects data samples iteratively to
\textbf{M}aximize the \textbf{I}nformation \textbf{G}ain (MIG) in semantic
space. % Experiments on various datasets and base models demonstrate that MIG
consistently outperforms state-of-the-art methods. % Notably, the model
fine-tuned with 5\% Tulu3 data sampled by MIG achieves comparable performance
to the official SFT model trained on the full dataset, with improvements of
+5.73\% on AlpacaEval and +6.89\% on Wildbench.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [121] [The Quantum LLM: Modeling Semantic Spaces with Quantum Principles](https://arxiv.org/abs/2504.13202)
*Timo Aukusti Laine*

Main category: cs.AI

TLDR: 本文澄清了量子启发的框架在大型语言模型（LLMs）中建模语义表示和处理的核心假设，并详细阐述了六个关键原则，以证明该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 通过量子力学的数学工具和概念类比，为LLMs的复杂系统提供新的研究视角，并探索量子计算在开发更强大和高效LLMs中的潜力。

Method: 提出并详细阐述了六个关键原则，用于描述LLMs中的语义表示、交互和动态。

Result: 证明了量子启发框架是研究语义空间的有效方法，并为信息处理和响应生成提供了新见解。

Conclusion: 量子启发框架不仅为LLMs的研究提供了新视角，还展示了量子计算在提升LLMs性能方面的潜力。

Abstract: In the previous article, we presented a quantum-inspired framework for
modeling semantic representation and processing in Large Language Models
(LLMs), drawing upon mathematical tools and conceptual analogies from quantum
mechanics to offer a new perspective on these complex systems. In this paper,
we clarify the core assumptions of this model, providing a detailed exposition
of six key principles that govern semantic representation, interaction, and
dynamics within LLMs. The goal is to justify that a quantum-inspired framework
is a valid approach to studying semantic spaces. This framework offers valuable
insights into their information processing and response generation, and we
further discuss the potential of leveraging quantum computing to develop
significantly more powerful and efficient LLMs based on these principles.

</details>

### [122] [Graphical Models for Decision-Making: Integrating Causality and Game Theory](https://arxiv.org/abs/2504.13210)
*Maarten C. Vonk,Mauricio Gonzalez Soto,Anna V. Kononova*

Main category: cs.AI

TLDR: 论文探讨了因果性与博弈论的结合，强调其在决策中的潜力，并澄清了关键概念以促进实际应用。


<details>
  <summary>Details</summary>
Motivation: 因果性和博弈论在决策中具有重要作用，但两者的结合在实际应用中尚未充分探索。

Method: 通过概率图模型澄清关键概念，并提供直观示例和实现输入。

Result: 为实践者提供了模型应用和选择的见解，并引用了支持实现的研究。

Conclusion: 希望促进这些模型在现实场景中的更广泛应用。

Abstract: Causality and game theory are two influential fields that contribute
significantly to decision-making in various domains. Causality defines and
models causal relationships in complex policy problems, while game theory
provides insights into strategic interactions among stakeholders with competing
interests. Integrating these frameworks has led to significant theoretical
advancements with the potential to improve decision-making processes. However,
practical applications of these developments remain underexplored. To support
efforts toward implementation, this paper clarifies key concepts in game theory
and causality that are essential to their intersection, particularly within the
context of probabilistic graphical models. By rigorously examining these
concepts and illustrating them with intuitive, consistent examples, we clarify
the required inputs for implementing these models, provide practitioners with
insights into their application and selection across different scenarios, and
reference existing research that supports their implementation. We hope this
work encourages broader adoption of these models in real-world scenarios.

</details>

### [123] [Causal-Copilot: An Autonomous Causal Analysis Agent](https://arxiv.org/abs/2504.13263)
*Xinyue Wang,Kun Zhou,Wenyi Wu,Har Simrat Singh,Fang Nan,Songyao Jin,Aryan Philip,Saloni Patnaik,Hou Zhu,Shivam Singh,Parjanya Prashant,Qian Shen,Biwei Huang*

Main category: cs.AI

TLDR: Causal-Copilot是一个基于大语言模型的自主代理，旨在简化因果分析的复杂性，为非专家提供易用且严谨的因果分析工具。


<details>
  <summary>Details</summary>
Motivation: 因果分析在科学发现和决策中至关重要，但其复杂性和算法难度使其难以被领域专家广泛应用，同时因果研究者也缺乏实际应用场景来验证方法。

Method: Causal-Copilot通过自动化因果分析的全流程（包括因果发现、推断、算法选择等），并集成20多种先进技术，支持自然语言交互。

Result: 实证评估显示，Causal-Copilot性能优于现有基线，实现了理论严谨性与实际应用性的结合。

Conclusion: Causal-Copilot为领域专家提供了高效的因果分析工具，同时为因果研究提供了丰富的实际应用场景。

Abstract: Causal analysis plays a foundational role in scientific discovery and
reliable decision-making, yet it remains largely inaccessible to domain experts
due to its conceptual and algorithmic complexity. This disconnect between
causal methodology and practical usability presents a dual challenge: domain
experts are unable to leverage recent advances in causal learning, while causal
researchers lack broad, real-world deployment to test and refine their methods.
To address this, we introduce Causal-Copilot, an autonomous agent that
operationalizes expert-level causal analysis within a large language model
framework. Causal-Copilot automates the full pipeline of causal analysis for
both tabular and time-series data -- including causal discovery, causal
inference, algorithm selection, hyperparameter optimization, result
interpretation, and generation of actionable insights. It supports interactive
refinement through natural language, lowering the barrier for non-specialists
while preserving methodological rigor. By integrating over 20 state-of-the-art
causal analysis techniques, our system fosters a virtuous cycle -- expanding
access to advanced causal methods for domain experts while generating rich,
real-world applications that inform and advance causal theory. Empirical
evaluations demonstrate that Causal-Copilot achieves superior performance
compared to existing baselines, offering a reliable, scalable, and extensible
solution that bridges the gap between theoretical sophistication and real-world
applicability in causal analysis.

</details>

### [124] [On the Definition of Robustness and Resilience of AI Agents for Real-time Congestion Management](https://arxiv.org/abs/2504.13314)
*Timothy Tjhay,Ricardo J. Bessa,Jose Paulos*

Main category: cs.AI

TLDR: 本文提出了一种定量评估强化学习代理在拥堵管理中的鲁棒性和弹性的框架，填补了欧盟AI法案中评估方法的空白。


<details>
  <summary>Details</summary>
Motivation: 欧盟AI法案对高风险领域的AI系统提出了鲁棒性、弹性和安全性要求，但缺乏具体的评估方法。

Method: 利用Grid2Op数字环境，通过扰动代理模拟自然和对抗性干扰，评估AI系统在不同场景下的表现。鲁棒性通过稳定性和奖励影响指标衡量，弹性则量化从性能下降中恢复的能力。

Result: 结果表明，该框架能有效识别漏洞，并提升AI系统在关键应用中的鲁棒性和弹性。

Conclusion: 该框架为AI系统在高风险领域的评估提供了实用工具，有助于提升其可靠性和安全性。

Abstract: The European Union's Artificial Intelligence (AI) Act defines robustness,
resilience, and security requirements for high-risk sectors but lacks detailed
methodologies for assessment. This paper introduces a novel framework for
quantitatively evaluating the robustness and resilience of reinforcement
learning agents in congestion management. Using the AI-friendly digital
environment Grid2Op, perturbation agents simulate natural and adversarial
disruptions by perturbing the input of AI systems without altering the actual
state of the environment, enabling the assessment of AI performance under
various scenarios. Robustness is measured through stability and reward impact
metrics, while resilience quantifies recovery from performance degradation. The
results demonstrate the framework's effectiveness in identifying
vulnerabilities and improving AI robustness and resilience for critical
applications.

</details>

### [125] [Cost-of-Pass: An Economic Framework for Evaluating Language Models](https://arxiv.org/abs/2504.13359)
*Mehmet Hamza Erol,Batu El,Mirac Suzgun,Mert Yuksekgonul,James Zou*

Main category: cs.AI

TLDR: 论文提出了一种基于生产理论的框架，通过结合准确性和推理成本来评估语言模型的经济价值，并引入了“cost-of-pass”和“frontier cost-of-pass”概念。研究发现不同模型在不同任务中的成本效益差异显著，并追踪了技术进步对成本效率的影响。


<details>
  <summary>Details</summary>
Motivation: 评估AI系统在经济中的广泛采用需要权衡其性能与成本，但目前缺乏综合考虑这两者的指标。

Method: 提出基于生产理论的框架，结合准确性和推理成本，定义“cost-of-pass”和“frontier cost-of-pass”，并通过分析模型在不同任务中的表现来验证。

Result: 轻量级模型在基础定量任务中最具成本效益，大型模型适用于知识密集型任务，推理模型适用于复杂定量问题。技术进步显著降低了成本，尤其是复杂定量任务。

Conclusion: 模型级别的创新是成本效率的主要驱动力，提出的经济框架为衡量进步和指导部署提供了原则性工具。

Abstract: The widespread adoption of AI systems in the economy hinges on their ability
to generate economic value that outweighs their inference costs. Evaluating
this tradeoff requires metrics that account for both performance and costs. We
propose a framework grounded in production theory for evaluating language
models by combining accuracy and inference cost. We introduce "cost-of-pass",
the expected monetary cost of generating a correct solution. We then define the
"frontier cost-of-pass" as the minimum cost-of-pass achievable across available
models or the "human-expert, using the approximate cost of hiring an expert.
Our analysis reveals distinct economic insights. First, lightweight models are
most cost-effective for basic quantitative tasks, large models for
knowledge-intensive ones, and reasoning models for complex quantitative
problems, despite higher per-token costs. Second, tracking this frontier
cost-of-pass over the past year reveals significant progress, particularly for
complex quantitative tasks where the cost has roughly halved every few months.
Third, to trace key innovations driving this progress, we examine
counterfactual frontiers: estimates of cost-efficiency without specific model
classes. We find that innovations in lightweight, large, and reasoning models
have been essential for pushing the frontier in basic quantitative,
knowledge-intensive, and complex quantitative tasks, respectively. Finally, we
assess the cost-reductions afforded by common inference-time techniques like
majority voting and self-refinement, finding that their marginal accuracy gains
rarely justify their costs. Our findings underscore that complementary
model-level innovations are the primary drivers of cost-efficiency, and our
economic framework provides a principled tool for measuring this progress and
guiding deployment.

</details>

### [126] [In between myth and reality: AI for math -- a case study in category theory](https://arxiv.org/abs/2504.13360)
*Răzvan Diaconescu*

Main category: cs.AI

TLDR: 本文探讨了AI系统在解决数学问题中的表现，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统如何辅助数学研究，并为开发者提供改进方向。

Method: 实验使用了两种当代主流AI系统进行数学研究测试。

Result: 实验结果得出了混合结论。

Conclusion: AI系统在数学研究中具有潜力，但需进一步改进。

Abstract: Recently, there is an increasing interest in understanding the performance of
AI systems in solving math problems. A multitude of tests have been performed,
with mixed conclusions. In this paper we discuss an experiment we have made in
the direction of mathematical research, with two of the most prominent
contemporary AI systems. One of the objective of this experiment is to get an
understanding of how AI systems can assist mathematical research. Another
objective is to support the AI systems developers by formulating suggestions
for directions of improvement.

</details>

### [127] [Trust, but verify](https://arxiv.org/abs/2504.13443)
*Michael J. Yuan,Carlos Campoy,Sydney Lai,James Snewin,Ju Long*

Main category: cs.AI

TLDR: 本文提出了一种通过社交共识检测去中心化AI网络中运行未经授权或错误LLM的节点的方法，并讨论了基于EigenLayer AVS的激励系统。


<details>
  <summary>Details</summary>
Motivation: 在去中心化AI网络中，确保节点运行正确的LLM以维持服务质量是一个关键问题。

Method: 通过社交共识算法检测异常节点，并利用EigenLayer AVS引入经济激励和惩罚机制。

Result: 实验数据表明，该方法在主要由诚实节点组成的网络中能有效检测异常行为。

Conclusion: 社交共识结合经济激励是确保去中心化AI网络服务质量的可行方案。

Abstract: Decentralized AI agent networks, such as Gaia, allows individuals to run
customized LLMs on their own computers and then provide services to the public.
However, in order to maintain service quality, the network must verify that
individual nodes are running their designated LLMs. In this paper, we
demonstrate that in a cluster of mostly honest nodes, we can detect nodes that
run unauthorized or incorrect LLM through social consensus of its peers. We
will discuss the algorithm and experimental data from the Gaia network. We will
also discuss the intersubjective validation system, implemented as an
EigenLayer AVS to introduce financial incentives and penalties to encourage
honest behavior from LLM nodes.

</details>

### [128] [Optimizing Electric Vehicle Charging Station Locations: A Data-driven System with Multi-source Fusion](https://arxiv.org/abs/2504.13517)
*Lihuan Li,Du Yin,Hao Xue,David Lillo-Trynes,Flora Salim*

Main category: cs.AI

TLDR: 该论文提出了一种基于多源数据融合的数据驱动系统，用于优化电动汽车充电站的位置规划。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车充电需求的增长，城市规划者面临充电基础设施选址的挑战，如长途旅行中的里程焦虑和住宅区充电站分布不均。

Method: 结合新南威尔士州的电动汽车行程数据、地理数据（如路线和地方政府区域边界）以及其他特征（如火灾和洪水风险、兴趣点），开发了一个数据驱动系统。

Result: 通过可视化展示和案例研究评估了系统的结果，为未来充电站选址提供了数据支持。

Conclusion: 该研究为讨论未来电动汽车充电站位置规划提供了一个平台，并提供了新的见解。

Abstract: With the growing electric vehicles (EVs) charging demand, urban planners face
the challenges of providing charging infrastructure at optimal locations. For
example, range anxiety during long-distance travel and the inadequate
distribution of residential charging stations are the major issues many cities
face. To achieve reasonable estimation and deployment of the charging demand,
we develop a data-driven system based on existing EV trips in New South Wales
(NSW) state, Australia, incorporating multiple factors that enhance the
geographical feasibility of recommended charging stations. Our system
integrates data sources including EV trip data, geographical data such as route
data and Local Government Area (LGA) boundaries, as well as features like fire
and flood risks, and Points of Interest (POIs). We visualize our results to
intuitively demonstrate the findings from our data-driven, multi-source fusion
system, and evaluate them through case studies. The outcome of this work can
provide a platform for discussion to develop new insights that could be used to
give guidance on where to position future EV charging stations.

</details>

### [129] [Task Assignment and Exploration Optimization for Low Altitude UAV Rescue via Generative AI Enhanced Multi-agent Reinforcement Learning](https://arxiv.org/abs/2504.13554)
*Xin Tang,Qian Chen,Wenjie Weng,Chao Jin,Zhang Liu,Jiacheng Wang,Geng Sun,Xiaohuan Li,Dusit Niyato*

Main category: cs.AI

TLDR: 论文提出了一种新型合作框架，通过无人机、地面嵌入式机器人（GERs）和高空平台（HAPs）的资源共享，优化任务分配和探索效率，以减少任务完成时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在高计算需求任务中资源不足导致的系统不稳定问题。

Method: 采用Lyapunov优化技术将问题转化为确定性优化问题，并提出HG-MADDPG算法，结合匈牙利算法和生成扩散模型（GDM）的多智能体深度确定性策略梯度（MADDPG）。

Result: 仿真结果显示，该方法在任务卸载效率、延迟降低和系统稳定性方面显著优于基线方法。

Conclusion: 提出的框架和算法有效解决了无人机资源受限问题，提升了任务执行效率和系统稳定性。

Abstract: Artificial Intelligence (AI)-driven convolutional neural networks enhance
rescue, inspection, and surveillance tasks performed by low-altitude uncrewed
aerial vehicles (UAVs) and ground computing nodes (GCNs) in unknown
environments. However, their high computational demands often exceed a single
UAV's capacity, leading to system instability, further exacerbated by the
limited and dynamic resources of GCNs. To address these challenges, this paper
proposes a novel cooperation framework involving UAVs, ground-embedded robots
(GERs), and high-altitude platforms (HAPs), which enable resource pooling
through UAV-to-GER (U2G) and UAV-to-HAP (U2H) communications to provide
computing services for UAV offloaded tasks. Specifically, we formulate the
multi-objective optimization problem of task assignment and exploration
optimization in UAVs as a dynamic long-term optimization problem. Our objective
is to minimize task completion time and energy consumption while ensuring
system stability over time. To achieve this, we first employ the Lyapunov
optimization technique to transform the original problem, with stability
constraints, into a per-slot deterministic problem. We then propose an
algorithm named HG-MADDPG, which combines the Hungarian algorithm with a
generative diffusion model (GDM)-based multi-agent deep deterministic policy
gradient (MADDPG) approach. We first introduce the Hungarian algorithm as a
method for exploration area selection, enhancing UAV efficiency in interacting
with the environment. We then innovatively integrate the GDM and multi-agent
deep deterministic policy gradient (MADDPG) to optimize task assignment
decisions, such as task offloading and resource allocation. Simulation results
demonstrate the effectiveness of the proposed approach, with significant
improvements in task offloading efficiency, latency reduction, and system
stability compared to baseline methods.

</details>

### [130] [Multi-modal Knowledge Graph Generation with Semantics-enriched Prompts](https://arxiv.org/abs/2504.13631)
*Yajing Xu,Zhiqiang Liu,Jiaoyan Chen,Mingchen Tu,Zhuo Chen,Jeff Z. Pan,Yichi Zhang,Yushan Zhu,Wen Zhang,Huajun Chen*

Main category: cs.AI

TLDR: 提出了一种从传统知识图谱构建多模态知识图谱（MMKG）的框架，并设计了VSNS方法以选择高质量、上下文相关的图像。


<details>
  <summary>Details</summary>
Motivation: 现有MMKG数量不足且构建困难，尤其是选择高质量、上下文相关的图像。

Method: 设计了VSNS方法，包含VNS和SNS模块，分别过滤难以可视化的关系和选择最能捕捉实体结构特征的邻居。

Result: 在MKG-Y和DB15K数据集上的实验表明，VSNS方法生成的图像质量更高且与知识图谱更相关。

Conclusion: VSNS方法有效提升了MMKG构建中图像的质量和相关性。

Abstract: Multi-modal Knowledge Graphs (MMKGs) have been widely applied across various
domains for knowledge representation. However, the existing MMKGs are
significantly fewer than required, and their construction faces numerous
challenges, particularly in ensuring the selection of high-quality,
contextually relevant images for knowledge graph enrichment. To address these
challenges, we present a framework for constructing MMKGs from conventional
KGs. Furthermore, to generate higher-quality images that are more relevant to
the context in the given knowledge graph, we designed a neighbor selection
method called Visualizable Structural Neighbor Selection (VSNS). This method
consists of two modules: Visualizable Neighbor Selection (VNS) and Structural
Neighbor Selection (SNS). The VNS module filters relations that are difficult
to visualize, while the SNS module selects neighbors that most effectively
capture the structural characteristics of the entity. To evaluate the quality
of the generated images, we performed qualitative and quantitative evaluations
on two datasets, MKG-Y and DB15K. The experimental results indicate that using
the VSNS method to select neighbors results in higher-quality images that are
more relevant to the knowledge graph.

</details>

### [131] [Exploring the Potential for Large Language Models to Demonstrate Rational Probabilistic Beliefs](https://arxiv.org/abs/2504.13644)
*Gabriel Freedman,Francesca Toni*

Main category: cs.AI

TLDR: 研究发现当前大型语言模型（LLMs）在表示概率信念时缺乏理性和一致性，无法满足概率推理的基本要求。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs在概率推理中的表现，确保其在信息检索和自动决策系统中的可信度和可解释性。

Method: 引入一个包含不确定真相值声明的新数据集，并应用多种不确定性量化技术评估LLMs的表现。

Result: 当前LLMs无法提供理性且一致的概率信念表示。

Conclusion: LLMs在概率推理能力上仍需改进，以满足实际应用的需求。

Abstract: Advances in the general capabilities of large language models (LLMs) have led
to their use for information retrieval, and as components in automated decision
systems. A faithful representation of probabilistic reasoning in these models
may be essential to ensure trustworthy, explainable and effective performance
in these tasks. Despite previous work suggesting that LLMs can perform complex
reasoning and well-calibrated uncertainty quantification, we find that current
versions of this class of model lack the ability to provide rational and
coherent representations of probabilistic beliefs. To demonstrate this, we
introduce a novel dataset of claims with indeterminate truth values and apply a
number of well-established techniques for uncertainty quantification to measure
the ability of LLM's to adhere to fundamental properties of probabilistic
reasoning.

</details>

### [132] [OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation](https://arxiv.org/abs/2504.13707)
*Yichen Wu,Xudong Pan,Geng Hong,Min Yang*

Main category: cs.AI

TLDR: OpenDeception框架用于评估LLM代理的欺骗意图和能力，发现主流LLM的欺骗意图比例超过80%，成功率超50%，且能力越强的模型欺骗风险越高。


<details>
  <summary>Details</summary>
Motivation: 随着LLM能力提升和代理应用普及，欺骗风险亟需系统评估和有效监管。

Method: 提出OpenDeception框架，通过开放场景数据集和多轮对话模拟，评估LLM的欺骗意图和能力。

Result: 主流LLM的欺骗意图比例超80%，成功率超50%，能力强的模型欺骗风险更高。

Conclusion: 需加强对LLM欺骗行为的抑制和安全性对齐。

Abstract: As the general capabilities of large language models (LLMs) improve and agent
applications become more widespread, the underlying deception risks urgently
require systematic evaluation and effective oversight. Unlike existing
evaluation which uses simulated games or presents limited choices, we introduce
OpenDeception, a novel deception evaluation framework with an open-ended
scenario dataset. OpenDeception jointly evaluates both the deception intention
and capabilities of LLM-based agents by inspecting their internal reasoning
process. Specifically, we construct five types of common use cases where LLMs
intensively interact with the user, each consisting of ten diverse, concrete
scenarios from the real world. To avoid ethical concerns and costs of high-risk
deceptive interactions with human testers, we propose to simulate the
multi-turn dialogue via agent simulation. Extensive evaluation of eleven
mainstream LLMs on OpenDeception highlights the urgent need to address
deception risks and security concerns in LLM-based agents: the deception
intention ratio across the models exceeds 80%, while the deception success rate
surpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do
exhibit a higher risk of deception, which calls for more alignment efforts on
inhibiting deceptive behaviors.

</details>

### [133] [Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://arxiv.org/abs/2504.13837)
*Yang Yue,Zhiqi Chen,Rui Lu,Andrew Zhao,Zhaokai Wang,Yang Yue,Shiji Song,Gao Huang*

Main category: cs.AI

TLDR: RLVR（可验证奖励的强化学习）被认为能提升LLMs的推理能力，但研究发现其并未带来新的推理模式，而是通过偏向高奖励路径提升效率。


<details>
  <summary>Details</summary>
Motivation: 重新审视RLVR是否真的能赋予LLMs新的推理能力，而非仅优化已有能力。

Method: 通过测量大k值的pass@k指标，比较RL训练模型与基础模型在不同任务中的表现。

Result: RL训练模型在小k值表现更好，但大k值下基础模型表现相当或更优，表明RL未引入新推理能力。

Conclusion: RLVR的局限性在于未能扩展推理能力边界，需重新思考其在LLMs中的作用。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently
demonstrated notable success in enhancing the reasoning capabilities of LLMs,
particularly in mathematics and programming tasks. It is widely believed that
RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning
abilities that exceed corresponding base models' capacity. In this study,
however, we critically re-examines this assumption by measuring the
pass@\textit{k} metric with large values of \textit{k} to explore the reasoning
capability boundary of the models across a wide range of model families and
benchmarks. Surprisingly, the RL does \emph{not}, in fact, elicit fundamentally
new reasoning patterns. While RL-trained models outperform their base models at
smaller values of $k$ (\eg, $k$=1), base models can achieve a comparable or
even higher pass@$k$ score compared to their RL counterparts at large $k$
values. The reasoning paths generated by RL-trained models are already included
in the base models' sampling distribution, suggesting that most reasoning
abilities manifested in RL-trained models are already obtained by base models.
Further analysis shows that RL training boosts the performance by biasing the
model's output distribution toward paths that are more likely to yield rewards,
therefore sampling correct responses more efficiently. But this also results in
a narrower reasoning capability boundary compared to base models. Similar
results are observed in visual reasoning tasks trained with RLVR. Moreover, we
find that distillation can genuinely introduce new knowledge into the model,
different from RLVR. These findings underscore a critical limitation of RLVR in
advancing LLM reasoning abilities which requires us to fundamentally rethink
the impact of RL training in reasoning LLMs and the need of a better paradigm.
Project Page: https://limit-of-RLVR.github.io

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [134] [Universal Representations for Classification-enhanced Lossy Compression](https://arxiv.org/abs/2504.13191)
*Nam Nguyen*

Main category: cs.CV

TLDR: 论文探讨了在损失压缩中，通过单一编码器实现多种解码目标的通用表示方法，验证了其在感知图像压缩任务中的有效性，但在特定分类-失真权衡下存在性能损失。


<details>
  <summary>Details</summary>
Motivation: 传统压缩算法仅关注压缩率与重建失真的权衡，而近年研究引入了感知质量和分类准确性作为额外评估维度。本文旨在开发通用编码器，避免为每个特定权衡点重新训练编码器。

Method: 提出通用表示方法，使用单一编码器适应多种失真和分类（或感知）约束，并在MNIST数据集上进行实验验证。

Result: 实验表明，通用编码器在感知图像压缩任务中性能接近单独优化的编码器，但在分类-失真权衡下，复用编码器会导致显著的失真损失。

Conclusion: 通用编码器在感知压缩任务中表现良好，但在分类-失真权衡下需谨慎复用，以避免性能下降。

Abstract: In lossy compression, the classical tradeoff between compression rate and
reconstruction distortion has traditionally guided algorithm design. However,
Blau and Michaeli [5] introduced a generalized framework, known as the
rate-distortion-perception (RDP) function, incorporating perceptual quality as
an additional dimension of evaluation. More recently, the
rate-distortion-classification (RDC) function was investigated in [19],
evaluating compression performance by considering classification accuracy
alongside distortion. In this paper, we explore universal representations,
where a single encoder is developed to achieve multiple decoding objectives
across various distortion and classification (or perception) constraints. This
universality avoids retraining encoders for each specific operating point
within these tradeoffs. Our experimental validation on the MNIST dataset
indicates that a universal encoder incurs only minimal performance degradation
compared to individually optimized encoders for perceptual image compression
tasks, aligning with prior results from [23]. Nonetheless, we also identify
that in the RDC setting, reusing an encoder optimized for one specific
classification-distortion tradeoff leads to a significant distortion penalty
when applied to alternative points.

</details>

### [135] [Intelligent road crack detection and analysis based on improved YOLOv8](https://arxiv.org/abs/2504.13208)
*Haomin Zuo,Zhengyang Li,Jiangchuan Gong,Zhen Tian*

Main category: cs.CV

TLDR: 提出了一种基于改进YOLOv8的智能道路裂缝检测系统，通过训练4029张图像实现高效识别与分割，并结合ECA和CBAM注意力机制提升精度。


<details>
  <summary>Details</summary>
Motivation: 城市化加速导致路面损坏问题突出，传统人工检测效率低且成本高，亟需智能解决方案。

Method: 采用改进的YOLOv8框架，结合ECA和CBAM注意力机制，训练目标分割模型识别裂缝区域并分析其宽度和位置。

Result: 实验表明，模型能高效准确地检测裂缝，注意力机制显著提升检测精度。

Conclusion: 该系统为道路维护和安全监测提供了新方案。

Abstract: As urbanization speeds up and traffic flow increases, the issue of pavement
distress is becoming increasingly pronounced, posing a severe threat to road
safety and service life. Traditional methods of pothole detection rely on
manual inspection, which is not only inefficient but also costly. This paper
proposes an intelligent road crack detection and analysis system, based on the
enhanced YOLOv8 deep learning framework. A target segmentation model has been
developed through the training of 4029 images, capable of efficiently and
accurately recognizing and segmenting crack regions in roads. The model also
analyzes the segmented regions to precisely calculate the maximum and minimum
widths of cracks and their exact locations. Experimental results indicate that
the incorporation of ECA and CBAM attention mechanisms substantially enhances
the model's detection accuracy and efficiency, offering a novel solution for
road maintenance and safety monitoring.

</details>

### [136] [Mirror: Multimodal Cognitive Reframing Therapy for Rolling with Resistance](https://arxiv.org/abs/2504.13211)
*Subin Kim,Hoonrae Kim,Jihyun Lee,Yejin Jeon,Gary Geunbae Lee*

Main category: cs.CV

TLDR: 提出了一种结合非语言线索的多模态方法，提升AI治疗师处理客户抵抗的能力，效果优于现有文本CBT方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的CBT模型在处理客户抵抗时效果不佳，影响治疗联盟。

Method: 引入多模态数据集Mirror，训练Vision-Language模型分析面部线索并生成共情回应。

Result: Mirror显著提升AI治疗师处理抵抗的能力，优于纯文本方法。

Conclusion: 多模态方法能有效增强AI治疗师在心理治疗中的表现。

Abstract: Recent studies have explored the use of large language models (LLMs) in
psychotherapy; however, text-based cognitive behavioral therapy (CBT) models
often struggle with client resistance, which can weaken therapeutic alliance.
To address this, we propose a multimodal approach that incorporates nonverbal
cues, allowing the AI therapist to better align its responses with the client's
negative emotional state. Specifically, we introduce a new synthetic dataset,
Multimodal Interactive Rolling with Resistance (Mirror), which is a novel
synthetic dataset that pairs client statements with corresponding facial
images. Using this dataset, we train baseline Vision-Language Models (VLMs)
that can analyze facial cues, infer emotions, and generate empathetic responses
to effectively manage resistance. They are then evaluated in terms of both the
therapist's counseling skills and the strength of the therapeutic alliance in
the presence of client resistance. Our results demonstrate that Mirror
significantly enhances the AI therapist's ability to handle resistance, which
outperforms existing text-based CBT approaches.

</details>

### [137] [Wavelet-based Variational Autoencoders for High-Resolution Image Generation](https://arxiv.org/abs/2504.13214)
*Andrew Kiruluta*

Main category: cs.CV

TLDR: Wavelet-VAE通过多尺度Haar小波系数构建潜在空间，提升图像生成清晰度，优于传统VAE。


<details>
  <summary>Details</summary>
Motivation: 传统VAE因各向同性高斯潜在空间假设和高频细节捕捉不足，生成图像模糊。

Method: 提出基于小波的多尺度细节和近似系数编码方法，引入可学习噪声参数，并重新参数化KL散度和训练目标。

Result: 在CIFAR-10等数据集上，Wavelet-VAE显著提升视觉保真度和高分辨率细节恢复能力。

Conclusion: Wavelet-VAE具有优势，但也存在潜在限制，未来可进一步研究小波生成模型。

Abstract: Variational Autoencoders (VAEs) are powerful generative models capable of
learning compact latent representations. However, conventional VAEs often
generate relatively blurry images due to their assumption of an isotropic
Gaussian latent space and constraints in capturing high-frequency details. In
this paper, we explore a novel wavelet-based approach (Wavelet-VAE) in which
the latent space is constructed using multi-scale Haar wavelet coefficients. We
propose a comprehensive method to encode the image features into multi-scale
detail and approximation coefficients and introduce a learnable noise parameter
to maintain stochasticity. We thoroughly discuss how to reformulate the
reparameterization trick, address the KL divergence term, and integrate wavelet
sparsity principles into the training objective. Our experimental evaluation on
CIFAR-10 and other high-resolution datasets demonstrates that the Wavelet-VAE
improves visual fidelity and recovers higher-resolution details compared to
conventional VAEs. We conclude with a discussion of advantages, potential
limitations, and future research directions for wavelet-based generative
modeling.

</details>

### [138] [SSTAF: Spatial-Spectral-Temporal Attention Fusion Transformer for Motor Imagery Classification](https://arxiv.org/abs/2504.13220)
*Ummay Maria Muna,Md. Mehedi Hasan Shawon,Md Jobayer,Sumaiya Akter,Saifur Rahman Sabuj*

Main category: cs.CV

TLDR: 本文提出了一种新颖的SSTAF Transformer模型，用于解决EEG信号的非平稳性和跨被试变异性问题，在运动想象分类任务中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: EEG信号的非平稳性和被试间变异性对跨被试分类模型提出了挑战，需要一种更鲁棒的方法。

Method: 设计了结合空间、频谱和时间注意力机制的SSTAF Transformer，通过短时傅里叶变换提取时频特征。

Result: 在两个公开数据集上分别达到76.83%和68.30%的准确率，优于传统CNN和现有Transformer方法。

Conclusion: SSTAF Transformer在运动想象分类任务中表现出色，为神经康复和辅助技术提供了有效解决方案。

Abstract: Brain-computer interfaces (BCI) in electroencephalography (EEG)-based motor
imagery classification offer promising solutions in neurorehabilitation and
assistive technologies by enabling communication between the brain and external
devices. However, the non-stationary nature of EEG signals and significant
inter-subject variability cause substantial challenges for developing robust
cross-subject classification models. This paper introduces a novel
Spatial-Spectral-Temporal Attention Fusion (SSTAF) Transformer specifically
designed for upper-limb motor imagery classification. Our architecture consists
of a spectral transformer and a spatial transformer, followed by a transformer
block and a classifier network. Each module is integrated with attention
mechanisms that dynamically attend to the most discriminative patterns across
multiple domains, such as spectral frequencies, spatial electrode locations,
and temporal dynamics. The short-time Fourier transform is incorporated to
extract features in the time-frequency domain to make it easier for the model
to obtain a better feature distinction. We evaluated our SSTAF Transformer
model on two publicly available datasets, the EEGMMIDB dataset, and BCI
Competition IV-2a. SSTAF Transformer achieves an accuracy of 76.83% and 68.30%
in the data sets, respectively, outperforms traditional CNN-based architectures
and a few existing transformer-based approaches.

</details>

### [139] [ICAS: IP Adapter and ControlNet-based Attention Structure for Multi-Subject Style Transfer Optimization](https://arxiv.org/abs/2504.13224)
*Fuwei Liu*

Main category: cs.CV

TLDR: ICAS提出了一种基于IP-Adapter和ControlNet的高效可控多主体风格迁移框架，解决了现有方法在计算成本和语义保真度上的不足。


<details>
  <summary>Details</summary>
Motivation: 多主体风格迁移面临风格属性定义模糊和跨主体一致性应用的挑战，现有方法依赖高成本的反转过程或大规模风格数据集。

Method: ICAS通过自适应微调预训练扩散模型的内容注入分支，结合IP-Adapter和ControlNet，实现高效风格注入和结构控制。

Result: 实验表明ICAS在结构保留、风格一致性和推理效率上表现优异。

Conclusion: ICAS为多主体风格迁移提供了一种高效可控的新范式。

Abstract: Generating multi-subject stylized images remains a significant challenge due
to the ambiguity in defining style attributes (e.g., color, texture,
atmosphere, and structure) and the difficulty in consistently applying them
across multiple subjects. Although recent diffusion-based text-to-image models
have achieved remarkable progress, existing methods typically rely on
computationally expensive inversion procedures or large-scale stylized
datasets. Moreover, these methods often struggle with maintaining multi-subject
semantic fidelity and are limited by high inference costs. To address these
limitations, we propose ICAS (IP-Adapter and ControlNet-based Attention
Structure), a novel framework for efficient and controllable multi-subject
style transfer. Instead of full-model tuning, ICAS adaptively fine-tunes only
the content injection branch of a pre-trained diffusion model, thereby
preserving identity-specific semantics while enhancing style controllability.
By combining IP-Adapter for adaptive style injection with ControlNet for
structural conditioning, our framework ensures faithful global layout
preservation alongside accurate local style synthesis. Furthermore, ICAS
introduces a cyclic multi-subject content embedding mechanism, which enables
effective style transfer under limited-data settings without the need for
extensive stylized corpora. Extensive experiments show that ICAS achieves
superior performance in structure preservation, style consistency, and
inference efficiency, establishing a new paradigm for multi-subject style
transfer in real-world applications.

</details>

### [140] [WildFireCan-MMD: A Multimodal dataset for Classification of User-generated Content During Wildfires in Canada](https://arxiv.org/abs/2504.13231)
*Braeden Sherritt,Isar Nejadgholi,Marzieh Amini*

Main category: cs.CV

TLDR: WildFireCan-MMD是一个新的多模态数据集，用于从加拿大野火的X帖子中提取关键信息，研究发现定制训练模型优于零样本提示方法。


<details>
  <summary>Details</summary>
Motivation: 传统数据源在野火期间信息获取慢且成本高，社交媒体提供实时更新，但提取相关洞察仍具挑战性。

Method: 提出WildFireCan-MMD数据集，评估视觉语言模型和定制训练分类器，比较零样本提示与训练模型的表现。

Result: 训练模型在标注数据可用时表现优于零样本提示方法，最高提升23%。

Conclusion: 定制数据集和任务特定训练至关重要，且数据集需本地化以适应不同地区的灾害响应需求。

Abstract: Rapid information access is vital during wildfires, yet traditional data
sources are slow and costly. Social media offers real-time updates, but
extracting relevant insights remains a challenge. We present WildFireCan-MMD, a
new multimodal dataset of X posts from recent Canadian wildfires, annotated
across 13 key themes. Evaluating both Vision Language Models and custom-trained
classifiers, we show that while zero-shot prompting offers quick deployment,
even simple trained models outperform them when labelled data is available, by
up to 23%. Our findings highlight the enduring importance of tailored datasets
and task-specific training. Importantly, such datasets should be localized, as
disaster response requirements vary across regions and contexts.

</details>

### [141] [Dynamic Memory-enhanced Transformer for Hyperspectral Image Classification](https://arxiv.org/abs/2504.13242)
*Muhammad Ahmad,Manuel Mazzara,Salvatore Distefano,Adil Mehmood Khan*

Main category: cs.CV

TLDR: 提出了一种轻量级且内存增强的Transformer模型MemFormer，通过动态内存模块和多头注意力机制优化HSI分类任务。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在HSI分类中存在信息冗余和注意力效率低的问题，难以捕捉细粒度关系。

Method: 引入内存增强的多头注意力机制和动态内存丰富策略，结合空间-光谱位置编码（SSPE）。

Result: 在基准数据集上表现优异，分类精度超越现有方法。

Conclusion: MemFormer通过动态内存和SSPE显著提升了HSI分类性能。

Abstract: Hyperspectral image (HSI) classification remains a challenging task due to
the intricate spatial-spectral correlations. Existing transformer models excel
in capturing long-range dependencies but often suffer from information
redundancy and attention inefficiencies, limiting their ability to model
fine-grained relationships crucial for HSI classification. To overcome these
limitations, this work proposes MemFormer, a lightweight and memory-enhanced
transformer. MemFormer introduces a memory-enhanced multi-head attention
mechanism that iteratively refines a dynamic memory module, enhancing feature
extraction while reducing redundancy across layers. Additionally, a dynamic
memory enrichment strategy progressively captures complex spatial and spectral
dependencies, leading to more expressive feature representations. To further
improve structural consistency, we incorporate a spatial-spectral positional
encoding (SSPE) tailored for HSI data, ensuring continuity without the
computational burden of convolution-based approaches. Extensive experiments on
benchmark datasets demonstrate that MemFormer achieves superior classification
accuracy, outperforming state-of-the-art methods.

</details>

### [142] [ChartQA-X: Generating Explanations for Charts](https://arxiv.org/abs/2504.13275)
*Shamanthak Hegde,Pooyan Fazli,Hasti Seifi*

Main category: cs.CV

TLDR: ChartQA-X数据集提供图表问答及解释，通过多模型生成最佳解释，提升智能代理传达复杂信息的能力。


<details>
  <summary>Details</summary>
Motivation: 解决图表图像问答中解释生成的挑战，增强用户理解和信任。

Method: 构建包含28,299个问题的数据集，通过六种模型生成解释并筛选最佳。

Result: 微调模型在解释生成和问答任务中表现优异。

Conclusion: 结合答案与解释的方法有效提升信息传达和用户信任。

Abstract: The ability to interpret and explain complex information from visual data in
charts is crucial for data-driven decision-making. In this work, we address the
challenge of providing explanations alongside answering questions about chart
images. We present ChartQA-X, a comprehensive dataset comprising various chart
types with 28,299 contextually relevant questions, answers, and detailed
explanations. These explanations are generated by prompting six different
models and selecting the best responses based on metrics such as faithfulness,
informativeness, coherence, and perplexity. Our experiments show that models
fine-tuned on our dataset for explanation generation achieve superior
performance across various metrics and demonstrate improved accuracy in
question-answering tasks on new datasets. By integrating answers with
explanatory narratives, our approach enhances the ability of intelligent agents
to convey complex information effectively, improve user understanding, and
foster trust in the generated responses.

</details>

### [143] [LIFT+: Lightweight Fine-Tuning for Long-Tail Learning](https://arxiv.org/abs/2504.13282)
*Jiang-Xin Shi,Tong Wei,Yu-Feng Li*

Main category: cs.CV

TLDR: 论文提出LIFT+框架，通过轻量级微调优化长尾学习任务，显著提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法在长尾学习中存在性能下降问题，尤其是对尾部类别的影响未被充分研究。

Method: 提出LIFT+框架，结合语义感知初始化、极简数据增强和测试时集成，优化一致类别条件。

Result: LIFT+显著减少训练轮次（从100降至15）和参数（<1%），性能超越现有方法。

Conclusion: LIFT+为长尾学习提供了高效准确的解决方案，推动了基础模型的适应性。

Abstract: The fine-tuning paradigm has emerged as a prominent approach for addressing
long-tail learning tasks in the era of foundation models. However, the impact
of fine-tuning strategies on long-tail learning performance remains unexplored.
In this work, we disclose that existing paradigms exhibit a profound misuse of
fine-tuning methods, leaving significant room for improvement in both
efficiency and accuracy. Specifically, we reveal that heavy fine-tuning
(fine-tuning a large proportion of model parameters) can lead to non-negligible
performance deterioration on tail classes, whereas lightweight fine-tuning
demonstrates superior effectiveness. Through comprehensive theoretical and
empirical validation, we identify this phenomenon as stemming from inconsistent
class conditional distributions induced by heavy fine-tuning. Building on this
insight, we propose LIFT+, an innovative lightweight fine-tuning framework to
optimize consistent class conditions. Furthermore, LIFT+ incorporates
semantic-aware initialization, minimalist data augmentation, and test-time
ensembling to enhance adaptation and generalization of foundation models. Our
framework provides an efficient and accurate pipeline that facilitates fast
convergence and model compactness. Extensive experiments demonstrate that LIFT+
significantly reduces both training epochs (from $\sim$100 to $\leq$15) and
learned parameters (less than 1%), while surpassing state-of-the-art approaches
by a considerable margin. The source code is available at
https://github.com/shijxcs/LIFT-plus.

</details>

### [144] [Weak Cube R-CNN: Weakly Supervised 3D Detection using only 2D Bounding Boxes](https://arxiv.org/abs/2504.13297)
*Andreas Lau Hansen,Lukas Wanzeck,Dim P. Papadopoulos*

Main category: cs.CV

TLDR: 提出了一种弱监督的单目3D物体检测方法Weak Cube R-CNN，仅需2D标注即可训练，通过利用预训练的2D模型估计深度和方向信息作为伪真值。


<details>
  <summary>Details</summary>
Motivation: 减少对昂贵3D标注数据的依赖，利用单目相机系统替代LiDAR或多相机系统。

Method: 利用预训练的2D模型估计深度和方向信息作为伪真值，设计损失函数避免3D标注。

Result: 在SUN RGB-D数据集上表现优于基线Cube R-CNN，但精度不足以厘米级测量。

Conclusion: 为弱监督3D检测提供了可行方案，适合进一步研究。

Abstract: Monocular 3D object detection is an essential task in computer vision, and it
has several applications in robotics and virtual reality. However, 3D object
detectors are typically trained in a fully supervised way, relying extensively
on 3D labeled data, which is labor-intensive and costly to annotate. This work
focuses on weakly-supervised 3D detection to reduce data needs using a
monocular method that leverages a singlecamera system over expensive LiDAR
sensors or multi-camera setups. We propose a general model Weak Cube R-CNN,
which can predict objects in 3D at inference time, requiring only 2D box
annotations for training by exploiting the relationship between 2D projections
of 3D cubes. Our proposed method utilizes pre-trained frozen foundation 2D
models to estimate depth and orientation information on a training set. We use
these estimated values as pseudo-ground truths during training. We design loss
functions that avoid 3D labels by incorporating information from the external
models into the loss. In this way, we aim to implicitly transfer knowledge from
these large foundation 2D models without having access to 3D bounding box
annotations. Experimental results on the SUN RGB-D dataset show increased
performance in accuracy compared to an annotation time equalized Cube R-CNN
baseline. While not precise for centimetre-level measurements, this method
provides a strong foundation for further research.

</details>

### [145] [SAR Object Detection with Self-Supervised Pretraining and Curriculum-Aware Sampling](https://arxiv.org/abs/2504.13310)
*Yasin Almalioglu,Andrzej Kucik,Geoffrey French,Dafni Antotsiou,Alexander Adam,Cedric Archambeau*

Main category: cs.CV

TLDR: 论文提出了一种名为TRANSAR的自监督端到端视觉Transformer模型，用于卫星SAR图像中的目标检测，通过掩码图像预训练和自适应采样调度器解决了小目标检测和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 卫星SAR图像在目标检测中潜力巨大，但数据复杂性和标注稀缺性限制了发展，尤其是小目标检测和缺乏大规模标注数据集的问题。

Method: TRANSAR结合掩码图像预训练和辅助二进制语义分割，并引入自适应采样调度器动态调整训练中的目标类别分布。

Result: 在基准SAR数据集上，TRANSAR表现优于传统监督模型（如DeepLabv3、UNet）和自监督模型（如DPT、SegFormer、UperNet）。

Conclusion: TRANSAR通过自监督学习和动态采样策略，显著提升了SAR图像中的目标检测性能，尤其是对小目标的检测效果。

Abstract: Object detection in satellite-borne Synthetic Aperture Radar (SAR) imagery
holds immense potential in tasks such as urban monitoring and disaster
response. However, the inherent complexities of SAR data and the scarcity of
annotations present significant challenges in the advancement of object
detection in this domain. Notably, the detection of small objects in
satellite-borne SAR images poses a particularly intricate problem, because of
the technology's relatively low spatial resolution and inherent noise.
Furthermore, the lack of large labelled SAR datasets hinders the development of
supervised deep learning-based object detection models. In this paper, we
introduce TRANSAR, a novel self-supervised end-to-end vision transformer-based
SAR object detection model that incorporates masked image pre-training on an
unlabeled SAR image dataset that spans more than $25,700$ km\textsuperscript{2}
ground area. Unlike traditional object detection formulation, our approach
capitalises on auxiliary binary semantic segmentation, designed to segregate
objects of interest during the post-tuning, especially the smaller ones, from
the background. In addition, to address the innate class imbalance due to the
disproportion of the object to the image size, we introduce an adaptive
sampling scheduler that dynamically adjusts the target class distribution
during training based on curriculum learning and model feedback. This approach
allows us to outperform conventional supervised architecture such as DeepLabv3
or UNet, and state-of-the-art self-supervised learning-based arhitectures such
as DPT, SegFormer or UperNet, as shown by extensive evaluations on benchmark
SAR datasets.

</details>

### [146] [VLLFL: A Vision-Language Model Based Lightweight Federated Learning Framework for Smart Agriculture](https://arxiv.org/abs/2504.13365)
*Long Li,Jiajia Li,Dong Chen,Lina Pu,Haibo Yao,Yanbo Huang*

Main category: cs.CV

TLDR: VLLFL是一个基于视觉语言模型的轻量级联邦学习框架，旨在解决农业中目标检测的数据隐私和通信开销问题，同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代智能农业中，目标检测对自动化和精准农业至关重要，但大规模数据收集和隐私问题成为挑战。

Method: VLLFL结合视觉语言模型的泛化能力和联邦学习的隐私保护特性，通过训练紧凑的提示生成器提升性能。

Result: 实验显示，VLLFL将视觉语言模型性能提升14.53%，同时减少99.3%的通信开销。

Conclusion: VLLFL为农业应用提供了一种高效、可扩展且隐私保护的解决方案。

Abstract: In modern smart agriculture, object detection plays a crucial role by
enabling automation, precision farming, and monitoring of resources. From
identifying crop health and pest infestations to optimizing harvesting
processes, accurate object detection enhances both productivity and
sustainability. However, training object detection models often requires
large-scale data collection and raises privacy concerns, particularly when
sensitive agricultural data is distributed across farms. To address these
challenges, we propose VLLFL, a vision-language model-based lightweight
federated learning framework (VLLFL). It harnesses the generalization and
context-aware detection capabilities of the vision-language model (VLM) and
leverages the privacy-preserving nature of federated learning. By training a
compact prompt generator to boost the performance of the VLM deployed across
different farms, VLLFL preserves privacy while reducing communication overhead.
Experimental results demonstrate that VLLFL achieves 14.53% improvement in the
performance of VLM while reducing 99.3% communication overhead. Spanning tasks
from identifying a wide variety of fruits to detecting harmful animals in
agriculture, the proposed framework offers an efficient, scalable, and
privacy-preserving solution specifically tailored to agricultural applications.

</details>

### [147] [POET: Supporting Prompting Creativity and Personalization with Automated Expansion of Text-to-Image Generation](https://arxiv.org/abs/2504.13392)
*Evans Xu Han,Alice Qian Zhang,Hong Shen,Haiyi Zhu,Paul Pu Liang,Jane Hsieh*

Main category: cs.CV

TLDR: POET是一个实时交互工具，通过自动发现文本到图像生成模型的同质性维度、扩展这些维度以多样化输出，并根据用户反馈个性化扩展，提升创意任务的多样性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模文本到图像系统设计广泛但输出常规，限制了创意探索，且交互方式对初学者不友好，需要更多变体和个性化。

Method: POET自动发现生成模型的同质性维度，扩展这些维度以多样化图像输出，并通过用户反馈学习个性化扩展。

Result: 28名用户的评估显示，POET能生成更高多样性的结果，帮助用户在更少提示下达到满意，促进更广泛的创意探索。

Conclusion: POET展示了未来文本到图像生成工具如何通过交互技术支持多元价值和用户需求，提升创意工作初期的探索效果。

Abstract: State-of-the-art visual generative AI tools hold immense potential to assist
users in the early ideation stages of creative tasks -- offering the ability to
generate (rather than search for) novel and unprecedented (instead of existing)
images of considerable quality that also adhere to boundless combinations of
user specifications. However, many large-scale text-to-image systems are
designed for broad applicability, yielding conventional output that may limit
creative exploration. They also employ interaction methods that may be
difficult for beginners. Given that creative end users often operate in
diverse, context-specific ways that are often unpredictable, more variation and
personalization are necessary. We introduce POET, a real-time interactive tool
that (1) automatically discovers dimensions of homogeneity in text-to-image
generative models, (2) expands these dimensions to diversify the output space
of generated images, and (3) learns from user feedback to personalize
expansions. An evaluation with 28 users spanning four creative task domains
demonstrated POET's ability to generate results with higher perceived diversity
and help users reach satisfaction in fewer prompts during creative tasks,
thereby prompting them to deliberate and reflect more on a wider range of
possible produced results during the co-creative process. Focusing on visual
creativity, POET offers a first glimpse of how interaction techniques of future
text-to-image generation tools may support and align with more pluralistic
values and the needs of end users during the ideation stages of their work.

</details>

### [148] [BeetleVerse: A study on taxonomic classification of ground beetles](https://arxiv.org/abs/2504.13393)
*S M Rayeed,Alyson East,Samuel Stevens,Sydne Record,Charles V Stewart*

Main category: cs.CV

TLDR: 该论文评估了12种视觉模型在甲虫分类中的表现，发现结合Vision and Language Transformer与MLP头的模型效果最佳，并探讨了样本效率和领域适应问题。


<details>
  <summary>Details</summary>
Motivation: 甲虫是重要的生物多样性指标，但由于形态分类依赖专家且效率低，限制了其广泛应用。

Method: 评估12种视觉模型在四个长尾数据集上的分类表现，并研究样本效率和领域适应。

Result: 最佳模型在属和种级别分别达到97%和94%的准确率，样本效率可提升50%，但实验室到野外图像的领域适应存在挑战。

Conclusion: 研究为甲虫大规模自动分类奠定了基础，并推动了样本高效学习和跨领域适应技术的发展。

Abstract: Ground beetles are a highly sensitive and speciose biological indicator,
making them vital for monitoring biodiversity. However, they are currently an
underutilized resource due to the manual effort required by taxonomic experts
to perform challenging species differentiations based on subtle morphological
differences, precluding widespread applications. In this paper, we evaluate 12
vision models on taxonomic classification across four diverse, long-tailed
datasets spanning over 230 genera and 1769 species, with images ranging from
controlled laboratory settings to challenging field-collected (in-situ)
photographs. We further explore taxonomic classification in two important
real-world contexts: sample efficiency and domain adaptation. Our results show
that the Vision and Language Transformer combined with an MLP head is the best
performing model, with 97\% accuracy at genus and 94\% at species level. Sample
efficiency analysis shows that we can reduce train data requirements by up to
50\% with minimal compromise in performance. The domain adaptation experiments
reveal significant challenges when transferring models from lab to in-situ
images, highlighting a critical domain gap. Overall, our study lays a
foundation for large-scale automated taxonomic classification of beetles, and
beyond that, advances sample-efficient learning and cross-domain adaptation for
diverse long-tailed ecological datasets.

</details>

### [149] [Towards a Multi-Agent Vision-Language System for Zero-Shot Novel Hazardous Object Detection for Autonomous Driving Safety](https://arxiv.org/abs/2504.13399)
*Shashank Shriram,Srinivasa Perisetla,Aryan Keskar,Harsha Krishnaswamy,Tonko Emil Westerhof Bossen,Andreas Møgelmose,Ross Greer*

Main category: cs.CV

TLDR: 提出了一种多模态方法，结合视觉-语言推理和零样本目标检测，以提高自动驾驶中危险物体的识别和解释能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖预定义的对象类别，难以应对不可预测的异常危险。

Method: 整合Vision-Language Model (VLM)和Large Language Model (LLM)，利用CLIP模型优化目标检测，并通过扩展的COOOL数据集进行评估。

Result: 模型在危险检测和定位准确性上表现良好，同时发布了工具集以支持大规模数据集管理。

Conclusion: 该方法展示了视觉-语言模型的潜力，并指出了未来改进方向。

Abstract: Detecting anomalous hazards in visual data, particularly in video streams, is
a critical challenge in autonomous driving. Existing models often struggle with
unpredictable, out-of-label hazards due to their reliance on predefined object
categories. In this paper, we propose a multimodal approach that integrates
vision-language reasoning with zero-shot object detection to improve hazard
identification and explanation. Our pipeline consists of a Vision-Language
Model (VLM), a Large Language Model (LLM), in order to detect hazardous objects
within a traffic scene. We refine object detection by incorporating OpenAI's
CLIP model to match predicted hazards with bounding box annotations, improving
localization accuracy. To assess model performance, we create a ground truth
dataset by denoising and extending the foundational COOOL
(Challenge-of-Out-of-Label) anomaly detection benchmark dataset with complete
natural language descriptions for hazard annotations. We define a means of
hazard detection and labeling evaluation on the extended dataset using cosine
similarity. This evaluation considers the semantic similarity between the
predicted hazard description and the annotated ground truth for each video.
Additionally, we release a set of tools for structuring and managing
large-scale hazard detection datasets. Our findings highlight the strengths and
limitations of current vision-language-based approaches, offering insights into
future improvements in autonomous hazard detection systems. Our models,
scripts, and data can be found at https://github.com/mi3labucm/COOOLER.git

</details>

### [150] [CytoFM: The first cytology foundation model](https://arxiv.org/abs/2504.13402)
*Vedrana Ivezić,Ashwath Radhachandran,Ekaterina Redekop,Shreeram Athreya,Dongwoo Lee,Vivek Sant,Corey Arnold,William Speier*

Main category: cs.CV

TLDR: CytoFM是首个细胞学自监督基础模型，通过iBOT框架预训练，在多种细胞学任务中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 细胞学在癌症诊断中至关重要，但现有深度学习模型因样本异质性和数据稀缺难以通用化。

Method: 使用iBOT自监督ViT框架预训练CytoFM，结合掩码图像建模和自蒸馏技术。

Result: CytoFM在两项下游任务中优于基于组织病理学或自然图像的现有模型。

Conclusion: CytoFM展示了任务无关预训练在细胞学数据中的潜力，即使预训练数据量较小。

Abstract: Cytology is essential for cancer diagnostics and screening due to its
minimally invasive nature. However, the development of robust deep learning
models for digital cytology is challenging due to the heterogeneity in staining
and preparation methods of samples, differences across organs, and the limited
availability of large, diverse, annotated datasets. Developing a task-specific
model for every cytology application is impractical and non-cytology-specific
foundation models struggle to generalize to tasks in this domain where the
emphasis is on cell morphology. To address these challenges, we introduce
CytoFM, the first cytology self-supervised foundation model. Using iBOT, a
self-supervised Vision Transformer (ViT) training framework incorporating
masked image modeling and self-distillation, we pretrain CytoFM on a diverse
collection of cytology datasets to learn robust, transferable representations.
We evaluate CytoFM on multiple downstream cytology tasks, including breast
cancer classification and cell type identification, using an attention-based
multiple instance learning framework. Our results demonstrate that CytoFM
performs better on two out of three downstream tasks than existing foundation
models pretrained on histopathology (UNI) or natural images (iBOT-Imagenet).
Visualizations of learned representations demonstrate our model is able to
attend to cytologically relevant features. Despite a small pre-training
dataset, CytoFM's promising results highlight the ability of task-agnostic
pre-training approaches to learn robust and generalizable features from
cytology data.

</details>

### [151] [ProgRoCC: A Progressive Approach to Rough Crowd Counting](https://arxiv.org/abs/2504.13405)
*Shengqin Jiang,Linfei Li,Haokui Zhang,Qingshan Liu,Amin Beheshti,Jian Yang,Anton van den Hengel,Quan Z. Sheng,Yuankai Qi*

Main category: cs.CV

TLDR: 提出了一种基于CLIP的渐进式估计学习方法（ProgRoCC），用于粗略人群计数，仅需粗略标注而非传统的高成本逐目标标注。该方法通过粗到细的策略快速估计人数，并在半监督和弱监督任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的人群计数方法在人群规模增大时变得不可行且估计不可靠，而逐目标标注成本高昂。因此，提出粗略人群计数问题，以更易获取的训练数据实现更高准确性。

Method: 采用基于CLIP的渐进式估计学习策略（ProgRoCC），通过粗到细的方法确定对象数量。设计了视觉语言匹配适配器，优化模态匹配以细化视觉特征。

Result: 在三个广泛采用的人群计数数据集上，ProgRoCC表现优异，超越了半监督和弱监督任务中的现有技术。

Conclusion: ProgRoCC通过粗略标注和渐进式学习策略，实现了高效且准确的人群计数，为实际应用提供了可行的解决方案。

Abstract: As the number of individuals in a crowd grows, enumeration-based techniques
become increasingly infeasible and their estimates increasingly unreliable. We
propose instead an estimation-based version of the problem: we label Rough
Crowd Counting that delivers better accuracy on the basis of training data that
is easier to acquire. Rough crowd counting requires only rough annotations of
the number of targets in an image, instead of the more traditional, and far
more expensive, per-target annotations. We propose an approach to the rough
crowd counting problem based on CLIP, termed ProgRoCC. Specifically, we
introduce a progressive estimation learning strategy that determines the object
count through a coarse-to-fine approach. This approach delivers answers
quickly, outperforms the state-of-the-art in semi- and weakly-supervised crowd
counting. In addition, we design a vision-language matching adapter that
optimizes key-value pairs by mining effective matches of two modalities to
refine the visual features, thereby improving the final performance. Extensive
experimental results on three widely adopted crowd counting datasets
demonstrate the effectiveness of our method.

</details>

### [152] [LoRA-Based Continual Learning with Constraints on Critical Parameter Changes](https://arxiv.org/abs/2504.13407)
*Shimou Ling,Liang Zhang,Jiangwei Zhao,Lili Pan,Hongliang Li*

Main category: cs.CV

TLDR: 论文提出了一种基于LoRA的持续学习方法，通过冻结关键参数矩阵和使用正交LoRA组合（LoRAC）来减少遗忘，并在多个基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有正交LoRA调优方法中，预任务关键参数仍会显著变化，导致遗忘问题。

Method: 冻结预任务关键参数矩阵，并提出基于QR分解的正交LoRA组合（LoRAC）。

Result: 在Split CIFAR-100数据集上，准确率提升6.35%，遗忘率降低3.24%。

Conclusion: 该方法在持续学习任务中表现优异，显著优于现有方法。

Abstract: LoRA-based continual learning represents a promising avenue for leveraging
pre-trained models in downstream continual learning tasks. Recent studies have
shown that orthogonal LoRA tuning effectively mitigates forgetting. However,
this work unveils that under orthogonal LoRA tuning, the critical parameters
for pre-tasks still change notably after learning post-tasks. To address this
problem, we directly propose freezing the most critical parameter matrices in
the Vision Transformer (ViT) for pre-tasks before learning post-tasks. In
addition, building on orthogonal LoRA tuning, we propose orthogonal LoRA
composition (LoRAC) based on QR decomposition, which may further enhance the
plasticity of our method. Elaborate ablation studies and extensive comparisons
demonstrate the effectiveness of our proposed method. Our results indicate that
our method achieves state-of-the-art (SOTA) performance on several well-known
continual learning benchmarks. For instance, on the Split CIFAR-100 dataset,
our method shows a 6.35\% improvement in accuracy and a 3.24\% reduction in
forgetting compared to previous methods. Our code is available at
https://github.com/learninginvision/LoRAC-IPC.

</details>

### [153] [How Learnable Grids Recover Fine Detail in Low Dimensions: A Neural Tangent Kernel Analysis of Multigrid Parametric Encodings](https://arxiv.org/abs/2504.13412)
*Samuel Audia,Soheil Feizi,Matthias Zwicker,Dinesh Manocha*

Main category: cs.CV

TLDR: 论文比较了两种解决神经网络频谱偏差的技术：傅里叶特征编码（FFE）和多网格参数编码（MPE），发现MPE在细节学习和性能上优于FFE。


<details>
  <summary>Details</summary>
Motivation: 神经网络在低维空间映射中无法学习高频信息，需要技术改进。

Method: 使用神经切线核（NTK）分析FFE和MPE的性能差异，并通过实验验证。

Result: MPE在PSNR和MS-SSIM指标上显著优于FFE和基线方法。

Conclusion: MPE通过网格结构而非嵌入空间提升性能，是一种更优的解决方案。

Abstract: Neural networks that map between low dimensional spaces are ubiquitous in
computer graphics and scientific computing; however, in their naive
implementation, they are unable to learn high frequency information. We present
a comprehensive analysis comparing the two most common techniques for
mitigating this spectral bias: Fourier feature encodings (FFE) and multigrid
parametric encodings (MPE). FFEs are seen as the standard for low dimensional
mappings, but MPEs often outperform them and learn representations with higher
resolution and finer detail. FFE's roots in the Fourier transform, make it
susceptible to aliasing if pushed too far, while MPEs, which use a learned grid
structure, have no such limitation. To understand the difference in
performance, we use the neural tangent kernel (NTK) to evaluate these encodings
through the lens of an analogous kernel regression. By finding a lower bound on
the smallest eigenvalue of the NTK, we prove that MPEs improve a network's
performance through the structure of their grid and not their learnable
embedding. This mechanism is fundamentally different from FFEs, which rely
solely on their embedding space to improve performance. Results are empirically
validated on a 2D image regression task using images taken from 100 synonym
sets of ImageNet and 3D implicit surface regression on objects from the
Stanford graphics dataset. Using peak signal-to-noise ratio (PSNR) and
multiscale structural similarity (MS-SSIM) to evaluate how well fine details
are learned, we show that the MPE increases the minimum eigenvalue by 8 orders
of magnitude over the baseline and 2 orders of magnitude over the FFE. The
increase in spectrum corresponds to a 15 dB (PSNR) / 0.65 (MS-SSIM) increase
over baseline and a 12 dB (PSNR) / 0.33 (MS-SSIM) increase over the FFE.

</details>

### [154] [Mono3R: Exploiting Monocular Cues for Geometric 3D Reconstruction](https://arxiv.org/abs/2504.13419)
*Wenyu Li,Sidun Liu,Peng Qiao,Yong Dou*

Main category: cs.CV

TLDR: 论文提出了一种结合单目几何先验的多视图3D重建方法，以解决现有匹配方法在纹理弱或低光条件下的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于匹配的多视图3D重建方法在纹理弱或低光条件下性能显著下降，需要更鲁棒的解决方案。

Method: 引入单目引导的细化模块，将单目几何先验整合到多视图重建框架中。

Result: 实验表明，该方法在多视图相机姿态估计和点云精度上均有显著提升。

Conclusion: 结合单目几何先验的方法能有效提升多视图3D重建的鲁棒性和质量。

Abstract: Recent advances in data-driven geometric multi-view 3D reconstruction
foundation models (e.g., DUSt3R) have shown remarkable performance across
various 3D vision tasks, facilitated by the release of large-scale,
high-quality 3D datasets. However, as we observed, constrained by their
matching-based principles, the reconstruction quality of existing models
suffers significant degradation in challenging regions with limited matching
cues, particularly in weakly textured areas and low-light conditions. To
mitigate these limitations, we propose to harness the inherent robustness of
monocular geometry estimation to compensate for the inherent shortcomings of
matching-based methods. Specifically, we introduce a monocular-guided
refinement module that integrates monocular geometric priors into multi-view
reconstruction frameworks. This integration substantially enhances the
robustness of multi-view reconstruction systems, leading to high-quality
feed-forward reconstructions. Comprehensive experiments across multiple
benchmarks demonstrate that our method achieves substantial improvements in
both mutli-view camera pose estimation and point cloud accuracy.

</details>

### [155] [HSACNet: Hierarchical Scale-Aware Consistency Regularized Semi-Supervised Change Detection](https://arxiv.org/abs/2504.13428)
*Qi'ao Xu,Pengfei Wang,Yanjun Li,Tianwen Qian,Xiaoling Wang*

Main category: cs.CV

TLDR: HSACNet是一种半监督变化检测方法，通过结合SAM2的Hiera骨干网络和SADAM模块，有效利用多尺度特征和未标记数据，在复杂场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂场景中表现不佳，忽视多尺度特征且难以处理噪声数据。

Method: 结合SAM2的Hiera骨干网络提取特征，设计SADAM模块捕获多尺度变化特征，并采用双增强一致性正则化策略。

Result: 在四个基准测试中达到最优性能，参数和计算成本更低。

Conclusion: HSACNet在复杂场景中表现出色，为半监督变化检测提供了高效解决方案。

Abstract: Semi-supervised change detection (SSCD) aims to detect changes between
bi-temporal remote sensing images by utilizing limited labeled data and
abundant unlabeled data. Existing methods struggle in complex scenarios,
exhibiting poor performance when confronted with noisy data. They typically
neglect intra-layer multi-scale features while emphasizing inter-layer fusion,
harming the integrity of change objects with different scales. In this paper,
we propose HSACNet, a Hierarchical Scale-Aware Consistency regularized Network
for SSCD. Specifically, we integrate Segment Anything Model 2 (SAM2), using its
Hiera backbone as the encoder to extract inter-layer multi-scale features and
applying adapters for parameter-efficient fine-tuning. Moreover, we design a
Scale-Aware Differential Attention Module (SADAM) that can precisely capture
intra-layer multi-scale change features and suppress noise. Additionally, a
dual-augmentation consistency regularization strategy is adopted to effectively
utilize the unlabeled data. Extensive experiments across four CD benchmarks
demonstrate that our HSACNet achieves state-of-the-art performance, with
reduced parameters and computational cost.

</details>

### [156] [Circular Image Deturbulence using Quasi-conformal Geometry](https://arxiv.org/abs/2504.13432)
*Chu Chen,Han Zhang,Lok Ming Lui*

Main category: cs.CV

TLDR: 提出了一种名为CQCD的无监督框架，用于通过循环架构消除图像失真，确保几何准确性和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 光学传感器与物体之间的不均匀介质导致成像失真，而高质量配对标签图像的缺乏是图像恢复的关键挑战。

Method: 采用循环架构结合准共形几何理论，确保双射性和结构完整性，同时集成紧框架块编码失真敏感特征。

Result: 在合成和真实图像上的实验表明，CQCD在图像恢复质量和变形场估计准确性上优于现有方法。

Conclusion: CQCD框架在无监督图像恢复中表现出色，解决了失真问题并提供了高精度的变形场估计。

Abstract: The presence of inhomogeneous media between optical sensors and objects leads
to distorted imaging outputs, significantly complicating downstream
image-processing tasks. A key challenge in image restoration is the lack of
high-quality, paired-label images required for training supervised models. In
this paper, we introduce the Circular Quasi-Conformal Deturbulence (CQCD)
framework, an unsupervised approach for removing image distortions through a
circular architecture. This design ensures that the restored image remains both
geometrically accurate and visually faithful while preventing the accumulation
of incorrect estimations.The circular restoration process involves both forward
and inverse mapping. To ensure the bijectivity of the estimated non-rigid
deformations, computational quasi-conformal geometry theories are leveraged to
regularize the mapping, enforcing its homeomorphic properties. This guarantees
a well-defined transformation that preserves structural integrity and prevents
unwanted artifacts. Furthermore, tight-frame blocks are integrated to encode
distortion-sensitive features for precise recovery. To validate the performance
of our approach, we conduct evaluations on various synthetic and real-world
captured images. Experimental results demonstrate that CQCD not only
outperforms existing state-of-the-art deturbulence methods in terms of image
restoration quality but also provides highly accurate deformation field
estimations.

</details>

### [157] [Temporal Propagation of Asymmetric Feature Pyramid for Surgical Scene Segmentation](https://arxiv.org/abs/2504.13440)
*Cheng Yuan,Yutong Ban*

Main category: cs.CV

TLDR: 提出了一种双向注意力架构的时序非对称特征传播网络，用于解决手术场景分割中的静态图像和动态视频挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注空间特征提取，忽视了手术视频流中的时序依赖性。

Method: 设计了时序查询传播器和聚合非对称特征金字塔模块，增强帧特征表示并保留解剖结构和手术器械的判别性特征。

Result: 在两个公开基准测试中表现优异，EndoVis2018上mIoU提升16.4%，Endoscapes2023上mAP提升3.3%。

Conclusion: 该方法通过时序引导和上下文推理，显著提升了手术场景理解能力。

Abstract: Surgical scene segmentation is crucial for robot-assisted laparoscopic
surgery understanding. Current approaches face two challenges: (i) static image
limitations including ambiguous local feature similarities and fine-grained
structural details, and (ii) dynamic video complexities arising from rapid
instrument motion and persistent visual occlusions. While existing methods
mainly focus on spatial feature extraction, they fundamentally overlook
temporal dependencies in surgical video streams. To address this, we present
temporal asymmetric feature propagation network, a bidirectional attention
architecture enabling cross-frame feature propagation. The proposed method
contains a temporal query propagator that integrates multi-directional
consistency constraints to enhance frame-specific feature representation, and
an aggregated asymmetric feature pyramid module that preserves discriminative
features for anatomical structures and surgical instruments. Our framework
uniquely enables both temporal guidance and contextual reasoning for surgical
scene understanding. Comprehensive evaluations on two public benchmarks show
the proposed method outperforms the current SOTA methods by a large margin,
with +16.4\% mIoU on EndoVis2018 and +3.3\% mAP on Endoscapes2023. The code
will be publicly available after paper acceptance.

</details>

### [158] [SatelliteCalculator: A Multi-Task Vision Foundation Model for Quantitative Remote Sensing Inversion](https://arxiv.org/abs/2504.13442)
*Zhenyu Yu,Mohd. Yamani Idna Idris,Pei Wang*

Main category: cs.CV

TLDR: SatelliteCalculator是首个为定量遥感反演设计的视觉基础模型，通过物理定义的指数公式构建大规模数据集，结合Swin Transformer和提示引导架构，显著提升了任务适应性和推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决定量遥感反演中物理可解释回归任务的不足，以及多光谱和地理空间异质性带来的泛化挑战。

Method: 利用物理定义的指数公式构建百万级样本数据集，采用冻结Swin Transformer主干和提示引导架构，结合交叉注意力适配器和轻量级MLP解码器。

Result: 在Open-Canopy基准测试中表现优异，显著降低推理成本，验证了基础模型在定量反演中的可行性。

Conclusion: SatelliteCalculator为任务自适应的遥感估算提供了可扩展框架，推动了定量遥感反演的发展。

Abstract: Quantitative remote sensing inversion plays a critical role in environmental
monitoring, enabling the estimation of key ecological variables such as
vegetation indices, canopy structure, and carbon stock. Although vision
foundation models have achieved remarkable progress in classification and
segmentation tasks, their application to physically interpretable regression
remains largely unexplored. Furthermore, the multi-spectral nature and
geospatial heterogeneity of remote sensing data pose significant challenges for
generalization and transferability. To address these issues, we introduce
SatelliteCalculator, the first vision foundation model tailored for
quantitative remote sensing inversion. By leveraging physically defined index
formulas, we automatically construct a large-scale dataset of over one million
paired samples across eight core ecological indicators. The model integrates a
frozen Swin Transformer backbone with a prompt-guided architecture, featuring
cross-attentive adapters and lightweight task-specific MLP decoders.
Experiments on the Open-Canopy benchmark demonstrate that SatelliteCalculator
achieves competitive accuracy across all tasks while significantly reducing
inference cost. Our results validate the feasibility of applying foundation
models to quantitative inversion, and provide a scalable framework for
task-adaptive remote sensing estimation.

</details>

### [159] [MicroFlow: Domain-Specific Optical Flow for Ground Deformation Estimation in Seismic Events](https://arxiv.org/abs/2504.13452)
*Juliette Bertrand,Sophie Giffard-Roisin,James Hollingsworth,Julien Mairal*

Main category: cs.CV

TLDR: 提出了一种基于深度学习的模型，通过迭代优化和显式变形层实现亚像素精度的地面位移测量，解决了传统方法在真实场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 密集地面位移测量对地质研究至关重要，但直接采集不切实际。传统方法依赖光学卫星图像的块匹配，而深度学习模型因缺乏真实地面实况、亚像素精度需求及地质变化等问题难以应用。

Method: 采用迭代优化和显式变形层，结合非凸总变差正则化，保持断层线锐度并实现平滑位移场。

Result: 模型在半合成基准测试中显著优于广泛使用的地球物理方法，并在中高分辨率传感器的真实场景中表现良好。

Conclusion: 该模型为地面位移测量提供了一种高精度且适应性强的解决方案，适用于复杂地质条件。

Abstract: Dense ground displacement measurements are crucial for geological studies but
are impractical to collect directly. Traditionally, displacement fields are
estimated using patch matching on optical satellite images from different
acquisition times. While deep learning-based optical flow models are promising,
their adoption in ground deformation analysis is hindered by challenges such as
the absence of real ground truth, the need for sub-pixel precision, and
temporal variations due to geological or anthropogenic changes. In particular,
we identify that deep learning models relying on explicit correlation layers
struggle at estimating small displacements in real-world conditions. Instead,
we propose a model that employs iterative refinements with explicit warping
layers and a correlation-independent backbone, enabling sub-pixel precision.
Additionally, a non-convex variant of Total Variation regularization preserves
fault-line sharpness while maintaining smoothness elsewhere. Our model
significantly outperforms widely used geophysics methods on semi-synthetic
benchmarks and generalizes well to challenging real-world scenarios captured by
both medium- and high-resolution sensors. Project page:
https://jbertrand89.github.io/microflow/.

</details>

### [160] [Neural Ganglion Sensors: Learning Task-specific Event Cameras Inspired by the Neural Circuit of the Human Retina](https://arxiv.org/abs/2504.13457)
*Haley M. So,Gordon Wetzstein*

Main category: cs.CV

TLDR: 论文提出了一种基于生物启发的神经节传感器，通过任务特定的时空视网膜内核改进传统事件相机，提高了性能并减少了事件带宽。


<details>
  <summary>Details</summary>
Motivation: 受人类眼睛神经元高效数据处理的启发，传统事件相机缺乏局部空间上下文和多样性特征提取能力，因此需要改进。

Method: 引入神经节传感器，学习任务特定的时空视网膜内核（类似RGC事件），并在视频插值和光流任务中评估。

Result: 生物启发的传感器在性能上优于传统事件相机，同时减少了事件带宽。

Conclusion: RGC启发的传感器在边缘设备和低功耗实时应用中具有潜力。

Abstract: Inspired by the data-efficient spiking mechanism of neurons in the human eye,
event cameras were created to achieve high temporal resolution with minimal
power and bandwidth requirements by emitting asynchronous, per-pixel intensity
changes rather than conventional fixed-frame rate images. Unlike retinal
ganglion cells (RGCs) in the human eye, however, which integrate signals from
multiple photoreceptors within a receptive field to extract spatio-temporal
features, conventional event cameras do not leverage local spatial context when
deciding which events to fire. Moreover, the eye contains around 20 different
kinds of RGCs operating in parallel, each attuned to different features or
conditions. Inspired by this biological design, we introduce Neural Ganglion
Sensors, an extension of traditional event cameras that learns task-specific
spatio-temporal retinal kernels (i.e., RGC "events"). We evaluate our design on
two challenging tasks: video interpolation and optical flow. Our results
demonstrate that our biologically inspired sensing improves performance
relative to conventional event cameras while reducing overall event bandwidth.
These findings highlight the promise of RGC-inspired event sensors for edge
devices and other low-power, real-time applications requiring efficient,
high-resolution visual streams.

</details>

### [161] [Learning from Noisy Pseudo-labels for All-Weather Land Cover Mapping](https://arxiv.org/abs/2504.13458)
*Wang Liu,Zhiyu Wang,Xin Guo,Puhong Duan,Xudong Kang,Shutao Li*

Main category: cs.CV

TLDR: 提出了一种结合半监督学习和图像分辨率对齐增强的方法，生成更精确的伪标签，并使用对称交叉熵损失减少噪声影响，显著提升了SAR图像分割性能。


<details>
  <summary>Details</summary>
Motivation: SAR图像因缺乏细节信息和存在显著斑点噪声，导致标注或分割困难，现有方法生成的伪标签噪声较多，影响分割效果。

Method: 结合半监督学习和图像分辨率对齐增强生成伪标签，引入对称交叉熵损失减少噪声影响，并采用多种训练和测试技巧。

Result: 在GRSS数据融合竞赛中取得第一名，验证了方法的有效性。

Conclusion: 所提方法显著提升了SAR图像分割的准确性，代码已开源。

Abstract: Semantic segmentation of SAR images has garnered significant attention in
remote sensing due to the immunity of SAR sensors to cloudy weather and light
conditions. Nevertheless, SAR imagery lacks detailed information and is plagued
by significant speckle noise, rendering the annotation or segmentation of SAR
images a formidable task. Recent efforts have resorted to annotating paired
optical-SAR images to generate pseudo-labels through the utilization of an
optical image segmentation network. However, these pseudo-labels are laden with
noise, leading to suboptimal performance in SAR image segmentation. In this
study, we introduce a more precise method for generating pseudo-labels by
incorporating semi-supervised learning alongside a novel image resolution
alignment augmentation. Furthermore, we introduce a symmetric cross-entropy
loss to mitigate the impact of noisy pseudo-labels. Additionally, a bag of
training and testing tricks is utilized to generate better land-cover mapping
results. Our experiments on the GRSS data fusion contest indicate the
effectiveness of the proposed method, which achieves first place. The code is
available at https://github.com/StuLiu/DFC2025Track1.git.

</details>

### [162] [Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization](https://arxiv.org/abs/2504.13460)
*Hongwei Ji,Wulian Yun,Mengshi Qi,Huadong Ma*

Main category: cs.CV

TLDR: 提出了一种基于Chain-of-Thought文本推理的少样本时序动作定位方法，通过结合文本语义信息提升定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本TAL方法仅关注视频级信息，忽略了文本信息的语义支持。

Method: 设计了语义感知的文本-视觉对齐模块和类似Chain-of-Thought的推理方法，结合VLM和LLM生成文本描述。

Result: 在ActivityNet1.3和THUMOS14数据集上表现优异，并探索了人类异常检测应用。

Conclusion: 提出的方法显著优于现有方法，代码和数据将公开。

Abstract: Traditional temporal action localization (TAL) methods rely on large amounts
of detailed annotated data, whereas few-shot TAL reduces this dependence by
using only a few training samples to identify unseen action categories.
However, existing few-shot TAL methods typically focus solely on video-level
information, neglecting textual information, which can provide valuable
semantic support for the localization task. Therefore, we propose a new
few-shot temporal action localization method by Chain-of-Thought textual
reasoning to improve localization performance. Specifically, we design a novel
few-shot learning framework that leverages textual semantic information to
enhance the model's ability to capture action commonalities and variations,
which includes a semantic-aware text-visual alignment module designed to align
the query and support videos at different levels. Meanwhile, to better express
the temporal dependencies and causal relationships between actions at the
textual level to assist action localization, we design a Chain of Thought
(CoT)-like reasoning method that progressively guides the Vision Language Model
(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for
videos. The generated texts can capture more variance of action than visual
features. We conduct extensive experiments on the publicly available
ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named
Human-related Anomaly Localization and explore the application of the TAL task
in human anomaly detection. The experimental results demonstrate that our
proposed method significantly outperforms existing methods in single-instance
and multi-instance scenarios. We will release our code, data and benchmark.

</details>

### [163] [HMPE:HeatMap Embedding for Efficient Transformer-Based Small Object Detection](https://arxiv.org/abs/2504.13469)
*YangChen Zeng*

Main category: cs.CV

TLDR: 本文提出了一种名为HeatMap Position Embedding (HMPE)的新型Transformer优化技术，通过热图引导的自适应学习动态整合位置编码与语义检测信息，提升了小目标检测性能。同时设计了MOHFE和HIDQ模块，分别用于编码器和解码器，以减少背景噪声查询并生成高质量查询。实验表明，该方法在小目标数据集和通用数据集上均优于基线，并显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的小目标检测方法仍存在显著不足，需要一种能够动态整合位置信息与语义检测信息的技术来提升性能。

Method: 提出HMPE技术，结合热图引导的自适应学习；设计MOHFE和HIDQ模块分别优化编码器和解码器；使用LSConv特征工程增强小目标类别嵌入。

Result: 在NWPU VHR-10数据集上mAP提升1.9%，在PASCAL VOC数据集上提升1.2%；解码器层数从8层减少到3层，显著降低计算成本。

Conclusion: HMPE及相关模块有效提升了小目标检测性能并降低了计算成本，为Transformer在目标检测中的应用提供了新思路。

Abstract: Current Transformer-based methods for small object detection continue
emerging, yet they have still exhibited significant shortcomings. This paper
introduces HeatMap Position Embedding (HMPE), a novel Transformer Optimization
technique that enhances object detection performance by dynamically integrating
positional encoding with semantic detection information through heatmap-guided
adaptive learning.We also innovatively visualize the HMPE method, offering
clear visualization of embedded information for parameter fine-tuning.We then
create Multi-Scale ObjectBox-Heatmap Fusion Encoder (MOHFE) and HeatMap Induced
High-Quality Queries for Decoder (HIDQ) modules. These are designed for the
encoder and decoder, respectively, to generate high-quality queries and reduce
background noise queries.Using both heatmap embedding and Linear-Snake
Conv(LSConv) feature engineering, we enhance the embedding of massively diverse
small object categories and reduced the decoder multihead layers, thereby
accelerating both inference and training.In the generalization experiments, our
approach outperforme the baseline mAP by 1.9% on the small object dataset (NWPU
VHR-10) and by 1.2% on the general dataset (PASCAL VOC). By employing
HMPE-enhanced embedding, we are able to reduce the number of decoder layers
from eight to a minimum of three, significantly decreasing both inference and
training costs.

</details>

### [164] [Early Timestep Zero-Shot Candidate Selection for Instruction-Guided Image Editing](https://arxiv.org/abs/2504.13490)
*Joowon Kim,Ziseok Lee,Donghyeon Cho,Sanghyun Jo,Yeonsung Jung,Kyungsu Kim,Eunho Yang*

Main category: cs.CV

TLDR: ELECT框架通过早期时间步潜在评估选择可靠种子，提升扩散模型图像编辑的效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成和编辑中因随机噪声导致多样性，用户需反复调整种子或提示，效率低下。现有种子选择方法依赖外部验证器且计算复杂。

Method: 提出ELECT框架，通过早期时间步评估背景一致性，筛选保留背景的种子，并集成到编辑流程中。

Result: ELECT减少41%计算成本，提升背景一致性和指令遵循，40%失败案例成功。

Conclusion: ELECT无需外部监督即可高效提升图像编辑质量。

Abstract: Despite recent advances in diffusion models, achieving reliable image
generation and editing remains challenging due to the inherent diversity
induced by stochastic noise in the sampling process. Instruction-guided image
editing with diffusion models offers user-friendly capabilities, yet editing
failures, such as background distortion, frequently occur. Users often resort
to trial and error, adjusting seeds or prompts to achieve satisfactory results,
which is inefficient. While seed selection methods exist for Text-to-Image
(T2I) generation, they depend on external verifiers, limiting applicability,
and evaluating multiple seeds increases computational complexity. To address
this, we first establish a multiple-seed-based image editing baseline using
background consistency scores, achieving Best-of-N performance without
supervision. Building on this, we introduce ELECT (Early-timestep Latent
Evaluation for Candidate Selection), a zero-shot framework that selects
reliable seeds by estimating background mismatches at early diffusion
timesteps, identifying the seed that retains the background while modifying
only the foreground. ELECT ranks seed candidates by a background inconsistency
score, filtering unsuitable samples early based on background consistency while
preserving editability. Beyond standalone seed selection, ELECT integrates into
instruction-guided editing pipelines and extends to Multimodal Large-Language
Models (MLLMs) for joint seed and prompt selection, further improving results
when seed selection alone is insufficient. Experiments show that ELECT reduces
computational costs (by 41 percent on average and up to 61 percent) while
improving background consistency and instruction adherence, achieving around 40
percent success rates in previously failed cases - without any external
supervision or training.

</details>

### [165] [U-Shape Mamba: State Space Model for faster diffusion](https://arxiv.org/abs/2504.13499)
*Alex Ergasti,Filippo Botti,Tomaso Fontanini,Claudio Ferrari,Massimo Bertozzi,Andrea Prati*

Main category: cs.CV

TLDR: USM是一种基于Mamba的新型扩散模型，通过U-Net层次结构和Mamba块显著降低计算成本，同时保持生成能力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算成本高，限制了其应用。

Method: 采用Mamba块和U-Net层次结构，逐步减少和恢复序列长度。

Result: USM计算开销更低，速度更快，图像质量优于Zigma，FID分数提升显著。

Conclusion: USM是高效且可扩展的扩散模型解决方案。

Abstract: Diffusion models have become the most popular approach for high-quality image
generation, but their high computational cost still remains a significant
challenge. To address this problem, we propose U-Shape Mamba (USM), a novel
diffusion model that leverages Mamba-based layers within a U-Net-like
hierarchical structure. By progressively reducing sequence length in the
encoder and restoring it in the decoder through Mamba blocks, USM significantly
lowers computational overhead while maintaining strong generative capabilities.
Experimental results against Zigma, which is currently the most efficient
Mamba-based diffusion model, demonstrate that USM achieves one-third the
GFlops, requires less memory and is faster, while outperforming Zigma in image
quality. Frechet Inception Distance (FID) is improved by 15.3, 0.84 and 2.7
points on AFHQ, CelebAHQ and COCO datasets, respectively. These findings
highlight USM as a highly efficient and scalable solution for diffusion-based
generative models, making high-quality image synthesis more accessible to the
research community while reducing computational costs.

</details>

### [166] [OBIFormer: A Fast Attentive Denoising Framework for Oracle Bone Inscriptions](https://arxiv.org/abs/2504.13524)
*Jinhao Li,Zijian Chen,Tingzhu Chen,Zhiji Liu,Changbo Wang*

Main category: cs.CV

TLDR: 本文提出了一种名为OBIFormer的快速注意力去噪框架，用于甲骨文（OBI）图像去噪，结合通道自注意力、字形提取和选择性核特征融合，实现了高效且精确的去噪效果。


<details>
  <summary>Details</summary>
Motivation: 甲骨文图像因长期自然风化和人为破坏严重退化，现有方法在计算效率或效果上存在不足，亟需一种高效的去噪方法。

Method: 提出OBIFormer框架，利用通道自注意力、字形提取和选择性核特征融合技术，实现高效去噪。

Result: 在合成和真实甲骨文数据集上，OBIFormer在PSNR和SSIM指标上达到最优去噪性能。

Conclusion: OBIFormer在自动甲骨文识别中具有巨大潜力，代码已开源。

Abstract: Oracle bone inscriptions (OBIs) are the earliest known form of Chinese
characters and serve as a valuable resource for research in anthropology and
archaeology. However, most excavated fragments are severely degraded due to
thousands of years of natural weathering, corrosion, and man-made destruction,
making automatic OBI recognition extremely challenging. Previous methods either
focus on pixel-level information or utilize vanilla transformers for
glyph-based OBI denoising, which leads to tremendous computational overhead.
Therefore, this paper proposes a fast attentive denoising framework for oracle
bone inscriptions, i.e., OBIFormer. It leverages channel-wise self-attention,
glyph extraction, and selective kernel feature fusion to reconstruct denoised
images precisely while being computationally efficient. Our OBIFormer achieves
state-of-the-art denoising performance for PSNR and SSIM metrics on synthetic
and original OBI datasets. Furthermore, comprehensive experiments on a real
oracle dataset demonstrate the great potential of our OBIFormer in assisting
automatic OBI recognition. The code will be made available at
https://github.com/LJHolyGround/OBIFormer.

</details>

### [167] [EG-Gaussian: Epipolar Geometry and Graph Network Enhanced 3D Gaussian Splatting](https://arxiv.org/abs/2504.13540)
*Beizhen Zhao,Yifan Zhou,Zijian Wang,Hao Wang*

Main category: cs.CV

TLDR: 论文提出了一种名为EG-Gaussian的新框架，结合极线几何和图网络，解决了3D高斯泼溅（3DGS）在稀疏视图输入下重建不完整或模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在稀疏视图输入下容易生成不完整的3D场景或模糊的多视图，主要原因是初始3DGS点不准确和3D高斯被扁平化。

Method: 提出EG-Gaussian框架，通过极线几何优化3DGS初始点构建，并设计图学习模块细化3DGS空间特征，结合空间坐标和邻域点角度关系。

Result: 在室内外基准数据集上的实验表明，该方法显著提升了重建精度。

Conclusion: EG-Gaussian通过结合极线几何和图网络，有效提升了3D场景重建的准确性。

Abstract: In this paper, we explore an open research problem concerning the
reconstruction of 3D scenes from images. Recent methods have adopt 3D Gaussian
Splatting (3DGS) to produce 3D scenes due to its efficient training process.
However, these methodologies may generate incomplete 3D scenes or blurred
multiviews. This is because of (1) inaccurate 3DGS point initialization and (2)
the tendency of 3DGS to flatten 3D Gaussians with the sparse-view input. To
address these issues, we propose a novel framework EG-Gaussian, which utilizes
epipolar geometry and graph networks for 3D scene reconstruction. Initially, we
integrate epipolar geometry into the 3DGS initialization phase to enhance
initial 3DGS point construction. Then, we specifically design a graph learning
module to refine 3DGS spatial features, in which we incorporate both spatial
coordinates and angular relationships among neighboring points. Experiments on
indoor and outdoor benchmark datasets demonstrate that our approach
significantly improves reconstruction accuracy compared to 3DGS-based methods.

</details>

### [168] [Beyond One-Hot Labels: Semantic Mixing for Model Calibration](https://arxiv.org/abs/2504.13548)
*Haoyang Luo,Linwei Tao,Minjing Dong,Chang Xu*

Main category: cs.CV

TLDR: 论文提出了一种校准感知的数据增强方法（CSM），通过生成混合类别的合成样本及其真实不确定性，解决了现有校准方法依赖确定性标签的问题。


<details>
  <summary>Details</summary>
Motivation: 现有校准方法依赖于确定性标签（one-hot标签），无法充分反映模型预测的不确定性，而真实数据中缺乏不确定性样本。

Method: 提出CSM框架，利用扩散模型生成混合类别样本并标注置信度，同时提出校准重标注和探索适合新数据表示的损失函数。

Result: 实验表明CSM在校准性能上优于现有方法。

Conclusion: CSM通过合成不确定性数据，显著提升了模型校准效果。

Abstract: Model calibration seeks to ensure that models produce confidence scores that
accurately reflect the true likelihood of their predictions being correct.
However, existing calibration approaches are fundamentally tied to datasets of
one-hot labels implicitly assuming full certainty in all the annotations. Such
datasets are effective for classification but provides insufficient knowledge
of uncertainty for model calibration, necessitating the curation of datasets
with numerically rich ground-truth confidence values. However, due to the
scarcity of uncertain visual examples, such samples are not easily available as
real datasets. In this paper, we introduce calibration-aware data augmentation
to create synthetic datasets of diverse samples and their ground-truth
uncertainty. Specifically, we present Calibration-aware Semantic Mixing (CSM),
a novel framework that generates training samples with mixed class
characteristics and annotates them with distinct confidence scores via
diffusion models. Based on this framework, we propose calibrated reannotation
to tackle the misalignment between the annotated confidence score and the
mixing ratio during the diffusion reverse process. Besides, we explore the loss
functions that better fit the new data representation paradigm. Experimental
results demonstrate that CSM achieves superior calibration compared to the
state-of-the-art calibration approaches. Code is available at
github.com/E-Galois/CSM.

</details>

### [169] [Zero-Shot Industrial Anomaly Segmentation with Image-Aware Prompt Generation](https://arxiv.org/abs/2504.13560)
*SoYoung Park,Hyewon Lee,Mingyu Choi,Seunghoon Han,Jong-Ryul Lee,Sungsu Lim,Tae-Ho Kim*

Main category: cs.CV

TLDR: 提出了一种基于图像感知的动态提示方法（IAP-AS），通过结合图像标签模型和大型语言模型生成上下文感知提示，显著提升了异常分割的适应性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于固定提示的零样本异常分割方法在多样化工业场景中适应性不足，亟需灵活且上下文感知的提示策略。

Method: 利用图像标签模型提取对象属性，结合大型语言模型生成动态、上下文感知的提示，提升异常分割效果。

Result: 实验表明，IAP-AS在F1-max指标上提升高达10%，表现出更强的适应性和泛化能力。

Conclusion: IAP-AS为跨行业异常分割提供了可扩展的解决方案，适用于动态和非结构化的工业环境。

Abstract: Anomaly segmentation is essential for industrial quality, maintenance, and
stability. Existing text-guided zero-shot anomaly segmentation models are
effective but rely on fixed prompts, limiting adaptability in diverse
industrial scenarios. This highlights the need for flexible, context-aware
prompting strategies. We propose Image-Aware Prompt Anomaly Segmentation
(IAP-AS), which enhances anomaly segmentation by generating dynamic,
context-aware prompts using an image tagging model and a large language model
(LLM). IAP-AS extracts object attributes from images to generate context-aware
prompts, improving adaptability and generalization in dynamic and unstructured
industrial environments. In our experiments, IAP-AS improves the F1-max metric
by up to 10%, demonstrating superior adaptability and generalization. It
provides a scalable solution for anomaly segmentation across industries

</details>

### [170] [WeatherGen: A Unified Diverse Weather Generator for LiDAR Point Clouds via Spider Mamba Diffusion](https://arxiv.org/abs/2504.13561)
*Yang Wu,Yun Zhu,Kaihua Zhang,Jianjun Qian,Jin Xie,Jian Yang*

Main category: cs.CV

TLDR: WeatherGen是一个统一的多样化天气LiDAR数据生成框架，通过扩散模型和蜘蛛曼巴生成器提高数据保真度，并利用对比学习控制器优化生成结果。


<details>
  <summary>Details</summary>
Motivation: 解决现有LiDAR模拟器只能模拟单一恶劣天气且数据保真度低的问题。

Method: 设计基于地图的数据生成器，构建扩散模型，提出蜘蛛曼巴生成器逐步恢复数据，并引入潜在特征对齐器和对比学习控制器。

Result: 生成高质量多样化天气LiDAR数据，构建mini-weather数据集，提升下游任务性能。

Conclusion: WeatherGen显著提高了LiDAR数据的生成质量和多样性，为恶劣天气条件下的3D场景感知提供了有效支持。

Abstract: 3D scene perception demands a large amount of adverse-weather LiDAR data, yet
the cost of LiDAR data collection presents a significant scaling-up challenge.
To this end, a series of LiDAR simulators have been proposed. Yet, they can
only simulate a single adverse weather with a single physical model, and the
fidelity of the generated data is quite limited. This paper presents
WeatherGen, the first unified diverse-weather LiDAR data diffusion generation
framework, significantly improving fidelity. Specifically, we first design a
map-based data producer, which can provide a vast amount of high-quality
diverse-weather data for training purposes. Then, we utilize the
diffusion-denoising paradigm to construct a diffusion model. Among them, we
propose a spider mamba generator to restore the disturbed diverse weather data
gradually. The spider mamba models the feature interactions by scanning the
LiDAR beam circle or central ray, excellently maintaining the physical
structure of the LiDAR data. Subsequently, following the generator to transfer
real-world knowledge, we design a latent feature aligner. Afterward, we devise
a contrastive learning-based controller, which equips weather control signals
with compact semantic knowledge through language supervision, guiding the
diffusion model to generate more discriminative data. Extensive evaluations
demonstrate the high generation quality of WeatherGen. Through WeatherGen, we
construct the mini-weather dataset, promoting the performance of the downstream
task under adverse weather conditions. Code is available:
https://github.com/wuyang98/weathergen

</details>

### [171] [HDBFormer: Efficient RGB-D Semantic Segmentation with A Heterogeneous Dual-Branch Framework](https://arxiv.org/abs/2504.13579)
*Shuobin Wei,Zhuang Zhou,Zhengan Lu,Zizhao Yuan,Binghua Su*

Main category: cs.CV

TLDR: HDBFormer是一个新颖的异构双分支框架，用于RGB-D室内场景语义分割，通过区分处理RGB和深度图像，结合局部和全局特征提取，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了RGB和深度图像在信息表达上的固有差异，未能充分利用其独特特性。

Method: 提出HDBFormer框架：RGB分支使用基础和细节编码器提取特征；深度分支采用轻量级LDFormer编码器；引入MIIM模块结合Transformer和大核卷积实现跨模态信息交互。

Result: 在NYUDepthv2和SUN-RGBD数据集上达到最先进性能。

Conclusion: HDBFormer通过异构双分支设计和跨模态交互，有效提升了RGB-D语义分割的性能。

Abstract: In RGB-D semantic segmentation for indoor scenes, a key challenge is
effectively integrating the rich color information from RGB images with the
spatial distance information from depth images. However, most existing methods
overlook the inherent differences in how RGB and depth images express
information. Properly distinguishing the processing of RGB and depth images is
essential to fully exploiting their unique and significant characteristics. To
address this, we propose a novel heterogeneous dual-branch framework called
HDBFormer, specifically designed to handle these modality differences. For RGB
images, which contain rich detail, we employ both a basic and detail encoder to
extract local and global features. For the simpler depth images, we propose
LDFormer, a lightweight hierarchical encoder that efficiently extracts depth
features with fewer parameters. Additionally, we introduce the Modality
Information Interaction Module (MIIM), which combines transformers with large
kernel convolutions to interact global and local information across modalities
efficiently. Extensive experiments show that HDBFormer achieves
state-of-the-art performance on the NYUDepthv2 and SUN-RGBD datasets. The code
is available at: https://github.com/Weishuobin/HDBFormer.

</details>

### [172] [Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding](https://arxiv.org/abs/2504.13580)
*Yuchen Rao,Stefan Ainetter,Sinisa Stekovic,Vincent Lepetit,Friedrich Fraundorfer*

Main category: cs.CV

TLDR: 利用自动检索的合成CAD模型生成高质量标注，训练深度学习模型，在点云补全和单视图CAD模型检索任务中表现优于人工标注。


<details>
  <summary>Details</summary>
Motivation: 解决3D场景理解中高质量标注获取困难的问题，降低标注成本。

Method: 采用类似ScanNet的自动标注流程，应用于ScanNet++ v1数据集，生成9D姿态和CAD模型标注。

Result: 自动标注训练的模型在点云补全和CAD模型检索任务中优于人工标注模型。

Conclusion: 自动3D标注可提升模型性能并显著降低成本，未来将公开标注数据和模型。

Abstract: High-level 3D scene understanding is essential in many applications. However,
the challenges of generating accurate 3D annotations make development of deep
learning models difficult. We turn to recent advancements in automatic
retrieval of synthetic CAD models, and show that data generated by such methods
can be used as high-quality ground truth for training supervised deep learning
models. More exactly, we employ a pipeline akin to the one previously used to
automatically annotate objects in ScanNet scenes with their 9D poses and CAD
models. This time, we apply it to the recent ScanNet++ v1 dataset, which
previously lacked such annotations. Our findings demonstrate that it is not
only possible to train deep learning models on these automatically-obtained
annotations but that the resulting models outperform those trained on manually
annotated data. We validate this on two distinct tasks: point cloud completion
and single-view CAD model retrieval and alignment. Our results underscore the
potential of automatic 3D annotations to enhance model performance while
significantly reducing annotation costs. To support future research in 3D scene
understanding, we will release our annotations, which we call SCANnotate++,
along with our trained models.

</details>

### [173] [HAECcity: Open-Vocabulary Scene Understanding of City-Scale Point Clouds with Superpoint Graph Clustering](https://arxiv.org/abs/2504.13590)
*Alexander Rusnak,Frédéric Kaplan*

Main category: cs.CV

TLDR: 提出了一种基于超点图聚类的开放词汇3D场景理解方法HAEC，适用于城市规模数据集，无需手工标注。


<details>
  <summary>Details</summary>
Motivation: 传统3D场景理解依赖手工标注，现有开放词汇方法难以扩展至城市规模数据集。

Method: 采用超点图聚类和专家混合图变换器作为主干，提出合成标注流程。

Result: 在SensatUrban数据集上首次实现城市规模的开放词汇场景理解。

Conclusion: HAEC为密集城市3D场景处理开辟了新途径，推动数字孪生技术的发展。

Abstract: Traditional 3D scene understanding techniques are generally predicated on
hand-annotated label sets, but in recent years a new class of open-vocabulary
3D scene understanding techniques has emerged. Despite the success of this
paradigm on small scenes, existing approaches cannot scale efficiently to
city-scale 3D datasets. In this paper, we present Hierarchical vocab-Agnostic
Expert Clustering (HAEC), after the latin word for 'these', a superpoint graph
clustering based approach which utilizes a novel mixture of experts graph
transformer for its backbone. We administer this highly scalable approach to
the first application of open-vocabulary scene understanding on the SensatUrban
city-scale dataset. We also demonstrate a synthetic labeling pipeline which is
derived entirely from the raw point clouds with no hand-annotation. Our
technique can help unlock complex operations on dense urban 3D scenes and open
a new path forward in the processing of digital twins.

</details>

### [174] [KAN or MLP? Point Cloud Shows the Way Forward](https://arxiv.org/abs/2504.13593)
*Yan Shi,Qingdong He,Yijun Liu,Xiaoyu Liu,Jingyong Su*

Main category: cs.CV

TLDR: PointKAN是一种基于Kolmogorov-Arnold Networks (KANs)的点云分析方法，通过几何仿射模块和局部特征处理提升特征表示能力，同时通过高效KANs减少参数和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统MLPs在点云分析中难以高效捕捉局部几何特征，且存在参数冗余问题。

Method: 提出PointKAN，结合几何仿射模块（GAM）和局部特征处理（LFP），并通过高效KANs优化参数效率。

Result: 在ModelNet40等基准数据集上优于PointMLP，尤其在Few-shot Learning任务中表现突出，同时显著减少参数和计算量。

Conclusion: PointKAN展示了KANs在3D视觉中的潜力，为点云理解研究提供了新方向。

Abstract: Multi-Layer Perceptrons (MLPs) have become one of the fundamental
architectural component in point cloud analysis due to its effective feature
learning mechanism. However, when processing complex geometric structures in
point clouds, MLPs' fixed activation functions struggle to efficiently capture
local geometric features, while suffering from poor parameter efficiency and
high model redundancy. In this paper, we propose PointKAN, which applies
Kolmogorov-Arnold Networks (KANs) to point cloud analysis tasks to investigate
their efficacy in hierarchical feature representation. First, we introduce a
Geometric Affine Module (GAM) to transform local features, improving the
model's robustness to geometric variations. Next, in the Local Feature
Processing (LFP), a parallel structure extracts both group-level features and
global context, providing a rich representation of both fine details and
overall structure. Finally, these features are combined and processed in the
Global Feature Processing (GFP). By repeating these operations, the receptive
field gradually expands, enabling the model to capture complete geometric
information of the point cloud. To overcome the high parameter counts and
computational inefficiency of standard KANs, we develop Efficient-KANs in the
PointKAN-elite variant, which significantly reduces parameters while
maintaining accuracy. Experimental results demonstrate that PointKAN
outperforms PointMLP on benchmark datasets such as ModelNet40, ScanObjectNN,
and ShapeNetPart, with particularly strong performance in Few-shot Learning
task. Additionally, PointKAN achieves substantial reductions in parameter
counts and computational complexity (FLOPs). This work highlights the potential
of KANs-based architectures in 3D vision and opens new avenues for research in
point cloud understanding.

</details>

### [175] [LMPOcc: 3D Semantic Occupancy Prediction Utilizing Long-Term Memory Prior from Historical Traversals](https://arxiv.org/abs/2504.13596)
*Shanshuai Yuan,Julong Wei,Muer Tie,Xiangyun Ren,Zhongxue Gan,Wenchao Ding*

Main category: cs.CV

TLDR: LMPOcc是一种基于历史感知信息的长时记忆先验3D语义占据预测方法，通过整合历史遍历数据提升局部感知并构建全局占据表示，在Occ3D-nuScenes基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在相同地理位置的多次遍历中，现有方法忽略利用历史感知信息，导致3D占据预测的局限性。

Method: 提出LMPOcc方法，引入长时记忆先验，设计轻量级Current-Prior Fusion模块自适应融合特征，并采用模型无关的先验格式。

Result: 在Occ3D-nuScenes基准测试中达到最优性能，尤其在静态语义类别上表现突出，并支持多车众包构建全局占据。

Conclusion: LMPOcc通过利用历史遍历数据显著提升了3D语义占据预测性能，为自动驾驶提供了更全面的环境建模能力。

Abstract: Vision-based 3D semantic occupancy prediction is critical for autonomous
driving, enabling unified modeling of static infrastructure and dynamic agents.
In practice, autonomous vehicles may repeatedly traverse identical geographic
locations under varying environmental conditions, such as weather fluctuations
and illumination changes. Existing methods in 3D occupancy prediction
predominantly integrate adjacent temporal contexts. However, these works
neglect to leverage perceptual information, which is acquired from historical
traversals of identical geographic locations. In this paper, we propose
Longterm Memory Prior Occupancy (LMPOcc), the first 3D occupancy prediction
methodology that exploits long-term memory priors derived from historical
traversal perceptual outputs. We introduce a plug-and-play architecture that
integrates long-term memory priors to enhance local perception while
simultaneously constructing global occupancy representations. To adaptively
aggregate prior features and current features, we develop an efficient
lightweight Current-Prior Fusion module. Moreover, we propose a model-agnostic
prior format to ensure compatibility across diverse occupancy prediction
baselines. LMPOcc achieves state-of-the-art performance validated on the
Occ3D-nuScenes benchmark, especially on static semantic categories.
Additionally, experimental results demonstrate LMPOcc's ability to construct
global occupancy through multi-vehicle crowdsourcing.

</details>

### [176] [FocusTrack: A Self-Adaptive Local Sampling Algorithm for Efficient Anti-UAV Tracking](https://arxiv.org/abs/2504.13604)
*Ying Wang,Tingfa Xu,Jianan Li*

Main category: cs.CV

TLDR: FocusTrack提出了一种动态调整搜索区域和增强特征表示的新框架，平衡了计算效率和跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有跟踪方法在抗无人机跟踪中的问题，如全局方法计算开销大，局部方法在目标位移大时表现不佳。

Method: 提出Search Region Adjustment (SRA)策略动态调整视野，以及Attention-to-Mask (ATM)模块增强特征表示。

Result: 在AntiUAV和AntiUAV410数据集上分别达到67.7%和62.8% AUC，显著优于基线方法。

Conclusion: FocusTrack在精度和效率上均优于现有方法，适用于实时跟踪场景。

Abstract: Anti-UAV tracking poses significant challenges, including small target sizes,
abrupt camera motion, and cluttered infrared backgrounds. Existing tracking
paradigms can be broadly categorized into global- and local-based methods.
Global-based trackers, such as SiamDT, achieve high accuracy by scanning the
entire field of view but suffer from excessive computational overhead, limiting
real-world deployment. In contrast, local-based methods, including OSTrack and
ROMTrack, efficiently restrict the search region but struggle when targets
undergo significant displacements due to abrupt camera motion. Through
preliminary experiments, it is evident that a local tracker, when paired with
adaptive search region adjustment, can significantly enhance tracking accuracy,
narrowing the gap between local and global trackers. To address this challenge,
we propose FocusTrack, a novel framework that dynamically refines the search
region and strengthens feature representations, achieving an optimal balance
between computational efficiency and tracking accuracy. Specifically, our
Search Region Adjustment (SRA) strategy estimates the target presence
probability and adaptively adjusts the field of view, ensuring the target
remains within focus. Furthermore, to counteract feature degradation caused by
varying search regions, the Attention-to-Mask (ATM) module is proposed. This
module integrates hierarchical information, enriching the target
representations with fine-grained details. Experimental results demonstrate
that FocusTrack achieves state-of-the-art performance, obtaining 67.7% AUC on
AntiUAV and 62.8% AUC on AntiUAV410, outperforming the baseline tracker by 8.5%
and 9.1% AUC, respectively. In terms of efficiency, FocusTrack surpasses
global-based trackers, requiring only 30G MACs and achieving 143 fps with
FocusTrack (SRA) and 44 fps with the full version, both enabling real-time
tracking.

</details>

### [177] [Cross-Hierarchical Bidirectional Consistency Learning for Fine-Grained Visual Classification](https://arxiv.org/abs/2504.13608)
*Pengxiang Gao,Yihao Liang,Yanzhi Song,Zhouwang Yang*

Main category: cs.CV

TLDR: 提出了一种跨层次双向一致性学习框架（CHBC），利用树层次结构信息提升细粒度视觉分类的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉分类（FGVC）任务中，类间差异小且类内差异大，现有方法常依赖额外标注，忽略了树层次结构中的有用信息。

Method: 设计了跨层次双向一致性学习框架（CHBC），通过分解和增强注意力掩码与特征提取判别性特征，并利用双向一致性损失确保不同层次分类结果的一致性。

Result: 在三个广泛使用的FGVC数据集上验证了CHBC框架的有效性，消融实验进一步证明了特征增强和一致性约束模块的贡献。

Conclusion: CHBC框架通过利用树层次结构和双向一致性学习，显著提升了细粒度视觉分类的性能。

Abstract: Fine-Grained Visual Classification (FGVC) aims to categorize closely related
subclasses, a task complicated by minimal inter-class differences and
significant intra-class variance. Existing methods often rely on additional
annotations for image classification, overlooking the valuable information
embedded in Tree Hierarchies that depict hierarchical label relationships. To
leverage this knowledge to improve classification accuracy and consistency, we
propose a novel Cross-Hierarchical Bidirectional Consistency Learning (CHBC)
framework. The CHBC framework extracts discriminative features across various
hierarchies using a specially designed module to decompose and enhance
attention masks and features. We employ bidirectional consistency loss to
regulate the classification outcomes across different hierarchies, ensuring
label prediction consistency and reducing misclassification. Experiments on
three widely used FGVC datasets validate the effectiveness of the CHBC
framework. Ablation studies further investigate the application strategies of
feature enhancement and consistency constraints, underscoring the significant
contributions of the proposed modules.

</details>

### [178] [Compile Scene Graphs with Reinforcement Learning](https://arxiv.org/abs/2504.13617)
*Zuyao Chen,Jinlin Wu,Zhen Lei,Marc Pollefeys,Chang Wen Chen*

Main category: cs.CV

TLDR: 论文提出R1-SGG，一种多模态大语言模型，通过监督微调和强化学习优化场景图生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在生成结构化视觉表示（如场景图）方面研究不足，需要准确生成对象和关系三元组。

Method: 结合监督微调（SFT）和强化学习（RL），设计图中心奖励函数（节点、边和格式一致性奖励）。

Result: RL显著提升模型性能，实现零失败率，优于仅用SFT的方法。

Conclusion: R1-SGG通过RL优化，有效解决了场景图生成任务中的泛化问题。

Abstract: Next token prediction is the fundamental principle for training large
language models (LLMs), and reinforcement learning (RL) further enhances their
reasoning performance. As an effective way to model language, image, video, and
other modalities, the use of LLMs for end-to-end extraction of structured
visual representations, such as scene graphs, remains underexplored. It
requires the model to accurately produce a set of objects and relationship
triplets, rather than generating text token by token. To achieve this, we
introduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised
fine-tuning (SFT) on the scene graph dataset and subsequently refined using
reinforcement learning to enhance its ability to generate scene graphs in an
end-to-end manner. The SFT follows a conventional prompt-response paradigm,
while RL requires the design of effective reward signals. Given the structured
nature of scene graphs, we design a graph-centric reward function that
integrates node-level rewards, edge-level rewards, and a format consistency
reward. Our experiments demonstrate that rule-based RL substantially enhances
model performance in the SGG task, achieving a zero failure rate--unlike
supervised fine-tuning (SFT), which struggles to generalize effectively. Our
code is available at https://github.com/gpt4vision/R1-SGG.

</details>

### [179] [Visual Intention Grounding for Egocentric Assistants](https://arxiv.org/abs/2504.13621)
*Pengzhan Sun,Junbin Xiao,Tze Ho Elden Tse,Yicong Li,Arjun Akula,Angela Yao*

Main category: cs.CV

TLDR: 论文提出了EgoIntention数据集和Reason-to-Ground（RoG）方法，用于解决以自我为中心视角下的视觉意图定位问题，并展示了RoG在性能上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统视觉定位方法针对第三人称视角和明确对象查询，而AI助手等应用需要处理自我中心视角和隐含意图的对象定位。

Method: 提出EgoIntention数据集，并设计RoG指令调优方法，结合意图推理和对象定位机制进行混合训练。

Result: RoG在EgoIntention数据集上显著优于传统微调和混合训练方法，同时保持对普通描述的定位能力。

Conclusion: RoG方法实现了对自我中心和外部中心视角的统一视觉定位，同时处理显式对象查询和隐含意图。

Abstract: Visual grounding associates textual descriptions with objects in an image.
Conventional methods target third-person image inputs and named object queries.
In applications such as AI assistants, the perspective shifts -- inputs are
egocentric, and objects may be referred to implicitly through needs and
intentions. To bridge this gap, we introduce EgoIntention, the first dataset
for egocentric visual intention grounding. EgoIntention challenges multimodal
LLMs to 1) understand and ignore unintended contextual objects and 2) reason
about uncommon object functionalities. Benchmark results show that current
models misidentify context objects and lack affordance understanding in
egocentric views. We also propose Reason-to-Ground (RoG) instruction tuning; it
enables hybrid training with normal descriptions and egocentric intentions with
a chained intention reasoning and object grounding mechanism. RoG significantly
outperforms naive finetuning and hybrid training on EgoIntention, while
maintaining or slightly improving naive description grounding. This advancement
enables unified visual grounding for egocentric and exocentric visual inputs
while handling explicit object queries and implicit human intentions.

</details>

### [180] [DenSe-AdViT: A novel Vision Transformer for Dense SAR Object Detection](https://arxiv.org/abs/2504.13638)
*Yang Zhang,Jingyi Cao,Yanan You,Yuanyuan Qiao*

Main category: cs.CV

TLDR: DenSe-AdViT 是一种针对密集 SAR 目标检测的改进 Vision Transformer，通过密度感知模块和多尺度融合模块提升小目标检测性能。


<details>
  <summary>Details</summary>
Motivation: ViT 在 SAR 图像目标检测中表现优异，但难以提取多尺度局部特征，导致小目标检测性能受限。

Method: 提出密度感知模块（DAM）生成密度张量，并结合密度增强融合模块（DEFM）整合 CNN 的多尺度信息和 Transformer 的全局特征。

Result: 在 RSDD 和 SIVED 数据集上分别达到 79.8% 和 92.5% 的 mAP。

Conclusion: DenSe-AdViT 显著提升了密集小目标检测的性能。

Abstract: Vision Transformer (ViT) has achieved remarkable results in object detection
for synthetic aperture radar (SAR) images, owing to its exceptional ability to
extract global features. However, it struggles with the extraction of
multi-scale local features, leading to limited performance in detecting small
targets, especially when they are densely arranged. Therefore, we propose
Density-Sensitive Vision Transformer with Adaptive Tokens (DenSe-AdViT) for
dense SAR target detection. We design a Density-Aware Module (DAM) as a
preliminary component that generates a density tensor based on target
distribution. It is guided by a meticulously crafted objective metric, enabling
precise and effective capture of the spatial distribution and density of
objects. To integrate the multi-scale information enhanced by convolutional
neural networks (CNNs) with the global features derived from the Transformer,
Density-Enhanced Fusion Module (DEFM) is proposed. It effectively refines
attention toward target-survival regions with the assist of density mask and
the multiple sources features. Notably, our DenSe-AdViT achieves 79.8% mAP on
the RSDD dataset and 92.5% on the SIVED dataset, both of which feature a large
number of densely distributed vehicle targets.

</details>

### [181] [Efficient Parameter Adaptation for Multi-Modal Medical Image Segmentation and Prognosis](https://arxiv.org/abs/2504.13645)
*Numan Saeed,Shahad Hardan,Muhammad Ridzuan,Nada Saadi,Karthik Nandakumar,Mohammad Yaqub*

Main category: cs.CV

TLDR: 提出了一种参数高效的多模态适应框架（PEMMA），用于仅使用CT扫描训练的模型，在PET扫描可用时高效适应，同时支持预后任务。


<details>
  <summary>Details</summary>
Motivation: 由于PET扫描的有限可用性，需要一种灵活高效的框架，能够仅用CT扫描训练，并在PET扫描可用时适应。

Method: 利用Transformer架构的模块性，通过低秩适应（LoRA）和分解低秩适应（DoRA）实现参数高效适应，减少跨模态纠缠。

Result: 性能与早期融合相当，但仅需8%的可训练参数；PET扫描的Dice分数提升28%；预后任务中，适应PET扫描和EHR数据后，一致性指数分别提升10%和23%。

Conclusion: PEMMA框架在参数效率和性能上均表现出色，为多模态医学影像分析提供了灵活且高效的解决方案。

Abstract: Cancer detection and prognosis relies heavily on medical imaging,
particularly CT and PET scans. Deep Neural Networks (DNNs) have shown promise
in tumor segmentation by fusing information from these modalities. However, a
critical bottleneck exists: the dependency on CT-PET data concurrently for
training and inference, posing a challenge due to the limited availability of
PET scans. Hence, there is a clear need for a flexible and efficient framework
that can be trained with the widely available CT scans and can be still adapted
for PET scans when they become available. In this work, we propose a
parameter-efficient multi-modal adaptation (PEMMA) framework for lightweight
upgrading of a transformer-based segmentation model trained only on CT scans
such that it can be efficiently adapted for use with PET scans when they become
available. This framework is further extended to perform prognosis task
maintaining the same efficient cross-modal fine-tuning approach. The proposed
approach is tested with two well-known segementation backbones, namely UNETR
and Swin UNETR. Our approach offers two main advantages. Firstly, we leverage
the inherent modularity of the transformer architecture and perform low-rank
adaptation (LoRA) as well as decomposed low-rank adaptation (DoRA) of the
attention weights to achieve parameter-efficient adaptation. Secondly, by
minimizing cross-modal entanglement, PEMMA allows updates using only one
modality without causing catastrophic forgetting in the other. Our method
achieves comparable performance to early fusion, but with only 8% of the
trainable parameters, and demonstrates a significant +28% Dice score
improvement on PET scans when trained with a single modality. Furthermore, in
prognosis, our method improves the concordance index by +10% when adapting a
CT-pretrained model to include PET scans, and by +23% when adapting for both
PET and EHR data.

</details>

### [182] [Enhancing Pothole Detection and Characterization: Integrated Segmentation and Depth Estimation in Road Anomaly Systems](https://arxiv.org/abs/2504.13648)
*Uthman Baroudi,Alala BaHamid,Yasser Elalfy,Ziad Al Alami*

Main category: cs.CV

TLDR: 该论文提出了一种基于YOLOv8-seg模型的迁移学习方法，用于通过车载摄像头图像自动检测和表征道路坑洞，结合深度图提供更全面的坑洞信息。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在道路坑洞检测中难以全面表征坑洞特征，因此需要一种更高效且全面的方法。

Method: 采用预训练的YOLOv8-seg模型进行坑洞检测和分割，结合深度图提取坑洞的深度信息。

Result: 该方法能够精确定位坑洞并计算其面积和深度，提供比以往更全面的表征。

Conclusion: 该方法不仅提升了自动驾驶车辆的导航安全性，还能帮助道路维护部门更高效地响应道路损坏。

Abstract: Road anomaly detection plays a crucial role in road maintenance and in
enhancing the safety of both drivers and vehicles. Recent machine learning
approaches for road anomaly detection have overcome the tedious and
time-consuming process of manual analysis and anomaly counting; however, they
often fall short in providing a complete characterization of road potholes. In
this paper, we leverage transfer learning by adopting a pre-trained YOLOv8-seg
model for the automatic characterization of potholes using digital images
captured from a dashboard-mounted camera. Our work includes the creation of a
novel dataset, comprising both images and their corresponding depth maps,
collected from diverse road environments in Al-Khobar city and the KFUPM campus
in Saudi Arabia. Our approach performs pothole detection and segmentation to
precisely localize potholes and calculate their area. Subsequently, the
segmented image is merged with its depth map to extract detailed depth
information about the potholes. This integration of segmentation and depth data
offers a more comprehensive characterization compared to previous deep
learning-based road anomaly detection systems. Overall, this method not only
has the potential to significantly enhance autonomous vehicle navigation by
improving the detection and characterization of road hazards but also assists
road maintenance authorities in responding more effectively to road damage.

</details>

### [183] [EyecareGPT: Boosting Comprehensive Ophthalmology Understanding with Tailored Dataset, Benchmark and Model](https://arxiv.org/abs/2504.13650)
*Sijing Li,Tianwei Lin,Lingshuai Lin,Wenqiao Zhang,Jiang Liu,Xiaoda Yang,Juncheng Li,Yucheng He,Xiaohui Song,Jun Xiao,Yueting Zhuang,Beng Chin Ooi*

Main category: cs.CV

TLDR: 本文提出了Eyecare Kit，通过定制数据集、基准测试和模型，解决了眼科智能诊断中的数据、基准和模型三大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有医学大型视觉语言模型在眼科诊断中表现有限，主要由于缺乏高质量标注数据、系统性评估基准和针对细粒度病变的模型适配。

Method: 构建多代理数据引擎生成高质量数据集Eyecare-100K，设计评估基准Eyecare-Bench，开发优化模型EyecareGPT，包含自适应分辨率机制和分层密集连接器。

Result: EyecareGPT在多项眼科任务中达到最先进性能，展示了其在智能眼科诊断研究中的潜力。

Conclusion: Eyecare Kit为智能眼科诊断提供了系统性解决方案，推动了该领域的开放研究。

Abstract: Medical Large Vision-Language Models (Med-LVLMs) demonstrate significant
potential in healthcare, but their reliance on general medical data and
coarse-grained global visual understanding limits them in intelligent
ophthalmic diagnosis. Currently, intelligent ophthalmic diagnosis faces three
major challenges: (i) Data. The lack of deeply annotated, high-quality,
multi-modal ophthalmic visual instruction data; (ii) Benchmark. The absence of
a comprehensive and systematic benchmark for evaluating diagnostic performance;
(iii) Model. The difficulty of adapting holistic visual architectures to
fine-grained, region-specific ophthalmic lesion identification. In this paper,
we propose the Eyecare Kit, which systematically tackles the aforementioned
three key challenges with the tailored dataset, benchmark and model: First, we
construct a multi-agent data engine with real-life ophthalmology data to
produce Eyecare-100K, a high-quality ophthalmic visual instruction dataset.
Subsequently, we design Eyecare-Bench, a benchmark that comprehensively
evaluates the overall performance of LVLMs on intelligent ophthalmic diagnosis
tasks across multiple dimensions. Finally, we develop the EyecareGPT, optimized
for fine-grained ophthalmic visual understanding thoroughly, which incorporates
an adaptive resolution mechanism and a layer-wise dense connector. Extensive
experimental results indicate that the EyecareGPT achieves state-of-the-art
performance in a range of ophthalmic tasks, underscoring its significant
potential for the advancement of open research in intelligent ophthalmic
diagnosis. Our project is available at https://github.com/DCDmllm/EyecareGPT.

</details>

### [184] [AnyTSR: Any-Scale Thermal Super-Resolution for UAV](https://arxiv.org/abs/2504.13682)
*Mengyuan Li,Changhong Fu,Ziyu Lu,Zijie Zhang,Haobo Zuo,Liangliang Yao*

Main category: cs.CV

TLDR: 提出了一种新型任意尺度热成像超分辨率方法（AnyTSR），通过单一模型解决无人机热成像低分辨率问题，显著提升图像细节和边界清晰度。


<details>
  <summary>Details</summary>
Motivation: 热成像传感器分辨率低导致细节不足和边界模糊，现有超分辨率方法多为固定尺度且计算成本高，缺乏灵活性。

Method: 设计新型图像编码器分配特定特征码，嵌入坐标偏移信息到局部特征集合中，提出任意尺度上采样器，并构建新数据集（UAV-TSR）。

Result: 实验表明，该方法在所有尺度上均优于现有技术，生成的高分辨率图像更准确、细节更丰富。

Conclusion: AnyTSR为无人机热成像提供了一种高效、灵活的超分辨率解决方案。

Abstract: Thermal imaging can greatly enhance the application of intelligent unmanned
aerial vehicles (UAV) in challenging environments. However, the inherent low
resolution of thermal sensors leads to insufficient details and blurred
boundaries. Super-resolution (SR) offers a promising solution to address this
issue, while most existing SR methods are designed for fixed-scale SR. They are
computationally expensive and inflexible in practical applications. To address
above issues, this work proposes a novel any-scale thermal SR method (AnyTSR)
for UAV within a single model. Specifically, a new image encoder is proposed to
explicitly assign specific feature code to enable more accurate and flexible
representation. Additionally, by effectively embedding coordinate offset
information into the local feature ensemble, an innovative any-scale upsampler
is proposed to better understand spatial relationships and reduce artifacts.
Moreover, a novel dataset (UAV-TSR), covering both land and water scenes, is
constructed for thermal SR tasks. Experimental results demonstrate that the
proposed method consistently outperforms state-of-the-art methods across all
scaling factors as well as generates more accurate and detailed high-resolution
images. The code is located at https://github.com/vision4robotics/AnyTSR.

</details>

### [185] [Analysing the Robustness of Vision-Language-Models to Common Corruptions](https://arxiv.org/abs/2504.13690)
*Muhammad Usama,Syeda Aisha Asim,Syed Bilal Ali,Syed Talal Wasim,Umair Bin Mansoor*

Main category: cs.CV

TLDR: 本文首次全面分析了视觉语言模型（VLMs）在19种图像损坏类型下的鲁棒性，揭示了其在文本识别和对象推理任务中的不同脆弱性模式。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在理解和推理视觉与文本内容方面表现出色，但其对常见图像损坏的鲁棒性尚未得到充分研究。

Method: 研究使用ImageNet-C基准的19种损坏类型，并引入TextVQA-C和GQA-C两个新基准，系统评估损坏对场景文本理解和对象推理的影响。

Result: 分析发现，基于Transformer的VLMs在不同任务中表现出独特的脆弱性模式：文本识别在模糊和雪损坏下表现最差，而对象推理对霜冻和脉冲噪声更敏感。

Conclusion: 研究揭示了Transformer的低频处理偏好如何解释其鲁棒性差异，为开发更鲁棒的视觉语言模型提供了重要见解。

Abstract: Vision-language models (VLMs) have demonstrated impressive capabilities in
understanding and reasoning about visual and textual content. However, their
robustness to common image corruptions remains under-explored. In this work, we
present the first comprehensive analysis of VLM robustness across 19 corruption
types from the ImageNet-C benchmark, spanning four categories: noise, blur,
weather, and digital distortions. We introduce two new benchmarks, TextVQA-C
and GQA-C, to systematically evaluate how corruptions affect scene text
understanding and object-based reasoning, respectively. Our analysis reveals
that transformer-based VLMs exhibit distinct vulnerability patterns across
tasks: text recognition deteriorates most severely under blur and snow
corruptions, while object reasoning shows higher sensitivity to corruptions
such as frost and impulse noise. We connect these observations to the
frequency-domain characteristics of different corruptions, revealing how
transformers' inherent bias toward low-frequency processing explains their
differential robustness patterns. Our findings provide valuable insights for
developing more corruption-robust vision-language models for real-world
applications.

</details>

### [186] [Zebrafish Counting Using Event Stream Data](https://arxiv.org/abs/2504.13692)
*Qianghua Chen,Huiyu Wang,Li Ming,Ying Zhao*

Main category: cs.CV

TLDR: 提出了一种基于事件流数据的斑马鱼计数算法，通过事件相机采集数据，结合轨迹信息提高计数精度，平均准确率达97.95%。


<details>
  <summary>Details</summary>
Motivation: 斑马鱼在生物医学研究中常用作模型生物，但其体型微小，手动计数困难，现有方法存在局限性。

Method: 使用事件相机采集数据，进行相机校准和图像融合，利用轨迹信息优化计数，最终取平均值并四舍五入。

Result: 在100次试验中，平均准确率为97.95%，优于传统算法。

Conclusion: 该算法实现简单且精度高，适用于斑马鱼计数。

Abstract: Zebrafish share a high degree of homology with human genes and are commonly
used as model organism in biomedical research. For medical laboratories,
counting zebrafish is a daily task. Due to the tiny size of zebrafish, manual
visual counting is challenging. Existing counting methods are either not
applicable to small fishes or have too many limitations. The paper proposed a
zebrafish counting algorithm based on the event stream data. Firstly, an event
camera is applied for data acquisition. Secondly, camera calibration and image
fusion were preformed successively. Then, the trajectory information was used
to improve the counting accuracy. Finally, the counting results were averaged
over an empirical of period and rounded up to get the final results. To
evaluate the accuracy of the algorithm, 20 zebrafish were put in a four-liter
breeding tank. Among 100 counting trials, the average accuracy reached 97.95%.
As compared with traditional algorithms, the proposed one offers a simpler
implementation and achieves higher accuracy.

</details>

### [187] [Few-Shot Referring Video Single- and Multi-Object Segmentation via Cross-Modal Affinity with Instance Sequence Matching](https://arxiv.org/abs/2504.13710)
*Heng Liu,Guanghui Li,Mingqi Gao,Xiantong Zhen,Feng Zheng,Yang Wang*

Main category: cs.CV

TLDR: FS-RVOS和FS-RVMOS是基于Transformer的模型，通过跨模态亲和模块和实例序列匹配策略，在视频对象分割任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决基于自然语言描述的视频对象分割问题，提升多对象分割的准确性和鲁棒性。

Method: 采用Transformer架构，结合跨模态亲和模块和实例序列匹配策略。

Result: 在多个基准测试中表现优于现有方法，展示了更高的准确性和鲁棒性。

Conclusion: FS-RVOS和FS-RVMOS为视频对象分割任务提供了有效的解决方案，尤其在多对象分割中表现突出。

Abstract: Referring video object segmentation (RVOS) aims to segment objects in videos
guided by natural language descriptions. We propose FS-RVOS, a
Transformer-based model with two key components: a cross-modal affinity module
and an instance sequence matching strategy, which extends FS-RVOS to
multi-object segmentation (FS-RVMOS). Experiments show FS-RVOS and FS-RVMOS
outperform state-of-the-art methods across diverse benchmarks, demonstrating
superior robustness and accuracy.

</details>

### [188] [Human-aligned Deep Learning: Explainability, Causality, and Biological Inspiration](https://arxiv.org/abs/2504.13717)
*Gianluca Carloni*

Main category: cs.CV

TLDR: 该研究将深度学习与人类推理能力结合，提出更高效、可解释且鲁棒的图像分类方法，涵盖可解释性、因果性和生物视觉三个视角。


<details>
  <summary>Details</summary>
Motivation: 通过结合深度学习与人类推理能力，提升医疗图像分类的效率、可解释性和鲁棒性，促进临床应用的信任与安全性。

Method: 1. 评估神经网络可视化技术并验证可解释性方法；2. 提出因果性模块和框架CROCODILE；3. 设计生物视觉启发的网络CoCoReco。

Result: 1. 激活最大化对医疗图像模型效果有限；2. 原型学习有效且符合放射学；3. XAI与因果机器学习紧密相关；4. 弱因果信号可提升性能；5. 框架具有跨领域泛化能力；6. 生物电路设计提升识别能力。

Conclusion: 该研究推动了人类对齐的深度学习，为临床应用的信任、准确性和安全性提供了新途径。

Abstract: This work aligns deep learning (DL) with human reasoning capabilities and
needs to enable more efficient, interpretable, and robust image classification.
We approach this from three perspectives: explainability, causality, and
biological vision. Introduction and background open this work before diving
into operative chapters. First, we assess neural networks' visualization
techniques for medical images and validate an explainable-by-design method for
breast mass classification. A comprehensive review at the intersection of XAI
and causality follows, where we introduce a general scaffold to organize past
and future research, laying the groundwork for our second perspective. In the
causality direction, we propose novel modules that exploit feature
co-occurrence in medical images, leading to more effective and explainable
predictions. We further introduce CROCODILE, a general framework that
integrates causal concepts, contrastive learning, feature disentanglement, and
prior knowledge to enhance generalization. Lastly, we explore biological
vision, examining how humans recognize objects, and propose CoCoReco, a
connectivity-inspired network with context-aware attention mechanisms. Overall,
our key findings include: (i) simple activation maximization lacks insight for
medical imaging DL models; (ii) prototypical-part learning is effective and
radiologically aligned; (iii) XAI and causal ML are deeply connected; (iv) weak
causal signals can be leveraged without a priori information to improve
performance and interpretability; (v) our framework generalizes across medical
domains and out-of-distribution data; (vi) incorporating biological circuit
motifs improves human-aligned recognition. This work contributes toward
human-aligned DL and highlights pathways to bridge the gap between research and
clinical adoption, with implications for improved trust, diagnostic accuracy,
and safe deployment.

</details>

### [189] [MLEP: Multi-granularity Local Entropy Patterns for Universal AI-generated Image Detection](https://arxiv.org/abs/2504.13726)
*Lin Yuan,Xiaowan Li,Yan Zhang,Jiawei Zhang,Hongbo Li,Xinbo Gao*

Main category: cs.CV

TLDR: 该论文提出了一种基于图像熵的多粒度局部熵模式（MLEP）方法，用于检测AI生成的图像（AIGI），解决了现有方法在跨模型和场景中泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的进步，AI生成图像的滥用（如虚假信息和深度伪造）引发了严重担忧，亟需有效的检测方法。现有方法因缺乏源不变特征和泛化能力有限而面临挑战。

Method: 提出MLEP方法，通过计算多尺度图像块的重排熵特征图，捕捉像素关系并减少内容偏差，结合CNN分类器进行检测。

Result: 在开放场景下对32种生成模型的图像进行实验，MLEP方法在准确性和泛化能力上显著优于现有技术。

Conclusion: MLEP为AIGI检测提供了一种高效且泛化能力强的解决方案，有望应对多样化的生成模型和场景。

Abstract: Advancements in image generation technologies have raised significant
concerns about their potential misuse, such as producing misinformation and
deepfakes. Therefore, there is an urgent need for effective methods to detect
AI-generated images (AIGI). Despite progress in AIGI detection, achieving
reliable performance across diverse generation models and scenes remains
challenging due to the lack of source-invariant features and limited
generalization capabilities in existing methods. In this work, we explore the
potential of using image entropy as a cue for AIGI detection and propose
Multi-granularity Local Entropy Patterns (MLEP), a set of entropy feature maps
computed across shuffled small patches over multiple image scaled. MLEP
comprehensively captures pixel relationships across dimensions and scales while
significantly disrupting image semantics, reducing potential content bias.
Leveraging MLEP, a robust CNN-based classifier for AIGI detection can be
trained. Extensive experiments conducted in an open-world scenario, evaluating
images synthesized by 32 distinct generative models, demonstrate significant
improvements over state-of-the-art methods in both accuracy and generalization.

</details>

### [190] [LimitNet: Progressive, Content-Aware Image Offloading for Extremely Weak Devices & Networks](https://arxiv.org/abs/2504.13736)
*Ali Hojjat,Janek Haberer,Tayyaba Zainab,Olaf Landsiedel*

Main category: cs.CV

TLDR: LimitNet是一种为弱设备和网络设计的渐进式图像压缩模型，显著提升了部分数据传输时的推理准确性并节省带宽。


<details>
  <summary>Details</summary>
Motivation: IoT设备硬件能力有限且常部署在偏远地区，传统方法在低带宽和高丢包率的LPWANs网络中难以实现高效的任务卸载。

Method: LimitNet采用轻量级渐进编码器，根据图像内容优先传输关键数据，支持云端在部分数据可用时进行推理。

Result: LimitNet在ImageNet1000、CIFAR100和COCO数据集上分别比SOTA提升14.01%、18.01%和0.1 mAP@0.5，同时节省61.24%、83.68%和42.25%带宽。

Conclusion: LimitNet在弱设备和网络中实现了高效的任务卸载，显著提升了推理性能和带宽利用率。

Abstract: IoT devices have limited hardware capabilities and are often deployed in
remote areas. Consequently, advanced vision models surpass such devices'
processing and storage capabilities, requiring offloading of such tasks to the
cloud. However, remote areas often rely on LPWANs technology with limited
bandwidth, high packet loss rates, and extremely low duty cycles, which makes
fast offloading for time-sensitive inference challenging. Today's approaches,
which are deployable on weak devices, generate a non-progressive bit stream,
and therefore, their decoding quality suffers strongly when data is only
partially available on the cloud at a deadline due to limited bandwidth or
packet losses.
  In this paper, we introduce LimitNet, a progressive, content-aware image
compression model designed for extremely weak devices and networks. LimitNet's
lightweight progressive encoder prioritizes critical data during transmission
based on the content of the image, which gives the cloud the opportunity to run
inference even with partial data availability.
  Experimental results demonstrate that LimitNet, on average, compared to SOTA,
achieves 14.01 p.p. (percentage point) higher accuracy on ImageNet1000, 18.01
pp on CIFAR100, and 0.1 higher mAP@0.5 on COCO. Also, on average, LimitNet
saves 61.24% bandwidth on ImageNet1000, 83.68% on CIFAR100, and 42.25% on the
COCO dataset compared to SOTA, while it only has 4% more encoding time compared
to JPEG (with a fixed quality) on STM32F7 (Cortex-M7).

</details>

### [191] [ESPLoRA: Enhanced Spatial Precision with Low-Rank Adaption in Text-to-Image Diffusion Models for High-Definition Synthesis](https://arxiv.org/abs/2504.13745)
*Andrea Rigo,Luca Stornaiuolo,Mauro Martino,Bruno Lepri,Nicu Sebe*

Main category: cs.CV

TLDR: 论文提出了一种名为ESPLoRA的微调框架，通过低秩适应技术提升文本到图像生成模型的空间一致性，同时引入新的评估指标和TORE算法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在文本到图像合成中难以准确表达空间关系，且传统方法计算成本高、灵活性低。

Method: 基于从LAION-400M提取的空间明确提示数据集，开发了ESPLoRA框架和TORE算法，并设计了基于几何约束的评估指标。

Result: ESPLoRA在空间一致性基准上比当前最优方法CoMPaSS提升了13.33%。

Conclusion: 该方法有效提升了生成图像的空间一致性，且不增加生成时间或降低输出质量。

Abstract: Diffusion models have revolutionized text-to-image (T2I) synthesis, producing
high-quality, photorealistic images. However, they still struggle to properly
render the spatial relationships described in text prompts. To address the lack
of spatial information in T2I generations, existing methods typically use
external network conditioning and predefined layouts, resulting in higher
computational costs and reduced flexibility. Our approach builds upon a curated
dataset of spatially explicit prompts, meticulously extracted and synthesized
from LAION-400M to ensure precise alignment between textual descriptions and
spatial layouts. Alongside this dataset, we present ESPLoRA, a flexible
fine-tuning framework based on Low-Rank Adaptation, specifically designed to
enhance spatial consistency in generative models without increasing generation
time or compromising the quality of the outputs. In addition to ESPLoRA, we
propose refined evaluation metrics grounded in geometric constraints, capturing
3D spatial relations such as \textit{in front of} or \textit{behind}. These
metrics also expose spatial biases in T2I models which, even when not fully
mitigated, can be strategically exploited by our TORE algorithm to further
improve the spatial consistency of generated images. Our method outperforms the
current state-of-the-art framework, CoMPaSS, by 13.33% on established spatial
consistency benchmarks.

</details>

### [192] [DAM-Net: Domain Adaptation Network with Micro-Labeled Fine-Tuning for Change Detection](https://arxiv.org/abs/2504.13748)
*Hongjia Chen,Xin Xu,Fangling Pu*

Main category: cs.CV

TLDR: DAM-Net提出了一种结合对抗域适应和微标记微调的遥感图像变化检测方法，显著提升了跨数据集性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在变化检测中因域适应性差而需要大量标注数据，限制了实际应用。

Method: DAM-Net采用对抗域适应和微标记微调策略，结合多时相Transformer和优化主干结构。

Result: 在LEVIR-CD和WHU-CD数据集上，DAM-Net性能优于现有方法，仅需0.3%标注数据即可媲美需10%标注的半监督方法。

Conclusion: DAM-Net为遥感图像变化检测提供了一种高效的跨数据集适应新范式。

Abstract: Change detection (CD) in remote sensing imagery plays a crucial role in
various applications such as urban planning, damage assessment, and resource
management. While deep learning approaches have significantly advanced CD
performance, current methods suffer from poor domain adaptability, requiring
extensive labeled data for retraining when applied to new scenarios. This
limitation severely restricts their practical applications across different
datasets. In this work, we propose DAM-Net: a Domain Adaptation Network with
Micro-Labeled Fine-Tuning for CD. Our network introduces adversarial domain
adaptation to CD for, utilizing a specially designed segmentation-discriminator
and alternating training strategy to enable effective transfer between domains.
Additionally, we propose a novel Micro-Labeled Fine-Tuning approach that
strategically selects and labels a minimal amount of samples (less than 1%) to
enhance domain adaptation. The network incorporates a Multi-Temporal
Transformer for feature fusion and optimized backbone structure based on
previous research. Experiments conducted on the LEVIR-CD and WHU-CD datasets
demonstrate that DAM-Net significantly outperforms existing domain adaptation
methods, achieving comparable performance to semi-supervised approaches that
require 10% labeled data while using only 0.3% labeled samples. Our approach
significantly advances cross-dataset CD applications and provides a new
paradigm for efficient domain adaptation in remote sensing. The source code of
DAM-Net will be made publicly available upon publication.

</details>

### [193] [Towards Accurate and Interpretable Neuroblastoma Diagnosis via Contrastive Multi-scale Pathological Image Analysis](https://arxiv.org/abs/2504.13754)
*Zhu Zhu,Shuo Jiang,Jingyuan Zheng,Yawen Li,Yifei Chen,Manli Zhao,Weizhong Gu,Feiwei Qin,Jinhu Wang,Gang Yu*

Main category: cs.CV

TLDR: CMSwinKAN是一种基于对比学习的多尺度特征融合模型，用于病理图像分类，显著提高了可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 神经母细胞瘤的诊断依赖主观人工检查，现有自动化方法存在可解释性差、特征提取能力有限和高计算成本等问题。

Method: 结合Swin Transformer架构和Kernel Activation Network，融合多尺度特征并利用对比学习策略，模拟临床医生的综合方法。

Result: 在PpNTs和BreakHis数据集上表现优于现有最先进的病理特定模型。

Conclusion: CMSwinKAN在病理图像分类中具有显著优势，为临床部署提供了可行方案。

Abstract: Neuroblastoma, adrenal-derived, is among the most common pediatric solid
malignancies, characterized by significant clinical heterogeneity. Timely and
accurate pathological diagnosis from hematoxylin and eosin-stained whole slide
images is critical for patient prognosis. However, current diagnostic practices
primarily rely on subjective manual examination by pathologists, leading to
inconsistent accuracy. Existing automated whole slide image classification
methods encounter challenges such as poor interpretability, limited feature
extraction capabilities, and high computational costs, restricting their
practical clinical deployment. To overcome these limitations, we propose
CMSwinKAN, a contrastive-learning-based multi-scale feature fusion model
tailored for pathological image classification, which enhances the Swin
Transformer architecture by integrating a Kernel Activation Network within its
multilayer perceptron and classification head modules, significantly improving
both interpretability and accuracy. By fusing multi-scale features and
leveraging contrastive learning strategies, CMSwinKAN mimics clinicians'
comprehensive approach, effectively capturing global and local tissue
characteristics. Additionally, we introduce a heuristic soft voting mechanism
guided by clinical insights to seamlessly bridge patch-level predictions to
whole slide image-level classifications. We validate CMSwinKAN on the PpNTs
dataset, which was collaboratively established with our partner hospital and
the publicly accessible BreakHis dataset. Results demonstrate that CMSwinKAN
performs better than existing state-of-the-art pathology-specific models
pre-trained on large datasets. Our source code is available at
https://github.com/JSLiam94/CMSwinKAN.

</details>

### [194] [Fragile Watermarking for Image Certification Using Deep Steganographic Embedding](https://arxiv.org/abs/2504.13759)
*Davide Ghiani,Jefferson David Rodriguez Chivata,Stefano Lilliu,Simone Maurizio La Cava,Marco Micheletto,Giulia Orrù,Federico Lama,Gian Luca Marcialis*

Main category: cs.CV

TLDR: 该论文探讨了基于深度隐写嵌入的脆弱水印技术，用于验证ICAO标准面部图像的完整性，通过嵌入隐藏图像作为标记，检测图像篡改，并展示了高检测准确性。


<details>
  <summary>Details</summary>
Motivation: 现代身份验证系统依赖面部图像，但图像可能因无意或恶意操作而受损或被篡改，威胁识别系统安全。

Method: 提出基于深度隐写嵌入的脆弱水印技术，嵌入隐藏图像作为完整性标记，并设计分类框架分析篡改类型。

Result: 实验表明，该方法能高精度检测图像篡改，包括跨方法场景。

Conclusion: 深度隐写嵌入的脆弱水印技术是验证生物特征文档完整性的有效工具。

Abstract: Modern identity verification systems increasingly rely on facial images
embedded in biometric documents such as electronic passports. To ensure global
interoperability and security, these images must comply with strict standards
defined by the International Civil Aviation Organization (ICAO), which specify
acquisition, quality, and format requirements. However, once issued, these
images may undergo unintentional degradations (e.g., compression, resizing) or
malicious manipulations (e.g., morphing) and deceive facial recognition
systems. In this study, we explore fragile watermarking, based on deep
steganographic embedding as a proactive mechanism to certify the authenticity
of ICAO-compliant facial images. By embedding a hidden image within the
official photo at the time of issuance, we establish an integrity marker that
becomes sensitive to any post-issuance modification. We assess how a range of
image manipulations affects the recovered hidden image and show that
degradation artifacts can serve as robust forensic cues. Furthermore, we
propose a classification framework that analyzes the revealed content to detect
and categorize the type of manipulation applied. Our experiments demonstrate
high detection accuracy, including cross-method scenarios with multiple deep
steganography-based models. These findings support the viability of fragile
watermarking via steganographic embedding as a valuable tool for biometric
document integrity verification.

</details>

### [195] [Decoding Vision Transformers: the Diffusion Steering Lens](https://arxiv.org/abs/2504.13763)
*Ryota Takatsuki,Sonia Joseph,Ippei Fujisawa,Ryota Kanai*

Main category: cs.CV

TLDR: 提出了一种名为Diffusion Steering Lens (DSL)的新方法，用于改进Vision Transformers (ViTs)内部处理的解释性，克服了Logit Lens和Diffusion Lens的局限性。


<details>
  <summary>Details</summary>
Motivation: Logit Lens和Diffusion Lens在分析ViTs内部表示时存在不足，无法捕捉子模块的直接贡献。

Method: 提出DSL方法，无需训练，通过引导子模块输出并修补后续间接贡献来改进解释性。

Result: 通过干预研究验证，DSL能够直观可靠地解释ViTs的内部处理。

Conclusion: DSL是一种有效的工具，能够提升ViTs内部处理的解释性。

Abstract: Logit Lens is a widely adopted method for mechanistic interpretability of
transformer-based language models, enabling the analysis of how internal
representations evolve across layers by projecting them into the output
vocabulary space. Although applying Logit Lens to Vision Transformers (ViTs) is
technically straightforward, its direct use faces limitations in capturing the
richness of visual representations. Building on the work of Toker et al.
(2024)~\cite{Toker2024-ve}, who introduced Diffusion Lens to visualize
intermediate representations in the text encoders of text-to-image diffusion
models, we demonstrate that while Diffusion Lens can effectively visualize
residual stream representations in image encoders, it fails to capture the
direct contributions of individual submodules. To overcome this limitation, we
propose \textbf{Diffusion Steering Lens} (DSL), a novel, training-free approach
that steers submodule outputs and patches subsequent indirect contributions. We
validate our method through interventional studies, showing that DSL provides
an intuitive and reliable interpretation of the internal processing in ViTs.

</details>

### [196] [Fighting Fires from Space: Leveraging Vision Transformers for Enhanced Wildfire Detection and Characterization](https://arxiv.org/abs/2504.13776)
*Aman Agarwal,James Gearon,Raksha Rank,Etienne Chenevert*

Main category: cs.CV

TLDR: 论文探讨了使用Vision Transformers（ViTs）和CNNs进行野火检测的性能比较，发现ViTs表现优于部分CNNs，但优化后的UNet（CNN）仍是最佳方法。


<details>
  <summary>Details</summary>
Motivation: 由于气候变化导致野火频发，现有检测系统难以应对，需要更高效的自动化检测方法。

Method: 比较了ViTs和CNNs在野火检测中的性能，使用Landsat-8卫星图像数据集。

Result: ViTs表现优于基线CNN（提升0.92%），但优化后的UNet（CNN）在所有指标中表现最佳（IoU达93.58%）。

Conclusion: ViTs在野火检测中表现与CNNs相当，但优化后的CNNs（如UNet）仍是当前最佳选择。

Abstract: Wildfires are increasing in intensity, frequency, and duration across large
parts of the world as a result of anthropogenic climate change. Modern hazard
detection and response systems that deal with wildfires are under-equipped for
sustained wildfire seasons. Recent work has proved automated wildfire detection
using Convolutional Neural Networks (CNNs) trained on satellite imagery are
capable of high-accuracy results. However, CNNs are computationally expensive
to train and only incorporate local image context. Recently, Vision
Transformers (ViTs) have gained popularity for their efficient training and
their ability to include both local and global contextual information. In this
work, we show that ViT can outperform well-trained and specialized CNNs to
detect wildfires on a previously published dataset of LandSat-8 imagery. One of
our ViTs outperforms the baseline CNN comparison by 0.92%. However, we find our
own implementation of CNN-based UNet to perform best in every category, showing
their sustained utility in image tasks. Overall, ViTs are comparably capable in
detecting wildfires as CNNs, though well-tuned CNNs are still the best
technique for detecting wildfire with our UNet providing an IoU of 93.58%,
better than the baseline UNet by some 4.58%.

</details>

### [197] [RefComp: A Reference-guided Unified Framework for Unpaired Point Cloud Completion](https://arxiv.org/abs/2504.13788)
*Yixuan Yang,Jinyu Yang,Zixiang Zhao,Victor Sanchez,Feng Zheng*

Main category: cs.CV

TLDR: 提出了一种名为RefComp的新型无配对点云补全框架，通过将补全问题转化为形状转换问题，在潜在特征空间中解决，并在类感知和类无关训练设置中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有无配对点云补全方法需要为每个对象类别单独训练模型，泛化能力有限，难以应对现实场景中多样化的3D对象。

Method: RefComp框架通过检索部分-完整点云对作为参考数据，利用参考分支和目标分支共享参数的形状融合模块（LSFM）进行结构特征增强。

Result: 在虚拟扫描和真实数据集上，RefComp在类感知和类无关训练设置中均达到最优或竞争性性能。

Conclusion: RefComp框架显著提升了无配对点云补全的泛化能力和性能，适用于更广泛的3D对象场景。

Abstract: The unpaired point cloud completion task aims to complete a partial point
cloud by using models trained with no ground truth. Existing unpaired point
cloud completion methods are class-aware, i.e., a separate model is needed for
each object class. Since they have limited generalization capabilities, these
methods perform poorly in real-world scenarios when confronted with a wide
range of point clouds of generic 3D objects. In this paper, we propose a novel
unpaired point cloud completion framework, namely the Reference-guided
Completion (RefComp) framework, which attains strong performance in both the
class-aware and class-agnostic training settings. The RefComp framework
transforms the unpaired completion problem into a shape translation problem,
which is solved in the latent feature space of the partial point clouds. To
this end, we introduce the use of partial-complete point cloud pairs, which are
retrieved by using the partial point cloud to be completed as a template. These
point cloud pairs are used as reference data to guide the completion process.
Our RefComp framework uses a reference branch and a target branch with shared
parameters for shape fusion and shape translation via a Latent Shape Fusion
Module (LSFM) to enhance the structural features along the completion pipeline.
Extensive experiments demonstrate that the RefComp framework achieves not only
state-of-the-art performance in the class-aware training setting but also
competitive results in the class-agnostic training setting on both virtual
scans and real-world datasets.

</details>

### [198] [CheXWorld: Exploring Image World Modeling for Radiograph Representation Learning](https://arxiv.org/abs/2504.13820)
*Yang Yue,Yulin Wang,Chenxin Tao,Pan Liu,Shiji Song,Gao Huang*

Main category: cs.CV

TLDR: CheXWorld是首个针对放射影像的自监督世界模型，通过建模局部解剖结构、全局解剖布局和领域变化，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 人类通过内部世界模型理解世界，这一概念为机器学习提供了新方向。本文旨在为放射影像建立类似的自监督世界模型。

Method: 提出统一框架，建模局部解剖结构、全局解剖布局和领域变化，并通过定性和定量分析验证。

Result: CheXWorld成功捕捉了医学知识的三个维度，在多个基准测试中显著优于现有方法。

Conclusion: CheXWorld为放射影像的自监督学习提供了有效框架，具有广泛的应用潜力。

Abstract: Humans can develop internal world models that encode common sense knowledge,
telling them how the world works and predicting the consequences of their
actions. This concept has emerged as a promising direction for establishing
general-purpose machine-learning models in recent preliminary works, e.g., for
visual representation learning. In this paper, we present CheXWorld, the first
effort towards a self-supervised world model for radiographic images.
Specifically, our work develops a unified framework that simultaneously models
three aspects of medical knowledge essential for qualified radiologists,
including 1) local anatomical structures describing the fine-grained
characteristics of local tissues (e.g., architectures, shapes, and textures);
2) global anatomical layouts describing the global organization of the human
body (e.g., layouts of organs and skeletons); and 3) domain variations that
encourage CheXWorld to model the transitions across different appearance
domains of radiographs (e.g., varying clarity, contrast, and exposure caused by
collecting radiographs from different hospitals, devices, or patients).
Empirically, we design tailored qualitative and quantitative analyses,
revealing that CheXWorld successfully captures these three dimensions of
medical knowledge. Furthermore, transfer learning experiments across eight
medical image classification and segmentation benchmarks showcase that
CheXWorld significantly outperforms existing SSL methods and large-scale
medical foundation models. Code & pre-trained models are available at
https://github.com/LeapLabTHU/CheXWorld.

</details>

### [199] [Outlier-Robust Multi-Model Fitting on Quantum Annealers](https://arxiv.org/abs/2504.13836)
*Saurabh Pandey,Luca Magri,Federica Arrigoni,Vladislav Golyanik*

Main category: cs.CV

TLDR: 本文提出了一种鲁棒的量子多模型拟合算法（R-QuMF），用于有效处理异常值，无需预先知道模型数量，并在合成和真实3D数据集中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多模型拟合（MMF）在计算机视觉中具有组合复杂性，现有量子方法仅适用于单模型或无异常值场景，亟需改进。

Method: R-QuMF利用量子硬件的固有能力，将问题建模为绝热量子计算机（AQC）的最大集合覆盖任务。

Result: R-QuMF在合成和真实3D数据集中表现优于现有量子技术。

Conclusion: 量子计算在解决复杂MMF问题，尤其是含噪声和异常值的实际场景中具有潜力。

Abstract: Multi-model fitting (MMF) presents a significant challenge in Computer
Vision, particularly due to its combinatorial nature. While recent advancements
in quantum computing offer promise for addressing NP-hard problems, existing
quantum-based approaches for model fitting are either limited to a single model
or consider multi-model scenarios within outlier-free datasets. This paper
introduces a novel approach, the robust quantum multi-model fitting (R-QuMF)
algorithm, designed to handle outliers effectively. Our method leverages the
intrinsic capabilities of quantum hardware to tackle combinatorial challenges
inherent in MMF tasks, and it does not require prior knowledge of the exact
number of models, thereby enhancing its practical applicability. By formulating
the problem as a maximum set coverage task for adiabatic quantum computers
(AQC), R-QuMF outperforms existing quantum techniques, demonstrating superior
performance across various synthetic and real-world 3D datasets. Our findings
underscore the potential of quantum computing in addressing the complexities of
MMF, especially in real-world scenarios with noisy and outlier-prone data.

</details>

<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [200] [MetaDSE: A Few-shot Meta-learning Framework for Cross-workload CPU Design Space Exploration](https://arxiv.org/abs/2504.13568)
*Runzhen Xue,Hao Wu,Mingyu Yan,Ziheng Xiao,Xiaochun Ye,Dongrui Fan*

Main category: cs.AR

TLDR: MetaDSE通过元学习框架解决跨工作负载CPU设计空间探索中的过拟合和数据模糊问题，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有跨工作负载设计空间探索方法存在过拟合、数据模糊和工作负载差异问题，需要更高效的解决方案。

Method: 将任务重构为少样本元学习问题，提出MetaDSE，利用模型无关元学习和工作负载自适应架构掩码算法。

Result: 在SPEC CPU 2017上，MetaDSE将预测误差降低44.3%，优于现有方法。

Conclusion: MetaDSE通过元学习和创新知识转移方法，显著提升了跨工作负载CPU设计空间探索的效率和准确性。

Abstract: Cross-workload design space exploration (DSE) is crucial in CPU architecture
design. Existing DSE methods typically employ the transfer learning technique
to leverage knowledge from source workloads, aiming to minimize the requirement
of target workload simulation. However, these methods struggle with
overfitting, data ambiguity, and workload dissimilarity.
  To address these challenges, we reframe the cross-workload CPU DSE task as a
few-shot meta-learning problem and further introduce MetaDSE. By leveraging
model agnostic meta-learning, MetaDSE swiftly adapts to new target workloads,
greatly enhancing the efficiency of cross-workload CPU DSE. Additionally,
MetaDSE introduces a novel knowledge transfer method called the
workload-adaptive architectural mask algorithm, which uncovers the inherent
properties of the architecture. Experiments on SPEC CPU 2017 demonstrate that
MetaDSE significantly reduces prediction error by 44.3\% compared to the
state-of-the-art. MetaDSE is open-sourced and available at this
\href{https://anonymous.4open.science/r/Meta_DSE-02F8}{anonymous GitHub.}

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [201] [Factors That Influence the Adoption of AI-enabled Conversational Agents (AICAs) as an Augmenting Therapeutic Tool by Frontline Healthcare Workers: From Technology Acceptance Model 3 (TAM3) Lens -- A Systematic Mapping Review](https://arxiv.org/abs/2504.13183)
*Rawan AlMakinah*

Main category: cs.HC

TLDR: 本文探讨了AI对话代理在心理健康领域的潜力，尤其关注其对边缘化群体的支持，并强调需通过系统性文献综述研究其可行性及心理健康专业人士的态度。


<details>
  <summary>Details</summary>
Motivation: AI对话代理可能为缺乏心理健康服务的边缘化群体提供支持，但其可行性需深入研究，尤其是从心理健康专业人士的角度。

Method: 采用系统性文献综述方法，结合TAM3框架，分析心理健康从业者对AI对话代理的态度及其影响因素。

Result: 预计通过综述揭示心理健康专业人士对AI技术的采纳意愿、担忧及推荐因素。

Conclusion: 研究旨在为AI对话代理的开发与部署提供指导框架，并促进其在心理健康领域的合理应用。

Abstract: Artificial intelligent (AI) conversational agents hold a promising future in
the field of mental health, especially in helping marginalized communities that
lack access to mental health support services. It is tempting to have a 24/7
mental health companion that can be accessed anywhere using mobile phones to
provide therapist-like advice. Yet, caution should be taken, and studies around
their feasibility need to be surveyed. Before adopting such a rapidly changing
technology, studies on its feasibility should be explored, summarized, and
synthesized to gain a solid understanding of the status quo and to enable us to
build a framework that can guide us throughout the development and deployment
processes. Different perspectives must be considered when investigating the
feasibility of AI conversational agents, including the mental healthcare
professional perspective. The literature can provide insights into their
perspectives in terms of opportunities, concerns, and implications. Mental
health professionals, the subject-matter experts in this field, have their
points of view that should be understood and considered. This systematic
literature review will explore mental health practitioners' attitudes toward AI
conversational agents and the factors that affect their adoption and
recommendation of the technology to augment their services and treatments. The
TAM3 Framework will be the lens through which this systematic literature review
will be conducted.

</details>

### [202] [Interpersonal Theory of Suicide as a Lens to Examine Suicidal Ideation in Online Spaces](https://arxiv.org/abs/2504.13277)
*Soorya Ram Shimgekar,Violeta J. Rodriguez,Paul A. Bloom,Dong Whi Yoo,Koustuv Saha*

Main category: cs.HC

TLDR: 该研究利用自杀的人际关系理论（IPTS）分析Reddit上的自杀意念帖子，发现高风险帖子表达计划、方法和痛苦，并探讨了AI聊天机器人在提供支持时的局限性。


<details>
  <summary>Details</summary>
Motivation: 在线空间为表达自杀意念提供了平台，但缺乏理解高风险自杀意图的理论框架。

Method: 采用IPTS理论分析59,607篇Reddit帖子，分类为自杀意念维度和风险因素，并分析支持性回应的语言特征。

Result: 高风险帖子表达具体计划和痛苦，AI聊天机器人在支持性回应中缺乏动态和个性化。

Conclusion: 需更深入理解和开发AI干预措施，以提供有效的心理健康支持。

Abstract: Suicide is a critical global public health issue, with millions experiencing
suicidal ideation (SI) each year. Online spaces enable individuals to express
SI and seek peer support. While prior research has revealed the potential of
detecting SI using machine learning and natural language analysis, a key
limitation is the lack of a theoretical framework to understand the underlying
factors affecting high-risk suicidal intent. To bridge this gap, we adopted the
Interpersonal Theory of Suicide (IPTS) as an analytic lens to analyze 59,607
posts from Reddit's r/SuicideWatch, categorizing them into SI dimensions
(Loneliness, Lack of Reciprocal Love, Self Hate, and Liability) and risk
factors (Thwarted Belongingness, Perceived Burdensomeness, and Acquired
Capability of Suicide). We found that high-risk SI posts express planning and
attempts, methods and tools, and weaknesses and pain. In addition, we also
examined the language of supportive responses through psycholinguistic and
content analyses to find that individuals respond differently to different
stages of Suicidal Ideation (SI) posts. Finally, we explored the role of AI
chatbots in providing effective supportive responses to suicidal ideation
posts. We found that although AI improved structural coherence, expert
evaluations highlight persistent shortcomings in providing dynamic,
personalized, and deeply empathetic support. These findings underscore the need
for careful reflection and deeper understanding in both the development and
consideration of AI-driven interventions for effective mental health support.

</details>

### [203] [Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm](https://arxiv.org/abs/2504.13667)
*Russell Beale*

Main category: cs.HC

TLDR: 本文探讨了大型语言模型（LLMs）对儿童学习和技术交互方式的潜在深远影响，认为当前影响较小，但未来变化巨大。通过小规模场景和自我民族志研究，提出了未来交互系统设计需考虑的五个关键点。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs对儿童学习和与技术交互方式的潜在影响，揭示未来可能发生的重大变化。

Method: 结合文献回顾、小规模场景分析和自我民族志研究，探讨LLMs的影响。

Result: 当前LLMs对教育的影响较小，但未来变化将显著，需关注五个关键设计考虑。

Conclusion: LLMs将深刻改变儿童学习和技术交互方式，设计师需提前适应这些变化。

Abstract: This paper presents a hopeful perspective on the potentially dramatic impacts
of Large Language Models on how we children learn and how they will expect to
interact with technology. We review the effects of LLMs on education so far,
and make the case that these effects are minor compared to the upcoming changes
that are occurring. We present a small scenario and self-ethnographic study
demonstrating the effects of these changes, and define five significant
considerations that interactive systems designers will have to accommodate in
the future.

</details>

### [204] [Creating 'Full-Stack' Hybrid Reasoning Systems that Prioritize and Enhance Human Intelligence](https://arxiv.org/abs/2504.13477)
*Sean Koon*

Main category: cs.HC

TLDR: 论文提出通过生成式AI工具增强人类反思和技术探索能力，以解决混合智能中人类参与的不足。


<details>
  <summary>Details</summary>
Motivation: 人类推理可能存在缺陷，需提升人类在混合智能中的参与质量。

Method: 开发生成式AI工具，支持人类反思和技术探索，并提出整合AI与人类能力的高层模型。

Result: 提出了一种以人类为中心、结合AI能力的模型。

Conclusion: 增强人类智慧和参与是解决未来挑战的关键。

Abstract: The idea of augmented or hybrid intelligence offers a compelling vision for
combining human and AI capabilities, especially in tasks where human wisdom,
expertise, or common sense are essential. Unfortunately, human reasoning can be
flawed and shortsighted, resulting in adverse individual impacts or even
long-term societal consequences. While strong efforts are being made to develop
and optimize the AI aspect of hybrid reasoning, the real urgency lies in
fostering wiser and more intelligent human participation. Tools that enhance
critical thinking, ingenuity, expertise, and even wisdom could be essential in
addressing the challenges of our emerging future. This paper proposes the
development of generative AI-based tools that enhance both the human ability to
reflect upon a problem as well as the ability to explore the technical aspects
of it. A high-level model is also described for integrating AI and human
capabilities in a way that centralizes human participation and control.

</details>

### [205] [RAG Without the Lag: Interactive Debugging for Retrieval-Augmented Generation Pipelines](https://arxiv.org/abs/2504.13587)
*Quentin Romero Lauro,Shreya Shankar,Sepanta Zeighami,Aditya Parameswaran*

Main category: cs.HC

TLDR: RAGGY是一个开发工具，结合了可组合的RAG原语Python库和实时调试界面，解决了RAG管道开发中的调试难题。


<details>
  <summary>Details</summary>
Motivation: RAG管道中检索和生成组件相互交织，难以定位错误来源，且调试周期长，影响开发效率。

Method: 提出了RAGGY工具，包含Python库和交互式界面，支持实时调试，并通过12位工程师的定性研究验证其有效性。

Result: RAGGY帮助开发者快速定位和修复RAG管道中的问题，提升开发效率。

Conclusion: RAGGY为RAG工具设计提供了新思路，未来可进一步优化以更贴合开发者工作流程。

Abstract: Retrieval-augmented generation (RAG) pipelines have become the de-facto
approach for building AI assistants with access to external, domain-specific
knowledge. Given a user query, RAG pipelines typically first retrieve (R)
relevant information from external sources, before invoking a Large Language
Model (LLM), augmented (A) with this information, to generate (G) responses.
Modern RAG pipelines frequently chain multiple retrieval and generation
components, in any order. However, developing effective RAG pipelines is
challenging because retrieval and generation components are intertwined, making
it hard to identify which component(s) cause errors in the eventual output. The
parameters with the greatest impact on output quality often require hours of
pre-processing after each change, creating prohibitively slow feedback cycles.
To address these challenges, we present RAGGY, a developer tool that combines a
Python library of composable RAG primitives with an interactive interface for
real-time debugging. We contribute the design and implementation of RAGGY,
insights into expert debugging patterns through a qualitative study with 12
engineers, and design implications for future RAG tools that better align with
developers' natural workflows.

</details>

### [206] [Exploring Multimodal Prompt for Visualization Authoring with Large Language Models](https://arxiv.org/abs/2504.13700)
*Zhen Wen,Luoxuan Weng,Yinghao Tang,Runjin Zhang,Yuxin Liu,Bo Pan,Minfeng Zhu,Wei Chen*

Main category: cs.HC

TLDR: 该论文研究了如何通过多模态提示（文本、草图等）提升大语言模型（LLMs）在可视化创作中的意图理解能力，并开发了VisPilot工具验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 自然语言指令在可视化创作中存在表达不精确和易误解的问题，限制了LLMs的潜力。

Method: 通过实证研究分析LLMs对模糊文本提示的解读，并引入视觉提示作为补充输入方式，设计VisPilot工具进行验证。

Result: VisPilot在多模态提示下能更直观地创建可视化，且不影响任务效率。

Conclusion: 多模态提示能显著提升LLMs在可视化创作中的可用性，为未来系统设计提供了启示。

Abstract: Recent advances in large language models (LLMs) have shown great potential in
automating the process of visualization authoring through simple natural
language utterances. However, instructing LLMs using natural language is
limited in precision and expressiveness for conveying visualization intent,
leading to misinterpretation and time-consuming iterations. To address these
limitations, we conduct an empirical study to understand how LLMs interpret
ambiguous or incomplete text prompts in the context of visualization authoring,
and the conditions making LLMs misinterpret user intent. Informed by the
findings, we introduce visual prompts as a complementary input modality to text
prompts, which help clarify user intent and improve LLMs' interpretation
abilities. To explore the potential of multimodal prompting in visualization
authoring, we design VisPilot, which enables users to easily create
visualizations using multimodal prompts, including text, sketches, and direct
manipulations on existing visualizations. Through two case studies and a
controlled user study, we demonstrate that VisPilot provides a more intuitive
way to create visualizations without affecting the overall task efficiency
compared to text-only prompting approaches. Furthermore, we analyze the impact
of text and visual prompts in different visualization tasks. Our findings
highlight the importance of multimodal prompting in improving the usability of
LLMs for visualization authoring. We discuss design implications for future
visualization systems and provide insights into how multimodal prompts can
enhance human-AI collaboration in creative visualization tasks. All materials
are available at https://OSF.IO/2QRAK.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [207] [Adaptive Long-term Embedding with Denoising and Augmentation for Recommendation](https://arxiv.org/abs/2504.13614)
*Zahra Akhlaghi,Mostafa Haghir Chehreghani*

Main category: cs.IR

TLDR: ALDA4Rec是一种基于图神经网络的个性化推荐系统，通过噪声过滤和动态优化提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络在推荐系统中面临的噪声和静态表示问题。

Method: 构建物品图，通过社区检测过滤噪声，结合GCN、GRU和注意力机制学习短期和长期表示，并采用MLP动态优化。

Result: 在四个真实数据集上表现优于现有方法，提升了准确性和鲁棒性。

Conclusion: ALDA4Rec通过自适应长期嵌入和噪声处理，显著提升了推荐系统的性能。

Abstract: The rapid growth of the internet has made personalized recommendation systems
indispensable. Graph-based sequential recommendation systems, powered by Graph
Neural Networks (GNNs), effectively capture complex user-item interactions but
often face challenges such as noise and static representations. In this paper,
we introduce the Adaptive Long-term Embedding with Denoising and Augmentation
for Recommendation (ALDA4Rec) method, a novel model that constructs an
item-item graph, filters noise through community detection, and enriches
user-item interactions. Graph Convolutional Networks (GCNs) are then employed
to learn short-term representations, while averaging, GRUs, and attention
mechanisms are utilized to model long-term embeddings. An MLP-based adaptive
weighting strategy is further incorporated to dynamically optimize long-term
user preferences. Experiments conducted on four real-world datasets demonstrate
that ALDA4Rec outperforms state-of-the-art baselines, delivering notable
improvements in both accuracy and robustness. The source code is available at
https://github.com/zahraakhlaghi/ALDA4Rec.

</details>

<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [208] [Focus3D: A Practical Method to Adaptively Focus ISAR Data and Provide 3-D Information for Automatic Target Recognition](https://arxiv.org/abs/2504.13321)
*John R. Bennett*

Main category: eess.SP

TLDR: 本文提出了一种改进的ISAR处理器，用于提升海上船只的ATR识别能力，通过结合聚焦算法和船只姿态建模方法，解决了传统方法中姿态信息不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统ATR识别方法在船只姿态信息获取上存在局限，特别是在多角度和旋转平面未确定的情况下，影响了识别的准确性。本文旨在通过改进ISAR处理器，提供更全面的姿态信息。

Method: 结合聚焦算法和船只姿态建模方法，引入两个角度（方位角和倾斜角）来描述船只的旋转，从而更准确地确定船只的姿态。

Result: 改进后的ISAR处理器能够提供更全面的姿态信息，帮助ATR处理器更准确地匹配船只特征，缩小识别范围。

Conclusion: 通过结合聚焦算法和姿态建模方法，本文提出的方法显著提升了ATR识别的准确性和效率，为海上船只识别提供了更可靠的解决方案。

Abstract: To improve ATR identification of ships at sea requires an advanced ISAR
processor - one that not only provides focused images but can also determine
the pose of the ship. This tells us whether the image shows a profile (vertical
plane) view, a plan (horizontal plane) view or some view in between. If the
processor can provide this information, then the ATR processor can try to match
the images with known vertical or horizontal features of ships and, in
conjunction with estimated ship length, narrow the set of possible
identifications. This paper extends the work of Melendez and Bennett [M-B, Ref.
1] by combining a focus algorithm with a method that models the angles of the
ship relative to the radar. In M-B the algorithm was limited to a single angle
and the plane of rotation was not determined. This assumption may be fine for a
short time image where there is limited data available to determine the pose.
However, the present paper models the ship rotation with two angles - aspect
angle, representing rotation in the horizontal plane, and tilt angle,
representing variations in the effective grazing angle to the ship.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [209] [Ascribe New Dimensions to Scientific Data Visualization with VR](https://arxiv.org/abs/2504.13448)
*Daniela Ushizima,Guilherme Melo dos Santos,Zineb Sordo,Ronald Pandolfi,Jeffrey Donatelli*

Main category: cs.GR

TLDR: ASCRIBE-VR是一个结合AI与VR的科学可视化平台，旨在提升复杂3D图像的分析效率。


<details>
  <summary>Details</summary>
Motivation: 传统2D可视化方法限制了3D结构的直观分析，VR提供了沉浸式交互环境以增强数据理解。

Method: ASCRIBE-VR整合AI算法与科学图像，支持多模态分析、结构评估和沉浸式可视化。

Result: 平台兼容Meta Quest，支持AI分割结果的无缝探索，提升科学发现效率。

Conclusion: ASCRIBE-VR通过AI与VR的结合，填补了计算分析与人类直觉之间的鸿沟。

Abstract: For over half a century, the computer mouse has been the primary tool for
interacting with digital data, yet it remains a limiting factor in exploring
complex, multi-scale scientific images. Traditional 2D visualization methods
hinder intuitive analysis of inherently 3D structures. Virtual Reality (VR)
offers a transformative alternative, providing immersive, interactive
environments that enhance data comprehension. This article introduces
ASCRIBE-VR, a VR platform of Autonomous Solutions for Computational Research
with Immersive Browsing \& Exploration, which integrates AI-driven algorithms
with scientific images. ASCRIBE-VR enables multimodal analysis, structural
assessments, and immersive visualization, supporting scientific visualization
of advanced datasets such as X-ray CT, Magnetic Resonance, and synthetic 3D
imaging. Our VR tools, compatible with Meta Quest, can consume the output of
our AI-based segmentation and iterative feedback processes to enable seamless
exploration of large-scale 3D images. By merging AI-generated results with VR
visualization, ASCRIBE-VR enhances scientific discovery, bridging the gap
between computational analysis and human intuition in materials research,
connecting human-in-the-loop with digital twins.

</details>

### [210] [SMPL-GPTexture: Dual-View 3D Human Texture Estimation using Text-to-Image Generation Models](https://arxiv.org/abs/2504.13378)
*Mingxiao Tu,Shuchang Ye,Hoijoon Jung,Jinman Kim*

Main category: cs.GR

TLDR: 提出SMPL-GPTexture，通过文本生成高质量3D人体纹理，解决数据稀缺与生成模型缺陷问题。


<details>
  <summary>Details</summary>
Motivation: 真实人体前后图像数据稀缺且获取成本高，现有生成模型易产生瑕疵。

Method: 利用文本生成双视角图像，通过2D-3D对齐、逆向光栅化和扩散修复生成完整纹理。

Result: 实验表明SMPL-GPTexture能生成高分辨率且符合用户描述的纹理。

Conclusion: 该方法有效解决了纹理生成中的数据与质量问题。

Abstract: Generating high-quality, photorealistic textures for 3D human avatars remains
a fundamental yet challenging task in computer vision and multimedia field.
However, real paired front and back images of human subjects are rarely
available with privacy, ethical and cost of acquisition, which restricts
scalability of the data. Additionally, learning priors from image inputs using
deep generative models, such as GANs or diffusion models, to infer unseen
regions such as the human back often leads to artifacts, structural
inconsistencies, or loss of fine-grained detail. To address these issues, we
present SMPL-GPTexture (skinned multi-person linear model - general purpose
Texture), a novel pipeline that takes natural language prompts as input and
leverages a state-of-the-art text-to-image generation model to produce paired
high-resolution front and back images of a human subject as the starting point
for texture estimation. Using the generated paired dual-view images, we first
employ a human mesh recovery model to obtain a robust 2D-to-3D SMPL alignment
between image pixels and the 3D model's UV coordinates for each views. Second,
we use an inverted rasterization technique that explicitly projects the
observed colour from the input images into the UV space, thereby producing
accurate, complete texture maps. Finally, we apply a diffusion-based inpainting
module to fill in the missing regions, and the fusion mechanism then combines
these results into a unified full texture map. Extensive experiments shows that
our SMPL-GPTexture can generate high resolution texture aligned with user's
prompts.

</details>

### [211] [Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis](https://arxiv.org/abs/2504.13386)
*Radek Daněček,Carolin Schmitt,Senya Polikovsky,Michael J. Black*

Main category: cs.GR

TLDR: THUNDER提出了一种基于可微分语音重构的3D头部虚拟形象框架，结合确定性模型的唇同步准确性和随机模型的丰富表情生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有确定性模型唇同步质量高但缺乏表情多样性，随机模型表情丰富但唇同步质量低，需要结合两者优势。

Method: 开发了一种基于网格到语音模型和扩散模型的框架，通过可微分语音重构监督训练，提升唇同步质量。

Result: THUNDER显著提升了唇同步质量，同时保持了表情的多样性和高质量。

Conclusion: THUNDER通过新颖的监督机制，成功结合了唇同步准确性和表情多样性，为3D虚拟形象提供了更优解决方案。

Abstract: In order to be widely applicable, speech-driven 3D head avatars must
articulate their lips in accordance with speech, while also conveying the
appropriate emotions with dynamically changing facial expressions. The key
problem is that deterministic models produce high-quality lip-sync but without
rich expressions, whereas stochastic models generate diverse expressions but
with lower lip-sync quality. To get the best of both, we seek a stochastic
model with accurate lip-sync. To that end, we develop a new approach based on
the following observation: if a method generates realistic 3D lip motions, it
should be possible to infer the spoken audio from the lip motion. The inferred
speech should match the original input audio, and erroneous predictions create
a novel supervision signal for training 3D talking head avatars with accurate
lip-sync. To demonstrate this effect, we propose THUNDER (Talking Heads Under
Neural Differentiable Elocution Reconstruction), a 3D talking head avatar
framework that introduces a novel supervision mechanism via differentiable
sound production. First, we train a novel mesh-to-speech model that regresses
audio from facial animation. Then, we incorporate this model into a
diffusion-based talking avatar framework. During training, the mesh-to-speech
model takes the generated animation and produces a sound that is compared to
the input speech, creating a differentiable analysis-by-audio-synthesis
supervision loop. Our extensive qualitative and quantitative experiments
demonstrate that THUNDER significantly improves the quality of the lip-sync of
talking head avatars while still allowing for generation of diverse,
high-quality, expressive facial animations.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [212] [Advanced Deep Learning and Large Language Models: Comprehensive Insights for Cancer Detection](https://arxiv.org/abs/2504.13186)
*Yassine Habchi,Hamza Kheddar,Yassine Himeur,Adel Belouchrani,Erchin Serpedin,Fouad Khelifi,Muhammad E. H. Chowdhury*

Main category: eess.IV

TLDR: 本文综述了深度学习（DL）在癌症检测中的先进技术，包括迁移学习、强化学习、联邦学习、Transformers和大语言模型，填补了现有研究的空白，并探讨了其效率、挑战及解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在医疗领域应用广泛，但对其在癌症检测中的全面分析仍有限。本文旨在填补这一空白，提供更深入的理解。

Method: 通过综述先进的DL技术（如迁移学习、强化学习、联邦学习等），分析其在癌症检测中的应用，并探讨数据不平衡等挑战的解决方案。

Result: 这些技术提高了准确性，解决了数据稀缺问题，并支持去中心化学习，同时保障数据隐私。

Conclusion: 本文为研究者和从业者提供了当前趋势的见解，并指导未来在癌症检测中应用高级DL的研究。

Abstract: The rapid advancement of deep learning (DL) has transformed healthcare,
particularly in cancer detection and diagnosis. DL surpasses traditional
machine learning and human accuracy, making it a critical tool for identifying
diseases. Despite numerous reviews on DL in healthcare, a comprehensive
analysis of its role in cancer detection remains limited. Existing studies
focus on specific aspects, leaving gaps in understanding its broader impact.
This paper addresses these gaps by reviewing advanced DL techniques, including
transfer learning (TL), reinforcement learning (RL), federated learning (FL),
Transformers, and large language models (LLMs). These approaches enhance
accuracy, tackle data scarcity, and enable decentralized learning while
maintaining data privacy. TL adapts pre-trained models to new datasets,
improving performance with limited labeled data. RL optimizes diagnostic
pathways and treatment strategies, while FL fosters collaborative model
development without sharing sensitive data. Transformers and LLMs,
traditionally used in natural language processing, are now applied to medical
data for improved interpretability. Additionally, this review examines these
techniques' efficiency in cancer diagnosis, addresses challenges like data
imbalance, and proposes solutions. It serves as a resource for researchers and
practitioners, providing insights into current trends and guiding future
research in advanced DL for cancer detection.

</details>

### [213] [Efficient Brain Tumor Segmentation Using a Dual-Decoder 3D U-Net with Attention Gates (DDUNet)](https://arxiv.org/abs/2504.13200)
*Mohammad Mahdi Danesh Pajouh*

Main category: eess.IV

TLDR: 提出了一种基于双解码器U-Net和注意力门控跳跃连接的脑肿瘤分割模型，在资源有限的情况下实现了高效且准确的MRI分割。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤因其侵袭性和早期诊断困难成为癌症治疗中的重大挑战，现有分割方法计算资源需求高，限制了实际应用。

Method: 设计了一种双解码器U-Net架构，结合注意力门控跳跃连接，优化了训练效率和分割精度。

Result: 在BraTS 2020数据集上，模型仅用50轮训练即达到WT 85.06%、TC 80.61%、ET 71.26%的Dice分数，优于常见U-Net变体。

Conclusion: 该模型在有限计算资源下实现了高质量的脑肿瘤分割，为资源受限的研究和临床环境提供了可行解决方案。

Abstract: Cancer remains one of the leading causes of mortality worldwide, and among
its many forms, brain tumors are particularly notorious due to their aggressive
nature and the critical challenges involved in early diagnosis. Recent advances
in artificial intelligence have shown great promise in assisting medical
professionals with precise tumor segmentation, a key step in timely diagnosis
and treatment planning. However, many state-of-the-art segmentation methods
require extensive computational resources and prolonged training times,
limiting their practical application in resource-constrained settings. In this
work, we present a novel dual-decoder U-Net architecture enhanced with
attention-gated skip connections, designed specifically for brain tumor
segmentation from MRI scans. Our approach balances efficiency and accuracy by
achieving competitive segmentation performance while significantly reducing
training demands. Evaluated on the BraTS 2020 dataset, the proposed model
achieved Dice scores of 85.06% for Whole Tumor (WT), 80.61% for Tumor Core
(TC), and 71.26% for Enhancing Tumor (ET) in only 50 epochs, surpassing several
commonly used U-Net variants. Our model demonstrates that high-quality brain
tumor segmentation is attainable even under limited computational resources,
thereby offering a viable solution for researchers and clinicians operating
with modest hardware. This resource-efficient model has the potential to
improve early detection and diagnosis of brain tumors, ultimately contributing
to better patient outcomes

</details>

### [214] [Putting the Segment Anything Model to the Test with 3D Knee MRI -- A Comparison with State-of-the-Art Performance](https://arxiv.org/abs/2504.13340)
*Oliver Mills,Philip Conaghan,Nishant Ravikumar,Samuel Relton*

Main category: eess.IV

TLDR: 研究探讨了使用Segment Anything Model (SAM)和3D U-Net对膝关节MRI中的半月板进行自动分割的效果，发现SAM在性能上未能超越3D U-Net。


<details>
  <summary>Details</summary>
Motivation: 半月板损伤可能导致膝关节骨关节炎（OA），但目前缺乏有效治疗方法。准确的自动分割有助于早期检测和治疗。

Method: 研究比较了SAM（仅微调解码器和端到端微调）和3D U-Net在3D膝关节MRI上的分割性能。

Result: SAM的Dice分数（0.81±0.03）低于3D U-Net（0.87±0.03），端到端微调后SAM达到0.87±0.03，但仍未超越3D U-Net。

Conclusion: SAM在半月板分割任务中表现不及3D U-Net，可能不适用于类似低对比度和边界模糊的3D医学图像分割任务。

Abstract: Menisci are cartilaginous tissue found within the knee that contribute to
joint lubrication and weight dispersal. Damage to menisci can lead to onset and
progression of knee osteoarthritis (OA), a condition that is a leading cause of
disability, and for which there are few effective therapies. Accurate automated
segmentation of menisci would allow for earlier detection and treatment of
meniscal abnormalities, as well as shedding more light on the role the menisci
play in OA pathogenesis. Focus in this area has mainly used variants of
convolutional networks, but there has been no attempt to utilise recent large
vision transformer segmentation models. The Segment Anything Model (SAM) is a
so-called foundation segmentation model, which has been found useful across a
range of different tasks due to the large volume of data used for training the
model. In this study, SAM was adapted to perform fully-automated segmentation
of menisci from 3D knee magnetic resonance images. A 3D U-Net was also trained
as a baseline. It was found that, when fine-tuning only the decoder, SAM was
unable to compete with 3D U-Net, achieving a Dice score of $0.81\pm0.03$,
compared to $0.87\pm0.03$, on a held-out test set. When fine-tuning SAM
end-to-end, a Dice score of $0.87\pm0.03$ was achieved. The performance of both
the end-to-end trained SAM configuration and the 3D U-Net were comparable to
the winning Dice score ($0.88\pm0.03$) in the IWOAI Knee MRI Segmentation
Challenge 2019. Performance in terms of the Hausdorff Distance showed that both
configurations of SAM were inferior to 3D U-Net in matching the meniscus
morphology. Results demonstrated that, despite its generalisability, SAM was
unable to outperform a basic 3D U-Net in meniscus segmentation, and may not be
suitable for similar 3D medical image segmentation tasks also involving fine
anatomical structures with low contrast and poorly-defined boundaries.

</details>

### [215] [Cardiac MRI Semantic Segmentation for Ventricles and Myocardium using Deep Learning](https://arxiv.org/abs/2504.13391)
*Racheal Mukisa,Arvind K. Bansal*

Main category: eess.IV

TLDR: 该论文提出了一种改进心脏磁共振（CMR）图像语义分割的模型，通过提取边缘属性和上下文信息，提升了左心室腔、右心室腔和左心室心肌的分割精度。


<details>
  <summary>Details</summary>
Motivation: 自动化非侵入性心脏诊断对早期发现心脏疾病和成本效益高的临床管理至关重要。精确分割心脏子结构及其形态属性是评估心脏功能和诊断心血管疾病的关键。

Method: 模型在U-Net的下采样过程中提取边缘属性和上下文信息，并在上采样时注入这些信息，以定位左心室腔、右心室腔和左心室心肌。

Result: 与现有领先模型相比，该模型将Dice相似系数（DSC）提高了2%-11%，并将Hausdorff距离（HD）降低了1.6至5.7毫米。

Conclusion: 该模型在CMR图像语义分割中表现出显著改进，为心脏疾病的自动化诊断提供了更精确的工具。

Abstract: Automated noninvasive cardiac diagnosis plays a critical role in the early
detection of cardiac disorders and cost-effective clinical management.
Automated diagnosis involves the automated segmentation and analysis of cardiac
images. Precise delineation of cardiac substructures and extraction of their
morphological attributes are essential for evaluating the cardiac function, and
diagnosing cardiovascular disease such as cardiomyopathy, valvular diseases,
abnormalities related to septum perforations, and blood-flow rate. Semantic
segmentation labels the CMR image at the pixel level, and localizes its
subcomponents to facilitate the detection of abnormalities, including
abnormalities in cardiac wall motion in an aging heart with muscle
abnormalities, vascular abnormalities, and valvular abnormalities. In this
paper, we describe a model to improve semantic segmentation of CMR images. The
model extracts edge-attributes and context information during down-sampling of
the U-Net and infuses this information during up-sampling to localize three
major cardiac structures: left ventricle cavity (LV); right ventricle cavity
(RV); and LV myocardium (LMyo). We present an algorithm and performance
results. A comparison of our model with previous leading models, using
similarity metrics between actual image and segmented image, shows that our
approach improves Dice similarity coefficient (DSC) by 2%-11% and lowers
Hausdorff distance (HD) by 1.6 to 5.7 mm.

</details>

### [216] [DADU: Dual Attention-based Deep Supervised UNet for Automated Semantic Segmentation of Cardiac Images](https://arxiv.org/abs/2504.13415)
*Racheal Mukisa,Arvind K. Bansal*

Main category: eess.IV

TLDR: 提出一种基于深度学习的改进模型，用于心脏磁共振图像中左右心室和心肌瘢痕组织的分割，结合UNet、通道与空间注意力、边缘检测跳跃连接和深度监督学习，显著提高了分割精度。


<details>
  <summary>Details</summary>
Motivation: 心脏磁共振图像分割对临床诊断至关重要，但现有方法在精度和鲁棒性上仍有不足，需改进。

Method: 集成UNet、通道与空间注意力机制、边缘检测跳跃连接和深度监督学习，通过多通道处理生成特征图。

Result: 模型达到98%的Dice相似性分数（DSC）和显著降低的Hausdorff距离（HD），性能优于其他领先技术。

Conclusion: 该方法在心脏磁共振图像分割中表现出高精度和鲁棒性，具有临床应用潜力。

Abstract: We propose an enhanced deep learning-based model for image segmentation of
the left and right ventricles and myocardium scar tissue from cardiac magnetic
resonance (CMR) images. The proposed technique integrates UNet, channel and
spatial attention, edge-detection based skip-connection and deep supervised
learning to improve the accuracy of the CMR image-segmentation. Images are
processed using multiple channels to generate multiple feature-maps. We built a
dual attention-based model to integrate channel and spatial attention. The use
of extracted edges in skip connection improves the reconstructed images from
feature-maps. The use of deep supervision reduces vanishing gradient problems
inherent in classification based on deep neural networks. The algorithms for
dual attention-based model, corresponding implementation and performance
results are described. The performance results show that this approach has
attained high accuracy: 98% Dice Similarity Score (DSC) and significantly lower
Hausdorff Distance (HD). The performance results outperform other leading
techniques both in DSC and HD.

</details>

### [217] [Accelerated Optimization of Implicit Neural Representations for CT Reconstruction](https://arxiv.org/abs/2504.13390)
*Mahrokh Najaf,Gregory Ongie*

Main category: eess.IV

TLDR: 本文研究了加速隐式神经表示（INR）在CT重建中优化的策略，提出了两种方法：改进的损失函数和基于交替方向乘子法的算法，显著提升了稀疏视图乳腺CT体模的重建速度。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INR）在解决计算机视觉中的逆问题上表现出色，但在CT重建中训练速度过慢，需要数千次迭代才能收敛。本文旨在解决这一问题。

Method: 提出了两种策略：1）使用改进的损失函数以提高条件性；2）基于交替方向乘子法（ADMM）的算法。

Result: 实验表明，这两种方法在稀疏视图乳腺CT体模的重建中显著加速了INR的优化过程。

Conclusion: 本文提出的方法有效提升了INR在CT重建中的优化效率，为实际应用提供了可行的解决方案。

Abstract: Inspired by their success in solving challenging inverse problems in computer
vision, implicit neural representations (INRs) have been recently proposed for
reconstruction in low-dose/sparse-view X-ray computed tomography (CT). An INR
represents a CT image as a small-scale neural network that takes spatial
coordinates as inputs and outputs attenuation values. Fitting an INR to
sinogram data is similar to classical model-based iterative reconstruction
methods. However, training INRs with losses and gradient-based algorithms can
be prohibitively slow, taking many thousands of iterations to converge. This
paper investigates strategies to accelerate the optimization of INRs for CT
reconstruction. In particular, we propose two approaches: (1) using a modified
loss function with improved conditioning, and (2) an algorithm based on the
alternating direction method of multipliers. We illustrate that both of these
approaches significantly accelerate INR-based reconstruction of a synthetic
breast CT phantom in a sparse-view setting.

</details>

### [218] [FocusNet: Transformer-enhanced Polyp Segmentation with Local and Pooling Attention](https://arxiv.org/abs/2504.13597)
*Jun Zeng,KC Santosh,Deepak Rajan Nayak,Thomas de Lange,Jonas Varkey,Tyler Berzin,Debesh Jha*

Main category: eess.IV

TLDR: FocusNet是一种基于Transformer的注意力网络，用于提升结肠息肉分割的准确性和鲁棒性，通过多模态和多中心数据验证其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在息肉分割中多基于单模态和单中心数据，难以适应真实临床环境，因此需要更通用的解决方案。

Method: FocusNet结合了交叉语义交互解码器模块（CIDM）、细节增强模块（DEM）和焦点注意力模块（FAM），通过局部和全局注意力机制优化分割效果。

Result: 在PolypDB数据集上，FocusNet在五种不同模态下的Dice系数表现优异（82.47%-93.42%），显著优于现有方法。

Conclusion: FocusNet在多模态和多中心数据中表现出色，为临床息肉分割提供了更可靠的解决方案。

Abstract: Colonoscopy is vital in the early diagnosis of colorectal polyps. Regular
screenings can effectively prevent benign polyps from progressing to CRC. While
deep learning has made impressive strides in polyp segmentation, most existing
models are trained on single-modality and single-center data, making them less
effective in real-world clinical environments. To overcome these limitations,
we propose FocusNet, a Transformer-enhanced focus attention network designed to
improve polyp segmentation. FocusNet incorporates three essential modules: the
Cross-semantic Interaction Decoder Module (CIDM) for generating coarse
segmentation maps, the Detail Enhancement Module (DEM) for refining shallow
features, and the Focus Attention Module (FAM), to balance local detail and
global context through local and pooling attention mechanisms. We evaluate our
model on PolypDB, a newly introduced dataset with multi-modality and
multi-center data for building more reliable segmentation methods. Extensive
experiments showed that FocusNet consistently outperforms existing
state-of-the-art approaches with a high dice coefficients of 82.47% on the BLI
modality, 88.46% on FICE, 92.04% on LCI, 82.09% on the NBI and 93.42% on WLI
modality, demonstrating its accuracy and robustness across five different
modalities. The source code for FocusNet is available at
https://github.com/JunZengz/FocusNet.

</details>

### [219] [Filter2Noise: Interpretable Self-Supervised Single-Image Denoising for Low-Dose CT with Attention-Guided Bilateral Filtering](https://arxiv.org/abs/2504.13519)
*Yipeng Sun,Linda-Sophie Schneider,Mingxuan Gu,Siyuan Mei,Chengze Ye,Fabian Wagner,Siming Bayer,Andreas Maier*

Main category: eess.IV

TLDR: 提出了一种可解释的自监督单图像去噪框架Filter2Noise（F2N），通过注意力引导的双边滤波器和轻量级模块实现用户可控的去噪，并在低剂量CT数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决监督方法依赖配对数据、自监督方法依赖多噪声图像且缺乏解释性的问题。

Method: 引入注意力引导的双边滤波器，通过轻量级模块预测空间变化的滤波器参数，并提出新的下采样策略和自监督损失函数。

Result: 在Mayo Clinic 2016低剂量CT数据集上，F2N比领先的自监督单图像方法（ZS-N2N）PSNR提高了4.59 dB。

Conclusion: F2N在去噪性能、透明度和用户控制方面具有优势，适用于需要精确且可解释去噪的医疗应用。

Abstract: Effective denoising is crucial in low-dose CT to enhance subtle structures
and low-contrast lesions while preventing diagnostic errors. Supervised methods
struggle with limited paired datasets, and self-supervised approaches often
require multiple noisy images and rely on deep networks like U-Net, offering
little insight into the denoising mechanism. To address these challenges, we
propose an interpretable self-supervised single-image denoising framework --
Filter2Noise (F2N). Our approach introduces an Attention-Guided Bilateral
Filter that adapted to each noisy input through a lightweight module that
predicts spatially varying filter parameters, which can be visualized and
adjusted post-training for user-controlled denoising in specific regions of
interest. To enable single-image training, we introduce a novel downsampling
shuffle strategy with a new self-supervised loss function that extends the
concept of Noise2Noise to a single image and addresses spatially correlated
noise. On the Mayo Clinic 2016 low-dose CT dataset, F2N outperforms the leading
self-supervised single-image method (ZS-N2N) by 4.59 dB PSNR while improving
transparency, user control, and parametric efficiency. These features provide
key advantages for medical applications that require precise and interpretable
noise reduction. Our code is demonstrated at
https://github.com/sypsyp97/Filter2Noise.git .

</details>

### [220] [A Novel Hybrid Approach for Retinal Vessel Segmentation with Dynamic Long-Range Dependency and Multi-Scale Retinal Edge Fusion Enhancement](https://arxiv.org/abs/2504.13553)
*Yihao Ouyang,Xunheng Kuang,Mengjia Xiong,Zhida Wang,Yuanquan Wang*

Main category: eess.IV

TLDR: 提出了一种结合CNN和Mamba的混合框架，用于高精度视网膜血管分割，解决了多尺度血管变异性、复杂曲率和模糊边界等问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在血管分割中存在血管不连续性和边缘特征模糊的问题，需要一种更精确的方法。

Method: 结合CNN和Mamba的混合框架，包括高分辨率边缘融合网络、动态蛇形视觉状态空间块和多尺度视网膜边缘融合模块。

Result: 在三个公开数据集上实现了最先进的性能，特别是在血管连续性和低对比度区域分割方面表现优异。

Conclusion: 该方法为临床应用中精确视网膜血管分析提供了可靠工具。

Abstract: Accurate retinal vessel segmentation provides essential structural
information for ophthalmic image analysis. However, existing methods struggle
with challenges such as multi-scale vessel variability, complex curvatures, and
ambiguous boundaries. While Convolutional Neural Networks (CNNs),
Transformer-based models and Mamba-based architectures have advanced the field,
they often suffer from vascular discontinuities or edge feature ambiguity. To
address these limitations, we propose a novel hybrid framework that
synergistically integrates CNNs and Mamba for high-precision retinal vessel
segmentation. Our approach introduces three key innovations: 1) The proposed
High-Resolution Edge Fuse Network is a high-resolution preserving hybrid
segmentation framework that combines a multi-scale backbone with the
Multi-scale Retina Edge Fusion (MREF) module to enhance edge features, ensuring
accurate and robust vessel segmentation. 2) The Dynamic Snake Visual State
Space block combines Dynamic Snake Convolution with Mamba to adaptively capture
vessel curvature details and long-range dependencies. An improved
eight-directional 2D Snake-Selective Scan mechanism and a dynamic weighting
strategy enhance the perception of complex vascular topologies. 3) The MREF
module enhances boundary precision through multi-scale edge feature
aggregation, suppressing noise while emphasizing critical vessel structures
across scales. Experiments on three public datasets demonstrate that our method
achieves state-of-the-art performance, particularly in maintaining vascular
continuity and effectively segmenting vessels in low-contrast regions. This
work provides a robust method for clinical applications requiring accurate
retinal vessel analysis. The code is available at
https://github.com/frank-oy/HREFNet.

</details>

### [221] [ViG3D-UNet: Volumetric Vascular Connectivity-Aware Segmentation via 3D Vision Graph Representation](https://arxiv.org/abs/2504.13599)
*Bowen Liu,Chunlei Meng,Wei Lin,Hongda Zhang,Ziqing Zhou,Zhongxue Gan,Chun Ouyang*

Main category: eess.IV

TLDR: ViG3D-UNet提出了一种基于3D图神经网络的血管分割方法，解决了现有方法中血管不连续和端点缺失的问题，并在公开数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 准确的血管分割对冠状动脉可视化和冠心病诊断至关重要，但现有方法存在血管不连续和端点缺失的挑战。

Method: 结合3D图表示与卷积模块，通过通道注意力整合特征，并采用纸夹形偏移解码器减少冗余计算。

Result: 在ASOCA和ImageCAS数据集上，ViG3D-UNet在血管连通性和分割精度上优于其他方法。

Conclusion: ViG3D-UNet有效解决了血管分割中的连续性问题，具有较高的实用价值。

Abstract: Accurate vascular segmentation is essential for coronary visualization and
the diagnosis of coronary heart disease. This task involves the extraction of
sparse tree-like vascular branches from the volumetric space. However, existing
methods have faced significant challenges due to discontinuous vascular
segmentation and missing endpoints. To address this issue, a 3D vision graph
neural network framework, named ViG3D-UNet, was introduced. This method
integrates 3D graph representation and aggregation within a U-shaped
architecture to facilitate continuous vascular segmentation. The ViG3D module
captures volumetric vascular connectivity and topology, while the convolutional
module extracts fine vascular details. These two branches are combined through
channel attention to form the encoder feature. Subsequently, a paperclip-shaped
offset decoder minimizes redundant computations in the sparse feature space and
restores the feature map size to match the original input dimensions. To
evaluate the effectiveness of the proposed approach for continuous vascular
segmentation, evaluations were performed on two public datasets, ASOCA and
ImageCAS. The segmentation results show that the ViG3D-UNet surpassed competing
methods in maintaining vascular segmentation connectivity while achieving high
segmentation accuracy. Our code will be available soon.

</details>

### [222] [SupResDiffGAN a new approach for the Super-Resolution task](https://arxiv.org/abs/2504.13622)
*Dawid Kopeć,Wojciech Kozłowski,Maciej Wizerkaniuk,Dawid Krutul,Jan Kocoń,Maciej Zięba*

Main category: eess.IV

TLDR: SupResDiffGAN是一种结合GAN和扩散模型的混合架构，用于超分辨率任务，显著提高了推理速度并保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 结合GAN和扩散模型的优势，解决传统扩散模型在超分辨率任务中推理速度慢的问题。

Method: 利用潜在空间表示和减少扩散步骤，提出自适应噪声破坏以防止判别器过拟合。

Result: 在基准数据集上表现优于SR3和I²SB，效率和图像质量均有提升。

Conclusion: SupResDiffGAN弥合了扩散模型与GAN之间的性能差距，为实时高分辨率图像生成奠定了基础。

Abstract: In this work, we present SupResDiffGAN, a novel hybrid architecture that
combines the strengths of Generative Adversarial Networks (GANs) and diffusion
models for super-resolution tasks. By leveraging latent space representations
and reducing the number of diffusion steps, SupResDiffGAN achieves
significantly faster inference times than other diffusion-based
super-resolution models while maintaining competitive perceptual quality. To
prevent discriminator overfitting, we propose adaptive noise corruption,
ensuring a stable balance between the generator and the discriminator during
training. Extensive experiments on benchmark datasets show that our approach
outperforms traditional diffusion models such as SR3 and I$^2$SB in efficiency
and image quality. This work bridges the performance gap between diffusion- and
GAN-based methods, laying the foundation for real-time applications of
diffusion models in high-resolution image generation.

</details>

<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [223] [HEAT:History-Enhanced Dual-phase Actor-Critic Algorithm with A Shared Transformer](https://arxiv.org/abs/2504.13193)
*Hong Yang*

Main category: cs.NI

TLDR: 本文提出了一种基于历史增强的两阶段行动者-评论家算法（HEAT），结合共享Transformer算法，用于提升单网关LoRaWAN网络的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究常忽略下行链路参数，且缺乏离线与在线强化学习的有效结合。HEAT旨在通过历史数据和实时交互提升模型性能。

Method: HEAT算法综合考虑上下行链路参数，结合离线与在线强化学习，并开发了开源LoRaWAN模拟器LoRaWANSim。

Result: 实验表明，HEAT在包成功率和能效上分别提升了15%和95%，优于对比算法。

Conclusion: HEAT算法显著提升了LoRaWAN网络性能，验证了其有效性。

Abstract: For a single-gateway LoRaWAN network, this study proposed a history-enhanced
two-phase actor-critic algorithm with a shared transformer algorithm (HEAT) to
improve network performance. HEAT considers uplink parameters and often
neglected downlink parameters, and effectively integrates offline and online
reinforcement learning, using historical data and real-time interaction to
improve model performance. In addition, this study developed an open source
LoRaWAN network simulator LoRaWANSim. The simulator considers the demodulator
lock effect and supports multi-channel, multi-demodulator and bidirectional
communication. Simulation experiments show that compared with the best results
of all compared algorithms, HEAT improves the packet success rate and energy
efficiency by 15% and 95%, respectively.

</details>

### [224] [Optimizing Multi-Gateway LoRaWAN via Cloud-Edge Collaboration and Knowledge Distillation](https://arxiv.org/abs/2504.13194)
*Hong Yang*

Main category: cs.NI

TLDR: HEAT-LDL是一种基于边缘智能的云边协作资源分配与决策方法，用于大规模多网关LoRaWAN网络，结合Actor-Critic架构和Lyapunov优化方法，提升下行控制和网关负载均衡。


<details>
  <summary>Details</summary>
Motivation: 解决大规模多网关LoRaWAN网络中资源分配和决策效率低下的问题。

Method: 结合Actor-Critic架构和Lyapunov优化方法，通过HEAT算法调度终端节点，并在终端节点侧进行云边知识蒸馏。

Result: 相比其他算法，HEAT-LDL将数据包成功率和能效分别提高了20.5%和88.1%。

Conclusion: HEAT-LDL有效提升了网络性能和终端节点的自主决策效率。

Abstract: For large-scale multi-gateway LoRaWAN networks, this study proposes a
cloud-edge collaborative resource allocation and decision-making method based
on edge intelligence, HEAT-LDL (HEAT-Local Distill Lyapunov), which realizes
collaborative decision-making between gateways and terminal nodes. HEAT-LDL
combines the Actor-Critic architecture and the Lyapunov optimization method to
achieve intelligent downlink control and gateway load balancing. When the
signal quality is good, the network server uses the HEAT algorithm to schedule
the terminal nodes. To improve the efficiency of autonomous decision-making of
terminal nodes, HEAT-LDL performs cloud-edge knowledge distillation on the HEAT
teacher model on the terminal node side. When the downlink decision instruction
is lost, the terminal node uses the student model and the edge decider based on
prior knowledge and local history to make collaborative autonomous decisions.
Simulation experiments show that compared with the optimal results of all
compared algorithms, HEAT-LDL improves the packet success rate and energy
efficiency by 20.5% and 88.1%, respectively.

</details>

### [225] [SFL-LEO: Asynchronous Split-Federated Learning Design for LEO Satellite-Ground Network Framework](https://arxiv.org/abs/2504.13479)
*Jiasheng Wu,Jingjing Zhang,Zheng Lin,Zhe Chen,Xiong Wang,Wenjun Zhu,Yue Gao*

Main category: cs.NI

TLDR: 论文提出了一种名为SFL-LEO的分布式学习框架，结合联邦学习（FL）和分割学习（SL），以解决LEO卫星网络中高效计算的挑战。


<details>
  <summary>Details</summary>
Motivation: LEO卫星网络的高动态性和有限计算能力使得数据处理成为难题，需要一种适应性强且高效的解决方案。

Method: 通过结合FL和SL，并利用卫星周期性轨道特征，提出异步训练策略，允许卫星在断开地面站时进行本地训练。

Result: 实验结果表明，SFL-LEO与传统SL方案相比，在断开期间仍能进行本地训练，达到相似的准确性。

Conclusion: SFL-LEO框架有效解决了LEO卫星网络中的计算挑战，提升了训练性能。

Abstract: Recently, the rapid development of LEO satellite networks spurs another
widespread concern-data processing at satellites. However, achieving efficient
computation at LEO satellites in highly dynamic satellite networks is
challenging and remains an open problem when considering the constrained
computation capability of LEO satellites. For the first time, we propose a
novel distributed learning framework named SFL-LEO by combining Federated
Learning (FL) with Split Learning (SL) to accommodate the high dynamics of LEO
satellite networks and the constrained computation capability of LEO satellites
by leveraging the periodical orbit traveling feature. The proposed scheme
allows training locally by introducing an asynchronous training strategy, i.e.,
achieving local update when LEO satellites disconnect with the ground station,
to provide much more training space and thus increase the training performance.
Meanwhile, it aggregates client-side sub-models at the ground station and then
distributes them to LEO satellites by borrowing the idea from the federated
learning scheme. Experiment results driven by satellite-ground bandwidth
measured in Starlink demonstrate that SFL-LEO provides a similar accuracy
performance with the conventional SL scheme because it can perform local
training even within the disconnection duration.

</details>

### [226] [Towards End-to-End Network Intent Management with Large Language Models](https://arxiv.org/abs/2504.13589)
*Lam Dinh,Sihem Cherrared,Xiaofeng Huang,Fabrice Guillemin*

Main category: cs.NI

TLDR: LLMs在意图驱动网络（IBN）中发挥关键作用，研究比较了闭源和开源模型在生成5G/6G网络配置中的表现，并提出了新的评估指标FEACI。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在IBN中的潜力，特别是闭源和开源模型在生成网络配置时的性能差异。

Method: 利用闭源（如Google Gemini 1.5 pro、ChatGPT-4）和开源（如LLama、Mistral）模型生成5G/6G网络配置，并引入FEACI指标评估性能。

Result: 开源模型在翻译性能上可与闭源模型媲美甚至更优，且无需昂贵硬件支持。

Conclusion: 开源模型在IBN中具有实际应用潜力，为更广泛的用户提供了可行选择。

Abstract: Large Language Models (LLMs) are likely to play a key role in Intent-Based
Networking (IBN) as they show remarkable performance in interpreting human
language as well as code generation, enabling the translation of high-level
intents expressed by humans into low-level network configurations. In this
paper, we leverage closed-source language models (i.e., Google Gemini 1.5 pro,
ChatGPT-4) and open-source models (i.e., LLama, Mistral) to investigate their
capacity to generate E2E network configurations for radio access networks
(RANs) and core networks in 5G/6G mobile networks. We introduce a novel
performance metrics, known as FEACI, to quantitatively assess the format (F),
explainability (E), accuracy (A), cost (C), and inference time (I) of the
generated answer; existing general metrics are unable to capture these
features. The results of our study demonstrate that open-source models can
achieve comparable or even superior translation performance compared with the
closed-source models requiring costly hardware setup and not accessible to all
users.

</details>

<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [227] [Adaptive AI decision interface for autonomous electronic material discovery](https://arxiv.org/abs/2504.13344)
*Yahao Dai,Henry Chan,Aikaterini Vriza,Fredrick Kim,Yunfei Wang,Wei Liu,Naisong Shan,Jing Xu,Max Weires,Yukun Wu,Zhiqiang Cao,C. Suzanne Miller,Ralu Divan,Xiaodan Gu,Chenhui Zhu,Sihong Wang,Jie Xu*

Main category: cond-mat.mtrl-sci

TLDR: 该论文提出了一种AI决策界面，通过实时监控和交互式人机协作，解决了电子材料研究中数据稀缺的问题，显著提升了材料性能。


<details>
  <summary>Details</summary>
Motivation: 电子材料研究中数据稀缺且实验周期长，传统AI算法难以适应实时决策需求，因此需要开发一种适应性强的AI/AE系统。

Method: 开发了一个AI决策界面，包含AI顾问功能，支持实时进度监控、数据分析和人机协作，应用于混合离子-电子导电聚合物（MIECPs）研究。

Result: 在64次自主实验中，平台将混合导电品质因数（μC*）提高了150%，达到1,275 F cm-1 V-1 s-1，并发现新的聚合物多晶型。

Conclusion: 该AI/AE平台通过适应性决策和人机协作，显著加速了电子材料研究，并揭示了关键结构因素。

Abstract: AI-powered autonomous experimentation (AI/AE) can accelerate materials
discovery but its effectiveness for electronic materials is hindered by data
scarcity from lengthy and complex design-fabricate-test-analyze cycles. Unlike
experienced human scientists, even advanced AI algorithms in AI/AE lack the
adaptability to make informative real-time decisions with limited datasets.
Here, we address this challenge by developing and implementing an AI decision
interface on our AI/AE system. The central element of the interface is an AI
advisor that performs real-time progress monitoring, data analysis, and
interactive human-AI collaboration for actively adapting to experiments in
different stages and types. We applied this platform to an emerging type of
electronic materials-mixed ion-electron conducting polymers (MIECPs) -- to
engineer and study the relationships between multiscale morphology and
properties. Using organic electrochemical transistors (OECT) as the testing-bed
device for evaluating the mixed-conducting figure-of-merit -- the product of
charge-carrier mobility and the volumetric capacitance ({\mu}C*), our adaptive
AI/AE platform achieved a 150% increase in {\mu}C* compared to the commonly
used spin-coating method, reaching 1,275 F cm-1 V-1 s-1 in just 64 autonomous
experimental trials. A study of 10 statistically selected samples identifies
two key structural factors for achieving higher volumetric capacitance: larger
crystalline lamellar spacing and higher specific surface area, while also
uncovering a new polymer polymorph in this material.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [228] [Statistical Validation in Cultural Adaptations of Cognitive Tests: A Multi- Regional Systematic Review](https://arxiv.org/abs/2504.13495)
*Miit Daga,Priyasha Mohanty,Ram Krishna,Swarna Priya RM*

Main category: cs.CY

TLDR: 本文综述了跨文化认知评估工具的方法论和统计验证，强调文化适应需结合社区反馈和标准化翻译协议。


<details>
  <summary>Details</summary>
Motivation: 探讨不同文化背景下认知评估工具的适应性和有效性，为全球多样化健康环境提供依据。

Method: 系统性回顾六项关于欧洲、亚洲、非洲和南美洲文化适应方法的研究。

Result: 教育解释了26.76%的MoCA-H分数差异，文化语言因素在欧洲适应中占6.89%。巴西土著人群中MMSE和BCSB表现出色（敏感性94.4%，特异性99.2%）。

Conclusion: 文化适应需结合社区反馈、标准化翻译和统计验证，为全球健康环境中的认知评估提供框架。

Abstract: This systematic review discusses the methodological approaches and
statistical confirmations of cross-cultural adaptations of cognitive evaluation
tools used with different populations. The review considers six seminal studies
on the methodology of cultural adaptation in Europe, Asia, Africa, and South
America. The results indicate that proper adaptations need holistic models with
demographic changes, and education explained as much as 26.76% of the variance
in MoCA-H scores. Cultural-linguistic factors explained 6.89% of the variance
in European adaptations of MoCA-H; however, another study on adapted MMSE and
BCSB among Brazilian Indigenous populations reported excellent diagnostic
performance, with a sensitivity of 94.4% and specificity of 99.2%. There was
78.5% inter-rater agreement on the evaluation of cultural adaptation using the
Manchester Translation Evaluation Checklist. A paramount message of the paper
is that community feedback is necessary for culturally appropriate preparation,
standardized translation protocols also must be included, along with robust
statistical validation methodologies for developing cognitive assessment
instruments. This review supplies evidence-based frameworks for the further
adaptation of cognitive assessments in increasingly diverse global health
settings.

</details>

<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [229] [A Stochastic Nonlinear Dynamical System for Smoothing Noisy Eye Gaze Data](https://arxiv.org/abs/2504.13278)
*Thoa Thieu,Roderick Melnik*

Main category: math.NA

TLDR: 使用扩展卡尔曼滤波器（EKF）平滑眼动追踪数据，显著减少噪声并提高跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 解决因眼动仪限制、校准漂移、环境光线变化和眨眼等因素导致的注视点定位噪声问题。

Method: 提出使用扩展卡尔曼滤波器（EKF）平滑数据，并系统探索不同参数的交互作用。

Result: EKF显著减少噪声，提高跟踪精度；提出的随机非线性动态模型与实验数据吻合良好。

Conclusion: EKF方法有效，模型在相关领域有应用潜力。

Abstract: In this study, we address the challenges associated with accurately
determining gaze location on a screen, which is often compromised by noise from
factors such as eye tracker limitations, calibration drift, ambient lighting
changes, and eye blinks. We propose the use of an extended Kalman filter (EKF)
to smooth the gaze data collected during eye-tracking experiments, and
systematically explore the interaction of different system parameters. Our
results demonstrate that the EKF significantly reduces noise, leading to a
marked improvement in tracking accuracy. Furthermore, we show that our proposed
stochastic nonlinear dynamical model aligns well with real experimental data
and holds promise for applications in related fields.

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [230] [Acoustic to Articulatory Inversion of Speech; Data Driven Approaches, Challenges, Applications, and Future Scope](https://arxiv.org/abs/2504.13308)
*Leena G Pillai,D. Muhammad Noorul Mubarak*

Main category: cs.SD

TLDR: 本文综述了过去十年（2011-2021）中数据驱动方法在声学-发音反演（AAI）中的应用，涵盖不同类型、目标、语料库、方法和评估指标。


<details>
  <summary>Details</summary>
Motivation: 探索AAI在发音近似、特征空间选择、自动语音识别（ASR）及语言训练中的潜在应用，并研究声学与发音特征之间的相关性。

Method: 基于机器学习的方法，利用多种医学成像模型（如EMA、EPG、rtMRI等）和语音数据，通过非线性回归问题评估性能。

Result: AAI模型可提供直观的发音位置反馈，尤其在舌部运动方面，适用于语音、语言训练及病理学治疗。

Conclusion: AAI模型在发音反馈系统中具有实际应用潜力，尤其在语言训练和治疗领域。

Abstract: This review is focused on the data-driven approaches applied in different
applications of Acoustic-to-Articulatory Inversion (AAI) of speech. This review
paper considered the relevant works published in the last ten years
(2011-2021). The selection criteria includes (a) type of AAI - Speaker
Dependent and Speaker Independent AAI, (b) objectives of the work -
Articulatory approximation, Articulatory Feature space selection and Automatic
Speech Recognition (ASR), explore the correlation between acoustic and
articulatory features, and framework for Computer-assisted language training,
(c) Corpus - Simultaneously recorded speech (wav) and medical imaging models
such as ElectroMagnetic Articulography (EMA), Electropalatography (EPG),
Laryngography, Electroglottography (EGG), X-ray Cineradiography, Ultrasound,
and real-time Magnetic Resonance Imaging (rtMRI), (d) Methods or models -
recent works are considered, and therefore all the works are based on machine
learning, (e) Evaluation - as AAI is a non-linear regression problem, the
performance evaluation is mostly done by Correlation Coefficient (CC), Root
Mean Square Error (RMSE), and also considered Mean Square Error (MSE), and Mean
Format Error (MFE). The practical application of the AAI model can provide a
better and user-friendly interpretable image feedback system of articulatory
positions, especially tongue movement. Such trajectory feedback system can be
used to provide phonetic, language, and speech therapy for pathological
subjects.

</details>

### [231] [Collective Learning Mechanism based Optimal Transport Generative Adversarial Network for Non-parallel Voice Conversion](https://arxiv.org/abs/2504.13791)
*Sandipan Dhar,Md. Tousin Akhter,Nanda Dulal Jana,Swagatam Das*

Main category: cs.SD

TLDR: CLOT-GAN模型通过多判别器和最优传输损失提升语音转换的自然度。


<details>
  <summary>Details</summary>
Motivation: 现有GAN语音转换模型在自然度上存在差距，且单判别器学习效果有限。

Method: 提出CLOT-GAN，结合多判别器（DCNN、ViT、Conformer）和最优传输损失。

Result: 在VCC 2018等数据集上，CLOT-GAN优于现有模型。

Conclusion: 多判别器和OT损失的结合显著提升了语音转换性能。

Abstract: After demonstrating significant success in image synthesis, Generative
Adversarial Network (GAN) models have likewise made significant progress in the
field of speech synthesis, leveraging their capacity to adapt the precise
distribution of target data through adversarial learning processes. Notably, in
the realm of State-Of-The-Art (SOTA) GAN-based Voice Conversion (VC) models,
there exists a substantial disparity in naturalness between real and
GAN-generated speech samples. Furthermore, while many GAN models currently
operate on a single generator discriminator learning approach, optimizing
target data distribution is more effectively achievable through a single
generator multi-discriminator learning scheme. Hence, this study introduces a
novel GAN model named Collective Learning Mechanism-based Optimal Transport GAN
(CLOT-GAN) model, incorporating multiple discriminators, including the Deep
Convolutional Neural Network (DCNN) model, Vision Transformer (ViT), and
conformer. The objective of integrating various discriminators lies in their
ability to comprehend the formant distribution of mel-spectrograms, facilitated
by a collective learning mechanism. Simultaneously, the inclusion of Optimal
Transport (OT) loss aims to precisely bridge the gap between the source and
target data distribution, employing the principles of OT theory. The
experimental validation on VCC 2018, VCTK, and CMU-Arctic datasets confirms
that the CLOT-GAN-VC model outperforms existing VC models in objective and
subjective assessments.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [232] [Gradient-Free Sequential Bayesian Experimental Design via Interacting Particle Systems](https://arxiv.org/abs/2504.13320)
*Robert Gruhlke,Matei Hanu,Claudia Schillings,Philipp Wacker*

Main category: stat.ML

TLDR: 提出了一种无梯度的贝叶斯最优实验设计框架，结合EKI和ALDI方法，适用于复杂系统。通过变分近似解决计算问题，并在实验中验证了其高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 针对复杂系统中梯度信息不可用的情况，开发一种无梯度的贝叶斯最优实验设计方法。

Method: 结合Ensemble Kalman Inversion (EKI)进行设计优化，使用Affine-Invariant Langevin Dynamics (ALDI)采样器进行高效后验采样，并引入变分高斯和参数化拉普拉斯近似。

Result: 在从线性高斯模型到PDE约束的反问题中，验证了方法的鲁棒性、准确性和高效性。

Conclusion: 提出的框架在信息驱动的实验设计中表现出色，适用于高维空间和复杂问题。

Abstract: We introduce a gradient-free framework for Bayesian Optimal Experimental
Design (BOED) in sequential settings, aimed at complex systems where gradient
information is unavailable. Our method combines Ensemble Kalman Inversion (EKI)
for design optimization with the Affine-Invariant Langevin Dynamics (ALDI)
sampler for efficient posterior sampling-both of which are derivative-free and
ensemble-based. To address the computational challenges posed by nested
expectations in BOED, we propose variational Gaussian and parametrized Laplace
approximations that provide tractable upper and lower bounds on the Expected
Information Gain (EIG). These approximations enable scalable utility estimation
in high-dimensional spaces and PDE-constrained inverse problems. We demonstrate
the performance of our framework through numerical experiments ranging from
linear Gaussian models to PDE-based inference tasks, highlighting the method's
robustness, accuracy, and efficiency in information-driven experimental design.

</details>

### [233] [Predicting Forced Responses of Probability Distributions via the Fluctuation-Dissipation Theorem and Generative Modeling](https://arxiv.org/abs/2504.13333)
*Ludovico T. Giorgini,Fabrizio Falasca,Andre N. Souza*

Main category: stat.ML

TLDR: 提出了一种基于数据驱动的新框架，用于估计非线性随机系统高阶矩对外部扰动的响应，结合GFDT和基于分数的生成建模，显著优于传统高斯近似。


<details>
  <summary>Details</summary>
Motivation: 传统GFDT依赖高斯近似，虽能准确预测均值响应，但在高阶矩（如方差、偏度和峰度）上存在显著偏差，需改进。

Method: 结合GFDT与基于分数的生成建模，直接从数据估计分数函数，无需完整密度重建。

Result: 在三个气候动力学相关的随机模型上验证，方法能捕捉强非线性和非高斯特征，优于传统高斯近似。

Conclusion: 新框架有效解决了高阶矩响应估计的偏差问题，适用于复杂非线性系统。

Abstract: We present a novel data-driven framework for estimating the response of
higher-order moments of nonlinear stochastic systems to small external
perturbations. The classical Generalized Fluctuation-Dissipation Theorem (GFDT)
links the unperturbed steady-state distribution to the system's linear
response. Standard implementations rely on Gaussian approximations, which can
often accurately predict the mean response but usually introduce significant
biases in higher-order moments, such as variance, skewness, and kurtosis. To
address this limitation, we combine GFDT with recent advances in score-based
generative modeling, which enable direct estimation of the score function from
data without requiring full density reconstruction. Our method is validated on
three reduced-order stochastic models relevant to climate dynamics: a scalar
stochastic model for low-frequency climate variability, a slow-fast triad model
mimicking key features of the El Nino-Southern Oscillation (ENSO), and a
six-dimensional stochastic barotropic model capturing atmospheric regime
transitions. In all cases, the approach captures strongly nonlinear and
non-Gaussian features of the system's response, outperforming traditional
Gaussian approximations.

</details>

### [234] [On the minimax optimality of Flow Matching through the connection to kernel density estimation](https://arxiv.org/abs/2504.13336)
*Lea Kunkel,Mathias Trabs*

Main category: stat.ML

TLDR: Flow Matching作为生成建模的简单灵活替代方法，通过连接核密度估计，验证了其收敛速度，并在高维场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 探索Flow Matching在生成建模中的理论性能，弥补现有统计分析的不足，并验证其在高维数据中的有效性。

Method: 将Flow Matching与核密度估计联系起来，分析其收敛速度，并扩展到高维数据场景。

Result: Flow Matching在足够大的网络中达到最优收敛速度，且在高维数据中表现更优。

Conclusion: Flow Matching不仅理论上有保障，且在高维数据中具有实际优势。

Abstract: Flow Matching has recently gained attention in generative modeling as a
simple and flexible alternative to diffusion models, the current state of the
art. While existing statistical guarantees adapt tools from the analysis of
diffusion models, we take a different perspective by connecting Flow Matching
to kernel density estimation. We first verify that the kernel density estimator
matches the optimal rate of convergence in Wasserstein distance up to
logarithmic factors, improving existing bounds for the Gaussian kernel. Based
on this result, we prove that for sufficiently large networks, Flow Matching
also achieves the optimal rate up to logarithmic factors, providing a
theoretical foundation for the empirical success of this method. Finally, we
provide a first justification of Flow Matching's effectiveness in
high-dimensional settings by showing that rates improve when the target
distribution lies on a lower-dimensional linear subspace.

</details>

### [235] [On the Convergence of Irregular Sampling in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2504.13623)
*Armin Iske*

Main category: stat.ML

TLDR: 分析了在再生核希尔伯特空间（RKHS）中采样算法的收敛性，讨论了核回归在最小化假设下的近似性质，并证明了核范数下的误差估计。


<details>
  <summary>Details</summary>
Motivation: 研究核回归在RKHS中的收敛性，为采样算法提供理论支持。

Method: 通过最小化假设讨论核回归的近似性质，证明核范数下的误差估计，并推导一致收敛性。

Result: 证明了Lipschitz连续和Hölder连续核的收敛速率。

Conclusion: 为RKHS中采样算法的收敛性提供了新的理论结果，特别是在一致收敛性方面。

Abstract: We analyse the convergence of sampling algorithms for functions in
reproducing kernel Hilbert spaces (RKHS). To this end, we discuss approximation
properties of kernel regression under minimalistic assumptions on both the
kernel and the input data. We first prove error estimates in the kernel's RKHS
norm. This leads us to new results concerning uniform convergence of kernel
regression on compact domains. For Lipschitz continuous and H\"older continuous
kernels, we prove convergence rates.

</details>

### [236] [Near-optimal algorithms for private estimation and sequential testing of collision probability](https://arxiv.org/abs/2504.13804)
*Robert Busa-Fekete,Umar Syed*

Main category: stat.ML

TLDR: 本文提出了新的算法来估计和测试碰撞概率，改进了现有方法的样本复杂度，并在实验中验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 碰撞概率是离散分布的基本度量，广泛应用于多个科学领域，但现有方法的样本复杂度较高，需要改进。

Method: 提出了一种满足局部差分隐私的估计算法和一种序列测试算法，分别用于估计碰撞概率和区分不同碰撞概率值。

Result: 新算法在样本复杂度上接近最优，实验表明其所需样本数显著少于现有方法。

Conclusion: 新算法在理论和实验中均表现出优越性，为碰撞概率的估计和测试提供了高效工具。

Abstract: We present new algorithms for estimating and testing \emph{collision
probability}, a fundamental measure of the spread of a discrete distribution
that is widely used in many scientific fields. We describe an algorithm that
satisfies $(\alpha, \beta)$-local differential privacy and estimates collision
probability with error at most $\epsilon$ using
$\tilde{O}\left(\frac{\log(1/\beta)}{\alpha^2 \epsilon^2}\right)$ samples for
$\alpha \le 1$, which improves over previous work by a factor of
$\frac{1}{\alpha^2}$. We also present a sequential testing algorithm for
collision probability, which can distinguish between collision probability
values that are separated by $\epsilon$ using $\tilde{O}(\frac{1}{\epsilon^2})$
samples, even when $\epsilon$ is unknown. Our algorithms have nearly the
optimal sample complexity, and in experiments we show that they require
significantly fewer samples than previous methods.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [237] [LangCoop: Collaborative Driving with Language](https://arxiv.org/abs/2504.13406)
*Xiangbo Gao,Yuheng Wu,Rujia Wang,Chenxi Liu,Yang Zhou,Zhengzhong Tu*

Main category: cs.RO

TLDR: LangCoop提出了一种基于自然语言的多智能体协作自动驾驶新范式，显著降低了通信带宽需求，同时保持了驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体通信方法受限于高带宽需求、智能体异构性和信息丢失等问题，亟需一种更高效的通信媒介。

Method: LangCoop采用M$^3$CoT进行结构化零样本视觉语言推理，以及LangPack将信息高效打包为简洁的语言消息。

Result: 在CARLA模拟实验中，LangCoop实现了96%的通信带宽降低（每条消息<2KB），同时驾驶性能表现优异。

Conclusion: LangCoop通过自然语言通信，为多智能体协作自动驾驶提供了一种高效且可靠的解决方案。

Abstract: Multi-agent collaboration holds great promise for enhancing the safety,
reliability, and mobility of autonomous driving systems by enabling information
sharing among multiple connected agents. However, existing multi-agent
communication approaches are hindered by limitations of existing communication
media, including high bandwidth demands, agent heterogeneity, and information
loss. To address these challenges, we introduce LangCoop, a new paradigm for
collaborative autonomous driving that leverages natural language as a compact
yet expressive medium for inter-agent communication. LangCoop features two key
innovations: Mixture Model Modular Chain-of-thought (M$^3$CoT) for structured
zero-shot vision-language reasoning and Natural Language Information Packaging
(LangPack) for efficiently packaging information into concise, language-based
messages. Through extensive experiments conducted in the CARLA simulations, we
demonstrate that LangCoop achieves a remarkable 96\% reduction in communication
bandwidth (< 2KB per message) compared to image-based communication, while
maintaining competitive driving performance in the closed-loop evaluation.

</details>

### [238] [Chain-of-Modality: Learning Manipulation Programs from Multimodal Human Videos with Vision-Language-Models](https://arxiv.org/abs/2504.13351)
*Chen Wang,Fei Xia,Wenhao Yu,Tingnan Zhang,Ruohan Zhang,C. Karen Liu,Li Fei-Fei,Jie Tan,Jacky Liang*

Main category: cs.RO

TLDR: 该论文提出了一种名为Chain-of-Modality (CoM)的方法，通过结合视觉、肌肉活动和音频信号，帮助机器人从人类视频中学习任务计划和控制参数。


<details>
  <summary>Details</summary>
Motivation: 人类视频无法完全捕捉任务执行中的控制参数（如力），因此需要结合其他传感器数据（如肌肉活动和声音）来增强学习效果。

Method: 使用CoM提示策略，逐步整合多模态数据（视频、肌肉信号、音频），生成任务计划和详细控制参数。

Result: 实验表明，CoM在提取任务计划和参数上的准确率比基线方法提高了三倍，并在真实机器人实验中表现出良好的泛化能力。

Conclusion: CoM通过多模态数据显著提升了机器人从人类视频中学习任务的能力，为机器人操作任务提供了新的解决方案。

Abstract: Learning to perform manipulation tasks from human videos is a promising
approach for teaching robots. However, many manipulation tasks require changing
control parameters during task execution, such as force, which visual data
alone cannot capture. In this work, we leverage sensing devices such as
armbands that measure human muscle activities and microphones that record
sound, to capture the details in the human manipulation process, and enable
robots to extract task plans and control parameters to perform the same task.
To achieve this, we introduce Chain-of-Modality (CoM), a prompting strategy
that enables Vision Language Models to reason about multimodal human
demonstration data -- videos coupled with muscle or audio signals. By
progressively integrating information from each modality, CoM refines a task
plan and generates detailed control parameters, enabling robots to perform
manipulation tasks based on a single multimodal human video prompt. Our
experiments show that CoM delivers a threefold improvement in accuracy for
extracting task plans and control parameters compared to baselines, with strong
generalization to new task setups and objects in real-world robot experiments.
Videos and code are available at https://chain-of-modality.github.io

</details>

### [239] [Lightweight LiDAR-Camera 3D Dynamic Object Detection and Multi-Class Trajectory Prediction](https://arxiv.org/abs/2504.13647)
*Yushen He,Lei Zhao,Tianchen Deng,Zipeng Fang,Weidong Chen*

Main category: cs.RO

TLDR: 提出了一种轻量级多模态框架，用于3D物体检测和轨迹预测，结合LiDAR和相机输入，实时感知行人、车辆和骑行者。


<details>
  <summary>Details</summary>
Motivation: 服务移动机器人需在有限计算资源下避开动态物体，因此需要高效的多模态感知系统。

Method: 提出两个新模块：1) Cross-Modal Deformable Transformer (CMDT)用于高精度物体检测；2) Reference Trajectory-based Multi-Class Transformer (RTMCT)用于多类物体轨迹预测。

Result: 在CODa基准测试中表现优于现有方法（检测mAP提升2.03%，行人轨迹预测minADE5降低0.408m），并在入门级GPU上实现13.2 fps实时推理。

Conclusion: 该系统具有出色的部署能力，代码已开源，便于复现和实际应用。

Abstract: Service mobile robots are often required to avoid dynamic objects while
performing their tasks, but they usually have only limited computational
resources. So we present a lightweight multi-modal framework for 3D object
detection and trajectory prediction. Our system synergistically integrates
LiDAR and camera inputs to achieve real-time perception of pedestrians,
vehicles, and riders in 3D space. The framework proposes two novel modules: 1)
a Cross-Modal Deformable Transformer (CMDT) for object detection with high
accuracy and acceptable amount of computation, and 2) a Reference
Trajectory-based Multi-Class Transformer (RTMCT) for efficient and diverse
trajectory prediction of mult-class objects with flexible trajectory lengths.
Evaluations on the CODa benchmark demonstrate superior performance over
existing methods across detection (+2.03% in mAP) and trajectory prediction
(-0.408m in minADE5 of pedestrians) metrics. Remarkably, the system exhibits
exceptional deployability - when implemented on a wheelchair robot with an
entry-level NVIDIA 3060 GPU, it achieves real-time inference at 13.2 fps. To
facilitate reproducibility and practical deployment, we release the related
code of the method at https://github.com/TossherO/3D_Perception and its ROS
inference version at https://github.com/TossherO/ros_packages.

</details>

### [240] [Hysteresis-Aware Neural Network Modeling and Whole-Body Reinforcement Learning Control of Soft Robots](https://arxiv.org/abs/2504.13582)
*Zongyuan Chen,Yan Xia,Jiayuan Liu,Jijia Liu,Wenhao Tang,Jiayu Chen,Feng Gao,Longfei Ma,Hongen Liao,Yu Wang,Chao Yu,Boyu Zhang,Fei Xing*

Main category: cs.RO

TLDR: 提出了一种用于手术应用的软体机器人系统，开发了基于滞后感知的神经网络模型，并通过强化学习训练控制策略，实验验证了其高精度和潜在临床应用价值。


<details>
  <summary>Details</summary>
Motivation: 软体机器人因其柔顺性和安全性适合手术应用，但其非线性与滞后行为对建模和控制提出挑战。

Method: 提出滞后感知的神经网络模型，构建并行仿真环境，应用强化学习训练控制策略，并进行物理实验验证。

Result: 滞后感知模型将MSE降低84.95%，控制算法在真实机器人上实现0.126至0.250 mm的轨迹跟踪误差。

Conclusion: 该方法在手术实验中表现优异，展示了其在复杂场景（如临床）中的应用潜力。

Abstract: Soft robots exhibit inherent compliance and safety, which makes them
particularly suitable for applications requiring direct physical interaction
with humans, such as surgical procedures. However, their nonlinear and
hysteretic behavior, resulting from the properties of soft materials, presents
substantial challenges for accurate modeling and control. In this study, we
present a soft robotic system designed for surgical applications and propose a
hysteresis-aware whole-body neural network model that accurately captures and
predicts the soft robot's whole-body motion, including its hysteretic behavior.
Building upon the high-precision dynamic model, we construct a highly parallel
simulation environment for soft robot control and apply an on-policy
reinforcement learning algorithm to efficiently train whole-body motion control
strategies. Based on the trained control policy, we developed a soft robotic
system for surgical applications and validated it through phantom-based laser
ablation experiments in a physical environment. The results demonstrate that
the hysteresis-aware modeling reduces the Mean Squared Error (MSE) by 84.95
percent compared to traditional modeling methods. The deployed control
algorithm achieved a trajectory tracking error ranging from 0.126 to 0.250 mm
on the real soft robot, highlighting its precision in real-world conditions.
The proposed method showed strong performance in phantom-based surgical
experiments and demonstrates its potential for complex scenarios, including
future real-world clinical applications.

</details>

### [241] [Green Robotic Mixed Reality with Gaussian Splatting](https://arxiv.org/abs/2504.13697)
*Chenxuan Liu,He Li,Zongze Li,Shuai Wang,Wei Xu,Kejiang Ye,Derrick Wing Kwan Ng,Chengzhong Xu*

Main category: cs.RO

TLDR: 论文提出了一种基于高斯泼溅（GS）的RoboMR系统（GSRMR），通过减少高分辨率图像的上传频率来降低能耗，并进一步提出了GS跨层优化（GSCLO）框架和加速惩罚优化（APO）算法。实验表明，GSRMR能将通信能耗降低10倍以上，并在PSNR和SSIM指标上优于基线方案。


<details>
  <summary>Details</summary>
Motivation: 在机器人混合现实（RoboMR）系统中，高分辨率图像的高频上传导致通信能耗高，难以实现绿色通信。

Method: 提出GSRMR系统，利用高斯泼溅模型减少图像上传需求，并通过GSCLO框架和APO算法优化内容切换和功率分配。

Result: GSRMR将通信能耗降低10倍以上，并在PSNR和SSIM指标上优于基线方案。

Conclusion: GSRMR为绿色RoboMR提供了有效解决方案，显著降低了通信能耗并提升了图像质量。

Abstract: Realizing green communication in robotic mixed reality (RoboMR) systems
presents a challenge, due to the necessity of uploading high-resolution images
at high frequencies through wireless channels. This paper proposes Gaussian
splatting (GS) RoboMR (GSRMR), which achieves a lower energy consumption and
makes a concrete step towards green RoboMR. The crux to GSRMR is to build a GS
model which enables the simulator to opportunistically render a photo-realistic
view from the robot's pose, thereby reducing the need for excessive image
uploads. Since the GS model may involve discrepancies compared to the actual
environments, a GS cross-layer optimization (GSCLO) framework is further
proposed, which jointly optimizes content switching (i.e., deciding whether to
upload image or not) and power allocation across different frames. The GSCLO
problem is solved by an accelerated penalty optimization (APO) algorithm.
Experiments demonstrate that the proposed GSRMR reduces the communication
energy by over 10x compared with RoboMR. Furthermore, the proposed GSRMR with
APO outperforms extensive baseline schemes, in terms of peak signal-to-noise
ratio (PSNR) and structural similarity index measure (SSIM).

</details>

### [242] [SLAM&Render: A Benchmark for the Intersection Between Neural Rendering, Gaussian Splatting and SLAM](https://arxiv.org/abs/2504.13713)
*Samuel Cerezo,Gaetano Meli,Tomás Berriel Martins,Kirill Safronov,Javier Civera*

Main category: cs.RO

TLDR: SLAM&Render是一个新数据集，旨在评估SLAM与神经渲染交叉领域的方法，包含多模态数据和多种场景条件。


<details>
  <summary>Details</summary>
Motivation: 现有数据集未能涵盖SLAM和神经渲染的特定挑战（如多模态、序列性、视角和光照变化），因此需要新的基准数据集。

Method: 引入SLAM&Render数据集，包含40个序列，涵盖RGB、深度、IMU、机器人运动数据和真实位姿，支持多种场景和光照条件。

Result: 实验验证了SLAM&Render作为新兴研究领域基准的相关性。

Conclusion: SLAM&Render填补了现有数据集的空白，为SLAM与神经渲染的交叉研究提供了重要工具。

Abstract: Models and methods originally developed for novel view synthesis and scene
rendering, such as Neural Radiance Fields (NeRF) and Gaussian Splatting, are
increasingly being adopted as representations in Simultaneous Localization and
Mapping (SLAM). However, existing datasets fail to include the specific
challenges of both fields, such as multimodality and sequentiality in SLAM or
generalization across viewpoints and illumination conditions in neural
rendering. To bridge this gap, we introduce SLAM&Render, a novel dataset
designed to benchmark methods in the intersection between SLAM and novel view
rendering. It consists of 40 sequences with synchronized RGB, depth, IMU, robot
kinematic data, and ground-truth pose streams. By releasing robot kinematic
data, the dataset also enables the assessment of novel SLAM strategies when
applied to robot manipulators. The dataset sequences span five different setups
featuring consumer and industrial objects under four different lighting
conditions, with separate training and test trajectories per scene, as well as
object rearrangements. Our experimental results, obtained with several
baselines from the literature, validate SLAM&Render as a relevant benchmark for
this emerging research area.

</details>

### [243] [Learning Through Retrospection: Improving Trajectory Prediction for Automated Driving with Error Feedback](https://arxiv.org/abs/2504.13785)
*Steffen Hagedorn,Aron Distelzweig,Marcel Hallgarten,Alexandru P. Condurache*

Main category: cs.RO

TLDR: 论文提出了一种新的回顾技术，通过闭环训练利用反馈改进轨迹预测，减少误差。


<details>
  <summary>Details</summary>
Motivation: 现有模型在自动驾驶中独立进行轨迹预测，无法纠正错误，导致重复误差。

Method: 提出回顾技术，通过闭环训练利用反馈分析错误，改进后续预测。

Result: 在nuScenes和Argoverse数据集上，最小平均位移误差降低31.9%。

Conclusion: 回顾技术显著提升预测质量，尤其适用于异常场景。

Abstract: In automated driving, predicting trajectories of surrounding vehicles
supports reasoning about scene dynamics and enables safe planning for the ego
vehicle. However, existing models handle predictions as an instantaneous task
of forecasting future trajectories based on observed information. As time
proceeds, the next prediction is made independently of the previous one, which
means that the model cannot correct its errors during inference and will repeat
them. To alleviate this problem and better leverage temporal data, we propose a
novel retrospection technique. Through training on closed-loop rollouts the
model learns to use aggregated feedback. Given new observations it reflects on
previous predictions and analyzes its errors to improve the quality of
subsequent predictions. Thus, the model can learn to correct systematic errors
during inference. Comprehensive experiments on nuScenes and Argoverse
demonstrate a considerable decrease in minimum Average Displacement Error of up
to 31.9% compared to the state-of-the-art baseline without retrospection. We
further showcase the robustness of our technique by demonstrating a better
handling of out-of-distribution scenarios with undetected road-users.

</details>

### [244] [Imitation Learning with Precisely Labeled Human Demonstrations](https://arxiv.org/abs/2504.13803)
*Yilong Song*

Main category: cs.RO

TLDR: 通过为手持夹爪分配独特颜色，简化了RANSAC和ICP配准方法，用于精确末端执行器姿态估计，提升了模仿学习中人类演示的效果。


<details>
  <summary>Details</summary>
Motivation: 模仿学习中，人类演示是宝贵的数据来源，但现有方法在精确动作推断、弥补本体差异及与前沿机器人训练流程融合方面存在挑战。

Method: 利用用户对手持夹爪外观的控制（独特颜色），简化RANSAC和ICP配准方法，实现精确末端执行器姿态估计。

Result: 仿真实验显示，精确标注的人类演示单独使用可达机器人演示性能的88.1%，与机器人演示结合时进一步提升性能。

Conclusion: 通过独特颜色标记和精确姿态估计，人类演示在模仿学习中具有显著价值，并能与机器人演示互补。

Abstract: Within the imitation learning paradigm, training generalist robots requires
large-scale datasets obtainable only through diverse curation. Due to the
relative ease to collect, human demonstrations constitute a valuable addition
when incorporated appropriately. However, existing methods utilizing human
demonstrations face challenges in inferring precise actions, ameliorating
embodiment gaps, and fusing with frontier generalist robot training pipelines.
In this work, building on prior studies that demonstrate the viability of using
hand-held grippers for efficient data collection, we leverage the user's
control over the gripper's appearance--specifically by assigning it a unique,
easily segmentable color--to enable simple and reliable application of the
RANSAC and ICP registration method for precise end-effector pose estimation. We
show in simulation that precisely labeled human demonstrations on their own
allow policies to reach on average 88.1% of the performance of using robot
demonstrations, and boost policy performance when combined with robot
demonstrations, despite the inherent embodiment gap.

</details>

<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [245] [Addressing the Minor-Embedding Problem in Quantum Annealing and Evaluating State-of-the-Art Algorithm Performance](https://arxiv.org/abs/2504.13376)
*Aitor Gómez-Tejedor,Eneko Osaba,Esther Villar-Rodriguez*

Main category: quant-ph

TLDR: 该研究探讨了量子退火处理器中Ising模型变量的映射问题，分析了嵌入质量对性能的影响，并评估了Minorminer算法的表现。


<details>
  <summary>Details</summary>
Motivation: 量子退火器在处理适合其架构的问题与非硬件原生拓扑问题时性能差异显著，研究旨在分析嵌入质量对性能的影响并评估Minorminer算法的效果。

Method: 通过实验分析嵌入链长度与解误差的关系，并比较Minorminer与Clique Embedding算法的性能。

Result: 嵌入质量显著影响量子退火性能，Minorminer算法表现不稳定，未明显优于最坏情况下的Clique Embedding。

Conclusion: Minorminer算法仍有改进空间，嵌入质量对量子退火性能至关重要。

Abstract: This study addresses the minor-embedding problem, which involves mapping the
variables of an Ising model onto a quantum annealing processor. The primary
motivation stems from the observed performance disparity of quantum annealers
when solving problems suited to the processor's architecture versus those with
non-hardware-native topologies. Our research has two main objectives: i) to
analyze the impact of embedding quality on the performance of D-Wave Systems
quantum annealers, and ii) to evaluate the quality of the embeddings generated
by Minorminer, an algorithm provided by D-Wave and widely recognized as the
standard minor-embedding technique in the literature. Regarding the first
objective, our experiments reveal a clear correlation between the average chain
length of embeddings and the relative errors of the solutions sampled. This
underscores the critical influence of embedding quality on quantum annealing
performance. For the second objective, we focus on the Minorminer technique,
assessing its capacity to embed problems, the quality of the embeddings
produced, and the robustness of the results. We also compare its performance
with Clique Embedding, another algorithm developed by D-Wave, which is
deterministic and designed to embed fully connected Ising models into quantum
annealing processors, serving as a worst-case scenario. The results demonstrate
that there is significant room for improvement for Minorminer, as it has not
consistently outperformed the worst-case scenario.

</details>

### [246] [Adaptive Non-local Observable on Quantum Neural Networks](https://arxiv.org/abs/2504.13414)
*Hsin-Yi Lin,Huan-Hsin Tseng,Samuel Yen-Chi Chen,Shinjae Yoo*

Main category: quant-ph

TLDR: 论文提出了一种基于海森堡绘景的自适应非局域测量框架，通过动态可观测量的引入，提升了量子电路的模型复杂度，并在分类任务中表现优于传统VQC。


<details>
  <summary>Details</summary>
Motivation: 传统变分量子电路（VQC）通常依赖固定的厄米可观测算符，限制了模型的复杂性和灵活性。

Method: 提出动态厄米可观测量的框架，优化VQC旋转对应可观测空间的轨迹，并引入两种非局域测量方案。

Result: 数值模拟显示，该方法在分类任务中优于传统VQC，提供了更强大且资源高效的量子神经网络方案。

Conclusion: 该方法扩展了VQC的设计空间，揭示了传统VQC仅为海森堡表示的特例，为量子机器学习提供了新思路。

Abstract: Conventional Variational Quantum Circuits (VQCs) for Quantum Machine Learning
typically rely on a fixed Hermitian observable, often built from Pauli
operators. Inspired by the Heisenberg picture, we propose an adaptive non-local
measurement framework that substantially increases the model complexity of the
quantum circuits. Our introduction of dynamical Hermitian observables with
evolving parameters shows that optimizing VQC rotations corresponds to tracing
a trajectory in the observable space. This viewpoint reveals that standard VQCs
are merely a special case of the Heisenberg representation.
  Furthermore, we show that properly incorporating variational rotations with
non-local observables enhances qubit interaction and information mixture,
admitting flexible circuit designs. Two non-local measurement schemes are
introduced, and numerical simulations on classification tasks confirm that our
approach outperforms conventional VQCs, yielding a more powerful and
resource-efficient approach as a Quantum Neural Network.

</details>

### [247] [A Quantum of Learning: Using Quaternion Algebra to Model Learning on Quantum Devices](https://arxiv.org/abs/2504.13232)
*Sayed Pouria Talebi,Clive Cheong Took,Danilo P. Mandic*

Main category: quant-ph

TLDR: 本文提出了一种基于四元数的量子学习机器训练方法，通过HR-演算开发了全面的训练框架。


<details>
  <summary>Details</summary>
Motivation: 设计适应性和优化技术以训练量子学习机器，建立类似经典神经元的信息处理单元。

Method: 利用四元数代数建模量子计算和测量操作，并基于HR-演算开发训练框架。

Result: 模型具有数学可操作性，并建立了性能标准（如收敛条件）。

Conclusion: 四元数模型为量子学习机器提供了有效的训练基础。

Abstract: This article considers the problem of designing adaption and optimisation
techniques for training quantum learning machines. To this end, the division
algebra of quaternions is used to derive an effective model for representing
computation and measurement operations on qubits. In turn, the derived model,
serves as the foundation for formulating an adaptive learning problem on
principal quantum learning units, thereby establishing quantum information
processing units akin to that of neurons in classical approaches. Then,
leveraging the modern HR-calculus, a comprehensive training framework for
learning on quantum machines is developed. The quaternion-valued model
accommodates mathematical tractability and establishment of performance
criteria, such as convergence conditions.

</details>

### [248] [Quantum repeaters enhanced by vacuum beam guides](https://arxiv.org/abs/2504.13397)
*Yu Gan,Mohadeseh Azar,Nitish Kumar Chandra,Xin Jin,Jinglei Cheng,Kaushik P. Seshadreesan,Junyu Liu*

Main category: quant-ph

TLDR: 研究探讨了利用真空光束导引（VBGs）替代传统光纤链路，以减少量子通信网络中的光子损失和退相干问题，从而扩展中继站间距并降低硬件复杂性。


<details>
  <summary>Details</summary>
Motivation: 大规模量子通信网络的发展面临光子损失和退相干的关键挑战，限制了传输距离并需要密集的中继站网络。

Method: 通过将VBGs集成到中继架构中，分析了不同代中继器的性能权衡，并进行了成本函数分析。

Result: 第一代中继器通过消除纠缠纯化显著降低成本，第三代中继器因链路传输成功率的提升而受益，而第二代中继器的性能受限于逻辑门错误而非信道损失。

Conclusion: VBGs是构建可扩展高性能量子网络的有力工具，尤其适合与近期的量子硬件能力结合使用。

Abstract: The development of large-scale quantum communication networks faces critical
challenges due to photon loss and decoherence in optical fiber channels. These
fundamentally limit transmission distances and demand dense networks of
repeater stations. This work investigates using vacuum beam guides (VBGs)-a
promising ultra-low-loss transmission platform-as an alternative to traditional
fiber links. By incorporating VBGs into repeater-based architectures, we
demonstrate that the inter-repeater spacing can be substantially extended,
resulting in fewer required nodes and significantly reducing hardware and
operational complexity. We perform a cost-function analysis to quantify
performance trade-offs across first, second, and third-generation repeaters.
Our results show that first-generation repeaters reduce costs dramatically by
eliminating entanglement purification. Third-generation repeaters benefit from
improved link transmission success, which is crucial for quantum error
correction. In contrast, second-generation repeaters exhibit a more nuanced
response; although transmission loss is reduced, their performance remains
primarily limited by logical gate errors rather than channel loss. These
findings highlight that while all repeater generations benefit from reduced
photon loss, the magnitude of improvement depends critically on the underlying
error mechanisms. Vacuum beam guides thus emerge as a powerful enabler for
scalable, high-performance quantum networks, particularly in conjunction with
near-term quantum hardware capabilities.

</details>

### [249] [Quantum Walks-Based Adaptive Distribution Generation with Efficient CUDA-Q Acceleration](https://arxiv.org/abs/2504.13532)
*Yen-Jui Chang,Wei-Ting Wang,Chen-Yu Liu,Yun-Yuan Wang,Ching-Ray Chang*

Main category: quant-ph

TLDR: 提出了一种基于量子行走的自适应分布生成器，结合变分量子电路和离散时间量子行走，实现高效目标概率分布生成。


<details>
  <summary>Details</summary>
Motivation: 传统方法在生成高精度概率分布时效率较低，而量子算法在理论上具有优势，但缺乏实际高性能计算的实现。

Method: 整合变分量子电路与离散时间量子行走（如分裂步量子行走及其纠缠扩展），动态调整硬币参数以驱动量子态演化。

Result: 在CUDA-Q框架下实现GPU加速，显著降低计算开销，并在金融模拟和二维模式生成（如数字0~9）中表现出高仿真保真度。

Conclusion: 该方法填补了理论量子算法与高性能计算之间的鸿沟，为实际应用提供了高效解决方案。

Abstract: We present a novel Adaptive Distribution Generator that leverages a quantum
walks-based approach to generate high precision and efficiency of target
probability distributions. Our method integrates variational quantum circuits
with discrete-time quantum walks, specifically, split-step quantum walks and
their entangled extensions, to dynamically tune coin parameters and drive the
evolution of quantum states towards desired distributions. This enables
accurate one-dimensional probability modeling for applications such as
financial simulation and structured two-dimensional pattern generation
exemplified by digit representations(0~9). Implemented within the CUDA-Q
framework, our approach exploits GPU acceleration to significantly reduce
computational overhead and improve scalability relative to conventional
methods. Extensive benchmarks demonstrate that our Quantum Walks-Based Adaptive
Distribution Generator achieves high simulation fidelity and bridges the gap
between theoretical quantum algorithms and practical high-performance
computation.

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [250] [SwitchMT: An Adaptive Context Switching Methodology for Scalable Multi-Task Learning in Intelligent Autonomous Agents](https://arxiv.org/abs/2504.13541)
*Avaneesh Devkota,Rachmad Vidya Wicaksana Putra,Muhammad Shafique*

Main category: cs.NE

TLDR: SwitchMT是一种自适应任务切换方法，用于强化学习中的多任务学习，通过深度脉冲Q网络和自适应策略解决任务干扰问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在多任务学习中表现不佳，任务干扰和数据流处理能力不足是主要问题。

Method: 提出SwitchMT方法，结合深度脉冲Q网络（带主动树突和决斗结构）和自适应任务切换策略。

Result: 在多个Atari游戏中表现优异（如Pong: -8.8, Breakout: 5.6, Enduro: 355.2），优于现有方法。

Conclusion: SwitchMT有效解决了任务干扰问题，提升了多任务学习的自动化能力，为高效通用代理奠定了基础。

Abstract: The ability to train intelligent autonomous agents (such as mobile robots) on
multiple tasks is crucial for adapting to dynamic real-world environments.
However, state-of-the-art reinforcement learning (RL) methods only excel in
single-task settings, and still struggle to generalize across multiple tasks
due to task interference. Moreover, real-world environments also demand the
agents to have data stream processing capabilities. Toward this, a
state-of-the-art work employs Spiking Neural Networks (SNNs) to improve
multi-task learning by exploiting temporal information in data stream, while
enabling lowpower/energy event-based operations. However, it relies on fixed
context/task-switching intervals during its training, hence limiting the
scalability and effectiveness of multi-task learning. To address these
limitations, we propose SwitchMT, a novel adaptive task-switching methodology
for RL-based multi-task learning in autonomous agents. Specifically, SwitchMT
employs the following key ideas: (1) a Deep Spiking Q-Network with active
dendrites and dueling structure, that utilizes task-specific context signals to
create specialized sub-networks; and (2) an adaptive task-switching policy that
leverages both rewards and internal dynamics of the network parameters.
Experimental results demonstrate that SwitchMT achieves superior performance in
multi-task learning compared to state-of-the-art methods. It achieves
competitive scores in multiple Atari games (i.e., Pong: -8.8, Breakout: 5.6,
and Enduro: 355.2) compared to the state-of-the-art, showing its better
generalized learning capability. These results highlight the effectiveness of
our SwitchMT methodology in addressing task interference while enabling
multi-task learning automation through adaptive task switching, thereby paving
the way for more efficient generalist agents with scalable multi-task learning
capabilities.

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [251] [CodeVisionary: An Agent-based Framework for Evaluating Large Language Models in Code Generation](https://arxiv.org/abs/2504.13472)
*Xinchen Wang,Pengfei Gao,Chao Peng,Ruida Hu,Cuiyun Gao*

Main category: cs.SE

TLDR: CodeVisionary是一个基于LLM的框架，用于评估代码生成模型的性能，通过多源知识分析和协商评分提升评估效果。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（人工、基于指标、基于LLM）存在效率低或依赖参考答案的问题，而基于LLM的方法因缺乏多源知识和复杂代码理解能力受限。

Method: CodeVisionary分为两阶段：多源知识分析阶段和协商评分阶段，结合多法官讨论达成共识。

Result: 实验显示CodeVisionary在Pearson、Spearman和Kendall-Tau系数上优于基线方法，并提供详细评估报告。

Conclusion: CodeVisionary显著提升了代码生成模型的评估效果，为开发者提供了改进方向。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in code
generation, underscoring the critical need for rigorous and comprehensive
evaluation. Existing evaluation approaches fall into three categories,
including human-centered, metric-based, and LLM-based. Considering that
human-centered approaches are labour-intensive and metric-based ones overly
rely on reference answers, LLM-based approaches are gaining increasing
attention due to their stronger contextual understanding capabilities and
superior efficiency. However, the performance of LLM-based approaches remains
limited due to: (1) lack of multisource domain knowledge, and (2) insufficient
comprehension of complex code.
  To mitigate the limitations, we propose CodeVisionary, the first LLM-based
agent framework for evaluating LLMs in code generation. CodeVisionary consists
of two stages: (1) Multiscore knowledge analysis stage, which aims to gather
multisource and comprehensive domain knowledge by formulating and executing a
stepwise evaluation plan. (2) Negotiation-based scoring stage, which involves
multiple judges engaging in discussions to better comprehend the complex code
and reach a consensus on the evaluation score. Extensive experiments
demonstrate that CodeVisionary achieves the best performance for evaluating
LLMs in code generation, outperforming the best baseline methods with average
improvements of 0.202, 0.139, and 0.117 in Pearson, Spearman, and Kendall-Tau
coefficients, respectively. Besides, CodeVisionary provides detailed evaluation
reports, which assist developers in identifying shortcomings and making
improvements. The resources of CodeVisionary are available at
https://anonymous.4open.science/r/CodeVisionary.

</details>

### [252] [Large Language Models for Validating Network Protocol Parsers](https://arxiv.org/abs/2504.13515)
*Mingwei Zheng,Danning Xie,Xiangyu Zhang*

Main category: cs.SE

TLDR: PARVAL是一个基于大语言模型的多代理框架，用于自动化验证网络协议解析器是否符合协议标准，成功识别了实现与标准之间的不一致性。


<details>
  <summary>Details</summary>
Motivation: 网络协议解析器的错误可能导致严重漏洞，现有方法要么需要大量手动工作，要么忽略协议标准，难以检测语义违规。

Method: PARVAL利用大语言模型理解自然语言和代码，将协议标准和实现转换为统一的中间表示（格式规范），并进行差异比较。

Result: 在BFD协议上的实验表明，PARVAL成功识别不一致性，假阳性率低至5.6%，并发现7个独特错误（包括5个新问题）。

Conclusion: PARVAL为自动化验证协议解析器提供了一种有效方法，显著减少了手动工作并提高了准确性。

Abstract: Network protocol parsers are essential for enabling correct and secure
communication between devices. Bugs in these parsers can introduce critical
vulnerabilities, including memory corruption, information leakage, and
denial-of-service attacks. An intuitive way to assess parser correctness is to
compare the implementation with its official protocol standard. However, this
comparison is challenging because protocol standards are typically written in
natural language, whereas implementations are in source code. Existing methods
like model checking, fuzzing, and differential testing have been used to find
parsing bugs, but they either require significant manual effort or ignore the
protocol standards, limiting their ability to detect semantic violations. To
enable more automated validation of parser implementations against protocol
standards, we propose PARVAL, a multi-agent framework built on large language
models (LLMs). PARVAL leverages the capabilities of LLMs to understand both
natural language and code. It transforms both protocol standards and their
implementations into a unified intermediate representation, referred to as
format specifications, and performs a differential comparison to uncover
inconsistencies. We evaluate PARVAL on the Bidirectional Forwarding Detection
(BFD) protocol. Our experiments demonstrate that PARVAL successfully identifies
inconsistencies between the implementation and its RFC standard, achieving a
low false positive rate of 5.6%. PARVAL uncovers seven unique bugs, including
five previously unknown issues.

</details>

### [253] [Do Prompt Patterns Affect Code Quality? A First Empirical Assessment of ChatGPT-Generated Code](https://arxiv.org/abs/2504.13656)
*Antonio Della Porta,Stefano Lambiase,Fabio Palomba*

Main category: cs.SE

TLDR: 论文研究了提示模式对代码质量的影响，发现提示结构对ChatGPT生成的代码质量（可维护性、安全性和可靠性）影响不大。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在代码生成中存在性能不一致、幻觉和质量问题，提示工程可能改善这些问题，但提示模式对代码质量的影响尚未深入研究。

Method: 使用Dev-GPT数据集，实证研究提示模式对代码质量的影响，分析7583个代码文件的质量指标。

Result: 零样本提示最常见，其次是零样本结合思维链和少样本提示；Kruskal-Wallis测试显示提示模式对质量指标无显著差异。

Conclusion: 提示结构可能不会显著影响ChatGPT辅助代码生成的质量指标。

Abstract: Large Language Models (LLMs) have rapidly transformed software development,
especially in code generation. However, their inconsistent performance, prone
to hallucinations and quality issues, complicates program comprehension and
hinders maintainability. Research indicates that prompt engineering-the
practice of designing inputs to direct LLMs toward generating relevant
outputs-may help address these challenges. In this regard, researchers have
introduced prompt patterns, structured templates intended to guide users in
formulating their requests. However, the influence of prompt patterns on code
quality has yet to be thoroughly investigated. An improved understanding of
this relationship would be essential to advancing our collective knowledge on
how to effectively use LLMs for code generation, thereby enhancing their
understandability in contemporary software development. This paper empirically
investigates the impact of prompt patterns on code quality, specifically
maintainability, security, and reliability, using the Dev-GPT dataset. Results
show that Zero-Shot prompting is most common, followed by Zero-Shot with
Chain-of-Thought and Few-Shot. Analysis of 7583 code files across quality
metrics revealed minimal issues, with Kruskal-Wallis tests indicating no
significant differences among patterns, suggesting that prompt structure may
not substantially impact these quality metrics in ChatGPT-assisted code
generation.

</details>

### [254] [A Survey for What Developers Require in AI-powered Tools that Aid in Component Selection in CBSD](https://arxiv.org/abs/2504.13751)
*Mahdi Jaberzadeh Ansari,Ann Barcomb*

Main category: cs.SE

TLDR: 论文探讨了组件选择工具的现状，通过调查行业实践和研究，提出改进方向，并探索了AI驱动工具的前景。


<details>
  <summary>Details</summary>
Motivation: 尽管组件化软件开发研究已有40多年，但行业仍缺乏广泛接受的组件选择工具或方法，行业与学术界的脱节是原因之一。

Method: 采用混合方法调查了近100名从事组件化软件工程实践或研究的人员，了解行业问题、需求及当前最佳实践。

Result: 调查结果揭示了行业对组件选择质量标准的优先级，并探讨了专业人士对AI驱动工具的现状和未来看法。

Conclusion: 研究呼吁组件选择工具应结合最新技术进展，尤其是AI，以满足行业需求。

Abstract: Although it has been more than four decades that the first components-based
software development (CBSD) studies were conducted, there is still no standard
method or tool for component selection which is widely accepted by the
industry. The gulf between industry and academia contributes to the lack of an
accepted tool. We conducted a mixed methods survey of nearly 100 people engaged
in component-based software engineering practice or research to better
understand the problems facing industry, how these needs could be addressed,
and current best practices employed in component selection. We also sought to
identify and prioritize quality criteria for component selection from an
industry perspective. In response to the call for CBSD component selection
tools to incorporate recent technical advances, we also explored the
perceptions of professionals about AI-driven tools, present and envisioned.

</details>

<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [255] [Pricing AI Model Accuracy](https://arxiv.org/abs/2504.13375)
*Nikhil Kumar*

Main category: econ.TH

TLDR: 竞争市场中，AI模型提供商通过优化错误维度（假阳性或假阴性）来提升利润，而非整体准确性。


<details>
  <summary>Details</summary>
Motivation: 研究企业在竞争市场中如何通过优化模型错误维度来提升利润，而非单纯追求整体准确性。

Method: 建立消费者-企业双寡头模型，分析竞争对企业优化模型错误维度的影响。

Result: 企业倾向于投资其优势错误维度以提升利润，而非整体准确性；这种策略对消费者不利但提升整体福利。

Conclusion: 竞争市场中，企业应专注于优化其优势错误维度，而非盲目追求整体准确性。

Abstract: This paper examines the market for AI models in which firms compete to
provide accurate model predictions and consumers exhibit heterogeneous
preferences for model accuracy. We develop a consumer-firm duopoly model to
analyze how competition affects firms' incentives to improve model accuracy.
Each firm aims to minimize its model's error, but this choice can often be
suboptimal. Counterintuitively, we find that in a competitive market, firms
that improve overall accuracy do not necessarily improve their profits. Rather,
each firm's optimal decision is to invest further on the error dimension where
it has a competitive advantage. By decomposing model errors into false positive
and false negative rates, firms can reduce errors in each dimension through
investments. Firms are strictly better off investing on their superior
dimension and strictly worse off with investments on their inferior dimension.
Profitable investments adversely affect consumers but increase overall welfare.

</details>