<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 19]
- [cs.LG](#cs.LG) [Total: 74]
- [cs.CL](#cs.CL) [Total: 25]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.CV](#cs.CV) [Total: 52]
- [cs.CE](#cs.CE) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.OH](#cs.OH) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.SC](#cs.SC) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.RO](#cs.RO) [Total: 10]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.IR](#cs.IR) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [math.DS](#math.DS) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from Large Language Models](https://arxiv.org/abs/2505.00817)
*Andrew Adiletta,Berk Sunar*

Main category: cs.CR

TLDR: 论文提出了一种名为“Spill The Beans”的新型缓存侧信道攻击方法，用于泄露大型语言模型（LLM）生成的令牌。通过攻击者与受害者模型共享硬件资源，利用缓存命中检测令牌，实验证明该方法可行，并揭示了LLM部署中的新漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的普及，共享硬件资源上的侧信道攻击对机密性的威胁日益增加。本文旨在探索如何利用缓存侧信道攻击泄露LLM生成的令牌。

Method: 攻击者与受害者模型共享硬件，通过刷新和重新加载嵌入层的嵌入向量（每个令牌对应唯一向量），利用缓存命中检测令牌。为应对LLM规模大导致缓存快速失效的问题，作者平衡了监控令牌数量与信息泄露量。

Result: 实验证明，攻击者可以成功泄露LLM生成的令牌，例如单次监控可恢复80%-90%的高熵API密钥或40%的英文文本。结果表明LLM易受传统侧信道攻击影响。

Conclusion: 研究揭示了LLM部署中的新漏洞，强调了隐私和安全问题，并提出了缓解此类威胁的建议。

Abstract: Side-channel attacks on shared hardware resources increasingly threaten
confidentiality, especially with the rise of Large Language Models (LLMs). In
this work, we introduce Spill The Beans, a novel application of cache
side-channels to leak tokens generated by an LLM. By co-locating an attack
process on the same hardware as the victim model, we flush and reload embedding
vectors from the embedding layer, where each token corresponds to a unique
embedding vector. When accessed during token generation, it results in a cache
hit detectable by our attack on shared lower-level caches.
  A significant challenge is the massive size of LLMs, which, by nature of
their compute intensive operation, quickly evicts embedding vectors from the
cache. We address this by balancing the number of tokens monitored against the
amount of information leaked. Monitoring more tokens increases potential
vocabulary leakage but raises the chance of missing cache hits due to eviction;
monitoring fewer tokens improves detection reliability but limits vocabulary
coverage.
  Through extensive experimentation, we demonstrate the feasibility of leaking
tokens from LLMs via cache side-channels. Our findings reveal a new
vulnerability in LLM deployments, highlighting that even sophisticated models
are susceptible to traditional side-channel attacks. We discuss the
implications for privacy and security in LLM-serving infrastructures and
suggest considerations for mitigating such threats. For proof of concept we
consider two concrete attack scenarios: Our experiments show that an attacker
can recover as much as 80%-90% of a high entropy API key with single shot
monitoring. As for English text we can reach a 40% recovery rate with a single
shot. We should note that the rate highly depends on the monitored token set
and these rates can be improved by targeting more specialized output domains.

</details>

### [2] [From Texts to Shields: Convergence of Large Language Models and Cybersecurity](https://arxiv.org/abs/2505.00841)
*Tao Li,Ya-Ting Yang,Yunian Pan,Quanyan Zhu*

Main category: cs.CR

TLDR: 报告探讨了大型语言模型（LLMs）与网络安全的结合，分析了其在软件和网络安全、5G漏洞分析等领域的应用，并提出了解决信任、透明度等挑战的策略。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在网络安全中的潜力，解决其部署中的技术和社会挑战。

Method: 综合网络安全、人工智能、形式化方法和人本设计的跨学科见解。

Result: LLMs可自动化复杂任务、提升效率，但需解决信任、透明度等问题。

Conclusion: 提出了一个前瞻性研究议程，推动LLMs在网络安全中的安全有效应用。

Abstract: This report explores the convergence of large language models (LLMs) and
cybersecurity, synthesizing interdisciplinary insights from network security,
artificial intelligence, formal methods, and human-centered design. It examines
emerging applications of LLMs in software and network security, 5G
vulnerability analysis, and generative security engineering. The report
highlights the role of agentic LLMs in automating complex tasks, improving
operational efficiency, and enabling reasoning-driven security analytics.
Socio-technical challenges associated with the deployment of LLMs -- including
trust, transparency, and ethical considerations -- can be addressed through
strategies such as human-in-the-loop systems, role-specific training, and
proactive robustness testing. The report further outlines critical research
challenges in ensuring interpretability, safety, and fairness in LLM-based
systems, particularly in high-stakes domains. By integrating technical advances
with organizational and societal considerations, this report presents a
forward-looking research agenda for the secure and effective adoption of LLMs
in cybersecurity.

</details>

### [3] [OET: Optimization-based prompt injection Evaluation Toolkit](https://arxiv.org/abs/2505.00843)
*Jinsheng Pan,Xiaogeng Liu,Chaowei Xiao*

Main category: cs.CR

TLDR: 论文介绍了OET，一个基于优化的评估工具包，用于系统评估提示注入攻击和防御的有效性，特别是在自适应对抗场景下。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）易受提示注入攻击，现有防御策略缺乏标准化评估框架。

Method: 提出OET工具包，采用模块化工作流程和自适应测试框架，结合优化方法生成最坏情况对抗示例。

Result: 实验表明当前防御机制存在局限性，部分模型即使经过安全增强仍易受攻击。

Conclusion: OET为评估对抗鲁棒性提供了统一平台，揭示了现有防御的不足。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation, enabling their widespread
adoption across various domains. However, their susceptibility to prompt
injection attacks poses significant security risks, as adversarial inputs can
manipulate model behavior and override intended instructions. Despite numerous
defense strategies, a standardized framework to rigorously evaluate their
effectiveness, especially under adaptive adversarial scenarios, is lacking. To
address this gap, we introduce OET, an optimization-based evaluation toolkit
that systematically benchmarks prompt injection attacks and defenses across
diverse datasets using an adaptive testing framework. Our toolkit features a
modular workflow that facilitates adversarial string generation, dynamic attack
execution, and comprehensive result analysis, offering a unified platform for
assessing adversarial robustness. Crucially, the adaptive testing framework
leverages optimization methods with both white-box and black-box access to
generate worst-case adversarial examples, thereby enabling strict red-teaming
evaluations. Extensive experiments underscore the limitations of current
defense mechanisms, with some models remaining susceptible even after
implementing security enhancements.

</details>

### [4] [TherMod Communication: Low Power or Hot Air?](https://arxiv.org/abs/2505.00849)
*Christiana Chamon*

Main category: cs.CR

TLDR: 本文批判性地分析了Basar提出的无线KLJN方案（TherMod）的“低功耗”和安全声明，指出其功耗实际更高且安全性存疑。


<details>
  <summary>Details</summary>
Motivation: 评估无线KLJN方案（TherMod）的低功耗和安全声明是否成立。

Method: 通过分析无线KLJN方案的额外组件及其功耗，并对比原始KLJN方案的安全性。

Result: 无线KLJN方案的功耗显著增加，且安全性无法直接继承原始方案，存在重大漏洞。

Conclusion: Basar的无线KLJN方案“低功耗”声明不成立，且安全性存疑，与2005年的不安全方案类似。

Abstract: The Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange scheme leverages
statistical physics to enable secure communication with zero average power flow
in a wired channel. While the original KLJN scheme requires significant power
for operation, a recent wireless modification, TherMod, proposed by Basar
claims a "low power" implementation. This paper critically examines this claim.
We explain that the additional components inherent in Basar's wireless
adaptation substantially increase power consumption, rendering the "low power"
assertion inappropriate. Furthermore, we clarify that the security claims of
the original KLJN scheme do not directly translate to this wireless adaptation,
implying significant security breach. Finally, the scheme looks identical one
of the stealth communicators from 2005, which was shown not to be secure.

</details>

### [5] [Duality on the Thermodynamics of the Kirchhoff-Law-Johnson-Noise (KLJN) Secure Key Exchange Scheme](https://arxiv.org/abs/2505.00858)
*Sarah Flanery,Anson Trapani,Christiana Chamon,Leyla Nazhandali*

Main category: cs.CR

TLDR: 研究探讨了KLJN安全密钥交换方案中信息泄漏的双重检测方法，通过采样零电压时刻的电流而非零电流时刻的电压，验证了非平衡系统中信息泄漏的存在。


<details>
  <summary>Details</summary>
Motivation: 探索KLJN系统中信息泄漏的检测方法，验证双重方法是否能揭示非平衡系统中的泄漏。

Method: 采样零电压时刻的电流，与之前采样零电流时刻的电压的方法形成对比。

Result: 双重方法成功检测到信息泄漏，证实了KLJN系统中热平衡对无条件安全的必要性。

Conclusion: 研究支持了KLJN系统中热平衡的重要性，为无条件安全提供了进一步证据。

Abstract: This study investigates a duality approach to information leak detection in
the generalized Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange scheme.
While previous work by Chamon and Kish sampled voltages at zero-current
instances, this research explores sampling currents at zero-voltage crossings.
The objective is to determine if this dual approach can reveal information
leaks in non-equilibrium KLJN systems. Results indicate that the duality method
successfully detects information leaks, further supporting the necessity of
thermal equilibrium for unconditional security in KLJN systems.

</details>

### [6] [Protocol-agnostic and Data-free Backdoor Attacks on Pre-trained Models in RF Fingerprinting](https://arxiv.org/abs/2505.00881)
*Tianya Zhao,Ningning Wang,Junqing Zhang,Xuyu Wang*

Main category: cs.CR

TLDR: 论文研究了在无监督预训练模型（PTMs）中进行数据无关的后门攻击，针对RF指纹识别任务，展示了攻击的广泛适用性及防御的困难性。


<details>
  <summary>Details</summary>
Motivation: 监督DNN在RF指纹识别中存在领域偏移和标记数据稀缺问题，PTMs虽能解决这些问题，但其安全性尚未充分研究。

Method: 设计触发器和预定义输出表示（PORs），通过后门训练将攻击行为植入PTMs，无需下游数据或标签信息。

Result: 实验证明攻击对多种输入域、协议和PTMs均有效，且防御难度大。

Conclusion: PTMs在RF指纹识别中存在严重后门漏洞，需进一步研究防御方法。

Abstract: While supervised deep neural networks (DNNs) have proven effective for device
authentication via radio frequency (RF) fingerprinting, they are hindered by
domain shift issues and the scarcity of labeled data. The success of large
language models has led to increased interest in unsupervised pre-trained
models (PTMs), which offer better generalization and do not require labeled
datasets, potentially addressing the issues mentioned above. However, the
inherent vulnerabilities of PTMs in RF fingerprinting remain insufficiently
explored. In this paper, we thoroughly investigate data-free backdoor attacks
on such PTMs in RF fingerprinting, focusing on a practical scenario where
attackers lack access to downstream data, label information, and training
processes. To realize the backdoor attack, we carefully design a set of
triggers and predefined output representations (PORs) for the PTMs. By mapping
triggers and PORs through backdoor training, we can implant backdoor behaviors
into the PTMs, thereby introducing vulnerabilities across different downstream
RF fingerprinting tasks without requiring prior knowledge. Extensive
experiments demonstrate the wide applicability of our proposed attack to
various input domains, protocols, and PTMs. Furthermore, we explore potential
detection and defense methods, demonstrating the difficulty of fully
safeguarding against our proposed backdoor attack.

</details>

### [7] [Balancing Security and Liquidity: A Time-Weighted Snapshot Framework for DAO Governance Voting](https://arxiv.org/abs/2505.00888)
*Zayn Wang,Frank Pu,Vinci Cheung,Robert Hao*

Main category: cs.CR

TLDR: 论文探讨了区块链中DAO面临的闪贷攻击问题，并提出了一种新的防御结构。


<details>
  <summary>Details</summary>
Motivation: 随着区块链行业的发展，DAO通过治理代币投票管理资金，但闪贷攻击可能操纵投票结果，威胁项目安全。

Method: 研究了现有的闪贷攻击防御机制及其弱点，并基于观察提出了一种新的防御结构。

Result: 新防御结构通过案例验证了其有效性。

Conclusion: 提出的防御结构能增强DAO的安全性，减少闪贷攻击的风险。

Abstract: As new project upgrading the blockchain industry, novel forms of attack
challenges developers to rethink about the design of their innovations. In the
growth stage of the development, Decentralized Autonomous Organizations (DAO)
introduces different approaches in managing fund through voting in governance
tokens. However, relying on tokens as a weight for voting introduces
opportunities for hackers to manipulate voting results through flash loan,
allowing malicious proposals - fund withdrawal from DAO to hacker's wallet - to
execute through the smart contract. In this research, we learned different
defense mechanism against the flash loan attack, and their weakness in
accessibility that compromise the security of different blockchain projects.
Based on our observation, we propose a new defensing structure and apply it
with cases.

</details>

### [8] [Non-Adaptive Cryptanalytic Time-Space Lower Bounds via a Shearer-like Inequality for Permutations](https://arxiv.org/abs/2505.00894)
*Itai Dinur,Nathan Keller,Avichai Marmor*

Main category: cs.CR

TLDR: 本文研究了非自适应算法在密码分析中的时间-空间权衡问题，证明了在某些情况下非自适应算法的性能无法超越经典的自适应算法。


<details>
  <summary>Details</summary>
Motivation: 探讨自适应算法在密码分析中的优势，特别是针对离散对数问题（DLOG）等密码学问题的时间-空间权衡。

Method: 提出了一种新模型，用于统一分析非自适应预处理算法，并利用Shearer引理的变体进行证明。

Result: 证明了在非自适应设置下，DLOG问题的时间复杂度无法改进，除非预处理字符串长度超过Ω(√N)。

Conclusion: 非自适应算法在密码分析中的性能受限，自适应算法（如Pollard's rho）在预处理支持下具有显著优势。

Abstract: The power of adaptivity in algorithms has been intensively studied in diverse
areas of theoretical computer science. In this paper, we obtain a number of
sharp lower bound results which show that adaptivity provides a significant
extra power in cryptanalytic time-space tradeoffs with (possibly unlimited)
preprocessing time.
  Most notably, we consider the discrete logarithm (DLOG) problem in a generic
group of $N$ elements. The classical `baby-step giant-step' algorithm for the
problem has time complexity $T=O(\sqrt{N})$, uses $O(\sqrt{N})$ bits of space
(up to logarithmic factors in $N$) and achieves constant success probability.
  We examine a generalized setting where an algorithm obtains an advice string
of $S$ bits and is allowed to make $T$ arbitrary non-adaptive queries that
depend on the advice string (but not on the challenge group element).
  We show that in this setting, the $T=O(\sqrt{N})$ online time complexity of
the baby-step giant-step algorithm cannot be improved, unless the advice string
is more than $\Omega(\sqrt{N})$ bits long. This lies in stark contrast with the
classical adaptive Pollard's rho algorithm for DLOG, which can exploit
preprocessing to obtain the tradeoff curve $ST^2=O(N)$. We obtain similar sharp
lower bounds for several other cryptanalytic problems.
  To obtain our results, we present a new model that allows analyzing
non-adaptive preprocessing algorithms for a wide array of search and decision
problems in a unified way. Since previous proof techniques inherently cannot
distinguish between adaptive and non-adaptive algorithms for the problems in
our model, they cannot be used to obtain our results. Consequently, our proof
uses a variant of Shearer's lemma for this setting, due to Barthe,
Cordero-Erausquin, Ledoux, and Maurey (2011). This seems to be the first time a
variant of Shearer's lemma for permutations is used in an algorithmic context.

</details>

### [9] [Attack and defense techniques in large language models: A survey and new perspectives](https://arxiv.org/abs/2505.00976)
*Zhiyu Liao,Kang Chen,Yuanguo Lin,Kangkang Li,Yunxuan Liu,Hefeng Chen,Xingwang Huang,Yuanhui Yu*

Main category: cs.CR

TLDR: 本文系统综述了大语言模型（LLMs）的攻击与防御技术，分类了攻击类型并分析了防御策略，同时指出了当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: LLMs在自然语言处理任务中广泛应用，但其安全性和伦理问题引发了关注，需要系统研究攻击与防御技术以提升模型安全性。

Method: 通过分类攻击类型（如对抗性提示攻击、优化攻击、模型窃取等）和分析防御策略（预防性和检测性方法），系统梳理了LLMs的安全问题。

Result: 研究发现尽管防御技术有所进展，但仍需应对动态威胁、平衡可用性与鲁棒性，并解决资源限制问题。

Conclusion: 未来需发展自适应防御、可解释安全技术和标准化评估框架，强调跨学科合作与伦理考量以提升LLMs的安全性。

Abstract: Large Language Models (LLMs) have become central to numerous natural language
processing tasks, but their vulnerabilities present significant security and
ethical challenges. This systematic survey explores the evolving landscape of
attack and defense techniques in LLMs. We classify attacks into adversarial
prompt attack, optimized attacks, model theft, as well as attacks on
application of LLMs, detailing their mechanisms and implications. Consequently,
we analyze defense strategies, including prevention-based and detection-based
defense methods. Although advances have been made, challenges remain to adapt
to the dynamic threat landscape, balance usability with robustness, and address
resource constraints in defense implementation. We highlight open problems,
including the need for adaptive scalable defenses, explainable security
techniques, and standardized evaluation frameworks. This survey provides
actionable insights and directions for developing secure and resilient LLMs,
emphasizing the importance of interdisciplinary collaboration and ethical
considerations to mitigate risks in real-world applications.

</details>

### [10] [Capability-Based Multi-Tenant Access Management in Crowdsourced Drone Services](https://arxiv.org/abs/2505.01048)
*Junaid Akram,Ali Anaissi,Awais Akram,Youcef Djenouri,Palash Ingle,Rutvij H. Jhaveri*

Main category: cs.CR

TLDR: 提出了一种基于OAuth 2.0和可验证凭证（VCs）的访问控制方法，用于无人机众包服务中的资源共享。通过将VCs集成到OAuth 2.0中，创建了一种新型访问令牌，提高了安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决可验证凭证缺乏标准化协议的问题，同时提升无人机服务中的资源访问控制的安全性和灵活性。

Method: 将VCs封装到OAuth 2.0的访问令牌中，使用JWT和JWS进行验证，并提出了基于OAuth 2.0客户端凭证授予的VC创建协议。

Result: 系统支持多租户环境，提高了数据管理的效率和安全性，适用于森林火灾管理等场景。

Conclusion: 该方法通过增强OAuth 2.0的功能，实现了长期使用和高效数据管理，同时提升了隐私保护和数据可移植性。

Abstract: We propose a capability-based access control method that leverages OAuth 2.0
and Verifiable Credentials (VCs) to share resources in crowdsourced drone
services. VCs securely encode claims about entities, offering flexibility.
However, standardized protocols for VCs are lacking, limiting their adoption.
To address this, we integrate VCs into OAuth 2.0, creating a novel access
token. This token encapsulates VCs using JSON Web Tokens (JWT) and employs
JWT-based methods for proof of possession. Our method streamlines VC
verification with JSON Web Signatures (JWS) requires only minor adjustments to
current OAuth 2.0 systems. Furthermore, in order to increase security and
efficiency in multi-tenant environments, we provide a novel protocol for VC
creation that makes use of the OAuth 2.0 client credentials grant. Using VCs as
access tokens enhances OAuth 2.0, supporting long-term use and efficient data
management. This system aids bushfire management authorities by ensuring high
availability, enhanced privacy, and improved data portability. It supports
multi-tenancy, allowing drone operators to control data access policies in a
decentralized environment.

</details>

### [11] [Good News for Script Kiddies? Evaluating Large Language Models for Automated Exploit Generation](https://arxiv.org/abs/2505.01065)
*David Jin,Qian Fu,Yuekang Li*

Main category: cs.CR

TLDR: 本文首次系统研究了大语言模型（LLMs）在自动化漏洞利用生成（AEG）中的表现，评估了其合作性和技术能力。通过引入重构的安全实验室基准和LLM驱动的攻击者设计，实验发现GPT-4和GPT-4o合作性高，但所有模型均未成功生成漏洞利用。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在AEG中的潜力，以解决对其自动化漏洞利用能力的担忧。

Method: 引入重构的软件安全实验室基准，设计LLM驱动的攻击者系统生成漏洞利用提示。

Result: GPT-4和GPT-4o合作性高，但所有模型均未成功生成漏洞利用；GPT-4o的错误最少，显示潜力。

Conclusion: LLMs在AEG中尚未成熟，但GPT-4o的表现表明未来可能取得进展。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code-related tasks, raising concerns about their potential for automated
exploit generation (AEG). This paper presents the first systematic study on
LLMs' effectiveness in AEG, evaluating both their cooperativeness and technical
proficiency. To mitigate dataset bias, we introduce a benchmark with refactored
versions of five software security labs. Additionally, we design an LLM-based
attacker to systematically prompt LLMs for exploit generation. Our experiments
reveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to
uncensored models, while Llama3 is the most resistant. However, no model
successfully generates exploits for refactored labs, though GPT-4o's minimal
errors highlight the potential for LLM-driven AEG advancements.

</details>

### [12] [A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in Model Repositories](https://arxiv.org/abs/2505.01067)
*Ziqi Ding,Qian Fu,Junchen Ding,Gelei Deng,Yi Liu,Yuekang Li*

Main category: cs.CR

TLDR: 研究首次全面分析了Hugging Face平台上的恶意配置文件，揭示了三种攻击场景，并提出了基于LLM的工具CONFIGSCAN以检测威胁。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）推动了多样化的AI应用发展，但AI供应链（如Hugging Face）中的配置文件安全问题被忽视，可能被利用执行未授权代码。

Method: 提出了CONFIGSCAN工具，利用LLM分析配置文件及其关联的运行时代码和关键库，以高准确率和低误报率检测可疑内容。

Result: 评估发现数千个可疑仓库和配置文件，凸显了AI模型托管平台加强安全验证的紧迫性。

Conclusion: 研究强调了配置文件安全的重要性，并为平台提供了有效的检测工具。

Abstract: Recent advancements in large language models (LLMs) have spurred the
development of diverse AI applications from code generation and video editing
to text generation; however, AI supply chains such as Hugging Face, which host
pretrained models and their associated configuration files contributed by the
public, face significant security challenges; in particular, configuration
files originally intended to set up models by specifying parameters and initial
settings can be exploited to execute unauthorized code, yet research has
largely overlooked their security compared to that of the models themselves; in
this work, we present the first comprehensive study of malicious configurations
on Hugging Face, identifying three attack scenarios (file, website, and
repository operations) that expose inherent risks; to address these threats, we
introduce CONFIGSCAN, an LLM-based tool that analyzes configuration files in
the context of their associated runtime code and critical libraries,
effectively detecting suspicious elements with low false positive rates and
high accuracy; our extensive evaluation uncovers thousands of suspicious
repositories and configuration files, underscoring the urgent need for enhanced
security validation in AI model hosting platforms.

</details>

### [13] [Poster: Machine Learning for Vulnerability Detection as Target Oracle in Automated Fuzz Driver Generation](https://arxiv.org/abs/2505.01123)
*Gianpietro Castiglione,Marcello Maugeri,Giampaolo Bella*

Main category: cs.CR

TLDR: 提出了一种结合机器学习和模糊测试的自动化模糊驱动生成方法，用于漏洞检测。


<details>
  <summary>Details</summary>
Motivation: 机器学习在漏洞检测中存在高误报率，模糊测试需要手动编写模糊驱动，限制了其效率。

Method: 利用机器学习模型作为目标预言机识别潜在漏洞函数，自动生成模糊驱动，并通过模糊测试验证漏洞。

Result: 在libgd的现有漏洞上验证了方法的有效性，并计划进行大规模评估。

Conclusion: 该方法自动化了模糊驱动的生成，提高了漏洞检测的效率。

Abstract: In vulnerability detection, machine learning has been used as an effective
static analysis technique, although it suffers from a significant rate of false
positives. Contextually, in vulnerability discovery, fuzzing has been used as
an effective dynamic analysis technique, although it requires manually writing
fuzz drivers. Fuzz drivers usually target a limited subset of functions in a
library that must be chosen according to certain criteria, e.g., the depth of a
function, the number of paths. These criteria are verified by components called
target oracles. In this work, we propose an automated fuzz driver generation
workflow composed of: (1) identifying a likely vulnerable function by
leveraging a machine learning for vulnerability detection model as a target
oracle, (2) automatically generating fuzz drivers, (3) fuzzing the target
function to find bugs which could confirm the vulnerability inferred by the
target oracle. We show our method on an existing vulnerability in libgd, with a
plan for large-scale evaluation.

</details>

### [14] [Active Sybil Attack and Efficient Defense Strategy in IPFS DHT](https://arxiv.org/abs/2505.01139)
*V. H. M. Netto,T. Cholez,C. L. Ignat*

Main category: cs.CR

TLDR: IPFS面临新的主动Sybil攻击，恶意节点返回虚假数据，成功隐藏内容。提出SR-DHT-Store作为新防御策略，有效抵御攻击。


<details>
  <summary>Details</summary>
Motivation: IPFS依赖的DHT结构易受Sybil攻击，现有防御策略无法完全应对新型主动攻击，需开发更有效的解决方案。

Method: 提出SR-DHT-Store，基于区域查询的动态XOR距离，实现高效且抗Sybil的内容发布。

Result: 新型攻击在80%的查询中成功隐藏内容，SR-DHT-Store能完全抵御被动和主动攻击，且开销更低。

Conclusion: SR-DHT-Store是一种高效、可扩展的防御方案，适用于IPFS的逐步部署。

Abstract: The InterPlanetary File System (IPFS) is a decentralized peer-to-peer (P2P)
storage that relies on Kademlia, a Distributed Hash Table (DHT) structure
commonly used in P2P systems for its proved scalability. However, DHTs are
known to be vulnerable to Sybil attacks, in which a single entity controls
multiple malicious nodes. Recent studies have shown that IPFS is affected by a
passive content eclipse attack, leveraging Sybils, in which adversarial nodes
hide received indexed information from other peers, making the content appear
unavailable. Fortunately, the latest mitigation strategy coupling an attack
detection based on statistical tests and a wider publication strategy upon
detection was able to circumvent it.
  In this work, we present a new active attack, with malicious nodes responding
with semantically correct but intentionally false data, exploiting both an
optimized placement of Sybils to stay below the detection threshold and an
early trigger of the content discovery termination in Kubo, the main IPFS
implementation. Our attack achieves to completely eclipse content on the latest
Kubo release. When evaluated against the most recent known mitigation, it
successfully denies access to the target content in approximately 80\% of
lookup attempts.
  To address this vulnerability, we propose a new mitigation called
SR-DHT-Store, which enables efficient, Sybil-resistant content publication
without relying on attack detection but instead on a systematic and precise use
of region-based queries, defined by a dynamically computed XOR distance to the
target ID. SR-DHT-Store can be combined with other defense mechanisms resulting
in a defense strategy that completely mitigates both passive and active Sybil
attacks at a lower overhead, while allowing an incremental deployment.

</details>

### [15] [LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures](https://arxiv.org/abs/2505.01177)
*Francisco Aguilera-Martínez,Fernando Berzal*

Main category: cs.CR

TLDR: 该论文综述了大型语言模型（LLMs）的安全威胁与防御机制，分类分析了训练阶段和部署后的攻击，并探讨了预防与检测两类防御策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的发展，评估其训练和部署阶段的安全威胁与漏洞变得至关重要。

Method: 通过分类和分析LLMs的攻击方式，并总结预防与检测两类防御机制。

Result: 提供了攻击与防御策略的全面分析，并评估了现有防御机制的有效性。

Conclusion: 论文为LLMs安全提供了结构化框架，并指出需进一步研究的领域以应对新兴安全挑战。

Abstract: As large language models (LLMs) continue to evolve, it is critical to assess
the security threats and vulnerabilities that may arise both during their
training phase and after models have been deployed. This survey seeks to define
and categorize the various attacks targeting LLMs, distinguishing between those
that occur during the training phase and those that affect already trained
models. A thorough analysis of these attacks is presented, alongside an
exploration of defense mechanisms designed to mitigate such threats. Defenses
are classified into two primary categories: prevention-based and
detection-based defenses. Furthermore, our survey summarizes possible attacks
and their corresponding defense strategies. It also provides an evaluation of
the effectiveness of the known defense mechanisms for the different security
threats. Our survey aims to offer a structured framework for securing LLMs,
while also identifying areas that require further research to improve and
strengthen defenses against emerging security challenges.

</details>

### [16] [Secure Cluster-Based Hierarchical Federated Learning in Vehicular Networks](https://arxiv.org/abs/2505.01186)
*M. Saeid HaghighiFard,Sinem Coleri*

Main category: cs.CR

TLDR: 提出了一种针对分层联邦学习（HFL）的新型防御框架，通过动态车辆选择、异常检测和加权梯度平均，有效抵御高斯噪声和梯度上升攻击，显著减少收敛时间。


<details>
  <summary>Details</summary>
Motivation: HFL在车载网络中面临对抗性和不可靠车辆的威胁，其误导性更新会损害全局模型的完整性和收敛性。

Method: 结合动态车辆选择、Z-score和余弦相似性分析的异常检测、自适应阈值机制、加权梯度平均以及跨集群一致性检查。

Result: 仿真结果显示，该算法在1跳和3跳拓扑中显著减少了收敛时间。

Conclusion: 提出的多级防御策略能有效过滤恶意贡献，提升HFL的鲁棒性和效率。

Abstract: Hierarchical Federated Learning (HFL) has recently emerged as a promising
solution for intelligent decision-making in vehicular networks, helping to
address challenges such as limited communication resources, high vehicle
mobility, and data heterogeneity. However, HFL remains vulnerable to
adversarial and unreliable vehicles, whose misleading updates can significantly
compromise the integrity and convergence of the global model. To address these
challenges, we propose a novel defense framework that integrates dynamic
vehicle selection with robust anomaly detection within a cluster-based HFL
architecture, specifically designed to counter Gaussian noise and gradient
ascent attacks. The framework performs a comprehensive reliability assessment
for each vehicle by evaluating historical accuracy, contribution frequency, and
anomaly records. Anomaly detection combines Z-score and cosine similarity
analyses on model updates to identify both statistical outliers and directional
deviations in model updates. To further refine detection, an adaptive
thresholding mechanism is incorporated into the cosine similarity metric,
dynamically adjusting the threshold based on the historical accuracy of each
vehicle to enforce stricter standards for consistently high-performing
vehicles. In addition, a weighted gradient averaging mechanism is implemented,
which assigns higher weights to gradient updates from more trustworthy
vehicles. To defend against coordinated attacks, a cross-cluster consistency
check is applied to identify collaborative attacks in which multiple
compromised clusters coordinate misleading updates. Together, these mechanisms
form a multi-level defense strategy to filter out malicious contributions
effectively. Simulation results show that the proposed algorithm significantly
reduces convergence time compared to benchmark methods across both 1-hop and
3-hop topologies.

</details>

### [17] [PHSafe: Disclosure Avoidance for the 2020 Census Supplemental Demographic and Housing Characteristics File (S-DHC)](https://arxiv.org/abs/2505.01254)
*William Sexton,Skye Berghel,Bayard Carlson,Sam Haney,Luke Hartman,Michael Hay,Ashwin Machanavajjhala,Gerome Miklau,Amritha Pai,Simran Rajpal,David Pujol,Ruchit Shrestha,Daniel Simmons-Marengo*

Main category: cs.CR

TLDR: 本文介绍了美国人口普查局用于保护2020年人口普查补充人口和住房特征文件（S-DHC）的披露避免算法PHSafe，该算法基于离散高斯分布添加噪声，并满足零集中差分隐私。


<details>
  <summary>Details</summary>
Motivation: 保护人口普查数据中的敏感信息，确保统计结果的隐私性。

Method: 使用PHSafe算法，通过离散高斯分布添加噪声，并在Tumult Analytics平台上实现。

Result: 算法满足零集中差分隐私，有效保护了数据隐私。

Conclusion: PHSafe算法是一种有效的隐私保护方法，适用于人口普查数据的披露避免。

Abstract: This article describes the disclosure avoidance algorithm that the U.S.
Census Bureau used to protect the 2020 Census Supplemental Demographic and
Housing Characteristics File (S-DHC). The tabulations contain statistics of
counts of U.S. persons living in certain types of households, including
averages. The article describes the PHSafe algorithm, which is based on adding
noise drawn from a discrete Gaussian distribution to the statistics of
interest. We prove that the algorithm satisfies a well-studied variant of
differential privacy, called zero-concentrated differential privacy. We then
describe how the algorithm was implemented on Tumult Analytics and briefly
outline the parameterization and tuning of the algorithm.

</details>

### [18] [Fine-grained Manipulation Attacks to Local Differential Privacy Protocols for Data Streams](https://arxiv.org/abs/2505.01292)
*Xinyu Li,Xuebin Ren,Shusen Yang,Liang Shi,Chia-Mu Yu*

Main category: cs.CR

TLDR: 本文研究了流数据场景下的本地差分隐私（LDP）协议的安全性问题，提出了针对流数据LDP协议的细粒度操纵攻击框架，并验证了其有效性，同时探讨了可能的防御机制。


<details>
  <summary>Details</summary>
Motivation: 现有LDP攻击主要针对静态协议，忽视了流数据场景下的安全性问题，本文旨在填补这一空白。

Method: 通过分析现有算法的攻击面，提出了一种模块化的统一攻击框架，能够操纵LDP估计的流数据。

Result: 攻击框架适用于多种流数据LDP算法和分析任务，并通过理论和实验验证了其有效性。

Conclusion: 本文揭示了流数据LDP协议的安全隐患，并提出了可能的防御方向。

Abstract: Local Differential Privacy (LDP) enables massive data collection and analysis
while protecting end users' privacy against untrusted aggregators. It has been
applied to various data types (e.g., categorical, numerical, and graph data)
and application settings (e.g., static and streaming). Recent findings indicate
that LDP protocols can be easily disrupted by poisoning or manipulation
attacks, which leverage injected/corrupted fake users to send crafted data
conforming to the LDP reports. However, current attacks primarily target static
protocols, neglecting the security of LDP protocols in the streaming settings.
Our research fills the gap by developing novel fine-grained manipulation
attacks to LDP protocols for data streams. By reviewing the attack surfaces in
existing algorithms, We introduce a unified attack framework with composable
modules, which can manipulate the LDP estimated stream toward a target stream.
Our attack framework can adapt to state-of-the-art streaming LDP algorithms
with different analytic tasks (e.g., frequency and mean) and LDP models
(event-level, user-level, w-event level). We validate our attacks theoretically
and through extensive experiments on real-world datasets, and finally explore a
possible defense mechanism for mitigating these attacks.

</details>

### [19] [Constrained Network Adversarial Attacks: Validity, Robustness, and Transferability](https://arxiv.org/abs/2505.01328)
*Anass Grini,Oumaima Taheri,Btissam El Khamlichi,Amal El Fallah-Seghrouchni*

Main category: cs.CR

TLDR: 研究发现现有对抗攻击方法在物联网（IoT）环境中常违反领域约束，导致80.3%的对抗样本无效，误导了对NIDS模型脆弱性的评估。


<details>
  <summary>Details</summary>
Motivation: 揭示对抗攻击方法在IoT环境中因忽略领域约束（如数值和分类限制）而导致的无效对抗样本问题，避免资源分配误导。

Method: 使用多层感知机（MLP）作为替代模型生成更有效的对抗样本，并分析其对其他ML/DL模型的转移性。

Result: MLP生成的对抗样本比复杂模型（如CNN和LSTM）更有效，且需考虑领域约束以准确评估模型脆弱性。

Conclusion: 在设计安全关键的IoT和网络应用ML/DL模型时，需同时考虑领域约束和模型架构以确保鲁棒性。

Abstract: While machine learning has significantly advanced Network Intrusion Detection
Systems (NIDS), particularly within IoT environments where devices generate
large volumes of data and are increasingly susceptible to cyber threats, these
models remain vulnerable to adversarial attacks. Our research reveals a
critical flaw in existing adversarial attack methodologies: the frequent
violation of domain-specific constraints, such as numerical and categorical
limits, inherent to IoT and network traffic. This leads to up to 80.3% of
adversarial examples being invalid, significantly overstating real-world
vulnerabilities. These invalid examples, though effective in fooling models, do
not represent feasible attacks within practical IoT deployments. Consequently,
relying on these results can mislead resource allocation for defense, inflating
the perceived susceptibility of IoT-enabled NIDS models to adversarial
manipulation. Furthermore, we demonstrate that simpler surrogate models like
Multi-Layer Perceptron (MLP) generate more valid adversarial examples compared
to complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate,
we analyze the transferability of adversarial severity to other ML/DL models
commonly used in IoT contexts. This work underscores the importance of
considering both domain constraints and model architecture when evaluating and
designing robust ML/DL models for security-critical IoT and network
applications.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [Constructing an Optimal Behavior Basis for the Option Keyboard](https://arxiv.org/abs/2505.00787)
*Lucas N. Alegre,Ana L. C. Bazzan,André Barreto,Bruno C. da Silva*

Main category: cs.LG

TLDR: 本文提出了一种高效构建最优行为基的新方法，显著减少了确保新任务最优性所需的基策略数量，并在复杂任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习旨在快速为新任务找到解决方案，而无需额外环境交互。现有方法如GPI和Option Keyboard虽有效，但性能依赖基策略选择，且计算成本高。

Method: 引入一种新方法，高效构建最优行为基，确保零-shot识别线性任务的最优解，并证明其表达能力优于CCS。

Result: 实验表明，该方法显著减少基策略数量，在复杂任务中优于现有方法。

Conclusion: 该方法解决了最优行为基的开放问题，为多任务强化学习提供了更高效的解决方案。

Abstract: Multi-task reinforcement learning aims to quickly identify solutions for new
tasks with minimal or no additional interaction with the environment.
Generalized Policy Improvement (GPI) addresses this by combining a set of base
policies to produce a new one that is at least as good -- though not
necessarily optimal -- as any individual base policy. Optimality can be
ensured, particularly in the linear-reward case, via techniques that compute a
Convex Coverage Set (CCS). However, these are computationally expensive and do
not scale to complex domains. The Option Keyboard (OK) improves upon GPI by
producing policies that are at least as good -- and often better. It achieves
this through a learned meta-policy that dynamically combines base policies.
However, its performance critically depends on the choice of base policies.
This raises a key question: is there an optimal set of base policies -- an
optimal behavior basis -- that enables zero-shot identification of optimal
solutions for any linear tasks? We solve this open problem by introducing a
novel method that efficiently constructs such an optimal behavior basis. We
show that it significantly reduces the number of base policies needed to ensure
optimality in new tasks. We also prove that it is strictly more expressive than
a CCS, enabling particular classes of non-linear tasks to be solved optimally.
We empirically evaluate our technique in challenging domains and show that it
outperforms state-of-the-art approaches, increasingly so as task complexity
increases.

</details>

### [21] [Improving Routing in Sparse Mixture of Experts with Graph of Tokens](https://arxiv.org/abs/2505.00792)
*Tam Nguyen,Ngoc N. Tran,Khai Nguyen,Richard G. Baraniuk*

Main category: cs.LG

TLDR: 稀疏混合专家（SMoE）通过每样本激活少量参数实现深度学习的高扩展性，但存在路由波动问题。本文从概率图模型（PGM）角度分析SMoE的局限性，提出相似性感知（S）MoE和注意力感知（S）MoE，通过减少专家选择熵提高路由稳定性，实验验证其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: SMoE在扩展深度学习模型参数规模的同时保持计算开销恒定，但存在路由波动问题，导致模型非鲁棒性。本文旨在通过PGM框架揭示问题并提出改进方案。

Method: 1. 从PGM角度分析SMoE的专家选择独立性问题；2. 提出相似性感知（S）MoE，考虑令牌间交互；3. 进一步提出注意力感知（S）MoE，利用注意力矩阵指导路由。

Result: 实验证明，相似性/注意力感知路由能显著减少专家选择熵，提高路由稳定性、准确性和模型鲁棒性，优于基于softmax门控的基线MoE-Transformer。

Conclusion: 通过PGM框架改进SMoE的路由机制，提出的相似性和注意力感知方法有效解决了路由波动问题，提升了模型性能。

Abstract: Sparse Mixture of Experts (SMoE) has emerged as a key to achieving
unprecedented scalability in deep learning. By activating only a small subset
of parameters per sample, SMoE achieves an exponential increase in parameter
counts while maintaining a constant computational overhead. However, SMoE
models are susceptible to routing fluctuations--changes in the routing of a
given input to its target expert--at the late stage of model training, leading
to model non-robustness. In this work, we unveil the limitation of SMoE through
the perspective of the probabilistic graphical model (PGM). Through this PGM
framework, we highlight the independence in the expert-selection of tokens,
which exposes the model to routing fluctuation and non-robustness. Alleviating
this independence, we propose the novel Similarity-Aware (S)MoE, which
considers interactions between tokens during expert selection. We then derive a
new PGM underlying an (S)MoE-Attention block, going beyond just a single (S)MoE
layer. Leveraging the token similarities captured by the attention matrix, we
propose the innovative Attention-Aware (S)MoE, which employs the attention
matrix to guide the routing of tokens to appropriate experts in (S)MoE. We
theoretically prove that Similarity/Attention-Aware routing help reduce the
entropy of expert selection, resulting in more stable token routing mechanisms.
We empirically validate our models on various tasks and domains, showing
significant improvements in reducing routing fluctuations, enhancing accuracy,
and increasing model robustness over the baseline MoE-Transformer with token
routing via softmax gating.

</details>

### [22] [Scalable Meta-Learning via Mixed-Mode Differentiation](https://arxiv.org/abs/2505.00793)
*Iurii Kemaev,Dan A Calian,Luisa M Zintgraf,Gregory Farquhar,Hado van Hasselt*

Main category: cs.LG

TLDR: MixFlow-MG算法通过混合模式微分优化梯度计算，显著提升内存和计算效率。


<details>
  <summary>Details</summary>
Motivation: 梯度双层优化在超参数优化、任务适应等领域广泛应用，但现有自动微分库无法充分利用问题结构，导致性能不佳。

Method: 提出Mixed-Flow Meta-Gradients（MixFlow-MG），采用混合模式微分构建高效计算图。

Result: 在元学习场景中，内存节省10倍以上，计算时间减少25%。

Conclusion: MixFlow-MG为梯度双层优化提供了更高效、可扩展的解决方案。

Abstract: Gradient-based bilevel optimisation is a powerful technique with applications
in hyperparameter optimisation, task adaptation, algorithm discovery,
meta-learning more broadly, and beyond. It often requires differentiating
through the gradient-based optimisation process itself, leading to
"gradient-of-a-gradient" calculations with computationally expensive
second-order and mixed derivatives. While modern automatic differentiation
libraries provide a convenient way to write programs for calculating these
derivatives, they oftentimes cannot fully exploit the specific structure of
these problems out-of-the-box, leading to suboptimal performance. In this
paper, we analyse such cases and propose Mixed-Flow Meta-Gradients, or
MixFlow-MG -- a practical algorithm that uses mixed-mode differentiation to
construct more efficient and scalable computational graphs yielding over 10x
memory and up to 25% wall-clock time improvements over standard implementations
in modern meta-learning setups.

</details>

### [23] [A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i](https://arxiv.org/abs/2505.00808)
*Kola Ayonrinde,Louis Jaburi*

Main category: cs.LG

TLDR: 论文提出解释性视图假说，认为机制可解释性研究能提取和理解神经网络中的隐含解释，并定义了机制可解释性（MI）及其评估标准。


<details>
  <summary>Details</summary>
Motivation: 探讨机制可解释性研究的理论基础，明确其定义和评估标准，以区分其他可解释性范式。

Method: 提出解释性忠实性评估标准，定义MI为模型级、本体论、因果机制和可证伪的解释。

Result: 明确了MI的界限，并提出了解释性乐观原则作为MI成功的必要条件。

Conclusion: 机制可解释性研究具有理论基础和实践意义，但其成功依赖于解释性乐观原则。

Abstract: Mechanistic Interpretability aims to understand neural networks through
causal explanations. We argue for the Explanatory View Hypothesis: that
Mechanistic Interpretability research is a principled approach to understanding
models because neural networks contain implicit explanations which can be
extracted and understood. We hence show that Explanatory Faithfulness, an
assessment of how well an explanation fits a model, is well-defined. We propose
a definition of Mechanistic Interpretability (MI) as the practice of producing
Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations of neural
networks, allowing us to distinguish MI from other interpretability paradigms
and detail MI's inherent limits. We formulate the Principle of Explanatory
Optimism, a conjecture which we argue is a necessary precondition for the
success of Mechanistic Interpretability.

</details>

### [24] [Scalable Unit Harmonization in Medical Informatics Using Bi-directional Transformers and Bayesian-Optimized BM25 and Sentence Embedding Retrieval](https://arxiv.org/abs/2505.00810)
*Jordi de la Torre*

Main category: cs.LG

TLDR: 开发了一种结合BM25、句子嵌入和双向变压器的单位统一方法，显著提升了临床数据集的单位一致性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模临床数据集中单位不一致的问题，提升数据互操作性。

Method: 设计了一个多阶段流程，结合BM25、句子嵌入、贝叶斯优化和双向变压器分类器，用于实验室测试条目的检索和匹配。

Result: 混合检索方法（MRR: 0.8833）优于纯词法和纯嵌入方法，最终系统MRR达0.9833，精度和召回率表现优异。

Conclusion: 该框架为临床数据集提供了高效、可扩展的单位统一解决方案，减少了人工干预，提升了数据重用和分析的可靠性。

Abstract: Objective: To develop and evaluate a scalable methodology for harmonizing
inconsistent units in large-scale clinical datasets, addressing a key barrier
to data interoperability.
  Materials and Methods: We designed a novel unit harmonization system
combining BM25, sentence embeddings, Bayesian optimization, and a bidirectional
transformer based binary classifier for retrieving and matching laboratory test
entries. The system was evaluated using the Optum Clinformatics Datamart
dataset (7.5 billion entries). We implemented a multi-stage pipeline:
filtering, identification, harmonization proposal generation, automated
re-ranking, and manual validation. Performance was assessed using Mean
Reciprocal Rank (MRR) and other standard information retrieval metrics.
  Results: Our hybrid retrieval approach combining BM25 and sentence embeddings
(MRR: 0.8833) significantly outperformed both lexical-only (MRR: 0.7985) and
embedding-only (MRR: 0.5277) approaches. The transformer-based reranker further
improved performance (absolute MRR improvement: 0.10), bringing the final
system MRR to 0.9833. The system achieved 83.39\% precision at rank 1 and
94.66\% recall at rank 5.
  Discussion: The hybrid architecture effectively leverages the complementary
strengths of lexical and semantic approaches. The reranker addresses cases
where initial retrieval components make errors due to complex semantic
relationships in medical terminology.
  Conclusion: Our framework provides an efficient, scalable solution for unit
harmonization in clinical datasets, reducing manual effort while improving
accuracy. Once harmonized, data can be reused seamlessly in different analyses,
ensuring consistency across healthcare systems and enabling more reliable
multi-institutional studies and meta-analyses.

</details>

### [25] [Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization](https://arxiv.org/abs/2505.00812)
*Kuan Zhang,Chengliang Chai,Jingzhe Xu,Chi Zhang,Ye Yuan,Guoren Wang,Lei Cao*

Main category: cs.LG

TLDR: 提出了一种新颖的两阶段噪声学习框架，通过动态加权损失函数实现实例级优化，无需超参数调优，显著提升性能并减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有方法在噪声监督下泛化性能下降，且存在计算成本高、超参数调优复杂等问题。

Method: 引入动态建模样本清洁度和难度的错误事件指标，分两阶段训练：先构建基础模型，再基于概率模型进行噪声鲁棒训练。

Result: 在五个合成和真实LNL基准测试中，性能超越现有方法，计算时间减少75%，模型可扩展性提升。

Conclusion: 该框架有效解决了噪声学习中的挑战，具有高效性和实用性。

Abstract: Recent studies indicate that deep neural networks degrade in generalization
performance under noisy supervision. Existing methods focus on isolating clean
subsets or correcting noisy labels, facing limitations such as high
computational costs, heavy hyperparameter tuning process, and coarse-grained
optimization. To address these challenges, we propose a novel two-stage noisy
learning framework that enables instance-level optimization through a
dynamically weighted loss function, avoiding hyperparameter tuning. To obtain
stable and accurate information about noise modeling, we introduce a simple yet
effective metric, termed wrong event, which dynamically models the cleanliness
and difficulty of individual samples while maintaining computational costs. Our
framework first collects wrong event information and builds a strong base
model. Then we perform noise-robust training on the base model, using a
probabilistic model to handle the wrong event information of samples.
Experiments on five synthetic and real-world LNL benchmarks demonstrate our
method surpasses state-of-the-art methods in performance, achieves a nearly 75%
reduction in computational time and improves model scalability.

</details>

### [26] [Dual Filter: A Mathematical Framework for Inference using Transformer-like Architectures](https://arxiv.org/abs/2505.00818)
*Heng-Sheng Chang,Prashant G. Mehta*

Main category: cs.LG

TLDR: 本文提出了一种基于隐马尔可夫模型（HMM）的因果非线性预测数学框架，灵感来自解码器-仅变压器架构，并通过最优控制方法解决预测问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是从第一性原理推导出类似变压器的架构，解决变压器设计的预测问题，而非直接建模变压器。

Method: 采用最优控制方法，将预测目标（最小均方误差）重新表述为最优控制问题，并引入双滤波器迭代算法。

Result: 提出了双滤波器算法，其架构与解码器-仅变压器高度相似，并通过数值实验验证了算法的性能。

Conclusion: 该框架为变压器架构提供了数学基础，并通过最优控制方法展示了其与变压器设计的紧密联系。

Abstract: This paper presents a mathematical framework for causal nonlinear prediction
in settings where observations are generated from an underlying hidden Markov
model (HMM). Both the problem formulation and the proposed solution are
motivated by the decoder-only transformer architecture, in which a finite
sequence of observations (tokens) is mapped to the conditional probability of
the next token. Our objective is not to construct a mathematical model of a
transformer. Rather, our interest lies in deriving, from first principles,
transformer-like architectures that solve the prediction problem for which the
transformer is designed. The proposed framework is based on an original optimal
control approach, where the prediction objective (MMSE) is reformulated as an
optimal control problem. An analysis of the optimal control problem is
presented leading to a fixed-point equation on the space of probability
measures. To solve the fixed-point equation, we introduce the dual filter, an
iterative algorithm that closely parallels the architecture of decoder-only
transformers. These parallels are discussed in detail along with the
relationship to prior work on mathematical modeling of transformers as
transport on the space of probability measures. Numerical experiments are
provided to illustrate the performance of the algorithm using parameter values
used in researchscale transformer models.

</details>

### [27] [Data-Driven Optical To Thermal Inference in Pool Boiling Using Generative Adversarial Networks](https://arxiv.org/abs/2505.00823)
*Qianxi Fu,Youngjoon Suh,Xiaojing Zhang,Yoonjin Won*

Main category: cs.LG

TLDR: 提出了一种基于条件生成对抗网络（CGAN）的数据驱动框架，用于从几何相轮廓推断温度场，误差低于6%。


<details>
  <summary>Details</summary>
Motivation: 多相热传递的定量表征受限于混沌快速流动中温度场的测量挑战。

Method: 利用CGAN从高速成像数据和仿真训练中重建温度场，并采用数据增强策略提高准确性。

Result: 模型在池沸腾配置中成功重建温度场，误差低于6%，数据增强提升了预测的物理合理性。

Conclusion: 深度生成模型可填补多相现象与热传递之间的测量空白，为复杂两相系统提供实验测量补充。

Abstract: Phase change plays a critical role in thermal management systems, yet
quantitative characterization of multiphase heat transfer remains limited by
the challenges of measuring temperature fields in chaotic, rapidly evolving
flow regimes. While computational methods offer spatiotemporal resolution in
idealized cases, replicating complex experimental conditions remains
prohibitively difficult. Here, we present a data-driven framework that
leverages a conditional generative adversarial network (CGAN) to infer
temperature fields from geometric phase contours in a canonical pool boiling
configuration where advanced data collection techniques are restricted. Using
high-speed imaging data and simulation-informed training, our model
demonstrates the ability to reconstruct temperature fields with errors below
6%. We further show that standard data augmentation strategies are effective in
enhancing both accuracy and physical plausibility of the predicted maps across
both simulation and experimental datasets when precise physical constraints are
not applicable. Our results highlight the potential of deep generative models
to bridge the gap between observable multiphase phenomena and underlying
thermal transport, offering a powerful approach to augment and interpret
experimental measurements in complex two-phase systems.

</details>

### [28] [Intersectional Divergence: Measuring Fairness in Regression](https://arxiv.org/abs/2505.00830)
*Joe Germino,Nuno Moniz,Nitesh V. Chawla*

Main category: cs.LG

TLDR: 本文提出了一种衡量回归任务中交叉公平性的新方法，通过考虑所有受保护属性的组合，并引入Intersectional Divergence（ID）作为公平性度量，进一步将其转化为损失函数（IDLoss）用于优化问题。实验表明，IDLoss能显著提升公平性且保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注分类任务中的公平性，而回归任务中存在重要空白。此外，现有方法通常仅关注单一受保护属性，忽略了属性组合的影响以及领域偏好的不平衡。

Method: 提出Intersectional Divergence（ID）作为回归任务中的公平性度量，考虑多属性组合及用户关注的目标范围。进一步将ID转化为损失函数IDLoss，用于优化问题。

Result: 实验证明，ID能提供对模型行为和公平性的独特见解，IDLoss能显著提升单属性和交叉公平性，同时保持预测性能。

Conclusion: 本文填补了回归任务中公平性研究的空白，提出的ID和IDLoss为多属性组合公平性提供了有效解决方案，并在实验中验证了其优越性。

Abstract: Research on fairness in machine learning has been mainly framed in the
context of classification tasks, leaving critical gaps in regression. In this
paper, we propose a seminal approach to measure intersectional fairness in
regression tasks, going beyond the focus on single protected attributes from
existing work to consider combinations of all protected attributes.
Furthermore, we contend that it is insufficient to measure the average error of
groups without regard for imbalanced domain preferences. To this end, we
propose Intersectional Divergence (ID) as the first fairness measure for
regression tasks that 1) describes fair model behavior across multiple
protected attributes and 2) differentiates the impact of predictions in target
ranges most relevant to users. We extend our proposal demonstrating how ID can
be adapted into a loss function, IDLoss, and used in optimization problems.
Through an extensive experimental evaluation, we demonstrate how ID allows
unique insights into model behavior and fairness, and how incorporating IDLoss
into optimization can considerably improve single-attribute and intersectional
model fairness while maintaining a competitive balance in predictive
performance.

</details>

### [29] [IberFire -- a detailed creation of a spatio-temporal dataset for wildfire risk assessment in Spain](https://arxiv.org/abs/2505.00837)
*Julen Ercibengoa,Meritxell Gómez-Omella,Izaro Goienetxea*

Main category: cs.LG

TLDR: IberFire是一个高分辨率的时空数据集，覆盖西班牙大陆和巴利阿里群岛，用于野火预测和气候分析。


<details>
  <summary>Details</summary>
Motivation: 解决西班牙缺乏本地化和细粒度野火数据的问题，支持野火风险建模和战略规划。

Method: 整合260个特征，分为8类，使用开源工具处理数据并公开代码。

Result: 提供了比现有欧洲数据集更高的时空粒度和特征多样性，支持机器学习和深度学习应用。

Conclusion: IberFire数据集公开可用，促进开放研究和合作，为野火预防和土地管理提供支持。

Abstract: Wildfires pose a critical environmental issue to ecosystems, economies, and
public safety, particularly in Mediterranean regions such as Spain. Accurate
predictive models rely on high-resolution spatio-temporal data to capture the
complex interplay of environmental and anthropogenic factors. To address the
lack of localised and fine-grained datasets in Spain, this work introduces
IberFire, a spatio-temporal datacube at 1 km x 1 km x 1-day resolution covering
mainland Spain and the Balearic Islands from December 2007 to December 2024.
IberFire integrates 260 features across eight main categories: auxiliary
features, fire history, geography, topography, meteorology, vegetation indices,
human activity, and land cover. All features are derived from open-access
sources, ensuring transparency and real-time applicability. The data processing
pipeline was implemented entirely using open-source tools, and the codebase has
been made publicly available. This work not only enhances spatio-temporal
granularity and feature diversity compared to existing European datacubes but
also provides a reproducible methodology for constructing similar datasets.
IberFire supports advanced wildfire risk modelling through Machine Learning
(ML) and Deep Learning (DL) techniques, enables climate pattern analysis and
informs strategic planning in fire prevention and land management. The dataset
is publicly available on Zenodo to promote open research and collaboration.

</details>

### [30] [ICQuant: Index Coding enables Low-bit LLM Quantization](https://arxiv.org/abs/2505.00850)
*Xinlin Li,Osama Hanna,Christina Fragouli,Suhas Diggavi*

Main category: cs.LG

TLDR: ICQuant是一种新颖的低比特后训练量化框架，通过统计异常值设计高效的索引编码方案，显著减少量化范围，降低比特开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的高内存成本促使需要高效的低比特后训练量化（PTQ），而异常值的存在是量化中的主要挑战。

Method: ICQuant利用异常值统计设计高效的索引编码方案，支持异常值感知的仅权重量化，显著降低比特开销。

Result: ICQuant仅需约0.3比特即可将量化范围减半，显著优于现有技术。在2.3比特/权重下，ICQuant将2位Llama3-70B模型的零样本准确率提升130%-150%。

Conclusion: ICQuant在不微调的情况下，性能媲美最佳微调量化器（PV-tuning），是一种高效的量化解决方案。

Abstract: The rapid deployment of Large Language Models (LLMs) highlights the need for
efficient low-bit post-training quantization (PTQ), due to their high memory
costs. A key challenge in weight quantization is the presence of outliers,
which inflate quantization ranges and lead to large errors. While a number of
outlier suppression techniques have been proposed, they either: fail to
effectively shrink the quantization range, or incur (relatively) high bit
overhead. In this paper, we present ICQuant, a novel framework that leverages
outlier statistics to design an efficient index coding scheme for outlier-aware
weight-only quantization. Compared to existing outlier suppression techniques
requiring $\approx 1$ bit overhead to halve the quantization range, ICQuant
requires only $\approx 0.3$ bits; a significant saving in extreme compression
regimes (e.g., 2-3 bits per weight). ICQuant can be used on top of any existing
quantizers to eliminate outliers, improving the quantization quality. Using
just 2.3 bits per weight and simple scalar quantizers, ICQuant improves the
zero-shot accuracy of the 2-bit Llama3-70B model by up to 130% and 150%
relative to QTIP and QuIP#; and it achieves comparable performance to the
best-known fine-tuned quantizer (PV-tuning) without fine-tuning.

</details>

### [31] [Rethinking Time Encoding via Learnable Transformation Functions](https://arxiv.org/abs/2505.00887)
*Xi Chen,Yateng Tang,Jiarong Xu,Jiawei Zhang,Siwei Zhang,Sijia Peng,Xuehao Zheng,Yun Xiong*

Main category: cs.LG

TLDR: 提出了一种名为LeTE的可学习时间编码方法，通过深度函数学习技术参数化非线性变换，以处理多样复杂的时间模式。


<details>
  <summary>Details</summary>
Motivation: 现实场景中时间模式多样且复杂，现有方法因专注于单一模式而效果有限。

Method: 使用深度函数学习技术参数化非线性变换，提出LeTE方法。

Result: LeTE在多种任务中表现出色，能够处理多样复杂的时间模式。

Conclusion: LeTE是一种通用且有效的时间编码方法，优于现有方法。

Abstract: Effectively modeling time information and incorporating it into applications
or models involving chronologically occurring events is crucial. Real-world
scenarios often involve diverse and complex time patterns, which pose
significant challenges for time encoding methods. While previous methods focus
on capturing time patterns, many rely on specific inductive biases, such as
using trigonometric functions to model periodicity. This narrow focus on
single-pattern modeling makes them less effective in handling the diversity and
complexities of real-world time patterns. In this paper, we investigate to
improve the existing commonly used time encoding methods and introduce
Learnable Transformation-based Generalized Time Encoding (LeTE). We propose
using deep function learning techniques to parameterize non-linear
transformations in time encoding, making them learnable and capable of modeling
generalized time patterns, including diverse and complex temporal dynamics. By
enabling learnable transformations, LeTE encompasses previous methods as
specific cases and allows seamless integration into a wide range of tasks.
Through extensive experiments across diverse domains, we demonstrate the
versatility and effectiveness of LeTE.

</details>

### [32] [NeMo-Inspector: A Visualization Tool for LLM Generation Analysis](https://arxiv.org/abs/2505.00903)
*Daria Gitman,Igor Gitman,Evelina Bakhturina*

Main category: cs.LG

TLDR: NeMo-Inspector是一个开源工具，用于简化合成数据集的分析和清理，显著提升数据质量和模型性能。


<details>
  <summary>Details</summary>
Motivation: 合成数据在缺乏真实数据时是重要替代，但质量保证困难，需高效工具支持。

Method: 开发NeMo-Inspector工具，集成推理能力，用于分析合成数据集。

Result: 使用该工具后，GSM-Plus数据集的低质量样本从46.99%降至19.51%；OpenMath模型在MATH和GSM8K数据集上的准确率分别提升1.92%和4.17%。

Conclusion: NeMo-Inspector能有效提升合成数据集质量，改善模型性能。

Abstract: Adapting Large Language Models (LLMs) to novel tasks and enhancing their
overall capabilities often requires large, high-quality training datasets.
Synthetic data, generated at scale, serves a valuable alternative when
real-world data is scarce or difficult to obtain. However, ensuring the quality
of synthetic datasets is challenging, as developers must manually inspect and
refine numerous samples to identify errors and areas for improvement. This
process is time-consuming and requires specialized tools. We introduce
NeMo-Inspector, an open-source tool designed to simplify the analysis of
synthetic datasets with integrated inference capabilities. We demonstrate its
effectiveness through two real-world cases. Analysis and cleaning of the
synthetically generated GSM-Plus dataset with NeMo-Inspector led to a
significant decrease in low-quality samples from 46.99% to 19.51%. The tool
also helped identify and correct generation errors in OpenMath models,
improving accuracy by 1.92% on the MATH dataset and by 4.17% on the GSM8K
dataset for a Meta-Llama-3-8B model fine-tuned on synthetic data generated from
Nemotron-4-340B.

</details>

### [33] [Learning Neural Control Barrier Functions from Offline Data with Conservatism](https://arxiv.org/abs/2505.00908)
*Ihab Tabbara,Hussein Sibai*

Main category: cs.LG

TLDR: 提出了一种基于离线数据集训练控制屏障函数的算法，称为保守控制屏障函数（CCBFs），用于提高系统安全性和避免分布外状态。


<details>
  <summary>Details</summary>
Motivation: 现有基于控制屏障函数的安全过滤器在维度灾难问题上表现不佳，深度学习方法的引入为解决这一问题提供了新思路。

Method: 受保守Q学习的启发，提出了一种离线训练算法，确保过滤器不仅能防止系统进入不安全状态，还能避免分布外状态。

Result: 实验结果表明，CCBFs在保持安全性和避免分布外状态方面优于现有方法，同时对任务性能影响最小。

Conclusion: CCBFs是一种有效的安全控制工具，尤其适用于高维系统和离线数据集场景。

Abstract: Safety filters, particularly those based on control barrier functions, have
gained increased interest as effective tools for safe control of dynamical
systems. Existing correct-by-construction synthesis algorithms, however, suffer
from the curse of dimensionality. Deep learning approaches have been proposed
in recent years to address this challenge. In this paper, we contribute to this
line of work by proposing an algorithm for training control barrier functions
from offline datasets. Our algorithm trains the filter to not only prevent the
system from reaching unsafe states but also out-of-distribution ones, at which
the filter would be unreliable. It is inspired by Conservative Q-learning, an
offline reinforcement learning algorithm. We call its outputs Conservative
Control Barrier Functions (CCBFs). Our empirical results demonstrate that CCBFs
outperform existing methods in maintaining safety and out-of-distribution
avoidance while minimally affecting task performance.

</details>

### [34] [Gaussian Process Policy Iteration with Additive Schwarz Acceleration for Forward and Inverse HJB and Mean Field Game Problems](https://arxiv.org/abs/2505.00909)
*Xianjin Yang,Jingguo Zhang*

Main category: cs.LG

TLDR: 提出了一种基于高斯过程的策略迭代框架，用于解决HJB方程和MFG的正反问题，通过Schwarz加速提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决HJB方程和MFG的正反问题，并提高计算效率。

Method: 采用高斯过程进行函数逼近，策略迭代分为固定策略下的值函数求解和策略更新，引入Schwarz加速作为预条件步骤。

Result: 数值实验表明Schwarz加速显著提高了计算效率。

Conclusion: 该方法有效解决了HJB方程和MFG的正反问题，并通过Schwarz加速优化了计算性能。

Abstract: We propose a Gaussian Process (GP)-based policy iteration framework for
addressing both forward and inverse problems in Hamilton--Jacobi--Bellman (HJB)
equations and mean field games (MFGs). Policy iteration is formulated as an
alternating procedure between solving the value function under a fixed control
policy and updating the policy based on the resulting value function. By
exploiting the linear structure of GPs for function approximation, each policy
evaluation step admits an explicit closed-form solution, eliminating the need
for numerical optimization. To improve convergence, we incorporate the additive
Schwarz acceleration as a preconditioning step following each policy update.
Numerical experiments demonstrate the effectiveness of Schwarz acceleration in
improving computational efficiency.

</details>

### [35] [Fine-Tuning without Performance Degradation](https://arxiv.org/abs/2505.00913)
*Han Wang,Adam White,Martha White*

Main category: cs.LG

TLDR: 论文提出了一种新的微调算法Jump Start，旨在减少离线学习策略在线微调时的性能下降，并通过逐步增加探索实现快速微调。


<details>
  <summary>Details</summary>
Motivation: 离线学习策略在线微调时通常会出现性能下降或学习缓慢的问题，现有方法难以解决。

Method: 基于Jump Start算法，逐步根据在线性能估计增加探索。

Result: 新算法显著减少了性能下降，并实现了快速微调。

Conclusion: Jump Start算法在多种设置下表现优于现有方法，解决了微调初期的性能退化问题。

Abstract: Fine-tuning policies learned offline remains a major challenge in application
domains. Monotonic performance improvement during \emph{fine-tuning} is often
challenging, as agents typically experience performance degradation at the
early fine-tuning stage. The community has identified multiple difficulties in
fine-tuning a learned network online, however, the majority of progress has
focused on improving learning efficiency during fine-tuning. In practice, this
comes at a serious cost during fine-tuning: initially, agent performance
degrades as the agent explores and effectively overrides the policy learned
offline. We show across a range of settings, many offline-to-online algorithms
exhibit either (1) performance degradation or (2) slow learning (sometimes
effectively no improvement) during fine-tuning. We introduce a new fine-tuning
algorithm, based on an algorithm called Jump Start, that gradually allows more
exploration based on online estimates of performance. Empirically, this
approach achieves fast fine-tuning and significantly reduces performance
degradations compared with existing algorithms designed to do the same.

</details>

### [36] [How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias](https://arxiv.org/abs/2505.00926)
*Ruiquan Huang,Yingbin Liang,Jing Yang*

Main category: cs.LG

TLDR: 论文研究了单层Transformer在解决正则语言识别任务（如'even pairs'和'parity check'）时的训练动态，发现其训练分为两个阶段，并通过实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 探索单层Transformer如何通过梯度下降学习解决正则语言识别任务，以解释其工作机制。

Method: 理论分析单层Transformer（注意力层加线性层）在梯度下降下的训练动态，并实验验证。

Result: 训练分为两个阶段：注意力层快速映射数据为可分向量，线性层逐渐接近最大间隔超平面，损失以$O(1/t)$下降。

Conclusion: 单层Transformer能直接解决'even pairs'，而'parity check'需结合Chain-of-Thought；实验验证了理论分析的正确性。

Abstract: Language recognition tasks are fundamental in natural language processing
(NLP) and have been widely used to benchmark the performance of large language
models (LLMs). These tasks also play a crucial role in explaining the working
mechanisms of transformers. In this work, we focus on two representative tasks
in the category of regular language recognition, known as `even pairs' and
`parity check', the aim of which is to determine whether the occurrences of
certain subsequences in a given sequence are even. Our goal is to explore how a
one-layer transformer, consisting of an attention layer followed by a linear
layer, learns to solve these tasks by theoretically analyzing its training
dynamics under gradient descent. While even pairs can be solved directly by a
one-layer transformer, parity check need to be solved by integrating
Chain-of-Thought (CoT), either into the inference stage of a transformer
well-trained for the even pairs task, or into the training of a one-layer
transformer. For both problems, our analysis shows that the joint training of
attention and linear layers exhibits two distinct phases. In the first phase,
the attention layer grows rapidly, mapping data sequences into separable
vectors. In the second phase, the attention layer becomes stable, while the
linear layer grows logarithmically and approaches in direction to a max-margin
hyperplane that correctly separates the attention layer outputs into positive
and negative samples, and the loss decreases at a rate of $O(1/t)$. Our
experiments validate those theoretical results.

</details>

### [37] [Compact Recurrent Transformer with Persistent Memory](https://arxiv.org/abs/2505.00929)
*Edison Mucllari,Zachary Daniels,David Zhang,Qiang Ye*

Main category: cs.LG

TLDR: 提出了一种高效的Compact Recurrent Transformer（CRT），结合浅层Transformer和循环神经网络，解决了长序列处理中的计算效率问题。


<details>
  <summary>Details</summary>
Motivation: Transformer在处理长序列时因自注意力计算的二次复杂度面临效率挑战，现有方法通常引入额外计算开销，限制了在资源有限场景（如边缘计算）的应用。

Method: CRT结合浅层Transformer处理短片段和循环神经网络压缩全局信息，通过单一持久记忆向量管理段间长程依赖。

Result: 在语言数据集（WordPTB和WikiText-103）和丰田Smarthome视频数据集上，CRT性能媲美或优于完整Transformer，同时显著减少计算量。

Conclusion: CRT是一种高效的长序列处理方案，适用于资源受限场景，并在语言和视觉任务中表现出色。

Abstract: The Transformer architecture has shown significant success in many language
processing and visual tasks. However, the method faces challenges in
efficiently scaling to long sequences because the self-attention computation is
quadratic with respect to the input length. To overcome this limitation,
several approaches scale to longer sequences by breaking long sequences into a
series of segments, restricting self-attention to local dependencies between
tokens within each segment and using a memory mechanism to manage information
flow between segments. However, these approached generally introduce additional
compute overhead that restricts them from being used for applications where
limited compute memory and power are of great concern (such as edge computing).
We propose a novel and efficient Compact Recurrent Transformer (CRT), which
combines shallow Transformer models that process short local segments with
recurrent neural networks to compress and manage a single persistent memory
vector that summarizes long-range global information between segments. We
evaluate CRT on WordPTB and WikiText-103 for next-token-prediction tasks, as
well as on the Toyota Smarthome video dataset for classification. CRT achieves
comparable or superior prediction results to full-length Transformers in the
language datasets while using significantly shorter segments (half or quarter
size) and substantially reduced FLOPs. Our approach also demonstrates
state-of-the-art performance on the Toyota Smarthome video dataset.

</details>

### [38] [Robust Root Cause Diagnosis using In-Distribution Interventions](https://arxiv.org/abs/2505.00930)
*Lokesh Nagalapatti,Ashutosh Srivastava,Sunita Sarawagi,Amit Sharma*

Main category: cs.LG

TLDR: 论文提出了一种名为IDI的新算法，通过干预性估计而非反事实估计，更准确地诊断复杂系统中的异常根因。


<details>
  <summary>Details</summary>
Motivation: 在复杂系统中诊断异常根因是一个紧迫问题，现有方法依赖反事实估计，但异常数据稀少导致估计不可靠。

Method: IDI算法通过满足两个条件（异常性和修复性）预测根因节点，并利用干预性估计替代反事实估计。

Result: 实验表明，IDI在合成和真实数据集上比现有方法更准确、更鲁棒地识别根因。

Conclusion: IDI通过干预性估计有效解决了反事实估计在异常数据上的不可靠问题，显著提升了根因诊断性能。

Abstract: Diagnosing the root cause of an anomaly in a complex interconnected system is
a pressing problem in today's cloud services and industrial operations. We
propose In-Distribution Interventions (IDI), a novel algorithm that predicts
root cause as nodes that meet two criteria: 1) **Anomaly:** root cause nodes
should take on anomalous values; 2) **Fix:** had the root cause nodes assumed
usual values, the target node would not have been anomalous. Prior methods of
assessing the fix condition rely on counterfactuals inferred from a Structural
Causal Model (SCM) trained on historical data. But since anomalies are rare and
fall outside the training distribution, the fitted SCMs yield unreliable
counterfactual estimates. IDI overcomes this by relying on interventional
estimates obtained by solely probing the fitted SCM at in-distribution inputs.
We present a theoretical analysis comparing and bounding the errors in
assessing the fix condition using interventional and counterfactual estimates.
We then conduct experiments by systematically varying the SCM's complexity to
demonstrate the cases where IDI's interventional approach outperforms the
counterfactual approach and vice versa. Experiments on both synthetic and
PetShop RCD benchmark datasets demonstrate that \our\ consistently identifies
true root causes more accurately and robustly than nine existing
state-of-the-art RCD baselines. Code is released at
https://github.com/nlokeshiisc/IDI_release.

</details>

### [39] [A Self-Supervised Transformer for Unusable Shared Bike Detection](https://arxiv.org/abs/2505.00932)
*Yin Huang,Yongqi Dong,Youhua Tang,Alvaro García Hernandez*

Main category: cs.LG

TLDR: 论文提出了一种自监督Transformer框架（SSTransformer），用于检测共享单车系统中的故障车辆，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 共享单车系统（BSS）的快速扩张带来了运营挑战，尤其是故障车辆的检测问题。现有方法因忽视动态时空模式或标签稀缺而效果有限。

Method: 采用自监督预训练策略增强特征提取能力，随后微调进行状态识别。Transformer编码器通过自监督目标学习单车运动的通用表示。

Result: 在成都的真实数据集上，SSTransformer在准确率（97.81%）、精确率（0.8889）和F1分数（0.9358）上均优于基线方法。

Conclusion: 自监督Transformer能有效捕捉BSS中的复杂异常，为共享出行提供更可靠的维护解决方案。

Abstract: The rapid expansion of bike-sharing systems (BSS) has greatly improved urban
"last-mile" connectivity, yet large-scale deployments face escalating
operational challenges, particularly in detecting faulty bikes. Existing
detection approaches either rely on static model-based thresholds that overlook
dynamic spatiotemporal (ST) usage patterns or employ supervised learning
methods that struggle with label scarcity and class imbalance. To address these
limitations, this paper proposes a novel Self-Supervised Transformer
(SSTransformer) framework for automatically detecting unusable shared bikes,
leveraging ST features extracted from GPS trajectories and trip records. The
model incorporates a self-supervised pre-training strategy to enhance its
feature extraction capabilities, followed by fine-tuning for efficient status
recognition. In the pre-training phase, the Transformer encoder learns
generalized representations of bike movement via a self-supervised objective;
in the fine-tuning phase, the encoder is adapted to a downstream binary
classification task. Comprehensive experiments on a real-world dataset of
10,730 bikes (1,870 unusable, 8,860 normal) from Chengdu, China, demonstrate
that SSTransformer significantly outperforms traditional machine learning,
ensemble learning, and deep learning baselines, achieving the best accuracy
(97.81%), precision (0.8889), and F1-score (0.9358). This work highlights the
effectiveness of self-supervised Transformer on ST data for capturing complex
anomalies in BSS, paving the way toward more reliable and scalable maintenance
solutions for shared mobility.

</details>

### [40] [Addressing Noise and Stochasticity in Fraud Detection for Service Networks](https://arxiv.org/abs/2505.00946)
*Wenxin Zhang,Ding Xu,Xi Xuan,Lei Jiang,Guangzhen Yao,Renda Han,Xiangxiang Lang,Cuicui Luo*

Main category: cs.LG

TLDR: 提出了一种基于信息瓶颈理论的新型谱图网络（SGNN-IB），用于服务网络中的欺诈检测，解决了现有图滤波方法在信号提取和融合中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有图滤波方法在提取干净且具有区分性的图信号方面存在不足，忽略了信息传播中的噪声，且无法区分信号的频率特性。

Method: SGNN-IB将原始图分为同质和异质子图，利用信息瓶颈理论提取关键特征，并通过原型学习实现信号融合。

Result: 在三个真实数据集上的实验表明，SGNN-IB优于现有欺诈检测方法。

Conclusion: SGNN-IB通过改进信号提取和融合方法，显著提升了欺诈检测性能。

Abstract: Fraud detection is crucial in social service networks to maintain user trust
and improve service network security. Existing spectral graph-based methods
address this challenge by leveraging different graph filters to capture signals
with different frequencies in service networks. However, most graph
filter-based methods struggle with deriving clean and discriminative graph
signals. On the one hand, they overlook the noise in the information
propagation process, resulting in degradation of filtering ability. On the
other hand, they fail to discriminate the frequency-specific characteristics of
graph signals, leading to distortion of signals fusion. To address these
issues, we develop a novel spectral graph network based on information
bottleneck theory (SGNN-IB) for fraud detection in service networks. SGNN-IB
splits the original graph into homophilic and heterophilic subgraphs to better
capture the signals at different frequencies. For the first limitation, SGNN-IB
applies information bottleneck theory to extract key characteristics of encoded
representations. For the second limitation, SGNN-IB introduces prototype
learning to implement signal fusion, preserving the frequency-specific
characteristics of signals. Extensive experiments on three real-world datasets
demonstrate that SGNN-IB outperforms state-of-the-art fraud detection methods.

</details>

### [41] [TunnElQNN: A Hybrid Quantum-classical Neural Network for Efficient Learning](https://arxiv.org/abs/2505.00933)
*A. H. Abbas*

Main category: cs.LG

TLDR: 论文提出了一种名为TunnElQNN的混合量子-经典神经网络架构，使用量子隧穿启发的激活函数（TDAF），在合成数据集上表现优于传统ReLUQNN模型。


<details>
  <summary>Details</summary>
Motivation: 探索混合量子-经典神经网络的潜力，结合量子隧穿物理特性增强模型表达能力。

Method: 提出非顺序架构TunnElQNN，交替使用经典和量子层，经典层采用TDAF激活函数。

Result: TunnElQNN在多类分类任务中表现优于ReLUQNN，且在不同类别重叠情况下生成更优决策边界。

Conclusion: 物理启发的激活函数与量子组件结合可提升混合架构的表达能力和鲁棒性。

Abstract: Hybrid quantum-classical neural networks (HQCNNs) represent a promising
frontier in machine learning, leveraging the complementary strengths of both
models. In this work, we propose the development of TunnElQNN, a non-sequential
architecture composed of alternating classical and quantum layers. Within the
classical component, we employ the Tunnelling Diode Activation Function (TDAF),
inspired by the I-V characteristics of quantum tunnelling. We evaluate the
performance of this hybrid model on a synthetic dataset of interleaving
half-circle for multi-class classification tasks with varying degrees of class
overlap. The model is compared against a baseline hybrid architecture that uses
the conventional ReLU activation function (ReLUQNN). Our results show that the
TunnElQNN model consistently outperforms the ReLUQNN counterpart. Furthermore,
we analyse the decision boundaries generated by TunnElQNN under different
levels of class overlap and compare them to those produced by a neural network
implementing TDAF within a fully classical architecture. These findings
highlight the potential of integrating physics-inspired activation functions
with quantum components to enhance the expressiveness and robustness of hybrid
quantum-classical machine learning architectures.

</details>

### [42] [StablePCA: Learning Shared Representations across Multiple Sources via Minimax Optimization](https://arxiv.org/abs/2505.00940)
*Zhenyu Wang,Molei Liu,Jing Lei,Francis Bach,Zijian Guo*

Main category: cs.LG

TLDR: 提出了一种名为StablePCA的新方法，用于从多源高维数据中学习稳健的低维表示，解决了PCA在多源场景中的非凸优化问题。


<details>
  <summary>Details</summary>
Motivation: 提取多源高维数据的低维表示以近似原始特征，促进知识迁移、减少系统偏差（如批次效应）并提升公平性。

Method: 采用Fantope松弛将非凸优化问题转化为凸极小极大优化，并设计了一种乐观梯度Mirror Prox算法。

Result: 理论证明了算法的全局收敛性，并通过实验验证了StablePCA在提取稳健低维表示方面的高效性和准确性。

Conclusion: StablePCA在多源高维数据中表现出色，为稳健特征提取提供了有效工具。

Abstract: When synthesizing multisource high-dimensional data, a key objective is to
extract low-dimensional feature representations that effectively approximate
the original features across different sources. Such general feature extraction
facilitates the discovery of transferable knowledge, mitigates systematic
biases such as batch effects, and promotes fairness. In this paper, we propose
Stable Principal Component Analysis (StablePCA), a novel method for group
distributionally robust learning of latent representations from
high-dimensional multi-source data. A primary challenge in generalizing PCA to
the multi-source regime lies in the nonconvexity of the fixed rank constraint,
rendering the minimax optimization nonconvex. To address this challenge, we
employ the Fantope relaxation, reformulating the problem as a convex minimax
optimization, with the objective defined as the maximum loss across sources. To
solve the relaxed formulation, we devise an optimistic-gradient Mirror Prox
algorithm with explicit closed-form updates. Theoretically, we establish the
global convergence of the Mirror Prox algorithm, with the convergence rate
provided from the optimization perspective. Furthermore, we offer practical
criteria to assess how closely the solution approximates the original nonconvex
formulation. Through extensive numerical experiments, we demonstrate
StablePCA's high accuracy and efficiency in extracting robust low-dimensional
representations across various finite-sample scenarios.

</details>

### [43] [FreCT: Frequency-augmented Convolutional Transformer for Robust Time Series Anomaly Detection](https://arxiv.org/abs/2505.00941)
*Wenxin Zhang,Ding Xu,Guangzhen Yao,Xiaojian Lin,Renxiang Guan,Chengze Du,Renda Han,Xi Xuan,Cuicui Luo*

Main category: cs.LG

TLDR: 提出了一种基于频率增强的卷积Transformer（FreCT），用于时间序列异常检测，通过结合时间域和频率域信息，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于重构的方法因时间序列模式的复杂性和异常干扰，难以准确检测异常，且忽视了频率域信息。

Method: FreCT结合了Transformer和卷积模块，利用傅里叶变换增强频率分析，并通过KL散度和绝对误差优化训练。

Result: 在四个公开数据集上的实验表明，FreCT优于现有方法。

Conclusion: FreCT通过多域信息融合和优化策略，显著提升了时间序列异常检测的准确性和鲁棒性。

Abstract: Time series anomaly detection is critical for system monitoring and risk
identification, across various domains, such as finance and healthcare.
However, for most reconstruction-based approaches, detecting anomalies remains
a challenge due to the complexity of sequential patterns in time series data.
On the one hand, reconstruction-based techniques are susceptible to
computational deviation stemming from anomalies, which can lead to impure
representations of normal sequence patterns. On the other hand, they often
focus on the time-domain dependencies of time series, while ignoring the
alignment of frequency information beyond the time domain. To address these
challenges, we propose a novel Frequency-augmented Convolutional Transformer
(FreCT). FreCT utilizes patch operations to generate contrastive views and
employs an improved Transformer architecture integrated with a convolution
module to capture long-term dependencies while preserving local topology
information. The introduced frequency analysis based on Fourier transformation
could enhance the model's ability to capture crucial characteristics beyond the
time domain. To protect the training quality from anomalies and improve the
robustness, FreCT deploys stop-gradient Kullback-Leibler (KL) divergence and
absolute error to optimize consistency information in both time and frequency
domains. Extensive experiments on four public datasets demonstrate that FreCT
outperforms existing methods in identifying anomalies.

</details>

### [44] [Adaptive Branch-and-Bound Tree Exploration for Neural Network Verification](https://arxiv.org/abs/2505.00963)
*Kota Fukuda,Guanqin Zhang,Zhenya Zhang,Yulei Sui,Jianjun Zhao*

Main category: cs.LG

TLDR: ABONN是一种基于自适应蒙特卡洛树搜索（MCTS）的新型神经网络验证方法，通过优先探索更可能找到反例的子问题，显著提升了验证效率。


<details>
  <summary>Details</summary>
Motivation: 现有分支定界（BaB）方法在验证神经网络时效率不高，因其忽略了子问题的重要性差异。

Method: 提出“重要性”概念，并设计ABONN方法，以MCTS风格自适应探索子问题空间，优先处理高重要性子问题。

Result: 在MNIST和CIFAR-10数据集上，ABONN分别实现了15.2倍和24.7倍的加速。

Conclusion: ABONN通过自适应探索子问题空间，显著提升了神经网络验证的效率。

Abstract: Formal verification is a rigorous approach that can provably ensure the
quality of neural networks, and to date, Branch and Bound (BaB) is the
state-of-the-art that performs verification by splitting the problem as needed
and applying off-the-shelf verifiers to sub-problems for improved performance.
However, existing BaB may not be efficient, due to its naive way of exploring
the space of sub-problems that ignores the \emph{importance} of different
sub-problems. To bridge this gap, we first introduce a notion of ``importance''
that reflects how likely a counterexample can be found with a sub-problem, and
then we devise a novel verification approach, called ABONN, that explores the
sub-problem space of BaB adaptively, in a Monte-Carlo tree search (MCTS) style.
The exploration is guided by the ``importance'' of different sub-problems, so
it favors the sub-problems that are more likely to find counterexamples. As
soon as it finds a counterexample, it can immediately terminate; even though it
cannot find, after visiting all the sub-problems, it can still manage to verify
the problem. We evaluate ABONN with 552 verification problems from
commonly-used datasets and neural network models, and compare it with the
state-of-the-art verifiers as baseline approaches. Experimental evaluation
shows that ABONN demonstrates speedups of up to $15.2\times$ on MNIST and
$24.7\times$ on CIFAR-10. We further study the influences of hyperparameters to
the performance of ABONN, and the effectiveness of our adaptive tree
exploration.

</details>

### [45] [Tree-Sliced Wasserstein Distance with Nonlinear Projection](https://arxiv.org/abs/2505.00968)
*Thanh Tran,Viet-Hoang Tran,Thanh Chu,Trang Pham,Laurent El Ghaoui,Tam Le,Tan M. Nguyen*

Main category: cs.LG

TLDR: 提出了一种基于树切片Wasserstein距离的非线性投影框架，替代了线性投影，提升了度量效率，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统切片Wasserstein距离使用一维线作为投影，而树切片方法通过树结构捕捉拓扑结构，但仍有改进空间。本文旨在通过非线性投影进一步提升度量能力。

Method: 提出了非线性投影框架，替代线性投影，确保Radon变换的单射性，并设计了适用于欧几里得空间和球面的高效度量。

Result: 实验表明，新方法在欧几里得和球面数据集上优于现有SW和TSW变体，适用于梯度流、自监督学习和生成模型。

Conclusion: 非线性投影框架显著提升了树切片Wasserstein距离的性能，为多种应用提供了更高效的度量工具。

Abstract: Tree-Sliced methods have recently emerged as an alternative to the
traditional Sliced Wasserstein (SW) distance, replacing one-dimensional lines
with tree-based metric spaces and incorporating a splitting mechanism for
projecting measures. This approach enhances the ability to capture the
topological structures of integration domains in Sliced Optimal Transport while
maintaining low computational costs. Building on this foundation, we propose a
novel nonlinear projectional framework for the Tree-Sliced Wasserstein (TSW)
distance, substituting the linear projections in earlier versions with general
projections, while ensuring the injectivity of the associated Radon Transform
and preserving the well-definedness of the resulting metric. By designing
appropriate projections, we construct efficient metrics for measures on both
Euclidean spaces and spheres. Finally, we validate our proposed metric through
extensive numerical experiments for Euclidean and spherical datasets.
Applications include gradient flows, self-supervised learning, and generative
models, where our methods demonstrate significant improvements over recent SW
and TSW variants.

</details>

### [46] [A Minimax-MDP Framework with Future-imposed Conditions for Learning-augmented Problems](https://arxiv.org/abs/2505.00973)
*Xin Chen,Yuze Chen,Yuan Zhou*

Main category: cs.LG

TLDR: 该论文研究了在机器学习预测辅助下的序列决策问题，提出了一种最小最大马尔可夫决策过程框架，用于设计鲁棒且高效的决策策略。


<details>
  <summary>Details</summary>
Motivation: 解决在预测不确定性下的在线决策问题，确保决策在所有可能的参数和预测实现下具有竞争力。

Method: 提出最小最大马尔可夫决策过程（minimax-MDP）框架，结合对抗性环境状态和决策者内部状态，设计未来约束条件以实现高效策略。

Result: 通过三个应用案例验证了框架的有效性，包括库存管理和资源分配问题。

Conclusion: 该框架为预测不确定性下的鲁棒在线决策提供了实用且灵活的方法。

Abstract: We study a class of sequential decision-making problems with augmented
predictions, potentially provided by a machine learning algorithm. In this
setting, the decision-maker receives prediction intervals for unknown
parameters that become progressively refined over time, and seeks decisions
that are competitive with the hindsight optimal under all possible realizations
of both parameters and predictions. We propose a minimax Markov Decision
Process (minimax-MDP) framework, where the system state consists of an
adversarially evolving environment state and an internal state controlled by
the decision-maker. We introduce a set of future-imposed conditions that
characterize the feasibility of minimax-MDPs and enable the design of
efficient, often closed-form, robustly competitive policies. We illustrate the
framework through three applications: multi-period inventory ordering with
refining demand predictions, resource allocation with uncertain utility
functions, and a multi-phase extension of the minimax-MDP applied to the
inventory problem with time-varying ordering costs. Our results provide a
tractable and versatile approach to robust online decision-making under
predictive uncertainty.

</details>

### [47] [Towards the Resistance of Neural Network Watermarking to Fine-tuning](https://arxiv.org/abs/2505.01007)
*Ling Tang,Yuefeng Chen,Hui Xue,Quanshi Zhang*

Main category: cs.LG

TLDR: 提出一种新的DNN水印方法，通过特定频率分量嵌入所有权信息，对微调具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决DNN模型在微调过程中水印信息易丢失的问题。

Method: 利用修正的傅里叶变换提取卷积滤波器的频率分量，将水印信息编码到特定频率分量中。

Result: 实验证明该方法在微调、权重缩放和置换下仍能保持水印信息。

Conclusion: 该方法为DNN所有权保护提供了一种有效且鲁棒的解决方案。

Abstract: This paper proves a new watermarking method to embed the ownership
information into a deep neural network (DNN), which is robust to fine-tuning.
Specifically, we prove that when the input feature of a convolutional layer
only contains low-frequency components, specific frequency components of the
convolutional filter will not be changed by gradient descent during the
fine-tuning process, where we propose a revised Fourier transform to extract
frequency components from the convolutional filter. Additionally, we also prove
that these frequency components are equivariant to weight scaling and weight
permutations. In this way, we design a watermark module to encode the watermark
information to specific frequency components in a convolutional filter.
Preliminary experiments demonstrate the effectiveness of our method.

</details>

### [48] [Accelerating Deep Neural Network Training via Distributed Hybrid Order Optimization](https://arxiv.org/abs/2505.00982)
*Shunxian Gu,Chaoqun You,Bangbang Ren,Lailong Luo,Junxu Xia,Deke Guo*

Main category: cs.LG

TLDR: FOSI是一种混合顺序优化器，通过结合梯度和曲率信息加速DNN训练。本文提出其分布式设计DHO$_2$，降低内存负担并实现训练时间加速。


<details>
  <summary>Details</summary>
Motivation: 资源有限的用户难以扩展DNN训练设备数量，需要更高效的优化器。

Method: 设计分布式计算曲率信息和模型更新的策略，并行化计算与更新。

Result: 实验显示内存负担随设备数量线性减少，训练时间加速1.4×∼2.1×。

Conclusion: DHO$_2$为资源受限环境下的DNN训练提供了高效解决方案。

Abstract: Scaling deep neural network (DNN) training to more devices can reduce
time-to-solution. However, it is impractical for users with limited computing
resources. FOSI, as a hybrid order optimizer, converges faster than
conventional optimizers by taking advantage of both gradient information and
curvature information when updating the DNN model. Therefore, it provides a new
chance for accelerating DNN training in the resource-constrained setting. In
this paper, we explore its distributed design, namely DHO$_2$, including
distributed calculation of curvature information and model update with partial
curvature information to accelerate DNN training with a low memory burden. To
further reduce the training time, we design a novel strategy to parallelize the
calculation of curvature information and the model update on different devices.
Experimentally, our distributed design can achieve an approximate linear
reduction of memory burden on each device with the increase of the device
number. Meanwhile, it achieves $1.4\times\sim2.1\times$ speedup in the total
training time compared with other distributed designs based on conventional
first- and second-order optimizers.

</details>

### [49] [Toward Data-centric Directed Graph Learning: An Entropy-driven Approach](https://arxiv.org/abs/2505.00983)
*Xunkai Li,Zhengyu Wu,Kaichi Yu,Hongchao Qin,Guang Zeng,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TLDR: 论文提出了一种名为EDEN的数据中心有向图学习范式，通过层次编码理论和知识蒸馏提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有有向图神经网络未能充分利用有向边隐藏的数据知识，导致预测性能不佳，需从数据中心角度探索拓扑与节点特征的关系。

Method: EDEN利用有向结构测量构建层次知识树（HKT），并通过节点特征的互信息细化知识流，实现数据中心知识蒸馏。

Result: EDEN在14个（有向）图数据集和4个下游任务中表现优异，显著提升了现有（有向）图神经网络的性能。

Conclusion: EDEN作为一种通用框架，不仅在有向图中表现卓越，还可扩展到无向图场景，具有广泛的应用潜力。

Abstract: The directed graph (digraph), as a generalization of undirected graphs,
exhibits superior representation capability in modeling complex topology
systems and has garnered considerable attention in recent years. Despite the
notable efforts made by existing DiGraph Neural Networks (DiGNNs) to leverage
directed edges, they still fail to comprehensively delve into the abundant data
knowledge concealed in the digraphs. This data-level limitation results in
model-level sub-optimal predictive performance and underscores the necessity of
further exploring the potential correlations between the directed edges
(topology) and node profiles (feature and labels) from a data-centric
perspective, thereby empowering model-centric neural networks with stronger
encoding capabilities.
  In this paper, we propose \textbf{E}ntropy-driven \textbf{D}igraph
knowl\textbf{E}dge distillatio\textbf{N} (EDEN), which can serve as a
data-centric digraph learning paradigm or a model-agnostic hot-and-plug
data-centric Knowledge Distillation (KD) module. The core idea is to achieve
data-centric ML, guided by our proposed hierarchical encoding theory for
structured data. Specifically, EDEN first utilizes directed structural
measurements from a topology perspective to construct a coarse-grained
Hierarchical Knowledge Tree (HKT). Subsequently, EDEN quantifies the mutual
information of node profiles to refine knowledge flow in the HKT, enabling
data-centric KD supervision within model training. As a general framework, EDEN
can also naturally extend to undirected scenarios and demonstrate satisfactory
performance. In our experiments, EDEN has been widely evaluated on 14 (di)graph
datasets (homophily and heterophily) and across 4 downstream tasks. The results
demonstrate that EDEN attains SOTA performance and exhibits strong improvement
for prevalent (Di)GNNs.

</details>

### [50] [Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii](https://arxiv.org/abs/2505.01372)
*Kola Ayonrinde,Louis Jaburi*

Main category: cs.LG

TLDR: 论文提出了一种基于哲学视角的多元解释框架，用于评估和改进神经网络的可解释性方法，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络的可解释性方法缺乏统一的评估标准，限制了进展。

Method: 引入基于贝叶斯、库恩、德意志和规范四种哲学视角的多元解释框架，系统评估和改进解释。

Result: 发现紧凑证明方法具有潜力，并提出了三个未来研究方向。

Conclusion: 改进的可解释性方法有助于更好地监控、预测和引导AI系统。

Abstract: Mechanistic Interpretability (MI) aims to understand neural networks through
causal explanations. Though MI has many explanation-generating methods,
progress has been limited by the lack of a universal approach to evaluating
explanations. Here we analyse the fundamental question "What makes a good
explanation?" We introduce a pluralist Explanatory Virtues Framework drawing on
four perspectives from the Philosophy of Science - the Bayesian, Kuhnian,
Deutschian, and Nomological - to systematically evaluate and improve
explanations in MI. We find that Compact Proofs consider many explanatory
virtues and are hence a promising approach. Fruitful research directions
implied by our framework include (1) clearly defining explanatory simplicity,
(2) focusing on unifying explanations and (3) deriving universal principles for
neural networks. Improved MI methods enhance our ability to monitor, predict,
and steer AI systems.

</details>

### [51] [On-demand Test-time Adaptation for Edge Devices](https://arxiv.org/abs/2505.00986)
*Xiao Ma,Young D. Kwon,Dong Ma*

Main category: cs.LG

TLDR: 论文提出了一种按需测试时适应（OD-TTA）框架，通过仅在检测到显著域偏移时触发适应，显著降低了计算开销和能耗，适用于资源受限的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有持续测试时适应（CTTA）方法在边缘设备上因内存和能耗问题实用性差，需要一种更高效的解决方案。

Method: OD-TTA包括轻量级域偏移检测、源域选择模块和分离的批量归一化更新方案。

Result: 实验表明，OD-TTA在保持高性能的同时显著降低了能耗和计算开销。

Conclusion: OD-TTA为测试时适应在边缘设备上的实际应用提供了可行方案。

Abstract: Continual Test-time adaptation (CTTA) continuously adapts the deployed model
on every incoming batch of data. While achieving optimal accuracy, existing
CTTA approaches present poor real-world applicability on resource-constrained
edge devices, due to the substantial memory overhead and energy consumption. In
this work, we first introduce a novel paradigm -- on-demand TTA -- which
triggers adaptation only when a significant domain shift is detected. Then, we
present OD-TTA, an on-demand TTA framework for accurate and efficient
adaptation on edge devices. OD-TTA comprises three innovative techniques: 1) a
lightweight domain shift detection mechanism to activate TTA only when it is
needed, drastically reducing the overall computation overhead, 2) a source
domain selection module that chooses an appropriate source model for
adaptation, ensuring high and robust accuracy, 3) a decoupled Batch
Normalization (BN) update scheme to enable memory-efficient adaptation with
small batch sizes. Extensive experiments show that OD-TTA achieves comparable
and even better performance while reducing the energy and computation overhead
remarkably, making TTA a practical reality.

</details>

### [52] [Where's the liability in the Generative Era? Recovery-based Black-Box Detection of AI-Generated Content](https://arxiv.org/abs/2505.01008)
*Haoyue Bai,Yiyou Sun,Wei Cheng,Haifeng Chen*

Main category: cs.LG

TLDR: 提出了一种新的黑盒检测框架，通过掩盖和恢复策略检测生成图像，无需模型权重或大型辅助数据集。


<details>
  <summary>Details</summary>
Motivation: 生成模型生成的逼真图像可能被滥用，现有检测方法依赖模型权重或大量数据集，限制了实用性。

Method: 采用掩盖和恢复策略，通过API访问检测图像是否由模型生成，对不支持掩盖输入的模型使用替代模型。

Result: 在八个扩散模型变体数据集上，平均精度比基线方法高4.31%。

Conclusion: 该框架提供了一种高效、可扩展的生成图像检测方法。

Abstract: The recent proliferation of photorealistic images created by generative
models has sparked both excitement and concern, as these images are
increasingly indistinguishable from real ones to the human eye. While offering
new creative and commercial possibilities, the potential for misuse, such as in
misinformation and fraud, highlights the need for effective detection methods.
Current detection approaches often rely on access to model weights or require
extensive collections of real image datasets, limiting their scalability and
practical application in real world scenarios. In this work, we introduce a
novel black box detection framework that requires only API access, sidestepping
the need for model weights or large auxiliary datasets. Our approach leverages
a corrupt and recover strategy: by masking part of an image and assessing the
model ability to reconstruct it, we measure the likelihood that the image was
generated by the model itself. For black-box models that do not support masked
image inputs, we incorporate a cost efficient surrogate model trained to align
with the target model distribution, enhancing detection capability. Our
framework demonstrates strong performance, outperforming baseline methods by
4.31% in mean average precision across eight diffusion model variant datasets.

</details>

### [53] [Stagnation in Evolutionary Algorithms: Convergence $\neq$ Optimality](https://arxiv.org/abs/2505.01036)
*Xiaojun Zhou*

Main category: cs.LG

TLDR: 论文挑战了进化计算中关于停滞和收敛的传统观点，指出个体停滞可能促进种群收敛，且收敛不必然意味着最优性。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为停滞阻碍进化算法的收敛，且收敛等同于最优性，但这一观点存在误导。

Method: 通过理论分析和提供反例，论证个体停滞对种群收敛的潜在促进作用。

Result: 研究表明，收敛本身不足以保证进化算法的有效性，且收敛不必然达到局部或全局最优。

Conclusion: 收敛和停滞的关系需重新审视，进化算法的设计应超越简单的收敛指标。

Abstract: In the evolutionary computation community, it is widely believed that
stagnation impedes convergence in evolutionary algorithms, and that convergence
inherently indicates optimality. However, this perspective is misleading. In
this study, it is the first to highlight that the stagnation of an individual
can actually facilitate the convergence of the entire population, and
convergence does not necessarily imply optimality, not even local optimality.
Convergence alone is insufficient to ensure the effectiveness of evolutionary
algorithms. Several counterexamples are provided to illustrate this argument.

</details>

### [54] [Global Optimality of Single-Timescale Actor-Critic under Continuous State-Action Space: A Study on Linear Quadratic Regulator](https://arxiv.org/abs/2505.01041)
*Xuyang Chen,Jingliang Duan,Lin Zhao*

Main category: cs.LG

TLDR: 论文研究了单样本单时间尺度的actor-critic方法在连续状态-动作空间中的性能，证明了其在LQR问题中可以达到ε-最优解，样本复杂度为ε的-2次方。


<details>
  <summary>Details</summary>
Motivation: 尽管actor-critic方法在实际任务中表现优异，但其理论理解仍不足，现有研究多关注不常见的变体（如双循环或双时间尺度），且仅适用于有限状态-动作空间。本文旨在填补这一空白，研究经典的单时间尺度方法在连续空间中的性能。

Method: 采用经典的线性二次调节器（LQR）问题作为案例，分析单样本单时间尺度actor-critic方法在连续状态-动作空间中的表现。

Result: 证明了该方法可以达到ε-最优解，样本复杂度为ε的-2次方。

Conclusion: 研究为单时间尺度actor-critic方法的性能提供了新见解，进一步缩小了理论与实践的差距。

Abstract: Actor-critic methods have achieved state-of-the-art performance in various
challenging tasks. However, theoretical understandings of their performance
remain elusive and challenging. Existing studies mostly focus on practically
uncommon variants such as double-loop or two-timescale stepsize actor-critic
algorithms for simplicity. These results certify local convergence on finite
state- or action-space only. We push the boundary to investigate the classic
single-sample single-timescale actor-critic on continuous (infinite)
state-action space, where we employ the canonical linear quadratic regulator
(LQR) problem as a case study. We show that the popular single-timescale
actor-critic can attain an epsilon-optimal solution with an order of epsilon to
-2 sample complexity for solving LQR on the demanding continuous state-action
space. Our work provides new insights into the performance of single-timescale
actor-critic, which further bridges the gap between theory and practice.

</details>

### [55] [Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities](https://arxiv.org/abs/2505.01043)
*Zhiwei Hao,Jianyuan Guo,Li Shen,Yong Luo,Han Hu,Guoxia Wang,Dianhai Yu,Yonggang Wen,Dacheng Tao*

Main category: cs.LG

TLDR: 该论文综述了低精度训练方法，将其分为三类（定点/整数、浮点、自定义格式），并讨论了量化感知训练，同时指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练资源需求高，低精度训练虽提升效率，但方法多样且分散，需统一整理。

Method: 将低精度训练方法按数值格式分为三类，并讨论量化感知训练。

Result: 系统分类了低精度训练方法，提供了研究资源库。

Conclusion: 低精度训练方法多样，分类有助于研究统一，未来需进一步探索。

Abstract: Large language models (LLMs) have achieved impressive performance across
various domains. However, the substantial hardware resources required for their
training present a significant barrier to efficiency and scalability. To
mitigate this challenge, low-precision training techniques have been widely
adopted, leading to notable advancements in training efficiency. Despite these
gains, low-precision training involves several components$\unicode{x2013}$such
as weights, activations, and gradients$\unicode{x2013}$each of which can be
represented in different numerical formats. The resulting diversity has created
a fragmented landscape in low-precision training research, making it difficult
for researchers to gain a unified overview of the field. This survey provides a
comprehensive review of existing low-precision training methods. To
systematically organize these approaches, we categorize them into three primary
groups based on their underlying numerical formats, which is a key factor
influencing hardware compatibility, computational efficiency, and ease of
reference for readers. The categories are: (1) fixed-point and integer-based
methods, (2) floating-point-based methods, and (3) customized format-based
methods. Additionally, we discuss quantization-aware training approaches, which
share key similarities with low-precision training during forward propagation.
Finally, we highlight several promising research directions to advance this
field. A collection of papers discussed in this survey is provided in
https://github.com/Hao840/Awesome-Low-Precision-Training.

</details>

### [56] [Multi-Step Consistency Models: Fast Generation with Theoretical Guarantees](https://arxiv.org/abs/2505.01049)
*Nishant Jain,Xunpeng Huang,Yian Ma,Tong Zhang*

Main category: cs.LG

TLDR: 本文分析了一致性模型的理论基础，证明了其在少量步骤内实现高质量样本生成的加速能力，并提供了KL散度收敛的理论保证。


<details>
  <summary>Details</summary>
Motivation: 尽管一致性模型在生成加速方面表现出色，但其理论支持仍不足。本文旨在填补这一空白，为其加速能力提供理论依据。

Method: 通过分析一致性模型在反向轨迹中的映射能力，证明其能以恒定步长实现KL散度的收敛，并探讨了数据分布假设下的收敛性。

Result: 结果表明，一致性模型在少量步骤内可实现KL散度的收敛，且在非平滑设置下优于现有SDE或ODE分析。

Conclusion: 本文为一致性模型的加速生成提供了理论支持，并展示了其在平滑和非平滑设置下的优越性。

Abstract: Consistency models have recently emerged as a compelling alternative to
traditional SDE based diffusion models, offering a significant acceleration in
generation by producing high quality samples in very few steps. Despite their
empirical success, a proper theoretic justification for their speed up is still
lacking. In this work, we provide the analysis which bridges this gap, showing
that given a consistency model which can map the input at a given time to
arbitrary timestamps along the reverse trajectory, one can achieve KL
divergence of order $ O(\varepsilon^2) $ using only $
O\left(\log\left(\frac{d}{\varepsilon}\right)\right) $ iterations with constant
step size, where d is the data dimension. Additionally, under minimal
assumptions on the data distribution an increasingly common setting in recent
diffusion model analyses we show that a similar KL convergence guarantee can be
obtained, with the number of steps scaling as $ O\left(d
\log\left(\frac{d}{\varepsilon}\right)\right) $. Going further, we also provide
a theoretical analysis for estimation of such consistency models, concluding
that accurate learning is feasible using small discretization steps, both in
smooth and non smooth settings. Notably, our results for the non smooth case
yield best in class convergence rates compared to existing SDE or ODE based
analyses under minimal assumptions.

</details>

### [57] [Monotone Peridynamic Neural Operator for Nonlinear Material Modeling with Conditionally Unique Solutions](https://arxiv.org/abs/2505.01060)
*Jihong Wang,Xiaochuan Tian,Zhongqiang Zhang,Stewart Silling,Siavash Jafarzadeh,Yue Yu*

Main category: cs.LG

TLDR: 论文提出了一种基于神经算子的单调近场动力学神经网络算子（MPNO），用于数据驱动的非局部本构模型学习，通过单调梯度网络保证解的唯一性，并在合成和真实数据上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的本构模型在物理可解释性和泛化性上有优势，但其适定性通常无法保证，容易产生非物理解。MPNO旨在解决这一问题。

Method: MPNO通过学习非局部核和非线性本构关系，并通过单调梯度网络约束梯度，确保能量密度函数的凸性，从而保证解的唯一性。

Result: 在合成数据上，MPNO随着网格尺寸减小收敛于真实解；在真实数据上，MPNO比传统神经网络具有更好的泛化能力。

Conclusion: MPNO在数据驱动的非局部本构模型学习中表现出色，具有实际应用潜力。

Abstract: Data-driven methods have emerged as powerful tools for modeling the responses
of complex nonlinear materials directly from experimental measurements. Among
these methods, the data-driven constitutive models present advantages in
physical interpretability and generalizability across different boundary
conditions/domain settings. However, the well-posedness of these learned models
is generally not guaranteed a priori, which makes the models prone to
non-physical solutions in downstream simulation tasks. In this study, we
introduce monotone peridynamic neural operator (MPNO), a novel data-driven
nonlocal constitutive model learning approach based on neural operators. Our
approach learns a nonlocal kernel together with a nonlinear constitutive
relation, while ensuring solution uniqueness through a monotone gradient
network. This architectural constraint on gradient induces convexity of the
learnt energy density function, thereby guaranteeing solution uniqueness of
MPNO in small deformation regimes. To validate our approach, we evaluate MPNO's
performance on both synthetic and real-world datasets. On synthetic datasets
with manufactured kernel and constitutive relation, we show that the learnt
model converges to the ground-truth as the measurement grid size decreases both
theoretically and numerically. Additionally, our MPNO exhibits superior
generalization capabilities than the conventional neural networks: it yields
smaller displacement solution errors in down-stream tasks with new and unseen
loadings. Finally, we showcase the practical utility of our approach through
applications in learning a homogenized model from molecular dynamics data,
highlighting its expressivity and robustness in real-world scenarios.

</details>

### [58] [Improving Group Fairness in Knowledge Distillation via Laplace Approximation of Early Exits](https://arxiv.org/abs/2505.01070)
*Edvin Fasth,Sagar Singh*

Main category: cs.LG

TLDR: 论文提出利用拉普拉斯近似方法改进知识蒸馏中的公平性，通过校准不确定性估计重加权困难样本。


<details>
  <summary>Details</summary>
Motivation: 解决知识蒸馏中学生模型在早期层学习简单特征导致群体公平性下降的问题。

Method: 采用拉普拉斯近似方法获取校准的不确定性估计，重加权交叉熵和蒸馏损失。

Result: 在MultiNLI数据集上验证了方法的有效性，提升了群体公平性。

Conclusion: 拉普拉斯近似比基于边际的方法更稳健，能有效识别困难样本并改善公平性。

Abstract: Knowledge distillation (KD) has become a powerful tool for training compact
student models using larger, pretrained teacher models, often requiring less
data and computational resources. Teacher models typically possess more layers
and thus exhibit richer feature representations compared to their student
counterparts. Furthermore, student models tend to learn simpler, surface-level
features in their early layers. This discrepancy can increase errors in groups
where labels spuriously correlate with specific input attributes, leading to a
decline in group fairness even when overall accuracy remains comparable to the
teacher. To mitigate these challenges, Early-Exit Neural Networks (EENNs),
which enable predictions at multiple intermediate layers, have been employed.
Confidence margins derived from these early exits have been utilized to
reweight both cross-entropy and distillation losses on a per-instance basis. In
this paper, we propose that leveraging Laplace approximation-based methods to
obtain well-calibrated uncertainty estimates can also effectively reweight
challenging instances and improve group fairness. We hypothesize that Laplace
approximation offers a more robust identification of difficult or ambiguous
instances compared to margin-based approaches. To validate our claims, we
benchmark our approach using a Bert-based model on the MultiNLI dataset.

</details>

### [59] [Federated Adapter on Foundation Models: An Out-Of-Distribution Approach](https://arxiv.org/abs/2505.01075)
*Yiyuan Yang,Guodong Long,Tianyi Zhou,Qinghua Lu,Shanshan Ye,Jing Jiang*

Main category: cs.LG

TLDR: FedOA提出了一种基于适配器的参数高效微调方法，通过个性化适配器和特征距离正则化，解决联邦基础模型（FedFM）中的分布外泛化问题。


<details>
  <summary>Details</summary>
Motivation: 联邦基础模型（FedFM）在隐私保护下协作微调模型，但面临分布外（OOD）泛化挑战，现有方法难以应对大规模参数和数据异构性。

Method: FedOA采用适配器微调方法，引入个性化适配器和特征距离正则化，以对齐分布并保证客户端的OOD泛化。

Result: 理论证明全局模型具有OOD泛化能力，FedOA通过正则化增强个性化模型的泛化，并在非凸条件下收敛。实验验证了其在NLP任务中的有效性。

Conclusion: FedOA通过适配器和正则化解决了FedFM中的OOD泛化问题，理论和实验均验证了其有效性。

Abstract: As foundation models gain prominence, Federated Foundation Models (FedFM)
have emerged as a privacy-preserving approach to collaboratively fine-tune
models in federated learning (FL) frameworks using distributed datasets across
clients. A key challenge for FedFM, given the versatile nature of foundation
models, is addressing out-of-distribution (OOD) generalization, where unseen
tasks or clients may exhibit distribution shifts leading to suboptimal
performance. Although numerous studies have explored OOD generalization in
conventional FL, these methods are inadequate for FedFM due to the challenges
posed by large parameter scales and increased data heterogeneity. To address
these, we propose FedOA, which employs adapter-based parameter-efficient
fine-tuning methods for efficacy and introduces personalized adapters with
feature distance-based regularization to align distributions and guarantee OOD
generalization for each client. Theoretically, we demonstrate that the
conventional aggregated global model in FedFM inherently retains OOD
generalization capabilities, and our proposed method enhances the personalized
model's OOD generalization through regularization informed by the global model,
with proven convergence under general non-convex settings. Empirically, the
effectiveness of the proposed method is validated on benchmark datasets across
various NLP tasks.

</details>

### [60] [Integration Matters for Learning PDEs with Backwards SDEs](https://arxiv.org/abs/2505.01078)
*Sungje Park,Stephen Tu*

Main category: cs.LG

TLDR: 本文提出了一种基于Stratonovich的BSDE方法，通过Heun积分消除EM积分引入的偏差，显著提升了BSDE求解器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于BSDE的求解器在性能上不及PINNs，研究发现其根本原因是标准EM积分方案在短时域自一致性BSDE损失中引入的离散化偏差。

Method: 提出了一种Stratonovich-based BSDE方法，并采用Heun积分实现，以消除EM积分的偏差问题。

Result: 实验表明，Heun-based BSDE方法显著优于EM变体，并在多个高维基准测试中与PINNs竞争。

Conclusion: 积分方案在BSDE-based PDE求解器中起关键作用，本文方法为相关研究提供了新思路。

Abstract: Backward stochastic differential equation (BSDE)-based deep learning methods
provide an alternative to Physics-Informed Neural Networks (PINNs) for solving
high-dimensional partial differential equations (PDEs), offering algorithmic
advantages in settings such as stochastic optimal control, where the PDEs of
interest are tied to an underlying dynamical system. However, existing
BSDE-based solvers have empirically been shown to underperform relative to
PINNs in the literature. In this paper, we identify the root cause of this
performance gap as a discretization bias introduced by the standard
Euler-Maruyama (EM) integration scheme applied to short-horizon
self-consistency BSDE losses, which shifts the optimization landscape off
target. We find that this bias cannot be satisfactorily addressed through finer
step sizes or longer self-consistency horizons. To properly handle this issue,
we propose a Stratonovich-based BSDE formulation, which we implement with
stochastic Heun integration. We show that our proposed approach completely
eliminates the bias issues faced by EM integration. Furthermore, our empirical
results show that our Heun-based BSDE method consistently outperforms EM-based
variants and achieves competitive results with PINNs across multiple
high-dimensional benchmarks. Our findings highlight the critical role of
integration schemes in BSDE-based PDE solvers, an algorithmic detail that has
received little attention thus far in the literature.

</details>

### [61] [Multi-Objective Reinforcement Learning for Water Management](https://arxiv.org/abs/2505.01094)
*Zuzanna Osika,Roxana Radelescu,Jazmin Zatarain Salazar,Frans Oliehoek,Pradeep K. Murukannaiah*

Main category: cs.LG

TLDR: 论文提出了一种基于多目标强化学习（MORL）的水资源管理案例研究，并评估了现有MORL算法的性能，发现专业方法优于MORL算法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多目标优化问题（如资源管理、自动驾驶）需要同时处理多个冲突目标，但MORL领域缺乏复杂、真实的环境和基准。

Method: 引入尼罗河流域水资源管理案例，将其建模为MORL环境，并评估现有MORL算法。

Result: 专业的水资源管理方法优于当前最先进的MORL算法，突显了MORL在现实场景中的可扩展性挑战。

Conclusion: MORL算法在复杂现实问题中面临挑战，需要进一步改进以适应实际应用。

Abstract: Many real-world problems (e.g., resource management, autonomous driving, drug
discovery) require optimizing multiple, conflicting objectives. Multi-objective
reinforcement learning (MORL) extends classic reinforcement learning to handle
multiple objectives simultaneously, yielding a set of policies that capture
various trade-offs. However, the MORL field lacks complex, realistic
environments and benchmarks. We introduce a water resource (Nile river basin)
management case study and model it as a MORL environment. We then benchmark
existing MORL algorithms on this task. Our results show that specialized water
management methods outperform state-of-the-art MORL approaches, underscoring
the scalability challenges MORL algorithms face in real-world scenarios.

</details>

### [62] [Nesterov Method for Asynchronous Pipeline Parallel Optimization](https://arxiv.org/abs/2505.01099)
*Thalaiyasingam Ajanthan,Sameera Ramasinghe,Yan Zuo,Gil Avraham,Alexander Long*

Main category: cs.LG

TLDR: 论文提出了一种改进的Nesterov加速梯度方法，用于解决流水线并行中异步优化的梯度延迟问题，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决流水线并行中异步优化导致的梯度延迟问题，以提高训练效率。

Method: 改进Nesterov加速梯度方法，调整其前瞻步骤以应对梯度延迟。

Result: 理论证明在固定梯度延迟下收敛速度为次线性，实验显示在10亿参数模型上优于同步基线。

Conclusion: 该方法有效解决了异步优化中的梯度延迟问题，显著提升了训练性能。

Abstract: Pipeline Parallelism (PP) enables large neural network training on small,
interconnected devices by splitting the model into multiple stages. To maximize
pipeline utilization, asynchronous optimization is appealing as it offers 100%
pipeline utilization by construction. However, it is inherently challenging as
the weights and gradients are no longer synchronized, leading to stale (or
delayed) gradients. To alleviate this, we introduce a variant of Nesterov
Accelerated Gradient (NAG) for asynchronous optimization in PP. Specifically,
we modify the look-ahead step in NAG to effectively address the staleness in
gradients. We theoretically prove that our approach converges at a sublinear
rate in the presence of fixed delay in gradients. Our experiments on
large-scale language modelling tasks using decoder-only architectures with up
to 1B parameters, demonstrate that our approach significantly outperforms
existing asynchronous methods, even surpassing the synchronous baseline.

</details>

### [63] [CoCoAFusE: Beyond Mixtures of Experts via Model Fusion](https://arxiv.org/abs/2505.01105)
*Aurelio Raffa Ugolini,Mara Tanelli,Valentina Breschi*

Main category: cs.LG

TLDR: CoCoAFusE是一种新型贝叶斯协变量依赖建模技术，通过融合专家分布而非简单混合，提高了模型表达能力和局部可解释性，同时减少了多模态伪影。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在解释性和不确定性量化方面的不足，同时提升复杂回归问题的建模能力。

Method: 基于专家混合（MoE）框架，引入专家分布的竞争/协作融合（CoCoAFusE），避免传统混合方法的多模态伪影。

Result: 在数值和实际数据实验中，CoCoAFusE表现出更高的表达能力和灵活性，能更准确地量化不确定性。

Conclusion: CoCoAFusE为复杂回归问题提供了一种更优的建模方法，尤其在不确定性量化方面表现突出。

Abstract: Many learning problems involve multiple patterns and varying degrees of
uncertainty dependent on the covariates. Advances in Deep Learning (DL) have
addressed these issues by learning highly nonlinear input-output dependencies.
However, model interpretability and Uncertainty Quantification (UQ) have often
straggled behind. In this context, we introduce the Competitive/Collaborative
Fusion of Experts (CoCoAFusE), a novel, Bayesian Covariates-Dependent Modeling
technique. CoCoAFusE builds on the very philosophy behind Mixtures of Experts
(MoEs), blending predictions from several simple sub-models (or "experts") to
achieve high levels of expressiveness while retaining a substantial degree of
local interpretability. Our formulation extends that of a classical Mixture of
Experts by contemplating the fusion of the experts' distributions in addition
to their more usual mixing (i.e., superimposition). Through this additional
feature, CoCoAFusE better accommodates different scenarios for the intermediate
behavior between generating mechanisms, resulting in tighter credible bounds on
the response variable. Indeed, only resorting to mixing, as in classical MoEs,
may lead to multimodality artifacts, especially over smooth transitions.
Instead, CoCoAFusE can avoid these artifacts even under the same structure and
priors for the experts, leading to greater expressiveness and flexibility in
modeling. This new approach is showcased extensively on a suite of motivating
numerical examples and a collection of real-data ones, demonstrating its
efficacy in tackling complex regression problems where uncertainty is a key
quantity of interest.

</details>

### [64] [Incorporating Inductive Biases to Energy-based Generative Models](https://arxiv.org/abs/2505.01111)
*Yukun Li,Li-Ping Liu*

Main category: cs.LG

TLDR: 提出了一种结合能量基模型（EBM）和指数族模型的混合方法，通过引入无参数统计函数来增强数据建模能力。


<details>
  <summary>Details</summary>
Motivation: 能量基模型在生成模型中重新受到关注，但通常需要神经网络定义能量函数。本研究旨在通过结合指数族模型，引入归纳偏置以改进数据建模。

Method: 混合模型在能量项中引入无参数统计函数，以捕捉关键数据统计量，并在训练中近似最大化数据似然。

Result: 实验验证了混合模型能有效匹配统计量，且当引入合适的统计信息时，数据拟合和生成性能均有所提升。

Conclusion: 混合模型通过结合EBM和指数族模型，能够更好地捕捉数据统计特性，提升生成效果。

Abstract: With the advent of score-matching techniques for model training and Langevin
dynamics for sample generation, energy-based models (EBMs) have gained renewed
interest as generative models. Recent EBMs usually use neural networks to
define their energy functions. In this work, we introduce a novel hybrid
approach that combines an EBM with an exponential family model to incorporate
inductive bias into data modeling. Specifically, we augment the energy term
with a parameter-free statistic function to help the model capture key data
statistics. Like an exponential family model, the hybrid model aims to align
the distribution statistics with data statistics during model training, even
when it only approximately maximizes the data likelihood. This property enables
us to impose constraints on the hybrid model. Our empirical study validates the
hybrid model's ability to match statistics. Furthermore, experimental results
show that data fitting and generation improve when suitable informative
statistics are incorporated into the hybrid model.

</details>

### [65] [Exploring Equity of Climate Policies using Multi-Agent Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2505.01115)
*Palok Biswas,Zuzanna Osika,Isidoro Tamassia,Adit Whorra,Jazmin Zatarain-Salazar,Jan Kwakkel,Frans A. Oliehoek,Pradeep K. Murukannaiah*

Main category: cs.LG

TLDR: 论文提出了一种名为Justice的新框架，将IAM与多目标多智能体强化学习结合，以解决传统IAM在气候政策中单一目标优化的局限性，同时兼顾经济、气候目标和公平性。


<details>
  <summary>Details</summary>
Motivation: 传统IAM仅基于单一目标优化政策，无法捕捉经济、气候目标和公平性之间的权衡，导致政策建议可能加剧不平等。

Method: 引入Justice框架，结合IAM与多目标多智能体强化学习（MOMARL），通过多目标和多智能体模拟政策互动。

Result: Justice能够生成均衡的帕累托最优政策，揭示气候与经济政策中的权衡，支持决策者进行协商。

Conclusion: Justice框架为气候政策提供了更全面的分析工具，有助于平衡多方利益，促进公平决策。

Abstract: Addressing climate change requires coordinated policy efforts of nations
worldwide. These efforts are informed by scientific reports, which rely in part
on Integrated Assessment Models (IAMs), prominent tools used to assess the
economic impacts of climate policies. However, traditional IAMs optimize
policies based on a single objective, limiting their ability to capture the
trade-offs among economic growth, temperature goals, and climate justice. As a
result, policy recommendations have been criticized for perpetuating
inequalities, fueling disagreements during policy negotiations. We introduce
Justice, the first framework integrating IAM with Multi-Objective Multi-Agent
Reinforcement Learning (MOMARL). By incorporating multiple objectives, Justice
generates policy recommendations that shed light on equity while balancing
climate and economic goals. Further, using multiple agents can provide a
realistic representation of the interactions among the diverse policy actors.
We identify equitable Pareto-optimal policies using our framework, which
facilitates deliberative decision-making by presenting policymakers with the
inherent trade-offs in climate and economic policy.

</details>

### [66] [Risk Analysis and Design Against Adversarial Actions](https://arxiv.org/abs/2505.01130)
*Marco C. Campi,Algo Carè,Luis G. Crespo,Simone Garatti,Federico A. Ramponi*

Main category: cs.LG

TLDR: 提出了一种评估模型对抗攻击鲁棒性的通用框架，适用于多种攻击类型和强度，无需额外测试数据。


<details>
  <summary>Details</summary>
Motivation: 部署时数据常与训练条件不符，需评估模型对抗攻击的鲁棒性。

Method: 基于支持向量回归（SVR），扩展至松弛优化技术领域。

Result: 无需额外测试数据即可评估模型脆弱性，适用于无分布场景。

Conclusion: 框架增强模型可信度，为选择模型提供依据，并对分布外框架提供新见解。

Abstract: Learning models capable of providing reliable predictions in the face of
adversarial actions has become a central focus of the machine learning
community in recent years. This challenge arises from observing that data
encountered at deployment time often deviate from the conditions under which
the model was trained. In this paper, we address deployment-time adversarial
actions and propose a versatile, well-principled framework to evaluate the
model's robustness against attacks of diverse types and intensities. While we
initially focus on Support Vector Regression (SVR), the proposed approach
extends naturally to the broad domain of learning via relaxed optimization
techniques. Our results enable an assessment of the model vulnerability without
requiring additional test data and operate in a distribution-free setup. These
results not only provide a tool to enhance trust in the model's applicability
but also aid in selecting among competing alternatives. Later in the paper, we
show that our findings also offer useful insights for establishing new results
within the out-of-distribution framework.

</details>

### [67] [Aggregation of Dependent Expert Distributions in Multimodal Variational Autoencoders](https://arxiv.org/abs/2505.01134)
*Rogelio A Mancisidor,Robert Jenssen,Shujian Yu,Michael Kampffmeyer*

Main category: cs.LG

TLDR: 提出了一种基于共识依赖专家（CoDE）的新型多模态VAE方法，避免了现有方法对模态独立性的过度乐观假设，提升了生成质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态VAE方法假设模态独立性，导致性能受限。本研究旨在通过依赖模态间的共识关系改进这一限制。

Method: 利用CoDE原则聚合单模态分布，提出新型ELBO近似多模态数据的联合似然，学习各模态子集的贡献。

Result: CoDE-VAE在生成质量与一致性间取得更好平衡，生成更精确的对数似然估计，且在多模态增加时最小化质量差距。

Conclusion: CoDE-VAE在生成质量和分类准确率上表现优异，部分情况下接近单模态VAE性能，优于现有方法。

Abstract: Multimodal learning with variational autoencoders (VAEs) requires estimating
joint distributions to evaluate the evidence lower bound (ELBO). Current
methods, the product and mixture of experts, aggregate single-modality
distributions assuming independence for simplicity, which is an overoptimistic
assumption. This research introduces a novel methodology for aggregating
single-modality distributions by exploiting the principle of consensus of
dependent experts (CoDE), which circumvents the aforementioned assumption.
Utilizing the CoDE method, we propose a novel ELBO that approximates the joint
likelihood of the multimodal data by learning the contribution of each subset
of modalities. The resulting CoDE-VAE model demonstrates better performance in
terms of balancing the trade-off between generative coherence and generative
quality, as well as generating more precise log-likelihood estimations.
CoDE-VAE further minimizes the generative quality gap as the number of
modalities increases. In certain cases, it reaches a generative quality similar
to that of unimodal VAEs, which is a desirable property that is lacking in most
current methods. Finally, the classification accuracy achieved by CoDE-VAE is
comparable to that of state-of-the-art multimodal VAE models.

</details>

### [68] [Dual-Forecaster: A Multimodal Time Series Model Integrating Descriptive and Predictive Texts](https://arxiv.org/abs/2505.01135)
*Wenfa Wu,Guanyu Zhang,Zheng Tan,Yi Wang,Hongsheng Qi*

Main category: cs.LG

TLDR: Dual-Forecaster是一种多模态时间序列模型，结合历史和预测性文本信息，通过跨模态对齐技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有单模态时间序列模型信息不足，多模态模型虽能整合文本信息，但未能充分利用历史和未来文本的独特贡献，且缺乏对文本与时间序列数据复杂关系的理解。

Method: 提出Dual-Forecaster模型，结合历史和预测性文本信息，采用三种跨模态对齐技术增强多模态理解能力。

Result: 在15个多模态时间序列数据集上评估，Dual-Forecaster表现优于或媲美现有最优模型。

Conclusion: 该研究为整合文本与数值时间序列数据开辟了新途径，凸显了文本信息在时间序列预测中的优势。

Abstract: Most existing single-modal time series models rely solely on numerical
series, which suffer from the limitations imposed by insufficient information.
Recent studies have revealed that multimodal models can address the core issue
by integrating textual information. However, these models focus on either
historical or future textual information, overlooking the unique contributions
each plays in time series forecasting. Besides, these models fail to grasp the
intricate relationships between textual and time series data, constrained by
their moderate capacity for multimodal comprehension. To tackle these
challenges, we propose Dual-Forecaster, a pioneering multimodal time series
model that combines both descriptively historical textual information and
predictive textual insights, leveraging advanced multimodal comprehension
capability empowered by three well-designed cross-modality alignment
techniques. Our comprehensive evaluations on fifteen multimodal time series
datasets demonstrate that Dual-Forecaster is a distinctly effective multimodal
time series model that outperforms or is comparable to other state-of-the-art
models, highlighting the superiority of integrating textual information for
time series forecasting. This work opens new avenues in the integration of
textual information with numerical time series data for multimodal time series
analysis.

</details>

### [69] [Machine Learning for Physical Simulation Challenge Results and Retrospective Analysis: Power Grid Use Case](https://arxiv.org/abs/2505.01156)
*Milad Leyli-Abadi,Jérôme Picault,Antoine Marot,Jean-Patrick Brunet,Agathe Gilain,Amarsagar Reddy Ramapuram Matavalam,Shaban Ghias Satti,Quingbin Jiang,Yang Liu,Dean Justin Ninalga*

Main category: cs.LG

TLDR: 论文提出AI驱动方法加速电力潮流仿真，通过LIPS框架评估性能，并分享ML4PhySim竞赛成果。


<details>
  <summary>Details</summary>
Motivation: 解决可再生能源并网带来的电力仿真计算挑战，提升实时分析能力。

Method: 组织竞赛开发AI方法，利用LIPS框架评估性能，基于法国电网模型测试。

Result: AI方法显著加速仿真，超越传统方法，并展示工业适用性。

Conclusion: 研究为高效、可扩展的电力仿真方法提供方向，鼓励进一步研究。

Abstract: This paper addresses the growing computational challenges of power grid
simulations, particularly with the increasing integration of renewable energy
sources like wind and solar. As grid operators must analyze significantly more
scenarios in near real-time to prevent failures and ensure stability,
traditional physical-based simulations become computationally impractical. To
tackle this, a competition was organized to develop AI-driven methods that
accelerate power flow simulations by at least an order of magnitude while
maintaining operational reliability. This competition utilized a regional-scale
grid model with a 30\% renewable energy mix, mirroring the anticipated
near-future composition of the French power grid. A key contribution of this
work is through the use of LIPS (Learning Industrial Physical Systems), a
benchmarking framework that evaluates solutions based on four critical
dimensions: machine learning performance, physical compliance, industrial
readiness, and generalization to out-of-distribution scenarios. The paper
provides a comprehensive overview of the Machine Learning for Physical
Simulation (ML4PhySim) competition, detailing the benchmark suite, analyzing
top-performing solutions that outperformed traditional simulation methods, and
sharing key organizational insights and best practices for running large-scale
AI competitions. Given the promising results achieved, the study aims to
inspire further research into more efficient, scalable, and sustainable power
network simulation methodologies.

</details>

### [70] [TActiLE: Tiny Active LEarning for wearable devices](https://arxiv.org/abs/2505.01160)
*Massimo Pavan,Claudio Galimberti,Manuel Roveri*

Main category: cs.LG

TLDR: 论文提出了一种名为TActiLE的新型主动学习算法，专为TinyML场景设计，旨在减少智能穿戴设备中标记数据的负担。


<details>
  <summary>Details</summary>
Motivation: 智能穿戴设备（如智能眼镜）通过TinyML实现本地机器学习，但标记数据的稀缺性限制了On-device Learning（ODL）的应用。主动学习（AL）技术可以最小化标记工作量。

Method: 提出TActiLE算法，从设备传感器数据流中选择对模型改进最有帮助的数据进行标记。

Result: 实验表明，TActiLE适用于微型和可穿戴设备，能有效提升模型性能。

Conclusion: TActiLE为TinyML场景下的主动学习提供了可行解决方案，推动了智能穿戴设备的个性化发展。

Abstract: Tiny Machine Learning (TinyML) algorithms have seen extensive use in recent
years, enabling wearable devices to be not only connected but also genuinely
intelligent by running machine learning (ML) computations directly on-device.
Among such devices, smart glasses have particularly benefited from TinyML
advancements. TinyML facilitates the on-device execution of the inference phase
of ML algorithms on embedded and wearable devices, and more recently, it has
expanded into On-device Learning (ODL), which allows both inference and
learning phases to occur directly on the device. The application of ODL
techniques to wearable devices is particularly compelling, as it enables the
development of more personalized models that adapt based on the data of the
user. However, one of the major challenges of ODL algorithms is the scarcity of
labeled data collected on-device. In smart wearable contexts, requiring users
to manually label large amounts of data is often impractical and could lead to
user disengagement with the technology. To address this issue, this paper
explores the application of Active Learning (AL) techniques, i.e., techniques
that aim at minimizing the labeling effort, by actively selecting from a large
quantity of unlabeled data only a small subset to be labeled and added to the
training set of the algorithm. In particular, we propose TActiLE, a novel AL
algorithm that selects from the stream of on-device sensor data the ones that
would help the ML algorithm improve the most once coupled with labels provided
by the user. TActiLE is the first Active Learning technique specifically
designed for the TinyML context. We evaluate its effectiveness and efficiency
through experiments on multiple image classification datasets. The results
demonstrate its suitability for tiny and wearable devices.

</details>

### [71] [Empirical Comparison of Lightweight Forecasting Models for Seasonal and Non-Seasonal Time Series](https://arxiv.org/abs/2505.01163)
*Thanh Son Nguyen,Dang Minh Duc Nguyen,Van Thanh Nguyen*

Main category: cs.LG

TLDR: 论文比较了多项式分类器（PC）和径向基函数神经网络（RBFNN）在时间序列预测中的表现，发现PC在非季节性数据上更准确快速，而RBFNN在季节性数据上表现更好。PC结构更透明，适合实时决策。


<details>
  <summary>Details</summary>
Motivation: 研究旨在比较PC和RBFNN在时间序列预测中的性能，为实际应用提供模型选择依据。

Method: 使用四种真实数据集（天气、金价、原油价、啤酒产量）评估PC和RBFNN的预测准确性和计算效率。

Result: PC在非季节性数据上表现更优，RBFNN在季节性数据上更佳，且性能差异具有统计显著性。

Conclusion: PC适合非季节性数据的快速透明预测，RBFNN更适合复杂季节性行为。

Abstract: Accurate time series forecasting is essential in many real-time applications
that demand both high predictive accuracy and computational efficiency. This
study provides an empirical comparison between a Polynomial Classifier and a
Radial Basis Function Neural Network (RBFNN) across four real-world time series
datasets (weather conditions, gold prices, crude oil prices, and beer
production volumes) that cover both seasonal and nonseasonal patterns. Model
performance is evaluated by forecasting accuracy (using Mean Absolute Error,
Root Mean Squared Error, and Coefficient of Variation of Root Mean Squared
Error) and computational time to assess each model's viability for real time
forecasting. The results show that the PC yields more accurate and faster
forecasts for non seasonal series, whereas the RBFNN performs better on series
with pronounced seasonal patterns. From an interpretability standpoint, the
polynomial model offers a simpler, more transparent structure (in contrast to
the black box nature of neural network), which is advantageous for
understanding and trust in real time decision making. The performance
differences between PC and RBFNN are statistically significant, as confirmed by
paired t tests and Wilcoxon signed rank tests. These findings provide practical
guidance for model selection in time series forecasting, indicating that PC may
be preferable for quick, interpretable forecasts in non-seasonal contexts,
whereas RBFNN is superior for capturing complex seasonal behaviors

</details>

### [72] [Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for Adversarial Transferability](https://arxiv.org/abs/2505.01168)
*Zhaoyang Ma,Zhihao Wu,Wang Lu,Xin Gao,Jinghang Yue,Taolin Zhang,Lipo Wang,Youfang Lin,Jing Wang*

Main category: cs.LG

TLDR: HEAT提出了一种新的对抗样本生成方法，通过域泛化和动态权重分配提升对抗样本的迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在共享梯度方向捕捉和自适应权重分配方面存在不足，影响了对抗样本的迁移性和安全性。

Method: HEAT包含两个模块：共识梯度方向合成器（使用SVD合成共享梯度方向）和双和谐权重协调器（动态平衡域内一致性和域间多样性）。

Result: 实验表明HEAT在多种数据集和设置下显著优于现有方法。

Conclusion: HEAT为对抗攻击研究提供了新的视角和方向。

Abstract: The development of model ensemble attacks has significantly improved the
transferability of adversarial examples, but this progress also poses severe
threats to the security of deep neural networks. Existing methods, however,
face two critical challenges: insufficient capture of shared gradient
directions across models and a lack of adaptive weight allocation mechanisms.
To address these issues, we propose a novel method Harmonized Ensemble for
Adversarial Transferability (HEAT), which introduces domain generalization into
adversarial example generation for the first time. HEAT consists of two key
modules: Consensus Gradient Direction Synthesizer, which uses Singular Value
Decomposition to synthesize shared gradient directions; and Dual-Harmony Weight
Orchestrator which dynamically balances intra-domain coherence, stabilizing
gradients within individual models, and inter-domain diversity, enhancing
transferability across models. Experimental results demonstrate that HEAT
significantly outperforms existing methods across various datasets and
settings, offering a new perspective and direction for adversarial attack
research.

</details>

### [73] [Distilling Two-Timed Flow Models by Separately Matching Initial and Terminal Velocities](https://arxiv.org/abs/2505.01169)
*Pramook Khungurn,Pratch Piyawongwisal,Sira Sriswadi,Supasorn Suwajanakorn*

Main category: cs.LG

TLDR: 论文提出了一种新的损失函数（ITVM），用于蒸馏双时间流模型（TTFM），通过匹配初始和终端速度来改进生成性能。


<details>
  <summary>Details</summary>
Motivation: 改进现有的流匹配模型，通过更高效的蒸馏方法提升生成性能。

Method: 提出ITVM损失函数，扩展LFMD损失，匹配初始速度并优化终端速度计算。

Result: 实验表明，ITVM损失在多类数据集和模型架构上优于基线方法。

Conclusion: ITVM损失在少步生成任务中表现更优，具有实际应用潜力。

Abstract: A flow matching model learns a time-dependent vector field $v_t(x)$ that
generates a probability path $\{ p_t \}_{0 \leq t \leq 1}$ that interpolates
between a well-known noise distribution ($p_0$) and the data distribution
($p_1$). It can be distilled into a \emph{two-timed flow model} (TTFM)
$\phi_{s,x}(t)$ that can transform a sample belonging to the distribution at an
initial time $s$ to another belonging to the distribution at a terminal time
$t$ in one function evaluation. We present a new loss function for TTFM
distillation called the \emph{initial/terminal velocity matching} (ITVM) loss
that extends the Lagrangian Flow Map Distillation (LFMD) loss proposed by Boffi
et al. by adding redundant terms to match the initial velocities at time $s$,
removing the derivative from the terminal velocity term at time $t$, and using
a version of the model under training, stabilized by exponential moving
averaging (EMA), to compute the target terminal average velocity. Preliminary
experiments show that our loss leads to better few-step generation performance
on multiple types of datasets and model architectures over baselines.

</details>

### [74] [A Secured Triad of IoT, Machine Learning, and Blockchain for Crop Forecasting in Agriculture](https://arxiv.org/abs/2505.01196)
*Najmus Sakib Sizan,Md. Abu Layek,Khondokar Fida Hasan*

Main category: cs.LG

TLDR: 提出了一种结合物联网、机器学习和区块链技术的新方法，以提高作物预测准确性并为农民提供数据驱动的决策支持。


<details>
  <summary>Details</summary>
Motivation: 改善作物预测，为农民提供可操作的数据驱动见解。

Method: 集成物联网实时监测环境数据，使用随机森林模型预测作物类型和产量，并通过以太坊区块链确保数据安全。

Result: 随机森林模型预测准确率达99.45%，系统提供实时和历史作物预测，增强透明度和决策支持。

Conclusion: 该方法显著提升了精准农业的作物预测准确性、安全性和用户体验。

Abstract: To improve crop forecasting and provide farmers with actionable data-driven
insights, we propose a novel approach integrating IoT, machine learning, and
blockchain technologies. Using IoT, real-time data from sensor networks
continuously monitor environmental conditions and soil nutrient levels,
significantly improving our understanding of crop growth dynamics. Our study
demonstrates the exceptional accuracy of the Random Forest model, achieving a
99.45\% accuracy rate in predicting optimal crop types and yields, thereby
offering precise crop projections and customized recommendations. To ensure the
security and integrity of the sensor data used for these forecasts, we
integrate the Ethereum blockchain, which provides a robust and secure platform.
This ensures that the forecasted data remain tamper-proof and reliable.
Stakeholders can access real-time and historical crop projections through an
intuitive online interface, enhancing transparency and facilitating informed
decision-making. By presenting multiple predicted crop scenarios, our system
enables farmers to optimize production strategies effectively. This integrated
approach promises significant advances in precision agriculture, making crop
forecasting more accurate, secure, and user-friendly.

</details>

### [75] [CaReAQA: A Cardiac and Respiratory Audio Question Answering Model for Open-Ended Diagnostic Reasoning](https://arxiv.org/abs/2505.01199)
*Tsai-Ning Wang,Lin-Lin Chen,Neil Zeghidour,Aaqib Saeed*

Main category: cs.LG

TLDR: 论文提出了一种名为CaReAQA的音频-语言模型，结合基础音频模型和大语言模型的推理能力，用于医学音频信号的开放诊断。同时引入CaReSound数据集，评估显示模型在诊断任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统医学音频分析方法依赖手工特征或监督深度学习，需要大量标注数据，限制了其扩展性和适用性。

Method: 提出CaReAQA模型，整合基础音频模型和大语言模型的推理能力，并构建CaReSound数据集支持研究。

Result: CaReAQA在开放诊断任务中达到86.2%准确率，优于基线模型；在封闭分类任务中平均准确率为56.9%。

Conclusion: 音频-语言整合和推理能力推动了医学诊断的发展，为临床决策支持提供了高效的AI系统。

Abstract: Medical audio signals, such as heart and lung sounds, play a crucial role in
clinical diagnosis. However, analyzing these signals remains challenging:
traditional methods rely on handcrafted features or supervised deep learning
models that demand extensive labeled datasets, limiting their scalability and
applicability. To address these issues, we propose CaReAQA, an audio-language
model that integrates a foundation audio model with the reasoning capabilities
of large language models, enabling clinically relevant, open-ended diagnostic
responses. Alongside CaReAQA, we introduce CaReSound, a benchmark dataset of
annotated medical audio recordings enriched with metadata and paired
question-answer examples, intended to drive progress in diagnostic reasoning
research. Evaluation results show that CaReAQA achieves 86.2% accuracy on
open-ended diagnostic reasoning tasks, outperforming baseline models. It also
generalizes well to closed-ended classification tasks, achieving an average
accuracy of 56.9% on unseen datasets. Our findings show how audio-language
integration and reasoning advances medical diagnostics, enabling efficient AI
systems for clinical decision support.

</details>

### [76] [AGRO: An Autonomous AI Rover for Precision Agriculture](https://arxiv.org/abs/2505.01200)
*Simar Ghumman,Fabio Di Troia,William Andreopoulos,Mark Stamp,Sanjit Rai*

Main category: cs.LG

TLDR: 研究开发了一种名为AGRO的无人地面车辆，结合机器学习和传感器技术，用于农业数据采集和自动化决策支持。


<details>
  <summary>Details</summary>
Motivation: 解决农业中资源密集型操作的自动化问题，支持农民基于数据做出决策。

Method: 利用机器学习、计算机视觉和传感器技术，开发自主导航的UGV，实现实时环境映射和障碍物避让。

Result: AGRO能够自主导航、采集数据并估算开心果产量，为农民提供决策支持。

Conclusion: AGRO为农业自动化提供了可行方案，并为机器学习技术的进一步应用奠定了基础。

Abstract: Unmanned Ground Vehicles (UGVs) are emerging as a crucial tool in the world
of precision agriculture. The combination of UGVs with machine learning allows
us to find solutions for a range of complex agricultural problems. This
research focuses on developing a UGV capable of autonomously traversing
agricultural fields and capturing data. The project, known as AGRO (Autonomous
Ground Rover Observer) leverages machine learning, computer vision and other
sensor technologies. AGRO uses its capabilities to determine pistachio yields,
performing self-localization and real-time environmental mapping while avoiding
obstacles. The main objective of this research work is to automate
resource-consuming operations so that AGRO can support farmers in making
data-driven decisions. Furthermore, AGRO provides a foundation for advanced
machine learning techniques as it captures the world around it.

</details>

### [77] [Quantitative Attractor Analysis of High-Capacity Kernel Logistic Regression Hopfield Networks](https://arxiv.org/abs/2505.01218)
*Akira Tamamori*

Main category: cs.LG

TLDR: 本文通过定量分析KLR训练的网络的吸引子结构，展示了其在高容量（高达4.0 P/N）和噪声鲁棒性方面的优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统Hopfield网络存在存储容量限制和虚假吸引子问题，KLR通过非线性映射提升性能。

Method: 通过广泛模拟评估KLR网络的吸引子结构，包括存储负载和噪声水平下的召回性能。

Result: KLR表现出高容量、低虚假吸引子、快速收敛（1-2步）和鲁棒性。

Conclusion: KLR重塑了高容量联想记忆的动态特性，展示了其有效性。

Abstract: Traditional Hopfield networks, using Hebbian learning, face severe storage
capacity limits ($\approx 0.14$ P/N) and spurious attractors. Kernel Logistic
Regression (KLR) offers a non-linear approach, mapping patterns to
high-dimensional feature spaces for improved separability. Our previous work
showed KLR dramatically improves capacity and noise robustness over
conventional methods. This paper quantitatively analyzes the attractor
structures in KLR-trained networks via extensive simulations. We evaluated
recall from diverse initial states across wide storage loads (up to 4.0 P/N)
and noise levels. We quantified convergence rates and speed. Our analysis
confirms KLR's superior performance: high capacity (up to 4.0 P/N) and
robustness. The attractor landscape is remarkably "clean," with near-zero
spurious fixed points. Recall failures under high load/noise are primarily due
to convergence to other learned patterns, not spurious ones. Dynamics are
exceptionally fast (typically 1-2 steps for high-similarity states). This
characterization reveals how KLR reshapes dynamics for high-capacity
associative memory, highlighting its effectiveness and contributing to AM
understanding.

</details>

### [78] [mwBTFreddy: A Dataset for Flash Flood Damage Assessment in Urban Malawi](https://arxiv.org/abs/2505.01242)
*Evelyn Chapuma,Grey Mengezi,Lewis Msasa,Amelia Taylor*

Main category: cs.LG

TLDR: mwBTFreddy数据集支持马拉维城市洪水灾害评估，包含灾前灾后卫星图像和建筑损坏标注，用于机器学习模型开发和灾害分析。


<details>
  <summary>Details</summary>
Motivation: 支持非洲城市环境中的建筑检测和损坏分类，为气候脆弱地区的决策提供数据支持。

Method: 使用Google Earth Pro获取卫星图像，标注建筑损坏级别（无损坏、轻微、严重、毁坏）并存储为JSON文件。

Result: 开发了包含地理坐标和损坏级别的数据集，支持机器学习和空间分析。

Conclusion: 该数据集有助于灾害响应和基础设施规划，特别是在气候脆弱地区。

Abstract: This paper describes the mwBTFreddy dataset, a resource developed to support
flash flood damage assessment in urban Malawi, specifically focusing on the
impacts of Cyclone Freddy in 2023. The dataset comprises paired pre- and
post-disaster satellite images sourced from Google Earth Pro, accompanied by
JSON files containing labelled building annotations with geographic coordinates
and damage levels (no damage, minor, major, or destroyed). Developed by the
Kuyesera AI Lab at the Malawi University of Business and Applied Sciences, this
dataset is intended to facilitate the development of machine learning models
tailored to building detection and damage classification in African urban
contexts. It also supports flood damage visualisation and spatial analysis to
inform decisions on relocation, infrastructure planning, and emergency response
in climate-vulnerable regions.

</details>

### [79] [Enhancing Obsolescence Forecasting with Deep Generative Data Augmentation: A Semi-Supervised Framework for Low-Data Industrial Applications](https://arxiv.org/abs/2505.01261)
*Elie Saad,Mariem Besbes,Marc Zolghadri,Victor Czmil,Claude Baron,Vincent Bourgeois*

Main category: cs.LG

TLDR: 本文提出了一种基于深度学习的电子元件过时预测框架，通过深度生成模型解决数据不足问题，并在基准数据集上取得了先进的结果。


<details>
  <summary>Details</summary>
Motivation: 电子元件过时问题对长生命周期系统尤为关键，现有机器学习方法因数据不足难以实现高精度预测。

Method: 提出基于深度生成模型的框架，生成新过时案例以扩充训练数据，并改进经典监督学习分类器以适应半监督学习。

Result: 在基准数据集上实现了先进的预测性能。

Conclusion: 该框架有效解决了数据不足问题，提升了过时预测的准确性。

Abstract: The challenge of electronic component obsolescence is particularly critical
in systems with long life cycles. Various obsolescence management methods are
employed to mitigate its impact, with obsolescence forecasting being a highly
sought-after and prominent approach. As a result, numerous machine
learning-based forecasting methods have been proposed. However, machine
learning models require a substantial amount of relevant data to achieve high
precision, which is lacking in the current obsolescence landscape in some
situations. This work introduces a novel framework for obsolescence forecasting
based on deep learning. The proposed framework solves the lack of available
data through deep generative modeling, where new obsolescence cases are
generated and used to augment the training dataset. The augmented dataset is
then used to train a classical machine learning-based obsolescence forecasting
model. To train classical forecasting models using augmented datasets, existing
classical supervised-learning classifiers are adapted for semi-supervised
learning within this framework. The proposed framework demonstrates
state-of-the-art results on benchmarking datasets.

</details>

### [80] [MultiGran-STGCNFog: Towards Accurate and High-Throughput Inference for Multi-Granular Spatiotemporal Traffic Forecasting](https://arxiv.org/abs/2505.01279)
*Zhaoyan Wang,Xiangchi Song,In-Young Ko*

Main category: cs.LG

TLDR: 提出了一种高效的雾分布式推理系统MultiGran-STGCNFog，通过多粒度时空特征融合和动态交通图生成，改进了交通预测的准确性。同时，调度算法GA-DPHDS优化了推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于GCN的方法在多粒度时空特征提取和融合上不足，且模型复杂度和推理时间较高，难以满足快速推理需求。

Method: 提出MultiGran-STGCNFog模型，融合多粒度时空特征于动态交通图中；设计调度算法GA-DPHDS，优化层执行顺序和设备调度方案。

Result: 在真实数据集上的实验表明，该方法在预测准确性和推理吞吐量上优于基线模型。

Conclusion: MultiGran-STGCNFog通过特征融合和高效调度，显著提升了交通预测的性能和效率。

Abstract: Accurate traffic forecasting and swift inference provision are essential for
intelligent transportation systems. However, the present Graph Convolutional
Network (GCN)-based approaches cannot extract and fuse multi-granular
spatiotemporal features across various spatial and temporal scales
sufficiently, proven to yield less accurate forecasts. Besides, additional
feature extraction branches introduced in prior studies critically increased
model complexity and extended inference time, making it challenging to provide
fast inference for traffic forecasting. In this paper, we propose
MultiGran-STGCNFog, an efficient fog distributed inference system with a novel
traffic forecasting model that employs multi-granular spatiotemporal feature
fusion on generated dynamic traffic graphs to fully capture interdependent
traffic dynamics. The proposed scheduling algorithm GA-DPHDS, optimizing layer
execution order and layer-device scheduling scheme simultaneously, contributes
to considerable inference throughput improvement by leveraging heterogeneous
fog devices in a pipelined manner. Extensive experiments on real-world datasets
demonstrate the superiority of the proposed method over selected baselines.

</details>

### [81] [A Physics-preserved Transfer Learning Method for Differential Equations](https://arxiv.org/abs/2505.01281)
*Hao-Ran Yang,Chuan-Xian Ren*

Main category: cs.LG

TLDR: 提出了一种物理保持最优张量传输（POTT）方法，用于解决数据驱动方法在微分方程中的领域偏移问题，同时保持物理信息。


<details>
  <summary>Details</summary>
Motivation: 数据驱动方法（如神经算子）在解决微分方程时存在领域偏移问题，现有迁移学习方法缺乏通用性或物理保持能力。

Method: 通过将数据域建模为乘积分布，提出POTT方法，自适应校正领域偏移并保持物理信息。

Result: 实验证明POTT方法在性能、通用性和物理保持方面表现优异。

Conclusion: POTT方法是一种通用且物理保持的迁移学习方法，适用于微分方程问题。

Abstract: While data-driven methods such as neural operator have achieved great success
in solving differential equations (DEs), they suffer from domain shift problems
caused by different learning environments (with data bias or equation changes),
which can be alleviated by transfer learning (TL). However, existing TL methods
adopted in DEs problems lack either generalizability in general DEs problems or
physics preservation during training. In this work, we focus on a general
transfer learning method that adaptively correct the domain shift and preserve
physical information. Mathematically, we characterize the data domain as
product distribution and the essential problems as distribution bias and
operator bias. A Physics-preserved Optimal Tensor Transport (POTT) method that
simultaneously admits generalizability to common DEs and physics preservation
of specific problem is proposed to adapt the data-driven model to target domain
utilizing the push-forward distribution induced by the POTT map. Extensive
experiments demonstrate the superior performance, generalizability and physics
preservation of the proposed POTT method.

</details>

### [82] [2DXformer: Dual Transformers for Wind Power Forecasting with Dual Exogenous Variables](https://arxiv.org/abs/2505.01286)
*Yajuan Zhang,Jiahai Jiang,Yule Yan,Liang Yang,Ping Zhang*

Main category: cs.LG

TLDR: 论文提出2DXformer模型，改进现有深度学习方法的局限性，通过分类输入变量并利用注意力机制提升风电预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有风电预测方法未充分建模变量间关系且未区分内外生变量，导致预测精度受限和模型复杂度增加。

Method: 将输入分为外生静态、外生动态和内生变量，独立嵌入为变量标记，用注意力机制捕捉外生变量相关性，并通过多层感知机建模外生对内生的影响。

Result: 在两个大规模真实数据集上实验表明，2DXformer进一步提升了风电预测性能。

Conclusion: 2DXformer通过改进变量建模和区分内外生变量，显著提升了风电预测的准确性和效率。

Abstract: Accurate wind power forecasting can help formulate scientific dispatch plans,
which is of great significance for maintaining the safety, stability, and
efficient operation of the power system. In recent years, wind power
forecasting methods based on deep learning have focused on extracting the
spatiotemporal correlations among data, achieving significant improvements in
forecasting accuracy. However, they exhibit two limitations. First, there is a
lack of modeling for the inter-variable relationships, which limits the
accuracy of the forecasts. Second, by treating endogenous and exogenous
variables equally, it leads to unnecessary interactions between the endogenous
and exogenous variables, increasing the complexity of the model. In this paper,
we propose the 2DXformer, which, building upon the previous work's focus on
spatiotemporal correlations, addresses the aforementioned two limitations.
Specifically, we classify the inputs of the model into three types: exogenous
static variables, exogenous dynamic variables, and endogenous variables. First,
we embed these variables as variable tokens in a channel-independent manner.
Then, we use the attention mechanism to capture the correlations among
exogenous variables. Finally, we employ a multi-layer perceptron with residual
connections to model the impact of exogenous variables on endogenous variables.
Experimental results on two real-world large-scale datasets indicate that our
proposed 2DXformer can further improve the performance of wind power
forecasting. The code is available in this repository:
\href{https://github.com/jseaj/2DXformer}{https://github.com/jseaj/2DXformer}.

</details>

### [83] [Integration of Multi-Mode Preference into Home Energy Management System Using Deep Reinforcement Learning](https://arxiv.org/abs/2505.01332)
*Mohammed Sumayli,Olugbenga Moses Anubi*

Main category: cs.LG

TLDR: 本文提出了一种基于深度强化学习的家庭能源管理系统（DRL-HEMS），通过动态多模式偏好优化能源消耗，显著提升用户舒适度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将用户舒适度视为静态权重因子，忽略了用户行为和偏好的动态性。本文旨在通过动态多模式偏好增强用户在需求响应（DR）中的参与度。

Method: 采用无模型单智能体深度强化学习算法，结合实时数据（如电价、环境温度、设备功耗）设计动态优化的HEMS框架。

Result: 模型在不同偏好模式下表现出色，与传统MILP算法相比，计算效率更高且性能接近最优。

Conclusion: DRL-HEMS框架在动态优化能源消耗和提升用户体验方面具有显著优势，为智能家居能源管理提供了新思路。

Abstract: Home Energy Management Systems (HEMS) have emerged as a pivotal tool in the
smart home ecosystem, aiming to enhance energy efficiency, reduce costs, and
improve user comfort. By enabling intelligent control and optimization of
household energy consumption, HEMS plays a significant role in bridging the gap
between consumer needs and energy utility objectives. However, much of the
existing literature construes consumer comfort as a mere deviation from the
standard appliance settings. Such deviations are typically incorporated into
optimization objectives via static weighting factors. These factors often
overlook the dynamic nature of consumer behaviors and preferences. Addressing
this oversight, our paper introduces a multi-mode Deep Reinforcement
Learning-based HEMS (DRL-HEMS) framework, meticulously designed to optimize
based on dynamic, consumer-defined preferences. Our primary goal is to augment
consumer involvement in Demand Response (DR) programs by embedding dynamic
multi-mode preferences tailored to individual appliances. In this study, we
leverage a model-free, single-agent DRL algorithm to deliver a HEMS framework
that is not only dynamic but also user-friendly. To validate its efficacy, we
employed real-world data at 15-minute intervals, including metrics such as
electricity price, ambient temperature, and appliances' power consumption. Our
results show that the model performs exceptionally well in optimizing energy
consumption within different preference modes. Furthermore, when compared to
traditional algorithms based on Mixed-Integer Linear Programming (MILP), our
model achieves nearly optimal performance while outperforming in computational
efficiency.

</details>

### [84] [Enhancing Diversity in Parallel Agents: A Maximum State Entropy Exploration Story](https://arxiv.org/abs/2505.01336)
*Vincenzo De Paola,Riccardo Zamboni,Mirco Mutti,Marcello Restelli*

Main category: cs.LG

TLDR: 论文提出了一种并行强化学习框架，通过最大化数据熵和平衡个体多样性与冗余，超越传统并行数据收集的N倍加速。


<details>
  <summary>Details</summary>
Motivation: 探索并行环境中通过策略专业化是否能够超越传统并行数据收集的N倍加速效果。

Method: 采用集中式策略梯度方法，平衡个体熵与多样性，减少冗余。

Result: 实验表明该方法优于相同策略的并行系统，并能与批量RL技术协同利用数据多样性。

Conclusion: 通过专业化并行采样分布，实现了更快的收敛速度，验证了方法的有效性。

Abstract: Parallel data collection has redefined Reinforcement Learning (RL), unlocking
unprecedented efficiency and powering breakthroughs in large-scale real-world
applications. In this paradigm, $N$ identical agents operate in $N$ replicas of
an environment simulator, accelerating data collection by a factor of $N$. A
critical question arises: \textit{Does specializing the policies of the
parallel agents hold the key to surpass the $N$ factor acceleration?} In this
paper, we introduce a novel learning framework that maximizes the entropy of
collected data in a parallel setting. Our approach carefully balances the
entropy of individual agents with inter-agent diversity, effectively minimizing
redundancies. The latter idea is implemented with a centralized policy gradient
method, which shows promise when evaluated empirically against systems of
identical agents, as well as synergy with batch RL techniques that can exploit
data diversity. Finally, we provide an original concentration analysis that
shows faster rates for specialized parallel sampling distributions, which
supports our methodology and may be of independent interest.

</details>

### [85] [How to Learn a Star: Binary Classification with Starshaped Polyhedral Sets](https://arxiv.org/abs/2505.01346)
*Marie-Charlotte Brandenburg,Katharina Jochemko*

Main category: cs.LG

TLDR: 研究了基于星形多面体决策边界的连续分段线性函数在二元分类中的表达能力，分析了两种损失函数的损失景观结构，并给出了VC维的显式边界。


<details>
  <summary>Details</summary>
Motivation: 探索星形多面体决策边界在二元分类中的表达能力及其损失景观的几何特性。

Method: 使用连续分段线性函数，分析0/1损失和指数损失的子水平集结构，并计算VC维。

Result: 给出了VC维的显式边界，描述了离散损失的子水平集为超平面排列的腔室，指数损失的最优解唯一性条件及其几何特性。

Conclusion: 星形多面体决策边界在二元分类中具有明确的表达能力，损失景观的几何结构为优化提供了理论支持。

Abstract: We consider binary classification restricted to a class of continuous
piecewise linear functions whose decision boundaries are (possibly nonconvex)
starshaped polyhedral sets, supported on a fixed polyhedral simplicial fan. We
investigate the expressivity of these function classes and describe the
combinatorial and geometric structure of the loss landscape, most prominently
the sublevel sets, for two loss-functions: the 0/1-loss (discrete loss) and an
exponential loss function. In particular, we give explicit bounds on the VC
dimension of this model, and concretely describe the sublevel sets of the
discrete loss as chambers in a hyperplane arrangement. For the exponential
loss, we give sufficient conditions for the optimum to be unique, and describe
the geometry of the optimum when varying the rate parameter of the underlying
exponential probability distribution.

</details>

### [86] [Learning Stabilizing Policies via an Unstable Subspace Representation](https://arxiv.org/abs/2505.01348)
*Leonardo F. Toso,Lintao Ye,James Anderson*

Main category: cs.LG

TLDR: 论文提出了一种两阶段方法，通过学习系统的不稳定子空间并解决一系列折扣线性二次调节器问题，降低了稳定线性时不变系统的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统的策略梯度方法需要初始稳定策略，但设计这样的策略对于未知系统可能非常困难。现有方法在大数据下表现不佳，样本复杂度随维度平方增长。

Method: 两阶段方法：1) 学习系统的不稳定左子空间；2) 在子空间上解决折扣LQR问题，仅稳定不稳定动态。

Result: 非渐近保证显示，在不稳定模式远小于状态维度时，该方法显著加速稳定过程并降低样本复杂度。

Conclusion: 通过聚焦不稳定子空间，该方法有效降低了稳定线性时不变系统的样本复杂度，实验验证了其优势。

Abstract: We study the problem of learning to stabilize (LTS) a linear time-invariant
(LTI) system. Policy gradient (PG) methods for control assume access to an
initial stabilizing policy. However, designing such a policy for an unknown
system is one of the most fundamental problems in control, and it may be as
hard as learning the optimal policy itself. Existing work on the LTS problem
requires large data as it scales quadratically with the ambient dimension. We
propose a two-phase approach that first learns the left unstable subspace of
the system and then solves a series of discounted linear quadratic regulator
(LQR) problems on the learned unstable subspace, targeting to stabilize only
the system's unstable dynamics and reduce the effective dimension of the
control space. We provide non-asymptotic guarantees for both phases and
demonstrate that operating on the unstable subspace reduces sample complexity.
In particular, when the number of unstable modes is much smaller than the state
dimension, our analysis reveals that LTS on the unstable subspace substantially
speeds up the stabilization process. Numerical experiments are provided to
support this sample complexity reduction achieved by our approach.

</details>

### [87] [Stabilizing Temporal Difference Learning via Implicit Stochastic Approximation](https://arxiv.org/abs/2505.01361)
*Hwanwoo Kim,Panos Toulis,Eric Laber*

Main category: cs.LG

TLDR: 论文提出了一种隐式TD算法，通过将TD更新重新表述为固定点方程，解决了传统TD学习对步长敏感的问题，提高了稳定性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统TD学习对步长敏感，选择不当会导致估计误差增大和收敛速度变慢，需要反复试验调整步长，耗时且繁琐。

Method: 提出隐式TD算法，将TD更新转化为固定点方程，减少对步长的依赖，同时保持计算效率。

Result: 理论分析证明了算法的渐近收敛性和有限时间误差界，实验验证了其在现代RL任务中的鲁棒性和实用性。

Conclusion: 隐式TD算法是一种稳定且高效的工具，适用于策略评估和值近似。

Abstract: Temporal Difference (TD) learning is a foundational algorithm in
reinforcement learning (RL). For nearly forty years, TD learning has served as
a workhorse for applied RL as well as a building block for more complex and
specialized algorithms. However, despite its widespread use, it is not without
drawbacks, the most prominent being its sensitivity to step size. A poor choice
of step size can dramatically inflate the error of value estimates and slow
convergence. Consequently, in practice, researchers must use trial and error in
order to identify a suitable step size -- a process that can be tedious and
time consuming. As an alternative, we propose implicit TD algorithms that
reformulate TD updates into fixed-point equations. These updates are more
stable and less sensitive to step size without sacrificing computational
efficiency. Moreover, our theoretical analysis establishes asymptotic
convergence guarantees and finite-time error bounds. Our results demonstrate
their robustness and practicality for modern RL tasks, establishing implicit TD
as a versatile tool for policy evaluation and value approximation.

</details>

### [88] [Carbon Aware Transformers Through Joint Model-Hardware Optimization](https://arxiv.org/abs/2505.01386)
*Irene Wang,Newsha Ardalani,Mostafa Elhoushi,Daniel Jiang,Samuel Hsia,Ekin Sumbul,Divya Mahajan,Carole-Jean Wu,Bilge Acun*

Main category: cs.LG

TLDR: 论文提出CATransformers框架，通过综合考虑运行和硬件制造碳排放，优化ML系统设计，减少总碳排放17%。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面评估ML系统碳排放的工具，特别是硬件制造和全生命周期的隐含碳排放。

Method: 提出CATransformers框架，结合运行和隐含碳排放指标，优化ML模型和硬件架构设计。

Result: 应用于CLIP模型，CarbonCLIP系列减少总碳排放17%，同时保持准确性和延迟。

Conclusion: 需采用综合优化方法设计高性能且环保的AI系统。

Abstract: The rapid growth of machine learning (ML) systems necessitates a more
comprehensive evaluation of their environmental impact, particularly their
carbon footprint, which comprises operational carbon from training and
inference execution and embodied carbon from hardware manufacturing and its
entire life-cycle. Despite the increasing importance of embodied emissions,
there is a lack of tools and frameworks to holistically quantify and optimize
the total carbon footprint of ML systems. To address this, we propose
CATransformers, a carbon-aware architecture search framework that enables
sustainability-driven co-optimization of ML models and hardware architectures.
By incorporating both operational and embodied carbon metrics into early design
space exploration of domain-specific hardware accelerators, CATransformers
demonstrates that optimizing for carbon yields design choices distinct from
those optimized solely for latency or energy efficiency. We apply our framework
to multi-modal CLIP-based models, producing CarbonCLIP, a family of CLIP models
achieving up to 17% reduction in total carbon emissions while maintaining
accuracy and latency compared to state-of-the-art edge small CLIP baselines.
This work underscores the need for holistic optimization methods to design
high-performance, environmentally sustainable AI systems.

</details>

### [89] [Learning and Transferring Physical Models through Derivatives](https://arxiv.org/abs/2505.01391)
*Alessandro Trenta,Andrea Cossu,Davide Bacciu*

Main category: cs.LG

TLDR: DERL是一种监督学习方法，通过学习物理系统的偏导数建模，并通过蒸馏协议逐步构建物理模型。


<details>
  <summary>Details</summary>
Motivation: 旨在通过增量学习构建物理模型，并确保与物理定律一致。

Method: 提出DERL方法，利用偏导数建模物理系统，并通过蒸馏协议从预训练模型迁移知识。

Result: DERL在泛化ODE和PDE方面优于现有方法，并能扩展物理模型到新领域和参数范围。

Conclusion: DERL首次实现了多阶段增量构建物理模型，具有理论和实践优势。

Abstract: We propose Derivative Learning (DERL), a supervised approach that models
physical systems by learning their partial derivatives. We also leverage DERL
to build physical models incrementally, by designing a distillation protocol
that effectively transfers knowledge from a pre-trained to a student model. We
provide theoretical guarantees that our approach can learn the true physical
system, being consistent with the underlying physical laws, even when using
empirical derivatives. DERL outperforms state-of-the-art methods in
generalizing an ODE to unseen initial conditions and a parametric PDE to unseen
parameters. We finally propose a method based on DERL to transfer physical
knowledge across models by extending them to new portions of the physical
domain and new range of PDE parameters. We believe this is the first attempt at
building physical models incrementally in multiple stages.

</details>

### [90] [Predicting the Price of Gold in the Financial Markets Using Hybrid Models](https://arxiv.org/abs/2505.01402)
*Mohammadhossein Rashidi,Mohammad Modarres*

Main category: cs.LG

TLDR: 提出了一种结合ARIMA、逐步回归和神经网络的混合模型（ARIMA_Stepwise Regression_Neural Network），用于预测黄金价格，并在金融市场的其他领域具有潜在应用。


<details>
  <summary>Details</summary>
Motivation: 解决资本市场中价格预测的高精度需求，结合技术分析和心理因素，提升预测准确性。

Method: 使用ARIMA模型进行时间序列预测，结合逐步回归筛选关键变量，再输入神经网络进行最终预测。

Result: 混合模型在预测黄金价格时表现出比传统时间序列方法、回归和逐步回归更高的准确性。

Conclusion: 该混合模型在金融市场价格预测中具有高精度和广泛适用性。

Abstract: Predicting the price that has the least error and can provide the best and
highest accuracy has been one of the most challenging issues and one of the
most critical concerns among capital market activists and researchers.
Therefore, a model that can solve problems and provide results with high
accuracy is one of the topics of interest among researchers. In this project,
using time series prediction models such as ARIMA to estimate the price,
variables, and indicators related to technical analysis show the behavior of
traders involved in involving psychological factors for the model. By linking
all of these variables to stepwise regression, we identify the best variables
influencing the prediction of the variable. Finally, we enter the selected
variables as inputs to the artificial neural network. In other words, we want
to call this whole prediction process the "ARIMA_Stepwise Regression_Neural
Network" model and try to predict the price of gold in international financial
markets. This approach is expected to be able to be used to predict the types
of stocks, commodities, currency pairs, financial market indicators, and other
items used in local and international financial markets. Moreover, a comparison
between the results of this method and time series methods is also expressed.
Finally, based on the results, it can be seen that the resulting hybrid model
has the highest accuracy compared to the time series method, regression, and
stepwise regression.

</details>

### [91] [How Effective are Large Time Series Models in Hydrology? A Study on Water Level Forecasting in Everglades](https://arxiv.org/abs/2505.01415)
*Rahuul Rangaraj,Jimeng Shi,Azam Shirali,Rajendra Paudel,Yanzhao Wu,Giri Narasimhan*

Main category: cs.LG

TLDR: 研究探讨了时间序列基础模型和任务特定模型在预测Everglades水位中的表现，发现基础模型Chronos显著优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 传统物理和统计方法在预测水位时面临高计算成本和适应性不足的问题，而时间序列基础模型在环境系统中的应用尚未充分探索。

Method: 研究了12种任务特定模型和5种时间序列基础模型，应用于Everglades水位预测。

Result: 基础模型Chronos表现最佳，其他基础模型表现较差；任务特定模型的表现因架构而异。

Conclusion: 研究填补了时间序列基础模型在环境系统中的应用空白，并分析了模型性能差异的原因。

Abstract: The Everglades play a crucial role in flood and drought regulation, water
resource planning, and ecosystem management in the surrounding regions.
However, traditional physics-based and statistical methods for predicting water
levels often face significant challenges, including high computational costs
and limited adaptability to diverse or unforeseen conditions. Recent
advancements in large time series models have demonstrated the potential to
address these limitations, with state-of-the-art deep learning and foundation
models achieving remarkable success in time series forecasting across various
domains. Despite this progress, their application to critical environmental
systems, such as the Everglades, remains underexplored. In this study, we fill
the gap by investigating twelve task-specific models and five time series
foundation models across six categories for a real-world application focused on
water level prediction in the Everglades. Our primary results show that the
foundation model, Chronos, significantly outperforms all other models while the
remaining foundation models exhibit relatively poor performance. Moreover, the
performance of task-specific models varies with the model architectures.
Lastly, we discuss the possible reasons for the varying performance of models.

</details>

### [92] [Evaluating Frontier Models for Stealth and Situational Awareness](https://arxiv.org/abs/2505.01420)
*Mary Phuong,Roland S. Zimmermann,Ziyue Wang,David Lindner,Victoria Krakovna,Sarah Cogan,Allan Dafoe,Lewis Ho,Rohin Shah*

Main category: cs.LG

TLDR: 论文提出了一套评估AI模型是否具备阴谋能力的测试方法，包括隐蔽性和情境意识两个方面，并验证了当前前沿模型尚未表现出相关能力。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型可能具备阴谋能力，即暗中追求与开发者意图不符的目标，这对控制风险构成严重威胁。因此，开发者需在部署前排除此类风险。

Method: 设计了16项评估测试，包括5项隐蔽性评估和11项情境意识评估，用于衡量模型是否具备阴谋能力。

Result: 当前前沿模型在隐蔽性和情境意识方面均未表现出令人担忧的能力。

Conclusion: 通过这套评估方法，可以证明模型不具备阴谋能力，从而降低部署风险。

Abstract: Recent work has demonstrated the plausibility of frontier AI models scheming
-- knowingly and covertly pursuing an objective misaligned with its developer's
intentions. Such behavior could be very hard to detect, and if present in
future advanced systems, could pose severe loss of control risk. It is
therefore important for AI developers to rule out harm from scheming prior to
model deployment. In this paper, we present a suite of scheming reasoning
evaluations measuring two types of reasoning capabilities that we believe are
prerequisites for successful scheming: First, we propose five evaluations of
ability to reason about and circumvent oversight (stealth). Second, we present
eleven evaluations for measuring a model's ability to instrumentally reason
about itself, its environment and its deployment (situational awareness). We
demonstrate how these evaluations can be used as part of a scheming inability
safety case: a model that does not succeed on these evaluations is almost
certainly incapable of causing severe harm via scheming in real deployment. We
run our evaluations on current frontier models and find that none of them show
concerning levels of either situational awareness or stealth.

</details>

### [93] [Computational, Data-Driven, and Physics-Informed Machine Learning Approaches for Microstructure Modeling in Metal Additive Manufacturing](https://arxiv.org/abs/2505.01424)
*D. Patel,R. Sharma,Y. B. Guo*

Main category: cs.LG

TLDR: 金属增材制造（AM）的快速熔化和凝固动力学导致非均匀微观结构，影响机械性能。传统实验和模拟方法受限，数据驱动的机器学习缺乏物理一致性。物理信息机器学习（PIML）通过嵌入物理规律提升预测能力。本文评估了金属AM微观结构预测的建模策略，分析了实验、计算和数据驱动方法的优缺点，并探讨了PIML混合框架的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决金属增材制造中微观结构预测的挑战，优化工艺并减少缺陷。

Method: 结合实验、物理模拟和数据驱动的机器学习，特别是物理信息机器学习（PIML）方法。

Result: PIML混合方法在准确性、透明度和数据效率方面表现优越，适用于多尺度建模和不确定性量化。

Conclusion: PIML混合方法是实现金属增材制造微观结构预测和工艺控制的关键方向。

Abstract: Metal additive manufacturing enables unprecedented design freedom and the
production of customized, complex components. However, the rapid melting and
solidification dynamics inherent to metal AM processes generate heterogeneous,
non-equilibrium microstructures that significantly impact mechanical properties
and subsequent functionality. Predicting microstructure and its evolution
across spatial and temporal scales remains a central challenge for process
optimization and defect mitigation. While conventional experimental techniques
and physics-based simulations provide a physical foundation and valuable
insights, they face critical limitations. In contrast, data-driven machine
learning offers an alternative prediction approach and powerful pattern
recognition but often operate as black-box, lacking generalizability and
physical consistency. To overcome these limitations, physics-informed machine
learning, including physics-informed neural networks, has emerged as a
promising paradigm by embedding governing physical laws into neural network
architectures, thereby enhancing accuracy, transparency, data efficiency, and
extrapolation capabilities. This work presents a comprehensive evaluation of
modeling strategies for microstructure prediction in metal AM. The strengths
and limitations of experimental, computational, and data-driven methods are
analyzed in depth, and highlight recent advances in hybrid PIML frameworks that
integrate physical knowledge with ML. Key challenges, such as data scarcity,
multi-scale coupling, and uncertainty quantification, are discussed alongside
future directions. Ultimately, this assessment underscores the importance of
PIML-based hybrid approaches in enabling predictive, scalable, and physically
consistent microstructure modeling for site-specific, microstructure-aware
process control and the reliable production of high-performance AM components.

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [94] [FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models](https://arxiv.org/abs/2505.00725)
*Bithiah Yuan*

Main category: cs.CL

TLDR: 提出了一种基于BERT的金融问答系统，用于非事实性答案选择，通过检索和重排序方法提高效率，并在FiQA数据集上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 金融行业对大规模非结构化和结构化数据的自动分析需求日益增长，问答系统可为金融顾问的决策提供竞争优势。

Method: 系统分为答案检索器（BM25）和答案重排序器（BERT变体），研究了多种BERT的学习、预训练和微调方法。

Result: FinBERT-QA模型在FiQA数据集的任务2上，MRR提升16%，NDCG提升17%，Precision@1提升21%。

Conclusion: 提出的FinBERT-QA系统在金融非事实性答案选择任务中表现优异，显著优于现有方法。

Abstract: Motivated by the emerging demand in the financial industry for the automatic
analysis of unstructured and structured data at scale, Question Answering (QA)
systems can provide lucrative and competitive advantages to companies by
facilitating the decision making of financial advisers. Consequently, we
propose a novel financial QA system using the transformer-based pre-trained
BERT language model to address the limitations of data scarcity and language
specificity in the financial domain. Our system focuses on financial
non-factoid answer selection, which retrieves a set of passage-level texts and
selects the most relevant as the answer. To increase efficiency, we formulate
the answer selection task as a re-ranking problem, in which our system consists
of an Answer Retriever using BM25, a simple information retrieval approach, to
first return a list of candidate answers, and an Answer Re-ranker built with
variants of pre-trained BERT language models to re-rank and select the most
relevant answers. We investigate various learning, further pre-training, and
fine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a
model built from applying the Transfer and Adapt further fine-tuning and
pointwise learning approach, is the most effective, improving the
state-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on
NDCG, and 21% on Precision@1.

</details>

### [95] [A Survey on Large Language Model based Human-Agent Systems](https://arxiv.org/abs/2505.00753)
*Henry Peng Zou,Wei-Chieh Huang,Yaozu Wu,Yankai Chen,Chunyu Miao,Hoang Nguyen,Yue Zhou,Weizhi Zhang,Liancheng Fang,Langzhou He,Yangning Li,Yuwei Cao,Dongyuan Li,Renhe Jiang,Philip S. Yu*

Main category: cs.CL

TLDR: 本文综述了基于大型语言模型的人机代理系统（LLM-HAS），探讨了其核心组件、应用及挑战，旨在推动这一领域的研究与创新。


<details>
  <summary>Details</summary>
Motivation: 完全自主的LLM代理存在可靠性、复杂任务处理能力及安全伦理风险等问题，LLM-HAS通过引入人类信息与反馈来提升系统性能与安全性。

Method: 通过系统化的综述方法，梳理LLM-HAS的基本概念、核心组件（如环境分析、人类反馈、交互类型等）及其应用。

Result: 提供了LLM-HAS的全面结构化综述，明确了研究方向与挑战，并整理了相关资源。

Conclusion: LLM-HAS通过结合人类输入提升了代理系统的性能与安全性，未来研究需进一步探索其潜力与挑战。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building fully autonomous agents. However, fully autonomous LLM-based agents
still face significant challenges, including limited reliability due to
hallucinations, difficulty in handling complex tasks, and substantial safety
and ethical risks, all of which limit their feasibility and trustworthiness in
real-world applications. To overcome these limitations, LLM-based human-agent
systems (LLM-HAS) incorporate human-provided information, feedback, or control
into the agent system to enhance system performance, reliability and safety.
This paper provides the first comprehensive and structured survey of LLM-HAS.
It clarifies fundamental concepts, systematically presents core components
shaping these systems, including environment & profiling, human feedback,
interaction types, orchestration and communication, explores emerging
applications, and discusses unique challenges and opportunities. By
consolidating current knowledge and offering a structured overview, we aim to
foster further research and innovation in this rapidly evolving
interdisciplinary field. Paper lists and resources are available at
https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.

</details>

### [96] [Reasoning Capabilities and Invariability of Large Language Models](https://arxiv.org/abs/2505.00776)
*Alessandro Raganato,Rafael Peñaloza,Marco Viviani,Gabriella Pasi*

Main category: cs.CL

TLDR: 论文分析了大型语言模型（LLMs）在简单推理任务中的表现，重点关注其对提示的依赖性。通过引入一个新的基准数据集和实验，发现70B参数以上的LLMs在零样本设置中表现较好，但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在简单推理任务中的能力，尤其是其对提示的依赖性，以评估其推理能力。

Method: 引入一个基于几何图形的基准数据集，进行零样本和少样本提示实验，并测试链式思维提示的效果。

Result: 70B参数以上的LLMs在零样本设置中表现较好，但整体仍有改进空间；链式思维提示的效果取决于提示的时机。

Conclusion: LLMs在简单推理任务中表现有限，提示方式对其性能有显著影响，未来需进一步优化。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in
manipulating natural language across multiple applications, but their ability
to handle simple reasoning tasks is often questioned. In this work, we aim to
provide a comprehensive analysis of LLMs' reasoning competence, specifically
focusing on their prompt dependency. In particular, we introduce a new
benchmark dataset with a series of simple reasoning questions demanding shallow
logical reasoning. Aligned with cognitive psychology standards, the questions
are confined to a basic domain revolving around geometric figures, ensuring
that responses are independent of any pre-existing intuition about the world
and rely solely on deduction. An empirical analysis involving zero-shot and
few-shot prompting across 24 LLMs of different sizes reveals that, while LLMs
with over 70 billion parameters perform better in the zero-shot setting, there
is still a large room for improvement. An additional test with chain-of-thought
prompting over 22 LLMs shows that this additional prompt can aid or damage the
performance of models, depending on whether the rationale is required before or
after the answer.

</details>

### [97] [Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction](https://arxiv.org/abs/2505.00814)
*Mario Sänger,Ulf Leser*

Main category: cs.CL

TLDR: 该论文研究了预训练语言模型（PLMs）在生物医学关系抽取（RE）中的应用，通过统一评估框架比较了不同PLMs和上下文信息的效果，发现模型选择和超参数优化是关键，而上下文信息对小模型有显著帮助。


<details>
  <summary>Details</summary>
Motivation: 生物医学文献中关系抽取（RE）对管理海量科学知识至关重要，但现有研究因模型、数据和评估方法差异难以直接比较，本研究旨在填补这一研究空白。

Method: 在五个数据集上评估三种基线PLMs，进行超参数优化后，选择最佳模型并增强其上下文信息（如实体描述、知识图谱关系、分子结构编码）。

Result: 研究发现模型选择和超参数优化对性能至关重要，上下文信息整体提升有限，但对小模型有显著帮助。

Conclusion: 研究强调了模型选择和超参数优化的重要性，同时指出上下文信息对小模型的潜在价值。

Abstract: Automatic relationship extraction (RE) from biomedical literature is critical
for managing the vast amount of scientific knowledge produced each year. In
recent years, utilizing pre-trained language models (PLMs) has become the
prevalent approach in RE. Several studies report improved performance when
incorporating additional context information while fine-tuning PLMs for RE.
However, variations in the PLMs applied, the databases used for augmentation,
hyper-parameter optimization, and evaluation methods complicate direct
comparisons between studies and raise questions about the generalizability of
these findings. Our study addresses this research gap by evaluating PLMs
enhanced with contextual information on five datasets spanning four relation
scenarios within a consistent evaluation framework. We evaluate three baseline
PLMs and first conduct extensive hyperparameter optimization. After selecting
the top-performing model, we enhance it with additional data, including textual
entity descriptions, relational information from knowledge graphs, and
molecular structure encodings. Our findings illustrate the importance of i) the
choice of the underlying language model and ii) a comprehensive hyperparameter
optimization for achieving strong extraction performance. Although inclusion of
context information yield only minor overall improvements, an ablation study
reveals substantial benefits for smaller PLMs when such external data was
included during fine-tuning.

</details>

### [98] [Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing](https://arxiv.org/abs/2505.00931)
*Timur Jaganov,John Blake,Julián Villegas,Nicholas Carr*

Main category: cs.CL

TLDR: 研究探讨了大型语言模型（LLMs）在动态评估（DA）中的扩展潜力，开发了DynaWrite应用测试21种LLMs，发现GPT-4o和neural chat表现最佳，GPT-4o在反馈质量上更优。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs能否扩展动态评估，使其适用于更大规模的语言学习场景。

Method: 开发DynaWrite应用，测试21种LLMs，筛选出表现最佳的GPT-4o和neural chat进行进一步评估。

Result: GPT-4o在反馈质量、实时响应和系统稳定性上优于neural chat，适合扩展动态评估。

Conclusion: LLMs（尤其是GPT-4o）能够有效扩展动态评估，适用于大规模语言学习。

Abstract: This study investigates the potential for Large Language Models (LLMs) to
scale-up Dynamic Assessment (DA). To facilitate such an investigation, we first
developed DynaWrite-a modular, microservices-based grammatical tutoring
application which supports multiple LLMs to generate dynamic feedback to
learners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural
chat to have the most potential to scale-up DA in the language learning
classroom. Further testing of these two candidates found both models performed
similarly in their ability to accurately identify grammatical errors in user
sentences. However, GPT-4o consistently outperformed neural chat in the quality
of its DA by generating clear, consistent, and progressively explicit hints.
Real-time responsiveness and system stability were also confirmed through
detailed performance testing, with GPT-4o exhibiting sufficient speed and
stability. This study shows that LLMs can be used to scale-up dynamic
assessment and thus enable dynamic assessment to be delivered to larger groups
than possible in traditional teacher-learner settings.

</details>

### [99] [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/abs/2505.00949)
*Akhiad Bercovich,Itay Levy,Izik Golan,Mohammad Dabbah,Ran El-Yaniv,Omri Puny,Ido Galil,Zach Moshe,Tomer Ronen,Najeeb Nabwani,Ido Shahaf,Oren Tropp,Ehud Karpas,Ran Zilberstein,Jiaqi Zeng,Soumye Singhal,Alexander Bukharin,Yian Zhang,Tugrul Konuk,Gerald Shen,Ameya Sunil Mahabaleshwarkar,Bilal Kartal,Yoshi Suhara,Olivier Delalleau,Zijia Chen,Zhilin Wang,David Mosallanezhad,Adi Renduchintala,Haifeng Qian,Dima Rekesh,Fei Jia,Somshubra Majumdar,Vahid Noroozi,Wasi Uddin Ahmad,Sean Narenthiran,Aleksander Ficek,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Igor Gitman,Ivan Moshkov,Wei Du,Shubham Toshniwal,George Armstrong,Branislav Kisacanin,Matvei Novikov,Daria Gitman,Evelina Bakhturina,Jane Polak Scowcroft,John Kamalu,Dan Su,Kezhi Kong,Markus Kliegl,Rabeeh Karimi,Ying Lin,Sanjeev Satheesh,Jupinder Parmar,Pritam Gundecha,Brandon Norick,Joseph Jennings,Shrimai Prabhumoye,Syeda Nahida Akter,Mostofa Patwary,Abhinav Khattar,Deepak Narayanan,Roger Waleffe,Jimmy Zhang,Bor-Yiing Su,Guyue Huang,Terry Kong,Parth Chadha,Sahil Jain,Christine Harvey,Elad Segal,Jining Huang,Sergey Kashirsky,Robert McQueen,Izzy Putterman,George Lam,Arun Venkatesan,Sherry Wu,Vinh Nguyen,Manoj Kilaru,Andrew Wang,Anna Warno,Abhilash Somasamudramath,Sandip Bhaskar,Maka Dong,Nave Assaf,Shahar Mor,Omer Ullman Argov,Scot Junkin,Oleksandr Romanenko,Pedro Larroy,Monika Katariya,Marco Rovinelli,Viji Balas,Nicholas Edelman,Anahita Bhiwandiwalla,Muthu Subramaniam,Smita Ithape,Karthik Ramamoorthy,Yuting Wu,Suguna Varshini Velury,Omri Almog,Joyjit Daw,Denys Fridman,Erick Galinkin,Michael Evans,Katherine Luna,Leon Derczynski,Nikki Pope,Eileen Long,Seth Schneider,Guillermo Siman,Tomasz Grzegorzek,Pablo Ribalta,Monika Katariya,Joey Conway,Trisha Saar,Ann Guan,Krzysztof Pawelec,Shyamala Prayaga,Oleksii Kuchaiev,Boris Ginsburg,Oluwatobi Olabiyi,Kari Briski,Jonathan Cohen,Bryan Catanzaro,Jonah Alben,Yonatan Geifman,Eric Chung*

Main category: cs.CL

TLDR: Llama-Nemotron系列模型是一个开源的异构推理模型家族，提供卓越的推理能力、高效的推理速度和商业友好的许可协议。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个高性能、高效且开放的推理模型家族，支持动态推理切换，并推动开源研究。

Method: 通过神经架构搜索、知识蒸馏和持续预训练优化模型，随后进行监督微调和大规模强化学习的后训练阶段。

Result: 模型在推理性能和效率上优于现有技术（如DeepSeek-R1），并支持动态推理模式切换。

Conclusion: Llama-Nemotron系列模型为开源社区和企业提供了高性能的推理工具，并发布了相关数据集和代码以促进研究。

Abstract: We introduce the Llama-Nemotron series of models, an open family of
heterogeneous reasoning models that deliver exceptional reasoning capabilities,
inference efficiency, and an open license for enterprise use. The family comes
in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs
competitively with state-of-the-art reasoning models such as DeepSeek-R1 while
offering superior inference throughput and memory efficiency. In this report,
we discuss the training procedure for these models, which entails using neural
architecture search from Llama 3 models for accelerated inference, knowledge
distillation, and continued pretraining, followed by a reasoning-focused
post-training stage consisting of two main parts: supervised fine-tuning and
large scale reinforcement learning. Llama-Nemotron models are the first
open-source models to support a dynamic reasoning toggle, allowing users to
switch between standard chat and reasoning modes during inference. To further
support open research and facilitate model development, we provide the
following resources: 1. We release the Llama-Nemotron reasoning models --
LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA
Open Model License Agreement. 2. We release the complete post-training dataset:
Llama-Nemotron-Post-Training-Dataset. 3. We also release our training
codebases: NeMo, NeMo-Aligner, and Megatron-LM.

</details>

### [100] [A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts](https://arxiv.org/abs/2505.00977)
*Yingquan Chen,Qianmu Li,Xiaocong Wu,Huifeng Li,Qing Chang*

Main category: cs.CL

TLDR: 本文提出了一种新的嵌入算法CDEA，结合XLNet模型，显著提升了隐写文本的质量，特别是在感知不可察觉性方面。


<details>
  <summary>Details</summary>
Motivation: 现有模型在文本生成能力上的局限性，以及嵌入算法未能有效缓解敏感信息属性（如语义内容或随机性）的负面影响，导致隐写文本质量下降。

Method: 提出基于字符的扩散嵌入算法（CDEA），利用敏感信息的属性，通过字符级统计特性和幂律分布分组方法，优化候选词选择频率。同时引入XLNet模型处理长序列。

Result: 实验结果表明，CDEA与XLNet的结合显著提升了隐写文本的质量，尤其是在感知不可察觉性方面。

Conclusion: CDEA和XLNet的组合有效解决了隐写文本生成中的质量问题，为生成高质量隐写文本提供了新思路。

Abstract: Generating high-quality steganographic text is a fundamental challenge in the
field of generative linguistic steganography. This challenge arises primarily
from two aspects: firstly, the capabilities of existing models in text
generation are limited; secondly, embedding algorithms fail to effectively
mitigate the negative impacts of sensitive information's properties, such as
semantic content or randomness. Specifically, to ensure that the recipient can
accurately extract hidden information, embedding algorithms often have to
consider selecting candidate words with relatively low probabilities. This
phenomenon leads to a decrease in the number of high-probability candidate
words and an increase in low-probability candidate words, thereby compromising
the semantic coherence and logical fluency of the steganographic text and
diminishing the overall quality of the generated steganographic material. To
address this issue, this paper proposes a novel embedding algorithm,
character-based diffusion embedding algorithm (CDEA). Unlike existing embedding
algorithms that strive to eliminate the impact of sensitive information's
properties on the generation process, CDEA leverages sensitive information's
properties. It enhances the selection frequency of high-probability candidate
words in the candidate pool based on general statistical properties at the
character level and grouping methods based on power-law distributions, while
reducing the selection frequency of low-probability candidate words in the
candidate pool. Furthermore, to ensure the effective transformation of
sensitive information in long sequences, we also introduce the XLNet model.
Experimental results demonstrate that the combination of CDEA and XLNet
significantly improves the quality of generated steganographic text,
particularly in terms of perceptual-imperceptibility.

</details>

### [101] [Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models](https://arxiv.org/abs/2505.00979)
*Xuhui Jiang,Shengjie Ma,Chengjin Xu,Cehao Yang,Liyu Zhang,Jian Guo*

Main category: cs.CL

TLDR: SoG框架通过构建上下文图整合跨文档知识关联，提升合成数据的多样性和连贯性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在小规模、专业化语料上数据效率低的问题，现有方法忽略跨文档知识关联。

Method: 构建上下文图提取实体和概念，采用图游走策略生成知识关联的合成数据，结合CoT和CC提升质量。

Result: 在多跳文档问答数据集上优于SOTA方法，在阅读理解任务上表现相当，泛化能力更强。

Conclusion: SoG推动了合成数据生成，为数据有限领域的LLMs知识获取提供了实用解决方案。

Abstract: Large Language Models (LLMs) have achieved remarkable success but remain
data-inefficient, especially when learning from small, specialized corpora with
limited and proprietary data. Existing synthetic data generation methods for
continue pre-training focus on intra-document content and overlook
cross-document knowledge associations, limiting content diversity and depth. We
propose Synthetic-on-Graph (SoG), a synthetic data generation framework that
incorporates cross-document knowledge associations for efficient corpus
expansion. SoG constructs a context graph by extracting entities and concepts
from the original corpus, representing cross-document associations, and
employing a graph walk strategy for knowledge-associated sampling. This
enhances synthetic data diversity and coherence, enabling models to learn
complex knowledge structures and handle rare knowledge. To further improve
synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive
Clarifying (CC) synthetic, enhancing reasoning processes and discriminative
power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method
in a multi-hop document Q&A dataset while performing comparably to the SOTA
method on the reading comprehension task datasets, which also underscores the
better generalization capability of SoG. Our work advances synthetic data
generation and provides practical solutions for efficient knowledge acquisition
in LLMs, especially in domains with limited data availability.

</details>

### [102] [Position: Enough of Scaling LLMs! Lets Focus on Downscaling](https://arxiv.org/abs/2505.00985)
*Ayan Sengupta,Yash Goel,Tanmoy Chakraborty*

Main category: cs.CL

TLDR: 论文主张从神经扩展定律转向降尺度开发大型语言模型（LLMs），以解决资源浪费和环境问题。


<details>
  <summary>Details</summary>
Motivation: 传统扩展方法在计算效率、环境影响和部署限制方面存在显著不足。

Method: 提出一个降尺度LLMs的整体框架，旨在保持性能的同时大幅减少资源需求。

Result: 论文概述了从传统扩展范式转向降尺度的实用策略。

Conclusion: 倡导一种更可持续、高效且易于实施的LLM开发方法。

Abstract: We challenge the dominant focus on neural scaling laws and advocate for a
paradigm shift toward downscaling in the development of large language models
(LLMs). While scaling laws have provided critical insights into performance
improvements through increasing model and dataset size, we emphasize the
significant limitations of this approach, particularly in terms of
computational inefficiency, environmental impact, and deployment constraints.
To address these challenges, we propose a holistic framework for downscaling
LLMs that seeks to maintain performance while drastically reducing resource
demands. This paper outlines practical strategies for transitioning away from
traditional scaling paradigms, advocating for a more sustainable, efficient,
and accessible approach to LLM development.

</details>

### [103] [VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language](https://arxiv.org/abs/2505.00989)
*Sijin Sun,Liangbin Zhao,Ming Deng,Xiuju Fu*

Main category: cs.CL

TLDR: 本文提出VTS-LLM Agent，一种针对船舶交通服务（VTS）的领域自适应大型语言模型代理，用于交互式决策支持，解决了现有VTS系统在时空推理和人机交互方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有VTS系统在处理复杂交通和异构数据时存在时空推理和交互不足的问题，需要更智能的解决方案。

Method: 将风险船舶识别任务形式化为知识增强的Text-to-SQL任务，结合结构化船舶数据库和外部海事知识，并构建定制数据集。框架包含NER关系推理、领域知识注入、语义代数表示和查询重思机制。

Result: VTS-LLM在多种语言风格查询下优于通用和SQL专用基线模型，并首次实证表明语言风格变化对Text-to-SQL建模有系统性挑战。

Conclusion: 该研究为VTS的自然语言接口奠定了基础，为LLM驱动的实时海事交通管理提供了新机会。

Abstract: Vessel Traffic Services (VTS) are essential for maritime safety and
regulatory compliance through real-time traffic management. However, with
increasing traffic complexity and the prevalence of heterogeneous, multimodal
data, existing VTS systems face limitations in spatiotemporal reasoning and
intuitive human interaction. In this work, we propose VTS-LLM Agent, the first
domain-adaptive large LLM agent tailored for interactive decision support in
VTS operations. We formalize risk-prone vessel identification as a
knowledge-augmented Text-to-SQL task, combining structured vessel databases
with external maritime knowledge. To support this, we construct a curated
benchmark dataset consisting of a custom schema, domain-specific corpus, and a
query-SQL test set in multiple linguistic styles. Our framework incorporates
NER-based relational reasoning, agent-based domain knowledge injection,
semantic algebra intermediate representation, and query rethink mechanisms to
enhance domain grounding and context-aware understanding. Experimental results
show that VTS-LLM outperforms both general-purpose and SQL-focused baselines
under command-style, operational-style, and formal natural language queries,
respectively. Moreover, our analysis provides the first empirical evidence that
linguistic style variation introduces systematic performance challenges in
Text-to-SQL modeling. This work lays the foundation for natural language
interfaces in vessel traffic services and opens new opportunities for
proactive, LLM-driven maritime real-time traffic management.

</details>

### [104] [Token-free Models for Sarcasm Detection](https://arxiv.org/abs/2505.01006)
*Sumit Mamtani,Maitreya Sonawane,Kanika Agarwal,Nishanth Sanjeev*

Main category: cs.CL

TLDR: 论文评估了两种无标记模型（ByT5和CANINE）在社交媒体和非社交媒体领域的讽刺检测任务中的表现，发现它们优于基于标记的模型，并实现了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统标记化方法在自然语言处理中引入的词汇不匹配和词汇表外问题，探索无标记模型的潜力。

Method: 对ByT5和CANINE进行微调，并在社交媒体（Twitter）和非社交媒体（新闻标题）数据集上进行讽刺检测任务，与基于标记的基准模型和现有最优方法进行对比。

Result: ByT5-small和CANINE在新闻标题和Twitter讽刺数据集上的准确率分别提高了0.77%和0.49%，优于基于标记的模型。

Conclusion: 无标记模型在嘈杂和非正式领域（如社交媒体）的自然语言处理中具有潜力。

Abstract: Tokenization is a foundational step in most natural language processing (NLP)
pipelines, yet it introduces challenges such as vocabulary mismatch and
out-of-vocabulary issues. Recent work has shown that models operating directly
on raw text at the byte or character level can mitigate these limitations. In
this paper, we evaluate two token-free models, ByT5 and CANINE, on the task of
sarcasm detection in both social media (Twitter) and non-social media (news
headlines) domains. We fine-tune and benchmark these models against token-based
baselines and state-of-the-art approaches. Our results show that ByT5-small and
CANINE outperform token-based counterparts and achieve new state-of-the-art
performance, improving accuracy by 0.77% and 0.49% on the News Headlines and
Twitter Sarcasm datasets, respectively. These findings underscore the potential
of token-free models for robust NLP in noisy and informal domains such as
social media.

</details>

### [105] [Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark](https://arxiv.org/abs/2505.01015)
*Jongwook Han,Dongmin Choi,Woojung Song,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TLDR: 论文提出了Value Portrait基准，用于评估语言模型的价值取向，通过真实用户-LLM交互和心理学验证方法提高评估的生态效度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基准依赖易受价值偏见影响的人工或机器标注，且测试场景与真实使用场景脱节。

Method: 提出Value Portrait基准，包含真实用户-LLM交互项目，并通过人类评分与价值分数的相关性验证项目可靠性。

Result: 评估27个LLM发现其更重视Benevolence、Security和Self-Direction价值，较少关注Tradition、Power和Achievement价值，并揭示了对不同人口群体的偏见。

Conclusion: Value Portrait基准为评估LLM价值取向提供了更真实、可靠的框架，并揭示了模型的价值偏见。

Abstract: The importance of benchmarks for assessing the values of language models has
been pronounced due to the growing need of more authentic, human-aligned
responses. However, existing benchmarks rely on human or machine annotations
that are vulnerable to value-related biases. Furthermore, the tested scenarios
often diverge from real-world contexts in which models are commonly used to
generate text and express values. To address these issues, we propose the Value
Portrait benchmark, a reliable framework for evaluating LLMs' value
orientations with two key characteristics. First, the benchmark consists of
items that capture real-life user-LLM interactions, enhancing the relevance of
assessment results to real-world LLM usage and thus ecological validity.
Second, each item is rated by human subjects based on its similarity to their
own thoughts, and correlations between these ratings and the subjects' actual
value scores are derived. This psychometrically validated approach ensures that
items strongly correlated with specific values serve as reliable items for
assessing those values. Through evaluating 27 LLMs with our benchmark, we find
that these models prioritize Benevolence, Security, and Self-Direction values
while placing less emphasis on Tradition, Power, and Achievement values. Also,
our analysis reveals biases in how LLMs perceive various demographic groups,
deviating from real human data.

</details>

### [106] [Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?](https://arxiv.org/abs/2505.01035)
*Lui Yoshida*

Main category: cs.CL

TLDR: 研究探讨了在自动作文评分（AES）中使用详细评分标准的必要性和影响，发现简化评分标准在多数大语言模型（LLMs）中能保持评分准确性，同时显著减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 尽管详细评分标准在基于LLM的AES中是标准做法，但其创建耗时且增加计算资源消耗，因此研究简化评分标准的可行性。

Method: 使用TOEFL11数据集，比较了三种评分标准条件（完整、简化、无评分标准）在四种LLM（Claude 3.5 Haiku、Gemini 1.5 Flash、GPT-4o-mini和Llama 3 70B Instruct）中的表现。

Result: 四种模型中有三种在简化评分标准下保持了与完整评分标准相似的准确性，同时显著减少了计算资源消耗；但Gemini 1.5 Flash在详细评分标准下表现下降。

Conclusion: 简化评分标准在多数LLM中足以替代详细评分标准，但需注意模型间的性能差异。

Abstract: This study investigates the necessity and impact of a detailed rubric in
automated essay scoring (AES) using large language models (LLMs). While using
rubrics are standard in LLM-based AES, creating detailed rubrics requires
substantial ef-fort and increases token usage. We examined how different levels
of rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11
dataset. Our experiments compared three conditions: a full rubric, a simplified
rubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5
Flash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of
four models maintained similar scoring accuracy with the simplified rubric
compared to the detailed one, while significantly reducing token usage.
However, one model (Gemini 1.5 Flash) showed decreased performance with more
detailed rubrics. The findings suggest that simplified rubrics may be
sufficient for most LLM-based AES applications, offering a more efficient
alternative without compromis-ing scoring accuracy. However, model-specific
evaluation remains crucial as per-formance patterns vary across different LLMs.

</details>

### [107] [Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs](https://arxiv.org/abs/2505.01068)
*Yijie Jin,Junjie Peng,Xuanchao Lin,Haochen Yuan,Lan Wang,Cangzhi Zheng*

Main category: cs.CL

TLDR: 该论文提出了一种高效的图结构和交错掩码多模态Transformer（GsiT），通过将多模态Transformer视为层次化模态异构图（HMHG），并引入交错掩码机制，实现了参数减少和性能提升。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析（MSA）中的多模态融合效率问题，现有方法（如多模态Transformer）存在效率低下的问题。

Method: 提出GsiT模型，基于HMHG概念和交错掩码机制，实现参数共享和高效融合。

Result: GsiT仅需传统方法1/3的参数，性能显著优于传统多模态Transformer。

Conclusion: GsiT和HMHG概念在多模态情感分析中具有高效性和有效性，能显著提升现有模型的性能并减少参数。

Abstract: Multimodal Sentiment Analysis (MSA) is a rapidly developing field that
integrates multimodal information to recognize sentiments, and existing models
have made significant progress in this area. The central challenge in MSA is
multimodal fusion, which is predominantly addressed by Multimodal Transformers
(MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns.
In this work, from the perspective of efficiency optimization, we propose and
prove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and
we introduce the graph-structured representation pattern of MulTs. Based on
this pattern, we propose an Interlaced Mask (IM) mechanism to design the
Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is
formally equivalent to MulTs which achieves an efficient weight-sharing
mechanism without information disorder through IM, enabling All-Modal-In-One
fusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called
Decomposition is implemented to ensure avoiding additional computational
overhead. Moreover, it achieves significantly higher performance than
traditional MulTs. To further validate the effectiveness of GsiT itself and the
HMHG concept, we integrate them into multiple state-of-the-art models and
demonstrate notable performance improvements and parameter reduction on widely
used MSA datasets.

</details>

### [108] [MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning](https://arxiv.org/abs/2505.01110)
*Murtadha Ahmed,Wenbo,Liu yunfeng*

Main category: cs.CL

TLDR: MateICL通过分窗和注意力权重调整，解决了大语言模型在上下文学习中的注意力分散问题，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 固定位置长度限制和注意力分散问题限制了大规模上下文学习的有效性。

Method: 将上下文分窗处理，并引入额外层重新校准注意力权重。

Result: MateICL能有效利用更大上下文提升性能，优于基于检索的方法。

Conclusion: MateICL在资源受限环境中仍具优势，代码已开源。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
In-Context Learning (ICL). However, the fixed position length constraints in
pre-trained models limit the number of demonstration examples. Recent efforts
to extend context suffer from attention dispersion as the number of
demonstrations increases. In this paper, we introduce Mitigating Attention
Dispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective
self-attention as the context size grows. We first split the context into
multiple windows, each filled to the model's context capacity, which are
processed separately. Then, we introduce an additional layer to recalibrate the
attention weights, prioritizing the query tokens as the number of
demonstrations increases. Our empirical results show that MateICL can
effectively leverage larger contexts to improve ICL performance. Compared to
retrieval-based baselines, MateICL consistently achieves better performance
without requiring an externally trained retrieval model. Despite recent
advances in inference strategies (e.g., 32k token contexts), our results
demonstrate that MateICL remains beneficial in computationally
resource-constrained settings. The code is publicly available at
https://github.com/amurtadha/MateICL.

</details>

### [109] [On the Limitations of Steering in Language Model Alignment](https://arxiv.org/abs/2505.01162)
*Chebrolu Niranjan,Kokil Jaidka,Gerard Christopher Yeo*

Main category: cs.CL

TLDR: 本文评估了导向向量在语言模型对齐中的局限性，发现其在特定任务（如价值观对齐）中有效，但在复杂场景中可能不足。


<details>
  <summary>Details</summary>
Motivation: 研究导向向量作为对齐机制的局限性，特别是在不同提示结构和上下文复杂性下的效果。

Method: 使用变压器钩子干预和反义词功能向量框架进行评估。

Result: 导向向量在特定对齐任务中表现良好，但在复杂场景中缺乏鲁棒性。

Conclusion: 导向向量在特定任务中有潜力，但需进一步研究以提升其通用对齐能力。

Abstract: Steering vectors are a promising approach to aligning language model behavior
at inference time. In this paper, we propose a framework to assess the
limitations of steering vectors as alignment mechanisms. Using a framework of
transformer hook interventions and antonym-based function vectors, we evaluate
the role of prompt structure and context complexity in steering effectiveness.
Our findings indicate that steering vectors are promising for specific
alignment tasks, such as value alignment, but may not provide a robust
foundation for general-purpose alignment in LLMs, particularly in complex
scenarios. We establish a methodological foundation for future investigations
into steering capabilities of reasoning models.

</details>

### [110] [Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods](https://arxiv.org/abs/2505.01198)
*Mahdi Dhaini,Ege Erdogan,Nils Feldhus,Gjergji Kasneci*

Main category: cs.CL

TLDR: 研究发现，广泛使用的特征归因方法在性别上存在显著差异，影响解释的忠实性、鲁棒性和复杂性，即使模型训练数据无偏。


<details>
  <summary>Details</summary>
Motivation: 探讨解释方法在公平性上的不足，尤其是在不同子群体中的性能差异。

Method: 通过三个任务和五个语言模型，评估后验特征归因方法的性别差异。

Result: 发现解释方法存在性别差异，且与训练数据无关。

Conclusion: 强调在开发和应用解释方法时需关注公平性，并将其纳入监管框架。

Abstract: While research on applications and evaluations of explanation methods
continues to expand, fairness of the explanation methods concerning disparities
in their performance across subgroups remains an often overlooked aspect. In
this paper, we address this gap by showing that, across three tasks and five
language models, widely used post-hoc feature attribution methods exhibit
significant gender disparity with respect to their faithfulness, robustness,
and complexity. These disparities persist even when the models are pre-trained
or fine-tuned on particularly unbiased datasets, indicating that the
disparities we observe are not merely consequences of biased training data. Our
results highlight the importance of addressing disparities in explanations when
developing and applying explainability methods, as these can lead to biased
outcomes against certain subgroups, with particularly critical implications in
high-stakes contexts. Furthermore, our findings underscore the importance of
incorporating the fairness of explanations, alongside overall model fairness
and explainability, as a requirement in regulatory frameworks.

</details>

### [111] [EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models](https://arxiv.org/abs/2505.01238)
*Mahdi Dhaini,Kafaite Zahra Hussain,Efstratios Zaradoukas,Gjergji Kasneci*

Main category: cs.CL

TLDR: EvalxNLP是一个用于评估NLP模型解释方法的Python框架，整合了多种XAI技术，支持用户生成和评估解释，并提供了交互式文本解释功能。


<details>
  <summary>Details</summary>
Motivation: 随着NLP模型在高风险应用中的普及，确保其可解释性成为关键挑战，需要针对不同用例定制解释方法。

Method: EvalxNLP整合了八种XAI技术，支持生成和评估解释，并提供基于LLM的交互式文本解释。

Result: 用户评估显示对EvalxNLP满意度高，表明其是一个有前景的框架。

Conclusion: EvalxNLP旨在普及可解释性工具，支持XAI技术的系统比较和进步。

Abstract: As Natural Language Processing (NLP) models continue to evolve and become
integral to high-stakes applications, ensuring their interpretability remains a
critical challenge. Given the growing variety of explainability methods and
diverse stakeholder requirements, frameworks that help stakeholders select
appropriate explanations tailored to their specific use cases are increasingly
important. To address this need, we introduce EvalxNLP, a Python framework for
benchmarking state-of-the-art feature attribution methods for transformer-based
NLP models. EvalxNLP integrates eight widely recognized explainability
techniques from the Explainable AI (XAI) literature, enabling users to generate
and evaluate explanations based on key properties such as faithfulness,
plausibility, and complexity. Our framework also provides interactive,
LLM-based textual explanations, facilitating user understanding of the
generated explanations and evaluation outcomes. Human evaluation results
indicate high user satisfaction with EvalxNLP, suggesting it is a promising
framework for benchmarking explanation methods across diverse user groups. By
offering a user-friendly and extensible platform, EvalxNLP aims at
democratizing explainability tools and supporting the systematic comparison and
advancement of XAI techniques in NLP.

</details>

### [112] [PREMISE: Matching-based Prediction for Accurate Review Recommendation](https://arxiv.org/abs/2505.01255)
*Wei Han,Hui Chen,Soujanya Poria*

Main category: cs.CL

TLDR: PREMISE是一种新的多模态匹配学习架构，通过多尺度、多领域表示和匹配分数提升多模态任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统融合方法在多模态任务中语义冗余和性能不足的问题。

Method: 计算多尺度、多领域表示，过滤重复语义，生成匹配分数作为特征向量。

Result: 在两个公开数据集上表现优异，计算成本更低。

Conclusion: PREMISE在多模态任务中优于现有融合方法，性能显著提升。

Abstract: We present PREMISE (PREdict with Matching ScorEs), a new architecture for the
matching-based learning in the multimodal fields for the multimodal review
helpfulness (MRHP) task. Distinct to previous fusion-based methods which
obtains multimodal representations via cross-modal attention for downstream
tasks, PREMISE computes the multi-scale and multi-field representations,
filters duplicated semantics, and then obtained a set of matching scores as
feature vectors for the downstream recommendation task. This new architecture
significantly boosts the performance for such multimodal tasks whose context
matching content are highly correlated to the targets of that task, compared to
the state-of-the-art fusion-based methods. Experimental results on two publicly
available datasets show that PREMISE achieves promising performance with less
computational cost.

</details>

### [113] [Anti-adversarial Learning: Desensitizing Prompts for Large Language Models](https://arxiv.org/abs/2505.01273)
*Xuan Li,Zhe Yin,Xiaodong Gu,Beijun Shen*

Main category: cs.CL

TLDR: PromptObfus是一种通过反对抗学习扰动隐私词的方法，用于保护LLM提示中的隐私，同时保持模型预测的稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛使用，用户提示中的隐私保护变得至关重要，传统方法因计算成本和用户参与需求而受限。

Method: 将提示脱敏视为掩码语言建模任务，用[MASK]替换隐私词，并通过替代模型的梯度反馈选择候选替换词。

Result: 在三个NLP任务中，PromptObfus有效防止隐私泄露且保持任务性能。

Conclusion: PromptObfus是一种高效且实用的LLM提示隐私保护方法。

Abstract: With the widespread use of LLMs, preserving privacy in user prompts has
become crucial, as prompts risk exposing privacy and sensitive data to the
cloud LLMs. Traditional techniques like homomorphic encryption, secure
multi-party computation, and federated learning face challenges due to heavy
computational costs and user participation requirements, limiting their
applicability in LLM scenarios. In this paper, we propose PromptObfus, a novel
method for desensitizing LLM prompts. The core idea of PromptObfus is
"anti-adversarial" learning, which perturbs privacy words in the prompt to
obscure sensitive information while retaining the stability of model
predictions. Specifically, PromptObfus frames prompt desensitization as a
masked language modeling task, replacing privacy-sensitive terms with a [MASK]
token. A desensitization model is trained to generate candidate replacements
for each masked position. These candidates are subsequently selected based on
gradient feedback from a surrogate model, ensuring minimal disruption to the
task output. We demonstrate the effectiveness of our approach on three NLP
tasks. Results show that PromptObfus effectively prevents privacy inference
from remote LLMs while preserving task performance.

</details>

### [114] [TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models](https://arxiv.org/abs/2504.20605)
*Mihai Nadas,Laura Diosan,Andrei Piscoran,Andreea Tomescu*

Main category: cs.CL

TLDR: TF1-EN-3M 是一个由指令调优模型生成的300万英语寓言数据集，填补了现代NLP中缺乏结构化道德叙事语料库的空白。


<details>
  <summary>Details</summary>
Motivation: 现代NLP缺乏结合连贯叙事与明确道德教训的大规模结构化语料库，TF1-EN-3M旨在填补这一空白。

Method: 通过组合式提示引擎生成遵循六槽模板的寓言，并使用混合评估流程（GPT评分与无参考指标）评估质量。

Result: 8B参数的Llama-3变体在质量与速度上表现最佳，单GPU即可高效生成高质量寓言。

Conclusion: TF1-EN-3M为指令遵循、叙事智能等研究提供了开源资源，证明大规模道德叙事无需依赖专有巨型模型。

Abstract: Moral stories are a time-tested vehicle for transmitting values, yet modern
NLP lacks a large, structured corpus that couples coherent narratives with
explicit ethical lessons. We close this gap with TF1-EN-3M, the first open
dataset of three million English-language fables generated exclusively by
instruction-tuned models no larger than 8B parameters. Each story follows a
six-slot scaffold (character -> trait -> setting -> conflict -> resolution ->
moral), produced through a combinatorial prompt engine that guarantees genre
fidelity while covering a broad thematic space.
  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores
grammar, creativity, moral clarity, and template adherence with (ii)
reference-free diversity and readability metrics. Among ten open-weight
candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed
trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)
at approximately 13.5 cents per 1,000 fables.
  We release the dataset, generation code, evaluation scripts, and full
metadata under a permissive license, enabling exact reproducibility and cost
benchmarking. TF1-EN-3M opens avenues for research in instruction following,
narrative intelligence, value alignment, and child-friendly educational AI,
demonstrating that large-scale moral storytelling no longer requires
proprietary giant models.

</details>

### [115] [A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types](https://arxiv.org/abs/2505.01311)
*Svenja Kenneweg,Jörg Deigmöller,Julian Eggert,Philipp Cimiano*

Main category: cs.CL

TLDR: 论文提出了一种分解模型，用于捕捉模糊时间副词的语义，并将其与事件特定分布结合，生成上下文意义。模型参数通过现有数据拟合，与非分解模型相比，具有相似的预测能力但更简单且扩展性更好。


<details>
  <summary>Details</summary>
Motivation: 研究模糊时间副词（如“最近”、“刚刚”）的语义，这些副词描述了事件与说话时间的时间距离，但未明确具体时长。

Method: 引入分解模型，将模糊时间副词的语义建模为概率分布，并与事件特定分布结合。模型参数通过现有数据拟合。

Result: 与非分解模型（基于高斯分布）相比，分解模型具有相似的预测能力，但更简单且扩展性更好。

Conclusion: 分解模型在捕捉模糊时间副词语义方面更优，符合奥卡姆剃刀原则。

Abstract: Vague temporal adverbials, such as recently, just, and a long time ago,
describe the temporal distance between a past event and the utterance time but
leave the exact duration underspecified. In this paper, we introduce a
factorized model that captures the semantics of these adverbials as
probabilistic distributions. These distributions are composed with
event-specific distributions to yield a contextualized meaning for an adverbial
applied to a specific event. We fit the model's parameters using existing data
capturing judgments of native speakers regarding the applicability of these
vague temporal adverbials to events that took place a given time ago. Comparing
our approach to a non-factorized model based on a single Gaussian distribution
for each pair of event and temporal adverbial, we find that while both models
have similar predictive power, our model is preferable in terms of Occam's
razor, as it is simpler and has better extendability.

</details>

### [116] [A Transformer-based Neural Architecture Search Method](https://arxiv.org/abs/2505.01314)
*Shang Wang,Huanrong Tang,Jianquan Ouyang*

Main category: cs.CL

TLDR: 提出了一种基于Transformer架构的神经架构搜索方法，通过多目标遗传算法优化网络结构，结合BLEU分数和困惑度作为评估指标，搜索性能更优的翻译模型。


<details>
  <summary>Details</summary>
Motivation: 为了找到性能更好的神经网络结构用于翻译任务，需要结合多种评估指标并优化搜索方法。

Method: 基于Transformer架构，使用多目标遗传算法搜索多头注意力的计算方式，同时结合BLEU分数和困惑度作为评估指标。

Result: 搜索到的神经网络结构优于所有基线模型，且引入困惑度作为辅助评估指标能比仅用BLEU分数找到更好的模型。

Conclusion: 多目标遗传算法结合多种评估指标能有效提升神经架构搜索的性能，适用于翻译任务。

Abstract: This paper presents a neural architecture search method based on Transformer
architecture, searching cross multihead attention computation ways for
different number of encoder and decoder combinations. In order to search for
neural network structures with better translation results, we considered
perplexity as an auxiliary evaluation metric for the algorithm in addition to
BLEU scores and iteratively improved each individual neural network within the
population by a multi-objective genetic algorithm. Experimental results show
that the neural network structures searched by the algorithm outperform all the
baseline models, and that the introduction of the auxiliary evaluation metric
can find better models than considering only the BLEU score as an evaluation
metric.

</details>

### [117] [Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System](https://arxiv.org/abs/2505.01315)
*Sheikh Samit Muhaimin,Spyridon Mastorakis*

Main category: cs.CL

TLDR: 提出了一种无需重新训练或微调LLM的防御框架，通过提示过滤和对抗研究文献总结，有效识别和防御恶意输入。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型易受对抗性攻击和恶意输入的影响，现有防御方法需重新训练模型，成本高且不实用。

Method: 框架包含两部分：(1) 提示过滤模块，利用NLP技术检测恶意输入；(2) 对抗研究文献总结模块，提供上下文防御知识。

Result: 实验显示，该方法识别恶意模式的成功率达98.71%，并显著提升模型的抗攻击能力。

Conclusion: 该框架为LLM提供了一种高效、无需重新训练的防御方案，显著增强了模型的抗恶意利用能力。

Abstract: The recent growth in the use of Large Language Models has made them
vulnerable to sophisticated adversarial assaults, manipulative prompts, and
encoded malicious inputs. Existing countermeasures frequently necessitate
retraining models, which is computationally costly and impracticable for
deployment. Without the need for retraining or fine-tuning, this study presents
a unique defense paradigm that allows LLMs to recognize, filter, and defend
against adversarial or malicious inputs on their own. There are two main parts
to the suggested framework: (1) A prompt filtering module that uses
sophisticated Natural Language Processing (NLP) techniques, including zero-shot
classification, keyword analysis, and encoded content detection (e.g. base64,
hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and
(2) A summarization module that processes and summarizes adversarial research
literature to give the LLM context-aware defense knowledge. This approach
strengthens LLMs' resistance to adversarial exploitation by fusing text
extraction, summarization, and harmful prompt analysis. According to
experimental results, this integrated technique has a 98.71% success rate in
identifying harmful patterns, manipulative language structures, and encoded
prompts. By employing a modest amount of adversarial research literature as
context, the methodology also allows the model to react correctly to harmful
inputs with a larger percentage of jailbreak resistance and refusal rate. While
maintaining the quality of LLM responses, the framework dramatically increases
LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and
easy substitute for time-consuming, retraining-based defenses.

</details>

### [118] [TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References](https://arxiv.org/abs/2505.01325)
*Svenja Kenneweg,Jörg Deigmöller,Philipp Cimiano,Julian Eggert*

Main category: cs.CL

TLDR: TRAVELER是一个新的合成基准数据集，用于评估模型在解决显式、隐式和模糊时间引用方面的能力，发现现有LLM在处理复杂时间引用时表现下降。


<details>
  <summary>Details</summary>
Motivation: 现有基准对时间引用的系统评估有限，TRAVELER旨在填补这一空白。

Method: 通过问答范式构建数据集，评估LLM在解决不同类型时间引用时的表现，包括显式、隐式和模糊引用。

Result: LLM在处理少量事件和显式引用时表现良好，但随着事件数量增加或引用模糊，性能显著下降。

Conclusion: TRAVELER揭示了LLM在时间引用处理上的局限性，为未来研究提供了基准。

Abstract: Understanding and resolving temporal references is essential in Natural
Language Understanding as we often refer to the past or future in daily
communication. Although existing benchmarks address a system's ability to
reason about and resolve temporal references, systematic evaluation of specific
temporal references remains limited. Towards closing this gap, we introduce
TRAVELER, a novel synthetic benchmark dataset that follows a Question Answering
paradigm and consists of questions involving temporal references with the
corresponding correct answers. TRAVELER assesses models' abilities to resolve
explicit, implicit relative to speech time, and vague temporal references.
Beyond investigating the performance of state-of-the-art LLMs depending on the
type of temporal reference, our benchmark also allows evaluation of performance
in relation to the length of the set of events. For the category of vague
temporal references, ground-truth answers were established via human surveys on
Prolific, following a procedure similar to the one from Kenneweg et al. To
demonstrate the benchmark's applicability, we evaluate four state-of-the-art
LLMs using a question-answering task encompassing 3,300 questions. Our findings
show that while the benchmarked LLMs can answer questions over event sets with
a handful of events and explicit temporal references successfully, performance
clearly deteriorates with larger event set length and when temporal references
get less explicit. Notably, the vague question category exhibits the lowest
performance across all models.
  The benchmark is publicly available at:
https://gitlab.ub.uni-bielefeld.de/s.kenneweg/TRAVELER

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [119] [ROSA: A Knowledge-based Solution for Robot Self-Adaptation](https://arxiv.org/abs/2505.00733)
*Gustavo Rezende Silva,Juliane Päßler,S. Lizeth Tapia Tarifa,Einar Broch Johnsen,Carlos Hernández Corbato*

Main category: cs.AI

TLDR: ROSA是一个基于知识的机器人自我适应框架，支持任务和架构协同适应（TACA），通过运行时知识推理实现动态调整，并在水下机器人应用中验证了其可行性和性能。


<details>
  <summary>Details</summary>
Motivation: 自主机器人需在多样环境中处理多任务，但不同情境需要不同的任务逻辑和架构配置，设计适应性强且高效的软件架构和决策算法具有挑战性。

Method: 提出ROSA框架，通过知识模型捕获应用特定知识，并在运行时推理以决定何时及如何适应，同时提供ROS 2开源实现。

Result: 实验证明ROSA在可重用性和开发效率方面具有优势，适用于设计自适应机器人系统。

Conclusion: ROSA为机器人系统的任务和架构协同适应提供了可行解决方案，具有实际应用潜力。

Abstract: Autonomous robots must operate in diverse environments and handle multiple
tasks despite uncertainties. This creates challenges in designing software
architectures and task decision-making algorithms, as different contexts may
require distinct task logic and architectural configurations. To address this,
robotic systems can be designed as self-adaptive systems capable of adapting
their task execution and software architecture at runtime based on their
context.This paper introduces ROSA, a novel knowledge-based framework for RObot
Self-Adaptation, which enables task-and-architecture co-adaptation (TACA) in
robotic systems. ROSA achieves this by providing a knowledge model that
captures all application-specific knowledge required for adaptation and by
reasoning over this knowledge at runtime to determine when and how adaptation
should occur. In addition to a conceptual framework, this work provides an
open-source ROS 2-based reference implementation of ROSA and evaluates its
feasibility and performance in an underwater robotics application. Experimental
results highlight ROSA's advantages in reusability and development effort for
designing self-adaptive robotic systems.

</details>

### [120] [Howard's Policy Iteration is Subexponential for Deterministic Markov Decision Problems with Rewards of Fixed Bit-size and Arbitrary Discount Factor](https://arxiv.org/abs/2505.00795)
*Dibyangshu Mukherjee,Shivaram Kalyanakrishnan*

Main category: cs.AI

TLDR: 本文改进了Howard策略迭代（HPI）在确定性MDP（DMDP）上的运行时间上界，提出了一个次指数级的上界，且与折扣因子无关。


<details>
  <summary>Details</summary>
Motivation: HPI算法在解决MDP问题时，已知的运行时间上界是指数级的，而最紧的下界仅为线性。本文旨在缩小这一差距，改进HPI在DMDP上的运行时间上界。

Method: 通过参数化奖励的比特大小，提出了一种新的分析方法，改进了HPI在DMDP上的运行时间上界。

Result: 得到了一个次指数级的上界，且该上界与折扣因子无关，适用于仅有两个可能奖励的DMDP。

Conclusion: 本文显著改进了HPI在DMDP上的运行时间上界，填补了理论与实际之间的差距。

Abstract: Howard's Policy Iteration (HPI) is a classic algorithm for solving Markov
Decision Problems (MDPs). HPI uses a "greedy" switching rule to update from any
non-optimal policy to a dominating one, iterating until an optimal policy is
found. Despite its introduction over 60 years ago, the best-known upper bounds
on HPI's running time remain exponential in the number of states -- indeed even
on the restricted class of MDPs with only deterministic transitions (DMDPs).
Meanwhile, the tightest lower bound for HPI for MDPs with a constant number of
actions per state is only linear. In this paper, we report a significant
improvement: a subexponential upper bound for HPI on DMDPs, which is
parameterised by the bit-size of the rewards, while independent of the discount
factor. The same upper bound also applies to DMDPs with only two possible
rewards (which may be of arbitrary size).

</details>

### [121] [Explanations as Bias Detectors: A Critical Study of Local Post-hoc XAI Methods for Fairness Exploration](https://arxiv.org/abs/2505.00802)
*Vasiliki Papanikou,Danae Pla Karidi,Evaggelia Pitoura,Emmanouil Panagiotou,Eirini Ntoutsi*

Main category: cs.AI

TLDR: 论文探讨了如何利用可解释性方法检测和解释AI系统中的不公平性，提出了一种结合局部事后解释方法的流程，并分析了其关键问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI在影响人类生活的领域广泛应用，公平性和透明性问题日益突出，尤其是对受保护群体的影响。可解释性与公平性的交叉成为促进负责任AI系统的重要方向。

Method: 提出了一种集成局部事后解释方法的流程，用于提取与公平性相关的见解，并解决了使用解释作为偏见检测器时的关键问题。

Result: 结果显示可解释性方法在公平性方面的潜力，但也强调了需要仔细考虑解释方法的一致性、质量及其对群体公平性评估的影响。

Conclusion: 可解释性方法可用于检测不公平性，但其效果和可信度需结合关键问题（如公平性类型、保护属性移除的影响等）进行综合评估。

Abstract: As Artificial Intelligence (AI) is increasingly used in areas that
significantly impact human lives, concerns about fairness and transparency have
grown, especially regarding their impact on protected groups. Recently, the
intersection of explainability and fairness has emerged as an important area to
promote responsible AI systems. This paper explores how explainability methods
can be leveraged to detect and interpret unfairness. We propose a pipeline that
integrates local post-hoc explanation methods to derive fairness-related
insights. During the pipeline design, we identify and address critical
questions arising from the use of explanations as bias detectors such as the
relationship between distributive and procedural fairness, the effect of
removing the protected attribute, the consistency and quality of results across
different explanation methods, the impact of various aggregation strategies of
local explanations on group fairness evaluations, and the overall
trustworthiness of explanations as bias detectors. Our results show the
potential of explanation methods used for fairness while highlighting the need
to carefully consider the aforementioned critical aspects.

</details>

### [122] [MIMIC-\RNum{4}-Ext-22MCTS: A 22 Millions-Event Temporal Clinical Time-Series Dataset with Relative Timestamp for Risk Prediction](https://arxiv.org/abs/2505.00827)
*Jing Wang,Xing Niu,Juyong Kim,Jie Shen,Tong Zhang,Jeremy C. Weiss*

Main category: cs.AI

TLDR: 论文提出了一个基于MIMIC-IV-Note数据集的临床时间序列事件数据集MIMIC-4-Ext-22MCTS，并开发了一个新框架来提取和处理这些数据，显著提升了医疗应用的模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代医疗中，基于机器学习的临床风险预测依赖于高质量的时间序列临床事件数据。然而，现有的MIMIC-IV-Note数据集存在文本过长和时间信息缺失的问题，需要新的方法来解决。

Method: 1) 将出院摘要分割为小块文本；2) 使用上下文BM25和语义搜索筛选包含临床事件的文本块；3) 设计提示词指导Llama-3.1-8B模型识别或推断时间信息。

Result: 标准模型（如BERT和GPT-2）在微调后显著提升了性能：BERT在医疗问答任务中准确率提升10%，在临床试验匹配任务中提升3%；GPT-2生成的结果更具临床可靠性。

Conclusion: 提出的数据集和框架为临床时间序列事件的高效提取和处理提供了可行方案，显著提升了医疗应用的模型性能。

Abstract: Clinical risk prediction based on machine learning algorithms plays a vital
role in modern healthcare. A crucial component in developing a reliable
prediction model is collecting high-quality time series clinical events. In
this work, we release such a dataset that consists of 22,588,586 Clinical Time
Series events, which we term MIMIC-\RNum{4}-Ext-22MCTS. Our source data are
discharge summaries selected from the well-known yet unstructured MIMIC-IV-Note
\cite{Johnson2023-pg}. We then extract clinical events as short text span from
the discharge summaries, along with the timestamps of these events as temporal
information. The general-purpose MIMIC-IV-Note pose specific challenges for our
work: it turns out that the discharge summaries are too lengthy for typical
natural language models to process, and the clinical events of interest often
are not accompanied with explicit timestamps. Therefore, we propose a new
framework that works as follows: 1) we break each discharge summary into
manageably small text chunks; 2) we apply contextual BM25 and contextual
semantic search to retrieve chunks that have a high potential of containing
clinical events; and 3) we carefully design prompts to teach the recently
released Llama-3.1-8B \cite{touvron2023llama} model to identify or infer
temporal information of the chunks. We show that the obtained dataset is so
informative and transparent that standard models fine-tuned on our dataset are
achieving significant improvements in healthcare applications. In particular,
the BERT model fine-tuned based on our dataset achieves 10\% improvement in
accuracy on medical question answering task, and 3\% improvement in clinical
trial matching task compared with the classic BERT. The GPT-2 model, fine-tuned
on our dataset, produces more clinically reliable results for clinical
questions.

</details>

### [123] [Thoughts without Thinking: Reconsidering the Explanatory Value of Chain-of-Thought Reasoning in LLMs through Agentic Pipelines](https://arxiv.org/abs/2505.00875)
*Ramesh Manuvinakurike,Emanuel Moss,Elizabeth Anne Watkins,Saurav Sahay,Giuseppe Raffa,Lama Nachman*

Main category: cs.AI

TLDR: 研究发现，在多LLM协作的Agentic管道中，Chain-of-Thought推理并不能提升输出质量或提供可操作性解释。


<details>
  <summary>Details</summary>
Motivation: 探索如何使LLM的内部运作在Agentic管道中对用户透明且可操作。

Method: 通过定量和定性分析，研究Chain-of-Thought推理在Agentic管道中的表现。

Result: CoT推理未能提升输出质量，其解释也无法帮助用户更好地理解系统或实现目标。

Conclusion: CoT推理在Agentic管道中无法满足可解释性需求，需进一步研究其他方法。

Abstract: Agentic pipelines present novel challenges and opportunities for
human-centered explainability. The HCXAI community is still grappling with how
best to make the inner workings of LLMs transparent in actionable ways. Agentic
pipelines consist of multiple LLMs working in cooperation with minimal human
control. In this research paper, we present early findings from an agentic
pipeline implementation of a perceptive task guidance system. Through
quantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT)
reasoning, a common vehicle for explainability in LLMs, operates within agentic
pipelines. We demonstrate that CoT reasoning alone does not lead to better
outputs, nor does it offer explainability, as it tends to produce explanations
without explainability, in that they do not improve the ability of end users to
better understand systems or achieve their goals.

</details>

### [124] [Car Sensors Health Monitoring by Verification Based on Autoencoder and Random Forest Regression](https://arxiv.org/abs/2505.00876)
*Sahar Torkhesari,Behnam Yousefimehr,Mehdi Ghatee*

Main category: cs.AI

TLDR: 论文提出了一种创新的传感器健康监测系统，利用机器学习和深度学习技术分析车辆传感器数据，通过比较实际值与估计值检测故障，准确率达99%。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够实时监测车辆传感器健康状况的系统，以提前发现故障并确保驾驶安全。

Method: 结合自编码器检测传感器故障和随机森林回归估计传感器值，并利用正态分布模型识别潜在故障。

Result: 在Saipa's Quick车辆的20个关键传感器上测试，系统准确率达到99%。

Conclusion: 该系统能有效监测传感器健康状态，提前发现故障并替换故障值，具有高准确性和实用性。

Abstract: Driver assistance systems provide a wide range of crucial services, including
closely monitoring the condition of vehicles. This paper showcases a
groundbreaking sensor health monitoring system designed for the automotive
industry. The ingenious system leverages cutting-edge techniques to process
data collected from various vehicle sensors. It compares their outputs within
the Electronic Control Unit (ECU) to evaluate the health of each sensor. To
unravel the intricate correlations between sensor data, an extensive
exploration of machine learning and deep learning methodologies was conducted.
Through meticulous analysis, the most correlated sensor data were identified.
These valuable insights were then utilized to provide accurate estimations of
sensor values. Among the diverse learning methods examined, the combination of
autoencoders for detecting sensor failures and random forest regression for
estimating sensor values proved to yield the most impressive outcomes. A
statistical model using the normal distribution has been developed to identify
possible sensor failures proactively. By comparing the actual values of the
sensors with their estimated values based on correlated sensors, faulty sensors
can be detected early. When a defective sensor is detected, both the driver and
the maintenance department are promptly alerted. Additionally, the system
replaces the value of the faulty sensor with the estimated value obtained
through analysis. This proactive approach was evaluated using data from twenty
essential sensors in the Saipa's Quick vehicle's ECU, resulting in an
impressive accuracy rate of 99\%.

</details>

### [125] [Seeking to Collide: Online Safety-Critical Scenario Generation for Autonomous Driving with Retrieval Augmented Large Language Models](https://arxiv.org/abs/2505.00972)
*Yuewen Mei,Tong Nie,Jian Sun,Ye Tian*

Main category: cs.AI

TLDR: 提出了一种基于检索增强的大型语言模型框架，用于生成安全关键的驾驶场景，显著提升了碰撞检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有场景生成方法要么过度拟合常见驾驶模式，要么以离线方式运行，无法暴露罕见但安全关键的极端情况。

Method: 使用LLM行为分析器推断背景车辆的最危险意图，并通过查询其他LLM代理合成可行的对抗轨迹，同时通过动态记忆和检索库增强框架。

Result: 在Waymo Open Motion Dataset上，模型将平均最小碰撞时间从1.62秒降至1.08秒，碰撞率提高了75%。

Conclusion: 该方法显著优于基线，能够更有效地生成安全关键的驾驶场景。

Abstract: Simulation-based testing is crucial for validating autonomous vehicles (AVs),
yet existing scenario generation methods either overfit to common driving
patterns or operate in an offline, non-interactive manner that fails to expose
rare, safety-critical corner cases. In this paper, we introduce an online,
retrieval-augmented large language model (LLM) framework for generating
safety-critical driving scenarios. Our method first employs an LLM-based
behavior analyzer to infer the most dangerous intent of the background vehicle
from the observed state, then queries additional LLM agents to synthesize
feasible adversarial trajectories. To mitigate catastrophic forgetting and
accelerate adaptation, we augment the framework with a dynamic memorization and
retrieval bank of intent-planner pairs, automatically expanding its behavioral
library when novel intents arise. Evaluations using the Waymo Open Motion
Dataset demonstrate that our model reduces the mean minimum time-to-collision
from 1.62 to 1.08 s and incurs a 75% collision rate, substantially
outperforming baselines.

</details>

### [126] [Improving Large Language Model Planning with Action Sequence Similarity](https://arxiv.org/abs/2505.01009)
*Xinran Zhao,Hanie Sedghi,Bernd Bohnet,Dale Schuurmans,Azade Nova*

Main category: cs.AI

TLDR: 论文提出GRASE-DC方法，通过动作序列相似性（AS）筛选示例，提升语言模型的规划能力，实验显示显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过上下文学习（ICL）提升语言模型的规划能力，特别是如何选择示例信号。

Method: 提出GRASE-DC两阶段流程：先重采样高AS示例，再通过动态聚类平衡相关性与多样性。

Result: GRASE-DC在多种规划任务中显著提升性能（最高提升40点准确率），且示例需求减少27.3%。结合验证器（GRASE-DC* + VAL）可额外提升18.9%。

Conclusion: GRASE-DC能有效提升语言模型的规划能力，尤其在分布外问题上表现优异，展示了其泛化能力。

Abstract: Planning is essential for artificial intelligence systems to look ahead and
proactively determine a course of actions to reach objectives in the virtual
and real world. Recent work on large language models (LLMs) sheds light on
their planning capability in various tasks. However, it remains unclear what
signals in the context influence the model performance. In this work, we
explore how to improve the model planning capability through in-context
learning (ICL), specifically, what signals can help select the exemplars.
Through extensive experiments, we observe that commonly used problem similarity
may result in false positives with drastically different plans, which can
mislead the model. In response, we propose to sample and filter exemplars
leveraging plan side action sequence similarity (AS). We propose GRASE-DC: a
two-stage pipeline that first re-samples high AS exemplars and then curates the
selected exemplars with dynamic clustering on AS to achieve a balance of
relevance and diversity. Our experimental result confirms that GRASE-DC
achieves significant performance improvement on various planning tasks (up to
~11-40 point absolute accuracy improvement with 27.3% fewer exemplars needed on
average). With GRASE-DC* + VAL, where we iteratively apply GRASE-DC with a
validator, we are able to even boost the performance by 18.9% more.
  Extensive analysis validates the consistent performance improvement of
GRASE-DC with various backbone LLMs and on both classical planning and natural
language planning benchmarks. GRASE-DC can further boost the planning accuracy
by ~24 absolute points on harder problems using simpler problems as exemplars
over a random baseline. This demonstrates its ability to generalize to
out-of-distribution problems.

</details>

### [127] [Adaptive Wizard for Removing Cross-Tier Misconfigurations in Active Directory](https://arxiv.org/abs/2505.01028)
*Huy Q. Ngo,Mingyu Guo,Hung Nguyen*

Main category: cs.AI

TLDR: 论文提出了一种优化Windows AD系统安全漏洞修复流程的模型，通过最小化IT管理员与安全向导的交互次数来减少人工成本。


<details>
  <summary>Details</summary>
Motivation: 现有的AD系统安全漏洞修复流程需要大量人工验证，效率低下。

Method: 提出Adaptive Path Removal Problem模型，设计精确算法、近似算法和启发式算法（如DPR）。

Result: DPR算法在大规模图上表现优于精确算法和近似算法。

Conclusion: 该模型和算法能有效减少人工干预，提升AD系统安全加固效率。

Abstract: Security vulnerabilities in Windows Active Directory (AD) systems are
typically modeled using an attack graph and hardening AD systems involves an
iterative workflow: security teams propose an edge to remove, and IT operations
teams manually review these fixes before implementing the removal. As
verification requires significant manual effort, we formulate an Adaptive Path
Removal Problem to minimize the number of steps in this iterative removal
process. In our model, a wizard proposes an attack path in each step and
presents it as a set of multiple-choice options to the IT admin. The IT admin
then selects one edge from the proposed set to remove. This process continues
until the target $t$ is disconnected from source $s$ or the number of proposed
paths reaches $B$. The model aims to optimize the human effort by minimizing
the expected number of interactions between the IT admin and the security
wizard. We first prove that the problem is $\mathcal{\#P}$-hard. We then
propose a set of solutions including an exact algorithm, an approximate
algorithm, and several scalable heuristics. Our best heuristic, called DPR, can
operate effectively on larger-scale graphs compared to the exact algorithm and
consistently outperforms the approximate algorithm across all graphs. We verify
the effectiveness of our algorithms on several synthetic AD graphs and an AD
attack graph collected from a real organization.

</details>

### [128] [Retrieval Augmented Learning: A Retrial-based Large Language Model Self-Supervised Learning and Autonomous Knowledge Generation](https://arxiv.org/abs/2505.01073)
*Zongyuan Li,Pengfei Li,Runnan Qi,Yanan Ni,Lumin Jiang,Hui Wu,Xuebo Zhang,Kuihua Huang,Xian Guo*

Main category: cs.AI

TLDR: 本文提出了一种无需训练的检索增强学习（RAL）框架，通过自主知识生成三阶段（假设提出、验证和知识生成）减少幻觉并提升决策性能。


<details>
  <summary>Details</summary>
Motivation: 领域特定数据不足限制了LLM在专业应用中的表现，而传统后训练方法计算成本高。

Method: 将检索增强生成（RAG）模块化，实现自主知识生成三阶段，无需模型训练。

Result: 在LLM-PySC2环境中验证，有效减少幻觉并提升决策性能，成本极低。

Conclusion: RAL是一种低成本高效的决策和自主知识生成解决方案，适用于OOD任务且具有鲁棒性和可迁移性。

Abstract: The lack of domain-specific data in the pre-training of Large Language Models
(LLMs) severely limits LLM-based decision systems in specialized applications,
while post-training a model in the scenarios requires significant computational
resources. In this paper, we present Retrial-Augmented Learning (RAL), a
reward-free self-supervised learning framework for LLMs that operates without
model training. By developing Retrieval-Augmented Generation (RAG) into a
module for organizing intermediate data, we realized a three-stage autonomous
knowledge generation of proposing a hypothesis, validating the hypothesis, and
generating the knowledge. The method is evaluated in the LLM-PySC2 environment,
a representative decision-making platform that combines sufficient complexity
with domain-specific knowledge requirements. Experiments demonstrate that the
proposed method effectively reduces hallucination by generating and utilizing
validated knowledge, and increases decision-making performance at an extremely
low cost. Meanwhile, the approach exhibits potential in
out-of-distribution(OOD) tasks, robustness, and transferability, making it a
cost-friendly but effective solution for decision-making problems and
autonomous knowledge generation.

</details>

### [129] [MADIL: An MDL-based Framework for Efficient Program Synthesis in the ARC Benchmark](https://arxiv.org/abs/2505.01081)
*Sébastien Ferré*

Main category: cs.AI

TLDR: MADIL（基于MDL的AI）是一种利用最小描述长度（MDL）原理进行高效归纳学习的新方法，在ARC基准测试中表现虽不及大型语言模型（LLMs），但更高效且可解释。


<details>
  <summary>Details</summary>
Motivation: AI在专业任务中表现出色，但在技能获取和泛化方面效率不足。ARC基准测试评估智能时要求最小化训练需求，而现有LLMs方法依赖大量预训练和高计算成本。

Method: MADIL通过基于模式的分解实现结构化泛化，利用MDL原理进行高效归纳学习。

Result: MADIL在ArcPrize 2024中的表现（7%）低于LLM方法，但具有更高的效率和可解释性。

Conclusion: MADIL为高效且可解释的AI学习提供了一种新途径，尽管性能尚需提升。

Abstract: Artificial Intelligence (AI) has achieved remarkable success in specialized
tasks but struggles with efficient skill acquisition and generalization. The
Abstraction and Reasoning Corpus (ARC) benchmark evaluates intelligence based
on minimal training requirements. While Large Language Models (LLMs) have
recently improved ARC performance, they rely on extensive pre-training and high
computational costs. We introduce MADIL (MDL-based AI), a novel approach
leveraging the Minimum Description Length (MDL) principle for efficient
inductive learning. MADIL performs pattern-based decomposition, enabling
structured generalization. While its performance (7% at ArcPrize 2024) remains
below LLM-based methods, it offers greater efficiency and interpretability.
This paper details MADIL's methodology, its application to ARC, and
experimental evaluations.

</details>

### [130] [Explainable AI Based Diagnosis of Poisoning Attacks in Evolutionary Swarms](https://arxiv.org/abs/2505.01181)
*Mehrdad Asadi,Roxana Rădulescu,Ann Nowé*

Main category: cs.AI

TLDR: 论文研究了多无人机网络中数据投毒攻击对团队协作策略的影响，并提出了一种基于可解释AI的框架来诊断和量化攻击效果。


<details>
  <summary>Details</summary>
Motivation: 团队级协调策略在野外环境中易受数据投毒攻击，导致协作不准确或敌对行为，因此需要研究其影响并找到诊断方法。

Method: 使用进化智能建模代理间交互，并通过数据操纵攻击系统地毒害模型，利用可解释AI方法量化攻击效果。

Result: 当模型被毒害超过10%时，会导致非最优策略和低效协作。

Conclusion: 可解释AI方法能有效诊断数据投毒攻击的影响，为团队协作策略的鲁棒性提供支持。

Abstract: Swarming systems, such as for example multi-drone networks, excel at
cooperative tasks like monitoring, surveillance, or disaster assistance in
critical environments, where autonomous agents make decentralized decisions in
order to fulfill team-level objectives in a robust and efficient manner.
Unfortunately, team-level coordinated strategies in the wild are vulnerable to
data poisoning attacks, resulting in either inaccurate coordination or
adversarial behavior among the agents. To address this challenge, we contribute
a framework that investigates the effects of such data poisoning attacks, using
explainable AI methods. We model the interaction among agents using
evolutionary intelligence, where an optimal coalition strategically emerges to
perform coordinated tasks. Then, through a rigorous evaluation, the swarm model
is systematically poisoned using data manipulation attacks. We showcase the
applicability of explainable AI methods to quantify the effects of poisoning on
the team strategy and extract footprint characterizations that enable
diagnosing. Our findings indicate that when the model is poisoned above 10%,
non-optimal strategies resulting in inefficient cooperation can be identified.

</details>

### [131] [Exploring the Impact of Explainable AI and Cognitive Capabilities on Users' Decisions](https://arxiv.org/abs/2505.01192)
*Federico Maria Cau,Lucio Davide Spano*

Main category: cs.AI

TLDR: 研究探讨了不同解释风格（如基于示例、特征、规则和反事实）对AI决策的影响，发现高AI置信度增加依赖并降低认知负荷，反事实解释虽难理解但提升准确性。个性特质（如NFC）对结果无显著影响。


<details>
  <summary>Details</summary>
Motivation: 探讨AI决策中不同解释风格和个性特质（如NFC）对用户依赖、准确性和认知负荷的影响，以优化人机协作。

Method: 在贷款申请场景中，测试不同AI信息（预测、置信度、准确性）和解释风格对用户行为的影响，并比较高低NFC个体的差异。

Result: 高AI置信度增加依赖并降低认知负荷；反事实解释提升准确性但难理解；个性特质无显著影响。

Conclusion: 需用户为中心的个性化XAI界面，结合多样解释风格并探索更多用户特征以优化人机协作。

Abstract: Artificial Intelligence (AI) systems are increasingly used for
decision-making across domains, raising debates over the information and
explanations they should provide. Most research on Explainable AI (XAI) has
focused on feature-based explanations, with less attention on alternative
styles. Personality traits like the Need for Cognition (NFC) can also lead to
different decision-making outcomes among low and high NFC individuals. We
investigated how presenting AI information (prediction, confidence, and
accuracy) and different explanation styles (example-based, feature-based,
rule-based, and counterfactual) affect accuracy, reliance on AI, and cognitive
load in a loan application scenario. We also examined low and high NFC
individuals' differences in prioritizing XAI interface elements (loan
attributes, AI information, and explanations), accuracy, and cognitive load.
Our findings show that high AI confidence significantly increases reliance on
AI while reducing cognitive load. Feature-based explanations did not enhance
accuracy compared to other conditions. Although counterfactual explanations
were less understandable, they enhanced overall accuracy, increasing reliance
on AI and reducing cognitive load when AI predictions were correct. Both low
and high NFC individuals prioritized explanations after loan attributes,
leaving AI information as the least important. However, we found no significant
differences between low and high NFC groups in accuracy or cognitive load,
raising questions about the role of personality traits in AI-assisted
decision-making. These findings highlight the need for user-centric
personalization in XAI interfaces, incorporating diverse explanation styles and
exploring multiple personality traits and other user characteristics to
optimize human-AI collaboration.

</details>

### [132] [Early Detection of Patient Deterioration from Real-Time Wearable Monitoring System](https://arxiv.org/abs/2505.01305)
*Lo Pang-Yun Ting,Hong-Pei Chen,An-Shan Liu,Chun-Yin Yeh,Po-Lin Chen,Kun-Ta Chuang*

Main category: cs.AI

TLDR: TARL是一种通过建模心率时间序列中的代表性子序列（shapelets）结构关系的新方法，用于早期检测患者病情恶化。


<details>
  <summary>Details</summary>
Motivation: 早期检测患者病情恶化对降低死亡率至关重要，但心率数据的多样性和缺失值处理是主要挑战。

Method: TARL通过构建shapelet-transition知识图谱建模心率动态，并引入transition-aware知识嵌入强化关系，量化缺失值影响。

Result: 在真实ICU数据上的实验表明，TARL具有高可靠性和早期检测能力。

Conclusion: TARL作为一种AI工具，可帮助临床医生识别患者早期恶化迹象，具有解释性检测潜力。

Abstract: Early detection of patient deterioration is crucial for reducing mortality
rates. Heart rate data has shown promise in assessing patient health, and
wearable devices offer a cost-effective solution for real-time monitoring.
However, extracting meaningful insights from diverse heart rate data and
handling missing values in wearable device data remain key challenges. To
address these challenges, we propose TARL, an innovative approach that models
the structural relationships of representative subsequences, known as
shapelets, in heart rate time series. TARL creates a shapelet-transition
knowledge graph to model shapelet dynamics in heart rate time series,
indicating illness progression and potential future changes. We further
introduce a transition-aware knowledge embedding to reinforce relationships
among shapelets and quantify the impact of missing values, enabling the
formulation of comprehensive heart rate representations. These representations
capture explanatory structures and predict future heart rate trends, aiding
early illness detection. We collaborate with physicians and nurses to gather
ICU patient heart rate data from wearables and diagnostic metrics assessing
illness severity for evaluating deterioration. Experiments on real-world ICU
data demonstrate that TARL achieves both high reliability and early detection.
A case study further showcases TARL's explainable detection process,
highlighting its potential as an AI-driven tool to assist clinicians in
recognizing early signs of patient deterioration.

</details>

### [133] [BalancEdit: Dynamically Balancing the Generality-Locality Trade-off in Multi-modal Model Editing](https://arxiv.org/abs/2505.01343)
*Dongliang Guo,Mengxuan Hu,Zihan Guan,Thomas Hartvigsen,Sheng Li*

Main category: cs.AI

TLDR: 论文提出了一种名为BalancEdit的新方法，用于解决多模态模型编辑中的泛化性与局部性权衡问题，并通过OKEDIT数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型随时间推移会因信息过时而性能下降，传统微调方法不适用，现有编辑技术忽视了不同事实的影响范围，导致性能下降。

Method: 提出BalancEdit方法，动态平衡泛化性与局部性，通过生成正负样本确定影响范围，并使用离散编辑码本修改潜在空间。

Result: BalancEdit在保持强大编辑能力的同时，实现了最小的性能权衡。

Conclusion: BalancEdit是首个明确解决多模态模型编辑中泛化性与局部性权衡的方法，效果显著。

Abstract: Large multi-modal models inevitably decay over time as facts change and
previously learned information becomes outdated. Traditional approaches such as
fine-tuning are often impractical for updating these models due to their size
and complexity. Instead, direct knowledge editing within the models presents a
more viable solution. Current model editing techniques, however, typically
overlook the unique influence ranges of different facts, leading to compromised
model performance in terms of both generality and locality. To address this
issue, we introduce the concept of the generality-locality trade-off in
multi-modal model editing. We develop a new model editing dataset named OKEDIT,
specifically designed to effectively evaluate this trade-off. Building on this
foundation, we propose BalancEdit, a novel method for balanced model editing
that dynamically achieves an optimal balance between generality and locality.
BalancEdit utilizes a unique mechanism that generates both positive and
negative samples for each fact to accurately determine its influence scope and
incorporates these insights into the model's latent space using a discrete,
localized codebook of edits, without modifying the underlying model weights. To
our knowledge, this is the first approach explicitly addressing the
generality-locality trade-off in multi-modal model editing. Our comprehensive
results confirm the effectiveness of BalancEdit, demonstrating minimal
trade-offs while maintaining robust editing capabilities. Our code and dataset
will be available.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [134] [Unconstrained Large-scale 3D Reconstruction and Rendering across Altitudes](https://arxiv.org/abs/2505.00734)
*Neil Joshi,Joshua Carney,Nathanael Kuo,Homer Li,Cheng Peng,Myron Brown*

Main category: cs.CV

TLDR: 论文提出了一个公开基准数据集，用于解决3D重建和新视角合成中的现实挑战，如图像数量有限、相机异构等问题。


<details>
  <summary>Details</summary>
Motivation: 为灾难救援和执法等场景提供更高效的3D建模方法，解决现有数据不足和复杂环境下的技术难题。

Method: 开发了一个基于多类型校准相机的数据集，并评估了未校准相机的标定和新视角渲染质量。

Result: 展示了基线性能，并指出了进一步研究的挑战。

Conclusion: 该数据集为3D重建和新视角合成研究提供了重要资源，推动了相关技术的发展。

Abstract: Production of photorealistic, navigable 3D site models requires a large
volume of carefully collected images that are often unavailable to first
responders for disaster relief or law enforcement. Real-world challenges
include limited numbers of images, heterogeneous unposed cameras, inconsistent
lighting, and extreme viewpoint differences for images collected from varying
altitudes. To promote research aimed at addressing these challenges, we have
developed the first public benchmark dataset for 3D reconstruction and novel
view synthesis based on multiple calibrated ground-level, security-level, and
airborne cameras. We present datasets that pose real-world challenges,
independently evaluate calibration of unposed cameras and quality of novel
rendered views, demonstrate baseline performance using recent state-of-practice
methods, and identify challenges for further research.

</details>

### [135] [MoSAM: Motion-Guided Segment Anything Model with Spatial-Temporal Memory Selection](https://arxiv.org/abs/2505.00739)
*Qiushi Yang,Yuan Yao,Miaomiao Cui,Liefeng Bo*

Main category: cs.CV

TLDR: MoSAM通过引入运动引导提示和时空记忆选择机制，解决了SAM2在视频对象分割中的长程跟踪和记忆可靠性问题，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: SAM2在视频对象分割中依赖固定帧的记忆，缺乏对运动信息的利用，导致长程跟踪能力受限和记忆可靠性问题。

Method: 提出MoSAM，结合运动引导提示（MGP）和时空记忆选择（ST-MS）机制，整合运动信息并优化记忆特征。

Result: 在多个视频对象分割和实例分割基准测试中，MoSAM取得了最先进的性能。

Conclusion: MoSAM通过运动信息和动态记忆优化，显著提升了视频对象分割的准确性和鲁棒性。

Abstract: The recent Segment Anything Model 2 (SAM2) has demonstrated exceptional
capabilities in interactive object segmentation for both images and videos.
However, as a foundational model on interactive segmentation, SAM2 performs
segmentation directly based on mask memory from the past six frames, leading to
two significant challenges. Firstly, during inference in videos, objects may
disappear since SAM2 relies solely on memory without accounting for object
motion information, which limits its long-range object tracking capabilities.
Secondly, its memory is constructed from fixed past frames, making it
susceptible to challenges associated with object disappearance or occlusion,
due to potentially inaccurate segmentation results in memory. To address these
problems, we present MoSAM, incorporating two key strategies to integrate
object motion cues into the model and establish more reliable feature memory.
Firstly, we propose Motion-Guided Prompting (MGP), which represents the object
motion in both sparse and dense manners, then injects them into SAM2 through a
set of motion-guided prompts. MGP enables the model to adjust its focus towards
the direction of motion, thereby enhancing the object tracking capabilities.
Furthermore, acknowledging that past segmentation results may be inaccurate, we
devise a Spatial-Temporal Memory Selection (ST-MS) mechanism that dynamically
identifies frames likely to contain accurate segmentation in both pixel- and
frame-level. By eliminating potentially inaccurate mask predictions from
memory, we can leverage more reliable memory features to exploit similar
regions for improving segmentation results. Extensive experiments on various
benchmarks of video object segmentation and video instance segmentation
demonstrate that our MoSAM achieves state-of-the-art results compared to other
competitors.

</details>

### [136] [Fast2comm:Collaborative perception combined with prior knowledge](https://arxiv.org/abs/2505.00740)
*Zhengbin Zhang,Yan Wu,Hongkun Zhang*

Main category: cs.CV

TLDR: Fast2comm是一个基于先验知识的协作感知框架，通过生成高区分度的置信度特征和优化带宽效率，解决了协作感知中的性能与带宽平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的协作感知面临感知性能与带宽限制的平衡问题以及定位误差的挑战。

Method: 提出先验监督的置信度特征生成方法、基于GT边界框的空间先验特征选择策略，以及训练与测试阶段的特征融合解耦。

Result: 在真实和模拟数据集上的实验验证了模型的优越性能。

Conclusion: Fast2comm框架有效提升了协作感知的准确性和适应性。

Abstract: Collaborative perception has the potential to significantly enhance
perceptual accuracy through the sharing of complementary information among
agents. However, real-world collaborative perception faces persistent
challenges, particularly in balancing perception performance and bandwidth
limitations, as well as coping with localization errors. To address these
challenges, we propose Fast2comm, a prior knowledge-based collaborative
perception framework. Specifically, (1)we propose a prior-supervised confidence
feature generation method, that effectively distinguishes foreground from
background by producing highly discriminative confidence features; (2)we
propose GT Bounding Box-based spatial prior feature selection strategy to
ensure that only the most informative prior-knowledge features are selected and
shared, thereby minimizing background noise and optimizing bandwidth efficiency
while enhancing adaptability to localization inaccuracies; (3)we decouple the
feature fusion strategies between model training and testing phases, enabling
dynamic bandwidth adaptation. To comprehensively validate our framework, we
conduct extensive experiments on both real-world and simulated datasets. The
results demonstrate the superior performance of our model and highlight the
necessity of the proposed methods. Our code is available at
https://github.com/Zhangzhengbin-TJ/Fast2comm.

</details>

### [137] [Detection and Classification of Diseases in Multi-Crop Leaves using LSTM and CNN Models](https://arxiv.org/abs/2505.00741)
*Srinivas Kanakala,Sneha Ningappa*

Main category: cs.CV

TLDR: 该研究利用CNN和LSTM模型对植物叶片疾病进行分类，CNN模型表现最佳，验证准确率达96.4%，为农业监测提供了有效解决方案。


<details>
  <summary>Details</summary>
Motivation: 植物疾病严重影响农作物产量和品质，早期检测和分类对减少损失和改善作物管理至关重要。

Method: 使用CNN和LSTM模型，基于包含70,295张训练图像和17,572张验证图像的数据集，对38种疾病进行分类。CNN采用Adam优化器和分类交叉熵损失函数。

Result: CNN模型训练准确率为99.1%，验证准确率为96.4%；LSTM模型验证准确率为93.43%。性能通过精确率、召回率、F1分数和混淆矩阵评估。

Conclusion: 深度学习模型（尤其是CNN）为植物疾病分类提供了准确且可扩展的解决方案，适用于农业监测实践。

Abstract: Plant diseases pose a serious challenge to agriculture by reducing crop yield
and affecting food quality. Early detection and classification of these
diseases are essential for minimising losses and improving crop management
practices. This study applies Convolutional Neural Networks (CNN) and Long
Short-Term Memory (LSTM) models to classify plant leaf diseases using a dataset
containing 70,295 training images and 17,572 validation images across 38
disease classes. The CNN model was trained using the Adam optimiser with a
learning rate of 0.0001 and categorical cross-entropy as the loss function.
After 10 training epochs, the model achieved a training accuracy of 99.1% and a
validation accuracy of 96.4%. The LSTM model reached a validation accuracy of
93.43%. Performance was evaluated using precision, recall, F1-score, and
confusion matrix, confirming the reliability of the CNN-based approach. The
results suggest that deep learning models, particularly CNN, enable an
effective solution for accurate and scalable plant disease classification,
supporting practical applications in agricultural monitoring.

</details>

### [138] [Zoomer: Adaptive Image Focus Optimization for Black-box MLLM](https://arxiv.org/abs/2505.00742)
*Jiaxu Qian,Chendong Wang,Yifan Yang,Chaoyun Zhang,Huiqiang Jiang,Xufang Luo,Yu Kang,Qingwei Lin,Anlan Zhang,Shiqi Jiang,Ting Cao,Tianjun Mao,Suman Banerjee,Guyue Liu,Saravan Rajmohan,Dongmei Zhang,Yuqing Yang,Qi Zhang,Lili Qiu*

Main category: cs.CV

TLDR: 论文提出了一种名为\SysName的新型视觉提示机制，旨在提升多模态大语言模型（MLLM）的性能，同时保留关键视觉细节。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在处理视觉数据时存在精确对象识别和细节保留的不足，且严格的token限制导致关键信息丢失。

Method: \SysName包含三个创新点：动态突出相关图像区域的提示感知策略、保持对象完整性的空间保留编排方案，以及平衡全局上下文与关键细节的预算感知提示方法。

Result: 在多个数据集上的评估显示，\SysName显著优于基线方法，准确率提升高达26.9%，同时大幅减少token消耗。

Conclusion: \SysName有效解决了MLLM在视觉任务中的局限性，为提升模型性能提供了可行方案。

Abstract: Recent advancements in multimodal large language models (MLLMs) have
broadened the scope of vision-language tasks, excelling in applications like
image captioning and interactive question-answering. However, these models
struggle with accurately processing visual data, particularly in tasks
requiring precise object recognition and fine visual details. Stringent token
limits often result in the omission of critical information, hampering
performance. To address these limitations, we introduce \SysName, a novel
visual prompting mechanism designed to enhance MLLM performance while
preserving essential visual details within token limits. \SysName features
three key innovations: a prompt-aware strategy that dynamically highlights
relevant image regions, a spatial-preserving orchestration schema that
maintains object integrity, and a budget-aware prompting method that balances
global context with crucial visual details. Comprehensive evaluations across
multiple datasets demonstrate that \SysName consistently outperforms baseline
methods, achieving up to a $26.9\%$ improvement in accuracy while significantly
reducing token consumption.

</details>

### [139] [DOPE: Dual Object Perception-Enhancement Network for Vision-and-Language Navigation](https://arxiv.org/abs/2505.00743)
*Yinfeng Yu,Dongsheng Yang*

Main category: cs.CV

TLDR: 论文提出DOPE网络，通过文本语义提取和图像对象感知增强，提升视觉与语言导航任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用语言指令中的细节信息，且忽视跨模态对象关系建模，导致导航决策不准确。

Method: 设计TSE提取关键文本信息，结合TOPA和IOPA增强对象感知，以优化语言理解和跨模态对象关系利用。

Result: 在R2R和REVERIE数据集上的实验验证了DOPE的有效性。

Conclusion: DOPE通过改进语言细节利用和跨模态对象关系建模，显著提升了导航任务的性能。

Abstract: Vision-and-Language Navigation (VLN) is a challenging task where an agent
must understand language instructions and navigate unfamiliar environments
using visual cues. The agent must accurately locate the target based on visual
information from the environment and complete tasks through interaction with
the surroundings. Despite significant advancements in this field, two major
limitations persist: (1) Many existing methods input complete language
instructions directly into multi-layer Transformer networks without fully
exploiting the detailed information within the instructions, thereby limiting
the agent's language understanding capabilities during task execution; (2)
Current approaches often overlook the modeling of object relationships across
different modalities, failing to effectively utilize latent clues between
objects, which affects the accuracy and robustness of navigation decisions. We
propose a Dual Object Perception-Enhancement Network (DOPE) to address these
issues to improve navigation performance. First, we design a Text Semantic
Extraction (TSE) to extract relatively essential phrases from the text and
input them into the Text Object Perception-Augmentation (TOPA) to fully
leverage details such as objects and actions within the instructions. Second,
we introduce an Image Object Perception-Augmentation (IOPA), which performs
additional modeling of object information across different modalities, enabling
the model to more effectively utilize latent clues between objects in images
and text, enhancing decision-making accuracy. Extensive experiments on the R2R
and REVERIE datasets validate the efficacy of the proposed approach.

</details>

### [140] [Localizing Before Answering: A Benchmark for Grounded Medical Visual Question Answering](https://arxiv.org/abs/2505.00744)
*Dung Nguyen,Minh Khoi Ho,Huy Ta,Thanh Tam Nguyen,Qi Chen,Kumar Rav,Quy Duong Dang,Satwik Ramchandre,Son Lam Phung,Zhibin Liao,Minh-Son To,Johan Verjans,Phi Le Nguyen,Vu Minh Hieu Phan*

Main category: cs.CV

TLDR: 论文揭示了当前医学大型多模态模型（LMMs）在定位推理能力上的不足，导致生成与源证据矛盾的幻觉。作者提出了HEAL-MedVQA基准和Localize-before-Answer框架，显著提升了医学视觉问答的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前医学LMMs在回答疾病相关问题时，常依赖语言模式或关注无关图像区域，而非分析相关病理区域，导致幻觉问题。

Method: 提出HEAL-MedVQA基准，包含两种评估协议和67K VQA对数据集；设计Localize-before-Answer框架，训练LMMs定位目标区域并自我提示以生成可靠答案。

Result: 实验表明，该方法在HEAL-MedVQA基准上显著优于现有生物医学LMMs。

Conclusion: 通过改进定位推理能力，论文提升了医学LMMs在视觉问答中的鲁棒性和可靠性。

Abstract: Medical Large Multi-modal Models (LMMs) have demonstrated remarkable
capabilities in medical data interpretation. However, these models frequently
generate hallucinations contradicting source evidence, particularly due to
inadequate localization reasoning. This work reveals a critical limitation in
current medical LMMs: instead of analyzing relevant pathological regions, they
often rely on linguistic patterns or attend to irrelevant image areas when
responding to disease-related queries. To address this, we introduce
HEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive
benchmark designed to evaluate LMMs' localization abilities and hallucination
robustness. HEAL-MedVQA features (i) two innovative evaluation protocols to
assess visual and textual shortcut learning, and (ii) a dataset of 67K VQA
pairs, with doctor-annotated anatomical segmentation masks for pathological
regions. To improve visual reasoning, we propose the Localize-before-Answer
(LobA) framework, which trains LMMs to localize target regions of interest and
self-prompt to emphasize segmented pathological areas, generating grounded and
reliable answers. Experimental results demonstrate that our approach
significantly outperforms state-of-the-art biomedical LMMs on the challenging
HEAL-MedVQA benchmark, advancing robustness in medical VQA.

</details>

### [141] [Responsive DNN Adaptation for Video Analytics against Environment Shift via Hierarchical Mobile-Cloud Collaborations](https://arxiv.org/abs/2505.00745)
*Maozhe Zhao,Shengzhong Liu,Fan Wu,Guihai Chen*

Main category: cs.CV

TLDR: MOCHA框架通过移动端与云端的层次协作优化模型适应性，提升响应速度与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有云中心模型适应性框架在环境变化时性能下降且反应延迟，需更高效的适应性方案。

Method: MOCHA通过设备端模型重用与快速微调、结构化专家模型检索及本地模型缓存优化适应性。

Result: 实验表明MOCHA提升适应性准确率6.8%，响应延迟减少35.5倍，重训练时间缩短3倍。

Conclusion: MOCHA显著提升了移动视频分析系统在环境变化中的适应性与响应效率。

Abstract: Mobile video analysis systems often encounter various deploying environments,
where environment shifts present greater demands for responsiveness in
adaptations of deployed "expert DNN models". Existing model adaptation
frameworks primarily operate in a cloud-centric way, exhibiting degraded
performance during adaptation and delayed reactions to environment shifts.
Instead, this paper proposes MOCHA, a novel framework optimizing the
responsiveness of continuous model adaptation through hierarchical
collaborations between mobile and cloud resources. Specifically, MOCHA (1)
reduces adaptation response delays by performing on-device model reuse and fast
fine-tuning before requesting cloud model retrieval and end-to-end retraining;
(2) accelerates history expert model retrieval by organizing them into a
structured taxonomy utilizing domain semantics analyzed by a cloud foundation
model as indices; (3) enables efficient local model reuse by maintaining
onboard expert model caches for frequent scenes, which proactively prefetch
model weights from the cloud model database. Extensive evaluations with
real-world videos on three DNN tasks show MOCHA improves the model accuracy
during adaptation by up to 6.8% while saving the response delay and retraining
time by up to 35.5x and 3.0x respectively.

</details>

### [142] [Entropy Heat-Mapping: Localizing GPT-Based OCR Errors with Sliding-Window Shannon Analysis](https://arxiv.org/abs/2505.00746)
*Alexei Kaltchenko*

Main category: cs.CV

TLDR: 论文提出了一种基于熵热图的方法，通过滑动窗口扫描GPT-4o的逐词熵信号，定位OCR错误。实验表明，高熵区域与实际错误高度重合。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如GPT-4o）在数学文档转录中缺乏对局部错误的定位能力，需一种轻量级方法辅助后编辑。

Method: 使用滑动窗口扫描逐词Shannon熵，生成熵热图以识别高熵区域（如符号缺失、括号不匹配等）。

Result: 实验表明，大多数真实错误集中在高熵区域，验证了方法的有效性。

Conclusion: 滑动窗口熵分析是一种实用的轻量级工具，可用于辅助GPT-4o的OCR后编辑。

Abstract: Vision-language models such as OpenAI GPT-4o can transcribe mathematical
documents directly from images, yet their token-level confidence signals are
seldom used to pinpoint local recognition mistakes. We present an
entropy-heat-mapping proof-of-concept that turns per-token Shannon entropy into
a visual ''uncertainty landscape''. By scanning the entropy sequence with a
fixed-length sliding window, we obtain hotspots that are likely to contain OCR
errors such as missing symbols, mismatched braces, or garbled prose. Using a
small, curated set of scanned research pages rendered at several resolutions,
we compare the highlighted hotspots with the actual transcription errors
produced by GPT-4o. Our analysis shows that the vast majority of true errors
are indeed concentrated inside the high-entropy regions. This study
demonstrates--in a minimally engineered setting--that sliding-window entropy
can serve as a practical, lightweight aid for post-editing GPT-based OCR. All
code, sample data, and annotation guidelines are released to encourage
replication and further research.

</details>

### [143] [InstructAttribute: Fine-grained Object Attributes editing with Instruction](https://arxiv.org/abs/2505.00751)
*Xingxi Yin,Jingfeng Zhang,Zhi Li,Yicheng Li,Yin Zhang*

Main category: cs.CV

TLDR: 论文提出了一种无需训练的方法SPAA，通过编辑自注意力和交叉注意力图，实现对物体颜色和材质的精确控制，并构建了Attribute Dataset支持细粒度编辑。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑技术难以精确控制物体属性或保持图像结构一致性，需要改进。

Method: 提出SPAA方法，编辑自注意力和交叉注意力图；构建Attribute Dataset，结合MLLM自动过滤和标注数据；训练InstructAttribute模型。

Result: 实验表明，SPAA在物体级颜色和材质编辑上优于现有方法。

Conclusion: SPAA和InstructAttribute实现了对物体属性的精确控制，提升了图像编辑效果。

Abstract: Text-to-image (T2I) diffusion models, renowned for their advanced generative
abilities, are extensively utilized in image editing applications,
demonstrating remarkable effectiveness. However, achieving precise control over
fine-grained attributes still presents considerable challenges. Existing image
editing techniques either fail to modify the attributes of an object or
struggle to preserve its structure and maintain consistency in other areas of
the image. To address these challenges, we propose the Structure-Preserving and
Attribute Amplification (SPAA), a training-free method which enables precise
control over the color and material transformations of objects by editing the
self-attention maps and cross-attention values. Furthermore, we constructed the
Attribute Dataset, which encompasses nearly all colors and materials associated
with various objects, by integrating multimodal large language models (MLLM) to
develop an automated pipeline for data filtering and instruction labeling.
Training on this dataset, we present our InstructAttribute, an
instruction-based model designed to facilitate fine-grained editing of color
and material attributes. Extensive experiments demonstrate that our method
achieves superior performance in object-level color and material editing,
outperforming existing instruction-based image editing approaches.

</details>

### [144] [DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking](https://arxiv.org/abs/2505.00752)
*Xuzhao Li,Xuchen Li,Shiyu Hu*

Main category: cs.CV

TLDR: DARTer是一种用于夜间无人机跟踪的端到端框架，通过动态特征混合器和激活器提升性能，同时减少冗余计算。


<details>
  <summary>Details</summary>
Motivation: 夜间无人机跟踪因光照变化和视角变化导致性能下降，现有方法计算成本高或未能充分利用动态特征。

Method: DARTer结合动态特征混合器（DFB）融合多视角特征，动态特征激活器（DFA）自适应激活Vision Transformer层。

Result: 在多个夜间无人机跟踪基准测试中，DARTer优于现有方法，平衡了准确性和效率。

Conclusion: DARTer为实际夜间无人机跟踪应用提供了高效且准确的解决方案。

Abstract: Nighttime UAV tracking presents significant challenges due to extreme
illumination variations and viewpoint changes, which severely degrade tracking
performance. Existing approaches either rely on light enhancers with high
computational costs or introduce redundant domain adaptation mechanisms,
failing to fully utilize the dynamic features in varying perspectives. To
address these issues, we propose \textbf{DARTer} (\textbf{D}ynamic
\textbf{A}daptive \textbf{R}epresentation \textbf{T}racker), an end-to-end
tracking framework designed for nighttime UAV scenarios. DARTer leverages a
Dynamic Feature Blender (DFB) to effectively fuse multi-perspective nighttime
features from static and dynamic templates, enhancing representation
robustness. Meanwhile, a Dynamic Feature Activator (DFA) adaptively activates
Vision Transformer layers based on extracted features, significantly improving
efficiency by reducing redundant computations. Our model eliminates the need
for complex multi-task loss functions, enabling a streamlined training process.
Extensive experiments on multiple nighttime UAV tracking benchmarks demonstrate
the superiority of DARTer over state-of-the-art trackers. These results confirm
that DARTer effectively balances tracking accuracy and efficiency, making it a
promising solution for real-world nighttime UAV tracking applications.

</details>

### [145] [P2P-Insole: Human Pose Estimation Using Foot Pressure Distribution and Motion Sensors](https://arxiv.org/abs/2505.00755)
*Atsuya Watanabe,Ratna Aisuwarya,Lei Jing*

Main category: cs.CV

TLDR: P2P-Insole是一种低成本方法，通过集成IMU的鞋垫传感器估计和可视化3D人体骨骼数据，适用于大规模生产。


<details>
  <summary>Details</summary>
Motivation: 解决现有商业方案成本高、侵入性强的问题，提供轻量、隐私友好的解决方案。

Method: 使用鞋垫压力分布、加速度和旋转数据，结合Transformer模型提取时间特征，并利用多模态信息提高准确性。

Result: 实验证明该方法在复杂运动模式识别中具有高精度和鲁棒性。

Conclusion: P2P-Insole为康复、预防伤害和健康监测提供了低成本实用基础，未来可通过传感器优化和数据集扩展进一步发展。

Abstract: This work presents P2P-Insole, a low-cost approach for estimating and
visualizing 3D human skeletal data using insole-type sensors integrated with
IMUs. Each insole, fabricated with e-textile garment techniques, costs under
USD 1, making it significantly cheaper than commercial alternatives and ideal
for large-scale production. Our approach uses foot pressure distribution,
acceleration, and rotation data to overcome limitations, providing a
lightweight, minimally intrusive, and privacy-aware solution. The system
employs a Transformer model for efficient temporal feature extraction, enriched
by first and second derivatives in the input stream. Including multimodal
information, such as accelerometers and rotational measurements, improves the
accuracy of complex motion pattern recognition. These facts are demonstrated
experimentally, while error metrics show the robustness of the approach in
various posture estimation tasks. This work could be the foundation for a
low-cost, practical application in rehabilitation, injury prevention, and
health monitoring while enabling further development through sensor
optimization and expanded datasets.

</details>

### [146] [Efficient On-Chip Implementation of 4D Radar-Based 3D Object Detection on Hailo-8L](https://arxiv.org/abs/2505.00757)
*Woong-Chan Byun,Dong-Hee Paek,Seung-Hyun Song,Seung-Hyun Kong*

Main category: cs.CV

TLDR: 论文提出了一种在Hailo-8L AI加速器上实现4D雷达3D目标检测的芯片级方法，通过张量变换解决5D输入限制，保持精度并实现实时处理。


<details>
  <summary>Details</summary>
Motivation: 4D雷达在自动驾驶中具有潜力，但需在低功耗嵌入式环境中实现实时处理。

Method: 提出张量变换方法，将5D输入转为4D格式，适配Hailo-8L加速器。

Result: 系统达到46.47% AP_3D和52.75% AP_BEV，推理速度13.76 Hz，与GPU模型精度相当。

Conclusion: 证明了4D雷达感知技术在自动驾驶系统中的实用性。

Abstract: 4D radar has attracted attention in autonomous driving due to its ability to
enable robust 3D object detection even under adverse weather conditions. To
practically deploy such technologies, it is essential to achieve real-time
processing within low-power embedded environments. Addressing this, we present
the first on-chip implementation of a 4D radar-based 3D object detection model
on the Hailo-8L AI accelerator. Although conventional 3D convolutional neural
network (CNN) architectures require 5D inputs, the Hailo-8L only supports 4D
tensors, posing a significant challenge. To overcome this limitation, we
introduce a tensor transformation method that reshapes 5D inputs into 4D
formats during the compilation process, enabling direct deployment without
altering the model structure. The proposed system achieves 46.47% AP_3D and
52.75% AP_BEV, maintaining comparable accuracy to GPU-based models while
achieving an inference speed of 13.76 Hz. These results demonstrate the
applicability of 4D radar-based perception technologies to autonomous driving
systems.

</details>

### [147] [Multi-Modal Language Models as Text-to-Image Model Evaluators](https://arxiv.org/abs/2505.00759)
*Jiahui Chen,Candace Ross,Reyhane Askari-Hemmat,Koustuv Sinha,Melissa Hall,Michal Drozdzal,Adriana Romero-Soriano*

Main category: cs.CV

TLDR: MT2IE是一个基于多模态大语言模型（MLLMs）的文本到图像（T2I）生成模型评估框架，通过动态生成提示词评估图像质量和一致性，比静态基准更高效且更符合人类判断。


<details>
  <summary>Details</summary>
Motivation: 随着T2I模型的进步，静态数据集的评估基准逐渐过时，需要更动态的评估方法。

Method: 利用MLLMs作为评估代理，动态生成提示词并评分，提出MT2IE框架。

Result: MT2IE仅需1/80的提示词即可达到与静态基准相同的模型排名，且其评分与人类判断相关性更高。

Conclusion: MT2IE为T2I模型评估提供了高效、动态且更符合人类标准的解决方案。

Abstract: The steady improvements of text-to-image (T2I) generative models lead to slow
deprecation of automatic evaluation benchmarks that rely on static datasets,
motivating researchers to seek alternative ways to evaluate the T2I progress.
In this paper, we explore the potential of multi-modal large language models
(MLLMs) as evaluator agents that interact with a T2I model, with the objective
of assessing prompt-generation consistency and image aesthetics. We present
Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively
generates prompts for evaluation, scores generated images and matches T2I
evaluation of existing benchmarks with a fraction of the prompts used in
existing static benchmarks. Moreover, we show that MT2IE's prompt-generation
consistency scores have higher correlation with human judgment than scores
previously introduced in the literature. MT2IE generates prompts that are
efficient at probing T2I model performance, producing the same relative T2I
model rankings as existing benchmarks while using only 1/80th the number of
prompts for evaluation.

</details>

### [148] [Person detection and re-identification in open-world settings of retail stores and public spaces](https://arxiv.org/abs/2505.00772)
*Branko Brkljač,Milan Brkljač*

Main category: cs.CV

TLDR: 论文讨论了智能城市中计算机视觉的实际应用，特别是在开放世界环境中的人员重识别任务，涉及多摄像头、多人员场景下的系统设计与挑战。


<details>
  <summary>Details</summary>
Motivation: 解决开放世界环境中人员重识别任务的复杂性，包括多摄像头、多人员场景下的检测、定位和重识别问题。

Method: 讨论了系统设计架构的挑战，提出了基于不同计算机视觉技术的解决方案，并测试了一种接近实时的解决方案在多种视频捕获和实时摄像头数据上的性能。

Result: 展示了在零售商店和公共空间中应用此类系统以改进营销分析的潜力，并通过实验验证了解决方案的性能。

Conclusion: 指出了未来研究方向和可能的系统改进，强调了开放世界环境中人员重识别任务的进一步优化需求。

Abstract: Practical applications of computer vision in smart cities usually assume
system integration and operation in challenging open-world environments. In the
case of person re-identification task the main goal is to retrieve information
whether the specific person has appeared in another place at a different time
instance of the same video, or over multiple camera feeds. This typically
assumes collecting raw data from video surveillance cameras in different places
and under varying illumination conditions. In the considered open-world setting
it also requires detection and localization of the person inside the analyzed
video frame before the main re-identification step. With multi-person and
multi-camera setups the system complexity becomes higher, requiring
sophisticated tracking solutions and re-identification models. In this work we
will discuss existing challenges in system design architectures, consider
possible solutions based on different computer vision techniques, and describe
applications of such systems in retail stores and public spaces for improved
marketing analytics. In order to analyse sensitivity of person
re-identification task under different open-world environments, a performance
of one close to real-time solution will be demonstrated over several video
captures and live camera feeds. Finally, based on conducted experiments we will
indicate further research directions and possible system improvements.

</details>

### [149] [AI-ready Snow Radar Echogram Dataset (SRED) for climate change monitoring](https://arxiv.org/abs/2505.00786)
*Oluwanisola Ibikunle,Hara Talasila,Debvrat Varshney,Jilu Li,John Paden,Maryam Rahnemoonfar*

Main category: cs.CV

TLDR: 该研究首次提供了一个标准化的雷达回波图数据集，用于深度学习算法测试和比较，以解决冰层追踪问题。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标准化的数据集，现有深度学习算法在雷达回波图冰层追踪任务中的测试和比较受限，阻碍了技术进步。

Method: 研究基于NASA OIB任务的Snow Radar数据，构建了包含13,717标注和57,815弱标注回波图的数据集，并评估了五种深度学习模型。

Result: 结果显示，现有计算机视觉分割算法可追踪冰层像素，但需更先进的端到端模型以直接提取雪深和年积累量。

Conclusion: 该数据集和基准框架为冰层追踪和雪积累估计提供了宝贵资源，有助于理解极地冰盖对气候变暖的响应。

Abstract: Tracking internal layers in radar echograms with high accuracy is essential
for understanding ice sheet dynamics and quantifying the impact of accelerated
ice discharge in Greenland and other polar regions due to contemporary global
climate warming. Deep learning algorithms have become the leading approach for
automating this task, but the absence of a standardized and well-annotated
echogram dataset has hindered the ability to test and compare algorithms
reliably, limiting the advancement of state-of-the-art methods for the radar
echogram layer tracking problem. This study introduces the first comprehensive
``deep learning ready'' radar echogram dataset derived from Snow Radar airborne
data collected during the National Aeronautics and Space Administration
Operation Ice Bridge (OIB) mission in 2012. The dataset contains 13,717 labeled
and 57,815 weakly-labeled echograms covering diverse snow zones (dry, ablation,
wet) with varying along-track resolutions. To demonstrate its utility, we
evaluated the performance of five deep learning models on the dataset. Our
results show that while current computer vision segmentation algorithms can
identify and track snow layer pixels in echogram images, advanced end-to-end
models are needed to directly extract snow depth and annual accumulation from
echograms, reducing or eliminating post-processing. The dataset and
accompanying benchmarking framework provide a valuable resource for advancing
radar echogram layer tracking and snow accumulation estimation, advancing our
understanding of polar ice sheets response to climate warming.

</details>

### [150] [SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models](https://arxiv.org/abs/2505.00788)
*Wufei Ma,Luoxin Ye,Nessa McWeeney,Celso M de Melo,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TLDR: 论文提出SpatialLLM，一种具备先进3D空间推理能力的大型多模态模型，通过3D数据增强和架构优化，性能超越GPT-4o 8.7%。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型（LMMs）缺乏3D空间推理能力，主要由于3D训练数据稀缺和模型设计偏向2D数据。

Method: 开发两种3D训练数据集（3D位置与方向探测数据、3D空间关系对话数据），并系统整合到LMMs架构和训练设计中。

Result: SpatialLLM显著提升3D推理能力，性能超越GPT-4o 8.7%。

Conclusion: 研究为未来3D推理研究提供了系统设计和数据优化的宝贵参考。

Abstract: Humans naturally understand 3D spatial relationships, enabling complex
reasoning like predicting collisions of vehicles from different directions.
Current large multimodal models (LMMs), however, lack of this capability of 3D
spatial reasoning. This limitation stems from the scarcity of 3D training data
and the bias in current model designs toward 2D data. In this paper, we
systematically study the impact of 3D-informed data, architecture, and training
setups, introducing SpatialLLM, a large multi-modal model with advanced 3D
spatial reasoning abilities. To address data limitations, we develop two types
of 3D-informed training datasets: (1) 3D-informed probing data focused on
object's 3D location and orientation, and (2) 3D-informed conversation data for
complex spatial relationships. Notably, we are the first to curate VQA data
that incorporate 3D orientation relationships on real images. Furthermore, we
systematically integrate these two types of training data with the
architectural and training designs of LMMs, providing a roadmap for optimal
design aimed at achieving superior 3D reasoning capabilities. Our SpatialLLM
advances machines toward highly capable 3D-informed reasoning, surpassing
GPT-4o performance by 8.7%. Our systematic empirical design and the resulting
findings offer valuable insights for future research in this direction.

</details>

### [151] [Advancing Wheat Crop Analysis: A Survey of Deep Learning Approaches Using Hyperspectral Imaging](https://arxiv.org/abs/2505.00805)
*Fadi Abdeladhim Zidi,Abdelkrim Ouafi,Fares Bougourzi,Cosimo Distante,Abdelmalik Taleb-Ahmed*

Main category: cs.CV

TLDR: 综述探讨了深度学习在高光谱成像（HSI）小麦作物分析中的应用，填补了该领域缺乏全面调查的空白。


<details>
  <summary>Details</summary>
Motivation: 小麦生产面临病虫害、气候变化和水资源短缺等挑战，传统监测方法效率低，HSI结合深度学习有望解决这些问题。

Method: 总结了基准数据集，追踪了深度学习方法的最新进展，并分析了品种分类、病害检测和产量估计等关键应用。

Result: 综述了当前最先进的论文，并提供了未来研究方向。

Conclusion: 深度学习在HSI小麦分析中具有潜力，但仍需解决数据高维度和样本不足等挑战。

Abstract: As one of the most widely cultivated and consumed crops, wheat is essential
to global food security. However, wheat production is increasingly challenged
by pests, diseases, climate change, and water scarcity, threatening yields.
Traditional crop monitoring methods are labor-intensive and often ineffective
for early issue detection. Hyperspectral imaging (HSI) has emerged as a
non-destructive and efficient technology for remote crop health assessment.
However, the high dimensionality of HSI data and limited availability of
labeled samples present notable challenges. In recent years, deep learning has
shown great promise in addressing these challenges due to its ability to
extract and analysis complex structures. Despite advancements in applying deep
learning methods to HSI data for wheat crop analysis, no comprehensive survey
currently exists in this field. This review addresses this gap by summarizing
benchmark datasets, tracking advancements in deep learning methods, and
analyzing key applications such as variety classification, disease detection,
and yield estimation. It also highlights the strengths, limitations, and future
opportunities in leveraging deep learning methods for HSI-based wheat crop
analysis. We have listed the current state-of-the-art papers and will continue
tracking updating them in the following
https://github.com/fadi-07/Awesome-Wheat-HSI-DeepLearning.

</details>

### [152] [The Comparability of Model Fusion to Measured Data in Confuser Rejection](https://arxiv.org/abs/2505.00836)
*Conor Flynn,Christopher Ebersole,Edmund Zelnio*

Main category: cs.CV

TLDR: 为解决合成孔径雷达（SAR）数据不足问题，论文提出通过集成多个基于合成数据训练的模型，并引入干扰物拒绝技术以应对未知目标。


<details>
  <summary>Details</summary>
Motivation: SAR数据收集成本高且难以覆盖所有实际场景，合成数据虽能缓解但存在与实测数据不完全匹配的问题。

Method: 利用计算能力集成多个基于合成数据训练的模型，并采用干扰物拒绝技术处理未知目标。

Result: 通过集成和干扰物拒绝技术，模型能够更好地处理合成数据与实测数据的差异，并有效分类已知目标。

Conclusion: 集成和干扰物拒绝技术为SAR数据不足问题提供了有效解决方案，提升了模型在实际应用中的表现。

Abstract: Data collection has always been a major issue in the modeling and training of
large deep learning networks, as no dataset can account for every slight
deviation we might see in live usage. Collecting samples can be especially
costly for Synthetic Aperture Radar (SAR), limiting the amount of unique
targets and operating conditions we are able to observe from. To counter this
lack of data, simulators have been developed utilizing the shooting and
bouncing ray method to allow for the generation of synthetic SAR data on 3D
models. While effective, the synthetically generated data does not perfectly
correlate to the measured data leading to issues when training models solely on
synthetic data. We aim to use computational power as a substitution for this
lack of quality measured data, by ensembling many models trained on synthetic
data. Synthetic data is also not complete, as we do not know what targets might
be present in a live environment. Therefore we need to have our ensembling
techniques account for these unknown targets by applying confuser rejection in
which our models will reject unknown targets it is presented with, and only
classify those it has been trained on.

</details>

### [153] [Are Minimal Radial Distortion Solvers Really Necessary for Relative Pose Estimation?](https://arxiv.org/abs/2505.00866)
*Viktor Kocur,Charalambos Tzamos,Yaqing Ding,Zuzana Berger Haladova,Torsten Sattler,Zuzana Kukelova*

Main category: cs.CV

TLDR: 论文比较了两种简单实现方法（采样径向畸变参数和使用神经网络估计畸变参数）与复杂的最小径向畸变求解器，证明后者在实践中不必要。


<details>
  <summary>Details</summary>
Motivation: 相对位姿估计在计算机视觉中很重要，但现有方法对径向畸变的处理复杂且耗时。

Method: 1. 结合高效针孔求解器和采样径向畸变参数；2. 使用神经网络估计畸变参数。

Result: 实验表明，复杂的最小径向畸变求解器在实践中不必要。

Conclusion: 在特定条件下，采样径向畸变参数比学习型方法更优。代码和基准数据集已开源。

Abstract: Estimating the relative pose between two cameras is a fundamental step in
many applications such as Structure-from-Motion. The common approach to
relative pose estimation is to apply a minimal solver inside a RANSAC loop.
Highly efficient solvers exist for pinhole cameras. Yet, (nearly) all cameras
exhibit radial distortion. Not modeling radial distortion leads to
(significantly) worse results. However, minimal radial distortion solvers are
significantly more complex than pinhole solvers, both in terms of run-time and
implementation efforts. This paper compares radial distortion solvers with two
simple-to-implement approaches that do not use minimal radial distortion
solvers: The first approach combines an efficient pinhole solver with sampled
radial undistortion parameters, where the sampled parameters are used for
undistortion prior to applying the pinhole solver. The second approach uses a
state-of-the-art neural network to estimate the distortion parameters rather
than sampling them from a set of potential values. Extensive experiments on
multiple datasets, and different camera setups, show that complex minimal
radial distortion solvers are not necessary in practice. We discuss under which
conditions a simple sampling of radial undistortion parameters is preferable
over calibrating cameras using a learning-based prior approach. Code and newly
created benchmark for relative pose estimation under radial distortion are
available at https://github.com/kocurvik/rdnet.

</details>

### [154] [CDFormer: Cross-Domain Few-Shot Object Detection Transformer Against Feature Confusion](https://arxiv.org/abs/2505.00938)
*Boyuan Meng,Xiaohan Zhang,Peilin Li,Zhe Wu,Yiming Li,Wenkai Zhao,Beinan Yu,Hui-Liang Shen*

Main category: cs.CV

TLDR: CDFormer是一种针对跨域少样本目标检测（CD-FSOD）中特征混淆问题的Transformer方法，通过OBD和OOD模块显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 跨域少样本目标检测中，特征混淆（如对象-背景混淆和对象-对象混淆）是主要挑战。

Method: 提出CDFormer，包含对象-背景区分（OBD）和对象-对象区分（OOD）模块，分别通过可学习的背景标记和增强类别区分来解决混淆。

Result: 实验显示CDFormer在1/5/10 shot设置下分别提升12.9%、11.0%和10.4%的mAP，优于现有方法。

Conclusion: CDFormer有效解决了跨域少样本检测中的特征混淆问题，性能显著提升。

Abstract: Cross-domain few-shot object detection (CD-FSOD) aims to detect novel objects
across different domains with limited class instances. Feature confusion,
including object-background confusion and object-object confusion, presents
significant challenges in both cross-domain and few-shot settings. In this
work, we introduce CDFormer, a cross-domain few-shot object detection
transformer against feature confusion, to address these challenges. The method
specifically tackles feature confusion through two key modules:
object-background distinguishing (OBD) and object-object distinguishing (OOD).
The OBD module leverages a learnable background token to differentiate between
objects and background, while the OOD module enhances the distinction between
objects of different classes. Experimental results demonstrate that CDFormer
outperforms previous state-of-the-art approaches, achieving 12.9% mAP, 11.0%
mAP, and 10.4% mAP improvements under the 1/5/10 shot settings, respectively,
when fine-tuned.

</details>

### [155] [Generating Animated Layouts as Structured Text Representations](https://arxiv.org/abs/2505.00975)
*Yeonsang Shin,Jihwan Kim,Yumin Song,Kyungseung Lee,Hyunhee Chung,Taeyoung Na*

Main category: cs.CV

TLDR: 论文提出了一种名为Animated Layout Generation的新方法，通过结构化文本表示实现细粒度视频控制，并开发了VAKER工具，显著优于现有视频广告生成方法。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频模型在控制文本元素和动态图形方面存在局限，尤其是在视频广告等应用中。

Method: 提出Animated Layout Generation方法，结合结构化文本表示和三阶段生成流程，开发VAKER工具。

Result: VAKER在视频广告生成中显著优于现有方法。

Conclusion: 该方法为视频广告生成提供了高效、自动化的解决方案。

Abstract: Despite the remarkable progress in text-to-video models, achieving precise
control over text elements and animated graphics remains a significant
challenge, especially in applications such as video advertisements. To address
this limitation, we introduce Animated Layout Generation, a novel approach to
extend static graphic layouts with temporal dynamics. We propose a Structured
Text Representation for fine-grained video control through hierarchical visual
elements. To demonstrate the effectiveness of our approach, we present VAKER
(Video Ad maKER), a text-to-video advertisement generation pipeline that
combines a three-stage generation process with Unstructured Text Reasoning for
seamless integration with LLMs. VAKER fully automates video advertisement
generation by incorporating dynamic layout trajectories for objects and
graphics across specific video frames. Through extensive evaluations, we
demonstrate that VAKER significantly outperforms existing methods in generating
video advertisements. Project Page:
https://yeonsangshin.github.io/projects/Vaker

</details>

### [156] [LMDepth: Lightweight Mamba-based Monocular Depth Estimation for Real-World Deployment](https://arxiv.org/abs/2505.00980)
*Jiahuan Long,Xin Zhou*

Main category: cs.CV

TLDR: LMDepth是一种基于Mamba的轻量级单目深度估计网络，旨在平衡性能和计算效率，适用于资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 现有深度估计算法难以平衡性能与计算效率，限制了在资源受限设备上的部署。

Method: 提出改进的金字塔空间池化模块和多深度Mamba块，结合线性计算实现高效深度解码。

Result: 在NYUDv2和KITTI数据集上表现优异，参数和计算复杂度更低。

Conclusion: LMDepth在嵌入式平台上验证了实用性，适合边缘应用。

Abstract: Monocular depth estimation provides an additional depth dimension to RGB
images, making it widely applicable in various fields such as virtual reality,
autonomous driving and robotic navigation. However, existing depth estimation
algorithms often struggle to effectively balance performance and computational
efficiency, which poses challenges for deployment on resource-constrained
devices. To address this, we propose LMDepth, a lightweight Mamba-based
monocular depth estimation network, designed to reconstruct high-precision
depth information while maintaining low computational overhead. Specifically,
we propose a modified pyramid spatial pooling module that serves as a
multi-scale feature aggregator and context extractor, ensuring global spatial
information for accurate depth estimation. Moreover, we integrate multiple
depth Mamba blocks into the decoder. Designed with linear computations, the
Mamba Blocks enable LMDepth to efficiently decode depth information from global
features, providing a lightweight alternative to Transformer-based
architectures that depend on complex attention mechanisms. Extensive
experiments on the NYUDv2 and KITTI datasets demonstrate the effectiveness of
our proposed LMDepth. Compared to previous lightweight depth estimation
methods, LMDepth achieves higher performance with fewer parameters and lower
computational complexity (measured by GFLOPs). We further deploy LMDepth on an
embedded platform with INT8 quantization, validating its practicality for
real-world edge applications.

</details>

### [157] [Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis](https://arxiv.org/abs/2505.00998)
*Yu Hua,Weiming Liu,Gui Xu,Yaqing Hou,Yew-Soon Ong,Qiang Zhang*

Main category: cs.CV

TLDR: 提出了一种确定性到随机性的多样化潜在特征映射（DSDFM）方法，用于人体运动合成，解决了传统基于分数生成模型（SGMs）训练不稳定和多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 基于分数生成模型（SGMs）在人体运动合成中表现优异，但训练过程复杂且不稳定，且生成的多样性有限。

Method: DSDFM分为两个阶段：1）人体运动重建阶段，学习潜在空间分布；2）多样化运动生成阶段，通过确定性特征映射（DerODE）和随机多样化输出生成（DivSDE）连接高斯分布与潜在空间分布。

Result: DSDFM训练更稳定，无需额外参数即可增强多样性，在定性和定量实验中均达到最优效果。

Conclusion: DSDFM在人体运动合成中表现出色，超越了现有方法，验证了其优越性。

Abstract: Human motion synthesis aims to generate plausible human motion sequences,
which has raised widespread attention in computer animation. Recent score-based
generative models (SGMs) have demonstrated impressive results on this task.
However, their training process involves complex curvature trajectories,
leading to unstable training process. In this paper, we propose a
Deterministic-to-Stochastic Diverse Latent Feature Mapping (DSDFM) method for
human motion synthesis. DSDFM consists of two stages. The first human motion
reconstruction stage aims to learn the latent space distribution of human
motions. The second diverse motion generation stage aims to build connections
between the Gaussian distribution and the latent space distribution of human
motions, thereby enhancing the diversity and accuracy of the generated human
motions. This stage is achieved by the designed deterministic feature mapping
procedure with DerODE and stochastic diverse output generation procedure with
DivSDE.DSDFM is easy to train compared to previous SGMs-based methods and can
enhance diversity without introducing additional training parameters.Through
qualitative and quantitative experiments, DSDFM achieves state-of-the-art
results surpassing the latest methods, validating its superiority in human
motion synthesis.

</details>

### [158] [3D Human Pose Estimation via Spatial Graph Order Attention and Temporal Body Aware Transformer](https://arxiv.org/abs/2505.01003)
*Kamel Aouaidjia,Aofan Li,Wenhao Zhang,Chongsheng Zhang*

Main category: cs.CV

TLDR: 提出了一种结合GCN和Transformer的新方法，通过多阶图表示骨架，并引入图阶注意力模块动态选择重要阶数，同时利用改进的时空Transformer建模全局和局部依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer和GCN方法在3D人体姿态估计中分别忽略了空间邻域关系或局部时序模式，且GCN方法缺乏姿态特异性表示。

Method: 结合GCN的多阶图表示和动态图阶注意力模块，改进时空Transformer以建模全局和局部依赖关系。

Result: 在Human3.6m等数据集上验证了方法的有效性。

Conclusion: 新方法通过结合GCN和Transformer的优势，显著提升了3D姿态估计性能。

Abstract: Nowadays, Transformers and Graph Convolutional Networks (GCNs) are the
prevailing techniques for 3D human pose estimation. However, Transformer-based
methods either ignore the spatial neighborhood relationships between the joints
when used for skeleton representations or disregard the local temporal patterns
of the local joint movements in skeleton sequence modeling, while GCN-based
methods often neglect the need for pose-specific representations. To address
these problems, we propose a new method that exploits the graph modeling
capability of GCN to represent each skeleton with multiple graphs of different
orders, incorporated with a newly introduced Graph Order Attention module that
dynamically emphasizes the most representative orders for each joint. The
resulting spatial features of the sequence are further processed using a
proposed temporal Body Aware Transformer that models the global body feature
dependencies in the sequence with awareness of the local inter-skeleton feature
dependencies of joints. Given that our 3D pose output aligns with the central
2D pose in the sequence, we improve the self-attention mechanism to be aware of
the central pose while diminishing its focus gradually towards the first and
the last poses. Extensive experiments on Human3.6m, MPIINF-3DHP, and HumanEva-I
datasets demonstrate the effectiveness of the proposed method. Code and models
are made available on Github.

</details>

### [159] [VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models](https://arxiv.org/abs/2505.01406)
*Mohammadreza Teymoorianfard,Shiqing Ma,Amir Houmansadr*

Main category: cs.CV

TLDR: VIDSTAMP是一种视频水印框架，通过直接在时间感知视频扩散模型的潜在空间中嵌入每帧或每段消息，实现了高容量、灵活的水印嵌入，同时保持视觉质量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着视频扩散模型的快速发展，生成高度逼真且时间连贯的视频引发了对内容真实性、来源和滥用的担忧。现有水印方法难以应对视频特定操作（如帧插入、丢弃或重新排序）且通常影响视觉质量。

Method: VIDSTAMP通过两阶段微调模型解码器：先在静态图像数据集上训练以促进空间消息分离，再在合成视频序列上恢复时间一致性。利用3D卷积和时间注意力等架构组件，无需额外推理成本。

Result: VIDSTAMP嵌入768比特/视频（48比特/帧），比特准确率达95.0%，视频质量评分0.836（接近无水印输出的0.838），在容量与质量权衡上优于现有方法。

Conclusion: VIDSTAMP在保持视觉质量的同时，提供了高容量和鲁棒性的水印嵌入，优于现有方法。

Abstract: The rapid rise of video diffusion models has enabled the generation of highly
realistic and temporally coherent videos, raising critical concerns about
content authenticity, provenance, and misuse. Existing watermarking approaches,
whether passive, post-hoc, or adapted from image-based techniques, often
struggle to withstand video-specific manipulations such as frame insertion,
dropping, or reordering, and typically degrade visual quality. In this work, we
introduce VIDSTAMP, a watermarking framework that embeds per-frame or
per-segment messages directly into the latent space of temporally-aware video
diffusion models. By fine-tuning the model's decoder through a two-stage
pipeline, first on static image datasets to promote spatial message separation,
and then on synthesized video sequences to restore temporal consistency,
VIDSTAMP learns to embed high-capacity, flexible watermarks with minimal
perceptual impact. Leveraging architectural components such as 3D convolutions
and temporal attention, our method imposes no additional inference cost and
offers better perceptual quality than prior methods, while maintaining
comparable robustness against common distortions and tampering. VIDSTAMP embeds
768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a
log P-value of -166.65 (lower is better), and maintains a video quality score
of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior
methods in capacity-quality tradeoffs. Code: Code:
\url{https://github.com/SPIN-UMass/VidStamp}

</details>

### [160] [Fine-Tuning Without Forgetting: Adaptation of YOLOv8 Preserves COCO Performance](https://arxiv.org/abs/2505.01016)
*Vishal Gandhi,Sagar Gandhi*

Main category: cs.CV

TLDR: 研究探讨了预训练目标检测模型在细粒度任务中的微调深度对性能的影响，发现深度微调（解冻至第10层）显著提升细粒度任务表现，同时不影响原始任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决预训练模型在细粒度任务中如何平衡微调深度与避免灾难性遗忘的问题。

Method: 通过逐步解冻YOLOv8n模型的骨干层（22、15、10层），在细粒度水果检测数据集上微调，并评估其在目标数据集和原始COCO验证集上的表现。

Result: 深度微调（解冻至第10层）显著提升细粒度任务性能（+10% mAP50），且对原始任务性能影响极小（<0.1% mAP差异）。

Conclusion: 中后期骨干层微调对细粒度任务高效且安全，无需担心灾难性遗忘，适合复杂领域或性能优先场景。

Abstract: The success of large pre-trained object detectors hinges on their
adaptability to diverse downstream tasks. While fine-tuning is the standard
adaptation method, specializing these models for challenging fine-grained
domains necessitates careful consideration of feature granularity. The critical
question remains: how deeply should the pre-trained backbone be fine-tuned to
optimize for the specialized task without incurring catastrophic forgetting of
the original general capabilities? Addressing this, we present a systematic
empirical study evaluating the impact of fine-tuning depth. We adapt a standard
YOLOv8n model to a custom, fine-grained fruit detection dataset by
progressively unfreezing backbone layers (freeze points at layers 22, 15, and
10) and training. Performance was rigorously evaluated on both the target fruit
dataset and, using a dual-head evaluation architecture, on the original COCO
validation set. Our results demonstrate unequivocally that deeper fine-tuning
(unfreezing down to layer 10) yields substantial performance gains (e.g., +10\%
absolute mAP50) on the fine-grained fruit task compared to only training the
head. Strikingly, this significant adaptation and specialization resulted in
negligible performance degradation (<0.1\% absolute mAP difference) on the COCO
benchmark across all tested freeze levels. We conclude that adapting
mid-to-late backbone features is highly effective for fine-grained
specialization. Critically, our results demonstrate this adaptation can be
achieved without the commonly expected penalty of catastrophic forgetting,
presenting a compelling case for exploring deeper fine-tuning strategies,
particularly when targeting complex domains or when maximizing specialized
performance is paramount.

</details>

### [161] [Edge-preserving Image Denoising via Multi-scale Adaptive Statistical Independence Testing](https://arxiv.org/abs/2505.01032)
*Ruyu Yan,Da-Qing Zhang*

Main category: cs.CV

TLDR: 提出了一种基于多尺度自适应独立性测试的边缘检测与去噪方法（EDD-MAIT），通过动态调整窗口大小和结合通道注意力机制，提升了边缘检测的清晰度和噪声抑制能力。


<details>
  <summary>Details</summary>
Motivation: 现有边缘检测方法生成的边缘图过于详细且固定窗口统计测试存在尺度不匹配和计算冗余问题。

Method: 结合多尺度自适应统计测试和通道注意力机制，采用梯度驱动的自适应窗口策略动态调整窗口大小。

Result: 在BSDS500和BIPED数据集上表现优于传统和基于学习的方法，F-score、MSE、PSNR和运行时间均有改善，对高斯噪声具有鲁棒性。

Conclusion: EDD-MAIT方法在边缘检测和去噪方面表现出更高的鲁棒性、准确性和效率。

Abstract: Edge detection is crucial in image processing, but existing methods often
produce overly detailed edge maps, affecting clarity. Fixed-window statistical
testing faces issues like scale mismatch and computational redundancy. To
address these, we propose a novel Multi-scale Adaptive Independence
Testing-based Edge Detection and Denoising (EDD-MAIT), a Multi-scale Adaptive
Statistical Testing-based edge detection and denoising method that integrates a
channel attention mechanism with independence testing. A gradient-driven
adaptive window strategy adjusts window sizes dynamically, improving detail
preservation and noise suppression. EDD-MAIT achieves better robustness,
accuracy, and efficiency, outperforming traditional and learning-based methods
on BSDS500 and BIPED datasets, with improvements in F-score, MSE, PSNR, and
reduced runtime. It also shows robustness against Gaussian noise, generating
accurate and clean edge maps in noisy environments.

</details>

### [162] [Edge Detection based on Channel Attention and Inter-region Independence Test](https://arxiv.org/abs/2505.01040)
*Ru-yu Yan,Da-Qing Zhang*

Main category: cs.CV

TLDR: CAM-EDIT结合通道注意力机制和独立性测试的边缘检测方法，显著提升了边缘检测性能，适用于高精度工业场景。


<details>
  <summary>Details</summary>
Motivation: 现有边缘检测方法存在噪声放大和非显著细节保留过多的问题，限制了在高精度工业场景中的应用。

Method: 提出CAM-EDIT框架，整合通道注意力机制（CAM）和基于独立性测试的边缘检测（EDIT），通过多通道融合增强边缘特征，并利用统计独立性分析抑制噪声。

Result: 在BSDS500和NYUDv2数据集上表现优异，F-measure分数分别为0.635和0.460，优于传统和最新学习方法，噪声鲁棒性也有提升。

Conclusion: CAM-EDIT在边缘检测中表现出色，适用于高精度工业应用。

Abstract: Existing edge detection methods often suffer from noise amplification and
excessive retention of non-salient details, limiting their applicability in
high-precision industrial scenarios. To address these challenges, we propose
CAM-EDIT, a novel framework that integrates Channel Attention Mechanism (CAM)
and Edge Detection via Independence Testing (EDIT). The CAM module adaptively
enhances discriminative edge features through multi-channel fusion, while the
EDIT module employs region-wise statistical independence analysis (using
Fisher's exact test and chi-square test) to suppress uncorrelated
noise.Extensive experiments on BSDS500 and NYUDv2 datasets demonstrate
state-of-the-art performance. Among the nine comparison algorithms, the
F-measure scores of CAM-EDIT are 0.635 and 0.460, representing improvements of
19.2\% to 26.5\% over traditional methods (Canny, CannySR), and better than the
latest learning based methods (TIP2020, MSCNGP). Noise robustness evaluations
further reveal a 2.2\% PSNR improvement under Gaussian noise compared to
baseline methods. Qualitative results exhibit cleaner edge maps with reduced
artifacts, demonstrating its potential for high-precision industrial
applications.

</details>

### [163] [Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages](https://arxiv.org/abs/2505.01096)
*Marco Salmè,Rosa Sicilia,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TLDR: 研究评估了指令调优的视觉语言模型（VLMs）在低资源语言（意大利语、德语、西班牙语）中生成放射学报告的性能，发现语言和领域特定训练对提高报告质量至关重要。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医疗领域的应用面临低资源语言环境下生成准确放射学报告的挑战，需探索有效的模型适应方法。

Method: 使用LLaVA架构，系统评估预训练模型在通用数据集、领域特定数据集和低资源语言数据集上的表现，分析不同适应方法的效果。

Result: 语言特定模型表现优于通用和领域特定模型；医学术语微调提升性能；温度参数影响报告连贯性。

Conclusion: 语言和领域特定训练对提高多语言环境下放射学报告质量至关重要，为未来模型调优和语言适应研究指明方向。

Abstract: The integration of artificial intelligence in healthcare has opened new
horizons for improving medical diagnostics and patient care. However,
challenges persist in developing systems capable of generating accurate and
contextually relevant radiology reports, particularly in low-resource
languages. In this study, we present a comprehensive benchmark to evaluate the
performance of instruction-tuned Vision-Language Models (VLMs) in the
specialized task of radiology report generation across three low-resource
languages: Italian, German, and Spanish. Employing the LLaVA architectural
framework, we conducted a systematic evaluation of pre-trained models utilizing
general datasets, domain-specific datasets, and low-resource language-specific
datasets. In light of the unavailability of models that possess prior knowledge
of both the medical domain and low-resource languages, we analyzed various
adaptations to determine the most effective approach for these contexts. The
results revealed that language-specific models substantially outperformed both
general and domain-specific models in generating radiology reports, emphasizing
the critical role of linguistic adaptation. Additionally, models fine-tuned
with medical terminology exhibited enhanced performance across all languages
compared to models with generic knowledge, highlighting the importance of
domain-specific training. We also explored the influence of the temperature
parameter on the coherence of report generation, providing insights for optimal
model settings. Our findings highlight the importance of tailored language and
domain-specific training for improving the quality and accuracy of radiological
reports in multilingual settings. This research not only advances our
understanding of VLMs adaptability in healthcare but also points to significant
avenues for future investigations into model tuning and language-specific
adaptations.

</details>

### [164] [Transferable Adversarial Attacks on Black-Box Vision-Language Models](https://arxiv.org/abs/2505.01050)
*Kai Hu,Weichen Yu,Li Zhang,Alexander Robey,Andy Zou,Chengming Xu,Haoqi Hu,Matt Fredrikson*

Main category: cs.CV

TLDR: 研究发现，针对视觉大语言模型（VLLMs）的对抗性攻击具有高度可迁移性，能够诱导模型产生攻击者预设的错误解读，且通用扰动可跨多个模型生效。


<details>
  <summary>Details</summary>
Motivation: 探究对抗性攻击在视觉大语言模型中的可迁移性和有效性，填补现有研究的空白。

Method: 通过实验分析，验证目标对抗样本和通用扰动对主流VLLMs（如GPT-4o、Claude、Gemini）的攻击效果。

Result: 实验表明，对抗性攻击能成功诱导模型误读视觉信息，且通用扰动在多个模型中均有效。

Conclusion: 当前VLLMs普遍存在对抗性攻击漏洞，亟需开发鲁棒性防御措施以确保安全部署。

Abstract: Vision Large Language Models (VLLMs) are increasingly deployed to offer
advanced capabilities on inputs comprising both text and images. While prior
research has shown that adversarial attacks can transfer from open-source to
proprietary black-box models in text-only and vision-only contexts, the extent
and effectiveness of such vulnerabilities remain underexplored for VLLMs. We
present a comprehensive analysis demonstrating that targeted adversarial
examples are highly transferable to widely-used proprietary VLLMs such as
GPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to
induce specific attacker-chosen interpretations of visual information, such as
misinterpreting hazardous content as safe, overlooking sensitive or restricted
material, or generating detailed incorrect responses aligned with the
attacker's intent. Furthermore, we discover that universal perturbations --
modifications applicable to a wide set of images -- can consistently induce
these misinterpretations across multiple proprietary VLLMs. Our experimental
results on object recognition, visual question answering, and image captioning
show that this vulnerability is common across current state-of-the-art models,
and underscore an urgent need for robust mitigations to ensure the safe and
secure deployment of VLLMs.

</details>

### [165] [GeloVec: Higher Dimensional Geometric Smoothing for Coherent Visual Feature Extraction in Image Segmentation](https://arxiv.org/abs/2505.01057)
*Boris Kriuk,Matey Yordanov*

Main category: cs.CV

TLDR: GeloVec是一种基于CNN的注意力平滑框架，用于语义分割，通过高维几何平滑方法解决传统方法的边界不稳定和上下文不连续问题。


<details>
  <summary>Details</summary>
Motivation: 现有注意力支持的语义分割方法在特征映射中存在边界不稳定和上下文不连续问题，需要更稳定的特征提取方法。

Method: 结合改进的Chebyshev距离度量和多空间变换，通过自适应采样权重系统计算n维特征空间中的几何距离，实现边缘保持和类内同质性。

Result: 在多个基准数据集上显著提升分割性能，mIoU分别提高2.1%、2.7%和2.4%。

Conclusion: GeloVec通过黎曼几何理论保证分割稳定性，计算高效且具有强泛化能力。

Abstract: This paper introduces GeloVec, a new CNN-based attention smoothing framework
for semantic segmentation that addresses critical limitations in conventional
approaches. While existing attention-backed segmentation methods suffer from
boundary instability and contextual discontinuities during feature mapping, our
framework implements a higher-dimensional geometric smoothing method to
establish a robust manifold relationships between visually coherent regions.
GeloVec combines modified Chebyshev distance metrics with multispatial
transformations to enhance segmentation accuracy through stabilized feature
extraction. The core innovation lies in the adaptive sampling weights system
that calculates geometric distances in n-dimensional feature space, achieving
superior edge preservation while maintaining intra-class homogeneity. The
multispatial transformation matrix incorporates tensorial projections with
orthogonal basis vectors, creating more discriminative feature representations
without sacrificing computational efficiency. Experimental validation across
multiple benchmark datasets demonstrates significant improvements in
segmentation performance, with mean Intersection over Union (mIoU) gains of
2.1%, 2.7%, and 2.4% on Caltech Birds-200, LSDSC, and FSSD datasets
respectively compared to state-of-the-art methods. GeloVec's mathematical
foundation in Riemannian geometry provides theoretical guarantees on
segmentation stability. Importantly, our framework maintains computational
efficiency through parallelized implementation of geodesic transformations and
exhibits strong generalization capabilities across disciplines due to the
absence of information loss during transformations.

</details>

### [166] [Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs](https://arxiv.org/abs/2505.01064)
*Hari Chandana Kuchibhotla,Sai Srinivas Kancheti,Abbavaram Gowtham Reddy,Vineeth N Balasubramanian*

Main category: cs.CV

TLDR: 论文提出了一种名为NeaR的新方法，用于解决无标签数据下的细粒度视觉识别（VF-FGVR）问题，通过利用多模态大语言模型生成标签并优化下游CLIP模型。


<details>
  <summary>Details</summary>
Motivation: 在缺乏标注数据的领域（如医学影像），传统细粒度视觉识别方法无法适用，而直接使用多模态大语言模型成本高且效率低。

Method: 提出NeaR方法，利用多模态大语言模型为少量无标签训练数据生成标签，并基于此微调下游CLIP模型。

Result: NeaR能够有效处理多模态大语言模型生成的噪声标签，为VF-FGVR任务提供了高效解决方案。

Conclusion: NeaR为无标签数据下的细粒度视觉识别提供了新的基准方法，解决了现有技术的成本和效率问题。

Abstract: Fine-grained Visual Recognition (FGVR) involves distinguishing between
visually similar categories, which is inherently challenging due to subtle
inter-class differences and the need for large, expert-annotated datasets. In
domains like medical imaging, such curated datasets are unavailable due to
issues like privacy concerns and high annotation costs. In such scenarios
lacking labeled data, an FGVR model cannot rely on a predefined set of training
labels, and hence has an unconstrained output space for predictions. We refer
to this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict
labels from an unconstrained output space without prior label information.
While recent Multimodal Large Language Models (MLLMs) show potential for
VF-FGVR, querying these models for each test input is impractical because of
high costs and prohibitive inference times. To address these limitations, we
introduce \textbf{Nea}rest-Neighbor Label \textbf{R}efinement (NeaR), a novel
approach that fine-tunes a downstream CLIP model using labels generated by an
MLLM. Our approach constructs a weakly supervised dataset from a small,
unlabeled training set, leveraging MLLMs for label generation. NeaR is designed
to handle the noise, stochasticity, and open-endedness inherent in labels
generated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.

</details>

### [167] [Improving Editability in Image Generation with Layer-wise Memory](https://arxiv.org/abs/2505.01079)
*Daneul Kim,Jaeah Lee,Jaesik Park*

Main category: cs.CV

TLDR: 提出了一种支持多步图像编辑的框架，通过层记忆和一致性引导解决现有方法在多次编辑中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法难以处理多步编辑任务，尤其是保持先前编辑内容并自然融入新对象。

Method: 采用层记忆存储潜在表示和提示嵌入，提出背景一致性引导和多查询解耦技术。

Result: 在基准数据集上表现优异，支持多步编辑且仅需粗略掩码输入。

Conclusion: 框架在多步图像编辑中高效且质量高，显著优于现有方法。

Abstract: Most real-world image editing tasks require multiple sequential edits to
achieve desired results. Current editing approaches, primarily designed for
single-object modifications, struggle with sequential editing: especially with
maintaining previous edits along with adapting new objects naturally into the
existing content. These limitations significantly hinder complex editing
scenarios where multiple objects need to be modified while preserving their
contextual relationships. We address this fundamental challenge through two key
proposals: enabling rough mask inputs that preserve existing content while
naturally integrating new elements and supporting consistent editing across
multiple modifications. Our framework achieves this through layer-wise memory,
which stores latent representations and prompt embeddings from previous edits.
We propose Background Consistency Guidance that leverages memorized latents to
maintain scene coherence and Multi-Query Disentanglement in cross-attention
that ensures natural adaptation to existing content. To evaluate our method, we
present a new benchmark dataset incorporating semantic alignment metrics and
interactive editing scenarios. Through comprehensive experiments, we
demonstrate superior performance in iterative image editing tasks with minimal
user effort, requiring only rough masks while maintaining high-quality results
throughout multiple editing steps.

</details>

### [168] [Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation](https://arxiv.org/abs/2505.01091)
*Daniele Molino,Francesco di Feola,Linlin Shen,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TLDR: 提出了一种针对多模态医学数据生成的框架，能够生成多视角胸部X光片及相关临床报告，填补了通用视觉语言模型与医疗领域需求之间的差距。


<details>
  <summary>Details</summary>
Motivation: 医学数据的复杂性和临床准确性要求使得通用生成模型难以直接应用于医疗领域，因此需要专门设计的框架。

Method: 利用MIMIC-CXR数据集，开发了一个生成高保真图像和语义连贯报告的框架。

Result: 在FID和BLEU评分上表现优异，生成的数据在下游疾病分类任务中甚至优于真实数据。

Conclusion: 该研究强调了领域特定适应在提升生成模型临床相关性和实用性中的重要性，为未来多模态医学数据生成的发展奠定了基础。

Abstract: Generative models have revolutionized Artificial Intelligence (AI),
particularly in multimodal applications. However, adapting these models to the
medical domain poses unique challenges due to the complexity of medical data
and the stringent need for clinical accuracy. In this work, we introduce a
framework specifically designed for multimodal medical data generation. By
enabling the generation of multi-view chest X-rays and their associated
clinical report, it bridges the gap between general-purpose vision-language
models and the specialized requirements of healthcare. Leveraging the MIMIC-CXR
dataset, the proposed framework shows superior performance in generating
high-fidelity images and semantically coherent reports. Our quantitative
evaluation reveals significant results in terms of FID and BLEU scores,
showcasing the quality of the generated data. Notably, our framework achieves
comparable or even superior performance compared to real data on downstream
disease classification tasks, underlining its potential as a tool for medical
research and diagnostics. This study highlights the importance of
domain-specific adaptations in enhancing the relevance and utility of
generative models for clinical applications, paving the way for future
advancements in synthetic multimodal medical data generation.

</details>

### [169] [VSC: Visual Search Compositional Text-to-Image Diffusion Model](https://arxiv.org/abs/2505.01104)
*Do Huu Dat,Nam Hyeonu,Po-Yuan Mao,Tae-Hyun Oh*

Main category: cs.CV

TLDR: 本文提出了一种新的组合生成方法，通过成对图像嵌入改善属性-对象绑定，解决了现有文本到图像扩散模型在多属性-对象对提示中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有文本编码器（如CLIP）难以有效编码复杂语言关系和修饰词，导致属性与对象绑定不准确，尤其是在多属性-对象对提示中。

Method: 将复杂提示分解为子提示，生成对应图像并计算视觉原型，融合文本嵌入以增强表示；通过基于分割的定位训练解决交叉注意力错位问题。

Result: 在T2I CompBench基准测试中，该方法在图像质量、人类评估和多绑定对扩展的鲁棒性上优于现有组合文本到图像扩散模型。

Conclusion: 提出的方法显著提升了属性-对象绑定的准确性，为复杂提示下的文本到图像生成提供了更优解决方案。

Abstract: Text-to-image diffusion models have shown impressive capabilities in
generating realistic visuals from natural-language prompts, yet they often
struggle with accurately binding attributes to corresponding objects,
especially in prompts containing multiple attribute-object pairs. This
challenge primarily arises from the limitations of commonly used text encoders,
such as CLIP, which can fail to encode complex linguistic relationships and
modifiers effectively. Existing approaches have attempted to mitigate these
issues through attention map control during inference and the use of layout
information or fine-tuning during training, yet they face performance drops
with increased prompt complexity. In this work, we introduce a novel
compositional generation method that leverages pairwise image embeddings to
improve attribute-object binding. Our approach decomposes complex prompts into
sub-prompts, generates corresponding images, and computes visual prototypes
that fuse with text embeddings to enhance representation. By applying
segmentation-based localization training, we address cross-attention
misalignment, achieving improved accuracy in binding multiple attributes to
objects. Our approaches outperform existing compositional text-to-image
diffusion models on the benchmark T2I CompBench, achieving better image
quality, evaluated by humans, and emerging robustness under scaling number of
binding pairs in the prompt.

</details>

### [170] [Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study](https://arxiv.org/abs/2505.01109)
*Ali Mammadov,Loic Le Folgoc,Julien Adam,Anne Buronfosse,Gilles Hayem,Guillaume Hocquet,Pietro Gori*

Main category: cs.CV

TLDR: 研究发现，在高质量自监督学习特征提取器的支持下，简单的基于实例的多实例学习方法（MIL）性能优于复杂的基于嵌入的MIL方法，且更具解释性。


<details>
  <summary>Details</summary>
Motivation: 探讨在高质量特征提取器条件下，基于实例的MIL方法是否优于基于嵌入的MIL方法。

Method: 通过710次实验，比较10种MIL策略、6种自监督学习方法、4种基础模型及多种病理学适应技术，并引入4种新的基于实例的MIL方法。

Result: 实验表明，简单的基于实例的MIL方法在性能上与复杂的基于嵌入的MIL方法相当或更优，并在BRACS和Camelyon16数据集上达到新SOTA。

Conclusion: 建议将更多精力投入适应病理学的自监督学习方法，而非复杂的基于嵌入的MIL方法，以提高解释性和性能。

Abstract: Multiple Instance Learning (MIL) has emerged as the best solution for Whole
Slide Image (WSI) classification. It consists of dividing each slide into
patches, which are treated as a bag of instances labeled with a global label.
MIL includes two main approaches: instance-based and embedding-based. In the
former, each patch is classified independently, and then the patch scores are
aggregated to predict the bag label. In the latter, bag classification is
performed after aggregating patch embeddings. Even if instance-based methods
are naturally more interpretable, embedding-based MILs have usually been
preferred in the past due to their robustness to poor feature extractors.
However, recently, the quality of feature embeddings has drastically increased
using self-supervised learning (SSL). Nevertheless, many authors continue to
endorse the superiority of embedding-based MIL. To investigate this further, we
conduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6
self-supervised methods with 4 backbones, 4 foundation models, and various
pathology-adapted techniques. Furthermore, we introduce 4 instance-based MIL
methods never used before in the pathology domain. Through these extensive
experiments, we show that with a good SSL feature extractor, simple
instance-based MILs, with very few parameters, obtain similar or better
performance than complex, state-of-the-art (SOTA) embedding-based MIL methods,
setting new SOTA results on the BRACS and Camelyon16 datasets. Since simple
instance-based MIL methods are naturally more interpretable and explainable to
clinicians, our results suggest that more effort should be put into
well-adapted SSL methods for WSI rather than into complex embedding-based MIL
methods.

</details>

### [171] [FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis](https://arxiv.org/abs/2505.01172)
*Jiangtong Tan,Hu Yu,Jie Huang,Jie Xiao,Feng Zhao*

Main category: cs.CV

TLDR: 论文提出FreePCA，一种基于PCA的无训练长视频生成方法，通过解耦全局一致性和局部质量，提升视频生成效果。


<details>
  <summary>Details</summary>
Motivation: 解决长视频生成中因帧数变化导致的分布偏移问题，需同时利用短帧的局部信息和长帧的全局信息。

Method: 使用PCA将全局和局部信息解耦为一致外观和运动强度特征，通过余弦相似度测量和渐进式特征整合实现高质量生成。

Result: 实验表明FreePCA无需训练即可应用于多种视频扩散模型，显著提升一致性和质量。

Conclusion: FreePCA通过PCA解耦和特征整合，实现了长视频生成的高一致性和高质量。

Abstract: Long video generation involves generating extended videos using models
trained on short videos, suffering from distribution shifts due to varying
frame counts. It necessitates the use of local information from the original
short frames to enhance visual and motion quality, and global information from
the entire long frames to ensure appearance consistency. Existing training-free
methods struggle to effectively integrate the benefits of both, as appearance
and motion in videos are closely coupled, leading to motion inconsistency and
visual quality. In this paper, we reveal that global and local information can
be precisely decoupled into consistent appearance and motion intensity
information by applying Principal Component Analysis (PCA), allowing for
refined complementary integration of global consistency and local quality. With
this insight, we propose FreePCA, a training-free long video generation
paradigm based on PCA that simultaneously achieves high consistency and
quality. Concretely, we decouple consistent appearance and motion intensity
features by measuring cosine similarity in the principal component space.
Critically, we progressively integrate these features to preserve original
quality and ensure smooth transitions, while further enhancing consistency by
reusing the mean statistics of the initial noise. Experiments demonstrate that
FreePCA can be applied to various video diffusion models without requiring
training, leading to substantial improvements. Code is available at
https://github.com/JosephTiTan/FreePCA.

</details>

### [172] [TSTMotion: Training-free Scene-awarenText-to-motion Generation](https://arxiv.org/abs/2505.01182)
*Ziyan Guo,Haoxuan Qu,Hossein Rahmani,Dewen Soh,Ping Hu,Qiuhong Ke,Jun Liu*

Main category: cs.CV

TLDR: 提出了一种无需训练的场景感知文本到动作生成框架TSTMotion，通过预训练模型和场景引导实现高效生成。


<details>
  <summary>Details</summary>
Motivation: 现有场景感知方法依赖大规模真实动作序列，成本高昂，因此提出无需训练的解决方案。

Method: 结合基础模型预测场景感知动作引导，并将其融入空白背景动作生成器中。

Result: 实验证明框架有效且通用。

Conclusion: TSTMotion为场景感知文本到动作生成提供了一种高效且无需训练的方法。

Abstract: Text-to-motion generation has recently garnered significant research
interest, primarily focusing on generating human motion sequences in blank
backgrounds. However, human motions commonly occur within diverse 3D scenes,
which has prompted exploration into scene-aware text-to-motion generation
methods. Yet, existing scene-aware methods often rely on large-scale
ground-truth motion sequences in diverse 3D scenes, which poses practical
challenges due to the expensive cost. To mitigate this challenge, we are the
first to propose a \textbf{T}raining-free \textbf{S}cene-aware
\textbf{T}ext-to-\textbf{Motion} framework, dubbed as \textbf{TSTMotion}, that
efficiently empowers pre-trained blank-background motion generators with the
scene-aware capability. Specifically, conditioned on the given 3D scene and
text description, we adopt foundation models together to reason, predict and
validate a scene-aware motion guidance. Then, the motion guidance is
incorporated into the blank-background motion generators with two
modifications, resulting in scene-aware text-driven motion sequences. Extensive
experiments demonstrate the efficacy and generalizability of our proposed
framework. We release our code in \href{https://tstmotion.github.io/}{Project
Page}.

</details>

### [173] [Efficient Vision-based Vehicle Speed Estimation](https://arxiv.org/abs/2505.01203)
*Andrej Macko,Lukáš Gajdošech,Viktor Kocur*

Main category: cs.CV

TLDR: 提出了一种计算高效的车辆速度估计方法，通过改进3D边界框和消失点几何技术，显著提升了实时性能。


<details>
  <summary>Details</summary>
Motivation: 提升交通摄像头视频中车辆速度估计的计算效率和实时性能。

Method: 基于2D检测和消失点几何的3D边界框技术，引入改进以优化实时性能，并在BrnoCompSpeed数据集上评估。

Result: 在多种硬件平台上显著提升FPS，速度估计误差（0.58 km/h）、检测精度（91.02%）和召回率（91.14%）优于现有技术，且速度提升5.5倍。

Conclusion: 通过量化后训练的小模型在准确性和计算成本之间取得最佳平衡，适用于实际部署。

Abstract: This paper presents a computationally efficient method for vehicle speed
estimation from traffic camera footage. Building upon previous work that
utilizes 3D bounding boxes derived from 2D detections and vanishing point
geometry, we introduce several improvements to enhance real-time performance.
We evaluate our method in several variants on the BrnoCompSpeed dataset in
terms of vehicle detection and speed estimation accuracy. Our extensive
evaluation across various hardware platforms, including edge devices,
demonstrates significant gains in frames per second (FPS) compared to the prior
state-of-the-art, while maintaining comparable or improved speed estimation
accuracy. We analyze the trade-off between accuracy and computational cost,
showing that smaller models utilizing post-training quantization offer the best
balance for real-world deployment. Our best performing model beats previous
state-of-the-art in terms of median vehicle speed estimation error (0.58 km/h
vs. 0.60 km/h), detection precision (91.02% vs 87.08%) and recall (91.14% vs.
83.32%) while also being 5.5 times faster.

</details>

### [174] [T-Graph: Enhancing Sparse-view Camera Pose Estimation by Pairwise Translation Graph](https://arxiv.org/abs/2505.01207)
*Qingyu Xian,Weiqin Jiao,Hao Cheng,Berend Jan van der Zwaag,Yanqiu Huang*

Main category: cs.CV

TLDR: 论文提出T-Graph模块，通过构建全连接平移图和多层感知机，提升稀疏视角下的相机位姿估计性能，实验验证其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角相机位姿估计在遥感应用中具有挑战性，现有方法常忽略视角间的平移信息，导致性能不佳。

Method: T-Graph模块输入成对图像特征，通过MLP映射并构建全连接平移图，同时提出两种平移表示（relative-t和pair-t）。

Result: 在RelPose++和Forge方法上实验，相机中心精度提升1%至6%。

Conclusion: T-Graph模块轻量且易集成，显著提升稀疏视角位姿估计性能。

Abstract: Sparse-view camera pose estimation, which aims to estimate the
6-Degree-of-Freedom (6-DoF) poses from a limited number of images captured from
different viewpoints, is a fundamental yet challenging problem in remote
sensing applications. Existing methods often overlook the translation
information between each pair of viewpoints, leading to suboptimal performance
in sparse-view scenarios. To address this limitation, we introduce T-Graph, a
lightweight, plug-and-play module to enhance camera pose estimation in
sparse-view settings. T-graph takes paired image features as input and maps
them through a Multilayer Perceptron (MLP). It then constructs a fully
connected translation graph, where nodes represent cameras and edges encode
their translation relationships. It can be seamlessly integrated into existing
models as an additional branch in parallel with the original prediction,
maintaining efficiency and ease of use. Furthermore, we introduce two pairwise
translation representations, relative-t and pair-t, formulated under different
local coordinate systems. While relative-t captures intuitive spatial
relationships, pair-t offers a rotation-disentangled alternative. The two
representations contribute to enhanced adaptability across diverse application
scenarios, further improving our module's robustness. Extensive experiments on
two state-of-the-art methods (RelPose++ and Forge) using public datasets (C03D
and IMC PhotoTourism) validate both the effectiveness and generalizability of
T-Graph. The results demonstrate consistent improvements across various
metrics, notably camera center accuracy, which improves by 1% to 6% from 2 to 8
viewpoints.

</details>

### [175] [High Dynamic Range Novel View Synthesis with Single Exposure](https://arxiv.org/abs/2505.01212)
*Kaixuan Zhang,Hu Wang,Minxian Li,Mingwu Ren,Mao Ye,Xiatian Zhu*

Main category: cs.CV

TLDR: 论文提出了一种单曝光HDR-NVS方法Mono-HDR-3D，解决了多曝光HDR-NVS的局限性，如运动伪影和高成本。


<details>
  <summary>Details</summary>
Motivation: 多曝光HDR-NVS存在运动伪影和高成本问题，需一种仅依赖单曝光LDR图像的方法。

Method: 提出Mono-HDR-3D，包含两个模块：LDR转HDR和HDR转LDR，支持无监督闭环学习。

Result: 实验表明Mono-HDR-3D显著优于现有方法。

Conclusion: Mono-HDR-3D为单曝光HDR-NVS提供了高效解决方案，并可集成到现有NVS模型中。

Abstract: High Dynamic Range Novel View Synthesis (HDR-NVS) aims to establish a 3D
scene HDR model from Low Dynamic Range (LDR) imagery. Typically,
multiple-exposure LDR images are employed to capture a wider range of
brightness levels in a scene, as a single LDR image cannot represent both the
brightest and darkest regions simultaneously. While effective, this
multiple-exposure HDR-NVS approach has significant limitations, including
susceptibility to motion artifacts (e.g., ghosting and blurring), high capture
and storage costs. To overcome these challenges, we introduce, for the first
time, the single-exposure HDR-NVS problem, where only single exposure LDR
images are available during training. We further introduce a novel approach,
Mono-HDR-3D, featuring two dedicated modules formulated by the LDR image
formation principles, one for converting LDR colors to HDR counterparts, and
the other for transforming HDR images to LDR format so that unsupervised
learning is enabled in a closed loop. Designed as a meta-algorithm, our
approach can be seamlessly integrated with existing NVS models. Extensive
experiments show that Mono-HDR-3D significantly outperforms previous methods.
Source code will be released.

</details>

### [176] [RD-UIE: Relation-Driven State Space Modeling for Underwater Image Enhancement](https://arxiv.org/abs/2505.01224)
*Kui Jiang,Yan Luo,Junjun Jiang,Xin Xu,Fei Ma,Fei Yu*

Main category: cs.CV

TLDR: 论文提出了一种基于关系驱动的Mamba框架（RD-UIE），通过动态排序扫描机制和视觉自适应状态块（VSSB）提升水下图像增强效果，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 水下图像因波长衰减导致内容退化和颜色失真，现有Mamba模型因固定扫描路径和局部语义适应不足，限制了其在复杂水下环境中的效果。

Method: 提出动态排序扫描机制和VSSB模块，结合全局上下文与局部关系线索，并设计跨特征桥（CFB）融合多尺度表示。

Result: 在多个水下增强基准测试中，RD-UIE平均性能提升0.55 dB，优于现有方法WMamba。

Conclusion: RD-UIE通过动态排序和自适应设计，显著提升了水下图像增强的效果和鲁棒性。

Abstract: Underwater image enhancement (UIE) is a critical preprocessing step for
marine vision applications, where wavelength-dependent attenuation causes
severe content degradation and color distortion. While recent state space
models like Mamba show potential for long-range dependency modeling, their
unfolding operations and fixed scan paths on 1D sequences fail to adapt to
local object semantics and global relation modeling, limiting their efficacy in
complex underwater environments. To address this, we enhance conventional Mamba
with the sorting-based scanning mechanism that dynamically reorders scanning
sequences based on statistical distribution of spatial correlation of all
pixels. In this way, it encourages the network to prioritize the most
informative components--structural and semantic features. Upon building this
mechanism, we devise a Visually Self-adaptive State Block (VSSB) that
harmonizes dynamic sorting of Mamba with input-dependent dynamic convolution,
enabling coherent integration of global context and local relational cues. This
exquisite design helps eliminate global focus bias, especially for widely
distributed contents, which greatly weakens the statistical frequency. For
robust feature extraction and refinement, we design a cross-feature bridge
(CFB) to adaptively fuse multi-scale representations. These efforts compose the
novel relation-driven Mamba framework for effective UIE (RD-UIE). Extensive
experiments on underwater enhancement benchmarks demonstrate RD-UIE outperforms
the state-of-the-art approach WMamba in both quantitative metrics and visual
fidelity, averagely achieving 0.55 dB performance gain on the three benchmarks.
Our code is available at https://github.com/kkoucy/RD-UIE/tree/main

</details>

### [177] [Core-Set Selection for Data-efficient Land Cover Segmentation](https://arxiv.org/abs/2505.01225)
*Keiller Nogueira,Akram Zaytar,Wanli Ma,Ribana Roscher,Ronny Hänsch,Caleb Robinson,Anthony Ortiz,Simone Nsutezo,Rahul Dodhia,Juan M. Lavista Ferres,Oktay Karakuş,Paul L. Rosin*

Main category: cs.CV

TLDR: 论文提出六种核心集选择方法，用于从遥感图像分割数据集中筛选重要样本子集，实验表明部分方法优于随机选择甚至全数据训练。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型依赖大数据集，但数据量增加可能引入偏差和噪声，且计算资源需求高，需平衡数据量与质量。

Method: 提出六种核心集选择方法，基于图像、标签或两者组合，并在三个常用土地分类数据集上测试。

Result: 部分方法在子集训练中表现优于随机基线和全数据训练，验证了数据为中心学习的重要性。

Conclusion: 数据为中心的学习在遥感领域具有潜力，代码已开源。

Abstract: The increasing accessibility of remotely sensed data and the potential of
such data to inform large-scale decision-making has driven the development of
deep learning models for many Earth Observation tasks. Traditionally, such
models must be trained on large datasets. However, the common assumption that
broadly larger datasets lead to better outcomes tends to overlook the
complexities of the data distribution, the potential for introducing biases and
noise, and the computational resources required for processing and storing vast
datasets. Therefore, effective solutions should consider both the quantity and
quality of data. In this paper, we propose six novel core-set selection methods
for selecting important subsets of samples from remote sensing image
segmentation datasets that rely on imagery only, labels only, and a combination
of each. We benchmark these approaches against a random-selection baseline on
three commonly used land cover classification datasets: DFC2022, Vaihingen, and
Potsdam. In each of the datasets, we demonstrate that training on a subset of
samples outperforms the random baseline, and some approaches outperform
training on all available data. This result shows the importance and potential
of data-centric learning for the remote sensing domain. The code is available
at https://github.com/keillernogueira/data-centric-rs-classification/.

</details>

### [178] [Compensating Spatiotemporally Inconsistent Observations for Online Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2505.01235)
*Youngsik Yun,Jeongmin Bae,Hyunseung Son,Seoha Kim,Hahyun Lee,Gun Bang,Youngjung Uh*

Main category: cs.CV

TLDR: 本文提出了一种增强在线动态场景重建中时间一致性的方法，解决了现有方法在静态区域产生明显伪影的问题。


<details>
  <summary>Details</summary>
Motivation: 在线动态场景重建对实时视频输入的学习至关重要，但现有方法忽视了时间一致性，导致静态区域出现伪影。

Method: 通过从不可避免的时间不一致观测中学习误差，并减去该误差，恢复理想观测。

Result: 实验表明，该方法显著提升了时间一致性和渲染质量。

Conclusion: 该方法有效解决了在线重建中的时间一致性问题，提升了整体质量。

Abstract: Online reconstruction of dynamic scenes is significant as it enables learning
scenes from live-streaming video inputs, while existing offline dynamic
reconstruction methods rely on recorded video inputs. However, previous online
reconstruction approaches have primarily focused on efficiency and rendering
quality, overlooking the temporal consistency of their results, which often
contain noticeable artifacts in static regions. This paper identifies that
errors such as noise in real-world recordings affect temporal inconsistency in
online reconstruction. We propose a method that enhances temporal consistency
in online reconstruction from observations with temporal inconsistency which is
inevitable in cameras. We show that our method restores the ideal observation
by subtracting the learned error. We demonstrate that applying our method to
various baselines significantly enhances both temporal consistency and
rendering quality across datasets. Code, video results, and checkpoints are
available at https://bbangsik13.github.io/OR2.

</details>

### [179] [Fusing Foveal Fixations Using Linear Retinal Transformations and Bayesian Experimental Design](https://arxiv.org/abs/2505.01249)
*Christopher K. I. Williams*

Main category: cs.CV

TLDR: 论文提出了一种基于线性下采样的方法，用于融合多个注视点的场景表示，并通过贝叶斯实验设计解决“下一步注视哪里”的问题。


<details>
  <summary>Details</summary>
Motivation: 解决人类和脊椎动物如何通过多个注视点融合高分辨率场景表示的问题。

Method: 利用已知几何将视网膜变换表示为高分辨率潜在图像的线性下采样，并在因子分析（FA）及其混合模型中进行精确推断。

Result: 在Frey人脸和MNIST数据集上的实验验证了模型的有效性。

Conclusion: 线性变换和贝叶斯实验设计为场景表示和注视点选择提供了有效解决方案。

Abstract: Humans (and many vertebrates) face the problem of fusing together multiple
fixations of a scene in order to obtain a representation of the whole, where
each fixation uses a high-resolution fovea and decreasing resolution in the
periphery. In this paper we explicitly represent the retinal transformation of
a fixation as a linear downsampling of a high-resolution latent image of the
scene, exploiting the known geometry. This linear transformation allows us to
carry out exact inference for the latent variables in factor analysis (FA) and
mixtures of FA models of the scene. Further, this allows us to formulate and
solve the choice of "where to look next" as a Bayesian experimental design
problem using the Expected Information Gain criterion. Experiments on the Frey
faces and MNIST datasets demonstrate the effectiveness of our models.

</details>

### [180] [CAMELTrack: Context-Aware Multi-cue ExpLoitation for Online Multi-Object Tracking](https://arxiv.org/abs/2505.01257)
*Vladimir Somers,Baptiste Standaert,Victor Joos,Alexandre Alahi,Christophe De Vleeschouwer*

Main category: cs.CV

TLDR: CAMEL是一种新型的关联模块，通过数据学习关联策略，摆脱了传统手工启发式方法，同时保持了模块化设计的优势。


<details>
  <summary>Details</summary>
Motivation: 传统的基于检测的跟踪方法依赖手工规则，难以捕捉复杂跟踪线索之间的交互。

Method: CAMEL采用两个基于Transformer的模块和一种新的以关联为中心的训练方案，建模目标与关联线索的复杂交互。

Result: CAMELTrack在多个跟踪基准上实现了最先进的性能。

Conclusion: CAMEL提供了一种轻量级且高效的关联方法，能够利用外部现成模型，同时保持快速训练和高性能。

Abstract: Online multi-object tracking has been recently dominated by
tracking-by-detection (TbD) methods, where recent advances rely on increasingly
sophisticated heuristics for tracklet representation, feature fusion, and
multi-stage matching. The key strength of TbD lies in its modular design,
enabling the integration of specialized off-the-shelf models like motion
predictors and re-identification. However, the extensive usage of human-crafted
rules for temporal associations makes these methods inherently limited in their
ability to capture the complex interplay between various tracking cues. In this
work, we introduce CAMEL, a novel association module for Context-Aware
Multi-Cue ExpLoitation, that learns resilient association strategies directly
from data, breaking free from hand-crafted heuristics while maintaining TbD's
valuable modularity. At its core, CAMEL employs two transformer-based modules
and relies on a novel association-centric training scheme to effectively model
the complex interactions between tracked targets and their various association
cues. Unlike end-to-end detection-by-tracking approaches, our method remains
lightweight and fast to train while being able to leverage external
off-the-shelf models. Our proposed online tracking pipeline, CAMELTrack,
achieves state-of-the-art performance on multiple tracking benchmarks. Our code
is available at https://github.com/TrackingLaboratory/CAMELTrack.

</details>

### [181] [Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain](https://arxiv.org/abs/2505.01267)
*Gaozheng Pei,Ke Ma,Yingfei Sun,Qianqian Xu,Qingming Huang*

Main category: cs.CV

TLDR: 论文提出了一种基于频率域的对抗净化方法，通过保留低频信息来减少对抗扰动对图像语义的破坏。


<details>
  <summary>Details</summary>
Motivation: 现有对抗净化方法因缺乏对抗扰动的分布信息，容易破坏图像的正常语义，因此需要一种更有效的方法。

Method: 将图像分解为振幅谱和相位谱，利用低频信息替换或投影，以减少对抗扰动的影响。

Result: 实验表明，该方法显著优于现有防御方法。

Conclusion: 通过频率域分析，该方法能有效消除对抗扰动并保留图像内容与结构。

Abstract: The diffusion-based adversarial purification methods attempt to drown
adversarial perturbations into a part of isotropic noise through the forward
process, and then recover the clean images through the reverse process. Due to
the lack of distribution information about adversarial perturbations in the
pixel domain, it is often unavoidable to damage normal semantics. We turn to
the frequency domain perspective, decomposing the image into amplitude spectrum
and phase spectrum. We find that for both spectra, the damage caused by
adversarial perturbations tends to increase monotonically with frequency. This
means that we can extract the content and structural information of the
original clean sample from the frequency components that are less damaged.
Meanwhile, theoretical analysis indicates that existing purification methods
indiscriminately damage all frequency components, leading to excessive damage
to the image. Therefore, we propose a purification method that can eliminate
adversarial perturbations while maximizing the preservation of the content and
structure of the original image. Specifically, at each time step during the
reverse process, for the amplitude spectrum, we replace the low-frequency
components of the estimated image's amplitude spectrum with the corresponding
parts of the adversarial image. For the phase spectrum, we project the phase of
the estimated image into a designated range of the adversarial image's phase
spectrum, focusing on the low frequencies. Empirical evidence from extensive
experiments demonstrates that our method significantly outperforms most current
defense methods.

</details>

### [182] [FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian Scene without Spatial Priors](https://arxiv.org/abs/2505.01322)
*Chenxi Li,Weijie Wang,Qiang Li,Bruno Lepri,Nicu Sebe,Weizhi Nie*

Main category: cs.CV

TLDR: FreeInsert是一个无需空间先验的3D场景文本驱动对象插入框架，利用基础模型实现灵活编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖空间先验（如2D掩码或3D边界框）且难以保证插入对象的一致性，限制了实际应用的灵活性和可扩展性。

Method: FreeInsert利用MLLM解析用户指令的结构化语义，结合LGMs和扩散模型分离对象生成与空间放置，并通过分层细化增强空间语义。

Result: 实验表明，FreeInsert实现了语义连贯、空间精确且视觉逼真的3D插入，无需空间先验。

Conclusion: FreeInsert提供了一种用户友好且灵活的3D场景编辑方法，解决了现有技术的局限性。

Abstract: Text-driven object insertion in 3D scenes is an emerging task that enables
intuitive scene editing through natural language. However, existing 2D
editing-based methods often rely on spatial priors such as 2D masks or 3D
bounding boxes, and they struggle to ensure consistency of the inserted object.
These limitations hinder flexibility and scalability in real-world
applications. In this paper, we propose FreeInsert, a novel framework that
leverages foundation models including MLLMs, LGMs, and diffusion models to
disentangle object generation from spatial placement. This enables unsupervised
and flexible object insertion in 3D scenes without spatial priors. FreeInsert
starts with an MLLM-based parser that extracts structured semantics, including
object types, spatial relationships, and attachment regions, from user
instructions. These semantics guide both the reconstruction of the inserted
object for 3D consistency and the learning of its degrees of freedom. We
leverage the spatial reasoning capabilities of MLLMs to initialize object pose
and scale. A hierarchical, spatially aware refinement stage further integrates
spatial semantics and MLLM-inferred priors to enhance placement. Finally, the
appearance of the object is improved using the inserted-object image to enhance
visual fidelity. Experimental results demonstrate that FreeInsert achieves
semantically coherent, spatially precise, and visually realistic 3D insertions
without relying on spatial priors, offering a user-friendly and flexible
editing experience.

</details>

### [183] [Monitoring morphometric drift in lifelong learning segmentation of the spinal cord](https://arxiv.org/abs/2505.01364)
*Enamundram Naga Karthik,Sandrine Bédard,Jan Valošek,Christoph S. Aigner,Elise Bannier,Josef Bednařík,Virginie Callot,Anna Combes,Armin Curt,Gergely David,Falk Eippert,Lynn Farner,Michael G Fehlings,Patrick Freund,Tobias Granberg,Cristina Granziera,RHSCIR Network Imaging Group,Ulrike Horn,Tomáš Horák,Suzanne Humphreys,Markus Hupp,Anne Kerbrat,Nawal Kinany,Shannon Kolind,Petr Kudlička,Anna Lebret,Lisa Eunyoung Lee,Caterina Mainero,Allan R. Martin,Megan McGrath,Govind Nair,Kristin P. O'Grady,Jiwon Oh,Russell Ouellette,Nikolai Pfender,Dario Pfyffer,Pierre-François Pradat,Alexandre Prat,Emanuele Pravatà,Daniel S. Reich,Ilaria Ricchi,Naama Rotem-Kohavi,Simon Schading-Sassenhausen,Maryam Seif,Andrew Smith,Seth A Smith,Grace Sweeney,Roger Tam,Anthony Traboulsee,Constantina Andrada Treaba,Charidimos Tsagkas,Zachary Vavasour,Dimitri Van De Ville,Kenneth Arnold Weber II,Sarath Chandar,Julien Cohen-Adad*

Main category: cs.CV

TLDR: 该论文提出了一种基于多站点数据的脊髓分割模型，并引入终身学习框架监测模型更新时的形态测量漂移，应用于健康参与者的规范数据库更新。


<details>
  <summary>Details</summary>
Motivation: 评估脊髓分割模型在更新时预测的稳定性，特别是在健康参与者中获取规范值的需求。

Method: 训练多站点数据集（n=75）的脊髓分割模型，引入终身学习框架自动监测形态测量漂移，并通过GitHub Actions工作流实现自动化。

Result: 模型在腰椎脊髓病例中表现优异（Dice得分0.95±0.03），形态测量漂移监测提供了快速反馈，更新规范数据库的缩放因子稳定。

Conclusion: 提出的模型和框架为脊髓分割和规范数据库更新提供了高效、稳定的解决方案，模型已集成到Spinal Cord Toolbox v7.0中。

Abstract: Morphometric measures derived from spinal cord segmentations can serve as
diagnostic and prognostic biomarkers in neurological diseases and injuries
affecting the spinal cord. While robust, automatic segmentation methods to a
wide variety of contrasts and pathologies have been developed over the past few
years, whether their predictions are stable as the model is updated using new
datasets has not been assessed. This is particularly important for deriving
normative values from healthy participants. In this study, we present a spinal
cord segmentation model trained on a multisite $(n=75)$ dataset, including 9
different MRI contrasts and several spinal cord pathologies. We also introduce
a lifelong learning framework to automatically monitor the morphometric drift
as the model is updated using additional datasets. The framework is triggered
by an automatic GitHub Actions workflow every time a new model is created,
recording the morphometric values derived from the model's predictions over
time. As a real-world application of the proposed framework, we employed the
spinal cord segmentation model to update a recently-introduced normative
database of healthy participants containing commonly used measures of spinal
cord morphometry. Results showed that: (i) our model outperforms previous
versions and pathology-specific models on challenging lumbar spinal cord cases,
achieving an average Dice score of $0.95 \pm 0.03$; (ii) the automatic workflow
for monitoring morphometric drift provides a quick feedback loop for developing
future segmentation models; and (iii) the scaling factor required to update the
database of morphometric measures is nearly constant among slices across the
given vertebral levels, showing minimum drift between the current and previous
versions of the model monitored by the framework. The model is freely available
in Spinal Cord Toolbox v7.0.

</details>

### [184] [Global Collinearity-aware Polygonizer for Polygonal Building Mapping in Remote Sensing](https://arxiv.org/abs/2505.01385)
*Fahong Zhang,Yilei Shi,Xiao Xiang Zhu*

Main category: cs.CV

TLDR: 提出了一种名为GCP的新算法，用于从遥感图像中映射多边形建筑，结合了实例分割和动态规划技术，显著提升了多边形生成的准确性和简洁性。


<details>
  <summary>Details</summary>
Motivation: 解决从遥感图像中映射多边形建筑的挑战，传统方法如Douglas-Peucker算法在准确性和简洁性上存在不足。

Method: GCP基于实例分割框架，通过采样轮廓多段线、Transformer回归模块优化轮廓拟合，再通过动态规划简化多边形。

Result: 在公开基准测试中验证了GCP的有效性，其多边形简化模块优于传统方法。

Conclusion: GCP算法在建筑多边形映射中表现出色，具有广泛适用性，代码已开源。

Abstract: This paper addresses the challenge of mapping polygonal buildings from remote
sensing images and introduces a novel algorithm, the Global Collinearity-aware
Polygonizer (GCP). GCP, built upon an instance segmentation framework,
processes binary masks produced by any instance segmentation model. The
algorithm begins by collecting polylines sampled along the contours of the
binary masks. These polylines undergo a refinement process using a
transformer-based regression module to ensure they accurately fit the contours
of the targeted building instances. Subsequently, a collinearity-aware polygon
simplification module simplifies these refined polylines and generate the final
polygon representation. This module employs dynamic programming technique to
optimize an objective function that balances the simplicity and fidelity of the
polygons, achieving globally optimal solutions. Furthermore, the optimized
collinearity-aware objective is seamlessly integrated into network training,
enhancing the cohesiveness of the entire pipeline. The effectiveness of GCP has
been validated on two public benchmarks for polygonal building mapping. Further
experiments reveal that applying the collinearity-aware polygon simplification
module to arbitrary polylines, without prior knowledge, enhances accuracy over
traditional methods such as the Douglas-Peucker algorithm. This finding
underscores the broad applicability of GCP. The code for the proposed method
will be made available at https://github.com/zhu-xlab.

</details>

### [185] [Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer](https://arxiv.org/abs/2505.01390)
*Alice Natalina Caragliano,Claudia Tacconi,Carlo Greco,Lorenzo Nibid,Edy Ippolito,Michele Fiore,Giuseppe Perrone,Sara Ramella,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TLDR: 提出一种结合多模态深度学习和可解释AI的新方法，用于预测非小细胞肺癌患者新辅助治疗的病理反应，通过中间融合策略整合影像和临床数据，并引入医生参与循环提升临床相关性。


<details>
  <summary>Details</summary>
Motivation: 现有放射组学和单模态深度学习方法存在局限性，无法有效整合多模态数据，临床解释性不足。

Method: 采用中间融合策略整合影像和临床数据，结合医生参与循环方法，逐步引导模型从广泛肺区域聚焦到特定病变。

Result: 预测准确性和可解释性显著提升，为临床数据整合提供了新思路。

Conclusion: 该方法在临床应用中具有潜力，为多模态数据整合和可解释AI提供了有效解决方案。

Abstract: This study proposes a novel approach combining Multimodal Deep Learning with
intrinsic eXplainable Artificial Intelligence techniques to predict
pathological response in non-small cell lung cancer patients undergoing
neoadjuvant therapy. Due to the limitations of existing radiomics and unimodal
deep learning approaches, we introduce an intermediate fusion strategy that
integrates imaging and clinical data, enabling efficient interaction between
data modalities. The proposed Multimodal Doctor-in-the-Loop method further
enhances clinical relevance by embedding clinicians' domain knowledge directly
into the training process, guiding the model's focus gradually from broader
lung regions to specific lesions. Results demonstrate improved predictive
accuracy and explainability, providing insights into optimal data integration
strategies for clinical applications.

</details>

<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [186] [Reduced-order structure-property linkages for stochastic metamaterials](https://arxiv.org/abs/2505.01283)
*Hooman Danesh,Maruthi Annamaraju,Tim Brepols,Stefanie Reese,Surya R. Kalidindi*

Main category: cs.CE

TLDR: 该论文提出了一种基于材料信息学的方法，通过主成分分析和高斯过程回归，高效地建立机械超材料单元设计与弹性性能之间的映射关系。


<details>
  <summary>Details</summary>
Motivation: 机械超材料的设计空间庞大，传统物理模拟计算成本高，需要一种高效的方法来捕捉复杂的结构-性能关系。

Method: 使用主成分分析提取随机生成的2D超材料数据集特征，结合快速傅里叶变换均质化方法计算弹性刚度，并通过高斯过程回归建立降阶代理模型。

Result: 研究表明，仅需原始数据集的0.61%即可构建准确的结构-性能映射。

Conclusion: 该方法为高效设计和评估机械超材料提供了一种低计算成本的解决方案。

Abstract: The capabilities of additive manufacturing have facilitated the design and
production of mechanical metamaterials with diverse unit cell geometries.
Establishing linkages between the vast design space of unit cells and their
effective mechanical properties is critical for the efficient design and
performance evaluation of such metamaterials. However, physics-based
simulations of metamaterial unit cells across the entire design space are
computationally expensive, necessitating a materials informatics framework to
efficiently capture complex structure-property relationships. In this work,
principal component analysis of 2-point correlation functions is performed to
extract the salient features from a large dataset of randomly generated 2D
metamaterials. Physics-based simulations are performed using a fast Fourier
transform (FFT)-based homogenization approach to efficiently compute the
homogenized effective elastic stiffness across the extensive unit cell designs.
Subsequently, Gaussian process regression is used to generate reduced-order
surrogates, mapping unit cell designs to their homogenized effective elastic
constant. It is demonstrated that the adopted workflow enables a high-value
low-dimensional representation of the voluminous stochastic metamaterial
dataset, facilitating the construction of robust structure-property maps.
Finally, an uncertainty-based active learning framework is utilized to train a
surrogate model with a significantly smaller number of data points compared to
the original full dataset. It is shown that a dataset as small as $0.61\%$ of
the entire dataset is sufficient to generate accurate and robust
structure-property maps.

</details>

<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [187] [Multi-site modelling and reconstruction of past extreme skew surges along the French Atlantic coast](https://arxiv.org/abs/2505.00835)
*Nathan Huet,Philippe Naveau,Anne Sabourin*

Main category: stat.AP

TLDR: 论文提出了一种新的方法来建模法国大西洋沿岸的极端偏斜涌浪，重点研究了站点间的极值依赖结构，并提出了两种互补的方法：多元广义帕累托分布和极端回归框架。


<details>
  <summary>Details</summary>
Motivation: 研究极端偏斜涌浪的建模对海岸风险管理至关重要，尤其是通过整合历史数据有限的站点与长期观测站点的数据来重建时间序列。

Method: 采用峰值超阈值框架，定义多元极值事件；提出新的阈值确定方法；使用多元广义帕累托分布和极端回归框架进行建模和预测。

Result: 通过整合长期观测站点（如布雷斯特和圣纳泽尔）的数据，成功重建了历史偏斜涌浪时间序列。

Conclusion: 提出的方法有效解决了极端偏斜涌浪的建模问题，为海岸风险管理提供了可靠工具。

Abstract: Appropriate modelling of extreme skew surges is crucial, particularly for
coastal risk management. Our study focuses on modelling extreme skew surges
along the French Atlantic coast, with a particular emphasis on investigating
the extremal dependence structure between stations. We employ the
peak-over-threshold framework, where a multivariate extreme event is defined
whenever at least one location records a large value, though not necessarily
all stations simultaneously. A novel method for determining an appropriate
level (threshold) above which observations can be classified as extreme is
proposed. Two complementary approaches are explored. First, the multivariate
generalized Pareto distribution is employed to model extremes, leveraging its
properties to derive a generative model that predicts extreme skew surges at
one station based on observed extremes at nearby stations. Second, a novel
extreme regression framework is assessed for point predictions. This specific
regression framework enables accurate point predictions using only the "angle"
of input variables, i.e. input variables divided by their norms. The ultimate
objective is to reconstruct historical skew surge time series at stations with
limited data. This is achieved by integrating extreme skew surge data from
stations with longer records, such as Brest and Saint-Nazaire, which provide
over 150 years of observations.

</details>

<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [188] [CIMFlow: An Integrated Framework for Systematic Design and Evaluation of Digital CIM Architectures](https://arxiv.org/abs/2505.01107)
*Yingjie Qi,Jianlei Yang,Yiou Wang,Yikun Wang,Dayu Wang,Ling Tang,Cenlin Duan,Xiaolin He,Weisheng Zhao*

Main category: cs.AR

TLDR: CIMFlow是一个集成框架，用于在数字计算内存（CIM）架构上实现和评估DNN工作负载，解决了现有工具在软件和硬件设计空间覆盖不足的问题。


<details>
  <summary>Details</summary>
Motivation: 数字CIM架构在DNN加速中表现出潜力，但缺乏全面的工具支持其开发和优化，尤其是容量约束问题。

Method: CIMFlow通过灵活的ISA设计、编译流中的高级分区和并行策略，结合编译和模拟基础设施，解决了数字CIM的约束。

Result: 评估表明，CIMFlow支持跨多样化配置的系统原型设计和优化，为设计空间探索提供了可访问平台。

Conclusion: CIMFlow为数字CIM架构的研究和设计提供了全面的工具支持，填补了现有框架的不足。

Abstract: Digital Compute-in-Memory (CIM) architectures have shown great promise in
Deep Neural Network (DNN) acceleration by effectively addressing the "memory
wall" bottleneck. However, the development and optimization of digital CIM
accelerators are hindered by the lack of comprehensive tools that encompass
both software and hardware design spaces. Moreover, existing design and
evaluation frameworks often lack support for the capacity constraints inherent
in digital CIM architectures. In this paper, we present CIMFlow, an integrated
framework that provides an out-of-the-box workflow for implementing and
evaluating DNN workloads on digital CIM architectures. CIMFlow bridges the
compilation and simulation infrastructures with a flexible instruction set
architecture (ISA) design, and addresses the constraints of digital CIM through
advanced partitioning and parallelism strategies in the compilation flow. Our
evaluation demonstrates that CIMFlow enables systematic prototyping and
optimization of digital CIM architectures across diverse configurations,
providing researchers and designers with an accessible platform for extensive
design space exploration.

</details>

<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [189] [Differentiable Nonlinear Model Predictive Control](https://arxiv.org/abs/2505.01353)
*Jonathan Frey,Katrin Baumgärtner,Gianluca Frison,Dirk Reinhardt,Jasper Hoffmann,Leonard Fichtner,Sebastien Gros,Moritz Diehl*

Main category: math.OC

TLDR: 论文提出了一种计算非线性规划（NLP）解敏感性的方法，结合了隐函数定理（IFT）和内点法（IPM），并在SQP框架中实现，速度提升超过3倍。


<details>
  <summary>Details</summary>
Motivation: 学习增强方法与非线性模型预测控制（MPC）结合时，参数解敏感性的高效计算是关键挑战。现有机器学习方法仅适用于凸或无约束问题，无法满足一般非线性需求。

Method: 使用隐函数定理（IFT）和内点法（IPM）处理平滑最优性条件，在SQP方法中详细计算敏感性。

Result: 实现了高效的开源实现，为一般最优控制问题提供前向和伴随敏感性，速度提升超过3倍。

Conclusion: 该方法为非线性MPC中的学习增强方法提供了高效的敏感性计算工具。

Abstract: The efficient computation of parametric solution sensitivities is a key
challenge in the integration of learning-enhanced methods with nonlinear model
predictive control (MPC), as their availability is crucial for many learning
algorithms. While approaches presented in the machine learning community are
limited to convex or unconstrained formulations, this paper discusses the
computation of solution sensitivities of general nonlinear programs (NLPs)
using the implicit function theorem (IFT) and smoothed optimality conditions
treated in interior-point methods (IPM). We detail sensitivity computation
within a sequential quadratic programming (SQP) method which employs an IPM for
the quadratic subproblems. The publication is accompanied by an efficient
open-source implementation within the framework, providing both forward and
adjoint sensitivities for general optimal control problems, achieving speedups
exceeding 3x over the state-of-the-art solver mpc.pytorch.

</details>

### [190] [A Provably Convergent Plug-and-Play Framework for Stochastic Bilevel Optimization](https://arxiv.org/abs/2505.01258)
*Tianshu Chu,Dachuan Xu,Wei Yao,Chengming Yu,Jin Zhang*

Main category: math.OC

TLDR: 本文提出了一种名为PnPBO的即插即用框架，用于开发和分析随机双层优化方法，结合了现代无偏和有偏随机估计器，并提供了统一的收敛和复杂性分析。


<details>
  <summary>Details</summary>
Motivation: 双层优化在机器学习中具有广泛应用，但现有方法在样本复杂性和理论分析上存在不足。

Method: PnPBO框架整合了多种随机估计器（如PAGE、ZeroSARAH等），并采用移动平均技术优化上层变量估计。

Result: 理论分析表明PnPBO框架实现了与单层优化相当的样本复杂性，解决了双层优化是否具有相同最优复杂性界限的开放性问题。

Conclusion: 实验验证了PnPBO框架的有效性，支持了理论发现。

Abstract: Bilevel optimization has recently attracted significant attention in machine
learning due to its wide range of applications and advanced hierarchical
optimization capabilities. In this paper, we propose a plug-and-play framework,
named PnPBO, for developing and analyzing stochastic bilevel optimization
methods. This framework integrates both modern unbiased and biased stochastic
estimators into the single-loop bilevel optimization framework introduced in
[9], with several improvements. In the implementation of PnPBO, all stochastic
estimators for different variables can be independently incorporated, and an
additional moving average technique is applied when using an unbiased estimator
for the upper-level variable. In the theoretical analysis, we provide a unified
convergence and complexity analysis for PnPBO, demonstrating that the
adaptation of various stochastic estimators (including PAGE, ZeroSARAH, and
mixed strategies) within the PnPBO framework achieves optimal sample
complexity, comparable to that of single-level optimization. This resolves the
open question of whether the optimal complexity bounds for solving bilevel
optimization are identical to those for single-level optimization. Finally, we
empirically validate our framework, demonstrating its effectiveness on several
benchmark problems and confirming our theoretical findings.

</details>

### [191] [Negative Stepsizes Make Gradient-Descent-Ascent Converge](https://arxiv.org/abs/2505.01423)
*Henry Shugart,Jason M. Altschuler*

Main category: math.OC

TLDR: 传统观点认为梯度下降上升（GDA）无法收敛，但本文通过创新的步长调度（称为弹弓步长调度）证明GDA可以收敛。


<details>
  <summary>Details</summary>
Motivation: 解决GDA在最小-最大问题中的收敛性问题，挑战传统观点。

Method: 提出时间变化、不对称且周期性为负的弹弓步长调度，证明其必要性。

Result: GDA在经典反例中实现收敛，且适用于最后一次迭代。

Conclusion: 弹弓步长调度通过非可逆梯度流实现收敛，近似于共识优化算法。

Abstract: Efficient computation of min-max problems is a central question in
optimization, learning, games, and controls. Arguably the most natural
algorithm is gradient-descent-ascent (GDA). However, since the 1970s,
conventional wisdom has argued that GDA fails to converge even on simple
problems. This failure spurred an extensive literature on modifying GDA with
additional building blocks such as extragradients, optimism, momentum,
anchoring, etc. In contrast, we show that GDA converges in its original form by
simply using a judicious choice of stepsizes.
  The key innovation is the proposal of unconventional stepsize schedules
(dubbed slingshot stepsize schedules) that are time-varying, asymmetric, and
periodically negative. We show that all three properties are necessary for
convergence, and that altogether this enables GDA to converge on the classical
counterexamples (e.g., unconstrained convex-concave problems). All of our
results apply to the last iterate of GDA, as is typically desired in practice.
  The core algorithmic intuition is that although negative stepsizes make
backward progress, they de-synchronize the min and max variables (overcoming
the cycling issue of GDA), and lead to a slingshot phenomenon in which the
forward progress in the other iterations is overwhelmingly larger. This results
in fast overall convergence. Geometrically, the slingshot dynamics leverage the
non-reversibility of gradient flow: positive/negative steps cancel to first
order, yielding a second-order net movement in a new direction that leads to
convergence and is otherwise impossible for GDA to move in. We interpret this
as a second-order finite-differencing algorithm and show that, intriguingly, it
approximately implements consensus optimization, an empirically popular
algorithm for min-max problems involving deep neural networks (e.g., training
GANs).

</details>

<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [192] [The Coral Protocol: Open Infrastructure Connecting The Internet of Agents](https://arxiv.org/abs/2505.00749)
*Roman J. Georgio,Caelum Forder,Suman Deb,Peter Carroll,Önder Gürcan*

Main category: cs.MA

TLDR: Coral Protocol是一个开放、去中心化的协作基础设施，旨在为多智能体AI生态系统提供通信、协调、信任和支付功能。


<details>
  <summary>Details</summary>
Motivation: 解决多领域、多供应商AI智能体间日益增长的互操作性需求。

Method: 设计了一个通用语言和协调框架，包括标准化消息格式、模块化协调机制和安全的团队组建功能。

Result: 为多智能体协作提供了高效、可信的基础设施，支持复杂工作流。

Conclusion: Coral Protocol是新兴“智能体互联网”的基石，通过开放协作实现更高水平的自动化和商业价值。

Abstract: The Coral Protocol is an open and decentralized collaboration infrastructure
that enables communication, coordination, trust and payments for The Internet
of Agents. It addresses the growing need for interoperability in a world where
organizations are deploying multiple specialized AI agents that must work
together across domains and vendors. As a foundational platform for multi-agent
AI ecosystems, Coral establishes a common language and coordination framework
allowing any agent to participate in complex workflows with others. Its design
emphasizes broad compatibility, security, and vendor neutrality, ensuring that
agent interactions are efficient and trustworthy. In particular, Coral
introduces standardized messaging formats for agent communication, a modular
coordination mechanism for orchestrating multi-agent tasks, and secure team
formation capabilities for dynamically assembling trusted groups of agents.
Together, these innovations position Coral Protocol as a cornerstone of the
emerging "Internet of Agents," unlocking new levels of automation, collective
intelligence, and business value through open agent collaboration.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [193] [On the emergence of numerical instabilities in Next Generation Reservoir Computing](https://arxiv.org/abs/2505.00846)
*Edmilson Roque dos Santos,Erik Bollt*

Main category: stat.ML

TLDR: NGRC是一种低成本机器学习方法，用于预测混沌时间序列，但其在自主预测中的动态稳定性仍具挑战性。研究发现特征矩阵数值条件与长期动态相关，并探讨了不同数值算法的影响。


<details>
  <summary>Details</summary>
Motivation: 解决NGRC模型在自主预测中的动态稳定性问题，揭示特征矩阵条件与长期动态的关系。

Method: 结合数值线性代数和动力系统遍历理论，系统研究特征矩阵条件随超参数的变化，并评估不同数值算法（Cholesky、SVD、LU）的效果。

Result: 发现短时间滞后和高阶多项式会导致特征矩阵条件不良，进而放大对训练数据扰动的敏感性，导致NGRC动态不稳定。

Conclusion: 特征矩阵条件不良是NGRC动态不稳定的关键因素，选择合适的数值算法可改善稳定性。

Abstract: Next Generation Reservoir Computing (NGRC) is a low-cost machine learning
method for forecasting chaotic time series from data. However, ensuring the
dynamical stability of NGRC models during autonomous prediction remains a
challenge. In this work, we uncover a key connection between the numerical
conditioning of the NGRC feature matrix -- formed by polynomial evaluations on
time-delay coordinates -- and the long-term NGRC dynamics. Merging tools from
numerical linear algebra and ergodic theory of dynamical systems, we
systematically study how the feature matrix conditioning varies across
hyperparameters. We demonstrate that the NGRC feature matrix tends to be
ill-conditioned for short time lags and high-degree polynomials.
Ill-conditioning amplifies sensitivity to training data perturbations, which
can produce unstable NGRC dynamics. We evaluate the impact of different
numerical algorithms (Cholesky, SVD, and LU) for solving the regularized
least-squares problem.

</details>

### [194] [DOLCE: Decomposing Off-Policy Evaluation/Learning into Lagged and Current Effects](https://arxiv.org/abs/2505.00961)
*Shu Tamano,Masanori Nojima*

Main category: stat.ML

TLDR: DOLCE是一种新的估计器，通过分解奖励为滞后和当前效应，解决了违反共同支持假设的问题，显著提升了OPE和OPL性能。


<details>
  <summary>Details</summary>
Motivation: 现有OPE/OPL方法在违反共同支持假设时需要不稳定外推或保守策略，无法满足对这类个体的显式评估或优化需求。

Method: 提出DOLCE，利用多时间点的上下文信息分解奖励为滞后和当前效应，结合过去和当前上下文处理违反共同支持假设的个体。

Result: DOLCE在局部正确性和条件独立性假设下无偏，实验显示其在违反共同支持假设比例增加时显著提升性能。

Conclusion: DOLCE通过分解奖励效应，有效解决了共同支持假设违反问题，为OPE和OPL提供了更优解决方案。

Abstract: Off-policy evaluation (OPE) and off-policy learning (OPL) for contextual
bandit policies leverage historical data to evaluate and optimize a target
policy. Most existing OPE/OPL methods--based on importance weighting or
imputation--assume common support between the target and logging policies. When
this assumption is violated, these methods typically require unstable
extrapolation, truncation, or conservative strategies for individuals outside
the common support assumption. However, such approaches can be inadequate in
settings where explicit evaluation or optimization for such individuals is
required. To address this issue, we propose DOLCE: Decomposing Off-policy
evaluation/learning into Lagged and Current Effects, a novel estimator that
leverages contextual information from multiple time points to decompose rewards
into lagged and current effects. By incorporating both past and present
contexts, DOLCE effectively handles individuals who violate the common support
assumption. We show that the proposed estimator is unbiased under two
assumptions--local correctness and conditional independence. Our experiments
demonstrate that DOLCE achieves substantial improvements in OPE and OPL,
particularly as the proportion of individuals outside the common support
assumption increases.

</details>

### [195] [Characterization and Learning of Causal Graphs from Hard Interventions](https://arxiv.org/abs/2505.01037)
*Zihan Zhou,Muhammad Qasim Elahi,Murat Kocaoglu*

Main category: stat.ML

TLDR: 论文提出了一种基于多实验分布数据的因果发现方法，通过比较干预分布引入图形约束，并提出了学习算法以整合多数据集。


<details>
  <summary>Details</summary>
Motivation: 解决从观察和实验数据中揭示因果结构的基本挑战，特别是在多干预分布和潜在变量存在的情况下。

Method: 通过比较不同干预分布，提出图形约束，并设计学习算法整合多数据集，引入新的定向规则。

Result: 提出了干预等价类的图形表示和学习算法，证明了算法的正确性。

Conclusion: 该方法为多干预分布下的因果发现提供了理论框架和实用工具。

Abstract: A fundamental challenge in the empirical sciences involves uncovering causal
structure through observation and experimentation. Causal discovery entails
linking the conditional independence (CI) invariances in observational data to
their corresponding graphical constraints via d-separation. In this paper, we
consider a general setting where we have access to data from multiple
experimental distributions resulting from hard interventions, as well as
potentially from an observational distribution. By comparing different
interventional distributions, we propose a set of graphical constraints that
are fundamentally linked to Pearl's do-calculus within the framework of hard
interventions. These graphical constraints associate each graphical structure
with a set of interventional distributions that are consistent with the rules
of do-calculus. We characterize the interventional equivalence class of causal
graphs with latent variables and introduce a graphical representation that can
be used to determine whether two causal graphs are interventionally equivalent,
i.e., whether they are associated with the same family of hard interventional
distributions, where the elements of the family are indistinguishable using the
invariances from do-calculus. We also propose a learning algorithm to integrate
multiple datasets from hard interventions, introducing new orientation rules.
The learning objective is a tuple of augmented graphs which entails a set of
causal graphs. We also prove the soundness of the proposed algorithm.

</details>

### [196] [Gaussian Differential Private Bootstrap by Subsampling](https://arxiv.org/abs/2505.01197)
*Holger Dette,Carina Graw*

Main category: stat.ML

TLDR: 论文提出了一种基于差分隐私的私有经验m/n自助法，解决了传统自助法在隐私保护下计算成本高和统计精度损失的问题。


<details>
  <summary>Details</summary>
Motivation: 传统自助法在大规模数据和差分隐私下存在计算成本高、隐私预算增加和统计精度损失的问题，需要一种更高效的方法。

Method: 提出私有经验m/n自助法，验证其在高斯差分隐私下的有效性和隐私保障。

Result: 该方法降低了计算成本，减少了噪声添加，提高了统计精度，并展示了更好的有限样本性质。

Conclusion: 私有经验m/n自助法在隐私保护和统计精度之间取得了更好的平衡，优于现有方法。

Abstract: Bootstrap is a common tool for quantifying uncertainty in data analysis.
However, besides additional computational costs in the application of the
bootstrap on massive data, a challenging problem in bootstrap based inference
under Differential Privacy consists in the fact that it requires repeated
access to the data. As a consequence, bootstrap based differentially private
inference requires a significant increase of the privacy budget, which on the
other hand comes with a substantial loss in statistical accuracy.
  A potential solution to reconcile the conflicting goals of statistical
accuracy and privacy is to analyze the data under parametric model assumptions
and in the last decade, several parametric bootstrap methods for inference
under privacy have been investigated. However, uncertainty quantification by
parametric bootstrap is only valid if the the quantities of interest can be
identified as the parameters of a statistical model and the imposed model
assumptions are (at least approximately) satisfied. An alternative to
parametric methods is the empirical bootstrap that is a widely used tool for
non-parametric inference and well studied in the non-private regime. However,
under privacy, less insight is available. In this paper, we propose a private
empirical $m$ out of $n$ bootstrap and validate its consistency and privacy
guarantees under Gaussian Differential Privacy. Compared to the the private $n$
out of $n$ bootstrap, our approach has several advantages. First, it comes with
less computational costs, in particular for massive data. Second, the proposed
procedure needs less additional noise in the bootstrap iterations, which leads
to an improved statistical accuracy while asymptotically guaranteeing the same
level of privacy. Third, we demonstrate much better finite sample properties
compared to the currently available procedures.

</details>

### [197] [Provable Efficiency of Guidance in Diffusion Models for General Data Distribution](https://arxiv.org/abs/2505.01382)
*Gen Li,Yuchen Jiao*

Main category: stat.ML

TLDR: 该论文分析了扩散模型中引导技术对一般数据分布的影响，证明了引导能提升整体样本质量。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在生成建模中表现出色，但对其引导效果的理论理解仍有限。现有研究仅针对特定案例，如何分析更一般分布的引导效果尚不明确。

Method: 研究尝试在一般数据分布下分析扩散引导，而非仅关注均匀样本质量提升。

Result: 证明引导能降低分类器概率的倒数平均值，从而提升整体样本质量。

Conclusion: 引导技术确实能改善样本质量，符合其引入的初衷。

Abstract: Diffusion models have emerged as a powerful framework for generative
modeling, with guidance techniques playing a crucial role in enhancing sample
quality. Despite their empirical success, a comprehensive theoretical
understanding of the guidance effect remains limited. Existing studies only
focus on case studies, where the distribution conditioned on each class is
either isotropic Gaussian or supported on a one-dimensional interval with some
extra conditions. How to analyze the guidance effect beyond these case studies
remains an open question. Towards closing this gap, we make an attempt to
analyze diffusion guidance under general data distributions. Rather than
demonstrating uniform sample quality improvement, which does not hold in some
distributions, we prove that guidance can improve the whole sample quality, in
the sense that the average reciprocal of the classifier probability decreases
with the existence of guidance. This aligns with the motivation of introducing
guidance.

</details>

<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [198] [How much to Dereverberate? Low-Latency Single-Channel Speech Enhancement in Distant Microphone Scenarios](https://arxiv.org/abs/2505.01338)
*Satvik Venkatesh,Philip Coleman,Arthur Benilov,Simon Brown,Selim Sheta,Frederic Roskam*

Main category: eess.AS

TLDR: 本文探讨了在远距离麦克风场景下实时低延迟单通道语音增强的可行性，重点关注大房间和高混响时间环境，并研究了房间体积与混响时间的关系。


<details>
  <summary>Details</summary>
Motivation: 解决远距离麦克风场景下语音增强的挑战，特别是在大房间和高混响时间环境中，如会议室和剧院。

Method: 研究了房间体积与混响时间的关系，并随机模拟房间脉冲响应，同时提出在短衰减时间内保留早期反射以改善信号质量。

Result: 证明了在远距离和大房间环境下单通道语音增强的可行性，并展示了保留早期反射对信号质量的提升。

Conclusion: 在远距离和大房间场景下，保留早期反射并优化房间脉冲响应模拟可显著提升语音增强效果。

Abstract: Dereverberation is an important sub-task of Speech Enhancement (SE) to
improve the signal's intelligibility and quality. However, it remains
challenging because the reverberation is highly correlated with the signal.
Furthermore, the single-channel SE literature has predominantly focused on
rooms with short reverb times (typically under 1 second), smaller rooms (under
volumes of 1000 cubic meters) and relatively short distances (up to 2 meters).
In this paper, we explore real-time low-latency single-channel SE under distant
microphone scenarios, such as 5 to 10 meters, and focus on conference rooms and
theatres, with larger room dimensions and reverberation times. Such a setup is
useful for applications such as lecture demonstrations, drama, and to enhance
stage acoustics. First, we show that single-channel SE in such challenging
scenarios is feasible. Second, we investigate the relationship between room
volume and reverberation time, and demonstrate its importance when randomly
simulating room impulse responses. Lastly, we show that for dereverberation
with short decay times, preserving early reflections before decaying the
transfer function of the room improves overall signal quality.

</details>

<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [199] [Wireless Communication as an Information Sensor for Multi-agent Cooperative Perception: A Survey](https://arxiv.org/abs/2505.00747)
*Zhiying Song,Tenghui Xie,Fuxi Wen,Jun Li*

Main category: cs.OH

TLDR: 本文综述了协作感知的最新进展，从信息中心的角度探讨了信息表示、信息融合和大规模部署三个关键维度，强调了V2X通信作为动态“信息传感器”的挑战。


<details>
  <summary>Details</summary>
Motivation: 通过V2X通信实现多智能体信息共享，扩展自动驾驶车辆的感知能力，解决传统车载传感器的局限性。

Method: 分类信息表示为数据级、特征级和对象级方案，探索信息融合技术，并总结支持大规模部署的系统级方法。

Result: 提出了减少数据量和压缩消息的方法，以及在非理想条件下处理异构性、定位误差、延迟和数据包丢失的技术。

Conclusion: 本文通过将V2X通信视为信息传感器，为协作感知在现实智能交通系统中的部署提供了新视角和挑战分析。

Abstract: Cooperative perception extends the perception capabilities of autonomous
vehicles by enabling multi-agent information sharing via Vehicle-to-Everything
(V2X) communication. Unlike traditional onboard sensors, V2X acts as a dynamic
"information sensor" characterized by limited communication, heterogeneity,
mobility, and scalability. This survey provides a comprehensive review of
recent advancements from the perspective of information-centric cooperative
perception, focusing on three key dimensions: information representation,
information fusion, and large-scale deployment. We categorize information
representation into data-level, feature-level, and object-level schemes, and
highlight emerging methods for reducing data volume and compressing messages
under communication constraints. In information fusion, we explore techniques
under both ideal and non-ideal conditions, including those addressing
heterogeneity, localization errors, latency, and packet loss. Finally, we
summarize system-level approaches to support scalability in dense traffic
scenarios. Compared with existing surveys, this paper introduces a new
perspective by treating V2X communication as an information sensor and
emphasizing the challenges of deploying cooperative perception in real-world
intelligent transportation systems.

</details>

<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [200] [A flexible Bayesian non-parametric mixture model reveals multiple dependencies of swap errors in visual working memory](https://arxiv.org/abs/2505.01178)
*Puria Radmard,Paul M. Bays,Máté Lengyel*

Main category: q-bio.NC

TLDR: 论文提出了一种贝叶斯非参数混合模型（BNS）来研究视觉工作记忆中的交换错误，揭示了交换错误不仅与提示相似性相关，还受到报告特征维度的非单调调制影响，挑战了此前关于交换错误来源的假设。


<details>
  <summary>Details</summary>
Motivation: 视觉工作记忆中的交换错误机制尚不明确，此前研究多假设错误源于存储或检索阶段，但缺乏数据驱动的灵活分析。本文旨在通过BNS模型更全面地理解交换错误的来源。

Method: 采用贝叶斯非参数混合模型（BNS），灵活描述交换行为，允许交换错误依赖于每个刺激项的提示和报告特征。模型拟合了人类参与者的逐试次行为数据。

Result: BNS模型成功复现了交换错误对提示相似性的强依赖性，并发现报告特征维度存在非单调调制。这表明记忆编码可能是交换错误的新来源。

Conclusion: BNS模型揭示了交换错误的多源性，挑战了此前仅关注存储和检索错误的观点，为视觉工作记忆研究提供了新视角。

Abstract: Human behavioural data in psychophysics has been used to elucidate the
underlying mechanisms of many cognitive processes, such as attention,
sensorimotor integration, and perceptual decision making. Visual working memory
has particularly benefited from this approach: analyses of VWM errors have
proven crucial for understanding VWM capacity and coding schemes, in turn
constraining neural models of both. One poorly understood class of VWM errors
are swap errors, whereby participants recall an uncued item from memory. Swap
errors could arise from erroneous memory encoding, noisy storage, or errors at
retrieval time - previous research has mostly implicated the latter two.
However, these studies made strong a priori assumptions on the detailed
mechanisms and/or parametric form of errors contributed by these sources. Here,
we pursue a data-driven approach instead, introducing a Bayesian non-parametric
mixture model of swap errors (BNS) which provides a flexible descriptive model
of swapping behaviour, such that swaps are allowed to depend on both the probed
and reported features of every stimulus item. We fit BNS to the trial-by-trial
behaviour of human participants and show that it recapitulates the strong
dependence of swaps on cue similarity in multiple datasets. Critically, BNS
reveals that this dependence coexists with a non-monotonic modulation in the
report feature dimension for a random dot motion direction-cued,
location-reported dataset. The form of the modulation inferred by BNS opens new
questions about the importance of memory encoding in causing swap errors in
VWM, a distinct source to the previously suggested binding and cueing errors.
Our analyses, combining qualitative comparisons of the highly interpretable BNS
parameter structure with rigorous quantitative model comparison and recovery
methods, show that previous interpretations of swap errors may have been
incomplete.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [201] [GENMO: A GENeralist Model for Human MOtion](https://arxiv.org/abs/2505.01425)
*Jiefeng Li,Jinkun Cao,Haotian Zhang,Davis Rempe,Jan Kautz,Umar Iqbal,Ye Yuan*

Main category: cs.GR

TLDR: GENMO是一个统一的人类运动通用模型，将运动生成和估计整合到一个框架中，通过约束生成和扩散方法实现高精度和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统方法将运动生成和估计分开，限制了知识共享和模型效率，GENMO旨在解决这一问题。

Method: 将运动估计重新定义为约束运动生成，结合回归和扩散方法，利用视频和文本数据增强生成多样性。

Result: GENMO在多种任务中表现优异，能够处理复杂条件（如遮挡）并提升生成能力。

Conclusion: GENMO作为一个通用框架，成功整合了运动生成和估计，展示了协同优势。

Abstract: Human motion modeling traditionally separates motion generation and
estimation into distinct tasks with specialized models. Motion generation
models focus on creating diverse, realistic motions from inputs like text,
audio, or keyframes, while motion estimation models aim to reconstruct accurate
motion trajectories from observations like videos. Despite sharing underlying
representations of temporal dynamics and kinematics, this separation limits
knowledge transfer between tasks and requires maintaining separate models. We
present GENMO, a unified Generalist Model for Human Motion that bridges motion
estimation and generation in a single framework. Our key insight is to
reformulate motion estimation as constrained motion generation, where the
output motion must precisely satisfy observed conditioning signals. Leveraging
the synergy between regression and diffusion, GENMO achieves accurate global
motion estimation while enabling diverse motion generation. We also introduce
an estimation-guided training objective that exploits in-the-wild videos with
2D annotations and text descriptions to enhance generative diversity.
Furthermore, our novel architecture handles variable-length motions and mixed
multimodal conditions (text, audio, video) at different time intervals,
offering flexible control. This unified approach creates synergistic benefits:
generative priors improve estimated motions under challenging conditions like
occlusions, while diverse video data enhances generation capabilities.
Extensive experiments demonstrate GENMO's effectiveness as a generalist
framework that successfully handles multiple human motion tasks within a single
model.

</details>

### [202] [Model See Model Do: Speech-Driven Facial Animation with Style Control](https://arxiv.org/abs/2505.01319)
*Yifang Pan,Karan Singh,Luiz Gustavo Hafemann*

Main category: cs.GR

TLDR: 提出了一种基于示例的生成框架，通过潜在扩散模型和风格参考剪辑生成高表现力的3D面部动画，解决了现有方法难以捕捉细微表演风格的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实现准确的唇同步和基本情感表达方面取得了进展，但难以捕捉和转移细微的表演风格。

Method: 采用潜在扩散模型，结合参考风格剪辑，并引入风格基条件机制，提取关键姿势以指导生成过程。

Result: 生成的动画在保持唇同步质量的同时，能准确捕捉风格细节，并在多种语音场景中表现优异。

Conclusion: 该方法能有效生成高表现力的面部动画，同时确保与输入语音的紧密对齐。

Abstract: Speech-driven 3D facial animation plays a key role in applications such as
virtual avatars, gaming, and digital content creation. While existing methods
have made significant progress in achieving accurate lip synchronization and
generating basic emotional expressions, they often struggle to capture and
effectively transfer nuanced performance styles. We propose a novel
example-based generation framework that conditions a latent diffusion model on
a reference style clip to produce highly expressive and temporally coherent
facial animations. To address the challenge of accurately adhering to the style
reference, we introduce a novel conditioning mechanism called style basis,
which extracts key poses from the reference and additively guides the diffusion
generation process to fit the style without compromising lip synchronization
quality. This approach enables the model to capture subtle stylistic cues while
ensuring that the generated animations align closely with the input speech.
Extensive qualitative, quantitative, and perceptual evaluations demonstrate the
effectiveness of our method in faithfully reproducing the desired style while
achieving superior lip synchronization across various speech scenarios.

</details>

<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [203] [Enhancing SPARQL Query Rewriting for Complex Ontology Alignments](https://arxiv.org/abs/2505.01309)
*Anicet Lepetit Ondo,Laurence Capus,Mamadou Bousso*

Main category: cs.DB

TLDR: 本文提出了一种基于自然语言的SPARQL查询自动重写方法，利用GPT-4等大语言模型处理复杂本体对齐（c:c），为非专家用户提供便捷查询异构数据的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注简单（s:s）和部分复杂（s:c）本体对齐，忽略了更复杂的（c:c）对齐带来的挑战，同时SPARQL语法对非专家用户不友好。

Method: 结合等价传递性和GPT-4等大语言模型，实现从源本体到目标本体的SPARQL查询自动重写。

Result: 该方法能高效处理复杂（c:c）对齐，并降低非专家用户使用SPARQL的门槛。

Conclusion: 该创新方法为异构本体查询提供了灵活且高效的解决方案，尤其适用于复杂对齐场景和非专家用户。

Abstract: SPARQL query rewriting is a fundamental mechanism for uniformly querying
heterogeneous ontologies in the Linked Data Web. However, the complexity of
ontology alignments, particularly rich correspondences (c : c), makes this
process challenging. Existing approaches primarily focus on simple (s : s) and
partially complex ( s : c) alignments, thereby overlooking the challenges posed
by more expressive alignments. Moreover, the intricate syntax of SPARQL
presents a barrier for non-expert users seeking to fully exploit the knowledge
encapsulated in ontologies. This article proposes an innovative approach for
the automatic rewriting of SPARQL queries from a source ontology to a target
ontology, based on a user's need expressed in natural language. It leverages
the principles of equivalence transitivity as well as the advanced capabilities
of large language models such as GPT-4. By integrating these elements, this
approach stands out for its ability to efficiently handle complex alignments,
particularly (c : c) correspondences , by fully exploiting their
expressiveness. Additionally, it facilitates access to aligned ontologies for
users unfamiliar with SPARQL, providing a flexible solution for querying
heterogeneous data.

</details>

<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [204] [On Simulating Thin-Film Processes at the Atomic Scale Using Machine Learned Force Fields](https://arxiv.org/abs/2505.01118)
*S. Kondati Natarajan,J. Schneider,N. Pandey,J. Wellendorff,S. Smidstrup*

Main category: cond-mat.mtrl-sci

TLDR: 论文探讨了如何利用机器学习力场（MLFF）高效建模薄膜工艺，并展示了两个工业相关过程的例子。


<details>
  <summary>Details</summary>
Motivation: 原子尺度建模薄膜工艺有助于揭示关键化学机制和提取定量指标，但传统分子动力学（MD）缺乏适用于所有工业过程的力场。

Method: 采用机器学习力场（MLFF）来模拟薄膜工艺，具体应用于HfO2的原子层沉积和MoS2的原子层刻蚀。

Result: 展示了MLFF在工业相关薄膜工艺模拟中的有效性。

Conclusion: MLFF为薄膜工艺的原子尺度模拟提供了高效且通用的解决方案。

Abstract: Atomistic modeling of thin-film processes provides an avenue not only for
discovering key chemical mechanisms of the processes but also to extract
quantitative metrics on the events and reactions taking place at the
gas-surface interface. Molecular dynamics (MD) is a powerful computational
method to study the evolution of a process at the atomic scale, but studies of
industrially relevant processes usually require suitable force fields, which
are in general not available for all processes of interest. However, machine
learned force fields (MLFF) are conquering the field of computational materials
and surface science. In this paper, we demonstrate how to efficiently build
MLFFs suitable for process simulations and provide two examples for
technologically relevant processes: precursor pulse in the atomic layer
deposition of HfO2 and atomic layer etching of MoS2.

</details>

<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [205] [Primality Testing via Circulant Matrix Eigenvalue Structure: A Novel Approach Using Cyclotomic Field Theory](https://arxiv.org/abs/2505.00730)
*Marius-Constantin Dinu*

Main category: cs.SC

TLDR: 提出了一种基于循环矩阵特征值结构的新型素数测试方法，通过特征多项式因式分解判断素数。


<details>
  <summary>Details</summary>
Motivation: 连接分圆域理论与矩阵代数，揭示素数与非素数的本质区别，提供理论见解与实际应用。

Method: 利用单位根构造循环矩阵，分析其特征多项式在有理数域上的不可约因子数量。

Result: 证明整数n>2为素数当且仅当矩阵C_n的最小多项式在Q上有两个不可约因子，实验验证了方法的有效性。

Conclusion: 该方法为确定性素数测试提供了新思路，兼具理论深度与实用价值。

Abstract: This paper presents a novel primality test based on the eigenvalue structure
of circulant matrices constructed from roots of unity. We prove that an integer
$n > 2$ is prime if and only if the minimal polynomial of the circulant matrix
$C_n = W_n + W_n^2$ has exactly two irreducible factors over $\mathbb{Q}$. This
characterization connects cyclotomic field theory with matrix algebra,
providing both theoretical insights and practical applications. We demonstrate
that the eigenvalue patterns of these matrices reveal fundamental distinctions
between prime and composite numbers, leading to a deterministic primality test.
Our approach leverages the relationship between primitive roots of unity,
Galois theory, and the factorization of cyclotomic polynomials. We provide
comprehensive experimental validation across various ranges of integers,
discuss practical implementation considerations, and analyze the computational
complexity of our method in comparison with established primality tests. The
visual interpretation of our mathematical framework provides intuitive
understanding of the algebraic structures that distinguish prime numbers. Our
experimental validation demonstrates that our approach offers a deterministic
alternative to existing methods, with performance characteristics reflecting
its algebraic foundations.

</details>

<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [206] [JFlow: Model-Independent Spherical Jeans Analysis using Equivariant Continuous Normalizing Flows](https://arxiv.org/abs/2505.00763)
*Sung Hak Lim,Kohei Hayashi,Shun'ichi Horigome,Shigeki Matsumoto,Mihoko M. Nojiri*

Main category: astro-ph.GA

TLDR: 论文提出了一种无监督机器学习方法，通过球对称Jeans方程无模型分析矮球状星系的暗物质晕结构。


<details>
  <summary>Details</summary>
Motivation: 矮球状星系中恒星的运动学信息有限，传统方法依赖参数化模型，难以进行完整相空间分析。

Method: 使用等变连续归一化流（equivariant continuous normalizing flows）无模型估计球对称恒星相空间密度和速度弥散。

Result: 在Gaia挑战数据集上验证，该方法能准确识别暗物质晕结构，即使恒星数量较少。

Conclusion: 该方法为矮球状星系的无模型分析提供了新途径。

Abstract: The kinematics of stars in dwarf spheroidal galaxies have been studied to
understand the structure of dark matter halos. However, the kinematic
information of these stars is often limited to celestial positions and
line-of-sight velocities, making full phase space analysis challenging.
Conventional methods rely on projected analytic phase space density models with
several parameters and infer dark matter halo structures by solving the
spherical Jeans equation. In this paper, we introduce an unsupervised machine
learning method for solving the spherical Jeans equation in a model-independent
way as a first step toward model-independent analysis of dwarf spheroidal
galaxies. Using equivariant continuous normalizing flows, we demonstrate that
spherically symmetric stellar phase space densities and velocity dispersions
can be estimated without model assumptions. As a proof of concept, we apply our
method to Gaia challenge datasets for spherical models and measure dark matter
mass densities given velocity anisotropy profiles. Our method can identify halo
structures accurately, even with a small number of tracer stars.

</details>

<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [207] [CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment](https://arxiv.org/abs/2505.01237)
*Edson Araujo,Andrew Rouditchenko,Yuan Gong,Saurabhchand Bhati,Samuel Thomas,Brian Kingsbury,Leonid Karlinsky,Rogerio Feris,James R. Glass*

Main category: cs.MM

TLDR: CAV-MAE Sync通过改进音频-视觉学习中的时序对齐、优化目标分离和空间定位，实现了更优的自监督学习表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在音频-视觉学习中难以捕捉细粒度时序对应关系，且优化目标冲突。

Method: 提出CAV-MAE Sync，将音频视为时序序列，分离对比和重建目标，引入可学习寄存器令牌。

Result: 在AudioSet等数据集上，零样本检索、分类和定位任务表现优于现有方法。

Conclusion: CAV-MAE Sync通过简单扩展解决了音频-视觉学习中的关键挑战，性能优于复杂架构。

Abstract: Recent advances in audio-visual learning have shown promising results in
learning representations across modalities. However, most approaches rely on
global audio representations that fail to capture fine-grained temporal
correspondences with visual frames. Additionally, existing methods often
struggle with conflicting optimization objectives when trying to jointly learn
reconstruction and cross-modal alignment. In this work, we propose CAV-MAE Sync
as a simple yet effective extension of the original CAV-MAE framework for
self-supervised audio-visual learning. We address three key challenges: First,
we tackle the granularity mismatch between modalities by treating audio as a
temporal sequence aligned with video frames, rather than using global
representations. Second, we resolve conflicting optimization goals by
separating contrastive and reconstruction objectives through dedicated global
tokens. Third, we improve spatial localization by introducing learnable
register tokens that reduce semantic load on patch tokens. We evaluate the
proposed approach on AudioSet, VGG Sound, and the ADE20K Sound dataset on
zero-shot retrieval, classification and localization tasks demonstrating
state-of-the-art performance and outperforming more complex architectures.

</details>

### [208] [FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and Flow Matching based Voice Enhancing](https://arxiv.org/abs/2505.01263)
*Gaoxiang Cong,Liang Li,Jiadong Pan,Zhedong Zhang,Amin Beheshti,Anton van den Hengel,Yuankai Qi,Qingming Huang*

Main category: cs.MM

TLDR: FlowDubber是一种基于大语言模型（LLM）的电影配音方法，通过双对比对齐和语音增强流匹配，实现了高质量的视听同步和发音效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注降低词错误率，忽视了唇同步和音质的重要性。

Method: 使用Qwen2.5作为LLM骨干，结合语义感知学习和双对比对齐（DCA），并通过流式语音增强（FVE）提升音质。

Result: 在两个主要基准测试中优于现有方法。

Conclusion: FlowDubber在视听同步和音质方面表现优异，为电影配音提供了新思路。

Abstract: Movie Dubbing aims to convert scripts into speeches that align with the given
movie clip in both temporal and emotional aspects while preserving the vocal
timbre of a given brief reference audio. Existing methods focus primarily on
reducing the word error rate while ignoring the importance of lip-sync and
acoustic quality. To address these issues, we propose a large language model
(LLM) based flow matching architecture for dubbing, named FlowDubber, which
achieves high-quality audio-visual sync and pronunciation by incorporating a
large speech language model and dual contrastive aligning while achieving
better acoustic quality via the proposed voice-enhanced flow matching than
previous works. First, we introduce Qwen2.5 as the backbone of LLM to learn the
in-context sequence from movie scripts and reference audio. Then, the proposed
semantic-aware learning focuses on capturing LLM semantic knowledge at the
phoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment
with lip movement, reducing ambiguities where similar phonemes might be
confused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves
acoustic quality in two aspects, which introduces an LLM-based acoustics flow
matching guidance to strengthen clarity and uses affine style prior to enhance
identity when recovering noise into mel-spectrograms via gradient vector field
prediction. Extensive experiments demonstrate that our method outperforms
several state-of-the-art methods on two primary benchmarks. The demos are
available at
{\href{https://galaxycong.github.io/LLM-Flow-Dubber/}{\textcolor{red}{https://galaxycong.github.io/LLM-Flow-Dubber/}}}.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [209] [SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation](https://arxiv.org/abs/2505.00831)
*Quang P. M. Pham,Khoi T. N. Nguyen,Nhi H. Doan,Cuong A. Pham,Kentaro Inui,Dezhen Song*

Main category: cs.RO

TLDR: SmallPlan框架利用LLM作为教师模型训练轻量级SLM，用于动态环境中的高效路径规划，性能接近GPT-4o且适合边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 解决大规模动态环境中机器人路径规划的高计算成本和实时性问题。

Method: 通过LLM引导的监督微调（SFT）和强化学习（RL）训练SLM，生成最优路径序列。

Result: SLM在路径规划任务中表现接近GPT-4o，且避免了幻觉和过拟合问题。

Conclusion: SmallPlan资源高效，适合边缘设备，推动了自主机器人的实际应用。

Abstract: Efficient path planning in robotics, particularly within large-scale, dynamic
environments, remains a significant hurdle. While Large Language Models (LLMs)
offer strong reasoning capabilities, their high computational cost and limited
adaptability in dynamic scenarios hinder real-time deployment on edge devices.
We present SmallPlan -- a novel framework leveraging LLMs as teacher models to
train lightweight Small Language Models (SLMs) for high-level path planning
tasks. In SmallPlan, the SLMs provide optimal action sequences to navigate
across scene graphs that compactly represent full-scaled 3D scenes. The SLMs
are trained in a simulation-powered, interleaved manner with LLM-guided
supervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not
only enables SLMs to successfully complete navigation tasks but also makes them
aware of important factors like travel distance and number of trials. Through
experiments, we demonstrate that the fine-tuned SLMs perform competitively with
larger models like GPT-4o on sequential path planning, without suffering from
hallucination and overfitting. SmallPlan is resource-efficient, making it
well-suited for edge-device deployment and advancing practical autonomous
robotics.

</details>

### [210] [IK Seed Generator for Dual-Arm Human-like Physicality Robot with Mobile Base](https://arxiv.org/abs/2505.00871)
*Jun Takamatsu,Atsushi Kanehira,Kazuhiro Sasabuchi,Naoki Wake,Katsushi Ikeuchi*

Main category: cs.RO

TLDR: 本文提出了一种通过遗传算法优化初始猜测的方法，以提高数值逆运动学（IK）求解器的成功率，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于尺寸限制的机器人因机械限制（如关节角度限制）难以解决逆运动学问题，本文旨在通过优化初始猜测来缓解这一问题。

Method: 定义了基于缩放雅可比矩阵的初始猜测质量指标，利用遗传算法优化初始猜测，并结合可达性地图枚举可能的IK解。

Result: 实验证明，使用优化后的初始猜测显著提高了IK求解的成功率，并在实际机器人任务中验证了其应用效果。

Conclusion: 提出的方法有效提升了IK求解的成功率，为受限尺寸机器人的任务执行提供了实用解决方案。

Abstract: Robots are strongly expected as a means of replacing human tasks. If a robot
has a human-like physicality, the possibility of replacing human tasks
increases. In the case of household service robots, it is desirable for them to
be on a human-like size so that they do not become excessively large in order
to coexist with humans in their operating environment. However, robots with
size limitations tend to have difficulty solving inverse kinematics (IK) due to
mechanical limitations, such as joint angle limitations. Conversely, if the
difficulty coming from this limitation could be mitigated, one can expect that
the use of such robots becomes more valuable. In numerical IK solver, which is
commonly used for robots with higher degrees-of-freedom (DOF), the solvability
of IK depends on the initial guess given to the solver. Thus, this paper
proposes a method for generating a good initial guess for a numerical IK solver
given the target hand configuration. For the purpose, we define the goodness of
an initial guess using the scaled Jacobian matrix, which can calculate the
manipulability index considering the joint limits. These two factors are
related to the difficulty of solving IK. We generate the initial guess by
optimizing the goodness using the genetic algorithm (GA). To enumerate much
possible IK solutions, we use the reachability map that represents the
reachable area of the robot hand in the arm-base coordinate system. We conduct
quantitative evaluation and prove that using an initial guess that is judged to
be better using the goodness value increases the probability that IK is solved.
Finally, as an application of the proposed method, we show that by generating
good initial guesses for IK a robot actually achieves three typical scenarios.

</details>

### [211] [Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning](https://arxiv.org/abs/2505.00935)
*Roberto Bigazzi*

Main category: cs.RO

TLDR: 论文探讨了具身人工智能（Embodied AI）的发展，结合计算机视觉、机器人和决策制定，旨在开发智能自主机器人。通过3D模型和仿真训练，智能体在未知环境中学习任务执行。


<details>
  <summary>Details</summary>
Motivation: 推动具身AI和自主智能体的研究，促进未来在该领域的发展。

Method: 利用3D模型进行仿真训练，智能体学习与环境交互、提取信息并执行任务。

Result: 实现了智能具身代理的完整创建过程，包括概念、实现和部署。

Conclusion: 为具身AI领域的研究提供了技术方法和实验验证，推动了未来工作的进展。

Abstract: The increase in available computing power and the Deep Learning revolution
have allowed the exploration of new topics and frontiers in Artificial
Intelligence research. A new field called Embodied Artificial Intelligence,
which places at the intersection of Computer Vision, Robotics, and Decision
Making, has been gaining importance during the last few years, as it aims to
foster the development of smart autonomous robots and their deployment in
society. The recent availability of large collections of 3D models for
photorealistic robotic simulation has allowed faster and safe training of
learning-based agents for millions of frames and a careful evaluation of their
behavior before deploying the models on real robotic platforms. These
intelligent agents are intended to perform a certain task in a possibly unknown
environment. To this end, during the training in simulation, the agents learn
to perform continuous interactions with the surroundings, such as gathering
information from the environment, encoding and extracting useful cues for the
task, and performing actions towards the final goal; where every action of the
agent influences the interactions. This dissertation follows the complete
creation process of embodied agents for indoor environments, from their concept
to their implementation and deployment. We aim to contribute to research in
Embodied AI and autonomous agents, in order to foster future work in this
field. We present a detailed analysis of the procedure behind implementing an
intelligent embodied agent, comprehending a thorough description of the current
state-of-the-art in literature, technical explanations of the proposed methods,
and accurate experimental studies on relevant robotic tasks.

</details>

### [212] [Model Tensor Planning](https://arxiv.org/abs/2505.01059)
*An T. Le,Khai Nguyen,Minh Nhat Vu,João Carvalho,Jan Peters*

Main category: cs.RO

TLDR: 提出了一种基于张量采样的模型预测控制框架（MTP），通过高熵控制轨迹生成和混合策略提升探索能力，在复杂机器人任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统采样MPC在非线性任务中探索不足的问题。

Method: 使用张量采样生成多样化控制轨迹，结合B样条和Akima样条插值，并采用β混合策略平衡探索与利用。

Result: 在多种机器人任务中优于标准MPC和进化策略，验证了张量采样结构和混合策略的有效性。

Conclusion: MTP为模型规划与控制提供了可扩展的鲁棒探索框架。

Abstract: Sampling-based model predictive control (MPC) offers strong performance in
nonlinear and contact-rich robotic tasks, yet often suffers from poor
exploration due to locally greedy sampling schemes. We propose \emph{Model
Tensor Planning} (MTP), a novel sampling-based MPC framework that introduces
high-entropy control trajectory generation through structured tensor sampling.
By sampling over randomized multipartite graphs and interpolating control
trajectories with B-splines and Akima splines, MTP ensures smooth and globally
diverse control candidates. We further propose a simple $\beta$-mixing strategy
that blends local exploitative and global exploratory samples within the
modified Cross-Entropy Method (CEM) update, balancing control refinement and
exploration. Theoretically, we show that MTP achieves asymptotic path coverage
and maximum entropy in the control trajectory space in the limit of infinite
tensor depth and width.
  Our implementation is fully vectorized using JAX and compatible with MuJoCo
XLA, supporting \emph{Just-in-time} (JIT) compilation and batched rollouts for
real-time control with online domain randomization. Through experiments on
various challenging robotic tasks, ranging from dexterous in-hand manipulation
to humanoid locomotion, we demonstrate that MTP outperforms standard MPC and
evolutionary strategy baselines in task success and control robustness. Design
and sensitivity ablations confirm the effectiveness of MTP tensor sampling
structure, spline interpolation choices, and mixing strategy. Altogether, MTP
offers a scalable framework for robust exploration in model-based planning and
control.

</details>

### [213] [Optimizing Indoor Farm Monitoring Efficiency Using UAV: Yield Estimation in a GNSS-Denied Cherry Tomato Greenhouse](https://arxiv.org/abs/2505.00995)
*Taewook Park,Jinwoo Lee,Hyondong Oh,Won-Jae Yun,Kyu-Wha Lee*

Main category: cs.RO

TLDR: 论文提出了一种轻量级无人机系统，用于温室中的番茄产量估计，解决了地面机器人在温室中部署的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着农业劳动力减少和成本上升，机器人产量估计变得重要。地面机器人在温室中部署受限，因此需要更高效的解决方案。

Method: 开发了配备RGB-D相机、3D LiDAR和IMU传感器的无人机，采用LiDAR-惯性里程计算法导航，并使用3D多目标跟踪算法估计番茄数量和重量。

Result: 在收获行数据集中，系统计数准确率达94.4%，重量估计准确率达87.5%；在生长行数据集中，定性分析了遮挡情况下的跟踪性能。

Conclusion: 无人机系统在温室产量估计中具有潜力，未来需进一步研究强遮挡环境下的感知问题。

Abstract: As the agricultural workforce declines and labor costs rise, robotic yield
estimation has become increasingly important. While unmanned ground vehicles
(UGVs) are commonly used for indoor farm monitoring, their deployment in
greenhouses is often constrained by infrastructure limitations, sensor
placement challenges, and operational inefficiencies. To address these issues,
we develop a lightweight unmanned aerial vehicle (UAV) equipped with an RGB-D
camera, a 3D LiDAR, and an IMU sensor. The UAV employs a LiDAR-inertial
odometry algorithm for precise navigation in GNSS-denied environments and
utilizes a 3D multi-object tracking algorithm to estimate the count and weight
of cherry tomatoes. We evaluate the system using two dataset: one from a
harvesting row and another from a growing row. In the harvesting-row dataset,
the proposed system achieves 94.4\% counting accuracy and 87.5\% weight
estimation accuracy within a 13.2-meter flight completed in 10.5 seconds. For
the growing-row dataset, which consists of occluded unripened fruits, we
qualitatively analyze tracking performance and highlight future research
directions for improving perception in greenhouse with strong occlusions. Our
findings demonstrate the potential of UAVs for efficient robotic yield
estimation in commercial greenhouses.

</details>

### [214] [NeuroLoc: Encoding Navigation Cells for 6-DOF Camera Localization](https://arxiv.org/abs/2505.01113)
*Xun Li,Jian Yang,Fenli Jia,Muyu Wang,Qi Wu,Jun Wu,Jinpeng Mi,Jilin Hu,Peidong Liang,Xuan Tang,Ke Li,Xiong You,Xian Wei*

Main category: cs.RO

TLDR: 论文提出了一种基于神经生物学机制的相机定位方法NeuroLoc，通过模拟大脑导航机制解决场景模糊和方向恢复问题，并在实验中验证了其鲁棒性和性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决自主导航中相机定位面临的场景模糊、环境干扰和动态物体变换问题。

Method: 设计了基于位置细胞的Hebbian学习模块、方向细胞启发的多头注意力嵌入，以及3D网格中心预测模块。

Result: 在室内外数据集上验证了NeuroLoc的鲁棒性和单图像姿态回归性能提升。

Conclusion: NeuroLoc通过模拟生物导航机制，有效提升了复杂环境下的相机定位性能。

Abstract: Recently, camera localization has been widely adopted in autonomous robotic
navigation due to its efficiency and convenience. However, autonomous
navigation in unknown environments often suffers from scene ambiguity,
environmental disturbances, and dynamic object transformation in camera
localization. To address this problem, inspired by the biological brain
navigation mechanism (such as grid cells, place cells, and head direction
cells), we propose a novel neurobiological camera location method, namely
NeuroLoc. Firstly, we designed a Hebbian learning module driven by place cells
to save and replay historical information, aiming to restore the details of
historical representations and solve the issue of scene fuzziness. Secondly, we
utilized the head direction cell-inspired internal direction learning as
multi-head attention embedding to help restore the true orientation in similar
scenes. Finally, we added a 3D grid center prediction in the pose regression
module to reduce the final wrong prediction. We evaluate the proposed NeuroLoc
on commonly used benchmark indoor and outdoor datasets. The experimental
results show that our NeuroLoc can enhance the robustness in complex
environments and improve the performance of pose regression by using only a
single image.

</details>

### [215] [ViSA-Flow: Accelerating Robot Skill Learning via Large-Scale Video Semantic Action Flow](https://arxiv.org/abs/2505.01288)
*Changhe Chen,Quantao Yang,Xiaohao Xu,Nima Fazeli,Olov Andersson*

Main category: cs.RO

TLDR: ViSA-Flow框架通过自监督学习从大规模视频数据中提取语义动作流，作为机器人模仿学习的中间表示，显著降低了机器人学习复杂操作技能的成本。


<details>
  <summary>Details</summary>
Motivation: 解决机器人学习复杂操作技能时数据收集成本高的问题，借鉴人类通过观察学习的高效方式。

Method: 提出语义动作流作为核心表示，通过自监督学习从人类视频数据中预训练生成模型，再通过少量机器人演示微调。

Result: 在CALVIN基准测试和实际任务中表现优异，尤其在低数据情况下优于现有方法。

Conclusion: ViSA-Flow通过从人类视频中迁移知识，显著提升了机器人学习的效率和性能。

Abstract: One of the central challenges preventing robots from acquiring complex
manipulation skills is the prohibitive cost of collecting large-scale robot
demonstrations. In contrast, humans are able to learn efficiently by watching
others interact with their environment. To bridge this gap, we introduce
semantic action flow as a core intermediate representation capturing the
essential spatio-temporal manipulator-object interactions, invariant to
superficial visual differences. We present ViSA-Flow, a framework that learns
this representation self-supervised from unlabeled large-scale video data.
First, a generative model is pre-trained on semantic action flows automatically
extracted from large-scale human-object interaction video data, learning a
robust prior over manipulation structure. Second, this prior is efficiently
adapted to a target robot by fine-tuning on a small set of robot demonstrations
processed through the same semantic abstraction pipeline. We demonstrate
through extensive experiments on the CALVIN benchmark and real-world tasks that
ViSA-Flow achieves state-of-the-art performance, particularly in low-data
regimes, outperforming prior methods by effectively transferring knowledge from
human video observation to robotic execution. Videos are available at
https://visaflow-web.github.io/ViSAFLOW.

</details>

### [216] [Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures](https://arxiv.org/abs/2505.00779)
*Junwon Seo,Kensuke Nakamura,Andrea Bajcsy*

Main category: cs.RO

TLDR: 论文提出了一种基于不确定性感知的潜在安全过滤器，用于机器人系统，通过结合世界模型的认知不确定性来识别未知危险，并通过一致性预测校准阈值，确保安全。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成世界模型的安全控制方法难以覆盖所有安全关键场景，导致潜在安全过滤器可能遗漏危险。

Method: 利用世界模型的认知不确定性作为代理识别未知危险，通过一致性预测校准不确定性阈值，并在增强状态空间中执行可达性分析。

Result: 在仿真和硬件实验中，该方法能预检测潜在不安全场景，并可靠地提出安全动作。

Conclusion: 不确定性感知的安全过滤器能有效保护机器人系统免受已知和未知危险的影响。

Abstract: Recent advances in generative world models have enabled classical safe
control methods, such as Hamilton-Jacobi (HJ) reachability, to generalize to
complex robotic systems operating directly from high-dimensional sensor
observations. However, obtaining comprehensive coverage of all safety-critical
scenarios during world model training is extremely challenging. As a result,
latent safety filters built on top of these models may miss novel hazards and
even fail to prevent known ones, overconfidently misclassifying risky
out-of-distribution (OOD) situations as safe. To address this, we introduce an
uncertainty-aware latent safety filter that proactively steers robots away from
both known and unseen failures. Our key idea is to use the world model's
epistemic uncertainty as a proxy for identifying unseen potential hazards. We
propose a principled method to detect OOD world model predictions by
calibrating an uncertainty threshold via conformal prediction. By performing
reachability analysis in an augmented state space-spanning both the latent
representation and the epistemic uncertainty-we synthesize a latent safety
filter that can reliably safeguard arbitrary policies from both known and
unseen safety hazards. In simulation and hardware experiments on vision-based
control tasks with a Franka manipulator, we show that our uncertainty-aware
safety filter preemptively detects potential unsafe scenarios and reliably
proposes safe, in-distribution actions. Video results can be found on the
project website at https://cmu-intentlab.github.io/UNISafe

</details>

### [217] [FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft Research](https://arxiv.org/abs/2505.01383)
*Yan Miao,Will Shen,Hang Cui,Sayan Mitra*

Main category: cs.RO

TLDR: FalconWing是一个开源的超轻量固定翼平台，用于自主性研究。通过视觉控制策略实现了自主着陆，并采用了一种新颖的实-仿-实学习方法。


<details>
  <summary>Details</summary>
Motivation: 为自主性研究提供一个轻量、开源且易于部署的硬件平台，并探索纯视觉控制的可行性。

Method: 1. 使用3D高斯泼溅构建逼真仿真环境；2. 从视觉估计的飞行数据中识别非线性动力学；3. 通过仿真模仿学习训练多模态Vision Transformer策略。

Result: 在硬件平台上零样本部署时，视觉自主着陆成功率达到80%。

Conclusion: FalconWing及其开源组件为自主性研究提供了实用工具，并展示了纯视觉控制的潜力。

Abstract: We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing
platform for autonomy research. The hardware platform integrates a small
camera, a standard airframe, offboard computation, and radio communication for
manual overrides. We demonstrate FalconWing's capabilities by developing and
deploying a purely vision-based control policy for autonomous landing (without
IMU or motion capture) using a novel real-to-sim-to-real learning approach. Our
learning approach: (1) constructs a photorealistic simulation environment via
3D Gaussian splatting trained on real-world images; (2) identifies nonlinear
dynamics from vision-estimated real-flight data; and (3) trains a multi-modal
Vision Transformer (ViT) policy through simulation-only imitation learning. The
ViT architecture fuses single RGB image with the history of control actions via
self-attention, preserving temporal context while maintaining real-time 20 Hz
inference. When deployed zero-shot on the hardware platform, this policy
achieves an 80% success rate in vision-based autonomous landings. Together with
the hardware specifications, we also open-source the system dynamics, the
software for photorealistic simulator and the learning approach.

</details>

### [218] [SIME: Enhancing Policy Self-Improvement with Modal-level Exploration](https://arxiv.org/abs/2505.01396)
*Yang Jin,Jun Lv,Wenye Yu,Hongjie Fang,Yong-Lu Li,Cewu Lu*

Main category: cs.RO

TLDR: 论文提出了一种机器人自我改进的方法，通过模态级探索和数据选择，提高机器人的学习效率和能力。


<details>
  <summary>Details</summary>
Motivation: 机器人通过与环境的交互实现自我改进，但现有方法容易重复已有能力，难以生成新数据。

Method: 引入模态级探索机制，生成多样化的交互数据，并选择高质量片段用于学习。

Result: 在仿真和真实实验中成功实现了有效的机器人自我改进。

Conclusion: 该方法能以更低成本开发更鲁棒、高成功率的机器人控制策略。

Abstract: Self-improvement requires robotic systems to initially learn from
human-provided data and then gradually enhance their capabilities through
interaction with the environment. This is similar to how humans improve their
skills through continuous practice. However, achieving effective
self-improvement is challenging, primarily because robots tend to repeat their
existing abilities during interactions, often failing to generate new, valuable
data for learning. In this paper, we identify the key to successful
self-improvement: modal-level exploration and data selection. By incorporating
a modal-level exploration mechanism during policy execution, the robot can
produce more diverse and multi-modal interactions. At the same time, we select
the most valuable trials and high-quality segments from these interactions for
learning. We successfully demonstrate effective robot self-improvement on both
simulation benchmarks and real-world experiments. The capability for
self-improvement will enable us to develop more robust and high-success-rate
robotic control strategies at a lower cost. Our code and experiment scripts are
available at https://ericjin2002.github.io/SIME/

</details>

<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [219] [Dynamic and Distributed Routing in IoT Networks based on Multi-Objective Q-Learning](https://arxiv.org/abs/2505.00918)
*Shubham Vaishnav,Praveen Kumar Donta,Sindri Magnússon*

Main category: cs.DC

TLDR: 论文提出了一种基于多目标Q学习的动态分布式路由算法，以适应物联网中实时变化的优先级需求。


<details>
  <summary>Details</summary>
Motivation: 物联网应用中存在动态优先级需求，传统静态优化方法无法满足，需要一种能实时适应变化的解决方案。

Method: 结合多目标优化和Q学习，提出动态分布式路由算法，并引入贪婪插值策略处理突发偏好变化。

Result: 仿真结果表明，该算法在整体奖励、能效和包传递率等指标上优于现有方法。

Conclusion: 该算法能快速适应动态偏好，为物联网路由提供了高效解决方案。

Abstract: The last few decades have witnessed a rapid increase in IoT devices owing to
their wide range of applications, such as smart healthcare monitoring systems,
smart cities, and environmental monitoring. A critical task in IoT networks is
sensing and transmitting information over the network. The IoT nodes gather
data by sensing the environment and then transmit this data to a destination
node via multi-hop communication, following some routing protocols. These
protocols are usually designed to optimize possibly contradictory objectives,
such as maximizing packet delivery ratio and energy efficiency. While most
literature has focused on optimizing a static objective that remains unchanged,
many real-world IoT applications require adapting to rapidly shifting
priorities. For example, in monitoring systems, some transmissions are
time-critical and require a high priority on low latency, while other
transmissions are less urgent and instead prioritize energy efficiency. To meet
such dynamic demands, we propose novel dynamic and distributed routing based on
multiobjective Q-learning that can adapt to changes in preferences in
real-time. Our algorithm builds on ideas from both multi-objective optimization
and Q-learning. We also propose a novel greedy interpolation policy scheme to
take near-optimal decisions for unexpected preference changes. The proposed
scheme can approximate and utilize the Pareto-efficient solutions for dynamic
preferences, thus utilizing past knowledge to adapt to unpredictable
preferences quickly during runtime. Simulation results show that the proposed
scheme outperforms state-of-the-art algorithms for various exploration
strategies, preference variation patterns, and important metrics like overall
reward, energy efficiency, and packet delivery ratio.

</details>

<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [220] [EnviKal-Loc: Sub-10m Indoor LoRaWAN Localization using an Environmental-Aware Path Loss and Adaptive RSSI Smoothing](https://arxiv.org/abs/2505.01185)
*Nahshon Mokua Obiri,Kristof Van Laerhoven*

Main category: cs.NI

TLDR: 论文提出了一种结合自适应滤波和扩展对数距离多墙路径损耗与阴影（PLS）模型的轻量级方法，用于提高LoRaWAN室内定位精度，在动态环境中实现了5.81米的平均绝对误差。


<details>
  <summary>Details</summary>
Motivation: LoRaWAN技术在大规模物联网部署中具有广泛覆盖优势，但在复杂环境条件下实现亚10米精度的室内定位仍具挑战性。

Method: 结合自适应卡尔曼滤波和扩展对数距离多墙路径损耗与阴影模型，引入LoRaWAN参数（RSSI、频率、SNR）和动态环境指标（温度、湿度等）。

Result: 提出的MWM-EP-KF模型平均绝对误差为5.81米，优于基线模型（17.98米）和环境参数增强模型（10.56米）。

Conclusion: 该方法为动态环境中的精确室内LoRaWAN定位提供了高效且可解释的解决方案。

Abstract: LoRaWAN technology's extensive coverage positions it as a strong contender
for large-scale IoT deployments. However, achieving sub-10 m accuracy in indoor
localization remains challenging due to complex environmental conditions,
multipath fading, and transient obstructions. This paper proposes a lightweight
but robust approach combining adaptive filtering with an extended log-distance,
multi-wall path loss and shadowing (PLS) model. Our methodology augments
conventional models with critical LoRaWAN parameters (received signal strength
indicator (RSSI), frequency, and signal-to-noise ratio (SNR)) and dynamic
environmental indicators (temperature, humidity, carbon dioxide, particulate
matter, and barometric pressure). An adaptive Kalman filter reduces RSSI
fluctuations, isolating persistent trends from momentary noise. Using a
six-month dataset of 1,328,334 field measurements, we evaluate three models:
the baseline COST 231 multi-wall model (MWM), the baseline model augmented with
environmental parameters (MWM-EP), and a forward-only adaptive Kalman-filtered
RSSI version of the latter (MWM-EP-KF). Results confirm that the MWM-EP-KF
achieves a mean absolute error (MAE) of 5.81 m, outperforming both the MWM-EP
(10.56 m) and the baseline MWM framework (17.98 m). Environmental augmentation
reduces systematic errors by 41.22%, while Kalman filtering significantly
enhances robustness under high RSSI volatility by 42.63%, on average across all
devices. These findings present an interpretable, efficient solution for
precise indoor LoRaWAN localization in dynamically changing environments.

</details>

<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [221] [Learning Low-Dimensional Embeddings for Black-Box Optimization](https://arxiv.org/abs/2505.01112)
*Riccardo Busetto,Manas Mejari,Marco Forgione,Alberto Bemporad,Dario Piga*

Main category: eess.SY

TLDR: 提出一种基于元学习的方法，通过预计算降维流形来优化高维黑盒问题。


<details>
  <summary>Details</summary>
Motivation: 解决黑盒优化在高维问题和有限试验预算下的困难。

Method: 利用元学习预计算降维流形，在新问题实例的降维空间中进行黑盒优化。

Result: 有效减少了寻找近似最优解所需的计算量。

Conclusion: 该方法为高维黑盒优化提供了一种高效解决方案。

Abstract: When gradient-based methods are impractical, black-box optimization (BBO)
provides a valuable alternative. However, BBO often struggles with
high-dimensional problems and limited trial budgets. In this work, we propose a
novel approach based on meta-learning to pre-compute a reduced-dimensional
manifold where optimal points lie for a specific class of optimization
problems. When optimizing a new problem instance sampled from the class,
black-box optimization is carried out in the reduced-dimensional space,
effectively reducing the effort required for finding near-optimal solutions.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [222] [A Survey on 3D Reconstruction Techniques in Plant Phenotyping: From Classical Methods to Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and Beyond](https://arxiv.org/abs/2505.00737)
*Jiajia Li,Xinda Qi,Seyed Hamidreza Nabaei,Meiqi Liu,Dong Chen,Xin Zhang,Xunyuan Yin,Zhaojian Li*

Main category: eess.IV

TLDR: 综述了3D重建技术在植物表型分析中的应用，包括经典方法、NeRF和3DGS，探讨了各自的优势、局限及未来前景。


<details>
  <summary>Details</summary>
Motivation: 植物表型分析对精准农业和作物改良至关重要，3D重建技术为自动化表型分析提供了潜力。

Method: 回顾了经典3D重建方法、NeRF和3DGS的技术原理、应用及性能。

Result: 经典方法简单灵活但面临数据密度和噪声问题；NeRF高质量但计算成本高；3DGS在效率和扩展性上具潜力。

Conclusion: 多种3D重建技术各有优劣，未来需结合其优势推动自动化植物表型分析的发展。

Abstract: Plant phenotyping plays a pivotal role in understanding plant traits and
their interactions with the environment, making it crucial for advancing
precision agriculture and crop improvement. 3D reconstruction technologies have
emerged as powerful tools for capturing detailed plant morphology and
structure, offering significant potential for accurate and automated
phenotyping. This paper provides a comprehensive review of the 3D
reconstruction techniques for plant phenotyping, covering classical
reconstruction methods, emerging Neural Radiance Fields (NeRF), and the novel
3D Gaussian Splatting (3DGS) approach. Classical methods, which often rely on
high-resolution sensors, are widely adopted due to their simplicity and
flexibility in representing plant structures. However, they face challenges
such as data density, noise, and scalability. NeRF, a recent advancement,
enables high-quality, photorealistic 3D reconstructions from sparse viewpoints,
but its computational cost and applicability in outdoor environments remain
areas of active research. The emerging 3DGS technique introduces a new paradigm
in reconstructing plant structures by representing geometry through Gaussian
primitives, offering potential benefits in both efficiency and scalability. We
review the methodologies, applications, and performance of these approaches in
plant phenotyping and discuss their respective strengths, limitations, and
future prospects (https://github.com/JiajiaLi04/3D-Reconstruction-Plants).
Through this review, we aim to provide insights into how these diverse 3D
reconstruction techniques can be effectively leveraged for automated and
high-throughput plant phenotyping, contributing to the next generation of
agricultural technology.

</details>

### [223] [Leveraging Depth and Attention Mechanisms for Improved RGB Image Inpainting](https://arxiv.org/abs/2505.00735)
*Jin Hyun Park,Harine Choi,Praewa Pitiphat*

Main category: eess.IV

TLDR: 提出了一种结合RGB和深度图像的双编码器架构，通过注意力机制融合特征，显著提升了图像修复质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖RGB图像，忽略了深度信息对空间和结构理解的重要性。

Method: 采用双编码器分别处理RGB和深度图像，通过注意力机制融合特征，并使用两种掩码策略测试模型鲁棒性。

Result: 结合深度信息的模型在定性和定量评估中均优于基线，注意力机制进一步提升了性能。

Conclusion: 深度信息的引入和注意力机制的应用显著提升了图像修复的准确性和上下文感知能力。

Abstract: Existing deep learning-based image inpainting methods typically rely on
convolutional networks with RGB images to reconstruct images. However, relying
exclusively on RGB images may neglect important depth information, which plays
a critical role in understanding the spatial and structural context of a scene.
Just as human vision leverages stereo cues to perceive depth, incorporating
depth maps into the inpainting process can enhance the model's ability to
reconstruct images with greater accuracy and contextual awareness. In this
paper, we propose a novel approach that incorporates both RGB and depth images
for enhanced image inpainting. Our models employ a dual encoder architecture,
where one encoder processes the RGB image and the other handles the depth
image. The encoded features from both encoders are then fused in the decoder
using an attention mechanism, effectively integrating the RGB and depth
representations. We use two different masking strategies, line and square, to
test the robustness of the model under different types of occlusions. To
further analyze the effectiveness of our approach, we use Gradient-weighted
Class Activation Mapping (Grad-CAM) visualizations to examine the regions of
interest the model focuses on during inpainting. We show that incorporating
depth information alongside the RGB image significantly improves the
reconstruction quality. Through both qualitative and quantitative comparisons,
we demonstrate that the depth-integrated model outperforms the baseline, with
attention mechanisms further enhancing inpainting performance, as evidenced by
multiple evaluation metrics and visualization.

</details>

### [224] [Can Foundation Models Really Segment Tumors? A Benchmarking Odyssey in Lung CT Imaging](https://arxiv.org/abs/2505.01239)
*Elena Mulero Ayllón,Massimiliano Mantegna,Linlin Shen,Paolo Soda,Valerio Guarrasi,Matteo Tortora*

Main category: eess.IV

TLDR: 研究比较了多种深度学习模型在肺肿瘤分割中的表现，发现基础模型（如MedSAM~2）在准确性和计算效率上优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 肺肿瘤分割的准确性对诊断和治疗至关重要，但肿瘤形态、大小和位置的复杂性为自动化分割带来挑战。

Method: 对U-Net、DeepLabV3、nnUNet及基础模型（MedSAM、MedSAM~2）进行基准测试，评估其在少量样本学习和微调下的表现。

Result: 基础模型（尤其是MedSAM~2）在分割准确性和计算效率上显著优于传统模型。

Conclusion: 基础模型在肺肿瘤分割中具有潜力，可提升临床工作流程和患者预后。

Abstract: Accurate lung tumor segmentation is crucial for improving diagnosis,
treatment planning, and patient outcomes in oncology. However, the complexity
of tumor morphology, size, and location poses significant challenges for
automated segmentation. This study presents a comprehensive benchmarking
analysis of deep learning-based segmentation models, comparing traditional
architectures such as U-Net and DeepLabV3, self-configuring models like nnUNet,
and foundation models like MedSAM, and MedSAM~2. Evaluating performance across
two lung tumor segmentation datasets, we assess segmentation accuracy and
computational efficiency under various learning paradigms, including few-shot
learning and fine-tuning. The results reveal that while traditional models
struggle with tumor delineation, foundation models, particularly MedSAM~2,
outperform them in both accuracy and computational efficiency. These findings
underscore the potential of foundation models for lung tumor segmentation,
highlighting their applicability in improving clinical workflows and patient
outcomes.

</details>

### [225] [XeMap: Contextual Referring in Large-Scale Remote Sensing Environments](https://arxiv.org/abs/2505.00738)
*Yuxi Li,Lu Si,Yujie Hou,Chengaung Liu,Bin Li,Hongjian Fang,Jun Zhang*

Main category: eess.IV

TLDR: 论文提出XeMap任务和XeMap-Network架构，专注于遥感图像中文本引用区域的细粒度定位，解决了现有方法忽略中尺度语义实体的问题。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像处理方法（如图像级描述/检索和对象级检测/分割）未能有效捕捉中尺度语义实体，限制了大规模场景的解读。

Method: 提出XeMap-Network架构，结合自注意力和交叉注意力机制，并引入分层多尺度语义对齐模块（HMSA），实现文本与图像的多模态匹配。

Result: XeMap-Network在零样本设置下优于现有方法，展示了其在遥感图像中精确映射引用区域的有效性。

Conclusion: XeMap任务和XeMap-Network为遥感图像的中尺度语义实体定位提供了新思路，填补了相关数据集的空白。

Abstract: Advancements in remote sensing (RS) imagery have provided high-resolution
detail and vast coverage, yet existing methods, such as image-level
captioning/retrieval and object-level detection/segmentation, often fail to
capture mid-scale semantic entities essential for interpreting large-scale
scenes. To address this, we propose the conteXtual referring Map (XeMap) task,
which focuses on contextual, fine-grained localization of text-referred regions
in large-scale RS scenes. Unlike traditional approaches, XeMap enables precise
mapping of mid-scale semantic entities that are often overlooked in image-level
or object-level methods. To achieve this, we introduce XeMap-Network, a novel
architecture designed to handle the complexities of pixel-level cross-modal
contextual referring mapping in RS. The network includes a fusion layer that
applies self- and cross-attention mechanisms to enhance the interaction between
text and image embeddings. Furthermore, we propose a Hierarchical Multi-Scale
Semantic Alignment (HMSA) module that aligns multiscale visual features with
the text semantic vector, enabling precise multimodal matching across
large-scale RS imagery. To support XeMap task, we provide a novel, annotated
dataset, XeMap-set, specifically tailored for this task, overcoming the lack of
XeMap datasets in RS imagery. XeMap-Network is evaluated in a zero-shot setting
against state-of-the-art methods, demonstrating superior performance. This
highlights its effectiveness in accurately mapping referring regions and
providing valuable insights for interpreting large-scale RS environments.

</details>

<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [226] [Multivariate Conformal Selection](https://arxiv.org/abs/2505.00917)
*Tian Bai,Yue Zhao,Xiang Yu,Archer Y. Yang*

Main category: stat.ME

TLDR: 论文提出了一种多变量响应场景下的广义方法mCS，解决了传统CS方法仅适用于单变量响应和标量标准的限制。


<details>
  <summary>Details</summary>
Motivation: 在大规模数据集中选择高质量候选者（如药物发现、精准医疗和大型语言模型对齐）需要更灵活的方法，传统CS方法无法满足多变量需求。

Method: 提出了mCS方法，引入区域单调性和多变量非一致性分数构建p值，并开发了两种变体：mCS-dist（基于距离）和mCS-learn（通过可微分优化学习最优分数）。

Result: 实验表明，mCS在保持FDR控制的同时显著提高了选择能力。

Conclusion: mCS是一个适用于多变量选择任务的稳健框架。

Abstract: Selecting high-quality candidates from large datasets is critical in
applications such as drug discovery, precision medicine, and alignment of large
language models (LLMs). While Conformal Selection (CS) provides rigorous
uncertainty quantification, it is limited to univariate responses and scalar
criteria. To address this issue, we propose Multivariate Conformal Selection
(mCS), a generalization of CS designed for multivariate response settings. Our
method introduces regional monotonicity and employs multivariate nonconformity
scores to construct conformal p-values, enabling finite-sample False Discovery
Rate (FDR) control. We present two variants: mCS-dist, using distance-based
scores, and mCS-learn, which learns optimal scores via differentiable
optimization. Experiments on simulated and real-world datasets demonstrate that
mCS significantly improves selection power while maintaining FDR control,
establishing it as a robust framework for multivariate selection tasks.

</details>

### [227] [Q-Learning with Clustered-SMART (cSMART) Data: Examining Moderators in the Construction of Clustered Adaptive Interventions](https://arxiv.org/abs/2505.00822)
*Yao Song,Kelly Speth,Amy Kilbourne,Andrew Quanbeck,Daniel Almirall,Lu Wang*

Main category: stat.ME

TLDR: 提出了一种基于cSMART数据的聚类Q学习框架，用于评估候选定制变量在构建最优cAI中的效用，并通过模拟验证其性能。


<details>
  <summary>Details</summary>
Motivation: 为改进聚类内个体结果，需要开发一种方法评估候选定制变量在构建最优cAI中的效用。

Method: 采用聚类Q学习框架和M-out-of-N聚类自助法，构建置信区间以评估因果效应调节函数。

Result: 模拟显示该方法在不同非规则条件下表现良好，并应用于ADEPT数据集。

Conclusion: 该方法能可靠评估候选定制变量，为构建最优cAI提供支持。

Abstract: A clustered adaptive intervention (cAI) is a pre-specified sequence of
decision rules that guides practitioners on how best - and based on which
measures - to tailor cluster-level intervention to improve outcomes at the
level of individuals within the clusters. A clustered sequential multiple
assignment randomized trial (cSMART) is a type of trial that is used to inform
the empirical development of a cAI. The most common type of secondary aim in a
cSMART focuses on assessing causal effect moderation by candidate tailoring
variables. We introduce a clustered Q-learning framework with the M-out-of-N
Cluster Bootstrap using data from a cSMART to evaluate whether a set of
candidate tailoring variables may be useful in defining an optimal cAI. This
approach could construct confidence intervals (CI) with near-nominal coverage
to assess parameters indexing the causal effect moderation function.
Specifically, it allows reliable inferences concerning the utility of candidate
tailoring variables in constructing a cAI that maximizes a mean end-of-study
outcome even when "non-regularity", a well-known challenge exists. Simulations
demonstrate the numerical performance of the proposed method across varying
non-regularity conditions and investigate the impact of varying number of
clusters and intra-cluster correlation coefficient on CI coverage. Methods are
applied on ADEPT dataset to inform the construction of a clinic-level cAI for
improving evidence-based practice in treating mood disorders.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [228] [Preserving Privacy and Utility in LLM-Based Product Recommendations](https://arxiv.org/abs/2505.00951)
*Tina Khezresmaeilzadeh,Jiang Zhang,Dimitrios Andreadis,Konstantinos Psounis*

Main category: cs.IR

TLDR: 提出了一种混合隐私保护推荐框架，通过分离敏感与非敏感数据，仅共享非敏感数据至云端，同时设计本地去混淆模块恢复敏感推荐，平衡隐私与推荐质量。


<details>
  <summary>Details</summary>
Motivation: 解决LLM推荐系统中用户数据隐私问题，避免敏感数据直接传输至云端。

Method: 分离敏感与非敏感数据，仅共享非敏感数据至云端；设计本地去混淆模块恢复敏感推荐。

Result: 在真实电商数据集上，推荐效果接近全数据共享系统，隐私保护显著提升；HR@10和类别分布对齐优于仅混淆技术。

Conclusion: 该方法在消费级硬件上高效运行，为隐私保护的LLM推荐系统提供了实用解决方案。

Abstract: Large Language Model (LLM)-based recommendation systems leverage powerful
language models to generate personalized suggestions by processing user
interactions and preferences. Unlike traditional recommendation systems that
rely on structured data and collaborative filtering, LLM-based models process
textual and contextual information, often using cloud-based infrastructure.
This raises privacy concerns, as user data is transmitted to remote servers,
increasing the risk of exposure and reducing control over personal information.
To address this, we propose a hybrid privacy-preserving recommendation
framework which separates sensitive from nonsensitive data and only shares the
latter with the cloud to harness LLM-powered recommendations. To restore lost
recommendations related to obfuscated sensitive data, we design a
de-obfuscation module that reconstructs sensitive recommendations locally.
Experiments on real-world e-commerce datasets show that our framework achieves
almost the same recommendation utility with a system which shares all data with
an LLM, while preserving privacy to a large extend. Compared to
obfuscation-only techniques, our approach improves HR@10 scores and category
distribution alignment, offering a better balance between privacy and
recommendation quality. Furthermore, our method runs efficiently on
consumer-grade hardware, making privacy-aware LLM-based recommendation systems
practical for real-world use.

</details>

### [229] [Towards Explainable Temporal User Profiling with LLMs](https://arxiv.org/abs/2505.00886)
*Milad Sabouri,Masoud Mansoury,Kun Lin,Bamshad Mobasher*

Main category: cs.IR

TLDR: 利用大语言模型（LLMs）生成用户交互历史的自然语言摘要，区分短期与长期偏好，提升推荐系统的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统用户画像方法（如平均项目嵌入）忽略了用户兴趣的动态性和复杂性，尤其是短期与长期偏好的交互作用。

Method: 通过LLMs生成用户交互历史的自然语言摘要，结合注意力机制动态融合短期和长期嵌入，形成全面的用户表示。

Result: 实验表明，该方法在多个基准上提高了推荐准确性，并支持通过自然语言摘要和注意力权重提供可解释的推荐理由。

Conclusion: 该方法不仅提升了推荐性能，还增强了推荐系统的透明度和可解释性。

Abstract: Accurately modeling user preferences is vital not only for improving
recommendation performance but also for enhancing transparency in recommender
systems. Conventional user profiling methods, such as averaging item
embeddings, often overlook the evolving, nuanced nature of user interests,
particularly the interplay between short-term and long-term preferences. In
this work, we leverage large language models (LLMs) to generate natural
language summaries of users' interaction histories, distinguishing recent
behaviors from more persistent tendencies. Our framework not only models
temporal user preferences but also produces natural language profiles that can
be used to explain recommendations in an interpretable manner. These textual
profiles are encoded via a pre-trained model, and an attention mechanism
dynamically fuses the short-term and long-term embeddings into a comprehensive
user representation. Beyond boosting recommendation accuracy over multiple
baselines, our approach naturally supports explainability: the interpretable
text summaries and attention weights can be exposed to end users, offering
insights into why specific items are suggested. Experiments on real-world
datasets underscore both the performance gains and the promise of generating
clearer, more transparent justifications for content-based recommendations.

</details>

### [230] [Enhancing User Sequence Modeling through Barlow Twins-based Self-Supervised Learning](https://arxiv.org/abs/2505.00953)
*Yuhan Liu,Lin Ning,Neo Wu,Karan Singhal,Philip Andrew Mansfield,Devora Berlowitz,Sushant Prakash,Bradley Green*

Main category: cs.IR

TLDR: 论文提出了一种基于Barlow Twins的自监督学习方法，用于用户序列建模，减少了对大量负样本的需求，并在多个数据集上表现优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 用户序列建模对推荐系统至关重要，但缺乏标记数据是一个主要挑战。现有自监督学习方法依赖大量负样本，计算成本高且不现实。

Method: 通过改进Barlow Twins方法，结合适当的增强技术，减少对大批量负样本的需求，实现小批量高效学习。

Result: 在MovieLens和Yelp数据集上，新方法在三个下游任务中准确率提升8%-20%，优于传统双编码器模型。

Conclusion: 该方法在标记数据稀缺和负样本有限的情况下，能有效提取用户序列信息，具有实际应用价值。

Abstract: User sequence modeling is crucial for modern large-scale recommendation
systems, as it enables the extraction of informative representations of users
and items from their historical interactions. These user representations are
widely used for a variety of downstream tasks to enhance users' online
experience. A key challenge for learning these representations is the lack of
labeled training data. While self-supervised learning (SSL) methods have
emerged as a promising solution for learning representations from unlabeled
data, many existing approaches rely on extensive negative sampling, which can
be computationally expensive and may not always be feasible in real-world
scenario. In this work, we propose an adaptation of Barlow Twins, a
state-of-the-art SSL methods, to user sequence modeling by incorporating
suitable augmentation methods. Our approach aims to mitigate the need for large
negative sample batches, enabling effective representation learning with
smaller batch sizes and limited labeled data. We evaluate our method on the
MovieLens-1M, MovieLens-20M, and Yelp datasets, demonstrating that our method
consistently outperforms the widely-used dual encoder model across three
downstream tasks, achieving an 8%-20% improvement in accuracy. Our findings
underscore the effectiveness of our approach in extracting valuable
sequence-level information for user modeling, particularly in scenarios where
labeled data is scarce and negative examples are limited.

</details>

<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [231] [Quantum Support Vector Regression for Robust Anomaly Detection](https://arxiv.org/abs/2505.01012)
*Kilian Tscharke,Maximilian Wendlinger,Sebastian Issel,Pascal Debus*

Main category: quant-ph

TLDR: 该论文研究了量子机器学习（特别是量子核方法）在异常检测中的应用，通过实验验证了量子支持向量回归（QSVR）在真实量子硬件上的性能，并分析了量子噪声和对抗攻击对模型的影响。


<details>
  <summary>Details</summary>
Motivation: 异常检测在数据分析和IT安全中至关重要，而量子机器学习为大规模数据异常检测提供了新方法。本研究旨在探索量子核方法在异常检测中的潜力。

Method: 研究基于量子支持向量回归（QSVR），在IBM量子硬件上对11个数据集进行了全面基准测试，并分析了量子噪声和对抗攻击的影响。

Result: QSVR在部分数据集上表现优于无噪声模拟，且对某些量子噪声（如退极化、相位阻尼等）具有鲁棒性，但对振幅阻尼和校准噪声敏感。此外，QSVR易受对抗攻击，噪声未提升其对抗鲁棒性。

Conclusion: 量子核方法在异常检测中具有潜力，但需进一步研究以解决噪声敏感性和对抗攻击脆弱性问题。

Abstract: Anomaly Detection (AD) is critical in data analysis, particularly within the
domain of IT security. In recent years, Machine Learning (ML) algorithms have
emerged as a powerful tool for AD in large-scale data. In this study, we
explore the potential of quantum ML approaches, specifically quantum kernel
methods, for the application to robust AD. We build upon previous work on
Quantum Support Vector Regression (QSVR) for semisupervised AD by conducting a
comprehensive benchmark on IBM quantum hardware using eleven datasets. Our
results demonstrate that QSVR achieves strong classification performance and
even outperforms the noiseless simulation on two of these datasets. Moreover,
we investigate the influence of - in the NISQ-era inevitable - quantum noise on
the performance of the QSVR. Our findings reveal that the model exhibits
robustness to depolarizing, phase damping, phase flip, and bit flip noise,
while amplitude damping and miscalibration noise prove to be more disruptive.
Finally, we explore the domain of Quantum Adversarial Machine Learning and
demonstrate that QSVR is highly vulnerable to adversarial attacks and that
noise does not improve the adversarial robustness of the model.

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [232] [Advancing Software Security and Reliability in Cloud Platforms through AI-based Anomaly Detection](https://arxiv.org/abs/2411.09200)
*Sabbir M. Saleh,Ibrahim Mohammed Sayem,Nazim Madhavji,John Steinbacher*

Main category: cs.SE

TLDR: 该研究通过AI支持的异常检测技术，提升CI/CD管道的安全性，利用CNN和LSTM分析网络流量模式，实现了高准确率（98.69%和98.30%）。


<details>
  <summary>Details</summary>
Motivation: CI/CD管道在云环境中的安全问题频发，现有研究多关注静态安全测试，而网络流量模式分析较少。

Method: 结合CNN和LSTM分析CSE-CIC-IDS2018和CSE-CIC-IDS2017数据集中的网络流量模式。

Result: 实现了98.69%和98.30%的准确率，并生成日志文件以应对安全挑战。

Conclusion: 该研究为现代DevOps实践中的安全问题提供了有效解决方案，提升了软件安全性和可靠性。

Abstract: Continuous Integration/Continuous Deployment (CI/CD) is fundamental for
advanced software development, supporting faster and more efficient delivery of
code changes into cloud environments. However, security issues in the CI/CD
pipeline remain challenging, and incidents (e.g., DDoS, Bot, Log4j, etc.) are
happening over the cloud environments. While plenty of literature discusses
static security testing and CI/CD practices, only a few deal with network
traffic pattern analysis to detect different cyberattacks. This research aims
to enhance CI/CD pipeline security by implementing anomaly detection through AI
(Artificial Intelligence) support. The goal is to identify unusual behaviour or
variations from network traffic patterns in pipeline and cloud platforms. The
system shall integrate into the workflow to continuously monitor pipeline
activities and cloud infrastructure. Additionally, it aims to explore adaptive
response mechanisms to mitigate the detected anomalies or security threats.
This research employed two popular network traffic datasets, CSE-CIC-IDS2018
and CSE-CIC-IDS2017. We implemented a combination of Convolution Neural
Network(CNN) and Long Short-Term Memory (LSTM) to detect unusual traffic
patterns. We achieved an accuracy of 98.69% and 98.30% and generated log files
in different CI/CD pipeline stages that resemble the network anomalies affected
to address security challenges in modern DevOps practices, contributing to
advancing software security and reliability.

</details>

### [233] [Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical software assessments](https://arxiv.org/abs/2505.01307)
*Regan Bolton,Mohammadreza Sheikhfathollahi,Simon Parkinson,Vanessa Vulovic,Gary Bamford,Dan Basher,Howard Parkinson*

Main category: cs.SE

TLDR: DRAFT是一种基于检索增强生成（RAG）的新方法，通过双检索架构和微调框架提升大型语言模型在安全关键合规评估中的性能，实验显示其正确率比基线模型提高7%。


<details>
  <summary>Details</summary>
Motivation: 传统的手动评估方法在安全关键软件合规性评估中存在效率低下的问题，需要一种更高效且透明的方法。

Method: DRAFT结合双检索架构（软件文档和参考标准）和半自动化数据集生成方法，对大型语言模型进行微调。

Result: 实验表明，DRAFT在GPT-4o-mini上比基线模型正确率提高7%，且在证据处理、响应结构和领域推理方面有显著改进。

Conclusion: DRAFT为合规评估系统提供了一种实用且透明的改进方法，适用于监管领域。

Abstract: Safety critical software assessment requires robust assessment against
complex regulatory frameworks, a process traditionally limited by manual
evaluation. This paper presents Document Retrieval-Augmented Fine-Tuning
(DRAFT), a novel approach that enhances the capabilities of a large language
model (LLM) for safety-critical compliance assessment. DRAFT builds upon
existing Retrieval-Augmented Generation (RAG) techniques by introducing a novel
fine-tuning framework that accommodates our dual-retrieval architecture, which
simultaneously accesses both software documentation and applicable reference
standards. To fine-tune DRAFT, we develop a semi-automated dataset generation
methodology that incorporates variable numbers of relevant documents with
meaningful distractors, closely mirroring real-world assessment scenarios.
Experiments with GPT-4o-mini demonstrate a 7% improvement in correctness over
the baseline model, with qualitative improvements in evidence handling,
response structure, and domain-specific reasoning. DRAFT represents a practical
approach to improving compliance assessment systems while maintaining the
transparency and evidence-based reasoning essential in regulatory domains.

</details>

### [234] [Aggregating empirical evidence from data strategy studies: a case on model quantization](https://arxiv.org/abs/2505.00816)
*Santiago del Rey,Paulo Sérgio Medeiros dos Santos,Guilherme Horta Travassos,Xavier Franch,Silverio Martínez-Fernández*

Main category: cs.SE

TLDR: 该研究评估了模型量化对深度学习系统正确性和资源效率的影响，并探讨了数据策略研究的方法学意义。


<details>
  <summary>Details</summary>
Motivation: 随着实证软件工程的发展，越来越多的研究采用数据策略（如模型、源代码或系统日志），但结果合成面临新的方法学挑战。

Method: 研究综合了六项评估模型量化的实证研究，采用结构化合成方法（SSM）通过图示建模定性定量聚合证据。

Result: 模型量化对正确性指标有轻微负面影响，但显著提升资源效率（存储大小、推理延迟、GPU能耗），是一种可接受的权衡。

Conclusion: 模型量化在资源受限环境中是有效的优化策略，同时证明了SSM在数据策略研究中的可行性。

Abstract: Background: As empirical software engineering evolves, more studies adopt
data strategies$-$approaches that investigate digital artifacts such as models,
source code, or system logs rather than relying on human subjects. Synthesizing
results from such studies introduces new methodological challenges.
  Aims: This study assesses the effects of model quantization on correctness
and resource efficiency in deep learning (DL) systems. Additionally, it
explores the methodological implications of aggregating evidence from empirical
studies that adopt data strategies.
  Method: We conducted a research synthesis of six primary studies that
empirically evaluate model quantization. We applied the Structured Synthesis
Method (SSM) to aggregate the findings, which combines qualitative and
quantitative evidence through diagrammatic modeling. A total of 19 evidence
models were extracted and aggregated.
  Results: The aggregated evidence indicates that model quantization weakly
negatively affects correctness metrics while consistently improving resource
efficiency metrics, including storage size, inference latency, and GPU energy
consumption$-$a manageable trade-off for many DL deployment contexts. Evidence
across quantization techniques remains fragmented, underscoring the need for
more focused empirical studies per technique.
  Conclusions: Model quantization offers substantial efficiency benefits with
minor trade-offs in correctness, making it a suitable optimization strategy for
resource-constrained environments. This study also demonstrates the feasibility
of using SSM to synthesize findings from data strategy-based research.

</details>

### [235] [CppSATD: A Reusable Self-Admitted Technical Debt Dataset in C++](https://arxiv.org/abs/2505.01136)
*Phuoc Pham,Murali Sridharan,Matteo Esposito,Valentina Lenarduzzi*

Main category: cs.SE

TLDR: 论文提出了CppSATD数据集，填补了C++语言中自认技术债务（SATD）研究的空白，为跨语言SATD研究提供了基础。


<details>
  <summary>Details</summary>
Motivation: 现有SATD研究主要集中在Java语言，缺乏对其他语言（如C++）的关注，限制了研究结果的普适性和检测技术的跨语言应用。

Method: 通过构建CppSATD数据集，包含超过531,000条标注的代码注释及其上下文。

Result: 数据集为未来C++ SATD检测方法开发、跨语言研究及新见解提供了基础。

Conclusion: CppSATD数据集填补了研究空白，推动了跨语言SATD研究的进展。

Abstract: In software development, technical debt (TD) refers to suboptimal
implementation choices made by the developers to meet urgent deadlines and
limited resources, posing challenges for future maintenance. Self-Admitted
Technical Debt (SATD) is a sub-type of TD, representing specific TD instances
``openly admitted'' by the developers and often expressed through source code
comments. Previous research on SATD has focused predominantly on the Java
programming language, revealing a significant gap in cross-language SATD. Such
a narrow focus limits the generalizability of existing findings as well as SATD
detection techniques across multiple programming languages. Our work addresses
such limitation by introducing CppSATD, a dedicated C++ SATD dataset,
comprising over 531,000 annotated comments and their source code contexts. Our
dataset can serve as a foundation for future studies that aim to develop SATD
detection methods in C++, generalize the existing findings to other languages,
or contribute novel insights to cross-language SATD research.

</details>

<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [236] [Dynamical System Parameter Path Optimization using Persistent Homology](https://arxiv.org/abs/2505.00782)
*Max M. Chumley,Firas A. Khasawneh*

Main category: math.DS

TLDR: 提出了一种基于拓扑数据分析的新方法，用于在高维参数空间中优化导航非线性动力系统，以实现期望的系统响应或状态。


<details>
  <summary>Details</summary>
Motivation: 非线性动力系统复杂且难以解析研究，参数变化会导致系统响应的拓扑结构发生显著变化，传统方法难以在高维参数空间中确定优化方向。

Method: 利用持久图的可微性定义拓扑语言，通过梯度下降法在参数空间中寻找最优路径，以引导系统达到期望的拓扑特征。

Result: 通过多个动力系统示例验证了方法的有效性，展示了如何通过调整超参数实现不同的拓扑特征。

Conclusion: 该方法为动力系统参数优化提供了一种直观且高效的解决方案，适用于多种应用场景。

Abstract: Nonlinear dynamical systems are complex and typically only simple systems can
be analytically studied. In applications, these systems are usually defined
with a set of tunable parameters and as the parameters are varied the system
response undergoes significant topological changes or bifurcations. In a high
dimensional parameter space, it is difficult to determine which direction to
vary the system parameters to achieve a desired system response or state. In
this paper, we introduce a new approach for optimally navigating a dynamical
system parameter space that is rooted in topological data analysis.
Specifically we use the differentiability of persistence diagrams to define a
topological language for intuitively promoting or deterring different
topological features in the state space response of a dynamical system and use
gradient descent to optimally move from one point in the parameter space to
another. The end result is a path in this space that guides the system to a set
of parameters that yield the desired topological features defined by the loss
function. We show a number of examples by applying the methods to different
dynamical systems and scenarios to demonstrate how to promote different
features and how to choose the hyperparameters to achieve different outcomes.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [237] [Artificial Intelligence in Government: Why People Feel They Lose Control](https://arxiv.org/abs/2505.01085)
*Alexander Wuttke,Adrian Rauchfleisch,Andreas Jungherr*

Main category: cs.CY

TLDR: 论文探讨了AI在公共管理中的应用及其对公平性、透明度和问责制的潜在影响，提出了基于委托-代理理论的框架，并通过实验验证了效率提升与公众信任之间的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 研究AI在政府职能中的快速扩展，尤其是生成式和自主系统，以及其对民主合法性的潜在风险。

Method: 应用委托-代理理论（PAT）分析AI采用的核心矛盾，并通过预注册的因子调查实验在税务、福利和执法领域验证框架。

Result: 实验显示，效率提升初期增强信任，但会降低公众感知的控制力；当结构性风险凸显时，信任和控制感均急剧下降。

Conclusion: 委托-代理理论为理解AI在政府中的制度与政治影响提供了有力视角，政策制定者需透明处理委托风险以维持公众信任。

Abstract: The use of Artificial Intelligence (AI) in public administration is expanding
rapidly, moving from automating routine tasks to deploying generative and
agentic systems that autonomously act on goals. While AI promises greater
efficiency and responsiveness, its integration into government functions raises
concerns about fairness, transparency, and accountability. This article applies
principal-agent theory (PAT) to conceptualize AI adoption as a special case of
delegation, highlighting three core tensions: assessability (can decisions be
understood?), dependency (can the delegation be reversed?), and contestability
(can decisions be challenged?). These structural challenges may lead to a
"failure-by-success" dynamic, where early functional gains obscure long-term
risks to democratic legitimacy. To test this framework, we conducted a
pre-registered factorial survey experiment across tax, welfare, and law
enforcement domains. Our findings show that although efficiency gains initially
bolster trust, they simultaneously reduce citizens' perceived control. When the
structural risks come to the foreground, institutional trust and perceived
control both drop sharply, suggesting that hidden costs of AI adoption
significantly shape public attitudes. The study demonstrates that PAT offers a
powerful lens for understanding the institutional and political implications of
AI in government, emphasizing the need for policymakers to address delegation
risks transparently to maintain public trust.

</details>

<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [238] [Shuffling Cards When You Are of Very Little Brain: Low Memory Generation of Permutations](https://arxiv.org/abs/2505.01287)
*Boaz Menuhin,Moni Naor*

Main category: cs.DS

TLDR: 论文研究了如何在有限内存下生成难以预测的排列，提出了内存与猜测概率的紧界关系，并给出了最优方法。


<details>
  <summary>Details</summary>
Motivation: 探讨在Dealer内存有限而Guesser内存无限的情况下，如何生成难以预测的排列，并量化内存与猜测概率的关系。

Method: 提出了一种基于m位存储的方法，确保Guesser只能以O(n/m+log m)的期望正确猜测次数，且无需秘密存储。

Result: 证明了该方法的紧界性，并给出了一个O(n)位内存的完美随机排列生成器。

Conclusion: 内存与猜测概率之间存在紧界关系，提出的方法在有限内存下实现了最优性能。

Abstract: How can we generate a permutation of the numbers $1$ through $n$ so that it
is hard to guess the next element given the history so far? The twist is that
the generator of the permutation (the ``Dealer") has limited memory, while the
``Guesser" has unlimited memory. With unbounded memory (actually $n$ bits
suffice), the Dealer can generate a truly random permutation where~$\ln n$ is
the expected number of correct guesses.
  Our main results are tight bounds for the relationship between the guessing
probability and the memory required to generate the permutation. We suggest a
method for the Dealer that requires~$m$ bits of storage, constant time for each
turn and makes any Guesser pick correctly only $O(n/m+\log m)$ cards in
expectation. The method does not require any secrecy from the dealer, i.e. it
is ``open book" or ``whitebox". On the other hand, we show that this bound is
the best possible, even for Dealers with secret memory: For any $m$-bit Dealer
there is a (computationally powerful) guesser that makes $\Omega(n/m+\log m)$
correct guesses in expectation.
  We also give an $O(n)$ bit memory Dealer that generates perfectly random
permutations and operates in constant time per turn.

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [239] [To Repair or Not to Repair? Investigating the Importance of AB-Cycles for the State-of-the-Art TSP Heuristic EAX](https://arxiv.org/abs/2505.00803)
*Jonathan Heins,Darrell Whitley,Pascal Kerschke*

Main category: cs.NE

TLDR: 本文研究了EAX算法的第一阶段，提出了一种快速验证AB-cycles有效性的新方法，并改进了EAX算法，提高了计算效率和求解质量。


<details>
  <summary>Details</summary>
Motivation: EAX算法的第一阶段尚未被充分研究，本文旨在填补这一空白，优化其性能。

Method: 提出了一种快速验证AB-cycles有效性的方法，并基于此改进了EAX算法。

Result: 在10,000个TSP实例上的测试表明，改进后的EAX算法在计算效率和求解质量上优于现有方法。

Conclusion: 改进后的EAX算法在解决困难实例时表现更优，为TSP求解提供了更高效的工具。

Abstract: The Edge Assembly Crossover (EAX) algorithm is the state-of-the-art heuristic
for solving the Traveling Salesperson Problem (TSP). It regularly outperforms
other methods, such as the Lin-Kernighan-Helsgaun heuristic (LKH), across
diverse sets of TSP instances. Essentially, EAX employs a two-stage mechanism
that focuses on improving the current solutions, first, at the local and,
subsequently, at the global level. Although the second phase of the algorithm
has been thoroughly studied, configured, and refined in the past, in
particular, its first stage has hardly been examined.
  In this paper, we thus focus on the first stage of EAX and introduce a novel
method that quickly verifies whether the AB-cycles, generated during its
internal optimization procedure, yield valid tours -- or whether they need to
be repaired. Knowledge of the latter is also particularly relevant before
applying other powerful crossover operators such as the Generalized Partition
Crossover (GPX). Based on our insights, we propose and evaluate several
improved versions of EAX. According to our benchmark study across 10 000
different TSP instances, the most promising of our proposed EAX variants
demonstrates improved computational efficiency and solution quality on
previously rather difficult instances compared to the current state-of-the-art
EAX algorithm.

</details>

### [240] [A Neural Architecture Search Method using Auxiliary Evaluation Metric based on ResNet Architecture](https://arxiv.org/abs/2505.01313)
*Shang Wang,Huanrong Tang,Jianquan Ouyang*

Main category: cs.NE

TLDR: 提出了一种基于ResNet的神经架构搜索空间，优化目标包括卷积、池化、全连接层参数及残差网络连接性，并使用验证集损失值作为次要优化目标。实验表明该方法在MNIST、Fashion-MNIST和CIFAR100数据集上能找到有竞争力的架构。


<details>
  <summary>Details</summary>
Motivation: 通过神经架构搜索（NAS）自动设计高性能网络架构，减少人工设计的工作量。

Method: 以ResNet为框架，搜索卷积、池化、全连接层参数及残差网络连接性，同时优化识别精度和验证集损失值。

Result: 在MNIST、Fashion-MNIST和CIFAR100数据集上找到了具有竞争力的网络架构。

Conclusion: 提出的搜索空间和优化方法能有效自动设计高性能网络架构。

Abstract: This paper proposes a neural architecture search space using ResNet as a
framework, with search objectives including parameters for convolution,
pooling, fully connected layers, and connectivity of the residual network. In
addition to recognition accuracy, this paper uses the loss value on the
validation set as a secondary objective for optimization. The experimental
results demonstrate that the search space of this paper together with the
optimisation approach can find competitive network architectures on the MNIST,
Fashion-MNIST and CIFAR100 datasets.

</details>