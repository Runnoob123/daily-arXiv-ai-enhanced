<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 18]
- [cs.LG](#cs.LG) [Total: 52]
- [cs.CL](#cs.CL) [Total: 32]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.CV](#cs.CV) [Total: 69]
- [math.NA](#math.NA) [Total: 2]
- [cs.RO](#cs.RO) [Total: 10]
- [cs.CE](#cs.CE) [Total: 2]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]
- [stat.ML](#stat.ML) [Total: 6]
- [cs.NI](#cs.NI) [Total: 2]
- [eess.IV](#eess.IV) [Total: 11]
- [cs.MM](#cs.MM) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.NE](#cs.NE) [Total: 6]
- [eess.SY](#eess.SY) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [hep-lat](#hep-lat) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.CY](#cs.CY) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Enabling Deep Visibility into VxWorks-Based Embedded Controllers in Cyber-Physical Systems for Anomaly Detection](https://arxiv.org/abs/2504.17875)
*Prashanth Krishnamurthy,Ramesh Karri,Farshad Khorrami*

Main category: cs.CR

TLDR: DIVER框架为嵌入式控制设备提供实时深度可见性，支持运行时异常检测，适用于资源受限的VxWorks设备。


<details>
  <summary>Details</summary>
Motivation: 传统动态监控方法不适用于VxWorks等实时操作系统，需开发新方法以实现嵌入式设备的运行时监控。

Method: DIVER由嵌入VxWorks内核的'measurer'和远程'listener'组成，前者收集数据，后者分析并提供交互界面。

Result: 在Motorola ACE远程终端单元上验证了DIVER的有效性，适用于电力系统等CPS。

Conclusion: DIVER为资源受限的嵌入式设备提供了一种有效的实时监控解决方案。

Abstract: We propose the DIVER (Defensive Implant for Visibility into Embedded
Run-times) framework for real-time deep visibility into embedded control
devices in cyber-physical systems (CPSs). DIVER enables run-time detection of
anomalies and is targeted at devices running the real-time operating system
(RTOS), VxWorks, which precludes traditional methods of implementing dynamic
monitors using OS (e.g., Linux, Windows) functions. DIVER has two components.
The "measurer" implant is embedded into the VxWorks kernel to collect run-time
measurements and provide interactive/streaming interfaces over a TCP/IP port.
The remote "listener" acquires and analyzes the measurements and provides an
interactive user interface. DIVER focuses on small embedded devices with
stringent resource constraints (e.g., insufficient storage to locally store
measurements). We demonstrate efficacy of DIVER on the Motorola ACE Remote
Terminal Unit used in CPS including power systems.

</details>

### [2] [Crypto-ncRNA: Non-coding RNA (ncRNA) Based Encryption Algorithm](https://arxiv.org/abs/2504.17878)
*Xu Wang,Yiquan Wang,Tin-yeh Huang*

Main category: cs.CR

TLDR: 提出了一种基于非编码RNA动态折叠特性的生物融合密码框架crypto-ncRNA，用于生成抗量子计算的高熵密钥和不可预测密文。


<details>
  <summary>Details</summary>
Motivation: 传统密码系统在量子计算攻击下日益脆弱，需要新的抗量子密码解决方案。

Method: 通过多阶段过程：将明文编码为RNA序列，预测和操纵RNA二级结构，利用RNA分子的物理不可克隆性生成密钥。

Result: 实验表明，crypto-ncRNA在效率和可扩展性上显著优于RSA，并通过NIST随机性测试。

Conclusion: crypto-ncRNA为抵御量子计算威胁提供了有前景的解决方案。

Abstract: In the looming post-quantum era, traditional cryptographic systems are
increasingly vulnerable to quantum computing attacks that can compromise their
mathematical foundations. To address this critical challenge, we propose
crypto-ncRNA-a bio-convergent cryptographic framework that leverages the
dynamic folding properties of non-coding RNA (ncRNA) to generate high-entropy,
quantum-resistant keys and produce unpredictable ciphertexts. The framework
employs a novel, multi-stage process: encoding plaintext into RNA sequences,
predicting and manipulating RNA secondary structures using advanced algorithms,
and deriving cryptographic keys through the intrinsic physical unclonability of
RNA molecules. Experimental evaluations indicate that, although crypto-ncRNA's
encryption speed is marginally lower than that of AES, it significantly
outperforms RSA in terms of efficiency and scalability while achieving a 100%
pass rate on the NIST SP 800-22 randomness tests. These results demonstrate
that crypto-ncRNA offers a promising and robust approach for securing digital
infrastructures against the evolving threats posed by quantum computing.

</details>

### [3] [Biting the CHERI bullet: Blockers, Enablers and Security Implications of CHERI in Defence](https://arxiv.org/abs/2504.17904)
*Shamal Faily*

Main category: cs.CR

TLDR: 12个月评估CHERI硬件安全解决方案，发现6种阻碍采用的因素和3种促进因素，同时指出5种潜在漏洞。


<details>
  <summary>Details</summary>
Motivation: 为软件和硬件供应链利益相关者提供长期采用安全硬件解决方案（如CHERI）的实际证据。

Method: 15个团队将国防相关软件移植到Arm的Morello板上，进行12个月评估。

Result: 识别出6种阻碍（如依赖性问题）、3种促进因素（如工具辅助），以及5种潜在漏洞（如内存泄漏）。

Conclusion: 未来需改进CHERI工具默认设置并建立知识库以促进采用。

Abstract: There is growing interest in securing the hardware foundations software
stacks build upon. However, before making any investment decision, software and
hardware supply chain stakeholders require evidence from realistic, multiple
long-term studies of adoption. We present results from a 12 month evaluation of
one such secure hardware solution, CHERI, where 15 teams from industry and
academia ported software relevant to Defence to Arm's experimental Morello
board. We identified six types of blocker inhibiting adoption: dependencies, a
knowledge premium, missing utilities, performance, platform instability, and
technical debt. We also identified three types of enabler: tool assistance,
improved quality, and trivial code porting. Finally, we identified five types
of potential vulnerability that CHERI could, if not appropriately configured,
expand a system's attack surface: state leaks, memory leaks, use after free
vulnerabilities, unsafe defaults, and tool chain instability. Future work
should remove potentially insecure defaults from CHERI tooling, and develop a
CHERI body of knowledge to further adoption.

</details>

### [4] ["Shifting Access Control Left" using Asset and Goal Models](https://arxiv.org/abs/2504.17906)
*Shamal Faily*

Main category: cs.CR

TLDR: 论文提出了一种工具支持的技术，通过资产和目标模型识别访问控制中的知识不对称问题，利用简单建模语言提高透明度。


<details>
  <summary>Details</summary>
Motivation: 访问控制需求广泛且设计复杂，但相关知识分散且不对称，需要透明化并利用现有专业知识。

Method: 基于资产和目标模型的工具支持技术，使用简单建模语言作为边界对象。

Result: 通过案例研究验证了技术在军事航空系统中重用软件组件的适用性。

Conclusion: 该方法能有效透明化访问控制知识，减少知识不对称问题。

Abstract: Access control needs have broad design implications, but access control
specifications may be elicited before, during, or after these needs are
captured. Because access control knowledge is distributed, we need to make
knowledge asymmetries more transparent, and use expertise already available to
stakeholders. In this paper, we present a tool-supported technique identifying
knowledge asymmetries around access control based on asset and goal models.
Using simple and conventional modelling languages that complement different
design techniques, we provide boundary objects to make access control
transparent, thereby making knowledge about access control concerns more
symmetric. We illustrate this technique using a case study example considering
the suitability of a reusable software component in a new military air system.

</details>

### [5] [Secured Encryption scheme based on the Ree groups](https://arxiv.org/abs/2504.17919)
*Gennady Khalimov,Yevgen Kotukh*

Main category: cs.CR

TLDR: 提出了一种基于小Ree群的改进密码系统设计，通过改变加密算法并使用对数签名，提高了安全性。


<details>
  <summary>Details</summary>
Motivation: 改进现有密码系统的安全性，特别是抵御顺序密钥恢复攻击。

Method: 使用对数签名覆盖整个Ree群，并采用3参数小Ree群进行群计算。

Result: 证明该方法能有效提升安全性，密钥恢复攻击的复杂度等同于对整个群的暴力破解。

Conclusion: 在小型有限域上构建安全密码系统时，3参数小Ree群是必要的。

Abstract: An improved design of a cryptosystem based on small Ree groups is proposed.
We have changed the encryption algorithm and propose to use a logarithmic
signature for the entire Ree group. This approach improves security against
sequential key recovery attacks. Hence, the complexity of the key recovery
attack will be defined by a brute-force attack over the entire group. In this
paper, we have proved that to construct secure cryptosystems with group
computations over a small finite field, it is needed to use a 3-parametric
small Ree group.

</details>

### [6] [Optimized Approaches to Malware Detection: A Study of Machine Learning and Deep Learning Techniques](https://arxiv.org/abs/2504.17930)
*Abrar Fahim,Shamik Dey,Md. Nurul Absur,Md Kamrul Siam,Md. Tahmidul Huque,Jafreen Jafor Godhuli*

Main category: cs.CR

TLDR: 论文探讨了机器学习和深度学习方法在恶意软件检测中的应用，比较了多种模型的性能，发现DNN模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统恶意软件检测方法效果不佳，误报率高，需探索更先进的检测技术。

Method: 使用随机森林、MLP和DNN等模型，结合Kaggle数据集进行特征选择和预处理。

Result: DNN模型表现最优，训练准确率达99.92%，AUC接近完美。

Conclusion: 特征选择和预处理能提升检测能力，为构建更可靠的网络安全解决方案提供参考。

Abstract: Digital systems find it challenging to keep up with cybersecurity threats.
The daily emergence of more than 560,000 new malware strains poses significant
hazards to the digital ecosystem. The traditional malware detection methods
fail to operate properly and yield high false positive rates with low accuracy
of the protection system. This study explores the ways in which malware can be
detected using these machine learning (ML) and deep learning (DL) approaches to
address those shortcomings. This study also includes a systematic comparison of
the performance of some of the widely used ML models, such as random forest,
multi-layer perceptron (MLP), and deep neural network (DNN), for determining
the effectiveness of the domain of modern malware threat systems. We use a
considerable-sized database from Kaggle, which has undergone optimized feature
selection and preprocessing to improve model performance. Our finding suggests
that the DNN model outperformed the other traditional models with the highest
training accuracy of 99.92% and an almost perfect AUC score. Furthermore, the
feature selection and preprocessing can help improve the capabilities of
detection. This research makes an important contribution by analyzing the
performance of the model on the performance metrics and providing insight into
the effectiveness of the advanced detection techniques to build more robust and
more reliable cybersecurity solutions against the growing malware threats.

</details>

### [7] [Fishing for Phishers: Learning-Based Phishing Detection in Ethereum Transactions](https://arxiv.org/abs/2504.17953)
*Ahod Alghuried,Abdulaziz Alghamdi,Ali Alkinoon,Soohyeon Choi,Manar Mohaisen,David Mohaisen*

Main category: cs.CR

TLDR: 论文研究了以太坊钓鱼检测中特征选择策略的有效性及图模型的作用，对比了显式交易特征与隐式图特征，并探讨了其对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注特征选择策略和图模型在提升钓鱼检测准确性中的作用，本文旨在填补这一空白。

Method: 通过实验和分析对比显式交易特征与隐式图特征，并探讨其对模型性能的影响，同时解决类别不平衡和数据集组成等挑战。

Result: 研究发现每种特征类型的优势和局限性，并揭示了特征如何影响模型在对抗环境中的鲁棒性和泛化能力。

Conclusion: 研究为以太坊钓鱼检测提供了更清晰的特征选择指导，并强调了图模型在提升检测精度中的潜力。

Abstract: Phishing detection on Ethereum has increasingly leveraged advanced machine
learning techniques to identify fraudulent transactions. However, limited
attention has been given to understanding the effectiveness of feature
selection strategies and the role of graph-based models in enhancing detection
accuracy. In this paper, we systematically examine these issues by analyzing
and contrasting explicit transactional features and implicit graph-based
features, both experimentally and analytically. We explore how different
feature sets impact the performance of phishing detection models, particularly
in the context of Ethereum's transactional network. Additionally, we address
key challenges such as class imbalance and dataset composition and their
influence on the robustness and precision of detection methods. Our findings
demonstrate the advantages and limitations of each feature type, while also
providing a clearer understanding of how feature affect model resilience and
generalization in adversarial environments.

</details>

### [8] [Cluster-Aware Attacks on Graph Watermarks](https://arxiv.org/abs/2504.17971)
*Alexander Nemecek,Emre Yilmaz,Erman Ayday*

Main category: cs.CR

TLDR: 论文研究了图数据水印技术中社区检测攻击的威胁，提出了两种新型攻击策略，并提出了一种轻量级嵌入增强方法以提高抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 图数据在多个领域具有敏感性，需确保安全共享和可追溯性。现有水印技术未考虑社区检测攻击的威胁。

Method: 提出基于社区的攻击模型和两种攻击策略，并设计一种轻量级嵌入增强方法。

Result: 实验显示社区攻击可降低80%的溯源准确率，而增强方法在密集图上提高60%的准确率。

Conclusion: 研究强调了社区拓扑意识在水印设计和对抗建模中的重要性。

Abstract: Data from domains such as social networks, healthcare, finance, and
cybersecurity can be represented as graph-structured information. Given the
sensitive nature of this data and their frequent distribution among
collaborators, ensuring secure and attributable sharing is essential. Graph
watermarking enables attribution by embedding user-specific signatures into
graph-structured data. While prior work has addressed random perturbation
attacks, the threat posed by adversaries leveraging structural properties
through community detection remains unexplored. In this work, we introduce a
cluster-aware threat model in which adversaries apply community-guided
modifications to evade detection. We propose two novel attack strategies and
evaluate them on real-world social network graphs. Our results show that
cluster-aware attacks can reduce attribution accuracy by up to 80% more than
random baselines under equivalent perturbation budgets on sparse graphs. To
mitigate this threat, we propose a lightweight embedding enhancement that
distributes watermark nodes across graph communities. This approach improves
attribution accuracy by up to 60% under attack on dense graphs, without
increasing runtime or structural distortion. Our findings underscore the
importance of cluster-topological awareness in both watermarking design and
adversarial modeling.

</details>

### [9] [Diffusion-Driven Universal Model Inversion Attack for Face Recognition](https://arxiv.org/abs/2504.18015)
*Hanrui Wang,Shuo Wang,Chun-Shien Lu,Isao Echizen*

Main category: cs.CR

TLDR: DiffUMI是一种无需训练的通用模型反演攻击方法，利用扩散模型高效重构面部图像，揭示人脸识别系统的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 传统面部识别系统依赖嵌入技术保护隐私，但模型反演攻击仍能重构敏感图像，现有方法需为目标模型单独训练生成器，计算成本高。

Method: 提出DiffUMI，基于预训练扩散模型，无需目标特定训练，通过优化对抗搜索实现高效高保真面部重构，并首次利用模型反演进行非面部输入检测。

Result: DiffUMI在隐私保护面部识别系统中取得先进成果，成功重构高保真图像，并验证了基于嵌入的非面部输入检测能力。

Conclusion: DiffUMI展示了扩散模型在模型反演中的潜力，为评估面部识别系统隐私风险提供了高效通用工具。

Abstract: Facial recognition technology poses significant privacy risks, as it relies
on biometric data that is inherently sensitive and immutable if compromised. To
mitigate these concerns, face recognition systems convert raw images into
embeddings, traditionally considered privacy-preserving. However, model
inversion attacks pose a significant privacy threat by reconstructing these
private facial images, making them a crucial tool for evaluating the privacy
risks of face recognition systems. Existing methods usually require training
individual generators for each target model, a computationally expensive
process. In this paper, we propose DiffUMI, a training-free diffusion-driven
universal model inversion attack for face recognition systems. DiffUMI is the
first approach to apply a diffusion model for unconditional image generation in
model inversion. Unlike other methods, DiffUMI is universal, eliminating the
need for training target-specific generators. It operates within a fixed
framework and pretrained diffusion model while seamlessly adapting to diverse
target identities and models. DiffUMI breaches privacy-preserving face
recognition systems with state-of-the-art success, demonstrating that an
unconditional diffusion model, coupled with optimized adversarial search,
enables efficient and high-fidelity facial reconstruction. Additionally, we
introduce a novel application of out-of-domain detection (OODD), marking the
first use of model inversion to distinguish non-face inputs from face inputs
based solely on embeddings.

</details>

### [10] [Automating Function-Level TARA for Automotive Full-Lifecycle Security](https://arxiv.org/abs/2504.18083)
*Yuqiao Yang,Yongzhao Zhang,Wenhao Liu,Jun Li,Pengtao Shi,DingYu Zhong,Jie Yang,Ting Chen,Sheng Cao,Yuntao Ren,Yongyue Wu,Xiaosong Zhang*

Main category: cs.CR

TLDR: DefenseWeaver是一个自动化功能级TARA系统，利用LLM和多代理框架动态生成攻击树和风险评估，验证了其高效性和跨领域适应性。


<details>
  <summary>Details</summary>
Motivation: 现代车辆的智能化和互联化带来了显著的网络安全风险，现有TARA方法依赖静态威胁库，无法满足功能级分析需求。

Method: DefenseWeaver通过扩展OpenXSAM++格式描述系统配置，利用LLM和多代理框架动态生成攻击树和风险评估，并结合LoRA微调和RAG技术适应多样标准。

Result: 在四个汽车安全项目中验证，识别了11条关键攻击路径，并成功应用于无人机和航海系统，生成8200多棵攻击树。

Conclusion: DefenseWeaver显著减少处理时间，具有可扩展性，对跨行业网络安全具有变革性影响。

Abstract: As modern vehicles evolve into intelligent and connected systems, their
growing complexity introduces significant cybersecurity risks. Threat Analysis
and Risk Assessment (TARA) has therefore become essential for managing these
risks under mandatory regulations. However, existing TARA automation methods
rely on static threat libraries, limiting their utility in the detailed,
function-level analyses demanded by industry. This paper introduces
DefenseWeaver, the first system that automates function-level TARA using
component-specific details and large language models (LLMs). DefenseWeaver
dynamically generates attack trees and risk evaluations from system
configurations described in an extended OpenXSAM++ format, then employs a
multi-agent framework to coordinate specialized LLM roles for more robust
analysis. To further adapt to evolving threats and diverse standards,
DefenseWeaver incorporates Low-Rank Adaptation (LoRA) fine-tuning and
Retrieval-Augmented Generation (RAG) with expert-curated TARA reports. We
validated DefenseWeaver through deployment in four automotive security
projects, where it identified 11 critical attack paths, verified through
penetration testing, and subsequently reported and remediated by the relevant
automakers and suppliers. Additionally, DefenseWeaver demonstrated cross-domain
adaptability, successfully applying to unmanned aerial vehicles (UAVs) and
marine navigation systems. In comparison to human experts, DefenseWeaver
outperformed manual attack tree generation across six assessment scenarios.
Integrated into commercial cybersecurity platforms such as UAES and Xiaomi,
DefenseWeaver has generated over 8,200 attack trees. These results highlight
its ability to significantly reduce processing time, and its scalability and
transformative impact on cybersecurity across industries.

</details>

### [11] [SoK: Timeline based event reconstruction for digital forensics: Terminology, methodology, and current challenges](https://arxiv.org/abs/2504.18131)
*Frank Breitinger,Hudan Studiawan,Chris Hargreaves*

Main category: cs.CR

TLDR: 本文提出了一种统一的事件重建框架，整合了现有术语并明确了重建过程中关键元素的关系，同时通过文献综述分类了主要挑战，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 事件重建在数字取证中具有重要意义，但现有研究零散且缺乏统一视角。本文旨在填补这一空白，提供一个全面的框架。

Method: 通过整合现有术语、绘制关系图、文献综述分类挑战，并提出未来研究方向。

Result: 提出了一个结构化的事件重建框架，明确了关键元素关系，并分类了主要挑战。

Conclusion: 本文为数字取证提供了更坚实的基础，并指出了未来研究的潜在方向。

Abstract: Event reconstruction is a technique that examiners can use to attempt to
infer past activities by analyzing digital artifacts. Despite its significance,
the field suffers from fragmented research, with studies often focusing
narrowly on aspects like timeline creation or tampering detection. This paper
addresses the lack of a unified perspective by proposing a comprehensive
framework for timeline-based event reconstruction, adapted from traditional
forensic science models. We begin by harmonizing existing terminology and
presenting a cohesive diagram that clarifies the relationships between key
elements of the reconstruction process. Through a comprehensive literature
survey, we classify and organize the main challenges, extending the discussion
beyond common issues like data volume. Lastly, we highlight recent advancements
and propose directions for future research, including specific research gaps.
By providing a structured approach, key findings, and a clearer understanding
of the underlying challenges, this work aims to strengthen the foundation of
digital forensics.

</details>

### [12] [NoEsis: Differentially Private Knowledge Transfer in Modular LLM Adaptation](https://arxiv.org/abs/2504.18147)
*Rob Romijnders,Stefanos Laskaridis,Ali Shahin Shamsabadi,Hamed Haddadi*

Main category: cs.CR

TLDR: NoEsis框架结合差分隐私和参数高效微调，实现模块化、隐私保护和知识迁移，在CodeXGLUE上验证了隐私保障和跨领域知识迁移能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在模块化设计下仍可能泄露隐私，而孤立训练又限制泛化能力，因此需要兼顾隐私、模块化和知识迁移的解决方案。

Method: NoEsis结合差分隐私与两阶段参数高效微调，使用领域特定低秩适配器（专家）和共享提示令牌（知识共享主干）。

Result: 在CodeXGLUE上，NoEsis实现可证明的隐私保障和跨领域知识迁移，有效防御成员推理攻击，并在代码补全任务中缩小77%准确率差距。

Conclusion: NoEsis成功平衡了隐私保护与知识迁移，为模块化LLM提供了一种可行的解决方案。

Abstract: Large Language Models (LLM) are typically trained on vast amounts of data
from various sources. Even when designed modularly (e.g., Mixture-of-Experts),
LLMs can leak privacy on their sources. Conversely, training such models in
isolation arguably prohibits generalization. To this end, we propose a
framework, NoEsis, which builds upon the desired properties of modularity,
privacy, and knowledge transfer. NoEsis integrates differential privacy with a
hybrid two-staged parameter-efficient fine-tuning that combines domain-specific
low-rank adapters, acting as experts, with common prompt tokens, acting as a
knowledge-sharing backbone. Results from our evaluation on CodeXGLUE showcase
that NoEsis can achieve provable privacy guarantees with tangible knowledge
transfer across domains, and empirically show protection against Membership
Inference Attacks. Finally, on code completion tasks, NoEsis bridges at least
77% of the accuracy gap between the non-shared and the non-private baseline.

</details>

### [13] [Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections](https://arxiv.org/abs/2504.18333)
*Narek Maloyan,Dmitry Namiot*

Main category: cs.CR

TLDR: 论文研究了LLM作为评估系统时对提示注入攻击的脆弱性，提出了一个框架区分内容作者攻击和系统提示攻击，并测试了五种模型在不同任务和防御措施下的表现。


<details>
  <summary>Details</summary>
Motivation: 揭示LLM评估系统在提示注入攻击下的脆弱性，并提出改进方法。

Method: 引入框架区分攻击类型，测试五种模型在四种任务中的表现，每种条件使用五十个提示。

Result: 攻击成功率高达73.8%，小模型更脆弱，攻击可迁移性在50.5%至62.6%之间。

Conclusion: 建议使用多模型委员会和比较评分，并公开了代码和数据集。

Abstract: LLM as judge systems used to assess text quality code correctness and
argument strength are vulnerable to prompt injection attacks. We introduce a
framework that separates content author attacks from system prompt attacks and
evaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3
Opus on four tasks with various defenses using fifty prompts per condition.
Attacks achieved up to seventy three point eight percent success smaller models
proved more vulnerable and transferability ranged from fifty point five to
sixty two point six percent. Our results contrast with Universal Prompt
Injection and AdvPrompter We recommend multi model committees and comparative
scoring and release all code and datasets

</details>

### [14] [ThreMoLIA: Threat Modeling of Large Language Model-Integrated Applications](https://arxiv.org/abs/2504.18369)
*Felix Viktor Jedrzejewski,Davide Fucci,Oleksandr Adamov*

Main category: cs.CR

TLDR: 论文提出了一种结合LLM和RAG的威胁建模工具，旨在减少安全专家参与并提高效率。


<details>
  <summary>Details</summary>
Motivation: LLM集成应用（LIA）扩大了攻击面，但传统威胁建模耗时且依赖安全专家。

Method: 结合LLM和RAG技术，利用现有威胁模型和架构库自动生成和更新威胁模型。

Result: 初步评估使用ChatGPT在简单LIA上取得积极结果。

Conclusion: 该方法有望减少安全专家需求并缩短威胁建模时间。

Abstract: Large Language Models (LLMs) are currently being integrated into industrial
software applications to help users perform more complex tasks in less time.
However, these LLM-Integrated Applications (LIA) expand the attack surface and
introduce new kinds of threats. Threat modeling is commonly used to identify
these threats and suggest mitigations. However, it is a time-consuming practice
that requires the involvement of a security practitioner. Our goals are to 1)
provide a method for performing threat modeling for LIAs early in their
lifecycle, (2) develop a threat modeling tool that integrates existing threat
models, and (3) ensure high-quality threat modeling. To achieve the goals, we
work in collaboration with our industry partner. Our proposed way of performing
threat modeling will benefit industry by requiring fewer security experts'
participation and reducing the time spent on this activity. Our proposed tool
combines LLMs and Retrieval Augmented Generation (RAG) and uses sources such as
existing threat models and application architecture repositories to
continuously create and update threat models. We propose to evaluate the tool
offline -- i.e., using benchmarking -- and online with practitioners in the
field. We conducted an early evaluation using ChatGPT on a simple LIA and
obtained results that encouraged us to proceed with our research efforts.

</details>

### [15] [Bandit on the Hunt: Dynamic Crawling for Cyber Threat Intelligence](https://arxiv.org/abs/2504.18375)
*Philipp Kuehn,Dilara Nadermahmoodi,Markus Bayer,Christian Reuter*

Main category: cs.CR

TLDR: 论文提出了一种基于多臂老虎机和多种爬取策略的CTI爬虫系统ThreatCrawl，能够动态调整爬取路径并识别相关文档，显著提高了信息获取率和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 公共信息中包含有价值的网络威胁情报（CTI），但现有研究多关注从已知来源提取信息，而忽略了新来源的识别。在线监测耗时且来源选择不确定，亟需自动化解决方案。

Method: 采用多臂老虎机（MAB）和多种爬取策略，结合SBERT模型识别相关文档，动态调整爬取路径。

Result: 系统ThreatCrawl的信息获取率超过25%，种子来源扩展超过300%，同时保持了主题聚焦，并识别了新的相关页面和数据集。

Conclusion: 该方法有效解决了CTI来源识别和爬取的自动化问题，显著提升了情报收集的效率和覆盖范围。

Abstract: Public information contains valuable Cyber Threat Intelligence (CTI) that is
used to prevent future attacks. While standards exist for sharing this
information, much appears in non-standardized news articles or blogs.
Monitoring online sources for threats is time-consuming and source selection is
uncertain. Current research focuses on extracting Indicators of Compromise from
known sources, rarely addressing new source identification. This paper proposes
a CTI-focused crawler using multi-armed bandit (MAB) and various crawling
strategies. It employs SBERT to identify relevant documents while dynamically
adapting its crawling path. Our system ThreatCrawl achieves a harvest rate
exceeding 25% and expands its seed by over 300% while maintaining topical
focus. Additionally, the crawler identifies previously unknown but highly
relevant overview pages, datasets, and domains.

</details>

### [16] [Heavy-Tailed Privacy: The Symmetric alpha-Stable Privacy Mechanism](https://arxiv.org/abs/2504.18411)
*Christopher C. Zawacki,Eyad H. Abed*

Main category: cs.CR

TLDR: 本文提出并分析了对称α稳定（SaS）机制，证明其满足纯差分隐私且保持卷积封闭性，同时比较了其与高斯机制在数据集查询中的误差表现。


<details>
  <summary>Details</summary>
Motivation: 随着数字平台的快速发展，个人数据的收集、存储和使用引发了广泛担忧，差分隐私（DP）成为量化系统保护水平的重要工具。高斯机制虽常用，但仅满足近似差分隐私。

Method: 提出并分析对称α稳定（SaS）机制，证明其满足纯差分隐私且卷积封闭性，研究隐私水平与密度参数的关系，并与高斯机制比较查询误差。

Result: SaS机制在实现纯差分隐私的同时保持卷积封闭性，且在数据集查询中表现出与高斯机制相当的误差水平。

Conclusion: SaS机制是隐私保护应用的理想选择。

Abstract: With the rapid growth of digital platforms, there is increasing apprehension
about how personal data is collected, stored, and used by various entities.
These concerns arise from the increasing frequency of data breaches,
cyber-attacks, and misuse of personal information for targeted advertising and
surveillance. To address these matters, Differential Privacy (DP) has emerged
as a prominent tool for quantifying a digital system's level of protection. The
Gaussian mechanism is commonly used because the Gaussian density is closed
under convolution, and is a common method utilized when aggregating datasets.
However, the Gaussian mechanism only satisfies an approximate form of
Differential Privacy. In this work, we present and analyze of the Symmetric
alpha-Stable (SaS) mechanism. We prove that the mechanism achieves pure
differential privacy while remaining closed under convolution. Additionally, we
study the nuanced relationship between the level of privacy achieved and the
parameters of the density. Lastly, we compare the expected error introduced to
dataset queries by the Gaussian and SaS mechanisms. From our analysis, we
believe the SaS Mechanism is an appealing choice for privacy-focused
applications.

</details>

### [17] [LLMpatronous: Harnessing the Power of LLMs For Vulnerability Detection](https://arxiv.org/abs/2504.18423)
*Rajesh Yarra*

Main category: cs.CR

TLDR: 论文探讨了利用大型语言模型（LLMs）进行漏洞检测的潜力与局限，并提出了一种结合RAG和MoA的创新方法以提升检测质量。


<details>
  <summary>Details</summary>
Motivation: 当前网络安全工具存在高误报率和浅层代码理解问题，而LLMs在漏洞检测中的应用面临幻觉、上下文限制等挑战。

Method: 采用检索增强生成（RAG）和混合代理（MoA）方法，结合LLMs的优势并弥补其不足。

Result: 研究旨在提供一种可靠且高效的AI驱动漏洞检测方案。

Conclusion: 通过创新方法，为不断演变的软件安全领域提供可行的AI解决方案。

Abstract: Despite the transformative impact of Artificial Intelligence (AI) across
various sectors, cyber security continues to rely on traditional static and
dynamic analysis tools, hampered by high false positive rates and superficial
code comprehension. While generative AI offers promising automation
capabilities for software development, leveraging Large Language Models (LLMs)
for vulnerability detection presents unique challenges. This paper explores the
potential and limitations of LLMs in identifying vulnerabilities, acknowledging
inherent weaknesses such as hallucinations, limited context length, and
knowledge cut-offs. Previous attempts employing machine learning models for
vulnerability detection have proven ineffective due to limited real-world
applicability, feature engineering challenges, lack of contextual
understanding, and the complexities of training models to keep pace with the
evolving threat landscape. Therefore, we propose a robust AI-driven approach
focused on mitigating these limitations and ensuring the quality and
reliability of LLM based vulnerability detection. Through innovative
methodologies combining Retrieval-Augmented Generation (RAG) and
Mixtureof-Agents (MoA), this research seeks to leverage the strengths of LLMs
while addressing their weaknesses, ultimately paving the way for dependable and
efficient AI-powered solutions in securing the ever-evolving software
landscape.

</details>

### [18] [DeSIA: Attribute Inference Attacks Against Limited Fixed Aggregate Statistics](https://arxiv.org/abs/2504.18497)
*Yifeng Mao,Bozhidar Stevanoski,Yves-Alexandre de Montjoye*

Main category: cs.CR

TLDR: 论文提出了一种针对固定聚合统计数据的推断攻击框架DeSIA，并在美国人口普查PPMF数据集上验证其优于基于重构的攻击。DeSIA在识别易受攻击用户方面表现优异，强调了仅靠聚合不足以保护隐私。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对固定聚合统计数据的推断攻击方法，尤其是在仅发布少量统计数据时。

Method: 提出DeSIA框架，针对固定聚合统计数据实施属性推断攻击，并在不同条件下测试其性能。

Result: DeSIA在低误报率下实现高真阳性率，且在不同噪声水平和统计量数量下表现稳定。

Conclusion: 聚合统计数据不足以保障隐私，需结合形式化隐私机制和测试后再发布。

Abstract: Empirical inference attacks are a popular approach for evaluating the privacy
risk of data release mechanisms in practice. While an active attack literature
exists to evaluate machine learning models or synthetic data release, we
currently lack comparable methods for fixed aggregate statistics, in particular
when only a limited number of statistics are released. We here propose an
inference attack framework against fixed aggregate statistics and an attribute
inference attack called DeSIA. We instantiate DeSIA against the U.S. Census
PPMF dataset and show it to strongly outperform reconstruction-based attacks.
In particular, we show DeSIA to be highly effective at identifying vulnerable
users, achieving a true positive rate of 0.14 at a false positive rate of
$10^{-3}$. We then show DeSIA to perform well against users whose attributes
cannot be verified and when varying the number of aggregate statistics and
level of noise addition. We also perform an extensive ablation study of DeSIA
and show how DeSIA can be successfully adapted to the membership inference
task. Overall, our results show that aggregation alone is not sufficient to
protect privacy, even when a relatively small number of aggregates are being
released, and emphasize the need for formal privacy mechanisms and testing
before aggregate statistics are released.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [CaRL: Learning Scalable Planning Policies with Simple Rewards](https://arxiv.org/abs/2504.17838)
*Bernhard Jaeger,Daniel Dauner,Jens Beißwenger,Simon Gerstenecker,Kashyap Chitta,Andreas Geiger*

Main category: cs.LG

TLDR: 论文提出了一种基于强化学习（RL）的自动驾驶规划方法，通过简化奖励设计（以路线完成为主）解决了现有RL方法在大批量训练时的优化问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶规划方法多为基于规则，难以应对复杂场景；而传统RL方法因复杂的奖励设计在大批量训练时效果不佳。

Method: 提出了一种以路线完成度为主的简单奖励设计，并采用PPO算法进行训练，支持大批量数据并行处理。

Result: 在CARLA和nuPlan数据集上，该方法显著优于其他RL方法，并在性能上实现了数量级的提升。

Conclusion: 简单奖励设计结合大批量训练是提升自动驾驶RL方法性能的有效途径。

Abstract: We investigate reinforcement learning (RL) for privileged planning in
autonomous driving. State-of-the-art approaches for this task are rule-based,
but these methods do not scale to the long tail. RL, on the other hand, is
scalable and does not suffer from compounding errors like imitation learning.
Contemporary RL approaches for driving use complex shaped rewards that sum
multiple individual rewards, \eg~progress, position, or orientation rewards. We
show that PPO fails to optimize a popular version of these rewards when the
mini-batch size is increased, which limits the scalability of these approaches.
Instead, we propose a new reward design based primarily on optimizing a single
intuitive reward term: route completion. Infractions are penalized by
terminating the episode or multiplicatively reducing route completion. We find
that PPO scales well with higher mini-batch sizes when trained with our simple
reward, even improving performance. Training with large mini-batch sizes
enables efficient scaling via distributed data parallelism. We scale PPO to
300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The
resulting model achieves 64 DS on the CARLA longest6 v2 benchmark,
outperforming other RL methods with more complex rewards by a large margin.
Requiring only minimal adaptations from its use in CARLA, the same method is
the best learning-based approach on nuPlan. It scores 91.3 in non-reactive and
90.6 in reactive traffic on the Val14 benchmark while being an order of
magnitude faster than prior work.

</details>

### [20] [High-Performance Reinforcement Learning on Spot: Optimizing Simulation Parameters with Distributional Measures](https://arxiv.org/abs/2504.17857)
*A. J Miller,Fangzhou Yu,Michael Brauckmann,Farbod Farshidian*

Main category: cs.LG

TLDR: 论文介绍了在波士顿动力Spot机器人上部署高性能强化学习策略的技术细节，首次公开展示了端到端的强化学习策略部署，并公开了训练和部署代码。


<details>
  <summary>Details</summary>
Motivation: 通过强化学习优化Spot机器人的运动性能，解决仿真与现实之间的差距问题。

Method: 使用Wasserstein距离和最大均值差异量化仿真与硬件数据的分布差异，并利用协方差矩阵自适应进化策略优化未知参数。

Result: 实现了超过5.2米/秒的运动速度，是默认控制器速度的三倍，并具备对滑面、干扰的鲁棒性。

Conclusion: 该方法成功提升了Spot机器人的运动性能，公开代码支持未来研究。

Abstract: This work presents an overview of the technical details behind a high
performance reinforcement learning policy deployment with the Spot RL
Researcher Development Kit for low level motor access on Boston Dynamics Spot.
This represents the first public demonstration of an end to end end
reinforcement learning policy deployed on Spot hardware with training code
publicly available through Nvidia IsaacLab and deployment code available
through Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean
Discrepancy to quantify the distributional dissimilarity of data collected on
hardware and in simulation to measure our sim2real gap. We use these measures
as a scoring function for the Covariance Matrix Adaptation Evolution Strategy
to optimize simulated parameters that are unknown or difficult to measure from
Spot. Our procedure for modeling and training produces high quality
reinforcement learning policies capable of multiple gaits, including a flight
phase. We deploy policies capable of over 5.2ms locomotion, more than triple
Spots default controller maximum speed, robustness to slippery surfaces,
disturbance rejection, and overall agility previously unseen on Spot. We detail
our method and release our code to support future work on Spot with the low
level API.

</details>

### [21] [Do We Need Transformers to Play FPS Video Games?](https://arxiv.org/abs/2504.17891)
*Karmanbir Batth,Krish Sethi,Aly Shariff,Leo Shi,Hetul Patel*

Main category: cs.LG

TLDR: 论文研究了基于Transformer的架构在Doom游戏环境中在线和离线强化学习的表现，发现传统方法优于Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在强化学习中的潜力，特别是在部分可观测环境和离线学习场景中的应用。

Method: 采用Deep Transformer Q-learning Networks（DTQN）进行在线学习，以及Decision Transformers（DT）进行离线强化学习。

Result: 在VizDoom环境中，传统方法表现优于基于Transformer的方法。

Conclusion: 尽管Transformer在Atari游戏中表现良好，但在VizDoom环境中，传统方法更有效。

Abstract: In this paper, we explore the Transformer based architectures for
reinforcement learning in both online and offline settings within the Doom game
environment. Our investigation focuses on two primary approaches: Deep
Transformer Q- learning Networks (DTQN) for online learning and Decision
Transformers (DT) for offline reinforcement learning. DTQN leverages the
sequential modelling capabilities of Transformers to enhance Q-learning in
partially observable environments,while Decision Transformers repurpose
sequence modelling techniques to enable offline agents to learn from past
trajectories without direct interaction with the environment. We conclude that
while Transformers might have performed well in Atari games, more traditional
methods perform better than Transformer based method in both the settings in
the VizDoom environment.

</details>

### [22] [The use of Multi-domain Electroencephalogram Representations in the building of Models based on Convolutional and Recurrent Neural Networks for Epilepsy Detection](https://arxiv.org/abs/2504.17908)
*Luiz Antonio Nicolau Anghinoni,Gustavo Weber Denardin,Jadson Castro Gertrudes,Dalcimar Casanova,Jefferson Tales Oliva*

Main category: cs.LG

TLDR: 该论文通过比较三种EEG数据表示方式（时域、频域、时频域），发现频域数据在癫痫发作检测中表现最佳，准确率超过97%。


<details>
  <summary>Details</summary>
Motivation: 癫痫诊断依赖专家手动分析EEG信号，存在主观性差异，需要自动化解决方案。现有研究未系统比较不同数据表示对深度学习模型性能的影响。

Method: 系统比较了在时域、频域和时频域上训练的深度神经网络，并通过统计测试确定最优数据表示和模型架构。

Result: 频域数据在癫痫发作检测中表现最佳，检测指标超过97%。

Conclusion: 频域数据为癫痫发作检测提供了更准确和可靠的基础，可推动自动化诊断系统的发展。

Abstract: Epilepsy, affecting approximately 50 million people globally, is
characterized by abnormal brain activity and remains challenging to treat. The
diagnosis of epilepsy relies heavily on electroencephalogram (EEG) data, where
specialists manually analyze epileptiform patterns across pre-ictal, ictal,
post-ictal, and interictal periods. However, the manual analysis of EEG signals
is prone to variability between experts, emphasizing the need for automated
solutions. Although previous studies have explored preprocessing techniques and
machine learning approaches for seizure detection, there is a gap in
understanding how the representation of EEG data (time, frequency, or
time-frequency domains) impacts the predictive performance of deep learning
models. This work addresses this gap by systematically comparing deep neural
networks trained on EEG data in these three domains. Through the use of
statistical tests, we identify the optimal data representation and model
architecture for epileptic seizure detection. The results demonstrate that
frequency-domain data achieves detection metrics exceeding 97\%, providing a
robust foundation for more accurate and reliable seizure detection systems.

</details>

### [23] [CANet: ChronoAdaptive Network for Enhanced Long-Term Time Series Forecasting under Non-Stationarity](https://arxiv.org/abs/2504.17913)
*Mert Sonmezer,Seyda Ertekin*

Main category: cs.LG

TLDR: 论文提出了一种名为CANet的新架构，通过非平稳自适应归一化模块解决长期时间序列预测中的非平稳性问题，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据具有非平稳性（如分布偏移和统计特性变化），这导致传统模型性能受限。论文旨在解决这一问题。

Method: CANet结合了Style Blending Gate和AdaIN模块，通过多分辨率分块和傅里叶分析自适应阈值处理噪声，并使用堆叠Kronecker积层优化效率。

Result: 在真实数据集上，CANet比现有方法在MSE和MAE上分别降低了42%和22%。

Conclusion: CANet有效解决了非平稳性问题，显著提升了长期时间序列预测的准确性。

Abstract: Long-term time series forecasting plays a pivotal role in various real-world
applications. Despite recent advancements and the success of different
architectures, forecasting is often challenging due to non-stationary nature of
the real-world data, which frequently exhibit distribution shifts and temporal
changes in statistical properties like mean and variance over time. Previous
studies suggest that this inherent variability complicates forecasting,
limiting the performance of many models by leading to loss of non-stationarity
and resulting in over-stationarization (Liu, Wu, Wang and Long, 2022). To
address this challenge, we introduce a novel architecture, ChoronoAdaptive
Network (CANet), inspired by style-transfer techniques. The core of CANet is
the Non-stationary Adaptive Normalization module, seamlessly integrating the
Style Blending Gate and Adaptive Instance Normalization (AdaIN) (Huang and
Belongie, 2017). The Style Blending Gate preserves and reintegrates
non-stationary characteristics, such as mean and standard deviation, by
blending internal and external statistics, preventing over-stationarization
while maintaining essential temporal dependencies. Coupled with AdaIN, which
dynamically adapts the model to statistical changes, this approach enhances
predictive accuracy under non-stationary conditions. CANet also employs
multi-resolution patching to handle short-term fluctuations and long-term
trends, along with Fourier analysis-based adaptive thresholding to reduce
noise. A Stacked Kronecker Product Layer further optimizes the model's
efficiency while maintaining high performance. Extensive experiments on
real-world datasets validate CANet's superiority over state-of-the-art methods,
achieving a 42% reduction in MSE and a 22% reduction in MAE. The source code is
publicly available at https://github.com/mertsonmezer/CANet.

</details>

### [24] [Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts](https://arxiv.org/abs/2504.17921)
*Mateo Espinosa Zarlenga,Gabriele Dominici,Pietro Barbiero,Zohreh Shams,Mateja Jamnik*

Main category: cs.LG

TLDR: 论文研究了概念模型（CMs）在分布外（OOD）输入下的表现，发现现有CMs存在泄漏中毒问题，并提出了MixCEM模型以动态利用泄漏信息，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探究概念模型在OOD输入下的表现及概念干预的影响，发现现有模型的局限性。

Method: 提出MixCEM模型，动态利用泄漏信息以改进性能。

Result: MixCEM在有无概念干预的情况下，均显著优于基线模型，提升了分布内和分布外样本的准确性。

Conclusion: MixCEM有效解决了现有CMs的泄漏中毒问题，提升了模型在OOD输入下的鲁棒性。

Abstract: In this paper, we investigate how concept-based models (CMs) respond to
out-of-distribution (OOD) inputs. CMs are interpretable neural architectures
that first predict a set of high-level concepts (e.g., stripes, black) and then
predict a task label from those concepts. In particular, we study the impact of
concept interventions (i.e., operations where a human expert corrects a CM's
mispredicted concepts at test time) on CMs' task predictions when inputs are
OOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we
term leakage poisoning, that prevents them from properly improving their
accuracy when intervened on for OOD inputs. To address this, we introduce
MixCEM, a new CM that learns to dynamically exploit leaked information missing
from its concepts only when this information is in-distribution. Our results
across tasks with and without complete sets of concept annotations demonstrate
that MixCEMs outperform strong baselines by significantly improving their
accuracy for both in-distribution and OOD samples in the presence and absence
of concept interventions.

</details>

### [25] [Causality-Driven Neural Network Repair: Challenges and Opportunities](https://arxiv.org/abs/2504.17946)
*Fatemeh Vares,Brittany Johnson*

Main category: cs.LG

TLDR: 论文探讨了利用因果推理（如因果调试、反事实分析和结构因果模型）来修复深度神经网络（DNN）的局限性，并讨论了其在公平性、对抗鲁棒性和后门缓解中的应用。


<details>
  <summary>Details</summary>
Motivation: DNN通常依赖统计相关性而非因果推理，这限制了其鲁棒性和可解释性。现有的测试方法难以有效调试和修复DNN。

Method: 采用因果推理方法，包括因果调试、反事实分析和结构因果模型（SCMs），以识别和纠正DNN的故障。

Result: 这些技术通过针对性干预支持公平性、对抗鲁棒性和后门缓解。

Conclusion: 论文指出了可扩展性、泛化性和计算效率等挑战，并提出了未来方向，以通过因果驱动干预提升DNN的可靠性。

Abstract: Deep Neural Networks (DNNs) often rely on statistical correlations rather
than causal reasoning, limiting their robustness and interpretability. While
testing methods can identify failures, effective debugging and repair remain
challenging. This paper explores causal inference as an approach primarily for
DNN repair, leveraging causal debugging, counterfactual analysis, and
structural causal models (SCMs) to identify and correct failures. We discuss in
what ways these techniques support fairness, adversarial robustness, and
backdoor mitigation by providing targeted interventions. Finally, we discuss
key challenges, including scalability, generalization, and computational
efficiency, and outline future directions for integrating causality-driven
interventions to enhance DNN reliability.

</details>

### [26] [Mathematics of Continual Learning](https://arxiv.org/abs/2504.17963)
*Liangzu Peng,René Vidal*

Main category: cs.LG

TLDR: 本文探讨了持续学习与自适应滤波之间的数学联系，旨在通过自适应滤波的理论基础增强持续学习的数学框架。


<details>
  <summary>Details</summary>
Motivation: 持续学习的数学基础尚不完善，而自适应滤波在信号处理领域有丰富的数学理论，本文试图通过两者的联系填补这一空白。

Method: 回顾持续学习和自适应滤波的基本原理，并进行比较分析，揭示两者之间的联系。

Result: 通过自适应滤波的理论结果增强了持续学习的数学基础，并提出了持续学习的新研究方向。

Conclusion: 持续学习可以从自适应滤波的历史发展中汲取灵感，未来研究可以进一步探索两者的交叉点。

Abstract: Continual learning is an emerging subject in machine learning that aims to
solve multiple tasks presented sequentially to the learner without forgetting
previously learned tasks. Recently, many deep learning based approaches have
been proposed for continual learning, however the mathematical foundations
behind existing continual learning methods remain underdeveloped. On the other
hand, adaptive filtering is a classic subject in signal processing with a rich
history of mathematically principled methods. However, its role in
understanding the foundations of continual learning has been underappreciated.
In this tutorial, we review the basic principles behind both continual learning
and adaptive filtering, and present a comparative analysis that highlights
multiple connections between them. These connections allow us to enhance the
mathematical foundations of continual learning based on existing results for
adaptive filtering, extend adaptive filtering insights using existing continual
learning methods, and discuss a few research directions for continual learning
suggested by the historical developments in adaptive filtering.

</details>

### [27] [Self-Balancing, Memory Efficient, Dynamic Metric Space Data Maintenance, for Rapid Multi-Kernel Estimation](https://arxiv.org/abs/2504.18003)
*Aditya S Ellendula,Chandrajit Bajaj*

Main category: cs.LG

TLDR: 提出一种动态自平衡八叉树数据结构，用于高效维护演化度量空间中的邻域关系，适用于现代机器学习系统。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习系统中，许多学习和生成模型作为动态系统运行，其表示在训练过程中不断演化，需要快速、自适应的空间组织。

Method: 设计了一种支持对数时间更新和查询的双参数八叉树，避免了数据分布变化时的完全重建成本。

Result: 在四个领域验证了其有效性：加速Stein变分梯度下降、实现实时增量KNN分类、支持高效的动态索引与检索、提升样本效率。

Conclusion: 该方法在所有应用中均实现了指数级加速，同时保持准确性，尤其在高维空间中表现突出。

Abstract: We present a dynamic self-balancing octree data structure that enables
efficient neighborhood maintenance in evolving metric spaces, a key challenge
in modern machine learning systems. Many learning and generative models operate
as dynamical systems whose representations evolve during training, requiring
fast, adaptive spatial organization. Our two-parameter octree supports
logarithmic-time updates and queries, eliminating the need for costly full
rebuilds as data distributions shift. We demonstrate its effectiveness in four
areas: (1) accelerating Stein variational gradient descent by supporting more
particles with lower overhead; (2) enabling real-time, incremental KNN
classification with logarithmic complexity; (3) facilitating efficient, dynamic
indexing and retrieval for retrieval-augmented generation; and (4) improving
sample efficiency by jointly optimizing input and latent spaces. Across all
applications, our approach yields exponential speedups while preserving
accuracy, particularly in high-dimensional spaces where maintaining adaptive
spatial structure is critical.

</details>

### [28] [TGDT: A Temporal Graph-based Digital Twin for Urban Traffic Corridors](https://arxiv.org/abs/2504.18008)
*Nooshin Yousefzadeh,Rahul Sengupta,Sanjay Ranka*

Main category: cs.LG

TLDR: 提出了一种基于时空图神经网络的数字孪生框架（TGDT），用于实时、可扩展的交通信号优化，解决了现有深度学习模型在空间泛化性和实时部署上的不足。


<details>
  <summary>Details</summary>
Motivation: 城市信号灯交叉口的拥堵导致延误、经济损失和排放增加，现有深度学习模型缺乏空间泛化性且难以实时部署。

Method: 结合时序卷积网络和注意力图神经网络，构建动态、方向感知的交通模型，评估交通流优化的关键指标（MOEs）。

Result: TGDT在多输出估计、鲁棒性和准确性上优于现有方法，能在极端交通条件下高效运行，并支持秒级千场景模拟。

Conclusion: TGDT为交通信号优化提供了一种低成本、可解释且实时的解决方案。

Abstract: Urban congestion at signalized intersections leads to significant delays,
economic losses, and increased emissions. Existing deep learning models often
lack spatial generalizability, rely on complex architectures, and struggle with
real-time deployment. To address these limitations, we propose the Temporal
Graph-based Digital Twin (TGDT), a scalable framework that integrates Temporal
Convolutional Networks and Attentional Graph Neural Networks for dynamic,
direction-aware traffic modeling and assessment at urban corridors. TGDT
estimates key Measures of Effectiveness (MOEs) for traffic flow optimization at
both the intersection level (e.g., queue length, waiting time) and the corridor
level (e.g., traffic volume, travel time). Its modular architecture and
sequential optimization scheme enable easy extension to any number of
intersections and MOEs. The model outperforms state-of-the-art baselines by
accurately producing high-dimensional, concurrent multi-output estimates. It
also demonstrates high robustness and accuracy across diverse traffic
conditions, including extreme scenarios, while relying on only a minimal set of
traffic features. Fully parallelized, TGDT can simulate over a thousand
scenarios within a matter of seconds, offering a cost-effective, interpretable,
and real-time solution for traffic signal optimization.

</details>

### [29] [Addressing Concept Mislabeling in Concept Bottleneck Models Through Preference Optimization](https://arxiv.org/abs/2504.18026)
*Emiliano Penaloza,Tianyue H. Zhan,Laurent Charlin,Mateo Espinosa Zarlenga*

Main category: cs.LG

TLDR: 论文提出了一种新的损失函数CPO，用于解决概念瓶颈模型（CBMs）中概念标签错误的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: CBMs依赖准确的概念标签，但实际数据中标签错误会显著降低性能（某些情况下达25%），因此需要一种鲁棒的优化方法。

Method: 引入基于直接偏好优化（DPO）的CPO目标函数，优化概念的后验分布，减少对标签噪声的敏感性。

Result: CPO在三个真实数据集上（无论是否有标签噪声）均优于传统的二元交叉熵（BCE）。

Conclusion: CPO是一种有效的损失函数，能显著提升CBMs在标签噪声环境下的性能。

Abstract: Concept Bottleneck Models (CBMs) propose to enhance the trustworthiness of AI
systems by constraining their decisions on a set of human understandable
concepts. However, CBMs typically assume that datasets contains accurate
concept labels an assumption often violated in practice, which we show can
significantly degrade performance (by 25% in some cases). To address this, we
introduce the Concept Preference Optimization (CPO) objective, a new loss
function based on Direct Preference Optimization, which effectively mitigates
the negative impact of concept mislabeling on CBM performance. We provide an
analysis on some key properties of the CPO objective showing it directly
optimizes for the concept's posterior distribution, and contrast it against
Binary Cross Entropy (BCE) where we show CPO is inherently less sensitive to
concept noise. We empirically confirm our analysis finding that CPO
consistently outperforms BCE in three real world datasets with and without
added label noise.

</details>

### [30] [Modes of Sequence Models and Learning Coefficients](https://arxiv.org/abs/2504.18048)
*Zhongtian Chen,Daniel Murfet*

Main category: cs.LG

TLDR: 论文提出了一种几何方法分析序列建模，通过Hilbert空间框架和局部学习系数（LLC）揭示数据模式与损失景观的关系。


<details>
  <summary>Details</summary>
Motivation: 探索序列建模中数据模式与损失景观的可测量属性之间的联系，以理解网络参数的行为。

Method: 1. 将条件序列分布映射到Hilbert空间框架，应用张量分解识别主模式；2. 理论证明LLC估计对小幅度模式不敏感。

Result: LLC估计实际表征的是有效分布而非真实分布的几何特性，解释了网络参数非严格最小化时仍能获得可靠LLC估计的原因。

Conclusion: SGLD中的逆温度可作为景观结构的分辨率调节器，揭示了有效分布与损失景观的关系。

Abstract: We develop a geometric account of sequence modelling that links patterns in
the data to measurable properties of the loss landscape in transformer
networks. First, we cast conditional sequence distributions into a
Hilbert-space framework and apply tensor decompositions to identify their
principal modes. Truncating the small-amplitude modes yields an effective data
distribution that preserves dominant structure while discarding statistical
detail. Second, we show theoretically that Local Learning Coefficient (LLC)
estimates are insensitive to modes below a data-dependent threshold.
Consequently, the LLC calculated in practice characterises the geometry of the
effective rather than the true distribution. This insight clarifies why
reliable LLC estimates can be obtained even when a network parameter is not a
strict minimiser of the population loss, and it highlights how the inverse
temperature in SGLD acts as a resolution dial on the landscape structure.

</details>

### [31] [A Model Zoo on Phase Transitions in Neural Networks](https://arxiv.org/abs/2504.18072)
*Konstantin Schürholt,Léo Meynent,Yefan Zhou,Haiquan Lu,Yaoqing Yang,Damian Borth*

Main category: cs.LG

TLDR: 该论文提出了一种结合模型动物园和相信息的方法，创建了12个大规模、多样化的模型数据集，用于权重空间学习（WSL）研究。


<details>
  <summary>Details</summary>
Motivation: 现有模型动物园缺乏结构化多样性，而统计物理学中的相信息为模型多样性提供了理论基础。

Method: 结合模型动物园和相信息，创建12个大规模数据集，覆盖不同架构、规模和模态，并验证相覆盖。

Result: 提供了多样化的模型数据集，验证了相信息在模型训练、分析和稀疏化中的作用。

Conclusion: 该数据集为WSL研究提供了资源，并展示了相信息在下游任务中的潜在应用。

Abstract: Using the weights of trained Neural Network (NN) models as data modality has
recently gained traction as a research field - dubbed Weight Space Learning
(WSL). Multiple recent works propose WSL methods to analyze models, evaluate
methods, or synthesize weights. Weight space learning methods require
populations of trained models as datasets for development and evaluation.
However, existing collections of models - called `model zoos' - are
unstructured or follow a rudimentary definition of diversity. In parallel, work
rooted in statistical physics has identified phases and phase transitions in NN
models. Models are homogeneous within the same phase but qualitatively differ
from one phase to another. We combine the idea of `model zoos' with phase
information to create a controlled notion of diversity in populations. We
introduce 12 large-scale zoos that systematically cover known phases and vary
over model architecture, size, and datasets. These datasets cover different
modalities, such as computer vision, natural language processing, and
scientific ML. For every model, we compute loss landscape metrics and validate
full coverage of the phases. With this dataset, we provide the community with a
resource with a wide range of potential applications for WSL and beyond.
Evidence suggests the loss landscape phase plays a role in applications such as
model training, analysis, or sparsification. We demonstrate this in an
exploratory study of the downstream methods like transfer learning or model
weights averaging.

</details>

### [32] [Privacy-Preserving Personalized Federated Learning for Distributed Photovoltaic Disaggregation under Statistical Heterogeneity](https://arxiv.org/abs/2504.18078)
*Xiaolu Chen,Chenghao Huang,Yanru Zhang,Hao Wang*

Main category: cs.LG

TLDR: 提出了一种基于个性化联邦学习的分布式光伏分解框架，解决了隐私和统计异质性挑战。


<details>
  <summary>Details</summary>
Motivation: 分布式光伏的快速扩张导致能源管理和电网运营的复杂性增加，光伏分解需求迫切，但隐私和统计异质性成为障碍。

Method: 采用两层次框架，本地使用基于Transformer的模型生成辐照度嵌入，并引入自适应聚合机制；全局通过中央服务器实现跨中心知识共享。

Result: 实验证明该方法在准确性和鲁棒性上优于基准方法。

Conclusion: 该框架为光伏分解提供了一种隐私保护且高效的解决方案。

Abstract: The rapid expansion of distributed photovoltaic (PV) installations worldwide,
many being behind-the-meter systems, has significantly challenged energy
management and grid operations, as unobservable PV generation further
complicates the supply-demand balance. Therefore, estimating this generation
from net load, known as PV disaggregation, is critical. Given privacy concerns
and the need for large training datasets, federated learning becomes a
promising approach, but statistical heterogeneity, arising from geographical
and behavioral variations among prosumers, poses new challenges to PV
disaggregation. To overcome these challenges, a privacy-preserving distributed
PV disaggregation framework is proposed using Personalized Federated Learning
(PFL). The proposed method employs a two-level framework that combines local
and global modeling. At the local level, a transformer-based PV disaggregation
model is designed to generate solar irradiance embeddings for representing
local PV conditions. A novel adaptive local aggregation mechanism is adopted to
mitigate the impact of statistical heterogeneity on the local model, extracting
a portion of global information that benefits the local model. At the global
level, a central server aggregates information uploaded from multiple data
centers, preserving privacy while enabling cross-center knowledge sharing.
Experiments on real-world data demonstrate the effectiveness of this proposed
framework, showing improved accuracy and robustness compared to benchmark
methods.

</details>

### [33] [Efficient GNN Training Through Structure-Aware Randomized Mini-Batching](https://arxiv.org/abs/2504.18082)
*Vignesh Balaji,Christos Kozyrakis,Gal Chechik,Haggai Maron*

Main category: cs.LG

TLDR: 论文提出了一种结合社区结构和随机性的GNN小批量训练方法COMM-RAND，显著提升了训练效率，同时保持了模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有GNN小批量训练方法要么完全随机化（影响效率），要么完全依赖图结构（影响精度），需要一种折中方案。

Method: 提出COMM-RAND方法，在构建小批量时兼顾图的社区结构和随机性，优化GPU缓存利用。

Result: 在四个基准测试中，COMM-RAND平均减少1.8倍训练时间，精度损失仅0.42%。

Conclusion: COMM-RAND在效率和精度之间取得了平衡，为大规模GNN训练提供了实用解决方案。

Abstract: Graph Neural Networks (GNNs) enable learning on realworld graphs and
mini-batch training has emerged as the de facto standard for training GNNs
because it can scale to very large graphs and improve convergence. Current
mini-batch construction policies largely ignore efficiency considerations of
GNN training. Specifically, existing mini-batching techniques employ
randomization schemes to improve accuracy and convergence. However, these
randomization schemes are often agnostic to the structural properties of the
graph (for eg. community structure), resulting in highly irregular memory
access patterns during GNN training that make suboptimal use of on-chip GPU
caches. On the other hand, while deterministic mini-batching based solely on
graph structure delivers fast runtime performance, the lack of randomness
compromises both the final model accuracy and training convergence speed. In
this paper, we present Community-structure-aware Randomized Mini-batching
(COMM-RAND), a novel methodology that bridges the gap between the above
extremes. COMM-RAND allows practitioners to explore the space between pure
randomness and pure graph structural awareness during mini-batch construction,
leading to significantly more efficient GNN training with similar accuracy. We
evaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND
cuts down GNN training time by up to 2.76x (1.8x on average) while achieving an
accuracy that is within 1.79% points (0.42% on average) compared to popular
random mini-batching approaches.

</details>

### [34] [Persistence of Backdoor-based Watermarks for Neural Networks: A Comprehensive Evaluation](https://arxiv.org/abs/2501.02704)
*Anh Tu Ngo,Chuan Song Heng,Nandish Chattopadhyay,Anupam Chattopadhyay*

Main category: cs.LG

TLDR: 本文研究了深度神经网络（DNN）中基于后门的水印方案的鲁棒性，提出了一种数据驱动的方法以在微调后恢复水印，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: DNN因其卓越性能被广泛部署，但训练成本高使其成为知识产权。现有后门水印方案在对抗攻击和微调等场景下的鲁棒性存疑，亟需研究其持久性。

Method: 提出一种数据驱动的方法，通过引入微调后的训练数据恢复水印，避免触发集暴露。实验验证了该方法在参数变化不大时的有效性。

Result: 实验表明，仅需引入微调后的训练数据，水印可恢复至高达100%的触发准确率。损失景观可视化进一步解释了恢复机制。

Conclusion: 研究证实了数据驱动方法在恢复水印中的潜力，并探讨了微调阶段引入训练数据以缓解水印消失的策略。

Abstract: Deep Neural Networks (DNNs) have gained considerable traction in recent years
due to the unparalleled results they gathered. However, the cost behind
training such sophisticated models is resource intensive, resulting in many to
consider DNNs to be intellectual property (IP) to model owners. In this era of
cloud computing, high-performance DNNs are often deployed all over the internet
so that people can access them publicly. As such, DNN watermarking schemes,
especially backdoor-based watermarks, have been actively developed in recent
years to preserve proprietary rights. Nonetheless, there lies much uncertainty
on the robustness of existing backdoor watermark schemes, towards both
adversarial attacks and unintended means such as fine-tuning neural network
models. One reason for this is that no complete guarantee of robustness can be
assured in the context of backdoor-based watermark. In this paper, we
extensively evaluate the persistence of recent backdoor-based watermarks within
neural networks in the scenario of fine-tuning, we propose/develop a novel
data-driven idea to restore watermark after fine-tuning without exposing the
trigger set. Our empirical results show that by solely introducing training
data after fine-tuning, the watermark can be restored if model parameters do
not shift dramatically during fine-tuning. Depending on the types of trigger
samples used, trigger accuracy can be reinstated to up to 100%. Our study
further explores how the restoration process works using loss landscape
visualization, as well as the idea of introducing training data in fine-tuning
stage to alleviate watermark vanishing.

</details>

### [35] [Reliable and Efficient Inverse Analysis using Physics-Informed Neural Networks with Distance Functions and Adaptive Weight Tuning](https://arxiv.org/abs/2504.18091)
*Shota Deguchi,Mitsuteru Asai*

Main category: cs.LG

TLDR: 本文提出了一种基于距离函数（R函数）的方法来精确满足边界条件，解决了传统惩罚方法在物理信息神经网络（PINN）中的局限性，并展示了其在逆问题中的高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统惩罚方法在PINN中无法精确满足边界条件且对惩罚参数敏感，限制了PINN的准确性。

Method: 利用R函数提供的归一化距离场来精确表示边界几何形状，并扩展到逆问题中，结合自适应权重调整技术。

Result: 数值实验表明，该方法在复杂非凸几何形状下比惩罚方法更准确高效。

Conclusion: 该方法为PINN的逆分析提供了可靠高效的框架，具有广泛的工程应用潜力。

Abstract: Physics-informed neural networks have attracted significant attention in
scientific machine learning for their capability to solve forward and inverse
problems governed by partial differential equations. However, the accuracy of
PINN solutions is often limited by the treatment of boundary conditions.
Conventional penalty-based methods, which incorporate boundary conditions as
penalty terms in the loss function, cannot guarantee exact satisfaction of the
given boundary conditions and are highly sensitive to the choice of penalty
parameters. This paper demonstrates that distance functions, specifically
R-functions, can be leveraged to enforce boundary conditions, overcoming these
limitations. R-functions provide normalized distance fields, enabling accurate
representation of boundary geometries, including non-convex domains, and
facilitating various types of boundary conditions. We extend this distance
function-based boundary condition imposition method to inverse problems using
PINNs and introduce an adaptive weight tuning technique to ensure reliable and
efficient inverse analysis. We demonstrate the efficacy of the method through
several numerical experiments. Numerical results show that the proposed method
solves inverse problems more accurately and efficiently than penalty-based
methods, even in the presence of complex non-convex geometries. This approach
offers a reliable and efficient framework for inverse analysis using PINNs,
with potential applications across a wide range of engineering problems.

</details>

### [36] [Subject-independent Classification of Meditative State from the Resting State using EEG](https://arxiv.org/abs/2504.18095)
*Jerrin Thomas Panachakel,Pradeep Kumar G.,Suryaa Seran,Kanishka Sharma,Ramakrishnan Angarai Ganesan*

Main category: cs.LG

TLDR: 研究旨在通过EEG数据区分冥想与静息状态，提出三种架构，其中CSP-LDA-LSTM在个体内分类中表现最佳（98.2%准确率），SVD-NN在个体间分类中表现显著（96.4%准确率）。


<details>
  <summary>Details</summary>
Motivation: 现有研究多依赖个体数据，本研究旨在实现不依赖个体的冥想状态识别。

Method: 提出三种架构：CSP-LDA、CSP-LDA-LSTM和SVD-NN，分别用于特征提取和分类。

Result: CSP-LDA-LSTM在个体内分类中准确率达98.2%，SVD-NN在个体间分类中达96.4%。

Conclusion: 两种架构能捕捉不依赖个体的EEG特征，系统鲁棒性强且泛化能力好。

Abstract: While it is beneficial to objectively determine whether a subject is
meditating, most research in the literature reports good results only in a
subject-dependent manner. This study aims to distinguish the modified state of
consciousness experienced during Rajyoga meditation from the resting state of
the brain in a subject-independent manner using EEG data. Three architectures
have been proposed and evaluated: The CSP-LDA Architecture utilizes common
spatial pattern (CSP) for feature extraction and linear discriminant analysis
(LDA) for classification. The CSP-LDA-LSTM Architecture employs CSP for feature
extraction, LDA for dimensionality reduction, and long short-term memory (LSTM)
networks for classification, modeling the binary classification problem as a
sequence learning problem. The SVD-NN Architecture uses singular value
decomposition (SVD) to select the most relevant components of the EEG signals
and a shallow neural network (NN) for classification. The CSP-LDA-LSTM
architecture gives the best performance with 98.2% accuracy for intra-subject
classification. The SVD-NN architecture provides significant performance with
96.4\% accuracy for inter-subject classification. This is comparable to the
best-reported accuracies in the literature for intra-subject classification.
Both architectures are capable of capturing subject-invariant EEG features for
effectively classifying the meditative state from the resting state. The high
intra-subject and inter-subject classification accuracies indicate these
systems' robustness and their ability to generalize across different subjects.

</details>

### [37] [Temperature Estimation in Induction Motors using Machine Learning](https://arxiv.org/abs/2504.18105)
*Dinan Li,Panagiotis Kakosimos*

Main category: cs.LG

TLDR: 论文探讨了数据驱动方法在感应电机温度和轴承温度估计中的应用，比较了多种机器学习算法，发现神经网络在瞬态条件下表现良好。


<details>
  <summary>Details</summary>
Motivation: 随着电动动力系统的普及，确保其可靠运行和预防故障变得至关重要，而传统建模方法复杂且依赖专家知识。

Method: 通过实验数据，研究多种机器学习算法（从线性模型到神经网络）的温度估计能力，并进行超参数优化。

Result: 神经网络在瞬态条件下表现优异，能够准确估计温度和轴承温度。

Conclusion: 数据驱动方法，尤其是神经网络，为电机温度监控提供了高效且准确的解决方案。

Abstract: The number of electrified powertrains is ever increasing today towards a more
sustainable future; thus, it is essential that unwanted failures are prevented,
and a reliable operation is secured. Monitoring the internal temperatures of
motors and keeping them under their thresholds is an important first step.
Conventional modeling methods require expert knowledge and complicated
mathematical approaches. With all the data a modern electric drive collects
nowadays during the system operation, it is feasible to apply data-driven
approaches for estimating thermal behaviors. In this paper, multiple
machine-learning methods are investigated on their capability to approximate
the temperatures of the stator winding and bearing in induction motors. The
explored algorithms vary from linear to neural networks. For this reason,
experimental lab data have been captured from a powertrain under predetermined
operating conditions. For each approach, a hyperparameter search is then
performed to find the optimal configuration. All the models are evaluated by
various metrics, and it has been found that neural networks perform
satisfactorily even under transient conditions.

</details>

### [38] [Learning from Less: SINDy Surrogates in RL](https://arxiv.org/abs/2504.18113)
*Aniket Dixit,Muhammad Ibrahim Khan,Faizan Ahmed,James Brusey*

Main category: cs.LG

TLDR: 该论文提出了一种使用SINDy算法开发强化学习（RL）替代环境的方法，实验表明该方法能显著降低计算成本并保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 通过开发高效的替代环境，减少RL训练的计算成本，同时保持性能。

Method: 使用SINDy算法构建替代环境，并在OpenAI Gym的Mountain Car和Lunar Lander环境中验证。

Result: 替代模型准确捕捉环境动态，计算成本降低20-35%，RL训练步数减少且性能相当。

Conclusion: 该方法为基于模型的RL提供了一种高效、准确的替代环境生成方案。

Abstract: This paper introduces an approach for developing surrogate environments in
reinforcement learning (RL) using the Sparse Identification of Nonlinear
Dynamics (SINDy) algorithm. We demonstrate the effectiveness of our approach
through extensive experiments in OpenAI Gym environments, particularly Mountain
Car and Lunar Lander. Our results show that SINDy-based surrogate models can
accurately capture the underlying dynamics of these environments while reducing
computational costs by 20-35%. With only 75 interactions for Mountain Car and
1000 for Lunar Lander, we achieve state-wise correlations exceeding 0.997, with
mean squared errors as low as 3.11e-06 for Mountain Car velocity and 1.42e-06
for LunarLander position. RL agents trained in these surrogate environments
require fewer total steps (65,075 vs. 100,000 for Mountain Car and 801,000 vs.
1,000,000 for Lunar Lander) while achieving comparable performance to those
trained in the original environments, exhibiting similar convergence patterns
and final performance metrics. This work contributes to the field of
model-based RL by providing an efficient method for generating accurate,
interpretable surrogate environments.

</details>

### [39] [Think, Prune, Train, Improve: Scaling Reasoning without Scaling Models](https://arxiv.org/abs/2504.18116)
*Caia Costello,Simon Guo,Anna Goldie,Azalia Mirhoseini*

Main category: cs.LG

TLDR: 论文提出了一种通过合成数据和迭代微调提升大语言模型性能的方法，显著提高了模型在数学推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在编程和数学推理任务中表现优异，但受限于高质量训练数据的不足。

Method: 引入Think, Prune, Train流程，通过迭代微调模型自身生成的推理轨迹，并结合真实数据修剪确保数据质量。

Result: 在GSM8K任务中，Gemma2-2B和Gemma2-9B的性能显著提升，LLaMA-3.1-70B甚至超过GPT-4o。

Conclusion: 通过自生成推理和系统性数据选择，可以有效提升大语言模型的能力。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
programming and mathematical reasoning tasks, but are constrained by limited
high-quality training data. Synthetic data can be leveraged to enhance
fine-tuning outcomes, but several factors influence this process, including
model size, synthetic data volume, pruning strategy, and number of fine-tuning
rounds. We explore these axes and investigate which conditions enable model
self-improvement. We introduce the Think, Prune, Train process, a scalable
framework that iteratively fine-tunes models on their own reasoning traces,
using ground-truth pruning to ensure high-quality training data. This approach
yields improved performance: on GSM8K, Gemma2-2B achieves a Pass@1 of 57.6%
(from 41.9%), Gemma2-9B reaches 82%, matching LLaMA-3.1-70B, and LLaMA-3.1-70B
attains 91%, even surpassing GPT-4o, demonstrating the effectiveness of
self-generated reasoning and systematic data selection for improving LLM
capabilities.

</details>

### [40] [Score-Based Deterministic Density Sampling](https://arxiv.org/abs/2504.18130)
*Vasily Ilin,Bamdad Hosseini,Jingwei Hu*

Main category: cs.LG

TLDR: 提出了一种基于分数传输建模（SBTM）的确定性采样框架，用于采样未归一化的目标密度π。SBTM通过动态学习时间变化的分数函数，近似Wasserstein梯度流，提供平滑且可解释的轨迹。


<details>
  <summary>Details</summary>
Motivation: 解决在仅知道∇logπ的情况下采样目标密度的挑战性问题，避免扩散生成模型对预训练分数函数的依赖。

Method: 使用分数匹配动态学习时间变化的分数函数，近似Wasserstein梯度流，并扩展至退火动力学以处理非对数凹目标。

Result: SBTM以最优速率收敛，轨迹平滑且易于与退火动力学结合，优于ULA和退火ULA基线。

Conclusion: SBTM为采样未归一化目标密度提供了一种高效、可解释且收敛性强的确定性方法。

Abstract: We propose and analyze a deterministic sampling framework using Score-Based
Transport Modeling (SBTM) for sampling an unnormalized target density $\pi$.
While diffusion generative modeling relies on pre-training the score function
$\nabla \log f_t$ using samples from $\pi$, SBTM addresses the more general and
challenging setting where only $\nabla \log\pi$ is known. SBTM approximates the
Wasserstein gradient flow on KL$(f_t\|\pi)$ by learning the time-varying score
$\nabla \log f_t$ on the fly using score matching. The learned score gives
immediate access to relative Fisher information, providing a built-in
convergence diagnostic. The deterministic trajectories are smooth,
interpretable, and free of Brownian-motion noise, while having the same
distribution as ULA. We prove that SBTM dissipates relative entropy at the same
rate as the exact gradient flow, provided sufficient training. We further
extend our framework to annealed dynamics, to handle non log-concave targets.
Numerical experiments validate our theoretical findings: SBTM converges at the
optimal rate, has smooth trajectories, and is easily integrated with annealed
dynamics. We compare to the baselines of ULA and annealed ULA.

</details>

### [41] [Tree Boosting Methods for Balanced andImbalanced Classification and their Robustness Over Time in Risk Assessment](https://arxiv.org/abs/2504.18133)
*Gissel Velarde,Michael Weichert,Anuj Deshmunkh,Sanjay Deshmane,Anindya Sudhir,Khushboo Sharma,Vaibhav Joshi*

Main category: cs.LG

TLDR: 论文评估了树提升方法（如XGBoost和Imbalance-XGBoost）在不平衡数据集上的性能，发现数据量和类别分布对检测性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 解决不平衡数据集中少数类难以检测的问题，提升机器学习在风险评估中的表现。

Method: 通过数据准备、树提升方法及超参数优化，评估不同数据量和类别分布下的性能。

Result: 方法在数据量增加时性能提升，但F1分数随不平衡加剧而下降；超参数优化可改善检测，但需谨慎应用。

Conclusion: 所提方法对数据变化具有鲁棒性，但需定期重新训练以维持性能。

Abstract: Most real-world classification problems deal with imbalanced datasets, posing
a challenge for Artificial Intelligence (AI), i.e., machine learning
algorithms, because the minority class, which is of extreme interest, often
proves difficult to be detected. This paper empirically evaluates tree boosting
methods' performance given different dataset sizes and class distributions,
from perfectly balanced to highly imbalanced. For tabular data, tree-based
methods such as XGBoost, stand out in several benchmarks due to detection
performance and speed. Therefore, XGBoost and Imbalance-XGBoost are evaluated.
After introducing the motivation to address risk assessment with machine
learning, the paper reviews evaluation metrics for detection systems or binary
classifiers. It proposes a method for data preparation followed by tree
boosting methods including hyper-parameter optimization. The method is
evaluated on private datasets of 1 thousand (K), 10K and 100K samples on
distributions with 50, 45, 25, and 5 percent positive samples. As expected, the
developed method increases its recognition performance as more data is given
for training and the F1 score decreases as the data distribution becomes more
imbalanced, but it is still significantly superior to the baseline of
precision-recall determined by the ratio of positives divided by positives and
negatives. Sampling to balance the training set does not provide consistent
improvement and deteriorates detection. In contrast, classifier hyper-parameter
optimization improves recognition, but should be applied carefully depending on
data volume and distribution. Finally, the developed method is robust to data
variation over time up to some point. Retraining can be used when performance
starts deteriorating.

</details>

### [42] [A Generative Graph Contrastive Learning Model with Global Signal](https://arxiv.org/abs/2504.18148)
*Xiaofan Wei,Binyan Zhang*

Main category: cs.LG

TLDR: CSG2L框架通过SVD增强模块和局部-全局依赖学习模块，解决了图对比学习中信号偏差和样本对权重分配问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习模型因随机扰动和样本对权重分配不均导致性能下降。

Method: 提出CSG2L框架，包括SVD增强模块（避免噪声）和局部-全局依赖学习模块（自适应加权）。

Result: 在基准数据集上优于现有方法，且兼容多种GNN。

Conclusion: CSG2L有效解决了信号偏差和权重分配问题，提升了图对比学习性能。

Abstract: Graph contrastive learning (GCL) has garnered significant attention recently
since it learns complex structural information from graphs through
self-supervised learning manner. However, prevalent GCL models may suffer from
performance degradation due to inappropriate contrastive signals. Concretely,
they commonly generate augmented views based on random perturbation, which
leads to biased essential structures due to the introduction of noise. In
addition, they assign equal weight to both hard and easy sample pairs, thereby
ignoring the difference in importance of the sample pairs. To address these
issues, this study proposes a novel Contrastive Signal Generative Framework for
Accurate Graph Learning (CSG2L) with the following two-fold ideas: a) building
a singular value decomposition (SVD)-directed augmented module (SVD-aug) to
obtain the global interactions as well as avoiding the random noise
perturbation; b) designing a local-global dependency learning module (LGDL)
with an adaptive reweighting strategy which can differentiate the effects of
hard and easy sample pairs. Extensive experiments on benchmark datasets
demonstrate that the proposed CSG2L outperforms the state-of-art baselines.
Moreover, CSG2L is compatible with a variety of GNNs.

</details>

### [43] [Offline Learning of Controllable Diverse Behaviors](https://arxiv.org/abs/2504.18160)
*Mathieu Petitbois,Rémy Portelas,Sylvain Lamprier,Ludovic Denoyer*

Main category: cs.LG

TLDR: 提出了一种基于时间一致性和可控性的模仿学习方法，克服传统方法在行为多样性和可控轨迹生成上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习方法主要依赖专家数据集生成单一策略，或仅关注过渡级多样性，无法充分复现演示的多样性或实现可控轨迹生成。

Method: 采用时间一致性确保整个片段的行为一致，并通过构建行为潜在空间实现可控性，允许用户按需激活特定行为。

Result: 在多种任务和环境中与现有方法对比，验证了方法的有效性。

Conclusion: 新方法在行为多样性和可控性上优于现有技术，为模仿学习提供了更灵活的解决方案。

Abstract: Imitation Learning (IL) techniques aim to replicate human behaviors in
specific tasks. While IL has gained prominence due to its effectiveness and
efficiency, traditional methods often focus on datasets collected from experts
to produce a single efficient policy. Recently, extensions have been proposed
to handle datasets of diverse behaviors by mainly focusing on learning
transition-level diverse policies or on performing entropy maximization at the
trajectory level. While these methods may lead to diverse behaviors, they may
not be sufficient to reproduce the actual diversity of demonstrations or to
allow controlled trajectory generation. To overcome these drawbacks, we propose
a different method based on two key features: a) Temporal Consistency that
ensures consistent behaviors across entire episodes and not just at the
transition level as well as b) Controllability obtained by constructing a
latent space of behaviors that allows users to selectively activate specific
behaviors based on their requirements. We compare our approach to
state-of-the-art methods over a diverse set of tasks and environments. Project
page: https://mathieu-petitbois.github.io/projects/swr/

</details>

### [44] [Unveiling 3D Ocean Biogeochemical Provinces: A Machine Learning Approach for Systematic Clustering and Validation](https://arxiv.org/abs/2504.18181)
*Yvonne Jenniges,Maike Sonnewald,Sebastian Maneth,Are Olsen,Boris P. Koch*

Main category: cs.LG

TLDR: 本文提出了一种数据驱动的机器学习方法，用于客观定义北大西洋的区域和水团，通过多种聚类方法和验证技术，最终确定了321个聚类，结果与现有水团定义高度一致。


<details>
  <summary>Details</summary>
Motivation: 传统海洋区域和水团定义常依赖主观决策，可能导致误导性和不可重复的结果，因此需要一种客观、系统的方法。

Method: 采用多种聚类方法（KMeans、Ward、DBSCAN）结合UMAP降维技术，并通过外部、内部和相对验证技术进行系统验证。

Result: UMAP-DBSCAN方法表现最佳，最终生成321个聚类，与现有水团定义高度一致，且区域化更详细。

Conclusion: 该方法客观、高效且可重复，有助于未来研究海洋生物地球化学差异和变化。

Abstract: Defining ocean regions and water masses helps to understand marine processes
and can serve downstream-tasks such as defining marine protected areas.
However, such definitions are often a result of subjective decisions
potentially producing misleading, unreproducible results. Here, the aim was to
objectively define regions of the North Atlantic. For this, a data-driven,
systematic machine learning approach was applied to generate and validate ocean
clusters employing external, internal and relative validation techniques. About
300 million measured salinity, temperature, and oxygen, nitrate, phosphate and
silicate concentration values served as input for various clustering methods
(KMeans, agglomerative Ward, and Density-Based Spatial Clustering of
Applications with Noise (DBSCAN)). Uniform Manifold Approximation and
Projection (UMAP) emphasised (dis-)similarities in the data while reducing
dimensionality. Based on a systematic validation of the considered clustering
methods and their hyperparameters, the results showed that UMAP-DBSCAN best
represented the data. To address stochastic variability, 100 UMAP-DBSCAN
clustering runs were conducted and aggregated using Native Emergent Manifold
Interrogation (NEMI), producing a final set of 321 clusters. Reproducibility
was evaluated by calculating the ensemble overlap (88.81 +- 1.8%) and the mean
grid cell-wise uncertainty estimated by NEMI (15.49 +- 20%). The presented
clustering results agreed very well with common water mass definitions. This
study revealed a more detailed regionalization compared to previous concepts
such as the Longhurst provinces. The applied method is objective, efficient and
reproducible and will support future research focusing on biogeochemical
differences and changes in oceanic regions.

</details>

### [45] [An Open-Source and Reproducible Implementation of LSTM and GRU Networks for Time Series Forecasting](https://arxiv.org/abs/2504.18185)
*Gissel Velarde,Pedro Branez,Alejandro Bueno,Rodrigo Heredia,Mateo Lopez-Ledezma*

Main category: cs.LG

TLDR: 本文介绍了LSTM和GRU网络的开源可复现实现，用于时间序列预测，并在两个数据集上评估了其性能。


<details>
  <summary>Details</summary>
Motivation: 评估LSTM和GRU网络在时间序列预测中的表现，因其在相关工作中报告的性能。

Method: 在两个数据集（金融股票数据和合成活动数据）上测试LSTM和GRU网络，使用RMSE和方向准确性作为评估指标。

Result: 在合成数据集上，LSTM和GRU显著优于基线；在股票数据集上表现与基线相似。

Conclusion: 单个时间序列可用于训练网络，前提是数据包含重复模式。开源实现和数据以促进未来研究。

Abstract: This paper introduces an open-source and reproducible implementation of Long
Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) Networks for time
series forecasting. We evaluated LSTM and GRU networks because of their
performance reported in related work. We describe our method and its results on
two datasets. The first dataset is the S&P BSE BANKEX, composed of stock time
series (closing prices) of ten financial institutions. The second dataset,
called Activities, comprises ten synthetic time series resembling weekly
activities with five days of high activity and two days of low activity. We
report Root Mean Squared Error (RMSE) between actual and predicted values, as
well as Directional Accuracy (DA). We show that a single time series from a
dataset can be used to adequately train the networks if the sequences in the
dataset contain patterns that repeat, even with certain variation, and are
properly processed. For 1-step ahead and 20-step ahead forecasts, LSTM and GRU
networks significantly outperform a baseline on the Activities dataset. The
baseline simply repeats the last available value. On the stock market dataset,
the networks perform just like the baseline, possibly due to the nature of
these series. We release the datasets used as well as the implementation with
all experiments performed to enable future comparisons and to make our research
reproducible.

</details>

### [46] [A Machine Learning Approach For Bitcoin Forecasting](https://arxiv.org/abs/2504.18206)
*Stefano Sossi-Rojas,Gissel Velarde,Damian Zieba*

Main category: cs.LG

TLDR: 论文提出了一种新的时间序列组合，结合机器学习集成方法，提高了比特币价格方向预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明仅使用收盘价不足以准确预测股市序列，因此需要引入更多时间序列以提高预测精度。

Method: 通过实验分析不同时间序列和机器学习算法的组合，确定最佳预测模型。

Result: 发现Open、High和Low时间序列对方向准确性贡献最大，其中Low的贡献最显著，结合GRU网络和基线预测效果最佳。

Conclusion: 提出的方法在方向准确性上与现有最优方法表现相当，非价格相关特征影响较小。

Abstract: Bitcoin is one of the cryptocurrencies that is gaining more popularity in
recent years. Previous studies have shown that closing price alone is not
enough to forecast stock market series. We introduce a new set of time series
and demonstrate that a subset is necessary to improve directional accuracy
based on a machine learning ensemble. In our experiments, we study which time
series and machine learning algorithms deliver the best results. We found that
the most relevant time series that contribute to improving directional accuracy
are Open, High and Low, with the largest contribution of Low in combination
with an ensemble of Gated Recurrent Unit network and a baseline forecast. The
relevance of other Bitcoin-related features that are not price-related is
negligible. The proposed method delivers similar performance to the
state-of-the-art when observing directional accuracy.

</details>

### [47] [Gradient Descent as a Shrinkage Operator for Spectral Bias](https://arxiv.org/abs/2504.18207)
*Simon Lucey*

Main category: cs.LG

TLDR: 论文探讨了激活函数与样条回归/平滑之间的联系，分析了其对1D浅层网络频谱偏差的影响，并提出梯度下降（GD）作为收缩算子的新视角。


<details>
  <summary>Details</summary>
Motivation: 研究激活函数选择如何影响频谱偏差，以及梯度下降如何通过隐式选择频率分量来控制这种偏差。

Method: 通过理论分析将GD重新解释为收缩算子，并提出GD超参数与带宽之间的显式关系。

Result: GD正则化仅对单调激活函数有效，非单调激活函数（如sinc、高斯）可作为频谱偏差的高效替代。

Conclusion: 研究揭示了激活函数选择与GD超参数对频谱偏差的联合影响，为非单调激活函数的应用提供了理论支持。

Abstract: We generalize the connection between activation function and spline
regression/smoothing and characterize how this choice may influence spectral
bias within a 1D shallow network. We then demonstrate how gradient descent (GD)
can be reinterpreted as a shrinkage operator that masks the singular values of
a neural network's Jacobian. Viewed this way, GD implicitly selects the number
of frequency components to retain, thereby controlling the spectral bias. An
explicit relationship is proposed between the choice of GD hyperparameters
(learning rate & number of iterations) and bandwidth (the number of active
components). GD regularization is shown to be effective only with monotonic
activation functions. Finally, we highlight the utility of non-monotonic
activation functions (sinc, Gaussian) as iteration-efficient surrogates for
spectral bias.

</details>

### [48] [Ultra-fast feature learning for the training of two-layer neural networks in the two-timescale regime](https://arxiv.org/abs/2504.18208)
*Raphaël Barboni,Gabriel Peyré,François-Xavier Vialard*

Main category: cs.LG

TLDR: 研究了均值场单隐藏层神经网络在平方损失下的梯度方法收敛性，通过变量投影（VarPro）算法消除线性变量，将学习问题简化为特征分布的训练。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络训练中特征分布的动态变化及其收敛性，特别是在教师-学生框架下。

Method: 采用变量投影（VarPro）或双时间尺度学习算法，将问题转化为特征分布的训练，并分析其动态方程。

Result: 在正则化强度趋近于零时，特征分布的动态对应于加权超快扩散方程，证明了特征分布向教师特征分布的收敛性。

Conclusion: 该方法为教师-学生框架下特征分布的收敛提供了理论保证。

Abstract: We study the convergence of gradient methods for the training of mean-field
single hidden layer neural networks with square loss. Observing this is a
separable non-linear least-square problem which is linear w.r.t. the outer
layer's weights, we consider a Variable Projection (VarPro) or two-timescale
learning algorithm, thereby eliminating the linear variables and reducing the
learning problem to the training of the feature distribution. Whereas most
convergence rates or the training of neural networks rely on a neural tangent
kernel analysis where features are fixed, we show such a strategy enables
provable convergence rates for the sampling of a teacher feature distribution.
Precisely, in the limit where the regularization strength vanishes, we show
that the dynamic of the feature distribution corresponds to a weighted
ultra-fast diffusion equation. Relying on recent results on the asymptotic
behavior of such PDEs, we obtain guarantees for the convergence of the trained
feature distribution towards the teacher feature distribution in a
teacher-student setup.

</details>

### [49] [Learning to fuse: dynamic integration of multi-source data for accurate battery lifespan prediction](https://arxiv.org/abs/2504.18230)
*He Shanxuan,Lin Zuhong,Yu Bolun,Gao Xu,Long Biao,Yao Jingjing*

Main category: cs.LG

TLDR: 提出了一种混合学习框架，通过动态多源数据融合和堆叠集成模型，精确预测锂离子电池寿命，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 锂离子电池寿命预测对电动汽车和智能电网等应用的可靠性和维护成本至关重要。

Method: 结合动态多源数据融合和堆叠集成模型（包括Ridge回归、LSTM和XGBoost），利用熵动态加权机制处理异构数据。

Result: MAE为0.0058，RMSE为0.0092，R2为0.9839，性能显著优于基线模型。

Conclusion: 该框架可扩展且可解释，有助于优化电池健康管理，提升能源存储系统的安全性和维护效率。

Abstract: Accurate prediction of lithium-ion battery lifespan is vital for ensuring
operational reliability and reducing maintenance costs in applications like
electric vehicles and smart grids. This study presents a hybrid learning
framework for precise battery lifespan prediction, integrating dynamic
multi-source data fusion with a stacked ensemble (SE) modeling approach. By
leveraging heterogeneous datasets from the National Aeronautics and Space
Administration (NASA), Center for Advanced Life Cycle Engineering (CALCE),
MIT-Stanford-Toyota Research Institute (TRC), and nickel cobalt aluminum (NCA)
chemistries, an entropy-based dynamic weighting mechanism mitigates variability
across heterogeneous datasets. The SE model combines Ridge regression, long
short-term memory (LSTM) networks, and eXtreme Gradient Boosting (XGBoost),
effectively capturing temporal dependencies and nonlinear degradation patterns.
It achieves a mean absolute error (MAE) of 0.0058, root mean square error
(RMSE) of 0.0092, and coefficient of determination (R2) of 0.9839,
outperforming established baseline models with a 46.2% improvement in R2 and an
83.2% reduction in RMSE. Shapley additive explanations (SHAP) analysis
identifies differential discharge capacity (Qdlin) and temperature of
measurement (Temp_m) as critical aging indicators. This scalable, interpretable
framework enhances battery health management, supporting optimized maintenance
and safety across diverse energy storage systems, thereby contributing to
improved battery health management in energy storage systems.

</details>

### [50] [DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering](https://arxiv.org/abs/2504.18243)
*Rong Cheng,Jinyi Liu,YAN ZHENG,Fei Ni,Jiazhen Du,Hangyu Mao,Fuzheng Zhang,Bo Wang,Jianye HAO*

Main category: cs.LG

TLDR: DualRAG框架通过协同的双过程（RaQ和pKA）提升多跳问答任务的表现，显著提高答案准确性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多跳问答任务中难以动态组织和整合知识，DualRAG旨在解决这一问题。

Method: 提出DualRAG框架，结合推理增强查询（RaQ）和渐进知识聚合（pKA）两个紧密耦合的过程。

Result: 实验表明DualRAG显著提升性能，部分情况下超越基于预知知识的性能。

Conclusion: DualRAG是复杂多跳推理任务的稳健高效解决方案。

Abstract: Multi-Hop Question Answering (MHQA) tasks permeate real-world applications,
posing challenges in orchestrating multi-step reasoning across diverse
knowledge domains. While existing approaches have been improved with iterative
retrieval, they still struggle to identify and organize dynamic knowledge. To
address this, we propose DualRAG, a synergistic dual-process framework that
seamlessly integrates reasoning and retrieval. DualRAG operates through two
tightly coupled processes: Reasoning-augmented Querying (RaQ) and progressive
Knowledge Aggregation (pKA). They work in concert: as RaQ navigates the
reasoning path and generates targeted queries, pKA ensures that newly acquired
knowledge is systematically integrated to support coherent reasoning. This
creates a virtuous cycle of knowledge enrichment and reasoning refinement.
Through targeted fine-tuning, DualRAG preserves its sophisticated reasoning and
retrieval capabilities even in smaller-scale models, demonstrating its
versatility and core advantages across different scales. Extensive experiments
demonstrate that this dual-process approach substantially improves answer
accuracy and coherence, approaching, and in some cases surpassing, the
performance achieved with oracle knowledge access. These results establish
DualRAG as a robust and efficient solution for complex multi-hop reasoning
tasks.

</details>

### [51] [Local Statistical Parity for the Estimation of Fair Decision Trees](https://arxiv.org/abs/2504.18262)
*Andrea Quintanilla,Johan Van Horebeek*

Main category: cs.LG

TLDR: 论文提出了一种基于局部公平性准则的决策树估计算法C-LRT，通过修改标准CART算法并结合约束逻辑回归，平衡分类准确性与公平性。


<details>
  <summary>Details</summary>
Motivation: 传统决策树构建方法逐节点递归，难以兼顾公平性。本文旨在提出一种局部公平性准则，并将其融入标准递归树估计算法。

Method: 提出C-LRT算法，结合局部线性分类器和约束逻辑回归的限制，修改标准CART算法。

Result: 在算法公平性文献常用数据集上评估，C-LRT能有效控制和平衡分类准确性与公平性。

Conclusion: C-LRT成功实现了对分类准确性和公平性的平衡控制，为决策树估计算法提供了新的公平性解决方案。

Abstract: Given the high computational complexity of decision tree estimation,
classical methods construct a tree by adding one node at a time in a recursive
way. To facilitate promoting fairness, we propose a fairness criterion local to
the tree nodes. We prove how it is related to the Statistical Parity criterion,
popular in the Algorithmic Fairness literature, and show how to incorporate it
into standard recursive tree estimation algorithms.
  We present a tree estimation algorithm called Constrained Logistic Regression
Tree (C-LRT), which is a modification of the standard CART algorithm using
locally linear classifiers and imposing restrictions as done in Constrained
Logistic Regression.
  Finally, we evaluate the performance of trees estimated with C-LRT on
datasets commonly used in the Algorithmic Fairness literature, using various
classification and fairness metrics. The results confirm that C-LRT
successfully allows to control and balance accuracy and fairness.

</details>

### [52] [Neural operators struggle to learn complex PDEs in pedestrian mobility: Hughes model case study](https://arxiv.org/abs/2504.18267)
*Prajwal Chauhan,Salah Eddine Choutri,Mohamed Ghattassi,Nader Masmoudi,Saif Eddin Jabari*

Main category: cs.LG

TLDR: 论文研究了神经算子在解决Hughes模型（一种人群动力学的双曲守恒律系统）时的局限性，发现其在复杂场景（如多初始间断和动态边界条件）中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究神经算子在处理非线性双曲系统（如Hughes模型）中的表现，尤其是面对复杂解结构（如间断和激波）时的能力。

Method: 评估了三种先进的神经算子（傅里叶神经算子、小波神经算子和多小波神经算子）在不同挑战性场景下的性能，包括间断和高斯初始条件以及多样边界条件。

Result: 神经算子在简单场景中表现良好，但在复杂场景中预测的解过于平滑，导致总变差减少和物理特征丢失。

Conclusion: 当前神经算子架构可能引入意外的正则化效应，限制了其对间断主导的输运动态的捕捉能力，对交通应用中的激波保留提出了挑战。

Abstract: This paper investigates the limitations of neural operators in learning
solutions for a Hughes model, a first-order hyperbolic conservation law system
for crowd dynamics. The model couples a Fokker-Planck equation representing
pedestrian density with a Hamilton-Jacobi-type (eikonal) equation. This Hughes
model belongs to the class of nonlinear hyperbolic systems that often exhibit
complex solution structures, including shocks and discontinuities. In this
study, we assess the performance of three state-of-the-art neural operators
(Fourier Neural Operator, Wavelet Neural Operator, and Multiwavelet Neural
Operator) in various challenging scenarios. Specifically, we consider (1)
discontinuous and Gaussian initial conditions and (2) diverse boundary
conditions, while also examining the impact of different numerical schemes.
  Our results show that these neural operators perform well in easy scenarios
with fewer discontinuities in the initial condition, yet they struggle in
complex scenarios with multiple initial discontinuities and dynamic boundary
conditions, even when trained specifically on such complex samples. The
predicted solutions often appear smoother, resulting in a reduction in total
variation and a loss of important physical features. This smoothing behavior is
similar to issues discussed by Daganzo (1995), where models that introduce
artificial diffusion were shown to miss essential features such as shock waves
in hyperbolic systems. These results suggest that current neural operator
architectures may introduce unintended regularization effects that limit their
ability to capture transport dynamics governed by discontinuities. They also
raise concerns about generalizing these methods to traffic applications where
shock preservation is essential.

</details>

### [53] [Studying Small Language Models with Susceptibilities](https://arxiv.org/abs/2504.18274)
*Garrett Baker,George Wang,Jesse Hoogland,Daniel Murfet*

Main category: cs.LG

TLDR: 论文提出了一种线性响应框架，将神经网络视为贝叶斯统计力学系统，通过扰动数据分布来量化网络组件的响应，并用于解释性分析。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够解释神经网络行为的框架，通过扰动数据分布来理解网络内部组件的功能。

Method: 使用贝叶斯统计力学方法，通过小扰动数据分布（如改变文本类型）来观察网络后验期望的变化，并利用局部SGLD样本估计敏感性。

Result: 敏感性可分解为带符号的每标记贡献，用于属性评分；低秩响应矩阵能分离功能模块（如多词和归纳头）。

Conclusion: 该框架将局部学习系数与线性响应理论联系起来，量化了数据分布变化对局部损失景观几何的影响。

Abstract: We develop a linear response framework for interpretability that treats a
neural network as a Bayesian statistical mechanical system. A small, controlled
perturbation of the data distribution, for example shifting the Pile toward
GitHub or legal text, induces a first-order change in the posterior expectation
of an observable localized on a chosen component of the network. The resulting
susceptibility can be estimated efficiently with local SGLD samples and
factorizes into signed, per-token contributions that serve as attribution
scores. Building a set of perturbations (probes) yields a response matrix whose
low-rank structure separates functional modules such as multigram and induction
heads in a 3M-parameter transformer. Susceptibilities link local learning
coefficients from singular learning theory with linear-response theory, and
quantify how local loss landscape geometry deforms under shifts in the data
distribution.

</details>

### [54] [A comprehensive review of classifier probability calibration metrics](https://arxiv.org/abs/2504.18278)
*Richard Oliver Lane*

Main category: cs.LG

TLDR: 本文综述了AI和ML模型的概率校准指标，提出了82种主要指标，并分类为四种分类器家族和一个目标检测家族，旨在帮助评估模型校准性能。


<details>
  <summary>Details</summary>
Motivation: AI和ML模型的置信度常与实际准确性不符，影响模型在安全或关键业务场景中的可信度，因此需要系统评估校准性能。

Method: 通过综合文献，整理出82种校准指标，并按点基、分箱基、核或曲线基、累积分类器和目标检测家族进行分类。

Result: 提出了82种校准指标的分类框架，并为每种指标提供公式，便于实现和比较。

Conclusion: 校准指标的系统分类和公式化为未来研究提供了实用工具，有助于提升模型的可信度和应用效果。

Abstract: Probabilities or confidence values produced by artificial intelligence (AI)
and machine learning (ML) models often do not reflect their true accuracy, with
some models being under or over confident in their predictions. For example, if
a model is 80% sure of an outcome, is it correct 80% of the time? Probability
calibration metrics measure the discrepancy between confidence and accuracy,
providing an independent assessment of model calibration performance that
complements traditional accuracy metrics. Understanding calibration is
important when the outputs of multiple systems are combined, for assurance in
safety or business-critical contexts, and for building user trust in models.
This paper provides a comprehensive review of probability calibration metrics
for classifier and object detection models, organising them according to a
number of different categorisations to highlight their relationships. We
identify 82 major metrics, which can be grouped into four classifier families
(point-based, bin-based, kernel or curve-based, and cumulative) and an object
detection family. For each metric, we provide equations where available,
facilitating implementation and comparison by future researchers.

</details>

### [55] [Deep Reinforcement Learning Based Navigation with Macro Actions and Topological Maps](https://arxiv.org/abs/2504.18300)
*Simon Hakenes,Tobias Glasmachers*

Main category: cs.LG

TLDR: 论文提出了一种基于拓扑地图和宏动作的方法，用于在视觉复杂环境中实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 解决在视觉复杂且奖励稀疏的大环境中导航的挑战。

Method: 使用基于拓扑地图的物体导向宏动作，结合DQN学习导航策略。

Result: 在3D仿真环境中显著优于随机基线，证明了方法的有效性。

Conclusion: 拓扑结构和宏动作抽象能够从像素数据中实现样本高效学习。

Abstract: This paper addresses the challenge of navigation in large, visually complex
environments with sparse rewards. We propose a method that uses object-oriented
macro actions grounded in a topological map, allowing a simple Deep Q-Network
(DQN) to learn effective navigation policies. The agent builds a map by
detecting objects from RGBD input and selecting discrete macro actions that
correspond to navigating to these objects. This abstraction drastically reduces
the complexity of the underlying reinforcement learning problem and enables
generalization to unseen environments. We evaluate our approach in a
photorealistic 3D simulation and show that it significantly outperforms a
random baseline under both immediate and terminal reward conditions. Our
results demonstrate that topological structure and macro-level abstraction can
enable sample-efficient learning even from pixel data.

</details>

### [56] [SSA-UNet: Advanced Precipitation Nowcasting via Channel Shuffling](https://arxiv.org/abs/2504.18309)
*Marco Turzi,Siamak Mehrkanoon*

Main category: cs.LG

TLDR: 论文提出了一种名为SSA-UNet的新设计，通过引入通道混洗机制优化SmaAt-UNet的性能和降低复杂性，并在荷兰降水数据集和法国云覆盖数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 天气预测对社会经济和环境保护至关重要，深度学习技术可作为数值天气预报（NWP）模型的补充，提供降低复杂性和增强适应性的潜力。

Method: 设计了SSA-UNet，结合通道混洗机制优化SmaAt-UNet架构，并在两个数据集上训练和评估其性能，同时使用Grad-CAM分析模型的预测机制。

Result: SSA-UNet在生成1、6和12个降水图的配置中表现出色，Grad-CAM分析揭示了模型预测的关键输入区域。

Conclusion: SSA-UNet通过通道混洗机制显著提升了性能，为天气预测提供了高效且可解释的深度学习解决方案。

Abstract: Weather forecasting is essential for facilitating diverse socio-economic
activity and environmental conservation initiatives. Deep learning techniques
are increasingly being explored as complementary approaches to Numerical
Weather Prediction (NWP) models, offering potential benefits such as reduced
complexity and enhanced adaptability in specific applications. This work
presents a novel design, Small Shuffled Attention UNet (SSA-UNet), which
enhances SmaAt-UNet's architecture by including a shuffle channeling mechanism
to optimize performance and diminish complexity. To assess its efficacy, this
architecture and its reduced variant are examined and trained on two datasets:
a Dutch precipitation dataset from 2016 to 2019, and a French cloud cover
dataset containing radar images from 2017 to 2018. Three output configurations
of the proposed architecture are evaluated, yielding outputs of 1, 6, and 12
precipitation maps, respectively. To better understand how this model operates
and produces its predictions, a gradient-based approach called Grad-CAM is used
to analyze the outputs generated. The analysis of heatmaps generated by
Grad-CAM facilitated the identification of regions within the input maps that
the model considers most informative for generating its predictions. The
implementation of SSA-UNet can be found on our
Github\footnote{\href{https://github.com/MarcoTurzi/SSA-UNet}{https://github.com/MarcoTurzi/SSA-UNet}}

</details>

### [57] [PHEATPRUNER: Interpretable Data-centric Feature Selection for Multivariate Time Series Classification through Persistent Homology](https://arxiv.org/abs/2504.18329)
*Anh-Duy Pham,Olivier Basole Kashongwe,Martin Atzmueller,Tim Römer*

Main category: cs.LG

TLDR: PHeatPruner结合持久同调和层理论，在保持或提升模型精度的同时，显著减少变量数量，并提供数据结构的解释性。


<details>
  <summary>Details</summary>
Motivation: 解决多变量时间序列分类中性能与可解释性难以平衡的问题。

Method: 利用持久同调修剪变量，层理论提供解释性向量，无需后验概率或监督优化算法。

Result: 在UEA Archive和奶牛乳腺炎数据集上验证，PHeatPruner保持模型精度，简化数据并提供可操作见解。

Conclusion: PHeatPruner在复杂数据简化与可解释性之间架起桥梁，具有广泛应用潜力。

Abstract: Balancing performance and interpretability in multivariate time series
classification is a significant challenge due to data complexity and high
dimensionality. This paper introduces PHeatPruner, a method integrating
persistent homology and sheaf theory to address these challenges. Persistent
homology facilitates the pruning of up to 45% of the applied variables while
maintaining or enhancing the accuracy of models such as Random Forest,
CatBoost, XGBoost, and LightGBM, all without depending on posterior
probabilities or supervised optimization algorithms. Concurrently, sheaf theory
contributes explanatory vectors that provide deeper insights into the data's
structural nuances. The approach was validated using the UEA Archive and a
mastitis detection dataset for dairy cows. The results demonstrate that
PHeatPruner effectively preserves model accuracy. Furthermore, our results
highlight PHeatPruner's key features, i.e. simplifying complex data and
offering actionable insights without increasing processing time or complexity.
This method bridges the gap between complexity reduction and interpretability,
suggesting promising applications in various fields.

</details>

### [58] [Testing Individual Fairness in Graph Neural Networks](https://arxiv.org/abs/2504.18353)
*Roya Nasiri*

Main category: cs.LG

TLDR: 该论文探讨了图神经网络（GNNs）中的个体公平性问题，旨在开发一个测试框架来评估和确保GNNs的公平性。


<details>
  <summary>Details</summary>
Motivation: AI模型中的偏见可能导致基于性别、种族等敏感属性的歧视性决策，而GNNs因其图结构特性使得偏见的传播和检测更为复杂，目前缺乏相关研究。

Method: 系统回顾个体公平性文献，构建分类体系，并开发适用于GNNs的公平性测试与缓解框架，通过工业案例验证。

Result: 预计开发一个能够测试和确保GNNs个体公平性的框架。

Conclusion: 该研究填补了GNNs个体公平性研究的空白，为实际应用中的公平性提供了解决方案。

Abstract: The biases in artificial intelligence (AI) models can lead to automated
decision-making processes that discriminate against groups and/or individuals
based on sensitive properties such as gender and race. While there are many
studies on diagnosing and mitigating biases in various AI models, there is
little research on individual fairness in Graph Neural Networks (GNNs). Unlike
traditional models, which treat data features independently and overlook their
inter-relationships, GNNs are designed to capture graph-based structure where
nodes are interconnected. This relational approach enables GNNs to model
complex dependencies, but it also means that biases can propagate through these
connections, complicating the detection and mitigation of individual fairness
violations. This PhD project aims to develop a testing framework to assess and
ensure individual fairness in GNNs. It first systematically reviews the
literature on individual fairness, categorizing existing approaches to define,
measure, test, and mitigate model biases, creating a taxonomy of individual
fairness. Next, the project will develop a framework for testing and ensuring
fairness in GNNs by adapting and extending current fairness testing and
mitigation techniques. The framework will be evaluated through industrial case
studies, focusing on graph-based large language models.

</details>

### [59] [Explainable AI for UAV Mobility Management: A Deep Q-Network Approach for Handover Minimization](https://arxiv.org/abs/2504.18371)
*Irshad A. Meer,Bruno Hörmann,Mustafa Ozger,Fabien Geyer,Alberto Viseras,Dominic Schupke,Cicek Cavdar*

Main category: cs.LG

TLDR: 论文提出了一种基于可解释AI（XAI）的框架，结合SHAP方法，提升DQN在无人机（UAV）移动管理中的决策可解释性。


<details>
  <summary>Details</summary>
Motivation: 无人机在蜂窝网络中频繁切换导致移动管理困难，现有基于强化学习的方法缺乏可解释性。

Method: 引入XAI框架，利用SHAP量化关键特征（如RSRP、RSRQ等）对切换决策的影响。

Result: 仿真结果表明，该方法能直观解释策略决策，提升AI模型的可靠性和可理解性。

Conclusion: 该框架有效弥合了AI模型与人类决策者之间的差距，为无人机移动管理提供了更可靠的解决方案。

Abstract: The integration of unmanned aerial vehicles (UAVs) into cellular networks
presents significant mobility management challenges, primarily due to frequent
handovers caused by probabilistic line-of-sight conditions with multiple ground
base stations (BSs). To tackle these challenges, reinforcement learning
(RL)-based methods, particularly deep Q-networks (DQN), have been employed to
optimize handover decisions dynamically. However, a major drawback of these
learning-based approaches is their black-box nature, which limits
interpretability in the decision-making process. This paper introduces an
explainable AI (XAI) framework that incorporates Shapley Additive Explanations
(SHAP) to provide deeper insights into how various state parameters influence
handover decisions in a DQN-based mobility management system. By quantifying
the impact of key features such as reference signal received power (RSRP),
reference signal received quality (RSRQ), buffer status, and UAV position, our
approach enhances the interpretability and reliability of RL-based handover
solutions. To validate and compare our framework, we utilize real-world network
performance data collected from UAV flight trials. Simulation results show that
our method provides intuitive explanations for policy decisions, effectively
bridging the gap between AI-driven models and human decision-makers.

</details>

### [60] [Model Evaluation in the Dark: Robust Classifier Metrics with Missing Labels](https://arxiv.org/abs/2504.18385)
*Danial Dervovic,Michael Cashmore*

Main category: cs.LG

TLDR: 提出一种多重插补技术，用于在标签缺失时评估分类器性能，避免因忽略缺失样本引入偏差。


<details>
  <summary>Details</summary>
Motivation: 监督学习中缺失数据问题已有研究，但模型评估时标签缺失的问题被忽视，忽略缺失样本可能引入偏差，尤其是在MNAR情况下。

Method: 采用多重插补技术，生成缺失标签的预测分布，用于计算精度、召回率和ROC-AUC等指标。

Result: 实证表明预测分布的位置和形状在MNAR情况下仍准确，且分布近似高斯，提供了有限样本收敛界和鲁棒性证明。

Conclusion: 该方法在标签缺失时提供可靠的点估计和预测分布，适用于MNAR场景。

Abstract: Missing data in supervised learning is well-studied, but the specific issue
of missing labels during model evaluation has been overlooked. Ignoring samples
with missing values, a common solution, can introduce bias, especially when
data is Missing Not At Random (MNAR). We propose a multiple imputation
technique for evaluating classifiers using metrics such as precision, recall,
and ROC-AUC. This method not only offers point estimates but also a predictive
distribution for these quantities when labels are missing. We empirically show
that the predictive distribution's location and shape are generally correct,
even in the MNAR regime. Moreover, we establish that this distribution is
approximately Gaussian and provide finite-sample convergence bounds.
Additionally, a robustness proof is presented, confirming the validity of the
approximation under a realistic error model.

</details>

### [61] [Machine Learning and Statistical Insights into Hospital Stay Durations: The Italian EHR Case](https://arxiv.org/abs/2504.18393)
*Marina Andric,Mauro Dragoni*

Main category: cs.LG

TLDR: 研究分析了意大利皮埃蒙特地区60多家医疗机构2020-2023年的住院记录，探索影响住院时长的因素，发现年龄、合并症、入院类型和月份等因素与住院时长显著相关。使用CatBoost和随机森林模型预测住院时长，CatBoost表现最佳（R2=0.49）。


<details>
  <summary>Details</summary>
Motivation: 住院时长是评估医疗质量和优化资源管理的关键指标，研究旨在识别意大利医疗背景下影响住院时长的因素。

Method: 利用住院记录数据集，分析患者特征、合并症、入院细节和医院特定因素，并使用CatBoost和随机森林模型进行预测。

Result: 年龄组、合并症评分、入院类型和入院月份与住院时长显著相关，CatBoost模型的R2得分为0.49，表现最佳。

Conclusion: 研究证实了多种因素对住院时长的影响，CatBoost模型在预测住院时长方面表现出色，为医疗资源管理提供了实用工具。

Abstract: Length of hospital stay is a critical metric for assessing healthcare quality
and optimizing hospital resource management. This study aims to identify
factors influencing LoS within the Italian healthcare context, using a dataset
of hospitalization records from over 60 healthcare facilities in the Piedmont
region, spanning from 2020 to 2023. We explored a variety of features,
including patient characteristics, comorbidities, admission details, and
hospital-specific factors. Significant correlations were found between LoS and
features such as age group, comorbidity score, admission type, and the month of
admission. Machine learning models, specifically CatBoost and Random Forest,
were used to predict LoS. The highest R2 score, 0.49, was achieved with
CatBoost, demonstrating good predictive performance.

</details>

### [62] [Three Types of Calibration with Properties and their Semantic and Formal Relationships](https://arxiv.org/abs/2504.18395)
*Rabanus Derr,Jessie Finocchiaro,Robert C. Williamson*

Main category: cs.LG

TLDR: N/A


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Fueled by discussions around "trustworthiness" and algorithmic fairness,
calibration of predictive systems has regained scholars attention. The vanilla
definition and understanding of calibration is, simply put, on all days on
which the rain probability has been predicted to be p, the actual frequency of
rain days was p. However, the increased attention has led to an immense variety
of new notions of "calibration." Some of the notions are incomparable, serve
different purposes, or imply each other. In this work, we provide two accounts
which motivate calibration: self-realization of forecasted properties and
precise estimation of incurred losses of the decision makers relying on
forecasts. We substantiate the former via the reflection principle and the
latter by actuarial fairness. For both accounts we formulate prototypical
definitions via properties $\Gamma$ of outcome distributions, e.g., the mean or
median. The prototypical definition for self-realization, which we call
$\Gamma$-calibration, is equivalent to a certain type of swap regret under
certain conditions. These implications are strongly connected to the
omniprediction learning paradigm. The prototypical definition for precise loss
estimation is a modification of decision calibration adopted from Zhao et al.
[73]. For binary outcome sets both prototypical definitions coincide under
appropriate choices of reference properties. For higher-dimensional outcome
sets, both prototypical definitions can be subsumed by a natural extension of
the binary definition, called distribution calibration with respect to a
property. We conclude by commenting on the role of groupings in both accounts
of calibration often used to obtain multicalibration. In sum, this work
provides a semantic map of calibration in order to navigate a fragmented
terrain of notions and definitions.

</details>

### [63] [Online learning to accelerate nonlinear PDE solvers: applied to multiphase porous media flow](https://arxiv.org/abs/2504.18414)
*Vinicius L S Silva,Pablo Salinas,Claire E Heaney,Matthew Jackson,Christopher C Pain*

Main category: cs.LG

TLDR: 提出一种基于在线/自适应学习的非线性求解器加速方法，用于多孔介质中的多相流问题，通过动态调整松弛因子和在线学习减少非线性迭代次数，计算时间减少高达85%。


<details>
  <summary>Details</summary>
Motivation: 解决多孔介质中多相流问题的非线性求解效率问题，通过机器学习和动态控制优化求解过程。

Method: 基于四个支柱：(i) 无量纲数作为机器学习输入，(ii) 二维简化模型离线训练，(iii) 动态控制非线性求解器参数，(iv) 在线学习实时改进模型。

Result: 在复杂三维模型中减少非线性迭代次数，计算时间减少高达85%。

Conclusion: 该方法显著提高了非线性求解效率，适用于复杂实际问题，并成功集成到开源模拟器中。

Abstract: We propose a novel type of nonlinear solver acceleration for systems of
nonlinear partial differential equations (PDEs) that is based on
online/adaptive learning. It is applied in the context of multiphase flow in
porous media. The proposed method rely on four pillars: (i) dimensionless
numbers as input parameters for the machine learning model, (ii) simplified
numerical model (two-dimensional) for the offline training, (iii) dynamic
control of a nonlinear solver tuning parameter (numerical relaxation), (iv) and
online learning for real-time improvement of the machine learning model. This
strategy decreases the number of nonlinear iterations by dynamically modifying
a single global parameter, the relaxation factor, and by adaptively learning
the attributes of each numerical model on-the-run. Furthermore, this work
performs a sensitivity study in the dimensionless parameters (machine learning
features), assess the efficacy of various machine learning models, demonstrate
a decrease in nonlinear iterations using our method in more intricate,
realistic three-dimensional models, and fully couple a machine learning model
into an open-source multiphase flow simulator achieving up to 85\% reduction in
computational time.

</details>

### [64] [An Axiomatic Assessment of Entropy- and Variance-based Uncertainty Quantification in Regression](https://arxiv.org/abs/2504.18433)
*Christopher Bülte,Yusuf Sale,Timo Löhr,Paul Hofman,Gitta Kutyniok,Eyke Hüllermeier*

Main category: cs.LG

TLDR: 本文提出了回归任务中不确定性量化的公理化框架，分析了熵和方差方法的局限性，并提供了理论和实践指导。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化研究多集中于分类任务，回归任务中缺乏理论支持和评估。

Method: 引入公理化框架，利用预测指数族泛化不确定性表示和度量方法，分析熵和方差方法的局限性。

Result: 为回归任务中的不确定性量化提供了理论基础和实践指南。

Conclusion: 研究填补了回归任务中不确定性量化的理论空白，为可靠评估提供了支持。

Abstract: Uncertainty quantification (UQ) is crucial in machine learning, yet most
(axiomatic) studies of uncertainty measures focus on classification, leaving a
gap in regression settings with limited formal justification and evaluations.
In this work, we introduce a set of axioms to rigorously assess measures of
aleatoric, epistemic, and total uncertainty in supervised regression. By
utilizing a predictive exponential family, we can generalize commonly used
approaches for uncertainty representation and corresponding uncertainty
measures. More specifically, we analyze the widely used entropy- and
variance-based measures regarding limitations and challenges. Our findings
provide a principled foundation for UQ in regression, offering theoretical
insights and practical guidelines for reliable uncertainty assessment.

</details>

### [65] [Enhancing Pre-Trained Model-Based Class-Incremental Learning through Neural Collapse](https://arxiv.org/abs/2504.18437)
*Kun He,Zijian Song,Shuoxi Zhang,John E. Hopcroft*

Main category: cs.LG

TLDR: 论文提出了一种基于神经崩溃（NC）的预训练模型类增量学习方法（NCPTM-CIL），通过动态调整特征空间以符合NC结构，显著提升了持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 理解预训练模型在类增量学习中特征演化和分布的动态行为是一个未解决的挑战。

Method: 利用神经崩溃现象，提出NCPTM-CIL方法，动态调整特征空间以符合NC几何结构。

Result: 在四个基准数据集上，NCPTM-CIL表现优于现有方法，尤其在ViT-B/16-IN1K初始化下，性能提升显著。

Conclusion: 通过NC几何结构优化特征空间，NCPTM-CIL有效提升了类增量学习的性能。

Abstract: Class-Incremental Learning (CIL) is a critical capability for real-world
applications, enabling learning systems to adapt to new tasks while retaining
knowledge from previous ones. Recent advancements in pre-trained models (PTMs)
have significantly advanced the field of CIL, demonstrating superior
performance over traditional methods. However, understanding how features
evolve and are distributed across incremental tasks remains an open challenge.
In this paper, we propose a novel approach to modeling feature evolution in
PTM-based CIL through the lens of neural collapse (NC), a striking phenomenon
observed in the final phase of training, which leads to a well-separated,
equiangular feature space. We explore the connection between NC and CIL
effectiveness, showing that aligning feature distributions with the NC geometry
enhances the ability to capture the dynamic behavior of continual learning.
Based on this insight, we introduce Neural Collapse-inspired Pre-Trained
Model-based CIL (NCPTM-CIL), a method that dynamically adjusts the feature
space to conform to the elegant NC structure, thereby enhancing the continual
learning process. Extensive experiments demonstrate that NCPTM-CIL outperforms
state-of-the-art methods across four benchmark datasets. Notably, when
initialized with ViT-B/16-IN1K, NCPTM-CIL surpasses the runner-up method by
6.73% on VTAB, 1.25% on CIFAR-100, and 2.5% on OmniBenchmark.

</details>

### [66] [Enhancing Strawberry Yield Forecasting with Backcasted IoT Sensor Data and Machine Learning](https://arxiv.org/abs/2504.18451)
*Tewodros Alemu Ayall,Andy Li,Matthew Beddows,Milan Markovic,Georgios Leontidis*

Main category: cs.LG

TLDR: 研究提出了一种基于AI的回溯方法，利用历史天气数据和现有传感器观测生成合成数据，以弥补草莓种植中缺失的物联网观测数据，从而提高产量预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于全球人口快速增长，数字化农业对可持续粮食生产和资源管理决策至关重要。然而，AI模型需要大量数据，而实际农场环境中物联网观测数据的收集面临挑战。

Method: 在草莓种植温室中部署物联网传感器，收集两季环境数据，并结合四季产量记录。提出基于AI的回溯方法，利用历史天气数据和现有观测生成合成数据。

Result: 结合合成数据的AI产量预测模型表现优于仅基于历史产量、天气记录和真实传感器数据的模型。

Conclusion: 合成数据的引入显著提高了产量预测的准确性，为农业数字化提供了实用解决方案。

Abstract: Due to rapid population growth globally, digitally-enabled agricultural
sectors are crucial for sustainable food production and making informed
decisions about resource management for farmers and various stakeholders. The
deployment of Internet of Things (IoT) technologies that collect real-time
observations of various environmental (e.g., temperature, humidity, etc.) and
operational factors (e.g., irrigation) influencing production is often seen as
a critical step to enable additional novel downstream tasks, such as AI-based
yield forecasting. However, since AI models require large amounts of data, this
creates practical challenges in a real-world dynamic farm setting where IoT
observations would need to be collected over a number of seasons. In this
study, we deployed IoT sensors in strawberry production polytunnels for two
growing seasons to collect environmental data, including water usage, external
and internal temperature, external and internal humidity, soil moisture, soil
temperature, and photosynthetically active radiation. The sensor observations
were combined with manually provided yield records spanning a period of four
seasons. To bridge the gap of missing IoT observations for two additional
seasons, we propose an AI-based backcasting approach to generate synthetic
sensor observations using historical weather data from a nearby weather station
and the existing polytunnel observations. We built an AI-based yield
forecasting model to evaluate our approach using the combination of real and
synthetic observations. Our results demonstrated that incorporating synthetic
data improved yield forecasting accuracy, with models incorporating synthetic
data outperforming those trained only on historical yield, weather records, and
real sensor data.

</details>

### [67] [Pseudo-Asynchronous Local SGD: Robust and Efficient Data-Parallel Training](https://arxiv.org/abs/2504.18454)
*Hiroki Naganuma,Xinzhi Zhang,Man-Chung Yue,Ioannis Mitliagkas,Philipp A. Witte,Russell J. Hewett,Yin Tat Lee*

Main category: cs.LG

TLDR: 论文提出了一种名为PALSGD的方法，通过伪异步机制减少数据并行训练中的通信频率，同时保持模型一致性，显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型的规模和数据集的增大，训练需要大量计算资源，数据并行训练中的频繁全局通信成为瓶颈。

Method: PALSGD是Local SGD和DiLoCo的扩展，引入伪同步机制以减少通信频率，同时通过理论分析确保收敛性。

Result: 实验表明，PALSGD在图像分类和语言建模任务中比DDP和DiLoCo更快，性能提升显著。

Conclusion: PALSGD通过减少通信频率提高了训练效率，同时保持模型性能，为大规模分布式深度学习提供了有效解决方案。

Abstract: Following AI scaling trends, frontier models continue to grow in size and
continue to be trained on larger datasets. Training these models requires huge
investments in exascale computational resources, which has in turn driven
development of distributed deep learning methods. Data parallelism is an
essential approach to speed up training, but it requires frequent global
communication between workers, which can bottleneck training at the largest
scales. In this work, we propose a method called Pseudo-Asynchronous Local SGD
(PALSGD) to improve the efficiency of data-parallel training. PALSGD is an
extension of Local SGD (Stich, 2018) and DiLoCo (Douillard et al., 2023),
designed to further reduce communication frequency by introducing a
pseudo-synchronization mechanism. PALSGD allows the use of longer
synchronization intervals compared to standard Local SGD. Despite the reduced
communication frequency, the pseudo-synchronization approach ensures that model
consistency is maintained, leading to performance results comparable to those
achieved with more frequent synchronization. Furthermore, we provide a
theoretical analysis of PALSGD, establishing its convergence and deriving its
convergence rate. This analysis offers insights into the algorithm's behavior
and performance guarantees. We evaluated PALSGD on image classification and
language modeling tasks. Our results show that PALSGD achieves better
performance in less time compared to existing methods like Distributed Data
Parallel (DDP), and DiLoCo. Notably, PALSGD trains 18.4% faster than DDP on
ImageNet-1K with ResNet-50, 24.4% faster than DDP on TinyStories with
GPT-Neo125M, and 21.1% faster than DDP on TinyStories with GPT-Neo-8M.

</details>

### [68] [Action-Minimization Meets Generative Modeling: Efficient Transition Path Sampling with the Onsager-Machlup Functional](https://arxiv.org/abs/2504.18506)
*Sanjeev Raja,Martin Šípka,Michael Psenka,Tobias Kreiman,Michal Pavelka,Aditi S. Krishnapriyan*

Main category: cs.LG

TLDR: 本文提出了一种利用预训练生成模型（如去噪扩散和流匹配）进行零样本过渡路径采样（TPS）的方法，避免了传统方法中昂贵且任务特定的训练过程。


<details>
  <summary>Details</summary>
Motivation: 由于真实原子系统的复杂性，传统的过渡路径采样方法面临挑战，且现有机器学习方法依赖昂贵且任务特定的训练，无法充分利用原子机器学习的最新进展。

Method: 将候选路径解释为由预训练生成模型学习的得分函数诱导的随机动力学轨迹，并通过最小化Onsager-Machlup（OM）作用泛函来寻找高概率路径。

Result: 在多种分子系统中展示了方法的有效性，生成了多样且物理真实的过渡路径，并能推广到预训练模型原始训练数据之外。

Conclusion: 该方法可轻松集成到新的生成模型中，具有实际应用价值，尤其是在数据可用性增加的背景下。

Abstract: Transition path sampling (TPS), which involves finding probable paths
connecting two points on an energy landscape, remains a challenge due to the
complexity of real-world atomistic systems. Current machine learning approaches
use expensive, task-specific, and data-free training procedures, limiting their
ability to benefit from recent advances in atomistic machine learning, such as
high-quality datasets and large-scale pre-trained models. In this work, we
address TPS by interpreting candidate paths as trajectories sampled from
stochastic dynamics induced by the learned score function of pre-trained
generative models, specifically denoising diffusion and flow matching. Under
these dynamics, finding high-likelihood transition paths becomes equivalent to
minimizing the Onsager-Machlup (OM) action functional. This enables us to
repurpose pre-trained generative models for TPS in a zero-shot manner, in
contrast with bespoke, task-specific TPS models trained in previous work. We
demonstrate our approach on varied molecular systems, obtaining diverse,
physically realistic transition pathways and generalizing beyond the
pre-trained model's original training dataset. Our method can be easily
incorporated into new generative models, making it practically relevant as
models continue to scale and improve with increased data availability.

</details>

### [69] [Intelligent Attacks and Defense Methods in Federated Learning-enabled Energy-Efficient Wireless Networks](https://arxiv.org/abs/2504.18519)
*Han Zhang,Hao Zhou,Medhat Elsayed,Majid Bavand,Raimundas Gaigalas,Yigit Ozcan,Melike Erol-Kantarci*

Main category: cs.LG

TLDR: 论文研究了联邦学习在无线网络中的安全问题，提出了两种攻击模型和两种防御方法，以提升网络能效和安全性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在无线网络中具有分布式实现的优势，但也面临恶意攻击的风险，尤其是在动态无线环境和非独立同分布数据下，攻击难以检测。因此，评估攻击影响并开发防御技术至关重要。

Method: 提出了基于生成对抗网络和正则化的两种攻击模型，并设计了基于自编码器和知识蒸馏的防御方法。

Result: 通过实验验证了攻击模型的有效性，并展示了防御方法在保护联邦学习系统中的作用。

Conclusion: 论文为联邦学习在无线网络中的安全应用提供了理论和实践支持，提出的防御方法能有效应对恶意攻击。

Abstract: Federated learning (FL) is a promising technique for learning-based functions
in wireless networks, thanks to its distributed implementation capability. On
the other hand, distributed learning may increase the risk of exposure to
malicious attacks where attacks on a local model may spread to other models by
parameter exchange. Meanwhile, such attacks can be hard to detect due to the
dynamic wireless environment, especially considering local models can be
heterogeneous with non-independent and identically distributed (non-IID) data.
Therefore, it is critical to evaluate the effect of malicious attacks and
develop advanced defense techniques for FL-enabled wireless networks. In this
work, we introduce a federated deep reinforcement learning-based cell sleep
control scenario that enhances the energy efficiency of the network. We propose
multiple intelligent attacks targeting the learning-based approach and we
propose defense methods to mitigate such attacks. In particular, we have
designed two attack models, generative adversarial network (GAN)-enhanced model
poisoning attack and regularization-based model poisoning attack. As a
counteraction, we have proposed two defense schemes, autoencoder-based defense,
and knowledge distillation (KD)-enabled defense. The autoencoder-based defense
method leverages an autoencoder to identify the malicious participants and only
aggregate the parameters of benign local models during the global aggregation,
while KD-based defense protects the model from attacks by controlling the
knowledge transferred between the global model and local models.

</details>

### [70] [Generalization Capability for Imitation Learning](https://arxiv.org/abs/2504.18538)
*Yixiao Wang*

Main category: cs.LG

TLDR: 该论文提出了一种基于信息论和数据分布特性的模仿学习泛化能力统一视角，揭示了泛化差距的上界与条件信息瓶颈及模型参数与训练数据集互信息的关系，并探讨了优化策略。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在有限数据集上训练的模型泛化能力不足，需要理论指导以提升其泛化性能。

Method: 通过信息论分析泛化差距的上界，并结合条件熵和梯度下降优化策略，提出改进方法。

Result: 高条件熵可降低泛化差距上界，并缩短SGD逃离尖锐局部极小值的时间，有助于找到全局最优。

Conclusion: 提升输入数据多样性和输出标签变异性是改善模仿学习泛化能力的关键。

Abstract: Imitation learning holds the promise of equipping robots with versatile
skills by learning from expert demonstrations. However, policies trained on
finite datasets often struggle to generalize beyond the training distribution.
In this work, we present a unified perspective on the generalization capability
of imitation learning, grounded in both information theorey and data
distribution property. We first show that the generalization gap can be upper
bounded by (i) the conditional information bottleneck on intermediate
representations and (ii) the mutual information between the model parameters
and the training dataset. This characterization provides theoretical guidance
for designing effective training strategies in imitation learning, particularly
in determining whether to freeze, fine-tune, or train large pretrained encoders
(e.g., vision-language models or vision foundation models) from scratch to
achieve better generalization. Furthermore, we demonstrate that high
conditional entropy from input to output induces a flatter likelihood
landscape, thereby reducing the upper bound on the generalization gap. In
addition, it shortens the stochastic gradient descent (SGD) escape time from
sharp local minima, which may increase the likelihood of reaching global optima
under fixed optimization budgets. These insights explain why imitation learning
often exhibits limited generalization and underscore the importance of not only
scaling the diversity of input data but also enriching the variability of
output labels conditioned on the same input.

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [71] [Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English](https://arxiv.org/abs/2504.17974)
*Sabur Butt,Fazlourrahman Balouchzahi,Ahmad Imam Amjad,Maaz Amjad,Hector G. Ceballos,Salud Maria Jimenez-Zafra*

Main category: cs.CL

TLDR: 论文介绍了一个多语言、细粒度的希望语音数据集PolyHope V2，包含超过30,000条标注的英文和西班牙语推文，区分四种希望子类型，并比较了预训练模型与大型语言模型的表现。


<details>
  <summary>Details</summary>
Motivation: 希望是一种复杂且未被充分探索的情感状态，对教育、心理健康和社交互动有重要影响，但其在自然语言处理中的检测存在挑战。

Method: 研究引入了PolyHope V2数据集，标注了四种希望子类型，并比较了预训练模型与大型语言模型（如GPT-4和Llama 3）在零样本和少样本下的表现。

Result: 研究发现微调的预训练模型在区分细微希望类别和讽刺方面优于基于提示的大型语言模型。

Conclusion: 数据集和结果为未来需要跨语言语义和上下文敏感性的情感识别任务提供了坚实基础。

Abstract: Hope is a complex and underexplored emotional state that plays a significant
role in education, mental health, and social interaction. Unlike basic
emotions, hope manifests in nuanced forms ranging from grounded optimism to
exaggerated wishfulness or sarcasm, making it difficult for Natural Language
Processing systems to detect accurately. This study introduces PolyHope V2, a
multilingual, fine-grained hope speech dataset comprising over 30,000 annotated
tweets in English and Spanish. This resource distinguishes between four hope
subtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances
existing datasets by explicitly labeling sarcastic instances. We benchmark
multiple pretrained transformer models and compare them with large language
models (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes.
Our findings show that fine-tuned transformers outperform prompt-based LLMs,
especially in distinguishing nuanced hope categories and sarcasm. Through
qualitative analysis and confusion matrices, we highlight systematic challenges
in separating closely related hope subtypes. The dataset and results provide a
robust foundation for future emotion recognition tasks that demand greater
semantic and contextual sensitivity across languages.

</details>

### [72] [Improving LLM Personas via Rationalization with Psychological Scaffolds](https://arxiv.org/abs/2504.17993)
*Brihi Joshi,Xiang Ren,Swabha Swayamdipta,Rik Koncel-Kedziorski,Tim Paek*

Main category: cs.CL

TLDR: PB&J框架通过结合心理学理论生成的用户判断理由，提升了语言模型构建用户角色的能力，显著优于仅基于人口统计或历史判断的方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖用户的人口统计或历史判断，未能捕捉用户判断背后的深层原因。

Method: 引入PB&J框架，利用心理学理论（如大五人格和原始世界信念）生成用户判断的理由。

Result: 在公共意见和电影偏好预测任务中，PB&J增强的语言模型角色表现优于传统方法，且与人类编写的理由表现相当。

Conclusion: PB&J框架通过心理学理论生成的理由，显著提升了语言模型用户角色的准确性和解释性。

Abstract: Language models prompted with a user description or persona can predict a
user's preferences and opinions, but existing approaches to building personas
-- based solely on a user's demographic attributes and/or prior judgments --
fail to capture the underlying reasoning behind said user judgments. We
introduce PB&J (Psychology of Behavior and Judgments), a framework that
improves LLM personas by incorporating rationales of why a user might make
specific judgments. These rationales are LLM-generated, and aim to reason about
a user's behavior on the basis of their experiences, personality traits or
beliefs. This is done using psychological scaffolds -- structured frameworks
grounded in theories such as the Big 5 Personality Traits and Primal World
Beliefs -- that help provide structure to the generated rationales. Experiments
on public opinion and movie preference prediction tasks demonstrate that LLM
personas augmented with PB&J rationales consistently outperform methods using
only a user's demographics and/or judgments. Additionally, LLM personas
constructed using scaffolds describing user beliefs perform competitively with
those using human-written rationales.

</details>

### [73] [Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation](https://arxiv.org/abs/2504.18012)
*Zhuang Yu,Shiliang Sun,Jing Zhao,Tengfei Song,Hao Yang*

Main category: cs.CL

TLDR: 研究了预训练编码器和解码器在多模态机器翻译中的作用，发现预训练解码器能显著提升翻译质量，而编码器的效果取决于视觉-文本对齐质量。


<details>
  <summary>Details</summary>
Motivation: 探索预训练语言和视觉模型在多模态机器翻译中的效果和角色，填补研究空白。

Method: 在统一的多模态翻译框架下，分析不同训练策略（从零训练到使用预训练和部分冻结组件）对翻译性能的影响。

Result: 预训练在多模态中起关键但不对称作用：解码器提升流畅性和准确性，编码器效果因视觉-文本对齐质量而异。

Conclusion: 预训练在多模态翻译中具有重要作用，为未来架构设计提供了指导。

Abstract: Multimodal Machine Translation (MMT) aims to improve translation quality by
leveraging auxiliary modalities such as images alongside textual input. While
recent advances in large-scale pre-trained language and vision models have
significantly benefited unimodal natural language processing tasks, their
effectiveness and role in MMT remain underexplored. In this work, we conduct a
systematic study on the impact of pre-trained encoders and decoders in
multimodal translation models. Specifically, we analyze how different training
strategies, from training from scratch to using pre-trained and partially
frozen components, affect translation performance under a unified MMT
framework. Experiments are carried out on the Multi30K and CoMMuTE dataset
across English-German and English-French translation tasks. Our results reveal
that pre-training plays a crucial yet asymmetrical role in multimodal settings:
pre-trained decoders consistently yield more fluent and accurate outputs, while
pre-trained encoders show varied effects depending on the quality of
visual-text alignment. Furthermore, we provide insights into the interplay
between modality fusion and pre-trained components, offering guidance for
future architecture design in multimodal translation systems.

</details>

### [74] [RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models](https://arxiv.org/abs/2504.18041)
*Bang An,Shiyue Zhang,Mark Dredze*

Main category: cs.CL

TLDR: 研究发现，RAG框架可能降低LLMs的安全性，且现有安全测试方法对RAG效果较差，需针对性研究。


<details>
  <summary>Details</summary>
Motivation: 探讨RAG框架如何影响LLMs的安全性，填补相关研究空白。

Method: 对11种LLMs进行RAG与非RAG框架的对比分析，评估安全性变化及现有测试方法效果。

Result: RAG可能降低模型安全性，安全模型与文档组合仍可能生成不安全内容，现有测试方法对RAG效果不佳。

Conclusion: 需开发针对RAG LLMs的安全研究和测试方法。

Abstract: Efforts to ensure the safety of large language models (LLMs) include safety
fine-tuning, evaluation, and red teaming. However, despite the widespread use
of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses
on standard LLMs, which means we know little about how RAG use cases change a
model's safety profile. We conduct a detailed comparative analysis of RAG and
non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe
and change their safety profile. We explore the causes of this change and find
that even combinations of safe models with safe documents can cause unsafe
generations. In addition, we evaluate some existing red teaming methods for RAG
settings and show that they are less effective than when used for non-RAG
settings. Our work highlights the need for safety research and red-teaming
methods specifically tailored for RAG LLMs.

</details>

### [75] [DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models](https://arxiv.org/abs/2504.18053)
*Jianyu Liu,Hangyu Guo,Ranjie Duan,Xingyuan Bu,Yancheng He,Shilong Li,Hui Huang,Jiaheng Liu,Yucheng Wang,Chenchen Jing,Xingwei Qu,Xiao Zhang,Yingshui Tan,Yanan Wu,Jihao Gu,Yangguang Li,Jianke Zhu*

Main category: cs.CL

TLDR: 论文提出DREAM方法，通过多模态风险解耦和强化学习提升MLLMs的安全性，实验显示效果显著。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）因结合视觉和文本数据带来新的安全挑战，需系统性解耦风险以提升安全性。

Method: 提出DREAM方法，结合监督微调和迭代强化学习（RLAIF），系统性解耦多模态风险。

Result: DREAM显著提升MLLMs的安全性（SIUO分数提高16.17%），且不影响正常任务性能。

Conclusion: DREAM通过风险解耦和强化学习有效增强MLLMs的安全性，为多模态模型安全提供新思路。

Abstract: Multimodal Large Language Models (MLLMs) pose unique safety challenges due to
their integration of visual and textual data, thereby introducing new
dimensions of potential attacks and complex risk combinations. In this paper,
we begin with a detailed analysis aimed at disentangling risks through
step-by-step reasoning within multimodal inputs. We find that systematic
multimodal risk disentanglement substantially enhances the risk awareness of
MLLMs. Via leveraging the strong discriminative abilities of multimodal risk
disentanglement, we further introduce \textbf{DREAM}
(\textit{\textbf{D}isentangling \textbf{R}isks to \textbf{E}nhance Safety
\textbf{A}lignment in \textbf{M}LLMs}), a novel approach that enhances safety
alignment in MLLMs through supervised fine-tuning and iterative Reinforcement
Learning from AI Feedback (RLAIF). Experimental results show that DREAM
significantly boosts safety during both inference and training phases without
compromising performance on normal tasks (namely oversafety), achieving a
16.17\% improvement in the SIUO safe\&effective score compared to GPT-4V. The
data and code are available at https://github.com/Kizna1ver/DREAM.

</details>

### [76] [Exploring Personality-Aware Interactions in Salesperson Dialogue Agents](https://arxiv.org/abs/2504.18058)
*Sijia Cheng,Wen-Yu Chang,Yun-Nung Chen*

Main category: cs.CL

TLDR: 研究探讨了基于MBTI用户人格对销售对话代理交互质量的影响，发现交互动态、任务完成率和对话自然性存在显著模式，并开发了跨领域用户模拟器。


<details>
  <summary>Details</summary>
Motivation: 理解不同人格用户如何与销售对话代理交互，以提升系统的适应性和个性化能力。

Method: 通过大规模测试和分析，评估预训练代理在MBTI定义的用户类型中的表现。

Result: 揭示了交互动态、任务完成率和对话自然性的显著模式，开发了跨领域用户模拟器。

Conclusion: 研究为构建更自适应的销售对话系统提供了实用见解，并推动了跨领域个性化对话系统的发展。

Abstract: The integration of dialogue agents into the sales domain requires a deep
understanding of how these systems interact with users possessing diverse
personas. This study explores the influence of user personas, defined using the
Myers-Briggs Type Indicator (MBTI), on the interaction quality and performance
of sales-oriented dialogue agents. Through large-scale testing and analysis, we
assess the pre-trained agent's effectiveness, adaptability, and personalization
capabilities across a wide range of MBTI-defined user types. Our findings
reveal significant patterns in interaction dynamics, task completion rates, and
dialogue naturalness, underscoring the future potential for dialogue agents to
refine their strategies to better align with varying personality traits. This
work not only provides actionable insights for building more adaptive and
user-centric conversational systems in the sales domain but also contributes
broadly to the field by releasing persona-defined user simulators. These
simulators, unconstrained by domain, offer valuable tools for future research
and demonstrate the potential for scaling personalized dialogue systems across
diverse applications.

</details>

### [77] [PropRAG: Guiding Retrieval with Beam Search over Proposition Paths](https://arxiv.org/abs/2504.18070)
*Jingjin Wang*

Main category: cs.CL

TLDR: PropRAG通过利用上下文丰富的命题和新的束搜索算法，改进了标准RAG的检索方法，实现了多步推理链的显式发现，并在多个数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 标准RAG方法无法捕捉人类记忆的关联性和上下文理解能力，导致复杂推理和情境理解的不足。

Method: PropRAG利用上下文丰富的命题和束搜索算法进行在线检索，完全避免了生成式LLM的在线调用，依赖高效的图遍历和预计算嵌入。

Result: PropRAG在PopQA、2Wiki、HotpotQA和MuSiQue等数据集上取得了零样本Recall@5和F1分数的最高成绩。

Conclusion: PropRAG通过改进证据检索和显式路径发现，推动了非参数化持续学习的进展。

Abstract: Retrieval Augmented Generation (RAG) has become the standard non-parametric
approach for equipping Large Language Models (LLMs) with up-to-date knowledge
and mitigating catastrophic forgetting common in continual learning. However,
standard RAG, relying on independent passage retrieval, fails to capture the
interconnected nature of human memory crucial for complex reasoning
(associativity) and contextual understanding (sense-making). While structured
RAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples,
the inherent context loss limits fidelity. We introduce PropRAG, a framework
leveraging contextually rich propositions and a novel beam search algorithm
over proposition paths to explicitly discover multi-step reasoning chains.
Crucially, PropRAG's online retrieval process operates entirely without
invoking generative LLMs, relying instead on efficient graph traversal and
pre-computed embeddings. This avoids online LLM inference costs and potential
inconsistencies during evidence gathering. LLMs are used effectively offline
for high-quality proposition extraction and post-retrieval for answer
generation. PropRAG achieves state-of-the-art zero-shot Recall@5 results on
PopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside
top F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through
richer representation and explicit, LLM-free online path finding, PropRAG
advances non-parametric continual learning.

</details>

### [78] [Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization](https://arxiv.org/abs/2504.18080)
*Wataru Kawakami,Keita Suzuki,Junichiro Iwasawa*

Main category: cs.CL

TLDR: 论文介绍了Preferred-MedLLM-Qwen-72B模型，通过两阶段微调优化日语医疗领域的准确性和推理可靠性，在IgakuQA基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在医疗领域应用中的准确性、语言限制和推理可靠性问题。

Method: 采用两阶段微调：1) 日语医疗语料库的持续预训练(CPT)；2) 基于偏好的推理优化(RPO)。

Result: 在IgakuQA基准测试中达到0.868准确率，优于GPT-4o，且在生成解释时保持高准确性。

Conclusion: 优化推理可靠性对医疗领域LLM至关重要，模型权重已公开以促进可信LLM研究。

Abstract: Large Language Models (LLMs) show potential in medicine, yet clinical
adoption is hindered by concerns over factual accuracy, language-specific
limitations (e.g., Japanese), and critically, their reliability when required
to generate reasoning explanations -- a prerequisite for trust. This paper
introduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the
Japanese medical domain to achieve both high accuracy and stable reasoning. We
employ a two-stage fine-tuning process on the Qwen2.5-72B base model: first,
Continued Pretraining (CPT) on a comprehensive Japanese medical corpus instills
deep domain knowledge. Second, Reasoning Preference Optimization (RPO), a
preference-based method, enhances the generation of reliable reasoning pathways
while preserving high answer accuracy. Evaluations on the Japanese Medical
Licensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves
state-of-the-art performance (0.868 accuracy), surpassing strong proprietary
models like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which
exhibit significant accuracy degradation (up to 11.5\% and 3.8\% respectively
on IgakuQA) when prompted for explanations, our model maintains its high
accuracy (0.868) under such conditions. This highlights RPO's effectiveness in
stabilizing reasoning generation. This work underscores the importance of
optimizing for reliable explanations alongside accuracy. We release the
Preferred-MedLLM-Qwen-72B model weights to foster research into trustworthy
LLMs for specialized, high-stakes applications.

</details>

### [79] [Random-Set Large Language Models](https://arxiv.org/abs/2504.18085)
*Muhammad Mubashar,Shireen Kudukkil Manchingal,Fabio Cuzzolin*

Main category: cs.CL

TLDR: 论文提出了一种新的随机集大语言模型（RSLLM），用于量化LLMs生成文本的不确定性，通过预测有限随机集（信念函数）而非传统概率向量，提高了答案正确性并具备检测幻觉的能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs生成文本的可信度问题，量化其不确定性，以提升模型的可信度和实用性。

Method: 提出RSLLM方法，基于分层聚类提取关键token子集，预测信念函数而非概率向量，实现高效且可扩展的不确定性量化。

Result: 在CoQA和OBQA数据集上，RSLLM表现优于标准模型，提高了答案正确性，并能有效估计预测的二级不确定性及检测幻觉。

Conclusion: RSLLM通过信念函数量化不确定性，显著提升了LLMs的可信度和实用性，尤其在检测幻觉和二级不确定性估计方面表现突出。

Abstract: Large Language Models (LLMs) are known to produce very high-quality tests and
responses to our queries. But how much can we trust this generated text? In
this paper, we study the problem of uncertainty quantification in LLMs. We
propose a novel Random-Set Large Language Model (RSLLM) approach which predicts
finite random sets (belief functions) over the token space, rather than
probability vectors as in classical LLMs. In order to allow so efficiently, we
also present a methodology based on hierarchical clustering to extract and use
a budget of "focal" subsets of tokens upon which the belief prediction is
defined, rather than using all possible collections of tokens, making the
method scalable yet effective. RS-LLMs encode the epistemic uncertainty induced
in their generation process by the size and diversity of its training set via
the size of the credal sets associated with the predicted belief functions. The
proposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b,
Mistral-7b and Phi-2 models and is shown to outperform the standard model in
both datasets in terms of correctness of answer while also showing potential in
estimating the second level uncertainty in its predictions and providing the
capability to detect when its hallucinating.

</details>

### [80] [Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation](https://arxiv.org/abs/2504.18104)
*Yinglong Yu,Hao Shen,Zhengyi Lyu,Qi He*

Main category: cs.CL

TLDR: 本文提出了一种基于提示调优的事实核查价值评估分类方法，通过设计提示模板和大语言模型结合，提升了在有限或无标签数据下的准确性。实验表明，该方法在F1分数和准确率上优于或匹配BERT、GPT-3.5和GPT-4等基线模型。


<details>
  <summary>Details</summary>
Motivation: 针对全球化和信息化背景下日益严重的错误信息问题，需要一种高效的事实核查价值评估方法。

Method: 基于提示调优技术，设计提示模板并应用于大语言模型，实现上下文学习，提升事实核查价值评估的准确性。

Result: 在公开数据集上的实验表明，该方法在F1分数和准确率等指标上优于或匹配BERT、GPT-3.5和GPT-4等基线模型。

Conclusion: 基于提示调优的方法在事实核查价值评估任务中表现出有效性和先进性。

Abstract: In response to the growing problem of misinformation in the context of
globalization and informatization, this paper proposes a classification method
for fact-check-worthiness estimation based on prompt tuning. We construct a
model for fact-check-worthiness estimation at the methodological level using
prompt tuning. By applying designed prompt templates to large language models,
we establish in-context learning and leverage prompt tuning technology to
improve the accuracy of determining whether claims have fact-check-worthiness,
particularly when dealing with limited or unlabeled data. Through extensive
experiments on public datasets, we demonstrate that the proposed method
surpasses or matches multiple baseline methods in the classification task of
fact-check-worthiness estimation assessment, including classical pre-trained
models such as BERT, as well as recent popular large models like GPT-3.5 and
GPT-4. Experiments show that the prompt tuning-based method proposed in this
study exhibits certain advantages in evaluation metrics such as F1 score and
accuracy, thereby effectively validating its effectiveness and advancement in
the task of fact-check-worthiness estimation.

</details>

### [81] [Comparative Study on the Discourse Meaning of Chinese and English Media in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering](https://arxiv.org/abs/2504.18106)
*Yinglong Yu,Zhaopu Yao,Fang Yuan*

Main category: cs.CL

TLDR: 该研究通过主题建模、大语言模型提示工程和语料库短语学方法，分析中英文媒体对巴黎奥运会的报道，探讨话语构建和态度意义的异同。


<details>
  <summary>Details</summary>
Motivation: 研究中英文媒体在报道巴黎奥运会时的差异，揭示不同文化背景下的报道倾向和态度表达。

Method: 采用主题建模、大语言模型提示工程和语料库短语学方法进行分析。

Result: 中文媒体关注具体运动项目、体育精神、兴奋剂争议和新技术，英文媒体则聚焦女性运动员、奖牌获得和资格争议。中文报道在开幕式和体育精神描述中更频繁使用介词共现和积极语义韵，英文报道在女性运动员报道中表现积极语义韵，但在开幕式反应预测和女子拳击争议中呈现消极语义韵。

Conclusion: 中英文媒体在报道巴黎奥运会时表现出显著差异，反映了文化背景和价值观的不同。

Abstract: This study analyzes Chinese and English media reports on the Paris Olympics
using topic modeling, Large Language Model (LLM) prompt engineering, and corpus
phraseology methods to explore similarities and differences in discourse
construction and attitudinal meanings. Common topics include the opening
ceremony, athlete performance, and sponsorship brands. Chinese media focus on
specific sports, sports spirit, doping controversies, and new technologies,
while English media focus on female athletes, medal wins, and eligibility
controversies. Chinese reports show more frequent prepositional co-occurrences
and positive semantic prosody in describing the opening ceremony and sports
spirit. English reports exhibit positive semantic prosody when covering female
athletes but negative prosody in predicting opening ceremony reactions and
discussing women's boxing controversies.

</details>

### [82] [Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection](https://arxiv.org/abs/2504.18114)
*Atharva Kulkarni,Yuan Zhang,Joel Ruben Antony Moniz,Xiou Ge,Bo-Hsiang Tseng,Dhivya Piraviperumal,Swabha Swayamdipta,Hong Yu*

Main category: cs.CL

TLDR: 论文研究了语言模型幻觉问题的评估方法，发现现有指标与人类判断不一致，且缺乏鲁棒性。GPT-4等LLM评估表现最佳，模式搜索解码方法可减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 语言模型的幻觉问题严重影响其可靠性和应用，但现有评估指标缺乏鲁棒性和泛化能力。

Method: 对6组幻觉检测指标进行大规模实证评估，覆盖4个数据集、37个语言模型和5种解码方法。

Result: 现有指标与人类判断不一致，GPT-4表现最佳，模式搜索解码方法减少幻觉。

Conclusion: 需开发更鲁棒的指标和策略以量化并缓解幻觉问题。

Abstract: Hallucinations pose a significant obstacle to the reliability and widespread
adoption of language models, yet their accurate measurement remains a
persistent challenge. While many task- and domain-specific metrics have been
proposed to assess faithfulness and factuality concerns, the robustness and
generalization of these metrics are still untested. In this paper, we conduct a
large-scale empirical evaluation of 6 diverse sets of hallucination detection
metrics across 4 datasets, 37 language models from 5 families, and 5 decoding
methods. Our extensive investigation reveals concerning gaps in current
hallucination evaluation: metrics often fail to align with human judgments,
take an overtly myopic view of the problem, and show inconsistent gains with
parameter scaling. Encouragingly, LLM-based evaluation, particularly with
GPT-4, yields the best overall results, and mode-seeking decoding methods seem
to reduce hallucinations, especially in knowledge-grounded settings. These
findings underscore the need for more robust metrics to understand and quantify
hallucinations, and better strategies to mitigate them.

</details>

### [83] [Temporal Entailment Pretraining for Clinical Language Models over EHR Data](https://arxiv.org/abs/2504.18128)
*Tatsunori Tanaka,Fi Zheng,Kai Sato,Zhifeng Li,Yuanyun Zhang,Shi Li*

Main category: cs.CL

TLDR: 提出了一种新的临床语言模型预训练目标，通过时间顺序的句子对训练模型，提升其在临床任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法将电子健康记录视为静态文档，忽略了其时间演化和因果关系。

Method: 将EHR片段作为时间顺序的句子对，训练模型判断后续状态是否由先前状态蕴含、矛盾或中立。

Result: 在MIMIC IV上预训练，在时间临床QA、早期预警预测和疾病进展建模中取得最佳结果。

Conclusion: 通过时间结构化的预训练任务，模型能够学习潜在临床推理，提升泛化能力。

Abstract: Clinical language models have achieved strong performance on downstream tasks
by pretraining on domain specific corpora such as discharge summaries and
medical notes. However, most approaches treat the electronic health record as a
static document, neglecting the temporally-evolving and causally entwined
nature of patient trajectories. In this paper, we introduce a novel temporal
entailment pretraining objective for language models in the clinical domain.
Our method formulates EHR segments as temporally ordered sentence pairs and
trains the model to determine whether a later state is entailed by,
contradictory to, or neutral with respect to an earlier state. Through this
temporally structured pretraining task, models learn to perform latent clinical
reasoning over time, improving their ability to generalize across forecasting
and diagnosis tasks. We pretrain on a large corpus derived from MIMIC IV and
demonstrate state of the art results on temporal clinical QA, early warning
prediction, and disease progression modeling.

</details>

### [84] [EDU-NER-2025: Named Entity Recognition in Urdu Educational Texts using XLM-RoBERTa with X (formerly Twitter)](https://arxiv.org/abs/2504.18142)
*Fida Ullah,Muhammad Ahmad,Muhammad Tayyab Zamir,Muhammad Arif,Grigori sidorov,Edgardo Manuel Felipe Riverón,Alexander Gelbukh*

Main category: cs.CL

TLDR: 该论文研究了乌尔都语教育领域中的命名实体识别（NER），填补了该领域缺乏标注数据集的空白，并提出了一个名为EDU-NER-2025的新数据集。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语在教育领域的NER研究不足，缺乏标注数据集，限制了模型对学术角色、课程名称等实体的识别能力。

Method: 研究创建了EDU-NER-2025数据集，详细描述了标注过程和指南，并分析了乌尔都语中的语言挑战。

Result: 提出了包含13个教育领域实体的标注数据集，并讨论了标注过程中的挑战和语言复杂性。

Conclusion: 该研究为乌尔都语教育领域的NER提供了重要资源，并为进一步研究奠定了基础。

Abstract: Named Entity Recognition (NER) plays a pivotal role in various Natural
Language Processing (NLP) tasks by identifying and classifying named entities
(NEs) from unstructured data into predefined categories such as person,
organization, location, date, and time. While extensive research exists for
high-resource languages and general domains, NER in Urdu particularly within
domain-specific contexts like education remains significantly underexplored.
This is Due to lack of annotated datasets for educational content which limits
the ability of existing models to accurately identify entities such as academic
roles, course names, and institutional terms, underscoring the urgent need for
targeted resources in this domain. To the best of our knowledge, no dataset
exists in the domain of the Urdu language for this purpose. To achieve this
objective this study makes three key contributions. Firstly, we created a
manually annotated dataset in the education domain, named EDU-NER-2025, which
contains 13 unique most important entities related to education domain. Second,
we describe our annotation process and guidelines in detail and discuss the
challenges of labelling EDU-NER-2025 dataset. Third, we addressed and analyzed
key linguistic challenges, such as morphological complexity and ambiguity,
which are prevalent in formal Urdu texts.

</details>

### [85] [Aligning Language Models for Icelandic Legal Text Summarization](https://arxiv.org/abs/2504.18180)
*Þórir Hrafn Harðarson,Hrafn Loftsson,Stefán Ólafsson*

Main category: cs.CL

TLDR: 研究探讨了基于偏好的训练技术（如RLHF和DPO）是否能提升语言模型在冰岛法律摘要生成中的表现，结果显示偏好训练提高了法律准确性，但对语言质量提升不明显。


<details>
  <summary>Details</summary>
Motivation: 法律领域的语言模型应用潜力大，但法律文本的专业性和复杂性带来了挑战，需探索更有效的训练方法。

Method: 比较了基于偏好训练（RLHF和DPO）和传统监督学习的模型在法律摘要生成中的表现。

Result: 偏好训练提高了法律准确性，但对冰岛语语言质量无显著改善；自动指标与人工评估存在差异。

Conclusion: 偏好训练在法律领域有潜力，但需结合定性评估以优化模型表现。

Abstract: The integration of language models in the legal domain holds considerable
promise for streamlining processes and improving efficiency in managing
extensive workloads. However, the specialized terminology, nuanced language,
and formal style of legal texts can present substantial challenges. This study
examines whether preference-based training techniques, specifically
Reinforcement Learning from Human Feedback and Direct Preference Optimization,
can enhance models' performance in generating Icelandic legal summaries that
align with domain-specific language standards and user preferences. We compare
models fine-tuned with preference training to those using conventional
supervised learning. Results indicate that preference training improves the
legal accuracy of generated summaries over standard fine-tuning but does not
significantly enhance the overall quality of Icelandic language usage.
Discrepancies between automated metrics and human evaluations further
underscore the importance of qualitative assessment in developing language
models for the legal domain.

</details>

### [86] [Optimising ChatGPT for creativity in literary translation: A case study from English into Dutch, Chinese, Catalan and Spanish](https://arxiv.org/abs/2504.18221)
*Shuxiang Du,Ana Guerberof Arenas,Antonio Toral,Kyo Gerrits,Josep Marco Borillo*

Main category: cs.CL

TLDR: 研究探讨了ChatGPT在六种配置下对四种语言的机器翻译输出变异性，重点关注文学文本的创造性。通过创造力评分公式评估不同文本粒度、温度设置和提示策略，发现最小指令提示（温度1.0）在西班牙语、荷兰语和中文中表现最佳，但仍不及人工翻译。


<details>
  <summary>Details</summary>
Motivation: 探索ChatGPT在不同配置下的机器翻译表现，尤其是文学文本的创造性翻译能力。

Method: 在四种语言中测试六种配置（文本粒度、温度设置、提示策略），使用创造力评分公式评估翻译质量。

Result: 最小指令提示（温度1.0）在西班牙语、荷兰语和中文中表现最佳，但ChatGPT整体表现仍低于人工翻译。

Conclusion: ChatGPT在创造性翻译中表现有限，仍需改进以接近人工翻译水平。

Abstract: This study examines the variability of Chat-GPT machine translation (MT)
outputs across six different configurations in four languages,with a focus on
creativity in a literary text. We evaluate GPT translations in different text
granularity levels, temperature settings and prompting strategies with a
Creativity Score formula. We found that prompting ChatGPT with a minimal
instruction yields the best creative translations, with "Translate the
following text into [TG] creatively" at the temperature of 1.0 outperforming
other configurations and DeepL in Spanish, Dutch, and Chinese. Nonetheless,
ChatGPT consistently underperforms compared to human translation (HT).

</details>

### [87] [Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family](https://arxiv.org/abs/2504.18225)
*Pierre-Carl Langlais,Pavel Chizhov,Mattia Nee,Carlos Rosas Hinostroza,Matthieu Delsart,Irène Girard,Othman Hicheur,Anastasia Stasenko,Ivan P. Yamshchikov*

Main category: cs.CL

TLDR: 介绍了新一代小型推理模型Pleias-RAG-350m和Pleias-RAG-1B，支持RAG、搜索和源摘要，性能优于4B参数以下的SLM，并与更大模型竞争。


<details>
  <summary>Details</summary>
Motivation: 开发小型高效模型，支持多语言RAG工作流，提升事实性和部署便利性。

Method: 基于大型合成数据集进行预训练，支持引用、查询路由、查询重构和源重排等功能。

Result: 在HotPotQA和2wiki等基准测试中表现优异，支持多语言并确保引用准确性。

Conclusion: 这些模型因其高效性和设计优势，为生成式AI开辟了新应用场景。

Abstract: We introduce a new generation of small reasoning models for RAG, search, and
source summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a
large synthetic dataset emulating the retrieval of a wide variety of
multilingual open sources from the Common Corpus. They provide native support
for citation and grounding with literal quotes and reintegrate multiple
features associated with RAG workflows, such as query routing, query
reformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B
outperform SLMs below 4 billion parameters on standardized RAG benchmarks
(HotPotQA, 2wiki) and are competitive with popular larger models, including
Qwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date
maintaining consistent RAG performance across leading European languages and
ensuring systematic reference grounding for statements. Due to their size and
ease of deployment on constrained infrastructure and higher factuality by
design, the models unlock a range of new use cases for generative AI.

</details>

### [88] [Efficient Single-Pass Training for Multi-Turn Reasoning](https://arxiv.org/abs/2504.18246)
*Ritesh Goru,Shanay Mehta,Prateek Jain*

Main category: cs.CL

TLDR: 提出一种新方法，通过响应令牌复制和自定义注意力掩码，解决多轮推理数据集中LLMs训练时的输入不一致问题，显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: LLMs在多轮推理任务中生成推理令牌时，这些令牌会被排除在后续输入之外，导致无法单次前向处理整个对话，限制了训练效率。

Method: 采用响应令牌复制和自定义注意力掩码，确保推理令牌的可见性约束，从而优化训练过程。

Result: 显著减少了训练时间，实现了对多轮推理数据集的高效微调。

Conclusion: 该方法有效解决了多轮推理数据集训练中的输入不一致问题，提升了训练效率。

Abstract: Training Large Language Models ( LLMs) to generate explicit reasoning before
they produce an answer has been shown to improve their performance across
various tasks such as mathematics and coding. However, fine-tuning LLMs on
multi-turn reasoning datasets presents a unique challenge: LLMs must generate
reasoning tokens that are excluded from subsequent inputs to the LLM. This
discrepancy prevents us from processing an entire conversation in a single
forward pass-an optimization readily available when we fine-tune on a
multi-turn non-reasoning dataset. This paper proposes a novel approach that
overcomes this limitation through response token duplication and a custom
attention mask that enforces appropriate visibility constraints. Our approach
significantly reduces the training time and allows efficient fine-tuning on
multi-turn reasoning datasets.

</details>

### [89] [MAGI: Multi-Agent Guided Interview for Psychiatric Assessment](https://arxiv.org/abs/2504.18260)
*Guanqun Bi,Zhuang Chen,Zhoufu Liu,Hongkai Wang,Xiyao Xiao,Yuqiang Xie,Wen Zhang,Yongkang Huang,Yuxuan Chen,Libiao Peng,Yi Feng,Minlie Huang*

Main category: cs.CL

TLDR: MAGI框架通过多智能体协作将MINI结构化临床访谈自动化，结合临床严谨性、对话适应性和可解释推理，提升LLM辅助心理健康评估。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型方法无法与精神病学诊断协议对齐，自动化结构化临床访谈可改善心理健康服务的可及性。

Method: MAGI通过四个专门智能体（导航、问题、判断和诊断）动态导航临床逻辑，生成PsyCoT追踪症状与临床标准的映射。

Result: 在1002名真实参与者中测试，涵盖抑郁、广泛性焦虑、社交焦虑和自杀倾向，MAGI显著提升了评估效果。

Conclusion: MAGI是首个将MINI转化为自动化计算流程的框架，为LLM辅助心理健康评估提供了新方向。

Abstract: Automating structured clinical interviews could revolutionize mental
healthcare accessibility, yet existing large language models (LLMs) approaches
fail to align with psychiatric diagnostic protocols. We present MAGI, the first
framework that transforms the gold-standard Mini International Neuropsychiatric
Interview (MINI) into automatic computational workflows through coordinated
multi-agent collaboration. MAGI dynamically navigates clinical logic via four
specialized agents: 1) an interview tree guided navigation agent adhering to
the MINI's branching structure, 2) an adaptive question agent blending
diagnostic probing, explaining, and empathy, 3) a judgment agent validating
whether the response from participants meet the node, and 4) a diagnosis Agent
generating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map
symptoms to clinical criteria. Experimental results on 1,002 real-world
participants covering depression, generalized anxiety, social anxiety and
suicide shows that MAGI advances LLM- assisted mental health assessment by
combining clinical rigor, conversational adaptability, and explainable
reasoning.

</details>

### [90] [TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation](https://arxiv.org/abs/2504.18269)
*Shintaro Ozaki,Kazuki Hayashi,Yusuke Sakai,Jingun Kwon,Hidetaka Kamigaito,Katsuhiko Hayashi,Manabu Okumura,Taro Watanabe*

Main category: cs.CL

TLDR: TextTIGER通过增强和总结实体相关的描述来优化图像生成提示，提升生成性能。


<details>
  <summary>Details</summary>
Motivation: 解决图像生成模型中实体知识记忆不足的问题。

Method: 使用LLM增强和总结实体描述，提出WiT-Cub数据集进行评估。

Result: 在IS、FID和CLIPScore等指标上优于仅使用标题提示的方法。

Conclusion: 优化提示中的实体描述能显著提升图像生成能力。

Abstract: Generating images from prompts containing specific entities requires models
to retain as much entity-specific knowledge as possible. However, fully
memorizing such knowledge is impractical due to the vast number of entities and
their continuous emergence. To address this, we propose Text-based Intelligent
Generation with Entity prompt Refinement (TextTIGER), which augments knowledge
on entities included in the prompts and then summarizes the augmented
descriptions using Large Language Models (LLMs) to mitigate performance
degradation from longer inputs. To evaluate our method, we introduce WiT-Cub
(WiT with Captions and Uncomplicated Background-explanations), a dataset
comprising captions, images, and an entity list. Experiments on four image
generation models and five LLMs show that TextTIGER improves image generation
performance in standard metrics (IS, FID, and CLIPScore) compared to
caption-only prompts. Additionally, multiple annotators' evaluation confirms
that the summarized descriptions are more informative, validating LLMs' ability
to generate concise yet rich descriptions. These findings demonstrate that
refining prompts with augmented and summarized entity-related descriptions
enhances image generation capabilities. The code and dataset will be available
upon acceptance.

</details>

### [91] [Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review](https://arxiv.org/abs/2504.18346)
*Toghrul Abbasli,Kentaroh Toyoda,Yuan Wang,Leon Witt,Muhammad Asif Ali,Yukai Miao,Dan Li,Qingsong Wei*

Main category: cs.CL

TLDR: 该论文系统综述了大语言模型（LLMs）中的不确定性量化（UQ）和校准方法，填补了文献空白，并提出了一个严格的基准测试。


<details>
  <summary>Details</summary>
Motivation: LLMs的幻觉问题（输出错误信息）是主要挑战之一，但缺乏对其不确定性评估和校准方法的深入分析和全面比较。

Method: 通过系统综述代表性文献，并引入一个严格的基准，使用两个广泛使用的可靠性数据集对六种相关方法进行实证评估。

Result: 实证评估验证了综述的重要发现，并展示了现有方法的有效性。

Conclusion: 论文首次专门研究了LLMs的校准方法和相关指标，并提出了未来研究方向和开放挑战。

Abstract: Large Language Models (LLMs) have been transformative across many domains.
However, hallucination -- confidently outputting incorrect information --
remains one of the leading challenges for LLMs. This raises the question of how
to accurately assess and quantify the uncertainty of LLMs. Extensive literature
on traditional models has explored Uncertainty Quantification (UQ) to measure
uncertainty and employed calibration techniques to address the misalignment
between uncertainty and accuracy. While some of these methods have been adapted
for LLMs, the literature lacks an in-depth analysis of their effectiveness and
does not offer a comprehensive benchmark to enable insightful comparison among
existing solutions. In this work, we fill this gap via a systematic survey of
representative prior works on UQ and calibration for LLMs and introduce a
rigorous benchmark. Using two widely used reliability datasets, we empirically
evaluate six related methods, which justify the significant findings of our
review. Finally, we provide outlooks for key future directions and outline open
challenges. To the best of our knowledge, this survey is the first dedicated
study to review the calibration methods and relevant metrics for LLMs.

</details>

### [92] [Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant](https://arxiv.org/abs/2504.18373)
*Lei Shen,Xiaoyu Shen*

Main category: cs.CL

TLDR: Auto-SLURP是一个专为评估基于LLM的多智能体框架设计的基准数据集，扩展了SLURP数据集，提供端到端评估。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对LLM多智能体框架的基准数据集，Auto-SLURP旨在填补这一空白。

Method: 通过重新标注SLURP数据并集成模拟服务器和外部服务，构建端到端评估管道。

Result: 实验表明Auto-SLURP对现有先进框架构成显著挑战。

Conclusion: 真正可靠的多智能体个人助手仍需进一步研究，数据集和代码已开源。

Abstract: In recent years, multi-agent frameworks powered by large language models
(LLMs) have advanced rapidly. Despite this progress, there is still a notable
absence of benchmark datasets specifically tailored to evaluate their
performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset
aimed at evaluating LLM-based multi-agent frameworks in the context of
intelligent personal assistants. Auto-SLURP extends the original SLURP dataset
-- initially developed for natural language understanding tasks -- by
relabeling the data and integrating simulated servers and external services.
This enhancement enables a comprehensive end-to-end evaluation pipeline,
covering language understanding, task execution, and response generation. Our
experiments demonstrate that Auto-SLURP presents a significant challenge for
current state-of-the-art frameworks, highlighting that truly reliable and
intelligent multi-agent personal assistants remain a work in progress. The
dataset and related code are available at
https://github.com/lorashen/Auto-SLURP/.

</details>

### [93] [Pushing the boundary on Natural Language Inference](https://arxiv.org/abs/2504.18376)
*Pablo Miralles-González,Javier Huertas-Tato,Alejandro Martín,David Camacho*

Main category: cs.CL

TLDR: 论文提出了一种基于强化学习的方法（GRPO）用于NLI任务，无需标注数据，并在ANLI等挑战性数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前NLI系统依赖监督学习，数据集存在偏差和人工标注问题，限制了泛化能力。

Method: 采用GRPO强化学习方法，结合Chain-of-Thought学习，使用LoRA和QLoRA参数高效技术微调7B至32B模型。

Result: 32B量化模型在11个对抗性数据集中7个上超越SOTA，内存占用仅22GB。

Conclusion: 该方法为构建鲁棒NLI系统提供了可扩展且实用的框架，不牺牲推理质量。

Abstract: Natural Language Inference (NLI) is a central task in natural language
understanding with applications in fact-checking, question answering, and
information retrieval. Despite its importance, current NLI systems heavily rely
on supervised learning with datasets that often contain annotation artifacts
and biases, limiting generalization and real-world applicability. In this work,
we apply a reinforcement learning-based approach using Group Relative Policy
Optimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the
need for labeled rationales and enabling this type of training on more
challenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language
models using parameter-efficient techniques (LoRA and QLoRA), demonstrating
strong performance across standard and adversarial NLI benchmarks. Our 32B
AWQ-quantized model surpasses state-of-the-art results on 7 out of 11
adversarial sets$\unicode{x2013}$or on all of them considering our
replication$\unicode{x2013}$within a 22GB memory footprint, showing that robust
reasoning can be retained under aggressive quantization. This work provides a
scalable and practical framework for building robust NLI systems without
sacrificing inference quality.

</details>

### [94] [A UD Treebank for Bohairic Coptic](https://arxiv.org/abs/2504.18386)
*Amir Zeldes,Nina Speransky,Nicholas Wagner,Caroline T. Schroeder*

Main category: cs.CL

TLDR: 本文介绍了首个语法标注的Bohairic Coptic语料库，并比较了其与Sahidic Coptic的主要差异，揭示了Bohairic的独特性。


<details>
  <summary>Details</summary>
Motivation: Bohairic Coptic作为重要但资源匮乏的方言，需要更多研究资源。

Method: 构建并评估Bohairic Coptic的语法标注语料库，并进行跨方言解析实验。

Result: 揭示了Bohairic与Sahidic Coptic的显著差异，验证了其独特性。

Conclusion: Bohairic Coptic是一种与Sahidic相关但独特的方言，需进一步研究。

Abstract: Despite recent advances in digital resources for other Coptic dialects,
especially Sahidic, Bohairic Coptic, the main Coptic dialect for pre-Mamluk,
late Byzantine Egypt, and the contemporary language of the Coptic Church,
remains critically under-resourced. This paper presents and evaluates the first
syntactically annotated corpus of Bohairic Coptic, sampling data from a range
of works, including Biblical text, saints' lives and Christian ascetic writing.
We also explore some of the main differences we observe compared to the
existing UD treebank of Sahidic Coptic, the classical dialect of the language,
and conduct joint and cross-dialect parsing experiments, revealing the unique
nature of Bohairic as a related, but distinct variety from the more often
studied Sahidic.

</details>

### [95] [HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?](https://arxiv.org/abs/2504.18406)
*Yusen Zhang,Wenliang Zheng,Aashrith Madasu,Peng Shi,Ryo Kamoi,Hao Zhou,Zhuoyang Zou,Shu Zhao,Sarkar Snigdha Sarathi Das,Vipul Gupta,Xiaoxin Lu,Nan Zhang,Ranran Haoran Zhang,Avitej Iyer,Renze Lou,Wenpeng Yin,Rui Zhang*

Main category: cs.CL

TLDR: 论文介绍了HRScene，一个用于评估高分辨率图像（HRI）理解的新基准，包含25个真实数据集和2个合成数据集，实验显示当前视觉大语言模型（VLMs）在HRI理解上表现有限。


<details>
  <summary>Details</summary>
Motivation: 现有视觉大语言模型（VLMs）缺乏针对高分辨率图像（HRI）理解的全面评估基准，HRScene填补了这一空白。

Method: HRScene整合了25个真实数据集和2个合成诊断数据集，覆盖多种场景，并通过28种VLMs进行实验评估。

Result: 当前VLMs在真实任务中平均准确率约50%，在合成数据中表现出区域利用不足的问题。

Conclusion: HRScene揭示了VLMs在HRI理解上的不足，为未来研究提供了方向。

Abstract: High-resolution image (HRI) understanding aims to process images with a large
number of pixels, such as pathological images and agricultural aerial images,
both of which can exceed 1 million pixels. Vision Large Language Models (VLMs)
can allegedly handle HRIs, however, there is a lack of a comprehensive
benchmark for VLMs to evaluate HRI understanding. To address this gap, we
introduce HRScene, a novel unified benchmark for HRI understanding with rich
scenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic
datasets with resolutions ranging from 1,024 $\times$ 1,024 to 35,503 $\times$
26,627. HRScene is collected and re-annotated by 10 graduate-level annotators,
covering 25 scenarios, ranging from microscopic to radiology images, street
views, long-range pictures, and telescope images. It includes HRIs of
real-world objects, scanned documents, and composite multi-image. The two
diagnostic evaluation datasets are synthesized by combining the target image
with the gold answer and distracting images in different orders, assessing how
well models utilize regions in HRI. We conduct extensive experiments involving
28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show
that current VLMs achieve an average accuracy of around 50% on real-world
tasks, revealing significant gaps in HRI understanding. Results on synthetic
datasets reveal that VLMs struggle to effectively utilize HRI regions, showing
significant Regional Divergence and lost-in-middle, shedding light on future
research.

</details>

### [96] [Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers](https://arxiv.org/abs/2504.18412)
*Jared Moore,Declan Grabb,William Agnew,Kevin Klyman,Stevie Chancellor,Desmond C. Ong,Nick Haber*

Main category: cs.CL

TLDR: 研究探讨了大型语言模型（LLM）是否适合替代心理治疗师，发现LLM在治疗关系中存在缺陷，如表达偏见和不恰当回应，结论是不应替代治疗师。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在心理治疗中的潜在应用，尤其是替代治疗师的可行性。

Method: 通过映射治疗指南并实验评估LLM（如GPT-4）在治疗关系中的表现。

Result: LLM存在偏见和不恰当回应，无法满足治疗关系的关键要求。

Conclusion: LLM不应替代治疗师，但可探索其他临床辅助角色。

Abstract: Should a large language model (LLM) be used as a therapist? In this paper, we
investigate the use of LLMs to *replace* mental health providers, a use case
promoted in the tech startup and research space. We conduct a mapping review of
therapy guides used by major medical institutions to identify crucial aspects
of therapeutic relationships, such as the importance of a therapeutic alliance
between therapist and client. We then assess the ability of LLMs to reproduce
and adhere to these aspects of therapeutic relationships by conducting several
experiments investigating the responses of current LLMs, such as `gpt-4o`.
Contrary to best practices in the medical community, LLMs 1) express stigma
toward those with mental health conditions and 2) respond inappropriately to
certain common (and critical) conditions in naturalistic therapy settings --
e.g., LLMs encourage clients' delusional thinking, likely due to their
sycophancy. This occurs even with larger and newer LLMs, indicating that
current safety practices may not address these gaps. Furthermore, we note
foundational and practical barriers to the adoption of LLMs as therapists, such
as that a therapeutic alliance requires human characteristics (e.g., identity
and stakes). For these reasons, we conclude that LLMs should not replace
therapists, and we discuss alternative roles for LLMs in clinical therapy.

</details>

### [97] [BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs](https://arxiv.org/abs/2504.18415)
*Hongyu Wang,Shuming Ma,Furu Wei*

Main category: cs.CL

TLDR: BitNet v2 是一种新框架，支持1-bit LLMs的4-bit激活量化，通过H-BitLinear模块解决激活异常值问题，显著降低内存和计算成本。


<details>
  <summary>Details</summary>
Motivation: 1-bit LLMs的高效部署受到激活异常值的阻碍，导致低比特量化困难。

Method: 提出H-BitLinear模块，应用在线Hadamard变换平滑激活分布，使其更适合低比特表示。

Result: BitNet v2在4-bit激活量化下性能损失极小，显著减少内存和计算开销。

Conclusion: BitNet v2为1-bit LLMs的高效部署提供了可行方案，同时保持性能。

Abstract: Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by
activation outliers, which complicate quantization to low bit-widths. We
introduce BitNet v2, a novel framework enabling native 4-bit activation
quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward
network activations, we propose H-BitLinear, a module applying an online
Hadamard transformation prior to activation quantization. This transformation
smooths sharp activation distributions into more Gaussian-like forms, suitable
for low-bit representation. Experiments show BitNet v2 trained from scratch
with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2
achieves minimal performance degradation when trained with native 4-bit
activations, significantly reducing memory footprint and computational cost for
batched inference.

</details>

### [98] [PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts](https://arxiv.org/abs/2504.18428)
*Yiming Wang,Pei Zhang,Jialong Tang,Haoran Wei,Baosong Yang,Rui Wang,Chenshu Sun,Feitong Sun,Jiran Zhang,Junxuan Wu,Qiqian Cang,Yichang Zhang,Fei Huang,Junyang Lin,Fei Huang,Jingren Zhou*

Main category: cs.CL

TLDR: PolyMath是一个多语言数学推理基准，涵盖18种语言和4个难度级别，评估显示当前LLMs在多语言推理中存在性能差异、输入输出语言一致性低等问题。


<details>
  <summary>Details</summary>
Motivation: 为多语言数学推理提供一个全面且高质量的基准测试工具，以评估和提升LLMs在多语言环境下的推理能力。

Method: 开发PolyMath基准，覆盖18种语言和4个难度级别，并对先进LLMs进行全面评估。

Result: Deepseek-R1-671B和Qwen-QwQ-32B的基准得分仅为43.4和41.8，最高难度下准确率不足30%。研究还揭示了LLMs在多语言推理中的关键挑战。

Conclusion: 控制输出语言可能影响推理性能，为提升LLMs的多语言能力提供了潜在方向。

Abstract: In this paper, we introduce PolyMath, a multilingual mathematical reasoning
benchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our
benchmark ensures difficulty comprehensiveness, language diversity, and
high-quality translation, making it a highly discriminative multilingual
mathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive
evaluation for advanced LLMs and find that even Deepseek-R1-671B and
Qwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30%
accuracy under the highest level. From a language perspective, our benchmark
reveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning
performance varies widely across languages for current LLMs; (2) Input-output
language consistency is low in reasoning LLMs and may be correlated with
performance; (3) The thinking length differs significantly by language for
current LLMs. Additionally, we demonstrate that controlling the output language
in the instructions has the potential to affect reasoning performance,
especially for some low-resource languages, suggesting a promising direction
for improving multilingual capabilities in LLMs.

</details>

### [99] [Fast-Slow Thinking for Large Vision-Language Model Reasoning](https://arxiv.org/abs/2504.18458)
*Wenyi Xiao,Leilei Gan,Weilong Dai,Wanggui He,Ziwei Huang,Haoyuan Li,Fangxun Shu,Zhelun Yu,Peng Zhang,Hao Jiang,Fei Wu*

Main category: cs.CL

TLDR: FAST框架通过动态调整推理深度，显著提升大型视觉语言模型的效率和准确性，同时减少冗余推理。


<details>
  <summary>Details</summary>
Motivation: 解决大型视觉语言模型在任务中普遍存在的过度推理（overthinking）问题，即模型无论问题复杂度如何都会生成冗长的推理。

Method: 提出FAST框架，结合快速-慢速思维动态调整推理深度，包括问题特征建模、自适应奖励机制和难度感知KL正则化。

Result: 在七个推理基准测试中，FAST相对基础模型提升10%准确率，同时减少32.7-67.3%的token使用。

Conclusion: FAST有效平衡推理长度与准确性，为大型视觉语言模型提供了一种高效的推理优化方案。

Abstract: Recent advances in large vision-language models (LVLMs) have revealed an
\textit{overthinking} phenomenon, where models generate verbose reasoning
across all tasks regardless of questions. To address this issue, we present
\textbf{FAST}, a novel \textbf{Fa}st-\textbf{S}low \textbf{T}hinking framework
that dynamically adapts reasoning depth based on question characteristics.
Through empirical analysis, we establish the feasibility of fast-slow thinking
in LVLMs by investigating how response length and data distribution affect
performance. We develop FAST-GRPO with three components: model-based metrics
for question characterization, an adaptive thinking reward mechanism, and
difficulty-aware KL regularization. Experiments across seven reasoning
benchmarks demonstrate that FAST achieves state-of-the-art accuracy with over
10\% relative improvement compared to the base model, while reducing token
usage by 32.7-67.3\% compared to previous slow-thinking approaches, effectively
balancing reasoning length and accuracy.

</details>

### [100] [Generative Induction of Dialogue Task Schemas with Streaming Refinement and Simulated Interactions](https://arxiv.org/abs/2504.18474)
*James D. Finch,Yasasvi Josyula,Jinho D. Choi*

Main category: cs.CL

TLDR: 本文提出了一种基于语言模型的槽模式归纳（SSI）方法，通过文本生成任务自动构建和优化槽模式，并开发了全自动LLM-based TOD模拟方法生成高质量数据。同时解决了评估中的数据泄漏和指标对齐问题。


<details>
  <summary>Details</summary>
Motivation: 提升任务导向对话系统中槽模式归纳的自动化水平，减少人工干预，并解决现有评估方法的不足。

Method: 将SSI作为文本生成任务，利用语言模型增量构建槽模式；开发全自动LLM-based模拟方法生成带标签数据；改进评估指标和数据。

Result: 提出了一种新的SoTA方法，解决了数据泄漏和评估指标问题，为未来SSI研究奠定了基础。

Conclusion: 该方法显著提升了对话理解和系统开发的SoTA，为SSI研究提供了新的方向和工具。

Abstract: In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is
essential for automatically identifying key information slots from dialogue
data without manual intervention. This paper presents a novel state-of-the-art
(SoTA) approach that formulates SSI as a text generation task, where a language
model incrementally constructs and refines a slot schema over a stream of
dialogue data. To develop this approach, we present a fully automatic LLM-based
TOD simulation method that creates data with high-quality state labels for
novel task domains. Furthermore, we identify issues in SSI evaluation due to
data leakage and poor metric alignment with human judgment. We resolve these by
creating new evaluation data using our simulation method with human guidance
and correction, as well as designing improved evaluation metrics. These
contributions establish a foundation for future SSI research and advance the
SoTA in dialogue understanding and system development.

</details>

### [101] [Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues](https://arxiv.org/abs/2504.18483)
*Leandra Fichtel,Maximilian Spliethöver,Eyke Hüllermeier,Patricia Jimenez,Nils Klowait,Stefan Kopp,Axel-Cyrille Ngonga Ngomo,Amelie Robrecht,Ingrid Scharlau,Lutz Terfloth,Anna-Lisa Vollmer,Henning Wachsmuth*

Main category: cs.CL

TLDR: 论文研究了大型语言模型（LLMs）在共同建构性解释对话中作为解释者的能力，发现其能部分促进理解，但监控和调整解释的能力有限。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否能根据解释者的背景和需求动态调整解释，以提升解释的可理解性。

Method: 通过用户研究，让解释者与LLMs互动，评估LLMs的共同建构行为和解释效果。

Result: LLMs表现出部分共同建构行为（如提问验证），能提升解释者的参与度和理解，但监控和调整能力不足。

Conclusion: 当前LLMs在共同建构性解释对话中有潜力，但需进一步改进其动态适应能力。

Abstract: The ability to generate explanations that are understood by explainees is the
quintessence of explainable artificial intelligence. Since understanding
depends on the explainee's background and needs, recent research has focused on
co-constructive explanation dialogues, where the explainer continuously
monitors the explainee's understanding and adapts explanations dynamically. We
investigate the ability of large language models (LLMs) to engage as explainers
in co-constructive explanation dialogues. In particular, we present a user
study in which explainees interact with LLMs, of which some have been
instructed to explain a predefined topic co-constructively. We evaluate the
explainees' understanding before and after the dialogue, as well as their
perception of the LLMs' co-constructive behavior. Our results indicate that
current LLMs show some co-constructive behaviors, such as asking verification
questions, that foster the explainees' engagement and can improve understanding
of a topic. However, their ability to effectively monitor the current
understanding and scaffold the explanations accordingly remains limited.

</details>

### [102] [TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation](https://arxiv.org/abs/2504.18535)
*Gwen Yidou Weng,Benjie Wang,Guy Van den Broeck*

Main category: cs.CL

TLDR: TRACE框架通过高效计算预期属性概率（EAP）和轻量级控制，解决了大型语言模型（LM）输出控制的问题，实现了全局合规的生成。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，需要控制其输出以符合人类价值观或特定属性，但现有方法成本高、灵活性差或效率低。

Method: TRACE通过从LM中提取隐马尔可夫模型（HMM）并结合小型分类器，精确计算EAP，并重新调整LM的下一词概率。

Result: TRACE在解毒任务中达到最先进水平，仅增加10%的解码开销，并能在几秒内适应76个低资源个性化LM。

Conclusion: TRACE提供了一种高效、灵活且可扩展的方法，用于控制LM的输出，适用于多种属性和任务。

Abstract: As large language models (LMs) advance, there is an increasing need to
control their outputs to align with human values (e.g., detoxification) or
desired attributes (e.g., personalization, topic). However, autoregressive
models focus on next-token predictions and struggle with global properties that
require looking ahead. Existing solutions either tune or post-train LMs for
each new attribute - expensive and inflexible - or approximate the Expected
Attribute Probability (EAP) of future sequences by sampling or training, which
is slow and unreliable for rare attributes. We introduce TRACE (Tractable
Probabilistic Reasoning for Adaptable Controllable gEneration), a novel
framework that efficiently computes EAP and adapts to new attributes through
tractable probabilistic reasoning and lightweight control. TRACE distills a
Hidden Markov Model (HMM) from an LM and pairs it with a small classifier to
estimate attribute probabilities, enabling exact EAP computation over the HMM's
predicted futures. This EAP is then used to reweigh the LM's next-token
probabilities for globally compliant continuations. Empirically, TRACE achieves
state-of-the-art results in detoxification with only 10% decoding overhead,
adapts to 76 low-resource personalized LLMs within seconds, and seamlessly
extends to composite attributes.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [103] [ApproXAI: Energy-Efficient Hardware Acceleration of Explainable AI using Approximate Computing](https://arxiv.org/abs/2504.17929)
*Ayesha Siddique,Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.AI

TLDR: XAIedge框架通过近似计算技术提升XAI算法的能效，适用于实时场景。


<details>
  <summary>Details</summary>
Motivation: 现有XAI硬件加速方法在实时场景中能效不足，限制了其应用。

Method: XAIedge将XAI算法转化为近似矩阵计算，结合卷积、傅里叶变换和近似计算范式，在TPU边缘设备上实现高效加速。

Result: XAIedge能效提升2倍，同时保持精度。

Conclusion: XAIedge有望推动可解释AI在能源受限实时应用中的部署。

Abstract: Explainable artificial intelligence (XAI) enhances AI system transparency by
framing interpretability as an optimization problem. However, this approach
often necessitates numerous iterations of computationally intensive operations,
limiting its applicability in real-time scenarios. While recent research has
focused on XAI hardware acceleration on FPGAs and TPU, these methods do not
fully address energy efficiency in real-time settings. To address this
limitation, we propose XAIedge, a novel framework that leverages approximate
computing techniques into XAI algorithms, including integrated gradients, model
distillation, and Shapley analysis. XAIedge translates these algorithms into
approximate matrix computations and exploits the synergy between convolution,
Fourier transform, and approximate computing paradigms. This approach enables
efficient hardware acceleration on TPU-based edge devices, facilitating faster
real-time outcome interpretations. Our comprehensive evaluation demonstrates
that XAIedge achieves a $2\times$ improvement in energy efficiency compared to
existing accurate XAI hardware acceleration techniques while maintaining
comparable accuracy. These results highlight the potential of XAIedge to
significantly advance the deployment of explainable AI in energy-constrained
real-time applications.

</details>

### [104] [LLM Agent Swarm for Hypothesis-Driven Drug Discovery](https://arxiv.org/abs/2504.17967)
*Kevin Song,Andrew Trotter,Jake Y. Chen*

Main category: cs.AI

TLDR: PharmaSwarm是一个多智能体框架，通过整合专业化的LLM代理来优化药物发现流程，提高假设生成和验证的效率。


<details>
  <summary>Details</summary>
Motivation: 药物发现成本高、失败率高，且数据分散，传统方法效率低下。

Method: PharmaSwarm利用多个专业LLM代理，结合基因组分析、知识图谱等功能，通过中央评估器持续优化假设。

Result: 系统支持多种药物发现场景，并通过四层验证确保透明度和可重复性。

Conclusion: PharmaSwarm作为AI助手，能显著加速转化研究，提供高置信度假设。

Abstract: Drug discovery remains a formidable challenge: more than 90 percent of
candidate molecules fail in clinical evaluation, and development costs often
exceed one billion dollars per approved therapy. Disparate data streams, from
genomics and transcriptomics to chemical libraries and clinical records, hinder
coherent mechanistic insight and slow progress. Meanwhile, large language
models excel at reasoning and tool integration but lack the modular
specialization and iterative memory required for regulated, hypothesis-driven
workflows. We introduce PharmaSwarm, a unified multi-agent framework that
orchestrates specialized LLM "agents" to propose, validate, and refine
hypotheses for novel drug targets and lead compounds. Each agent accesses
dedicated functionality--automated genomic and expression analysis; a curated
biomedical knowledge graph; pathway enrichment and network simulation;
interpretable binding affinity prediction--while a central Evaluator LLM
continuously ranks proposals by biological plausibility, novelty, in silico
efficacy, and safety. A shared memory layer captures validated insights and
fine-tunes underlying submodels over time, yielding a self-improving system.
Deployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm
supports literature-driven discovery, omics-guided target identification, and
market-informed repurposing. We also describe a rigorous four-tier validation
pipeline spanning retrospective benchmarking, independent computational assays,
experimental testing, and expert user studies to ensure transparency,
reproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm
can accelerate translational research and deliver high-confidence hypotheses
more efficiently than traditional pipelines.

</details>

### [105] [Differential Privacy-Driven Framework for Enhancing Heart Disease Prediction](https://arxiv.org/abs/2504.18007)
*Yazan Otoum,Amiya Nayak*

Main category: cs.AI

TLDR: 论文探讨了在医疗数据快速数字化的背景下，如何利用机器学习的差分隐私和联邦学习技术保护患者隐私，同时提取有价值的医疗洞察。


<details>
  <summary>Details</summary>
Motivation: 随着医疗系统的数字化，私人健康数据的生成和共享大幅增加，保护患者信息对维护信任和合规至关重要。

Method: 采用差分隐私和联邦学习技术，前者通过添加噪声保护数据隐私，后者支持在分散数据集上协作训练模型。

Result: 在心脏病数据上的实验表明，结合差分隐私的联邦学习模型测试准确率达到85%，同时确保数据隐私。

Conclusion: 差分隐私和联邦学习能有效保护医疗数据隐私，同时提供有价值的分析结果。

Abstract: With the rapid digitalization of healthcare systems, there has been a
substantial increase in the generation and sharing of private health data.
Safeguarding patient information is essential for maintaining consumer trust
and ensuring compliance with legal data protection regulations. Machine
learning is critical in healthcare, supporting personalized treatment, early
disease detection, predictive analytics, image interpretation, drug discovery,
efficient operations, and patient monitoring. It enhances decision-making,
accelerates research, reduces errors, and improves patient outcomes. In this
paper, we utilize machine learning methodologies, including differential
privacy and federated learning, to develop privacy-preserving models that
enable healthcare stakeholders to extract insights without compromising
individual privacy. Differential privacy introduces noise to data to guarantee
statistical privacy, while federated learning enables collaborative model
training across decentralized datasets. We explore applying these technologies
to Heart Disease Data, demonstrating how they preserve privacy while delivering
valuable insights and comprehensive analysis. Our results show that using a
federated learning model with differential privacy achieved a test accuracy of
85%, ensuring patient data remained secure and private throughout the process.

</details>

### [106] [MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and Theory of Mind](https://arxiv.org/abs/2504.18039)
*Zheng Zhang,Nuoqian Xiao,Qi Chai,Deheng Ye,Hao Wang*

Main category: cs.AI

TLDR: MultiMind框架通过整合多模态信息和心理理论模型，提升了LLM代理在社交推理游戏中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有SDG代理仅依赖文本信息，忽略了多模态线索（如面部表情和语音语调），且未建模玩家之间的相互感知。

Method: 使用ONUW作为测试平台，结合多模态信息（面部表情、语音语调）和心理理论模型，通过MCTS优化沟通策略。

Result: 在代理对代理模拟和人类玩家研究中，MultiMind表现出卓越的游戏性能。

Conclusion: MultiMind为LLM代理在多模态社交推理领域迈向类人能力提供了重要进展。

Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities
in social deduction games (SDGs) like Werewolf, where strategic reasoning and
social deception are essential. However, current approaches remain limited to
textual information, ignoring crucial multimodal cues such as facial
expressions and tone of voice that humans naturally use to communicate.
Moreover, existing SDG agents primarily focus on inferring other players'
identities without modeling how others perceive themselves or fellow players.
To address these limitations, we use One Night Ultimate Werewolf (ONUW) as a
testbed and present MultiMind, the first framework integrating multimodal
information into SDG agents. MultiMind processes facial expressions and vocal
tones alongside verbal content, while employing a Theory of Mind (ToM) model to
represent each player's suspicion levels toward others. By combining this ToM
model with Monte Carlo Tree Search (MCTS), our agent identifies communication
strategies that minimize suspicion directed at itself. Through comprehensive
evaluation in both agent-versus-agent simulations and studies with human
players, we demonstrate MultiMind's superior performance in gameplay. Our work
presents a significant advancement toward LLM agents capable of human-like
social reasoning across multimodal domains.

</details>

### [107] [Combating the Bucket Effect:Multi-Knowledge Alignment for Medication Recommendation](https://arxiv.org/abs/2504.18096)
*Xiang Li,Haixu Ma,Guanyong Wu,Shi Mu,Chen Li,Shunpan Liang*

Main category: cs.AI

TLDR: 论文提出MKMed框架，通过跨模态编码器整合多种药物知识，解决药物推荐中的“桶效应”问题，显著提升推荐准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 药物推荐中，不同药物知识数据的不平衡（如仅有文本描述而无结构化数据）导致模型性能受限，称为“桶效应”。

Method: 提出跨模态药物编码器，通过对比学习预训练五种知识模态，将其对齐到统一空间，并结合患者记录进行推荐。

Result: 在MIMIC-III和MIMIC-IV数据集上，MKMed显著优于现有基线，缓解了“桶效应”。

Conclusion: MKMed框架通过整合多模态知识，有效提升药物推荐的准确性和安全性。

Abstract: Medication recommendation is crucial in healthcare, offering effective
treatments based on patient's electronic health records (EHR). Previous studies
show that integrating more medication-related knowledge improves medication
representation accuracy. However, not all medications encompass multiple types
of knowledge data simultaneously. For instance, some medications provide only
textual descriptions without structured data. This imbalance in data
availability limits the performance of existing models, a challenge we term the
"bucket effect" in medication recommendation. Our data analysis uncovers the
severity of the "bucket effect" in medication recommendation. To fill this gap,
we introduce a cross-modal medication encoder capable of seamlessly aligning
data from different modalities and propose a medication recommendation
framework to integrate Multiple types of Knowledge, named MKMed. Specifically,
we first pre-train a cross-modal encoder with contrastive learning on five
knowledge modalities, aligning them into a unified space. Then, we combine the
multi-knowledge medication representations with patient records for
recommendations. Extensive experiments on the MIMIC-III and MIMIC-IV datasets
demonstrate that MKMed mitigates the "bucket effect" in data, and significantly
outperforms state-of-the-art baselines in recommendation accuracy and safety.

</details>

### [108] [Pseudo-Boolean Proof Logging for Optimal Classical Planning](https://arxiv.org/abs/2504.18443)
*Simon Dold,Malte Helmert,Jakob Nordström,Gabriele Röger,Tanja Schindler*

Main category: cs.AI

TLDR: 论文提出了一种用于经典规划任务的下界证明方法，可独立验证任务的不可解性或计划的优化性。


<details>
  <summary>Details</summary>
Motivation: 为规划任务提供可验证的下界证明，确保结果的独立性和可靠性。

Method: 基于伪布尔约束的通用框架，适用于任何规划算法，并以A*算法为例展示优化性证明的生成。

Result: 通过模式数据库启发式和h^max启发式，验证了方法的有效性，且开销较小。

Conclusion: 该方法适用于任何能高效表达为伪布尔约束推理的启发式，具有广泛适用性。

Abstract: We introduce lower-bound certificates for classical planning tasks, which can
be used to prove the unsolvability of a task or the optimality of a plan in a
way that can be verified by an independent third party. We describe a general
framework for generating lower-bound certificates based on pseudo-Boolean
constraints, which is agnostic to the planning algorithm used.
  As a case study, we show how to modify the $A^{*}$ algorithm to produce
proofs of optimality with modest overhead, using pattern database heuristics
and $h^\textit{max}$ as concrete examples. The same proof logging approach
works for any heuristic whose inferences can be efficiently expressed as
reasoning over pseudo-Boolean constraints.

</details>

### [109] [Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning for Verifiable Report Generation](https://arxiv.org/abs/2504.18453)
*Peiyuan Jing,Kinhei Lee,Zhenxuan Zhang,Huichi Zhou,Zhengqing Yuan,Zhifan Gao,Lei Zhu,Giorgos Papanastasiou,Yingying Fang,Guang Yang*

Main category: cs.AI

TLDR: BoxMed-RL 是一种创新的统一训练框架，通过结合医学概念学习和空间验证强化学习，生成可验证和可解释的放射学报告，显著提升了报告质量。


<details>
  <summary>Details</summary>
Motivation: 当前放射学报告生成模型缺乏专家的结构化推理能力，无法将视觉发现与精确的解剖位置联系起来，影响了临床信任和可解释性。

Method: BoxMed-RL 分为两个阶段：预训练阶段通过医学概念学习和空间验证强化学习优化模型；下游适配器阶段冻结预训练权重，训练适配器生成流畅且临床可信的报告。

Result: 在公开数据集上，BoxMed-RL 在 METEOR 和 ROUGE-L 指标上平均提升 7%，在大语言模型指标上平均提升 5%。

Conclusion: BoxMed-RL 通过模仿放射科医生的工作流程，显著提升了放射学报告的质量和可解释性。

Abstract: Radiology report generation is critical for efficiency but current models
lack the structured reasoning of experts, hindering clinical trust and
explainability by failing to link visual findings to precise anatomical
locations. This paper introduces BoxMed-RL, a groundbreaking unified training
framework for generating spatially verifiable and explainable radiology
reports. Built on a large vision-language model, BoxMed-RL revolutionizes
report generation through two integrated phases: (1) In the Pretraining Phase,
we refine the model via medical concept learning, using Chain-of-Thought
supervision to internalize the radiologist-like workflow, followed by spatially
verifiable reinforcement, which applies reinforcement learning to align medical
findings with bounding boxes. (2) In the Downstream Adapter Phase, we freeze
the pretrained weights and train a downstream adapter to ensure fluent and
clinically credible reports. This framework precisely mimics radiologists'
workflow, compelling the model to connect high-level medical concepts with
definitive anatomical evidence. Extensive experiments on public datasets
demonstrate that BoxMed-RL achieves an average 7% improvement in both METEOR
and ROUGE-L metrics compared to state-of-the-art methods. An average 5%
improvement in large language model-based metrics further underscores
BoxMed-RL's robustness in generating high-quality radiology reports.

</details>

### [110] [Scaling Laws For Scalable Oversight](https://arxiv.org/abs/2504.18530)
*Joshua Engels,David D. Baek,Subhash Kantamneni,Max Tegmark*

Main category: cs.AI

TLDR: 论文提出一个量化监督成功概率的框架，研究能力不匹配的AI系统间的监督问题，并通过游戏验证框架，探讨嵌套可扩展监督（NSO）的成功条件。


<details>
  <summary>Details</summary>
Motivation: 解决可扩展监督（scalable oversight）如何随AI能力变化的问题，为控制超级智能系统提供理论支持。

Method: 提出基于能力不匹配玩家的游戏框架，量化监督成功概率，并通过Nim游戏和四种监督游戏验证。

Result: 发现监督成功率随AI能力差距增大而下降，NSO在监督强400 Elo点的系统时成功率低于52%。

Conclusion: 可扩展监督的成功率受能力差距限制，NSO在监督极强系统时效果有限。

Abstract: Scalable oversight, the process by which weaker AI systems supervise stronger
ones, has been proposed as a key strategy to control future superintelligent
systems. However, it is still unclear how scalable oversight itself scales. To
address this gap, we propose a framework that quantifies the probability of
successful oversight as a function of the capabilities of the overseer and the
system being overseen. Specifically, our framework models oversight as a game
between capability-mismatched players; the players have oversight-specific and
deception-specific Elo scores that are a piecewise-linear function of their
general intelligence, with two plateaus corresponding to task incompetence and
task saturation. We validate our framework with a modified version of the game
Nim and then apply it to four oversight games: "Mafia", "Debate", "Backdoor
Code" and "Wargames". For each game, we find scaling laws that approximate how
domain performance depends on general AI system capability (using Chatbot Arena
Elo as a proxy for general capability). We then build on our findings in a
theoretical study of Nested Scalable Oversight (NSO), a process in which
trusted models oversee untrusted stronger models, which then become the trusted
models in the next step. We identify conditions under which NSO succeeds and
derive numerically (and in some cases analytically) the optimal number of
oversight levels to maximize the probability of oversight success. In our
numerical examples, the NSO success rate is below 52% when overseeing systems
that are 400 Elo points stronger than the baseline overseer, and it declines
further for overseeing even stronger systems.

</details>

### [111] [Adapting Probabilistic Risk Assessment for AI](https://arxiv.org/abs/2504.18536)
*Anna Katariina Wisakanto,Joe Rogero,Avyay M. Casheekar,Richard Mallah*

Main category: cs.AI

TLDR: 该论文提出了一个基于概率风险评估（PRA）的AI风险分析框架，借鉴高可靠性行业的成熟方法，系统化评估AI系统的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 现代通用AI系统的快速发展和潜在危害超出了现有风险评估方法的能力，亟需一种系统化的评估框架。

Method: 采用概率风险评估（PRA）方法，结合面向方面的危害分析、风险路径建模和不确定性管理技术。

Result: 开发了一个工具，生成风险报告卡，汇总所有评估风险的概率和严重性。

Conclusion: 该框架为AI开发者、评估者和监管者提供了一种系统化、可量化的风险评估方法。

Abstract: Modern general-purpose artificial intelligence (AI) systems present an urgent
risk management challenge, as their rapidly evolving capabilities and potential
for catastrophic harm outpace our ability to reliably assess their risks.
Current methods often rely on selective testing and undocumented assumptions
about risk priorities, frequently failing to make a serious attempt at
assessing the set of pathways through which Al systems pose direct or indirect
risks to society and the biosphere. This paper introduces the probabilistic
risk assessment (PRA) for AI framework, adapting established PRA techniques
from high-reliability industries (e.g., nuclear power, aerospace) for the new
challenges of advanced AI. The framework guides assessors in identifying
potential risks, estimating likelihood and severity, and explicitly documenting
evidence, underlying assumptions, and analyses at appropriate granularities.
The framework's implementation tool synthesizes the results into a risk report
card with aggregated risk estimates from all assessed risks. This systematic
approach integrates three advances: (1) Aspect-oriented hazard analysis
provides systematic hazard coverage guided by a first-principles taxonomy of AI
system aspects (e.g. capabilities, domain knowledge, affordances); (2) Risk
pathway modeling analyzes causal chains from system aspects to societal impacts
using bidirectional analysis and incorporating prospective techniques; and (3)
Uncertainty management employs scenario decomposition, reference scales, and
explicit tracing protocols to structure credible projections with novelty or
limited data. Additionally, the framework harmonizes diverse assessment methods
by integrating evidence into comparable, quantified absolute risk estimates for
critical decisions. We have implemented this as a workbook tool for AI
developers, evaluators, and regulators, available on the project website.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [112] [Spectral Dictionary Learning for Generative Image Modeling](https://arxiv.org/abs/2504.17804)
*Andrew Kiruluta*

Main category: cs.CV

TLDR: 提出了一种基于频谱生成模型的新型图像合成方法，通过学习的频谱基函数重构图像，具有高解释性和物理意义。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型（如变分、对抗和扩散模型）依赖随机推断或对抗训练，缺乏解释性。本文旨在提供一种确定性、可解释的图像生成方法。

Method: 将图像展平为一维信号，用学习的频谱基函数（参数化为频率、相位和振幅）线性组合重构图像，并结合概率模型生成新图像。使用短时傅里叶变换（STFT）计算频域损失。

Result: 在CIFAR-10基准测试中，模型在重建质量和感知保真度上表现优异，同时训练稳定性和计算效率更高。

Conclusion: 该方法为图像合成提供了新的可控生成途径，频谱字典的直接操作增强了图像频率内容的解释性，适用于图像操纵和分析的新应用。

Abstract: We propose a novel spectral generative model for image synthesis that departs
radically from the common variational, adversarial, and diffusion paradigms. In
our approach, images, after being flattened into one-dimensional signals, are
reconstructed as linear combinations of a set of learned spectral basis
functions, where each basis is explicitly parameterized in terms of frequency,
phase, and amplitude. The model jointly learns a global spectral dictionary
with time-varying modulations and per-image mixing coefficients that quantify
the contributions of each spectral component. Subsequently, a simple
probabilistic model is fitted to these mixing coefficients, enabling the
deterministic generation of new images by sampling from the latent space. This
framework leverages deterministic dictionary learning, offering a highly
interpretable and physically meaningful representation compared to methods
relying on stochastic inference or adversarial training. Moreover, the
incorporation of frequency-domain loss functions, computed via the short-time
Fourier transform (STFT), ensures that the synthesized images capture both
global structure and fine-grained spectral details, such as texture and edge
information. Experimental evaluations on the CIFAR-10 benchmark demonstrate
that our approach not only achieves competitive performance in terms of
reconstruction quality and perceptual fidelity but also offers improved
training stability and computational efficiency. This new type of generative
model opens up promising avenues for controlled synthesis, as the learned
spectral dictionary affords a direct handle on the intrinsic frequency content
of the images, thus providing enhanced interpretability and potential for novel
applications in image manipulation and analysis.

</details>

### [113] [SmallGS: Gaussian Splatting-based Camera Pose Estimation for Small-Baseline Videos](https://arxiv.org/abs/2504.17810)
*Yuxin Yao,Yan Zhang,Zhening Huang,Joan Lasenby*

Main category: cs.CV

TLDR: SmallGS是一种针对小基线视频的相机姿态估计框架，利用高斯泼溅技术优化相机姿态，结合预训练视觉特征提升鲁棒性，在动态场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 小基线视频在社交媒体中普遍存在，但现有姿态估计框架因特征模糊、漂移累积和三角约束不足而难以处理。

Method: SmallGS通过高斯泼溅重建场景，结合DINOv2等预训练特征，优化相机姿态，无需显式特征对应或强视差运动。

Result: 在TUM-Dynamics序列中，SmallGS在小基线视频的相机姿态估计上优于MonST3R和DORID-SLAM。

Conclusion: SmallGS为小基线视频提供了一种高效、鲁棒的相机姿态估计解决方案。

Abstract: Dynamic videos with small baseline motions are ubiquitous in daily life,
especially on social media. However, these videos present a challenge to
existing pose estimation frameworks due to ambiguous features, drift
accumulation, and insufficient triangulation constraints. Gaussian splatting,
which maintains an explicit representation for scenes, provides a reliable
novel view rasterization when the viewpoint change is small. Inspired by this,
we propose SmallGS, a camera pose estimation framework that is specifically
designed for small-baseline videos. SmallGS optimizes sequential camera poses
using Gaussian splatting, which reconstructs the scene from the first frame in
each video segment to provide a stable reference for the rest. The temporal
consistency of Gaussian splatting within limited viewpoint differences reduced
the requirement of sufficient depth variations in traditional camera pose
estimation. We further incorporate pretrained robust visual features, e.g.
DINOv2, into Gaussian splatting, where high-dimensional feature map rendering
enhances the robustness of camera pose estimation. By freezing the Gaussian
splatting and optimizing camera viewpoints based on rasterized features,
SmallGS effectively learns camera poses without requiring explicit feature
correspondences or strong parallax motion. We verify the effectiveness of
SmallGS in small-baseline videos in TUM-Dynamics sequences, which achieves
impressive accuracy in camera pose estimation compared to MonST3R and
DORID-SLAM for small-baseline videos in dynamic scenes. Our project page is at:
https://yuxinyao620.github.io/SmallGS

</details>

### [114] [Object Learning and Robust 3D Reconstruction](https://arxiv.org/abs/2504.17812)
*Sara Sabour*

Main category: cs.CV

TLDR: 论文探讨了无监督神经网络的架构设计和训练方法，用于图像中目标分割，并扩展到3D场景中的动态目标检测与移除。


<details>
  <summary>Details</summary>
Motivation: 研究无监督方法在计算机视觉中目标分割的应用，探索无需监督定义目标或前景的可能性。

Method: FlowCapsules利用运动作为2D场景中目标的线索；3D任务中利用几何一致性检测动态目标。

Result: 设计了瞬态目标掩码，用于优化3D建模，展示了无监督目标方法的优势。

Conclusion: 论文展示了无监督目标方法的潜力，并提出了未来研究方向，鼓励社区进一步探索显式目标表示。

Abstract: In this thesis we discuss architectural designs and training methods for a
neural network to have the ability of dissecting an image into objects of
interest without supervision. The main challenge in 2D unsupervised object
segmentation is distinguishing between foreground objects of interest and
background. FlowCapsules uses motion as a cue for the objects of interest in 2D
scenarios. The last part of this thesis focuses on 3D applications where the
goal is detecting and removal of the object of interest from the input images.
In these tasks, we leverage the geometric consistency of scenes in 3D to detect
the inconsistent dynamic objects. Our transient object masks are then used for
designing robust optimization kernels to improve 3D modelling in a casual
capture setup. One of our goals in this thesis is to show the merits of
unsupervised object based approaches in computer vision. Furthermore, we
suggest possible directions for defining objects of interest or foreground
objects without requiring supervision. Our hope is to motivate and excite the
community into further exploring explicit object representations in image
understanding tasks.

</details>

### [115] [CLOC: Contrastive Learning for Ordinal Classification with Multi-Margin N-pair Loss](https://arxiv.org/abs/2504.17813)
*Dileepa Pitawela,Gustavo Carneiro,Hsiang-Ting Chen*

Main category: cs.CV

TLDR: CLOC是一种新的基于边界的对比学习方法，用于有序分类，通过优化多边界损失（MMNP）学习有序表示，解决了现有方法未考虑不同类别边界重要性差异的问题。


<details>
  <summary>Details</summary>
Motivation: 在有序分类中，相邻类别的误分类后果不同，但现有方法未考虑这种差异，CLOC旨在解决这一问题。

Method: CLOC使用多边界n对损失（MMNP）学习有序表示，支持关键相邻类别间的灵活决策边界。

Result: 在五个真实图像数据集和一个合成数据集上，CLOC优于现有有序分类方法，并展示了其可解释性和可控性。

Conclusion: CLOC通过学习与临床需求一致的有序表示，提升了有序分类的性能和实用性。

Abstract: In ordinal classification, misclassifying neighboring ranks is common, yet
the consequences of these errors are not the same. For example, misclassifying
benign tumor categories is less consequential, compared to an error at the
pre-cancerous to cancerous threshold, which could profoundly influence
treatment choices. Despite this, existing ordinal classification methods do not
account for the varying importance of these margins, treating all neighboring
classes as equally significant. To address this limitation, we propose CLOC, a
new margin-based contrastive learning method for ordinal classification that
learns an ordered representation based on the optimization of multiple margins
with a novel multi-margin n-pair loss (MMNP). CLOC enables flexible decision
boundaries across key adjacent categories, facilitating smooth transitions
between classes and reducing the risk of overfitting to biases present in the
training data. We provide empirical discussion regarding the properties of MMNP
and show experimental results on five real-world image datasets (Adience,
Historical Colour Image Dating, Knee Osteoarthritis, Indian Diabetic
Retinopathy Image, and Breast Carcinoma Subtyping) and one synthetic dataset
simulating clinical decision bias. Our results demonstrate that CLOC
outperforms existing ordinal classification methods and show the
interpretability and controllability of CLOC in learning meaningful, ordered
representations that align with clinical and practical needs.

</details>

### [116] [Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene Conceptional Learning](https://arxiv.org/abs/2504.17815)
*Mingxuan Cui,Qing Guo,Yuyi Wang,Hongkai Yu,Di Lin,Qin Zou,Ming-Ming Cheng,Xi Li*

Main category: cs.CV

TLDR: 本文提出了一种基于3D高斯泼溅（3DGS）的3D修复方法（3DGI），通过可见性不确定性引导和场景概念学习，实现了高质量的无缝修复。


<details>
  <summary>Details</summary>
Motivation: 解决3D修复中如何有效利用多视角互补视觉和语义信息的挑战。

Method: 提出VISTA框架，结合可见性不确定性引导的3D修复和场景概念学习，并使用扩散模型填充掩码区域。

Result: 在SPIn-NeRF和UTB180数据集上表现优于现有技术，支持动态干扰物处理。

Conclusion: VISTA框架在3D修复中实现了高质量、自然的修复效果，并具有广泛适用性。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful and efficient 3D
representation for novel view synthesis. This paper extends 3DGS capabilities
to inpainting, where masked objects in a scene are replaced with new contents
that blend seamlessly with the surroundings. Unlike 2D image inpainting, 3D
Gaussian inpainting (3DGI) is challenging in effectively leveraging
complementary visual and semantic cues from multiple input views, as occluded
areas in one view may be visible in others. To address this, we propose a
method that measures the visibility uncertainties of 3D points across different
input views and uses them to guide 3DGI in utilizing complementary visual cues.
We also employ uncertainties to learn a semantic concept of scene without the
masked object and use a diffusion model to fill masked objects in input images
based on the learned concept. Finally, we build a novel 3DGI framework, VISTA,
by integrating VISibility-uncerTainty-guided 3DGI with scene conceptuAl
learning. VISTA generates high-quality 3DGS models capable of synthesizing
artifact-free and naturally inpainted novel views. Furthermore, our approach
extends to handling dynamic distractors arising from temporal object changes,
enhancing its versatility in diverse scene reconstruction scenarios. We
demonstrate the superior performance of our method over state-of-the-art
techniques using two challenging datasets: the SPIn-NeRF dataset, featuring 10
diverse static 3D inpainting scenes, and an underwater 3D inpainting dataset
derived from UTB180, including fast-moving fish as inpainting targets.

</details>

### [117] [Subject-driven Video Generation via Disentangled Identity and Motion](https://arxiv.org/abs/2504.17816)
*Daneul Kim,Jingxu Zhang,Wonjoon Jin,Sunghyun Cho,Qi Dai,Jaesik Park,Chong Luo*

Main category: cs.CV

TLDR: 提出一种零样本、无需调优的主题驱动视频生成模型，通过分离主题学习和时间动态，利用图像定制数据集直接训练视频模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖大量标注视频数据，计算成本高且需大量标注。本文旨在通过图像数据集直接训练视频模型，降低成本并提高效率。

Method: 1. 通过图像定制数据集注入主题；2. 利用少量未标注视频通过图像到视频训练方法保留时间动态；3. 采用随机图像标记丢弃和初始化缓解复制粘贴问题；4. 引入随机切换优化主题和时间特征。

Result: 模型在零样本设置下表现优异，主题一致性和可扩展性优于现有视频定制模型。

Conclusion: 提出的框架有效解决了视频定制中的计算和标注问题，展示了零样本学习的潜力。

Abstract: We propose to train a subject-driven customized video generation model
through decoupling the subject-specific learning from temporal dynamics in
zero-shot without additional tuning. A traditional method for video
customization that is tuning-free often relies on large, annotated video
datasets, which are computationally expensive and require extensive annotation.
In contrast to the previous approach, we introduce the use of an image
customization dataset directly on training video customization models,
factorizing the video customization into two folds: (1) identity injection
through image customization dataset and (2) temporal modeling preservation with
a small set of unannotated videos through the image-to-video training method.
Additionally, we employ random image token dropping with randomized image
initialization during image-to-video fine-tuning to mitigate the copy-and-paste
issue. To further enhance learning, we introduce stochastic switching during
joint optimization of subject-specific and temporal features, mitigating
catastrophic forgetting. Our method achieves strong subject consistency and
scalability, outperforming existing video customization models in zero-shot
settings, demonstrating the effectiveness of our framework.

</details>

### [118] [Learning Underwater Active Perception in Simulation](https://arxiv.org/abs/2504.17817)
*Alexandre Cardaillac,Donald G. Dansereau*

Main category: cs.CV

TLDR: 提出一种基于多层感知机（MLP）的主动感知框架，用于在水下不同条件下获取高质量图像，显著提升视觉覆盖和图像质量。


<details>
  <summary>Details</summary>
Motivation: 水下资产自主检测中，水质条件（如浑浊度和后向散射）对机器人操作和图像质量有重大影响，传统方法存在机动和设置限制。

Method: 使用合成数据集（包含10种水质类型）训练MLP，预测图像质量（基于目标距离和人工光强度），并改进Blender软件以模拟水下光传播特性。

Result: 仿真验证表明，相比传统方法，该方法在视觉覆盖和图像质量上有显著提升。

Conclusion: 该方法简单高效，适用于广泛水质条件下的高质量图像采集，代码已开源。

Abstract: When employing underwater vehicles for the autonomous inspection of assets,
it is crucial to consider and assess the water conditions. Indeed, they have a
significant impact on the visibility, which also affects robotic operations.
Turbidity can jeopardise the whole mission as it may prevent correct visual
documentation of the inspected structures. Previous works have introduced
methods to adapt to turbidity and backscattering, however, they also include
manoeuvring and setup constraints. We propose a simple yet efficient approach
to enable high-quality image acquisition of assets in a broad range of water
conditions. This active perception framework includes a multi-layer perceptron
(MLP) trained to predict image quality given a distance to a target and
artificial light intensity. We generated a large synthetic dataset including
ten water types with different levels of turbidity and backscattering. For
this, we modified the modelling software Blender to better account for the
underwater light propagation properties. We validated the approach in
simulation and showed significant improvements in visual coverage and quality
of imagery compared to traditional approaches. The project code is available on
our project page at https://roboticimaging.org/Projects/ActiveUW/.

</details>

### [119] [VideoVista-CulturalLingo: 360$^\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension](https://arxiv.org/abs/2504.17821)
*Xinyu Chen,Yunxin Li,Haoyuan Shi,Baotian Hu,Wenhan Luo,Yaowei Wang,Min Zhang*

Main category: cs.CV

TLDR: VideoVista-CulturalLingo是一个跨文化、多语言的视频理解评估基准，填补了现有基准的不足，并评估了24种视频大模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频评估基准多局限于单一语言和文化背景，无法全面衡量AI系统的跨文化理解能力。

Method: 构建包含1,389个视频和3,134个QA对的基准，涵盖中、美、欧文化，支持中英双语，并评估24种视频大模型。

Result: 现有模型在中文问题（尤其是历史类）表现较差；开源模型在时间理解任务中表现不佳（最高45.2%）；主流模型在科学问题中表现优异，开源模型在数学中较弱。

Conclusion: VideoVista-CulturalLingo为跨文化视频理解提供了新基准，揭示了现有模型的局限性，尤其是语言和文化偏见问题。

Abstract: Assessing the video comprehension capabilities of multimodal AI systems can
effectively measure their understanding and reasoning abilities. Most video
evaluation benchmarks are limited to a single language, typically English, and
predominantly feature videos rooted in Western cultural contexts. In this
paper, we present VideoVista-CulturalLingo, the first video evaluation
benchmark designed to bridge cultural, linguistic, and domain divide in video
comprehension. Our work differs from existing benchmarks in the following ways:
1) Cultural diversity, incorporating cultures from China, North America, and
Europe; 2) Multi-linguistics, with questions presented in Chinese and
English-two of the most widely spoken languages; and 3) Broad domain, featuring
videos sourced from hundreds of human-created domains. VideoVista-CulturalLingo
contains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent
open-source or proprietary video large models. From the experiment results, we
observe that: 1) Existing models perform worse on Chinese-centric questions
than Western-centric ones, particularly those related to Chinese history; 2)
Current open-source models still exhibit limitations in temporal understanding,
especially in the Event Localization task, achieving a maximum score of only
45.2%; 3) Mainstream models demonstrate strong performance in general
scientific questions, while open-source models demonstrate weak performance in
mathematics.

</details>

### [120] [A multi-scale vision transformer-based multimodal GeoAI model for mapping Arctic permafrost thaw](https://arxiv.org/abs/2504.17822)
*Wenwen Li,Chia-Yu Hsu,Sizhe Wang,Zhining Gu,Yili Yang,Brendan M. Rogers,Anna Liljedahl*

Main category: cs.CV

TLDR: 本文提出了一种基于Cascade Mask R-CNN和多尺度视觉Transformer的深度学习方法，用于精确检测北极地区的Retrogressive Thaw Slumps（RTS），并引入了两种新策略优化多模态学习。


<details>
  <summary>Details</summary>
Motivation: RTS是北极地区重要的永久冻土地貌，其出现是永久冻土融化的明显标志。然而，其小尺度、模糊边界和时空变化给准确检测带来挑战。

Method: 采用Cascade Mask R-CNN结合多尺度视觉Transformer，提出特征级残差跨模态注意力融合策略和预训练单模态学习后多模态微调策略。

Result: 实验表明，该方法在RTS检测中优于现有模型，为多模态数据的高效利用提供了新思路。

Conclusion: 该研究不仅提升了RTS检测的准确性，还为理解永久冻土地貌及其环境影响提供了重要参考。

Abstract: Retrogressive Thaw Slumps (RTS) in Arctic regions are distinct permafrost
landforms with significant environmental impacts. Mapping these RTS is crucial
because their appearance serves as a clear indication of permafrost thaw.
However, their small scale compared to other landform features, vague
boundaries, and spatiotemporal variation pose significant challenges for
accurate detection. In this paper, we employed a state-of-the-art deep learning
model, the Cascade Mask R-CNN with a multi-scale vision transformer-based
backbone, to delineate RTS features across the Arctic. Two new strategies were
introduced to optimize multimodal learning and enhance the model's predictive
performance: (1) a feature-level, residual cross-modality attention fusion
strategy, which effectively integrates feature maps from multiple modalities to
capture complementary information and improve the model's ability to understand
complex patterns and relationships within the data; (2) pre-trained unimodal
learning followed by multimodal fine-tuning to alleviate high computing demand
while achieving strong model performance. Experimental results demonstrated
that our approach outperformed existing models adopting data-level fusion,
feature-level convolutional fusion, and various attention fusion strategies,
providing valuable insights into the efficient utilization of multimodal data
for RTS mapping. This research contributes to our understanding of permafrost
landforms and their environmental implications.

</details>

### [121] [Dual Prompting Image Restoration with Diffusion Transformers](https://arxiv.org/abs/2504.17825)
*Dehong Kong,Fan Li,Zhixin Wang,Jiaqi Xu,Renjing Pei,Wenbo Li,WenQi Ren*

Main category: cs.CV

TLDR: DPIR是一种新型图像修复方法，通过双提示控制分支和多视角条件信息提取，显著提升了修复质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如基于U-Net的扩散模型）在图像修复中能力有限，而扩散变换器（DiTs）因其更好的质量和可扩展性成为有前景的替代方案。

Method: DPIR包含两个分支：低质量图像条件分支和双提示控制分支。前者高效整合图像先验，后者通过全局-局部视觉提示补充文本提示，增强修复效果。

Result: 实验表明，DPIR在图像修复中表现优异。

Conclusion: DPIR通过双提示和多视角条件提取，显著提升了图像修复质量，为未来研究提供了新方向。

Abstract: Recent state-of-the-art image restoration methods mostly adopt latent
diffusion models with U-Net backbones, yet still facing challenges in achieving
high-quality restoration due to their limited capabilities. Diffusion
transformers (DiTs), like SD3, are emerging as a promising alternative because
of their better quality with scalability. In this paper, we introduce DPIR
(Dual Prompting Image Restoration), a novel image restoration method that
effectivly extracts conditional information of low-quality images from multiple
perspectives. Specifically, DPIR consits of two branches: a low-quality image
conditioning branch and a dual prompting control branch. The first branch
utilizes a lightweight module to incorporate image priors into the DiT with
high efficiency. More importantly, we believe that in image restoration,
textual description alone cannot fully capture its rich visual characteristics.
Therefore, a dual prompting module is designed to provide DiT with additional
visual cues, capturing both global context and local appearance. The extracted
global-local visual prompts as extra conditional control, alongside textual
prompts to form dual prompts, greatly enhance the quality of the restoration.
Extensive experimental results demonstrate that DPIR delivers superior image
restoration performance.

</details>

### [122] [FashionM3: Multimodal, Multitask, and Multiround Fashion Assistant based on Unified Vision-Language Model](https://arxiv.org/abs/2504.17826)
*Kaicheng Pang,Xingxing Zou,Waikeung Wong*

Main category: cs.CV

TLDR: FashionM3是一个基于视觉语言模型的多模态、多任务、多轮时尚助手，通过个性化推荐、替代建议、产品图像生成和虚拟试穿等功能提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 现代零售中时尚造型和个性化推荐具有重要经济价值，视觉语言模型的出现为零售业提供了新的机会。

Method: 基于视觉语言模型微调，构建FashionM3，并在FashionRec数据集（331,124个多模态对话样本）上进行训练。

Result: 定量和定性评估及用户研究表明，FashionM3在推荐效果和实用价值上表现优异。

Conclusion: FashionM3通过多轮交互提供个性化建议，展示了其在时尚助手领域的潜力。

Abstract: Fashion styling and personalized recommendations are pivotal in modern
retail, contributing substantial economic value in the fashion industry. With
the advent of vision-language models (VLM), new opportunities have emerged to
enhance retailing through natural language and visual interactions. This work
proposes FashionM3, a multimodal, multitask, and multiround fashion assistant,
built upon a VLM fine-tuned for fashion-specific tasks. It helps users discover
satisfying outfits by offering multiple capabilities including personalized
recommendation, alternative suggestion, product image generation, and virtual
try-on simulation. Fine-tuned on the novel FashionRec dataset, comprising
331,124 multimodal dialogue samples across basic, personalized, and alternative
recommendation tasks, FashionM3 delivers contextually personalized suggestions
with iterative refinement through multiround interactions. Quantitative and
qualitative evaluations, alongside user studies, demonstrate FashionM3's
superior performance in recommendation effectiveness and practical value as a
fashion assistant.

</details>

### [123] [VEU-Bench: Towards Comprehensive Understanding of Video Editing](https://arxiv.org/abs/2504.17828)
*Bozheng Li,Yongliang Wu,Yi Lu,Jiashuo Yu,Licheng Tang,Jiawang Cao,Wenqing Zhu,Yuyang Sun,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TLDR: 论文提出了VEU-Bench，一个用于评估视频编辑理解能力的综合基准，发现当前Vid-LLMs在此任务上表现不佳，并开发了专家模型Oscars以提升性能。


<details>
  <summary>Details</summary>
Motivation: 互联网上广泛传播的视频常经过编辑，但现有Vid-LLMs在视频编辑理解（VEU）任务上的能力尚未被探索。

Method: 构建VEU-Bench基准，包含19个细粒度任务，并通过基于知识库的标注流程自动增强标注。开发专家模型Oscars。

Result: 当前Vid-LLMs在VEU任务上表现差，Oscars模型在VEU-Bench上准确率提升28.3%，性能接近GPT-4o。VEU数据还能提升通用视频理解任务性能8.3%。

Conclusion: VEU-Bench填补了研究空白，Oscars模型显著提升了VEU任务性能，同时证明了VEU数据对通用任务的积极影响。

Abstract: Widely shared videos on the internet are often edited. Recently, although
Video Large Language Models (Vid-LLMs) have made great progress in general
video understanding tasks, their capabilities in video editing understanding
(VEU) tasks remain unexplored. To address this gap, in this paper, we introduce
VEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark
that categorizes video editing components across various dimensions, from
intra-frame features like shot size to inter-shot attributes such as cut types
and transitions. Unlike previous video editing understanding benchmarks that
focus mainly on editing element classification, VEU-Bench encompasses 19
fine-grained tasks across three stages: recognition, reasoning, and judging. To
enhance the annotation of VEU automatically, we built an annotation pipeline
integrated with an ontology-based knowledge base. Through extensive experiments
with 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs
face significant challenges in VEU tasks, with some performing worse than
random choice. To alleviate this issue, we develop Oscars, a VEU expert model
fine-tuned on the curated VEU-Bench dataset. It outperforms existing
open-source Vid-LLMs on VEU-Bench by over 28.3% in accuracy and achieves
performance comparable to commercial models like GPT-4o. We also demonstrate
that incorporating VEU data significantly enhances the performance of Vid-LLMs
on general video understanding benchmarks, with an average improvement of 8.3%
across nine reasoning tasks.

</details>

### [124] [Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing](https://arxiv.org/abs/2504.17829)
*Vlad Vasilescu,Ana Neacsu,Daniela Faur*

Main category: cs.CV

TLDR: 论文研究了单图像去雾模型对对抗性噪声的脆弱性，并提出两种轻量级微调策略以提高其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 单图像去雾在遥感应用中很重要，但现有模型对对抗性噪声的脆弱性尚未充分研究。

Method: 提出两种轻量级微调策略，增强预训练Transformer的鲁棒性。

Result: 微调后的模型在保持清洁数据性能的同时，显著提高了对抗性数据的防御能力。

Conclusion: 该方法在遥感场景中表现出对分布外数据的鲁棒性，代码已开源。

Abstract: Single-image dehazing is an important topic in remote sensing applications,
enhancing the quality of acquired images and increasing object detection
precision. However, the reliability of such structures has not been
sufficiently analyzed, which poses them to the risk of imperceptible
perturbations that can significantly hinder their performance. In this work, we
show that state-of-the-art image-to-image dehazing transformers are susceptible
to adversarial noise, with even 1 pixel change being able to decrease the PSNR
by as much as 2.8 dB. Next, we propose two lightweight fine-tuning strategies
aimed at increasing the robustness of pre-trained transformers. Our methods
results in comparable clean performance, while significantly increasing the
protection against adversarial data. We further present their applicability in
two remote sensing scenarios, showcasing their robust behavior for
out-of-distribution data. The source code for adversarial fine-tuning and
attack algorithms can be found at github.com/Vladimirescu/RobustDehazing.

</details>

### [125] [Token Sequence Compression for Efficient Multimodal Computing](https://arxiv.org/abs/2504.17892)
*Yasmine Omri,Parth Shroff,Thierry Tambe*

Main category: cs.CV

TLDR: 该论文提出了一种自适应压缩方法，用于优化视觉语言模型中的冗余和低效问题，通过聚类级标记聚合方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉编码器存在冗余和低效问题，影响了跨模态推理的计算成本。

Method: 通过基准测试和定性分析，研究了多种视觉标记选择和合并方法，提出聚类级标记聚合方法。

Result: 聚类级标记聚合方法在标记选择和合并方面优于现有技术，包括视觉编码器级合并和基于注意力的方法。

Conclusion: 该研究为高维数据的更有效编码和处理提供了初步探索，推动了可扩展和可持续的多模态系统发展。

Abstract: The exponential growth of Large Multimodal Models (LMMs) has driven
advancements in cross-modal reasoning but at significant computational costs.
In this work, we focus on visual language models. We highlight the redundancy
and inefficiency in current vision encoders, and seek to construct an adaptive
compression method for multimodal data. In this work, we characterize a panoply
of visual token selection and merging approaches through both benchmarking and
qualitative analysis. In particular, we demonstrate that simple cluster-level
token aggregation outperforms prior state-of-the-art works in token selection
and merging, including merging at the vision encoder level and attention-based
approaches. We underline the redundancy in current vision encoders, and shed
light on several puzzling trends regarding principles of visual token selection
through cross-modal attention visualizations. This work is a first effort
towards more effective encoding and processing of high-dimensional data, and
paves the way for more scalable and sustainable multimodal systems.

</details>

### [126] [DCT-Shield: A Robust Frequency Domain Defense against Malicious Image Editing](https://arxiv.org/abs/2504.17894)
*Aniruddha Bala,Rohit Chowdhury,Rohan Jaiswal,Siddharth Roheda*

Main category: cs.CV

TLDR: 提出了一种基于频率域的对抗扰动方法，通过修改DCT系数保护图像免受恶意编辑，同时减少视觉伪影。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的发展使得通过文本提示轻松编辑图像成为可能，但也带来了图像安全问题。现有防御方法在像素空间添加噪声容易被察觉且不鲁棒。

Method: 在频率域通过优化修改DCT系数，利用JPEG管道生成对抗图像。

Result: 实验表明，该方法在保护编辑能力的同时减少了视觉伪影，并对噪声净化技术更鲁棒。

Conclusion: 该方法在频率域引入对抗扰动，有效平衡了保护效果与视觉质量。

Abstract: Advancements in diffusion models have enabled effortless image editing via
text prompts, raising concerns about image security. Attackers with access to
user images can exploit these tools for malicious edits. Recent defenses
attempt to protect images by adding a limited noise in the pixel space to
disrupt the functioning of diffusion-based editing models. However, the
adversarial noise added by previous methods is easily noticeable to the human
eye. Moreover, most of these methods are not robust to purification techniques
like JPEG compression under a feasible pixel budget. We propose a novel
optimization approach that introduces adversarial perturbations directly in the
frequency domain by modifying the Discrete Cosine Transform (DCT) coefficients
of the input image. By leveraging the JPEG pipeline, our method generates
adversarial images that effectively prevent malicious image editing. Extensive
experiments across a variety of tasks and datasets demonstrate that our
approach introduces fewer visual artifacts while maintaining similar levels of
edit protection and robustness to noise purification techniques.

</details>

### [127] [CAMU: Context Augmentation for Meme Understanding](https://arxiv.org/abs/2504.17902)
*Girish A. Koushik,Diptesh Kanojia,Helen Treharne,Aditya Joshi*

Main category: cs.CV

TLDR: CAMU框架结合视觉语言模型和参数高效微调，显著提升仇恨和冒犯性表情包检测性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体的表情包结合了视觉和文本线索，文化背景复杂，传统方法难以有效检测仇恨内容。

Method: CAMU利用大型视觉语言模型生成描述性标题，通过标题评分网络突出仇恨相关内容，并高效微调CLIP文本编码器。

Result: 在Hateful Memes数据集上达到0.807准确率和0.806 F1分数，MultiOFF数据集上F1分数为0.673。

Conclusion: CAMU在高效性和性能上优于现有方法，适用于实际场景，并公开模型以促进研究。

Abstract: Social media memes are a challenging domain for hate detection because they
intertwine visual and textual cues into culturally nuanced messages. We
introduce a novel framework, CAMU, which leverages large vision-language models
to generate more descriptive captions, a caption-scoring neural network to
emphasise hate-relevant content, and parameter-efficient fine-tuning of CLIP's
text encoder for an improved multimodal understanding of memes. Experiments on
publicly available hateful meme datasets show that simple projection layer
fine-tuning yields modest gains, whereas selectively tuning deeper text encoder
layers significantly boosts performance on all evaluation metrics. Moreover,
our approach attains high accuracy (0.807) and F1-score (0.806) on the Hateful
Memes dataset, at par with the existing SoTA framework while being much more
efficient, offering practical advantages in real-world scenarios that rely on
fixed decision thresholds. CAMU also achieves the best F1-score of 0.673 on the
MultiOFF dataset for offensive meme identification, demonstrating its
generalisability. Additional analyses on benign confounders reveal that robust
visual grounding and nuanced text representations are crucial for reliable hate
and offence detection. We will publicly release CAMU along with the resultant
models for further research.
  Disclaimer: This paper includes references to potentially disturbing,
hateful, or offensive content due to the nature of the task.

</details>

### [128] [Masked strategies for images with small objects](https://arxiv.org/abs/2504.17935)
*H. Martin Gillis,Ming Hill,Paul Hollensen,Alan Fine,Thomas Trappenberg*

Main category: cs.CV

TLDR: 该论文研究了在血液成分分析中，使用掩码自编码器（MAE）和ViT编码器表示的方法，通过调整掩码比例和补丁大小来优化小目标的重建和语义分割效果。


<details>
  <summary>Details</summary>
Motivation: 血液成分检测和分类中的小目标（如像素级实体）在传统深度学习方法中表现不佳，尤其是在目标尺寸小于掩码大小时，全局上下文信息丢失。

Method: 采用MAE学习ViT编码器表示，调整掩码比例和补丁大小，并将预训练权重应用于U-Net Transformer进行语义分割。

Result: 实验表明，较小的掩码比例和补丁大小能改善MAE的图像重建效果，预训练权重对小尺寸血液成分的分割效果更佳。

Conclusion: 该方法为小目标的语义分割和分类提供了一种高效策略。

Abstract: The hematology analytics used for detection and classification of small blood
components is a significant challenge. In particular, when objects exists as
small pixel-sized entities in a large context of similar objects. Deep learning
approaches using supervised models with pre-trained weights, such as residual
networks and vision transformers have demonstrated success for many
applications. Unfortunately, when applied to images outside the domain of
learned representations, these methods often result with less than acceptable
performance. A strategy to overcome this can be achieved by using
self-supervised models, where representations are learned and weights are then
applied for downstream applications. Recently, masked autoencoders have proven
to be effective to obtain representations that captures global context
information. By masking regions of an image and having the model learn to
reconstruct both the masked and non-masked regions, weights can be used for
various applications. However, if the sizes of the objects in images are less
than the size of the mask, the global context information is lost, making it
almost impossible to reconstruct the image. In this study, we investigated the
effect of mask ratios and patch sizes for blood components using a MAE to
obtain learned ViT encoder representations. We then applied the encoder weights
to train a U-Net Transformer for semantic segmentation to obtain both local and
global contextual information. Our experimental results demonstrates that both
smaller mask ratios and patch sizes improve the reconstruction of images using
a MAE. We also show the results of semantic segmentation with and without
pre-trained weights, where smaller-sized blood components benefited with
pre-training. Overall, our proposed method offers an efficient and effective
strategy for the segmentation and classification of small objects.

</details>

### [129] [From Mapping to Composing: A Two-Stage Framework for Zero-shot Composed Image Retrieval](https://arxiv.org/abs/2504.17990)
*Yabing Wang,Zhuotao Tian,Qingpei Guo,Zheng Qin,Sanping Zhou,Ming Yang,Le Wang*

Main category: cs.CV

TLDR: 论文提出了一种两阶段框架解决零样本组合图像检索中的三个关键挑战，通过视觉语义注入模块和软文本对齐目标增强表示能力，并在少量合成数据上优化文本编码器，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 由于组合图像检索（CIR）数据标注成本高，零样本CIR成为有前景的替代方案，但现有投影方法存在表示能力不足、训练与推理阶段不一致及依赖大规模合成数据的问题。

Method: 提出两阶段框架：第一阶段通过视觉语义注入模块和软文本对齐目标增强图像到伪词标记的学习；第二阶段利用少量合成数据优化文本编码器，提取组合语义。

Result: 在三个公开数据集上实验，性能优于现有方法。

Conclusion: 两阶段框架解决了零样本CIR的关键挑战，兼容不同质量合成数据，仅需少量数据即可显著提升性能。

Abstract: Composed Image Retrieval (CIR) is a challenging multimodal task that
retrieves a target image based on a reference image and accompanying
modification text. Due to the high cost of annotating CIR triplet datasets,
zero-shot (ZS) CIR has gained traction as a promising alternative. Existing
studies mainly focus on projection-based methods, which map an image to a
single pseudo-word token. However, these methods face three critical
challenges: (1) insufficient pseudo-word token representation capacity, (2)
discrepancies between training and inference phases, and (3) reliance on
large-scale synthetic data. To address these issues, we propose a two-stage
framework where the training is accomplished from mapping to composing. In the
first stage, we enhance image-to-pseudo-word token learning by introducing a
visual semantic injection module and a soft text alignment objective, enabling
the token to capture richer and fine-grained image information. In the second
stage, we optimize the text encoder using a small amount of synthetic triplet
data, enabling it to effectively extract compositional semantics by combining
pseudo-word tokens with modification text for accurate target image retrieval.
The strong visual-to-pseudo mapping established in the first stage provides a
solid foundation for the second stage, making our approach compatible with both
high- and low-quality synthetic data, and capable of achieving significant
performance gains with only a small amount of synthetic data. Extensive
experiments were conducted on three public datasets, achieving superior
performance compared to existing approaches.

</details>

### [130] [RSRNav: Reasoning Spatial Relationship for Image-Goal Navigation](https://arxiv.org/abs/2504.17991)
*Zheng Qin,Le Wang,Yabing Wang,Sanping Zhou,Gang Hua,Wei Tang*

Main category: cs.CV

TLDR: RSRNav通过建模目标与当前观测的空间关系，提升图像目标导航性能，解决了语义特征方向信息不准确和视角不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，语义特征缺乏准确的方向信息，且训练与应用时的视角不一致导致性能下降。

Method: RSRNav通过构建目标与当前观测的空间相关性，并利用细粒度互相关和方向感知相关性逐步优化，指导导航策略。

Result: 在三个基准数据集上，RSRNav表现出优越的导航性能，尤其在“用户匹配目标”场景中。

Conclusion: RSRNav通过空间关系建模，显著提升了图像目标导航的准确性和实用性。

Abstract: Recent image-goal navigation (ImageNav) methods learn a perception-action
policy by separately capturing semantic features of the goal and egocentric
images, then passing them to a policy network. However, challenges remain: (1)
Semantic features often fail to provide accurate directional information,
leading to superfluous actions, and (2) performance drops significantly when
viewpoint inconsistencies arise between training and application. To address
these challenges, we propose RSRNav, a simple yet effective method that reasons
spatial relationships between the goal and current observations as navigation
guidance. Specifically, we model the spatial relationship by constructing
correlations between the goal and current observations, which are then passed
to the policy network for action prediction. These correlations are
progressively refined using fine-grained cross-correlation and direction-aware
correlation for more precise navigation. Extensive evaluation of RSRNav on
three benchmark datasets demonstrates superior navigation performance,
particularly in the "user-matched goal" setting, highlighting its potential for
real-world applications.

</details>

### [131] [Back to Fundamentals: Low-Level Visual Features Guided Progressive Token Pruning](https://arxiv.org/abs/2504.17996)
*Yuanbing Ouyang,Yizhuo Liang,Qingpeng Li,Xinfei Guo,Yiming Luo,Di Wu,Hao Wang,Yushan Pan*

Main category: cs.CV

TLDR: LVTP是一种基于多尺度Tsallis熵和低层视觉特征的渐进式令牌剪枝框架，显著降低计算成本且性能损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 解决Vision Transformers在语义分割中计算成本高的问题，同时保留关键边缘信息。

Method: 结合多尺度Tsallis熵和低层视觉特征，通过动态评分机制进行令牌剪枝。

Result: 在多个数据集上实现20%-45%的计算成本降低，性能损失极小。

Conclusion: LVTP在计算成本和准确性之间取得了更好的平衡，尤其在复杂边缘区域表现优异。

Abstract: Vision Transformers (ViTs) excel in semantic segmentation but demand
significant computation, posing challenges for deployment on
resource-constrained devices. Existing token pruning methods often overlook
fundamental visual data characteristics. This study introduces 'LVTP', a
progressive token pruning framework guided by multi-scale Tsallis entropy and
low-level visual features with twice clustering. It integrates high-level
semantics and basic visual attributes for precise segmentation. A novel dynamic
scoring mechanism using multi-scale Tsallis entropy weighting overcomes
limitations of traditional single-parameter entropy. The framework also
incorporates low-level feature analysis to preserve critical edge information
while optimizing computational cost. As a plug-and-play module, it requires no
architectural changes or additional training. Evaluations across multiple
datasets show 20%-45% computational reductions with negligible performance
loss, outperforming existing methods in balancing cost and accuracy, especially
in complex edge regions.

</details>

### [132] [TSCL:Multi-party loss Balancing scheme for deep learning Image steganography based on Curriculum learning](https://arxiv.org/abs/2504.18348)
*Fengchun Liu. Tong Zhang,Chunying Zhang*

Main category: cs.CV

TLDR: 提出了一种两阶段课程学习损失调度器（TSCL），用于平衡深度学习图像隐写算法中的多项损失，提升隐写质量、解码准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统方法中固定损失权重无法适应隐写任务的重要性和训练过程，需动态调整以优化性能。

Method: TSCL分为两阶段：先验课程控制和损失动态控制，分别调整损失权重以优化信息嵌入、解码准确性和抗隐写分析能力。

Result: 在ALASKA2、VOC2012和ImageNet数据集上，TSCL显著提升了隐写质量、解码准确性和安全性。

Conclusion: TSCL通过动态调整损失权重，有效优化了深度学习图像隐写算法的性能。

Abstract: For deep learning-based image steganography frameworks, in order to ensure
the invisibility and recoverability of the information embedding, the loss
function usually contains several losses such as embedding loss, recovery loss
and steganalysis loss. In previous research works, fixed loss weights are
usually chosen for training optimization, and this setting is not linked to the
importance of the steganography task itself and the training process. In this
paper, we propose a Two-stage Curriculum Learning loss scheduler (TSCL) for
balancing multinomial losses in deep learning image steganography algorithms.
TSCL consists of two phases: a priori curriculum control and loss dynamics
control. The first phase firstly focuses the model on learning the information
embedding of the original image by controlling the loss weights in the
multi-party adversarial training; secondly, it makes the model shift its
learning focus to improving the decoding accuracy; and finally, it makes the
model learn to generate a steganographic image that is resistant to
steganalysis. In the second stage, the learning speed of each training task is
evaluated by calculating the loss drop of the before and after iteration rounds
to balance the learning of each task. Experimental results on three large
public datasets, ALASKA2, VOC2012 and ImageNet, show that the proposed TSCL
strategy improves the quality of steganography, decoding accuracy and security.

</details>

### [133] [Federated Client-tailored Adapter for Medical Image Segmentation](https://arxiv.org/abs/2504.18020)
*Guyue Hu,Siyuan Song,Yukun Kang,Zhu Yin,Gangming Zhao,Chenglong Li,Jin Tang*

Main category: cs.CV

TLDR: 提出了一种联邦客户端定制适配器（FCA）框架，用于解决医学图像分割中的分布式数据岛屿问题，通过稳定训练和客户端定制化适配实现高效分割。


<details>
  <summary>Details</summary>
Motivation: 现有集中式学习方法不适用于分布式数据岛屿的医学场景，且联邦学习因客户端域异构性（如分布多样性和类别不平衡）导致训练不稳定。

Method: FCA框架利用现成医学基础模型的通用知识稳定训练，并开发两种客户端定制化更新策略，将适配器分解为通用和个体组件，分别更新。

Result: 在三个大规模数据集上的实验表明，FCA框架在联邦医学分割中具有高效性和优越性。

Conclusion: FCA框架通过稳定训练和客户端定制化，实现了无需共享敏感数据的分布式医学图像分割，优于传统方法。

Abstract: Medical image segmentation in X-ray images is beneficial for computer-aided
diagnosis and lesion localization. Existing methods mainly fall into a
centralized learning paradigm, which is inapplicable in the practical medical
scenario that only has access to distributed data islands. Federated Learning
has the potential to offer a distributed solution but struggles with heavy
training instability due to client-wise domain heterogeneity (including
distribution diversity and class imbalance). In this paper, we propose a novel
Federated Client-tailored Adapter (FCA) framework for medical image
segmentation, which achieves stable and client-tailored adaptive segmentation
without sharing sensitive local data. Specifically, the federated adapter stirs
universal knowledge in off-the-shelf medical foundation models to stabilize the
federated training process. In addition, we develop two client-tailored
federated updating strategies that adaptively decompose the adapter into common
and individual components, then globally and independently update the parameter
groups associated with common client-invariant and individual client-specific
units, respectively. They further stabilize the heterogeneous federated
learning process and realize optimal client-tailored instead of sub-optimal
global-compromised segmentation models. Extensive experiments on three
large-scale datasets demonstrate the effectiveness and superiority of the
proposed FCA framework for federated medical segmentation.

</details>

### [134] [Revisiting Data Auditing in Large Vision-Language Models](https://arxiv.org/abs/2504.18349)
*Hongyu Zhu,Sichu Liang,Wenwen Wang,Boheng Li,Tongxin Yuan,Fangqi Li,ShiLin Wang,Zhuosheng Zhang*

Main category: cs.CV

TLDR: 论文探讨了大型视觉语言模型（VLMs）中成员推断（MI）的局限性，揭示了当前基准测试中的分布偏移问题，并提出了一种基于最优传输的度量方法。研究发现，在无偏条件下，现有MI方法表现不佳，但指出了三种可行场景。


<details>
  <summary>Details</summary>
Motivation: 随着VLMs的广泛应用，数据审计的需求日益迫切，但现有MI方法在真实场景中表现不佳，亟需系统性研究其局限性。

Method: 通过分析分布偏移问题，提出基于最优传输的度量方法，并构建新的无偏基准测试。进一步探讨了MI的理论上限和可行场景。

Result: 现有MI方法在无偏条件下表现接近随机猜测，但识别了三种可行场景（微调、真实文本访问和集合推断）。

Conclusion: 研究系统性地揭示了VLMs中MI的局限性，并为未来可信数据审计提供了指导。

Abstract: With the surge of large language models (LLMs), Large Vision-Language Models
(VLMs)--which integrate vision encoders with LLMs for accurate visual
grounding--have shown great potential in tasks like generalist agents and
robotic control. However, VLMs are typically trained on massive web-scraped
images, raising concerns over copyright infringement and privacy violations,
and making data auditing increasingly urgent. Membership inference (MI), which
determines whether a sample was used in training, has emerged as a key auditing
technique, with promising results on open-source VLMs like LLaVA (AUC > 80%).
In this work, we revisit these advances and uncover a critical issue: current
MI benchmarks suffer from distribution shifts between member and non-member
images, introducing shortcut cues that inflate MI performance. We further
analyze the nature of these shifts and propose a principled metric based on
optimal transport to quantify the distribution discrepancy. To evaluate MI in
realistic settings, we construct new benchmarks with i.i.d. member and
non-member images. Existing MI methods fail under these unbiased conditions,
performing only marginally better than chance. Further, we explore the
theoretical upper bound of MI by probing the Bayes Optimality within the VLM's
embedding space and find the irreducible error rate remains high. Despite this
pessimistic outlook, we analyze why MI for VLMs is particularly challenging and
identify three practical scenarios--fine-tuning, access to ground-truth texts,
and set-based inference--where auditing becomes feasible. Our study presents a
systematic view of the limits and opportunities of MI for VLMs, providing
guidance for future efforts in trustworthy data auditing.

</details>

### [135] [ShapeSpeak: Body Shape-Aware Textual Alignment for Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2504.18025)
*Shuanglin Yan,Neng Dong,Shuang Li,Rui Yan,Hao Tang,Jing Qin*

Main category: cs.CV

TLDR: 提出了一种名为BSaTa的框架，通过显式建模身体形状信息来提升可见光-红外行人重识别（VIReID）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖身份标签监督，难以充分提取高级语义信息，且未显式建模身体形状特征。

Method: 设计了BSTA模块提取身体形状信息并转换为文本表示，TVCR确保文本与视觉特征对齐，SRL机制结合多文本监督和分布一致性约束。

Result: 在SYSU-MM01和RegDB数据集上表现优异。

Conclusion: BSaTa框架通过显式建模身体形状信息，显著提升了VIReID的性能。

Abstract: Visible-Infrared Person Re-identification (VIReID) aims to match visible and
infrared pedestrian images, but the modality differences and the complexity of
identity features make it challenging. Existing methods rely solely on identity
label supervision, which makes it difficult to fully extract high-level
semantic information. Recently, vision-language pre-trained models have been
introduced to VIReID, enhancing semantic information modeling by generating
textual descriptions. However, such methods do not explicitly model body shape
features, which are crucial for cross-modal matching. To address this, we
propose an effective Body Shape-aware Textual Alignment (BSaTa) framework that
explicitly models and utilizes body shape information to improve VIReID
performance. Specifically, we design a Body Shape Textual Alignment (BSTA)
module that extracts body shape information using a human parsing model and
converts it into structured text representations via CLIP. We also design a
Text-Visual Consistency Regularizer (TVCR) to ensure alignment between body
shape textual representations and visual body shape features. Furthermore, we
introduce a Shape-aware Representation Learning (SRL) mechanism that combines
Multi-text Supervision and Distribution Consistency Constraints to guide the
visual encoder to learn modality-invariant and discriminative identity
features, thus enhancing modality invariance. Experimental results demonstrate
that our method achieves superior performance on the SYSU-MM01 and RegDB
datasets, validating its effectiveness.

</details>

### [136] [A Large Vision-Language Model based Environment Perception System for Visually Impaired People](https://arxiv.org/abs/2504.18027)
*Zezhou Chen,Zhaoxiang Liu,Kai Wang,Kohou Wang,Shiguo Lian*

Main category: cs.CV

TLDR: 本文提出了一种基于大型视觉语言模型（LVLM）的环境感知系统，帮助视障人士通过可穿戴设备捕捉并分析周围环境，提高环境感知能力。


<details>
  <summary>Details</summary>
Motivation: 视障人士因自然场景的复杂性难以感知周围环境，限制了其个人和社交活动。

Method: 系统结合LVLM和分割模型，通过长按、点击或双击屏幕获取场景全局描述、物体分类及详细描述，并将RGB图像分割结果作为外部知识输入以减少LVLM的幻觉。

Result: 在POPE、MME和LLaVA-QA90上的实验表明，系统比Qwen-VL-Chat提供更准确的场景描述；探索性实验证实其有效帮助视障人士感知环境。

Conclusion: 该系统通过结合LVLM和分割模型，显著提升了视障人士的环境感知能力。

Abstract: It is a challenging task for visually impaired people to perceive their
surrounding environment due to the complexity of the natural scenes. Their
personal and social activities are thus highly limited. This paper introduces a
Large Vision-Language Model(LVLM) based environment perception system which
helps them to better understand the surrounding environment, by capturing the
current scene they face with a wearable device, and then letting them retrieve
the analysis results through the device. The visually impaired people could
acquire a global description of the scene by long pressing the screen to
activate the LVLM output, retrieve the categories of the objects in the scene
resulting from a segmentation model by tapping or swiping the screen, and get a
detailed description of the objects they are interested in by double-tapping
the screen. To help visually impaired people more accurately perceive the
world, this paper proposes incorporating the segmentation result of the RGB
image as external knowledge into the input of LVLM to reduce the LVLM's
hallucination. Technical experiments on POPE, MME and LLaVA-QA90 show that the
system could provide a more accurate description of the scene compared to
Qwen-VL-Chat, exploratory experiments show that the system helps visually
impaired people to perceive the surrounding environment effectively.

</details>

### [137] [Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models](https://arxiv.org/abs/2504.18032)
*Chen Chen,Daochang Liu,Mubarak Shah,Chang Xu*

Main category: cs.CV

TLDR: PRSS方法通过提示重新锚定和语义提示搜索，优化了扩散模型的隐私-效用权衡，显著提升了隐私保护同时保持高文本对齐。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像扩散模型因记忆训练图像导致的隐私和原创性问题，同时避免现有方法对输出效用的显著降低。

Method: 结合提示重新锚定（PR）和语义提示搜索（SS），改进扩散模型中的无分类器引导方法。

Result: 在不同隐私级别下，PRSS方法显著改善了隐私-效用权衡，达到新的最优性能。

Conclusion: PRSS为扩散模型提供了一种有效平衡隐私保护与生成效用的新方法。

Abstract: Text-to-image diffusion models have demonstrated remarkable capabilities in
creating images highly aligned with user prompts, yet their proclivity for
memorizing training set images has sparked concerns about the originality of
the generated images and privacy issues, potentially leading to legal
complications for both model owners and users, particularly when the memorized
images contain proprietary content. Although methods to mitigate these issues
have been suggested, enhancing privacy often results in a significant decrease
in the utility of the outputs, as indicated by text-alignment scores. To bridge
the research gap, we introduce a novel method, PRSS, which refines the
classifier-free guidance approach in diffusion models by integrating prompt
re-anchoring (PR) to improve privacy and incorporating semantic prompt search
(SS) to enhance utility. Extensive experiments across various privacy levels
demonstrate that our approach consistently improves the privacy-utility
trade-off, establishing a new state-of-the-art.

</details>

### [138] [Cabbage: A Differential Growth Framework for Open Surfaces](https://arxiv.org/abs/2504.18040)
*Xiaoyi Liu,Hao Tang*

Main category: cs.CV

TLDR: Cabbage是一个差分生长框架，用于模拟自然中3D开放表面的屈曲行为（如花瓣卷曲），生成高质量无自交的三角网格。


<details>
  <summary>Details</summary>
Motivation: 模拟自然界中复杂表面的屈曲行为，为计算建模、数字制造和教育提供高质量数据。

Method: 通过边缘细分驱动差分增长，结合壳力扩展表面，特征感知平滑和重网格确保网格质量，校正碰撞防止自交。

Result: Cabbage在形态表达、网格质量和稳定性上优于现有方法，能稳定生成大规模复杂模式。

Conclusion: Cabbage是首个开源的高质量框架，适用于计算建模、数字制造和教育，并为几何处理和形状分析提供数据。

Abstract: We propose Cabbage, a differential growth framework to model buckling
behavior in 3D open surfaces found in nature-like the curling of flower petals.
Cabbage creates high-quality triangular meshes free of self-intersection.
Cabbage-Shell is driven by edge subdivision which differentially increases
discretization resolution. Shell forces expands the surface, generating
buckling over time. Feature-aware smoothing and remeshing ensures mesh quality.
Corrective collision effectively prevents self-collision even in tight spaces.
We additionally provide Cabbage-Collision, and approximate alternative,
followed by CAD-ready surface generation. Cabbage is the first open-source
effort with this calibre and robustness, outperforming SOTA methods in its
morphological expressiveness, mesh quality, and stably generates large, complex
patterns over hundreds of simulation steps. It is a source not only of
computational modeling, digital fabrication, education, but also high-quality,
annotated data for geometry processing and shape analysis.

</details>

### [139] [DMS-Net:Dual-Modal Multi-Scale Siamese Network for Binocular Fundus Image Classification](https://arxiv.org/abs/2504.18046)
*Guohao Huo,Zibo Lin,Zitong Wang,Ruiting Dai,Hao Tang*

Main category: cs.CV

TLDR: DMS-Net是一种双模态多尺度Siamese网络，用于双眼眼底图像分类，通过多尺度上下文感知模块和双模态特征融合模块提升性能，在ODIR-5K数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统诊断方法和单眼深度学习模型未能考虑双眼病理相关性，需改进。

Method: 使用权重共享的Siamese ResNet-152提取特征，结合多尺度上下文感知模块和双模态特征融合模块。

Result: 在ODIR-5K数据集上达到80.5%准确率、86.1%召回率和83.8%Cohen's kappa。

Conclusion: DMS-Net能有效检测对称病理，提升眼科疾病的临床决策能力。

Abstract: Ophthalmic diseases pose a significant global health challenge, yet
traditional diagnosis methods and existing single-eye deep learning approaches
often fail to account for binocular pathological correlations. To address this,
we propose DMS-Net, a dual-modal multi-scale Siamese network for binocular
fundus image classification. Our framework leverages weight-shared Siamese
ResNet-152 backbones to extract deep semantic features from paired fundus
images. To tackle challenges such as lesion boundary ambiguity and scattered
pathological distributions, we introduce a Multi-Scale Context-Aware Module
(MSCAM) that integrates adaptive pooling and attention mechanisms for
multi-resolution feature aggregation. Additionally, a Dual-Modal Feature Fusion
(DMFF) module enhances cross-modal interaction through spatial-semantic
recalibration and bidirectional attention, effectively combining global context
and local edge features. Evaluated on the ODIR-5K dataset, DMS-Net achieves
state-of-the-art performance with 80.5% accuracy, 86.1% recall, and 83.8%
Cohen's kappa, demonstrating superior capability in detecting symmetric
pathologies and advancing clinical decision-making for ocular diseases.

</details>

### [140] [A BERT-Style Self-Supervised Learning CNN for Disease Identification from Retinal Images](https://arxiv.org/abs/2504.18049)
*Xin Li,Wenhui Zhu,Peijie Qiu,Oana M. Dumitrascu,Amal Youssef,Yalin Wang*

Main category: cs.CV

TLDR: 该研究提出了一种结合轻量级CNN框架（nn-MobileNet）和自监督学习的方法，用于医学图像分析，解决了标签数据稀缺和高计算需求的问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析中，深度学习依赖大量标注数据，但高质量标签获取成本高且困难。ViT虽能利用无标签数据，但其高计算需求和缺乏局部性限制了应用。

Method: 采用nn-MobileNet框架，结合BERT风格的自监督学习，在无标签的视网膜图像上进行预训练，并在下游任务（如阿尔茨海默病和帕金森病识别）中验证效果。

Result: 预训练模型显著提升了下游任务的性能，证明了在标签稀缺情况下CNN的潜力。

Conclusion: 该研究展示了CNN结合自监督学习在医学图像分析中的优势，为标签稀缺问题提供了有效解决方案。

Abstract: In the field of medical imaging, the advent of deep learning, especially the
application of convolutional neural networks (CNNs) has revolutionized the
analysis and interpretation of medical images. Nevertheless, deep learning
methods usually rely on large amounts of labeled data. In medical imaging
research, the acquisition of high-quality labels is both expensive and
difficult. The introduction of Vision Transformers (ViT) and self-supervised
learning provides a pre-training strategy that utilizes abundant unlabeled
data, effectively alleviating the label acquisition challenge while broadening
the breadth of data utilization. However, ViT's high computational density and
substantial demand for computing power, coupled with the lack of localization
characteristics of its operations on image patches, limit its efficiency and
applicability in many application scenarios. In this study, we employ
nn-MobileNet, a lightweight CNN framework, to implement a BERT-style
self-supervised learning approach. We pre-train the network on the unlabeled
retinal fundus images from the UK Biobank to improve downstream application
performance. We validate the results of the pre-trained model on Alzheimer's
disease (AD), Parkinson's disease (PD), and various retinal diseases
identification. The results show that our approach can significantly improve
performance in the downstream tasks. In summary, this study combines the
benefits of CNNs with the capabilities of advanced self-supervised learning in
handling large-scale unlabeled data, demonstrating the potential of CNNs in the
presence of label scarcity.

</details>

### [141] [POET: Prompt Offset Tuning for Continual Human Action Adaptation](https://arxiv.org/abs/2504.18059)
*Prachi Garg,Joseph K J,Vineeth N Balasubramanian,Necati Cihan Camgoz,Chengde Wan,Kenrick Kin,Weiguang Si,Shugao Ma,Fernando De La Torre*

Main category: cs.CV

TLDR: POET是一种隐私感知的少样本持续动作识别方法，通过轻量级骨干网络和时空可学习提示偏移调优，提升个性化体验。


<details>
  <summary>Details</summary>
Motivation: 解决XR设备中模型静态且类别有限的问题，使用户能高效、低样本且隐私安全地添加新动作类别。

Method: 提出POET（Prompt-Offset Tuning），基于轻量级骨干网络和时空可学习提示偏移调优，首次应用于图神经网络。

Result: 在NTU RGB+D和SHREC-2017数据集上表现优于基准方法。

Conclusion: POET为隐私感知的少样本持续动作识别提供了高效解决方案，适用于XR设备。

Abstract: As extended reality (XR) is redefining how users interact with computing
devices, research in human action recognition is gaining prominence. Typically,
models deployed on immersive computing devices are static and limited to their
default set of classes. The goal of our research is to provide users and
developers with the capability to personalize their experience by adding new
action classes to their device models continually. Importantly, a user should
be able to add new classes in a low-shot and efficient manner, while this
process should not require storing or replaying any of user's sensitive
training data. We formalize this problem as privacy-aware few-shot continual
action recognition. Towards this end, we propose POET: Prompt-Offset Tuning.
While existing prompt tuning approaches have shown great promise for continual
learning of image, text, and video modalities; they demand access to
extensively pretrained transformers. Breaking away from this assumption, POET
demonstrates the efficacy of prompt tuning a significantly lightweight
backbone, pretrained exclusively on the base class data. We propose a novel
spatio-temporal learnable prompt offset tuning approach, and are the first to
apply such prompt tuning to Graph Neural Networks. We contribute two new
benchmarks for our new problem setting in human action recognition: (i) NTU
RGB+D dataset for activity recognition, and (ii) SHREC-2017 dataset for hand
gesture recognition. We find that POET consistently outperforms comprehensive
benchmarks. Source code at
https://github.com/humansensinglab/POET-continual-action-recognition.

</details>

### [142] [S3MOT: Monocular 3D Object Tracking with Selective State Space Model](https://arxiv.org/abs/2504.18068)
*Zhuohao Yan,Shaoquan Feng,Xingxing Li,Yuxuan Zhou,Chunxi Xia,Shengyu Li*

Main category: cs.CV

TLDR: 论文提出了三种创新技术（HSSM、FCOE、VeloSSM）来提升单目3D多目标跟踪的性能，在KITTI基准测试中取得了新SOTA。


<details>
  <summary>Details</summary>
Motivation: 单目3D多目标跟踪在机器人学和计算机视觉中至关重要，但现有方法难以从2D视频流中挖掘3D时空关联。

Method: 1. HSSM：高效的数据关联机制；2. FCOE：改进目标重识别；3. VeloSSM：增强6-DoF姿态估计。

Result: 在KITTI测试中达到76.86 HOTA（31 FPS），优于之前最佳方法。

Conclusion: 该方法在单目3D MOT任务中表现出高效性和鲁棒性，代码已开源。

Abstract: Accurate and reliable multi-object tracking (MOT) in 3D space is essential
for advancing robotics and computer vision applications. However, it remains a
significant challenge in monocular setups due to the difficulty of mining 3D
spatiotemporal associations from 2D video streams. In this work, we present
three innovative techniques to enhance the fusion and exploitation of
heterogeneous cues for monocular 3D MOT: (1) we introduce the Hungarian State
Space Model (HSSM), a novel data association mechanism that compresses
contextual tracking cues across multiple paths, enabling efficient and
comprehensive assignment decisions with linear complexity. HSSM features a
global receptive field and dynamic weights, in contrast to traditional linear
assignment algorithms that rely on hand-crafted association costs. (2) We
propose Fully Convolutional One-stage Embedding (FCOE), which eliminates ROI
pooling by directly using dense feature maps for contrastive learning, thus
improving object re-identification accuracy under challenging conditions such
as varying viewpoints and lighting. (3) We enhance 6-DoF pose estimation
through VeloSSM, an encoder-decoder architecture that models temporal
dependencies in velocity to capture motion dynamics, overcoming the limitations
of frame-based 3D inference. Experiments on the KITTI public test benchmark
demonstrate the effectiveness of our method, achieving a new state-of-the-art
performance of 76.86~HOTA at 31~FPS. Our approach outperforms the previous best
by significant margins of +2.63~HOTA and +3.62~AssA, showcasing its robustness
and efficiency for monocular 3D MOT tasks. The code and models are available at
https://github.com/bytepioneerX/s3mot.

</details>

### [143] [Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation](https://arxiv.org/abs/2504.18087)
*Weipeng Tan,Chuming Lin,Chengming Xu,FeiFan Xu,Xiaobin Hu,Xiaozhong Ji,Junwei Zhu,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TLDR: DICE-Talk提出了一种新框架，通过解耦身份与情感并合作相似情感特征，解决了现有情感说话头生成中的三个关键问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成情感表达丰富的肖像时难以保持说话者身份，且未能充分利用音频的情感线索、存在身份泄漏问题以及情感相关性学习不足。

Method: 1. 开发了解耦情感嵌入器，通过跨模态注意力建模音频-视觉情感线索；2. 引入相关性增强的情感条件模块，通过可学习的情感银行捕获情感间关系；3. 设计了情感判别目标，通过潜在空间分类确保情感一致性。

Result: 在MEAD和HDTF数据集上的实验表明，DICE-Talk在情感准确性上优于现有方法，同时保持竞争力的唇同步性能。

Conclusion: DICE-Talk能够生成身份保持且情感丰富的肖像，并能自然适应未见过的身份。

Abstract: Recent advances in Talking Head Generation (THG) have achieved impressive lip
synchronization and visual quality through diffusion models; yet existing
methods struggle to generate emotionally expressive portraits while preserving
speaker identity. We identify three critical limitations in current emotional
talking head generation: insufficient utilization of audio's inherent emotional
cues, identity leakage in emotion representations, and isolated learning of
emotion correlations. To address these challenges, we propose a novel framework
dubbed as DICE-Talk, following the idea of disentangling identity with emotion,
and then cooperating emotions with similar characteristics. First, we develop a
disentangled emotion embedder that jointly models audio-visual emotional cues
through cross-modal attention, representing emotions as identity-agnostic
Gaussian distributions. Second, we introduce a correlation-enhanced emotion
conditioning module with learnable Emotion Banks that explicitly capture
inter-emotion relationships through vector quantization and attention-based
feature aggregation. Third, we design an emotion discrimination objective that
enforces affective consistency during the diffusion process through
latent-space classification. Extensive experiments on MEAD and HDTF datasets
demonstrate our method's superiority, outperforming state-of-the-art approaches
in emotion accuracy while maintaining competitive lip-sync performance.
Qualitative results and user studies further confirm our method's ability to
generate identity-preserving portraits with rich, correlated emotional
expressions that naturally adapt to unseen identities.

</details>

### [144] [Study on Real-Time Road Surface Reconstruction Using Stereo Vision](https://arxiv.org/abs/2504.18112)
*Deepak Ghimire,Byoungjun Kim,Donghoon Kim,SungHwan Jeong*

Main category: cs.CV

TLDR: 本文优化了RoadBEV框架，通过异构全局结构化剪枝和重新设计头部网络，提升了实时道路表面重建的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 道路表面重建对自动驾驶至关重要，但现有方法在边缘设备上的实时性和准确性不足。

Method: 采用异构全局结构化剪枝优化立体特征提取主干网络，并重新设计头部网络，包括优化的沙漏结构、动态注意力头、减少特征通道、混合精度推理和高效概率体积计算。

Result: 方法提高了推理速度并降低了重建误差。

Conclusion: 该框架适用于自动驾驶中的实时道路表面重建。

Abstract: Road surface reconstruction plays a crucial role in autonomous driving,
providing essential information for safe and smooth navigation. This paper
enhances the RoadBEV [1] framework for real-time inference on edge devices by
optimizing both efficiency and accuracy. To achieve this, we proposed to apply
Isomorphic Global Structured Pruning to the stereo feature extraction backbone,
reducing network complexity while maintaining performance. Additionally, the
head network is redesigned with an optimized hourglass structure, dynamic
attention heads, reduced feature channels, mixed precision inference, and
efficient probability volume computation. Our approach improves inference speed
while achieving lower reconstruction error, making it well-suited for real-time
road surface reconstruction in autonomous driving.

</details>

### [145] [Salient Region-Guided Spacecraft Image Arbitrary-Scale Super-Resolution Network](https://arxiv.org/abs/2504.18127)
*Jingfan Yang,Hu Gao,Ying Zhang,Depeng Dang*

Main category: cs.CV

TLDR: 提出了一种基于显著区域引导的航天器图像任意尺度超分辨率网络（SGSASR），通过识别航天器核心区域并选择性融合特征，显著提升超分辨率效果。


<details>
  <summary>Details</summary>
Motivation: 现有任意尺度超分辨率方法在航天器图像中忽视了核心区域与黑色背景的特征差异，导致引入无关噪声。

Method: 设计了航天器核心区域识别块（SCRRB）和自适应加权特征融合增强机制（AFFEM），通过动态权重参数选择性融合特征。

Result: 实验表明，SGSASR优于现有最先进方法。

Conclusion: SGSASR通过显著区域引导和特征融合，有效提升了航天器图像的超分辨率质量。

Abstract: Spacecraft image super-resolution seeks to enhance low-resolution spacecraft
images into high-resolution ones. Although existing arbitrary-scale
super-resolution methods perform well on general images, they tend to overlook
the difference in features between the spacecraft core region and the large
black space background, introducing irrelevant noise. In this paper, we propose
a salient region-guided spacecraft image arbitrary-scale super-resolution
network (SGSASR), which uses features from the spacecraft core salient regions
to guide latent modulation and achieve arbitrary-scale super-resolution.
Specifically, we design a spacecraft core region recognition block (SCRRB) that
identifies the core salient regions in spacecraft images using a pre-trained
saliency detection model. Furthermore, we present an adaptive-weighted feature
fusion enhancement mechanism (AFFEM) to selectively aggregate the spacecraft
core region features with general image features by dynamic weight parameter to
enhance the response of the core salient regions. Experimental results
demonstrate that the proposed SGSASR outperforms state-of-the-art approaches.

</details>

### [146] [MASF-YOLO: An Improved YOLOv11 Network for Small Object Detection on Drone View](https://arxiv.org/abs/2504.18136)
*Liugang Lu,Dabin He,Congxiang Liu,Zhixiang Deng*

Main category: cs.CV

TLDR: 提出了一种基于YOLOv11的新型目标检测网络MASF-YOLO，通过多尺度特征聚合和改进的注意力机制，显著提升了无人机图像中小目标的检测精度和背景噪声抑制能力。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中目标像素极小、尺度变化大、背景复杂，限制了实际应用，需解决这些问题。

Method: 设计了多尺度特征聚合模块（MFAM）、改进的高效多尺度注意力模块（IEMA）和维度感知选择性集成模块（DASI）。

Result: 在VisDrone2019数据集上，MASF-YOLO-s比YOLOv11-s在mAP@0.5和mAP@0.5:0.95上分别提升了4.6%和3.5%，且模型参数和计算成本更低。

Conclusion: MASF-YOLO在检测精度和模型效率上均具有明显竞争优势。

Abstract: With the rapid advancement of Unmanned Aerial Vehicle (UAV) and computer
vision technologies, object detection from UAV perspectives has emerged as a
prominent research area. However, challenges for detection brought by the
extremely small proportion of target pixels, significant scale variations of
objects, and complex background information in UAV images have greatly limited
the practical applications of UAV. To address these challenges, we propose a
novel object detection network Multi-scale Context Aggregation and
Scale-adaptive Fusion YOLO (MASF-YOLO), which is developed based on YOLOv11.
Firstly, to tackle the difficulty of detecting small objects in UAV images, we
design a Multi-scale Feature Aggregation Module (MFAM), which significantly
improves the detection accuracy of small objects through parallel multi-scale
convolutions and feature fusion. Secondly, to mitigate the interference of
background noise, we propose an Improved Efficient Multi-scale Attention Module
(IEMA), which enhances the focus on target regions through feature grouping,
parallel sub-networks, and cross-spatial learning. Thirdly, we introduce a
Dimension-Aware Selective Integration Module (DASI), which further enhances
multi-scale feature fusion capabilities by adaptively weighting and fusing
low-dimensional features and high-dimensional features. Finally, we conducted
extensive performance evaluations of our proposed method on the VisDrone2019
dataset. Compared to YOLOv11-s, MASFYOLO-s achieves improvements of 4.6% in
mAP@0.5 and 3.5% in mAP@0.5:0.95 on the VisDrone2019 validation set.
Remarkably, MASF-YOLO-s outperforms YOLOv11-m while requiring only
approximately 60% of its parameters and 65% of its computational cost.
Furthermore, comparative experiments with state-of-the-art detectors confirm
that MASF-YOLO-s maintains a clear competitive advantage in both detection
accuracy and model efficiency.

</details>

### [147] [ActionArt: Advancing Multimodal Large Models for Fine-Grained Human-Centric Video Understanding](https://arxiv.org/abs/2504.18152)
*Yi-Xing Peng,Qize Yang,Yu-Ming Tang,Shenghao Fu,Kun-Yu Lin,Xihan Wei,Wei-Shi Zheng*

Main category: cs.CV

TLDR: ActionArt是一个细粒度视频描述数据集，用于提升人类中心多模态理解研究。通过自动生成的代理任务减少对昂贵人工标注的依赖，显著缩小与人工标注数据的性能差距。


<details>
  <summary>Details</summary>
Motivation: 细粒度理解人类动作和姿态对人类中心AI应用至关重要，但现有大型多模态模型在细粒度理解上表现不足，主要因缺乏精细标注数据。

Method: 开发ActionArt数据集，包含数千个视频和详细标注；设计八个子任务评估模型能力；提出自动生成代理任务以减少人工标注依赖。

Result: 当前大型多模态模型在细粒度理解上表现有限；代理任务显著缩小与人工标注数据的性能差距。

Conclusion: 代理任务能有效提升模型细粒度理解能力，减少对昂贵人工标注的依赖。

Abstract: Fine-grained understanding of human actions and poses in videos is essential
for human-centric AI applications. In this work, we introduce ActionArt, a
fine-grained video-caption dataset designed to advance research in
human-centric multimodal understanding. Our dataset comprises thousands of
videos capturing a broad spectrum of human actions, human-object interactions,
and diverse scenarios, each accompanied by detailed annotations that
meticulously label every limb movement. We develop eight sub-tasks to evaluate
the fine-grained understanding capabilities of existing large multimodal models
across different dimensions. Experimental results indicate that, while current
large multimodal models perform commendably on various tasks, they often fall
short in achieving fine-grained understanding. We attribute this limitation to
the scarcity of meticulously annotated data, which is both costly and difficult
to scale manually. Since manual annotations are costly and hard to scale, we
propose proxy tasks to enhance the model perception ability in both spatial and
temporal dimensions. These proxy tasks are carefully crafted to be driven by
data automatically generated from existing MLLMs, thereby reducing the reliance
on costly manual labels. Experimental results show that the proposed proxy
tasks significantly narrow the gap toward the performance achieved with
manually annotated fine-grained data.

</details>

### [148] [E-InMeMo: Enhanced Prompting for Visual In-Context Learning](https://arxiv.org/abs/2504.18158)
*Jiahao Zhang,Bowen Wang,Hong Liu,Liangzhi Li,Yuta Nakashima,Hajime Nagahara*

Main category: cs.CV

TLDR: 论文提出E-InMeMo方法，通过可学习的扰动优化视觉上下文学习（ICL）中的提示，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 视觉ICL的成功依赖于提示质量，现有方法存在优化不足的问题。

Method: 提出E-InMeMo，在上下文对中引入可学习的扰动以优化提示。

Result: 在标准视觉任务中，E-InMeMo显著优于现有方法，mIoU提升7.99（前景分割）和17.04（单目标检测）。

Conclusion: E-InMeMo是一种轻量且有效的视觉ICL优化策略，代码已开源。

Abstract: Large-scale models trained on extensive datasets have become the standard due
to their strong generalizability across diverse tasks. In-context learning
(ICL), widely used in natural language processing, leverages these models by
providing task-specific prompts without modifying their parameters. This
paradigm is increasingly being adapted for computer vision, where models
receive an input-output image pair, known as an in-context pair, alongside a
query image to illustrate the desired output. However, the success of visual
ICL largely hinges on the quality of these prompts. To address this, we propose
Enhanced Instruct Me More (E-InMeMo), a novel approach that incorporates
learnable perturbations into in-context pairs to optimize prompting. Through
extensive experiments on standard vision tasks, E-InMeMo demonstrates superior
performance over existing state-of-the-art methods. Notably, it improves mIoU
scores by 7.99 for foreground segmentation and by 17.04 for single object
detection when compared to the baseline without learnable prompts. These
results highlight E-InMeMo as a lightweight yet effective strategy for
enhancing visual ICL. Code is publicly available at:
https://github.com/Jackieam/E-InMeMo

</details>

### [149] [PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models](https://arxiv.org/abs/2504.18165)
*Michel Gokan Khan,Renan Guarese,Fabian Johnson,Xi Vincent Wang,Anders Bergman,Benjamin Edvinsson,Mario Romero,Jérémy Vachier,Jan Kronqvist*

Main category: cs.CV

TLDR: PerfCam是一个开源的数字孪生框架，结合相机和传感器数据、3D高斯泼溅及计算机视觉模型，用于工业生产线中的数字孪生、对象跟踪和KPI提取。


<details>
  <summary>Details</summary>
Motivation: 为工业生产线提供实时KPI（如可用性、性能、OEE等）的数字孪生解决方案，支持智能制造环境的操作分析。

Method: 利用3D重建和卷积神经网络（CNNs）实现半自动化的对象跟踪和空间映射。

Result: 在实际制药行业生产线中验证了PerfCam的有效性，并公开数据集支持进一步研究。

Conclusion: PerfCam通过精确的数字孪生能力提供可操作的洞察，是智能制造环境中开发可用数字孪生的有效工具。

Abstract: We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning
framework that combines camera and sensory data with 3D Gaussian Splatting and
computer vision models for digital twinning, object tracking, and Key
Performance Indicators (KPIs) extraction in industrial production lines. By
utilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam
offers a semi-automated approach to object tracking and spatial mapping,
enabling digital twins that capture real-time KPIs such as availability,
performance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts
in the production line. We validate the effectiveness of PerfCam through a
practical deployment within realistic test production lines in the
pharmaceutical industry and contribute an openly published dataset to support
further research and development in the field. The results demonstrate
PerfCam's ability to deliver actionable insights through its precise digital
twin capabilities, underscoring its value as an effective tool for developing
usable digital twins in smart manufacturing environments and extracting
operational analytics.

</details>

### [150] [Label-independent hyperparameter-free self-supervised single-view deep subspace clustering](https://arxiv.org/abs/2504.18179)
*Lovro Sindicic,Ivica Kopriva*

Main category: cs.CV

TLDR: 提出了一种新的单视图深度子空间聚类方法，解决了现有方法的五个主要问题，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有深度子空间聚类方法存在五个主要问题：仅使用编码器输出层评估聚类质量、独立处理表示学习和子空间聚类、依赖外部数据集调参、需要标签终止学习、依赖后处理技术。

Method: 提出了一种新方法：(i) 使用联合表示矩阵最小化分层自表达损失；(ii) 优化子空间结构范数以提升聚类质量；(iii) 采用多阶段顺序学习框架，无需调参；(iv) 基于相对误差的自停止机制；(v) 固定表示矩阵中前导系数。

Result: 在六个数据集（人脸、数字、物体）上评估，优于大多数线性子空间聚类算法，与最佳线性方法性能相当。

Conclusion: 新方法解决了现有问题，无需标签和调参，性能优越。

Abstract: Deep subspace clustering (DSC) algorithms face several challenges that hinder
their widespread adoption across variois application domains. First, clustering
quality is typically assessed using only the encoder's output layer,
disregarding valuable information present in the intermediate layers. Second,
most DSC approaches treat representation learning and subspace clustering as
independent tasks, limiting their effectiveness. Third, they assume the
availability of a held-out dataset for hyperparameter tuning, which is often
impractical in real-world scenarios. Fourth, learning termination is commonly
based on clustering error monitoring, requiring external labels. Finally, their
performance often depends on post-processing techniques that rely on labeled
data. To address this limitations, we introduce a novel single-view DSC
approach that: (i) minimizes a layer-wise self expression loss using a joint
representation matrix; (ii) optimizes a subspace-structured norm to enhance
clustering quality; (iii) employs a multi-stage sequential learning framework,
consisting of pre-training and fine-tuning, enabling the use of multiple
regularization terms without hyperparameter tuning; (iv) incorporates a
relative error-based self-stopping mechanism to terminate training without
labels; and (v) retains a fixed number of leading coefficients in the learned
representation matrix based on prior knowledge. We evaluate the proposed method
on six datasets representing faces, digits, and objects. The results show that
our method outperforms most linear SC algorithms with careffulyl tuned
hyperparameters while maintaining competitive performance with the best
performing linear appoaches.

</details>

### [151] [What is the Added Value of UDA in the VFM Era?](https://arxiv.org/abs/2504.18190)
*Brunó B. Englert,Tommie Kerssies,Gijs Dubbelman*

Main category: cs.CV

TLDR: 研究探讨了无监督域适应（UDA）在更现实数据场景下的表现，发现其优势在强合成源数据或少量目标标签下显著，但在多样真实源数据中无额外价值。


<details>
  <summary>Details</summary>
Motivation: 解决UDA在现实数据场景中的表现问题，明确其与源数据微调（source-only fine-tuning）的比较优势。

Method: 通过语义分割任务评估UDA在合成到真实和真实到真实场景中的表现，并研究少量目标标签的影响。

Result: UDA在强合成源数据下优势减小（+8 mIoU到+2 mIoU），在多样真实源数据中无优势；但合成数据场景下UDA始终优于源数据微调。少量目标标签（1/16 Cityscapes）下UDA达到全监督模型性能（85 mIoU）。

Conclusion: UDA在特定场景（如合成数据或少量目标标签）下仍具价值，但需根据数据特性选择方法以支持大规模自动驾驶的鲁棒性。

Abstract: Unsupervised Domain Adaptation (UDA) can improve a perception model's
generalization to an unlabeled target domain starting from a labeled source
domain. UDA using Vision Foundation Models (VFMs) with synthetic source data
can achieve generalization performance comparable to fully-supervised learning
with real target data. However, because VFMs have strong generalization from
their pre-training, more straightforward, source-only fine-tuning can also
perform well on the target. As data scenarios used in academic research are not
necessarily representative for real-world applications, it is currently unclear
(a) how UDA behaves with more representative and diverse data and (b) if
source-only fine-tuning of VFMs can perform equally well in these scenarios.
Our research aims to close these gaps and, similar to previous studies, we
focus on semantic segmentation as a representative perception task. We assess
UDA for synth-to-real and real-to-real use cases with different source and
target data combinations. We also investigate the effect of using a small
amount of labeled target data in UDA. We clarify that while these scenarios are
more realistic, they are not necessarily more challenging. Our results show
that, when using stronger synthetic source data, UDA's improvement over
source-only fine-tuning of VFMs reduces from +8 mIoU to +2 mIoU, and when using
more diverse real source data, UDA has no added value. However, UDA
generalization is always higher in all synthetic data scenarios than
source-only fine-tuning and, when including only 1/16 of Cityscapes labels,
synthetic UDA obtains the same state-of-the-art segmentation quality of 85 mIoU
as a fully-supervised model using all labels. Considering the mixed results, we
discuss how UDA can best support robust autonomous driving at scale.

</details>

### [152] [Multi-Grained Compositional Visual Clue Learning for Image Intent Recognition](https://arxiv.org/abs/2504.18201)
*Yin Tang,Jiankai Li,Hongyu Yang,Xuan Dong,Lifeng Fan,Weixin Li*

Main category: cs.CV

TLDR: 论文提出了一种名为MCCL的新方法，通过多粒度视觉线索组合学习解决图像意图识别的挑战，显著提升了现有方法的准确性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体时代，图像分享反映了用户意图和兴趣，但传统计算机视觉任务难以捕捉隐含的视觉线索，且现有方法无法处理意图类别的视觉多样性。

Method: MCCL方法将意图识别分解为视觉线索组合，整合多粒度特征，使用类特定原型缓解数据不平衡，并通过图卷积网络引入标签嵌入相关性。

Result: 在Intentonomy和MDID数据集上取得了最先进的性能，同时具备良好的可解释性。

Conclusion: 该方法为理解人类复杂多样的表达形式提供了新思路，具有未来探索的潜力。

Abstract: In an era where social media platforms abound, individuals frequently share
images that offer insights into their intents and interests, impacting
individual life quality and societal stability. Traditional computer vision
tasks, such as object detection and semantic segmentation, focus on concrete
visual representations, while intent recognition relies more on implicit visual
clues. This poses challenges due to the wide variation and subjectivity of such
clues, compounded by the problem of intra-class variety in conveying abstract
concepts, e.g. "enjoy life". Existing methods seek to solve the problem by
manually designing representative features or building prototypes for each
class from global features. However, these methods still struggle to deal with
the large visual diversity of each intent category. In this paper, we introduce
a novel approach named Multi-grained Compositional visual Clue Learning (MCCL)
to address these challenges for image intent recognition. Our method leverages
the systematic compositionality of human cognition by breaking down intent
recognition into visual clue composition and integrating multi-grained
features. We adopt class-specific prototypes to alleviate data imbalance. We
treat intent recognition as a multi-label classification problem, using a graph
convolutional network to infuse prior knowledge through label embedding
correlations. Demonstrated by a state-of-the-art performance on the Intentonomy
and MDID datasets, our approach advances the accuracy of existing methods while
also possessing good interpretability. Our work provides an attempt for future
explorations in understanding complex and miscellaneous forms of human
expression.

</details>

### [153] [LiDAR-Guided Monocular 3D Object Detection for Long-Range Railway Monitoring](https://arxiv.org/abs/2504.18203)
*Raul David Dominguez Sanchez,Xavier Diaz Ortiz,Xingcheng Zhou,Max Peter Ronecker,Michael Karner,Daniel Watzenig,Alois Knoll*

Main category: cs.CV

TLDR: 本文提出了一种基于深度学习的单目图像长距离3D物体检测方法，专为自动驾驶列车设计，通过改进的YOLOv9和深度估计网络，实现了250米范围内的物体检测。


<details>
  <summary>Details</summary>
Motivation: 德国铁路系统需要高自动化以应对老旧基础设施挑战并安全增加列车流量，而长距离感知是自动化关键，需检测1公里范围内的障碍物。

Method: 方法基于单目图像，受Faraway-Frustum启发，结合LiDAR数据训练改进深度估计，包括改进的YOLOv9、深度估计网络及短/长距离3D检测头。

Result: 在OSDaR23数据集上验证，方法能有效检测250米内物体，展示了铁路自动化潜力。

Conclusion: 该方法为铁路自动化提供了有效解决方案，并指出了未来改进方向。

Abstract: Railway systems, particularly in Germany, require high levels of automation
to address legacy infrastructure challenges and increase train traffic safely.
A key component of automation is robust long-range perception, essential for
early hazard detection, such as obstacles at level crossings or pedestrians on
tracks. Unlike automotive systems with braking distances of ~70 meters, trains
require perception ranges exceeding 1 km. This paper presents an
deep-learning-based approach for long-range 3D object detection tailored for
autonomous trains. The method relies solely on monocular images, inspired by
the Faraway-Frustum approach, and incorporates LiDAR data during training to
improve depth estimation. The proposed pipeline consists of four key modules:
(1) a modified YOLOv9 for 2.5D object detection, (2) a depth estimation
network, and (3-4) dedicated short- and long-range 3D detection heads.
Evaluations on the OSDaR23 dataset demonstrate the effectiveness of the
approach in detecting objects up to 250 meters. Results highlight its potential
for railway automation and outline areas for future improvement.

</details>

### [154] [Optimizing Multi-Round Enhanced Training in Diffusion Models for Improved Preference Understanding](https://arxiv.org/abs/2504.18204)
*Kun Li,Jianhui Wang,Yangfan He,Xinyuan Song,Ruoyu Wang,Hongyang He,Wenxin Zhang,Jiaqi Chen,Keqin Li,Sida Li,Miao Zhang,Tianyu Shi,Xueqian Wang*

Main category: cs.CV

TLDR: 提出了一种视觉协同适应（VCA）框架，通过多轮对话数据集和人类反馈优化生成图像，显著提升了图像一致性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在文本驱动图像生成中存在高分辨率和细粒度用户偏好对齐的挑战，需多轮交互优化。

Method: 结合人类反馈和奖励模型，利用多轮对话数据集，通过LoRA微调扩散模型，应用多样性、一致性和偏好反馈等多奖励函数。

Result: 实验表明，该方法在图像一致性和用户意图对齐上优于现有基线，用户满意度显著提升。

Conclusion: VCA框架在多轮对话场景中表现优异，为生成式AI的交互优化提供了有效解决方案。

Abstract: Generative AI has significantly changed industries by enabling text-driven
image generation, yet challenges remain in achieving high-resolution outputs
that align with fine-grained user preferences. Consequently, multi-round
interactions are necessary to ensure the generated images meet expectations.
Previous methods enhanced prompts via reward feedback but did not optimize over
a multi-round dialogue dataset. In this work, we present a Visual Co-Adaptation
(VCA) framework incorporating human-in-the-loop feedback, leveraging a
well-trained reward model aligned with human preferences. Using a diverse
multi-turn dialogue dataset, our framework applies multiple reward functions,
such as diversity, consistency, and preference feedback, while fine-tuning the
diffusion model through LoRA, thus optimizing image generation based on user
input. We also construct multi-round dialogue datasets of prompts and image
pairs aligned with user intent. Experiments demonstrate that our method
outperforms state-of-the-art baselines, significantly improving image
consistency and alignment with user intent. Our approach consistently surpasses
competing models in user satisfaction, especially in multi-turn dialogue
scenarios.

</details>

### [155] [A Data-Centric Approach to 3D Semantic Segmentation of Railway Scenes](https://arxiv.org/abs/2504.18213)
*Nicolas Münger,Max Peter Ronecker,Xavier Diaz,Michael Karner,Daniel Watzenig,Jan Skaloud*

Main category: cs.CV

TLDR: 论文提出两种数据增强方法，提升LiDAR语义分割在铁路场景中的远距离性能。


<details>
  <summary>Details</summary>
Motivation: 解决铁路场景中LiDAR语义分割在远距离预测精度不足的问题。

Method: 采用行人实例粘贴和轨道稀疏化两种数据增强方法。

Result: 显著提升远距离分割性能，同时保持近距离预测的鲁棒性。

Conclusion: 数据为中心的方法能有效解决铁路场景的感知挑战。

Abstract: LiDAR-based semantic segmentation is critical for autonomous trains,
requiring accurate predictions across varying distances. This paper introduces
two targeted data augmentation methods designed to improve segmentation
performance on the railway-specific OSDaR23 dataset. The person instance
pasting method enhances segmentation of pedestrians at distant ranges by
injecting realistic variations into the dataset. The track sparsification
method redistributes point density in LiDAR scans, improving track segmentation
at far distances with minimal impact on close-range accuracy. Both methods are
evaluated using a state-of-the-art 3D semantic segmentation network,
demonstrating significant improvements in distant-range performance while
maintaining robustness in close-range predictions. We establish the first 3D
semantic segmentation benchmark for OSDaR23, demonstrating the potential of
data-centric approaches to address railway-specific challenges in autonomous
train perception.

</details>

### [156] [Unify3D: An Augmented Holistic End-to-end Monocular 3D Human Reconstruction via Anatomy Shaping and Twins Negotiating](https://arxiv.org/abs/2504.18215)
*Nanjie Yao,Gangjian Zhang,Wenhao Shen,Jian Shu,Hao Wang*

Main category: cs.CV

TLDR: 提出了一种端到端的单目3D人体重建方法，直接预测2D图像到3D化身，无需显式中间几何表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖前置模型，忽视任务完整性，限制了重建效果。

Method: 提出整体重建范式，包含解剖形状提取模块和双模态U-Net交互重建模块，并采用数据增强策略。

Result: 在多个测试集和实际案例中表现优于现有方法。

Conclusion: 新方法通过端到端设计和特征交互，显著提升了重建效果。

Abstract: Monocular 3D clothed human reconstruction aims to create a complete 3D avatar
from a single image. To tackle the human geometry lacking in one RGB image,
current methods typically resort to a preceding model for an explicit geometric
representation. For the reconstruction itself, focus is on modeling both it and
the input image. This routine is constrained by the preceding model, and
overlooks the integrity of the reconstruction task. To address this, this paper
introduces a novel paradigm that treats human reconstruction as a holistic
process, utilizing an end-to-end network for direct prediction from 2D image to
3D avatar, eliminating any explicit intermediate geometry display. Based on
this, we further propose a novel reconstruction framework consisting of two
core components: the Anatomy Shaping Extraction module, which captures implicit
shape features taking into account the specialty of human anatomy, and the
Twins Negotiating Reconstruction U-Net, which enhances reconstruction through
feature interaction between two U-Nets of different modalities. Moreover, we
propose a Comic Data Augmentation strategy and construct 15k+ 3D human scans to
bolster model performance in more complex case input. Extensive experiments on
two test sets and many in-the-wild cases show the superiority of our method
over SOTA methods. Our demos can be found in :
https://e2e3dgsrecon.github.io/e2e3dgsrecon/.

</details>

### [157] [Dense Geometry Supervision for Underwater Depth Estimation](https://arxiv.org/abs/2504.18233)
*Wenxiang Gua,Lin Qia*

Main category: cs.CV

TLDR: 提出了一种针对水下场景的单目深度估计新方法，包括构建经济高效的数据集和设计纹理-深度融合模块，显著提升了水下深度估计的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 水下场景的单目深度估计研究有限，且缺乏相关数据和方法支持，本文旨在解决这些问题。

Method: 通过多视角深度估计生成监督信号和增强水下图像，构建数据集；设计基于水下光学成像原理的纹理-深度融合模块。

Result: 在FLSea数据集上的实验表明，该方法显著提高了水下深度估计的准确性和模型适应性。

Conclusion: 提供了一种经济高效的水下单目深度估计解决方案，具有实际应用潜力。

Abstract: The field of monocular depth estimation is continually evolving with the
advent of numerous innovative models and extensions. However, research on
monocular depth estimation methods specifically for underwater scenes remains
limited, compounded by a scarcity of relevant data and methodological support.
This paper proposes a novel approach to address the existing challenges in
current monocular depth estimation methods for underwater environments. We
construct an economically efficient dataset suitable for underwater scenarios
by employing multi-view depth estimation to generate supervisory signals and
corresponding enhanced underwater images. we introduces a texture-depth fusion
module, designed according to the underwater optical imaging principles, which
aims to effectively exploit and integrate depth information from texture cues.
Experimental results on the FLSea dataset demonstrate that our approach
significantly improves the accuracy and adaptability of models in underwater
settings. This work offers a cost-effective solution for monocular underwater
depth estimation and holds considerable promise for practical applications.

</details>

### [158] [BiasBench: A reproducible benchmark for tuning the biases of event cameras](https://arxiv.org/abs/2504.18235)
*Andreas Ziegler,David Joseph,Thomas Gossard,Emil Moldovan,Andreas Zell*

Main category: cs.CV

TLDR: BiasBench是一个新的事件数据集，用于优化事件相机的偏置设置，并提出了一种基于强化学习的在线偏置调整方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机因其高时间分辨率、低延迟和高动态范围等优势在计算机视觉和机器人领域应用广泛，但其偏置设置的优化工具较少。

Method: 提出了BiasBench数据集，包含多个场景和网格采样的设置，并开发了一种基于强化学习的在线偏置调整方法。

Result: BiasBench数据集为事件相机的偏置优化提供了可重复性支持，并展示了三种场景及其下游应用的质量指标。

Conclusion: BiasBench和基于强化学习的在线调整方法为事件相机的偏置优化提供了实用工具和框架。

Abstract: Event-based cameras are bio-inspired sensors that detect light changes
asynchronously for each pixel. They are increasingly used in fields like
computer vision and robotics because of several advantages over traditional
frame-based cameras, such as high temporal resolution, low latency, and high
dynamic range. As with any camera, the output's quality depends on how well the
camera's settings, called biases for event-based cameras, are configured. While
frame-based cameras have advanced automatic configuration algorithms, there are
very few such tools for tuning these biases. A systematic testing framework
would require observing the same scene with different biases, which is tricky
since event cameras only generate events when there is movement. Event
simulators exist, but since biases heavily depend on the electrical circuit and
the pixel design, available simulators are not well suited for bias tuning. To
allow reproducibility, we present BiasBench, a novel event dataset containing
multiple scenes with settings sampled in a grid-like pattern. We present three
different scenes, each with a quality metric of the downstream application.
Additionally, we present a novel, RL-based method to facilitate online bias
adjustments.

</details>

### [159] [Event-Based Eye Tracking. 2025 Event-based Vision Workshop](https://arxiv.org/abs/2504.18249)
*Qinyu Chen,Chang Gao,Min Liu,Daniele Perrone,Yan Ru Pei,Zuowen Wang,Zhuo Zou,Shihang Tan,Tao Han,Guorui Lu,Zhen Xu,Junyuan Ding,Ziteng Wang,Zongwei Wu,Han Han,Yuliang Wu,Jinze Chen,Wei Zhai,Yang Cao,Zheng-jun Zha,Nuwan Bandara,Thivya Kandappu,Archan Misra,Xiaopeng Lin,Hongxiang Huang,Hongwei Ren,Bojun Cheng,Hoang M. Truong,Vinh-Thuan Ly,Huy G. Tran,Thuan-Phat Nguyen,Tram T. Doan*

Main category: cs.CV

TLDR: 本文是对2025年CVPR事件视觉研讨会中基于事件的眼动追踪挑战赛的综述，总结了排名靠前团队的创新方法，并讨论了硬件设计的视角。


<details>
  <summary>Details</summary>
Motivation: 推动基于事件的眼动追踪研究的未来发展，通过总结挑战赛中优秀方法的特点和性能。

Method: 回顾并总结了挑战赛中排名靠前团队的创新方法，包括准确性、模型大小和计算量等指标。

Result: 提供了各团队方法的性能比较，并讨论了硬件设计对眼动追踪的影响。

Conclusion: 本文为未来基于事件的眼动追踪研究提供了有价值的参考和方向。

Abstract: This survey serves as a review for the 2025 Event-Based Eye Tracking
Challenge organized as part of the 2025 CVPR event-based vision workshop. This
challenge focuses on the task of predicting the pupil center by processing
event camera recorded eye movement. We review and summarize the innovative
methods from teams rank the top in the challenge to advance future event-based
eye tracking research. In each method, accuracy, model size, and number of
operations are reported. In this survey, we also discuss event-based eye
tracking from the perspective of hardware design.

</details>

### [160] [SSL4Eco: A Global Seasonal Dataset for Geospatial Foundation Models in Ecology](https://arxiv.org/abs/2504.18256)
*Elena Plekhanova,Damien Robert,Johannes Dollinger,Emilia Arens,Philipp Brun,Jan Dirk Wegner,Niklaus Zimmermann*

Main category: cs.CV

TLDR: 论文提出了一种基于物候学的采样策略和SSL4Eco数据集，用于改进全球植被季节性建模，并在多个下游任务中取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 生物多样性和气候危机加剧，亟需全球生物多样性制图等宏观生态研究。现有遥感数据存在标签稀缺和区域偏差问题，且季节性建模不足。

Method: 提出物候学采样策略，构建SSL4Eco数据集，并采用季节对比目标训练模型。

Result: 模型在8个下游任务中7个达到最优性能，验证了采样策略的有效性。

Conclusion: 数据集构建对模型性能至关重要，SSL4Eco为宏观生态和计算机视觉研究提供了支持。

Abstract: With the exacerbation of the biodiversity and climate crises, macroecological
pursuits such as global biodiversity mapping become more urgent. Remote sensing
offers a wealth of Earth observation data for ecological studies, but the
scarcity of labeled datasets remains a major challenge. Recently,
self-supervised learning has enabled learning representations from unlabeled
data, triggering the development of pretrained geospatial models with
generalizable features. However, these models are often trained on datasets
biased toward areas of high human activity, leaving entire ecological regions
underrepresented. Additionally, while some datasets attempt to address
seasonality through multi-date imagery, they typically follow calendar seasons
rather than local phenological cycles. To better capture vegetation seasonality
at a global scale, we propose a simple phenology-informed sampling strategy and
introduce corresponding SSL4Eco, a multi-date Sentinel-2 dataset, on which we
train an existing model with a season-contrastive objective. We compare
representations learned from SSL4Eco against other datasets on diverse
ecological downstream tasks and demonstrate that our straightforward sampling
method consistently improves representation quality, highlighting the
importance of dataset construction. The model pretrained on SSL4Eco reaches
state of the art performance on 7 out of 8 downstream tasks spanning
(multi-label) classification and regression. We release our code, data, and
model weights to support macroecological and computer vision research at
https://github.com/PlekhanovaElena/ssl4eco.

</details>

### [161] [Seeing Soundscapes: Audio-Visual Generation and Separation from Soundscapes Using Audio-Visual Separator](https://arxiv.org/abs/2504.18283)
*Minjae Kang,Martim Brandão*

Main category: cs.CV

TLDR: 论文提出了一种音频-视觉生成与分离模型（AV-GAS），用于从混合音频（多类别声音）生成图像，解决了现有方法无法处理混合音频的问题。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉生成模型仅能处理单类别音频，无法生成混合音频对应的图像。

Method: 提出AV-GAS模型，结合音频-视觉分离器，支持从混合音频生成图像，并引入新的音频-视觉分离任务。

Result: 在VGGSound数据集上，模型表现优于现有方法，CRS提高7%，R@2*提高4%。

Conclusion: AV-GAS成功解决了从混合音频生成图像的挑战，并提出了新的任务和评估指标。

Abstract: Recent audio-visual generative models have made substantial progress in
generating images from audio. However, existing approaches focus on generating
images from single-class audio and fail to generate images from mixed audio. To
address this, we propose an Audio-Visual Generation and Separation model
(AV-GAS) for generating images from soundscapes (mixed audio containing
multiple classes). Our contribution is threefold: First, we propose a new
challenge in the audio-visual generation task, which is to generate an image
given a multi-class audio input, and we propose a method that solves this task
using an audio-visual separator. Second, we introduce a new audio-visual
separation task, which involves generating separate images for each class
present in a mixed audio input. Lastly, we propose new evaluation metrics for
the audio-visual generation task: Class Representation Score (CRS) and a
modified R@K. Our model is trained and evaluated on the VGGSound dataset. We
show that our method outperforms the state-of-the-art, achieving 7% higher CRS
and 4% higher R@2* in generating plausible images with mixed audio.

</details>

### [162] [Enhancing Long-Term Re-Identification Robustness Using Synthetic Data: A Comparative Analysis](https://arxiv.org/abs/2504.18286)
*Christian Pionzewski,Rebecca Rademacher,Jérôme Rutinowski,Antonia Ponikarov,Stephan Matzke,Tim Chilla,Pia Schreynemackers,Alice Kirchheim*

Main category: cs.CV

TLDR: 论文研究了合成训练数据对材料磨损和老化预测在重识别中的影响，通过实验和扩展策略提升了性能，并发布了一个开源数据集。


<details>
  <summary>Details</summary>
Motivation: 探索合成训练数据在材料老化重识别中的作用，以及如何通过动态更新数据集提升性能。

Method: 测试不同实验设置和扩展策略，使用动态更新的数据集和合成训练数据。

Result: 动态更新数据集使Rank-1准确率提升24%，合成训练数据使准确率提升13%。

Conclusion: 合成数据和动态更新策略显著提升重识别性能，开源数据集为未来研究提供支持。

Abstract: This contribution explores the impact of synthetic training data usage and
the prediction of material wear and aging in the context of re-identification.
Different experimental setups and gallery set expanding strategies are tested,
analyzing their impact on performance over time for aging re-identification
subjects. Using a continuously updating gallery, we were able to increase our
mean Rank-1 accuracy by 24%, as material aging was taken into account step by
step. In addition, using models trained with 10% artificial training data,
Rank-1 accuracy could be increased by up to 13%, in comparison to a model
trained on only real-world data, significantly boosting generalized performance
on hold-out data. Finally, this work introduces a novel, open-source
re-identification dataset, pallet-block-2696. This dataset contains 2,696
images of Euro pallets, taken over a period of 4 months. During this time,
natural aging processes occurred and some of the pallets were damaged during
their usage. These wear and tear processes significantly changed the appearance
of the pallets, providing a dataset that can be used to generate synthetically
aged pallets or other wooden materials.

</details>

### [163] [Task-Oriented Communications for Visual Navigation with Edge-Aerial Collaboration in Low Altitude Economy](https://arxiv.org/abs/2504.18317)
*Zhengru Fang,Zhenghao Liu,Jingjing Wang,Senkang Hu,Yu Guo,Yiqin Deng,Yuguang Fang*

Main category: cs.CV

TLDR: 论文提出了一种任务导向的通信框架O-VIB，用于无人机在无GPS信号的城市环境中高效定位，通过多视角特征提取和边缘服务器卸载任务，实现高精度定位。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在无GPS信号的城区定位问题，同时应对轻量级无人机在带宽、存储和处理能力上的限制。

Method: 提出O-VIB编码器，结合自动相关性确定（ARD）剪除非信息特征，并通过正交约束减少冗余，实现高效特征传输。

Result: 在专用LAE无人机数据集上，O-VIB在严格带宽限制下实现了高精度定位。

Conclusion: O-VIB框架为无人机在复杂环境中的定位提供了高效解决方案，代码和数据集将公开。

Abstract: To support the Low Altitude Economy (LAE), precise unmanned aerial vehicles
(UAVs) localization in urban areas where global positioning system (GPS)
signals are unavailable. Vision-based methods offer a viable alternative but
face severe bandwidth, memory and processing constraints on lightweight UAVs.
Inspired by mammalian spatial cognition, we propose a task-oriented
communication framework, where UAVs equipped with multi-camera systems extract
compact multi-view features and offload localization tasks to edge servers. We
introduce the Orthogonally-constrained Variational Information Bottleneck
encoder (O-VIB), which incorporates automatic relevance determination (ARD) to
prune non-informative features while enforcing orthogonality to minimize
redundancy. This enables efficient and accurate localization with minimal
transmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows
that O-VIB achieves high-precision localization under stringent bandwidth
budgets. Code and dataset will be made publicly available:
github.com/fangzr/TOC-Edge-Aerial.

</details>

### [164] [STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D Gaussian Splatting](https://arxiv.org/abs/2504.18318)
*Yunze Deng,Haijun Xiong,Bin Feng,Xinggang Wang,Wenyu Liu*

Main category: cs.CV

TLDR: STP4D是一种新颖的文本到4D生成方法，通过整合时空提示一致性建模，解决了现有方法在时空建模和提示对齐上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本到4D生成方法缺乏统一的时空建模和提示对齐框架，导致时间不一致、几何失真或低质量内容。

Method: STP4D采用三个模块：时变提示嵌入、几何信息增强和时间扩展变形，并结合扩散模型生成4D高斯分布。

Result: 实验表明，STP4D能以高效速度（约4.6秒/资产）生成高质量4D内容，质量和速度均优于现有方法。

Conclusion: STP4D通过统一框架实现了高质量的文本到4D生成，为相关领域提供了新思路。

Abstract: Text-to-4D generation is rapidly developing and widely applied in various
scenarios. However, existing methods often fail to incorporate adequate
spatio-temporal modeling and prompt alignment within a unified framework,
resulting in temporal inconsistencies, geometric distortions, or low-quality 4D
content that deviates from the provided texts. Therefore, we propose STP4D, a
novel approach that aims to integrate comprehensive spatio-temporal-prompt
consistency modeling for high-quality text-to-4D generation. Specifically,
STP4D employs three carefully designed modules: Time-varying Prompt Embedding,
Geometric Information Enhancement, and Temporal Extension Deformation, which
collaborate to accomplish this goal. Furthermore, STP4D is among the first
methods to exploit the Diffusion model to generate 4D Gaussians, combining the
fine-grained modeling capabilities and the real-time rendering process of 4DGS
with the rapid inference speed of the Diffusion model. Extensive experiments
demonstrate that STP4D excels in generating high-fidelity 4D content with
exceptional efficiency (approximately 4.6s per asset), surpassing existing
methods in both quality and speed.

</details>

### [165] [Depth3DLane: Monocular 3D Lane Detection via Depth Prior Distillation](https://arxiv.org/abs/2504.18325)
*Dongxin Lyu,Han Huang,Cheng Tan,Zimu Li*

Main category: cs.CV

TLDR: 提出了一种基于BEV的单目3D车道检测框架，通过多尺度深度特征和深度先验蒸馏提升检测精度，并利用条件随机场优化车道连续性。


<details>
  <summary>Details</summary>
Motivation: 单目3D车道检测因缺乏深度信息而困难，传统IPM方法因平面假设和上下文信息丢失导致高度重建不准确。

Method: 结合分层深度感知头提供多尺度深度特征，利用深度先验蒸馏传递语义深度知识，并通过条件随机场模块优化车道连续性。

Result: 在z轴误差和整体性能上优于现有方法，达到SOTA。

Conclusion: 该方法有效解决了单目3D车道检测中的深度重建问题，提升了检测精度和连续性。

Abstract: Monocular 3D lane detection is challenging due to the difficulty in capturing
depth information from single-camera images. A common strategy involves
transforming front-view (FV) images into bird's-eye-view (BEV) space through
inverse perspective mapping (IPM), facilitating lane detection using BEV
features. However, IPM's flat-ground assumption and loss of contextual
information lead to inaccuracies in reconstructing 3D information, especially
height. In this paper, we introduce a BEV-based framework to address these
limitations and improve 3D lane detection accuracy. Our approach incorporates a
Hierarchical Depth-Aware Head that provides multi-scale depth features,
mitigating the flat-ground assumption by enhancing spatial awareness across
varying depths. Additionally, we leverage Depth Prior Distillation to transfer
semantic depth knowledge from a teacher model, capturing richer structural and
contextual information for complex lane structures. To further refine lane
continuity and ensure smooth lane reconstruction, we introduce a Conditional
Random Field module that enforces spatial coherence in lane predictions.
Extensive experiments validate that our method achieves state-of-the-art
performance in terms of z-axis error and outperforms other methods in the field
in overall performance. The code is released at:
https://anonymous.4open.science/r/Depth3DLane-DCDD.

</details>

### [166] [SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse Observations](https://arxiv.org/abs/2504.18332)
*Shuting Zhao,Linxin Bai,Liangjing Shao,Ye Zhang,Xinrong Chen*

Main category: cs.CV

TLDR: SSD-Poser是一种轻量高效的模型，用于从稀疏观测中实现实时全身姿态估计，结合了状态空间注意力编码器和频率感知解码器，显著提升了运动平滑性和计算效率。


<details>
  <summary>Details</summary>
Motivation: AR/VR应用中，从HMDs实时估计全身姿态的需求增加，但现有方法在精度和推理速度之间难以平衡。

Method: 设计了SSD-Poser模型，采用混合编码器（状态空间注意力编码器）和频率感知解码器，以处理复杂运动并减少抖动。

Result: 在AMASS数据集上的实验显示，SSD-Poser在精度和计算效率上优于现有方法。

Conclusion: SSD-Poser在实时全身姿态估计任务中表现出色，平衡了精度和速度。

Abstract: The growing applications of AR/VR increase the demand for real-time full-body
pose estimation from Head-Mounted Displays (HMDs). Although HMDs provide joint
signals from the head and hands, reconstructing a full-body pose remains
challenging due to the unconstrained lower body. Recent advancements often rely
on conventional neural networks and generative models to improve performance in
this task, such as Transformers and diffusion models. However, these approaches
struggle to strike a balance between achieving precise pose reconstruction and
maintaining fast inference speed. To overcome these challenges, a lightweight
and efficient model, SSD-Poser, is designed for robust full-body motion
estimation from sparse observations. SSD-Poser incorporates a well-designed
hybrid encoder, State Space Attention Encoders, to adapt the state space
duality to complex motion poses and enable real-time realistic pose
reconstruction. Moreover, a Frequency-Aware Decoder is introduced to mitigate
jitter caused by variable-frequency motion signals, remarkably enhancing the
motion smoothness. Comprehensive experiments on the AMASS dataset demonstrate
that SSD-Poser achieves exceptional accuracy and computational efficiency,
showing outstanding inference efficiency compared to state-of-the-art methods.

</details>

### [167] [Interpretable Affordance Detection on 3D Point Clouds with Probabilistic Prototypes](https://arxiv.org/abs/2504.18355)
*Maximilian Xiling Li,Korbinian Rudolf,Nils Blank,Rudolf Lioutikov*

Main category: cs.CV

TLDR: 将原型学习应用于3D点云的功能检测，提供可解释性且性能与黑盒模型相当。


<details>
  <summary>Details</summary>
Motivation: 机器人需要理解如何与环境中的物体交互，但现有黑盒模型缺乏决策透明度。

Method: 采用原型学习方法（如ProtoPNet）于3D点云功能检测，替代传统黑盒模型。

Result: 在3D-AffordanceNet数据集上，原型模型性能与先进黑盒模型相当，且具有可解释性。

Conclusion: 原型模型因其可解释性和性能，适用于需要信任和安全的人机交互场景。

Abstract: Robotic agents need to understand how to interact with objects in their
environment, both autonomously and during human-robot interactions. Affordance
detection on 3D point clouds, which identifies object regions that allow
specific interactions, has traditionally relied on deep learning models like
PointNet++, DGCNN, or PointTransformerV3. However, these models operate as
black boxes, offering no insight into their decision-making processes.
Prototypical Learning methods, such as ProtoPNet, provide an interpretable
alternative to black-box models by employing a "this looks like that"
case-based reasoning approach. However, they have been primarily applied to
image-based tasks. In this work, we apply prototypical learning to models for
affordance detection on 3D point clouds. Experiments on the 3D-AffordanceNet
benchmark dataset show that prototypical models achieve competitive performance
with state-of-the-art black-box models and offer inherent interpretability.
This makes prototypical models a promising candidate for human-robot
interaction scenarios that require increased trust and safety.

</details>

### [168] [COCO-Inpaint: A Benchmark for Image Inpainting Detection and Manipulation Localization](https://arxiv.org/abs/2504.18361)
*Haozhen Yan,Yan Hong,Jiahui Zhan,Yikun Ji,Jun Lan,Huijia Zhu,Weiqiang Wang,Jianfu Zhang*

Main category: cs.CV

TLDR: 论文提出了COCOInpaint基准，专注于检测基于修复的图像篡改，填补了现有IMDL方法在修复篡改检测上的空白。


<details>
  <summary>Details</summary>
Motivation: 现有图像篡改检测方法主要关注拼接或复制移动伪造，缺乏针对修复篡改的专用基准，因此需要填补这一空白。

Method: 构建COCOInpaint基准，包含高质量修复样本、多样化生成场景和大规模覆盖，强调修复区域与真实区域的内在不一致性。

Result: 提供了258,266张修复图像，支持多种生成策略，并建立了严格的评估协议。

Conclusion: COCOInpaint基准将公开，以促进修复篡改检测的未来研究。

Abstract: Recent advancements in image manipulation have achieved unprecedented
progress in generating photorealistic content, but also simultaneously
eliminating barriers to arbitrary manipulation and editing, raising concerns
about multimedia authenticity and cybersecurity. However, existing Image
Manipulation Detection and Localization (IMDL) methodologies predominantly
focus on splicing or copy-move forgeries, lacking dedicated benchmarks for
inpainting-based manipulations. To bridge this gap, we present COCOInpaint, a
comprehensive benchmark specifically designed for inpainting detection, with
three key contributions: 1) High-quality inpainting samples generated by six
state-of-the-art inpainting models, 2) Diverse generation scenarios enabled by
four mask generation strategies with optional text guidance, and 3) Large-scale
coverage with 258,266 inpainted images with rich semantic diversity. Our
benchmark is constructed to emphasize intrinsic inconsistencies between
inpainted and authentic regions, rather than superficial semantic artifacts
such as object shapes. We establish a rigorous evaluation protocol using three
standard metrics to assess existing IMDL approaches. The dataset will be made
publicly available to facilitate future research in this area.

</details>

### [169] [Fast Autoregressive Models for Continuous Latent Generation](https://arxiv.org/abs/2504.18391)
*Tiankai Hang,Jianmin Bao,Fangyun Wei,Dong Chen*

Main category: cs.CV

TLDR: FAR模型通过替换MAR的扩散头为轻量级快捷头，实现了高效采样，同时保持自回归特性，推理速度提升2.3倍。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在连续域图像生成中面临计算成本高的问题，MAR模型因迭代去噪过程导致推理速度慢。

Method: 提出FAR模型，用轻量级快捷头替代MAR的扩散头，支持高效少步采样，并与因果Transformer无缝集成。

Result: FAR推理速度比MAR快2.3倍，同时保持竞争力的FID和IS分数。

Conclusion: FAR首次实现了高效自回归范式，填补了视觉自回归建模中质量与可扩展性的关键空白。

Abstract: Autoregressive models have demonstrated remarkable success in sequential data
generation, particularly in NLP, but their extension to continuous-domain image
generation presents significant challenges. Recent work, the masked
autoregressive model (MAR), bypasses quantization by modeling per-token
distributions in continuous spaces using a diffusion head but suffers from slow
inference due to the high computational cost of the iterative denoising
process. To address this, we propose the Fast AutoRegressive model (FAR), a
novel framework that replaces MAR's diffusion head with a lightweight shortcut
head, enabling efficient few-step sampling while preserving autoregressive
principles. Additionally, FAR seamlessly integrates with causal Transformers,
extending them from discrete to continuous token generation without requiring
architectural modifications. Experiments demonstrate that FAR achieves
$2.3\times$ faster inference than MAR while maintaining competitive FID and IS
scores. This work establishes the first efficient autoregressive paradigm for
high-fidelity continuous-space image generation, bridging the critical gap
between quality and scalability in visual autoregressive modeling.

</details>

### [170] [Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization](https://arxiv.org/abs/2504.18397)
*Kesen Zhao,Beier Zhu,Qianru Sun,Hanwang Zhang*

Main category: cs.CV

TLDR: UV-CoT是一种无监督视觉链式思维推理框架，通过偏好优化提升多模态大语言模型的视觉理解能力，无需标注边界框数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注文本链式思维，忽略了视觉线索的利用，且依赖有监督微调，难以泛化到未见案例。

Method: 通过自动生成偏好数据（模型生成的边界框及其响应排名），利用偏好优化训练目标模型，无需边界框标注。

Result: 在六个数据集上表现优于现有文本和视觉链式思维方法，并在四个未见数据集上展示了强泛化能力。

Conclusion: UV-CoT通过模拟人类感知（识别关键区域并基于其推理），显著提升了视觉理解能力，尤其在空间推理任务中。

Abstract: Chain-of-thought (CoT) reasoning greatly improves the interpretability and
problem-solving abilities of multimodal large language models (MLLMs). However,
existing approaches are focused on text CoT, limiting their ability to leverage
visual cues. Visual CoT remains underexplored, and the only work is based on
supervised fine-tuning (SFT) that relies on extensive labeled bounding-box data
and is hard to generalize to unseen cases. In this paper, we introduce
Unsupervised Visual CoT (UV-CoT), a novel framework for image-level CoT
reasoning via preference optimization. UV-CoT performs preference comparisons
between model-generated bounding boxes (one is preferred and the other is
dis-preferred), eliminating the need for bounding-box annotations. We get such
preference data by introducing an automatic data generation pipeline. Given an
image, our target MLLM (e.g., LLaVA-1.5-7B) generates seed bounding boxes using
a template prompt and then answers the question using each bounded region as
input. An evaluator MLLM (e.g., OmniLLM-12B) ranks the responses, and these
rankings serve as supervision to train the target MLLM with UV-CoT by
minimizing negative log-likelihood losses. By emulating human
perception--identifying key regions and reasoning based on them--UV-CoT can
improve visual comprehension, particularly in spatial reasoning tasks where
textual descriptions alone fall short. Our experiments on six datasets
demonstrate the superiority of UV-CoT, compared to the state-of-the-art textual
and visual CoT methods. Our zero-shot testing on four unseen datasets shows the
strong generalization of UV-CoT. The code is available in
https://github.com/kesenzhao/UV-CoT.

</details>

### [171] [A Multimodal Hybrid Late-Cascade Fusion Network for Enhanced 3D Object Detection](https://arxiv.org/abs/2504.18419)
*Carlo Sgaravatti,Roberto Basla,Riccardo Pieroni,Matteo Corno,Sergio M. Savaresi,Luca Magri,Giacomo Boracchi*

Main category: cs.CV

TLDR: 提出了一种基于多模态输入（LiDAR和RGB相机）的3D物体检测新方法，通过混合级联方案减少LiDAR的误报和漏报。


<details>
  <summary>Details</summary>
Motivation: 解决LiDAR检测中的误报和漏报问题，提升3D物体检测性能。

Method: 采用混合级联方案，结合RGB检测网络和3D LiDAR检测器，利用后期融合减少误报，通过级联融合恢复漏报。

Result: 在KITTI基准测试中表现优异，尤其在行人和骑行者检测上显著提升。

Conclusion: 该方法灵活且高效，可适配多种单模态检测器，显著提升多模态检测性能。

Abstract: We present a new way to detect 3D objects from multimodal inputs, leveraging
both LiDAR and RGB cameras in a hybrid late-cascade scheme, that combines an
RGB detection network and a 3D LiDAR detector. We exploit late fusion
principles to reduce LiDAR False Positives, matching LiDAR detections with RGB
ones by projecting the LiDAR bounding boxes on the image. We rely on cascade
fusion principles to recover LiDAR False Negatives leveraging epipolar
constraints and frustums generated by RGB detections of separate views. Our
solution can be plugged on top of any underlying single-modal detectors,
enabling a flexible training process that can take advantage of pre-trained
LiDAR and RGB detectors, or train the two branches separately. We evaluate our
results on the KITTI object detection benchmark, showing significant
performance improvements, especially for the detection of Pedestrians and
Cyclists.

</details>

### [172] [LaRI: Layered Ray Intersections for Single-view 3D Geometric Reasoning](https://arxiv.org/abs/2504.18424)
*Rui Li,Biao Zhang,Zhenyu Li,Federico Tombari,Peter Wonka*

Main category: cs.CV

TLDR: LaRI是一种从单张图像推断未见几何结构的新方法，通过分层点图建模相机光线相交的多个表面，实现高效、完整的几何推理。


<details>
  <summary>Details</summary>
Motivation: 传统深度估计仅能处理可见表面，而LaRI旨在解决未见几何结构的推理问题，统一对象和场景级任务。

Method: LaRI使用分层点图建模光线相交的多个表面，并预测光线停止索引以识别有效像素和层。同时，构建了合成和真实数据的训练生成流程。

Result: LaRI在对象级任务中仅用4%的训练数据和17%的参数达到与大型生成模型相当的效果，并在场景级任务中实现单次前馈的遮挡几何推理。

Conclusion: LaRI作为一种通用方法，在高效性和性能上表现出色，适用于对象和场景级的几何推理任务。

Abstract: We present layered ray intersections (LaRI), a new method for unseen geometry
reasoning from a single image. Unlike conventional depth estimation that is
limited to the visible surface, LaRI models multiple surfaces intersected by
the camera rays using layered point maps. Benefiting from the compact and
layered representation, LaRI enables complete, efficient, and view-aligned
geometric reasoning to unify object- and scene-level tasks. We further propose
to predict the ray stopping index, which identifies valid intersecting pixels
and layers from LaRI's output. We build a complete training data generation
pipeline for synthetic and real-world data, including 3D objects and scenes,
with necessary data cleaning steps and coordination between rendering engines.
As a generic method, LaRI's performance is validated in two scenarios: It
yields comparable object-level results to the recent large generative model
using 4% of its training data and 17% of its parameters. Meanwhile, it achieves
scene-level occluded geometry reasoning in only one feed-forward.

</details>

### [173] [Iterative Event-based Motion Segmentation by Variational Contrast Maximization](https://arxiv.org/abs/2504.18447)
*Ryo Yamaki,Shintaro Shiba,Guillermo Gallego,Yoshimitsu Aoki*

Main category: cs.CV

TLDR: 提出了一种基于事件相机的迭代运动分割方法，通过将事件分类为背景和前景，扩展了对比度最大化框架，显著提高了运动物体检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 事件相机能够捕捉场景变化，但需要将事件数据分类为不同运动以实现运动分割，这对物体检测和视觉伺服等任务至关重要。

Method: 提出了一种迭代运动分割方法，将事件分为背景（主导运动假设）和前景（独立运动残差），扩展了对比度最大化框架。

Result: 实验结果表明，该方法成功分类了公共和自录数据集中的事件簇，生成了清晰的运动补偿边缘图像，并在运动物体检测基准上实现了超过30%的准确率提升。

Conclusion: 该方法扩展了对比度最大化框架的敏感性，为基于事件的运动分割理论提供了新的进展。

Abstract: Event cameras provide rich signals that are suitable for motion estimation
since they respond to changes in the scene. As any visual changes in the scene
produce event data, it is paramount to classify the data into different motions
(i.e., motion segmentation), which is useful for various tasks such as object
detection and visual servoing. We propose an iterative motion segmentation
method, by classifying events into background (e.g., dominant motion
hypothesis) and foreground (independent motion residuals), thus extending the
Contrast Maximization framework. Experimental results demonstrate that the
proposed method successfully classifies event clusters both for public and
self-recorded datasets, producing sharp, motion-compensated edge-like images.
The proposed method achieves state-of-the-art accuracy on moving object
detection benchmarks with an improvement of over 30%, and demonstrates its
possibility of applying to more complex and noisy real-world scenes. We hope
this work broadens the sensitivity of Contrast Maximization with respect to
both motion parameters and input events, thus contributing to theoretical
advancements in event-based motion segmentation estimation.
https://github.com/aoki-media-lab/event_based_segmentation_vcmax

</details>

### [174] [NoiseController: Towards Consistent Multi-view Video Generation via Noise Decomposition and Collaboration](https://arxiv.org/abs/2504.18448)
*Haotian Dong,Xin Wang,Di Lin,Yipeng Wu,Qin Chen,Ruonan Liu,Kairui Yang,Ping Li,Qing Guo*

Main category: cs.CV

TLDR: 本文提出NoiseController方法，通过多级噪声分解、多帧噪声协作和联合去噪，提升视频生成的时空一致性。


<details>
  <summary>Details</summary>
Motivation: 高质量视频生成对电影和自动驾驶等领域至关重要，但现有方法常忽略全局时空信息，导致时空一致性不足。

Method: 采用多级噪声分解（场景级和个体级）、多帧噪声协作（时空协作矩阵）和联合去噪（并行U-Net）来增强一致性。

Result: 在公开数据集上验证了NoiseController的先进性能。

Conclusion: NoiseController有效提升了视频生成的时空一致性，适用于多种下游任务。

Abstract: High-quality video generation is crucial for many fields, including the film
industry and autonomous driving. However, generating videos with spatiotemporal
consistencies remains challenging. Current methods typically utilize attention
mechanisms or modify noise to achieve consistent videos, neglecting global
spatiotemporal information that could help ensure spatial and temporal
consistency during video generation. In this paper, we propose the
NoiseController, consisting of Multi-Level Noise Decomposition, Multi-Frame
Noise Collaboration, and Joint Denoising, to enhance spatiotemporal
consistencies in video generation. In multi-level noise decomposition, we first
decompose initial noises into scene-level foreground/background noises,
capturing distinct motion properties to model multi-view foreground/background
variations. Furthermore, each scene-level noise is further decomposed into
individual-level shared and residual components. The shared noise preserves
consistency, while the residual component maintains diversity. In multi-frame
noise collaboration, we introduce an inter-view spatiotemporal collaboration
matrix and an intra-view impact collaboration matrix , which captures mutual
cross-view effects and historical cross-frame impacts to enhance video quality.
The joint denoising contains two parallel denoising U-Nets to remove each
scene-level noise, mutually enhancing video generation. We evaluate our
NoiseController on public datasets focusing on video generation and downstream
tasks, demonstrating its state-of-the-art performance.

</details>

### [175] [RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny Objects](https://arxiv.org/abs/2504.18468)
*Georgios Kouros,Minye Wu,Tinne Tuytelaars*

Main category: cs.CV

TLDR: RGS-DR是一种新的逆渲染方法，专注于重建和渲染具有光泽和反射特性的物体，支持灵活的重新照明和场景编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如NeRF和3D高斯泼溅）在处理视角依赖效果时表现不佳，RGS-DR旨在解决这一问题。

Method: 采用2D高斯面元表示法估计几何和表面法线，通过可学习基元建模几何和材质属性，并使用多级立方体mipmap近似环境光照积分。

Result: 实验表明，RGS-DR在光泽物体的高质量重建和渲染方面表现优异，优于无法重新照明的最新方法。

Conclusion: RGS-DR通过创新的表示和渲染技术，显著提升了光泽物体的逆渲染质量。

Abstract: We introduce RGS-DR, a novel inverse rendering method for reconstructing and
rendering glossy and reflective objects with support for flexible relighting
and scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian
Splatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D
Gaussian surfel representation to accurately estimate geometry and surface
normals, an essential property for high-quality inverse rendering. Our approach
explicitly models geometric and material properties through learnable
primitives rasterized into a deferred shading pipeline, effectively reducing
rendering artifacts and preserving sharp reflections. By employing a
multi-level cube mipmap, RGS-DR accurately approximates environment lighting
integrals, facilitating high-quality reconstruction and relighting. A residual
pass with spherical-mipmap-based directional encoding further refines the
appearance modeling. Experiments demonstrate that RGS-DR achieves high-quality
reconstruction and rendering quality for shiny objects, often outperforming
reconstruction-exclusive state-of-the-art methods incapable of relighting.

</details>

### [176] [An Improved ResNet50 Model for Predicting Pavement Condition Index (PCI) Directly from Pavement Images](https://arxiv.org/abs/2504.18490)
*Andrews Danyo,Anthony Dontoh,Armstrong Aboah*

Main category: cs.CV

TLDR: 提出了一种改进的ResNet50-CBAM模型，用于直接从路面图像预测PCI，显著降低了误差。


<details>
  <summary>Details</summary>
Motivation: 准确预测路面状况指数（PCI）对基础设施维护至关重要。

Method: 结合CBAM的ResNet50架构，自主优先提取关键特征。

Result: 改进模型的MAPE为58.16%，优于基线模型的70.76%和65.48%。

Conclusion: 注意力机制可提升特征提取精度，推动自动化路面分析。

Abstract: Accurately predicting the Pavement Condition Index (PCI), a measure of
roadway conditions, from pavement images is crucial for infrastructure
maintenance. This study proposes an enhanced version of the Residual Network
(ResNet50) architecture, integrated with a Convolutional Block Attention Module
(CBAM), to predict PCI directly from pavement images without additional
annotations. By incorporating CBAM, the model autonomously prioritizes critical
features within the images, improving prediction accuracy. Compared to the
original baseline ResNet50 and DenseNet161 architectures, the enhanced
ResNet50-CBAM model achieved a significantly lower mean absolute percentage
error (MAPE) of 58.16%, compared to the baseline models that achieved 70.76%
and 65.48% respectively. These results highlight the potential of using
attention mechanisms to refine feature extraction, ultimately enabling more
accurate and efficient assessments of pavement conditions. This study
emphasizes the importance of targeted feature refinement in advancing automated
pavement analysis through attention mechanisms.

</details>

### [177] [Eval3D: Interpretable and Fine-grained Evaluation for 3D Generation](https://arxiv.org/abs/2504.18509)
*Shivam Duggal,Yushi Hu,Oscar Michel,Aniruddha Kembhavi,William T. Freeman,Noah A. Smith,Ranjay Krishna,Antonio Torralba,Ali Farhadi,Wei-Chiu Ma*

Main category: cs.CV

TLDR: Eval3D是一种细粒度、可解释的3D生成评估工具，通过多模型一致性检测提升评估质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成系统常无法保证多视角下的几何与语义一致性，且缺乏可靠的评估工具。

Method: 利用多种基础模型和工具作为探针，检测生成3D资产在不同方面的一致性。

Result: Eval3D提供像素级测量、精确3D空间反馈，并更贴近人类判断。

Conclusion: Eval3D揭示了当前3D生成模型的局限性，为改进提供了方向。

Abstract: Despite the unprecedented progress in the field of 3D generation, current
systems still often fail to produce high-quality 3D assets that are visually
appealing and geometrically and semantically consistent across multiple
viewpoints. To effectively assess the quality of the generated 3D data, there
is a need for a reliable 3D evaluation tool. Unfortunately, existing 3D
evaluation metrics often overlook the geometric quality of generated assets or
merely rely on black-box multimodal large language models for coarse
assessment. In this paper, we introduce Eval3D, a fine-grained, interpretable
evaluation tool that can faithfully evaluate the quality of generated 3D assets
based on various distinct yet complementary criteria. Our key observation is
that many desired properties of 3D generation, such as semantic and geometric
consistency, can be effectively captured by measuring the consistency among
various foundation models and tools. We thus leverage a diverse set of models
and tools as probes to evaluate the inconsistency of generated 3D assets across
different aspects. Compared to prior work, Eval3D provides pixel-wise
measurement, enables accurate 3D spatial feedback, and aligns more closely with
human judgments. We comprehensively evaluate existing 3D generation models
using Eval3D and highlight the limitations and challenges of current models.

</details>

### [178] [Examining the Impact of Optical Aberrations to Image Classification and Object Detection Models](https://arxiv.org/abs/2504.18510)
*Patrick Müller,Alexander Braun,Margret Keuper*

Main category: cs.CV

TLDR: 论文提出了两个数据集（OpticsBench和LensCorruptions）来评估模型对真实光学模糊效果的鲁棒性，填补了现有基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试对模糊的模拟过于简化，忽略了光学系统中不同模糊核形状的影响，无法全面评估模型对真实模糊的鲁棒性。

Method: 通过Zernike多项式参数化光学像差，构建了两个数据集：OpticsBench（单一参数像差）和LensCorruptions（100种真实镜头的线性组合）。

Result: 在ImageNet和MSCOCO上的实验表明，不同预训练模型在数据集上的性能差异显著，说明真实模糊对模型鲁棒性评估的重要性。

Conclusion: 研究强调了考虑真实图像模糊的必要性，为模型鲁棒性评估提供了更全面的基准。

Abstract: Deep neural networks (DNNs) have proven to be successful in various computer
vision applications such that models even infer in safety-critical situations.
Therefore, vision models have to behave in a robust way to disturbances such as
noise or blur. While seminal benchmarks exist to evaluate model robustness to
diverse corruptions, blur is often approximated in an overly simplistic way to
model defocus, while ignoring the different blur kernel shapes that result from
optical systems. To study model robustness against realistic optical blur
effects, this paper proposes two datasets of blur corruptions, which we denote
OpticsBench and LensCorruptions. OpticsBench examines primary aberrations such
as coma, defocus, and astigmatism, i.e. aberrations that can be represented by
varying a single parameter of Zernike polynomials. To go beyond the principled
but synthetic setting of primary aberrations, LensCorruptions samples linear
combinations in the vector space spanned by Zernike polynomials, corresponding
to 100 real lenses. Evaluations for image classification and object detection
on ImageNet and MSCOCO show that for a variety of different pre-trained models,
the performance on OpticsBench and LensCorruptions varies significantly,
indicating the need to consider realistic image corruptions to evaluate a
model's robustness against blur.

</details>

### [179] [E-VLC: A Real-World Dataset for Event-based Visible Light Communication And Localization](https://arxiv.org/abs/2504.18521)
*Shintaro Shiba,Quan Kong,Norimasa Kobori*

Main category: cs.CV

TLDR: 论文提出了首个公开数据集，用于评估事件相机在LED信号解码和定位中的性能，并提出了基于对比度最大化的新型定位方法。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏公开数据集来评估事件相机在LED信号解码和定位中的性能，尤其是在不同现实场景下的表现。

Method: 提出了一个包含事件相机、帧相机和精确同步地面真实姿态的数据集，并提出了一种基于对比度最大化框架的新型定位方法。

Result: 实验结果表明，基于事件的LED定位优于传统的基于帧的AR标记定位，且提出的方法在定位中表现高效。

Conclusion: 该数据集有望成为未来计算机视觉任务和LED解码任务的基准，推动事件相机在移动设备上的广泛应用。

Abstract: Optical communication using modulated LEDs (e.g., visible light
communication) is an emerging application for event cameras, thanks to their
high spatio-temporal resolutions. Event cameras can be used simply to decode
the LED signals and also to localize the camera relative to the LED marker
positions. However, there is no public dataset to benchmark the decoding and
localization in various real-world settings. We present, to the best of our
knowledge, the first public dataset that consists of an event camera, a frame
camera, and ground-truth poses that are precisely synchronized with hardware
triggers. It provides various camera motions with various sensitivities in
different scene brightness settings, both indoor and outdoor. Furthermore, we
propose a novel method of localization that leverages the Contrast Maximization
framework for motion estimation and compensation. The detailed analysis and
experimental results demonstrate the advantages of LED-based localization with
events over the conventional AR-marker--based one with frames, as well as the
efficacy of the proposed method in localization. We hope that the proposed
dataset serves as a future benchmark for both motion-related classical computer
vision tasks and LED marker decoding tasks simultaneously, paving the way to
broadening applications of event cameras on mobile devices.
https://woven-visionai.github.io/evlc-dataset

</details>

### [180] [Augmenting Perceptual Super-Resolution via Image Quality Predictors](https://arxiv.org/abs/2504.18524)
*Fengjia Zhang,Samrudhdhi B. Rangrej,Tristan Aumentado-Armstrong,Afsaneh Fazly,Alex Levinshtein*

Main category: cs.CV

TLDR: 论文探讨了在超分辨率（SR）任务中利用非参考图像质量评估（NR-IQA）模型的方法，以改善感知质量与失真之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 超分辨率问题本质上是病态的，传统方法倾向于生成模糊图像，而实际需求是高质量的样本。

Method: 分析了NR-IQA指标在人类生成SR数据上的表现，并探索了两种应用方法：改变数据采样和直接优化可微分质量分数。

Result: 实验结果表明，该方法在感知质量与失真之间取得了更符合人类视觉的平衡。

Conclusion: 利用NR-IQA模型可以更有效地优化SR任务的感知质量。

Abstract: Super-resolution (SR), a classical inverse problem in computer vision, is
inherently ill-posed, inducing a distribution of plausible solutions for every
input. However, the desired result is not simply the expectation of this
distribution, which is the blurry image obtained by minimizing pixelwise error,
but rather the sample with the highest image quality. A variety of techniques,
from perceptual metrics to adversarial losses, are employed to this end. In
this work, we explore an alternative: utilizing powerful non-reference image
quality assessment (NR-IQA) models in the SR context. We begin with a
comprehensive analysis of NR-IQA metrics on human-derived SR data, identifying
both the accuracy (human alignment) and complementarity of different metrics.
Then, we explore two methods of applying NR-IQA models to SR learning: (i)
altering data sampling, by building on an existing multi-ground-truth SR
framework, and (ii) directly optimizing a differentiable quality score. Our
results demonstrate a more human-centric perception-distortion tradeoff,
focusing less on non-perceptual pixel-wise distortion, instead improving the
balance between perceptual fidelity and human-tuned NR-IQA measures.

</details>

<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [181] [Outlier-aware Tensor Robust Principal Component Analysis with Self-guided Data Augmentation](https://arxiv.org/abs/2504.18323)
*Yangyang Xu,Kexin Li,Li Yang,You-Wei Wen*

Main category: math.NA

TLDR: 提出了一种自引导数据增强方法，通过自适应加权抑制异常值影响，将TRPCA问题转化为标准TPCA问题，提高了处理结构化损坏的能力。


<details>
  <summary>Details</summary>
Motivation: 现有TRPCA方法依赖稀疏异常值假设，在结构化损坏下表现不佳。

Method: 采用优化驱动的加权方案，动态识别并降低异常值贡献，开发了高效的近端块坐标下降算法。

Result: 在合成和真实数据集上验证了方法的有效性，包括人脸恢复、背景减除和高光谱去噪。

Conclusion: 方法在准确性和计算效率上优于现有技术。

Abstract: Tensor Robust Principal Component Analysis (TRPCA) is a fundamental technique
for decomposing multi-dimensional data into a low-rank tensor and an outlier
tensor, yet existing methods relying on sparse outlier assumptions often fail
under structured corruptions. In this paper, we propose a self-guided data
augmentation approach that employs adaptive weighting to suppress outlier
influence, reformulating the original TRPCA problem into a standard Tensor
Principal Component Analysis (TPCA) problem. The proposed model involves an
optimization-driven weighting scheme that dynamically identifies and
downweights outlier contributions during tensor augmentation. We develop an
efficient proximal block coordinate descent algorithm with closed-form updates
to solve the resulting optimization problem, ensuring computational efficiency.
Theoretical convergence is guaranteed through a framework combining block
coordinate descent with majorization-minimization principles. Numerical
experiments on synthetic and real-world datasets, including face recovery,
background subtraction, and hyperspectral denoising, demonstrate that our
method effectively handles various corruption patterns. The results show the
improvements in both accuracy and computational efficiency compared to
state-of-the-art methods.

</details>

### [182] [PODNO: Proper Orthogonal Decomposition Neural Operators](https://arxiv.org/abs/2504.18513)
*Zilan Cheng,Zhongjian Wang,Li-Lian Wang,Mejdi Azaiez*

Main category: math.NA

TLDR: PODNO是一种基于POD方法的神经算子，用于解决高频主导的PDE问题，相比FNO在精度和计算效率上可能更优。


<details>
  <summary>Details</summary>
Motivation: 针对高频主导的PDE问题，传统FNO方法可能表现不佳，因此提出PODNO以改进性能。

Method: 用POD方法替换FNO中的傅里叶变换，构建积分核，并引入广义谱算子（GSO）进行理论分析。

Result: 数值实验表明，PODNO在非线性薛定谔方程和KP方程等色散方程中表现良好。

Conclusion: PODNO在高频PDE问题中具有潜力，理论分析和数值结果均支持其优越性。

Abstract: In this paper, we introduce Proper Orthogonal Decomposition Neural Operators
(PODNO) for solving partial differential equations (PDEs) dominated by
high-frequency components. Building on the structure of Fourier Neural
Operators (FNO), PODNO replaces the Fourier transform with (inverse)
orthonormal transforms derived from the Proper Orthogonal Decomposition (POD)
method to construct the integral kernel. Due to the optimality of POD basis,
the PODNO has potential to outperform FNO in both accuracy and computational
efficiency for high-frequency problems. From analysis point of view, we
established the universality of a generalization of PODNO, termed as
Generalized Spectral Operator (GSO). In addition, we evaluate PODNO's
performance numerically on dispersive equations such as the Nonlinear
Schrodinger (NLS) equation and the Kadomtsev-Petviashvili (KP) equation.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [183] [Flow Matching Ergodic Coverage](https://arxiv.org/abs/2504.17872)
*Max Muchen Sun,Allison Pinosky,Todd Murphey*

Main category: cs.RO

TLDR: 提出了一种基于流匹配的遍历覆盖方法，解决了现有遍历度量在控制合成中的局限性，并引入了生成推理中的新度量。


<details>
  <summary>Details</summary>
Motivation: 现有遍历覆盖方法受限于可用的遍历度量，限制了性能。

Method: 基于流匹配技术，将遍历覆盖问题转化为线性二次调节器问题，并支持生成推理中的新度量。

Result: 新方法在数值基准测试和机器人任务中表现出更高的性能和计算效率。

Conclusion: 该方法扩展了遍历度量的选择，提升了覆盖性能，适用于复杂动态系统。

Abstract: Ergodic coverage effectively generates exploratory behaviors for embodied
agents by aligning the spatial distribution of the agent's trajectory with a
target distribution, where the difference between these two distributions is
measured by the ergodic metric. However, existing ergodic coverage methods are
constrained by the limited set of ergodic metrics available for control
synthesis, fundamentally limiting their performance. In this work, we propose
an alternative approach to ergodic coverage based on flow matching, a technique
widely used in generative inference for efficient and scalable sampling. We
formally derive the flow matching problem for ergodic coverage and show that it
is equivalent to a linear quadratic regulator problem with a closed-form
solution. Our formulation enables alternative ergodic metrics from generative
inference that overcome the limitations of existing ones. These metrics were
previously infeasible for control synthesis but can now be supported with no
computational overhead. Specifically, flow matching with the Stein variational
gradient flow enables control synthesis directly over the score function of the
target distribution, improving robustness to the unnormalized distributions; on
the other hand, flow matching with the Sinkhorn divergence flow enables an
optimal transport-based ergodic metric, improving coverage performance on
non-smooth distributions with irregular supports. We validate the improved
performance and competitive computational efficiency of our method through
comprehensive numerical benchmarks and across different nonlinear dynamics. We
further demonstrate the practicality of our method through a series of drawing
and erasing tasks on a Franka robot.

</details>

### [184] [Beyond Task and Motion Planning: Hierarchical Robot Planning with General-Purpose Policies](https://arxiv.org/abs/2504.17901)
*Benned Hedegaard,Ziyi Yang,Yichen Wei,Ahmed Jaafar,Stefanie Tellex,George Konidaris,Naman Shah*

Main category: cs.RO

TLDR: 论文提出了一种结合运动规划与闭环电机控制器的新方法，通过Composable Interaction Primitives（CIPs）实现分层机器人规划。


<details>
  <summary>Details</summary>
Motivation: 传统任务与运动规划方法假设任务级动作可简化为运动规划，但实际中需考虑更多非运动学因素。

Method: 提出Task and Skill Planning（TASP）方法，利用CIPs将预学习技能集成到运动规划中。

Result: 通过真实场景实验验证，CIPs使移动操作机器人能有效结合运动规划与通用技能完成复杂任务。

Conclusion: TASP方法扩展了传统任务与运动规划的能力，适用于更复杂的机器人任务。

Abstract: Task and motion planning is a well-established approach for solving
long-horizon robot planning problems. However, traditional methods assume that
each task-level robot action, or skill, can be reduced to kinematic motion
planning. In this work, we address the challenge of planning with both
kinematic skills and closed-loop motor controllers that go beyond kinematic
considerations. We propose a novel method that integrates these controllers
into motion planning using Composable Interaction Primitives (CIPs), enabling
the use of diverse, non-composable pre-learned skills in hierarchical robot
planning. Toward validating our Task and Skill Planning (TASP) approach, we
describe ongoing robot experiments in real-world scenarios designed to
demonstrate how CIPs can allow a mobile manipulator robot to effectively
combine motion planning with general-purpose skills to accomplish complex
tasks.

</details>

### [185] [Fuzzy-RRT for Obstacle Avoidance in a 2-DOF Semi-Autonomous Surgical Robotic Arm](https://arxiv.org/abs/2504.17979)
*Kaaustaaub Shankar,Wilhelm Louw,Bharadwaj Dogga,Nick Ernest,Tim Arnett,Kelly Cohen*

Main category: cs.RO

TLDR: AI驱动的半自主机器人手术通过改进的模糊快速探索随机树算法，显著提升了路径搜索时间和成本，适用于太空任务中的手术需求。


<details>
  <summary>Details</summary>
Motivation: 解决长期星际任务中医疗手术的挑战，如有限的船员和通信延迟，传统手术方法受限。

Method: 提出改进的模糊快速探索随机树算法，用于两自由度机器人臂的障碍物避障和协作控制。

Result: 路径搜索时间提升743%，路径成本降低43%。

Conclusion: 该算法显著提升了机器人手术系统的效率和可行性，适用于太空任务。

Abstract: AI-driven semi-autonomous robotic surgery is essential for addressing the
medical challenges of long-duration interplanetary missions, where limited crew
sizes and communication delays restrict traditional surgical approaches.
Current robotic surgery systems require full surgeon control, demanding
extensive expertise and limiting feasibility in space. We propose a novel
adaptation of the Fuzzy Rapidly-exploring Random Tree algorithm for obstacle
avoidance and collaborative control in a two-degree-of-freedom robotic arm
modeled on the Miniaturized Robotic-Assisted surgical system. It was found that
the Fuzzy Rapidly-exploring Random Tree algorithm resulted in an 743 percent
improvement to path search time and 43 percent improvement to path cost.

</details>

### [186] [Sky-Drive: A Distributed Multi-Agent Simulation Platform for Socially-Aware and Human-AI Collaborative Future Transportation](https://arxiv.org/abs/2504.18010)
*Zilin Huang,Zihao Sheng,Zhengyang Wan,Yansong Qu,Yuhao Luo,Boyue Wang,Pei Li,Yen-Jung Chen,Jiancong Chen,Keke Long,Jiayi Meng,Yue Leng,Sikai Chen*

Main category: cs.RO

TLDR: Sky-Drive是一个分布式多智能体仿真平台，通过四项创新技术解决现有仿真平台的不足，支持自动驾驶研究中的社会感知和人机协作。


<details>
  <summary>Details</summary>
Motivation: 现有仿真平台未能满足未来交通研究的需求，特别是在社会感知驾驶智能体和有效人机协作方面。

Method: Sky-Drive采用分布式架构、多模态人机交互框架、人机协作机制和数字孪生技术。

Result: Sky-Drive支持多种应用，如自动驾驶与弱势道路使用者的交互建模、社会感知强化学习等。

Conclusion: Sky-Drive有望成为下一代社会感知和以人为中心的自动驾驶研究的基础平台。

Abstract: Recent advances in autonomous system simulation platforms have significantly
enhanced the safe and scalable testing of driving policies. However, existing
simulators do not yet fully meet the needs of future transportation research,
particularly in modeling socially-aware driving agents and enabling effective
human-AI collaboration. This paper introduces Sky-Drive, a novel distributed
multi-agent simulation platform that addresses these limitations through four
key innovations: (a) a distributed architecture for synchronized simulation
across multiple terminals; (b) a multi-modal human-in-the-loop framework
integrating diverse sensors to collect rich behavioral data; (c) a human-AI
collaboration mechanism supporting continuous and adaptive knowledge exchange;
and (d) a digital twin (DT) framework for constructing high-fidelity virtual
replicas of real-world transportation environments. Sky-Drive supports diverse
applications such as autonomous vehicle (AV)-vulnerable road user (VRU)
interaction modeling, human-in-the-loop training, socially-aware reinforcement
learning, personalized driving policy, and customized scenario generation.
Future extensions will incorporate foundation models for context-aware decision
support and hardware-in-the-loop (HIL) testing for real-world validation. By
bridging scenario generation, data collection, algorithm training, and hardware
integration, Sky-Drive has the potential to become a foundational platform for
the next generation of socially-aware and human-centered autonomous
transportation research. The demo video and code are available
at:https://sky-lab-uw.github.io/Sky-Drive-website/

</details>

### [187] [Opportunistic Collaborative Planning with Large Vision Model Guided Control and Joint Query-Service Optimization](https://arxiv.org/abs/2504.18057)
*Jiayi Chen,Shuai Wang,Guoliang Li,Wei Xu,Guangxu Zhu,Derrick Wing Kwan Ng,Chengzhong Xu*

Main category: cs.RO

TLDR: 本文提出了一种机会协作规划（OCP）方法，通过结合高效本地模型与强大云端模型，解决了自动驾驶在开放场景中的导航问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在开放场景中导航时，难以处理未见过的物体，现有方法要么泛化能力差，要么资源消耗大。协作方法虽有望解决，但关键挑战在于何时及如何调用大模型。

Method: 提出OCP，包含两个创新：1) LVM-MPC，利用云端大模型进行感知与决策，指导本地MPC；2) CTO，通过ODCT和CFS优化协作时机。

Result: 实验表明，OCP在导航时间和成功率上均优于现有方法。

Conclusion: OCP通过智能协作本地与云端模型，有效提升了自动驾驶在开放场景中的导航性能。

Abstract: Navigating autonomous vehicles in open scenarios is a challenge due to the
difficulties in handling unseen objects. Existing solutions either rely on
small models that struggle with generalization or large models that are
resource-intensive. While collaboration between the two offers a promising
solution, the key challenge is deciding when and how to engage the large model.
To address this issue, this paper proposes opportunistic collaborative planning
(OCP), which seamlessly integrates efficient local models with powerful cloud
models through two key innovations. First, we propose large vision model guided
model predictive control (LVM-MPC), which leverages the cloud for LVM
perception and decision making. The cloud output serves as a global guidance
for a local MPC, thereby forming a closed-loop perception-to-control system.
Second, to determine the best timing for large model query and service, we
propose collaboration timing optimization (CTO), including object detection
confidence thresholding (ODCT) and cloud forward simulation (CFS), to decide
when to seek cloud assistance and when to offer cloud service. Extensive
experiments show that the proposed OCP outperforms existing methods in terms of
both navigation time and success rate.

</details>

### [188] [Depth-Constrained ASV Navigation with Deep RL and Limited Sensing](https://arxiv.org/abs/2504.18253)
*Amirhossein Zhalehmehrabi,Daniele Meli,Francesco Dal Santo,Francesco Trotti,Alessandro Farinelli*

Main category: cs.RO

TLDR: 提出了一种基于强化学习的ASV导航框架，结合高斯过程回归，利用稀疏声纳数据估计水深图，提升浅水环境下的导航性能。


<details>
  <summary>Details</summary>
Motivation: 解决ASV在浅水环境中因动态干扰和深度限制导致的导航困难，传统方法因传感器信息有限而难以实现安全高效操作。

Method: 结合强化学习与高斯过程回归，利用单波束测深仪数据逐步估计水深图，提升环境感知能力。

Result: 实验验证了该方法在浅水环境中提升导航性能并确保安全性，同时实现了有效的仿真到现实的迁移。

Conclusion: 提出的框架显著改善了ASV在浅水环境中的导航能力，为实际应用提供了可靠解决方案。

Abstract: Autonomous Surface Vehicles (ASVs) play a crucial role in maritime
operations, yet their navigation in shallow-water environments remains
challenging due to dynamic disturbances and depth constraints. Traditional
navigation strategies struggle with limited sensor information, making safe and
efficient operation difficult. In this paper, we propose a reinforcement
learning (RL) framework for ASV navigation under depth constraints, where the
vehicle must reach a target while avoiding unsafe areas with only a single
depth measurement per timestep from a downward-facing Single Beam Echosounder
(SBES). To enhance environmental awareness, we integrate Gaussian Process (GP)
regression into the RL framework, enabling the agent to progressively estimate
a bathymetric depth map from sparse sonar readings. This approach improves
decision-making by providing a richer representation of the environment.
Furthermore, we demonstrate effective sim-to-real transfer, ensuring that
trained policies generalize well to real-world aquatic conditions. Experimental
results validate our method's capability to improve ASV navigation performance
while maintaining safety in challenging shallow-water environments.

</details>

### [189] [CIVIL: Causal and Intuitive Visual Imitation Learning](https://arxiv.org/abs/2504.17959)
*Yinlong Dai,Robert Ramirez Sanchez,Ryan Jeronimus,Shahabedin Sagheb,Cara M. Nunez,Heramb Nemlekar,Dylan P. Losey*

Main category: cs.RO

TLDR: 论文提出了一种新的视觉模仿学习方法CIVIL，通过结合人类标记和语言提示，提取任务相关特征，提升机器人在变化环境中的任务表现。


<details>
  <summary>Details</summary>
Motivation: 传统视觉模仿学习仅模仿人类行为，而忽略了行为背后的决策特征，导致环境变化时机器人表现不佳。

Method: 提出CIVIL算法，利用人类标记和语言提示提取因果特征，训练基于Transformer的策略。

Result: 实验表明，CIVIL在减少演示次数和适应新场景方面优于现有方法。

Conclusion: CIVIL通过理解人类决策特征，显著提升了机器人的学习效率和适应性。

Abstract: Today's robots learn new tasks by imitating human examples. However, this
standard approach to visual imitation learning is fundamentally limited: the
robot observes what the human does, but not why the human chooses those
behaviors. Without understanding the features that factor into the human's
decisions, robot learners often misinterpret the data and fail to perform the
task when the environment changes. We therefore propose a shift in perspective:
instead of asking human teachers just to show what actions the robot should
take, we also enable humans to indicate task-relevant features using markers
and language prompts. Our proposed algorithm, CIVIL, leverages this augmented
data to filter the robot's visual observations and extract a feature
representation that causally informs human actions. CIVIL then applies these
causal features to train a transformer-based policy that emulates human
behaviors without being confused by visual distractors. Our simulations,
real-world experiments, and user study demonstrate that robots trained with
CIVIL can learn from fewer human demonstrations and perform better than
state-of-the-art baselines, especially in previously unseen scenarios. See
videos at our project website: https://civil2025.github.io

</details>

### [190] [Plug-and-Play Physics-informed Learning using Uncertainty Quantified Port-Hamiltonian Models](https://arxiv.org/abs/2504.17966)
*Kaiyuan Tan,Peilun Li,Jun Wang,Thomas Beckers*

Main category: cs.RO

TLDR: 提出了一种基于物理信息的机器学习框架（PnP-PIML），用于在数据分布外场景下提供可靠的轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动方法在分布外观测时性能下降的问题。

Method: 结合共形预测识别异常动态，并切换到分布式Port-Hamiltonian系统（dPHS）进行物理一致的预测。

Result: 框架在分布外场景下仍能提供可靠的物理信息预测。

Conclusion: PnP-PIML框架通过结合数据驱动和物理模型，提升了预测的鲁棒性和可靠性。

Abstract: The ability to predict trajectories of surrounding agents and obstacles is a
crucial component in many robotic applications. Data-driven approaches are
commonly adopted for state prediction in scenarios where the underlying
dynamics are unknown. However, the performance, reliability, and uncertainty of
data-driven predictors become compromised when encountering out-of-distribution
observations relative to the training data. In this paper, we introduce a
Plug-and-Play Physics-Informed Machine Learning (PnP-PIML) framework to address
this challenge. Our method employs conformal prediction to identify outlier
dynamics and, in that case, switches from a nominal predictor to a
physics-consistent model, namely distributed Port-Hamiltonian systems (dPHS).
We leverage Gaussian processes to model the energy function of the dPHS,
enabling not only the learning of system dynamics but also the quantification
of predictive uncertainty through its Bayesian nature. In this way, the
proposed framework produces reliable physics-informed predictions even for the
out-of-distribution scenarios.

</details>

### [191] [Set Phasers to Stun: Beaming Power and Control to Mobile Robots with Laser Light](https://arxiv.org/abs/2504.17865)
*Charles J. Carver,Hadleigh Schwartz,Toma Itagaki,Zachary Englhardt,Kechen Liu,Megan Graciela Nauli Manik,Chun-Cheng Chang,Vikram Iyer,Brian Plancher,Xia Zhou*

Main category: cs.RO

TLDR: Phaser是一个利用窄光束激光为移动机器人提供无线供电和通信的系统，结合立体视觉跟踪与光束控制，实现高效能量传输和数据通信。


<details>
  <summary>Details</summary>
Motivation: 解决移动机器人无线供电和通信的需求，提高能量传输效率和数据通信的可靠性。

Method: 设计半自动校准程序，结合立体视觉3D跟踪与高功率光束控制，利用激光光作为数据通道。

Result: 原型系统在几米范围内提供110 mW/cm²的功率密度和无误数据传输，功耗比蓝牙低97%。

Conclusion: Phaser成功为无电池机器人提供高效供电和通信，显著提升其性能。

Abstract: We present Phaser, a flexible system that directs narrow-beam laser light to
moving robots for concurrent wireless power delivery and communication. We
design a semi-automatic calibration procedure to enable fusion of
stereo-vision-based 3D robot tracking with high-power beam steering, and a
low-power optical communication scheme that reuses the laser light as a data
channel. We fabricate a Phaser prototype using off-the-shelf hardware and
evaluate its performance with battery-free autonomous robots. Phaser delivers
optical power densities of over 110 mW/cm$^2$ and error-free data to mobile
robots at multi-meter ranges, with on-board decoding drawing 0.3 mA (97\% less
current than Bluetooth Low Energy). We demonstrate Phaser fully powering
gram-scale battery-free robots to nearly 2x higher speeds than prior work while
simultaneously controlling them to navigate around obstacles and along paths.
Code, an open-source design guide, and a demonstration video of Phaser is
available at https://mobilex.cs.columbia.edu/phaser.

</details>

### [192] [Action Flow Matching for Continual Robot Learning](https://arxiv.org/abs/2504.18471)
*Alejandro Murillo-Gonzalez,Lantao Liu*

Main category: cs.RO

TLDR: 论文提出了一种基于流匹配的生成框架，用于在线机器人动力学模型对齐，通过优化动作而非探索来提升数据收集效率和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决机器人持续学习中的动力学模型对齐问题，包括安全适应、灾难性遗忘、异常管理、数据效率等挑战。

Method: 利用流匹配生成框架，优化动作以匹配对齐模型的行为，减少对重放缓冲区或旧模型的依赖。

Result: 在无人地面车辆和四旋翼平台上验证，任务成功率提升34.2%。

Conclusion: 该方法展示了在持续机器人学习中的高效性和适应性潜力。

Abstract: Continual learning in robotics seeks systems that can constantly adapt to
changing environments and tasks, mirroring human adaptability. A key challenge
is refining dynamics models, essential for planning and control, while
addressing issues such as safe adaptation, catastrophic forgetting, outlier
management, data efficiency, and balancing exploration with exploitation -- all
within task and onboard resource constraints. Towards this goal, we introduce a
generative framework leveraging flow matching for online robot dynamics model
alignment. Rather than executing actions based on a misaligned model, our
approach refines planned actions to better match with those the robot would
take if its model was well aligned. We find that by transforming the actions
themselves rather than exploring with a misaligned model -- as is traditionally
done -- the robot collects informative data more efficiently, thereby
accelerating learning. Moreover, we validate that the method can handle an
evolving and possibly imperfect model while reducing, if desired, the
dependency on replay buffers or legacy model snapshots. We validate our
approach using two platforms: an unmanned ground vehicle and a quadrotor. The
results highlight the method's adaptability and efficiency, with a record
34.2\% higher task success rate, demonstrating its potential towards enabling
continual robot learning. Code:
https://github.com/AlejandroMllo/action_flow_matching.

</details>

<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [193] [SMARTFinRAG: Interactive Modularized Financial RAG Benchmark](https://arxiv.org/abs/2504.18024)
*Yiwei Zha*

Main category: cs.CE

TLDR: SMARTFinRAG是一个针对金融领域RAG系统的评估平台，解决了模块化架构、文档中心评估和界面易用性三大问题。


<details>
  <summary>Details</summary>
Motivation: 金融领域快速采用语言模型技术，但评估专用RAG系统仍具挑战性。

Method: 提出SMARTFinRAG，包括模块化架构、文档中心评估范式和直观界面。

Result: 评估显示不同配置下检索效果和响应质量存在显著差异。

Conclusion: 开源架构支持透明研究，同时解决金融机构部署RAG系统的实际问题。

Abstract: Financial sectors are rapidly adopting language model technologies, yet
evaluating specialized RAG systems in this domain remains challenging. This
paper introduces SMARTFinRAG, addressing three critical gaps in financial RAG
assessment: (1) a fully modular architecture where components can be
dynamically interchanged during runtime; (2) a document-centric evaluation
paradigm generating domain-specific QA pairs from newly ingested financial
documents; and (3) an intuitive interface bridging research-implementation
divides. Our evaluation quantifies both retrieval efficacy and response
quality, revealing significant performance variations across configurations.
The platform's open-source architecture supports transparent, reproducible
research while addressing practical deployment challenges faced by financial
institutions implementing RAG systems.

</details>

### [194] [Discovering Governing Equations of Geomagnetic Storm Dynamics with Symbolic Regression](https://arxiv.org/abs/2504.18461)
*Stefano Markidis,Jonah Ekelund,Luca Pennati,Andong Hu,Ivy Peng*

Main category: cs.CE

TLDR: 使用符号回归方法从历史数据中推导出描述Dst指数时间演化的数据驱动方程，优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 地磁风暴对空间和地面基础设施构成风险，需更准确预测Dst指数。

Method: 利用PySR框架和OMNIweb数据，通过符号回归建立dDst/dt与太阳风参数的数学关系。

Result: 符号回归模型在多数情况下表现更优，尤其在中等风暴期间，且保持物理可解释性。

Conclusion: 该方法提供了可解释的非线性表达式，优于传统模型。

Abstract: Geomagnetic storms are large-scale disturbances of the Earth's magnetosphere
driven by solar wind interactions, posing significant risks to space-based and
ground-based infrastructure. The Disturbance Storm Time (Dst) index quantifies
geomagnetic storm intensity by measuring global magnetic field variations. This
study applies symbolic regression to derive data-driven equations describing
the temporal evolution of the Dst index. We use historical data from the NASA
OMNIweb database, including solar wind density, bulk velocity, convective
electric field, dynamic pressure, and magnetic pressure. The PySR framework, an
evolutionary algorithm-based symbolic regression library, is used to identify
mathematical expressions linking dDst/dt to key solar wind. The resulting
models include a hierarchy of complexity levels and enable a comparison with
well-established empirical models such as the Burton-McPherron-Russell and
O'Brien-McPherron models. The best-performing symbolic regression models
demonstrate superior accuracy in most cases, particularly during moderate
geomagnetic storms, while maintaining physical interpretability. Performance
evaluation on historical storm events includes the 2003 Halloween Storm, the
2015 St. Patrick's Day Storm, and a 2017 moderate storm. The results provide
interpretable, closed-form expressions that capture nonlinear dependencies and
thresholding effects in Dst evolution.

</details>

<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [195] [Quantum Lifting for Invertible Permutations and Ideal Ciphers](https://arxiv.org/abs/2504.18188)
*Alexandru Cojocaru,Minki Hhan,Qipeng Liu,Takashi Yamakawa,Aaram Yun*

Main category: quant-ph

TLDR: 论文提出了首个用于量子随机置换和理想密码模型安全的提升定理，将量子攻击者的成功概率与少量经典查询的经典算法关联。应用这些定理改进了现有结果，获得了新的量子查询复杂度界限和后量子安全结果。


<details>
  <summary>Details</summary>
Motivation: 研究量子计算环境下密码模型的安全性，填补量子随机置换和理想密码模型安全提升定理的空白。

Method: 通过提升定理将量子攻击者的成功概率与经典算法的查询能力关联，分析量子查询复杂度。

Result: 改进了现有结果，获得了新的量子查询复杂度界限，证明了双面零搜索游戏的量子硬度，以及后量子安全性。

Conclusion: 提升定理为量子密码模型的安全性提供了新工具，改进了多个后量子安全结果。

Abstract: In this work, we derive the first lifting theorems for establishing security
in the quantum random permutation and ideal cipher models. These theorems
relate the success probability of an arbitrary quantum adversary to that of a
classical algorithm making only a small number of classical queries.
  By applying these lifting theorems, we improve previous results and obtain
new quantum query complexity bounds and post-quantum security results. Notably,
we derive tight bounds for the quantum hardness of the double-sided zero search
game and establish the post-quantum security for the preimage resistance,
one-wayness, and multi-collision resistance of constant-round sponge, as well
as the collision resistance of the Davies-Meyer construction.

</details>

### [196] [Bayesian Quantum Orthogonal Neural Networks for Anomaly Detection](https://arxiv.org/abs/2504.18103)
*Natansh Mathur,Brian Coyle,Nishant Jain,Snehal Raj,Akshat Tandon,Jasper Simon Krauser,Rainer Stoessel*

Main category: quant-ph

TLDR: 结合贝叶斯学习和量子启发的正交神经网络，用于3D物体异常检测，并在IBM量子设备上验证可行性。


<details>
  <summary>Details</summary>
Motivation: 确保3D物体功能正确需要检测缺陷或异常，结合贝叶斯学习和量子机器学习可提升检测能力。

Method: 开发正交（量子）版本的3D卷积神经网络，利用贝叶斯学习量化预测不确定性，并在IBM量子设备上进行硬件实验。

Result: 模型成功检测3D物体异常，验证了量子增强检测流程的可行性。

Conclusion: 正交量子神经网络在异常检测中表现良好，量子计算的实际应用潜力得到初步验证。

Abstract: Identification of defects or anomalies in 3D objects is a crucial task to
ensure correct functionality. In this work, we combine Bayesian learning with
recent developments in quantum and quantum-inspired machine learning,
specifically orthogonal neural networks, to tackle this anomaly detection
problem for an industrially relevant use case. Bayesian learning enables
uncertainty quantification of predictions, while orthogonality in weight
matrices enables smooth training. We develop orthogonal (quantum) versions of
3D convolutional neural networks and show that these models can successfully
detect anomalies in 3D objects. To test the feasibility of incorporating
quantum computers into a quantum-enhanced anomaly detection pipeline, we
perform hardware experiments with our models on IBM's 127-qubit Brisbane
device, testing the effect of noise and limited measurement shots.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [197] [iVR-GS: Inverse Volume Rendering for Explorable Visualization via Editable 3D Gaussian Splatting](https://arxiv.org/abs/2504.17954)
*Kaiyuan Tang,Siyuan Yao,Chaoli Wang*

Main category: cs.GR

TLDR: 本文提出了一种基于高斯点云的逆体积渲染方法（iVR-GS），通过组合多个基本模型实现交互式体积探索，降低了渲染成本并支持场景编辑。


<details>
  <summary>Details</summary>
Motivation: 现有新视角合成（NVS）方法虽然渲染速度快，但可见部分受限于预设的传输函数（TF），限制了用户探索。iVR-GS旨在解决这一问题。

Method: 通过高斯点云构建多个基本模型，每个模型包含可编辑的3D高斯点，支持实时渲染和编辑。

Result: iVR-GS在多种体积数据集上展示了优于其他NVS方法（如Plenoxels、CCNeRF和3DGS）的重建质量和可组合性。

Conclusion: iVR-GS是一种高效且灵活的NVS方法，适用于交互式体积探索。

Abstract: In volume visualization, users can interactively explore the
three-dimensional data by specifying color and opacity mappings in the transfer
function (TF) or adjusting lighting parameters, facilitating meaningful
interpretation of the underlying structure. However, rendering large-scale
volumes demands powerful GPUs and high-speed memory access for real-time
performance. While existing novel view synthesis (NVS) methods offer faster
rendering speeds with lower hardware requirements, the visible parts of a
reconstructed scene are fixed and constrained by preset TF settings,
significantly limiting user exploration. This paper introduces inverse volume
rendering via Gaussian splatting (iVR-GS), an innovative NVS method that
reduces the rendering cost while enabling scene editing for interactive volume
exploration. Specifically, we compose multiple iVR-GS models associated with
basic TFs covering disjoint visible parts to make the entire volumetric scene
visible. Each basic model contains a collection of 3D editable Gaussians, where
each Gaussian is a 3D spatial point that supports real-time scene rendering and
editing. We demonstrate the superior reconstruction quality and composability
of iVR-GS against other NVS solutions (Plenoxels, CCNeRF, and base 3DGS) on
various volume datasets. The code is available at
https://github.com/TouKaienn/iVR-GS.

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [198] [EduBot -- Can LLMs Solve Personalized Learning and Programming Assignments?](https://arxiv.org/abs/2504.17824)
*Yibin Wang,Jiaxi Xie,Lakshminarayanan Subramanian*

Main category: cs.SE

TLDR: EduBot是一个基于LLMs的智能编程助手系统，通过递归提示驱动方法解决复杂编程任务，包括教学、代码开发和调试。


<details>
  <summary>Details</summary>
Motivation: 探索预训练LLMs在多步推理和代码生成中的潜力，解决个性化编程任务。

Method: 结合概念教学、端到端代码开发和递归提示驱动方法，无需微调LLMs。

Result: 在20个场景的基准测试中，EduBot能在20分钟内完成大部分任务，并验证了其在不同LLMs中的兼容性和鲁棒性。

Conclusion: EduBot展示了预训练LLMs在解决复杂编程任务中的潜力，为个性化学习和代码生成提供了新思路。

Abstract: The prevalence of Large Language Models (LLMs) is revolutionizing the process
of writing code. General and code LLMs have shown impressive performance in
generating standalone functions and code-completion tasks with one-shot
queries. However, the ability to solve comprehensive programming tasks with
recursive requests and bug fixes remains questionable. In this paper, we
propose EduBot, an intelligent automated assistant system that combines
conceptual knowledge teaching, end-to-end code development, personalized
programming through recursive prompt-driven methods, and debugging with limited
human interventions powered by LLMs. We show that EduBot can solve complicated
programming tasks consisting of sub-tasks with increasing difficulties ranging
from conceptual to coding questions by recursive automatic prompt-driven
systems without finetuning on LLMs themselves. To further evaluate EduBot's
performance, we design and conduct a benchmark suite consisting of 20 scenarios
in algorithms, machine learning, and real-world problems. The result shows that
EduBot can complete most scenarios in less than 20 minutes. Based on the
benchmark suites, we perform a comparative study to take different LLMs as the
backbone and to verify EduBot's compatibility and robustness across LLMs with
varying capabilities. We believe that EduBot is an exploratory approach to
explore the potential of pre-trained LLMs in multi-step reasoning and code
generation for solving personalized assignments with knowledge learning and
code generation.

</details>

### [199] [Validating Network Protocol Parsers with Traceable RFC Document Interpretation](https://arxiv.org/abs/2504.18050)
*Mingwei Zheng,Danning Xie,Qingkai Shi,Chengpeng Wang,Xiangyu Zhang*

Main category: cs.SE

TLDR: 利用大型语言模型（LLMs）从RFC文档生成形式化协议规范，解决网络协议实现验证中的oracle和traceability问题，并检测出69个bug。


<details>
  <summary>Details</summary>
Motivation: 网络协议实现的正确性验证因oracle和traceability问题而困难，现有方法很少同时解决这两个问题。

Method: 通过LLMs将RFC文档转化为形式化协议规范，作为准oracle验证协议解析器，并逐步优化oracle。

Result: 在9种网络协议及其C、Python、Go实现中检测出69个bug（36个已确认），优于现有方法。

Conclusion: 展示了基于自然语言规范自动化软件验证的潜力，减少了传统手动验证的需求。

Abstract: Validating the correctness of network protocol implementations is highly
challenging due to the oracle and traceability problems. The former determines
when a protocol implementation can be considered buggy, especially when the
bugs do not cause any observable symptoms. The latter allows developers to
understand how an implementation violates the protocol specification, thereby
facilitating bug fixes. Unlike existing works that rarely take both problems
into account, this work considers both and provides an effective solution using
recent advances in large language models (LLMs). Our key observation is that
network protocols are often released with structured specification documents,
a.k.a. RFC documents, which can be systematically translated to formal protocol
message specifications via LLMs. Such specifications, which may contain errors
due to the hallucination of LLMs, are used as a quasi-oracle to validate
protocol parsers, while the validation results in return gradually refine the
oracle. Since the oracle is derived from the document, any bugs we find in a
protocol implementation can be traced back to the document, thus addressing the
traceability problem. We have extensively evaluated our approach using nine
network protocols and their implementations written in C, Python, and Go. The
results show that our approach outperforms the state-of-the-art and has
detected 69 bugs, with 36 confirmed. The project also demonstrates the
potential for fully automating software validation based on natural language
specifications, a process previously considered predominantly manual due to the
need to understand specification documents and derive expected outputs for test
inputs.

</details>

### [200] [Towards Adaptive Software Agents for Debugging](https://arxiv.org/abs/2504.18316)
*Yacine Majdoub,Eya Ben Charrada,Haifa Touati*

Main category: cs.SE

TLDR: 提出了一种自适应多代理设计，动态调整代理数量和角色以优化LLM调试能力，相比单次提示平均提升11%效果。


<details>
  <summary>Details</summary>
Motivation: 多代理虽能提升LLM调试能力，但固定代理数量和角色会增加成本和分散注意力，需动态调整以提高效率。

Method: 设计自适应代理框架，根据任务特性动态生成代理数量和角色，无需预定义。

Result: 代理数量随代码复杂度变化，简单问题仅需1个代理，复杂问题生成更多代理，平均修复效果提升11%。

Conclusion: 自适应设计在多代理调试中表现优异，未来可进一步优化以实现自主规划和执行软件目标。

Abstract: Using multiple agents was found to improve the debugging capabilities of
Large Language Models. However, increasing the number of LLM-agents has several
drawbacks such as increasing the running costs and rising the risk for the
agents to lose focus. In this work, we propose an adaptive agentic design,
where the number of agents and their roles are determined dynamically based on
the characteristics of the task to be achieved. In this design, the agents
roles are not predefined, but are generated after analyzing the problem to be
solved. Our initial evaluation shows that, with the adaptive design, the number
of agents that are generated depends on the complexity of the buggy code. In
fact, for simple code with mere syntax issues, the problem was usually fixed
using one agent only. However, for more complex problems, we noticed the
creation of a higher number of agents. Regarding the effectiveness of the fix,
we noticed an average improvement of 11% compared to the one-shot prompting.
Given these promising results, we outline future research directions to improve
our design for adaptive software agents that can autonomously plan and conduct
their software goals.

</details>

### [201] [Spatial Reasoner: A 3D Inference Pipeline for XR Applications](https://arxiv.org/abs/2504.18380)
*Steven Häsler,Philipp Ackermann*

Main category: cs.SE

TLDR: 提出了一种空间推理框架，通过几何事实与符号谓词结合，处理3D场景中的语义任务。


<details>
  <summary>Details</summary>
Motivation: 现代XR系统需要能够以语义方式推理3D场景的AR/VR应用。

Method: 基于定向3D边界框表示，结合空间谓词构建空间知识图，并通过管道推理模型支持查询与动态规则评估。

Result: 框架能够高效将几何数据转化为可操作知识，支持复杂3D环境中的可扩展空间推理。

Conclusion: 该框架促进了空间本体的创建，并丰富了XR应用中的机器学习、自然语言处理和规则系统。

Abstract: Modern extended reality XR systems provide rich analysis of image data and
fusion of sensor input and demand AR/VR applications that can reason about 3D
scenes in a semantic manner. We present a spatial reasoning framework that
bridges geometric facts with symbolic predicates and relations to handle key
tasks such as determining how 3D objects are arranged among each other ('on',
'behind', 'near', etc.). Its foundation relies on oriented 3D bounding box
representations, enhanced by a comprehensive set of spatial predicates, ranging
from topology and connectivity to directionality and orientation, expressed in
a formalism related to natural language. The derived predicates form a spatial
knowledge graph and, in combination with a pipeline-based inference model,
enable spatial queries and dynamic rule evaluation. Implementations for client-
and server-side processing demonstrate the framework's capability to
efficiently translate geometric data into actionable knowledge, ensuring
scalable and technology-independent spatial reasoning in complex 3D
environments. The Spatial Reasoner framework is fostering the creation of
spatial ontologies, and seamlessly integrates with and therefore enriches
machine learning, natural language processing, and rule systems in XR
applications.

</details>

### [202] [Paradigm shift on Coding Productivity Using GenAI](https://arxiv.org/abs/2504.18404)
*Liang Yu*

Main category: cs.SE

TLDR: 生成式AI（GenAI）在软件工程中通过自动化代码共创提升生产力，但在复杂任务中表现受限。


<details>
  <summary>Details</summary>
Motivation: 研究GenAI编码助手在电信和金融科技领域的工业应用，填补实证证据的空白。

Method: 通过调查和访谈工业领域专家，分析影响生产力的关键因素。

Result: GenAI在常规任务中提升效率，但在复杂任务中因上下文感知不足而受限。

Conclusion: 提出新范式，强调迭代提示优化和沉浸式开发环境对GenAI有效使用的重要性。

Abstract: Generative AI (GenAI) applications are transforming software engineering by
enabling automated code co-creation. However, empirical evidence on GenAI's
productivity effects in industrial settings remains limited. This paper
investigates the adoption of GenAI coding assistants (e.g., Codeium, Amazon Q)
within telecommunications and FinTech domains. Through surveys and interviews
with industrial domain-experts, we identify primary productivity-influencing
factors, including task complexity, coding skills, domain knowledge, and GenAI
integration. Our findings indicate that GenAI tools enhance productivity in
routine coding tasks (e.g., refactoring and Javadoc generation) but face
challenges in complex, domain-specific activities due to limited
context-awareness of codebases and insufficient support for customized design
rules. We highlight new paradigms for coding transfer, emphasizing iterative
prompt refinement, immersive development environment, and automated code
evaluation as essential for effective GenAI usage.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [203] [Learning Enhanced Ensemble Filters](https://arxiv.org/abs/2504.17836)
*Eviatar Bach,Ricardo Baptista,Edoardo Calvello,Bohan Chen,Andrew Stuart*

Main category: stat.ML

TLDR: 论文提出了一种基于神经算子的新滤波方法MNMEF，通过Measure Neural Mapping（MNM）改进传统EnKF的局限性，并在实验中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统EnKF方法因高斯假设限制了精度，需一种更精确的滤波方法。

Method: 使用MNM设计MNMEF方法，结合集合粒子近似和集合变换器实现。

Result: MNMEF在Lorenz 96和Kuramoto-Sivashinsky模型中表现优于现有方法。

Conclusion: MNMEF通过神经算子提升了滤波精度，具有实际应用潜力。

Abstract: The filtering distribution in hidden Markov models evolves according to the
law of a mean-field model in state--observation space. The ensemble Kalman
filter (EnKF) approximates this mean-field model with an ensemble of
interacting particles, employing a Gaussian ansatz for the joint distribution
of the state and observation at each observation time. These methods are
robust, but the Gaussian ansatz limits accuracy. This shortcoming is addressed
by approximating the mean-field evolution using a novel form of neural operator
taking probability distributions as input: a Measure Neural Mapping (MNM). A
MNM is used to design a novel approach to filtering, the MNM-enhanced ensemble
filter (MNMEF), which is defined in both the mean-fieldlimit and for
interacting ensemble particle approximations. The ensemble approach uses
empirical measures as input to the MNM and is implemented using the set
transformer, which is invariant to ensemble permutation and allows for
different ensemble sizes. The derivation of methods from a mean-field
formulation allows a single parameterization of the algorithm to be deployed at
different ensemble sizes. In practice fine-tuning of a small number of
parameters, for specific ensemble sizes, further enhances the accuracy of the
scheme. The promise of the approach is demonstrated by its superior
root-mean-square-error performance relative to leading methods in filtering the
Lorenz 96 and Kuramoto-Sivashinsky models.

</details>

### [204] [Learning Operators by Regularized Stochastic Gradient Descent with Operator-valued Kernels](https://arxiv.org/abs/2504.18184)
*Jia-Qi Yang,Lei Shi*

Main category: stat.ML

TLDR: 本文研究了正则化随机梯度下降（SGD）算法，用于估计从波兰空间到可分希尔伯特空间的非线性算子。在两种设置下分析了预测和估计误差的收敛性，证明了其接近最优性，并提出了一种新技术用于高概率边界推导。


<details>
  <summary>Details</summary>
Motivation: 研究非线性算子的估计问题，特别是在高维或无限维空间中，正则化SGD算法的收敛性和效率。

Method: 采用正则化SGD算法，考虑在线和有限范围两种设置，引入结构和平滑性条件，分析预测和估计误差的收敛性。

Result: 在给定条件下，证明了算法的收敛性接近最优，并提供了高概率边界和几乎必然收敛的保证。

Conclusion: 正则化SGD算法在非线性算子估计中具有高效性和理论保证，未来可扩展至更一般的算子值核和编码器-解码器框架。

Abstract: This paper investigates regularized stochastic gradient descent (SGD)
algorithms for estimating nonlinear operators from a Polish space to a
separable Hilbert space. We assume that the regression operator lies in a
vector-valued reproducing kernel Hilbert space induced by an operator-valued
kernel. Two significant settings are considered: an online setting with
polynomially decaying step sizes and regularization parameters, and a
finite-horizon setting with constant step sizes and regularization parameters.
We introduce regularity conditions on the structure and smoothness of the
target operator and the input random variables. Under these conditions, we
provide a dimension-free convergence analysis for the prediction and estimation
errors, deriving both expectation and high-probability error bounds. Our
analysis demonstrates that these convergence rates are nearly optimal.
Furthermore, we present a new technique for deriving bounds with high
probability for general SGD schemes, which also ensures almost-sure
convergence. Finally, we discuss potential extensions to more general
operator-valued kernels and the encoder-decoder framework.

</details>

### [205] [Post-Transfer Learning Statistical Inference in High-Dimensional Regression](https://arxiv.org/abs/2504.18212)
*Nguyen Vu Khai Tam,Cao Huyen My,Vo Nguyen Le Duy*

Main category: stat.ML

TLDR: 提出了一种名为PTL-SI的统计推断框架，用于评估迁移学习高维回归中特征选择的可靠性，并提供有效的p值以控制假阳性率。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏在迁移学习高维回归中量化特征与响应关系统计显著性的方法。

Method: 引入PTL-SI框架，结合分治策略，提供有效p值并控制假阳性率。

Result: 通过合成和真实高维数据集实验验证了PTL-SI的有效性和理论性质。

Conclusion: PTL-SI为迁移学习高维回归中的特征选择提供了可靠的统计推断工具。

Abstract: Transfer learning (TL) for high-dimensional regression (HDR) is an important
problem in machine learning, particularly when dealing with limited sample size
in the target task. However, there currently lacks a method to quantify the
statistical significance of the relationship between features and the response
in TL-HDR settings. In this paper, we introduce a novel statistical inference
framework for assessing the reliability of feature selection in TL-HDR, called
PTL-SI (Post-TL Statistical Inference). The core contribution of PTL-SI is its
ability to provide valid $p$-values to features selected in TL-HDR, thereby
rigorously controlling the false positive rate (FPR) at desired significance
level $\alpha$ (e.g., 0.05). Furthermore, we enhance statistical power by
incorporating a strategic divide-and-conquer approach into our framework. We
demonstrate the validity and effectiveness of the proposed PTL-SI through
extensive experiments on both synthetic and real-world high-dimensional
datasets, confirming its theoretical properties and utility in testing the
reliability of feature selection in TL scenarios.

</details>

### [206] [Generalization Guarantees for Multi-View Representation Learning and Application to Regularization via Gaussian Product Mixture Prior](https://arxiv.org/abs/2504.18455)
*Milad Sefidgaran,Abdellatif Zaidi,Piotr Krasnowski*

Main category: stat.ML

TLDR: 研究分布式多视图表示学习问题，探讨无显式协调下各代理应提取的必要且充分信息，提出基于泛化误差的MDL正则化方法，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决多代理独立提取视图表示时，如何确保解码器正确估计隐藏标签的问题。

Method: 通过泛化误差分析，提出基于相对熵和MDL的正则化方法，并研究数据依赖的高斯混合先验选择。

Result: 单视图实验优于VIB和CDVIB，多视图下高斯乘积混合先验隐式鼓励冗余特征提取。

Conclusion: 数据依赖先验和正则化方法在多视图学习中表现优异，且揭示了冗余特征提取的意外价值。

Abstract: We study the problem of distributed multi-view representation learning. In
this problem, $K$ agents observe each one distinct, possibly statistically
correlated, view and independently extracts from it a suitable representation
in a manner that a decoder that gets all $K$ representations estimates
correctly the hidden label. In the absence of any explicit coordination between
the agents, a central question is: what should each agent extract from its view
that is necessary and sufficient for a correct estimation at the decoder? In
this paper, we investigate this question from a generalization error
perspective. First, we establish several generalization bounds in terms of the
relative entropy between the distribution of the representations extracted from
training and "test" datasets and a data-dependent symmetric prior, i.e., the
Minimum Description Length (MDL) of the latent variables for all views and
training and test datasets. Then, we use the obtained bounds to devise a
regularizer; and investigate in depth the question of the selection of a
suitable prior. In particular, we show and conduct experiments that illustrate
that our data-dependent Gaussian mixture priors with judiciously chosen weights
lead to good performance. For single-view settings (i.e., $K=1$), our
experimental results are shown to outperform existing prior art Variational
Information Bottleneck (VIB) and Category-Dependent VIB (CDVIB) approaches.
Interestingly, we show that a weighted attention mechanism emerges naturally in
this setting. Finally, for the multi-view setting, we show that the selection
of the joint prior as a Gaussians product mixture induces a Gaussian mixture
marginal prior for each marginal view and implicitly encourages the agents to
extract and output redundant features, a finding which is somewhat
counter-intuitive.

</details>

### [207] [Enhancing Visual Interpretability and Explainability in Functional Survival Trees and Forests](https://arxiv.org/abs/2504.18498)
*Giuseppe Loffredo,Elvira Romano,Fabrizio MAturo*

Main category: stat.ML

TLDR: 论文研究了功能生存树（FST）和功能随机生存森林（FRSF）两种生存模型，提出了增强其可解释性的新方法和工具，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 功能生存模型在处理复杂预测因子时预测能力强，但缺乏可解释性，限制了其在实践中的应用。

Method: 引入了增强FST可解释性和FRSF可解释性的新方法，并使用真实和模拟数据集进行验证。

Result: 提出的方法生成了高效且易于理解的决策树，准确捕捉了模型集成的决策过程。

Conclusion: 研究为功能生存模型的可解释性提供了实用工具，有助于实际决策和风险分析。

Abstract: Functional survival models are key tools for analyzing time-to-event data
with complex predictors, such as functional or high-dimensional inputs. Despite
their predictive strength, these models often lack interpretability, which
limits their value in practical decision-making and risk analysis. This study
investigates two key survival models: the Functional Survival Tree (FST) and
the Functional Random Survival Forest (FRSF). It introduces novel methods and
tools to enhance the interpretability of FST models and improve the
explainability of FRSF ensembles. Using both real and simulated datasets, the
results demonstrate that the proposed approaches yield efficient,
easy-to-understand decision trees that accurately capture the underlying
decision-making processes of the model ensemble.

</details>

### [208] [Representation Learning for Distributional Perturbation Extrapolation](https://arxiv.org/abs/2504.18522)
*Julius von Kügelgen,Jakob Ketterer,Xinwei Shen,Nicolai Meinshausen,Jonas Peters*

Main category: stat.ML

TLDR: 论文提出了一种基于潜在变量模型的方法PDAE，用于预测未见过的扰动（如基因敲除或药物组合）对RNA测序数据的影响。通过证明在足够多样化的训练扰动下，模型可识别表示和扰动效应，并提供了外推保证。


<details>
  <summary>Details</summary>
Motivation: 研究如何从已知扰动的数据中预测新扰动对低水平测量（如RNA测序数据）的影响，解决外推任务的挑战。

Method: 提出PDAE方法，通过潜在变量模型将扰动建模为潜在空间的均值偏移，并最大化真实与预测扰动分布之间的相似性。

Result: PDAE在预测未见扰动效果上优于现有方法和基线。

Conclusion: PDAE通过潜在空间建模和分布相似性优化，为未见扰动的预测提供了有效解决方案。

Abstract: We consider the problem of modelling the effects of unseen perturbations such
as gene knockdowns or drug combinations on low-level measurements such as RNA
sequencing data. Specifically, given data collected under some perturbations,
we aim to predict the distribution of measurements for new perturbations. To
address this challenging extrapolation task, we posit that perturbations act
additively in a suitable, unknown embedding space. More precisely, we formulate
the generative process underlying the observed data as a latent variable model,
in which perturbations amount to mean shifts in latent space and can be
combined additively. Unlike previous work, we prove that, given sufficiently
diverse training perturbations, the representation and perturbation effects are
identifiable up to affine transformation, and use this to characterize the
class of unseen perturbations for which we obtain extrapolation guarantees. To
estimate the model from data, we propose a new method, the perturbation
distribution autoencoder (PDAE), which is trained by maximising the
distributional similarity between true and predicted perturbation
distributions. The trained model can then be used to predict previously unseen
perturbation distributions. Empirical evidence suggests that PDAE compares
favourably to existing methods and baselines at predicting the effects of
unseen perturbations.

</details>

<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [209] [Research on Cloud Platform Network Traffic Monitoring and Anomaly Detection System based on Large Language Models](https://arxiv.org/abs/2504.17807)
*Ze Yang,Yihong Jin,Juntian Liu,Xinhe Xu,Yihan Zhang,Shuyang Ji*

Main category: cs.NI

TLDR: 本文提出了一种基于大语言模型（LLM）的网络流量监控与异常检测系统，结合了注意力机制和监督学习框架，显著提升了检测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着云平台的快速发展和网络流量的日益复杂，传统的网络监控和异常检测方法已难以满足需求，需要更高效、准确的解决方案。

Method: 采用预训练的大语言模型处理网络流量序列数据，结合注意力机制和监督学习框架，并引入基于迁移学习的方法以快速适应未知网络结构和对抗条件。

Result: 实验结果表明，该模型在检测精度和计算效率上优于传统方法，能有效识别零日攻击和流量拥塞等异常，并显著降低误报率。

Conclusion: 该研究证明了基于大语言模型的混合方法在网络流量监控和异常检测中的有效性，为未来研究提供了新方向。

Abstract: The rapidly evolving cloud platforms and the escalating complexity of network
traffic demand proper network traffic monitoring and anomaly detection to
ensure network security and performance. This paper introduces a large language
model (LLM)-based network traffic monitoring and anomaly detection system. In
addition to existing models such as autoencoders and decision trees, we harness
the power of large language models for processing sequence data from network
traffic, which allows us a better capture of underlying complex patterns, as
well as slight fluctuations in the dataset. We show for a given detection task,
the need for a hybrid model that incorporates the attention mechanism of the
transformer architecture into a supervised learning framework in order to
achieve better accuracy. A pre-trained large language model analyzes and
predicts the probable network traffic, and an anomaly detection layer that
considers temporality and context is added. Moreover, we present a novel
transfer learning-based methodology to enhance the model's effectiveness to
quickly adapt to unknown network structures and adversarial conditions without
requiring extensive labeled datasets. Actual results show that the designed
model outperforms traditional methods in detection accuracy and computational
efficiency, effectively identify various network anomalies such as zero-day
attacks and traffic congestion pattern, and significantly reduce the false
positive rate.

</details>

### [210] [LLM-Guided Open RAN: Empowering Hierarchical RAN Intelligent Control](https://arxiv.org/abs/2504.18062)
*Lingyan Bao,Sinwoong Yun,Jemin Lee,Tony Q. S. Quek*

Main category: cs.NI

TLDR: 论文提出了一种基于大语言模型（LLM）的分层RIC框架（LLM-hRIC），用于提升无线通信网络中RIC的协作效率，结合强化学习（RL）实现资源管理。


<details>
  <summary>Details</summary>
Motivation: 利用LLM和O-RAN技术的灵活性，解决无线通信网络中不同时间尺度的资源管理问题。

Method: 提出LLM-hRIC框架，LLM驱动的非实时RIC提供策略指导，RL驱动的近实时RIC执行低延迟任务。

Result: 在IAB网络环境中，仿真显示该框架性能优越。

Conclusion: LLM-hRIC框架有效提升了RIC协作效率，未来需解决LLM在O-RAN中的应用挑战。

Abstract: Recent advancements in large language models (LLMs) have led to a significant
interest in deploying LLMempowered algorithms for wireless communication
networks. Meanwhile, open radio access network (O-RAN) techniques offer
unprecedented flexibility, with the non-real-time (non-RT) radio access network
(RAN) intelligent controller (RIC) (non-RT RIC) and near-real-time (near-RT)
RIC (near-RT RIC) components enabling intelligent resource management across
different time scales. In this paper, we propose the LLM empowered hierarchical
RIC (LLM-hRIC) framework to improve the collaboration between RICs. This
framework integrates LLMs with reinforcement learning (RL) for efficient
network resource management. In this framework, LLMs-empowered non-RT RICs
provide strategic guidance and high-level policies based on environmental
context. Concurrently, RL-empowered near-RT RICs perform low-latency tasks
based on strategic guidance and local near-RT observation. We evaluate the
LLM-hRIC framework in an integrated access and backhaul (IAB) network setting.
Simulation results demonstrate that the proposed framework achieves superior
performance. Finally, we discuss the key future challenges in applying LLMs to
O-RAN.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [211] [A Deep Bayesian Convolutional Spiking Neural Network-based CAD system with Uncertainty Quantification for Medical Images Classification](https://arxiv.org/abs/2504.17819)
*Mohaddeseh Chegini,Ali Mahloojifar*

Main category: eess.IV

TLDR: 提出了一种基于深度贝叶斯卷积脉冲神经网络的CAD系统，通过蒙特卡洛Dropout方法量化不确定性，提高了医学图像分类的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 利用第三代神经网络（SNN）的优势（如事件驱动处理、低功耗等）开发CAD系统，但深度SNN存在不可靠性问题，需解决预测不确定性的量化问题。

Method: 提出一种深度贝叶斯卷积脉冲神经网络，结合蒙特卡洛Dropout方法作为不确定性量化手段，应用于医学图像分类任务。

Result: 实验表明，该模型在准确性和可靠性上表现优异，可作为传统深度学习的替代方案。

Conclusion: 该研究为医学图像分类提供了一种更可靠且高效的解决方案。

Abstract: The Computer_Aided Diagnosis (CAD) systems facilitate accurate diagnosis of
diseases. The development of CADs by leveraging third generation neural
network, namely, Spiking Neural Network (SNN), is essential to utilize of the
benefits of SNNs, such as their event_driven processing, parallelism, low power
consumption, and the ability to process sparse temporal_spatial information.
However, Deep SNN as a deep learning model faces challenges with unreliability.
To deal with unreliability challenges due to inability to quantify the
uncertainty of the predictions, we proposed a deep Bayesian Convolutional
Spiking Neural Network based_CADs with uncertainty_aware module. In this study,
the Monte Carlo Dropout method as Bayesian approximation is used as an
uncertainty quantification method. This method was applied to several medical
image classification tasks. Our experimental results demonstrate that our
proposed model is accurate and reliable and will be a proper alternative to
conventional deep learning for medical image classification.

</details>

### [212] [Predicting Dairy Calf Body Weight from Depth Images Using Deep Learning (YOLOv8) and Threshold Segmentation with Cross-Validation and Longitudinal Analysis](https://arxiv.org/abs/2504.17943)
*Mingsi Liao,Gota Morota,Ye Bi,Rebecca R. Cockrum*

Main category: eess.IV

TLDR: 论文提出了一种基于深度学习的非接触式方法，用于预测断奶前犊牛的体重，解决了传统方法在劳动力和设施上的限制。


<details>
  <summary>Details</summary>
Motivation: 由于劳动、时间和设施限制，传统犊牛体重监测方法难以实施，且荷斯坦犊牛的毛色图案增加了图像分析的难度。

Method: 研究开发了深度学习分割模型（YOLOv8），并与阈值方法对比，使用线性回归（LR）、极端梯度提升（XGBoost）和线性混合模型（LMM）进行体重预测验证。

Result: YOLOv8分割效果优于阈值方法（IoU=0.98）。XGBoost在单时间点验证中表现最佳（R²=0.91），LMM在纵向预测中最准确（R²=0.99）。

Conclusion: 深度学习在自动化体重预测中具有潜力，可优化农场管理。

Abstract: Monitoring calf body weight (BW) before weaning is essential for assessing
growth, feed efficiency, health, and weaning readiness. However, labor, time,
and facility constraints limit BW collection. Additionally, Holstein calf coat
patterns complicate image-based BW estimation, and few studies have explored
non-contact measurements taken at early time points for predicting later BW.
The objectives of this study were to (1) develop deep learning-based
segmentation models for extracting calf body metrics, (2) compare deep learning
segmentation with threshold-based methods, and (3) evaluate BW prediction using
single-time-point cross-validation with linear regression (LR) and extreme
gradient boosting (XGBoost) and multiple-time-point cross-validation with LR,
XGBoost, and a linear mixed model (LMM). Depth images from Holstein (n = 63)
and Jersey (n = 5) pre-weaning calves were collected, with 20 Holstein calves
being weighed manually. Results showed that You Only Look Once version 8
(YOLOv8) deep learning segmentation (intersection over union = 0.98)
outperformed threshold-based methods (0.89). In single-time-point
cross-validation, XGBoost achieved the best BW prediction (R^2 = 0.91, mean
absolute percentage error (MAPE) = 4.37%), while LMM provided the most accurate
longitudinal BW prediction (R^2 = 0.99, MAPE = 2.39%). These findings highlight
the potential of deep learning for automated BW prediction, enhancing farm
management.

</details>

### [213] [Spectral Bias Correction in PINNs for Myocardial Image Registration of Pathological Data](https://arxiv.org/abs/2504.17945)
*Bastien C. Baluyot,Marta Varela,Chen Qin*

Main category: eess.IV

TLDR: 论文提出了一种改进物理信息神经网络（PINN）的方法，通过集成傅里叶特征映射和调制策略，解决了神经网络中的频谱偏差问题，提高了心肌图像配准的准确性。


<details>
  <summary>Details</summary>
Motivation: 心肌图像配准对心脏应变分析和疾病诊断至关重要，但神经网络中的频谱偏差导致高频率变形建模不准确，尤其在病理数据中。

Method: 在PINN框架中集成傅里叶特征映射并引入调制策略，以解决频谱偏差问题。

Result: 在两个不同数据集上的实验表明，该方法显著提升了PINN捕捉高频率复杂变形的能力，配准精度更高且保持生物力学合理性。

Conclusion: 该方法为可扩展的心脏图像配准提供了基础，能够泛化到不同患者和病理情况。

Abstract: Accurate myocardial image registration is essential for cardiac strain
analysis and disease diagnosis. However, spectral bias in neural networks
impedes modeling high-frequency deformations, producing inaccurate,
biomechanically implausible results, particularly in pathological data. This
paper addresses spectral bias in physics-informed neural networks (PINNs) by
integrating Fourier Feature mappings and introducing modulation strategies into
a PINN framework. Experiments on two distinct datasets demonstrate that the
proposed methods enhance the PINN's ability to capture complex, high-frequency
deformations in cardiomyopathies, achieving superior registration accuracy
while maintaining biomechanical plausibility - thus providing a foundation for
scalable cardiac image registration and generalization across multiple patients
and pathologies.

</details>

### [214] [A Multimodal Deep Learning Approach for White Matter Shape Prediction in Diffusion MRI Tractography](https://arxiv.org/abs/2504.18400)
*Yui Lo,Yuqian Chen,Dongnan Liu,Leo Zekelman,Jarrett Rushmore,Yogesh Rathi,Nikos Makris,Alexandra J. Golby,Fan Zhang,Weidong Cai,Lauren J. O'Donnell*

Main category: eess.IV

TLDR: Tract2Shape是一种新型多模态深度学习框架，用于快速预测白质纤维束的形状测量，优于现有方法，并展示出强大的跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统计算白质纤维束形状测量的方法计算成本高且耗时，限制了大规模数据集的分析。

Method: 提出Tract2Shape，结合几何（点云）和标量（表格）特征，利用降维算法预测形状测量。

Result: 在HCP-YA数据集上表现优于现有模型，并在PPMI数据集上验证了其泛化能力。

Conclusion: Tract2Shape为未来大规模白质形状分析提供了高效、准确的解决方案。

Abstract: Shape measures have emerged as promising descriptors of white matter
tractography, offering complementary insights into anatomical variability and
associations with cognitive and clinical phenotypes. However, conventional
methods for computing shape measures are computationally expensive and
time-consuming for large-scale datasets due to reliance on voxel-based
representations. We propose Tract2Shape, a novel multimodal deep learning
framework that leverages geometric (point cloud) and scalar (tabular) features
to predict ten white matter tractography shape measures. To enhance model
efficiency, we utilize a dimensionality reduction algorithm for the model to
predict five primary shape components. The model is trained and evaluated on
two independently acquired datasets, the HCP-YA dataset, and the PPMI dataset.
We evaluate the performance of Tract2Shape by training and testing it on the
HCP-YA dataset and comparing the results with state-of-the-art models. To
further assess its robustness and generalization ability, we also test
Tract2Shape on the unseen PPMI dataset. Tract2Shape outperforms SOTA deep
learning models across all ten shape measures, achieving the highest average
Pearson's r and the lowest nMSE on the HCP-YA dataset. The ablation study shows
that both multimodal input and PCA contribute to performance gains. On the
unseen testing PPMI dataset, Tract2Shape maintains a high Pearson's r and low
nMSE, demonstrating strong generalizability in cross-dataset evaluation.
Tract2Shape enables fast, accurate, and generalizable prediction of white
matter shape measures from tractography data, supporting scalable analysis
across datasets. This framework lays a promising foundation for future
large-scale white matter shape analysis.

</details>

### [215] [Physics-Driven Neural Compensation For Electrical Impedance Tomography](https://arxiv.org/abs/2504.18067)
*Chuyu Wang,Huiting Deng,Dong Liu*

Main category: eess.IV

TLDR: PhyNC（Physics-driven Neural Compensation）是一种无监督深度学习框架，结合EIT的物理原理，动态分配神经表示能力以解决EIT的逆问题和灵敏度分布问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: EIT在医学和工业中有广泛应用潜力，但面临逆问题的不适定性和灵敏度分布不均的挑战。传统方法或忽略灵敏度变化，或依赖大量数据且泛化性差。

Method: 提出PhyNC，通过动态分配神经表示能力至低灵敏度区域，结合物理原理进行无监督学习。

Result: 在仿真和实验数据上，PhyNC在细节保留和抗伪影方面优于现有方法，尤其在低灵敏度区域表现突出。

Conclusion: PhyNC提升了EIT重建的鲁棒性，并为类似问题的其他成像模态提供了灵活框架。

Abstract: Electrical Impedance Tomography (EIT) provides a non-invasive, portable
imaging modality with significant potential in medical and industrial
applications. Despite its advantages, EIT encounters two primary challenges:
the ill-posed nature of its inverse problem and the spatially variable,
location-dependent sensitivity distribution. Traditional model-based methods
mitigate ill-posedness through regularization but overlook sensitivity
variability, while supervised deep learning approaches require extensive
training data and lack generalization. Recent developments in neural fields
have introduced implicit regularization techniques for image reconstruction,
but these methods typically neglect the physical principles underlying EIT,
thus limiting their effectiveness. In this study, we propose PhyNC
(Physics-driven Neural Compensation), an unsupervised deep learning framework
that incorporates the physical principles of EIT. PhyNC addresses both the
ill-posed inverse problem and the sensitivity distribution by dynamically
allocating neural representational capacity to regions with lower sensitivity,
ensuring accurate and balanced conductivity reconstructions. Extensive
evaluations on both simulated and experimental data demonstrate that PhyNC
outperforms existing methods in terms of detail preservation and artifact
resistance, particularly in low-sensitivity regions. Our approach enhances the
robustness of EIT reconstructions and provides a flexible framework that can be
adapted to other imaging modalities with similar challenges.

</details>

### [216] [Towards a deep learning approach for classifying treatment response in glioblastomas](https://arxiv.org/abs/2504.18268)
*Ana Matoso,Catarina Passarinho,Marta P. Loureiro,José Maria Moreira,Patrícia Figueiredo,Rita G. Nunes*

Main category: eess.IV

TLDR: 该研究首次提出了一种基于深度学习的管道，用于根据连续两次MRI扫描对神经肿瘤学反应评估（RANO）标准进行分类，并在LUMIERE数据集上测试了多种方法，最终使用Densenet264模型取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 胶质母细胞瘤的治疗反应评估复杂且耗时，深度学习在分类问题中的应用为解决这一问题提供了可能。

Method: 研究测试了五种方法，包括输入图像减法、不同模态组合、不同模型架构、不同预训练任务及添加临床数据，最终选择了Densenet264模型。

Result: 最佳模型的平衡准确率为50.96%，并通过可解释性方法验证了模型的关注区域。

Conclusion: 该研究为未来基于RANO标准的胶质母细胞瘤治疗反应评估研究设定了基准，并强调了评估肿瘤治疗反应时多种因素的异质性。

Abstract: Glioblastomas are the most aggressive type of glioma, having a 5-year
survival rate of 6.9%. Treatment typically involves surgery, followed by
radiotherapy and chemotherapy, and frequent magnetic resonance imaging (MRI)
scans to monitor disease progression. To assess treatment response,
radiologists use the Response Assessment in Neuro-Oncology (RANO) criteria to
categorize the tumor into one of four labels based on imaging and clinical
features: complete response, partial response, stable disease, and progressive
disease. This assessment is very complex and time-consuming. Since deep
learning (DL) has been widely used to tackle classification problems, this work
aimed to implement the first DL pipeline for the classification of RANO
criteria based on two consecutive MRI acquisitions. The models were trained and
tested on the open dataset LUMIERE. Five approaches were tested: 1) subtraction
of input images, 2) different combinations of modalities, 3) different model
architectures, 4) different pretraining tasks, and 5) adding clinical data. The
pipeline that achieved the best performance used a Densenet264 considering only
T1-weighted, T2-weighted, and Fluid Attenuated Inversion Recovery (FLAIR)
images as input without any pretraining. A median Balanced Accuracy of 50.96%
was achieved. Additionally, explainability methods were applied. Using Saliency
Maps, the tumor region was often successfully highlighted. In contrast,
Grad-CAM typically failed to highlight the tumor region, with some exceptions
observed in the Complete Response and Progressive Disease classes, where it
effectively identified the tumor region. These results set a benchmark for
future studies on glioblastoma treatment response assessment based on the RANO
criteria while emphasizing the heterogeneity of factors that might play a role
when assessing the tumor's response to treatment.

</details>

### [217] [NUDF: Neural Unsigned Distance Fields for high resolution 3D medical image segmentation](https://arxiv.org/abs/2504.18344)
*Kristine Sørensen,Oscar Camara,Ole de Backer,Klaus Kofoed,Rasmus Paulsen*

Main category: eess.IV

TLDR: 论文提出了一种基于神经无符号距离场（NUDF）的医学图像分割方法，解决了传统方法在高分辨率处理中的内存问题，并能够生成高精度的3D网格模型。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像分割方法在高分辨率处理时面临内存不足问题，而降低分辨率会导致细节丢失。

Method: 通过直接学习神经无符号距离场（NUDF）从图像中，避免了传统离散标签图的问题，支持高分辨率处理和任意拓扑结构的3D网格生成。

Result: 在左心耳（LAA）分割任务中，NUDF方法能够生成高精度3D网格模型，精度达到CT图像体素间距级别。

Conclusion: NUDF方法在医学图像分割中具有高效性和高精度优势，尤其适用于复杂形状的建模。

Abstract: Medical image segmentation is often considered as the task of labelling each
pixel or voxel as being inside or outside a given anatomy. Processing the
images at their original size and resolution often result in insuperable memory
requirements, but downsampling the images leads to a loss of important details.
Instead of aiming to represent a smooth and continuous surface in a binary
voxel-grid, we propose to learn a Neural Unsigned Distance Field (NUDF)
directly from the image. The small memory requirements of NUDF allow for high
resolution processing, while the continuous nature of the distance field allows
us to create high resolution 3D mesh models of shapes of any topology (i.e.
open surfaces). We evaluate our method on the task of left atrial appendage
(LAA) segmentation from Computed Tomography (CT) images. The LAA is a complex
and highly variable shape, being thus difficult to represent with traditional
segmentation methods using discrete labelmaps. With our proposed method, we are
able to predict 3D mesh models that capture the details of the LAA and achieve
accuracy in the order of the voxel spacing in the CT images.

</details>

### [218] [Partition Map-Based Fast Block Partitioning for VVC Inter Coding](https://arxiv.org/abs/2504.18398)
*Xinmin Feng,Zhuoyuan Li,Li Li,Dong Liu,Feng Wu*

Main category: eess.IV

TLDR: 提出了一种基于分区图的快速块划分算法，用于VVC编码中的帧间编码，通过神经网络预测分区图并结合MTT掩码，实现了编码时间节省51.30%，BDBR损失2.12%。


<details>
  <summary>Details</summary>
Motivation: VVC编码中QT+MTT块结构增加了编码复杂度，需要一种高效的方法减少递归分区搜索的开销。

Method: 基于分区图的方法，结合MTT掩码和神经网络预测，采用双阈值决策方案平衡复杂度与性能。

Result: 实验结果显示平均节省51.30%编码时间，BDBR损失2.12%。

Conclusion: 该方法有效降低了VVC编码复杂度，同时保持了良好的率失真性能。

Abstract: Among the new techniques of Versatile Video Coding (VVC), the quadtree with
nested multi-type tree (QT+MTT) block structure yields significant coding gains
by providing more flexible block partitioning patterns. However, the recursive
partition search in the VVC encoder increases the encoder complexity
substantially. To address this issue, we propose a partition map-based
algorithm to pursue fast block partitioning in inter coding. Based on our
previous work on partition map-based methods for intra coding, we analyze the
characteristics of VVC inter coding, and thus improve the partition map by
incorporating an MTT mask for early termination. Next, we develop a neural
network that uses both spatial and temporal features to predict the partition
map. It consists of several special designs including stacked top-down and
bottom-up processing, quantization parameter modulation layers, and
partitioning-adaptive warping. Furthermore, we present a dual-threshold
decision scheme to achieve a fine-grained trade-off between complexity
reduction and rate-distortion (RD) performance loss. The experimental results
demonstrate that the proposed method achieves an average 51.30% encoding time
saving with a 2.12% Bjontegaard Delta Bit Rate (BDBR) under the random access
configuration.

</details>

### [219] [HepatoGEN: Generating Hepatobiliary Phase MRI with Perceptual and Adversarial Models](https://arxiv.org/abs/2504.18405)
*Jens Hooge,Gerard Sanroma-Guell,Faidra Stavropoulou,Alexander Ullmann,Gesine Knobloch,Mark Klemens,Carola Schmidt,Sabine Weckbach,Andreas Bolz*

Main category: eess.IV

TLDR: 研究提出了一种基于深度学习的合成HBP图像方法，比较了三种生成模型，以减少扫描时间并保持诊断效果。


<details>
  <summary>Details</summary>
Motivation: HBP图像的获取时间长，影响患者舒适度和扫描效率，因此需要一种高效替代方法。

Method: 使用三种生成模型（感知U-Net、感知GAN和DDPM）从早期对比阶段合成HBP图像，并通过CES评估数据质量。

Result: pGAN在定量评估中表现最佳，但U-Net在一致性上更优；DDPM表现较差。

Conclusion: 合成HBP图像可行，能减少扫描时间且不损害诊断效果，展示了深度学习在肝脏MRI中的临床潜力。

Abstract: Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays a
crucial role in the detection and characterization of focal liver lesions, with
the hepatobiliary phase (HBP) providing essential diagnostic information.
However, acquiring HBP images requires prolonged scan times, which may
compromise patient comfort and scanner throughput. In this study, we propose a
deep learning based approach for synthesizing HBP images from earlier contrast
phases (precontrast and transitional) and compare three generative models: a
perceptual U-Net, a perceptual GAN (pGAN), and a denoising diffusion
probabilistic model (DDPM). We curated a multi-site DCE-MRI dataset from
diverse clinical settings and introduced a contrast evolution score (CES) to
assess training data quality, enhancing model performance. Quantitative
evaluation using pixel-wise and perceptual metrics, combined with qualitative
assessment through blinded radiologist reviews, showed that pGAN achieved the
best quantitative performance but introduced heterogeneous contrast in
out-of-distribution cases. In contrast, the U-Net produced consistent liver
enhancement with fewer artifacts, while DDPM underperformed due to limited
preservation of fine structural details. These findings demonstrate the
feasibility of synthetic HBP image generation as a means to reduce scan time
without compromising diagnostic utility, highlighting the clinical potential of
deep learning for dynamic contrast enhancement in liver MRI. A project demo is
available at: https://jhooge.github.io/hepatogen

</details>

### [220] [Nearly isotropic segmentation for medial temporal lobe subregions in multi-modality MRI](https://arxiv.org/abs/2504.18442)
*Yue Li,Pulkit Khandelwal,Long Xie,Laura E. M. Wisse,Nidhi Mundada,Christopher A. Brown,Emily McGrew,Amanda Denning,Sandhitsu R. Das,David A. Wolk,Paul A. Yushkevich*

Main category: eess.IV

TLDR: 开发了一种近乎各向同性的分割流程，结合图像和标签上采样，提高了T2加权MRI中内侧颞叶亚区厚度测量的准确性。


<details>
  <summary>Details</summary>
Motivation: T2加权MRI的高平面分辨率虽适合海马亚区分割，但其低平面外分辨率影响了亚区厚度测量的准确性。

Method: 创建高分辨率图谱，结合图像和标签上采样，训练多模态深度学习分割模型。

Result: 近乎各向同性的亚区分割提高了T2加权MRI中皮层厚度作为神经退行性病变生物标志物的准确性。

Conclusion: 该方法显著提升了T2加权MRI中内侧颞叶亚区分割的精度，为神经退行性疾病的诊断提供了更可靠的影像学生物标志物。

Abstract: Morphometry of medial temporal lobe (MTL) subregions in brain MRI is
sensitive biomarker to Alzheimers Disease and other related conditions. While
T2-weighted (T2w) MRI with high in-plane resolution is widely used to segment
hippocampal subfields due to its higher contrast in hippocampus, its lower
out-of-plane resolution reduces the accuracy of subregion thickness
measurements. To address this issue, we developed a nearly isotropic
segmentation pipeline that incorporates image and label upsampling and
high-resolution segmentation in T2w MRI. First, a high-resolution atlas was
created based on an existing anisotropic atlas derived from 29 individuals.
Both T1-weighted and T2w images in the atlas were upsampled from their original
resolution to a nearly isotropic resolution 0.4x0.4x0.52mm3 using a non-local
means approach. Manual segmentations within the atlas were also upsampled to
match this resolution using a UNet-based neural network, which was trained on a
cohort consisting of both high-resolution ex vivo and low-resolution
anisotropic in vivo MRI with manual segmentations. Second, a multi-modality
deep learning-based segmentation model was trained within this nearly isotropic
atlas. Finally, experiments showed the nearly isotropic subregion segmentation
improved the accuracy of cortical thickness as an imaging biomarker for
neurodegeneration in T2w MRI.

</details>

### [221] [RSFR: A Coarse-to-Fine Reconstruction Framework for Diffusion Tensor Cardiac MRI with Semantic-Aware Refinement](https://arxiv.org/abs/2504.18520)
*Jiahao Huang,Fanwen Wang,Pedro F. Ferreira,Haosen Zhang,Yinzhe Wu,Zhifan Gao,Lei Zhu,Angelica I. Aviles-Rivero,Carola-Bibiane Schonlieb,Andrew D. Scott,Zohya Khalique,Maria Dwornik,Ramyah Rajakulasingam,Ranil De Silva,Dudley J. Pennell,Guang Yang,Sonia Nielles-Vallespin*

Main category: eess.IV

TLDR: RSFR框架通过结合语义先验和Vision Mamba重建方法，显著提升了心脏扩散张量成像的重建质量，解决了现有技术中的噪声和伪影问题。


<details>
  <summary>Details</summary>
Motivation: 心脏扩散张量成像（DTI）在临床应用中面临信号噪声比低、伪影和定量保真度不足等技术挑战。

Method: 提出RSFR框架，采用从粗到精的策略，结合Segment Anything Model的零样本语义先验和Vision Mamba重建主干。

Result: 实验表明，RSFR在高欠采样率下实现了最先进的重建质量和准确的DT参数估计。

Conclusion: RSFR具有鲁棒性、可扩展性和临床转化潜力，为心脏DTI提供了有效的解决方案。

Abstract: Cardiac diffusion tensor imaging (DTI) offers unique insights into
cardiomyocyte arrangements, bridging the gap between microscopic and
macroscopic cardiac function. However, its clinical utility is limited by
technical challenges, including a low signal-to-noise ratio, aliasing
artefacts, and the need for accurate quantitative fidelity. To address these
limitations, we introduce RSFR (Reconstruction, Segmentation, Fusion &
Refinement), a novel framework for cardiac diffusion-weighted image
reconstruction. RSFR employs a coarse-to-fine strategy, leveraging zero-shot
semantic priors via the Segment Anything Model and a robust Vision Mamba-based
reconstruction backbone. Our framework integrates semantic features effectively
to mitigate artefacts and enhance fidelity, achieving state-of-the-art
reconstruction quality and accurate DT parameter estimation under high
undersampling rates. Extensive experiments and ablation studies demonstrate the
superior performance of RSFR compared to existing methods, highlighting its
robustness, scalability, and potential for clinical translation in quantitative
cardiac DTI.

</details>

<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [222] [Machine Learning-Based Prediction of Quality Shifts on Video Streaming Over 5G](https://arxiv.org/abs/2504.17938)
*Raza Ul Mustafa,Sesha Dassanayake*

Main category: cs.MM

TLDR: 研究探讨了YouTube视频流中质量切换与信道指标（RSRP、RSRQ、SNR）的关系，发现它们正相关，并提出利用这些指标预测分辨率切换以提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统QoS指标（如带宽、延迟）不足以提升用户体验，需探索新的方法。

Method: 通过分析质量切换与信道指标的关系，使用传统ML分类器进行预测。

Result: 仅用RSRP、RSRQ和SNR，分类准确率达77%。

Conclusion: 在5G时代，该方法可优化OTT服务，提升用户体验。

Abstract: The Quality of Experience (QoE) is the users satisfaction while streaming a
video session over an over-the-top (OTT) platform like YouTube. QoE of YouTube
reflects the smooth streaming session without any buffering and quality shift
events. One of the most important factors nowadays affecting QoE of YouTube is
frequent shifts from higher to lower resolutions and vice versa. These shifts
ensure a smooth streaming session; however, it might get a lower mean opinion
score. For instance, dropping from 1080p to 480p during a video can preserve
continuity but might reduce the viewers enjoyment. Over time, OTT platforms are
looking for alternative ways to boost user experience instead of relying on
traditional Quality of Service (QoS) metrics such as bandwidth, latency, and
throughput. As a result, we look into the relationship between quality shifting
in YouTube streaming sessions and the channel metrics RSRP, RSRQ, and SNR. Our
findings state that these channel metrics positively correlate with shifts.
Thus, in real-time, OTT can only rely on them to predict video streaming
sessions into lower- and higher-resolution categories, thus providing more
resources to improve user experience. Using traditional Machine Learning (ML)
classifiers, we achieved an accuracy of 77-percent, while using only RSRP,
RSRQ, and SNR. In the era of 5G and beyond, where ultra-reliable, low-latency
networks promise enhanced streaming capabilities, the proposed methodology can
be used to improve OTT services.

</details>

<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [223] [Non-identifiability distinguishes Neural Networks among Parametric Models](https://arxiv.org/abs/2504.18017)
*Sourav Chatterjee,Timothy Sudijono*

Main category: math.ST

TLDR: 论文通过证明前馈神经网络在回归任务中与其他参数模型的区别，指出神经网络总能学习到变量间的非平凡关系，而传统参数模型在某些情况下只能学习到常数预测器。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络与传统统计模型的根本区别，特别是在回归任务中的表现差异。

Method: 通过理论证明，比较前馈神经网络与其他平滑参数模型在变量关系学习上的能力。

Result: 神经网络总能学习到非平凡关系，而传统参数模型在某些情况下只能学习到常数预测器。

Conclusion: 神经网络的不可辨识性是其区别于其他平滑参数模型的关键特征。

Abstract: One of the enduring problems surrounding neural networks is to identify the
factors that differentiate them from traditional statistical models. We prove a
pair of results which distinguish feedforward neural networks among parametric
models at the population level, for regression tasks. Firstly, we prove that
for any pair of random variables $(X,Y)$, neural networks always learn a
nontrivial relationship between $X$ and $Y$, if one exists. Secondly, we prove
that for reasonable smooth parametric models, under local and global
identifiability conditions, there exists a nontrivial $(X,Y)$ pair for which
the parametric model learns the constant predictor $\mathbb{E}[Y]$. Together,
our results suggest that a lack of identifiability distinguishes neural
networks among the class of smooth parametric models.

</details>

<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [224] [Enhanced Sampling, Public Dataset and Generative Model for Drug-Protein Dissociation Dynamics](https://arxiv.org/abs/2504.18367)
*Maodong Li,Jiying Zhang,Bin Feng,Wenqi Zeng,Dechin Chen,Zhijun Pan,Yu Li,Zijing Liu,Yi Isaac Yang*

Main category: physics.comp-ph

TLDR: 论文提出了一种结合分子动力学模拟、增强采样和AI生成模型的新方法，用于研究药物-蛋白质结合与解离动力学，并构建了包含大量解离轨迹的数据集DD-13M和生成模型UnbindingFlow。


<details>
  <summary>Details</summary>
Motivation: 现有工具在预测药物-蛋白质结合/解离动力学方面仍有限，需要更高效的方法。

Method: 结合分子动力学模拟、增强采样和AI生成模型，提出增强采样策略以高效实现解离过程并估计自由能表面。

Result: 构建了包含26,612条解离轨迹的DD-13M数据集，并训练了生成模型UnbindingFlow。

Conclusion: DD-13M和UnbindingFlow为计算结构生物学提供了重要进展，未来将扩展至更多药物-蛋白质复合体及应用。

Abstract: Drug-protein binding and dissociation dynamics are fundamental to
understanding molecular interactions in biological systems. While many tools
for drug-protein interaction studies have emerged, especially artificial
intelligence (AI)-based generative models, predictive tools on
binding/dissociation kinetics and dynamics are still limited. We propose a
novel research paradigm that combines molecular dynamics (MD) simulations,
enhanced sampling, and AI generative models to address this issue. We propose
an enhanced sampling strategy to efficiently implement the drug-protein
dissociation process in MD simulations and estimate the free energy surface
(FES). We constructed a program pipeline of MD simulations based on this
sampling strategy, thus generating a dataset including 26,612 drug-protein
dissociation trajectories containing about 13 million frames. We named this
dissociation dynamics dataset DD-13M and used it to train a deep equivariant
generative model UnbindingFlow, which can generate collision-free dissociation
trajectories. The DD-13M database and UnbindingFlow model represent a
significant advancement in computational structural biology, and we anticipate
its broad applicability in machine learning studies of drug-protein
interactions. Our ongoing efforts focus on expanding this methodology to
encompass a broader spectrum of drug-protein complexes and exploring novel
applications in pathway prediction.

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [225] [Tracking Articulatory Dynamics in Speech with a Fixed-Weight BiLSTM-CNN Architecture](https://arxiv.org/abs/2504.18099)
*Leena G Pillai,D. Muhammad Noorul Mubarak,Elizabeth Sherly*

Main category: cs.SD

TLDR: 提出了一种基于BiLSTM和CNN的新方法，用于预测语音声学中的舌和唇发音特征，固定权重初始化优于自适应权重，性能在多种模式下表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究语音产生中舌和唇的发音特征预测，以提升语音清晰度和准确性。

Method: 使用堆叠BiLSTM结合一维CNN进行后处理，固定权重初始化，训练基于EMA和语音数据集。

Result: 固定权重方法在SD、SI、CD和CC模式下表现更优，训练周期更少。

Conclusion: 该方法为发音特征预测提供了高效模型，推动了语音产生研究和应用的发展。

Abstract: Speech production is a complex sequential process which involve the
coordination of various articulatory features. Among them tongue being a highly
versatile active articulator responsible for shaping airflow to produce
targeted speech sounds that are intellectual, clear, and distinct. This paper
presents a novel approach for predicting tongue and lip articulatory features
involved in a given speech acoustics using a stacked Bidirectional Long
Short-Term Memory (BiLSTM) architecture, combined with a one-dimensional
Convolutional Neural Network (CNN) for post-processing with fixed weights
initialization. The proposed network is trained with two datasets consisting of
simultaneously recorded speech and Electromagnetic Articulography (EMA)
datasets, each introducing variations in terms of geographical origin,
linguistic characteristics, phonetic diversity, and recording equipment. The
performance of the model is assessed in Speaker Dependent (SD), Speaker
Independent (SI), corpus dependent (CD) and cross corpus (CC) modes.
Experimental results indicate that the proposed model with fixed weights
approach outperformed the adaptive weights initialization with in relatively
minimal number of training epochs. These findings contribute to the development
of robust and efficient models for articulatory feature prediction, paving the
way for advancements in speech production research and applications.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [226] [My Precious Crash Data: Barriers and Opportunities in Encouraging Autonomous Driving Companies to Share Safety-Critical Data](https://arxiv.org/abs/2504.17792)
*Hauke Sandhaus,Angel Hsing-Chi Hwang,Wendy Ju,Qian Yang*

Main category: cs.HC

TLDR: 本文探讨了自动驾驶公司不愿共享安全关键数据的原因，并提出了促进共享的新方法。


<details>
  <summary>Details</summary>
Motivation: 共享安全关键数据有助于提升自动驾驶车辆的安全性，但公司因竞争和资源问题不愿共享。

Method: 通过采访12名自动驾驶公司员工，分析数据共享的障碍。

Result: 发现两大障碍：数据共享涉及公司内部政治竞争，以及公司视安全知识为竞争优势而非公共资源。

Conclusion: 提出激励数据共享的新方法，包括区分公共与私有知识、创新数据工具和补偿数据整理成本。

Abstract: Safety-critical data, such as crash and near-crash records, are crucial to
improving autonomous vehicle (AV) design and development. Sharing such data
across AV companies, academic researchers, regulators, and the public can help
make all AVs safer. However, AV companies rarely share safety-critical data
externally. This paper aims to pinpoint why AV companies are reluctant to share
safety-critical data, with an eye on how these barriers can inform new
approaches to promote sharing. We interviewed twelve AV company employees who
actively work with such data in their day-to-day work. Findings suggest two
key, previously unknown barriers to data sharing: (1) Datasets inherently embed
salient knowledge that is key to improving AV safety and are
resource-intensive. Therefore, data sharing, even within a company, is fraught
with politics. (2) Interviewees believed AV safety knowledge is private
knowledge that brings competitive edges to their companies, rather than public
knowledge for social good. We discuss the implications of these findings for
incentivizing and enabling safety-critical AV data sharing, specifically,
implications for new approaches to (1) debating and stratifying public and
private AV safety knowledge, (2) innovating data tools and data sharing
pipelines that enable easier sharing of public AV safety data and knowledge;
(3) offsetting costs of curating safety-critical data and incentivizing data
sharing.

</details>

### [227] [Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents](https://arxiv.org/abs/2504.17934)
*Chaoran Chen,Zhiping Zhang,Ibrahim Khalilov,Bingcan Guo,Simret A Gebreegziabher,Yanfang Ye,Ziang Xiao,Yaxing Yao,Tianshi Li,Toby Jia-Jun Li*

Main category: cs.HC

TLDR: 本文探讨了LLM驱动的GUI代理在处理敏感数据时的隐私和安全风险，提出了一种以人为中心的评估框架。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的GUI代理在自动化处理敏感数据时缺乏隐私和安全评估，亟需解决这一问题。

Method: 分析了GUI代理的三大风险，比较了与传统GUI自动化和通用自主代理的区别，并提出了整合人类评估者的挑战。

Result: 现有评估主要关注性能，隐私和安全评估不足，需改进框架。

Conclusion: 建议采用以人为中心的评估框架，结合风险评估、用户意识和隐私安全设计。

Abstract: The rise of Large Language Models (LLMs) has revolutionized Graphical User
Interface (GUI) automation through LLM-powered GUI agents, yet their ability to
process sensitive data with limited human oversight raises significant privacy
and security risks. This position paper identifies three key risks of GUI
agents and examines how they differ from traditional GUI automation and general
autonomous agents. Despite these risks, existing evaluations focus primarily on
performance, leaving privacy and security assessments largely unexplored. We
review current evaluation metrics for both GUI and general LLM agents and
outline five key challenges in integrating human evaluators for GUI agent
assessments. To address these gaps, we advocate for a human-centered evaluation
framework that incorporates risk assessments, enhances user awareness through
in-context consent, and embeds privacy and security considerations into GUI
agent design and evaluation.

</details>

### [228] [Evaluating Machine Expertise: How Graduate Students Develop Frameworks for Assessing GenAI Content](https://arxiv.org/abs/2504.17964)
*Celia Chen,Alex Leitch*

Main category: cs.HC

TLDR: 研究生通过与大型语言模型（LLM）的互动，构建了评估AI生成内容的框架，受专业身份、验证能力和系统导航经验的影响。


<details>
  <summary>Details</summary>
Motivation: 研究探讨研究生如何评估和与AI生成内容互动，以揭示人类与生成式AI的交互模式。

Method: 通过定性研究，结合调查、LLM互动记录和对14名研究生的深度访谈。

Result: 学生构建的评估框架受专业身份、验证能力和系统导航经验影响，对不同内容采取选择性接受或拒绝。

Conclusion: 研究揭示了人类与生成式AI的交互模式，为平台支持用户评估AI生成内容提供了建议。

Abstract: This paper examines how graduate students develop frameworks for evaluating
machine-generated expertise in web-based interactions with large language
models (LLMs). Through a qualitative study combining surveys, LLM interaction
transcripts, and in-depth interviews with 14 graduate students, we identify
patterns in how these emerging professionals assess and engage with
AI-generated content. Our findings reveal that students construct evaluation
frameworks shaped by three main factors: professional identity, verification
capabilities, and system navigation experience. Rather than uniformly accepting
or rejecting LLM outputs, students protect domains central to their
professional identities while delegating others--with managers preserving
conceptual work, designers safeguarding creative processes, and programmers
maintaining control over core technical expertise. These evaluation frameworks
are further influenced by students' ability to verify different types of
content and their experience navigating complex systems. This research
contributes to web science by highlighting emerging human-genAI interaction
patterns and suggesting how platforms might better support users in developing
effective frameworks for evaluating machine-generated expertise signals in
AI-mediated web environments.

</details>

### [229] [Streaming, Fast and Slow: Cognitive Load-Aware Streaming for Efficient LLM Serving](https://arxiv.org/abs/2504.17999)
*Chang Xiao,Brenda Yang*

Main category: cs.HC

TLDR: 论文提出了一种自适应流式方法，根据推断的认知负载动态调整LLM输出的速度，以减少计算资源浪费。


<details>
  <summary>Details</summary>
Motivation: 当前LLM生成的对话接口通常以固定的速度输出内容，忽略了人类阅读速度和认知负载的差异，导致计算资源浪费。

Method: 提出了一种自适应流式方法，实时调整输出速度，根据内容复杂度动态调整。

Result: 实验表明，该方法可减少高达16.8%的计算资源消耗，同时不影响用户体验。

Conclusion: 该方法为云对话AI接口提供了一种高效的计算资源管理策略。

Abstract: Generative conversational interfaces powered by large language models (LLMs)
typically stream output token-by-token at a rate determined by computational
budget, often neglecting actual human reading speeds and the cognitive load
associated with the content. This mismatch frequently leads to inefficient use
of computational resources. For example, in cloud-based services, streaming
content faster than users can read appears unnecessary, resulting in wasted
computational resources and potential delays for other users, particularly
during peak usage periods. To address this issue, we propose an adaptive
streaming method that dynamically adjusts the pacing of LLM streaming output in
real-time based on inferred cognitive load. Our approach estimates the
cognitive load associated with streaming content and strategically slows down
the stream during complex or information-rich segments, thereby freeing
computational resources for other users. Our statistical analysis of
computational savings, combined with crowdsourced user studies, provides
insights into the trade-offs between service efficiency and user satisfaction,
demonstrating that our method can significantly reduce computational
consumption up to 16.8\%. This context-aware computational resource management
strategy presents a practical framework for enhancing system efficiency in
cloud-based conversational AI interfaces without compromising user experience.

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [230] [Subfunction Structure Matters: A New Perspective on Local Optima Networks](https://arxiv.org/abs/2504.17799)
*S. L. Thomson,M. W. Przewozniczek*

Main category: cs.NE

TLDR: 论文提出了一种改进局部最优网络（LON）分析的方法，通过结合子函数信息（已知或学习得到）来丰富优化动态的洞察。


<details>
  <summary>Details</summary>
Motivation: 当前LON的构建和分析未充分利用问题结构信息，限制了其对优化动态的理解。

Method: 采用三种方法构建LON：标准算法、确定性灰盒交叉算法和基于学习变量交互的扰动选择算法，并提出了与子函数变化相关的度量指标。

Result: 结合问题结构的LON分析能提供更丰富的优化动态信息，有助于理解问题求解难度。

Conclusion: 建议在已知或疑似子函数结构的问题中，将问题结构纳入景观分析的新范式。

Abstract: Local optima networks (LONs) capture fitness landscape information. They are
typically constructed in a black-box manner; information about the problem
structure is not utilised. This also applies to the analysis of LONs: knowledge
about the problem, such as interaction between variables, is not considered. We
challenge this status-quo with an alternative approach: we consider how LON
analysis can be improved by incorporating subfunction-based information - this
can either be known a-priori or learned during search. To this end, LONs are
constructed for several benchmark pseudo-boolean problems using three
approaches: firstly, the standard algorithm; a second algorithm which uses
deterministic grey-box crossover; and a third algorithm which selects
perturbations based on learned information about variable interactions. Metrics
related to subfunction changes in a LON are proposed and compared with metrics
from previous literature which capture other aspects of a LON. Incorporating
problem structure in LON construction and analysing it can bring enriched
insight into optimisation dynamics. Such information may be crucial to
understanding the difficulty of solving a given problem with state-of-the-art
linkage learning optimisers. In light of the results, we suggest incorporation
of problem structure as an alternative paradigm in landscape analysis for
problems with known or suspected subfunction structure.

</details>

### [231] [Evolution of Optimization Algorithms for Global Placement via Large Language Models](https://arxiv.org/abs/2504.17801)
*Xufeng Yao,Jiaxi Jiang,Yuxuan Zhao,Peiyu Liao,Yibo Lin,Bei Yu*

Main category: cs.NE

TLDR: 本文提出了一种利用大语言模型（LLM）自动生成和优化全局布局算法的方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 手动设计优化算法耗时且依赖专家知识，而现有全局布局算法的核心优化仍依赖启发式和定制组件。

Method: 通过精心设计的提示生成多样化候选算法，并引入基于LLM的遗传流程进行优化。

Result: 发现的算法在多个基准测试中平均HPWL提升5.05%至8.30%，个别案例提升达17%。

Conclusion: 该方法不仅性能优越，还具有良好的泛化能力，并能与现有参数调优方法互补。

Abstract: Optimization algorithms are widely employed to tackle complex problems, but
designing them manually is often labor-intensive and requires significant
expertise. Global placement is a fundamental step in electronic design
automation (EDA). While analytical approaches represent the state-of-the-art
(SOTA) in global placement, their core optimization algorithms remain heavily
dependent on heuristics and customized components, such as initialization
strategies, preconditioning methods, and line search techniques. This paper
presents an automated framework that leverages large language models (LLM) to
evolve optimization algorithms for global placement. We first generate diverse
candidate algorithms using LLM through carefully crafted prompts. Then we
introduce an LLM-based genetic flow to evolve selected candidate algorithms.
The discovered optimization algorithms exhibit substantial performance
improvements across many benchmarks. Specifically, Our design-case-specific
discovered algorithms achieve average HPWL improvements of \textbf{5.05\%},
\text{5.29\%} and \textbf{8.30\%} on MMS, ISPD2005 and ISPD2019 benchmarks, and
up to \textbf{17\%} improvements on individual cases. Additionally, the
discovered algorithms demonstrate good generalization ability and are
complementary to existing parameter-tuning methods.

</details>

### [232] [Fuzzy Logic -- Based Scheduling System for Part-Time Workforce](https://arxiv.org/abs/2504.17805)
*Tri Nguyen,Kelly Cohen*

Main category: cs.NE

TLDR: 论文研究了遗传模糊系统在生成大学兼职学生排班表中的应用，算法高效且适应人员不足的情况。


<details>
  <summary>Details</summary>
Motivation: 解决大学兼职学生排班问题，考虑员工偏好和运营需求。

Method: 使用遗传模糊系统，基于学生可用性和偏好训练和测试算法。

Result: 算法能高效生成满足运营需求的排班表，且在人员不足时表现稳健。

Conclusion: 遗传模糊系统适用于复杂排班问题，具有实用性和鲁棒性。

Abstract: This paper explores the application of genetic fuzzy systems to efficiently
generate schedules for a team of part-time student workers at a university.
Given the preferred number of working hours and availability of employees, our
model generates feasible solutions considering various factors, such as maximum
weekly hours, required number of workers on duty, and the preferred number of
working hours. The algorithm is trained and tested with availability data
collected from students at the University of Cincinnati. The results
demonstrate the algorithm's efficiency in producing schedules that meet
operational criteria and its robustness in understaffed conditions.

</details>

### [233] [Evolution Meets Diffusion: Efficient Neural Architecture Generation](https://arxiv.org/abs/2504.17827)
*Bingye Zhou,Caiyang Yu*

Main category: cs.NE

TLDR: 论文提出了一种名为EDNAG的新方法，结合进化算法和扩散模型，实现高效且无需训练的神经网络架构生成，显著提升了性能和速度。


<details>
  <summary>Details</summary>
Motivation: 神经架构搜索（NAS）的计算和时间成本高，现有方法如扩散模型在全局搜索能力和效率上存在不足。

Method: 提出EDNAG方法，利用进化算法模拟扩散模型的去噪过程，通过适应度引导从随机分布生成最优架构。

Result: EDNAG在架构优化中达到SOTA性能，准确率提升10.45%，推理速度平均提升50倍。

Conclusion: EDNAG结合进化策略和扩散模型的优势，实现了高效且无需训练的架构生成，具有显著的实际应用价值。

Abstract: Neural Architecture Search (NAS) has gained widespread attention for its
transformative potential in deep learning model design. However, the vast and
complex search space of NAS leads to significant computational and time costs.
Neural Architecture Generation (NAG) addresses this by reframing NAS as a
generation problem, enabling the precise generation of optimal architectures
for specific tasks. Despite its promise, mainstream methods like diffusion
models face limitations in global search capabilities and are still hindered by
high computational and time demands. To overcome these challenges, we propose
Evolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel
approach that achieves efficient and training-free architecture generation.
EDNAG leverages evolutionary algorithms to simulate the denoising process in
diffusion models, using fitness to guide the transition from random Gaussian
distributions to optimal architecture distributions. This approach combines the
strengths of evolutionary strategies and diffusion models, enabling rapid and
effective architecture generation. Extensive experiments demonstrate that EDNAG
achieves state-of-the-art (SOTA) performance in architecture optimization, with
an improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need
for time-consuming training and boosts inference speed by an average of 50
times, showcasing its exceptional efficiency and effectiveness.

</details>

### [234] [Near-Driven Autonomous Rover Navigation in Complex Environments: Extensions to Urban Search-and-Rescue and Industrial Inspection](https://arxiv.org/abs/2504.17794)
*Dhadkan Shrestha,Lincoln Bhattarai*

Main category: cs.NE

TLDR: 论文探讨了基于NEAT的扩展神经进化方法在动态环境中用于自主机器人的表现，展示了其在复杂任务中的适应性和优化效果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证NEAT方法在危险任务（如灭火、搜救和工业检查）中的适用性，并探索其作为深度强化学习替代方案的潜力。

Method: 通过结合NEAT和强化学习的最新进展，使用现代仿真框架和混合算法进行优化，并在复杂3D导航环境中测试。

Result: 实验表明，NEAT演化的控制器在成功率上与先进深度强化学习方法相当，且结构适应性更强，户外测试成功率达80%。

Conclusion: NEAT在多样化自主任务中表现优异，具备实际部署潜力，可作为深度强化学习的补充或替代方案。

Abstract: This paper explores the use of an extended neuroevolutionary approach, based
on NeuroEvolution of Augmenting Topologies (NEAT), for autonomous robots in
dynamic environments associated with hazardous tasks like firefighting, urban
search-and-rescue (USAR), and industrial inspections. Building on previous
research, it expands the simulation environment to larger and more complex
settings, demonstrating NEAT's adaptability across different applications. By
integrating recent advancements in NEAT and reinforcement learning, the study
uses modern simulation frameworks for realism and hybrid algorithms for
optimization. Experimental results show that NEAT-evolved controllers achieve
success rates comparable to state-of-the-art deep reinforcement learning
methods, with superior structural adaptability. The agents reached ~80% success
in outdoor tests, surpassing baseline models. The paper also highlights the
benefits of transfer learning among tasks and evaluates the effectiveness of
NEAT in complex 3D navigation. Contributions include evaluating NEAT for
diverse autonomous applications and discussing real-world deployment
considerations, emphasizing the approach's potential as an alternative or
complement to deep reinforcement learning in autonomous navigation tasks.

</details>

### [235] [Switch-Based Multi-Part Neural Network](https://arxiv.org/abs/2504.18241)
*Surajit Majumder,Paritosh Ranjan,Prodip Roy,Bhuban Padhan*

Main category: cs.NE

TLDR: 本文提出了一种去中心化和模块化的神经网络框架，旨在提升AI系统的可扩展性、可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统神经网络在可扩展性、可解释性和分布式环境中的局限性，设计了一种动态切换机制，模拟生物大脑的任务专业化。

Method: 采用动态切换机制选择性地激活和训练神经元，结合联邦学习和去中心化训练，模拟局部训练。

Result: 实验表明，模块化网络能高效训练和评估，同时提升模型透明度和可解释性。

Conclusion: 该框架为设计可扩展、隐私保护且高效的AI系统提供了新思路，适用于多样化应用。

Abstract: This paper introduces decentralized and modular neural network framework
designed to enhance the scalability, interpretability, and performance of
artificial intelligence (AI) systems. At the heart of this framework is a
dynamic switch mechanism that governs the selective activation and training of
individual neurons based on input characteristics, allowing neurons to
specialize in distinct segments of the data domain. This approach enables
neurons to learn from disjoint subsets of data, mimicking biological brain
function by promoting task specialization and improving the interpretability of
neural network behavior. Furthermore, the paper explores the application of
federated learning and decentralized training for real-world AI deployments,
particularly in edge computing and distributed environments. By simulating
localized training on non-overlapping data subsets, we demonstrate how modular
networks can be efficiently trained and evaluated. The proposed framework also
addresses scalability, enabling AI systems to handle large datasets and
distributed processing while preserving model transparency and
interpretability. Finally, we discuss the potential of this approach in
advancing the design of scalable, privacy-preserving, and efficient AI systems
for diverse applications.

</details>

<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [236] [Time and Frequency Domain-based Anomaly Detection in Smart Meter Data for Distribution Network Studies](https://arxiv.org/abs/2504.18231)
*Petar Labura,Tomislav Antic,Tomislav Capuder*

Main category: eess.SY

TLDR: 本文提出了一种基于孤立森林算法和快速傅里叶变换滤波的异常检测框架，用于检测和缓解低压配电网络中智能电表数据的异常影响。


<details>
  <summary>Details</summary>
Motivation: 随着低压配电网络中智能电表等新技术的广泛应用，运营商需要实时计算网络状态，但现有数据驱动算法未考虑数据质量，缺乏异常检测机制。

Method: 结合孤立森林机器学习算法和快速傅里叶变换滤波，在时域和频域中检测异常，不受点异常或上下文异常的影响。

Result: 提出的框架能够有效区分和处理智能电表数据中的异常，适用于高比例智能电表的配电网络。

Conclusion: 集成异常检测方法对配电网络至关重要，特别是在智能电表普及的情况下。

Abstract: The widespread integration of new technologies in low-voltage distribution
networks on the consumer side creates the need for distribution system
operators to perform advanced real-time calculations to estimate network
conditions. In recent years, data-driven models based on machine learning and
big data analysis have emerged for calculation purposes, leveraging the
information available in large datasets obtained from smart meters and other
advanced measurement infrastructure. However, existing data-driven algorithms
do not take into account the quality of data collected from smart meters. They
lack built-in anomaly detection mechanisms and fail to differentiate anomalies
based on whether the value or context of anomalous data instances deviates from
the norm. This paper focuses on methods for detecting and mitigating the impact
of anomalies on the consumption of active and reactive power datasets. It
proposes an anomaly detection framework based on the Isolation Forest machine
learning algorithm and Fast Fourier Transform filtering that works in both the
time and frequency domain and is unaffected by point anomalies or contextual
anomalies of the power consumption data. The importance of integrating anomaly
detection methods is demonstrated in the analysis important for distribution
networks with a high share of smart meters.

</details>

### [237] [Boosting-Enabled Robust System Identification of Partially Observed LTI Systems Under Heavy-Tailed Noise](https://arxiv.org/abs/2504.18444)
*Vinay Kanakeri,Aritra Mitra*

Main category: eess.SY

TLDR: 论文提出了一种针对部分观测线性时不变（LTI）系统的系统辨识方法，适用于重尾噪声环境，仅需噪声的二阶矩假设。通过鲁棒统计和增强技术，算法在样本复杂度上接近子高斯噪声假设下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常假设噪声为高斯或子高斯分布，限制了实际应用。本文旨在放宽噪声假设，仅需二阶矩条件，以适应更广泛的噪声分布。

Method: 提出了一种基于鲁棒统计和增强技术的系统辨识算法，适用于重尾噪声环境。

Result: 算法在样本复杂度上接近子高斯噪声假设下的性能，且仅需噪声的四阶矩有限。

Conclusion: 该算法在更弱的噪声假设下实现了接近最优的性能，扩展了系统辨识的应用范围。

Abstract: We consider the problem of system identification of partially observed linear
time-invariant (LTI) systems. Given input-output data, we provide
non-asymptotic guarantees for identifying the system parameters under general
heavy-tailed noise processes. Unlike previous works that assume Gaussian or
sub-Gaussian noise, we consider significantly broader noise distributions that
are required to admit only up to the second moment. For this setting, we
leverage tools from robust statistics to propose a novel system identification
algorithm that exploits the idea of boosting. Despite the much weaker noise
assumptions, we show that our proposed algorithm achieves sample complexity
bounds that nearly match those derived under sub-Gaussian noise. In particular,
we establish that our bounds retain a logarithmic dependence on the prescribed
failure probability. Interestingly, we show that such bounds can be achieved by
requiring just a finite fourth moment on the excitatory input process.

</details>

<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [238] [Material Identification Via RFID For Smart Shopping](https://arxiv.org/abs/2504.17898)
*David Wang,Derek Goh,Jiale Zhang*

Main category: eess.SP

TLDR: 论文提出了一种利用RFID标签信号衰减和散射特性检测隐藏物品的系统，结合神经网络分类和距离测量，实现了高精度的实时防盗。


<details>
  <summary>Details</summary>
Motivation: 解决无人商店中隐藏物品（如背包、口袋中的商品）导致的盗窃问题。

Method: 利用RFID标签的RSSI和相位角数据训练神经网络，分类七种常见容器，并结合距离测量提高准确性。

Result: 在模拟零售环境中，系统对单次读取的准确率为74%，一秒样本的准确率为89%；结合距离测量后，0.3-2米范围内的准确率为82%。

Conclusion: 该系统通过结合材料识别和计算机视觉跟踪，利用现有基础设施实现了主动防盗，适用于无人零售场景。

Abstract: Cashierless stores rely on computer vision and RFID tags to associate
shoppers with items, but concealed items placed in backpacks, pockets, or bags
create challenges for theft prevention. We introduce a system that turns
existing RFID tagged items into material sensors by exploiting how different
containers attenuate and scatter RF signals. Using RSSI and phase angle, we
trained a neural network to classify seven common containers. In a simulated
retail environment, the model achieves 89% accuracy with one second samples and
74% accuracy from single reads. Incorporating distance measurements, our system
achieves 82% accuracy across 0.3-2m tag to reader separations. When deployed at
aisle or doorway choke points, the system can flag suspicious events in real
time, prompting camera screening or staff intervention. By combining material
identification with computer vision tracking, our system provides proactive
loss prevention for cashierless retail while utilizing existing infrastructure.

</details>

<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [239] [Kimi-Audio Technical Report](https://arxiv.org/abs/2504.18425)
*KimiTeam,Ding Ding,Zeqian Ju,Yichong Leng,Songxiang Liu,Tong Liu,Zeyu Shang,Kai Shen,Wei Song,Xu Tan,Heyi Tang,Zhengtao Wang,Chu Wei,Yifei Xin,Xinran Xu,Jianwei Yu,Yutao Zhang,Xinyu Zhou,Y. Charles,Jun Chen,Yanru Chen,Yulun Du,Weiran He,Zhenxing Hu,Guokun Lai,Qingcheng Li,Yangyang Liu,Weidong Sun,Jianzhou Wang,Yuzhi Wang,Yuefeng Wu,Yuxin Wu,Dongchao Yang,Hao Yang,Ying Yang,Zhilin Yang,Aoxiong Yin,Ruibin Yuan,Yutong Zhang,Zaida Zhou*

Main category: eess.AS

TLDR: Kimi-Audio是一个开源音频基础模型，擅长音频理解、生成和对话，通过创新的架构和大规模数据集训练，在多个音频任务上达到领先水平。


<details>
  <summary>Details</summary>
Motivation: 构建一个多功能音频基础模型，支持广泛的音频任务，并通过开源促进社区发展。

Method: 采用12.5Hz音频分词器，设计基于LLM的架构，结合流式去分词器，利用超过1300万小时的预训练数据和高质量后训练数据。

Result: 在语音识别、音频理解、问答和对话等任务上表现优异，达到最先进水平。

Conclusion: Kimi-Audio是一个强大的音频基础模型，通过开源代码和工具推动音频领域的研究和应用。

Abstract: We present Kimi-Audio, an open-source audio foundation model that excels in
audio understanding, generation, and conversation. We detail the practices in
building Kimi-Audio, including model architecture, data curation, training
recipe, inference deployment, and evaluation. Specifically, we leverage a
12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous
features as input and discrete tokens as output, and develop a chunk-wise
streaming detokenizer based on flow matching. We curate a pre-training dataset
that consists of more than 13 million hours of audio data covering a wide range
of modalities including speech, sound, and music, and build a pipeline to
construct high-quality and diverse post-training data. Initialized from a
pre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text
data with several carefully designed tasks, and then fine-tuned to support a
diverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio
achieves state-of-the-art performance on a range of audio benchmarks including
speech recognition, audio understanding, audio question answering, and speech
conversation. We release the codes, model checkpoints, as well as the
evaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.

</details>

<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [240] [Lecture Notes on Normalizing Flows for Lattice Quantum Field Theories](https://arxiv.org/abs/2504.18126)
*Miranda C. N. Cheng,Niki Stratikopoulou*

Main category: hep-lat

TLDR: 论文探讨了机器学习在晶格场理论中的应用，特别是在非微扰区域和临界点附近的研究。


<details>
  <summary>Details</summary>
Motivation: 晶格场理论在非微扰区域的研究中面临挑战，尤其是在连续极限或临界点附近，机器学习提供了一种潜在的解决方案。

Method: 结合晶格场理论和归一化流（normalizing flows）方法，利用机器学习技术进行研究。

Result: 机器学习为晶格场理论的研究提供了新的工具和视角。

Conclusion: 机器学习在晶格场理论中的应用具有广阔前景，尤其是在处理复杂拓扑结构时。

Abstract: Numerical simulations of quantum field theories on lattices serve as a
fundamental tool for studying the non-perturbative regime of the theories,
where analytic tools often fall short. Challenges arise when one takes the
continuum limit or as the system approaches a critical point, especially in the
presence of non-trivial topological structures in the theory. Rapid recent
advances in machine learning provide a promising avenue for progress in this
area. These lecture notes aim to give a brief account of lattice field
theories, normalizing flows, and how the latter can be applied to study the
former. The notes are based on the lectures given by the first author in
various recent research schools.

</details>

<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [241] [Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning](https://arxiv.org/abs/2504.17950)
*Isadora White,Kolby Nottingham,Ayush Maniar,Max Robinson,Hansen Lillemark,Mehul Maheshwari,Lianhui Qin,Prithviraj Ammanabrolu*

Main category: cs.MA

TLDR: 研究探讨了LLM在复杂具身推理任务中的自适应协作能力，提出了MINDcraft平台和MineCollab基准，发现当前LLM代理在协作中的主要瓶颈是自然语言沟通效率。


<details>
  <summary>Details</summary>
Motivation: 协作在日常生活和任务完成中至关重要，研究旨在探索LLM在具身协作任务中的潜力。

Method: 开发了MINDcraft平台和MineCollab基准，用于测试LLM代理在Minecraft游戏中的协作能力。

Result: 实验表明，当前LLM代理在协作中的主要瓶颈是自然语言沟通，性能下降高达15%。

Conclusion: 现有LLM代理在多代理协作中表现不佳，需探索超越上下文学习和模仿学习的方法。

Abstract: Collaboration is ubiquitous and essential in day-to-day life -- from
exchanging ideas, to delegating tasks, to generating plans together. This work
studies how LLMs can adaptively collaborate to perform complex embodied
reasoning tasks. To this end we introduce MINDcraft, an easily extensible
platform built to enable LLM agents to control characters in the open-world
game of Minecraft; and MineCollab, a benchmark to test the different dimensions
of embodied and collaborative reasoning. An experimental study finds that the
primary bottleneck in collaborating effectively for current state-of-the-art
agents is efficient natural language communication, with agent performance
dropping as much as 15% when they are required to communicate detailed task
completion plans. We conclude that existing LLM agents are ill-optimized for
multi-agent collaboration, especially in embodied scenarios, and highlight the
need to employ methods beyond in-context and imitation learning. Our website
can be found here: https://mindcraft-minecollab.github.io/

</details>

<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [242] [SOFARI-R: High-Dimensional Manifold-Based Inference for Latent Responses](https://arxiv.org/abs/2504.17874)
*Zemin Zheng,Xin Zhou,Jinchi Lv*

Main category: stat.ME

TLDR: 本文提出了一种新的高维流形SOFAR推断方法（SOFARI-R），用于潜在右因子向量的推断，解决了左、右奇异向量不对称的挑战。


<details>
  <summary>Details</summary>
Motivation: 在多任务学习中，数据降维和不确定性量化至关重要，但现有方法对右因子向量的推断存在困难。

Method: 提出了两种SOFARI-R变体：一种处理强正交因子，另一种处理弱正交因子，均生成偏差校正估计量。

Result: 新方法生成了具有渐近正态分布的偏差校正估计量，并通过模拟研究和经济应用验证了其有效性。

Conclusion: SOFARI-R为潜在右因子向量的推断提供了有效且可靠的方法。

Abstract: Data reduction with uncertainty quantification plays a key role in various
multi-task learning applications, where large numbers of responses and features
are present. To this end, a general framework of high-dimensional
manifold-based SOFAR inference (SOFARI) was introduced recently in Zheng, Zhou,
Fan and Lv (2024) for interpretable multi-task learning inference focusing on
the left factor vectors and singular values exploiting the latent singular
value decomposition (SVD) structure. Yet, designing a valid inference procedure
on the latent right factor vectors is not straightforward from that of the left
ones and can be even more challenging due to asymmetry of left and right
singular vectors in the response matrix. To tackle these issues, in this paper
we suggest a new method of high-dimensional manifold-based SOFAR inference for
latent responses (SOFARI-R), where two variants of SOFARI-R are introduced. The
first variant deals with strongly orthogonal factors by coupling left singular
vectors with the design matrix and then appropriately rescaling them to
generate new Stiefel manifolds. The second variant handles the more general
weakly orthogonal factors by employing the hard-thresholded SOFARI estimates
and delicately incorporating approximation errors into the distribution. Both
variants produce bias-corrected estimators for the latent right factor vectors
that enjoy asymptotically normal distributions with justified asymptotic
variance estimates. We demonstrate the effectiveness of the newly suggested
method using extensive simulation studies and an economic application.

</details>

<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [243] [Efficient Learning on Large Graphs using a Densifying Regularity Lemma](https://arxiv.org/abs/2504.18273)
*Jonathan Kouchly,Ben Finkelshtein,Michael Bronstein,Ron Levie*

Main category: cs.SI

TLDR: 提出了一种基于相交二分图组件的低秩分解方法（IBG），用于高效近似大规模有向图，解决了传统图神经网络的计算和内存成本问题。


<details>
  <summary>Details</summary>
Motivation: 传统消息传递神经网络在大规模图上的计算和内存成本随边数线性增长，限制了其应用。

Method: 引入IBG，通过低秩分解和加权非边的方式近似任何图，并证明其构造性弱正则引理。

Result: IBG的秩仅依赖于近似精度，与图的稀疏性无关，且在节点分类、时空图分析和知识图谱补全任务中表现优异。

Conclusion: IBG提供了一种高效且通用的图表示方法，显著降低了计算和内存复杂度。

Abstract: Learning on large graphs presents significant challenges, with traditional
Message Passing Neural Networks suffering from computational and memory costs
scaling linearly with the number of edges. We introduce the Intersecting Block
Graph (IBG), a low-rank factorization of large directed graphs based on
combinations of intersecting bipartite components, each consisting of a pair of
communities, for source and target nodes. By giving less weight to non-edges,
we show how to efficiently approximate any graph, sparse or dense, by a dense
IBG. Specifically, we prove a constructive version of the weak regularity
lemma, showing that for any chosen accuracy, every graph, regardless of its
size or sparsity, can be approximated by a dense IBG whose rank depends only on
the accuracy. This dependence of the rank solely on the accuracy, and not on
the sparsity level, is in contrast to previous forms of the weak regularity
lemma. We present a graph neural network architecture operating on the IBG
representation of the graph and demonstrating competitive performance on node
classification, spatio-temporal graph analysis, and knowledge graph completion,
while having memory and computational complexity linear in the number of nodes
rather than edges.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [244] [Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense Retrieval](https://arxiv.org/abs/2504.17884)
*Yongkang Li,Panagiotis Eustratiadis,Simon Lupart,Evangelos Kanoulas*

Main category: cs.IR

TLDR: 本文提出一种针对密集信息检索的语料库投毒攻击方法，直接在嵌入空间优化，无需查询先验知识，速度快且难以检测。


<details>
  <summary>Details</summary>
Motivation: 解决现有攻击方法在离散词空间操作与连续嵌入空间检索不匹配的问题，并挑战无查询先验知识的更困难场景。

Method: 训练扰动模型，保持原始与对抗文档嵌入的几何距离，同时最大化词级差异；支持白盒和黑盒攻击。

Result: 攻击速度快（每目标文档2分钟内），生成文本自然（低困惑度），在多种数据集上有效。

Conclusion: 提出了一种高效、隐蔽的语料库投毒攻击方法，为信息检索安全提供了新视角。

Abstract: This paper concerns corpus poisoning attacks in dense information retrieval,
where an adversary attempts to compromise the ranking performance of a search
algorithm by injecting a small number of maliciously generated documents into
the corpus. Our work addresses two limitations in the current literature.
First, attacks that perform adversarial gradient-based word substitution search
do so in the discrete lexical space, while retrieval itself happens in the
continuous embedding space. We thus propose an optimization method that
operates in the embedding space directly. Specifically, we train a perturbation
model with the objective of maintaining the geometric distance between the
original and adversarial document embeddings, while also maximizing the
token-level dissimilarity between the original and adversarial documents.
Second, it is common for related work to have a strong assumption that the
adversary has prior knowledge about the queries. In this paper, we focus on a
more challenging variant of the problem where the adversary assumes no prior
knowledge about the query distribution (hence, unsupervised). Our core
contribution is an adversarial corpus attack that is fast and effective. We
present comprehensive experimental results on both in- and out-of-domain
datasets, focusing on two related tasks: a top-1 attack and a corpus poisoning
attack. We consider attacks under both a white-box and a black-box setting.
Notably, our method can generate successful adversarial examples in under two
minutes per target document; four times faster compared to the fastest
gradient-based word substitution methods in the literature with the same
hardware. Furthermore, our adversarial generation method generates text that is
more likely to occur under the distribution of natural text (low perplexity),
and is therefore more difficult to detect.

</details>

### [245] [OmniSage: Large Scale, Multi-Entity Heterogeneous Graph Representation Learning](https://arxiv.org/abs/2504.17811)
*Anirudhan Badrinath,Alex Yang,Kousik Rajesh,Prabhat Agarwal,Jaewon Yang,Haoyu Chen,Jiajing Xu,Charles Rosenberg*

Main category: cs.IR

TLDR: OmniSage是一个统一表示学习框架，整合了图神经网络、内容模型和用户序列模型，显著提升了Pinterest的用户体验。


<details>
  <summary>Details</summary>
Motivation: 当前表示学习方法多样但缺乏统一框架，OmniSage旨在整合多种技术以支持多应用场景。

Method: 结合图神经网络、内容模型和用户序列模型，采用对比学习任务处理图数据、用户序列和内容信号。

Result: OmniSage显著提升了Pinterest的用户体验，全站“保存”操作增加了约2.5%。

Conclusion: 统一表示学习方法具有显著效果，OmniSage代码将开源。

Abstract: Representation learning, a task of learning latent vectors to represent
entities, is a key task in improving search and recommender systems in web
applications. Various representation learning methods have been developed,
including graph-based approaches for relationships among entities,
sequence-based methods for capturing the temporal evolution of user activities,
and content-based models for leveraging text and visual content. However, the
development of a unifying framework that integrates these diverse techniques to
support multiple applications remains a significant challenge. This paper
presents OmniSage, a large-scale representation framework that learns universal
representations for a variety of applications at Pinterest. OmniSage integrates
graph neural networks with content-based models and user sequence models by
employing multiple contrastive learning tasks to effectively process graph
data, user sequence data, and content signals. To support the training and
inference of OmniSage, we developed an efficient infrastructure capable of
supporting Pinterest graphs with billions of nodes. The universal
representations generated by OmniSage have significantly enhanced user
experiences on Pinterest, leading to an approximate 2.5% increase in sitewide
repins (saves) across five applications. This paper highlights the impact of
unifying representation learning methods, and we will open source the OmniSage
code by the time of publication.

</details>

### [246] [Bridge the Domains: Large Language Models Enhanced Cross-domain Sequential Recommendation](https://arxiv.org/abs/2504.18383)
*Qidong Liu,Xiangyu Zhao,Yejing Wang,Zijian Zhang,Howard Zhong,Chong Chen,Xiang Li,Wei Huang,Feng Tian*

Main category: cs.IR

TLDR: 论文提出LLM4CDSR模型，利用大语言模型（LLMs）解决跨域序列推荐中的重叠困境和转移复杂性，通过语义视角捕捉用户偏好。


<details>
  <summary>Details</summary>
Motivation: 现有跨域序列推荐方法依赖用户在所有域中的交互数据，且难以学习复杂转移模式，限制了实用性。

Method: 提出基于LLM的统一表示模块和可训练适配器，结合对比正则化，设计分层LLMs分析模块，整合为三线程框架。

Result: 在三个公开跨域数据集上验证了LLM4CDSR的有效性。

Conclusion: LLM4CDSR通过语义视角解决了跨域推荐中的关键问题，提升了推荐性能。

Abstract: Cross-domain Sequential Recommendation (CDSR) aims to extract the preference
from the user's historical interactions across various domains. Despite some
progress in CDSR, two problems set the barrier for further advancements, i.e.,
overlap dilemma and transition complexity. The former means existing CDSR
methods severely rely on users who own interactions on all domains to learn
cross-domain item relationships, compromising the practicability. The latter
refers to the difficulties in learning the complex transition patterns from the
mixed behavior sequences. With powerful representation and reasoning abilities,
Large Language Models (LLMs) are promising to address these two problems by
bridging the items and capturing the user's preferences from a semantic view.
Therefore, we propose an LLMs Enhanced Cross-domain Sequential Recommendation
model (LLM4CDSR). To obtain the semantic item relationships, we first propose
an LLM-based unified representation module to represent items. Then, a
trainable adapter with contrastive regularization is designed to adapt the CDSR
task. Besides, a hierarchical LLMs profiling module is designed to summarize
user cross-domain preferences. Finally, these two modules are integrated into
the proposed tri-thread framework to derive recommendations. We have conducted
extensive experiments on three public cross-domain datasets, validating the
effectiveness of LLM4CDSR. We have released the code online.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [247] [The Cloud Weaving Model for AI development](https://arxiv.org/abs/2504.17823)
*Darcy Kim,Aida Kalender,Sennay Ghebreab,Giovanni Sileno*

Main category: cs.CY

TLDR: 论文提出了一种名为“云编织模型”的替代性概念框架，用于在AI开发中融入社会背景，并应用于边缘化社区的共创试点项目。


<details>
  <summary>Details</summary>
Motivation: 现有范式难以表达边缘化社区AI开发中的挑战，因此需要一种新的框架来更好地理解和应对这些挑战。

Method: 通过借鉴土著知识、自然元素和东方传统，构建了“云编织模型”，并详细阐述了其核心元素及其在AI中的解释。

Result: 该框架成功应用于共创试点项目，揭示了负责任AI开发中被忽视但重要的维度。

Conclusion: “云编织模型”为AI开发提供了一种新的视角，强调了社会背景的重要性，并为边缘化社区的参与提供了更全面的理解。

Abstract: While analysing challenges in pilot projects developing AI with marginalized
communities, we found it difficult to express them within commonly used
paradigms. We therefore constructed an alternative conceptual framework to
ground AI development in the social fabric -- the Cloud Weaving Model --
inspired (amongst others) by indigenous knowledge, motifs from nature, and
Eastern traditions. This paper introduces and elaborates on the fundamental
elements of the model (clouds, spiders, threads, spiderwebs, and weather) and
their interpretation in an AI context. The framework is then applied to
comprehend patterns observed in co-creation pilots approaching marginalized
communities, highlighting neglected yet relevant dimensions for responsible AI
development.

</details>

### [248] [The Role of Open-Source LLMs in Shaping the Future of GeoAI](https://arxiv.org/abs/2504.17833)
*Xiao Huang,Zhengzhong Tu,Xinyue Ye,Michael Goodchild*

Main category: cs.CY

TLDR: 论文探讨了开源大语言模型（LLMs）在推动地理空间人工智能（GeoAI）发展中的关键作用，强调其相较于专有模型的优势，如可定制性和透明度，但也指出需注意安全与伦理问题。


<details>
  <summary>Details</summary>
Motivation: 专有LLMs在地理空间任务中存在局限性，而开源LLMs能促进GIScience的创新和适应性，因此需要研究其机遇与挑战。

Method: 通过对比开源与专有LLMs的优缺点，结合FAIR原则和实际案例，分析开源LLMs在GeoAI中的应用潜力。

Result: 开源LLMs显著提升了GIScience的可适应性、可重复性和社区驱动创新，但需解决安全、伦理和治理问题。

Conclusion: GIScience的最佳发展路径是构建多样化的开源生态系统，结合定制化模型和跨学科合作，以实现科学严谨、公平可持续的空间研究和决策支持。

Abstract: Large Language Models (LLMs) are transforming geospatial artificial
intelligence (GeoAI), offering new capabilities in data processing, spatial
analysis, and decision support. This paper examines the open-source paradigm's
pivotal role in this transformation. While proprietary LLMs offer
accessibility, they often limit the customization, interoperability, and
transparency vital for specialized geospatial tasks. Conversely, open-source
alternatives significantly advance Geographic Information Science (GIScience)
by fostering greater adaptability, reproducibility, and community-driven
innovation. Open frameworks empower researchers to tailor solutions, integrate
cutting-edge methodologies (e.g., reinforcement learning, advanced spatial
indexing), and align with FAIR principles. However, the growing reliance on any
LLM necessitates careful consideration of security vulnerabilities, ethical
risks, and robust governance for AI-generated geospatial outputs. Ongoing
debates on accessibility, regulation, and misuse underscore the critical need
for responsible AI development strategies. This paper argues that GIScience
advances best not through a single model type, but by cultivating a diverse,
interoperable ecosystem combining open-source foundations for innovation,
bespoke geospatial models, and interdisciplinary collaboration. By critically
evaluating the opportunities and challenges of open-source LLMs within the
broader GeoAI landscape, this work contributes to a nuanced discourse on
leveraging AI to effectively advance spatial research, policy, and
decision-making in an equitable, sustainable, and scientifically rigorous
manner.

</details>

### [249] [AI Ethics and Social Norms: Exploring ChatGPT's Capabilities From What to How](https://arxiv.org/abs/2504.18044)
*Omid Veisi,Sasan Bahrami,Roman Englert,Claudia Müller*

Main category: cs.CY

TLDR: 研究探讨了ChatGPT作为日常工具时的伦理和社会规范问题，通过混合方法（调查和专家访谈）揭示了其在六个伦理方面的表现及挑战。


<details>
  <summary>Details</summary>
Motivation: 评估ChatGPT在实证环境中是否符合伦理和社会规范，以指导工业与学术研究中的行动，并推动机器伦理的实现。

Method: 采用混合方法研究，包括111人的在线调查和38位专家的访谈。

Result: 揭示了ChatGPT在偏见、可信度、安全性、毒性、社会规范和伦理数据六个方面的表现，并指出透明度和无监督数据收集中的偏见是主要伦理问题。

Conclusion: 研究为AI伦理提供了初步见解，强调了透明度和偏见问题的重要性，为未来研究提供了方向。

Abstract: Using LLMs in healthcare, Computer-Supported Cooperative Work, and Social
Computing requires the examination of ethical and social norms to ensure safe
incorporation into human life. We conducted a mixed-method study, including an
online survey with 111 participants and an interview study with 38 experts, to
investigate the AI ethics and social norms in ChatGPT as everyday life tools.
This study aims to evaluate whether ChatGPT in an empirical context operates
following ethics and social norms, which is critical for understanding actions
in industrial and academic research and achieving machine ethics. The findings
of this study provide initial insights into six important aspects of AI ethics,
including bias, trustworthiness, security, toxicology, social norms, and
ethical data. Significant obstacles related to transparency and bias in
unsupervised data collection methods are identified as ChatGPT's ethical
concerns.

</details>

<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [250] [A computational model of infant sensorimotor exploration in the mobile paradigm](https://arxiv.org/abs/2504.17939)
*Josua Spisak,Sergiu Tcaci Popescu,Stefan Wermter,Matej Hoffmann,J. Kevin O'Regan*

Main category: q-bio.NC

TLDR: 本文提出了一种计算模型，模拟婴儿在“移动范式”中的行为机制，探讨其如何学习动作与感官效果的联系。模型结合神经网络、动作-结果预测等机制，成功复现了婴儿偏好移动连接肢体的行为，并揭示了断开连接后的突发动作现象。


<details>
  <summary>Details</summary>
Motivation: 研究婴儿如何学习动作与感官效果的联系（即“感觉运动偶联”），这是发展心理学中重要的认知能力基础。

Method: 构建了一个包含神经网络、动作-结果预测、探索行为、运动噪声和生物启发的运动控制的计算模型。

Result: 模型成功复现了婴儿偏好移动连接肢体的行为，并揭示了断开连接后的突发动作现象。此外，模型还复现了两种不同连接方式（渐进式或全或无）的实验数据。

Conclusion: 动作-结果预测、探索行为、运动噪声和生物启发的运动控制是婴儿感觉运动学习的关键组成部分。

Abstract: We present a computational model of the mechanisms that may determine
infants' behavior in the "mobile paradigm". This paradigm has been used in
developmental psychology to explore how infants learn the sensory effects of
their actions. In this paradigm, a mobile (an articulated and movable object
hanging above an infant's crib) is connected to one of the infant's limbs,
prompting the infant to preferentially move that "connected" limb. This ability
to detect a "sensorimotor contingency" is considered to be a foundational
cognitive ability in development. To understand how infants learn sensorimotor
contingencies, we built a model that attempts to replicate infant behavior. Our
model incorporates a neural network, action-outcome prediction, exploration,
motor noise, preferred activity level, and biologically-inspired motor control.
We find that simulations with our model replicate the classic findings in the
literature showing preferential movement of the connected limb. An interesting
observation is that the model sometimes exhibits a burst of movement after the
mobile is disconnected, casting light on a similar occasional finding in
infants. In addition to these general findings, the simulations also replicate
data from two recent more detailed studies using a connection with the mobile
that was either gradual or all-or-none. A series of ablation studies further
shows that the inclusion of mechanisms of action-outcome prediction,
exploration, motor noise, and biologically-inspired motor control was essential
for the model to correctly replicate infant behavior. This suggests that these
components are also involved in infants' sensorimotor learning.

</details>

<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [251] [Artificial Intelligence health advice accuracy varies across languages and contexts](https://arxiv.org/abs/2504.18310)
*Prashant Garg,Thiemo Fetzer*

Main category: econ.GN

TLDR: 论文评估了六种大型语言模型在21种语言中的表现，发现尽管在英语教科书类声明上准确性高，但在非欧洲语言和不同主题及来源上表现波动，强调了全球健康传播中多语言和领域感知验证的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估大型语言模型在全球健康传播中的表现，特别是在多语言和多样化主题背景下的准确性。

Method: 方法包括使用来自英国和欧盟注册的基本健康声明，以及9,100条经过记者审查的公共卫生声明，涵盖堕胎、COVID-19和政治等主题，来源包括同行评审期刊、政府建议、社交媒体和跨政治光谱的新闻。

Result: 结果显示，模型在英语教科书类声明上表现良好，但在非欧洲语言和不同主题及来源上准确性波动。

Conclusion: 结论强调了在全球健康传播中部署AI前，需进行全面的多语言和领域感知验证的紧迫性。

Abstract: Using basic health statements authorized by UK and EU registers and 9,100
journalist-vetted public-health assertions on topics such as abortion, COVID-19
and politics from sources ranging from peer-reviewed journals and government
advisories to social media and news across the political spectrum, we benchmark
six leading large language models from in 21 languages, finding that, despite
high accuracy on English-centric textbook claims, performance falls in multiple
non-European languages and fluctuates by topic and source, highlighting the
urgency of comprehensive multilingual, domain-aware validation before deploying
AI in global health communication.

</details>