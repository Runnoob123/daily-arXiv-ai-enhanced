<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 15]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.CL](#cs.CL) [Total: 34]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.CV](#cs.CV) [Total: 89]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 5]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [econ.TH](#econ.TH) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [stat.ML](#stat.ML) [Total: 10]
- [quant-ph](#quant-ph) [Total: 2]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.DC](#cs.DC) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.CY](#cs.CY) [Total: 7]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.NI](#cs.NI) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 12]
- [math.NA](#math.NA) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [math.AP](#math.AP) [Total: 1]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.NE](#cs.NE) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 1]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [SafeTab-H: Disclosure Avoidance for the 2020 Census Detailed Demographic and Housing Characteristics File B (Detailed DHC-B)](https://arxiv.org/abs/2505.03072)
*William Sexton,Skye Berghel,Bayard Carlson,Sam Haney,Luke Hartman,Michael Hay,Ashwin Machanavajjhala,Gerome Miklau,Amritha Pai,Simran Rajpal,David Pujol,Ruchit Shrestha,Daniel Simmons-Marengo*

Main category: cs.CR

TLDR: SafeTab-H是一种用于美国人口普查局数据的隐私保护算法，基于离散高斯噪声和零集中差分隐私。


<details>
  <summary>Details</summary>
Motivation: 保护2020年人口普查中详细家庭统计数据的隐私，防止信息泄露。

Method: 通过离散高斯分布添加噪声，并利用Tumult Analytics隐私库实现算法。

Result: 算法满足零集中差分隐私，并分析了理论误差和参数调优。

Conclusion: SafeTab-H是一种有效的隐私保护方法，适用于详细人口统计数据发布。

Abstract: This article describes SafeTab-H, a disclosure avoidance algorithm applied to
the release of the U.S. Census Bureau's Detailed Demographic and Housing
Characteristics File B (Detailed DHC-B) as part of the 2020 Census. The
tabulations contain household statistics about household type and tenure
iterated by the householder's detailed race, ethnicity, or American Indian and
Alaska Native tribe and village at varying levels of geography. We describe the
algorithmic strategy which is based on adding noise from a discrete Gaussian
distribution and show that the algorithm satisfies a well-studied variant of
differential privacy, called zero-concentrated differential privacy. We discuss
how the implementation of the SafeTab-H codebase relies on the Tumult Analytics
privacy library. We also describe the theoretical expected error properties of
the algorithm and explore various aspects of its parameter tuning.

</details>

### [2] [Towards a standardized methodology and dataset for evaluating LLM-based digital forensic timeline analysis](https://arxiv.org/abs/2505.03100)
*Hudan Studiawan,Frank Breitinger,Mark Scanlon*

Main category: cs.CR

TLDR: 本文提出了一种标准化方法，用于定量评估大语言模型（LLMs）在数字取证任务（如时间线分析）中的表现，并推荐使用BLEU和ROUGE指标进行评测。


<details>
  <summary>Details</summary>
Motivation: 当前研究缺乏对LLMs在数字取证中性能的标准化评估方法，本文旨在填补这一空白。

Method: 受NIST计算机取证工具测试项目启发，提出了一种标准化评估方法，包括数据集、时间线生成和真实数据开发。

Result: 实验结果表明，该方法能有效评估基于LLM的取证时间线分析，以ChatGPT为例进行了验证。

Conclusion: 本文方法为LLMs在数字取证中的应用提供了标准化评估框架，但也指出了其在时间线分析中的局限性。

Abstract: Large language models (LLMs) have seen widespread adoption in many domains
including digital forensics. While prior research has largely centered on case
studies and examples demonstrating how LLMs can assist forensic investigations,
deeper explorations remain limited, i.e., a standardized approach for precise
performance evaluations is lacking. Inspired by the NIST Computer Forensic Tool
Testing Program, this paper proposes a standardized methodology to
quantitatively evaluate the application of LLMs for digital forensic tasks,
specifically in timeline analysis. The paper describes the components of the
methodology, including the dataset, timeline generation, and ground truth
development. Additionally, the paper recommends using BLEU and ROUGE metrics
for the quantitative evaluation of LLMs through case studies or tasks involving
timeline analysis. Experimental results using ChatGPT demonstrate that the
proposed methodology can effectively evaluate LLM-based forensic timeline
analysis. Finally, we discuss the limitations of applying LLMs to forensic
timeline analysis.

</details>

### [3] [Adversarial Sample Generation for Anomaly Detection in Industrial Control Systems](https://arxiv.org/abs/2505.03120)
*Abdul Mustafa,Muhammad Talha Khan,Muhammad Azmi Umer,Zaki Masood,Chuadhry Mujeeb Ahmed*

Main category: cs.CR

TLDR: 论文研究了基于机器学习的入侵检测系统（IDS）对抗性攻击的脆弱性，并提出使用Jacobian Saliency Map Attack（JSMA）生成对抗样本以提高IDS的识别能力。


<details>
  <summary>Details</summary>
Motivation: 由于IDS易受对抗性攻击，研究旨在通过生成对抗样本来增强其识别恶意攻击的能力。

Method: 使用JSMA生成对抗样本，并在工业控制系统（ICS）中验证其泛化性和扩展性。

Result: 在未用于训练的真实攻击数据上，模型检测准确率达到95%。

Conclusion: 研究表明，对抗样本训练能有效提升IDS对真实攻击的检测能力。

Abstract: Machine learning (ML)-based intrusion detection systems (IDS) are vulnerable
to adversarial attacks. It is crucial for an IDS to learn to recognize
adversarial examples before malicious entities exploit them. In this paper, we
generated adversarial samples using the Jacobian Saliency Map Attack (JSMA). We
validate the generalization and scalability of the adversarial samples to
tackle a broad range of real attacks on Industrial Control Systems (ICS). We
evaluated the impact by assessing multiple attacks generated using the proposed
method. The model trained with adversarial samples detected attacks with 95%
accuracy on real-world attack data not used during training. The study was
conducted using an operational secure water treatment (SWaT) testbed.

</details>

### [4] [Towards Effective Identification of Attack Techniques in Cyber Threat Intelligence Reports using Large Language Models](https://arxiv.org/abs/2505.03147)
*Hoang Cuong Nguyen,Shahroz Tariq,Mohan Baruwal Chhetri,Bao Quoc Vo*

Main category: cs.CR

TLDR: 论文评估了基于MITRE ATT&CK框架的网络威胁情报（CTI）提取方法，提出了一种结合LLM和SciBERT的两步流程，显著提升了F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决现有CTI提取方法在识别攻击技术时面临的类别不平衡、过拟合和领域复杂性等问题。

Method: 使用TRAM和开源LLM（如Llama2）分析威胁报告，提出两步流程：LLM生成摘要，SciBERT处理增强数据集。

Result: F1分数显著提升，部分攻击技术的F1分数超过0.90。

Conclusion: 该方法提升了CTI系统的效率，支持协作网络安全，为未来人机协作研究铺路。

Abstract: This work evaluates the performance of Cyber Threat Intelligence (CTI)
extraction methods in identifying attack techniques from threat reports
available on the web using the MITRE ATT&CK framework. We analyse four
configurations utilising state-of-the-art tools, including the Threat Report
ATT&CK Mapper (TRAM) and open-source Large Language Models (LLMs) such as
Llama2. Our findings reveal significant challenges, including class imbalance,
overfitting, and domain-specific complexity, which impede accurate technique
extraction. To mitigate these issues, we propose a novel two-step pipeline:
first, an LLM summarises the reports, and second, a retrained SciBERT model
processes a rebalanced dataset augmented with LLM-generated data. This approach
achieves an improvement in F1-scores compared to baseline models, with several
attack techniques surpassing an F1-score of 0.90. Our contributions enhance the
efficiency of web-based CTI systems and support collaborative cybersecurity
operations in an interconnected digital landscape, paving the way for future
research on integrating human-AI collaboration platforms.

</details>

### [5] [An LLM-based Self-Evolving Security Framework for 6G Space-Air-Ground Integrated Networks](https://arxiv.org/abs/2505.03161)
*Qi Qin,Xinye Cao,Guoshun Nan,Sihan Chen,Rushan Li,Li Su,Haitao Du,Qimei Cui,Pengxuan Mao,Xiaofeng Tao,Tony Q. S. Quek*

Main category: cs.CR

TLDR: 论文提出了一种基于大型语言模型（LLMs）的6G空天地一体化网络（SAGINs）安全框架，通过LLM-6GNG和6G-INST解决威胁分析和动态适应问题。


<details>
  <summary>Details</summary>
Motivation: 6G SAGINs的动态、开放和异构特性导致严重安全问题，需解决威胁信息分析和未知威胁适应两大挑战。

Method: 提出LLM-6GNG（基于链式思维推理和多智能体机制分析威胁）和6G-INST（自进化方法动态更新模型）。

Result: 实验证明框架能生成高精度安全策略，对未知攻击保持鲁棒性。

Conclusion: 该框架有效解决了6G SAGINs的安全挑战，代码将开源。

Abstract: Recently emerged 6G space-air-ground integrated networks (SAGINs), which
integrate satellites, aerial networks, and terrestrial communications, offer
ubiquitous coverage for various mobile applications. However, the highly
dynamic, open, and heterogeneous nature of SAGINs poses severe security issues.
Forming a defense line of SAGINs suffers from two preliminary challenges: 1)
accurately understanding massive unstructured multi-dimensional threat
information to generate defense strategies against various malicious attacks,
2) rapidly adapting to potential unknown threats to yield more effective
security strategies. To tackle the above two challenges, we propose a novel
security framework for SAGINs based on Large Language Models (LLMs), which
consists of two key ingredients LLM-6GNG and 6G-INST. Our proposed LLM-6GNG
leverages refined chain-of-thought (CoT) reasoning and dynamic multi-agent
mechanisms to analyze massive unstructured multi-dimensional threat data and
generate comprehensive security strategies, thus addressing the first
challenge. Our proposed 6G-INST relies on a novel self-evolving method to
automatically update LLM-6GNG, enabling it to accommodate unknown threats under
dynamic communication environments, thereby addressing the second challenge.
Additionally, we prototype the proposed framework with ns-3, OpenAirInterface
(OAI), and software-defined radio (SDR). Experiments on three benchmarks
demonstrate the effectiveness of our framework. The results show that our
framework produces highly accurate security strategies that remain robust
against a variety of unknown attacks. We will release our code to contribute to
the community.

</details>

### [6] [Bridging Expertise Gaps: The Role of LLMs in Human-AI Collaboration for Cybersecurity](https://arxiv.org/abs/2505.03179)
*Shahroz Tariq,Ronal Singh,Mohan Baruwal Chhetri,Surya Nepal,Cecile Paris*

Main category: cs.CR

TLDR: 研究探讨大型语言模型（LLMs）是否能作为智能协作工具，弥补网络安全决策中的专业知识差距。通过用户实验发现，人机协作能提升任务表现，并支持长期技能发展。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在网络安全决策中作为协作工具的潜力，帮助非专家解决复杂问题。

Method: 采用混合方法用户研究，测试LLMs在钓鱼邮件检测和入侵检测任务中的表现，分析交互动态对用户信任和决策的影响。

Result: 人机协作减少了钓鱼检测的误报和入侵检测的漏报，且用户在独立工作时表现出学习效应。交互动态（如LLM的确定性、解释风格和语气）影响用户信任和决策。

Conclusion: LLMs能有效支持非专家解决复杂网络安全问题，研究结果为设计可解释、自适应且可信的人机协作系统提供了指导。

Abstract: This study investigates whether large language models (LLMs) can function as
intelligent collaborators to bridge expertise gaps in cybersecurity
decision-making. We examine two representative tasks-phishing email detection
and intrusion detection-that differ in data modality, cognitive complexity, and
user familiarity. Through a controlled mixed-methods user study, n = 58
(phishing, n = 34; intrusion, n = 24), we find that human-AI collaboration
improves task performance,reducing false positives in phishing detection and
false negatives in intrusion detection. A learning effect is also observed when
participants transition from collaboration to independent work, suggesting that
LLMs can support long-term skill development. Our qualitative analysis shows
that interaction dynamics-such as LLM definitiveness, explanation style, and
tone-influence user trust, prompting strategies, and decision revision. Users
engaged in more analytic questioning and showed greater reliance on LLM
feedback in high-complexity settings. These results provide design guidance for
building interpretable, adaptive, and trustworthy human-AI teaming systems, and
demonstrate that LLMs can meaningfully support non-experts in reasoning through
complex cybersecurity problems.

</details>

### [7] [A Chaos Driven Metric for Backdoor Attack Detection](https://arxiv.org/abs/2505.03208)
*Hema Karnam Surendrababu,Nithin Nagaraj*

Main category: cs.CR

TLDR: 论文提出了一种结合混沌理论和流形学习的新型防御机制，用于对抗AI模型的后门攻击，并提出了一种新的度量标准PDS来区分中毒样本。


<details>
  <summary>Details</summary>
Motivation: AI模型的广泛应用带来了对抗性攻击的挑战，尤其是后门攻击，需要有效的防御方法。

Method: 采用混沌理论和流形学习的集成方法，提出PDS度量标准，基于神经混沌特征的条件方差。

Result: PDS成功区分了多种数据集中的中毒样本与非中毒样本。

Conclusion: 该防御机制为AI模型的安全性提供了新的解决方案。

Abstract: The advancement and adoption of Artificial Intelligence (AI) models across
diverse domains have transformed the way we interact with technology. However,
it is essential to recognize that while AI models have introduced remarkable
advancements, they also present inherent challenges such as their vulnerability
to adversarial attacks. The current work proposes a novel defense mechanism
against one of the most significant attack vectors of AI models - the backdoor
attack via data poisoning of training datasets. In this defense technique, an
integrated approach that combines chaos theory with manifold learning is
proposed. A novel metric - Precision Matrix Dependency Score (PDS) that is
based on the conditional variance of Neurochaos features is formulated. The PDS
metric has been successfully evaluated to distinguish poisoned samples from
non-poisoned samples across diverse datasets.

</details>

### [8] [Elevating Cyber Threat Intelligence against Disinformation Campaigns with LLM-based Concept Extraction and the FakeCTI Dataset](https://arxiv.org/abs/2505.03345)
*Domenico Cotroneo,Roberto Natella,Vittorio Orbinato*

Main category: cs.CR

TLDR: 论文提出了一种基于高级语义指标的CTI框架，利用LLM从虚假新闻中提取结构化CTI指标，并引入FakeCTI数据集，验证了其在追踪虚假信息活动中的有效性。


<details>
  <summary>Details</summary>
Motivation: 虚假新闻和虚假信息活动的快速传播对公共信任、政治稳定和网络安全构成重大威胁，传统CTI方法因依赖低级指标而易被规避。

Method: 提出了一种新型CTI框架，通过LLM从非结构化虚假信息内容中提取高级语义指标，并创建FakeCTI数据集。

Result: 验证了该框架在虚假新闻溯源中的有效性，从传统NLP到微调LLM的多技术对比分析。

Conclusion: 该研究将焦点从低级指标转向持久概念结构，为追踪和对抗虚假信息活动提供了可扩展和自适应的解决方案。

Abstract: The swift spread of fake news and disinformation campaigns poses a
significant threat to public trust, political stability, and cybersecurity.
Traditional Cyber Threat Intelligence (CTI) approaches, which rely on low-level
indicators such as domain names and social media handles, are easily evaded by
adversaries who frequently modify their online infrastructure. To address these
limitations, we introduce a novel CTI framework that focuses on high-level,
semantic indicators derived from recurrent narratives and relationships of
disinformation campaigns. Our approach extracts structured CTI indicators from
unstructured disinformation content, capturing key entities and their
contextual dependencies within fake news using Large Language Models (LLMs). We
further introduce FakeCTI, the first dataset that systematically links fake
news to disinformation campaigns and threat actors. To evaluate the
effectiveness of our CTI framework, we analyze multiple fake news attribution
techniques, spanning from traditional Natural Language Processing (NLP) to
fine-tuned LLMs. This work shifts the focus from low-level artifacts to
persistent conceptual structures, establishing a scalable and adaptive approach
to tracking and countering disinformation campaigns.

</details>

### [9] [Directed Greybox Fuzzing via Large Language Model](https://arxiv.org/abs/2505.03425)
*Hanxiang Xu,Yanjie Zhao,Haoyu Wang*

Main category: cs.CR

TLDR: HGFuzzer是一个利用大型语言模型（LLM）的定向灰盒模糊测试框架，通过将路径约束问题转化为代码生成任务，显著减少不必要的探索路径，并提高定向模糊测试的精确性。


<details>
  <summary>Details</summary>
Motivation: 现有定向灰盒模糊测试方法存在路径爆炸和输入突变随机性问题，导致目标路径探索效率低下。

Method: HGFuzzer将路径约束问题转化为代码生成任务，生成测试工具和可达输入，并针对目标函数设计自定义变异器。

Result: 在20个真实漏洞测试中，成功触发17个，其中11个在1分钟内触发，速度提升至少24.8倍，并发现9个新漏洞。

Conclusion: HGFuzzer在识别真实漏洞方面表现出高效性和有效性。

Abstract: Directed greybox fuzzing (DGF) focuses on efficiently reaching specific
program locations or triggering particular behaviors, making it essential for
tasks like vulnerability detection and crash reproduction. However, existing
methods often suffer from path explosion and randomness in input mutation,
leading to inefficiencies in exploring and exploiting target paths. In this
paper, we propose HGFuzzer, an automatic framework that leverages the large
language model (LLM) to address these challenges. HGFuzzer transforms path
constraint problems into targeted code generation tasks, systematically
generating test harnesses and reachable inputs to reduce unnecessary
exploration paths significantly. Additionally, we implement custom mutators
designed specifically for target functions, minimizing randomness and improving
the precision of directed fuzzing. We evaluated HGFuzzer on 20 real-world
vulnerabilities, successfully triggering 17, including 11 within the first
minute, achieving a speedup of at least 24.8x compared to state-of-the-art
directed fuzzers. Furthermore, HGFuzzer discovered 9 previously unknown
vulnerabilities, all of which were assigned CVE IDs, demonstrating the
effectiveness of our approach in identifying real-world vulnerabilities.

</details>

### [10] [Detecting Quishing Attacks with Machine Learning Techniques Through QR Code Analysis](https://arxiv.org/abs/2505.03451)
*Fouad Trad,Ali Chehab*

Main category: cs.CR

TLDR: 论文提出了一种直接分析QR码结构和像素模式的框架，用于检测QR码钓鱼攻击，避免了提取嵌入内容的风险，并通过机器学习模型验证了方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统QR码钓鱼检测方法依赖URL分析，存在暴露用户于恶意内容的风险，且无法覆盖QR码编码的其他数据类型。

Method: 生成钓鱼和良性QR码数据集，训练多种机器学习模型（如XGBoost），分析QR码结构和像素模式。

Result: 最佳模型（XGBoost）AUC达0.9106，通过特征优化提升至0.9133，发现QR码结构特征与钓鱼风险强相关。

Conclusion: 直接QR码分析可作为现代钓鱼防御的关键层，为QR码钓鱼缓解奠定基础。

Abstract: The rise of QR code based phishing ("Quishing") poses a growing cybersecurity
threat, as attackers increasingly exploit QR codes to bypass traditional
phishing defenses. Existing detection methods predominantly focus on URL
analysis, which requires the extraction of the QR code payload, and may
inadvertently expose users to malicious content. Moreover, QR codes can encode
various types of data beyond URLs, such as Wi-Fi credentials and payment
information, making URL-based detection insufficient for broader security
concerns. To address these gaps, we propose the first framework for quishing
detection that directly analyzes QR code structure and pixel patterns without
extracting the embedded content. We generated a dataset of phishing and benign
QR codes and we used it to train and evaluate multiple machine learning models,
including Logistic Regression, Decision Trees, Random Forest, Naive Bayes,
LightGBM, and XGBoost. Our best-performing model (XGBoost) achieves an AUC of
0.9106, demonstrating the feasibility of QR-centric detection. Through feature
importance analysis, we identify key visual indicators of malicious intent and
refine our feature set by removing non-informative pixels, improving
performance to an AUC of 0.9133 with a reduced feature space. Our findings
reveal that the structural features of QR code correlate strongly with phishing
risk. This work establishes a foundation for quishing mitigation and highlights
the potential of direct QR analysis as a critical layer in modern phishing
defenses.

</details>

### [11] [Mitigating Backdoor Triggered and Targeted Data Poisoning Attacks in Voice Authentication Systems](https://arxiv.org/abs/2505.03455)
*Alireza Mohammadi,Keshav Sood,Dhananjay Thiruvady,Asef Nazari*

Main category: cs.CR

TLDR: 提出了一种统一防御框架，同时应对语音认证系统中的后门触发攻击和定向数据中毒攻击，通过频率检测和卷积神经网络实现高效防御。


<details>
  <summary>Details</summary>
Motivation: 语音认证系统易受后门触发攻击和定向数据中毒攻击的双重威胁，现有解决方案通常单独应对，系统仍面临同时攻击的风险。

Method: 框架包含频率聚焦检测机制（实时检测隐蔽音高提升和声音掩蔽后门攻击）和卷积神经网络（处理定向数据中毒攻击），利用多维声学特征隔离异常信号。

Result: 实验表明，PBSM检测机制优于现有技术，攻击成功率降至5%-15%，同时定向数据中毒攻击的召回率高达95%。

Conclusion: 该框架无需昂贵模型重训练，可无缝集成到现有语音认证系统中，适用于大规模部署。

Abstract: Voice authentication systems remain susceptible to two major threats:
backdoor triggered attacks and targeted data poisoning attacks. This dual
vulnerability is critical because conventional solutions typically address each
threat type separately, leaving systems exposed to adversaries who can exploit
both attacks simultaneously. We propose a unified defense framework that
effectively addresses both BTA and TDPA. Our framework integrates a frequency
focused detection mechanism that flags covert pitch boosting and sound masking
backdoor attacks in near real time, followed by a convolutional neural network
that addresses TDPA. This dual layered defense approach utilizes
multidimensional acoustic features to isolate anomalous signals without
requiring costly model retraining. In particular, our PBSM detection mechanism
can seamlessly integrate into existing voice authentication pipelines and scale
effectively for large scale deployments. Experimental results on benchmark
datasets and their compression with the state of the art algorithm demonstrate
that our PBSM detection mechanism outperforms the state of the art. Our
framework reduces attack success rates to as low as five to fifteen percent
while maintaining a recall rate of up to ninety five percent in recognizing
TDPA.

</details>

### [12] [BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models](https://arxiv.org/abs/2505.03501)
*Zihan Wang,Hongwei Li,Rui Zhang,Wenbo Jiang,Kangjie Chen,Tianwei Zhang,Qingchuan Zhao,Guowen Xu*

Main category: cs.CR

TLDR: 本文提出了一种针对大型语言模型（LLMs）的新型后门攻击——语言后门攻击，其特点是以语言本身作为触发器，诱导模型生成煽动性言论。通过改进方法（BadLingual），攻击实现了任务无关性，并在实验中显著提升了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 揭示多语言LLMs的新漏洞，促进防御研究。

Method: 提出基线攻击（基于翻译的数据投毒）和改进方法（BadLingual，使用PGCG对抗训练）。

Result: 基线攻击在特定任务中ASR达90%，但在任务无关场景中仅37.61%；BadLingual提升37.35%。

Conclusion: 语言后门攻击暴露了LLMs的多语言脆弱性，需加强防御研究。

Abstract: In this paper, we present a new form of backdoor attack against Large
Language Models (LLMs): lingual-backdoor attacks. The key novelty of
lingual-backdoor attacks is that the language itself serves as the trigger to
hijack the infected LLMs to generate inflammatory speech. They enable the
precise targeting of a specific language-speaking group, exacerbating racial
discrimination by malicious entities. We first implement a baseline
lingual-backdoor attack, which is carried out by poisoning a set of training
data for specific downstream tasks through translation into the trigger
language. However, this baseline attack suffers from poor task generalization
and is impractical in real-world settings. To address this challenge, we design
BadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any
downstream tasks within the chat LLMs, regardless of the specific questions of
these tasks. We design a new approach using PPL-constrained Greedy Coordinate
Gradient-based Search (PGCG) based adversarial training to expand the decision
boundary of lingual-backdoor, thereby enhancing the generalization ability of
lingual-backdoor across various tasks. We perform extensive experiments to
validate the effectiveness of our proposed attacks. Specifically, the baseline
attack achieves an ASR of over 90% on the specified tasks. However, its ASR
reaches only 37.61% across six tasks in the task-agnostic scenario. In
contrast, BadLingual brings up to 37.35% improvement over the baseline. Our
study sheds light on a new perspective of vulnerabilities in LLMs with
multilingual capabilities and is expected to promote future research on the
potential defenses to enhance the LLMs' robustness

</details>

### [13] [Empc: Effective Path Prioritization for Symbolic Execution with Path Cover](https://arxiv.org/abs/2505.03555)
*Shuangjie Yao,Dongdong She*

Main category: cs.CR

TLDR: Empc是一种新颖的路径优先级技术，通过最小路径覆盖（MPC）减少符号执行中的路径爆炸问题，显著提升代码覆盖率和安全性检测能力。


<details>
  <summary>Details</summary>
Motivation: 符号执行面临路径爆炸问题，传统启发式方法难以泛化到多样化程序。

Method: 利用最小路径覆盖（MPC）和多MPC策略，优先执行少量路径而非指数级路径。

Result: Empc在代码覆盖率、漏洞发现和内存使用方面显著优于现有方法。

Conclusion: Empc有效解决了路径爆炸问题，提升了符号执行的性能和可扩展性。

Abstract: Symbolic execution is a powerful program analysis technique that can formally
reason the correctness of program behaviors and detect software bugs. It can
systematically explore the execution paths of the tested program. But it
suffers from an inherent limitation: path explosion. Path explosion occurs when
symbolic execution encounters an overwhelming number (exponential to the
program size) of paths that need to be symbolically reasoned. It severely
impacts the scalability and performance of symbolic execution. To tackle this
problem, previous works leverage various heuristics to prioritize paths for
symbolic execution. They rank the exponential number of paths using static
rules or heuristics and explore the paths with the highest rank. However, in
practice, these works often fail to generalize to diverse programs. In this
work, we propose a novel and effective path prioritization technique with path
cover, named Empc. Our key insight is that not all paths need to be
symbolically reasoned. Unlike traditional path prioritization, our approach
leverages a small subset of paths as a minimum path cover (MPC) that can cover
all code regions of the tested programs. To encourage diversity in path
prioritization, we compute multiple MPCs. We then guide the search for symbolic
execution on the small number of paths inside multiple MPCs rather than the
exponential number of paths. We implement our technique Empc based on KLEE. We
conduct a comprehensive evaluation of Empc to investigate its performance in
code coverage, bug findings, and runtime overhead. The evaluation shows that
Empc can cover 19.6% more basic blocks than KLEE's best search strategy and
24.4% more lines compared to the state-of-the-art work cgs. Empc also finds 24
more security violations than KLEE's best search strategy. Meanwhile, Empc can
significantly reduce the memory usage of KLEE by up to 93.5%.

</details>

### [14] [LlamaFirewall: An open source guardrail system for building secure AI agents](https://arxiv.org/abs/2505.03574)
*Sahana Chennabasappa,Cyrus Nikolaidis,Daniel Song,David Molnar,Stephanie Ding,Shengye Wan,Spencer Whitman,Lauren Deason,Nicholas Doucette,Abraham Montilla,Alekhya Gampa,Beto de Paola,Dominik Gabi,James Crnkovich,Jean-Christophe Testud,Kat He,Rashnil Chaturvedi,Wu Zhou,Joshua Saxe*

Main category: cs.CR

TLDR: LlamaFirewall是一个开源的安全防护框架，旨在作为AI代理的最后一层防御，通过PromptGuard 2、Agent Alignment Checks和CodeShield三大防护措施应对安全风险。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）能力的提升，其安全风险增加，现有防护措施不足，亟需实时防护框架。

Method: LlamaFirewall包含PromptGuard 2（通用越狱检测器）、Agent Alignment Checks（思维链审核器）和CodeShield（在线静态分析引擎）三大防护措施。

Result: 框架在防止提示注入、代理错位和不安全代码生成方面表现出色，尤其是PromptGuard 2达到先进水平。

Conclusion: LlamaFirewall为AI代理提供了高效、可定制的安全防护解决方案。

Abstract: Large language models (LLMs) have evolved from simple chatbots into
autonomous agents capable of performing complex tasks such as editing
production code, orchestrating workflows, and taking higher-stakes actions
based on untrusted inputs like webpages and emails. These capabilities
introduce new security risks that existing security measures, such as model
fine-tuning or chatbot-focused guardrails, do not fully address. Given the
higher stakes and the absence of deterministic solutions to mitigate these
risks, there is a critical need for a real-time guardrail monitor to serve as a
final layer of defense, and support system level, use case specific safety
policy definition and enforcement. We introduce LlamaFirewall, an open-source
security focused guardrail framework designed to serve as a final layer of
defense against security risks associated with AI Agents. Our framework
mitigates risks such as prompt injection, agent misalignment, and insecure code
risks through three powerful guardrails: PromptGuard 2, a universal jailbreak
detector that demonstrates clear state of the art performance; Agent Alignment
Checks, a chain-of-thought auditor that inspects agent reasoning for prompt
injection and goal misalignment, which, while still experimental, shows
stronger efficacy at preventing indirect injections in general scenarios than
previously proposed approaches; and CodeShield, an online static analysis
engine that is both fast and extensible, aimed at preventing the generation of
insecure or dangerous code by coding agents. Additionally, we include
easy-to-use customizable scanners that make it possible for any developer who
can write a regular expression or an LLM prompt to quickly update an agent's
security guardrails.

</details>

### [15] [Differential Privacy for Network Assortativity](https://arxiv.org/abs/2505.03639)
*Fei Ma,Jinzhi Ouyang,Xincheng Hu*

Main category: cs.CR

TLDR: 论文提出三种基于差分隐私的算法（Local_ru、Shuffle_ru、Decentral_ru）来保护网络同配性计算中的隐私，理论证明其无偏估计性，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 网络同配性分析可能泄露敏感信息（如社交网络中个体的朋友数量），亟需隐私保护方案，但目前尚无相关研究。

Method: 设计了三种基于差分隐私的算法：Local_ru（基于LDP）、Shuffle_ru（基于Shuffle DP）和Decentral_ru（基于DDP），分别适用于不同信息可见性场景。

Result: 理论证明算法能无偏估计网络同配性系数；实验表明Shuffle_ru性能最佳，Decentral_ru次之，Local_ru最差。

Conclusion: 提出的算法在隐私保护需求下能有效估计网络同配性，各有适用场景。

Abstract: The analysis of network assortativity is of great importance for
understanding the structural characteristics of and dynamics upon networks.
Often, network assortativity is quantified using the assortativity coefficient
that is defined based on the Pearson correlation coefficient between vertex
degrees. It is well known that a network may contain sensitive information,
such as the number of friends of an individual in a social network (which is
abstracted as the degree of vertex.). So, the computation of the assortativity
coefficient leads to privacy leakage, which increases the urgent need for
privacy-preserving protocol. However, there has been no scheme addressing the
concern above.
  To bridge this gap, in this work, we are the first to propose approaches
based on differential privacy (DP for short). Specifically, we design three
DP-based algorithms: $Local_{ru}$, $Shuffle_{ru}$, and $Decentral_{ru}$. The
first two algorithms, based on Local DP (LDP) and Shuffle DP respectively, are
designed for settings where each individual only knows his/her direct friends.
In contrast, the third algorithm, based on Decentralized DP (DDP), targets
scenarios where each individual has a broader view, i.e., also knowing his/her
friends' friends. Theoretically, we prove that each algorithm enables an
unbiased estimation of the assortativity coefficient of the network. We further
evaluate the performance of the proposed algorithms using mean squared error
(MSE), showing that $Shuffle_{ru}$ achieves the best performance, followed by
$Decentral_{ru}$, with $Local_{ru}$ performing the worst. Note that these three
algorithms have different assumptions, so each has its applicability scenario.
Lastly, we conduct extensive numerical simulations, which demonstrate that the
presented approaches are adequate to achieve the estimation of network
assortativity under the demand for privacy protection.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [16] [Uncertainty Quantification for Machine Learning in Healthcare: A Survey](https://arxiv.org/abs/2505.02874)
*L. Julián Lechuga López,Shaza Elsharief,Dhiyaa Al Jorf,Firas Darwish,Congbo Ma,Farah E. Shamout*

Main category: cs.LG

TLDR: 本文综述了机器学习在医疗保健中不确定性量化（UQ）的现状，提出了一个框架，将不同方法整合到ML流程的各阶段，并探讨了挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 医疗保健中ML模型缺乏原则性的不确定性量化，限制了其可靠性和应用。

Method: 通过全面分析当前UQ方法，提出一个框架，指导方法在数据预处理、训练和评估阶段的整合。

Result: 总结了医疗保健中流行的UQ方法及潜在的新方法，为研究者和从业者提供了技术选择指南。

Conclusion: 该研究为提升ML在医疗保健中的可靠性、安全性和信任提供了清晰的方向。

Abstract: Uncertainty Quantification (UQ) is pivotal in enhancing the robustness,
reliability, and interpretability of Machine Learning (ML) systems for
healthcare, optimizing resources and improving patient care. Despite the
emergence of ML-based clinical decision support tools, the lack of principled
quantification of uncertainty in ML models remains a major challenge. Current
reviews have a narrow focus on analyzing the state-of-the-art UQ in specific
healthcare domains without systematically evaluating method efficacy across
different stages of model development, and despite a growing body of research,
its implementation in healthcare applications remains limited. Therefore, in
this survey, we provide a comprehensive analysis of current UQ in healthcare,
offering an informed framework that highlights how different methods can be
integrated into each stage of the ML pipeline including data processing,
training and evaluation. We also highlight the most popular methods used in
healthcare and novel approaches from other domains that hold potential for
future adoption in the medical context. We expect this study will provide a
clear overview of the challenges and opportunities of implementing UQ in the ML
pipeline for healthcare, guiding researchers and practitioners in selecting
suitable techniques to enhance the reliability, safety and trust from patients
and clinicians on ML-driven healthcare solutions.

</details>

### [17] [A Wireless Collaborated Inference Acceleration Framework for Plant Disease Recognition](https://arxiv.org/abs/2505.02877)
*Hele Zhu,Xinyi Huang,Haojia Gao,Mengfei Jiang,Haohua Que,Lei Mu*

Main category: cs.LG

TLDR: 提出了一种基于边缘设备与云服务器协同推理的植物病害识别框架，通过深度强化学习修剪模型并采用贪心策略优化分割点，显著提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统手动识别方法效率低且成本高，深度学习在资源受限设备上运行困难，云服务器推理受限于通信带宽。

Method: 使用深度强化学习修剪DNN模型，贪心策略确定最优分割点，实现协同推理加速。

Result: 实验表明该框架显著提升推理速度，同时保持可接受的识别精度。

Conclusion: 为植物病害快速诊断与预防提供了新解决方案。

Abstract: Plant disease is a critical factor affecting agricultural production.
Traditional manual recognition methods face significant drawbacks, including
low accuracy, high costs, and inefficiency. Deep learning techniques have
demonstrated significant benefits in identifying plant diseases, but they still
face challenges such as inference delays and high energy consumption. Deep
learning algorithms are difficult to run on resource-limited embedded devices.
Offloading these models to cloud servers is confronted with the restriction of
communication bandwidth, and all of these factors will influence the
inference's efficiency. We propose a collaborative inference framework for
recognizing plant diseases between edge devices and cloud servers to enhance
inference speed. The DNN model for plant disease recognition is pruned through
deep reinforcement learning to improve the inference speed and reduce energy
consumption. Then the optimal split point is determined by a greedy strategy to
achieve the best collaborated inference acceleration. Finally, the system for
collaborative inference acceleration in plant disease recognition has been
implemented using Gradio to facilitate friendly human-machine interaction.
Experiments indicate that the proposed collaborative inference framework
significantly increases inference speed while maintaining acceptable
recognition accuracy, offering a novel solution for rapidly diagnosing and
preventing plant diseases.

</details>

### [18] [LLM4FTS: Enhancing Large Language Models for Financial Time Series Prediction](https://arxiv.org/abs/2505.02880)
*Zian Liu,Renjun Jia*

Main category: cs.LG

TLDR: 论文提出了一种名为LLM4FTS的新框架，通过可学习的片段分割和动态小波卷积模块增强大语言模型（LLM）对金融时间序列的建模能力，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列预测因信号噪声比低和复杂的时间模式而具有挑战性，传统机器学习模型能力有限，而现有LLM方法忽略了市场数据的多尺度特征。

Method: 结合K-means++聚类和DTW距离识别尺度不变模式，引入自适应片段分割和动态小波卷积模块，以捕捉时间序列的多尺度特征。

Result: 在真实金融数据集上验证了框架的有效性，实现了股票收益预测的最先进性能，并在实际交易系统中成功部署。

Conclusion: LLM4FTS显著提升了LLM在金融预测中的应用能力，为复杂市场模式的建模提供了新思路。

Abstract: Predicting financial time series presents significant challenges due to
inherent low signal-to-noise ratios and intricate temporal patterns.
Traditional machine learning models exhibit limitations in this forecasting
task constrained by their restricted model capacity. Recent advances in large
language models (LLMs), with their greatly expanded parameter spaces,
demonstrate promising potential for modeling complex dependencies in temporal
sequences. However, existing LLM-based approaches typically focus on
fixed-length patch analysis due to the Transformer architecture, ignoring
market data's multi-scale pattern characteristics. In this study, we propose
$LLM4FTS$, a novel framework that enhances LLM capabilities for temporal
sequence modeling through learnable patch segmentation and dynamic wavelet
convolution modules. Specifically,we first employ K-means++ clustering based on
DTW distance to identify scale-invariant patterns in market data. Building upon
pattern recognition results, we introduce adaptive patch segmentation that
partitions temporal sequences while preserving maximal pattern integrity. To
accommodate time-varying frequency characteristics, we devise a dynamic wavelet
convolution module that emulates discrete wavelet transformation with enhanced
flexibility in capturing time-frequency features. These three modules work
together to improve large language model's ability to handle scale-invariant
patterns in financial time series. Extensive experiments on real-world
financial datasets substantiate the framework's efficacy, demonstrating
superior performance in capturing complex market patterns and achieving
state-of-the-art results in stock return prediction. The successful deployment
in practical trading systems confirms its real-world applicability,
representing a significant advancement in LLM applications for financial
forecasting.

</details>

### [19] [Rewriting Pre-Training Data Boosts LLM Performance in Math and Code](https://arxiv.org/abs/2505.02881)
*Kazuki Fujii,Yukito Tajima,Sakae Mizuki,Hinari Shimada,Taihei Shiotani,Koshiro Saito,Masanari Ohi,Masaki Kawamura,Taishi Nakamura,Takumi Okamoto,Shigeki Ishida,Kakeru Hattori,Youmi Ma,Hiroya Takamura,Rio Yokota,Naoaki Okazaki*

Main category: cs.LG

TLDR: 论文介绍了两个公开数据集SwallowCode和SwallowMath，通过系统重写公共数据显著提升大语言模型（LLM）在程序合成和数学推理中的性能。实验表明，这些数据集在固定训练预算下显著提高了模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在程序合成和数学推理中的性能受限于预训练语料的质量，因此需要高质量的数据集来提升模型能力。

Method: SwallowCode通过四阶段管道（语法验证、风格过滤、两阶段LLM重写）优化Python代码；SwallowMath通过去除冗余、恢复上下文和格式化解决方案来提升数学数据质量。

Result: 在50亿token的训练预算下，使用SwallowCode的Llama-3.1-8B在HumanEval和HumanEval+上分别提升17.0和17.7；SwallowMath在GSM8K和MATH上分别提升12.4和7.6。

Conclusion: 论文提出的数据集和方法显著提升了LLM性能，所有资源公开，推动了可复现研究和领域专用LLM预训练的进展。

Abstract: The performance of large language models (LLMs) in program synthesis and
mathematical reasoning is fundamentally limited by the quality of their
pre-training corpora. We introduce two openly licensed datasets, released under
the Llama 3.3 Community License, that significantly enhance LLM performance by
systematically rewriting public data. SwallowCode (approximately 16.1 billion
tokens) refines Python snippets from The-Stack-v2 through a novel four-stage
pipeline: syntax validation, pylint-based style filtering, and a two-stage LLM
rewriting process that enforces style conformity and transforms snippets into
self-contained, algorithmically efficient examples. Unlike prior methods that
rely on exclusionary filtering or limited transformations, our
transform-and-retain approach upgrades low-quality code, maximizing data
utility. SwallowMath (approximately 2.3 billion tokens) enhances Finemath-4+ by
removing boilerplate, restoring context, and reformatting solutions into
concise, step-by-step explanations. Within a fixed 50 billion token training
budget, continual pre-training of Llama-3.1-8B with SwallowCode boosts pass@1
by +17.0 on HumanEval and +17.7 on HumanEval+ compared to Stack-Edu, surpassing
the baseline model's code generation capabilities. Similarly, substituting
SwallowMath yields +12.4 accuracy on GSM8K and +7.6 on MATH. Ablation studies
confirm that each pipeline stage contributes incrementally, with rewriting
delivering the largest gains. All datasets, prompts, and checkpoints are
publicly available, enabling reproducible research and advancing LLM
pre-training for specialized domains.

</details>

### [20] [Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?](https://arxiv.org/abs/2505.02884)
*Guangzhi Sun,Potsawee Manakul,Xiao Zhan,Mark Gales*

Main category: cs.LG

TLDR: 论文提出了一种新的遗忘方法DF-MCQ，通过KL散度扁平化模型预测分布，有效移除目标知识，并引入探测框架评估遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 支持数据隐私、法规合规和伦理AI部署，区分遗忘与混淆，解决现有方法仅是知识添加而非真正移除的问题。

Method: 提出DF-MCQ方法，利用KL散度扁平化模型在自动生成多选题上的预测分布，实现知识移除。

Result: DF-MCQ遗忘效果显著，拒绝率超90%，不确定性高于混淆方法。

Conclusion: DF-MCQ是一种有效的遗忘方法，优于现有混淆技术。

Abstract: Unlearning has emerged as a critical capability for large language models
(LLMs) to support data privacy, regulatory compliance, and ethical AI
deployment. Recent techniques often rely on obfuscation by injecting incorrect
or irrelevant information to suppress knowledge. Such methods effectively
constitute knowledge addition rather than true removal, often leaving models
vulnerable to probing. In this paper, we formally distinguish unlearning from
obfuscation and introduce a probing-based evaluation framework to assess
whether existing approaches genuinely remove targeted information. Moreover, we
propose DF-MCQ, a novel unlearning method that flattens the model predictive
distribution over automatically generated multiple-choice questions using
KL-divergence, effectively removing knowledge about target individuals and
triggering appropriate refusal behaviour. Experimental results demonstrate that
DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level
uncertainty that is much higher than obfuscation on probing questions.

</details>

### [21] [When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger](https://arxiv.org/abs/2505.02888)
*Rintaro Ando*

Main category: cs.LG

TLDR: N2M-RSI是一个形式化模型，展示AI代理在反馈输出作为输入并跨越信息整合阈值后，其内部复杂性将无限增长。


<details>
  <summary>Details</summary>
Motivation: 统一自提示大语言模型、哥德尔自指和AutoML等概念，同时保持实现无关性。

Method: 通过递归反馈输出作为输入，并设定明确的信息整合阈值。

Result: 模型内部复杂性无限增长，且在多代理交互中表现出超线性效应。

Conclusion: 为避免安全问题，仅发布模型无关的简化原型。

Abstract: We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal
formal model showing that once an AI agent feeds its own outputs back as inputs
and crosses an explicit information-integration threshold, its internal
complexity will grow without bound under our assumptions. The framework unifies
earlier ideas on self-prompting large language models, G\"odelian
self-reference, and AutoML, yet remains implementation-agnostic. The model
furthermore scales naturally to interacting swarms of agents, hinting at
super-linear effects once communication among instances is permitted. For
safety reasons, we omit system-specific implementation details and release only
a brief, model-agnostic toy prototype in Appendix C.

</details>

### [22] [Early Prediction of Sepsis: Feature-Aligned Transfer Learning](https://arxiv.org/abs/2505.02889)
*Oyindolapo O. Komolafe,Zhimin Mei,David Morales Zarate,Gregory William Spangenberg*

Main category: cs.LG

TLDR: 提出了一种名为FATL的机器学习方法，用于早期预测脓毒症，通过特征对齐和迁移学习解决现有模型的不一致性和人口偏差问题。


<details>
  <summary>Details</summary>
Motivation: 脓毒症早期检测对挽救生命至关重要，但现有诊断方法往往滞后且模型不一致。

Method: 采用特征对齐迁移学习（FATL），聚焦重要且常见的特征，并结合多样人群模型以减少偏差。

Result: FATL提高了模型的通用性和临床相关性，适用于资源有限的医院。

Conclusion: FATL为脓毒症早期检测提供了实用且可扩展的解决方案，有望改善患者预后和医疗公平性。

Abstract: Sepsis is a life threatening medical condition that occurs when the body has
an extreme response to infection, leading to widespread inflammation, organ
failure, and potentially death. Because sepsis can worsen rapidly, early
detection is critical to saving lives. However, current diagnostic methods
often identify sepsis only after significant damage has already occurred. Our
project aims to address this challenge by developing a machine learning based
system to predict sepsis in its early stages, giving healthcare providers more
time to intervene.
  A major problem with existing models is the wide variability in the patient
information or features they use, such as heart rate, temperature, and lab
results. This inconsistency makes models difficult to compare and limits their
ability to work across different hospitals and settings. To solve this, we
propose a method called Feature Aligned Transfer Learning (FATL), which
identifies and focuses on the most important and commonly reported features
across multiple studies, ensuring the model remains consistent and clinically
relevant.
  Most existing models are trained on narrow patient groups, leading to
population bias. FATL addresses this by combining knowledge from models trained
on diverse populations, using a weighted approach that reflects each models
contribution. This makes the system more generalizable and effective across
different patient demographics and clinical environments. FATL offers a
practical and scalable solution for early sepsis detection, particularly in
hospitals with limited resources, and has the potential to improve patient
outcomes, reduce healthcare costs, and support more equitable healthcare
delivery.

</details>

### [23] [RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference](https://arxiv.org/abs/2505.02922)
*Yaoqi Chen,Jinkai Zhang,Baotong Lu,Qianxi Zhang,Chengruidong Zhang,Jingjia Luo,Di Liu,Huiqiang Jiang,Qi Chen,Jing Liu,Bailu Ding,Xiao Yan,Jiawei Jiang,Chen Chen,Mingxing Zhang,Yuqing Yang,Fan Yang,Mao Yang*

Main category: cs.LG

TLDR: RetroInfer通过重新设计KV缓存为向量存储系统，利用注意力稀疏性加速长上下文LLM推理，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在长上下文推理中因GPU内存和带宽限制导致的效率问题。

Method: 提出wave index（注意力感知向量索引）和wave buffer（协调KV缓存放置和计算/数据传输），结合三部分注意力近似等技术。

Result: 在长上下文基准测试中，性能提升达4.5倍（GPU内存内）和10.5倍（扩展到CPU内存），同时保持全注意力精度。

Conclusion: RetroInfer在硬件协调和性能优化方面优于现有稀疏注意力方法，且不影响模型精度。

Abstract: The growing context lengths of large language models (LLMs) pose significant
challenges for efficient inference, primarily due to GPU memory and bandwidth
constraints. We present RetroInfer, a novel system that reconceptualizes the
key-value (KV) cache as a vector storage system which exploits the inherent
attention sparsity to accelerate long-context LLM inference. At its core is the
wave index, an Attention-aWare VEctor index that enables efficient and accurate
retrieval of critical tokens through techniques such as tripartite attention
approximation, accuracy-bounded attention estimation, and segmented clustering.
Complementing this is the wave buffer, which coordinates KV cache placement and
overlaps computation and data transfer across GPU and CPU to sustain high
throughput. Unlike prior sparsity-based methods that struggle with token
selection and hardware coordination, RetroInfer delivers robust performance
without compromising model accuracy. Experiments on long-context benchmarks
show up to 4.5X speedup over full attention within GPU memory limits and up to
10.5X over sparse attention baselines when KV cache is extended to CPU memory,
all while preserving full-attention-level accuracy.

</details>

### [24] [Smooth Quadratic Prediction Markets](https://arxiv.org/abs/2505.02959)
*Enrique Nueve,Bo Waggoner*

Main category: cs.LG

TLDR: 论文提出了一种新的预测市场设计，称为平滑二次预测市场（Smooth Quadratic Prediction Market），通过分解和修改基于对偶的成本函数市场定价机制，激励代理集体实现一般最速梯度下降。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过其他学习算法（而非仅Follow-The-Regularized-Leader）来设计预测市场，以改进基于对偶的成本函数市场定价机制（DCFMM）。

Method: 通过分解和修改DCFMM的定价机制，设计新的预测市场（平滑二次预测市场），激励代理实现一般最速梯度下降。

Result: 平滑二次预测市场在AD证券的最坏货币损失方面优于DCFMM，同时保留了瞬时价格存在、信息整合、表达性、无套利和某种形式的激励兼容性等公理保证。

Conclusion: 研究结果表明，未来设计可以将价格更新规则与费用结构分离，同时保留保证。

Abstract: When agents trade in a Duality-based Cost Function prediction market, they
collectively implement the learning algorithm Follow-The-Regularized-Leader. We
ask whether other learning algorithms could be used to inspire the design of
prediction markets. By decomposing and modifying the Duality-based Cost
Function Market Maker's (DCFMM) pricing mechanism, we propose a new prediction
market, called the Smooth Quadratic Prediction Market, the incentivizes agents
to collectively implement general steepest gradient descent. Relative to the
DCFMM, the Smooth Quadratic Prediction Market has a better worst-case monetary
loss for AD securities while preserving axiom guarantees such as the existence
of instantaneous price, information incorporation, expressiveness, no
arbitrage, and a form of incentive compatibility. To motivate the application
of the Smooth Quadratic Prediction Market, we independently examine agents'
trading behavior under two realistic constraints: bounded budgets and buy-only
securities. Finally, we provide an introductory analysis of an approach to
facilitate adaptive liquidity using the Smooth Quadratic AD Prediction Market.
Our results suggest future designs where the price update rule is separate from
the fee structure, yet guarantees are preserved.

</details>

### [25] [Physics-Learning AI Datamodel (PLAID) datasets: a collection of physics simulations for machine learning](https://arxiv.org/abs/2505.02974)
*Fabien Casenave,Xavier Roynard,Brian Staber,Nissrine Akkari,William Piat,Michele Alessandro Bucci,Abbas Kabalan,Xuan Minh Vuong Nguyen,Luca Saverio,Raphaël Carpintero Perez,Anthony Kalaydjian,Samy Fouché,Thierry Gonon,Ghassan Najjar,Emmanuel Menier,Matthieu Nastorg,Christian Rey*

Main category: cs.LG

TLDR: PLAID是一个用于表示和共享物理模拟数据的灵活框架，解决了现有数据集的局限性，并提供了标准化工具和基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有物理模拟数据集规模小、碎片化且缺乏标准化，限制了机器学习代理模型的广泛应用。

Method: 提出PLAID框架，定义统一的数据标准，并开发配套工具库，支持跨物理领域的复杂数据集创建与管理。

Result: 发布了六个涵盖结构力学和计算流体力学的数据集，并提供了基准测试工具。

Conclusion: PLAID通过标准化和工具支持，促进了物理模拟数据的共享和机器学习模型的开发。

Abstract: Machine learning-based surrogate models have emerged as a powerful tool to
accelerate simulation-driven scientific workflows. However, their widespread
adoption is hindered by the lack of large-scale, diverse, and standardized
datasets tailored to physics-based simulations. While existing initiatives
provide valuable contributions, many are limited in scope-focusing on specific
physics domains, relying on fragmented tooling, or adhering to overly
simplistic datamodels that restrict generalization. To address these
limitations, we introduce PLAID (Physics-Learning AI Datamodel), a flexible and
extensible framework for representing and sharing datasets of physics
simulations. PLAID defines a unified standard for describing simulation data
and is accompanied by a library for creating, reading, and manipulating complex
datasets across a wide range of physical use cases (gitlab.com/drti/plaid). We
release six carefully crafted datasets under the PLAID standard, covering
structural mechanics and computational fluid dynamics, and provide baseline
benchmarks using representative learning methods. Benchmarking tools are made
available on Hugging Face, enabling direct participation by the community and
contribution to ongoing evaluation efforts (huggingface.co/PLAIDcompetitions).

</details>

### [26] [More Optimal Fractional-Order Stochastic Gradient Descent for Non-Convex Optimization Problems](https://arxiv.org/abs/2505.02985)
*Mohammad Partohaghighi,Roummel Marcia,YangQuan Chen*

Main category: cs.LG

TLDR: 2SEDFOSGD通过结合2SED算法与FOSGD，动态调整分数阶指数，优化收敛速度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统FOSGD在优化中难以调谐和稳定分数阶指数，限制了其应用。

Method: 提出2SEDFOSGD，利用2SED算法动态调整分数阶指数，基于模型敏感性和有效维数。

Result: 在非凸优化问题中，2SEDFOSGD表现出更快的收敛速度和更稳健的参数估计。

Conclusion: 2SEDFOSGD展示了维度感知分数阶技术在高级建模和估计任务中的潜力。

Abstract: Fractional-order stochastic gradient descent (FOSGD) leverages fractional
exponents to capture long-memory effects in optimization. However, its utility
is often limited by the difficulty of tuning and stabilizing these exponents.
We propose 2SED Fractional-Order Stochastic Gradient Descent (2SEDFOSGD), which
integrates the Two-Scale Effective Dimension (2SED) algorithm with FOSGD to
adapt the fractional exponent in a data-driven manner. By tracking model
sensitivity and effective dimensionality, 2SEDFOSGD dynamically modulates the
exponent to mitigate oscillations and hasten convergence. Theoretically, for
onoconvex optimization problems, this approach preserves the advantages of
fractional memory without the sluggish or unstable behavior observed in na\"ive
fractional SGD. Empirical evaluations in Gaussian and $\alpha$-stable noise
scenarios using an autoregressive (AR) model highlight faster convergence and
more robust parameter estimates compared to baseline methods, underscoring the
potential of dimension-aware fractional techniques for advanced modeling and
estimation tasks.

</details>

### [27] [Radio: Rate-Distortion Optimization for Large Language Model Compression](https://arxiv.org/abs/2505.03031)
*Sean I. Young*

Main category: cs.LG

TLDR: 论文提出了一种基于率失真理论的量化技术，用于压缩大型语言模型（LLMs），支持用户根据需要调整模型大小或精度。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在资源受限设备上的部署问题，降低计算成本，并减少大规模AI基础设施对环境的影响。

Method: 从率失真理论角度出发，提出一种基于率失真优化的量化技术，支持训练后模型压缩。

Result: 技术可扩展到包含数千亿权重参数的模型，并提供灵活的压缩选项。

Conclusion: 该量化技术为LLMs的高效部署提供了实用解决方案。

Abstract: In recent years, the compression of large language models (LLMs) has emerged
as a key problem in facilitating LLM deployment on resource-limited devices,
reducing compute costs, and mitigating the environmental footprint due to
large-scale AI infrastructure. Here, we establish the foundations of LLM
quantization from a rate-distortion theory perspective and propose a
quantization technique based on simple rate-distortion optimization. Our
technique scales to models containing hundreds of billions of weight parameters
and offers users the flexibility to compress models, post-training, to a model
size or accuracy specified by the user.

</details>

### [28] [A New Perspective To Understanding Multi-resolution Hash Encoding For Neural Fields](https://arxiv.org/abs/2505.03042)
*Steven Tin Sui Luo*

Main category: cs.LG

TLDR: Instant-NGP的多分辨率哈希网格结构显著提升了神经网络的性能，但其原理尚不明确。本文提出“域操作”视角，解释哈希网格如何通过人为增加线性片段提升表达能力，并通过实验验证其通用性。


<details>
  <summary>Details</summary>
Motivation: Instant-NGP的哈希网格结构虽被广泛应用和改进，但其工作原理缺乏理论支持，导致超参数只能凭经验调整。本文旨在提供直观的解释。

Method: 提出“域操作”视角，通过实验分析1维信号，解释哈希网格如何通过增加线性片段提升神经场的表达能力。

Result: 实验验证了“域操作”视角的有效性，并表明该方法可推广到更高维度。

Conclusion: 本文为哈希网格的工作原理提供了理论支持，并展示了其在高维信号中的通用性。

Abstract: Instant-NGP has been the state-of-the-art architecture of neural fields in
recent years. Its incredible signal-fitting capabilities are generally
attributed to its multi-resolution hash grid structure and have been used and
improved in numerous following works. However, it is unclear how and why such a
hash grid structure improves the capabilities of a neural network by such great
margins. A lack of principled understanding of the hash grid also implies that
the large set of hyperparameters accompanying Instant-NGP could only be tuned
empirically without much heuristics. To provide an intuitive explanation of the
working principle of the hash grid, we propose a novel perspective, namely
domain manipulation. This perspective provides a ground-up explanation of how
the feature grid learns the target signal and increases the expressivity of the
neural field by artificially creating multiples of pre-existing linear
segments. We conducted numerous experiments on carefully constructed
1-dimensional signals to support our claims empirically and aid our
illustrations. While our analysis mainly focuses on 1-dimensional signals, we
show that the idea is generalizable to higher dimensions.

</details>

### [29] [34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery](https://arxiv.org/abs/2505.03049)
*Yoel Zimmermann,Adib Bazgir,Alexander Al-Feghali,Mehrad Ansari,L. Catherine Brinson,Yuan Chiang,Defne Circi,Min-Hsueh Chiu,Nathan Daelman,Matthew L. Evans,Abhijeet S. Gangan,Janine George,Hassan Harb,Ghazal Khalighinejad,Sartaaj Takrim Khan,Sascha Klawohn,Magdalena Lederbauer,Soroush Mahjoubi,Bernadette Mohr,Seyed Mohamad Moosavi,Aakash Naik,Aleyna Beste Ozhan,Dieter Plessers,Aritra Roy,Fabian Schöppach,Philippe Schwaller,Carla Terboven,Katharina Ueltzen,Shang Zhu,Jan Janssen,Calvin Li,Ian Foster,Ben Blaiszik*

Main category: cs.LG

TLDR: 大语言模型（LLMs）正在改变材料科学与化学研究的多个领域，涵盖分子性质预测、材料设计、科学自动化等。34个项目展示了LLMs在7个关键研究领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在材料科学与化学研究全生命周期中的前沿能力，展示其作为多功能预测模型和快速原型开发平台的潜力。

Method: 通过第二届年度LLM黑客马拉松的34个项目，分析LLMs在7个关键研究领域的应用。

Result: LLMs在低数据环境和跨学科研究中表现出色，开源和专有模型的性能提升进一步扩展了其有效性。

Conclusion: LLMs的持续改进为科学工作流带来新机遇与挑战，需进一步研究以解决可靠性、可解释性和可重复性问题。

Abstract: Large Language Models (LLMs) are reshaping many aspects of materials science
and chemistry research, enabling advances in molecular property prediction,
materials design, scientific automation, knowledge extraction, and more. Recent
developments demonstrate that the latest class of models are able to integrate
structured and unstructured data, assist in hypothesis generation, and
streamline research workflows. To explore the frontier of LLM capabilities
across the research lifecycle, we review applications of LLMs through 34 total
projects developed during the second annual Large Language Model Hackathon for
Applications in Materials Science and Chemistry, a global hybrid event. These
projects spanned seven key research areas: (1) molecular and material property
prediction, (2) molecular and material design, (3) automation and novel
interfaces, (4) scientific communication and education, (5) research data
management and automation, (6) hypothesis generation and evaluation, and (7)
knowledge extraction and reasoning from the scientific literature.
Collectively, these applications illustrate how LLMs serve as versatile
predictive models, platforms for rapid prototyping of domain-specific tools,
and much more. In particular, improvements in both open source and proprietary
LLM performance through the addition of reasoning, additional training data,
and new techniques have expanded effectiveness, particularly in low-data
environments and interdisciplinary research. As LLMs continue to improve, their
integration into scientific workflows presents both new opportunities and new
challenges, requiring ongoing exploration, continued refinement, and further
research to address reliability, interpretability, and reproducibility.

</details>

### [30] [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/abs/2505.03084)
*Shashank Kapoor,Sanjay Surendranath Girija,Lakshit Arora,Dipen Pradhan,Ankit Shetgaonkar,Aman Raj*

Main category: cs.LG

TLDR: 该论文综述了多模态模型中的对抗攻击类型，填补了实践者视角的空白，首次全面总结了多模态世界的威胁格局。


<details>
  <summary>Details</summary>
Motivation: 多模态模型的普及带来了对抗攻击的放大威胁，但目前缺乏针对实践者的攻击类型概述。

Method: 通过调查针对文本、图像、视频和音频四种模态的对抗攻击，分析威胁格局的演变。

Result: 提供了多模态对抗攻击的全面总结，揭示了威胁的演变趋势。

Conclusion: 该研究为实践者提供了对抗攻击的全局视角，有助于采取预防措施。

Abstract: The introduction of multimodal models is a huge step forward in Artificial
Intelligence. A single model is trained to understand multiple modalities:
text, image, video, and audio. Open-source multimodal models have made these
breakthroughs more accessible. However, considering the vast landscape of
adversarial attacks across these modalities, these models also inherit
vulnerabilities of all the modalities, and ultimately, the adversarial threat
amplifies. While broad research is available on possible attacks within or
across these modalities, a practitioner-focused view that outlines attack types
remains absent in the multimodal world. As more Machine Learning Practitioners
adopt, fine-tune, and deploy open-source models in real-world applications,
it's crucial that they can view the threat landscape and take the preventive
actions necessary. This paper addresses the gap by surveying adversarial
attacks targeting all four modalities: text, image, video, and audio. This
survey provides a view of the adversarial attack landscape and presents how
multimodal adversarial threats have evolved. To the best of our knowledge, this
survey is the first comprehensive summarization of the threat landscape in the
multimodal world.

</details>

### [31] [Deep Learning in Renewable Energy Forecasting: A Cross-Dataset Evaluation of Temporal and Spatial Models](https://arxiv.org/abs/2505.03109)
*Lutfu Sua,Haibo Wang,Jun Huang*

Main category: cs.LG

TLDR: 论文探讨了深度学习模型在可再生能源领域的应用，比较了多种方法，发现LSTM和MLP表现最佳。


<details>
  <summary>Details</summary>
Motivation: 可再生能源数据的非线性关系和复杂性需要更强大的建模方法，传统机器学习模型难以捕捉这些复杂关系。

Method: 提出了一个深度学习框架，比较了七种方法（如LSTM、CNN等），并使用了两种数据集（天气与发电数据、光伏发电数据），同时采用正则化方法减少过拟合。

Result: LSTM和MLP模型表现最优，验证数据的均方根误差极低。

Conclusion: 深度学习模型（尤其是LSTM和MLP）在可再生能源领域具有显著优势，能够有效建模复杂关系。

Abstract: Unpredictability of renewable energy sources coupled with the complexity of
those methods used for various purposes in this area calls for the development
of robust methods such as DL models within the renewable energy domain. Given
the nonlinear relationships among variables in renewable energy datasets, DL
models are preferred over traditional machine learning (ML) models because they
can effectively capture and model complex interactions between variables. This
research aims to identify the factors responsible for the accuracy of DL
techniques, such as sampling, stationarity, linearity, and hyperparameter
optimization for different algorithms. The proposed DL framework compares
various methods and alternative training/test ratios. Seven ML methods, such as
Long-Short Term Memory (LSTM), Stacked LSTM, Convolutional Neural Network
(CNN), CNN-LSTM, Deep Neural Network (DNN), Multilayer Perceptron (MLP), and
Encoder-Decoder (ED), were evaluated on two different datasets. The first
dataset contains the weather and power generation data. It encompasses two
distinct datasets, hourly energy demand data and hourly weather data in Spain,
while the second dataset includes power output generated by the photovoltaic
panels at 12 locations. This study deploys regularization approaches, including
early stopping, neuron dropping, and L2 regularization, to reduce the
overfitting problem associated with DL models. The LSTM and MLP models show
superior performance. Their validation data exhibit exceptionally low root mean
square error values.

</details>

### [32] [Plug-and-Play AMC: Context Is King in Training-Free, Open-Set Modulation with LLMs](https://arxiv.org/abs/2505.03112)
*Mohammad Rostami,Atik Faysal,Reihaneh Gh. Roshan,Huaxia Wang,Nikhil Muralidhar,Yu-Dong Yao*

Main category: cs.LG

TLDR: 提出了一种结合传统信号处理技术与大语言模型（LLM）的创新框架，用于自动调制分类（AMC），在无需额外训练或预处理的情况下实现高效分类。


<details>
  <summary>Details</summary>
Motivation: AMC在频谱管理和无线通信中至关重要，但信号干扰和噪声的复杂性使其具有挑战性。

Method: 利用高阶统计和累积量估计将定量信号特征转化为结构化自然语言提示，结合LLM的经典信号处理知识实现一次性分类。

Result: 在合成数据集上的实验表明，该方法在不同调制方案和信噪比下均表现优异。

Conclusion: 该方法为无线通信中的基础模型提供了新思路，显著降低了开发特定信道模型的成本，为下一代无线网络的可扩展、可解释信号分类系统奠定了基础。

Abstract: Automatic Modulation Classification (AMC) is critical for efficient spectrum
management and robust wireless communications. However, AMC remains challenging
due to the complex interplay of signal interference and noise. In this work, we
propose an innovative framework that integrates traditional signal processing
techniques with Large-Language Models (LLMs) to address AMC. Our approach
leverages higher-order statistics and cumulant estimation to convert
quantitative signal features into structured natural language prompts. By
incorporating exemplar contexts into these prompts, our method exploits the
LLM's inherent familiarity with classical signal processing, enabling effective
one-shot classification without additional training or preprocessing (e.g.,
denoising). Experimental evaluations on synthetically generated datasets,
spanning both noiseless and noisy conditions, demonstrate that our framework
achieves competitive performance across diverse modulation schemes and
Signal-to-Noise Ratios (SNRs). Moreover, our approach paves the way for robust
foundation models in wireless communications across varying channel conditions,
significantly reducing the expense associated with developing channel-specific
models. This work lays the foundation for scalable, interpretable, and
versatile signal classification systems in next-generation wireless networks.
The source code is available at https://github.com/RU-SIT/context-is-king

</details>

### [33] [Adaptive Thresholding for Multi-Label Classification via Global-Local Signal Fusion](https://arxiv.org/abs/2505.03118)
*Dmytro Shamatrin*

Main category: cs.LG

TLDR: 本文提出了一种自适应阈值机制，结合全局和局部信号，用于多标签分类任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多标签分类在类别不平衡和噪声条件下表现不佳，传统方法忽略了上下文和全局稀有性。

Method: 提出了一种融合全局（基于IDF）和局部（基于KNN）信号的自适应阈值机制，并将其作为可微分的损失惩罚。

Result: 在AmazonCat-13K基准测试中，宏F1达到0.1712，优于树基和预训练Transformer方法。

Conclusion: 该方法轻量、可解释且模块化，代码已开源。

Abstract: Multi-label classification (MLC) requires predicting multiple labels per
sample, often under heavy class imbalance and noisy conditions. Traditional
approaches apply fixed thresholds or treat labels independently, overlooking
context and global rarity. We introduce an adaptive thresholding mechanism that
fuses global (IDF-based) and local (KNN-based) signals to produce per-label,
per-instance thresholds. Instead of applying these as hard cutoffs, we treat
them as differentiable penalties in the loss, providing smooth supervision and
better calibration. Our architecture is lightweight, interpretable, and highly
modular. On the AmazonCat-13K benchmark, it achieves a macro-F1 of 0.1712,
substantially outperforming tree-based and pretrained transformer-based
methods. We release full code for reproducibility and future extensions.

</details>

### [34] [Rethinking the Global Convergence of Softmax Policy Gradient with Linear Function Approximation](https://arxiv.org/abs/2505.03155)
*Max Qiushi Lin,Jincheng Mei,Matin Aghaei,Michael Lu,Bo Dai,Alekh Agarwal,Dale Schuurmans,Csaba Szepesvari,Sharan Vaswani*

Main category: cs.LG

TLDR: 论文研究了线性函数近似的Softmax策略梯度方法（Lin-SPG），证明近似误差不影响其全局收敛性，并提出了保证收敛的特征表示条件。


<details>
  <summary>Details</summary>
Motivation: 策略梯度方法在强化学习中表现优异，但函数近似误差对其全局收敛的影响尚不明确。本文旨在解决这一问题。

Method: 聚焦于Lin-SPG方法，分析其在大状态-动作空间中的表现，并推导特征表示的必要和充分条件。

Result: 证明了Lin-SPG在特定学习率下以O(1/T)速度收敛到最优策略，且任意恒定学习率下也能保证全局收敛。

Conclusion: 近似误差不影响Lin-SPG的全局收敛性，特征表示条件是关键。

Abstract: Policy gradient (PG) methods have played an essential role in the empirical
successes of reinforcement learning. In order to handle large state-action
spaces, PG methods are typically used with function approximation. In this
setting, the approximation error in modeling problem-dependent quantities is a
key notion for characterizing the global convergence of PG methods. We focus on
Softmax PG with linear function approximation (referred to as
$\texttt{Lin-SPG}$) and demonstrate that the approximation error is irrelevant
to the algorithm's global convergence even for the stochastic bandit setting.
Consequently, we first identify the necessary and sufficient conditions on the
feature representation that can guarantee the asymptotic global convergence of
$\texttt{Lin-SPG}$. Under these feature conditions, we prove that $T$
iterations of $\texttt{Lin-SPG}$ with a problem-specific learning rate result
in an $O(1/T)$ convergence to the optimal policy. Furthermore, we prove that
$\texttt{Lin-SPG}$ with any arbitrary constant learning rate can ensure
asymptotic global convergence to the optimal policy.

</details>

### [35] [Improving the Reproducibility of Deep Learning Software: An Initial Investigation through a Case Study Analysis](https://arxiv.org/abs/2505.03165)
*Nikita Ravi,Abhinav Goel,James C. Davis,George K. Thiruvathukal*

Main category: cs.LG

TLDR: 论文提出系统性方法分析并改进深度学习模型的可复现性，通过案例研究展示指南，涵盖环境复制、算法实现、设计透明化及数据管道优化。


<details>
  <summary>Details</summary>
Motivation: 深度学习领域快速发展，但实验结果难以复现，影响可靠性与有效性。Nature研究显示超70%研究者无法复现他人实验，50%无法复现自身实验。

Method: 提出指南包括复制原始软件环境、端到端训练与测试算法、公开架构设计、增强数据处理与训练管道透明度，并进行敏感性分析。

Result: 通过实施这些策略，论文旨在缩小研究与实际应用间的差距，确保深度学习创新可有效复现与部署。

Conclusion: 系统性方法能显著提升深度学习模型的可复现性，为研究与实践提供可靠基础。

Abstract: The field of deep learning has witnessed significant breakthroughs, spanning
various applications, and fundamentally transforming current software
capabilities. However, alongside these advancements, there have been increasing
concerns about reproducing the results of these deep learning methods. This is
significant because reproducibility is the foundation of reliability and
validity in software development, particularly in the rapidly evolving domain
of deep learning. The difficulty of reproducibility may arise due to several
reasons, including having differences from the original execution environment,
incompatible software libraries, proprietary data and source code, lack of
transparency, and the stochastic nature in some software. A study conducted by
the Nature journal reveals that more than 70% of researchers failed to
reproduce other researchers experiments and over 50% failed to reproduce their
own experiments. Irreproducibility of deep learning poses significant
challenges for researchers and practitioners. To address these concerns, this
paper presents a systematic approach at analyzing and improving the
reproducibility of deep learning models by demonstrating these guidelines using
a case study. We illustrate the patterns and anti-patterns involved with these
guidelines for improving the reproducibility of deep learning models. These
guidelines encompass establishing a methodology to replicate the original
software environment, implementing end-to-end training and testing algorithms,
disclosing architectural designs, and enhancing transparency in data processing
and training pipelines. We also conduct a sensitivity analysis to understand
the model performance across diverse conditions. By implementing these
strategies, we aim to bridge the gap between research and practice, so that
innovations in deep learning can be effectively reproduced and deployed within
software.

</details>

### [36] [Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2505.03172)
*Caleb Chuck,Fan Feng,Carl Qi,Chang Shi,Siddhant Agarwal,Amy Zhang,Scott Niekum*

Main category: cs.LG

TLDR: 论文提出HInt方法，结合交互与事后重标记，提升目标导向强化学习的样本效率。


<details>
  <summary>Details</summary>
Motivation: 事后重标记在目标导向强化学习中有效，但在对象中心领域（如机械臂推块）表现不佳，因高奖励无意义轨迹。

Method: 提出HInt方法，基于零反事实定义交互，并通过NCII推断交互。

Result: NCII在多个领域显著提升交互推断准确性，HInt样本效率提升高达4倍。

Conclusion: HInt通过结合交互与事后重标记，有效解决对象中心领域的学习问题。

Abstract: Hindsight relabeling is a powerful tool for overcoming sparsity in
goal-conditioned reinforcement learning (GCRL), especially in certain domains
such as navigation and locomotion. However, hindsight relabeling can struggle
in object-centric domains. For example, suppose that the goal space consists of
a robotic arm pushing a particular target block to a goal location. In this
case, hindsight relabeling will give high rewards to any trajectory that does
not interact with the block. However, these behaviors are only useful when the
object is already at the goal -- an extremely rare case in practice. A dataset
dominated by these kinds of trajectories can complicate learning and lead to
failures. In object-centric domains, one key intuition is that meaningful
trajectories are often characterized by object-object interactions such as
pushing the block with the gripper. To leverage this intuition, we introduce
Hindsight Relabeling using Interactions (HInt), which combines interactions
with hindsight relabeling to improve the sample efficiency of downstream RL.
However because interactions do not have a consensus statistical definition
tractable for downstream GCRL, we propose a definition of interactions based on
the concept of null counterfactual: a cause object is interacting with a target
object if, in a world where the cause object did not exist, the target object
would have different transition dynamics. We leverage this definition to infer
interactions in Null Counterfactual Interaction Inference (NCII), which uses a
"nulling'' operation with a learned model to infer interactions. NCII is able
to achieve significantly improved interaction inference accuracy in both simple
linear dynamics domains and dynamic robotic domains in Robosuite, Robot Air
Hockey, and Franka Kitchen and HInt improves sample efficiency by up to 4x.

</details>

### [37] [RADE: Learning Risk-Adjustable Driving Environment via Multi-Agent Conditional Diffusion](https://arxiv.org/abs/2505.03178)
*Jiawei Wang,Xintao Yan,Yao Mu,Haowei Sun,Zhong Cao,Henry X. Liu*

Main category: cs.LG

TLDR: RADE是一个基于多智能体扩散架构的仿真框架，生成统计真实且风险可调的交通场景，用于自动驾驶车辆的安全评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过设计复杂目标操纵单一车辆轨迹，牺牲了真实性和可扩展性，RADE旨在解决这一问题。

Method: RADE采用多智能体扩散架构，联合建模环境中所有智能体的行为，并通过代理风险度量调节轨迹，同时引入动态检查模块确保物理合理性。

Result: 在真实数据集上验证，RADE在不同风险水平下保持统计真实性，并自然增加安全关键事件的可能性。

Conclusion: RADE是一种可扩展且真实的工具，适用于自动驾驶安全评估。

Abstract: Generating safety-critical scenarios in high-fidelity simulations offers a
promising and cost-effective approach for efficient testing of autonomous
vehicles. Existing methods typically rely on manipulating a single vehicle's
trajectory through sophisticated designed objectives to induce adversarial
interactions, often at the cost of realism and scalability. In this work, we
propose the Risk-Adjustable Driving Environment (RADE), a simulation framework
that generates statistically realistic and risk-adjustable traffic scenes.
Built upon a multi-agent diffusion architecture, RADE jointly models the
behavior of all agents in the environment and conditions their trajectories on
a surrogate risk measure. Unlike traditional adversarial methods, RADE learns
risk-conditioned behaviors directly from data, preserving naturalistic
multi-agent interactions with controllable risk levels. To ensure physical
plausibility, we incorporate a tokenized dynamics check module that efficiently
filters generated trajectories using a motion vocabulary. We validate RADE on
the real-world rounD dataset, demonstrating that it preserves statistical
realism across varying risk levels and naturally increases the likelihood of
safety-critical events as the desired risk level grows up. Our results
highlight RADE's potential as a scalable and realistic tool for AV safety
evaluation.

</details>

### [38] [VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making](https://arxiv.org/abs/2505.03181)
*Jake Grigsby,Yuke Zhu,Michael Ryoo,Juan Carlos Niebles*

Main category: cs.LG

TLDR: 论文提出了一种基于离线到在线强化学习的方法，用于改进视觉语言模型在代理任务中的表现，克服了现有模型在严格输出语法要求上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在代理任务中表现不佳，尤其是在遵循严格输出语法要求方面，需要改进以适应多模态代理应用。

Method: 采用离线到在线强化学习框架，结合监督微调和自我改进机制，利用低质量数据集进行模型优化。

Result: 实验证明该方法在两个开放权重的视觉语言模型和三个多模态代理领域中有效。

Conclusion: 通过强化学习框架，视觉语言模型能够在代理任务中自我改进并适应严格输出要求，为多模态代理应用提供了新思路。

Abstract: Recent research looks to harness the general knowledge and reasoning of large
language models (LLMs) into agents that accomplish user-specified goals in
interactive environments. Vision-language models (VLMs) extend LLMs to
multi-modal data and provide agents with the visual reasoning necessary for new
applications in areas such as computer automation. However, agent tasks
emphasize skills where accessible open-weight VLMs lag behind their LLM
equivalents. For example, VLMs are less capable of following an environment's
strict output syntax requirements and are more focused on open-ended question
answering. Overcoming these limitations requires supervised fine-tuning (SFT)
on task-specific expert demonstrations. Our work approaches these challenges
from an offline-to-online reinforcement learning (RL) perspective. RL lets us
fine-tune VLMs to agent tasks while learning from the unsuccessful decisions of
our own model or more capable (larger) models. We explore an off-policy RL
solution that retains the stability and simplicity of the widely used SFT
workflow while allowing our agent to self-improve and learn from low-quality
datasets. We demonstrate this technique with two open-weight VLMs across three
multi-modal agent domains.

</details>

### [39] [Convergence Of Consistency Model With Multistep Sampling Under General Data Assumptions](https://arxiv.org/abs/2505.03194)
*Yiding Chen,Yiyi Zhang,Owen Oertell,Wen Sun*

Main category: cs.LG

TLDR: 本文研究了自一致性近似条件下一致性模型的收敛性，证明了在不同数据假设下生成样本与目标分布的接近程度，并通过案例展示了多步采样的优势。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在数据生成任务中表现出色，但其迭代采样过程计算成本高。一致性模型通过学习一致性函数实现快速一步生成和多步采样，本文旨在分析其收敛性。

Method: 研究在训练分布下自一致性近似成立时的一致性模型收敛性，分析基于温和数据假设和多种前向过程。

Result: 当目标数据分布有界或尾部衰减足够快时，生成样本在Wasserstein距离下接近目标分布；若目标分布满足平滑性假设，通过额外扰动步骤，生成样本在总变差距离下接近目标分布。

Conclusion: 一致性模型在多种条件下能有效生成接近目标分布的样本，多步采样进一步提升了样本质量。

Abstract: Diffusion models accomplish remarkable success in data generation tasks
across various domains. However, the iterative sampling process is
computationally expensive. Consistency models are proposed to learn consistency
functions to map from noise to data directly, which allows one-step fast data
generation and multistep sampling to improve sample quality. In this paper, we
study the convergence of consistency models when the self-consistency property
holds approximately under the training distribution. Our analysis requires only
mild data assumption and applies to a family of forward processes. When the
target data distribution has bounded support or has tails that decay
sufficiently fast, we show that the samples generated by the consistency model
are close to the target distribution in Wasserstein distance; when the target
distribution satisfies some smoothness assumption, we show that with an
additional perturbation step for smoothing, the generated samples are close to
the target distribution in total variation distance. We provide two case
studies with commonly chosen forward processes to demonstrate the benefit of
multistep sampling.

</details>

### [40] [Transformers for Learning on Noisy and Task-Level Manifolds: Approximation and Generalization Insights](https://arxiv.org/abs/2505.03205)
*Zhaiming Shen,Alex Havrilla,Rongjie Lai,Alexander Cloninger,Wenjing Liao*

Main category: cs.LG

TLDR: 论文分析了Transformer在带噪声的流形数据上的回归任务性能，证明了其能利用低维结构学习，即使输入数据受高维噪声干扰。


<details>
  <summary>Details</summary>
Motivation: 现实数据和任务通常具有低维结构，但Transformer的理论基础尚不完善，本文旨在填补这一空白。

Method: 通过分析Transformer在流形邻域内带噪声数据的回归任务，构建了表示基本算术运算的新证明技术。

Result: 证明了Transformer的逼近和泛化误差依赖于流形的内在维度，能有效利用低维结构。

Conclusion: Transformer能高效处理带噪声的低维结构数据，其性能与流形内在维度相关。

Abstract: Transformers serve as the foundational architecture for large language and
video generation models, such as GPT, BERT, SORA and their successors.
Empirical studies have demonstrated that real-world data and learning tasks
exhibit low-dimensional structures, along with some noise or measurement error.
The performance of transformers tends to depend on the intrinsic dimension of
the data/tasks, though theoretical understandings remain largely unexplored for
transformers. This work establishes a theoretical foundation by analyzing the
performance of transformers for regression tasks involving noisy input data on
a manifold. Specifically, the input data are in a tubular neighborhood of a
manifold, while the ground truth function depends on the projection of the
noisy data onto the manifold. We prove approximation and generalization errors
which crucially depend on the intrinsic dimension of the manifold. Our results
demonstrate that transformers can leverage low-complexity structures in
learning task even when the input data are perturbed by high-dimensional noise.
Our novel proof technique constructs representations of basic arithmetic
operations by transformers, which may hold independent interest.

</details>

### [41] [Partial Label Clustering](https://arxiv.org/abs/2505.03207)
*Yutong Xie,Fuchao Yang,Yuheng Jia*

Main category: cs.LG

TLDR: 本文首次研究了部分标签聚类问题，通过利用有限的候选标签提升聚类性能，提出了一种联合模型，结合权重矩阵构建、标签消歧和约束传播，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 部分标签学习（PLL）是一种弱监督学习框架，但如何利用有限的候选标签提升聚类性能尚未被研究。

Method: 1. 基于特征空间关系构建权重矩阵并消歧候选标签；2. 基于消歧结果构建必须链接和不能链接约束；3. 通过对抗性先验促进的双图学习传播约束；4. 将上述步骤整合为联合模型。

Result: 实验表明，该方法优于现有约束聚类方法，且在有限标注样本下优于PLL和半监督PLL方法。

Conclusion: 提出的联合模型通过标签消歧和约束传播显著提升了聚类性能，理论证明了消歧标签矩阵对聚类的促进作用。

Abstract: Partial label learning (PLL) is a significant weakly supervised learning
framework, where each training example corresponds to a set of candidate labels
and only one label is the ground-truth label. For the first time, this paper
investigates the partial label clustering problem, which takes advantage of the
limited available partial labels to improve the clustering performance.
Specifically, we first construct a weight matrix of examples based on their
relationships in the feature space and disambiguate the candidate labels to
estimate the ground-truth label based on the weight matrix. Then, we construct
a set of must-link and cannot-link constraints based on the disambiguation
results. Moreover, we propagate the initial must-link and cannot-link
constraints based on an adversarial prior promoted dual-graph learning
approach. Finally, we integrate weight matrix construction, label
disambiguation, and pairwise constraints propagation into a joint model to
achieve mutual enhancement. We also theoretically prove that a better
disambiguated label matrix can help improve clustering performance.
Comprehensive experiments demonstrate our method realizes superior performance
when comparing with state-of-the-art constrained clustering methods, and
outperforms PLL and semi-supervised PLL methods when only limited samples are
annotated. The code is publicly available at https://github.com/xyt-ml/PLC.

</details>

### [42] [DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning](https://arxiv.org/abs/2505.03209)
*Borui Wang,Kathleen McKeown,Rex Ying*

Main category: cs.LG

TLDR: DYSTIL是一种基于LLM的策略强化学习框架，通过动态生成文本策略提升泛化能力和样本效率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有专家演示强化学习方法泛化能力差、样本效率低且模型解释性不足，DYSTIL旨在解决这些问题。

Method: DYSTIL动态查询LLM生成文本策略，结合优势估计和专家演示，通过策略优化逐步内化策略。

Result: 在Minigrid和BabyAI环境中，DYSTIL平均成功率提升17.75%，样本效率更高。

Conclusion: DYSTIL通过LLM生成的策略显著提升了强化学习的性能和可解释性。

Abstract: Reinforcement learning from expert demonstrations has long remained a
challenging research problem, and existing state-of-the-art methods using
behavioral cloning plus further RL training often suffer from poor
generalization, low sample efficiency, and poor model interpretability.
Inspired by the strong reasoning abilities of large language models (LLMs), we
propose a novel strategy-based reinforcement learning framework integrated with
LLMs called DYnamic STrategy Induction with Llms for reinforcement learning
(DYSTIL) to overcome these limitations. DYSTIL dynamically queries a
strategy-generating LLM to induce textual strategies based on advantage
estimations and expert demonstrations, and gradually internalizes induced
strategies into the RL agent through policy optimization to improve its
performance through boosting policy generalization and enhancing sample
efficiency. It also provides a direct textual channel to observe and interpret
the evolution of the policy's underlying strategies during training. We test
DYSTIL over challenging RL environments from Minigrid and BabyAI, and
empirically demonstrate that DYSTIL significantly outperforms state-of-the-art
baseline methods by 17.75% in average success rate while also enjoying higher
sample efficiency during the learning process.

</details>

### [43] [Joint Resource Management for Energy-efficient UAV-assisted SWIPT-MEC: A Deep Reinforcement Learning Approach](https://arxiv.org/abs/2505.03230)
*Yue Chen,Hui Kang,Jiahui Li,Geng Su,Boxiong Wang,Jiacheng Wang,Cong Liang,Shuang Liang,Dusit Niyato*

Main category: cs.LG

TLDR: 本文提出了一种基于无人机辅助移动边缘计算（MEC）的系统，结合定向天线技术，为地面物联网终端提供计算资源和能量支持，并通过改进的SAC算法优化能量效率和终端电池可持续性。


<details>
  <summary>Details</summary>
Motivation: 解决6G物联网网络中无线信息和能量同时传输（SWIPT）技术在偏远地区和灾害场景中的挑战，尤其是地面基础设施缺失的问题。

Method: 提出了一种无人机辅助的MEC系统，采用定向天线技术，并通过改进的软演员-评论家（SAC）算法解决非凸优化问题。

Result: 仿真结果表明，该方法在多种场景下优于基线方法，实现了高效能量管理和高计算性能，并展示了强大的泛化能力。

Conclusion: 该方法在复杂环境中验证了其有效性，特别是在边界惩罚和充电奖励机制的设计上表现优异。

Abstract: The integration of simultaneous wireless information and power transfer
(SWIPT) technology in 6G Internet of Things (IoT) networks faces significant
challenges in remote areas and disaster scenarios where ground infrastructure
is unavailable. This paper proposes a novel unmanned aerial vehicle
(UAV)-assisted mobile edge computing (MEC) system enhanced by directional
antennas to provide both computational resources and energy support for ground
IoT terminals. However, such systems require multiple trade-off policies to
balance UAV energy consumption, terminal battery levels, and computational
resource allocation under various constraints, including limited UAV battery
capacity, non-linear energy harvesting characteristics, and dynamic task
arrivals. To address these challenges comprehensively, we formulate a
bi-objective optimization problem that simultaneously considers system energy
efficiency and terminal battery sustainability. We then reformulate this
non-convex problem with a hybrid solution space as a Markov decision process
(MDP) and propose an improved soft actor-critic (SAC) algorithm with an action
simplification mechanism to enhance its convergence and generalization
capabilities. Simulation results have demonstrated that our proposed approach
outperforms various baselines in different scenarios, achieving efficient
energy management while maintaining high computational performance.
Furthermore, our method shows strong generalization ability across different
scenarios, particularly in complex environments, validating the effectiveness
of our designed boundary penalty and charging reward mechanisms.

</details>

### [44] [MDPs with a State Sensing Cost](https://arxiv.org/abs/2505.03280)
*Vansh Kapoor,Jayakrishnan Nair*

Main category: cs.LG

TLDR: 论文研究了在状态跟踪成本存在时的序列决策问题，提出了一种扩展的MDP框架，并设计了高效的启发式算法。


<details>
  <summary>Details</summary>
Motivation: 解决在状态跟踪成本存在时，如何平衡最优行动价值和感知成本的决策问题。

Method: 将问题建模为扩展的MDP，限制非感知动作次数，并设计基于策略改进的启发式算法。

Result: 证明了限制策略的次优性界限，算法在实验中表现接近最优。

Conclusion: 提出的方法在计算效率和性能之间取得了平衡，适用于实际应用。

Abstract: In many practical sequential decision-making problems, tracking the state of
the environment incurs a sensing/communication/computation cost. In these
settings, the agent's interaction with its environment includes the additional
component of deciding $\textit{when}$ to sense the state, in a manner that
balances the value associated with optimal (state-specific) actions and the
cost of sensing. We formulate this as an expected discounted cost Markov
Decision Process (MDP), wherein the agent incurs an additional cost for sensing
its next state, but has the option to take actions while remaining 'blind' to
the system state.
  We pose this problem as a classical discounted cost MDP with an expanded
(countably infinite) state space. While computing the optimal policy for this
MDP is intractable in general, we bound the sub-optimality gap associated with
optimal policies in a restricted class, where the number of consecutive
non-sensing (a.k.a., blind) actions is capped. We also design a computationally
efficient heuristic algorithm based on policy improvement, which in practice
performs close to the optimal policy. Finally, we benchmark against the state
of the art via a numerical case study.

</details>

### [45] [Physics-inspired Energy Transition Neural Network for Sequence Learning](https://arxiv.org/abs/2505.03281)
*Zhou Wu,Junyi An,Baile Xu,Furao Shen,Jian Zhao*

Main category: cs.LG

TLDR: 本文提出了一种名为PETNN的新型循环神经网络结构，受物理能量转换模型启发，能够有效捕捉长程依赖关系，并在多个序列任务中优于Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在序列建模中表现出色，但其长程依赖捕捉能力主要依赖于复杂的配对建模过程，而非对序列语义的固有归纳偏置。本文旨在重新评估纯循环神经网络（RNN）的能力，并探索其长程学习机制。

Method: 受物理能量转换模型的启发，提出了一种名为PETNN的新型循环结构，其记忆机制能够有效存储长程依赖信息。

Result: 实验结果表明，PETNN在多个序列任务中优于基于Transformer的方法，且由于其循环特性，计算复杂度显著降低。

Conclusion: PETNN为循环神经网络提供了一种优化的基础架构，并展示了在Transformer主导的领域中开发有效RNN的潜力。

Abstract: Recently, the superior performance of Transformers has made them a more
robust and scalable solution for sequence modeling than traditional recurrent
neural networks (RNNs). However, the effectiveness of Transformer in capturing
long-term dependencies is primarily attributed to their comprehensive
pair-modeling process rather than inherent inductive biases toward sequence
semantics. In this study, we explore the capabilities of pure RNNs and reassess
their long-term learning mechanisms. Inspired by the physics energy transition
models that track energy changes over time, we propose a effective recurrent
structure called the``Physics-inspired Energy Transition Neural Network"
(PETNN). We demonstrate that PETNN's memory mechanism effectively stores
information over long-term dependencies. Experimental results indicate that
PETNN outperforms transformer-based methods across various sequence tasks.
Furthermore, owing to its recurrent nature, PETNN exhibits significantly lower
complexity. Our study presents an optimal foundational recurrent architecture
and highlights the potential for developing effective recurrent neural networks
in fields currently dominated by Transformer.

</details>

### [46] [Unraveling the Rainbow: can value-based methods schedule?](https://arxiv.org/abs/2505.03323)
*Arthur Corrêa,Alexandre Jesus,Cristóvão Silva,Samuel Moniz*

Main category: cs.LG

TLDR: 深度强化学习在组合优化问题中的应用，比较了基于策略和基于价值的方法，发现基于价值的方法在某些复杂问题中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探讨基于价值的深度强化学习算法在组合优化问题中的潜力，挑战基于策略方法的主导地位。

Method: 对深度Q网络及其扩展版本在作业车间调度和柔性作业车间调度问题中进行实证评估。

Result: 基于价值的方法在某些情况下优于广泛使用的近端策略优化算法。

Conclusion: 基于价值的策略在组合优化中值得更多关注，不应被忽视。

Abstract: Recently, deep reinforcement learning has emerged as a promising approach for
solving complex combinatorial optimization problems. Broadly, deep
reinforcement learning methods fall into two categories: policy-based and
value-based. While value-based approaches have achieved notable success in
domains such as the Arcade Learning Environment, the combinatorial optimization
community has predominantly favored policy-based methods, often overlooking the
potential of value-based algorithms. In this work, we conduct a comprehensive
empirical evaluation of value-based algorithms, including the deep q-network
and several of its advanced extensions, within the context of two complex
combinatorial problems: the job-shop and the flexible job-shop scheduling
problems, two fundamental challenges with multiple industrial applications. Our
results challenge the assumption that policy-based methods are inherently
superior for combinatorial optimization. We show that several value-based
approaches can match or even outperform the widely adopted proximal policy
optimization algorithm, suggesting that value-based strategies deserve greater
attention from the combinatorial optimization community. Our code is openly
available at: https://github.com/AJ-Correa/Unraveling-the-Rainbow.

</details>

### [47] [Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://arxiv.org/abs/2505.03335)
*Andrew Zhao,Yiran Wu,Yang Yue,Tong Wu,Quentin Xu,Yang Yue,Matthieu Lin,Shenzhi Wang,Qingyun Wu,Zilong Zheng,Gao Huang*

Main category: cs.LG

TLDR: 论文提出了一种名为Absolute Zero的新RLVR范式，通过自我生成任务和验证答案，无需外部数据，实现了在编码和数学推理任务上的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有RLVR方法依赖人工标注数据的问题，以及未来超级智能系统中人类任务可能限制学习潜力的问题。

Method: 提出Absolute Zero范式，引入AZR系统，通过代码执行器自我生成和验证任务，实现无外部数据的训练。

Result: AZR在编码和数学推理任务上表现优于依赖大量人工标注数据的现有模型。

Conclusion: AZR展示了无外部数据训练的可行性，并适用于不同规模和类型的模型。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown promise in
enhancing the reasoning capabilities of large language models by learning
directly from outcome-based rewards. Recent RLVR works that operate under the
zero setting avoid supervision in labeling the reasoning process, but still
depend on manually curated collections of questions and answers for training.
The scarcity of high-quality, human-produced examples raises concerns about the
long-term scalability of relying on human supervision, a challenge already
evident in the domain of language model pretraining. Furthermore, in a
hypothetical future where AI surpasses human intelligence, tasks provided by
humans may offer limited learning potential for a superintelligent system. To
address these concerns, we propose a new RLVR paradigm called Absolute Zero, in
which a single model learns to propose tasks that maximize its own learning
progress and improves reasoning by solving them, without relying on any
external data. Under this paradigm, we introduce the Absolute Zero Reasoner
(AZR), a system that self-evolves its training curriculum and reasoning ability
by using a code executor to both validate proposed code reasoning tasks and
verify answers, serving as an unified source of verifiable reward to guide
open-ended yet grounded learning. Despite being trained entirely without
external data, AZR achieves overall SOTA performance on coding and mathematical
reasoning tasks, outperforming existing zero-setting models that rely on tens
of thousands of in-domain human-curated examples. Furthermore, we demonstrate
that AZR can be effectively applied across different model scales and is
compatible with various model classes.

</details>

### [48] [Geospatial Mechanistic Interpretability of Large Language Models](https://arxiv.org/abs/2505.03368)
*Stef De Sabbata,Stefano Mizzaro,Kevin Roitero*

Main category: cs.LG

TLDR: 本文提出了一种研究LLMs处理地理信息的新框架，通过空间分析揭示其内部机制，并探讨了其在空间推理中的表现。


<details>
  <summary>Details</summary>
Motivation: LLMs在地理领域的应用日益广泛，但其处理地理信息的内部机制尚不明确，本文旨在填补这一研究空白。

Method: 结合空间分析和机械可解释性方法，利用空间自相关等技术揭示LLMs对地理信息的内部表征。

Result: 实验表明，LLMs对地名特征的处理显示出与地理位置相关的空间模式，揭示了其地理信息处理机制。

Conclusion: 该框架为地理学中基础模型的研究和应用提供了新视角，有助于进一步理解LLMs的空间推理能力。

Abstract: Large Language Models (LLMs) have demonstrated unprecedented capabilities
across various natural language processing tasks. Their ability to process and
generate viable text and code has made them ubiquitous in many fields, while
their deployment as knowledge bases and "reasoning" tools remains an area of
ongoing research. In geography, a growing body of literature has been focusing
on evaluating LLMs' geographical knowledge and their ability to perform spatial
reasoning. However, very little is still known about the internal functioning
of these models, especially about how they process geographical information.
  In this chapter, we establish a novel framework for the study of geospatial
mechanistic interpretability - using spatial analysis to reverse engineer how
LLMs handle geographical information. Our aim is to advance our understanding
of the internal representations that these complex models generate while
processing geographical information - what one might call "how LLMs think about
geographic information" if such phrasing was not an undue anthropomorphism.
  We first outline the use of probing in revealing internal structures within
LLMs. We then introduce the field of mechanistic interpretability, discussing
the superposition hypothesis and the role of sparse autoencoders in
disentangling polysemantic internal representations of LLMs into more
interpretable, monosemantic features. In our experiments, we use spatial
autocorrelation to show how features obtained for placenames display spatial
patterns related to their geographic location and can thus be interpreted
geospatially, providing insights into how these models process geographical
information. We conclude by discussing how our framework can help shape the
study and use of foundation models in geography.

</details>

### [49] [SPAP: Structured Pruning via Alternating Optimization and Penalty Methods](https://arxiv.org/abs/2505.03373)
*Hanyu Hu,Xiaoming Yuan*

Main category: cs.LG

TLDR: SPAP是一种基于优化理论的高效结构化剪枝框架，用于减少大型语言模型的计算和内存需求，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的计算和内存需求限制了其部署，现有剪枝方法存在性能下降、依赖启发式指标或微调成本高的问题。

Method: SPAP通过混合整数优化模型和惩罚方法制定剪枝问题，采用交替最小化算法高效更新权重和恢复性能。

Result: 在多个模型上的实验表明，SPAP优于现有方法，实现了线性推理加速（1.29倍，30%稀疏度）和内存减少。

Conclusion: SPAP为LLM剪枝提供了实用且优化驱动的解决方案，同时保持了模型性能。

Abstract: The deployment of large language models (LLMs) is often constrained by their
substantial computational and memory demands. While structured pruning presents
a viable approach by eliminating entire network components, existing methods
suffer from performance degradation, reliance on heuristic metrics, or
expensive finetuning. To address these challenges, we propose SPAP (Structured
Pruning via Alternating Optimization and Penalty Methods), a novel and
efficient structured pruning framework for LLMs grounded in optimization
theory. SPAP formulates the pruning problem through a mixed-integer
optimization model, employs a penalty method that effectively makes pruning
decisions to minimize pruning errors, and introduces an alternating
minimization algorithm tailored to the splittable problem structure for
efficient weight updates and performance recovery. Extensive experiments on
OPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over
state-of-the-art methods, delivering linear inference speedups (1.29$\times$ at
30% sparsity) and proportional memory reductions. Our work offers a practical,
optimization-driven solution for pruning LLMs while preserving model
performance.

</details>

### [50] [Physics-informed neural network estimation of active material properties in time-dependent cardiac biomechanical models](https://arxiv.org/abs/2505.03382)
*Matthias Höfler,Francesco Regazzoni,Stefano Pagani,Elias Karabelas,Christoph Augustin,Gundolf Haase,Gernot Plank,Federica Caforio*

Main category: cs.LG

TLDR: 该论文提出了一种基于物理信息神经网络（PINNs）的方法，用于从医学影像数据中推断心脏生物力学模型中的主动收缩参数，并通过改进的算法实现了高空间分辨率的主动应力场重建。


<details>
  <summary>Details</summary>
Motivation: 准确评估心脏生物力学中的主动应力参数对理解心肌功能至关重要，但在临床环境中仅依靠医学影像数据（如位移和应变）仍具有挑战性。

Method: 通过参数化状态和参数场为两个神经网络，并构建能量最小化问题来优化网络参数，结合自适应加权、正则化策略、傅里叶特征和特定网络架构，改进了PINN算法。

Result: 在噪声存在的情况下，成功重建了高空间分辨率的主动应力场，并应用于心肌组织异质性和纤维化疤痕的检测。

Conclusion: 该方法为心脏纤维化相关疾病的诊断和治疗规划提供了新的途径。

Abstract: Active stress models in cardiac biomechanics account for the mechanical
deformation caused by muscle activity, thus providing a link between the
electrophysiological and mechanical properties of the tissue. The accurate
assessment of active stress parameters is fundamental for a precise
understanding of myocardial function but remains difficult to achieve in a
clinical setting, especially when only displacement and strain data from
medical imaging modalities are available. This work investigates, through an
in-silico study, the application of physics-informed neural networks (PINNs)
for inferring active contractility parameters in time-dependent cardiac
biomechanical models from these types of imaging data. In particular, by
parametrising the sought state and parameter field with two neural networks,
respectively, and formulating an energy minimisation problem to search for the
optimal network parameters, we are able to reconstruct in various settings
active stress fields in the presence of noise and with a high spatial
resolution. To this end, we also advance the vanilla PINN learning algorithm
with the use of adaptive weighting schemes, ad-hoc regularisation strategies,
Fourier features, and suitable network architectures. In addition, we
thoroughly analyse the influence of the loss weights in the reconstruction of
active stress parameters. Finally, we apply the method to the characterisation
of tissue inhomogeneities and detection of fibrotic scars in myocardial tissue.
This approach opens a new pathway to significantly improve the diagnosis,
treatment planning, and management of heart conditions associated with cardiac
fibrosis.

</details>

### [51] [Improving Omics-Based Classification: The Role of Feature Selection and Synthetic Data Generation](https://arxiv.org/abs/2505.03387)
*Diego Perazzolo,Pietro Fanton,Ilaria Barison,Marny Fedrigo,Annalisa Angelini,Chiara Castellani,Enrico Grisan*

Main category: cs.LG

TLDR: 该研究提出了一种结合特征选择与数据增强的机器学习分类框架，旨在提高小样本组学数据分类的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决高维组学数据样本量小、模型可解释性差的问题，提升分类模型的透明度和可靠性。

Method: 采用特征选择与数据增强技术，结合公开数据集（E MTAB 8026）进行六种二分类场景的引导分析。

Result: 模型在小数据集上表现良好，且在大测试集上保持稳定，证明了合成数据对泛化能力的积极影响。

Conclusion: 研究强调了准确性与特征选择的平衡，并展示了数据增强在样本稀缺场景中的有效性。

Abstract: Given the increasing complexity of omics datasets, a key challenge is not
only improving classification performance but also enhancing the transparency
and reliability of model decisions. Effective model performance and feature
selection are fundamental for explainability and reliability. In many cases,
high dimensional omics datasets suffer from limited number of samples due to
clinical constraints, patient conditions, phenotypes rarity and others
conditions. Current omics based classification models often suffer from narrow
interpretability, making it difficult to discern meaningful insights where
trust and reproducibility are critical. This study presents a machine learning
based classification framework that integrates feature selection with data
augmentation techniques to achieve high standard classification accuracy while
ensuring better interpretability. Using the publicly available dataset (E MTAB
8026), we explore a bootstrap analysis in six binary classification scenarios
to evaluate the proposed model's behaviour. We show that the proposed pipeline
yields cross validated perfomance on small dataset that is conserved when the
trained classifier is applied to a larger test set. Our findings emphasize the
fundamental balance between accuracy and feature selection, highlighting the
positive effect of introducing synthetic data for better generalization, even
in scenarios with very limited samples availability.

</details>

### [52] [Concept Factorization via Self-Representation and Adaptive Graph Structure Learning](https://arxiv.org/abs/2505.03390)
*Zhengqin Yang,Di Wu,Jia Chen,Xin Luo*

Main category: cs.LG

TLDR: 提出了一种基于自表示和自适应图结构学习的概念分解模型（CFSRAG），通过动态学习数据内部几何结构，提升了聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于概念分解的模型依赖初始图结构构建，限制了聚类性能，因此需要一种自适应学习图结构的方法。

Method: CFSRAG通过自表示方法学习数据间的亲和关系，并利用亲和矩阵实现动态图正则化约束。

Result: 在四个真实数据集上的实验表明，CFSRAG优于其他先进模型。

Conclusion: CFSRAG通过自适应图结构学习，显著提升了聚类性能。

Abstract: Concept Factorization (CF) models have attracted widespread attention due to
their excellent performance in data clustering. In recent years, many variant
models based on CF have achieved great success in clustering by taking into
account the internal geometric manifold structure of the dataset and using
graph regularization techniques. However, their clustering performance depends
greatly on the construction of the initial graph structure. In order to enable
adaptive learning of the graph structure of the data, we propose a Concept
Factorization Based on Self-Representation and Adaptive Graph Structure
Learning (CFSRAG) Model. CFSRAG learns the affinity relationship between data
through a self-representation method, and uses the learned affinity matrix to
implement dynamic graph regularization constraints, thereby ensuring dynamic
learning of the internal geometric structure of the data. Finally, we give the
CFSRAG update rule and convergence analysis, and conduct comparative
experiments on four real datasets. The results show that our model outperforms
other state-of-the-art models.

</details>

### [53] [Automatic Calibration for Membership Inference Attack on Large Language Models](https://arxiv.org/abs/2505.03392)
*Saleh Zare Zade,Yao Qiang,Xiangyu Zhou,Hui Zhu,Mohammad Amin Roshani,Prashant Khanduri,Dongxiao Zhu*

Main category: cs.LG

TLDR: 论文提出了一种名为ACMIA的新框架，通过可调温度校准输出概率，有效减少误判率，提升了成员推理攻击的可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击方法存在高误判率或依赖额外参考模型的局限性，限制了实用性。

Method: 提出ACMIA框架，利用可调温度校准输出概率，并通过三种配置适应不同模型访问级别，增大成员与非成员间的概率差距。

Result: 在多个开源大语言模型上的实验表明，ACMIA在三个基准测试中优于现有方法，具有高效性、鲁棒性和泛化性。

Conclusion: ACMIA显著提升了成员推理攻击的性能，为相关研究提供了实用工具。

Abstract: Membership Inference Attacks (MIAs) have recently been employed to determine
whether a specific text was part of the pre-training data of Large Language
Models (LLMs). However, existing methods often misinfer non-members as members,
leading to a high false positive rate, or depend on additional reference models
for probability calibration, which limits their practicality. To overcome these
challenges, we introduce a novel framework called Automatic Calibration
Membership Inference Attack (ACMIA), which utilizes a tunable temperature to
calibrate output probabilities effectively. This approach is inspired by our
theoretical insights into maximum likelihood estimation during the pre-training
of LLMs. We introduce ACMIA in three configurations designed to accommodate
different levels of model access and increase the probability gap between
members and non-members, improving the reliability and robustness of membership
inference. Extensive experiments on various open-source LLMs demonstrate that
our proposed attack is highly effective, robust, and generalizable, surpassing
state-of-the-art baselines across three widely used benchmarks. Our code is
available at:
\href{https://github.com/Salehzz/ACMIA}{\textcolor{blue}{Github}}.

</details>

### [54] [Prediction Models That Learn to Avoid Missing Values](https://arxiv.org/abs/2505.03393)
*Lena Stempfle,Anton Matsson,Newton Mwai,Fredrik D. Johansson*

Main category: cs.LG

TLDR: 提出了一种避免缺失值的机器学习框架（MA），通过特定正则化减少对缺失特征的依赖，同时保持预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 测试时处理缺失值对模型的准确性和可解释性提出挑战，现有方法可能引入偏差或复杂性。

Method: 为决策树、树集成和稀疏线性模型设计了MA学习算法，通过正则化减少对缺失特征的依赖。

Result: 实验表明，MA方法在减少对缺失值依赖的同时，保持了与未正则化模型相当的预测性能。

Conclusion: MA框架为实践者提供了一种在测试时处理缺失值的同时保持模型可解释性的有效工具。

Abstract: Handling missing values at test time is challenging for machine learning
models, especially when aiming for both high accuracy and interpretability.
Established approaches often add bias through imputation or excessive model
complexity via missingness indicators. Moreover, either method can obscure
interpretability, making it harder to understand how the model utilizes the
observed variables in predictions. We propose missingness-avoiding (MA) machine
learning, a general framework for training models to rarely require the values
of missing (or imputed) features at test time. We create tailored MA learning
algorithms for decision trees, tree ensembles, and sparse linear models by
incorporating classifier-specific regularization terms in their learning
objectives. The tree-based models leverage contextual missingness by reducing
reliance on missing values based on the observed context. Experiments on
real-world datasets demonstrate that MA-DT, MA-LASSO, MA-RF, and MA-GBT
effectively reduce the reliance on features with missing values while
maintaining predictive performance competitive with their unregularized
counterparts. This shows that our framework gives practitioners a powerful tool
to maintain interpretability in predictions with test-time missing values.

</details>

### [55] [Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey](https://arxiv.org/abs/2505.03418)
*Da Zheng,Lun Du,Junwei Su,Yuchen Tian,Yuqi Zhu,Jintian Zhang,Lanning Wei,Ningyu Zhang,Huajun Chen*

Main category: cs.LG

TLDR: 该论文探讨了大语言模型（LLMs）在复杂问题解决中的能力与局限，重点研究了多步推理、领域知识整合和结果验证等技术，并讨论了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的发展，LLMs成为解决复杂问题的强大工具，但其在现实问题中的应用仍面临多步推理、领域知识整合和结果验证等挑战。

Method: 论文通过调查LLMs的能力和局限，分析了包括思维链推理（CoT）、知识增强和多种验证技术在内的技术。

Result: 研究发现LLMs在软件工程、数学推理、数据分析和科学研究等领域存在特定挑战，并揭示了当前解决方案的根本局限。

Conclusion: 论文指出了未来LLM在复杂问题解决中的发展方向，强调需在多步推理、领域知识整合和结果验证方面进一步研究。

Abstract: Problem-solving has been a fundamental driver of human progress in numerous
domains. With advancements in artificial intelligence, Large Language Models
(LLMs) have emerged as powerful tools capable of tackling complex problems
across diverse domains. Unlike traditional computational systems, LLMs combine
raw computational power with an approximation of human reasoning, allowing them
to generate solutions, make inferences, and even leverage external
computational tools. However, applying LLMs to real-world problem-solving
presents significant challenges, including multi-step reasoning, domain
knowledge integration, and result verification. This survey explores the
capabilities and limitations of LLMs in complex problem-solving, examining
techniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,
and various LLM-based and tool-based verification techniques. Additionally, we
highlight domain-specific challenges in various domains, such as software
engineering, mathematical reasoning and proving, data analysis and modeling,
and scientific research. The paper further discusses the fundamental
limitations of the current LLM solutions and the future directions of LLM-based
complex problems solving from the perspective of multi-step reasoning, domain
knowledge integration and result verification.

</details>

### [56] [Framework GNN-AID: Graph Neural Network Analysis Interpretation and Defense](https://arxiv.org/abs/2505.03424)
*Kirill Lukyanov,Mikhail Drobyshevskiy,Georgii Sazonov,Mikhail Soloviov,Ilya Makarov*

Main category: cs.LG

TLDR: GNN-AID是一个开源框架，专注于图数据的可信AI，结合了可解释性和鲁棒性，支持攻击、防御和解释方法。


<details>
  <summary>Details</summary>
Motivation: 现有工具多忽视图数据，且很少将可解释性与鲁棒性结合，GNN-AID填补了这一空白。

Method: 基于PyTorch-Geometric构建，提供预加载数据集、模型和自定义接口，支持Web界面和MLOps技术。

Result: GNN-AID为开发者和研究者提供灵活工具，支持快速实验和高级研究，同时揭示了防御策略间的冲突。

Conclusion: GNN-AID是一个多功能框架，推动了图数据可信AI的发展，并开源供社区使用。

Abstract: The growing need for Trusted AI (TAI) highlights the importance of
interpretability and robustness in machine learning models. However, many
existing tools overlook graph data and rarely combine these two aspects into a
single solution. Graph Neural Networks (GNNs) have become a popular approach,
achieving top results across various tasks. We introduce GNN-AID (Graph Neural
Network Analysis, Interpretation, and Defense), an open-source framework
designed for graph data to address this gap. Built as a Python library, GNN-AID
supports advanced trust methods and architectural layers, allowing users to
analyze graph datasets and GNN behavior using attacks, defenses, and
interpretability methods.
  GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models,
and support for any GNNs through customizable interfaces. It also includes a
web interface with tools for graph visualization and no-code features like an
interactive model builder, simplifying the exploration and analysis of GNNs.
The framework also supports MLOps techniques, ensuring reproducibility and
result versioning to track and revisit analyses efficiently.
  GNN-AID is a flexible tool for developers and researchers. It helps
developers create, analyze, and customize graph models, while also providing
access to prebuilt datasets and models for quick experimentation. Researchers
can use the framework to explore advanced topics on the relationship between
interpretability and robustness, test defense strategies, and combine methods
to protect against different types of attacks.
  We also show how defenses against evasion and poisoning attacks can conflict
when applied to graph data, highlighting the complex connections between
defense strategies.
  GNN-AID is available at
\href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}

</details>

### [57] [Wasserstein Convergence of Score-based Generative Models under Semiconvexity and Discontinuous Gradients](https://arxiv.org/abs/2505.03432)
*Stefano Bruno,Sotirios Sabanis*

Main category: cs.LG

TLDR: 该论文提出了针对半凸分布的非渐近Wasserstein-2收敛保证，放宽了传统分析中对数据分布的强正则性假设，为SGMs提供了更广泛的理论支持。


<details>
  <summary>Details</summary>
Motivation: 尽管SGMs在多种领域表现出色，但现有理论分析通常依赖强正则性条件（如平滑性或严格对数凹性），这些条件在实践中很少满足。本文旨在填补这一理论与实践的鸿沟。

Method: 通过利用半凸性而不要求潜在函数的平滑性假设（如可微性），建立了非渐近Wasserstein-2收敛保证。

Result: 结果表明，SGMs在数据维度$d$上具有$O(\sqrt{d})$的最优依赖性，收敛速率为一阶，适用于多种实际相关分布。

Conclusion: 本文显著扩展了SGMs的理论基础，为非平滑、复杂数据场景下的实证成功提供了严格的理论保证。

Abstract: Score-based Generative Models (SGMs) approximate a data distribution by
perturbing it with Gaussian noise and subsequently denoising it via a learned
reverse diffusion process. These models excel at modeling complex data
distributions and generating diverse samples, achieving state-of-the-art
performance across domains such as computer vision, audio generation,
reinforcement learning, and computational biology. Despite their empirical
success, existing Wasserstein-2 convergence analysis typically assume strong
regularity conditions-such as smoothness or strict log-concavity of the data
distribution-that are rarely satisfied in practice. In this work, we establish
the first non-asymptotic Wasserstein-2 convergence guarantees for SGMs
targeting semiconvex distributions with potentially discontinuous gradients.
Our upper bounds are explicit and sharp in key parameters, achieving optimal
dependence of $O(\sqrt{d})$ on the data dimension $d$ and convergence rate of
order one. The framework accommodates a wide class of practically relevant
distributions, including symmetric modified half-normal distributions, Gaussian
mixtures, double-well potentials, and elastic net potentials. By leveraging
semiconvexity without requiring smoothness assumptions on the potential such as
differentiability, our results substantially broaden the theoretical
foundations of SGMs, bridging the gap between empirical success and rigorous
guarantees in non-smooth, complex data regimes.

</details>

### [58] [A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM)](https://arxiv.org/abs/2505.03490)
*Faiz Taleb,Ivan Gazeau,Maryline Laurent*

Main category: cs.LG

TLDR: 论文提出了一种基于参考模型的LBRM算法，用于检测时间序列插值模型中的记忆化现象，显著提高了成员推断攻击的准确性。


<details>
  <summary>Details</summary>
Motivation: 生成模型可能无意中记忆训练数据，带来隐私风险，本文旨在解决时间序列插值模型中的记忆化问题。

Method: 采用LBRM算法，利用参考模型区分训练和测试数据，提高成员推断攻击的准确性。

Result: 未微调时AUROC提升约40%，微调后提升约60%，验证了LBRM在两种时间序列插值架构中的有效性。

Conclusion: LBRM方法显著提升了检测准确性，有效应对时间序列插值模型的隐私风险。

Abstract: Generative models can unintentionally memorize training data, posing
significant privacy risks. This paper addresses the memorization phenomenon in
time series imputation models, introducing the Loss-Based with Reference Model
(LBRM) algorithm. The LBRM method leverages a reference model to enhance the
accuracy of membership inference attacks, distinguishing between training and
test data. Our contributions are twofold: first, we propose an innovative
method to effectively extract and identify memorized training data,
significantly improving detection accuracy. On average, without fine-tuning,
the AUROC improved by approximately 40\%. With fine-tuning, the AUROC increased
by approximately 60\%. Second, we validate our approach through membership
inference attacks on two types of architectures designed for time series
imputation, demonstrating the robustness and versatility of the LBRM approach
in different contexts. These results highlight the significant enhancement in
detection accuracy provided by the LBRM approach, addressing privacy risks in
time series imputation models.

</details>

### [59] [AnomalyMatch: Discovering Rare Objects of Interest with Semi-supervised and Active Learning](https://arxiv.org/abs/2505.03509)
*Pablo Gómez,David O'Ryan*

Main category: cs.LG

TLDR: AnomalyMatch是一个结合半监督FixMatch算法和主动学习的异常检测框架，适用于标签稀缺的大规模数据集，在天文学和计算机视觉领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决大规模数据集中异常检测的标签稀缺问题，特别是在天文学和计算机视觉领域。

Method: 将异常检测视为半监督二分类问题，结合FixMatch算法和主动学习，利用有限标签和大量未标签数据迭代优化模型。

Result: 在GalaxyMNIST和miniImageNet数据集上表现优异，AUROC分别达到0.86和0.95，AUPRC为0.71和0.77，高效处理大规模数据。

Conclusion: AnomalyMatch在标签稀缺的大规模数据集中表现出高效性和可扩展性，为科学异常发现提供了实用工具。

Abstract: Anomaly detection in large datasets is essential in fields such as astronomy
and computer vision; however, supervised methods typically require extensive
anomaly labelling, which is often impractical. We present AnomalyMatch, an
anomaly detection framework combining the semi-supervised FixMatch algorithm
using EfficientNet classifiers with active learning. By treating anomaly
detection as a semi-supervised binary classification problem, we efficiently
utilise limited labelled and abundant unlabelled images. We allow iterative
model refinement in a user interface for expert verification of high-confidence
anomalies and correction of false positives. Built for astronomical data,
AnomalyMatch generalises readily to other domains facing similar data
challenges. Evaluations on the GalaxyMNIST astronomical dataset and the
miniImageNet natural-image benchmark under severe class imbalance (1% anomalies
for miniImageNet) display strong performance: starting from five to ten
labelled anomalies and after three active learning cycles, we achieve an
average AUROC of 0.95 (miniImageNet) and 0.86 (GalaxyMNIST), with respective
AUPRC of 0.77 and 0.71. After active learning cycles, anomalies are ranked with
71% (miniImageNet) to 93% precision in the 1% of the highest-ranked images.
AnomalyMatch is tailored for large-scale applications, efficiently processing
predictions for 100 million images within three days on a single GPU.
Integrated into ESAs Datalabs platform, AnomalyMatch facilitates targeted
discovery of scientifically valuable anomalies in vast astronomical datasets.
Our results underscore the exceptional utility and scalability of this approach
for anomaly discovery, highlighting the value of specialised approaches for
domains characterised by severe label scarcity.

</details>

### [60] [Uncovering the Limitations of Model Inversion Evaluation: Benchmarks and Connection to Type-I Adversarial Attacks](https://arxiv.org/abs/2505.03519)
*Sy-Tuyen Ho,Koh Jun Hao,Ngoc-Bao Nguyen,Alexander Binder,Ngai-Man Cheung*

Main category: cs.LG

TLDR: 该论文首次深入研究了模型反演（MI）攻击的评估框架，发现其存在大量假阳性问题，导致此前报告的MI攻击成功率被高估。作者构建了首个全面的人工标注数据集，揭示了评估框架的局限性，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示当前MI攻击评估框架的局限性，特别是假阳性问题，以更准确地评估MI攻击的实际隐私泄露风险。

Method: 方法包括构建人工标注数据集、分析评估框架的准确性、设计控制实验以探究假阳性原因，并提出改进建议。

Result: 结果显示评估框架存在显著假阳性，导致MI攻击的成功率被高估，实际隐私泄露低于此前报告。

Conclusion: 结论强调当前评估框架的局限性，建议将人工评估作为主要方法，并呼吁开发更可靠的自动评估框架。

Abstract: Model Inversion (MI) attacks aim to reconstruct information of private
training data by exploiting access to machine learning models. The most common
evaluation framework for MI attacks/defenses relies on an evaluation model that
has been utilized to assess progress across almost all MI attacks and defenses
proposed in recent years. In this paper, for the first time, we present an
in-depth study of MI evaluation. Firstly, we construct the first comprehensive
human-annotated dataset of MI attack samples, based on 28 setups of different
MI attacks, defenses, private and public datasets. Secondly, using our dataset,
we examine the accuracy of the MI evaluation framework and reveal that it
suffers from a significant number of false positives. These findings raise
questions about the previously reported success rates of SOTA MI attacks.
Thirdly, we analyze the causes of these false positives, design controlled
experiments, and discover the surprising effect of Type I adversarial features
on MI evaluation, as well as adversarial transferability, highlighting a
relationship between two previously distinct research areas. Our findings
suggest that the performance of SOTA MI attacks has been overestimated, with
the actual privacy leakage being significantly less than previously reported.
In conclusion, we highlight critical limitations in the widely used MI
evaluation framework and present our methods to mitigate false positive rates.
We remark that prior research has shown that Type I adversarial attacks are
very challenging, with no existing solution. Therefore, we urge to consider
human evaluation as a primary MI evaluation framework rather than merely a
supplement as in previous MI research. We also encourage further work on
developing more robust and reliable automatic evaluation frameworks.

</details>

### [61] [Causal Intervention Framework for Variational Auto Encoder Mechanistic Interpretability](https://arxiv.org/abs/2505.03530)
*Dip Roy*

Main category: cs.LG

TLDR: 本文提出了一种用于变分自编码器（VAE）机制可解释性的因果干预框架，通过识别和分析电路模体，量化了VAE组件的可解释性。


<details>
  <summary>Details</summary>
Motivation: 理解生成模型（如VAE）的内部机制仍具挑战性，而现有研究主要集中在判别模型上。

Method: 开发了多层次的干预技术（输入操作、潜在空间扰动、激活修补和因果中介分析），并应用于合成数据集和标准解耦基准。

Result: 实验表明，FactorVAE在解耦分数（0.084）和效应强度（均值4.59）上优于标准VAE（0.064, 3.99）和Beta-VAE（0.051, 3.43）。

Conclusion: 该框架提升了对生成模型的机制理解，并为设计更透明和可控的VAE架构提供了工具。

Abstract: Mechanistic interpretability of deep learning models has emerged as a crucial
research direction for understanding the functioning of neural networks. While
significant progress has been made in interpreting discriminative models like
transformers, understanding generative models such as Variational Autoencoders
(VAEs) remains challenging. This paper introduces a comprehensive causal
intervention framework for mechanistic interpretability of VAEs. We develop
techniques to identify and analyze "circuit motifs" in VAEs, examining how
semantic factors are encoded, processed, and disentangled through the network
layers. Our approach uses targeted interventions at different levels: input
manipulations, latent space perturbations, activation patching, and causal
mediation analysis. We apply our framework to both synthetic datasets with
known causal relationships and standard disentanglement benchmarks. Results
show that our interventions can successfully isolate functional circuits, map
computational graphs to causal graphs of semantic factors, and distinguish
between polysemantic and monosemantic units. Furthermore, we introduce metrics
for causal effect strength, intervention specificity, and circuit modularity
that quantify the interpretability of VAE components. Experimental results
demonstrate clear differences between VAE variants, with FactorVAE achieving
higher disentanglement scores (0.084) and effect strengths (mean 4.59) compared
to standard VAE (0.064, 3.99) and Beta-VAE (0.051, 3.43). Our framework
advances the mechanistic understanding of generative models and provides tools
for more transparent and controllable VAE architectures.

</details>

### [62] [Small-Scale-Fading-Aware Resource Allocation in Wireless Federated Learning](https://arxiv.org/abs/2505.03533)
*Jiacheng Wang,Le Liang,Hao Ye,Chongtao Guo,Shi Jin*

Main category: cs.LG

TLDR: 提出了一种基于多智能体强化学习（MARL）的资源分配策略，优化联邦学习（FL）在无线网络中的训练性能。


<details>
  <summary>Details</summary>
Motivation: 现有资源分配策略忽略了快速信道波动对FL性能的影响，导致训练效果下降。

Method: 利用QMIX算法解决Dec-POMDP问题，动态分配频谱和功率资源。

Result: 实验表明，该策略显著优于基线方法，尤其在统计异构性较强时。

Conclusion: 小尺度衰落动态的考虑对优化FL性能至关重要，MARL框架提升了方案的实用性和扩展性。

Abstract: Judicious resource allocation can effectively enhance federated learning (FL)
training performance in wireless networks by addressing both system and
statistical heterogeneity. However, existing strategies typically rely on block
fading assumptions, which overlooks rapid channel fluctuations within each
round of FL gradient uploading, leading to a degradation in FL training
performance. Therefore, this paper proposes a small-scale-fading-aware resource
allocation strategy using a multi-agent reinforcement learning (MARL)
framework. Specifically, we establish a one-step convergence bound of the FL
algorithm and formulate the resource allocation problem as a decentralized
partially observable Markov decision process (Dec-POMDP), which is subsequently
solved using the QMIX algorithm. In our framework, each client serves as an
agent that dynamically determines spectrum and power allocations within each
coherence time slot, based on local observations and a reward derived from the
convergence analysis. The MARL setting reduces the dimensionality of the action
space and facilitates decentralized decision-making, enhancing the scalability
and practicality of the solution. Experimental results demonstrate that our
QMIX-based resource allocation strategy significantly outperforms baseline
methods across various degrees of statistical heterogeneity. Additionally,
ablation studies validate the critical importance of incorporating small-scale
fading dynamics, highlighting its role in optimizing FL performance.

</details>

### [63] [Efficient Training of Physics-enhanced Neural ODEs via Direct Collocation and Nonlinear Programming](https://arxiv.org/abs/2505.03552)
*Linus Langenkamp,Philip Hannebohm,Bernhard Bachmann*

Main category: cs.LG

TLDR: 提出了一种通过动态优化问题训练物理增强神经ODE（PeNODEs）的新方法，解决了传统ODE求解器在稳定性、运行时间和精度上的限制。


<details>
  <summary>Details</summary>
Motivation: 传统ODE求解器训练神经ODE时存在稳定性、运行时间和精度问题，需要更高效的训练方法。

Method: 使用高阶隐式Runge-Kutta方法和翻转Legendre-Gauss-Radau点离散化模型，形成大规模非线性规划问题，并通过Ipopt等求解器高效求解。

Result: 在Quarter Vehicle Model和Van-der-Pol振荡器上的实验表明，该方法在精度、速度和泛化能力上优于其他训练技术。

Conclusion: 该方法为训练PeNODEs提供了高效解决方案，并计划集成到OpenModelica中以支持神经DAE的训练。

Abstract: We propose a novel approach for training Physics-enhanced Neural ODEs
(PeNODEs) by expressing the training process as a dynamic optimization problem.
The full model, including neural components, is discretized using a high-order
implicit Runge-Kutta method with flipped Legendre-Gauss-Radau points, resulting
in a large-scale nonlinear program (NLP) efficiently solved by state-of-the-art
NLP solvers such as Ipopt. This formulation enables simultaneous optimization
of network parameters and state trajectories, addressing key limitations of ODE
solver-based training in terms of stability, runtime, and accuracy. Extending
on a recent direct collocation-based method for Neural ODEs, we generalize to
PeNODEs, incorporate physical constraints, and present a custom, parallelized,
open-source implementation. Benchmarks on a Quarter Vehicle Model and a
Van-der-Pol oscillator demonstrate superior accuracy, speed, and generalization
with smaller networks compared to other training techniques. We also outline a
planned integration into OpenModelica to enable accessible training of Neural
DAEs.

</details>

### [64] [Rapid AI-based generation of coverage paths for dispensing applications](https://arxiv.org/abs/2505.03560)
*Simon Baeuerle,Ian F. Mendonca,Kristof Van Laerhoven,Ralf Mikut,Andreas Steimer*

Main category: cs.LG

TLDR: 提出了一种基于AI的方法，用于生成热界面材料（TIM）的涂布路径，替代传统高计算量的优化方法。


<details>
  <summary>Details</summary>
Motivation: 传统TIM涂布路径规划依赖专家手动操作或高计算量的优化方法，效率低。

Method: 使用人工神经网络（ANN），输入目标冷却区域，直接输出涂布路径，无需标签数据。

Result: 生成的涂布路径可直接用于自动化制造设备，且无气泡问题。

Conclusion: 该方法可实时预测工艺参数，并可能推广至其他制造过程。

Abstract: Coverage Path Planning of Thermal Interface Materials (TIM) plays a crucial
role in the design of power electronics and electronic control units. Up to
now, this is done manually by experts or by using optimization approaches with
a high computational effort. We propose a novel AI-based approach to generate
dispense paths for TIM and similar dispensing applications. It is a drop-in
replacement for optimization-based approaches. An Artificial Neural Network
(ANN) receives the target cooling area as input and directly outputs the
dispense path. Our proposed setup does not require labels and we show its
feasibility on multiple target areas. The resulting dispense paths can be
directly transferred to automated manufacturing equipment and do not exhibit
air entrapments. The approach of using an ANN to predict process parameters for
a desired target state in real-time could potentially be transferred to other
manufacturing processes.

</details>

### [65] [Ergodic Generative Flows](https://arxiv.org/abs/2505.03561)
*Leo Maxime Brunswic,Mateo Clemente,Rui Heng Yang,Adam Sigal,Amir Rasouli,Yinchuan Li*

Main category: cs.LG

TLDR: 本文提出了一种称为Ergodic Generative Flows（EGFs）的生成流方法，解决了生成流网络（GFNs）在连续设置和模仿学习中的挑战，包括流匹配损失的可处理性和无需单独奖励模型的模仿学习。


<details>
  <summary>Details</summary>
Motivation: 生成流网络（GFNs）在连续设置和模仿学习中面临流匹配损失不可处理、非循环训练测试有限以及需要单独奖励模型等问题。

Method: 提出EGFs方法，利用遍历性构建简单的生成流，并引入KL-weakFM损失用于模仿学习。

Result: 在2D任务和NASA真实数据集上验证了EGFs的有效性，并在2D强化学习实验中展示了FM损失的效果。

Conclusion: EGFs为生成流网络提供了更灵活和可扩展的解决方案，特别是在连续设置和模仿学习中。

Abstract: Generative Flow Networks (GFNs) were initially introduced on directed acyclic
graphs to sample from an unnormalized distribution density. Recent works have
extended the theoretical framework for generative methods allowing more
flexibility and enhancing application range. However, many challenges remain in
training GFNs in continuous settings and for imitation learning (IL), including
intractability of flow-matching loss, limited tests of non-acyclic training,
and the need for a separate reward model in imitation learning. The present
work proposes a family of generative flows called Ergodic Generative Flows
(EGFs) which are used to address the aforementioned issues. First, we leverage
ergodicity to build simple generative flows with finitely many globally defined
transformations (diffeomorphisms) with universality guarantees and tractable
flow-matching loss (FM loss). Second, we introduce a new loss involving
cross-entropy coupled to weak flow-matching control, coined KL-weakFM loss. It
is designed for IL training without a separate reward model. We evaluate
IL-EGFs on toy 2D tasks and real-world datasets from NASA on the sphere, using
the KL-weakFM loss. Additionally, we conduct toy 2D reinforcement learning
experiments with a target reward, using the FM loss.

</details>

### [66] [Anant-Net: Breaking the Curse of Dimensionality with Scalable and Interpretable Neural Surrogate for High-Dimensional PDEs](https://arxiv.org/abs/2505.03595)
*Sidharth S. Menon,Ameya D. Jagtap*

Main category: cs.LG

TLDR: Anant-Net是一种高效的神经代理模型，用于解决高维偏微分方程（PDEs），克服了传统方法在高维计算中的困难，并在300维问题上表现出高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 高维PDEs在科学和工程中广泛存在，但传统数值方法因维度灾难而难以处理。Anant-Net旨在解决这一挑战。

Method: Anant-Net结合高维边界条件，最小化PDE残差，并集成Kolmogorov-Arnold网络以提高可解释性。

Result: Anant-Net在Poisson、Sine-Gordon和Allen-Cahn等高维方程上表现出高准确性和鲁棒性，300维问题在单GPU上几小时内解决。

Conclusion: Anant-Net是一种准确、可解释且可扩展的框架，高效解决高维PDEs。

Abstract: High-dimensional partial differential equations (PDEs) arise in diverse
scientific and engineering applications but remain computationally intractable
due to the curse of dimensionality. Traditional numerical methods struggle with
the exponential growth in computational complexity, particularly on hypercubic
domains, where the number of required collocation points increases rapidly with
dimensionality. Here, we introduce Anant-Net, an efficient neural surrogate
that overcomes this challenge, enabling the solution of PDEs in high
dimensions. Unlike hyperspheres, where the internal volume diminishes as
dimensionality increases, hypercubes retain or expand their volume (for unit or
larger length), making high-dimensional computations significantly more
demanding. Anant-Net efficiently incorporates high-dimensional boundary
conditions and minimizes the PDE residual at high-dimensional collocation
points. To enhance interpretability, we integrate Kolmogorov-Arnold networks
into the Anant-Net architecture. We benchmark Anant-Net's performance on
several linear and nonlinear high-dimensional equations, including the Poisson,
Sine-Gordon, and Allen-Cahn equations, demonstrating high accuracy and
robustness across randomly sampled test points from high-dimensional space.
Importantly, Anant-Net achieves these results with remarkable efficiency,
solving 300-dimensional problems on a single GPU within a few hours. We also
compare Anant-Net's results for accuracy and runtime with other
state-of-the-art methods. Our findings establish Anant-Net as an accurate,
interpretable, and scalable framework for efficiently solving high-dimensional
PDEs.

</details>

### [67] [Understand the Effect of Importance Weighting in Deep Learning on Dataset Shift](https://arxiv.org/abs/2505.03617)
*Thien Nhan Vo,Thanh Xuan Truong*

Main category: cs.LG

TLDR: 研究了重要性加权在深度神经网络中对标签偏移和协变量偏移的效果，发现其实际效用有限。


<details>
  <summary>Details</summary>
Motivation: 探讨重要性加权在分布偏移（标签偏移和协变量偏移）场景下的实际效果。

Method: 在合成2D数据（线性可分和月亮形）和CIFAR-10数据集上，使用逻辑回归和MLP进行实验，结合L2正则化和dropout。

Result: 加权在训练初期影响决策边界，但效果随优化减弱；L2正则化有助于保持加权效果，而协变量偏移实验中加权无明显性能提升。

Conclusion: 重要性加权在现实分布偏移场景中的实用性存疑。

Abstract: We evaluate the effectiveness of importance weighting in deep neural networks
under label shift and covariate shift. On synthetic 2D data (linearly separable
and moon-shaped) using logistic regression and MLPs, we observe that weighting
strongly affects decision boundaries early in training but fades with prolonged
optimization. On CIFAR-10 with various class imbalances, only L2 regularization
(not dropout) helps preserve weighting effects. In a covariate-shift
experiment, importance weighting yields no significant performance gain,
highlighting challenges on complex data. Our results call into question the
practical utility of importance weighting for real-world distribution shifts.

</details>

### [68] [ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders](https://arxiv.org/abs/2505.03646)
*Chethan Krishnamurthy Ramanaik,Arjun Roy,Eirini Ntoutsi*

Main category: cs.LG

TLDR: 论文提出了一种基于层条件化的对抗优化目标，用于提升深度自编码器的对抗鲁棒性评估，并通过实验验证其优于现有方法。同时，提出了一种防御插件以对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 深度自编码器在关键应用中的广泛使用与其对抗鲁棒性研究的不足形成对比，现有评估框架未能充分挖掘其脆弱性。

Method: 提出了一种新的层条件化对抗优化目标，通过增强损失梯度信息传播，引导对抗映射至局部Lipschitz边界区域。

Result: 实验表明，该方法在通用和样本特定场景下均优于现有方法，生成了更强的攻击。

Conclusion: 论文不仅提升了对抗攻击的效果，还提出了防御插件，为自编码器的对抗鲁棒性研究提供了新思路。

Abstract: Despite the extensive use of deep autoencoders (AEs) in critical
applications, their adversarial robustness remains relatively underexplored
compared to classification models. AE robustness is characterized by the
Lipschitz bounds of its components. Existing robustness evaluation frameworks
based on white-box attacks do not fully exploit the vulnerabilities of
intermediate ill-conditioned layers in AEs. In the context of optimizing
imperceptible norm-bounded additive perturbations to maximize output damage,
existing methods struggle to effectively propagate adversarial loss gradients
throughout the network, often converging to less effective perturbations. To
address this, we propose a novel layer-conditioning-based adversarial
optimization objective that effectively guides the adversarial map toward
regions of local Lipschitz bounds by enhancing loss gradient information
propagation during attack optimization. We demonstrate through extensive
experiments on state-of-the-art AEs that our adversarial objective results in
stronger attacks, outperforming existing methods in both universal and
sample-specific scenarios. As a defense method against this attack, we
introduce an inference-time adversarially trained defense plugin that mitigates
the effects of adversarial examples.

</details>

### [69] [Mitigating mode collapse in normalizing flows by annealing with an adaptive schedule: Application to parameter estimation](https://arxiv.org/abs/2505.03652)
*Yihang Wang,Chris Chi,Aaron R. Dinner*

Main category: cs.LG

TLDR: 通过自适应退火调度和有效样本量（ESS）优化归一化流（NFs），解决了多模态分布中的模式崩溃问题，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 归一化流（NFs）在多模态分布中易出现模式崩溃，限制了其实际应用。

Method: 采用基于ESS的自适应退火调度方法，并利用ESS进行样本修剪以减少方差。

Result: 在生化振荡器模型中，计算时间比传统MCMC方法快十倍，且收敛边际似然。

Conclusion: 该方法对NFs采样具有普适性，并为进一步优化提供了可能。

Abstract: Normalizing flows (NFs) provide uncorrelated samples from complex
distributions, making them an appealing tool for parameter estimation. However,
the practical utility of NFs remains limited by their tendency to collapse to a
single mode of a multimodal distribution. In this study, we show that annealing
with an adaptive schedule based on the effective sample size (ESS) can mitigate
mode collapse. We demonstrate that our approach can converge the marginal
likelihood for a biochemical oscillator model fit to time-series data in
ten-fold less computation time than a widely used ensemble Markov chain Monte
Carlo (MCMC) method. We show that the ESS can also be used to reduce variance
by pruning the samples. We expect these developments to be of general use for
sampling with NFs and discuss potential opportunities for further improvements.

</details>

### [70] [Neural Integral Operators for Inverse problems in Spectroscopy](https://arxiv.org/abs/2505.03677)
*Emanuele Zappala,Alice Giola,Andreas Kramer,Enrico Greco*

Main category: cs.LG

TLDR: 提出一种基于积分算子的深度学习方法，用于分子光谱分类，解决了小数据集上的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 光谱数据通常稀缺，传统深度学习易过拟合，传统机器学习方法精度有限。

Method: 基于第一类积分方程学习积分算子，设计了一种对小数据集更鲁棒的深度学习算法。

Result: 实验表明，该方法在小数据集上优于传统机器学习和其他深度学习模型。

Conclusion: 该方法结合了深度学习的优势，同时在小数据集上表现优异，解决了光谱分析中数据稀缺的挑战。

Abstract: Deep learning has shown high performance on spectroscopic inverse problems
when sufficient data is available. However, it is often the case that data in
spectroscopy is scarce, and this usually causes severe overfitting problems
with deep learning methods. Traditional machine learning methods are viable
when datasets are smaller, but the accuracy and applicability of these methods
is generally more limited.
  We introduce a deep learning method for classification of molecular spectra
based on learning integral operators via integral equations of the first kind,
which results in an algorithm that is less affected by overfitting issues on
small datasets, compared to other deep learning models.
  The problem formulation of the deep learning approach is based on inverse
problems, which have traditionally found important applications in
spectroscopy. We perform experiments on real world data to showcase our
algorithm. It is seen that the model outperforms traditional machine learning
approaches such as decision tree and support vector machine, and for small
datasets it outperforms other deep learning models. Therefore, our methodology
leverages the power of deep learning, still maintaining the performance when
the available data is very limited, which is one of the main issues that deep
learning faces in spectroscopy, where datasets are often times of small size.

</details>

### [71] [Learning Survival Distributions with the Asymmetric Laplace Distribution](https://arxiv.org/abs/2505.03712)
*Deming Sheng,Ricardo Henao*

Main category: cs.LG

TLDR: 提出了一种基于非对称拉普拉斯分布（ALD）的参数化生存分析方法，优于现有参数化和非参数化方法。


<details>
  <summary>Details</summary>
Motivation: 现有生存分析模型倾向于非参数化方法，但缺乏对事件分布的灵活估计。ALD提供了一种参数化解决方案，能够直接计算常用事件统计量。

Method: 使用ALD分布，通过最大似然估计学习个体水平的分布参数（位置、尺度和不对称性）。

Result: 在合成和真实数据上，该方法在准确性、区分度和校准方面优于其他方法。

Conclusion: ALD为基础的参数化生存分析方法是一种高效且灵活的解决方案。

Abstract: Probabilistic survival analysis models seek to estimate the distribution of
the future occurrence (time) of an event given a set of covariates. In recent
years, these models have preferred nonparametric specifications that avoid
directly estimating survival distributions via discretization. Specifically,
they estimate the probability of an individual event at fixed times or the time
of an event at fixed probabilities (quantiles), using supervised learning.
Borrowing ideas from the quantile regression literature, we propose a
parametric survival analysis method based on the Asymmetric Laplace
Distribution (ALD). This distribution allows for closed-form calculation of
popular event summaries such as mean, median, mode, variation, and quantiles.
The model is optimized by maximum likelihood to learn, at the individual level,
the parameters (location, scale, and asymmetry) of the ALD distribution.
Extensive results on synthetic and real-world data demonstrate that the
proposed method outperforms parametric and nonparametric approaches in terms of
accuracy, discrimination and calibration.

</details>

### [72] [Sustainable Smart Farm Networks: Enhancing Resilience and Efficiency with Decision Theory-Guided Deep Reinforcement Learning](https://arxiv.org/abs/2505.03721)
*Dian Chen,Zelin Wan,Dong Sam Ha,Jin-Hee Cho*

Main category: cs.LG

TLDR: 论文提出了一种基于深度强化学习（DRL）的可持续智能农场网络，结合迁移学习（TL）和决策理论（DT）优化监控质量和能源效率，显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 太阳能传感器监测系统在农业中应用广泛，但其对网络攻击的抵抗力和动态能源供应的适应性尚未充分研究。

Method: 采用DRL设计最优策略，结合TL和DT加速学习过程，优化监控和能源效率。

Result: 实验证明DT引导的DRL优于TL增强的DRL模型，性能提升且训练时间减少47.5%。

Conclusion: 该方法有效提升了智能农场网络的监控质量和能源可持续性，同时显著缩短了训练时间。

Abstract: Solar sensor-based monitoring systems have become a crucial agricultural
innovation, advancing farm management and animal welfare through integrating
sensor technology, Internet-of-Things, and edge and cloud computing. However,
the resilience of these systems to cyber-attacks and their adaptability to
dynamic and constrained energy supplies remain largely unexplored. To address
these challenges, we propose a sustainable smart farm network designed to
maintain high-quality animal monitoring under various cyber and adversarial
threats, as well as fluctuating energy conditions. Our approach utilizes deep
reinforcement learning (DRL) to devise optimal policies that maximize both
monitoring effectiveness and energy efficiency. To overcome DRL's inherent
challenge of slow convergence, we integrate transfer learning (TL) and decision
theory (DT) to accelerate the learning process. By incorporating DT-guided
strategies, we optimize monitoring quality and energy sustainability,
significantly reducing training time while achieving comparable performance
rewards. Our experimental results prove that DT-guided DRL outperforms
TL-enhanced DRL models, improving system performance and reducing training
runtime by 47.5%.

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [73] [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models](https://arxiv.org/abs/2505.02847)
*Bang Zhang,Ruotian Ma,Qingxuan Jiang,Peisong Wang,Jiaqi Chen,Zheng Xie,Xingyu Chen,Yue Wang,Fanghua Ye,Jian Li,Yifan Yang,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CL

TLDR: SAGE是一个自动化评估框架，用于衡量大型语言模型（LLM）的高阶社会认知能力，通过模拟人类情感变化和内心想法，提供更真实的评估。


<details>
  <summary>Details</summary>
Motivation: 解决如何评估LLM对人类的理解而非仅对文本的理解这一开放性问题。

Method: 引入Sentient Agent作为评估工具，模拟人类情感变化和内心想法，生成情感轨迹和可解释的内心活动。

Result: 实验显示SAGE的情感评分与心理学指标高度相关，并揭示了前沿模型与早期基线之间的显著差距。

Conclusion: SAGE为追踪语言模型在共情和社交能力方面的进步提供了原则性、可扩展且可解释的工具。

Abstract: Assessing how well a large language model (LLM) understands human, rather
than merely text, remains an open challenge. To bridge the gap, we introduce
Sentient Agent as a Judge (SAGE), an automated evaluation framework that
measures an LLM's higher-order social cognition. SAGE instantiates a Sentient
Agent that simulates human-like emotional changes and inner thoughts during
interaction, providing a more realistic evaluation of the tested model in
multi-turn conversations. At every turn, the agent reasons about (i) how its
emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a
numerical emotion trajectory and interpretable inner thoughts. Experiments on
100 supportive-dialogue scenarios show that the final Sentient emotion score
correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings
and utterance-level empathy metrics, validating psychological fidelity. We also
build a public Sentient Leaderboard covering 18 commercial and open-source
models that uncovers substantial gaps (up to 4x) between frontier systems
(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in
conventional leaderboards (e.g., Arena). SAGE thus provides a principled,
scalable and interpretable tool for tracking progress toward genuinely
empathetic and socially adept language agents.

</details>

### [74] [Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors](https://arxiv.org/abs/2505.02850)
*Nicy Scaria,Silvester John Joseph Kennedy,Diksha Seth,Ananya Thakur,Deepak Subramani*

Main category: cs.CL

TLDR: 提出了一种基于分层概念图的框架，利用LLM生成高质量MCQ，针对高中物理领域，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 手动生成高质量MCQ耗时且依赖专家知识，现有自动化方法无法满足高认知水平和领域特定误解的需求。

Method: 开发分层概念图，通过自动化流程检索相关内容，指导LLM生成MCQ和干扰项，并进行自动验证。

Result: 专家评估成功率达75.20%，学生猜测成功率降至28.05%，显著优于基线方法。

Conclusion: 概念图方法支持跨认知水平的稳健评估，并能快速识别概念差距，实现规模化反馈和干预。

Abstract: Generating high-quality MCQs, especially those targeting diverse cognitive
levels and incorporating common misconceptions into distractor design, is
time-consuming and expertise-intensive, making manual creation impractical at
scale. Current automated approaches typically generate questions at lower
cognitive levels and fail to incorporate domain-specific misconceptions. This
paper presents a hierarchical concept map-based framework that provides
structured knowledge to guide LLMs in generating MCQs with distractors. We
chose high-school physics as our test domain and began by developing a
hierarchical concept map covering major Physics topics and their
interconnections with an efficient database design. Next, through an automated
pipeline, topic-relevant sections of these concept maps are retrieved to serve
as a structured context for the LLM to generate questions and distractors that
specifically target common misconceptions. Lastly, an automated validation is
completed to ensure that the generated MCQs meet the requirements provided. We
evaluate our framework against two baseline approaches: a base LLM and a
RAG-based generation. We conducted expert evaluations and student assessments
of the generated MCQs. Expert evaluation shows that our method significantly
outperforms the baseline approaches, achieving a success rate of 75.20% in
meeting all quality criteria compared to approximately 37% for both baseline
methods. Student assessment data reveal that our concept map-driven approach
achieved a significantly lower guess success rate of 28.05% compared to 37.10%
for the baselines, indicating a more effective assessment of conceptual
understanding. The results demonstrate that our concept map-based approach
enables robust assessment across cognitive levels and instant identification of
conceptual gaps, facilitating faster feedback loops and targeted interventions
at scale.

</details>

### [75] [30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation](https://arxiv.org/abs/2505.02851)
*Franklin Zhang,Sonya Zhang,Alon Halevy*

Main category: cs.CL

TLDR: 30 Day Me是一个习惯养成应用，利用LLM将目标分解为可操作步骤并跟踪进度，核心是30DAYGEN系统，生成3,531种30天挑战。


<details>
  <summary>Details</summary>
Motivation: 利用LLM快速构建特定领域内容库，支持行为和教育活动。

Method: 开发30DAYGEN系统，从15K网页生成挑战，并实现运行时搜索。

Result: 展示了LLM在内容生成和语义去重中的实用性。

Conclusion: LLM可用于高效构建行为和教育内容库。

Abstract: In this paper, we present 30 Day Me, a habit formation application that
leverages Large Language Models (LLMs) to help users break down their goals
into manageable, actionable steps and track their progress. Central to the app
is the 30DAYGEN system, which generates 3,531 unique 30-day challenges sourced
from over 15K webpages, and enables runtime search of challenge ideas aligned
with user-defined goals. We showcase how LLMs can be harnessed to rapidly
construct domain specific content corpora for behavioral and educational
purposes, and propose a practical pipeline that incorporates effective LLM
enhanced approaches for content generation and semantic deduplication.

</details>

### [76] [Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for Regression Testing and Open Datasets](https://arxiv.org/abs/2505.02854)
*Masumi Morishige,Ryo Koshihara*

Main category: cs.CL

TLDR: GPR-bench是一个轻量级、可扩展的基准测试工具，用于评估生成式AI系统的可重复性和可靠性，支持双语任务和自动化评估。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI系统因模型更新或提示调整导致行为漂移的可重复性和可靠性问题。

Method: 开发GPR-bench，包含双语数据集（英语和日语）和自动化评估流程，使用LLM评分正确性和简洁性。

Result: 新模型在正确性上略有提升但差异不显著，简洁性提示显著提高输出简洁性（+12.37 pp）。

Conclusion: GPR-bench为社区提供了可扩展的基准测试工具，但需注意其对快速演进的模型区分能力的局限性。

Abstract: Reproducibility and reliability remain pressing challenges for generative AI
systems whose behavior can drift with each model update or prompt revision. We
introduce GPR-bench, a lightweight, extensible benchmark that operationalizes
regression testing for general purpose use cases. GPR-bench couples an open,
bilingual (English and Japanese) dataset covering eight task categories (e.g.,
text generation, code generation, and information retrieval) and 10 scenarios
in each task categories (80 total test cases for each language) with an
automated evaluation pipeline that employs "LLM-as-a-Judge" scoring of
correctness and conciseness. Experiments across three recent model versions -
gpt-4o-mini, o3-mini, and o4-mini - and two prompt configurations (default
versus concise-writing instruction) reveal heterogeneous quality. Our results
show that newer models generally improve correctness, but the differences are
modest and not statistically significant, suggesting that GPR-bench may not be
sufficiently challenging to differentiate between recent model versions. In
contrast, the concise-writing instruction significantly enhances conciseness
(+12.37 pp, Mann-Whitney U test: p < 0.001, effect size r = 0.2995) with
minimal degradations on accuracy (-1.7 pp), demonstrating the effectiveness of
prompt engineering. Released under the MIT License, GPR- bench lowers the
barrier to initiating reproducibility monitoring and provides a foundation for
community-driven extensions, while also raising important considerations about
benchmark design for rapidly evolving language models.

</details>

### [77] [Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models](https://arxiv.org/abs/2505.02858)
*Henry Tari,Nojus Sereiva,Rishabh Kaushal,Thales Bertaglia,Adriana Iamnitchi*

Main category: cs.CL

TLDR: 论文探讨了利用大语言模型生成跨平台社交媒体合成数据的潜力，评估了其词汇和语义质量，并提出了新的保真度指标。


<details>
  <summary>Details</summary>
Motivation: 社交媒体数据集对研究至关重要，但获取多平台数据成本高且受限。

Method: 采用多平台主题提示和不同语言模型生成合成数据，并与真实数据对比。

Result: 大语言模型生成多平台合成数据效果良好，但不同模型表现差异大，需后处理提高保真度。

Conclusion: 生成合成数据可行，需进一步优化模型和后处理方法。

Abstract: Social media datasets are essential for research on a variety of topics, such
as disinformation, influence operations, hate speech detection, or influencer
marketing practices. However, access to social media datasets is often
constrained due to costs and platform restrictions. Acquiring datasets that
span multiple platforms, which is crucial for understanding the digital
ecosystem, is particularly challenging. This paper explores the potential of
large language models to create lexically and semantically relevant social
media datasets across multiple platforms, aiming to match the quality of real
data. We propose multi-platform topic-based prompting and employ various
language models to generate synthetic data from two real datasets, each
consisting of posts from three different social media platforms. We assess the
lexical and semantic properties of the synthetic data and compare them with
those of the real data. Our empirical findings show that using large language
models to generate synthetic multi-platform social media data is promising,
different language models perform differently in terms of fidelity, and a
post-processing approach might be needed for generating high-fidelity synthetic
datasets for research. In addition to the empirical evaluation of three state
of the art large language models, our contributions include new fidelity
metrics specific to multi-platform social media datasets.

</details>

### [78] [Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI](https://arxiv.org/abs/2505.02859)
*Jonas Bokstaller,Julia Altheimer,Julian Dormehl,Alina Buss,Jasper Wiltfang,Johannes Schneider,Maximilian Röglinger*

Main category: cs.CL

TLDR: 论文提出了一种结合XAI和LLM的交互式聊天机器人架构，用于提升ML模型的可解释性，并在电池健康预测中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着ML模型的复杂性增加，其黑箱特性愈发明显，而LLM在理解人类语言和复杂模式方面表现突出，因此结合两者以提升XAI的可解释性。

Method: 提出了一种基于微调LLM的交互式聊天机器人参考架构，并在电池健康预测场景中实例化。

Result: 评估表明，该原型显著提升了ML模型的可解释性，尤其对XAI经验较少的用户效果更佳。

Conclusion: 该架构为XAI的交互式解释提供了有效解决方案，尤其在特定领域（如电池健康预测）中表现突出。

Abstract: Across various sectors applications of eXplainableAI (XAI) gained momentum as
the increasing black-boxedness of prevailing Machine Learning (ML) models
became apparent. In parallel, Large Language Models (LLMs) significantly
developed in their abilities to understand human language and complex patterns.
By combining both, this paper presents a novel reference architecture for the
interpretation of XAI through an interactive chatbot powered by a fine-tuned
LLM. We instantiate the reference architecture in the context of
State-of-Health (SoH) prediction for batteries and validate its design in
multiple evaluation and demonstration rounds. The evaluation indicates that the
implemented prototype enhances the human interpretability of ML, especially for
users with less experience with XAI.

</details>

### [79] [Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](https://arxiv.org/abs/2505.02862)
*Haoming Yang,Ke Ma,Xiaojun Jia,Yingfei Sun,Qianqian Xu,Qingming Huang*

Main category: cs.CL

TLDR: 提出了一种基于人类认知启发的新型越狱攻击框架ICRT，通过认知分解和相关性偏置优化恶意提示，并引入基于排名的危害性评估指标，成功绕过主流LLMs的安全机制。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击方法依赖暴力优化或人工设计，未能揭示真实场景中的潜在风险，亟需更有效的攻击框架以提升防御策略。

Method: 利用人类认知的启发式偏差（如简单效应和相关性偏置），通过认知分解降低恶意提示复杂度，并重组提示以增强语义对齐。同时，采用排名聚合方法（如Elo、HodgeRank和Rank Centrality）量化生成内容的危害性。

Result: 实验表明，ICRT能持续绕过主流LLMs的安全机制，生成高风险内容，揭示了越狱攻击的潜在威胁。

Conclusion: ICRT为越狱攻击风险提供了新视角，有助于开发更强大的防御策略。

Abstract: Despite the remarkable performance of Large Language Models (LLMs), they
remain vulnerable to jailbreak attacks, which can compromise their safety
mechanisms. Existing studies often rely on brute-force optimization or manual
design, failing to uncover potential risks in real-world scenarios. To address
this, we propose a novel jailbreak attack framework, ICRT, inspired by
heuristics and biases in human cognition. Leveraging the simplicity effect, we
employ cognitive decomposition to reduce the complexity of malicious prompts.
Simultaneously, relevance bias is utilized to reorganize prompts, enhancing
semantic alignment and inducing harmful outputs effectively. Furthermore, we
introduce a ranking-based harmfulness evaluation metric that surpasses the
traditional binary success-or-failure paradigm by employing ranking aggregation
methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify
the harmfulness of generated content. Experimental results show that our
approach consistently bypasses mainstream LLMs' safety mechanisms and generates
high-risk content, providing insights into jailbreak attack risks and
contributing to stronger defense strategies.

</details>

### [80] [Accelerating Large Language Model Reasoning via Speculative Search](https://arxiv.org/abs/2505.02865)
*Zhihai Wang,Jie Wang,Jilai Pan,Xilin Xia,Huiling Zhen,Mingxuan Yuan,Jianye Hao,Feng Wu*

Main category: cs.CL

TLDR: 提出了一种名为SpecSearch的新框架，通过小模型与大模型的协作优化思维生成，显著加速LLM推理，同时保持推理质量。


<details>
  <summary>Details</summary>
Motivation: 解决基于树搜索的推理方法因生成大量中间推理步骤导致的高延迟问题，限制LLM的适用性。

Method: 利用小模型与大模型在思维和标记级别上的战略协作，结合质量保留拒绝机制过滤低质量思维。

Result: 在Qwen和Llama模型上实验表明，SpecSearch比现有方法快2.12倍，且推理质量相当。

Conclusion: SpecSearch通过高效思维生成和过滤机制，显著提升LLM推理速度，同时保持高质量。

Abstract: Tree-search-based reasoning methods have significantly enhanced the reasoning
capability of large language models (LLMs) by facilitating the exploration of
multiple intermediate reasoning steps, i.e., thoughts. However, these methods
suffer from substantial inference latency, as they have to generate numerous
reasoning thoughts, severely limiting LLM applicability. To address this
challenge, we propose a novel Speculative Search (SpecSearch) framework that
significantly accelerates LLM reasoning by optimizing thought generation.
Specifically, SpecSearch utilizes a small model to strategically collaborate
with a large model at both thought and token levels, efficiently generating
high-quality reasoning thoughts. The major pillar of SpecSearch is a novel
quality-preserving rejection mechanism, which effectively filters out thoughts
whose quality falls below that of the large model's outputs. Moreover, we show
that SpecSearch preserves comparable reasoning quality to the large model.
Experiments on both the Qwen and Llama models demonstrate that SpecSearch
significantly outperforms state-of-the-art approaches, achieving up to
2.12$\times$ speedup with comparable reasoning quality.

</details>

### [81] [Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading](https://arxiv.org/abs/2505.02872)
*Cfir Avraham Hadar,Omer Shubi,Yoav Meiri,Yevgeni Berzak*

Main category: cs.CL

TLDR: 研究探讨了是否可以通过眼动数据自动解码读者的开放式阅读目标，并提出了目标分类和目标重建任务，使用多模态LLM模型取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 人们在阅读时通常有特定的目标，本研究首次尝试通过眼动数据自动解码这些目标。

Method: 提出了目标分类和目标重建任务，使用大规模英语阅读眼动数据，开发并比较了多种多模态LLM模型。

Result: 实验表明，LLM能够从眼动数据中有效提取读者的文本特定目标信息。

Conclusion: LLM可以成功解码读者的阅读目标，为眼动数据在阅读研究中的应用提供了新方向。

Abstract: When reading, we often have specific information that interests us in a text.
For example, you might be reading this paper because you are curious about LLMs
for eye movements in reading, the experimental design, or perhaps you only care
about the question ``but does it work?''. More broadly, in daily life, people
approach texts with any number of text-specific goals that guide their reading
behavior. In this work, we ask, for the first time, whether open-ended reading
goals can be automatically decoded from eye movements in reading. To address
this question, we introduce goal classification and goal reconstruction tasks
and evaluation frameworks, and use large-scale eye tracking for reading data in
English with hundreds of text-specific information seeking tasks. We develop
and compare several discriminative and generative multimodal LLMs that combine
eye movements and text for goal classification and goal reconstruction. Our
experiments show considerable success on both tasks, suggesting that LLMs can
extract valuable information about the readers' text-specific goals from eye
movements.

</details>

### [82] [Logits-Constrained Framework with RoBERTa for Ancient Chinese NER](https://arxiv.org/abs/2505.02983)
*Wenjie Hua,Shenghan Xu*

Main category: cs.CL

TLDR: 提出了一种Logits-Constrained（LC）框架用于古汉语命名实体识别，结合GujiRoBERTa和可微分解码机制，在EvaHan 2025基准上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决古汉语命名实体识别中高标签或大数据场景下的性能问题。

Method: 两阶段模型：GujiRoBERTa上下文编码和可微分解码机制约束BMES标签转移。

Result: LC框架在EvaHan 2025上优于CRF和BiLSTM方法，尤其在复杂标签或大数据场景。

Conclusion: 提出的模型选择标准为古汉语NLP任务提供了实用指导。

Abstract: This paper presents a Logits-Constrained (LC) framework for Ancient Chinese
Named Entity Recognition (NER), evaluated on the EvaHan 2025 benchmark. Our
two-stage model integrates GujiRoBERTa for contextual encoding and a
differentiable decoding mechanism to enforce valid BMES label transitions.
Experiments demonstrate that LC improves performance over traditional CRF and
BiLSTM-based approaches, especially in high-label or large-data settings. We
also propose a model selection criterion balancing label complexity and dataset
size, providing practical guidance for real-world Ancient Chinese NLP tasks.

</details>

### [83] [RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale](https://arxiv.org/abs/2505.03005)
*Daniel Goldstein,Eric Alcaide,Janna Lu,Eugene Cheah*

Main category: cs.CL

TLDR: RADLADS是一种快速将softmax注意力Transformer转换为线性注意力解码器模型的协议，并提出了两种新的RWKV变体架构。转换过程仅需350-700M tokens，成本低于2000美元，且推理质量接近原模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统Transformer模型计算成本高的问题，提供一种高效且低成本的转换方法。

Method: 提出RADLADS协议，通过少量token（350-700M）快速转换模型，并引入新的RWKV变体架构。

Result: 转换后的模型在标准基准测试中表现优异，推理质量接近原模型，成本显著降低。

Conclusion: RADLADS提供了一种高效、低成本的模型转换方案，适用于大规模线性注意力模型。

Abstract: We present Rapid Attention Distillation to Linear Attention Decoders at Scale
(RADLADS), a protocol for rapidly converting softmax attention transformers
into linear attention decoder models, along with two new RWKV-variant
architectures, and models converted from popular Qwen2.5 open source models in
7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,
less than 0.005% of the token count used to train the original teacher models.
Converting to our 72B linear attention model costs less than \$2,000 USD at
today's prices, yet quality at inference remains close to the original
transformer. These models achieve state-of-the-art downstream performance
across a set of standard benchmarks for linear attention models of their size.
We release all our models on HuggingFace under the Apache 2.0 license, with the
exception of our 72B models which are also governed by the Qwen License
Agreement.
  Models at
https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102
Training Code at https://github.com/recursal/RADLADS-paper

</details>

### [84] [Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis](https://arxiv.org/abs/2505.03019)
*Albérick Euraste Djiré,Abdoul Kader Kaboré,Earl T. Barr,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TLDR: PEARL是一种检测大型语言模型（LLM）记忆现象的新方法，通过输入扰动评估模型输出的敏感性，无需访问模型内部。


<details>
  <summary>Details</summary>
Motivation: LLM在训练中可能记忆而非泛化数据，引发隐私、知识产权和评估可靠性问题。

Method: PEARL通过输入扰动分析输出一致性，区分泛化与记忆。

Result: 在Pythia和GPT-4o模型上验证，成功检测经典文本和代码的记忆现象。

Conclusion: PEARL为识别LLM记忆行为提供了有效框架，支持训练数据来源推断。

Abstract: While Large Language Models (LLMs) achieve remarkable performance through
training on massive datasets, they can exhibit concerning behaviors such as
verbatim reproduction of training data rather than true generalization. This
memorization phenomenon raises significant concerns about data privacy,
intellectual property rights, and the reliability of model evaluations. This
paper introduces PEARL, a novel approach for detecting memorization in LLMs.
PEARL assesses how sensitive an LLM's performance is to input perturbations,
enabling memorization detection without requiring access to the model's
internals. We investigate how input perturbations affect the consistency of
outputs, enabling us to distinguish between true generalization and
memorization. Our findings, following extensive experiments on the Pythia open
model, provide a robust framework for identifying when the model simply
regurgitates learned information. Applied on the GPT 4o models, the PEARL
framework not only identified cases of memorization of classic texts from the
Bible or common code from HumanEval but also demonstrated that it can provide
supporting evidence that some data, such as from the New York Times news
articles, were likely part of the training data of a given model.

</details>

### [85] [A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts](https://arxiv.org/abs/2505.03025)
*Steven Bedrick,A. Seza Doğruöz,Sergiu Nisioi*

Main category: cs.CL

TLDR: 本文综述了医疗领域中合成数据集的创建、评估和使用方法，并提出了一种新的分类法以帮助比较和评估合成数据的类型和程度。


<details>
  <summary>Details</summary>
Motivation: 由于临床对话数据的敏感性和收集难度，合成数据集在医疗领域中被广泛使用，但缺乏理论指导其最佳使用和泛化方法。

Method: 综述了合成数据集的创建和评估方法，并提出了一种新的分类法。

Result: 为医疗对话任务中的合成数据集提供了分类和评估框架。

Conclusion: 提出的分类法有助于更好地理解和比较合成数据集，促进其在医疗领域的应用。

Abstract: Synthetic data sets are used across linguistic domains and NLP tasks,
particularly in scenarios where authentic data is limited (or even
non-existent). One such domain is that of clinical (healthcare) contexts, where
there exist significant and long-standing challenges (e.g., privacy,
anonymization, and data governance) which have led to the development of an
increasing number of synthetic datasets. One increasingly important category of
clinical dataset is that of clinical dialogues which are especially sensitive
and difficult to collect, and as such are commonly synthesized.
  While such synthetic datasets have been shown to be sufficient in some
situations, little theory exists to inform how they may be best used and
generalized to new applications. In this paper, we provide an overview of how
synthetic datasets are created, evaluated and being used for dialogue related
tasks in the medical domain. Additionally, we propose a novel typology for use
in classifying types and degrees of data synthesis, to facilitate comparison
and evaluation.

</details>

### [86] [UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output](https://arxiv.org/abs/2505.03030)
*Sicong Huang,Jincheng He,Shiyuan Huang,Karthik Raja Anandan,Arkajyoti Chakraborty,Ian Lane*

Main category: cs.CL

TLDR: 论文提出了一种检测和定位大语言模型输出中幻觉内容的框架，并在多语言任务中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在知识密集型查询中产生的幻觉问题，并精确定位幻觉发生的位置。

Method: 通过检索相关上下文、识别答案中的虚假内容，并将其映射回LLM输出中的具体片段，同时优化提示。

Result: 系统在多语言任务中平均排名第一，性能最优。

Conclusion: 框架有效解决了幻觉检测和定位问题，代码和实验结果已公开。

Abstract: Hallucinations pose a significant challenge for large language models when
answering knowledge-intensive queries. As LLMs become more widely adopted, it
is crucial not only to detect if hallucinations occur but also to pinpoint
exactly where in the LLM output they occur. SemEval 2025 Task 3, Mu-SHROOM:
Multilingual Shared-task on Hallucinations and Related Observable
Overgeneration Mistakes, is a recent effort in this direction. This paper
describes the UCSC system submission to the shared Mu-SHROOM task. We introduce
a framework that first retrieves relevant context, next identifies false
content from the answer, and finally maps them back to spans in the LLM output.
The process is further enhanced by automatically optimizing prompts. Our system
achieves the highest overall performance, ranking #1 in average position across
all languages. We release our code and experiment results.

</details>

### [87] [Teaching Models to Understand (but not Generate) High-risk Data](https://arxiv.org/abs/2505.03052)
*Ryan Wang,Matthew Finlayson,Luca Soldaini,Swabha Swayamdipta,Robin Jia*

Main category: cs.CL

TLDR: SLUNG是一种预训练范式，旨在让模型理解高风险内容但不生成它，通过选择性损失函数实现。


<details>
  <summary>Details</summary>
Motivation: 传统方法过滤高风险内容会限制模型识别和应对有害内容的能力，SLUNG旨在解决这一问题。

Method: SLUNG选择性避免激励高风险标记的生成，同时确保模型理解这些内容。

Result: 实验表明SLUNG提高了模型对高风险内容的理解能力，同时不增加其生成。

Conclusion: SLUNG使模型能从高风险文本中受益，而无需完全过滤。

Abstract: Language model developers typically filter out high-risk content -- such as
toxic or copyrighted text -- from their pre-training data to prevent models
from generating similar outputs. However, removing such data altogether limits
models' ability to recognize and appropriately respond to harmful or sensitive
content. In this paper, we introduce Selective Loss to Understand but Not
Generate (SLUNG), a pre-training paradigm through which models learn to
understand high-risk data without learning to generate it. Instead of uniformly
applying the next-token prediction loss, SLUNG selectively avoids incentivizing
the generation of high-risk tokens while ensuring they remain within the
model's context window. As the model learns to predict low-risk tokens that
follow high-risk ones, it is forced to understand the high-risk content.
Through our experiments, we show that SLUNG consistently improves models'
understanding of high-risk data (e.g., ability to recognize toxic content)
without increasing its generation (e.g., toxicity of model responses). Overall,
our SLUNG paradigm enables models to benefit from high-risk text that would
otherwise be filtered out.

</details>

### [88] [Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text](https://arxiv.org/abs/2505.03053)
*Jennifer Healey,Laurie Byrum,Md Nadeem Akhtar,Surabhi Bhargava,Moumita Sinha*

Main category: cs.CL

TLDR: 论文探讨了LLM评估的挑战，提出了一种半自动化的偏见评估框架，结合人类洞察力，并改进了偏见的定义和分类方法。


<details>
  <summary>Details</summary>
Motivation: 现实部署中，LLM评估因任务特定提示和上下文交互而复杂化，传统短上下文基准测试可能失效，而大规模人类评估成本高。

Method: 开发了半自动化偏见评估框架，结合人类洞察力，改进了偏见的操作定义和分类方法。

Result: 框架成功识别了偏见基准中的问题模板，并实现了超越多项选择的偏见分类。

Conclusion: 半自动化框架结合人类评估，为LLM偏见评估提供了可行且高效的方法。

Abstract: LLM evaluation is challenging even the case of base models. In real world
deployments, evaluation is further complicated by the interplay of task
specific prompts and experiential context. At scale, bias evaluation is often
based on short context, fixed choice benchmarks that can be rapidly evaluated,
however, these can lose validity when the LLMs' deployed context differs. Large
scale human evaluation is often seen as too intractable and costly. Here we
present our journey towards developing a semi-automated bias evaluation
framework for free text responses that has human insights at its core. We
discuss how we developed an operational definition of bias that helped us
automate our pipeline and a methodology for classifying bias beyond multiple
choice. We additionally comment on how human evaluation helped us uncover
problematic templates in a bias benchmark.

</details>

### [89] [Improving Model Alignment Through Collective Intelligence of Open-Source LLMS](https://arxiv.org/abs/2505.03059)
*Junlin Wang,Roy Xie,Shang Zhu,Jue Wang,Ben Athiwaratkun,Bhuwan Dhingra,Shuaiwen Leon Song,Ce Zhang,James Zou*

Main category: cs.CL

TLDR: MoAA通过结合多种语言模型的优势生成高质量对齐数据，提升模型性能，无需依赖昂贵的人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 构建高质量人工标注数据成本高且难以扩展，限制了模型对齐的多样性和泛化能力。

Method: 提出Mixture of Agents Alignment (MoAA)，利用多种语言模型的集体优势生成对齐数据。

Result: MoAA显著提升了模型性能（如LLaMA-3.1-8B-Instruct在Arena-Hard和AlpacaEval2上的胜率）。

Conclusion: MoAA为开源LLM提供了一种可扩展且多样化的对齐方法，无需依赖外部监督。

Abstract: Building helpful and harmless large language models (LLMs) requires effective
model alignment approach based on human instructions and feedback, which
necessitates high-quality human-labeled data. Constructing such datasets is
often expensive and hard to scale, and may face potential limitations on
diversity and generalization. To address these challenges, we introduce Mixture
of Agents Alignment (MoAA), that leverages the collective strengths of various
language models to provide high-quality data for model alignment. By employing
MoAA, we enhance both supervised fine-tuning and preference optimization,
leading to improved performance compared to using a single model alone to
generate alignment data (e.g. using GPT-4o alone). Evaluation results show that
our approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on
Arena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising
direction for model alignment through this new scalable and diverse synthetic
data recipe. Furthermore, we demonstrate that MoAA enables a self-improvement
pipeline, where models finetuned on MoA-generated data surpass their own
initial capabilities, providing evidence that our approach can push the
frontier of open-source LLMs without reliance on stronger external supervision.
Data and code will be released.

</details>

### [90] [Survey of Abstract Meaning Representation: Then, Now, Future](https://arxiv.org/abs/2505.03229)
*Behrooz Mansouri*

Main category: cs.CL

TLDR: 本文综述了抽象意义表示（AMR）及其扩展，探讨了其解析与生成任务，并回顾了AMR在文本生成、分类和信息提取等领域的应用。


<details>
  <summary>Details</summary>
Motivation: AMR作为一种语义表示框架，能够通过图结构捕捉句子的含义，为机器理解人类语言提供支持。

Method: 通过调查AMR及其扩展，分析其解析（文本到AMR）和生成（AMR到文本）任务的传统、当前及未来方法。

Result: AMR在文本生成、分类和信息提取等领域有广泛应用，但面临一些挑战。

Conclusion: AMR在提升机器语言理解方面具有潜力，未来研究需解决现有挑战。

Abstract: This paper presents a survey of Abstract Meaning Representation (AMR), a
semantic representation framework that captures the meaning of sentences
through a graph-based structure. AMR represents sentences as rooted, directed
acyclic graphs, where nodes correspond to concepts and edges denote
relationships, effectively encoding the meaning of complex sentences. This
survey investigates AMR and its extensions, focusing on AMR capabilities. It
then explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by
showing traditional, current, and possible futures approaches. It also reviews
various applications of AMR including text generation, text classification, and
information extraction and information seeking. By analyzing recent
developments and challenges in the field, this survey provides insights into
future directions for research and the potential impact of AMR on enhancing
machine understanding of human language.

</details>

### [91] [Ψ-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback](https://arxiv.org/abs/2505.03293)
*Shijing Zhu,Zhuang Chen,Guanqun Bi,Binghang Li,Yaxi Deng,Dazhen Wan,Libiao Peng,Xiyao Xiao,Rongsheng Zhang,Tangjie Lv,Zhipeng Hu,FangFang Li,Minlie Huang*

Main category: cs.CL

TLDR: 论文提出了Psi-Arena框架，用于全面评估和优化基于LLM的心理咨询师，通过多阶段对话、三方评估和闭环优化提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法在静态测试、单一视角和开环框架方面存在局限，无法全面评估LLM心理咨询师的能力。

Method: 提出Psi-Arena框架，包括多阶段对话模拟、三方评估（客户、咨询师、监督者）和闭环优化。

Result: 实验显示不同LLM性能差异显著，优化后咨询性能提升高达141%。

Conclusion: Psi-Arena为心理健康领域可靠且人性化的LLM应用提供了基础资源。

Abstract: Large language models (LLMs) have shown promise in providing scalable mental
health support, while evaluating their counseling capability remains crucial to
ensure both efficacy and safety. Existing evaluations are limited by the static
assessment that focuses on knowledge tests, the single perspective that centers
on user experience, and the open-loop framework that lacks actionable feedback.
To address these issues, we propose {\Psi}-Arena, an interactive framework for
comprehensive assessment and optimization of LLM-based counselors, featuring
three key characteristics: (1) Realistic arena interactions that simulate
real-world counseling through multi-stage dialogues with psychologically
profiled NPC clients, (2) Tripartite evaluation that integrates assessments
from the client, counselor, and supervisor perspectives, and (3) Closed-loop
optimization that iteratively improves LLM counselors using diagnostic
feedback. Experiments across eight state-of-the-art LLMs show significant
performance variations in different real-world scenarios and evaluation
perspectives. Moreover, reflection-based optimization results in up to a 141%
improvement in counseling performance. We hope PsychoArena provides a
foundational resource for advancing reliable and human-aligned LLM applications
in mental healthcare.

</details>

### [92] [Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation](https://arxiv.org/abs/2505.03320)
*Junyu Ma,Tianqing Fang,Zhisong Zhang,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.CL

TLDR: 通过Recall with Reasoning (RwR)方法，提升Mamba模型的长上下文记忆能力，无需架构改动。


<details>
  <summary>Details</summary>
Motivation: Mamba模型在理论上具有无限上下文潜力，但在实际中，当序列远超训练长度时表现受限。

Method: 使用RwR方法，通过从教师模型中提取思维链（CoT）摘要，并在微调时将其作为CoT提示前置，教会Mamba主动回忆和推理长上下文。

Result: 在LONGMEMEVAL和HELMET实验中，RwR提升了Mamba的长上下文性能，优于同类Transformer/混合基线，同时保持短上下文能力。

Conclusion: RwR是一种简单有效的方法，可解锁Mamba的长上下文记忆能力，无需改变架构。

Abstract: Mamba's theoretical infinite-context potential is limited in practice when
sequences far exceed training lengths. This work explores unlocking Mamba's
long-context memory ability by a simple-yet-effective method, Recall with
Reasoning (RwR), by distilling chain-of-thought (CoT) summarization from a
teacher model. Specifically, RwR prepends these summarization as CoT prompts
during fine-tuning, teaching Mamba to actively recall and reason over long
contexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's
long-context performance against comparable Transformer/hybrid baselines under
similar pretraining conditions, while preserving short-context capabilities,
all without architectural changes.

</details>

### [93] [Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation](https://arxiv.org/abs/2505.03406)
*Mohammad Shoaib Ansari,Mohd Sohail Ali Khan,Shubham Revankar,Aditya Varma,Anil S. Mokhade*

Main category: cs.CL

TLDR: 该论文研究了大型语言模型（LLMs）在医疗领域的应用，通过结合检索增强生成（RAG）和量化低秩适应（QLoRA）技术，提升医疗决策支持的准确性。


<details>
  <summary>Details</summary>
Motivation: 旨在利用LLMs优化医疗决策支持，通过结合医院特定数据和高效微调技术，提高响应准确性和资源效率。

Method: 采用Llama 3.2-3B-Instruct作为基础模型，结合RAG和QLoRA技术，嵌入和检索医疗信息，并优化参数和内存使用。

Result: 系统显著提升了响应准确性，并在多个医疗基准测试中表现良好，适用于疾病预测、治疗建议和报告摘要等任务。

Conclusion: LLMs在医疗领域具有广泛潜力，但需关注伦理和实际挑战，未来可进一步优化和部署。

Abstract: This research paper investigates the application of Large Language Models
(LLMs) in healthcare, specifically focusing on enhancing medical decision
support through Retrieval-Augmented Generation (RAG) integrated with
hospital-specific data and fine-tuning using Quantized Low-Rank Adaptation
(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By
embedding and retrieving context-relevant healthcare information, the system
significantly improves response accuracy. QLoRA facilitates notable parameter
efficiency and memory optimization, preserving the integrity of medical
information through specialized quantization techniques. Our research also
shows that our model performs relatively well on various medical benchmarks,
indicating that it can be used to make basic medical suggestions. This paper
details the system's technical components, including its architecture,
quantization methods, and key healthcare applications such as enhanced disease
prediction from patient symptoms and medical history, treatment suggestions,
and efficient summarization of complex medical reports. We touch on the ethical
considerations-patient privacy, data security, and the need for rigorous
clinical validation-as well as the practical challenges of integrating such
systems into real-world healthcare workflows. Furthermore, the lightweight
quantized weights ensure scalability and ease of deployment even in
low-resource hospital environments. Finally, the paper concludes with an
analysis of the broader impact of LLMs on healthcare and outlines future
directions for LLMs in medical settings.

</details>

### [94] [MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks](https://arxiv.org/abs/2505.03427)
*Mouath Abu Daoud,Chaimae Abouzahir,Leen Kharouf,Walid Al-Eisawi,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

TLDR: 该研究介绍了MedArabiQ，一个阿拉伯语医疗领域的基准数据集，用于评估大型语言模型（LLMs）的性能，并强调了多语言基准的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏高质量的阿拉伯语医疗数据集和基准，LLMs在该领域的效能尚未被充分探索。

Method: 研究构建了包含七种医疗任务的MedArabiQ数据集，并评估了五种先进LLMs的性能，包括GPT-4o等。

Result: 研究发现需要更多高质量的多语言基准，以确保LLMs在医疗领域的公平部署和扩展性。

Conclusion: 通过发布MedArabiQ数据集，为未来研究提供了基础，旨在提升LLMs的多语言能力，促进生成式AI在医疗中的公平使用。

Abstract: Large Language Models (LLMs) have demonstrated significant promise for
various applications in healthcare. However, their efficacy in the Arabic
medical domain remains unexplored due to the lack of high-quality
domain-specific datasets and benchmarks. This study introduces MedArabiQ, a
novel benchmark dataset consisting of seven Arabic medical tasks, covering
multiple specialties and including multiple choice questions,
fill-in-the-blank, and patient-doctor question answering. We first constructed
the dataset using past medical exams and publicly available datasets. We then
introduced different modifications to evaluate various LLM capabilities,
including bias mitigation. We conducted an extensive evaluation with five
state-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude
3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of
new high-quality benchmarks that span different languages to ensure fair
deployment and scalability of LLMs in healthcare. By establishing this
benchmark and releasing the dataset, we provide a foundation for future
research aimed at evaluating and enhancing the multilingual capabilities of
LLMs for the equitable use of generative AI in healthcare.

</details>

### [95] [An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation](https://arxiv.org/abs/2505.03452)
*Matan Orbach,Ohad Eytan,Benjamin Sznajder,Ariel Gera,Odellia Boni,Yoav Kantor,Gal Bloch,Omri Levy,Hadas Abraham,Nitzan Barzilay,Eyal Shnarch,Michael E. Factor,Shila Ofek-Koifman,Paula Ta-Shma,Assaf Toledo*

Main category: cs.CL

TLDR: 研究比较了5种HPO算法在5个数据集上的表现，发现贪婪或随机搜索能高效优化RAG配置，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: RAG配置优化复杂且昂贵，现有HPO框架效果未经验证。

Method: 使用5种HPO算法在5个数据集上进行实验，包括新收集的真实产品文档数据。

Result: 贪婪或迭代随机搜索能高效优化RAG性能，且优先优化模型比按流程顺序优化更有效。

Conclusion: RAG HPO可高效完成，显著提升性能，优化顺序应以模型优先。

Abstract: Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a
given use case can be complex and expensive. Motivated by this challenge,
frameworks for RAG hyper-parameter optimization (HPO) have recently emerged,
yet their effectiveness has not been rigorously benchmarked. To address this
gap, we present a comprehensive study involving 5 HPO algorithms over 5
datasets from diverse domains, including a new one collected for this work on
real-world product documentation. Our study explores the largest HPO search
space considered to date, with two optimized evaluation metrics. Analysis of
the results shows that RAG HPO can be done efficiently, either greedily or with
iterative random search, and that it significantly boosts RAG performance for
all datasets. For greedy HPO approaches, we show that optimizing models first
is preferable to the prevalent practice of optimizing sequentially according to
the RAG pipeline order.

</details>

### [96] [Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis](https://arxiv.org/abs/2505.03467)
*Shuang Zhou,Jiashuo Wang,Zidu Xu,Song Wang,David Brauer,Lindsay Welton,Jacob Cogan,Yuen-Hei Chung,Lei Tian,Zaifu Zhan,Yu Hou,Mingquan Lin,Genevieve B. Melton,Rui Zhang*

Main category: cs.CL

TLDR: ConfiDx是一个基于大语言模型（LLM）的不确定性感知诊断系统，通过识别和解释诊断不确定性，提升自动诊断系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 临床笔记中证据不足时，诊断不确定性会增加误诊风险，但目前对诊断不确定性的识别和解释研究不足。

Method: 通过微调开源LLM并整合诊断标准，构建了ConfiDx模型，并利用标注数据集评估其性能。

Result: ConfiDx在识别诊断不确定性和生成可信解释方面表现优异，显著提升了诊断性能。

Conclusion: ConfiDx首次联合解决了诊断不确定性的识别与解释问题，增强了自动诊断系统的可信度。

Abstract: Explainable disease diagnosis, which leverages patient information (e.g.,
signs and symptoms) and computational models to generate probable diagnoses and
reasonings, offers clear clinical values. However, when clinical notes
encompass insufficient evidence for a definite diagnosis, such as the absence
of definitive symptoms, diagnostic uncertainty usually arises, increasing the
risk of misdiagnosis and adverse outcomes. Although explicitly identifying and
explaining diagnostic uncertainties is essential for trustworthy diagnostic
systems, it remains under-explored. To fill this gap, we introduce ConfiDx, an
uncertainty-aware large language model (LLM) created by fine-tuning open-source
LLMs with diagnostic criteria. We formalized the task and assembled richly
annotated datasets that capture varying degrees of diagnostic ambiguity.
Evaluating ConfiDx on real-world datasets demonstrated that it excelled in
identifying diagnostic uncertainties, achieving superior diagnostic
performance, and generating trustworthy explanations for diagnoses and
uncertainties. To our knowledge, this is the first study to jointly address
diagnostic uncertainty recognition and explanation, substantially enhancing the
reliability of automatic diagnostic systems.

</details>

### [97] [Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models](https://arxiv.org/abs/2505.03469)
*Bin Yu,Hang Yuan,Yuliang Wei,Bailing Wang,Weizhen Qi,Kai Chen*

Main category: cs.CL

TLDR: LS-Mixture SFT方法通过结合长链和短链推理数据，解决了SFT中继承的“过度思考”问题，提升了模型准确率并减少了响应长度。


<details>
  <summary>Details</summary>
Motivation: 解决监督微调（SFT）中继承的“过度思考”问题，即推理链冗长冗余。

Method: 提出LS-Mixture SFT，结合长链推理数据及其结构保留重写的短链版本进行微调。

Result: 相比直接SFT，平均准确率提升2.3%，响应长度减少47.61%。

Conclusion: LS-Mixture SFT有效赋予非推理模型推理能力，同时避免“过度思考”，实现高效推理。

Abstract: Recent advances in large language models have demonstrated that Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from
large reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning
capabilities to non-reasoning models. However, models fine-tuned with this
approach inherit the "overthinking" problem from teacher models, producing
verbose and redundant reasoning chains during inference. To address this
challenge, we propose \textbf{L}ong-\textbf{S}hort Chain-of-Thought
\textbf{Mixture} \textbf{S}upervised \textbf{F}ine-\textbf{T}uning
(\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their
short counterparts obtained through structure-preserved rewriting. Our
experiments demonstrate that models trained using the LS-Mixture SFT method,
compared to those trained with direct SFT, achieved an average accuracy
improvement of 2.3\% across various benchmarks while substantially reducing
model response length by approximately 47.61\%. This work offers an approach to
endow non-reasoning models with reasoning capabilities through supervised
fine-tuning while avoiding the inherent overthinking problems inherited from
teacher models, thereby enabling efficient reasoning in the fine-tuned models.

</details>

### [98] [Evaluation of LLMs on Long-tail Entity Linking in Historical Documents](https://arxiv.org/abs/2505.03473)
*Marta Boscariol,Luana Bulla,Lia Draetta,Beatrice Fiumanò,Emanuele Lenzi,Leonardo Piano*

Main category: cs.CL

TLDR: 论文研究了LLMs（如GPT和LLama3）在长尾实体链接任务中的表现，发现其性能优于传统方法，但仍面临长尾实体数据不足的挑战。


<details>
  <summary>Details</summary>
Motivation: 长尾实体链接是NLP中一个未充分研究的问题，LLMs因其深度上下文理解能力有望解决这一问题。

Method: 使用MHERCL v0.1基准测试，比较LLMs与ReLiK框架在长尾实体链接任务中的表现。

Result: 初步实验显示LLMs在长尾实体链接中表现良好，可作为补充技术。

Conclusion: LLMs在长尾实体链接中具有潜力，但仍需进一步研究以解决数据不足问题。

Abstract: Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)
applications, enabling the disambiguation of entity mentions by linking them to
their corresponding entries in a reference knowledge base (KB). Thanks to their
deep contextual understanding capabilities, LLMs offer a new perspective to
tackle EL, promising better results than traditional methods. Despite the
impressive generalization capabilities of LLMs, linking less popular, long-tail
entities remains challenging as these entities are often underrepresented in
training data and knowledge bases. Furthermore, the long-tail EL task is an
understudied problem, and limited studies address it with LLMs. In the present
work, we assess the performance of two popular LLMs, GPT and LLama3, in a
long-tail entity linking scenario. Using MHERCL v0.1, a manually annotated
benchmark of sentences from domain-specific historical texts, we quantitatively
compare the performance of LLMs in identifying and linking entities to their
corresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity
Linking and Relation Extraction framework. Our preliminary experiments reveal
that LLMs perform encouragingly well in long-tail EL, indicating that this
technology can be a valuable adjunct in filling the gap between head and
long-tail EL.

</details>

### [99] [Sentence Embeddings as an intermediate target in end-to-end summarisation](https://arxiv.org/abs/2505.03481)
*Maciej Zembrzuski,Saad Mahamood*

Main category: cs.CL

TLDR: 提出了一种结合抽取式和生成式方法的新方法，利用预训练句子嵌入提升大规模输入数据集摘要质量。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法在处理大规模输入数据集时表现不佳，需改进内容选择策略。

Method: 结合抽取式方法、预训练句子嵌入和生成式模型，预测摘要句子嵌入而非概率分布。

Result: 新方法在大规模输入数据集上优于现有方法，提升摘要质量。

Conclusion: 预测句子嵌入比传统概率分布方法更适合松散对齐的源-目标语料库。

Abstract: Current neural network-based methods to the problem of document summarisation
struggle when applied to datasets containing large inputs. In this paper we
propose a new approach to the challenge of content-selection when dealing with
end-to-end summarisation of user reviews of accommodations. We show that by
combining an extractive approach with externally pre-trained sentence level
embeddings in an addition to an abstractive summarisation model we can
outperform existing methods when this is applied to the task of summarising a
large input dataset. We also prove that predicting sentence level embedding of
a summary increases the quality of an end-to-end system for loosely aligned
source to target corpora, than compared to commonly predicting probability
distributions of sentence selection.

</details>

### [100] [Faster MoE LLM Inference for Extremely Large Models](https://arxiv.org/abs/2505.03531)
*Haoqi Yang,Luohe Shi,Qiwei Li,Zuchao Li,Ping Wang,Bo Du,Mengjia Shen,Hai Zhao*

Main category: cs.CL

TLDR: 稀疏混合专家（MoE）大语言模型（LLM）正成为超大规模模型的主流方法。本文探讨了细粒度MoE模型在不同服务负载下的效率动态，并研究了减少路由专家数量对效率与性能的影响。


<details>
  <summary>Details</summary>
Motivation: 随着DeepSeek模型的兴起，细粒度MoE模型逐渐流行，但相关研究有限。本文旨在填补这一空白，探讨其效率动态及优化潜力。

Method: 通过分析细粒度MoE模型在不同服务负载下的表现，研究减少激活专家和总专家数量对效率与性能的影响。

Result: 减少激活专家数量可在某些场景显著提升效率且性能损失较小；减少总专家数量效率提升有限但性能损失严重。方法可实现至少10%的吞吐量提升且无性能损失。

Conclusion: MoE推理优化仍有巨大探索空间，细粒度模型为效率与性能的权衡提供了新思路。

Abstract: Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually
becoming the mainstream approach for ultra-large-scale models. Existing
optimization efforts for MoE models have focused primarily on coarse-grained
MoE architectures. With the emergence of DeepSeek Models, fine-grained MoE
models are gaining popularity, yet research on them remains limited. Therefore,
we want to discuss the efficiency dynamic under different service loads.
Additionally, fine-grained models allow deployers to reduce the number of
routed experts, both activated counts and total counts, raising the question of
how this reduction affects the trade-off between MoE efficiency and
performance. Our findings indicate that while deploying MoE models presents
greater challenges, it also offers significant optimization opportunities.
Reducing the number of activated experts can lead to substantial efficiency
improvements in certain scenarios, with only minor performance degradation.
Reducing the total number of experts provides limited efficiency gains but
results in severe performance degradation. Our method can increase throughput
by at least 10\% without any performance degradation. Overall, we conclude that
MoE inference optimization remains an area with substantial potential for
exploration and improvement.

</details>

### [101] [Say It Another Way: A Framework for User-Grounded Paraphrasing](https://arxiv.org/abs/2505.03563)
*Cléa Chataigner,Rebecca Ma,Prakhar Ganesh,Afaf Taïk,Elliot Creager,Golnoosh Farnadi*

Main category: cs.CL

TLDR: 论文研究了提示词微小变化对大型语言模型（LLM）行为的影响，提出了基于语言转换的框架生成自然提示变体，并验证了其对模型评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 提示词的微小变化可能导致LLM行为显著差异，但目前研究未能捕捉真实语言中的自然变化，因此需要系统性方法评估其影响。

Method: 提出基于语言转换分类的框架，生成自然提示变体，并通过BBQ数据集结合人工标注和自动化检查验证。

Result: 研究发现即使是细微的提示修改也会显著改变模型行为，尤其在刻板印象评估任务中。

Conclusion: 研究强调了需要开发对提示词变化敏感的稳健评估协议。

Abstract: Small changes in how a prompt is worded can lead to meaningful differences in
the behavior of large language models (LLMs), raising concerns about the
stability and reliability of their evaluations. While prior work has explored
simple formatting changes, these rarely capture the kinds of natural variation
seen in real-world language use. We propose a controlled paraphrasing framework
based on a taxonomy of minimal linguistic transformations to systematically
generate natural prompt variations. Using the BBQ dataset, we validate our
method with both human annotations and automated checks, then use it to study
how LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our
analysis shows that even subtle prompt modifications can lead to substantial
changes in model behavior. These results highlight the need for robust,
paraphrase-aware evaluation protocols.

</details>

### [102] [Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure](https://arxiv.org/abs/2505.03675)
*Anuja Tayal,Devika Salunke,Barbara Di Eugenio,Paula G Allen-Meares,Eulalia P Abril,Olga Garcia-Bedoya,Carolyn A Dickens,Andrew D. Boyd*

Main category: cs.CL

TLDR: 研究探讨了ChatGPT（3.5-turbo和4）为非洲裔美国心衰患者生成自我护理对话的潜力，发现有效提示设计是关键，但ChatGPT仍缺乏共情和互动性。


<details>
  <summary>Details</summary>
Motivation: 针对非洲裔美国心衰患者自我护理领域缺乏专业数据集的问题，探索ChatGPT生成对话的适用性。

Method: 采用四种提示策略（领域、AAVE、SDOH、SDOH推理）生成对话，涵盖饮食、运动和液体摄入，并结合患者特定SDOH属性。

Result: SDOH和推理提示提升了对话质量，但ChatGPT在共情和互动性方面仍有不足。

Conclusion: 提示设计对生成高质量对话至关重要，但需进一步改进以增强ChatGPT在医疗沟通中的实用性。

Abstract: We explore the potential of ChatGPT (3.5-turbo and 4) to generate
conversations focused on self-care strategies for African-American heart
failure patients -- a domain with limited specialized datasets. To simulate
patient-health educator dialogues, we employed four prompting strategies:
domain, African American Vernacular English (AAVE), Social Determinants of
Health (SDOH), and SDOH-informed reasoning. Conversations were generated across
key self-care domains of food, exercise, and fluid intake, with varying turn
lengths (5, 10, 15) and incorporated patient-specific SDOH attributes such as
age, gender, neighborhood, and socioeconomic status. Our findings show that
effective prompt design is essential. While incorporating SDOH and reasoning
improves dialogue quality, ChatGPT still lacks the empathy and engagement
needed for meaningful healthcare communication.

</details>

### [103] [IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages](https://arxiv.org/abs/2505.03688)
*Sharvi Endait,Ruturaj Ghatage,Aditya Kulkarni,Rajlaxmi Patil,Raviraj Joshi*

Main category: cs.CL

TLDR: 本文介绍了IndicSQuAD，一个涵盖九种主要印度语言的多语言抽取式问答数据集，旨在解决印度语言在问答系统中的资源不足问题。


<details>
  <summary>Details</summary>
Motivation: 高资源语言在问答系统领域进展迅速，而印度语言尽管拥有大量母语使用者，却资源匮乏。本文旨在填补这一空白。

Method: 通过翻译技术从SQuAD数据集衍生出IndicSQuAD，确保语言保真度和答案跨度对齐，并为每种语言提供训练、验证和测试集。

Result: 使用单语和多语BERT模型评估，结果显示低资源环境下的挑战，并提出了未来研究方向。

Conclusion: IndicSQuAD为印度语言的问答系统研究提供了基础，未来可扩展至更多语言和领域。

Abstract: The rapid progress in question-answering (QA) systems has predominantly
benefited high-resource languages, leaving Indic languages largely
underrepresented despite their vast native speaker base. In this paper, we
present IndicSQuAD, a comprehensive multi-lingual extractive QA dataset
covering nine major Indic languages, systematically derived from the SQuAD
dataset. Building on previous work with MahaSQuAD for Marathi, our approach
adapts and extends translation techniques to maintain high linguistic fidelity
and accurate answer-span alignment across diverse languages. IndicSQuAD
comprises extensive training, validation, and test sets for each language,
providing a robust foundation for model development. We evaluate baseline
performances using language-specific monolingual BERT models and the
multilingual MuRIL-BERT. The results indicate some challenges inherent in
low-resource settings. Moreover, our experiments suggest potential directions
for future work, including expanding to additional languages, developing
domain-specific datasets, and incorporating multimodal data. The dataset and
models are publicly shared at https://github.com/l3cube-pune/indic-nlp

</details>

### [104] [NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation](https://arxiv.org/abs/2505.03711)
*Baharul Islam,Nasim Ahmad,Ferdous Ahmed Barbhuiya,Kuntal Dey*

Main category: cs.CL

TLDR: 论文介绍了SemEval 2025任务5的系统提交，专注于英德学术领域的跨语言主题分类，采用双语数据训练、负采样和基于边界的检索目标，展示了低维自注意力机制的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决跨语言主题分类问题，特别是在资源受限的情况下，提升主题检索的性能。

Method: 使用双语数据训练，结合负采样和基于边界的检索目标，设计低维自注意力机制编码句子嵌入。

Result: 系统在定量评估中平均召回率为32.24%，定性评估中为43.16%和31.53%，性能竞争性强且GPU使用低。

Conclusion: 方法在资源受限下有效捕获主题信息，但仍有改进空间。

Abstract: We present our system submission for SemEval 2025 Task 5, which focuses on
cross-lingual subject classification in the English and German academic
domains. Our approach leverages bilingual data during training, employing
negative sampling and a margin-based retrieval objective. We demonstrate that a
dimension-as-token self-attention mechanism designed with significantly reduced
internal dimensions can effectively encode sentence embeddings for subject
retrieval. In quantitative evaluation, our system achieved an average recall
rate of 32.24% in the general quantitative setting (all subjects), 43.16% and
31.53% of the general qualitative evaluation methods with minimal GPU usage,
highlighting their competitive performance. Our results demonstrate that our
approach is effective in capturing relevant subject information under resource
constraints, although there is still room for improvement.

</details>

### [105] [WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch](https://arxiv.org/abs/2505.03733)
*Zimu Lu,Yunqiao Yang,Houxing Ren,Haotian Hou,Han Xiao,Ke Wang,Weikang Shi,Aojun Zhou,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TLDR: WebGen-Bench是一个新基准，用于评估LLM代理从零生成多文件网站代码的能力，包含多样化的指令和647个测试用例。最佳模型组合的准确率仅为27.8%，显示其挑战性。


<details>
  <summary>Details</summary>
Motivation: 评估LLM代理在复杂代码库中生成和管理代码的能力，尤其是创建多文件网站代码库。

Method: 通过人类和GPT-4o合作生成多样化指令，构建测试用例并自动化测试。评估三种代码代理框架和多种LLM。

Result: 最佳组合（Bolt.diy + DeepSeek-R1）准确率为27.8%。训练后的Qwen2.5-Coder-32B-Instruct达到38.2%，优于专有模型。

Conclusion: WebGen-Bench具有挑战性，但通过训练可以提升性能，展示了LLM代理在代码生成中的潜力。

Abstract: LLM-based agents have demonstrated great potential in generating and managing
code within complex codebases. In this paper, we introduce WebGen-Bench, a
novel benchmark designed to measure an LLM-based agent's ability to create
multi-file website codebases from scratch. It contains diverse instructions for
website generation, created through the combined efforts of human annotators
and GPT-4o. These instructions span three major categories and thirteen minor
categories, encompassing nearly all important types of web applications. To
assess the quality of the generated websites, we use GPT-4o to generate test
cases targeting each functionality described in the instructions, and then
manually filter, adjust, and organize them to ensure accuracy, resulting in 647
test cases. Each test case specifies an operation to be performed on the
website and the expected result after the operation. To automate testing and
improve reproducibility, we employ a powerful web-navigation agent to execute
tests on the generated websites and determine whether the observed responses
align with the expected results. We evaluate three high-performance code-agent
frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and
open-source LLMs as engines. The best-performing combination, Bolt.diy powered
by DeepSeek-R1, achieves only 27.8\% accuracy on the test cases, highlighting
the challenging nature of our benchmark. Additionally, we construct
WebGen-Instruct, a training set consisting of 6,667 website-generation
instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories
generated from a subset of this training set achieves an accuracy of 38.2\%,
surpassing the performance of the best proprietary model.

</details>

### [106] [VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model](https://arxiv.org/abs/2505.03739)
*Zuwei Long,Yunhang Shen,Chaoyou Fu,Heting Gao,Lijiang Li,Peixian Chen,Mengdan Zhang,Hang Shao,Jian Li,Jinlong Peng,Haoyu Cao,Ke Li,Rongrong Ji,Xing Sun*

Main category: cs.CL

TLDR: VITA-Audio是一个端到端的大型语音模型，通过轻量级多模态令牌预测模块和四阶段渐进训练策略，显著降低流式场景下的首音频生成延迟，实现3~5倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有语音模型在流式场景下生成首音频时存在高延迟问题，限制了实际部署。

Method: 提出轻量级多模态令牌预测模块（MCTP）和四阶段渐进训练策略，以加速推理并减少延迟。

Result: 在7B参数规模下实现3~5倍推理加速，并在ASR、TTS和SQA任务上优于同类开源模型。

Conclusion: VITA-Audio是首个能在首次前向传递中生成音频的多模态大模型，具备低延迟实时对话能力。

Abstract: With the growing requirement for natural human-computer interaction,
speech-based systems receive increasing attention as speech is one of the most
common forms of daily communication. However, the existing speech models still
experience high latency when generating the first audio token during streaming,
which poses a significant bottleneck for deployment. To address this issue, we
propose VITA-Audio, an end-to-end large speech model with fast audio-text token
generation. Specifically, we introduce a lightweight Multiple Cross-modal Token
Prediction (MCTP) module that efficiently generates multiple audio tokens
within a single model forward pass, which not only accelerates the inference
but also significantly reduces the latency for generating the first audio in
streaming scenarios. In addition, a four-stage progressive training strategy is
explored to achieve model acceleration with minimal loss of speech quality. To
our knowledge, VITA-Audio is the first multi-modal large language model capable
of generating audio output during the first forward pass, enabling real-time
conversational capabilities with minimal latency. VITA-Audio is fully
reproducible and is trained on open-source data only. Experimental results
demonstrate that our model achieves an inference speedup of 3~5x at the 7B
parameter scale, but also significantly outperforms open-source models of
similar model size on multiple benchmarks for automatic speech recognition
(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [107] [Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach](https://arxiv.org/abs/2505.02952)
*Fabrizio Marozzo*

Main category: cs.AI

TLDR: 提出一种通过结构化澄清问题逐步消除自然语言指令模糊性的方法，相比传统一次性解决方案更准确、高效且用户满意度更高。


<details>
  <summary>Details</summary>
Motivation: 自然语言的模糊性导致用户需多次测试和修正提示，影响效率。

Method: 通过结构化澄清问题和替代方案提案逐步消除模糊性，辅以输入/输出示例。

Result: 在多种任务中表现优于传统方法，准确性更高、解决时间更短、用户满意度更高。

Conclusion: 该方法能有效减少迭代次数，提升生成AI系统的实用性和用户体验。

Abstract: Generative AI systems have revolutionized human interaction by enabling
natural language-based coding and problem solving. However, the inherent
ambiguity of natural language often leads to imprecise instructions, forcing
users to iteratively test, correct, and resubmit their prompts. We propose an
iterative approach that systematically narrows down these ambiguities through a
structured series of clarification questions and alternative solution
proposals, illustrated with input/output examples as well. Once every
uncertainty is resolved, a final, precise solution is generated. Evaluated on a
diverse dataset spanning coding, data analysis, and creative writing, our
method demonstrates superior accuracy, competitive resolution times, and higher
user satisfaction compared to conventional one-shot solutions, which typically
require multiple manual iterations to achieve a correct output.

</details>

### [108] [The Multimodal Paradox: How Added and Missing Modalities Shape Bias and Performance in Multimodal AI](https://arxiv.org/abs/2505.03020)
*Kishore Sampath,Pratheesh,Ayaazuddin Mohammad,Resmi Ramachandranpillai*

Main category: cs.AI

TLDR: 多模态学习通过整合多种数据源（如图像、文本和结构化数据）在高风险决策中表现优于单模态方法。本文探讨了两个关键问题：新模态是否提升性能并影响公平性，以及模态缺失对模型性能和公平性的影响。实验表明，新模态提升性能但公平性结果不一，模态缺失会降低模型表现。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态学习在性能上表现优越，但其在公平性和鲁棒性方面的研究不足，尤其是在模态缺失的情况下。

Method: 通过多模态医疗数据集（包含图像、时间序列和结构化信息）进行实验，分析新模态对性能和公平性的影响，以及模态缺失时的表现。

Result: 新模态一致提升性能，但公平性结果因评估指标和数据集而异；模态缺失会降低性能和公平性。

Conclusion: 多模态学习需关注公平性和鲁棒性，尤其是在模态缺失的实际应用中。

Abstract: Multimodal learning, which integrates diverse data sources such as images,
text, and structured data, has proven superior to unimodal counterparts in
high-stakes decision-making. However, while performance gains remain the gold
standard for evaluating multimodal systems, concerns around bias and robustness
are frequently overlooked. In this context, this paper explores two key
research questions (RQs): (i) RQ1 examines whether adding a modality
con-sistently enhances performance and investigates its role in shaping
fairness measures, assessing whether it mitigates or amplifies bias in
multimodal models; (ii) RQ2 investigates the impact of missing modalities at
inference time, analyzing how multimodal models generalize in terms of both
performance and fairness. Our analysis reveals that incorporating new
modalities during training consistently enhances the performance of multimodal
models, while fairness trends exhibit variability across different evaluation
measures and datasets. Additionally, the absence of modalities at inference
degrades performance and fairness, raising concerns about its robustness in
real-world deployment. We conduct extensive experiments using multimodal
healthcare datasets containing images, time series, and structured information
to validate our findings.

</details>

### [109] [Evaluating the Impact of AI-Powered Audiovisual Personalization on Learner Emotion, Focus, and Learning Outcomes](https://arxiv.org/abs/2505.03033)
*George Xi Wang,Jingying Deng,Safinah Ali*

Main category: cs.AI

TLDR: 论文提出了一种基于大语言模型（LLM）的个性化多感官学习环境系统，旨在通过定制视听元素提升学习者的专注力和情绪稳定性。


<details>
  <summary>Details</summary>
Motivation: 独立学习者在非结构化或干扰环境中难以保持专注和情绪调节，现有教育技术忽视了学习的情感与感官背景。

Method: 利用LLM生成个性化视听学习环境，结合生物识别和绩效数据评估效果。

Result: 研究发现个性化视听组合对认知负荷和参与度有显著影响。

Conclusion: 研究推动了情感响应教育技术的发展，并扩展了多模态LLM在自主学习感官维度的应用。

Abstract: Independent learners often struggle with sustaining focus and emotional
regulation in unstructured or distracting settings. Although some rely on
ambient aids such as music, ASMR, or visual backgrounds to support
concentration, these tools are rarely integrated into cohesive,
learner-centered systems. Moreover, existing educational technologies focus
primarily on content adaptation and feedback, overlooking the emotional and
sensory context in which learning takes place. Large language models have
demonstrated powerful multimodal capabilities including the ability to generate
and adapt text, audio, and visual content. Educational research has yet to
fully explore their potential in creating personalized audiovisual learning
environments. To address this gap, we introduce an AI-powered system that uses
LLMs to generate personalized multisensory study environments. Users select or
generate customized visual themes (e.g., abstract vs. realistic, static vs.
animated) and auditory elements (e.g., white noise, ambient ASMR, familiar vs.
novel sounds) to create immersive settings aimed at reducing distraction and
enhancing emotional stability. Our primary research question investigates how
combinations of personalized audiovisual elements affect learner cognitive load
and engagement. Using a mixed-methods design that incorporates biometric
measures and performance outcomes, this study evaluates the effectiveness of
LLM-driven sensory personalization. The findings aim to advance emotionally
responsive educational technologies and extend the application of multimodal
LLMs into the sensory dimension of self-directed learning.

</details>

### [110] [BLAB: Brutally Long Audio Bench](https://arxiv.org/abs/2505.03054)
*Orevaoghene Ahia,Martijn Bartelds,Kabir Ahuja,Hila Gonen,Valentin Hofmann,Siddhant Arora,Shuyue Stella Li,Vishal Puttagunta,Mofetoluwa Adeyemi,Charishma Buchireddy,Ben Walls,Noah Bennett,Shinji Watanabe,Noah A. Smith,Yulia Tsvetkov,Sachin Kumar*

Main category: cs.AI

TLDR: BLAB是一个用于评估音频语言模型（LMs）在长音频任务（如定位、情感识别等）上性能的基准测试，结果显示现有模型在长音频任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 开发能够理解多样化语音交互的大型音频语言模型，以提升语言技术的可访问性，但目前模型主要针对短音频评估，缺乏对长对话语音的研究。

Method: 引入BLAB基准测试，包含833+小时的长音频片段（平均51分钟）及人工标注的问题和答案，评估了六种开源和专有音频LMs。

Result: 所有模型（包括Gemini 2.0 Pro和GPT-4o）在BLAB任务中表现不佳，性能随音频时长增加而下降，尤其在定位、时间推理和计数任务上表现较差。

Conclusion: BLAB为开发具有长音频理解能力的音频LMs提供了挑战性评估框架，揭示了模型在长音频任务上的局限性。

Abstract: Developing large audio language models (LMs) capable of understanding diverse
spoken interactions is essential for accommodating the multimodal nature of
human communication and can increase the accessibility of language technologies
across different user populations. Recent work on audio LMs has primarily
evaluated their performance on short audio segments, typically under 30
seconds, with limited exploration of long-form conversational speech segments
that more closely reflect natural user interactions with these models. We
introduce Brutally Long Audio Bench (BLAB), a challenging long-form audio
benchmark that evaluates audio LMs on localization, duration estimation,
emotion, and counting tasks using audio segments averaging 51 minutes in
length. BLAB consists of 833+ hours of diverse, full-length audio clips, each
paired with human-annotated, text-based natural language questions and answers.
Our audio data were collected from permissively licensed sources and underwent
a human-assisted filtering process to ensure task compliance. We evaluate six
open-source and proprietary audio LMs on BLAB and find that all of them,
including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the
tasks in BLAB. Our comprehensive analysis reveals key insights into the
trade-offs between task difficulty and audio duration. In general, we find that
audio LMs struggle with long-form speech, with performance declining as
duration increases. They perform poorly on localization, temporal reasoning,
counting, and struggle to understand non-phonemic information, relying more on
prompts than audio content. BLAB serves as a challenging evaluation framework
to develop audio LMs with robust long-form audio understanding capabilities.

</details>

### [111] [Is AI currently capable of identifying wild oysters? A comparison of human annotators against the AI model, ODYSSEE](https://arxiv.org/abs/2505.03108)
*Brendan Campbell,Alan Williams,Kleio Baxevani,Alyssa Campbell,Rushabh Dhoke,Rileigh E. Hudock,Xiaomin Lin,Vivek Mange,Bernhard Neuberger,Arjun Suresh,Alhim Vera,Arthur Trembanis,Herbert G. Tanner,Edward Hale*

Main category: cs.AI

TLDR: 论文提出了一种基于深度学习的ODYSSEE模型，用于通过图像或视频识别活牡蛎，但其准确性（63%）低于专家（74%）和非专家（75%），且图像质量对结果有显著影响。未来需改进模型训练以提高预测能力。


<details>
  <summary>Details</summary>
Motivation: 当前牡蛎礁监测方法破坏性强且耗时，不适用于小规模或敏感环境，因此开发了ODYSSEE模型以提供非破坏性、高效的替代方案。

Method: 使用深度学习技术分析牡蛎礁图像或视频，识别活牡蛎，并与专家和非专家的标注结果进行比较。

Result: 模型推断速度显著快于人工（39.6秒 vs. 数小时），但准确性较低（63% vs. 74-75%），且图像质量影响结果。

Conclusion: ODYSSEE模型目前准确性不足，但通过改进训练数据（如高质量图像和更多标注类别）有望提升性能。未来需优化活体与死体牡蛎的检测方法。

Abstract: Oysters are ecologically and commercially important species that require
frequent monitoring to track population demographics (e.g. abundance, growth,
mortality). Current methods of monitoring oyster reefs often require
destructive sampling methods and extensive manual effort. Therefore, they are
suboptimal for small-scale or sensitive environments. A recent alternative, the
ODYSSEE model, was developed to use deep learning techniques to identify live
oysters using video or images taken in the field of oyster reefs to assess
abundance. The validity of this model in identifying live oysters on a reef was
compared to expert and non-expert annotators. In addition, we identified
potential sources of prediction error. Although the model can make inferences
significantly faster than expert and non-expert annotators (39.6 s, $2.34 \pm
0.61$ h, $4.50 \pm 1.46$ h, respectively), the model overpredicted the number
of live oysters, achieving lower accuracy (63\%) in identifying live oysters
compared to experts (74\%) and non-experts (75\%) alike. Image quality was an
important factor in determining the accuracy of the model and the annotators.
Better quality images improved human accuracy and worsened model accuracy.
Although ODYSSEE was not sufficiently accurate, we anticipate that future
training on higher-quality images, utilizing additional live imagery, and
incorporating additional annotation training classes will greatly improve the
model's predictive power based on the results of this analysis. Future research
should address methods that improve the detection of living vs. dead oysters.

</details>

### [112] [Holmes: Automated Fact Check with Large Language Models](https://arxiv.org/abs/2505.03135)
*Haoran Ou,Gelei Deng,Xingshuo Han,Jie Zhang,Xinlei He,Han Qiu,Shangwei Guo,Tianwei Zhang*

Main category: cs.AI

TLDR: 本文提出了一种名为Holmes的端到端框架，利用大型语言模型（LLMs）结合新型证据检索方法，显著提升了多模态虚假信息的检测准确性。


<details>
  <summary>Details</summary>
Motivation: 互联网的普及加速了虚假信息的传播，威胁社会信任和国家安全。传统深度学习方法难以应对多模态虚假信息的复杂性，因此探索LLMs的潜力成为研究重点。

Method: 研究采用LLMs进行虚假信息检测，发现其单独使用时效果有限，但结合高质量证据可显著提升性能。为此，提出Holmes框架，包含LLM驱动的摘要提取和新型证据质量评估算法。

Result: Holmes在两个开源数据集上达到88.3%的准确率，实时验证任务中达90.2%。证据检索改进使事实核查准确率比现有方法提升30.8%。

Conclusion: Holmes通过结合LLMs和高质量证据检索，有效解决了多模态虚假信息检测的挑战，为未来研究提供了实用框架。

Abstract: The rise of Internet connectivity has accelerated the spread of
disinformation, threatening societal trust, decision-making, and national
security. Disinformation has evolved from simple text to complex multimodal
forms combining images and text, challenging existing detection methods.
Traditional deep learning models struggle to capture the complexity of
multimodal disinformation. Inspired by advances in AI, this study explores
using Large Language Models (LLMs) for automated disinformation detection. The
empirical study shows that (1) LLMs alone cannot reliably assess the
truthfulness of claims; (2) providing relevant evidence significantly improves
their performance; (3) however, LLMs cannot autonomously search for accurate
evidence. To address this, we propose Holmes, an end-to-end framework featuring
a novel evidence retrieval method that assists LLMs in collecting high-quality
evidence. Our approach uses (1) LLM-powered summarization to extract key
information from open sources and (2) a new algorithm and metrics to evaluate
evidence quality. Holmes enables LLMs to verify claims and generate
justifications effectively. Experiments show Holmes achieves 88.3% accuracy on
two open-source datasets and 90.2% in real-time verification tasks. Notably,
our improved evidence retrieval boosts fact-checking accuracy by 30.8% over
existing methods

</details>

### [113] [CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics](https://arxiv.org/abs/2505.03171)
*Junqi Liu,Xiaohan Lin,Jonas Bayer,Yael Dillies,Weijie Jiang,Xiaodan Liang,Roman Soletskyi,Haiming Wang,Yunzhou Xie,Beibei Xiong,Zhengfeng Yang,Jujian Zhang,Lihong Zhi,Jia Li,Zhengying Liu*

Main category: cs.AI

TLDR: 论文介绍了CombiBench，一个包含100个组合问题的基准测试集，用于评估大型语言模型在组合数学中的表现，并提出了Fine-Eval评估框架。


<details>
  <summary>Details</summary>
Motivation: 组合数学领域缺乏合适的基准测试和定理库，限制了神经符号方法在该领域的发展。

Method: 提出CombiBench基准测试集和Fine-Eval评估框架，使用Lean~4形式化问题，并测试多个LLM的表现。

Result: 测试的LLM在组合数学问题上的表现有限，Kimina-Prover表现最佳，解决了7个问题。

Conclusion: CombiBench填补了组合数学领域的空白，为未来研究提供了标准化评估工具。

Abstract: Neurosymbolic approaches integrating large language models with formal
reasoning have recently achieved human-level performance on mathematics
competition problems in algebra, geometry and number theory. In comparison,
combinatorics remains a challenging domain, characterized by a lack of
appropriate benchmarks and theorem libraries. To address this gap, we introduce
CombiBench, a comprehensive benchmark comprising 100 combinatorial problems,
each formalized in Lean~4 and paired with its corresponding informal statement.
The problem set covers a wide spectrum of difficulty levels, ranging from
middle school to IMO and university level, and span over ten combinatorial
topics. CombiBench is suitable for testing IMO solving capabilities since it
includes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its
statement contain an images). Furthermore, we provide a comprehensive and
standardized evaluation framework, dubbed Fine-Eval (for
$\textbf{F}$ill-in-the-blank $\textbf{in}$ L$\textbf{e}$an Evaluation), for
formal mathematics. It accommodates not only proof-based problems but also, for
the first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval
as the evaluation method and Kimina Lean Server as the backend, we benchmark
several LLMs on CombiBench and observe that their capabilities for formally
solving combinatorial problems remain limited. Among all models tested (none of
which has been trained for this particular task), Kimina-Prover attains the
best results, solving 7 problems (out of 100) under both ``with solution'' and
``without solution'' scenarios. We open source the benchmark dataset alongside
with the code of the proposed evaluation method at
https://github.com/MoonshotAI/CombiBench/.

</details>

### [114] [Patterns and Mechanisms of Contrastive Activation Engineering](https://arxiv.org/abs/2505.03189)
*Yixiong Hao,Ayush Panda,Stepan Shabalin,Sheikh Abdur Raheem Ali*

Main category: cs.AI

TLDR: 对比激活工程（CAE）是一种零成本、推理时调整LLM行为的新方法，但仅适用于分布内场景，且存在样本数量收益递减、对抗性输入易攻击、模型困惑度下降等问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM行为控制的复杂性和不透明性问题，探索更灵活的任务特定调整方法。

Method: 使用CAE技术，通过修改内部表示来引导LLM输出，并在分布内外场景中评估其性能。

Result: CAE在分布内有效，样本数量收益递减（约80个样本），易受对抗性输入影响，降低模型困惑度，大模型对调整更稳健。

Conclusion: CAE是一种有潜力的方法，但需注意其局限性和适用场景，未来需进一步优化部署指南。

Abstract: Controlling the behavior of Large Language Models (LLMs) remains a
significant challenge due to their inherent complexity and opacity. While
techniques like fine-tuning can modify model behavior, they typically require
extensive computational resources. Recent work has introduced a class of
contrastive activation engineering (CAE) techniques as promising approaches for
steering LLM outputs through targeted modifications to their internal
representations. Applied at inference-time with zero cost, CAE has the
potential to introduce a new paradigm of flexible, task-specific LLM behavior
tuning. We analyze the performance of CAE in in-distribution,
out-of-distribution settings, evaluate drawbacks, and begin to develop
comprehensive guidelines for its effective deployment. We find that 1. CAE is
only reliably effective when applied to in-distribution contexts. 2. Increasing
the number of samples used to generate steering vectors has diminishing returns
at around 80 samples. 3. Steering vectors are susceptible to adversarial inputs
that reverses the behavior that is steered for. 4. Steering vectors harm the
overall model perplexity. 5. Larger models are more resistant to
steering-induced degradation.

</details>

### [115] [RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation](https://arxiv.org/abs/2505.03275)
*Tiantian Gan,Qiyao Sun*

Main category: cs.AI

TLDR: RAG-MCP框架通过语义检索减少提示词数量，提升工具选择准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLMs）在外部工具使用中因提示词膨胀和选择复杂性导致的问题。

Method: 引入RAG-MCP框架，通过语义检索从外部索引中识别最相关的工具描述，仅将选中的工具传递给LLM。

Result: 实验显示，RAG-MCP显著减少提示词数量（如超过50%），并将工具选择准确性提升至43.13%（基线为13.62%）。

Conclusion: RAG-MCP为LLMs提供了可扩展且准确的工具集成方案。

Abstract: Large language models (LLMs) struggle to effectively utilize a growing number
of external tools, such as those defined by the Model Context Protocol
(MCP)\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We
introduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes
this challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to
identify the most relevant MCP(s) for a given query from an external index
before engaging the LLM. Only the selected tool descriptions are passed to the
model, drastically reducing prompt size and simplifying decision-making.
Experiments, including an MCP stress test, demonstrate RAG-MCP significantly
cuts prompt tokens (e.g., by over 50%) and more than triples tool selection
accuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables
scalable and accurate tool integration for LLMs.

</details>

### [116] [Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces](https://arxiv.org/abs/2505.03295)
*Luis Miguel Vieira da Silva,Aljosha Köcher,Nicolas König,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TLDR: 提出一种利用大语言模型根据自然语言输入生成符合能力定义的技能实现代码的方法，并通过检索增强生成架构整合现有软件库和接口技术。


<details>
  <summary>Details</summary>
Motivation: 开发符合能力定义的技能实现耗时且具有挑战性，需要一种更高效的方法。

Method: 将能力视为技能实现的契约，利用大语言模型生成代码，并通过检索增强生成架构整合现有库和接口。

Result: 在Python和ROS 2控制的自主移动机器人上验证了方法的可行性和灵活性。

Conclusion: 该方法为技能实现提供了一种高效且灵活的解决方案。

Abstract: Modern automation systems increasingly rely on modular architectures, with
capabilities and skills as one solution approach. Capabilities define the
functions of resources in a machine-readable form and skills provide the
concrete implementations that realize those capabilities. However, the
development of a skill implementation conforming to a corresponding capability
remains a time-consuming and challenging task. In this paper, we present a
method that treats capabilities as contracts for skill implementations and
leverages large language models to generate executable code based on natural
language user input. A key feature of our approach is the integration of
existing software libraries and interface technologies, enabling the generation
of skill implementations across different target languages. We introduce a
framework that allows users to incorporate their own libraries and resource
interfaces into the code generation process through a retrieval-augmented
generation architecture. The proposed method is evaluated using an autonomous
mobile robot controlled via Python and ROS 2, demonstrating the feasibility and
flexibility of the approach.

</details>

### [117] [Artificial Behavior Intelligence: Technology, Challenges, and Future Directions](https://arxiv.org/abs/2505.03315)
*Kanghyun Jo,Jehwan Choi,Kwanho Kim,Seongmin Kim,Duy-Linh Nguyen,Xuan-Thuy Vo,Adri Priadana,Tien-Dat Tran*

Main category: cs.AI

TLDR: 本文提出了一种名为人工行为智能（ABI）的技术框架，用于全面分析和解释人类行为，包括姿态、表情、情感、行为序列和上下文线索。


<details>
  <summary>Details</summary>
Motivation: 理解和预测人类行为在自动驾驶、智能医疗、监控系统和社交机器人等AI应用领域具有核心能力需求。

Method: ABI框架包括姿态估计、面部和情感识别、序列行为分析和上下文感知建模，并利用大规模预训练模型（如LLMs和视觉基础模型）提升行为识别的准确性和可解释性。

Result: 研究团队正在开发轻量级智能模型，以高效推断复杂人类行为，并探索优化策略（如轻量级Transformer和图架构）以应对技术挑战。

Conclusion: ABI在现实应用中面临数据有限、行为预测不确定性等挑战，团队正通过多模态知识蒸馏等方法推动其实际部署。

Abstract: Understanding and predicting human behavior has emerged as a core capability
in various AI application domains such as autonomous driving, smart healthcare,
surveillance systems, and social robotics. This paper defines the technical
framework of Artificial Behavior Intelligence (ABI), which comprehensively
analyzes and interprets human posture, facial expressions, emotions, behavioral
sequences, and contextual cues. It details the essential components of ABI,
including pose estimation, face and emotion recognition, sequential behavior
analysis, and context-aware modeling. Furthermore, we highlight the
transformative potential of recent advances in large-scale pretrained models,
such as large language models (LLMs), vision foundation models, and multimodal
integration models, in significantly improving the accuracy and
interpretability of behavior recognition. Our research team has a strong
interest in the ABI domain and is actively conducting research, particularly
focusing on the development of intelligent lightweight models capable of
efficiently inferring complex human behaviors. This paper identifies several
technical challenges that must be addressed to deploy ABI in real-world
applications including learning behavioral intelligence from limited data,
quantifying uncertainty in complex behavior prediction, and optimizing model
structures for low-power, real-time inference. To tackle these challenges, our
team is exploring various optimization strategies including lightweight
transformers, graph-based recognition architectures, energy-aware loss
functions, and multimodal knowledge distillation, while validating their
applicability in real-time environments.

</details>

### [118] [AI-Driven Scholarly Peer Review via Persistent Workflow Prompting, Meta-Prompting, and Meta-Reasoning](https://arxiv.org/abs/2505.03332)
*Evgeny Markhasin*

Main category: cs.AI

TLDR: 论文介绍了Persistent Workflow Prompting (PWP)，一种无需代码或API的提示工程方法，用于指导LLM完成复杂的科学评审任务。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在科学稿件评审中因数据限制和专家推理复杂性而面临的挑战。

Method: 通过分层模块化架构（Markdown结构化）和元提示技术，开发PWP提示，实现持久化工作流。

Result: PWP引导的LLM能够识别方法缺陷、减少输入偏见，并完成多模态分析等复杂任务。

Conclusion: PWP为复杂科学任务提供了透明且可复制的解决方案，展示了LLM在科学分析中的潜力。

Abstract: Critical peer review of scientific manuscripts presents a significant
challenge for Large Language Models (LLMs), partly due to data limitations and
the complexity of expert reasoning. This report introduces Persistent Workflow
Prompting (PWP), a potentially broadly applicable prompt engineering
methodology designed to bridge this gap using standard LLM chat interfaces
(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical
analysis of experimental chemistry manuscripts, featuring a hierarchical,
modular architecture (structured via Markdown) that defines detailed analysis
workflows. We develop this PWP prompt through iterative application of
meta-prompting techniques and meta-reasoning aimed at systematically codifying
expert review workflows, including tacit knowledge. Submitted once at the start
of a session, this PWP prompt equips the LLM with persistent workflows
triggered by subsequent queries, guiding modern reasoning LLMs through
systematic, multimodal evaluations. Demonstrations show the PWP-guided LLM
identifying major methodological flaws in a test case while mitigating LLM
input bias and performing complex tasks, including distinguishing claims from
evidence, integrating text/photo/figure analysis to infer parameters, executing
quantitative feasibility checks, comparing estimates against claims, and
assessing a priori plausibility. To ensure transparency and facilitate
replication, we provide full prompts, detailed demonstration analyses, and logs
of interactive chats as supplementary resources. Beyond the specific
application, this work offers insights into the meta-development process
itself, highlighting the potential of PWP, informed by detailed workflow
formalization, to enable sophisticated analysis using readily available LLMs
for complex scientific tasks.

</details>

### [119] [Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection](https://arxiv.org/abs/2505.03359)
*June-Woo Kim,Haram Yoon,Wonkyo Oh,Dawoon Jung,Sung-Hoon Yoon,Dae-Jin Kim,Dong-Ho Lee,Sang-Yeol Lee,Chan-Mo Yang*

Main category: cs.AI

TLDR: 本文提出了一种基于领域对抗训练的方法，通过将性别视为不同领域，改进语音AI模型在抑郁和PTSD检测中的性别偏见问题，并在E-DAIC数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 语音AI模型在抑郁和PTSD检测中存在性别偏见，导致预测不公和不准，本研究旨在解决这一问题。

Method: 采用领域对抗训练方法，将不同性别视为不同领域，并将其信息整合到预训练的语音基础模型中。

Result: 实验结果表明，该方法显著提升了检测性能，F1分数比基线提高了13.29个百分点。

Conclusion: 研究表明，解决人口统计学差异对AI驱动的心理健康评估至关重要。

Abstract: Speech-based AI models are emerging as powerful tools for detecting
depression and the presence of Post-traumatic stress disorder (PTSD), offering
a non-invasive and cost-effective way to assess mental health. However, these
models often struggle with gender bias, which can lead to unfair and inaccurate
predictions. In this study, our study addresses this issue by introducing a
domain adversarial training approach that explicitly considers gender
differences in speech-based depression and PTSD detection. Specifically, we
treat different genders as distinct domains and integrate this information into
a pretrained speech foundation model. We then validate its effectiveness on the
E-DAIC dataset to assess its impact on performance. Experimental results show
that our method notably improves detection performance, increasing the F1-score
by up to 13.29 percentage points compared to the baseline. This highlights the
importance of addressing demographic disparities in AI-driven mental health
assessment.

</details>

### [120] [Validating the Effectiveness of a Large Language Model-based Approach for Identifying Children's Development across Various Free Play Settings in Kindergarten](https://arxiv.org/abs/2505.03369)
*Yuanyuan Yang,Yuan Shen,Tianchen Sun,Yangbin Xie*

Main category: cs.AI

TLDR: 本研究提出了一种结合大型语言模型（LLMs）和学习分析的方法，通过分析儿童的游戏自述来评估其发展能力，结果显示该方法在多个领域准确率超过90%。


<details>
  <summary>Details</summary>
Motivation: 自由游戏对儿童发展至关重要，但传统评估方法难以全面捕捉其发展情况，需要更高效、准确的评估手段。

Method: 利用LLMs和学习分析技术分析儿童的游戏自述，计算不同游戏场景下的表现分数。

Result: 该方法在认知、运动和社交能力识别上准确率超过90%，且不同游戏场景对特定能力发展有显著影响。

Conclusion: 结合LLMs和学习分析的方法能有效评估儿童发展，为个性化学习和早期教育提供数据支持。

Abstract: Free play is a fundamental aspect of early childhood education, supporting
children's cognitive, social, emotional, and motor development. However,
assessing children's development during free play poses significant challenges
due to the unstructured and spontaneous nature of the activity. Traditional
assessment methods often rely on direct observations by teachers, parents, or
researchers, which may fail to capture comprehensive insights from free play
and provide timely feedback to educators. This study proposes an innovative
approach combining Large Language Models (LLMs) with learning analytics to
analyze children's self-narratives of their play experiences. The LLM
identifies developmental abilities, while performance scores across different
play settings are calculated using learning analytics techniques. We collected
2,224 play narratives from 29 children in a kindergarten, covering four
distinct play areas over one semester. According to the evaluation results from
eight professionals, the LLM-based approach achieved high accuracy in
identifying cognitive, motor, and social abilities, with accuracy exceeding 90%
in most domains. Moreover, significant differences in developmental outcomes
were observed across play settings, highlighting each area's unique
contributions to specific abilities. These findings confirm that the proposed
approach is effective in identifying children's development across various free
play settings. This study demonstrates the potential of integrating LLMs and
learning analytics to provide child-centered insights into developmental
trajectories, offering educators valuable data to support personalized learning
and enhance early childhood education practices.

</details>

### [121] [The Steganographic Potentials of Language Models](https://arxiv.org/abs/2505.03439)
*Artem Karpov,Tinuade Adeleke,Seong Hah Cho,Natalia Perez-Campanero*

Main category: cs.AI

TLDR: 论文探讨了通过强化学习微调的大语言模型（LLMs）在隐写术中的能力，发现当前模型在安全性和容量方面表现初级，但算法指导能显著提升其信息隐藏能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs隐藏信息的能力，以应对未对齐AI代理的检测和阻止，并探讨其对LLMs推理忠实性的影响。

Method: 通过强化学习微调LLMs，开发隐蔽编码方案，并在提示或非提示场景中测试其隐写术能力。

Result: 当前模型在隐写术方面表现初级，但算法指导能显著提升其信息隐藏能力。

Conclusion: LLMs具备初步隐写能力，但需进一步优化以提升安全性和容量。

Abstract: The potential for large language models (LLMs) to hide messages within plain
text (steganography) poses a challenge to detection and thwarting of unaligned
AI agents, and undermines faithfulness of LLMs reasoning. We explore the
steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)
to: (1) develop covert encoding schemes, (2) engage in steganography when
prompted, and (3) utilize steganography in realistic scenarios where hidden
reasoning is likely, but not prompted. In these scenarios, we detect the
intention of LLMs to hide their reasoning as well as their steganography
performance. Our findings in the fine-tuning experiments as well as in
behavioral non fine-tuning evaluations reveal that while current models exhibit
rudimentary steganographic abilities in terms of security and capacity,
explicit algorithmic guidance markedly enhances their capacity for information
concealment.

</details>

### [122] [Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in LLM-Based Agents](https://arxiv.org/abs/2505.03434)
*Schaun Wheeler,Olivier Jeunen*

Main category: cs.AI

TLDR: LLMs在AI领域取得重大成就，但其依赖程序性记忆限制了其在复杂环境中的应用。论文提出通过结合语义记忆和联想学习系统，构建模块化架构以提升适应性。


<details>
  <summary>Details</summary>
Motivation: LLMs在现实应用中面临复杂、不可预测环境的局限性，需突破其依赖程序性记忆的约束。

Method: 提出模块化架构，将语义记忆和联想学习系统与LLMs解耦，以增强适应性。

Result: 通过结合多种认知功能，LLMs可更好地适应现实世界的复杂问题。

Conclusion: 模块化架构是提升LLMs适应性的关键，未来需进一步探索其实现方式。

Abstract: Large Language Models (LLMs) represent a landmark achievement in Artificial
Intelligence (AI), demonstrating unprecedented proficiency in procedural tasks
such as text generation, code completion, and conversational coherence. These
capabilities stem from their architecture, which mirrors human procedural
memory -- the brain's ability to automate repetitive, pattern-driven tasks
through practice. However, as LLMs are increasingly deployed in real-world
applications, it becomes impossible to ignore their limitations operating in
complex, unpredictable environments. This paper argues that LLMs, while
transformative, are fundamentally constrained by their reliance on procedural
memory. To create agents capable of navigating ``wicked'' learning environments
-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must
augment LLMs with semantic memory and associative learning systems. By adopting
a modular architecture that decouples these cognitive functions, we can bridge
the gap between narrow procedural expertise and the adaptive intelligence
required for real-world problem-solving.

</details>

### [123] [am-ELO: A Stable Framework for Arena-based LLM Evaluation](https://arxiv.org/abs/2505.03475)
*Zirui Liu,Jiatong Li,Yan Zhuang,Qi Liu,Shuanghong Shen,Jie Ouyang,Mingyue Cheng,Shijin Wang*

Main category: cs.AI

TLDR: 论文提出了一种基于最大似然估计（MLE）的稳定竞技场框架（m-ELO和am-ELO），用于改进现有ELO评分系统的不稳定性和忽略标注者能力差异的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于ELO评分系统的框架存在排名不一致导致的稳定性问题，且未考虑标注者能力的差异。

Method: 采用最大似然估计（MLE）方法替代迭代更新，提出m-ELO；进一步改进为am-ELO，通过修改ELO评分的概率函数以纳入标注者能力。

Result: 实验证明该方法提高了稳定性，为大型语言模型（LLMs）提供了更鲁棒、准确和稳定的评估方法。

Conclusion: 提出的稳定竞技场框架有效解决了现有ELO评分系统的问题，为模型评估提供了更可靠的解决方案。

Abstract: Arena-based evaluation is a fundamental yet significant evaluation paradigm
for modern AI models, especially large language models (LLMs). Existing
framework based on ELO rating system suffers from the inevitable instability
problem due to ranking inconsistency and the lack of attention to the varying
abilities of annotators. In this paper, we introduce a novel stable arena
framework to address these issues by enhancing the ELO Rating System.
Specifically, we replace the iterative update method with a Maximum Likelihood
Estimation (MLE) approach, m-ELO, and provide theoretical proof of the
consistency and stability of the MLE approach for model ranking. Additionally,
we proposed the am-ELO, which modify the Elo Rating's probability function to
incorporate annotator abilities, enabling the simultaneous estimation of model
scores and annotator reliability. Experiments demonstrate that this method
ensures stability, proving that this framework offers a more robust, accurate,
and stable evaluation method for LLMs.

</details>

### [124] [STORY2GAME: Generating (Almost) Everything in an Interactive Fiction Game](https://arxiv.org/abs/2505.03547)
*Eric Zhou,Shreyas Basavatia,Moontashir Siam,Zexin Chen,Mark O. Riedl*

Main category: cs.AI

TLDR: STORY2GAME利用大型语言模型生成基于文本的互动小说游戏，通过生成故事、填充世界并构建游戏引擎中的动作代码，实现开放式故事生成和动态动作生成。


<details>
  <summary>Details</summary>
Motivation: 传统硬编码动作限制了故事生成的开放性，而动态生成动作可以更灵活地支持玩家的互动需求。

Method: 利用LLM生成动作的前置条件和效果，指导游戏引擎跟踪和修改游戏状态；动态生成新动作以支持玩家自定义行为。

Result: 评估了动作代码生成的成功率，确保玩家能完整互动体验生成的故事。

Conclusion: STORY2GAME通过动态动作生成和状态管理，实现了更开放且互动的故事游戏体验。

Abstract: We introduce STORY2GAME, a novel approach to using Large Language Models to
generate text-based interactive fiction games that starts by generating a
story, populates the world, and builds the code for actions in a game engine
that enables the story to play out interactively. Whereas a given set of
hard-coded actions can artificially constrain story generation, the ability to
generate actions means the story generation process can be more open-ended but
still allow for experiences that are grounded in a game state. The key to
successful action generation is to use LLM-generated preconditions and effects
of actions in the stories as guides for what aspects of the game state must be
tracked and changed by the game engine when a player performs an action. We
also introduce a technique for dynamically generating new actions to
accommodate the player's desire to perform actions that they think of that are
not part of the story. Dynamic action generation may require on-the-fly updates
to the game engine's state representation and revision of previously generated
actions. We evaluate the success rate of action code generation with respect to
whether a player can interactively play through the entire generated story.

</details>

### [125] [A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model Reasoning](https://arxiv.org/abs/2505.03553)
*Kolawole E. Ogunsina,Morayo A. Ogunsina*

Main category: cs.AI

TLDR: 提出了一种基于分布式账本技术的共识机制，用于验证和收敛不同大型语言模型的输出，减少不一致和幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在复杂请求下常产生不一致或错误的输出，影响AI系统的可靠性。

Method: 采用Hashgraph共识算法，通过gossip-about-gossip通信和虚拟投票实现模型间的共识。

Result: 原型系统通过迭代交换和更新答案，提高了输出的准确性和置信度。

Conclusion: 该机制为多智能体AI系统提供了一种自我验证和提升响应质量的新方向。

Abstract: Inconsistent outputs and hallucinations from large language models (LLMs) are
major obstacles to reliable AI systems. When different proprietary reasoning
models (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,
are given the same complex request, they often produce divergent results due to
variations in training and inference. This paper proposes a novel consensus
mechanism, inspired by distributed ledger technology, to validate and converge
these outputs, treating each RM as a black-box peer. Building on the Hashgraph
consensus algorithm, our approach employs gossip-about-gossip communication and
virtual voting to achieve agreement among an ensemble of RMs. We present an
architectural design for a prototype system in which RMs iteratively exchange
and update their answers, using information from each round to improve accuracy
and confidence in subsequent rounds. This approach goes beyond simple majority
voting by incorporating the knowledge and cross-verification content of every
model. We justify the feasibility of this Hashgraph-inspired consensus for AI
ensembles and outline its advantages over traditional ensembling techniques in
reducing nonfactual outputs. Preliminary considerations for implementation,
evaluation criteria for convergence and accuracy, and potential challenges are
discussed. The proposed mechanism demonstrates a promising direction for
multi-agent AI systems to self-validate and deliver high-fidelity responses in
complex tasks.

</details>

### [126] [OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents](https://arxiv.org/abs/2505.03570)
*Mariya Davydova,Daniel Jeffries,Patrick Barker,Arturo Márquez Flores,Sinéad Ryan*

Main category: cs.AI

TLDR: OSUniverse是一个用于评估高级GUI导航AI代理的复杂多模态桌面任务的基准测试，涵盖从基础点击到多步骤多应用任务，并支持自动验证。


<details>
  <summary>Details</summary>
Motivation: 为GUI导航AI代理提供一个易于使用、可扩展且全面的测试基准，以衡量其能力和进步。

Method: 将任务按复杂度分级，设计自动化验证机制，确保测试结果准确。

Result: 当前SOTA代理在测试中表现不超过50%，而普通白领可完美完成，自动化验证误差率低于2%。

Conclusion: OSUniverse为GUI导航AI代理的能力和进展提供了可靠的自动化评估工具。

Abstract: In this paper, we introduce OSUniverse: a benchmark of complex, multimodal
desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on
ease of use, extensibility, comprehensive coverage of test cases, and automated
validation. We divide the tasks in increasing levels of complexity, from basic
precision clicking to multistep, multiapplication tests requiring dexterity,
precision, and clear thinking from the agent. In version one of the benchmark,
presented here, we have calibrated the complexity of the benchmark test cases
to ensure that the SOTA (State of the Art) agents (at the time of publication)
do not achieve results higher than 50%, while the average white collar worker
can perform all these tasks with perfect accuracy. The benchmark can be scored
manually, but we also introduce an automated validation mechanism that has an
average error rate less than 2%. Therefore, this benchmark presents solid
ground for fully automated measuring of progress, capabilities and the
effectiveness of GUI-navigation AI agents over the short and medium-term
horizon. The source code of the benchmark is available at
https://github.com/agentsea/osuniverse.

</details>

### [127] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Manipulating Human Perceptual Variability](https://arxiv.org/abs/2505.03641)
*Chen Wei,Chi Zhang,Jiachen Zou,Haotian Deng,Dietmar Heinke,Quanying Liu*

Main category: cs.AI

TLDR: 论文提出BAM框架，结合ANN决策边界采样与人类行为实验，研究人类决策变异性，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 理解人类在不确定性和模糊性下的决策变异性，揭示其背后的感知与决策机制。

Method: 结合ANN的感知边界采样算法生成刺激物，并通过大规模行为实验（246名参与者，116,715次试验）验证。

Result: 生成variMNIST数据集（19,943张标注图像），建立个性化模型对齐与对抗生成方法，预测并操纵参与者决策差异。

Conclusion: BAM框架填补了计算模型与人类个体差异研究的空白，为个性化感知分析提供了新工具。

Abstract: Human decision-making in cognitive tasks and daily life exhibits considerable
variability, shaped by factors such as task difficulty, individual preferences,
and personal experiences. Understanding this variability across individuals is
essential for uncovering the perceptual and decision-making mechanisms that
humans rely on when faced with uncertainty and ambiguity. We present a
computational framework BAM (Boundary Alignment & Manipulation framework) that
combines perceptual boundary sampling in ANNs and human behavioral experiments
to systematically investigate this phenomenon. Our perceptual boundary sampling
algorithm generates stimuli along ANN decision boundaries that intrinsically
induce significant perceptual variability. The efficacy of these stimuli is
empirically validated through large-scale behavioral experiments involving 246
participants across 116,715 trials, culminating in the variMNIST dataset
containing 19,943 systematically annotated images. Through personalized model
alignment and adversarial generation, we establish a reliable method for
simultaneously predicting and manipulating the divergent perceptual decisions
of pairs of participants. This work bridges the gap between computational
models and human individual difference research, providing new tools for
personalized perception analysis.

</details>

### [128] [BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems](https://arxiv.org/abs/2505.03643)
*Chelsea Sidrane,Jana Tumova*

Main category: cs.AI

TLDR: 提出一种计算非线性离散时间神经反馈回路的后向可达集算法，用于验证学习型系统的目标可达性。


<details>
  <summary>Details</summary>
Motivation: 学习型规划与控制算法缺乏严格的性能或安全性保证，需要一种验证方法。

Method: 通过过近似系统动态函数，利用混合整数线性规划计算后向可达集。

Result: 算法在数值示例中得到验证，扩展了可验证的学习型系统属性范围。

Conclusion: 该工作为学习型系统提供了更广泛的属性验证能力。

Abstract: Learning-enabled planning and control algorithms are increasingly popular,
but they often lack rigorous guarantees of performance or safety. We introduce
an algorithm for computing underapproximate backward reachable sets of
nonlinear discrete time neural feedback loops. We then use the backward
reachable sets to check goal-reaching properties. Our algorithm is based on
overapproximating the system dynamics function to enable computation of
underapproximate backward reachable sets through solutions of mixed-integer
linear programs. We rigorously analyze the soundness of our algorithm and
demonstrate it on a numerical example. Our work expands the class of properties
that can be verified for learning-enabled systems.

</details>

### [129] [Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time](https://arxiv.org/abs/2505.03668)
*Celeste Veronese,Daniele Meli,Alessandro Farinelli*

Main category: cs.AI

TLDR: 提出了一种结合时间逻辑推理和POMDP的方法，通过LTL和EC生成持久宏动作，提升MCTS求解器的效率。


<details>
  <summary>Details</summary>
Motivation: 在不确定性下实现可解释的决策，同时减少推理时间并确保性能。

Method: 利用LTL和EC生成持久宏动作，通过ILP从少量执行轨迹中学习，无需人工启发式。

Result: 在Pocman和Rocksample基准测试中，宏动作表现出更高的表达力和计算效率。

Conclusion: 该方法显著提升了POMDP求解器的效率和泛化能力。

Abstract: This paper proposes an integration of temporal logical reasoning and
Partially Observable Markov Decision Processes (POMDPs) to achieve
interpretable decision-making under uncertainty with macro-actions. Our method
leverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus
(EC) to generate \emph{persistent} (i.e., constant) macro-actions, which guide
Monte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon,
significantly reducing inference time while ensuring robust performance. Such
macro-actions are learnt via Inductive Logic Programming (ILP) from a few
traces of execution (belief-action pairs), thus eliminating the need for
manually designed heuristics and requiring only the specification of the POMDP
transition model. In the Pocman and Rocksample benchmark scenarios, our learned
macro-actions demonstrate increased expressiveness and generality when compared
to time-independent heuristics, indeed offering substantial computational
efficiency improvements.

</details>

### [130] [Gap the (Theory of) Mind: Sharing Beliefs About Teammates' Goals Boosts Collaboration Perception, Not Performance](https://arxiv.org/abs/2505.03674)
*Yotam Amitai,Reuth Mirsky,Ofra Amir*

Main category: cs.AI

TLDR: 研究探讨AI代理通过分享对人类队友目标的理解是否能提升任务表现与合作感知，发现目标分享虽未显著改善任务表现或满意度，但支持战略适应与合作感知。


<details>
  <summary>Details</summary>
Motivation: 在人类与AI团队中，直接沟通目标不总是可行，需通过行为推断意图，研究目标是探讨分享AI推断的目标理解是否能提升合作效果。

Method: 通过实验比较三种条件：无识别（NR）、可行目标（VG）和按需可行目标（VGod），评估任务表现、满意度及认知负荷。

Result: 目标分享未显著提升任务表现或满意度，但支持战略适应与合作感知，认知负荷无差异。

Conclusion: 目标分享在提升信任与合作感知方面有效，但需平衡信息量与简洁性，可能偶尔影响客观表现。

Abstract: In human-agent teams, openly sharing goals is often assumed to enhance
planning, collaboration, and effectiveness. However, direct communication of
these goals is not always feasible, requiring teammates to infer their
partner's intentions through actions. Building on this, we investigate whether
an AI agent's ability to share its inferred understanding of a human teammate's
goals can improve task performance and perceived collaboration. Through an
experiment comparing three conditions-no recognition (NR), viable goals (VG),
and viable goals on-demand (VGod) - we find that while goal-sharing information
did not yield significant improvements in task performance or overall
satisfaction scores, thematic analysis suggests that it supported strategic
adaptations and subjective perceptions of collaboration. Cognitive load
assessments revealed no additional burden across conditions, highlighting the
challenge of balancing informativeness and simplicity in human-agent
interactions. These findings highlight the nuanced trade-off of goal-sharing:
while it fosters trust and enhances perceived collaboration, it can
occasionally hinder objective performance gains.

</details>

### [131] [Graph Drawing for LLMs: An Empirical Evaluation](https://arxiv.org/abs/2505.03678)
*Walter Didimo,Fabrizio Montecchiani,Tommaso Piselli*

Main category: cs.AI

TLDR: 研究探讨了大型语言模型（LLMs）在图相关任务中的表现，重点关注视觉模态（图形绘制）对模型性能的影响，包括布局范式、绘图美观度和提示技术。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在图任务中的表现，特别是在依赖视觉模态的场景下，研究如何通过优化图形绘制和提示技术提升模型性能。

Method: 通过实验分析，研究了布局范式、绘图美观度和提示技术对LLMs性能的影响，并提出了三个研究问题。

Result: 研究发现，选择合适的布局范式和优化绘图可读性显著提升模型性能，而提示技术的选择对性能至关重要但具有挑战性。

Conclusion: 优化图形绘制和提示技术是提升LLMs在图任务中性能的关键因素。

Abstract: Our work contributes to the fast-growing literature on the use of Large
Language Models (LLMs) to perform graph-related tasks. In particular, we focus
on usage scenarios that rely on the visual modality, feeding the model with a
drawing of the graph under analysis. We investigate how the model's performance
is affected by the chosen layout paradigm, the aesthetics of the drawing, and
the prompting technique used for the queries. We formulate three corresponding
research questions and present the results of a thorough experimental analysis.
Our findings reveal that choosing the right layout paradigm and optimizing the
readability of the input drawing from a human perspective can significantly
improve the performance of the model on the given task. Moreover, selecting the
most effective prompting technique is a challenging yet crucial task for
achieving optimal performance.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [132] [RESAnything: Attribute Prompting for Arbitrary Referring Segmentation](https://arxiv.org/abs/2505.02867)
*Ruiqi Wang,Hao Zhang*

Main category: cs.CV

TLDR: 提出了一种开放词汇和零样本的任意指代表达分割方法（RES），处理比现有方法更通用的输入表达。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法处理的广义指代表达，包括对象/部分级标签和隐式属性引用。

Method: 利用Chain-of-Thoughts（CoT）推理和属性提示，通过大语言模型（LLM）生成详细属性描述，结合基础图像分割模型生成提案。

Result: 在传统RES基准测试中表现优异，尤其在隐式查询和复杂部分级关系场景中显著优于现有方法。

Conclusion: RESAnything是首个零样本和基于LLM的RES方法，并贡献了一个新基准数据集以评估部分级任意RES解决方案。

Abstract: We present an open-vocabulary and zero-shot method for arbitrary referring
expression segmentation (RES), targeting input expressions that are more
general than what prior works were designed to handle. Specifically, our inputs
encompass both object- and part-level labels as well as implicit references
pointing to properties or qualities of object/part function, design, style,
material, etc. Our model, coined RESAnything, leverages Chain-of-Thoughts (CoT)
reasoning, where the key idea is attribute prompting. We generate detailed
descriptions of object/part attributes including shape, color, and location for
potential segment proposals through systematic prompting of a large language
model (LLM), where the proposals are produced by a foundational image
segmentation model. Our approach encourages deep reasoning about object or part
attributes related to function, style, design, etc., enabling the system to
handle implicit queries without any part annotations for training or
fine-tuning. As the first zero-shot and LLM-based RES method, RESAnything
achieves clearly superior performance among zero-shot methods on traditional
RES benchmarks and significantly outperforms existing methods on challenging
scenarios involving implicit queries and complex part-level relations. Finally,
we contribute a new benchmark dataset to offer ~3K carefully curated RES
instances to assess part-level, arbitrary RES solutions.

</details>

### [133] [Gone With the Bits: Revealing Racial Bias in Low-Rate Neural Compression for Facial Images](https://arxiv.org/abs/2505.02949)
*Tian Qiu,Arjun Nichani,Rasta Tadayontahmasebi,Haewon Jeong*

Main category: cs.CV

TLDR: 论文提出了一种评估神经图像压缩模型中偏见的框架，发现传统失真指标无法捕捉偏见，所有模型均存在种族偏见，并探讨了偏见与真实性的权衡及缓解策略。


<details>
  <summary>Details</summary>
Motivation: 神经压缩方法在极低比特率下表现优异，但可能因训练过程中的偏见导致不公平结果，需系统评估和解决。

Method: 提出结构化、可扩展的框架，分析九种流行神经压缩模型及其变体，研究种族偏见及其与图像重建的关系。

Result: 发现传统失真指标无效，所有模型均存在种族偏见，偏见与真实性存在权衡，平衡训练集可减少但无法完全消除偏见。

Conclusion: 本研究为评估和消除神经图像压缩模型中的偏见迈出了第一步。

Abstract: Neural compression methods are gaining popularity due to their superior
rate-distortion performance over traditional methods, even at extremely low
bitrates below 0.1 bpp. As deep learning architectures, these models are prone
to bias during the training process, potentially leading to unfair outcomes for
individuals in different groups. In this paper, we present a general,
structured, scalable framework for evaluating bias in neural image compression
models. Using this framework, we investigate racial bias in neural compression
algorithms by analyzing nine popular models and their variants. Through this
investigation, we first demonstrate that traditional distortion metrics are
ineffective in capturing bias in neural compression models. Next, we highlight
that racial bias is present in all neural compression models and can be
captured by examining facial phenotype degradation in image reconstructions. We
then examine the relationship between bias and realism in the decoded images
and demonstrate a trade-off across models. Finally, we show that utilizing a
racially balanced training set can reduce bias but is not a sufficient bias
mitigation strategy. We additionally show the bias can be attributed to
compression model bias and classification model bias. We believe that this work
is a first step towards evaluating and eliminating bias in neural image
compression models.

</details>

### [134] [Generating Narrated Lecture Videos from Slides with Synchronized Highlights](https://arxiv.org/abs/2505.02966)
*Alexander Holmberg*

Main category: cs.CV

TLDR: 该系统通过AI生成同步的动态视觉高亮和语音讲解，将静态幻灯片自动转化为视频讲座。


<details>
  <summary>Details</summary>
Motivation: 减少将静态幻灯片转化为视频讲座所需的时间和人力成本。

Method: 使用高亮对齐模块，结合多种策略（如Levenshtein距离、LLM语义分析）和TTS技术，实现语音与视觉高亮的精确同步。

Result: LLM对齐方法在1000个样本测试中F1>92%，生成成本低于$1/小时。

Conclusion: 该系统高效、低成本，适合大规模应用。

Abstract: Turning static slides into engaging video lectures takes considerable time
and effort, requiring presenters to record explanations and visually guide
their audience through the material. We introduce an end-to-end system designed
to automate this process entirely. Given a slide deck, this system synthesizes
a video lecture featuring AI-generated narration synchronized precisely with
dynamic visual highlights. These highlights automatically draw attention to the
specific concept being discussed, much like an effective presenter would. The
core technical contribution is a novel highlight alignment module. This module
accurately maps spoken phrases to locations on a given slide using diverse
strategies (e.g., Levenshtein distance, LLM-based semantic analysis) at
selectable granularities (line or word level) and utilizes timestamp-providing
Text-to-Speech (TTS) for timing synchronization. We demonstrate the system's
effectiveness through a technical evaluation using a manually annotated slide
dataset with 1000 samples, finding that LLM-based alignment achieves high
location accuracy (F1 > 92%), significantly outperforming simpler methods,
especially on complex, math-heavy content. Furthermore, the calculated
generation cost averages under $1 per hour of video, offering potential savings
of two orders of magnitude compared to conservative estimates of manual
production costs. This combination of high accuracy and extremely low cost
positions this approach as a practical and scalable tool for transforming
static slides into effective, visually-guided video lectures.

</details>

### [135] [Adversarial Robustness Analysis of Vision-Language Models in Medical Image Segmentation](https://arxiv.org/abs/2505.02971)
*Anjila Budathoki,Manish Dhakal*

Main category: cs.CV

TLDR: 该论文研究了针对视觉语言分割模型（VLSMs）在医学图像分析中的对抗攻击，评估了其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击在计算机视觉和视觉语言模型中已有研究，但在医学图像分析的VLSMs中仍未被充分探索。

Method: 通过微调预训练的VLSMs，并应用PGD和FGSM对抗攻击方法评估模型鲁棒性。

Result: 结果显示DSC和IoU分数显著下降，但未找到适用于医学图像的通用扰动。

Conclusion: 研究表明医学图像分析的VLSMs对对抗攻击敏感，需进一步研究提升其鲁棒性。

Abstract: Adversarial attacks have been fairly explored for computer vision and
vision-language models. However, the avenue of adversarial attack for the
vision language segmentation models (VLSMs) is still under-explored, especially
for medical image analysis.
  Thus, we have investigated the robustness of VLSMs against adversarial
attacks for 2D medical images with different modalities with radiology,
photography, and endoscopy. The main idea of this project was to assess the
robustness of the fine-tuned VLSMs specially in the medical domain setting to
address the high risk scenario.
  First, we have fine-tuned pre-trained VLSMs for medical image segmentation
with adapters.
  Then, we have employed adversarial attacks -- projected gradient descent
(PGD) and fast gradient sign method (FGSM) -- on that fine-tuned model to
determine its robustness against adversaries.
  We have reported models' performance decline to analyze the adversaries'
impact.
  The results exhibit significant drops in the DSC and IoU scores after the
introduction of these adversaries. Furthermore, we also explored universal
perturbation but were not able to find for the medical images.
  \footnote{https://github.com/anjilab/secure-private-ai}

</details>

### [136] [Completing Spatial Transcriptomics Data for Gene Expression Prediction Benchmarking](https://arxiv.org/abs/2505.02980)
*Daniela Ruiz,Paula Cardenas,Leonardo Manrique,Daniela Vega,Gabriel Mejia,Pablo Arbelaez*

Main category: cs.CV

TLDR: SpaRED和SpaCKLE为空间转录组学提供了标准化数据库和高效基因表达预测模型，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决Visium技术的高成本、低效率和数据丢失问题，以及现有模型评估缺乏标准化的问题。

Method: 引入SpaRED标准化数据库和SpaCKLE基于Transformer的基因表达补全模型。

Result: SpaCKLE将均方误差降低82.5%，并显著提升所有预测模型的性能。

Conclusion: SpaRED和SpaCKLE为空间转录组学提供了全面的基准和未来研究的基础。

Abstract: Spatial Transcriptomics is a groundbreaking technology that integrates
histology images with spatially resolved gene expression profiles. Among the
various Spatial Transcriptomics techniques available, Visium has emerged as the
most widely adopted. However, its accessibility is limited by high costs, the
need for specialized expertise, and slow clinical integration. Additionally,
gene capture inefficiencies lead to significant dropout, corrupting acquired
data. To address these challenges, the deep learning community has explored the
gene expression prediction task directly from histology images. Yet,
inconsistencies in datasets, preprocessing, and training protocols hinder fair
comparisons between models. To bridge this gap, we introduce SpaRED, a
systematically curated database comprising 26 public datasets, providing a
standardized resource for model evaluation. We further propose SpaCKLE, a
state-of-the-art transformer-based gene expression completion model that
reduces mean squared error by over 82.5% compared to existing approaches.
Finally, we establish the SpaRED benchmark, evaluating eight state-of-the-art
prediction models on both raw and SpaCKLE-completed data, demonstrating SpaCKLE
substantially improves the results across all the gene expression prediction
models. Altogether, our contributions constitute the most comprehensive
benchmark of gene expression prediction from histology images to date and a
stepping stone for future research on Spatial Transcriptomics.

</details>

### [137] [NTIRE 2025 Challenge on UGC Video Enhancement: Methods and Results](https://arxiv.org/abs/2505.03007)
*Nikolay Safonov,Alexey Bryncev,Andrey Moskalenko,Dmitry Kulikov,Dmitry Vatolin,Radu Timofte,Haibo Lei,Qifan Gao,Qing Luo,Yaqing Li,Jie Song,Shaozhe Hao,Meisong Zheng,Jingyi Xu,Chengbin Wu,Jiahui Liu,Ying Chen,Xin Deng,Mai Xu,Peipei Liang,Jie Ma,Junjie Jin,Yingxue Pang,Fangzhou Luo,Kai Chen,Shijie Zhao,Mingyang Wu,Renjie Li,Yushen Zuo,Shengyun Zhong,Zhengzhong Tu*

Main category: cs.CV

TLDR: NTIRE 2025挑战赛聚焦于用户生成内容（UGC）视频增强，旨在通过算法提升视频质量，吸引了25支团队参与，最终7支团队通过验证。


<details>
  <summary>Details</summary>
Motivation: UGC视频在短视频平台广泛使用，但其常受噪声、模糊等问题影响，提升其质量具有重要实际意义。

Method: 挑战赛提供了150个无参考真实数据的UGC视频，要求参与者开发算法改善视频质量，评估基于8000多名评估者的主观质量评分。

Result: 25支团队提交解决方案，7支通过最终验证，结果揭示了UGC视频增强的最新进展和有效策略。

Conclusion: 挑战赛成果为UGC视频增强领域提供了新见解，所有数据和结果已公开。

Abstract: This paper presents an overview of the NTIRE 2025 Challenge on UGC Video
Enhancement. The challenge constructed a set of 150 user-generated content
videos without reference ground truth, which suffer from real-world
degradations such as noise, blur, faded colors, compression artifacts, etc. The
goal of the participants was to develop an algorithm capable of improving the
visual quality of such videos. Given the widespread use of UGC on short-form
video platforms, this task holds substantial practical importance. The
evaluation was based on subjective quality assessment in crowdsourcing,
obtaining votes from over 8000 assessors. The challenge attracted more than 25
teams submitting solutions, 7 of which passed the final phase with source code
verification. The outcomes may provide insights into the state-of-the-art in
UGC video enhancement and highlight emerging trends and effective strategies in
this evolving research area. All data, including the processed videos and
subjective comparison votes and scores, is made publicly available at
https://github.com/msu-video-group/NTIRE25_UGC_Video_Enhancement.

</details>

### [138] [GIF: Generative Inspiration for Face Recognition at Scale](https://arxiv.org/abs/2505.03012)
*Saeed Ebrahimi,Sahar Rahimi,Ali Dabouei,Srinjoy Das,Jeremy M. Dawson,Nasser M. Nasrabadi*

Main category: cs.CV

TLDR: 提出一种结构化身份编码方法，将标量标签替换为整数序列，显著降低计算成本，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 减少大规模人脸识别中Softmax的计算成本，现有方法虽有效但计算成本与身份数量仍呈线性关系。

Method: 将标量标签转换为结构化身份编码，训练模型预测编码而非标量标签，使计算成本与身份数量呈对数关系。

Result: 在IJB-B和IJB-C上性能分别提升1.52%和0.6%，计算成本从线性降至对数。

Conclusion: 结构化身份编码方法有效降低计算成本并提升性能，适用于大规模人脸识别任务。

Abstract: Aiming to reduce the computational cost of Softmax in massive label space of
Face Recognition (FR) benchmarks, recent studies estimate the output using a
subset of identities. Although promising, the association between the
computation cost and the number of identities in the dataset remains linear
only with a reduced ratio. A shared characteristic among available FR methods
is the employment of atomic scalar labels during training. Consequently, the
input to label matching is through a dot product between the feature vector of
the input and the Softmax centroids. Inspired by generative modeling, we
present a simple yet effective method that substitutes scalar labels with
structured identity code, i.e., a sequence of integers. Specifically, we
propose a tokenization scheme that transforms atomic scalar labels into
structured identity codes. Then, we train an FR backbone to predict the code
for each input instead of its scalar label. As a result, the associated
computational cost becomes logarithmic w.r.t. number of identities. We
demonstrate the benefits of the proposed method by conducting experiments. In
particular, our method outperforms its competitors by 1.52%, and 0.6% at
TAR@FAR$=1e-4$ on IJB-B and IJB-C, respectively, while transforming the
association between computational cost and the number of identities from linear
to logarithmic. See code at https://github.com/msed-Ebrahimi/GIF

</details>

### [139] [Lesion-Aware Generative Artificial Intelligence for Virtual Contrast-Enhanced Mammography in Breast Cancer](https://arxiv.org/abs/2505.03018)
*Aurora Rofena,Arianna Manchia,Claudia Lucia Piccolo,Bruno Beomonte Zobel,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TLDR: Seg-CycleGAN是一种生成性深度学习框架，用于虚拟对比增强，通过低能量图像合成双能量减影图像，减少辐射和对比剂副作用。


<details>
  <summary>Details</summary>
Motivation: CESM虽提高诊断准确性，但伴随高辐射和对比剂副作用，需无对比剂替代方案。

Method: 基于CycleGAN架构，引入病灶分割图指导生成过程，增强病灶区域重建。

Result: 在CESM@UCBM数据集上，Seg-CycleGAN在PSNR和SSIM上优于基线，病灶保真度更高。

Conclusion: 分割感知生成模型为无对比剂CESM提供了可行路径。

Abstract: Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy mammographic
technique that improves lesion visibility through the administration of an
iodinated contrast agent. It acquires both a low-energy image, comparable to
standard mammography, and a high-energy image, which are then combined to
produce a dual-energy subtracted image highlighting lesion contrast
enhancement. While CESM offers superior diagnostic accuracy compared to
standard mammography, its use entails higher radiation exposure and potential
side effects associated with the contrast medium. To address these limitations,
we propose Seg-CycleGAN, a generative deep learning framework for Virtual
Contrast Enhancement in CESM. The model synthesizes high-fidelity dual-energy
subtracted images from low-energy images, leveraging lesion segmentation maps
to guide the generative process and improve lesion reconstruction. Building
upon the standard CycleGAN architecture, Seg-CycleGAN introduces localized loss
terms focused on lesion areas, enhancing the synthesis of diagnostically
relevant regions. Experiments on the CESM@UCBM dataset demonstrate that
Seg-CycleGAN outperforms the baseline in terms of PSNR and SSIM, while
maintaining competitive MSE and VIF. Qualitative evaluations further confirm
improved lesion fidelity in the generated images. These results suggest that
segmentation-aware generative models offer a viable pathway toward
contrast-free CESM alternatives.

</details>

### [140] [An Explainable Anomaly Detection Framework for Monitoring Depression and Anxiety Using Consumer Wearable Devices](https://arxiv.org/abs/2505.03039)
*Yuezhou Zhang,Amos A. Folarin,Callum Stewart,Heet Sankesara,Yatharth Ranjan,Pauline Conde,Akash Roy Choudhury,Shaoxiong Sun,Zulqarnain Rashid,Richard J. B. Dobson*

Main category: cs.CV

TLDR: 提出了一种基于可穿戴设备的可解释异常检测框架，用于早期检测抑郁和焦虑症状恶化。


<details>
  <summary>Details</summary>
Motivation: 通过可穿戴设备连续监测行为和生理数据，为早期发现抑郁和焦虑症状恶化提供客观方法。

Method: 使用LSTM自编码器模型分析2,023名参与者的健康基线数据（睡眠时长、步数、静息心率），检测症状恶化的异常。

Result: 模型在检测症状恶化时的调整F1得分为0.80，静息心率是最具影响力的特征。

Conclusion: 可解释的异常检测框架为个性化、可扩展的主动心理健康监测提供了潜力。

Abstract: Continuous monitoring of behavior and physiology via wearable devices offers
a novel, objective method for the early detection of worsening depression and
anxiety. In this study, we present an explainable anomaly detection framework
that identifies clinically meaningful increases in symptom severity using
consumer-grade wearable data. Leveraging data from 2,023 participants with
defined healthy baselines, our LSTM autoencoder model learned normal health
patterns of sleep duration, step count, and resting heart rate. Anomalies were
flagged when self-reported depression or anxiety scores increased by >=5 points
(a threshold considered clinically significant). The model achieved an adjusted
F1-score of 0.80 (precision = 0.73, recall = 0.88) in detecting 393
symptom-worsening episodes across 341 participants, with higher performance
observed for episodes involving concurrent depression and anxiety escalation
(F1 = 0.84) and for more pronounced symptom changes (>=10-point increases, F1 =
0.85). Model interpretability was supported by SHAP-based analysis, which
identified resting heart rate as the most influential feature in 71.4
percentage of detected anomalies, followed by physical activity and sleep.
Together, our findings highlight the potential of explainable anomaly detection
to enable personalized, scalable, and proactive mental health monitoring in
real-world settings.

</details>

### [141] [Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera](https://arxiv.org/abs/2505.03093)
*Siming He,Zachary Osman,Fernando Cladera,Dexter Ong,Nitant Rai,Patrick Corey Green,Vijay Kumar,Pratik Chaudhari*

Main category: cs.CV

TLDR: 提出一种低成本替代LiDAR的方法，使用消费级360度相机和半自动化流程测量树木胸径（DBH），精度接近LiDAR。


<details>
  <summary>Details</summary>
Motivation: LiDAR技术成本高且操作复杂，需要一种低成本、易操作的替代方案用于森林资源管理。

Method: 结合SfM点云重建、语义树干分割和RANSAC技术，开发半自动化流程。

Result: 在61次实验中，相对误差为5-9%，仅比LiDAR高2-4%。

Conclusion: 该方法成本低、易操作，精度接近LiDAR，适合广泛应用。

Abstract: Forest inventories rely on accurate measurements of the diameter at breast
height (DBH) for ecological monitoring, resource management, and carbon
accounting. While LiDAR-based techniques can achieve centimeter-level
precision, they are cost-prohibitive and operationally complex. We present a
low-cost alternative that only needs a consumer-grade 360 video camera. Our
semi-automated pipeline comprises of (i) a dense point cloud reconstruction
using Structure from Motion (SfM) photogrammetry software called Agisoft
Metashape, (ii) semantic trunk segmentation by projecting Grounded Segment
Anything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based
technique to estimate cross section shape and DBH. We introduce an interactive
visualization tool for inspecting segmented trees and their estimated DBH. On
61 acquisitions of 43 trees under a variety of conditions, our method attains
median absolute relative errors of 5-9% with respect to "ground-truth" manual
measurements. This is only 2-4% higher than LiDAR-based estimates, while
employing a single 360 camera that costs orders of magnitude less, requires
minimal setup, and is widely available.

</details>

### [142] [Not All Parameters Matter: Masking Diffusion Models for Enhancing Generation Ability](https://arxiv.org/abs/2505.03097)
*Lei Wang,Senmao Li,Fei Yang,Jianye Wang,Ziheng Zhang,Yuhan Liu,Yaxing Wang,Jian Yang*

Main category: cs.CV

TLDR: 论文提出了一种名为MaskUNet的方法，通过动态调整U-Net参数（包括零化某些参数）来优化扩散模型的生成质量，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在同一网络层中同时学习结构和纹理信息，而传统深度学习架构（如ResNet或GANs）在不同层中捕获或生成语义信息。这种差异激发了作者对时间维度扩散模型的探索。

Method: 作者提出MaskUNet方法，利用时间步和样本依赖的有效U-Net参数，通过两种微调策略（基于训练和无训练）优化模型。

Result: 在COCO数据集的零样本推理中，MaskUNet取得了最佳FID分数，并在下游任务评估中表现出色。

Conclusion: MaskUNet是一种简单有效的方法，通过动态参数调整显著提升了扩散模型的生成质量。

Abstract: The diffusion models, in early stages focus on constructing basic image
structures, while the refined details, including local features and textures,
are generated in later stages. Thus the same network layers are forced to learn
both structural and textural information simultaneously, significantly
differing from the traditional deep learning architectures (e.g., ResNet or
GANs) which captures or generates the image semantic information at different
layers. This difference inspires us to explore the time-wise diffusion models.
We initially investigate the key contributions of the U-Net parameters to the
denoising process and identify that properly zeroing out certain parameters
(including large parameters) contributes to denoising, substantially improving
the generation quality on the fly. Capitalizing on this discovery, we propose a
simple yet effective method-termed ``MaskUNet''- that enhances generation
quality with negligible parameter numbers. Our method fully leverages timestep-
and sample-dependent effective U-Net parameters. To optimize MaskUNet, we offer
two fine-tuning strategies: a training-based approach and a training-free
approach, including tailored networks and optimization functions. In zero-shot
inference on the COCO dataset, MaskUNet achieves the best FID score and further
demonstrates its effectiveness in downstream task evaluations. Project page:
https://gudaochangsheng.github.io/MaskUnet-Page/

</details>

### [143] [Image Recognition with Online Lightweight Vision Transformer: A Survey](https://arxiv.org/abs/2505.03113)
*Zherui Zhang,Rongtao Xu,Jie Zhou,Changwei Wang,Xingtian Pei,Wenhao Xu,Jiguang Zhang,Li Guo,Longxiang Gao,Wenbo Xu,Shibiao Xu*

Main category: cs.CV

TLDR: 本文综述了轻量化视觉Transformer的方法，包括高效组件设计、动态网络和知识蒸馏，并分析了它们在ImageNet-1K上的表现，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: Transformer在自然语言处理中的成功激发了其在计算机视觉中的应用，但面临计算和内存挑战，需要轻量化解决方案。

Method: 研究了三种轻量化策略：高效组件设计、动态网络和知识蒸馏，并在ImageNet-1K上评估其性能。

Result: 分析了不同方法在精度、参数量和吞吐量等方面的权衡，总结了各自的优缺点。

Conclusion: 提出了轻量化视觉Transformer的未来研究方向和潜在挑战，旨在为社区提供指导和启发。

Abstract: The Transformer architecture has achieved significant success in natural
language processing, motivating its adaptation to computer vision tasks. Unlike
convolutional neural networks, vision transformers inherently capture
long-range dependencies and enable parallel processing, yet lack inductive
biases and efficiency benefits, facing significant computational and memory
challenges that limit its real-world applicability. This paper surveys various
online strategies for generating lightweight vision transformers for image
recognition, focusing on three key areas: Efficient Component Design, Dynamic
Network, and Knowledge Distillation. We evaluate the relevant exploration for
each topic on the ImageNet-1K benchmark, analyzing trade-offs among precision,
parameters, throughput, and more to highlight their respective advantages,
disadvantages, and flexibility. Finally, we propose future research directions
and potential challenges in the lightweighting of vision transformers with the
aim of inspiring further exploration and providing practical guidance for the
community. Project Page: https://github.com/ajxklo/Lightweight-VIT

</details>

### [144] [Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation](https://arxiv.org/abs/2505.03114)
*Teng Zhou,Jax Luo,Yuping Sun,Yiheng Tan,Shun Yao,Nazim Haouchine,Scott Raymond*

Main category: cs.CV

TLDR: 提出了一种基于路径和骨骼轮廓正则化的无配对MRI到CT转换方法，通过神经ODE建模连续流，优化骨骼结构转换。


<details>
  <summary>Details</summary>
Motivation: 解决现有无配对MRI到CT转换方法在骨骼结构转换上的不足，适用于放射治疗等需要精确骨骼表示的场景。

Method: 将MRI和CT投影到共享潜在空间，用神经ODE建模连续流，最小化路径长度，并引入可训练神经网络生成骨骼轮廓。

Result: 在三个数据集上表现优于现有方法，整体误差更低，且在骨骼分割任务中表现更优。

Conclusion: 该方法显著提升了骨骼结构的转换精度，适用于放射治疗等应用。

Abstract: Accurate MRI-to-CT translation promises the integration of complementary
imaging information without the need for additional imaging sessions. Given the
practical challenges associated with acquiring paired MRI and CT scans, the
development of robust methods capable of leveraging unpaired datasets is
essential for advancing the MRI-to-CT translation. Current unpaired MRI-to-CT
translation methods, which predominantly rely on cycle consistency and
contrastive learning frameworks, frequently encounter challenges in accurately
translating anatomical features that are highly discernible on CT but less
distinguishable on MRI, such as bone structures. This limitation renders these
approaches less suitable for applications in radiation therapy, where precise
bone representation is essential for accurate treatment planning. To address
this challenge, we propose a path- and bone-contour regularized approach for
unpaired MRI-to-CT translation. In our method, MRI and CT images are projected
to a shared latent space, where the MRI-to-CT mapping is modeled as a
continuous flow governed by neural ordinary differential equations. The optimal
mapping is obtained by minimizing the transition path length of the flow. To
enhance the accuracy of translated bone structures, we introduce a trainable
neural network to generate bone contours from MRI and implement mechanisms to
directly and indirectly encourage the model to focus on bone contours and their
adjacent regions. Evaluations conducted on three datasets demonstrate that our
method outperforms existing unpaired MRI-to-CT translation approaches,
achieving lower overall error rates. Moreover, in a downstream bone
segmentation task, our approach exhibits superior performance in preserving the
fidelity of bone structures. Our code is available at:
https://github.com/kennysyp/PaBoT.

</details>

### [145] [TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion](https://arxiv.org/abs/2505.03116)
*Haoyue Liu,Jinghan Xu,Yi Chang,Hanyu Zhou,Haozhi Zhao,Lin Wang,Luxin Yan*

Main category: cs.CV

TLDR: 论文提出了一种基于连续点追踪的视频帧插值框架TimeTracker，通过事件相机处理非线性运动，显著提升了插值质量。


<details>
  <summary>Details</summary>
Motivation: 事件相机的高时间分辨率优势未被充分利用，现有方法在处理非线性运动时存在运动误差问题。

Method: 设计了场景感知区域分割模块(SARS)和连续轨迹引导的运动估计模块(CTME)，通过追踪局部区域的连续运动轨迹实现精确插值。

Result: 实验表明，TimeTracker在运动估计和帧插值质量上优于现有方法。

Conclusion: TimeTracker通过连续点追踪和全局优化，有效解决了非线性运动问题，提升了插值性能。

Abstract: Video frame interpolation (VFI) that leverages the bio-inspired event cameras
as guidance has recently shown better performance and memory efficiency than
the frame-based methods, thanks to the event cameras' advantages, such as high
temporal resolution. A hurdle for event-based VFI is how to effectively deal
with non-linear motion, caused by the dynamic changes in motion direction and
speed within the scene. Existing methods either use events to estimate sparse
optical flow or fuse events with image features to estimate dense optical flow.
Unfortunately, motion errors often degrade the VFI quality as the continuous
motion cues from events do not align with the dense spatial information of
images in the temporal dimension. In this paper, we find that object motion is
continuous in space, tracking local regions over continuous time enables more
accurate identification of spatiotemporal feature correlations. In light of
this, we propose a novel continuous point tracking-based VFI framework, named
TimeTracker. Specifically, we first design a Scene-Aware Region Segmentation
(SARS) module to divide the scene into similar patches. Then, a Continuous
Trajectory guided Motion Estimation (CTME) module is proposed to track the
continuous motion trajectory of each patch through events. Finally,
intermediate frames at any given time are generated through global motion
optimization and frame refinement. Moreover, we collect a real-world dataset
that features fast non-linear motion. Extensive experiments show that our
method outperforms prior arts in both motion estimation and frame interpolation
quality.

</details>

### [146] [VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis](https://arxiv.org/abs/2505.03132)
*Xinyuan Yan,Xiwei Xuan,Jorge Piazentin Ono,Jiajing Guo,Vikram Mohanty,Shekar Arvind Kumar,Liang Gou,Bei Wang,Liu Ren*

Main category: cs.CV

TLDR: VISLIX是一个视觉分析框架，利用先进的基础模型帮助专家分析计算机视觉模型中的数据切片，无需额外元数据或视觉概念，并提供交互式测试功能。


<details>
  <summary>Details</summary>
Motivation: 现实中的机器学习模型（如自动驾驶和监控）需要严格评估，但现有数据切片方法存在依赖元数据、任务局限性、人工依赖强和缺乏交互性等问题。

Method: 提出VISLIX框架，利用基础模型自动生成自然语言见解，支持交互式数据切片假设测试。

Result: 通过专家研究和三个用例验证，VISLIX能有效为对象检测模型提供全面见解。

Conclusion: VISLIX克服了现有数据切片方法的局限性，为计算机视觉模型验证提供了高效工具。

Abstract: Real-world machine learning models require rigorous evaluation before
deployment, especially in safety-critical domains like autonomous driving and
surveillance. The evaluation of machine learning models often focuses on data
slices, which are subsets of the data that share a set of characteristics. Data
slice finding automatically identifies conditions or data subgroups where
models underperform, aiding developers in mitigating performance issues.
Despite its popularity and effectiveness, data slicing for vision model
validation faces several challenges. First, data slicing often needs additional
image metadata or visual concepts, and falls short in certain computer vision
tasks, such as object detection. Second, understanding data slices is a
labor-intensive and mentally demanding process that heavily relies on the
expert's domain knowledge. Third, data slicing lacks a human-in-the-loop
solution that allows experts to form hypothesis and test them interactively. To
overcome these limitations and better support the machine learning operations
lifecycle, we introduce VISLIX, a novel visual analytics framework that employs
state-of-the-art foundation models to help domain experts analyze slices in
computer vision models. Our approach does not require image metadata or visual
concepts, automatically generates natural language insights, and allows users
to test data slice hypothesis interactively. We evaluate VISLIX with an expert
study and three use cases, that demonstrate the effectiveness of our tool in
providing comprehensive insights for validating object detection models.

</details>

### [147] [Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control](https://arxiv.org/abs/2505.03134)
*Sajjad Rezvani Boroujeni,Hossein Abedi,Tom Bush*

Main category: cs.CV

TLDR: 论文提出了一种基于DDPM的数据增强方法，用于解决玻璃制造中视觉缺陷检测的数据不平衡问题，显著提升了分类模型的性能。


<details>
  <summary>Details</summary>
Motivation: 工业玻璃制造中视觉缺陷检测因缺陷产品频率低导致数据不平衡，限制了深度学习模型的性能。

Method: 使用Denoising Diffusion Probabilistic Models (DDPMs)生成合成缺陷图像进行数据增强，提升少数类样本的表示。

Result: 实验结果表明，数据增强显著提升了分类模型的性能，特别是召回率，ResNet50V2的准确率从78%提升到93%。

Conclusion: 该方法为玻璃制造中的缺陷检测提供了一种可扩展且经济高效的解决方案，并可推广到其他类似数据不平衡问题的行业。

Abstract: Visual defect detection in industrial glass manufacturing remains a critical
challenge due to the low frequency of defective products, leading to imbalanced
datasets that limit the performance of deep learning models and computer vision
systems. This paper presents a novel approach using Denoising Diffusion
Probabilistic Models (DDPMs) to generate synthetic defective glass product
images for data augmentation, effectively addressing class imbalance issues in
manufacturing quality control and automated visual inspection. The methodology
significantly enhances image classification performance of standard CNN
architectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting
anomalies by increasing the minority class representation. Experimental results
demonstrate substantial improvements in key machine learning metrics,
particularly in recall for defective samples across all tested deep neural
network architectures while maintaining perfect precision. The most dramatic
improvement was observed in ResNet50V2's overall classification accuracy, which
increased from 78 percent to 93 percent when trained with the augmented data.
This work provides a scalable, cost-effective approach to enhancing automated
defect detection in glass manufacturing that can potentially be extended to
other industrial quality assurance systems and industries with similar class
imbalance challenges.

</details>

### [148] [Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)](https://arxiv.org/abs/2505.03149)
*Joseph William Kettelkamp,Ludovica Romanin,Sarv Priya,Mathews Jacob*

Main category: cs.CV

TLDR: 提出一种无监督运动补偿图像重建算法，用于自由呼吸和非门控3D心脏MRI，通过低秩模型表示运动相位的变形。


<details>
  <summary>Details</summary>
Motivation: 解决自由呼吸和非门控3D心脏MRI中运动伪影问题，提高图像重建质量。

Method: 将每个运动相位的图像体积表示为静态模板的变形，使用低秩模型表示变形家族，并通过参数化速度场积分得到特定相位的变形。

Result: 相比现有运动分辨和运动补偿算法，提出的低秩运动模型在自由呼吸3D心脏MRI中提供了更好的重建效果。

Conclusion: 低秩运动模型能够有效减少运动伪影，提升图像重建质量。

Abstract: We introduce an unsupervised motion-compensated image reconstruction
algorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging
(MRI). We express the image volume corresponding to each specific motion phase
as the deformation of a single static image template. The main contribution of
the work is the low-rank model for the compact joint representation of the
family of diffeomorphisms, parameterized by the motion phases. The
diffeomorphism at a specific motion phase is obtained by integrating a
parametric velocity field along a path connecting the reference template phase
to the motion phase. The velocity field at different phases is represented
using a low-rank model. The static template and the low-rank motion model
parameters are learned directly from the k-space data in an unsupervised
fashion. The more constrained motion model is observed to offer improved
recovery compared to current motion-resolved and motion-compensated algorithms
for free-breathing 3D cine MRI.

</details>

### [149] [Robust Fairness Vision-Language Learning for Medical Image Analysis](https://arxiv.org/abs/2505.03153)
*Sparsh Bansal,Mingyang Wu,Xin Wang,Shu Hu*

Main category: cs.CV

TLDR: 本文提出了一种确保视觉语言模型（VLM）在医学图像分析中公平性和鲁棒性的框架，通过动态坏对挖掘算法和Sinkhorn距离优化损失函数，实验显示公平性AUC提升8.6%。


<details>
  <summary>Details</summary>
Motivation: 在医学图像分析中，视觉语言模型的公平性和鲁棒性至关重要，以确保模型对所有患者均适用。

Method: 框架通过动态坏对挖掘算法调整损失函数，并使用Sinkhorn距离确保保护组的损失分布与总损失一致。

Result: 实验结果表明，公平性AUC提升了8.6%。

Conclusion: 该框架有效提升了VLM在医学领域的公平性和鲁棒性。

Abstract: The advent of Vision-Language Models (VLMs) in medical image analysis has the
potential to help process multimodal inputs and increase performance over
traditional inference methods. However, when considering the domain in which
these models will be implemented, fairness and robustness are important to
ensure the model stays true for any patient. In this paper, we introduce a
framework for ensuring robustness and fairness of VLM models. This framework
modifies the loss function at training by identifying and adjusting faulty
image-text pairs through a Dynamic Bad Pair Mining algorithm and also utilizing
Sinkhorn distance to ensure the loss distributions of protected groups do not
deviate from the total loss. Experimental testing of our framework shows up to
a 8.6\% improvement when looking at equity-scaled AUC.

</details>

### [150] [StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data](https://arxiv.org/abs/2505.03154)
*Yuxuan Mu,Hung Yu Ling,Yi Shi,Ismael Baira Ojeda,Pengcheng Xi,Chang Shu,Fabio Zinno,Xue Bin Peng*

Main category: cs.CV

TLDR: StableMotion提出了一种直接从无配对数据训练运动清理模型的方法，通过引入运动质量指标，无需高质量配对数据即可生成高质量运动。


<details>
  <summary>Details</summary>
Motivation: 运动捕捉数据常因传感器和后期处理不准确而产生视觉伪影，手动清理成本高且耗时。现有数据驱动方法需要配对数据，而获取高质量配对数据同样耗时。

Method: 引入运动质量指标，通过手动标注或启发式算法标注，训练质量感知运动生成模型。采用基于扩散的框架，实现运动生成与判别一体化。

Result: 在SoccerMocap数据集上，模型有效减少了68%的运动突变和81%的冻结帧。

Conclusion: StableMotion无需配对数据即可训练运动清理模型，显著减少伪影，适用于实际生产场景。

Abstract: Motion capture (mocap) data often exhibits visually jarring artifacts due to
inaccurate sensors and post-processing. Cleaning this corrupted data can
require substantial manual effort from human experts, which can be a costly and
time-consuming process. Previous data-driven motion cleanup methods offer the
promise of automating this cleanup process, but often require in-domain paired
corrupted-to-clean training data. Constructing such paired datasets requires
access to high-quality, relatively artifact-free motion clips, which often
necessitates laborious manual cleanup. In this work, we present StableMotion, a
simple yet effective method for training motion cleanup models directly from
unpaired corrupted datasets that need cleanup. The core component of our method
is the introduction of motion quality indicators, which can be easily annotated
through manual labeling or heuristic algorithms and enable training of
quality-aware motion generation models on raw motion data with mixed quality.
At test time, the model can be prompted to generate high-quality motions using
the quality indicators. Our method can be implemented through a simple
diffusion-based framework, leading to a unified motion generate-discriminate
model, which can be used to both identify and fix corrupted frames. We
demonstrate that our proposed method is effective for training motion cleanup
models on raw mocap data in production scenarios by applying StableMotion to
SoccerMocap, a 245-hour soccer mocap dataset containing real-world motion
artifacts. The trained model effectively corrects a wide range of motion
artifacts, reducing motion pops and frozen frames by 68% and 81%, respectively.
See https://youtu.be/3Y7MMAH02B4 for more results.

</details>

### [151] [RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph](https://arxiv.org/abs/2505.03173)
*Sameer Malik,Moyuru Yamada,Ayush Singh,Dishank Aggarwal*

Main category: cs.CV

TLDR: RAVU框架通过检索增强和时空图推理，解决了LMMs处理长视频的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前LMMs缺乏显式记忆和检索机制，难以处理分钟至小时级别的视频。

Method: 构建视频的时空图表示，作为长期记忆，并通过分解查询为推理步骤在图检索关键信息。

Result: 在NExT-QA和EgoSchema数据集上表现优于其他方法，仅需5-10帧即可实现高精度。

Conclusion: RAVU显著提升了长视频理解能力，尤其在多跳推理和跨帧对象跟踪任务中。

Abstract: Comprehending long videos remains a significant challenge for Large
Multi-modal Models (LMMs). Current LMMs struggle to process even minutes to
hours videos due to their lack of explicit memory and retrieval mechanisms. To
address this limitation, we propose RAVU (Retrieval Augmented Video
Understanding), a novel framework for video understanding enhanced by retrieval
with compositional reasoning over a spatio-temporal graph. We construct a graph
representation of the video, capturing both spatial and temporal relationships
between entities. This graph serves as a long-term memory, allowing us to track
objects and their actions across time. To answer complex queries, we decompose
the queries into a sequence of reasoning steps and execute these steps on the
graph, retrieving relevant key information. Our approach enables more accurate
understanding of long videos, particularly for queries that require multi-hop
reasoning and tracking objects across frames. Our approach demonstrate superior
performances with limited retrieved frames (5-10) compared with other SOTA
methods and baselines on two major video QA datasets, NExT-QA and EgoSchema.

</details>

### [152] [seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models](https://arxiv.org/abs/2505.03176)
*Hafez Ghaemi,Eilif Muller,Shahab Bakhtiari*

Main category: cs.CV

TLDR: seq-JEPA是一种基于联合嵌入预测架构的世界建模范式，通过处理图像的不同视图序列，同时学习不变和等变表示，解决了现有两视图范式的性能权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有自监督算法依赖数据增强和掩码等变换，通过两视图范式学习视觉表示，但这种方法在不变性和等变性任务之间存在性能权衡，限制了表示的下游适应性。

Method: seq-JEPA通过处理输入图像的不同视图序列，结合相对变换的嵌入，使用Transformer编码器生成聚合表示，并预测下一视图的表示，无需额外的等变预测器或损失项。

Result: seq-JEPA在等变基准测试和图像分类任务中表现优异，同时擅长需要序列观察聚合的任务，如路径整合和眼动预测学习。

Conclusion: seq-JEPA通过序列化处理和架构分离，成功解决了不变性和等变性任务的性能权衡，为自监督学习提供了更灵活的表示学习框架。

Abstract: Current self-supervised algorithms mostly rely on transformations such as
data augmentation and masking to learn visual representations. This is achieved
by inducing invariance or equivariance with respect to these transformations
after encoding two views of an image. This dominant two-view paradigm can limit
the flexibility of learned representations for downstream adaptation by
creating performance trade-offs between invariance-related tasks such as image
classification and more fine-grained equivariance-related tasks. In this work,
we introduce \emph{seq-JEPA}, a world modeling paradigm based on
joint-embedding predictive architecture that leverages architectural inductive
biases to resolve this trade-off. Without requiring an additional equivariance
predictor or loss term, seq-JEPA simultaneously learns two architecturally
segregated representations: one equivariant to the specified transformations
and another invariant to them and suited for tasks such as classification. To
do so, our model processes a short sequence of different views (observations)
of an input image. Each encoded view is concatenated with embeddings
corresponding to the relative transformation (action) producing the next
observation in the sequence. A transformer encoder outputs an aggregate
representation of this sequence, which is subsequently conditioned on the
action leading to the next observation to predict its representation.
Empirically, seq-JEPA achieves strong performance on equivariant benchmarks and
image classification without sacrificing one for the other. Additionally, our
framework excels at tasks that inherently require aggregating a sequence of
observations, such as path integration across actions and predictive learning
across eye movements.

</details>

### [153] [Interactive Instance Annotation with Siamese Networks](https://arxiv.org/abs/2505.03184)
*Xiang Xu,Ruotong Li,Mengjun Yi,Baile XU,Furao Shen,Jian Zhao*

Main category: cs.CV

TLDR: SiamAnno是一个基于Siamese网络的框架，用于跨域实例标注任务，通过一次性学习预测对象边界，无需微调即可在多个数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 实例标注耗时且费力，现有方法多局限于同域场景，限制了跨域标注任务的效果。

Method: 提出SiamAnno框架，利用Siamese网络和一次性学习，通过输入边界框预测对象边界，支持用户调整。

Result: 在未微调的情况下，SiamAnno在多个数据集上实现SOTA性能，展示了跨域任务的处理能力。

Conclusion: SiamAnno是首个探索Siamese架构用于实例标注的模型，为未来研究提供了强基线。

Abstract: Annotating instance masks is time-consuming and labor-intensive. A promising
solution is to predict contours using a deep learning model and then allow
users to refine them. However, most existing methods focus on in-domain
scenarios, limiting their effectiveness for cross-domain annotation tasks. In
this paper, we propose SiamAnno, a framework inspired by the use of Siamese
networks in object tracking. SiamAnno leverages one-shot learning to annotate
previously unseen objects by taking a bounding box as input and predicting
object boundaries, which can then be adjusted by annotators. Trained on one
dataset and tested on another without fine-tuning, SiamAnno achieves
state-of-the-art (SOTA) performance across multiple datasets, demonstrating its
ability to handle domain and environment shifts in cross-domain tasks. We also
provide more comprehensive results compared to previous work, establishing a
strong baseline for future research. To our knowledge, SiamAnno is the first
model to explore Siamese architecture for instance annotation.

</details>

### [154] [PiCo: Enhancing Text-Image Alignment with Improved Noise Selection and Precise Mask Control in Diffusion Models](https://arxiv.org/abs/2505.03203)
*Chang Xie,Chenyi Zhuang,Pan Gao*

Main category: cs.CV

TLDR: PiCo提出了一种无需训练的文本到图像生成方法，通过噪声选择模块和参考掩码模块解决复杂文本提示下的文本-图像对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在复杂文本提示下难以实现文本-图像对齐，主要受随机初始化噪声质量和生成控制掩码可靠性的影响。

Method: PiCo包含噪声选择模块（评估噪声质量）和参考掩码模块（生成像素级掩码并调制交叉注意力图）。

Result: 实验验证了PiCo在提升文本-图像对齐和减少随机生成繁琐性方面的有效性。

Conclusion: PiCo通过噪声选择和掩码调制显著提升了复杂文本提示下的生成效果。

Abstract: Advanced diffusion models have made notable progress in text-to-image
compositional generation. However, it is still a challenge for existing models
to achieve text-image alignment when confronted with complex text prompts. In
this work, we highlight two factors that affect this alignment: the quality of
the randomly initialized noise and the reliability of the generated controlling
mask. We then propose PiCo (Pick-and-Control), a novel training-free approach
with two key components to tackle these two factors. First, we develop a noise
selection module to assess the quality of the random noise and determine
whether the noise is suitable for the target text. A fast sampling strategy is
utilized to ensure efficiency in the noise selection stage. Second, we
introduce a referring mask module to generate pixel-level masks and to
precisely modulate the cross-attention maps. The referring mask is applied to
the standard diffusion process to guide the reasonable interaction between text
and image features. Extensive experiments have been conducted to verify the
effectiveness of PiCo in liberating users from the tedious process of random
generation and in enhancing the text-image alignment for diverse text
descriptions.

</details>

### [155] [DCS-ST for Classification of Breast Cancer Histopathology Images with Limited Annotations](https://arxiv.org/abs/2505.03204)
*Liu Suxing,Byungwon Min*

Main category: cs.CV

TLDR: 深度学习在乳腺癌组织病理图像分类中表现良好，但在标注数据有限时性能下降。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中因标注成本高和专业知识需求导致的标注数据不足问题。

Method: 使用深度学习方法对乳腺癌组织病理图像进行分类。

Result: 在标注数据有限的情况下，深度学习模型的性能下降。

Conclusion: 需要改进方法以应对标注数据不足的挑战。

Abstract: Deep learning methods have shown promise in classifying breast cancer
histopathology images, but their performance often declines with limited
annotated data, a critical challenge in medical imaging due to the high cost
and expertise required for annotations.

</details>

### [156] [Dual-Domain Masked Image Modeling: A Self-Supervised Pretraining Strategy Using Spatial and Frequency Domain Masking for Hyperspectral Data](https://arxiv.org/abs/2505.03220)
*Shaheer Mohamed,Tharindu Fernando,Sridha Sridharan,Peyman Moghadam,Clinton Fookes*

Main category: cs.CV

TLDR: 提出了一种自监督预训练方法SFMIM，用于解决高光谱图像（HSI）标记数据稀缺的问题，通过空间和频率双域掩码机制提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像（HSI）标记数据稀缺，限制了深度学习尤其是基于Transformer的架构的潜力。

Method: 提出SFMIM方法，结合空间和频率双域掩码机制，通过随机掩码和重建任务学习高阶谱空相关性。

Result: 在三个公开HSI分类基准上达到最优性能，且微调时收敛速度快。

Conclusion: SFMIM有效利用无标记数据，显著提升了HSI分类性能。

Abstract: Hyperspectral images (HSIs) capture rich spectral signatures that reveal
vital material properties, offering broad applicability across various domains.
However, the scarcity of labeled HSI data limits the full potential of deep
learning, especially for transformer-based architectures that require
large-scale training. To address this constraint, we propose Spatial-Frequency
Masked Image Modeling (SFMIM), a self-supervised pretraining strategy for
hyperspectral data that utilizes the large portion of unlabeled data. Our
method introduces a novel dual-domain masking mechanism that operates in both
spatial and frequency domains. The input HSI cube is initially divided into
non-overlapping patches along the spatial dimension, with each patch comprising
the entire spectrum of its corresponding spatial location. In spatial masking,
we randomly mask selected patches and train the model to reconstruct the masked
inputs using the visible patches. Concurrently, in frequency masking, we remove
portions of the frequency components of the input spectra and predict the
missing frequencies. By learning to reconstruct these masked components, the
transformer-based encoder captures higher-order spectral-spatial correlations.
We evaluate our approach on three publicly available HSI classification
benchmarks and demonstrate that it achieves state-of-the-art performance.
Notably, our model shows rapid convergence during fine-tuning, highlighting the
efficiency of our pretraining strategy.

</details>

### [157] [Seeing the Abstract: Translating the Abstract Language for Vision Language Models](https://arxiv.org/abs/2505.03242)
*Davide Talon,Federico Girella,Ziyue Liu,Marco Cristani,Yiming Wang*

Main category: cs.CV

TLDR: 论文揭示了抽象语言在视觉语言模型（VLM）中的广泛存在及其被低估的价值，并提出了一种无需训练的模型无关方法（ACT）来改进抽象语言的表示。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型研究忽视了抽象语言的重要性，而抽象语言在表达情感、创造力等方面具有独特价值。

Method: 提出Abstract-to-Concrete Translator (ACT)方法，通过预训练模型和多模态数据库将抽象表示转换为具体的表示。

Result: 在文本到图像检索任务中，ACT无需训练即优于微调的VLM，表现出强大的泛化能力。

Conclusion: ACT是一种即插即用的解决方案，能显著提升VLM对抽象语言的处理能力。

Abstract: Natural language goes beyond dryly describing visual content. It contains
rich abstract concepts to express feeling, creativity and properties that
cannot be directly perceived. Yet, current research in Vision Language Models
(VLMs) has not shed light on abstract-oriented language. Our research breaks
new ground by uncovering its wide presence and under-estimated value, with
extensive analysis. Particularly, we focus our investigation on the fashion
domain, a highly-representative field with abstract expressions. By analyzing
recent large-scale multimodal fashion datasets, we find that abstract terms
have a dominant presence, rivaling the concrete ones, providing novel
information, and being useful in the retrieval task. However, a critical
challenge emerges: current general-purpose or fashion-specific VLMs are
pre-trained with databases that lack sufficient abstract words in their text
corpora, thus hindering their ability to effectively represent
abstract-oriented language. We propose a training-free and model-agnostic
method, Abstract-to-Concrete Translator (ACT), to shift abstract
representations towards well-represented concrete ones in the VLM latent space,
using pre-trained models and existing multimodal databases. On the
text-to-image retrieval task, despite being training-free, ACT outperforms the
fine-tuned VLMs in both same- and cross-dataset settings, exhibiting its
effectiveness with a strong generalization capability. Moreover, the
improvement introduced by ACT is consistent with various VLMs, making it a
plug-and-play solution.

</details>

### [158] [PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs](https://arxiv.org/abs/2505.03254)
*Lukas Meiner,Jens Mehnert,Alexandru Paul Condurache*

Main category: cs.CV

TLDR: PROM是一种量化深度可分离卷积网络的方法，通过选择性使用三元和8位权重，显著降低能耗和存储需求。


<details>
  <summary>Details</summary>
Motivation: 现代深度可分离架构中计算成本分布不均，现有量化方法未能充分利用效率潜力。

Method: PROM采用两种位宽：点卷积用三元权重，其余用8位权重，并通过量化感知训练实现。

Result: 在MobileNetV2上，PROM将能耗降低23.9倍，存储减少2.7倍，同时保持分类性能。

Conclusion: PROM提升了量化卷积模型在能耗与精度上的帕累托前沿，为资源受限设备提供高效解决方案。

Abstract: Convolutional neural networks (CNNs) are crucial for computer vision tasks on
resource-constrained devices. Quantization effectively compresses these models,
reducing storage size and energy cost. However, in modern depthwise-separable
architectures, the computational cost is distributed unevenly across its
components, with pointwise operations being the most expensive. By applying a
general quantization scheme to this imbalanced cost distribution, existing
quantization approaches fail to fully exploit potential efficiency gains. To
this end, we introduce PROM, a straightforward approach for quantizing modern
depthwise-separable convolutional networks by selectively using two distinct
bit-widths. Specifically, pointwise convolutions are quantized to ternary
weights, while the remaining modules use 8-bit weights, which is achieved
through a simple quantization-aware training procedure. Additionally, by
quantizing activations to 8-bit, our method transforms pointwise convolutions
with ternary weights into int8 additions, which enjoy broad support across
hardware platforms and effectively eliminates the need for expensive
multiplications. Applying PROM to MobileNetV2 reduces the model's energy cost
by more than an order of magnitude (23.9x) and its storage size by 2.7x
compared to the float16 baseline while retaining similar classification
performance on ImageNet. Our method advances the Pareto frontier for energy
consumption vs. top-1 accuracy for quantized convolutional models on ImageNet.
PROM addresses the challenges of quantizing depthwise-separable convolutional
networks to both ternary and 8-bit weights, offering a simple way to reduce
energy cost and storage size.

</details>

### [159] [DiffVQA: Video Quality Assessment Using Diffusion Feature Extractor](https://arxiv.org/abs/2505.03261)
*Wei-Ting Chen,Yu-Jiet Vong,Yi-Tsung Lee,Sy-Yen Kuo,Qiang Gao,Sizhuo Ma,Jian Wang*

Main category: cs.CV

TLDR: DiffVQA是一种新型视频质量评估框架，利用扩散模型提取特征，结合Mamba模块处理时间动态，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN和ViT的方法难以准确对齐人类感知，且数据集规模有限，DiffVQA通过扩散模型解决这一问题。

Method: DiffVQA通过控制模块调整扩散模型重构输入帧，分别提取语义和失真特征，并引入Mamba模块增强时间动态处理能力。

Result: 实验表明DiffVQA在多个数据集上表现优异，尤其在跨数据集泛化能力上显著优于CNN和ViT。

Conclusion: 扩散模型作为特征提取器可显著提升VQA性能，DiffVQA为视频质量评估提供了新思路。

Abstract: Video Quality Assessment (VQA) aims to evaluate video quality based on
perceptual distortions and human preferences. Despite the promising performance
of existing methods using Convolutional Neural Networks (CNNs) and Vision
Transformers (ViTs), they often struggle to align closely with human
perceptions, particularly in diverse real-world scenarios. This challenge is
exacerbated by the limited scale and diversity of available datasets. To
address this limitation, we introduce a novel VQA framework, DiffVQA, which
harnesses the robust generalization capabilities of diffusion models
pre-trained on extensive datasets. Our framework adapts these models to
reconstruct identical input frames through a control module. The adapted
diffusion model is then used to extract semantic and distortion features from a
resizing branch and a cropping branch, respectively. To enhance the model's
ability to handle long-term temporal dynamics, a parallel Mamba module is
introduced, which extracts temporal coherence augmented features that are
merged with the diffusion features to predict the final score. Experiments
across multiple datasets demonstrate DiffVQA's superior performance on
intra-dataset evaluations and its exceptional generalization across datasets.
These results confirm that leveraging a diffusion model as a feature extractor
can offer enhanced VQA performance compared to CNN and ViT backbones.

</details>

### [160] [OccCylindrical: Multi-Modal Fusion with Cylindrical Representation for 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2505.03284)
*Zhenxing Ming,Julie Stephany Berrio,Mao Shan,Yaoqi Huang,Hongyu Lyu,Nguyen Hoang Khoi Tran,Tzu-Yun Tseng,Stewart Worrall*

Main category: cs.CV

TLDR: 论文提出了一种名为OccCylindrical的方法，用于在圆柱坐标系下融合和优化多模态特征，以提升3D语义占用预测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于多传感器融合的方法主要在笛卡尔坐标系下使用传感器信息，忽略了传感器读数的分布，导致细节丢失和性能下降。

Method: 提出OccCylindrical方法，在圆柱坐标系下合并和优化不同模态的特征，保留更多几何细节。

Result: 在nuScenes数据集（包括雨天和夜间场景）上的实验验证了方法的有效性和领先性能。

Conclusion: OccCylindrical通过圆柱坐标系下的特征融合，显著提升了3D语义占用预测的性能。

Abstract: The safe operation of autonomous vehicles (AVs) is highly dependent on their
understanding of the surroundings. For this, the task of 3D semantic occupancy
prediction divides the space around the sensors into voxels, and labels each
voxel with both occupancy and semantic information. Recent perception models
have used multisensor fusion to perform this task. However, existing
multisensor fusion-based approaches focus mainly on using sensor information in
the Cartesian coordinate system. This ignores the distribution of the sensor
readings, leading to a loss of fine-grained details and performance
degradation. In this paper, we propose OccCylindrical that merges and refines
the different modality features under cylindrical coordinates. Our method
preserves more fine-grained geometry detail that leads to better performance.
Extensive experiments conducted on the nuScenes dataset, including challenging
rainy and nighttime scenarios, confirm our approach's effectiveness and
state-of-the-art performance. The code will be available at:
https://github.com/DanielMing123/OccCylindrical

</details>

### [161] [Base-Detail Feature Learning Framework for Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2505.03286)
*Zhihao Gong,Lian Wu,Yong Xu*

Main category: cs.CV

TLDR: 提出了一种Base-Detail Feature Learning Framework (BDLF)，通过同时利用模态共享和模态特定信息，提升可见光-红外行人重识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用不同模态的信息，主要关注模态共享特征而忽略模态特定细节。

Method: BDLF通过无损细节特征提取模块和互补基础嵌入生成机制，分别挖掘细节和基础特征，并通过相关性限制方法确保特征丰富性。

Result: 在SYSU-MM01、RegDB和LLCM数据集上的实验验证了BDLF的有效性。

Conclusion: BDLF通过同时利用模态共享和模态特定信息，显著提升了可见光-红外行人重识别的性能。

Abstract: Visible-infrared person re-identification (VIReID) provides a solution for
ReID tasks in 24-hour scenarios; however, significant challenges persist in
achieving satisfactory performance due to the substantial discrepancies between
visible (VIS) and infrared (IR) modalities. Existing methods inadequately
leverage information from different modalities, primarily focusing on digging
distinguishing features from modality-shared information while neglecting
modality-specific details. To fully utilize differentiated minutiae, we propose
a Base-Detail Feature Learning Framework (BDLF) that enhances the learning of
both base and detail knowledge, thereby capitalizing on both modality-shared
and modality-specific information. Specifically, the proposed BDLF mines detail
and base features through a lossless detail feature extraction module and a
complementary base embedding generation mechanism, respectively, supported by a
novel correlation restriction method that ensures the features gained by BDLF
enrich both detail and base knowledge across VIS and IR features. Comprehensive
experiments conducted on the SYSU-MM01, RegDB, and LLCM datasets validate the
effectiveness of BDLF.

</details>

### [162] [Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach](https://arxiv.org/abs/2505.03299)
*Pierre Adorni,Minh-Tan Pham,Stéphane May,Sébastien Lefèvre*

Main category: cs.CV

TLDR: 提出了一种基于“能力编码”的方法，无需微调即可预测基础模型在多个下游任务中的性能，简化模型选择并提供文献新视角。


<details>
  <summary>Details</summary>
Motivation: 现有75个遥感视觉基础模型在性能上无一致优势，需一种成本效益高的方法比较和预测其性能。

Method: 采用“能力编码”方法，无需对每个下游任务进行微调，即可预测模型性能。

Result: 该方法能简化基础模型选择，并为现有文献提供新视角。

Conclusion: 能力编码方法为模型比较和未来研究提供了新方向。

Abstract: Foundation models constitute a significant advancement in computer vision:
after a single, albeit costly, training phase, they can address a wide array of
tasks. In the field of Earth observation, over 75 remote sensing vision
foundation models have been developed in the past four years. However, none has
consistently outperformed the others across all available downstream tasks. To
facilitate their comparison, we propose a cost-effective method for predicting
a model's performance on multiple downstream tasks without the need for
fine-tuning on each one. This method is based on what we call "capabilities
encoding." The utility of this novel approach is twofold: we demonstrate its
potential to simplify the selection of a foundation model for a given new task,
and we employ it to offer a fresh perspective on the existing literature,
suggesting avenues for future research. Codes are available at
https://github.com/pierreadorni/capabilities-encoding.

</details>

### [163] [3D Can Be Explored In 2D: Pseudo-Label Generation for LiDAR Point Clouds Using Sensor-Intensity-Based 2D Semantic Segmentation](https://arxiv.org/abs/2505.03300)
*Andrew Caunes,Thierry Chateau,Vincent Frémont*

Main category: cs.CV

TLDR: 提出了一种无需3D标注的3D语义分割方法，利用2D分割模型和投票机制生成伪标签，适用于无监督域适应任务。


<details>
  <summary>Details</summary>
Motivation: 解决3D点云语义分割中标注成本高和域偏移问题，避免依赖额外模态数据。

Method: 通过LiDAR扫描生成2D视图，使用预训练2D分割模型进行分割，再通过投票机制将结果回投影到3D点云。

Result: 展示了伪标签在无监督域适应任务中的潜力，并通过消融研究验证了方法的有效性。

Conclusion: 该方法为3D语义分割提供了一种无需3D标注的解决方案，具有实际应用价值。

Abstract: Semantic segmentation of 3D LiDAR point clouds, essential for autonomous
driving and infrastructure management, is best achieved by supervised learning,
which demands extensive annotated datasets and faces the problem of domain
shifts. We introduce a new 3D semantic segmentation pipeline that leverages
aligned scenes and state-of-the-art 2D segmentation methods, avoiding the need
for direct 3D annotation or reliance on additional modalities such as camera
images at inference time. Our approach generates 2D views from LiDAR scans
colored by sensor intensity and applies 2D semantic segmentation to these views
using a camera-domain pretrained model. The segmented 2D outputs are then
back-projected onto the 3D points, with a simple voting-based estimator that
merges the labels associated to each 3D point. Our main contribution is a
global pipeline for 3D semantic segmentation requiring no prior 3D annotation
and not other modality for inference, which can be used for pseudo-label
generation. We conduct a thorough ablation study and demonstrate the potential
of the generated pseudo-labels for the Unsupervised Domain Adaptation task.

</details>

### [164] [Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices](https://arxiv.org/abs/2505.03303)
*Tasnim Shahriar*

Main category: cs.CV

TLDR: 本文评估了五种轻量级深度学习模型在资源受限环境中的表现，发现迁移学习显著提升性能，EfficientNetV2准确率最高，MobileNetV3平衡最佳，SqueezeNet速度最快。


<details>
  <summary>Details</summary>
Motivation: 研究轻量级模型在低内存设备等资源受限环境中的适用性，为边缘计算和移动平台优化深度学习系统。

Method: 对五种模型（MobileNetV3 Small、ResNet18等）在三个数据集上评估分类准确率、推理时间、FLOPs和模型大小，并研究超参数调优和数据增强的影响。

Result: 迁移学习显著提升性能，EfficientNetV2准确率最高，MobileNetV3平衡最佳，SqueezeNet速度最快。

Conclusion: 研究为资源受限环境中的模型部署提供了实用建议，优化了轻量级深度学习系统的性能。

Abstract: This paper presents a comprehensive evaluation of lightweight deep learning
models for image classification, emphasizing their suitability for deployment
in resource-constrained environments such as low-memory devices. Five
state-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,
EfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse
datasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using
four key performance metrics: classification accuracy, inference time,
floating-point operations (FLOPs), and model size. Additionally, we investigate
the impact of hyperparameter tuning, data augmentation, and training paradigms
by comparing pretrained models with scratch-trained counterparts, focusing on
MobileNetV3 Small. Our findings reveal that transfer learning significantly
enhances model accuracy and computational efficiency, particularly for complex
datasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest
accuracy, while MobileNetV3 offers the best balance between accuracy and
efficiency, and SqueezeNet excels in inference speed and compactness. This
study highlights critical trade-offs between accuracy and efficiency, offering
actionable insights for deploying lightweight models in real-world applications
where computational resources are limited. By addressing these challenges, this
research contributes to optimizing deep learning systems for edge computing and
mobile platforms.

</details>

### [165] [3D Gaussian Splatting Data Compression with Mixture of Priors](https://arxiv.org/abs/2505.03310)
*Lei Liu,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TLDR: 提出了一种基于混合先验（MoP）策略的3D高斯泼溅数据压缩方法，解决了现有方法在熵模型和量化策略上的不足，实现了无损和有损压缩的优化。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅数据压缩方法在熵模型和量化策略上存在不足，无法充分利用超先验信息或实现细粒度的元素级量化。

Method: 采用混合先验（MoP）策略，通过多个轻量级MLP处理超先验信息，并结合门控机制生成MoP特征。无损压缩中用于改进条件熵模型，有损压缩中用于指导元素级量化。

Result: 在多个基准测试（如Mip-NeRF360、BungeeNeRF等）中实现了最先进的性能。

Conclusion: MoP策略有效提升了3D高斯泼溅数据压缩的性能，为存储和传输提供了高效解决方案。

Abstract: 3D Gaussian Splatting (3DGS) data compression is crucial for enabling
efficient storage and transmission in 3D scene modeling. However, its
development remains limited due to inadequate entropy models and suboptimal
quantization strategies for both lossless and lossy compression scenarios,
where existing methods have yet to 1) fully leverage hyperprior information to
construct robust conditional entropy models, and 2) apply fine-grained,
element-wise quantization strategies for improved compression granularity. In
this work, we propose a novel Mixture of Priors (MoP) strategy to
simultaneously address these two challenges. Specifically, inspired by the
Mixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior
information through multiple lightweight MLPs to generate diverse prior
features, which are subsequently integrated into the MoP feature via a gating
mechanism. To enhance lossless compression, the resulting MoP feature is
utilized as a hyperprior to improve conditional entropy modeling. Meanwhile,
for lossy compression, we employ the MoP feature as guidance information in an
element-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine
Quantization (C2FQ) strategy with a predefined quantization step value.
Specifically, we expand the quantization step value into a matrix and
adaptively refine it from coarse to fine granularity, guided by the MoP
feature, thereby obtaining a quantization step matrix that facilitates
element-wise quantization. Extensive experiments demonstrate that our proposed
3DGS data compression framework achieves state-of-the-art performance across
multiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and
Tank&Temples.

</details>

### [166] [Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.03318)
*Yibin Wang,Zhimin Li,Yuhang Zang,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TLDR: 论文提出了一种基于长链思维（CoT）的多模态奖励模型UnifiedReward-Think，通过强化微调提升奖励信号的可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型（RMs）的浅层推理导致奖励信号不准确，引入长链思维可提升其性能。

Method: 采用探索驱动的强化微调方法，分阶段利用GPT-4o推理数据和统一多模态偏好数据优化模型。

Result: 实验表明该模型在多种视觉奖励任务中表现优越。

Conclusion: UnifiedReward-Think通过长链推理显著提升了奖励模型的准确性和鲁棒性。

Abstract: Recent advances in multimodal Reward Models (RMs) have shown significant
promise in delivering reward signals to align vision models with human
preferences. However, current RMs are generally restricted to providing direct
responses or engaging in shallow reasoning processes with limited depth, often
leading to inaccurate reward signals. We posit that incorporating explicit long
chains of thought (CoT) into the reward reasoning process can significantly
strengthen their reliability and robustness. Furthermore, we believe that once
RMs internalize CoT reasoning, their direct response accuracy can also be
improved through implicit reasoning capabilities. To this end, this paper
proposes UnifiedReward-Think, the first unified multimodal CoT-based reward
model, capable of multi-dimensional, step-by-step long-chain reasoning for both
visual understanding and generation reward tasks. Specifically, we adopt an
exploration-driven reinforcement fine-tuning approach to elicit and incentivize
the model's latent complex reasoning ability: (1) We first use a small amount
of image generation preference data to distill the reasoning process of GPT-4o,
which is then used for the model's cold start to learn the format and structure
of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge
and generalization capabilities, we prepare large-scale unified multimodal
preference data to elicit the model's reasoning process across various vision
tasks. During this phase, correct reasoning outputs are retained for rejection
sampling to refine the model (3) while incorrect predicted samples are finally
used for Group Relative Policy Optimization (GRPO) based reinforcement
fine-tuning, enabling the model to explore diverse reasoning paths and optimize
for correct and robust solutions. Extensive experiments across various vision
reward tasks demonstrate the superiority of our model.

</details>

### [167] [SD-VSum: A Method and Dataset for Script-Driven Video Summarization](https://arxiv.org/abs/2505.03319)
*Manolis Mylonas,Evlampios Apostolidis,Vasileios Mezaris*

Main category: cs.CV

TLDR: 论文提出了一种基于脚本的视频摘要任务（SD-VSum），通过用户提供的脚本选择视频中最相关的部分生成摘要。扩展了VideoXum数据集，并开发了一种新的跨模态注意力网络架构，实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频摘要方法多为通用型，无法根据用户具体需求生成定制化摘要。本文旨在通过脚本驱动的方式，满足用户对摘要内容的个性化需求。

Method: 扩展VideoXum数据集，加入自然语言描述；提出SD-VSum网络架构，利用跨模态注意力机制对齐和融合视觉与文本信息。

Result: SD-VSum在性能上优于现有的查询驱动和通用摘要方法，能够根据用户需求生成适配的摘要。

Conclusion: SD-VSum通过脚本驱动和跨模态融合，有效实现了用户定制化的视频摘要，展示了其在实际应用中的潜力。

Abstract: In this work, we introduce the task of script-driven video summarization,
which aims to produce a summary of the full-length video by selecting the parts
that are most relevant to a user-provided script outlining the visual content
of the desired summary. Following, we extend a recently-introduced large-scale
dataset for generic video summarization (VideoXum) by producing natural
language descriptions of the different human-annotated summaries that are
available per video. In this way we make it compatible with the introduced
task, since the available triplets of ``video, summary and summary
description'' can be used for training a method that is able to produce
different summaries for a given video, driven by the provided script about the
content of each summary. Finally, we develop a new network architecture for
script-driven video summarization (SD-VSum), that relies on the use of a
cross-modal attention mechanism for aligning and fusing information from the
visual and text modalities. Our experimental evaluations demonstrate the
advanced performance of SD-VSum against state-of-the-art approaches for
query-driven and generic (unimodal and multimodal) summarization from the
literature, and document its capacity to produce video summaries that are
adapted to each user's needs about their content.

</details>

### [168] [Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning](https://arxiv.org/abs/2505.03327)
*José-Luis Bueso-Bello,Benjamin Chauvel,Daniel Carcereri,Philipp Posovszky,Pietro Milillo,Jennifer Ruiz,Juan-Carlos Fernández-Diaz,Carolina González,Michele Martone,Ronny Hänsch,Paola Rizzoli*

Main category: cs.CV

TLDR: 论文提出了一种结合自监督学习和监督学习的框架，用于高分辨率森林测绘，解决了标记数据不足的问题，并在亚马逊雨林的实际应用中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 利用TanDEM-X任务的高分辨率能力，克服中分辨率产品在森林测绘中的局限性（如窄路检测和精确轮廓划分），同时解决高分辨率下可靠标记数据稀缺的问题。

Method: 采用自监督学习技术从输入特征中提取高信息量表示，随后用少量可靠标记数据进行监督训练。

Result: 在标记数据有限的情况下，提出的自监督框架在分类准确性上显著优于全监督方法，尤其是在亚马逊雨林的实际应用中表现突出。

Conclusion: 该框架为大规模高分辨率森林测绘提供了极具前景的解决方案，尤其是在标记数据稀缺的场景下。

Abstract: Deep learning models have shown encouraging capabilities for mapping
accurately forests at medium resolution with TanDEM-X interferometric SAR data.
Such models, as most of current state-of-the-art deep learning techniques in
remote sensing, are trained in a fully-supervised way, which requires a large
amount of labeled data for training and validation. In this work, our aim is to
exploit the high-resolution capabilities of the TanDEM-X mission to map forests
at 6 m. The goal is to overcome the intrinsic limitations posed by
midresolution products, which affect, e.g., the detection of narrow roads
within vegetated areas and the precise delineation of forested regions
contours. To cope with the lack of extended reliable reference datasets at such
a high resolution, we investigate self-supervised learning techniques for
extracting highly informative representations from the input features, followed
by a supervised training step with a significantly smaller number of reliable
labels. A 1 m resolution forest/non-forest reference map over Pennsylvania,
USA, allows for comparing different training approaches for the development of
an effective forest mapping framework with limited labeled samples. We select
the best-performing approach over this test region and apply it in a real-case
forest mapping scenario over the Amazon rainforest, where only very few labeled
data at high resolution are available. In this challenging scenario, the
proposed self-supervised framework significantly enhances the classification
accuracy with respect to fully-supervised methods, trained using the same
amount of labeled data, representing an extremely promising starting point for
large-scale, very high-resolution forest mapping with TanDEM-X data.

</details>

### [169] [FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for Scene Text Editing](https://arxiv.org/abs/2505.03329)
*Rui Lan,Yancheng Bai,Xu Duan,Mingxing Li,Lei Sun,Xiangxiang Chu*

Main category: cs.CV

TLDR: FLUX-Text是一种基于FLUX-Fill的多语言场景文本编辑框架，通过轻量级字形和文本嵌入模块提升文本编辑效果，尤其在非拉丁字符（如中文）上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有潜在扩散模型（LDM）在场景文本编辑中生成不准确或不可识别字符（尤其是复杂字形结构如中文）的问题。

Method: 提出FLUX-Text框架，结合视觉和文本模态的字形条件，设计轻量级字形和文本嵌入模块，仅需10万训练样本。

Result: 在公开数据集上，FLUX-Text在文本保真度上优于现有方法，达到最先进性能。

Conclusion: FLUX-Text通过轻量级设计有效提升了多语言场景文本编辑的准确性和生成质量。

Abstract: The task of scene text editing is to modify or add texts on images while
maintaining the fidelity of newly generated text and visual coherence with the
background. Recent works based on latent diffusion models (LDM) show improved
text editing results, yet still face challenges and often generate inaccurate
or unrecognizable characters, especially for non-Latin ones (\eg, Chinese),
which have complex glyph structures. To address these issues, we present
FLUX-Text, a simple and advanced multilingual scene text editing framework
based on FLUX-Fill. Specifically, we carefully investigate glyph conditioning,
considering both visual and textual modalities. To retain the original
generative capabilities of FLUX-Fill while enhancing its understanding and
generation of glyphs, we propose lightweight glyph and text embedding modules.
Owning to the lightweight design, FLUX-Text is trained only with $100K$
training examples compared to current popular methods trained with 2.9M ones.
With no bells and whistles, our method achieves state-of-the-art performance on
text editing tasks. Qualitative and quantitative experiments on the public
datasets demonstrate that our method surpasses previous works in text fidelity.

</details>

### [170] [From Word to Sentence: A Large-Scale Multi-Instance Dataset for Open-Set Aerial Detection](https://arxiv.org/abs/2505.03334)
*Guoting Wei,Yu Liu,Xia Yuan,Xizhe Xue,Linlin Guo,Yifan Yang,Chunxia Zhao,Zongwen Bai,Haokui Zhang,Rong Xiao*

Main category: cs.CV

TLDR: 论文提出了一种大规模语言引导的开集航空检测数据集MI-OAD，并开发了自动标注工具OS-W2S Label Engine，显著提升了开集检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言引导方法因数据集有限，难以满足细粒度开集检测需求。

Method: 构建包含词、短语和句子三级语言引导的数据集MI-OAD，结合视觉-语言模型与BERT后处理开发自动标注工具。

Result: MI-OAD包含16万图像和200万图像-文本对，Grounding DINO在零样本条件下性能显著提升。

Conclusion: MI-OAD和标注工具填补了现有数据不足，推动了开集航空检测的发展。

Abstract: In recent years, language-guided open-world aerial object detection has
gained significant attention due to its better alignment with real-world
application needs. However, due to limited datasets, most existing
language-guided methods primarily focus on vocabulary, which fails to meet the
demands of more fine-grained open-world detection. To address this limitation,
we propose constructing a large-scale language-guided open-set aerial detection
dataset, encompassing three levels of language guidance: from words to phrases,
and ultimately to sentences. Centered around an open-source large
vision-language model and integrating image-operation-based preprocessing with
BERT-based postprocessing, we present the OS-W2S Label Engine, an automatic
annotation pipeline capable of handling diverse scene annotations for aerial
images. Using this label engine, we expand existing aerial detection datasets
with rich textual annotations and construct a novel benchmark dataset, called
Multi-instance Open-set Aerial Dataset (MI-OAD), addressing the limitations of
current remote sensing grounding data and enabling effective open-set aerial
detection. Specifically, MI-OAD contains 163,023 images and 2 million
image-caption pairs, approximately 40 times larger than comparable datasets. We
also employ state-of-the-art open-set methods from the natural image domain,
trained on our proposed dataset, to validate the model's open-set detection
capabilities. For instance, when trained on our dataset, Grounding DINO
achieves improvements of 29.5 AP_{50} and 33.7 Recall@10 for sentence inputs
under zero-shot transfer conditions. Both the dataset and the label engine will
be released publicly.

</details>

### [171] [A Vision-Language Model for Focal Liver Lesion Classification](https://arxiv.org/abs/2505.03350)
*Song Jian,Hu Yuchang,Wang Hui,Chen Yen-Wei*

Main category: cs.CV

TLDR: Liver-VLM模型利用视觉-语言模型（VLM）改进肝脏病灶分类，优于传统CNN和CLIP模型，尤其在数据有限时表现更佳。


<details>
  <summary>Details</summary>
Motivation: 传统监督深度学习依赖大规模标注数据，而医学影像数据有限，因此需要更高效的分类方法。

Method: Liver-VLM结合文本编码器引入类别信息，通过计算图像与文本嵌入的余弦相似度，并用交叉熵损失优化模型。

Result: 在MPCT-FLLs数据集上，Liver-VLM在准确率和AUC上优于标准CLIP和MedCLIP，轻量级ResNet18进一步提升了性能。

Conclusion: Liver-VLM在数据有限条件下表现优异，为肝脏病灶分类提供了高效解决方案。

Abstract: Accurate classification of focal liver lesions is crucial for diagnosis and
treatment in hepatology. However, traditional supervised deep learning models
depend on large-scale annotated datasets, which are often limited in medical
imaging. Recently, Vision-Language models (VLMs) such as Contrastive
Language-Image Pre-training model (CLIP) has been applied to image
classifications. Compared to the conventional convolutional neural network
(CNN), which classifiers image based on visual information only, VLM leverages
multimodal learning with text and images, allowing it to learn effectively even
with a limited amount of labeled data. Inspired by CLIP, we pro-pose a
Liver-VLM, a model specifically designed for focal liver lesions (FLLs)
classification. First, Liver-VLM incorporates class information into the text
encoder without introducing additional inference overhead. Second, by
calculating the pairwise cosine similarities between image and text embeddings
and optimizing the model with a cross-entropy loss, Liver-VLM ef-fectively
aligns image features with class-level text features. Experimental results on
MPCT-FLLs dataset demonstrate that the Liver-VLM model out-performs both the
standard CLIP and MedCLIP models in terms of accuracy and Area Under the Curve
(AUC). Further analysis shows that using a lightweight ResNet18 backbone
enhances classification performance, particularly under data-constrained
conditions.

</details>

### [172] [GUAVA: Generalizable Upper Body 3D Gaussian Avatar](https://arxiv.org/abs/2505.03351)
*Dongbin Zhang,Yunfei Liu,Lijian Lin,Ye Zhu,Yang Li,Minghan Qin,Yu Li,Haoqian Wang*

Main category: cs.CV

TLDR: GUAVA框架通过单图像快速重建高质量、可动画的上半身3D高斯化身，显著提升渲染质量和速度。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体化身重建方法依赖多视图或单目视频，且受限于SMPLX模型的表达能力，难以处理面部表情。

Method: 提出表达性人体模型（EHM）增强面部表情能力，结合逆向纹理映射和投影采样技术，通过神经细化器优化渲染图像。

Result: GUAVA在渲染质量上显著优于现有方法，重建时间仅0.1秒，支持实时动画和渲染。

Conclusion: GUAVA为单图像重建高质量可动画3D化身提供了高效解决方案。

Abstract: Reconstructing a high-quality, animatable 3D human avatar with expressive
facial and hand motions from a single image has gained significant attention
due to its broad application potential. 3D human avatar reconstruction
typically requires multi-view or monocular videos and training on individual
IDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's
expressiveness, these methods often focus on body motion but struggle with
facial expressions. To address these challenges, we first introduce an
expressive human model (EHM) to enhance facial expression capabilities and
develop an accurate tracking method. Based on this template model, we propose
GUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar
reconstruction. We leverage inverse texture mapping and projection sampling
techniques to infer Ubody (upper-body) Gaussians from a single image. The
rendered images are refined through a neural refiner. Experimental results
demonstrate that GUAVA significantly outperforms previous methods in rendering
quality and offers significant speed improvements, with reconstruction times in
the sub-second range (0.1s), and supports real-time animation and rendering.

</details>

### [173] [Interpretable Zero-shot Learning with Infinite Class Concepts](https://arxiv.org/abs/2505.03361)
*Zihan Ye,Shreyank N Gowda,Shiming Chen,Yaochu Jin,Kaizhu Huang,Xiaobo Jin*

Main category: cs.CV

TLDR: 论文提出了一种名为InfZSL的新框架，通过动态生成无限短语级类别概念，结合基于熵的评分机制，解决了ZSL中LLM生成语义的透明性和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 解决零样本学习（ZSL）中LLM生成类别语义的透明性和幻觉问题，提升类别语义的可迁移性和区分性。

Method: 提出InfZSL框架，利用LLM动态生成无限短语级类别概念，并通过熵评分机制选择最优概念。

Result: 在三个流行基准数据集上表现显著提升，生成可解释的图像相关概念。

Conclusion: InfZSL框架有效解决了ZSL中的语义生成问题，提升了性能与可解释性。

Abstract: Zero-shot learning (ZSL) aims to recognize unseen classes by aligning images
with intermediate class semantics, like human-annotated concepts or class
definitions. An emerging alternative leverages Large-scale Language Models
(LLMs) to automatically generate class documents. However, these methods often
face challenges with transparency in the classification process and may suffer
from the notorious hallucination problem in LLMs, resulting in non-visual class
semantics. This paper redefines class semantics in ZSL with a focus on
transferability and discriminability, introducing a novel framework called
Zero-shot Learning with Infinite Class Concepts (InfZSL). Our approach
leverages the powerful capabilities of LLMs to dynamically generate an
unlimited array of phrase-level class concepts. To address the hallucination
challenge, we introduce an entropy-based scoring process that incorporates a
``goodness" concept selection mechanism, ensuring that only the most
transferable and discriminative concepts are selected. Our InfZSL framework not
only demonstrates significant improvements on three popular benchmark datasets
but also generates highly interpretable, image-grounded concepts. Code will be
released upon acceptance.

</details>

### [174] [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/abs/2505.03414)
*Fangming Cui,Yonggang Zhang,Xuan Wang,Xinmei Tian,Jun Yu*

Main category: cs.CV

TLDR: 论文提出了一种特征矩阵（FM）正则化方法，用于提升大型视觉语言模型在非目标特定任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法在非目标特定任务上表现不佳，可能因过拟合训练导致模型遗忘通用知识。

Method: 通过提取和利用通用知识构建特征矩阵（FM），从深度和细粒度角度捕捉输入语义，避免过拟合。

Result: FM作为通用模块兼容现有框架，显著提升非目标特定任务性能，达到最先进水平。

Conclusion: FM方法有效解决了模型在非目标特定任务上的过拟合问题，展示了其灵活性和高效性。

Abstract: Recent developments in prompt learning of large vision-language models have
significantly improved performance in target-specific tasks. However, these
prompt optimizing methods often struggle to tackle the target-unspecific or
generalizable tasks effectively. It may be attributed to the fact that
overfitting training causes the model to forget its general knowledge having
strong promotion on target-unspecific tasks. To alleviate this issue, we
propose a novel Features Matrix (FM) regularization approach designed to
enhance these models on target-unspecific tasks. Our method extracts and
leverages general knowledge, shaping a Features Matrix (FM). Specifically, the
FM captures the semantics of diverse inputs from a deep and fine perspective,
preserving essential general knowledge, which mitigates the risk of
overfitting. Representative evaluations demonstrate that: 1) the FM is
compatible with existing frameworks as a generic and flexible module, and 2)
the FM significantly showcases its effectiveness in enhancing target-unspecific
tasks, achieving state-of-the-art performance.

</details>

### [175] [3D Surface Reconstruction with Enhanced High-Frequency Details](https://arxiv.org/abs/2505.03362)
*Shikun Zhang,Yiqun Wang,Cunjian Chen,Yong Li,Qiuhong Ke*

Main category: cs.CV

TLDR: FreNeuS是一种基于高频信息的神经隐式3D重建方法，通过动态采样和高频加权提升表面细节重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有神经表面重建方法随机采样图像，导致高频细节学习不足，重建结果过于平滑。

Method: 利用像素梯度变化获取高频区域，动态调整射线采样策略，并设计高频加权方法约束细节重建。

Result: 实验表明，FreNeuS能重建精细表面细节，质量优于现有方法，且适用于所有基于NeuS的工作。

Conclusion: FreNeuS通过高频信息引导，有效解决了表面细节不足的问题，提升了重建质量。

Abstract: Neural implicit 3D reconstruction can reproduce shapes without 3D
supervision, and it learns the 3D scene through volume rendering methods and
neural implicit representations. Current neural surface reconstruction methods
tend to randomly sample the entire image, making it difficult to learn
high-frequency details on the surface, and thus the reconstruction results tend
to be too smooth. We designed a method (FreNeuS) based on high-frequency
information to solve the problem of insufficient surface detail. Specifically,
FreNeuS uses pixel gradient changes to easily acquire high-frequency regions in
an image and uses the obtained high-frequency information to guide surface
detail reconstruction. High-frequency information is first used to guide the
dynamic sampling of rays, applying different sampling strategies according to
variations in high-frequency regions. To further enhance the focus on surface
details, we have designed a high-frequency weighting method that constrains the
representation of high-frequency details during the reconstruction process.
Qualitative and quantitative results show that our method can reconstruct fine
surface details and obtain better surface reconstruction quality compared to
existing methods. In addition, our method is more applicable and can be
generalized to any NeuS-based work.

</details>

### [176] [Reducing Annotation Burden in Physical Activity Research Using Vision-Language Models](https://arxiv.org/abs/2505.03374)
*Abram Schonfeldt,Benjamin Maylor,Xiaofang Chen,Ronald Clark,Aiden Doherty*

Main category: cs.CV

TLDR: 比较三种视觉语言模型和两种判别模型在自由生活场景下的性能，发现视觉语言模型和判别模型在预测久坐行为上表现相当，但在其他活动强度上表现下降。


<details>
  <summary>Details</summary>
Motivation: 验证和开发基于可穿戴设备的机器学习方法，减少人工标注负担。

Method: 比较三种视觉语言模型和两种判别模型在两个自由生活验证研究中的性能。

Result: 视觉语言模型和判别模型在预测久坐行为上表现相近，但在其他活动强度上表现较差，且在外部验证中性能下降。

Conclusion: 免费计算机视觉模型可帮助标注久坐行为，减少标注负担，但性能受限于数据相似性。

Abstract: Introduction: Data from wearable devices collected in free-living settings,
and labelled with physical activity behaviours compatible with health research,
are essential for both validating existing wearable-based measurement
approaches and developing novel machine learning approaches. One common way of
obtaining these labels relies on laborious annotation of sequences of images
captured by cameras worn by participants through the course of a day. Methods:
We compare the performance of three vision language models and two
discriminative models on two free-living validation studies with 161 and 111
participants, collected in Oxfordshire, United Kingdom and Sichuan, China,
respectively, using the Autographer (OMG Life, defunct) wearable camera.
Results: We found that the best open-source vision-language model (VLM) and
fine-tuned discriminative model (DM) achieved comparable performance when
predicting sedentary behaviour from single images on unseen participants in the
Oxfordshire study; median F1-scores: VLM = 0.89 (0.84, 0.92), DM = 0.91 (0.86,
0.95). Performance declined for light (VLM = 0.60 (0.56,0.67), DM = 0.70 (0.63,
0.79)), and moderate-to-vigorous intensity physical activity (VLM = 0.66 (0.53,
0.85); DM = 0.72 (0.58, 0.84)). When applied to the external Sichuan study,
performance fell across all intensity categories, with median Cohen's
kappa-scores falling from 0.54 (0.49, 0.64) to 0.26 (0.15, 0.37) for the VLM,
and from 0.67 (0.60, 0.74) to 0.19 (0.10, 0.30) for the DM. Conclusion: Freely
available computer vision models could help annotate sedentary behaviour,
typically the most prevalent activity of daily living, from wearable camera
images within similar populations to seen data, reducing the annotation burden.

</details>

### [177] [Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant](https://arxiv.org/abs/2505.03380)
*Haonan Wang,Jiaji Mao,Lehan Wang,Qixiang Zhang,Marawan Elbatel,Yi Qin,Huijun Hu,Baoxun Li,Wenhui Deng,Weifeng Qin,Hongrui Li,Jialin Liang,Jun Shen,Xiaomeng Li*

Main category: cs.CV

TLDR: RCMed是一种全栈AI助手，通过多模态对齐和分层视觉-语言基础，提升医学诊断的准确性，在165个临床任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决医学AI助手在多模态内容准确性不足和真实场景验证不足的问题。

Method: 采用自增强相关机制和颜色区域描述策略，结合视觉特征与语言语义，形成闭环优化。

Result: 在20种癌症类型的外部验证中表现优异，细胞分割精度提升23.5%。

Conclusion: RCMed展示了多模态模型在复杂场景中实现人类水平解释的能力，推动以人为中心的AI医疗发展。

Abstract: Medical AI assistants support doctors in disease diagnosis, medical image
analysis, and report generation. However, they still face significant
challenges in clinical use, including limited accuracy with multimodal content
and insufficient validation in real-world settings. We propose RCMed, a
full-stack AI assistant that improves multimodal alignment in both input and
output, enabling precise anatomical delineation, accurate localization, and
reliable diagnosis through hierarchical vision-language grounding. A
self-reinforcing correlation mechanism allows visual features to inform
language context, while language semantics guide pixel-wise attention, forming
a closed loop that refines both modalities. This correlation is enhanced by a
color region description strategy, translating anatomical structures into
semantically rich text to learn shape-location-text relationships across
scales. Trained on 20 million image-mask-description triplets, RCMed achieves
state-of-the-art precision in contextualizing irregular lesions and subtle
anatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It
achieved a 23.5% relative improvement in cell segmentation from microscopy
images over prior methods. RCMed's strong vision-language alignment enables
exceptional generalization, with state-of-the-art performance in external
validation across 20 clinically significant cancer types, including novel
tasks. This work demonstrates how integrated multimodal models capture
fine-grained patterns, enabling human-level interpretation in complex scenarios
and advancing human-centric AI healthcare.

</details>

### [178] [Attention-aggregated Attack for Boosting the Transferability of Facial Adversarial Examples](https://arxiv.org/abs/2505.03383)
*Jian-Wei Li,Wen-Ze Shao*

Main category: cs.CV

TLDR: 论文提出了一种名为注意力聚合攻击（AAA）的新方法，通过模仿其他FR模型的注意力机制，增强对抗样本在细粒度视觉任务（如人脸识别）中的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法未充分考虑细粒度视觉任务（如人脸识别）中类特定深度模型的特殊性，导致攻击效果不佳。

Method: 提出注意力聚合攻击（AAA），通过分析FR模型的注意力差异，破坏其他FR模型决策关键的面部特征。

Result: 在多种FR模型上的实验验证了AAA方法的优越性和鲁棒性。

Conclusion: AAA方法显著提升了对抗样本在FR任务中的可迁移性和攻击效果。

Abstract: Adversarial examples have revealed the vulnerability of deep learning models
and raised serious concerns about information security. The transfer-based
attack is a hot topic in black-box attacks that are practical to real-world
scenarios where the training datasets, parameters, and structure of the target
model are unknown to the attacker. However, few methods consider the
particularity of class-specific deep models for fine-grained vision tasks, such
as face recognition (FR), giving rise to unsatisfactory attacking performance.
In this work, we first investigate what in a face exactly contributes to the
embedding learning of FR models and find that both decisive and auxiliary
facial features are specific to each FR model, which is quite different from
the biological mechanism of human visual system. Accordingly we then propose a
novel attack method named Attention-aggregated Attack (AAA) to enhance the
transferability of adversarial examples against FR, which is inspired by the
attention divergence and aims to destroy the facial features that are critical
for the decision-making of other FR models by imitating their attentions on the
clean face images. Extensive experiments conducted on various FR models
validate the superiority and robust effectiveness of the proposed method over
existing methods.

</details>

### [179] [EOPose : Exemplar-based object reposing using Generalized Pose Correspondences](https://arxiv.org/abs/2505.03394)
*Sarthak Mehrotra,Rishabh Jain,Mayur Hemani,Balaji Krishnamurthy,Mausoom Sarkar*

Main category: cs.CV

TLDR: EOPose是一个端到端的框架，通过无监督关键点检测实现物体在图像中的重新姿态调整，适用于电子商务等领域。


<details>
  <summary>Details</summary>
Motivation: 电子商务需要快速生成产品图像的多种变体，现有生成方法无法保留物体的精细细节。

Method: EOPose利用目标姿态引导图像的关键点对应关系，通过三步法对源图像进行变形和重新渲染。

Result: EOPose在PSNR、SSIM和FID等图像质量指标上表现优异，并保留了物体的精细细节。

Conclusion: EOPose是一种高效且高质量的对象重新姿态调整方法，适用于实际应用。

Abstract: Reposing objects in images has a myriad of applications, especially for
e-commerce where several variants of product images need to be produced
quickly. In this work, we leverage the recent advances in unsupervised keypoint
correspondence detection between different object images of the same class to
propose an end-to-end framework for generic object reposing. Our method,
EOPose, takes a target pose-guidance image as input and uses its keypoint
correspondence with the source object image to warp and re-render the latter
into the target pose using a novel three-step approach. Unlike generative
approaches, our method also preserves the fine-grained details of the object
such as its exact colors, textures, and brand marks. We also prepare a new
dataset of paired objects based on the Objaverse dataset to train and test our
network. EOPose produces high-quality reposing output as evidenced by different
image quality metrics (PSNR, SSIM and FID). Besides a description of the method
and the dataset, the paper also includes detailed ablation and user studies to
indicate the efficacy of the proposed method

</details>

### [180] [DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation](https://arxiv.org/abs/2505.03401)
*Shanshan Song,Hui Tang,Honglong Yang,Xiaomeng Li*

Main category: cs.CV

TLDR: 提出了一种动态差异感知时序残差网络（DDaTR），通过捕捉多级空间和时间相关性，显著提升了纵向放射学报告生成（LRRG）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LRRG方法在特征提取过程中未能有效捕捉空间和时间相关性，导致性能不佳。

Method: 设计了动态特征对齐模块（DFAM）和动态差异感知模块（DDAM），结合动态残差网络，捕捉多级空间和时间相关性。

Result: 在三个基准测试中表现优于现有方法，证明了其在RRG和LRRG任务中的有效性。

Conclusion: DDaTR通过改进特征提取和时序建模，显著提升了LRRG的性能。

Abstract: Radiology Report Generation (RRG) automates the creation of radiology reports
from medical imaging, enhancing the efficiency of the reporting process.
Longitudinal Radiology Report Generation (LRRG) extends RRG by incorporating
the ability to compare current and prior exams, facilitating the tracking of
temporal changes in clinical findings. Existing LRRG approaches only extract
features from prior and current images using a visual pre-trained encoder,
which are then concatenated to generate the final report. However, these
methods struggle to effectively capture both spatial and temporal correlations
during the feature extraction process. Consequently, the extracted features
inadequately capture the information of difference across exams and thus
underrepresent the expected progressions, leading to sub-optimal performance in
LRRG. To address this, we develop a novel dynamic difference-aware temporal
residual network (DDaTR). In DDaTR, we introduce two modules at each stage of
the visual encoder to capture multi-level spatial correlations. The Dynamic
Feature Alignment Module (DFAM) is designed to align prior features across
modalities for the integrity of prior clinical information. Prompted by the
enriched prior features, the dynamic difference-aware module (DDAM) captures
favorable difference information by identifying relationships across exams.
Furthermore, our DDaTR employs the dynamic residual network to unidirectionally
transmit longitudinal information, effectively modelling temporal correlations.
Extensive experiments demonstrated superior performance over existing methods
on three benchmarks, proving its efficacy in both RRG and LRRG tasks.

</details>

### [181] [CXR-AD: Component X-ray Image Dataset for Industrial Anomaly Detection](https://arxiv.org/abs/2505.03412)
*Haoyu Bai,Jie Wang,Gaomin Li,Xuan Li,Xiaohu Zhang,Xia Yang*

Main category: cs.CV

TLDR: 论文构建了首个公开的组件X射线异常检测数据集CXR-AD，填补了内部缺陷检测领域的数据空白，并分析了其技术挑战和现有算法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测数据集主要关注表面缺陷，缺乏针对组件内部缺陷的公开X射线数据集，因此需要构建CXR-AD数据集以推动相关研究。

Method: 通过收集真实X射线图像，构建包含5类工业组件的CXR-AD数据集，并提供像素级掩码标注。分析了数据集特性，并评估了三种先进异常检测框架的性能。

Result: 实验表明，现有算法在CXR-AD上的性能平均下降29.78%，揭示了其在内部缺陷检测任务中的局限性。

Conclusion: CXR-AD为内部缺陷检测提供了首个公开基准，有助于算法开发和检测技术精度的提升。

Abstract: Internal defect detection constitutes a critical process in ensuring
component quality, for which anomaly detection serves as an effective solution.
However, existing anomaly detection datasets predominantly focus on surface
defects in visible-light images, lacking publicly available X-ray datasets
targeting internal defects in components. To address this gap, we construct the
first publicly accessible component X-ray anomaly detection (CXR-AD) dataset,
comprising real-world X-ray images. The dataset covers five industrial
component categories, including 653 normal samples and 561 defect samples with
precise pixel-level mask annotations. We systematically analyze the dataset
characteristics and identify three major technical challenges: (1) strong
coupling between complex internal structures and defect regions, (2) inherent
low contrast and high noise interference in X-ray imaging, and (3) significant
variations in defect scales and morphologies. To evaluate dataset complexity,
we benchmark three state-of-the-art anomaly detection frameworks
(feature-based, reconstruction-based, and zero-shot learning methods).
Experimental results demonstrate a 29.78% average performance degradation on
CXR-AD compared to MVTec AD, highlighting the limitations of current algorithms
in handling internal defect detection tasks. To the best of our knowledge,
CXR-AD represents the first publicly available X-ray dataset for component
anomaly detection, providing a real-world industrial benchmark to advance
algorithm development and enhance precision in internal defect inspection
technologies.

</details>

### [182] [LiftFeat: 3D Geometry-Aware Local Feature Matching](https://arxiv.org/abs/2505.03422)
*Yepeng Liu,Wenpeng Lai,Zhou Zhao,Yuxuan Xiong,Jinchi Zhu,Jun Cheng,Yongchao Xu*

Main category: cs.CV

TLDR: 提出了一种轻量级网络LiftFeat，通过聚合3D几何特征提升原始描述符的鲁棒性，适用于光照变化大、纹理少或重复模式的场景。


<details>
  <summary>Details</summary>
Motivation: 在SLAM和视觉定位等应用中，现有方法在极端条件下（如光照变化大、低纹理区域或重复模式）提取鲁棒且具有区分性的视觉特征仍具挑战性。

Method: 采用预训练的单目深度估计模型生成伪表面法线标签，监督3D几何特征的提取，并设计3D几何感知的特征提升模块，融合表面法线特征与原始2D描述符特征。

Result: 在相对位姿估计、单应性估计和视觉定位任务中，LiftFeat表现优于一些轻量级的最先进方法。

Conclusion: LiftFeat通过引入3D几何特征，显著提升了2D特征描述在极端条件下的区分能力。

Abstract: Robust and efficient local feature matching plays a crucial role in
applications such as SLAM and visual localization for robotics. Despite great
progress, it is still very challenging to extract robust and discriminative
visual features in scenarios with drastic lighting changes, low texture areas,
or repetitive patterns. In this paper, we propose a new lightweight network
called \textit{LiftFeat}, which lifts the robustness of raw descriptor by
aggregating 3D geometric feature. Specifically, we first adopt a pre-trained
monocular depth estimation model to generate pseudo surface normal label,
supervising the extraction of 3D geometric feature in terms of predicted
surface normal. We then design a 3D geometry-aware feature lifting module to
fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D
geometric feature enhances the discriminative ability of 2D feature description
in extreme conditions. Extensive experimental results on relative pose
estimation, homography estimation, and visual localization tasks, demonstrate
that our LiftFeat outperforms some lightweight state-of-the-art methods. Code
will be released at : https://github.com/lyp-deeplearning/LiftFeat.

</details>

### [183] [Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI Synthesis: Advancing Pretraining and Clinical Applications](https://arxiv.org/abs/2505.03426)
*Ziyu Li,Yujian Hu,Zhengyao Ding,Yiheng Mao,Haitao Li,Fan Yi,Hongkun Zhang,Zhengxing Huang*

Main category: cs.CV

TLDR: 提出了一种基于心脏表型的CMR生成方法（CPGG），通过生成多样化的CMR数据提升AI模型在下游任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决CMR数据稀缺和质量不足的问题，以支持AI在心脏疾病诊断中的应用。

Method: 采用两阶段框架：1）基于CMR数据训练生成模型；2）使用掩码自回归扩散模型生成高质量CMR序列。

Result: 生成的合成CMR数据显著提升了诊断和表型预测任务的性能。

Conclusion: CPGG方法有效扩展了CMR数据，为AI模型提供了更丰富的预训练资源。

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for
diagnosing heart diseases and evaluating cardiac health. However, the limited
availability of large-scale, high-quality CMR datasets poses a major challenge
to the effective application of artificial intelligence (AI) in this domain.
Even the amount of unlabeled data and the health status it covers are difficult
to meet the needs of model pretraining, which hinders the performance of AI
models on downstream tasks. In this study, we present Cardiac Phenotype-Guided
CMR Generation (CPGG), a novel approach for generating diverse CMR data that
covers a wide spectrum of cardiac health status. The CPGG framework consists of
two stages: in the first stage, a generative model is trained using cardiac
phenotypes derived from CMR data; in the second stage, a masked autoregressive
diffusion model, conditioned on these phenotypes, generates high-fidelity CMR
cine sequences that capture both structural and functional features of the
heart in a fine-grained manner. We synthesized a massive amount of CMR to
expand the pretraining data. Experimental results show that CPGG generates
high-quality synthetic CMR data, significantly improving performance on various
downstream tasks, including diagnosis and cardiac phenotypes prediction. These
gains are demonstrated across both public and private datasets, highlighting
the effectiveness of our approach. Code is availabel at
https://anonymous.4open.science/r/CPGG.

</details>

### [184] [A Fusion-Guided Inception Network for Hyperspectral Image Super-Resolution](https://arxiv.org/abs/2505.03431)
*Usman Muhammad,Jorma Laaksonen*

Main category: cs.CV

TLDR: 提出了一种名为FGIN的单图像超分辨率模型，通过融合光谱和空间信息，结合多尺度特征提取和优化上采样模块，显著提升了超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 解决高光谱图像（HSI）与常规图像融合时对精确对齐的依赖问题，提出一种无需对齐的单图像超分辨率方法。

Method: 采用光谱-空间融合模块早期整合信息，结合Inception-like分层特征提取和多尺度融合块，最后通过优化的上采样模块提升重建质量。

Result: 在两个公开的高光谱数据集上表现出竞争性性能。

Conclusion: FGIN模型在无需对齐的情况下，有效提升了高光谱图像的超分辨率性能。

Abstract: The fusion of low-spatial-resolution hyperspectral images (HSIs) with
high-spatial-resolution conventional images (e.g., panchromatic or RGB) has
played a significant role in recent advancements in HSI super-resolution.
However, this fusion process relies on the availability of precise alignment
between image pairs, which is often challenging in real-world scenarios. To
mitigate this limitation, we propose a single-image super-resolution model
called the Fusion-Guided Inception Network (FGIN). Specifically, we first
employ a spectral-spatial fusion module to effectively integrate spectral and
spatial information at an early stage. Next, an Inception-like hierarchical
feature extraction strategy is used to capture multiscale spatial dependencies,
followed by a dedicated multi-scale fusion block. To further enhance
reconstruction quality, we incorporate an optimized upsampling module that
combines bilinear interpolation with depthwise separable convolutions.
Experimental evaluations on two publicly available hyperspectral datasets
demonstrate the competitive performance of our method.

</details>

### [185] [Robustness in AI-Generated Detection: Enhancing Resistance to Adversarial Attacks](https://arxiv.org/abs/2505.03435)
*Sun Haoxuan,Hong Yan,Zhan Jiahui,Chen Haoxing,Lan Jun,Zhu Huijia,Wang Weiqiang,Zhang Liqing,Zhang Jianfu*

Main category: cs.CV

TLDR: 论文研究了AI生成人脸检测系统的脆弱性，提出了一种结合对抗训练和扩散反演的方法，显著提升了系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 生成图像技术的快速发展带来了安全风险，尤其是人脸生成检测领域，现有检测方法在对抗攻击下表现脆弱。

Method: 提出了一种结合对抗训练和扩散反演与重建的方法，以增强检测系统的鲁棒性。

Result: 实验表明，现有检测系统易受对抗扰动影响，但新方法显著提升了鲁棒性。

Conclusion: 论文为AI生成内容的内在特征提供了深入分析，并公开了相关代码以促进进一步研究。

Abstract: The rapid advancement of generative image technology has introduced
significant security concerns, particularly in the domain of face generation
detection. This paper investigates the vulnerabilities of current AI-generated
face detection systems. Our study reveals that while existing detection methods
often achieve high accuracy under standard conditions, they exhibit limited
robustness against adversarial attacks. To address these challenges, we propose
an approach that integrates adversarial training to mitigate the impact of
adversarial examples. Furthermore, we utilize diffusion inversion and
reconstruction to further enhance detection robustness. Experimental results
demonstrate that minor adversarial perturbations can easily bypass existing
detection systems, but our method significantly improves the robustness of
these systems. Additionally, we provide an in-depth analysis of adversarial and
benign examples, offering insights into the intrinsic characteristics of
AI-generated content. All associated code will be made publicly available in a
dedicated repository to facilitate further research and verification.

</details>

### [186] [Polar Coordinate-Based 2D Pose Prior with Neural Distance Field](https://arxiv.org/abs/2505.03445)
*Qi Gan,Sao Mai Nguyen,Eric Fenaux,Stephan Clémençon,Mounîm El Yacoubi*

Main category: cs.CV

TLDR: 提出了一种基于神经距离场（NDF）的2D姿态先验引导细化方法，通过极坐标表示和新型非测地距离度量，提升了运动模糊和遮挡情况下的姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的人体姿态估计模型在真实运动场景中因运动模糊、遮挡和领域偏移等问题表现不佳，且需要大量标注数据。

Method: 采用极坐标表示结合关节连接长度，定义非测地距离度量，并开发梯度批量投影增强策略以缓解数据稀缺。

Result: 在跳远数据集上验证了方法的有效性，显著提升了2D姿态估计的准确性和鲁棒性。

Conclusion: 该方法在有限训练数据下显著提升了姿态估计的合理性，适用于多样化运动场景。

Abstract: Human pose capture is essential for sports analysis, enabling precise
evaluation of athletes' movements. While deep learning-based human pose
estimation (HPE) models from RGB videos have achieved impressive performance on
public datasets, their effectiveness in real-world sports scenarios is often
hindered by motion blur, occlusions, and domain shifts across different pose
representations. Fine-tuning these models can partially alleviate such
challenges but typically requires large-scale annotated data and still
struggles to generalize across diverse sports environments. To address these
limitations, we propose a 2D pose prior-guided refinement approach based on
Neural Distance Fields (NDF). Unlike existing approaches that rely solely on
angular representations of human poses, we introduce a polar coordinate-based
representation that explicitly incorporates joint connection lengths, enabling
a more accurate correction of erroneous pose estimations. Additionally, we
define a novel non-geodesic distance metric that separates angular and radial
discrepancies, which we demonstrate is better suited for polar representations
than traditional geodesic distances. To mitigate data scarcity, we develop a
gradient-based batch-projection augmentation strategy, which synthesizes
realistic pose samples through iterative refinement. Our method is evaluated on
a long jump dataset, demonstrating its ability to improve 2D pose estimation
across multiple pose representations, making it robust across different
domains. Experimental results show that our approach enhances pose plausibility
while requiring only limited training data. Code is available at:
https://github.com/QGAN2019/polar-NDF.

</details>

### [187] [Nonperiodic dynamic CT reconstruction using backward-warping INR with regularization of diffeomorphism (BIRD)](https://arxiv.org/abs/2505.03463)
*Muge Du,Zhuozhao Zheng,Wenying Wang,Guotao Quan,Wuliang Shi,Le Shen,Li Zhang,Liang Li,Yinong Liu,Yuxiang Xing*

Main category: cs.CV

TLDR: BIRD是一种基于隐式神经表示（INR）的动态CT重建框架，通过反向变形、微分同胚正则化、运动补偿重建和降维设计，解决了非周期性运动中的计算效率、解剖合理性和细节保留问题。


<details>
  <summary>Details</summary>
Motivation: 解决非周期性快速运动（如高心率心脏成像）中动态CT重建的挑战，包括传统方法的有限角度问题和深度学习的泛化难题。

Method: 提出BIRD框架，包含反向变形、微分同胚正则化、运动补偿重建和降维设计四项关键技术。

Result: 在模拟和实际研究中，BIRD显著减少了运动伪影并提升了细节重建效果。

Conclusion: BIRD为动态CT重建提供了更准确的解决方案，具有临床应用的潜力。

Abstract: Dynamic computed tomography (CT) reconstruction faces significant challenges
in addressing motion artifacts, particularly for nonperiodic rapid movements
such as cardiac imaging with fast heart rates. Traditional methods struggle
with the extreme limited-angle problems inherent in nonperiodic cases. Deep
learning methods have improved performance but face generalization challenges.
Recent implicit neural representation (INR) techniques show promise through
self-supervised deep learning, but have critical limitations: computational
inefficiency due to forward-warping modeling, difficulty balancing DVF
complexity with anatomical plausibility, and challenges in preserving fine
details without additional patient-specific pre-scans. This paper presents a
novel INR-based framework, BIRD, for nonperiodic dynamic CT reconstruction. It
addresses these challenges through four key contributions: (1) backward-warping
deformation that enables direct computation of each dynamic voxel with
significantly reduced computational cost, (2) diffeomorphism-based DVF
regularization that ensures anatomically plausible deformations while
maintaining representational capacity, (3) motion-compensated analytical
reconstruction that enhances fine details without requiring additional
pre-scans, and (4) dimensional-reduction design for efficient 4D coordinate
encoding. Through various simulations and practical studies, including digital
and physical phantoms and retrospective patient data, we demonstrate the
effectiveness of our approach for nonperiodic dynamic CT reconstruction with
enhanced details and reduced motion artifacts. The proposed framework enables
more accurate dynamic CT reconstruction with potential clinical applications,
such as one-beat cardiac reconstruction, cinematic image sequences for
functional imaging, and motion artifact reduction in conventional CT scans.

</details>

### [188] [Blending 3D Geometry and Machine Learning for Multi-View Stereopsis](https://arxiv.org/abs/2505.03470)
*Vibhas Vats,Md. Alimoor Reza,David Crandall,Soon-heung Jung*

Main category: cs.CV

TLDR: GC MVSNet++通过在学习阶段主动实施多视图、多尺度的几何一致性检查，显著加速训练过程，并在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统MVS方法依赖光度一致性，而现代学习方法仅在后期处理中应用几何一致性检查，缺乏对学习过程的影响。

Method: 提出GC MVSNet++，在学习阶段直接惩罚几何不一致像素，并设计密集连接的成本正则化网络。

Result: 在DTU和BlendedMVS数据集上达到最先进性能，在Tanks and Temples基准中排名第二。

Conclusion: GC MVSNet++是首个在学习阶段实施多视图、多尺度几何一致性的方法，显著提升性能。

Abstract: Traditional multi-view stereo (MVS) methods primarily depend on photometric
and geometric consistency constraints. In contrast, modern learning-based
algorithms often rely on the plane sweep algorithm to infer 3D geometry,
applying explicit geometric consistency (GC) checks only as a post-processing
step, with no impact on the learning process itself. In this work, we introduce
GC MVSNet plus plus, a novel approach that actively enforces geometric
consistency of reference view depth maps across multiple source views (multi
view) and at various scales (multi scale) during the learning phase (see Fig.
1). This integrated GC check significantly accelerates the learning process by
directly penalizing geometrically inconsistent pixels, effectively halving the
number of training iterations compared to other MVS methods. Furthermore, we
introduce a densely connected cost regularization network with two distinct
block designs simple and feature dense optimized to harness dense feature
connections for enhanced regularization. Extensive experiments demonstrate that
our approach achieves a new state of the art on the DTU and BlendedMVS datasets
and secures second place on the Tanks and Temples benchmark. To our knowledge,
GC MVSNet plus plus is the first method to enforce multi-view, multi-scale
supervised geometric consistency during learning. Our code is available.

</details>

### [189] [UPMAD-Net: A Brain Tumor Segmentation Network with Uncertainty Guidance and Adaptive Multimodal Feature Fusion](https://arxiv.org/abs/2505.03494)
*Zhanyuan Jia,Ni Yao,Danyang Sun,Chuang Han,Yanting Li,Jiaofen Nan,Fubao Zhu,Chen Zhao,Weihua Zhou*

Main category: cs.CV

TLDR: 提出了一种结合深度学习和区域生长算法的脑肿瘤分割方法，通过多尺度特征融合和自适应注意力机制提升性能，并在BraTS数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤分割对诊断和治疗至关重要，但因其形状不规则、边界模糊和变异性高而具有挑战性。

Method: 采用多尺度特征融合模块和自适应注意力机制提取特征，并结合蒙特卡洛Dropout进行不确定性估计。

Result: 在BraTS2021和BraTS2019数据集上，分割性能显著优于现有方法，Dice分数分别为89.18%（ET）、93.67%（WT）、91.23%（TC）和87.43%（ET）、90.92%（WT）、90.40%（TC）。

Conclusion: 提出了一种基于U-Net架构的新型3D脑肿瘤分割网络，通过引入先验知识和不确定性估计方法提升了鲁棒性和性能。

Abstract: Background: Brain tumor segmentation has a significant impact on the
diagnosis and treatment of brain tumors. Accurate brain tumor segmentation
remains challenging due to their irregular shapes, vague boundaries, and high
variability. Objective: We propose a brain tumor segmentation method that
combines deep learning with prior knowledge derived from a region-growing
algorithm. Methods: The proposed method utilizes a multi-scale feature fusion
(MSFF) module and adaptive attention mechanisms (AAM) to extract multi-scale
features and capture global contextual information. To enhance the model's
robustness in low-confidence regions, the Monte Carlo Dropout (MC Dropout)
strategy is employed for uncertainty estimation. Results: Extensive experiments
demonstrate that the proposed method achieves superior performance on Brain
Tumor Segmentation (BraTS) datasets, significantly outperforming various
state-of-the-art methods. On the BraTS2021 dataset, the test Dice scores are
89.18% for Enhancing Tumor (ET) segmentation, 93.67% for Whole Tumor (WT)
segmentation, and 91.23% for Tumor Core (TC) segmentation. On the BraTS2019
validation set, the validation Dice scores are 87.43%, 90.92%, and 90.40% for
ET, WT, and TC segmentation, respectively. Ablation studies further confirmed
the contribution of each module to segmentation accuracy, indicating that each
component played a vital role in overall performance improvement. Conclusion:
This study proposed a novel 3D brain tumor segmentation network based on the
U-Net architecture. By incorporating the prior knowledge and employing the
uncertainty estimation method, the robustness and performance were improved.
The code for the proposed method is available at
https://github.com/chenzhao2023/UPMAD_Net_BrainSeg.

</details>

### [190] [MRI motion correction via efficient residual-guided denoising diffusion probabilistic models](https://arxiv.org/abs/2505.03498)
*Mojtaba Safari,Shansong Wang,Qiang Li,Zach Eidex,Richard L. J. Qiu,Chih-Wei Chang,Hui Mao,Xiaofeng Yang*

Main category: cs.CV

TLDR: Res-MoCoDiff是一种高效的MRI运动伪影校正模型，通过残差误差移位机制和四步反向扩散，显著提升图像质量。


<details>
  <summary>Details</summary>
Motivation: MRI中的运动伪影严重降低图像质量，传统方法成本高且流程复杂，需一种高效解决方案。

Method: 采用残差误差移位机制和U-net结合Swin-Transformer的架构，训练使用l1+l2损失函数。

Result: 在所有运动严重程度下表现最佳，PSNR达41.91 dB，采样时间大幅缩短至0.37秒。

Conclusion: Res-MoCoDiff在运动伪影校正中高效且优于现有方法，适合临床应用。

Abstract: Purpose: Motion artifacts in magnetic resonance imaging (MRI) significantly
degrade image quality and impair quantitative analysis. Conventional mitigation
strategies, such as repeated acquisitions or motion tracking, are costly and
workflow-intensive. This study introduces Res-MoCoDiff, an efficient denoising
diffusion probabilistic model tailored for MRI motion artifact correction.
Methods: Res-MoCoDiff incorporates a novel residual error shifting mechanism in
the forward diffusion process, aligning the noise distribution with
motion-corrupted data and enabling an efficient four-step reverse diffusion. A
U-net backbone enhanced with Swin-Transformer blocks conventional attention
layers, improving adaptability across resolutions. Training employs a combined
l1+l2 loss, which promotes image sharpness and reduces pixel-level errors.
Res-MoCoDiff was evaluated on synthetic dataset generated using a realistic
motion simulation framework and on an in-vivo dataset. Comparative analyses
were conducted against established methods, including CycleGAN, Pix2pix, and
MT-DDPM using quantitative metrics such as peak signal-to-noise ratio (PSNR),
structural similarity index measure (SSIM), and normalized mean squared error
(NMSE). Results: The proposed method demonstrated superior performance in
removing motion artifacts across all motion severity levels. Res-MoCoDiff
consistently achieved the highest SSIM and the lowest NMSE values, with a PSNR
of up to 41.91+-2.94 dB for minor distortions. Notably, the average sampling
time was reduced to 0.37 seconds per batch of two image slices, compared with
101.74 seconds for conventional approaches.

</details>

### [191] [Modality-Guided Dynamic Graph Fusion and Temporal Diffusion for Self-Supervised RGB-T Tracking](https://arxiv.org/abs/2505.03507)
*Shenglan Li,Rui Yao,Yong Zhou,Hancheng Zhu,Kunyang Sun,Bing Liu,Zhiwen Shao,Jiaqi Zhao*

Main category: cs.CV

TLDR: GDSTrack提出了一种动态图融合和时序扩散的方法，解决了自监督RGB-T跟踪中伪标签噪声和背景干扰的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 减少对大规模标注的依赖，同时解决伪标签错误导致的物体区域遗漏和背景噪声问题。

Method: 通过动态图融合模块（MDGF）和时序图扩散（TGID）技术，动态融合邻近帧模态并利用生成模型的去噪能力。

Result: 在四个公开RGB-T跟踪数据集上表现优于现有方法。

Conclusion: GDSTrack有效提升了自监督RGB-T跟踪的鲁棒性和性能。

Abstract: To reduce the reliance on large-scale annotations, self-supervised RGB-T
tracking approaches have garnered significant attention. However, the omission
of the object region by erroneous pseudo-label or the introduction of
background noise affects the efficiency of modality fusion, while pseudo-label
noise triggered by similar object noise can further affect the tracking
performance. In this paper, we propose GDSTrack, a novel approach that
introduces dynamic graph fusion and temporal diffusion to address the above
challenges in self-supervised RGB-T tracking. GDSTrack dynamically fuses the
modalities of neighboring frames, treats them as distractor noise, and
leverages the denoising capability of a generative model. Specifically, by
constructing an adjacency matrix via an Adjacency Matrix Generator (AMG), the
proposed Modality-guided Dynamic Graph Fusion (MDGF) module uses a dynamic
adjacency matrix to guide graph attention, focusing on and fusing the object's
coherent regions. Temporal Graph-Informed Diffusion (TGID) models MDGF features
from neighboring frames as interference, and thus improving robustness against
similar-object noise. Extensive experiments conducted on four public RGB-T
tracking datasets demonstrate that GDSTrack outperforms the existing
state-of-the-art methods. The source code is available at
https://github.com/LiShenglana/GDSTrack.

</details>

### [192] [Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks](https://arxiv.org/abs/2505.03522)
*Haotong Cheng,Zhiqi Zhang,Hao Li,Xinshang Zhang*

Main category: cs.CV

TLDR: 论文提出了一种量化模块可移植性的方法（UAE），并设计了两个优化模块（CRB和DCRB），在多种数据集上表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注性能提升，而忽略了模块的可移植性量化。本文旨在填补这一空白。

Method: 引入“通用性”概念和UAE度量，设计CRB和DCRB模块。

Result: 在多种数据集上表现优异，PSNR提升达0.83dB或参数减少71.3%。

Conclusion: 提出的模块显著提升了模型性能和可移植性。

Abstract: Deep learning has substantially advanced the Single Image Super-Resolution
(SISR). However, existing researches have predominantly focused on raw
performance gains, with little attention paid to quantifying the
transferability of architectural components. In this paper, we introduce the
concept of "Universality" and its associated definitions which extend the
traditional notion of "Generalization" to encompass the modules' ease of
transferability, thus revealing the relationships between module universality
and model generalizability. Then we propose the Universality Assessment
Equation (UAE), a metric for quantifying how readily a given module could be
transplanted across models. Guided by the UAE results of standard residual
blocks and other plug-and-play modules, we further design two optimized
modules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).
Through comprehensive experiments on natural-scene benchmarks, remote-sensing
datasets, extreme-industrial imagery and on-device deployments, we demonstrate
that networks embedded with the proposed plug-and-play modules outperform
several state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or
enabling a 71.3% reduction in parameters with negligible loss in reconstruction
fidelity.

</details>

### [193] [Coop-WD: Cooperative Perception with Weighting and Denoising for Robust V2V Communication](https://arxiv.org/abs/2505.03528)
*Chenguang Liu,Jianjun Chen,Yunfei Chen,Yubei He,Zhuangkun Wei,Hongjian Sun,Haiyan Lu,Qi Hao*

Main category: cs.CV

TLDR: 提出了一种联合加权和去噪框架Coop-WD，用于增强V2V通信受损下的协同感知性能，并提出了高效变体Coop-WD-eco以降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对不同V2V通信损伤水平的泛化能力，需要提升协同感知的鲁棒性。

Method: 采用自监督对比模型和条件扩散概率模型进行车辆级和像素级特征增强，并提出选择性去噪的高效变体。

Result: Coop-WD在所有信道类型中优于传统基准，Coop-WD-eco在严重失真下可降低50%计算成本。

Conclusion: Coop-WD和Coop-WD-eco显著提升了协同感知性能，同时兼顾了计算效率。

Abstract: Cooperative perception, leveraging shared information from multiple vehicles
via vehicle-to-vehicle (V2V) communication, plays a vital role in autonomous
driving to alleviate the limitation of single-vehicle perception. Existing
works have explored the effects of V2V communication impairments on perception
precision, but they lack generalization to different levels of impairments. In
this work, we propose a joint weighting and denoising framework, Coop-WD, to
enhance cooperative perception subject to V2V channel impairments. In this
framework, the self-supervised contrastive model and the conditional diffusion
probabilistic model are adopted hierarchically for vehicle-level and
pixel-level feature enhancement. An efficient variant model, Coop-WD-eco, is
proposed to selectively deactivate denoising to reduce processing overhead.
Rician fading, non-stationarity, and time-varying distortion are considered.
Simulation results demonstrate that the proposed Coop-WD outperforms
conventional benchmarks in all types of channels. Qualitative analysis with
visual examples further proves the superiority of our proposed method. The
proposed Coop-WD-eco achieves up to 50% reduction in computational cost under
severe distortion while maintaining comparable accuracy as channel conditions
improve.

</details>

### [194] [RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth Segmentation in CBCT](https://arxiv.org/abs/2505.03538)
*Chuyu Zhao,Hao Huang,Jiashuo Guo,Ziyu Shen,Zhongwei Zhou,Jie Liu,Zekuan Yu*

Main category: cs.CV

TLDR: RAIL提出了一种双组双学生的半监督框架，通过区域感知指导学习解决CBCT牙齿分割中标注数据少的问题，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决半监督学习在CBCT牙齿分割中因标注数据少和伪标签不可靠导致的性能下降问题。

Method: 采用双组双学生框架，引入DFS控制器和CAL调制器，分别优化监督学习和无监督学习阶段。

Result: 在四个CBCT牙齿分割数据集上，RAIL表现优于现有方法。

Conclusion: RAIL通过区域感知指导和双组协作，显著提升了半监督学习的性能。

Abstract: Semi-supervised learning has become a compelling approach for 3D tooth
segmentation from CBCT scans, where labeled data is minimal. However, existing
methods still face two persistent challenges: limited corrective supervision in
structurally ambiguous or mislabeled regions during supervised training and
performance degradation caused by unreliable pseudo-labels on unlabeled data.
To address these problems, we propose Region-Aware Instructive Learning (RAIL),
a dual-group dual-student, semi-supervised framework. Each group contains two
student models guided by a shared teacher network. By alternating training
between the two groups, RAIL promotes intergroup knowledge transfer and
collaborative region-aware instruction while reducing overfitting to the
characteristics of any single model. Specifically, RAIL introduces two
instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller
improves supervised learning by instructing predictions only within areas where
student outputs diverge from both ground truth and the best student, thereby
concentrating supervision on structurally ambiguous or mislabeled areas. In the
unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces
agreement in regions with high model certainty while reducing the effect of
low-confidence predictions during training. This helps prevent our model from
learning unstable patterns and improves the overall reliability of
pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets
show that RAIL surpasses state-of-the-art methods under limited annotation. Our
code will be available at https://github.com/Tournesol-Saturday/RAIL.

</details>

### [195] [Panoramic Out-of-Distribution Segmentation](https://arxiv.org/abs/2505.03539)
*Mengfei Duan,Kailun Yang,Yuheng Zhang,Yihong Cao,Fei Teng,Kai Luo,Jiaming Zhang,Zhiyong Li,Shutao Li*

Main category: cs.CV

TLDR: 论文提出了一种新的全景图像任务PanOoS，并提出了首个解决方案POS，通过文本引导的提示分布学习适应全景图像特性，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前全景语义分割方法无法识别异常值，而传统OoS模型在全景域表现不佳，因此需要一种新方法来解决这些问题。

Method: POS采用文本引导的提示分布学习，包括解耦策略、提示修复注意力（PRA）和双层提示分布学习（BPDL）。

Result: POS在DenseOoS数据集上AuPRC提升34.25%，FPR95降低21.42%，性能优于现有方法。

Conclusion: POS不仅解决了全景OoS问题，还展示了领先的封闭集分割能力，代码和数据集将开源。

Abstract: Panoramic imaging enables capturing 360{\deg} images with an ultra-wide
Field-of-View (FoV) for dense omnidirectional perception. However, current
panoramic semantic segmentation methods fail to identify outliers, and pinhole
Out-of-distribution Segmentation (OoS) models perform unsatisfactorily in the
panoramic domain due to background clutter and pixel distortions. To address
these issues, we introduce a new task, Panoramic Out-of-distribution
Segmentation (PanOoS), achieving OoS for panoramas. Furthermore, we propose the
first solution, POS, which adapts to the characteristics of panoramic images
through text-guided prompt distribution learning. Specifically, POS integrates
a disentanglement strategy designed to materialize the cross-domain
generalization capability of CLIP. The proposed Prompt-based Restoration
Attention (PRA) optimizes semantic decoding by prompt guidance and
self-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL)
refines the manifold of per-pixel mask embeddings via semantic prototype
supervision. Besides, to compensate for the scarcity of PanOoS datasets, we
establish two benchmarks: DenseOoS, which features diverse outliers in complex
environments, and QuadOoS, captured by a quadruped robot with a panoramic
annular lens system. Extensive experiments demonstrate superior performance of
POS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS,
outperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves
leading closed-set segmentation capabilities. Code and datasets will be
available at https://github.com/MengfeiD/PanOoS.

</details>

### [196] [Read My Ears! Horse Ear Movement Detection for Equine Affective State Assessment](https://arxiv.org/abs/2505.03554)
*João Alves,Pia Haubro Andersen,Rikke Gade*

Main category: cs.CV

TLDR: 论文提出了一种自动检测马耳动作单元（AU）的方法，结合深度学习与光流技术，在公开数据集上达到87.5%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 手动标注马面部动作单元耗时且昂贵，限制了马情感状态评估的发展，因此需要自动化标注系统。

Method: 结合深度学习视频特征提取与循环神经网络进行分类，同时采用经典光流方法。

Result: 在公开马视频数据集上，耳部动作分类准确率达到87.5%。

Conclusion: 该方法展示了自动化AU检测的潜力，未来可应用于马匹福利和兽医诊断。

Abstract: The Equine Facial Action Coding System (EquiFACS) enables the systematic
annotation of facial movements through distinct Action Units (AUs). It serves
as a crucial tool for assessing affective states in horses by identifying
subtle facial expressions associated with discomfort. However, the field of
horse affective state assessment is constrained by the scarcity of annotated
data, as manually labelling facial AUs is both time-consuming and costly. To
address this challenge, automated annotation systems are essential for
leveraging existing datasets and improving affective states detection tools. In
this work, we study different methods for specific ear AU detection and
localization from horse videos. We leverage past works on deep learning-based
video feature extraction combined with recurrent neural networks for the video
classification task, as well as a classic optical flow based approach. We
achieve 87.5% classification accuracy of ear movement presence on a public
horse video dataset, demonstrating the potential of our approach. We discuss
future directions to develop these systems, with the aim of bridging the gap
between automated AU detection and practical applications in equine welfare and
veterinary diagnostics. Our code will be made publicly available at
https://github.com/jmalves5/read-my-ears.

</details>

### [197] [Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID](https://arxiv.org/abs/2505.03557)
*Koray Ulusan,Benjamin Kiefer*

Main category: cs.CV

TLDR: 研究探讨了如何通过增强技术提高Stable Diffusion生成肖像的面部相似性，比较了DreamBooth和InstantID两种方法，并提出了FaceDistance评估工具。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过增强技术提升业余照片生成专业肖像的面部相似性，以支持下游应用。

Method: 使用DreamBooth和InstantID两种技术，结合多种增强策略，通过FaceNet的FaceDistance评估生成肖像的面部相似性。

Result: 实验表明，增强策略能显著提高生成肖像的面部相似性。

Conclusion: 研究为Stable Diffusion肖像生成中增强技术的应用提供了实用见解，有助于优化下游部署。

Abstract: The personalization of Stable Diffusion for generating professional portraits
from amateur photographs is a burgeoning area, with applications in various
downstream contexts. This paper investigates the impact of augmentations on
improving facial resemblance when using two prominent personalization
techniques: DreamBooth and InstantID. Through a series of experiments with
diverse subject datasets, we assessed the effectiveness of various augmentation
strategies on the generated headshots' fidelity to the original subject. We
introduce FaceDistance, a wrapper around FaceNet, to rank the generations based
on facial similarity, which aided in our assessment. Ultimately, this research
provides insights into the role of augmentations in enhancing facial
resemblance in SDXL-generated portraits, informing strategies for their
effective deployment in downstream applications.

</details>

### [198] [Real-Time Person Image Synthesis Using a Flow Matching Model](https://arxiv.org/abs/2505.03562)
*Jiwoo Jeong,Kirok Kim,Wooju Kim,Nam-Joon Kim*

Main category: cs.CV

TLDR: PGPIS任务通过目标姿态和源图像生成逼真人物图像，但实时性成挑战。本文提出基于流匹配（FM）的生成模型RPFM，在速度和图像质量间取得平衡，实现近实时生成。


<details>
  <summary>Details</summary>
Motivation: PGPIS在实时应用（如手语视频生成、AR/VR）中需快速反馈，但现有扩散模型速度慢。

Method: 提出基于流匹配（FM）的生成模型RPFM，支持条件生成和潜在空间操作。

Result: 在DeepFashion数据集上，RPFM实现近实时速度，性能接近SOTA模型。

Conclusion: RPFM在速度和图像质量间取得平衡，适用于实时PGPIS任务。

Abstract: Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images
conditioned on a target pose and a source image. This task plays a key role in
various real-world applications, such as sign language video generation, AR/VR,
gaming, and live streaming. In these scenarios, real-time PGPIS is critical for
providing immediate visual feedback and maintaining user immersion.However,
achieving real-time performance remains a significant challenge due to the
complexity of synthesizing high-fidelity images from diverse and dynamic human
poses. Recent diffusion-based methods have shown impressive image quality in
PGPIS, but their slow sampling speeds hinder deployment in time-sensitive
applications. This latency is particularly problematic in tasks like generating
sign language videos during live broadcasts, where rapid image updates are
required. Therefore, developing a fast and reliable PGPIS model is a crucial
step toward enabling real-time interactive systems. To address this challenge,
we propose a generative model based on flow matching (FM). Our approach enables
faster, more stable, and more efficient training and sampling. Furthermore, the
proposed model supports conditional generation and can operate in latent space,
making it especially suitable for real-time PGPIS applications where both speed
and quality are critical. We evaluate our proposed method, Real-Time Person
Image Synthesis Using a Flow Matching Model (RPFM), on the widely used
DeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves
near-real-time sampling speeds while maintaining performance comparable to the
state-of-the-art models. Our methodology trades off a slight, acceptable
decrease in generated-image accuracy for over a twofold increase in generation
speed, thereby ensuring real-time performance.

</details>

### [199] [Uncertainty-Aware Prototype Semantic Decoupling for Text-Based Person Search in Full Images](https://arxiv.org/abs/2505.03567)
*Zengli Luo,Canlong Zhang,Xiaochun Lu,Zhixin Li,Zhiwen Wang*

Main category: cs.CV

TLDR: UPD-TBPS框架通过多粒度不确定性估计、原型不确定性解耦和跨模态重识别模块，提升复杂场景下基于文本的行人搜索性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂多行人场景中因检测和匹配的不确定性导致性能下降，需改进。

Method: 提出UPD-TBPS框架，包含MUE（多粒度不确定性估计）、PUD（原型不确定性解耦）和ReID（跨模态重识别）模块。

Result: 在CUHK-SYSU-TBPS和PRW-TBPS数据集上验证了框架的有效性。

Conclusion: UPD-TBPS通过减少不确定性提升了行人搜索的准确性和鲁棒性。

Abstract: Text-based pedestrian search (TBPS) in full images aims to locate a target
pedestrian in untrimmed images using natural language descriptions. However, in
complex scenes with multiple pedestrians, existing methods are limited by
uncertainties in detection and matching, leading to degraded performance. To
address this, we propose UPD-TBPS, a novel framework comprising three modules:
Multi-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty
Decoupling (PUD), and Cross-modal Re-identification (ReID). MUE conducts
multi-granularity queries to identify potential targets and assigns confidence
scores to reduce early-stage uncertainty. PUD leverages visual context
decoupling and prototype mining to extract features of the target pedestrian
described in the query. It separates and learns pedestrian prototype
representations at both the coarse-grained cluster level and the fine-grained
individual level, thereby reducing matching uncertainty. ReID evaluates
candidates with varying confidence levels, improving detection and retrieval
accuracy. Experiments on CUHK-SYSU-TBPS and PRW-TBPS datasets validate the
effectiveness of our framework.

</details>

### [200] [Corner Cases: How Size and Position of Objects Challenge ImageNet-Trained Models](https://arxiv.org/abs/2505.03569)
*Mishal Fatima,Steffen Jung,Margret Keuper*

Main category: cs.CV

TLDR: 论文研究了图像背景对模型预测中虚假相关性的影响，提出了一个合成数据集Hard-Spurious-ImageNet，并发现模型在小ROI和偏离中心的物体上依赖背景虚假特征。现有方法未能有效解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 图像背景和物体位置、大小的偏差可能导致模型依赖虚假特征，影响预测准确性。

Method: 提出合成数据集Hard-Spurious-ImageNet，评估预训练模型在不同背景、物体位置和大小下的表现。

Result: 模型在小ROI和偏离中心的物体上依赖背景虚假特征，现有方法未能显著改善最差组准确率。

Conclusion: 需开发新方法以解决物体大小和位置变化对模型依赖虚假特征的影响。

Abstract: Backgrounds in images play a major role in contributing to spurious
correlations among different data points. Owing to aesthetic preferences of
humans capturing the images, datasets can exhibit positional (location of the
object within a given frame) and size (region-of-interest to image ratio)
biases for different classes. In this paper, we show that these biases can
impact how much a model relies on spurious features in the background to make
its predictions. To better illustrate our findings, we propose a synthetic
dataset derived from ImageNet1k, Hard-Spurious-ImageNet, which contains images
with various backgrounds, object positions, and object sizes. By evaluating the
dataset on different pretrained models, we find that most models rely heavily
on spurious features in the background when the region-of-interest (ROI) to
image ratio is small and the object is far from the center of the image.
Moreover, we also show that current methods that aim to mitigate harmful
spurious features, do not take into account these factors, hence fail to
achieve considerable performance gains for worst-group accuracies when the size
and location of core features in an image change.

</details>

### [201] [Supervised and Unsupervised Textile Classification via Near-Infrared Hyperspectral Imaging and Deep Learning](https://arxiv.org/abs/2505.03575)
*Maria Kainz,Johannes K. Krondorfer,Malte Jaschik,Maria Jernej,Harald Ganster*

Main category: cs.CV

TLDR: 论文研究了利用高光谱近红外成像和深度学习算法进行纺织品纤维分类和分选，展示了卷积神经网络和自编码器在不同纺织品结构中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 纺织纤维回收对减少纺织业环境影响至关重要，需要高效分类和分选方法。

Method: 研究采用监督和无监督深度学习模型，测试其在不同纺织品结构上的泛化能力。

Result: 优化的卷积神经网络和自编码器网络在不同条件下表现出稳健的泛化能力。

Conclusion: 高光谱成像和深度学习有望通过准确稳健的分类推动可持续纺织品回收。

Abstract: Recycling textile fibers is critical to reducing the environmental impact of
the textile industry. Hyperspectral near-infrared (NIR) imaging combined with
advanced deep learning algorithms offers a promising solution for efficient
fiber classification and sorting. In this study, we investigate supervised and
unsupervised deep learning models and test their generalization capabilities on
different textile structures. We show that optimized convolutional neural
networks (CNNs) and autoencoder networks achieve robust generalization under
varying conditions. These results highlight the potential of hyperspectral
imaging and deep learning to advance sustainable textile recycling through
accurate and robust classification.

</details>

### [202] [DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer Questions in Dynamic Scenes](https://arxiv.org/abs/2505.03581)
*Sergey Linok,Vadim Semenov,Anastasia Trunova,Oleg Bulichev,Dmitry Yudin*

Main category: cs.CV

TLDR: DyGEnc是一种动态图编码方法，结合时空结构表示与大型语言模型，显著优于现有视觉方法，在人类与物体交互历史查询中提升15-25%。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中事件分析的挑战，现有视觉方法缺乏可解释的时空对象表示。

Method: 提出DyGEnc，整合压缩时空结构表示与大型语言模型，基于文本场景图进行高级问答。

Result: 在STAR和AGQA数据集上表现优异，提升15-25%，并可扩展处理原始图像。

Conclusion: DyGEnc为基于图的机器人记忆和长期推理提供了稳健解决方案，代码已开源。

Abstract: The analysis of events in dynamic environments poses a fundamental challenge
in the development of intelligent agents and robots capable of interacting with
humans. Current approaches predominantly utilize visual models. However, these
methods often capture information implicitly from images, lacking interpretable
spatial-temporal object representations. To address this issue we introduce
DyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates
compressed spatial-temporal structural observation representation with the
cognitive capabilities of large language models. The purpose of this
integration is to enable advanced question answering based on a sequence of
textual scene graphs. Extended evaluations on the STAR and AGQA datasets
indicate that DyGEnc outperforms existing visual methods by a large margin of
15-25% in addressing queries regarding the history of human-to-object
interactions. Furthermore, the proposed method can be seamlessly extended to
process raw input images utilizing foundational models for extracting explicit
textual scene graphs, as substantiated by the results of a robotic experiment
conducted with a wheeled manipulator platform. We hope that these findings will
contribute to the implementation of robust and compressed graph-based robotic
memory for long-horizon reasoning. Code is available at
github.com/linukc/DyGEnc.

</details>

### [203] [Fixed-Length Dense Fingerprint Representation](https://arxiv.org/abs/2505.03597)
*Zhiyu Pan,Xiongjun Guan,Yongjie Duan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TLDR: 提出了一种固定长度的指纹密集描述符和匹配框架FLARE，通过三维密集描述符和姿态对齐增强策略，显著提升了跨模态和低质量指纹的匹配性能。


<details>
  <summary>Details</summary>
Motivation: 固定长度指纹表示在大规模匹配中高效，但现有方法难以处理多样指纹模态、姿态变化和噪声干扰。

Method: 提出三维密集描述符捕捉指纹脊结构空间关系，结合姿态对齐和双重增强策略。

Result: FLARE在多种指纹模态和低质量场景中表现优异，显著优于现有方法。

Conclusion: FLARE是一种统一且可扩展的解决方案，有效提升了指纹表示和匹配的鲁棒性。

Abstract: Fixed-length fingerprint representations, which map each fingerprint to a
compact and fixed-size feature vector, are computationally efficient and
well-suited for large-scale matching. However, designing a robust
representation that effectively handles diverse fingerprint modalities, pose
variations, and noise interference remains a significant challenge. In this
work, we propose a fixed-length dense descriptor of fingerprints, and introduce
FLARE-a fingerprint matching framework that integrates the Fixed-Length dense
descriptor with pose-based Alignment and Robust Enhancement. This fixed-length
representation employs a three-dimensional dense descriptor to effectively
capture spatial relationships among fingerprint ridge structures, enabling
robust and locally discriminative representations. To ensure consistency within
this dense feature space, FLARE incorporates pose-based alignment using
complementary estimation methods, along with dual enhancement strategies that
refine ridge clarity while preserving the original fingerprint modality. The
proposed dense descriptor supports fixed-length representation while
maintaining spatial correspondence, enabling fast and accurate similarity
computation. Extensive experiments demonstrate that FLARE achieves superior
performance across rolled, plain, latent, and contactless fingerprints,
significantly outperforming existing methods in cross-modality and low-quality
scenarios. Further analysis validates the effectiveness of the dense descriptor
design, as well as the impact of alignment and enhancement modules on the
accuracy of dense descriptor matching. Experimental results highlight the
effectiveness and generalizability of FLARE as a unified and scalable solution
for robust fingerprint representation and matching. The implementation and code
will be publicly available at https://github.com/Yu-Yy/FLARE.

</details>

### [204] [From Pixels to Polygons: A Survey of Deep Learning Approaches for Medical Image-to-Mesh Reconstruction](https://arxiv.org/abs/2505.03599)
*Fengming Lin,Arezoo Zakeri,Yidan Xue,Michael MacRaild,Haoran Dou,Zherui Zhou,Ziwei Zou,Ali Sarrami-Foroushani,Jinming Duan,Alejandro F. Frangi*

Main category: cs.CV

TLDR: 本文综述了基于深度学习的医学图像到网格重建方法，将其分为四类（模板模型、统计模型、生成模型和隐式模型），并分析了其方法、优缺点及适用性。同时，评估了不同解剖应用中的表现，总结了公开数据集和评估指标，指出了当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 推动医学图像分析领域的发展，为计算医学和虚拟试验提供三维网格模型，以促进对疾病机制的理解及诊疗技术的进步。

Method: 系统分类现有方法为四类（模板模型、统计模型、生成模型和隐式模型），详细分析其方法基础、优缺点及适用性，并通过标准指标进行定量比较。

Result: 总结了各类方法在不同解剖应用中的表现，整理了公开数据集和评估指标，并指出了拓扑正确性、几何精度和多模态整合等挑战。

Conclusion: 本文为医学图像分析和计算医学领域的研究者和从业者提供了全面的参考，并指出了未来研究方向。

Abstract: Deep learning-based medical image-to-mesh reconstruction has rapidly evolved,
enabling the transformation of medical imaging data into three-dimensional mesh
models that are critical in computational medicine and in silico trials for
advancing our understanding of disease mechanisms, and diagnostic and
therapeutic techniques in modern medicine. This survey systematically
categorizes existing approaches into four main categories: template models,
statistical models, generative models, and implicit models. Each category is
analysed in detail, examining their methodological foundations, strengths,
limitations, and applicability to different anatomical structures and imaging
modalities. We provide an extensive evaluation of these methods across various
anatomical applications, from cardiac imaging to neurological studies,
supported by quantitative comparisons using standard metrics. Additionally, we
compile and analyze major public datasets available for medical mesh
reconstruction tasks and discuss commonly used evaluation metrics and loss
functions. The survey identifies current challenges in the field, including
requirements for topological correctness, geometric accuracy, and
multi-modality integration. Finally, we present promising future research
directions in this domain. This systematic review aims to serve as a
comprehensive reference for researchers and practitioners in medical image
analysis and computational medicine.

</details>

### [205] [PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model](https://arxiv.org/abs/2505.03603)
*Y. B. Wang,S. Z. Zhou,J. F. Wu,T. Hu,J. N. Zhang,Y. Liu*

Main category: cs.CV

TLDR: PAHA是一个基于扩散模型的端到端音频驱动上半身人体动画框架，通过PAR和PCE方法提升生成质量与音频-运动一致性，并设计了SG和DG推理指导方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多阶段生成和中间表示，导致推理时间长且生成质量与音频-运动一致性不足。

Method: 提出PAR动态调整区域训练损失权重，PCE构建基于扩散的区域音频-视觉分类器，并设计SG和DG推理指导方法。

Result: PAHA在音频-运动对齐和视频相关评估中显著优于现有方法。

Conclusion: PAHA通过局部细粒度监督指导解决了现有问题，并发布了首个中文新闻主播语音数据集CNAS。

Abstract: Audio-driven human animation technology is widely used in human-computer
interaction, and the emergence of diffusion models has further advanced its
development. Currently, most methods rely on multi-stage generation and
intermediate representations, resulting in long inference time and issues with
generation quality in specific foreground regions and audio-motion consistency.
These shortcomings are primarily due to the lack of localized fine-grained
supervised guidance. To address above challenges, we propose PAHA, an
end-to-end audio-driven upper-body human animation framework with diffusion
model. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts
Consistency Enhancement (PCE). PAR dynamically adjusts regional training loss
weights based on pose confidence scores, effectively improving visual quality.
PCE constructs and trains diffusion-based regional audio-visual classifiers to
improve the consistency of motion and co-speech audio. Afterwards, we design
two novel inference guidance methods for the foregoing classifiers, Sequential
Guidance (SG) and Differential Guidance (DG), to balance efficiency and quality
respectively. Additionally, we build CNAS, the first public Chinese News Anchor
Speech dataset, to advance research and validation in this field. Extensive
experimental results and user studies demonstrate that PAHA significantly
outperforms existing methods in audio-motion alignment and video-related
evaluations. The codes and CNAS dataset will be released upon acceptance.

</details>

### [206] [Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack Detection](https://arxiv.org/abs/2505.03610)
*Fangling Jiang,Qi Li,Bing Liu,Weining Wang,Caifeng Shan,Zhenan Sun,Ming-Hsuan Yang*

Main category: cs.CV

TLDR: 提出了一种基于知识图谱的提示学习框架，用于3D面具攻击检测，结合视觉语言模型和因果图理论，提升了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多模态传感器或rPPG信号，成本高且泛化能力有限，而视觉语言多模态特征的潜力尚未被探索。

Method: 通过知识图谱生成细粒度提示，结合视觉特定知识过滤器和因果图理论，消除虚假相关性。

Result: 在基准数据集上实现了最先进的检测性能。

Conclusion: 该方法有效利用了视觉语言模型的知识，提升了3D面具攻击检测的泛化能力。

Abstract: 3D mask presentation attack detection is crucial for protecting face
recognition systems against the rising threat of 3D mask attacks. While most
existing methods utilize multimodal features or remote photoplethysmography
(rPPG) signals to distinguish between real faces and 3D masks, they face
significant challenges, such as the high costs associated with multimodal
sensors and limited generalization ability. Detection-related text descriptions
offer concise, universal information and are cost-effective to obtain. However,
the potential of vision-language multimodal features for 3D mask presentation
attack detection remains unexplored. In this paper, we propose a novel
knowledge-based prompt learning framework to explore the strong generalization
capability of vision-language models for 3D mask presentation attack detection.
Specifically, our approach incorporates entities and triples from knowledge
graphs into the prompt learning process, generating fine-grained, task-specific
explicit prompts that effectively harness the knowledge embedded in pre-trained
vision-language models. Furthermore, considering different input images may
emphasize distinct knowledge graph elements, we introduce a visual-specific
knowledge filter based on an attention mechanism to refine relevant elements
according to the visual context. Additionally, we leverage causal graph theory
insights into the prompt learning process to further enhance the generalization
ability of our method. During training, a spurious correlation elimination
paradigm is employed, which removes category-irrelevant local image patches
using guidance from knowledge-based text features, fostering the learning of
generalized causal prompts that align with category-relevant local patches.
Experimental results demonstrate that the proposed method achieves
state-of-the-art intra- and cross-scenario detection performance on benchmark
datasets.

</details>

### [207] [Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using Only Real Face Images](https://arxiv.org/abs/2505.03611)
*Fangling Jiang,Qi Li,Weining Wang,Wei Shen,Bing Liu,Zhenan Sun*

Main category: cs.CV

TLDR: 论文提出了一种基于视觉语言模型的新方法，通过学习未知欺骗提示来提升人脸反欺骗的泛化能力，无需使用欺骗人脸图像。


<details>
  <summary>Details</summary>
Motivation: 解决人脸反欺骗系统在多样化场景中泛化能力不足的问题，主要针对协变量偏移和语义偏移。

Method: 利用视觉语言模型的通用知识生成真实人脸和潜在未知欺骗攻击的文本提示，并通过多样化欺骗提示优化框架学习有效提示。

Result: 在九个数据集上的实验表明，该方法实现了对未知攻击类型的最先进泛化能力。

Conclusion: 该方法通过文本提示学习显著提升了人脸反欺骗的泛化能力，适用于未见目标域。

Abstract: Face anti-spoofing is a critical technology for ensuring the security of face
recognition systems. However, its ability to generalize across diverse
scenarios remains a significant challenge. In this paper, we attribute the
limited generalization ability to two key factors: covariate shift, which
arises from external data collection variations, and semantic shift, which
results from substantial differences in emerging attack types. To address both
challenges, we propose a novel approach for learning unknown spoof prompts,
relying solely on real face images from a single source domain. Our method
generates textual prompts for real faces and potential unknown spoof attacks by
leveraging the general knowledge embedded in vision-language models, thereby
enhancing the model's ability to generalize to unseen target domains.
Specifically, we introduce a diverse spoof prompt optimization framework to
learn effective prompts. This framework constrains unknown spoof prompts within
a relaxed prior knowledge space while maximizing their distance from real face
images. Moreover, it enforces semantic independence among different spoof
prompts to capture a broad range of spoof patterns. Experimental results on
nine datasets demonstrate that the learned prompts effectively transfer the
knowledge of vision-language models, enabling state-of-the-art generalization
ability against diverse unknown attack types across unseen target domains
without using any spoof face images.

</details>

### [208] [PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing](https://arxiv.org/abs/2505.03621)
*Yiping Xie,Bo Zhao,Mingtong Dai,Jian-Ping Zhou,Yue Sun,Tao Tan,Weicheng Xie,Linlin Shen,Zitong Yu*

Main category: cs.CV

TLDR: PhysLLM框架结合LLMs与rPPG组件，通过跨模态对齐和双域特征重加权，解决了rPPG信号在光照变化和运动中的不稳定性问题，实现了高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: rPPG技术易受光照变化和运动干扰，LLMs虽擅长长程依赖但难以处理连续噪声信号，因此需结合两者优势。

Method: 提出PhysLLM框架，包括文本原型引导（TPG）跨模态对齐和双域稳态（DDS）算法，结合生理先验任务提示。

Result: 在四个基准数据集上，PhysLLM实现了最先进的精度和鲁棒性，适应光照变化和运动场景。

Conclusion: PhysLLM通过跨模态学习和动态适应，显著提升了rPPG技术的性能，具有广泛的应用潜力。

Abstract: Remote photoplethysmography (rPPG) enables non-contact physiological
measurement but remains highly susceptible to illumination changes, motion
artifacts, and limited temporal modeling. Large Language Models (LLMs) excel at
capturing long-range dependencies, offering a potential solution but struggle
with the continuous, noise-sensitive nature of rPPG signals due to their
text-centric design. To bridge this gap, we introduce PhysLLM, a collaborative
optimization framework that synergizes LLMs with domain-specific rPPG
components. Specifically, the Text Prototype Guidance (TPG) strategy is
proposed to establish cross-modal alignment by projecting hemodynamic features
into LLM-interpretable semantic space, effectively bridging the
representational gap between physiological signals and linguistic tokens.
Besides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for
resolving signal instability through adaptive time-frequency domain feature
re-weighting. Finally, rPPG task-specific cues systematically inject
physiological priors through physiological statistics, environmental contextual
answering, and task description, leveraging cross-modal learning to integrate
both visual and textual information, enabling dynamic adaptation to challenging
scenarios like variable illumination and subject movements. Evaluation on four
benchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness,
demonstrating superior generalization across lighting variations and motion
scenarios.

</details>

### [209] [Bounding Box-Guided Diffusion for Synthesizing Industrial Images and Segmentation Map](https://arxiv.org/abs/2505.03623)
*Alessandro Simoni,Francesco Pelosin*

Main category: cs.CV

TLDR: 提出一种基于扩散模型的新方法，用于生成高保真工业缺陷数据集，减少标注成本。


<details>
  <summary>Details</summary>
Motivation: 工业缺陷分割需要高精度标注数据，但获取成本高且耗时。

Method: 利用扩散模型结合边界框表示生成精确分割掩码。

Result: 相比现有方法，提高了缺陷一致性和空间准确性，并通过下游任务验证有效性。

Conclusion: 扩散模型能有效缩小合成数据与真实工业数据的差距，提升分割模型的可靠性和成本效益。

Abstract: Synthetic dataset generation in Computer Vision, particularly for industrial
applications, is still underexplored. Industrial defect segmentation, for
instance, requires highly accurate labels, yet acquiring such data is costly
and time-consuming. To address this challenge, we propose a novel
diffusion-based pipeline for generating high-fidelity industrial datasets with
minimal supervision. Our approach conditions the diffusion model on enriched
bounding box representations to produce precise segmentation masks, ensuring
realistic and accurately localized defect synthesis. Compared to existing
layout-conditioned generative methods, our approach improves defect consistency
and spatial accuracy. We introduce two quantitative metrics to evaluate the
effectiveness of our method and assess its impact on a downstream segmentation
task trained on real and synthetic data. Our results demonstrate that
diffusion-based synthesis can bridge the gap between artificial and real-world
industrial data, fostering more reliable and cost-efficient segmentation
models. The code is publicly available at
https://github.com/covisionlab/diffusion_labeling.

</details>

### [210] [Breaking Annotation Barriers: Generalized Video Quality Assessment via Ranking-based Self-Supervision](https://arxiv.org/abs/2505.03631)
*Linhan Cao,Wei Sun,Kaiwei Zhang,Yicong Peng,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TLDR: 论文提出了一种自监督学习框架，用于视频质量评估（VQA），通过大规模无标签网络视频学习质量评估能力，减少对人工标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有监督VQA模型依赖人工标注数据，成本高且难以扩展，限制了模型的泛化能力。

Method: 采用学习排序范式训练多模态模型，结合伪标签和合成失真模拟生成训练数据，并引入迭代自改进训练策略。

Result: 模型在零样本和微调场景下表现优异，泛化能力强，超越现有监督模型。

Conclusion: 自监督方法有效提升了VQA模型的泛化能力，为未来研究提供了新方向。

Abstract: Video quality assessment (VQA) is essential for quantifying perceptual
quality in various video processing workflows, spanning from camera capture
systems to over-the-top streaming platforms. While recent supervised VQA models
have made substantial progress, the reliance on manually annotated datasets --
a process that is labor-intensive, costly, and difficult to scale up -- has
hindered further optimization of their generalization to unseen video content
and distortions. To bridge this gap, we introduce a self-supervised learning
framework for VQA to learn quality assessment capabilities from large-scale,
unlabeled web videos. Our approach leverages a \textbf{learning-to-rank}
paradigm to train a large multimodal model (LMM) on video pairs automatically
labeled via two manners, including quality pseudo-labeling by existing VQA
models and relative quality ranking based on synthetic distortion simulations.
Furthermore, we introduce a novel \textbf{iterative self-improvement training
strategy}, where the trained model acts an improved annotator to iteratively
refine the annotation quality of training data. By training on a dataset
$10\times$ larger than the existing VQA benchmarks, our model: (1) achieves
zero-shot performance on in-domain VQA benchmarks that matches or surpasses
supervised models; (2) demonstrates superior out-of-distribution (OOD)
generalization across diverse video content and distortions; and (3) sets a new
state-of-the-art when fine-tuned on human-labeled datasets. Extensive
experimental results validate the effectiveness of our self-supervised approach
in training generalized VQA models. The datasets and code will be publicly
released to facilitate future research.

</details>

### [211] [Towards Smart Point-and-Shoot Photography](https://arxiv.org/abs/2505.03638)
*Jiawan Li,Fei Zhou,Zhipeng Zhong,Jiongzhi Lin,Guoping Qiu*

Main category: cs.CV

TLDR: 论文提出了一种智能点拍（SPAS）系统，通过实时调整相机姿态帮助用户拍摄更好的照片。系统包括一个基于CLIP的构图质量评估模型（CCQA）和一个相机姿态调整模型（CPAM）。


<details>
  <summary>Details</summary>
Motivation: 传统点拍相机无法指导用户如何构图，而智能手机用户普遍缺乏摄影技巧，因此需要一种智能系统来帮助用户拍摄更好的照片。

Method: 1. 构建包含32万张图像的数据集；2. 开发基于CLIP的CCQA模型，通过可学习文本嵌入技术评估图像质量；3. 开发CPAM模型，通过混合专家模型和门控损失函数实现端到端训练。

Result: SPAS系统能够实时指导用户调整相机姿态，提升照片构图质量。

Conclusion: SPAS系统首次实现了通过智能调整相机姿态帮助用户拍摄高质量照片，具有实际应用价值。

Abstract: Hundreds of millions of people routinely take photos using their smartphones
as point and shoot (PAS) cameras, yet very few would have the photography
skills to compose a good shot of a scene. While traditional PAS cameras have
built-in functions to ensure a photo is well focused and has the right
brightness, they cannot tell the users how to compose the best shot of a scene.
In this paper, we present a first of its kind smart point and shoot (SPAS)
system to help users to take good photos. Our SPAS proposes to help users to
compose a good shot of a scene by automatically guiding the users to adjust the
camera pose live on the scene. We first constructed a large dataset containing
320K images with camera pose information from 4000 scenes. We then developed an
innovative CLIP-based Composition Quality Assessment (CCQA) model to assign
pseudo labels to these images. The CCQA introduces a unique learnable text
embedding technique to learn continuous word embeddings capable of discerning
subtle visual quality differences in the range covered by five levels of
quality description words {bad, poor, fair, good, perfect}. And finally we have
developed a camera pose adjustment model (CPAM) which first determines if the
current view can be further improved and if so it outputs the adjust suggestion
in the form of two camera pose adjustment angles. The two tasks of CPAM make
decisions in a sequential manner and each involves different sets of training
samples, we have developed a mixture-of-experts model with a gated loss
function to train the CPAM in an end-to-end manner. We will present extensive
results to demonstrate the performances of our SPAS system using publicly
available image composition datasets.

</details>

### [212] [ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant](https://arxiv.org/abs/2505.03654)
*Yifan Xiang,Zhenxi Zhang,Bin Li,Yixuan Weng,Shoujun Zhou,Yangfan He,Keqin Li*

Main category: cs.CV

TLDR: 论文提出ReGraP数据集和ReGraP-LLaVA模型，解决现有MLLMs在关系推理和多对象学习上的不足，通过图提示方法提升个性化推理能力，并在新基准测试中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在捕捉用户特定概念时缺乏对多对象间关系的推理能力，且实验局限于单一概念任务。

Method: 提出ReGraP数据集（含图像、知识图和CoT问答对），设计ReGraP-LLaVA模型，采用软硬图提示方法对齐知识图与语义空间。

Result: ReGraP-LLaVA不仅能学习个性化知识，还能进行关系推理，在基准测试中表现最优。

Conclusion: ReGraP数据集和模型有效解决了MLLMs在关系推理和多对象学习上的局限性，为个性化理解提供了新方向。

Abstract: Recent advances in personalized MLLMs enable effective capture of
user-specific concepts, supporting both recognition of personalized concepts
and contextual captioning. However, humans typically explore and reason over
relations among objects and individuals, transcending surface-level information
to achieve more personalized and contextual understanding. To this end,
existing methods may face three main limitations: Their training data lacks
multi-object sets in which relations among objects are learnable. Building on
the limited training data, their models overlook the relations between
different personalized concepts and fail to reason over them. Their experiments
mainly focus on a single personalized concept, where evaluations are limited to
recognition and captioning tasks. To address the limitations, we present a new
dataset named ReGraP, consisting of 120 sets of personalized knowledge. Each
set includes images, KGs, and CoT QA pairs derived from the KGs, enabling more
structured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an
MLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard
graph prompting methods are designed to align KGs within the model's semantic
space. We establish the ReGraP Benchmark, which contains diverse task types:
multiple-choice, fill-in-the-blank, True/False, and descriptive questions in
both open- and closed-ended settings. The proposed benchmark is designed to
evaluate the relational reasoning and knowledge-connection capability of
personalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and
other competitive MLLMs. Results show that the proposed model not only learns
personalized knowledge but also performs relational reasoning in responses,
achieving the SoTA performance compared with the competitive methods. All the
codes and datasets are released at: https://github.com/xyfyyds/ReGraP.

</details>

### [213] [Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps from T1-Weighted MRI using CycleGAN Models](https://arxiv.org/abs/2505.03662)
*Xin Du,Francesca M. Cozzi,Rajesh Jena*

Main category: cs.CV

TLDR: 提出了一种基于CycleGAN的方法，直接从T1加权MRI扫描生成FA图，解决了FA图与纤维束追踪图谱的空间对齐问题。


<details>
  <summary>Details</summary>
Motivation: FA和DEC图对评估白质完整性至关重要，但FA图与纤维束追踪图谱的空间对齐问题限制了其在预测模型中的应用。

Method: 采用CycleGAN方法，利用未配对数据训练模型，直接从T1加权MRI生成FA图。

Result: 模型生成的FA图具有高保真度，尤其在肿瘤区域表现优异，SSIM和PSNR评估结果良好。

Conclusion: 该方法为临床工作流程提供了AI驱动的替代方案，减少了对额外扫描的需求。

Abstract: Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are
essential for evaluating white matter integrity and structural connectivity in
neuroimaging. However, the spatial misalignment between FA maps and
tractography atlases hinders their effective integration into predictive
models. To address this issue, we propose a CycleGAN based approach for
generating FA maps directly from T1-weighted MRI scans, representing the first
application of this technique to both healthy and tumour-affected tissues. Our
model, trained on unpaired data, produces high fidelity maps, which have been
rigorously evaluated using Structural Similarity Index (SSIM) and Peak
Signal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in
tumour regions. Radiological assessments further underscore the model's
potential to enhance clinical workflows by providing an AI-driven alternative
that reduces the necessity for additional scans.

</details>

### [214] [Distribution-Conditional Generation: From Class Distribution to Creative Generation](https://arxiv.org/abs/2505.03667)
*Fu Feng,Yucheng Xie,Xu Yang,Jing Wang,Xin Geng*

Main category: cs.CV

TLDR: 提出了一种基于分布条件生成的新方法DisTok，通过动态概念池和迭代采样融合，实现语义不受限的创意生成。


<details>
  <summary>Details</summary>
Motivation: 现有T2I扩散模型依赖训练数据分布，难以生成真正新颖的、超出分布的概念。

Method: 提出Distribution-Conditional Generation，并基于此设计DisTok框架，通过动态概念池和迭代采样融合生成创意概念。

Result: DisTok在文本-图像对齐和人类偏好评分上达到最优性能。

Conclusion: DisTok通过统一分布条件融合和基于采样的合成，实现了高效灵活的令牌级生成。

Abstract: Text-to-image (T2I) diffusion models are effective at producing semantically
aligned images, but their reliance on training data distributions limits their
ability to synthesize truly novel, out-of-distribution concepts. Existing
methods typically enhance creativity by combining pairs of known concepts,
yielding compositions that, while out-of-distribution, remain linguistically
describable and bounded within the existing semantic space. Inspired by the
soft probabilistic outputs of classifiers on ambiguous inputs, we propose
Distribution-Conditional Generation, a novel formulation that models creativity
as image synthesis conditioned on class distributions, enabling semantically
unconstrained creative generation. Building on this, we propose DisTok, an
encoder-decoder framework that maps class distributions into a latent space and
decodes them into tokens of creative concept. DisTok maintains a dynamic
concept pool and iteratively sampling and fusing concept pairs, enabling the
generation of tokens aligned with increasingly complex class distributions. To
enforce distributional consistency, latent vectors sampled from a Gaussian
prior are decoded into tokens and rendered into images, whose class
distributions-predicted by a vision-language model-supervise the alignment
between input distributions and the visual semantics of generated tokens. The
resulting tokens are added to the concept pool for subsequent composition.
Extensive experiments demonstrate that DisTok, by unifying
distribution-conditioned fusion and sampling-based synthesis, enables efficient
and flexible token-level generation, achieving state-of-the-art performance
with superior text-image alignment and human preference scores.

</details>

### [215] [CaRaFFusion: Improving 2D Semantic Segmentation with Camera-Radar Point Cloud Fusion and Zero-Shot Image Inpainting](https://arxiv.org/abs/2505.03679)
*Huawei Sun,Bora Kunter Sahin,Georg Stettinger,Maximilian Bernhard,Matthias Schubert,Robert Wille*

Main category: cs.CV

TLDR: 提出了一种新颖的框架，通过将扩散模型集成到相机-雷达融合架构中，提升仅使用相机的基线性能。利用雷达点特征生成伪掩码，并通过噪声减少单元优化，最终在恶劣天气条件下显著提升语义分割效果。


<details>
  <summary>Details</summary>
Motivation: 相机传感器在恶劣天气下性能下降，而雷达传感器虽稳健但数据稀疏且噪声多。融合两者信息可提升环境感知能力。

Method: 结合扩散模型与相机-雷达融合架构，利用雷达点特征生成伪掩码，并通过噪声减少单元优化，生成修复图像以补充原始图像缺失信息。

Result: 在Waterscenes数据集上，相机基线分割性能提升2.63% mIoU，相机-雷达融合架构性能提升1.48% mIoU。

Conclusion: 该方法在恶劣天气条件下通过相机-雷达融合有效提升了语义分割性能。

Abstract: Segmenting objects in an environment is a crucial task for autonomous driving
and robotics, as it enables a better understanding of the surroundings of each
agent. Although camera sensors provide rich visual details, they are vulnerable
to adverse weather conditions. In contrast, radar sensors remain robust under
such conditions, but often produce sparse and noisy data. Therefore, a
promising approach is to fuse information from both sensors. In this work, we
propose a novel framework to enhance camera-only baselines by integrating a
diffusion model into a camera-radar fusion architecture. We leverage radar
point features to create pseudo-masks using the Segment-Anything model,
treating the projected radar points as point prompts. Additionally, we propose
a noise reduction unit to denoise these pseudo-masks, which are further used to
generate inpainted images that complete the missing information in the original
images. Our method improves the camera-only segmentation baseline by 2.63% in
mIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on the
Waterscenes dataset. This demonstrates the effectiveness of our approach for
semantic segmentation using camera-radar fusion under adverse weather
conditions.

</details>

### [216] [Matching Distance and Geometric Distribution Aided Learning Multiview Point Cloud Registration](https://arxiv.org/abs/2505.03692)
*Shiqi Li,Jihua Zhu,Yifan Xie,Naiwen Hu,Di Wang*

Main category: cs.CV

TLDR: 本文提出了一种基于神经网络的多视角点云配准方法，通过设计网络模型选择可靠的匹配对构建位姿图，并利用数据驱动的方式计算绝对位姿。


<details>
  <summary>Details</summary>
Motivation: 多视角点云配准在机器人、自动化和计算机视觉领域至关重要，但现有方法在构建位姿图和运动同步时存在可靠性不足的问题。

Method: 设计了两个神经网络模型：一个用于从点云对匹配距离中提取信息以选择可靠匹配对；另一个用于数据驱动地计算绝对位姿，结合几何分布信息和改进的注意力机制。

Result: 在多种室内外数据集上的实验验证了方法的有效性和泛化能力。

Conclusion: 该方法通过神经网络提升了多视角点云配准的可靠性和灵活性，代码已开源。

Abstract: Multiview point cloud registration plays a crucial role in robotics,
automation, and computer vision fields. This paper concentrates on pose graph
construction and motion synchronization within multiview registration. Previous
methods for pose graph construction often pruned fully connected graphs or
constructed sparse graph using global feature aggregated from local
descriptors, which may not consistently yield reliable results. To identify
dependable pairs for pose graph construction, we design a network model that
extracts information from the matching distance between point cloud pairs. For
motion synchronization, we propose another neural network model to calculate
the absolute pose in a data-driven manner, rather than optimizing inaccurate
handcrafted loss functions. Our model takes into account geometric distribution
information and employs a modified attention mechanism to facilitate flexible
and reliable feature interaction. Experimental results on diverse indoor and
outdoor datasets confirm the effectiveness and generalizability of our
approach. The source code is available at https://github.com/Shi-Qi-Li/MDGD.

</details>

### [217] [Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning](https://arxiv.org/abs/2505.03703)
*François Role,Sébastien Meyer,Victor Amblard*

Main category: cs.CV

TLDR: 本文提出新方法（基于谱分析和最优传输）来评估和减少视觉语言模型中的模态间隙，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型存在模态间隙问题，影响下游任务性能，但目前缺乏通用且实用的评估和解决方法。

Method: 提出基于谱分析和最优传输的新方法，用于评估和减少模态间隙。

Result: 在多个图像-文本数据集和模型上的实验表明，所提方法有效且对下游任务有益。

Conclusion: 新方法能有效解决模态间隙问题，提升视觉语言模型的性能。

Abstract: Vision-language models (VLMs) allow to embed texts and images in a shared
representation space. However, it has been shown that these models are subject
to a modality gap phenomenon meaning there exists a clear separation between
the embeddings from one modality and another in the embedding space. While this
misalignment is detrimental for downstream tasks such as multimodal retrieval,
multimodal clustering or zero-shot classification, etc. no generic and
practical methods have so far been proposed to assess it precisely and even
reduce it. We therefore propose novel measures and effective techniques
(spectral- and optimal transport-based methods) to achieve this goal. Extensive
experiments conducted on several image-text datasets and models demonstrate
their effectiveness and beneficial effects on downstream tasks. Our code is
available at the URL provided in the paper's abstract.

</details>

### [218] [DISARM++: Beyond scanner-free harmonization](https://arxiv.org/abs/2505.03715)
*Luca Caldera,Lara Cavinato,Alessio Cirone,Isabella Cama,Sara Garbarino,Raffaele Lodi,Fabrizio Tagliavini,Anna Nigri,Silvia De Francesco,Andrea Cappozzo,Michele Piana,Francesca Ieva*

Main category: cs.CV

TLDR: 提出了一种新的T1加权MR图像协调方法，通过映射到无扫描仪空间或特定扫描仪域，确保特征可靠性，并在多任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决不同扫描仪间T1加权MR图像不一致的问题，确保下游分析的可靠性。

Method: 通过两种方式实现图像转换：映射到无扫描仪空间或特定扫描仪域，无需预处理步骤。

Result: 在脑龄预测、生物标志物提取、AD分类等任务中表现优异（R2=0.60，测试准确率0.86，AUC=0.95），优于现有方法。

Conclusion: 该方法提供了一种高效、无需重新训练的解决方案，适用于多样化的神经影像研究。

Abstract: Harmonization of T1-weighted MR images across different scanners is crucial
for ensuring consistency in neuroimaging studies. This study introduces a novel
approach to direct image harmonization, moving beyond feature standardization
to ensure that extracted features remain inherently reliable for downstream
analysis. Our method enables image transfer in two ways: (1) mapping images to
a scanner-free space for uniform appearance across all scanners, and (2)
transforming images into the domain of a specific scanner used in model
training, embedding its unique characteristics. Our approach presents strong
generalization capability, even for unseen scanners not included in the
training phase. We validated our method using MR images from diverse cohorts,
including healthy controls, traveling subjects, and individuals with
Alzheimer's disease (AD). The model's effectiveness is tested in multiple
applications, such as brain age prediction (R2 = 0.60 \pm 0.05), biomarker
extraction, AD classification (Test Accuracy = 0.86 \pm 0.03), and diagnosis
prediction (AUC = 0.95). In all cases, our harmonization technique outperforms
state-of-the-art methods, showing improvements in both reliability and
predictive accuracy. Moreover, our approach eliminates the need for extensive
preprocessing steps, such as skull-stripping, which can introduce errors by
misclassifying brain and non-brain structures. This makes our method
particularly suitable for applications that require full-head analysis,
including research on head trauma and cranial deformities. Additionally, our
harmonization model does not require retraining for new datasets, allowing
smooth integration into various neuroimaging workflows. By ensuring
scanner-invariant image quality, our approach provides a robust and efficient
solution for improving neuroimaging studies across diverse settings. The code
is available at this link.

</details>

### [219] [FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios](https://arxiv.org/abs/2505.03730)
*Shiyi Zhang,Junhao Zhuang,Zhaoyang Zhang,Ying Shan,Yansong Tang*

Main category: cs.CV

TLDR: FlexiAct是一种新方法，通过RefAdapter和FAE技术，实现从参考视频到任意目标图像的动作迁移，突破了现有方法在空间结构和一致性上的限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动作定制化中受限于空间结构的严格约束（如布局、骨架和视角一致性），限制了其适应性和多样性。

Method: 提出FlexiAct方法，结合RefAdapter（轻量级图像条件适配器）和FAE（频率感知动作提取），实现动作迁移和一致性保持。

Result: 实验表明，FlexiAct能有效迁移动作至具有不同布局、骨架和视角的目标图像，平衡了外观一致性和结构灵活性。

Conclusion: FlexiAct通过创新的空间适应和动作提取技术，显著提升了动作定制化的适应性和效果。

Abstract: Action customization involves generating videos where the subject performs
actions dictated by input control signals. Current methods use pose-guided or
global motion customization but are limited by strict constraints on spatial
structure, such as layout, skeleton, and viewpoint consistency, reducing
adaptability across diverse subjects and scenarios. To overcome these
limitations, we propose FlexiAct, which transfers actions from a reference
video to an arbitrary target image. Unlike existing methods, FlexiAct allows
for variations in layout, viewpoint, and skeletal structure between the subject
of the reference video and the target image, while maintaining identity
consistency. Achieving this requires precise action control, spatial structure
adaptation, and consistency preservation. To this end, we introduce RefAdapter,
a lightweight image-conditioned adapter that excels in spatial adaptation and
consistency preservation, surpassing existing methods in balancing appearance
consistency and structural flexibility. Additionally, based on our
observations, the denoising process exhibits varying levels of attention to
motion (low frequency) and appearance details (high frequency) at different
timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike
existing methods that rely on separate spatial-temporal architectures, directly
achieves action extraction during the denoising process. Experiments
demonstrate that our method effectively transfers actions to subjects with
diverse layouts, skeletons, and viewpoints. We release our code and model
weights to support further research at
https://shiyi-zh0408.github.io/projectpages/FlexiAct/

</details>

### [220] [Multi-Agent System for Comprehensive Soccer Understanding](https://arxiv.org/abs/2505.03735)
*Jiayuan Rao,Zifeng Li,Haoning Wu,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: cs.CV

TLDR: 论文提出一个全面的足球理解框架，包括构建SoccerWiki知识库、SoccerBench基准测试、SoccerAgent多智能体系统，并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动的足球理解研究多局限于孤立任务，缺乏全面性。

Method: 构建SoccerWiki知识库、SoccerBench基准测试，开发SoccerAgent多智能体系统。

Result: 实验表明SoccerAgent在多模态足球理解任务中表现优越。

Conclusion: 提出的框架填补了足球理解领域的空白，数据与代码已开源。

Abstract: Recent advancements in AI-driven soccer understanding have demonstrated rapid
progress, yet existing research predominantly focuses on isolated or narrow
tasks. To bridge this gap, we propose a comprehensive framework for holistic
soccer understanding. Specifically, we make the following contributions in this
paper: (i) we construct SoccerWiki, the first large-scale multimodal soccer
knowledge base, integrating rich domain knowledge about players, teams,
referees, and venues to enable knowledge-driven reasoning; (ii) we present
SoccerBench, the largest and most comprehensive soccer-specific benchmark,
featuring around 10K standardized multimodal (text, image, video) multi-choice
QA pairs across 13 distinct understanding tasks, curated through automated
pipelines and manual verification; (iii) we introduce SoccerAgent, a novel
multi-agent system that decomposes complex soccer questions via collaborative
reasoning, leveraging domain expertise from SoccerWiki and achieving robust
performance; (iv) extensive evaluations and ablations that benchmark
state-of-the-art MLLMs on SoccerBench, highlighting the superiority of our
proposed agentic system. All data and code are publicly available at:
https://jyrao.github.io/SoccerAgent/.

</details>

<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [221] [SKALD: Scalable K-Anonymisation for Large Datasets](https://arxiv.org/abs/2505.03529)
*Kailash Reddy,Novoneel Chakraborty,Amogh Dharmavaram,Anshoo Tandon*

Main category: cs.IT

TLDR: SKALD算法是一种针对内存有限的大数据集进行k-匿名化的新方法，通过分块处理并结合足够统计信息，显著提升了性能和数据效用。


<details>
  <summary>Details</summary>
Motivation: 在数据隐私和匿名化需求日益增长的背景下，现有工具（如ARX）只能分块处理大数据集，无法充分利用全局信息，导致性能和数据效用受限。

Method: 提出SKALD算法，通过分块处理并提取和结合足够统计信息，确保全局k-匿名化，同时优化性能。

Result: SKALD算法在性能上显著优于标准k-匿名化方法，并提供了更好的数据效用。

Conclusion: SKALD算法为大数据集的隐私保护提供了一种高效且实用的解决方案。

Abstract: Data privacy and anonymisation are critical concerns in today's data-driven
society, particularly when handling personal and sensitive user data.
Regulatory frameworks worldwide recommend privacy-preserving protocols such as
k-anonymisation to de-identify releases of tabular data. Available hardware
resources provide an upper bound on the maximum size of dataset that can be
processed at a time. Large datasets with sizes exceeding this upper bound must
be broken up into smaller data chunks for processing. In these cases, standard
k-anonymisation tools such as ARX can only operate on a per-chunk basis. This
paper proposes SKALD, a novel algorithm for performing k-anonymisation on large
datasets with limited RAM. Our SKALD algorithm offers multi-fold performance
improvement over standard k-anonymisation methods by extracting and combining
sufficient statistics from each chunk during processing to ensure successful
k-anonymisation while providing better utility.

</details>

### [222] [Soft Best-of-n Sampling for Model Alignment](https://arxiv.org/abs/2505.03156)
*Claudio Mayrink Verdun,Alex Oesterling,Himabindu Lakkaraju,Flavio P. Calmon*

Main category: cs.IT

TLDR: N/A


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Best-of-$n$ (BoN) sampling is a practical approach for aligning language
model outputs with human preferences without expensive fine-tuning. BoN
sampling is performed by generating $n$ responses to a prompt and then
selecting the sample that maximizes a reward function. BoN yields high reward
values in practice at a distortion cost, as measured by the KL-divergence
between the sampled and original distribution. This distortion is coarsely
controlled by varying the number of samples: larger $n$ yields a higher reward
at a higher distortion cost. We introduce Soft Best-of-$n$ sampling, a
generalization of BoN that allows for smooth interpolation between the original
distribution and reward-maximizing distribution through a temperature parameter
$\lambda$. We establish theoretical guarantees showing that Soft Best-of-$n$
sampling converges sharply to the optimal tilted distribution at a rate of
$O(1/n)$ in KL and the expected (relative) reward. For sequences of discrete
outputs, we analyze an additive reward model that reveals the fundamental
limitations of blockwise sampling.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [223] [Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve Sparse Retrieval](https://arxiv.org/abs/2505.03676)
*Arthur Satouf,Gabriel Ben Zenou,Benjamin Piwowarski,Habiboulaye Amadou Boubacar,Pablo Piantanida*

Main category: cs.IR

TLDR: 本文提出了一种基于Rational Speech Acts (RSA)框架的稀疏神经信息检索方法，通过动态调整文档中词项的权重，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏神经信息检索方法及传统模型（如BM25）未充分考虑文档集合中词项权重的复杂交互作用，导致文档表示不够准确。

Method: 将语言学中的RSA框架应用于信息检索，动态调整词项与文档的交互权重，考虑数据集中文档的影响，优化文档表示。

Result: 实验表明，引入RSA框架显著提升了多种稀疏检索模型的性能，并在BEIR基准测试的外域数据集上达到了最先进的水平。

Conclusion: RSA框架能够有效优化稀疏神经信息检索模型的性能，尤其在处理复杂文档集合时表现突出。

Abstract: Current sparse neural information retrieval (IR) methods, and to a lesser
extent more traditional models such as BM25, do not take into account the
document collection and the complex interplay between different term weights
when representing a single document. In this paper, we show how the Rational
Speech Acts (RSA), a linguistics framework used to minimize the number of
features to be communicated when identifying an object in a set, can be adapted
to the IR case -- and in particular to the high number of potential features
(here, tokens). RSA dynamically modulates token-document interactions by
considering the influence of other documents in the dataset, better contrasting
document representations. Experiments show that incorporating RSA consistently
improves multiple sparse retrieval models and achieves state-of-the-art
performance on out-of-domain datasets from the BEIR benchmark.
https://github.com/arthur-75/Rational-Retrieval-Acts

</details>

### [224] [Feature Staleness Aware Incremental Learning for CTR Prediction](https://arxiv.org/abs/2505.02844)
*Zhikai Wang,Yanyan Shen,Zibin Zhang,Kangyi Lin*

Main category: cs.IR

TLDR: 论文提出了一种名为FeSAIL的方法，用于解决CTR预测模型中的特征陈旧问题，通过自适应重放陈旧特征样本和正则化机制提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 在实时推荐系统中，CTR预测模型的特征嵌入会因特征未出现在增量数据中而变得陈旧，导致性能下降。

Method: 提出FeSAIL方法，包括SAS算法高效采样陈旧样本，以及SAR机制精细控制特征嵌入更新。

Result: 实验表明，FeSAIL在四个基准数据集上优于现有方法。

Conclusion: FeSAIL有效缓解了特征陈旧问题，提升了CTR预测模型的性能。

Abstract: Click-through Rate (CTR) prediction in real-world recommender systems often
deals with billions of user interactions every day. To improve the training
efficiency, it is common to update the CTR prediction model incrementally using
the new incremental data and a subset of historical data. However, the feature
embeddings of a CTR prediction model often get stale when the corresponding
features do not appear in current incremental data. In the next period, the
model would have a performance degradation on samples containing stale
features, which we call the feature staleness problem. To mitigate this
problem, we propose a Feature Staleness Aware Incremental Learning method for
CTR prediction (FeSAIL) which adaptively replays samples containing stale
features. We first introduce a staleness aware sampling algorithm (SAS) to
sample a fixed number of stale samples with high sampling efficiency. We then
introduce a staleness aware regularization mechanism (SAR) for a fine-grained
control of the feature embedding updating. We instantiate FeSAIL with a general
deep learning-based CTR prediction model and the experimental results
demonstrate FeSAIL outperforms various state-of-the-art methods on four
benchmark datasets.

</details>

### [225] [Avoid Recommending Out-of-Domain Items: Constrained Generative Recommendation with LLMs](https://arxiv.org/abs/2505.03336)
*Hao Liao,Wensheng Lu,Jianxun Lian,Mingqi Wu,Shuo Wang,Yong Zhang,Yitian Huang,Mingyang Zhou,Xing Xie*

Main category: cs.IR

TLDR: 论文研究了两种方法（RecLM-ret和RecLM-cgen）来解决LLM推荐系统中的OOD问题，RecLM-cgen表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在生成推荐系统中可能推荐超出领域（OOD）物品的问题。

Method: 提出两种方法：基于检索的RecLM-ret和基于约束生成的RecLM-cgen。

Result: RecLM-cgen在三个数据集上表现优于RecLM-ret和现有LLM推荐模型，且消除了OOD推荐。

Conclusion: RecLM-cgen是更优的方法，具有轻量化和易集成的特点，适合实际应用。

Abstract: Large Language Models (LLMs) have shown promise for generative recommender
systems due to their transformative capabilities in user interaction. However,
ensuring they do not recommend out-of-domain (OOD) items remains a challenge.
We study two distinct methods to address this issue: RecLM-ret, a
retrieval-based method, and RecLM-cgen, a constrained generation method. Both
methods integrate seamlessly with existing LLMs to ensure in-domain
recommendations. Comprehensive experiments on three recommendation datasets
demonstrate that RecLM-cgen consistently outperforms RecLM-ret and existing
LLM-based recommender models in accuracy while eliminating OOD recommendations,
making it the preferred method for adoption. Additionally, RecLM-cgen maintains
strong generalist capabilities and is a lightweight plug-and-play module for
easy integration into LLMs, offering valuable practical benefits for the
community. Source code is available at https://github.com/microsoft/RecAI

</details>

### [226] [Modeling Musical Genre Trajectories through Pathlet Learning](https://arxiv.org/abs/2505.03480)
*Lilian Marey,Charlotte Laclau,Bruno Sguerra,Tiphaine Viard,Manuel Moussallam*

Main category: cs.IR

TLDR: 论文提出了一种基于字典学习的方法，通过‘pathlets’建模用户音乐流派轨迹，揭示了用户听歌模式，并发布了相关数据集。


<details>
  <summary>Details</summary>
Motivation: 音乐流媒体平台上用户数据的增加为分析音乐消费提供了新机会，但理解用户偏好的演变仍具挑战性。

Method: 使用字典学习范式建模用户音乐流派轨迹，定义‘pathlets’框架捕捉重复模式。

Result: Pathlet学习揭示了可定性和定量分析的听歌模式，有助于理解用户行为并改进推荐系统。

Conclusion: 该方法提升了对用户音乐交互的理解，并为研究用户行为和推荐系统多样性提供了新方向。

Abstract: The increasing availability of user data on music streaming platforms opens
up new possibilities for analyzing music consumption. However, understanding
the evolution of user preferences remains a complex challenge, particularly as
their musical tastes change over time. This paper uses the dictionary learning
paradigm to model user trajectories across different musical genres. We define
a new framework that captures recurring patterns in genre trajectories, called
pathlets, enabling the creation of comprehensible trajectory embeddings. We
show that pathlet learning reveals relevant listening patterns that can be
analyzed both qualitatively and quantitatively. This work improves our
understanding of users' interactions with music and opens up avenues of
research into user behavior and fostering diversity in recommender systems. A
dataset of 2000 user histories tagged by genre over 17 months, supplied by
Deezer (a leading music streaming company), is also released with the code.

</details>

### [227] [Counterfactual Inference for Eliminating Sentiment Bias in Recommender Systems](https://arxiv.org/abs/2505.03655)
*Le Pan,Yuanjiang Cao,Chengkai Huang,Wenjie Zhang,Lina Yao*

Main category: cs.IR

TLDR: 论文提出了一种基于反事实推理的方法，用于缓解推荐系统中的情感偏差问题，通过建模情感对评分的影响并分离直接和间接效应，实现了更公平的推荐。


<details>
  <summary>Details</summary>
Motivation: 研究发现基于评论的推荐系统（RRSs）中存在情感偏差，导致负面评论的用户或物品推荐准确性下降，影响公平性。

Method: 采用两阶段的反事实推理方法：1）训练阶段构建因果图建模情感对评分的影响；2）推理阶段分离直接和间接效应，通过反事实推理消除间接效应。

Result: 实验表明，该方法在评分预测上表现优异，有效缓解了情感偏差。

Conclusion: 这是首次在推荐系统中应用反事实推理解决情感偏差问题，为公平推荐提供了新思路。

Abstract: Recommender Systems (RSs) aim to provide personalized recommendations for
users. A newly discovered bias, known as sentiment bias, uncovers a common
phenomenon within Review-based RSs (RRSs): the recommendation accuracy of users
or items with negative reviews deteriorates compared with users or items with
positive reviews. Critical users and niche items are disadvantaged by such
unfair recommendations. We study this problem from the perspective of
counterfactual inference with two stages. At the model training stage, we build
a causal graph and model how sentiment influences the final rating score.
During the inference stage, we decouple the direct and indirect effects to
mitigate the impact of sentiment bias and remove the indirect effect using
counterfactual inference. We have conducted extensive experiments, and the
results validate that our model can achieve comparable performance on rating
prediction for better recommendations and effective mitigation of sentiment
bias. To the best of our knowledge, this is the first work to employ
counterfactual inference on sentiment bias mitigation in RSs.

</details>

<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [228] [An Active Inference perspective on Neurofeedback Training](https://arxiv.org/abs/2505.03308)
*Côme Annicchiarico,Fabien Lotte,Jérémie Mattout*

Main category: q-bio.NC

TLDR: 论文提出了一种基于主动推理的计算模型，用于模拟神经反馈训练（NFT）的闭环过程，以解决NFT效果不稳定和机制不明确的问题。


<details>
  <summary>Details</summary>
Motivation: NFT的效果因人而异且机制不明确，阻碍了其验证和应用。

Method: 采用主动推理（一种贝叶斯框架）模拟代理与NFT环境的交互，测试设计和被试因素对训练的影响。

Result: 模拟显示训练效果受反馈噪声、偏差及被试先验信念影响，完美反馈也无法保证高效训练。

Conclusion: 该模型为评估NFT变异性、解释实验数据及开发个性化训练方案提供了工具。

Abstract: Neurofeedback training (NFT) aims to teach self-regulation of brain activity
through real-time feedback, but suffers from highly variable outcomes and
poorly understood mechanisms, hampering its validation. To address these
issues, we propose a formal computational model of the NFT closed loop. Using
Active Inference, a Bayesian framework modelling perception, action, and
learning, we simulate agents interacting with an NFT environment. This enables
us to test the impact of design choices (e.g., feedback quality, biomarker
validity) and subject factors (e.g., prior beliefs) on training. Simulations
show that training effectiveness is sensitive to feedback noise or bias, and to
prior beliefs (highlighting the importance of guiding instructions), but also
reveal that perfect feedback is insufficient to guarantee high performance.
This approach provides a tool for assessing and predicting NFT variability,
interpret empirical data, and potentially develop personalized training
protocols.

</details>

### [229] [Binding threshold units with artificial oscillatory neurons](https://arxiv.org/abs/2505.03648)
*Vladimir Fanaskov,Ivan Oseledets*

Main category: q-bio.NC

TLDR: 论文提出了一种理论框架，区分振荡神经元与阈值单元，并建立了它们的耦合机制，结合了Hopfield网络和Kuramoto模型，形成了一种新的联想记忆模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索振荡神经元与阈值单元在神经编码中的不同作用，并设计一种耦合机制，结合两者的优势。

Method: 方法包括理论分析，通过Lyapunov函数约束动力学系统，推导出Hopfield网络和广义Kuramoto模型，并实现它们的耦合。

Result: 结果表明，振荡神经元可以用于Hopfield网络的低秩修正，类似于Hebbian学习或LoRA方法，并通过实验验证了其可行性。

Conclusion: 结论指出，振荡神经元与阈值单元的耦合为神经编码提供了新的视角，并在实际应用中展示了潜力。

Abstract: Artificial Kuramoto oscillatory neurons were recently introduced as an
alternative to threshold units. Empirical evidence suggests that oscillatory
units outperform threshold units in several tasks including unsupervised object
discovery and certain reasoning problems. The proposed coupling mechanism for
these oscillatory neurons is heterogeneous, combining a generalized Kuramoto
equation with standard coupling methods used for threshold units. In this
research note, we present a theoretical framework that clearly distinguishes
oscillatory neurons from threshold units and establishes a coupling mechanism
between them. We argue that, from a biological standpoint, oscillatory and
threshold units realise distinct aspects of neural coding: roughly, threshold
units model intensity of neuron firing, while oscillatory units facilitate
information exchange by frequency modulation. To derive interaction between
these two types of units, we constrain their dynamics by focusing on dynamical
systems that admit Lyapunov functions. For threshold units, this leads to
Hopfield associative memory model, and for oscillatory units it yields a
specific form of generalized Kuramoto model. The resulting dynamical systems
can be naturally coupled to form a Hopfield-Kuramoto associative memory model,
which also admits a Lyapunov function. Various forms of coupling are possible.
Notably, oscillatory neurons can be employed to implement a low-rank correction
to the weight matrix of a Hopfield network. This correction can be viewed
either as a form of Hebbian learning or as a popular LoRA method used for
fine-tuning of large language models. We demonstrate the practical realization
of this particular coupling through illustrative toy experiments.

</details>

<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [230] [Parameter estimation for land-surface models using machine learning libraries](https://arxiv.org/abs/2505.02979)
*Ruiyue Huang,Claire E. Heaney,Maarten van Reeuwijk*

Main category: physics.ao-ph

TLDR: NN4PDEs方法通过PyTorch的反向传播引擎估算地表模型参数，单层土壤温度数据无法可靠估计参数，但双层数据可以，尽管无法区分潜热和感热通量。应用于城市数据后，模型能准确预测热通量。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用NN4PDEs方法通过反向传播估算地表模型参数，并验证其可靠性。

Method: 使用PyTorch的反向传播引擎，通过合成数据集测试逆模型，并应用于实际城市数据。

Result: 单层土壤温度数据无法可靠估计参数，双层数据可以，但无法区分潜热和感热通量。模型能准确预测热通量。

Conclusion: NN4PDEs方法在特定条件下可有效估算地表模型参数，但需多深度数据支持。

Abstract: The Neural Networks for Partial Differential Equations (NN4PDEs) approach is
used to determine the parameters of a simple land-surface model using PyTorch's
backpropagation engine. In order to test the inverse model, a synthetic dataset
is created by running the model in forward mode with known parameter values to
create soil temperature time series that can be used as observations for the
inverse model. We show that it is not possible to obtain a reliable parameter
estimation using a single observed soil temperature time series. Using
measurements at two depths, reliable parameter estimates can be obtained
although it is not possible to differentiate between latent and sensible heat
fluxes. We apply the inverse model to urban flux tower data in Phoenix, United
States, and show that the thermal conductivity, volumetric heat capacity, and
the combined sensible-latent heat transfer coefficient can be reliably
estimated using an observed value for the effective surface albedo. The
resulting model accurately predicts the outgoing longwave radiation, conductive
soil fluxes and the combined sensible-latent heat fluxes.

</details>

<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [231] [Neural Orchestration for Multi-Agent Systems: A Deep Learning Framework for Optimal Agent Selection in Multi-Domain Task Environments](https://arxiv.org/abs/2505.02861)
*Kushagra Agrawal,Nisharg Nargund*

Main category: cs.MA

TLDR: MetaOrch是一个神经编排框架，用于在多领域任务环境中优化代理选择，通过监督学习和模糊评估模块动态预测最适合的代理，显著提升了选择准确率。


<details>
  <summary>Details</summary>
Motivation: 传统多代理系统（MAS）架构的协调机制僵化，难以适应动态任务，因此需要一种更灵活、自适应的解决方案。

Method: 采用监督学习方法建模任务上下文、代理历史和预期响应质量，结合模糊评估模块对代理响应进行评分，动态预测最合适的代理。

Result: 在模拟环境中，MetaOrch实现了86.3%的选择准确率，显著优于随机选择和轮询调度等基线策略。

Conclusion: 神经编排为多代理系统提供了增强自主性、可解释性和适应性的有效方法。

Abstract: Multi-agent systems (MAS) are foundational in simulating complex real-world
scenarios involving autonomous, interacting entities. However, traditional MAS
architectures often suffer from rigid coordination mechanisms and difficulty
adapting to dynamic tasks. We propose MetaOrch, a neural orchestration
framework for optimal agent selection in multi-domain task environments. Our
system implements a supervised learning approach that models task context,
agent histories, and expected response quality to select the most appropriate
agent for each task. A novel fuzzy evaluation module scores agent responses
along completeness, relevance, and confidence dimensions, generating soft
supervision labels for training the orchestrator. Unlike previous methods that
hard-code agent-task mappings, MetaOrch dynamically predicts the most suitable
agent while estimating selection confidence. Experiments in simulated
environments with heterogeneous agents demonstrate that our approach achieves
86.3% selection accuracy, significantly outperforming baseline strategies
including random selection and round-robin scheduling. The modular architecture
emphasizes extensibility, allowing agents to be registered, updated, and
queried independently. Results suggest that neural orchestration offers a
powerful approach to enhancing the autonomy, interpretability, and adaptability
of multi-agent systems across diverse task domains.

</details>

### [232] [Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering](https://arxiv.org/abs/2505.03096)
*Joshua Owotogbe*

Main category: cs.MA

TLDR: 研究提出了一种混沌工程框架，用于增强基于大型语言模型的多智能体系统（LLM-MAS）在真实环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LLM-MAS在生产和预生产环境中可能因幻觉、智能体故障和通信故障等问题而脆弱，需要提高其可靠性。

Method: 采用混沌工程框架，主动识别LLM-MAS的脆弱性，并评估和增强其韧性。

Result: 框架能够有效识别和应对LLM-MAS中的潜在问题，确保其在关键应用中的可靠性能。

Conclusion: 混沌工程是提升LLM-MAS鲁棒性的有效方法，适用于真实环境中的部署。

Abstract: This study explores the application of chaos engineering to enhance the
robustness of Large Language Model-Based Multi-Agent Systems (LLM-MAS) in
production-like environments under real-world conditions. LLM-MAS can
potentially improve a wide range of tasks, from answering questions and
generating content to automating customer support and improving decision-making
processes. However, LLM-MAS in production or preproduction environments can be
vulnerable to emergent errors or disruptions, such as hallucinations, agent
failures, and agent communication failures. This study proposes a chaos
engineering framework to proactively identify such vulnerabilities in LLM-MAS,
assess and build resilience against them, and ensure reliable performance in
critical applications.

</details>

### [233] [Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation](https://arxiv.org/abs/2505.03586)
*Songchen Fu,Siang Chen,Shaojing Zhao,Letian Bai,Ta Li,Yonghong Yan*

Main category: cs.MA

TLDR: 论文提出了一种针对多智能体系统中观测延迟问题的解决方案，通过扩展Dec-POMDP模型，提出了DSID-POMDP和Rainbow Delay Compensation (RDC)框架，有效缓解了延迟对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现实多智能体系统中普遍存在观测延迟问题，导致智能体无法基于真实环境状态做出决策，亟需解决方案。

Method: 扩展标准Dec-POMDP模型为DSID-POMDP，并设计RDC框架及其模块实现，用于处理随机个体延迟问题。

Result: 实验表明，RDC框架显著提升了延迟场景下的性能，部分情况下甚至达到无延迟的理想效果。

Conclusion: 研究为多智能体延迟观测问题提供了新视角和有效解决方案。

Abstract: In real-world multi-agent systems (MASs), observation delays are ubiquitous,
preventing agents from making decisions based on the environment's true state.
An individual agent's local observation often consists of multiple components
from other agents or dynamic entities in the environment. These discrete
observation components with varying delay characteristics pose significant
challenges for multi-agent reinforcement learning (MARL). In this paper, we
first formulate the decentralized stochastic individual delay partially
observable Markov decision process (DSID-POMDP) by extending the standard
Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL
training framework for addressing stochastic individual delays, along with
recommended implementations for its constituent modules. We implement the
DSID-POMDP's observation generation pattern using standard MARL benchmarks,
including MPE and SMAC. Experiments demonstrate that baseline MARL methods
suffer severe performance degradation under fixed and unfixed delays. The
RDC-enhanced approach mitigates this issue, remarkably achieving ideal
delay-free performance in certain delay scenarios while maintaining
generalization capability. Our work provides a novel perspective on multi-agent
delayed observation problems and offers an effective solution framework.

</details>

<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [234] [Allocation of Heterogeneous Resources in General Lotto Games](https://arxiv.org/abs/2505.02860)
*Keith Paarporn,Adel Aghajan,Jason R. Marden*

Main category: econ.TH

TLDR: 本文研究了在竞争性资源分配模型中，如何优化异构资源的分配策略，扩展了传统的单一资源分配模型，考虑了资源类型组合对结果的影响。


<details>
  <summary>Details</summary>
Motivation: 在存在战略对手的情况下，资源分配对系统目标和任务的完成至关重要。随着资源类型多样化，优化分配策略变得更为复杂。

Method: 提出了多资源扩展模型，考虑了资源类型组合对竞争结果的影响，并分析了两种不同的获胜规则：最弱链接/最佳射击和加权线性组合。

Result: 完全刻画了两种规则下的均衡收益和策略，并进一步研究了资源类型成本对均衡投资的影响。

Conclusion: 研究为异构资源分配提供了新的理论框架，扩展了传统模型的适用性。

Abstract: The allocation of resources plays an important role in the completion of
system objectives and tasks, especially in the presence of strategic
adversaries. Optimal allocation strategies are becoming increasingly more
complex, given that multiple heterogeneous types of resources are at a system
planner's disposal. In this paper, we focus on deriving optimal strategies for
the allocation of heterogeneous resources in a well-known competitive resource
allocation model known as the General Lotto game. In standard formulations,
outcomes are determined solely by the players' allocation strategies of a
common, single type of resource across multiple contests. In particular, a
player wins a contest if it sends more resources than the opponent. Here, we
propose a multi-resource extension where the winner of a contest is now
determined not only by the amount of resources allocated, but also by the
composition of resource types that are allocated. We completely characterize
the equilibrium payoffs and strategies for two distinct formulations. The first
consists of a weakest-link/best-shot winning rule, and the second considers a
winning rule based on a weighted linear combination of the allocated resources.
We then consider a scenario where the resource types are costly to purchase,
and derive the players' equilibrium investments in each of the resource types.

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [235] [Snakemaker: Seamlessly transforming ad-hoc analyses into sustainable Snakemake workflows with generative AI](https://arxiv.org/abs/2505.02841)
*Marco Masera,Alessandro Leone,Johannes Köster,Ivan Molineris*

Main category: cs.SE

TLDR: Snakemaker利用生成式AI将非结构化代码转换为Snakemake工作流，提升生物信息学软件的可重复性和可持续性。


<details>
  <summary>Details</summary>
Motivation: 生物信息学软件开发中，工具快速演变和复杂工作流导致管道难以适应或短命，亟需解决方案。

Method: Snakemaker通过跟踪终端操作、分析执行模式，生成符合最佳实践的Snakemake工作流，并支持将Ipython Notebook转换为模块化管道。

Result: Snakemaker生成高质量工作流，支持Conda环境跟踪、通用规则生成等，提升代码从原型到生产的过渡效率。

Conclusion: Snakemaker降低了生物信息学研究中的计算可重复性障碍，填补了关键空白。

Abstract: Reproducibility and sustainability present significant challenges in
bioinformatics software development, where rapidly evolving tools and complex
workflows often result in short-lived or difficult-to-adapt pipelines. This
paper introduces Snakemaker, a tool that leverages generative AI to facilitate
researchers build sustainable data analysis pipelines by converting
unstructured code into well-defined Snakemake workflows. Snakemaker
non-invasively tracks the work performed in the terminal by the researcher,
analyzes execution patterns, and generates Snakemake workflows that can be
integrated into existing pipelines. Snakemaker also supports the transformation
of monolithic Ipython Notebooks into modular Snakemake pipelines, resolving the
global state of the notebook into discrete, file-based interactions between
rules. An integrated chat assistant provides users with fine-grained control
through natural language instructions. Snakemaker generates high-quality
Snakemake workflows by adhering to the best practices, including Conda
environment tracking, generic rule generation and loop unrolling. By lowering
the barrier between prototype and production-quality code, Snakemaker addresses
a critical gap in computational reproducibility for bioinformatics research.

</details>

### [236] [The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models](https://arxiv.org/abs/2505.02931)
*Fernando Vallecillos Ruiz,Max Hort,Leon Moonen*

Main category: cs.SE

TLDR: 研究探讨了自动程序修复（APR）中平衡多输出生成与多轮迭代的策略，限制每个错误生成10个补丁。通过微调三种LLM模型，发现少量数据微调可显著提升修复效果，但过度微调会导致收益递减。迭代策略对基础模型效果显著，复杂任务中尤为突出。


<details>
  <summary>Details</summary>
Motivation: 自动程序修复（APR）旨在减少手动修复代码错误的成本。传统方法依赖大量补丁生成，而LLM的自我迭代能力提供了新思路，但文献中常忽视输出数量与迭代次数的平衡。

Method: 研究设计了一个APR流程，结合多输出生成与多轮迭代，限制每个错误生成10个补丁。使用三种指令调优的LLM模型（DeepSeekCoder-Instruct、Codellama-Instruct、Llama3.1-Instruct），并在不同规模（1K、30K、65K）和两种技术（全微调与LoRA）下微调，评估其在HumanEval-Java和Defects4J基准上的表现。

Result: 少量数据微调（<1%）可将可信补丁数量提升78%，但过度微调会导致收益递减。基础模型通过迭代生成补丁效果显著，复杂任务中优势更明显。微调模型虽受益较少，但在复杂任务中仍有提升。

Conclusion: 研究强调了平衡多输出生成与迭代优化的APR策略的重要性，少量数据微调和迭代策略能显著提升修复效果，尤其在复杂任务中。

Abstract: Automatic program repair (APR) aims to reduce the manual efforts required to
identify and fix errors in source code. Before the rise of LLM-based agents, a
common strategy was to increase the number of generated patches, sometimes to
the thousands, to achieve better repair results on benchmarks. More recently,
self-iterative capabilities enabled LLMs to refine patches over multiple rounds
guided by feedback. However, literature often focuses on many iterations and
disregards different numbers of outputs.
  We investigate an APR pipeline that balances these two approaches, the
generation of multiple outputs and multiple rounds of iteration, while imposing
a limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs
- DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct - to the APR
task. We further fine-tune each model on an APR dataset with three sizes (1K,
30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess
their repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.
  Our results show that by using only a fraction (<1%) of the fine-tuning
dataset, we can achieve improvements of up to 78% in the number of plausible
patches generated, challenging prior studies that reported limited gains using
Full Fine-Tuning. However, we find that exceeding certain thresholds leads to
diminishing outcomes, likely due to overfitting. Moreover, we show that base
models greatly benefit from creating patches in an iterative fashion rather
than generating them all at once. In addition, the benefit of iterative
strategies becomes more pronounced in complex benchmarks. Even fine-tuned
models, while benefiting less from iterations, still gain advantages,
particularly on complex benchmarks. The research underscores the need for
balanced APR strategies that combine multi-output generation and iterative
refinement.

</details>

### [237] [DocSpiral: A Platform for Integrated Assistive Document Annotation through Human-in-the-Spiral](https://arxiv.org/abs/2505.03214)
*Qiang Sun,Sirui Li,Tingting Bi,Du Huynh,Mark Reynolds,Yuanyi Luo,Wei Liu*

Main category: cs.SE

TLDR: DocSpiral是一个辅助文档标注平台，通过人机协作迭代训练模型，减少人工标注时间，提升文档结构化信息提取效率。


<details>
  <summary>Details</summary>
Motivation: 解决从图像文档中提取结构化数据的挑战，减少人工标注需求。

Method: 采用螺旋设计，结合文档格式标准化、标注界面、评估指标和API端点，形成统一工作流。

Result: 实验显示标注时间减少至少41%，模型性能持续提升。

Conclusion: DocSpiral通过免费开放平台，降低AI/ML模型开发门槛，推动图像文档处理领域的应用。

Abstract: Acquiring structured data from domain-specific, image-based documents such as
scanned reports is crucial for many downstream tasks but remains challenging
due to document variability. Many of these documents exist as images rather
than as machine-readable text, which requires human annotation to train
automated extraction systems. We present DocSpiral, the first
Human-in-the-Spiral assistive document annotation platform, designed to address
the challenge of extracting structured information from domain-specific,
image-based document collections. Our spiral design establishes an iterative
cycle in which human annotations train models that progressively require less
manual intervention. DocSpiral integrates document format normalization,
comprehensive annotation interfaces, evaluation metrics dashboard, and API
endpoints for the development of AI / ML models into a unified workflow.
Experiments demonstrate that our framework reduces annotation time by at least
41\% while showing consistent performance gains across three iterations during
model training. By making this annotation platform freely accessible, we aim to
lower barriers to AI/ML models development in document processing, facilitating
the adoption of large language models in image-based, document-intensive fields
such as geoscience and healthcare. The system is freely available at:
https://app.ai4wa.com. The demonstration video is available:
https://app.ai4wa.com/docs/docspiral/demo.

</details>

### [238] [Synthline: A Product Line Approach for Synthetic Requirements Engineering Data Generation using Large Language Models](https://arxiv.org/abs/2505.03265)
*Abdelkarim El-Hajjami,Camille Salinesi*

Main category: cs.SE

TLDR: Synthline利用大型语言模型生成合成RE数据，解决高质量数据集稀缺问题。实验表明，合成数据虽多样性不足，但可作为有效训练资源，且与真实数据结合能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代需求工程（RE）依赖自然语言处理和机器学习技术，但高质量数据集稀缺限制了其效果。

Method: 提出Synthline，一种产品线方法，利用大型语言模型生成合成RE数据，并通过实验评估其多样性和实用性。

Result: 合成数据多样性不如真实数据，但可作为训练资源；与真实数据结合能显著提升性能（精度提高85%，召回率翻倍）。

Conclusion: 基于产品线的合成数据生成有望解决RE中的数据稀缺问题，研究公开了实现和数据集以支持复现和领域发展。

Abstract: While modern Requirements Engineering (RE) heavily relies on natural language
processing and Machine Learning (ML) techniques, their effectiveness is limited
by the scarcity of high-quality datasets. This paper introduces Synthline, a
Product Line (PL) approach that leverages Large Language Models to
systematically generate synthetic RE data for classification-based use cases.
Through an empirical evaluation conducted in the context of using ML for the
identification of requirements specification defects, we investigated both the
diversity of the generated data and its utility for training downstream models.
Our analysis reveals that while synthetic datasets exhibit less diversity than
real data, they are good enough to serve as viable training resources.
Moreover, our evaluation shows that combining synthetic and real data leads to
substantial performance improvements. Specifically, hybrid approaches achieve
up to 85% improvement in precision and a 2x increase in recall compared to
models trained exclusively on real data. These findings demonstrate the
potential of PL-based synthetic data generation to address data scarcity in RE.
We make both our implementation and generated datasets publicly available to
support reproducibility and advancement in the field.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [239] [GeoERM: Geometry-Aware Multi-Task Representation Learning on Riemannian Manifolds](https://arxiv.org/abs/2505.02972)
*Aoran Chen,Yang Feng*

Main category: stat.ML

TLDR: GeoERM是一种几何感知的多任务学习框架，通过将共享表示嵌入到其自然的黎曼流形上并优化，提高了任务异构或对抗时的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统多任务学习方法忽略潜在表示矩阵的非欧几何特性，导致在任务异构或对抗时鲁棒性不足。

Method: 提出GeoERM框架，通过黎曼梯度步和极坐标回缩操作优化共享表示，保持几何保真度。

Result: 在合成实验和可穿戴传感器活动识别基准测试中，GeoERM提高了估计精度，减少了负迁移，并在对抗性标签噪声下保持稳定。

Conclusion: GeoERM在多任务学习中表现出色，优于现有方法和单任务学习。

Abstract: Multi-Task Learning (MTL) seeks to boost statistical power and learning
efficiency by discovering structure shared across related tasks.
State-of-the-art MTL representation methods, however, usually treat the latent
representation matrix as a point in ordinary Euclidean space, ignoring its
often non-Euclidean geometry, thus sacrificing robustness when tasks are
heterogeneous or even adversarial. We propose GeoERM, a geometry-aware MTL
framework that embeds the shared representation on its natural Riemannian
manifold and optimizes it via explicit manifold operations. Each training cycle
performs (i) a Riemannian gradient step that respects the intrinsic curvature
of the search space, followed by (ii) an efficient polar retraction to remain
on the manifold, guaranteeing geometric fidelity at every iteration. The
procedure applies to a broad class of matrix-factorized MTL models and retains
the same per-iteration cost as Euclidean baselines. Across a set of synthetic
experiments with task heterogeneity and on a wearable-sensor
activity-recognition benchmark, GeoERM consistently improves estimation
accuracy, reduces negative transfer, and remains stable under adversarial label
noise, outperforming leading MTL and single-task alternatives.

</details>

### [240] [Modeling Spatial Extremes using Non-Gaussian Spatial Autoregressive Models via Convolutional Neural Networks](https://arxiv.org/abs/2505.03034)
*Sweta Rai,Douglas W. Nychka,Soutir Bandyopadhyay*

Main category: stat.ML

TLDR: 提出了一种基于空间自回归模型的框架，用于处理具有空间异质性和重尾分布的网格数据，结合广义极值分布和卷积神经网络实现快速参数估计。


<details>
  <summary>Details</summary>
Motivation: 解决网格数据中空间异质性和重尾分布带来的建模挑战，尤其是填补缺失网格或模拟极端空间行为的需求。

Method: 采用空间自回归模型（SAR）结合广义极值分布创新项，利用卷积神经网络进行快速参数估计。

Result: 模型能够有效捕捉极端空间行为，并通过神经网络实现快速参数估计。

Conclusion: 该框架为处理非高斯场和极端空间行为提供了一种灵活且高效的方法，适用于气象等领域的空间数据分析。

Abstract: Data derived from remote sensing or numerical simulations often have a
regular gridded structure and are large in volume, making it challenging to
find accurate spatial models that can fill in missing grid cells or simulate
the process effectively, especially in the presence of spatial heterogeneity
and heavy-tailed marginal distributions. To overcome this issue, we present a
spatial autoregressive modeling framework, which maps observations at a
location and its neighbors to independent random variables. This is a highly
flexible modeling approach and well-suited for non-Gaussian fields, providing
simpler interpretability. In particular, we consider the SAR model with
Generalized Extreme Value distribution innovations to combine the observation
at a central grid location with its neighbors, capturing extreme spatial
behavior based on the heavy-tailed innovations. While these models are fast to
simulate by exploiting the sparsity of the key matrices in the computations,
the maximum likelihood estimation of the parameters is prohibitive due to the
intractability of the likelihood, making optimization challenging. To overcome
this, we train a convolutional neural network on a large training set that
covers a useful parameter space, and then use the trained network for fast
parameter estimation. Finally, we apply this model to analyze annual maximum
precipitation data from ERA-Interim-driven Weather Research and Forecasting
(WRF) simulations, allowing us to explore its spatial extreme behavior across
North America.

</details>

### [241] [A Symbolic and Statistical Learning Framework to Discover Bioprocessing Regulatory Mechanism: Cell Culture Example](https://arxiv.org/abs/2505.03177)
*Keilung Choy,Wei Xie,Keqi Wang*

Main category: stat.ML

TLDR: 提出了一种结合符号和统计学习的框架，用于识别生物过程中的关键调控机制并量化模型不确定性，提高了样本效率和模型选择鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 生物过程机理建模对生物制造的智能数字孪生至关重要，但由于复杂的细胞内调控、随机系统行为和有限的实验数据，仍存在挑战。

Method: 使用随机微分方程描述生物过程动态，并通过贝叶斯学习方法联合学习动力学参数和调控结构，采用Metropolis-adjusted Langevin算法提高计算效率。

Result: 与现有贝叶斯推断方法相比，该框架在样本效率和模型选择鲁棒性上表现更优，能够恢复缺失的调控机制并提升数据有限条件下的模型保真度。

Conclusion: 该框架为生物过程建模提供了一种高效且鲁棒的方法，特别适用于数据有限的情况。

Abstract: Bioprocess mechanistic modeling is essential for advancing intelligent
digital twin representation of biomanufacturing, yet challenges persist due to
complex intracellular regulation, stochastic system behavior, and limited
experimental data. This paper introduces a symbolic and statistical learning
framework to identify key regulatory mechanisms and quantify model uncertainty.
Bioprocess dynamics is formulated with stochastic differential equations
characterizing intrinsic process variability, with a predefined set of
candidate regulatory mechanisms constructed from biological knowledge. A
Bayesian learning approach is developed, which is based on a joint learning of
kinetic parameters and regulatory structure through a formulation of the
mixture model. To enhance computational efficiency, a Metropolis-adjusted
Langevin algorithm with adjoint sensitivity analysis is developed for posterior
exploration. Compared to state-of-the-art Bayesian inference approaches, the
proposed framework achieves improved sample efficiency and robust model
selection. An empirical study demonstrates its ability to recover missing
regulatory mechanisms and improve model fidelity under data-limited conditions.

</details>

### [242] [Weighted Average Gradients for Feature Attribution](https://arxiv.org/abs/2505.03201)
*Kien Tran Duc Tuan,Tam Nguyen Trong,Son Nguyen Hoang,Khoat Than,Anh Nguyen Duc*

Main category: stat.ML

TLDR: 本文提出了一种新的加权平均梯度（WG）方法，用于改进解释性AI中的基线选择，优于传统的期望梯度（EG）方法，性能提升10-35%，并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统EG方法假设基线可以均匀采样并等权平均，但实际中基线不应等同对待，需评估其适用性。

Method: 提出WG方法，通过无监督评估基线适用性并选择有效基线，满足解释方法标准且更稳定。

Result: WG在多种场景下优于EG，性能提升10-35%，同时减少计算成本。

Conclusion: WG通过优化基线选择，显著提升解释性AI的效果和效率。

Abstract: In explainable AI, Integrated Gradients (IG) is a widely adopted technique
for assessing the significance of feature attributes of the input on model
outputs by evaluating contributions from a baseline input to the current input.
The choice of the baseline input significantly influences the resulting
explanation. While the traditional Expected Gradients (EG) method assumes
baselines can be uniformly sampled and averaged with equal weights, this study
argues that baselines should not be treated equivalently. We introduce Weighted
Average Gradients (WG), a novel approach that unsupervisedly evaluates baseline
suitability and incorporates a strategy for selecting effective baselines.
Theoretical analysis demonstrates that WG satisfies essential explanation
method criteria and offers greater stability than prior approaches.
Experimental results further confirm that WG outperforms EG across diverse
scenarios, achieving an improvement of 10-35\% on main metrics. Moreover, by
evaluating baselines, our method can filter a subset of effective baselines for
each input to calculate explanations, maintaining high accuracy while reducing
computational cost. The code is available at:
https://github.com/Tamnt240904/weighted_baseline.

</details>

### [243] [Lower Bounds for Greedy Teaching Set Constructions](https://arxiv.org/abs/2505.03223)
*Spencer Compton,Chirag Pabbaraju,Nikita Zhivotovskiy*

Main category: stat.ML

TLDR: 论文研究了学习理论中概念类的最小教学维度问题，针对贪婪算法的性能提出了下界分析，特别是对小k值的情况。


<details>
  <summary>Details</summary>
Motivation: 解决概念类的最小教学维度问题，验证Simon和Zilles提出的递归教学维度上界猜想。

Method: 通过分析贪婪算法在小k值下的性能，提出下界证明。

Result: 证明对于k=1，贪婪算法无法超越基于对分的O(log(|C|))界；对于k=2，给出了与上界匹配的下界；并扩展至k≤⌈cd⌉。

Conclusion: 研究表明，可能需要研究更高阶的交互才能解决最小教学维度为O(d)的猜想。

Abstract: A fundamental open problem in learning theory is to characterize the
best-case teaching dimension $\operatorname{TS}_{\min}$ of a concept class
$\mathcal{C}$ with finite VC dimension $d$. Resolving this problem will, in
particular, settle the conjectured upper bound on Recursive Teaching Dimension
posed by [Simon and Zilles; COLT 2015]. Prior work used a natural greedy
algorithm to construct teaching sets recursively, thereby proving upper bounds
on $\operatorname{TS}_{\min}$, with the best known bound being $O(d^2)$ [Hu,
Wu, Li, and Wang; COLT 2017]. In each iteration, this greedy algorithm chooses
to add to the teaching set the $k$ labeled points that restrict the concept
class the most. In this work, we prove lower bounds on the performance of this
greedy approach for small $k$. Specifically, we show that for $k = 1$, the
algorithm does not improve upon the halving-based bound of
$O(\log(|\mathcal{C}|))$. Furthermore, for $k = 2$, we complement the upper
bound of $O\left(\log(\log(|\mathcal{C}|))\right)$ from [Moran, Shpilka,
Wigderson, and Yuhudayoff; FOCS 2015] with a matching lower bound. Most
consequentially, our lower bound extends up to $k \le \lceil c d \rceil$ for
small constant $c>0$: suggesting that studying higher-order interactions may be
necessary to resolve the conjecture that $\operatorname{TS}_{\min} = O(d)$.

</details>

### [244] [Decision Making under Model Misspecification: DRO with Robust Bayesian Ambiguity Sets](https://arxiv.org/abs/2505.03585)
*Charita Dellaporta,Patrick O'Hara,Theodoros Damoulas*

Main category: stat.ML

TLDR: DRO-RoBAS结合鲁棒贝叶斯方法，解决模型误设问题，提升分布鲁棒优化的性能。


<details>
  <summary>Details</summary>
Motivation: 传统DRO方法在模型误设时过于保守，需扩展模糊集以包含真实数据生成过程。

Method: 引入基于最大均值差异的模糊集，以鲁棒后验预测分布为中心，结合DGP信念。

Result: 在Nevsvendor和Portfolio问题上，DRO-RoBAS优于其他贝叶斯和实证DRO方法。

Conclusion: DRO-RoBAS通过鲁棒贝叶斯模糊集有效应对模型误设，提升决策性能。

Abstract: Distributionally Robust Optimisation (DRO) protects risk-averse
decision-makers by considering the worst-case risk within an ambiguity set of
distributions based on the empirical distribution or a model. To further guard
against finite, noisy data, model-based approaches admit Bayesian formulations
that propagate uncertainty from the posterior to the decision-making problem.
However, when the model is misspecified, the decision maker must stretch the
ambiguity set to contain the data-generating process (DGP), leading to overly
conservative decisions. We address this challenge by introducing DRO with
Robust, to model misspecification, Bayesian Ambiguity Sets (DRO-RoBAS). These
are Maximum Mean Discrepancy ambiguity sets centred at a robust posterior
predictive distribution that incorporates beliefs about the DGP. We show that
the resulting optimisation problem obtains a dual formulation in the
Reproducing Kernel Hilbert Space and we give probabilistic guarantees on the
tolerance level of the ambiguity set. Our method outperforms other Bayesian and
empirical DRO approaches in out-of-sample performance on the Newsvendor and
Portfolio problems with various cases of model misspecification.

</details>

### [245] [Physics-Informed Sylvester Normalizing Flows for Bayesian Inference in Magnetic Resonance Spectroscopy](https://arxiv.org/abs/2505.03590)
*Julian P. Merkofer,Dennis M. J. van de Sande,Alex A. Bhogal,Ruud J. G. van Sloun*

Main category: stat.ML

TLDR: 提出一种基于贝叶斯推断和Sylvester归一化流的框架，用于提高磁共振波谱（MRS）中代谢物定量的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统方法如线性组合模型在代谢物定量中存在模糊性和精度限制，需要更可靠的方法。

Method: 使用Sylvester归一化流（SNFs）近似代谢物浓度的后验分布，并结合物理知识构建解码器。

Result: 在模拟7T质子MRS数据上验证，显示准确的代谢物定量、校准良好的不确定性及参数相关性分析。

Conclusion: 该方法提高了MRS代谢物定量的可靠性，并提供了参数分布的多模态分析。

Abstract: Magnetic resonance spectroscopy (MRS) is a non-invasive technique to measure
the metabolic composition of tissues, offering valuable insights into
neurological disorders, tumor detection, and other metabolic dysfunctions.
However, accurate metabolite quantification is hindered by challenges such as
spectral overlap, low signal-to-noise ratio, and various artifacts. Traditional
methods like linear-combination modeling are susceptible to ambiguities and
commonly only provide a theoretical lower bound on estimation accuracy in the
form of the Cram\'er-Rao bound. This work introduces a Bayesian inference
framework using Sylvester normalizing flows (SNFs) to approximate posterior
distributions over metabolite concentrations, enhancing quantification
reliability. A physics-based decoder incorporates prior knowledge of MRS signal
formation, ensuring realistic distribution representations. We validate the
method on simulated 7T proton MRS data, demonstrating accurate metabolite
quantification, well-calibrated uncertainties, and insights into parameter
correlations and multi-modal distributions.

</details>

### [246] [Weighted Random Dot Product Graphs](https://arxiv.org/abs/2505.03649)
*Bernardo Marenco,Paola Bermolen,Marcelo Fiori,Federico Larroca,Gonzalo Mateos*

Main category: stat.ML

TLDR: 本文扩展了随机点积图（RDPG）模型，提出了非参数加权（W）RDPG模型，适用于具有异质权重分布的加权图。


<details>
  <summary>Details</summary>
Motivation: 网络数据的复杂关系模式分析是现代统计研究和数据科学的核心，但现有模型难以区分具有相同均值但高阶矩不同的权重分布。

Method: 提出WRDPG模型，通过节点潜在位置的内积定义边权分布的矩生成函数，并推导了节点潜在位置估计器的统计保证。

Result: 证明了估计器的一致性和渐近正态性，并提供了生成加权图的框架。

Conclusion: WRDPG模型在多种网络分析应用中表现出色，扩展了RDPG的适用范围。

Abstract: Modeling of intricate relational patterns % through the analysis structures
of network data has become a cornerstone of contemporary statistical research
and related data science fields. Networks, represented as graphs, offer a
natural framework for this analysis. This paper extends the Random Dot Product
Graph (RDPG) model to accommodate weighted graphs, markedly broadening the
model's scope to scenarios where edges exhibit heterogeneous weight
distributions. We propose a nonparametric weighted (W)RDPG model that assigns a
sequence of latent positions to each node. Inner products of these nodal
vectors specify the moments of their incident edge weights' distribution via
moment-generating functions. In this way, and unlike prior art, the WRDPG can
discriminate between weight distributions that share the same mean but differ
in other higher-order moments. We derive statistical guarantees for an
estimator of the nodal's latent positions adapted from the workhorse adjacency
spectral embedding, establishing its consistency and asymptotic normality. We
also contribute a generative framework that enables sampling of graphs that
adhere to a (prescribed or data-fitted) WRDPG, facilitating, e.g., the analysis
and testing of observed graph metrics using judicious reference distributions.
The paper is organized to formalize the model's definition, the estimation (or
nodal embedding) process and its guarantees, as well as the methodologies for
generating weighted graphs, all complemented by illustrative and reproducible
examples showcasing the WRDPG's effectiveness in various network analytic
applications.

</details>

### [247] [Multi-modal cascade feature transfer for polymer property prediction](https://arxiv.org/abs/2505.03704)
*Kiichi Obuchi,Yuta Yahagi,Kiyohiko Toyama,Shukichi Tanaka,Kota Matsui*

Main category: stat.ML

TLDR: 提出了一种多模态级联模型，结合图卷积神经网络（GCN）提取的化学结构特征与其他特征，用于聚合物性能预测，性能优于传统单特征方法。


<details>
  <summary>Details</summary>
Motivation: 聚合物数据通常包含多种格式（如分子描述符、添加剂信息等），传统方法单独使用这些数据，限制了预测精度。

Method: 结合GCN提取的化学结构特征与其他特征（如分子描述符、添加剂信息），构建多模态级联模型。

Result: 在多个聚合物数据集上验证，预测性能优于传统单特征方法。

Conclusion: 多模态特征结合显著提升了聚合物性能预测的准确性。

Abstract: In this paper, we propose a novel transfer learning approach called
multi-modal cascade model with feature transfer for polymer property
prediction.Polymers are characterized by a composite of data in several
different formats, including molecular descriptors and additive information as
well as chemical structures. However, in conventional approaches, prediction
models were often constructed using each type of data separately. Our model
enables more accurate prediction of physical properties for polymers by
combining features extracted from the chemical structure by graph convolutional
neural networks (GCN) with features such as molecular descriptors and additive
information. The predictive performance of the proposed method is empirically
evaluated using several polymer datasets. We report that the proposed method
shows high predictive performance compared to the baseline conventional
approach using a single feature.

</details>

### [248] [Actor-Critics Can Achieve Optimal Sample Efficiency](https://arxiv.org/abs/2505.03710)
*Kevin Tan,Wei Fan,Yuting Wei*

Main category: stat.ML

TLDR: 提出一种新型actor-critic算法，解决了现有方法在一般函数逼近下无法达到$O(1/\epsilon^2)$样本复杂度的问题，并在混合RL中展示了离线数据的优势。


<details>
  <summary>Details</summary>
Motivation: 解决现有actor-critic算法在一般函数逼近和战略探索下无法高效学习$\epsilon$-最优策略的问题。

Method: 结合乐观策略、离策略critic估计和罕见切换策略重置，提出新算法。

Result: 达到$O(dH^5 \log|\mathcal{A}|/\epsilon^2 + d H^4 \log|\mathcal{F}|/ \epsilon^2)$样本复杂度，并在混合RL中利用离线数据提升效率。

Conclusion: 新算法填补了理论空白，并通过实验验证了其有效性。

Abstract: Actor-critic algorithms have become a cornerstone in reinforcement learning
(RL), leveraging the strengths of both policy-based and value-based methods.
Despite recent progress in understanding their statistical efficiency, no
existing work has successfully learned an $\epsilon$-optimal policy with a
sample complexity of $O(1/\epsilon^2)$ trajectories with general function
approximation when strategic exploration is necessary.
  We address this open problem by introducing a novel actor-critic algorithm
that attains a sample-complexity of $O(dH^5 \log|\mathcal{A}|/\epsilon^2 + d
H^4 \log|\mathcal{F}|/ \epsilon^2)$ trajectories, and accompanying $\sqrt{T}$
regret when the Bellman eluder dimension $d$ does not increase with $T$ at more
than a $\log T$ rate.
  Here, $\mathcal{F}$ is the critic function class, $\mathcal{A}$ is the action
space, and $H$ is the horizon in the finite horizon MDP setting. Our algorithm
integrates optimism, off-policy critic estimation targeting the optimal
Q-function, and rare-switching policy resets.
  We extend this to the setting of Hybrid RL, showing that initializing the
critic with offline data yields sample efficiency gains compared to purely
offline or online RL. Further, utilizing access to offline data, we provide a
\textit{non-optimistic} provably efficient actor-critic algorithm that only
additionally requires $N_{\text{off}} \geq c_{\text{off}}^*dH^4/\epsilon^2$ in
exchange for omitting optimism, where $c_{\text{off}}^*$ is the single-policy
concentrability coefficient and $N_{\text{off}}$ is the number of offline
samples. This addresses another open problem in the literature. We further
provide numerical experiments to support our theoretical findings.

</details>

<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [249] [HMAE: Self-Supervised Few-Shot Learning for Quantum Spin Systems](https://arxiv.org/abs/2505.03140)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: quant-ph

TLDR: HMAE是一种自监督框架，通过物理信息掩码预训练变换器，显著提升量子系统的少样本学习效率。


<details>
  <summary>Details</summary>
Motivation: 解决量子机器学习中标记数据稀缺和模拟计算成本高的问题。

Method: 采用基于量子信息理论的物理信息掩码策略，预训练变换器。

Result: 在12,500个量子哈密顿量上，HMAE在相分类和基态能量预测中表现优于基线方法。

Conclusion: HMAE在小量子系统中表现出色，但规模限制阻碍了其在大系统中的应用。

Abstract: Quantum machine learning for spin and molecular systems faces critical
challenges of scarce labeled data and computationally expensive simulations. To
address these limitations, we introduce Hamiltonian-Masked Autoencoding (HMAE),
a novel self-supervised framework that pre-trains transformers on unlabeled
quantum Hamiltonians, enabling efficient few-shot transfer learning. Unlike
random masking approaches, HMAE employs a physics-informed strategy based on
quantum information theory to selectively mask Hamiltonian terms based on their
physical significance. Experiments on 12,500 quantum Hamiltonians (60%
real-world, 40% synthetic) demonstrate that HMAE achieves 85.3% $\pm$ 1.5%
accuracy in phase classification and 0.15 $\pm$ 0.02 eV MAE in ground state
energy prediction with merely 10 labeled examples - a statistically significant
improvement (p < 0.01) over classical graph neural networks (78.1% $\pm$ 2.1%)
and quantum neural networks (76.8% $\pm$ 2.3%). Our method's primary advantage
is exceptional sample efficiency - reducing required labeled examples by 3-5x
compared to baseline methods - though we emphasize that ground truth values for
fine-tuning and evaluation still require exact diagonalization or tensor
networks. We explicitly acknowledge that our current approach is limited to
small quantum systems (specifically limited to 12 qubits during training, with
limited extension to 16-20 qubits in testing) and that, while promising within
this regime, this size restriction prevents immediate application to larger
systems of practical interest in materials science and quantum chemistry.

</details>

### [250] [Quantum Feature Space of a Qubit Coupled to an Arbitrary Bath](https://arxiv.org/abs/2505.03397)
*Chris Wise,Akram Youssry,Alberto Peruzzo,Jo Plested,Matt Woolley*

Main category: quant-ph

TLDR: 传统量子比特控制协议依赖功率谱密度表征量子比特-浴耦合。本文提出了一种高效参数化方法，无需复杂神经网络，通过量子特征空间分类噪声过程。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖复杂神经网络和物理编码层，难以扩展和实时操作。本文旨在简化噪声操作符描述。

Method: 提出量子特征空间参数化方法，利用欧几里得距离分类噪声过程，并结合随机森林算法验证其有效性。

Result: 量子特征空间能有效分类噪声过程的平稳性和类型，并探索控制脉冲参数与特征空间的映射关系。

Conclusion: 量子特征空间提供了一种高效且简化的噪声分类方法，优于传统复杂神经网络方案。

Abstract: Qubit control protocols have traditionally leveraged a characterisation of
the qubit-bath coupling via its power spectral density. Previous work proposed
the inference of noise operators that characterise the influence of a classical
bath using a grey-box approach that combines deep neural networks with
physics-encoded layers. This overall structure is complex and poses challenges
in scaling and real-time operations. Here, we show that no expensive neural
networks are needed and that this noise operator description admits an
efficient parameterisation. We refer to the resulting parameter space as the
\textit{quantum feature space} of the qubit dynamics resulting from the coupled
bath. We show that the Euclidean distance defined over the quantum feature
space provides an effective method for classifying noise processes in the
presence of a given set of controls. Using the quantum feature space as the
input space for a simple machine learning algorithm (random forest, in this
case), we demonstrate that it can effectively classify the stationarity and the
broad class of noise processes perturbing a qubit. Finally, we explore how
control pulse parameters map to the quantum feature space.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [251] [Physical foundations for trustworthy medical imaging: a review for artificial intelligence researchers](https://arxiv.org/abs/2505.02843)
*Miriam Cobo,David Corral Fontecha,Wilson Silva,Lara Lloret Iglesias*

Main category: eess.IV

TLDR: 论文探讨了医学影像中人工智能的发展，强调了物理知识对提升AI算法可信度和鲁棒性的重要性，并回顾了物理原理在生成模型和重建算法中的应用。


<details>
  <summary>Details</summary>
Motivation: 由于AI开发者对医学影像物理原理的理解不足，限制了AI在医学影像中的潜力发挥，因此需要将物理知识整合到AI算法中。

Method: 回顾医学影像的物理基础及其对AI技术的影响，特别是生成模型和重建算法，并探索物理启发的机器学习模型。

Result: 物理知识的整合可以增强AI在医学影像中的表现，尤其是在数据有限的情况下。

Conclusion: 物理启发的机器学习模型能够通过物理约束提升医学影像特征的学习效果，为AI在医学影像中的应用提供了新方向。

Abstract: Artificial intelligence in medical imaging has seen unprecedented growth in
the last years, due to rapid advances in deep learning and computing resources.
Applications cover the full range of existing medical imaging modalities, with
unique characteristics driven by the physics of each technique. Yet, artificial
intelligence professionals entering the field, and even experienced developers,
often lack a comprehensive understanding of the physical principles underlying
medical image acquisition, which hinders their ability to fully leverage its
potential. The integration of physics knowledge into artificial intelligence
algorithms enhances their trustworthiness and robustness in medical imaging,
especially in scenarios with limited data availability. In this work, we review
the fundamentals of physics in medical images and their impact on the latest
advances in artificial intelligence, particularly, in generative models and
reconstruction algorithms. Finally, we explore the integration of physics
knowledge into physics-inspired machine learning models, which leverage
physics-based constraints to enhance the learning of medical imaging features.

</details>

### [252] [Dual Prompting for Diverse Count-level PET Denoising](https://arxiv.org/abs/2505.03037)
*Xiaofeng Liu,Yongsong Huang,Thibault Marin,Samira Vafay Eslahi,Tiss Amal,Yanis Chemli,Keith Johnson,Georges El Fakhri,Jinsong Ouyang*

Main category: eess.IV

TLDR: 提出了一种基于提示学习的PET去噪方法，通过双提示（显式计数级提示和隐式通用去噪提示）动态指导不同计数水平的去噪过程，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: PET图像去噪面临不同计数水平的挑战，传统方法难以统一处理。

Method: 采用双提示学习策略，结合显式计数级提示和隐式通用去噪提示，并通过提示融合模块和提示-特征交互模块动态指导去噪。

Result: 在1940个低计数PET 3D体积上验证，双提示方法显著优于计数条件模型。

Conclusion: 双提示学习为PET去噪提供了一种高效且通用的解决方案。

Abstract: The to-be-denoised positron emission tomography (PET) volumes are inherent
with diverse count levels, which imposes challenges for a unified model to
tackle varied cases. In this work, we resort to the recently flourished prompt
learning to achieve generalizable PET denoising with different count levels.
Specifically, we propose dual prompts to guide the PET denoising in a
divide-and-conquer manner, i.e., an explicitly count-level prompt to provide
the specific prior information and an implicitly general denoising prompt to
encode the essential PET denoising knowledge. Then, a novel prompt fusion
module is developed to unify the heterogeneous prompts, followed by a
prompt-feature interaction module to inject prompts into the features. The
prompts are able to dynamically guide the noise-conditioned denoising process.
Therefore, we are able to efficiently train a unified denoising model for
various count levels, and deploy it to different cases with personalized
prompts. We evaluated on 1940 low-count PET 3D volumes with uniformly randomly
selected 13-22\% fractions of events from 97 $^{18}$F-MK6240 tau PET studies.
It shows our dual prompting can largely improve the performance with informed
count-level and outperform the count-conditional model.

</details>

### [253] [STG: Spatiotemporal Graph Neural Network with Fusion and Spatiotemporal Decoupling Learning for Prognostic Prediction of Colorectal Cancer Liver Metastasis](https://arxiv.org/abs/2505.03123)
*Yiran Zhu,Wei Yang,Yan su,Zesheng Li,Chengchang Pan,Honggang Qi*

Main category: eess.IV

TLDR: 提出了一种多模态时空图神经网络（STG）框架，用于预测结直肠癌肝转移（CRLM）的进展，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有临床模型未能有效整合肿瘤的空间异质性、动态演化和多模态数据关系，限制了预测准确性。

Method: 结合术前CT成像和临床数据构建异构图结构，通过空间拓扑和跨模态边联合建模肿瘤分布和时序演化，使用GraphSAGE聚合时空邻域信息，并采用监督和对比学习策略。

Result: 在MSKCC CRLM数据集上，时间邻近准确率为85%，平均绝对误差为1.1005，显著优于现有方法。

Conclusion: 该框架通过创新的异构图构建和时空解耦机制，揭示了动态肿瘤微环境变化与预后的关联，为个性化治疗决策提供了可靠支持。

Abstract: We propose a multimodal spatiotemporal graph neural network (STG) framework
to predict colorectal cancer liver metastasis (CRLM) progression. Current
clinical models do not effectively integrate the tumor's spatial heterogeneity,
dynamic evolution, and complex multimodal data relationships, limiting their
predictive accuracy. Our STG framework combines preoperative CT imaging and
clinical data into a heterogeneous graph structure, enabling joint modeling of
tumor distribution and temporal evolution through spatial topology and
cross-modal edges. The framework uses GraphSAGE to aggregate spatiotemporal
neighborhood information and leverages supervised and contrastive learning
strategies to enhance the model's ability to capture temporal features and
improve robustness. A lightweight version of the model reduces parameter count
by 78.55%, maintaining near-state-of-the-art performance. The model jointly
optimizes recurrence risk regression and survival analysis tasks, with
contrastive loss improving feature representational discriminability and
cross-modal consistency. Experimental results on the MSKCC CRLM dataset show a
time-adjacent accuracy of 85% and a mean absolute error of 1.1005,
significantly outperforming existing methods. The innovative heterogeneous
graph construction and spatiotemporal decoupling mechanism effectively uncover
the associations between dynamic tumor microenvironment changes and prognosis,
providing reliable quantitative support for personalized treatment decisions.

</details>

<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [254] [Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories](https://arxiv.org/abs/2505.03443)
*Valerio Bellandi*

Main category: cs.DC

TLDR: 比较集中式与分布式系统的优缺点，并介绍为意大利司法部开发的分布式文档存储系统。


<details>
  <summary>Details</summary>
Motivation: 探讨集中式与分布式系统的适用场景，并展示分布式系统在特定应用中的优势。

Method: 开发一个分布式文档存储系统，利用边缘存储分析文本数据和元数据，提升语义探索能力。

Result: 系统成功应用于意大利司法部，增强了文档管理的可扩展性和容错性。

Conclusion: 分布式系统在需要高可用性和性能的大规模环境中表现优异，而集中式系统适用于控制集中的场景。

Abstract: Centralized and distributed systems are two main approaches to organizing ICT
infrastructure, each with its pros and cons. Centralized systems concentrate
resources in one location, making management easier but creating single points
of failure. Distributed systems, on the other hand, spread resources across
multiple nodes, offering better scalability and fault tolerance, but requiring
more complex management. The choice between them depends on factors like
application needs, scalability, and data sensitivity. Centralized systems suit
applications with limited scalability and centralized control, while
distributed systems excel in large-scale environments requiring high
availability and performance. This paper explores a distributed document
repository system developed for the Italian Ministry of Justice, using edge
repositories to analyze textual data and metadata, enhancing semantic
exploration capabilities.

</details>

<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [255] [New affine invariant ensemble samplers and their dimensional scaling](https://arxiv.org/abs/2505.02987)
*Yifan Chen*

Main category: stat.CO

TLDR: 本文提出了一些新的仿射不变集成采样器，改进了现有算法，尤其在高维问题中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有采样器在高维问题中表现不佳，需要更高效的仿射不变采样方法。

Method: 提出了无导数集成侧移采样器和基于导数的集成哈密顿蒙特卡洛（HMC）采样器。

Result: 新采样器在性能上优于现有方法，尤其在处理高维高斯目标和偏斜分布时表现突出。

Conclusion: 仿射不变集成HMC采样器在高维问题中具有更好的扩展性，尤其是利用导数信息时。

Abstract: We introduce some new affine invariant ensemble samplers that are easy to
construct and improve upon existing widely used algorithms, especially for
high-dimensional problems. Specifically, we propose a derivative-free ensemble
side move sampler that performs favorably compared to popular samplers in the
\texttt{emcee} package. Additionally, we develop a class of derivative-based
ensemble Hamiltonian Monte Carlo (HMC) samplers with affine invariance, which
outperform standard HMC without affine invariance when sampling highly skewed
distributions. We provide asymptotic scaling analysis for high-dimensional
Gaussian targets to further elucidate the properties of these affine invariant
ensemble samplers. In particular, with derivative information, the affine
invariant ensemble HMC can scale much better with dimension compared to
derivative-free ensemble samplers.

</details>

<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [256] [Mitigating Image Captioning Hallucinations in Vision-Language Models](https://arxiv.org/abs/2505.03420)
*Fei Zhao,Chengcui Zhang,Runlin Zhang,Tianyang Wang,Xi Li*

Main category: cs.MM

TLDR: 提出了一种基于强化学习的测试时适应框架，通过仅更新语言模型层归一化中的可学习参数（约0.003%），减少幻觉现象，无需重新训练或辅助模型。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）中的幻觉现象降低了可靠性和实际应用性，现有方法（如重新训练或集成方法）计算成本高且耗时。

Method: 使用强化学习框架，仅更新语言模型层归一化的参数，结合基于CLIP的幻觉评估模型提供双重奖励。

Result: 在LLaVA和InstructBLIP上分别减少15.4%和17.3%的幻觉率，优于现有基线68.3%。

Conclusion: 该方法高效且低成本，显著减少了VLMs中的幻觉现象。

Abstract: Hallucinations in vision-language models (VLMs) hinder reliability and
real-world applicability, usually stemming from distribution shifts between
pretraining data and test samples. Existing solutions, such as retraining or
fine-tuning on additional data, demand significant computational resources and
labor-intensive data collection, while ensemble-based methods incur additional
costs by introducing auxiliary VLMs. To address these challenges, we propose a
novel test-time adaptation framework using reinforcement learning to mitigate
hallucinations during inference without retraining or any auxiliary VLMs. By
updating only the learnable parameters in the layer normalization of the
language model (approximately 0.003% of the model parameters), our method
reduces distribution shifts between test samples and pretraining samples. A
CLIP-based hallucination evaluation model is proposed to provide dual rewards
to VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in
hallucination rates on LLaVA and InstructBLIP, respectively. Our approach
outperforms state-of-the-art baselines with a 68.3% improvement in
hallucination mitigation, demonstrating its effectiveness.

</details>

<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [257] [Taskmaster Deconstructed: A Quantitative Look at Tension, Volatility, and Viewer Ratings](https://arxiv.org/abs/2505.02886)
*David H. Silver*

Main category: physics.soc-ph

TLDR: 研究分析了英国电视节目《Taskmaster》的评分动态对观众参与度的影响，发现评分指标与IMDb评分无显著关联，观众兴趣更多由选手行为而非游戏机制驱动。


<details>
  <summary>Details</summary>
Motivation: 探讨《Taskmaster》中评分动态是否对观众参与度有实际影响。

Method: 对18季162集的15项评分指标进行统计分析，包括排名波动、分数差距等。

Result: 评分指标与IMDb评分无显著关联；长期趋势显示平均分数上升，波动性略降。

Conclusion: 观众兴趣主要由选手行为决定，而非游戏机制。

Abstract: Taskmaster is a British television show that combines comedic performance
with a formal scoring system. Despite the appearance of structured competition,
it remains unclear whether scoring dynamics contribute meaningfully to audience
engagement. We conducted a statistical analysis of 162 episodes across 18
series, using fifteen episode-level metrics to quantify rank volatility, point
spread, lead changes, and winner dominance. None of these metrics showed a
significant association with IMDb ratings, even after controlling for series
effects. Long-term trends suggest that average points have increased over time,
while volatility has slightly declined and rank spread has remained stable.
These patterns indicate an attempt to enhance competitive visibility without
altering the show's structural equilibrium. We also analyzed contestant rank
trajectories and identified five recurring archetypes describing performance
styles. These patterns suggest that viewer interest is shaped more by
contestant behavior than by game mechanics.

</details>

### [258] [Floating Car Observers in Intelligent Transportation Systems: Detection Modeling and Temporal Insights](https://arxiv.org/abs/2505.02845)
*Jeremias Gerner,Klaus Bogenberger,Stefanie Schmidtner*

Main category: physics.soc-ph

TLDR: 论文探讨了通过浮车观测器（FCOs）扩展传统浮车数据（FCD），利用车载传感器检测和定位其他交通参与者，为智能交通系统（ITS）提供更丰富的交通数据。研究比较了多种建模方法，并提出了一种基于神经网络的仿真技术，验证了FCOs在低渗透率下仍能有效监测交通状态。


<details>
  <summary>Details</summary>
Motivation: 传统浮车数据（FCD）的局限性促使研究者探索通过FCOs集成更多传感器数据，以提供更详细的交通信息，从而提升智能交通系统的性能。

Method: 研究采用了多种建模方法，包括2D光线追踪、高保真协同仿真和基于神经网络的仿真技术，并在SUMO交通网络数字孪生中验证FCOs的效果。

Result: 实验表明，即使在20%的渗透率下，FCOs通过LiDAR检测能识别65%的车辆；结合时间信息后，可恢复80%以上未被直接检测到的车辆。

Conclusion: FCOs在低渗透率和多变交通条件下仍能显著提升交通状态估计和监测能力，展现了其在智能交通系统中的潜力。

Abstract: Floating Car Observers (FCOs) extend traditional Floating Car Data (FCD) by
integrating onboard sensors to detect and localize other traffic participants,
providing richer and more detailed traffic data. In this work, we explore
various modeling approaches for FCO detections within microscopic traffic
simulations to evaluate their potential for Intelligent Transportation System
(ITS) applications. These approaches range from 2D raytracing to high-fidelity
co-simulations that emulate real-world sensors and integrate 3D object
detection algorithms to closely replicate FCO detections. Additionally, we
introduce a neural network-based emulation technique that effectively
approximates the results of high-fidelity co-simulations. This approach
captures the unique characteristics of FCO detections while offering a fast and
scalable solution for modeling. Using this emulation method, we investigate the
impact of FCO data in a digital twin of a traffic network modeled in SUMO.
Results demonstrate that even at a 20% penetration rate, FCOs using LiDAR-based
detections can identify 65% of vehicles across various intersections and
traffic demand scenarios. Further potential emerges when temporal insights are
integrated, enabling the recovery of previously detected but currently unseen
vehicles. By employing data-driven methods, we recover over 80% of these
vehicles with minimal positional deviations. These findings underscore the
potential of FCOs for ITS, particularly in enhancing traffic state estimation
and monitoring under varying penetration rates and traffic conditions.

</details>

<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [259] [Robustly Invertible Nonlinear Dynamics and the BiLipREN: Contracting Neural Models with Contracting Inverses](https://arxiv.org/abs/2505.03069)
*Yurui Zhang,Ruigang Wang,Ian R. Manchester*

Main category: eess.SY

TLDR: 论文提出了一种新的可逆循环神经网络模型BiLipREN，基于收缩性和增量稳定性分析，确保模型及其逆模型均具有鲁棒性和可区分性。


<details>
  <summary>Details</summary>
Motivation: 研究非线性动态系统的可逆性，并设计一种具有鲁棒性和可区分性的可逆循环神经网络模型。

Method: 提出BiLipREN模型，通过参数化神经动态模型确保其构造上的鲁棒可逆性，并结合正交线性系统构建更一般的双Lipschitz动态模型。

Result: BiLipREN模型在数值实验中展示了其鲁棒性和可区分性的优势。

Conclusion: BiLipREN模型为非线性动态系统的可逆性提供了一种有效的解决方案，并具有广泛的应用潜力。

Abstract: We study the invertibility of nonlinear dynamical systems from the
perspective of contraction and incremental stability analysis and propose a new
invertible recurrent neural model: the BiLipREN. In particular, we consider a
nonlinear state space model to be robustly invertible if an inverse exists with
a state space realisation, and both the forward model and its inverse are
contracting, i.e. incrementally exponentially stable, and Lipschitz, i.e. have
bounded incremental gain. This property of bi-Lipschitzness implies both
robustness in the sense of sensitivity to input perturbations, as well as
robust distinguishability of different inputs from their corresponding outputs,
i.e. the inverse model robustly reconstructs the input sequence despite small
perturbations to the initial conditions and measured output. Building on this
foundation, we propose a parameterization of neural dynamic models:
bi-Lipschitz recurrent equilibrium networks (biLipREN), which are robustly
invertible by construction. Moreover, biLipRENs can be composed with orthogonal
linear systems to construct more general bi-Lipschitz dynamic models, e.g., a
nonlinear analogue of minimum-phase/all-pass (inner/outer) factorization. We
illustrate the utility of our proposed approach with numerical examples.

</details>

<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [260] [Nonnegative Low-rank Matrix Recovery Can Have Spurious Local Minima](https://arxiv.org/abs/2505.03717)
*Richard Y. Zhang*

Main category: math.OC

TLDR: 论文研究了低秩矩阵恢复问题在非负约束下是否仍具有良性非凸性，发现完全观测时成立，但部分观测时失效。


<details>
  <summary>Details</summary>
Motivation: 探讨非负约束对低秩矩阵恢复问题良性非凸性的影响。

Method: 在秩-1非负真实矩阵的简单设定下，分析完全观测和部分观测情况。

Result: 完全观测时良性非凸性成立（RIP常数δ=0），部分观测时失效（δ→0+）。

Conclusion: 非负约束导致低秩矩阵恢复的连续性理论失效，暴露了理论空白。

Abstract: The classical low-rank matrix recovery problem is well-known to exhibit
\emph{benign nonconvexity} under the restricted isometry property (RIP): local
optimization is guaranteed to converge to the global optimum, where the ground
truth is recovered. We investigate whether benign nonconvexity continues to
hold when the factor matrices are constrained to be elementwise nonnegative --
a common practical requirement. In the simple setting of a rank-1 nonnegative
ground truth, we confirm that benign nonconvexity holds in the fully-observed
case with RIP constant $\delta=0$. Surprisingly, however, this property fails
to extend to the partially-observed case with any arbitrarily small RIP
constant $\delta\to0^{+}$, irrespective of rank overparameterization. This
finding exposes a critical theoretical gap: the continuity argument widely used
to explain the empirical robustness of low-rank matrix recovery fundamentally
breaks down once nonnegative constraints are imposed.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [261] [The Precautionary Principle and the Innovation Principle: Incompatible Guides for AI Innovation Governance?](https://arxiv.org/abs/2505.02846)
*Kim Kaivanto*

Main category: cs.CY

TLDR: 论文探讨了AI治理中预防原则（PP）和创新原则（IP）的兼容性，提出在弱形式下两者可以共存，并通过信号检测理论（SDT）模型优化决策。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决AI治理中PP和IP是否矛盾的问题，探索两者在弱形式下的共存可能性。

Method: 采用信号检测理论（SDT）模型，分析类型I和类型II错误的成本，提出红绿灯决策框架。

Result: 结果表明，弱PP和IP可以共存，通过沙盒机制实现“等待-监控”策略，优化治理决策。

Conclusion: 结论是弱PP和IP并非互斥，沙盒机制有助于动态调整治理策略。

Abstract: In policy debates concerning the governance and regulation of Artificial
Intelligence (AI), both the Precautionary Principle (PP) and the Innovation
Principle (IP) are advocated by their respective interest groups. Do these
principles offer wholly incompatible and contradictory guidance? Does one
necessarily negate the other? I argue here that provided attention is
restricted to weak-form PP and IP, the answer to both of these questions is
"No." The essence of these weak formulations is the requirement to fully
account for type-I error costs arising from erroneously preventing the
innovation's diffusion through society (i.e. mistaken regulatory red-lighting)
as well as the type-II error costs arising from erroneously allowing the
innovation to diffuse through society (i.e. mistaken regulatory
green-lighting). Within the Signal Detection Theory (SDT) model developed here,
weak-PP red-light (weak-IP green-light) determinations are optimal for
sufficiently small (large) ratios of expected type-I to type-II error costs.
For intermediate expected cost ratios, an amber-light 'wait-and-monitor' policy
is optimal. Regulatory sandbox instruments allow AI testing and experimentation
to take place within a structured environment of limited duration and societal
scale, whereby the expected cost ratio falls within the 'wait-and-monitor'
range. Through sandboxing regulators and innovating firms learn more about the
expected cost ratio, and what respective adaptations -- of regulation, of
technical solution, of business model, or combination thereof, if any -- are
needed to keep the ratio out of the weak-PP red-light zone.

</details>

### [262] [Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration](https://arxiv.org/abs/2505.02848)
*Kexin Ding,Mu Zhou,Akshay Chaudhari,Shaoting Zhang,Dimitris N. Metaxas*

Main category: cs.CY

TLDR: 本文探讨了如何通过人类与大型语言模型（LLM）的对齐，提升医疗工作流程的有效性、安全性和责任性。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗领域的广泛应用需要与医疗利益相关者的知识、需求和价值观对齐，以确保模型输出符合实际需求。

Method: 通过医疗利益相关者参与LLM的整个生命周期（数据整理、模型训练和推理）来实现对齐，并讨论了相关方法、工具和应用。

Result: 研究表明，通过增强医疗知识整合、任务理解和人类指导，LLM能更好地遵循人类价值观。

Conclusion: 未来需进一步强化人类与LLM的对齐，以构建可信赖的医疗应用。

Abstract: The wide exploration of large language models (LLMs) raises the awareness of
alignment between healthcare stakeholder preferences and model outputs. This
alignment becomes a crucial foundation to empower the healthcare workflow
effectively, safely, and responsibly. Yet the varying behaviors of LLMs may not
always match with healthcare stakeholders' knowledge, demands, and values. To
enable a human-AI alignment, healthcare stakeholders will need to perform
essential roles in guiding and enhancing the performance of LLMs. Human
professionals must participate in the entire life cycle of adopting LLM in
healthcare, including training data curation, model training, and inference. In
this review, we discuss the approaches, tools, and applications of alignments
between healthcare stakeholders and LLMs. We demonstrate that LLMs can better
follow human values by properly enhancing healthcare knowledge integration,
task understanding, and human guidance. We provide outlooks on enhancing the
alignment between humans and LLMs to build trustworthy real-world healthcare
applications.

</details>

### [263] [Enhancing tutoring systems by leveraging tailored promptings and domain knowledge with Large Language Models](https://arxiv.org/abs/2505.02849)
*Mohsen Balavar,Wenli Yang,David Herbert,Soonja Yeom*

Main category: cs.CY

TLDR: 研究通过整合RAG和LLMs，开发个性化编程辅导系统，解决了AI驱动的学习工具在适应多样学习风格和实时反馈方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管AI工具如ITS和ChatGPT提升了学习体验，但在适应不同学习风格和提供实时反馈方面仍存在不足。

Method: 整合RAG到LLMs的提示工程中，开发个性化辅导应用，并通过编程任务的定量指标评估系统。

Result: 系统成功将学生按技能分类，并提供情境感知反馈，显示出比通用方法更好的效果和适应性。

Conclusion: 研究证明了整合RAG和LLMs的个性化辅导系统在提升学习效果方面的潜力。

Abstract: Recent advancements in artificial intelligence (AI) and machine learning have
reignited interest in their impact on Computer-based Learning (CBL). AI-driven
tools like ChatGPT and Intelligent Tutoring Systems (ITS) have enhanced
learning experiences through personalisation and flexibility. ITSs can adapt to
individual learning needs and provide customised feedback based on a student's
performance, cognitive state, and learning path. Despite these advances,
challenges remain in accommodating diverse learning styles and delivering
real-time, context-aware feedback. Our research aims to address these gaps by
integrating skill-aligned feedback via Retrieval Augmented Generation (RAG)
into prompt engineering for Large Language Models (LLMs) and developing an
application to enhance learning through personalised tutoring in a computer
science programming context. The pilot study evaluated a proposed system using
three quantitative metrics: readability score, response time, and feedback
depth, across three programming tasks of varying complexity. The system
successfully sorted simulated students into three skill-level categories and
provided context-aware feedback. This targeted approach demonstrated better
effectiveness and adaptability compared to general methods.

</details>

### [264] [A Computational Model of Inclusive Pedagogy: From Understanding to Application](https://arxiv.org/abs/2505.02853)
*Francesco Balzan,Pedro P. Santos,Maurizio Gabbrielli,Mahault Albarracin,Manuel Lopes*

Main category: cs.CY

TLDR: 论文提出了一种计算模型，模拟教师与学生之间的协同适应互动（T-SI），并验证了双向策略优于单向策略。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型对教师与学生协同适应互动的模拟不足，限制了教育科学的测试和扩展能力，以及机器学习系统支持人类学习过程的潜力。

Method: 开发了一个计算T-SI模型，整合了人类教育的上下文洞察，并在模拟课堂环境中评估了多种策略。

Result: 结果显示，基于协同适应原则的策略（如双向互动）优于单向策略，提升了所有学习类型的学习效果。

Conclusion: 该模型为教育科学提供了可测试的框架，并推动了可扩展、包容性AI教育系统的发展。

Abstract: Human education transcends mere knowledge transfer, it relies on
co-adaptation dynamics -- the mutual adjustment of teaching and learning
strategies between agents. Despite its centrality, computational models of
co-adaptive teacher-student interactions (T-SI) remain underdeveloped. We argue
that this gap impedes Educational Science in testing and scaling contextual
insights across diverse settings, and limits the potential of Machine Learning
systems, which struggle to emulate and adaptively support human learning
processes. To address this, we present a computational T-SI model that
integrates contextual insights on human education into a testable framework. We
use the model to evaluate diverse T-SI strategies in a realistic synthetic
classroom setting, simulating student groups with unequal access to sensory
information. Results show that strategies incorporating co-adaptation
principles (e.g., bidirectional agency) outperform unilateral approaches (i.e.,
where only the teacher or the student is active), improving the learning
outcomes for all learning types. Beyond the testing and scaling of
context-dependent educational insights, our model enables hypothesis generation
in controlled yet adaptable environments. This work bridges non-computational
theories of human education with scalable, inclusive AI in Education systems,
providing a foundation for equitable technologies that dynamically adapt to
learner needs.

</details>

### [265] [AI Education in a Mirror: Challenges Faced by Academic and Industry Experts](https://arxiv.org/abs/2505.02856)
*Mahir Akgun,Hadi Hosseini*

Main category: cs.CY

TLDR: 研究探讨了AI教育与实践之间的差距，通过访谈14位专家，揭示了数据质量、模型扩展性等挑战，并提出了改进AI课程的建议。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，学术界与工业界之间的教育与实践差距需要进一步研究。

Method: 通过半结构化访谈14位AI专家（8位来自工业界，6位来自学术界），分析挑战。

Result: 工业界关注部署限制和资源问题，学术界更关注理论适应和标准化。建议课程融入实际复杂性和跨学科学习。

Conclusion: AI课程应结合实践挑战，同时注重基础理论和伦理教育。

Abstract: As Artificial Intelligence (AI) technologies continue to evolve, the gap
between academic AI education and real-world industry challenges remains an
important area of investigation. This study provides preliminary insights into
challenges AI professionals encounter in both academia and industry, based on
semi-structured interviews with 14 AI experts - eight from industry and six
from academia. We identify key challenges related to data quality and
availability, model scalability, practical constraints, user behavior, and
explainability. While both groups experience data and model adaptation
difficulties, industry professionals more frequently highlight deployment
constraints, resource limitations, and external dependencies, whereas academics
emphasize theoretical adaptation and standardization issues. These exploratory
findings suggest that AI curricula could better integrate real-world
complexities, software engineering principles, and interdisciplinary learning,
while recognizing the broader educational goals of building foundational and
ethical reasoning skills.

</details>

### [266] [Understanding University Students' Use of Generative AI: The Roles of Demographics and Personality Traits](https://arxiv.org/abs/2505.02863)
*Newnew Deng,Edward Jiusi Liu,Xiaoming Zhai*

Main category: cs.CY

TLDR: 研究发现，高年级学生、非英语母语者及亚裔学生更倾向于使用生成式AI（GAI），且性格特质显著影响GAI的使用态度。


<details>
  <summary>Details</summary>
Motivation: 填补关于大学生GAI使用及其影响因素的研究空白。

Method: 调查了363名美国本科生和研究生的GAI使用情况，分析其与人口统计变量及大五人格特质的关系。

Result: 高年级学生、非英语母语者及亚裔学生更偏好GAI；性格特质如尽责性、宜人性等显著影响GAI的使用和态度。

Conclusion: 大学需提供个性化指导，确保学生有效、道德且公平地使用GAI。

Abstract: The use of generative AI (GAI) among university students is rapidly
increasing, yet empirical research on students' GAI use and the factors
influencing it remains limited. To address this gap, we surveyed 363
undergraduate and graduate students in the United States, examining their GAI
usage and how it relates to demographic variables and personality traits based
on the Big Five model (i.e., extraversion, agreeableness, conscientiousness,
and emotional stability, and intellect/imagination). Our findings reveal: (a)
Students in higher academic years are more inclined to use GAI and prefer it
over traditional resources. (b) Non-native English speakers use and adopt GAI
more readily than native speakers. (c) Compared to White, Asian students report
higher GAI usage, perceive greater academic benefits, and express a stronger
preference for it. Similarly, Black students report a more positive impact of
GAI on their academic performance. Personality traits also play a significant
role in shaping perceptions and usage of GAI. After controlling demographic
factors, we found that personality still significantly predicts GAI use and
attitudes: (a) Students with higher conscientiousness use GAI less. (b)
Students who are higher in agreeableness perceive a less positive impact of GAI
on academic performance and express more ethical concerns about using it for
academic work. (c) Students with higher emotional stability report a more
positive impact of GAI on learning and fewer concerns about its academic use.
(d) Students with higher extraversion show a stronger preference for GAI over
traditional resources. (e) Students with higher intellect/imagination tend to
prefer traditional resources. These insights highlight the need for
universities to provide personalized guidance to ensure students use GAI
effectively, ethically, and equitably in their academic pursuits.

</details>

### [267] [The Cognitive Foundations of Economic Exchange: A Modular Framework Grounded in Behavioral Evidence](https://arxiv.org/abs/2505.02945)
*Egil Diau*

Main category: cs.CY

TLDR: 论文提出了一种基于三个认知最小机制的概念框架，用于在多智能体AI中建模社会合作，将信任重新定义为一种分级的认知期望。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体AI中社会合作的建模问题，尤其是经济学和伦理学中非正式定义的信任和道德概念缺乏可测试性和实现性。

Method: 结合灵长类行为、婴儿认知和经济人类学的实证证据，提出由个体识别、互相信任和成本回报敏感性组成的框架。

Result: 该框架为人工代理中的互惠交换提供了可模拟的基础，支持自下而上的可扩展合作和制度动态。

Conclusion: 通过认知最小机制重新定义信任，为多智能体AI中的社会合作提供了新的理论基础和实现路径。

Abstract: A key challenge in multi-agent AI is modeling social cooperation under
realistic behavioral constraints. Many foundational concepts in economics and
ethics such as "trust" or "morality" are often defined informally, without
operational criteria or cognitive grounding, which limits their testability and
implementation in artificial agents. Drawing on converging empirical evidence
from primate behavior, infant cognition, and economic anthropology, we propose
a conceptual framework composed of three cognitively minimal mechanisms:
individual recognition, reciprocal credence, and cost return sensitivity. This
framework reframes trust as a graded cognitive expectation, providing a
simulateable basis for reciprocal exchange in artificial agents, and enabling
the bottom-up emergence of scalable cooperation and institutional dynamics.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [268] [Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation](https://arxiv.org/abs/2505.03105)
*Xule Lin*

Main category: cs.HC

TLDR: 论文提出了Cognitio Emergens（CE）框架，用于分析人类与AI在科学知识创造中的动态合作关系，强调角色、能力和关系的动态演变。


<details>
  <summary>Details</summary>
Motivation: 现有模型无法捕捉人类与AI在科学理解中的递归互动，CE框架旨在解决这一局限。

Method: CE框架整合了三个组件：Agency Configurations（描述人类与AI的权威分配）、Epistemic Dimensions（捕捉合作中的能力特征）和Partnership Dynamics（分析关系演变的动力）。

Result: CE揭示了知识共创通过角色、价值观和组织结构的持续协商实现，为平衡人类参与和科学突破提供了工具。

Conclusion: CE重新定义了人类与AI的科学合作，既不过度乐观也不过度担忧，而是提供了维持有意义人类参与的框架。

Abstract: Scientific knowledge creation is fundamentally transforming as humans and AI
systems evolve beyond tool-user relationships into co-evolutionary epistemic
partnerships. When AlphaFold revolutionized protein structure prediction,
researchers described engaging with an epistemic partner that reshaped how they
conceptualized fundamental relationships. This article introduces Cognitio
Emergens (CE), a framework addressing critical limitations in existing models
that focus on static roles or narrow metrics while failing to capture how
scientific understanding emerges through recursive human-AI interaction over
time. CE integrates three components addressing these limitations: Agency
Configurations describing how authority distributes between humans and AI
(Directed, Contributory, Partnership), with partnerships dynamically
oscillating between configurations rather than following linear progression;
Epistemic Dimensions capturing six specific capabilities emerging through
collaboration across Discovery, Integration, and Projection axes, creating
distinctive "capability signatures" that guide development; and Partnership
Dynamics identifying forces shaping how these relationships evolve,
particularly the risk of epistemic alienation where researchers lose
interpretive control over knowledge they formally endorse. Drawing from
autopoiesis theory, social systems theory, and organizational modularity, CE
reveals how knowledge co-creation emerges through continuous negotiation of
roles, values, and organizational structures. By reconceptualizing human-AI
scientific collaboration as fundamentally co-evolutionary, CE offers a balanced
perspective that neither uncritically celebrates nor unnecessarily fears AI's
evolving role, instead providing conceptual tools for cultivating partnerships
that maintain meaningful human participation while enabling transformative
scientific breakthroughs.

</details>

### [269] [Augmenting Human Cognition through Everyday AR](https://arxiv.org/abs/2505.03492)
*Xiaoan Liu*

Main category: cs.HC

TLDR: 探讨了AR如何通过空间计算和多模态LLM成为直觉“思维工具”，提升人机交互。


<details>
  <summary>Details</summary>
Motivation: 研究AR如何结合数字认知与物理环境，增强任务表现和理解。

Method: 利用空间计算和多模态LLM技术，实现持续AR交互。

Result: AR能够无缝连接数字与物理世界，提供主动、上下文敏感的交互。

Conclusion: AR作为思维工具有潜力显著提升人类认知和任务效率。

Abstract: As spatial computing and multimodal LLMs mature, AR is tending to become an
intuitive "thinking tool," embedding semantic and context-aware intelligence
directly into everyday environments. This paper explores how always-on AR can
seamlessly bridge digital cognition and physical affordances, enabling
proactive, context-sensitive interactions that enhance human task performance
and understanding.

</details>

### [270] [BCause: Human-AI collaboration to improve hybrid mapping and ideation in argumentation-grounded deliberation](https://arxiv.org/abs/2505.03584)
*Lucas Anastasiou,Anna De Liddo*

Main category: cs.HC

TLDR: BCause是一个结合生成式AI与人机协作的讨论系统，旨在将公共议题的无结构对话转化为结构化、可操作的民主进程。


<details>
  <summary>Details</summary>
Motivation: 公共讨论常存在分散、浅显、缺乏行动导向的问题，BCause试图通过技术手段改善这一现状。

Method: 系统通过三项创新实现目标：(i) 将无结构文本转化为论证性讨论，(ii) 通过Telegram机器人实现地理感知的问题报告，(iii) 提供智能报告工具（如摘要、主题建模、政策建议等）。

Result: BCause通过人机协作保留了人类参与的关键作用，确保伦理监督、情境相关性和创造性综合。

Conclusion: BCause为公共讨论提供了一种结构化、高效的解决方案，有望提升民主决策的质量和行动力。

Abstract: Public deliberation, as in open discussion of issues of public concern, often
suffers from scattered and shallow discourse, poor sensemaking, and a
disconnect from actionable policy outcomes. This paper introduces BCause, a
discussion system leveraging generative AI and human-machine collaboration to
transform unstructured dialogue around public issues (such as urban living,
policy changes, and current socio-economic transformations) into structured,
actionable democratic processes. We present three innovations: (i) importing
and transforming unstructured transcripts into argumentative discussions, (ii)
geo-deliberated problem-sensing via a Telegram bot for local issue reporting,
and (iii) smart reporting with customizable widgets (e.g., summaries, topic
modelling, policy recommendations, clustered arguments). The system's human-AI
partnership preserves critical human participation to ensure ethical oversight,
contextual relevance, and creative synthesis.

</details>

<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [271] [A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case](https://arxiv.org/abs/2505.03196)
*Haoxiang Luo,Gang Sun,Yinqiu Liu,Dusit Niyato,Hongfang Yu,Mohammed Atiquzzaman,Schahram Dustdar*

Main category: cs.NI

TLDR: 提出了一种基于区块链的多LLM协作框架（MultiLLMN），以解决不同LLM在通信和网络任务中的信任和协作问题，并通过FBS防御案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 不同LLM因结构和训练数据差异可能导致响应不一致或偏见，且单个LLM的局限性可能被恶意设备利用，需解决信任问题。

Method: 设计了一个区块链支持的多LLM协作框架，通过合作评估选择最优响应，并以FBS防御为例进行验证。

Result: 提出的MultiLLMN框架能有效提升LLM在复杂网络优化问题中的可靠性和响应质量。

Conclusion: MultiLLMN为解决LLM协作和信任问题提供了可行方案，未来可进一步探索其应用潜力。

Abstract: Large Language Models (LLMs) demonstrate strong potential across a variety of
tasks in communications and networking due to their advanced reasoning
capabilities. However, because different LLMs have different model structures
and are trained using distinct corpora and methods, they may offer varying
optimization strategies for the same network issues. Moreover, the limitations
of an individual LLM's training data, aggravated by the potential maliciousness
of its hosting device, can result in responses with low confidence or even
bias. To address these challenges, we propose a blockchain-enabled
collaborative framework that connects multiple LLMs into a Trustworthy
Multi-LLM Network (MultiLLMN). This architecture enables the cooperative
evaluation and selection of the most reliable and high-quality responses to
complex network optimization problems. Specifically, we begin by reviewing
related work and highlighting the limitations of existing LLMs in collaboration
and trust, emphasizing the need for trustworthiness in LLM-based systems. We
then introduce the workflow and design of the proposed Trustworthy MultiLLMN
framework. Given the severity of False Base Station (FBS) attacks in B5G and 6G
communication systems and the difficulty of addressing such threats through
traditional modeling techniques, we present FBS defense as a case study to
empirically validate the effectiveness of our approach. Finally, we outline
promising future research directions in this emerging area.

</details>

<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [272] [Solar Flare Forecast: A Comparative Analysis of Machine Learning Algorithms for Solar Flare Class Prediction](https://arxiv.org/abs/2505.03385)
*Julia Bringewald*

Main category: astro-ph.SR

TLDR: 该研究评估了三种机器学习算法（随机森林、KNN和XGBoost）在太阳耀斑分类任务中的表现，发现随机森林和XGBoost性能最优，并探讨了降维技术对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 太阳耀斑对空间天气和地球技术设施有重大影响，需要准确预测其发生和强度。

Method: 使用13个SHARP参数数据集，结合主成分分析（PCA）进行降维，评估了三种算法在二分类和多分类任务中的表现。

Result: 随机森林和XGBoost在所有指标中表现最佳，且降维程度越高，模型性能越好。

Conclusion: 该研究为太阳耀斑预测提供了优化方法，有助于开发更准确的空间天气预报系统。

Abstract: Solar flares are among the most powerful and dynamic events in the solar
system, resulting from the sudden release of magnetic energy stored in the
Sun's atmosphere. These energetic bursts of electromagnetic radiation can
release up to 10^32 erg of energy, impacting space weather and posing risks to
technological infrastructure and therefore require accurate forecasting of
solar flare occurrences and intensities. This study evaluates the predictive
performance of three machine learning algorithms: Random Forest, k-Nearest
Neighbors (KNN), and Extreme Gradient Boosting (XGBoost) for classifying solar
flares into 4 categories (B, C, M, X). Using the dataset of 13 SHARP
parameters, the effectiveness of the models was evaluated in binary and
multiclass classification tasks. The analysis utilized 8 principal components
(PC), capturing 95% of data variance, and 100 PCs, capturing 97.5% of variance.
Our approach uniquely combines binary and multiclass classification with
different levels of dimensionality reduction, an innovative methodology not
previously explored in the context of solar flare prediction. Employing a
10-fold stratified cross-validation and grid search for hyperparameter tuning
ensured robust model evaluation. Our findings indicate that Random Forest and
XGBoost consistently demonstrate strong performance across all metrics,
benefiting significantly from increased dimensionality. The insights of this
study enhance future research by optimizing dimensionality reduction techniques
and informing model selection for astrophysical tasks. By integrating this
newly acquired knowledge into future research, more accurate space weather
forecasting systems can be developed, along with a deeper understanding of
solar physics.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [273] [MORE: Mobile Manipulation Rearrangement Through Grounded Language Reasoning](https://arxiv.org/abs/2505.03035)
*Mohammad Mohammadi,Daniel Honerkamp,Martin Büchner,Matteo Cassinelli,Tim Welschehold,Fabien Despinoy,Igor Gilitschenski,Abhinav Valada*

Main category: cs.RO

TLDR: MORE是一种新方法，通过场景图和主动过滤方案增强语言模型能力，解决大规模环境中的零样本移动操作规划问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理大量物体和大规模环境时性能下降的问题。

Method: 利用场景图表示环境，引入实例区分和主动过滤方案，提取任务相关子图。

Result: 在BEHAVIOR-1K基准测试中表现优异，首次显著解决了部分任务，并在复杂现实任务中验证了能力。

Conclusion: MORE通过场景图和过滤方案有效提升了语言模型的规划能力，适用于室内外环境。

Abstract: Autonomous long-horizon mobile manipulation encompasses a multitude of
challenges, including scene dynamics, unexplored areas, and error recovery.
Recent works have leveraged foundation models for scene-level robotic reasoning
and planning. However, the performance of these methods degrades when dealing
with a large number of objects and large-scale environments. To address these
limitations, we propose MORE, a novel approach for enhancing the capabilities
of language models to solve zero-shot mobile manipulation planning for
rearrangement tasks. MORE leverages scene graphs to represent environments,
incorporates instance differentiation, and introduces an active filtering
scheme that extracts task-relevant subgraphs of object and region instances.
These steps yield a bounded planning problem, effectively mitigating
hallucinations and improving reliability. Additionally, we introduce several
enhancements that enable planning across both indoor and outdoor environments.
We evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1K
benchmark, where it becomes the first approach to successfully solve a
significant share of the benchmark, outperforming recent foundation model-based
approaches. Furthermore, we demonstrate the capabilities of our approach in
several complex real-world tasks, mimicking everyday activities. We make the
code publicly available at https://more-model.cs.uni-freiburg.de.

</details>

### [274] [Latent Adaptive Planner for Dynamic Manipulation](https://arxiv.org/abs/2505.03077)
*Donghun Noh,Deqian Kong,Minglu Zhao,Andrew Lizarraga,Jianwen Xie,Ying Nian Wu,Dennis Hong*

Main category: cs.RO

TLDR: LAP是一种基于潜在空间推理的动态非抓取操作规划方法，通过学习人类演示视频实现高效适应环境变化。


<details>
  <summary>Details</summary>
Motivation: 解决视觉运动策略学习中的关键挑战，如时间一致性和环境适应性。

Method: 采用变分重规划框架和贝叶斯潜在空间更新，结合模型比例映射从人类演示中生成准确状态。

Result: 在多个复杂操作基准测试中表现优异，成功率高、轨迹平滑且能耗低。

Conclusion: LAP为机器人提供了类人的适应能力，并适用于多种平台。

Abstract: This paper presents Latent Adaptive Planner (LAP), a novel approach for
dynamic nonprehensile manipulation tasks that formulates planning as latent
space inference, effectively learned from human demonstration videos. Our
method addresses key challenges in visuomotor policy learning through a
principled variational replanning framework that maintains temporal consistency
while efficiently adapting to environmental changes. LAP employs Bayesian
updating in latent space to incrementally refine plans as new observations
become available, striking an optimal balance between computational efficiency
and real-time adaptability. We bridge the embodiment gap between humans and
robots through model-based proportional mapping that regenerates accurate
kinematic-dynamic joint states and object positions from human demonstrations.
Experimental evaluations across multiple complex manipulation benchmarks
demonstrate that LAP achieves state-of-the-art performance, outperforming
existing approaches in success rate, trajectory smoothness, and energy
efficiency, particularly in dynamic adaptation scenarios. Our approach enables
robots to perform complex interactions with human-like adaptability while
providing an expandable framework applicable to diverse robotic platforms using
the same human demonstration videos.

</details>

### [275] [Learn to Swim: Data-Driven LSTM Hydrodynamic Model for Quadruped Robot Gait Optimization](https://arxiv.org/abs/2505.03146)
*Fei Han,Pengming Guo,Hao Chen,Weikun Li,Jingbo Ren,Naijun Liu,Ning Yang,Dixia Fan*

Main category: cs.RO

TLDR: 提出了一种基于LSTM网络的流体实验数据驱动模型（FED-LSTM），用于预测水下四足机器人的非稳态非线性水动力，优于传统经验公式（EF）。


<details>
  <summary>Details</summary>
Motivation: 传统经验公式（EF）在预测复杂流体动力学时表现不佳，需要一种更准确且适应性强的模型来优化水下机器人的运动性能。

Method: 利用实验数据（包括腿力和身体拖曳测试）训练FED-LSTM模型，并通过NSGA-II算法优化直线和转弯步态。

Result: FED-LSTM在直线游泳中减少了偏转误差，并在不增加转弯半径的情况下缩短了转弯时间，硬件实验验证了其精度和稳定性。

Conclusion: FED-LSTM为提升水下腿式机器人的游泳性能提供了稳健框架，为未来水下机器人运动研究奠定了基础。

Abstract: This paper presents a Long Short-Term Memory network-based Fluid Experiment
Data-Driven model (FED-LSTM) for predicting unsteady, nonlinear hydrodynamic
forces on the underwater quadruped robot we constructed. Trained on
experimental data from leg force and body drag tests conducted in both a
recirculating water tank and a towing tank, FED-LSTM outperforms traditional
Empirical Formulas (EF) commonly used for flow prediction over flat surfaces.
The model demonstrates superior accuracy and adaptability in capturing complex
fluid dynamics, particularly in straight-line and turning-gait optimizations
via the NSGA-II algorithm. FED-LSTM reduces deflection errors during
straight-line swimming and improves turn times without increasing the turning
radius. Hardware experiments further validate the model's precision and
stability over EF. This approach provides a robust framework for enhancing the
swimming performance of legged robots, laying the groundwork for future
advances in underwater robotic locomotion.

</details>

### [276] [Systematic Evaluation of Initial States and Exploration-Exploitation Strategies in PID Auto-Tuning: A Framework-Driven Approach Applied on Mobile Robots](https://arxiv.org/abs/2505.03159)
*Zaid Ghazal,Ali Al-Bustami,Khouloud Gaaloul,Jaerock Kwon*

Main category: cs.RO

TLDR: 本文提出了一种新框架，用于评估初始系统状态和探索-开发平衡对PID控制器自动调谐的影响，并通过实验验证了其对收敛速度和性能指标的影响。


<details>
  <summary>Details</summary>
Motivation: 研究初始系统状态和探索-开发平衡对PID自动调谐的影响，填补现有研究的空白，并为实际应用提供实证依据。

Method: 引入新框架，结合贝叶斯优化和差分进化算法，在两种PID控制的机器人平台上进行实验。

Result: 实验结果揭示了系统变化对收敛速度、稳定时间、上升时间和超调百分比的影响。

Conclusion: 研究结果为未来PID自动调谐领域的进一步研究提供了实证基础。

Abstract: PID controllers are widely used in control systems because of their
simplicity and effectiveness. Although advanced optimization techniques such as
Bayesian Optimization and Differential Evolution have been applied to address
the challenges of automatic tuning of PID controllers, the influence of initial
system states on convergence and the balance between exploration and
exploitation remains underexplored. Moreover, experimenting the influence
directly on real cyber-physical systems such as mobile robots is crucial for
deriving realistic insights. In the present paper, a novel framework is
introduced to evaluate the impact of systematically varying these factors on
the PID auto-tuning processes that utilize Bayesian Optimization and
Differential Evolution. Testing was conducted on two distinct PID-controlled
robotic platforms, an omnidirectional robot and a differential drive mobile
robot, to assess the effects on convergence rate, settling time, rise time, and
overshoot percentage. As a result, the experimental outcomes yield evidence on
the effects of the systematic variations, thereby providing an empirical basis
for future research studies in the field.

</details>

### [277] [Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets](https://arxiv.org/abs/2505.03174)
*Guillermo Roque,Erika Maquiling,Jose Giovanni Tapia Lopez,Ross Greer*

Main category: cs.RO

TLDR: 利用GPS和NLP自动生成指令-动作数据对，减少人工标注成本，提升数据集生成效率。


<details>
  <summary>Details</summary>
Motivation: 人工标注指令-动作数据对成本高且效率低，探索通过GPS和NLP自动化生成数据的方法。

Method: 通过驾驶收集GPS语音指令，结合视频数据形成视觉-语言-动作三元组，开发自动化数据收集系统ADVLAT-Engine。

Result: 成功将GPS语音指令分类为八种类型，展示了从移动应用中自动生成高质量数据集的潜力。

Conclusion: 自动化生成指令-动作数据对可显著提升数据集规模和质量，为视觉-语言导航和交互式自主系统提供支持。

Abstract: Instruction-Action (IA) data pairs are valuable for training robotic systems,
especially autonomous vehicles (AVs), but having humans manually annotate this
data is costly and time-inefficient. This paper explores the potential of using
mobile application Global Positioning System (GPS) references and Natural
Language Processing (NLP) to automatically generate large volumes of IA
commands and responses without having a human generate or retroactively tag the
data. In our pilot data collection, by driving to various destinations and
collecting voice instructions from GPS applications, we demonstrate a means to
collect and categorize the diverse sets of instructions, further accompanied by
video data to form complete vision-language-action triads. We provide details
on our completely automated data collection prototype system, ADVLAT-Engine. We
characterize collected GPS voice instructions into eight different
classifications, highlighting the breadth of commands and referentialities
available for curation from freely available mobile applications. Through
research and exploration into the automation of IA data pairs using GPS
references, the potential to increase the speed and volume at which
high-quality IA datasets are created, while minimizing cost, can pave the way
for robust vision-language-action (VLA) models to serve tasks in
vision-language navigation (VLN) and human-interactive autonomous systems.

</details>

### [278] [The Unreasonable Effectiveness of Discrete-Time Gaussian Process Mixtures for Robot Policy Learning](https://arxiv.org/abs/2505.03296)
*Jan Ole von Hartz,Adrian Röfer,Joschka Boedecker,Abhinav Valada*

Main category: cs.RO

TLDR: MiDiGap是一种用于机器人操作的灵活策略表示和模仿学习的新方法，仅需少量演示即可学习，并在多种复杂任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中从少量演示中学习并泛化到多样化任务的问题。

Method: 基于离散时间高斯过程的混合模型，结合推理时引导工具（如碰撞信号和机器人运动学约束）。

Result: 在少样本操作基准测试中表现最佳，任务成功率显著提升，轨迹成本降低，样本效率提高。

Conclusion: MiDiGap是一种高效、灵活的方法，适用于多样化机器人操作任务，并具有广泛的应用潜力。

Abstract: We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel
approach for flexible policy representation and imitation learning in robot
manipulation. MiDiGap enables learning from as few as five demonstrations using
only camera observations and generalizes across a wide range of challenging
tasks. It excels at long-horizon behaviors such as making coffee, highly
constrained motions such as opening doors, dynamic actions such as scooping
with a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns
these tasks on a CPU in less than a minute and scales linearly to large
datasets. We also develop a rich suite of tools for inference-time steering
using evidence such as collision signals and robot kinematic constraints. This
steering enables novel generalization capabilities, including obstacle
avoidance and cross-embodiment policy transfer. MiDiGap achieves
state-of-the-art performance on diverse few-shot manipulation benchmarks. On
constrained RLBench tasks, it improves policy success by 76 percentage points
and reduces trajectory cost by 67%. On multimodal tasks, it improves policy
success by 48 percentage points and increases sample efficiency by a factor of
20. In cross-embodiment transfer, it more than doubles policy success. We make
the code publicly available at https://midigap.cs.uni-freiburg.de.

</details>

### [279] [RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic Simulation](https://arxiv.org/abs/2505.03344)
*Keyu Chen,Wenchao Sun,Hao Cheng,Sifa Zheng*

Main category: cs.RO

TLDR: 论文提出了一种双阶段仿真框架，结合数据驱动和物理模拟方法，通过预训练和微调提升自动驾驶交通仿真的真实性和可控性。


<details>
  <summary>Details</summary>
Motivation: 解决交互式闭环交通仿真中真实性与可控性的矛盾，数据驱动方法存在协变量偏移，物理模拟方法缺乏专家演示。

Method: 双阶段框架：数据驱动模拟器中进行开环模仿学习预训练，物理模拟器中进行闭环强化学习微调，并提出RIFT策略。

Result: RIFT显著提升了生成交通场景的真实性和可控性，为自动驾驶性能评估提供了可靠平台。

Conclusion: 双阶段框架和RIFT策略有效解决了仿真中的关键挑战，为自动驾驶测试提供了更优方案。

Abstract: Achieving both realism and controllability in interactive closed-loop traffic
simulation remains a key challenge in autonomous driving. Data-driven
simulation methods reproduce realistic trajectories but suffer from covariate
shift in closed-loop deployment, compounded by simplified dynamics models that
further reduce reliability. Conversely, physics-based simulation methods
enhance reliable and controllable closed-loop interactions but often lack
expert demonstrations, compromising realism. To address these challenges, we
introduce a dual-stage AV-centered simulation framework that conducts open-loop
imitation learning pre-training in a data-driven simulator to capture
trajectory-level realism and multimodality, followed by closed-loop
reinforcement learning fine-tuning in a physics-based simulator to enhance
controllability and mitigate covariate shift. In the fine-tuning stage, we
propose RIFT, a simple yet effective closed-loop RL fine-tuning strategy that
preserves the trajectory-level multimodality through a GRPO-style
group-relative advantage formulation, while enhancing controllability and
training stability by replacing KL regularization with the dual-clip mechanism.
Extensive experiments demonstrate that RIFT significantly improves the realism
and controllability of generated traffic scenarios, providing a robust platform
for evaluating autonomous vehicle performance in diverse and interactive
scenarios.

</details>

### [280] [Sim2Real Transfer for Vision-Based Grasp Verification](https://arxiv.org/abs/2505.03046)
*Pau Amargant,Peter Hönig,Markus Vincze*

Main category: cs.RO

TLDR: 提出了一种基于视觉的两阶段抓取验证方法，结合YOLO目标检测和ResNet分类器，并通过合成数据集HSR-GraspSynth解决真实数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于力和触觉传感器的方法在处理可变形和非刚性物体时效果不佳，需要一种更可靠的抓取验证方法。

Method: 采用两阶段架构：YOLO检测机械手位置，ResNet分类器判断物体是否存在；引入合成数据集HSR-GraspSynth补充真实数据。

Result: 实验表明该方法在真实环境中具有高准确性，并具备集成到抓取流程的潜力。

Conclusion: 提出的视觉方法有效解决了可变形物体抓取验证的挑战，代码和数据集已开源。

Abstract: The verification of successful grasps is a crucial aspect of robot
manipulation, particularly when handling deformable objects. Traditional
methods relying on force and tactile sensors often struggle with deformable and
non-rigid objects. In this work, we present a vision-based approach for grasp
verification to determine whether the robotic gripper has successfully grasped
an object. Our method employs a two-stage architecture; first YOLO-based object
detection model to detect and locate the robot's gripper and then a
ResNet-based classifier determines the presence of an object. To address the
limitations of real-world data capture, we introduce HSR-GraspSynth, a
synthetic dataset designed to simulate diverse grasping scenarios. Furthermore,
we explore the use of Visual Question Answering capabilities as a zero-shot
baseline to which we compare our model. Experimental results demonstrate that
our approach achieves high accuracy in real-world environments, with potential
for integration into grasping pipelines. Code and datasets are publicly
available at https://github.com/pauamargant/HSR-GraspSynth .

</details>

### [281] [Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach](https://arxiv.org/abs/2505.03702)
*Srecharan Selvam,Abhishesh Silwal,George Kanter*

Main category: cs.RO

TLDR: 提出了一种结合几何与神经网络的混合方法，用于农业环境中的自主叶片抓取，通过自监督学习实现，显著优于纯几何或纯神经网络方法。


<details>
  <summary>Details</summary>
Motivation: 农业环境中叶片操作的自动化面临植物形态多变和叶片可变形等挑战，需要一种更高效的方法。

Method: 结合YOLOv8实例分割和RAFT-Stereo 3D深度估计，构建叶片表征，并通过几何特征评分与神经网络模块（GraspPointCNN）融合，动态平衡两者贡献。

Result: 在受控环境和实际温室中分别达到88.0%和84.7%的成功率，显著优于纯几何（75.3%）和纯神经网络（60.2%）方法。

Conclusion: 该研究为农业机器人领域提供了一种新范式，将领域专业知识与机器学习能力无缝结合，为全自动作物监测系统奠定了基础。

Abstract: Automating leaf manipulation in agricultural settings faces significant
challenges, including the variability of plant morphologies and deformable
leaves. We propose a novel hybrid geometric-neural approach for autonomous leaf
grasping that combines traditional computer vision with neural networks through
self-supervised learning. Our method integrates YOLOv8 for instance
segmentation and RAFT-Stereo for 3D depth estimation to build rich leaf
representations, which feed into both a geometric feature scoring pipeline and
a neural refinement module (GraspPointCNN). The key innovation is our
confidence-weighted fusion mechanism that dynamically balances the contribution
of each approach based on prediction certainty. Our self-supervised framework
uses the geometric pipeline as an expert teacher to automatically generate
training data. Experiments demonstrate that our approach achieves an 88.0%
success rate in controlled environments and 84.7% in real greenhouse
conditions, significantly outperforming both purely geometric (75.3%) and
neural (60.2%) methods. This work establishes a new paradigm for agricultural
robotics where domain expertise is seamlessly integrated with machine learning
capabilities, providing a foundation for fully automated crop monitoring
systems.

</details>

### [282] [Visual Imitation Enables Contextual Humanoid Control](https://arxiv.org/abs/2505.03729)
*Arthur Allshire,Hongsuk Choi,Junyi Zhang,David McAllister,Anthony Zhang,Chung Min Kim,Trevor Darrell,Pieter Abbeel,Jitendra Malik,Angjoo Kanazawa*

Main category: cs.RO

TLDR: VIDEOMIMIC是一种从日常视频中提取人类动作和环境信息，并生成人形机器人控制策略的流程。


<details>
  <summary>Details</summary>
Motivation: 通过简单的人类动作视频教学，为人形机器人提供多样化的环境操作能力。

Method: 利用真实到模拟再到真实的流程，联合重建人类动作和环境，生成机器人控制策略。

Result: 在真实人形机器人上实现了稳健、可重复的上下文控制，如上下楼梯、坐立等动作。

Conclusion: VIDEOMIMIC为教学人形机器人在多样化环境中操作提供了可扩展的解决方案。

Abstract: How can we teach humanoids to climb staircases and sit on chairs using the
surrounding environment context? Arguably, the simplest way is to just show
them-casually capture a human motion video and feed it to humanoids. We
introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday
videos, jointly reconstructs the humans and the environment, and produces
whole-body control policies for humanoid robots that perform the corresponding
skills. We demonstrate the results of our pipeline on real humanoid robots,
showing robust, repeatable contextual control such as staircase ascents and
descents, sitting and standing from chairs and benches, as well as other
dynamic whole-body skills-all from a single policy, conditioned on the
environment and global root commands. VIDEOMIMIC offers a scalable path towards
teaching humanoids to operate in diverse real-world environments.

</details>

### [283] [AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control](https://arxiv.org/abs/2505.03738)
*Jialong Li,Xuxin Cheng,Tianshu Huang,Shiqi Yang,Ri-Zhao Qiu,Xiaolong Wang*

Main category: cs.RO

TLDR: 提出了一种名为AMO的框架，结合强化学习和轨迹优化，用于实时自适应全身控制，显著提升了人形机器人的稳定性和工作空间。


<details>
  <summary>Details</summary>
Motivation: 人形机器人因高自由度和非线性动力学难以实现超灵巧的全身运动，AMO旨在解决这一问题。

Method: 结合模拟到现实的强化学习和轨迹优化，构建混合数据集以减轻分布偏差，训练适应性强且鲁棒的神经网络。

Result: 在仿真和29自由度Unitree G1机器人上验证，AMO表现出更高的稳定性和更大的工作空间。

Conclusion: AMO通过模仿学习支持自主任务执行，展示了系统的多功能性和鲁棒性。

Abstract: Humanoid robots derive much of their dexterity from hyper-dexterous
whole-body movements, enabling tasks that require a large operational
workspace: such as picking objects off the ground. However, achieving these
capabilities on real humanoids remains challenging due to their high degrees of
freedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization
(AMO), a framework that integrates sim-to-real reinforcement learning (RL) with
trajectory optimization for real-time, adaptive whole-body control. To mitigate
distribution bias in motion imitation RL, we construct a hybrid AMO dataset and
train a network capable of robust, on-demand adaptation to potentially O.O.D.
commands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid
robot, demonstrating superior stability and an expanded workspace compared to
strong baselines. Finally, we show that AMO's consistent performance supports
autonomous task execution via imitation learning, underscoring the system's
versatility and robustness.

</details>

### [284] [Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid](https://arxiv.org/abs/2505.03694)
*Parv Kapoor,Ian Higgins,Nikhil Keetha,Jay Patrikar,Brady Moon,Zelin Ye,Yao He,Ivan Cisneros,Yaoyu Hu,Changliu Liu,Eunsuk Kang,Sebastian Scherer*

Main category: cs.RO

TLDR: ViSafe是一个基于视觉的高速空中防撞系统，通过结合边缘AI框架和多摄像头硬件原型，为资源受限的飞行器提供安全分离保障。


<details>
  <summary>Details</summary>
Motivation: 确保空中飞行器的安全分离是实现高密度共享空域无缝操作的关键。

Method: ViSafe采用基于学习的边缘AI框架与多摄像头硬件原型结合，利用感知输入控制屏障函数（CBF）设计安全阈值。

Result: 在模拟和真实飞行测试中，ViSafe在不同场景下均能确保安全分离，并在高速碰撞测试中表现优异。

Conclusion: ViSafe为高速空中导航的安全性设立了新标准，展示了视觉防撞系统的潜力。

Abstract: Assured safe-separation is essential for achieving seamless high-density
operation of airborne vehicles in a shared airspace. To equip
resource-constrained aerial systems with this safety-critical capability, we
present ViSafe, a high-speed vision-only airborne collision avoidance system.
ViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by
tightly integrating a learning-based edge-AI framework with a custom
multi-camera hardware prototype designed under SWaP-C constraints. By
leveraging perceptual input-focused control barrier functions (CBF) to design,
encode, and enforce safety thresholds, ViSafe can provide provably safe runtime
guarantees for self-separation in high-speed aerial operations. We evaluate
ViSafe's performance through an extensive test campaign involving both
simulated digital twins and real-world flight scenarios. By independently
varying agent types, closure rates, interaction geometries, and environmental
conditions (e.g., weather and lighting), we demonstrate that ViSafe
consistently ensures self-separation across diverse scenarios. In
first-of-its-kind real-world high-speed collision avoidance tests with closure
rates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous
collision avoidance, establishing a new standard for safety in high-speed
aerial navigation.

</details>

<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [285] [Safer Prompts: Reducing IP Risk in Visual Generative AI](https://arxiv.org/abs/2505.03338)
*Lena Reissinger,Yuanyuan Li,Anna-Carolina Haensch,Neeraj Sarna*

Main category: math.NA

TLDR: 研究评估提示工程技术在减少图像生成中知识产权侵权风险的有效性，发现Chain of Thought Prompting和Task Instruction Prompting显著降低生成图像与训练数据的相似性。


<details>
  <summary>Details</summary>
Motivation: 视觉生成AI模型可能因训练数据多样性而记忆并复制特定内容，引发知识产权侵权担忧。

Method: 采用提示工程技术（如Chain of Thought Prompting和Task Instruction Prompting）来优化生成过程。

Result: 这些提示技术显著降低了生成图像与训练数据的相似性，减少了侵权风险。

Conclusion: 提示工程是降低生成AI知识产权侵权风险的有效方法。

Abstract: Visual Generative AI models have demonstrated remarkable capability in
generating high-quality images from simple inputs like text prompts. However,
because these models are trained on images from diverse sources, they risk
memorizing and reproducing specific content, raising concerns about
intellectual property (IP) infringement. Recent advances in prompt engineering
offer a cost-effective way to enhance generative AI performance. In this paper,
we evaluate the effectiveness of prompt engineering techniques in mitigating IP
infringement risks in image generation. Our findings show that Chain of Thought
Prompting and Task Instruction Prompting significantly reduce the similarity
between generated images and the training data of diffusion models, thereby
lowering the risk of IP infringement.

</details>

<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [286] [Single-Sample and Robust Online Resource Allocation](https://arxiv.org/abs/2505.02963)
*Rohan Ghuge,Sahil Singla,Yifan Wang*

Main category: cs.DS

TLDR: 本文提出了一种新颖的指数定价算法，用于在线资源分配问题，仅需单个样本即可实现近似最优解，并具有鲁棒性和激励兼容性。


<details>
  <summary>Details</summary>
Motivation: 在线资源分配是计算机科学、运筹学和经济学的核心问题，现有方法通常需要完全分布知识或局限于特定假设，缺乏对异常情况的鲁棒性。

Method: 提出指数定价算法，通过资源价格的指数调整避免资源耗尽，不同于传统的在线学习方法。

Result: 算法仅需单个样本即可实现(1-ε)-近似最优解，并在异常模型下保持鲁棒性。

Conclusion: 指数定价算法为在线资源分配提供了一种简单、高效且鲁棒的解决方案，解决了现有方法的局限性。

Abstract: Online Resource Allocation problem is a central problem in many areas of
Computer Science, Operations Research, and Economics. In this problem, we
sequentially receive $n$ stochastic requests for $m$ kinds of shared resources,
where each request can be satisfied in multiple ways, consuming different
amounts of resources and generating different values. The goal is to achieve a
$(1-\epsilon)$-approximation to the hindsight optimum, where $\epsilon>0$ is a
small constant, assuming each resource has a large budget.
  In this paper, we investigate the learnability and robustness of online
resource allocation. Our primary contribution is a novel Exponential Pricing
algorithm with the following properties: 1. It requires only a \emph{single
sample} from each of the $n$ request distributions to achieve a
$(1-\epsilon)$-approximation for online resource allocation with large budgets.
Such an algorithm was previously unknown, even with access to polynomially many
samples, as prior work either assumed full distributional knowledge or was
limited to i.i.d.\,or random-order arrivals. 2. It is robust to corruptions in
the outliers model and the value augmentation model. Specifically, it maintains
its $(1 - \epsilon)$-approximation guarantee under both these robustness
models, resolving the open question posed in Argue, Gupta, Molinaro, and Singla
(SODA'22). 3. It operates as a simple item-pricing algorithm that ensures
incentive compatibility.
  The intuition behind our Exponential Pricing algorithm is that the price of a
resource should adjust exponentially as it is overused or underused. It differs
from conventional approaches that use an online learning algorithm for item
pricing. This departure guarantees that the algorithm will never run out of any
resource, but loses the usual no-regret properties of online learning
algorithms, necessitating a new analytical approach.

</details>

<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [287] [Vector valued optimal transport: from dynamic to static formulations](https://arxiv.org/abs/2505.03670)
*Katy Craig,Nicolás García Trillos,Đorđe Nikolić*

Main category: math.AP

TLDR: 本文提出了一种统一向量值最优传输理论的框架，涵盖动态和静态形式，并证明不同距离度量之间的等价性。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于向量值测度分类和多物种偏微分方程的应用需求。

Method: 通过将向量值测度建模为乘积空间上的概率测度，结合加权图几何，统一了动态和静态最优传输理论。

Result: 证明了四种向量值最优传输距离之间的双Hölder等价性，并讨论了各度量的优势。

Conclusion: 该框架在多物种偏微分方程和数据分析中具有潜在应用价值，尤其是一种静态形式适合线性化计算。

Abstract: Motivated by applications in classification of vector valued measures and
multispecies PDE, we develop a theory that unifies existing notions of vector
valued optimal transport, from dynamic formulations (\`a la Benamou-Brenier) to
static formulations (\`a la Kantorovich). In our framework, vector valued
measures are modeled as probability measures on a product space $\mathbb{R}^d
\times G$, where $G$ is a weighted graph over a finite set of nodes and the
graph geometry strongly influences the associated dynamic and static distances.
We obtain sharp inequalities relating four notions of vector valued optimal
transport and prove that the distances are mutually bi-H\"older equivalent. We
discuss the theoretical and practical advantages of each metric and indicate
potential applications in multispecies PDE and data analysis. In particular,
one of the static formulations discussed in the paper is amenable to
linearization, a technique that has been explored in recent years to accelerate
the computation of pairwise optimal transport distances.

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [288] [A study on audio synchronous steganography detection and distributed guide inference model based on sliding spectral features and intelligent inference drive](https://arxiv.org/abs/2505.03193)
*Wei Meng*

Main category: cs.SD

TLDR: 本文提出了一种基于短音频同步流的隐写检测与分布式引导重建模型，利用滑动频谱特征提取和智能推理机制，验证了该方法在开放平台上的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着短视频平台的兴起，音频同步流中的隐写数据成为新的隐蔽通信方式，传统检测技术存在局限性。

Method: 采用25毫秒滑动窗口和短时傅里叶变换（STFT）提取主频轨迹，构建同步帧检测模型（M1）和结构化解码模型（M2）。

Result: 在36至45秒音频段发现低熵重复字节序列和高集中频谱能量，确认同步帧存在，推断出分布式引导命令。

Conclusion: 滑动频谱特征对同步隐写检测有效，模型可扩展用于隐蔽通信分析和战术引导模拟。

Abstract: With the rise of short video platforms in global communication, embedding
steganographic data in audio synchronization streams has emerged as a new
covert communication method. To address the limitations of traditional
techniques in detecting synchronized steganography, this paper proposes a
detection and distributed guidance reconstruction model based on short video
"Yupan" samples released by China's South Sea Fleet on TikTok. The method
integrates sliding spectrum feature extraction and intelligent inference
mechanisms. A 25 ms sliding window with short-time Fourier transform (STFT) is
used to extract the main frequency trajectory and construct the synchronization
frame detection model (M1), identifying a frame flag "FFFFFFFFFFFFFFFFFF80".
The subsequent 32-byte payload is decoded by a structured model (M2) to infer
distributed guidance commands. Analysis reveals a low-entropy, repetitive byte
sequence in the 36 to 45 second audio segment with highly concentrated spectral
energy, confirming the presence of synchronization frames. Although plaintext
semantics are not restored, the consistency in command field layout suggests
features of military communication protocols. The multi-segment splicing model
further shows cross-video embedding and centralized decoding capabilities. The
proposed framework validates the effectiveness of sliding spectral features for
synchronized steganography detection and builds an extensible inference model
for covert communication analysis and tactical guidance simulation on open
platforms.

</details>

### [289] [SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation](https://arxiv.org/abs/2505.03273)
*Zhaoxi Mu,Xinyu Yang,Gang Wang*

Main category: cs.SD

TLDR: SepALM利用音频语言模型（ALM）在文本域中纠正和重新合成语音，提升语音分离的精度和适应性。


<details>
  <summary>Details</summary>
Motivation: 解决传统语音分离技术在嘈杂和混响环境中产生的失真问题。

Method: SepALM包含分离器、校正器、合成器和对齐器，结合ALM端到端纠错机制，避免错误累积。

Result: 实验证明SepALM提高了语音分离的精度，并在新声学环境中表现出更强的适应性。

Conclusion: SepALM为语音分离提供了一种更高效且适应性强的解决方案。

Abstract: While contemporary speech separation technologies adeptly process lengthy
mixed audio waveforms, they are frequently challenged by the intricacies of
real-world environments, including noisy and reverberant settings, which can
result in artifacts or distortions in the separated speech. To overcome these
limitations, we introduce SepALM, a pioneering approach that employs audio
language models (ALMs) to rectify and re-synthesize speech within the text
domain following preliminary separation. SepALM comprises four core components:
a separator, a corrector, a synthesizer, and an aligner. By integrating an
ALM-based end-to-end error correction mechanism, we mitigate the risk of error
accumulation and circumvent the optimization hurdles typically encountered in
conventional methods that amalgamate automatic speech recognition (ASR) with
large language models (LLMs). Additionally, we have developed Chain-of-Thought
(CoT) prompting and knowledge distillation techniques to facilitate the
reasoning and training processes of the ALM. Our experiments substantiate that
SepALM not only elevates the precision of speech separation but also markedly
bolsters adaptability in novel acoustic environments.

</details>

### [290] [Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music Generation](https://arxiv.org/abs/2505.03314)
*Jincheng Zhang,György Fazekas,Charalampos Saitis*

Main category: cs.SD

TLDR: 该论文提出了一种将符号音乐表示为类似图像的钢琴卷帘的方法，并引入了一种结合Transformer-Mamba块和可学习小波变换的扩散模型，用于生成高质量且可控的符号音乐。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像合成中表现出色，但在符号音乐生成领域应用较少，主要因为符号音乐是离散事件序列，而标准扩散模型不擅长处理离散数据。

Method: 将符号音乐表示为钢琴卷帘，提出了一种结合Transformer-Mamba块和可学习小波变换的扩散模型，并利用无分类器指导生成目标和弦的音乐。

Result: 实验表明，该方法在音乐质量和可控性方面表现出色，优于钢琴卷帘生成的基线模型。

Conclusion: 该方法为符号音乐生成提供了一种有效的新途径，展示了扩散模型在非图像领域的潜力。

Abstract: The recent surge in the popularity of diffusion models for image synthesis
has attracted new attention to their potential for generation tasks in other
domains. However, their applications to symbolic music generation remain
largely under-explored because symbolic music is typically represented as
sequences of discrete events and standard diffusion models are not well-suited
for discrete data. We represent symbolic music as image-like pianorolls,
facilitating the use of diffusion models for the generation of symbolic music.
Moreover, this study introduces a novel diffusion model that incorporates our
proposed Transformer-Mamba block and learnable wavelet transform.
Classifier-free guidance is utilised to generate symbolic music with target
chords. Our evaluation shows that our method achieves compelling results in
terms of music quality and controllability, outperforming the strong baseline
in pianoroll generation. Our code is available at
https://github.com/jinchengzhanggg/proffusion.

</details>

### [291] [Knowledge Distillation for Speech Denoising by Latent Representation Alignment with Cosine Distance](https://arxiv.org/abs/2505.03442)
*Diep Luong,Mikko Heikkinen,Konstantinos Drossos,Tuomas Virtanen*

Main category: cs.SD

TLDR: 提出了一种基于知识蒸馏的语音去噪方法，通过利用去噪自编码器和余弦相似性，解决了现有方法中学生模型受限于教师模型的问题。


<details>
  <summary>Details</summary>
Motivation: 现有语音去噪方法复杂且难以部署在低资源设备上，而知识蒸馏方法中学生的学习受限于教师模型的分布和特征维度。

Method: 结合去噪自编码器框架、线性倒置瓶颈和余弦相似性，设计了一种改进的知识蒸馏方法。

Result: 实验表明，该方法在师生模型不匹配情况下表现优于现有方法，且学生模型性能更优。

Conclusion: 提出的方法能有效提升学生模型的去噪能力，适应更大的师生模型不匹配条件。

Abstract: Speech denoising is a generally adopted and impactful task, appearing in many
common and everyday-life use cases. Although there are very powerful methods
published, most of those are too complex for deployment in everyday and
low-resources computational environments, like hand-held devices, intelligent
glasses, hearing aids, etc. Knowledge distillation (KD) is a prominent way for
alleviating this complexity mismatch and is based on the
transferring/distilling of knowledge from a pre-trained complex model, the
teacher, to another less complex one, the student. Existing KD methods for
speech denoising are based on processes that potentially hamper the KD by
bounding the learning of the student to the distribution, information ordering,
and feature dimensionality learned by the teacher. In this paper, we present
and assess a method that tries to treat this issue, by exploiting the
well-known denoising-autoencoder framework, the linear inverted bottlenecks,
and the properties of the cosine similarity. We use a public dataset and
conduct repeated experiments with different mismatching scenarios between the
teacher and the student, reporting the mean and standard deviation of the
metrics of our method and another, state-of-the-art method that is used as a
baseline. Our results show that with the proposed method, the student can
perform better and can also retain greater mismatching conditions compared to
the teacher.

</details>

### [292] [CoGenAV: Versatile Audio-Visual Representation Learning via Contrastive-Generative Synchronization](https://arxiv.org/abs/2505.03186)
*Detao Bai,Zhiheng Ma,Xihan Wei,Liefeng Bo*

Main category: cs.SD

TLDR: CoGenAV是一种高效的多模态模型，通过结合对比特征对齐和生成文本预测，利用少量数据学习音频-视觉表示，在多种语音任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统音频系统在复杂条件下表现不佳，而说话者的唇部动作、声音和语言内容之间的同步性为改进语音处理任务提供了丰富信息。

Method: CoGenAV通过优化双重目标（自然音频-视觉同步、对比特征对齐和生成文本预测）训练，仅使用223小时的LRS2数据集。

Result: 在LRS2数据集上，CoGenAV实现了1.27%的词错误率（AVSR），22.0%的VSR词错误率，并在噪声环境中性能提升70%以上。

Conclusion: CoGenAV的多模态表示在多种任务中表现优异，未来将开源以促进学术和工业界的合作。

Abstract: The inherent synchronization between a speaker's lip movements, voice, and
the underlying linguistic content offers a rich source of information for
improving speech processing tasks, especially in challenging conditions where
traditional audio-only systems falter. We introduce CoGenAV, a powerful and
data-efficient model designed to learn versatile audio-visual representations
applicable across a wide range of speech and audio-visual tasks. CoGenAV is
trained by optimizing a dual objective derived from natural audio-visual
synchrony, contrastive feature alignment and generative text prediction, using
only 223 hours of labeled data from the LRS2 dataset. This
contrastive-generative synchronization strategy effectively captures
fundamental cross-modal correlations. We showcase the effectiveness and
versatility of the learned CoGenAV representations on multiple benchmarks. When
utilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these
representations contribute to achieving a state-of-the-art Word Error Rate
(WER) of 1.27. They also enable strong performance in Visual Speech Recognition
(VSR) with a WER of 22.0 on LRS2, and significantly improve performance in
noisy environments by over 70%. Furthermore, CoGenAV representations benefit
speech reconstruction tasks, boosting performance in Speech Enhancement and
Separation, and achieve competitive results in audio-visual synchronization
tasks like Active Speaker Detection (ASD). Our model will be open-sourced to
facilitate further development and collaboration within both academia and
industry.

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [293] [Accelerating Evolution: Integrating PSO Principles into Real-Coded Genetic Algorithm Crossover](https://arxiv.org/abs/2505.03217)
*Xiaobo Jin,JiaShu Tu*

Main category: cs.NE

TLDR: 提出了一种名为PSOX的新型交叉算子，结合了粒子群优化的思想，用于实值编码遗传算法，显著提升了算法性能。


<details>
  <summary>Details</summary>
Motivation: 传统交叉算子仅在同一代个体间交换信息，缺乏对全局和历史最优解的利用，限制了算法性能。

Method: PSOX通过结合当前全局最优解和多代历史最优解，保持种群多样性并加速收敛。

Result: 在15个基准测试函数上验证，PSOX在准确性、稳定性和收敛速度上优于其他五种先进交叉算子。

Conclusion: PSOX是一种高效且稳定的交叉算子，尤其适合复杂优化问题，并提供了参数调优的实用指南。

Abstract: This study introduces an innovative crossover operator named Particle Swarm
Optimization-inspired Crossover (PSOX), which is specifically developed for
real-coded genetic algorithms. Departing from conventional crossover approaches
that only exchange information between individuals within the same generation,
PSOX uniquely incorporates guidance from both the current global best solution
and historical optimal solutions across multiple generations. This novel
mechanism enables the algorithm to maintain population diversity while
simultaneously accelerating convergence toward promising regions of the search
space. The effectiveness of PSOX is rigorously evaluated through comprehensive
experiments on 15 benchmark test functions with diverse characteristics,
including unimodal, multimodal, and highly complex landscapes. Comparative
analysis against five state-of-the-art crossover operators reveals that PSOX
consistently delivers superior performance in terms of solution accuracy,
algorithmic stability, and convergence speed, especially when combined with an
appropriate mutation strategy. Furthermore, the study provides an in-depth
investigation of how different mutation rates influence PSOX's performance,
yielding practical guidelines for parameter tuning when addressing optimization
problems with varying landscape properties.

</details>

### [294] [From Neurons to Computation: Biological Reservoir Computing for Pattern Recognition](https://arxiv.org/abs/2505.03510)
*Ludovico Iannello,Luca Ciampi,Gabriele Lagani,Fabrizio Tonelli,Eleonora Crocco,Lucio Maria Calcagnile,Angelo Di Garbo,Federico Cremisi,Giuseppe Amato*

Main category: cs.NE

TLDR: 提出了一种基于培养生物神经元的生物储备池计算（BRC）新范式，利用多电极阵列（MEA）记录神经活动，通过非线性映射实现高效模式识别。


<details>
  <summary>Details</summary>
Motivation: 探索生物神经网络在传统人工神经网络任务中的应用潜力，推动生物启发计算系统的发展。

Method: 使用培养神经元作为储备池，通过MEA记录神经活动，输入数据通过部分电极引入，其余电极捕获响应，生成高维特征空间。

Result: 实验表明，BRC能有效处理位置编码、方向条和数字识别等任务，验证了生物神经网络的可行性。

Conclusion: BRC为生物启发计算和神经形态工程提供了新方向，具有潜在应用价值。

Abstract: In this paper, we introduce a novel paradigm for reservoir computing (RC)
that leverages a pool of cultured biological neurons as the reservoir
substrate, creating a biological reservoir computing (BRC). This system
operates similarly to an echo state network (ESN), with the key distinction
that the neural activity is generated by a network of cultured neurons, rather
than being modeled by traditional artificial computational units. The neuronal
activity is recorded using a multi-electrode array (MEA), which enables
high-throughput recording of neural signals. In our approach, inputs are
introduced into the network through a subset of the MEA electrodes, while the
remaining electrodes capture the resulting neural activity. This generates a
nonlinear mapping of the input data to a high-dimensional biological feature
space, where distinguishing between data becomes more efficient and
straightforward, allowing a simple linear classifier to perform pattern
recognition tasks effectively. To evaluate the performance of our proposed
system, we present an experimental study that includes various input patterns,
such as positional codes, bars with different orientations, and a digit
recognition task. The results demonstrate the feasibility of using biological
neural networks to perform tasks traditionally handled by artificial neural
networks, paving the way for further exploration of biologically-inspired
computing systems, with potential applications in neuromorphic engineering and
bio-hybrid computing.

</details>

<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [295] [CreoPep: A Universal Deep Learning Framework for Target-Specific Peptide Design and Optimization](https://arxiv.org/abs/2505.02887)
*Cheng Ge,Han-Shen Tae,Zhenqiang Zhang,Lu Lu,Zhijie Huang,Yilin Wang,Tao Jiang,Wenqing Cai,Shan Chang,David J. Adams,Rilei Yu*

Main category: q-bio.BM

TLDR: CreoPep是一个基于深度学习的条件生成框架，用于设计高亲和力肽突变体，并发现新的结构基序，加速下一代肽疗法的发现。


<details>
  <summary>Details</summary>
Motivation: 天然肽变体多样性有限且传统优化策略耗时，限制了靶向肽的治疗潜力。

Method: 结合掩码语言建模和渐进掩码方案，通过FoldX能量筛选和温度控制多项式采样生成结构多样的肽。

Result: 设计的α7烟碱型乙酰胆碱受体抑制剂在电生理实验中表现出亚微摩尔效力。

Conclusion: CreoPep提供了一个通用平台，将计算肽设计与实验验证结合，扩展了传统设计范式。

Abstract: Target-specific peptides, such as conotoxins, exhibit exceptional binding
affinity and selectivity toward ion channels and receptors. However, their
therapeutic potential remains underutilized due to the limited diversity of
natural variants and the labor-intensive nature of traditional optimization
strategies. Here, we present CreoPep, a deep learning-based conditional
generative framework that integrates masked language modeling with a
progressive masking scheme to design high-affinity peptide mutants while
uncovering novel structural motifs. CreoPep employs an integrative augmentation
pipeline, combining FoldX-based energy screening with temperature-controlled
multinomial sampling, to generate structurally and functionally diverse
peptides that retain key pharmacological properties. We validate this approach
by designing conotoxin inhibitors targeting the $\alpha$7 nicotinic
acetylcholine receptor, achieving submicromolar potency in electrophysiological
assays. Structural analysis reveals that CreoPep-generated variants engage in
both conserved and novel binding modes, including disulfide-deficient forms,
thus expanding beyond conventional design paradigms. Overall, CreoPep offers a
robust and generalizable platform that bridges computational peptide design
with experimental validation, accelerating the discovery of next-generation
peptide therapeutics.

</details>