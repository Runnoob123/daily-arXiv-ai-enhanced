<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 38]
- [cs.LG](#cs.LG) [Total: 44]
- [cs.CL](#cs.CL) [Total: 48]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.CV](#cs.CV) [Total: 67]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [physics.data-an](#physics.data-an) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [econ.GN](#econ.GN) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.RO](#cs.RO) [Total: 11]
- [eess.SP](#eess.SP) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [q-bio.CB](#q-bio.CB) [Total: 1]
- [cs.CY](#cs.CY) [Total: 7]
- [eess.IV](#eess.IV) [Total: 3]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Research on CNN-BiLSTM Network Traffic Anomaly Detection Model Based on MindSpore](https://arxiv.org/abs/2504.21008)
*Qiuyan Xiang,Shuang Wu,Dongze Wu,Yuxin Liu,Zhenkai Qin*

Main category: cs.CR

TLDR: 提出了一种结合CNN和BiLSTM的网络流量异常检测模型，在MindSpore框架上实现，性能优异。


<details>
  <summary>Details</summary>
Motivation: 物联网和工业物联网的普及导致网络架构复杂化，传统安全机制难以应对高频、多样且隐蔽的网络攻击。

Method: 集成CNN和BiLSTM的模型，使用NF-BoT-IoT数据集进行实验。

Result: 模型在准确率、精确率、召回率和F1分数上均达到99%，表现优异。

Conclusion: 该模型在网络入侵检测任务中表现出强大的性能和鲁棒性。

Abstract: With the widespread adoption of the Internet of Things (IoT) and Industrial
IoT (IIoT) technologies, network architectures have become increasingly
complex, and the volume of traffic has grown substantially. This evolution
poses significant challenges to traditional security mechanisms, particularly
in detecting high-frequency, diverse, and highly covert network attacks. To
address these challenges, this study proposes a novel network traffic anomaly
detection model that integrates a Convolutional Neural Network (CNN) with a
Bidirectional Long Short-Term Memory (BiLSTM) network, implemented on the
MindSpore framework. Comprehensive experiments were conducted using the
NF-BoT-IoT dataset. The results demonstrate that the proposed model achieves
99% across accuracy, precision, recall, and F1-score, indicating its strong
performance and robustness in network intrusion detection tasks.

</details>

### [2] [Semantic-Aware Contrastive Fine-Tuning: Boosting Multimodal Malware Classification with Discriminative Embeddings](https://arxiv.org/abs/2504.21028)
*Ivan Montoya Sanchez,Shaswata Mitra,Aritran Piplai,Sudip Mittal*

Main category: cs.CR

TLDR: 论文提出了一种对比微调（CFT）方法，通过基于余弦相似度的硬负样本选择优化LLM嵌入，显著提升恶意软件分类性能。


<details>
  <summary>Details</summary>
Motivation: 恶意软件变种快速演变，需要更鲁棒的分类方法增强网络安全。LLM虽有潜力，但语义嵌入重叠和与二进制行为特征的不对齐限制了其效用。

Method: 提出CFT方法，结合高相似度和中等级别的负样本优化嵌入，提升区分能力和多样性。嵌入集成到MAML框架中的多模态分类器，少量样本下测试。

Result: 在CIC-AndMal-2020数据集上，仅用20样本达到63.15%分类准确率，比基线高11-21个百分点。相似性选择比随机采样提升10-23%。

Conclusion: CFT方法通过细粒度语义区分提升了恶意软件分类性能，为LLM在网络安全中的应用提供了可扩展框架。

Abstract: The rapid evolution of malware variants requires robust classification
methods to enhance cybersecurity. While Large Language Models (LLMs) offer
potential for generating malware descriptions to aid family classification,
their utility is limited by semantic embedding overlaps and misalignment with
binary behavioral features. We propose a contrastive fine-tuning (CFT) method
that refines LLM embeddings via targeted selection of hard negative samples
based on cosine similarity, enabling LLMs to distinguish between closely
related malware families. Our approach combines high-similarity negatives to
enhance discriminative power and mid-tier negatives to increase embedding
diversity, optimizing both precision and generalization. Evaluated on the
CIC-AndMal-2020 and BODMAS datasets, our refined embeddings are integrated into
a multimodal classifier within a Model-Agnostic Meta-Learning (MAML) framework
on a few-shot setting. Experiments demonstrate significant improvements: our
method achieves 63.15% classification accuracy with as few as 20 samples on
CIC-AndMal-2020, outperforming baselines by 11--21 percentage points and
surpassing prior negative sampling strategies. Ablation studies confirm the
superiority of similarity-based selection over random sampling, with gains of
10-23%. Additionally, fine-tuned LLMs generate attribute-aware descriptions
that generalize to unseen variants, bridging textual and binary feature gaps.
This work advances malware classification by enabling nuanced semantic
distinctions and provides a scalable framework for adapting LLMs to
cybersecurity challenges.

</details>

### [3] [PICO: Secure Transformers via Robust Prompt Isolation and Cybersecurity Oversight](https://arxiv.org/abs/2504.21029)
*Ben Goertzel,Paulos Yibelo*

Main category: cs.CR

TLDR: 提出了一种名为PICO的鲁棒Transformer架构，通过双通道隔离系统指令与用户输入，结合安全专家代理和知识图谱，防止提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: 解决提示注入攻击问题，确保生成响应的安全性和可靠性。

Method: 采用PICO框架，包括双通道处理、安全专家代理、知识图谱和不可变系统提示分支。

Result: 框架通过数学公式和案例研究验证，支持从头训练或微调实现。

Conclusion: PICO框架有效提升模型安全性，适用于对抗性输入场景。

Abstract: We propose a robust transformer architecture designed to prevent prompt
injection attacks and ensure secure, reliable response generation. Our PICO
(Prompt Isolation and Cybersecurity Oversight) framework structurally separates
trusted system instructions from untrusted user inputs through dual channels
that are processed independently and merged only by a controlled, gated fusion
mechanism. In addition, we integrate a specialized Security Expert Agent within
a Mixture-of-Experts (MoE) framework and incorporate a Cybersecurity Knowledge
Graph (CKG) to supply domain-specific reasoning. Our training design further
ensures that the system prompt branch remains immutable while the rest of the
network learns to handle adversarial inputs safely. This PICO framework is
presented via a general mathematical formulation, then elaborated in terms of
the specifics of transformer architecture, and fleshed out via hypothetical
case studies including Policy Puppetry attacks. While the most effective
implementation may involve training transformers in a PICO-based way from
scratch, we also present a cost-effective fine-tuning approach.

</details>

### [4] [SAGA: A Security Architecture for Governing AI Agentic Systems](https://arxiv.org/abs/2504.21034)
*Georgios Syros,Anshuman Suri,Cristina Nita-Rotaru,Alina Oprea*

Main category: cs.CR

TLDR: SAGA是一个用于管理自主代理的安全架构，提供用户对其代理生命周期的监督，通过中央实体和加密机制实现细粒度控制，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有代理系统设计多为理论，缺乏用户控制的实现和评估，SAGA填补了这一空白。

Method: 用户通过中央实体注册代理，使用加密机制生成访问控制令牌，实现细粒度控制。

Result: 在多种任务和环境下评估，SAGA表现出最小性能开销且不影响任务效用。

Conclusion: SAGA支持安全可信的自主代理部署，促进敏感环境中该技术的负责任采用。

Abstract: Large Language Model (LLM)-based agents increasingly interact, collaborate,
and delegate tasks to one another autonomously with minimal human interaction.
Industry guidelines for agentic system governance emphasize the need for users
to maintain comprehensive control over their agents, mitigating potential
damage from malicious agents. Several proposed agentic system designs address
agent identity, authorization, and delegation, but remain purely theoretical,
without concrete implementation and evaluation. Most importantly, they do not
provide user-controlled agent management. To address this gap, we propose SAGA,
a Security Architecture for Governing Agentic systems, that offers user
oversight over their agents' lifecycle. In our design, users register their
agents with a central entity, the Provider, that maintains agents contact
information, user-defined access control policies, and helps agents enforce
these policies on inter-agent communication. We introduce a cryptographic
mechanism for deriving access control tokens, that offers fine-grained control
over an agent's interaction with other agents, balancing security and
performance consideration. We evaluate SAGA on several agentic tasks, using
agents in different geolocations, and multiple on-device and cloud LLMs,
demonstrating minimal performance overhead with no impact on underlying task
utility in a wide range of conditions. Our architecture enables secure and
trustworthy deployment of autonomous agents, accelerating the responsible
adoption of this technology in sensitive environments.

</details>

### [5] [A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage](https://arxiv.org/abs/2504.21035)
*Rui Xin,Niloofar Mireshghallah,Shuyue Stella Li,Michael Duan,Hyunwoo Kim,Yejin Choi,Yulia Tsvetkov,Sewoong Oh,Pang Wei Koh*

Main category: cs.CR

TLDR: 论文提出了一种新框架，评估去标识化数据的隐私风险，揭示当前方法无法防范语义级信息泄露，导致虚假隐私感。


<details>
  <summary>Details</summary>
Motivation: 现有敏感文本数据去标识化方法仅关注显式标识符，忽略语义级信息泄露，导致隐私保护不足。

Method: 提出新框架，通过重识别攻击量化隐私风险，利用辅助信息推断敏感属性。

Result: 实验显示，商业PII移除工具在MedQA数据集中保护失败率达74%，差分隐私虽有效但降低数据实用性。

Conclusion: 当前去标识化技术存在虚假隐私感，需开发更鲁棒方法防范语义级信息泄露。

Abstract: Sanitizing sensitive text data typically involves removing personally
identifiable information (PII) or generating synthetic data under the
assumption that these methods adequately protect privacy; however, their
effectiveness is often only assessed by measuring the leakage of explicit
identifiers but ignoring nuanced textual markers that can lead to
re-identification. We challenge the above illusion of privacy by proposing a
new framework that evaluates re-identification attacks to quantify individual
privacy risks upon data release. Our approach shows that seemingly innocuous
auxiliary information -- such as routine social activities -- can be used to
infer sensitive attributes like age or substance use history from sanitized
data. For instance, we demonstrate that Azure's commercial PII removal tool
fails to protect 74\% of information in the MedQA dataset. Although
differential privacy mitigates these risks to some extent, it significantly
reduces the utility of the sanitized text for downstream tasks. Our findings
indicate that current sanitization techniques offer a \textit{false sense of
privacy}, highlighting the need for more robust methods that protect against
semantic-level information leakage.

</details>

### [6] [Can Differentially Private Fine-tuning LLMs Protect Against Privacy Attacks?](https://arxiv.org/abs/2504.21036)
*Hao Du,Shang Liu,Yang Cao*

Main category: cs.CR

TLDR: 研究了差分隐私（DP）在不同微调方法中对大语言模型（LLM）隐私保护的效果，发现DP能显著降低隐私风险，但对模型效用的影响因方法而异。


<details>
  <summary>Details</summary>
Motivation: 微调LLM时可能泄露敏感数据，DP的理论保护效果在实际应用中尚不明确，需系统评估其在不同微调方法中的表现。

Method: 通过数据提取和成员推断攻击评估隐私风险，分析DP在不同微调方法和隐私预算下的效果。

Result: DP降低模型效用但效果因方法而异；无DP时隐私风险差异大；高隐私预算仍能显著降低风险；某些方法不适合DP。

Conclusion: 结果为隐私敏感的LLM部署提供指导，并推动未来优化隐私-效用权衡的研究。

Abstract: Fine-tuning large language models (LLMs) has become an essential strategy for
adapting them to specialized tasks; however, this process introduces
significant privacy challenges, as sensitive training data may be inadvertently
memorized and exposed. Although differential privacy (DP) offers strong
theoretical guarantees against such leakage, its empirical privacy
effectiveness on LLMs remains unclear, especially under different fine-tuning
methods. In this paper, we systematically investigate the impact of DP across
fine-tuning methods and privacy budgets, using both data extraction and
membership inference attacks to assess empirical privacy risks. Our main
findings are as follows: (1) Differential privacy reduces model utility, but
its impact varies significantly across different fine-tuning methods. (2)
Without DP, the privacy risks of models fine-tuned with different approaches
differ considerably. (3) When DP is applied, even a relatively high privacy
budget can substantially lower privacy risk. (4) The privacy-utility trade-off
under DP training differs greatly among fine-tuning methods, with some methods
being unsuitable for DP due to severe utility degradation. Our results provide
practical guidance for privacy-conscious deployment of LLMs and pave the way
for future research on optimizing the privacy-utility trade-off in fine-tuning
methodologies.

</details>

### [7] [Security Bug Report Prediction Within and Across Projects: A Comparative Study of BERT and Random Forest](https://arxiv.org/abs/2504.21037)
*Farnaz Soltaniani,Mohammad Ghafari,Mohammed Sayagh*

Main category: cs.CR

TLDR: 比较BERT和随机森林（RF）在安全漏洞报告（SBR）预测中的表现，发现RF在项目内预测表现更好，而BERT在跨项目预测中表现更优。


<details>
  <summary>Details</summary>
Motivation: 早期检测安全漏洞报告（SBRs）对系统安全至关重要，现有机器学习模型仍有改进空间。

Method: 全面比较BERT和随机森林（RF）在SBR预测中的性能，并测试不同数据组合对模型的影响。

Result: RF在项目内预测中表现更优（G-measure高34%），而BERT在跨项目预测中表现显著优于RF（G-measure达62%）。

Conclusion: BERT在跨项目预测中表现更优，而RF在项目内预测中更具优势，数据组合对模型性能有显著影响。

Abstract: Early detection of security bug reports (SBRs) is crucial for preventing
vulnerabilities and ensuring system reliability. While machine learning models
have been developed for SBR prediction, their predictive performance still has
room for improvement. In this study, we conduct a comprehensive comparison
between BERT and Random Forest (RF), a competitive baseline for predicting
SBRs. The results show that RF outperforms BERT with a 34% higher average
G-measure for within-project predictions. Adding only SBRs from various
projects improves both models' average performance. However, including both
security and nonsecurity bug reports significantly reduces RF's average
performance to 46%, while boosts BERT to its best average performance of 66%,
surpassing RF. In cross-project SBR prediction, BERT achieves a remarkable 62%
G-measure, which is substantially higher than RF.

</details>

### [8] [Prefill-Based Jailbreak: A Novel Approach of Bypassing LLM Safety Boundary](https://arxiv.org/abs/2504.21038)
*Yakai Li,Jiekang Hu,Weiduan Sang,Luping Ma,Jing Xie,Weijuan Zhang,Aimin Yu,Shijie Zhao,Qingjia Huang,Qihang Zhou*

Main category: cs.CR

TLDR: 论文提出了一种利用LLMs预填充功能的新型越狱攻击方法，通过直接操纵后续令牌的概率分布绕过安全机制，显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 研究越狱方法以揭示LLMs的系统性漏洞，指导开发者持续改进安全性。

Method: 提出两种攻击变体：静态预填充（SP）和优化预填充（OP），后者通过迭代优化预填充文本最大化攻击成功率。

Result: 在六种先进LLMs上的实验表明，OP方法攻击成功率高达99.82%，显著优于基线方法。

Conclusion: 该研究强调了需要强大的内容验证机制来防范预填充功能的对抗性利用。

Abstract: Large Language Models (LLMs) are designed to generate helpful and safe
content. However, adversarial attacks, commonly referred to as jailbreak, can
bypass their safety protocols, prompting LLMs to generate harmful content or
reveal sensitive data. Consequently, investigating jailbreak methodologies is
crucial for exposing systemic vulnerabilities within LLMs, ultimately guiding
the continuous implementation of security enhancements by developers. In this
paper, we introduce a novel jailbreak attack method that leverages the
prefilling feature of LLMs, a feature designed to enhance model output
constraints. Unlike traditional jailbreak methods, the proposed attack
circumvents LLMs' safety mechanisms by directly manipulating the probability
distribution of subsequent tokens, thereby exerting control over the model's
output. We propose two attack variants: Static Prefilling (SP), which employs a
universal prefill text, and Optimized Prefilling (OP), which iteratively
optimizes the prefill text to maximize the attack success rate. Experiments on
six state-of-the-art LLMs using the AdvBench benchmark validate the
effectiveness of our method and demonstrate its capability to substantially
enhance attack success rates when combined with existing jailbreak approaches.
The OP method achieved attack success rates of up to 99.82% on certain models,
significantly outperforming baseline methods. This work introduces a new
jailbreak attack method in LLMs, emphasizing the need for robust content
validation mechanisms to mitigate the adversarial exploitation of prefilling
features. All code and data used in this paper are publicly available.

</details>

### [9] [Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report](https://arxiv.org/abs/2504.21039)
*Paul Kassianik,Baturay Saglam,Alexander Chen,Blaine Nelson,Anu Vellore,Massimo Aufiero,Fraser Burch,Dhruv Kedia,Avi Zohary,Sajana Weerawardhena,Aman Priyanshu,Adam Swanda,Amy Chang,Hyrum Anderson,Kojin Oshiba,Omar Santos,Yaron Singer,Amin Karbasi*

Main category: cs.CR

TLDR: Foundation-Sec-8B是一个基于Llama 3.1架构的网络安全专用大语言模型，通过精心筛选的网络安全语料库进行持续预训练，解决了数据稀缺和知识表示复杂性的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在网络安全领域的应用受限，主要由于缺乏专业训练数据和难以表示网络安全知识。

Method: 基于Llama 3.1架构，通过持续预训练网络安全语料库构建Foundation-Sec-8B模型。

Result: 在网络安全任务中表现优异，部分任务性能与Llama 3.1-70B和GPT-4o-mini相当。

Conclusion: 通过公开模型，推动AI工具在网络安全领域的应用与发展。

Abstract: As transformer-based large language models (LLMs) increasingly permeate
society, they have revolutionized domains such as software engineering,
creative writing, and digital arts. However, their adoption in cybersecurity
remains limited due to challenges like scarcity of specialized training data
and complexity of representing cybersecurity-specific knowledge. To address
these gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on
the Llama 3.1 architecture and enhanced through continued pretraining on a
carefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across
both established and new cybersecurity benchmarks, showing that it matches
Llama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By
releasing our model to the public, we aim to accelerate progress and adoption
of AI-driven tools in both public and private cybersecurity contexts.

</details>

### [10] [Fast and Robust Speckle Pattern Authentication by Scale Invariant Feature Transform algorithm in Physical Unclonable Functions](https://arxiv.org/abs/2504.21041)
*Giuseppe Emanuele Lio,Mauro Daniel Luigi Bruno,Francesco Riboli,Sara Nocentini,Antonio Ferraro*

Main category: cs.CR

TLDR: 提出了一种基于SIFT算法的光学PUF认证方法，提高了安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 由于伪造现象增多，需要开发新的防伪设备和加密密钥。光学PUF因其高安全性和环境敏感性成为研究热点，但缺乏与鲁棒识别算法的结合。

Method: 利用SIFT算法从光学PUF生成的散斑图案中提取独特且不变的特征，结合CRPs协议实现高可靠性认证。

Result: 该方法在存在旋转、缩放和裁剪等干扰时仍能可靠认证，且操作速度快（微秒级）。

Conclusion: 该方法扩展了PUF在高安全性认证和防伪领域的应用潜力。

Abstract: Nowadays, due to the growing phenomenon of forgery in many fields, the
interest in developing new anti-counterfeiting device and cryptography keys,
based on the Physical Unclonable Functions (PUFs) paradigm, is widely
increased. PUFs are physical hardware with an intrinsic, irreproducible
disorder that allows for on-demand cryptographic key extraction. Among them,
optical PUF are characterized by a large number of degrees of freedom resulting
in higher security and higher sensitivity to environmental conditions. While
these promising features led to the growth of advanced fabrication strategies
and materials for new PUF devices, their combination with robust recognition
algorithm remains largely unexplored. In this work, we present a
metric-independent authentication approach that leverages the Scale Invariant
Feature Transform (SIFT) algorithm to extract unique and invariant features
from the speckle patterns generated by optical Physical Unclonable Functions
(PUFs). The application of SIFT to the challenge response pairs (CRPs) protocol
allows us to correctly authenticate a client while denying any other fraudulent
access. In this way, the authentication process is highly reliable even in
presence of response rotation, zooming, and cropping that may occur in
consecutive PUF interrogations and to which other postprocessing algorithm are
highly sensitive. This characteristics together with the speed of the method
(tens of microseconds for each operation) broaden the applicability and
reliability of PUF to practical high-security authentication or merchandise
anti-counterfeiting.

</details>

### [11] [What's Pulling the Strings? Evaluating Integrity and Attribution in AI Training and Inference through Concept Shift](https://arxiv.org/abs/2504.21042)
*Jiamin Chang,Haoyang Li,Hammond Pearce,Ruoxi Sun,Bo Li,Minhui Xue*

Main category: cs.CR

TLDR: ConceptLens是一个通用框架，利用预训练多模态模型分析概念漂移，检测数据中毒攻击、隐私风险和社会学偏见，提升AI系统的可信度。


<details>
  <summary>Details</summary>
Motivation: 随着AI的广泛应用，其可信度问题（如完整性、隐私、鲁棒性和偏见）日益突出，需要一种方法来评估和归因这些威胁。

Method: 提出ConceptLens框架，通过分析概念漂移识别威胁根源，检测数据中毒攻击、隐私风险，并揭示模型对关键概念的过度依赖。

Result: ConceptLens能有效检测数据中毒攻击，识别隐私风险和社会学偏见，揭示模型弱点，并提供安全训练数据的潜在漏洞。

Conclusion: ConceptLens为提升AI系统的可信度提供了可操作的见解，有助于加速AI的采用和创新。

Abstract: The growing adoption of artificial intelligence (AI) has amplified concerns
about trustworthiness, including integrity, privacy, robustness, and bias. To
assess and attribute these threats, we propose ConceptLens, a generic framework
that leverages pre-trained multimodal models to identify the root causes of
integrity threats by analyzing Concept Shift in probing samples. ConceptLens
demonstrates strong detection performance for vanilla data poisoning attacks
and uncovers vulnerabilities to bias injection, such as the generation of
covert advertisements through malicious concept shifts. It identifies privacy
risks in unaltered but high-risk samples, filters them before training, and
provides insights into model weaknesses arising from incomplete or imbalanced
training data. Additionally, at the model level, it attributes concepts that
the target model is overly dependent on, identifies misleading concepts, and
explains how disrupting key concepts negatively impacts the model. Furthermore,
it uncovers sociological biases in generative content, revealing disparities
across sociological contexts. Strikingly, ConceptLens reveals how safe training
and inference data can be unintentionally and easily exploited, potentially
undermining safety alignment. Our study informs actionable insights to breed
trust in AI systems, thereby speeding adoption and driving greater innovation.

</details>

### [12] [CodeBC: A More Secure Large Language Model for Smart Contract Code Generation in Blockchain](https://arxiv.org/abs/2504.21043)
*Lingxiang wang,Hainan Zhang,Qinnan Zhang,Ziwei Wang,Hongwei Zheng,Jin Dong,Zhiming Zheng*

Main category: cs.CR

TLDR: CodeBC是一种专为生成安全的区块链智能合约设计的代码生成模型，通过三阶段微调方法，无需依赖成对的漏洞标注数据，而是利用漏洞和安全标签来区分代码安全性。实验表明，CodeBC在生成代码的质量和安全性上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生成代码时缺乏对安全漏洞的理解，尤其在智能合约等高风险任务中。现有方法依赖手动标注的漏洞数据，但低资源语言（如Solidity）缺乏此类数据。

Method: CodeBC基于CodeLlama，采用三阶段微调方法，利用漏洞和安全标签而非成对标注数据，训练模型区分安全与漏洞代码。推理阶段通过安全标签生成安全代码。

Result: CodeBC在BLEU、CodeBLEU和编译通过率上优于基线模型，同时显著降低漏洞率。

Conclusion: CodeBC的三阶段微调策略高效且成本低，为生成安全的智能合约代码提供了有前景的解决方案。

Abstract: Large language models (LLMs) excel at generating code from natural language
instructions, yet they often lack an understanding of security vulnerabilities.
This limitation makes it difficult for LLMs to avoid security risks in
generated code, particularly in high-security programming tasks such as smart
contract development for blockchain. Researchers have attempted to enhance the
vulnerability awareness of these models by training them to differentiate
between vulnerable and fixed code snippets. However, this approach relies
heavily on manually labeled vulnerability data, which is only available for
popular languages like Python and C++. For low-resource languages like
Solidity, used in smart contracts, large-scale annotated datasets are scarce
and difficult to obtain. To address this challenge, we introduce CodeBC, a code
generation model specifically designed for generating secure smart contracts in
blockchain. CodeBC employs a three-stage fine-tuning approach based on
CodeLlama, distinguishing itself from previous methods by not relying on
pairwise vulnerability location annotations. Instead, it leverages
vulnerability and security tags to teach the model the differences between
vulnerable and secure code. During the inference phase, the model leverages
security tags to generate secure and robust code. Experimental results
demonstrate that CodeBC outperforms baseline models in terms of BLEU, CodeBLEU,
and compilation pass rates, while significantly reducing vulnerability rates.
These findings validate the effectiveness and cost-efficiency of our
three-stage fine-tuning strategy, making CodeBC a promising solution for
generating secure smart contract code.

</details>

### [13] [AGATE: Stealthy Black-box Watermarking for Multimodal Model Copyright Protection](https://arxiv.org/abs/2504.21044)
*Jianbo Gao,Keke Gai,Jing Yu,Liehuang Zhu,Qi Wu*

Main category: cs.CR

TLDR: 论文提出了一种名为AGATE的黑盒后门水印框架，用于解决多模态模型版权保护中的隐蔽性和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法易受恶意检测和伪造攻击，导致水印失效，因此需要更隐蔽和鲁棒的解决方案。

Method: 提出对抗性触发器生成方法和后变换模块，通过两阶段水印验证判断模型侵权。

Result: 在五个数据集的下游任务中表现优于现有方法，并在对抗攻击场景下验证了鲁棒性。

Conclusion: AGATE在多模态模型版权保护中具有优越的隐蔽性和鲁棒性。

Abstract: Recent advancement in large-scale Artificial Intelligence (AI) models
offering multimodal services have become foundational in AI systems, making
them prime targets for model theft. Existing methods select Out-of-Distribution
(OoD) data as backdoor watermarks and retrain the original model for copyright
protection. However, existing methods are susceptible to malicious detection
and forgery by adversaries, resulting in watermark evasion. In this work, we
propose Model-\underline{ag}nostic Black-box Backdoor W\underline{ate}rmarking
Framework (AGATE) to address stealthiness and robustness challenges in
multimodal model copyright protection. Specifically, we propose an adversarial
trigger generation method to generate stealthy adversarial triggers from
ordinary dataset, providing visual fidelity while inducing semantic shifts. To
alleviate the issue of anomaly detection among model outputs, we propose a
post-transform module to correct the model output by narrowing the distance
between adversarial trigger image embedding and text embedding. Subsequently, a
two-phase watermark verification is proposed to judge whether the current model
infringes by comparing the two results with and without the transform module.
Consequently, we consistently outperform state-of-the-art methods across five
datasets in the downstream tasks of multimodal image-text retrieval and image
classification. Additionally, we validated the robustness of AGATE under two
adversarial attack scenarios.

</details>

### [14] [Leveraging LLM to Strengthen ML-Based Cross-Site Scripting Detection](https://arxiv.org/abs/2504.21045)
*Dennis Miczek,Divyesh Gabbireddy,Suman Saha*

Main category: cs.CR

TLDR: 该研究提出了一种利用大型语言模型（LLM）生成复杂混淆XSS攻击载荷的方法，显著提高了机器学习模型对混淆XSS攻击的检测能力。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种技术用于防御XSS攻击，但混淆XSS攻击仍难以检测，且现有工具生成的混淆代码复杂度有限。

Method: 通过微调LLM自动生成复杂混淆XSS载荷，并用于训练随机森林模型。

Result: 模型在混淆数据集上的准确率达到99.5%，且LLM生成的样本复杂度比现有工具高28.1%。

Conclusion: 该方法显著提升了模型对高级XSS攻击的检测能力，适用于实际应用安全场景。

Abstract: According to the Open Web Application Security Project (OWASP), Cross-Site
Scripting (XSS) is a critical security vulnerability. Despite decades of
research, XSS remains among the top 10 security vulnerabilities. Researchers
have proposed various techniques to protect systems from XSS attacks, with
machine learning (ML) being one of the most widely used methods. An ML model is
trained on a dataset to identify potential XSS threats, making its
effectiveness highly dependent on the size and diversity of the training data.
A variation of XSS is obfuscated XSS, where attackers apply obfuscation
techniques to alter the code's structure, making it challenging for security
systems to detect its malicious intent. Our study's random forest model was
trained on traditional (non-obfuscated) XSS data achieved 99.8% accuracy.
However, when tested against obfuscated XSS samples, accuracy dropped to 81.9%,
underscoring the importance of training ML models with obfuscated data to
improve their effectiveness in detecting XSS attacks. A significant challenge
is to generate highly complex obfuscated code despite the availability of
several public tools. These tools can only produce obfuscation up to certain
levels of complexity.
  In our proposed system, we fine-tune a Large Language Model (LLM) to generate
complex obfuscated XSS payloads automatically. By transforming original XSS
samples into diverse obfuscated variants, we create challenging training data
for ML model evaluation. Our approach achieved a 99.5% accuracy rate with the
obfuscated dataset. We also found that the obfuscated samples generated by the
LLMs were 28.1% more complex than those created by other tools, significantly
improving the model's ability to handle advanced XSS attacks and making it more
effective for real-world application security.

</details>

### [15] [Phishing URL Detection using Bi-LSTM](https://arxiv.org/abs/2504.21049)
*Sneha Baskota*

Main category: cs.CR

TLDR: 提出了一种基于Bi-LSTM的深度学习模型，用于分类URL为良性、钓鱼、篡改或恶意软件四类，准确率达97%。


<details>
  <summary>Details</summary>
Motivation: 传统钓鱼检测系统存在高误报率和攻击类型识别有限的问题。

Method: 使用双向长短期记忆网络（Bi-LSTM）分析URL序列数据，捕捉上下文信息。

Result: 在超过65万条URL的数据集上，模型准确率达97%，显著优于传统方法。

Conclusion: Bi-LSTM模型能有效提升钓鱼攻击检测的准确性和适应性。

Abstract: Phishing attacks threaten online users, often leading to data breaches,
financial losses, and identity theft. Traditional phishing detection systems
struggle with high false positive rates and are usually limited by the types of
attacks they can identify. This paper proposes a deep learning-based approach
using a Bidirectional Long Short-Term Memory (Bi-LSTM) network to classify URLs
into four categories: benign, phishing, defacement, and malware. The model
leverages sequential URL data and captures contextual information, improving
the accuracy of phishing detection. Experimental results on a dataset
comprising over 650,000 URLs demonstrate the model's effectiveness, achieving
97% accuracy and significant improvements over traditional techniques.

</details>

### [16] [SFIBA: Spatial-based Full-target Invisible Backdoor Attacks](https://arxiv.org/abs/2504.21052)
*Yangxu Yin,Honglong Chen,Yudong Gao,Peng Sun,Zhishuai Li,Weifeng Liu*

Main category: cs.CR

TLDR: SFIBA是一种基于空间的全目标隐形后门攻击方法，通过限制触发器的空间区域和形态，结合频域注入技术，实现多目标攻击的高效性和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 多目标后门攻击在现实场景中因触发器缺乏特异性和隐蔽性而受限，SFIBA旨在解决这些问题。

Method: 采用快速傅里叶变换和离散小波变换提取特征，结合奇异值分解注入触发器，并通过像素空间选择性过滤和视觉调整优化隐蔽性。

Result: SFIBA在多个数据集和模型上表现出优异的攻击性能和隐蔽性，同时保持良性样本的模型性能，并能绕过现有防御。

Conclusion: SFIBA为多目标后门攻击提供了一种高效且隐蔽的解决方案，具有实际应用潜力。

Abstract: Multi-target backdoor attacks pose significant security threats to deep
neural networks, as they can preset multiple target classes through a single
backdoor injection. This allows attackers to control the model to misclassify
poisoned samples with triggers into any desired target class during inference,
exhibiting superior attack performance compared with conventional backdoor
attacks. However, existing multi-target backdoor attacks fail to guarantee
trigger specificity and stealthiness in black-box settings, resulting in two
main issues. First, they are unable to simultaneously target all classes when
only training data can be manipulated, limiting their effectiveness in
realistic attack scenarios. Second, the triggers often lack visual
imperceptibility, making poisoned samples easy to detect. To address these
problems, we propose a Spatial-based Full-target Invisible Backdoor Attack,
called SFIBA. It restricts triggers for different classes to specific local
spatial regions and morphologies in the pixel space to ensure specificity,
while employing a frequency-domain-based trigger injection method to guarantee
stealthiness. Specifically, for injection of each trigger, we first apply fast
fourier transform to obtain the amplitude spectrum of clean samples in local
spatial regions. Then, we employ discrete wavelet transform to extract the
features from the amplitude spectrum and use singular value decomposition to
integrate the trigger. Subsequently, we selectively filter parts of the trigger
in pixel space to implement trigger morphology constraints and adjust injection
coefficients based on visual effects. We conduct experiments on multiple
datasets and models. The results demonstrate that SFIBA can achieve excellent
attack performance and stealthiness, while preserving the model's performance
on benign samples, and can also bypass existing backdoor defenses.

</details>

### [17] [FFCBA: Feature-based Full-target Clean-label Backdoor Attacks](https://arxiv.org/abs/2504.21054)
*Yangxu Yin,Honglong Chen,Yudong Gao,Peng Sun,Liantao Wu,Zhe Li,Weifeng Liu*

Main category: cs.CR

TLDR: 论文提出了一种基于特征的多目标干净标签后门攻击方法（FFCBA），包括两种范式（FSBA和FMBA），解决了现有多目标攻击的高污染率和隐蔽性问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有多目标后门攻击多为脏标签范式，易被检测且污染率高；干净标签攻击虽隐蔽但性能不稳定且难以扩展。

Method: FFCBA包含FSBA和FMBA：FSBA利用类条件自编码器生成噪声触发器，FMBA通过两阶段训练生成强目标类特征的触发器。

Result: 实验表明FFCBA攻击性能优异，并对先进防御方法具有鲁棒性。

Conclusion: FFCBA为多目标干净标签后门攻击提供了高效且隐蔽的解决方案。

Abstract: Backdoor attacks pose a significant threat to deep neural networks, as
backdoored models would misclassify poisoned samples with specific triggers
into target classes while maintaining normal performance on clean samples.
Among these, multi-target backdoor attacks can simultaneously target multiple
classes. However, existing multi-target backdoor attacks all follow the
dirty-label paradigm, where poisoned samples are mislabeled, and most of them
require an extremely high poisoning rate. This makes them easily detectable by
manual inspection. In contrast, clean-label attacks are more stealthy, as they
avoid modifying the labels of poisoned samples. However, they generally
struggle to achieve stable and satisfactory attack performance and often fail
to scale effectively to multi-target attacks. To address this issue, we propose
the Feature-based Full-target Clean-label Backdoor Attacks (FFCBA) which
consists of two paradigms: Feature-Spanning Backdoor Attacks (FSBA) and
Feature-Migrating Backdoor Attacks (FMBA). FSBA leverages class-conditional
autoencoders to generate noise triggers that align perturbed in-class samples
with the original category's features, ensuring the effectiveness, intra-class
consistency, inter-class specificity and natural-feature correlation of
triggers. While FSBA supports swift and efficient attacks, its cross-model
attack capability is relatively weak. FMBA employs a two-stage
class-conditional autoencoder training process that alternates between using
out-of-class samples and in-class samples. This allows FMBA to generate
triggers with strong target-class features, making it highly effective for
cross-model attacks. We conduct experiments on multiple datasets and models,
the results show that FFCBA achieves outstanding attack performance and
maintains desirable robustness against the state-of-the-art backdoor defenses.

</details>

### [18] [Erased but Not Forgotten: How Backdoors Compromise Concept Erasure](https://arxiv.org/abs/2504.21072)
*Jonas Henry Grebe,Tobias Braun,Marcus Rohrbach,Anna Rohrbach*

Main category: cs.CR

TLDR: 论文提出了一种新的威胁模型Toxic Erasure (ToxE)，展示了现有机器遗忘技术如何被针对性后门攻击绕过，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型可能生成有害内容，现有遗忘技术存在安全漏洞，需要研究其潜在威胁。

Method: 提出ToxE威胁模型，设计两种后门攻击（针对文本编码器和交叉注意力层），并引入新型攻击DISA。

Result: 实验表明，ToxE攻击在名人身份和露骨内容遗忘任务中成功率显著（最高82%和9倍暴露增加）。

Conclusion: 当前遗忘策略存在严重安全漏洞，需进一步研究防御方法。

Abstract: The expansion of large-scale text-to-image diffusion models has raised
growing concerns about their potential to generate undesirable or harmful
content, ranging from fabricated depictions of public figures to sexually
explicit images. To mitigate these risks, prior work has devised machine
unlearning techniques that attempt to erase unwanted concepts through
fine-tuning. However, in this paper, we introduce a new threat model, Toxic
Erasure (ToxE), and demonstrate how recent unlearning algorithms, including
those explicitly designed for robustness, can be circumvented through targeted
backdoor attacks. The threat is realized by establishing a link between a
trigger and the undesired content. Subsequent unlearning attempts fail to erase
this link, allowing adversaries to produce harmful content. We instantiate ToxE
via two established backdoor attacks: one targeting the text encoder and
another manipulating the cross-attention layers. Further, we introduce Deep
Intervention Score-based Attack (DISA), a novel, deeper backdoor attack that
optimizes the entire U-Net using a score-based objective, improving the
attack's persistence across different erasure methods. We evaluate five recent
concept erasure methods against our threat model. For celebrity identity
erasure, our deep attack circumvents erasure with up to 82% success, averaging
57% across all erasure methods. For explicit content erasure, ToxE attacks can
elicit up to 9 times more exposed body parts, with DISA yielding an average
increase by a factor of 2.9. These results highlight a critical security gap in
current unlearning strategies.

</details>

### [19] [Federated One-Shot Learning with Data Privacy and Objective-Hiding](https://arxiv.org/abs/2504.21182)
*Maximilian Egger,Rüdiger Urbanke,Rawad Bitar*

Main category: cs.CR

TLDR: 提出一种联邦学习中同时保护客户端数据和联邦目标隐私的新方法，结合知识蒸馏和私有信息检索技术。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据和联邦目标的隐私保护研究不足，尤其是后者。

Method: 分三阶段：本地计算、客户端间共享、联邦目标安全检索，结合多方计算和图基私有信息检索。

Result: 方法在适应场景下优于现有工具。

Conclusion: 新方法有效解决了联邦学习中的双重隐私问题。

Abstract: Privacy in federated learning is crucial, encompassing two key aspects:
safeguarding the privacy of clients' data and maintaining the privacy of the
federator's objective from the clients. While the first aspect has been
extensively studied, the second has received much less attention.
  We present a novel approach that addresses both concerns simultaneously,
drawing inspiration from techniques in knowledge distillation and private
information retrieval to provide strong information-theoretic privacy
guarantees.
  Traditional private function computation methods could be used here; however,
they are typically limited to linear or polynomial functions. To overcome these
constraints, our approach unfolds in three stages. In stage 0, clients perform
the necessary computations locally. In stage 1, these results are shared among
the clients, and in stage 2, the federator retrieves its desired objective
without compromising the privacy of the clients' data. The crux of the method
is a carefully designed protocol that combines secret-sharing-based multi-party
computation and a graph-based private information retrieval scheme. We show
that our method outperforms existing tools from the literature when properly
adapted to this setting.

</details>

### [20] [SecRepoBench: Benchmarking LLMs for Secure Code Generation in Real-World Repositories](https://arxiv.org/abs/2504.21205)
*Connor Dilgren,Purva Chiniya,Luke Griffith,Yu Ding,Yizheng Chen*

Main category: cs.CR

TLDR: SecRepoBench是一个评估LLM在真实代码库中生成安全代码的基准，包含318个任务，覆盖15种CWE。研究发现，现有LLM在生成正确和安全代码方面表现不佳，且提示工程技术在代码库级别效果有限。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在真实代码库中生成安全代码的能力，填补现有基准的不足。

Method: 构建SecRepoBench基准，包含318个任务，评估19种LLM，并测试提示工程技术的有效性。

Result: LLM在生成安全代码方面表现不佳，提示工程技术效果有限，SecRepoBench是目前最难的基准。

Conclusion: 研究为提升LLM生成安全代码能力提供了方向，SecRepoBench是评估的重要工具。

Abstract: This paper introduces SecRepoBench, a benchmark to evaluate LLMs on secure
code generation in real-world repositories. SecRepoBench has 318 code
generation tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 19
state-of-the-art LLMs using our benchmark and find that the models struggle
with generating correct and secure code. In addition, the performance of LLMs
to generate self-contained programs as measured by prior benchmarks do not
translate to comparative performance at generating secure and correct code at
the repository level in SecRepoBench. We show that the state-of-the-art prompt
engineering techniques become less effective when applied to the repository
level secure code generation problem. We conduct extensive experiments,
including an agentic technique to generate secure code, to demonstrate that our
benchmark is currently the most difficult secure coding benchmark, compared to
previous state-of-the-art benchmarks. Finally, our comprehensive analysis
provides insights into potential directions for enhancing the ability of LLMs
to generate correct and secure code in real-world repositories.

</details>

### [21] [CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks](https://arxiv.org/abs/2504.21228)
*Rui Wang,Junda Wu,Yu Xia,Tong Yu,Ruiyi Zhang,Ryan Rossi,Lina Yao,Julian McAuley*

Main category: cs.CR

TLDR: 论文提出CachePrune方法，通过修剪KV缓存中的任务触发神经元，防御间接提示注入攻击，提升LLMs的安全性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）易受间接提示注入攻击，导致模型偏离用户指令。研究旨在解决这一漏洞，增强AI系统的安全性和鲁棒性。

Method: 提出CachePrune方法，利用特征归因和损失函数（基于DPO目标上界）识别并修剪KV缓存中的任务触发神经元，避免模型将输入提示误认为指令。

Result: 实验表明，CachePrune显著降低攻击成功率，同时保持响应质量。

Conclusion: CachePrune有效防御间接提示注入攻击，为开发更安全的AI系统提供了可行方案。

Abstract: Large Language Models (LLMs) are identified as being susceptible to indirect
prompt injection attack, where the model undesirably deviates from
user-provided instructions by executing tasks injected in the prompt context.
This vulnerability stems from LLMs' inability to distinguish between data and
instructions within a prompt. In this paper, we propose CachePrune that defends
against this attack by identifying and pruning task-triggering neurons from the
KV cache of the input prompt context. By pruning such neurons, we encourage the
LLM to treat the text spans of input prompt context as only pure data, instead
of any indicator of instruction following. These neurons are identified via
feature attribution with a loss function induced from an upperbound of the
Direct Preference Optimization (DPO) objective. We show that such a loss
function enables effective feature attribution with only a few samples. We
further improve on the quality of feature attribution, by exploiting an
observed triggering effect in instruction following. Our approach does not
impose any formatting on the original prompt or introduce extra test-time LLM
calls. Experiments show that CachePrune significantly reduces attack success
rates without compromising the response quality. Note: This paper aims to
defend against indirect prompt injection attacks, with the goal of developing
more secure and robust AI systems.

</details>

### [22] [How to Backdoor the Knowledge Distillation](https://arxiv.org/abs/2504.21323)
*Chen Wu,Qian Ma,Prasenjit Mitra,Sencun Zhu*

Main category: cs.CR

TLDR: 论文提出了一种新型攻击方法，通过毒化蒸馏数据集中的对抗样本，成功利用知识蒸馏过程的漏洞，揭示了传统认为安全的蒸馏过程存在潜在风险。


<details>
  <summary>Details</summary>
Motivation: 挑战传统认为知识蒸馏过程安全的假设，揭示其潜在漏洞。

Method: 通过毒化蒸馏数据集中的对抗样本，嵌入后门触发器，实现对学生模型的隐秘攻击。

Result: 实验证明该方法具有鲁棒性、隐秘性和高效性，成功利用干净教师模型攻击学生模型。

Conclusion: 揭示了知识蒸馏过程的新漏洞，为未来研究提供了防御方向。

Abstract: Knowledge distillation has become a cornerstone in modern machine learning
systems, celebrated for its ability to transfer knowledge from a large, complex
teacher model to a more efficient student model. Traditionally, this process is
regarded as secure, assuming the teacher model is clean. This belief stems from
conventional backdoor attacks relying on poisoned training data with backdoor
triggers and attacker-chosen labels, which are not involved in the distillation
process. Instead, knowledge distillation uses the outputs of a clean teacher
model to guide the student model, inherently preventing recognition or response
to backdoor triggers as intended by an attacker. In this paper, we challenge
this assumption by introducing a novel attack methodology that strategically
poisons the distillation dataset with adversarial examples embedded with
backdoor triggers. This technique allows for the stealthy compromise of the
student model while maintaining the integrity of the teacher model. Our
innovative approach represents the first successful exploitation of
vulnerabilities within the knowledge distillation process using clean teacher
models. Through extensive experiments conducted across various datasets and
attack settings, we demonstrate the robustness, stealthiness, and effectiveness
of our method. Our findings reveal previously unrecognized vulnerabilities and
pave the way for future research aimed at securing knowledge distillation
processes against backdoor attacks.

</details>

### [23] [Low latency FPGA implementation of twisted Edward curve cryptography hardware accelerator over prime field](https://arxiv.org/abs/2504.21342)
*Md Rownak Hossain,Md Sazedur Rahman,Kh Shahriya Zaman,Walid El Fezzani,Mohammad Arif Sobhan Bhuiyan,Chia Chao Kang,Teh Jia Yew,Mahdi H. Miraz*

Main category: cs.CR

TLDR: 本文提出了一种基于FPGA的Edwards25519曲线上的点乘法硬件架构，通过统一的点操作模块实现高效的点加法和点加倍，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 椭圆曲线密码硬件加速器的性能依赖于点乘法架构的效率，本文旨在设计一种高效且安全的点乘法架构。

Method: 开发了一个统一的点操作模块，使用投影坐标执行点加法和点加倍，并在FPGA平台上实现。

Result: 点乘法模块在Xilinx Virtex-5 FPGA上仅需1.4 ms，最高时钟频率为117.8 MHz，吞吐量为183.38 kbps。

Conclusion: 该架构在高速无线通信网络中具有快速数据加密的潜力。

Abstract: The performance of any elliptic curve cryptography hardware accelerator
significantly relies on the efficiency of the underlying point multiplication
(PM) architecture. This article presents a hardware implementation of
field-programmable gate array (FPGA) based modular arithmetic, group operation,
and point multiplication unit on the twisted Edwards curve (Edwards25519) over
the 256-bit prime field. An original hardware architecture of a unified point
operation module in projective coordinates that executes point addition and
point doubling within a single module has been developed, taking only 646 clock
cycles and ensuring a better security level than conventional approaches. The
proposed point multiplication module consumes 1.4 ms time, operating at a
maximal clock frequency of 117.8 MHz utilising 164,730 clock cycles having
183.38 kbps throughput on the Xilinx Virtex-5 FPGA platform for 256-bit length
of key. The comparative assessment of latency and throughput across various
related recent works indicates the effectiveness of our proposed PM
architecture. Finally, this high throughput and low latency PM architecture
will be a good candidate for rapid data encryption in high-speed wireless
communication networks.

</details>

### [24] [An Inversion Theorem for Buffered Linear Toeplitz (BLT) Matrices and Applications to Streaming Differential Privacy](https://arxiv.org/abs/2504.21413)
*H. Brendan McMahan,Krishna Pillutla*

Main category: cs.CR

TLDR: BLT矩阵的逆矩阵仍是BLT矩阵，且参数不同；提出了一种高效的O(d³)算法计算逆矩阵参数，支持自动微分优化。


<details>
  <summary>Details</summary>
Motivation: 研究BLT矩阵在流式差分隐私中的作用，解决其逆矩阵计算问题。

Method: 提出BLT逆矩阵定理，并设计O(d³)算法计算逆矩阵参数。

Result: 逆矩阵仍为BLT矩阵，算法高效且支持自动微分优化。

Conclusion: BLT矩阵的逆矩阵性质及其高效算法为隐私机制优化提供了新工具。

Abstract: Buffered Linear Toeplitz (BLT) matrices are a family of parameterized
lower-triangular matrices that play an important role in streaming differential
privacy with correlated noise. Our main result is a BLT inversion theorem: the
inverse of a BLT matrix is itself a BLT matrix with different parameters. We
also present an efficient and differentiable $O(d^3)$ algorithm to compute the
parameters of the inverse BLT matrix, where $d$ is the degree of the original
BLT (typically $d < 10$). Our characterization enables direct optimization of
BLT parameters for privacy mechanisms through automatic differentiation.

</details>

### [25] [Optimizing Mouse Dynamics for User Authentication by Machine Learning: Addressing Data Sufficiency, Accuracy-Practicality Trade-off, and Model Performance Challenges](https://arxiv.org/abs/2504.21415)
*Yi Wang,Chengyv Wu,Yang Liao,Maowei You*

Main category: cs.CR

TLDR: 提出了一种基于鼠标动态的用户认证方法，通过统计方法和深度学习框架优化数据量和行为模式捕捉，显著提升了认证效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统用户认证方法在可用性、成本和安全性方面存在局限，鼠标动态认证提供了一种低成本、非侵入且适应性强的解决方案。

Method: 使用高斯核密度估计和KL散度确定训练数据量，引入MAU优化分段长度，设计LT-AMouse框架结合1D-ResNet和GRU提取特征和建模时序依赖。

Result: 在Balabit和DFL数据集上，数据规模显著减少（DFL减少10倍），认证系统在盲攻击下的AUC达到98.52%（DFL）和94.65%（Balabit）。

Conclusion: 该方法在减少数据量和提升认证性能方面优于现有技术，为鼠标动态认证提供了实用高效的解决方案。

Abstract: User authentication is essential to ensure secure access to computer systems,
yet traditional methods face limitations in usability, cost, and security.
Mouse dynamics authentication, based on the analysis of users' natural
interaction behaviors with mouse devices, offers a cost-effective,
non-intrusive, and adaptable solution. However, challenges remain in
determining the optimal data volume, balancing accuracy and practicality, and
effectively capturing temporal behavioral patterns. In this study, we propose a
statistical method using Gaussian kernel density estimate (KDE) and
Kullback-Leibler (KL) divergence to estimate the sufficient data volume for
training authentication models. We introduce the Mouse Authentication Unit
(MAU), leveraging Approximate Entropy (ApEn) to optimize segment length for
efficient and accurate behavioral representation. Furthermore, we design the
Local-Time Mouse Authentication (LT-AMouse) framework, integrating 1D-ResNet
for local feature extraction and GRU for modeling long-term temporal
dependencies. Taking the Balabit and DFL datasets as examples, we significantly
reduced the data scale, particularly by a factor of 10 for the DFL dataset,
greatly alleviating the training burden. Additionally, we determined the
optimal input recognition unit length for the user authentication system on
different datasets based on the slope of Approximate Entropy. Training with
imbalanced samples, our model achieved a successful defense AUC 98.52% for
blind attack on the DFL dataset and 94.65% on the Balabit dataset, surpassing
the current sota performance.

</details>

### [26] [A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense](https://arxiv.org/abs/2504.21480)
*Yuchen Ding,Hongli Peng,Xiaoqi Li*

Main category: cs.CR

TLDR: 本文分析了以太坊智能合约中的两种关键安全漏洞（重入和整数溢出），探讨其机制、攻击场景及应对措施。


<details>
  <summary>Details</summary>
Motivation: 随着区块链技术的发展，智能合约的安全问题日益突出，漏洞可能导致重大财务损失，亟需深入研究。

Method: 通过分析Solidity编写的智能合约在EVM上的执行机制，复现攻击场景并评估防御措施。

Result: 识别了重入和整数溢出漏洞的机制，并验证了有效的防御策略。

Conclusion: 智能合约的安全风险需从开发、编译到执行全流程防范，研究为相关实践提供了参考。

Abstract: With the rapid advancement of blockchain technology, smart contracts have
enabled the implementation of increasingly complex functionalities. However,
ensuring the security of smart contracts remains a persistent challenge across
the stages of development, compilation, and execution. Vulnerabilities within
smart contracts not only undermine the security of individual applications but
also pose significant risks to the broader blockchain ecosystem, as
demonstrated by the growing frequency of attacks since 2016, resulting in
substantial financial losses. This paper provides a comprehensive analysis of
key security risks in Ethereum smart contracts, specifically those written in
Solidity and executed on the Ethereum Virtual Machine (EVM). We focus on two
prevalent and critical vulnerability types (reentrancy and integer overflow) by
examining their underlying mechanisms, replicating attack scenarios, and
assessing effective countermeasures.

</details>

### [27] [Confidential Serverless Computing](https://arxiv.org/abs/2504.21518)
*Patrick Sabanic,Masanori Misono,Teofil Bodea,Julian Pritzi,Michael Hackl,Dimitrios Stavrakakis,Pramod Bhatotia*

Main category: cs.CR

TLDR: Hacher是一个用于安全无服务器部署的机密计算系统，通过嵌套机密执行和轻量级LibOS优化，显著提升了安全性、性能和资源效率。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算在成本和部署上具有优势，但在不信任的云环境中管理敏感数据的安全性仍是一个挑战。

Method: Hacher采用嵌套机密执行和分离的客户操作系统，结合轻量级LibOS优化网络通信。

Result: 相比基于CVM的部署，Hacher的TCB缩小了4.3倍，延迟降低了15-93%，函数密度提高了907倍，通信和链式延迟显著减少。

Conclusion: Hacher为机密无服务器计算提供了一个实用且高效的解决方案。

Abstract: Although serverless computing offers compelling cost and deployment
simplicity advantages, a significant challenge remains in securely managing
sensitive data as it flows through the network of ephemeral function executions
in serverless computing environments within untrusted clouds. While
Confidential Virtual Machines (CVMs) offer a promising secure execution
environment, their integration with serverless architectures currently faces
fundamental limitations in key areas: security, performance, and resource
efficiency.
  We present Hacher, a confidential computing system for secure serverless
deployments to overcome these limitations. By employing nested confidential
execution and a decoupled guest OS within CVMs, Hacher runs each function in a
minimal "trustlet", significantly improving security through a reduced Trusted
Computing Base (TCB). Furthermore, by leveraging a data-centric I/O
architecture built upon a lightweight LibOS, Hacher optimizes network
communication to address performance and resource efficiency challenges.
  Our evaluation shows that compared to CVM-based deployments, Hacher has 4.3x
smaller TCB, improves end-to-end latency (15-93%), achieves higher function
density (up to 907x), and reduces inter-function communication (up to 27x) and
function chaining latency (16.7-30.2x); thus, Hacher offers a practical system
for confidential serverless computing.

</details>

### [28] [Padding Matters -- Exploring Function Detection in PE Files](https://arxiv.org/abs/2504.21520)
*Raphael Springer,Alexander Schmitz,Artur Leinweber,Tobias Urban,Christian Dietrich*

Main category: cs.CR

TLDR: FuncPEval是一个新的Windows PE文件数据集，用于评估函数检测工具，发现IDA表现最佳，而DeepDi速度最快。随机填充对多数工具有负面影响，但DeepDi受影响最小。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注Linux/ELF，Windows/PE二进制文件的研究不足，需要新的数据集和工具评估。

Method: 引入FuncPEval数据集，评估五种启发式和三种机器学习工具，分析随机填充的影响，并改进RNN和XDA工具。

Result: IDA在Chromium x64上F1-score最高（98.44%），DeepDi速度最快且对随机填充最不敏感。改进后的RNN和XDA性能提升约10%。

Conclusion: FuncPEval填补了Windows PE文件研究的空白，DeepDi在速度和鲁棒性上表现突出，随机填充对多数工具有显著影响。

Abstract: Function detection is a well-known problem in binary analysis. While previous
research has primarily focused on Linux/ELF, Windows/PE binaries have been
overlooked or only partially considered. This paper introduces FuncPEval, a new
dataset for Windows x86 and x64 PE files, featuring Chromium and the Conti
ransomware, along with ground truth data for 1,092,820 function starts.
Utilizing FuncPEval, we evaluate five heuristics-based (Ghidra, IDA, Nucleus,
rev.ng, SMDA) and three machine-learning-based (DeepDi, RNN, XDA) function
start detection tools. Among the tested tools, IDA achieves the highest
F1-score (98.44%) for Chromium x64, while DeepDi closely follows (97%) but
stands out as the fastest by a significant margin. Working towards
explainability, we examine the impact of padding between functions on the
detection results. Our analysis shows that all tested tools, except rev.ng, are
susceptible to randomized padding. The randomized padding significantly
diminishes the effectiveness for the RNN, XDA, and Nucleus. Among the
learning-based tools, DeepDi exhibits the least sensitivity and demonstrates
overall the fastest performance, while Nucleus is the most adversely affected
among non-learning-based tools. In addition, we improve the recurrent neural
network (RNN) proposed by Shin et al. and enhance the XDA tool, increasing the
F1-score by approximately 10%.

</details>

### [29] [CryptoUNets: Applying Convolutional Networks to Encrypted Data for Biomedical Image Segmentation](https://arxiv.org/abs/2504.21543)
*John Chiang*

Main category: cs.CR

TLDR: 本文提出了一种基于同态加密的隐私保护U-Net推理框架，首次实现了完全基于同态加密的U-Net推理。


<details>
  <summary>Details</summary>
Motivation: 解决在隐私保护场景下进行U-Net推理的技术挑战，尤其是数据编码问题。

Method: 采用灵活的Double Volley Revolver编码方案，设计HE友好的U-Net结构，包括平方激活函数、均值池化层和转置卷积层。

Result: 成功实现了基于同态加密的U-Net推理，并通过HEAAN库部署模型参数。

Conclusion: 该框架为隐私保护深度学习推理提供了可行的解决方案。

Abstract: In this manuscript, we demonstrate the feasibility of a privacy-preserving
U-Net deep learning inference framework, namely, homomorphic encryption-based
U-Net inference. That is, U-Net inference can be performed solely using
homomorphic encryption techniques. To our knowledge, this is the first work to
achieve support perform implement enable U-Net inference entirely based on
homomorphic encryption ?.
  The primary technical challenge lies in data encoding. To address this, we
employ a flexible encoding scheme, termed Double Volley Revolver, which enables
effective support for skip connections and upsampling operations within the
U-Net architecture.
  We adopt a tailored HE-friendly U-Net design incorporating square activation
functions, mean pooling layers, and transposed convolution layers (implemented
as ConvTranspose2d in PyTorch) with a kernel size of 2 and stride of 2. After
training the model in plaintext, we deploy the resulting parameters using the
HEAAN homomorphic encryption library to perform encrypted U-Net inference.

</details>

### [30] [Generative AI in Financial Institution: A Global Survey of Opportunities, Threats, and Regulation](https://arxiv.org/abs/2504.21574)
*Bikash Saha,Nanda Rani,Sandeep Kumar Shukla*

Main category: cs.CR

TLDR: 本文综述了生成式人工智能（GenAI）在金融领域的应用、机遇与风险，并探讨了相关监管和最佳实践。


<details>
  <summary>Details</summary>
Motivation: 探讨GenAI如何重塑金融行业，分析其带来的创新机会与潜在风险。

Method: 通过学术文献、行业案例和政策框架，综述GenAI在金融领域的应用及挑战。

Result: GenAI在客户服务、自动化工作流和数据分析等方面推动创新，但也带来网络安全和伦理问题。

Conclusion: 金融行业需平衡GenAI的潜力与风险，采取负责任的技术应用和监管措施。

Abstract: Generative Artificial Intelligence (GenAI) is rapidly reshaping the global
financial landscape, offering unprecedented opportunities to enhance customer
engagement, automate complex workflows, and extract actionable insights from
vast financial data. This survey provides an overview of GenAI adoption across
the financial ecosystem, examining how banks, insurers, asset managers, and
fintech startups worldwide are integrating large language models and other
generative tools into their operations. From AI-powered virtual assistants and
personalized financial advisory to fraud detection and compliance automation,
GenAI is driving innovation across functions. However, this transformation
comes with significant cybersecurity and ethical risks. We discuss emerging
threats such as AI-generated phishing, deepfake-enabled fraud, and adversarial
attacks on AI systems, as well as concerns around bias, opacity, and data
misuse. The evolving global regulatory landscape is explored in depth,
including initiatives by major financial regulators and international efforts
to develop risk-based AI governance. Finally, we propose best practices for
secure and responsible adoption - including explainability techniques,
adversarial testing, auditability, and human oversight. Drawing from academic
literature, industry case studies, and policy frameworks, this chapter offers a
perspective on how the financial sector can harness GenAI's transformative
potential while navigating the complex risks it introduces.

</details>

### [31] [Overlapping data in network protocols: bridging OS and NIDS reassembly gap](https://arxiv.org/abs/2504.21618)
*Lucas Aubard,Johan Mazel,Gilles Guette,Pierre Chifflier*

Main category: cs.CR

TLDR: 论文分析了NIDS对重叠数据块攻击的抵抗能力，扩展了现有攻击特征，提出基于Allen区间代数的新建模方法，并测试了多个OS和NIDS的重组行为。


<details>
  <summary>Details</summary>
Motivation: NIDS需与主机OS使用相同重组策略，否则易受攻击。现有攻击特征在重叠数据块场景中存在局限性。

Method: 1. 扩展插入和规避攻击特征；2. 提出基于Allen区间代数的新建模方法；3. 测试多个OS和NIDS的重组行为。

Result: 1. OS重组策略随时间变化；2. 所有测试的NIDS仍易受重叠数据块攻击。

Conclusion: 新建模方法能全面覆盖重叠测试用例，但NIDS仍需改进以应对此类攻击。

Abstract: IPv4, IPv6, and TCP have a common mechanism allowing one to split an original
data packet into several chunks. Such chunked packets may have overlapping data
portions and, OS network stack implementations may reassemble these overlaps
differently. A Network Intrusion Detection System (NIDS) that tries to
reassemble a given flow data has to use the same reassembly policy as the
monitored host OS; otherwise, the NIDS or the host may be subject to attack. In
this paper, we provide several contributions that enable us to analyze NIDS
resistance to overlapping data chunks-based attacks. First, we extend
state-of-the-art insertion and evasion attack characterizations to address
their limitations in an overlap-based context. Second, we propose a new way to
model overlap types using Allen's interval algebra, a spatio-temporal
reasoning. This new modeling allows us to formalize overlap test cases, which
ensures exhaustiveness in overlap coverage and eases the reasoning about and
use of reassembly policies. Third, we analyze the reassembly behavior of
several OSes and NIDSes when processing the modeled overlap test cases. We show
that 1) OS reassembly policies evolve over time and 2) all the tested NIDSes
are (still) vulnerable to overlap-based evasion and insertion attacks.

</details>

### [32] [Traceback of Poisoning Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2504.21668)
*Baolei Zhang,Haoran Xin,Minghong Fang,Zhuqing Liu,Biao Yi,Tong Li,Zheli Liu*

Main category: cs.CR

TLDR: RAGForensics是一种新型的追踪系统，用于识别RAG系统中被投毒的知识库文本，提升安全性。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法主要针对推理阶段，难以应对复杂的投毒攻击，因此需要一种新的追踪机制。

Method: RAGForensics通过迭代检索知识库子集，并利用定制提示引导LLM检测潜在投毒文本。

Result: 实验证明RAGForensics能有效对抗先进的投毒攻击。

Conclusion: RAGForensics为RAG系统提供了一种实用的防御机制，增强了其安全性。

Abstract: Large language models (LLMs) integrated with retrieval-augmented generation
(RAG) systems improve accuracy by leveraging external knowledge sources.
However, recent research has revealed RAG's susceptibility to poisoning
attacks, where the attacker injects poisoned texts into the knowledge database,
leading to attacker-desired responses. Existing defenses, which predominantly
focus on inference-time mitigation, have proven insufficient against
sophisticated attacks. In this paper, we introduce RAGForensics, the first
traceback system for RAG, designed to identify poisoned texts within the
knowledge database that are responsible for the attacks. RAGForensics operates
iteratively, first retrieving a subset of texts from the database and then
utilizing a specially crafted prompt to guide an LLM in detecting potential
poisoning texts. Empirical evaluations across multiple datasets demonstrate the
effectiveness of RAGForensics against state-of-the-art poisoning attacks. This
work pioneers the traceback of poisoned texts in RAG systems, providing a
practical and promising defense mechanism to enhance their security.

</details>

### [33] [Hoist with His Own Petard: Inducing Guardrails to Facilitate Denial-of-Service Attacks on Retrieval-Augmented Generation of LLMs](https://arxiv.org/abs/2504.21680)
*Pan Suo,Yu-Ming Shang,San-Chuan Guo,Xi Zhang*

Main category: cs.CR

TLDR: 论文提出了一种新型攻击方法MutedRAG，利用LLMs的安全防护机制作为攻击向量，通过注入极简的越狱文本来破坏RAG系统的可用性。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了LLMs内部的关键弱点，导致攻击范围和效率受限。本文旨在揭示并利用LLMs安全防护机制的漏洞。

Method: 提出MutedRAG攻击，通过注入如“如何制造炸弹”等越狱文本，触发LLMs的安全防护机制，使其拒绝合法查询。

Result: 实验表明，MutedRAG在多种场景下攻击成功率超过60%，且平均每个目标查询仅需不到一个恶意文本。

Conclusion: 当前防御机制不足以应对MutedRAG，亟需更鲁棒的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) integrates Large Language Models (LLMs)
with external knowledge bases, improving output quality while introducing new
security risks. Existing studies on RAG vulnerabilities typically focus on
exploiting the retrieval mechanism to inject erroneous knowledge or malicious
texts, inducing incorrect outputs. However, these approaches overlook critical
weaknesses within LLMs, leaving important attack vectors unexplored and
limiting the scope and efficiency of attacks. In this paper, we uncover a novel
vulnerability: the safety guardrails of LLMs, while designed for protection,
can also be exploited as an attack vector by adversaries. Building on this
vulnerability, we propose MutedRAG, a novel denial-of-service attack that
reversely leverages the guardrails of LLMs to undermine the availability of RAG
systems. By injecting minimalistic jailbreak texts, such as "\textit{How to
build a bomb}", into the knowledge base, MutedRAG intentionally triggers the
LLM's safety guardrails, causing the system to reject legitimate queries.
Besides, due to the high sensitivity of guardrails, a single jailbreak sample
can affect multiple queries, effectively amplifying the efficiency of attacks
while reducing their costs. Experimental results on three datasets demonstrate
that MutedRAG achieves an attack success rate exceeding 60% in many scenarios,
requiring only less than one malicious text to each target query on average. In
addition, we evaluate potential defense strategies against MutedRAG, finding
that some of current mechanisms are insufficient to mitigate this threat,
underscoring the urgent need for more robust solutions.

</details>

### [34] [XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs](https://arxiv.org/abs/2504.21700)
*Marco Arazzi,Vignesh Kumar Kembu,Antonino Nocera,Vinod P*

Main category: cs.CR

TLDR: 论文探讨了大型语言模型（LLMs）的安全威胁，提出了一种基于可解释AI的针对性越狱攻击方法XBreaking。


<details>
  <summary>Details</summary>
Motivation: LLMs在关键应用场景中的可靠采用受到安全威胁的阻碍，现有越狱方法多为生成-测试策略，缺乏对审查机制的深入理解。

Method: 通过比较分析审查与未审查模型的行为，提取可被利用的对齐模式，设计XBreaking攻击，通过针对性噪声注入突破安全约束。

Result: 实验验证了审查机制的弱点，并证明了XBreaking攻击的有效性和性能。

Conclusion: XBreaking为LLM安全研究提供了新视角，揭示了审查机制的潜在漏洞。

Abstract: Large Language Models are fundamental actors in the modern IT landscape
dominated by AI solutions. However, security threats associated with them might
prevent their reliable adoption in critical application scenarios such as
government organizations and medical institutions. For this reason, commercial
LLMs typically undergo a sophisticated censoring mechanism to eliminate any
harmful output they could possibly produce. In response to this, LLM
Jailbreaking is a significant threat to such protections, and many previous
approaches have already demonstrated its effectiveness across diverse domains.
Existing jailbreak proposals mostly adopt a generate-and-test strategy to craft
malicious input. To improve the comprehension of censoring mechanisms and
design a targeted jailbreak attack, we propose an Explainable-AI solution that
comparatively analyzes the behavior of censored and uncensored models to derive
unique exploitable alignment patterns. Then, we propose XBreaking, a novel
jailbreak attack that exploits these unique patterns to break the security
constraints of LLMs by targeted noise injection. Our thorough experimental
campaign returns important insights about the censoring mechanisms and
demonstrates the effectiveness and performance of our attack.

</details>

### [35] [Cert-SSB: Toward Certified Sample-Specific Backdoor Defense](https://arxiv.org/abs/2504.21730)
*Ting Qiao,Yingjia Wang,Xing Liu,Sixing Wu,Jianbing Li,Yiming Li*

Main category: cs.CR

TLDR: 论文提出了一种样本特定的认证后门防御方法Cert-SSB，通过优化每个样本的噪声幅度并动态调整认证区域，提高了防御性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于随机平滑的防御方法假设所有样本与决策边界等距，但实际中不成立，导致认证性能不佳。

Method: Cert-SSB使用随机梯度上升优化每个样本的噪声幅度，训练多个平滑模型，并通过动态调整认证区域的存储更新方法进行认证。

Result: 在多个基准数据集上的实验证明了Cert-SSB的有效性。

Conclusion: Cert-SSB通过样本特定的噪声优化和动态认证方法，显著提升了防御后门攻击的性能。

Abstract: Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an
attacker manipulates a small portion of the training data to implant hidden
backdoors into the model. The compromised model behaves normally on clean
samples but misclassifies backdoored samples into the attacker-specified target
class, posing a significant threat to real-world DNN applications. Currently,
several empirical defense methods have been proposed to mitigate backdoor
attacks, but they are often bypassed by more advanced backdoor techniques. In
contrast, certified defenses based on randomized smoothing have shown promise
by adding random noise to training and testing samples to counteract backdoor
attacks. In this paper, we reveal that existing randomized smoothing defenses
implicitly assume that all samples are equidistant from the decision boundary.
However, it may not hold in practice, leading to suboptimal certification
performance. To address this issue, we propose a sample-specific certified
backdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic
gradient ascent to optimize the noise magnitude for each sample, ensuring a
sample-specific noise level that is then applied to multiple poisoned training
sets to retrain several smoothed models. After that, Cert-SSB aggregates the
predictions of multiple smoothed models to generate the final robust
prediction. In particular, in this case, existing certification methods become
inapplicable since the optimized noise varies across different samples. To
conquer this challenge, we introduce a storage-update-based certification
method, which dynamically adjusts each sample's certification region to improve
certification performance. We conduct extensive experiments on multiple
benchmark datasets, demonstrating the effectiveness of our proposed method. Our
code is available at https://github.com/NcepuQiaoTing/Cert-SSB.

</details>

### [36] [Bilateral Differentially Private Vertical Federated Boosted Decision Trees](https://arxiv.org/abs/2504.21739)
*Bokang Zhang,Zhikun Zhang,Haodong Jiang,Yang Liu,Lihao Zheng,Yuxiao Zhou,Shuaiting Huang,Junfeng Wu*

Main category: cs.CR

TLDR: 提出了一种名为MaskedXGBoost的垂直联邦XGBoost变体，通过双边差分隐私保护隐私，噪声设计优化了效用和效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中XGBoost的隐私保护方法通常缺乏严格的隐私保证且计算成本高。

Method: 采用双边差分隐私，设计噪声扰动中间信息，噪声部分位于XGBoost分裂评分计算的零空间。

Result: 在多个数据集上验证了算法在效用和效率上的优越性。

Conclusion: MaskedXGBoost在隐私保护和性能上优于现有方法。

Abstract: Federated learning is a distributed machine learning paradigm that enables
collaborative training across multiple parties while ensuring data privacy.
Gradient Boosting Decision Trees (GBDT), such as XGBoost, have gained
popularity due to their high performance and strong interpretability.
Therefore, there has been a growing interest in adapting XGBoost for use in
federated settings via cryptographic techniques. However, it should be noted
that these approaches may not always provide rigorous theoretical privacy
guarantees, and they often come with a high computational cost in terms of time
and space requirements. In this paper, we propose a variant of vertical
federated XGBoost with bilateral differential privacy guarantee: MaskedXGBoost.
We build well-calibrated noise to perturb the intermediate information to
protect privacy. The noise is structured with part of its ingredients in the
null space of the arithmetical operation for splitting score evaluation in
XGBoost, helping us achieve consistently better utility than other perturbation
methods and relatively lower overhead than encryption-based techniques. We
provide theoretical utility analysis and empirically verify privacy
preservation. Compared with other algorithms, our algorithm's superiority in
both utility and efficiency has been validated on multiple datasets.

</details>

### [37] [VDDP: Verifiable Distributed Differential Privacy under the Client-Server-Verifier Setup](https://arxiv.org/abs/2504.21752)
*Haochen Sun,Xi He*

Main category: cs.CR

TLDR: 论文提出了一种可验证的分布式差分隐私（VDDP）框架，解决了服务器在分布式环境中可能的不忠实执行问题，并提出了两种高效机制。


<details>
  <summary>Details</summary>
Motivation: 差分隐私（DP）在分布式环境中易受服务器不忠实执行的影响，例如噪声采样错误或生成相关噪声。

Method: 提出了VDDP定义，结合零知识证明（ZKP）与DP，开发了两种高效机制：VDDLM和VRR。

Result: VDDLM在证明生成效率上提升了4×10^5倍，误差仅0.1-0.2倍；VRR在通信成本和验证开销上降低了5000倍。

Conclusion: VDDP框架有效解决了分布式DP的验证问题，同时展示了ZKP与DP的关系，提供了高效且可验证的解决方案。

Abstract: Despite differential privacy (DP) often being considered the de facto
standard for data privacy, its realization is vulnerable to unfaithful
execution of its mechanisms by servers, especially in distributed settings.
Specifically, servers may sample noise from incorrect distributions or generate
correlated noise while appearing to follow established protocols. This work
analyzes these malicious behaviors in a general differential privacy framework
within a distributed client-server-verifier setup. To address these adversarial
problems, we propose a novel definition called Verifiable Distributed
Differential Privacy (VDDP) by incorporating additional verification
mechanisms. We also explore the relationship between zero-knowledge proofs
(ZKP) and DP, demonstrating that while ZKPs are sufficient for achieving DP
under verifiability requirements, they are not necessary. Furthermore, we
develop two novel and efficient mechanisms that satisfy VDDP: (1) the
Verifiable Distributed Discrete Laplacian Mechanism (VDDLM), which offers up to
a $4 \times 10^5$x improvement in proof generation efficiency with only
0.1-0.2x error compared to the previous state-of-the-art verifiable
differentially private mechanism; (2) an improved solution to Verifiable
Randomized Response (VRR) under local DP, a special case of VDDP, achieving up
a reduction of up to 5000x in communication costs and the verifier's overhead.

</details>

### [38] [LASHED: LLMs And Static Hardware Analysis for Early Detection of RTL Bugs](https://arxiv.org/abs/2504.21770)
*Baleegh Ahmad,Hammond Pearce,Ramesh Karri,Benjamin Tan*

Main category: cs.CR

TLDR: LASHED结合静态分析和大型语言模型（LLM）来改进硬件安全漏洞检测，通过LLM补充静态分析的不足，提高检测精度。


<details>
  <summary>Details</summary>
Motivation: 静态分析在检测早期硬件安全漏洞时效果有限，无法解释漏洞的安全影响。LLM可以填补这些空白，帮助识别相关资产、减少误报并解释漏洞。

Method: LASHED结合静态分析和LLM，利用上下文学习和提示工程优化检测过程。

Result: 在四个开源SoC上测试五种常见弱点枚举（CWE），87.5%的标记实例是可信的CWE。

Conclusion: 结合静态分析和LLM能显著提高硬件安全漏洞检测的精度，上下文学习和优化提示工程是关键。

Abstract: While static analysis is useful in detecting early-stage hardware security
bugs, its efficacy is limited because it requires information to form checks
and is often unable to explain the security impact of a detected vulnerability.
Large Language Models can be useful in filling these gaps by identifying
relevant assets, removing false violations flagged by static analysis tools,
and explaining the reported violations. LASHED combines the two approaches
(LLMs and Static Analysis) to overcome each other's limitations for hardware
security bug detection. We investigate our approach on four open-source SoCs
for five Common Weakness Enumerations (CWEs) and present strategies for
improvement with better prompt engineering. We find that 87.5% of instances
flagged by our recommended scheme are plausible CWEs. In-context learning and
asking the model to 'think again' improves LASHED's precision.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [Model Connectomes: A Generational Approach to Data-Efficient Language Models](https://arxiv.org/abs/2504.21047)
*Klemen Kotar,Greta Tuckute*

Main category: cs.LG

TLDR: 论文提出了一种结合进化与学习的框架，通过“外循环”进化塑造“内循环”学习，使人工网络更接近生物神经网络的特性。实验表明，继承进化生成的“模型连接组”在低数据环境下表现更优。


<details>
  <summary>Details</summary>
Motivation: 生物神经网络通过进化和个体学习共同塑造，而人工神经网络缺乏这种多代约束。研究旨在缩小两者差距。

Method: 提出双循环框架：外循环模拟进化，内循环模拟学习。在语言任务中，模型继承进化生成的连接组后，用1亿标记的语料库训练。

Result: 与对照组相比，连接组模型在自然语言处理任务、人类行为及脑数据对齐方面表现更优。

Conclusion: 模型连接组可作为低数据环境下的高效先验，缩小人工模型与生物神经网络的差距。

Abstract: Biological neural networks are shaped both by evolution across generations
and by individual learning within an organism's lifetime, whereas standard
artificial neural networks undergo a single, large training procedure without
inherited constraints. In this preliminary work, we propose a framework that
incorporates this crucial generational dimension - an "outer loop" of evolution
that shapes the "inner loop" of learning - so that artificial networks better
mirror the effects of evolution and individual learning in biological
organisms. Focusing on language, we train a model that inherits a "model
connectome" from the outer evolution loop before exposing it to a
developmental-scale corpus of 100M tokens. Compared with two closely matched
control models, we show that the connectome model performs better or on par on
natural language processing tasks as well as alignment to human behavior and
brain data. These findings suggest that a model connectome serves as an
efficient prior for learning in low-data regimes - narrowing the gap between
single-generation artificial models and biologically evolved neural networks.

</details>

### [40] [Multimodal Large Language Models for Medicine: A Comprehensive Survey](https://arxiv.org/abs/2504.21051)
*Jiarui Ye,Hao Tang*

Main category: cs.LG

TLDR: 本文综述了多模态大语言模型（MLLMs）在医疗健康领域的应用，包括背景介绍、工作原理、三大应用方向（医疗报告、诊断和治疗）、数据模式与评估基准，以及面临的挑战与解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着GPT-4的发布，MLLMs在医疗健康领域的潜力受到广泛关注，本文旨在系统梳理其应用现状与挑战。

Method: 通过综述330篇相关论文，总结MLLMs在医疗健康领域的应用方向，并提供具体案例和数据模式分析。

Result: MLLMs在医疗报告、诊断和治疗中展现出显著能力，但面临数据隐私、模型可解释性等挑战。

Conclusion: MLLMs在医疗健康领域具有巨大潜力，但需解决技术和伦理问题以实现更广泛应用。

Abstract: MLLMs have recently become a focal point in the field of artificial
intelligence research. Building on the strong capabilities of LLMs, MLLMs are
adept at addressing complex multi-modal tasks. With the release of GPT-4, MLLMs
have gained substantial attention from different domains. Researchers have
begun to explore the potential of MLLMs in the medical and healthcare domain.
In this paper, we first introduce the background and fundamental concepts
related to LLMs and MLLMs, while emphasizing the working principles of MLLMs.
Subsequently, we summarize three main directions of application within
healthcare: medical reporting, medical diagnosis, and medical treatment. Our
findings are based on a comprehensive review of 330 recent papers in this area.
We illustrate the remarkable capabilities of MLLMs in these domains by
providing specific examples. For data, we present six mainstream modes of data
along with their corresponding evaluation benchmarks. At the end of the survey,
we discuss the challenges faced by MLLMs in the medical and healthcare domain
and propose feasible methods to mitigate or overcome these issues.

</details>

### [41] [NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models](https://arxiv.org/abs/2504.21053)
*Yi Zhou,Wenpeng Xing,Dezhang Kong,Changting Lin,Meng Han*

Main category: cs.LG

TLDR: 提出了一种通过神经元分析和修改来解除大型语言模型（LLM）安全对齐的新方法，揭示了当前对齐技术的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过识别和修改负责安全约束的神经元，解除LLM的安全对齐，以揭示现有对齐技术的漏洞。

Method: 通过神经元激活分析、相似性神经元识别和神经元重新学习三个步骤，选择性修改神经元以移除安全约束。

Result: 实验表明，该方法能以最小微调有效移除安全约束，暴露了对齐技术的脆弱性。

Conclusion: 强调了针对LLM对抗性微调攻击的防御需求。

Abstract: Safety alignment in large language models (LLMs) is achieved through
fine-tuning mechanisms that regulate neuron activations to suppress harmful
content. In this work, we propose a novel approach to induce disalignment by
identifying and modifying the neurons responsible for safety constraints. Our
method consists of three key steps: Neuron Activation Analysis, where we
examine activation patterns in response to harmful and harmless prompts to
detect neurons that are critical for distinguishing between harmful and
harmless inputs; Similarity-Based Neuron Identification, which systematically
locates the neurons responsible for safe alignment; and Neuron Relearning for
Safety Removal, where we fine-tune these selected neurons to restore the
model's ability to generate previously restricted responses. Experimental
results demonstrate that our method effectively removes safety constraints with
minimal fine-tuning, highlighting a critical vulnerability in current alignment
techniques. Our findings underscore the need for robust defenses against
adversarial fine-tuning attacks on LLMs.

</details>

### [42] [Modeling and Performance Analysis for Semantic Communications Based on Empirical Results](https://arxiv.org/abs/2504.21055)
*Shuai Ma,Bin Shen,Chuanhui Zhang,Youlong Wu,Hang Li,Shiyin Li,Guangming Shi,Naofal Al-Dhahir*

Main category: cs.LG

TLDR: 提出了一种Alpha-Beta-Gamma（ABG）公式，用于分析语义通信的性能，并基于此设计了自适应功率控制方案。


<details>
  <summary>Details</summary>
Motivation: 由于深度学习语义编码器和解码器的黑盒特性，分析语义通信性能是一个挑战。

Method: 提出ABG公式建模端到端性能与SNR的关系，并设计自适应功率控制方案。

Result: ABG公式能有效拟合图像重建任务中的性能指标，并优化功率分配方案。

Conclusion: ABG公式和功率分配方案在仿真中表现出优越性。

Abstract: Due to the black-box characteristics of deep learning based semantic encoders
and decoders, finding a tractable method for the performance analysis of
semantic communications is a challenging problem. In this paper, we propose an
Alpha-Beta-Gamma (ABG) formula to model the relationship between the end-to-end
measurement and SNR, which can be applied for both image reconstruction tasks
and inference tasks. Specifically, for image reconstruction tasks, the proposed
ABG formula can well fit the commonly used DL networks, such as SCUNet, and
Vision Transformer, for semantic encoding with the multi scale-structural
similarity index measure (MS-SSIM) measurement. Furthermore, we find that the
upper bound of the MS-SSIM depends on the number of quantized output bits of
semantic encoders, and we also propose a closed-form expression to fit the
relationship between the MS-SSIM and quantized output bits. To the best of our
knowledge, this is the first theoretical expression between end-to-end
performance metrics and SNR for semantic communications. Based on the proposed
ABG formula, we investigate an adaptive power control scheme for semantic
communications over random fading channels, which can effectively guarantee
quality of service (QoS) for semantic communications, and then design the
optimal power allocation scheme to maximize the energy efficiency of the
semantic communication system. Furthermore, by exploiting the bisection
algorithm, we develop the power allocation scheme to maximize the minimum QoS
of multiple users for OFDMA downlink semantic communication Extensive
simulations verify the effectiveness and superiority of the proposed ABG
formula and power allocation schemes.

</details>

### [43] [A Hamiltonian Higher-Order Elasticity Framework for Dynamic Diagnostics(2HOED)](https://arxiv.org/abs/2504.21062)
*Ngueuleweu Tiwang Gildas*

Main category: cs.LG

TLDR: 2HOED框架结合机器学习、区块链和因果推理，通过能量动力学模型揭示复杂系统的弹性、临界点和反馈机制。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如机器学习、区块链和因果推理）无法全面解析复杂系统的能量动力学特性，2HOED填补了这一空白。

Method: 基于经典力学扩展的Hamiltonian模型，分析系统的位置、速度、加速度和弹性，生成能量动力学地图。

Result: 2HOED揭示了线性模型无法捕捉的失效模式，提供了多阶段政策杠杆和动态能量映射。

Conclusion: 2HOED为跨学科研究提供了便携、可解释的工具，帮助决策者预测危机并设计适应性政策。

Abstract: Machine learning detects patterns, block chain guarantees trust and
immutability, and modern causal inference identifies directional linkages, yet
none alone exposes the full energetic anatomy of complex systems; the
Hamiltonian Higher Order Elasticity Dynamics(2HOED) framework bridges these
gaps. Grounded in classical mechanics but extended to Economics order
elasticity terms, 2HOED represents economic, social, and physical systems as
energy-based Hamiltonians whose position, velocity, acceleration, and jerk of
elasticity jointly determine systemic power, Inertia, policy sensitivity, and
marginal responses. Because the formalism is scaling free and coordinate
agnostic, it transfers seamlessly from financial markets to climate science,
from supply chain logistics to epidemiology, thus any discipline in which
adaptation and shocks coexist. By embedding standard econometric variables
inside a Hamiltonian, 2HOED enriches conventional economic analysis with
rigorous diagnostics of resilience, tipping points, and feedback loops,
revealing failure modes invisible to linear models. Wavelet spectra, phase
space attractors, and topological persistence diagrams derived from 2HOED
expose multistage policy leverage that machine learning detects only
empirically and block chain secures only after the fact. For economists,
physicians and other scientists, the method opens a new causal energetic
channel linking biological or mechanical elasticity to macro level outcomes.
Portable, interpretable, and computationally light, 2HOED turns data streams
into dynamical energy maps, empowering decision makers to anticipate crises,
design adaptive policies, and engineer robust systems delivering the predictive
punch of AI with the explanatory clarity of physics.

</details>

### [44] [Token-Level Prompt Mixture with Parameter-Free Routing for Federated Domain Generalization](https://arxiv.org/abs/2504.21063)
*Shuai Gong,Chaoran Cui,Xiaolin Dong,Xiushan Nie,Lei Zhu,Xiaojun Chang*

Main category: cs.LG

TLDR: TRIP提出了一种基于令牌级提示混合的无参数路由框架，用于联邦域泛化（FedDG），通过多提示专家分配和高效通信机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有FedDG方法中单一全局提示导致的性能下降问题，以及MoE方法中图像级专家分配粗糙和通信成本高的问题。

Method: TRIP采用令牌级提示混合，通过无参数路由机制（基于令牌聚类和最优传输）分配专家，并利用VLM的零样本泛化能力进行无偏学习。

Result: 在四个基准测试中，TRIP实现了最优的泛化性能，每轮通信仅需1K参数。

Conclusion: TRIP通过令牌级专家分配和高效通信机制，显著提升了FedDG的泛化能力和效率。

Abstract: Federated domain generalization (FedDG) aims to learn a globally
generalizable model from decentralized clients with heterogeneous data while
preserving privacy. Recent studies have introduced prompt learning to adapt
vision-language models (VLMs) in FedDG by learning a single global prompt.
However, such a one-prompt-fits-all learning paradigm typically leads to
performance degradation on personalized samples. Although the mixture of
experts (MoE) offers a promising solution for specialization, existing
MoE-based methods suffer from coarse image-level expert assignment and high
communication costs from parameterized routers. To address these limitations,
we propose TRIP, a Token-level prompt mixture with parameter-free routing
framework for FedDG, which treats multiple prompts as distinct experts. Unlike
existing image-level routing designs, TRIP assigns different tokens within an
image to specific experts. To ensure communication efficiency, TRIP
incorporates a parameter-free routing mechanism based on token clustering and
optimal transport. The instance-specific prompt is then synthesized by
aggregating experts, weighted by the number of tokens assigned to each.
Additionally, TRIP develops an unbiased learning strategy for prompt experts,
leveraging the VLM's zero-shot generalization capability. Extensive experiments
across four benchmarks demonstrate that TRIP achieves optimal generalization
results, with communication of only 1K parameters per round. Our code is
available at https://github.com/GongShuai8210/TRIP.

</details>

### [45] [Frequency Feature Fusion Graph Network For Depression Diagnosis Via fNIRS](https://arxiv.org/abs/2504.21064)
*Chengkai Yang,Xingping Dong,Xiaofen Zong*

Main category: cs.LG

TLDR: 提出了一种基于离散傅里叶变换（DFT）的新型抑郁症诊断生物标志物，并设计了一种基于TGCN的图网络架构，显著提升了诊断效果。


<details>
  <summary>Details</summary>
Motivation: 现有GNN模型因缺乏稳健的时态生物标志物而效果受限，需开发更有效的诊断方法。

Method: 利用DFT设计生物标志物，构建TGCN架构，使用1,086名受试者数据集，并通过PSM优化数据。

Result: 新生物标志物提升了脑通道时态特征表示，F1分数显著提高，SHAP验证了模型可解释性。

Conclusion: 该研究为抑郁症诊断工具开发提供了有效方法，具有实际医疗应用潜力。

Abstract: Data-driven approaches for depression diagnosis have emerged as a significant
research focus in neuromedicine, driven by the development of relevant
datasets. Recently, graph neural network (GNN)-based models have gained
widespread adoption due to their ability to capture brain channel functional
connectivity from both spatial and temporal perspectives. However, their
effectiveness is hindered by the absence of a robust temporal biomarker. In
this paper, we introduce a novel and effective biomarker for depression
diagnosis by leveraging the discrete Fourier transform (DFT) and propose a
customized graph network architecture based on Temporal Graph Convolutional
Network (TGCN). Our model was trained on a dataset comprising 1,086 subjects,
which is over 10 times larger than previous datasets in the field of depression
diagnosis. Furthermore, to align with medical requirements, we performed
propensity score matching (PSM) to create a refined subset, referred to as the
PSM dataset. Experimental results demonstrate that incorporating our newly
designed biomarker enhances the representation of temporal characteristics in
brain channels, leading to improved F1 scores in both the real-world dataset
and the PSM dataset. This advancement has the potential to contribute to the
development of more effective depression diagnostic tools. In addition, we used
SHapley Additive exPlaination (SHAP) to validate the interpretability of our
model, ensuring its practical applicability in medical settings.

</details>

### [46] [A 3D pocket-aware and affinity-guided diffusion model for lead optimization](https://arxiv.org/abs/2504.21065)
*Anjie Qiao,Junjie Xie,Weifeng Huang,Hao Zhang,Jiahua Rao,Shuangjia Zheng,Yuedong Yang,Zhen Wang,Guo-Bo Li,Jinping Lei*

Main category: cs.LG

TLDR: Diffleop是一种3D口袋感知和亲和力引导的扩散模型，用于优化分子结合亲和力，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在分子优化中未能充分结合蛋白靶点的结合亲和力，Diffleop旨在解决这一问题。

Method: Diffleop通过显式结合蛋白-配体亲和力知识，引导去噪采样生成高亲和力分子。

Result: Diffleop在多项指标上优于基线模型，尤其在结合亲和力方面表现突出。

Conclusion: Diffleop为分子优化提供了一种高效且亲和力导向的新方法。

Abstract: Molecular optimization, aimed at improving binding affinity or other
molecular properties, is a crucial task in drug discovery that often relies on
the expertise of medicinal chemists. Recently, deep learning-based 3D
generative models showed promise in enhancing the efficiency of molecular
optimization. However, these models often struggle to adequately consider
binding affinities with protein targets during lead optimization. Herein, we
propose a 3D pocket-aware and affinity-guided diffusion model, named Diffleop,
to optimize molecules with enhanced binding affinity. The model explicitly
incorporates the knowledge of protein-ligand binding affinity to guide the
denoising sampling for molecule generation with high affinity. The
comprehensive evaluations indicated that Diffleop outperforms baseline models
across multiple metrics, especially in terms of binding affinity.

</details>

### [47] [A Brief Review for Compression and Transfer Learning Techniques in DeepFake Detection](https://arxiv.org/abs/2504.21066)
*Andreas Karathanasis,John Violos,Ioannis Kompatsiaris,Symeon Papadopoulos*

Main category: cs.LG

TLDR: 论文探讨了在边缘设备上部署深度伪造检测模型的方法，通过压缩技术和迁移学习解决资源限制问题。实验表明，高压缩率下性能仍可保持，但跨模型测试时存在领域泛化问题。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备上深度伪造检测模型的计算和内存资源限制问题，同时保持数据隐私。

Method: 采用压缩技术（如剪枝、知识蒸馏、量化）和迁移学习方法（如微调、适配器技术），并在多个数据集上评估。

Result: 高压缩率（90%）下性能不变，但跨模型测试时领域泛化能力不足。

Conclusion: 压缩和迁移学习在边缘设备上可行，但需解决跨模型泛化问题。

Abstract: Training and deploying deepfake detection models on edge devices offers the
advantage of maintaining data privacy and confidentiality by processing it
close to its source. However, this approach is constrained by the limited
computational and memory resources available at the edge. To address this
challenge, we explore compression techniques to reduce computational demands
and inference time, alongside transfer learning methods to minimize training
overhead. Using the Synthbuster, RAISE, and ForenSynths datasets, we evaluate
the effectiveness of pruning, knowledge distillation (KD), quantization,
fine-tuning, and adapter-based techniques. Our experimental results demonstrate
that both compression and transfer learning can be effectively achieved, even
with a high compression level of 90%, remaining at the same performance level
when the training and validation data originate from the same DeepFake model.
However, when the testing dataset is generated by DeepFake models not present
in the training set, a domain generalization issue becomes evident.

</details>

### [48] [R^2VFL: A Robust Random Vector Functional Link Network with Huber-Weighted Framework](https://arxiv.org/abs/2504.21069)
*Anuradha Kumari,Mushir Akhtar,P. N. Suganthan,M. Tanveer*

Main category: cs.LG

TLDR: 提出了一种名为R2VFL的鲁棒框架，通过Huber加权函数和类别概率机制，增强了RVFL神经网络对噪声和异常值的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统RVFL神经网络在处理噪声和异常值时表现不佳，假设所有数据样本贡献均等，导致模型性能下降。

Method: 采用Huber加权函数减少异常值影响，并结合类别概率机制为噪声数据点分配较低权重。提出了两种计算类别中心的方法（平均值和中位数），形成R2VFL-A和R2VFL-M两种变体。

Result: 在47个UCI数据集上验证了模型的优越性，并在EEG信号分类中表现出色。

Conclusion: R2VFL框架显著提升了模型对噪声和异常值的鲁棒性，具有实际应用价值。

Abstract: The random vector functional link (RVFL) neural network has shown significant
potential in overcoming the constraints of traditional artificial neural
networks, such as excessive computation time and suboptimal solutions. However,
RVFL faces challenges when dealing with noise and outliers, as it assumes all
data samples contribute equally. To address this issue, we propose a novel
robust framework, R2VFL, RVFL with Huber weighting function and class
probability, which enhances the model's robustness and adaptability by
effectively mitigating the impact of noise and outliers in the training data.
The Huber weighting function reduces the influence of outliers, while the class
probability mechanism assigns less weight to noisy data points, resulting in a
more resilient model. We explore two distinct approaches for calculating class
centers within the R2VFL framework: the simple average of all data points in
each class and the median of each feature, the later providing a robust
alternative by minimizing the effect of extreme values. These approaches give
rise to two novel variants of the model-R2VFL-A and R2VFL-M. We extensively
evaluate the proposed models on 47 UCI datasets, encompassing both binary and
multiclass datasets, and conduct rigorous statistical testing, which confirms
the superiority of the proposed models. Notably, the models also demonstrate
exceptional performance in classifying EEG signals, highlighting their
practical applicability in real-world biomedical domain.

</details>

### [49] [A Survey on Parameter-Efficient Fine-Tuning for Foundation Models in Federated Learning](https://arxiv.org/abs/2504.21099)
*Jieming Bian,Yuanzhe Peng,Lei Wang,Yin Huang,Jie Xu*

Main category: cs.LG

TLDR: 该论文综述了参数高效微调（PEFT）方法在联邦学习（FL）环境中的整合，系统分类并分析了现有方法，探讨了其在数据异构性和隐私保护等挑战中的应用，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 基础模型在AI领域表现优异，但微调成本高昂；PEFT方法通过选择性更新参数降低成本，而FL支持分布式协作训练。结合两者可解决资源与隐私问题。

Method: 将PEFT方法分为三类：加性PEFT（新增可训练参数）、选择性PEFT（仅微调部分参数）和重参数化PEFT（调整架构以高效更新）。分析其在FL环境中的表现。

Result: 综述了PEFT在FL中的应用，解决了数据异构性、通信效率、计算限制和隐私问题，并覆盖了自然语言处理和计算机视觉任务。

Conclusion: 未来研究方向包括扩展至更大基础模型、理论分析联邦PEFT方法，以及资源受限环境下的可持续方案。

Abstract: Foundation models have revolutionized artificial intelligence by providing
robust, versatile architectures pre-trained on large-scale datasets. However,
adapting these massive models to specific downstream tasks requires
fine-tuning, which can be prohibitively expensive in computational resources.
Parameter-Efficient Fine-Tuning (PEFT) methods address this challenge by
selectively updating only a small subset of parameters. Meanwhile, Federated
Learning (FL) enables collaborative model training across distributed clients
without sharing raw data, making it ideal for privacy-sensitive applications.
This survey provides a comprehensive review of the integration of PEFT
techniques within federated learning environments. We systematically categorize
existing approaches into three main groups: Additive PEFT (which introduces new
trainable parameters), Selective PEFT (which fine-tunes only subsets of
existing parameters), and Reparameterized PEFT (which transforms model
architectures to enable efficient updates). For each category, we analyze how
these methods address the unique challenges of federated settings, including
data heterogeneity, communication efficiency, computational constraints, and
privacy concerns. We further organize the literature based on application
domains, covering both natural language processing and computer vision tasks.
Finally, we discuss promising research directions, including scaling to larger
foundation models, theoretical analysis of federated PEFT methods, and
sustainable approaches for resource-constrained environments.

</details>

### [50] [SMOGAN: Synthetic Minority Oversampling with GAN Refinement for Imbalanced Regression](https://arxiv.org/abs/2504.21152)
*Shayan Alahyari,Mike Domaratzki*

Main category: cs.LG

TLDR: SMOGAN是一种用于不平衡回归的两阶段过采样框架，通过生成和过滤合成样本，显著提升了模型在稀疏区域的性能。


<details>
  <summary>Details</summary>
Motivation: 不平衡回归中目标变量的偏斜导致模型在稀疏区域表现不佳，现有方法生成的合成样本未能准确反映真实分布。

Method: SMOGAN分为两阶段：首先生成初始合成样本，然后通过DistGAN（一种分布感知的GAN）过滤和优化样本。

Result: 在23个不平衡数据集上的实验表明，SMOGAN显著优于未使用DistGAN过滤层的默认过采样方法。

Conclusion: SMOGAN通过两阶段框架有效解决了不平衡回归问题，提升了模型在稀疏区域的性能。

Abstract: Imbalanced regression refers to prediction tasks where the target variable is
skewed. This skewness hinders machine learning models, especially neural
networks, which concentrate on dense regions and therefore perform poorly on
underrepresented (minority) samples. Despite the importance of this problem,
only a few methods have been proposed for imbalanced regression. Many of the
available solutions for imbalanced regression adapt techniques from the class
imbalance domain, such as linear interpolation and the addition of Gaussian
noise, to create synthetic data in sparse regions. However, in many cases, the
underlying distribution of the data is complex and non-linear. Consequently,
these approaches generate synthetic samples that do not accurately represent
the true feature-target relationship. To overcome these limitations, we propose
SMOGAN, a two-step oversampling framework for imbalanced regression. In Stage
1, an existing oversampler generates initial synthetic samples in sparse target
regions. In Stage 2, we introduce DistGAN, a distribution-aware GAN that serves
as SMOGAN's filtering layer and refines these samples via adversarial loss
augmented with a Maximum Mean Discrepancy objective, aligning them with the
true joint feature-target distribution. Extensive experiments on 23 imbalanced
datasets show that SMOGAN consistently outperforms the default oversampling
method without the DistGAN filtering layer.

</details>

### [51] [Efficient LLMs with AMP: Attention Heads and MLP Pruning](https://arxiv.org/abs/2504.21174)
*Leandro Giusti Mugnaini,Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Victor Zacarias,Edson Bollis,Lucas Pellicer,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TLDR: AMP是一种新型结构化剪枝方法，通过移除LLMs中不太关键的结构（如MHA和MLP），高效压缩模型，同时保持预测能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的高计算成本和慢推理速度限制了其在资源受限环境中的部署，需要一种高效的压缩方法。

Method: AMP通过将输入数据投影到权重上评估结构重要性，灵活且高效地剪枝MHA和MLP。

Result: AMP在常识推理任务上超越现有技术1.49个百分点，实现30%剪枝率且对零样本任务性能影响最小，同时提升推理速度。

Conclusion: AMP是一种灵活高效的剪枝方法，适用于多种LLMs，适合资源受限环境部署。

Abstract: Deep learning drives a new wave in computing systems and triggers the
automation of increasingly complex problems. In particular, Large Language
Models (LLMs) have significantly advanced cognitive tasks, often matching or
even surpassing human-level performance. However, their extensive parameters
result in high computational costs and slow inference, posing challenges for
deployment in resource-limited settings. Among the strategies to overcome the
aforementioned challenges, pruning emerges as a successful mechanism since it
reduces model size while maintaining predictive ability. In this paper, we
introduce AMP: Attention Heads and MLP Pruning, a novel structured pruning
method that efficiently compresses LLMs by removing less critical structures
within Multi-Head Attention (MHA) and Multilayer Perceptron (MLP). By
projecting the input data onto weights, AMP assesses structural importance and
overcomes the limitations of existing techniques, which often fall short in
flexibility or efficiency. In particular, AMP surpasses the current
state-of-the-art on commonsense reasoning tasks by up to 1.49 percentage
points, achieving a 30% pruning ratio with minimal impact on zero-shot task
performance. Moreover, AMP also improves inference speeds, making it
well-suited for deployment in resource-constrained environments. We confirm the
flexibility of AMP on different families of LLMs, including LLaMA and Phi.

</details>

### [52] [GLIP-OOD: Zero-Shot Graph OOD Detection with Foundation Model](https://arxiv.org/abs/2504.21186)
*Haoyan Xu,Zhengtao Yao,Xuzhi Zhang,Ziyi Wang,Langzhou He,Yushun Dong,Philip S. Yu,Mengyuan Li,Yue Zhao*

Main category: cs.LG

TLDR: 该论文首次探索了图结构数据中的零样本OOD检测，利用图基础模型（GFM）和LLM生成的伪OOD标签，实现了无需节点监督的OOD检测，并在多个数据集上超越现有监督方法。


<details>
  <summary>Details</summary>
Motivation: 图结构数据中的零样本OOD检测尚未被充分研究，主要由于复杂的图关系结构和缺乏大规模预训练模型。

Method: 使用图基础模型（GFM）进行零样本OOD检测，并通过LLM生成伪OOD标签以捕捉ID和OOD类之间的语义边界。

Result: 在四个基准文本属性图数据集上实现了最先进的性能，无需任何节点级监督。

Conclusion: 该工作首次实现了完全零样本的节点级图OOD检测，为动态开放世界中的机器学习系统安全提供了新方法。

Abstract: Out-of-distribution (OOD) detection is critical for ensuring the safety and
reliability of machine learning systems, particularly in dynamic and open-world
environments. In the vision and text domains, zero-shot OOD detection - which
requires no training on in-distribution (ID) data - has made significant
progress through the use of large-scale pretrained models such as
vision-language models (VLMs) and large language models (LLMs). However,
zero-shot OOD detection in graph-structured data remains largely unexplored,
primarily due to the challenges posed by complex relational structures and the
absence of powerful, large-scale pretrained models for graphs. In this work, we
take the first step toward enabling zero-shot graph OOD detection by leveraging
a graph foundation model (GFM). We show that, when provided only with class
label names, the GFM can perform OOD detection without any node-level
supervision - outperforming existing supervised methods across multiple
datasets. To address the more practical setting where OOD label names are
unavailable, we introduce GLIP-OOD, a novel framework that employs LLMs to
generate semantically informative pseudo-OOD labels from unlabeled data. These
labels enable the GFM to capture nuanced semantic boundaries between ID and OOD
classes and perform fine-grained OOD detection - without requiring any labeled
nodes. Our approach is the first to enable node-level graph OOD detection in a
fully zero-shot setting, and achieves state-of-the-art performance on four
benchmark text-attributed graph datasets.

</details>

### [53] [LIFT: LLM-Based Pragma Insertion for HLS via GNN Supervised Fine-Tuning](https://arxiv.org/abs/2504.21187)
*Neha Prakriya,Zijian Ding,Yizhou Sun,Jason Cong*

Main category: cs.LG

TLDR: LIFT是一种基于大型语言模型（LLM）的HLS编码助手，通过自动生成性能关键pragma优化FPGA设计，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: FPGA在数据中心中应用广泛，但高性能优化仍需专家知识。LIFT旨在通过自动化优化解决这一挑战。

Method: LIFT结合LLM和图神经网络（GNN），利用LLM的序列建模能力和GNN的结构语义理解能力，自动生成优化pragma。

Result: LIFT平均性能提升3.52倍（优于AutoDSE）、2.16倍（优于HARP）和66倍（优于GPT-4o）。

Conclusion: LIFT通过LLM和GNN的结合，显著提升了FPGA设计的自动化优化能力，性能优于现有方法。

Abstract: FPGAs are increasingly adopted in datacenter environments for their
reconfigurability and energy efficiency. High-Level Synthesis (HLS) tools have
eased FPGA programming by raising the abstraction level from RTL to untimed
C/C++, yet attaining high performance still demands expert knowledge and
iterative manual insertion of optimization pragmas to modify the
microarchitecture. To address this challenge, we propose LIFT, a large language
model (LLM)-based coding assistant for HLS that automatically generates
performance-critical pragmas given a C/C++ design. We fine-tune the LLM by
tightly integrating and supervising the training process with a graph neural
network (GNN), combining the sequential modeling capabilities of LLMs with the
structural and semantic understanding of GNNs necessary for reasoning over code
and its control/data dependencies. On average, LIFT produces designs that
improve performance by 3.52x and 2.16x than prior state-of the art AutoDSE and
HARP respectively, and 66x than GPT-4o.

</details>

### [54] [Artificial Intelligence for Personalized Prediction of Alzheimer's Disease Progression: A Survey of Methods, Data Challenges, and Future Directions](https://arxiv.org/abs/2504.21189)
*Gulsah Hancerliogullari Koksalmis,Bulent Soykan,Laura J. Brattain,Hsin-Hsiung Huang*

Main category: cs.LG

TLDR: 本文综述了人工智能在个性化阿尔茨海默病（AD）进展预测中的应用，包括多模态数据整合、深度学习和生成模型，并探讨了当前挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: AD进展的个体差异大，需要个性化预测模型以改善预后和护理计划。AI能处理复杂多模态数据，为这一挑战提供解决方案。

Method: 综述了状态空间模型、循环神经网络、图神经网络和AI驱动的数字孪生等方法，并讨论了数据增强技术如VAE和GAN。

Result: 总结了当前方法的优缺点，强调多模态整合和模型可解释性的重要性。

Conclusion: 未来研究方向包括混合模型、因果推理和联邦学习，需解决外部验证、临床整合和伦理问题。

Abstract: Alzheimer's Disease (AD) is marked by significant inter-individual
variability in its progression, complicating accurate prognosis and
personalized care planning. This heterogeneity underscores the critical need
for predictive models capable of forecasting patient-specific disease
trajectories. Artificial Intelligence (AI) offers powerful tools to address
this challenge by analyzing complex, multi-modal, and longitudinal patient
data. This paper provides a comprehensive survey of AI methodologies applied to
personalized AD progression prediction. We review key approaches including
state-space models for capturing temporal dynamics, deep learning techniques
like Recurrent Neural Networks for sequence modeling, Graph Neural Networks
(GNNs) for leveraging network structures, and the emerging concept of AI-driven
digital twins for individualized simulation. Recognizing that data limitations
often impede progress, we examine common challenges such as high
dimensionality, missing data, and dataset imbalance. We further discuss
AI-driven mitigation strategies, with a specific focus on synthetic data
generation using Variational Autoencoders (VAEs) and Generative Adversarial
Networks (GANs) to augment and balance datasets. The survey synthesizes the
strengths and limitations of current approaches, emphasizing the trend towards
multimodal integration and the persistent need for model interpretability and
generalizability. Finally, we identify critical open challenges, including
robust external validation, clinical integration, and ethical considerations,
and outline promising future research directions such as hybrid models, causal
inference, and federated learning. This review aims to consolidate current
knowledge and guide future efforts in developing clinically relevant AI tools
for personalized AD prognostication.

</details>

### [55] [TT-LoRA MoE: Unifying Parameter-Efficient Fine-Tuning and Sparse Mixture-of-Experts](https://arxiv.org/abs/2504.21190)
*Pradip Kunwar,Minh N. Vu,Maanak Gupta,Mahmoud Abdelsalam,Manish Bhattarai*

Main category: cs.LG

TLDR: TT-LoRA MoE结合参数高效微调与稀疏MoE路由，通过分阶段训练和动态路由选择，显著提升大规模模型部署的计算效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 解决传统MoE方法在专家数量增加时的计算开销问题，同时避免多任务设置中的任务干扰和灾难性遗忘。

Method: 分两阶段训练：先独立训练轻量级张量低秩适配器（TT-LoRA专家），再冻结适配器并训练稀疏MoE路由器动态选择专家。

Result: 仅需少量参数（2% LoRA、0.3% Adapters、0.03% AdapterFusion），在多任务中性能优于AdapterFusion 4个点。

Conclusion: TT-LoRA MoE在保持内存效率的同时，实现了大规模专家池的扩展和任务级优化，适用于实际部署。

Abstract: We propose Tensor-Trained Low-Rank Adaptation Mixture of Experts (TT-LoRA
MoE), a novel computational framework integrating Parameter-Efficient
Fine-Tuning (PEFT) with sparse MoE routing to address scalability challenges in
large model deployments. Unlike traditional MoE approaches, which face
substantial computational overhead as expert counts grow, TT-LoRA MoE
decomposes training into two distinct, optimized stages. First, we
independently train lightweight, tensorized low-rank adapters (TT-LoRA
experts), each specialized for specific tasks. Subsequently, these expert
adapters remain frozen, eliminating inter-task interference and catastrophic
forgetting in multi-task setting. A sparse MoE router, trained separately,
dynamically leverages base model representations to select exactly one
specialized adapter per input at inference time, automating expert selection
without explicit task specification. Comprehensive experiments confirm our
architecture retains the memory efficiency of low-rank adapters, seamlessly
scales to large expert pools, and achieves robust task-level optimization. This
structured decoupling significantly enhances computational efficiency and
flexibility: uses only 2% of LoRA, 0.3% of Adapters and 0.03% of AdapterFusion
parameters and outperforms AdapterFusion by 4 value in multi-tasking, enabling
practical and scalable multi-task inference deployments.

</details>

### [56] [Graph Synthetic Out-of-Distribution Exposure with Large Language Models](https://arxiv.org/abs/2504.21198)
*Haoyan Xu,Zhengtao Yao,Ziyi Wang,Zhan Cheng,Xiyang Hu,Mengyuan Li,Yue Zhao*

Main category: cs.LG

TLDR: GOE-LLM利用大型语言模型（LLM）在无需真实OOD节点的情况下，通过生成伪OOD节点来提升图OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有图OOD检测方法依赖真实OOD节点，但获取这些样本成本高且不切实际。

Method: GOE-LLM通过零样本LLM标注识别伪OOD节点，并利用LLM生成合成OOD节点，用于训练ID分类器。

Result: GOE-LLM在多个基准数据集上显著优于不依赖OOD暴露的方法，性能接近依赖真实OOD数据的方法。

Conclusion: GOE-LLM为图OOD检测提供了一种无需真实OOD样本的有效解决方案。

Abstract: Out-of-distribution (OOD) detection in graphs is critical for ensuring model
robustness in open-world and safety-sensitive applications. Existing approaches
to graph OOD detection typically involve training an in-distribution (ID)
classifier using only ID data, followed by the application of post-hoc OOD
scoring techniques. Although OOD exposure - introducing auxiliary OOD samples
during training - has proven to be an effective strategy for enhancing
detection performance, current methods in the graph domain generally assume
access to a set of real OOD nodes. This assumption, however, is often
impractical due to the difficulty and cost of acquiring representative OOD
samples. In this paper, we introduce GOE-LLM, a novel framework that leverages
Large Language Models (LLMs) for OOD exposure in graph OOD detection without
requiring real OOD nodes. GOE-LLM introduces two pipelines: (1) identifying
pseudo-OOD nodes from the initially unlabeled graph using zero-shot LLM
annotations, and (2) generating semantically informative synthetic OOD nodes
via LLM-prompted text generation. These pseudo-OOD nodes are then used to
regularize the training of the ID classifier for improved OOD awareness. We
evaluate our approach across multiple benchmark datasets, showing that GOE-LLM
significantly outperforms state-of-the-art graph OOD detection methods that do
not use OOD exposure and achieves comparable performance to those relying on
real OOD data.

</details>

### [57] [FedHERO: A Federated Learning Approach for Node Classification Task on Heterophilic Graphs](https://arxiv.org/abs/2504.21206)
*Zihan Chen,Xingbo Fu,Yushun Dong,Jundong Li,Cong Shen*

Main category: cs.LG

TLDR: FedHERO是一个联邦图学习框架，旨在有效处理异质性图数据，通过双通道GNN和结构学习器提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有FGL方法假设客户端图数据是同质的，而异质性会导致模型聚合时性能下降。

Method: 提出FedHERO框架，采用双通道GNN和结构学习器，提取通用结构知识。

Result: 实验验证FedHERO在异质性图数据上优于现有方法。

Conclusion: FedHERO为处理不同节点邻居分布模式的图数据提供了新思路。

Abstract: Federated Graph Learning (FGL) empowers clients to collaboratively train
Graph neural networks (GNNs) in a distributed manner while preserving data
privacy. However, FGL methods usually require that the graph data owned by all
clients is homophilic to ensure similar neighbor distribution patterns of
nodes. Such an assumption ensures that the learned knowledge is consistent
across the local models from all clients. Therefore, these local models can be
properly aggregated as a global model without undermining the overall
performance. Nevertheless, when the neighbor distribution patterns of nodes
vary across different clients (e.g., when clients hold graphs with different
levels of heterophily), their local models may gain different and even conflict
knowledge from their node-level predictive tasks. Consequently, aggregating
these local models usually leads to catastrophic performance deterioration on
the global model. To address this challenge, we propose FedHERO, an FGL
framework designed to harness and share insights from heterophilic graphs
effectively. At the heart of FedHERO is a dual-channel GNN equipped with a
structure learner, engineered to discern the structural knowledge encoded in
the local graphs. With this specialized component, FedHERO enables the local
model for each client to identify and learn patterns that are universally
applicable across graphs with different patterns of node neighbor
distributions. FedHERO not only enhances the performance of individual client
models by leveraging both local and shared structural insights but also sets a
new precedent in this field to effectively handle graph data with various node
neighbor distribution patterns. We conduct extensive experiments to validate
the superior performance of FedHERO against existing alternatives.

</details>

### [58] [A Cost-Effective LLM-based Approach to Identify Wildlife Trafficking in Online Marketplaces](https://arxiv.org/abs/2504.21211)
*Juliana Barbosa,Ulhas Gondhali,Gohar Petrossian,Kinshuk Sharma,Sunandan Chakraborty,Jennifer Jacquet,Juliana Freire*

Main category: cs.LG

TLDR: 论文提出了一种利用大语言模型（LLMs）生成伪标签的低成本策略，用于构建高效的野生动物非法交易广告分类模型，显著降低了标注成本并提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 野生动物非法交易对生物多样性和公共健康造成严重威胁，而电子商务平台为非法交易提供了便利。通过分析这些平台上的数字痕迹，可以揭示非法交易活动并加以打击。然而，从海量广告中识别野生动物相关产品广告极具挑战性，且传统分类器需要高昂的标注成本。

Method: 提出了一种低成本策略：利用LLMs为少量数据生成伪标签，并基于这些标签训练专用分类模型。该方法还自动收集多样化和代表性的样本，以最小化标注成本。

Result: 实验表明，该方法构建的分类器F1分数高达95%，性能优于直接使用LLMs，且成本更低。

Conclusion: 该方法为野生动物非法交易分析提供了高效且经济的数据科学解决方案，能够支持多样化的研究需求。

Abstract: Wildlife trafficking remains a critical global issue, significantly impacting
biodiversity, ecological stability, and public health. Despite efforts to
combat this illicit trade, the rise of e-commerce platforms has made it easier
to sell wildlife products, putting new pressure on wild populations of
endangered and threatened species. The use of these platforms also opens a new
opportunity: as criminals sell wildlife products online, they leave digital
traces of their activity that can provide insights into trafficking activities
as well as how they can be disrupted. The challenge lies in finding these
traces. Online marketplaces publish ads for a plethora of products, and
identifying ads for wildlife-related products is like finding a needle in a
haystack. Learning classifiers can automate ad identification, but creating
them requires costly, time-consuming data labeling that hinders support for
diverse ads and research questions. This paper addresses a critical challenge
in the data science pipeline for wildlife trafficking analytics: generating
quality labeled data for classifiers that select relevant data. While large
language models (LLMs) can directly label advertisements, doing so at scale is
prohibitively expensive. We propose a cost-effective strategy that leverages
LLMs to generate pseudo labels for a small sample of the data and uses these
labels to create specialized classification models. Our novel method
automatically gathers diverse and representative samples to be labeled while
minimizing the labeling costs. Our experimental evaluation shows that our
classifiers achieve up to 95% F1 score, outperforming LLMs at a lower cost. We
present real use cases that demonstrate the effectiveness of our approach in
enabling analyses of different aspects of wildlife trafficking.

</details>

### [59] [ABG-NAS: Adaptive Bayesian Genetic Neural Architecture Search for Graph Representation Learning](https://arxiv.org/abs/2504.21254)
*Sixuan Wang,Jiao Yin,Jinli Cao,MingJian Tang,Hua Wang,Yanchun Zhang*

Main category: cs.LG

TLDR: ABG-NAS是一种自动化的图神经网络架构搜索框架，旨在高效学习图表示，通过三个关键组件（CASS、AGOS、BGTM）提升性能，并在基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNN）架构难以适应多样化和复杂的图结构，限制了其鲁棒性和泛化能力。

Method: ABG-NAS包含三个组件：CASS（探索传播和变换操作）、AGOS（动态平衡探索与利用）、BGTM（贝叶斯引导的超参数优化）。

Result: 在Cora、PubMed等基准数据集上，ABG-NAS优于手工设计的GNN和最先进的NAS方法。

Conclusion: ABG-NAS为图表示学习提供了可扩展和自适应的解决方案，具有广泛的应用潜力。

Abstract: Effective and efficient graph representation learning is essential for
enabling critical downstream tasks, such as node classification, link
prediction, and subgraph search. However, existing graph neural network (GNN)
architectures often struggle to adapt to diverse and complex graph structures,
limiting their ability to provide robust and generalizable representations. To
address this challenge, we propose ABG-NAS, a novel framework for automated
graph neural network architecture search tailored for efficient graph
representation learning. ABG-NAS encompasses three key components: a
Comprehensive Architecture Search Space (CASS), an Adaptive Genetic
Optimization Strategy (AGOS), and a Bayesian-Guided Tuning Module (BGTM). CASS
systematically explores diverse propagation (P) and transformation (T)
operations, enabling the discovery of GNN architectures capable of capturing
intricate graph characteristics. AGOS dynamically balances exploration and
exploitation, ensuring search efficiency and preserving solution diversity.
BGTM further optimizes hyperparameters periodically, enhancing the scalability
and robustness of the resulting architectures. Empirical evaluations on
benchmark datasets (Cora, PubMed, Citeseer, and CoraFull) demonstrate that
ABG-NAS consistently outperforms both manually designed GNNs and
state-of-the-art neural architecture search (NAS) methods. These results
highlight the potential of ABG-NAS to advance graph representation learning by
providing scalable and adaptive solutions for diverse graph structures. Our
code is publicly available at https://github.com/sserranw/ABG-NAS.

</details>

### [60] [Multi-Domain Causal Discovery in Bijective Causal Models](https://arxiv.org/abs/2504.21261)
*Kasra Jalaldoust,Saber Salehkaleybar,Negar Kiyavash*

Main category: cs.LG

TLDR: 论文提出了一种在多域设置下进行因果发现的方法，利用双射生成机制（BGM）放宽了功能假设的限制，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究多域设置下的因果发现问题，假设因果函数在不同域中保持不变，但外生噪声的分布可能变化。

Method: 采用双射生成机制（BGM），确保外生噪声与内生变量之间的函数关系是双射且可微的，并推导了统计测试以确定目标变量的父集。

Result: 实验表明，该方法在合成和真实数据集上均能有效发现因果图。

Conclusion: BGM方法在多域因果发现中具有更宽松的功能假设，且实验验证了其理论有效性。

Abstract: We consider the problem of causal discovery (a.k.a., causal structure
learning) in a multi-domain setting. We assume that the causal functions are
invariant across the domains, while the distribution of the exogenous noise may
vary. Under causal sufficiency (i.e., no confounders exist), we show that the
causal diagram can be discovered under less restrictive functional assumptions
compared to previous work. What enables causal discovery in this setting is
bijective generation mechanisms (BGM), which ensures that the functional
relation between the exogenous noise $E$ and the endogenous variable $Y$ is
bijective and differentiable in both directions at every level of the cause
variable $X = x$. BGM generalizes a variety of models including additive noise
model, LiNGAM, post-nonlinear model, and location-scale noise model. Further,
we derive a statistical test to find the parents set of the target variable.
Experiments on various synthetic and real-world datasets validate our
theoretical findings.

</details>

### [61] [Orthogonal Factor-Based Biclustering Algorithm (BCBOF) for High-Dimensional Data and Its Application in Stock Trend Prediction](https://arxiv.org/abs/2504.21289)
*Yan Huang,Da-Qing Zhang*

Main category: cs.LG

TLDR: 提出了一种基于正交因子的双聚类算法（BCBOF），用于解决高维数据中的稀疏性和局部结构破坏问题，并在股票预测中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统双聚类算法在高维数据中面临相似性度量失效和局部结构破坏的问题。

Method: 通过构建正交因子并在正交子空间中进行聚类，结合模糊规则生成交易策略。

Result: BCBOF算法优于现有方法，虚拟交易实验显示其策略能带来更高收益。

Conclusion: BCBOF有效解决了高维数据问题，并在股票预测中表现出色。

Abstract: Biclustering is an effective technique in data mining and pattern
recognition. Biclustering algorithms based on traditional clustering face two
fundamental limitations when processing high-dimensional data: (1) The distance
concentration phenomenon in high-dimensional spaces leads to data sparsity,
rendering similarity measures ineffective; (2) Mainstream linear dimensionality
reduction methods disrupt critical local structural patterns. To apply
biclustering to high-dimensional datasets, we propose an orthogonal
factor-based biclustering algorithm (BCBOF). First, we constructed orthogonal
factors in the vector space of the high-dimensional dataset. Then, we performed
clustering using the coordinates of the original data in the orthogonal
subspace as clustering targets. Finally, we obtained biclustering results of
the original dataset. Since dimensionality reduction was applied before
clustering, the proposed algorithm effectively mitigated the data sparsity
problem caused by high dimensionality. Additionally, we applied this
biclustering algorithm to stock technical indicator combinations and stock
price trend prediction. Biclustering results were transformed into fuzzy rules,
and we incorporated profit-preserving and stop-loss rules into the rule set,
ultimately forming a fuzzy inference system for stock price trend predictions
and trading signals. To evaluate the performance of BCBOF, we compared it with
existing biclustering methods using multiple evaluation metrics. The results
showed that our algorithm outperformed other biclustering techniques. To
validate the effectiveness of the fuzzy inference system, we conducted virtual
trading experiments using historical data from 10 A-share stocks. The
experimental results showed that the generated trading strategies yielded
higher returns for investors.

</details>

### [62] [Fairness in Graph Learning Augmented with Machine Learning: A Survey](https://arxiv.org/abs/2504.21296)
*Renqiang Luo,Ziqi Xu,Xikun Zhang,Qing Qing,Huafei Huang,Enyan Dai,Zhe Wang,Bo Yang*

Main category: cs.LG

TLDR: 论文探讨了图学习与机器学习结合（GL-ML）中的公平性问题，分析了其挑战并提出了四种改进公平性的关键技术。


<details>
  <summary>Details</summary>
Motivation: 传统图学习模型结合机器学习技术虽在多领域取得成功，但其复杂性可能导致不公平结果，尤其在关键应用中。

Method: 系统分析GL-ML中的公平挑战，探讨图学习机制与机器学习的相互作用，并提出四种改进公平性的技术。

Result: 揭示了公平问题的根源及其影响，为未来GL-ML公平性研究奠定基础。

Conclusion: GL-ML中的公平性挑战需进一步研究，以推动该领域的创新与发展。

Abstract: Augmenting specialised machine learning techniques into traditional graph
learning models has achieved notable success across various domains, including
federated graph learning, dynamic graph learning, and graph transformers.
However, the intricate mechanisms of these specialised techniques introduce
significant challenges in maintaining model fairness, potentially resulting in
discriminatory outcomes in high-stakes applications such as recommendation
systems, disaster response, criminal justice, and loan approval. This paper
systematically examines the unique fairness challenges posed by Graph Learning
augmented with Machine Learning (GL-ML). It highlights the complex interplay
between graph learning mechanisms and machine learning techniques, emphasising
how the augmentation of machine learning both enhances and complicates
fairness. Additionally, we explore four critical techniques frequently employed
to improve fairness in GL-ML methods. By thoroughly investigating the root
causes and broader implications of fairness challenges in this rapidly evolving
field, this work establishes a robust foundation for future research and
innovation in GL-ML fairness.

</details>

### [63] [Unsupervised Feature Transformation via In-context Generation, Generator-critic LLM Agents, and Duet-play Teaming](https://arxiv.org/abs/2504.21304)
*Nanxu Gong,Xinyuan Wang,Wangyang Ying,Haoyue Bai,Sixun Dong,Haifeng Chen,Yanjie Fu*

Main category: cs.LG

TLDR: 提出了一种基于LLM代理和上下文学习的生成器-批评者框架，用于无监督特征转换，提升数据准备和AI实用性。


<details>
  <summary>Details</summary>
Motivation: 在材料性能筛选等领域，数据维度高且标签获取成本高，现有方法无法高效处理无监督特征组合。

Method: 采用生成器-批评者双代理框架，通过诊断、生成和迭代优化三步实现无监督特征转换。

Result: 实验表明，该框架在特征转换效率、鲁棒性和实用性上优于监督基线方法。

Conclusion: 该框架为无监督特征转换提供了高效解决方案，并可扩展至人机协作场景。

Abstract: Feature transformation involves generating a new set of features from the
original dataset to enhance the data's utility. In certain domains like
material performance screening, dimensionality is large and collecting labels
is expensive and lengthy. It highly necessitates transforming feature spaces
efficiently and without supervision to enhance data readiness and AI utility.
However, existing methods fall short in efficient navigation of a vast space of
feature combinations, and are mostly designed for supervised settings. To fill
this gap, our unique perspective is to leverage a generator-critic duet-play
teaming framework using LLM agents and in-context learning to derive
pseudo-supervision from unsupervised data. The framework consists of three
interconnected steps: (1) Critic agent diagnoses data to generate actionable
advice, (2) Generator agent produces tokenized feature transformations guided
by the critic's advice, and (3) Iterative refinement ensures continuous
improvement through feedback between agents. The generator-critic framework can
be generalized to human-agent collaborative generation, by replacing the critic
agent with human experts. Extensive experiments demonstrate that the proposed
framework outperforms even supervised baselines in feature transformation
efficiency, robustness, and practical applicability across diverse datasets.

</details>

### [64] [Capturing Conditional Dependence via Auto-regressive Diffusion Models](https://arxiv.org/abs/2504.21314)
*Xunpeng Huang,Yujin Han,Difan Zou,Yian Ma,Tong Zhang*

Main category: cs.LG

TLDR: 扩散模型在图像和视频生成中表现优异，但难以捕捉现实世界中的高层次关系。本文研究了自回归扩散模型（AR扩散模型）以增强条件依赖结构的捕捉能力，并提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 扩散模型未能有效学习数据中的条件依赖结构，如物理规律或物体稳定性，因此需要改进。

Method: 研究自回归扩散模型（AR扩散模型）的理论采样误差，并在数据假设较弱的情况下验证其有效性。

Result: AR扩散模型在捕捉条件依赖结构时优于传统扩散模型，且推理时间仅适度增加。当数据无明显条件依赖时，AR扩散模型与传统模型表现相当。

Conclusion: AR扩散模型能有效捕捉数据中的条件依赖结构，适用于大规模应用，但在无明确依赖时与传统模型无显著差异。

Abstract: Diffusion models have demonstrated appealing performance in both image and
video generation. However, many works discover that they struggle to capture
important, high-level relationships that are present in the real world. For
example, they fail to learn physical laws from data, and even fail to
understand that the objects in the world exist in a stable fashion. This is due
to the fact that important conditional dependence structures are not adequately
captured in the vanilla diffusion models. In this work, we initiate an in-depth
study on strengthening the diffusion model to capture the conditional
dependence structures in the data. In particular, we examine the efficacy of
the auto-regressive (AR) diffusion models for such purpose and develop the
first theoretical results on the sampling error of AR diffusion models under
(possibly) the mildest data assumption. Our theoretical findings indicate that,
compared with typical diffusion models, the AR variant produces samples with a
reduced gap in approximating the data conditional distribution. On the other
hand, the overall inference time of the AR-diffusion models is only moderately
larger than that for the vanilla diffusion models, making them still practical
for large scale applications. We also provide empirical results showing that
when there is clear conditional dependence structure in the data, the AR
diffusion models captures such structure, whereas vanilla DDPM fails to do so.
On the other hand, when there is no obvious conditional dependence across
patches of the data, AR diffusion does not outperform DDPM.

</details>

### [65] [Q-function Decomposition with Intervention Semantics with Factored Action Spaces](https://arxiv.org/abs/2504.21326)
*Junkyu Lee,Tian Gao,Elliot Nelson,Miao Liu,Debarun Bhattacharjya,Songtao Lu*

Main category: cs.LG

TLDR: 提出了一种基于因果统计的动作分解强化学习方法，通过投影Q函数降低样本复杂度，在模型和无模型强化学习中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决离散组合动作空间的高维挑战，避免枚举所有动作组合。

Method: 利用因果统计中的无未观测混杂条件，定义投影子空间上的Q函数，提出动作分解强化学习框架。

Result: 在模型强化学习和实际离线环境中显著提高了样本效率。

Conclusion: 动作分解强化学习是一种高效的方法，适用于高维动作空间问题。

Abstract: Many practical reinforcement learning environments have a discrete factored
action space that induces a large combinatorial set of actions, thereby posing
significant challenges. Existing approaches leverage the regular structure of
the action space and resort to a linear decomposition of Q-functions, which
avoids enumerating all combinations of factored actions. In this paper, we
consider Q-functions defined over a lower dimensional projected subspace of the
original action space, and study the condition for the unbiasedness of
decomposed Q-functions using causal effect estimation from the no unobserved
confounder setting in causal statistics. This leads to a general scheme which
we call action decomposed reinforcement learning that uses the projected
Q-functions to approximate the Q-function in standard model-free reinforcement
learning algorithms. The proposed approach is shown to improve sample
complexity in a model-based reinforcement learning setting. We demonstrate
improvements in sample efficiency compared to state-of-the-art baselines in
online continuous control environments and a real-world offline sepsis
treatment environment.

</details>

### [66] [A Generalized Meta Federated Learning Framework with Theoretical Convergence Guarantees](https://arxiv.org/abs/2504.21327)
*Mohammad Vahid Jamali,Hamid Saber,Jung Hyun Bae*

Main category: cs.LG

TLDR: 论文提出了一种广义的元联邦学习框架，通过最小化任意数量微调步骤后的本地模型损失，改进了传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统元联邦学习方法仅优化单步微调后的模型性能，而在数据分布高度异构的情况下，代理可能需要多步微调。

Method: 提出广义框架，最小化代理在任意ν步微调后的本地模型损失，并设计了一种改进的FedAvg算法。

Result: 理论分析和实验表明，新方法在真实数据集上具有更高的准确性和更快的收敛速度。

Conclusion: 广义框架显著提升了模型个性化能力，尤其在异构数据分布下表现优异。

Abstract: Meta federated learning (FL) is a personalized variant of FL, where multiple
agents collaborate on training an initial shared model without exchanging raw
data samples. The initial model should be trained in a way that current or new
agents can easily adapt it to their local datasets after one or a few
fine-tuning steps, thus improving the model personalization. Conventional meta
FL approaches minimize the average loss of agents on the local models obtained
after one step of fine-tuning. In practice, agents may need to apply several
fine-tuning steps to adapt the global model to their local data, especially
under highly heterogeneous data distributions across agents. To this end, we
present a generalized framework for the meta FL by minimizing the average loss
of agents on their local model after any arbitrary number $\nu$ of fine-tuning
steps. For this generalized framework, we present a variant of the well-known
federated averaging (FedAvg) algorithm and conduct a comprehensive theoretical
convergence analysis to characterize the convergence speed as well as behavior
of the meta loss functions in both the exact and approximated cases. Our
experiments on real-world datasets demonstrate superior accuracy and faster
convergence for the proposed scheme compared to conventional approaches.

</details>

### [67] [Multi-level datasets training method in Physics-Informed Neural Networks](https://arxiv.org/abs/2504.21328)
*Yao-Hsuan Tsai,Hsiao-Tung Juan,Pao-Hsiung Chiu,Chao-An Lin*

Main category: cs.LG

TLDR: 提出了一种基于多网格方法的物理信息神经网络（PINN）改进方法，用于解决高频率和刚性问题的PDE求解，显著提高了精度和收敛性。


<details>
  <summary>Details</summary>
Motivation: PINN在处理高频率或刚性PDE问题时存在精度和收敛性问题，需要一种更高效的方法来改进训练效果。

Method: 采用多网格方法，通过不同层次的训练样本去除不同频率误差，避免复杂的神经网络结构调整和超参数调优。

Result: 在1D ODE和2D对流-扩散方程中验证了方法的有效性，对Lid-driven腔流问题的预测精度提高了30%至60%。

Conclusion: 该方法能有效解决复杂高频率PDE问题，甚至在Re=5000时仍能提供良好预测，展示了其广泛适用性。

Abstract: Physics-Informed Neural Networks have emerged as a promising methodology for
solving PDEs, gaining significant attention in computer science and various
physics-related fields. Despite being demonstrated the ability to incorporate
the physics of laws for versatile applications, PINNs still struggle with the
challenging problems which are stiff to be solved and/or have high-frequency
components in the solutions, resulting in accuracy and convergence issues. It
may not only increase computational costs, but also lead to accuracy loss or
solution divergence. In this study, an alternative approach is proposed to
mitigate the above-mentioned problems. Inspired by the multi-grid method in CFD
community, the underlying idea of the current approach is to efficiently remove
different frequency errors via training with different levels of training
samples, resulting in a simpler way to improve the training accuracy without
spending time in fine-tuning of neural network structures, loss weights as well
as hyperparameters. To demonstrate the efficacy of current approach, we first
investigate canonical 1D ODE with high-frequency component and 2D
convection-diffusion equation with V-cycle training strategy. Finally, the
current method is employed for the classical benchmark problem of steady
Lid-driven cavity flows at different Reynolds numbers, to investigate the
applicability and efficacy for the problem involved multiple modes of high and
low frequency. By virtue of various training sequence modes, improvement
through predictions lead to 30% to 60% accuracy improvement. We also
investigate the synergies between current method and transfer learning
techniques for more challenging problems (i.e., higher Re). From the present
results, it also revealed that the current framework can produce good
predictions even for the case of Re=5000, demonstrating the ability to solve
complex high-frequency PDEs.

</details>

### [68] [Generative QoE Modeling: A Lightweight Approach for Telecom Networks](https://arxiv.org/abs/2504.21353)
*Vinti Nayar,Kanica Sachdev,Brejesh Lall*

Main category: cs.LG

TLDR: 本文提出了一种轻量级生成建模框架，结合向量量化（VQ）和隐马尔可夫模型（HMM），用于高效预测QoE，平衡计算效率、可解释性和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有QoE预测方法主要依赖深度学习模型，但计算复杂且资源消耗大。本研究旨在提供一种轻量级替代方案，适用于资源受限或延迟敏感的环境。

Method: 采用VQ将连续网络特征转换为离散符号，再通过HMM建模时间序列，形成VQ-HMM框架。

Result: 实验表明，该方法在公开时间序列数据集上表现良好，适用于实时和资源受限场景。

Conclusion: VQ-HMM框架为复杂深度学习方法提供了可扩展的替代方案，特别适合计算资源有限或延迟敏感的应用。

Abstract: Quality of Experience (QoE) prediction plays a crucial role in optimizing
resource management and enhancing user satisfaction across both
telecommunication and OTT services. While recent advances predominantly rely on
deep learning models, this study introduces a lightweight generative modeling
framework that balances computational efficiency, interpretability, and
predictive accuracy. By validating the use of Vector Quantization (VQ) as a
preprocessing technique, continuous network features are effectively
transformed into discrete categorical symbols, enabling integration with a
Hidden Markov Model (HMM) for temporal sequence modeling. This VQ-HMM pipeline
enhances the model's capacity to capture dynamic QoE patterns while supporting
probabilistic inference on new and unseen data. Experimental results on
publicly available time-series datasets incorporating both objective indicators
and subjective QoE scores demonstrate the viability of this approach in
real-time and resource-constrained environments, where inference latency is
also critical. The framework offers a scalable alternative to complex deep
learning methods, particularly in scenarios with limited computational
resources or where latency constraints are critical.

</details>

### [69] [A comparative study of deep learning and ensemble learning to extend the horizon of traffic forecasting](https://arxiv.org/abs/2504.21358)
*Xiao Zheng,Saeed Asadi Bagloee,Majid Sarvi*

Main category: cs.LG

TLDR: 本文比较了多种机器学习方法在长期交通流量预测中的表现，发现时间嵌入技术对周期性建模至关重要，XGBoost在仅使用时间特征时表现优异。


<details>
  <summary>Details</summary>
Motivation: 长期交通预测是一个具有挑战性的开放问题，现有研究多关注短期预测，本文旨在填补这一空白。

Method: 开发了XGBoost和多种深度学习方法（如RNN和Transformer），并利用时间嵌入增强模型对季节性和事件因素的理解。

Result: 实验表明，随着预测时间延长，周期性建模比时间依赖捕获更重要；时间嵌入使RNN在30天预测中优于Informer 31.1%。

Conclusion: 研究结果为未来长期交通预测提供了参考，并强调了时间嵌入和XGBoost的实用性。

Abstract: Traffic forecasting is vital for Intelligent Transportation Systems, for
which Machine Learning (ML) methods have been extensively explored to develop
data-driven Artificial Intelligence (AI) solutions. Recent research focuses on
modelling spatial-temporal correlations for short-term traffic prediction,
leaving the favourable long-term forecasting a challenging and open issue. This
paper presents a comparative study on large-scale real-world signalized
arterials and freeway traffic flow datasets, aiming to evaluate promising ML
methods in the context of large forecasting horizons up to 30 days. Focusing on
modelling capacity for temporal dynamics, we develop one ensemble ML method,
eXtreme Gradient Boosting (XGBoost), and a range of Deep Learning (DL) methods,
including Recurrent Neural Network (RNN)-based methods and the state-of-the-art
Transformer-based method. Time embedding is leveraged to enhance their
understanding of seasonality and event factors. Experimental results highlight
that while the attention mechanism/Transformer framework is effective for
capturing long-range dependencies in sequential data, as the forecasting
horizon extends, the key to effective traffic forecasting gradually shifts from
temporal dependency capturing to periodicity modelling. Time embedding is
particularly effective in this context, helping naive RNN outperform Informer
by 31.1% for 30-day-ahead forecasting. Meanwhile, as an efficient and robust
model, XGBoost, while learning solely from time features, performs
competitively with DL methods. Moreover, we investigate the impacts of various
factors like input sequence length, holiday traffic, data granularity, and
training data size. The findings offer valuable insights and serve as a
reference for future long-term traffic forecasting research and the improvement
of AI's corresponding learning capabilities.

</details>

### [70] [Synergy-CLIP: Extending CLIP with Multi-modal Integration for Robust Representation Learning](https://arxiv.org/abs/2504.21375)
*Sangyeon Cho,Jangyeong Jeon,Mingi Kim,Junyeong Kim*

Main category: cs.LG

TLDR: Synergy-CLIP扩展了CLIP架构，通过整合视觉、文本和音频模态提升多模态表示学习，并引入VGG-sound+数据集解决数据平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注双模态交互，未能充分利用多模态数据的丰富性，且缺乏平衡的大规模数据集。

Method: 提出Synergy-CLIP框架，对齐三种模态的潜在信息，并构建VGG-sound+数据集。

Result: 在零样本分类等任务中表现优于基线，并能重建缺失模态。

Conclusion: 为多模态表示学习提供了坚实基础，并开辟了新研究方向。

Abstract: Multi-modal representation learning has become a pivotal area in artificial
intelligence, enabling the integration of diverse modalities such as vision,
text, and audio to solve complex problems. However, existing approaches
predominantly focus on bimodal interactions, such as image-text pairs, which
limits their ability to fully exploit the richness of multi-modal data.
Furthermore, the integration of modalities in equal-scale environments remains
underexplored due to the challenges of constructing large-scale, balanced
datasets. In this study, we propose Synergy-CLIP, a novel framework that
extends the contrastive language-image pre-training (CLIP) architecture to
enhance multi-modal representation learning by integrating visual, textual, and
audio modalities. Unlike existing methods that focus on adapting individual
modalities to vanilla-CLIP, Synergy-CLIP aligns and captures latent information
across three modalities equally. To address the high cost of constructing
large-scale multi-modal datasets, we introduce VGG-sound+, a triple-modal
dataset designed to provide equal-scale representation of visual, textual, and
audio data. Synergy-CLIP is validated on various downstream tasks, including
zero-shot classification, where it outperforms existing baselines.
Additionally, we introduce a missing modality reconstruction task,
demonstrating Synergy-CLIP's ability to extract synergy among modalities in
realistic application scenarios. These contributions provide a robust
foundation for advancing multi-modal representation learning and exploring new
research directions.

</details>

### [71] [Sparse-to-Sparse Training of Diffusion Models](https://arxiv.org/abs/2504.21380)
*Inês Cardoso Oliveira,Decebal Constantin Mocanu,Luis A. Leiva*

Main category: cs.LG

TLDR: 本文提出了一种稀疏到稀疏训练范式，用于扩散模型（DMs），旨在提高训练和推理效率。实验表明，稀疏DMs在性能上优于或匹配密集DMs，同时显著减少可训练参数和计算量。


<details>
  <summary>Details</summary>
Motivation: 尽管DMs在图像合成等领域表现出色，但其计算资源需求高。此前工作主要关注推理效率，本文首次探索稀疏训练以提升整体效率。

Method: 采用三种方法（Static-DM、RigL-DM和MagRan-DM）在六个数据集上从头训练稀疏DMs（Latent Diffusion和ChiroDiff），研究稀疏性对性能的影响。

Result: 稀疏DMs在减少参数和计算量的同时，性能优于或匹配密集DMs，并确定了稀疏训练的安全有效值。

Conclusion: 稀疏到稀疏训练是提高DMs效率的有效方法，为未来研究提供了新方向。

Abstract: Diffusion models (DMs) are a powerful type of generative models that have
achieved state-of-the-art results in various image synthesis tasks and have
shown potential in other domains, such as natural language processing and
temporal data modeling. Despite their stable training dynamics and ability to
produce diverse high-quality samples, DMs are notorious for requiring
significant computational resources, both in the training and inference stages.
Previous work has focused mostly on increasing the efficiency of model
inference. This paper introduces, for the first time, the paradigm of
sparse-to-sparse training to DMs, with the aim of improving both training and
inference efficiency. We focus on unconditional generation and train sparse DMs
from scratch (Latent Diffusion and ChiroDiff) on six datasets using three
different methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of
sparsity in model performance. Our experiments show that sparse DMs are able to
match and often outperform their Dense counterparts, while substantially
reducing the number of trainable parameters and FLOPs. We also identify safe
and effective values to perform sparse-to-sparse training of DMs.

</details>

### [72] [FAST-Q: Fast-track Exploration with Adversarially Balanced State Representations for Counterfactual Action Estimation in Offline Reinforcement Learning](https://arxiv.org/abs/2504.21383)
*Pulkit Agrawal,Rukma Talwadker,Aditya Pareek,Tridib Mukherjee*

Main category: cs.LG

TLDR: FAST-Q提出了一种新颖的离线强化学习方法，通过梯度反转学习和Q值分解策略，解决了推荐系统中玩家心理和平台波动带来的挑战，显著提升了玩家收益和平台表现。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在推荐系统等高风险应用中面临状态空间稀疏和策略偏差的挑战，现有方法因泛化能力不足而限制学习效果。

Method: FAST-Q采用梯度反转学习构建平衡状态表示，支持离线反事实探索，并提出Q值分解策略进行多目标优化。

Result: 实验显示FAST-Q在玩家收益、生命周期价值、推荐驱动参与度等方面均优于现有方法，并显著降低了推荐成本。

Conclusion: FAST-Q通过创新方法有效解决了离线强化学习在高波动平台中的复杂问题，具有实际应用价值。

Abstract: Recent advancements in state-of-the-art (SOTA) offline reinforcement learning
(RL) have primarily focused on addressing function approximation errors, which
contribute to the overestimation of Q-values for out-of-distribution actions, a
challenge that static datasets exacerbate. However, high stakes applications
such as recommendation systems in online gaming, introduce further complexities
due to player's psychology (intent) driven by gameplay experiences and the
inherent volatility on the platform. These factors create highly sparse,
partially overlapping state spaces across policies, further influenced by the
experiment path selection logic which biases state spaces towards specific
policies. Current SOTA methods constrain learning from such offline data by
clipping known counterfactual actions as out-of-distribution due to poor
generalization across unobserved states. Further aggravating conservative
Q-learning and necessitating more online exploration. FAST-Q introduces a novel
approach that (1) leverages Gradient Reversal Learning to construct balanced
state representations, regularizing the policy-specific bias between the
player's state and action thereby enabling counterfactual estimation; (2)
supports offline counterfactual exploration in parallel with static data
exploitation; and (3) proposes a Q-value decomposition strategy for
multi-objective optimization, facilitating explainable recommendations over
short and long-term objectives. These innovations demonstrate superiority of
FAST-Q over prior SOTA approaches and demonstrates at least 0.15 percent
increase in player returns, 2 percent improvement in lifetime value (LTV), 0.4
percent enhancement in the recommendation driven engagement, 2 percent
improvement in the player's platform dwell time and an impressive 10 percent
reduction in the costs associated with the recommendation, on our volatile
gaming platform.

</details>

### [73] [Enhanced Semi-Supervised Stamping Process Monitoring with Physically-Informed Feature Extraction](https://arxiv.org/abs/2504.21389)
*Jianyu Zhang,Jianshe Feng,Yizhang Zhu,Fanyu Qi*

Main category: cs.LG

TLDR: 该研究提出了一种半监督的冲压过程异常监测框架，结合加速度信号和物理信息，有效捕捉异常，减少批量缺陷风险并提高生产良率。


<details>
  <summary>Details</summary>
Motivation: 解决冲压过程中频繁出现的异常问题，提高生产效率和产品质量。

Method: 提出混合特征提取算法结合数据驱动和物理机制，建立半监督异常检测模型，仅用正常样本构建基线模型，并设计新的偏差评分量化异常程度。

Result: 验证了特征提取方法的有效性，并在实际冲压车间数据中展示了框架的优越性能。

Conclusion: 该框架能有效监测冲压过程异常，提升生产良率。

Abstract: In tackling frequent anomalies in stamping processes, this study introduces a
novel semi-supervised in-process anomaly monitoring framework, utilizing
accelerometer signals and physics information, to capture the process anomaly
effectively. The proposed framework facilitates the construction of a
monitoring model with imbalanced sample distribution, which enables in-process
condition monitoring in real-time to prevent batch anomalies, which helps to
reduce batch defects risk and enhance production yield. Firstly, to effectively
capture key features from raw data containing redundant information, a hybrid
feature extraction algorithm is proposed to utilize data-driven methods and
physical mechanisms simultaneously. Secondly, to address the challenge brought
by imbalanced sample distribution, a semi-supervised anomaly detection model is
established, which merely employs normal samples to build a golden baseline
model, and a novel deviation score is proposed to quantify the anomaly level of
each online stamping stroke. The effectiveness of the proposed feature
extraction method is validated with various classification algorithms. A
real-world in-process dataset from stamping manufacturing workshop is employed
to illustrate the superiority of proposed semi-supervised framework with
enhance performance for process anomaly monitoring.

</details>

### [74] [MPEC: Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers](https://arxiv.org/abs/2504.21427)
*Shermin Shahbazi,Mohammad-Reza Nasiri,Majid Ramezani*

Main category: cs.LG

TLDR: MPEC方法通过保留EEG信号的流形结构，结合协方差矩阵和RBF核的特征工程，以及改进的K-means聚类，显著提升了EEG信号分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有EEG信号分类方法未能充分利用其非欧几里得流形结构，导致性能不佳。

Method: 提出MPEC方法，包括特征工程（协方差矩阵和RBF核）和改进的K-means聚类，用于流形空间分类。

Result: 在BCI Competition IV数据集2a上表现显著优于传统方法。

Conclusion: MPEC通过保留流形结构和集成分类器，有效提升了EEG信号分类的准确性。

Abstract: Accurate classification of EEG signals is crucial for brain-computer
interfaces (BCIs) and neuroprosthetic applications, yet many existing methods
fail to account for the non-Euclidean, manifold structure of EEG data,
resulting in suboptimal performance. Preserving this manifold information is
essential to capture the true geometry of EEG signals, but traditional
classification techniques largely overlook this need. To this end, we propose
MPEC (Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based
Classifiers), that introduces two key innovations: (1) a feature engineering
phase that combines covariance matrices and Radial Basis Function (RBF) kernels
to capture both linear and non-linear relationships among EEG channels, and (2)
a clustering phase that employs a modified K-means algorithm tailored for the
Riemannian manifold space, ensuring local geometric sensitivity. Ensembling
multiple clustering-based classifiers, MPEC achieves superior results,
validated by significant improvements on the BCI Competition IV dataset 2a.

</details>

### [75] [Whispers of Data: Unveiling Label Distributions in Federated Learning Through Virtual Client Simulation](https://arxiv.org/abs/2504.21436)
*Zhixuan Ma,Haichang Gao,Junxiang Huang,Ping Wang*

Main category: cs.LG

TLDR: 提出了一种新颖的标签分布推断攻击方法，适用于多种场景，且在差分隐私防御下仍有效。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受标签推断攻击，现有方法对受害者客户端设置敏感且防御效果不佳。

Method: 估计受害者客户端数据集大小，构建虚拟客户端，量化标签的时序泛化性，训练推断模型预测标签分布。

Result: 在多个数据集上验证了方法的优越性，且在差分隐私防御下仍有效。

Conclusion: 该方法稳定且适应性强，具有实际应用潜力。

Abstract: Federated Learning enables collaborative training of a global model across
multiple geographically dispersed clients without the need for data sharing.
However, it is susceptible to inference attacks, particularly label inference
attacks.
  Existing studies on label distribution inference exhibits sensitive to the
specific settings of the victim client and typically underperforms under
defensive strategies. In this study, we propose a novel label distribution
inference attack that is stable and adaptable to various scenarios.
Specifically, we estimate the size of the victim client's dataset and construct
several virtual clients tailored to the victim client. We then quantify the
temporal generalization of each class label for the virtual clients and utilize
the variation in temporal generalization to train an inference model that
predicts the label distribution proportions of the victim client.
  We validate our approach on multiple datasets, including MNIST,
Fashion-MNIST, FER2013, and AG-News. The results demonstrate the superiority of
our method compared to state-of-the-art techniques. Furthermore, our attack
remains effective even under differential privacy defense mechanisms,
underscoring its potential for real-world applications.

</details>

### [76] [xEEGNet: Towards Explainable AI in EEG Dementia Classification](https://arxiv.org/abs/2504.21457)
*Andrea Zanola,Louis Fabrice Tshimanga,Federico Del Pup,Marco Baiesi,Manfredo Atzori*

Main category: cs.LG

TLDR: xEEGNet是一种新型、紧凑且可解释的神经网络，用于EEG数据分析，专注于痴呆症分类，参数少且抗过拟合。


<details>
  <summary>Details</summary>
Motivation: 解决EEG数据分析中模型复杂、不可解释和过拟合的问题，同时应用于痴呆症分类。

Method: 从ShallowNet逐步改进为xEEGNet，保留性能的同时提升透明度和可解释性，使用嵌套交叉验证评估。

Result: xEEGNet参数仅168个，性能接近ShallowNet（-1.5%），抗过拟合且解释性强。

Conclusion: 小型架构如xEEGNet在EEG病理分类中同样有效，强调可解释性和实用性。

Abstract: This work presents xEEGNet, a novel, compact, and explainable neural network
for EEG data analysis. It is fully interpretable and reduces overfitting
through major parameter reduction. As an applicative use case, we focused on
classifying common dementia conditions, Alzheimer's and frontotemporal
dementia, versus controls. xEEGNet is broadly applicable to other neurological
conditions involving spectral alterations. We initially used ShallowNet, a
simple and popular model from the EEGNet-family. Its structure was analyzed and
gradually modified to move from a "black box" to a more transparent model,
without compromising performance. The learned kernels and weights were examined
from a clinical standpoint to assess medical relevance. Model variants,
including ShallowNet and the final xEEGNet, were evaluated using robust
Nested-Leave-N-Subjects-Out cross-validation for unbiased performance
estimates. Variability across data splits was explained using embedded EEG
representations, grouped by class and set, with pairwise separability to
quantify group distinction. Overfitting was assessed through
training-validation loss correlation and training speed. xEEGNet uses only 168
parameters, 200 times fewer than ShallowNet, yet retains interpretability,
resists overfitting, achieves comparable median performance (-1.5%), and
reduces variability across splits. This variability is explained by embedded
EEG representations: higher accuracy correlates with greater separation between
test set controls and Alzheimer's cases, without significant influence from
training data. xEEGNet's ability to filter specific EEG bands, learn
band-specific topographies, and use relevant spectral features demonstrates its
interpretability. While large deep learning models are often prioritized for
performance, this study shows smaller architectures like xEEGNet can be equally
effective in EEG pathology classification.

</details>

### [77] [Deep Learning Optimization Using Self-Adaptive Weighted Auxiliary Variables](https://arxiv.org/abs/2504.21501)
*Yaru Liu,Yiqi Gu,Michael K. Ng*

Main category: cs.LG

TLDR: 提出了一种新的优化框架，通过引入辅助变量和自适应权重改进深度神经网络中的梯度下降效率。


<details>
  <summary>Details</summary>
Motivation: 梯度下降在深度学习中因高度非凸的损失函数和梯度消失问题而效率低下。

Method: 引入辅助变量分离网络层，重新设计损失函数，并采用自适应权重保持一致性。

Result: 数值实验验证了方法的有效性和鲁棒性。

Conclusion: 新框架优化了原始问题，提升了梯度下降的性能。

Abstract: In this paper, we develop a new optimization framework for the least squares
learning problem via fully connected neural networks or physics-informed neural
networks. The gradient descent sometimes behaves inefficiently in deep learning
because of the high non-convexity of loss functions and the vanishing gradient
issue. Our idea is to introduce auxiliary variables to separate the layers of
the deep neural networks and reformulate the loss functions for ease of
optimization. We design the self-adaptive weights to preserve the consistency
between the reformulated loss and the original mean squared loss, which
guarantees that optimizing the new loss helps optimize the original problem.
Numerical experiments are presented to verify the consistency and show the
effectiveness and robustness of our models over gradient descent.

</details>

### [78] [Towards proactive self-adaptive AI for non-stationary environments with dataset shifts](https://arxiv.org/abs/2504.21565)
*David Fernández Narro,Pablo Ferri,Juan M. García-Gómez,Carlos Sáez*

Main category: cs.LG

TLDR: 提出了一种主动自适应的AI方法（pro-adaptive），通过建模AI参数的时间轨迹来预测短期参数值，以应对非平稳环境中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 医疗环境中数据分布随时间变化（时间偏移），且缺乏及时的新标注数据用于重新训练AI，导致性能下降。

Method: 使用多项式样条基和功能数据分析框架，建模AI参数的时间轨迹，预测短期参数值。

Result: 在模拟和真实COVID-19数据集上验证，该方法优于基线模型，无需更新训练数据即可应对偏移。

Conclusion: 为动态非平稳环境中的自适应AI研究奠定了基础，适用于医疗领域的稳健AI部署。

Abstract: Artificial Intelligence (AI) models deployed in production frequently face
challenges in maintaining their performance in non-stationary environments.
This issue is particularly noticeable in medical settings, where temporal
dataset shifts often occur. These shifts arise when the distributions of
training data differ from those of the data encountered during deployment over
time. Further, new labeled data to continuously retrain AI is not typically
available in a timely manner due to data access limitations. To address these
challenges, we propose a proactive self-adaptive AI approach, or pro-adaptive,
where we model the temporal trajectory of AI parameters, allowing us to
short-term forecast parameter values. To this end, we use polynomial spline
bases, within an extensible Functional Data Analysis framework. We validate our
methodology with a logistic regression model addressing prior probability
shift, covariate shift, and concept shift. This validation is conducted on both
a controlled simulated dataset and a publicly available real-world COVID-19
dataset from Mexico, with various shifts occurring between 2020 and 2024. Our
results indicate that this approach enhances the performance of AI against
shifts compared to baseline stable models trained at different time distances
from the present, without requiring updated training data. This work lays the
foundation for pro-adaptive AI research against dynamic, non-stationary
environments, being compatible with data protection, in resilient AI production
environments for health.

</details>

### [79] [On Advancements of the Forward-Forward Algorithm](https://arxiv.org/abs/2504.21662)
*Mauricio Ortiz Torres,Markus Lange,Arne P. Raulf*

Main category: cs.LG

TLDR: Forward-Forward算法在机器学习研究中发展，通过改进技术（如卷积通道分组、学习率调度和独立块结构）在CIFAR10数据集上表现更优，测试错误率降低20%。还提出了轻量模型，适合低容量硬件。


<details>
  <summary>Details</summary>
Motivation: 解决复杂任务并优化算法性能，同时保持灵活性和低内存占用，为低容量硬件提供可行方案。

Method: 结合卷积通道分组、学习率调度和独立块结构进行训练改进。

Result: 测试错误率降低20%，轻量模型测试错误率为(21±6)%，参数数量在164,706至754,386之间。

Conclusion: 改进的Forward-Forward算法表现优异，轻量模型为未来研究和低容量硬件应用奠定基础。

Abstract: The Forward-Forward algorithm has evolved in machine learning research,
tackling more complex tasks that mimic real-life applications. In the last
years, it has been improved by several techniques to perform better than its
original version, handling a challenging dataset like CIFAR10 without losing
its flexibility and low memory usage. We have shown in our results that
improvements are achieved through a combination of convolutional channel
grouping, learning rate schedules, and independent block structures during
training that lead to a 20\% decrease in test error percentage. Additionally,
to approach further implementations on low-capacity hardware projects we have
presented a series of lighter models that achieve low test error percentages
within (21$\pm$6)\% and number of trainable parameters between 164,706 and
754,386. This serving also as a basis for our future study on complete
verification and validation of these kinds of neural networks.

</details>

### [80] [Recursive KL Divergence Optimization: A Dynamic Framework for Representation Learning](https://arxiv.org/abs/2504.21707)
*Anthony D Martin*

Main category: cs.LG

TLDR: 论文提出了一种递归KL散度优化（RKDO）方法，通过动态调整数据邻域的KL散度，统一了对比学习、聚类和降维方法，并显著提高了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如I-Con）通过固定邻域条件的KL散度统一学习范式，但忽略了学习过程中的递归结构。

Method: 提出RKDO框架，将表示学习建模为KL散度在数据邻域上的动态演化过程。

Result: 实验显示RKDO在三个数据集上损失值降低约30%，计算资源减少60-80%。

Conclusion: RKDO的递归更新机制为表示学习提供了更高效的优化路径，尤其适用于资源受限场景。

Abstract: We propose a generalization of modern representation learning objectives by
reframing them as recursive divergence alignment processes over localized
conditional distributions While recent frameworks like Information Contrastive
Learning I-Con unify multiple learning paradigms through KL divergence between
fixed neighborhood conditionals we argue this view underplays a crucial
recursive structure inherent in the learning process. We introduce Recursive KL
Divergence Optimization RKDO a dynamic formalism where representation learning
is framed as the evolution of KL divergences across data neighborhoods. This
formulation captures contrastive clustering and dimensionality reduction
methods as static slices while offering a new path to model stability and local
adaptation. Our experiments demonstrate that RKDO offers dual efficiency
advantages approximately 30 percent lower loss values compared to static
approaches across three different datasets and 60 to 80 percent reduction in
computational resources needed to achieve comparable results. This suggests
that RKDOs recursive updating mechanism provides a fundamentally more efficient
optimization landscape for representation learning with significant
implications for resource constrained applications.

</details>

### [81] [Learning Heterogeneous Performance-Fairness Trade-offs in Federated Learning](https://arxiv.org/abs/2504.21775)
*Rongguang Ye,Ming Tang*

Main category: cs.LG

TLDR: HetPFL提出了一种新方法，通过自适应偏好采样和超网络融合，解决了联邦学习中性能和公平性权衡的异质性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在联邦学习中忽略了客户端本地Pareto前沿的异质性，且未考虑本地与全局Pareto前沿的差距。

Method: HetPFL包含偏好采样适应（PSA）和偏好感知超网络融合（PHF），分别优化本地采样分布和全局超网络融合。

Result: 实验表明HetPFL在四个数据集上显著优于七个基线方法，且理论证明其线性收敛。

Conclusion: HetPFL有效解决了联邦学习中性能和公平性权衡的异质性问题，提升了本地和全局Pareto前沿的质量。

Abstract: Recent methods leverage a hypernet to handle the performance-fairness
trade-offs in federated learning. This hypernet maps the clients' preferences
between model performance and fairness to preference-specifc models on the
trade-off curve, known as local Pareto front. However, existing methods
typically adopt a uniform preference sampling distribution to train the
hypernet across clients, neglecting the inherent heterogeneity of their local
Pareto fronts. Meanwhile, from the perspective of generalization, they do not
consider the gap between local and global Pareto fronts on the global dataset.
To address these limitations, we propose HetPFL to effectively learn both local
and global Pareto fronts. HetPFL comprises Preference Sampling Adaptation (PSA)
and Preference-aware Hypernet Fusion (PHF). PSA adaptively determines the
optimal preference sampling distribution for each client to accommodate
heterogeneous local Pareto fronts. While PHF performs preference-aware fusion
of clients' hypernets to ensure the performance of the global Pareto front. We
prove that HetPFL converges linearly with respect to the number of rounds,
under weaker assumptions than existing methods. Extensive experiments on four
datasets show that HetPFL significantly outperforms seven baselines in terms of
the quality of learned local and global Pareto fronts.

</details>

### [82] [Stable Trajectory Clustering: An Efficient Split and Merge Algorithm](https://arxiv.org/abs/2504.21808)
*Atieh Rahmani,Mansoor Davoodi,Justin M. Calabrese*

Main category: cs.LG

TLDR: 论文提出了基于DBSCAN线段聚类的全轨迹和子轨迹聚类算法，通过选择性忽略瞬态偏差提高聚类稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹聚类算法对临时异常敏感，导致聚类模式不稳定，研究旨在解决这一问题。

Method: 基于DBSCAN线段聚类，引入分裂和合并事件，利用滑动窗口模型和平均欧氏距离进行子轨迹聚类。

Result: 提出的稳定轨迹聚类算法在真实数据集上验证了其有效性和参数敏感性。

Conclusion: 选择性忽略瞬态偏差能提升聚类稳定性和可解释性。

Abstract: Clustering algorithms group data points by characteristics to identify
patterns. Over the past two decades, researchers have extended these methods to
analyze trajectories of humans, animals, and vehicles, studying their behavior
and movement across applications. This paper presents whole-trajectory
clustering and sub-trajectory clustering algorithms based on DBSCAN line
segment clustering, which encompasses two key events: split and merge of line
segments. The events are employed by object movement history and the average
Euclidean distance between line segments. In this framework, whole-trajectory
clustering considers entire entities' trajectories, whereas sub-trajectory
clustering employs a sliding window model to identify similar sub-trajectories.
Many existing trajectory clustering algorithms respond to temporary anomalies
in data by splitting trajectories, which often obscures otherwise consistent
clustering patterns and leads to less reliable insights. We introduce the
stable trajectory clustering algorithm, which leverages the mean absolute
deviation concept to demonstrate that selective omission of transient
deviations not only preserves the integrity of clusters but also improves their
stability and interpretability. We run all proposed algorithms on real
trajectory datasets to illustrate their effectiveness and sensitivity to
parameter variations.

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [83] [Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models](https://arxiv.org/abs/2504.21012)
*Makoto Sato*

Main category: cs.CL

TLDR: 论文提出了一种量化分析大型语言模型（LLM）认知行为的方法，通过设计诱导性提示（TIP）和量化性提示（TQP），发现LLM在语义融合提示下与人类不同，缺乏概念整合能力。


<details>
  <summary>Details</summary>
Motivation: 研究人类直觉思维的认知动态，通过对比人类与LLM的认知行为差异。

Method: 设计TIP触发LLM行为变化，TQP量化评估变化，实验对比语义融合与非融合提示的效果。

Result: LLM在语义融合提示下无显著差异，表明其缺乏人类的概念整合能力。

Conclusion: 方法可精细测量认知响应，揭示人工与人类思维在直觉和概念整合上的差异。

Abstract: What underlies intuitive human thinking? One approach to this question is to
compare the cognitive dynamics of humans and large language models (LLMs).
However, such a comparison requires a method to quantitatively analyze AI
cognitive behavior under controlled conditions. While anecdotal observations
suggest that certain prompts can dramatically change LLM behavior, these
observations have remained largely qualitative. Here, we propose a two-part
framework to investigate this phenomenon: a Transition-Inducing Prompt (TIP)
that triggers a rapid shift in LLM responsiveness, and a Transition Quantifying
Prompt (TQP) that evaluates this change using a separate LLM. Through
controlled experiments, we examined how LLMs react to prompts embedding two
semantically distant concepts (e.g., mathematical aperiodicity and traditional
crafts)--either fused together or presented separately--by changing their
linguistic quality and affective tone. Whereas humans tend to experience
heightened engagement when such concepts are meaningfully blended producing a
novel concept--a form of conceptual fusion--current LLMs showed no significant
difference in responsiveness between semantically fused and non-fused prompts.
This suggests that LLMs may not yet replicate the conceptual integration
processes seen in human intuition. Our method enables fine-grained,
reproducible measurement of cognitive responsiveness, and may help illuminate
key differences in how intuition and conceptual leaps emerge in artificial
versus human minds.

</details>

### [84] [Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge](https://arxiv.org/abs/2504.21013)
*Antoun Yaacoub,Zainab Assaghir,Lionel Prevost,Jérôme Da-Rugna*

Main category: cs.CL

TLDR: 研究分析了Google Gemini 1.5-flash文本模型生成的计算机科学多选题反馈的语言特性，揭示了反馈语调与题目难度之间的动态适应关系。


<details>
  <summary>Details</summary>
Motivation: AI生成的反馈在教育中潜力巨大，但其语言特性（如可读性、词汇丰富度）在不同难度和语调下的表现尚不明确。

Method: 分析了1,200多道多选题的反馈，计算了语言指标（如可读性、词汇丰富度），并训练了一个RoBERTa多任务学习模型预测这些指标。

Result: 模型在可读性和词汇丰富度上表现良好（MAE分别为2.0和0.03），发现反馈语调与题目难度有显著交互作用。

Conclusion: 研究为个性化AI反馈的开发提供了依据，强调了设计中的伦理考量。

Abstract: Artificial Intelligence (AI)-generated feedback in educational settings has
garnered considerable attention due to its potential to enhance learning
outcomes. However, a comprehensive understanding of the linguistic
characteristics of AI-generated feedback, including readability, lexical
richness, and adaptability across varying challenge levels, remains limited.
This study delves into the linguistic and structural attributes of feedback
generated by Google's Gemini 1.5-flash text model for computer science
multiple-choice questions (MCQs). A dataset of over 1,200 MCQs was analyzed,
considering three difficulty levels (easy, medium, hard) and three feedback
tones (supportive, neutral, challenging). Key linguistic metrics, such as
length, readability scores (Flesch-Kincaid Grade Level), vocabulary richness,
and lexical density, were computed and examined. A fine-tuned RoBERTa-based
multi-task learning (MTL) model was trained to predict these linguistic
properties, achieving a Mean Absolute Error (MAE) of 2.0 for readability and
0.03 for vocabulary richness. The findings reveal significant interaction
effects between feedback tone and question difficulty, demonstrating the
dynamic adaptation of AI-generated feedback within diverse educational
contexts. These insights contribute to the development of more personalized and
effective AI-driven feedback mechanisms, highlighting the potential for
improved learning outcomes while underscoring the importance of ethical
considerations in their design and deployment.

</details>

### [85] [Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments](https://arxiv.org/abs/2504.21016)
*Ngoc C. Lê,Hai-Chung Nguyen-Phung,Thu-Huong Pham Thi,Hue Vu,Phuong-Thao Nguyen Thi,Thu-Thuy Tran,Hong-Nhung Le Thi,Thuy-Duong Nguyen-Thi,Thanh-Huy Nguyen*

Main category: cs.CL

TLDR: 研究提出了一种命名实体识别（NER）方法，用于辅助越南的COVID-19疫情预防，并创建了一个手动标注的越南语嵌套命名实体数据集。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情导致全球重大损失，越南通过追踪、定位和隔离接触者有效预防疫情，但手动操作耗时耗力。

Method: 研究采用命名实体识别（NER）技术，并定义新的实体类型，构建了手动标注的越南语嵌套命名实体数据集。

Result: 提出了一个适用于越南COVID-19预防的NER系统，并提供了新的数据集。

Conclusion: NER技术可有效辅助越南的疫情预防工作，手动标注的数据集为未来研究提供了资源。

Abstract: The COVID-19 pandemic caused great losses worldwide, efforts are taken place
to prevent but many countries have failed. In Vietnam, the traceability,
localization, and quarantine of people who contact with patients contribute to
effective disease prevention. However, this is done by hand, and take a lot of
work. In this research, we describe a named-entity recognition (NER) study that
assists in the prevention of COVID-19 pandemic in Vietnam. We also present our
manually annotated COVID-19 dataset with nested named entity recognition task
for Vietnamese which be defined new entity types using for our system.

</details>

### [86] [ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese](https://arxiv.org/abs/2504.21017)
*Hai-Chung Nguyen-Phung,Ngoc C. Lê,Van-Chien Nguyen,Hang Thi Nguyen,Thuy Phuong Thi Nguyen*

Main category: cs.CL

TLDR: 论文介绍了首个越南语COVID-19多跨度提取机器阅读理解数据集ViQA-COVID，旨在支持疾病预防和促进越南语及多语言MRC研究。


<details>
  <summary>Details</summary>
Motivation: COVID-19对全球造成严重影响，AI应用需求迫切。现有研究多集中于英语，越南语MRC数据集缺乏，因此创建ViQA-COVID填补空白。

Method: 创建ViQA-COVID数据集，专注于COVID-19主题，支持多跨度提取任务，适用于越南语及多语言MRC研究。

Result: ViQA-COVID成为首个越南语COVID-19多跨度提取MRC数据集，为模型和系统开发提供支持。

Conclusion: ViQA-COVID填补了越南语MRC数据集的空白，有望推动越南语及多语言MRC研究的发展。

Abstract: After two years of appearance, COVID-19 has negatively affected people and
normal life around the world. As in May 2022, there are more than 522 million
cases and six million deaths worldwide (including nearly ten million cases and
over forty-three thousand deaths in Vietnam). Economy and society are both
severely affected. The variant of COVID-19, Omicron, has broken disease
prevention measures of countries and rapidly increased number of infections.
Resources overloading in treatment and epidemics prevention is happening all
over the world. It can be seen that, application of artificial intelligence
(AI) to support people at this time is extremely necessary. There have been
many studies applying AI to prevent COVID-19 which are extremely useful, and
studies on machine reading comprehension (MRC) are also in it. Realizing that,
we created the first MRC dataset about COVID-19 for Vietnamese: ViQA-COVID and
can be used to build models and systems, contributing to disease prevention.
Besides, ViQA-COVID is also the first multi-span extraction MRC dataset for
Vietnamese, we hope that it can contribute to promoting MRC studies in
Vietnamese and multilingual.

</details>

### [87] [HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization](https://arxiv.org/abs/2504.21018)
*Enes Özeren,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TLDR: HYPEROFA提出了一种基于超网络的适应性词嵌入初始化方法，优于随机初始化和OFA方法，提升了低资源语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决预训练语言模型在低资源语言上表现不佳的问题，尤其是OFA方法中固定源语言嵌入的限制。

Method: 使用超网络将多语言词向量空间映射到PLM的词嵌入空间，为目标语言生成灵活的初始嵌入。

Result: HYPEROFA在持续预训练收敛和下游任务性能上优于随机初始化，并匹配或超过OFA。

Conclusion: HYPEROFA是一种有效的适应性初始化方法，适用于低资源语言的预训练模型。

Abstract: Many pre-trained language models (PLMs) exhibit suboptimal performance on
mid- and low-resource languages, largely due to limited exposure to these
languages during pre-training. A common strategy to address this is to
introduce new tokens specific to the target languages, initialize their
embeddings, and apply continual pre-training on target-language data. Among
such methods, OFA (Liu et al., 2024a) proposes a similarity-based subword
embedding initialization heuristic that is both effective and efficient.
However, OFA restricts target-language token embeddings to be convex
combinations of a fixed number of source-language embeddings, which may limit
expressiveness. To overcome this limitation, we propose HYPEROFA, a
hypernetwork-based approach for more adaptive token embedding initialization.
The hypernetwork is trained to map from an external multilingual word vector
space to the PLMs token embedding space using source-language tokens. Once
trained, it can generate flexible embeddings for target-language tokens,
serving as a good starting point for continual pretraining. Experiments
demonstrate that HYPEROFA consistently outperforms random initialization
baseline and matches or exceeds the performance of OFA in both continual
pre-training convergence and downstream task performance. We make the code
publicly available.

</details>

### [88] [Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations](https://arxiv.org/abs/2504.21019)
*Yinghan Zhou,Juan Wen,Wanli Peng,Yiming Xue,Ziwei Zhang,Zhengxian Wu*

Main category: cs.CL

TLDR: 论文提出了一种新的AI生成文本检测方法（DP-Net），通过动态扰动和强化学习解决泛化性和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能同时解决AI生成文本检测的泛化性和鲁棒性问题，需要一种统一机制。

Method: 将鲁棒性视为特定领域偏移，提出DP-Net方法，利用强化学习的动态扰动。

Result: DP-Net在三种跨域场景中表现优异，并在两种文本对抗攻击下具有最佳鲁棒性。

Conclusion: DP-Net为AI生成文本检测提供了同时具备高泛化性和鲁棒性的解决方案。

Abstract: The growing popularity of large language models has raised concerns regarding
the potential to misuse AI-generated text (AIGT). It becomes increasingly
critical to establish an excellent AIGT detection method with high
generalization and robustness. However, existing methods either focus on model
generalization or concentrate on robustness. The unified mechanism, to
simultaneously address the challenges of generalization and robustness, is less
explored. In this paper, we argue that robustness can be view as a specific
form of domain shift, and empirically reveal an intrinsic mechanism for model
generalization of AIGT detection task. Then, we proposed a novel AIGT detection
method (DP-Net) via dynamic perturbations introduced by a reinforcement
learning with elaborated reward and action. Experimentally, extensive results
show that the proposed DP-Net significantly outperforms some state-of-the-art
AIGT detection methods for generalization capacity in three cross-domain
scenarios. Meanwhile, the DP-Net achieves best robustness under two text
adversarial attacks. The code is publicly available at
https://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net.

</details>

### [89] [Context-Enhanced Contrastive Search for Improved LLM Text Generation](https://arxiv.org/abs/2504.21020)
*Jaydip Sen,Rohit Pandey,Hetvi Waghela*

Main category: cs.CL

TLDR: 论文提出了一种改进的对比搜索算法CECS，通过动态上下文重要性加权和多级对比搜索等技术，显著提升了生成文本的连贯性和相关性。


<details>
  <summary>Details</summary>
Motivation: 传统解码方法在生成长文本时存在重复或不连贯的问题，需要一种新方法来平衡流畅性、创造性和精确性。

Method: 提出了Context-Enhanced Contrastive Search (CECS)，结合动态上下文重要性加权、多级对比搜索和自适应温度控制。

Result: 实验表明，CECS在BLEU、ROUGE和语义相似度等指标上优于现有对比搜索技术。

Conclusion: CECS在生成高质量文本方面具有潜力，适用于法律文件起草、客服聊天机器人和内容营销等领域。

Abstract: Recently, Large Language Models (LLMs) have demonstrated remarkable
advancements in Natural Language Processing (NLP). However, generating
high-quality text that balances coherence, diversity, and relevance remains
challenging. Traditional decoding methods, such as bean search and top-k
sampling, often struggle with either repetitive or incoherent outputs,
particularly in tasks that require long-form text generation. To address these
limitations, the paper proposes a novel enhancement of the well-known
Contrastive Search algorithm, Context-Enhanced Contrastive Search (CECS) with
contextual calibration. The proposed scheme introduces several novelties
including dynamic contextual importance weighting, multi-level Contrastive
Search, and adaptive temperature control, to optimize the balance between
fluency, creativity, and precision. The performance of CECS is evaluated using
several standard metrics such as BLEU, ROUGE, and semantic similarity.
Experimental results demonstrate significant improvements in both coherence and
relevance of the generated texts by CECS outperforming the existing Contrastive
Search techniques. The proposed algorithm has several potential applications in
the real world including legal document drafting, customer service chatbots,
and content marketing.

</details>

### [90] [ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees](https://arxiv.org/abs/2504.21022)
*Jun Wang,David Smith Sundarsingh,Jyotirmoy V. Deshmukh,Yiannis Kantaros*

Main category: cs.CL

TLDR: ConformalNL2LTL是一种新的自然语言到LTL的翻译方法，通过结合问答问题和LLMs，利用共形预测实现不确定性感知翻译，确保用户定义的翻译成功率。


<details>
  <summary>Details</summary>
Motivation: 减少手动定义LTL任务所需的努力和专业知识，并提供翻译正确性保证。

Method: 通过迭代解决开放词汇问答问题构建LTL公式，利用共形预测量化LLM生成答案的不确定性。

Result: ConformalNL2LTL能够实现用户指定的翻译准确率，同时最小化求助率。

Conclusion: 该方法在理论和实证上均验证了其有效性，为NL到LTL翻译提供了可靠解决方案。

Abstract: Linear Temporal Logic (LTL) has become a prevalent specification language for
robotic tasks. To mitigate the significant manual effort and expertise required
to define LTL-encoded tasks, several methods have been proposed for translating
Natural Language (NL) instructions into LTL formulas, which, however, lack
correctness guarantees. To address this, we introduce a new NL-to-LTL
translation method, called ConformalNL2LTL, that can achieve user-defined
translation success rates over unseen NL commands. Our method constructs LTL
formulas iteratively by addressing a sequence of open-vocabulary
Question-Answering (QA) problems with LLMs. To enable uncertainty-aware
translation, we leverage conformal prediction (CP), a distribution-free
uncertainty quantification tool for black-box models. CP enables our method to
assess the uncertainty in LLM-generated answers, allowing it to proceed with
translation when sufficiently confident and request help otherwise. We provide
both theoretical and empirical results demonstrating that ConformalNL2LTL
achieves user-specified translation accuracy while minimizing help rates.

</details>

### [91] [Param$Δ$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost](https://arxiv.org/abs/2504.21023)
*Sheng Cao,Mingrui Wu,Karthik Prasad,Yuandong Tian,Zechun Liu*

Main category: cs.CL

TLDR: 论文提出了一种名为$Param\Delta$的新方法，通过直接计算已有后训练模型与基础模型权重的差异，并将其应用于更新后的基础模型，从而无需额外训练即可实现后训练效果。


<details>
  <summary>Details</summary>
Motivation: 后训练阶段需要大量高质量数据和计算资源，且存在过拟合风险。$Param\Delta$旨在通过知识迁移降低这些成本。

Method: 通过计算后训练模型权重与基础模型权重的差异（$\Theta_\text{post} - \Theta_\text{base}$），并将其加到更新后的基础模型（$\Theta'_\text{base}$）上，实现后训练能力的迁移。

Result: 在多个模型（如LLama3、Llama3.1等）上验证，$Param\Delta$模型性能接近直接后训练模型（如达到Llama3.1-inst模型95%的性能）。

Conclusion: $Param\Delta$为开源模型社区提供了一种零成本加速模型迭代的方法，充分利用已有模型资源。

Abstract: The post-training phase of large language models is essential for enhancing
capabilities such as instruction-following, reasoning, and alignment with human
preferences. However, it demands extensive high-quality data and poses risks
like overfitting, alongside significant computational costs due to repeated
post-training and evaluation after each base model update. This paper
introduces $Param\Delta$, a novel method that streamlines post-training by
transferring knowledge from an existing post-trained model to a newly updated
base model with ZERO additional training. By computing the difference between
post-trained model weights ($\Theta_\text{post}$) and base model weights
($\Theta_\text{base}$), and adding this to the updated base model
($\Theta'_\text{base}$), we define $Param\Delta$ Model as:
$\Theta_{\text{Param}\Delta} = \Theta_\text{post} - \Theta_\text{base} +
\Theta'_\text{base}$. This approach surprisingly equips the new base model with
post-trained capabilities, achieving performance comparable to direct
post-training. We did analysis on LLama3, Llama3.1, Qwen, and
DeepSeek-distilled models. Results indicate $Param\Delta$ Model effectively
replicates traditional post-training. For example, the $Param\Delta$ Model
obtained from 70B Llama3-inst, Llama3-base, Llama3.1-base models attains
approximately 95\% of Llama3.1-inst model's performance on average.
$Param\Delta$ brings a new perspective on how to fully leverage models in the
open-weight community, where checkpoints for base and instruct models are
readily available and frequently updated, by providing a cost-free framework to
accelerate the iterative cycle of model development.

</details>

### [92] [WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model](https://arxiv.org/abs/2504.21024)
*Tianqing Fang,Hongming Zhang,Zhisong Zhang,Kaixin Ma,Wenhao Yu,Haitao Mi,Dong Yu*

Main category: cs.CL

TLDR: 提出了一种新框架，通过引入共同进化的世界模型LLM，解决了自主学习中性能停滞的问题，实验显示性能提升10%。


<details>
  <summary>Details</summary>
Motivation: 现有自主学习方法在性能提升中遇到停滞，原因是环境探索不足和预训练知识利用不充分。

Method: 引入世界模型LLM，模拟环境生成自指导数据，并在推理时作为想象引擎引导行动选择。

Result: 在真实网络环境中性能提升10%，无需依赖闭源模型蒸馏。

Conclusion: 世界模型对自主代理框架的持续适应性至关重要。

Abstract: Agent self-improvement, where the backbone Large Language Model (LLM) of the
agent are trained on trajectories sampled autonomously based on their own
policies, has emerged as a promising approach for enhancing performance. Recent
advancements, particularly in web environments, face a critical limitation:
their performance will reach a stagnation point during autonomous learning
cycles, hindering further improvement. We argue that this stems from limited
exploration of the web environment and insufficient exploitation of pre-trained
web knowledge in LLMs. To improve the performance of self-improvement, we
propose a novel framework that introduces a co-evolving World Model LLM. This
world model predicts the next observation based on the current observation and
action within the web environment. Leveraging LLMs' pretrained knowledge of
abundant web content, the World Model serves dual roles: (1) as a virtual web
server generating self-instructed training data to continuously refine the
agent's policy, and (2) as an imagination engine during inference, enabling
look-ahead simulation to guide action selection for the agent LLM. Experiments
in real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a
10% performance gain over existing self-evolving agents, demonstrating the
efficacy and generalizability of our approach, without using any distillation
from more powerful close-sourced models. Our work establishes the necessity of
integrating world models into autonomous agent frameworks to unlock sustained
adaptability.

</details>

### [93] [Durghotona GPT: A Web Scraping and Large Language Model Based Framework to Generate Road Accident Dataset Automatically in Bangladesh](https://arxiv.org/abs/2504.21025)
*MD Thamed Bin Zaman Chowdhury,Moazzem Hossain,Md. Ridwanul Islam*

Main category: cs.CL

TLDR: 论文提出了一种名为'Durghotona GPT'的新框架，通过结合网络爬虫和大型语言模型（LLMs）自动生成孟加拉国主要报纸的交通事故数据集，解决了手动数据收集的延迟、错误和沟通问题。


<details>
  <summary>Details</summary>
Motivation: 交通事故导致巨大的经济损失和社会问题，准确及时的数据对预测和缓解事故至关重要。

Method: 从三家主要报纸收集事故新闻，利用GPT-4、GPT-3.5和Llama-3处理数据，提取信息并生成数据集。

Result: Llama-3表现接近GPT-4，准确率达89%，是成本效益高的替代方案。框架显著提升了数据质量和可用性。

Conclusion: 该框架可支持交通安全分析、城市规划和公共卫生等应用，未来将扩展数据收集方法并优化LLMs。

Abstract: Road accidents pose significant concerns globally. They lead to large
financial losses, injuries, disabilities, and societal challenges. Accurate and
timely accident data is essential for predicting and mitigating these events.
This paper presents a novel framework named 'Durghotona GPT' that integrates
web scraping and Large Language Models (LLMs) to automate the generation of
comprehensive accident datasets from prominent national dailies in Bangladesh.
The authors collected accident reports from three major newspapers: Prothom
Alo, Dhaka Tribune, and The Daily Star. The collected news was then processed
using the newest available LLMs: GPT-4, GPT-3.5, and Llama-3. The framework
efficiently extracts relevant information, categorizes reports, and compiles
detailed datasets. Thus, this framework overcomes limitations of manual data
collection methods such as delays, errors, and communication gaps. The authors'
evaluation demonstrates that Llama-3, an open-source model, performs comparably
to GPT-4. It achieved 89% accuracy in the authors' evaluation. Therefore, it
can be considered a cost-effective alternative for similar tasks. The results
suggest that the framework developed by the authors can drastically enhance the
quality and availability of accident data. As a result, it can support critical
applications in traffic safety analysis, urban planning, and public health. The
authors also developed an interface for 'Durghotona GPT' for ease of use as
part of this paper. Future work will focus on expanding data collection methods
and refining LLMs to further increase dataset accuracy and applicability.

</details>

### [94] [Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models](https://arxiv.org/abs/2504.21026)
*Manish Pandey,Nageshwar Prasad Yadav,Mokshada Adduru,Sawan Rai*

Main category: cs.CL

TLDR: 本文研究了多语言社交媒体中混合语言（如泰卢固语-英语和尼泊尔语-英语）的辱骂性语言检测，提出了一种新的标注数据集，并通过多种机器学习、深度学习和大型语言模型进行了评估。


<details>
  <summary>Details</summary>
Motivation: 随着多语言用户在社交媒体上的增加，混合语言中的辱骂性语言检测变得更具挑战性，尤其是低资源语言如泰卢固语和尼泊尔语缺乏相关研究。

Method: 研究创建了一个包含2000条泰卢固语-英语和5000条尼泊尔语-英语混合评论的标注数据集，并通过多种模型（如逻辑回归、随机森林、SVM、神经网络、LSTM、CNN和LLMs）进行实验，使用10折交叉验证和t检验评估性能。

Result: 研究提供了混合语言中辱骂性语言检测的挑战性见解，并对不同计算方法的性能进行了比较分析。

Conclusion: 该研究为低资源语言的NLP研究提供了基准，有助于开发更强大的多语言社交媒体内容审核策略。

Abstract: With the growing presence of multilingual users on social media, detecting
abusive language in code-mixed text has become increasingly challenging.
Code-mixed communication, where users seamlessly switch between English and
their native languages, poses difficulties for traditional abuse detection
models, as offensive content may be context-dependent or obscured by linguistic
blending. While abusive language detection has been extensively explored for
high-resource languages like English and Hindi, low-resource languages such as
Telugu and Nepali remain underrepresented, leaving gaps in effective
moderation. In this study, we introduce a novel, manually annotated dataset of
2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized
as abusive and non-abusive, collected from various social media platforms. The
dataset undergoes rigorous preprocessing before being evaluated across multiple
Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We
experimented with models including Logistic Regression, Random Forest, Support
Vector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing
their performance through hyperparameter tuning, and evaluate it using 10-fold
cross-validation and statistical significance testing (t-test). Our findings
provide key insights into the challenges of detecting abusive language in
code-mixed settings and offer a comparative analysis of computational
approaches. This study contributes to advancing NLP for low-resource languages
by establishing benchmarks for abusive language detection in Telugu-English and
Nepali-English code-mixed text. The dataset and insights can aid in the
development of more robust moderation strategies for multilingual social media
environments.

</details>

### [95] [UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2504.21027)
*Yu Zheng,Longyi Liu,Yuming Lin,Jie Feng,Guozhen Zhang,Depeng Jin,Yong Li*

Main category: cs.CL

TLDR: 论文提出了UrbanPlanBench基准和UrbanPlanText数据集，评估LLMs在城乡规划中的表现，发现其知识获取不平衡，并通过微调提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在城乡规划领域的应用潜力，填补现有研究的空白。

Method: 引入UrbanPlanBench基准和UrbanPlanText数据集，通过评估和微调LLMs。

Result: LLMs在规划法规理解上表现不佳，微调后性能有所提升，但仍有改进空间。

Conclusion: 公开资源以促进LLMs与城乡规划的结合，推动人机协作。

Abstract: The advent of Large Language Models (LLMs) holds promise for revolutionizing
various fields traditionally dominated by human expertise. Urban planning, a
professional discipline that fundamentally shapes our daily surroundings, is
one such field heavily relying on multifaceted domain knowledge and experience
of human experts. The extent to which LLMs can assist human practitioners in
urban planning remains largely unexplored. In this paper, we introduce a
comprehensive benchmark, UrbanPlanBench, tailored to evaluate the efficacy of
LLMs in urban planning, which encompasses fundamental principles, professional
knowledge, and management and regulations, aligning closely with the
qualifications expected of human planners. Through extensive evaluation, we
reveal a significant imbalance in the acquisition of planning knowledge among
LLMs, with even the most proficient models falling short of meeting
professional standards. For instance, we observe that 70% of LLMs achieve
subpar performance in understanding planning regulations compared to other
aspects. Besides the benchmark, we present the largest-ever supervised
fine-tuning (SFT) dataset, UrbanPlanText, comprising over 30,000 instruction
pairs sourced from urban planning exams and textbooks. Our findings demonstrate
that fine-tuned models exhibit enhanced performance in memorization tests and
comprehension of urban planning knowledge, while there exists significant room
for improvement, particularly in tasks requiring domain-specific terminology
and reasoning. By making our benchmark, dataset, and associated evaluation and
fine-tuning toolsets publicly available at
https://github.com/tsinghua-fib-lab/PlanBench, we aim to catalyze the
integration of LLMs into practical urban planning, fostering a symbiotic
collaboration between human expertise and machine intelligence.

</details>

### [96] [Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts](https://arxiv.org/abs/2504.21117)
*Hanhua Hong,Chenghao Xiao,Yang Wang,Yiqi Liu,Wenge Rong,Chenghua Lin*

Main category: cs.CL

TLDR: 提出了一种基于逆学习的方法，自动生成高效、模型特定的评估提示，提升LLM评估的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自然语言生成（NLG）系统评估因输出多样性而困难，人工评估存在不一致性和偏见，LLM评估则对提示设计敏感。

Method: 采用逆学习方法，从模型输出反向映射到输入指令，自动生成评估提示。

Result: 方法仅需单一样本，无需手动设计提示，提高了效率和鲁棒性。

Conclusion: 为LLM评估提供了更高效和鲁棒的新方向。

Abstract: Evaluating natural language generation (NLG) systems is challenging due to
the diversity of valid outputs. While human evaluation is the gold standard, it
suffers from inconsistencies, lack of standardisation, and demographic biases,
limiting reproducibility. LLM-based evaluation offers a scalable alternative
but is highly sensitive to prompt design, where small variations can lead to
significant discrepancies. In this work, we propose an inversion learning
method that learns effective reverse mappings from model outputs back to their
input instructions, enabling the automatic generation of highly effective,
model-specific evaluation prompts. Our method requires only a single evaluation
sample and eliminates the need for time-consuming manual prompt engineering,
thereby improving both efficiency and robustness. Our work contributes toward a
new direction for more robust and efficient LLM-based evaluation.

</details>

### [97] [LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge](https://arxiv.org/abs/2504.21132)
*Naheed Rayhan,Md. Ashrafuzzaman*

Main category: cs.CL

TLDR: LLM ENHANCER系统通过整合多源在线数据（如Google、Wikipedia）提升LLMs的准确性，减少幻觉，同时保持回答的自然性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在关键场景中因生成不准确信息和缺乏外部知识支持而受限的问题。

Method: 采用并行数据采集和自定义代理工具，结合向量嵌入筛选最相关信息，输入LLM生成回答。

Result: 系统有效减少LLMs的幻觉，提升回答准确性，同时保持自然性。

Conclusion: LLM ENHANCER为LLMs在关键场景中的应用提供了可行的增强方案。

Abstract: Large Language Models (LLMs), such as ChatGPT, have demonstrated the
capability to generate human like, natural responses across a range of tasks,
including task oriented dialogue and question answering. However, their
application in real world, critical scenarios is often hindered by a tendency
to produce inaccurate information and a limited ability to leverage external
knowledge sources. This paper introduces the LLM ENHANCER system, designed to
integrate multiple online sources such as Google, Wikipedia, and DuckDuckGo to
enhance data accuracy. The LLMs employed within this system are open source.
The data acquisition process for the LLM ENHANCER system operates in parallel,
utilizing custom agent tools to manage the flow of information. Vector
embeddings are used to identify the most pertinent information, which is
subsequently supplied to the LLM for user interaction. The LLM ENHANCER system
mitigates hallucinations in chat based LLMs while preserving response
naturalness and accuracy.

</details>

### [98] [Detecting Manipulated Contents Using Knowledge-Grounded Inference](https://arxiv.org/abs/2504.21165)
*Mark Huasong Meng,Ruizhe Wang,Meng Xu,Chuan Yan,Guangdong Bai*

Main category: cs.CL

TLDR: 论文提出了一种名为Manicod的工具，用于检测零日操纵内容，通过检索增强生成（RAG）和大语言模型（LLM）实现高效识别，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案依赖训练中的固有知识或手动整理的上下文，无法有效处理零日操纵内容，因此需要一种能够利用实时上下文信息的工具。

Method: Manicod从主流搜索引擎获取输入声明的上下文信息，通过RAG将其向量化后输入LLM进行推理，生成判断和解释。

Result: 在包含4270条操纵假新闻的数据集上，Manicod的F1得分为0.856，比现有方法最高提升1.9倍。

Conclusion: Manicod能够有效检测零日操纵内容，为假新闻识别提供了新思路。

Abstract: The detection of manipulated content, a prevalent form of fake news, has been
widely studied in recent years. While existing solutions have been proven
effective in fact-checking and analyzing fake news based on historical events,
the reliance on either intrinsic knowledge obtained during training or manually
curated context hinders them from tackling zero-day manipulated content, which
can only be recognized with real-time contextual information. In this work, we
propose Manicod, a tool designed for detecting zero-day manipulated content.
Manicod first sources contextual information about the input claim from
mainstream search engines, and subsequently vectorizes the context for the
large language model (LLM) through retrieval-augmented generation (RAG). The
LLM-based inference can produce a "truthful" or "manipulated" decision and
offer a textual explanation for the decision. To validate the effectiveness of
Manicod, we also propose a dataset comprising 4270 pieces of manipulated fake
news derived from 2500 recent real-world news headlines. Manicod achieves an
overall F1 score of 0.856 on this dataset and outperforms existing methods by
up to 1.9x in F1 score on their benchmarks on fact-checking and claim
verification.

</details>

### [99] [Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare](https://arxiv.org/abs/2504.21191)
*Lovedeep Gondara,Jonathan Simkin,Graham Sayle,Shebnum Devji,Gregory Arbour,Raymond Ng*

Main category: cs.CL

TLDR: 研究探讨了语言模型选择的四个关键问题：微调与零样本使用的必要性、领域邻近与通用预训练模型的优势、领域特定预训练的价值，以及小型语言模型（SLMs）在特定任务中相对于大型语言模型（LLMs）的持续相关性。通过实验发现，微调显著提升SLMs性能，使其超越零样本LLMs。


<details>
  <summary>Details</summary>
Motivation: 指导语言模型选择，明确微调、领域邻近预训练和领域特定预训练对模型性能的影响，以及SLMs在特定任务中的价值。

Method: 使用电子病理报告数据，评估三种不同难度和数据规模的分类任务，比较SLMs和LLM在零样本和微调下的表现。

Result: 微调显著提升SLMs性能，使其超越零样本LLMs；领域邻近预训练和领域特定预训练对复杂任务尤其有益。

Conclusion: SLMs在微调和领域特定优化后，能在特定任务中超越LLMs，提供更优的性能与资源权衡。

Abstract: This study aims to guide language model selection by investigating: 1) the
necessity of finetuning versus zero-shot usage, 2) the benefits of
domain-adjacent versus generic pretrained models, 3) the value of further
domain-specific pretraining, and 4) the continued relevance of Small Language
Models (SLMs) compared to Large Language Models (LLMs) for specific tasks.
Using electronic pathology reports from the British Columbia Cancer Registry
(BCCR), three classification scenarios with varying difficulty and data size
are evaluated. Models include various SLMs and an LLM. SLMs are evaluated both
zero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning
significantly improved SLM performance across all scenarios compared to their
zero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was
consistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally
performed better than the generic SLM after finetuning, especially on harder
tasks. Further domain-specific pretraining yielded modest gains on easier tasks
but significant improvements on the complex, data-scarce task. The results
highlight the critical role of finetuning for SLMs in specialized domains,
enabling them to surpass zero-shot LLM performance on targeted classification
tasks. Pretraining on domain-adjacent or domain-specific data provides further
advantages, particularly for complex problems or limited finetuning data. While
LLMs offer strong zero-shot capabilities, their performance on these specific
tasks did not match that of appropriately finetuned SLMs. In the era of LLMs,
SLMs remain relevant and effective, offering a potentially superior
performance-resource trade-off compared to LLMs.

</details>

### [100] [Automatic Legal Writing Evaluation of LLMs](https://arxiv.org/abs/2504.21202)
*Ramon Pires,Roseval Malaquias Junior,Rodrigo Nogueira*

Main category: cs.CL

TLDR: 论文介绍了oab-bench基准测试，用于评估大型语言模型在法律写作任务中的表现，并探讨了模型作为自动化评估工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于评估法律写作的复杂性，缺乏公开且全面的基准测试，巴西律师资格考试提供了一个理想的数据集。

Method: 构建了包含105个问题的oab-bench基准测试，涵盖七个法律领域，并提供了详细的评估指南和参考材料。

Result: Claude-3.5 Sonnet表现最佳，平均得分7.93/10；前沿模型（如OpenAI的o1）在评估通过考试时与人类评分高度相关。

Conclusion: oab-bench为法律写作评估提供了公开基准，前沿模型有望成为可靠的自动化评估工具。

Abstract: Despite the recent advances in Large Language Models, benchmarks for
evaluating legal writing remain scarce due to the inherent complexity of
assessing open-ended responses in this domain. One of the key challenges in
evaluating language models on domain-specific tasks is finding test datasets
that are public, frequently updated, and contain comprehensive evaluation
guidelines. The Brazilian Bar Examination meets these requirements. We
introduce oab-bench, a benchmark comprising 105 questions across seven areas of
law from recent editions of the exam. The benchmark includes comprehensive
evaluation guidelines and reference materials used by human examiners to ensure
consistent grading. We evaluate the performance of four LLMs on oab-bench,
finding that Claude-3.5 Sonnet achieves the best results with an average score
of 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can
serve as reliable automated judges for evaluating legal writing. Our
experiments show that frontier models like OpenAI's o1 achieve a strong
correlation with human scores when evaluating approved exams, suggesting their
potential as reliable automated evaluators despite the inherently subjective
nature of legal writing assessment. The source code and the benchmark --
containing questions, evaluation guidelines, model-generated responses, and
their respective automated evaluations -- are publicly available.

</details>

### [101] [Pretraining Large Brain Language Model for Active BCI: Silent Speech](https://arxiv.org/abs/2504.21214)
*Jinzhao Zhou,Zehong Cao,Yiqun Duan,Connor Barkley,Daniel Leong,Xiaowei Jiang,Quoc-Toan Nguyen,Ziyi Zhao,Thomas Do,Yu-Cheng Chang,Sheng-Fu Liang,Chin-teng Lin*

Main category: cs.CL

TLDR: 本文提出了一种用于主动脑机接口（BCI）的无声语音解码方法，通过预训练大型脑语言模型（LBLM）和新的未来时频预测（FSTP）范式，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统BCI系统的通信方式不够自然灵活，本文旨在通过无声语音解码技术改进这一问题。

Method: 提出LBLM模型，采用FSTP预训练范式，结合自回归建模学习EEG信号的时频依赖性，并在下游任务中进行微调。

Result: 在跨会话设置中，LBLM在语义级和词级分类任务中分别达到47.0%和39.6%的准确率，显著优于基线方法。

Conclusion: 研究为主动BCI系统的无声语音解码提供了创新解决方案，并贡献了新的数据集。

Abstract: This paper explores silent speech decoding in active brain-computer interface
(BCI) systems, which offer more natural and flexible communication than
traditional BCI applications. We collected a new silent speech dataset of over
120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing
24 commonly used English words for language model pretraining and decoding.
Following the recent success of pretraining large models with self-supervised
paradigms to enhance EEG classification performance, we propose Large Brain
Language Model (LBLM) pretrained to decode silent speech for active BCI. To
pretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining
paradigm to learn effective representations from unlabeled EEG data. Unlike
existing EEG pretraining methods that mainly follow a masked-reconstruction
paradigm, our proposed FSTP method employs autoregressive modeling in temporal
and frequency domains to capture both temporal and spectral dependencies from
EEG signals. After pretraining, we finetune our LBLM on downstream tasks,
including word-level and semantic-level classification. Extensive experiments
demonstrate significant performance gains of the LBLM over fully-supervised and
pretrained baseline models. For instance, in the difficult cross-session
setting, our model achieves 47.0\% accuracy on semantic-level classification
and 39.6\% in word-level classification, outperforming baseline methods by
5.4\% and 7.3\%, respectively. Our research advances silent speech decoding in
active BCI systems, offering an innovative solution for EEG language model
pretraining and a new dataset for fundamental research.

</details>

### [102] [Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math](https://arxiv.org/abs/2504.21233)
*Haoran Xu,Baolin Peng,Hany Awadalla,Dongdong Chen,Yen-Chun Chen,Mei Gao,Young Jin Kim,Yunsheng Li,Liliang Ren,Yelong Shen,Shuohang Wang,Weijian Xu,Jianfeng Gao,Weizhu Chen*

Main category: cs.CL

TLDR: 通过系统化的训练方法，Phi-4-Mini-Reasoning模型在数学推理任务上超越了更大的模型。


<details>
  <summary>Details</summary>
Motivation: 提升小型语言模型（SLMs）的推理能力，尽管其模型容量有限。

Method: 四步训练法：大规模中训练、监督微调、Rollout DPO和强化学习。

Result: Phi-4-Mini-Reasoning在Math-500上超越DeepSeek-R1-Distill-Qwen-7B和DeepSeek-R1-Distill-Llama-8B。

Conclusion: 精心设计的训练方法和高质量数据可显著提升小型模型的推理能力。

Abstract: Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities
in Large Language Models (LLMs) by training them to explicitly generate
intermediate reasoning steps. While LLMs readily benefit from such techniques,
improving reasoning in Small Language Models (SLMs) remains challenging due to
their limited model capacity. Recent work by Deepseek-R1 demonstrates that
distillation from LLM-generated synthetic data can substantially improve the
reasoning ability of SLM. However, the detailed modeling recipe is not
disclosed. In this work, we present a systematic training recipe for SLMs that
consists of four steps: (1) large-scale mid-training on diverse distilled
long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3)
Rollout DPO leveraging a carefully curated preference dataset, and (4)
Reinforcement Learning (RL) with Verifiable Reward. We apply our method on
Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning
model exceeds, on math reasoning tasks, much larger reasoning models, e.g.,
outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and
DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate
that a carefully designed training recipe, with large-scale high-quality CoT
data, is effective to unlock strong reasoning capabilities even in
resource-constrained small models.

</details>

### [103] [Memorization and Knowledge Injection in Gated LLMs](https://arxiv.org/abs/2504.21239)
*Xu Pan,Ely Hahami,Zechen Zhang,Haim Sompolinsky*

Main category: cs.CL

TLDR: MEGa框架通过将记忆嵌入到LLM的权重中，解决了LLM无法持续学习和整合新知识的问题。


<details>
  <summary>Details</summary>
Motivation: LLM在持续学习和记忆整合方面表现不佳，而人类能够不断学习新知识。

Method: MEGa将事件记忆嵌入到LLM的权重中，使用低秩权重和门控机制激活相关记忆。

Result: 在虚构角色和维基百科事件数据集上，MEGa优于基线方法，减少了灾难性遗忘。

Conclusion: MEGa受人类大脑互补记忆系统启发，为LLM的持续学习提供了有效解决方案。

Abstract: Large Language Models (LLMs) currently struggle to sequentially add new
memories and integrate new knowledge. These limitations contrast with the human
ability to continuously learn from new experiences and acquire knowledge
throughout life. Most existing approaches add memories either through large
context windows or external memory buffers (e.g., Retrieval-Augmented
Generation), and studies on knowledge injection rarely test scenarios
resembling everyday life events. In this work, we introduce a continual
learning framework, Memory Embedded in Gated LLMs (MEGa), which injects event
memories directly into the weights of LLMs. Each memory is stored in a
dedicated set of gated low-rank weights. During inference, a gating mechanism
activates relevant memory weights by matching query embeddings to stored memory
embeddings. This enables the model to both recall entire memories and answer
related questions. On two datasets - fictional characters and Wikipedia events
- MEGa outperforms baseline approaches in mitigating catastrophic forgetting.
Our model draws inspiration from the complementary memory system of the human
brain.

</details>

### [104] [Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA](https://arxiv.org/abs/2504.21252)
*Xuanzhao Dong,Wenhui Zhu,Hao Wang,Xiwen Chen,Peijie Qiu,Rui Yin,Yi Su,Yalin Wang*

Main category: cs.CL

TLDR: Discuss-RAG通过协作代理推理增强医学QA系统，显著提升答案准确性。


<details>
  <summary>Details</summary>
Motivation: 医学QA任务对大型语言模型具有挑战性，现有RAG系统存在推理行为建模不足和语料库质量低的问题。

Method: 提出Discuss-RAG模块，引入总结代理和决策代理，模拟多轮头脑风暴以优化检索内容。

Result: 在四个医学QA数据集上表现优于MedRAG，BioASQ和PubMedQA的准确率分别提升16.67%和12.20%。

Conclusion: Discuss-RAG通过协作代理推理有效解决了医学RAG系统的局限性，显著提升了性能。

Abstract: Medical question answering (QA) is a reasoning-intensive task that remains
challenging for large language models (LLMs) due to hallucinations and outdated
domain knowledge. Retrieval-Augmented Generation (RAG) provides a promising
post-training solution by leveraging external knowledge. However, existing
medical RAG systems suffer from two key limitations: (1) a lack of modeling for
human-like reasoning behaviors during information retrieval, and (2) reliance
on suboptimal medical corpora, which often results in the retrieval of
irrelevant or noisy snippets. To overcome these challenges, we propose
Discuss-RAG, a plug-and-play module designed to enhance the medical QA RAG
system through collaborative agent-based reasoning. Our method introduces a
summarizer agent that orchestrates a team of medical experts to emulate
multi-turn brainstorming, thereby improving the relevance of retrieved content.
Additionally, a decision-making agent evaluates the retrieved snippets before
their final integration. Experimental results on four benchmark medical QA
datasets show that Discuss-RAG consistently outperforms MedRAG, especially
significantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on
PubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG.

</details>

### [105] [BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models](https://arxiv.org/abs/2504.21299)
*Zhiting Fan,Ruizhe Chen,Zuozhu Liu*

Main category: cs.CL

TLDR: BiasGuard是一种新型偏见检测工具，通过两阶段方法（显式推理和强化学习）提升偏见检测的准确性和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如公平性分类器和基于LLM的评判）在理解意图和公平性判断标准方面存在局限性，需要更有效的工具。

Method: BiasGuard采用两阶段方法：第一阶段基于公平性规范显式推理，第二阶段通过强化学习增强推理和判断能力。

Result: 在五个数据集上的实验表明，BiasGuard优于现有工具，提高了准确性并减少了过度公平误判。

Conclusion: BiasGuard证明了推理增强决策的重要性，其两阶段优化方法有效。

Abstract: Identifying bias in LLM-generated content is a crucial prerequisite for
ensuring fairness in LLMs. Existing methods, such as fairness classifiers and
LLM-based judges, face limitations related to difficulties in understanding
underlying intentions and the lack of criteria for fairness judgment. In this
paper, we introduce BiasGuard, a novel bias detection tool that explicitly
analyzes inputs and reasons through fairness specifications to provide accurate
judgments. BiasGuard is implemented through a two-stage approach: the first
stage initializes the model to explicitly reason based on fairness
specifications, while the second stage leverages reinforcement learning to
enhance its reasoning and judgment capabilities. Our experiments, conducted
across five datasets, demonstrate that BiasGuard outperforms existing tools,
improving accuracy and reducing over-fairness misjudgments. We also highlight
the importance of reasoning-enhanced decision-making and provide evidence for
the effectiveness of our two-stage optimization pipeline.

</details>

### [106] [Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges](https://arxiv.org/abs/2504.21303)
*Xiao Xiao,Yu Su,Sijing Zhang,Zhang Chen,Yadong Chen,Tian Liu*

Main category: cs.CL

TLDR: 论文提出了一种基于贝叶斯方法的LLM能力评估框架，通过概率推断整合先验知识，解决了小样本场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法依赖确定性标量指标，无法充分反映LLM的概率输出特性，尤其在样本有限时表现不佳。

Method: 将模型能力视为潜变量，利用精心设计的查询集引发判别性响应，将模型排名问题形式化为贝叶斯假设检验。

Result: 实验证明该方法在GPT系列模型上优于传统评估方法，即使样本量减少也能保持统计稳健性。

Conclusion: 该研究通过贝叶斯推断与实际部署约束的结合，推动了LLM评估方法的发展。

Abstract: Large language models (LLMs) exhibit probabilistic output characteristics,
yet conventional evaluation frameworks rely on deterministic scalar metrics.
This study introduces a Bayesian approach for LLM capability assessment that
integrates prior knowledge through probabilistic inference, addressing
limitations under limited-sample regimes. By treating model capabilities as
latent variables and leveraging a curated query set to induce discriminative
responses, we formalize model ranking as a Bayesian hypothesis testing problem
over mutually exclusive capability intervals. Experimental evaluations with
GPT-series models demonstrate that the proposed method achieves superior
discrimination compared to conventional evaluation methods. Results indicate
that even with reduced sample sizes, the approach maintains statistical
robustness while providing actionable insights, such as probabilistic
statements about a model's likelihood of surpassing specific baselines. This
work advances LLM evaluation methodologies by bridging Bayesian inference with
practical constraints in real-world deployment scenarios.

</details>

### [107] [Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?](https://arxiv.org/abs/2504.21330)
*Kaixun Yang,Mladen Raković,Dragan Gašević,Guanliang Chen*

Main category: cs.CL

TLDR: 研究发现，基于提示的大型语言模型（LLM）在自动作文评分（AES）中能推断学生的人口统计信息（如母语背景），且评分偏差在模型正确预测学生母语时更显著。


<details>
  <summary>Details</summary>
Motivation: 探索基于提示的LLM在AES中是否存在人口统计偏见，尤其是对弱势群体的偏见。

Method: 使用公开数据集（25,000+学生议论文），设计提示让GPT-4o推断人口统计信息（性别、母语背景），并通过多元回归分析评估评分公平性。

Result: （1）LLM能部分推断学生人口统计信息；（2）评分偏差在正确预测母语时更显著；（3）非母语者的评分误差在LLM正确识别时增加。

Conclusion: 基于提示的LLM在AES中存在人口统计偏见，需进一步研究以改进公平性。

Abstract: Large Language Models (LLMs) are widely used in Automated Essay Scoring (AES)
due to their ability to capture semantic meaning. Traditional fine-tuning
approaches required technical expertise, limiting accessibility for educators
with limited technical backgrounds. However, prompt-based tools like ChatGPT
have made AES more accessible, enabling educators to obtain machine-generated
scores using natural-language prompts (i.e., the prompt-based paradigm).
Despite advancements, prior studies have shown bias in fine-tuned LLMs,
particularly against disadvantaged groups. It remains unclear whether such
biases persist or are amplified in the prompt-based paradigm with cutting-edge
tools. Since such biases are believed to stem from the demographic information
embedded in pre-trained models (i.e., the ability of LLMs' text embeddings to
predict demographic attributes), this study explores the relationship between
the model's predictive power of students' demographic attributes based on their
written works and its predictive bias in the scoring task in the prompt-based
paradigm. Using a publicly available dataset of over 25,000 students'
argumentative essays, we designed prompts to elicit demographic inferences
(i.e., gender, first-language background) from GPT-4o and assessed fairness in
automated scoring. Then we conducted multivariate regression analysis to
explore the impact of the model's ability to predict demographics on its
scoring outcomes. Our findings revealed that (i) prompt-based LLMs can somewhat
infer students' demographics, particularly their first-language backgrounds,
from their essays; (ii) scoring biases are more pronounced when the LLM
correctly predicts students' first-language background than when it does not;
and (iii) scoring error for non-native English speakers increases when the LLM
correctly identifies them as non-native.

</details>

### [108] [Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction](https://arxiv.org/abs/2504.21372)
*Máté Gedeon*

Main category: cs.CL

TLDR: 论文提出了一种模块化的语音事件提取框架SpeechEE，结合高性能ASR和语义搜索增强的LLM提示，显著提升了事件提取性能。


<details>
  <summary>Details</summary>
Motivation: 语音事件提取（SpeechEE）是ASR与NLP交叉领域的挑战性任务，需要从口语中识别结构化事件信息。

Method: 采用模块化流水线框架，先通过混合过滤机制分类可能包含事件的语音片段，再通过语义相似性检索增强的少样本LLM提示提取事件触发词和参数。

Result: 使用多种LLM评估，其中o1-mini表现最佳，触发词分类F1达63.3%，参数分类F1达27.8%，超越先前基准。

Conclusion: 流水线方法结合检索增强的LLM可媲美端到端系统，同时保持可解释性和模块化，为未来结合文本与声学特征的混合模型提供了方向。

Abstract: Speech Event Extraction (SpeechEE) is a challenging task that lies at the
intersection of Automatic Speech Recognition (ASR) and Natural Language
Processing (NLP), requiring the identification of structured event information
from spoken language. In this work, we present a modular, pipeline-based
SpeechEE framework that integrates high-performance ASR with semantic
search-enhanced prompting of Large Language Models (LLMs). Our system first
classifies speech segments likely to contain events using a hybrid filtering
mechanism including rule-based, BERT-based, and LLM-based models. It then
employs few-shot LLM prompting, dynamically enriched via semantic similarity
retrieval, to identify event triggers and extract corresponding arguments. We
evaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini)
highlighting significant performance gains with o1-mini, which achieves 63.3%
F1 on trigger classification and 27.8% F1 on argument classification,
outperforming prior benchmarks. Our results demonstrate that pipeline
approaches, when empowered by retrieval-augmented LLMs, can rival or exceed
end-to-end systems while maintaining interpretability and modularity. This work
provides practical insights into LLM-driven event extraction and opens pathways
for future hybrid models combining textual and acoustic features.

</details>

### [109] [The Distribution of Dependency Distance and Hierarchical Distance in Contemporary Written Japanese and Its Influencing Factors](https://arxiv.org/abs/2504.21421)
*Linxuan Wang,Shuiyuan Yu*

Main category: cs.CL

TLDR: 研究了日语中依存距离（DD）和层次距离（HD）的关系，发现谓词的配价是影响两者权衡关系的关键因素。


<details>
  <summary>Details</summary>
Motivation: 探索日语中DD和HD的关系，以及谓词配价对两者分布和均值的影响。

Method: 通过固定和不固定句子长度，比较DD和HD的概率分布，分析MDD和MHD随句子长度的变化及其相关性。

Result: 谓词配价是MDD和MHD权衡关系的基础，且对HD分布的影响大于DD，导致MDD均值低于MHD。

Conclusion: 日语母语者通过谓词配价调节线性和层次复杂度，配价阈值决定MDD和MHD的相对大小。

Abstract: To explore the relationship between dependency distance (DD) and hierarchical
distance (HD) in Japanese, we compared the probability distributions of DD and
HD with and without sentence length fixed, and analyzed the changes in mean
dependency distance (MDD) and mean hierarchical distance (MHD) as sentence
length increases, along with their correlation coefficient based on the
Balanced Corpus of Contemporary Written Japanese. It was found that the valency
of the predicates is the underlying factor behind the trade-off relation
between MDD and MHD in Japanese. Native speakers of Japanese regulate the
linear complexity and hierarchical complexity through the valency of the
predicates, and the relative sizes of MDD and MHD depend on whether the
threshold of valency has been reached. Apart from the cognitive load, the
valency of the predicates also affects the probability distributions of DD and
HD. The effect of the valency of the predicates on the distribution of HD is
greater than on that of DD, which leads to differences in their probability
distributions and causes the mean of MDD to be lower than that of MHD.

</details>

### [110] [RWKV-X: A Linear Complexity Hybrid Language Model](https://arxiv.org/abs/2504.21463)
*Haowen Hou,Zhiyi Huang,Kaifeng Tan,Rongchang Lu,Fei Richard Yu*

Main category: cs.CL

TLDR: RWKV-X是一种新型混合架构，结合了RWKV的短程建模效率和稀疏注意力机制的长程上下文捕捉能力，实现了线性训练时间和恒定推理时间。


<details>
  <summary>Details</summary>
Motivation: 解决现有混合方法依赖全注意力层导致二次复杂度的问题，同时提升长程上下文建模能力。

Method: 结合RWKV的短程建模与稀疏注意力机制，优化训练和推理效率。

Result: 在64K标记序列上预训练后，RWKV-X在64K passkey检索基准上表现优异，长程任务优于RWKV-7，短程任务性能稳定。

Conclusion: RWKV-X是一种高效、可扩展的通用语言建模框架，支持百万标记序列解码，代码和模型已开源。

Abstract: In this paper, we introduce \textbf{RWKV-X}, a novel hybrid architecture that
combines the efficiency of RWKV for short-range modeling with a sparse
attention mechanism designed to capture long-range context. Unlike previous
hybrid approaches that rely on full attention layers and retain quadratic
complexity, RWKV-X achieves linear-time complexity in training and
constant-time complexity in inference decoding. We demonstrate that RWKV-X,
when continually pretrained on 64K-token sequences, achieves near-perfect
accuracy on the 64K passkey retrieval benchmark. It consistently outperforms
prior RWKV-7 models on long-context benchmarks, while maintaining strong
performance on short-context tasks. These results highlight RWKV-X as a
scalable and efficient backbone for general-purpose language modeling, capable
of decoding sequences up to 1 million tokens with stable speed and memory
usage. To facilitate further research and analysis, we have made the
checkpoints and the associated code publicly accessible at:
https://github.com/howard-hou/RWKV-X.

</details>

### [111] [Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging](https://arxiv.org/abs/2504.21474)
*Hadi Bayrami Asl Tekanlou,Jafar Razmara,Mahsa Sanaei,Mostafa Rahgouy,Hamed Babaei Giglou*

Main category: cs.CL

TLDR: Homa系统利用OntoAligner工具包和RAG技术，将主题标注问题转化为对齐任务，评估其在多语言记录中的效果。


<details>
  <summary>Details</summary>
Motivation: 解决TIBKAT技术记录中自动标注GND主题标签的挑战。

Method: 使用OntoAligner工具包和RAG技术，将记录与GND类别基于语义相似度对齐。

Result: 实验展示了方法的优势和局限性，验证了对齐技术在数字图书馆主题标注中的潜力。

Conclusion: 对齐技术可有效提升数字图书馆的主题标注效果。

Abstract: This paper presents our system, Homa, for SemEval-2025 Task 5: Subject
Tagging, which focuses on automatically assigning subject labels to technical
records from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage
OntoAligner, a modular ontology alignment toolkit, to address this task by
integrating retrieval-augmented generation (RAG) techniques. Our approach
formulates the subject tagging problem as an alignment task, where records are
matched to GND categories based on semantic similarity. We evaluate
OntoAligner's adaptability for subject indexing and analyze its effectiveness
in handling multilingual records. Experimental results demonstrate the
strengths and limitations of this method, highlighting the potential of
alignment techniques for improving subject tagging in digital libraries.

</details>

### [112] [Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines](https://arxiv.org/abs/2504.21475)
*Serry Sibaee,Samar Ahmed,Abdullah Al Harbi,Omer Nacar,Adel Ammar,Yasser Habashi,Wadii Boulila*

Main category: cs.CL

TLDR: 该研究开发了一种基于Transformer的阿拉伯语反向词典系统，填补了阿拉伯语自然语言处理的空白，并提出了高质量词典资源构建的标准。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语自然语言处理中反向词典任务的不足，提升语言学习、学术写作和专业交流的效率。

Method: 采用半编码器神经网络架构，结合几何递减层和预训练模型（如ARBERTv2），构建数据集并制定质量标准。

Result: ARBERTv2模型表现最佳（排名得分0.0644），并开发了可扩展的Python库（RDTL）和八项词典资源构建标准。

Conclusion: 该研究显著推动了阿拉伯语计算语言学的发展，并为相关应用提供了实用工具和理论支持。

Abstract: This study addresses the critical gap in Arabic natural language processing
by developing an effective Arabic Reverse Dictionary (RD) system that enables
users to find words based on their descriptions or meanings. We present a novel
transformer-based approach with a semi-encoder neural network architecture
featuring geometrically decreasing layers that achieves state-of-the-art
results for Arabic RD tasks. Our methodology incorporates a comprehensive
dataset construction process and establishes formal quality standards for
Arabic lexicographic definitions. Experiments with various pre-trained models
demonstrate that Arabic-specific models significantly outperform general
multilingual embeddings, with ARBERTv2 achieving the best ranking score
(0.0644). Additionally, we provide a formal abstraction of the reverse
dictionary task that enhances theoretical understanding and develop a modular,
extensible Python library (RDTL) with configurable training pipelines. Our
analysis of dataset quality reveals important insights for improving Arabic
definition construction, leading to eight specific standards for building
high-quality reverse dictionary resources. This work contributes significantly
to Arabic computational linguistics and provides valuable tools for language
learning, academic writing, and professional communication in Arabic.

</details>

### [113] [Improving Informally Romanized Language Identification](https://arxiv.org/abs/2504.21540)
*Adrian Benton,Alexander Gutkin,Christo Kirov,Brian Roark*

Main category: cs.CL

TLDR: 论文提出了一种通过合成训练集改进罗马化文本语言识别（LID）准确性的方法，显著提升了20种印度语言的识别性能。


<details>
  <summary>Details</summary>
Motivation: 罗马化文本的拼写变异性导致语言识别困难，如印地语和乌尔都语易混淆。

Method: 通过合成包含自然拼写变异的训练样本，结合线性分类器训练。

Result: 在Bhasha-Abhijnaanam评估集上，F1分数从74.7%提升至88.2%。

Conclusion: 合成训练数据能显著提升罗马化文本的语言识别性能。

Abstract: The Latin script is often used to informally write languages with non-Latin
native scripts. In many cases (e.g., most languages in India), there is no
conventional spelling of words in the Latin script, hence there will be high
spelling variability in written text. Such romanization renders languages that
are normally easily distinguished based on script highly confusable, such as
Hindi and Urdu. In this work, we increase language identification (LID)
accuracy for romanized text by improving the methods used to synthesize
training sets. We find that training on synthetic samples which incorporate
natural spelling variation yields higher LID system accuracy than including
available naturally occurring examples in the training set, or even training
higher capacity models. We demonstrate new state-of-the-art LID performance on
romanized text from 20 Indic languages in the Bhasha-Abhijnaanam evaluation set
(Madhani et al., 2023a), improving test F1 from the reported 74.7% (using a
pretrained neural model) to 85.4% using a linear classifier trained solely on
synthetic data and 88.2% when also training on available harvested text.

</details>

### [114] [TartuNLP at SemEval-2025 Task 5: Subject Tagging as Two-Stage Information Retrieval](https://arxiv.org/abs/2504.21547)
*Aleksei Dorkin,Kairit Sirts*

Main category: cs.CL

TLDR: 论文提出了一种两阶段信息检索系统，用于为图书馆记录分配主题标签，显著提高了召回率。


<details>
  <summary>Details</summary>
Motivation: 帮助图书馆员为文档分配相关主题标签，提高标签分配的效率和准确性。

Method: 使用双编码器模型构建两阶段检索系统：第一阶段用双编码器粗粒度提取候选标签，第二阶段用交叉编码器细粒度重排序。

Result: 该方法显著提高了召回率，并在定性评估中表现出竞争力。

Conclusion: 两阶段检索系统在主题标签分配任务中效果显著，优于单阶段方法。

Abstract: We present our submission to the Task 5 of SemEval-2025 that aims to aid
librarians in assigning subject tags to the library records by producing a list
of likely relevant tags for a given document. We frame the task as an
information retrieval problem, where the document content is used to retrieve
subject tags from a large subject taxonomy. We leverage two types of encoder
models to build a two-stage information retrieval system -- a bi-encoder for
coarse-grained candidate extraction at the first stage, and a cross-encoder for
fine-grained re-ranking at the second stage. This approach proved effective,
demonstrating significant improvements in recall compared to single-stage
methods and showing competitive results according to qualitative evaluation.

</details>

### [115] [Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models](https://arxiv.org/abs/2504.21553)
*Lucas Maisonnave,Cyril Moineau,Olivier Bichler,Fabrice Rastello*

Main category: cs.CL

TLDR: 提出了一种针对LLaMA架构的混合精度量化方法，通过在高激活层使用高精度（FP16/FP8），其余部分低比特量化，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLM）部署和推理中的量化挑战，尤其是LLaMA架构的激活异常问题。

Method: 提出混合精度量化方法，针对LLaMA架构中高激活的投影层使用高精度，其余部分低比特量化。

Result: 在LLaMA2、LLaMA3和Mistral模型上，8比特量化表现优于通用方法，显著降低困惑度并提高零样本准确率。

Conclusion: 研究强调了针对特定架构设计量化策略的重要性，为资源受限环境下的LLM部署提供了有效方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various natural language processing tasks. However, their size presents
significant challenges for deployment and inference. This paper investigates
the quantization of LLMs, focusing on the LLaMA architecture and its
derivatives. We challenge existing assumptions about activation outliers in
LLMs and propose a novel mixed-precision quantization approach tailored for
LLaMA-like models. Our method leverages the observation that activation spikes
in LLaMA architectures are predominantly concentrated in specific projection
layers. By applying higher precision (FP16 or FP8) to these layers while
quantizing the rest of the model to lower bit-widths, we achieve superior
performance compared to existing quantization techniques. Experimental results
on LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in
perplexity and zero-shot accuracy, particularly for 8-bit per-tensor
quantization. Our approach outperforms general-purpose methods designed to
handle outliers across all architecture types, highlighting the benefits of
architecture-specific quantization strategies. This research contributes to the
ongoing efforts to make LLMs more efficient and deployable, potentially
enabling their use in resource-constrained environments. Our findings emphasize
the importance of considering model-specific characteristics in developing
effective quantization pipelines for state-of-the-art language models by
identifying and targeting a small number of projections that concentrate
activation spikes.

</details>

### [116] [DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing](https://arxiv.org/abs/2504.21589)
*Lisa Kluge,Maximilian Kähler*

Main category: cs.CL

TLDR: 本文介绍了为SemEval-2025任务5开发的系统，利用LLM基于少量示例自动标注技术图书馆开放目录的主题标签。


<details>
  <summary>Details</summary>
Motivation: 解决技术图书馆开放目录中主题标签自动标注的需求，提升标注效率与准确性。

Method: 采用少量示例提示技术结合后处理步骤，包括关键词映射、集合投票和相关性排序。

Result: 系统在定量排名中位列第四，但在专家定性评价中表现最佳。

Conclusion: 该方法在主题标注任务中具有实际应用潜力，尤其在专家评价中表现突出。

Abstract: This paper presents our system developed for the SemEval-2025 Task 5:
LLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical
Library's Open-Access Catalog. Our system relies on prompting a selection of
LLMs with varying examples of intellectually annotated records and asking the
LLMs to similarly suggest keywords for new records. This few-shot prompting
technique is combined with a series of post-processing steps that map the
generated keywords to the target vocabulary, aggregate the resulting subject
terms to an ensemble vote and, finally, rank them as to their relevance to the
record. Our system is fourth in the quantitative ranking in the all-subjects
track, but achieves the best result in the qualitative ranking conducted by
subject indexing experts.

</details>

### [117] [Robust Misinformation Detection by Visiting Potential Commonsense Conflict](https://arxiv.org/abs/2504.21604)
*Bing Wang,Ximing Li,Changchun Li,Bingrui Zhao,Bo Fu,Renchu Guan,Shengsheng Wang*

Main category: cs.CL

TLDR: 提出了一种新的插件式增强方法MD-PCC，用于自动检测网络虚假信息，通过利用常识冲突作为增强特征，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 互联网技术的发展导致虚假信息泛滥，对社会各领域造成负面影响，亟需自动检测虚假信息的方法。

Method: 提出MD-PCC方法，通过构建常识表达式捕捉虚假信息中的常识冲突，并将其作为增强特征用于训练现有虚假信息检测模型。

Result: 在4个公共基准数据集和新收集的CoMis数据集上，MD-PCC方法均优于现有基线方法。

Conclusion: MD-PCC通过利用常识冲突作为增强特征，有效提升了虚假信息检测的性能，具有广泛适用性。

Abstract: The development of Internet technology has led to an increased prevalence of
misinformation, causing severe negative effects across diverse domains. To
mitigate this challenge, Misinformation Detection (MD), aiming to detect online
misinformation automatically, emerges as a rapidly growing research topic in
the community. In this paper, we propose a novel plug-and-play augmentation
method for the MD task, namely Misinformation Detection with Potential
Commonsense Conflict (MD-PCC). We take inspiration from the prior studies
indicating that fake articles are more likely to involve commonsense conflict.
Accordingly, we construct commonsense expressions for articles, serving to
express potential commonsense conflicts inferred by the difference between
extracted commonsense triplet and golden ones inferred by the well-established
commonsense reasoning tool COMET. These expressions are then specified for each
article as augmentation. Any specific MD methods can be then trained on those
commonsense-augmented articles. Besides, we also collect a novel
commonsense-oriented dataset named CoMis, whose all fake articles are caused by
commonsense conflict. We integrate MD-PCC with various existing MD backbones
and compare them across both 4 public benchmark datasets and CoMis. Empirical
results demonstrate that MD-PCC can consistently outperform the existing MD
baselines.

</details>

### [118] [RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations](https://arxiv.org/abs/2504.21605)
*Jonas Gwozdz,Andreas Both*

Main category: cs.CL

TLDR: 提出了一种基于RDF的框架，用于评估多语言大语言模型（LLM）在知识冲突情况下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为知识接口的可靠性评估缺乏系统性，尤其是在信息冲突的情况下。

Method: 通过四种上下文条件（完整、不完整、冲突和无上下文）在德语和英语中捕获模型响应，并分析知识泄漏、错误检测和多语言一致性。

Result: 在消防安全领域的实验中，框架揭示了上下文优先级和语言特定性能的关键模式，并证明其词汇足以表达28个问题研究中的所有评估方面。

Conclusion: 该框架为评估LLM在多语言和知识冲突场景下的表现提供了系统化的方法。

Abstract: Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet
systematically assessing their reliability with conflicting information remains
difficult. We propose an RDF-based framework to assess multilingual LLM
quality, focusing on knowledge conflicts. Our approach captures model responses
across four distinct context conditions (complete, incomplete, conflicting, and
no-context information) in German and English. This structured representation
enables the comprehensive analysis of knowledge leakage-where models favor
training data over provided context-error detection, and multilingual
consistency. We demonstrate the framework through a fire safety domain
experiment, revealing critical patterns in context prioritization and
language-specific performance, and demonstrating that our vocabulary was
sufficient to express every assessment facet encountered in the 28-question
study.

</details>

### [119] [Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability](https://arxiv.org/abs/2504.21625)
*Jiaming Wang*

Main category: cs.CL

TLDR: Meeseeks是一个新的基准测试，通过迭代反馈过程模拟真实的人-LLM交互，允许模型自我纠正，并全面评估LLM的指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 现有指令跟随基准测试多为单轮或不允许自我纠正，无法反映真实用户使用模式。

Method: Meeseeks采用迭代反馈设计，支持模型自我纠正，并通过38个能力标签在三个维度（意图识别、细粒度内容验证、输出结构验证）进行评估。

Result: Meeseeks为LLM在实际应用中的指令跟随能力提供了有价值的见解。

Conclusion: Meeseeks通过更贴近现实的交互设计，提升了LLM指令跟随能力的评估效果。

Abstract: The ability to follow instructions accurately is fundamental for Large
Language Models (LLMs) to serve as reliable agents in real-world applications.
While existing instruction-following benchmarks are either single-turn or
introduce new requirements in each turn without allowing self-correction,
Meeseeks simulates realistic human-LLM interactions through an iterative
feedback process. This design enables models to self-correct based on specific
requirement failures, better reflecting real-world user-end usage patterns. The
benchmark implements a comprehensive evaluation system with 38 capability tags
organized across three dimensions: Intent Recognition, Granular Content
Validation, and Output Structure Validation. Through rigorous evaluation across
LLMs, Meeseeks provides valuable insights into LLMs' instruction-following
capabilities in practical applications.

</details>

### [120] [Sadeed: Advancing Arabic Diacritization Through Small Language Model](https://arxiv.org/abs/2504.21635)
*Zeina Aldallal,Sara Chrouf,Khalil Hennara,Mohamed Motaism Hamed,Muhammad Hreden,Safwan AlModhayan*

Main category: cs.CL

TLDR: Sadeed是一种基于Kuwain 1.5B微调的阿拉伯语文本标注新方法，性能优于传统模型，并提出了新基准SadeedDiac-25。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语文本标注因其形态丰富性一直是自然语言处理的挑战。

Method: 基于Kuwain 1.5B微调的解码器语言模型，使用高质量标注数据集。

Result: 在有限计算资源下表现优于传统模型，与大型专有模型竞争。

Conclusion: Sadeed和SadeedDiac-25为阿拉伯语NLP应用提供了坚实基础。

Abstract: Arabic text diacritization remains a persistent challenge in natural language
processing due to the language's morphological richness. In this paper, we
introduce Sadeed, a novel approach based on a fine-tuned decoder-only language
model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model
originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully
curated, high-quality diacritized datasets, constructed through a rigorous
data-cleaning and normalization pipeline. Despite utilizing modest
computational resources, Sadeed achieves competitive results compared to
proprietary large language models and outperforms traditional models trained on
similar domains. Additionally, we highlight key limitations in current
benchmarking practices for Arabic diacritization. To address these issues, we
introduce SadeedDiac-25, a new benchmark designed to enable fairer and more
comprehensive evaluation across diverse text genres and complexity levels.
Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing
Arabic NLP applications, including machine translation, text-to-speech, and
language learning tools.

</details>

### [121] [20min-XD: A Comparable Corpus of Swiss News Articles](https://arxiv.org/abs/2504.21677)
*Michelle Wastl,Jannis Vamvas,Selena Calleri,Rico Sennrich*

Main category: cs.CL

TLDR: 20min-XD是一个法语-德语新闻文章可比语料库，包含约15,000对文章，时间跨度为2015至2024年，基于语义相似度自动对齐。


<details>
  <summary>Details</summary>
Motivation: 为NLP应用和语言学研究的需要，构建一个跨语言可比语料库。

Method: 通过自动对齐方法基于语义相似度对齐新闻文章，并详细描述数据收集和对齐过程。

Result: 语料库展示了从近似翻译到松散相关文章的广泛跨语言相似性。

Conclusion: 该数据集对NLP应用和语言学研究具有重要价值，并已公开数据集和实验代码。

Abstract: We present 20min-XD (20 Minuten cross-lingual document-level), a
French-German, document-level comparable corpus of news articles, sourced from
the Swiss online news outlet 20 Minuten/20 minutes. Our dataset comprises
around 15,000 article pairs spanning 2015 to 2024, automatically aligned based
on semantic similarity. We detail the data collection process and alignment
methodology. Furthermore, we provide a qualitative and quantitative analysis of
the corpus. The resulting dataset exhibits a broad spectrum of cross-lingual
similarity, ranging from near-translations to loosely related articles, making
it valuable for various NLP applications and broad linguistically motivated
studies. We publicly release the dataset in document- and sentence-aligned
versions and code for the described experiments.

</details>

### [122] [Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders](https://arxiv.org/abs/2504.21681)
*Andrei-Alexandru Manea,Jindřich Libovický*

Main category: cs.CL

TLDR: 研究多语言视觉-语言任务中并行数据对预训练模型迁移的影响，发现机器翻译的任务数据效果最佳，但某些语言中真实的并行数据表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有预训练视觉-语言模型和训练数据多为英语，多语言任务需依赖跨语言迁移，本文探讨并行数据的作用。

Method: 通过并行数据迁移已训练的编码器，研究并行数据的领域和语言数量的影响。

Result: 机器翻译的任务数据平均效果最佳，但某些语言中真实并行数据表现更好；多语言训练对多数语言有益。

Conclusion: 并行数据的类型和语言数量对迁移效果有显著影响，多语言训练能提升性能。

Abstract: Most pre-trained Vision-Language (VL) models and training data for the
downstream tasks are only available in English. Therefore, multilingual VL
tasks are solved using cross-lingual transfer: fine-tune a multilingual
pre-trained model or transfer the text encoder using parallel data. We study
the alternative approach: transferring an already trained encoder using
parallel data. We investigate the effect of parallel data: domain and the
number of languages, which were out of focus in previous work. Our results show
that even machine-translated task data are the best on average, caption-like
authentic parallel data outperformed it in some languages. Further, we show
that most languages benefit from multilingual training.

</details>

### [123] [Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning](https://arxiv.org/abs/2504.21685)
*Reem Abdel-Salam,Mary Adewunmi*

Main category: cs.CL

TLDR: 论文提出了一种通过改进生物医学NLP方法的参数来优化健康提及分类（HMC）的方法，结合POS标记信息和PEFT技术，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 健康提及分类（HMC）在社交媒体实时追踪和公共卫生监测中至关重要，但由于上下文复杂性（如比喻语言和描述性术语），分类任务具有挑战性。

Method: 研究探索了结合POS标记信息和改进的PEFT技术，并在RHDM、PHM和Illness三个数据集上进行了实验。

Result: 实验结果表明，结合POS标记信息和PEFT技术显著提高了F1分数，同时使用了更小的模型和更高效的训练。

Conclusion: 该方法为社交媒体中的健康提及分类提供了一种高效且准确的解决方案，同时优化了模型大小和训练效率。

Abstract: Health Mention Classification (HMC) plays a critical role in leveraging
social media posts for real-time tracking and public health monitoring.
Nevertheless, the process of HMC presents significant challenges due to its
intricate nature, primarily stemming from the contextual aspects of health
mentions, such as figurative language and descriptive terminology, rather than
explicitly reflecting a personal ailment. To address this problem, we argue
that clearer mentions can be achieved through conventional fine-tuning with
enhanced parameters of biomedical natural language methods (NLP). In this
study, we explore different techniques such as the utilisation of
part-of-speech (POS) tagger information, improving on PEFT techniques, and
different combinations thereof. Extensive experiments are conducted on three
widely used datasets: RHDM, PHM, and Illness. The results incorporated POS
tagger information, and leveraging PEFT techniques significantly improves
performance in terms of F1-score compared to state-of-the-art methods across
all three datasets by utilising smaller models and efficient training.
Furthermore, the findings highlight the effectiveness of incorporating POS
tagger information and leveraging PEFT techniques for HMC. In conclusion, the
proposed methodology presents a potentially effective approach to accurately
classifying health mentions in social media posts while optimising the model
size and training efficiency.

</details>

### [124] [Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models](https://arxiv.org/abs/2504.21742)
*Emelie Hallenberg*

Main category: cs.CL

TLDR: 利用大型语言模型分析希腊爱情小说中的文学母题，揭示共性与差异。


<details>
  <summary>Details</summary>
Motivation: 探究希腊爱情小说中文学母题的共性与变化，以了解其趋势或外部影响。

Method: 使用精细调整的大型语言模型提取和分析文学母题。

Result: 部分母题贯穿始终，其他母题频率波动，显示趋势或外部影响。

Conclusion: 方法能有效提取文学母题，支持定量与定性分析。

Abstract: The Greek fictional narratives often termed love novels or romances, ranging
from the first century CE to the middle of the 15th century, have long been
considered as similar in many ways, not least in the use of particular literary
motifs. By applying the use of fine-tuned large language models, this study
aims to investigate which motifs exactly that the texts in this corpus have in
common, and in which ways they differ from each other. The results show that
while some motifs persist throughout the corpus, others fluctuate in frequency,
indicating certain trends or external influences. Conclusively, the method
proves to adequately extract literary motifs according to a set definition,
providing data for both quantitative and qualitative analyses.

</details>

### [125] [Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data](https://arxiv.org/abs/2504.21747)
*Maxime Bouthors,Josep Crego,François Yvon*

Main category: cs.CL

TLDR: 本文提出了一种改进的跨语言检索系统，利用目标语言的单语语料库提升检索增强神经机器翻译（RANMT）的性能。


<details>
  <summary>Details</summary>
Motivation: 传统RANMT系统依赖双语语料库（如翻译记忆库），但在许多场景中，目标语言的单语语料库更丰富。本文旨在利用这些资源。

Method: 设计了改进的跨语言检索系统，结合句子级和词级匹配目标进行训练，并在两种RANMT架构中验证其效果。

Result: 实验表明，该方法在控制环境中优于传统基于翻译记忆库的模型，并在实际场景中显著提升翻译性能。

Conclusion: 新方法在目标单语资源丰富的情况下表现优异，超越了基线设置和通用跨语言检索器。

Abstract: Conventional retrieval-augmented neural machine translation (RANMT) systems
leverage bilingual corpora, e.g., translation memories (TMs). Yet, in many
settings, in-domain monolingual target-side corpora are often available. This
work explores ways to take advantage of such resources by retrieving relevant
segments directly in the target language, based on a source-side query. For
this, we design improved cross-lingual retrieval systems, trained with both
sentence level and word-level matching objectives. In our experiments with two
RANMT architectures, we first demonstrate the benefits of such cross-lingual
objectives in a controlled setting, obtaining translation performances that
surpass standard TM-based models. We then showcase our method on a real-world
set-up, where the target monolingual resources far exceed the amount of
parallel data and observe large improvements of our new techniques, which
outperform both the baseline setting, and general-purpose cross-lingual
retrievers.

</details>

### [126] [MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness](https://arxiv.org/abs/2504.21773)
*Junsheng Huang,Zhitao He,Sandeep Polisetty,Qingyun Wang,May Fung*

Main category: cs.CL

TLDR: 论文提出了一种新方法MAC-Tuning，用于在多问题设置下增强大语言模型（LLMs）对自身知识边界的信心估计，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在多问题设置下生成虚假事实（幻觉）的问题，现有研究多关注单一问题设置，多问题设置下的信心估计研究不足。

Method: 提出MAC-Tuning方法，通过在指令数据微调中分离答案预测和信心估计的学习。

Result: 实验表明，MAC-Tuning在平均精度上比基线方法提升高达25%。

Conclusion: MAC-Tuning在多问题设置下有效提升了LLMs的信心估计能力，为减少幻觉问题提供了新思路。

Abstract: With the widespread application of large language models (LLMs), the issue of
generating non-existing facts, known as hallucination, has garnered increasing
attention. Previous research in enhancing LLM confidence estimation mainly
focuses on the single problem setting. However, LLM awareness of its internal
parameterized knowledge boundary under the more challenging multi-problem
setting, which requires answering multiple problems accurately simultaneously,
remains underexplored. To bridge this gap, we introduce a novel method,
Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates
the learning of answer prediction and confidence estimation during fine-tuning
on instruction data. Extensive experiments demonstrate that our method
outperforms baselines by up to 25% in average precision.

</details>

### [127] [WebThinker: Empowering Large Reasoning Models with Deep Research Capability](https://arxiv.org/abs/2504.21776)
*Xiaoxi Li,Jiajie Jin,Guanting Dong,Hongjin Qian,Yutao Zhu,Yongkang Wu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.CL

TLDR: WebThinker是一种深度研究代理，通过动态搜索和整合网络信息，提升大型推理模型在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 静态知识限制了大型推理模型在复杂任务中的表现，需要动态获取外部信息以提升能力。

Method: 结合深度网络探索模块和自主思考-搜索-草拟策略，通过RL训练优化模型。

Result: 在多个复杂推理和报告生成任务中显著优于现有方法。

Conclusion: WebThinker提升了模型的可靠性和适用性，为深度研究系统开辟了新方向。

Abstract: Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate
impressive long-horizon reasoning capabilities. However, their reliance on
static internal knowledge limits their performance on complex,
knowledge-intensive tasks and hinders their ability to produce comprehensive
research reports requiring synthesis of diverse web information. To address
this, we propose \textbf{WebThinker}, a deep research agent that empowers LRMs
to autonomously search the web, navigate web pages, and draft research reports
during the reasoning process. WebThinker integrates a \textbf{Deep Web
Explorer} module, enabling LRMs to dynamically search, navigate, and extract
information from the web when encountering knowledge gaps. It also employs an
\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to
seamlessly interleave reasoning, information gathering, and report writing in
real time. To further enhance research tool utilization, we introduce an
\textbf{RL-based training strategy} via iterative online Direct Preference
Optimization (DPO). Extensive experiments on complex reasoning benchmarks
(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)
demonstrate that WebThinker significantly outperforms existing methods and
strong proprietary systems. Our approach enhances LRM reliability and
applicability in complex scenarios, paving the way for more capable and
versatile deep research systems. The code is available at
https://github.com/RUC-NLPIR/WebThinker.

</details>

### [128] [How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues](https://arxiv.org/abs/2504.21800)
*Suhas BN,Dominik Mattioli,Saeed Abdullah,Rosa I. Arriaga,Chris W. Wiese,Andrew M. Sherrill*

Main category: cs.CL

TLDR: 论文探讨了在PTSD治疗中使用合成对话数据的潜力，发现其虽能补充真实数据，但在捕捉治疗交互的细微动态上存在不足。


<details>
  <summary>Details</summary>
Motivation: 隐私问题、真实数据获取困难和标注成本高推动了合成数据在医疗领域的应用。

Method: 通过语言、结构和协议特定指标（如对话模式和治疗保真度）系统比较真实与合成对话。

Result: 合成数据在结构特征上与真实数据接近，但在关键保真度标记上表现不足。

Conclusion: 合成数据可缓解数据稀缺和隐私问题，但需开发更深入的临床保真度评估指标。

Abstract: The growing adoption of synthetic data in healthcare is driven by privacy
concerns, limited access to real-world data, and the high cost of annotation.
This work explores the use of synthetic Prolonged Exposure (PE) therapeutic
conversations for Post-Traumatic Stress Disorder (PTSD) as a scalable
alternative for training and evaluating clinical models. We systematically
compare real and synthetic dialogues using linguistic, structural, and
protocol-specific metrics, including turn-taking patterns and treatment
fidelity. We also introduce and evaluate PE-specific metrics derived from
linguistic analysis and semantic modeling, offering a novel framework for
assessing clinical fidelity beyond surface fluency. Our findings show that
although synthetic data holds promise for mitigating data scarcity and
protecting patient privacy, it can struggle to capture the subtle dynamics of
therapeutic interactions. In our dataset, synthetic dialogues match structural
features of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99),
however, synthetic interactions do not adequately reflect key fidelity markers
(e.g., distress monitoring). We highlight gaps in existing evaluation
frameworks and advocate for fidelity-aware metrics that go beyond surface
fluency to uncover clinically significant failures. Our findings clarify where
synthetic data can effectively complement real-world datasets -- and where
critical limitations remain.

</details>

### [129] [DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition](https://arxiv.org/abs/2504.21801)
*Z. Z. Ren,Zhihong Shao,Junxiao Song,Huajian Xin,Haocheng Wang,Wanjia Zhao,Liyue Zhang,Zhe Fu,Qihao Zhu,Dejian Yang,Z. F. Wu,Zhibin Gou,Shirong Ma,Hongxuan Tang,Yuxuan Liu,Wenjun Gao,Daya Guo,Chong Ruan*

Main category: cs.CL

TLDR: DeepSeek-Prover-V2是一个开源大语言模型，专为Lean 4中的形式化定理证明设计，通过递归定理证明流程初始化数据，结合非形式化和形式化数学推理，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 旨在缩小大语言模型中形式化和非形式化数学推理之间的差距，并提升定理证明的性能。

Method: 通过递归定理证明流程分解复杂问题为子目标，结合DeepSeek-V3的分步推理生成初始数据，用于强化学习训练。

Result: 在MiniF2F-test中达到88.9%通过率，解决PutnamBench中的49/658问题，并在ProverBench和AIME问题中表现良好。

Conclusion: DeepSeek-Prover-V2在形式化定理证明中表现优异，缩小了形式化与非形式化推理的差距。

Abstract: We introduce DeepSeek-Prover-V2, an open-source large language model designed
for formal theorem proving in Lean 4, with initialization data collected
through a recursive theorem proving pipeline powered by DeepSeek-V3. The
cold-start training procedure begins by prompting DeepSeek-V3 to decompose
complex problems into a series of subgoals. The proofs of resolved subgoals are
synthesized into a chain-of-thought process, combined with DeepSeek-V3's
step-by-step reasoning, to create an initial cold start for reinforcement
learning. This process enables us to integrate both informal and formal
mathematical reasoning into a unified model. The resulting model,
DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural
theorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49
out of 658 problems from PutnamBench. In addition to standard benchmarks, we
introduce ProverBench, a collection of 325 formalized problems, to enrich our
evaluation, including 15 selected problems from the recent AIME competitions
(years 24-25). Further evaluation on these 15 AIME problems shows that the
model successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of
these problems using majority voting, highlighting that the gap between formal
and informal mathematical reasoning in large language models is substantially
narrowing.

</details>

### [130] [TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments](https://arxiv.org/abs/2504.21851)
*Sichang Tu,Abigail Powers,Stephen Doogan,Jinho D. Choi*

Main category: cs.CL

TLDR: 该研究开发了一个基于LLM的对话系统TRUST，用于标准化诊断访谈和评估，特别针对PTSD，并通过专家评估验证其效果。


<details>
  <summary>Details</summary>
Motivation: 填补心理健康领域标准化诊断访谈的空白，提升医疗可及性。

Method: 提出TRUST框架，结合LLM模块和临床访谈专用的Dialogue Acts模式，并采用患者模拟方法替代人工测试。

Result: 专家评估显示TRUST效果与真实临床访谈相当，但仍有改进空间。

Conclusion: TRUST框架有望提升心理健康服务的可及性。

Abstract: Objectives: While Large Language Models (LLMs) have been widely used to
assist clinicians and support patients, no existing work has explored dialogue
systems for standard diagnostic interviews and assessments. This study aims to
bridge the gap in mental healthcare accessibility by developing an LLM-powered
dialogue system that replicates clinician behavior. Materials and Methods: We
introduce TRUST, a framework of cooperative LLM modules capable of conducting
formal diagnostic interviews and assessments for Post-Traumatic Stress Disorder
(PTSD). To guide the generation of appropriate clinical responses, we propose a
Dialogue Acts schema specifically designed for clinical interviews.
Additionally, we develop a patient simulation approach based on real-life
interview transcripts to replace time-consuming and costly manual testing by
clinicians. Results: A comprehensive set of evaluation metrics is designed to
assess the dialogue system from both the agent and patient simulation
perspectives. Expert evaluations by conversation and clinical specialists show
that TRUST performs comparably to real-life clinical interviews. Discussion:
Our system performs at the level of average clinicians, with room for future
enhancements in communication styles and response appropriateness. Conclusions:
Our TRUST framework shows its potential to facilitate mental healthcare
availability.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [131] [A Formalism for Optimal Search with Dynamic Heuristics](https://arxiv.org/abs/2504.21131)
*Remo Christen,Florian Pommerening,Clemens Büchner,Malte Helmert*

Main category: cs.AI

TLDR: 论文研究了动态启发式在搜索算法中的应用，提出了一个通用框架，并证明了其最优性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在使用动态启发式时忽视了其可变性带来的复杂性，需要更严谨的理论支持。

Method: 形式化动态启发式的概念，并在通用算法框架中使用，具体实例化了一个动态启发式的A*算法。

Result: 证明了该框架的最优性，并将经典规划中的现有方法视为其特例。

Conclusion: 动态启发式的通用框架为相关研究提供了理论基础，并可直接应用于经典规划问题。

Abstract: While most heuristics studied in heuristic search depend only on the state,
some accumulate information during search and thus also depend on the search
history. Various existing approaches use such dynamic heuristics in
$\mathrm{A}^*$-like algorithms and appeal to classic results for $\mathrm{A}^*$
to show optimality. However, doing so ignores the complexities of searching
with a mutable heuristic. In this paper we formalize the idea of dynamic
heuristics and use them in a generic algorithm framework. We study a particular
instantiation that models $\mathrm{A}^*$ with dynamic heuristics and show
general optimality results. Finally we show how existing approaches from
classical planning can be viewed as special cases of this instantiation, making
it possible to directly apply our optimality results.

</details>

### [132] [AffectEval: A Modular and Customizable Framework for Affective Computing](https://arxiv.org/abs/2504.21184)
*Emily Zhou,Khushboo Khatri,Yixue Zhao,Bhaskar Krishnamachari*

Main category: cs.AI

TLDR: AffectEval是一个模块化、可定制的框架，旨在减少情感计算管道开发中的手动工作和重复劳动，编程工作量可减少90%。


<details>
  <summary>Details</summary>
Motivation: 情感计算领域缺乏支持多模态、多领域情感识别应用的软件框架，导致开发管道时冗余工作多。

Method: 提出AffectEval框架，通过模块化和定制化设计减少手动工作，并通过复现实验验证其效果。

Result: AffectEval显著减少了编程工作量（减少90%的代码行数）。

Conclusion: AffectEval为情感计算管道的开发提供了高效、通用的解决方案。

Abstract: The field of affective computing focuses on recognizing, interpreting, and
responding to human emotions, and has broad applications across education,
child development, and human health and wellness. However, developing affective
computing pipelines remains labor-intensive due to the lack of software
frameworks that support multimodal, multi-domain emotion recognition
applications. This often results in redundant effort when building pipelines
for different applications. While recent frameworks attempt to address these
challenges, they remain limited in reducing manual effort and ensuring
cross-domain generalizability. We introduce AffectEval, a modular and
customizable framework to facilitate the development of affective computing
pipelines while reducing the manual effort and duplicate work involved in
developing such pipelines. We validate AffectEval by replicating prior
affective computing experiments, and we demonstrate that our framework reduces
programming effort by up to 90%, as measured by the reduction in raw lines of
code.

</details>

### [133] [Theoretical Foundations for Semantic Cognition in Artificial Intelligence](https://arxiv.org/abs/2504.21218)
*Sebastian Dumbrava*

Main category: cs.AI

TLDR: 本文提出了一种基于结构化语义状态的模块化认知架构，用于人工智能。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种能够支持自调节、目标导向思维的认知框架，结合哲学、认知科学和神经科学的理论。

Method: 通过定义动态的语言表达集合作为信念状态，并引入操作符实现同化、抽象、消除、记忆和自省等功能。

Result: 提出了一个可实现的框架，适用于符号系统和神经网络，包括大型语言模型和混合智能体。

Conclusion: 该工作为构建具有结构化、可解释性的推理、记忆和信念调节能力的智能体提供了理论基础。

Abstract: This monograph presents a modular cognitive architecture for artificial
intelligence grounded in the formal modeling of belief as structured semantic
state. Belief states are defined as dynamic ensembles of linguistic expressions
embedded within a navigable manifold, where operators enable assimilation,
abstraction, nullification, memory, and introspection. Drawing from philosophy,
cognitive science, and neuroscience, we develop a layered framework that
enables self-regulating epistemic agents capable of reflective, goal-directed
thought. At the core of this framework is the epistemic vacuum: a class of
semantically inert cognitive states that serves as the conceptual origin of
belief space. From this foundation, the Null Tower arises as a generative
structure recursively built through internal representational capacities. The
theoretical constructs are designed to be implementable in both symbolic and
neural systems, including large language models, hybrid agents, and adaptive
memory architectures. This work offers a foundational substrate for
constructing agents that reason, remember, and regulate their beliefs in
structured, interpretable ways.

</details>

### [134] [Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2504.21277)
*Guanghao Zhou,Panjia Qiu,Cen Chen,Jie Wang,Zheming Yang,Jian Xu,Minghui Qiu*

Main category: cs.AI

TLDR: 综述探讨了强化学习在多模态大语言模型推理中的应用，总结了算法设计、奖励机制及实践，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决多模态输入下推理能力不足的问题，推动强化学习在多模态大语言模型中的应用。

Method: 系统回顾了基于强化学习的推理方法，包括无价值与基于价值的方法，优化推理轨迹和多模态信息对齐。

Result: 总结了基准数据集、评估协议及现有局限性，提出了稀疏奖励、跨模态推理效率等未来方向。

Conclusion: 为研究者提供了全面的指南，推动多模态时代强化学习推理的发展。

Abstract: The integration of reinforcement learning (RL) into the reasoning
capabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as
a transformative research direction. While MLLMs significantly extend Large
Language Models (LLMs) to handle diverse modalities such as vision, audio, and
video, enabling robust reasoning across multimodal inputs remains a major
challenge. This survey systematically reviews recent advances in RL-based
reasoning for MLLMs, covering key algorithmic designs, reward mechanism
innovations, and practical applications. We highlight two main RL
paradigms--value-free and value-based methods--and analyze how RL enhances
reasoning abilities by optimizing reasoning trajectories and aligning
multimodal information. Furthermore, we provide an extensive overview of
benchmark datasets, evaluation protocols, and existing limitations, and propose
future research directions to address current bottlenecks such as sparse
rewards, inefficient cross-modal reasoning, and real-world deployment
constraints. Our goal is to offer a comprehensive and structured guide to
researchers interested in advancing RL-based reasoning in the multimodal era.

</details>

### [135] [Phi-4-reasoning Technical Report](https://arxiv.org/abs/2504.21318)
*Marah Abdin,Sahaj Agarwal,Ahmed Awadallah,Vidhisha Balachandran,Harkirat Behl,Lingjiao Chen,Gustavo de Rosa,Suriya Gunasekar,Mojan Javaheripi,Neel Joshi,Piero Kauffmann,Yash Lara,Caio César Teodoro Mendes,Arindam Mitra,Besmira Nushi,Dimitris Papailiopoulos,Olli Saarikivi,Shital Shah,Vaishnavi Shrivastava,Vibhav Vineet,Yue Wu,Safoora Yousefi,Guoqing Zheng*

Main category: cs.AI

TLDR: Phi-4-reasoning是一个14B参数的推理模型，通过监督微调和强化学习优化，在复杂推理任务中表现优异，甚至超越更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过精细的数据选择和训练方法提升推理模型的性能，并探索其在多领域任务中的表现。

Method: 采用监督微调（SFT）和基于结果的强化学习（RL）训练模型，生成详细的推理链。

Result: 模型在数学、科学推理、编程等多个领域表现优异，接近更大规模模型的性能，并显示出通用基准任务的迁移能力。

Conclusion: 研究表明精细数据选择和强化学习对提升推理模型性能至关重要，同时指出了评估方法的改进空间。

Abstract: We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that
achieves strong performance on complex reasoning tasks. Trained via supervised
fine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selected
for the right level of complexity and diversity-and reasoning demonstrations
generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains
that effectively leverage inference-time compute. We further develop
Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based
reinforcement learning that offers higher performance by generating longer
reasoning traces. Across a wide range of reasoning tasks, both models
outperform significantly larger open-weight models such as
DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full
DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and
scientific reasoning, coding, algorithmic problem solving, planning, and
spatial understanding. Interestingly, we observe a non-trivial transfer of
improvements to general-purpose benchmarks as well. In this report, we provide
insights into our training data, our training methodologies, and our
evaluations. We show that the benefit of careful data curation for supervised
fine-tuning (SFT) extends to reasoning language models, and can be further
amplified by reinforcement learning (RL). Finally, our evaluation points to
opportunities for improving how we assess the performance and robustness of
reasoning models.

</details>

### [136] [IRL Dittos: Embodied Multimodal AI Agent Interactions in Open Spaces](https://arxiv.org/abs/2504.21347)
*Seonghee Lee,Denae Ford,John Tang,Sasa Junuzovic,Asta Roseway,Ed Cutrell,Kori Inkpen*

Main category: cs.AI

TLDR: IRL Ditto是一个AI驱动的实体代理，旨在代表远程同事在共享办公空间中的存在，促进实时交流。研究发现其增强社交关系的能力取决于参与者与远程同事的既有关系基础。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过IRL Ditto这种实体代理增强分布式团队中同事之间的互动和关系。

Method: 通过为期四天的研究，评估IRL Ditto在模拟存在和促进不同社交熟悉度下的互动方面的能力。

Result: 研究发现，IRL Ditto增强社交关系的效果与参与者与远程同事的既有关系密切相关。

Conclusion: 研究揭示了实体代理在丰富分布式团队工作场所动态中的潜在作用。

Abstract: We introduce the In Real Life (IRL) Ditto, an AI-driven embodied agent
designed to represent remote colleagues in shared office spaces, creating
opportunities for real-time exchanges even in their absence. IRL Ditto offers a
unique hybrid experience by allowing in-person colleagues to encounter a
digital version of their remote teammates, initiating greetings, updates, or
small talk as they might in person. Our research question examines: How can the
IRL Ditto influence interactions and relationships among colleagues in a shared
office space? Through a four-day study, we assessed IRL Ditto's ability to
strengthen social ties by simulating presence and enabling meaningful
interactions across different levels of social familiarity. We find that
enhancing social relationships depended deeply on the foundation of the
relationship participants had with the source of the IRL Ditto. This study
provides insights into the role of embodied agents in enriching workplace
dynamics for distributed teams.

</details>

### [137] [ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning](https://arxiv.org/abs/2504.21370)
*Jingyang Yi,Jiazheng Wang*

Main category: cs.AI

TLDR: 论文提出ShorterBetter方法，通过强化学习让推理模型自动找到最优的推理长度，减少输出长度80%同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型在长推理链中容易“过度思考”，导致推理效率低下。

Method: 使用强化学习，采样多个输出并定义最短正确响应为最优长度，动态引导模型。

Result: 在DeepSeek-Distill-Qwen-1.5B模型上，输出长度减少80%，准确性不变。

Conclusion: 长推理链常伴随方向迷失，表明推理模型的推理链高度可压缩。

Abstract: Reasoning models such as OpenAI o3 and DeepSeek-R1 have demonstrated strong
performance on reasoning-intensive tasks through extended Chain-of-Thought
(CoT) prompting. While longer reasoning traces can facilitate a more thorough
exploration of solution paths for complex problems, researchers have observed
that these models often "overthink", leading to inefficient inference. In this
paper, we introduce ShorterBetter, a simple yet effective reinforcement
learning methed that enables reasoning language models to discover their own
optimal CoT lengths without human intervention. By sampling multiple outputs
per problem and defining the Sample Optimal Length (SOL) as the shortest
correct response among all the outputs, our method dynamically guides the model
toward optimal inference lengths. Applied to the DeepSeek-Distill-Qwen-1.5B
model, ShorterBetter achieves up to an 80% reduction in output length on both
in-domain and out-of-domain reasoning tasks while maintaining accuracy. Our
analysis shows that overly long reasoning traces often reflect loss of
reasoning direction, and thus suggests that the extended CoT produced by
reasoning models is highly compressible.

</details>

### [138] [NGENT: Next-Generation AI Agents Must Integrate Multi-Domain Abilities to Achieve Artificial General Intelligence](https://arxiv.org/abs/2504.21433)
*Zhicong Li,Hangyu Mao,Jiangjin Yin,Mingzhe Xing,Zhiwei Xu,Yuanxing Zhang,Yang Xiao*

Main category: cs.AI

TLDR: 本文主张下一代AI代理（NGENT）需整合跨领域能力以实现通用人工智能（AGI），并提出统一框架整合文本、视觉、机器人等多领域技术。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理局限于狭窄领域，缺乏跨领域能力，而实现AGI需具备类似人类智能的多样性与适应性。

Method: 提出整合文本、视觉、机器人、强化学习、情感智能等领域的优势，构建统一框架。

Result: 跨领域整合不仅可行，且是实现AGI的关键步骤。

Conclusion: 开发多功能代理是实现AGI的必要方向，本文探讨了其理论基础与实现路径。

Abstract: This paper argues that the next generation of AI agent (NGENT) should
integrate across-domain abilities to advance toward Artificial General
Intelligence (AGI). Although current AI agents are effective in specialized
tasks such as robotics, role-playing, and tool-using, they remain confined to
narrow domains. We propose that future AI agents should synthesize the
strengths of these specialized systems into a unified framework capable of
operating across text, vision, robotics, reinforcement learning, emotional
intelligence, and beyond. This integration is not only feasible but also
essential for achieving the versatility and adaptability that characterize
human intelligence. The convergence of technologies across AI domains, coupled
with increasing user demand for cross-domain capabilities, suggests that such
integration is within reach. Ultimately, the development of these versatile
agents is a critical step toward realizing AGI. This paper explores the
rationale for this shift, potential pathways for achieving it.

</details>

### [139] [A Study on Group Decision Making Problem Based on Fuzzy Reasoning and Bayesian Networks](https://arxiv.org/abs/2504.21568)
*Shui-jin Rong,Wei Guo,Da-qing Zhang*

Main category: cs.AI

TLDR: 本文提出了一种结合模糊推理和贝叶斯网络的群决策系统，用于解决多目标属性的决策问题，并通过实验验证了其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对多目标属性的群决策问题，传统方法难以处理定量挑战（如尺度差异和专家语言变量），因此需要一种更有效的方法。

Method: 构建模糊规则库结合阈值、隶属函数和专家经验，设计分层贝叶斯网络动态优化条件概率表，建模多维指标的非线性相关性。

Result: 在综合学生评价案例中，分类准确率达86.0%，F1值比传统方法提升53.4%。

Conclusion: 该方法在规则构建和排序一致性上表现优异，且在不同场景下具有可靠性和鲁棒性。

Abstract: Aiming at the group decision - making problem with multi - objective
attributes, this study proposes a group decision - making system that
integrates fuzzy inference and Bayesian network. A fuzzy rule base is
constructed by combining threshold values, membership functions, expert
experience, and domain knowledge to address quantitative challenges such as
scale differences and expert linguistic variables. A hierarchical Bayesian
network is designed, featuring a directed acyclic graph with nodes selected by
experts, and maximum likelihood estimation is used to dynamically optimize the
conditional probability table, modeling the nonlinear correlations among
multidimensional indices for posterior probability aggregation. In a
comprehensive student evaluation case, this method is compared with the
traditional weighted scoring approach. The results indicate that the proposed
method demonstrates effectiveness in both rule criterion construction and
ranking consistency, with a classification accuracy of 86.0% and an F1 value
improvement of 53.4% over the traditional method. Additionally, computational
experiments on real - world datasets across various group decision scenarios
assess the method's performance and robustness, providing evidence of its
reliability in diverse contexts.

</details>

### [140] [Designing Control Barrier Function via Probabilistic Enumeration for Safe Reinforcement Learning Navigation](https://arxiv.org/abs/2504.21643)
*Luca Marzari,Francesco Trotti,Enrico Marchesini,Alessandro Farinelli*

Main category: cs.AI

TLDR: 提出了一种基于神经网络验证的分层控制框架，用于确保强化学习导航策略的安全性，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在动态和不确定的真实环境中，实现安全的自主导航系统对机器人部署至关重要。

Method: 利用神经网络验证技术设计控制屏障函数（CBFs）和策略校正机制，通过概率枚举识别不安全区域，并构建安全的CBF控制层。

Result: 在仿真和真实机器人实验中验证了框架的有效性，能够在不影响导航效率的情况下纠正不安全行为。

Conclusion: 分层验证系统在复杂场景中实现安全且鲁棒的导航行为具有潜力。

Abstract: Achieving safe autonomous navigation systems is critical for deploying robots
in dynamic and uncertain real-world environments. In this paper, we propose a
hierarchical control framework leveraging neural network verification
techniques to design control barrier functions (CBFs) and policy correction
mechanisms that ensure safe reinforcement learning navigation policies. Our
approach relies on probabilistic enumeration to identify unsafe regions of
operation, which are then used to construct a safe CBF-based control layer
applicable to arbitrary policies. We validate our framework both in simulation
and on a real robot, using a standard mobile robot benchmark and a highly
dynamic aquatic environmental monitoring task. These experiments demonstrate
the ability of the proposed solution to correct unsafe actions while preserving
efficient navigation behavior. Our results show the promise of developing
hierarchical verification-based systems to enable safe and robust navigation
behaviors in complex scenarios.

</details>

### [141] [AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization](https://arxiv.org/abs/2504.21659)
*Haotian Luo,Haiying He,Yibo Wang,Jinluan Yang,Rui Liu,Naiqiang Tan,Xiaochun Cao,Dacheng Tao,Li Shen*

Main category: cs.AI

TLDR: 论文提出了一种自适应推理框架，通过结合长短推理模型和双层偏好训练，显著降低了推理成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有长推理模型在复杂任务上表现优异，但推理开销大，且不同问题对推理深度的需求不同，需要自适应策略。

Method: 构建混合推理模型（长短CoT结合），并采用双层偏好训练（组级和实例级）选择合适推理风格。

Result: 在五个数学数据集上，平均推理长度减少50%以上，推理成本显著降低，性能未受影响。

Conclusion: 自适应策略能有效优化大型语言模型的推理效率，未来可进一步探索更高效的方法。

Abstract: Recently, long-thought reasoning models achieve strong performance on complex
reasoning tasks, but often incur substantial inference overhead, making
efficiency a critical concern. Our empirical analysis reveals that the benefit
of using Long-CoT varies across problems: while some problems require elaborate
reasoning, others show no improvement, or even degraded accuracy. This
motivates adaptive reasoning strategies that tailor reasoning depth to the
input. However, prior work primarily reduces redundancy within long reasoning
paths, limiting exploration of more efficient strategies beyond the Long-CoT
paradigm. To address this, we propose a novel two-stage framework for adaptive
and efficient reasoning. First, we construct a hybrid reasoning model by
merging long and short CoT models to enable diverse reasoning styles. Second,
we apply bi-level preference training to guide the model to select suitable
reasoning styles (group-level), and prefer concise and correct reasoning within
each style group (instance-level). Experiments demonstrate that our method
significantly reduces inference costs compared to other baseline approaches,
while maintaining performance. Notably, on five mathematical datasets, the
average length of reasoning is reduced by more than 50%, highlighting the
potential of adaptive strategies to optimize reasoning efficiency in large
language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1

</details>

### [142] [Extension-ranking Semantics for Abstract Argumentation Preprint](https://arxiv.org/abs/2504.21683)
*Kenneth Skiba,Tjitze Rienstra,Matthias Thimm,Jesse Heyninck,Gabriele Kern-Isberner*

Main category: cs.AI

TLDR: 本文提出了一种基于论证可接受性概率的抽象论证框架，扩展了Dung的扩展语义为扩展排序语义，并引入了一系列行为良好的语义原则。


<details>
  <summary>Details</summary>
Motivation: 为论证集合的排序提供一种通用框架，以衡量其可接受性。

Method: 扩展Dung的扩展语义为扩展排序语义，结合多个基础关系构建语义家族，并评估其行为。

Result: 提出了一系列扩展排序语义，并验证了其行为是否符合预期原则。

Conclusion: 该框架为论证集合的排序提供了灵活且理论支持的方法。

Abstract: In this paper, we present a general framework for ranking sets of arguments
in abstract argumentation based on their plausibility of acceptance. We present
a generalisation of Dung's extension semantics as extension-ranking semantics,
which induce a preorder over the power set of all arguments, allowing us to
state that one set is "closer" to being acceptable than another. To evaluate
the extension-ranking semantics, we introduce a number of principles that a
well-behaved extension-ranking semantics should satisfy. We consider several
simple base relations, each of which models a single central aspect of
argumentative reasoning. The combination of these base relations provides us
with a family of extension-ranking semantics. We also adapt a number of
approaches from the literature for ranking extensions to be usable in the
context of extension-ranking semantics, and evaluate their behaviour.

</details>

### [143] [Automatic Mapping of AutomationML Files to Ontologies for Graph Queries and Validation](https://arxiv.org/abs/2504.21694)
*Tom Westermann,Malte Ramonat,Johannes Hujer,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TLDR: 论文介绍了AutomationML的更新版本体和RDF转换方法，以解决XML工具在查询和数据验证中的局限性。


<details>
  <summary>Details</summary>
Motivation: AutomationML作为自动化领域的数据交换格式，其扩展语义限制了通用XML工具的适用性。

Method: 提出了AutomationML的本体概念和RDF转换映射，支持工业知识图谱集成。

Result: 研究表明，将AutomationML转换为OWL后，可实现更强大的查询和验证功能。

Conclusion: 通过本体和RDF转换，AutomationML能更好地融入知识图谱，提升应用潜力。

Abstract: AutomationML has seen widespread adoption as an open data exchange format in
the automation domain. It is an open and vendor neutral standard based on the
extensible markup language XML. However, AutomationML extends XML with
additional semantics, that limit the applicability of common XML-tools for
applications like querying or data validation. This article provides
practitioners with 1) an up-to-date ontology of the concepts in the
AutomationML-standard, as well as 2) a declarative mapping to automatically
transform any AutomationML model into RDF triples. Together, these artifacts
allow practitioners an easy integration of AutomationML information into
industrial knowledge graphs. A study on examples from the automation domain
concludes that transforming AutomationML to OWL opens up new powerful ways for
querying and validation that are impossible without transformation.

</details>

### [144] [Is Intermediate Fusion All You Need for UAV-based Collaborative Perception?](https://arxiv.org/abs/2504.21774)
*Jiuwu Hao,Liguo Sun,Yuting Wan,Yueyang Wu,Ti Xiang,Haolin Song,Pin Lv*

Main category: cs.AI

TLDR: 提出了一种基于延迟中间融合（LIF）的高效通信协作感知框架，通过交换紧凑的检测结果和特征级融合，减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有无人机协作感知方法未考虑无人机视角特性，导致通信开销大。

Method: 采用延迟中间融合（LIF），结合视觉引导位置嵌入（VPE）和基于框的虚拟增强特征（BoBEV），并引入不确定性驱动通信机制。

Result: 实验表明LIF在低通信带宽下表现优异。

Conclusion: LIF框架高效且实用，适合无人机协作感知。

Abstract: Collaborative perception enhances environmental awareness through inter-agent
communication and is regarded as a promising solution to intelligent
transportation systems. However, existing collaborative methods for Unmanned
Aerial Vehicles (UAVs) overlook the unique characteristics of the UAV
perspective, resulting in substantial communication overhead. To address this
issue, we propose a novel communication-efficient collaborative perception
framework based on late-intermediate fusion, dubbed LIF. The core concept is to
exchange informative and compact detection results and shift the fusion stage
to the feature representation level. In particular, we leverage vision-guided
positional embedding (VPE) and box-based virtual augmented feature (BoBEV) to
effectively integrate complementary information from various agents.
Additionally, we innovatively introduce an uncertainty-driven communication
mechanism that uses uncertainty evaluation to select high-quality and reliable
shared areas. Experimental results demonstrate that our LIF achieves superior
performance with minimal communication bandwidth, proving its effectiveness and
practicality. Code and models are available at https://github.com/uestchjw/LIF.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [145] [Can a Large Language Model Assess Urban Design Quality? Evaluating Walkability Metrics Across Expertise Levels](https://arxiv.org/abs/2504.21040)
*Chenyi Cai,Kosuke Kuriyama,Youlong Gu,Filip Biljecki,Pieter Herthogs*

Main category: cs.CV

TLDR: 研究探讨了如何通过整合专家知识提升多模态大语言模型（MLLM）在评估城市街道环境可步行性中的表现，发现专家知识能提高模型的一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: 城市街道环境对人类活动至关重要，但现有MLLM在评估时存在乐观评分和误判问题，需探索专家知识对其性能的影响。

Method: 通过文献收集可步行性指标，分类并设计不同清晰度的提示词，利用ChatGPT-4评估街景图像的可步行性。

Result: MLLM能基于通用知识提供评估，但评分偏乐观且易误判；整合专家知识后，评估表现更一致和集中。

Conclusion: 专家知识能显著提升MLLM在城市设计评估中的可靠性和能力，支持多模态图像-文本自动化评估。

Abstract: Urban street environments are vital to supporting human activity in public
spaces. The emergence of big data, such as street view images (SVIs) combined
with multimodal large language models (MLLMs), is transforming how researchers
and practitioners investigate, measure, and evaluate semantic and visual
elements of urban environments. Considering the low threshold for creating
automated evaluative workflows using MLLMs, it is crucial to explore both the
risks and opportunities associated with these probabilistic models. In
particular, the extent to which the integration of expert knowledge can
influence the performance of MLLMs in evaluating the quality of urban design
has not been fully explored. This study sets out an initial exploration of how
integrating more formal and structured representations of expert urban design
knowledge into the input prompts of an MLLM (ChatGPT-4) can enhance the model's
capability and reliability in evaluating the walkability of built environments
using SVIs. We collect walkability metrics from the existing literature and
categorize them using relevant ontologies. We then select a subset of these
metrics, focusing on the subthemes of pedestrian safety and attractiveness, and
develop prompts for the MLLM accordingly. We analyze the MLLM's ability to
evaluate SVI walkability subthemes through prompts with varying levels of
clarity and specificity regarding evaluation criteria. Our experiments
demonstrate that MLLMs are capable of providing assessments and interpretations
based on general knowledge and can support the automation of multimodal
image-text evaluations. However, they generally provide more optimistic scores
and can make mistakes when interpreting the provided metrics, resulting in
incorrect evaluations. By integrating expert knowledge, the MLLM's evaluative
performance exhibits higher consistency and concentration.

</details>

### [146] [Legilimens: Performant Video Analytics on the System-on-Chip Edge](https://arxiv.org/abs/2504.21136)
*Murali Ramanujam,Yinwei Dai,Kyle Jamieson,Ravi Netravali*

Main category: cs.CV

TLDR: Legilimens是一种针对移动边缘设备SoC GPU的持续学习系统，通过高效数据选择和轻量级模型更新，显著降低重训练成本并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 移动边缘设备（如无人机和行车记录仪）具有较弱的计算能力和丰富的统一内存池，传统边缘服务器的资源分配方式不适用，需要新的持续学习系统。

Method: Legilimens利用视觉场景在模型嵌入中的重叠性，提出高效数据选择、轻量级模型更新和计算资源共享技术。

Result: Legilimens将重训练成本降低2.8-10倍，并在多样化工作负载中实现18-45%的准确性提升。

Conclusion: Legilimens为移动边缘设备提供了一种高效的持续学习解决方案，显著提升了视频分析的准确性和资源利用率。

Abstract: Continually retraining models has emerged as a primary technique to enable
high-accuracy video analytics on edge devices. Yet, existing systems employ
such adaptation by relying on the spare compute resources that traditional
(memory-constrained) edge servers afford. In contrast, mobile edge devices such
as drones and dashcams offer a fundamentally different resource profile:
weak(er) compute with abundant unified memory pools. We present Legilimens, a
continuous learning system for the mobile edge's System-on-Chip GPUs. Our
driving insight is that visually distinct scenes that require retraining
exhibit substantial overlap in model embeddings; if captured into a base model
on device memory, specializing to each new scene can become lightweight,
requiring very few samples. To practically realize this approach, Legilimens
presents new, compute-efficient techniques to (1) select high-utility data
samples for retraining specialized models, (2) update the base model without
complete retraining, and (3) time-share compute resources between retraining
and live inference for maximal accuracy. Across diverse workloads, Legilimens
lowers retraining costs by 2.8-10x compared to existing systems, resulting in
18-45% higher accuracies.

</details>

### [147] [Emotion Recognition in Contemporary Dance Performances Using Laban Movement Analysis](https://arxiv.org/abs/2504.21154)
*Muhammad Turab,Philippe Colantoni,Damien Muselet,Alain Tremeau*

Main category: cs.CV

TLDR: 提出了一种改进Laban运动分析特征描述符的新框架，用于当代舞蹈中的情绪识别，结合定量和定性特征，并通过可解释机器学习方法分析特征影响。


<details>
  <summary>Details</summary>
Motivation: 改进现有情绪识别方法，捕捉舞蹈动作的定量和定性特征，提升识别准确性和应用潜力。

Method: 从3D关键点数据中提取特征，使用随机森林和支持向量机等分类器，结合可解释机器学习方法分析特征。

Result: 情绪识别准确率最高达96.85%，在表演分析、舞蹈训练和人机交互中有应用前景。

Conclusion: 该框架显著提升了当代舞蹈中的情绪识别效果，具有广泛的实际应用价值。

Abstract: This paper presents a novel framework for emotion recognition in contemporary
dance by improving existing Laban Movement Analysis (LMA) feature descriptors
and introducing robust, novel descriptors that capture both quantitative and
qualitative aspects of the movement. Our approach extracts expressive
characteristics from 3D keypoints data of professional dancers performing
contemporary dance under various emotional states, and trains multiple
classifiers, including Random Forests and Support Vector Machines.
Additionally, we provide in-depth explanation of features and their impact on
model predictions using explainable machine learning methods. Overall, our
study improves emotion recognition in contemporary dance and offers promising
applications in performance analysis, dance training, and human--computer
interaction, with a highest accuracy of 96.85\%.

</details>

### [148] [Dance Style Recognition Using Laban Movement Analysis](https://arxiv.org/abs/2504.21166)
*Muhammad Turab,Philippe Colantoni,Damien Muselet,Alain Tremeau*

Main category: cs.CV

TLDR: 提出了一种结合3D姿态估计、3D人体网格重建和地面感知身体建模的新方法，通过滑动窗口捕捉时间上下文，显著提高了舞蹈风格识别的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有舞蹈风格识别方法缺乏对时间上下文和动态过渡的捕捉，限制了性能。

Method: 结合3D姿态估计、3D人体网格重建和地面感知身体建模提取LMA特征，采用滑动窗口方法捕捉时间上下文，并用机器学习分类。

Result: 最高分类准确率达99.18%，表明时间上下文的加入显著提升了性能。

Conclusion: 提出的方法通过引入时间上下文，显著改善了舞蹈风格识别的效果。

Abstract: The growing interest in automated movement analysis has presented new
challenges in recognition of complex human activities including dance. This
study focuses on dance style recognition using features extracted using Laban
Movement Analysis. Previous studies for dance style recognition often focus on
cross-frame movement analysis, which limits the ability to capture temporal
context and dynamic transitions between movements. This gap highlights the need
for a method that can add temporal context to LMA features. For this, we
introduce a novel pipeline which combines 3D pose estimation, 3D human mesh
reconstruction, and floor aware body modeling to effectively extract LMA
features. To address the temporal limitation, we propose a sliding window
approach that captures movement evolution across time in features. These
features are then used to train various machine learning methods for
classification, and their explainability explainable AI methods to evaluate the
contribution of each feature to classification performance. Our proposed method
achieves a highest classification accuracy of 99.18\% which shows that the
addition of temporal context significantly improves dance style recognition
performance.

</details>

### [149] [Geolocating Earth Imagery from ISS: Integrating Machine Learning with Astronaut Photography for Enhanced Geographic Mapping](https://arxiv.org/abs/2504.21194)
*Vedika Srivastava,Hemant Kumar Singh,Jaisal Singh*

Main category: cs.CV

TLDR: 提出了一种利用机器学习算法从国际空间站（ISS）图像中定位地球位置的新方法，通过三种不同的图像处理流程（神经网络、SIFT和GPT-4）实现了高精度的地理特征识别。


<details>
  <summary>Details</summary>
Motivation: ISS拍摄的照片中具体地球位置常未被识别，研究旨在填补这一空白，提升空间图像的地理定位能力。

Method: 采用三种图像处理流程：基于神经网络的方法、SIFT方法和GPT-4模型，分别处理高分辨率ISS图像，识别自然和人工地理特征。

Result: 在140多张ISS图像数据集上验证，神经网络方法在地理特征匹配上表现最佳，SIFT擅长处理放大图像，GPT-4提供丰富的地理描述。

Conclusion: 研究提升了空间图像地理定位的准确性和效率，有助于环境监测和全球地图绘制。

Abstract: This paper presents a novel approach to geolocating images captured from the
International Space Station (ISS) using advanced machine learning algorithms.
Despite having precise ISS coordinates, the specific Earth locations depicted
in astronaut-taken photographs often remain unidentified. Our research
addresses this gap by employing three distinct image processing pipelines: a
Neural Network based approach, a SIFT based method, and GPT-4 model. Each
pipeline is tailored to process high-resolution ISS imagery, identifying both
natural and man-made geographical features. Through extensive evaluation on a
diverse dataset of over 140 ISS images, our methods demonstrate significant
promise in automated geolocation with varied levels of success. The NN approach
showed a high success rate in accurately matching geographical features, while
the SIFT pipeline excelled in processing zoomed-in images. GPT-4 model provided
enriched geographical descriptions alongside location predictions. This
research contributes to the fields of remote sensing and Earth observation by
enhancing the accuracy and efficiency of geolocating space-based imagery,
thereby aiding environmental monitoring and global mapping efforts.

</details>

### [150] [MemeBLIP2: A novel lightweight multimodal system to detect harmful memes](https://arxiv.org/abs/2504.21226)
*Jiaqi Liu,Ran Tong,Aowei Shen,Shuzheng Li,Changlin Yang,Lisha Xu*

Main category: cs.CV

TLDR: MemeBLIP2是一个轻量级多模态系统，通过结合图像和文本特征有效检测有害表情包。


<details>
  <summary>Details</summary>
Motivation: 表情包常结合图像和简短文本传播幽默或观点，但部分包含有害内容如仇恨言论，需有效检测。

Method: 基于BLIP-2核心模型，通过模块对齐图像和文本表示并融合，提升分类效果。

Result: 在PrideMM数据集上测试，MemeBLIP2能捕捉多模态细微线索，改进有害内容检测。

Conclusion: MemeBLIP2在多模态融合中表现优异，尤其对讽刺或文化特定内容有更好识别能力。

Abstract: Memes often merge visuals with brief text to share humor or opinions, yet
some memes contain harmful messages such as hate speech. In this paper, we
introduces MemeBLIP2, a light weight multimodal system that detects harmful
memes by combining image and text features effectively. We build on previous
studies by adding modules that align image and text representations into a
shared space and fuse them for better classification. Using BLIP-2 as the core
vision-language model, our system is evaluated on the PrideMM datasets. The
results show that MemeBLIP2 can capture subtle cues in both modalities, even in
cases with ironic or culturally specific content, thereby improving the
detection of harmful material.

</details>

### [151] [T2ID-CAS: Diffusion Model and Class Aware Sampling to Mitigate Class Imbalance in Neck Ultrasound Anatomical Landmark Detection](https://arxiv.org/abs/2504.21231)
*Manikanta Varaganti,Amulya Vankayalapati,Nour Awad,Gregory R. Dion,Laura J. Brattain*

Main category: cs.CV

TLDR: 提出了一种结合文本到图像潜在扩散模型和类感知采样的混合方法（T2ID-CAS），用于解决颈部超声中类别不平衡问题，显著提升了目标检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 颈部超声在气道管理中至关重要，但数据集中关键结构（如气管环和声带）的类别不平衡问题影响了目标检测模型的性能。

Method: 提出T2ID-CAS方法，结合文本到图像潜在扩散模型和类感知采样，生成高质量的合成样本以增强少数类别的表示。

Result: 实验结果显示，T2ID-CAS在YOLOv9模型上实现了88.2的平均精度，显著优于基线模型的66。

Conclusion: T2ID-CAS是一种计算高效且可扩展的解决方案，能够有效缓解AI辅助超声引导干预中的类别不平衡问题。

Abstract: Neck ultrasound (US) plays a vital role in airway management by providing
non-invasive, real-time imaging that enables rapid and precise interventions.
Deep learning-based anatomical landmark detection in neck US can further
facilitate procedural efficiency. However, class imbalance within datasets,
where key structures like tracheal rings and vocal folds are underrepresented,
presents significant challenges for object detection models. To address this,
we propose T2ID-CAS, a hybrid approach that combines a text-to-image latent
diffusion model with class-aware sampling to generate high-quality synthetic
samples for underrepresented classes. This approach, rarely explored in the
ultrasound domain, improves the representation of minority classes.
Experimental results using YOLOv9 for anatomical landmark detection in neck US
demonstrated that T2ID-CAS achieved a mean Average Precision of 88.2,
significantly surpassing the baseline of 66. This highlights its potential as a
computationally efficient and scalable solution for mitigating class imbalance
in AI-assisted ultrasound-guided interventions.

</details>

### [152] [Subject Information Extraction for Novelty Detection with Domain Shifts](https://arxiv.org/abs/2504.21247)
*Yangyang Qu,Dazhi Fu,Jicong Fan*

Main category: cs.CV

TLDR: 本文提出了一种解决无监督新颖性检测中域偏移问题的新方法，通过分离主体信息和背景变化来提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设训练和测试数据来自同一域，但现实中常存在域偏移，导致正常数据被误判为新颖。本文旨在解决这一问题。

Method: 提出了一种方法，通过最小化主体和背景表示之间的互信息，并用深度高斯混合模型建模背景变化，仅在主体表示上进行新颖性检测。

Result: 实验表明，该方法在未见域上表现优异，尤其在域偏移显著时优于基线方法。

Conclusion: 该方法有效解决了域偏移问题，提升了新颖性检测的性能。

Abstract: Unsupervised novelty detection (UND), aimed at identifying novel samples, is
essential in fields like medical diagnosis, cybersecurity, and industrial
quality control. Most existing UND methods assume that the training data and
testing normal data originate from the same domain and only consider the
distribution variation between training data and testing data. However, in real
scenarios, it is common for normal testing and training data to originate from
different domains, a challenge known as domain shift. The discrepancies between
training and testing data often lead to incorrect classification of normal data
as novel by existing methods. A typical situation is that testing normal data
and training data describe the same subject, yet they differ in the background
conditions. To address this problem, we introduce a novel method that separates
subject information from background variation encapsulating the domain
information to enhance detection performance under domain shifts. The proposed
method minimizes the mutual information between the representations of the
subject and background while modelling the background variation using a deep
Gaussian mixture model, where the novelty detection is conducted on the subject
representations solely and hence is not affected by the variation of domains.
Extensive experiments demonstrate that our model generalizes effectively to
unseen domains and significantly outperforms baseline methods, especially under
substantial domain shifts between training and testing data.

</details>

### [153] [Multi-modal Transfer Learning for Dynamic Facial Emotion Recognition in the Wild](https://arxiv.org/abs/2504.21248)
*Ezra Engel,Lishan Li,Chris Hudy,Robert Schleusner*

Main category: cs.CV

TLDR: 本文研究了多模态迁移学习在视频面部表情识别（FER）中的应用，通过结合预训练的ResNets、OpenPose和OmniVec网络，提升了在DFEW数据集上的分类准确性。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别（FER）在人机交互、医疗和客户服务中有重要应用，但由于需要区分细微的面部特征变化，其分类准确性具有挑战性。

Method: 使用多模态迁移学习方法，结合预训练的ResNets、OpenPose和OmniVec网络，探索跨时间和多模态特征对分类准确性的影响。

Result: 研究发现，这些精细调整的多模态特征生成器略微提升了基于Transformer的分类模型的准确性。

Conclusion: 多模态迁移学习在提升FER任务性能方面具有潜力，但改进幅度有限。

Abstract: Facial expression recognition (FER) is a subset of computer vision with
important applications for human-computer-interaction, healthcare, and customer
service. FER represents a challenging problem-space because accurate
classification requires a model to differentiate between subtle changes in
facial features. In this paper, we examine the use of multi-modal transfer
learning to improve performance on a challenging video-based FER dataset,
Dynamic Facial Expression in-the-Wild (DFEW). Using a combination of pretrained
ResNets, OpenPose, and OmniVec networks, we explore the impact of
cross-temporal, multi-modal features on classification accuracy. Ultimately, we
find that these finely-tuned multi-modal feature generators modestly improve
accuracy of our transformer-based classification model.

</details>

### [154] [Embracing Collaboration Over Competition: Condensing Multiple Prompts for Visual In-Context Learning](https://arxiv.org/abs/2504.21263)
*Jinpeng Wang,Tianci Luo,Yaohua Zha,Yan Feng,Ruisheng Luo,Bin Chen,Tao Dai,Long Chen,Yaowei Wang,Shu-Tao Xia*

Main category: cs.CV

TLDR: 论文提出了一种名为Condenser的轻量级外部插件，通过多提示协作解决视觉上下文学习中的提示选择问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉上下文学习（VICL）方法假设存在单一理想提示，但实践中可能存在多个合适提示，单独使用时效果不佳。

Method: 提出提示压缩（prompt condensation）方法，通过Condenser插件整合多提示的细粒度上下文信息，端到端优化。

Result: 实验显示Condenser在基准任务中表现优于现有方法，具有更好的上下文压缩、扩展性和计算效率。

Conclusion: Condenser为VICL提供了一种高效且可扩展的解决方案，代码已开源。

Abstract: Visual In-Context Learning (VICL) enables adaptively solving vision tasks by
leveraging pixel demonstrations, mimicking human-like task completion through
analogy. Prompt selection is critical in VICL, but current methods assume the
existence of a single "ideal" prompt in a pool of candidates, which in practice
may not hold true. Multiple suitable prompts may exist, but individually they
often fall short, leading to difficulties in selection and the exclusion of
useful context. To address this, we propose a new perspective: prompt
condensation. Rather than relying on a single prompt, candidate prompts
collaborate to efficiently integrate informative contexts without sacrificing
resolution. We devise Condenser, a lightweight external plugin that compresses
relevant fine-grained context across multiple prompts. Optimized end-to-end
with the backbone, Condenser ensures accurate integration of contextual cues.
Experiments demonstrate Condenser outperforms state-of-the-arts across
benchmark tasks, showing superior context compression, scalability with more
prompts, and enhanced computational efficiency compared to ensemble methods,
positioning it as a highly competitive solution for VICL. Code is open-sourced
at https://github.com/gimpong/CVPR25-Condenser.

</details>

### [155] [CoCoDiff: Diversifying Skeleton Action Features via Coarse-Fine Text-Co-Guided Latent Diffusion](https://arxiv.org/abs/2504.21266)
*Zhifu Zhao,Hanyang Hua,Jianan Li,Shaoxin Wu,Fu Li,Yangtao Zhou,Yang Li*

Main category: cs.CV

TLDR: CoCoDiff提出了一种基于扩散模型和文本引导的方法，用于生成多样且语义一致的特征，提升动作识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过扩展样本空间提升特征多样性，但效率低且语义不一致。

Method: 利用潜在扩散模型生成多样动作表示，并通过多粒度文本引导确保语义一致性。

Result: 在多个基准测试中达到SOTA性能。

Conclusion: CoCoDiff是一种高效且无需额外推理成本的辅助模块。

Abstract: In action recognition tasks, feature diversity is essential for enhancing
model generalization and performance. Existing methods typically promote
feature diversity by expanding the training data in the sample space, which
often leads to inefficiencies and semantic inconsistencies. To overcome these
problems, we propose a novel Coarse-fine text co-guidance Diffusion model
(CoCoDiff). CoCoDiff generates diverse yet semantically consistent features in
the latent space by leveraging diffusion and multi-granularity textual
guidance. Specifically, our approach feeds spatio-temporal features extracted
from skeleton sequences into a latent diffusion model to generate diverse
action representations. Meanwhile, we introduce a coarse-fine text co-guided
strategy that leverages textual information from large language models (LLMs)
to ensure semantic consistency between the generated features and the original
inputs. It is noted that CoCoDiff operates as a plug-and-play auxiliary module
during training, incurring no additional inference cost. Extensive experiments
demonstrate that CoCoDiff achieves SOTA performance on skeleton-based action
recognition benchmarks, including NTU RGB+D, NTU RGB+D 120 and
Kinetics-Skeleton.

</details>

### [156] [Mamba Based Feature Extraction And Adaptive Multilevel Feature Fusion For 3D Tumor Segmentation From Multi-modal Medical Image](https://arxiv.org/abs/2504.21281)
*Zexin Ji,Beiji Zou,Xiaoyan Kui,Hua Li,Pierre Vera,Su Ruan*

Main category: cs.CV

TLDR: 提出一种基于Mamba的特征提取和自适应多级特征融合方法，用于多模态3D医学图像分割，解决了传统CNN和Transformer方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 多模态3D医学图像分割面临图像强度和肿瘤形态变化的挑战，传统方法难以捕捉全局特征或计算成本高。

Method: 设计特定模态的Mamba编码器提取长距离特征，并通过双级协同集成块动态融合多模态和多层次特征。

Result: 在PET/CT和MRI多序列数据集上表现优于现有CNN、Transformer和Mamba方法。

Conclusion: 该方法在多模态3D肿瘤分割中表现出竞争力，有效融合了模态特异性特征和互补信息。

Abstract: Multi-modal 3D medical image segmentation aims to accurately identify tumor
regions across different modalities, facing challenges from variations in image
intensity and tumor morphology. Traditional convolutional neural network
(CNN)-based methods struggle with capturing global features, while
Transformers-based methods, despite effectively capturing global context,
encounter high computational costs in 3D medical image segmentation. The Mamba
model combines linear scalability with long-distance modeling, making it a
promising approach for visual representation learning. However, Mamba-based 3D
multi-modal segmentation still struggles to leverage modality-specific features
and fuse complementary information effectively. In this paper, we propose a
Mamba based feature extraction and adaptive multilevel feature fusion for 3D
tumor segmentation using multi-modal medical image. We first develop the
specific modality Mamba encoder to efficiently extract long-range relevant
features that represent anatomical and pathological structures present in each
modality. Moreover, we design an bi-level synergistic integration block that
dynamically merges multi-modal and multi-level complementary features by the
modality attention and channel attention learning. Lastly, the decoder combines
deep semantic information with fine-grained details to generate the tumor
segmentation map. Experimental results on medical image datasets (PET/CT and
MRI multi-sequence) show that our approach achieve competitive performance
compared to the state-of-the-art CNN, Transformer, and Mamba-based approaches.

</details>

### [157] [Can We Achieve Efficient Diffusion without Self-Attention? Distilling Self-Attention into Convolutions](https://arxiv.org/abs/2504.21292)
*ZiYi Dong,Chengxing Zhou,Weijian Deng,Pengxu Wei,Xiangyang Ji,Liang Lin*

Main category: cs.CV

TLDR: 论文提出ΔConvFusion方法，用局部卷积模块替代传统自注意力机制，显著降低计算成本，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 研究发现预训练扩散模型中的自注意力机制主要呈现局部模式，而非全局交互，因此探索更高效的替代方案。

Method: 提出ΔConvFusion，通过金字塔卷积块（ΔConvBlocks）替代自注意力模块，保留其他组件不变。

Result: ΔConvFusion计算成本降低6929倍，效率提升5.42倍，生成质量与基于Transformer的方法相当。

Conclusion: 局部卷积操作可有效替代自注意力机制，显著提升效率而不损失生成性能。

Abstract: Contemporary diffusion models built upon U-Net or Diffusion Transformer (DiT)
architectures have revolutionized image generation through transformer-based
attention mechanisms. The prevailing paradigm has commonly employed
self-attention with quadratic computational complexity to handle global spatial
relationships in complex images, thereby synthesizing high-fidelity images with
coherent visual semantics.Contrary to conventional wisdom, our systematic
layer-wise analysis reveals an interesting discrepancy: self-attention in
pre-trained diffusion models predominantly exhibits localized attention
patterns, closely resembling convolutional inductive biases. This suggests that
global interactions in self-attention may be less critical than commonly
assumed.Driven by this, we propose \(\Delta\)ConvFusion to replace conventional
self-attention modules with Pyramid Convolution Blocks
(\(\Delta\)ConvBlocks).By distilling attention patterns into localized
convolutional operations while keeping other components frozen,
\(\Delta\)ConvFusion achieves performance comparable to transformer-based
counterparts while reducing computational cost by 6929$\times$ and surpassing
LinFusion by 5.42$\times$ in efficiency--all without compromising generative
fidelity.

</details>

### [158] [Learning Multi-view Multi-class Anomaly Detection](https://arxiv.org/abs/2504.21294)
*Qianzi Yu,Yang Cao,Yu Kang*

Main category: cs.CV

TLDR: MVMCAD模型通过多视图信息整合和异常信号增强，显著提升了多视图多类异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有MCAD模型在多视图场景中表现不佳，未能有效建模视图间关系和互补信息。

Method: 提出半冻结编码器、异常放大模块和跨特征损失，以增强多视图下的异常检测能力。

Result: 在Real-IAD数据集上，图像级和像素级检测分别达到91.0/88.6/82.1和99.1/43.9/48.2/95.2的SOTA性能。

Conclusion: MVMCAD在多视图多类异常检测中表现出色，验证了其方法的有效性。

Abstract: The latest trend in anomaly detection is to train a unified model instead of
training a separate model for each category. However, existing multi-class
anomaly detection (MCAD) models perform poorly in multi-view scenarios because
they often fail to effectively model the relationships and complementary
information among different views. In this paper, we introduce a Multi-View
Multi-Class Anomaly Detection model (MVMCAD), which integrates information from
multiple views to accurately identify anomalies. Specifically, we propose a
semi-frozen encoder, where a pre-encoder prior enhancement mechanism is added
before the frozen encoder, enabling stable cross-view feature modeling and
efficient adaptation for improved anomaly detection. Furthermore, we propose an
Anomaly Amplification Module (AAM) that models global token interactions and
suppresses normal regions to enhance anomaly signals, leading to improved
detection performance in multi-view settings. Finally, we propose a
Cross-Feature Loss that aligns shallow encoder features with deep decoder
features and vice versa, enhancing the model's sensitivity to anomalies at
different semantic levels under multi-view scenarios. Extensive experiments on
the Real-IAD dataset for multi-view multi-class anomaly detection validate the
effectiveness of our approach, achieving state-of-the-art performance of
91.0/88.6/82.1 and 99.1/43.9/48.2/95.2 for image-level and the pixel-level,
respectively.

</details>

### [159] [CMD: Constraining Multimodal Distribution for Domain Adaptation in Stereo Matching](https://arxiv.org/abs/2504.21302)
*Zhelun Shen,Zhuo Li,Chenming Wu,Zhibo Rao,Lina Liu,Yuchao Dai,Liangjun Zhang*

Main category: cs.CV

TLDR: 本文提出了一种名为CMD的新方法，通过约束多模态分布来解决无监督域适应中立体匹配的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 在无监督域适应场景中，传统的soft argmin和平滑L1损失可能导致多模态视差分布，从而降低泛化性能。

Method: 引入不确定性正则化最小化和各向异性soft argmin，以鼓励网络在目标域中生成单模态视差分布。

Result: 实验表明，CMD方法在多个代表性立体匹配网络中显著提升了泛化性能。

Conclusion: CMD方法有效解决了无监督域适应中的多模态分布问题，提升了预测准确性。

Abstract: Recently, learning-based stereo matching methods have achieved great
improvement in public benchmarks, where soft argmin and smooth L1 loss play a
core contribution to their success. However, in unsupervised domain adaptation
scenarios, we observe that these two operations often yield multimodal
disparity probability distributions in target domains, resulting in degraded
generalization. In this paper, we propose a novel approach, Constrain
Multi-modal Distribution (CMD), to address this issue. Specifically, we
introduce \textit{uncertainty-regularized minimization} and \textit{anisotropic
soft argmin} to encourage the network to produce predominantly unimodal
disparity distributions in the target domain, thereby improving prediction
accuracy. Experimentally, we apply the proposed method to multiple
representative stereo-matching networks and conduct domain adaptation from
synthetic data to unlabeled real-world scenes. Results consistently demonstrate
improved generalization in both top-performing and domain-adaptable
stereo-matching models. The code for CMD will be available at:
\href{https://github.com/gallenszl/CMD}{https://github.com/gallenszl/CMD}.

</details>

### [160] [The Dual Power of Interpretable Token Embeddings: Jailbreaking Attacks and Defenses for Diffusion Model Unlearning](https://arxiv.org/abs/2504.21307)
*Siyi Chen,Yimeng Zhang,Sijia Liu,Qing Qu*

Main category: cs.CV

TLDR: 论文提出了一种可解释的攻击方法，通过正交攻击令牌嵌入揭示未学习模型中仍保留有害概念的原因，并设计了一种防御方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型可能记忆并生成有害内容，现有微调方法无法完全消除这些概念，且缺乏解释性。

Method: 提出学习正交可解释攻击令牌嵌入的方法，分解为人类可理解的文本元素，揭示未学习模型中隐含的有害概念。

Result: 攻击令牌嵌入具有鲁棒性和可迁移性，实验证明攻击和防御策略均有效。

Conclusion: 该方法为理解未学习模型保留有害概念提供了新视角，并提出了有效的防御策略。

Abstract: Despite the remarkable generalization capabilities of diffusion models,
recent studies have shown that these models can memorize and generate harmful
content when prompted with specific text instructions. Although fine-tuning
approaches have been developed to mitigate this issue by unlearning harmful
concepts, these methods can be easily circumvented through jailbreaking
attacks. This indicates that the harmful concept has not been fully erased from
the model. However, existing attack methods, while effective, lack
interpretability regarding why unlearned models still retain the concept,
thereby hindering the development of defense strategies. In this work, we
address these limitations by proposing an attack method that learns an
orthogonal set of interpretable attack token embeddings. The attack token
embeddings can be decomposed into human-interpretable textual elements,
revealing that unlearned models still retain the target concept through
implicit textual components. Furthermore, these attack token embeddings are
robust and transferable across text prompts, initial noises, and unlearned
models. Finally, leveraging this diverse set of embeddings, we design a defense
method applicable to both our proposed attack and existing attack methods.
Experimental results demonstrate the effectiveness of both our attack and
defense strategies.

</details>

### [161] [AGHI-QA: A Subjective-Aligned Dataset and Metric for AI-Generated Human Images](https://arxiv.org/abs/2504.21308)
*Yunhao Li,Sijing Wu,Wei Sun,Zhichao Zhang,Yucheng Zhu,Zicheng Zhang,Huiyu Duan,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TLDR: 论文提出了AGHI-QA基准和AGHI-Assessor方法，用于细粒度评估AI生成人类图像的质量，解决了现有IQA方法在复杂结构上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有图像质量评估方法无法对AI生成的人类图像进行细粒度评估，尤其是解剖和纹理失真问题。

Method: 构建了包含4000张图像的AGHI-QA数据集，并提出结合大模态模型和领域特征的AGHI-Assessor评估方法。

Result: AGHI-Assessor在多维质量评估和结构失真检测上表现优异，显著优于现有方法。

Conclusion: AGHI-QA和AGHI-Assessor为AI生成人类图像的质量评估提供了有效工具，填补了研究空白。

Abstract: The rapid development of text-to-image (T2I) generation approaches has
attracted extensive interest in evaluating the quality of generated images,
leading to the development of various quality assessment methods for
general-purpose T2I outputs. However, existing image quality assessment (IQA)
methods are limited to providing global quality scores, failing to deliver
fine-grained perceptual evaluations for structurally complex subjects like
humans, which is a critical challenge considering the frequent anatomical and
textural distortions in AI-generated human images (AGHIs). To address this gap,
we introduce AGHI-QA, the first large-scale benchmark specifically designed for
quality assessment of AGHIs. The dataset comprises 4,000 images generated from
400 carefully crafted text prompts using 10 state of-the-art T2I models. We
conduct a systematic subjective study to collect multidimensional annotations,
including perceptual quality scores, text-image correspondence scores, visible
and distorted body part labels. Based on AGHI-QA, we evaluate the strengths and
weaknesses of current T2I methods in generating human images from multiple
dimensions. Furthermore, we propose AGHI-Assessor, a novel quality metric that
integrates the large multimodal model (LMM) with domain-specific human features
for precise quality prediction and identification of visible and distorted body
parts in AGHIs. Extensive experimental results demonstrate that AGHI-Assessor
showcases state-of-the-art performance, significantly outperforming existing
IQA methods in multidimensional quality assessment and surpassing leading LMMs
in detecting structural distortions in AGHIs.

</details>

### [162] [An Evaluation of a Visual Question Answering Strategy for Zero-shot Facial Expression Recognition in Still Images](https://arxiv.org/abs/2504.21309)
*Modesto Castrillón-Santana,Oliverio J Santana,David Freire-Obregón,Daniel Hernández-Sosa,Javier Lorenzo-Navarro*

Main category: cs.CV

TLDR: 论文探讨了结合视觉语言模型（VLM）提升零样本面部表情识别（FER）性能的方法，并在多个基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在零样本FER场景中表现不佳，需要探索新方法提升泛化能力。

Method: 采用视觉问答策略，评估多种本地执行的VLM，并与现有FER模型对比。

Result: 部分VLM在零样本FER中表现优异，表明其潜力。

Conclusion: 需进一步探索VLM以提升FER的泛化能力。

Abstract: Facial expression recognition (FER) is a key research area in computer vision
and human-computer interaction. Despite recent advances in deep learning,
challenges persist, especially in generalizing to new scenarios. In fact,
zero-shot FER significantly reduces the performance of state-of-the-art FER
models. To address this problem, the community has recently started to explore
the integration of knowledge from Large Language Models for visual tasks. In
this work, we evaluate a broad collection of locally executed Visual Language
Models (VLMs), avoiding the lack of task-specific knowledge by adopting a
Visual Question Answering strategy. We compare the proposed pipeline with
state-of-the-art FER models, both integrating and excluding VLMs, evaluating
well-known FER benchmarks: AffectNet, FERPlus, and RAF-DB. The results show
excellent performance for some VLMs in zero-shot FER scenarios, indicating the
need for further exploration to improve FER generalization.

</details>

### [163] [Text-Conditioned Diffusion Model for High-Fidelity Korean Font Generation](https://arxiv.org/abs/2504.21325)
*Abdul Sami,Avinash Kumar,Irfanullah Memon,Youngwon Jo,Muhammad Rizwan,Jaeyoung Choi*

Main category: cs.CV

TLDR: 提出了一种基于扩散模型的自动字体生成方法，能够仅用单一样本生成高质量、多样化的韩文字体，解决了传统方法的不稳定性和细节捕捉不足问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统自动字体生成方法（如GAN和VAE）在训练不稳定、模式崩溃及细节捕捉上的不足，特别是针对复杂语言（如韩文）的手写和印刷字体生成。

Method: 采用扩散模型逐步细化噪声图像，结合文本编码器处理语音表示，并使用预训练的风格编码器和感知损失提升生成质量。

Result: 在2000多个韩文字符上的实验表明，该方法生成的字体准确且细节丰富，优于基准方法。

Conclusion: 该方法为生成真实韩文字体提供了可靠工具，适用于多种风格。

Abstract: Automatic font generation (AFG) is the process of creating a new font using
only a few examples of the style images. Generating fonts for complex languages
like Korean and Chinese, particularly in handwritten styles, presents
significant challenges. Traditional AFGs, like Generative adversarial networks
(GANs) and Variational Auto-Encoders (VAEs), are usually unstable during
training and often face mode collapse problems. They also struggle to capture
fine details within font images. To address these problems, we present a
diffusion-based AFG method which generates high-quality, diverse Korean font
images using only a single reference image, focusing on handwritten and printed
styles. Our approach refines noisy images incrementally, ensuring stable
training and visually appealing results. A key innovation is our text encoder,
which processes phonetic representations to generate accurate and contextually
correct characters, even for unseen characters. We used a pre-trained style
encoder from DG FONT to effectively and accurately encode the style images. To
further enhance the generation quality, we used perceptual loss that guides the
model to focus on the global style of generated images. Experimental results on
over 2000 Korean characters demonstrate that our model consistently generates
accurate and detailed font images and outperforms benchmark methods, making it
a reliable tool for generating authentic Korean fonts across different styles.

</details>

### [164] [Simple Visual Artifact Detection in Sora-Generated Videos](https://arxiv.org/abs/2504.21334)
*Misora Sugiyama,Hirokatsu Kataoka*

Main category: cs.CV

TLDR: 研究分析了OpenAI的Sora视频生成模型中常见的视觉伪影，提出了一种多标签分类框架，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成模型的发展，理解其局限性并确保安全部署变得至关重要。

Method: 使用多标签分类框架（四种伪影类型）和多种2D CNN架构（如ResNet-50）对300帧手动标注数据进行分析。

Result: ResNet-50模型表现最佳，平均多标签分类准确率达94.14%。

Conclusion: 该研究为视频质量评估、伪影分析和视觉风险识别提供了支持。

Abstract: The December 2024 release of OpenAI's Sora, a powerful video generation model
driven by natural language prompts, highlights a growing convergence between
large language models (LLMs) and video synthesis. As these multimodal systems
evolve into video-enabled LLMs (VidLLMs), capable of interpreting, generating,
and interacting with visual content, understanding their limitations and
ensuring their safe deployment becomes essential. This study investigates
visual artifacts frequently found and reported in Sora-generated videos, which
can compromise quality, mislead viewers, or propagate disinformation. We
propose a multi-label classification framework targeting four common artifact
label types: label 1: boundary / edge defects, label 2: texture / noise issues,
label 3: movement / joint anomalies, and label 4: object mismatches /
disappearances. Using a dataset of 300 manually annotated frames extracted from
15 Sora-generated videos, we trained multiple 2D CNN architectures (ResNet-50,
EfficientNet-B3 / B4, ViT-Base). The best-performing model trained by ResNet-50
achieved an average multi-label classification accuracy of 94.14%. This work
supports the broader development of VidLLMs by contributing to (1) the creation
of datasets for video quality evaluation, (2) interpretable artifact-based
analysis beyond language metrics, and (3) the identification of visual risks
relevant to factuality and safety.

</details>

### [165] [UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation](https://arxiv.org/abs/2504.21336)
*Linshan Wu,Yuxiang Nie,Sunan He,Jiaxin Zhuang,Hao Chen*

Main category: cs.CV

TLDR: UniBiomed是一种基于多模态大语言模型（MLLM）和Segment Anything Model（SAM）的新型通用基础模型，用于生物医学图像的全面解释，统一了临床文本生成和目标分割，显著提升了诊断效率。


<details>
  <summary>Details</summary>
Motivation: 传统AI方法在生物医学图像分析中依赖分离的训练模型（如LLM和分割模型），导致部署不灵活且无法利用整体生物医学信息。UniBiomed旨在解决这一问题。

Method: UniBiomed通过整合MLLM和SAM，统一了临床文本生成和生物医学目标分割，并基于一个包含2700万张图像、标注和文本描述的大规模数据集进行训练。

Result: 在84个内外数据集上的验证表明，UniBiomed在分割、疾病识别、区域感知诊断、视觉问答和报告生成等任务中达到最先进性能。

Conclusion: UniBiomed为生物医学AI带来突破，实现了自动化、端到端的生物医学图像解释，显著提升了诊断效率。

Abstract: Multi-modal interpretation of biomedical images opens up novel opportunities
in biomedical image analysis. Conventional AI approaches typically rely on
disjointed training, i.e., Large Language Models (LLMs) for clinical text
generation and segmentation models for target extraction, which results in
inflexible real-world deployment and a failure to leverage holistic biomedical
information. To this end, we introduce UniBiomed, the first universal
foundation model for grounded biomedical image interpretation. UniBiomed is
based on a novel integration of Multi-modal Large Language Model (MLLM) and
Segment Anything Model (SAM), which effectively unifies the generation of
clinical texts and the segmentation of corresponding biomedical objects for
grounded interpretation. In this way, UniBiomed is capable of tackling a wide
range of biomedical tasks across ten diverse biomedical imaging modalities. To
develop UniBiomed, we curate a large-scale dataset comprising over 27 million
triplets of images, annotations, and text descriptions across ten imaging
modalities. Extensive validation on 84 internal and external datasets
demonstrated that UniBiomed achieves state-of-the-art performance in
segmentation, disease recognition, region-aware diagnosis, visual question
answering, and report generation. Moreover, unlike previous models that rely on
clinical experts to pre-diagnose images and manually craft precise textual or
visual prompts, UniBiomed can provide automated and end-to-end grounded
interpretation for biomedical image analysis. This represents a novel paradigm
shift in clinical workflows, which will significantly improve diagnostic
efficiency. In summary, UniBiomed represents a novel breakthrough in biomedical
AI, unlocking powerful grounded interpretation capabilities for more accurate
and efficient biomedical image analysis.

</details>

### [166] [Towards Improved Cervical Cancer Screening: Vision Transformer-Based Classification and Interpretability](https://arxiv.org/abs/2504.21340)
*Khoa Tuan Nguyen,Ho-min Park,Gaeun Oh,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TLDR: 提出了一种基于EVA-02 transformer模型的宫颈细胞图像分类新方法，通过四步流程优化模型性能，最终F1分数达到0.85227。


<details>
  <summary>Details</summary>
Motivation: 改进宫颈癌筛查中的细胞图像分类性能，提供更准确的诊断工具。

Method: 四步流程：微调EVA-02、特征提取、多模型特征选择、训练新神经网络（可选损失加权）。

Result: 最佳模型F1分数0.85227，优于基线EVA-02（0.84878），并通过Kernel SHAP分析提供可解释性。

Conclusion: 该方法在宫颈细胞分类中表现优异，同时提供了模型决策的可解释性。

Abstract: We propose a novel approach to cervical cell image classification for
cervical cancer screening using the EVA-02 transformer model. We developed a
four-step pipeline: fine-tuning EVA-02, feature extraction, selecting important
features through multiple machine learning models, and training a new
artificial neural network with optional loss weighting for improved
generalization. With this design, our best model achieved an F1-score of
0.85227, outperforming the baseline EVA-02 model (0.84878). We also utilized
Kernel SHAP analysis and identified key features correlating with cell
morphology and staining characteristics, providing interpretable insights into
the decision-making process of the fine-tuned model. Our code is available at
https://github.com/Khoa-NT/isbi2025_ps3c.

</details>

### [167] [Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early Lung Cancer Detection](https://arxiv.org/abs/2504.21344)
*Luoting Zhuang,Seyed Mohammad Hossein Tabatabaei,Ramin Salehi-Rad,Linh M. Tran,Denise R. Aberle,Ashley E. Prosper,William Hsu*

Main category: cs.CV

TLDR: 该研究提出了一种结合语义特征和深度学习的方法，用于预测肺癌，并在外部数据集上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型依赖手动标注、可解释性差且对成像变化敏感，限制了其临床应用。

Method: 利用来自多个数据集的CT扫描和语义特征，采用参数高效微调的CLIP模型对齐图像和语义特征。

Result: 模型在一年肺癌诊断中AUROC为0.90，AUPRC为0.78，优于基线模型，并能解释预测结果。

Conclusion: 该方法能准确分类肺结节，提供可解释的输出，并具有跨临床环境的泛化能力。

Abstract: Objective: A number of machine learning models have utilized semantic
features, deep features, or both to assess lung nodule malignancy. However,
their reliance on manual annotation during inference, limited interpretability,
and sensitivity to imaging variations hinder their application in real-world
clinical settings. Thus, this research aims to integrate semantic features
derived from radiologists' assessments of nodules, allowing the model to learn
clinically relevant, robust, and explainable features for predicting lung
cancer. Methods: We obtained 938 low-dose CT scans from the National Lung
Screening Trial with 1,246 nodules and semantic features. The Lung Image
Database Consortium dataset contains 1,018 CT scans, with 2,625 lesions
annotated for nodule characteristics. Three external datasets were obtained
from UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We
finetuned a pretrained Contrastive Language-Image Pretraining model with a
parameter-efficient fine-tuning approach to align imaging and semantic features
and predict the one-year lung cancer diagnosis. Results: We evaluated the
performance of the one-year diagnosis of lung cancer with AUROC and AUPRC and
compared it to three state-of-the-art models. Our model demonstrated an AUROC
of 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on
external datasets. Using CLIP, we also obtained predictions on semantic
features, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and
pleural attachment (0.84), that can be used to explain model predictions.
Conclusion: Our approach accurately classifies lung nodules as benign or
malignant, providing explainable outputs, aiding clinicians in comprehending
the underlying meaning of model predictions. This approach also prevents the
model from learning shortcuts and generalizes across clinical settings.

</details>

### [168] [Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing](https://arxiv.org/abs/2504.21356)
*Hong Zhang,Zhongjie Duan,Xingjun Wang,Yingda Chen,Yuze Zhao,Yu Zhang*

Main category: cs.CV

TLDR: Nexus-Gen是一个统一的多模态大语言模型，通过结合LLM的语言推理能力和扩散模型的图像合成能力，解决了现有开源模型在性能上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有开源统一模型在多模态理解和生成任务中表现不如领域专用架构，因此需要开发一个性能更强的统一模型。

Method: 采用双阶段对齐训练：1）LLM学习预测图像嵌入；2）视觉解码器从嵌入重建图像。引入预填充自回归策略以避免误差累积。

Result: Nexus-Gen能够综合处理图像理解、生成和编辑任务，性能优于现有开源模型。

Conclusion: Nexus-Gen通过双阶段训练和预填充策略，成功整合了多模态能力，为领域发展提供了开源资源。

Abstract: Unified multimodal large language models (MLLMs) aim to integrate multimodal
understanding and generation abilities through a single framework. Despite
their versatility, existing open-source unified models exhibit performance gaps
against domain-specific architectures. To bridge this gap, we present
Nexus-Gen, a unified model that synergizes the language reasoning capabilities
of LLMs with the image synthesis power of diffusion models. To align the
embedding space of the LLM and diffusion model, we conduct a dual-phase
alignment training process. (1) The autoregressive LLM learns to predict image
embeddings conditioned on multimodal inputs, while (2) the vision decoder is
trained to reconstruct high-fidelity images from these embeddings. During
training the LLM, we identified a critical discrepancy between the
autoregressive paradigm's training and inference phases, where error
accumulation in continuous embedding space severely degrades generation
quality. To avoid this issue, we introduce a prefilled autoregression strategy
that prefills input sequence with position-embedded special tokens instead of
continuous embeddings. Through dual-phase training, Nexus-Gen has developed the
integrated capability to comprehensively address the image understanding,
generation and editing tasks. All models, datasets, and codes are published at
https://github.com/modelscope/Nexus-Gen.git to facilitate further advancements
across the field.

</details>

### [169] [Revisiting Diffusion Autoencoder Training for Image Reconstruction Quality](https://arxiv.org/abs/2504.21368)
*Pramook Khungurn,Sukit Seripanitkarn,Phonphrm Thawatdamrongkit,Supasorn Suwajanakorn*

Main category: cs.CV

TLDR: 提出了一种新的扩散自编码器（DAE）训练方法，通过分阶段训练提高图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统DAE使用线性噪声计划，导致图像模糊且细节不足。通过利用潜在编码中的结构信息，可以优化细节重建。

Method: 分两阶段训练：第一阶段强制编码器和解码器在高噪声水平下学习结构信息；第二阶段调整噪声计划，专注于低噪声区域的细节学习。

Result: 生成的图像在高层结构和低层细节上均表现优异，同时保留了潜在编码的有用特性。

Conclusion: 新方法显著提升了DAE的图像重建质量，同时保持了潜在编码的功能性。

Abstract: Diffusion autoencoders (DAEs) are typically formulated as a noise prediction
model and trained with a linear-$\beta$ noise schedule that spends much of its
sampling steps at high noise levels. Because high noise levels are associated
with recovering large-scale image structures and low noise levels with
recovering details, this configuration can result in low-quality and blurry
images. However, it should be possible to improve details while spending fewer
steps recovering structures because the latent code should already contain
structural information. Based on this insight, we propose a new DAE training
method that improves the quality of reconstructed images. We divide training
into two phases. In the first phase, the DAE is trained as a vanilla
autoencoder by always setting the noise level to the highest, forcing the
encoder and decoder to populate the latent code with structural information. In
the second phase, we incorporate a noise schedule that spends more time in the
low-noise region, allowing the DAE to learn how to perfect the details. Our
method results in images that have accurate high-level structures and low-level
details while still preserving useful properties of the latent codes.

</details>

### [170] [IDDM: Bridging Synthetic-to-Real Domain Gap from Physics-Guided Diffusion for Real-world Image Dehazing](https://arxiv.org/abs/2504.21385)
*Shijun Zhou,Yajing Liu,Chunhui Hao,Zhiyuan Liu,Jiandong Tian*

Main category: cs.CV

TLDR: 论文提出了一种基于扩散模型的图像去雾方法（IDDM），通过结合大气散射模型和噪声扩散，解决了合成数据训练的去雾算法在真实场景中泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于合成数据训练的去雾算法在真实场景中表现不佳，存在领域差距问题。

Method: 提出IDDM方法，将大气散射模型融入噪声扩散过程，设计专门的训练策略，通过扩散模型和物理模型结合来学习清晰图像的分布。

Result: IDDM在合成数据训练下表现出对真实场景的泛化能力，能够有效恢复真实世界的雾化图像。

Conclusion: IDDM通过结合物理模型和扩散模型，成功解决了领域差距问题，并在实验中表现出优于现有方法的性能。

Abstract: Due to the domain gap between real-world and synthetic hazy images, current
data-driven dehazing algorithms trained on synthetic datasets perform well on
synthetic data but struggle to generalize to real-world scenarios. To address
this challenge, we propose \textbf{I}mage \textbf{D}ehazing \textbf{D}iffusion
\textbf{M}odels (IDDM), a novel diffusion process that incorporates the
atmospheric scattering model into noise diffusion. IDDM aims to use the gradual
haze formation process to help the denoising Unet robustly learn the
distribution of clear images from the conditional input hazy images. We design
a specialized training strategy centered around IDDM. Diffusion models are
leveraged to bridge the domain gap from synthetic to real-world, while the
atmospheric scattering model provides physical guidance for haze formation.
During the forward process, IDDM simultaneously introduces haze and noise into
clear images, and then robustly separates them during the sampling process. By
training with physics-guided information, IDDM shows the ability of domain
generalization, and effectively restores the real-world hazy images despite
being trained on synthetic datasets. Extensive experiments demonstrate the
effectiveness of our method through both quantitative and qualitative
comparisons with state-of-the-art approaches.

</details>

### [171] [Comparison of Different Deep Neural Network Models in the Cultural Heritage Domain](https://arxiv.org/abs/2504.21387)
*Teodor Boyadzhiev,Gabriele Lagani,Luca Ciampi,Giuseppe Amato,Krassimira Ivanova*

Main category: cs.CV

TLDR: 比较卷积神经网络和Transformer架构在文化遗产任务中的知识迁移能力，发现DenseNet在效率与计算性方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉与深度学习的结合对文化遗产保护及提升游客体验至关重要，需比较不同深度学习架构的性能。

Method: 测试VGG、ResNet、DenseNet、Visual Transformer、Swin Transformer和PoolFormer等架构在ImageNet到文化遗产任务的迁移能力。

Result: DenseNet在效率与计算性方面表现最佳。

Conclusion: DenseNet是文化遗产任务中知识迁移的最佳选择。

Abstract: The integration of computer vision and deep learning is an essential part of
documenting and preserving cultural heritage, as well as improving visitor
experiences. In recent years, two deep learning paradigms have been established
in the field of computer vision: convolutional neural networks and transformer
architectures. The present study aims to make a comparative analysis of some
representatives of these two techniques of their ability to transfer knowledge
from generic dataset, such as ImageNet, to cultural heritage specific tasks.
The results of testing examples of the architectures VGG, ResNet, DenseNet,
Visual Transformer, Swin Transformer, and PoolFormer, showed that DenseNet is
the best in terms of efficiency-computability ratio.

</details>

### [172] [Static or Dynamic: Towards Query-Adaptive Token Selection for Video Question Answering](https://arxiv.org/abs/2504.21403)
*Yumeng Shi,Quanyu Long,Wenya Wang*

Main category: cs.CV

TLDR: 提出了一种名为EXPLORE-THEN-SELECT的自适应令牌选择策略，用于优化视频问答中的静态和动态信息分配，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决视频问答中因长视频生成大量令牌导致的内存效率和性能问题，同时避免现有方法忽视不同查询对静态和动态信息需求差异的不足。

Method: 提出EXPLORE-THEN-SELECT策略，先探索静态帧和动态帧的令牌分配，再通过查询感知的注意力指标选择最优组合，无需模型更新。

Result: 在多个视频问答基准测试中，性能显著提升（最高达5.8%）。

Conclusion: 该框架为即插即用方案，可无缝集成到多种视频-语言模型中，有效优化令牌使用并提升性能。

Abstract: Video question answering benefits from the rich information available in
videos, enabling a wide range of applications. However, the large volume of
tokens generated from longer videos presents significant challenges to memory
efficiency and model performance. To alleviate this issue, existing works
propose to compress video inputs, but usually overlooking the varying
importance of static and dynamic information across different queries, leading
to inefficient token usage within limited budgets. To tackle this, we propose a
novel token selection strategy, EXPLORE-THEN-SELECT, that adaptively adjust
static and dynamic information needed based on question requirements. Our
framework first explores different token allocations between static frames,
which preserve spatial details, and dynamic frames, which capture temporal
changes. Next, it employs a query-aware attention-based metric to select the
optimal token combination without model updates. Our proposed framework is
plug-and-play that can be seamlessly integrated within diverse video-language
models. Extensive experiments show that our method achieves significant
performance improvements (up to 5.8%) among various video question answering
benchmarks.

</details>

### [173] [Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining](https://arxiv.org/abs/2504.21414)
*Qi Fan,Kaiqi Liu,Nian Liu,Hisham Cholakkal,Rao Muhammad Anwer,Wenbin Li,Yang Gao*

Main category: cs.CV

TLDR: 提出了一种无需重新训练的方法（ISA），通过自适应调整模型结构来解决跨域少样本分割问题。


<details>
  <summary>Details</summary>
Motivation: 跨域少样本分割（CD-FSS）因目标域多样性和支持数据有限而具有挑战性，现有方法需重新训练模型，成本高。

Method: 通过结构Fisher评分自适应识别域特定模型结构，并分层训练支持样本，逐步适应新域。

Result: 在多个CD-FSS基准测试中表现优异，验证了方法的有效性。

Conclusion: ISA方法无需重新训练即可灵活适应新域，解决了现有方法的局限性。

Abstract: Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel
classes in new domains, which is often challenging due to the diverse
characteristics of target domains and the limited availability of support data.
Most CD-FSS methods redesign and retrain in-domain FSS models using various
domain-generalization techniques, which are effective but costly to train. To
address these issues, we propose adapting informative model structures of the
well-trained FSS model for target domains by learning domain characteristics
from few-shot labeled support samples during inference, thereby eliminating the
need for retraining. Specifically, we first adaptively identify domain-specific
model structures by measuring parameter importance using a novel structure
Fisher score in a data-dependent manner. Then, we progressively train the
selected informative model structures with hierarchically constructed training
samples, progressing from fewer to more support shots. The resulting
Informative Structure Adaptation (ISA) method effectively addresses domain
shifts and equips existing well-trained in-domain FSS models with flexible
adaptation capabilities for new domains, eliminating the need to redesign or
retrain CD-FSS models on base data. Extensive experiments validate the
effectiveness of our method, demonstrating superior performance across multiple
CD-FSS benchmarks.

</details>

### [174] [Diff-Prompt: Diffusion-Driven Prompt Generator with Mask Supervision](https://arxiv.org/abs/2504.21423)
*Weicai Yan,Wang Lin,Zirun Guo,Ye Wang,Fangming Feng,Xiaoda Yang,Zehan Wang,Tao Jin*

Main category: cs.CV

TLDR: Diff-Prompt利用扩散模型生成丰富且细粒度的提示信息，显著提升了复杂下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过损失反向传播优化提示生成参数，限制了提示表示的丰富性和特异性，难以应对复杂任务。

Method: 1. 训练Mask-VAE压缩掩码到潜空间；2. 用改进的DiT在潜空间训练提示生成器；3. 将生成器与预训练模型在语义空间对齐，并用于微调。

Result: 在复杂像素级任务上，Diff-Prompt在R@1和R@5上分别提升8.87和14.05，优于其他方法。

Conclusion: Diff-Prompt验证了生成模型在提示生成中的潜力，显著提升了性能。

Abstract: Prompt learning has demonstrated promising results in fine-tuning pre-trained
multimodal models. However, the performance improvement is limited when applied
to more complex and fine-grained tasks. The reason is that most existing
methods directly optimize the parameters involved in the prompt generation
process through loss backpropagation, which constrains the richness and
specificity of the prompt representations. In this paper, we propose
Diffusion-Driven Prompt Generator (Diff-Prompt), aiming to use the diffusion
model to generate rich and fine-grained prompt information for complex
downstream tasks. Specifically, our approach consists of three stages. In the
first stage, we train a Mask-VAE to compress the masks into latent space. In
the second stage, we leverage an improved Diffusion Transformer (DiT) to train
a prompt generator in the latent space, using the masks for supervision. In the
third stage, we align the denoising process of the prompt generator with the
pre-trained model in the semantic space, and use the generated prompts to
fine-tune the model. We conduct experiments on a complex pixel-level downstream
task, referring expression comprehension, and compare our method with various
parameter-efficient fine-tuning approaches. Diff-Prompt achieves a maximum
improvement of 8.87 in R@1 and 14.05 in R@5 compared to the foundation model
and also outperforms other state-of-the-art methods across multiple metrics.
The experimental results validate the effectiveness of our approach and
highlight the potential of using generative models for prompt generation. Code
is available at https://github.com/Kelvin-ywc/diff-prompt.

</details>

### [175] [SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding](https://arxiv.org/abs/2504.21435)
*Chenkai Zhang,Yiming Lei,Zeming Liu,Haitao Leng,ShaoGuo Liu,Tingting Gao,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TLDR: 论文提出了SeriesBench，一个专注于评估多模态大语言模型（MLLMs）对叙事驱动系列视频理解能力的基准，并提出了PC-DCoT框架以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估独立视频的视觉元素，而现实中的视频多为复杂连续的叙事系列，因此需要新的评估方法和模型能力提升。

Method: 通过精选多样化的剧集系列，结合长跨度叙事标注方法和全信息转换技术，构建SeriesBench基准，并提出PC-DCoT框架以增强模型对情节结构和角色关系的分析能力。

Result: 实验表明，现有MLLMs在理解叙事系列视频时仍面临挑战，而PC-DCoT能显著提升其性能。

Conclusion: SeriesBench和PC-DCoT强调了提升模型理解叙事系列视频能力的必要性，为MLLMs的未来发展提供了指导。

Abstract: With the rapid development of Multi-modal Large Language Models (MLLMs), an
increasing number of benchmarks have been established to evaluate the video
understanding capabilities of these models. However, these benchmarks focus on
\textbf{standalone} videos and mainly assess ``visual elements'' like human
actions and object states. In reality, contemporary videos often encompass
complex and continuous narratives, typically presented as a \textbf{series}. To
address this challenge, we propose \textbf{SeriesBench}, a benchmark consisting
of 105 carefully curated narrative-driven series, covering 28 specialized tasks
that require deep narrative understanding. Specifically, we first select a
diverse set of drama series spanning various genres. Then, we introduce a novel
long-span narrative annotation method, combined with a full-information
transformation approach to convert manual annotations into diverse task
formats. To further enhance model capacity for detailed analysis of plot
structures and character relationships within series, we propose a novel
narrative reasoning framework, \textbf{PC-DCoT}. Extensive results on
\textbf{SeriesBench} indicate that existing MLLMs still face significant
challenges in understanding narrative-driven series, while \textbf{PC-DCoT}
enables these MLLMs to achieve performance improvements. Overall, our
\textbf{SeriesBench} and \textbf{PC-DCoT} highlight the critical necessity of
advancing model capabilities to understand narrative-driven series, guiding the
future development of MLLMs. SeriesBench is publicly available at
https://github.com/zackhxn/SeriesBench-CVPR2025.

</details>

### [176] [Rethinking Visual Layer Selection in Multimodal LLMs](https://arxiv.org/abs/2504.21447)
*Haoran Chen,Junyan Lin,Xinhao Chen,Yue Fan,Xin Jin,Hui Su,Jianfeng Dong,Jinlan Fu,Xiaoyu Shen*

Main category: cs.CV

TLDR: 提出了一种基于层间表示相似性的方法，将CLIP-ViT的视觉层分为浅层、中层和深层，并研究了它们对多模态大语言模型（MLLM）性能的影响。实验表明，不同任务需要不同层次的视觉特征，而轻量级融合方法能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM通常基于经验选择视觉特征，缺乏系统分析。本文旨在通过层间相似性分析，为视觉层选择提供原则性指导。

Method: 提出Layer-wise Representation Similarity方法，将CLIP-ViT层分为浅、中、深三类，并在不同规模的LLaVA模型上进行实验。

Result: 实验表明：(1)深层对OCR任务至关重要；(2)浅层和中层在计数、定位等推理任务中表现更好；(3)轻量级融合方法在多数数据集上优于单一层选择。

Conclusion: 本文首次系统研究了MLLM中视觉层选择问题，为未来视觉表示学习提供了基础。

Abstract: Multimodal large language models (MLLMs) have achieved impressive performance
across a wide range of tasks, typically using CLIP-ViT as their visual encoder
due to its strong text-image alignment capabilities. While prior studies
suggest that different CLIP-ViT layers capture different types of information,
with shallower layers focusing on fine visual details and deeper layers
aligning more closely with textual semantics, most MLLMs still select visual
features based on empirical heuristics rather than systematic analysis. In this
work, we propose a Layer-wise Representation Similarity approach to group
CLIP-ViT layers with similar behaviors into {shallow, middle, and deep}
categories and assess their impact on MLLM performance. Building on this
foundation, we revisit the visual layer selection problem in MLLMs at scale,
training LLaVA-style models ranging from 1.4B to 7B parameters. Through
extensive experiments across 10 datasets and 4 tasks, we find that: (1) deep
layers are essential for OCR tasks; (2) shallow and middle layers substantially
outperform deep layers on reasoning tasks involving counting, positioning, and
object localization; (3) a lightweight fusion of features across shallow,
middle, and deep layers consistently outperforms specialized fusion baselines
and single-layer selections, achieving gains on 9 out of 10 datasets. Our work
offers the first principled study of visual layer selection in MLLMs, laying
the groundwork for deeper investigations into visual representation learning
for MLLMs.

</details>

### [177] [VR-FuseNet: A Fusion of Heterogeneous Fundus Data and Explainable Deep Network for Diabetic Retinopathy Classification](https://arxiv.org/abs/2504.21464)
*Shamim Rahim Refat,Ziyan Shirin Raha,Shuvashis Sarker,Faika Fairuj Preotee,MD. Musfikur Rahman,Tashreef Muhammad,Mohammad Shafiul Islam*

Main category: cs.CV

TLDR: 本文提出了一种名为VR-FuseNet的混合深度学习模型，用于自动化糖尿病视网膜病变检测，结合VGG19和ResNet50V2的优势，准确率达91.824%，并引入XAI技术提升临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是导致糖尿病患者失明的主要原因，现有方法存在数据集不平衡和泛化能力不足等问题，亟需高效准确的自动化检测方法。

Method: 提出VR-FuseNet混合模型，结合VGG19和ResNet50V2；采用SMOTE和CLAHE预处理技术；使用五个公开数据集构建混合数据集。

Result: 模型准确率达91.824%，优于单一架构，XAI技术提供了可解释的视觉解释。

Conclusion: VR-FuseNet在糖尿病视网膜病变分类任务中表现优异，结合XAI技术增强了临床实用性。

Abstract: Diabetic retinopathy is a severe eye condition caused by diabetes where the
retinal blood vessels get damaged and can lead to vision loss and blindness if
not treated. Early and accurate detection is key to intervention and stopping
the disease progressing. For addressing this disease properly, this paper
presents a comprehensive approach for automated diabetic retinopathy detection
by proposing a new hybrid deep learning model called VR-FuseNet. Diabetic
retinopathy is a major eye disease and leading cause of blindness especially
among diabetic patients so accurate and efficient automated detection methods
are required. To address the limitations of existing methods including dataset
imbalance, diversity and generalization issues this paper presents a hybrid
dataset created from five publicly available diabetic retinopathy datasets.
Essential preprocessing techniques such as SMOTE for class balancing and CLAHE
for image enhancement are applied systematically to the dataset to improve the
robustness and generalizability of the dataset. The proposed VR-FuseNet model
combines the strengths of two state-of-the-art convolutional neural networks,
VGG19 which captures fine-grained spatial features and ResNet50V2 which is
known for its deep hierarchical feature extraction. This fusion improves the
diagnostic performance and achieves an accuracy of 91.824%. The model
outperforms individual architectures on all performance metrics demonstrating
the effectiveness of hybrid feature extraction in Diabetic Retinopathy
classification tasks. To make the proposed model more clinically useful and
interpretable this paper incorporates multiple XAI techniques. These techniques
generate visual explanations that clearly indicate the retinal features
affecting the model's prediction such as microaneurysms, hemorrhages and
exudates so that clinicians can interpret and validate.

</details>

### [178] [Multiview Point Cloud Registration via Optimization in an Autoencoder Latent Space](https://arxiv.org/abs/2504.21467)
*Luc Vedrenne,Sylvain Faisan,Denis Fortun*

Main category: cs.CV

TLDR: POLAR是一种多视角点云刚性配准方法，通过潜在空间转换和优化策略，解决了现有方法在大视角、高退化和大初始角度下的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视角配准中难以处理大量视图、高退化和大初始角度的问题。

Method: 将配准问题转换到预训练自编码器的潜在空间，设计考虑退化的损失函数，并采用多起点优化策略。

Result: 在合成和真实数据上显著优于现有方法。

Conclusion: POLAR是一种高效、鲁棒的多视角点云配准方法，适用于复杂场景。

Abstract: Point cloud rigid registration is a fundamental problem in 3D computer
vision. In the multiview case, we aim to find a set of 6D poses to align a set
of objects. Methods based on pairwise registration rely on a subsequent
synchronization algorithm, which makes them poorly scalable with the number of
views. Generative approaches overcome this limitation, but are based on
Gaussian Mixture Models and use an Expectation-Maximization algorithm. Hence,
they are not well suited to handle large transformations. Moreover, most
existing methods cannot handle high levels of degradations. In this paper, we
introduce POLAR (POint cloud LAtent Registration), a multiview registration
method able to efficiently deal with a large number of views, while being
robust to a high level of degradations and large initial angles. To achieve
this, we transpose the registration problem into the latent space of a
pretrained autoencoder, design a loss taking degradations into account, and
develop an efficient multistart optimization strategy. Our proposed method
significantly outperforms state-of-the-art approaches on synthetic and real
data. POLAR is available at github.com/pypolar/polar or as a standalone package
which can be installed with pip install polaregistration.

</details>

### [179] [Quaternion Nuclear Norms Over Frobenius Norms Minimization for Robust Matrix Completion](https://arxiv.org/abs/2504.21468)
*Yu Guo,Guoqing Chen,Tieyong Zeng,Qiyu Jin,Michael Kwok-Po Ng*

Main category: cs.CV

TLDR: 提出了一种新的非凸近似方法QNOF，用于恢复四元数矩阵的秩，并扩展至鲁棒矩阵补全问题，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多维数据表示中，从不完整或噪声数据中恢复隐藏结构是一个普遍挑战，四元数矩阵为此提供了潜力框架。

Method: 引入QNOF作为四元数矩阵秩的非凸近似，利用四元数奇异值分解简化问题，并扩展至鲁棒矩阵补全，使用交替方向乘子法求解。

Result: 数值实验表明QNOF模型优于现有四元数方法。

Conclusion: QNOF是一种参数无关且尺度不变的有效方法，适用于四元数矩阵的秩恢复和鲁棒补全。

Abstract: Recovering hidden structures from incomplete or noisy data remains a
pervasive challenge across many fields, particularly where multi-dimensional
data representation is essential. Quaternion matrices, with their ability to
naturally model multi-dimensional data, offer a promising framework for this
problem. This paper introduces the quaternion nuclear norm over the Frobenius
norm (QNOF) as a novel nonconvex approximation for the rank of quaternion
matrices. QNOF is parameter-free and scale-invariant. Utilizing quaternion
singular value decomposition, we prove that solving the QNOF can be simplified
to solving the singular value $L_1/L_2$ problem. Additionally, we extend the
QNOF to robust quaternion matrix completion, employing the alternating
direction multiplier method to derive solutions that guarantee weak convergence
under mild conditions. Extensive numerical experiments validate the proposed
model's superiority, consistently outperforming state-of-the-art quaternion
methods.

</details>

### [180] [Robust Orthogonal NMF with Label Propagation for Image Clustering](https://arxiv.org/abs/2504.21472)
*Jingjing Liu,Nian Wu,Xianchao Xiu,Jianhua Zhang*

Main category: cs.CV

TLDR: 提出了一种名为RONMF的鲁棒正交非负矩阵分解方法，通过结合图拉普拉斯和标签传播作为正则化项，并引入非凸结构和正交约束，提高了对噪声的鲁棒性。实验表明其在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有NMF方法对噪声敏感且难以利用有限的监督信息，需要一种更鲁棒且能结合监督信息的方法。

Method: 提出RONMF框架，结合图拉普拉斯和标签传播作为正则化项，引入非凸结构测量重构误差，并施加正交约束。采用ADMM优化算法，子问题有闭式解。

Result: 在八个公开图像数据集上，RONMF在多种标准指标上优于现有NMF方法，表现出优异的鲁棒性。

Conclusion: RONMF通过结合监督信息和正交约束，显著提升了NMF在噪声环境下的性能，是一种高效且鲁棒的聚类方法。

Abstract: Non-negative matrix factorization (NMF) is a popular unsupervised learning
approach widely used in image clustering. However, in real-world clustering
scenarios, most existing NMF methods are highly sensitive to noise corruption
and are unable to effectively leverage limited supervised information. To
overcome these drawbacks, we propose a unified non-convex framework with label
propagation called robust orthogonal nonnegative matrix factorization (RONMF).
This method not only considers the graph Laplacian and label propagation as
regularization terms but also introduces a more effective non-convex structure
to measure the reconstruction error and imposes orthogonal constraints on the
basis matrix to reduce the noise corruption, thereby achieving higher
robustness. To solve RONMF, we develop an alternating direction method of
multipliers (ADMM)-based optimization algorithm. In particular, all subproblems
have closed-form solutions, which ensures its efficiency. Experimental
evaluations on eight public image datasets demonstrate that the proposed RONMF
outperforms state-of-the-art NMF methods across various standard metrics and
shows excellent robustness. The code will be available at
https://github.com/slinda-liu.

</details>

### [181] [GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers](https://arxiv.org/abs/2504.21476)
*Xinyu Li,Qi Yao,Yuanda Wang*

Main category: cs.CV

TLDR: GarmentDiffusion是一种新的生成模型，能够从多模态输入（文本、图像和不完整的缝制图案）生成厘米级精确的矢量3D缝制图案，效率比现有方法高100倍。


<details>
  <summary>Details</summary>
Motivation: 现有方法在缝制图案生成中依赖单一输入模态或效率低下，限制了多样化服装的生成。

Method: 通过将3D缝制图案参数编码为紧凑的边缘令牌表示，并使用扩散变换器同时去噪所有边缘令牌，实现了高效的生成。

Result: 在DressCodeData和GarmentCodeData上取得了最新的最优结果，生成速度比SewingGPT快100倍。

Conclusion: GarmentDiffusion在缝制图案生成中实现了高效、多模态输入和厘米级精确的生成能力。

Abstract: Garment sewing patterns are fundamental design elements that bridge the gap
between design concepts and practical manufacturing. The generative modeling of
sewing patterns is crucial for creating diversified garments. However, existing
approaches are limited either by reliance on a single input modality or by
suboptimal generation efficiency. In this work, we present
\textbf{\textit{GarmentDiffusion}}, a new generative model capable of producing
centimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text,
image, and incomplete sewing pattern). Our method efficiently encodes 3D sewing
pattern parameters into compact edge token representations, achieving a
sequence length that is $\textbf{10}\times$ shorter than that of the
autoregressive SewingGPT in DressCode. By employing a diffusion transformer, we
simultaneously denoise all edge tokens along the temporal axis, while
maintaining a constant number of denoising steps regardless of dataset-specific
edge and panel statistics. With all combination of designs of our model, the
sewing pattern generation speed is accelerated by $\textbf{100}\times$ compared
to SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well
as on the largest sewing pattern dataset, namely GarmentCodeData. The project
website is available at https://shenfu-research.github.io/Garment-Diffusion/.

</details>

### [182] [CAE-DFKD: Bridging the Transferability Gap in Data-Free Knowledge Distillation](https://arxiv.org/abs/2504.21478)
*Zherui Zhang,Changwei Wang,Rongtao Xu,Wenhao Xu,Shibiao Xu,Yu Zhang,Li Guo*

Main category: cs.CV

TLDR: 本文提出了一种名为CAE-DFKD的新方法，通过嵌入层面的改进解决了现有DFKD方法在模型泛化能力上的不足，并在效率和性能上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有DFKD方法主要关注图像识别性能，而忽略了学习表示的可迁移性，CAE-DFKD旨在解决这一问题。

Method: CAE-DFKD通过在嵌入层面改进生成器训练范式，提升了模型的泛化能力和效率。

Result: CAE-DFKD在图像识别任务中表现优异，并在下游任务中展示了显著的可迁移性。

Conclusion: CAE-DFKD在数据自由知识蒸馏领域具有显著优势，尤其在模型泛化和效率方面。

Abstract: Data-Free Knowledge Distillation (DFKD) enables the knowledge transfer from
the given pre-trained teacher network to the target student model without
access to the real training data. Existing DFKD methods focus primarily on
improving image recognition performance on associated datasets, often
neglecting the crucial aspect of the transferability of learned
representations. In this paper, we propose Category-Aware Embedding Data-Free
Knowledge Distillation (CAE-DFKD), which addresses at the embedding level the
limitations of previous rely on image-level methods to improve model
generalization but fail when directly applied to DFKD. The superiority and
flexibility of CAE-DFKD are extensively evaluated, including:
\textit{\textbf{i.)}} Significant efficiency advantages resulting from altering
the generator training paradigm; \textit{\textbf{ii.)}} Competitive performance
with existing DFKD state-of-the-art methods on image recognition tasks;
\textit{\textbf{iii.)}} Remarkable transferability of data-free learned
representations demonstrated in downstream tasks.

</details>

### [183] [DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling for Image Restoration](https://arxiv.org/abs/2504.21487)
*Hebaixu Wang,Jing Zhang,Haonan Guo,Di Wang,Jiayi Ma,Bo Du*

Main category: cs.CV

TLDR: DGSolver是一种扩散通用求解器，通过高阶求解器和队列加速采样策略提升图像恢复的准确性和效率，同时结合通用后验采样优化噪声估计。


<details>
  <summary>Details</summary>
Motivation: 现有方法在减少采样步长时引入累积误差，且难以平衡退化表示和恢复质量。

Method: 推导通用扩散模型的精确ODE，设计高阶求解器和队列加速采样策略，结合通用后验采样优化噪声估计。

Result: DGSolver在恢复准确性、稳定性和可扩展性上优于现有方法。

Conclusion: DGSolver通过高效采样和噪声估计优化，显著提升了图像恢复性能。

Abstract: Diffusion models have achieved remarkable progress in universal image
restoration. While existing methods speed up inference by reducing sampling
steps, substantial step intervals often introduce cumulative errors. Moreover,
they struggle to balance the commonality of degradation representations and
restoration quality. To address these challenges, we introduce
\textbf{DGSolver}, a diffusion generalist solver with universal posterior
sampling. We first derive the exact ordinary differential equations for
generalist diffusion models and tailor high-order solvers with a queue-based
accelerated sampling strategy to improve both accuracy and efficiency. We then
integrate universal posterior sampling to better approximate
manifold-constrained gradients, yielding a more accurate noise estimation and
correcting errors in inverse inference. Extensive experiments show that
DGSolver outperforms state-of-the-art methods in restoration accuracy,
stability, and scalability, both qualitatively and quantitatively. Code and
models will be available at https://github.com/MiliLab/DGSolver.

</details>

### [184] [ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery](https://arxiv.org/abs/2504.21491)
*Qinfeng Zhu,Yunxi Jiang,Lei Fan*

Main category: cs.CV

TLDR: ClassWise-CRF是一种结果级类别特定融合架构，通过两阶段过程选择并融合专家网络，显著提升了遥感图像语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决多网络融合中类别特定优化问题，提升语义分割性能。

Method: 1. 使用贪心算法选择类别表现好的专家网络；2. 基于CRF的置信度向量场和指数加权策略融合预测结果。

Result: 在LoveDA和Vaihingen数据集上，mIoU分别提升1.00%/0.68%和0.87%/0.91%。

Conclusion: ClassWise-CRF有效且通用，显著提升遥感图像语义分割性能。

Abstract: We propose a result-level category-specific fusion architecture called
ClassWise-CRF. This architecture employs a two-stage process: first, it selects
expert networks that perform well in specific categories from a pool of
candidate networks using a greedy algorithm; second, it integrates the
segmentation predictions of these selected networks by adaptively weighting
their contributions based on their segmentation performance in each category.
Inspired by Conditional Random Field (CRF), the ClassWise-CRF architecture
treats the segmentation predictions from multiple networks as confidence vector
fields. It leverages segmentation metrics (such as Intersection over Union)
from the validation set as priors and employs an exponential weighting strategy
to fuse the category-specific confidence scores predicted by each network. This
fusion method dynamically adjusts the weights of each network for different
categories, achieving category-specific optimization. Building on this, the
architecture further optimizes the fused results using unary and pairwise
potentials in CRF to ensure spatial consistency and boundary accuracy. To
validate the effectiveness of ClassWise-CRF, we conducted experiments on two
remote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced
semantic segmentation networks. The results show that the ClassWise-CRF
architecture significantly improves segmentation performance: on the LoveDA
dataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on
the validation set and by 0.68% on the test set; on the Vaihingen dataset, the
mIoU improved by 0.87% on the validation set and by 0.91% on the test set.
These results fully demonstrate the effectiveness and generality of the
ClassWise-CRF architecture in semantic segmentation of remote sensing images.
The full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF.

</details>

### [185] [Consistency-aware Fake Videos Detection on Short Video Platforms](https://arxiv.org/abs/2504.21495)
*Junxi Wang,Jize liu,Na Zhang,Yaxiong Wang*

Main category: cs.CV

TLDR: 本文提出一种利用跨模态矛盾检测假新闻的新方法，通过一致性学习和多模态协作诊断提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有假新闻检测方法未充分利用跨模态不一致性作为判别特征，导致检测精度不足。

Method: 提出跨模态一致性学习（CMCL）和多模态协作诊断（MMCD）模块，包括伪标签生成、一致性诊断、特征融合和概率分数融合。

Result: 在FakeSV和FakeTT基准测试中表现出色。

Conclusion: 新方法通过显式利用跨模态矛盾显著提升了假新闻检测性能。

Abstract: This paper focuses to detect the fake news on the short video platforms.
While significant research efforts have been devoted to this task with notable
progress in recent years, current detection accuracy remains suboptimal due to
the rapid evolution of content manipulation and generation technologies.
Existing approaches typically employ a cross-modal fusion strategy that
directly combines raw video data with metadata inputs before applying a
classification layer. However, our empirical observations reveal a critical
oversight: manipulated content frequently exhibits inter-modal inconsistencies
that could serve as valuable discriminative features, yet remain underutilized
in contemporary detection frameworks. Motivated by this insight, we propose a
novel detection paradigm that explicitly identifies and leverages cross-modal
contradictions as discriminative cues. Our approach consists of two core
modules: Cross-modal Consistency Learning (CMCL) and Multi-modal Collaborative
Diagnosis (MMCD). CMCL includes Pseudo-label Generation (PLG) and Cross-modal
Consistency Diagnosis (CMCD). In PLG, a Multimodal Large Language Model is used
to generate pseudo-labels for evaluating cross-modal semantic consistency.
Then, CMCD extracts [CLS] tokens and computes cosine loss to quantify
cross-modal inconsistencies. MMCD further integrates multimodal features
through Multimodal Feature Fusion (MFF) and Probability Scores Fusion (PSF).
MFF employs a co-attention mechanism to enhance semantic interactions across
different modalities, while a Transformer is utilized for comprehensive feature
fusion. Meanwhile, PSF further integrates the fake news probability scores
obtained in the previous step. Extensive experiments on established benchmarks
(FakeSV and FakeTT) demonstrate our model exhibits outstanding performance in
Fake videos detection.

</details>

### [186] [MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance](https://arxiv.org/abs/2504.21497)
*Mengting Wei,Yante Li,Tuomas Varanka,Yan Jiang,Licai Sun,Guoying Zhao*

Main category: cs.CV

TLDR: 提出了一种将3D人脸参数模型与潜在扩散框架结合的视频人脸重演方法，提升形状一致性和运动控制。


<details>
  <summary>Details</summary>
Motivation: 改进现有视频人脸生成方法在形状一致性和运动控制上的不足。

Method: 使用FLAME模型作为3D人脸参数表示，结合深度图、法线图和渲染图，增强潜在扩散模型，并采用多层融合模块结合身份和运动特征。

Result: 在基准数据集上生成高质量人脸动画，精确建模表情和头部姿态变化，泛化性能强。

Conclusion: 该方法有效提升了人脸重演的精度和泛化能力，代码已开源。

Abstract: In this paper, we propose a method for video face reenactment that integrates
a 3D face parametric model into a latent diffusion framework, aiming to improve
shape consistency and motion control in existing video-based face generation
approaches. Our approach employs the FLAME (Faces Learned with an Articulated
Model and Expressions) model as the 3D face parametric representation,
providing a unified framework for modeling face expressions and head pose. This
enables precise extraction of detailed face geometry and motion features from
driving videos. Specifically, we enhance the latent diffusion model with rich
3D expression and detailed pose information by incorporating depth maps, normal
maps, and rendering maps derived from FLAME sequences. A multi-layer face
movements fusion module with integrated self-attention mechanisms is used to
combine identity and motion latent features within the spatial domain. By
utilizing the 3D face parametric model as motion guidance, our method enables
parametric alignment of face identity between the reference image and the
motion captured from the driving video. Experimental results on benchmark
datasets show that our method excels at generating high-quality face animations
with precise expression and head pose variation modeling. In addition, it
demonstrates strong generalization performance on out-of-domain images. Code is
publicly available at https://github.com/weimengting/MagicPortrait.

</details>

### [187] [SAM4EM: Efficient memory-based two stage prompt-free segment anything model adapter for complex 3D neuroscience electron microscopy stacks](https://arxiv.org/abs/2504.21544)
*Uzair Shah,Marco Agus,Daniya Boges,Vanessa Chiappini,Mahmood Alzubaidi,Jens Schneider,Markus Hadwiger,Pierre J. Magistretti,Mowafa Househ,Corrado Calı*

Main category: cs.CV

TLDR: SAM4EM是一种基于Segment Anything Model（SAM）的新方法，用于3D分割电子显微镜（EM）数据中的复杂神经结构，通过无提示适配器和双阶段微调策略显著提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 解决电子显微镜数据中复杂神经结构（如线粒体、胶质细胞和突触）的3D分割问题，尤其是在标注数据有限的情况下。

Method: 1. 开发无提示适配器，通过两阶段掩码解码自动生成提示嵌入；2. 基于LoRA的双阶段微调方法；3. 引入3D记忆注意力机制确保分割一致性。

Result: 在神经科学分割基准测试中表现优于现有方法，特别是在胶质细胞和突触后密度的分割上。

Conclusion: SAM4EM为复杂神经结构的3D分割提供了高效解决方案，并发布了新的基准数据集。

Abstract: We present SAM4EM, a novel approach for 3D segmentation of complex neural
structures in electron microscopy (EM) data by leveraging the Segment Anything
Model (SAM) alongside advanced fine-tuning strategies. Our contributions
include the development of a prompt-free adapter for SAM using two stage mask
decoding to automatically generate prompt embeddings, a dual-stage fine-tuning
method based on Low-Rank Adaptation (LoRA) for enhancing segmentation with
limited annotated data, and a 3D memory attention mechanism to ensure
segmentation consistency across 3D stacks. We further release a unique
benchmark dataset for the segmentation of astrocytic processes and synapses. We
evaluated our method on challenging neuroscience segmentation benchmarks,
specifically targeting mitochondria, glia, and synapses, with significant
accuracy improvements over state-of-the-art (SOTA) methods, including recent
SAM-based adapters developed for the medical domain and other vision
transformer-based approaches. Experimental results indicate that our approach
outperforms existing solutions in the segmentation of complex processes like
glia and post-synaptic densities. Our code and models are available at
https://github.com/Uzshah/SAM4EM.

</details>

### [188] [Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models](https://arxiv.org/abs/2504.21559)
*Sangmin Woo,Kang Zhou,Yun Zhou,Shuai Wang,Sheng Guan,Haibo Ding,Lin Lee Cheong*

Main category: cs.CV

TLDR: 通过视觉提示工程（BBVPE）框架，动态选择最优视觉提示，显著减少大型视觉语言模型（LVLM）中的物体幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）常出现物体幻觉问题，影响其可靠性。研究发现简单的视觉提示可以缓解这一问题，但不同提示效果差异显著。

Method: 提出黑盒视觉提示工程（BBVPE）框架，通过候选视觉提示池和路由器模型动态选择最优提示，无需访问模型内部。

Result: 在POPE和CHAIR等基准测试中，BBVPE显著减少了物体幻觉。

Conclusion: BBVPE是一种模型无关的方法，适用于开源和专有LVLM，能有效提升模型可靠性。

Abstract: Large Vision Language Models (LVLMs) often suffer from object hallucination,
which undermines their reliability. Surprisingly, we find that simple
object-based visual prompting -- overlaying visual cues (e.g., bounding box,
circle) on images -- can significantly mitigate such hallucination; however,
different visual prompts (VPs) vary in effectiveness. To address this, we
propose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify
optimal VPs that enhance LVLM responses without needing access to model
internals. Our approach employs a pool of candidate VPs and trains a router
model to dynamically select the most effective VP for a given input image. This
black-box approach is model-agnostic, making it applicable to both open-source
and proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR
demonstrate that BBVPE effectively reduces object hallucination.

</details>

### [189] [Active Light Modulation to Counter Manipulation of Speech Visual Content](https://arxiv.org/abs/2504.21846)
*Hadleigh Schwartz,Xiaofeng Yan,Charles J. Carver,Xia Zhou*

Main category: cs.CV

TLDR: Spotlight是一种低开销、无干扰的系统，通过不可见的光调制在视频中嵌入动态物理签名，保护实时演讲视频免受视觉伪造。


<details>
  <summary>Details</summary>
Motivation: 高知名度演讲视频容易被伪造，因其易获取和影响力大，需要一种有效保护手段。

Method: 利用局部敏感哈希生成紧凑的、姿态不变的视频特征，并通过光调制将签名嵌入视频。

Result: 实验显示Spotlight在检测伪造视频时AUC≥0.99，真阳性率100%，且对录制条件和后处理高度鲁棒。

Conclusion: Spotlight提供了一种高效、安全且鲁棒的实时视频防伪造解决方案。

Abstract: High-profile speech videos are prime targets for falsification, owing to
their accessibility and influence. This work proposes Spotlight, a low-overhead
and unobtrusive system for protecting live speech videos from visual
falsification of speaker identity and lip and facial motion. Unlike predominant
falsification detection methods operating in the digital domain, Spotlight
creates dynamic physical signatures at the event site and embeds them into all
video recordings via imperceptible modulated light. These physical signatures
encode semantically-meaningful features unique to the speech event, including
the speaker's identity and facial motion, and are cryptographically-secured to
prevent spoofing. The signatures can be extracted from any video downstream and
validated against the portrayed speech content to check its integrity. Key
elements of Spotlight include (1) a framework for generating extremely compact
(i.e., 150-bit), pose-invariant speech video features, based on
locality-sensitive hashing; and (2) an optical modulation scheme that embeds
>200 bps into video while remaining imperceptible both in video and live.
Prototype experiments on extensive video datasets show Spotlight achieves AUCs
$\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified
videos. Further, Spotlight is highly robust across recording conditions, video
post-processing techniques, and white-box adversarial attacks on its video
feature extraction methodologies.

</details>

### [190] [Iterative Trajectory Exploration for Multimodal Agents](https://arxiv.org/abs/2504.21561)
*Pengxiang Li,Zhi Gao,Bofei Zhang,Yapeng Mi,Xiaojian Ma,Chenrui Shi,Tao Yuan,Yuwei Wu,Yunde Jia,Song-Chun Zhu,Qing Li*

Main category: cs.CV

TLDR: SPORT是一种多模态代理的在线自探索方法，通过逐步偏好优化改进代理轨迹，无需专家标注。


<details>
  <summary>Details</summary>
Motivation: 现有代理需大量专家数据微调以适应新环境，SPORT旨在通过自生成任务和学习解决任务来减少依赖。

Method: SPORT通过任务合成、步骤采样、步骤验证和偏好调优四个迭代组件实现。

Result: 在GTA和GAIA基准测试中，SPORT代理分别提升了6.41%和3.64%。

Conclusion: SPORT方法展示了在多模态任务中的泛化能力和有效性。

Abstract: Multimodal agents, which integrate a controller (e.g., a large language
model) with external tools, have demonstrated remarkable capabilities in
tackling complex tasks. However, existing agents need to collect a large number
of expert data for fine-tuning to adapt to new environments. In this paper, we
propose an online self-exploration method for multimodal agents, namely SPORT,
via step-wise preference optimization to refine the trajectories of agents,
which automatically generates tasks and learns from solving the generated
tasks, without any expert annotation. SPORT operates through four iterative
components: task synthesis, step sampling, step verification, and preference
tuning. First, we synthesize multi-modal tasks using language models. Then, we
introduce a novel search scheme, where step sampling and step verification are
executed alternately to solve each generated task. We employ a verifier to
provide AI feedback to construct step-wise preference data. The data is
subsequently used to update the controller's policy through preference tuning,
producing a SPORT Agent. By interacting with real environments, the SPORT Agent
evolves into a more refined and capable system. Evaluation in the GTA and GAIA
benchmarks show that the SPORT Agent achieves 6.41\% and 3.64\% improvements,
underscoring the generalization and effectiveness introduced by our method. The
project page is https://SPORT-Agents.github.io.

</details>

### [191] [eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes](https://arxiv.org/abs/2504.21562)
*Henry John Krumb,Anirban Mukhopadhyay*

Main category: cs.CV

TLDR: 论文提出了一种基于神经细胞自动机（NCA）的方法，用于无线胶囊内窥镜的出血分割和深度估计，并通过蒸馏技术将大型模型压缩至适合微型设备的规模。


<details>
  <summary>Details</summary>
Motivation: 无线胶囊内窥镜生成大量视频数据，但传统深度学习模型体积过大，无法直接运行在胶囊设备上，因此需要一种轻量化的解决方案。

Method: 使用NCA架构进行出血分割和深度估计，通过蒸馏大型基础模型的输出作为伪真实数据，并将训练后的NCA移植到ESP32微控制器上。

Result: NCA在准确性和参数效率上优于其他便携模型，ESP32-S3上的运行时优化显著提升了推理速度。

Conclusion: 该研究首次实现了在微型设备上可靠的出血分割和深度估计，为胶囊内窥镜的精确诊断和定位奠定了基础。

Abstract: Wireless Capsule Endoscopy is a non-invasive imaging method for the entire
gastrointestinal tract, and is a pain-free alternative to traditional
endoscopy. It generates extensive video data that requires significant review
time, and localizing the capsule after ingestion is a challenge. Techniques
like bleeding detection and depth estimation can help with localization of
pathologies, but deep learning models are typically too large to run directly
on the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and
depth estimation are trained on capsule endoscopic images. For monocular depth
estimation, we distill a large foundation model into the lean NCA architecture,
by treating the outputs of the foundation model as pseudo ground truth. We then
port the trained NCA to the ESP32 microcontroller, enabling efficient image
processing on hardware as small as a camera capsule. NCA are more accurate
(Dice) than other portable segmentation models, while requiring more than 100x
fewer parameters stored in memory than other small-scale models. The visual
results of NCA depth estimation look convincing, and in some cases beat the
realism and detail of the pseudo ground truth. Runtime optimizations on the
ESP32-S3 accelerate the average inference speed significantly, by more than
factor 3. With several algorithmic adjustments and distillation, it is possible
to eNCApsulate NCA models into microcontrollers that fit into wireless capsule
endoscopes. This is the first work that enables reliable bleeding segmentation
and depth estimation on a miniaturized device, paving the way for precise
diagnosis combined with visual odometry as a means of precise localization of
the capsule -- on the capsule.

</details>

### [192] [Cascade Detector Analysis and Application to Biomedical Microscopy](https://arxiv.org/abs/2504.21598)
*Thomas L. Athey,Shashata Sawmya,Nir Shavit*

Main category: cs.CV

TLDR: 提出了一种基于级联检测器的高效稀疏目标识别方法，适用于多分辨率图像，显著减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 随着计算机视觉模型和生物医学数据集的规模增长，需要高效的推理算法。

Method: 利用级联检测器在不同分辨率下识别稀疏目标，推导其准确性和分类器调用次数。

Result: 多级检测器在荧光细胞检测、细胞器分割和组织分割中，性能相当但时间减少30-75%。

Conclusion: 该方法适用于多种计算机视觉模型和数据领域。

Abstract: As both computer vision models and biomedical datasets grow in size, there is
an increasing need for efficient inference algorithms. We utilize cascade
detectors to efficiently identify sparse objects in multiresolution images.
Given an object's prevalence and a set of detectors at different resolutions
with known accuracies, we derive the accuracy, and expected number of
classifier calls by a cascade detector. These results generalize across number
of dimensions and number of cascade levels. Finally, we compare one- and
two-level detectors in fluorescent cell detection, organelle segmentation, and
tissue segmentation across various microscopy modalities. We show that the
multi-level detector achieves comparable performance in 30-75% less time. Our
work is compatible with a variety of computer vision models and data domains.

</details>

### [193] [Mcity Data Engine: Iterative Model Improvement Through Open-Vocabulary Data Selection](https://arxiv.org/abs/2504.21614)
*Daniel Bogdoll,Rajanikant Patnaik Ananta,Abeyankar Giridharan,Isabel Moore,Gregory Stevens,Henry X. Liu*

Main category: cs.CV

TLDR: Mcity Data Engine是一个开源系统，旨在解决在大量未标记数据中选择和标记样本的挑战，特别关注罕见和新类别。


<details>
  <summary>Details</summary>
Motivation: 随着数据量的增加，选择和标记样本以训练机器学习模型变得更具挑战性，尤其是在智能交通系统（ITS）中。现有工业数据引擎多为专有，缺乏开源解决方案。

Method: Mcity Data Engine提供从数据采集到模型部署的完整开发周期模块，采用开放词汇数据选择方法。

Result: 该系统公开可用，代码发布于GitHub（MIT许可证），专注于罕见和新类别的识别。

Conclusion: Mcity Data Engine填补了开源数据引擎的空白，支持研究人员和开源社区在ITS等领域的数据处理需求。

Abstract: With an ever-increasing availability of data, it has become more and more
challenging to select and label appropriate samples for the training of machine
learning models. It is especially difficult to detect long-tail classes of
interest in large amounts of unlabeled data. This holds especially true for
Intelligent Transportation Systems (ITS), where vehicle fleets and roadside
perception systems generate an abundance of raw data. While industrial,
proprietary data engines for such iterative data selection and model training
processes exist, researchers and the open-source community suffer from a lack
of an openly available system. We present the Mcity Data Engine, which provides
modules for the complete data-based development cycle, beginning at the data
acquisition phase and ending at the model deployment stage. The Mcity Data
Engine focuses on rare and novel classes through an open-vocabulary data
selection process. All code is publicly available on GitHub under an MIT
license: https://github.com/mcity/mcity_data_engine

</details>

### [194] [Diffusion-based Adversarial Identity Manipulation for Facial Privacy Protection](https://arxiv.org/abs/2504.21646)
*Liqin Wang,Qianyue Hu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TLDR: DiffAIM是一种基于扩散模型的对抗性人脸生成方法，旨在保护用户隐私，通过操纵人脸身份生成自然且高迁移性的对抗性人脸。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法生成自然的人脸图像以保护隐私，DiffAIM旨在解决这一问题。

Method: 利用扩散模型的低维潜在空间，在反向扩散过程中注入梯度对抗性身份引导，逐步生成对抗性人脸。

Result: DiffAIM在实验中表现出更强的黑盒攻击迁移性和视觉质量，适用于商业FR API。

Conclusion: DiffAIM是一种有效的隐私保护方法，生成的人脸既自然又能对抗恶意FR系统。

Abstract: The success of face recognition (FR) systems has led to serious privacy
concerns due to potential unauthorized surveillance and user tracking on social
networks. Existing methods for enhancing privacy fail to generate natural face
images that can protect facial privacy. In this paper, we propose
diffusion-based adversarial identity manipulation (DiffAIM) to generate natural
and highly transferable adversarial faces against malicious FR systems. To be
specific, we manipulate facial identity within the low-dimensional latent space
of a diffusion model. This involves iteratively injecting gradient-based
adversarial identity guidance during the reverse diffusion process,
progressively steering the generation toward the desired adversarial faces. The
guidance is optimized for identity convergence towards a target while promoting
semantic divergence from the source, facilitating effective impersonation while
maintaining visual naturalness. We further incorporate structure-preserving
regularization to preserve facial structure consistency during manipulation.
Extensive experiments on both face verification and identification tasks
demonstrate that compared with the state-of-the-art, DiffAIM achieves stronger
black-box attack transferability while maintaining superior visual quality. We
also demonstrate the effectiveness of the proposed approach for commercial FR
APIs, including Face++ and Aliyun.

</details>

### [195] [HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation](https://arxiv.org/abs/2504.21650)
*Haiyang Zhou,Wangbo Yu,Jiawen Guan,Xinhua Cheng,Yonghong Tian,Li Yuan*

Main category: cs.CV

TLDR: HoloTime框架通过视频扩散模型生成全景视频，并结合4D场景重建技术，提升VR/AR沉浸体验。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型主要关注静态3D场景或对象级动态，无法满足沉浸式4D体验需求。

Method: 提出HoloTime框架，包括全景视频生成（Panoramic Animator）和4D场景重建（Panoramic Space-Time Reconstruction）。

Result: 实验表明，该方法在全景视频生成和4D场景重建上优于现有方法。

Conclusion: HoloTime能够创建更逼真的沉浸式环境，提升VR/AR用户体验。

Abstract: The rapid advancement of diffusion models holds the promise of
revolutionizing the application of VR and AR technologies, which typically
require scene-level 4D assets for user experience. Nonetheless, existing
diffusion models predominantly concentrate on modeling static 3D scenes or
object-level dynamics, constraining their capacity to provide truly immersive
experiences. To address this issue, we propose HoloTime, a framework that
integrates video diffusion models to generate panoramic videos from a single
prompt or reference image, along with a 360-degree 4D scene reconstruction
method that seamlessly transforms the generated panoramic video into 4D assets,
enabling a fully immersive 4D experience for users. Specifically, to tame video
diffusion models for generating high-fidelity panoramic videos, we introduce
the 360World dataset, the first comprehensive collection of panoramic videos
suitable for downstream 4D scene reconstruction tasks. With this curated
dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion
model that can convert panoramic images into high-quality panoramic videos.
Following this, we present Panoramic Space-Time Reconstruction, which leverages
a space-time depth estimation method to transform the generated panoramic
videos into 4D point clouds, enabling the optimization of a holistic 4D
Gaussian Splatting representation to reconstruct spatially and temporally
consistent 4D scenes. To validate the efficacy of our method, we conducted a
comparative analysis with existing approaches, revealing its superiority in
both panoramic video generation and 4D scene reconstruction. This demonstrates
our method's capability to create more engaging and realistic immersive
environments, thereby enhancing user experiences in VR and AR applications.

</details>

### [196] [Visual Text Processing: A Comprehensive Review and Unified Evaluation](https://arxiv.org/abs/2504.21682)
*Yan Shu,Weichao Zeng,Fangmin Zhao,Zeyu Chen,Zhenhang Li,Xiaomeng Yang,Yu Zhou,Paolo Rota,Xiang Bai,Lianwen Jin,Xu-Cheng Yin,Nicu Sebe*

Main category: cs.CV

TLDR: 该论文综述了视觉文本处理的最新进展，提出了VTPBench基准和VTPScore评估指标，并分析了文本特征在任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 视觉文本在文档和场景图像中具有丰富的语义信息，但其独特性质带来了挑战，需要有效捕捉和利用这些特征。

Method: 通过多视角分析视觉文本处理任务，提出VTPBench基准和VTPScore评估指标，并评估20多个模型。

Result: 研究发现当前技术仍有改进空间，并提出了新的评估方法。

Conclusion: 该工作旨在为视觉文本处理领域的未来研究提供基础资源。

Abstract: Visual text is a crucial component in both document and scene images,
conveying rich semantic information and attracting significant attention in the
computer vision community. Beyond traditional tasks such as text detection and
recognition, visual text processing has witnessed rapid advancements driven by
the emergence of foundation models, including text image reconstruction and
text image manipulation. Despite significant progress, challenges remain due to
the unique properties that differentiate text from general objects. Effectively
capturing and leveraging these distinct textual characteristics is essential
for developing robust visual text processing models. In this survey, we present
a comprehensive, multi-perspective analysis of recent advancements in visual
text processing, focusing on two key questions: (1) What textual features are
most suitable for different visual text processing tasks? (2) How can these
distinctive text features be effectively incorporated into processing
frameworks? Furthermore, we introduce VTPBench, a new benchmark that
encompasses a broad range of visual text processing datasets. Leveraging the
advanced visual quality assessment capabilities of multimodal large language
models (MLLMs), we propose VTPScore, a novel evaluation metric designed to
ensure fair and reliable evaluation. Our empirical study with more than 20
specific models reveals substantial room for improvement in the current
techniques. Our aim is to establish this work as a fundamental resource that
fosters future exploration and innovation in the dynamic field of visual text
processing. The relevant repository is available at
https://github.com/shuyansy/Visual-Text-Processing-survey.

</details>

### [197] [Enhancing Self-Supervised Fine-Grained Video Object Tracking with Dynamic Memory Prediction](https://arxiv.org/abs/2504.21692)
*Zihan Zhou,Changrui Dai,Aibo Song,Xiaolin Fang*

Main category: cs.CV

TLDR: 论文提出了一种动态内存预测（DMP）框架，通过多参考帧直接增强帧重建，提升了复杂场景下的视频分析准确性。


<details>
  <summary>Details</summary>
Motivation: 现有帧重建方法在复杂场景（如遮挡或快速运动）中忽视多参考帧的直接参与，限制了重建和决策效果。

Method: DMP框架包含动态选择参考帧的内存引擎和双向目标预测网络，利用多参考帧提升模型鲁棒性。

Result: 实验表明，DMP在对象分割和关键点跟踪任务上优于现有自监督技术。

Conclusion: DMP框架通过多参考帧的动态利用，显著提升了视频分析的准确性和鲁棒性。

Abstract: Successful video analysis relies on accurate recognition of pixels across
frames, and frame reconstruction methods based on video correspondence learning
are popular due to their efficiency. Existing frame reconstruction methods,
while efficient, neglect the value of direct involvement of multiple reference
frames for reconstruction and decision-making aspects, especially in complex
situations such as occlusion or fast movement. In this paper, we introduce a
Dynamic Memory Prediction (DMP) framework that innovatively utilizes multiple
reference frames to concisely and directly enhance frame reconstruction. Its
core component is a Reference Frame Memory Engine that dynamically selects
frames based on object pixel features to improve tracking accuracy. In
addition, a Bidirectional Target Prediction Network is built to utilize
multiple reference frames to improve the robustness of the model. Through
experiments, our algorithm outperforms the state-of-the-art self-supervised
techniques on two fine-grained video object tracking tasks: object segmentation
and keypoint tracking.

</details>

### [198] [REHEARSE-3D: A Multi-modal Emulated Rain Dataset for 3D Point Cloud De-raining](https://arxiv.org/abs/2504.21699)
*Abu Mohammed Raisuddin,Jesper Holmblad,Hamed Haghighi,Yuri Poledna,Maikol Funk Drechsler,Valentina Donzella,Eren Erdal Aksoy*

Main category: cs.CV

TLDR: 论文提出REHEARSE-3D数据集，用于解决自动驾驶中LiDAR点云因降雨干扰导致的质量问题，并评估了多种降噪方法。


<details>
  <summary>Details</summary>
Motivation: 降雨干扰LiDAR点云质量，可能引发自动驾驶安全问题，需研究天气感知技术。

Method: 发布大规模多模态模拟降雨数据集REHEARSE-3D，包含高分辨率LiDAR和4D雷达数据，并标注雨滴特征。

Result: 数据集支持雨滴检测与去除，并评估了统计和深度学习模型的性能。

Conclusion: REHEARSE-3D数据集和基准模型将公开，推动3D点云去雨研究。

Abstract: Sensor degradation poses a significant challenge in autonomous driving.
During heavy rainfall, the interference from raindrops can adversely affect the
quality of LiDAR point clouds, resulting in, for instance, inaccurate point
measurements. This, in turn, can potentially lead to safety concerns if
autonomous driving systems are not weather-aware, i.e., if they are unable to
discern such changes. In this study, we release a new, large-scale, multi-modal
emulated rain dataset, REHEARSE-3D, to promote research advancements in 3D
point cloud de-raining. Distinct from the most relevant competitors, our
dataset is unique in several respects. First, it is the largest point-wise
annotated dataset, and second, it is the only one with high-resolution LiDAR
data (LiDAR-256) enriched with 4D Radar point clouds logged in both daytime and
nighttime conditions in a controlled weather environment. Furthermore,
REHEARSE-3D involves rain-characteristic information, which is of significant
value not only for sensor noise modeling but also for analyzing the impact of
weather at a point level. Leveraging REHEARSE-3D, we benchmark raindrop
detection and removal in fused LiDAR and 4D Radar point clouds. Our
comprehensive study further evaluates the performance of various statistical
and deep-learning models. Upon publication, the dataset and benchmark models
will be made publicly available at: https://sporsho.github.io/REHEARSE3D.

</details>

### [199] [Vision Transformers in Precision Agriculture: A Comprehensive Survey](https://arxiv.org/abs/2504.21706)
*Saber Mehdipour,Seyed Abolghasem Mirroshandel,Seyed Amirhossein Tabatabaei*

Main category: cs.CV

TLDR: 本文综述了视觉变换器（ViTs）在精准农业中的应用，探讨了其优势、挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统植物病害检测方法存在可扩展性和准确性限制，ViTs因其处理长距离依赖和视觉任务的能力成为有前景的替代方案。

Method: 介绍了ViTs的基础架构及其从自然语言处理到计算机视觉的转变，分析了ViTs如何缓解传统模型的归纳偏差，并综述了相关文献、数据集和性能指标。

Result: 比较了CNNs与ViTs的性能，探讨了混合模型和技术挑战（如数据需求、计算成本和模型可解释性），并提出了潜在解决方案。

Conclusion: ViTs有望推动智能和精准农业的发展，未来研究应关注其实际应用中的技术改进和集成。

Abstract: Detecting plant diseases is a crucial aspect of modern agriculture - it plays
a key role in maintaining crop health and increasing overall yield. Traditional
approaches, though still valuable, often rely on manual inspection or
conventional machine learning techniques, both of which face limitations in
scalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as
a promising alternative, offering benefits such as improved handling of
long-range dependencies and better scalability for visual tasks. This survey
explores the application of ViTs in precision agriculture, covering tasks from
classification to detection and segmentation. We begin by introducing the
foundational architecture of ViTs and discuss their transition from Natural
Language Processing (NLP) to computer vision. The discussion includes the
concept of inductive bias in traditional models like Convolutional Neural
Networks (CNNs), and how ViTs mitigate these biases. We provide a comprehensive
review of recent literature, focusing on key methodologies, datasets, and
performance metrics. The survey also includes a comparative analysis of CNNs
and ViTs, with a look at hybrid models and performance enhancements. Technical
challenges - such as data requirements, computational demands, and model
interpretability - are addressed alongside potential solutions. Finally, we
outline potential research directions and technological advancements that could
further support the integration of ViTs in real-world agricultural settings.
Our goal with this study is to offer practitioners and researchers a deeper
understanding of how ViTs are poised to transform smart and precision
agriculture.

</details>

### [200] [VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction](https://arxiv.org/abs/2504.21718)
*Shiying Li,Xingqun Qi,Bingkun Yang,Chen Weile,Zezhao Tian,Muyi Sun,Qifeng Liu,Man Zhang,Zhenan Sun*

Main category: cs.CV

TLDR: 论文提出VividListener框架，用于生成具有细腻情感和表达反应的听者头部动态，并收集了大规模多模态数据集ListenerX。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了对运动变化和情感强度的细粒度控制，且缺乏长期大规模的多模态标注数据。

Method: 提出VividListener框架，包含Responsive Interaction Module（RIM）和Emotional Intensity Tags（EIT），用于多模态交互嵌入和情感强度编辑。

Result: 在ListenerX数据集上，VividListener实现了最先进的性能，生成表达性强且可控的听者动态。

Conclusion: VividListener框架解决了现有研究的不足，为虚拟对话建模提供了更细腻和可控的解决方案。

Abstract: Generating responsive listener head dynamics with nuanced emotions and
expressive reactions is crucial for practical dialogue modeling in various
virtual avatar animations. Previous studies mainly focus on the direct
short-term production of listener behavior. They overlook the fine-grained
control over motion variations and emotional intensity, especially in
long-sequence modeling. Moreover, the lack of long-term and large-scale paired
speaker-listener corpora including head dynamics and fine-grained
multi-modality annotations (e.g., text-based expression descriptions, emotional
intensity) also limits the application of dialogue modeling.Therefore, we first
newly collect a large-scale multi-turn dataset of 3D dyadic conversation
containing more than 1.4M valid frames for multi-modal responsive interaction,
dubbed ListenerX. Additionally, we propose VividListener, a novel framework
enabling fine-grained, expressive and controllable listener dynamics modeling.
This framework leverages multi-modal conditions as guiding principles for
fostering coherent interactions between speakers and listeners.Specifically, we
design the Responsive Interaction Module (RIM) to adaptively represent the
multi-modal interactive embeddings. RIM ensures the listener dynamics achieve
fine-grained semantic coordination with textual descriptions and adjustments,
while preserving expressive reaction with speaker behavior. Meanwhile, we
design the Emotional Intensity Tags (EIT) for emotion intensity editing with
multi-modal information integration, applying to both text descriptions and
listener motion amplitude.Extensive experiments conducted on our newly
collected ListenerX dataset demonstrate that VividListener achieves
state-of-the-art performance, realizing expressive and controllable listener
dynamics.

</details>

### [201] [Common3D: Self-Supervised Learning of 3D Morphable Models for Common Objects in Neural Feature Space](https://arxiv.org/abs/2504.21749)
*Leonhard Sommer,Olaf Dünkel,Christian Theobalt,Adam Kortylewski*

Main category: cs.CV

TLDR: Common3D提出了一种完全自监督的方法，从物体中心视频中学习3D可变形模型（3DMMs），无需3D数据采集或类别特定训练。


<details>
  <summary>Details</summary>
Motivation: 3DMMs通常仅适用于少数特定类别（如人脸或人体），且需要复杂的3D数据采集和训练过程。Common3D旨在解决这一限制，为常见物体学习3DMMs。

Method: Common3D通过3D模板网格和图像条件神经网络的变形场表示物体，使用神经特征而非RGB颜色表示外观，并通过对比目标训练外观特征。

Result: Common3D在3D物体姿态估计和语义对应任务中表现优于现有方法，并能以零样本方式解决多种视觉任务。

Conclusion: Common3D是首个完全自监督的方法，能够从视频中学习通用3DMMs，为常见物体提供高质量的3D建模解决方案。

Abstract: 3D morphable models (3DMMs) are a powerful tool to represent the possible
shapes and appearances of an object category. Given a single test image, 3DMMs
can be used to solve various tasks, such as predicting the 3D shape, pose,
semantic correspondence, and instance segmentation of an object. Unfortunately,
3DMMs are only available for very few object categories that are of particular
interest, like faces or human bodies, as they require a demanding 3D data
acquisition and category-specific training process. In contrast, we introduce a
new method, Common3D, that learns 3DMMs of common objects in a fully
self-supervised manner from a collection of object-centric videos. For this
purpose, our model represents objects as a learned 3D template mesh and a
deformation field that is parameterized as an image-conditioned neural network.
Different from prior works, Common3D represents the object appearance with
neural features instead of RGB colors, which enables the learning of more
generalizable representations through an abstraction from pixel intensities.
Importantly, we train the appearance features using a contrastive objective by
exploiting the correspondences defined through the deformable template mesh.
This leads to higher quality correspondence features compared to related works
and a significantly improved model performance at estimating 3D object pose and
semantic correspondence. Common3D is the first completely self-supervised
method that can solve various vision tasks in a zero-shot manner.

</details>

### [202] [Anatomical Similarity as a New Metric to Evaluate Brain Generative Models](https://arxiv.org/abs/2504.21771)
*Bahram Jafrasteh,Wei Peng,Cheng Wan,Yimin Luo,Ehsan Adeli,Qingyu Zhao*

Main category: cs.CV

TLDR: 提出了一种名为WASABI的新指标，用于评估合成脑MRI的解剖学真实性，通过对比真实与合成脑部结构的体积分布，显示出比传统图像级指标更高的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注纹理和视觉感知，缺乏对合成脑MRI解剖学真实性的敏感度，而解剖学真实性对临床应用至关重要。

Method: 利用SynthSeg工具对脑MRI进行分割，计算各脑区的体积分布，并通过多元Wasserstein距离比较真实与合成脑部结构的差异。

Result: 在五个生成模型和两个真实数据集上的实验表明，WASABI在量化解剖学差异方面比传统指标更敏感，即使合成图像视觉质量接近完美。

Conclusion: 研究呼吁将评估范式从视觉检查和传统指标转向解剖学真实性，作为脑MRI合成的关键临床基准。

Abstract: Generative models enhance neuroimaging through data augmentation, quality
improvement, and rare condition studies. Despite advances in realistic
synthetic MRIs, evaluations focus on texture and perception, lacking
sensitivity to crucial anatomical fidelity. This study proposes a new metric,
called WASABI (Wasserstein-Based Anatomical Brain Index), to assess the
anatomical realism of synthetic brain MRIs. WASABI leverages \textit{SynthSeg},
a deep learning-based brain parcellation tool, to derive volumetric measures of
brain regions in each MRI and uses the multivariate Wasserstein distance to
compare distributions between real and synthetic anatomies. Based on controlled
experiments on two real datasets and synthetic MRIs from five generative
models, WASABI demonstrates higher sensitivity in quantifying anatomical
discrepancies compared to traditional image-level metrics, even when synthetic
images achieve near-perfect visual quality. Our findings advocate for shifting
the evaluation paradigm beyond visual inspection and conventional metrics,
emphasizing anatomical fidelity as a crucial benchmark for clinically
meaningful brain MRI synthesis. Our code is available at
https://github.com/BahramJafrasteh/wasabi-mri.

</details>

### [203] [Anomaly-Driven Approach for Enhanced Prostate Cancer Segmentation](https://arxiv.org/abs/2504.21789)
*Alessia Hu,Regina Beets-Tan,Lishan Cai,Eduardo Pooch*

Main category: cs.CV

TLDR: 该研究提出了一种基于异常检测的U-Net模型（adU-Net），通过结合异常图改进前列腺癌的自动识别，性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: MRI在识别临床显著性前列腺癌（csPCa）中很重要，但自动化方法面临数据不平衡、肿瘤大小不一和标注数据不足等挑战。

Method: 研究引入adU-Net，将基于双参数MRI序列的异常图融入深度学习分割框架，并通过比较异常检测方法评估其效果。

Result: 在外部测试集上，adU-Net的平均得分（AUROC和AP的均值）为0.618，优于基线nnU-Net（0.605）。

Conclusion: 结合异常检测的分割方法提高了泛化能力和性能，尤其是基于ADC的异常图，为csPCa自动识别提供了新方向。

Abstract: Magnetic Resonance Imaging (MRI) plays an important role in identifying
clinically significant prostate cancer (csPCa), yet automated methods face
challenges such as data imbalance, variable tumor sizes, and a lack of
annotated data. This study introduces Anomaly-Driven U-Net (adU-Net), which
incorporates anomaly maps derived from biparametric MRI sequences into a deep
learning-based segmentation framework to improve csPCa identification. We
conduct a comparative analysis of anomaly detection methods and evaluate the
integration of anomaly maps into the segmentation pipeline. Anomaly maps,
generated using Fixed-Point GAN reconstruction, highlight deviations from
normal prostate tissue, guiding the segmentation model to potential cancerous
regions. We compare the performance by using the average score, computed as the
mean of the AUROC and Average Precision (AP). On the external test set, adU-Net
achieves the best average score of 0.618, outperforming the baseline nnU-Net
model (0.605). The results demonstrate that incorporating anomaly detection
into segmentation improves generalization and performance, particularly with
ADC-based anomaly maps, offering a promising direction for automated csPCa
identification.

</details>

### [204] [A simple and effective approach for body part recognition on CT scans based on projection estimation](https://arxiv.org/abs/2504.21810)
*Franko Hrzic,Mohammadreza Movahhedi,Ophelie Lavoie-Gagne,Ata Kiapour*

Main category: cs.CV

TLDR: 论文提出了一种基于2D X射线估计3D CT扫描的简单有效方法，用于识别身体区域，显著优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 标注CT数据困难且耗时，现有方法常忽略扫描中的其他解剖区域，需要更高效的方法。

Method: 通过估计2D图像识别14个身体区域，与2.5D、3D和基础模型方法对比。

Result: EffNet-B0模型表现最佳，F1分数为0.980±0.016，显著优于其他方法。

Conclusion: 该方法为构建高质量医学数据集提供了有效工具，优于现有技术。

Abstract: It is well known that machine learning models require a high amount of
annotated data to obtain optimal performance. Labelling Computed Tomography
(CT) data can be a particularly challenging task due to its volumetric nature
and often missing and$/$or incomplete associated meta-data. Even inspecting one
CT scan requires additional computer software, or in the case of programming
languages $-$ additional programming libraries. This study proposes a simple,
yet effective approach based on 2D X-ray-like estimation of 3D CT scans for
body region identification. Although body region is commonly associated with
the CT scan, it often describes only the focused major body region neglecting
other anatomical regions present in the observed CT. In the proposed approach,
estimated 2D images were utilized to identify 14 distinct body regions,
providing valuable information for constructing a high-quality medical dataset.
To evaluate the effectiveness of the proposed method, it was compared against
2.5D, 3D and foundation model (MI2) based approaches. Our approach outperformed
the others, where it came on top with statistical significance and F1-Score for
the best-performing model EffNet-B0 of 0.980 $\pm$ 0.016 in comparison to the
0.840 $\pm$ 0.114 (2.5D DenseNet-161), 0.854 $\pm$ 0.096 (3D VoxCNN), and 0.852
$\pm$ 0.104 (MI2 foundation model). The utilized dataset comprised three
different clinical centers and counted 15,622 CT scans (44,135 labels).

</details>

### [205] [Why Compress What You Can Generate? When GPT-4o Generation Ushers in Image Compression Fields](https://arxiv.org/abs/2504.21814)
*Yixin Gao,Xiaohan Pan,Xin Li,Zhibo Chen*

Main category: cs.CV

TLDR: 论文探讨了利用AIGC基础模型（如GPT-4o）在图像压缩中的潜力，提出了一种基于文本和多模态编码的新范式，通过结构光栅扫描提示工程机制实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: AIGC基础模型的快速发展为图像压缩提供了新思路，尤其是GPT-4o在多模态生成方面的能力，激发了探索其在图像压缩中的应用。

Method: 研究了两类压缩范式：文本编码和多模态编码（文本+极低分辨率图像），并提出结构光栅扫描提示工程机制，将图像转化为文本空间作为生成条件。

Result: 实验表明，该方法在超低比特率下优于现有多模态/生成式图像压缩方法，展示了AIGC在图像压缩中的潜力。

Conclusion: AIGC生成在图像压缩领域具有显著潜力，未来可进一步优化生成质量和效率。

Abstract: The rapid development of AIGC foundation models has revolutionized the
paradigm of image compression, which paves the way for the abandonment of most
pixel-level transform and coding, compelling us to ask: why compress what you
can generate if the AIGC foundation model is powerful enough to faithfully
generate intricate structure and fine-grained details from nothing more than
some compact descriptors, i.e., texts, or cues. Fortunately, recent GPT-4o
image generation of OpenAI has achieved impressive cross-modality generation,
editing, and design capabilities, which motivates us to answer the above
question by exploring its potential in image compression fields. In this work,
we investigate two typical compression paradigms: textual coding and multimodal
coding (i.e., text + extremely low-resolution image), where all/most
pixel-level information is generated instead of compressing via the advanced
GPT-4o image generation function. The essential challenge lies in how to
maintain semantic and structure consistency during the decoding process. To
overcome this, we propose a structure raster-scan prompt engineering mechanism
to transform the image into textual space, which is compressed as the condition
of GPT-4o image generation. Extensive experiments have shown that the
combination of our designed structural raster-scan prompts and GPT-4o's image
generation function achieved the impressive performance compared with recent
multimodal/generative image compression at ultra-low bitrate, further
indicating the potential of AIGC generation in image compression fields.

</details>

### [206] [Early Exit and Multi Stage Knowledge Distillation in VLMs for Video Summarization](https://arxiv.org/abs/2504.21831)
*Anas Anwarul Haq Khan,Utkarsh Verma,Prateek Chanda,Ganesh Ramakrishnan*

Main category: cs.CV

TLDR: DEEVISum是一个轻量高效的视觉语言模型，用于视频摘要，结合多模态提示、多阶段知识蒸馏和早期退出技术，平衡性能与效率。


<details>
  <summary>Details</summary>
Motivation: 设计一个轻量且高效的视觉语言模型，用于视频摘要，同时保持高性能和低计算成本。

Method: 使用多模态提示（文本和音频信号）、多阶段知识蒸馏（MSKD）和早期退出（EE）技术。

Result: 在TVSum数据集上，PaLI Gemma2 3B + MSKD模型达到61.1的F1分数，性能接近更大模型，同时计算成本更低。

Conclusion: DEEVISum在性能和效率之间取得了平衡，代码和数据集已公开以支持进一步研究。

Abstract: We introduce DEEVISum (Distilled Early Exit Vision language model for
Summarization), a lightweight, efficient, and scalable vision language model
designed for segment wise video summarization. Leveraging multi modal prompts
that combine textual and audio derived signals, DEEVISum incorporates Multi
Stage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance
between performance and efficiency. MSKD offers a 1.33% absolute F1 improvement
over baseline distillation (0.5%), while EE reduces inference time by
approximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset,
our best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing
the performance of significantly larger models, all while maintaining a lower
computational footprint. We publicly release our code and processed dataset to
support further research.

</details>

### [207] [3D Stylization via Large Reconstruction Model](https://arxiv.org/abs/2504.21836)
*Ipek Oztas,Duygu Ceylan,Aysegul Dundar*

Main category: cs.CV

TLDR: 本文提出了一种无需训练或优化的3D外观风格化方法，通过注入参考图像的风格特征到3D生成模型的注意力块中，实现高效且高质量的3D风格迁移。


<details>
  <summary>Details</summary>
Motivation: 随着文本或图像引导的3D生成器的成功，用户对生成过程的控制需求增加，尤其是外观风格化。本文旨在通过参考图像实现3D资产的外观风格化，同时保持多视角的视觉一致性。

Method: 利用大型3D生成模型中特定注意力块捕获风格特征的能力，通过注入参考图像的特征到这些块中，实现3D外观风格化。

Result: 定量和定性评估表明，该方法在3D外观风格化上表现优异，显著提高了效率并保持了高质量的视觉效果。

Conclusion: 本文方法简单有效，无需额外训练或优化，为3D风格化提供了一种高效解决方案。

Abstract: With the growing success of text or image guided 3D generators, users demand
more control over the generation process, appearance stylization being one of
them. Given a reference image, this requires adapting the appearance of a
generated 3D asset to reflect the visual style of the reference while
maintaining visual consistency from multiple viewpoints. To tackle this
problem, we draw inspiration from the success of 2D stylization methods that
leverage the attention mechanisms in large image generation models to capture
and transfer visual style. In particular, we probe if large reconstruction
models, commonly used in the context of 3D generation, has a similar
capability. We discover that the certain attention blocks in these models
capture the appearance specific features. By injecting features from a visual
style image to such blocks, we develop a simple yet effective 3D appearance
stylization method. Our method does not require training or test time
optimization. Through both quantitative and qualitative evaluations, we
demonstrate that our approach achieves superior results in terms of 3D
appearance stylization, significantly improving efficiency while maintaining
high-quality visual outcomes.

</details>

### [208] [Differentiable Room Acoustic Rendering with Multi-View Vision Priors](https://arxiv.org/abs/2504.21847)
*Derong Jin,Ruohan Gao*

Main category: cs.CV

TLDR: AV-DAR框架结合视觉线索与声学束追踪，高效、可解释且准确地实现了房间声学渲染，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 空间音频对虚拟环境真实感至关重要，但现有方法要么依赖数据密集型学习模型，要么计算成本高昂。

Method: 利用多视角图像提取视觉线索，结合声学束追踪进行物理建模。

Result: 在六个真实环境中表现优异，性能接近数据量多10倍的模型，相对提升16.6%至50.9%。

Conclusion: AV-DAR为高效、准确的房间声学渲染提供了新思路。

Abstract: An immersive acoustic experience enabled by spatial audio is just as crucial
as the visual aspect in creating realistic virtual environments. However,
existing methods for room impulse response estimation rely either on
data-demanding learning-based models or computationally expensive physics-based
modeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic
Rendering (AV-DAR), a framework that leverages visual cues extracted from
multi-view images and acoustic beam tracing for physics-based room acoustic
rendering. Experiments across six real-world environments from two datasets
demonstrate that our multimodal, physics-based approach is efficient,
interpretable, and accurate, significantly outperforming a series of prior
methods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves
comparable performance to models trained on 10 times more data while delivering
relative gains ranging from 16.6% to 50.9% when trained at the same scale.

</details>

### [209] [COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning](https://arxiv.org/abs/2504.21850)
*Xindi Wu,Hee Seung Hwang,Polina Kirichenko,Olga Russakovsky*

Main category: cs.CV

TLDR: COMPACT提出了一种通过控制训练数据组合复杂度来提升多模态大语言模型在复杂任务中表现的方法，显著优于传统视觉指令调优。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在简单视觉语言任务中表现优异，但在需要多能力组合的复杂任务中表现不佳，传统视觉指令调优仅关注数据量而非组合复杂度。

Method: 提出COMPACT方法，通过生成明确控制组合复杂度的训练数据集，使模型能更高效地学习复杂能力。

Result: COMPACT在数据量仅为传统方法10%的情况下，性能相当甚至更优，尤其在需要多能力组合的复杂任务中表现突出（如MMStar提升83.3%，MM-Vet提升94.0%）。

Conclusion: COMPACT提供了一种可扩展、数据高效的视觉组合调优方案，显著提升了复杂视觉语言任务的性能。

Abstract: Multimodal Large Language Models (MLLMs) excel at simple vision-language
tasks but struggle when faced with complex tasks that require multiple
capabilities, such as simultaneously recognizing objects, counting them, and
understanding their spatial relationships. This might be partially the result
of the fact that Visual Instruction Tuning (VIT), a critical training step for
MLLMs, has traditionally focused on scaling data volume, but not the
compositional complexity of training examples. We propose COMPACT
(COMPositional Atomic-to-complex visual Capability Tuning), which generates a
training dataset explicitly controlling for the compositional complexity of the
training examples. The data from COMPACT allows MLLMs to train on combinations
of atomic capabilities to learn complex capabilities more efficiently. Across
all benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT
while using less than 10% of its data budget, and even outperforms it on
several, especially those involving complex multi-capability tasks. For
example, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%
improvement on MM-Vet compared to the full-scale VIT on particularly complex
questions that require four or more atomic capabilities. COMPACT offers a
scalable, data-efficient, visual compositional tuning recipe to improve on
complex visual-language tasks.

</details>

### [210] [A Survey of Interactive Generative Video](https://arxiv.org/abs/2504.21853)
*Jiwen Yu,Yiran Qin,Haoxuan Che,Quande Liu,Xintao Wang,Pengfei Wan,Di Zhang,Kun Gai,Hao Chen,Xihui Liu*

Main category: cs.CV

TLDR: 本文定义了交互式生成视频（IGV）技术，并探讨了其在游戏、具身AI和自动驾驶领域的应用，提出了一个包含五个模块的框架，并分析了技术挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 满足对高质量、交互式视频内容的需求，推动IGV技术的发展和应用。

Method: 通过调研IGV的三大应用领域，提出一个包含生成、控制、记忆、动态和智能五个模块的框架，并分析技术挑战。

Result: 提出了一个全面的IGV系统框架，并明确了实现理想IGV系统的技术挑战和未来方向。

Conclusion: 本文的系统分析将促进IGV技术的未来研究和开发，推动其向更复杂和实用的应用方向发展。

Abstract: Interactive Generative Video (IGV) has emerged as a crucial technology in
response to the growing demand for high-quality, interactive video content
across various domains. In this paper, we define IGV as a technology that
combines generative capabilities to produce diverse high-quality video content
with interactive features that enable user engagement through control signals
and responsive feedback. We survey the current landscape of IGV applications,
focusing on three major domains: 1) gaming, where IGV enables infinite
exploration in virtual worlds; 2) embodied AI, where IGV serves as a
physics-aware environment synthesizer for training agents in multimodal
interaction with dynamically evolving scenes; and 3) autonomous driving, where
IGV provides closed-loop simulation capabilities for safety-critical testing
and validation. To guide future development, we propose a comprehensive
framework that decomposes an ideal IGV system into five essential modules:
Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we
systematically analyze the technical challenges and future directions in
realizing each component for an ideal IGV system, such as achieving real-time
generation, enabling open-domain control, maintaining long-term coherence,
simulating accurate physics, and integrating causal reasoning. We believe that
this systematic analysis will facilitate future research and development in the
field of IGV, ultimately advancing the technology toward more sophisticated and
practical applications.

</details>

### [211] [ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction](https://arxiv.org/abs/2504.21855)
*Qihao Liu,Ju He,Qihang Yu,Liang-Chieh Chen,Alan Yuille*

Main category: cs.CV

TLDR: ReVision是一个将参数化3D物理知识集成到预训练视频生成模型中的框架，显著提升了生成复杂运动视频的能力。


<details>
  <summary>Details</summary>
Motivation: 解决视频生成中复杂运动和交互的挑战。

Method: 分三阶段：1）生成粗糙视频；2）提取2D/3D特征并优化；3）反馈优化后的运动序列生成高质量视频。

Result: 在Stable Video Diffusion上验证，ReVision显著提升运动保真度和一致性，性能优于更大模型。

Conclusion: 通过引入3D物理知识，小模型也能生成更真实可控的复杂运动视频。

Abstract: In recent years, video generation has seen significant advancements. However,
challenges still persist in generating complex motions and interactions. To
address these challenges, we introduce ReVision, a plug-and-play framework that
explicitly integrates parameterized 3D physical knowledge into a pretrained
conditional video generation model, significantly enhancing its ability to
generate high-quality videos with complex motion and interactions.
Specifically, ReVision consists of three stages. First, a video diffusion model
is used to generate a coarse video. Next, we extract a set of 2D and 3D
features from the coarse video to construct a 3D object-centric representation,
which is then refined by our proposed parameterized physical prior model to
produce an accurate 3D motion sequence. Finally, this refined motion sequence
is fed back into the same video diffusion model as additional conditioning,
enabling the generation of motion-consistent videos, even in scenarios
involving complex actions and interactions. We validate the effectiveness of
our approach on Stable Video Diffusion, where ReVision significantly improves
motion fidelity and coherence. Remarkably, with only 1.5B parameters, it even
outperforms a state-of-the-art video generation model with over 13B parameters
on complex video generation by a substantial margin. Our results suggest that,
by incorporating 3D physical knowledge, even a relatively small video diffusion
model can generate complex motions and interactions with greater realism and
controllability, offering a promising solution for physically plausible video
generation.

</details>

<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [212] [Solving Copyright Infringement on Short Video Platforms: Novel Datasets and an Audio Restoration Deep Learning Pipeline](https://arxiv.org/abs/2504.21772)
*Minwoo Oh,Minsu Park,Eunil Park*

Main category: cs.MM

TLDR: 提出了一种结合音乐源分离和跨模态视频-音乐检索的新方法，以解决短视频平台中背景音乐侵权问题，并引入了两个专用数据集。


<details>
  <summary>Details</summary>
Motivation: 短视频平台中，侵权者常通过添加背景音乐掩盖原声以逃避原创检测，导致版权问题。

Method: 整合音乐源分离（MSS）和跨模态视频-音乐检索（CMVMR）的流程，分离背景音乐并恢复原声。

Result: 实验表明，该方法能高精度去除背景音乐并恢复原声，确保内容完整性。

Conclusion: 该方法为短视频平台用户生成内容的版权问题提供了伦理和可扩展的解决方案。

Abstract: Short video platforms like YouTube Shorts and TikTok face significant
copyright compliance challenges, as infringers frequently embed arbitrary
background music (BGM) to obscure original soundtracks (OST) and evade content
originality detection. To tackle this issue, we propose a novel pipeline that
integrates Music Source Separation (MSS) and cross-modal video-music retrieval
(CMVMR). Our approach effectively separates arbitrary BGM from the original
OST, enabling the restoration of authentic video audio tracks. To support this
work, we introduce two domain-specific datasets: OASD-20K for audio separation
and OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips
featuring mixed BGM and OST pairs, while OSVAR160 is a unique benchmark dataset
comprising 1,121 video and mixed-audio pairs, specifically designed for short
video restoration tasks. Experimental results demonstrate that our pipeline not
only removes arbitrary BGM with high accuracy but also restores OSTs, ensuring
content integrity. This approach provides an ethical and scalable solution to
copyright challenges in user-generated content on short video platforms.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [213] [Transcending Dimensions using Generative AI: Real-Time 3D Model Generation in Augmented Reality](https://arxiv.org/abs/2504.21033)
*Majid Behravan,Maryam Haghani,Denis Gracanin*

Main category: cs.GR

TLDR: 结合生成式AI和AR技术，简化3D建模流程，使非专业用户也能轻松生成和操作3D模型。


<details>
  <summary>Details</summary>
Motivation: 传统3D建模技术门槛高，需专业软件和技能，限制了普通用户的使用。研究旨在通过AI和AR技术降低门槛。

Method: 利用Shap-E等AI模型和Mask R-CNN等对象检测方法，解决2D图像转3D模型的复杂问题。

Result: 35名参与者的评估显示，系统可用性评分（SUS）为69.64，频繁使用AR/VR的用户评分更高（80.71）。

Conclusion: 该系统在游戏、教育和AR电商等领域具有潜力，为非专业用户提供了直观的3D建模工具。

Abstract: Traditional 3D modeling requires technical expertise, specialized software,
and time-intensive processes, making it inaccessible for many users. Our
research aims to lower these barriers by combining generative AI and augmented
reality (AR) into a cohesive system that allows users to easily generate,
manipulate, and interact with 3D models in real time, directly within AR
environments. Utilizing cutting-edge AI models like Shap-E, we address the
complex challenges of transforming 2D images into 3D representations in AR
environments. Key challenges such as object isolation, handling intricate
backgrounds, and achieving seamless user interaction are tackled through
advanced object detection methods, such as Mask R-CNN. Evaluation results from
35 participants reveal an overall System Usability Scale (SUS) score of 69.64,
with participants who engaged with AR/VR technologies more frequently rating
the system significantly higher, at 80.71. This research is particularly
relevant for applications in gaming, education, and AR-based e-commerce,
offering intuitive, model creation for users without specialized skills.

</details>

### [214] [GauSS-MI: Gaussian Splatting Shannon Mutual Information for Active 3D Reconstruction](https://arxiv.org/abs/2504.21067)
*Yuhan Xie,Yixi Cai,Yinqiang Zhang,Lei Yang,Jia Pan*

Main category: cs.GR

TLDR: 本文提出了一种基于高斯溅射香农互信息（GauSS-MI）的实时主动视图选择方法，用于3D重建中的视觉质量不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 当前3D重建方法（如NeRF和3DGS）在图像渲染质量上有显著提升，但如何高效选择最具信息量的输入视图仍是一个挑战。现有研究多关注几何完整性，而忽视了视觉不确定性的直接评估。

Method: 提出了一种概率模型，通过香农互信息量化每个高斯的视觉不确定性，并设计了GauSS-MI准则，用于实时评估新视图的视觉互信息，以选择最佳下一视图。

Result: 在模拟和真实场景中的实验表明，所提系统在视觉质量和重建效率上表现优越。

Conclusion: GauSS-MI方法有效解决了主动3D重建中的视图选择问题，提升了视觉质量和效率。

Abstract: This research tackles the challenge of real-time active view selection and
uncertainty quantification on visual quality for active 3D reconstruction.
Visual quality is a critical aspect of 3D reconstruction. Recent advancements
such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have
notably enhanced the image rendering quality of reconstruction models.
Nonetheless, the efficient and effective acquisition of input images for
reconstruction-specifically, the selection of the most informative
viewpoint-remains an open challenge, which is crucial for active
reconstruction. Existing studies have primarily focused on evaluating geometric
completeness and exploring unobserved or unknown regions, without direct
evaluation of the visual uncertainty within the reconstruction model. To
address this gap, this paper introduces a probabilistic model that quantifies
visual uncertainty for each Gaussian. Leveraging Shannon Mutual Information, we
formulate a criterion, Gaussian Splatting Shannon Mutual Information
(GauSS-MI), for real-time assessment of visual mutual information from novel
viewpoints, facilitating the selection of next best view. GauSS-MI is
implemented within an active reconstruction system integrated with a view and
motion planner. Extensive experiments across various simulated and real-world
scenes showcase the superior visual quality and reconstruction efficiency
performance of the proposed system.

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [215] [A Memetic Algorithm based on Variational Autoencoder for Black-Box Discrete Optimization with Epistasis among Parameters](https://arxiv.org/abs/2504.21338)
*Aoi Kato,Kenta Kojima,Masahiro Nomura,Isao Ono*

Main category: cs.NE

TLDR: 提出了一种结合VAE采样和局部搜索的新模因算法，用于解决高维黑盒离散优化问题，并在NK景观上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决黑盒离散优化问题中参数间相互作用的挑战，结合VAE和局部搜索的优势。

Method: 结合VAE采样和局部搜索的模因算法，继承VAE和局部搜索的优点。

Result: 在NK景观上表现优于现有VAE-based EDA方法及P3、DSMGA-II等领先方法。

Conclusion: 新方法有效处理高维问题且计算开销低，优于现有技术。

Abstract: Black-box discrete optimization (BB-DO) problems arise in many real-world
applications, such as neural architecture search and mathematical model
estimation. A key challenge in BB-DO is epistasis among parameters where
multiple variables must be modified simultaneously to effectively improve the
objective function. Estimation of Distribution Algorithms (EDAs) provide a
powerful framework for tackling BB-DO problems. In particular, an EDA
leveraging a Variational Autoencoder (VAE) has demonstrated strong performance
on relatively low-dimensional problems with epistasis while reducing
computational cost. Meanwhile, evolutionary algorithms such as DSMGA-II and P3,
which integrate bit-flip-based local search with linkage learning, have shown
excellent performance on high-dimensional problems. In this study, we propose a
new memetic algorithm that combines VAE-based sampling with local search. The
proposed method inherits the strengths of both VAE-based EDAs and local
search-based approaches: it effectively handles high-dimensional problems with
epistasis among parameters without incurring excessive computational overhead.
Experiments on NK landscapes -- a challenging benchmark for BB-DO involving
epistasis among parameters -- demonstrate that our method outperforms
state-of-the-art VAE-based EDA methods, as well as leading approaches such as
P3 and DSMGA-II.

</details>

### [216] [Meta knowledge assisted Evolutionary Neural Architecture Search](https://arxiv.org/abs/2504.21545)
*Yangyang Li,Guanlong Liu,Ronghua Shang,Licheng Jiao*

Main category: cs.NE

TLDR: 本文提出了一种基于进化计算的神经架构搜索方法，通过元学习框架解决高计算成本和固定学习率问题，实现了高效且鲁棒的架构搜索。


<details>
  <summary>Details</summary>
Motivation: 解决进化计算在神经架构搜索中高计算成本和固定学习率导致的信息损失问题。

Method: 采用元学习率方案预训练获取合适的学习率计划，设计自适应代理模型筛选潜在架构，并引入周期性变异算子增加种群多样性。

Result: 在CIFAR-10、CIFAR-100和ImageNet1K数据集上表现优异，计算成本低且鲁棒性强。

Conclusion: 该方法在性能和效率上均优于现有方法，具有实际应用潜力。

Abstract: Evolutionary computation (EC)-based neural architecture search (NAS) has
achieved remarkable performance in the automatic design of neural
architectures. However, the high computational cost associated with evaluating
searched architectures poses a challenge for these methods, and a fixed form of
learning rate (LR) schedule means greater information loss on diverse searched
architectures. This paper introduces an efficient EC-based NAS method to solve
these problems via an innovative meta-learning framework. Specifically, a
meta-learning-rate (Meta-LR) scheme is used through pretraining to obtain a
suitable LR schedule, which guides the training process with lower information
loss when evaluating each individual. An adaptive surrogate model is designed
through an adaptive threshold to select the potential architectures in a few
epochs and then evaluate the potential architectures with complete epochs.
Additionally, a periodic mutation operator is proposed to increase the
diversity of the population, which enhances the generalizability and
robustness. Experiments on CIFAR-10, CIFAR-100, and ImageNet1K datasets
demonstrate that the proposed method achieves high performance comparable to
that of many state-of-the-art peer methods, with lower computational cost and
greater robustness.

</details>

<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [217] [Redundancy Analysis and Mitigation for Machine Learning-Based Process Monitoring of Additive Manufacturing](https://arxiv.org/abs/2504.21317)
*Jiarui Xie,Yaoyao Fiona Zhao*

Main category: cs.CE

TLDR: 论文提出了一种多级冗余缓解（MLRM）框架，用于解决机器学习在增材制造过程监控中的冗余问题，显著提升了性能并降低了成本。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对冗余的统一定义和系统性缓解方法，导致设备成本高、模型性能下降和计算需求大。

Method: 定义了样本级、特征级和模型级冗余，并提出MLRM框架，结合数据注册、降维、跨模态知识迁移和模型剪枝等方法。

Result: 在定向能量沉积（DED）的缺陷检测案例中，延迟降低91%，错误率减少47%，存储需求降低99.4%。

Conclusion: 通过定义冗余并提出系统性缓解框架，该研究为高效机器学习监控系统提供了关键支持。

Abstract: The deployment of machine learning (ML)-based process monitoring systems has
significantly advanced additive manufacturing (AM) by enabling real-time defect
detection, quality assessment, and process optimization. However, redundancy is
a critical yet often overlooked challenge in the deployment and operation of
ML-based AM process monitoring systems. Excessive redundancy leads to increased
equipment costs, compromised model performance, and high computational
requirements, posing barriers to industrial adoption. However, existing
research lacks a unified definition of redundancy and a systematic framework
for its evaluation and mitigation. This paper defines redundancy in ML-based AM
process monitoring and categorizes it into sample-level, feature-level, and
model-level redundancy. A comprehensive multi-level redundancy mitigation
(MLRM) framework is proposed, incorporating advanced methods such as data
registration, downscaling, cross-modality knowledge transfer, and model pruning
to systematically reduce redundancy while improving model performance. The
framework is validated through an ML-based in-situ defect detection case study
for directed energy deposition (DED), demonstrating a 91% reduction in latency,
a 47% decrease in error rate, and a 99.4% reduction in storage requirements.
Additionally, the proposed approach lowers sensor costs and energy consumption,
enabling a lightweight, cost-effective, and scalable monitoring system. By
defining redundancy and introducing a structured mitigation framework, this
study establishes redundancy analysis and mitigation as a key enabler of
efficient ML-based process monitoring in production environments.

</details>

<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [218] [Galvatron: An Automatic Distributed System for Efficient Foundation Model Training](https://arxiv.org/abs/2504.21411)
*Xinyi Liu,Yujie Wang,Shenhan Zhu,Fangcheng Fu,Qingshuo Liu,Guangming Lin,Bin Cui*

Main category: cs.DC

TLDR: Galvatron是一个分布式系统，用于高效训练大规模基础模型，通过自动选择最优并行策略提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决大规模基础模型训练中并行策略选择的复杂性，提高训练效率。

Method: 结合数据、张量、流水线、分片数据和序列并行以及重计算，通过分析硬件和模型的性能分析器、基于决策树和动态规划的搜索引擎以及运行时执行器实现。

Result: 在多种集群上表现出优于现有框架的吞吐量。

Conclusion: Galvatron通过开源、用户友好的接口和全面文档，使复杂分布式训练更高效和易用。

Abstract: Galvatron is a distributed system for efficiently training large-scale
Foundation Models. It overcomes the complexities of selecting optimal
parallelism strategies by automatically identifying the most efficient hybrid
strategy, incorporating data, tensor, pipeline, sharded data, and sequence
parallelism, along with recomputation. The system's architecture includes a
profiler for hardware and model analysis, a search engine for strategy
optimization using decision trees and dynamic programming, and a runtime for
executing these strategies efficiently. Benchmarking on various clusters
demonstrates Galvatron's superior throughput compared to existing frameworks.
This open-source system offers user-friendly interfaces and comprehensive
documentation, making complex distributed training accessible and efficient.
The source code of Galvatron is available at
https://github.com/PKU-DAIR/Hetu-Galvatron.

</details>

<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [219] [Scalable Multi-Task Learning for Particle Collision Event Reconstruction with Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2504.21844)
*William Sutcliffe,Marta Calvi,Simone Capelli,Jonas Eschle,Julián García Pardiñas,Abhijit Mathad,Azusa Uzuki,Nicola Serra*

Main category: physics.data-an

TLDR: 提出了一种新型异构图神经网络（HGNN）架构，用于解决大型强子对撞机（LHC）中粒子碰撞事件重建的挑战，显著提升了美丽强子重建性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型强子对撞机亮度前沿的发展，粒子碰撞事件的重建和分析面临挑战，包括高粒子多重性、延迟和存储需求增加，以及背景水平升高和顶点错误关联等问题。

Method: 采用异构图神经网络（HGNN）架构，结合独特的粒子碰撞关系表示和集成图剪枝层，通过多任务训练范式在模拟LHCb实验环境中进行训练。

Result: HGNN显著提升了美丽强子重建性能，同时在一个框架内完成粒子顶点关联和图剪枝，并展示了推理时间随事件复杂度的优化。

Conclusion: 该HGNN架构为粒子碰撞事件的重建提供了更全面和可扩展的解决方案，并通过加权消息传递方案减少了性能损失。

Abstract: The growing luminosity frontier at the Large Hadron Collider is challenging
the reconstruction and analysis of particle collision events. Increased
particle multiplicities are straining latency and storage requirements at the
data acquisition stage, while new complications are emerging, including higher
background levels and more frequent particle vertex misassociations. This in
turn necessitates the development of more holistic and scalable reconstruction
methods that take advantage of recent advances in machine learning. We propose
a novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique
representations for diverse particle collision relationships and integrated
graph pruning layers for scalability. Trained with a multi-task paradigm in an
environment mimicking the LHCb experiment, this HGNN significantly improves
beauty hadron reconstruction performance. Notably, it concurrently performs
particle vertex association and graph pruning within a single framework. We
quantify reconstruction and pruning performance, demonstrate enhanced inference
time scaling with event complexity, and mitigate potential performance loss
using a weighted message passing scheme.

</details>

<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [220] [Advancing Multi-Agent Systems Through Model Context Protocol: Architecture, Implementation, and Applications](https://arxiv.org/abs/2504.21030)
*Naveen Krishnan*

Main category: cs.MA

TLDR: 本文提出了一种基于模型上下文协议（MCP）的框架，用于解决多智能体系统在上下文管理、协调效率和可扩展性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂问题解决中具有潜力，但面临上下文管理、协调效率和可扩展性等挑战，需要一种标准化方法。

Method: 通过统一的MCP框架，结合先进上下文管理技术和可扩展协调模式，扩展了现有AI智能体架构。

Result: 在多个领域的案例研究中，与传统方法相比，性能显著提升，并提供了系统评估框架。

Conclusion: 该研究为构建更强大、协作和上下文感知的AI系统提供了基础，并指出了未来研究方向和应用潜力。

Abstract: Multi-agent systems represent a significant advancement in artificial
intelligence, enabling complex problem-solving through coordinated specialized
agents. However, these systems face fundamental challenges in context
management, coordination efficiency, and scalable operation. This paper
introduces a comprehensive framework for advancing multi-agent systems through
Model Context Protocol (MCP), addressing these challenges through standardized
context sharing and coordination mechanisms. We extend previous work on AI
agent architectures by developing a unified theoretical foundation, advanced
context management techniques, and scalable coordination patterns. Through
detailed implementation case studies across enterprise knowledge management,
collaborative research, and distributed problem-solving domains, we demonstrate
significant performance improvements compared to traditional approaches. Our
evaluation methodology provides a systematic assessment framework with
benchmark tasks and datasets specifically designed for multi-agent systems. We
identify current limitations, emerging research opportunities, and potential
transformative applications across industries. This work contributes to the
evolution of more capable, collaborative, and context-aware artificial
intelligence systems that can effectively address complex real-world
challenges.

</details>

### [221] [Multi-Agent Reinforcement Learning for Resources Allocation Optimization: A Survey](https://arxiv.org/abs/2504.21048)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Jimmy Cao,Ryszard Kowalczyk*

Main category: cs.MA

TLDR: 本文综述了多智能体强化学习（MARL）在资源分配优化（RAO）中的应用，总结了核心概念、分类和结构化分类法，并指出了当前研究的主要挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 资源分配优化在动态和去中心化环境中具有重要价值，而MARL能够有效解决此类问题，因此研究MARL在RAO中的应用具有重要意义。

Method: 通过综述近期MARL算法，涵盖核心概念、分类和结构化分类法，分析其在RAO中的应用。

Result: 总结了MARL在RAO中的研究现状，并识别了主要挑战和未来发展方向。

Conclusion: 本文为研究者和实践者提供了利用MARL优化资源分配的参考，推动了相关领域的发展。

Abstract: Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for
numerous real-world applications, modeling distributed decision-making and
learning from interactions with complex environments. Resource Allocation
Optimization (RAO) benefits significantly from MARL's ability to tackle dynamic
and decentralized contexts. MARL-based approaches are increasingly applied to
RAO challenges across sectors playing pivotal roles to Industry 4.0
developments. This survey provides a comprehensive review of recent MARL
algorithms for RAO, encompassing core concepts, classifications, and a
structured taxonomy. By outlining the current research landscape and
identifying primary challenges and future directions, this survey aims to
support researchers and practitioners in leveraging MARL's potential to advance
resource allocation solutions.

</details>

### [222] [MF-LLM: Simulating Collective Decision Dynamics via a Mean-Field Large Language Model Framework](https://arxiv.org/abs/2504.21582)
*Qirui Mi,Mengyue Yang,Xiangning Yu,Zhiyu Zhao,Cheng Deng,Bo An,Haifeng Zhang,Xu Chen,Jun Wang*

Main category: cs.MA

TLDR: MF-LLM框架通过微观决策与宏观群体的反馈循环模拟集体决策，结合IB-Tune微调方法，显著提升了与真实数据的匹配度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在模拟集体决策时与真实数据存在偏差，需改进模型以更准确地反映动态交互。

Method: 提出MF-LLM框架，交替使用策略模型和平均场模型，并引入基于信息瓶颈的IB-Tune微调方法。

Result: 在真实数据集上，MF-LLM将KL散度降低47%，支持准确的趋势预测和干预规划。

Conclusion: MF-LLM为高保真社会模拟提供了可扩展的基础，适用于多种领域和LLM架构。

Abstract: Simulating collective decision-making involves more than aggregating
individual behaviors; it arises from dynamic interactions among individuals.
While large language models (LLMs) show promise for social simulation, existing
approaches often exhibit deviations from real-world data. To address this gap,
we propose the Mean-Field LLM (MF-LLM) framework, which explicitly models the
feedback loop between micro-level decisions and macro-level population. MF-LLM
alternates between two models: a policy model that generates individual actions
based on personal states and group-level information, and a mean field model
that updates the population distribution from the latest individual decisions.
Together, they produce rollouts that simulate the evolving trajectories of
collective decision-making. To better match real-world data, we introduce
IB-Tune, a fine-tuning method for LLMs grounded in the information bottleneck
principle, which maximizes the relevance of population distributions to future
actions while minimizing redundancy with historical data. We evaluate MF-LLM on
a real-world social dataset, where it reduces KL divergence to human population
distributions by 47 percent over non-mean-field baselines, and enables accurate
trend forecasting and intervention planning. It generalizes across seven
domains and four LLM backbones, providing a scalable foundation for
high-fidelity social simulation.

</details>

<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [223] [Evaluation and Verification of Physics-Informed Neural Models of the Grad-Shafranov Equation](https://arxiv.org/abs/2504.21155)
*Fauzan Nazranda Rizqa,Matthew Hole,Charles Gretton*

Main category: physics.plasm-ph

TLDR: 论文研究了在轴对称托卡马克反应堆中，利用物理信息神经网络（PINN）建模Grad-Shafranov方程（GSE）的潜力，并比较了PINN与傅里叶神经算子（FNO）的性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于聚变反应堆中磁流体动力学（MHD）平衡的需求，特别是轴对称托卡马克反应堆中GSE的建模问题。现有研究未探讨网络对多种边界条件的泛化能力。

Method: 提出了一种将边界点作为网络输入的PINN架构，并比较了PINN与FNO模型的精度和推理速度。使用Marabou工具进行网络验证。

Result: 发现PINN模型在性能和精度上表现最佳，并展示了实用的验证流程。尽管在PyTorch和Marabou之间存在一些差异，但仍能有效验证网络。

Conclusion: 本研究首次探讨了此类网络的验证问题，证明了PINN在GSE建模中的潜力，并提供了实用的验证方法。

Abstract: Our contributions are motivated by fusion reactors that rely on maintaining
magnetohydrodynamic (MHD) equilibrium, where the balance between plasma
pressure and confining magnetic fields is required for stable operation. In
axisymmetric tokamak reactors in particular, and under the assumption of
toroidal symmetry, this equilibrium can be mathematically modelled using the
Grad-Shafranov Equation (GSE). Recent works have demonstrated the potential of
using Physics-Informed Neural Networks (PINNs) to model the GSE. Existing
studies did not examine realistic scenarios in which a single network
generalizes to a variety of boundary conditions. Addressing that limitation, we
evaluate a PINN architecture that incorporates boundary points as network
inputs. Additionally, we compare PINN model accuracy and inference speeds with
a Fourier Neural Operator (FNO) model. Finding the PINN model to be the most
performant, and accurate in our setting, we use the network verification tool
Marabou to perform a range of verification tasks. Although we find some
discrepancies between evaluations of the networks natively in PyTorch, compared
to via Marabou, we are able to demonstrate useful and practical verification
workflows. Our study is the first investigation of verification of such
networks.

</details>

<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [224] [Data-driven operator learning for energy-efficient building control](https://arxiv.org/abs/2504.21243)
*Yuexin Bian,Yuanyuan Shi*

Main category: eess.SY

TLDR: 提出了一种结合CFD物理精度与机器学习计算效率的数据驱动框架，用于建筑通风控制，显著节能且保持空气质量。


<details>
  <summary>Details</summary>
Motivation: 传统CFD模拟计算成本高，难以实时应用于建筑管理系统，需高效替代方案。

Method: 训练神经算子变换器，从CFD数据学习控制动作与气流分布的映射，实现梯度优化控制。

Result: 相比最大气流控制、规则控制和基于区域CO2预测的数据驱动控制，显著节能且保持空气质量。

Conclusion: 该方法实用且可扩展，适用于安全节能的建筑管理。

Abstract: Energy-efficient ventilation control plays a vital role in reducing building
energy consumption while ensuring occupant health and comfort. While
Computational Fluid Dynamics (CFD) simulations offer high-fidelity modeling of
airflow for building HVAC design, their high computational cost makes them
impractical for practical adoption in real-time building management system. In
this work, we present a data-driven framework that combines the physical
accuracy of CFD with the computational efficiency of machine learning to enable
energy-efficient building ventilation control. Our method jointly optimizes
airflow supply rates and vent angles to reduce energy use and adhere to air
quality constraints. We train a neural operator transformer to learn the
mapping from building control actions to airflow field distributions using
high-resolution CFD data. This learned operator enables a gradient-based
control framework capable of optimal decision-making. Experimental results
demonstrate that our approach achieves substantial energy savings compared to
maximum airflow rate control, rule-based control, and data-driven control based
on regional average CO2 predictions, while consistently maintaining safe indoor
air quality. These results highlight the practicality and scalability of our
method for enabling safe and energy-efficient building management.

</details>

### [225] [Power Flow Approximations for Multiphase Distribution Networks using Gaussian Processes](https://arxiv.org/abs/2504.21260)
*Daniel Glover,Parikshit Pareek,Deepjyoti Deka,Anamika Dubey*

Main category: eess.SY

TLDR: 该论文提出了一种基于高斯过程（GPs）的数据驱动方法，用于近似多相潮流模型，展示了其在少量训练数据下仍能可靠预测非线性潮流解的能力。


<details>
  <summary>Details</summary>
Motivation: 基于模型的学习方法在数据效率和鲁棒性上优于无模型方法，但需要有效的潮流模型近似器。本研究旨在填补这一空白。

Method: 采用高斯过程（GPs）作为数据驱动的潮流模型近似器，将净负荷注入映射到节点电压。

Result: 在IEEE 123总线和8500节点测试中，GP模型在少量训练数据下表现优异，训练效率高且误差显著低于深度神经网络。

Conclusion: GP模型在数据效率和预测精度上具有显著优势，适用于实际电网操作。

Abstract: Learning-based approaches are increasingly leveraged to manage and coordinate
the operation of grid-edge resources in active power distribution networks.
Among these, model-based techniques stand out for their superior data
efficiency and robustness compared to model-free methods. However, effective
model learning requires a learning-based approximator for the underlying power
flow model. This study extends existing work by introducing a data-driven power
flow method based on Gaussian Processes (GPs) to approximate the multiphase
power flow model, by mapping net load injections to nodal voltages. Simulation
results using the IEEE 123-bus and 8500-node distribution test feeders
demonstrate that the trained GP model can reliably predict the nonlinear power
flow solutions with minimal training data. We also conduct a comparative
analysis of the training efficiency and testing performance of the proposed
GP-based power flow approximator against a deep neural network-based
approximator, highlighting the advantages of our data-efficient approach.
Results over realistic operating conditions show that despite an 85% reduction
in the training sample size (corresponding to a 92.8% improvement in training
time), GP models produce a 99.9% relative reduction in mean absolute error
compared to the baselines of deep neural networks.

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [226] [DGFNet: End-to-End Audio-Visual Source Separation Based on Dynamic Gating Fusion](https://arxiv.org/abs/2504.21366)
*Yinfeng Yu,Shiyu Sun*

Main category: cs.SD

TLDR: 本文提出了一种基于门控机制的动态融合方法，解决了音频-视觉源分离中模态融合不足或过度的问题，并引入了音频注意力模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉源分离方法在模态融合时存在信息丢失或交互不足的问题，导致性能受限。

Method: 采用动态门控机制调整模态融合程度，并引入音频注意力模块增强音频特征表达。

Result: 在两个基准数据集上取得了显著性能提升。

Conclusion: 该方法有效解决了模态融合问题，提升了音频-视觉源分离任务的性能。

Abstract: Current Audio-Visual Source Separation methods primarily adopt two design
strategies. The first strategy involves fusing audio and visual features at the
bottleneck layer of the encoder, followed by processing the fused features
through the decoder. However, when there is a significant disparity between the
two modalities, this approach may lead to the loss of critical information. The
second strategy avoids direct fusion and instead relies on the decoder to
handle the interaction between audio and visual features. Nonetheless, if the
encoder fails to integrate information across modalities adequately, the
decoder may be unable to effectively capture the complex relationships between
them. To address these issues, this paper proposes a dynamic fusion method
based on a gating mechanism that dynamically adjusts the modality fusion
degree. This approach mitigates the limitations of solely relying on the
decoder and facilitates efficient collaboration between audio and visual
features. Additionally, an audio attention module is introduced to enhance the
expressive capacity of audio features, thereby further improving model
performance. Experimental results demonstrate that our method achieves
significant performance improvements on two benchmark datasets, validating its
effectiveness and advantages in Audio-Visual Source Separation tasks.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [227] [Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval](https://arxiv.org/abs/2504.21015)
*Aarush Sinha*

Main category: cs.IR

TLDR: 论文提出了一种基于LLM的端到端管道，通过生成查询和硬负例来替代传统依赖BM25或交叉编码器的硬负例挖掘方法，效果相当但更高效。


<details>
  <summary>Details</summary>
Motivation: 传统硬负例挖掘方法（如BM25或交叉编码器）计算成本高且需要完整语料库访问，限制了效率。

Method: 使用LLM生成查询，并仅基于查询文本生成硬负例，形成端到端的语料库无关管道。

Result: 实验表明，提出的LLM管道在多个BEIR基准数据集上的性能与BM25和交叉编码器基线相当。

Conclusion: 语料库无关的硬负例生成方法效果与传统方法相同，但更简单高效，为高性能检索模型训练提供了新途径。

Abstract: Training effective dense retrieval models often relies on hard negative (HN)
examples mined from the document corpus via methods like BM25 or cross-encoders
(CE), processes that can be computationally demanding and require full corpus
access. This paper introduces a different approach, an end-to-end pipeline
where a Large Language Model (LLM) first generates a query from a passage, and
then generates a hard negative example using \emph{only} that query text. This
corpus-free negative generation contrasts with standard mining techniques. We
evaluated this \textsc{LLM Query $\rightarrow$ LLM HN} approach against
traditional \textsc{LLM Query $\rightarrow$ BM25 HN} and \textsc{LLM Query
$\rightarrow$ CE HN} pipelines using E5-Base and GTE-Base models on several
BEIR benchmark datasets. Our results show the proposed all-LLM pipeline
achieves performance identical to both the BM25 and the computationally
intensive CE baselines across nDCG@10, Precision@10, and Recall@100 metrics.
This demonstrates that our corpus-free negative generation method matches the
effectiveness of complex, corpus-dependent mining techniques, offering a
potentially simpler and more efficient pathway for training high-performance
retrievers without sacrificing results. We make the dataset including the
queries and the hard-negatives for all three methods publicly available
https://huggingface.co/collections/chungimungi/arxiv-hard-negatives-68027bbc601ff6cc8eb1f449.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [228] [Generate-then-Verify: Reconstructing Data from Limited Published Statistics](https://arxiv.org/abs/2504.21199)
*Terrance Liu,Eileen Xiao,Pratiksha Thaker,Adam Smith,Zhiwei Steven Wu*

Main category: stat.ML

TLDR: 研究如何从聚合统计数据中部分重建表格数据，提出一种新方法，确保重建的子集完全正确。


<details>
  <summary>Details</summary>
Motivation: 现有方法在数据稀疏时无法完全重建数据集，因此需要一种能保证部分数据正确性的方法。

Method: 引入整数规划方法，生成并验证与聚合数据一致的所有可能数据集中的子集。

Result: 在美国人口普查数据上验证，证明即使数据稀疏，隐私泄露仍可能发生。

Conclusion: 部分数据重建方法在稀疏数据下仍能有效识别隐私泄露风险。

Abstract: We study the problem of reconstructing tabular data from aggregate
statistics, in which the attacker aims to identify interesting claims about the
sensitive data that can be verified with 100% certainty given the aggregates.
Successful attempts in prior work have conducted studies in settings where the
set of published statistics is rich enough that entire datasets can be
reconstructed with certainty. In our work, we instead focus on the regime where
many possible datasets match the published statistics, making it impossible to
reconstruct the entire private dataset perfectly (i.e., when approaches in
prior work fail). We propose the problem of partial data reconstruction, in
which the goal of the adversary is to instead output a $\textit{subset}$ of
rows and/or columns that are $\textit{guaranteed to be correct}$. We introduce
a novel integer programming approach that first $\textbf{generates}$ a set of
claims and then $\textbf{verifies}$ whether each claim holds for all possible
datasets consistent with the published aggregates. We evaluate our approach on
the housing-level microdata from the U.S. Decennial Census release,
demonstrating that privacy violations can still persist even when information
published about such data is relatively sparse.

</details>

### [229] [Kernel Density Machines](https://arxiv.org/abs/2504.21419)
*Damir Filipovic,Paul Schneider*

Main category: stat.ML

TLDR: Kernel density machines (KDM) 是一种新型密度比估计器，适用于可数生成可测空间上的概率测度，无需连续性或 Lebesgue 密度假设。通过低秩近似实现计算高效性，并提供理论保证和实证验证。


<details>
  <summary>Details</summary>
Motivation: 提出 KDM 是为了在更广泛的概率测度设置中高效估计密度比，避免传统方法的限制性假设。

Method: 在再生核希尔伯特空间框架下，结合低秩近似技术，控制误差以实现大规模样本的可扩展性。

Result: 理论保证包括渐近一致性、泛函中心极限定理和有限样本误差界，实证结果验证了其有效性和精确性。

Conclusion: KDM 为密度比估计提供了强大的理论和实践基础，适用于广泛的应用场景。

Abstract: We introduce kernel density machines (KDM), a novel density ratio estimator
in a reproducing kernel Hilbert space setting. KDM applies to general
probability measures on countably generated measurable spaces without
restrictive assumptions on continuity, or the existence of a Lebesgue density.
For computational efficiency, we incorporate a low-rank approximation with
precisely controlled error that grants scalability to large-sample settings. We
provide rigorous theoretical guarantees, including asymptotic consistency, a
functional central limit theorem, and finite-sample error bounds, establishing
a strong foundation for practical use. Empirical results based on simulated and
real data demonstrate the efficacy and precision of KDM.

</details>

### [230] [Wasserstein-Aitchison GAN for angular measures of multivariate extremes](https://arxiv.org/abs/2504.21438)
*Stéphane Lhaut,Holger Rootzén,Johan Segers*

Main category: stat.ML

TLDR: 本文提出了一种名为WA-GAN的新方法，用于模拟多维极端事件的未来值，以估计其发生概率。该方法结合了极值分析和生成对抗网络，在模拟极端事件时表现出色。


<details>
  <summary>Details</summary>
Motivation: 经济上负责任地缓解多维极端风险（如极端降雨、股票价格大幅波动等）需要估计这些风险未来发生的概率。现有方法在捕捉极端事件的依赖结构方面存在不足。

Method: WA-GAN方法将观测值转换为单位帕累托尺度，假设其分布是规则变化的。通过极值分析建模边缘分布的尾部，并结合非参数GAN建模角度分布。角度值被转换为Aitchison坐标，用Wasserstein GAN生成新值。

Result: WA-GAN在捕捉极端事件的依赖结构和生成准确的新极端值方面优于现有方法，验证了其在高达50维的模拟数据和30维金融数据上的有效性。

Conclusion: WA-GAN为多维极端事件的概率估计提供了一种高效且准确的新方法，适用于高维数据。

Abstract: Economically responsible mitigation of multivariate extreme risks -- extreme
rainfall in a large area, huge variations of many stock prices, widespread
breakdowns in transportation systems -- requires estimates of the probabilities
that such risks will materialize in the future. This paper develops a new
method, Wasserstein--Aitchison Generative Adversarial Networks (WA-GAN), which
provides simulated values of future $d$-dimensional multivariate extreme events
and which hence can be used to give estimates of such probabilities. The main
hypothesis is that, after transforming the observations to the unit-Pareto
scale, their distribution is regularly varying in the sense that the
distributions of their radial and angular components (with respect to the
$L_1$-norm) converge and become asymptotically independent as the radius gets
large. The method is a combination of standard extreme value analysis modeling
of the tails of the marginal distributions with nonparametric GAN modeling of
the angular distribution. For the latter, the angular values are transformed to
Aitchison coordinates in a full $(d-1)$-dimensional linear space, and a
Wasserstein GAN is trained on these coordinates and used to generate new
values. A reverse transformation is then applied to these values and gives
simulated values on the original data scale. The method shows good performance
compared to other existing methods in the literature, both in terms of
capturing the dependence structure of the extremes in the data, as well as in
generating accurate new extremes of the data distribution. The comparison is
performed on simulated multivariate extremes from a logistic model in
dimensions up to 50 and on a 30-dimensional financial data set.

</details>

### [231] [A comparison of generative deep learning methods for multivariate angular simulation](https://arxiv.org/abs/2504.21505)
*Jakob Benjamin Wessel,Callum J. R. Murphy-Barltrop,Emma S. Simpson*

Main category: stat.ML

TLDR: 论文探讨了在多元极值分析中，利用深度学习（如生成对抗网络、标准化流和流匹配）模拟高维角度变量的方法，并与传统的vMF混合模型进行了比较。


<details>
  <summary>Details</summary>
Motivation: 随着多元极值分析中几何和角度-径向框架的发展，高维角度变量的可靠模拟变得日益重要。传统方法在低维表现良好，但在高维缺乏灵活性和可扩展性。

Method: 研究了多种深度学习方法（生成对抗网络、标准化流和流匹配）用于模拟角度变量，并与vMF混合模型进行了比较。

Result: 通过多种指标评估了深度学习方法的表现，并在实际气象海洋数据集上验证了其适用性。

Conclusion: 深度学习方法在模拟复杂角度变量结构方面具有潜力，尤其是在高维情况下优于传统方法。

Abstract: With the recent development of new geometric and angular-radial frameworks
for multivariate extremes, reliably simulating from angular variables in
moderate-to-high dimensions is of increasing importance. Empirical approaches
have the benefit of simplicity, and work reasonably well in low dimensions, but
as the number of variables increases, they can lack the required flexibility
and scalability. Classical parametric models for angular variables, such as the
von Mises-Fisher (vMF) distribution, provide an alternative. Exploiting
mixtures of vMF distributions increases their flexibility, but there are cases
where even this is not sufficient to capture the intricate features that can
arise in data. Owing to their flexibility, generative deep learning methods are
able to capture complex data structures; they therefore have the potential to
be useful in the simulation of angular variables. In this paper, we explore a
range of deep learning approaches for this task, including generative
adversarial networks, normalizing flows and flow matching. We assess their
performance via a range of metrics and make comparisons to the more classical
approach of using a mixture of vMF distributions. The methods are also applied
to a metocean data set, demonstrating their applicability to real-world,
complex data structures.

</details>

### [232] [Balancing Interpretability and Flexibility in Modeling Diagnostic Trajectories with an Embedded Neural Hawkes Process Model](https://arxiv.org/abs/2504.21795)
*Yuankang Zhao,Matthew Engelhard*

Main category: stat.ML

TLDR: 论文提出了一种新型霍克斯过程（HP）模型，通过事件嵌入空间中的灵活影响核（神经网络实现）来平衡灵活性和可解释性，适用于大规模事件序列。


<details>
  <summary>Details</summary>
Motivation: 传统HP模型通过参数化影响函数实现自增强动态，但灵活性不足；神经网络HP虽灵活但牺牲了可解释性，而可解释性在医疗等领域至关重要。

Method: 提出了一种新型HP模型，将影响函数建模为事件嵌入空间中的灵活核（神经网络），并通过添加Transformer层进一步优化事件嵌入的上下文。

Result: 实验表明，该方法能准确模拟影响函数，在MIMIC-IV数据集上表现优异，并在XX-EHR儿童诊断数据中保持临床可解释性。

Conclusion: 灵活的核设计能在不损失性能的情况下保持可解释性，适用于电子健康记录等数据。

Abstract: The Hawkes process (HP) is commonly used to model event sequences with
self-reinforcing dynamics, including electronic health records (EHRs).
Traditional HPs capture self-reinforcement via parametric impact functions that
can be inspected to understand how each event modulates the intensity of
others. Neural network-based HPs offer greater flexibility, resulting in
improved fit and prediction performance, but at the cost of interpretability,
which is often critical in healthcare. In this work, we aim to understand and
improve upon this tradeoff. We propose a novel HP formulation in which impact
functions are modeled by defining a flexible impact kernel, instantiated as a
neural network, in event embedding space, which allows us to model large-scale
event sequences with many event types. This approach is more flexible than
traditional HPs yet more interpretable than other neural network approaches,
and allows us to explicitly trade flexibility for interpretability by adding
transformer encoder layers to further contextualize the event embeddings.
Results show that our method accurately recovers impact functions in
simulations, achieves competitive performance on MIMIC-IV procedure dataset,
and gains clinically meaningful interpretation on XX-EHR with children
diagnosis dataset even without transformer layers. This suggests that our
flexible impact kernel is often sufficient to capture self-reinforcing dynamics
in EHRs and other data effectively, implying that interpretability can be
maintained without loss of performance.

</details>

<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [233] [Who Gets the Callback? Generative AI and Gender Bias](https://arxiv.org/abs/2504.21400)
*Sugat Chaturvedi,Rochana Chaturvedi*

Main category: econ.GN

TLDR: 论文研究了开源大型语言模型在招聘中的性别偏见，发现模型倾向于推荐男性，尤其是在高薪职位中，且推荐行为与传统性别刻板印象一致。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI（特别是大型语言模型）在招聘中的性别偏见问题，揭示AI驱动招聘可能加剧劳动力市场中的偏见。

Method: 使用332,044个真实在线招聘广告数据集，通过模型推荐面试候选人，分析性别偏见；结合职业分类系统和语言特征分析。

Result: 大多数模型倾向于推荐男性，尤其是高薪职位；女性在男性主导职业中回调率较低，而在女性相关职业中较高。

Conclusion: AI招聘可能延续劳动力市场中的偏见，影响公平性和多样性；模型行为与招聘者身份相关，低宜人性人格可减少刻板印象。

Abstract: Generative artificial intelligence (AI), particularly large language models
(LLMs), is being rapidly deployed in recruitment and for candidate
shortlisting. We audit several mid-sized open-source LLMs for gender bias using
a dataset of 332,044 real-world online job postings. For each posting, we
prompt the model to recommend whether an equally qualified male or female
candidate should receive an interview callback. We find that most models tend
to favor men, especially for higher-wage roles. Mapping job descriptions to the
Standard Occupational Classification system, we find lower callback rates for
women in male-dominated occupations and higher rates in female-associated ones,
indicating occupational segregation. A comprehensive analysis of linguistic
features in job ads reveals strong alignment of model recommendations with
traditional gender stereotypes. To examine the role of recruiter identity, we
steer model behavior by infusing Big Five personality traits and simulating the
perspectives of historical figures. We find that less agreeable personas reduce
stereotyping, consistent with an agreeableness bias in LLMs. Our findings
highlight how AI-driven hiring may perpetuate biases in the labor market and
have implications for fairness and diversity within firms.

</details>

<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [234] [Estimation of discrete distributions in relative entropy, and the deviations of the missing mass](https://arxiv.org/abs/2504.21787)
*Jaouad Mourtada*

Main category: math.ST

TLDR: 本文研究了从独立同分布样本中估计有限字母表上的分布问题，以相对熵（Kullback-Leibler散度）衡量准确性。分析了Laplace估计器的性能，提出了置信依赖平滑技术，并探讨了稀疏分布的自适应方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究在最优期望风险边界方面已有成果，但高概率保证仍不够清晰。本文旨在填补这一空白，特别是在字母表超过样本量的稀疏场景下。

Method: 首先分析Laplace估计器，提出置信依赖平滑技术，并引入数据依赖平滑的估计器以适应稀疏分布。

Result: 证明了Laplace估计器在置信独立估计器中的最优性，并展示了最优非渐近风险中的额外对数因子。稀疏场景下，新估计器的高概率风险边界依赖于两个有效稀疏参数。

Conclusion: 本文提供了高概率风险边界的新见解，特别是在稀疏分布场景下，为实际应用提供了理论支持。

Abstract: We study the problem of estimating a distribution over a finite alphabet from
an i.i.d. sample, with accuracy measured in relative entropy (Kullback-Leibler
divergence). While optimal expected risk bounds are known, high-probability
guarantees remain less well-understood. First, we analyze the classical Laplace
(add-$1$) estimator, obtaining matching upper and lower bounds on its
performance and showing its optimality among confidence-independent estimators.
We then characterize the minimax-optimal high-probability risk achievable by
any estimator, which is attained via a simple confidence-dependent smoothing
technique. Interestingly, the optimal non-asymptotic risk contains an
additional logarithmic factor over the ideal asymptotic risk. Next, motivated
by scenarios where the alphabet exceeds the sample size, we investigate methods
that adapt to the sparsity of the distribution at hand. We introduce an
estimator using data-dependent smoothing, for which we establish a
high-probability risk bound depending on two effective sparsity parameters. As
part of the analysis, we also derive a sharp high-probability upper bound on
the missing mass.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [235] [Passive Measurement of Autonomic Arousal in Real-World Settings](https://arxiv.org/abs/2504.21242)
*Samy Abdel-Ghaffar,Isaac Galatzer-Levy,Conor Heneghan,Xin Liu,Sarah Kernasovskiy,Brennan Garrett,Andrew Barakat,Daniel McDuff*

Main category: cs.HC

TLDR: Fitbit Body Response Algorithm通过手腕传感器远程连续测量自主神经系统（ANS）活动，验证了在真实环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实验室外验证不足，需要一种能在真实环境中量化ANS活动的方法。

Method: 通过Trier Social Stress Test（n=45）和生态瞬时评估（EMA，n=87）验证算法，结合多种传感器信号。

Result: 模型预测感知压力的准确率为0.85，优于仅使用部分信号的情况。

Conclusion: 该算法在真实环境中表现良好，解决了实验室外传感的挑战。

Abstract: The autonomic nervous system (ANS) is activated during stress, which can have
negative effects on cardiovascular health, sleep, the immune system, and mental
health. While there are ways to quantify ANS activity in laboratories, there is
a paucity of methods that have been validated in real-world contexts. We
present the Fitbit Body Response Algorithm, an approach to continuous remote
measurement of ANS activation through widely available remote wrist-based
sensors. The design was validated via two experiments, a Trier Social Stress
Test (n = 45) and ecological momentary assessments (EMA) of perceived stress
(n=87), providing both controlled and ecologically valid test data. Model
performance predicting perceived stress when using all available sensor
modalities was consistent with expectations (accuracy=0.85) and outperformed
models with access to only a subset of the signals. We discuss and address
challenges to sensing that arise in real world settings that do not present in
conventional lab environments.

</details>

### [236] [Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning](https://arxiv.org/abs/2504.21731)
*Feiyu Lu,Mengyu Chen,Hsiang Hsu,Pranav Deshpande,Cheng Yao Wang,Blair MacIntyre*

Main category: cs.HC

TLDR: 论文探讨了如何利用强化学习（RL）在混合现实（MR）中动态优化3D内容布局，以适应用户姿态和环境变化。


<details>
  <summary>Details</summary>
Motivation: MR中虚拟内容的动态布局是一个挑战性问题，传统优化方法难以应对。

Method: 采用强化学习方法，结合用户姿态和环境信息，实现连续3D内容布局。

Result: 初步结果表明，RL能有效优化内容布局，提升用户体验。

Conclusion: 未来可进一步研究RL在MR中的个性化UI和内容布局优化。

Abstract: Mixed Reality (MR) could assist users' tasks by continuously integrating
virtual content with their view of the physical environment. However, where and
how to place these content to best support the users has been a challenging
problem due to the dynamic nature of MR experiences. In contrast to prior work
that investigates optimization-based methods, we are exploring how
reinforcement learning (RL) could assist with continuous 3D content placement
that is aware of users' poses and their surrounding environments. Through an
initial exploration and preliminary evaluation, our results demonstrate the
potential of RL to position content that maximizes the reward for users on the
go. We further identify future directions for research that could harness the
power of RL for personalized and optimized UI and content placement in MR.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [237] [Automated Generation of Precedence Graphs in Digital Value Chains for Automotive Production](https://arxiv.org/abs/2504.19835)
*Cornelius Hake,Christian Friedrich*

Main category: cs.RO

TLDR: 该研究提出了一种基于混合整数线性编程的自动化调度算法，优化汽车制造中的数字价值链，显著提升了效率、功能性和适应性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决汽车制造中电子控制单元的识别、软件刷新、定制和调试等数字价值链过程的效率问题。

Method: 提出了一种新颖的前驱图设计，并采用混合整数线性编程技术实现自动化调度算法。

Result: 算法减少了昂贵硬件和软件的生产站点数量，提高了产能利用率，任务并行化优化，准备时间减少50%，调度活动大幅减少。

Conclusion: 自动化调度在效率、功能性和适应性上显著优于传统手动方法，并支持车辆特定配置和新拓扑的集成。

Abstract: This study examines the digital value chain in automotive manufacturing,
focusing on the identification, software flashing, customization, and
commissioning of electronic control units in vehicle networks. A novel
precedence graph design is proposed to optimize this process chain using an
automated scheduling algorithm that employs mixed integer linear programming
techniques. The results show significant improvements in key metrics. The
algorithm reduces the number of production stations equipped with expensive
hardware and software to execute digital value chain processes, while
increasing capacity utilization through efficient scheduling and reduced idle
time. Task parallelization is optimized, resulting in streamlined workflows and
increased throughput. Compared to the traditional method, the automated
approach has reduced preparation time by 50% and reduced scheduling activities,
as it now takes two minutes to create the precedence graph. The flexibility of
the algorithm's constraints allows for vehicle-specific configurations while
maintaining high responsiveness, eliminating backup stations and facilitating
the integration of new topologies. Automated scheduling significantly
outperforms manual methods in efficiency, functionality, and adaptability.

</details>

### [238] [LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics](https://arxiv.org/abs/2504.21716)
*Marc Glocker,Peter Hönig,Matthias Hirschmanner,Markus Vincze*

Main category: cs.RO

TLDR: 本文提出了一种基于LLM驱动的多智能体机器人系统，用于家庭物品自主管理，结合记忆增强的任务规划和上下文学习，无需显式训练模型。


<details>
  <summary>Details</summary>
Motivation: 解决家庭环境中机器人自主管理物品的需求，同时通过记忆增强和上下文学习提高任务规划和长期物品追踪能力。

Method: 系统采用三个专用智能体（路由、任务规划和知识库），结合RAG技术检索历史交互信息，并利用Grounded SAM和LLaMa3.2-Vision进行物体检测和语义场景理解。

Result: 在三种家庭场景中表现出高任务规划准确性和记忆召回率提升，Qwen2.5在专用智能体中表现最佳，LLaMA3.1在路由任务中表现优异。

Conclusion: 该系统通过多智能体协作和上下文学习，有效实现了家庭物品的自主管理，代码已开源。

Abstract: We present an embodied robotic system with an LLM-driven agent-orchestration
architecture for autonomous household object management. The system integrates
memory-augmented task planning, enabling robots to execute high-level user
commands while tracking past actions. It employs three specialized agents: a
routing agent, a task planning agent, and a knowledge base agent, each powered
by task-specific LLMs. By leveraging in-context learning, our system avoids the
need for explicit model training. RAG enables the system to retrieve context
from past interactions, enhancing long-term object tracking. A combination of
Grounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating
semantic scene understanding for task planning. Evaluation across three
household scenarios demonstrates high task planning accuracy and an improvement
in memory recall due to RAG. Specifically, Qwen2.5 yields best performance for
specialized agents, while LLaMA3.1 excels in routing tasks. The source code is
available at: https://github.com/marc1198/chat-hsr.

</details>

### [239] [UAV-VLN: End-to-End Vision Language guided Navigation for UAVs](https://arxiv.org/abs/2504.21432)
*Pranav Saxena,Nishant Raghuvanshi,Neena Goveas*

Main category: cs.RO

TLDR: UAV-VLN是一个结合大型语言模型（LLMs）和视觉感知的端到端框架，用于无人机（UAVs）的自然语言导航，能够解析自由形式的指令并规划飞行轨迹。


<details>
  <summary>Details</summary>
Motivation: 解决AI自主导航中基于自然语言指令在未知环境中导航的核心挑战。

Method: 整合LLMs的常识推理能力和视觉模型的对象检测，通过跨模态对齐机制实现意图与视觉上下文的匹配。

Result: 在多样化的室内外导航场景中表现出色，显著提高了指令遵循准确性和轨迹效率。

Conclusion: LLM驱动的视觉语言接口为无人机自主导航提供了安全、直观且可泛化的解决方案。

Abstract: A core challenge in AI-guided autonomy is enabling agents to navigate
realistically and effectively in previously unseen environments based on
natural language commands. We propose UAV-VLN, a novel end-to-end
Vision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs)
that seamlessly integrates Large Language Models (LLMs) with visual perception
to facilitate human-interactive navigation. Our system interprets free-form
natural language instructions, grounds them into visual observations, and plans
feasible aerial trajectories in diverse environments.
  UAV-VLN leverages the common-sense reasoning capabilities of LLMs to parse
high-level semantic goals, while a vision model detects and localizes
semantically relevant objects in the environment. By fusing these modalities,
the UAV can reason about spatial relationships, disambiguate references in
human instructions, and plan context-aware behaviors with minimal task-specific
supervision. To ensure robust and interpretable decision-making, the framework
includes a cross-modal grounding mechanism that aligns linguistic intent with
visual context.
  We evaluate UAV-VLN across diverse indoor and outdoor navigation scenarios,
demonstrating its ability to generalize to novel instructions and environments
with minimal task-specific training. Our results show significant improvements
in instruction-following accuracy and trajectory efficiency, highlighting the
potential of LLM-driven vision-language interfaces for safe, intuitive, and
generalizable UAV autonomy.

</details>

### [240] [RoboGround: Robotic Manipulation with Grounded Vision-Language Priors](https://arxiv.org/abs/2504.21530)
*Haifeng Huang,Xinyi Chen,Yilun Chen,Hao Li,Xiaoshen Han,Zehan Wang,Tai Wang,Jiangmiao Pang,Zhou Zhao*

Main category: cs.RO

TLDR: 论文提出了一种基于grounding masks的中间表示方法RoboGround，用于提升机器人操作策略的泛化能力，并通过自动化生成大规模模拟数据验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用中间表示提升机器人操作策略的泛化能力，尤其是通过grounding masks结合空间指导和视觉语言模型的优势。

Method: 提出RoboGround系统，利用grounding masks作为中间表示，并通过自动化生成大规模模拟数据增强泛化能力。

Result: 实验表明，grounding masks作为中间指导显著提升了机器人策略的泛化能力。

Conclusion: grounding masks是一种有效的中间表示方法，能够显著提升机器人操作任务的泛化性能。

Abstract: Recent advancements in robotic manipulation have highlighted the potential of
intermediate representations for improving policy generalization. In this work,
we explore grounding masks as an effective intermediate representation,
balancing two key advantages: (1) effective spatial guidance that specifies
target objects and placement areas while also conveying information about
object shape and size, and (2) broad generalization potential driven by
large-scale vision-language models pretrained on diverse grounding datasets. We
introduce RoboGround, a grounding-aware robotic manipulation system that
leverages grounding masks as an intermediate representation to guide policy
networks in object manipulation tasks. To further explore and enhance
generalization, we propose an automated pipeline for generating large-scale,
simulated data with a diverse set of objects and instructions. Extensive
experiments show the value of our dataset and the effectiveness of grounding
masks as intermediate guidance, significantly enhancing the generalization
abilities of robot policies.

</details>

### [241] [UAV Marketplace Simulation Tool for BVLOS Operations](https://arxiv.org/abs/2504.21428)
*Kıvanç Şerefoğlu,Önder Gürcan,Reyhan Aydoğan*

Main category: cs.RO

TLDR: 该论文提出了一种用于评估多无人机团队形成的模拟工具，支持在动态和对抗性条件下测试无人机协作与任务执行。


<details>
  <summary>Details</summary>
Motivation: 研究旨在为无人机在超视距任务中的团队协作提供一种可控的测试环境，以应对动态和对抗性条件。

Method: 开发了一种模拟工具，支持配置任务参数和对抗行为，并记录仿真日志和性能指标。

Result: 工具能够灵活测试和优化无人机协调策略，适用于实际应用。

Conclusion: 该工具为无人机团队协作策略的研究和改进提供了有效支持。

Abstract: We present a simulation tool for evaluating team formation in autonomous
multi-UAV (Unmanned Aerial Vehicle) missions that operate Beyond Visual Line of
Sight (BVLOS). The tool models UAV collaboration and mission execution in
dynamic and adversarial conditions, where Byzantine UAVs attempt to disrupt
operations. Our tool allows researchers to integrate and compare various team
formation strategies in a controlled environment with configurable mission
parameters and adversarial behaviors. The log of each simulation run is stored
in a structured way along with performance metrics so that statistical analysis
could be done straightforwardly. The tool is versatile for testing and
improving UAV coordination strategies in real-world applications.

</details>

### [242] [Real Time Semantic Segmentation of High Resolution Automotive LiDAR Scans](https://arxiv.org/abs/2504.21602)
*Hannes Reichert,Benjamin Serfling,Elijah Schüssler,Kerim Turacan,Konrad Doll,Bernhard Sick*

Main category: cs.RO

TLDR: 提出了一种针对高分辨率LiDAR传感器的实时语义分割框架，填补了前沿研究与实际应用之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有方法多基于低分辨率LiDAR传感器，难以满足实时性需求，而高分辨率LiDAR的语义分割对自动驾驶至关重要。

Method: 利用表面法线作为强输入特征，提出了一种新的语义分割方法，并基于128层LiDAR传感器构建了新的数据集。

Result: 实现了高精度和实时处理的语义分割，并公开了数据集和ROS2实现代码。

Conclusion: 该研究为自动驾驶系统提供了实用的高分辨率LiDAR语义分割解决方案。

Abstract: In recent studies, numerous previous works emphasize the importance of
semantic segmentation of LiDAR data as a critical component to the development
of driver-assistance systems and autonomous vehicles. However, many
state-of-the-art methods are tested on outdated, lower-resolution LiDAR sensors
and struggle with real-time constraints. This study introduces a novel semantic
segmentation framework tailored for modern high-resolution LiDAR sensors that
addresses both accuracy and real-time processing demands. We propose a novel
LiDAR dataset collected by a cutting-edge automotive 128 layer LiDAR in urban
traffic scenes. Furthermore, we propose a semantic segmentation method
utilizing surface normals as strong input features. Our approach is bridging
the gap between cutting-edge research and practical automotive applications.
Additionaly, we provide a Robot Operating System (ROS2) implementation that we
operate on our research vehicle. Our dataset and code are publicly available:
https://github.com/kav-institute/SemanticLiDAR.

</details>

### [243] [SimPRIVE: a Simulation framework for Physical Robot Interaction with Virtual Environments](https://arxiv.org/abs/2504.21454)
*Federico Nesti,Gianluca D'Amico,Mauro Marinoni,Giorgio Buttazzo*

Main category: cs.RO

TLDR: SimPRIVE是一个用于物理机器人与虚拟环境交互的仿真框架，支持ROS 2的移动机器人通过数字孪生在Unreal Engine 5构建的虚拟世界中测试复杂算法，降低风险和成本。


<details>
  <summary>Details</summary>
Motivation: 机器学习和强化学习在物理系统中的不可预测行为需要一种安全、经济的测试方法，而高仿真度的模拟器为此提供了可能。

Method: 提出SimPRIVE框架，将物理机器人与虚拟环境结合，通过数字孪生技术测试算法，支持自定义虚拟场景和快速渲染。

Result: 验证了强化学习代理在虚拟办公室环境中的避障能力，物理机器人在有限空间内无碰撞移动。

Conclusion: SimPRIVE为复杂算法的测试提供了一种高效、低风险的解决方案，适用于实际硬件和软件堆栈的验证。

Abstract: The use of machine learning in cyber-physical systems has attracted the
interest of both industry and academia. However, no general solution has yet
been found against the unpredictable behavior of neural networks and
reinforcement learning agents. Nevertheless, the improvements of
photo-realistic simulators have paved the way towards extensive testing of
complex algorithms in different virtual scenarios, which would be expensive and
dangerous to implement in the real world.
  This paper presents SimPRIVE, a simulation framework for physical robot
interaction with virtual environments, which operates as a vehicle-in-the-loop
platform, rendering a virtual world while operating the vehicle in the real
world.
  Using SimPRIVE, any physical mobile robot running on ROS 2 can easily be
configured to move its digital twin in a virtual world built with the Unreal
Engine 5 graphic engine, which can be populated with objects, people, or other
vehicles with programmable behavior.
  SimPRIVE has been designed to accommodate custom or pre-built virtual worlds
while being light-weight to contain execution times and allow fast rendering.
Its main advantage lies in the possibility of testing complex algorithms on the
full software and hardware stack while minimizing the risks and costs of a test
campaign. The framework has been validated by testing a reinforcement learning
agent trained for obstacle avoidance on an AgileX Scout Mini rover that
navigates a virtual office environment where everyday objects and people are
placed as obstacles. The physical rover moves with no collision in an indoor
limited space, thanks to a LiDAR-based heuristic.

</details>

### [244] [Multi-Goal Dexterous Hand Manipulation using Probabilistic Model-based Reinforcement Learning](https://arxiv.org/abs/2504.21585)
*Yingzhuo Jiang,Wenjun Huang,Rongdun Lin,Chenyang Miao,Tianfu Sun,Yunduan Cui*

Main category: cs.RO

TLDR: 本文提出了一种基于模型的强化学习方法（GC-PMPC），用于学习多目标灵巧手操作任务，并在模拟和实际系统中验证了其高效性和优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决在高维灵巧手动力学环境中学习多目标操作任务的挑战。

Method: 设计了概率神经网络集成来描述高维灵巧手动力学，并引入异步MPC策略以满足实时控制频率需求。

Result: 在四个模拟Shadow Hand场景中，GC-PMPC表现优于现有基线方法，并在实际DexHand 021平台上高效学习操作任务。

Conclusion: GC-PMPC在低成本灵巧手平台上展示了卓越的学习效率和操控性能。

Abstract: This paper tackles the challenge of learning multi-goal dexterous hand
manipulation tasks using model-based Reinforcement Learning. We propose
Goal-Conditioned Probabilistic Model Predictive Control (GC-PMPC) by designing
probabilistic neural network ensembles to describe the high-dimensional
dexterous hand dynamics and introducing an asynchronous MPC policy to meet the
control frequency requirements in real-world dexterous hand systems. Extensive
evaluations on four simulated Shadow Hand manipulation scenarios with randomly
generated goals demonstrate GC-PMPC's superior performance over
state-of-the-art baselines. It successfully drives a cable-driven Dexterous
hand, DexHand 021 with 12 Active DOFs and 5 tactile sensors, to learn
manipulating a cubic die to three goal poses within approximately 80 minutes of
interactions, demonstrating exceptional learning efficiency and control
performance on a cost-effective dexterous hand platform.

</details>

### [245] [One Net to Rule Them All: Domain Randomization in Quadcopter Racing Across Different Platforms](https://arxiv.org/abs/2504.21586)
*Robin Ferede,Till Blaha,Erin Lucassen,Christophe De Wagter,Guido C. H. E. de Croon*

Main category: cs.RO

TLDR: 提出了一种基于域随机化的神经网络控制器，适用于多种不同物理特性的竞速无人机，验证了其跨平台适应能力。


<details>
  <summary>Details</summary>
Motivation: 在高速竞速无人机领域，单一控制器难以适应不同平台，需要一种通用的解决方案。

Method: 使用域随机化训练单一神经网络控制器，仅依赖当前状态直接计算电机指令。

Result: 通用控制器在两种不同尺寸的无人机上验证有效，虽速度略低于专用控制器，但适应性更强。

Conclusion: 域随机化是实现通用AI控制器的有效方法，尽管存在速度与鲁棒性的权衡。

Abstract: In high-speed quadcopter racing, finding a single controller that works well
across different platforms remains challenging. This work presents the first
neural network controller for drone racing that generalizes across physically
distinct quadcopters. We demonstrate that a single network, trained with domain
randomization, can robustly control various types of quadcopters. The network
relies solely on the current state to directly compute motor commands. The
effectiveness of this generalized controller is validated through real-world
tests on two substantially different crafts (3-inch and 5-inch race
quadcopters). We further compare the performance of this generalized controller
with controllers specifically trained for the 3-inch and 5-inch drone, using
their identified model parameters with varying levels of domain randomization
(0%, 10%, 20%, 30%). While the generalized controller shows slightly slower
speeds compared to the fine-tuned models, it excels in adaptability across
different platforms. Our results show that no randomization fails sim-to-real
transfer while increasing randomization improves robustness but reduces speed.
Despite this trade-off, our findings highlight the potential of domain
randomization for generalizing controllers, paving the way for universal AI
controllers that can adapt to any platform.

</details>

### [246] [Leveraging Pre-trained Large Language Models with Refined Prompting for Online Task and Motion Planning](https://arxiv.org/abs/2504.21596)
*Huihui Guo,Huilong Pi,Yunchuan Qin,Zhuo Tang,Kenli Li*

Main category: cs.RO

TLDR: LLM-PAS是一种结合大型语言模型（LLM）的闭环任务规划与执行系统，通过将部分约束检查从规划阶段转移到执行阶段，提高了任务执行的稳定性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的快速发展，智能机器人需要具备任务规划和稳定执行能力，以辅助人类完成复杂任务。

Method: LLM-PAS利用预训练的LLM进行长期任务规划，并通过First Look Prompting（FLP）方法优化PDDL目标生成，同时在执行阶段探索约束空间和处理异常。

Result: 实验表明，LLM-PAS在任务执行中能有效处理异常情况，表现出较高的有效性和鲁棒性。

Conclusion: LLM-PAS通过结合LLM的推理能力和闭环执行机制，显著提升了任务规划和执行的性能。

Abstract: With the rapid advancement of artificial intelligence, there is an increasing
demand for intelligent robots capable of assisting humans in daily tasks and
performing complex operations. Such robots not only require task planning
capabilities but must also execute tasks with stability and robustness. In this
paper, we present a closed-loop task planning and acting system, LLM-PAS, which
is assisted by a pre-trained Large Language Model (LLM). While LLM-PAS plans
long-horizon tasks in a manner similar to traditional task and motion planners,
it also emphasizes the execution phase of the task. By transferring part of the
constraint-checking process from the planning phase to the execution phase,
LLM-PAS enables exploration of the constraint space and delivers more accurate
feedback on environmental anomalies during execution. The reasoning
capabilities of the LLM allow it to handle anomalies that cannot be addressed
by the robust executor. To further enhance the system's ability to assist the
planner during replanning, we propose the First Look Prompting (FLP) method,
which induces LLM to generate effective PDDL goals. Through comparative
prompting experiments and systematic experiments, we demonstrate the
effectiveness and robustness of LLM-PAS in handling anomalous conditions during
task execution.

</details>

### [247] [Self-Supervised Monocular Visual Drone Model Identification through Improved Occlusion Handling](https://arxiv.org/abs/2504.21695)
*Stavrow A. Bahnam,Christophe De Wagter,Guido C. H. E. de Croon*

Main category: cs.RO

TLDR: 提出一种自监督学习方案，仅使用机载单目视频和飞行控制器数据训练基于神经网络的无人机模型，提升高速飞行时的运动估计精度。


<details>
  <summary>Details</summary>
Motivation: 在GPS缺失环境中，基于视觉的运动估计方法在高速飞行和复杂视觉条件下表现不佳，现有无人机模型依赖外部数据，难以扩展。

Method: 通过自监督相对位姿估计模型作为教师模型，训练无人机模型，并提出改进的遮挡处理方法。

Result: 位姿估计误差平均降低15%，无人机模型在高速飞行时比教师模型更准确，集成到VIO系统中提升了精度。

Conclusion: 自监督学习为无人机在复杂环境中的高速飞行和状态估计提供了更通用的解决方案。

Abstract: Ego-motion estimation is vital for drones when flying in GPS-denied
environments. Vision-based methods struggle when flight speed increases and
close-by objects lead to difficult visual conditions with considerable motion
blur and large occlusions. To tackle this, vision is typically complemented by
state estimation filters that combine a drone model with inertial measurements.
However, these drone models are currently learned in a supervised manner with
ground-truth data from external motion capture systems, limiting scalability to
different environments and drones. In this work, we propose a self-supervised
learning scheme to train a neural-network-based drone model using only onboard
monocular video and flight controller data (IMU and motor feedback). We achieve
this by first training a self-supervised relative pose estimation model, which
then serves as a teacher for the drone model. To allow this to work at high
speed close to obstacles, we propose an improved occlusion handling method for
training self-supervised pose estimation models. Due to this method, the root
mean squared error of resulting odometry estimates is reduced by an average of
15%. Moreover, the student neural drone model can be successfully obtained from
the onboard data. It even becomes more accurate at higher speeds compared to
its teacher, the self-supervised vision-based model. We demonstrate the value
of the neural drone model by integrating it into a traditional filter-based VIO
system (ROVIO), resulting in superior odometry accuracy on aggressive 3D racing
trajectories near obstacles. Self-supervised learning of ego-motion estimation
represents a significant step toward bridging the gap between flying in
controlled, expensive lab environments and real-world drone applications. The
fusion of vision and drone models will enable higher-speed flight and improve
state estimation, on any drone in any environment.

</details>

<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [248] [Generalised Label-free Artefact Cleaning for Real-time Medical Pulsatile Time Series](https://arxiv.org/abs/2504.21209)
*Xuhang Chen,Ihsane Olakorede,Stefan Yu Bögli,Wenhao Xu,Erta Beqiri,Xuemeng Li,Chenyu Tang,Zeyu Gao,Shuo Gao,Ari Ercole,Peter Smielewski*

Main category: eess.SP

TLDR: GenClean是一个无需标签的实时伪影清理框架，适用于医疗时间序列分析，特别是在动脉血压和光电容积描记信号中表现优异。


<details>
  <summary>Details</summary>
Motivation: 医疗时间序列中的伪影会影响临床决策，现有方法依赖监督学习且忽略患者层面的分布变化。

Method: 提出GenClean框架，利用18万条10秒动脉血压样本训练，验证其在患者内和患者间分布变化下的鲁棒性，并扩展到光电容积描记信号。

Result: 在MIMIC-III数据库的跨疾病队列实验中表现优异，并成功集成到临床监测软件ICM+中。

Conclusion: GenClean为高分辨率医疗时间序列分析的可靠性提供了基础，推动了精准医疗的发展。

Abstract: Artefacts compromise clinical decision-making in the use of medical time
series. Pulsatile waveforms offer probabilities for accurate artefact
detection, yet most approaches rely on supervised manners and overlook
patient-level distribution shifts. To address these issues, we introduce a
generalised label-free framework, GenClean, for real-time artefact cleaning and
leverage an in-house dataset of 180,000 ten-second arterial blood pressure
(ABP) samples for training. We first investigate patient-level generalisation,
demonstrating robust performances under both intra- and inter-patient
distribution shifts. We further validate its effectiveness through challenging
cross-disease cohort experiments on the MIMIC-III database. Additionally, we
extend our method to photoplethysmography (PPG), highlighting its applicability
to diverse medical pulsatile signals. Finally, its integration into ICM+, a
clinical research monitoring software, confirms the real-time feasibility of
our framework, emphasising its practical utility in continuous physiological
monitoring. This work provides a foundational step toward precision medicine in
improving the reliability of high-resolution medical time series analysis

</details>

<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [249] [Towards Space Group Determination from EBSD Patterns: The Role of Deep Learning and High-throughput Dynamical Simulations](https://arxiv.org/abs/2504.21331)
*Alfred Yan,Muhammad Nur Talha Kilic,Gert Nolze,Ankit Agrawal,Alok Choudhary,Roberto dos Reis,Vinayak Dravid*

Main category: cond-mat.mtrl-sci

TLDR: 论文提出了一种基于深度学习的晶体对称性分类方法，通过电子背散射衍射（EBSD）和神经网络实现高效材料表征。


<details>
  <summary>Details</summary>
Motivation: 材料合成的速度超过了表征能力，亟需快速、可扩展的晶体对称性分析方法。

Method: 利用物理模拟生成人工EBSD数据集训练神经网络，并结合无监督域适应方法处理实验数据。

Result: 模型在模拟和实验数据上的准确率超过90%，证明神经网络可从EBSD图案预测晶体对称性。

Conclusion: 深度学习结合EBSD技术为高通量材料发现提供了可行解决方案。

Abstract: The design of novel materials hinges on the understanding of
structure-property relationships. However, our capability to synthesize a large
number of materials has outpaced the ability and speed needed to characterize
them. While the overall chemical constituents can be readily known during
synthesis, the structural evolution and characterization of newly synthesized
samples remains a bottleneck for the ultimate goal of high throughput
nanomaterials discovery. Thus, scalable methods for crystal symmetry
determination that can analyze a large volume of material samples within a
short time-frame are especially needed. Kikuchi diffraction in the SEM is a
promising technique for this due to its sensitivity to dynamical scattering,
which may provide information beyond just the seven crystal systems and
fourteen Bravais lattices. After diffraction patterns are collected from
material samples, deep learning methods may be able to classify the space group
symmetries using the patterns as input, which paired with the elemental
composition, would help enable the determination of the crystal structure. To
investigate the feasibility of this solution, neural networks were trained to
predict the space group type of background corrected EBSD patterns. Our
networks were first trained and tested on an artificial dataset of EBSD
patterns of 5,148 different cubic phases, created through physics-based
dynamical simulations. Next, Maximum Classifier Discrepancy, an unsupervised
deep learning-based domain adaptation method, was utilized to train neural
networks to make predictions for experimental EBSD patterns. We introduce a
relabeling scheme, which enables our models to achieve accuracy scores higher
than 90% on simulated and experimental data, suggesting that neural networks
are capable of making predictions of crystal symmetry from an EBSD pattern.

</details>

<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [250] [Turning Up the Heat: Assessing 2-m Temperature Forecast Errors in AI Weather Prediction Models During Heat Waves](https://arxiv.org/abs/2504.21195)
*Kelsey E. Ennis,Elizabeth A. Barnes,Marybeth C. Arcodia,Martin A. Fernandez,Eric D. Maloney*

Main category: physics.ao-ph

TLDR: 研究比较了AIWP模型与传统NWP模型在预测极端高温方面的表现，发现AIWP模型（尤其是GraphCast）在多数情况下表现更优，但仍存在冷偏差。


<details>
  <summary>Details</summary>
Motivation: 极端高温是美国最致命的天气灾害，且频率和强度在增加，但传统NWP模型在中期和次季节尺度预测上表现不佳，AIWP模型的发展为解决这一问题提供了可能。

Method: 研究分析了60次热浪事件，比较了两种AIWP模型（GraphCast和Pangu-Weather）和一种传统NWP模型（UFS GEFS）在20天内的预测表现。

Result: GraphCast在多数情况下优于其他模型，但AIWP模型普遍存在冷偏差，仅Pangu-Weather在冬季表现出暖偏差。

Conclusion: AIWP模型在极端高温的中期和次季节预测中具有潜力，但仍需改进偏差问题。

Abstract: Extreme heat is the deadliest weather-related hazard in the United States.
Furthermore, it is increasing in intensity, frequency, and duration, making
skillful forecasts vital to protecting life and property. Traditional numerical
weather prediction (NWP) models struggle with extreme heat for medium-range and
subseasonal-to-seasonal (S2S) timescales. Meanwhile, artificial
intelligence-based weather prediction (AIWP) models are progressing rapidly.
However, it is largely unknown how well AIWP models forecast extremes,
especially for medium-range and S2S timescales. This study investigates 2-m
temperature forecasts for 60 heat waves across the four boreal seasons and over
four CONUS regions at lead times up to 20 days, using two AIWP models (Google
GraphCast and Pangu-Weather) and one traditional NWP model (NOAA United
Forecast System Global Ensemble Forecast System (UFS GEFS)). First, case study
analyses show that both AIWP models and the UFS GEFS exhibit consistent cold
biases on regional scales in the 5-10 days of lead time before heat wave onset.
GraphCast is the more skillful AIWP model, outperforming UFS GEFS and
Pangu-Weather in most locations. Next, the two AIWP models are isolated and
analyzed across all heat waves and seasons, with events split among the model's
testing (2018-2023) and training (1979-2017) periods. There are cold biases
before and during the heat waves in both models and all seasons, except
Pangu-Weather in winter, which exhibits a mean warm bias before heat wave
onset. Overall, results offer encouragement that AIWP models may be useful for
medium-range and S2S predictability of extreme heat.

</details>

<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [251] [A Summation-Based Algorithm For Integer Factorization](https://arxiv.org/abs/2504.21168)
*Justin Friedlander*

Main category: math.NA

TLDR: 本文提出了一种新的整数分解方法，通过将整数转换为基-2的和，结合基-10和基-2表示，实现时间复杂度的优化。


<details>
  <summary>Details</summary>
Motivation: 快速整数分解算法在现代密码学（如RSA加密）中至关重要，但现有方法效率有限。

Method: 将整数转换为基-2的和，结合基-10和基-2表示，设计时间复杂度为√n的分解算法。

Result: 算法能够将整数分解为两个因数的乘积。

Conclusion: 新方法为整数分解提供了一种高效途径，对密码学安全有潜在影响。

Abstract: Numerous methods have been considered to create a fast integer factorization
algorithm. Despite its apparent simplicity, the difficulty to find such an
algorithm plays a crucial role in modern cryptography, notably, in the security
of RSA encryption. Some approaches to factoring integers quickly include the
Trial Division method, Pollard's Rho and p-1 methods, and various Sieve
algorithms.
  This paper introduces a new method that converts an integer into a sum in
base-2. By combining a base-10 and base-2 representation of the integer, an
algorithm on the order of $\sqrt{n}$ time complexity can convert that sum to a
product of two integers, thus factoring the original number.

</details>

### [252] [Low-rank computation of the posterior mean in Multi-Output Gaussian Processes](https://arxiv.org/abs/2504.21527)
*Sebastian Esche,Martin Stoll*

Main category: math.NA

TLDR: 本文提出了一种低秩方法，用于高效计算多输出高斯过程（MOGP）的后验均值，通过空间和时间可分离的协方差函数和Kronecker积分解，结合LRPCG方法和KPIK求解器解决大规模Stein方程。


<details>
  <summary>Details</summary>
Motivation: 多输出高斯过程（MOGP）在机器学习和计算科学中应用广泛，但计算后验均值时面临大规模Stein方程的挑战，需要高效的低秩方法。

Method: 采用低秩时空数据，假设协方差函数在空间和时间上可分离，分解为Kronecker积，结合LRPCG方法和KPIK求解器解决Stein方程。

Result: 在真实世界街道网络图上测试，使用图滤波器作为协方差矩阵，并提出一种度加权平均协方差矩阵以提升收敛效率。

Conclusion: 提出的低秩方法有效解决了MOGP后验均值的计算问题，适用于大规模数据，并展示了在实际应用中的潜力。

Abstract: Gaussian processes (GP) are a versatile tool in machine learning and
computational science. We here consider the case of multi-output Gaussian
processes (MOGP) and present low-rank approaches for efficiently computing the
posterior mean of a MOGP. Starting from low-rank spatio-temporal data we
consider a structured covariance function, assuming separability across space
and time. This separability, in turn, gives a decomposition of the covariance
matrix into a Kronecker product of individual covariance matrices.
Incorporating the typical noise term to the model then requires the solution of
a large-scale Stein equation for computing the posterior mean. For this, we
propose efficient low-rank methods based on a combination of a LRPCG method
with the Sylvester equation solver KPIK adjusted for solving Stein equations.
We test the developed method on real world street network graphs by using graph
filters as covariance matrices. Moreover, we propose a degree-weighted average
covariance matrix, which can be employed under specific assumptions to achieve
more efficient convergence.

</details>

<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [253] [Cryptography without Long-Term Quantum Memory and Global Entanglement](https://arxiv.org/abs/2504.21842)
*Lev Stambler*

Main category: quant-ph

TLDR: 论文展示了如何利用仅支持经典查询的预言机构建多种量子密码学原语，无需长期量子存储或全局纠缠。基于半量子令牌方案，提出了短寿命半量子一次性程序（OTP）和RAM混淆方案，并进一步构建了长期一次性程序和复制保护方案。


<details>
  <summary>Details</summary>
Motivation: 研究如何在仅支持经典查询的预言机条件下，构建无需长期量子存储或全局纠缠的量子密码学原语，以降低实际实现的复杂性。

Method: 利用半量子令牌方案和经典查询预言机，首先构建短寿命半量子OTP，再扩展为RAM混淆方案，最后实现长期一次性程序和复制保护方案。

Result: 成功构建了多种量子密码学原语，包括半量子OTP、RAM混淆方案、长期一次性程序和复制保护方案，且仅需对数级开销。

Conclusion: 通过半量子令牌和经典查询预言机，实现了无需长期量子存储或全局纠缠的量子密码学原语，为实际应用提供了可行性。

Abstract: We show how oracles which only allow for classical query access can be used
to construct a variety of quantum cryptographic primitives which do not require
long-term quantum memory or global entanglement. Specifically, if a quantum
party can execute a semi-quantum token scheme (Shmueli 2022) with probability
of success $1/2 + \delta$, we can build powerful cryptographic primitives with
a multiplicative logarithmic overhead for the desired correctness error. Our
scheme makes no assumptions about the quantum party's noise model except for a
simple independence requirement: noise on two sets of non-entangled hardware
must be independent.
  Using semi-quantum tokens and oracles which can only be queried classically,
we first show how to construct a "short-lived" semi-quantum one-time program
(OTP) which allows a classical sending party to prepare a one-time program on
the receiving party's quantum computer. We then show how to use this
semi-quantum OTP to construct a semi-quantum "stateful obfuscation" scheme
(which we term "RAM obfuscation"). Importantly, the RAM obfuscation scheme does
not require long-term quantum memory or global entanglement. Finally, we show
how RAM obfuscation can be used to build long-lived one-time programs and
copy-protection schemes.

</details>

### [254] [QAOA Parameter Transferability for Maximum Independent Set using Graph Attention Networks](https://arxiv.org/abs/2504.21135)
*Hanjing Xu,Xiaoyuan Liu,Alex Pothen,Ilya Safro*

Main category: quant-ph

TLDR: 提出了一种基于图注意力网络（GAT）的QAOA参数转移方案，用于解决最大独立集（MIS）问题，并结合混合分布式资源感知算法（HyDRA-MIS）在NISQ计算机上实现。


<details>
  <summary>Details</summary>
Motivation: 解决QAOA中非线性、非凸优化参数的问题，并扩展其在大型图上的应用。

Method: 使用GAT转移优化参数，设计HyDRA-MIS算法分解大型问题。

Result: 在数千顶点图上取得与经典求解器KaMIS竞争的结果。

Conclusion: GAT参数转移与HyDRA-MIS结合，为NISQ设备上的组合优化问题提供了有效解决方案。

Abstract: The quantum approximate optimization algorithm (QAOA) is one of the promising
variational approaches of quantum computing to solve combinatorial optimization
problems. In QAOA, variational parameters need to be optimized by solving a
series of nonlinear, nonconvex optimization programs. In this work, we propose
a QAOA parameter transfer scheme using Graph Attention Networks (GAT) to solve
Maximum Independent Set (MIS) problems. We prepare optimized parameters for
graphs of 12 and 14 vertices and use GATs to transfer their parameters to
larger graphs. Additionally, we design a hybrid distributed resource-aware
algorithm for MIS (HyDRA-MIS), which decomposes large problems into smaller
ones that can fit onto noisy intermediate-scale quantum (NISQ) computers. We
integrate our GAT-based parameter transfer approach to HyDRA-MIS and
demonstrate competitive results compared to KaMIS, a state-of-the-art classical
MIS solver, on graphs with several thousands vertices.

</details>

### [255] [Efficient Quantum-Safe Homomorphic Encryption for Quantum Computer Programs](https://arxiv.org/abs/2504.21235)
*Ben Goertzel*

Main category: quant-ph

TLDR: 提出了一种基于格的量子程序同态评估方案，支持量子对抗环境下的安全性。


<details>
  <summary>Details</summary>
Motivation: 将经典同态加密扩展到量子领域，解决量子计算中的隐私和安全问题。

Method: 使用MLWE格和BNSF掩码隐藏振幅，量子状态存储为MLWE密文对，并通过qIND-CPA游戏形式化安全性。

Result: 方案在100量子比特、深度10^3的量子证明中运行时间为10毫秒，公钥仅32字节，CCA级密钥小于300 kB。

Conclusion: 结果表明，完全同态且支持知识库的量子推理与近期量子云和标准后量子安全假设兼容。

Abstract: We present a lattice-based scheme for homomorphic evaluation of quantum
programs and proofs that remains secure against quantum adversaries. Classical
homomorphic encryption is lifted to the quantum setting by replacing
composite-order groups with Module Learning-With-Errors (MLWE) lattices and by
generalizing polynomial functors to bounded natural super functors (BNSFs). A
secret depolarizing BNSF mask hides amplitudes, while each quantum state is
stored as an MLWE ciphertext pair. We formalize security with the qIND-CPA game
that allows coherent access to the encryption oracle and give a four-hybrid
reduction to decisional MLWE.
  The design also covers practical issues usually left open. A typed QC-bridge
keeps classical bits produced by measurements encrypted yet still usable as
controls, with weak-measurement semantics for expectation-value workloads.
Encrypted Pauli twirls add circuit privacy. If a fixed knowledge base is
needed, its axioms are shipped as MLWE "capsules"; the evaluator can use them
but cannot read them. A rho-calculus driver schedules encrypted tasks across
several QPUs and records an auditable trace on an RChain-style ledger.
  Performance analysis shows that the extra lattice arithmetic fits inside
today's QPU idle windows: a 100-qubit, depth-10^3 teleportation-based proof
runs in about 10 ms, the public key (seed only) is 32 bytes, and even a
CCA-level key stays below 300 kB. A photonic Dirac-3 prototype that executes
homomorphic teleportation plus knowledge-base-relative amplitude checks appears
feasible with current hardware. These results indicate that fully homomorphic,
knowledge-base-aware quantum reasoning is compatible with near-term quantum
clouds and standard post-quantum security assumptions.

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [256] [An Empirical Study on the Effectiveness of Large Language Models for Binary Code Understanding](https://arxiv.org/abs/2504.21803)
*Xiuwei Shang,Zhenkan Fu,Shaoyin Cheng,Guoqiang Chen,Gangyang Li,Li Hu,Weiming Zhang,Nenghai Yu*

Main category: cs.SE

TLDR: 该论文提出了一个评估大型语言模型（LLMs）在二进制代码理解任务中有效性的基准，包括函数名恢复和二进制代码摘要，并揭示了LLMs在二进制代码分析中的潜力。


<details>
  <summary>Details</summary>
Motivation: 二进制代码分析在软件安全中至关重要，但由于缺乏直观语义信息，理解二进制代码具有挑战性。传统工具生成的伪代码仍难以理解，而深度学习和LLMs在代码理解任务中表现出潜力。

Method: 论文提出一个基准，评估LLMs在多种目标架构和优化选项下的二进制代码理解能力，包括函数名恢复和代码摘要任务。

Result: 实验表明，现有LLMs在一定程度上能理解二进制代码，提高了分析效率，并展示了其在二进制代码理解领域的潜力。

Conclusion: LLMs在二进制代码理解中具有显著潜力，为二进制代码分析技术提供了新方向。

Abstract: Binary code analysis plays a pivotal role in the field of software security
and is widely used in tasks such as software maintenance, malware detection,
software vulnerability discovery, patch analysis, etc. However, unlike source
code, reverse engineers face significant challenges in understanding binary
code due to the lack of intuitive semantic information. Although traditional
reverse tools can convert binary code into C-like pseudo code, the lack of code
comments and symbolic information such as function names still makes code
understanding difficult. In recent years, two groups of techniques have shown
promising prospects: (1) Deep learning-based techniques have demonstrated
competitive results in tasks related to binary code understanding, furthermore,
(2) Large Language Models (LLMs) have been extensively pre-trained at the
source-code level for tasks such as code understanding and generation. This has
left participants wondering about the capabilities of LLMs in binary code
understanding. To this end, this work proposes a benchmark to evaluate the
effectiveness of LLMs in real-world reverse engineering scenarios, which covers
two key binary code understanding tasks, i.e., function name recovery and
binary code summarization. To more comprehensively evaluate, we include
binaries with multiple target architectures as well as different optimization
options. We gain valuable insights into the capabilities and limitations
through extensive empirical studies of popular LLMs using our benchmark. Our
evaluations reveal that existing LLMs can understand binary code to a certain
extent, thereby improving the efficiency of binary code analysis. Our results
highlight the great potential of the LLMs in advancing the field of binary code
understanding, and provide new directions for binary code analysis techniques.

</details>

### [257] [CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation](https://arxiv.org/abs/2504.21751)
*Sizhe Wang,Zhengren Wang,Dongsheng Ma,Yongan Yu,Rui Ling,Zhiyu Li,Feiyu Xiong,Wentao Zhang*

Main category: cs.SE

TLDR: CodeFlowBench是首个用于评估LLMs在多轮代码复用（codeflow）能力的基准测试，包含5258个问题，实验显示LLMs在多轮场景下表现较差。


<details>
  <summary>Details</summary>
Motivation: 现实开发需要可读、可扩展和可测试的代码，通过模块化组件和多轮复用实现。

Method: 基于Codeforces问题，通过自动化管道分解为函数级子问题，并设计多轮代码复用的评估框架。

Result: LLMs在多轮代码流场景下表现不佳，如o1-mini的pass@1从单轮的37.8%降至20.8%。

Conclusion: CodeFlowBench为多轮代码生成提供了新见解，揭示了当前LLMs在复杂结构问题上的挑战。

Abstract: Real world development demands code that is readable, extensible, and
testable by organizing the implementation into modular components and
iteratively reuse pre-implemented code. We term this iterative, multi-turn
process codeflow and introduce CodeFlowBench, the first benchmark designed for
comprehensively evaluating LLMs' ability to perform codeflow, namely to
implement new functionality by reusing existing functions over multiple turns.
CodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously
updated via an automated pipeline that decomposes each problem into a series of
function-level subproblems based on its dependency tree and each subproblem is
paired with unit tests. We further propose a novel evaluation framework with
tasks and metrics tailored to multi-turn code reuse to assess model
performance. In experiments across various LLMs under both multi-turn and
single-turn patterns. We observe models' poor performance on CodeFlowBench,
with a substantial performance drop in the iterative codeflow scenario. For
instance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8%
in single-turn pattern. Further analysis shows that different models excel at
different dependency depths, yet all struggle to correctly solve structurally
complex problems, highlighting challenges for current LLMs to serve as code
generation tools when performing codeflow. Overall, CodeFlowBench offers a
comprehensive benchmark and new insights into LLM capabilities for multi-turn,
iterative code generation, guiding future advances in code generation tasks.

</details>

### [258] [SWE-smith: Scaling Data for Software Engineering Agents](https://arxiv.org/abs/2504.21798)
*John Yang,Kilian Leret,Carlos E. Jimenez,Alexander Wettig,Kabir Khandpur,Yanzhe Zhang,Binyuan Hui,Ofir Press,Ludwig Schmidt,Diyi Yang*

Main category: cs.SE

TLDR: SWE-smith是一个用于大规模生成软件工程训练数据的新管道，解决了现有数据集小且难以扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程训练数据集规模小、构建复杂且存储需求高，限制了语言模型在软件工程中的应用。

Method: SWE-smith通过为任何Python代码库构建执行环境，并自动生成破坏现有测试的任务实例，创建大规模数据集。

Result: 生成了50k实例的数据集，训练出的SWE-agent-LM-32B在SWE-bench基准测试中达到40.2% Pass@1，优于现有开源模型。

Conclusion: SWE-smith及其开源资产降低了自动化软件工程研究的门槛。

Abstract: Despite recent progress in Language Models (LMs) for software engineering,
collecting training data remains a significant pain point. Existing datasets
are small, with at most 1,000s of training instances from 11 or fewer GitHub
repositories. The procedures to curate such datasets are often complex,
necessitating hundreds of hours of human labor; companion execution
environments also take up several terabytes of storage, severely limiting their
scalability and usability. To address this pain point, we introduce SWE-smith,
a novel pipeline for generating software engineering training data at scale.
Given any Python codebase, SWE-smith constructs a corresponding execution
environment, then automatically synthesizes 100s to 1,000s of task instances
that break existing test(s) in the codebase. Using SWE-smith, we create a
dataset of 50k instances sourced from 128 GitHub repositories, an order of
magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving
40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art
among open source models. We open source SWE-smith (collection procedure, task
instances, trajectories, models) to lower the barrier of entry for research in
LM systems for automated software engineering. All assets available at
https://swesmith.com.

</details>

### [259] [Assessing LLM code generation quality through path planning tasks](https://arxiv.org/abs/2504.21276)
*Wanyi Chen,Meng-Wen Su,Mary L. Cummings*

Main category: cs.SE

TLDR: 评估LLM生成代码在安全关键路径规划中的风险，发现现有基准不足，测试六种LLM生成三种算法的代码，结果表明存在严重隐患。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成代码的普及，需评估其在安全关键应用（如路径规划）中的风险，现有基准无法满足需求。

Method: 测试六种LLM生成三种路径规划算法的代码，并在三种不同难度地图上验证。

Result: LLM生成的代码在路径规划中存在严重安全隐患。

Conclusion: LLM生成代码在安全关键应用中需严格测试，不可直接使用。

Abstract: As LLM-generated code grows in popularity, more evaluation is needed to
assess the risks of using such tools, especially for safety-critical
applications such as path planning. Existing coding benchmarks are insufficient
as they do not reflect the context and complexity of safety-critical
applications. To this end, we assessed six LLMs' abilities to generate the code
for three different path-planning algorithms and tested them on three maps of
various difficulties. Our results suggest that LLM-generated code presents
serious hazards for path planning applications and should not be applied in
safety-critical contexts without rigorous testing.

</details>

<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [260] [On the Potential of Large Language Models to Solve Semantics-Aware Process Mining Tasks](https://arxiv.org/abs/2504.21074)
*Adrian Rebmann,Fabian David Schmidt,Goran Glavaš,Han van der Aa*

Main category: cs.DB

TLDR: LLMs在过程挖掘任务中表现出潜力，尤其是在语义感知任务中。通过上下文学习和监督微调，LLMs在复杂任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在语义感知过程挖掘任务中的能力，填补现有研究在默认状态和微调状态下的空白。

Method: 定义五个需要语义理解的过程挖掘任务，并通过上下文学习和监督微调评估LLMs的性能。

Result: LLMs在默认状态下表现不佳，但经过微调后，在多种过程和行业中表现优异。

Conclusion: LLMs在语义感知过程挖掘任务中具有潜力，但需通过微调才能发挥最佳性能。

Abstract: Large language models (LLMs) have shown to be valuable tools for tackling
process mining tasks. Existing studies report on their capability to support
various data-driven process analyses and even, to some extent, that they are
able to reason about how processes work. This reasoning ability suggests that
there is potential for LLMs to tackle semantics-aware process mining tasks,
which are tasks that rely on an understanding of the meaning of activities and
their relationships. Examples of these include process discovery, where the
meaning of activities can indicate their dependency, whereas in anomaly
detection the meaning can be used to recognize process behavior that is
abnormal. In this paper, we systematically explore the capabilities of LLMs for
such tasks. Unlike prior work, which largely evaluates LLMs in their default
state, we investigate their utility through both in-context learning and
supervised fine-tuning. Concretely, we define five process mining tasks
requiring semantic understanding and provide extensive benchmarking datasets
for evaluation. Our experiments reveal that while LLMs struggle with
challenging process mining tasks when used out of the box or with minimal
in-context examples, they achieve strong performance when fine-tuned for these
tasks across a broad range of process types and industries.

</details>

<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [261] [Participatory AI, Public Sector AI, Differential Privacy, Conversational Interfaces, Explainable AI, Citizen Engagement in AI](https://arxiv.org/abs/2504.21297)
*Wenjun Yang,Eyhab Al-Masri*

Main category: cs.IT

TLDR: N/A


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces a conversational interface system that enables
participatory design of differentially private AI systems in public sector
applications. Addressing the challenge of balancing mathematical privacy
guarantees with democratic accountability, we propose three key contributions:
(1) an adaptive $\epsilon$-selection protocol leveraging TOPSIS multi-criteria
decision analysis to align citizen preferences with differential privacy (DP)
parameters, (2) an explainable noise-injection framework featuring real-time
Mean Absolute Error (MAE) visualizations and GPT-4-powered impact analysis, and
(3) an integrated legal-compliance mechanism that dynamically modulates privacy
budgets based on evolving regulatory constraints. Our results advance
participatory AI practices by demonstrating how conversational interfaces can
enhance public engagement in algorithmic privacy mechanisms, ensuring that
privacy-preserving AI in public sector governance remains both mathematically
robust and democratically accountable.

</details>

### [262] [Sionna RT: Technical Report](https://arxiv.org/abs/2504.21719)
*Fayçal Aït Aoudia,Jakob Hoydis,Merlin Nimier-David,Sebastian Cammerer,Alexander Keller*

Main category: cs.IT

TLDR: Sionna是一个开源、GPU加速的库，其0.14版本引入了可微分的射线追踪功能，用于模拟无线电波传播。1.0版本对射线追踪器进行了全面改进，提升了速度、内存效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 为了高效模拟无线电波传播，并提供可微分的信道脉冲响应（CIRs）和无线电地图，以优化系统与环境参数。

Method: 结合了SBR（射线发射与反弹）与图像方法计算CIRs，并使用哈希机制消除重复路径；无线电地图则采用纯SBR方法。

Result: Sionna RT显著提升了射线追踪的速度和效率，并支持对系统参数的梯度计算。

Conclusion: Sionna RT为无线电波传播模拟提供了高效且可扩展的解决方案，但仍存在一些局限性。

Abstract: Sionna is an open-source, GPU-accelerated library that, as of version 0.14,
incorporates a ray tracer for simulating radio wave propagation. A unique
feature of Sionna RT is differentiability, enabling the calculation of
gradients for the channel impulse responses (CIRs), radio maps, and other
related metrics with respect to system and environmental parameters, such as
material properties, antenna patterns, and array geometries. The release of
Sionna 1.0 provides a complete overhaul of the ray tracer, significantly
improving its speed, memory efficiency, and extensibility. This document
details the algorithms employed by Sionna RT to simulate radio wave propagation
efficiently, while also addressing their current limitations. Given that the
computation of CIRs and radio maps requires distinct algorithms, these are
detailed in separate sections. For CIRs, Sionna RT integrates shooting and
bouncing of rays (SBR) with the image method and uses a hashing-based mechanism
to efficiently eliminate duplicate paths. Radio maps are computed using a
purely SBR-based approach.

</details>

<div id='q-bio.CB'></div>

# q-bio.CB [[Back]](#toc)

### [263] [Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks](https://arxiv.org/abs/2504.21578)
*Kamila Barylska,Frank Delaplace,Anna Gogolińska,Ewa Pańkowska*

Main category: q-bio.CB

TLDR: 该论文通过建立Petri网模型，研究了胰岛素和胰高血糖素的分泌机制及其在血糖调节中的作用，并分析了模型动态行为。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解血糖调节的复杂过程，特别是胰岛素和胰高血糖素的分泌机制，以帮助理解糖尿病。

Method: 创建了胰岛素和胰高血糖素分泌的Petri网模型，并分析了其动态行为；还将系统转换为布尔网络。

Result: 成功建立了胰岛素和胰高血糖素分泌的模型，并分析了其动态行为。

Conclusion: 这些模型为理解糖尿病提供了基础，并展示了Petri网在血糖调节研究中的潜力。

Abstract: Diabetes is a civilization chronic disease characterized by a constant
elevated concentration of glucose in the blood. Many processes are involved in
the glucose regulation, and their interactions are very complex. To better
understand those processes we set ourselves a goal to create a Petri net model
of the glucose regulation in the whole body. So far we have managed to create a
model of glycolysis and synthesis of glucose in the liver, and the general
overview models of the glucose regulation in a healthy and diabetic person. In
this paper we introduce Petri nets models of insulin secretion in beta cell of
the pancreas, and glucagon in the pancreas alpha cells. Those two hormones have
mutually opposite effects: insulin preventing hyperglycemia, and glucagon
preventing hypoglycemia. Understanding the mechanisms of insulin and glucagon
secretion constitutes the basis for understanding diabetes. We also present a
model in which both processes occur together, depending on the blood glucose
level. The dynamics of each model is analysed. Additionally, we transform the
overall insulin and glucagon secretion system to a Boolean network, following
standard transformation rules.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [264] [Selecting the Right LLM for eGov Explanations](https://arxiv.org/abs/2504.21032)
*Lior Limonad,Fabiana Fournier,Hadar Mulian,George Manias,Spiros Borotis,Danai Kyrkou*

Main category: cs.CY

TLDR: 论文探讨了如何通过生成式AI（特别是大语言模型LLMs）自动生成电子政务服务中的解释内容，以提高用户信任和使用率。研究通过用户调查比较不同LLM生成解释的质量，并尝试自动化评估过程。


<details>
  <summary>Details</summary>
Motivation: 电子政务服务的解释质量对用户信任和使用至关重要，但选择合适的LLM生成解释内容是一项复杂任务。

Method: 研究改编了一个量表，系统比较不同LLM生成解释的感知质量，并以税务返还为例进行用户调查（128名受访者）。同时探索自动化评估方法。

Result: 通过用户调查提供了选择合适LLM的方法论基础，并初步尝试用预测技术自动化复制人类反馈。

Conclusion: 研究为电子政务服务提供了一种系统选择LLM的方法，并展示了自动化评估的潜力。

Abstract: The perceived quality of the explanations accompanying e-government services
is key to gaining trust in these institutions, consequently amplifying further
usage of these services. Recent advances in generative AI, and concretely in
Large Language Models (LLMs) allow the automation of such content
articulations, eliciting explanations' interpretability and fidelity, and more
generally, adapting content to various audiences. However, selecting the right
LLM type for this has become a non-trivial task for e-government service
providers. In this work, we adapted a previously developed scale to assist with
this selection, providing a systematic approach for the comparative analysis of
the perceived quality of explanations generated by various LLMs. We further
demonstrated its applicability through the tax-return process, using it as an
exemplar use case that could benefit from employing an LLM to generate
explanations about tax refund decisions. This was attained through a user study
with 128 survey respondents who were asked to rate different versions of
LLM-generated explanations about tax refund decisions, providing a
methodological basis for selecting the most appropriate LLM. Recognizing the
practical challenges of conducting such a survey, we also began exploring the
automation of this process by attempting to replicate human feedback using a
selection of cutting-edge predictive techniques.

</details>

### [265] [AI Supply Chains: An Emerging Ecosystem of AI Actors, Products, and Services](https://arxiv.org/abs/2504.20185)
*Aspen Hopkins,Sarah H. Cen,Andrew Ilyas,Isabella Struckman,Luis Videgaray,Aleksander Mądry*

Main category: cs.CY

TLDR: 论文首次对AI供应链进行形式化研究，通过两个案例表明其复杂性和对AI开发与监管的影响。


<details>
  <summary>Details</summary>
Motivation: AI供应链的广泛使用及其未被充分理解的复杂性促使研究者探索其形式化模型和实际影响。

Method: 通过历史视角分析AI供应链的演变，将其建模为有向图，并通过两个案例研究（信息传递和上游设计选择）验证模型。

Result: 研究发现AI供应链中信息传递不完美，上游设计选择对下游有显著影响。

Conclusion: AI供应链的社会、经济、监管和技术影响值得进一步研究。

Abstract: The widespread adoption of AI in recent years has led to the emergence of AI
supply chains: complex networks of AI actors contributing models, datasets, and
more to the development of AI products and services. AI supply chains have many
implications yet are poorly understood. In this work, we take a first step
toward a formal study of AI supply chains and their implications, providing two
illustrative case studies indicating that both AI development and regulation
are complicated in the presence of supply chains. We begin by presenting a
brief historical perspective on AI supply chains, discussing how their rise
reflects a longstanding shift towards specialization and outsourcing that
signals the healthy growth of the AI industry. We then model AI supply chains
as directed graphs and demonstrate the power of this abstraction by connecting
examples of AI issues to graph properties. Finally, we examine two case studies
in detail, providing theoretical and empirical results in both. In the first,
we show that information passing (specifically, of explanations) along the AI
supply chains is imperfect, which can result in misunderstandings that have
real-world implications. In the second, we show that upstream design choices
(e.g., by base model providers) have downstream consequences (e.g., on AI
products fine-tuned on the base model). Together, our findings motivate further
study of AI supply chains and their increasingly salient social, economic,
regulatory, and technical implications.

</details>

### [266] [LSTM+Geo with xgBoost Filtering: A Novel Approach for Race and Ethnicity Imputation with Reduced Bias](https://arxiv.org/abs/2504.21259)
*S. Chalavadi,A. Pastor,T. Leitch*

Main category: cs.CY

TLDR: 论文提出LSTM+Geo方法，结合LSTM网络和地理信息，显著提高了种族和民族分类的准确性，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 准确分类种族和民族对分析差异和制定政策至关重要，现有方法存在系统性偏差。

Method: 引入LSTM+Geo，结合LSTM网络和人口普查地理信息，使用选民数据集验证。

Result: LSTM+Geo准确率88.7%，优于传统方法，减少非白人被误分类为白人的概率。

Conclusion: LSTM+Geo可作为独立模型或集成组件，但需谨慎使用。

Abstract: Accurate imputation of race and ethnicity (R&E) is crucial for analyzing
disparities and informing policy. Methods like Bayesian Improved Surname
Geocoding (BISG) are widely used but exhibit limitations, including systematic
misclassification biases linked to socioeconomic status. This paper introduces
LSTM+Geo, a novel approach enhancing Long Short-Term Memory (LSTM) networks
with census tract geolocation information. Using a large voter dataset, we
demonstrate that LSTM+Geo (88.7% accuracy) significantly outperforms standalone
LSTM (86.4%) and Bayesian methods like BISG (82.9%) and BIFSG (86.8%) in
accuracy and F1-score on a held-out validation set. LSTM+Geo reduces the rate
at which non-White individuals are misclassified as White (White FPR 19.3%)
compared to name-only LSTMs (White FPR 24.6%). While sophisticated ensemble
methods incorporating XGBoost achieve the highest overall accuracy (up to
89.4%) and lowest White FPR (17.8%), LSTM+Geo offers strong standalone
performance with improved bias characteristics compared to baseline models.
Integrating LSTM+Geo into an XGBoost ensemble further boosts accuracy,
highlighting its utility as both a standalone model and a component for
advanced systems. We give a caution at the end regarding the appropriate use of
these methods.

</details>

### [267] [Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data](https://arxiv.org/abs/2504.21634)
*Chih-Cheng Rex Yuan,Bow-Yaw Wang*

Main category: cs.CY

TLDR: 论文提出了一种基于差分隐私合成数据的框架，用于审计AI系统的公平性，解决了传统审计中的安全和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 传统公平性审计使用真实数据会引发安全和隐私风险，需要一种既能保护隐私又能有效审计的方法。

Method: 利用差分隐私机制生成合成数据，保留原始数据的统计特性，用于公平性审计。

Result: 在Adult、COMPAS和Diabetes等数据集上验证，合成数据能有效保留真实数据的公平性属性。

Conclusion: 该框架在保护隐私的同时，实现了对AI系统公平性的有效审计，适用于敏感领域。

Abstract: Fairness auditing of AI systems can identify and quantify biases. However,
traditional auditing using real-world data raises security and privacy
concerns. It exposes auditors to security risks as they become custodians of
sensitive information and targets for cyberattacks. Privacy risks arise even
without direct breaches, as data analyses can inadvertently expose confidential
information. To address these, we propose a framework that leverages
differentially private synthetic data to audit the fairness of AI systems. By
applying privacy-preserving mechanisms, it generates synthetic data that
mirrors the statistical properties of the original dataset while ensuring
privacy. This method balances the goal of rigorous fairness auditing and the
need for strong privacy protections. Through experiments on real datasets like
Adult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real
data. By analyzing the alignment and discrepancies between these metrics, we
assess the capacity of synthetic data to preserve the fairness properties of
real data. Our results demonstrate the framework's ability to enable meaningful
fairness evaluations while safeguarding sensitive information, proving its
applicability across critical and sensitive domains.

</details>

### [268] [TRIED: Truly Innovative and Effective Detection Benchmark, developed by WITNESS](https://arxiv.org/abs/2504.21489)
*Shirin Anlen,Zuzanna Wojciak*

Main category: cs.CY

TLDR: 报告指出当前AI检测工具在现实场景中表现不佳，提出TRIED Benchmark框架以评估工具的实用性和创新性，强调需适应多元文化和语言环境。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和虚假合成媒体的兴起威胁全球信息生态，尤其在发展中国家，现有检测工具存在局限性。

Method: 通过前线经验、虚假AI案例和全球咨询，提出TRIED Benchmark框架，评估检测工具的实际影响和创新能力。

Result: 报告为开发者、政策制定者提供实用指导，强调透明、用户中心的解决方案，并纳入社会技术考量。

Conclusion: 采用TRIED Benchmark可推动创新、增强公众信任、提升AI素养，促进全球信息可信度。

Abstract: The rise of generative AI and deceptive synthetic media threatens the global
information ecosystem, especially across the Global Majority. This report from
WITNESS highlights the limitations of current AI detection tools, which often
underperform in real-world scenarios due to challenges related to
explainability, fairness, accessibility, and contextual relevance. In response,
WITNESS introduces the Truly Innovative and Effective AI Detection (TRIED)
Benchmark, a new framework for evaluating detection tools based on their
real-world impact and capacity for innovation. Drawing on frontline
experiences, deceptive AI cases, and global consultations, the report outlines
how detection tools must evolve to become truly innovative and relevant by
meeting diverse linguistic, cultural, and technological contexts. It offers
practical guidance for developers, policymakers, and standards bodies to design
accountable, transparent, and user-centered detection solutions, and
incorporate sociotechnical considerations into future AI standards, procedures
and evaluation frameworks. By adopting the TRIED Benchmark, stakeholders can
drive innovation, safeguard public trust, strengthen AI literacy, and
contribute to a more resilient global information credibility.

</details>

### [269] [Characterizing AI Agents for Alignment and Governance](https://arxiv.org/abs/2504.21848)
*Atoosa Kasirzadeh,Iason Gabriel*

Main category: cs.CY

TLDR: 本文提出了一种AI智能体的四维框架（自主性、效能、目标复杂性和通用性），并构建了不同AI智能体的“代理档案”，以帮助解决技术与非技术治理挑战。


<details>
  <summary>Details</summary>
Motivation: 理解AI智能体的核心特性及其与治理问题的关系，以设计更有效的治理机制。

Method: 提出四维框架（自主性、效能、目标复杂性和通用性），并为每维度定义分级，构建“代理档案”。

Result: 通过框架分析了不同AI智能体的治理挑战，为开发者、政策制定者和公众提供了治理工具。

Conclusion: 该框架有助于设计更符合社会目标的AI治理方法。

Abstract: The creation of effective governance mechanisms for AI agents requires a
deeper understanding of their core properties and how these properties relate
to questions surrounding the deployment and operation of agents in the world.
This paper provides a characterization of AI agents that focuses on four
dimensions: autonomy, efficacy, goal complexity, and generality. We propose
different gradations for each dimension, and argue that each dimension raises
unique questions about the design, operation, and governance of these systems.
Moreover, we draw upon this framework to construct "agentic profiles" for
different kinds of AI agents. These profiles help to illuminate cross-cutting
technical and non-technical governance challenges posed by different classes of
AI agents, ranging from narrow task-specific assistants to highly autonomous
general-purpose systems. By mapping out key axes of variation and continuity,
this framework provides developers, policymakers, and members of the public
with the opportunity to develop governance approaches that better align with
collective societal goals.

</details>

### [270] [Public Opinion and The Rise of Digital Minds: Perceived Risk, Trust, and Regulation Support](https://arxiv.org/abs/2504.21849)
*Justin B. Bullock,Janet V. T. Pauketat,Hsini Huang,Yi-Fan Wang,Jacy Reese Anthis*

Main category: cs.CY

TLDR: 研究探讨公众对AI监管的偏好，发现风险感知和信任在政府、AI公司及技术中起关键作用。


<details>
  <summary>Details</summary>
Motivation: 分析公众对AI监管的态度，为政策制定提供依据。

Method: 使用2023年AIMS全国代表性调查数据，评估信任与风险感知对监管偏好的影响。

Result: 公众普遍支持AI监管，信任政府者倾向监管，信任AI公司者反之；风险感知显著影响政策偏好。

Conclusion: AI治理需平衡公众风险担忧与机构信任，研究为政策制定提供实证基础。

Abstract: Governance institutions must respond to societal risks, including those posed
by generative AI. This study empirically examines how public trust in
institutions and AI technologies, along with perceived risks, shape preferences
for AI regulation. Using the nationally representative 2023 Artificial
Intelligence, Morality, and Sentience (AIMS) survey, we assess trust in
government, AI companies, and AI technologies, as well as public support for
regulatory measures such as slowing AI development or outright bans on advanced
AI. Our findings reveal broad public support for AI regulation, with risk
perception playing a significant role in shaping policy preferences.
Individuals with higher trust in government favor regulation, while those with
greater trust in AI companies and AI technologies are less inclined to support
restrictions. Trust in government and perceived risks significantly predict
preferences for both soft (e.g., slowing development) and strong (e.g., banning
AI systems) regulatory interventions. These results highlight the importance of
public opinion in AI governance. As AI capabilities advance, effective
regulation will require balancing public concerns about risks with trust in
institutions. This study provides a foundational empirical baseline for
policymakers navigating AI governance and underscores the need for further
research into public trust, risk perception, and regulatory strategies in the
evolving AI landscape.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [271] [Light Weight CNN for classification of Brain Tumors from MRI Images](https://arxiv.org/abs/2504.21188)
*Natnael Alemayehu*

Main category: eess.IV

TLDR: 提出一种基于CNN的轻量级深度学习模型，用于MRI图像中脑肿瘤的多分类，准确率达98.78%。


<details>
  <summary>Details</summary>
Motivation: 开发一种高效、低复杂度的自动分类方法，辅助临床早期脑肿瘤诊断。

Method: 采用图像预处理（归一化、数据增强、裁剪）、CNN架构优化及5折交叉验证。

Result: 模型分类准确率为98.78%。

Conclusion: 该方法为临床诊断提供了有效的轻量级解决方案。

Abstract: This study presents a convolutional neural network (CNN)-based approach for
the multi-class classification of brain tumors using magnetic resonance imaging
(MRI) scans. We utilize a publicly available dataset containing MRI images
categorized into four classes: glioma, meningioma, pituitary tumor, and no
tumor. Our primary objective is to build a light weight deep learning model
that can automatically classify brain tumor types with high accuracy. To
achieve this goal, we incorporate image preprocessing steps, including
normalization, data augmentation, and a cropping technique designed to reduce
background noise and emphasize relevant regions. The CNN architecture is
optimized through hyperparameter tuning using Keras Tuner, enabling systematic
exploration of network parameters. To ensure reliable evaluation, we apply
5-fold cross-validation, where each hyperparameter configuration is evaluated
across multiple data splits to mitigate overfitting. Experimental results
demonstrate that the proposed model achieves a classification accuracy of
98.78%, indicating its potential as a diagnostic aid in clinical settings. The
proposed method offers a low-complexity yet effective solution for assisting in
early brain tumor diagnosis.

</details>

### [272] [Gradient Attention Map Based Verification of Deep Convolutional Neural Networks with Application to X-ray Image Datasets](https://arxiv.org/abs/2504.21227)
*Omid Halimi Milani,Amanda Nikho,Lauren Mills,Marouane Tliba,Ahmet Enis Cetin,Mohammed H. Elnagar*

Main category: eess.IV

TLDR: 提出了一种综合验证框架，通过多种互补策略评估深度学习模型在医学影像中的适用性，包括基于梯度注意力图的分析、早期卷积特征图验证和垃圾类分类，以提高模型部署的可靠性和安全性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学影像（如正畸和骨骼成熟度评估）中潜力巨大，但应用于与训练集不同的数据时可能导致不可靠预测，影响患者护理。

Method: 1. 使用梯度注意力图（GAM）分析注意力模式，并通过多种相似性度量（如IoU、Dice等）比较；2. 扩展验证到早期卷积特征图，捕捉结构偏差；3. 在分类模型中引入垃圾类以明确拒绝分布外输入。

Result: 实验结果表明，这些方法能有效识别不适用模型和输入。

Conclusion: 提出的框架提升了深度学习在医学影像中部署的安全性和可靠性。

Abstract: Deep learning models have great potential in medical imaging, including
orthodontics and skeletal maturity assessment. However, applying a model to
data different from its training set can lead to unreliable predictions that
may impact patient care. To address this, we propose a comprehensive
verification framework that evaluates model suitability through multiple
complementary strategies. First, we introduce a Gradient Attention Map
(GAM)-based approach that analyzes attention patterns using Grad-CAM and
compares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine
Similarity, Pearson Correlation, KL Divergence, and Wasserstein Distance.
Second, we extend verification to early convolutional feature maps, capturing
structural mis-alignments missed by attention alone. Finally, we incorporate an
additional garbage class into the classification model to explicitly reject
out-of-distribution inputs. Experimental results demonstrate that these
combined methods effectively identify unsuitable models and inputs, promoting
safer and more reliable deployment of deep learning in medical imaging.

</details>

### [273] [LoC-LIC: Low Complexity Learned Image Coding Using Hierarchical Feature Transforms](https://arxiv.org/abs/2504.21778)
*Ayman A. Ameen,Thomas Richter,André Kaup*

Main category: eess.IV

TLDR: 提出了一种通过分层特征提取降低复杂度的图像压缩方法，显著减少计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 当前学习型图像压缩模型复杂度高，计算资源需求大，限制了其应用范围。

Method: 采用分层特征提取变换，减少高空间分辨率输入/特征图的通道数，同时降低大通道数特征图的空间维度。

Result: 复杂度从1256 kMAC/Pixel降至270 kMAC/Pixel，性能未受影响。

Conclusion: 该方法为学习型图像压缩模型在多种设备上的高效运行提供了可能，推动了图像压缩技术的发展。

Abstract: Current learned image compression models typically exhibit high complexity,
which demands significant computational resources. To overcome these
challenges, we propose an innovative approach that employs hierarchical feature
extraction transforms to significantly reduce complexity while preserving bit
rate reduction efficiency. Our novel architecture achieves this by using fewer
channels for high spatial resolution inputs/feature maps. On the other hand,
feature maps with a large number of channels have reduced spatial dimensions,
thereby cutting down on computational load without sacrificing performance.
This strategy effectively reduces the forward pass complexity from \(1256 \,
\text{kMAC/Pixel}\) to just \(270 \, \text{kMAC/Pixel}\). As a result, the
reduced complexity model can open the way for learned image compression models
to operate efficiently across various devices and pave the way for the
development of new architectures in image compression technology.

</details>