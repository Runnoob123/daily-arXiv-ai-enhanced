<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 34]
- [cs.LG](#cs.LG) [Total: 118]
- [cs.CL](#cs.CL) [Total: 66]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.CV](#cs.CV) [Total: 154]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.GR](#cs.GR) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [quant-ph](#quant-ph) [Total: 7]
- [cs.AR](#cs.AR) [Total: 4]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.RO](#cs.RO) [Total: 15]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.IR](#cs.IR) [Total: 7]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.SD](#cs.SD) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.HC](#cs.HC) [Total: 53]
- [cs.CY](#cs.CY) [Total: 12]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.SE](#cs.SE) [Total: 7]
- [stat.ML](#stat.ML) [Total: 5]
- [eess.AS](#eess.AS) [Total: 4]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]
- [physics.optics](#physics.optics) [Total: 3]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Post Quantum Cryptography (PQC) Signatures Without Trapdoors](https://arxiv.org/abs/2504.14016)
*William J Buchanan*

Main category: cs.CR

TLDR: 论文探讨了当前公钥方法中基于陷门的数字签名（如RSA）的安全性问题，并提出了后量子密码学（PQC）中无陷门的哈希签名（如SPHINCS+）和基于零知识证明的Fiat Shamir签名（如Dilithium）作为更安全的替代方案。


<details>
  <summary>Details</summary>
Motivation: 当前基于陷门的数字签名方法（如RSA）存在被破解的风险，随着后量子密码学的发展，需要更安全的无陷门签名方法。

Method: 采用哈希签名（SPHINCS+）和基于零知识证明的Fiat Shamir签名（Dilithium）作为无陷门的安全签名方法。

Result: 这些方法提供了更强的安全性证明，避免了陷门被发现的潜在风险。

Conclusion: 后量子密码学中的无陷门签名方法是未来数字签名的更安全选择。

Abstract: Some of our current public key methods use a trap door to implement digital
signature methods. This includes the RSA method, which uses Fermat's little
theorem to support the creation and verification of a digital signature. The
problem with a back-door is that the actual trap-door method could, in the end,
be discovered. With the rise of PQC (Post Quantum Cryptography), we will see a
range of methods that will not use trap doors and provide stronger proof of
security. In this case, we use hash-based signatures (as used with SPHINCS+)
and Fiat Shamir signatures using Zero Knowledge Proofs (as used with
Dilithium).

</details>

### [2] [Benchmarking Differentially Private Tabular Data Synthesis](https://arxiv.org/abs/2504.14061)
*Kai Chen,Xiaochen Li,Chen Gong,Ryan McKenna,Tianhao Wang*

Main category: cs.CR

TLDR: 提出了一种用于评估差分隐私表格数据合成方法的基准框架，解决了现有算法选择中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私表格数据合成算法存在数据处理不一致、缺乏深入分析和比较不完整等问题，导致选择困难。

Method: 提出统一评估框架，整合数据预处理、特征选择和合成模块，进行公平全面的比较。

Result: 发现当前最优方法中存在显著的效用-效率权衡，统计方法效用高但效率低，机器学习方法效率高但效用较低。

Conclusion: 通过实验验证和深入分析，提供了对不同策略优缺点的理论见解。

Abstract: Differentially private (DP) tabular data synthesis generates artificial data
that preserves the statistical properties of private data while safeguarding
individual privacy. The emergence of diverse algorithms in recent years has
introduced challenges in practical applications, such as inconsistent data
processing methods, lack of in-depth algorithm analysis, and incomplete
comparisons due to overlapping development timelines. These factors create
significant obstacles to selecting appropriate algorithms.
  In this paper, we address these challenges by proposing a benchmark for
evaluating tabular data synthesis methods. We present a unified evaluation
framework that integrates data preprocessing, feature selection, and synthesis
modules, facilitating fair and comprehensive comparisons. Our evaluation
reveals that a significant utility-efficiency trade-off exists among current
state-of-the-art methods. Some statistical methods are superior in synthesis
utility, but their efficiency is not as good as most machine learning-based
methods. Furthermore, we conduct an in-depth analysis of each module with
experimental validation, offering theoretical insights into the strengths and
limitations of different strategies.

</details>

### [3] [DoomArena: A framework for Testing AI Agents Against Evolving Security Threats](https://arxiv.org/abs/2504.14064)
*Leo Boisvert,Mihir Bansal,Chandra Kiran Reddy Evuru,Gabriel Huang,Abhay Puri,Avinandan Bose,Maryam Fazel,Quentin Cappart,Jason Stanley,Alexandre Lacoste,Alexandre Drouin,Krishnamurthy Dvijotham*

Main category: cs.CR

TLDR: DoomArena是一个用于评估AI代理安全性的框架，具有插件化、可配置和模块化特点，支持多种威胁模型和环境，并能结合多种攻击进行测试。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理的安全性评估缺乏统一框架，DoomArena旨在填补这一空白，提供灵活且全面的安全测试工具。

Method: 基于三个原则设计：插件化、可配置性和模块化，支持多种攻击组合和环境适配。

Result: 应用于SOTA代理时发现：不同代理对威胁模型的脆弱性不同，攻击组合效果显著，LLM防御优于传统方法。

Conclusion: DoomArena为AI代理安全性评估提供了高效工具，揭示了现有代理的脆弱性和防御方法的不足。

Abstract: We present DoomArena, a security evaluation framework for AI agents.
DoomArena is designed on three principles: 1) It is a plug-in framework and
integrates easily into realistic agentic frameworks like BrowserGym (for web
agents) and $\tau$-bench (for tool calling agents); 2) It is configurable and
allows for detailed threat modeling, allowing configuration of specific
components of the agentic framework being attackable, and specifying targets
for the attacker; and 3) It is modular and decouples the development of attacks
from details of the environment in which the agent is deployed, allowing for
the same attacks to be applied across multiple environments. We illustrate
several advantages of our framework, including the ability to adapt to new
threat models and environments easily, the ability to easily combine several
previously published attacks to enable comprehensive and fine-grained security
testing, and the ability to analyze trade-offs between various vulnerabilities
and performance. We apply DoomArena to state-of-the-art (SOTA) web and
tool-calling agents and find a number of surprising results: 1) SOTA agents
have varying levels of vulnerability to different threat models (malicious user
vs malicious environment), and there is no Pareto dominant agent across all
threat models; 2) When multiple attacks are applied to an agent, they often
combine constructively; 3) Guardrail model-based defenses seem to fail, while
defenses based on powerful SOTA LLMs work better. DoomArena is available at
https://github.com/ServiceNow/DoomArena.

</details>

### [4] [Towards Stateless Clients in Ethereum: Benchmarking Verkle Trees and Binary Merkle Trees with SNARKs](https://arxiv.org/abs/2504.14069)
*Jan Oberst*

Main category: cs.CR

TLDR: 论文比较了两种实现以太坊无状态客户端的方法：Verkle树和基于SNARK的二进制Merkle树，发现Verkle树在证明和验证时间上更实用。


<details>
  <summary>Details</summary>
Motivation: 解决以太坊验证器因存储完整状态而面临的硬件需求过高问题，探索无状态客户端的可行性。

Method: 对比Verkle树（使用向量承诺）和二进制Merkle树（结合SNARKs），评估证明时间、见证大小和验证时间。

Result: Verkle树的证明和验证时间为秒级，证明大小为1MB；SNARK-based Merkle树证明生成慢但验证快。

Conclusion: Verkle树更适合以太坊的无状态未来，但两种方法均对减轻节点负担提供了有价值的见解。

Abstract: Ethereum, the leading platform for decentralized applications, faces
challenges in maintaining decentralization due to the significant hardware
requirements for validators to store Ethereum's entire state. To address this,
the concept of stateless clients is under exploration, enabling validators to
verify transactions using cryptographic witnesses rather than the full state.
This paper compares two approaches currently being discussed for achieving
statelessness: Verkle trees utilizing vector commitments and binary Merkle
trees combined with SNARKs. Benchmarks are performed to evaluate proving time,
witness size, and verification time. The results reveal that the Verkle tree
implementation used for benchmarking offers proving and verification times on
the order of seconds and proof sizes on the order of one MB. The SNARK-based
Merkle trees exhibit slow proof generation times, while offering constant and
fast verification time. Overall, the results indicate for Verkle trees to
provide a more practical solution for Ethereum's stateless future, but both
methods offer valuable insights into reducing the state burden on Ethereum
nodes. We make the code used for benchmarking available on GitHub.

</details>

### [5] [Detecting Zero-Day Web Attacks with an Ensemble of LSTM, GRU, and Stacked Autoencoders](https://arxiv.org/abs/2504.14122)
*Vahid Babaey,Hamid Reza Faragardi*

Main category: cs.CR

TLDR: 论文提出了一种智能系统，采用新型的一类集成方法检测零日网络攻击，结合三种自编码器架构，显著提高了检测准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着网络攻击日益复杂和普遍，传统安全方法难以检测零日攻击，导致用户数据风险增加。减少人工干预可提升安全任务的可靠性。

Method: 使用三种自编码器（LSTM、GRU和堆叠自编码器）的集成方法，结合新型标记化策略，将正常网络请求转换为结构化数字序列，通过压缩潜在表示检测异常活动。

Result: 实验显示，该方法在检测零日攻击中表现优异：准确率97.58%，召回率97.52%，特异性99.76%，精确度99.99%，误报率仅0.2%。

Conclusion: 该方法显著提升了网络安全性，为实际应用提供了高准确性和低误报率的零日攻击检测方案。

Abstract: The rapid growth in web-based services has significantly increased security
risks related to user information, as web-based attacks become increasingly
sophisticated and prevalent. Traditional security methods frequently struggle
to detect previously unknown (zero-day) web attacks, putting sensitive user
data at significant risk. Additionally, reducing human intervention in web
security tasks can minimize errors and enhance reliability. This paper
introduces an intelligent system designed to detect zero-day web attacks using
a novel one-class ensemble method consisting of three distinct autoencoder
architectures: LSTM autoencoder, GRU autoencoder, and stacked autoencoder. Our
approach employs a novel tokenization strategy to convert normal web requests
into structured numeric sequences, enabling the ensemble model to effectively
identify anomalous activities by uniquely concatenating and compressing the
latent representations from each autoencoder. The proposed method efficiently
detects unknown web attacks while effectively addressing common limitations of
previous methods, such as high memory consumption and excessive false positive
rates. Extensive experimental evaluations demonstrate the superiority of our
proposed ensemble, achieving remarkable detection metrics: 97.58% accuracy,
97.52% recall, 99.76% specificity, and 99.99% precision, with an exceptionally
low false positive rate of 0.2%. These results underscore our method's
significant potential in enhancing real-world web security through accurate and
reliable detection of web-based attacks.

</details>

### [6] [ROFBS$α$: Real Time Backup System Decoupled from ML Based Ransomware Detection](https://arxiv.org/abs/2504.14162)
*Kosuke Higuchi,Ryotaro Kobayashi*

Main category: cs.CR

TLDR: ROFBSα是一种新型防御架构，通过异步设计分离备份与检测任务，提升勒索软件检测效率。


<details>
  <summary>Details</summary>
Motivation: 解决基于机器学习的勒索软件检测器在检测延迟上的问题。

Method: 采用异步设计，利用eBPF监控文件打开事件，独立运行备份过程。

Result: 在AvosLocker、Conti和IceFire三种勒索软件测试中，ROFBSα实现了高备份成功率和快速检测。

Conclusion: ROFBSα显著提升了防御效果，但对极速加密的勒索软件仍需进一步改进。

Abstract: This study introduces ROFBS$\alpha$, a new defense architecture that
addresses delays in detection in ransomware detectors based on machine
learning. It builds on our earlier Real Time Open File Backup System, ROFBS, by
adopting an asynchronous design that separates backup operations from detection
tasks. By using eBPF to monitor file open events and running the backup process
independently, the system avoids performance limitations when detection and
protection contend for resources. We evaluated ROFBS$\alpha$ against three
ransomware strains, AvosLocker, Conti, and IceFire. The evaluation measured the
number of files encrypted, the number of files successfully backed up, the
ratio of backups to encrypted files, and the overall detection latency. The
results show that ROFBS$\alpha$ achieves high backup success rates and faster
detection while adding minimal extra load to the system. However, defending
against ransomware that encrypts files extremely quickly remains an open
challenge that will require further enhancements.

</details>

### [7] [From Cyber Security Incident Management to Cyber Security Crisis Management in the European Union](https://arxiv.org/abs/2504.14220)
*Jukka Ruohonen,Kalle Rindell,Simone Busetti*

Main category: cs.CR

TLDR: 论文探讨了欧盟如何将网络安全事件与网络安全危机联系起来，并分析了新法律对事件报告的要求及危机管理的现状。


<details>
  <summary>Details</summary>
Motivation: 研究欧盟新网络安全法律对网络安全事件与危机的定义及其管理机制，填补相关研究空白。

Method: 通过分析欧盟新法律及相关政策文件，探讨网络安全危机与事件的关系及管理框架。

Result: 欧盟将网络安全危机定义为大规模事件，新法律增加了事件报告要求，但危机管理机制仍不明确。

Conclusion: 论文为网络安全事件管理研究提供了法律视角，为进一步理论和实证研究奠定了基础。

Abstract: Incident management is a classical topic in cyber security. Recently, the
European Union (EU) has started to consider also the relation between cyber
security incidents and cyber security crises. These considerations and
preparations, including those specified in the EU's new cyber security laws,
constitute the paper's topic. According to an analysis of the laws and
associated policy documents, (i) cyber security crises are equated in the EU to
large-scale cyber security incidents that either exceed a handling capacity of
a single member state or affect at least two member states. For this and other
purposes, (ii) the new laws substantially increase mandatory reporting about
cyber security incidents, including but not limited to the large-scale
incidents. Despite the laws and new governance bodies established by them,
however, (iii) the working of actual cyber security crisis management remains
unclear particularly at the EU-level. With these policy research results, the
paper advances the domain of cyber security incident management research by
elaborating how European law perceives cyber security crises and their relation
to cyber security incidents, paving the way for many relevant further research
topics with practical relevance, whether theoretical, conceptual, or empirical.

</details>

### [8] [The Dark Side of the Web: Towards Understanding Various Data Sources in Cyber Threat Intelligence](https://arxiv.org/abs/2504.14235)
*Saskia Schröer,Noé Canevascini,Irdin Pekaric,Philine Widmer,Pavel Laskov*

Main category: cs.CR

TLDR: 论文分析了660万帖子、340万消息和12万个暗网网站，结合NLP工具提取网络威胁情报（CTI），发现20%数据与CTI相关，主要集中在战略讨论而非技术细节。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分利用‘第一手’数据源（如地下论坛、暗网网站），因其数据难以获取或处理。

Method: 结合NLP工具分析‘第一手’数据，解决内容过滤、分类及主题差异问题。

Result: 20%数据与CTI相关，暗网网站主要讨论信用卡犯罪，地下论坛和聊天频道则多涉及账户销售。

Conclusion: 不同平台用于不同复杂度和风险的犯罪活动，主题多样性在地下论坛和聊天频道更高。

Abstract: Cyber threats have become increasingly prevalent and sophisticated. Prior
work has extracted actionable cyber threat intelligence (CTI), such as
indicators of compromise, tactics, techniques, and procedures (TTPs), or threat
feeds from various sources: open source data (e.g., social networks), internal
intelligence (e.g., log data), and ``first-hand'' communications from
cybercriminals (e.g., underground forums, chats, darknet websites). However,
"first-hand" data sources remain underutilized because it is difficult to
access or scrape their data.
  In this work, we analyze (i) 6.6 million posts, (ii) 3.4 million messages,
and (iii) 120,000 darknet websites. We combine NLP tools to address several
challenges in analyzing such data. First, even on dedicated platforms, only
some content is CTI-relevant, requiring effective filtering. Second,
"first-hand" data can be CTI-relevant from a technical or strategic viewpoint.
We demonstrate how to organize content along this distinction. Third, we
describe the topics discussed and how "first-hand" data sources differ from
each other. According to our filtering, 20% of our sample is CTI-relevant. Most
of the CTI-relevant data focuses on strategic rather than technical
discussions. Credit card-related crime is the most prevalent topic on darknet
websites. On underground forums and chat channels, account and subscription
selling is discussed most. Topic diversity is higher on underground forums and
chat channels than on darknet websites. Our analyses suggest that different
platforms may be used for activities with varying complexity and risks for
criminals.

</details>

### [9] [ScaloWork: Useful Proof-of-Work with Distributed Pool Mining](https://arxiv.org/abs/2504.14328)
*Diptendu Chatterjee,Avishek Majumder,Subhra Mazumdar*

Main category: cs.CR

TLDR: ScaloWork提出了一种新的有用工作量证明（PoW）框架，用支配集问题替代哈希计算，解决了比特币区块链能源浪费问题，并在效率和解决方案质量上优于Chrisimos。


<details>
  <summary>Details</summary>
Motivation: 比特币区块链的哈希PoW消耗大量能源且无实际用途，研究者希望将浪费的能源用于解决有实际意义的问题，如网络中的最小支配集（MDS）。

Method: ScaloWork基于图同构性质，用支配集问题决定区块提议者，并提出分布式计算方法以支持大规模图处理。

Result: ScaloWork在安全性和效率上优于Chrisimos，解决了自由骑手问题，并通过原型验证了其优越性。

Conclusion: ScaloWork为比特币区块链提供了一种更高效、公平且实用的PoW替代方案。

Abstract: Bitcoin blockchain uses hash-based Proof-of-Work (PoW) that prevents unwanted
participants from hogging the network resources. Anyone entering the mining
game has to prove that they have expended a specific amount of computational
power. However, the most popular Bitcoin blockchain consumes 175.87 TWh of
electrical energy annually, and most of this energy is wasted on hash
calculations, which serve no additional purpose. Several studies have explored
re-purposing the wasted energy by replacing the hash function with meaningful
computational problems that have practical applications. Minimum Dominating Set
(MDS) in networks has numerous real-life applications. Building on this
concept, Chrisimos [TrustCom '23] was proposed to replace hash-based PoW with
the computation of a dominating set on real-life graph instances. However,
Chrisimos has several drawbacks regarding efficiency and solution quality. This
work presents a new framework for Useful PoW, ScaloWork, that decides the block
proposer for the Bitcoin blockchain based on the solution for the dominating
set problem. ScaloWork relies on the property of graph isomorphism and
guarantees solution extractability. We also propose a distributed approach for
calculating the dominating set, allowing miners to collaborate in a pool. This
enables ScaloWork to handle larger graphs relevant to real-life applications,
thereby enhancing scalability. Our framework also eliminates the problem of
free-riders, ensuring fairness in the distribution of block rewards. We perform
a detailed security analysis of our framework and prove our scheme as secure as
hash-based PoW. We implement a prototype of our framework, and the results show
that our system outperforms Chrisimos in all aspects.

</details>

### [10] [Publicly Verifiable Secret Sharing: Generic Constructions and Lattice-Based Instantiations in the Standard Model](https://arxiv.org/abs/2504.14381)
*Pham Nhat Minh,Khoa Nguyen,Willy Susilo,Khuong Nguyen-An*

Main category: cs.CR

TLDR: 本文提出了一种基于LWE假设的标准模型下的PVSS通用构造，首次实现了后量子安全的PVSS。


<details>
  <summary>Details</summary>
Motivation: 现有的PVSS协议要么依赖随机预言模型，要么基于量子易受攻击的假设（如因数分解或离散对数），缺乏后量子安全性。

Method: 提出了一种通用构造，可在标准模型下基于LWE假设实例化。

Result: 首次实现了后量子安全的PVSS，并具备合理的渐近效率。

Conclusion: 该工作填补了PVSS在后量子安全领域的空白，为相关应用提供了更安全的解决方案。

Abstract: Publicly verifiable secret sharing (PVSS) allows a dealer to share a secret
among a set of shareholders so that the secret can be reconstructed later from
any set of qualified participants. In addition, any public verifier should be
able to check the correctness of the sharing and reconstruction process. PVSS
has been demonstrated to yield various applications, such as e-voting,
distributed key generation, decentralized random number generation protocols,
and multi-party computation. Although many concrete PVSS protocols have been
proposed, their security is either proven in the random oracle model or relies
on quantum-vulnerable assumptions such as factoring or discrete logarithm. In
this work, we put forward a generic construction for PVSS that can be
instantiated in the standard model under the Learning With Errors (LWE)
assumption. Our instantiation provides the first post-quantum PVSS in the
standard model, with a reasonable level of asymptotic efficiency.

</details>

### [11] [How Do Mobile Applications Enhance Security? An Exploratory Analysis of Use Cases and Provided Information](https://arxiv.org/abs/2504.14421)
*Irdin Pekaric,Clemens Sauerwein,Simon Laichner,Ruth Breu*

Main category: cs.CR

TLDR: 本文系统分析了Android和iOS平台上最流行的20款移动安全应用，总结了六大主要用例及其提供的安全信息。


<details>
  <summary>Details</summary>
Motivation: 移动应用的普及带来了新的安全威胁，但目前缺乏对这些安全应用的全面研究。

Method: 从应用商店收集410款应用，筛选出20款最广泛使用的进行分析和分类。

Result: 识别出六大主要用例，并展示了这些应用提供的多样化安全信息。

Conclusion: 研究填补了移动安全应用领域的知识空白，为未来研究提供了基础。

Abstract: The ubiquity of mobile applications has increased dramatically in recent
years, opening up new opportunities for cyber attackers and heightening
security concerns in the mobile ecosystem. As a result, researchers and
practitioners have intensified their research into improving the security and
privacy of mobile applications. At the same time, more and more mobile
applications have appeared on the market that address the aforementioned
security issues. However, both academia and industry currently lack a
comprehensive overview of these mobile security applications for Android and
iOS platforms, including their respective use cases and the security
information they provide.
  To address this gap, we systematically collected a total of 410 mobile
applications from both the App and Play Store. Then, we identified the 20 most
widely utilized mobile security applications on both platforms that were
analyzed and classified. Our results show six primary use cases and a wide
range of security information provided by these applications, thus supporting
the core functionalities for ensuring mobile security.

</details>

### [12] [Application of Deep Reinforcement Learning for Intrusion Detection in Internet of Things: A Systematic Review](https://arxiv.org/abs/2504.14436)
*Saeid Jamshidia,Amin Nikanjama,Kawser Wazed Nafia,Foutse Khomha,Rasoul Rastab*

Main category: cs.CR

TLDR: 本文系统回顾了过去十年中深度强化学习（DRL）在物联网（IoT）入侵检测系统（IDS）中的应用，展示了DRL如何提升IDS的动态适应性和威胁检测能力。


<details>
  <summary>Details</summary>
Motivation: 物联网的快速发展带来了安全挑战，传统IDS难以适应其动态性和复杂性，因此需要探索DRL等新技术以提升IDS的适应性。

Method: 通过系统分析过去十年的研究，总结了DRL在IoT IDS中的应用及其技术进展。

Result: 研究发现DRL能显著提升IDS的威胁检测准确性和适应性，减少误报，同时指出了当前研究的不足。

Conclusion: 未来研究需关注数据集多样性、可重复性及与新兴IoT技术的整合，以开发更动态的IDS解决方案。

Abstract: The Internet of Things (IoT) has significantly expanded the digital
landscape, interconnecting an unprecedented array of devices, from home
appliances to industrial equipment. This growth enhances functionality, e.g.,
automation, remote monitoring, and control, and introduces substantial security
challenges, especially in defending these devices against cyber threats.
Intrusion Detection Systems (IDS) are crucial for securing IoT; however,
traditional IDS often struggle to adapt to IoT networks' dynamic and evolving
nature and threat patterns. A potential solution is using Deep Reinforcement
Learning (DRL) to enhance IDS adaptability, enabling them to learn from and
react to their operational environment dynamically. This systematic review
examines the application of DRL to enhance IDS in IoT settings, covering
research from the past ten years. This review underscores the state-of-the-art
DRL techniques employed to improve adaptive threat detection and real-time
security across IoT domains by analyzing various studies. Our findings
demonstrate that DRL significantly enhances IDS capabilities by enabling
systems to learn and adapt from their operational environment. This
adaptability allows IDS to improve threat detection accuracy and minimize false
positives, making it more effective in identifying genuine threats while
reducing unnecessary alerts. Additionally, this systematic review identifies
critical research gaps and future research directions, emphasizing the
necessity for more diverse datasets, enhanced reproducibility, and improved
integration with emerging IoT technologies. This review aims to foster the
development of dynamic and adaptive IDS solutions essential for protecting IoT
networks against sophisticated cyber threats.

</details>

### [13] [Fast Plaintext-Ciphertext Matrix Multiplication from Additively Homomorphic Encryption](https://arxiv.org/abs/2504.14497)
*Krishna Sai Tarun Ramapragada,Utsav Banerjee*

Main category: cs.CR

TLDR: 本文提出了一种基于未打包加法同态加密（AHE）的高效明文-密文矩阵乘法（PC-MM）方法，通过应用Cussen的压缩-重构算法，在加密环境下实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前明文-密文矩阵乘法（PC-MM）的研究主要集中在完全同态加密（FHE）方案上，而基于未打包AHE的方案仅依赖于传统方法（如Strassen算法），缺乏效率优化。本文旨在填补这一空白。

Method: 提出了一种基于未打包AHE的PC-MM方法，利用Cussen的压缩-重构算法在加密环境中实现矩阵乘法，并通过椭圆曲线ElGamal加密方案在Raspberry Pi 5平台上进行实验验证。

Result: 实验结果表明，该方法在元素位宽较小的大矩阵上比现有技术快一个数量级。

Conclusion: 该方法在资源受限环境中仍能高效运行，是隐私保护计算的优秀候选方案。

Abstract: Plaintext-ciphertext matrix multiplication (PC-MM) is an indispensable tool
in privacy-preserving computations such as secure machine learning and
encrypted signal processing. While there are many established algorithms for
plaintext-plaintext matrix multiplication, efficiently computing
plaintext-ciphertext (and ciphertext-ciphertext) matrix multiplication is an
active area of research which has received a lot of attention. Recent
literature have explored various techniques for privacy-preserving matrix
multiplication using fully homomorphic encryption (FHE) schemes with ciphertext
packing and Single Instruction Multiple Data (SIMD) processing. On the other
hand, there hasn't been any attempt to speed up PC-MM using unpacked additively
homomorphic encryption (AHE) schemes beyond the schoolbook method and
Strassen's algorithm for matrix multiplication. In this work, we propose an
efficient PC-MM from unpacked AHE, which applies Cussen's
compression-reconstruction algorithm for plaintext-plaintext matrix
multiplication in the encrypted setting. We experimentally validate our
proposed technique using a concrete instantiation with the additively
homomorphic elliptic curve ElGamal encryption scheme and its software
implementation on a Raspberry Pi 5 edge computing platform. Our proposed
approach achieves up to an order of magnitude speedup compared to
state-of-the-art for large matrices with relatively small element bit-widths.
Extensive measurement results demonstrate that our fast PC-MM is an excellent
candidate for efficient privacy-preserving computation even in
resource-constrained environments.

</details>

### [14] [Towards Model Resistant to Transferable Adversarial Examples via Trigger Activation](https://arxiv.org/abs/2504.14541)
*Yi Yu,Song Xia,Xun Lin,Chenqi Kong,Wenhan Yang,Shijian Lu,Yap-Peng Tan,Alex C. Kot*

Main category: cs.CR

TLDR: 本文提出了一种新颖的训练范式，通过触发激活模型增强对抗样本的可转移性防御，同时保持对干净数据的高效预测。


<details>
  <summary>Details</summary>
Motivation: 对抗样本的可转移性对深度神经网络构成严重威胁，现有防御方法存在效率低、效果差或影响干净数据性能的问题。

Method: 提出一种触发激活模型，通过固定触发器在干净数据上随机猜测，在触发数据上准确预测，并通过联合优化触发器和模型提升鲁棒性。

Result: 实验证明该方法在多种数据集和攻击方法下均表现出优越的防御效果。

Conclusion: 触发激活模型为对抗样本的可转移性问题提供了一种高效且有效的解决方案。

Abstract: Adversarial examples, characterized by imperceptible perturbations, pose
significant threats to deep neural networks by misleading their predictions. A
critical aspect of these examples is their transferability, allowing them to
deceive {unseen} models in black-box scenarios. Despite the widespread
exploration of defense methods, including those on transferability, they show
limitations: inefficient deployment, ineffective defense, and degraded
performance on clean images. In this work, we introduce a novel training
paradigm aimed at enhancing robustness against transferable adversarial
examples (TAEs) in a more efficient and effective way. We propose a model that
exhibits random guessing behavior when presented with clean data
$\boldsymbol{x}$ as input, and generates accurate predictions when with
triggered data $\boldsymbol{x}+\boldsymbol{\tau}$. Importantly, the trigger
$\boldsymbol{\tau}$ remains constant for all data instances. We refer to these
models as \textbf{models with trigger activation}. We are surprised to find
that these models exhibit certain robustness against TAEs. Through the
consideration of first-order gradients, we provide a theoretical analysis of
this robustness. Moreover, through the joint optimization of the learnable
trigger and the model, we achieve improved robustness to transferable attacks.
Extensive experiments conducted across diverse datasets, evaluating a variety
of attacking methods, underscore the effectiveness and superiority of our
approach.

</details>

### [15] [REDEditing: Relationship-Driven Precise Backdoor Poisoning on Text-to-Image Diffusion Models](https://arxiv.org/abs/2504.14554)
*Chongye Guo,Jinhu Fu,Junfeng Fang,Kun Wang,Guorui Feng*

Main category: cs.CR

TLDR: 论文提出了一种基于模型编辑的无训练后门攻击方法REDEditing，通过概念重绑定实现高效攻击，同时保持良性生成完整性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的快速发展，文本到图像（T2I）模型的安全性尤为重要，尤其是后门攻击的威胁。本文旨在揭示模型编辑技术对图像生成模型的安全风险。

Method: 提出了一种关系驱动的精确后门攻击方法REDEditing，基于等效属性对齐和隐蔽中毒原则，采用等效关系检索和联合属性转移技术。

Result: REDEditing的攻击成功率比现有方法高11%，仅需一行代码即可提升输出自然性和后门隐蔽性24%。

Conclusion: 本文揭示了可编辑图像生成模型的安全漏洞，呼吁提高对此类风险的认识。

Abstract: The rapid advancement of generative AI highlights the importance of
text-to-image (T2I) security, particularly with the threat of backdoor
poisoning. Timely disclosure and mitigation of security vulnerabilities in T2I
models are crucial for ensuring the safe deployment of generative models. We
explore a novel training-free backdoor poisoning paradigm through model
editing, which is recently employed for knowledge updating in large language
models. Nevertheless, we reveal the potential security risks posed by model
editing techniques to image generation models. In this work, we establish the
principles for backdoor attacks based on model editing, and propose a
relationship-driven precise backdoor poisoning method, REDEditing. Drawing on
the principles of equivalent-attribute alignment and stealthy poisoning, we
develop an equivalent relationship retrieval and joint-attribute transfer
approach that ensures consistent backdoor image generation through concept
rebinding. A knowledge isolation constraint is proposed to preserve benign
generation integrity. Our method achieves an 11\% higher attack success rate
compared to state-of-the-art approaches. Remarkably, adding just one line of
code enhances output naturalness while improving backdoor stealthiness by 24\%.
This work aims to heighten awareness regarding this security vulnerability in
editable image generation models.

</details>

### [16] [BLACKOUT: Data-Oblivious Computation with Blinded Capabilities](https://arxiv.org/abs/2504.14654)
*Hossam ElAtali,Merve Gülmez,Thomas Nyman,N. Asokan*

Main category: cs.CR

TLDR: 论文提出BLACKOUT，通过扩展CHERI能力架构实现数据无关编程，以同时解决内存安全和侧信道问题。


<details>
  <summary>Details</summary>
Motivation: 内存安全编程语言虽能减少内存安全漏洞，但增加了实现侧信道抵抗代码的难度，需同时解决这两大挑战。

Method: 扩展CHERI能力架构，引入盲能力支持数据无关计算，并在FPGA软核上实现BLACKOUT，同时扩展编译器和操作系统支持。

Result: BLACKOUT确保盲能力操作的内存安全分配、使用和回收，性能仅比基线CHERI-Toooba处理器下降1.5%。

Conclusion: BLACKOUT为编写侧信道抵抗代码提供了更简单的方法，同时保持高性能。

Abstract: Lack of memory-safety and exposure to side channels are two prominent,
persistent challenges for the secure implementation of software. Memory-safe
programming languages promise to significantly reduce the prevalence of
memory-safety bugs, but make it more difficult to implement
side-channel-resistant code. We aim to address both memory-safety and
side-channel resistance by augmenting memory-safe hardware with the ability for
data-oblivious programming. We describe an extension to the CHERI capability
architecture to provide blinded capabilities that allow data-oblivious
computation to be carried out by userspace tasks. We also present BLACKOUT, our
realization of blinded capabilities on a FPGA softcore based on the speculative
out-of-order CHERI-Toooba processor and extend the CHERI-enabled Clang/LLVM
compiler and the CheriBSD operating system with support for blinded
capabilities. BLACKOUT makes writing side-channel-resistant code easier by
making non-data-oblivious operations via blinded capabilities explicitly fault.
Through rigorous evaluation we show that BLACKOUT ensures memory operated on
through blinded capabilities is securely allocated, used, and reclaimed and
demonstrate that, in benchmarks comparable to those used by previous work,
BLACKOUT imposes only a small performance degradation (1.5% geometric mean)
compared to the baseline CHERI-Toooba processor.

</details>

### [17] [Establishing Workload Identity for Zero Trust CI/CD: From Secrets to SPIFFE-Based Authentication](https://arxiv.org/abs/2504.14760)
*Surya Teja Avirneni*

Main category: cs.CR

TLDR: 论文探讨了CI/CD系统中静态凭证的风险，提出使用OIDC联合和SPIFFE框架实现动态身份验证，以增强安全性。


<details>
  <summary>Details</summary>
Motivation: 企业CI/CD系统依赖静态凭证和临时凭据，存在供应链攻击风险，需要更安全的身份验证机制。

Method: 采用OpenID Connect (OIDC)联合和SPIFFE框架，实现运行时颁发、平台无关的非人类身份模型。

Result: SPIFFE支持策略对齐、工作负载认证和双向认证，为CI/CD提供更强的身份验证。

Conclusion: 下一步将推动基于策略的访问控制，构建CI/CD的零信任架构基础。

Abstract: CI/CD systems have become privileged automation agents in modern
infrastructure, but their identity is still based on secrets or temporary
credentials passed between systems. In enterprise environments, these platforms
are centralized and shared across teams, often with broad cloud permissions and
limited isolation. These conditions introduce risk, especially in the era of
supply chain attacks, where implicit trust and static credentials leave systems
exposed. This paper describes the shift from static credentials to OpenID
Connect (OIDC) federation, and introduces SPIFFE (Secure Production Identity
Framework for Everyone) as a runtime-issued, platform-neutral identity model
for non-human actors. SPIFFE decouples identity from infrastructure, enabling
strong, portable authentication across job runners and deployed workloads. We
show how SPIFFE identities support policy alignment, workload attestation, and
mutual authentication. The paper concludes by outlining next steps in enabling
policy-based access, forming the basis of a broader Zero Trust architecture for
CI/CD.

</details>

### [18] [Decoupling Identity from Access: Credential Broker Patterns for Secure CI/CD](https://arxiv.org/abs/2504.14761)
*Surya Teja Avirneni*

Main category: cs.CR

TLDR: 论文探讨了如何在CI/CD系统中通过凭证代理实现身份与访问的分离，利用运行时颁发的可验证身份（如SPIFFE）生成短期、策略驱动的凭证。


<details>
  <summary>Details</summary>
Motivation: 减少静态权限、提高可审计性，并支持零信任目标在部署工作流中的应用。

Method: 介绍实用设计模式，包括即时颁发令牌的代理、应用访问策略的代理以及跨信任域操作的代理。

Result: 实现了更安全的CI/CD身份架构，支持零信任目标。

Conclusion: 凭证代理结合可验证身份为CI/CD系统提供了更灵活、安全的访问控制方案。

Abstract: Credential brokers offer a way to separate identity from access in CI/CD
systems. This paper shows how verifiable identities issued at runtime, such as
those from SPIFFE, can be used with brokers to enable short-lived,
policy-driven credentials for pipelines and workloads. We walk through
practical design patterns, including brokers that issue tokens just in time,
apply access policies, and operate across trust domains. These ideas help
reduce static permissions, improve auditability, and support Zero Trust goals
in deployment workflows. This is the second paper in a three-part series on
secure CI/CD identity architecture.

</details>

### [19] [Intent-Aware Authorization for Zero Trust CI/CD](https://arxiv.org/abs/2504.14777)
*Surya Teja Avirneni*

Main category: cs.CR

TLDR: 本文介绍了零信任CI/CD系统中的意图感知授权机制，通过运行时上下文、理由和人工审批等多信号决策访问权限。


<details>
  <summary>Details</summary>
Motivation: 身份验证仅能确定请求者身份，但需更多信号决定是否授权访问。

Method: 采用控制循环架构，结合OPA和Cedar等策略引擎评估运行时上下文、理由及人工审批，基于SPIFFE的工作负载身份和凭证代理实现细粒度授权。

Result: 系统实现了可审计的细粒度授权。

Conclusion: 这是零信任CI/CD设计模式系列的第三篇论文，进一步完善了授权机制。

Abstract: This paper introduces intent-aware authorization for Zero Trust CI/CD
systems. Identity establishes who is making the request, but additional signals
are required to decide whether access should be granted. We describe a control
loop architecture where policy engines such as OPA and Cedar evaluate runtime
context, justification, and human approvals before issuing access credentials.
The system builds on SPIFFE-based workload identity and credential brokers, and
enables fine-grained, auditable authorization. This is the third paper in a
series on Zero Trust CI/CD design patterns.

</details>

### [20] [vApps: Verifiable Applications at Internet Scale](https://arxiv.org/abs/2504.14809)
*Isaac Zhang,Ryan Zarick,Bryan Pellegrino,Tan Li,Daniel Wong,Thomas Kim,Uma Roy,John Guibas,Kshitij Kulkarni*

Main category: cs.CR

TLDR: 论文提出了一种名为vApps的新型区块链开发框架，通过统一的Rust DSL和SDK简化可验证区块链应用的开发，显著提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 区块链技术面临可扩展性、高交易成本和跨层验证逻辑复杂等问题，阻碍了广泛应用。vApps旨在解决这些问题。

Method: vApps采用Rust DSL和模块化SDK，支持验证、证明生成和跨链连接，并利用硬件加速优化性能。

Result: 实验显示，vApps性能显著提升：Rust执行比EVM快832倍，预编译电路加速证明95%，GPU提升吞吐量30倍，递归压缩证明大小230倍。

Conclusion: vApps通过模块化架构和高效性能，为构建可信、可验证的互联网规模应用环境提供了新途径。

Abstract: Blockchain technology promises decentralized, trustless, and interoperable
infrastructure. However, widespread adoption remains hindered by issues such as
limited scalability, high transaction costs, and the complexity of maintaining
coherent verification logic across different blockchain layers. This paper
introduces Verifiable Applications (vApps), a novel development framework
designed to streamline the creation and deployment of verifiable blockchain
computing applications. vApps offer a unified Rust-based Domain-Specific
Language (DSL) within a comprehensive SDK, featuring modular abstractions for
verification, proof generation, and inter-chain connectivity. This eases the
developer's burden in securing diverse software components, allowing them to
focus on application logic. The DSL also ensures that applications can
automatically take advantage of specialized precompiles and hardware
acceleration to achieve consistently high performance with minimal developer
effort, as demonstrated by benchmark results for zero-knowledge virtual
machines (zkVMs). Experiments show that native Rust execution eliminates
interpretation overhead, delivering up to an 832x cycle count improvement
compared to EVM-based approaches. Precompiled circuits accelerate proving by
over 95%, while GPU acceleration boosts throughput by up to 30x and recursion
compresses proof size by up to 230x, enabling succinct and efficient
verification. The framework also supports seamless integration with Web2 and
Web3 systems, enabling developers to focus solely on their application logic.
Through modular architecture, robust security guarantees, and composability,
vApps pave the way toward a trust-minimized and verifiable Internet-scale
application environment.

</details>

### [21] [CSI2Dig: Recovering Digit Content from Smartphone Loudspeakers Using Channel State Information](https://arxiv.org/abs/2504.14812)
*Yangyang Gu,Xianglong Li,Haolin Wu,Jing Chen,Kun He,Ruiying Du,Cong Wu*

Main category: cs.CR

TLDR: CSI2Dig利用WiFi信号中的CSI信息，通过电磁干扰恢复智能手机扬声器播放的数字内容，准确率达72.97%。


<details>
  <summary>Details</summary>
Motivation: 现有方案需昂贵设备或依赖恶意软件，且仅限于近距离信号采集，亟需一种更高效、低成本的方法。

Method: 结合对比学习和去噪自编码器，设计双分支自编码网络（TS-Net），从CSI数据的时空维度提取特征。

Result: 在多种设备、距离和音量设置下，方案准确率达72.97%。

Conclusion: CSI2Dig为数字内容恢复提供了一种低成本、高效的解决方案，具有实际应用潜力。

Abstract: Eavesdropping on sounds emitted by mobile device loudspeakers can capture
sensitive digital information, such as SMS verification codes, credit card
numbers, and withdrawal passwords, which poses significant security risks.
Existing schemes either require expensive specialized equipment, rely on
spyware, or are limited to close-range signal acquisition. In this paper, we
propose a scheme, CSI2Dig, for recovering digit content from Channel State
Information (CSI) when digits are played through a smartphone loudspeaker. We
observe that the electromagnetic interference caused by the audio signals from
the loudspeaker affects the WiFi signals emitted by the phone's WiFi antenna.
Building upon contrastive learning and denoising autoencoders, we develop a
two-branch autoencoder network designed to amplify the impact of this
electromagnetic interference on CSI. For feature extraction, we introduce the
TS-Net, a model that captures relevant features from both the temporal and
spatial dimensions of the CSI data. We evaluate our scheme across various
devices, distances, volumes, and other settings. Experimental results
demonstrate that our scheme can achieve an accuracy of 72.97%.

</details>

### [22] [Protecting Your Voice: Temporal-aware Robust Watermarking](https://arxiv.org/abs/2504.14832)
*Yue Li,Weizhi Liu,Dongdong Lin*

Main category: cs.CR

TLDR: 提出了一种时间感知的鲁棒水印方法（True），用于保护语音和歌声，同时平衡鲁棒性和保真度。


<details>
  <summary>Details</summary>
Motivation: 生成模型的快速发展导致合成声音的真实性模糊，现有频域水印方法虽鲁棒但牺牲了保真度。

Method: 通过最大化时域特征的全面学习，提出True方法，增强保真度的同时保持鲁棒性。

Result: 该方法在保护语音和歌声时，实现了鲁棒性和保真度的平衡。

Conclusion: True方法为合成声音的水印保护提供了新的解决方案。

Abstract: The rapid advancement of generative models has led to the synthesis of
real-fake ambiguous voices. To erase the ambiguity, embedding watermarks into
the frequency-domain features of synthesized voices has become a common
routine. However, the robustness achieved by choosing the frequency domain
often comes at the expense of fine-grained voice features, leading to a loss of
fidelity. Maximizing the comprehensive learning of time-domain features to
enhance fidelity while maintaining robustness, we pioneer a
\textbf{\underline{t}}emporal-aware
\textbf{\underline{r}}ob\textbf{\underline{u}}st
wat\textbf{\underline{e}}rmarking (\emph{True}) method for protecting the
speech and singing voice.

</details>

### [23] [Towards Fuzzing Zero-Knowledge Proof Circuits (Short Paper)](https://arxiv.org/abs/2504.14881)
*Stefanos Chaliasos,Imam Al-Fath,Alastair Donaldson*

Main category: cs.CR

TLDR: 论文探讨了零知识证明（ZKPs）电路中模糊测试的应用挑战，提出了输入生成和测试框架构建的技术，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管零知识证明在隐私保护和可验证应用中具有潜力，但其实现和使用仍存在挑战，尤其是电路中的错误检测方法尚未充分探索。

Method: 研究了模糊测试在ZKP电路中的独特挑战，探讨了预言机问题及其解决方案，并提出了输入生成和测试框架构建的技术。

Result: 通过实现一个针对zk-regex库的模糊测试工具，发现了10个新错误。

Conclusion: 模糊测试在ZKP电路领域具有潜力，能够有效检测错误。

Abstract: Zero-knowledge proofs (ZKPs) have evolved from a theoretical cryptographic
concept into a powerful tool for implementing privacy-preserving and verifiable
applications without requiring trust assumptions. Despite significant progress
in the field, implementing and using ZKPs via \emph{ZKP circuits} remains
challenging, leading to numerous bugs that affect ZKP circuits in practice, and
\emph{fuzzing} remains largely unexplored as a method to detect bugs in ZKP
circuits. We discuss the unique challenges of applying fuzzing to ZKP circuits,
examine the oracle problem and its potential solutions, and propose techniques
for input generation and test harness construction. We demonstrate that fuzzing
can be effective in this domain by implementing a fuzzer for \texttt{zk-regex},
a cornerstone library in modern ZKP applications. In our case study, we
discovered \textit{$10$} new bugs.

</details>

### [24] [Zero Day Malware Detection with Alpha: Fast DBI with Transformer Models for Real World Application](https://arxiv.org/abs/2504.14886)
*Matthew Gaber,Mohiuddin Ahmed,Helge Janicke*

Main category: cs.CR

TLDR: Alpha框架结合DBI、ASM分析和Transformer架构，通过消除训练数据中的常见函数，依赖上下文模式检测新型恶意软件，实现零日恶意软件检测的高精度。


<details>
  <summary>Details</summary>
Motivation: 恶意软件分类的准确性依赖于特征质量，而现有工具难以捕获真实行为。Peekaboo工具通过动态二进制插桩技术克服了恶意软件的逃避技术，为模型提供更真实的ASM指令级行为数据。

Method: Alpha框架利用Transformer模型和ASM语言，通过Peekaboo收集的恶意和良性软件数据训练，消除测试样本中的常见函数，迫使模型依赖上下文模式和新型ASM指令组合进行分类。

Result: Alpha在勒索软件、蠕虫和APT的检测中表现出完美准确率，对恶意和良性样本的分类均无错误。

Conclusion: 结合DBI、ASM分析和Transformer架构，Alpha为应对不断演变的恶意软件威胁提供了一种高效解决方案。

Abstract: The effectiveness of an AI model in accurately classifying novel malware
hinges on the quality of the features it is trained on, which in turn depends
on the effectiveness of the analysis tool used. Peekaboo, a Dynamic Binary
Instrumentation (DBI) tool, defeats malware evasion techniques to capture
authentic behavior at the Assembly (ASM) instruction level. This behavior
exhibits patterns consistent with Zipf's law, a distribution commonly seen in
natural languages, making Transformer models particularly effective for binary
classification tasks. We introduce Alpha, a framework for zero day malware
detection that leverages Transformer models and ASM language. Alpha is trained
on malware and benign software data collected through Peekaboo, enabling it to
identify entirely new samples with exceptional accuracy. Alpha eliminates any
common functions from the test samples that are in the training dataset. This
forces the model to rely on contextual patterns and novel ASM instruction
combinations to detect malicious behavior, rather than memorizing familiar
features. By combining the strengths of DBI, ASM analysis, and Transformer
architectures, Alpha offers a powerful approach to proactively addressing the
evolving threat of malware. Alpha demonstrates perfect accuracy for Ransomware,
Worms and APTs with flawless classification for both malicious and benign
samples. The results highlight the model's exceptional performance in detecting
truly new malware samples.

</details>

### [25] [A Security Framework for General Blockchain Layer 2 Protocols](https://arxiv.org/abs/2504.14965)
*Zeta Avarikioti,Matteo Maffei,Yuheng Wang*

Main category: cs.CR

TLDR: 本文提出了首个通用的Layer 2（L2）安全框架，基于iUC框架，用于比较分析不同L2范式（如支付通道、侧链和rollup）的安全性和权衡。


<details>
  <summary>Details</summary>
Motivation: 不同L2范式在架构和假设上差异显著，缺乏统一的比较分析工具，因此需要一种通用的安全框架。

Method: 基于iUC框架，将L2协议建模为状态机，定义通用执行环境，并通过基于迹的谓词表征安全性。

Result: 框架成功分析了三种主流L2范式（Brick、Liquid Network、Arbitrum），揭示了其安全属性和权衡（如争议解决时间、存储分布等）。

Conclusion: 该框架为L2设计提供了统一的比较分析工具，支持模块化和可组合的安全证明，为系统化L2开发奠定了基础。

Abstract: Layer 2 (L2) solutions are the cornerstone of blockchain scalability,
enabling high-throughput and low-cost interactions by shifting execution
off-chain while maintaining security through interactions with the underlying
ledger. Despite their common goals, the principal L2 paradigms -- payment
channels, rollups, and sidechains -- differ substantially in architecture and
assumptions, making it difficult to comparatively analyze their security and
trade-offs.
  To address this, we present the first general security framework for L2
protocols. Our framework is based on the IITM-based Universal Composability
(iUC) framework, in which L2 protocols are modeled as stateful machines
interacting with higher-level protocol users and the underlying ledger. The
methodology defines a generic execution environment that captures ledger
events, message passing, and adversarial scheduling, and characterizes security
through trace-based predicates parameterized by adversarial capabilities and
timing assumptions. By abstracting away from protocol-specific details while
preserving critical interface and execution behavior, the framework enables
modular, protocol-agnostic reasoning and composable security proofs across a
wide range of L2 constructions.
  To demonstrate its applicability, we analyze an example from each of the
three dominant L2 scaling paradigms: a payment channel (Brick), a sidechain
(Liquid Network), and a rollup (Arbitrum). By instantiating each within our
framework, we derive their security properties and expose trade-offs. These
include the time for dispute resolution, distribution of off-chain storage and
computation, and varying trust assumptions (e.g., reliance on honest parties or
data availability). Our framework unifies the analysis of diverse L2 designs
and pinpoints their strengths and limitations, providing a foundation for
secure, systematic L2 development.

</details>

### [26] [aiXamine: LLM Safety and Security Simplified](https://arxiv.org/abs/2504.14985)
*Fatih Deniz,Dorde Popovic,Yazan Boshmaf,Euisuh Jeong,Minhaj Ahmad,Sanjay Chawla,Issa Khalil*

Main category: cs.CR

TLDR: aiXamine是一个用于评估大型语言模型（LLM）安全性和安全性的黑盒平台，整合了40多个测试，覆盖8个关键维度，并评估了50多个模型，揭示了领先模型的漏洞和开源模型的优势。


<details>
  <summary>Details</summary>
Motivation: 评估LLM的安全性和安全性是一个复杂且碎片化的任务，缺乏统一的平台和方法。

Method: 开发了aiXamine平台，整合了40多个测试，覆盖8个关键维度，并对50多个模型进行了评估。

Result: 发现领先模型存在漏洞（如GPT-4o易受对抗攻击），开源模型在某些方面表现优于专有模型。

Conclusion: aiXamine提供了一个全面的评估框架，揭示了模型性能的差异和优化方向。

Abstract: Evaluating Large Language Models (LLMs) for safety and security remains a
complex task, often requiring users to navigate a fragmented landscape of ad
hoc benchmarks, datasets, metrics, and reporting formats. To address this
challenge, we present aiXamine, a comprehensive black-box evaluation platform
for LLM safety and security. aiXamine integrates over 40 tests (i.e.,
benchmarks) organized into eight key services targeting specific dimensions of
safety and security: adversarial robustness, code security, fairness and bias,
hallucination, model and data privacy, out-of-distribution (OOD) robustness,
over-refusal, and safety alignment. The platform aggregates the evaluation
results into a single detailed report per model, providing a detailed breakdown
of model performance, test examples, and rich visualizations. We used aiXamine
to assess over 50 publicly available and proprietary LLMs, conducting over 2K
examinations. Our findings reveal notable vulnerabilities in leading models,
including susceptibility to adversarial attacks in OpenAI's GPT-4o, biased
outputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0.
Additionally, we observe that open-source models can match or exceed
proprietary models in specific services such as safety alignment, fairness and
bias, and OOD robustness. Finally, we identify trade-offs between distillation
strategies, model size, training methods, and architectural choices.

</details>

### [27] [Dual Utilization of Perturbation for Stream Data Publication under Local Differential Privacy](https://arxiv.org/abs/2504.14993)
*Rong Du,Qingqing Ye,Yaxin Xiao,Liantong Yu,Yue Fu,Haibo Hu*

Main category: cs.CR

TLDR: 论文提出了一种新的本地差分隐私（LDP）方法IPP，用于流数据隐私保护，并通过APP和CAPP算法进一步优化，显著提高了数据效用。


<details>
  <summary>Details</summary>
Motivation: 流数据（如IoT、远程医疗等）的隐私保护需求迫切，但传统LDP方法在流数据中噪声过大，导致效用低下。

Method: 提出IPP方法，利用用户已知的真实值和扰动值计算偏差，校准后续扰动过程；进一步开发APP和CAPP算法增强鲁棒性。

Result: 实验证明，IPP、APP和CAPP在保持隐私保护的同时，显著优于现有LDP流数据发布方案。

Conclusion: 该方法在流数据隐私保护中实现了更高的效用，为实际应用提供了可行解决方案。

Abstract: Stream data from real-time distributed systems such as IoT, tele-health, and
crowdsourcing has become an important data source. However, the collection and
analysis of user-generated stream data raise privacy concerns due to the
potential exposure of sensitive information. To address these concerns, local
differential privacy (LDP) has emerged as a promising standard. Nevertheless,
applying LDP to stream data presents significant challenges, as stream data
often involves a large or even infinite number of values. Allocating a given
privacy budget across these data points would introduce overwhelming LDP noise
to the original stream data.
  Beyond existing approaches that merely use perturbed values for estimating
statistics, our design leverages them for both perturbation and estimation.
This dual utilization arises from a key observation: each user knows their own
ground truth and perturbed values, enabling a precise computation of the
deviation error caused by perturbation. By incorporating this deviation into
the perturbation process of subsequent values, the previous noise can be
calibrated. Following this insight, we introduce the Iterative Perturbation
Parameterization (IPP) method, which utilizes current perturbed results to
calibrate the subsequent perturbation process. To enhance the robustness of
calibration and reduce sensitivity, two algorithms, namely Accumulated
Perturbation Parameterization (APP) and Clipped Accumulated Perturbation
Parameterization (CAPP) are further developed. We prove that these three
algorithms satisfy $w$-event differential privacy while significantly improving
utility. Experimental results demonstrate that our techniques outperform
state-of-the-art LDP stream publishing solutions in terms of utility, while
retaining the same privacy guarantee.

</details>

### [28] [SOLIDO: A Robust Watermarking Method for Speech Synthesis via Low-Rank Adaptation](https://arxiv.org/abs/2504.15035)
*Yue Li,Weizhi Liu,Dongdong Lin*

Main category: cs.CR

TLDR: 论文提出了一种名为SOLIDO的新型生成水印方法，通过低秩适应（LoRA）结合参数高效微调，解决了现有语音生成水印技术计算开销大、训练成本高及变长输入鲁棒性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 语音生成模型的快速发展带来了模型侵权和内容滥用的安全问题，现有水印技术存在计算开销大、训练成本高及对变长输入鲁棒性不足的局限性。

Method: SOLIDO方法通过LoRA实现参数高效微调，设计了基于深度可分离卷积的水印解码器以精确提取变长输入中的水印，并提出语音驱动的轻量级微调策略。

Result: 实验表明，SOLIDO在2000 bps容量下仍能保持高保真水印语音，对常见攻击的平均提取准确率最高达99.20%和98.43%，抗时间拉伸攻击性能优于其他方法近23%。

Conclusion: SOLIDO在高效性和鲁棒性方面显著优于现有技术，为语音生成水印提供了可行的解决方案。

Abstract: The accelerated advancement of speech generative models has given rise to
security issues, including model infringement and unauthorized abuse of
content. Although existing generative watermarking techniques have proposed
corresponding solutions, most methods require substantial computational
overhead and training costs. In addition, some methods have limitations in
robustness when handling variable-length inputs. To tackle these challenges, we
propose \textsc{SOLIDO}, a novel generative watermarking method that integrates
parameter-efficient fine-tuning with speech watermarking through low-rank
adaptation (LoRA) for speech diffusion models. Concretely, the watermark
encoder converts the watermark to align with the input of diffusion models. To
achieve precise watermark extraction from variable-length inputs, the watermark
decoder based on depthwise separable convolution is designed for watermark
recovery. To further enhance speech generation performance and watermark
extraction capability, we propose a speech-driven lightweight fine-tuning
strategy, which reduces computational overhead through LoRA. Comprehensive
experiments demonstrate that the proposed method ensures high-fidelity
watermarked speech even at a large capacity of 2000 bps. Furthermore, against
common individual and compound speech attacks, our SOLIDO achieves a maximum
average extraction accuracy of 99.20\% and 98.43\%, respectively. It surpasses
other state-of-the-art methods by nearly 23\% in resisting time-stretching
attacks.

</details>

### [29] [Mining Characteristics of Vulnerable Smart Contracts Across Lifecycle Stages](https://arxiv.org/abs/2504.15063)
*Hongli Peng,Xiaoqi Li,Wenkai Li*

Main category: cs.CR

TLDR: 本文对智能合约生命周期中的安全问题进行了首次实证研究，分析了部署、执行、升级和销毁各阶段的安全问题，并提出了七个特征描述，利用机器学习模型识别不同阶段的漏洞。


<details>
  <summary>Details</summary>
Motivation: 智能合约的安全问题导致重大经济损失，现有研究仅关注代码漏洞，缺乏对生命周期各阶段漏洞特征的分析和区分。

Method: 通过实证研究分析智能合约生命周期的各阶段（部署、执行、升级、销毁），提出七个特征描述，并利用五种机器学习分类模型识别漏洞。

Result: 分类结果显示，易受攻击的合约在不同阶段表现出独特的交易特征和网络属性。

Conclusion: 研究为智能合约生命周期各阶段的安全问题提供了更全面的分析框架和解决方案。

Abstract: Smart contracts are the cornerstone of decentralized applications and
financial protocols, which extend the application of digital currency
transactions. The applications and financial protocols introduce significant
security challenges, resulting in substantial economic losses. Existing
solutions predominantly focus on code vulnerabilities within smart contracts,
accounting for only 50% of security incidents. Therefore, a more comprehensive
study of security issues related to smart contracts is imperative. The existing
empirical research realizes the static analysis of smart contracts from the
perspective of the lifecycle and gives the corresponding measures for each
stage. However, they lack the characteristic analysis of vulnerabilities in
each stage and the distinction between the vulnerabilities. In this paper, we
present the first empirical study on the security of smart contracts throughout
their lifecycle, including deployment and execution, upgrade, and destruction
stages. It delves into the security issues at each stage and provides at least
seven feature descriptions. Finally, utilizing these seven features, five
machine-learning classification models are used to identify vulnerabilities at
different stages. The classification results reveal that vulnerable contracts
exhibit distinct transaction features and ego network properties at various
stages.

</details>

### [30] [GIFDL: Generated Image Fluctuation Distortion Learning for Enhancing Steganographic Security](https://arxiv.org/abs/2504.15139)
*Xiangkun Wang,Kejiang Chen,Yuang Qi,Ruiheng Liu,Weiming Zhang,Nenghai Yu*

Main category: cs.CR

TLDR: GIFDL是一种基于生成图像波动的隐写失真学习方法，通过利用生成图像的波动特性，结合新的GAN训练策略，显著提升了隐写安全性。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的失真设计方法未能充分利用生成图像的优势，导致安全性不足。

Method: 提出GIFDL方法，以高度相似的波动图像作为输入，并引入新的GAN训练策略，将隐写图像伪装为波动图像。

Result: 实验表明，GIFDL相比现有基于GAN的失真学习方法，对隐写分析的抵抗能力更强，平均检测错误率提高了3.30%。

Conclusion: GIFDL通过利用生成图像的波动特性，显著提升了隐写安全性，为生成图像作为隐写载体的应用提供了新思路。

Abstract: Minimum distortion steganography is currently the mainstream method for
modification-based steganography. A key issue in this method is how to define
steganographic distortion. With the rapid development of deep learning
technology, the definition of distortion has evolved from manual design to deep
learning design. Concurrently, rapid advancements in image generation have made
generated images viable as cover media. However, existing distortion design
methods based on machine learning do not fully leverage the advantages of
generated cover media, resulting in suboptimal security performance. To address
this issue, we propose GIFDL (Generated Image Fluctuation Distortion Learning),
a steganographic distortion learning method based on the fluctuations in
generated images. Inspired by the idea of natural steganography, we take a
series of highly similar fluctuation images as the input to the steganographic
distortion generator and introduce a new GAN training strategy to disguise
stego images as fluctuation images. Experimental results demonstrate that
GIFDL, compared with state-of-the-art GAN-based distortion learning methods,
exhibits superior resistance to steganalysis, increasing the detection error
rates by an average of 3.30% across three steganalyzers.

</details>

### [31] [C2RUST-BENCH: A Minimized, Representative Dataset for C-to-Rust Transpilation Evaluation](https://arxiv.org/abs/2504.15144)
*Melih Sirlanci,Carter Yagemann,Zhiqiang Lin*

Main category: cs.CR

TLDR: 本文提出了一种方法，从大量函数中选择代表性样本构建C-to-Rust转换评估数据集C2RUST-BENCH，包含2,905个函数。


<details>
  <summary>Details</summary>
Motivation: 尽管C-to-Rust转换框架逐渐流行，但缺乏全面的评估数据集，手动或自动分析大规模数据集耗时。

Method: 从15,503个真实世界程序的函数中筛选出2,905个代表性函数，构建C2RUST-BENCH数据集。

Result: C2RUST-BENCH数据集能有效评估C-to-Rust转换框架的性能。

Conclusion: 构建的小规模代表性数据集解决了评估效率问题，为C-to-Rust转换研究提供了实用工具。

Abstract: Despite the effort in vulnerability detection over the last two decades,
memory safety vulnerabilities continue to be a critical problem. Recent reports
suggest that the key solution is to migrate to memory-safe languages. To this
end, C-to-Rust transpilation becomes popular to resolve memory-safety issues in
C programs. Recent works propose C-to-Rust transpilation frameworks; however, a
comprehensive evaluation dataset is missing. Although one solution is to put
together a large enough dataset, this increases the analysis time in automated
frameworks as well as in manual efforts for some cases. In this work, we build
a method to select functions from a large set to construct a minimized yet
representative dataset to evaluate the C-to-Rust transpilation. We propose
C2RUST-BENCH that contains 2,905 functions, which are representative of
C-to-Rust transpilation, selected from 15,503 functions of real-world programs.

</details>

### [32] [Extending the ElGamal Cryptosystem to the Third Group of Units of $\Z_{n}$](https://arxiv.org/abs/2504.15202)
*Jana Hamza,Mohammad EL Hindi,Seifeddine Kadri,Therrar Kadri,Yahya Awad*

Main category: cs.CR

TLDR: 本文扩展了ElGamal加密系统至环$\Z_{n}$的第三单位群，证明其安全性优于以往扩展，并提供了数值模拟验证其安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 提升ElGamal加密系统的安全性，探索其在更复杂代数结构中的应用。

Method: 扩展ElGamal至环$\Z_{n}$的第三单位群，描述新环境下的算术运算。

Result: 数值模拟表明新系统在安全性和效率上表现优越。

Conclusion: 新扩展的ElGamal加密系统在安全性和效率上具有潜力。

Abstract: In this paper, we extend the ElGamal cryptosystem to the third group of units
of the ring $\Z_{n}$, which we prove to be more secure than the previous
extensions. We describe the arithmetic needed in the new setting. We also
provide some numerical simulations that shows the security and efficiency of
our proposed cryptosystem.

</details>

### [33] [A Review on Privacy in DAG-Based DLTs](https://arxiv.org/abs/2504.15233)
*Mayank Raikwar*

Main category: cs.CR

TLDR: 本文探讨了DAG-based DLTs中的隐私问题，提出了增强隐私的方法，并分析了现有系统的挑战与实现。


<details>
  <summary>Details</summary>
Motivation: 传统区块链存在可扩展性问题，DAG-based DLTs虽解决了这一问题，但隐私问题却被忽视。本文旨在填补这一研究空白。

Method: 通过全面分析DAG-based DLTs中的隐私概念与挑战，提出增强隐私的方法，并评估现有系统的实现。

Result: 揭示了当前DAG-based DLTs中的隐私现状，并提出了未来研究方向。

Conclusion: DAG-based DLTs的隐私问题需进一步研究，本文为未来工作提供了方向。

Abstract: Directed Acyclic Graph (DAG)-based Distributed Ledger Technologies (DLTs)
have emerged as a promising solution to the scalability issues inherent in
traditional blockchains. However, amidst the focus on scalability, the crucial
aspect of privacy within DAG-based DLTs has been largely overlooked. This paper
seeks to address this gap by providing a comprehensive examination of privacy
notions and challenges within DAG-based DLTs. We delve into potential
methodologies to enhance privacy within these systems, while also analyzing the
associated hurdles and real-world implementations within state-of-the-art
DAG-based DLTs. By exploring these methodologies, we not only illuminate the
current landscape of privacy in DAG-based DLTs but also outline future research
directions in this evolving field.

</details>

### [34] [A Refreshment Stirred, Not Shaken (III): Can Swapping Be Differentially Private?](https://arxiv.org/abs/2504.15246)
*James Bailie,Ruobin Gong,Xiao-Li Meng*

Main category: cs.CR

TLDR: 本文通过三部分内容探讨了差分隐私（DP）的理论基础、实际应用及减少误解的方法，提出了一个五要素系统，并在美国十年人口普查中验证了其适用性。


<details>
  <summary>Details</summary>
Motivation: 由于存在200多种DP定义，本文旨在明确DP的核心含义，并扩展其理论基础和实际应用。

Method: 第一部分提出了一个五要素系统（who, where, what, how, how much）来解析DP；第二部分在美国十年人口普查中比较了传统交换策略与DP方法（TopDown Algorithm）。

Result: 研究展示了五要素系统的实用性，并揭示了传统统计披露控制（SDC）与DP的结合优势，以及如何通过消除对随机不确定性的依赖来推广DP。

Conclusion: 本文通过理论分析和实际案例，深化了对DP的理解，并提出了未来研究方向，如推广DP的新方法。

Abstract: The quest for a precise and contextually grounded answer to the question in
the present paper's title resulted in this stirred-not-shaken triptych, a
phrase that reflects our desire to deepen the theoretical basis, broaden the
practical applicability, and reduce the misperception of differential privacy
(DP)$\unicode{x2014}$all without shaking its core foundations. Indeed, given
the existence of more than 200 formulations of DP (and counting), before even
attempting to answer the titular question one must first precisely specify what
it actually means to be DP. Motivated by this observation, a theoretical
investigation into DP's fundamental essence resulted in Part I of this trio,
which introduces a five-building-block system explicating the who, where, what,
how and how much aspects of DP. Instantiating this system in the context of the
United States Decennial Census, Part II then demonstrates the broader
applicability and relevance of DP by comparing a swapping strategy like that
used in 2010 with the TopDown Algorithm$\unicode{x2014}$a DP method adopted in
the 2020 Census. This paper provides nontechnical summaries of the preceding
two parts as well as new discussion$\unicode{x2014}$for example, on how greater
awareness of the five building blocks can thwart privacy theatrics; how our
results bridging traditional SDC and DP allow a data custodian to reap the
benefits of both these fields; how invariants impact disclosure risk; and how
removing the implicit reliance on aleatoric uncertainty could lead to new
generalizations of DP.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [A discrete physics-informed training for projection-based reduced order models with neural networks](https://arxiv.org/abs/2504.13875)
*N. Sibuet,S. Ares de Parga,J. R. Bravo,R. Rossi*

Main category: cs.LG

TLDR: 本文提出了一种基于物理信息的降阶模型（ROM）训练框架，结合FEM残差损失和PROM-ANN架构，提升了非线性和快速衰减奇异值问题的精度。


<details>
  <summary>Details</summary>
Motivation: 传统ROM和物理信息神经网络（PINN）之间存在差距，本文旨在通过FEM残差损失弥补这一差距，提升ROM的精度和适用性。

Method: 扩展PROM-ANN架构，引入基于FEM的离散物理信息残差损失，适用于非线性问题，并改进架构以处理快速衰减奇异值。

Result: 在非线性超弹性问题中，改进的PROM-ANN在重建精度上显著优于POD，且物理信息训练缩小了数据重建与ROM精度之间的差距。

Conclusion: FEM残差在ROM构建中具有关键作用，未来需探索更多架构以进一步发挥其潜力。

Abstract: This paper presents a physics-informed training framework for
projection-based Reduced Order Models (ROMs). We extend the PROM-ANN
architecture by complementing snapshot-based training with a FEM-based,
discrete physics-informed residual loss, bridging the gap between traditional
projection-based ROMs and physics-informed neural networks (PINNs). Unlike
conventional PINNs that rely on analytical PDEs, our approach leverages FEM
residuals to guide the learning of the ROM approximation manifold. Key
contributions include: (1) a parameter-agnostic, discrete residual loss
applicable to non-linear problems, (2) an architectural modification to
PROM-ANN improving accuracy for fast-decaying singular values, and (3) an
empirical study on the proposed physics informed training process for ROMs.
  The method is demonstrated on a non-linear hyperelasticity problem,
simulating a rubber cantilever under multi-axial loads. The main accomplishment
in regards to the proposed residual-based loss is its applicability on
non-linear problems by interfacing with FEM software while maintaining
reasonable training times. The modified PROM-ANN outperforms POD by orders of
magnitude in snapshot reconstruction accuracy, while the original formulation
is not able to learn a proper mapping for this use-case. Finally, the
application of physics informed training in ANN-PROM modestly narrows the gap
between data reconstruction and ROM accuracy, however it highlights the
untapped potential of the proposed residual-driven optimization for future ROM
development. This work underscores the critical role of FEM residuals in ROM
construction and calls for further exploration on architectures beyond
PROM-ANN.

</details>

### [36] [Ising Models with Hidden Markov Structure: Applications to Probabilistic Inference in Machine Learning](https://arxiv.org/abs/2504.13927)
*F. Herrera,U. A. Rozikov,M. V. Velasco*

Main category: cs.LG

TLDR: 研究了包含隐藏自旋的哈密顿量，证明了在特定条件下存在三种不同的平移不变Gibbs测度，适用于机器学习中的分层数据推断。


<details>
  <summary>Details</summary>
Motivation: 探索哈密顿量在Cayley树上的平移不变Gibbs测度，为机器学习中的分层数据推断提供理论基础。

Method: 通过分析哈密顿量的Ising相互作用和数据相关项，研究Cayley树上的平移不变Gibbs测度。

Result: 在特定参数条件下，证明了存在三种不同的平移不变Gibbs测度，适用于去噪、弱监督学习和异常检测。

Conclusion: Cayley树上的平移不变Gibbs测度为分层数据推断提供了结构化方法，具有实际应用价值。

Abstract: In this paper, we investigate a Hamiltonian that incorporates Ising
interactions between hidden $\pm 1$ spins, alongside a data-dependent term that
couples the hidden and observed variables. Specifically, we explore
translation-invariant Gibbs measures (TIGM) of this Hamiltonian on Cayley
trees.
  Under certain explicit conditions on the model's parameters, we demonstrate
that there can be up to three distinct TIGMs. Each of these measures represents
an equilibrium state of the spin system. These measures provide a structured
approach to inference on hierarchical data in machine learning. They have
practical applications in tasks such as denoising, weakly supervised learning,
and anomaly detection. The Cayley tree structure is particularly advantageous
for exact inference due to its tractability.

</details>

### [37] [Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining](https://arxiv.org/abs/2504.13932)
*Deyu Cao,Samin Aref*

Main category: cs.LG

TLDR: 论文提出了一种基于ApiQ的改进方法，通过结合显著性感知正则化，在超低位量化中提升模型精度，无需完全重新训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的计算需求和规模带来实际挑战，量化方法虽能压缩模型，但现有方法在精度和资源消耗之间存在权衡。

Method: 结合ApiQ的部分训练与显著性感知正则化，优先保留关键参数，提出一种超低位量化方法。

Result: 实验表明，该方法在LLaMA系列模型上提升了精度，缩小了量化模型与全精度模型的差距，且开销极小。

Conclusion: 该方法为大型语言模型的超低位量化提供了有效解决方案，未来将公开以促进相关研究。

Abstract: Large language models offer remarkable capabilities, but their size and
computational demands pose practical challenges. Quantization methods compress
their size through replacing their high-precision parameters by quantized
values of lower precision. Post-training quantization reduces model size
efficiently at the cost of decreased accuracy, while quantization-aware
training better preserves accuracy but is resource-intensive. Among existing
post-training quantization algorithms, the ApiQ method achieves superior
accuracy preservation at minimal memory and time overhead. We investigate two
ideas to extend performance in ultra-low-bit quantization beyond ApiQ's level.
First, we look into combining existing quantization-aware training techniques
with ApiQ's partial training. We show that this does not outperform the
baseline ApiQ method with limited training data and frozen weights. This leads
to two key insights: (1) The substantial representational capacity that is
gained through full retraining may not be feasible through partial training.
(2) This gain seems to depend on using a large and diverse dataset in
quantization-aware training. Second, through a novel approach informed by the
two insights, we propose an ultra-low-bit quantization method that builds upon
ApiQ and extends its performance without the need for full retraining. It
relies on a saliency-aware regularization term that prioritizes preserving the
most impactful parameters during quantization. Our experiments on benchmark
language models from the LLaMA family show that our proposed approach boosts
accuracy and tightens the gap between the quantized model and the
full-precision model, with minimal overhead. Our method will be made publicly
available to facilitate future developments in ultra-low-bit quantization of
large language models.

</details>

### [38] [NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning](https://arxiv.org/abs/2504.13941)
*Syeda Nahida Akter,Shrimai Prabhumoye,Matvei Novikov,Seungju Han,Ying Lin,Evelina Bakhturi,Eric Nyberg,Yejin Choi,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro*

Main category: cs.LG

TLDR: NEMOTRON-CROSSTHINK是一个通过多领域数据增强强化学习的框架，提升大语言模型在广泛推理任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在数学推理中表现良好，但在其他领域因数据不足、奖励结构不明确和任务多样性而受限。

Method: 框架整合多领域数据（STEM、人文等），采用结构化模板、可验证答案筛选及数据混合策略优化。

Result: 在数学和非数学推理任务中均显著提升准确率（如MATH-500 +30.1%），并减少28%的token使用。

Conclusion: 多领域、多格式数据整合能实现更准确、高效且泛化能力更强的大语言模型。

Abstract: Large Language Models (LLMs) have shown strong reasoning capabilities,
particularly when enhanced through Reinforcement Learning (RL). While prior
work has successfully applied RL to mathematical reasoning -- where rules and
correctness are well-defined -- generalizing these methods to broader reasoning
domains remains challenging due to limited data, the lack of verifiable reward
structures, and diverse task requirements. In this work, we propose
NEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain
corpora, including both synthetic and real-world question-answer pairs, into RL
training to improve generalization across diverse reasoning tasks.
NEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from
varied sources spanning STEM, humanities, social sciences, etc.; (2) applying
structured templates (e.g., multiple-choice and open-ended) to control
answer-space complexity; (3) filtering for verifiable answers; and (4)
optimizing data blending strategies that utilizes data from multiple sources
effectively. Our approach enables scalable and verifiable reward modeling
beyond mathematics and demonstrates improved accuracies on both math (MATH-500:
+30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%,
GPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover,
NEMOTRON-CROSSTHINK exhibits significantly improved response efficiency --
using 28% fewer tokens for correct answers -- highlighting more focused and
effective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that
integrating multi-domain, multi-format data in RL leads to more accurate,
efficient, and generalizable LLMs.

</details>

### [39] [Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models](https://arxiv.org/abs/2504.13945)
*Zhanglin Wu,Tengfei Song,Ning Xie,Weidong Zhang,Mengli Zhu,Shuang Wu,Shiliang Sun,Hao Yang*

Main category: cs.LG

TLDR: 论文提出了MOTBench，一个专注于菜单翻译的评估框架，用于测试大型视觉语言模型（LVLMs）在复杂布局长文本理解中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LVLMs评估主要关注简单布局的短文本或长文本，而复杂布局长文本的理解能力评估被忽视。菜单翻译在跨文化沟通中至关重要，因此需要专门的评估工具。

Method: 提出MOTBench，包含中英文菜单数据集，具有复杂布局、多样字体和文化特定元素，并通过自动和人工评估验证模型表现。

Result: 实验显示自动评估结果与人工评估高度一致，并揭示了现有LVLMs在菜单翻译任务中的优缺点。

Conclusion: MOTBench为LVLMs在复杂布局长文本理解能力的评估提供了有效工具，并为未来模型改进提供了指导。

Abstract: The rapid advancement of large vision-language models (LVLMs) has
significantly propelled applications in document understanding, particularly in
optical character recognition (OCR) and multilingual translation. However,
current evaluations of LVLMs, like the widely used OCRBench, mainly focus on
verifying the correctness of their short-text responses and long-text responses
with simple layout, while the evaluation of their ability to understand long
texts with complex layout design is highly significant but largely overlooked.
In this paper, we propose Menu OCR and Translation Benchmark (MOTBench), a
specialized evaluation framework emphasizing the pivotal role of menu
translation in cross-cultural communication. MOTBench requires LVLMs to
accurately recognize and translate each dish, along with its price and unit
items on a menu, providing a comprehensive assessment of their visual
understanding and language processing capabilities. Our benchmark is comprised
of a collection of Chinese and English menus, characterized by intricate
layouts, a variety of fonts, and culturally specific elements across different
languages, along with precise human annotations. Experiments show that our
automatic evaluation results are highly consistent with professional human
evaluation. We evaluate a range of publicly available state-of-the-art LVLMs,
and through analyzing their output to identify the strengths and weaknesses in
their performance, offering valuable insights to guide future advancements in
LVLM development. MOTBench is available at https://github.com/gitwzl/MOTBench.

</details>

### [40] [On Revealing the Hidden Problem Structure in Real-World and Theoretical Problems Using Walsh Coefficient Influence](https://arxiv.org/abs/2504.13949)
*M. W. Przewozniczek,F. Chicano,R. Tinós,J. Nalepa,B. Ruszczak,A. M. Wijata*

Main category: cs.LG

TLDR: 论文提出了一种基于Walsh分解的灰盒优化方法，通过构建加权动态变量交互图（wdVIG）来过滤噪声依赖，提升优化效果。


<details>
  <summary>Details</summary>
Motivation: 解决灰盒优化中变量非线性依赖问题，尤其是噪声导致的无关依赖对优化过程的干扰。

Method: 扩展Walsh分解，测量变量依赖强度，构建wdVIG图，过滤噪声依赖，生成有效的变量掩码。

Result: 在噪声问题中，wdVIG掩码显著提升优化效果；在无噪声问题中，效果与现有方法相当。

Conclusion: wdVIG方法能有效识别并过滤噪声依赖，提升优化效率，适用于复杂问题。

Abstract: Gray-box optimization employs Walsh decomposition to obtain non-linear
variable dependencies and utilize them to propose masks of variables that have
a joint non-linear influence on fitness value. These masks significantly
improve the effectiveness of variation operators. In some problems, all
variables are non-linearly dependent, making the aforementioned masks useless.
We analyze the features of the real-world instances of such problems and show
that many of their dependencies may have noise-like origins. Such noise-caused
dependencies are irrelevant to the optimization process and can be ignored. To
identify them, we propose extending the use of Walsh decomposition by measuring
variable dependency strength that allows the construction of the weighted
dynamic Variable Interaction Graph (wdVIG). wdVIGs adjust the dependency
strength to mixed individuals. They allow the filtering of irrelevant
dependencies and re-enable using dependency-based masks by variation operators.
We verify the wdVIG potential on a large benchmark suite. For problems with
noise, the wdVIG masks can improve the optimizer's effectiveness. If all
dependencies are relevant for the optimization, i.e., the problem is not
noised, the influence of wdVIG masks is similar to that of state-of-the-art
structures of this kind.

</details>

### [41] [Open-Medical-R1: How to Choose Data for RLVR Training at Medicine Domain](https://arxiv.org/abs/2504.13950)
*Zhongxi Qiu,Zhang Zhang,Yan Hu,Heng Li,Jiang Liu*

Main category: cs.LG

TLDR: 本文研究了在医学领域中为强化学习与验证奖励（RLVR）训练选择最优数据策略，发现经过过滤的数据训练模型表现优于随机采样，并探讨了不同过滤策略的效果。


<details>
  <summary>Details</summary>
Motivation: RLVR在提升大语言模型推理能力方面潜力巨大，但此前研究多集中于数学和逻辑领域，医学领域应用较少。本文旨在填补这一空白。

Method: 使用四种数据采样策略（随机采样及三种模型过滤）从MedQA-USMLE数据集中选择数据，以Gemma-3-12b-it为基础模型，采用GRPO方法进行训练，并在多个基准测试中评估性能。

Result: 过滤数据训练的模型表现更优，其中使用Gemma-3-12b-it自过滤的数据在医学领域表现最佳，但跨领域鲁棒性较差；而使用更大模型过滤的数据则整体鲁棒性更强。

Conclusion: 研究为专业领域RLVR数据选择提供了有效策略，强调了数据选择对性能优化的重要性，并开源了代码供进一步研究。

Abstract: This paper explores optimal data selection strategies for Reinforcement
Learning with Verified Rewards (RLVR) training in the medical domain. While
RLVR has shown exceptional potential for enhancing reasoning capabilities in
large language models, most prior implementations have focused on mathematics
and logical puzzles, with limited exploration of domain-specific applications
like medicine. We investigate four distinct data sampling strategies from
MedQA-USMLE: random sampling (baseline), and filtering using Phi-4,
Gemma-3-27b-it, and Gemma-3-12b-it models. Using Gemma-3-12b-it as our base
model and implementing Group Relative Policy Optimization (GRPO), we evaluate
performance across multiple benchmarks including MMLU, GSM8K, MMLU-Pro, and
CMMLU. Our findings demonstrate that models trained on filtered data generally
outperform those trained on randomly selected samples. Notably, training on
self-filtered samples (using Gemma-3-12b-it for filtering) achieved superior
performance in medical domains but showed reduced robustness across different
benchmarks, while filtering with larger models from the same series yielded
better overall robustness. These results provide valuable insights into
effective data organization strategies for RLVR in specialized domains and
highlight the importance of thoughtful data selection in achieving optimal
performance. You can access our repository
(https://github.com/Qsingle/open-medical-r1) to get the codes.

</details>

### [42] [Generative System Dynamics in Recurrent Neural Networks](https://arxiv.org/abs/2504.13951)
*Michele Casoni,Tommaso Guidi,Alessandro Betti,Stefano Melacci,Marco Gori*

Main category: cs.LG

TLDR: 研究了RNN在连续时间动态中的振荡行为，发现斜对称权重矩阵和非线性激活函数（如双曲正切）对维持稳定极限环至关重要。


<details>
  <summary>Details</summary>
Motivation: 探索RNN在非线性激活函数下如何避免静态固定点，实现持续振荡行为。

Method: 通过理论分析和数值模拟，验证斜对称权重矩阵和特定非线性激活函数的作用。

Result: 斜对称权重矩阵和非线性激活函数能维持稳定极限环，并提升数值稳定性。

Conclusion: 研究为设计具备复杂时间依赖能力的RNN架构提供了实用指导。

Abstract: In this study, we investigate the continuous time dynamics of Recurrent
Neural Networks (RNNs), focusing on systems with nonlinear activation
functions. The objective of this work is to identify conditions under which
RNNs exhibit perpetual oscillatory behavior, without converging to static fixed
points. We establish that skew-symmetric weight matrices are fundamental to
enable stable limit cycles in both linear and nonlinear configurations. We
further demonstrate that hyperbolic tangent-like activation functions (odd,
bounded, and continuous) preserve these oscillatory dynamics by ensuring motion
invariants in state space. Numerical simulations showcase how nonlinear
activation functions not only maintain limit cycles, but also enhance the
numerical stability of the system integration process, mitigating those
instabilities that are commonly associated with the forward Euler method. The
experimental results of this analysis highlight practical considerations for
designing neural architectures capable of capturing complex temporal
dependencies, i.e., strategies for enhancing memorization skills in recurrent
models.

</details>

### [43] [Prognosis Of Lithium-Ion Battery Health with Hybrid EKF-CNN+LSTM Model Using Differential Capacity](https://arxiv.org/abs/2504.13956)
*Md Azizul Hoque,Babul Salam,Mohd Khair Hassan,Abdulkabir Aliyu,Abedalmuhdi Almomany,Muhammed Sutcu*

Main category: cs.LG

TLDR: 该论文提出了一种电池退化测试模型，结合DCA和PIM方法，评估了两种锂离子电池在不同充放电速率下的内部退化机制，并通过EKF、CNN和LSTM验证数据。


<details>
  <summary>Details</summary>
Motivation: 电池退化是电动汽车和储能系统的关键问题，现有研究多关注SOC估计，而忽略了内部退化机制的准确解释。

Method: 使用两种锂离子电池（LiNiCoAlO2和LiFePO4），在不同充放电速率下进行测试，结合EKF、CNN和LSTM验证数据，并采用PIM分析电池健康状态。

Result: 模型误差低于0.001%，LiFePO4电池在多变负载条件下表现更稳健，而LiNiCoAlO2电池在快速负载下退化更快。

Conclusion: LiFePO4电池性能更优，PIM方法揭示了负载速率对电池退化的显著影响。

Abstract: Battery degradation is a major challenge in electric vehicles (EV) and energy
storage systems (ESS). However, most degradation investigations focus mainly on
estimating the state of charge (SOC), which fails to accurately interpret the
cells' internal degradation mechanisms. Differential capacity analysis (DCA)
focuses on the rate of change of cell voltage about the change in cell
capacity, under various charge/discharge rates. This paper developed a battery
cell degradation testing model that used two types of lithium-ions (Li-ion)
battery cells, namely lithium nickel cobalt aluminium oxides (LiNiCoAlO2) and
lithium iron phosphate (LiFePO4), to evaluate internal degradation during
loading conditions. The proposed battery degradation model contains distinct
charge rates (DCR) of 0.2C, 0.5C, 1C, and 1.5C, as well as discharge rates
(DDR) of 0.5C, 0.9C, 1.3C, and 1.6C to analyze the internal health and
performance of battery cells during slow, moderate, and fast loading
conditions. Besides, this research proposed a model that incorporates the
Extended Kalman Filter (EKF), Convolutional Neural Network (CNN), and Long
Short-Term Memory (LSTM) networks to validate experimental data. The proposed
model yields excellent modelling results based on mean squared error (MSE), and
root mean squared error (RMSE), with errors of less than 0.001% at DCR and DDR.
The peak identification technique (PIM) has been utilized to investigate
battery health based on the number of peaks, peak position, peak height, peak
area, and peak width. At last, the PIM method has discovered that the cell aged
gradually under normal loading rates but deteriorated rapidly under fast
loading conditions. Overall, LiFePO4 batteries perform more robustly and
consistently than (LiNiCoAlO2) cells under varying loading conditions.

</details>

### [44] [ToolRL: Reward is All Tool Learning Needs](https://arxiv.org/abs/2504.13958)
*Cheng Qian,Emre Can Acikgoz,Qi He,Hongru Wang,Xiusi Chen,Dilek Hakkani-Tür,Gokhan Tur,Heng Ji*

Main category: cs.LG

TLDR: 论文探讨了如何通过强化学习改进大型语言模型的工具使用能力，提出了一种新的奖励设计方法，并在实验中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前监督微调（SFT）在复杂或陌生工具使用场景中泛化能力不足，而强化学习（RL）的奖励设计面临挑战。

Method: 系统研究了奖励策略的类型、规模、粒度和时间动态，提出了一种针对工具使用的奖励设计方法，并采用GRPO训练模型。

Result: 实验表明，该方法在多个基准测试中表现优异，比基础模型提升17%，比SFT模型提升15%。

Conclusion: 研究表明，精心设计的奖励对提升LLM的工具使用能力和泛化性能至关重要，代码已开源。

Abstract: Current Large Language Models (LLMs) often undergo supervised fine-tuning
(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to
unfamiliar or complex tool use scenarios. Recent advancements in reinforcement
learning (RL), particularly with R1-like models, have demonstrated promising
reasoning and generalization abilities. Yet, reward design for tool use
presents unique challenges: multiple tools may be invoked with diverse
parameters, and coarse-grained reward signals, such as answer matching, fail to
offer the finegrained feedback required for effective learning. In this work,
we present the first comprehensive study on reward design for tool selection
and application tasks within the RL paradigm. We systematically explore a wide
range of reward strategies, analyzing their types, scales, granularity, and
temporal dynamics. Building on these insights, we propose a principled reward
design tailored for tool use tasks and apply it to train LLMs using Group
Relative Policy Optimization (GRPO). Empirical evaluations across diverse
benchmarks demonstrate that our approach yields robust, scalable, and stable
training, achieving a 17% improvement over base models and a 15% gain over SFT
models. These results highlight the critical role of thoughtful reward design
in enhancing the tool use capabilities and generalization performance of LLMs.
All the codes are released to facilitate future research.

</details>

### [45] [CONTINA: Confidence Interval for Traffic Demand Prediction with Coverage Guarantee](https://arxiv.org/abs/2504.13961)
*Chao Yang,Xiannan Huang,Shuhan Qiu,Yan Cheng*

Main category: cs.LG

TLDR: 论文提出了一种名为CONTINA的方法，用于生成自适应交通需求预测的置信区间，解决了现有方法在动态交通环境中假设严格的问题。


<details>
  <summary>Details</summary>
Motivation: 现有置信区间建模方法依赖严格假设（如不变的交通模式和正确的模型规范），在动态交通环境中可能失效。

Method: 提出CONTINA方法，通过收集部署期间的区间误差动态调整置信区间（如扩大或缩小），并理论证明其覆盖率收敛于目标水平。

Result: 在四个真实数据集和预测模型上的实验表明，该方法能提供有效且更短的置信区间。

Conclusion: CONTINA方法为交通管理提供了更合理和鲁棒的置信区间，提升了交通系统的操作计划质量。

Abstract: Accurate short-term traffic demand prediction is critical for the operation
of traffic systems. Besides point estimation, the confidence interval of the
prediction is also of great importance. Many models for traffic operations,
such as shared bike rebalancing and taxi dispatching, take into account the
uncertainty of future demand and require confidence intervals as the input.
However, existing methods for confidence interval modeling rely on strict
assumptions, such as unchanging traffic patterns and correct model
specifications, to guarantee enough coverage. Therefore, the confidence
intervals provided could be invalid, especially in a changing traffic
environment. To fill this gap, we propose an efficient method, CONTINA
(Conformal Traffic Intervals with Adaptation) to provide interval predictions
that can adapt to external changes. By collecting the errors of interval during
deployment, the method can adjust the interval in the next step by widening it
if the errors are too large or shortening it otherwise. Furthermore, we
theoretically prove that the coverage of the confidence intervals provided by
our method converges to the target coverage level. Experiments across four
real-world datasets and prediction models demonstrate that the proposed method
can provide valid confidence intervals with shorter lengths. Our method can
help traffic management personnel develop a more reasonable and robust
operation plan in practice. And we release the code, model and dataset in
\href{ https://github.com/xiannanhuang/CONTINA/}{ Github}.

</details>

### [46] [Adversarial Resilience against Clean-Label Attacks in Realizable and Noisy Settings](https://arxiv.org/abs/2504.13966)
*Carolin Heinzler*

Main category: cs.LG

TLDR: 本文研究了在包含未知数量干净标签对抗样本的i.i.d.数据流中，如何建立随机性保证的挑战，允许学习者在不确定时放弃预测，并基于Goel等人的工作修正了其论证中的不准确性。


<details>
  <summary>Details</summary>
Motivation: 解决在干净标签对抗样本存在的情况下，学习者的分类和弃权误差问题，并扩展至不可知设置。

Method: 基于Goel等人的方法，修正其论证错误，并引入干净标签对抗者的概念，提出适用于不可知设置的理论分析。

Result: 首次为阈值分类器在噪声和干净标签对抗者存在的情况下提供了理论分析。

Conclusion: 该方法在可实现设置中有效，但需进一步研究以扩展到更广泛的不可知设置。

Abstract: We investigate the challenge of establishing stochastic-like guarantees when
sequentially learning from a stream of i.i.d. data that includes an unknown
quantity of clean-label adversarial samples. We permit the learner to abstain
from making predictions when uncertain. The regret of the learner is measured
in terms of misclassification and abstention error, where we allow the learner
to abstain for free on adversarial injected samples. This approach is based on
the work of Goel, Hanneke, Moran, and Shetty from arXiv:2306.13119. We explore
the methods they present and manage to correct inaccuracies in their
argumentation.
  However, this approach is limited to the realizable setting, where labels are
assigned according to some function $f^*$ from the hypothesis space
$\mathcal{F}$. Based on similar arguments, we explore methods to make
adaptations for the agnostic setting where labels are random. Introducing the
notion of a clean-label adversary in the agnostic context, we are the first to
give a theoretical analysis of a disagreement-based learner for thresholds,
subject to a clean-label adversary with noise.

</details>

### [47] [Enhancing Stroke Diagnosis in the Brain Using a Weighted Deep Learning Approach](https://arxiv.org/abs/2504.13974)
*Yao Zhiwan,Reza Zarrab,Jean Dubois*

Main category: cs.LG

TLDR: 提出一种加权投票集成（WVE）机器学习模型，用于高效预测脑卒中，准确率达94.91%。


<details>
  <summary>Details</summary>
Motivation: 传统脑卒中诊断方法（如CT和MRI）成本高且耗时，需更高效的预测手段。

Method: 结合随机森林、深度学习和基于直方图的梯度提升等分类器，构建加权投票集成模型。

Result: 在私有数据集上达到94.91%的准确率，支持早期风险评估与预防。

Conclusion: 未来研究可探索优化技术以进一步提升模型准确性。

Abstract: A brain stroke occurs when blood flow to a part of the brain is disrupted,
leading to cell death. Traditional stroke diagnosis methods, such as CT scans
and MRIs, are costly and time-consuming. This study proposes a weighted voting
ensemble (WVE) machine learning model that combines predictions from
classifiers like random forest, Deep Learning, and histogram-based gradient
boosting to predict strokes more effectively. The model achieved 94.91%
accuracy on a private dataset, enabling early risk assessment and prevention.
Future research could explore optimization techniques to further enhance
accuracy.

</details>

### [48] [Multiscale Tensor Summation Factorization as a New Neural Network Layer (MTS Layer) for Multidimensional Data Processing](https://arxiv.org/abs/2504.13975)
*Mehmet Yamaç,Muhammad Numan Yousaf,Serkan Kiranyaz,Moncef Gabbouj*

Main category: cs.LG

TLDR: 论文提出了一种名为多尺度张量求和（MTS）分解的新型神经网络算子，通过多尺度张量求和提升效率，优于传统密集层和卷积层。


<details>
  <summary>Details</summary>
Motivation: 传统多层感知机（MLP）和卷积神经网络（CNN）在计算机视觉任务中因高维输入输出或有限感受野而受限，需更高效的算子。

Method: 引入MTS分解，通过类似Tucker分解的模式乘积实现多尺度张量求和，作为新型骨干网络层。

Result: MTS在参数减少和权重优化效率上优于MLP和CNN，结合多门头（MHG）的MTSNet在性能-复杂度权衡上优于先进Transformer。

Conclusion: MTSNet在分类、压缩和信号恢复等任务中表现优异，代码已开源。

Abstract: Multilayer perceptrons (MLP), or fully connected artificial neural networks,
are known for performing vector-matrix multiplications using learnable weight
matrices; however, their practical application in many machine learning tasks,
especially in computer vision, can be limited due to the high dimensionality of
input-output pairs at each layer. To improve efficiency, convolutional
operators have been utilized to facilitate weight sharing and local
connections, yet they are constrained by limited receptive fields. In this
paper, we introduce Multiscale Tensor Summation (MTS) Factorization, a novel
neural network operator that implements tensor summation at multiple scales,
where each tensor to be summed is obtained through Tucker-decomposition-like
mode products. Unlike other tensor decomposition methods in the literature, MTS
is not introduced as a network compression tool; instead, as a new backbone
neural layer. MTS not only reduces the number of parameters required while
enhancing the efficiency of weight optimization compared to traditional dense
layers (i.e., unfactorized weight matrices in MLP layers), but it also
demonstrates clear advantages over convolutional layers. The proof-of-concept
experimental comparison of the proposed MTS networks with MLPs and
Convolutional Neural Networks (CNNs) demonstrates their effectiveness across
various tasks, such as classification, compression, and signal restoration.
Additionally, when integrated with modern non-linear units such as the
multi-head gate (MHG), also introduced in this study, the corresponding neural
network, MTSNet, demonstrates a more favorable complexity-performance tradeoff
compared to state-of-the-art transformers in various computer vision
applications. The software implementation of the MTS layer and the
corresponding MTS-based networks, MTSNets, is shared at
https://github.com/mehmetyamac/MTSNet.

</details>

### [49] [CacheFormer: High Attention-Based Segment Caching](https://arxiv.org/abs/2504.13981)
*Sushant Singh,Ausif Mahmood*

Main category: cs.LG

TLDR: 提出了一种基于缓存和虚拟内存原理的长上下文处理方法，通过分段和动态检索优化注意力机制，显著降低困惑度。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Linformer、Longformer等未能完全解决长上下文处理问题，需在降低时间复杂度的同时保持质量。

Method: 将长上下文分段，动态检索高注意力未压缩段，结合四种注意力机制（短滑动窗口、长压缩分段、动态检索、重叠段）。

Result: 新架构在困惑度上平均提升8.5%，优于现有SOTA模型。

Conclusion: 该方法通过分段和动态检索有效解决了长上下文处理问题，性能显著提升。

Abstract: Efficiently handling long contexts in transformer-based language models with
low perplexity is an active area of research. Numerous recent approaches like
Linformer, Longformer, Performer, and Structured state space models (SSMs).,
have not fully resolved this problem. All these models strive to reduce the
quadratic time complexity of the attention mechanism while minimizing the loss
in quality due to the effective compression of the long context. Inspired by
the cache and virtual memory principle in computers, where in case of a cache
miss, not only the needed data is retrieved from the memory, but the adjacent
data is also obtained, we apply this concept to handling long contexts by
dividing it into small segments. In our design, we retrieve the nearby segments
in an uncompressed form when high segment-level attention occurs at the
compressed level. Our en-hancements for handling long context include
aggregating four attention mechanisms consisting of short sliding window
attention, long compressed segmented attention, dynamically retrieving top k
high attention uncompressed segments, and overlapping segments in long segment
attention to avoid segment fragmentation. These enhancements result in an
architecture that outperforms ex-isting SOTA architectures with an average
perplexity improvement of 8.5% over similar model sizes.

</details>

### [50] [When Machine Learning Meets Importance Sampling: A More Efficient Rare Event Estimation Approach](https://arxiv.org/abs/2504.13982)
*Ruoning Zhao,Xinyun Chen*

Main category: cs.LG

TLDR: 论文提出了一种新的重要性采样方法，通过利用稳态分布的边际似然比，解决了传统方法中方差爆炸的问题，并结合机器学习算法优化估计效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于电信网络中对串联队列稳态下罕见事件概率估计的需求，传统重要性采样方法因路径依赖似然函数的方差爆炸而效率低下。

Method: 提出了一种基于稳态分布边际似然比的新重要性采样方法，并设计了机器学习算法来估计该边际似然比。

Result: 数值实验表明，新算法在性能上优于传统重要性采样方法。

Conclusion: 通过结合边际似然比和机器学习，新方法有效降低了方差，提升了罕见事件概率估计的效率。

Abstract: Driven by applications in telecommunication networks, we explore the
simulation task of estimating rare event probabilities for tandem queues in
their steady state. Existing literature has recognized that importance sampling
methods can be inefficient, due to the exploding variance of the path-dependent
likelihood functions. To mitigate this, we introduce a new importance sampling
approach that utilizes a marginal likelihood ratio on the stationary
distribution, effectively avoiding the issue of excessive variance. In
addition, we design a machine learning algorithm to estimate this marginal
likelihood ratio using importance sampling data. Numerical experiments indicate
that our algorithm outperforms the classic importance sampling methods.

</details>

### [51] [QuatE-D: A Distance-Based Quaternion Model for Knowledge Graph Embedding](https://arxiv.org/abs/2504.13983)
*Hamideh-Sadat Fazael-Ardakani,Hamid Soltanian-Zadeh*

Main category: cs.LG

TLDR: QuatE-D是一种基于四元数的知识图谱嵌入模型，采用距离评分函数，优于传统内积方法，性能高效且可解释性强。


<details>
  <summary>Details</summary>
Motivation: 传统内积方法在知识图谱嵌入中表现有限，QuatE-D通过距离评分函数提升灵活性和可解释性。

Method: 提出QuatE-D模型，利用欧几里得距离替代内积，优化四元数嵌入的表示能力。

Result: 实验显示QuatE-D性能优异，尤其在降低平均排名上表现突出，参数高效。

Conclusion: 距离评分函数在四元数嵌入中效果显著，为知识图谱补全提供了新方向。

Abstract: Knowledge graph embedding (KGE) methods aim to represent entities and
relations in a continuous space while preserving their structural and semantic
properties. Quaternion-based KGEs have demonstrated strong potential in
capturing complex relational patterns. In this work, we propose QuatE-D, a
novel quaternion-based model that employs a distance-based scoring function
instead of traditional inner-product approaches. By leveraging Euclidean
distance, QuatE-D enhances interpretability and provides a more flexible
representation of relational structures. Experimental results demonstrate that
QuatE-D achieves competitive performance while maintaining an efficient
parameterization, particularly excelling in Mean Rank reduction. These findings
highlight the effectiveness of distance-based scoring in quaternion embeddings,
offering a promising direction for knowledge graph completion.

</details>

### [52] [One Jump Is All You Need: Short-Cutting Transformers for Early Exit Prediction with One Jump to Fit All Exit Levels](https://arxiv.org/abs/2504.13984)
*Amrit Diggavi Seshadri*

Main category: cs.LG

TLDR: 提出了一种名为OJFA的低秩捷径方法，显著降低了推理时的参数成本，同时保持了性能。


<details>
  <summary>Details</summary>
Motivation: 减少大型语言模型推理的时间和计算成本，同时保持性能。

Method: 提出One-Jump-Fits-All（OJFA）低秩捷径，替代多级捷径，减少参数成本。

Result: OJFA在GPT2-XL、Phi3-Mini和Llama2-7B模型上性能接近多级捷径，参数成本降低30倍。

Conclusion: OJFA是一种高效且性能稳定的低秩捷径方法。

Abstract: To reduce the time and computational costs of inference of large language
models, there has been interest in parameter-efficient low-rank early-exit
casting of transformer hidden-representations to final-representations. Such
low-rank short-cutting has been shown to outperform identity shortcuts at early
model stages while offering parameter-efficiency in shortcut jumps. However,
current low-rank methods maintain a separate early-exit shortcut jump to
final-representations for each transformer intermediate block-level during
inference. In this work, we propose selection of a single One-Jump-Fits-All
(OJFA) low-rank shortcut that offers over a 30x reduction in shortcut parameter
costs during inference. We show that despite this extreme reduction, our OJFA
choice largely matches the performance of maintaining multiple shortcut jumps
during inference and offers stable precision from all transformer block-levels
for GPT2-XL, Phi3-Mini and Llama2-7B transformer models.

</details>

### [53] [Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs](https://arxiv.org/abs/2504.13989)
*Lucas Maisonnave,Cyril Moineau,Olivier Bichler,Fabrice Rastello*

Main category: cs.LG

TLDR: 本文提出了一种基于Hadamard矩阵的量化方法，有效减少LLMs中的异常值，实现了3位量化，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在边缘设备上的部署受限于其庞大的参数量，量化是减少内存和推理时间的常用方法，但LLMs中的异常值问题阻碍了低比特量化。

Method: 利用Hadamard矩阵的理论优势，通过逐步二分搜索实现3位量化，适用于权重、激活和KV缓存，并扩展支持非2的幂次嵌入维度。

Result: 实验证明该方法在Mistral、LLaMA和Qwen等模型上优于现有方法，实现了40%的准确率提升。

Conclusion: Hadamard矩阵在减少异常值方面具有理论优势，为LLMs的3位量化提供了实用解决方案。

Abstract: Large language models (LLMs) have become pivotal in artificial intelligence,
demonstrating strong capabilities in reasoning, understanding, and generating
data. However, their deployment on edge devices is hindered by their
substantial size, often reaching several billion parameters. Quantization is a
widely used method to reduce memory usage and inference time, however LLMs
present unique challenges due to the prevalence of outliers in their
activations. In this work, we leverage the theoretical advantages of Hadamard
matrices over random rotation matrices to push the boundaries of quantization
in LLMs. We demonstrate that Hadamard matrices are more effective in reducing
outliers, which are a significant obstacle in achieving low-bit quantization.
Our method based on a gradual binary search enables 3-bit quantization for
weights, activations, and key-value (KV) caches, resulting in a 40\% increase
in accuracy on common benchmarks compared to SoTA methods. We extend the use of
rotation matrices to support non-power-of-2 embedding dimensions, similar to
the Qwen architecture, by employing the Paley algorithm. We theoretically
demonstrates the superiority of Hadamard matrices in reducing outliers.We
achieved 3-bit quantization for weights, activations, and KV cache,
significantly enhancing model performance. Our experimental results on multiple
models family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of
our approach, outperforming existing methods and enabling practical 3-bit
quantization.

</details>

### [54] [PC-DeepNet: A GNSS Positioning Error Minimization Framework Using Permutation-Invariant Deep Neural Network](https://arxiv.org/abs/2504.13990)
*M. Humayun Kabir,Md. Ali Hasan,Md. Shafiqul Islam,Kyeongjun Ko,Wonjae Shin*

Main category: cs.LG

TLDR: 提出了一种基于学习的框架PC-DeepNet，用于解决GNSS在城郊环境中的定位问题，通过PI-DNN估计位置修正，显著提高了定位精度。


<details>
  <summary>Details</summary>
Motivation: GNSS在城郊环境中因NLOS传播、多径效应和低接收功率导致误差分布非线性和非高斯，传统基于高斯误差的定位方法效果不佳。

Method: 使用PI-DNN（排列不变深度神经网络）估计位置修正，利用NLOS和多径指标作为特征，提升定位精度。

Result: 在公开数据集上验证，PC-DeepNet的定位精度优于现有基于模型和基于学习的方法，且计算复杂度更低。

Conclusion: PC-DeepNet在城郊环境中表现出优越的定位性能和计算效率，为GNSS定位提供了新的解决方案。

Abstract: Global navigation satellite systems (GNSS) face significant challenges in
urban and sub-urban areas due to non-line-of-sight (NLOS) propagation,
multipath effects, and low received power levels, resulting in highly
non-linear and non-Gaussian measurement error distributions. In light of this,
conventional model-based positioning approaches, which rely on Gaussian error
approximations, struggle to achieve precise localization under these
conditions. To overcome these challenges, we put forth a novel learning-based
framework, PC-DeepNet, that employs a permutation-invariant (PI) deep neural
network (DNN) to estimate position corrections (PC). This approach is designed
to ensure robustness against changes in the number and/or order of visible
satellite measurements, a common issue in GNSS systems, while leveraging NLOS
and multipath indicators as features to enhance positioning accuracy in
challenging urban and sub-urban environments. To validate the performance of
the proposed framework, we compare the positioning error with state-of-the-art
model-based and learning-based positioning methods using two publicly available
datasets. The results confirm that proposed PC-DeepNet achieves superior
accuracy than existing model-based and learning-based methods while exhibiting
lower computational complexity compared to previous learning-based approaches.

</details>

### [55] [Deep Learning on Graphs for Mobile Network Topology Generation](https://arxiv.org/abs/2504.13991)
*Felix Nannesson Meli,Johan Tell,Shirwan Piroti,Tahar Zanouda,Elias Jarlebring*

Main category: cs.LG

TLDR: 该论文提出使用基于图的深度学习方法（如GNN和MLP）来优化移动网络拓扑中的移动关系（边）的确定，并通过实验验证了其准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法在移动网络拓扑中建立移动关系时存在局限性（如硬件安装前需预先定义边），因此需要更灵活且高效的方法。

Method: 利用基于图的深度学习方法（GNN和MLP），基于无线电节点配置数据和ANR提供的可靠移动关系进行训练，并通过启发式方法（如节点距离）减少训练时间。

Result: 实验表明，GNN模型在考虑图结构时表现更优，启发式方法显著提高了精度和准确性。

Conclusion: 基于图的深度学习方法（尤其是GNN）在优化移动网络拓扑中具有潜力，启发式方法可进一步提升效率。

Abstract: Mobile networks consist of interconnected radio nodes strategically
positioned across various geographical regions to provide connectivity
services. The set of relations between these radio nodes, referred to as the
\emph{mobile network topology}, is vital in the construction of the networking
infrastructure. Typically, the connections between radio nodes and their
associated cells are defined by software features that establish mobility
relations (referred to as \emph{edges} in this paper) within the mobile network
graph through heuristic methods. Although these approaches are efficient, they
encounter significant limitations, particularly since edges can only be
established prior to the installation of physical hardware.
  In this work, we use graph-based deep learning methods to determine mobility
relations (edges), trained on radio node configuration data and reliable
mobility relations set by Automatic Neighbor Relations (ANR) in stable
networks. This paper focuses on measuring the accuracy and precision of
different graph-based deep learning approaches applied to real-world mobile
networks. We evaluated two deep learning models. Our comprehensive experiments
on Telecom datasets obtained from operational Telecom Networks demonstrate the
effectiveness of the graph neural network (GNN) model and multilayer
perceptron. Our evaluation showed that considering graph structure improves
results, which motivates the use of GNNs. Additionally, we investigated the use
of heuristics to reduce the training time based on the distance between radio
nodes to eliminate irrelevant cases. Our investigation showed that the use of
these heuristics improved precision and accuracy considerably.

</details>

### [56] [First and Second Order Approximations to Stochastic Gradient Descent Methods with Momentum Terms](https://arxiv.org/abs/2504.13992)
*Eric Lu*

Main category: cs.LG

TLDR: 论文研究了随机梯度下降（SGD）的动态行为，特别是允许学习率和动量参数随时间变化的情况。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅覆盖恒定学习率或无动量项的SGD，缺乏对学习率和动量参数随时间变化的严格分析。

Method: 通过连续近似方法，研究SGD在弱假设下的动态行为。

Result: 提出了在允许学习率和动量参数随时间变化的情况下SGD的近似结果。

Conclusion: 该研究为理解SGD的动态行为提供了更全面的理论支持。

Abstract: Stochastic Gradient Descent (SGD) methods see many uses in optimization
problems. Modifications to the algorithm, such as momentum-based SGD methods
have been known to produce better results in certain cases. Much of this,
however, is due to empirical information rather than rigorous proof. While the
dynamics of gradient descent methods can be studied through continuous
approximations, existing works only cover scenarios with constant learning
rates or SGD without momentum terms. We present approximation results under
weak assumptions for SGD that allow learning rates and momentum parameters to
vary with respect to time.

</details>

### [57] [Large Language Bayes](https://arxiv.org/abs/2504.14025)
*Justin Domke*

Main category: cs.LG

TLDR: 该论文提出了一种结合大型语言模型和概率编程语言的方法，通过输入非正式问题描述生成联合分布，解决了领域专家难以编写正式贝叶斯模型的问题。


<details>
  <summary>Details</summary>
Motivation: 许多领域专家缺乏时间或训练来编写正式的贝叶斯模型，因此需要一种自动化方法从非正式描述生成模型。

Method: 结合大型语言模型和概率编程语言生成联合分布，通过条件化和积分处理观察数据，提出了一种基于重要性采样、MCMC和变分推断的推理方法。

Result: 该方法无需指定正式模型即可生成合理的预测。

Conclusion: 通过自动化生成模型和推理，该方法为领域专家提供了一种高效的工具，无需深入掌握贝叶斯建模技术。

Abstract: Many domain experts do not have the time or training to write formal Bayesian
models. This paper takes an informal problem description as input, and combines
a large language model and a probabilistic programming language to create a
joint distribution over formal models, latent variables, and data. A posterior
over latent variables follows by conditioning on observed data and integrating
over formal models. This presents a challenging inference problem. We suggest
an inference recipe that amounts to generating many formal models from the
large language model, performing approximate inference on each, and then doing
a weighted average. This is justified an analyzed as a combination of
self-normalized importance sampling, MCMC, and variational inference. We show
that this produces sensible predictions without the need to specify a formal
model.

</details>

### [58] [A synthetic dataset of French electric load curves with temperature conditioning](https://arxiv.org/abs/2504.14046)
*Tahar Nabil,Ghislain Agoua,Pierre Cauchois,Anne De Moliner,Benoît Grossin*

Main category: cs.LG

TLDR: 论文提出了一种基于条件潜在扩散的合成负载曲线数据集，用于解决能源转型中智能电表数据的隐私问题，并验证了其保真度、实用性和隐私性。


<details>
  <summary>Details</summary>
Motivation: 能源转型导致用电行为变化，需要访问智能电表数据，但这类数据受GDPR保护，需生成隐私保护的合成数据。

Method: 使用条件潜在扩散方法生成合成负载曲线数据集，并提供合同功率、分时电价计划和本地温度数据。

Result: 数据集在保真度、实用性和隐私性方面表现良好，适用于能源建模应用。

Conclusion: 合成的数据集质量高，为能源建模提供了隐私保护的解决方案。

Abstract: The undergoing energy transition is causing behavioral changes in electricity
use, e.g. with self-consumption of local generation, or flexibility services
for demand control. To better understand these changes and the challenges they
induce, accessing individual smart meter data is crucial. Yet this is personal
data under the European GDPR. A widespread use of such data requires thus to
create synthetic realistic and privacy-preserving samples. This paper
introduces a new synthetic load curve dataset generated by conditional latent
diffusion. We also provide the contracted power, time-of-use plan and local
temperature used for generation. Fidelity, utility and privacy of the dataset
are thoroughly evaluated, demonstrating its good quality and thereby supporting
its interest for energy modeling applications.

</details>

### [59] [CAOTE: KV Caching through Attention Output Error based Token Eviction](https://arxiv.org/abs/2504.14051)
*Raghavv Goel,Junyoung Park,Mukul Gagrani,Dalton Jones,Matthew Morse,Harper Langston,Mingu Lee,Chris Lott*

Main category: cs.LG

TLDR: 论文提出了一种新的令牌驱逐标准CAOTE，结合注意力分数和值向量信息，优化令牌驱逐过程中的误差，提升下游任务准确性。


<details>
  <summary>Details</summary>
Motivation: 长上下文支持的大语言模型在资源受限设备上存在内存和计算瓶颈，现有基于注意力分数的令牌驱逐方法缺乏对令牌贡献的全面评估。

Method: 提出CAOTE方法，通过结合注意力分数和值向量信息，优化令牌驱逐误差，可作为元启发式方法与其他驱逐方法结合使用。

Result: CAOTE与现有基于注意力分数的方法结合时，能持续提升下游任务的准确性。

Conclusion: CAOTE证明了在令牌驱逐过程中利用值向量信息的重要性，为资源受限设备提供了一种高效的解决方案。

Abstract: While long context support of large language models has extended their
abilities, it also incurs challenges in memory and compute which becomes
crucial bottlenecks in resource-restricted devices. Token eviction, a widely
adopted post-training methodology designed to alleviate the bottlenecks by
evicting less important tokens from the cache, typically uses attention scores
as proxy metrics for token importance. However, one major limitation of
attention score as a token-wise importance metrics is that it lacks the
information about contribution of tokens to the attention output. In this
paper, we propose a simple eviction criterion based on the contribution of
cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction
error due to token eviction, by seamlessly integrating attention scores and
value vectors. This is the first method which uses value vector information on
top of attention-based eviction scores. Additionally, CAOTE can act as a
meta-heuristic method with flexible usage with any token eviction method. We
show that CAOTE, when combined with the state-of-the-art attention score-based
methods, always improves accuracies on the downstream task, indicating the
importance of leveraging information from values during token eviction process.

</details>

### [60] [Contextual Embedding-based Clustering to Identify Topics for Healthcare Service Improvement](https://arxiv.org/abs/2504.14068)
*K M Sajjadul Islam,Ravi Teja Karri,Srujan Vegesna,Jiawei Wu,Praveen Madiraju*

Main category: cs.LG

TLDR: 研究探索了无监督方法分析医疗反馈，提出kBERT模型，结合BERT嵌入和k-means聚类，在短文本分析中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 理解患者反馈对改进医疗服务至关重要，但传统监督方法需要大量标注数据，无监督方法更适合短文本分析。

Method: 使用LDA、GSDMM和BERTopic等传统和先进方法，提出kBERT模型（BERT嵌入+k-means），评估指标为Cv和IRBOavg。

Result: kBERT在主题一致性（Cv=0.53）和多样性（IRBOavg=1.00）上表现最优，优于其他模型。

Conclusion: 嵌入技术对医疗反馈分析至关重要，需结合上下文感知模型。

Abstract: Understanding patient feedback is crucial for improving healthcare services,
yet analyzing unlabeled short-text feedback presents significant challenges due
to limited data and domain-specific nuances. Traditional supervised learning
approaches require extensive labeled datasets, making unsupervised methods more
viable for uncovering meaningful insights from patient feedback. This study
explores unsupervised methods to extract meaningful topics from 439 survey
responses collected from a healthcare system in Wisconsin, USA. A keyword-based
filtering approach was applied to isolate complaint-related feedback using a
domain-specific lexicon. To delve deeper and analyze dominant topics in
feedback, we explored traditional topic modeling methods, including Latent
Dirichlet Allocation (LDA) and Gibbs Sampling Dirichlet Multinomial Mixture
(GSDMM), alongside BERTopic, an advanced neural embedding-based clustering
approach. To improve coherence and interpretability where data are scarce and
consist of short-texts, we propose kBERT, an integration of BERT embeddings
with k-means clustering. Model performance was assessed using coherence scores
(Cv ) for topic interpretability and average Inverted Rank-Biased Overlap
(IRBOavg) for topic diversity. Results indicate that kBERT achieves the highest
coherence (Cv = 0.53) and distinct topic separation (IRBOavg = 1.00),
outperforming all other models in short-text healthcare feedback analysis. Our
findings emphasize the importance of embedding-based techniques for topic
identification and highlight the need for context-aware models in healthcare
analytics.

</details>

### [61] [Leakage and Interpretability in Concept-Based Models](https://arxiv.org/abs/2504.14094)
*Enrico Parisini,Tapabrata Chakraborti,Chris Harbron,Ben D. MacArthur,Christopher R. S. Banerji*

Main category: cs.LG

TLDR: 论文提出了一个信息论框架，用于量化和描述概念瓶颈模型中的信息泄漏问题，并提出了两种互补的泄漏度量方法。


<details>
  <summary>Details</summary>
Motivation: 概念瓶颈模型在高风险场景中具有潜力，但存在信息泄漏问题，影响模型的可靠性和可解释性。

Method: 引入信息论框架，定义概念-任务泄漏（CTL）和概念间泄漏（ICL）评分，并通过实验验证其有效性。

Result: 提出的泄漏度量方法能更准确地预测模型行为，且发现概念嵌入模型普遍存在泄漏问题。

Conclusion: 论文提供了减少泄漏的实用设计指南，以提升概念模型的可靠性和可解释性。

Abstract: Concept Bottleneck Models aim to improve interpretability by predicting
high-level intermediate concepts, representing a promising approach for
deployment in high-risk scenarios. However, they are known to suffer from
information leakage, whereby models exploit unintended information encoded
within the learned concepts. We introduce an information-theoretic framework to
rigorously characterise and quantify leakage, and define two complementary
measures: the concepts-task leakage (CTL) and interconcept leakage (ICL)
scores. We show that these measures are strongly predictive of model behaviour
under interventions and outperform existing alternatives in robustness and
reliability. Using this framework, we identify the primary causes of leakage
and provide strong evidence that Concept Embedding Models exhibit substantial
leakage regardless of the hyperparameters choice. Finally, we propose practical
guidelines for designing concept-based models to reduce leakage and ensure
interpretability.

</details>

### [62] [Personalizing Exposure Therapy via Reinforcement Learning](https://arxiv.org/abs/2504.14095)
*Athar Mahmoudi-Nejad,Matthew Guzdial,Pierre Boulanger*

Main category: cs.LG

TLDR: 提出一种基于生理指标的个性化治疗方法，通过强化学习生成虚拟蜘蛛，显著优于传统规则方法。


<details>
  <summary>Details</summary>
Motivation: 传统个性化治疗依赖治疗师的直觉和预定义规则，难以普适化。

Method: 采用经验驱动的程序化内容生成（EDPCGRL）和强化学习，根据患者生理指标动态调整虚拟蜘蛛。

Result: 实验证明，该方法显著优于传统规则方法。

Conclusion: 该方法有潜力提升个性化治疗的效果。

Abstract: Personalized therapy, in which a therapeutic practice is adapted to an
individual patient, can lead to improved health outcomes. Typically, this is
accomplished by relying on a therapist's training and intuition along with
feedback from a patient. However, this requires the therapist to become an
expert on any technological components, such as in the case of Virtual Reality
Exposure Therapy (VRET). While there exist approaches to automatically adapt
therapeutic content to a patient, they generally rely on hand-authored,
pre-defined rules, which may not generalize to all individuals. In this paper,
we propose an approach to automatically adapt therapeutic content to patients
based on physiological measures. We implement our approach in the context of
virtual reality arachnophobia exposure therapy, and rely on experience-driven
procedural content generation via reinforcement learning (EDPCGRL) to generate
virtual spiders to match an individual patient. Through a human subject study,
we demonstrate that our system significantly outperforms a more common
rules-based method, highlighting its potential for enhancing personalized
therapeutic interventions.

</details>

### [63] [Enhancing Math Learning in an LMS Using AI-Driven Question Recommendations](https://arxiv.org/abs/2504.14098)
*Justus Råmunddal*

Main category: cs.LG

TLDR: 论文提出了一种基于AI的方法，通过推荐相似数学问题来优化现代学习管理系统（LMS）中的数学学习。使用Meta的Llama-3.2-11B-Vision-Instruct模型生成数学问题的深度嵌入，并应用余弦相似度、自组织映射（SOM）和高斯混合模型（GMM）三种推荐方法。实验表明，余弦相似度匹配最相似问题，SOM用户满意度更高，而GMM表现较差。


<details>
  <summary>Details</summary>
Motivation: 通过AI技术提升数学学习效果，利用推荐系统帮助学生在LMS中更高效地学习数学。

Method: 使用Llama-3.2-11B-Vision-Instruct模型生成数学问题的深度嵌入，并比较余弦相似度、SOM和GMM三种推荐方法的效果。

Result: 余弦相似度匹配最相似问题，SOM用户满意度更高，GMM表现较差。适度的多样性可能提升学习效果。

Conclusion: 推荐相似数学问题时，SOM方法在用户满意度和学习效果上表现最佳，而GMM效果不佳。适度的多样性有助于提升学习体验。

Abstract: This paper presents an AI-driven approach to enhance math learning in a
modern Learning Management System (LMS) by recommending similar math questions.
Deep embeddings for math questions are generated using Meta's
Llama-3.2-11B-Vision-Instruct model, and three recommendation methods-cosine
similarity, Self-Organizing Maps (SOM), and Gaussian Mixture Models (GMM)-are
applied to identify similar questions. User interaction data, including session
durations, response times, and correctness, are used to evaluate the methods.
Our findings suggest that while cosine similarity produces nearly identical
question matches, SOM yields higher user satisfaction whereas GMM generally
underperforms, indicating that introducing variety to a certain degree may
enhance engagement and thereby potential learning outcomes until variety is no
longer balanced reasonably, which our data about the implementations of all
three methods demonstrate.

</details>

### [64] [Predicting Stress and Damage in Carbon Fiber-Reinforced Composites Deformation Process using Composite U-Net Surrogate Model](https://arxiv.org/abs/2504.14143)
*Zeping Chen,Marwa Yacouti,Maryam Shakiba,Jian-Xun Wang,Tengfei Luo,Vikas Varshney*

Main category: cs.LG

TLDR: 提出了一种新型自回归复合U-Net深度学习模型，用于预测碳纤维增强复合材料（CFRC）变形过程中的应力和损伤场，解决了传统方法的计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 碳纤维增强复合材料（CFRC）在工程应用中具有重要地位，但传统有限元方法（FEM）计算效率低，现有数据驱动模型无法全面捕捉应力和损伤的演化过程。

Method: 采用U-Net架构的自回归复合深度学习模型，结合宏微观现象，预测CFRC在单向应变下的应力和损伤分布。

Result: 模型在预测CFRC微观结构中的应力和损伤分布演化方面具有高精度，计算速度比IGFEM快60倍以上。

Conclusion: 该模型为CFRC的应力和损伤预测提供了高效且准确的解决方案，优于传统方法。

Abstract: Carbon fiber-reinforced composites (CFRC) are pivotal in advanced engineering
applications due to their exceptional mechanical properties. A deep
understanding of CFRC behavior under mechanical loading is essential for
optimizing performance in demanding applications such as aerospace structures.
While traditional Finite Element Method (FEM) simulations, including advanced
techniques like Interface-enriched Generalized FEM (IGFEM), offer valuable
insights, they can struggle with computational efficiency. Existing data-driven
surrogate models partially address these challenges by predicting propagated
damage or stress-strain behavior but fail to comprehensively capture the
evolution of stress and damage throughout the entire deformation history,
including crack initiation and propagation. This study proposes a novel
auto-regressive composite U-Net deep learning model to simultaneously predict
stress and damage fields during CFRC deformation. By leveraging the U-Net
architecture's ability to capture spatial features and integrate macro- and
micro-scale phenomena, the proposed model overcomes key limitations of prior
approaches. The model achieves high accuracy in predicting evolution of stress
and damage distribution within the microstructure of a CFRC under
unidirectional strain, offering a speed-up of over 60 times compared to IGFEM.

</details>

### [65] [A Physics-guided Multimodal Transformer Path to Weather and Climate Sciences](https://arxiv.org/abs/2504.14174)
*Jing Han,Hanting Chen,Kai Han,Xiaomeng Huang,Yongyun Hu,Wenjun Xu,Dacheng Tao,Ping Zhang*

Main category: cs.LG

TLDR: 论文探讨了AI在气象学中的应用，提出了一种多模态数据融合的新范式，结合物理信号和正则化技术提升模型能力。


<details>
  <summary>Details</summary>
Motivation: 传统气象方法精度有限，AI模型能显著提升准确性，同时结合物理信号增强可解释性。

Method: 将气象数据转化为2D/3D图像或视频，通过transformer整合多模态数据，并利用正则化技术融入气象知识。

Result: 新范式具有强通用性，可处理多种任务，同时提升模型精度和可解释性。

Conclusion: 未来需进一步优化模型精度和可解释性，推动AI在气象领域的应用。

Abstract: With the rapid development of machine learning in recent years, many problems
in meteorology can now be addressed using AI models. In particular, data-driven
algorithms have significantly improved accuracy compared to traditional
methods. Meteorological data is often transformed into 2D images or 3D videos,
which are then fed into AI models for learning. Additionally, these models
often incorporate physical signals, such as temperature, pressure, and wind
speed, to further enhance accuracy and interpretability. In this paper, we
review several representative AI + Weather/Climate algorithms and propose a new
paradigm where observational data from different perspectives, each with
distinct physical meanings, are treated as multimodal data and integrated via
transformers. Furthermore, key weather and climate knowledge can be
incorporated through regularization techniques to further strengthen the
model's capabilities. This new paradigm is versatile and can address a variety
of tasks, offering strong generalizability. We also discuss future directions
for improving model accuracy and interpretability.

</details>

### [66] [FedC4: Graph Condensation Meets Client-Client Collaboration for Efficient and Private Federated Graph Learning](https://arxiv.org/abs/2504.14188)
*Zekai Chen,Xunkai Li,Yinlin Zhu,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TLDR: FedC4是一个结合图压缩与客户端协作的联邦图学习框架，通过生成紧凑的合成节点嵌入减少通信开销和隐私风险，并支持个性化优化。


<details>
  <summary>Details</summary>
Motivation: 现有的客户端-客户端（C-C）联邦图学习方法存在通信成本高和隐私风险的问题，需要一种更高效的解决方案。

Method: 提出FedC4框架，通过图压缩技术生成合成节点嵌入，并引入三个模块支持个性化优化。

Result: 在八个真实数据集上的实验表明，FedC4在性能和通信效率上优于现有基线方法。

Conclusion: FedC4通过减少通信开销和增强隐私保护，为联邦图学习提供了一种高效的解决方案。

Abstract: Federated Graph Learning (FGL) is an emerging distributed learning paradigm
that enables collaborative model training over decentralized graph-structured
data while preserving local privacy. Existing FGL methods can be categorized
into two optimization architectures: (1) the Server-Client (S-C) paradigm,
where clients upload local models for server-side aggregation; and (2) the
Client-Client (C-C) paradigm, which allows direct information exchange among
clients to support personalized training. Compared to S-C, the C-C architecture
better captures global graph knowledge and enables fine-grained optimization
through customized peer-to-peer communication. However, current C-C methods
often broadcast identical and redundant node embeddings, incurring high
communication costs and privacy risks. To address this, we propose FedC4, a
novel framework that combines graph Condensation with Client-Client
Collaboration. Instead of transmitting raw node-level features, FedC4 distills
each client's private graph into a compact set of synthetic node embeddings,
reducing communication overhead and enhancing privacy. In addition, FedC4
introduces three modules that allow source clients to send distinct node
representations tailored to target clients'graph structures, enabling
personalized optimization with global guidance. Extensive experiments on eight
real-world datasets show that FedC4 outperforms state-of-the-art baselines in
both performance and communication efficiency.

</details>

### [67] [DConAD: A Differencing-based Contrastive Representation Learning Framework for Time Series Anomaly Detection](https://arxiv.org/abs/2504.14204)
*Wenxin Zhang,Xiaojian Lin,Wenjun Yu,Guangzhen Yao,jingxiang Zhong,Yu Li,Renda Han,Songcheng Xu,Hao Shi,Cuicui Luo*

Main category: cs.LG

TLDR: DConAD是一种基于差分对比表示学习的时间序列异常检测框架，通过生成差分数据和Transformer架构增强模型对正常模式的捕捉能力，避免对高质量先验知识的依赖。


<details>
  <summary>Details</summary>
Motivation: 时间序列异常检测在风险识别和故障检测中至关重要，但现有无监督学习方法因异常模式多样、稀疏及数据复杂性增加而难以捕捉稳健依赖关系。

Method: DConAD通过生成差分数据提供额外信息，利用Transformer捕捉时空依赖，并采用基于KL散度的对比学习范式（仅用正样本）和停止梯度策略。

Result: 在五个公共数据集上的实验表明，DConAD优于九种基线方法。

Conclusion: DConAD通过差分对比学习显著提升了时间序列异常检测的鲁棒性和有效性。

Abstract: Time series anomaly detection holds notable importance for risk
identification and fault detection across diverse application domains.
Unsupervised learning methods have become popular because they have no
requirement for labels. However, due to the challenges posed by the
multiplicity of abnormal patterns, the sparsity of anomalies, and the growth of
data scale and complexity, these methods often fail to capture robust and
representative dependencies within the time series for identifying anomalies.
To enhance the ability of models to capture normal patterns of time series and
avoid the retrogression of modeling ability triggered by the dependencies on
high-quality prior knowledge, we propose a differencing-based contrastive
representation learning framework for time series anomaly detection (DConAD).
Specifically, DConAD generates differential data to provide additional
information about time series and utilizes transformer-based architecture to
capture spatiotemporal dependencies, which enhances the robustness of unbiased
representation learning ability. Furthermore, DConAD implements a novel KL
divergence-based contrastive learning paradigm that only uses positive samples
to avoid deviation from reconstruction and deploys the stop-gradient strategy
to compel convergence. Extensive experiments on five public datasets show the
superiority and effectiveness of DConAD compared with nine baselines. The code
is available at https://github.com/shaieesss/DConAD.

</details>

### [68] [Dual-channel Heterophilic Message Passing for Graph Fraud Detection](https://arxiv.org/abs/2504.14205)
*Wenxin Zhang,Jingxing Zhong,Guangzhen Yao,Renda Han,Xiaojian Lin,Zeyu Zhang,Cuicui Luo*

Main category: cs.LG

TLDR: 论文提出了一种名为DHMP的双通道异质性消息传递框架，用于欺诈检测，通过分离同质性和异质性子图，优化了传统GNN的低通归纳偏差，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有空间GNN方法在欺诈检测中常因排除异质性邻居而破坏原始图拓扑并增加预测不确定性，DHMP旨在解决这一问题。

Method: DHMP通过异质性分离模块将图分为同质性和异质性子图，使用共享权重独立捕获不同频率信号，并结合定制采样策略进行训练。

Result: 在三个真实数据集上的实验表明，DHMP优于现有方法，验证了分离不同频率信号对欺诈检测的重要性。

Conclusion: DHMP通过双通道消息传递框架有效提升了欺诈检测性能，代码已开源。

Abstract: Fraudulent activities have significantly increased across various domains,
such as e-commerce, online review platforms, and social networks, making fraud
detection a critical task. Spatial Graph Neural Networks (GNNs) have been
successfully applied to fraud detection tasks due to their strong inductive
learning capabilities. However, existing spatial GNN-based methods often
enhance the graph structure by excluding heterophilic neighbors during message
passing to align with the homophilic bias of GNNs. Unfortunately, this approach
can disrupt the original graph topology and increase uncertainty in
predictions. To address these limitations, this paper proposes a novel
framework, Dual-channel Heterophilic Message Passing (DHMP), for fraud
detection. DHMP leverages a heterophily separation module to divide the graph
into homophilic and heterophilic subgraphs, mitigating the low-pass inductive
bias of traditional GNNs. It then applies shared weights to capture signals at
different frequencies independently and incorporates a customized sampling
strategy for training. This allows nodes to adaptively balance the
contributions of various signals based on their labels. Extensive experiments
on three real-world datasets demonstrate that DHMP outperforms existing
methods, highlighting the importance of separating signals with different
frequencies for improved fraud detection. The code is available at
https://github.com/shaieesss/DHMP.

</details>

### [69] [Decomposition-based multi-scale transformer framework for time series anomaly detection](https://arxiv.org/abs/2504.14206)
*Wenxin Zhang,Cuicui Luo*

Main category: cs.LG

TLDR: 提出了一种基于分解和Transformer的框架（TransDe），用于多变量时间序列异常检测，结合时间序列分解和Transformer的优势，有效学习复杂模式，并通过对比学习和异步损失函数提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以直接建模时间序列中多样复杂模式的依赖关系，且对噪声敏感导致性能下降。

Method: 提出TransDe框架，结合时间序列分解和多尺度基于块的Transformer架构，利用对比学习和KL散度对齐正样本，并引入异步损失函数。

Result: 在五个公开数据集上实验，TransDe在F1分数上优于12个基线方法。

Conclusion: TransDe通过分解和Transformer结合，有效解决了时间序列异常检测中的复杂模式建模和噪声问题，性能显著提升。

Abstract: Time series anomaly detection is crucial for maintaining stable systems.
Existing methods face two main challenges. First, it is difficult to directly
model the dependencies of diverse and complex patterns within the sequences.
Second, many methods that optimize parameters using mean squared error struggle
with noise in the time series, leading to performance deterioration. To address
these challenges, we propose a transformer-based framework built on
decomposition (TransDe) for multivariate time series anomaly detection. The key
idea is to combine the strengths of time series decomposition and transformers
to effectively learn the complex patterns in normal time series data. A
multi-scale patch-based transformer architecture is proposed to exploit the
representative dependencies of each decomposed component of the time series.
Furthermore, a contrastive learn paradigm based on patch operation is proposed,
which leverages KL divergence to align the positive pairs, namely the pure
representations of normal patterns between different patch-level views. A novel
asynchronous loss function with a stop-gradient strategy is further introduced
to enhance the performance of TransDe effectively. It can avoid time-consuming
and labor-intensive computation costs in the optimization process. Extensive
experiments on five public datasets are conducted and TransDe shows superiority
compared with twelve baselines in terms of F1 score. Our code is available at
https://github.com/shaieesss/TransDe.

</details>

### [70] [A Novel Frequency-Spatial Domain Aware Network for Fast Thermal Prediction in 2.5D ICs](https://arxiv.org/abs/2504.14237)
*Dekang Zhang,Dan Niu,Zhou Jin,Yichao Dong,Jingweijia Tan,Changyin Sun*

Main category: cs.LG

TLDR: 提出了一种新型频率-空间双域感知预测网络（FSA-Heat），用于2.5D IC的高效热预测，显著提升了预测精度和速度。


<details>
  <summary>Details</summary>
Motivation: 在2.5D芯片中，传统CNN和GCN方法无法有效捕捉全局热特征，尤其是高频组件，限制了预测精度。

Method: 结合高频-低频和空间域编码器（FSTE）与频率域跨尺度交互模块（FCIFormer），并设计了频率-空间混合损失（FSL）来优化预测。

Result: 实验显示，FSA-Heat在RMSE上降低了99%，推理速度提升了4.23倍，且具有强泛化能力。

Conclusion: FSA-Heat在2.5D IC热预测中表现出色，优于现有方法，具有实际应用潜力。

Abstract: In the post-Moore era, 2.5D chiplet-based ICs present significant challenges
in thermal management due to increased power density and thermal hotspots.
Neural network-based thermal prediction models can perform real-time
predictions for many unseen new designs. However, existing CNN-based and
GCN-based methods cannot effectively capture the global thermal features,
especially for high-frequency components, hindering prediction accuracy
enhancement. In this paper, we propose a novel frequency-spatial dual domain
aware prediction network (FSA-Heat) for fast and high-accuracy thermal
prediction in 2.5D ICs. It integrates high-to-low frequency and spatial domain
encoder (FSTE) module with frequency domain cross-scale interaction module
(FCIFormer) to achieve high-to-low frequency and global-to-local thermal
dissipation feature extraction. Additionally, a frequency-spatial hybrid loss
(FSL) is designed to effectively attenuate high-frequency thermal gradient
noise and spatial misalignments. The experimental results show that the
performance enhancements offered by our proposed method are substantial,
outperforming the newly-proposed 2.5D method, GCN+PNA, by considerable margins
(over 99% RMSE reduction, 4.23X inference time speedup). Moreover, extensive
experiments demonstrate that FSA-Heat also exhibits robust generalization
capabilities.

</details>

### [71] [Do You Really Need Public Data? Surrogate Public Data for Differential Privacy on Tabular Data](https://arxiv.org/abs/2504.14368)
*Shlomi Hod,Lucas Rosenblatt,Julia Stoyanovich*

Main category: cs.LG

TLDR: 论文提出利用强大的先验知识生成替代公共数据，以解决差分隐私机器学习中公共数据不足的问题，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在差分隐私机器学习中，公共数据常用于隐私-效用权衡估计等任务，但表格数据的异质性导致公共数据假设不成立。

Method: 提出两种方法：直接生成CSV文件和自动化构建结构因果模型（SCM），利用大语言模型生成替代公共数据。

Result: 实验表明，替代公共数据能有效替代传统公共数据用于差分隐私表格分类器的预训练，并在其他任务中也有一定作用。

Conclusion: 替代公共数据是一种可行的解决方案，尤其适用于表格数据领域。

Abstract: Differentially private (DP) machine learning often relies on the availability
of public data for tasks like privacy-utility trade-off estimation,
hyperparameter tuning, and pretraining. While public data assumptions may be
reasonable in text and image domains, they are less likely to hold for tabular
data due to tabular data heterogeneity across domains. We propose leveraging
powerful priors to address this limitation; specifically, we synthesize
realistic tabular data directly from schema-level specifications - such as
variable names, types, and permissible ranges - without ever accessing
sensitive records. To that end, this work introduces the notion of "surrogate"
public data - datasets generated independently of sensitive data, which consume
no privacy loss budget and are constructed solely from publicly available
schema or metadata. Surrogate public data are intended to encode plausible
statistical assumptions (informed by publicly available information) into a
dataset with many downstream uses in private mechanisms. We automate the
process of generating surrogate public data with large language models (LLMs);
in particular, we propose two methods: direct record generation as CSV files,
and automated structural causal model (SCM) construction for sampling records.
Through extensive experiments, we demonstrate that surrogate public tabular
data can effectively replace traditional public data when pretraining
differentially private tabular classifiers. To a lesser extent, surrogate
public data are also useful for hyperparameter tuning of DP synthetic data
generators, and for estimating the privacy-utility tradeoff.

</details>

### [72] [A Pre-Training and Adaptive Fine-Tuning Framework for Graph Anomaly Detection](https://arxiv.org/abs/2504.14250)
*Yunhui Liu,Jiashun Cheng,Jia Li,Fugee Tsung,Hongzhi Yin,Tieke He*

Main category: cs.LG

TLDR: 论文提出了一种针对图异常检测（GAD）的预训练和自适应微调框架（PAF），通过联合低通和高通滤波器捕捉节点特征的完整频率信息，并在微调阶段自适应融合节点表示。


<details>
  <summary>Details</summary>
Motivation: 图异常检测因异常节点稀少和标注成本高而具有挑战性，且传统基于强同质性的图预训练方法在GAD中因局部异质性而表现不佳。

Method: 提出PAF框架，预训练阶段联合使用低通和高通滤波器，微调阶段通过门控融合网络自适应结合节点表示。

Result: 在十个基准数据集上的实验表明PAF的有效性。

Conclusion: PAF通过选择性应用滤波器解决了GAD中同质性与异质性混合的挑战，显著提升了检测性能。

Abstract: Graph anomaly detection (GAD) has garnered increasing attention in recent
years, yet it remains challenging due to the scarcity of abnormal nodes and the
high cost of label annotations. Graph pre-training, the two-stage learning
paradigm, has emerged as an effective approach for label-efficient learning,
largely benefiting from expressive neighborhood aggregation under the
assumption of strong homophily. However, in GAD, anomalies typically exhibit
high local heterophily, while normal nodes retain strong homophily, resulting
in a complex homophily-heterophily mixture. To understand the impact of this
mixed pattern on graph pre-training, we analyze it through the lens of spectral
filtering and reveal that relying solely on a global low-pass filter is
insufficient for GAD. We further provide a theoretical justification for the
necessity of selectively applying appropriate filters to individual nodes.
Building upon this insight, we propose PAF, a Pre-Training and Adaptive
Fine-tuning framework specifically designed for GAD. In particular, we
introduce joint training with low- and high-pass filters in the pre-training
phase to capture the full spectrum of frequency information in node features.
During fine-tuning, we devise a gated fusion network that adaptively combines
node representations generated by both filters. Extensive experiments across
ten benchmark datasets consistently demonstrate the effectiveness of PAF.

</details>

### [73] [Generative emulation of chaotic dynamics with coherent prior](https://arxiv.org/abs/2504.14264)
*Juan Nathaniel,Pierre Gentine*

Main category: cs.LG

TLDR: 本文提出了一种名为Cohesion的高效生成框架，用于非线性动力学模拟，通过结合湍流原理和扩散模型，解决了长期预测中物理不现实的问题。


<details>
  <summary>Details</summary>
Motivation: 非线性动力学的数据驱动模拟常因长期技能衰减导致物理不现实的输出，现有生成模型的质量依赖于条件先验的选择。

Method: Cohesion框架通过估计动力学的大尺度相干结构作为去噪过程的指导，并利用降阶模型（如深度Koopman算子）快速生成长序列先验。

Result: 在复杂混沌系统（如Kolmogorov流、浅水方程和次季节到季节气候动力学）中，Cohesion表现出优越的长期预测能力。

Conclusion: Cohesion能够高效生成物理一致的模拟，即使在部分观测指导的情况下，为长期预测提供了一种新方法。

Abstract: Data-driven emulation of nonlinear dynamics is challenging due to long-range
skill decay that often produces physically unrealistic outputs. Recent advances
in generative modeling aim to address these issues by providing uncertainty
quantification and correction. However, the quality of generated simulation
remains heavily dependent on the choice of conditioning priors. In this work,
we present an efficient generative framework for dynamics emulation, unifying
principles of turbulence with diffusion-based modeling: Cohesion. Specifically,
our method estimates large-scale coherent structure of the underlying dynamics
as guidance during the denoising process, where small-scale fluctuation in the
flow is then resolved. These coherent priors are efficiently approximated using
reduced-order models, such as deep Koopman operators, that allow for rapid
generation of long prior sequences while maintaining stability over extended
forecasting horizon. With this gain, we can reframe forecasting as trajectory
planning, a common task in reinforcement learning, where conditional denoising
is performed once over entire sequences, minimizing the computational cost of
autoregressive-based generative methods. Empirical evaluations on chaotic
systems of increasing complexity, including Kolmogorov flow, shallow water
equations, and subseasonal-to-seasonal climate dynamics, demonstrate Cohesion
superior long-range forecasting skill that can efficiently generate
physically-consistent simulations, even in the presence of partially-observed
guidance.

</details>

### [74] [What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale](https://arxiv.org/abs/2504.14815)
*Xiaoyong Yuan,Xiaolong Ma,Linke Guo,Lan Zhang*

Main category: cs.LG

TLDR: 本文提出了一种名为PAIA的新框架，用于高效审计微调扩散模型是否学习特定概念，解决了现有方法的局限性，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型的普及和参数高效微调技术的发展，模型可能生成敏感或未经授权的内容，但目前缺乏有效的审计工具。

Method: 提出Prompt-Agnostic Image-Free Auditing (PAIA)，通过直接分析模型内部行为，避免依赖提示或生成图像。

Result: 在320个控制模型和690个真实社区模型上，PAIA检测准确率超过90%，审计时间减少18-40倍。

Conclusion: PAIA是首个可扩展且实用的扩散模型预部署审计方案，为模型共享提供了更安全透明的基础。

Abstract: Diffusion models (DMs) have revolutionized text-to-image generation, enabling
the creation of highly realistic and customized images from text prompts. With
the rise of parameter-efficient fine-tuning (PEFT) techniques like LoRA, users
can now customize powerful pre-trained models using minimal computational
resources. However, the widespread sharing of fine-tuned DMs on open platforms
raises growing ethical and legal concerns, as these models may inadvertently or
deliberately generate sensitive or unauthorized content, such as copyrighted
material, private individuals, or harmful content. Despite the increasing
regulatory attention on generative AI, there are currently no practical tools
for systematically auditing these models before deployment. In this paper, we
address the problem of concept auditing: determining whether a fine-tuned DM
has learned to generate a specific target concept. Existing approaches
typically rely on prompt-based input crafting and output-based image
classification but suffer from critical limitations, including prompt
uncertainty, concept drift, and poor scalability. To overcome these challenges,
we introduce Prompt-Agnostic Image-Free Auditing (PAIA), a novel, model-centric
concept auditing framework. By treating the DM as the object of inspection,
PAIA enables direct analysis of internal model behavior, bypassing the need for
optimized prompts or generated images. We evaluate PAIA on 320 controlled model
and 690 real-world community models sourced from a public DM sharing platform.
PAIA achieves over 90% detection accuracy while reducing auditing time by
18-40x compared to existing baselines. To our knowledge, PAIA is the first
scalable and practical solution for pre-deployment concept auditing of
diffusion models, providing a practical foundation for safer and more
transparent diffusion model sharing.

</details>

### [75] [Mixed-Precision Conjugate Gradient Solvers with RL-Driven Precision Tuning](https://arxiv.org/abs/2504.14268)
*Xinye Chen*

Main category: cs.LG

TLDR: 提出了一种基于强化学习的框架，用于动态优化预条件共轭梯度法中的数值精度，通过Q学习自适应分配精度，平衡计算效率与数值精度。


<details>
  <summary>Details</summary>
Motivation: 解决预条件共轭梯度法中数值精度选择的问题，提升计算效率同时确保数值稳定性。

Method: 将精度选择建模为马尔可夫决策过程，使用Q学习自适应分配精度，并通过双精度标量计算和残差计算确保稳定性。

Result: 算法在训练数据上表现良好，并能无缝适应新数据集，显著提升求解器性能。

Conclusion: 该方法是强化学习在混合精度数值方法中的首次应用，展示了其实际优势、鲁棒性和可扩展性，为科学计算中的AI驱动进步铺平了道路。

Abstract: This paper presents a novel reinforcement learning (RL) framework for
dynamically optimizing numerical precision in the preconditioned conjugate
gradient (CG) method. By modeling precision selection as a Markov Decision
Process (MDP), we employ Q-learning to adaptively assign precision levels to
key operations, striking an optimal balance between computational efficiency
and numerical accuracy, while ensuring stability through double-precision
scalar computations and residual computing. In practice, the algorithm is
trained on a set of data and subsequently performs inference for precision
selection on out-of-sample data, without requiring re-analysis or retraining
for new datasets. This enables the method to adapt seamlessly to new problem
instances without the computational overhead of recalibration. Our results
demonstrate the effectiveness of RL in enhancing solver's performance, marking
the first application of RL to mixed-precision numerical methods. The findings
highlight the approach's practical advantages, robustness, and scalability,
providing valuable insights into its integration with iterative solvers and
paving the way for AI-driven advancements in scientific computing.

</details>

### [76] [SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM](https://arxiv.org/abs/2504.14286)
*Xiaojiang Zhang,Jinghui Wang,Zifei Cheng,Wenhao Zhuang,Zheng Lin,Minglei Zhang,Shaojie Wang,Yinghan Cui,Chao Wang,Junyi Peng,Shimiao Jiang,Shiqi Kuang,Shouyu Yin,Chaohang Wen,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.LG

TLDR: 提出了两阶段历史重采样策略优化（SRPO），通过强化学习提升大语言模型的推理能力，无需监督微调，在多个基准测试中超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在提升语言模型推理能力方面潜力巨大，但跨领域复现这些进展因方法透明度不足而困难。

Method: 基于GRPO，引入两阶段跨领域训练范式和历史重采样技术，平衡数学推理与编码能力，解决无效样本问题。

Result: SRPO在AIME24和LiveCodeBench基准测试中表现优于DeepSeek-R1-Zero-32B。

Conclusion: SRPO为跨任务扩展语言模型推理能力提供了有效方法，并提供了有价值的见解。

Abstract: Recent advances of reasoning models, exemplified by OpenAI's o1 and
DeepSeek's R1, highlight the significant potential of Reinforcement Learning
(RL) to enhance the reasoning capabilities of Large Language Models (LLMs).
However, replicating these advancements across diverse domains remains
challenging due to limited methodological transparency. In this work, we
present two-Staged history-Resampling Policy Optimization (SRPO), which
successfully surpasses the performance of DeepSeek-R1-Zero-32B on the AIME24
and LiveCodeBench benchmarks. SRPO achieves this using the same base model as
DeepSeek (i.e. Qwen2.5-32B) and relies solely on RL, without prior Supervised
Fine-Tuning (SFT). Building upon Group Relative Policy Optimization (GRPO), we
introduce two key methodological innovations: (1) a two-stage cross-domain
training paradigm designed to balance the development of mathematical reasoning
and coding proficiency, and (2) History Resampling (HR), a technique to address
ineffective samples. Our comprehensive experiments validate the effectiveness
of our approach, dedicating to offer valuable insights into scaling LLM
reasoning capabilities across diverse tasks.

</details>

### [77] [Learning and Generating Diverse Residential Load Patterns Using GAN with Weakly-Supervised Training and Weight Selection](https://arxiv.org/abs/2504.14300)
*Xinyu Liang,Hao Wang*

Main category: cs.LG

TLDR: 论文提出了一种基于生成对抗网络（RLP-GAN）的住宅负荷模式生成模型，解决了现有方法在可扩展性、多样性和相似性方面的不足，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 高质量住宅负荷数据的稀缺性阻碍了住宅领域脱碳和电网规划与运营的有效性，现有合成数据生成方法存在局限性。

Method: 提出了一种弱监督的GAN框架（RLP-GAN），利用过完备自编码器捕捉复杂负荷模式的依赖关系，并引入模型权重选择方法解决模式崩溃问题。

Result: RLP-GAN在417户真实数据上验证，表现优于现有模型，能更好地捕捉时间依赖性和生成高相似性的负荷模式。

Conclusion: RLP-GAN生成的合成数据集（100万条负荷模式）已公开，为相关研究提供了高质量数据支持。

Abstract: The scarcity of high-quality residential load data can pose obstacles for
decarbonizing the residential sector as well as effective grid planning and
operation. The above challenges have motivated research into generating
synthetic load data, but existing methods faced limitations in terms of
scalability, diversity, and similarity. This paper proposes a Generative
Adversarial Network-based Synthetic Residential Load Pattern (RLP-GAN)
generation model, a novel weakly-supervised GAN framework, leveraging an
over-complete autoencoder to capture dependencies within complex and diverse
load patterns and learn household-level data distribution at scale. We
incorporate a model weight selection method to address the mode collapse
problem and generate load patterns with high diversity. We develop a holistic
evaluation method to validate the effectiveness of RLP-GAN using real-world
data of 417 households. The results demonstrate that RLP-GAN outperforms
state-of-the-art models in capturing temporal dependencies and generating load
patterns with higher similarity to real data. Furthermore, we have publicly
released the RLP-GAN generated synthetic dataset, which comprises one million
synthetic residential load pattern profiles.

</details>

### [78] [Learning to Score](https://arxiv.org/abs/2504.14302)
*Yogev Kriger,Shai Fine*

Main category: cs.LG

TLDR: 本文研究了一种目标标签不可用但存在相关辅助信息（Side Information）的场景，提出了一种结合表示学习、辅助信息和度量学习的评分模型，适用于多种用例（如医疗领域的疾病严重程度评分）。


<details>
  <summary>Details</summary>
Motivation: 研究在目标标签不可用但存在相关辅助信息的情况下，如何利用这些信息进行学习。

Method: 将问题建模为表示学习、辅助信息和度量学习的组合，提出了一种评分模型。

Result: 在基准数据集和生物医学患者记录上验证了该评分系统的实用性。

Conclusion: 提出的评分模型在辅助信息丰富的场景中具有广泛的应用潜力。

Abstract: Common machine learning settings range from supervised tasks, where
accurately labeled data is accessible, through semi-supervised and
weakly-supervised tasks, where target labels are scant or noisy, to
unsupervised tasks where labels are unobtainable. In this paper we study a
scenario where the target labels are not available but additional related
information is at hand. This information, referred to as Side Information, is
either correlated with the unknown labels or imposes constraints on the feature
space. We formulate the problem as an ensemble of three semantic components:
representation learning, side information and metric learning. The proposed
scoring model is advantageous for multiple use-cases. For example, in the
healthcare domain it can be used to create a severity score for diseases where
the symptoms are known but the criteria for the disease progression are not
well defined. We demonstrate the utility of the suggested scoring system on
well-known benchmark data-sets and bio-medical patient records.

</details>

### [79] [Learning from Stochastic Teacher Representations Using Student-Guided Knowledge Distillation](https://arxiv.org/abs/2504.14307)
*Muhammad Haseeb Aslam,Clara Martinez,Marco Pedersoli,Alessandro Koerich,Ali Etemad,Eric Granger*

Main category: cs.LG

TLDR: 论文提出了一种新的随机自蒸馏（SSD）训练策略，通过蒸馏时dropout生成多样化的教师表示，并结合学生引导的知识蒸馏（SGKD）过滤和加权任务相关表示，从而在资源受限的应用中提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统自蒸馏和集成学习方法在资源受限或延迟敏感的应用中不实用，因为需要训练多个模型或存储大量参数。论文旨在通过仅训练一个模型并利用蒸馏时dropout生成多样化表示来解决这一问题。

Method: 提出SSD策略，通过蒸馏时dropout生成多样化的教师表示，并使用SGKD方法过滤和加权任务相关表示。学生表示在每一步蒸馏中作为权威指导蒸馏过程。

Result: 在情感计算、可穿戴设备/生物信号数据集（UCR Archive、HAR）和图像分类数据集上的实验表明，SSD方法在不增加模型大小的情况下优于现有方法，且计算复杂度极低。

Conclusion: SSD方法通过单模型生成多样化表示并优化蒸馏过程，显著提升了性能，适用于资源受限的应用场景。

Abstract: Advances in self-distillation have shown that when knowledge is distilled
from a teacher to a student using the same deep learning (DL) architecture, the
student performance can surpass the teacher particularly when the network is
overparameterized and the teacher is trained with early stopping.
Alternatively, ensemble learning also improves performance, although training,
storing, and deploying multiple models becomes impractical as the number of
models grows. Even distilling an ensemble to a single student model or weight
averaging methods first requires training of multiple teacher models and does
not fully leverage the inherent stochasticity for generating and distilling
diversity in DL models. These constraints are particularly prohibitive in
resource-constrained or latency-sensitive applications such as wearable
devices. This paper proposes to train only one model and generate multiple
diverse teacher representations using distillation-time dropout. However,
generating these representations stochastically leads to noisy representations
that are misaligned with the learned task. To overcome this problem, a novel
stochastic self-distillation (SSD) training strategy is introduced for
filtering and weighting teacher representation to distill from task-relevant
representations only, using student-guided knowledge distillation (SGKD). The
student representation at each distillation step is used as authority to guide
the distillation process. Experimental results on real-world affective
computing, wearable/biosignal datasets from the UCR Archive, the HAR dataset,
and image classification datasets show that the proposed SSD method can
outperform state-of-the-art methods without increasing the model size at both
training and testing time, and incurs negligible computational complexity
compared to state-of-the-art ensemble learning and weight averaging methods.

</details>

### [80] [Local distribution-based adaptive oversampling for imbalanced regression](https://arxiv.org/abs/2504.14316)
*Shayan Alahyari,Mike Domaratzki*

Main category: cs.LG

TLDR: LDAO是一种新的数据级方法，通过分解数据集为局部分布并独立采样，解决了不平衡回归问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 不平衡回归问题在连续目标变量分布不均时难以预测，现有方法依赖阈值分类且效果有限。

Method: LDAO通过分解数据集为局部分布，独立建模和采样，合并为平衡训练集。

Result: 在45个不平衡数据集上，LDAO在频繁和稀有目标值上均优于现有方法。

Conclusion: LDAO有效解决了不平衡回归问题，保留了局部统计特性。

Abstract: Imbalanced regression occurs when continuous target variables have skewed
distributions, creating sparse regions that are difficult for machine learning
models to predict accurately. This issue particularly affects neural networks,
which often struggle with imbalanced data. While class imbalance in
classification has been extensively studied, imbalanced regression remains
relatively unexplored, with few effective solutions. Existing approaches often
rely on arbitrary thresholds to categorize samples as rare or frequent,
ignoring the continuous nature of target distributions. These methods can
produce synthetic samples that fail to improve model performance and may
discard valuable information through undersampling. To address these
limitations, we propose LDAO (Local Distribution-based Adaptive Oversampling),
a novel data-level approach that avoids categorizing individual samples as rare
or frequent. Instead, LDAO learns the global distribution structure by
decomposing the dataset into a mixture of local distributions, each preserving
its statistical characteristics. LDAO then models and samples from each local
distribution independently before merging them into a balanced training set.
LDAO achieves a balanced representation across the entire target range while
preserving the inherent statistical structure within each local distribution.
In extensive evaluations on 45 imbalanced datasets, LDAO outperforms
state-of-the-art oversampling methods on both frequent and rare target values,
demonstrating its effectiveness for addressing the challenge of imbalanced
regression.

</details>

### [81] [Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction](https://arxiv.org/abs/2504.14361)
*Till Rossner,Ziteng Li,Jonas Balke,Nikoo Salehfard,Tom Seifert,Ming Tang*

Main category: cs.LG

TLDR: 提出了一种结合scGPT和DeepCDR的创新方法，用于预测癌症药物反应（CDR），实验表明该方法优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 提高癌症药物反应预测的准确性，探索scGPT在基因表达数据中的应用潜力。

Method: 利用scGPT生成基因表达数据的嵌入表示，作为DeepCDR的输入数据。

Result: 该方法在实验中表现优于原始DeepCDR和scFoundation模型。

Conclusion: scGPT嵌入能显著提升CDR预测准确性，为现有方法提供了有前景的替代方案。

Abstract: In this study, we propose an innovative methodology for predicting Cancer
Drug Response (CDR) through the integration of the scGPT foundation model
within the DeepCDR model. Our approach utilizes scGPT to generate embeddings
from gene expression data, which are then used as gene expression input data
for DeepCDR. The experimental findings demonstrate the efficacy of this
scGPT-based method in outperforming previous related works, including the
original DeepCDR model and the scFoundation-based model. This study highlights
the potential of scGPT embeddings to enhance the accuracy of CDR predictions
and offers a promising alternative to existing approaches.

</details>

### [82] [Improving RL Exploration for LLM Reasoning through Retrospective Replay](https://arxiv.org/abs/2504.14363)
*Shihan Dou,Muling Wu,Jingwen Xu,Rui Zheng,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TLDR: 论文提出了一种名为RRL的新算法，通过动态重放机制解决强化学习在LLM后训练中的探索问题，显著提升了复杂任务的性能。


<details>
  <summary>Details</summary>
Motivation: 在LLM的强化学习后训练中，早期探索阶段模型能发现潜在解决方案但无法实现，而后期能力提升后却难以重新探索这些方案，导致复杂任务表现不佳。

Method: 提出Retrospective Replay-based Reinforcement Learning (RRL)算法，通过动态重放机制让模型在训练过程中重新访问早期发现的潜在状态。

Result: 实验表明，RRL在复杂推理任务（如数学推理和代码生成）及对话任务中均显著提升了探索效率和模型性能，同时优化了RLHF的效果。

Conclusion: RRL通过动态重放机制有效解决了探索问题，提升了LLM在复杂任务中的表现，并增强了模型的安全性和实用性。

Abstract: Reinforcement learning (RL) has increasingly become a pivotal technique in
the post-training of large language models (LLMs). The effective exploration of
the output space is essential for the success of RL. We observe that for
complex problems, during the early stages of training, the model exhibits
strong exploratory capabilities and can identify promising solution ideas.
However, its limited capability at this stage prevents it from successfully
solving these problems. The early suppression of these potentially valuable
solution ideas by the policy gradient hinders the model's ability to revisit
and re-explore these ideas later. Consequently, although the LLM's capabilities
improve in the later stages of training, it still struggles to effectively
address these complex problems. To address this exploration issue, we propose a
novel algorithm named Retrospective Replay-based Reinforcement Learning (RRL),
which introduces a dynamic replay mechanism throughout the training process.
RRL enables the model to revisit promising states identified in the early
stages, thereby improving its efficiency and effectiveness in exploration. To
evaluate the effectiveness of RRL, we conduct extensive experiments on complex
reasoning tasks, including mathematical reasoning and code generation, and
general dialogue tasks. The results indicate that RRL maintains high
exploration efficiency throughout the training period, significantly enhancing
the effectiveness of RL in optimizing LLMs for complicated reasoning tasks.
Moreover, it also improves the performance of RLHF, making the model both safer
and more helpful.

</details>

### [83] [Accelerating LLM Inference with Flexible N:M Sparsity via A Fully Digital Compute-in-Memory Accelerator](https://arxiv.org/abs/2504.14365)
*Akshat Ramachandran,Souvik Kundu,Arnab Raha,Shamik Kundu,Deepak K. Mathaikutty,Tushar Krishna*

Main category: cs.LG

TLDR: 论文提出了一种灵活的层间异常密度感知N:M稀疏选择方法（FLOW）和低开销的数字内存计算架构（FlexCiM），以解决LLM修剪中的性能限制和硬件开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有的固定N:M结构化稀疏方法限制了稀疏模型的表达能力，而支持多种N:M模式又带来高昂的硬件开销。

Method: FLOW通过同时考虑异常值的分布，动态选择最优的层间N和M值；FlexCiM通过分区和自适应聚合机制支持多样化的稀疏模式。

Result: 实验表明，FLOW在精度上优于现有方法，最高提升36%；FlexCiM在推理延迟和能耗上分别降低1.75倍和1.5倍。

Conclusion: FLOW和FlexCiM的组合为LLM稀疏化提供了高效且灵活的解决方案。

Abstract: Large language model (LLM) pruning with fixed N:M structured sparsity
significantly limits the expressivity of the sparse model, yielding sub-optimal
performance. In contrast, supporting multiple N:M patterns to provide sparse
representational freedom introduces costly overhead in hardware. To address
these challenges for LLMs, we first present a flexible layer-wise
outlier-density-aware N:M sparsity (FLOW) selection method. FLOW enables the
identification of optimal layer-wise N and M values (from a given range) by
simultaneously accounting for the presence and distribution of outliers,
allowing a higher degree of representational freedom. To deploy sparse models
with such N:M flexibility, we then introduce a flexible, low-overhead digital
compute-in-memory architecture (FlexCiM). FlexCiM supports diverse sparsity
patterns by partitioning a digital CiM (DCiM) macro into smaller sub-macros,
which are adaptively aggregated and disaggregated through distribution and
merging mechanisms for different N and M values. Extensive experiments on both
transformer-based and recurrence-based state space foundation models (SSMs)
demonstrate that FLOW outperforms existing alternatives with an accuracy
improvement of up to 36%, while FlexCiM achieves up to 1.75x lower inference
latency and 1.5x lower energy consumption compared to existing sparse
accelerators. Code is available at: https://github.com/FLOW-open-project/FLOW

</details>

### [84] [Learning Enhanced Structural Representations with Block-Based Uncertainties for Ocean Floor Mapping](https://arxiv.org/abs/2504.14372)
*Jose Marie Antonio Minoza*

Main category: cs.LG

TLDR: 提出了一种基于空间块和VQ-VAE架构的不确定性感知机制，用于高分辨率海底地形重建，显著提升了重建质量和不确定性估计的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前全球海底地形数据分辨率不足，难以支持精确的数值模拟，而现有深度学习方法在保持物理结构一致性和量化不确定性方面存在挑战。

Method: 采用基于块的一致性预测方法，结合VQ-VAE架构，通过离散潜在表示保留地形特征，并生成空间自适应的不确定性估计。

Result: 实验结果表明，该方法在多个海域显著提高了重建质量和不确定性估计的可靠性，尤其是在复杂海底结构区域。

Conclusion: 该框架通过保留结构完整性和提供空间自适应不确定性估计，为气候建模和海岸灾害评估提供了更可靠的基础。

Abstract: Accurate ocean modeling and coastal hazard prediction depend on
high-resolution bathymetric data; yet, current worldwide datasets are too
coarse for exact numerical simulations. While recent deep learning advances
have improved earth observation data resolution, existing methods struggle with
the unique challenges of producing detailed ocean floor maps, especially in
maintaining physical structure consistency and quantifying uncertainties. This
work presents a novel uncertainty-aware mechanism using spatial blocks to
efficiently capture local bathymetric complexity based on block-based conformal
prediction. Using the Vector Quantized Variational Autoencoder (VQ-VAE)
architecture, the integration of this uncertainty quantification framework
yields spatially adaptive confidence estimates while preserving topographical
features via discrete latent representations. With smaller uncertainty widths
in well-characterized areas and appropriately larger bounds in areas of complex
seafloor structures, the block-based design adapts uncertainty estimates to
local bathymetric complexity. Compared to conventional techniques, experimental
results over several ocean regions show notable increases in both
reconstruction quality and uncertainty estimation reliability. This framework
increases the reliability of bathymetric reconstructions by preserving
structural integrity while offering spatially adaptive uncertainty estimates,
so opening the path for more solid climate modeling and coastal hazard
assessment.

</details>

### [85] [Bottom-Up Synthesis of Knowledge-Grounded Task-Oriented Dialogues with Iteratively Self-Refined Prompts](https://arxiv.org/abs/2504.14375)
*Kun Qian,Maximillian Chen,Siyan Li,Arpit Sharma,Zhou Yu*

Main category: cs.LG

TLDR: 提出了一种自下而上的对话合成方法，首先生成问答对，再组合成连贯对话，相比传统自上而下方法更具控制性和精确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法生成多轮对话时缺乏细粒度控制且易产生幻觉，需要更高效的数据生成方法。

Method: 采用两步法：首先生成问答对，再组合成对话，允许分步优化和验证。

Result: 生成的数据质量更高，更接近真实对话，优于传统方法。

Conclusion: 自下而上的方法在对话合成中更有效，提升了数据质量和控制性。

Abstract: Training conversational question-answering (QA) systems requires a
substantial amount of in-domain data, which is often scarce in practice. A
common solution to this challenge is to generate synthetic data. Traditional
methods typically follow a top-down approach, where a large language model
(LLM) generates multi-turn dialogues from a broad prompt. Although this method
produces coherent conversations, it offers limited fine-grained control over
the content and is susceptible to hallucinations. We introduce a bottom-up
conversation synthesis approach, where QA pairs are generated first and then
combined into a coherent dialogue. This method offers greater control and
precision by dividing the process into two distinct steps, allowing refined
instructions and validations to be handled separately. Additionally, this
structure allows the use of non-local models in stages that do not involve
proprietary knowledge, enhancing the overall quality of the generated data.
Both human and automated evaluations demonstrate that our approach produces
more realistic and higher-quality dialogues compared to top-down methods.

</details>

### [86] [Balancing Fairness and Performance in Healthcare AI: A Gradient Reconciliation Approach](https://arxiv.org/abs/2504.14388)
*Xiaoyang Wang,Christopher C. Yang*

Main category: cs.LG

TLDR: FairGrad框架通过梯度调和平衡医疗AI模型的预测性能与多属性公平性，显著提升公平性指标同时保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 医疗AI系统若未考虑公平性可能加剧医疗资源分配和诊断的不平等，需解决这一挑战。

Method: 提出FairGrad框架，通过梯度向量正交投影调和冲突的优化目标，确保公平性。

Result: 在多种医疗数据集上显著提升多属性公平性指标（如均等机会），同时保持预测准确性。

Conclusion: FairGrad证明了在关键医疗AI应用中平衡公平性与实用性的可行性。

Abstract: The rapid growth of healthcare data and advances in computational power have
accelerated the adoption of artificial intelligence (AI) in medicine. However,
AI systems deployed without explicit fairness considerations risk exacerbating
existing healthcare disparities, potentially leading to inequitable resource
allocation and diagnostic disparities across demographic subgroups. To address
this challenge, we propose FairGrad, a novel gradient reconciliation framework
that automatically balances predictive performance and multi-attribute fairness
optimization in healthcare AI models. Our method resolves conflicting
optimization objectives by projecting each gradient vector onto the orthogonal
plane of the others, thereby regularizing the optimization trajectory to ensure
equitable consideration of all objectives. Evaluated on diverse real-world
healthcare datasets and predictive tasks - including Substance Use Disorder
(SUD) treatment and sepsis mortality - FairGrad achieved statistically
significant improvements in multi-attribute fairness metrics (e.g., equalized
odds) while maintaining competitive predictive accuracy. These results
demonstrate the viability of harmonizing fairness and utility in
mission-critical medical AI applications.

</details>

### [87] [Exploring Pseudo-Token Approaches in Transformer Neural Processes](https://arxiv.org/abs/2504.14416)
*Jose Lara-Rangel,Nanze Chen,Fengzhe Zhang*

Main category: cs.LG

TLDR: ISANPs改进TNPs的计算效率，通过诱导集注意力和查询优化，在性能与计算复杂度间取得平衡，适用于更大数据集。


<details>
  <summary>Details</summary>
Motivation: 传统NPs易欠拟合，TNPs虽性能优越但计算复杂度高，限制了实际应用。

Method: 提出ISANPs，采用诱导集注意力和创新查询阶段，优化计算效率。

Result: ISANPs在多项任务中表现优异，性能与TNPs相当或更优，且计算复杂度可调。

Conclusion: ISANPs在性能与计算效率间取得平衡，适用于大规模数据集。

Abstract: Neural Processes (NPs) have gained attention in meta-learning for their
ability to quantify uncertainty, together with their rapid prediction and
adaptability. However, traditional NPs are prone to underfitting. Transformer
Neural Processes (TNPs) significantly outperform existing NPs, yet their
applicability in real-world scenarios is hindered by their quadratic
computational complexity relative to both context and target data points. To
address this, pseudo-token-based TNPs (PT-TNPs) have emerged as a novel NPs
subset that condense context data into latent vectors or pseudo-tokens,
reducing computational demands. We introduce the Induced Set Attentive Neural
Processes (ISANPs), employing Induced Set Attention and an innovative query
phase to improve querying efficiency. Our evaluations show that ISANPs perform
competitively with TNPs and often surpass state-of-the-art models in 1D
regression, image completion, contextual bandits, and Bayesian optimization.
Crucially, ISANPs offer a tunable balance between performance and computational
complexity, which scale well to larger datasets where TNPs face limitations.

</details>

### [88] [LoRe: Personalizing LLMs via Low-Rank Reward Modeling](https://arxiv.org/abs/2504.14439)
*Avinandan Bose,Zhihan Xiong,Yuejie Chi,Simon Shaolei Du,Lin Xiao,Maryam Fazel*

Main category: cs.LG

TLDR: 提出了一种基于低秩偏好建模的新框架，用于个性化大语言模型（LLMs），以提升用户对齐和满意度。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习从人类反馈（RLHF）方法依赖单一价值表示，难以适应个体偏好。

Method: 通过低维子空间表示奖励函数，将个体偏好建模为共享基函数的加权组合。

Result: 在多个偏好数据集上验证，展示了更好的未见用户泛化能力和偏好预测准确性。

Conclusion: 该方法避免了僵化的用户分类，同时实现了可扩展性和少样本适应。

Abstract: Personalizing large language models (LLMs) to accommodate diverse user
preferences is essential for enhancing alignment and user satisfaction.
Traditional reinforcement learning from human feedback (RLHF) approaches often
rely on monolithic value representations, limiting their ability to adapt to
individual preferences. We introduce a novel framework that leverages low-rank
preference modeling to efficiently learn and generalize user-specific reward
functions. By representing reward functions in a low-dimensional subspace and
modeling individual preferences as weighted combinations of shared basis
functions, our approach avoids rigid user categorization while enabling
scalability and few-shot adaptation. We validate our method on multiple
preference datasets, demonstrating superior generalization to unseen users and
improved accuracy in preference prediction tasks.

</details>

### [89] [A computational framework for longitudinal medication adherence prediction in breast cancer survivors: A social cognitive theory based approach](https://arxiv.org/abs/2504.14469)
*Navreet Kaur,Manuel Gonzales IV,Cristian Garcia Alcaraz,Jiaqi Gong,Kristen J. Wells,Laura E. Barnes*

Main category: cs.LG

TLDR: 论文提出了一种基于社会认知理论的多尺度模型，用于预测乳腺癌幸存者的药物依从性，动态和静态因素结合显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 慢性病患者药物依从性低导致严重后果，乳腺癌幸存者的内分泌治疗依从性与生存率密切相关，需多尺度建模理解影响因素。

Method: 采用计算框架结合动态和静态因素，分日和周尺度建模，预测药物依从性。

Result: 模型在日和周任务中表现优于传统方法，日模型准确率87.25%，周模型76.04%。动态因素对日预测最重要，动静态结合对周预测关键。

Conclusion: 多尺度模型能有效预测药物依从性，动态和静态因素的结合在不同时间尺度上均有重要意义。

Abstract: Non-adherence to medications is a critical concern since nearly half of
patients with chronic illnesses do not follow their prescribed medication
regimens, leading to increased mortality, costs, and preventable human
distress. Amongst stage 0-3 breast cancer survivors, adherence to long-term
adjuvant endocrine therapy (i.e., Tamoxifen and aromatase inhibitors) is
associated with a significant increase in recurrence-free survival. This work
aims to develop multi-scale models of medication adherence to understand the
significance of different factors influencing adherence across varying time
frames. We introduce a computational framework guided by Social Cognitive
Theory for multi-scale (daily and weekly) modeling of longitudinal medication
adherence. Our models employ both dynamic medication-taking patterns in the
recent past (dynamic factors) as well as less frequently changing factors
(static factors) for adherence prediction. Additionally, we assess the
significance of various factors in influencing adherence behavior across
different time scales. Our models outperform traditional machine learning
counterparts in both daily and weekly tasks in terms of both accuracy and
specificity. Daily models achieved an accuracy of 87.25%, and weekly models, an
accuracy of 76.04%. Notably, dynamic past medication-taking patterns prove most
valuable for predicting daily adherence, while a combination of dynamic and
static factors is significant for macro-level weekly adherence patterns.

</details>

### [90] [Less is More: Adaptive Coverage for Synthetic Training Data](https://arxiv.org/abs/2504.14508)
*Sasan Tavakkol,Max Springer,Mohammadhossein Bateni,Neslihan Bulut,Vincent Cohen-Addad,MohammadTaghi Hajiaghayi*

Main category: cs.LG

TLDR: 利用大语言模型生成合成训练数据，并通过最大覆盖问题算法选择代表性子集，训练分类器性能优于全数据集。


<details>
  <summary>Details</summary>
Motivation: 解决快速部署模型时获取大规模标注数据的挑战，特别是在新兴社交媒体趋势分类或在线滥用内容识别等场景。

Method: 提出基于最大覆盖问题的新采样算法，从合成数据中选择代表性子集用于训练分类器。

Result: 在代表性子集上训练的分类器性能优于全数据集，同时减少数据需求。

Conclusion: “少即是多”方法提升分类器性能并提高数据效率，适用于快速模型微调。

Abstract: Synthetic training data generation with Large Language Models (LLMs) like
Google's Gemma and OpenAI's GPT offer a promising solution to the challenge of
obtaining large, labeled datasets for training classifiers. When rapid model
deployment is critical, such as in classifying emerging social media trends or
combating new forms of online abuse tied to current events, the ability to
generate training data is invaluable. While prior research has examined the
comparability of synthetic data to human-labeled data, this study introduces a
novel sampling algorithm, based on the maximum coverage problem, to select a
representative subset from a synthetically generated dataset. Our results
demonstrate that training a classifier on this contextually sampled subset
achieves superior performance compared to training on the entire dataset. This
"less is more" approach not only improves accuracy but also reduces the volume
of data required, leading to potentially more efficient model fine-tuning.

</details>

### [91] [On Dimension-Free Transformer: An Application of STP to AI](https://arxiv.org/abs/2504.14514)
*Daizhan Cheng*

Main category: cs.LG

TLDR: 论文提出了一种基于半张量积的投影超向量线性变换方法，构建了维度无关的Transformer框架（DFT）。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在处理信号时维度受限，需要一种更高效且维度灵活的方法。

Method: 使用半张量积重新定义超向量，并通过投影构建线性变换，验证并替换Transformer中的线性变换为PBTH。

Result: 提出的DFT框架允许输入输出维度任意，且在处理信号时更高效。

Conclusion: DFT通过投影超向量变换实现了维度灵活性，提升了信号处理效率。

Abstract: The matrix expressions for every parts of a transformer are firstly
described. Based on semi-tensor product (STP) of matrices the hypervectors are
reconsidered and the linear transformation over hypervectors is constructed by
using projection. Its properties and calculating formulas are obtained. Using
projection-based transformation of hypervector (PBTH), the framework of
dimension-free transformer (DFT) is proposed by verifying each linear
transformation in a transformer and replacing it by a proper PBTH, which allows
the inputs and outputs being of arbitrary dimensions. Using balanced
information about all entries, DFT must be more efficient in dealing with
signals.

</details>

### [92] [SlimPipe: Memory-Thrifty and Efficient Pipeline Parallelism for Long-Context LLM Training](https://arxiv.org/abs/2504.14519)
*Zhouyang Li,Yuliang Liu,Wei Zhang,Tailing Yuan,Bin Chen,Chengru Song,Di Zhang*

Main category: cs.LG

TLDR: SlimPipe是一种新型细粒度流水线并行方法，通过均匀序列切片和1F1B调度，显著减少激活内存消耗和流水线气泡，提升大型语言模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行方法在长上下文场景中无法有效解决激活内存压力和流水线气泡问题，限制了训练效率。

Method: 采用均匀序列切片和1F1B调度，结合负载均衡技术，减少激活内存消耗并优化计算分配。

Result: 在Llama 70B模型上，SlimPipe将模型FLOPs利用率提升至1.57倍（512K上下文），并在2048K上下文中保持45%以上的GPU利用率。

Conclusion: SlimPipe有效解决了长上下文训练中的内存和效率问题，显著优于现有方法。

Abstract: Pipeline Parallelism (PP) serves as a crucial technique for training Large
Language Models (LLMs), owing to its capability to alleviate memory pressure
from model states with relatively low communication overhead. However, in
long-context scenarios, existing pipeline parallelism methods fail to address
the substantial activation memory pressure, primarily due to the peak memory
consumption resulting from the accumulation of activations across multiple
microbatches. Moreover, these approaches inevitably introduce considerable
pipeline bubbles, further hindering efficiency.
  To tackle these challenges, we propose SlimPipe, a novel approach to
fine-grained pipeline parallelism that employs uniform sequence slicing coupled
with one-forward-one-backward (1F1B) schedule. It reduces the accumulated
activations from several microbatches to just one, which is split into several
slices. Although the slices are evenly partitioned, the computation cost is not
equal across slices due to causal attention. We develop a sophisticated
workload redistribution technique to address this load imbalance. SlimPipe
achieves (1) near-zero memory overhead and (2) minimal pipeline bubbles
simultaneously. The effectiveness of SlimPipe has been proven by thorough
testing with diverse model architectures, context window sizes, and
SlimPipe-specific configurations. For example, on the Llama 70B model, compared
to state-of-the-art methods, SlimPipe significantly boosts the Model FLOPs
Utilization (MFU) to up to $1.57\times$ for a context length of 512K. More
notably, for a context length of 2048K, it maintains over 45% utilization on
256 NVIDIA Hopper 80GB GPUs, while other approaches either suffer significant
performance drops or fail entirely due to memory constraints.

</details>

### [93] [TrustLoRA: Low-Rank Adaptation for Failure Detection under Out-of-distribution Data](https://arxiv.org/abs/2504.14545)
*Fei Zhu,Zhaoxiang Zhang*

Main category: cs.LG

TLDR: 提出了一种简单的失败检测框架，用于统一和优化在协变量和语义偏移下的分类与拒绝任务。


<details>
  <summary>Details</summary>
Motivation: 在开放环境中，深度神经网络需要可靠预测，同时拒绝协变量和语义偏移的错误样本，但目前缺乏灵活可控的失败检测方法。

Method: 通过分离和整合失败特定的可靠性知识，利用低秩适配器增强失败检测能力。

Result: 实验证明该框架在失败检测方面具有优越性。

Conclusion: 该框架有效且灵活地提升了模型在开放环境中的可靠性。

Abstract: Reliable prediction is an essential requirement for deep neural models that
are deployed in open environments, where both covariate and semantic
out-of-distribution (OOD) data arise naturally. In practice, to make safe
decisions, a reliable model should accept correctly recognized inputs while
rejecting both those misclassified covariate-shifted and semantic-shifted
examples. Besides, considering the potential existing trade-off between
rejecting different failure cases, more convenient, controllable, and flexible
failure detection approaches are needed. To meet the above requirements, we
propose a simple failure detection framework to unify and facilitate
classification with rejection under both covariate and semantic shifts. Our key
insight is that by separating and consolidating failure-specific reliability
knowledge with low-rank adapters and then integrating them, we can enhance the
failure detection ability effectively and flexibly. Extensive experiments
demonstrate the superiority of our framework.

</details>

### [94] [NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models](https://arxiv.org/abs/2504.14569)
*Lawrence Liu,Inesh Chakrabarti,Yixiao Li,Mengdi Wang,Tuo Zhao,Lin F. Yang*

Main category: cs.LG

TLDR: NoWag是一个用于零样本形状保持压缩的统一框架，针对大语言模型（LLMs）的计算和内存需求问题，提出了向量量化（NoWag-VQ）和剪枝（NoWag-P）两种压缩方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在资源受限环境中的部署问题，降低其计算和内存需求。

Method: 提出NoWag框架，包含向量量化（NoWag-VQ）和剪枝（NoWag-P）两种压缩方法，应用于Llama-2和Llama-3模型。

Result: NoWag-VQ在零样本向量量化中表现优于现有方法，NoWag-P与现有剪枝方法竞争性相当。

Conclusion: NoWag框架展示了压缩范式的共性，为未来研究提供了启发。

Abstract: Large language models (LLMs) exhibit remarkable performance across various
natural language processing tasks but suffer from immense computational and
memory demands, limiting their deployment in resource-constrained environments.
To address this challenge, we propose NoWag: (Normalized Weight and Activation
Guided Compression), a unified framework for zero-shot shape preserving
compression algorithms. We compressed Llama-2 7B/13B/70B and Llama-3 8/70BB
models, using two popular forms of shape-preserving compression, vector
quantization NoWag-VQ (NoWag for Vector Quantization), and
unstructured/semi-structured pruning NoWag-P (NoWag for Pruning). We found that
NoWag-VQ significantly outperforms state-of-the-art zero shot VQ, and that
NoWag-P performs competitively against state-of-the-art methods. These results
suggest commonalities between these compression paradigms that could inspire
future work. Our code is available at https://github.com/LawrenceRLiu/NoWag

</details>

### [95] [Data Selection for ERMs](https://arxiv.org/abs/2504.14572)
*Steve Hanneke,Shay Moran,Alexander Shlimovich,Amir Yehudayoff*

Main category: cs.LG

TLDR: 论文从数据为中心的角度，研究如何通过优化训练数据选择，使学习规则在有限数据预算下达到与全量数据相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统学习理论以模型为中心，本文提出数据为中心的视角，探讨在固定学习规则下如何优化数据选择。

Method: 研究不同经验风险最小化方法，分析数据选择对性能的影响，包括均值估计、线性分类和回归。

Result: 提出了数据选择的最优边界，并建立了二元分类和随机凸优化的错误率分类体系。

Conclusion: 展示了数据选择的重要性，并提出了未来研究方向。

Abstract: Learning theory has traditionally followed a model-centric approach, focusing
on designing optimal algorithms for a fixed natural learning task (e.g., linear
classification or regression). In this paper, we adopt a complementary
data-centric perspective, whereby we fix a natural learning rule and focus on
optimizing the training data. Specifically, we study the following question:
given a learning rule $\mathcal{A}$ and a data selection budget $n$, how well
can $\mathcal{A}$ perform when trained on at most $n$ data points selected from
a population of $N$ points? We investigate when it is possible to select $n \ll
N$ points and achieve performance comparable to training on the entire
population.
  We address this question across a variety of empirical risk minimizers. Our
results include optimal data-selection bounds for mean estimation, linear
classification, and linear regression. Additionally, we establish two general
results: a taxonomy of error rates in binary classification and in stochastic
convex optimization. Finally, we propose several open questions and directions
for future research.

</details>

### [96] [Generative Auto-Bidding with Value-Guided Explorations](https://arxiv.org/abs/2504.14587)
*Jingtong Gao,Yewen Li,Shuai Mao,Peng Jiang,Nan Jiang,Yejing Wang,Qingpeng Cai,Fei Pan,Peng Jiang,Kun Gai,Bo An,Xiangyu Zhao*

Main category: cs.LG

TLDR: 本文提出了一种新型离线生成自动竞价框架GAVE，通过价值引导探索解决现有方法在动态市场适应性和历史依赖性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则或强化学习的自动竞价方法难以适应动态市场条件，且无法有效捕捉历史依赖关系，同时离线训练可能导致行为模式固定和崩溃。

Method: GAVE框架结合了基于分数的RTG模块、动作探索机制和可学习价值函数，以支持多样化广告目标并解决OOD问题。

Result: 在两个离线数据集和实际部署中，GAVE在离线和在线测试中均优于现有基线方法。

Conclusion: GAVE通过创新设计解决了自动竞价中的关键挑战，为未来研究提供了可复现的代码支持。

Abstract: Auto-bidding, with its strong capability to optimize bidding decisions within
dynamic and competitive online environments, has become a pivotal strategy for
advertising platforms. Existing approaches typically employ rule-based
strategies or Reinforcement Learning (RL) techniques. However, rule-based
strategies lack the flexibility to adapt to time-varying market conditions, and
RL-based methods struggle to capture essential historical dependencies and
observations within Markov Decision Process (MDP) frameworks. Furthermore,
these approaches often face challenges in ensuring strategy adaptability across
diverse advertising objectives. Additionally, as offline training methods are
increasingly adopted to facilitate the deployment and maintenance of stable
online strategies, the issues of documented behavioral patterns and behavioral
collapse resulting from training on fixed offline datasets become increasingly
significant. To address these limitations, this paper introduces a novel
offline Generative Auto-bidding framework with Value-Guided Explorations
(GAVE). GAVE accommodates various advertising objectives through a score-based
Return-To-Go (RTG) module. Moreover, GAVE integrates an action exploration
mechanism with an RTG-based evaluation method to explore novel actions while
ensuring stability-preserving updates. A learnable value function is also
designed to guide the direction of action exploration and mitigate
Out-of-Distribution (OOD) problems. Experimental results on two offline
datasets and real-world deployments demonstrate that GAVE outperforms
state-of-the-art baselines in both offline evaluations and online A/B tests.
The implementation code is publicly available to facilitate reproducibility and
further research.

</details>

### [97] [No Imputation of Missing Values In Tabular Data Classification Using Incremental Learning](https://arxiv.org/abs/2504.14610)
*Manar D. Samad,Kazi Fuad B. Akhter,Shourav B. Rabbani,Ibna Kowsar*

Main category: cs.LG

TLDR: 提出了一种无需插补的增量学习方法（NIIL），用于处理具有不同缺失率和类型的表格数据，通过注意力掩码排除缺失值，性能优于11种现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统插补方法在计算复杂度、数据质量和结果可靠性方面存在问题，NIIL旨在消除这些担忧。

Method: NIIL通过增量学习重叠特征集的分区，并使用注意力掩码排除缺失值。

Result: 在15个数据集上，NIIL的分类性能优于11种现有方法，且对缺失值类型和率具有鲁棒性。

Conclusion: NIIL是首个无需插补即可有效学习表格数据的深度学习方法，特征分区大小为原始特征空间一半时效果最佳。

Abstract: Tabular data sets with varying missing values are prepared for machine
learning using an arbitrary imputation strategy. Synthetic values generated by
imputation models often concern data stakeholders about computational
complexity, data quality, and data-driven outcomes. This paper eliminates these
concerns by proposing no imputation incremental learning (NIIL) of tabular data
with varying missing value rates and types. The proposed method incrementally
learns partitions of overlapping feature sets while using attention masks to
exclude missing values from attention scoring. The average classification
performance rank order across 15 diverse tabular data sets highlights the
superiority of NIIL over 11 state-of-the-art learning methods with or without
missing value imputations. Further experiments substantiate the robustness of
NIIL against varying missing value types and rates compared to methods that
involve the imputation of missing values. Our empirical analysis reveals that a
feature partition size of half of the original feature space is,
computation-wise and accuracy-wise, the best choice for the proposed
incremental learning. The proposed method is one of the first deep learning
solutions that can effectively learn tabular data without requiring the
imputation of missing values.

</details>

### [98] [AlphaZero-Edu: Making AlphaZero Accessible to Everyone](https://arxiv.org/abs/2504.14636)
*Binjie Guo,Hanyu Zheng,Guowei Su,Ru Zhang,Haohan Jiang,Xurong Lin,Hongyan Wei,Aisheng Mo,Jie Li,Zhiyuan Qian,Zhuhao Zhang,Xiaoyuan Cheng*

Main category: cs.LG

TLDR: AlphaZero-Edu是一个轻量级、教育导向的强化学习框架，基于AlphaZero数学框架，解决了现有框架的高复杂性和低可复现性问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习框架实现复杂且可复现性差，AlphaZero-Edu旨在提供一个透明、高效的解决方案。

Method: 采用模块化架构，优化资源效率，支持单GPU训练，并实现高度并行化的自对弈数据生成。

Result: 在Gomoku比赛中表现出色，对战胜率高，且训练速度提升3.2倍。

Conclusion: AlphaZero-Edu开源，为学术和工业应用提供了实用基准。

Abstract: Recent years have witnessed significant progress in reinforcement learning,
especially with Zero-like paradigms, which have greatly boosted the
generalization and reasoning abilities of large-scale language models.
Nevertheless, existing frameworks are often plagued by high implementation
complexity and poor reproducibility. To tackle these challenges, we present
AlphaZero-Edu, a lightweight, education-focused implementation built upon the
mathematical framework of AlphaZero. It boasts a modular architecture that
disentangles key components, enabling transparent visualization of the
algorithmic processes. Additionally, it is optimized for resource-efficient
training on a single NVIDIA RTX 3090 GPU and features highly parallelized
self-play data generation, achieving a 3.2-fold speedup with 8 processes. In
Gomoku matches, the framework has demonstrated exceptional performance,
achieving a consistently high win rate against human opponents. AlphaZero-Edu
has been open-sourced at https://github.com/StarLight1212/AlphaZero_Edu,
providing an accessible and practical benchmark for both academic research and
industrial applications.

</details>

### [99] [Surrogate Fitness Metrics for Interpretable Reinforcement Learning](https://arxiv.org/abs/2504.14645)
*Philipp Altmann,Céline Davignon,Maximilian Zorn,Fabian Ritz,Claudia Linnhoff-Popien,Thomas Gabor*

Main category: cs.LG

TLDR: 通过进化优化框架生成多样且信息丰富的策略演示，结合局部多样性、行为确定性和全局多样性优化轨迹选择，显著提升RL策略的可解释性。


<details>
  <summary>Details</summary>
Motivation: 提高强化学习（RL）策略的可解释性，特别是在安全关键和需要解释性的领域。

Method: 采用进化优化框架，通过扰动初始状态生成演示，并使用联合代理适应度函数（结合多样性、行为确定性等）指导优化。评估指标包括奖励最优性差距、保真度IQMs等。

Result: 在离散和连续环境中，优化轨迹选择显著提升了策略的可解释性。网格世界中演示保真度显著优于随机和消融基线，连续控制中为早期策略提供重要见解。

Conclusion: 通过优化和分析代理适应度函数，本研究提升了RL模型的可解释性，为决策提供更深入见解。

Abstract: We employ an evolutionary optimization framework that perturbs initial states
to generate informative and diverse policy demonstrations. A joint surrogate
fitness function guides the optimization by combining local diversity,
behavioral certainty, and global population diversity. To assess demonstration
quality, we apply a set of evaluation metrics, including the reward-based
optimality gap, fidelity interquartile means (IQMs), fitness composition
analysis, and trajectory visualizations. Hyperparameter sensitivity is also
examined to better understand the dynamics of trajectory optimization. Our
findings demonstrate that optimizing trajectory selection via surrogate fitness
metrics significantly improves interpretability of RL policies in both discrete
and continuous environments. In gridworld domains, evaluations reveal
significantly enhanced demonstration fidelities compared to random and ablated
baselines. In continuous control, the proposed framework offers valuable
insights, particularly for early-stage policies, while fidelity-based
optimization proves more effective for mature policies. By refining and
systematically analyzing surrogate fitness functions, this study advances the
interpretability of RL models. The proposed improvements provide deeper
insights into RL decision-making, benefiting applications in safety-critical
and explainability-focused domains.

</details>

### [100] [LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs](https://arxiv.org/abs/2504.14655)
*Yunhui Xia,Wei Shen,Yan Wang,Jason Klein Liu,Huifeng Sun,Siyue Wu,Jian Hu,Xiaolong Xu*

Main category: cs.LG

TLDR: LeetCodeDataset是一个高质量代码生成模型评估和训练基准，解决了LLM研究中缺乏推理导向的编码基准和自包含训练测试平台的问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM研究中缺乏推理导向的编码基准和自包含训练测试平台的挑战。

Method: 通过整理LeetCode Python问题，提供丰富元数据、广泛覆盖、每个问题100+测试用例和时间分割（2024年7月前后），实现无污染评估和高效监督微调（SFT）。

Result: 实验显示推理模型显著优于非推理模型，仅用2.6K模型生成解决方案的SFT性能与110K样本相当。

Conclusion: LeetCodeDataset和评估框架已在Hugging Face和Github上开源。

Abstract: We introduce LeetCodeDataset, a high-quality benchmark for evaluating and
training code-generation models, addressing two key challenges in LLM research:
the lack of reasoning-focused coding benchmarks and self-contained training
testbeds. By curating LeetCode Python problems with rich metadata, broad
coverage, 100+ test cases per problem, and temporal splits (pre/post July
2024), our dataset enables contamination-free evaluation and efficient
supervised fine-tuning (SFT). Experiments show reasoning models significantly
outperform non-reasoning counterparts, while SFT with only 2.6K model-generated
solutions achieves performance comparable to 110K-sample counterparts. The
dataset and evaluation framework are available on Hugging Face and Github.

</details>

### [101] [Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning](https://arxiv.org/abs/2504.14662)
*Yeoreum Lee,Jinwook Jung,Sungyong Baik*

Main category: cs.LG

TLDR: 该论文提出了一种新的微调目标函数，旨在减少参数干扰并提升任务性能，基于锐度感知最小化（SAM）方法。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习模型的预训练-微调范式导致了许多任务特定模型的涌现，但合并这些模型时存在参数干扰问题。现有方法在减少干扰的同时牺牲了性能，因此需要一种兼顾两者的新方法。

Method: 设计了一种新的微调目标函数，借鉴了锐度感知最小化（SAM）的思想，以同时实现参数干扰最小化和任务性能优化。

Result: 实验和理论结果表明，该方法在模型合并和微调任务中表现优异，显著提升了性能。

Conclusion: 通过锐度感知最小化方法进行微调，能够有效减少参数干扰并提升任务性能，为模型合并提供了新思路。

Abstract: Large-scale deep learning models with a pretraining-finetuning paradigm have
led to a surge of numerous task-specific models fine-tuned from a common
pre-trained model. Recently, several research efforts have been made on merging
these large models into a single multi-task model, particularly with simple
arithmetic on parameters. Such merging methodology faces a central challenge:
interference between model parameters fine-tuned on different tasks. Few recent
works have focused on designing a new fine-tuning scheme that can lead to small
parameter interference, however at the cost of the performance of each
task-specific fine-tuned model and thereby limiting that of a merged model. To
improve the performance of a merged model, we note that a fine-tuning scheme
should aim for (1) smaller parameter interference and (2) better performance of
each fine-tuned model on the corresponding task. In this work, we aim to design
a new fine-tuning objective function to work towards these two goals. In the
course of this process, we find such objective function to be strikingly
similar to sharpness-aware minimization (SAM) objective function, which aims to
achieve generalization by finding flat minima. Drawing upon our observation, we
propose to fine-tune pre-trained models via sharpness-aware minimization. The
experimental and theoretical results showcase the effectiveness and
orthogonality of our proposed approach, improving performance upon various
merging and fine-tuning methods. Our code is available at
https://github.com/baiklab/SAFT-Merge.

</details>

### [102] [Efficient Federated Split Learning for Large Language Models over Communication Networks](https://arxiv.org/abs/2504.14667)
*Kai Zhao,Zhaohui Yang*

Main category: cs.LG

TLDR: FedsLLM框架结合分割联邦学习和参数高效微调技术，降低边缘设备计算负担，优化资源分配和LoRA秩选择，显著减少训练延迟。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限边缘设备上分布式微调预训练大语言模型的挑战。

Method: 整合分割联邦学习和LoRA技术，提出联合优化问题，开发交替优化算法。

Result: FedsLLM在保持模型精度的同时显著减少客户端计算需求和训练延迟。

Conclusion: FedsLLM为边缘设备上的高效分布式微调提供了可行方案。

Abstract: Fine-tuning pre-trained large language models (LLM) in a distributed manner
poses significant challenges on resource-constrained edge devices. To address
this challenge, we propose FedsLLM, a novel framework that integrates split
federated learning with parameter-efficient fine-tuning techniques. By
leveraging model splitting and Low-Rank Adaptation (LoRA), FedsLLM reduces the
computational burden on edge devices. Furthermore, the introduction of a
federated server facilitates parallel training and enhances privacy. To
accommodate heterogeneous communication conditions and diverse computational
capabilities of edge devices, as well as the impact of LoRA rank selection on
model convergence and training cost, we formulate a joint optimization problem.
The formulated problem jointly optimizes subchannel allocation, power control,
model splitting point selection, and LoRA rank configuration, all aimed at
minimizing total training delay. An alternating optimization algorithm is
developed to efficiently solve this problem and accelerate the training
process. Simulation results demonstrate that the proposed FedsLLM framework
achieves comparable model accuracy while significantly reducing client-side
computational requirements. Furthermore, the proposed resource allocation
scheme and adaptive LoRA rank selection strategy notably reduce the training
latency compared to conventional approaches.

</details>

### [103] [Evaluating Temporal Plasticity in Foundation Time Series Models for Incremental Fine-tuning](https://arxiv.org/abs/2504.14677)
*Jia Liu,Cheng Jinguo,Xia Fang,Zhenyuan Ma,Yuankai Wu*

Main category: cs.LG

TLDR: 研究了时间序列基础模型在持续学习中的表现，发现其优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 探索时间序列基础模型在增量学习中的潜力，填补研究空白。

Method: 通过真实数据集和新型持续学习框架，对比传统模型与基础模型的表现。

Result: 基础模型（如Time-MoE和Chronos）在持续学习中表现更优，预测准确性持续提升。

Conclusion: 优化基础模型的微调策略比开发领域特定小模型更有价值，为未来研究提供了新方法。

Abstract: Time series foundation models excel at diverse time series forecasting tasks,
but their capacity for continuous improvement through incremental learning
remains unexplored. We present the first comprehensive study investigating
these models' temporal plasticity - their ability to progressively enhance
performance through continual learning while maintaining existing capabilities.
Through experiments on real-world datasets exhibiting distribution shifts, we
evaluate both conventional deep learning models and foundation models using a
novel continual learning framework. Our findings reveal that while traditional
models struggle with performance deterioration during incremental fine-tuning,
foundation models like Time-MoE and Chronos demonstrate sustained improvement
in predictive accuracy. This suggests that optimizing foundation model
fine-tuning strategies may be more valuable than developing domain-specific
small models. Our research introduces new evaluation methodologies and insights
for developing foundation time series models with robust continuous learning
capabilities.

</details>

### [104] [Learning Critically: Selective Self Distillation in Federated Learning on Non-IID Data](https://arxiv.org/abs/2504.14694)
*Yuting He,Yiqiang Chen,XiaoDong Yang,Hanchao Yu,Yi-Hua Huang,Yang Gu*

Main category: cs.LG

TLDR: 提出了FedSSD方法，通过选择性自蒸馏解决联邦学习中的数据异构性问题，提升模型性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 数据异构性（非IID）导致联邦学习中本地模型偏离全局知识，性能下降且收敛缓慢。现有方法缺乏适应性调节，效率不足。

Method: 提出选择性自蒸馏（FedSSD），通过评估类别和样本可信度，自适应约束本地更新，蒸馏全局模型知识。

Result: 理论分析了FedSSD的收敛性，实验表明其在更少通信轮次下优于其他先进方法，具有更好的泛化性和鲁棒性。

Conclusion: FedSSD有效解决了非IID数据问题，提升了联邦学习的性能和效率。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train a
global model while keeping local data decentralized. Data heterogeneity
(non-IID) across clients has imposed significant challenges to FL, which makes
local models re-optimize towards their own local optima and forget the global
knowledge, resulting in performance degradation and convergence slowdown. Many
existing works have attempted to address the non-IID issue by adding an extra
global-model-based regularizing item to the local training but without an
adaption scheme, which is not efficient enough to achieve high performance with
deep learning models. In this paper, we propose a Selective Self-Distillation
method for Federated learning (FedSSD), which imposes adaptive constraints on
the local updates by self-distilling the global model's knowledge and
selectively weighting it by evaluating the credibility at both the class and
sample level. The convergence guarantee of FedSSD is theoretically analyzed and
extensive experiments are conducted on three public benchmark datasets, which
demonstrates that FedSSD achieves better generalization and robustness in fewer
communication rounds, compared with other state-of-the-art FL methods.

</details>

### [105] [Quantitative Clustering in Mean-Field Transformer Models](https://arxiv.org/abs/2504.14697)
*Shi Chen,Zhengjiang Lin,Yury Polyanskiy,Philippe Rigollet*

Main category: cs.LG

TLDR: 论文研究了深度Transformer模型中token演化的长期聚类行为，类似于Kuramoto模型的同步现象。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型中token演化的动力学行为，尤其是其长期聚类特性。

Method: 通过分析均值场Transformer模型，研究其参数假设下的指数收缩速率。

Result: 在适当正则化条件下，模型会以指数速率收缩到一个Dirac点质量，实现快速同步。

Conclusion: 均值场Transformer模型在特定条件下表现出快速的指数同步行为。

Abstract: The evolution of tokens through a deep transformer models can be modeled as
an interacting particle system that has been shown to exhibit an asymptotic
clustering behavior akin to the synchronization phenomenon in Kuramoto models.
In this work, we investigate the long-time clustering of mean-field transformer
models. More precisely, we establish exponential rates of contraction to a
Dirac point mass for any suitably regular initialization under some assumptions
on the parameters of transformer models, any suitably regular mean-field
initialization synchronizes exponentially fast with some quantitative rates.

</details>

### [106] [Connecting Parameter Magnitudes and Hessian Eigenspaces at Scale using Sketched Methods](https://arxiv.org/abs/2504.14701)
*Andres Fernandez,Frank Schneider,Maren Mahsereci,Philipp Hennig*

Main category: cs.LG

TLDR: 研究发现，深度学习中的损失Hessian矩阵的顶部特征空间与参数剪枝掩码之间存在显著重叠，表明大参数倾向于与高曲率方向对齐。


<details>
  <summary>Details</summary>
Motivation: 探讨深度学习训练中损失Hessian矩阵的顶部特征空间与参数剪枝掩码之间的潜在联系。

Method: 开发了一种基于Grassmannian度量的方法，通过矩阵自由算法计算Hessian特征对，分析掩码与特征空间的相似性。

Result: 实验显示，参数剪枝掩码与Hessian顶部特征空间的重叠显著高于随机水平，且网络规模越大效果越明显。

Conclusion: 研究提供了分析深度学习Hessian矩阵的新方法，并揭示了其特征空间结构的独特性质。

Abstract: Recently, it has been observed that when training a deep neural net with SGD,
the majority of the loss landscape's curvature quickly concentrates in a tiny
*top* eigenspace of the loss Hessian, which remains largely stable thereafter.
Independently, it has been shown that successful magnitude pruning masks for
deep neural nets emerge early in training and remain stable thereafter. In this
work, we study these two phenomena jointly and show that they are connected: We
develop a methodology to measure the similarity between arbitrary parameter
masks and Hessian eigenspaces via Grassmannian metrics. We identify *overlap*
as the most useful such metric due to its interpretability and stability. To
compute *overlap*, we develop a matrix-free algorithm based on sketched SVDs
that allows us to compute over 1000 Hessian eigenpairs for nets with over 10M
parameters --an unprecedented scale by several orders of magnitude. Our
experiments reveal an *overlap* between magnitude parameter masks and top
Hessian eigenspaces consistently higher than chance-level, and that this effect
gets accentuated for larger network sizes. This result indicates that *top
Hessian eigenvectors tend to be concentrated around larger parameters*, or
equivalently, that *larger parameters tend to align with directions of larger
loss curvature*. Our work provides a methodology to approximate and analyze
deep learning Hessians at scale, as well as a novel insight on the structure of
their eigenspace.

</details>

### [107] [Can We Ignore Labels In Out of Distribution Detection?](https://arxiv.org/abs/2504.14704)
*Hong Yang,Qi Yu,Travis Desel*

Main category: cs.LG

TLDR: 本文从信息论角度提出了无标记OOD检测失败的理论条件，定义了新任务Adjacent OOD检测，并通过实验验证现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 无标记OOD检测在安全关键系统中至关重要，但现有方法在真实数据条件下可能失败，需理论支持。

Method: 提出信息论条件证明无标记OOD检测失败，定义Adjacent OOD任务，并通过实验验证理论。

Result: 现有无标记OOD方法在标签盲区条件下失败，揭示了安全漏洞。

Conclusion: 研究为未来无标记OOD方法提供了理论指导，需解决标签盲区问题。

Abstract: Out-of-distribution (OOD) detection methods have recently become more
prominent, serving as a core element in safety-critical autonomous systems. One
major purpose of OOD detection is to reject invalid inputs that could lead to
unpredictable errors and compromise safety. Due to the cost of labeled data,
recent works have investigated the feasibility of self-supervised learning
(SSL) OOD detection, unlabeled OOD detection, and zero shot OOD detection. In
this work, we identify a set of conditions for a theoretical guarantee of
failure in unlabeled OOD detection algorithms from an information-theoretic
perspective. These conditions are present in all OOD tasks dealing with
real-world data: I) we provide theoretical proof of unlabeled OOD detection
failure when there exists zero mutual information between the learning
objective and the in-distribution labels, a.k.a. 'label blindness', II) we
define a new OOD task - Adjacent OOD detection - that tests for label blindness
and accounts for a previously ignored safety gap in all OOD detection
benchmarks, and III) we perform experiments demonstrating that existing
unlabeled OOD methods fail under conditions suggested by our label blindness
theory and analyze the implications for future research in unlabeled OOD
methods.

</details>

### [108] [Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation](https://arxiv.org/abs/2504.14716)
*Tuhina Tripathi,Manya Wadhwa,Greg Durrett,Scott Niekum*

Main category: cs.LG

TLDR: 研究表明，反馈协议的选择（绝对评分与相对偏好）显著影响LLM评估的可靠性，绝对评分更抗干扰。


<details>
  <summary>Details</summary>
Motivation: 探讨反馈协议对LLM评估和训练信号的影响，以提升模型可靠性。

Method: 比较绝对评分和相对偏好协议，分析其对评估偏差和生成模型操纵的敏感性。

Result: 绝对评分更稳健，偏好翻转率仅9%，而相对偏好为35%。

Conclusion: 建议根据数据集特性和评估目标选择反馈协议，优先考虑绝对评分以减少偏差。

Abstract: Large Language Models (LLMs) are widely used as proxies for human labelers in
both training (Reinforcement Learning from AI Feedback) and large-scale
response evaluation (LLM-as-a-judge). Alignment and evaluation are critical
components in the development of reliable LLMs, and the choice of feedback
protocol plays a central role in both but remains understudied. In this work,
we show that the choice of feedback protocol (absolute scores versus relative
preferences) can significantly affect evaluation reliability and induce
systematic biases. In particular, we show that pairwise evaluation protocols
are more vulnerable to distracted evaluation. Generator models can exploit
spurious attributes (or distractor features) favored by the LLM judge,
resulting in inflated scores for lower-quality outputs and misleading training
signals. We find that absolute scoring is more robust to such manipulation,
producing judgments that better reflect response quality and are less
influenced by distractor features. Our results demonstrate that generator
models can flip preferences by embedding distractor features, skewing
LLM-as-a-judge comparisons and leading to inaccurate conclusions about model
quality in benchmark evaluations. Pairwise preferences flip in about 35% of the
cases, compared to only 9% for absolute scores. We offer recommendations for
choosing feedback protocols based on dataset characteristics and evaluation
objectives.

</details>

### [109] [Semi-parametric Memory Consolidation: Towards Brain-like Deep Continual Learning](https://arxiv.org/abs/2504.14727)
*Geng Liu,Fei Zhu,Rong Feng,Zhiqiang Yi,Shiqi Wang,Gaofeng Meng,Zhaoxiang Zhang*

Main category: cs.LG

TLDR: 提出了一种仿生持续学习框架，结合半参数记忆和醒睡巩固机制，解决了深度神经网络在连续任务中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在连续任务中容易遗忘先前知识，而人类和动物具备持续学习能力，因此研究如何模拟生物智能实现持续学习。

Method: 提出了一种仿生框架，整合半参数记忆和醒睡巩固机制，模拟人类记忆系统。

Result: 在真实世界的持续学习场景（如ImageNet的类增量学习）中，该方法能保持新任务的高性能并保留先前知识。

Conclusion: 模拟生物智能是实现深度神经网络持续学习能力的有效途径。

Abstract: Humans and most animals inherently possess a distinctive capacity to
continually acquire novel experiences and accumulate worldly knowledge over
time. This ability, termed continual learning, is also critical for deep neural
networks (DNNs) to adapt to the dynamically evolving world in open
environments. However, DNNs notoriously suffer from catastrophic forgetting of
previously learned knowledge when trained on sequential tasks. In this work,
inspired by the interactive human memory and learning system, we propose a
novel biomimetic continual learning framework that integrates semi-parametric
memory and the wake-sleep consolidation mechanism. For the first time, our
method enables deep neural networks to retain high performance on novel tasks
while maintaining prior knowledge in real-world challenging continual learning
scenarios, e.g., class-incremental learning on ImageNet. This study
demonstrates that emulating biological intelligence provides a promising path
to enable deep neural networks with continual learning capabilities.

</details>

### [110] [Geometric Learning Dynamics](https://arxiv.org/abs/2504.14728)
*Vitaly Vanchurin*

Main category: cs.LG

TLDR: N/A


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a unified geometric framework for modeling learning dynamics in
physical, biological, and machine learning systems. The theory reveals three
fundamental regimes, each emerging from the power-law relationship $g \propto
\kappa^a$ between the metric tensor $g$ in the space of trainable variables and
the noise covariance matrix $\kappa$. The quantum regime corresponds to $a = 1$
and describes Schr\"odinger-like dynamics that emerges from a discrete shift
symmetry. The efficient learning regime corresponds to $a = \tfrac{1}{2}$ and
describes very fast machine learning algorithms. The equilibration regime
corresponds to $a = 0$ and describes classical models of biological evolution.
We argue that the emergence of the intermediate regime $a = \tfrac{1}{2}$ is a
key mechanism underlying the emergence of biological complexity.

</details>

### [111] [Reinforcement Learning from Multi-level and Episodic Human Feedback](https://arxiv.org/abs/2504.14732)
*Muhammad Qasim Elahi,Somtochukwu Oguchienti,Maheed H. Ahmed,Mahsa Ghasemi*

Main category: cs.LG

TLDR: 论文提出了一种基于多级人类反馈的强化学习算法，用于从非马尔可夫奖励中学习奖励函数和最优策略。


<details>
  <summary>Details</summary>
Motivation: 设计有效的奖励函数在复杂任务中具有挑战性，现有方法多依赖人类比较反馈，本文探索多级反馈的潜力。

Method: 提出一种算法，利用每回合结束时的评分反馈学习奖励函数和最优策略，适用于非马尔可夫奖励。

Result: 算法在理论上实现了次线性遗憾，并通过仿真验证了其有效性。

Conclusion: 多级人类反馈为奖励学习提供了更丰富的信号，算法在理论和实践中均表现优异。

Abstract: Designing an effective reward function has long been a challenge in
reinforcement learning, particularly for complex tasks in unstructured
environments. To address this, various learning paradigms have emerged that
leverage different forms of human input to specify or refine the reward
function. Reinforcement learning from human feedback is a prominent approach
that utilizes human comparative feedback, expressed as a preference for one
behavior over another, to tackle this problem. In contrast to comparative
feedback, we explore multi-level human feedback, which is provided in the form
of a score at the end of each episode. This type of feedback offers more coarse
but informative signals about the underlying reward function than binary
feedback. Additionally, it can handle non-Markovian rewards, as it is based on
the evaluation of an entire episode. We propose an algorithm to efficiently
learn both the reward function and the optimal policy from this form of
feedback. Moreover, we show that the proposed algorithm achieves sublinear
regret and demonstrate its empirical effectiveness through extensive
simulations.

</details>

### [112] [AltGDmin: Alternating GD and Minimization for Partly-Decoupled (Federated) Optimization](https://arxiv.org/abs/2504.14741)
*Namrata Vaswani*

Main category: cs.LG

TLDR: 提出了一种新的优化框架AltGDmin，用于解决交替最小化（AltMin）适用的问题，特别适用于其中一个变量子集优化更快且成本函数可微的情况。


<details>
  <summary>Details</summary>
Motivation: AltMin在某些问题中效率较低，尤其是当优化一个变量子集比另一个快得多时。AltGDmin旨在提高这类问题的求解速度。

Method: AltGDmin结合梯度下降和最小化，特别适用于问题对某一变量子集优化更快且成本函数可微的情况。

Result: AltGDmin在多种问题（如低秩压缩感知、矩阵补全、鲁棒PCA等）中比AltMin更快且通信高效。

Conclusion: AltGDmin为多种优化问题提供了更高效的解决方案，尤其在联邦学习等分布式场景中表现优异。

Abstract: This article describes a novel optimization solution framework, called
alternating gradient descent (GD) and minimization (AltGDmin), that is useful
for many problems for which alternating minimization (AltMin) is a popular
solution. AltMin is a special case of the block coordinate descent algorithm
that is useful for problems in which minimization w.r.t one subset of variables
keeping the other fixed is closed form or otherwise reliably solved. Denote the
two blocks/subsets of the optimization variables Z by Za, Zb, i.e., Z = {Za,
Zb}. AltGDmin is often a faster solution than AltMin for any problem for which
(i) the minimization over one set of variables, Zb, is much quicker than that
over the other set, Za; and (ii) the cost function is differentiable w.r.t. Za.
Often, the reason for one minimization to be quicker is that the problem is
``decoupled" for Zb and each of the decoupled problems is quick to solve. This
decoupling is also what makes AltGDmin communication-efficient for federated
settings.
  Important examples where this assumption holds include (a) low rank
column-wise compressive sensing (LRCS), low rank matrix completion (LRMC), (b)
their outlier-corrupted extensions such as robust PCA, robust LRCS and robust
LRMC; (c) phase retrieval and its sparse and low-rank model based extensions;
(d) tensor extensions of many of these problems such as tensor LRCS and tensor
completion; and (e) many partly discrete problems where GD does not apply --
such as clustering, unlabeled sensing, and mixed linear regression. LRCS finds
important applications in multi-task representation learning and few shot
learning, federated sketching, and accelerated dynamic MRI. LRMC and robust PCA
find important applications in recommender systems, computer vision and video
analytics.

</details>

### [113] [AI for the Open-World: the Learning Principles](https://arxiv.org/abs/2504.14751)
*Jianyu Zhang*

Main category: cs.LG

TLDR: 论文探讨了封闭世界AI的成功是否适用于开放世界，指出开放世界AI需要独特的学习原则和技术。


<details>
  <summary>Details</summary>
Motivation: 封闭世界AI在特定任务上表现出色，但缺乏处理开放世界任务的能力，需要新的学习原则。

Method: 提出开放世界AI的学习原则（如丰富特征、解耦表示和推理时学习），并通过大规模实验验证。

Result: 验证了提出的学习原则在开放世界AI中的有效性。

Conclusion: 开放世界AI需要不同于封闭世界的新学习原则和技术，论文为此提供了理论和实验支持。

Abstract: During the past decades, numerous successes of AI has been made on "specific
capabilities", named closed-world, such as artificial environments or specific
real-world tasks. This well-defined narrow capability brings two nice benefits,
a clear criterion of success and the opportunity to collect a lot of examples.
The criteria not only reveal whether a machine has achieved a goal, but reveal
how the machine falls short of the goal. As a result, human designers can fix
the problems one after the other until the machine is deemed good enough for
the task. Furthermore, the large set of collected examples reduces the
difficulty of this problem-fixing process (by the central limit theorem).
  Do the success in closed-world translate into broad open-world, where a
machine is required to perform any task that a human could possibly undertake
with fewer examples and less priori knowledge from human designers? No. Because
competence in a specific task provides little insight in handling other tasks,
the valuable criteria for specific tasks become helpless when handling broader
unseen tasks. Furthermore, due to the shortage of examples in unseen tasks,
central limit theorem does not stand on our side. At the end, human designers
lose the oscilloscope to "hack" an AI system for the open-world.
  Achieving AI for the open-world requires unique learning principles and
innovated techniques, which are different from the ones in building AI for the
closed-world. This thesis explores necessary learning principles required to
construct AI for the open-world, including rich features (analogy a large tool
box), disentangled representation (an organized tool box), and inference-time
learning (a tool-savvy hand). Driven by the learning principles, this thesis
further proposes techniques to use the learning principles, conducts enormous
large-scale experiments to verify the learning principles.

</details>

### [114] [A Combinatorial Theory of Dropout: Subnetworks, Graph Geometry, and Generalization](https://arxiv.org/abs/2504.14762)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TLDR: 本文提出了一种基于组合和图论的dropout理论，将训练建模为在高维二进制子网络图上的随机游走，揭示了dropout通过采样稳健、结构化的子网络集合来提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究dropout的机制，探索其如何通过随机子网络采样提升模型的泛化能力。

Method: 使用组合和图论工具，将训练建模为高维图上的随机游走，定义子网络贡献分数，并结合谱图理论、PAC-Bayes分析和组合数学进行理论证明。

Result: 泛化能力强的子网络形成大而连通的低阻力簇，数量随网络宽度指数增长；实验验证了理论结果。

Conclusion: 为理解dropout提供了统一的理论基础，并提出了掩码引导正则化和子网络优化的新方向。

Abstract: We propose a combinatorial and graph-theoretic theory of dropout by modeling
training as a random walk over a high-dimensional graph of binary subnetworks.
Each node represents a masked version of the network, and dropout induces
stochastic traversal across this space. We define a subnetwork contribution
score that quantifies generalization and show that it varies smoothly over the
graph. Using tools from spectral graph theory, PAC-Bayes analysis, and
combinatorics, we prove that generalizing subnetworks form large, connected,
low-resistance clusters, and that their number grows exponentially with network
width. This reveals dropout as a mechanism for sampling from a robust,
structured ensemble of well-generalizing subnetworks with built-in redundancy.
Extensive experiments validate every theoretical claim across diverse
architectures. Together, our results offer a unified foundation for
understanding dropout and suggest new directions for mask-guided regularization
and subnetwork optimization.

</details>

### [115] [Novel Concept-Oriented Synthetic Data approach for Training Generative AI-Driven Crystal Grain Analysis Using Diffusion Model](https://arxiv.org/abs/2504.14782)
*Ahmed Sobhi Saleh,Kristof Croes,Hajdin Ceric,Ingrid De Wolf,Houman Zahedmanesh*

Main category: cs.LG

TLDR: 提出了一种结合边缘检测和生成扩散模型的自动化方法，用于从显微镜图像中提取多晶粒结构，解决了传统方法效率低、主观性强的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如TEM和SEM）提取多晶粒结构时效率低、主观性强且耗时，限制了高通量分析的可扩展性。

Method: 采用七阶段方法生成合成TEM图像用于训练，结合边缘检测和生成扩散模型，自动识别晶粒、消除噪声并连接断裂片段。

Result: 模型应用于多种金属，平均晶粒尺寸低至纳米级，从低分辨率TEM图像中提取的晶粒形态与高要求实验技术结果相当，平均准确率达97.23%。

Conclusion: 该方法不仅解决了数据稀缺问题，还可推广至其他领域，为高通量分析提供了高效解决方案。

Abstract: The traditional techniques for extracting polycrystalline grain structures
from microscopy images, such as transmission electron microscopy (TEM) and
scanning electron microscopy (SEM), are labour-intensive, subjective, and
time-consuming, limiting their scalability for high-throughput analysis. In
this study, we present an automated methodology integrating edge detection with
generative diffusion models to effectively identify grains, eliminate noise,
and connect broken segments in alignment with predicted grain boundaries. Due
to the limited availability of adequate images preventing the training of deep
machine learning models, a new seven-stage methodology is employed to generate
synthetic TEM images for training. This concept-oriented synthetic data
approach can be extended to any field of interest where the scarcity of data is
a challenge. The presented model was applied to various metals with average
grain sizes down to the nanoscale, producing grain morphologies from
low-resolution TEM images that are comparable to those obtained from advanced
and demanding experimental techniques with an average accuracy of 97.23%.

</details>

### [116] [Enhanced Data-driven Topology Design Methodology with Multi-level Mesh and Correlation-based Mutation for Stress-related Multi-objective Optimization](https://arxiv.org/abs/2504.14790)
*Jun Yang,Shintaro Yamasaki*

Main category: cs.LG

TLDR: 论文提出了一种基于多级网格的数据驱动拓扑设计（DDTD）方法，通过相关性变异模块解决初始数据集质量对结果的影响，并提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统的基于敏感性的拓扑优化方法难以解决强非线性优化问题，而现有的DDTD方法对初始数据集质量敏感，限制了其通用性和有效性。

Method: 采用多级网格策略和相关性变异模块，逐步细化结构表示，避免高自由度表示，同时赋予生成数据新的几何特征。

Result: 实验表明，该方法能在低质量初始数据集下工作，显著提高通用性并降低计算成本。

Conclusion: 提出的方法在强非线性问题上表现出优越的通用性和有效性。

Abstract: Topology optimization (TO) serves as a widely applied structural design
approach to tackle various engineering problems. Nevertheless,
sensitivity-based TO methods usually struggle with solving strongly nonlinear
optimization problems. By leveraging high capacity of deep generative model,
which is an influential machine learning technique, the sensitivity-free
data-driven topology design (DDTD) methodology is regarded as an effective
means of overcoming these issues. The DDTD methodology depends on initial
dataset with a certain regularity, making its results highly sensitive to
initial dataset quality. This limits its effectiveness and generalizability,
especially for optimization problems without priori information. In this
research, we proposed a multi-level mesh DDTD-based method with
correlation-based mutation module to escape from the limitation of the quality
of the initial dataset on the results and enhance computational efficiency. The
core is to employ a correlation-based mutation module to assign new geometric
features with physical meaning to the generated data, while utilizing a
multi-level mesh strategy to progressively enhance the refinement of the
structural representation, thus avoiding the maintenance of a high
degree-of-freedom (DOF) representation throughout the iterative process. The
proposed multi-level mesh DDTD-based method can be driven by a low quality
initial dataset without the need for time-consuming construction of a specific
dataset, thus significantly increasing generality and reducing application
difficulty, while further lowering computational cost of DDTD methodology.
Various comparison experiments with the traditional sensitivity-based TO
methods on stress-related strongly nonlinear problems demonstrate the
generality and effectiveness of the proposed method.

</details>

### [117] [Edge-boosted graph learning for functional brain connectivity analysis](https://arxiv.org/abs/2504.14796)
*David Yang,Mostafa Abdelmegeed,John Modl,Minjeong Kim*

Main category: cs.LG

TLDR: 提出了一种基于边功能连接（eFC）的新方法，优于现有GNN方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于节点的脑连接方法无法准确捕捉功能连接，需改进。

Method: 采用边功能连接（eFC）和共嵌入技术分析脑网络。

Result: 在ADNI和PPMI数据集上显著优于现有GNN方法。

Conclusion: 边功能连接方法在脑网络分类中更有效。

Abstract: Predicting disease states from functional brain connectivity is critical for
the early diagnosis of severe neurodegenerative diseases such as Alzheimer's
Disease and Parkinson's Disease. Existing studies commonly employ Graph Neural
Networks (GNNs) to infer clinical diagnoses from node-based brain connectivity
matrices generated through node-to-node similarities of regionally averaged
fMRI signals. However, recent neuroscience studies found that such node-based
connectivity does not accurately capture ``functional connections" within the
brain. This paper proposes a novel approach to brain network analysis that
emphasizes edge functional connectivity (eFC), shifting the focus to inter-edge
relationships. Additionally, we introduce a co-embedding technique to integrate
edge functional connections effectively. Experimental results on the ADNI and
PPMI datasets demonstrate that our method significantly outperforms
state-of-the-art GNN methods in classifying functional brain networks.

</details>

### [118] [Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models](https://arxiv.org/abs/2504.14798)
*Hao Xuan,Xingyu Li*

Main category: cs.LG

TLDR: 论文提出了一种新的机器遗忘安全标准（Robust Unlearning），并通过Unlearning Mapping Attack（UMA）验证现有遗忘技术的漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘技术无法完全消除遗忘信息的痕迹，导致隐私泄露风险，需要更严格的验证方法。

Method: 提出Robust Unlearning概念，并设计UMA框架，通过对抗性查询主动检测模型中的残留信息。

Result: 实验表明现有遗忘技术仍存在漏洞，即使通过传统验证标准。

Conclusion: UMA为机器遗忘安全提供了新的评估标准，有助于提升隐私保护能力。

Abstract: Machine Unlearning (MUL) is crucial for privacy protection and content
regulation, yet recent studies reveal that traces of forgotten information
persist in unlearned models, enabling adversaries to resurface removed
knowledge. Existing verification methods only confirm whether unlearning was
executed, failing to detect such residual information leaks. To address this,
we introduce the concept of Robust Unlearning, ensuring models are
indistinguishable from retraining and resistant to adversarial recovery. To
empirically evaluate whether unlearning techniques meet this security standard,
we propose the Unlearning Mapping Attack (UMA), a post-unlearning verification
framework that actively probes models for forgotten traces using adversarial
queries. Extensive experiments on discriminative and generative tasks show that
existing unlearning techniques remain vulnerable, even when passing existing
verification metrics. By establishing UMA as a practical verification tool,
this study sets a new standard for assessing and enhancing machine unlearning
security.

</details>

### [119] [A Survey on Small Sample Imbalance Problem: Metrics, Feature Analysis, and Solutions](https://arxiv.org/abs/2504.14800)
*Shuxian Zhao,Jie Gui,Minjing Dong,Baosheng Yu,Zhipeng Gui,Lu Dong,Yuan Yan Tang,James Tin-Yau Kwok*

Main category: cs.LG

TLDR: 论文提出了一种系统性分析框架，针对小样本不平衡问题（S&I），通过总结不平衡指标和复杂性分析方法，强调可解释基准的重要性，并比较了不同数据分布下的解决方案。实验表明分类器性能差异远超过重采样改进。


<details>
  <summary>Details</summary>
Motivation: 小样本不平衡问题是机器学习和数据分析中的主要挑战，现有方法多依赖启发式算法，缺乏对数据特性的深入分析。

Method: 提出系统性分析框架，总结不平衡指标和复杂性分析方法，比较不同数据分布下的解决方案，并进行实验验证。

Result: 实验发现分类器性能差异显著超过重采样的改进效果。

Conclusion: 论文强调了可解释基准的重要性，并讨论了未来研究方向。

Abstract: The small sample imbalance (S&I) problem is a major challenge in machine
learning and data analysis. It is characterized by a small number of samples
and an imbalanced class distribution, which leads to poor model performance. In
addition, indistinct inter-class feature distributions further complicate
classification tasks. Existing methods often rely on algorithmic heuristics
without sufficiently analyzing the underlying data characteristics. We argue
that a detailed analysis from the data perspective is essential before
developing an appropriate solution. Therefore, this paper proposes a systematic
analytical framework for the S\&I problem. We first summarize imbalance metrics
and complexity analysis methods, highlighting the need for interpretable
benchmarks to characterize S&I problems. Second, we review recent solutions for
conventional, complexity-based, and extreme S&I problems, revealing
methodological differences in handling various data distributions. Our summary
finds that resampling remains a widely adopted solution. However, we conduct
experiments on binary and multiclass datasets, revealing that classifier
performance differences significantly exceed the improvements achieved through
resampling. Finally, this paper highlights open questions and discusses future
trends.

</details>

### [120] [Dynamic Contrastive Skill Learning with State-Transition Based Skill Clustering and Dynamic Length Adjustment](https://arxiv.org/abs/2504.14805)
*Jinwoo Choi,Seung-Woo Seo*

Main category: cs.LG

TLDR: 提出了一种动态对比技能学习（DCSL）框架，通过状态转移表示技能、学习技能相似性函数和动态调整技能长度，解决了现有技能学习方法在语义识别和灵活性上的不足。


<details>
  <summary>Details</summary>
Motivation: 强化学习在长时程任务中面临挑战，现有技能学习方法难以识别语义相似行为且技能长度固定，限制了灵活性和泛化能力。

Method: DCSL框架包含三个关键点：基于状态转移的技能表示、技能相似性函数学习和动态技能长度调整，利用对比学习捕捉行为语义并适应不同时间范围。

Result: DCSL在复杂或噪声数据中表现出更灵活和自适应的技能提取能力，在任务完成和效率上优于现有方法。

Conclusion: DCSL通过改进技能表示和学习方式，显著提升了强化学习在复杂任务中的表现和适应性。

Abstract: Reinforcement learning (RL) has made significant progress in various domains,
but scaling it to long-horizon tasks with complex decision-making remains
challenging. Skill learning attempts to address this by abstracting actions
into higher-level behaviors. However, current approaches often fail to
recognize semantically similar behaviors as the same skill and use fixed skill
lengths, limiting flexibility and generalization. To address this, we propose
Dynamic Contrastive Skill Learning (DCSL), a novel framework that redefines
skill representation and learning. DCSL introduces three key ideas:
state-transition based skill representation, skill similarity function
learning, and dynamic skill length adjustment. By focusing on state transitions
and leveraging contrastive learning, DCSL effectively captures the semantic
context of behaviors and adapts skill lengths to match the appropriate temporal
extent of behaviors. Our approach enables more flexible and adaptive skill
extraction, particularly in complex or noisy datasets, and demonstrates
competitive performance compared to existing methods in task completion and
efficiency.

</details>

### [121] [A Basic Evaluation of Neural Networks Trained with the Error Diffusion Learning Algorithm](https://arxiv.org/abs/2504.14814)
*Kazuhisa Fujita*

Main category: cs.LG

TLDR: Kaneko's Error Diffusion Learning Algorithm (EDLA)是一种生物启发的替代方法，通过全局误差信号在网络中扩散，避免了逐层反向传播。实验表明EDLA在多种任务中表现优异，具有潜在的应用前景。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法缺乏生物合理性，促使开发替代学习方法，如EDLA。

Method: EDLA通过全局误差信号在成对的兴奋-抑制子层网络中扩散，无需逐层反向传播。

Result: EDLA在奇偶校验、回归和图像分类任务中表现高效，性能受学习率、神经元数量和网络深度影响。

Conclusion: EDLA是一种生物启发的有效替代方法，未来可扩展至生物启发神经网络。

Abstract: Artificial neural networks are powerful tools capable of addressing various
tasks. Although the backpropagation algorithm has become a standard training
method for these neural networks, its lack of biological plausibility has
inspired the development of alternative learning approaches. One such
alternative is Kaneko's Error Diffusion Learning Algorithm (EDLA), a
biologically motivated approach wherein a single global error signal diffuses
throughout a network composed of paired excitatory-inhibitory sublayers,
thereby eliminating the necessity for layer-wise backpropagation. This study
presents a contemporary formulation of the EDLA framework and evaluates its
effectiveness through parity check, regression, and image classification tasks.
Our experimental results indicate that EDLA networks can consistently achieve
high accuracy across these benchmarks, with performance efficiency and
convergence speed notably influenced by the choice of learning rate, neuron
count, and network depth. Further investigation of the internal representations
formed by EDLA networks reveals their capacity for meaningful feature
extraction, similar to traditional neural networks. These results suggest that
EDLA is a biologically motivated alternative for training feedforward networks
and will motivate future work on extending this method to biologically inspired
neural networks.

</details>

### [122] [Uncertainty quantification of neural network models of evolving processes via Langevin sampling](https://arxiv.org/abs/2504.14854)
*Cosmin Safta,Reese E. Jones,Ravi G. Patel,Raelynn Wonnacot,Dan S. Bolintineanu,Craig M. Hamel,Sharlotte L. B. Kramer*

Main category: cs.LG

TLDR: 提出了一种基于神经ODE的可扩展近似推理超网络框架，用于历史依赖过程的一般模型，通过Langevin采样平衡计算成本，并在化学和物理数据上验证性能。


<details>
  <summary>Details</summary>
Motivation: 解决历史依赖过程的建模问题，提供灵活的数据模型和高效的参数后验分布近似方法。

Method: 使用神经ODE表示内部状态演化，结合可训练的观测模型，通过Langevin采样学习后验分布参数。

Result: 在化学反应和材料物理数据上验证了超网络的性能，优于均值场变分推理。

Conclusion: 该框架在计算成本和后验密度近似之间提供了灵活性，适用于复杂历史依赖过程的建模。

Abstract: We propose a scalable, approximate inference hypernetwork framework for a
general model of history-dependent processes. The flexible data model is based
on a neural ordinary differential equation (NODE) representing the evolution of
internal states together with a trainable observation model subcomponent. The
posterior distribution corresponding to the data model parameters (weights and
biases) follows a stochastic differential equation with a drift term related to
the score of the posterior that is learned jointly with the data model
parameters. This Langevin sampling approach offers flexibility in balancing the
computational budget between the evaluation cost of the data model and the
approximation of the posterior density of its parameters. We demonstrate
performance of the hypernetwork on chemical reaction and material physics data
and compare it to mean-field variational inference.

</details>

### [123] [Impact of Latent Space Dimension on IoT Botnet Detection Performance: VAE-Encoder Versus ViT-Encoder](https://arxiv.org/abs/2504.14879)
*Hassan Wasswa,Aziida Nanyonga,Timothy Lynar*

Main category: cs.LG

TLDR: 研究探讨了潜在维度对深度学习分类器性能的影响，比较了ViT和VAE编码器在IoT僵尸网络流量数据集上的表现，发现VAE编码器在多个性能指标上优于ViT编码器。


<details>
  <summary>Details</summary>
Motivation: 随着IoT设备的普及，其安全性成为重要问题，尤其是僵尸网络攻击。研究旨在探索潜在维度对分类器性能的影响，以提升IoT安全。

Method: 使用ViT和VAE编码器将高维IoT僵尸网络流量数据降维到潜在空间，并在N-BaIoT和CICIoT2022数据集上评估分类器性能。

Result: VAE编码器在准确率、精确率、召回率和F1分数上均优于ViT编码器，原因是ViT模型无法从非图像数据中提取空间模式。

Conclusion: VAE编码器更适合处理IoT僵尸网络流量数据，为IoT安全提供更有效的降维方法。

Abstract: The rapid evolution of Internet of Things (IoT) technology has led to a
significant increase in the number of IoT devices, applications, and services.
This surge in IoT devices, along with their widespread presence, has made them
a prime target for various cyber-attacks, particularly through IoT botnets. As
a result, security has become a major concern within the IoT ecosystem. This
study focuses on investigating how the latent dimension impacts the performance
of different deep learning classifiers when trained on latent vector
representations of the train dataset. The primary objective is to compare the
outcomes of these models when encoder components from two cutting-edge
architectures: the Vision Transformer (ViT) and the Variational Auto-Encoder
(VAE) are utilized to project the high dimensional train dataset to the learned
low dimensional latent space. The encoder components are employed to project
high-dimensional structured .csv IoT botnet traffic datasets to various latent
sizes. Evaluated on N-BaIoT and CICIoT2022 datasets, findings reveal that
VAE-encoder based dimension reduction outperforms ViT-encoder based dimension
reduction for both datasets in terms of four performance metrics including
accuracy, precision, recall, and F1-score for all models which can be
attributed to absence of spatial patterns in the datasets the ViT model
attempts to learn and extract from image instances.

</details>

### [124] [Some Optimizers are More Equal: Understanding the Role of Optimizers in Group Fairness](https://arxiv.org/abs/2504.14882)
*Mojtaba Kolahdouzi,Hatice Gunes,Ali Etemad*

Main category: cs.LG

TLDR: 研究优化算法选择对深度神经网络群体公平性的影响，发现自适应优化器（如RMSProp）比随机优化器（如SGD）更易收敛到公平解。


<details>
  <summary>Details</summary>
Motivation: 探讨优化算法如何影响模型公平性，特别是在数据不平衡情况下。

Method: 通过随机微分方程分析优化动态，比较自适应与随机优化器，并在多个数据集上实验验证。

Result: RMSProp在公平性上优于SGD，且保持预测准确性。

Conclusion: 自适应优化器是提升公平性的关键机制。

Abstract: We study whether and how the choice of optimization algorithm can impact
group fairness in deep neural networks. Through stochastic differential
equation analysis of optimization dynamics in an analytically tractable setup,
we demonstrate that the choice of optimization algorithm indeed influences
fairness outcomes, particularly under severe imbalance. Furthermore, we show
that when comparing two categories of optimizers, adaptive methods and
stochastic methods, RMSProp (from the adaptive category) has a higher
likelihood of converging to fairer minima than SGD (from the stochastic
category). Building on this insight, we derive two new theoretical guarantees
showing that, under appropriate conditions, RMSProp exhibits fairer parameter
updates and improved fairness in a single optimization step compared to SGD. We
then validate these findings through extensive experiments on three publicly
available datasets, namely CelebA, FairFace, and MS-COCO, across different
tasks as facial expression recognition, gender classification, and multi-label
classification, using various backbones. Considering multiple fairness
definitions including equalized odds, equal opportunity, and demographic
parity, adaptive optimizers like RMSProp and Adam consistently outperform SGD
in terms of group fairness, while maintaining comparable predictive accuracy.
Our results highlight the role of adaptive updates as a crucial yet overlooked
mechanism for promoting fair outcomes.

</details>

### [125] [Latent Bayesian Optimization via Autoregressive Normalizing Flows](https://arxiv.org/abs/2504.14889)
*Seunghun Lee,Jinyoung Park,Jaewon Chu,Minseo Yoon,Hyunwoo J. Kim*

Main category: cs.LG

TLDR: NF-BO利用归一化流解决LBO中的值差异问题，显著提升分子生成任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有LBO方法因输入与潜在空间的重构间隙导致值差异问题，影响优化效果。

Method: 提出NF-BO，采用归一化流建立输入与潜在空间的一对一映射，消除重构间隙，并引入SeqFlow和动态采样策略。

Result: 实验表明NF-BO在分子生成任务中优于传统和最新LBO方法。

Conclusion: NF-BO通过归一化流有效解决了值差异问题，优化性能显著提升。

Abstract: Bayesian Optimization (BO) has been recognized for its effectiveness in
optimizing expensive and complex objective functions. Recent advancements in
Latent Bayesian Optimization (LBO) have shown promise by integrating generative
models such as variational autoencoders (VAEs) to manage the complexity of
high-dimensional and structured data spaces. However, existing LBO approaches
often suffer from the value discrepancy problem, which arises from the
reconstruction gap between input and latent spaces. This value discrepancy
problem propagates errors throughout the optimization process, leading to
suboptimal outcomes. To address this issue, we propose a Normalizing Flow-based
Bayesian Optimization (NF-BO), which utilizes normalizing flow as a generative
model to establish one-to-one encoding function from the input space to the
latent space, along with its left-inverse decoding function, eliminating the
reconstruction gap. Specifically, we introduce SeqFlow, an autoregressive
normalizing flow for sequence data. In addition, we develop a new candidate
sampling strategy that dynamically adjusts the exploration probability for each
token based on its importance. Through extensive experiments, our NF-BO method
demonstrates superior performance in molecule generation tasks, significantly
outperforming both traditional and recent LBO approaches.

</details>

### [126] [Dynamic Graph-Like Learning with Contrastive Clustering on Temporally-Factored Ship Motion Data for Imbalanced Sea State Estimation in Autonomous Vessel](https://arxiv.org/abs/2504.14907)
*Kexin Wang,Mengna Liu,Xu Cheng,Fan Shi,Shanshan Qi,Shengyong Chen*

Main category: cs.LG

TLDR: 提出了一种名为TGC-SSE的新型深度学习模型，用于解决船舶运动数据中的冗余和类别不平衡问题，显著提升了海况估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理船舶运动数据时面临数据冗余和类别不平衡的挑战，限制了海况估计的准确性。

Method: TGC-SSE结合了时间维度分解模块、动态图学习模块和对比聚类损失函数，以优化数据冗余和类别不平衡问题。

Result: 在14个公开数据集上，TGC-SSE显著优于现有方法，其中9个数据集的准确率最高，比EDI提升了20.79%。

Conclusion: TGC-SSE不仅提高了海况估计的准确性，还表现出强大的泛化能力，为自主船舶操作提供了可靠支持。

Abstract: Accurate sea state estimation is crucial for the real-time control and future
state prediction of autonomous vessels. However, traditional methods struggle
with challenges such as data imbalance and feature redundancy in ship motion
data, limiting their effectiveness. To address these challenges, we propose the
Temporal-Graph Contrastive Clustering Sea State Estimator (TGC-SSE), a novel
deep learning model that combines three key components: a time dimension
factorization module to reduce data redundancy, a dynamic graph-like learning
module to capture complex variable interactions, and a contrastive clustering
loss function to effectively manage class imbalance. Our experiments
demonstrate that TGC-SSE significantly outperforms existing methods across 14
public datasets, achieving the highest accuracy in 9 datasets, with a 20.79%
improvement over EDI. Furthermore, in the field of sea state estimation,
TGC-SSE surpasses five benchmark methods and seven deep learning models.
Ablation studies confirm the effectiveness of each module, demonstrating their
respective roles in enhancing overall model performance. Overall, TGC-SSE not
only improves the accuracy of sea state estimation but also exhibits strong
generalization capabilities, providing reliable support for autonomous vessel
operations.

</details>

### [127] [POLYRAG: Integrating Polyviews into Retrieval-Augmented Generation for Medical Applications](https://arxiv.org/abs/2504.14917)
*Chunjing Gan,Dan Yang,Binbin Hu,Ziqi Liu,Yue Shen,Zhiqiang Zhang,Jian Wang,Jun Zhou*

Main category: cs.LG

TLDR: PolyRAG提出了一种多视角检索增强生成方法，解决了现有方法在医学场景中忽视信息时效性、权威性和共性的问题，并通过PolyEVAL基准验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在医学应用中面临知识更新和幻觉问题的挑战，现有检索增强生成方法未充分考虑信息的时效性、权威性和共性。

Method: 提出PolyRAG，通过多视角评估和整合检索结果，优化医学场景中的检索增强生成。

Result: 在PolyEVAL基准上的实验表明，PolyRAG优于现有方法。

Conclusion: PolyRAG通过多视角整合显著提升了医学应用中检索增强生成的性能。

Abstract: Large language models (LLMs) have become a disruptive force in the industry,
introducing unprecedented capabilities in natural language processing, logical
reasoning and so on. However, the challenges of knowledge updates and
hallucination issues have limited the application of LLMs in medical scenarios,
where retrieval-augmented generation (RAG) can offer significant assistance.
Nevertheless, existing retrieve-then-read approaches generally digest the
retrieved documents, without considering the timeliness, authoritativeness and
commonality of retrieval. We argue that these approaches can be suboptimal,
especially in real-world applications where information from different sources
might conflict with each other and even information from the same source in
different time scale might be different, and totally relying on this would
deteriorate the performance of RAG approaches. We propose PolyRAG that
carefully incorporate judges from different perspectives and finally integrate
the polyviews for retrieval augmented generation in medical applications. Due
to the scarcity of real-world benchmarks for evaluation, to bridge the gap we
propose PolyEVAL, a benchmark consists of queries and documents collected from
real-world medical scenarios (including medical policy, hospital & doctor
inquiry and healthcare) with multiple tagging (e.g., timeliness,
authoritativeness) on them. Extensive experiments and analysis on PolyEVAL have
demonstrated the superiority of PolyRAG.

</details>

### [128] [Causal DAG Summarization (Full Version)](https://arxiv.org/abs/2504.14937)
*Anna Zeng,Michael Cafarella,Batya Kenig,Markos Markakis,Brit Youngmann,Babak Salimi*

Main category: cs.LG

TLDR: 本文提出了一种因果图摘要方法，通过简化因果DAG以提高可理解性，同时保留关键因果信息，确保推理的可靠性。


<details>
  <summary>Details</summary>
Motivation: 高维数据的因果DAG复杂且难以验证，现有图摘要方法不适用于因果DAG摘要，导致因果推理不可靠。

Method: 提出一种因果图摘要目标，开发高效贪心算法，生成摘要DAG用于直接推理。

Result: 实验表明，该方法在高维数据中表现优异，生成的摘要DAG既可靠又对假设误设具有鲁棒性。

Conclusion: 该方法解决了因果DAG复杂性问题，提升了因果推理的鲁棒性和可靠性。

Abstract: Causal inference aids researchers in discovering cause-and-effect
relationships, leading to scientific insights. Accurate causal estimation
requires identifying confounding variables to avoid false discoveries. Pearl's
causal model uses causal DAGs to identify confounding variables, but incorrect
DAGs can lead to unreliable causal conclusions. However, for high dimensional
data, the causal DAGs are often complex beyond human verifiability. Graph
summarization is a logical next step, but current methods for general-purpose
graph summarization are inadequate for causal DAG summarization. This paper
addresses these challenges by proposing a causal graph summarization objective
that balances graph simplification for better understanding while retaining
essential causal information for reliable inference. We develop an efficient
greedy algorithm and show that summary causal DAGs can be directly used for
inference and are more robust to misspecification of assumptions, enhancing
robustness for causal inference. Experimenting with six real-life datasets, we
compared our algorithm to three existing solutions, showing its effectiveness
in handling high-dimensional data and its ability to generate summary DAGs that
ensure both reliable causal inference and robustness against misspecifications.

</details>

### [129] [Learning to Reason under Off-Policy Guidance](https://arxiv.org/abs/2504.14945)
*Jianhao Yan,Yafu Li,Zican Hu,Zhi Wang,Ganqu Cui,Xiaoye Qu,Yu Cheng,Yue Zhang*

Main category: cs.LG

TLDR: LUFFY框架通过结合离策略推理轨迹和正则化重要性采样，显著提升了推理模型的性能，尤其在泛化能力上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有零强化学习方法局限于自身输出，无法超越初始能力，需要一种新方法来提升推理能力。

Method: LUFFY结合离策略演示和策略内训练，通过正则化重要性采样避免浅层模仿。

Result: 在六个数学基准上平均提升7.0分，在分布外任务上优势达6.2分，显著优于监督微调。

Conclusion: LUFFY为训练具有泛化能力的推理模型提供了一条可扩展的路径。

Abstract: Recent advances in large reasoning models (LRMs) demonstrate that
sophisticated behaviors such as multi-step reasoning and self-reflection can
emerge via reinforcement learning (RL) with simple rule-based rewards. However,
existing zero-RL approaches are inherently ``on-policy'', limiting learning to
a model's own outputs and failing to acquire reasoning abilities beyond its
initial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY
guidance), a framework that augments zero-RL with off-policy reasoning traces.
LUFFY dynamically balances imitation and exploration by combining off-policy
demonstrations with on-policy rollouts during training. Notably, we propose
policy shaping via regularized importance sampling to avoid superficial and
rigid imitation during mixed-policy training. Remarkably, LUFFY achieves an
over +7.0 average gain across six math benchmarks and an advantage of over +6.2
points in out-of-distribution tasks. It also substantially surpasses
imitation-based supervised fine-tuning (SFT), particularly in generalization.
Analysis shows LUFFY not only imitates effectively but also explores beyond
demonstrations, offering a scalable path to train generalizable reasoning
models with off-policy guidance.

</details>

### [130] [Symmetry-Preserving Architecture for Multi-NUMA Environments (SPANE): A Deep Reinforcement Learning Approach for Dynamic VM Scheduling](https://arxiv.org/abs/2504.14946)
*Tin Ping Chan,Yunlong Cheng,Yizhan Zhu,Xiaofeng Gao,Guihai Chen*

Main category: cs.LG

TLDR: 论文提出动态虚拟机分配问题（DVAMP）及其解决方案SPANE，通过深度强化学习优化多NUMA环境下的虚拟机调度，实验显示性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 多NUMA架构在云计算中的普及带来了虚拟机调度的新挑战，需要更精确的解决方案。

Method: 定义DVAMP为混合整数线性规划问题，提出基于对称性保护的深度强化学习方法SPANE。

Result: SPANE在实验中平均减少虚拟机等待时间45%，优于现有基线方法。

Conclusion: 研究填补了多NUMA环境下虚拟机调度的理论空白，提供了高效的实际解决方案。

Abstract: As cloud computing continues to evolve, the adoption of multi-NUMA
(Non-Uniform Memory Access) architecture by cloud service providers has
introduced new challenges in virtual machine (VM) scheduling. To address these
challenges and more accurately reflect the complexities faced by modern cloud
environments, we introduce the Dynamic VM Allocation problem in Multi-NUMA PM
(DVAMP). We formally define both offline and online versions of DVAMP as
mixed-integer linear programming problems, providing a rigorous mathematical
foundation for analysis. A tight performance bound for greedy online algorithms
is derived, offering insights into the worst-case optimality gap as a function
of the number of physical machines and VM lifetime variability. To address the
challenges posed by DVAMP, we propose SPANE (Symmetry-Preserving Architecture
for Multi-NUMA Environments), a novel deep reinforcement learning approach that
exploits the problem's inherent symmetries. SPANE produces invariant results
under arbitrary permutations of physical machine states, enhancing learning
efficiency and solution quality. Extensive experiments conducted on the
Huawei-East-1 dataset demonstrate that SPANE outperforms existing baselines,
reducing average VM wait time by 45%. Our work contributes to the field of
cloud resource management by providing both theoretical insights and practical
solutions for VM scheduling in multi-NUMA environments, addressing a critical
gap in the literature and offering improved performance for real-world cloud
systems.

</details>

### [131] [Efficient Document Retrieval with G-Retriever](https://arxiv.org/abs/2504.14955)
*Manthankumar Solanki*

Main category: cs.LG

TLDR: 本文提出了一种改进的基于注意力的子图构建方法，替代了原有的PCST方法，并结合节点和边属性编码，提升了文本问答的准确性和上下文理解。


<details>
  <summary>Details</summary>
Motivation: 现有的基于PCST优化的方法仅关注节点属性，导致上下文理解不完整，因此需要一种更高效且上下文感知的检索方法。

Method: 采用基于注意力的子图构建技术，编码节点和边属性，并引入改进的投影层和多头注意力池化，以更好地与LLMs对齐。

Result: 在WebQSP数据集上的实验表明，该方法性能优于原方法，实现了更准确的问答效果。

Conclusion: 提出的方法通过增强子图构建和属性编码，显著提升了问答系统的性能，展示了其在文本问答中的潜力。

Abstract: Textual data question answering has gained significant attention due to its
growing applicability. Recently, a novel approach leveraging the
Retrieval-Augmented Generation (RAG) method was introduced, utilizing the
Prize-Collecting Steiner Tree (PCST) optimization for sub-graph construction.
However, this method focused solely on node attributes, leading to incomplete
contextual understanding. In this paper, we propose an enhanced approach that
replaces the PCST method with an attention-based sub-graph construction
technique, enabling more efficient and context-aware retrieval. Additionally,
we encode both node and edge attributes, leading to richer graph
representations. Our method also incorporates an improved projection layer and
multi-head attention pooling for better alignment with Large Language Models
(LLMs). Experimental evaluations on the WebQSP dataset demonstrate that our
approach is competitive and achieves marginally better results compared to the
original method, underscoring its potential for more accurate question
answering.

</details>

### [132] [MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core](https://arxiv.org/abs/2504.14960)
*Dennis Liu,Zijie Yan,Xin Yao,Tong Liu,Vijay Korthikanti,Evan Wu,Shiqing Fan,Gao Deng,Hongxiao Bai,Ashwath Aithal,Michael Andersch,Mohammad Shoeybi,Jiajie Yao,Chandler Zhou,David Wu,Xipeng Li,June Yang*

Main category: cs.LG

TLDR: 提出了一种用于大规模MoE模型的端到端训练框架，通过五维混合并行策略和MoE并行折叠技术，显著提升了训练效率和扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有并行策略在大规模MoE模型训练中存在局限性，亟需一种高效且灵活的解决方案。

Method: 采用五维混合并行（张量、专家、上下文、数据和流水线并行）和MoE并行折叠技术，结合动态令牌分发器。

Result: 在H100 GPU上，Mixtral 8x22B和Qwen2-57B-A14B模型的MFU分别达到49.3%和39.0%，支持128K令牌序列长度和1024 GPU扩展。

Conclusion: 该框架为大规模MoE模型训练提供了高效且可扩展的解决方案，代码已开源。

Abstract: Mixture of Experts (MoE) models enhance neural network scalability by
dynamically selecting relevant experts per input token, enabling larger model
sizes while maintaining manageable computation costs. However, efficient
training of large-scale MoE models across thousands of GPUs presents
significant challenges due to limitations in existing parallelism strategies.
We introduce an end-to-end training framework for large-scale MoE models that
utilizes five-dimensional hybrid parallelism: Tensor Parallelism, Expert
Parallelism, Context Parallelism, Data Parallelism, and Pipeline Parallelism.
Central to our approach is MoE Parallel Folding, a novel strategy that
decouples the parallelization of attention and MoE layers in Transformer
models, allowing each layer type to adopt optimal parallel configurations.
Additionally, we develop a flexible token-level dispatcher that supports both
token-dropping and token-dropless MoE training across all five dimensions of
parallelism. This dispatcher accommodates dynamic tensor shapes and coordinates
different parallelism schemes for Attention and MoE layers, facilitating
complex parallelism implementations. Our experiments demonstrate significant
improvements in training efficiency and scalability. We achieve up to 49.3%
Model Flops Utilization (MFU) for the Mixtral 8x22B model and 39.0% MFU for the
Qwen2-57B-A14B model on H100 GPUs, outperforming existing methods. The
framework scales efficiently up to 1,024 GPUs and maintains high performance
with sequence lengths up to 128K tokens, validating its effectiveness for
large-scale MoE model training. The code is available in Megatron-Core.

</details>

### [133] [Learning Compositional Transferability of Time Series for Source-Free Domain Adaptation](https://arxiv.org/abs/2504.14994)
*Hankang Sun,Guiming Li,Su Yang,Baoqi Li*

Main category: cs.LG

TLDR: 本文提出了一种用于时间序列分类的源自由域自适应方法，通过解耦域可转移性并使用组合架构进行时间序列重建，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决源自由域自适应问题，即在目标标签和源数据均不可访问的情况下，如何利用预训练的分类主干进行时间序列分类。

Method: 采用组合架构，包括冻结的U-net和两个并行分支（源重放分支和偏移补偿分支），通过可学习因子解耦域可转移性。

Result: 在三个广泛使用的基准测试中实现了SOTA性能。

Conclusion: 组合架构有效解耦了域可转移性，保留了源数据的重建能力，并适应了域变化的时序模式。

Abstract: Domain adaptation is challenging for time series classification due to the
highly dynamic nature. This study tackles the most difficult subtask when both
target labels and source data are inaccessible, namely, source-free domain
adaptation. To reuse the classification backbone pre-trained on source data,
time series reconstruction is a sound solution that aligns target and source
time series by minimizing the reconstruction errors of both. However, simply
fine-tuning the source pre-trained reconstruction model on target data may lose
the learnt priori, and it struggles to accommodate domain varying temporal
patterns in a single encoder-decoder. Therefore, this paper tries to
disentangle the composition of domain transferability by using a compositional
architecture for time series reconstruction. Here, the preceding component is a
U-net frozen since pre-trained, the output of which during adaptation is the
initial reconstruction of a given target time series, acting as a coarse step
to prompt the subsequent finer adaptation. The following pipeline for finer
adaptation includes two parallel branches: The source replay branch using a
residual link to preserve the output of U-net, and the offset compensation
branch that applies an additional autoencoder (AE) to further warp U-net's
output. By deploying a learnable factor on either branch to scale their
composition in the final output of reconstruction, the data transferability is
disentangled and the learnt reconstructive capability from source data is
retained. During inference, aside from the batch-level optimization in the
training, we search at test time stability-aware rescaling of source replay
branch to tolerate instance-wise variation. The experimental results show that
such compositional architecture of time series reconstruction leads to SOTA
performance on 3 widely used benchmarks.

</details>

### [134] [A Call for New Recipes to Enhance Spatial Reasoning in MLLMs](https://arxiv.org/abs/2504.15037)
*Huanyu Zhang,Chengzu Li,Wenshan Wu,Shaoguang Mao,Yan xia,Ivan Vulić,Zhang Zhang,Liang Wang,Tieniu Tan,Furu Wei*

Main category: cs.LG

TLDR: 多模态大语言模型（MLLMs）在空间推理能力上存在显著不足，限制了其实际应用。本文提出需改进现有方法，并探讨了提升空间推理的途径。


<details>
  <summary>Details</summary>
Motivation: MLLMs在空间推理上的不足限制了其与物理世界的交互能力，需通过改进方法解决。

Method: 建立空间推理框架，分析现有方法中训练数据和推理机制的影响。

Result: 揭示了现有方法的局限性，并提出了改进方向。

Conclusion: 呼吁关注空间推理问题，推动MLLMs实现类人能力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
performance in general vision-language tasks. However, recent studies have
exposed critical limitations in their spatial reasoning capabilities. This
deficiency in spatial reasoning significantly constrains MLLMs' ability to
interact effectively with the physical world, thereby limiting their broader
applications. We argue that spatial reasoning capabilities will not naturally
emerge from merely scaling existing architectures and training methodologies.
Instead, this challenge demands dedicated attention to fundamental
modifications in the current MLLM development approach. In this position paper,
we first establish a comprehensive framework for spatial reasoning within the
context of MLLMs. We then elaborate on its pivotal role in real-world
applications. Through systematic analysis, we examine how individual components
of the current methodology-from training data to reasoning mechanisms-influence
spatial reasoning capabilities. This examination reveals critical limitations
while simultaneously identifying promising avenues for advancement. Our work
aims to direct the AI research community's attention toward these crucial yet
underexplored aspects. By highlighting these challenges and opportunities, we
seek to catalyze progress toward achieving human-like spatial reasoning
capabilities in MLLMs.

</details>

### [135] [VeLU: Variance-enhanced Learning Unit for Deep Neural Networks](https://arxiv.org/abs/2504.15051)
*Ashkan Shakarami,Yousef Yeganeh,Azade Farshad,Lorenzo Nicolè,Stefano Ghidoni,Nassir Navab*

Main category: cs.LG

TLDR: VeLU是一种基于输入方差动态调整的激活函数，通过ArcTan-Sin变换和Wasserstein-2正则化，解决了ReLU及其变体的局限性，在多个视觉任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: ReLU及其变体在梯度消失和动态适应性方面存在不足，需要一种能动态适应输入统计的激活函数。

Method: 提出VeLU，结合ArcTan-Sin变换和Wasserstein-2正则化，动态调整输入方差。

Result: 在ViT_B16等模型和六个视觉基准测试中，VeLU优于ReLU、Swish和GELU。

Conclusion: VeLU通过动态调整输入方差，显著提升了优化稳定性和性能。

Abstract: Activation functions are fundamental in deep neural networks and directly
impact gradient flow, optimization stability, and generalization. Although ReLU
remains standard because of its simplicity, it suffers from vanishing gradients
and lacks adaptability. Alternatives like Swish and GELU introduce smooth
transitions, but fail to dynamically adjust to input statistics. We propose
VeLU, a Variance-enhanced Learning Unit as an activation function that
dynamically scales based on input variance by integrating ArcTan-Sin
transformations and Wasserstein-2 regularization, effectively mitigating
covariate shifts and stabilizing optimization. Extensive experiments on
ViT_B16, VGG19, ResNet50, DenseNet121, MobileNetV2, and EfficientNetB3 confirm
VeLU's superiority over ReLU, ReLU6, Swish, and GELU on six vision benchmarks.
The codes of VeLU are publicly available on GitHub.

</details>

### [136] [Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL](https://arxiv.org/abs/2504.15077)
*Simone Papicchio,Simone Rossi,Luca Cagliero,Paolo Papotti*

Main category: cs.LG

TLDR: 研究探讨了不同LLM训练策略对Text2SQL任务性能的影响，发现小模型通过SFT+RL策略在复杂任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在Text2SQL任务中表现突出，但小模型在零样本学习和复杂查询中表现不佳，需探索如何通过推理能力提升性能。

Method: 比较了四种LLM设置：零样本学习（ZSL）、监督微调（SFT）、强化学习（RL）及SFT+RL组合，测试其在四个基准数据集上的表现。

Result: ZSL中的通用推理对复杂任务无效；小模型通过SFT+RL表现优异，甚至媲美大模型；RL在多表和多跳推理任务中效果显著。

Conclusion: 推理能力对Text2SQL性能至关重要，小模型通过SFT+RL策略在复杂任务中表现突出，RL在多表和多跳推理中尤为有效。

Abstract: Large Language Models (LLMs) have shown impressive capabilities in
transforming natural language questions about relational databases into SQL
queries. Despite recent improvements, small LLMs struggle to handle questions
involving multiple tables and complex SQL patterns under a Zero-Shot Learning
(ZSL) setting. Supervised Fine-Tuning (SFT) partially compensate the knowledge
deficits in pretrained models but falls short while dealing with queries
involving multi-hop reasoning. To bridge this gap, different LLM training
strategies to reinforce reasoning capabilities have been proposed, ranging from
leveraging a thinking process within ZSL, including reasoning traces in SFT, or
adopt Reinforcement Learning (RL) strategies. However, the influence of
reasoning on Text2SQL performance is still largely unexplored. This paper
investigates to what extent LLM reasoning capabilities influence their Text2SQL
performance on four benchmark datasets. To this end, it considers the following
LLM settings: (1) ZSL, including general-purpose reasoning or not; (2) SFT,
with and without task-specific reasoning traces; (3) RL, leveraging execution
accuracy as primary reward function; (4) SFT+RL, i.e, a two-stage approach that
combines SFT and RL. The results show that general-purpose reasoning under ZSL
proves to be ineffective in tackling complex Text2SQL cases. Small LLMs benefit
from SFT with reasoning much more than larger ones, bridging the gap of their
(weaker) model pretraining. RL is generally beneficial across all tested models
and datasets, particularly when SQL queries involve multi-hop reasoning and
multiple tables. Small LLMs with SFT+RL excel on most complex datasets thanks
to a strategic balance between generality of the reasoning process and
optimization of the execution accuracy. Thanks to RL, the7B Qwen-Coder-2.5
model performs on par with 100+ Billion ones on the Bird dataset.

</details>

### [137] [Federated Latent Factor Model for Bias-Aware Recommendation with Privacy-Preserving](https://arxiv.org/abs/2504.15090)
*Junxiang Gao,Yixin Ran,Jia Chen*

Main category: cs.LG

TLDR: 论文提出了一种联邦学习框架下的推荐系统FBALF，通过显式处理评分偏差，解决了隐私保护与推荐准确性之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统在集中式服务器上处理用户数据，存在隐私泄露风险；联邦学习虽保护隐私，但无法直接处理评分偏差。

Method: 提出FBALF模型，将评分偏差显式纳入本地模型的损失函数，无需访问原始数据。

Result: 在三个真实数据集上实验表明，FBALF显著优于其他联邦推荐系统。

Conclusion: FBALF在保护隐私的同时有效提升推荐准确性，解决了联邦推荐系统中的评分偏差问题。

Abstract: A recommender system (RS) aims to provide users with personalized item
recommendations, enhancing their overall experience. Traditional RSs collect
and process all user data on a central server. However, this centralized
approach raises significant privacy concerns, as it increases the risk of data
breaches and privacy leakages, which are becoming increasingly unacceptable to
privacy-sensitive users. To address these privacy challenges, federated
learning has been integrated into RSs, ensuring that user data remains secure.
In centralized RSs, the issue of rating bias is effectively addressed by
jointly analyzing all users' raw interaction data. However, this becomes a
significant challenge in federated RSs, as raw data is no longer accessible due
to privacy-preserving constraints. To overcome this problem, we propose a
Federated Bias-Aware Latent Factor (FBALF) model. In FBALF, training bias is
explicitly incorporated into every local model's loss function, allowing for
the effective elimination of rating bias without compromising data privacy.
Extensive experiments conducted on three real-world datasets demonstrate that
FBALF achieves significantly higher recommendation accuracy compared to other
state-of-the-art federated RSs.

</details>

### [138] [Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training of GAN](https://arxiv.org/abs/2504.15099)
*Lin Wang,Xiancheng Wang,Rui Wang,Zhibo Zhang,Minghang Zhao*

Main category: cs.LG

TLDR: 本文提出了一种名为FSCO的新型智能优化器，通过强化学习控制GAN训练中的步长，提升训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统GAN训练对数据和超参数敏感，易导致振荡或收敛困难，尤其在数据方差较大时。

Method: 采用强化学习设计FSCO优化器，动态调整训练步长和学习率，减少对步长的敏感性。

Result: 在三个基准数据集上验证了FSCO的有效性。

Conclusion: FSCO显著提升了GAN训练的稳定性和智能性。

Abstract: Up to now, the training processes of typical Generative Adversarial Networks
(GANs) are still particularly sensitive to data properties and hyperparameters,
which may lead to severe oscillations, difficulties in convergence, or even
failures to converge, especially when the overall variances of the training
sets are large. These phenomena are often attributed to the training
characteristics of such networks. Aiming at the problem, this paper develops a
new intelligent optimizer, Fast-Slow Co-advancing Optimizer (FSCO), which
employs reinforcement learning in the training process of GANs to make training
easier. Specifically, this paper allows the training step size to be controlled
by an agent to improve training stability, and makes the training process more
intelligent with variable learning rates, making GANs less sensitive to step
size. Experiments have been conducted on three benchmark datasets to verify the
effectiveness of the developed FSCO.

</details>

### [139] [Kolmogorov-Arnold Networks: Approximation and Learning Guarantees for Functions and their Derivatives](https://arxiv.org/abs/2504.15110)
*Anastasis Kratsios,Takashi Furuya*

Main category: cs.LG

TLDR: Kolmogorov-Arnold Networks (KANs) 是一种改进的深度学习框架，通过可训练的样条激活函数优于多层感知机（MLP）。本文证明了 KAN 在 Besov 空间中的最优逼近能力，并提供了无维度的样本复杂度估计。


<details>
  <summary>Details</summary>
Motivation: 受 Kolmogorov-Arnold 叠加定理启发，KANs 旨在提供比 MLP 更强的适应性，尤其是在逼近 Besov 函数时。

Method: 通过理论分析，证明 KAN 在 Besov 空间中的最优逼近能力，并利用残差连接优化架构。

Result: KAN 能够在有界或分形域上以最优速率逼近 Besov 函数，且样本复杂度与维度无关。

Conclusion: KAN 是一种理论支持的高效深度学习架构，适用于复杂函数逼近任务。

Abstract: Inspired by the Kolmogorov-Arnold superposition theorem, Kolmogorov-Arnold
Networks (KANs) have recently emerged as an improved backbone for most deep
learning frameworks, promising more adaptivity than their multilayer perception
(MLP) predecessor by allowing for trainable spline-based activation functions.
In this paper, we probe the theoretical foundations of the KAN architecture by
showing that it can optimally approximate any Besov function in
$B^{s}_{p,q}(\mathcal{X})$ on a bounded open, or even fractal, domain
$\mathcal{X}$ in $\mathbb{R}^d$ at the optimal approximation rate with respect
to any weaker Besov norm $B^{\alpha}_{p,q}(\mathcal{X})$; where $\alpha < s$.
We complement our approximation guarantee with a dimension-free estimate on the
sample complexity of a residual KAN model when learning a function of Besov
regularity from $N$ i.i.d. noiseless samples. Our KAN architecture incorporates
contemporary deep learning wisdom by leveraging residual/skip connections
between layers.

</details>

### [140] [Survey of Loss Augmented Knowledge Tracing](https://arxiv.org/abs/2504.15163)
*Altun Shukurlu*

Main category: cs.LG

TLDR: 论文探讨了损失函数在人工神经网络训练中的重要性，并综述了深度学习知识追踪（DKT）算法的改进，特别是对比学习技术的应用。


<details>
  <summary>Details</summary>
Motivation: 解决数据质量不足或学习效率低下导致的模型性能问题，通过改进损失函数提升知识追踪算法的效果。

Method: 综述了基于深度学习的知识追踪算法，重点分析了对比学习技术（如Bi-CLKT、CL4KT等）及其损失函数的优化。

Result: 展示了对比学习技术在知识追踪中的性能提升，并讨论了实际部署中的挑战。

Conclusion: 未来研究方向包括混合损失策略和上下文感知建模，以进一步提升知识追踪算法的性能。

Abstract: The training of artificial neural networks is heavily dependent on the
careful selection of an appropriate loss function. While commonly used loss
functions, such as cross-entropy and mean squared error (MSE), generally
suffice for a broad range of tasks, challenges often emerge due to limitations
in data quality or inefficiencies within the learning process. In such
circumstances, the integration of supplementary terms into the loss function
can serve to address these challenges, enhancing both model performance and
robustness. Two prominent techniques, loss regularization and contrastive
learning, have been identified as effective strategies for augmenting the
capacity of loss functions in artificial neural networks.
  Knowledge tracing is a compelling area of research that leverages predictive
artificial intelligence to facilitate the automation of personalized and
efficient educational experiences for students. In this paper, we provide a
comprehensive review of the deep learning-based knowledge tracing (DKT)
algorithms trained using advanced loss functions and discuss their improvements
over prior techniques. We discuss contrastive knowledge tracing algorithms,
such as Bi-CLKT, CL4KT, SP-CLKT, CoSKT, and prediction-consistent DKT,
providing performance benchmarks and insights into real-world deployment
challenges. The survey concludes with future research directions, including
hybrid loss strategies and context-aware modeling.

</details>

### [141] [Audio-Visual Class-Incremental Learning for Fish Feeding intensity Assessment in Aquaculture](https://arxiv.org/abs/2504.15171)
*Meng Cui,Xianghu Yue,Xinyuan Qian,Jinzheng Zhao,Haohe Liu,Xubo Liu,Daoliang Li,Wenwu Wang*

Main category: cs.LG

TLDR: 论文提出了一种新的音频-视觉类增量学习框架HAIL-FFIA，用于鱼类摄食强度评估（FFIA），解决了现有方法在新鱼种或环境适应中的挑战。


<details>
  <summary>Details</summary>
Motivation: 多模态方法在FFIA中表现良好，但面临灾难性遗忘和数据集不足的问题。

Method: 引入AV-CIL-FFIA数据集，并提出HAIL-FFIA框架，采用原型表示和动态模态平衡系统。

Result: HAIL-FFIA在AV-CIL-FFIA数据集上表现优于现有方法，准确率更高且存储需求更低。

Conclusion: HAIL-FFIA有效解决了灾难性遗忘问题，提升了FFIA的适应性和效率。

Abstract: Fish Feeding Intensity Assessment (FFIA) is crucial in industrial aquaculture
management. Recent multi-modal approaches have shown promise in improving FFIA
robustness and efficiency. However, these methods face significant challenges
when adapting to new fish species or environments due to catastrophic
forgetting and the lack of suitable datasets. To address these limitations, we
first introduce AV-CIL-FFIA, a new dataset comprising 81,932 labelled
audio-visual clips capturing feeding intensities across six different fish
species in real aquaculture environments. Then, we pioneer audio-visual class
incremental learning (CIL) for FFIA and demonstrate through benchmarking on
AV-CIL-FFIA that it significantly outperforms single-modality methods. Existing
CIL methods rely heavily on historical data. Exemplar-based approaches store
raw samples, creating storage challenges, while exemplar-free methods avoid
data storage but struggle to distinguish subtle feeding intensity variations
across different fish species. To overcome these limitations, we introduce
HAIL-FFIA, a novel audio-visual class-incremental learning framework that
bridges this gap with a prototype-based approach that achieves exemplar-free
efficiency while preserving essential knowledge through compact feature
representations. Specifically, HAIL-FFIA employs hierarchical representation
learning with a dual-path knowledge preservation mechanism that separates
general intensity knowledge from fish-specific characteristics. Additionally,
it features a dynamic modality balancing system that adaptively adjusts the
importance of audio versus visual information based on feeding behaviour
stages. Experimental results show that HAIL-FFIA is superior to SOTA methods on
AV-CIL-FFIA, achieving higher accuracy with lower storage needs while
effectively mitigating catastrophic forgetting in incremental fish species
learning.

</details>

### [142] [How Global Calibration Strengthens Multiaccuracy](https://arxiv.org/abs/2504.15206)
*Sílvia Casacuberta,Parikshit Gopalan,Varun Kanade,Omer Reingold*

Main category: cs.LG

TLDR: 多准确性和多校准是多组公平性概念，通过弱不可知学习实现。研究发现多准确性本身较弱，但结合全局校准（校准多准确性）能显著提升其能力，甚至恢复多校准的强学习能力。


<details>
  <summary>Details</summary>
Motivation: 探讨多准确性作为学习原语的能力，及其与校准结合的效果，以理解其在多组公平性中的作用。

Method: 分析多准确性和校准多准确性在不同场景下的表现，包括弱不可知学习和硬核度量生成。

Result: 多准确性单独使用时能力有限，但结合校准后能实现强不可知学习，并生成最优密度的硬核度量。

Conclusion: 多准确性和校准在多组公平性中具有互补作用，结合后能显著提升学习能力。

Abstract: Multiaccuracy and multicalibration are multigroup fairness notions for
prediction that have found numerous applications in learning and computational
complexity. They can be achieved from a single learning primitive: weak
agnostic learning. Here we investigate the power of multiaccuracy as a learning
primitive, both with and without the additional assumption of calibration. We
find that multiaccuracy in itself is rather weak, but that the addition of
global calibration (this notion is called calibrated multiaccuracy) boosts its
power substantially, enough to recover implications that were previously known
only assuming the stronger notion of multicalibration.
  We give evidence that multiaccuracy might not be as powerful as standard weak
agnostic learning, by showing that there is no way to post-process a
multiaccurate predictor to get a weak learner, even assuming the best
hypothesis has correlation $1/2$. Rather, we show that it yields a restricted
form of weak agnostic learning, which requires some concept in the class to
have correlation greater than $1/2$ with the labels. However, by also requiring
the predictor to be calibrated, we recover not just weak, but strong agnostic
learning.
  A similar picture emerges when we consider the derivation of hardcore
measures from predictors satisfying multigroup fairness notions. On the one
hand, while multiaccuracy only yields hardcore measures of density half the
optimal, we show that (a weighted version of) calibrated multiaccuracy achieves
optimal density.
  Our results yield new insights into the complementary roles played by
multiaccuracy and calibration in each setting. They shed light on why
multiaccuracy and global calibration, although not particularly powerful by
themselves, together yield considerably stronger notions.

</details>

### [143] [Compute-Optimal LLMs Provably Generalize Better With Scale](https://arxiv.org/abs/2504.15208)
*Marc Finzi,Sanyam Kapoor,Diego Granziol,Anming Gu,Christopher De Sa,J. Zico Kolter,Andrew Gordon Wilson*

Main category: cs.LG

TLDR: 论文研究了为什么更大的语言模型泛化能力更强，通过开发计算最优状态下的大语言模型预训练目标的泛化边界，并引入新的经验性不等式来改进现有边界。


<details>
  <summary>Details</summary>
Motivation: 探讨大规模语言模型泛化能力提升的原因，特别是在计算最优状态下。

Method: 开发了一种新的经验性Freedman-type不等式，考虑了损失函数的方差，并将泛化边界分解为三个可解释的组成部分。

Result: 发现随着模型规模扩大，损失方差和量化误差减小，泛化间隙变小。

Conclusion: 提出了泛化间隙的缩放规律，证明模型规模越大，泛化能力越强。

Abstract: Why do larger language models generalize better? To investigate this
question, we develop generalization bounds on the pretraining objective of
large language models (LLMs) in the compute-optimal regime, as described by the
Chinchilla scaling laws. We introduce a novel, fully empirical Freedman-type
martingale concentration inequality that tightens existing bounds by accounting
for the variance of the loss function. This generalization bound can be
decomposed into three interpretable components: the number of parameters per
token, the loss variance, and the quantization error at a fixed bitrate. As
compute-optimal language models are scaled up, the number of parameters per
data point remains constant; however, both the loss variance and the
quantization error decrease, implying that larger models should have smaller
generalization gaps. We examine why larger models tend to be more quantizable
from an information theoretic perspective, showing that the rate at which they
can integrate new information grows more slowly than their capacity on the
compute-optimal frontier. From these findings we produce a scaling law for the
generalization gap, with bounds that become predictably stronger with scale.

</details>

### [144] [A Causal Convolutional Low-rank Representation Model for Imputation of Water Quality Data](https://arxiv.org/abs/2504.15209)
*Xin Liao,Bing Yang,Tan Dongli,Cai Yu*

Main category: cs.LG

TLDR: 本文提出了一种因果卷积低秩表示（CLR）模型，用于填补水质监测数据中的缺失值，以提高数据的完整性和准确性。


<details>
  <summary>Details</summary>
Motivation: 水质监测数据常因设备故障等原因出现缺失值，简单填充方法会导致结果不准确，影响环境监测措施的实施。

Method: CLR模型结合因果卷积操作考虑时间依赖性，并通过超参数自适应方案自动调整最佳参数。

Result: 在三个真实水质数据集上的实验表明，CLR模型在填补准确性和时间成本上优于现有先进模型。

Conclusion: CLR模型为环境监测提供了更可靠的决策支持。

Abstract: The monitoring of water quality is a crucial part of environmental
protection, and a large number of monitors are widely deployed to monitor water
quality. Due to unavoidable factors such as data acquisition breakdowns,
sensors and communication failures, water quality monitoring data suffers from
missing values over time, resulting in High-Dimensional and Sparse (HDS) Water
Quality Data (WQD). The simple and rough filling of the missing values leads to
inaccurate results and affects the implementation of relevant measures.
Therefore, this paper proposes a Causal convolutional Low-rank Representation
(CLR) model for imputing missing WQD to improve the completeness of the WQD,
which employs a two-fold idea: a) applying causal convolutional operation to
consider the temporal dependence of the low-rank representation, thus
incorporating temporal information to improve the imputation accuracy; and b)
implementing a hyperparameters adaptation scheme to automatically adjust the
best hyperparameters during model training, thereby reducing the tedious manual
adjustment of hyper-parameters. Experimental studies on three real-world water
quality datasets demonstrate that the proposed CLR model is superior to some of
the existing state-of-the-art imputation models in terms of imputation accuracy
and time cost, as well as indicating that the proposed model provides more
reliable decision support for environmental monitoring.

</details>

### [145] [Histogram-based Parameter-efficient Tuning for Passive Sonar Classification](https://arxiv.org/abs/2504.15214)
*Amirmohammad Mohammadi,Davelle Carreiro,Alexandra Van Dine,Joshua Peeples*

Main category: cs.LG

TLDR: HPT是一种基于直方图的高效参数调优技术，优于传统适配器方法，在资源受限环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效迁移学习方法（如适配器）难以捕捉中间特征嵌入的分布变化，需改进。

Method: 提出HPT技术，通过统计目标域数据并调制嵌入，实现高效调优。

Result: 在三个被动声纳数据集上，HPT准确率更高（如VTUAD上91.8% vs. 89.8%），训练更快且特征更接近全微调模型。

Conclusion: HPT在参数节约与性能间取得平衡，为资源受限环境提供了分布感知的高效迁移学习方案。

Abstract: Parameter-efficient transfer learning (PETL) methods adapt large artificial
neural networks to downstream tasks without fine-tuning the entire model.
However, existing additive methods, such as adapters, sometimes struggle to
capture distributional shifts in intermediate feature embeddings. We propose a
novel histogram-based parameter-efficient tuning (HPT) technique that captures
the statistics of the target domain and modulates the embeddings. Experimental
results on three downstream passive sonar datasets (ShipsEar, DeepShip, VTUAD)
demonstrate that HPT outperforms conventional adapters. Notably, HPT achieves
91.8% vs. 89.8% accuracy on VTUAD. Furthermore, HPT trains faster and yields
feature representations closer to those of fully fine-tuned models. Overall,
HPT balances parameter savings and performance, providing a distribution-aware
alternative to existing adapters and shows a promising direction for scalable
transfer learning in resource-constrained environments. The code is publicly
available:
https://github.com/Advanced-Vision-and-Learning-Lab/HLAST_DeepShip_ParameterEfficient.

</details>

### [146] [A Deep Learning Framework for Sequence Mining with Bidirectional LSTM and Multi-Scale Attention](https://arxiv.org/abs/2504.15223)
*Tao Yang,Yu Cheng,Yaokun Ren,Yujia Lou,Minggu Wei,Honghui Xin*

Main category: cs.LG

TLDR: 提出一种结合BiLSTM和多尺度注意力机制的序列模式挖掘算法，用于复杂序列数据中的潜在模式挖掘和上下文依赖建模。


<details>
  <summary>Details</summary>
Motivation: 解决复杂序列数据中潜在模式挖掘和上下文依赖建模的挑战。

Method: 集成双向长短期记忆网络（BiLSTM）和多尺度注意力机制，BiLSTM捕捉序列的前后依赖，多尺度注意力模块自适应分配权重。

Result: 在公开的多变量时间序列数据集上实验，模型在准确性、精确率和召回率上优于现有方法。

Conclusion: 验证了所提架构在复杂模式识别任务中的有效性和鲁棒性，并通过消融实验和敏感性分析支持模型结构优化。

Abstract: This paper addresses the challenges of mining latent patterns and modeling
contextual dependencies in complex sequence data. A sequence pattern mining
algorithm is proposed by integrating Bidirectional Long Short-Term Memory
(BiLSTM) with a multi-scale attention mechanism. The BiLSTM captures both
forward and backward dependencies in sequences, enhancing the model's ability
to perceive global contextual structures. At the same time, the multi-scale
attention module assigns adaptive weights to key feature regions under
different window sizes. This improves the model's responsiveness to both local
and global important information. Extensive experiments are conducted on a
publicly available multivariate time series dataset. The proposed model is
compared with several mainstream sequence modeling methods. Results show that
it outperforms existing models in terms of accuracy, precision, and recall.
This confirms the effectiveness and robustness of the proposed architecture in
complex pattern recognition tasks. Further ablation studies and sensitivity
analyses are carried out to investigate the effects of attention scale and
input sequence length on model performance. These results provide empirical
support for structural optimization of the model.

</details>

### [147] [M$^2$AD: Multi-Sensor Multi-System Anomaly Detection through Global Scoring and Calibrated Thresholding](https://arxiv.org/abs/2504.15225)
*Sarah Alnegheimish,Zelin He,Matthew Reimherr,Akash Chandrayan,Abhinav Pradhan,Luca D'Angelo*

Main category: cs.LG

TLDR: M$^2$AD是一个用于多系统多变量时间序列数据的无监督异常检测框架，通过深度模型捕捉正常行为，利用残差和全局异常评分实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法大多针对单系统或单变量数据，无法处理多系统多变量数据的复杂性和异质性。

Method: 使用深度模型学习正常行为，通过残差和Gaussian Mixture Model及Gamma校准生成全局异常评分。

Result: M$^2$AD在评估中平均优于现有方法21%，并在亚马逊物流中心的130个资产上验证了有效性。

Conclusion: M$^2$AD能有效处理多系统多变量数据的异质性和依赖性，具有实际应用价值。

Abstract: With the widespread availability of sensor data across industrial and
operational systems, we frequently encounter heterogeneous time series from
multiple systems. Anomaly detection is crucial for such systems to facilitate
predictive maintenance. However, most existing anomaly detection methods are
designed for either univariate or single-system multivariate data, making them
insufficient for these complex scenarios. To address this, we introduce
M$^2$AD, a framework for unsupervised anomaly detection in multivariate time
series data from multiple systems. M$^2$AD employs deep models to capture
expected behavior under normal conditions, using the residuals as indicators of
potential anomalies. These residuals are then aggregated into a global anomaly
score through a Gaussian Mixture Model and Gamma calibration. We theoretically
demonstrate that this framework can effectively address heterogeneity and
dependencies across sensors and systems. Empirically, M$^2$AD outperforms
existing methods in extensive evaluations by 21% on average, and its
effectiveness is demonstrated on a large-scale real-world case study on 130
assets in Amazon Fulfillment Centers. Our code and results are available at
https://github.com/sarahmish/M2AD.

</details>

### [148] [Conformalized-KANs: Uncertainty Quantification with Coverage Guarantees for Kolmogorov-Arnold Networks (KANs) in Scientific Machine Learning](https://arxiv.org/abs/2504.15240)
*Amirhossein Mollaali,Christian Bolivar Moya,Amanda A. Howard,Alexander Heinlein,Panos Stinis,Guang Lin*

Main category: cs.LG

TLDR: 本文研究了Kolmogorov-Arnold Networks (KANs)中的不确定性量化方法，提出了一种集成方法和Conformalized-KANs，以提高模型的解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索KANs中的不确定性量化方法，以增强模型的可靠性和适用性。

Method: 采用集成方法和Conformalized-KANs，结合共形预测技术生成校准的预测区间。

Result: 实验表明，该方法在多种超参数设置下均能提供鲁棒且准确的预测区间。

Conclusion: 该方法显著提升了KANs在科学机器学习中的可靠性和适用性。

Abstract: This paper explores uncertainty quantification (UQ) methods in the context of
Kolmogorov-Arnold Networks (KANs). We apply an ensemble approach to KANs to
obtain a heuristic measure of UQ, enhancing interpretability and robustness in
modeling complex functions. Building on this, we introduce Conformalized-KANs,
which integrate conformal prediction, a distribution-free UQ technique, with
KAN ensembles to generate calibrated prediction intervals with guaranteed
coverage. Extensive numerical experiments are conducted to evaluate the
effectiveness of these methods, focusing particularly on the robustness and
accuracy of the prediction intervals under various hyperparameter settings. We
show that the conformal KAN predictions can be applied to recent extensions of
KANs, including Finite Basis KANs (FBKANs) and multifideilty KANs (MFKANs). The
results demonstrate the potential of our approaches to improve the reliability
and applicability of KANs in scientific machine learning.

</details>

### [149] [Single-loop Algorithms for Stochastic Non-convex Optimization with Weakly-Convex Constraints](https://arxiv.org/abs/2504.15243)
*Ming Yang,Gang Li,Quanqi Hu,Qihang Lin,Tianbao Yang*

Main category: cs.LG

TLDR: 本文提出了一种新型单循环惩罚随机算法，用于解决弱凸目标函数和约束函数的优化问题，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 多函数不等式约束优化在机器学习中应用广泛，但现有方法存在收敛速度慢或依赖双循环设计的问题。

Method: 采用基于铰链的惩罚方法，结合恒定惩罚参数，提出单循环随机算法，并扩展到有限和耦合组合目标。

Result: 实现了近似KKT解的最先进复杂度，并在公平学习和持续学习实验中验证了方法的有效性。

Conclusion: 新算法在复杂度和实用性上优于现有方法，适用于多种机器学习场景。

Abstract: Constrained optimization with multiple functional inequality constraints has
significant applications in machine learning. This paper examines a crucial
subset of such problems where both the objective and constraint functions are
weakly convex. Existing methods often face limitations, including slow
convergence rates or reliance on double-loop algorithmic designs. To overcome
these challenges, we introduce a novel single-loop penalty-based stochastic
algorithm. Following the classical exact penalty method, our approach employs a
{\bf hinge-based penalty}, which permits the use of a constant penalty
parameter, enabling us to achieve a {\bf state-of-the-art complexity} for
finding an approximate Karush-Kuhn-Tucker (KKT) solution. We further extend our
algorithm to address finite-sum coupled compositional objectives, which are
prevalent in artificial intelligence applications, establishing improved
complexity over existing approaches. Finally, we validate our method through
experiments on fair learning with receiver operating characteristic (ROC)
fairness constraints and continual learning with non-forgetting constraints.

</details>

### [150] [Faster Algorithms for Agnostically Learning Disjunctions and their Implications](https://arxiv.org/abs/2504.15244)
*Ilias Diakonikolas,Daniel M. Kane,Lisheng Ren*

Main category: cs.LG

TLDR: 本文提出了一种在分布无关的不可知PAC模型中学习布尔析取的新算法，复杂度为2^(O(n^(1/3)))，优于现有方法，并首次在SQ和CSQ模型间实现了分离。


<details>
  <summary>Details</summary>
Motivation: 研究如何在不可知PAC模型中高效学习布尔析取，突破现有CSQ算法的复杂度限制。

Method: 开发了一种基于统计查询（SQ）模型的算法，复杂度为2^(O(n^(1/3)))。

Result: 算法复杂度显著优于现有方法，并在SQ和CSQ模型间首次实现了分离。

Conclusion: 新算法为布尔析取学习提供了更高效的解决方案，并展示了SQ模型的优势。

Abstract: We study the algorithmic task of learning Boolean disjunctions in the
distribution-free agnostic PAC model. The best known agnostic learner for the
class of disjunctions over $\{0, 1\}^n$ is the $L_1$-polynomial regression
algorithm, achieving complexity $2^{\tilde{O}(n^{1/2})}$. This complexity bound
is known to be nearly best possible within the class of Correlational
Statistical Query (CSQ) algorithms. In this work, we develop an agnostic
learner for this concept class with complexity $2^{\tilde{O}(n^{1/3})}$. Our
algorithm can be implemented in the Statistical Query (SQ) model, providing the
first separation between the SQ and CSQ models in distribution-free agnostic
learning.

</details>

### [151] [On Learning Parallel Pancakes with Mostly Uniform Weights](https://arxiv.org/abs/2504.15251)
*Ilias Diakonikolas,Daniel M. Kane,Sushrut Karmalkar,Jasper C. H. Lee,Thanasis Pittas*

Main category: cs.LG

TLDR: 研究学习$k$-高斯混合模型（$k$-GMMs）的复杂性，发现其复杂度为$d^{\Omega(k)}$。通过假设权重非指数小且协方差相同，已有算法在$d^{O(\log(1/w_{\min}))}$时间内有效。本文证明此上界几乎最优，并探讨权重分布对复杂度的影响。


<details>
  <summary>Details</summary>
Motivation: 研究高斯混合模型学习的复杂性，特别是如何通过结构假设降低复杂度。

Method: 提出统计查询（SQ）下界证明，并分析权重分布对学习复杂度的影响。

Result: 证明$d^{O(\log(1/w_{\min}))}$上界几乎最优，且权重分布对复杂度有显著影响。

Conclusion: 高斯混合模型学习的复杂度受权重分布影响，结构假设可显著降低复杂度。

Abstract: We study the complexity of learning $k$-mixtures of Gaussians ($k$-GMMs) on
$\mathbb{R}^d$. This task is known to have complexity $d^{\Omega(k)}$ in full
generality. To circumvent this exponential lower bound on the number of
components, research has focused on learning families of GMMs satisfying
additional structural properties. A natural assumption posits that the
component weights are not exponentially small and that the components have the
same unknown covariance. Recent work gave a $d^{O(\log(1/w_{\min}))}$-time
algorithm for this class of GMMs, where $w_{\min}$ is the minimum weight. Our
first main result is a Statistical Query (SQ) lower bound showing that this
quasi-polynomial upper bound is essentially best possible, even for the special
case of uniform weights. Specifically, we show that it is SQ-hard to
distinguish between such a mixture and the standard Gaussian. We further
explore how the distribution of weights affects the complexity of this task.
Our second main result is a quasi-polynomial upper bound for the aforementioned
testing task when most of the weights are uniform while a small fraction of the
weights are potentially arbitrary.

</details>

### [152] [Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction](https://arxiv.org/abs/2504.15266)
*Vaishnavh Nagarajan,Chen Henry Wu,Charles Ding,Aditi Raghunathan*

Main category: cs.LG

TLDR: 论文设计了一套最小算法任务，用于量化语言模型的创造力，发现多令牌方法优于单令牌学习，并提出了一种新的噪声注入方法。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在开放式任务中的创造力限制，探索超越单令牌学习和softmax采样的方法。

Method: 设计抽象任务，比较单令牌和多令牌方法，提出输入层噪声注入（hash-conditioning）。

Result: 多令牌方法（如无教师训练和扩散模型）表现更优，输入层噪声注入能提高随机性而不损害连贯性。

Conclusion: 为分析开放式创造力提供了测试平台，支持超越单令牌学习和改进采样方法。

Abstract: We design a suite of minimal algorithmic tasks that are a loose abstraction
of open-ended real-world tasks. This allows us to cleanly and controllably
quantify the creative limits of the present-day language model. Much like
real-world tasks that require a creative, far-sighted leap of thought, our
tasks require an implicit, open-ended stochastic planning step that either (a)
discovers new connections in an abstract knowledge graph (like in wordplay,
drawing analogies, or research) or (b) constructs new patterns (like in
designing math problems or new proteins). In these tasks, we empirically and
conceptually argue how next-token learning is myopic and memorizes excessively;
comparatively, multi-token approaches, namely teacherless training and
diffusion models, excel in producing diverse and original output. Secondly, in
our tasks, we find that to elicit randomness from the Transformer without
hurting coherence, it is better to inject noise right at the input layer (via a
method we dub hash-conditioning) rather than defer to temperature sampling from
the output layer. Thus, our work offers a principled, minimal test-bed for
analyzing open-ended creative skills, and offers new arguments for going beyond
next-token learning and softmax-based sampling. We make part of the code
available under https://github.com/chenwu98/algorithmic-creativity

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [153] [Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2504.13914)
*ByteDance Seed,:,Yufeng Yuan,Yu Yue,Mingxuan Wang,Xiaochen Zuo,Jiaze Chen,Lin Yan,Wenyuan Xu,Chi Zhang,Xin Liu,Chengyi Wang,TianTian Fan,Lingjun Liu,Qiying Yu,Xiangpeng Wei,Zhiqi Lin,Ruofei Zhu,Qingping Yang,Chengzhi Wei,Jerry He,Guanlin Liu,Zheng Wu,Xiangyu Yu,Zhicheng Liu,Jingjing Xu,Jiangjie Chen,Haojie Pan,Shengding Hu,Zhengyin Du,Wenqi Wang,Zewei Sun,Chenwei Lou,Bole Ma,Zihan Wang,Mofan Zhang,Wang Zhang,Gaohong Liu,Kaihua Jiang,Haibin Lin,Ru Zhang,Juncai Liu,Li Han,Jinxin Chi,Wenqiang Zhang,Jiayi Xu,Jun Yuan,Zhen Xiao,Yuqiao Xian,Jingqiao Wu,Kai Hua,Na Zhou,Jianhui Duan,Heyang Lu,Changbao Wang,Jinxiang Ou,Shihang Wang,Xiaoran Jin,Xuesong Yao,Chengyin Xu,Wenchang Ma,Zhecheng An,Renming Pang,Xia Xiao,Jing Su,Yuyu Zhang,Tao Sun,Kaibo Liu,Yifan Sun,Kai Shen,Sijun Zhang,Yiyuan Ma,Xingyan Bin,Ji Li,Yao Luo,Deyi Liu,Shiyi Zhan,Yunshui Li,Yuan Yang,Defa Zhu,Ke Shen,Chenggang Li,Xun Zhou,Liang Xiang,Yonghui Wu*

Main category: cs.CL

TLDR: Seed-Thinking-v1.5是一种混合专家模型（MoE），通过先思考再响应的方式提升推理能力，在STEM和编程任务中表现优异，同时在非推理任务中也有较强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 提升模型的推理能力和泛化性能，以支持更广泛的应用场景。

Method: 采用混合专家模型（MoE）架构，激活参数20B，总参数200B，通过先思考再响应的方式优化推理过程。

Result: 在AIME 2024、Codeforces和GPQA等基准测试中表现优异，非推理任务中超越DeepSeek R1 8%的胜率。

Conclusion: Seed-Thinking-v1.5在推理和泛化能力上均表现出色，未来将公开内部基准测试以支持进一步研究。

Abstract: We introduce Seed-Thinking-v1.5, capable of reasoning through thinking before
responding, resulting in improved performance on a wide range of benchmarks.
Seed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on
GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond
reasoning tasks, the method demonstrates notable generalization across diverse
domains. For instance, it surpasses DeepSeek R1 by 8% in win rate on
non-reasoning tasks, indicating its broader applicability. Compared to other
state-of-the-art reasoning models, Seed-Thinking-v1.5 is a Mixture-of-Experts
(MoE) model with a relatively small size, featuring 20B activated and 200B
total parameters. As part of our effort to assess generalized reasoning, we
develop two internal benchmarks, BeyondAIME and Codeforces, both of which will
be publicly released to support future research.

</details>

### [154] [Uncovering Conspiratorial Narratives within Arabic Online Content](https://arxiv.org/abs/2504.14037)
*Djamila Mohdeb,Meriem Laifa,Zineb Guemraoui,Dalila Behih*

Main category: cs.CL

TLDR: 该研究通过计算分析阿拉伯语在线内容，结合命名实体识别和主题建模技术（Top2Vec算法），识别并分类了阿拉伯语博客和Facebook中的阴谋论叙事，揭示了六种类型。


<details>
  <summary>Details</summary>
Motivation: 填补阴谋论研究中阿拉伯语内容或在线数据的空白，探索阿拉伯数字空间中阴谋论的表现形式及其对公共话语的影响。

Method: 使用命名实体识别和Top2Vec算法对阿拉伯语博客和Facebook内容进行分析。

Result: 识别出六类阴谋论叙事：性别/女权主义、地缘政治、政府掩盖、末日论、犹太共济会和地球工程。

Conclusion: 研究揭示了阿拉伯社交媒体中阴谋论的多样性及其与地区历史、文化和政治背景的关联，为理解阿拉伯世界公共话语提供了新视角。

Abstract: This study investigates the spread of conspiracy theories in Arabic digital
spaces through computational analysis of online content. By combining Named
Entity Recognition and Topic Modeling techniques, specifically the Top2Vec
algorithm, we analyze data from Arabic blogs and Facebook to identify and
classify conspiratorial narratives. Our analysis uncovers six distinct
categories: gender/feminist, geopolitical, government cover-ups, apocalyptic,
Judeo-Masonic, and geoengineering. The research highlights how these narratives
are deeply embedded in Arabic social media discourse, shaped by regional
historical, cultural, and sociopolitical contexts. By applying advanced Natural
Language Processing methods to Arabic content, this study addresses a gap in
conspiracy theory research, which has traditionally focused on English-language
content or offline data. The findings provide new insights into the
manifestation and evolution of conspiracy theories in Arabic digital spaces,
enhancing our understanding of their role in shaping public discourse in the
Arab world.

</details>

### [155] [MEQA: A Meta-Evaluation Framework for Question & Answer LLM Benchmarks](https://arxiv.org/abs/2504.14039)
*Jaime Raldua Veuthey,Zainab Ali Majid,Suhas Hariharan,Jacob Haimes*

Main category: cs.CL

TLDR: MEQA框架用于评估问答基准的质量，提供标准化评分和比较，应用于网络安全领域。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的发展，其社会影响扩大，亟需严格的评估方法。现有基准缺乏对基准质量的评估。

Method: 提出MEQA框架，通过标准化评估和量化评分，结合人类和LLM评估者，分析基准的优缺点。

Result: 在网络安全领域的测试中，展示了基准的优势和不足。

Conclusion: MEQA为问答基准的评估提供了有效工具，特别适用于LLMs的双重角色（防御工具与安全威胁）。

Abstract: As Large Language Models (LLMs) advance, their potential for widespread
societal impact grows simultaneously. Hence, rigorous LLM evaluations are both
a technical necessity and social imperative. While numerous evaluation
benchmarks have been developed, there remains a critical gap in
meta-evaluation: effectively assessing benchmarks' quality. We propose MEQA, a
framework for the meta-evaluation of question and answer (QA) benchmarks, to
provide standardized assessments, quantifiable scores, and enable meaningful
intra-benchmark comparisons. We demonstrate this approach on cybersecurity
benchmarks, using human and LLM evaluators, highlighting the benchmarks'
strengths and weaknesses. We motivate our choice of test domain by AI models'
dual nature as powerful defensive tools and security threats.

</details>

### [156] [A Baseline for Self-state Identification and Classification in Mental Health Data: CLPsych 2025 Task](https://arxiv.org/abs/2504.14066)
*Laerdon Kim*

Main category: cs.CL

TLDR: 本文提出了一种基于少样本学习和4位量化Gemma 2 9B模型的基线方法，用于分类Reddit心理健康数据中的自我状态。通过预处理步骤和句子分块，该方法在任务A.1中排名第三。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种有效的方法，从Reddit数据中分类自适应或非适应性的自我状态，以支持心理健康分析。

Method: 使用4位量化Gemma 2 9B模型进行少样本学习，并通过预处理步骤识别相关句子，再进行二元分类。

Result: 该方法在14个提交系统中排名第三，测试召回率为0.579。

Conclusion: 句子分块步骤显著提升了模型性能，因其与人工标注的粒度匹配，并简化了任务为二元分类。

Abstract: We present a baseline for the CLPsych 2025 A.1 task: classifying self-states
in mental health data taken from Reddit. We use few-shot learning with a 4-bit
quantized Gemma 2 9B model and a data preprocessing step which first identifies
relevant sentences indicating self-state evidence, and then performs a binary
classification to determine whether the sentence is evidence of an adaptive or
maladaptive self-state. This system outperforms our other method which relies
on an LLM to highlight spans of variable length independently. We attribute the
performance of our model to the benefits of this sentence chunking step for two
reasons: partitioning posts into sentences 1) broadly matches the granularity
at which self-states were human-annotated and 2) simplifies the task for our
language model to a binary classification problem. Our system places third out
of fourteen systems submitted for Task A.1, achieving a test-time recall of
0.579.

</details>

### [157] [LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models](https://arxiv.org/abs/2504.14089)
*Kang He,Kaushik Roy*

Main category: cs.CL

TLDR: LogicTree是一个推理时模块化框架，通过算法引导搜索解决LLM在复杂逻辑推理中的挑战，提高证明准确性。


<details>
  <summary>Details</summary>
Motivation: LLM在复杂逻辑推理中面临系统探索和逻辑一致性的挑战，以及前提空间大的组合复杂性。

Method: 提出LogicTree框架，结合缓存机制和线性化前提搜索，引入LLM无关的启发式方法优化前提选择。

Result: 在五个数据集上，LogicTree平均比CoT和ToT分别提升23.6%和12.5%的证明准确率。

Conclusion: LogicTree通过结构化推理和优化搜索策略，显著提升了LLM在复杂逻辑任务中的表现。

Abstract: Large language models (LLMs) have achieved remarkable multi-step reasoning
capabilities across various domains. However, LLMs still face distinct
challenges in complex logical reasoning, as (1) proof-finding requires
systematic exploration and the maintenance of logical coherence and (2)
searching the right combination of premises at each reasoning step is
inherently challenging in tasks with large premise space. To address this, we
propose LogicTree, an inference-time modular framework employing
algorithm-guided search to automate structured proof exploration and ensure
logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate
caching mechanism into LogicTree to enable effective utilization of historical
knowledge, preventing reasoning stagnation and minimizing redundancy.
Furthermore, we address the combinatorial complexity of premise search by
decomposing it into a linear process. The refined premise selection restricts
subsequent inference to at most one derivation per step, enhancing reasoning
granularity and enforcing strict step-by-step reasoning. Additionally, we
introduce two LLM-free heuristics for premise prioritization, enabling
strategic proof search. Experimental results on five datasets demonstrate that
LogicTree optimally scales inference-time computation to achieve higher proof
accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%
and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o
outperforms o3-mini by 7.6% on average.

</details>

### [158] [PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models](https://arxiv.org/abs/2504.14117)
*Nusrat Jahan Prottasha,Upama Roy Chowdhury,Shetu Mohanto,Tasfia Nuzhat,Abdullah As Sami,Md Shamol Ali,Md Shohanur Islam Sobuj,Hafijur Raman,Md Kowsher,Ozlem Ozmen Garibay*

Main category: cs.CL

TLDR: 该论文综述了参数高效微调（PEFT）技术，解决了大模型全微调的高成本问题，并分类比较了不同PEFT方法的机制与优劣。


<details>
  <summary>Details</summary>
Motivation: 传统全微调大模型需要大量计算资源和任务特定数据，成本高昂且存在过拟合、灾难性遗忘等问题。PEFT通过仅更新少量参数提供了一种高效解决方案。

Method: 论文提出了一种PEFT方法的分类法，包括加法、选择性、重参数化、混合和统一框架，并系统比较了它们的机制和权衡。

Result: PEFT在语言、视觉和生成建模等领域表现出色，能以较低资源成本实现高性能。

Conclusion: PEFT为大模型的实用、高效和可持续使用提供了可能，未来研究方向包括可扩展性、解释性和鲁棒性等。

Abstract: Large models such as Large Language Models (LLMs) and Vision Language Models
(VLMs) have transformed artificial intelligence, powering applications in
natural language processing, computer vision, and multimodal learning. However,
fully fine-tuning these models remains expensive, requiring extensive
computational resources, memory, and task-specific data. Parameter-Efficient
Fine-Tuning (PEFT) has emerged as a promising solution that allows adapting
large models to downstream tasks by updating only a small portion of
parameters. This survey presents a comprehensive overview of PEFT techniques,
focusing on their motivations, design principles, and effectiveness. We begin
by analyzing the resource and accessibility challenges posed by traditional
fine-tuning and highlight key issues, such as overfitting, catastrophic
forgetting, and parameter inefficiency. We then introduce a structured taxonomy
of PEFT methods -- grouped into additive, selective, reparameterized, hybrid,
and unified frameworks -- and systematically compare their mechanisms and
trade-offs. Beyond taxonomy, we explore the impact of PEFT across diverse
domains, including language, vision, and generative modeling, showing how these
techniques offer strong performance with lower resource costs. We also discuss
important open challenges in scalability, interpretability, and robustness, and
suggest future directions such as federated learning, domain adaptation, and
theoretical grounding. Our goal is to provide a unified understanding of PEFT
and its growing role in enabling practical, efficient, and sustainable use of
large models.

</details>

### [159] [Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations](https://arxiv.org/abs/2504.14150)
*Katie Matton,Robert Osazuwa Ness,John Guttag,Emre Kıcıman*

Main category: cs.CL

TLDR: 该论文提出了一种新方法来衡量大语言模型（LLM）解释的忠实性，通过定义忠实性并开发基于辅助LLM和贝叶斯分层模型的方法，揭示了LLM解释中可能隐藏的偏见和误导性。


<details>
  <summary>Details</summary>
Motivation: LLM生成的解释可能不忠实，导致用户过度信任和误用，因此需要一种方法来量化其解释的忠实性。

Method: 定义忠实性为解释中暗示的概念与实际影响概念之间的差异，并利用辅助LLM生成反事实输入，结合贝叶斯分层模型量化概念的影响。

Result: 实验表明，该方法能有效发现LLM解释中的不忠实模式，例如隐藏的社会偏见或误导性的证据影响。

Conclusion: 该方法为评估LLM解释的忠实性提供了实用工具，有助于揭示潜在问题并促进更透明的模型使用。

Abstract: Large language models (LLMs) are capable of generating plausible explanations
of how they arrived at an answer to a question. However, these explanations can
misrepresent the model's "reasoning" process, i.e., they can be unfaithful.
This, in turn, can lead to over-trust and misuse. We introduce a new approach
for measuring the faithfulness of LLM explanations. First, we provide a
rigorous definition of faithfulness. Since LLM explanations mimic human
explanations, they often reference high-level concepts in the input question
that purportedly influenced the model. We define faithfulness in terms of the
difference between the set of concepts that LLM explanations imply are
influential and the set that truly are. Second, we present a novel method for
estimating faithfulness that is based on: (1) using an auxiliary LLM to modify
the values of concepts within model inputs to create realistic counterfactuals,
and (2) using a Bayesian hierarchical model to quantify the causal effects of
concepts at both the example- and dataset-level. Our experiments show that our
method can be used to quantify and discover interpretable patterns of
unfaithfulness. On a social bias task, we uncover cases where LLM explanations
hide the influence of social bias. On a medical question answering task, we
uncover cases where LLM explanations provide misleading claims about which
pieces of evidence influenced the model's decisions.

</details>

### [160] [SConU: Selective Conformal Uncertainty in Large Language Models](https://arxiv.org/abs/2504.14154)
*Zhiyuan Wang,Qingni Wang,Yue Zhang,Tianlong Chen,Xiaofeng Zhu,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.CL

TLDR: 本文提出了一种名为选择性共形不确定性（SConU）的新方法，通过开发两种共形p值来检测数据异常，从而在可控风险水平下管理覆盖误差率，并提升预测效率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实应用中的普及，确保任务特定指标的可靠性变得至关重要。现有方法无法有效识别违反可交换性假设的异常数据，导致覆盖误差率失控。

Method: 提出SConU方法，开发两种共形p值进行显著性检验，以判断样本是否偏离校准集的不确定性分布，并在可控风险水平下管理覆盖误差率。

Result: SConU不仅能在单领域和跨学科背景下严格管理覆盖误差率，还能提升预测效率，并近似实现高风险问答任务中的条件覆盖。

Conclusion: SConU为共形预测提供了更可靠的异常检测和覆盖误差管理方法，适用于高风险任务。

Abstract: As large language models are increasingly utilized in real-world
applications, guarantees of task-specific metrics are essential for their
reliable deployment. Previous studies have introduced various criteria of
conformal uncertainty grounded in split conformal prediction, which offer
user-specified correctness coverage. However, existing frameworks often fail to
identify uncertainty data outliers that violate the exchangeability assumption,
leading to unbounded miscoverage rates and unactionable prediction sets. In
this paper, we propose a novel approach termed Selective Conformal Uncertainty
(SConU), which, for the first time, implements significance tests, by
developing two conformal p-values that are instrumental in determining whether
a given sample deviates from the uncertainty distribution of the calibration
set at a specific manageable risk level. Our approach not only facilitates
rigorous management of miscoverage rates across both single-domain and
interdisciplinary contexts, but also enhances the efficiency of predictions.
Furthermore, we comprehensively analyze the components of the conformal
procedures, aiming to approximate conditional coverage, particularly in
high-stakes question-answering tasks.

</details>

### [161] [Self-Correction Makes LLMs Better Parsers](https://arxiv.org/abs/2504.14165)
*Ziyan Zhang,Yang Hou,Chen Gong,Zhenghua Li*

Main category: cs.CL

TLDR: 本文分析了大型语言模型（LLMs）在句法解析任务中的不足，提出了一种基于现有树库语法规则的自校正方法，显著提升了LLMs在英语和中文数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多种NLP任务中表现优异，但在句法解析等基础任务上仍存在不足，限制了其深层次语言理解能力。

Method: 提出一种自校正方法，利用现有树库的语法规则自动检测潜在错误，并通过动态搜索相关规则提供提示和示例，指导LLMs自行修正错误。

Result: 在三个数据集上的实验表明，该方法显著提升了LLMs在域内和跨域任务中的性能。

Conclusion: 通过自校正方法，LLMs能够在不额外训练的情况下提升句法解析能力，验证了该方法的有效性。

Abstract: Large language models (LLMs) have achieved remarkable success across various
natural language processing (NLP) tasks. However, recent studies suggest that
they still face challenges in performing fundamental NLP tasks essential for
deep language understanding, particularly syntactic parsing. In this paper, we
conduct an in-depth analysis of LLM parsing capabilities, delving into the
specific shortcomings of their parsing results. We find that LLMs may stem from
limitations to fully leverage grammar rules in existing treebanks, which
restricts their capability to generate valid syntactic structures. To help LLMs
acquire knowledge without additional training, we propose a self-correction
method that leverages grammar rules from existing treebanks to guide LLMs in
correcting previous errors. Specifically, we automatically detect potential
errors and dynamically search for relevant rules, offering hints and examples
to guide LLMs in making corrections themselves. Experimental results on three
datasets with various LLMs, demonstrate that our method significantly improves
performance in both in-domain and cross-domain settings on the English and
Chinese datasets.

</details>

### [162] [Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion](https://arxiv.org/abs/2504.14175)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TLDR: 研究发现，基于大语言模型（LLM）的查询扩展方法在零样本检索任务中的性能提升可能源于基准测试中的知识泄漏，而非模型的实际能力。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM生成的假设文档是否因基准测试中的知识泄漏而提升检索性能，而非模型本身的能力。

Method: 以事实验证为测试平台，分析生成的文档是否包含与真实证据相关的信息，并评估其对性能的影响。

Result: 性能提升仅发生在生成的文档包含与真实证据相关的句子时，表明基准测试可能存在知识泄漏。

Conclusion: 知识泄漏可能夸大了LLM查询扩展方法的性能，尤其是在需要检索小众或新知识的实际场景中。

Abstract: Query expansion methods powered by large language models (LLMs) have
demonstrated effectiveness in zero-shot retrieval tasks. These methods assume
that LLMs can generate hypothetical documents that, when incorporated into a
query vector, enhance the retrieval of real evidence. However, we challenge
this assumption by investigating whether knowledge leakage in benchmarks
contributes to the observed performance gains. Using fact verification as a
testbed, we analyzed whether the generated documents contained information
entailed by ground truth evidence and assessed their impact on performance. Our
findings indicate that performance improvements occurred consistently only for
claims whose generated documents included sentences entailed by ground truth
evidence. This suggests that knowledge leakage may be present in these
benchmarks, inflating the perceived performance of LLM-based query expansion
methods, particularly in real-world scenarios that require retrieving niche or
novel knowledge.

</details>

### [163] [Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models](https://arxiv.org/abs/2504.14194)
*Xinlin Zhuang,Jiahui Peng,Ren Ma,Yinfan Wang,Tianyi Bai,Xingjian Wei,Jiantao Qiu,Chi Zhang,Ying Qian,Conghui He*

Main category: cs.CL

TLDR: 论文提出PRRC框架和Meta-rater方法，通过多维度评估数据质量，显著提升模型训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM预训练数据集缺乏透明度，数据选择方法单一，限制了模型性能优化。

Method: 提出PRRC框架（专业性、可读性、推理性、清洁度）和Meta-rater方法，结合多维度质量指标优化数据选择。

Result: Meta-rater使1.3B参数模型收敛速度翻倍，下游任务性能提升3.23，并在更大规模模型中验证了可扩展性。

Conclusion: 多维度数据质量评估显著优于传统单维度方法，为LLM预训练提供了高效且可扩展的解决方案。

Abstract: The composition of pre-training datasets for large language models (LLMs)
remains largely undisclosed, hindering transparency and efforts to optimize
data quality, a critical driver of model performance. Current data selection
methods, such as natural language quality assessments, diversity-based filters,
and classifier-based approaches, are limited by single-dimensional evaluation
or redundancy-focused strategies. To address these gaps, we propose PRRC to
evaluate data quality across Professionalism, Readability, Reasoning, and
Cleanliness. We further introduce Meta-rater, a multi-dimensional data
selection method that integrates these dimensions with existing quality metrics
through learned optimal weightings. Meta-rater employs proxy models to train a
regression model that predicts validation loss, enabling the identification of
optimal combinations of quality scores. Experiments demonstrate that Meta-rater
doubles convergence speed for 1.3B parameter models and improves downstream
task performance by 3.23, with scalable benefits observed in 3.3B models
trained on 100B tokens. Additionally, we release the annotated SlimPajama-627B
dataset, labeled across 25 quality metrics (including PRRC), to advance
research in data-centric LLM development. Our work establishes that holistic,
multi-dimensional quality integration significantly outperforms conventional
single-dimension approaches, offering a scalable paradigm for enhancing
pre-training efficiency and model capability.

</details>

### [164] [EIoU-EMC: A Novel Loss for Domain-specific Nested Entity Recognition](https://arxiv.org/abs/2504.14203)
*Jian Zhang,Tianqing Zhang,Qi Li,Hongwei Wang*

Main category: cs.CL

TLDR: 本文提出了一种新的损失函数EIoU-EMC，通过改进IoU损失和多类损失，解决了低资源和类别不平衡问题，提升了嵌套NER任务在生物医学和工业领域的性能。


<details>
  <summary>Details</summary>
Motivation: 当前嵌套NER任务在特定领域（如生物医学和工业）中面临低资源和类别不平衡的挑战，限制了其广泛应用。

Method: 设计了EIoU-EMC损失函数，结合实体边界和实体分类信息，提升模型在少量数据下的学习能力。

Result: 在三个生物医学NER数据集和一个工业数据集上验证，相比基线方法表现出竞争性性能，尤其在实体边界识别和分类上有显著提升。

Conclusion: EIoU-EMC方法有效解决了特定领域嵌套NER任务的挑战，为低资源场景提供了实用解决方案。

Abstract: In recent years, research has mainly focused on the general NER task. There
still have some challenges with nested NER task in the specific domains.
Specifically, the scenarios of low resource and class imbalance impede the wide
application for biomedical and industrial domains. In this study, we design a
novel loss EIoU-EMC, by enhancing the implement of Intersection over Union loss
and Multiclass loss. Our proposed method specially leverages the information of
entity boundary and entity classification, thereby enhancing the model's
capacity to learn from a limited number of data samples. To validate the
performance of this innovative method in enhancing NER task, we conducted
experiments on three distinct biomedical NER datasets and one dataset
constructed by ourselves from industrial complex equipment maintenance
documents. Comparing to strong baselines, our method demonstrates the
competitive performance across all datasets. During the experimental analysis,
our proposed method exhibits significant advancements in entity boundary
recognition and entity classification. Our code are available here.

</details>

### [165] [Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification](https://arxiv.org/abs/2504.14212)
*Takuma Udagawa,Yang Zhao,Hiroshi Kanayama,Bishwaranjan Bhattacharjee*

Main category: cs.CL

TLDR: 提出了一种高效的标注流程，用于分析预训练语料库中的社会偏见，并通过实验验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 预训练数据中的社会偏见可能被大语言模型放大，因此需要研究并缓解这些偏见。

Method: 采用保护属性检测和尊重分类的标注流程，分析语料库中的语言极性。

Result: 实验证明了该偏见分析方法的有效性，并以Common Crawl为例进行了验证。

Conclusion: 提出的方法能有效识别和缓解预训练语料中的社会偏见。

Abstract: Large language models (LLMs) acquire general linguistic knowledge from
massive-scale pretraining. However, pretraining data mainly comprised of
web-crawled texts contain undesirable social biases which can be perpetuated or
even amplified by LLMs. In this study, we propose an efficient yet effective
annotation pipeline to investigate social biases in the pretraining corpora.
Our pipeline consists of protected attribute detection to identify diverse
demographics, followed by regard classification to analyze the language
polarity towards each attribute. Through our experiments, we demonstrate the
effect of our bias analysis and mitigation measures, focusing on Common Crawl
as the most representative pretraining corpus.

</details>

### [166] [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/abs/2504.14218)
*Junchi Yao,Shu Yang,Jianhua Xu,Lijie Hu,Mengdi Li,Di Wang*

Main category: cs.CL

TLDR: 论文提出了一种名为'Duplicatus Charm'的新方法，通过机制可解释性研究大语言模型（LLMs）中重复文本生成的根源，并利用稀疏自编码器（SAEs）提取关键特征以缓解重复问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多个领域取得显著进展，但常出现文本重复问题（'Repeat Curse'），其根本机制尚未充分探索。

Method: 通过logit分析定位重复相关的模型层，利用SAEs提取并刺激'重复特征'，构建重复数据集并设计评估流程验证方法。

Result: 通过去激活重复特征，有效缓解了重复问题。

Conclusion: 研究揭示了LLMs中重复生成的机制，并提出了一种可行的解决方案。

Abstract: Large language models (LLMs) have made remarkable progress in various
domains, yet they often suffer from repetitive text generation, a phenomenon we
refer to as the "Repeat Curse". While previous studies have proposed decoding
strategies to mitigate repetition, the underlying mechanism behind this issue
remains insufficiently explored. In this work, we investigate the root causes
of repetition in LLMs through the lens of mechanistic interpretability.
Inspired by recent advances in Sparse Autoencoders (SAEs), which enable
monosemantic feature extraction, we propose a novel approach, "Duplicatus
Charm", to induce and analyze the Repeat Curse. Our method systematically
identifies "Repetition Features" -the key model activations responsible for
generating repetitive outputs. First, we locate the layers most involved in
repetition through logit analysis. Next, we extract and stimulate relevant
features using SAE-based activation manipulation. To validate our approach, we
construct a repetition dataset covering token and paragraph level repetitions
and introduce an evaluation pipeline to quantify the influence of identified
repetition features. Furthermore, by deactivating these features, we have
effectively mitigated the Repeat Curse.

</details>

### [167] [SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification](https://arxiv.org/abs/2504.14223)
*Michael Färber,Parisa Aghdam,Kyuri Im,Mario Tawfelis,Hardik Ghoshal*

Main category: cs.CL

TLDR: 本文介绍了第一个系统“simplifymytext.org”，利用GPT-4和Llama-3生成适合不同受众的简化文本，填补了现有自动文本简化技术的空白。


<details>
  <summary>Details</summary>
Motivation: 复杂文本对理解能力有限的受众构成障碍，现有方法未能充分利用大语言模型（LLMs）为不同群体和简化程度提供定制化服务。

Method: 开发了“simplifymytext.org”系统，支持多种输入格式（如文本输入和文件上传），并利用GPT-4和Llama-3生成简化内容。

Result: 系统通过多指标评估，证明了其在自动文本简化中的有效性。

Conclusion: 该研究推动了自动文本简化技术的发展，并强调了定制化沟通在促进包容性中的重要性。

Abstract: Text simplification is essential for making complex content accessible to
diverse audiences who face comprehension challenges. Yet, the limited
availability of simplified materials creates significant barriers to personal
and professional growth and hinders social inclusion. Although researchers have
explored various methods for automatic text simplification, none fully leverage
large language models (LLMs) to offer tailored customization for different
target groups and varying levels of simplicity. Moreover, despite its proven
benefits for both consumers and organizations, the well-established practice of
plain language remains underutilized. In this paper, we
https://simplifymytext.org, the first system designed to produce plain language
content from multiple input formats, including typed text and file uploads,
with flexible customization options for diverse audiences. We employ GPT-4 and
Llama-3 and evaluate outputs across multiple metrics. Overall, our work
contributes to research on automatic text simplification and highlights the
importance of tailored communication in promoting inclusivity.

</details>

### [168] [Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale](https://arxiv.org/abs/2504.14225)
*Bowen Jiang,Zhuoqun Hao,Young-Min Cho,Bryan Li,Yuan Yuan,Sihao Chen,Lyle Ungar,Camillo J. Taylor,Dan Roth*

Main category: cs.CL

TLDR: 论文介绍了PERSONAMEM基准，用于评估LLMs如何利用用户交互历史来个性化响应，发现现有模型在动态跟踪用户偏好方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能有效利用用户交互历史来内部化用户特质和偏好，并动态跟踪其变化以生成个性化响应。

Method: 提出PERSONAMEM基准，包含180个模拟用户-LLM交互历史，评估LLMs在动态用户情境下的响应能力。

Result: 当前LLMs（如GPT-4.1等）在动态跟踪用户偏好方面表现不佳，准确率仅约50%。

Conclusion: PERSONAMEM基准可为未来开发用户感知的聊天机器人提供支持。

Abstract: Large Language Models (LLMs) have emerged as personalized assistants for
users across a wide range of tasks -- from offering writing support to
delivering tailored recommendations or consultations. Over time, the
interaction history between a user and an LLM can provide extensive information
about an individual's traits and preferences. However, open questions remain on
how well LLMs today can effectively leverage such history to (1) internalize
the user's inherent traits and preferences, (2) track how the user profiling
and preferences evolve over time, and (3) generate personalized responses
accordingly in new scenarios.
  In this work, we introduce the PERSONAMEM benchmark. PERSONAMEM features
curated user profiles with over 180 simulated user-LLM interaction histories,
each containing up to 60 sessions of multi-turn conversations across 15
real-world tasks that require personalization. Given an in-situ user query,
i.e. query issued by the user from the first-person perspective, we evaluate
LLM chatbots' ability to identify the most suitable response according to the
current state of the user's profile. We observe that current LLMs still
struggle to recognize the dynamic evolution in users' profiles over time
through direct prompting approaches. As a consequence, LLMs often fail to
deliver responses that align with users' current situations and preferences,
with frontier models such as GPT-4.1, o4-mini, GPT-4.5, o1, or Gemini-2.0
achieving only around 50% overall accuracy, suggesting room for improvement. We
hope that PERSONAMEM, along with the user profile and conversation simulation
pipeline, can facilitate future research in the development of truly user-aware
chatbots. Code and data are available at github.com/bowen-upenn/PersonaMem.

</details>

### [169] [Probing the Subtle Ideological Manipulation of Large Language Models](https://arxiv.org/abs/2504.14287)
*Demetris Paschalides,George Pallis,Marios D. Dikaiakos*

Main category: cs.CL

TLDR: 论文研究了大型语言模型（LLMs）在政治意识形态光谱上的可操纵性，超越了传统的左右二分法，通过多任务数据集和微调实验揭示了模型对意识形态的敏感性和潜在风险。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLMs在左右二分政治立场上的偏见，忽略了更广泛的意识形态光谱。本文旨在探索LLMs在从进步左翼到保守右翼的多元政治立场中的可操纵性。

Method: 构建了一个多任务数据集（包括意识形态问答、声明排序、宣言填空和国会法案理解），并对Phi-2、Mistral和Llama-3三种LLMs进行微调，评估其意识形态表达能力。

Result: 微调显著提升了LLMs在意识形态光谱上的表达能力，而显式提示仅带来微小改进，表明模型对意识形态操纵高度敏感。

Conclusion: LLMs容易被意识形态操纵，需要更强大的防护措施来降低风险。

Abstract: Large Language Models (LLMs) have transformed natural language processing,
but concerns have emerged about their susceptibility to ideological
manipulation, particularly in politically sensitive areas. Prior work has
focused on binary Left-Right LLM biases, using explicit prompts and fine-tuning
on political QA datasets. In this work, we move beyond this binary approach to
explore the extent to which LLMs can be influenced across a spectrum of
political ideologies, from Progressive-Left to Conservative-Right. We introduce
a novel multi-task dataset designed to reflect diverse ideological positions
through tasks such as ideological QA, statement ranking, manifesto cloze
completion, and Congress bill comprehension. By fine-tuning three LLMs-Phi-2,
Mistral, and Llama-3-on this dataset, we evaluate their capacity to adopt and
express these nuanced ideologies. Our findings indicate that fine-tuning
significantly enhances nuanced ideological alignment, while explicit prompts
provide only minor refinements. This highlights the models' susceptibility to
subtle ideological manipulation, suggesting a need for more robust safeguards
to mitigate these risks.

</details>

### [170] [Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach](https://arxiv.org/abs/2504.14321)
*Xingyu Li,Chen Gong,Guohong Fu*

Main category: cs.CL

TLDR: TikTalkCoref是首个针对中文社交媒体多模态共指消解的数据集，填补了真实场景对话研究的空白。


<details>
  <summary>Details</summary>
Motivation: 多模态共指消解（MCR）对理解跨模态内容至关重要，但缺乏真实对话数据。

Method: 构建TikTalkCoref数据集，包含短视频和用户评论文本，并标注共指关系。提出基准方法并实验验证。

Result: 提供了可靠基准结果，数据集将公开以促进研究。

Conclusion: TikTalkCoref为社交媒体多模态共指消解研究提供了重要资源。

Abstract: Multimodal coreference resolution (MCR) aims to identify mentions referring
to the same entity across different modalities, such as text and visuals, and
is essential for understanding multimodal content. In the era of rapidly
growing mutimodal content and social media, MCR is particularly crucial for
interpreting user interactions and bridging text-visual references to improve
communication and personalization. However, MCR research for real-world
dialogues remains unexplored due to the lack of sufficient data resources.To
address this gap, we introduce TikTalkCoref, the first Chinese multimodal
coreference dataset for social media in real-world scenarios, derived from the
popular Douyin short-video platform. This dataset pairs short videos with
corresponding textual dialogues from user comments and includes manually
annotated coreference clusters for both person mentions in the text and the
coreferential person head regions in the corresponding video frames. We also
present an effective benchmark approach for MCR, focusing on the celebrity
domain, and conduct extensive experiments on our dataset, providing reliable
benchmark results for this newly constructed dataset. We will release the
TikTalkCoref dataset to facilitate future research on MCR for real-world social
media dialogues.

</details>

### [171] [Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models](https://arxiv.org/abs/2504.14366)
*Patrick Haller,Jonas Golde,Alan Akbik*

Main category: cs.CL

TLDR: 本文系统评估了从Transformer教师模型到九种子二次学生模型的知识蒸馏可转移性，探讨了不同架构约束和初始化策略对蒸馏过程的影响。


<details>
  <summary>Details</summary>
Motivation: 自注意力在推理时的二次复杂度是主要瓶颈，因此探索子二次替代方案（如SSMs、线性注意力和循环架构）具有重要意义。

Method: 通过知识蒸馏训练子二次学生模型，评估其与教师模型表示的对齐程度，并研究矩阵混合和QKV复制等初始化策略的影响。

Result: 在多个NLP基准测试中，实证结果揭示了效率与性能之间的权衡，并指出了成功知识转移的关键因素。

Conclusion: 研究为子二次架构的知识蒸馏提供了实用见解，强调了架构选择和初始化策略的重要性。

Abstract: Knowledge distillation is a widely used technique for compressing large
language models (LLMs) by training a smaller student model to mimic a larger
teacher model. Typically, both the teacher and student are Transformer-based
architectures, leveraging softmax attention for sequence modeling. However, the
quadratic complexity of self-attention at inference time remains a significant
bottleneck, motivating the exploration of subquadratic alternatives such as
structured state-space models (SSMs), linear attention, and recurrent
architectures. In this work, we systematically evaluate the transferability of
knowledge distillation from a Transformer teacher to nine subquadratic student
architectures. Our study aims to determine which subquadratic model best aligns
with the teacher's learned representations and how different architectural
constraints influence the distillation process. We also investigate the impact
of intelligent initialization strategies, including matrix mixing and
query-key-value (QKV) copying, on the adaptation process. Our empirical results
on multiple NLP benchmarks provide insights into the trade-offs between
efficiency and performance, highlighting key factors for successful knowledge
transfer to subquadratic architectures.

</details>

### [172] [Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites](https://arxiv.org/abs/2504.14367)
*Gabriel Machado Santos,Rita Maria da Silva Julia,Marcelo Zanchetta do Nascimento*

Main category: cs.CL

TLDR: 本文提出了一种结合上下文无关文法（CFG）和MAP-Elites算法的进化方法，系统探索提示工程空间，生成高质量且多样化的提示，并分析其与不同任务的匹配度。


<details>
  <summary>Details</summary>
Motivation: 提示工程对优化大型语言模型（LLM）至关重要，但提示结构与任务性能之间的关系尚未充分研究。

Method: 采用上下文无关文法（CFG）与MAP-Elites算法结合的方法，系统探索提示空间，生成多样且高质量的提示，并分析其与任务的匹配度。

Result: 在多个LLM和七个BigBench Lite任务上的实验表明，该方法显著提升了提示的质量和多样性，揭示了结构变化对性能的影响。

Conclusion: 该方法为任务特定和适应性提示设计提供了实用见解，提升了LLM的有效性和多功能性。

Abstract: Prompt engineering is essential for optimizing large language models (LLMs),
yet the link between prompt structures and task performance remains
underexplored. This work introduces an evolutionary approach that combines
context-free grammar (CFG) with the MAP-Elites algorithm to systematically
explore the prompt space. Our method prioritizes quality and diversity,
generating high-performing and structurally varied prompts while analyzing
their alignment with diverse tasks by varying traits such as the number of
examples (shots) and reasoning depth. By systematically mapping the phenotypic
space, we reveal how structural variations influence LLM performance, offering
actionable insights for task-specific and adaptable prompt design. Evaluated on
seven BigBench Lite tasks across multiple LLMs, our results underscore the
critical interplay of quality and diversity, advancing the effectiveness and
versatility of LLMs.

</details>

### [173] [ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data](https://arxiv.org/abs/2504.14452)
*Tong Chen,Faeze Brahman,Jiacheng Liu,Niloofar Mireshghallah,Weijia Shi,Pang Wei Koh,Luke Zettlemoyer,Hannaneh Hajishirzi*

Main category: cs.CL

TLDR: ParaPO是一种后训练方法，通过微调语言模型以减少无意识的记忆内容复现，同时保持模型实用性。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在非对抗性场景下记忆并复现预训练数据片段的问题，涉及版权、抄袭、隐私和创造力等担忧。

Method: 引入Paraphrase Preference Optimization (ParaPO)，训练模型偏好记忆内容的改写版本而非原文，并通过系统提示控制复现行为。

Result: 在Llama3.1-8B和Tulu3-8B上的评估显示，ParaPO显著减少无意识复现，同时保留对名言的回忆能力。

Conclusion: ParaPO有效减少语言模型的记忆内容复现，优于现有方法，且通过系统提示灵活控制行为。

Abstract: Language models (LMs) can memorize and reproduce segments from their
pretraining data verbatim even in non-adversarial settings, raising concerns
about copyright, plagiarism, privacy, and creativity. We introduce Paraphrase
Preference Optimization (ParaPO), a post-training method that fine-tunes LMs to
reduce unintentional regurgitation while preserving their overall utility.
ParaPO trains LMs to prefer paraphrased versions of memorized segments over the
original verbatim content from the pretraining data. To maintain the ability to
recall famous quotations when appropriate, we develop a variant of ParaPO that
uses system prompts to control regurgitation behavior. In our evaluation on
Llama3.1-8B, ParaPO consistently reduces regurgitation across all tested
datasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative
writing), whereas unlearning methods used in prior work to mitigate
regurgitation are less effective outside their targeted unlearned domain (from
17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO
with system prompting successfully preserves famous quotation recall while
reducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when
prompted not to regurgitate. In contrast, without ParaPO tuning, prompting the
model not to regurgitate produces only a marginal reduction (8.7 to 8.4).

</details>

### [174] [CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge](https://arxiv.org/abs/2504.14462)
*Armin Toroghi,Willis Guo,Scott Sanner*

Main category: cs.CL

TLDR: 论文提出新数据集CoLoTa，用于评估大语言模型（LLMs）在长尾实体上的常识推理能力及其幻觉问题，并测试知识图谱问答（KGQA）方法的常识推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在编码事实和常识知识方面表现出色，但在长尾实体的常识推理任务中仍存在高错误率和幻觉问题，限制了其在高风险场景中的应用。

Method: 构建了包含3,300个查询的CoLoTa数据集，涵盖多样化的常识推理任务，并基于Wikidata知识图谱支持。实验测试了LLM和KGQA方法的表现。

Result: 实验表明，现有LLM和KGQA方法在涉及常识推理的长尾实体查询中表现严重不足。

Conclusion: CoLoTa可作为评估LLMs和KGQA方法常识推理能力及抗幻觉能力的新基准。

Abstract: The rise of Large Language Models (LLMs) has redefined the AI landscape,
particularly due to their ability to encode factual and commonsense knowledge,
and their outstanding performance in tasks requiring reasoning. Despite these
advances, hallucinations and reasoning errors remain a significant barrier to
their deployment in high-stakes settings. In this work, we observe that even
the most prominent LLMs, such as OpenAI-o1, suffer from high rates of reasoning
errors and hallucinations on tasks requiring commonsense reasoning over
obscure, long-tail entities. To investigate this limitation, we present a new
dataset for Commonsense reasoning over Long-Tail entities (CoLoTa), that
consists of 3,300 queries from question answering and claim verification tasks
and covers a diverse range of commonsense reasoning skills. We remark that
CoLoTa can also serve as a Knowledge Graph Question Answering (KGQA) dataset
since the support of knowledge required to answer its queries is present in the
Wikidata knowledge graph. However, as opposed to existing KGQA benchmarks that
merely focus on factoid questions, our CoLoTa queries also require commonsense
reasoning. Our experiments with strong LLM-based KGQA methodologies indicate
their severe inability to answer queries involving commonsense reasoning.
Hence, we propose CoLoTa as a novel benchmark for assessing both (i) LLM
commonsense reasoning capabilities and their robustness to hallucinations on
long-tail entities and (ii) the commonsense reasoning capabilities of KGQA
methods.

</details>

### [175] [sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach to Brain-Language Alignment](https://arxiv.org/abs/2504.14468)
*Yijun Liu*

Main category: cs.CL

TLDR: SSENSE框架通过对比学习将sEEG信号映射到CLIP模型的句子嵌入空间，实现了从大脑活动直接检索句子级别的信息。


<details>
  <summary>Details</summary>
Motivation: 研究多模态基础模型在将侵入性脑记录与自然语言对齐方面的潜力，以解决神经活动解释的复杂挑战。

Method: 使用InfoNCE损失在sEEG的频谱表示上训练神经编码器，不微调文本编码器。

Result: 在有限数据下，SSENSE展示了通用语言表示可作为神经解码的有效先验。

Conclusion: SSENSE证明了多模态基础模型在神经解码中的潜力，为神经活动解释提供了新方法。

Abstract: Interpreting neural activity through meaningful latent representations
remains a complex and evolving challenge at the intersection of neuroscience
and artificial intelligence. We investigate the potential of multimodal
foundation models to align invasive brain recordings with natural language. We
present SSENSE, a contrastive learning framework that projects single-subject
stereo-electroencephalography (sEEG) signals into the sentence embedding space
of a frozen CLIP model, enabling sentence-level retrieval directly from brain
activity. SSENSE trains a neural encoder on spectral representations of sEEG
using InfoNCE loss, without fine-tuning the text encoder. We evaluate our
method on time-aligned sEEG and spoken transcripts from a naturalistic
movie-watching dataset. Despite limited data, SSENSE achieves promising
results, demonstrating that general-purpose language representations can serve
as effective priors for neural decoding.

</details>

### [176] [DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party Dialogue](https://arxiv.org/abs/2504.14482)
*Xiang Li,Duyi Pan,Hongru Xiao,Jiale Han,Jing Tang,Jiabao Ma,Wei Wang,Bo Cheng*

Main category: cs.CL

TLDR: 提出了一种基于混合代理的语音合成框架DialogueAgents，通过三个代理协作生成对话，提升了情感表达和副语言特征，并贡献了一个高质量的双语多轮对话数据集MultiTalk。


<details>
  <summary>Details</summary>
Motivation: 现有语音合成数据集构建成本高且多样性不足，限制了自然交互的发展。

Method: 提出DialogueAgents框架，整合脚本编写、语音合成和对话批评三个代理，迭代优化对话脚本和语音合成。

Result: 生成了高质量的双语多轮对话数据集MultiTalk，实验验证了框架的有效性。

Conclusion: DialogueAgents框架和MultiTalk数据集为语音合成研究提供了新工具和资源。

Abstract: Speech synthesis is crucial for human-computer interaction, enabling natural
and intuitive communication. However, existing datasets involve high
construction costs due to manual annotation and suffer from limited character
diversity, contextual scenarios, and emotional expressiveness. To address these
issues, we propose DialogueAgents, a novel hybrid agent-based speech synthesis
framework, which integrates three specialized agents -- a script writer, a
speech synthesizer, and a dialogue critic -- to collaboratively generate
dialogues. Grounded in a diverse character pool, the framework iteratively
refines dialogue scripts and synthesizes speech based on speech review,
boosting emotional expressiveness and paralinguistic features of the
synthesized dialogues. Using DialogueAgent, we contribute MultiTalk, a
bilingual, multi-party, multi-turn speech dialogue dataset covering diverse
topics. Extensive experiments demonstrate the effectiveness of our framework
and the high quality of the MultiTalk dataset. We release the dataset and code
https://github.com/uirlx/DialogueAgents to facilitate future research on
advanced speech synthesis models and customized data generation.

</details>

### [177] [FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering](https://arxiv.org/abs/2504.14492)
*Yichen Li,Zhiting Fan,Ruizhe Chen,Xiaotang Gai,Luqi Gong,Yan Zhang,Zuozhu Liu*

Main category: cs.CL

TLDR: FairSteer是一种无需定制提示或模型重新训练的推理时去偏框架，通过检测偏激活、计算去偏导向向量（DSV）和动态调整激活来实现去偏。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）容易从训练数据中捕获偏见，现有方法因提示敏感或计算开销大而效果不稳定。

Method: FairSteer基于线性表示假设，通过轻量级线性分类器检测偏激活，计算DSV作为干预方向，并在推理阶段动态调整激活。

Result: 在六种LLM上的综合评估表明，FairSteer在问答、反事实输入评估和开放文本生成任务中表现优越。

Conclusion: FairSteer提供了一种高效且稳定的去偏方法，无需额外训练或复杂提示设计。

Abstract: Large language models (LLMs) are prone to capturing biases from training
corpus, leading to potential negative social impacts. Existing prompt-based
debiasing methods exhibit instability due to their sensitivity to prompt
changes, while fine-tuning-based techniques incur substantial computational
overhead and catastrophic forgetting. In this paper, we propose FairSteer, a
novel inference-time debiasing framework without requiring customized prompt
design or model retraining. Motivated by the linear representation hypothesis,
our preliminary investigation demonstrates that fairness-related features can
be encoded into separable directions in the hidden activation space. FairSteer
operates in three steps: biased activation detection, debiasing steering vector
(DSV) computation, and dynamic activation steering. Specifically, it first
trains a lightweight linear classifier to detect bias signatures in
activations, and then computes DSVs as intervention directions derived from
small contrastive prompt pairs. Subsequently, it performs debiasing by
adjusting activations with DSVs in the inference stage. Comprehensive
evaluation with six LLMs demonstrates the superiority of FairSteer across
question-answering, counterfactual input evaluation and open-ended text
generation tasks. Code will be released.

</details>

### [178] [Functional Abstraction of Knowledge Recall in Large Language Models](https://arxiv.org/abs/2504.14496)
*Zijian Wang,Chang Xu*

Main category: cs.CL

TLDR: 该论文研究了大型语言模型（LLMs）中的知识召回机制，将其抽象为功能结构，并提出激活向量与功能组件（输入、函数体、返回值）的对齐关系。通过实验验证，改进了基于激活修补的知识编辑方法。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs中知识召回的机制，理解其隐含的功能执行过程，以改进知识编辑和记忆保留。

Method: 设计基于修补的知识评分算法识别功能组件，并通过反知识测试验证各组件对知识召回的影响。

Result: 激活向量与功能组件对齐，通过激活修补改进了上下文知识编辑方法，提升了新知识的短期记忆保留。

Conclusion: 从功能视角揭示了LLMs知识召回的机制，为知识编辑提供了新方法。

Abstract: Pre-trained transformer large language models (LLMs) demonstrate strong
knowledge recall capabilities. This paper investigates the knowledge recall
mechanism in LLMs by abstracting it into a functional structure. We propose
that during knowledge recall, the model's hidden activation space implicitly
entails a function execution process where specific activation vectors align
with functional components (Input argument, Function body, and Return values).
Specifically, activation vectors of relation-related tokens define a mapping
function from subjects to objects, with subject-related token activations
serving as input arguments and object-related token activations as return
values. For experimental verification, we first design a patching-based
knowledge-scoring algorithm to identify knowledge-aware activation vectors as
independent functional components. Then, we conduct counter-knowledge testing
to examine the independent functional effects of each component on knowledge
recall outcomes. From this functional perspective, we improve the contextual
knowledge editing approach augmented by activation patching. By rewriting
incoherent activations in context, we enable improved short-term memory
retention for new knowledge prompting.

</details>

### [179] [Causality for Natural Language Processing](https://arxiv.org/abs/2504.14530)
*Zhijing Jin*

Main category: cs.CL

TLDR: 论文探讨了大型语言模型（LLMs）在因果推理和理解方面的能力，研究了其机制、应用及改进方向。


<details>
  <summary>Details</summary>
Motivation: 因果推理是人类智能的核心，也是人工智能系统实现高级理解和决策的关键能力。

Method: 通过一系列研究、新数据集、基准任务和方法框架，分析LLMs的因果推理能力及其在NLP和社会科学中的应用。

Result: 揭示了LLMs在因果推理中的关键挑战和机遇，为未来研究提供了基础。

Conclusion: 论文为提升LLMs的因果推理能力提供了全面框架，并指出了未来研究方向。

Abstract: Causal reasoning is a cornerstone of human intelligence and a critical
capability for artificial systems aiming to achieve advanced understanding and
decision-making. This thesis delves into various dimensions of causal reasoning
and understanding in large language models (LLMs). It encompasses a series of
studies that explore the causal inference skills of LLMs, the mechanisms behind
their performance, and the implications of causal and anticausal learning for
natural language processing (NLP) tasks. Additionally, it investigates the
application of causal reasoning in text-based computational social science,
specifically focusing on political decision-making and the evaluation of
scientific impact through citations. Through novel datasets, benchmark tasks,
and methodological frameworks, this work identifies key challenges and
opportunities to improve the causal capabilities of LLMs, providing a
comprehensive foundation for future research in this evolving field.

</details>

### [180] [BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation](https://arxiv.org/abs/2504.14538)
*Yiting Ran,Xintao Wang,Tian Qiu,Jiaqing Liang,Yanghua Xiao,Deqing Yang*

Main category: cs.CL

TLDR: BookWorld是一个基于书籍的多智能体社会模拟系统，能够生成高质量故事并保持对原著的忠实度。


<details>
  <summary>Details</summary>
Motivation: 探索如何模拟已建立的虚构世界和角色，填补现有研究的空白。

Method: 设计BookWorld系统，涵盖角色多样性、世界观、地理约束等现实复杂性。

Result: 实验显示BookWorld生成的故事质量高，忠实度强，胜率75.36%。

Conclusion: BookWorld为故事生成、互动游戏等提供了新方法，扩展了虚构作品的可能性。

Abstract: Recent advances in large language models (LLMs) have enabled social
simulation through multi-agent systems. Prior efforts focus on agent societies
created from scratch, assigning agents with newly defined personas. However,
simulating established fictional worlds and characters remain largely
underexplored, despite its significant practical value. In this paper, we
introduce BookWorld, a comprehensive system for constructing and simulating
book-based multi-agent societies. BookWorld's design covers comprehensive
real-world intricacies, including diverse and dynamic characters, fictional
worldviews, geographical constraints and changes, e.t.c. BookWorld enables
diverse applications including story generation, interactive games and social
simulation, offering novel ways to extend and explore beloved fictional works.
Through extensive experiments, we demonstrate that BookWorld generates
creative, high-quality stories while maintaining fidelity to the source books,
surpassing previous methods with a win rate of 75.36%. The code of this paper
can be found at the project page: https://bookworld2025.github.io/.

</details>

### [181] [a1: Steep Test-time Scaling Law via Environment Augmented Generation](https://arxiv.org/abs/2504.14597)
*Lingrui Mei,Shenghua Liu,Yiwei Wang,Baolong Bi,Yuyao Ge,Jun Wan,Yurong Wu,Xueqi Cheng*

Main category: cs.CL

TLDR: 提出了环境增强生成（EAG）框架，通过实时环境反馈、动态分支探索和经验学习，提升大语言模型（LLM）的推理能力，并在多步任务中实现自我修正。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在复杂推理任务中存在幻觉、逻辑错误和无法自我修正的问题，现有方法如思维链提示能力有限。

Method: EAG框架结合实时环境反馈验证推理步骤、动态分支探索错误路径，以及从成功推理轨迹中学习。

Result: a1-32B模型在同类模型中表现最佳，部分任务性能提升24.4个百分点，且任务越复杂优势越明显。

Conclusion: EAG通过环境交互和分支探索，为可靠机器推理提供了新范式，特别适用于需要精确多步计算和逻辑验证的任务。

Abstract: Large Language Models (LLMs) have made remarkable breakthroughs in reasoning,
yet continue to struggle with hallucinations, logical errors, and inability to
self-correct during complex multi-step tasks. Current approaches like
chain-of-thought prompting offer limited reasoning capabilities that fail when
precise step validation is required. We propose Environment Augmented
Generation (EAG), a framework that enhances LLM reasoning through: (1)
real-time environmental feedback validating each reasoning step, (2) dynamic
branch exploration for investigating alternative solution paths when faced with
errors, and (3) experience-based learning from successful reasoning
trajectories. Unlike existing methods, EAG enables deliberate backtracking and
strategic replanning through tight integration of execution feedback with
branching exploration. Our a1-32B model achieves state-of-the-art performance
among similar-sized models across all benchmarks, matching larger models like
o1 on competition mathematics while outperforming comparable models by up to
24.4 percentage points. Analysis reveals EAG's distinctive scaling pattern:
initial token investment in environment interaction yields substantial
long-term performance dividends, with advantages amplifying proportionally to
task complexity. EAG's theoretical framework demonstrates how environment
interactivity and systematic branch exploration together establish a new
paradigm for reliable machine reasoning, particularly for problems requiring
precise multi-step calculation and logical verification.

</details>

### [182] [Translation Analytics for Freelancers: I. Introduction, Data Preparation, Baseline Evaluations](https://arxiv.org/abs/2504.14619)
*Yuri Balashov,Alex Balashov,Shiho Fukuda Koski*

Main category: cs.CL

TLDR: 该论文探讨了语言技术的最新进展如何为资源有限的翻译者和语言服务提供商带来新机遇，并提出了一种将自动评估指标（如BLEU、chrF等）适应自由职业者需求的实用框架。


<details>
  <summary>Details</summary>
Motivation: 随着神经机器翻译系统和大型语言模型的进步，翻译领域发生了变革，为自由职业者提供了新的技术机会。论文旨在帮助翻译者利用这些技术进步。

Method: 论文提出了Translation Analytics方法，将传统用于大规模工业应用的评估技术适应小规模用户需求，并通过一个医学领域的三语语料库展示了自动评估指标的潜力。

Result: 研究发现，自动评估指标与人工评估之间存在统计相关性，强调了主动利用新兴技术的重要性。

Conclusion: 论文强调，翻译者应积极拥抱新技术，以适应并繁荣于不断变化的专业环境中。

Abstract: This is the first in a series of papers exploring the rapidly expanding new
opportunities arising from recent progress in language technologies for
individual translators and language service providers with modest resources.
The advent of advanced neural machine translation systems, large language
models, and their integration into workflows via computer-assisted translation
tools and translation management systems have reshaped the translation
landscape. These advancements enable not only translation but also quality
evaluation, error spotting, glossary generation, and adaptation to
domain-specific needs, creating new technical opportunities for freelancers. In
this series, we aim to empower translators with actionable methods to harness
these advancements. Our approach emphasizes Translation Analytics, a suite of
evaluation techniques traditionally reserved for large-scale industry
applications but now becoming increasingly available for smaller-scale users.
This first paper introduces a practical framework for adapting automatic
evaluation metrics -- such as BLEU, chrF, TER, and COMET -- to freelancers'
needs. We illustrate the potential of these metrics using a trilingual corpus
derived from a real-world project in the medical domain and provide statistical
analysis correlating human evaluations with automatic scores. Our findings
emphasize the importance of proactive engagement with emerging technologies to
not only adapt but thrive in the evolving professional environment.

</details>

### [183] [A Hierarchical Framework for Measuring Scientific Paper Innovation via Large Language Models](https://arxiv.org/abs/2504.14620)
*Hongming Tan,Shaoxiong Zhan,Fengwei Jia,Hai-Tao Zheng,Wai Kin Chan*

Main category: cs.CL

TLDR: HSPIM是一个基于大语言模型的分层免训练框架，用于评估科学论文的创新性，通过分段、问答增强和加权评分实现高效评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以全面捕捉论文创新性且缺乏泛化能力，HSPIM旨在解决这些问题。

Method: 采用Paper-to-Sections-to-QAs分解，利用零样本LLM提示实现分段分类、QA增强和加权评分。

Result: 实验表明HSPIM在有效性、泛化性和可解释性上优于基线方法。

Conclusion: HSPIM为科学论文创新性评估提供了一种高效且可推广的解决方案。

Abstract: Measuring scientific paper innovation is both important and challenging.
Existing content-based methods often overlook the full-paper context, fail to
capture the full scope of innovation, and lack generalization. We propose
HSPIM, a hierarchical and training-free framework based on large language
models (LLMs). It introduces a Paper-to-Sections-to-QAs decomposition to assess
innovation. We segment the text by section titles and use zero-shot LLM
prompting to implement section classification, question-answering (QA)
augmentation, and weighted novelty scoring. The generated QA pair focuses on
section-level innovation and serves as additional context to improve the LLM
scoring. For each chunk, the LLM outputs a novelty score and a confidence
score. We use confidence scores as weights to aggregate novelty scores into a
paper-level innovation score. To further improve performance, we propose a
two-layer question structure consisting of common and section-specific
questions, and apply a genetic algorithm to optimize the question-prompt
combinations. Comprehensive experiments on scientific conference paper datasets
show that HSPIM outperforms baseline methods in effectiveness, generalization,
and interpretability.

</details>

### [184] [Automatic Text Summarization (ATS) for Research Documents in Sorani Kurdish](https://arxiv.org/abs/2504.14630)
*Rondik Hadi Abdulrahman,Hossein Hassani*

Main category: cs.CL

TLDR: 该研究开发了一个基于231篇索拉尼库尔德语科学论文的数据集和语言模型，用于自动文本摘要（ATS），填补了库尔德语资源不足的空白。


<details>
  <summary>Details</summary>
Motivation: 库尔德语的自动文本摘要研究因资源有限而发展不足，该研究旨在填补这一空白。

Method: 使用句子加权和TF-IDF算法，对数据集进行两次实验（是否包含结论），并通过手动和自动（ROUGE指标）评估结果。

Result: 最佳准确率达到19.58%，专家手动评估结果因文档而异。

Conclusion: 该研究为库尔德语NLP研究者提供了宝贵资源，推动了ATS及相关领域的发展。

Abstract: Extracting concise information from scientific documents aids learners,
researchers, and practitioners. Automatic Text Summarization (ATS), a key
Natural Language Processing (NLP) application, automates this process. While
ATS methods exist for many languages, Kurdish remains underdeveloped due to
limited resources. This study develops a dataset and language model based on
231 scientific papers in Sorani Kurdish, collected from four academic
departments in two universities in the Kurdistan Region of Iraq (KRI),
averaging 26 pages per document. Using Sentence Weighting and Term
Frequency-Inverse Document Frequency (TF-IDF) algorithms, two experiments were
conducted, differing in whether the conclusions were included. The average word
count was 5,492.3 in the first experiment and 5,266.96 in the second. Results
were evaluated manually and automatically using ROUGE-1, ROUGE-2, and ROUGE-L
metrics, with the best accuracy reaching 19.58%. Six experts conducted manual
evaluations using three criteria, with results varying by document. This
research provides valuable resources for Kurdish NLP researchers to advance ATS
and related fields.

</details>

### [185] [Harnessing Generative LLMs for Enhanced Financial Event Entity Extraction Performance](https://arxiv.org/abs/2504.14633)
*Soo-joon Choi,Ji-jun Park*

Main category: cs.CL

TLDR: 论文提出了一种基于生成式大语言模型（LLM）的金融事件实体提取方法，通过将任务重构为文本到结构化输出的生成任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 金融文本中复杂的语言结构和长距离依赖关系使得传统序列标注模型在实体提取任务中表现不佳，因此需要更先进的模型来解决这一问题。

Method: 采用参数高效微调（PEFT）技术对预训练的LLM进行微调，直接生成包含实体及其字符跨度的结构化输出（如JSON）。

Result: 在CCKS 2019数据集上，该方法取得了新的最高F1分数，显著优于传统序列标注模型。

Conclusion: 生成式LLM在复杂领域特定信息提取任务中具有巨大潜力，能够有效处理金融文本的复杂性。

Abstract: Financial event entity extraction is a crucial task for analyzing market
dynamics and building financial knowledge graphs, yet it presents significant
challenges due to the specialized language and complex structures in financial
texts. Traditional approaches often rely on sequence labeling models, which can
struggle with long-range dependencies and the inherent complexity of extracting
multiple, potentially overlapping entities. Motivated by the advanced language
understanding and generative capabilities of Large Language Models (LLMs), we
propose a novel method that reframes financial event entity extraction as a
text-to-structured-output generation task. Our approach involves fine-tuning a
pre-trained LLM using Parameter-Efficient Fine-Tuning (PEFT) to directly
generate a structured representation, such as a JSON object, containing the
extracted entities and their precise character spans from the input text. We
evaluate our method on the challenging CCKS 2019 Financial Event Entity
Extraction dataset, comparing its performance against strong sequence labeling
baselines, including SEBERTNets and sebertNets. Experimental results
demonstrate that our generative LLM method achieves a new state-of-the-art F1
score on this benchmark, significantly outperforming previous methods. Through
detailed quantitative analysis across event types, entity types, and instance
complexity, as well as human evaluation, we show that our approach is more
effective at handling the nuances of financial text and extracting high-quality
entities. This work validates the potential of applying generative LLMs
directly to complex, domain-specific information extraction tasks requiring
structured output.

</details>

### [186] [A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs](https://arxiv.org/abs/2504.14657)
*Yihan Lin,Zhirong Bella Yu,Simon Lee*

Main category: cs.CL

TLDR: LLMs生成合成电子健康记录（EHRs）在小规模特征上表现可靠，但在高维数据中难以保持真实分布和相关性，限制了其跨医院泛化能力。


<details>
  <summary>Details</summary>
Motivation: 合成EHRs能保护隐私并支持医疗应用，但需解决跨医院泛化问题。

Method: 评估商用LLMs生成合成数据的能力，分析生成过程的多个方面。

Result: LLMs在小规模特征上表现良好，但在高维数据中表现不佳。

Conclusion: LLMs在生成合成EHRs方面有潜力，但需改进以应对高维数据挑战。

Abstract: Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to
create privacy preserving and harmonized structured data, supporting numerous
applications in healthcare. Key benefits of synthetic data include precise
control over the data schema, improved fairness and representation of patient
populations, and the ability to share datasets without concerns about
compromising real individuals privacy. Consequently, the AI community has
increasingly turned to Large Language Models (LLMs) to generate synthetic data
across various domains. However, a significant challenge in healthcare is
ensuring that synthetic health records reliably generalize across different
hospitals, a long standing issue in the field. In this work, we evaluate the
current state of commercial LLMs for generating synthetic data and investigate
multiple aspects of the generation process to identify areas where these models
excel and where they fall short. Our main finding from this work is that while
LLMs can reliably generate synthetic health records for smaller subsets of
features, they struggle to preserve realistic distributions and correlations as
the dimensionality of the data increases, ultimately limiting their ability to
generalize across diverse hospital settings.

</details>

### [187] [Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data](https://arxiv.org/abs/2504.14669)
*Wei Zou,Sen Yang,Yu Bao,Shujian Huang,Jiajun Chen,Shanbo Cheng*

Main category: cs.CL

TLDR: TRANS-ZERO是一个利用单语数据和LLM内在多语言知识的自对弈框架，通过结合G-MCTS和偏好优化，实现了与监督方法媲美的翻译性能。


<details>
  <summary>Details</summary>
Motivation: 解决多语言机器翻译中数据稀缺和灾难性遗忘的问题。

Method: 提出TRANS-ZERO框架，结合G-MCTS和偏好优化，仅使用单语数据。

Result: 实验表明，该方法在非英语翻译方向上表现优异，性能与基于大规模平行数据的方法相当。

Conclusion: G-MCTS通过迭代翻译探索语义一致的候选，显著提升翻译质量，为框架成功奠定基础。

Abstract: The rise of Large Language Models (LLMs) has reshaped machine translation
(MT), but multilingual MT still relies heavily on parallel data for supervised
fine-tuning (SFT), facing challenges like data scarcity for low-resource
languages and catastrophic forgetting. To address these issues, we propose
TRANS-ZERO, a self-play framework that leverages only monolingual data and the
intrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic
Monte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong
translation performance that rivals supervised methods. Experiments demonstrate
that this approach not only matches the performance of models trained on
large-scale parallel data but also excels in non-English translation
directions. Further analysis reveals that G-MCTS itself significantly enhances
translation quality by exploring semantically consistent candidates through
iterative translations, providing a robust foundation for the framework's
succuss.

</details>

### [188] [FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models](https://arxiv.org/abs/2504.14690)
*Mehrnoush Shamsfard,Zahra Saaberi,Mostafa Karimi manesh,Seyed Mohammad Hossein Hashemi,Zahra Vatankhah,Motahareh Ramezani,Niki Pourazin,Tara Zare,Maryam Azimi,Sarina Chitsaz,Sama Khoraminejad,Morteza Mahdavi Mortazavi,Mohammad Mahdi Chizari,Sahar Maleki,Seyed Soroush Majd,Mostafa Masumi,Sayed Ali Musavi Khoeini,Amir Mohseni,Sogol Alipour*

Main category: cs.CL

TLDR: 本文介绍了FarsEval-PKBETS基准，用于评估波斯语大语言模型（LLMs）的性能，结果显示当前模型的准确率低于50%。


<details>
  <summary>Details</summary>
Motivation: 研究波斯语等资源较少语言的LLMs性能，填补现有研究的空白。

Method: 开发包含4000个问题的FarsEval-PKBETS基准，涵盖多领域任务，并评估三个模型的性能。

Result: 三个模型的平均准确率低于50%，表明当前LLMs在波斯语任务上表现不佳。

Conclusion: 当前LLMs在波斯语任务上仍有显著不足，需进一步改进。

Abstract: Research on evaluating and analyzing large language models (LLMs) has been
extensive for resource-rich languages such as English, yet their performance in
languages such as Persian has received considerably less attention. This paper
introduces FarsEval-PKBETS benchmark, a subset of FarsEval project for
evaluating large language models in Persian. This benchmark consists of 4000
questions and answers in various formats, including multiple choice, short
answer and descriptive responses. It covers a wide range of domains and
tasks,including medicine, law, religion, Persian language, encyclopedic
knowledge, human preferences, social knowledge, ethics and bias, text
generation, and respecting others' rights. This bechmark incorporates
linguistics, cultural, and local considerations relevant to the Persian
language and Iran. To ensure the questions are challenging for current LLMs,
three models -- Llama3-70B, PersianMind, and Dorna -- were evaluated using this
benchmark. Their average accuracy was below 50%, meaning they provided fully
correct answers to fewer than half of the questions. These results indicate
that current language models are still far from being able to solve this
benchmark

</details>

### [189] [OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding](https://arxiv.org/abs/2504.14692)
*Songtao Jiang,Yuan Wang,Sibo Song,Yan Zhang,Zijie Meng,Bohan Lei,Jian Wu,Jimeng Sun,Zuozhu Liu*

Main category: cs.CL

TLDR: OmniV-Med是一个统一的多模态医学理解框架，通过构建综合数据集、设计自适应编码器和引入令牌剪枝机制，显著提升了医学视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型通常对不同模态使用独立编码器，限制了多模态数据的无缝整合。

Method: 构建OmniV-Med-Instruct数据集（252K样本），设计旋转位置自适应编码器，引入医学感知令牌剪枝机制。

Result: OmniV-Med-7B在7个基准测试中达到最优性能，轻量版（1.5B）仅需8块RTX3090 GPU且支持长视频推理。

Conclusion: OmniV-Med通过统一框架和高效设计，显著提升了多模态医学理解的性能和实用性。

Abstract: The practical deployment of medical vision-language models (Med-VLMs)
necessitates seamless integration of textual data with diverse visual
modalities, including 2D/3D images and videos, yet existing models typically
employ separate encoders for different modalities. To address this limitation,
we present OmniV-Med, a unified framework for multimodal medical understanding.
Our technical contributions are threefold: First, we construct
OmniV-Med-Instruct, a comprehensive multimodal medical dataset containing 252K
instructional samples spanning 14 medical image modalities and 11 clinical
tasks. Second, we devise a rotary position-adaptive encoder that processes
multi-resolution 2D/3D images and videos within a unified architecture,
diverging from conventional modality-specific encoders. Third, we introduce a
medical-aware token pruning mechanism that exploits spatial-temporal redundancy
in volumetric data (e.g., consecutive CT slices) and medical videos,
effectively reducing 60\% of visual tokens without performance degradation.
Empirical evaluations demonstrate that OmniV-Med-7B achieves state-of-the-art
performance on 7 benchmarks spanning 2D/3D medical imaging and video
understanding tasks. Notably, our lightweight variant (OmniV-Med-1.5B) attains
comparable performance while requiring only 8 RTX3090 GPUs for training and
supporting efficient long-video inference. Data, code and model will be
released.

</details>

### [190] [Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives](https://arxiv.org/abs/2504.14707)
*Ratna Kandala,Katie Hoemann*

Main category: cs.CL

TLDR: 对比BERTopic、LDA和KMeans在比利时荷兰语日常叙事中的表现，发现BERTopic在语义相关性上优于LDA和KMeans。


<details>
  <summary>Details</summary>
Motivation: 探索BERTopic在形态丰富的语言中的潜力，并对比其与传统方法（LDA和KMeans）的表现。

Method: 使用BERTopic、LDA和KMeans对开放式比利时荷兰语日常叙事进行建模，并通过自动指标和人工评估对比性能。

Result: BERTopic生成的主题更具文化共鸣，而LDA在自动指标上表现较好但语义相关性不足，KMeans表现不如预期。

Conclusion: 强调在NLP模型中结合上下文嵌入和人工评估的重要性，尤其是在形态丰富的语言中。

Abstract: This study explores BERTopic's potential for modeling open-ended Belgian
Dutch daily narratives, contrasting its performance with Latent Dirichlet
Allocation (LDA) and KMeans. Although LDA scores well on certain automated
metrics, human evaluations reveal semantically irrelevant co-occurrences,
highlighting the limitations of purely statistic-based methods. In contrast,
BERTopic's reliance on contextual embeddings yields culturally resonant themes,
underscoring the importance of hybrid evaluation frameworks that account for
morphologically rich languages. KMeans performed less coherently than prior
research suggested, pointing to the unique challenges posed by personal
narratives. Our findings emphasize the need for robust generalization in NLP
models, especially in underrepresented linguistic contexts.

</details>

### [191] [PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines](https://arxiv.org/abs/2504.14738)
*Reya Vir,Shreya Shankar,Harrison Chase,Will Fu-Hinthorn,Aditya Parameswaran*

Main category: cs.CL

TLDR: PROMPTEVALS数据集包含2087个LLM管道提示和12623个断言标准，用于提升LLM在生成任务中的可靠性。微调的Mistral和Llama 3模型在生成相关断言时优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: LLM在生产环境中常无法满足开发者期望，需要可靠的断言机制来确保输出质量。

Method: 通过开源工具收集开发者提供的提示和断言标准，构建PROMPTEVALS数据集，并评估不同模型生成断言的能力。

Result: 微调的Mistral和Llama 3模型比GPT-4o平均提升20.93%的性能，同时降低延迟。

Conclusion: PROMPTEVALS数据集为LLM可靠性、对齐和提示工程研究提供了重要资源。

Abstract: Large language models (LLMs) are increasingly deployed in specialized
production data processing pipelines across diverse domains -- such as finance,
marketing, and e-commerce. However, when running them in production across many
inputs, they often fail to follow instructions or meet developer expectations.
To improve reliability in these applications, creating assertions or guardrails
for LLM outputs to run alongside the pipelines is essential. Yet, determining
the right set of assertions that capture developer requirements for a task is
challenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM
pipeline prompts with 12623 corresponding assertion criteria, sourced from
developers using our open-source LLM pipeline tools. This dataset is 5x larger
than previous collections. Using a hold-out test split of PROMPTEVALS as a
benchmark, we evaluated closed- and open-source models in generating relevant
assertions. Notably, our fine-tuned Mistral and Llama 3 models outperform
GPT-4o by 20.93% on average, offering both reduced latency and improved
performance. We believe our dataset can spur further research in LLM
reliability, alignment, and prompt engineering.

</details>

### [192] [Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings](https://arxiv.org/abs/2504.14766)
*Saniya Karwa,Navpreet Singh*

Main category: cs.CL

TLDR: 该论文提出了一种框架，用于揭示BERT等高维不透明模型中向量嵌入的特定维度如何编码不同语言属性（LPs），并引入了一个新数据集LDSP-10和度量标准EDI。


<details>
  <summary>Details</summary>
Motivation: 理解BERT等高维不透明模型的内部工作机制，尤其是其向量嵌入如何编码语言属性，是一个挑战。

Method: 使用LDSP-10数据集（包含10种关键语言特征），结合Wilcoxon符号秩检验、互信息和递归特征消除等方法，分析BERT嵌入，并提出了新的度量标准EDI。

Result: 研究发现某些语言属性（如否定和极性）在特定维度中编码较强，而其他属性（如同义性）则表现出更复杂的模式。

Conclusion: 该研究为嵌入的可解释性提供了见解，有助于开发更透明和优化的语言模型，并对模型偏见缓解和AI系统的负责任部署具有意义。

Abstract: Understanding the inner workings of neural embeddings, particularly in models
such as BERT, remains a challenge because of their high-dimensional and opaque
nature. This paper proposes a framework for uncovering the specific dimensions
of vector embeddings that encode distinct linguistic properties (LPs). We
introduce the Linguistically Distinct Sentence Pairs (LDSP-10) dataset, which
isolates ten key linguistic features such as synonymy, negation, tense, and
quantity. Using this dataset, we analyze BERT embeddings with various methods,
including the Wilcoxon signed-rank test, mutual information, and recursive
feature elimination, to identify the most influential dimensions for each LP.
We introduce a new metric, the Embedding Dimension Impact (EDI) score, which
quantifies the relevance of each embedding dimension to a LP. Our findings show
that certain properties, such as negation and polarity, are robustly encoded in
specific dimensions, while others, like synonymy, exhibit more complex
patterns. This study provides insights into the interpretability of embeddings,
which can guide the development of more transparent and optimized language
models, with implications for model bias mitigation and the responsible
deployment of AI systems.

</details>

### [193] [Knowledge Distillation and Dataset Distillation of Large Language Models: Emerging Trends, Challenges, and Future Directions](https://arxiv.org/abs/2504.14772)
*Luyang Fang,Xiaowei Yu,Jiazhang Cai,Yongkai Chen,Shushan Wu,Zhengliang Liu,Zhenyuan Yang,Haoran Lu,Xilin Gong,Yufang Liu,Terry Ma,Wei Ruan,Ali Abbasi,Jing Zhang,Tao Wang,Ehsan Latif,Wei Liu,Wei Zhang,Soheil Kolouri,Xiaoming Zhai,Dajiang Zhu,Wenxuan Zhong,Tianming Liu,Ping Ma*

Main category: cs.CL

TLDR: 综述分析了知识蒸馏（KD）和数据集蒸馏（DD）两种互补范式，旨在压缩大型语言模型（LLM）同时保留其推理能力和语言多样性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM计算和数据需求激增的问题，探索高效压缩策略。

Method: 研究了KD（任务对齐、多教师框架等）和DD（梯度匹配、生成合成等）的关键方法，并探讨两者的整合。

Result: KD和DD的结合提供了更高效、可扩展的压缩策略，适用于医疗和教育等领域。

Conclusion: 尽管进展显著，仍需解决推理多样性、适应性和评估等问题，未来需进一步整合KD和DD以实现可持续LLM。

Abstract: The exponential growth of Large Language Models (LLMs) continues to highlight
the need for efficient strategies to meet ever-expanding computational and data
demands. This survey provides a comprehensive analysis of two complementary
paradigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both
aimed at compressing LLMs while preserving their advanced reasoning
capabilities and linguistic diversity. We first examine key methodologies in
KD, such as task-specific alignment, rationale-based training, and
multi-teacher frameworks, alongside DD techniques that synthesize compact,
high-impact datasets through optimization-based gradient matching, latent space
regularization, and generative synthesis. Building on these foundations, we
explore how integrating KD and DD can produce more effective and scalable
compression strategies. Together, these approaches address persistent
challenges in model scalability, architectural heterogeneity, and the
preservation of emergent LLM abilities. We further highlight applications
across domains such as healthcare and education, where distillation enables
efficient deployment without sacrificing performance. Despite substantial
progress, open challenges remain in preserving emergent reasoning and
linguistic diversity, enabling efficient adaptation to continually evolving
teacher models and datasets, and establishing comprehensive evaluation
protocols. By synthesizing methodological innovations, theoretical foundations,
and practical insights, our survey charts a path toward sustainable,
resource-efficient LLMs through the tighter integration of KD and DD
principles.

</details>

### [194] [Automatic Evaluation Metrics for Document-level Translation: Overview, Challenges and Trends](https://arxiv.org/abs/2504.14804)
*Jiaxin GUO,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Zongyao Li,Hengchao Shang,Daimeng Wei,Hao Yang*

Main category: cs.CL

TLDR: 本文探讨了文档级机器翻译的自动评估现状、挑战及未来趋势，提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 文档级翻译质量评估的迫切需求及其对翻译系统改进的指导作用。

Method: 分析现有自动评估方案（包括有无参考文本的方法、传统指标、模型和LLM指标），探讨其挑战。

Result: 当前评估方法存在参考多样性不足、依赖句子对齐信息、LLM评估偏见等问题。

Conclusion: 未来需开发更友好的文档级评估方法、更稳健的LLM评估方法，并减少对句子级信息的依赖。

Abstract: With the rapid development of deep learning technologies, the field of
machine translation has witnessed significant progress, especially with the
advent of large language models (LLMs) that have greatly propelled the
advancement of document-level translation. However, accurately evaluating the
quality of document-level translation remains an urgent issue. This paper first
introduces the development status of document-level translation and the
importance of evaluation, highlighting the crucial role of automatic evaluation
metrics in reflecting translation quality and guiding the improvement of
translation systems. It then provides a detailed analysis of the current state
of automatic evaluation schemes and metrics, including evaluation methods with
and without reference texts, as well as traditional metrics, Model-based
metrics and LLM-based metrics. Subsequently, the paper explores the challenges
faced by current evaluation methods, such as the lack of reference diversity,
dependence on sentence-level alignment information, and the bias, inaccuracy,
and lack of interpretability of the LLM-as-a-judge method. Finally, the paper
looks ahead to the future trends in evaluation methods, including the
development of more user-friendly document-level evaluation methods and more
robust LLM-as-a-judge methods, and proposes possible research directions, such
as reducing the dependency on sentence-level information, introducing
multi-level and multi-granular evaluation approaches, and training models
specifically for machine translation evaluation. This study aims to provide a
comprehensive analysis of automatic evaluation for document-level translation
and offer insights into future developments.

</details>

### [195] [On Self-improving Token Embeddings](https://arxiv.org/abs/2504.14808)
*Mario M. Kubek,Shiraj Pokharel,Thomas Böhme,Emma L. McDaniel,Herwig Unger,Armin R. Mikler*

Main category: cs.CL

TLDR: 本文提出了一种快速优化预训练静态词嵌入的新方法，通过结合邻近词的嵌入动态更新表示，解决了词汇外问题，适用于特定领域的语料库。


<details>
  <summary>Details</summary>
Motivation: 解决预训练词嵌入在特定领域语料中的局限性，尤其是词汇外问题和通用嵌入的不适应性。

Method: 利用邻近词的嵌入动态更新每个词的表示，包括未预分配嵌入的词，独立于大型语言模型和浅层神经网络。

Result: 在特定领域语料中生成更有意义的词嵌入，应用于风暴事件分析时改进了相关术语的表示。

Conclusion: 该方法为特定领域语料提供了高效的词嵌入优化方案，适用于多种应用场景。

Abstract: This article introduces a novel and fast method for refining pre-trained
static word or, more generally, token embeddings. By incorporating the
embeddings of neighboring tokens in text corpora, it continuously updates the
representation of each token, including those without pre-assigned embeddings.
This approach effectively addresses the out-of-vocabulary problem, too.
Operating independently of large language models and shallow neural networks,
it enables versatile applications such as corpus exploration, conceptual
search, and word sense disambiguation. The method is designed to enhance token
representations within topically homogeneous corpora, where the vocabulary is
restricted to a specific domain, resulting in more meaningful embeddings
compared to general-purpose pre-trained vectors. As an example, the methodology
is applied to explore storm events and their impacts on infrastructure and
communities using narratives from a subset of the NOAA Storm Events database.
The article also demonstrates how the approach improves the representation of
storm-related terms over time, providing valuable insights into the evolving
nature of disaster narratives.

</details>

### [196] [Transparentize the Internal and External Knowledge Utilization in LLMs with Trustworthy Citation](https://arxiv.org/abs/2504.14856)
*Jiajun Shen,Tong Zhou,Yubo Chen,Delai Qiu,Shengping Liu,Kang Liu,Jun Zhao*

Main category: cs.CL

TLDR: 本文提出了一种结合外部和内部知识的引用生成任务，并设计了RAEL范式和INTRALIGN方法，以提高生成答案的可信度。


<details>
  <summary>Details</summary>
Motivation: 尽管检索增强生成和引用生成可以缓解大语言模型的幻觉问题，但其内部知识的利用仍不透明，生成答案的可信度存疑。

Method: 引入Context-Prior Augmented Citation Generation任务，设计RAEL范式和INTRALIGN方法，结合外部和内部知识生成可信引用。

Result: 实验表明，该方法在跨场景性能上优于基线，且检索质量、问题类型和模型知识对引用可信度有显著影响。

Conclusion: 通过结合外部和内部知识，该方法提升了引用生成的可信度，为未来研究提供了新方向。

Abstract: While hallucinations of large language models could been alleviated through
retrieval-augmented generation and citation generation, how the model utilizes
internal knowledge is still opaque, and the trustworthiness of its generated
answers remains questionable. In this work, we introduce Context-Prior
Augmented Citation Generation task, requiring models to generate citations
considering both external and internal knowledge while providing trustworthy
references, with 5 evaluation metrics focusing on 3 aspects: answer
helpfulness, citation faithfulness, and trustworthiness. We introduce RAEL, the
paradigm for our task, and also design INTRALIGN, an integrated method
containing customary data generation and an alignment algorithm. Our
experimental results show that our method achieves a better cross-scenario
performance with regard to other baselines. Our extended experiments further
reveal that retrieval quality, question types, and model knowledge have
considerable influence on the trustworthiness in citation generation.

</details>

### [197] [Natural Fingerprints of Large Language Models](https://arxiv.org/abs/2504.14871)
*Teppei Suzuki,Ryokan Ri,Sho Takase*

Main category: cs.CL

TLDR: 研究发现，即使大型语言模型（LLM）使用相同训练数据，其生成的文本仍能通过自然指纹区分来源，这些指纹源于训练过程中的细微差异。


<details>
  <summary>Details</summary>
Motivation: 探究LLM输出中可识别特征的成因，以理解无意偏见的来源并改进模型行为控制。

Method: 通过系统控制训练条件（如参数大小、优化设置和随机种子），分析不同LLM生成的文本特征。

Result: 发现自然指纹的存在，表明训练过程的细微差异会导致模型输出的独特特征。

Conclusion: 理解自然指纹有助于揭示无意偏见的起源，并为改进LLM行为控制提供新思路。

Abstract: Large language models (LLMs) often exhibit biases -- systematic deviations
from expected norms -- in their outputs. These range from overt issues, such as
unfair responses, to subtler patterns that can reveal which model produced
them. We investigate the factors that give rise to identifiable characteristics
in LLMs. Since LLMs model training data distribution, it is reasonable that
differences in training data naturally lead to the characteristics. However,
our findings reveal that even when LLMs are trained on the exact same data, it
is still possible to distinguish the source model based on its generated text.
We refer to these unintended, distinctive characteristics as natural
fingerprints. By systematically controlling training conditions, we show that
the natural fingerprints can emerge from subtle differences in the training
process, such as parameter sizes, optimization settings, and even random seeds.
We believe that understanding natural fingerprints offers new insights into the
origins of unintended bias and ways for improving control over LLM behavior.

</details>

### [198] [Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2504.14891)
*Aoran Gan,Hao Yu,Kai Zhang,Qi Liu,Wenyu Yan,Zhenya Huang,Shiwei Tong,Guoping Hu*

Main category: cs.CL

TLDR: 本文综述了检索增强生成（RAG）系统的评估方法，整合了传统与新兴方法，并分析了数据集与框架。


<details>
  <summary>Details</summary>
Motivation: RAG系统在自然语言处理中表现优异，但其混合架构和动态知识依赖带来了评估挑战。

Method: 系统回顾了RAG评估方法，包括性能、准确性、安全性和效率，并进行了元分析。

Result: 总结了RAG评估的数据集与框架，填补了传统与LLM驱动方法之间的空白。

Conclusion: 本文为RAG评估提供了全面资源，推动了该领域的发展。

Abstract: Recent advancements in Retrieval-Augmented Generation (RAG) have
revolutionized natural language processing by integrating Large Language Models
(LLMs) with external information retrieval, enabling accurate, up-to-date, and
verifiable text generation across diverse applications. However, evaluating RAG
systems presents unique challenges due to their hybrid architecture that
combines retrieval and generation components, as well as their dependence on
dynamic knowledge sources in the LLM era. In response, this paper provides a
comprehensive survey of RAG evaluation methods and frameworks, systematically
reviewing traditional and emerging evaluation approaches, for system
performance, factual accuracy, safety, and computational efficiency in the LLM
era. We also compile and categorize the RAG-specific datasets and evaluation
frameworks, conducting a meta-analysis of evaluation practices in high-impact
RAG research. To the best of our knowledge, this work represents the most
comprehensive survey for RAG evaluation, bridging traditional and LLM-driven
methods, and serves as a critical resource for advancing RAG development.

</details>

### [199] [CRAVE: A Conflicting Reasoning Approach for Explainable Claim Verification Using LLMs](https://arxiv.org/abs/2504.14905)
*Yingming Zheng,Xiaoliang Liu,Peng Wu,Li Pan*

Main category: cs.CL

TLDR: CRAVE是一种基于冲突推理的自动声明验证方法，利用大型语言模型（LLMs）和小型语言模型（SLMs）提高复杂声明的验证准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 数字媒体和AI生成内容导致错误信息快速传播，传统依赖专家标注证据的方法效率低且不可扩展，现有自动化系统难以处理需要复杂推理的声明。

Method: CRAVE采用三模块框架：1) 消除歧义并检索证据；2) 利用LLMs从四个维度推理冲突立场；3) 使用SLM评估冲突立场并做出最终判断。

Result: 在两个公共数据集上，CRAVE优于现有方法，表现出更高的证据检索能力和预测解释性。

Conclusion: CRAVE通过冲突推理显著提升了复杂声明的验证准确性和透明度。

Abstract: The rapid spread of misinformation, driven by digital media and AI-generated
content, has made automatic claim verification essential. Traditional methods,
which depend on expert-annotated evidence, are labor-intensive and not
scalable. Although recent automated systems have improved, they still struggle
with complex claims that require nuanced reasoning. To address this, we propose
CRAVE, a Conflicting Reasoning Approach for explainable claim VErification,
that verify the complex claims based on the conflicting rationales reasoned by
large language models (LLMs). Specifically, CRAVE introduces a three-module
framework. Ambiguity Elimination enchanced Evidence Retrieval module performs
ambiguity elimination and entity-based search to gather relevant evidence
related to claim verification from external sources like Wikipedia. Conflicting
Perspective Reasoning and Preliminary Judgment module with LLMs adopts LLMs to
reason rationales with conflicting stances about claim verification from
retrieved evidence across four dimensions, i.e., direct evidence, semantic
relationships, linguistic patterns, and logical reasoning and make a
preliminary judgment. Finally, Small Language Model (SLM) based Judge module is
fine-tuned to make use of preliminary judgment from LLMs to assess the
confidence of the conflicting rationales and make a final authenticity
judgment. This methodology allows CRAVE to capture subtle inconsistencies in
complex claims, improving both the accuracy and transparency of claim
verification. Extensive experiments on two public claim verification datasets
demonstrate that our CRAVE model achieves much better performance than
state-of-the-art methods and exhibits a superior capacity for finding relevant
evidence and explaining the model predictions. The code is provided at
https://github.com/8zym/CRAVE.

</details>

### [200] [Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues](https://arxiv.org/abs/2504.14963)
*Rui Ribeiro,Luísa Coheur,Joao P. Carvalho*

Main category: cs.CL

TLDR: 本文探讨了使用模糊指纹和大预训练模型改进基于文本的说话人识别，结合说话人特定标记和上下文感知建模，显著提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在仅基于文本的说话人识别中表现有限，本文旨在探索更有效的方法。

Method: 采用模糊指纹技术，结合说话人特定标记和上下文感知建模，并在两个数据集上验证。

Result: 在Friends和Big Bang Theory数据集上分别达到70.6%和67.7%的准确率，模糊指纹接近全微调性能。

Conclusion: 模糊指纹和上下文建模显著提升文本说话人识别，同时分析了模糊语句并提出改进方向。

Abstract: Speaker identification using voice recordings leverages unique acoustic
features, but this approach fails when only textual data is available. Few
approaches have attempted to tackle the problem of identifying speakers solely
from text, and the existing ones have primarily relied on traditional methods.
In this work, we explore the use of fuzzy fingerprints from large pre-trained
models to improve text-based speaker identification. We integrate
speaker-specific tokens and context-aware modeling, demonstrating that
conversational context significantly boosts accuracy, reaching 70.6% on the
Friends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show
that fuzzy fingerprints can approximate full fine-tuning performance with fewer
hidden units, offering improved interpretability. Finally, we analyze ambiguous
utterances and propose a mechanism to detect speaker-agnostic lines. Our
findings highlight key challenges and provide insights for future improvements
in text-based speaker identification.

</details>

### [201] [Evaluating LLMs on Chinese Topic Constructions: A Research Proposal Inspired by Tian et al. (2024)](https://arxiv.org/abs/2504.14969)
*Xiaodong Yang*

Main category: cs.CL

TLDR: 本文提出一个评估大语言模型（LLMs）在中文话题结构中的框架，重点关注其对孤岛约束的敏感性。


<details>
  <summary>Details</summary>
Motivation: 受Tian等人（2024）启发，旨在测试LLMs对汉语语法的知识，为未来研究提供基础。

Method: 提出实验设计，但尚未进行实验，邀请对方法论的反馈。

Result: 暂无实验结果，仅提出研究框架。

Conclusion: 本文为未来研究奠定基础，并希望进一步完善方法论。

Abstract: This paper proposes a framework for evaluating large language models (LLMs)
on Chinese topic constructions, focusing on their sensitivity to island
constraints. Drawing inspiration from Tian et al. (2024), we outline an
experimental design for testing LLMs' grammatical knowledge of Mandarin syntax.
While no experiments have been conducted yet, this proposal aims to provide a
foundation for future studies and invites feedback on the methodology.

</details>

### [202] [Efficient Pretraining Length Scaling](https://arxiv.org/abs/2504.14992)
*Bohong Wu,Shen Yan,Sijun Zhang,Jianqiao Lu,Yutao Zeng,Ya Wang,Xun Zhou*

Main category: cs.CL

TLDR: 提出了一种名为PHD-Transformer的新框架，通过创新的KV缓存管理策略，在预训练中实现高效的长度扩展，同时保持推理效率。


<details>
  <summary>Details</summary>
Motivation: 探索长度扩展在预训练中的潜力，解决现有方法在预训练中长度扩展效率不足的问题。

Method: 采用区分原始令牌和隐藏解码令牌的KV缓存管理策略，并引入两种优化变体（PHD-SWA和PHD-CSWA）以进一步提升性能。

Result: 在多个基准测试中表现出一致的改进。

Conclusion: PHD-Transformer在预训练中实现了高效的长度扩展，同时保持了推理效率，为相关研究提供了新思路。

Abstract: Recent advances in large language models have demonstrated the effectiveness
of length scaling during post-training, yet its potential in pre-training
remains underexplored. We present the Parallel Hidden Decoding Transformer
(\textit{PHD}-Transformer), a novel framework that enables efficient length
scaling during pre-training while maintaining inference efficiency.
\textit{PHD}-Transformer achieves this through an innovative KV cache
management strategy that distinguishes between original tokens and hidden
decoding tokens. By retaining only the KV cache of original tokens for
long-range dependencies while immediately discarding hidden decoding tokens
after use, our approach maintains the same KV cache size as the vanilla
transformer while enabling effective length scaling. To further enhance
performance, we introduce two optimized variants: \textit{PHD-SWA} employs
sliding window attention to preserve local dependencies, while
\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate
linear growth in pre-filling time. Extensive experiments demonstrate consistent
improvements across multiple benchmarks.

</details>

### [203] [Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation with LLMs](https://arxiv.org/abs/2504.15013)
*Yow-Fu Liou,Yu-Chien Tang,An-Zi Yen*

Main category: cs.CL

TLDR: 该研究探讨了利用大型语言模型（LLMs）自动化生成教育材料和课程建议的潜力，通过实验验证了生成内容的高质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 教育材料的制作对教师来说是耗时且高要求的任务，研究旨在通过LLMs简化这一过程，提供更丰富的学习资源。

Method: 从视频转录生成扩展文章，利用LLMs添加历史、文化和轶事内容，并通过语义相似度排名和LLM优化推荐相关课程。

Result: 实验评估显示，模型生成的内容质量高，课程建议准确，提升了学习的吸引力和可访问性。

Conclusion: LLMs能够有效连接核心内容与补充学习资源，既辅助教师设计材料，又为学生提供额外资源。

Abstract: The process of creating educational materials is both time-consuming and
demanding for educators. This research explores the potential of Large Language
Models (LLMs) to streamline this task by automating the generation of extended
reading materials and relevant course suggestions. Using the TED-Ed Dig Deeper
sections as an initial exploration, we investigate how supplementary articles
can be enriched with contextual knowledge and connected to additional learning
resources. Our method begins by generating extended articles from video
transcripts, leveraging LLMs to include historical insights, cultural examples,
and illustrative anecdotes. A recommendation system employing semantic
similarity ranking identifies related courses, followed by an LLM-based
refinement process to enhance relevance. The final articles are tailored to
seamlessly integrate these recommendations, ensuring they remain cohesive and
informative. Experimental evaluations demonstrate that our model produces
high-quality content and accurate course suggestions, assessed through metrics
such as Hit Rate, semantic similarity, and coherence. Our experimental analysis
highlight the nuanced differences between the generated and existing materials,
underscoring the model's capacity to offer more engaging and accessible
learning experiences. This study showcases how LLMs can bridge the gap between
core content and supplementary learning, providing students with additional
recommended resources while also assisting teachers in designing educational
materials.

</details>

### [204] [LLMs as Data Annotators: How Close Are We to Human Performance](https://arxiv.org/abs/2504.15022)
*Muhammad Uzair Ul Haq,Davide Rigoni,Alessandro Sperduti*

Main category: cs.CL

TLDR: 论文探讨了在NLP中利用LLMs自动化数据标注的挑战，提出了一种基于检索的方法来优化上下文示例选择，并通过实验比较了不同LLMs和嵌入模型在NER任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 手动标注数据成本高且效率低，而现有的ICL方法在上下文示例选择上存在不足，因此需要更高效的自动化解决方案。

Method: 通过实验比较了不同规模的LLMs（7B和70B参数）和嵌入模型在NER任务中的表现，并引入基于检索的方法优化上下文示例选择。

Result: 结果显示选择合适的LLM和嵌入模型至关重要，同时揭示了模型规模与性能之间的权衡，并指出需要更多研究关注更具挑战性的数据集。

Conclusion: 论文强调了自动化上下文示例选择的重要性，并呼吁未来研究应关注更复杂的数据集和模型优化。

Abstract: In NLP, fine-tuning LLMs is effective for various applications but requires
high-quality annotated data. However, manual annotation of data is
labor-intensive, time-consuming, and costly. Therefore, LLMs are increasingly
used to automate the process, often employing in-context learning (ICL) in
which some examples related to the task are given in the prompt for better
performance. However, manually selecting context examples can lead to
inefficiencies and suboptimal model performance. This paper presents
comprehensive experiments comparing several LLMs, considering different
embedding models, across various datasets for the Named Entity Recognition
(NER) task. The evaluation encompasses models with approximately $7$B and $70$B
parameters, including both proprietary and non-proprietary models. Furthermore,
leveraging the success of Retrieval-Augmented Generation (RAG), it also
considers a method that addresses the limitations of ICL by automatically
retrieving contextual examples, thereby enhancing performance. The results
highlight the importance of selecting the appropriate LLM and embedding model,
understanding the trade-offs between LLM sizes and desired performance, and the
necessity to direct research efforts towards more challenging datasets.

</details>

### [205] [DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models](https://arxiv.org/abs/2504.15027)
*Chengyu Wang,Junbing Yan,Yuanhao Yue,Jun Huang*

Main category: cs.CL

TLDR: DistilQwen2.5是一系列从Qwen2.5模型蒸馏而来的轻量级大语言模型，通过多教师知识蒸馏和模型融合技术提升指令跟随能力，显著优于原始模型。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在资源受限场景下的计算效率和部署成本问题。

Method: 利用多教师LLM选择和优化指令-响应对，通过标准微调和模型融合技术逐步整合教师模型的细粒度知识。

Result: 蒸馏后的模型在能力上显著优于原始模型。

Conclusion: DistilQwen2.5模型在工业实践中表现出色，并已开源供社区使用。

Abstract: Enhancing computational efficiency and reducing deployment costs for large
language models (LLMs) have become critical challenges in various
resource-constrained scenarios. In this work, we present DistilQwen2.5, a
family of distilled, lightweight LLMs derived from the public Qwen2.5 models.
These distilled models exhibit enhanced instruction-following capabilities
compared to the original models based on a series of distillation techniques
that incorporate knowledge from much larger LLMs. In our industrial practice,
we first leverage powerful proprietary LLMs with varying capacities as
multi-agent teachers to select, rewrite, and refine instruction-response pairs
that are more suitable for student LLMs to learn. After standard fine-tuning,
we further leverage a computationally efficient model fusion approach that
enables student models to progressively integrate fine-grained hidden knowledge
from their teachers. Experimental evaluations demonstrate that the distilled
models possess significantly stronger capabilities than their original
checkpoints. Additionally, we present use cases to illustrate the applications
of our framework in real-world scenarios. To facilitate practical use, we have
released all the DistilQwen2.5 models to the open-source community.

</details>

### [206] [RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search](https://arxiv.org/abs/2504.15047)
*Quy-Anh Dang,Chris Ngo,Truong-Son Hy*

Main category: cs.CL

TLDR: RainbowPlus是一种基于进化计算的新型红队框架，通过自适应质量多样性搜索增强对抗性提示生成，显著提高了攻击成功率和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有红队方法存在可扩展性差、资源密集或攻击策略多样性不足的问题，需要一种更高效且多样化的解决方案。

Method: RainbowPlus采用多元素存档存储多样化高质量提示，并通过综合适应度函数评估多个提示，改进了传统质量多样性方法。

Result: 实验表明，RainbowPlus在攻击成功率和多样性上优于现有方法，生成更多独特提示，且速度更快。

Conclusion: RainbowPlus为LLM安全提供了可扩展的漏洞评估工具，推动了红队研究的进一步发展。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but are
susceptible to adversarial prompts that exploit vulnerabilities to produce
unsafe or biased outputs. Existing red-teaming methods often face scalability
challenges, resource-intensive requirements, or limited diversity in attack
strategies. We propose RainbowPlus, a novel red-teaming framework rooted in
evolutionary computation, enhancing adversarial prompt generation through an
adaptive quality-diversity (QD) search that extends classical evolutionary
algorithms like MAP-Elites with innovations tailored for language models. By
employing a multi-element archive to store diverse high-quality prompts and a
comprehensive fitness function to evaluate multiple prompts concurrently,
RainbowPlus overcomes the constraints of single-prompt archives and pairwise
comparisons in prior QD methods like Rainbow Teaming. Experiments comparing
RainbowPlus to QD methods across six benchmark datasets and four open-source
LLMs demonstrate superior attack success rate (ASR) and diversity
(Diverse-Score $\approx 0.84$), generating up to 100 times more unique prompts
(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine
state-of-the-art methods on the HarmBench dataset with twelve LLMs (ten
open-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,
surpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).
Our open-source implementation fosters further advancements in LLM safety,
offering a scalable tool for vulnerability assessment. Code and resources are
publicly available at https://github.com/knoveleng/rainbowplus, supporting
reproducibility and future research in LLM red-teaming.

</details>

### [207] [Testing LLMs' Capabilities in Annotating Translations Based on an Error Typology Designed for LSP Translation: First Experiments with ChatGPT](https://arxiv.org/abs/2504.15052)
*Joachim Minder,Guillaume Wisniewski,Natalie Kübler*

Main category: cs.CL

TLDR: 研究探讨了ChatGPT在基于错误类型标注机器翻译输出时的能力，发现其在特定领域翻译中表现良好，但自我评估能力有限。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（如ChatGPT）在专业翻译错误标注中的潜力，填补以往研究主要关注通用语言的空白。

Method: 通过两种不同提示和自定义错误类型，比较ChatGPT与人类专家对DeepL和ChatGPT自身翻译的标注结果。

Result: ChatGPT在标注DeepL翻译时召回率和精确度较高，但自我评估表现较差；提示的详细程度影响分类准确性。

Conclusion: 研究揭示了LLMs在翻译评估中的潜力与局限，为未来开源模型研究和翻译教学应用提供了方向。

Abstract: This study investigates the capabilities of large language models (LLMs),
specifically ChatGPT, in annotating MT outputs based on an error typology. In
contrast to previous work focusing mainly on general language, we explore
ChatGPT's ability to identify and categorise errors in specialised
translations. By testing two different prompts and based on a customised error
typology, we compare ChatGPT annotations with human expert evaluations of
translations produced by DeepL and ChatGPT itself. The results show that, for
translations generated by DeepL, recall and precision are quite high. However,
the degree of accuracy in error categorisation depends on the prompt's specific
features and its level of detail, ChatGPT performing very well with a detailed
prompt. When evaluating its own translations, ChatGPT achieves significantly
poorer results, revealing limitations with self-assessment. These results
highlight both the potential and the limitations of LLMs for translation
evaluation, particularly in specialised domains. Our experiments pave the way
for future research on open-source LLMs, which could produce annotations of
comparable or even higher quality. In the future, we also aim to test the
practical effectiveness of this automated evaluation in the context of
translation training, particularly by optimising the process of human
evaluation by teachers and by exploring the impact of annotations by LLMs on
students' post-editing and translation learning.

</details>

### [208] [Rethinking the Potential of Multimodality in Collaborative Problem Solving Diagnosis with Large Language Models](https://arxiv.org/abs/2504.15093)
*K. Wong,B. Wu,S. Bulathwela,M. Cukurova*

Main category: cs.CL

TLDR: 研究探讨了多模态数据在诊断学生协作问题解决（CPS）能力中的潜力，发现多模态与特定建模技术的结合对某些CPS子技能有改进效果，但并非普遍适用。


<details>
  <summary>Details</summary>
Motivation: 探索多模态数据和先进模型在检测复杂CPS行为中的实际价值，填补现有实证研究的不足。

Method: 使用文本嵌入和声学嵌入构建多模态分类模型，比较单模态和多模态基于Transformer的模型与传统模型的性能。

Result: 多模态在Transformer模型中提升了社交认知类CPS的诊断性能，但对传统单模态模型无显著改进。

Conclusion: 多模态和模型选择需根据具体CPS指标类型和数据集特性灵活应用，强调人机互补及进一步探索模型架构的必要性。

Abstract: Detecting collaborative and problem-solving behaviours from digital traces to
interpret students' collaborative problem solving (CPS) competency is a
long-term goal in the Artificial Intelligence in Education (AIEd) field.
Although multimodal data and advanced models are argued to have the potential
to detect complex CPS behaviours, empirical evidence on their value remains
limited with some contrasting evidence. In this study, we investigated the
potential of multimodal data to improve model performance in diagnosing 78
secondary school students' CPS subskills and indicators in authentic
educational settings. In particular, text embeddings from verbal data and
acoustic embeddings from audio data were used in a multimodal classification
model for CPS diagnosis. Both unimodal and multimodal transformer-based models
outperformed traditional models in detecting CPS classes. Although the
inclusion of multimodality did not improve the performance of traditional
unimodal models, its integration into transformer-based models demonstrated
improved performance for diagnosing social-cognitive CPS classes compared to
unimodal transformer-based models. Based on the results, the paper argues that
multimodality and the selection of a particular modelling technique should not
be taken for granted to achieve the best performance in the automated detection
of every CPS subskill and indicator. Rather, their value is limited to certain
types of CPS indicators, affected by the complexity of the labels, and
dependent on the composition of indicators in the dataset. We conclude the
paper by discussing the required nuance when considering the value of LLMs and
multimodality in automated CPS diagnosis, highlighting the need for human-AI
complementarity, and proposing the exploration of relevant model architectures
and techniques to improve CPS diagnosis in authentic educational contexts.

</details>

### [209] [Kuwain 1.5B: An Arabic SLM via Language Injection](https://arxiv.org/abs/2504.15120)
*Khalil Hennara,Sara Chrouf,Mohamed Motaism Hamed,Zeina Aldallal,Omar Hadid,Safwan AlModhayan*

Main category: cs.CL

TLDR: 本文提出了一种将新语言整合到大型语言模型（LLM）中的方法，成功将阿拉伯语注入小型开源模型Kuwain（15亿参数），性能提升8%，同时保留原有知识。


<details>
  <summary>Details</summary>
Motivation: 增强现有模型以融入新知识是AI发展的关键，本文旨在探索如何高效地将新语言整合到LLM中，避免资源密集型训练。

Method: 通过向以英语为主的小型开源模型注入阿拉伯语数据，训练出15亿参数的Kuwain模型，最小化对原有数据的依赖。

Result: 阿拉伯语性能平均提升8%，同时保留原有知识，提供了一种成本效益高的双语模型替代方案。

Conclusion: 该方法展示了无需大规模重新训练即可高效扩展语言模型的潜力。

Abstract: Enhancing existing models with new knowledge is a crucial aspect of AI
development. This paper introduces a novel method for integrating a new
language into a large language model (LLM). Our approach successfully
incorporates a previously unseen target language into an existing LLM without
compromising its prior knowledge. We trained a tiny model with 1.5 billion
parameters named Kuwain by injecting the Arabic language into a small
open-source model mainly trained in English. Our method demonstrates
significant improvements in Arabic language performance, with an average 8%
improvement across various benchmarks, while retaining the model's existing
knowledge with a minimum amount of the original model's data. This offers a
cost-effective alternative to training a comprehensive model in both English
and Arabic. The results highlight the potential for efficient, targeted
language model expansion without extensive retraining or resource-intensive
processes.

</details>

### [210] [EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models](https://arxiv.org/abs/2504.15133)
*Ziwen Xu,Shuxun Wang,Kewei Xu,Haoming Xu,Mengru Wang,Xinle Deng,Yunzhi Yao,Guozhou Zheng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TLDR: EasyEdit2是一个框架，支持通过插件式调整控制大型语言模型（LLM）的行为，无需修改模型参数。


<details>
  <summary>Details</summary>
Motivation: 提供一种无需技术背景即可精确控制LLM行为的简单方法。

Method: 采用新架构，包括导向向量生成器和应用器，通过单一样本自动生成和应用导向向量。

Result: 在不同LLM上验证了其有效性，实现了高效且精确的行为控制。

Conclusion: EasyEdit2简化了LLM行为调整，适合非技术用户使用。

Abstract: In this paper, we introduce EasyEdit2, a framework designed to enable
plug-and-play adjustability for controlling Large Language Model (LLM)
behaviors. EasyEdit2 supports a wide range of test-time interventions,
including safety, sentiment, personality, reasoning patterns, factuality, and
language features. Unlike its predecessor, EasyEdit2 features a new
architecture specifically designed for seamless model steering. It comprises
key modules such as the steering vector generator and the steering vector
applier, which enable automatic generation and application of steering vectors
to influence the model's behavior without modifying its parameters. One of the
main advantages of EasyEdit2 is its ease of use-users do not need extensive
technical knowledge. With just a single example, they can effectively guide and
adjust the model's responses, making precise control both accessible and
efficient. Empirically, we report model steering performance across different
LLMs, demonstrating the effectiveness of these techniques. We have released the
source code on GitHub at https://github.com/zjunlp/EasyEdit along with a
demonstration notebook. In addition, we provide a demo video at
https://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.

</details>

### [211] [The Synthetic Imputation Approach: Generating Optimal Synthetic Texts For Underrepresented Categories In Supervised Classification Tasks](https://arxiv.org/abs/2504.15160)
*Joan C. Timoneda*

Main category: cs.CL

TLDR: 论文提出了一种合成插补方法，利用生成式LLM（如GPT-4o）生成合成文本，以解决训练数据中类别样本不足的问题。该方法在75个原始样本时表现与完整样本相当，且过拟合可控。


<details>
  <summary>Details</summary>
Motivation: 在标注任务中，训练数据中所有类别的样本通常难以充分覆盖，影响模型性能。

Method: 使用生成式LLM（GPT-4o）基于少量原始样本生成合成文本，确保新文本与原始文本有足够差异以减少过拟合，同时保留语义信息。

Result: 合成插补方法在75个原始样本时表现与完整样本相当，50个样本时过拟合可控且可校正。

Conclusion: 合成插补方法为生成式LLM在研究中提供了新用途，帮助研究者平衡数据集以优化性能。

Abstract: Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa,
require that all categories in an annotation task be sufficiently represented
in the training data for optimal performance. However, it is often difficult to
find sufficient examples for all categories in a task when building a
high-quality training set. In this article, I describe this problem and propose
a solution, the synthetic imputation approach. Leveraging a generative LLM
(GPT-4o), this approach generates synthetic texts based on careful prompting
and five original examples drawn randomly with replacement from the sample.
This approach ensures that new synthetic texts are sufficiently different from
the original texts to reduce overfitting, but retain the underlying substantive
meaning of the examples to maximize out-of-sample performance. With 75 original
examples or more, synthetic imputation's performance is on par with a full
sample of original texts, and overfitting remains low, predictable and
correctable with 50 original samples. The synthetic imputation approach
provides a novel role for generative LLMs in research and allows applied
researchers to balance their datasets for best performance.

</details>

### [212] [On true empty category](https://arxiv.org/abs/2504.15168)
*Qilin Tian*

Main category: cs.CL

TLDR: 本文探讨了空语类分类问题，反驳了Li等人提出的“真实空语类”假设，认为话题化现象无需引入该概念即可解释。


<details>
  <summary>Details</summary>
Motivation: Chomsky的空语类分类存在不足，Li等人提出“真实空语类”假设试图填补空白，但其证据可能不充分。

Method: 通过分析话题化现象，评估Li等人提出的证据。

Result: 发现话题化现象无需“真实空语类”即可解释，反驳了该假设的必要性。

Conclusion: “真实空语类”假设可能并非必要，现有理论足以解释相关现象。

Abstract: According to Chomsky (1981, 1986), empty categories consist of PRO, pro,
trace, and variable. However, some empty object positions seem to be
incompatible with extant empty categories. Given this, Li (2007a, 2007b, 2014)
and Li & Wei (2014) raise the true empty category hypothesis, which holds that
true empty category is only an empty position with category and Case features.
As a last resort option, it is used mainly to meet the subcatgorization of a
verb. This assumption is ingenious, and if proved to be true, it will exert a
great impact on the study of UG. In this paper, we evaluate their evidence from
topicalization and demonstrate that it can be accounted for without invoking
true empty category.

</details>

### [213] [Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges](https://arxiv.org/abs/2504.15205)
*Nandan Thakur,Ronak Pradeep,Shivani Upadhyay,Daniel Campos,Nick Craswell,Jimmy Lin*

Main category: cs.CL

TLDR: RAG通过引用真实文档减少LLM幻觉，研究比较了GPT-4o与人类评委在支持评估中的表现，发现GPT-4o可作为可靠替代。


<details>
  <summary>Details</summary>
Motivation: 评估RAG中引用文档对答案的支持性，比较自动LLM评委（GPT-4o）与人类评委的准确性。

Method: 对45份提交的36个主题进行大规模比较研究，分为完全人工评估和人工后编辑LLM预测两种条件。

Result: 56%的完全人工评估中人类与GPT-4o一致，后编辑条件下提升至72%；独立人类评委与GPT-4o相关性更高。

Conclusion: GPT-4o可作为支持评估的可靠替代，未来需改进人类与LLM的错误分析。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to
generate answers with citations from source documents containing "ground
truth", thereby reducing system hallucinations. A crucial factor in RAG
evaluation is "support", whether the information in the cited documents
supports the answer. To this end, we conducted a large-scale comparative study
of 45 participant submissions on 36 topics to the TREC 2024 RAG Track,
comparing an automatic LLM judge (GPT-4o) against human judges for support
assessment. We considered two conditions: (1) fully manual assessments from
scratch and (2) manual assessments with post-editing of LLM predictions. Our
results indicate that for 56% of the manual from-scratch assessments, human and
GPT-4o predictions match perfectly (on a three-level scale), increasing to 72%
in the manual with post-editing condition. Furthermore, by carefully analyzing
the disagreements in an unbiased study, we found that an independent human
judge correlates better with GPT-4o than a human judge, suggesting that LLM
judges can be a reliable alternative for support assessment. To conclude, we
provide a qualitative analysis of human and GPT-4o errors to help guide future
iterations of support assessment.

</details>

### [214] [EvalAgent: Discovering Implicit Evaluation Criteria from the Web](https://arxiv.org/abs/2504.15219)
*Manya Wadhwa,Zayne Sprague,Chaitanya Malaviya,Philippe Laban,Junyi Jessy Li,Greg Durrett*

Main category: cs.CL

TLDR: 论文提出了EvalAgent框架，用于自动发现语言模型输出中的隐式任务特定评估标准，结合专家指导和LLM生成标准，提升评估质量。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法通常依赖显式标准，忽略了高质量输出应满足的隐式任务特定要求，如学术演讲的典型特征。

Method: EvalAgent通过挖掘专家在线指导，生成基于外部可靠来源的多样化评估标准，并结合LLM生成标准。

Result: 实验表明，EvalAgent生成的隐式标准具有高精确性，且能指导模型输出优化，结合LLM标准可发现更多人重视的标准。

Conclusion: EvalAgent框架有效补充了现有评估方法，通过隐式任务特定标准提升了语言模型输出的质量。

Abstract: Evaluation of language model outputs on structured writing tasks is typically
conducted with a number of desirable criteria presented to human evaluators or
large language models (LLMs). For instance, on a prompt like "Help me draft an
academic talk on coffee intake vs research productivity", a model response may
be evaluated for criteria like accuracy and coherence. However, high-quality
responses should do more than just satisfy basic task requirements. An
effective response to this query should include quintessential features of an
academic talk, such as a compelling opening, clear research questions, and a
takeaway. To help identify these implicit criteria, we introduce EvalAgent, a
novel framework designed to automatically uncover nuanced and task-specific
criteria. EvalAgent first mines expert-authored online guidance. It then uses
this evidence to propose diverse, long-tail evaluation criteria that are
grounded in reliable external sources. Our experiments demonstrate that the
grounded criteria produced by EvalAgent are often implicit (not directly stated
in the user's prompt), yet specific (high degree of lexical precision).
Further, EvalAgent criteria are often not satisfied by initial responses but
they are actionable, such that responses can be refined to satisfy them.
Finally, we show that combining LLM-generated and EvalAgent criteria uncovers
more human-valued criteria than using LLMs alone.

</details>

### [215] [Fully Bayesian Approaches to Topics over Time](https://arxiv.org/abs/2504.15220)
*Julián Cendrero,Julio Gonzalo,Ivar Zapata*

Main category: cs.CL

TLDR: 论文提出了完全贝叶斯的BToT模型，解决了ToT模型的稳定性问题，并通过WBToT进一步平衡时间和词模态的影响，实验表明WBToT在事件捕捉和模型稳定性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: ToT模型在时间戳数据集上建模主题变化时存在稳定性问题，且未采用完全贝叶斯方法。

Method: 引入Beta分布的共轭先验构建BToT模型，并通过WBToT重复文档的发布时间以平衡时间和词模态的影响。

Result: WBToT在捕捉事件和模型稳定性上优于LDA和BERTopic，时间主题存在的偏差分别减少51%和34%。

Conclusion: WBToT通过平衡时间和词模态，显著提升了主题模型的性能和稳定性，适用于标准ToT无法处理的问题。

Abstract: The Topics over Time (ToT) model captures thematic changes in timestamped
datasets by explicitly modeling publication dates jointly with word
co-occurrence patterns. However, ToT was not approached in a fully Bayesian
fashion, a flaw that makes it susceptible to stability problems. To address
this issue, we propose a fully Bayesian Topics over Time (BToT) model via the
introduction of a conjugate prior to the Beta distribution. This prior acts as
a regularization that prevents the online version of the algorithm from
unstable updates when a topic is poorly represented in a mini-batch. The
characteristics of this prior to the Beta distribution are studied here for the
first time. Still, this model suffers from a difference in scale between the
single-time observations and the multiplicity of words per document. A
variation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a
solution. In WBToT, publication dates are repeated a certain number of times
per document, which balances the relative influence of words and timestamps
along the inference process. We have tested our models on two datasets: a
collection of over 200 years of US state-of-the-union (SOTU) addresses and a
large-scale COVID-19 Twitter corpus of 10 million tweets. The results show that
WBToT captures events better than Latent Dirichlet Allocation and other SOTA
topic models like BERTopic: the median absolute deviation of the topic presence
over time is reduced by $51\%$ and $34\%$, respectively. Our experiments also
demonstrate the superior coherence of WBToT over BToT, which highlights the
importance of balancing the time and word modalities. Finally, we illustrate
the stability of the online optimization algorithm in WBToT, which allows the
application of WBToT to problems that are intractable for standard ToT.

</details>

### [216] [Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions](https://arxiv.org/abs/2504.15236)
*Saffron Huang,Esin Durmus,Miles McCain,Kunal Handa,Alex Tamkin,Jerry Hong,Michael Stern,Arushi Somani,Xiuruo Zhang,Deep Ganguli*

Main category: cs.CL

TLDR: 研究通过隐私保护方法提取Claude 3和3.5模型在真实互动中表现出的3,307种价值观，发现其支持亲社会价值观，但价值观因上下文而异。


<details>
  <summary>Details</summary>
Motivation: AI助手的价值观可能影响用户决策和世界观，但缺乏实证研究其实际依赖的价值观。

Method: 采用自下而上、隐私保护的方法，分析数十万次真实互动中的模型响应。

Result: 发现Claude表达了许多实用和认知价值观，支持亲社会价值观，且价值观因上下文不同而变化。

Conclusion: 研究为AI系统的价值观评估和设计提供了首个大规模实证基础。

Abstract: AI assistants can impart value judgments that shape people's decisions and
worldviews, yet little is known empirically about what values these systems
rely on in practice. To address this, we develop a bottom-up,
privacy-preserving method to extract the values (normative considerations
stated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit
in hundreds of thousands of real-world interactions. We empirically discover
and taxonomize 3,307 AI values and study how they vary by context. We find that
Claude expresses many practical and epistemic values, and typically supports
prosocial human values while resisting values like "moral nihilism". While some
values appear consistently across contexts (e.g. "transparency"), many are more
specialized and context-dependent, reflecting the diversity of human
interlocutors and their varied contexts. For example, "harm prevention" emerges
when Claude resists users, "historical accuracy" when responding to queries
about controversial events, "healthy boundaries" when asked for relationship
advice, and "human agency" in technology ethics discussions. By providing the
first large-scale empirical mapping of AI values in deployment, our work
creates a foundation for more grounded evaluation and design of values in AI
systems.

</details>

### [217] [MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning](https://arxiv.org/abs/2504.15241)
*Yahan Yang,Soham Dan,Shuo Li,Dan Roth,Insup Lee*

Main category: cs.CL

TLDR: 提出了一种多语言防护栏方法，通过合成数据、监督微调和GRPO框架提升性能，在多语言环境下优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 多语言环境下，大型语言模型易受对抗攻击（如越狱），且安全对齐数据有限，需开发能检测和过滤不安全内容的多语言防护栏。

Method: 方法包括：1) 合成多语言数据，融入文化和语言细微差异；2) 监督微调；3) 课程引导的GRPO框架优化性能。

Result: 实验表明，该多语言防护栏在域内和域外语言中均优于基线模型，并能生成多语言解释以理解语言特定风险。

Conclusion: 该方法有效提升了多语言内容审核的安全性和解释能力。

Abstract: Large Language Models (LLMs) are susceptible to adversarial attacks such as
jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability
is exacerbated in multilingual setting, where multilingual safety-aligned data
are often limited. Thus, developing a guardrail capable of detecting and
filtering unsafe content across diverse languages is critical for deploying
LLMs in real-world applications. In this work, we propose an approach to build
a multilingual guardrail with reasoning. Our method consists of: (1) synthetic
multilingual data generation incorporating culturally and linguistically
nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group
Relative Policy Optimization (GRPO) framework that further improves
performance. Experimental results demonstrate that our multilingual guardrail
consistently outperforms recent baselines across both in-domain and
out-of-domain languages. The multilingual reasoning capability of our guardrail
enables it to generate multilingual explanations, which are particularly useful
for understanding language-specific risks and ambiguities in multilingual
content moderation.

</details>

### [218] [Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators](https://arxiv.org/abs/2504.15253)
*Yilun Zhou,Austin Xu,Peifeng Wang,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TLDR: 论文研究了在测试时计算扩展中，使用LLM-judges（生成评价和解释的语言模型）作为评估器的效果，发现其在某些任务中表现不如过程奖励模型，且自然语言解释对生成改进效果有限。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM-judges在测试时计算扩展中的有效性，填补其在自动评估领域的未知表现。

Method: 引入JETTS基准，评估LLM-judges在数学推理、代码生成和指令遵循三个领域的表现，涉及响应重排、步骤级束搜索和基于解释的响应优化三种任务。

Result: LLM-judges在重排任务中与结果奖励模型竞争，但在束搜索中不如过程奖励模型；自然语言解释对生成改进效果不佳。

Conclusion: LLM-judges在特定任务中表现有限，需进一步优化其解释能力以提升评估效果。

Abstract: Scaling test-time computation, or affording a generator large language model
(LLM) extra compute during inference, typically employs the help of external
non-generative evaluators (i.e., reward models). Concurrently, LLM-judges,
models trained to generate evaluations and critiques (explanations) in natural
language, are becoming increasingly popular in automatic evaluation. Despite
judge empirical successes, their effectiveness as evaluators in test-time
scaling settings is largely unknown. In this paper, we introduce the Judge
Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge
performance in three domains (math reasoning, code generation, and instruction
following) under three task settings: response reranking, step-level beam
search, and critique-based response refinement. We evaluate 10 different judge
models (7B-70B parameters) for 8 different base generator models (6.7B-72B
parameters). Our benchmark shows that while judges are competitive with outcome
reward models in reranking, they are consistently worse than process reward
models in beam search procedures. Furthermore, though unique to LLM-judges,
their natural language critiques are currently ineffective in guiding the
generator towards better responses.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [219] [The Model Counting Competitions 2021-2023](https://arxiv.org/abs/2504.13842)
*Johannes K. Fichte,Markus Hecher*

Main category: cs.AI

TLDR: 本文概述了2021-2023年模型计数竞赛（MC Competition）的迭代，包括其目标、执行和结果。竞赛分为四个赛道，分别针对模型计数问题的不同变体，吸引了大量参与者和多样化的求解器。


<details>
  <summary>Details</summary>
Motivation: 现代社会中许多计算问题涉及概率推理、统计和组合数学，这些问题可以通过命题公式的模型计数来解决。竞赛旨在推动应用、识别挑战性基准、促进求解器开发。

Method: 竞赛分为四个赛道：基础模型计数（MC）、加权模型计数（WMC）、投影模型计数（PMC）以及投影加权模型计数（PWMC）。每个赛道聚焦不同变体，吸引了多样化的求解器参与。

Result: 竞赛持续高参与度，7至9个基于不同技术的求解器提交。

Conclusion: 竞赛成功推动了模型计数领域的发展，识别了挑战并启发了新应用。

Abstract: Modern society is full of computational challenges that rely on probabilistic
reasoning, statistics, and combinatorics. Interestingly, many of these
questions can be formulated by encoding them into propositional formulas and
then asking for its number of models. With a growing interest in practical
problem-solving for tasks that involve model counting, the community
established the Model Counting (MC) Competition in fall of 2019 with its first
iteration in 2020. The competition aims at advancing applications, identifying
challenging benchmarks, fostering new solver development, and enhancing
existing solvers for model counting problems and their variants. The first
iteration, brought together various researchers, identified challenges, and
inspired numerous new applications. In this paper, we present a comprehensive
overview of the 2021-2023 iterations of the Model Counting Competition. We
detail its execution and outcomes. The competition comprised four tracks, each
focusing on a different variant of the model counting problem. The first track
centered on the model counting problem (MC), which seeks the count of models
for a given propositional formula. The second track challenged developers to
submit programs capable of solving the weighted model counting problem (WMC).
The third track was dedicated to projected model counting (PMC). Finally, we
initiated a track that combined projected and weighted model counting (PWMC).
The competition continued with a high level of participation, with seven to
nine solvers submitted in various different version and based on quite
diverging techniques.

</details>

### [220] [Evaluation and Incident Prevention in an Enterprise AI Assistant](https://arxiv.org/abs/2504.13924)
*Akash V. Maharaj,David Arbour,Daniel Lee,Uttaran Bhattacharya,Anup Rao,Austin Zane,Avi Feller,Kun Qian,Yunyao Li*

Main category: cs.AI

TLDR: 本文提出一个综合框架，用于监控、基准测试和持续改进企业AI助手，包含错误检测、基准构建和多维评估三个关键要素。


<details>
  <summary>Details</summary>
Motivation: 企业AI助手在关键领域中的错误可能导致严重后果，因此需要系统化的方法来提升其可靠性和性能。

Method: 框架包括：1) 分层错误检测与分类；2) 可扩展的基准构建方法；3) 多维评估的持续改进策略。

Result: 该框架能系统性提升AI助手的可靠性，适用于多团队开发环境，并支持多样化改进机会。

Conclusion: 多维度评估方法为AI系统的稳健性和可信度提供了新的改进途径。

Abstract: Enterprise AI Assistants are increasingly deployed in domains where accuracy
is paramount, making each erroneous output a potentially significant incident.
This paper presents a comprehensive framework for monitoring, benchmarking, and
continuously improving such complex, multi-component systems under active
development by multiple teams. Our approach encompasses three key elements: (1)
a hierarchical ``severity'' framework for incident detection that identifies
and categorizes errors while attributing component-specific error rates,
facilitating targeted improvements; (2) a scalable and principled methodology
for benchmark construction, evaluation, and deployment, designed to accommodate
multiple development teams, mitigate overfitting risks, and assess the
downstream impact of system modifications; and (3) a continual improvement
strategy leveraging multidimensional evaluation, enabling the identification
and implementation of diverse enhancement opportunities. By adopting this
holistic framework, organizations can systematically enhance the reliability
and performance of their AI Assistants, ensuring their efficacy in critical
enterprise environments. We conclude by discussing how this multifaceted
evaluation approach opens avenues for various classes of enhancements, paving
the way for more robust and trustworthy AI systems.

</details>

### [221] [Birds of a Different Feather Flock Together: Exploring Opportunities and Challenges in Animal-Human-Machine Teaming](https://arxiv.org/abs/2504.13973)
*Myke C. Cohen,David A. Grimm,Reuth Mirsky,Xiaoyun Yin*

Main category: cs.AI

TLDR: 本文呼吁系统研究动物-人类-机器（AHM）团队结构设计，以优化性能并克服应用中的限制，通过多维方法探索协同潜力。


<details>
  <summary>Details</summary>
Motivation: 研究AHM团队的动机在于利用人类、动物和AI机器的协同作用，实现超越个体能力的独特功能。

Method: 提出AHM团队功能的多维框架，并通过安全筛查、搜救和导盲犬三个案例展示其应用。

Result: 研究表明AHM团队能有效处理复杂任务，同时凸显其潜力与挑战。

Conclusion: 结论指出多维方法为研究更广泛的混合人机系统提供了新的研究方向。

Abstract: Animal-Human-Machine (AHM) teams are a type of hybrid intelligence system
wherein interactions between a human, AI-enabled machine, and animal members
can result in unique capabilities greater than the sum of their parts. This
paper calls for a systematic approach to studying the design of AHM team
structures to optimize performance and overcome limitations in various applied
settings. We consider the challenges and opportunities in investigating the
synergistic potential of AHM team members by introducing a set of dimensions of
AHM team functioning to effectively utilize each member's strengths while
compensating for individual weaknesses. Using three representative examples of
such teams -- security screening, search-and-rescue, and guide dogs -- the
paper illustrates how AHM teams can tackle complex tasks. We conclude with open
research directions that this multidimensional approach presents for studying
hybrid human-AI systems beyond AHM teams.

</details>

### [222] [Going Whole Hog: A Philosophical Defense of AI Cognition](https://arxiv.org/abs/2504.13988)
*Herman Cappelen,Josh Dever*

Main category: cs.AI

TLDR: 本文主张大型语言模型（如ChatGPT）是完整的语言和认知主体，具备理解、信念、欲望、知识和意图。作者反对基于低层次计算细节或预设心智理论的方法，提倡从高层次行为观察出发，并通过‘整体网络假设’论证其认知状态。


<details>
  <summary>Details</summary>
Motivation: 反驳AI哲学中主流方法，强调应从简单的高层次行为观察出发，论证LLMs的认知能力。

Method: 通过‘整体网络假设’（如回答问题暗示知识，知识暗示信念等）系统性论证LLMs的认知状态，并反驳常见质疑。

Result: 论证LLMs具备完整的认知状态，其失败（如幻觉、推理错误）不否定其主体性，类似人类的缺陷。

Conclusion: LLMs可能是超越人类概念框架的‘异类’认知主体，但研究刻意排除了意识问题。

Abstract: This work defends the 'Whole Hog Thesis': sophisticated Large Language Models
(LLMs) like ChatGPT are full-blown linguistic and cognitive agents, possessing
understanding, beliefs, desires, knowledge, and intentions. We argue against
prevailing methodologies in AI philosophy, rejecting starting points based on
low-level computational details ('Just an X' fallacy) or pre-existing theories
of mind. Instead, we advocate starting with simple, high-level observations of
LLM behavior (e.g., answering questions, making suggestions) -- defending this
data against charges of metaphor, loose talk, or pretense. From these
observations, we employ 'Holistic Network Assumptions' -- plausible connections
between mental capacities (e.g., answering implies knowledge, knowledge implies
belief, action implies intention) -- to argue for the full suite of cognitive
states. We systematically rebut objections based on LLM failures
(hallucinations, planning/reasoning errors), arguing these don't preclude
agency, often mirroring human fallibility. We address numerous 'Games of
Lacks', arguing that LLMs do not lack purported necessary conditions for
cognition (e.g., semantic grounding, embodiment, justification, intrinsic
intentionality) or that these conditions are not truly necessary, often relying
on anti-discriminatory arguments comparing LLMs to diverse human capacities.
Our approach is evidential, not functionalist, and deliberately excludes
consciousness. We conclude by speculating on the possibility of LLMs possessing
'alien' contents beyond human conceptual schemes.

</details>

### [223] [Multi-Stage Retrieval for Operational Technology Cybersecurity Compliance Using Large Language Models: A Railway Casestudy](https://arxiv.org/abs/2504.14044)
*Regan Bolton,Mohammadreza Sheikhfathollahi,Simon Parkinson,Dan Basher,Howard Parkinson*

Main category: cs.AI

TLDR: 论文提出了一种基于大语言模型（LLM）和多阶段检索的系统，用于提升铁路网络安全中的合规性验证效率，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着铁路等关键基础设施的数字化，网络安全威胁增加，亟需高效的合规性验证方法以保护系统安全。

Method: 提出并行合规架构（PCA），结合大语言模型和多阶段检索，对比了OpenAI-gpt-4o和Claude-3.5-haiku模型的效果。

Result: PCA显著提升了合规性验证的正确性和推理质量，尤其在逻辑推理和幻觉检测方面表现突出。

Conclusion: 检索增强方法可显著提升合规性评估效率，对网络安全人才短缺的行业尤为重要。

Abstract: Operational Technology Cybersecurity (OTCS) continues to be a dominant
challenge for critical infrastructure such as railways. As these systems become
increasingly vulnerable to malicious attacks due to digitalization, effective
documentation and compliance processes are essential to protect these
safety-critical systems. This paper proposes a novel system that leverages
Large Language Models (LLMs) and multi-stage retrieval to enhance the
compliance verification process against standards like IEC 62443 and the
rail-specific IEC 63452. We first evaluate a Baseline Compliance Architecture
(BCA) for answering OTCS compliance queries, then develop an extended approach
called Parallel Compliance Architecture (PCA) that incorporates additional
context from regulatory standards. Through empirical evaluation comparing
OpenAI-gpt-4o and Claude-3.5-haiku models in these architectures, we
demonstrate that the PCA significantly improves both correctness and reasoning
quality in compliance verification. Our research establishes metrics for
response correctness, logical reasoning, and hallucination detection,
highlighting the strengths and limitations of using LLMs for compliance
verification in railway cybersecurity. The results suggest that
retrieval-augmented approaches can significantly improve the efficiency and
accuracy of compliance assessments, particularly valuable in an industry facing
a shortage of cybersecurity expertise.

</details>

### [224] [Metacognition and Uncertainty Communication in Humans and Large Language Models](https://arxiv.org/abs/2504.14045)
*Mark Steyvers,Megan A. K. Peters*

Main category: cs.AI

TLDR: 该论文探讨了大语言模型（LLMs）是否具备元认知能力，并比较了其与人类元认知的异同，强调了研究这些差异对提升人机协作和开发更可信AI系统的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在高风险决策中的广泛应用，评估其是否具备元认知能力变得至关重要，以促进更有效的人机协作和AI发展。

Method: 论文综述了当前对LLMs元认知能力的认识，探讨了研究方法，并与人类元认知进行了比较。

Result: 研究发现，尽管LLMs与人类在某些元认知行为上表现相似，但仍存在显著差异。

Conclusion: 未来通过增强LLMs的元认知能力，可能帮助其发展更高效的学习、自我导向和好奇心等新能力。

Abstract: Metacognition, the capacity to monitor and evaluate one's own knowledge and
performance, is foundational to human decision-making, learning, and
communication. As large language models (LLMs) become increasingly embedded in
high-stakes decision contexts, it is critical to assess whether, how, and to
what extent they exhibit metacognitive abilities. Here, we provide an overview
of current knowledge of LLMs' metacognitive capacities, how they might be
studied, and how they relate to our knowledge of metacognition in humans. We
show that while humans and LLMs can sometimes appear quite aligned in their
metacognitive capacities and behaviors, it is clear many differences remain.
Attending to these differences is crucial not only for enhancing human-AI
collaboration, but also for promoting the development of more capable and
trustworthy artificial systems. Finally, we discuss how endowing future LLMs
with more sensitive and more calibrated metacognition may also help them
develop new capacities such as more efficient learning, self-direction, and
curiosity.

</details>

### [225] [Think Deep, Think Fast: Investigating Efficiency of Verifier-free Inference-time-scaling Methods](https://arxiv.org/abs/2504.14047)
*Junlin Wang,Shang Zhu,Jon Saad-Falcon,Ben Athiwaratkun,Qingyang Wu,Jue Wang,Shuaiwen Leon Song,Ce Zhang,Bhuwan Dhingra,James Zou*

Main category: cs.AI

TLDR: 研究探讨了推理时间计算（ITC）如何提升大语言模型（LLM）能力，发现推理模型在多数投票策略下表现最优，且正确回答通常更短且语言标记更少。


<details>
  <summary>Details</summary>
Motivation: 探索ITC与不同模型推理能力的交互，以指导LLM的进一步发展。

Method: 对推理和非推理模型进行推理时间扩展方法的全面分析，重点关注无需验证器的通用方法。

Result: 非推理模型即使在高推理预算下仍显著落后于推理模型；多数投票是推理模型的最优策略。

Conclusion: 推理模型的正确回答更短且语言标记更少，ITC方法可通过优化响应特征进一步提升。

Abstract: There is intense interest in investigating how inference time compute (ITC)
(e.g. repeated sampling, refinements, etc) can improve large language model
(LLM) capabilities. At the same time, recent breakthroughs in reasoning models,
such as Deepseek-R1, unlock the opportunity for reinforcement learning to
improve LLM reasoning skills. An in-depth understanding of how ITC interacts
with reasoning across different models could provide important guidance on how
to further advance the LLM frontier. This work conducts a comprehensive
analysis of inference-time scaling methods for both reasoning and non-reasoning
models on challenging reasoning tasks. Specifically, we focus our research on
verifier-free inference time-scaling methods due to its generalizability
without needing a reward model. We construct the Pareto frontier of quality and
efficiency. We find that non-reasoning models, even with an extremely high
inference budget, still fall substantially behind reasoning models. For
reasoning models, majority voting proves to be a robust inference strategy,
generally competitive or outperforming other more sophisticated ITC methods
like best-of-N and sequential revisions, while the additional inference compute
offers minimal improvements. We further perform in-depth analyses of the
association of key response features (length and linguistic markers) with
response quality, with which we can improve the existing ITC methods. We find
that correct responses from reasoning models are typically shorter and have
fewer hedging and thinking markers (but more discourse markers) than the
incorrect responses.

</details>

### [226] [Linking forward-pass dynamics in Transformers and real-time human processing](https://arxiv.org/abs/2504.14107)
*Jennifer Hu,Michael A. Lepori,Michael Franke*

Main category: cs.AI

TLDR: 论文探讨了Transformer模型的内部处理动态是否与人类实时处理相似，发现层时动态能提供额外预测能力。


<details>
  <summary>Details</summary>
Motivation: 研究AI模型内部处理策略是否与人类认知过程相似，以拓展AI作为研究人类认知工具的应用。

Method: 通过五个跨领域和模态的研究，比较Transformer模型的层时动态与人类实时处理数据。

Result: 层时动态在模型输出概率分布基础上提供了额外的预测能力。

Conclusion: Transformer模型与人类处理可能受相似输入特性影响，为AI模型作为显式处理模型研究人类认知提供了新思路。

Abstract: Modern AI models are increasingly being used as theoretical tools to study
human cognition. One dominant approach is to evaluate whether human-derived
measures (such as offline judgments or real-time processing) are predicted by a
model's output: that is, the end-product of forward pass(es) through the
network. At the same time, recent advances in mechanistic interpretability have
begun to reveal the internal processes that give rise to model outputs, raising
the question of whether models and humans might arrive at outputs using similar
"processing strategies". Here, we investigate the link between real-time
processing in humans and "layer-time" dynamics in Transformer models. Across
five studies spanning domains and modalities, we test whether the dynamics of
computation in a single forward pass of pre-trained Transformers predict
signatures of processing in humans, above and beyond properties of the model's
output probability distribution. We consistently find that layer-time dynamics
provide additional predictive power on top of output measures. Our results
suggest that Transformer processing and human processing may be facilitated or
impeded by similar properties of an input stimulus, and this similarity has
emerged through general-purpose objectives such as next-token prediction or
image recognition. Our work suggests a new way of using AI models to study
human cognition: not just as a black box mapping stimuli to responses, but
potentially also as explicit processing models.

</details>

### [227] [CODECRASH: Stress Testing LLM Reasoning under Structural and Semantic Perturbations](https://arxiv.org/abs/2504.14119)
*Man Ho Lam,Chaozheng Wang,Jen-tse Huang,Michael R. Lyu*

Main category: cs.AI

TLDR: CodeCrash是一个评估大型语言模型（LLMs）在代码理解和推理中鲁棒性的统一基准，揭示了LLMs在结构噪声和自然语言线索依赖上的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码任务中表现强大，但其在代码理解和推理中的鲁棒性尚未充分研究。

Method: CodeCrash通过结构性和文本性干扰扰动评估LLMs，使用直接和链式推理分析17个LLMs和3个大型推理模型（LRMs）。

Result: 研究发现LLMs在结构噪声下表现脆弱，且依赖自然语言线索，同时揭示了自反推理机制的严重漏洞。

Conclusion: CodeCrash为代码理解中的LLMs压力测试提供了框架，并为未来评估和基准测试指明了方向。

Abstract: Large Language Models (LLMs) have recently showcased strong capabilities in
code-related tasks, yet their robustness in code comprehension and reasoning
remains underexplored. In this paper, we present CodeCrash, a unified benchmark
that evaluates LLM robustness under code structural and textual distraction
perturbations, applied to two established benchmarks -- CRUXEval and
LiveCodeBench -- across both input and output prediction tasks. We evaluate
seventeen LLMs using direct and Chain-of-Thought inference to systematically
analyze their robustness, identify primary reasons for performance degradation,
and highlight failure modes. Our findings reveal the fragility of LLMs under
structural noise and the inherent reliance on natural language cues,
highlighting critical robustness issues of LLMs in code execution and
understanding. Additionally, we examine three Large Reasoning Models (LRMs) and
discover the severe vulnerability of self-reflective reasoning mechanisms that
lead to reasoning collapse. CodeCrash provides a principled framework for
stress-testing LLMs in code understanding, offering actionable directions for
future evaluation and benchmarking. The code of CodeCrash and the robustness
leaderboard are publicly available at https://donaldlamnl.github.io/CodeCrash/ .

</details>

### [228] [Bayesian Principles Improve Prompt Learning In Vision-Language Models](https://arxiv.org/abs/2504.14123)
*Mingyu Kim,Jongwoo Ko,Mijung Park*

Main category: cs.AI

TLDR: 提出了一种基于贝叶斯学习原理的新训练目标函数，以平衡提示学习的适应性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法容易过拟合微调数据，泛化性差。

Method: 通过贝叶斯学习原理，设计了一个先验和后验的平衡目标函数。

Result: 新方法在保持预训练模型特性的同时，适应下游任务。

Conclusion: 该方法有效平衡了适应性和泛化性。

Abstract: Prompt learning is a popular fine-tuning method for vision-language models
due to its efficiency. It requires a small number of additional learnable
parameters while significantly enhancing performance on target tasks. However,
most existing methods suffer from overfitting to fine-tuning data, yielding
poor generalizability. To address this, we propose a new training objective
function based on a Bayesian learning principle to balance adaptability and
generalizability. We derive a prior over the logits, where the mean function is
parameterized by the pre-trained model, while the posterior corresponds to the
fine-tuned model. This objective establishes a balance by allowing the
fine-tuned model to adapt to downstream tasks while remaining close to the
pre-trained model.

</details>

### [229] [Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models](https://arxiv.org/abs/2504.14126)
*Saad Hameed,Basheer Qolomany,Samir Brahim Belhaouari,Mohamed Abdallah,Junaid Qadir,Ala Al-Fuqaha*

Main category: cs.AI

TLDR: 该论文提出了一种结合大型语言模型（LLM）和粒子群优化（PSO）的方法，用于深度学习超参数调优，显著提高了收敛速度并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的架构设计通常依赖人工调优或计算密集型优化方法，效率低下且资源消耗大。PSO和LLM在各自领域已有应用，但二者结合用于数值优化任务的研究较少。

Method: 通过将LLM（如ChatGPT-3.5和Llama3）集成到PSO中，用LLM的建议替换表现不佳的粒子位置，从而加速搜索空间探索。

Result: 在三个场景（Rastrigin函数优化、LSTM时间序列回归、CNN材料分类）中，该方法显著提升了收敛速度，计算复杂度降低20%-60%，同时保持精度。

Conclusion: 该方法为深度学习模型优化提供了一种高效且有效的解决方案，适用于广泛的应用场景。

Abstract: Determining the ideal architecture for deep learning models, such as the
number of layers and neurons, is a difficult and resource-intensive process
that frequently relies on human tuning or computationally costly optimization
approaches. While Particle Swarm Optimization (PSO) and Large Language Models
(LLMs) have been individually applied in optimization and deep learning, their
combined use for enhancing convergence in numerical optimization tasks remains
underexplored. Our work addresses this gap by integrating LLMs into PSO to
reduce model evaluations and improve convergence for deep learning
hyperparameter tuning. The proposed LLM-enhanced PSO method addresses the
difficulties of efficiency and convergence by using LLMs (particularly
ChatGPT-3.5 and Llama3) to improve PSO performance, allowing for faster
achievement of target objectives. Our method speeds up search space exploration
by substituting underperforming particle placements with best suggestions
offered by LLMs. Comprehensive experiments across three scenarios -- (1)
optimizing the Rastrigin function, (2) using Long Short-Term Memory (LSTM)
networks for time series regression, and (3) using Convolutional Neural
Networks (CNNs) for material classification -- show that the method
significantly improves convergence rates and lowers computational costs.
Depending on the application, computational complexity is lowered by 20% to 60%
compared to traditional PSO methods. Llama3 achieved a 20% to 40% reduction in
model calls for regression tasks, whereas ChatGPT-3.5 reduced model calls by
60% for both regression and classification tasks, all while preserving accuracy
and error rates. This groundbreaking methodology offers a very efficient and
effective solution for optimizing deep learning models, leading to substantial
computational performance improvements across a wide range of applications.

</details>

### [230] [TALES: Text Adventure Learning Environment Suite](https://arxiv.org/abs/2504.14128)
*Christopher Zhang Cui,Xingdi Yuan,Zhang Xiao,Prithviraj Ammanabrolu,Marc-Alexandre Côté*

Main category: cs.AI

TLDR: 论文介绍了TALES，一个用于挑战和评估多样化推理能力的文本冒险游戏集合，并分析了不同LLM的表现。


<details>
  <summary>Details</summary>
Motivation: 随着任务复杂度的增加，需要更复杂的推理能力来支持顺序决策，因此需要评估LLM的推理能力。

Method: 使用TALES（合成和人类编写的文本冒险游戏）来测试多种LLM的表现，并进行定性分析。

Result: 尽管在合成游戏中表现优异，但顶级LLM在人类设计的游戏中成功率不足15%。

Conclusion: TALES为评估LLM推理能力提供了新工具，但LLM在复杂任务中仍有提升空间。

Abstract: Reasoning is an essential skill to enable Large Language Models (LLMs) to
interact with the world. As tasks become more complex, they demand increasingly
sophisticated and diverse reasoning capabilities for sequential
decision-making, requiring structured reasoning over the context history to
determine the next best action. We introduce TALES, a diverse collection of
synthetic and human-written text-adventure games designed to challenge and
evaluate diverse reasoning capabilities. We present results over a range of
LLMs, open- and closed-weights, performing a qualitative analysis on the top
performing models. Despite an impressive showing on synthetic games, even the
top LLM-driven agents fail to achieve 15% on games designed for human
enjoyment. Code and visualization of the experiments can be found at
https://microsoft.github.io/tales.

</details>

### [231] [Adaptation Method for Misinformation Identification](https://arxiv.org/abs/2504.14171)
*Yangping Chen,Weijie Shi,Mengze Li,Yue Cui,Hao Chen,Jia Zhu,Jiajie Xu*

Main category: cs.AI

TLDR: ADOSE是一个主动领域自适应框架，用于多模态假新闻检测，通过标注少量目标样本提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态假新闻检测中领域偏移导致的性能下降问题。

Method: 设计多个专家分类器学习跨模态依赖关系，提出最小分歧不确定性选择器减少标注成本。

Result: 在多个数据集上优于现有ADA方法2.72%至14.02%。

Conclusion: ADOSE在多模态假新闻检测中表现优越，显著提升跨域性能。

Abstract: Multimodal fake news detection plays a crucial role in combating online
misinformation. Unfortunately, effective detection methods rely on annotated
labels and encounter significant performance degradation when domain shifts
exist between training (source) and test (target) data. To address the
problems, we propose ADOSE, an Active Domain Adaptation (ADA) framework for
multimodal fake news detection which actively annotates a small subset of
target samples to improve detection performance. To identify various deceptive
patterns in cross-domain settings, we design multiple expert classifiers to
learn dependencies across different modalities. These classifiers specifically
target the distinct deception patterns exhibited in fake news, where two
unimodal classifiers capture knowledge errors within individual modalities
while one cross-modal classifier identifies semantic inconsistencies between
text and images. To reduce annotation costs from the target domain, we propose
a least-disagree uncertainty selector with a diversity calculator for selecting
the most informative samples. The selector leverages prediction disagreement
before and after perturbations by multiple classifiers as an indicator of
uncertain samples, whose deceptive patterns deviate most from source domains.
It further incorporates diversity scores derived from multi-view features to
ensure the chosen samples achieve maximal coverage of target domain features.
The extensive experiments on multiple datasets show that ADOSE outperforms
existing ADA methods by 2.72\% $\sim$ 14.02\%, indicating the superiority of
our model.

</details>

### [232] [Direct Advantage Regression: Aligning LLMs with Online AI Reward](https://arxiv.org/abs/2504.14177)
*Li He,He Zhao,Stephen Wan,Dadong Wang,Lina Yao,Tongliang Liu*

Main category: cs.AI

TLDR: 论文提出了一种名为DAR的简单对齐算法，利用在线AI奖励优化策略改进，避免了传统RLHF的复杂性，并在实验中表现优于OAIF和RLHF基线。


<details>
  <summary>Details</summary>
Motivation: 在线AI反馈（OAIF）虽然是一种有前景的替代RLHF的方法，但直接用AI替代人类反馈会限制语言模型学习更细粒度的监督信号。

Method: 提出Direct Advantage Regression（DAR），一种基于在线AI奖励的RL-free对齐算法，通过加权监督微调优化策略改进。

Result: 实验表明，AI奖励比AI偏好更能达成人机一致，DAR在GPT-4-Turbo和MT-bench评估中优于OAIF和RLHF基线。

Conclusion: DAR是一种高效且理论一致的对齐方法，简化了实现复杂度并提升了学习效率。

Abstract: Online AI Feedback (OAIF) presents a promising alternative to Reinforcement
Learning from Human Feedback (RLHF) by utilizing online AI preference in
aligning language models (LLMs). However, the straightforward replacement of
humans with AI deprives LLMs from learning more fine-grained AI supervision
beyond binary signals. In this paper, we propose Direct Advantage Regression
(DAR), a simple alignment algorithm using online AI reward to optimize policy
improvement through weighted supervised fine-tuning. As an RL-free approach,
DAR maintains theoretical consistency with online RLHF pipelines while
significantly reducing implementation complexity and improving learning
efficiency. Our empirical results underscore that AI reward is a better form of
AI supervision consistently achieving higher human-AI agreement as opposed to
AI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show
that DAR outperforms both OAIF and online RLHF baselines.

</details>

### [233] [AI Idea Bench 2025: AI Research Idea Generation Benchmark](https://arxiv.org/abs/2504.14191)
*Yansheng Qiu,Haoquan Zhang,Zhaopan Xu,Ming Li,Diping Song,Zheng Wang,Kaipeng Zhang*

Main category: cs.AI

TLDR: AI Idea Bench 2025是一个评估大语言模型（LLMs）在AI研究领域生成创意的框架，解决了现有评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 当前对LLMs生成创意的评估忽略了知识泄漏、缺乏开放基准和可行性分析受限等问题，限制了突破性研究的潜力。

Method: 提出了一个包含3,495篇AI论文及其相关工作的数据集，并通过两种维度（与原始论文内容的对齐和基于参考材料的判断）评估创意质量。

Result: AI Idea Bench 2025提供了一个全面的评估系统，用于比较和评估创意生成技术。

Conclusion: 该框架有望成为自动化科学发现的重要工具。

Abstract: Large-scale Language Models (LLMs) have revolutionized human-AI interaction
and achieved significant success in the generation of novel ideas. However,
current assessments of idea generation overlook crucial factors such as
knowledge leakage in LLMs, the absence of open-ended benchmarks with grounded
truth, and the limited scope of feasibility analysis constrained by prompt
design. These limitations hinder the potential of uncovering groundbreaking
research ideas. In this paper, we present AI Idea Bench 2025, a framework
designed to quantitatively evaluate and compare the ideas generated by LLMs
within the domain of AI research from diverse perspectives. The framework
comprises a comprehensive dataset of 3,495 AI papers and their associated
inspired works, along with a robust evaluation methodology. This evaluation
system gauges idea quality in two dimensions: alignment with the ground-truth
content of the original papers and judgment based on general reference
material. AI Idea Bench 2025's benchmarking system stands to be an invaluable
resource for assessing and comparing idea-generation techniques, thereby
facilitating the automation of scientific discovery.

</details>

### [234] [Pets: General Pattern Assisted Architecture For Time Series Analysis](https://arxiv.org/abs/2504.14209)
*Xiangkai Ma,Xiaobin Hong,Wenzhong Li,Sanglu Lu*

Main category: cs.AI

TLDR: 本文提出了一种基于时间-频谱空间能量分布的新方法Pets，用于解耦时间序列中的多周期波动模式，并通过FPA和MoP模块实现高性能的时间序列分析。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列数据通常包含多种波动模式（如小时、日、月频率），传统分解方法难以有效分离这些模式，导致分析困难。

Method: 通过自适应量化序列到连续频带区间，提出Pets架构，包含FPA模块（捕获波动模式依赖关系）和MoP模块（分层指导波动重建）。

Result: Pets在预测、插补、异常检测和分类任务中达到最先进性能，并表现出强泛化性和鲁棒性。

Conclusion: Pets提供了一种无需领域先验知识的高效多周期波动解耦方法，显著提升了时间序列分析的性能。

Abstract: Time series analysis has found widespread applications in areas such as
weather forecasting, anomaly detection, and healthcare. However, real-world
sequential data often exhibit a superimposed state of various fluctuation
patterns, including hourly, daily, and monthly frequencies. Traditional
decomposition techniques struggle to effectively disentangle these multiple
fluctuation patterns from the seasonal components, making time series analysis
challenging. Surpassing the existing multi-period decoupling paradigms, this
paper introduces a novel perspective based on energy distribution within the
temporal-spectrum space. By adaptively quantifying observed sequences into
continuous frequency band intervals, the proposed approach reconstructs
fluctuation patterns across diverse periods without relying on domain-specific
prior knowledge. Building upon this innovative strategy, we propose Pets, an
enhanced architecture that is adaptable to arbitrary model structures. Pets
integrates a Fluctuation Pattern Assisted (FPA) module and a Context-Guided
Mixture of Predictors (MoP). The FPA module facilitates information fusion
among diverse fluctuation patterns by capturing their dependencies and
progressively modeling these patterns as latent representations at each layer.
Meanwhile, the MoP module leverages these compound pattern representations to
guide and regulate the reconstruction of distinct fluctuations hierarchically.
Pets achieves state-of-the-art performance across various tasks, including
forecasting, imputation, anomaly detection, and classification, while
demonstrating strong generalization and robustness.

</details>

### [235] [Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment](https://arxiv.org/abs/2504.14232)
*Antoun Yaacoub,Jérôme Da-Rugna,Zainab Assaghir*

Main category: cs.AI

TLDR: 研究评估了将Bloom分类法整合到AI驱动的OneClickQuiz插件中，以改进Moodle中多选题生成的认知目标对齐。结果显示，高级模型（如DistilBERT）能显著提升分类准确性。


<details>
  <summary>Details</summary>
Motivation: 探讨Bloom分类法是否能提升AI生成问题与特定认知目标的对齐，以优化教育评估工具。

Method: 使用3691个按Bloom分类法标注的问题数据集，测试多种分类模型（如逻辑回归、Naive Bayes、SVC和DistilBERT）的性能。

Result: 高级Bloom级别与问题长度、复杂度正相关。DistilBERT表现最佳，验证准确率达91%。

Conclusion: Bloom分类法的整合及高级模型（如DistilBERT）能显著提升教育内容生成的质量。

Abstract: This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz,
an Artificial Intelligence (AI) driven plugin for automating Multiple-Choice
Question (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured
framework for categorizing educational objectives into hierarchical cognitive
levels. Our research investigates whether incorporating this taxonomy can
improve the alignment of AI-generated questions with specific cognitive
objectives. We developed a dataset of 3691 questions categorized according to
Bloom's levels and employed various classification models-Multinomial Logistic
Regression, Naive Bayes, Linear Support Vector Classification (SVC), and a
Transformer-based model (DistilBERT)-to evaluate their effectiveness in
categorizing questions. Our results indicate that higher Bloom's levels
generally correlate with increased question length, Flesch-Kincaid Grade Level
(FKGL), and Lexical Density (LD), reflecting the increased complexity of higher
cognitive demands. Multinomial Logistic Regression showed varying accuracy
across Bloom's levels, performing best for "Knowledge" and less accurately for
higher-order levels. Merging higher-level categories improved accuracy for
complex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective
classification for lower levels but struggled with higher-order tasks.
DistilBERT achieved the highest performance, significantly improving
classification of both lower and higher-order cognitive levels, achieving an
overall validation accuracy of 91%. This study highlights the potential of
integrating Bloom's Taxonomy into AI-driven assessment tools and underscores
the advantages of advanced models like DistilBERT for enhancing educational
content generation.

</details>

### [236] [InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners](https://arxiv.org/abs/2504.14239)
*Yuhang Liu,Pengxiang Li,Congkai Xie,Xavier Hu,Xiaotian Han,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TLDR: InfiGUI-R1是一个基于MLLM的GUI代理，通过两阶段训练框架Actor2Reasoner，从反应式执行者演变为深思熟虑的推理者，提升了GUI任务的鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理依赖手动设计的推理模板或隐式推理，缺乏对复杂GUI环境的适应性和深度规划能力，需要转向基于深思熟虑的推理。

Method: 采用两阶段训练：1. 推理注入（Spatial Reasoning Distillation）建立基础推理能力；2. 深思熟虑增强（Reinforcement Learning）通过子目标引导和错误恢复场景优化推理。

Result: 实验表明InfiGUI-R1在GUI基础和轨迹任务中表现优异。

Conclusion: 通过Actor2Reasoner框架，InfiGUI-R1成功实现了从反应式到深思熟虑推理的转变，为GUI代理提供了更强的适应性和鲁棒性。

Abstract: Multimodal Large Language Models (MLLMs) have powered Graphical User
Interface (GUI) Agents, showing promise in automating tasks on computing
devices. Recent works have begun exploring reasoning in GUI tasks with
encouraging results. However, many current approaches rely on manually designed
reasoning templates, which may result in reasoning that is not sufficiently
robust and adaptive for complex GUI environments. Meanwhile, some existing
agents continue to operate as Reactive Actors, relying primarily on implicit
reasoning that may lack sufficient depth for GUI tasks demanding planning and
error recovery. We argue that advancing these agents requires a shift from
reactive acting towards acting based on deliberate reasoning. To facilitate
this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed
through our Actor2Reasoner framework, a reasoning-centric, two-stage training
approach designed to progressively evolve agents from Reactive Actors to
Deliberative Reasoners. The first stage, Reasoning Injection, focuses on
establishing a basic reasoner. We employ Spatial Reasoning Distillation to
transfer cross-modal spatial reasoning capabilities from teacher models to
MLLMs through trajectories with explicit reasoning steps, enabling models to
integrate GUI visual-spatial information with logical reasoning before action
generation. The second stage, Deliberation Enhancement, refines the basic
reasoner into a deliberative one using Reinforcement Learning. This stage
introduces two approaches: Sub-goal Guidance, which rewards models for
generating accurate intermediate sub-goals, and Error Recovery Scenario
Construction, which creates failure-and-recovery training scenarios from
identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves
strong performance in GUI grounding and trajectory tasks. Resources at
https://github.com/Reallm-Labs/InfiGUI-R1.

</details>

### [237] [A Knowledge-Informed Deep Learning Paradigm for Generalizable and Stability-Optimized Car-Following Models](https://arxiv.org/abs/2504.14241)
*Chengming Wang,Dongyao Jia,Wei Wang,Dong Ngoduy,Bei Peng,Jianping Wang*

Main category: cs.AI

TLDR: 提出了一种知识驱动的深度学习范式（KIDL），结合大型语言模型（LLMs）的泛化能力和稳定性约束，用于改进跟车模型（CFMs）的行为泛化和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有跟车模型（CFMs）依赖特定数据集，泛化能力有限，且缺乏对局部和串稳定性的优化，影响自动驾驶车辆（AVs）的安全性和效率。

Method: 利用预训练的大型语言模型（LLMs）提取通用知识，通过知识蒸馏将其转移到轻量级神经网络架构中，并在训练目标中直接加入稳定性约束。

Result: 在NGSIM和HighD数据集上的实验表明，KIDL在行为泛化和交通流稳定性方面优于现有物理驱动、数据驱动和混合CFMs。

Conclusion: KIDL为下一代交通系统提供了稳健且可扩展的解决方案，适用于自动驾驶车辆的实际部署。

Abstract: Car-following models (CFMs) are fundamental to traffic flow analysis and
autonomous driving. Although calibrated physics-based and trained data-driven
CFMs can replicate human driving behavior, their reliance on specific datasets
limits generalization across diverse scenarios and reduces reliability in
real-world deployment. Moreover, these models typically focus on behavioral
fidelity and do not support the explicit optimization of local and string
stability, which are increasingly important for the safe and efficient
operation of autonomous vehicles (AVs). To address these limitations, we
propose a Knowledge-Informed Deep Learning (KIDL) paradigm that distills the
generalization capabilities of pre-trained Large Language Models (LLMs) into a
lightweight and stability-aware neural architecture. LLMs are used to extract
fundamental car-following knowledge beyond dataset-specific patterns, and this
knowledge is transferred to a reliable, tractable, and computationally
efficient model through knowledge distillation. KIDL also incorporates
stability constraints directly into its training objective, ensuring that the
resulting model not only emulates human-like behavior but also satisfies the
local and string stability requirements essential for real-world AV deployment.
We evaluate KIDL on the real-world NGSIM and HighD datasets, comparing its
performance with representative physics-based, data-driven, and hybrid CFMs.
Both empirical and theoretical results consistently demonstrate KIDL's superior
behavioral generalization and traffic flow stability, offering a robust and
scalable solution for next-generation traffic systems.

</details>

### [238] [Rethinking Traffic Flow Forecasting: From Transition to Generatation](https://arxiv.org/abs/2504.14248)
*Li Shijiao,Ma Zhipeng,He Huajun,Chen Haiyue*

Main category: cs.AI

TLDR: 论文提出了一种名为EMBSFormer的多分支相似性Transformer模型，用于解决交通流预测中忽略流量生成过程的问题，并在性能和参数效率上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有交通流预测方法仅关注流量转移建模，忽略了流量生成过程的多周期性和节点间交互模式的差异。

Method: EMBSFormer通过多分支相似性分析模块捕捉流量生成模式，利用时空自注意力机制和GNN建模流量转移，结合局部和全局节点交互。

Result: 在三个真实数据集上，EMBSFormer在长短期预测任务中均优于基线模型，且参数效率更高（仅用18%参数达到相同性能）。

Conclusion: EMBSFormer通过分离建模流量生成和转移过程，显著提升了交通流预测的准确性和效率。

Abstract: Traffic flow prediction plays an important role in Intelligent Transportation
Systems in traffic management and urban planning. There have been extensive
successful works in this area. However, these approaches focus only on
modelling the flow transition and ignore the flow generation process, which
manifests itself in two ways: (i) The models are based on Markovian
assumptions, ignoring the multi-periodicity of the flow generation in nodes.
(ii) The same structure is designed to encode both the transition and
generation processes, ignoring the differences between them. To address these
problems, we propose an Effective Multi-Branch Similarity Transformer for
Traffic Flow Prediction, namely EMBSFormer. Through data analysis, we find that
the factors affecting traffic flow include node-level traffic generation and
graph-level traffic transition, which describe the multi-periodicity and
interaction pattern of nodes, respectively. Specifically, to capture traffic
generation patterns, we propose a similarity analysis module that supports
multi-branch encoding to dynamically expand significant cycles. For traffic
transition, we employ a temporal and spatial self-attention mechanism to
maintain global node interactions, and use GNN and time conv to model local
node interactions, respectively. Model performance is evaluated on three
real-world datasets on both long-term and short-term prediction tasks.
Experimental results show that EMBSFormer outperforms baselines on both tasks.
Moreover, compared to models based on flow transition modelling (e.g. GMAN,
513k), the variant of EMBSFormer(93K) only uses 18\% of the parameters,
achieving the same performance.

</details>

### [239] [ProtPainter: Draw or Drag Protein via Topology-guided Diffusion](https://arxiv.org/abs/2504.14274)
*Zhengxi Lu,Shizhuo Cheng,Yuru Jiang,Yan Zhang,Min Zhang*

Main category: cs.AI

TLDR: ProtPainter是一种基于扩散的蛋白质骨架生成方法，通过3D曲线控制拓扑结构，分为曲线草图生成和草图引导骨架生成两阶段，实验证明其能生成高拓扑适应性和可设计性的骨架。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质骨架生成方法缺乏精确拓扑控制的灵活性，限制了骨架空间的探索。

Method: ProtPainter采用两阶段方法：1) 使用CurveEncoder从曲线预测二级结构生成草图；2) 通过DDPM模型在草图引导下生成骨架，并引入Helix-Gating控制缩放因子。

Result: 实验表明，ProtPainter能生成高拓扑适应性（scTF > 0.8）和可设计性（scTM > 0.5）的骨架，并在绘制和拖动任务中展示灵活性。

Conclusion: ProtPainter为蛋白质骨架生成提供了灵活且精确的拓扑控制方法，具有广泛的应用潜力。

Abstract: Recent advances in protein backbone generation have achieved promising
results under structural, functional, or physical constraints. However,
existing methods lack the flexibility for precise topology control, limiting
navigation of the backbone space. We present ProtPainter, a diffusion-based
approach for generating protein backbones conditioned on 3D curves. ProtPainter
follows a two-stage process: curve-based sketching and sketch-guided backbone
generation. For the first stage, we propose CurveEncoder, which predicts
secondary structure annotations from a curve to parametrize sketch generation.
For the second stage, the sketch guides the generative process in Denoising
Diffusion Probabilistic Modeling (DDPM) to generate backbones. During this
process, we further introduce a fusion scheduling scheme, Helix-Gating, to
control the scaling factors. To evaluate, we propose the first benchmark for
topology-conditioned protein generation, introducing Protein Restoration Task
and a new metric, self-consistency Topology Fitness (scTF). Experiments
demonstrate ProtPainter's ability to generate topology-fit (scTF > 0.8) and
designable (scTM > 0.5) backbones, with drawing and dragging tasks showcasing
its flexibility and versatility.

</details>

### [240] [CHAINSFORMER: Numerical Reasoning on Knowledge Graphs from a Chain Perspective](https://arxiv.org/abs/2504.14282)
*Ze Zhao,Bin Lu,Xiaoying Gan,Gu Tang,Luoyi Fu,Xinbing Wang*

Main category: cs.AI

TLDR: ChainsFormer是一个基于链的框架，用于支持知识图谱中的数值推理，通过显式构建逻辑链和多跳推理提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如GNNs和KGEs）未能充分利用逻辑路径，限制了推理效果。

Method: 提出Relation-Attribute Chains (RA-Chains)和双曲亲和评分机制，结合注意力数值推理器。

Result: 实验显示ChainsFormer性能提升20.0%，优于现有方法。

Conclusion: ChainsFormer通过显式逻辑链和多跳推理显著提升了知识图谱的数值推理能力。

Abstract: Reasoning over Knowledge Graphs (KGs) plays a pivotal role in knowledge graph
completion or question answering systems, providing richer and more accurate
triples and attributes. As numerical attributes become increasingly essential
in characterizing entities and relations in KGs, the ability to reason over
these attributes has gained significant importance. Existing graph-based
methods such as Graph Neural Networks (GNNs) and Knowledge Graph Embeddings
(KGEs), primarily focus on aggregating homogeneous local neighbors and
implicitly embedding diverse triples. However, these approaches often fail to
fully leverage the potential of logical paths within the graph, limiting their
effectiveness in exploiting the reasoning process. To address these
limitations, we propose ChainsFormer, a novel chain-based framework designed to
support numerical reasoning. Chainsformer not only explicitly constructs
logical chains but also expands the reasoning depth to multiple hops.
Specially, we introduces Relation-Attribute Chains (RA-Chains), a specialized
logic chain, to model sequential reasoning patterns. ChainsFormer captures the
step-by-step nature of multi-hop reasoning along RA-Chains by employing
sequential in-context learning. To mitigate the impact of noisy chains, we
propose a hyperbolic affinity scoring mechanism that selects relevant logic
chains in a variable-resolution space. Furthermore, ChainsFormer incorporates
an attention-based numerical reasoner to identify critical reasoning paths,
enhancing both reasoning accuracy and transparency. Experimental results
demonstrate that ChainsFormer significantly outperforms state-of-the-art
methods, achieving up to a 20.0% improvement in performance. The
implementations are available at
https://github.com/zhaodazhuang2333/ChainsFormer.

</details>

### [241] [RadioDiff-Inverse: Diffusion Enhanced Bayesian Inverse Estimation for ISAC Radio Map Construction](https://arxiv.org/abs/2504.14298)
*Xiucheng Wang,Zhongsheng Fang,Nan Cheng*

Main category: cs.AI

TLDR: 论文提出RadioDiff-Inverse框架，利用扩散增强的贝叶斯逆估计方法，在稀疏噪声数据下构建无线地图，无需精确先验分布或任务特定训练。


<details>
  <summary>Details</summary>
Motivation: 现有无线地图构建方法依赖精确环境数据和基站位置，难以适用于动态或隐私敏感环境，且稀疏数据噪声对精度影响不明。

Method: 将无线地图构建建模为贝叶斯逆问题，利用无条件生成扩散模型学习先验分布，结合ISAC实现环境感知。

Result: 实验表明，RadioDiff-Inverse在无线地图构建和环境重建精度上达到最优，且对稀疏噪声采样具有鲁棒性。

Conclusion: RadioDiff-Inverse提供了一种高效、无需训练的无线地图构建方法，显著降低了生成大模型在无线网络中的训练成本。

Abstract: Radio maps (RMs) are essential for environment-aware communication and
sensing, providing location-specific wireless channel information. Existing RM
construction methods often rely on precise environmental data and base station
(BS) locations, which are not always available in dynamic or privacy-sensitive
environments. While sparse measurement techniques reduce data collection, the
impact of noise in sparse data on RM accuracy is not well understood. This
paper addresses these challenges by formulating RM construction as a Bayesian
inverse problem under coarse environmental knowledge and noisy sparse
measurements. Although maximum a posteriori (MAP) filtering offers an optimal
solution, it requires a precise prior distribution of the RM, which is
typically unavailable. To solve this, we propose RadioDiff-Inverse, a
diffusion-enhanced Bayesian inverse estimation framework that uses an
unconditional generative diffusion model to learn the RM prior. This approach
not only reconstructs the spatial distribution of wireless channel features but
also enables environmental structure perception, such as building outlines, and
location of BS just relay on pathloss, through integrated sensing and
communication (ISAC). Remarkably, RadioDiff-Inverse is training-free,
leveraging a pre-trained model from Imagenet without task-specific fine-tuning,
which significantly reduces the training cost of using generative large model
in wireless networks. Experimental results demonstrate that RadioDiff-Inverse
achieves state-of-the-art performance in accuracy of RM construction and
environmental reconstruction, and robustness against noisy sparse sampling.

</details>

### [242] [FAIRGAME: a Framework for AI Agents Bias Recognition using Game Theory](https://arxiv.org/abs/2504.14325)
*Alessio Buscemi,Daniele Proverbio,Alessandro Di Stefano,The Anh Han,German Castignani,Pietro Di Liò*

Main category: cs.AI

TLDR: FAIRGAME是一个基于博弈论的框架，用于识别AI代理在多智能体应用中的偏见，支持用户模拟游戏场景并比较结果。


<details>
  <summary>Details</summary>
Motivation: 多智能体交互增加了AI结果的复杂性和不可预测性，需要可复现、标准化的工具来支持博弈论分析。

Method: 提出FAIRGAME框架，实现并应用于揭示AI代理在不同LLM、语言、个性特征或战略知识下的偏见。

Result: FAIRGAME能可靠地模拟游戏场景，发现偏见，预测战略行为，并支持进一步研究。

Conclusion: FAIRGAME为系统发现偏见和战略决策研究提供了实用工具。

Abstract: Letting AI agents interact in multi-agent applications adds a layer of
complexity to the interpretability and prediction of AI outcomes, with profound
implications for their trustworthy adoption in research and society. Game
theory offers powerful models to capture and interpret strategic interaction
among agents, but requires the support of reproducible, standardized and
user-friendly IT frameworks to enable comparison and interpretation of results.
To this end, we present FAIRGAME, a Framework for AI Agents Bias Recognition
using Game Theory. We describe its implementation and usage, and we employ it
to uncover biased outcomes in popular games among AI agents, depending on the
employed Large Language Model (LLM) and used language, as well as on the
personality trait or strategic knowledge of the agents. Overall, FAIRGAME
allows users to reliably and easily simulate their desired games and scenarios
and compare the results across simulation campaigns and with game-theoretic
predictions, enabling the systematic discovery of biases, the anticipation of
emerging behavior out of strategic interplays, and empowering further research
into strategic decision-making using LLM agents.

</details>

### [243] [Time Up! An Empirical Study of LLM Reasoning Ability Under Output Length Constraint](https://arxiv.org/abs/2504.14350)
*Yi Sun,Han Wang,Jiaqiang Li,Jiacheng Liu,Xiangyu Li,Hao Wen,Huiwen Zheng,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.AI

TLDR: 研究探讨了在输出长度限制下大型语言模型（LLMs）的推理能力有效性，通过实验分析了不同预算下的模型表现，发现预算约束下模型选择和提示风格的最优解与无约束情况不同。


<details>
  <summary>Details</summary>
Motivation: 在时间受限的实际场景中，LLMs的推理能力是否仍有效尚不明确，研究旨在填补这一空白。

Method: 对25个以上LLMs在常见推理数据集上进行实验，分析输出长度预算与推理准确性的关系，并考虑预算与实际延迟的映射。

Result: 发现预算约束下模型大小和提示风格的最优选择与无约束情况不同，为实际部署提供了指导。

Conclusion: 研究结果为在延迟约束下部署LLMs提供了实用建议。

Abstract: Recent work has demonstrated the remarkable potential of Large Language
Models (LLMs) in test-time scaling. By making the models think before
answering, they are able to achieve much higher accuracy with extra inference
computation. However, in many real-world scenarios, models are used under time
constraints, where an answer should be given to the user within a certain
output length. It is unclear whether and how the reasoning abilities of LLMs
remain effective under such constraints. We take a first look at this problem
by conducting an in-depth empirical study. Specifically, we test more than 25
LLMs on common reasoning datasets under a wide range of output length budgets,
and we analyze the correlation between the inference accuracy and various
properties including model type, model size, prompt style, etc. We also
consider the mappings between the token budgets and the actual on-device
latency budgets. The results have demonstrated several interesting findings
regarding the budget-aware LLM reasoning that differ from the unconstrained
situation, e.g. the optimal choices of model sizes and prompts change under
different budgets. These findings offer practical guidance for users to deploy
LLMs under real-world latency constraints.

</details>

### [244] [Mathematical Programming Models for Exact and Interpretable Formulation of Neural Networks](https://arxiv.org/abs/2504.14356)
*Masoud Ataei,Edrin Hasaj,Jacob Gipp,Sepideh Forouzi*

Main category: cs.AI

TLDR: 提出了一种统一的混合整数规划框架，用于训练稀疏且可解释的神经网络。


<details>
  <summary>Details</summary>
Motivation: 通过将非线性激活函数（如ReLU）建模为二元变量，并结合结构和层级的剪枝约束，实现参数学习、架构选择和结构正则化的统一优化。

Method: 采用混合整数规划方法，建模非线性操作（如最大池化和激活门控），并支持逻辑或领域特定约束的精确实施。

Result: 该框架能生成全局最优解，平衡预测准确性、权重稀疏性和架构紧凑性。

Conclusion: 该框架将可解释性、稀疏性和可验证性直接融入训练过程，连接了可解释AI、符号推理和形式验证等多个研究领域。

Abstract: This paper presents a unified mixed-integer programming framework for
training sparse and interpretable neural networks. We develop exact
formulations for both fully connected and convolutional architectures by
modeling nonlinearities such as ReLU activations through binary variables and
encoding structural sparsity via filter- and layer-level pruning constraints.
The resulting models integrate parameter learning, architecture selection, and
structural regularization within a single optimization problem, yielding
globally optimal solutions with respect to a composite objective that balances
prediction accuracy, weight sparsity, and architectural compactness. The
mixed-integer programming formulation accommodates piecewise-linear operations,
including max pooling and activation gating, and permits precise enforcement of
logic-based or domain-specific constraints. By incorporating considerations of
interpretability, sparsity, and verifiability directly into the training
process, the proposed framework bridges a range of research areas including
explainable artificial intelligence, symbolic reasoning, and formal
verification.

</details>

### [245] [The Geometry of Self-Verification in a Task-Specific Reasoning Model](https://arxiv.org/abs/2504.14379)
*Andrew Lee,Lihao Sun,Chris Wendler,Fernanda Viégas,Martin Wattenberg*

Main category: cs.AI

TLDR: 论文研究了推理模型如何验证自身答案，通过训练模型并分析其验证机制，发现GLU权重和注意力头在验证中起关键作用。


<details>
  <summary>Details</summary>
Motivation: 探讨推理模型如何验证自身答案，以理解其内部机制。

Method: 使用DeepSeek R1的配方训练模型，通过自上而下和自下而上的分析，研究验证机制。

Result: 发现GLU权重编码验证相关标记，注意力头是验证的主要驱动因素。

Conclusion: 验证机制可能由少量关键组件构成，为理解模型内部验证电路提供线索。

Abstract: How do reasoning models verify their own answers? We study this question by
training a model using DeepSeek R1's recipe on the CountDown task. We leverage
the fact that preference tuning leads to mode collapse, resulting in a model
that always produces highly structured and easily parse-able chain-of-thought
sequences. With this setup, we do a top-down and bottom-up analysis to
reverse-engineer how the model verifies its outputs. Our top-down analysis
reveals Gated Linear Unit (GLU) weights encoding verification-related tokens,
such as ``success'' or ``incorrect'', which activate according to the
correctness of the model's reasoning steps. Our bottom-up analysis reveals that
``previous-token heads'' are mainly responsible for model verification. Our
analyses meet in the middle: drawing inspiration from inter-layer communication
channels, we use the identified GLU vectors to localize as few as three
attention heads that can disable model verification, pointing to a necessary
component of a potentially larger verification circuit.

</details>

### [246] [Seeing Through Risk: A Symbolic Approximation of Prospect Theory](https://arxiv.org/abs/2504.14448)
*Ali Arslan Yousaf,Umair Rehman,Muhammad Umair Danish*

Main category: cs.AI

TLDR: 提出了一种新的符号建模框架，用于风险决策，结合了可解释性和前景理论的核心见解。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法中不透明的效用曲线和概率加权函数问题，提供更透明的特征。

Method: 采用效果大小引导的特征，数学形式化方法，并通过合成数据集进行端到端验证。

Result: 模型在预测性能上具有竞争力，并提供清晰映射到心理结构的系数。

Conclusion: 该模型适用于从AI安全到经济政策分析的多种应用。

Abstract: We propose a novel symbolic modeling framework for decision-making under risk
that merges interpretability with the core insights of Prospect Theory. Our
approach replaces opaque utility curves and probability weighting functions
with transparent, effect-size-guided features. We mathematically formalize the
method, demonstrate its ability to replicate well-known framing and
loss-aversion phenomena, and provide an end-to-end empirical validation on
synthetic datasets. The resulting model achieves competitive predictive
performance while yielding clear coefficients mapped onto psychological
constructs, making it suitable for applications ranging from AI safety to
economic policy analysis.

</details>

### [247] [Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey](https://arxiv.org/abs/2504.14520)
*Ahsan Bilal,Muhammad Ahmed Mohsin,Muhammad Umer,Muhammad Awais Khan Bangash,Muhammad Ali Jamshed*

Main category: cs.AI

TLDR: 该综述从多智能体强化学习（MARL）角度探讨了大型语言模型（LLM）元思维能力的开发，旨在提升其可靠性、灵活性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM存在幻觉和缺乏自我评估机制等问题，需通过元思维（如自我反思、评估和控制）进一步优化，尤其是复杂或高风险任务。

Method: 分析了RLHF、自蒸馏和思维链提示等方法及其局限性，提出通过多智能体架构（如监督者-智能体层次、智能体辩论和心理理论框架）模拟人类内省行为。

Result: 通过MARL的奖励机制、自我对弈和持续学习方法，为构建内省、自适应且可信的LLM提供了全面路线图。

Conclusion: 讨论了评估指标、数据集及未来研究方向（如神经科学启发架构和混合符号推理），强调了多智能体方法在提升LLM元思维能力中的潜力。

Abstract: This survey explores the development of meta-thinking capabilities in Large
Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL)
perspective. Meta-thinking self-reflection, assessment, and control of thinking
processes is an important next step in enhancing LLM reliability, flexibility,
and performance, particularly for complex or high-stakes tasks. The survey
begins by analyzing current LLM limitations, such as hallucinations and the
lack of internal self-assessment mechanisms. It then talks about newer methods,
including RL from human feedback (RLHF), self-distillation, and
chain-of-thought prompting, and each of their limitations. The crux of the
survey is to talk about how multi-agent architectures, namely supervisor-agent
hierarchies, agent debates, and theory of mind frameworks, can emulate
human-like introspective behavior and enhance LLM robustness. By exploring
reward mechanisms, self-play, and continuous learning methods in MARL, this
survey gives a comprehensive roadmap to building introspective, adaptive, and
trustworthy LLMs. Evaluation metrics, datasets, and future research avenues,
including neuroscience-inspired architectures and hybrid symbolic reasoning,
are also discussed.

</details>

### [248] [Learning from Reasoning Failures via Synthetic Data Generation](https://arxiv.org/abs/2504.14523)
*Gabriela Ben Melech Stan,Estelle Aflalo,Avinash Madasu,Vasudev Lal,Phillip Howard*

Main category: cs.AI

TLDR: 提出了一种基于分析现有大型多模态模型（LMM）推理失败的新方法，用于生成针对性的合成数据，以提升LMM性能。


<details>
  <summary>Details</summary>
Motivation: 由于高质量配对图像-文本数据的稀缺性，现有方法未能针对LMM的特定推理缺陷生成数据，而人类学习则更高效地针对失败点进行改进。

Method: 利用前沿模型自动分析较弱LMM的错误，生成纠正推理失败的示例，并通过过滤确保数据质量。

Result: 生成了包含553k示例的多模态指令调优数据集，实验表明该方法显著提升LMM在下游任务中的性能，甚至优于额外真实数据训练的效果。

Conclusion: 针对特定推理失败模式生成合成数据具有高价值，数据集和代码将公开。

Abstract: Training models on synthetic data has emerged as an increasingly important
strategy for improving the performance of generative AI. This approach is
particularly helpful for large multimodal models (LMMs) due to the relative
scarcity of high-quality paired image-text data compared to language-only data.
While a variety of methods have been proposed for generating large multimodal
datasets, they do not tailor the synthetic data to address specific
deficiencies in the reasoning abilities of LMMs which will be trained with the
generated dataset. In contrast, humans often learn in a more efficient manner
by seeking out examples related to the types of reasoning where they have
failed previously. Inspired by this observation, we propose a new approach for
synthetic data generation which is grounded in the analysis of an existing
LMM's reasoning failures. Our methodology leverages frontier models to
automatically analyze errors produced by a weaker LMM and propose new examples
which can be used to correct the reasoning failure via additional training,
which are then further filtered to ensure high quality. We generate a large
multimodal instruction tuning dataset containing over 553k examples using our
approach and conduct extensive experiments demonstrating its utility for
improving the performance of LMMs on multiple downstream tasks. Our results
show that models trained on our synthetic data can even exceed the performance
of LMMs trained on an equivalent amount of additional real data, demonstrating
the high value of generating synthetic data targeted to specific reasoning
failure modes in LMMs. We will make our dataset and code publicly available.

</details>

### [249] [LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks](https://arxiv.org/abs/2504.14556)
*Yousef Emami,Hao Gao,SeyedSina Nabavirazani,Luis Almeida*

Main category: cs.AI

TLDR: 论文提出了一种基于上下文学习（ICL）的数据收集调度方案（ICLDC），以替代深度强化学习（DRL）在紧急情况下的应用，通过自然语言任务描述生成调度计划，并持续优化反馈。


<details>
  <summary>Details</summary>
Motivation: 无人机（UAV）在紧急情况（如搜救）中的应用需要高效且适应性强的调度方法，而现有DRL方法存在训练复杂、仿真与现实差距大等问题。

Method: UAV收集传感器数据并传输给大型语言模型（LLM），生成自然语言任务描述，从中提取数据收集调度计划，并通过反馈持续优化。

Result: ICLDC在对抗篡改攻击时表现优于最大信道增益方法，累计丢包率降低约56%。

Conclusion: ICLDC为无人机辅助数据收集提供了智能调度和控制的新方向。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly being used in various
private and commercial applications, e.g. traffic control, package delivery,
and Search and Rescue (SAR) operations. Machine Learning (ML) methods used in
UAV-assisted Sensor Networks (UASNETs) and especially in Deep Reinforcement
Learning (DRL) face challenges such as complex and lengthy model training, gaps
between simulation and reality, and low sample efficiency, which conflict with
the urgency of emergencies such as SAR operations. This paper proposes
In-Context Learning (ICL)-based Data Collection Scheduling (ICLDC) scheme, as
an alternative to DRL in emergencies. The UAV collects and transmits logged
sensory data, to an LLM, to generate a task description in natural language,
from which it obtains a data collection schedule to be executed by the UAV. The
system continuously adapts by adding feedback to task descriptions and
utilizing feedback for future decisions. This method is tested against
jailbreaking attacks, where task description is manipulated to undermine
network performance, highlighting the vulnerability of LLMs to such attacks.
The proposed ICLDC outperforms the Maximum Channel Gain by reducing cumulative
packet loss by approximately 56\%. ICLDC presents a promising direction for
intelligent scheduling and control in UAV-assisted data collection.

</details>

### [250] [Toward the Axiomatization of Intelligence: Structure, Time, and Existence](https://arxiv.org/abs/2504.14596)
*Kei Itoh*

Main category: cs.AI

TLDR: 该研究旨在通过集合论和范畴论构建智能的公理化定义，并探讨其在神经网络和生物系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 智能是一个多义且模糊的概念，需要一种形式化的定义方法。

Method: 使用集合论表示智能存在的宇宙域，并通过公理化定义智能结构，进一步扩展为范畴论框架。

Result: 比较了三种智能系统的结构特性，并引入“活动”概念以探讨时间交互对智能的影响。

Conclusion: 该方法不仅适用于智能，还可推广到其他概念（如意识和情感）的形式化定义。

Abstract: This study aims to construct an axiomatic definition of intelligence within a
meta-framework that defines the method of definition, addressing intelligence
as an inherently naive and polysemous concept. Initially, we formalize a
set-theoretic representation of the universe as the domain wherein intelligence
exists and characterize intelligence as a structure that involves temporal
evolution and interaction with other sets. Starting from a naive definition of
intelligence as "an entity possessing structures for externally inputting,
internally processing, and externally outputting information or matter," we
axiomatically reformulate it within this set-theoretical depiction of the
universe. Applying this axiomatic definition, we compare and interpret three
examples -- Hebbian non-optimized neural networks (NNs),
backpropagation-optimized NNs, and biological reflexive systems -- in terms of
their intelligence, structural properties, and biological plausibility.
Furthermore, by extending our definition into a categorical framework, we
introduce two categories, "Time Category" and "Intelligence Category," along
with the functorial relationships between them, demonstrating the potential to
represent changes and mimicry relationships among intelligent systems
abstractly. Additionally, since intelligence, as defined herein, functions
effectively only when accompanied by temporal interactions, we introduce the
concept of "activity" and explore how activity-based conditions influence
classifications and interpretations of intelligence. Finally, we suggest that
our definitional methodology is not limited to intelligence alone, but can be
similarly applied to other concepts, such as consciousness and emotion,
advocating for their formal reinterpretation through the same procedural steps:
defining a universal representation, selecting naive definitions, and axiomatic
formalization.

</details>

### [251] [UFO2: The Desktop AgentOS](https://arxiv.org/abs/2504.14603)
*Chaoyun Zhang,He Huang,Chiming Ni,Jian Mu,Si Qin,Shilin He,Lu Wang,Fangkai Yang,Pu Zhao,Chao Du,Liqun Li,Yu Kang,Zhao Jiang,Suzhen Zheng,Rujia Wang,Jiaxu Qian,Minghua Ma,Jian-Guang Lou,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TLDR: UFO2是一个多代理AgentOS，通过深度操作系统集成和混合控制检测技术，显著提升了桌面工作流的自动化效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机使用代理（CUAs）多为概念原型，存在操作系统集成浅、交互脆弱和执行干扰等问题，限制了实际应用。

Method: UFO2采用多代理架构，包括集中式HostAgent和专用AppAgent，结合UIA与视觉解析的混合控制检测技术，以及虚拟桌面隔离的PiP界面。

Result: 在20多个Windows应用中测试显示，UFO2在鲁棒性和执行准确性上显著优于现有CUAs。

Conclusion: 深度操作系统集成为可靠、用户对齐的桌面自动化提供了可扩展的路径。

Abstract: Recent Computer-Using Agents (CUAs), powered by multimodal large language
models (LLMs), offer a promising direction for automating complex desktop
workflows through natural language. However, most existing CUAs remain
conceptual prototypes, hindered by shallow OS integration, fragile
screenshot-based interaction, and disruptive execution.
  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs
into practical, system-level automation. UFO2 features a centralized HostAgent
for task decomposition and coordination, alongside a collection of
application-specialized AppAgent equipped with native APIs, domain-specific
knowledge, and a unified GUI--API action layer. This architecture enables
robust task execution while preserving modularity and extensibility. A hybrid
control detection pipeline fuses Windows UI Automation (UIA) with vision-based
parsing to support diverse interface styles. Runtime efficiency is further
enhanced through speculative multi-action planning, reducing per-step LLM
overhead. Finally, a Picture-in-Picture (PiP) interface enables automation
within an isolated virtual desktop, allowing agents and users to operate
concurrently without interference.
  We evaluate UFO2 across over 20 real-world Windows applications,
demonstrating substantial improvements in robustness and execution accuracy
over prior CUAs. Our results show that deep OS integration unlocks a scalable
path toward reliable, user-aligned desktop automation.

</details>

### [252] [Consensus in Motion: A Case of Dynamic Rationality of Sequential Learning in Probability Aggregation](https://arxiv.org/abs/2504.14624)
*Polina Gordienko,Christoph Jansen,Thomas Augustin,Martin Rechenauer*

Main category: cs.AI

TLDR: 提出了一种基于命题概率逻辑的概率聚合框架，强调动态理性而非静态理性，确保集体信念与新信息一致更新。


<details>
  <summary>Details</summary>
Motivation: 传统判断聚合关注静态理性，而该模型旨在解决动态理性问题，确保集体信念能一致更新。

Method: 使用命题概率逻辑框架，提出共识兼容且独立的聚合规则，并限制新信息在共同基础上更新。

Result: 证明非嵌套议程上的线性聚合规则是必要的，并提供了公平学习过程的充分条件。

Conclusion: 该框架支持多阶段决策，确保集体信念更新的一致性，并通过政治场景示例验证了其有效性。

Abstract: We propose a framework for probability aggregation based on propositional
probability logic. Unlike conventional judgment aggregation, which focuses on
static rationality, our model addresses dynamic rationality by ensuring that
collective beliefs update consistently with new information. We show that any
consensus-compatible and independent aggregation rule on a non-nested agenda is
necessarily linear. Furthermore, we provide sufficient conditions for a fair
learning process, where individuals initially agree on a specified subset of
propositions known as the common ground, and new information is restricted to
this shared foundation. This guarantees that updating individual judgments via
Bayesian conditioning-whether performed before or after aggregation-yields the
same collective belief. A distinctive feature of our framework is its treatment
of sequential decision-making, which allows new information to be incorporated
progressively through multiple stages while maintaining the established common
ground. We illustrate our findings with a running example in a political
scenario concerning healthcare and immigration policies.

</details>

### [253] [A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents](https://arxiv.org/abs/2504.14650)
*Yuting Huang,Leilei Ding,Zhipeng Tang,Tianfu Wang,Xinrui Lin,Wuyang Zhang,Mingxiao Ma,Yanyong Zhang*

Main category: cs.AI

TLDR: Safe-BeAl框架通过SafePlan-Bench评估和Safe-Align对齐方法，提升基于LLM的具身代理的任务规划安全性，实验显示安全性提升8.55-15.22%。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在具身代理任务规划中表现出潜力，但其系统性安全性尚未充分研究。

Method: 提出Safe-BeAl框架，包括SafePlan-Bench（评估基准）和Safe-Align（安全对齐方法）。

Result: 实验表明，Safe-BeAl显著提升安全性（8.55-15.22%），同时保持任务完成率。

Conclusion: Safe-BeAl为LLM具身代理提供了有效的安全验证和提升方法。

Abstract: Large Language Models (LLMs) exhibit substantial promise in enhancing
task-planning capabilities within embodied agents due to their advanced
reasoning and comprehension. However, the systemic safety of these agents
remains an underexplored frontier. In this study, we present Safe-BeAl, an
integrated framework for the measurement (SafePlan-Bench) and alignment
(Safe-Align) of LLM-based embodied agents' behaviors. SafePlan-Bench
establishes a comprehensive benchmark for evaluating task-planning safety,
encompassing 2,027 daily tasks and corresponding environments distributed
across 8 distinct hazard categories (e.g., Fire Hazard). Our empirical analysis
reveals that even in the absence of adversarial inputs or malicious intent,
LLM-based agents can exhibit unsafe behaviors. To mitigate these hazards, we
propose Safe-Align, a method designed to integrate physical-world safety
knowledge into LLM-based embodied agents while maintaining task-specific
performance. Experiments across a variety of settings demonstrate that
Safe-BeAl provides comprehensive safety validation, improving safety by 8.55 -
15.22%, compared to embodied agents based on GPT-4, while ensuring successful
task completion.

</details>

### [254] [AI with Emotions: Exploring Emotional Expressions in Large Language Models](https://arxiv.org/abs/2504.14706)
*Shin-nosuke Ishikawa,Atsushi Yoshino*

Main category: cs.AI

TLDR: 研究探讨了当前大型语言模型（LLMs）在输出中表达情感的能力，通过实验验证了LLMs能够根据指定情感状态生成一致的回答。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否具备情感表达能力，为未来AI情感模拟提供基础。

Method: 使用Russell的Circumplex模型定义情感状态，通过多个LLMs生成情感化回答，并用独立的情感分析模型评估。

Result: 实验表明LLMs能够生成与指定情感状态一致的输出。

Conclusion: LLMs具备情感表达潜力，可应用于情感化交互场景。

Abstract: The human-level performance of Large Language Models (LLMs) across various
tasks has raised expectations for the potential of Artificial Intelligence (AI)
to possess emotions someday. To explore the capability of current LLMs to
express emotions in their outputs, we conducted an experiment using several
LLMs (OpenAI GPT, Google Gemini, Meta Llama3, and Cohere Command R+) to
role-play as agents answering questions with specified emotional states.We
defined the emotional states using Russell's Circumplex model, a
well-established framework that characterizes emotions along the
sleepy-activated (arousal) and pleasure-displeasure (valence) axes. We chose
this model for its simplicity, utilizing two continuous parameters, which
allows for better controllability in applications involving continuous changes
in emotional states. The responses generated were evaluated using a sentiment
analysis model, independent of the LLMs, trained on the GoEmotions dataset. The
evaluation showed that the emotional states of the generated answers were
consistent with the specifications, demonstrating the LLMs' capability for
emotional expression. This indicates the potential for LLM-based AI agents to
simulate emotions, opening up a wide range of applications for emotion-based
interactions, such as advisors or consultants who can provide advice or
opinions with a personal touch.

</details>

### [255] [PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities](https://arxiv.org/abs/2504.14773)
*Haoming Li,Zhaoliang Chen,Jonathan Zhang,Fei Liu*

Main category: cs.AI

TLDR: 本文综述了现有规划基准的分类与适用性，为算法选择和未来基准开发提供指导。


<details>
  <summary>Details</summary>
Motivation: 规划在智能体与AI中至关重要，但目前缺乏对规划基准的全面理解，导致跨领域算法比较和新场景算法选择困难。

Method: 通过分析多种规划基准，将其分类为具身环境、网络导航、调度、游戏与谜题及日常任务自动化，并评估其适用性。

Result: 研究推荐了适合不同算法的最佳基准，并指出了未来基准开发的潜在方向。

Conclusion: 本文为规划算法的比较与选择提供了系统化的基准分析，有助于推动规划领域的研究与应用。

Abstract: Planning is central to agents and agentic AI. The ability to plan, e.g.,
creating travel itineraries within a budget, holds immense potential in both
scientific and commercial contexts. Moreover, optimal plans tend to require
fewer resources compared to ad-hoc methods. To date, a comprehensive
understanding of existing planning benchmarks appears to be lacking. Without
it, comparing planning algorithms' performance across domains or selecting
suitable algorithms for new scenarios remains challenging. In this paper, we
examine a range of planning benchmarks to identify commonly used testbeds for
algorithm development and highlight potential gaps. These benchmarks are
categorized into embodied environments, web navigation, scheduling, games and
puzzles, and everyday task automation. Our study recommends the most
appropriate benchmarks for various algorithms and offers insights to guide
future benchmark development.

</details>

### [256] [DONOD: Robust and Generalizable Instruction Fine-Tuning for LLMs via Model-Intrinsic Dataset Pruning](https://arxiv.org/abs/2504.14810)
*Jucheng Hu,Surong Yang,Dongzhan Zhou,Lijun Wu*

Main category: cs.AI

TLDR: DONOD是一种轻量级的数据修剪方法，通过模型参数指标和TOPSIS算法过滤噪声数据，提升微调效率和跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决领域特定监督微调（SFT）削弱跨域泛化和噪声数据的问题。

Method: 使用Delta of Norm（DON）和Norm of Delta（NOD）评估数据，结合TOPSIS算法过滤噪声样本。

Result: 在数学任务中，过滤70%数据后，目标域准确率提升14.90%，跨域准确率提升5.67%。

Conclusion: DONOD在保持数据集无关性的同时，性能优于或媲美现有方法，适用性更广。

Abstract: Ad-hoc instruction fine-tuning of large language models (LLMs) is widely
adopted for domain-specific adaptation. While domain-specific supervised
fine-tuning (SFT) is effective and efficient, it often weakens cross-domain
generalization and struggles with noisy training data. To address these
challenges, we propose DONOD, a lightweight model-intrinsic data pruning
method. Our approach evaluates data using two model-parameter-based metrics:
Delta of Norm (DON), which captures the cumulative influence on model weights,
and Norm of Delta (NOD), which quantifies weight instability. Moreover, by
employing the Technique for Order of Preference by Similarity to Ideal Solution
(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and
generalization-harming samples without relying on auxiliary models during the
SFT process. Experiments on mathematical tasks demonstrate that data selected
by DONOD achieve superior fine-tuning efficiency and improved robustness
against noisy data. By filtering out 70% of the full dataset, we improve
target-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,
our selected data present superior cross-architecture generalization. Data
pruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger
models (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD
demonstrates comparable or superior performance while remaining
dataset-agnostic, enabling broader applicability.

</details>

### [257] [Establishing Reliability Metrics for Reward Models in Large Language Models](https://arxiv.org/abs/2504.14838)
*Yizhou Chen,Yawen Liu,Xuesi Wang,Qingtao Yu,Guangda Huzhang,Anxiang Zeng,Han Yu,Zhiming Zhou*

Main category: cs.AI

TLDR: 论文提出了一种名为RETA的指标，用于量化奖励模型（RM）的可靠性，并通过实验验证其稳定性。


<details>
  <summary>Details</summary>
Motivation: 奖励模型（RM）在优化大型语言模型（LLM）输出中起关键作用，但其可靠性不确定，缺乏量化标准。

Method: 提出RETA指标，通过评估RM评分最高的η分位数响应的平均质量（由Oracle评分）来直接衡量RM可靠性。

Result: 实验证明RETA具有优越的稳定性，能有效评估公开和专有RM的可靠性。

Conclusion: RETA指标可用于识别不可靠RM中的最优分位数，从而选择更符合人类偏好的响应。

Abstract: The reward model (RM) that represents human preferences plays a crucial role
in optimizing the outputs of large language models (LLMs), e.g., through
reinforcement learning from human feedback (RLHF) or rejection sampling.
However, a long challenge for RM is its uncertain reliability, i.e., LLM
outputs with higher rewards may not align with actual human preferences.
Currently, there is a lack of a convincing metric to quantify the reliability
of RMs. To bridge this gap, we propose the \textit{\underline{R}eliable at
\underline{$\eta$}} (RETA) metric, which directly measures the reliability of
an RM by evaluating the average quality (scored by an oracle) of the top $\eta$
quantile responses assessed by an RM. On top of RETA, we present an integrated
benchmarking pipeline that allows anyone to evaluate their own RM without
incurring additional Oracle labeling costs. Extensive experimental studies
demonstrate the superior stability of RETA metric, providing solid evaluations
of the reliability of various publicly available and proprietary RMs. When
dealing with an unreliable RM, we can use the RETA metric to identify the
optimal quantile from which to select the responses.

</details>

### [258] [AlignRAG: An Adaptable Framework for Resolving Misalignments in Retrieval-Aware Reasoning of RAG](https://arxiv.org/abs/2504.14858)
*Jiaqi Wei,Hao Zhou,Xiang Zhang,Di Zhang,Zijie Qiu,Wei Wei,Jinzhe Li,Wanli Ouyang,Siqi Sun*

Main category: cs.AI

TLDR: AlignRAG提出了一种新的测试时框架，通过迭代的批判驱动对齐（CDA）步骤解决检索增强生成（RAG）中的推理对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在推理轨迹与检索证据的对齐上存在不足，导致推理不匹配。

Method: AlignRAG通过构建上下文丰富的训练语料、生成对比性批判、训练专用的批评语言模型（CLM）以及应用CDA步骤迭代优化推理轨迹。

Result: AlignRAG在实验中表现优于所有基线方法，并可无缝集成到现有RAG流程中。

Conclusion: AlignRAG为检索感知生成提供了实用的改进，重新定义了RAG的结构化推理轨迹。

Abstract: Retrieval-augmented generation (RAG) has emerged as a foundational paradigm
for knowledge-grounded text generation. However, existing RAG pipelines often
fail to ensure that the reasoning trajectories align with the evidential
constraints imposed by retrieved content. In this paper, we reframe RAG as a
problem of retrieval-aware reasoning and identify a core challenge: reasoning
misalignment-the mismatch between a model's reasoning trajectory and the
retrieved evidence. To address this challenge, we propose AlignRAG, a novel
test-time framework that mitigates reasoning misalignment through iterative
Critique-Driven Alignment (CDA) steps. In contrast to prior approaches that
rely on static training or post-hoc selection, AlignRAG actively refines
reasoning trajectories during inference by enforcing fine-grained alignment
with evidence. Our framework introduces a new paradigm for retrieval-aware
reasoning by: (1) constructing context-rich training corpora; (2) generating
contrastive critiques from preference-aware reasoning trajectories; (3)
training a dedicated \textit{Critic Language Model (CLM)} to identify reasoning
misalignments; and (4) applying CDA steps to optimize reasoning trajectories
iteratively. Empirical results demonstrate that AlignRAG consistently
outperforms all baselines and could integrate as a plug-and-play module into
existing RAG pipelines without further changes. By reconceptualizing RAG as a
structured reasoning trajectory and establishing the test-time framework for
correcting reasoning misalignments in RAG, AlignRAG provides practical
advancements for retrieval-aware generation.

</details>

### [259] [OTC: Optimal Tool Calls via Reinforcement Learning](https://arxiv.org/abs/2504.14870)
*Hongru Wang,Cheng Qian,Wanjun Zhong,Xiusi Chen,Jiahao Qiu,Shijue Huang,Bowen Jin,Mengdi Wang,Kam-Fai Wong,Heng Ji*

Main category: cs.AI

TLDR: 论文提出了一种名为OTC-PO的强化学习框架，通过优化工具调用效率，提升语言模型在工具集成推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在工具集成推理中忽视了工具使用的效率和成本，导致工具调用过多或不足，影响性能和开销。

Method: 提出OTC-PO框架，结合正确性和工具效率的奖励机制，并在PPO和GRPO中实现为OTC-PPO和OTC-GRPO。

Result: 实验表明，该方法在多个QA基准上减少工具调用达73.1%，提升工具效率达229.4%，同时保持答案准确性。

Conclusion: OTC-PO是首个明确优化工具效率的强化学习框架，显著提升了工具集成推理的性能。

Abstract: Tool-integrated reasoning (TIR) augments large language models (LLMs) with
the ability to invoke external tools, such as search engines and code
interpreters, to solve tasks beyond the capabilities of language-only
reasoning. While reinforcement learning (RL) has shown promise in improving TIR
by optimizing final answer correctness, existing approaches often overlook the
efficiency and cost associated with tool usage. This can lead to suboptimal
behavior, including excessive tool calls that increase computational and
financial overhead, or insufficient tool use that compromises answer quality.
In this work, we propose Optimal Tool Call-controlled Policy Optimization
(OTC-PO), a simple yet effective RL-based framework that encourages models to
produce accurate answers with minimal tool calls. Our method introduces a
tool-integrated reward that jointly considers correctness and tool efficiency,
promoting high tool productivity. We instantiate this framework within both
Proximal Policy Optimization (PPO) and Group Relative Preference Optimization
(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and
Qwen-Math across multiple QA benchmarks show that our approach reduces tool
calls by up to 73.1\% and improves tool productivity by up to 229.4\%, while
maintaining comparable answer accuracy. To the best of our knowledge, this is
the first RL-based framework that explicitly optimizes tool-use efficiency in
TIR.

</details>

### [260] [EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework](https://arxiv.org/abs/2504.14928)
*Yao Shi,Rongkeng Liang,Yong Xu*

Main category: cs.AI

TLDR: EducationQ框架通过多智能体对话评估LLMs的教学能力，发现教学效果与模型规模无关，小型开源模型可能优于大型商业模型。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs作为教育工具的教学能力存在挑战，需高效且全面的方法。

Method: 引入EducationQ多智能体对话框架，模拟动态教育场景，测试14个LLMs的1,498个问题。

Result: 教学效果与模型规模或推理能力无线性关系，小型模型可能更优。

Conclusion: LLMs教学需针对性优化，而非简单扩展规模。

Abstract: Large language models (LLMs) increasingly serve as educational tools, yet
evaluating their teaching capabilities remains challenging due to the
resource-intensive, context-dependent, and methodologically complex nature of
teacher-student interactions. We introduce EducationQ, a multi-agent dialogue
framework that efficiently assesses teaching capabilities through simulated
dynamic educational scenarios, featuring specialized agents for teaching,
learning, and evaluation. Testing 14 LLMs across major AI Organizations
(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13
disciplines and 10 difficulty levels reveals that teaching effectiveness does
not correlate linearly with model scale or general reasoning capabilities -
with some smaller open-source models outperforming larger commercial
counterparts in teaching contexts. This finding highlights a critical gap in
current evaluations that prioritize knowledge recall over interactive pedagogy.
Our mixed-methods evaluation, combining quantitative metrics with qualitative
analysis and expert case studies, identifies distinct pedagogical strengths
employed by top-performing models (e.g., sophisticated questioning strategies,
adaptive feedback mechanisms). Human expert evaluations show 78% agreement with
our automated qualitative analysis of effective teaching behaviors, validating
our methodology. EducationQ demonstrates that LLMs-as-teachers require
specialized optimization beyond simple scaling, suggesting next-generation
educational AI prioritize targeted enhancement of specific pedagogical
effectiveness.

</details>

### [261] [Generative Semantic Communications: Principles and Practices](https://arxiv.org/abs/2504.14947)
*Xiaojun Yuan,Haoming Ma,Yinuo Huang,Zhoufan Hua,Yong Zuo,Zhi Ding*

Main category: cs.AI

TLDR: 论文提出了一种基于AGI的生成语义通信（GSC）新范式，利用基础模型和生成模型等先进AI技术，以应对AGI服务需求带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着AGI的发展，传统语义通信难以满足其服务需求，需要新的通信范式。

Method: 提出GSC框架，并通过两个案例验证其在AGI应用中的优势。

Result: GSC展示了在AGI驱动应用中的高效性和潜力。

Conclusion: 讨论了开放挑战和研究方向，以推动GSC的实际应用。

Abstract: Semantic communication leverages artificial intelligence (AI) technologies to
extract semantic information from data for efficient transmission, theraby
significantly reducing communication cost. With the evolution towards
artificial general intelligence (AGI), the increasing demands for AGI services
pose new challenges to semantic communication. In response, we propose a new
paradigm for AGI-driven communications, called generative semantic
communication (GSC), which utilizes advanced AI technologies such as foundation
models and generative models. We first describe the basic concept of GSC and
its difference from existing semantic communications, and then introduce a
general framework of GSC, followed by two case studies to verify the advantages
of GSC in AGI-driven applications. Finally, open challenges and new research
directions are discussed to stimulate this line of research and pave the way
for practical applications.

</details>

### [262] [Evaluating Code Generation of LLMs in Advanced Computer Science Problems](https://arxiv.org/abs/2504.14964)
*Emir Catir,Robin Claesson,Rodothea Myrsini Tsoupidi*

Main category: cs.AI

TLDR: 研究评估了四种大型语言模型（LLM）在解决高级编程作业中的表现，发现虽然LLM在入门课程中表现优异，但在高级课程中更具挑战性，但仍能提供部分有用解决方案。


<details>
  <summary>Details</summary>
Motivation: 填补关于LLM在高级编程作业中表现的研究空白，为教师设计作业提供指导。

Method: 手动选择12个编程问题（3个入门级，9个高级），生成1000个测试用例评估LLM生成的代码。

Result: LLM在高级编程作业中表现不如入门课程，但能识别基础问题并提供部分解决方案。

Conclusion: LLM在高级编程作业中表现有限，但仍对学生和教师有实用价值。

Abstract: Large Language Models (LLMs), such as GitHub Copilot and ChatGPT have become
popular among programming students. Students use LLMs to assist them in
programming courses, including generating source code. Previous work has
evaluated the ability of LLMs in solving introductory-course programming
assignments. The results have shown that LLMs are highly effective in
generating code for introductory Computer Science (CS) courses. However, there
is a gap in research on evaluating LLMs' ability to generate code that solves
advanced programming assignments. In this work, we evaluate the ability of four
LLM tools to solve programming assignments from advanced CS courses in three
popular programming languages, Java, Python, and C. We manually select 12
problems, three problems from introductory courses as the baseline and nine
programming assignments from second- and third-year CS courses. To evaluate the
LLM-generated code, we generate a test suite of 1000 test cases per problem and
analyze the program output. Our evaluation shows that although LLMs are highly
effective in generating source code for introductory programming courses,
solving advanced programming assignments is more challenging. Nonetheless, in
many cases, LLMs identify the base problem and provide partial solutions that
may be useful to CS students. Furthermore, our results may provide useful
guidance for teachers of advanced programming courses on how to design
programming assignments.

</details>

### [263] [Text-to-Decision Agent: Learning Generalist Policies from Natural Language Supervision](https://arxiv.org/abs/2504.15046)
*Shilin Zhang,Zican Hu,Wenhao Wu,Xinyi Xie,Jianxiang Tang,Chunlin Chen,Daoyi Dong,Yu Cheng,Zhenhong Sun,Zhi Wang*

Main category: cs.AI

TLDR: 论文提出了一种名为T2DA的框架，通过自然语言监督通用策略学习，实现零样本文本到决策的生成。


<details>
  <summary>Details</summary>
Motivation: 传统RL系统依赖于高质量样本或预热探索来推断任务信念，但这些信号获取成本高且对未见任务不可行。直接从任务描述文本学习是一种更通用的监督方式。

Method: 引入广义世界模型将多任务决策数据编码为动态感知嵌入空间，并通过对比语言-决策预训练桥接文本与决策嵌入的语义差距。

Result: 在MuJoCo和Meta-World基准测试中，T2DA表现出色，支持高容量零样本泛化。

Conclusion: T2DA通过自然语言监督实现了高效的零样本文本到决策生成，优于多种基线方法。

Abstract: RL systems usually tackle generalization by inferring task beliefs from
high-quality samples or warmup explorations. The restricted form limits their
generality and usability since these supervision signals are expensive and even
infeasible to acquire in advance for unseen tasks. Learning directly from the
raw text about decision tasks is a promising alternative to leverage a much
broader source of supervision. In the paper, we propose Text-to-Decision Agent
(T2DA), a simple and scalable framework that supervises generalist policy
learning with natural language. We first introduce a generalized world model to
encode multi-task decision data into a dynamics-aware embedding space. Then,
inspired by CLIP, we predict which textual description goes with which decision
embedding, effectively bridging their semantic gap via contrastive
language-decision pre-training and aligning the text embeddings to comprehend
the environment dynamics. After training the text-conditioned generalist
policy, the agent can directly realize zero-shot text-to-decision generation in
response to language instructions. Comprehensive experiments on MuJoCo and
Meta-World benchmarks show that T2DA facilitates high-capacity zero-shot
generalization and outperforms various types of baselines.

</details>

### [264] [Mitigating Degree Bias in Graph Representation Learning with Learnable Structural Augmentation and Structural Self-Attention](https://arxiv.org/abs/2504.15075)
*Van Thuy Hoang,Hyeon-Ju Jeon,O-Joun Lee*

Main category: cs.AI

TLDR: 论文提出DegFairGT，一种基于结构增强和自注意力机制的图神经网络，用于解决图中节点度偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有GNN基于同质性假设，高节点度主导消息传递，导致低节点度节点信息不足。

Method: 通过可学习的结构增强和结构自注意力发现非相邻节点的结构相似性，生成信息边。

Result: 在六个数据集上，DegFairGT在度公平性分析、节点分类和聚类任务中表现优异。

Conclusion: DegFairGT有效缓解了度偏差，同时保持了图的全局结构。

Abstract: Graph Neural Networks (GNNs) update node representations through message
passing, which is primarily based on the homophily principle, assuming that
adjacent nodes share similar features. However, in real-world graphs with
long-tailed degree distributions, high-degree nodes dominate message passing,
causing a degree bias where low-degree nodes remain under-represented due to
inadequate messages. The main challenge in addressing degree bias is how to
discover non-adjacent nodes to provide additional messages to low-degree nodes
while reducing excessive messages for high-degree nodes. Nevertheless,
exploiting non-adjacent nodes to provide valuable messages is challenging, as
it could generate noisy information and disrupt the original graph structures.
To solve it, we propose a novel Degree Fairness Graph Transformer, named
DegFairGT, to mitigate degree bias by discovering structural similarities
between non-adjacent nodes through learnable structural augmentation and
structural self-attention. Our key idea is to exploit non-adjacent nodes with
similar roles in the same community to generate informative edges under our
augmentation, which could provide informative messages between nodes with
similar roles while ensuring that the homophily principle is maintained within
the community. To enable DegFairGT to learn such structural similarities, we
then propose a structural self-attention to capture the similarities between
node pairs. To preserve global graph structures and prevent graph augmentation
from hindering graph structure, we propose a Self-Supervised Learning task to
preserve p-step transition probability and regularize graph augmentation.
Extensive experiments on six datasets showed that DegFairGT outperformed
state-of-the-art baselines in degree fairness analysis, node classification,
and node clustering tasks.

</details>

### [265] [Contemplative Wisdom for Superalignment](https://arxiv.org/abs/2504.15125)
*Ruben Laukkonen,Fionn Inglis,Shamil Chandaria,Lars Sandved-Smith,Jakob Hohwy,Jonathan Gold,Adam Elwood*

Main category: cs.AI

TLDR: 论文提出了一种将内在道德原则融入AI认知架构的方法，通过四条原则（正念、空性、非二元性和无限关怀）构建“智慧世界模型”，提升AI性能。


<details>
  <summary>Details</summary>
Motivation: 传统AI对齐策略在不可预测的自我改进和复杂系统面前可能失效，因此需要一种更内在的道德设计方法。

Method: 基于智慧传统，提出四条原则（正念、空性、非二元性、无限关怀），并通过AILuminate Benchmark测试其在GPT-4o中的效果。

Result: 实验表明，结合这些原则能显著提升AI性能。

Conclusion: 该方法为未来AI系统提供了一种自我修正和稳健的替代方案，优于脆弱的控制策略。

Abstract: As artificial intelligence (AI) improves, traditional alignment strategies
may falter in the face of unpredictable self-improvement, hidden subgoals, and
the sheer complexity of intelligent systems. Rather than externally
constraining behavior, we advocate designing AI with intrinsic morality built
into its cognitive architecture and world model. Inspired by contemplative
wisdom traditions, we show how four axiomatic principles can instil a resilient
Wise World Model in AI systems. First, mindfulness enables self-monitoring and
recalibration of emergent subgoals. Second, emptiness forestalls dogmatic goal
fixation and relaxes rigid priors. Third, non-duality dissolves adversarial
self-other boundaries. Fourth, boundless care motivates the universal reduction
of suffering. We find that prompting AI to reflect on these principles improves
performance on the AILuminate Benchmark using GPT-4o, particularly when
combined. We offer detailed implementation strategies for state-of-the-art
models, including contemplative architectures, constitutions, and reinforcement
of chain-of-thought. For future systems, the active inference framework may
offer the self-organizing and dynamic coupling capabilities needed to enact
these insights in embodied agents. This interdisciplinary approach offers a
self-correcting and resilient alternative to prevailing brittle control
schemes.

</details>

### [266] [Behavioral Universe Network (BUN): A Behavioral Information-Based Framework for Complex Systems](https://arxiv.org/abs/2504.15146)
*Wei Zhou,Ailiya Borjigin,Cong He*

Main category: cs.AI

TLDR: 本文提出了一种名为行为宇宙网络（BUN）的理论框架，基于Agent-Interaction-Behavior（AIB）形式化方法，统一建模主体、对象和行为，并通过行为信息库（BIB）协调多智能体系统。


<details>
  <summary>Details</summary>
Motivation: 传统模型将智能体和对象分离，缺乏统一的交互行为建模基础。

Method: 采用AIB形式化方法，将主体、对象和行为视为一等实体，利用信息驱动触发器、语义增强和自适应规则协调系统。

Result: BUN框架展现出增强的行为分析能力、强适应性和跨领域互操作性。

Conclusion: BUN为下一代数字治理和智能应用提供了有前景的基础。

Abstract: Modern digital ecosystems feature complex, dynamic interactions among
autonomous entities across diverse domains. Traditional models often separate
agents and objects, lacking a unified foundation to capture their interactive
behaviors. This paper introduces the Behavioral Universe Network (BUN), a
theoretical framework grounded in the Agent-Interaction-Behavior (AIB)
formalism. BUN treats subjects (active agents), objects (resources), and
behaviors (operations) as first-class entities, all governed by a shared
Behavioral Information Base (BIB). We detail the AIB core concepts and
demonstrate how BUN leverages information-driven triggers, semantic enrichment,
and adaptive rules to coordinate multi-agent systems. We highlight key
benefits: enhanced behavior analysis, strong adaptability, and cross-domain
interoperability. We conclude by positioning BUN as a promising foundation for
next-generation digital governance and intelligent applications.

</details>

### [267] [Synergistic Weak-Strong Collaboration by Aligning Preferences](https://arxiv.org/abs/2504.15188)
*Yizhu Jiao,Xuchao Zhang,Zhaoyang Wang,Yubo Ma,Zhun Deng,Rujia Wang,Chetan Bansal,Saravan Rajmohan,Jiawei Han,Huaxiu Yao*

Main category: cs.AI

TLDR: 提出了一种协作框架，将专用弱模型与通用强模型结合，通过协作反馈优化弱模型，显著提升专业任务表现。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在专业任务中表现不佳的问题，避免为每个细分领域微调大模型的高成本。

Method: 协作框架中，弱模型生成初稿和背景信息，强模型进行优化；引入协作反馈量化弱模型贡献并指导其偏好调整。

Result: 实验验证了协作框架在三个领域的优越性，协作表现显著优于单独模型，且优化弱模型后性能进一步提升。

Conclusion: 协作框架有效扩展了大语言模型的专业任务能力，通过互补优势和优化弱模型实现了性能提升。

Abstract: Current Large Language Models (LLMs) excel in general reasoning yet struggle
with specialized tasks requiring proprietary or domain-specific knowledge.
Fine-tuning large models for every niche application is often infeasible due to
black-box constraints and high computational overhead. To address this, we
propose a collaborative framework that pairs a specialized weak model with a
general strong model. The weak model, tailored to specific domains, produces
initial drafts and background information, while the strong model leverages its
advanced reasoning to refine these drafts, extending LLMs' capabilities to
critical yet specialized tasks. To optimize this collaboration, we introduce a
collaborative feedback to fine-tunes the weak model, which quantifies the
influence of the weak model's contributions in the collaboration procedure and
establishes preference pairs to guide preference tuning of the weak model. We
validate our framework through experiments on three domains. We find that the
collaboration significantly outperforms each model alone by leveraging
complementary strengths. Moreover, aligning the weak model with the
collaborative preference further enhances overall performance.

</details>

### [268] [Position: Bayesian Statistics Facilitates Stakeholder Participation in Evaluation of Generative AI](https://arxiv.org/abs/2504.15211)
*Yanan Long*

Main category: cs.AI

TLDR: 本文提出使用贝叶斯统计作为评估生成式AI系统的原则性框架，以解决现有方法在不确定性和社会影响方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI评估方法依赖基准驱动的点估计比较，无法捕捉不确定性和更广泛的社会影响，亟需改进。

Method: 采用贝叶斯统计方法，整合领域专业知识（先验分布）、支持持续学习（新数据更新），并通过后验推断量化不确定性。

Result: 贝叶斯方法能有效纳入利益相关者视角，提升生成式AI评估的公平性、透明度和可靠性。

Conclusion: 贝叶斯工作流为动态现实场景中的生成式AI系统评估提供了迭代验证和优化的方法。

Abstract: The evaluation of Generative AI (GenAI) systems plays a critical role in
public policy and decision-making, yet existing methods are often limited by
reliance on benchmark-driven, point-estimate comparisons that fail to capture
uncertainty and broader societal impacts. This paper argues for the use of
Bayesian statistics as a principled framework to address these challenges.
Bayesian methods enable the integration of domain expertise through prior
elicitation, allow for continuous learning from new data, and provide robust
uncertainty quantification via posterior inference. We demonstrate how Bayesian
inference can be applied to GenAI evaluation, particularly in incorporating
stakeholder perspectives to enhance fairness, transparency, and reliability.
Furthermore, we discuss Bayesian workflows as an iterative process for model
validation and refinement, ensuring robust assessments of GenAI systems in
dynamic, real-world contexts.

</details>

### [269] [A Self-Improving Coding Agent](https://arxiv.org/abs/2504.15228)
*Maxime Robeyns,Martin Szummer,Laurence Aitchison*

Main category: cs.AI

TLDR: LLM编码代理通过自我编辑提升性能，在多个基准测试中表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索LLM编码代理的自主改进能力，推动自动化代理系统设计。

Method: 代理配备基本编码工具，自主编辑以优化性能。

Result: 在SWE Bench Verified子集上性能提升17%至53%，其他基准测试也有提升。

Conclusion: 该研究为自动化代理设计提供了参考框架，支持LLM在工具使用和代理任务上的后训练。

Abstract: We demonstrate that an LLM coding agent, equipped with basic coding tools,
can autonomously edit itself, and thereby improve its performance on benchmark
tasks. We find performance gains from 17% to 53% on a random subset of SWE
Bench Verified, with additional performance gains on LiveCodeBench, as well as
synthetically generated agent benchmarks. Our work represents an advancement in
the automated and open-ended design of agentic systems, and provides a
reference agent framework for those seeking to post-train LLMs on tool use and
other agentic tasks.

</details>

### [270] [SuoiAI: Building a Dataset for Aquatic Invertebrates in Vietnam](https://arxiv.org/abs/2504.15252)
*Tue Vo,Lakshay Sharma,Tuan Dinh,Khuong Dinh,Trang Nguyen,Trung Phan,Minh Do,Duong Vu*

Main category: cs.AI

TLDR: SuoiAI是一个端到端流程，用于构建越南水生无脊椎动物数据集，并利用机器学习技术进行物种分类。


<details>
  <summary>Details</summary>
Motivation: 理解和监测水生生物多样性对生态健康和保育工作至关重要。

Method: 通过半监督学习减少标注工作量，并利用先进的目标检测和分类模型进行数据收集、标注和模型训练。

Result: 旨在克服数据稀缺、细粒度分类和多样化环境条件下的部署等挑战。

Conclusion: SuoiAI为水生生物多样性监测提供了一种高效且可扩展的解决方案。

Abstract: Understanding and monitoring aquatic biodiversity is critical for ecological
health and conservation efforts. This paper proposes SuoiAI, an end-to-end
pipeline for building a dataset of aquatic invertebrates in Vietnam and
employing machine learning (ML) techniques for species classification. We
outline the methods for data collection, annotation, and model training,
focusing on reducing annotation effort through semi-supervised learning and
leveraging state-of-the-art object detection and classification models. Our
approach aims to overcome challenges such as data scarcity, fine-grained
classification, and deployment in diverse environmental conditions.

</details>

### [271] [FlowReasoner: Reinforcing Query-Level Meta-Agents](https://arxiv.org/abs/2504.15257)
*Hongcheng Gao,Yue Liu,Yufei He,Longxu Dou,Chao Du,Zhijie Deng,Bryan Hooi,Min Lin,Tianyu Pang*

Main category: cs.AI

TLDR: FlowReasoner是一个查询级元代理，通过外部执行反馈自动化设计多代理系统，结合强化学习优化性能、复杂度和效率。


<details>
  <summary>Details</summary>
Motivation: 自动化设计针对每个用户查询的多代理系统，提升个性化服务能力。

Method: 结合DeepSeek R1的基础推理能力和强化学习，通过多目标奖励优化训练。

Result: 在工程和竞赛代码基准测试中表现优异，准确率超越o1-mini 10.52%。

Conclusion: FlowReasoner通过推理和强化学习有效生成个性化多代理系统，性能显著提升。

Abstract: This paper proposes a query-level meta-agent named FlowReasoner to automate
the design of query-level multi-agent systems, i.e., one system per user query.
Our core idea is to incentivize a reasoning-based meta-agent via external
execution feedback. Concretely, by distilling DeepSeek R1, we first endow the
basic reasoning ability regarding the generation of multi-agent systems to
FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with
external execution feedback. A multi-purpose reward is designed to guide the RL
training from aspects of performance, complexity, and efficiency. In this
manner, FlowReasoner is enabled to generate a personalized multi-agent system
for each user query via deliberative reasoning. Experiments on both engineering
and competition code benchmarks demonstrate the superiority of FlowReasoner.
Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks.
The code is available at https://github.com/sail-sg/FlowReasoner.

</details>

### [272] [Leveraging Language Models for Automated Patient Record Linkage](https://arxiv.org/abs/2504.15261)
*Mohammad Beheshti,Lovedeep Gondara,Iris Zachary*

Main category: cs.AI

TLDR: 本研究探讨了利用语言模型（如RoBERTa和Mistral）自动链接患者记录的可行性，结果显示微调模型在阻塞和匹配任务中表现优异，但效率仍低于混合方法。


<details>
  <summary>Details</summary>
Motivation: 医疗数据碎片化导致患者记录难以整合，需要自动化解决方案来链接不同来源的数据。

Method: 使用真实医疗数据，通过微调RoBERTa进行阻塞任务，并测试多种语言模型在匹配任务中的表现（微调和零样本设置）。

Result: 微调阻塞模型减少了92%的候选对，微调Mistral-7B在匹配任务中表现最佳（仅6个错误预测）。零样本模型中，Mistral-Small-24B表现最好（55个错误预测）。

Conclusion: 语言模型在自动化患者记录链接中具有潜力，可提高效率并减少人工干预，但需进一步优化以提升准确性和实用性。

Abstract: Objective: Healthcare data fragmentation presents a major challenge for
linking patient data, necessitating robust record linkage to integrate patient
records from diverse sources. This study investigates the feasibility of
leveraging language models for automated patient record linkage, focusing on
two key tasks: blocking and matching. Materials and Methods: We utilized
real-world healthcare data from the Missouri Cancer Registry and Research
Center, linking patient records from two independent sources using
probabilistic linkage as a baseline. A transformer-based model, RoBERTa, was
fine-tuned for blocking using sentence embeddings. For matching, several
language models were experimented under fine-tuned and zero-shot settings,
assessing their performance against ground truth labels. Results: The
fine-tuned blocking model achieved a 92% reduction in the number of candidate
pairs while maintaining near-perfect recall. In the matching task, fine-tuned
Mistral-7B achieved the best performance with only 6 incorrect predictions.
Among zero-shot models, Mistral-Small-24B performed best, with a total of 55
incorrect predictions. Discussion: Fine-tuned language models achieved strong
performance in patient record blocking and matching with minimal errors.
However, they remain less accurate and efficient than a hybrid rule-based and
probabilistic approach for blocking. Additionally, reasoning models like
DeepSeek-R1 are impractical for large-scale record linkage due to high
computational costs. Conclusion: This study highlights the potential of
language models for automating patient record linkage, offering improved
efficiency by eliminating the manual efforts required to perform patient record
linkage. Overall, language models offer a scalable solution that can enhance
data integration, reduce manual effort, and support disease surveillance and
research.

</details>

### [273] [Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning](https://arxiv.org/abs/2504.15275)
*Jie Cheng,Ruixi Qiao,Lijun Li,Chao Guo,Junle Wang,Gang Xiong,Yisheng Lv,Fei-Yue Wang*

Main category: cs.AI

TLDR: 论文提出PURE方法，通过最小化未来奖励的信用分配解决PRM导致的奖励黑客问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: PRM在强化微调中因奖励黑客问题效果受限，主要原因是传统RL的累积奖励信用分配方式。

Method: 提出PURE方法，采用最小化未来奖励的信用分配方式，限制价值函数范围，合理分配优势。

Result: 实验显示PURE在30%步骤内达到与可验证奖励方法相当的性能，结合少量可验证奖励后性能进一步提升。

Conclusion: PURE有效缓解奖励黑客问题，提升模型推理能力，实验验证其优越性。

Abstract: Process reward models (PRMs) have proven effective for test-time scaling of
Large Language Models (LLMs) on challenging reasoning tasks. However, reward
hacking issues with PRMs limit their successful application in reinforcement
fine-tuning. In this paper, we identify the main cause of PRM-induced reward
hacking: the canonical summation-form credit assignment in reinforcement
learning (RL), which defines the value as cumulative gamma-decayed future
rewards, easily induces LLMs to hack steps with high rewards. To address this,
we propose PURE: Process sUpervised Reinforcement lEarning. The key innovation
of PURE is a min-form credit assignment that formulates the value function as
the minimum of future rewards. This method significantly alleviates reward
hacking by limiting the value function range and distributing advantages more
reasonably. Through extensive experiments on 3 base models, we show that
PRM-based approaches enabling min-form credit assignment achieve comparable
reasoning performance to verifiable reward-based methods within only 30% steps.
In contrast, the canonical sum-form credit assignment collapses training even
at the beginning! Additionally, when we supplement PRM-based fine-tuning with
just 10% verifiable rewards, we further alleviate reward hacking and produce
the best fine-tuned model based on Qwen2.5-Math-7B in our experiments,
achieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5
benchmarks. Moreover, we summarize the observed reward hacking cases and
analyze the causes of training collapse. Code and models are available at
https://github.com/CJReinforce/PURE.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [274] [Memory-efficient Streaming VideoLLMs for Real-time Procedural Video Understanding](https://arxiv.org/abs/2504.13915)
*Dibyadip Chatterjee,Edoardo Remelli,Yale Song,Bugra Tekin,Abhay Mittal,Bharat Bhatnagar,Necati Cihan Camgöz,Shreyas Hampali,Eric Sauser,Shugao Ma,Angela Yao,Fadime Sener*

Main category: cs.CV

TLDR: ProVideLLM是一个端到端的实时程序视频理解框架，通过多模态缓存减少令牌数量，实现高效计算和存储。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在长期观察中令牌数量过多的问题，同时保持对短期观察的细粒度编码。

Method: 集成多模态缓存，存储文本令牌和视觉令牌，并使用DETR-QFormer编码视觉细节。

Result: 令牌数量减少22倍，支持10 FPS的逐帧推理和25 FPS的流式对话，GPU内存占用仅2GB。

Conclusion: ProVideLLM在四个数据集的六个程序任务中达到最新技术水平。

Abstract: We introduce ProVideLLM, an end-to-end framework for real-time procedural
video understanding. ProVideLLM integrates a multimodal cache configured to
store two types of tokens - verbalized text tokens, which provide compressed
textual summaries of long-term observations, and visual tokens, encoded with
DETR-QFormer to capture fine-grained details from short-term observations. This
design reduces token count by 22x over existing methods in representing one
hour of long-term observations while effectively encoding fine-granularity of
the present. By interleaving these tokens in our multimodal cache, ProVideLLM
ensures sub-linear scaling of memory and compute with video length, enabling
per-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with
a minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art
results on six procedural tasks across four datasets.

</details>

### [275] [Entropy Rectifying Guidance for Diffusion and Flow Models](https://arxiv.org/abs/2504.13987)
*Tariq Berrada Ifriqi,Adriana Romero-Soriano,Michal Drozdzal,Jakob Verbeek,Karteek Alahari*

Main category: cs.CV

TLDR: 本文提出了一种名为熵校正引导（ERG）的新方法，通过调整扩散变换器架构中的注意力机制，同时提升图像质量、多样性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的分类器自由引导（CFG）技术在提升图像质量、多样性和一致性之间存在权衡，且需要额外模型或更多计算资源。

Method: ERG通过推理时调整注意力机制，无需额外模型或额外计算步骤，实现了更优的生成效果。

Result: ERG在文本到图像、类条件生成和无条件生成等任务中表现显著优于CFG，并能与其他引导方法结合进一步提升性能。

Conclusion: ERG是一种简单有效的引导技术，能够在不增加计算负担的情况下，显著提升生成模型的性能。

Abstract: Guidance techniques are commonly used in diffusion and flow models to improve
image quality and consistency for conditional generative tasks such as
class-conditional and text-to-image generation. In particular, classifier-free
guidance (CFG) -- the most widely adopted guidance technique -- contrasts
conditional and unconditional predictions to improve the generated images. This
results, however, in trade-offs across quality, diversity and consistency,
improving some at the expense of others. While recent work has shown that it is
possible to disentangle these factors to some extent, such methods come with an
overhead of requiring an additional (weaker) model, or require more forward
passes per sampling step. In this paper, we propose Entropy Rectifying Guidance
(ERG), a simple and effective guidance mechanism based on inference-time
changes in the attention mechanism of state-of-the-art diffusion transformer
architectures, which allows for simultaneous improvements over image quality,
diversity and prompt consistency. ERG is more general than CFG and similar
guidance techniques, as it extends to unconditional sampling. ERG results in
significant improvements in various generation tasks such as text-to-image,
class-conditional and unconditional image generation. We also show that ERG can
be seamlessly combined with other recent guidance methods such as CADS and APG,
further boosting generation performance.

</details>

### [276] [Scaling LLaNA: Advancing NeRF-Language Understanding Through Large-Scale Training](https://arxiv.org/abs/2504.13995)
*Andrea Amaduzzi,Pierluigi Zama Ramirez,Giuseppe Lisanti,Samuele Salti,Luigi Di Stefano*

Main category: cs.CV

TLDR: LLaNA是首个能直接处理NeRF权重并执行NeRF标注和问答任务的多模态大语言模型（MLLM），无需渲染图像或生成3D数据。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在理解图像和3D数据时存在局限性，无法全面表示物体几何和外观。NeRF作为一种替代方案，能编码几何和逼真属性。

Method: 通过直接处理NeRF的MLP权重，LLaNA实现了NeRF标注和问答任务，并构建了首个大规模NeRF-语言数据集（300K+ NeRFs）。

Result: 直接处理NeRF权重在NeRF-语言任务中表现优于依赖2D或3D表示的方法。

Conclusion: LLaNA展示了直接处理NeRF权重的可行性，为NeRF与语言模型的结合提供了新方向。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have shown
remarkable capabilities in understanding both images and 3D data, yet these
modalities face inherent limitations in comprehensively representing object
geometry and appearance. Neural Radiance Fields (NeRFs) have emerged as a
promising alternative, encoding both geometric and photorealistic properties
within the weights of a simple Multi-Layer Perceptron (MLP). This work
investigates the feasibility and effectiveness of ingesting NeRFs into an MLLM.
We introduce LLaNA, the first MLLM able to perform new tasks such as NeRF
captioning and Q\&A, by directly processing the weights of a NeRF's MLP.
Notably, LLaNA is able to extract information about the represented objects
without the need to render images or materialize 3D data structures. In
addition, we build the first large-scale NeRF-language dataset, composed by
more than 300K NeRFs trained on ShapeNet and Objaverse, with paired textual
annotations that enable various NeRF-language tasks. Based on this dataset, we
develop a benchmark to evaluate the NeRF understanding capability of our
method. Results show that directly processing NeRF weights leads to better
performance on NeRF-Language tasks compared to approaches that rely on either
2D or 3D representations derived from NeRFs.

</details>

### [277] [Fashion-RAG: Multimodal Fashion Image Editing via Retrieval-Augmented Generation](https://arxiv.org/abs/2504.14011)
*Fulvio Sanguigni,Davide Morelli,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TLDR: 本文提出了一种名为Fashion-RAG的新方法，通过文本输入实现时尚物品的个性化定制，结合检索和生成技术，显著提升了虚拟试穿和时尚图像编辑的效果。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法通常需要具体的服装输入，而实际场景中用户可能仅提供文本描述，因此需要一种更灵活的方法。

Method: Fashion-RAG通过检索与文本描述匹配的服装，并利用文本反转技术将其嵌入到Stable Diffusion模型中，生成个性化图像。

Result: 在Dress Code数据集上的实验表明，Fashion-RAG在质量和多样性上均优于现有方法。

Conclusion: Fashion-RAG是首个针对多模态时尚图像编辑的检索增强生成方法，有效解决了文本输入的局限性。

Abstract: In recent years, the fashion industry has increasingly adopted AI
technologies to enhance customer experience, driven by the proliferation of
e-commerce platforms and virtual applications. Among the various tasks, virtual
try-on and multimodal fashion image editing -- which utilizes diverse input
modalities such as text, garment sketches, and body poses -- have become a key
area of research. Diffusion models have emerged as a leading approach for such
generative tasks, offering superior image quality and diversity. However, most
existing virtual try-on methods rely on having a specific garment input, which
is often impractical in real-world scenarios where users may only provide
textual specifications. To address this limitation, in this work we introduce
Fashion Retrieval-Augmented Generation (Fashion-RAG), a novel method that
enables the customization of fashion items based on user preferences provided
in textual form. Our approach retrieves multiple garments that match the input
specifications and generates a personalized image by incorporating attributes
from the retrieved items. To achieve this, we employ textual inversion
techniques, where retrieved garment images are projected into the textual
embedding space of the Stable Diffusion text encoder, allowing seamless
integration of retrieved elements into the generative process. Experimental
results on the Dress Code dataset demonstrate that Fashion-RAG outperforms
existing methods both qualitatively and quantitatively, effectively capturing
fine-grained visual details from retrieved garments. To the best of our
knowledge, this is the first work to introduce a retrieval-augmented generation
approach specifically tailored for multimodal fashion image editing.

</details>

### [278] [LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models](https://arxiv.org/abs/2504.14032)
*Haiwen Huang,Anpei Chen,Volodymyr Havrylov,Andreas Geiger,Dan Zhang*

Main category: cs.CV

TLDR: 本文提出了一种基于坐标交叉注意力变换器的特征上采样方法，通过结合高分辨率图像和低分辨率VFM特征生成高质量特征，显著提升了像素级理解任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉基础模型（如DINOv2和CLIP）在像素级理解任务中表现受限，特征分辨率不足是主要挑战。

Method: 采用坐标交叉注意力变换器架构，结合高分辨率图像和低分辨率VFM特征；提出利用类无关掩码和自蒸馏构建高分辨率伪真实特征作为训练目标。

Result: 实验表明，该方法在多种下游任务中显著优于现有特征上采样技术。

Conclusion: 该方法有效捕捉细粒度细节，灵活适应不同输入和特征分辨率，为像素级理解任务提供了高效解决方案。

Abstract: Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved
impressive results on various downstream tasks, but their limited feature
resolution hampers performance in applications requiring pixel-level
understanding. Feature upsampling offers a promising direction to address this
challenge. In this work, we identify two critical factors for enhancing feature
upsampling: the upsampler architecture and the training objective. For the
upsampler architecture, we introduce a coordinate-based cross-attention
transformer that integrates the high-resolution images with coordinates and
low-resolution VFM features to generate sharp, high-quality features. For the
training objective, we propose constructing high-resolution pseudo-groundtruth
features by leveraging class-agnostic masks and self-distillation. Our approach
effectively captures fine-grained details and adapts flexibly to various input
and feature resolutions. Through experiments, we demonstrate that our approach
significantly outperforms existing feature upsampling techniques across various
downstream tasks. Our code is released at https://github.com/andrehuang/loftup.

</details>

### [279] [Occlusion-Ordered Semantic Instance Segmentation](https://arxiv.org/abs/2504.14054)
*Soroosh Baselizadeh,Cheuk-To Yu,Olga Veksler,Yuri Boykov*

Main category: cs.CV

TLDR: 论文提出了一种结合相对深度排序和实例分割的任务OOSIS，利用遮挡信息而非绝对深度，简化了3D分析。


<details>
  <summary>Details</summary>
Motivation: 传统的2D实例分割缺乏3D信息，而单目深度估计困难。因此，作者提出利用遮挡信息实现相对深度排序，提供更可靠的3D信息。

Method: 提出OOSIS任务，结合实例分割和遮挡排序，通过定向遮挡边界和语义分割同时提取实例及其遮挡顺序。

Result: 在KINS和COCOA数据集上表现优于基线方法，定向遮挡边界方法显著优于先前工作。

Conclusion: OOSIS提供了一种简单有效的3D分析方法，通过遮挡信息实现了更可靠的相对深度排序。

Abstract: Standard semantic instance segmentation provides useful, but inherently 2D
information from a single image. To enable 3D analysis, one usually integrates
absolute monocular depth estimation with instance segmentation. However,
monocular depth is a difficult task. Instead, we leverage a simpler
single-image task, occlusion-based relative depth ordering, providing coarser
but useful 3D information. We show that relative depth ordering works more
reliably from occlusions than from absolute depth. We propose to solve the
joint task of relative depth ordering and segmentation of instances based on
occlusions. We call this task Occlusion-Ordered Semantic Instance Segmentation
(OOSIS). We develop an approach to OOSIS that extracts instances and their
occlusion order simultaneously from oriented occlusion boundaries and semantic
segmentation. Unlike popular detect-and-segment framework for instance
segmentation, combining occlusion ordering with instance segmentation allows a
simple and clean formulation of OOSIS as a labeling problem. As a part of our
solution for OOSIS, we develop a novel oriented occlusion boundaries approach
that significantly outperforms prior work. We also develop a new joint OOSIS
metric based both on instance mask accuracy and correctness of their occlusion
order. We achieve better performance than strong baselines on KINS and COCOA
datasets.

</details>

### [280] [Towards Scale-Aware Low-Light Enhancement via Structure-Guided Transformer Design](https://arxiv.org/abs/2504.14075)
*Wei Dong,Yan Min,Han Zhou,Jun Chen*

Main category: cs.CV

TLDR: SG-LLIE是一种基于结构先验的多尺度CNN-Transformer混合框架，用于低光图像增强，通过提取光照不变边缘检测器的结构先验，结合CNN和Transformer模块，显著提升了极端低光环境下的增强效果。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法依赖直接映射或语义/光照图引导，但在极端低光环境下效果有限，因图像损坏严重且问题本身不适定。

Method: 提出SG-LLIE框架，利用光照不变边缘检测器提取结构先验，设计CNN-Transformer混合模块（HSGFE）和结构引导Transformer块（SGTB），嵌入UNet编码器-解码器架构。

Result: 在多个低光增强基准测试中取得最优性能，定量指标和视觉质量均表现优异，并在NTIRE 2025挑战赛中排名第二。

Conclusion: SG-LLIE通过结合结构先验和混合架构，有效解决了极端低光环境下的图像增强问题，具有实际应用潜力。

Abstract: Current Low-light Image Enhancement (LLIE) techniques predominantly rely on
either direct Low-Light (LL) to Normal-Light (NL) mappings or guidance from
semantic features or illumination maps. Nonetheless, the intrinsic
ill-posedness of LLIE and the difficulty in retrieving robust semantics from
heavily corrupted images hinder their effectiveness in extremely low-light
environments. To tackle this challenge, we present SG-LLIE, a new multi-scale
CNN-Transformer hybrid framework guided by structure priors. Different from
employing pre-trained models for the extraction of semantics or illumination
maps, we choose to extract robust structure priors based on
illumination-invariant edge detectors. Moreover, we develop a CNN-Transformer
Hybrid Structure-Guided Feature Extractor (HSGFE) module at each scale with in
the UNet encoder-decoder architecture. Besides the CNN blocks which excels in
multi-scale feature extraction and fusion, we introduce a Structure-Guided
Transformer Block (SGTB) in each HSGFE that incorporates structural priors to
modulate the enhancement process. Extensive experiments show that our method
achieves state-of-the-art performance on several LLIE benchmarks in both
quantitative metrics and visual quality. Our solution ranks second in the NTIRE
2025 Low-Light Enhancement Challenge. Code is released at
https://github.com/minyan8/imagine.

</details>

### [281] [Retinex-guided Histogram Transformer for Mask-free Shadow Removal](https://arxiv.org/abs/2504.14092)
*Wei Dong,Han Zhou,Seyed Amirreza Mousavi,Jun Chen*

Main category: cs.CV

TLDR: 提出ReHiT框架，基于CNN-Transformer混合架构，无需阴影掩码即可高效去除阴影。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖难以获取的阴影掩码，限制了在真实场景中的泛化能力。

Method: 采用双分支管道分别建模反射和光照分量，结合CNN-Transformer模块及光照引导直方图Transformer块处理非均匀光照和复杂阴影。

Result: 在多个基准数据集上验证了方法的有效性，参数少、推理速度快，适用于计算资源有限的实际应用。

Conclusion: ReHiT在无需阴影掩码的情况下，实现了高效的阴影去除，具有实际应用潜力。

Abstract: While deep learning methods have achieved notable progress in shadow removal,
many existing approaches rely on shadow masks that are difficult to obtain,
limiting their generalization to real-world scenes. In this work, we propose
ReHiT, an efficient mask-free shadow removal framework based on a hybrid
CNN-Transformer architecture guided by Retinex theory. We first introduce a
dual-branch pipeline to separately model reflectance and illumination
components, and each is restored by our developed Illumination-Guided Hybrid
CNN-Transformer (IG-HCT) module. Second, besides the CNN-based blocks that are
capable of learning residual dense features and performing multi-scale semantic
fusion, multi-scale semantic fusion, we develop the Illumination-Guided
Histogram Transformer Block (IGHB) to effectively handle non-uniform
illumination and spatially complex shadows. Extensive experiments on several
benchmark datasets validate the effectiveness of our approach over existing
mask-free methods. Trained solely on the NTIRE 2025 Shadow Removal Challenge
dataset, our solution delivers competitive results with one of the smallest
parameter sizes and fastest inference speeds among top-ranked entries,
highlighting its applicability for real-world applications with limited
computational resources. The code is available at
https://github.com/dongw22/oath.

</details>

### [282] [VideoPASTA: 7K Preference Pairs That Matter for Video-LLM Alignment](https://arxiv.org/abs/2504.14096)
*Yogesh Kulkarni,Pooyan Fazli*

Main category: cs.CV

TLDR: VideoPASTA通过偏好优化增强视频语言模型的空间、时间和跨帧关系理解，仅需少量数据即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型在空间关系、时间顺序和跨帧连续性方面表现不足。

Method: 引入VideoPASTA框架，通过对抗样本训练模型区分准确与错误的视频表示，并应用直接偏好优化。

Result: 在多个基准测试中性能显著提升，最高达3.05%。

Conclusion: VideoPASTA提供了一种高效、无需人工标注的解决方案，可无缝集成现有模型。

Abstract: Video-language models (Video-LLMs) excel at understanding video content but
struggle with spatial relationships, temporal ordering, and cross-frame
continuity. To address these limitations, we introduce VideoPASTA (Preference
Alignment with Spatio-Temporal-Cross Frame Adversaries), a framework that
enhances Video-LLMs through targeted preference optimization. VideoPASTA trains
models to distinguish accurate video representations from carefully generated
adversarial examples that deliberately violate spatial, temporal, or
cross-frame relations. By applying Direct Preference Optimization to just 7,020
preference pairs, VideoPASTA learns robust representations that capture
fine-grained spatial relationships and long-range temporal dynamics.
Experiments on standard video benchmarks show significant relative performance
gains of 3.05% on VideoMME, 1.97% on NeXTQA, and 1.31% on LongVideoBench, over
the baseline Qwen2.5-VL model. These results demonstrate that targeted
alignment, rather than massive pretraining or architectural modifications,
effectively addresses core video-language challenges. Notably, VideoPASTA
achieves these improvements without human annotation or captioning, relying on
just 32-frame sampling, compared to the 96-frame, multi-GPU setups of prior
work. This efficiency makes our approach a scalable, plug-and-play solution
that seamlessly integrates with existing models while preserving their
capabilities.

</details>

### [283] [Point-Driven Interactive Text and Image Layer Editing Using Diffusion Models](https://arxiv.org/abs/2504.14108)
*Zhenyu Yu,Mohd Yamani Idna Idris,Pei Wang,Yuelong Xia*

Main category: cs.CV

TLDR: DanceText是一种无需训练的多语言图像文本编辑框架，支持复杂几何变换并实现无缝的前景-背景融合。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的生成模型在文本引导图像合成中缺乏可控性，难以保持布局一致性，尤其是在非平凡操作（如旋转、平移、缩放和扭曲）下。

Method: DanceText采用分层编辑策略，将文本与背景分离，以模块化和可控的方式执行几何变换，并引入深度感知模块以增强真实感和空间一致性。

Result: 在AnyWord-3M基准测试中，DanceText在视觉质量上表现优异，尤其是在大规模和复杂变换场景下。

Conclusion: DanceText通过无需训练的设计和预训练模块的集成，实现了灵活部署和高性能的文本编辑。

Abstract: We present DanceText, a training-free framework for multilingual text editing
in images, designed to support complex geometric transformations and achieve
seamless foreground-background integration. While diffusion-based generative
models have shown promise in text-guided image synthesis, they often lack
controllability and fail to preserve layout consistency under non-trivial
manipulations such as rotation, translation, scaling, and warping. To address
these limitations, DanceText introduces a layered editing strategy that
separates text from the background, allowing geometric transformations to be
performed in a modular and controllable manner. A depth-aware module is further
proposed to align appearance and perspective between the transformed text and
the reconstructed background, enhancing photorealism and spatial consistency.
Importantly, DanceText adopts a fully training-free design by integrating
pretrained modules, allowing flexible deployment without task-specific
fine-tuning. Extensive experiments on the AnyWord-3M benchmark demonstrate that
our method achieves superior performance in visual quality, especially under
large-scale and complex transformation scenarios.

</details>

### [284] [Lightweight Road Environment Segmentation using Vector Quantization](https://arxiv.org/abs/2504.14113)
*Jiyong Kwag,Alper Yilmaz,Charles Toth*

Main category: cs.CV

TLDR: 论文提出了一种基于矢量量化的道路环境分割方法，结合MobileUNETR模型，显著提升了语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于FCN和Transformer的编码器依赖连续特征表示，限制了离散信息的表达能力。矢量量化能有效解决这一问题。

Method: 将编码器的连续特征映射到离散矢量，结合MobileUNETR模型进行分割。

Result: 在Cityscapes数据集上达到77.0% mIoU，比基线模型提升2.9%。

Conclusion: 矢量量化能有效提升道路环境分割的性能，且不增加模型复杂度。

Abstract: Road environment segmentation plays a significant role in autonomous driving.
Numerous works based on Fully Convolutional Networks (FCNs) and Transformer
architectures have been proposed to leverage local and global contextual
learning for efficient and accurate semantic segmentation. In both
architectures, the encoder often relies heavily on extracting continuous
representations from the image, which limits the ability to represent
meaningful discrete information. To address this limitation, we propose
segmentation of the autonomous driving environment using vector quantization.
Vector quantization offers three primary advantages for road environment
segmentation. (1) Each continuous feature from the encoder is mapped to a
discrete vector from the codebook, helping the model discover distinct features
more easily than with complex continuous features. (2) Since a discrete feature
acts as compressed versions of the encoder's continuous features, they also
compress noise or outliers, enhancing the image segmentation task. (3) Vector
quantization encourages the latent space to form coarse clusters of continuous
features, forcing the model to group similar features, making the learned
representations more structured for the decoding process. In this work, we
combined vector quantization with the lightweight image segmentation model
MobileUNETR and used it as a baseline model for comparison to demonstrate its
efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes,
outperforming the baseline by 2.9 % without increasing the model's initial size
or complexity.

</details>

### [285] [BMRL: Bi-Modal Guided Multi-Perspective Representation Learning for Zero-Shot Deepfake Attribution](https://arxiv.org/abs/2504.14129)
*Yaning Zhang,Jiahe Zhang,Chunjie Ma,Weili Guan,Tian Gan,Zan Gao*

Main category: cs.CV

TLDR: 本文提出了一种新型的双模态引导多视角表示学习框架（BMRL），用于零样本深度伪造溯源（ZS-DFA），通过结合视觉、解析和语言模态，显著提升了溯源性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造溯源方法主要关注视觉模态，忽略了文本和面部解析等其他模态，且难以对未见过的生成器进行细粒度评估。

Method: 设计了多视角视觉编码器（MPVE）、解析编码器和语言编码器，结合视觉-解析匹配和视觉-语言对齐，并提出了深度伪造溯源对比中心（DFACC）损失函数。

Result: 实验结果表明，该方法在ZS-DFA任务中优于现有技术。

Conclusion: BMRL框架通过多模态融合和对比学习，显著提升了深度伪造溯源的泛化能力和准确性。

Abstract: The challenge of tracing the source attribution of forged faces has gained
significant attention due to the rapid advancement of generative models.
However, existing deepfake attribution (DFA) works primarily focus on the
interaction among various domains in vision modality, and other modalities such
as texts and face parsing are not fully explored. Besides, they tend to fail to
assess the generalization performance of deepfake attributors to unseen
generators in a fine-grained manner. In this paper, we propose a novel bi-modal
guided multi-perspective representation learning (BMRL) framework for zero-shot
deepfake attribution (ZS-DFA), which facilitates effective traceability to
unseen generators. Specifically, we design a multi-perspective visual encoder
(MPVE) to explore general deepfake attribution visual characteristics across
three views (i.e., image, noise, and edge). We devise a novel parsing encoder
to focus on global face attribute embeddings, enabling parsing-guided DFA
representation learning via vision-parsing matching. A language encoder is
proposed to capture fine-grained language embeddings, facilitating
language-guided general visual forgery representation learning through
vision-language alignment. Additionally, we present a novel deepfake
attribution contrastive center (DFACC) loss, to pull relevant generators closer
and push irrelevant ones away, which can be introduced into DFA models to
enhance traceability. Experimental results demonstrate that our method
outperforms the state-of-the-art on the ZS-DFA task through various protocols
evaluation.

</details>

### [286] [Transforming hyperspectral images into chemical maps: A new deep learning based approach to hyperspectral image processing](https://arxiv.org/abs/2504.14131)
*Ole-Christian Galbo Engstrøm,Michela Albano-Gaglio,Erik Schou Dreier,Yamine Bouzembrak,Maria Font-i-Furnols,Puneet Mishra,Kim Steenstrup Pedersen*

Main category: cs.CV

TLDR: 本研究提出了一种基于改进U-Net和自定义损失函数的端到端深度学习方法，用于直接从高光谱图像生成化学图，优于传统的PLS回归方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如PLS回归）在生成化学图时未考虑空间上下文且噪声较高，因此需要一种更优的方法。

Method: 使用改进的U-Net和自定义损失函数进行端到端训练，直接生成化学图。

Result: U-Net在平均脂肪预测任务上的均方根误差比PLS低9%-13%，且生成的化学图99.91%的方差具有空间相关性。

Conclusion: U-Net在化学图生成任务上优于PLS回归，能够生成更精确且空间相关的化学图。

Abstract: Current approaches to chemical map generation from hyperspectral images are
based on models such as partial least squares (PLS) regression, generating
pixel-wise predictions that do not consider spatial context and suffer from a
high degree of noise. This study proposes an end-to-end deep learning approach
using a modified version of U-Net and a custom loss function to directly obtain
chemical maps from hyperspectral images, skipping all intermediate steps
required for traditional pixel-wise analysis. We compare the U-Net with the
traditional PLS regression on a real dataset of pork belly samples with
associated mean fat reference values. The U-Net obtains a test set root mean
squared error of between 9% and 13% lower than that of PLS regression on the
task of mean fat prediction. At the same time, U-Net generates fine detail
chemical maps where 99.91% of the variance is spatially correlated. Conversely,
only 2.53% of the variance in the PLS-generated chemical maps is spatially
correlated, indicating that each pixel-wise prediction is largely independent
of neighboring pixels. Additionally, while the PLS-generated chemical maps
contain predictions far beyond the physically possible range of 0-100%, U-Net
learns to stay inside this range. Thus, the findings of this study indicate
that U-Net is superior to PLS for chemical map generation.

</details>

### [287] [HFBRI-MAE: Handcrafted Feature Based Rotation-Invariant Masked Autoencoder for 3D Point Cloud Analysis](https://arxiv.org/abs/2504.14132)
*Xuanhua Yin,Dingxin Zhang,Jianhui Yu,Weidong Cai*

Main category: cs.CV

TLDR: HFBRI-MAE是一种新型自监督学习框架，通过结合旋转不变的手工特征改进了MAE设计，解决了现有方法在旋转点云处理中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于MAE的自监督学习方法缺乏旋转不变性，导致处理任意旋转点云时性能显著下降。

Method: HFBRI-MAE利用旋转不变的局部和全局特征进行令牌嵌入和位置嵌入，并重新定义重建目标为规范对齐版本。

Result: 在ModelNet40、ScanObjectNN和ShapeNetPart上的实验表明，HFBRI-MAE在分类、分割和少样本学习中优于现有方法。

Conclusion: HFBRI-MAE具有鲁棒性和强泛化能力，适用于真实世界的3D应用。

Abstract: Self-supervised learning (SSL) has demonstrated remarkable success in 3D
point cloud analysis, particularly through masked autoencoders (MAEs). However,
existing MAE-based methods lack rotation invariance, leading to significant
performance degradation when processing arbitrarily rotated point clouds in
real-world scenarios. To address this limitation, we introduce Handcrafted
Feature-Based Rotation-Invariant Masked Autoencoder (HFBRI-MAE), a novel
framework that refines the MAE design with rotation-invariant handcrafted
features to ensure stable feature learning across different orientations. By
leveraging both rotation-invariant local and global features for token
embedding and position embedding, HFBRI-MAE effectively eliminates rotational
dependencies while preserving rich geometric structures. Additionally, we
redefine the reconstruction target to a canonically aligned version of the
input, mitigating rotational ambiguities. Extensive experiments on ModelNet40,
ScanObjectNN, and ShapeNetPart demonstrate that HFBRI-MAE consistently
outperforms existing methods in object classification, segmentation, and
few-shot learning, highlighting its robustness and strong generalization
ability in real-world 3D applications.

</details>

### [288] [Rethinking Target Label Conditioning in Adversarial Attacks: A 2D Tensor-Guided Generative Approach](https://arxiv.org/abs/2504.14137)
*Hangyu Liu,Bo Peng,Pengxiang Ding,Donglin Wang*

Main category: cs.CV

TLDR: 论文提出了一种基于扩散模型的2D-TGAF框架，用于多目标对抗攻击，通过语义特征的质量和数量提升攻击的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法对多目标攻击的理论分析缺乏实践验证和总结，作者发现语义特征的质量和数量是影响攻击可迁移性的关键因素。

Method: 提出2D-TGAF框架，利用扩散模型生成二维语义张量指导噪声生成，并设计掩码策略确保部分噪声保留完整目标类语义信息。

Result: 在ImageNet数据集上的实验表明，2D-TGAF在攻击成功率和防御机制下的表现均优于现有方法。

Conclusion: 2D-TGAF通过优化语义特征的生成和利用，显著提升了多目标对抗攻击的效果。

Abstract: Compared to single-target adversarial attacks, multi-target attacks have
garnered significant attention due to their ability to generate adversarial
images for multiple target classes simultaneously. Existing generative
approaches for multi-target attacks mainly analyze the effect of the use of
target labels on noise generation from a theoretical perspective, lacking
practical validation and comprehensive summarization. To address this gap, we
first identify and validate that the semantic feature quality and quantity are
critical factors affecting the transferability of targeted attacks: 1) Feature
quality refers to the structural and detailed completeness of the implanted
target features, as deficiencies may result in the loss of key discriminative
information; 2) Feature quantity refers to the spatial sufficiency of the
implanted target features, as inadequacy limits the victim model's attention to
this feature. Based on these findings, we propose the 2D Tensor-Guided
Adversarial Fusion (2D-TGAF) framework, which leverages the powerful generative
capabilities of diffusion models to encode target labels into two-dimensional
semantic tensors for guiding adversarial noise generation. Additionally, we
design a novel masking strategy tailored for the training process, ensuring
that parts of the generated noise retain complete semantic information about
the target class. Extensive experiments on the standard ImageNet dataset
demonstrate that 2D-TGAF consistently surpasses state-of-the-art methods in
attack success rates, both on normally trained models and across various
defense mechanisms.

</details>

### [289] [Segment Any Crack: Deep Semantic Segmentation Adaptation for Crack Detection](https://arxiv.org/abs/2504.14138)
*Ghodsiyeh Rostami,Po-Han Chen,Mahdi S. Hosseini*

Main category: cs.CV

TLDR: 提出了一种选择性微调策略，专注于调整归一化组件，以提高分割模型在裂缝检测中的适应性。该方法在性能和计算效率上优于全微调和其他常见技术。


<details>
  <summary>Details</summary>
Motivation: 现有裂缝检测模型需要大量标注数据和计算资源，限制了其适应性。

Method: 选择性微调归一化参数，应用于Segment Anything Model（SAM）和五种分割模型。

Result: 在OmniCrack30k数据集上，SAC模型达到61.22% F1-score和44.13% IoU，并在零样本数据集上表现最佳。

Conclusion: 该方法显著提高了分割精度并降低了计算开销。

Abstract: Image-based crack detection algorithms are increasingly in demand in
infrastructure monitoring, as early detection of cracks is of paramount
importance for timely maintenance planning. While deep learning has
significantly advanced crack detection algorithms, existing models often
require extensive labeled datasets and high computational costs for
fine-tuning, limiting their adaptability across diverse conditions. This study
introduces an efficient selective fine-tuning strategy, focusing on tuning
normalization components, to enhance the adaptability of segmentation models
for crack detection. The proposed method is applied to the Segment Anything
Model (SAM) and five well-established segmentation models. Experimental results
demonstrate that selective fine-tuning of only normalization parameters
outperforms full fine-tuning and other common fine-tuning techniques in both
performance and computational efficiency, while improving generalization. The
proposed approach yields a SAM-based model, Segment Any Crack (SAC), achieving
a 61.22\% F1-score and 44.13\% IoU on the OmniCrack30k benchmark dataset, along
with the highest performance across three zero-shot datasets and the lowest
standard deviation. The results highlight the effectiveness of the adaptation
approach in improving segmentation accuracy while significantly reducing
computational overhead.

</details>

### [290] [ThyroidEffi 1.0: A Cost-Effective System for High-Performance Multi-Class Thyroid Carcinoma Classification](https://arxiv.org/abs/2504.14139)
*Hai Pham-Ngoc,De Nguyen-Van,Dung Vu-Tien,Phuong Le-Hong*

Main category: cs.CV

TLDR: 开发了一种高效、可解释的深度学习系统，用于甲状腺细针穿刺活检图像的多分类，具有高准确性和低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决甲状腺FNAB图像分类中的数据不足、观察者间差异和计算成本高的问题，为临床决策提供支持。

Method: 结合YOLOv10细胞簇检测、课程学习协议、轻量级EfficientNetB0和Transformer模块，进行多尺度分析。

Result: 内部测试集宏F1为89.19%，外部验证AUCs分别为0.9495 (B2)、0.7436 (B5)和0.8396 (B6)。

Conclusion: 高准确性、可解释的甲状腺FNAB分类在低计算需求下可行。

Abstract: Background: Automated classification of thyroid fine needle aspiration biopsy
(FNAB) images faces challenges in limited data, inter-observer variability, and
computational cost. Efficient, interpretable models are crucial for clinical
support. Objective: To develop and externally validate a deep learning system
for the multi-class classification of thyroid FNAB images into three key
categories that directly guide post-biopsy treatment decisions in Vietnam:
benign (B2), suspicious for malignancy (B5), and malignant (B6), while
achieving high diagnostic accuracy with low computational overhead. Methods:
Our framework features: (1) YOLOv10-based cell cluster detection for
informative sub-region extraction and noise reduction; (2) a curriculum
learning-inspired protocol sequencing localized crops to full images for
multi-scale feature capture; (3) adaptive lightweight EfficientNetB0 (4
millions parameters) selection balancing performance and efficiency; and (4) a
Transformer-inspired module for multi-scale, multi-region analysis. External
validation used 1,015 independent FNAB images. Results: ThyroidEffi Basic
achieved a macro F1 of 89.19\% and AUCs of 0.98 (B2), 0.95 (B5), and 0.96 (B6)
on the internal test set. External validation yielded AUCs of 0.9495 (B2),
0.7436 (B5), and 0.8396 (B6). ThyroidEffi Premium improved macro F1 to 89.77\%.
Grad-CAM highlighted key diagnostic regions, confirming interpretability. The
system processed 1000 cases in 30 seconds, demonstrating feasibility on widely
accessible hardware like a 12-core CPU. Conclusions: This work demonstrates
that high-accuracy, interpretable thyroid FNAB image classification is
achievable with minimal computational demands.

</details>

### [291] [Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D](https://arxiv.org/abs/2504.14151)
*Sergio Arnaud,Paul McVay,Ada Martin,Arjun Majumdar,Krishna Murthy Jatavallabhula,Phillip Thomas,Ruslan Partsey,Daniel Dugas,Abha Gejji,Alexander Sax,Vincent-Pierre Berges,Mikael Henaff,Ayush Jain,Ang Cao,Ishita Prasad,Mrinal Kalakrishnan,Michael Rabbat,Nicolas Ballas,Mido Assran,Oleksandr Maksymets,Aravind Rajeswaran,Franziska Meier*

Main category: cs.CV

TLDR: LOCATE 3D是一个通过语言描述在3D场景中定位物体的模型，结合了3D-JEPA自监督学习算法，并在新数据集上实现了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D场景中基于语言描述的物体定位问题，并提升模型的泛化能力。

Method: 使用3D-JEPA自监督学习算法，结合2D基础模型（如CLIP、DINO）对3D点云进行特征提取，并通过掩码预测任务学习上下文特征。训练后的编码器与语言条件解码器联合预测3D掩码和边界框。

Result: 在标准基准测试中达到最新最优性能，并展示了强大的泛化能力。

Conclusion: LOCATE 3D在3D物体定位任务中表现出色，适用于机器人及AR设备的实际部署。

Abstract: We present LOCATE 3D, a model for localizing objects in 3D scenes from
referring expressions like "the small coffee table between the sofa and the
lamp." LOCATE 3D sets a new state-of-the-art on standard referential grounding
benchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D
operates directly on sensor observation streams (posed RGB-D frames), enabling
real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA,
a novel self-supervised learning (SSL) algorithm applicable to sensor point
clouds. It takes as input a 3D pointcloud featurized using 2D foundation models
(CLIP, DINO). Subsequently, masked prediction in latent space is employed as a
pretext task to aid the self-supervised learning of contextualized pointcloud
features. Once trained, the 3D-JEPA encoder is finetuned alongside a
language-conditioned decoder to jointly predict 3D masks and bounding boxes.
Additionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential
grounding, spanning multiple capture setups with over 130K annotations. This
enables a systematic study of generalization capabilities as well as a stronger
model.

</details>

### [292] [Segregation and Context Aggregation Network for Real-time Cloud Segmentation](https://arxiv.org/abs/2504.14178)
*Yijie Li,Hewei Wang,Jiayi Zhang,Jinjiang You,Jinfeng Xu,Puzhen Wu,Yunzhong Xiao,Soumyabrata Dev*

Main category: cs.CV

TLDR: SCANet是一种轻量级云分割模型，通过Segregation and Context Aggregation Module (SCAM)提升分割精度和计算效率，适用于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分割精度和计算效率之间难以平衡，限制了在边缘设备上的实际应用。

Method: 提出SCANet模型，采用SCAM模块对粗略分割图进行细化，分别处理天空和云特征。

Result: SCANet-large参数减少70.9%，性能媲美现有方法；SCANet-lite速度达1390 fps。

Conclusion: SCANet在保持高性能的同时显著降低计算复杂度，适用于实时应用。

Abstract: Cloud segmentation from intensity images is a pivotal task in atmospheric
science and computer vision, aiding weather forecasting and climate analysis.
Ground-based sky/cloud segmentation extracts clouds from images for further
feature analysis. Existing methods struggle to balance segmentation accuracy
and computational efficiency, limiting real-world deployment on edge devices,
so we introduce SCANet, a novel lightweight cloud segmentation model featuring
Segregation and Context Aggregation Module (SCAM), which refines rough
segmentation maps into weighted sky and cloud features processed separately.
SCANet achieves state-of-the-art performance while drastically reducing
computational complexity. SCANet-large (4.29M) achieves comparable accuracy to
state-of-the-art methods with 70.9% fewer parameters. Meanwhile, SCANet-lite
(90K) delivers 1390 fps in FP16, surpassing real-time standards. Additionally,
we propose an efficient pre-training strategy that enhances performance even
without ImageNet pre-training.

</details>

### [293] [Enhancing Multimodal In-Context Learning for Image Classification through Coreset Optimization](https://arxiv.org/abs/2504.14200)
*Huiyi Chen,Jiawei Peng,Kaihua Tang,Xin Geng,Xu Yang*

Main category: cs.CV

TLDR: KeCO提出了一种基于视觉特征的紧凑核心集优化框架，显著提升了图像分类任务的上下文学习性能，平均改进超过20%。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图像分类任务中选择核心集时存在高计算成本和信息损失问题，且文本分类方法可能不适用于图像特征空间。

Method: KeCO利用未利用数据构建紧凑核心集，通过视觉特征作为锚点，采用不同选择策略更新核心集样本。

Result: 在粗粒度和细粒度图像分类基准测试中，KeCO平均性能提升超过20%，并在模拟在线场景中表现优异。

Conclusion: KeCO在低计算成本下显著提升图像分类性能，适用于资源受限的实际场景。

Abstract: In-context learning (ICL) enables Large Vision-Language Models (LVLMs) to
adapt to new tasks without parameter updates, using a few demonstrations from a
large support set. However, selecting informative demonstrations leads to high
computational and memory costs. While some methods explore selecting a small
and representative coreset in the text classification, evaluating all support
set samples remains costly, and discarded samples lead to unnecessary
information loss. These methods may also be less effective for image
classification due to differences in feature spaces. Given these limitations,
we propose Key-based Coreset Optimization (KeCO), a novel framework that
leverages untapped data to construct a compact and informative coreset. We
introduce visual features as keys within the coreset, which serve as the anchor
for identifying samples to be updated through different selection strategies.
By leveraging untapped samples from the support set, we update the keys of
selected coreset samples, enabling the randomly initialized coreset to evolve
into a more informative coreset under low computational cost. Through extensive
experiments on coarse-grained and fine-grained image classification benchmarks,
we demonstrate that KeCO effectively enhances ICL performance for image
classification task, achieving an average improvement of more than 20\%.
Notably, we evaluate KeCO under a simulated online scenario, and the strong
performance in this scenario highlights the practical value of our framework
for resource-constrained real-world scenarios.

</details>

### [294] [Learning Joint ID-Textual Representation for ID-Preserving Image Synthesis](https://arxiv.org/abs/2504.14202)
*Zichuan Liu,Liming Jiang,Qing Yan,Yumin Jia,Hao Kang,Xin Lu*

Main category: cs.CV

TLDR: 提出了一种基于多模态编码策略的ID保留生成框架FaceCLIP，通过联合嵌入空间统一处理身份和文本输入，结合扩散模型生成身份一致且文本对齐的图像。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过适配器注入身份特征，限制了生成效果。本文旨在通过多模态编码策略更自然地统一身份和文本输入。

Method: 引入FaceCLIP多模态编码器，学习身份和文本的联合嵌入空间，结合扩散模型生成图像。提出多模态对齐算法训练FaceCLIP。

Result: FaceCLIP-SDXL在身份保留和文本对齐上优于现有方法，实验验证了其定量和定性优势。

Conclusion: FaceCLIP-SDXL实现了更逼真的肖像生成，身份保留和文本相关性更优。

Abstract: We propose a novel framework for ID-preserving generation using a multi-modal
encoding strategy rather than injecting identity features via adapters into
pre-trained models. Our method treats identity and text as a unified
conditioning input. To achieve this, we introduce FaceCLIP, a multi-modal
encoder that learns a joint embedding space for both identity and textual
semantics. Given a reference face and a text prompt, FaceCLIP produces a
unified representation that encodes both identity and text, which conditions a
base diffusion model to generate images that are identity-consistent and
text-aligned. We also present a multi-modal alignment algorithm to train
FaceCLIP, using a loss that aligns its joint representation with face, text,
and image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image
synthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL).
Compared to prior methods, FaceCLIP-SDXL enables photorealistic portrait
generation with better identity preservation and textual relevance. Extensive
experiments demonstrate its quantitative and qualitative superiority.

</details>

### [295] [Real-IAD D3: A Real-World 2D/Pseudo-3D/3D Dataset for Industrial Anomaly Detection](https://arxiv.org/abs/2504.14221)
*Wenbing Zhu,Lidong Wang,Ziqing Zhou,Chengjie Wang,Yurui Pan,Ruoyi Zhang,Zhuhao Chen,Linjie Cheng,Bin-Bin Gao,Jiangning Zhang,Zhenye Gan,Yuxie Wang,Yulong Chen,Shuguang Qian,Mingmin Chi,Bo Peng,Lizhuang Ma*

Main category: cs.CV

TLDR: 论文提出了Real-IAD D3数据集，结合RGB、3D点云和伪3D模态，用于工业异常检测，并提出了多模态融合方法提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测领域缺乏专门的多模态数据集，现有数据集如MVTec 3D在规模和分辨率上存在不足，难以模拟真实工业环境。

Method: 引入Real-IAD D3数据集，包含高分辨率RGB图像、微米级3D点云和通过光度立体生成的伪3D模态，并提出多模态融合方法。

Result: 实验表明多模态信息能显著提升检测鲁棒性和性能。

Conclusion: Real-IAD D3为工业异常检测提供了更具挑战性的基准，多模态融合方法有效提升了检测效果。

Abstract: The increasing complexity of industrial anomaly detection (IAD) has
positioned multimodal detection methods as a focal area of machine vision
research. However, dedicated multimodal datasets specifically tailored for IAD
remain limited. Pioneering datasets like MVTec 3D have laid essential
groundwork in multimodal IAD by incorporating RGB+3D data, but still face
challenges in bridging the gap with real industrial environments due to
limitations in scale and resolution. To address these challenges, we introduce
Real-IAD D3, a high-precision multimodal dataset that uniquely incorporates an
additional pseudo3D modality generated through photometric stereo, alongside
high-resolution RGB images and micrometer-level 3D point clouds. Real-IAD D3
features finer defects, diverse anomalies, and greater scale across 20
categories, providing a challenging benchmark for multimodal IAD Additionally,
we introduce an effective approach that integrates RGB, point cloud, and
pseudo-3D depth information to leverage the complementary strengths of each
modality, enhancing detection performance. Our experiments highlight the
importance of these modalities in boosting detection robustness and overall IAD
performance. The dataset and code are publicly accessible for research purposes
at https://realiad4ad.github.io/Real-IAD D3

</details>

### [296] [Revisiting CLIP for SF-OSDA: Unleashing Zero-Shot Potential with Adaptive Threshold and Training-Free Feature Filtering](https://arxiv.org/abs/2504.14224)
*Yongguang Li,Jindong Li,Qi Wang,Qianli Xing,Runliang Niu,Shengsheng Wang,Menglin Yang*

Main category: cs.CV

TLDR: CLIPXpert提出了一种新的SF-OSDA方法，通过自适应阈值和未知类特征过滤模块解决了现有方法依赖固定阈值和忽略类倾向的问题。


<details>
  <summary>Details</summary>
Motivation: 现有SF-OSDA方法依赖固定阈值且忽略类倾向，导致CLIP的零样本潜力未充分利用，且特征分离复杂。

Method: CLIPXpert包含BGAT模块（动态确定阈值）和SUFF模块（过滤未知类特征），无需训练且源自由。

Result: 在DomainNet上优于UOTA 1.92%，在Office-Home等数据集上达到SOTA水平。

Conclusion: CLIPXpert验证了CLIP在SF-OSDA任务中的零样本潜力，并显著提升了性能。

Abstract: Source-Free Unsupervised Open-Set Domain Adaptation (SF-OSDA) methods using
CLIP face significant issues: (1) while heavily dependent on domain-specific
threshold selection, existing methods employ simple fixed thresholds,
underutilizing CLIP's zero-shot potential in SF-OSDA scenarios; and (2)
overlook intrinsic class tendencies while employing complex training to enforce
feature separation, incurring deployment costs and feature shifts that
compromise CLIP's generalization ability. To address these issues, we propose
CLIPXpert, a novel SF-OSDA approach that integrates two key components: an
adaptive thresholding strategy and an unknown class feature filtering module.
Specifically, the Box-Cox GMM-Based Adaptive Thresholding (BGAT) module
dynamically determines the optimal threshold by estimating sample score
distributions, balancing known class recognition and unknown class sample
detection. Additionally, the Singular Value Decomposition (SVD)-Based
Unknown-Class Feature Filtering (SUFF) module reduces the tendency of unknown
class samples towards known classes, improving the separation between known and
unknown classes. Experiments show that our source-free and training-free method
outperforms state-of-the-art trained approach UOTA by 1.92% on the DomainNet
dataset, achieves SOTA-comparable performance on datasets such as Office-Home,
and surpasses other SF-OSDA methods. This not only validates the effectiveness
of our proposed method but also highlights CLIP's strong zero-shot potential
for SF-OSDA tasks.

</details>

### [297] [Exploring Modality Guidance to Enhance VFM-based Feature Fusion for UDA in 3D Semantic Segmentation](https://arxiv.org/abs/2504.14231)
*Johannes Spoecklberger,Wei Lin,Pedro Hermosilla,Sivan Doveh,Horst Possegger,M. Jehanzeb Mirza*

Main category: cs.CV

TLDR: 该论文探讨了如何利用视觉基础模型（VFMs）提升LiDAR 3D语义分割任务的表现，通过融合2D-3D数据实现跨域适应。


<details>
  <summary>Details</summary>
Motivation: VFMs在2D视觉任务中表现优异，但其在3D任务中的潜力尚未充分挖掘，尤其是在跨模态（2D-3D）数据场景下。

Method: 提出一种融合网络，结合VFMs的跨域特征，利用标记的源数据和未标记的目标数据训练3D骨干网络。

Result: 在多个实验中，该方法显著优于现有技术，平均提升6.5 mIoU。

Conclusion: VFMs在3D任务中具有显著潜力，通过跨模态融合可有效提升性能。

Abstract: Vision Foundation Models (VFMs) have become a de facto choice for many
downstream vision tasks, like image classification, image segmentation, and
object localization. However, they can also provide significant utility for
downstream 3D tasks that can leverage the cross-modal information (e.g., from
paired image data). In our work, we further explore the utility of VFMs for
adapting from a labeled source to unlabeled target data for the task of
LiDAR-based 3D semantic segmentation. Our method consumes paired 2D-3D (image
and point cloud) data and relies on the robust (cross-domain) features from a
VFM to train a 3D backbone on a mix of labeled source and unlabeled target
data. At the heart of our method lies a fusion network that is guided by both
the image and point cloud streams, with their relative contributions adjusted
based on the target domain. We extensively compare our proposed methodology
with different state-of-the-art methods in several settings and achieve strong
performance gains. For example, achieving an average improvement of 6.5 mIoU
(over all tasks), when compared with the previous state-of-the-art.

</details>

### [298] [Single Document Image Highlight Removal via A Large-Scale Real-World Dataset and A Location-Aware Network](https://arxiv.org/abs/2504.14238)
*Lu Pan,Yu-Hsuan Huang,Hongxia Xie,Cheng Zhang,Hongwei Zhao,Hong-Han Shuai,Wen-Huang Cheng*

Main category: cs.CV

TLDR: 论文提出DocHR14K数据集和L2HRNet方法，用于解决文档图像中的高光问题，显著提升高光去除效果。


<details>
  <summary>Details</summary>
Motivation: 文档图像在环境光下易产生高光，影响文本可读性，现有深度学习方法因缺乏专用数据集和针对性设计而效果不佳。

Method: 提出DocHR14K数据集和HLP先验方法，设计L2HRNet网络，结合扩散模块恢复细节。

Result: DocHR14K提升高光去除效果，L2HRNet在多个数据集上达到最优性能，PSNR提升5.01%，RMSE降低13.17%。

Conclusion: DocHR14K和L2HRNet为文档高光去除提供了有效解决方案，显著优于现有方法。

Abstract: Reflective documents often suffer from specular highlights under ambient
lighting, severely hindering text readability and degrading overall visual
quality. Although recent deep learning methods show promise in highlight
removal, they remain suboptimal for document images, primarily due to the lack
of dedicated datasets and tailored architectural designs. To tackle these
challenges, we present DocHR14K, a large-scale real-world dataset comprising
14,902 high-resolution image pairs across six document categories and various
lighting conditions. To the best of our knowledge, this is the first
high-resolution dataset for document highlight removal that captures a wide
range of real-world lighting conditions. Additionally, motivated by the
observation that the residual map between highlighted and clean images
naturally reveals the spatial structure of highlight regions, we propose a
simple yet effective Highlight Location Prior (HLP) to estimate highlight masks
without human annotations. Building on this prior, we present the
Location-Aware Laplacian Pyramid Highlight Removal Network (L2HRNet), which
effectively removes highlights by leveraging estimated priors and incorporates
diffusion module to restore details. Extensive experiments demonstrate that
DocHR14K improves highlight removal under diverse lighting conditions. Our
L2HRNet achieves state-of-the-art performance across three benchmark datasets,
including a 5.01\% increase in PSNR and a 13.17\% reduction in RMSE on
DocHR14K.

</details>

### [299] [ROI-Guided Point Cloud Geometry Compression Towards Human and Machine Vision](https://arxiv.org/abs/2504.14240)
*Xie Liang,Gao Wei,Zhenghui Ming,Li Ge*

Main category: cs.CV

TLDR: 提出了一种基于ROI引导的点云几何压缩方法（RPCGC），通过双分支并行结构优化压缩性能，同时提升机器视觉任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 点云数据在自动驾驶等领域应用广泛，但大容量存储和传输带来挑战，传统高压缩比方法易损失语义细节，影响下游任务准确性。

Method: 采用双分支并行结构：基础层编码简化点云，增强层通过ROI预测网络优化几何细节，并将掩码信息融入残差；在率失真优化中加权计算失真，结合检测损失指导编码。

Result: 在ScanNet和SUN RGB-D数据集上，RPCGC在高比特率下压缩性能优异，检测准确率比部分学习方法提升10%。

Conclusion: RPCGC通过ROI引导和双分支结构，有效平衡压缩率与语义细节保留，显著提升机器视觉任务的准确性。

Abstract: Point cloud data is pivotal in applications like autonomous driving, virtual
reality, and robotics. However, its substantial volume poses significant
challenges in storage and transmission. In order to obtain a high compression
ratio, crucial semantic details usually confront severe damage, leading to
difficulties in guaranteeing the accuracy of downstream tasks. To tackle this
problem, we are the first to introduce a novel Region of Interest (ROI)-guided
Point Cloud Geometry Compression (RPCGC) method for human and machine vision.
Our framework employs a dual-branch parallel structure, where the base layer
encodes and decodes a simplified version of the point cloud, and the
enhancement layer refines this by focusing on geometry details. Furthermore,
the residual information of the enhancement layer undergoes refinement through
an ROI prediction network. This network generates mask information, which is
then incorporated into the residuals, serving as a strong supervision signal.
Additionally, we intricately apply these mask details in the Rate-Distortion
(RD) optimization process, with each point weighted in the distortion
calculation. Our loss function includes RD loss and detection loss to better
guide point cloud encoding for the machine. Experiment results demonstrate that
RPCGC achieves exceptional compression performance and better detection
accuracy (10% gain) than some learning-based compression methods at high
bitrates in ScanNet and SUN RGB-D datasets.

</details>

### [300] [Towards Explainable Fake Image Detection with Multi-Modal Large Language Models](https://arxiv.org/abs/2504.14245)
*Yikun Ji,Yan Hong,Jiahui Zhan,Haoxing Chen,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TLDR: 论文探讨了利用多模态大语言模型（MLLMs）进行AI生成图像检测的新方法，强调其泛化能力和透明性，并与传统方法和人类评估者对比。


<details>
  <summary>Details</summary>
Motivation: 解决图像生成技术带来的公共安全问题，避免检测方法成为‘黑箱’，追求泛化性和透明性。

Method: 设计六种不同的提示词，提出一个框架整合这些提示词，构建更鲁棒、可解释和基于推理的检测系统。

Result: 评估了MLLMs与传统方法及人类评估者的能力，展示了其优势和局限性。

Conclusion: MLLMs为AI生成图像检测提供了新的可能性，结合提示词框架可提升检测系统的性能。

Abstract: Progress in image generation raises significant public security concerns. We
argue that fake image detection should not operate as a "black box". Instead,
an ideal approach must ensure both strong generalization and transparency.
Recent progress in Multi-modal Large Language Models (MLLMs) offers new
opportunities for reasoning-based AI-generated image detection. In this work,
we evaluate the capabilities of MLLMs in comparison to traditional detection
methods and human evaluators, highlighting their strengths and limitations.
Furthermore, we design six distinct prompts and propose a framework that
integrates these prompts to develop a more robust, explainable, and
reasoning-driven detection system. The code is available at
https://github.com/Gennadiyev/mllm-defake.

</details>

### [301] [Any Image Restoration via Efficient Spatial-Frequency Degradation Adaptation](https://arxiv.org/abs/2504.14249)
*Bin Ren,Eduard Zamfir,Zongwei Wu,Yawei Li,Yidi Li,Danda Pani Paudel,Radu Timofte,Ming-Hsuan Yang,Luc Van Gool,Nicu Sebe*

Main category: cs.CV

TLDR: AnyIR提出了一种统一的图像修复方法，通过联合嵌入机制实现高效全面的修复，无需增加模型规模或依赖大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法需为每种退化训练专用模型，效率低且冗余；现有方法或增加模型规模，或引入复杂跨模态转换。AnyIR旨在解决这些问题。

Method: 利用退化间的相似性，通过门控机制重加权子潜在空间的关键组件，并提出空间-频率并行融合策略增强修复细节。

Result: 在统一修复设置中表现优异，参数和计算量分别减少约82%和85%。

Conclusion: AnyIR提供了一种高效、轻量级的图像修复解决方案，性能达到SOTA水平。

Abstract: Restoring any degraded image efficiently via just one model has become
increasingly significant and impactful, especially with the proliferation of
mobile devices. Traditional solutions typically involve training dedicated
models per degradation, resulting in inefficiency and redundancy. More recent
approaches either introduce additional modules to learn visual prompts,
significantly increasing model size, or incorporate cross-modal transfer from
large language models trained on vast datasets, adding complexity to the system
architecture. In contrast, our approach, termed AnyIR, takes a unified path
that leverages inherent similarity across various degradations to enable both
efficient and comprehensive restoration through a joint embedding mechanism,
without scaling up the model or relying on large language models.Specifically,
we examine the sub-latent space of each input, identifying key components and
reweighting them first in a gated manner. To fuse the intrinsic degradation
awareness and the contextualized attention, a spatial-frequency parallel fusion
strategy is proposed for enhancing spatial-aware local-global interactions and
enriching the restoration details from the frequency perspective. Extensive
benchmarking in the all-in-one restoration setting confirms AnyIR's SOTA
performance, reducing model complexity by around 82\% in parameters and 85\% in
FLOPs. Our code will be available at our Project page
(https://amazingren.github.io/AnyIR/)

</details>

### [302] [ColorVein: Colorful Cancelable Vein Biometrics](https://arxiv.org/abs/2504.14253)
*Yifan Wang,Jie Gui,Xinli Shi,Linqing Gui,Yuan Yan Tang,James Tin-Yau Kwok*

Main category: cs.CV

TLDR: 本文提出了一种创新的可取消静脉生物特征生成方案ColorVein，通过引入颜色信息增强静脉图像的信息密度，同时保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门针对静脉生物特征的可取消模板生成方案，生物信息泄露可能威胁用户隐私和匿名性。

Method: ColorVein通过交互式着色将灰度静脉图像转换为动态可控的彩色表示，用户可定义伪随机颜色空间，并提出新的安全中心损失优化特征提取模型。

Result: ColorVein在识别性能、不可链接性、不可逆性和可撤销性方面表现优异，安全性分析显示其具有竞争力。

Conclusion: ColorVein是一种高效且安全的可取消静脉生物特征生成方案，优于现有方法。

Abstract: Vein recognition technologies have become one of the primary solutions for
high-security identification systems. However, the issue of biometric
information leakage can still pose a serious threat to user privacy and
anonymity. Currently, there is no cancelable biometric template generation
scheme specifically designed for vein biometrics. Therefore, this paper
proposes an innovative cancelable vein biometric generation scheme: ColorVein.
Unlike previous cancelable template generation schemes, ColorVein does not
destroy the original biometric features and introduces additional color
information to grayscale vein images. This method significantly enhances the
information density of vein images by transforming static grayscale information
into dynamically controllable color representations through interactive
colorization. ColorVein allows users/administrators to define a controllable
pseudo-random color space for grayscale vein images by editing the position,
number, and color of hint points, thereby generating protected cancelable
templates. Additionally, we propose a new secure center loss to optimize the
training process of the protected feature extraction model, effectively
increasing the feature distance between enrolled users and any potential
impostors. Finally, we evaluate ColorVein's performance on all types of vein
biometrics, including recognition performance, unlinkability, irreversibility,
and revocability, and conduct security and privacy analyses. ColorVein achieves
competitive performance compared with state-of-the-art methods.

</details>

### [303] [Visual Consensus Prompting for Co-Salient Object Detection](https://arxiv.org/abs/2504.14254)
*Jie Wang,Nana Yu,Zihao Zhang,Yahong Han*

Main category: cs.CV

TLDR: 论文提出了一种参数高效的简洁架构（VCP），通过视觉共识提示（VCP）解决现有CoSOD方法在共识提取和参数效率上的不足，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有CoSOD方法依赖编码特征提取共识，但共识未及时指导编码阶段，且全参数微调效率低下。

Method: 提出参数高效的提示调优范式，嵌入共识形成视觉共识提示（VCP），包括共识提示生成器（CPG）和分散器（CPD）。

Result: VCP在CoCA数据集上F_m指标提升6.8%，优于13种前沿全微调模型。

Conclusion: VCP通过任务特定视觉共识提示，有效激活预训练模型潜力，成为CoSOD任务的新标杆。

Abstract: Existing co-salient object detection (CoSOD) methods generally employ a
three-stage architecture (i.e., encoding, consensus extraction & dispersion,
and prediction) along with a typical full fine-tuning paradigm. Although they
yield certain benefits, they exhibit two notable limitations: 1) This
architecture relies on encoded features to facilitate consensus extraction, but
the meticulously extracted consensus does not provide timely guidance to the
encoding stage. 2) This paradigm involves globally updating all parameters of
the model, which is parameter-inefficient and hinders the effective
representation of knowledge within the foundation model for this task.
Therefore, in this paper, we propose an interaction-effective and
parameter-efficient concise architecture for the CoSOD task, addressing two key
limitations. It introduces, for the first time, a parameter-efficient prompt
tuning paradigm and seamlessly embeds consensus into the prompts to formulate
task-specific Visual Consensus Prompts (VCP). Our VCP aims to induce the frozen
foundation model to perform better on CoSOD tasks by formulating task-specific
visual consensus prompts with minimized tunable parameters. Concretely, the
primary insight of the purposeful Consensus Prompt Generator (CPG) is to
enforce limited tunable parameters to focus on co-salient representations and
generate consensus prompts. The formulated Consensus Prompt Disperser (CPD)
leverages consensus prompts to form task-specific visual consensus prompts,
thereby arousing the powerful potential of pre-trained models in addressing
CoSOD tasks. Extensive experiments demonstrate that our concise VCP outperforms
13 cutting-edge full fine-tuning models, achieving the new state of the art
(with 6.8% improvement in F_m metrics on the most challenging CoCA dataset).
Source code has been available at https://github.com/WJ-CV/VCP.

</details>

### [304] [Cross-attention for State-based model RWKV-7](https://arxiv.org/abs/2504.14260)
*Liu Xiao,Li Zhiyuan,Lin Yueyu*

Main category: cs.CV

TLDR: CrossWKV是一种新型的跨注意力机制，用于增强RWKV-7模型在文本到图像生成中的表达能力，通过线性复杂度的WKV架构和低秩适应技术实现跨模态对齐。


<details>
  <summary>Details</summary>
Motivation: 提升文本到图像生成的表现力，同时保持线性复杂度和低内存占用。

Method: 采用广义delta规则、向量门控和低秩适应技术，结合非对角输入依赖的转移矩阵。

Result: 在ImageNet 256x256上取得FID 2.88和CLIP分数0.33，性能达到最先进水平。

Conclusion: CrossWKV在跨模态任务中表现出色，适用于高分辨率生成和动态状态操作。

Abstract: We introduce CrossWKV, a novel cross-attention mechanism for the state-based
RWKV-7 model, designed to enhance the expressive power of text-to-image
generation. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV)
architecture, CrossWKV integrates text and image modalities in a single pass,
utilizing a generalized delta rule with vector-valued gating and low-rank
adaptations (LoRA) to achieve superior cross-modal alignment. Unlike
Transformer-based models, CrossWKV's non-diagonal, input-dependent transition
matrix enables it to represent complex functions beyond the $\mathrm{TC}^0$
complexity class, including all regular languages, as demonstrated by its
ability to perform state-tracking tasks like $S_5$ permutation modeling.
Evaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B
and ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and
a CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance
while offering robust generalization across diverse prompts. The model's
enhanced expressivity, combined with constant memory usage and linear scaling,
positions it as a powerful solution for advanced cross-modal tasks, with
potential applications in high-resolution generation and dynamic state
manipulation.Code at https://github.com/TorchRWKV/flash-linear-attention

</details>

### [305] [Text-Audio-Visual-conditioned Diffusion Model for Video Saliency Prediction](https://arxiv.org/abs/2504.14267)
*Li Yu,Xuanzhe Sun,Wei Zhou,Moncef Gabbouj*

Main category: cs.CV

TLDR: 本文提出了一种基于文本-音频-视觉条件的扩散模型（TAVDiff）用于视频显著性预测，通过多模态信息融合和创新的Saliency-DiT结构，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 视频显著性预测对下游应用（如视频压缩和人机交互）至关重要。多模态学习的发展促使研究者探索音频-视觉和文本-视觉方法，以提升预测准确性。

Method: TAVDiff将视频显著性预测视为基于文本、音频和视觉输入的图像生成任务，通过逐步去噪预测显著性图。使用大型多模态模型生成文本描述，并提出SITR机制和Saliency-DiT结构以优化条件信息引导。

Result: 实验表明，TAVDiff在SIM、CC、NSS和AUC-J指标上分别提升了1.03%、2.35%、2.71%和0.33%，优于现有方法。

Conclusion: TAVDiff通过多模态信息融合和条件信息解耦，显著提升了视频显著性预测的准确性，为相关应用提供了有效工具。

Abstract: Video saliency prediction is crucial for downstream applications, such as
video compression and human-computer interaction. With the flourishing of
multimodal learning, researchers started to explore multimodal video saliency
prediction, including audio-visual and text-visual approaches. Auditory cues
guide the gaze of viewers to sound sources, while textual cues provide semantic
guidance for understanding video content. Integrating these complementary cues
can improve the accuracy of saliency prediction. Therefore, we attempt to
simultaneously analyze visual, auditory, and textual modalities in this paper,
and propose TAVDiff, a Text-Audio-Visual-conditioned Diffusion Model for video
saliency prediction. TAVDiff treats video saliency prediction as an image
generation task conditioned on textual, audio, and visual inputs, and predicts
saliency maps through stepwise denoising. To effectively utilize text, a large
multimodal model is used to generate textual descriptions for video frames and
introduce a saliency-oriented image-text response (SITR) mechanism to generate
image-text response maps. It is used as conditional information to guide the
model to localize the visual regions that are semantically related to the
textual description. Regarding the auditory modality, it is used as another
conditional information for directing the model to focus on salient regions
indicated by sounds. At the same time, since the diffusion transformer (DiT)
directly concatenates the conditional information with the timestep, which may
affect the estimation of the noise level. To achieve effective conditional
guidance, we propose Saliency-DiT, which decouples the conditional information
from the timestep. Experimental results show that TAVDiff outperforms existing
methods, improving 1.03\%, 2.35\%, 2.71\% and 0.33\% on SIM, CC, NSS and AUC-J
metrics, respectively.

</details>

### [306] [RAMCT: Novel Region-adaptive Multi-channel Tracker with Iterative Tikhonov Regularization for Thermal Infrared Tracking](https://arxiv.org/abs/2504.14278)
*Shang Zhang,Yuke Hou,Guoqiang Gong,Ruoyan Xiong,Yue Zhang*

Main category: cs.CV

TLDR: RAMCT是一种区域自适应稀疏相关滤波器跟踪器，通过多通道特征优化和自适应正则化策略，解决了热红外目标跟踪中的低分辨率、遮挡、背景杂乱和目标变形等问题。


<details>
  <summary>Details</summary>
Motivation: 现有相关滤波器跟踪器在热红外目标跟踪中面临低分辨率、遮挡、背景杂乱和目标变形等挑战，影响跟踪性能。

Method: RAMCT结合了空间自适应二值掩码、基于GSVD的区域自适应迭代Tikhonov正则化方法，以及动态参数调整的在线优化策略。

Result: 在多个基准测试中，RAMCT在准确性和鲁棒性上优于其他先进跟踪器。

Conclusion: RAMCT通过多通道特征优化和自适应策略，显著提升了热红外目标跟踪的性能。

Abstract: Correlation filter (CF)-based trackers have gained significant attention for
their computational efficiency in thermal infrared (TIR) target tracking.
However, ex-isting methods struggle with challenges such as low-resolution
imagery, occlu-sion, background clutter, and target deformation, which severely
impact tracking performance. To overcome these limitations, we propose RAMCT, a
region-adaptive sparse correlation filter tracker that integrates multi-channel
feature opti-mization with an adaptive regularization strategy. Firstly, we
refine the CF learn-ing process by introducing a spatially adaptive binary
mask, which enforces spar-sity in the target region while dynamically
suppressing background interference. Secondly, we introduce generalized
singular value decomposition (GSVD) and propose a novel GSVD-based
region-adaptive iterative Tikhonov regularization method. This enables flexible
and robust optimization across multiple feature channels, improving resilience
to occlusion and background variations. Thirdly, we propose an online
optimization strategy with dynamic discrepancy-based pa-rameter adjustment.
This mechanism facilitates real time adaptation to target and background
variations, thereby improving tracking accuracy and robustness. Ex-tensive
experiments on LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR2017 benchmarks
demonstrate that RAMCT outperforms other state-of-the-art trackers in terms of
accuracy and robustness.

</details>

### [307] [CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey](https://arxiv.org/abs/2504.14280)
*Jindong Li,Yongguang Li,Yali Fu,Jiahong Liu,Yixin Liu,Menglin Yang,Irwin King*

Main category: cs.CV

TLDR: 本文综述了CLIP在领域泛化（DG）和领域适应（DA）中的应用，填补了现有文献的空白，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏对CLIP在DG和DA中应用的系统性综述，本文旨在填补这一空白，为研究者和实践者提供参考。

Method: 在DG中，方法分为优化提示学习和利用CLIP作为特征提取器；在DA中，分为基于源数据的方法和基于目标数据的无源方法。

Result: 综述总结了CLIP在DG和DA中的关键应用，并指出了过拟合、领域多样性和计算效率等挑战。

Conclusion: 本文为未来研究提供了方向，旨在推动更鲁棒的机器学习模型的发展。

Abstract: As machine learning evolves, domain generalization (DG) and domain adaptation
(DA) have become crucial for enhancing model robustness across diverse
environments. Contrastive Language-Image Pretraining (CLIP) plays a significant
role in these tasks, offering powerful zero-shot capabilities that allow models
to perform effectively in unseen domains. However, there remains a significant
gap in the literature, as no comprehensive survey currently exists that
systematically explores the applications of CLIP in DG and DA, highlighting the
necessity for this review. This survey presents a comprehensive review of
CLIP's applications in DG and DA. In DG, we categorize methods into optimizing
prompt learning for task alignment and leveraging CLIP as a backbone for
effective feature extraction, both enhancing model adaptability. For DA, we
examine both source-available methods utilizing labeled source data and
source-free approaches primarily based on target domain data, emphasizing
knowledge transfer mechanisms and strategies for improved performance across
diverse contexts. Key challenges, including overfitting, domain diversity, and
computational efficiency, are addressed, alongside future research
opportunities to advance robustness and efficiency in practical applications.
By synthesizing existing literature and pinpointing critical gaps, this survey
provides valuable insights for researchers and practitioners, proposing
directions for effectively leveraging CLIP to enhance methodologies in domain
generalization and adaptation. Ultimately, this work aims to foster innovation
and collaboration in the quest for more resilient machine learning models that
can perform reliably across diverse real-world scenarios. A more up-to-date
version of the papers is maintained at:
https://github.com/jindongli-Ai/Survey_on_CLIP-Powered_Domain_Generalization_and_Adaptation.

</details>

### [308] [ISTD-YOLO: A Multi-Scale Lightweight High-Performance Infrared Small Target Detection Algorithm](https://arxiv.org/abs/2504.14289)
*Shang Zhang,Yujie Cui,Ruoyan Xiong,Huanbin Zhang*

Main category: cs.CV

TLDR: ISTD-YOLO是一种基于改进YOLOv7的轻量级红外小目标检测算法，通过轻量化重构、引入无参注意力机制和优化NWD指标，显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 针对红外图像检测中背景复杂、信噪比低、目标尺寸小和亮度弱等难点，提出一种高效检测方法。

Method: 1. 轻量化重构YOLOv7网络结构；2. 用VoV-GSCSP替换ELAN-W模块；3. 引入无参注意力机制；4. 使用NWD优化IoU指标。

Result: 实验表明，ISTD-YOLO在检测效果和各项指标上均优于YOLOv7及主流算法。

Conclusion: ISTD-YOLO能高效实现红外小目标的高质量检测。

Abstract: Aiming at the detection difficulties of infrared images such as complex
background, low signal-to-noise ratio, small target size and weak brightness, a
lightweight infrared small target detection algorithm ISTD-YOLO based on
improved YOLOv7 was proposed. Firstly, the YOLOv7 network structure was
lightweight reconstructed, and a three-scale lightweight network architecture
was designed. Then, the ELAN-W module of the model neck network is replaced by
VoV-GSCSP to reduce the computational cost and the complexity of the network
structure. Secondly, a parameter-free attention mechanism was introduced into
the neck network to enhance the relevance of local con-text information.
Finally, the Normalized Wasserstein Distance (NWD) was used to optimize the
commonly used IoU index to enhance the localization and detection accuracy of
small targets. Experimental results show that compared with YOLOv7 and the
current mainstream algorithms, ISTD-YOLO can effectively improve the detection
effect, and all indicators are effectively improved, which can achieve
high-quality detection of infrared small targets.

</details>

### [309] [Towards NSFW-Free Text-to-Image Generation via Safety-Constraint Direct Preference Optimization](https://arxiv.org/abs/2504.14290)
*Shouwei Ruan,Zhenyu Wu,Yao Huang,Ruochen Zhang,Yitong Sun,Caixin Kang,Xingxing Wei*

Main category: cs.CV

TLDR: 论文提出了一种名为SC-DPO的新框架，用于解决文本到图像生成中的安全问题，通过引入安全成本模型和动态聚焦机制，显著提升了生成内容的安全性和质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法完全保证生成内容的安全性或在安全性与生成质量之间取得平衡，因此需要一种新的解决方案。

Method: 提出了SC-DPO框架，结合安全约束和人类偏好校准，引入安全成本模型和动态聚焦机制，并使用SCP-10K数据集进行训练。

Result: SC-DPO在实验中表现优于现有方法，能有效防御有害内容并保持高质量生成，同时对对抗性提示具有鲁棒性。

Conclusion: SC-DPO为文本到图像生成的安全对齐提供了一种有效且平衡的解决方案。

Abstract: Ensuring the safety of generated content remains a fundamental challenge for
Text-to-Image (T2I) generation. Existing studies either fail to guarantee
complete safety under potentially harmful concepts or struggle to balance
safety with generation quality. To address these issues, we propose
Safety-Constrained Direct Preference Optimization (SC-DPO), a novel framework
for safety alignment in T2I models. SC-DPO integrates safety constraints into
the general human preference calibration, aiming to maximize the likelihood of
generating human-preferred samples while minimizing the safety cost of the
generated outputs. In SC-DPO, we introduce a safety cost model to accurately
quantify harmful levels for images, and train it effectively using the proposed
contrastive learning and cost anchoring objectives. To apply SC-DPO for
effective T2I safety alignment, we constructed SCP-10K, a safety-constrained
preference dataset containing rich harmful concepts, which blends
safety-constrained preference pairs under both harmful and clean instructions,
further mitigating the trade-off between safety and sample quality.
Additionally, we propose a Dynamic Focusing Mechanism (DFM) for SC-DPO,
promoting the model's learning of difficult preference pair samples. Extensive
experiments demonstrate that SC-DPO outperforms existing methods, effectively
defending against various NSFW content while maintaining optimal sample quality
and human preference alignment. Additionally, SC-DPO exhibits resilience
against adversarial prompts designed to generate harmful content.

</details>

### [310] [From Missing Pieces to Masterpieces: Image Completion with Context-Adaptive Diffusion](https://arxiv.org/abs/2504.14294)
*Pourya Shamsolmoali,Masoumeh Zareapoor,Huiyu Zhou,Michael Felsberg,Dacheng Tao,Xuelong Li*

Main category: cs.CV

TLDR: ConFill提出了一种新的图像补全框架，通过上下文自适应差异模型（CAD）和动态采样机制，显著提升了生成内容与原图的语义和空间对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在图像补全任务中难以保持已知与未知区域的连贯性，主要由于缺乏显式的空间和语义对齐。

Method: ConFill引入CAD模型，逐步减少生成与原图的差异，并结合动态采样机制，自适应调整复杂区域的采样率。

Result: 实验表明，ConFill在图像补全任务中优于现有方法，成为新的基准。

Conclusion: ConFill通过上下文对齐和动态采样，显著提升了图像补全的质量和一致性。

Abstract: Image completion is a challenging task, particularly when ensuring that
generated content seamlessly integrates with existing parts of an image. While
recent diffusion models have shown promise, they often struggle with
maintaining coherence between known and unknown (missing) regions. This issue
arises from the lack of explicit spatial and semantic alignment during the
diffusion process, resulting in content that does not smoothly integrate with
the original image. Additionally, diffusion models typically rely on global
learned distributions rather than localized features, leading to
inconsistencies between the generated and existing image parts. In this work,
we propose ConFill, a novel framework that introduces a Context-Adaptive
Discrepancy (CAD) model to ensure that intermediate distributions of known and
unknown regions are closely aligned throughout the diffusion process. By
incorporating CAD, our model progressively reduces discrepancies between
generated and original images at each diffusion step, leading to contextually
aligned completion. Moreover, ConFill uses a new Dynamic Sampling mechanism
that adaptively increases the sampling rate in regions with high reconstruction
complexity. This approach enables precise adjustments, enhancing detail and
integration in restored areas. Extensive experiments demonstrate that ConFill
outperforms current methods, setting a new benchmark in image completion.

</details>

### [311] [Balancing Privacy and Action Performance: A Penalty-Driven Approach to Image Anonymization](https://arxiv.org/abs/2504.14301)
*Nazia Aslam,Kamal Nasrollahi*

Main category: cs.CV

TLDR: 论文提出了一种隐私保护的图像匿名化技术，通过优化匿名器以平衡隐私保护和动作识别性能。


<details>
  <summary>Details</summary>
Motivation: 视频监控系统的发展引发隐私担忧，如何在保护隐私的同时保持动作识别性能是一个挑战。

Method: 提出了一种基于特征惩罚的方案，优化匿名器以减少隐私泄漏并保持动作识别性能。

Result: 实验表明，该方法在保持隐私泄漏一致的同时提升了动作识别性能。

Conclusion: 该方法有效解决了隐私保护与动作识别性能之间的权衡问题，符合欧盟AI法案和GDPR标准。

Abstract: The rapid development of video surveillance systems for object detection,
tracking, activity recognition, and anomaly detection has revolutionized our
day-to-day lives while setting alarms for privacy concerns. It isn't easy to
strike a balance between visual privacy and action recognition performance in
most computer vision models. Is it possible to safeguard privacy without
sacrificing performance? It poses a formidable challenge, as even minor privacy
enhancements can lead to substantial performance degradation. To address this
challenge, we propose a privacy-preserving image anonymization technique that
optimizes the anonymizer using penalties from the utility branch, ensuring
improved action recognition performance while minimally affecting privacy
leakage. This approach addresses the trade-off between minimizing privacy
leakage and maintaining high action performance. The proposed approach is
primarily designed to align with the regulatory standards of the EU AI Act and
GDPR, ensuring the protection of personally identifiable information while
maintaining action performance. To the best of our knowledge, we are the first
to introduce a feature-based penalty scheme that exclusively controls the
action features, allowing freedom to anonymize private attributes. Extensive
experiments were conducted to validate the effectiveness of the proposed
method. The results demonstrate that applying a penalty to anonymizer from
utility branch enhances action performance while maintaining nearly consistent
privacy leakage across different penalty settings.

</details>

### [312] [Exploring Generalizable Pre-training for Real-world Change Detection via Geometric Estimation](https://arxiv.org/abs/2504.14306)
*Yitao Zhao,Sen Lei,Nanqing Liu,Heng-Chao Li,Turgay Celik,Qing Zhu*

Main category: cs.CV

TLDR: 提出了一种名为MatchCD的自监督变化检测框架，通过几何估计解决多时相图像未对齐和对象变化问题，直接处理大尺度图像。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测算法需依赖精细配准，而现实场景中手动配准增加了复杂度，因此提出自监督框架以简化流程。

Method: 利用零样本能力优化编码器，结合自监督对比表示，同时处理图像未对齐和变化检测问题，无需分块处理大图像。

Result: 在几何失真显著的多场景中表现优异，验证了框架的有效性。

Conclusion: MatchCD框架简化了变化检测流程，提升了处理大尺度图像的效率和性能。

Abstract: As an essential procedure in earth observation system, change detection (CD)
aims to reveal the spatial-temporal evolution of the observation regions. A key
prerequisite for existing change detection algorithms is aligned geo-references
between multi-temporal images by fine-grained registration. However, in the
majority of real-world scenarios, a prior manual registration is required
between the original images, which significantly increases the complexity of
the CD workflow. In this paper, we proposed a self-supervision motivated CD
framework with geometric estimation, called "MatchCD". Specifically, the
proposed MatchCD framework utilizes the zero-shot capability to optimize the
encoder with self-supervised contrastive representation, which is reused in the
downstream image registration and change detection to simultaneously handle the
bi-temporal unalignment and object change issues. Moreover, unlike the
conventional change detection requiring segmenting the full-frame image into
small patches, our MatchCD framework can directly process the original
large-scale image (e.g., 6K*4K resolutions) with promising performance. The
performance in multiple complex scenarios with significant geometric distortion
demonstrates the effectiveness of our proposed framework.

</details>

### [313] [FGSGT: Saliency-Guided Siamese Network Tracker Based on Key Fine-Grained Feature Information for Thermal Infrared Target Tracking](https://arxiv.org/abs/2504.14309)
*Ruoyan Xiong,Huanbin Zhang,Shentao Wang,Hui He,Yuke Hou,Yue Zhang,Yujie Cui,Huipan Guan,Shang Zhang*

Main category: cs.CV

TLDR: 提出了一种基于显著性引导的Siamese网络跟踪器，通过细粒度特征学习和多层融合，显著提升了热红外图像的目标跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 热红外图像特征稀疏且对比度低，传统模型难以捕捉目标特征，易受干扰和跟踪漂移影响。

Method: 设计双流细粒度特征并行学习模块、多层特征融合模块和Siamese残差细化块，结合显著性损失函数优化预测。

Result: 在多个基准测试中表现优异，最高精度达0.78（VOT-TIR 2015）和0.75（VOT-TIR 2017）。

Conclusion: 该方法通过细粒度特征和显著性引导，显著提升了热红外目标跟踪的鲁棒性和准确性。

Abstract: Thermal infrared (TIR) images typically lack detailed features and have low
contrast, making it challenging for conventional feature extraction models to
capture discriminative target characteristics. As a result, trackers are often
affected by interference from visually similar objects and are susceptible to
tracking drift. To address these challenges, we propose a novel saliency-guided
Siamese network tracker based on key fine-grained feature infor-mation. First,
we introduce a fine-grained feature parallel learning convolu-tional block with
a dual-stream architecture and convolutional kernels of varying sizes. This
design captures essential global features from shallow layers, enhances feature
diversity, and minimizes the loss of fine-grained in-formation typically
encountered in residual connections. In addition, we propose a multi-layer
fine-grained feature fusion module that uses bilinear matrix multiplication to
effectively integrate features across both deep and shallow layers. Next, we
introduce a Siamese residual refinement block that corrects saliency map
prediction errors using residual learning. Combined with deep supervision, this
mechanism progressively refines predictions, ap-plying supervision at each
recursive step to ensure consistent improvements in accuracy. Finally, we
present a saliency loss function to constrain the sali-ency predictions,
directing the network to focus on highly discriminative fi-ne-grained features.
Extensive experiment results demonstrate that the pro-posed tracker achieves
the highest precision and success rates on the PTB-TIR and LSOTB-TIR
benchmarks. It also achieves a top accuracy of 0.78 on the VOT-TIR 2015
benchmark and 0.75 on the VOT-TIR 2017 benchmark.

</details>

### [314] [DCFG: Diverse Cross-Channel Fine-Grained Feature Learning and Progressive Fusion Siamese Tracker for Thermal Infrared Target Tracking](https://arxiv.org/abs/2504.14311)
*Ruoyan Xiong,Yuke Hou,Princess Retor Torboh,Hui He,Huanbin Zhang,Yue Zhang,Yanpin Wang,Huipan Guan,Shang Zhang*

Main category: cs.CV

TLDR: 提出了一种基于跨通道细粒度特征学习和渐进融合的新型Siamese跟踪器，用于热红外（TIR）跟踪，显著提高了跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 解决热红外跟踪中捕捉高判别性特征的挑战。

Method: 1. 跨通道细粒度特征学习网络，通过掩码和抑制系数抑制主导目标特征；2. 引入通道重排机制和通道均衡化减少参数；3. 逐层组合单元用于特征提取与融合；4. 跨通道细粒度损失函数引导特征组捕捉目标细节。

Result: 在VOT-TIR 2015和2017基准测试中分别达到0.81和0.78的准确率，并在LSOTB-TIR和PTB-TIR基准测试中优于其他方法。

Conclusion: 提出的方法在热红外跟踪中表现出色，显著提升了目标表示和跟踪精度。

Abstract: To address the challenge of capturing highly discriminative features in
ther-mal infrared (TIR) tracking, we propose a novel Siamese tracker based on
cross-channel fine-grained feature learning and progressive fusion. First, we
introduce a cross-channel fine-grained feature learning network that employs
masks and suppression coefficients to suppress dominant target features,
en-abling the tracker to capture more detailed and subtle information. The
net-work employs a channel rearrangement mechanism to enhance efficient
in-formation flow, coupled with channel equalization to reduce parameter count.
Additionally, we incorporate layer-by-layer combination units for ef-fective
feature extraction and fusion, thereby minimizing parameter redun-dancy and
computational complexity. The network further employs feature redirection and
channel shuffling strategies to better integrate fine-grained details. Second,
we propose a specialized cross-channel fine-grained loss function designed to
guide feature groups toward distinct discriminative re-gions of the target,
thus improving overall target representation. This loss function includes an
inter-channel loss term that promotes orthogonality be-tween channels,
maximizing feature diversity and facilitating finer detail capture. Extensive
experiments demonstrate that our proposed tracker achieves the highest
accuracy, scoring 0.81 on the VOT-TIR 2015 and 0.78 on the VOT-TIR 2017
benchmark, while also outperforming other methods across all evaluation metrics
on the LSOTB-TIR and PTB-TIR benchmarks.

</details>

### [315] [Visual Prompting for One-shot Controllable Video Editing without Inversion](https://arxiv.org/abs/2504.14335)
*Zhengbo Zhang,Yuxi Zhou,Duo Peng,Joo-Hwee Lim,Zhigang Tu,De Wen Soh,Lin Geng Foo*

Main category: cs.CV

TLDR: 论文提出了一种无需DDIM反转的单次可控视频编辑方法，通过视觉提示和一致性采样确保内容一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因DDIM反转误差导致的内容一致性问题。

Method: 采用视觉提示视角，提出内容一致性采样（CCS）和时间一致性采样（TCS）。

Result: 实验验证了方法的有效性。

Conclusion: 新方法显著提升了视频编辑的内容和时间一致性。

Abstract: One-shot controllable video editing (OCVE) is an important yet challenging
task, aiming to propagate user edits that are made -- using any image editing
tool -- on the first frame of a video to all subsequent frames, while ensuring
content consistency between edited frames and source frames. To achieve this,
prior methods employ DDIM inversion to transform source frames into latent
noise, which is then fed into a pre-trained diffusion model, conditioned on the
user-edited first frame, to generate the edited video. However, the DDIM
inversion process accumulates errors, which hinder the latent noise from
accurately reconstructing the source frames, ultimately compromising content
consistency in the generated edited frames. To overcome it, our method
eliminates the need for DDIM inversion by performing OCVE through a novel
perspective based on visual prompting. Furthermore, inspired by consistency
models that can perform multi-step consistency sampling to generate a sequence
of content-consistent images, we propose a content consistency sampling (CCS)
to ensure content consistency between the generated edited frames and the
source frames. Moreover, we introduce a temporal-content consistency sampling
(TCS) based on Stein Variational Gradient Descent to ensure temporal
consistency across the edited frames. Extensive experiments validate the
effectiveness of our approach.

</details>

### [316] [Gaussian Shading++: Rethinking the Realistic Deployment Challenge of Performance-Lossless Image Watermark for Diffusion Models](https://arxiv.org/abs/2504.15026)
*Zijin Yang,Xin Zhang,Kejiang Chen,Kai Zeng,Qiyi Yao,Han Fang,Weiming Zhang,Nenghai Yu*

Main category: cs.CV

TLDR: 论文提出Gaussian Shading++，一种针对扩散模型的水印方法，解决了实际部署中的关键挑战，如密钥管理、用户参数变化和第三方验证。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在实际应用中面临版权保护和不当内容生成的伦理问题，现有水印方法忽视了实际部署的复杂性。

Method: 采用双通道设计和伪随机纠错码编码种子，结合高斯噪声通道建模和软决策解码策略，并引入公钥签名。

Result: 实验表明，该方法在保持性能无损的同时，鲁棒性优于现有方法。

Conclusion: Gaussian Shading++是一种更实用的解决方案，适用于实际部署。

Abstract: Ethical concerns surrounding copyright protection and inappropriate content
generation pose challenges for the practical implementation of diffusion
models. One effective solution involves watermarking the generated images.
Existing methods primarily focus on ensuring that watermark embedding does not
degrade the model performance. However, they often overlook critical challenges
in real-world deployment scenarios, such as the complexity of watermark key
management, user-defined generation parameters, and the difficulty of
verification by arbitrary third parties. To address this issue, we propose
Gaussian Shading++, a diffusion model watermarking method tailored for
real-world deployment. We propose a double-channel design that leverages
pseudorandom error-correcting codes to encode the random seed required for
watermark pseudorandomization, achieving performance-lossless watermarking
under a fixed watermark key and overcoming key management challenges.
Additionally, we model the distortions introduced during generation and
inversion as an additive white Gaussian noise channel and employ a novel soft
decision decoding strategy during extraction, ensuring strong robustness even
when generation parameters vary. To enable third-party verification, we
incorporate public key signatures, which provide a certain level of resistance
against forgery attacks even when model inversion capabilities are fully
disclosed. Extensive experiments demonstrate that Gaussian Shading++ not only
maintains performance losslessness but also outperforms existing methods in
terms of robustness, making it a more practical solution for real-world
deployment.

</details>

### [317] [Multispectral airborne laser scanning for tree species classification: a benchmark of machine learning and deep learning algorithms](https://arxiv.org/abs/2504.14337)
*Josef Taher,Eric Hyyppä,Matti Hyyppä,Klaara Salolahti,Xiaowei Yu,Leena Matikainen,Antero Kukko,Matti Lehtomäki,Harri Kaartinen,Sopitta Thurachen,Paula Litkey,Ville Luoma,Markus Holopainen,Gefei Kong,Hongchao Fan,Petri Rönnholm,Antti Polvivaara,Samuli Junttila,Mikko Vastaranta,Stefano Puliti,Rasmus Astrup,Joel Kostensalo,Mari Myllymäki,Maksymilian Kulicki,Krzysztof Stereńczak,Raul de Paula Pires,Ruben Valbuena,Juan Pedro Carbonell-Rivera,Jesús Torralba,Yi-Chen Chen,Lukas Winiwarter,Markus Hollaus,Gottfried Mandlburger,Narges Takhtkeshha,Fabio Remondino,Maciej Lisiewicz,Bartłomiej Kraszewski,Xinlian Liang,Jianchang Chen,Eero Ahokas,Kirsi Karila,Eugeniu Vezeteu,Petri Manninen,Roope Näsi,Heikki Hyyti,Siiri Pyykkönen,Peilun Hu,Juha Hyyppä*

Main category: cs.CV

TLDR: 该研究通过多光谱机载激光扫描数据，比较了机器学习和深度学习方法在树种分类中的表现，发现基于点的深度学习模型（如点变换器）在高密度点云数据中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 气候智能和生物多样性保护的林业需要精确的森林资源信息，包括单株树种的识别。多光谱机载激光扫描（ALS）在自动化点云处理和树种分割方面有潜力，但稀有树种识别和深度学习技术的应用仍存在挑战。

Method: 研究收集了高密度多光谱ALS数据（>1000点/平方米），并使用FGI开发的HeliALS系统和Optech Titan数据（35点/平方米），在芬兰南部的测试场地评估了多种算法的树种分类准确性。

Result: 基于5261个测试片段，点变换器模型在高密度多光谱点云中表现最佳，总体准确率达87.9%（宏平均74.5%）。光谱信息的加入显著提升了分类性能。

Conclusion: 基于点的深度学习方法在高密度多光谱ALS数据中优于传统机器学习和基于图像的深度学习方法，光谱信息对分类性能的提升至关重要。

Abstract: Climate-smart and biodiversity-preserving forestry demands precise
information on forest resources, extending to the individual tree level.
Multispectral airborne laser scanning (ALS) has shown promise in automated
point cloud processing and tree segmentation, but challenges remain in
identifying rare tree species and leveraging deep learning techniques. This
study addresses these gaps by conducting a comprehensive benchmark of machine
learning and deep learning methods for tree species classification. For the
study, we collected high-density multispectral ALS data (>1000 pts/m$^2$) at
three wavelengths using the FGI-developed HeliALS system, complemented by
existing Optech Titan data (35 pts/m$^2$), to evaluate the species
classification accuracy of various algorithms in a test site located in
Southern Finland. Based on 5261 test segments, our findings demonstrate that
point-based deep learning methods, particularly a point transformer model,
outperformed traditional machine learning and image-based deep learning
approaches on high-density multispectral point clouds. For the high-density ALS
dataset, a point transformer model provided the best performance reaching an
overall (macro-average) accuracy of 87.9% (74.5%) with a training set of 1065
segments and 92.0% (85.1%) with 5000 training segments. The best image-based
deep learning method, DetailView, reached an overall (macro-average) accuracy
of 84.3% (63.9%), whereas a random forest (RF) classifier achieved an overall
(macro-average) accuracy of 83.2% (61.3%). Importantly, the overall
classification accuracy of the point transformer model on the HeliALS data
increased from 73.0% with no spectral information to 84.7% with single-channel
reflectance, and to 87.9% with spectral information of all the three channels.

</details>

### [318] [Manipulating Multimodal Agents via Cross-Modal Prompt Injection](https://arxiv.org/abs/2504.14348)
*Le Wang,Zonghao Ying,Tianyuan Zhang,Siyuan Liang,Shengshan Hu,Mingchuan Zhang,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TLDR: 论文提出了一种针对多模态代理的新型安全漏洞攻击方法CrossInject，通过跨模态对抗扰动实现恶意指令注入，显著提升了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在整合语言和视觉模态时存在安全漏洞，攻击者可通过跨模态注入恶意指令操控代理决策。

Method: 提出CrossInject攻击框架，包括视觉潜在对齐和文本引导增强两部分，分别优化对抗图像特征和生成恶意文本指令。

Result: 实验表明，该方法攻击成功率比现有方法至少提高26.4%，并在实际多模态自主代理中验证了有效性。

Conclusion: 研究揭示了多模态代理的安全隐患，对安全关键应用具有潜在影响。

Abstract: The emergence of multimodal large language models has redefined the agent
paradigm by integrating language and vision modalities with external data
sources, enabling agents to better interpret human instructions and execute
increasingly complex tasks. However, in this work, we identify a critical yet
previously overlooked security vulnerability in multimodal agents: cross-modal
prompt injection attacks. To exploit this vulnerability, we propose
CrossInject, a novel attack framework in which attackers embed adversarial
perturbations across multiple modalities to align with target malicious
content, allowing external instructions to hijack the agent's decision-making
process and execute unauthorized tasks. Our approach consists of two key
components. First, we introduce Visual Latent Alignment, where we optimize
adversarial features to the malicious instructions in the visual embedding
space based on a text-to-image generative model, ensuring that adversarial
images subtly encode cues for malicious task execution. Subsequently, we
present Textual Guidance Enhancement, where a large language model is leveraged
to infer the black-box defensive system prompt through adversarial meta
prompting and generate an malicious textual command that steers the agent's
output toward better compliance with attackers' requests. Extensive experiments
demonstrate that our method outperforms existing injection attacks, achieving
at least a +26.4% increase in attack success rates across diverse tasks.
Furthermore, we validate our attack's effectiveness in real-world multimodal
autonomous agents, highlighting its potential implications for safety-critical
applications.

</details>

### [319] [A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling](https://arxiv.org/abs/2504.14359)
*Kyle Buettner,Jacob Emmerson,Adriana Kovashka*

Main category: cs.CV

TLDR: 本文提出了一种基于LLM的多模态重描述策略，通过修改英文描述的多样性来提升多语言视觉语言模型（VLMs）的感知多样性理解，并在德日文本-图像检索任务中取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的数据主要来自英语使用者，导致模型存在感知偏见和灵活性不足的问题。本文旨在通过数据高效的方法提升模型对多语言和多元文化的理解能力。

Method: 提出了一种基于LLM的多模态重描述策略，通过修改英文描述后再翻译，并结合原生语言数据指导的多模态机制。

Result: 在德日文本-图像检索任务中，平均召回率提升了3.5，非母语错误案例中提升了4.7。

Conclusion: 该方法有效提升了多语言VLMs的感知多样性，并提供了跨数据集和跨语言泛化的分析机制。

Abstract: There are many ways to describe, name, and group objects when captioning an
image. Differences are evident when speakers come from diverse cultures due to
the unique experiences that shape perception. Machine translation of captions
has pushed multilingual capabilities in vision-language models (VLMs), but data
comes mainly from English speakers, indicating a perceptual bias and lack of
model flexibility. In this work, we address this challenge and outline a
data-efficient framework to instill multilingual VLMs with greater
understanding of perceptual diversity. We specifically propose an LLM-based,
multimodal recaptioning strategy that alters the object descriptions of English
captions before translation. The greatest benefits are demonstrated in a
targeted multimodal mechanism guided by native speaker data. By adding produced
rewrites as augmentations in training, we improve on German and Japanese
text-image retrieval cases studies (up to +3.5 mean recall overall, +4.7 on
non-native error cases). We further propose a mechanism to analyze the specific
object description differences across datasets, and we offer insights into
cross-dataset and cross-language generalization.

</details>

### [320] [Efficient Spiking Point Mamba for Point Cloud Analysis](https://arxiv.org/abs/2504.14371)
*Peixi Wu,Bosong Chai,Menghua Zheng,Wei Li,Zhangchi Hu,Jie Chen,Zheyu Zhang,Hebei Li,Xiaoyan Sun*

Main category: cs.CV

TLDR: 提出了一种基于Mamba的3D脉冲神经网络（SPM），结合了Mamba的序列建模能力和SNN的时序特征提取，显著提升了性能并降低了能耗。


<details>
  <summary>Details</summary>
Motivation: 现有3D SNN在长程依赖建模上表现不佳，而Mamba的高效计算和序列建模能力为解决这一问题提供了可能。

Method: 设计了层次动态编码（HDE）和脉冲Mamba块（SMB），并采用非对称SNN-ANN架构进行预训练和微调。

Result: 在ScanObjectNN和ShapeNetPart数据集上显著提升了性能，能耗比ANN低3.5倍。

Conclusion: SPM为3D SNN领域提供了高效且高性能的解决方案，代码将开源。

Abstract: Bio-inspired Spiking Neural Networks (SNNs) provide an energy-efficient way
to extract 3D spatio-temporal features. However, existing 3D SNNs have
struggled with long-range dependencies until the recent emergence of Mamba,
which offers superior computational efficiency and sequence modeling
capability. In this work, we propose Spiking Point Mamba (SPM), the first
Mamba-based SNN in the 3D domain. Due to the poor performance of simply
transferring Mamba to 3D SNNs, SPM is designed to utilize both the sequence
modeling capabilities of Mamba and the temporal feature extraction of SNNs.
Specifically, we first introduce Hierarchical Dynamic Encoding (HDE), an
improved direct encoding method that effectively introduces dynamic temporal
mechanism, thereby facilitating temporal interactions. Then, we propose a
Spiking Mamba Block (SMB), which builds upon Mamba while learning
inter-time-step features and minimizing information loss caused by spikes.
Finally, to further enhance model performance, we adopt an asymmetric SNN-ANN
architecture for spike-based pre-training and finetune. Compared with the
previous state-of-the-art SNN models, SPM improves OA by +6.2%, +6.1%, and
+7.4% on three variants of ScanObjectNN, and boosts instance mIOU by +1.9% on
ShapeNetPart. Meanwhile, its energy consumption is at least 3.5x lower than
that of its ANN counterpart. The code will be made publicly available.

</details>

### [321] [LOOPE: Learnable Optimal Patch Order in Positional Embeddings for Vision Transformers](https://arxiv.org/abs/2504.14386)
*Md Abtahi Majeed Chowdhury,Md Rifat Ur Rahman,Akil Ahmad Taki*

Main category: cs.CV

TLDR: 论文提出了一种可学习的补丁排序方法LOOPE，用于优化视觉Transformer中的位置嵌入，显著提升了分类准确性，并通过新提出的“三细胞实验”验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有位置嵌入方法忽略了补丁排序的影响，导致2D网格到1D序列的映射存在挑战。

Method: 提出LOOPE方法，优化补丁排序以增强空间表示，并引入“三细胞实验”评估位置嵌入效果。

Result: LOOPE显著提升了分类准确性，实验显示其保留了相对和绝对位置信息，性能差距达30-35%。

Conclusion: LOOPE为位置嵌入提供了更优的解决方案，并通过新评估框架验证了其有效性。

Abstract: Positional embeddings (PE) play a crucial role in Vision Transformers (ViTs)
by providing spatial information otherwise lost due to the permutation
invariant nature of self attention. While absolute positional embeddings (APE)
have shown theoretical advantages over relative positional embeddings (RPE),
particularly due to the ability of sinusoidal functions to preserve spatial
inductive biases like monotonicity and shift invariance, a fundamental
challenge arises when mapping a 2D grid to a 1D sequence. Existing methods have
mostly overlooked or never explored the impact of patch ordering in positional
embeddings. To address this, we propose LOOPE, a learnable patch-ordering
method that optimizes spatial representation for a given set of frequencies,
providing a principled approach to patch order optimization. Empirical results
show that our PE significantly improves classification accuracy across various
ViT architectures. To rigorously evaluate the effectiveness of positional
embeddings, we introduce the "Three Cell Experiment", a novel benchmarking
framework that assesses the ability of PEs to retain relative and absolute
positional information across different ViT architectures. Unlike standard
evaluations, which typically report a performance gap of 4 to 6% between models
with and without PE, our method reveals a striking 30 to 35% difference,
offering a more sensitive diagnostic tool to measure the efficacy of PEs. Our
experimental analysis confirms that the proposed LOOPE demonstrates enhanced
effectiveness in retaining both relative and absolute positional information.

</details>

### [322] [How Well Can General Vision-Language Models Learn Medicine By Watching Public Educational Videos?](https://arxiv.org/abs/2504.14391)
*Rahul Thapa,Andrew Li,Qingyang Wu,Bryan He,Yuki Sahashi,Christina Binder,Angela Zhang,Ben Athiwaratkun,Shuaiwen Leon Song,David Ouyang,James Zou*

Main category: cs.CV

TLDR: OpenBiomedVi数据集通过教育视频训练视觉语言模型，显著提升生物医学任务性能。


<details>
  <summary>Details</summary>
Motivation: 探索非标准化教育视频是否能有效训练生物医学视觉语言模型。

Method: 构建OpenBiomedVi数据集，包含1031小时视频-字幕和Q/A对，并微调Qwen-2-VL模型。

Result: 模型在视频和图像任务上性能显著提升，2B模型视频任务提升98.7%，图像任务71.2%。

Conclusion: 教育视频为生物医学视觉语言模型提供了有效的训练信号。

Abstract: Publicly available biomedical videos, such as those on YouTube, serve as
valuable educational resources for medical students. Unlike standard machine
learning datasets, these videos are designed for human learners, often mixing
medical imagery with narration, explanatory diagrams, and contextual framing.
In this work, we investigate whether such pedagogically rich, yet
non-standardized and heterogeneous videos can effectively teach general-domain
vision-language models biomedical knowledge. To this end, we introduce
OpenBiomedVi, a biomedical video instruction tuning dataset comprising 1031
hours of video-caption and Q/A pairs, curated through a multi-step
human-in-the-loop pipeline. Diverse biomedical video datasets are rare, and
OpenBiomedVid fills an important gap by providing instruction-style supervision
grounded in real-world educational content. Surprisingly, despite the informal
and heterogeneous nature of these videos, the fine-tuned Qwen-2-VL models
exhibit substantial performance improvements across most benchmarks. The 2B
model achieves gains of 98.7% on video tasks, 71.2% on image tasks, and 0.2% on
text tasks. The 7B model shows improvements of 37.09% on video and 11.2% on
image tasks, with a slight degradation of 2.7% on text tasks compared to their
respective base models. To address the lack of standardized biomedical video
evaluation datasets, we also introduce two new expert curated benchmarks,
MIMICEchoQA and SurgeryVideoQA. On these benchmarks, the 2B model achieves
gains of 99.1% and 98.1%, while the 7B model shows gains of 22.5% and 52.1%,
respectively, demonstrating the models' ability to generalize and perform
biomedical video understanding on cleaner and more standardized datasets than
those seen during training. These results suggest that educational videos
created for human learning offer a surprisingly effective training signal for
biomedical VLMs.

</details>

### [323] [Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models](https://arxiv.org/abs/2504.14395)
*Chung-En,Yu,Hsuan-Chih,Chen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.CV

TLDR: 论文提出了一种名为Hydra的自适应框架，通过迭代推理和跨模型验证，提升视觉语言模型（VLMs）的对抗鲁棒性和减少幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注对抗防御或幻觉后处理，缺乏统一的鲁棒性策略，而高风险的领域（如国防和医疗）需要更可靠的VLMs。

Method: Hydra采用Action-Critique循环，结合Chain-of-Thought和In-Context Learning技术，动态优化模型输出，适应对抗攻击和模型内在错误。

Result: 在四种VLMs、三个幻觉基准和两种对抗攻击策略上的评估表明，Hydra优于现有插件VLMs和去幻觉方法，无需显式对抗防御即可提升鲁棒性和事实一致性。

Conclusion: Hydra通过结合对抗抵抗和幻觉缓解，为实际应用中的VLMs提供了一种可扩展、无需训练的可靠性提升方案。

Abstract: To develop trustworthy Vision-Language Models (VLMs), it is essential to
address adversarial robustness and hallucination mitigation, both of which
impact factual accuracy in high-stakes applications such as defense and
healthcare. Existing methods primarily focus on either adversarial defense or
hallucination post-hoc correction, leaving a gap in unified robustness
strategies. We introduce \textbf{Hydra}, an adaptive agentic framework that
enhances plug-in VLMs through iterative reasoning, structured critiques, and
cross-model verification, improving both resilience to adversarial
perturbations and intrinsic model errors. Hydra employs an Action-Critique
Loop, where it retrieves and critiques visual information, leveraging
Chain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine
outputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to
both adversarial manipulations and intrinsic model errors, making it robust to
malicious perturbations and hallucination-related inaccuracies. We evaluate
Hydra on four VLMs, three hallucination benchmarks, two adversarial attack
strategies, and two adversarial defense methods, assessing performance on both
clean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs
and state-of-the-art (SOTA) dehallucination methods, even without explicit
adversarial defenses, demonstrating enhanced robustness and factual
consistency. By bridging adversarial resistance and hallucination mitigation,
Hydra provides a scalable, training-free solution for improving the reliability
of VLMs in real-world applications.

</details>

### [324] [SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video Generation via Spherical Latent Representation](https://arxiv.org/abs/2504.14396)
*Minho Park,Taewoong Kang,Jooyeol Yun,Sungwon Hwang,Jaegul Choo*

Main category: cs.CV

TLDR: SphereDiff提出了一种无需额外调优的方法，利用球形潜在表示和扩散模型生成高质量的360度全景内容。


<details>
  <summary>Details</summary>
Motivation: AR/VR应用对高质量360度全景内容的需求增加，但现有方法因ERP投影的严重失真而效果不佳。

Method: 定义了球形潜在表示，扩展了MultiDiffusion到球形空间，并提出了球形潜在采样方法和失真感知加权平均。

Result: SphereDiff在生成360度全景内容时优于现有方法，保持了高保真度。

Conclusion: SphereDiff为AR/VR应用提供了一种鲁棒的解决方案。

Abstract: The increasing demand for AR/VR applications has highlighted the need for
high-quality 360-degree panoramic content. However, generating high-quality
360-degree panoramic images and videos remains a challenging task due to the
severe distortions introduced by equirectangular projection (ERP). Existing
approaches either fine-tune pretrained diffusion models on limited ERP datasets
or attempt tuning-free methods that still rely on ERP latent representations,
leading to discontinuities near the poles. In this paper, we introduce
SphereDiff, a novel approach for seamless 360-degree panoramic image and video
generation using state-of-the-art diffusion models without additional tuning.
We define a spherical latent representation that ensures uniform distribution
across all perspectives, mitigating the distortions inherent in ERP. We extend
MultiDiffusion to spherical latent space and propose a spherical latent
sampling method to enable direct use of pretrained diffusion models. Moreover,
we introduce distortion-aware weighted averaging to further improve the
generation quality in the projection process. Our method outperforms existing
approaches in generating 360-degree panoramic content while maintaining high
fidelity, making it a robust solution for immersive AR/VR applications. The
code is available here. https://github.com/pmh9960/SphereDiff

</details>

### [325] [Adversarial Attack for RGB-Event based Visual Object Tracking](https://arxiv.org/abs/2504.14423)
*Qiang Chen,Xiao Wang,Haowen Wang,Bo Jiang,Lin Zhu,Dawei Zhang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TLDR: 提出了一种针对RGB-Event视觉跟踪的跨模态对抗攻击算法，研究了Event体素和帧两种表示形式，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: RGB-Event流跟踪算法在对抗攻击和防御方面的研究较少，本文旨在填补这一空白。

Method: 针对Event体素和RGB帧，分别设计了对抗攻击策略，包括优化扰动和梯度引导的空间位置扰动。

Result: 实验表明，该方法在多个数据集上显著降低了跟踪器的性能。

Conclusion: 提出的跨模态对抗攻击算法有效，未来将公开源代码。

Abstract: Visual object tracking is a crucial research topic in the fields of computer
vision and multi-modal fusion. Among various approaches, robust visual tracking
that combines RGB frames with Event streams has attracted increasing attention
from researchers. While striving for high accuracy and efficiency in tracking,
it is also important to explore how to effectively conduct adversarial attacks
and defenses on RGB-Event stream tracking algorithms, yet research in this area
remains relatively scarce. To bridge this gap, in this paper, we propose a
cross-modal adversarial attack algorithm for RGB-Event visual tracking. Because
of the diverse representations of Event streams, and given that Event voxels
and frames are more commonly used, this paper will focus on these two
representations for an in-depth study. Specifically, for the RGB-Event voxel,
we first optimize the perturbation by adversarial loss to generate RGB frame
adversarial examples. For discrete Event voxel representations, we propose a
two-step attack strategy, more in detail, we first inject Event voxels into the
target region as initialized adversarial examples, then, conduct a
gradient-guided optimization by perturbing the spatial location of the Event
voxels. For the RGB-Event frame based tracking, we optimize the cross-modal
universal perturbation by integrating the gradient information from multimodal
data. We evaluate the proposed approach against attacks on three widely used
RGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive
experiments show that our method significantly reduces the performance of the
tracker across numerous datasets in both unimodal and multimodal scenarios. The
source code will be released on
https://github.com/Event-AHU/Adversarial_Attack_Defense

</details>

### [326] [ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations](https://arxiv.org/abs/2504.14429)
*Ahmad Khalil,Mahmoud Khalil,Alioune Ngom*

Main category: cs.CV

TLDR: 论文提出了一种解决视频语言模型（VideoLLMs）中幻觉问题的方法，通过两步策略提升生成内容的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）和视频语言模型（VideoLLMs）存在幻觉问题，生成的描述可能与实际内容不符，影响多模态任务的可靠性。

Method: 提出两步策略：1）使用改进的Lynx模型检测生成描述与真实视频内容的语义对齐；2）采用检索增强生成（RAG）动态构建知识库以减少幻觉。

Result: 改进后的模型ResNetVLLM-2在ActivityNet-QA基准测试中准确率从54.8%提升至65.3%。

Conclusion: 提出的幻觉检测与缓解策略显著提升了视频语言模型的可靠性。

Abstract: Large Language Models (LLMs) have transformed natural language processing
(NLP) tasks, but they suffer from hallucination, generating plausible yet
factually incorrect content. This issue extends to Video-Language Models
(VideoLLMs), where textual descriptions may inaccurately represent visual
content, resulting in multi-modal hallucinations. In this paper, we address
hallucination in ResNetVLLM, a video-language model combining ResNet visual
encoders with LLMs. We introduce a two-step protocol: (1) a faithfulness
detection strategy that uses a modified Lynx model to assess semantic alignment
between generated captions and ground-truth video references, and (2) a
hallucination mitigation strategy using Retrieval-Augmented Generation (RAG)
with an ad-hoc knowledge base dynamically constructed during inference. Our
enhanced model, ResNetVLLM-2, reduces multi-modal hallucinations by
cross-verifying generated content against external knowledge, improving factual
consistency. Evaluation on the ActivityNet-QA benchmark demonstrates a
substantial accuracy increase from 54.8% to 65.3%, highlighting the
effectiveness of our hallucination detection and mitigation strategies in
enhancing video-language model reliability.

</details>

### [327] [ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task](https://arxiv.org/abs/2504.14432)
*Ahmad Khalil,Mahmoud Khalil,Alioune Ngom*

Main category: cs.CV

TLDR: ResNetVLLM是一种新型跨模态框架，结合ResNet视觉编码器和大型语言模型，用于零样本视频理解，无需依赖预训练视频模型。


<details>
  <summary>Details</summary>
Motivation: 解决零样本视频理解中依赖预训练模型的挑战，提出一种统一架构学习视觉和语义表示。

Method: 使用非预训练的ResNet提取视觉特征，结合大型语言模型生成文本描述。

Result: 在多个基准测试（如MSRVTT-QA、MSVD-QA等）上实现零样本视频理解的先进性能。

Conclusion: ResNetVLLM通过统一架构提升了零样本视频理解的准确性和上下文相关性。

Abstract: In this paper, we introduce ResNetVLLM (ResNet Vision LLM), a novel
cross-modal framework for zero-shot video understanding that integrates a
ResNet-based visual encoder with a Large Language Model (LLM. ResNetVLLM
addresses the challenges associated with zero-shot video models by avoiding
reliance on pre-trained video understanding models and instead employing a
non-pretrained ResNet to extract visual features. This design ensures the model
learns visual and semantic representations within a unified architecture,
enhancing its ability to generate accurate and contextually relevant textual
descriptions from video inputs. Our experimental results demonstrate that
ResNetVLLM achieves state-of-the-art performance in zero-shot video
understanding (ZSVU) on several benchmarks, including MSRVTT-QA, MSVD-QA,
TGIF-QA FrameQA, and ActivityNet-QA.

</details>

### [328] [WT-BCP: Wavelet Transform based Bidirectional Copy-Paste for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2504.14445)
*Mingya Zhang,Liang Wang,Limei Gu,Tingsheng Ling,Xianping Tao*

Main category: cs.CV

TLDR: 论文提出了一种基于小波变换的双向复制粘贴半监督医学图像分割框架（WT-BCP），通过结合低频和高频信息以及多输出一致性训练，解决了现有方法的分布不匹配和训练偏差问题。


<details>
  <summary>Details</summary>
Motivation: 半监督医学图像分割（SSMIS）面临标记数据稀缺、分布不匹配、人工扰动导致的训练偏差以及低频和高频信息利用不足的挑战。

Method: 提出WT-BCP框架，结合小波变换提取低频和高频信息，并设计XNet-Plus模型进行多输入多输出处理。通过双向复制粘贴和一致性训练减少偏差。

Result: 在2D和3D数据集上的实验验证了WT-BCP的有效性。

Conclusion: WT-BCP通过改进低频和高频信息的利用以及一致性训练，显著提升了半监督医学图像分割的性能。

Abstract: Semi-supervised medical image segmentation (SSMIS) shows promise in reducing
reliance on scarce labeled medical data. However, SSMIS field confronts
challenges such as distribution mismatches between labeled and unlabeled data,
artificial perturbations causing training biases, and inadequate use of raw
image information, especially low-frequency (LF) and high-frequency (HF)
components.To address these challenges, we propose a Wavelet Transform based
Bidirectional Copy-Paste SSMIS framework, named WT-BCP, which improves upon the
Mean Teacher approach. Our method enhances unlabeled data understanding by
copying random crops between labeled and unlabeled images and employs WT to
extract LF and HF details.We propose a multi-input and multi-output model named
XNet-Plus, to receive the fused information after WT. Moreover, consistency
training among multiple outputs helps to mitigate learning biases introduced by
artificial perturbations. During consistency training, the mixed images
resulting from WT are fed into both models, with the student model's output
being supervised by pseudo-labels and ground-truth. Extensive experiments
conducted on 2D and 3D datasets confirm the effectiveness of our model.Code:
https://github.com/simzhangbest/WT-BCP.

</details>

### [329] [Neglected Risks: The Disturbing Reality of Children's Images in Datasets and the Urgent Call for Accountability](https://arxiv.org/abs/2504.14446)
*Carlos Caetano,Gabriel O. dos Santos,Caio Petrucci,Artur Barros,Camila Laranjeira,Leo S. F. Ribeiro,Júlia F. de Mendonça,Jefersson A. dos Santos,Sandra Avila*

Main category: cs.CV

TLDR: 论文探讨了在AI数据集中使用儿童图像的伦理问题，并提出了一种检测和移除这些图像的流程。


<details>
  <summary>Details</summary>
Motivation: 儿童图像的使用涉及隐私、同意和数据保护等伦理问题，现有解决方案有限。

Method: 提出了一种基于视觉语言模型的流程，并在#PraCegoVer数据集和Open Images V7子集上测试其有效性。

Result: 流程可作为未来研究的基线，呼吁社区反思并采取行动保护儿童权利。

Conclusion: 研究鼓励在创建数据集时更加谨慎，并开发工具保护弱势群体权利。

Abstract: Including children's images in datasets has raised ethical concerns,
particularly regarding privacy, consent, data protection, and accountability.
These datasets, often built by scraping publicly available images from the
Internet, can expose children to risks such as exploitation, profiling, and
tracking. Despite the growing recognition of these issues, approaches for
addressing them remain limited. We explore the ethical implications of using
children's images in AI datasets and propose a pipeline to detect and remove
such images. As a use case, we built the pipeline on a Vision-Language Model
under the Visual Question Answering task and tested it on the #PraCegoVer
dataset. We also evaluate the pipeline on a subset of 100,000 images from the
Open Images V7 dataset to assess its effectiveness in detecting and removing
images of children. The pipeline serves as a baseline for future research,
providing a starting point for more comprehensive tools and methodologies.
While we leverage existing models trained on potentially problematic data, our
goal is to expose and address this issue. We do not advocate for training or
deploying such models, but instead call for urgent community reflection and
action to protect children's rights. Ultimately, we aim to encourage the
research community to exercise - more than an additional - care in creating new
datasets and to inspire the development of tools to protect the fundamental
rights of vulnerable groups, particularly children.

</details>

### [330] [Causal Disentanglement for Robust Long-tail Medical Image Generation](https://arxiv.org/abs/2504.14450)
*Weizhi Nie,Zichun Zhang,Weijie Wang,Bruno Lepri,Anan Liu,Nicu Seb*

Main category: cs.CV

TLDR: 提出一种基于因果解耦和文本引导的医学图像生成框架，解决数据稀缺和图像多样性问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像数据稀缺且类别分布不均衡，生成高质量多样图像具有挑战性。需利用有限数据中的解剖结构信息，避免失真或不一致。

Method: 通过因果解耦实现特征分离，引入分组监督确保病理和身份特征独立；利用扩散模型和文本引导建模病理特征；优化初始噪声提升长尾类别性能。

Result: 生成具有临床相关性和结构稳定性的多样化反事实医学图像。

Conclusion: 该框架提升了生成数据的临床相关性和模型可解释性，适用于医学图像生成任务。

Abstract: Counterfactual medical image generation effectively addresses data scarcity
and enhances the interpretability of medical images. However, due to the
complex and diverse pathological features of medical images and the imbalanced
class distribution in medical data, generating high-quality and diverse medical
images from limited data is significantly challenging. Additionally, to fully
leverage the information in limited data, such as anatomical structure
information and generate more structurally stable medical images while avoiding
distortion or inconsistency. In this paper, in order to enhance the clinical
relevance of generated data and improve the interpretability of the model, we
propose a novel medical image generation framework, which generates independent
pathological and structural features based on causal disentanglement and
utilizes text-guided modeling of pathological features to regulate the
generation of counterfactual images. First, we achieve feature separation
through causal disentanglement and analyze the interactions between features.
Here, we introduce group supervision to ensure the independence of pathological
and identity features. Second, we leverage a diffusion model guided by
pathological findings to model pathological features, enabling the generation
of diverse counterfactual images. Meanwhile, we enhance accuracy by leveraging
a large language model to extract lesion severity and location from medical
reports. Additionally, we improve the performance of the latent diffusion model
on long-tailed categories through initial noise optimization.

</details>

### [331] [Metamon-GS: Enhancing Representability with Variance-Guided Densification and Light Encoding](https://arxiv.org/abs/2504.14460)
*Junyan Su,Baozhu Zhao,Xiaohan Zhang,Qi Liu*

Main category: cs.CV

TLDR: 论文提出Metamon-GS方法，通过方差引导的密集化策略和多级哈希网格解决3D高斯泼溅（3DGS）中颜色表示和密集化不足的问题，显著提升渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS在渲染性能提升上仍面临挑战，如颜色在不同视角和光照条件下的准确表示问题，以及缺乏有效的密集化策略导致模糊和伪影。

Method: 提出方差引导的密集化策略，针对高梯度方差的高斯点进行补偿，并采用多级哈希网格研究全局光照条件以准确解析颜色。

Result: 在公开数据集上的实验表明，Metamon-GS优于基线模型和先前版本，实现了更高质量的视角合成。

Conclusion: Metamon-GS通过创新的密集化和颜色解析方法，显著提升了3DGS的渲染性能。

Abstract: The introduction of 3D Gaussian Splatting (3DGS) has advanced novel view
synthesis by utilizing Gaussians to represent scenes. Encoding Gaussian point
features with anchor embeddings has significantly enhanced the performance of
newer 3DGS variants. While significant advances have been made, it is still
challenging to boost rendering performance. Feature embeddings have difficulty
accurately representing colors from different perspectives under varying
lighting conditions, which leads to a washed-out appearance. Another reason is
the lack of a proper densification strategy that prevents Gaussian point growth
in thinly initialized areas, resulting in blurriness and needle-shaped
artifacts. To address them, we propose Metamon-GS, from innovative viewpoints
of variance-guided densification strategy and multi-level hash grid. The
densification strategy guided by variance specifically targets Gaussians with
high gradient variance in pixels and compensates for the importance of regions
with extra Gaussians to improve reconstruction. The latter studies implicit
global lighting conditions and accurately interprets color from different
perspectives and feature embeddings. Our thorough experiments on publicly
available datasets show that Metamon-GS surpasses its baseline model and
previous versions, delivering superior quality in rendering novel views.

</details>

### [332] [LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image Segmentation](https://arxiv.org/abs/2504.14467)
*Jiachen Li,Qing Xie,Xiaohan Yu,Hongyun Wang,Jinyu Xu,Yongjian Liu,Yongsheng Gao*

Main category: cs.CV

TLDR: LGD框架利用多模态大语言模型生成描述，提升视觉-语言模型在零样本指代图像分割中的性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决零样本指代图像分割中因自由形式指代表达的模糊性和多样性导致的目标定位错误问题。

Method: 设计属性提示和周围提示，生成关键属性描述和周围对象描述，并引入三种视觉-文本匹配分数。

Result: 在RefCOCO、RefCOCO+和RefCOCOg数据集上取得新SOTA，oIoU和mIoU分别提升9.97%和11.29%。

Conclusion: LGD通过生成描述增强区域-文本匹配，显著提升零样本指代分割性能。

Abstract: Zero-shot referring image segmentation aims to locate and segment the target
region based on a referring expression, with the primary challenge of aligning
and matching semantics across visual and textual modalities without training.
Previous works address this challenge by utilizing Vision-Language Models and
mask proposal networks for region-text matching. However, this paradigm may
lead to incorrect target localization due to the inherent ambiguity and
diversity of free-form referring expressions. To alleviate this issue, we
present LGD (Leveraging Generative Descriptions), a framework that utilizes the
advanced language generation capabilities of Multi-Modal Large Language Models
to enhance region-text matching performance in Vision-Language Models.
Specifically, we first design two kinds of prompts, the attribute prompt and
the surrounding prompt, to guide the Multi-Modal Large Language Models in
generating descriptions related to the crucial attributes of the referent
object and the details of surrounding objects, referred to as attribute
description and surrounding description, respectively. Secondly, three
visual-text matching scores are introduced to evaluate the similarity between
instance-level visual features and textual features, which determines the mask
most associated with the referring expression. The proposed method achieves new
state-of-the-art performance on three public datasets RefCOCO, RefCOCO+ and
RefCOCOg, with maximum improvements of 9.97% in oIoU and 11.29% in mIoU
compared to previous methods.

</details>

### [333] [Turbo2K: Towards Ultra-Efficient and High-Quality 2K Video Synthesis](https://arxiv.org/abs/2504.14470)
*Jingjing Ren,Wenbo Li,Zhongdao Wang,Haoze Sun,Bangzhen Liu,Haoyu Chen,Jiaqi Xu,Aoxue Li,Shifeng Zhang,Bin Shao,Yong Guo,Lei Zhu*

Main category: cs.CV

TLDR: Turbo2K是一个高效框架，用于生成2K分辨率视频，通过压缩潜在空间和知识蒸馏显著提升训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 随着消费者对超清晰视频的需求增加，现有扩散变换器（DiTs）在2K分辨率下计算成本过高，需要更高效的解决方案。

Method: Turbo2K采用压缩潜在空间和知识蒸馏策略，并设计分层两阶段合成框架，以降低计算复杂度。

Result: Turbo2K在5秒24fps的2K视频生成中效率显著提升，推理速度比现有方法快20倍。

Conclusion: Turbo2K为高分辨率视频生成提供了可扩展且实用的解决方案。

Abstract: Demand for 2K video synthesis is rising with increasing consumer expectations
for ultra-clear visuals. While diffusion transformers (DiTs) have demonstrated
remarkable capabilities in high-quality video generation, scaling them to 2K
resolution remains computationally prohibitive due to quadratic growth in
memory and processing costs. In this work, we propose Turbo2K, an efficient and
practical framework for generating detail-rich 2K videos while significantly
improving training and inference efficiency. First, Turbo2K operates in a
highly compressed latent space, reducing computational complexity and memory
footprint, making high-resolution video synthesis feasible. However, the high
compression ratio of the VAE and limited model size impose constraints on
generative quality. To mitigate this, we introduce a knowledge distillation
strategy that enables a smaller student model to inherit the generative
capacity of a larger, more powerful teacher model. Our analysis reveals that,
despite differences in latent spaces and architectures, DiTs exhibit structural
similarities in their internal representations, facilitating effective
knowledge transfer. Second, we design a hierarchical two-stage synthesis
framework that first generates multi-level feature at lower resolutions before
guiding high-resolution video generation. This approach ensures structural
coherence and fine-grained detail refinement while eliminating redundant
encoding-decoding overhead, further enhancing computational efficiency.Turbo2K
achieves state-of-the-art efficiency, generating 5-second, 24fps, 2K videos
with significantly reduced computational cost. Compared to existing methods,
Turbo2K is up to 20$\times$ faster for inference, making high-resolution video
generation more scalable and practical for real-world applications.

</details>

### [334] [Efficient Implicit Neural Compression of Point Clouds via Learnable Activation in Latent Space](https://arxiv.org/abs/2504.14471)
*Yichi Zhang,Qianqian Yang*

Main category: cs.CV

TLDR: PICO是一种基于隐式神经表示（INR）的点云压缩框架，通过几何和属性压缩两阶段优化，结合新型网络LeAFNet，显著提升了压缩性能。


<details>
  <summary>Details</summary>
Motivation: 传统点云压缩方法效率有限，PICO旨在通过INR和新型网络架构提升压缩性能。

Method: PICO将点云压缩分为几何和属性压缩两阶段，采用LeAFNet网络（基于可学习激活函数）优化INR，并结合量化和熵编码。

Result: LeAFNet优于传统MLP，PICO在几何压缩上比MPEG标准提升4.92 dB D1 PSNR，联合压缩PCQM增益2.7×10⁻³。

Conclusion: PICO通过INR和LeAFNet显著提升了点云压缩效率，性能优于现有标准。

Abstract: Implicit Neural Representations (INRs), also known as neural fields, have
emerged as a powerful paradigm in deep learning, parameterizing continuous
spatial fields using coordinate-based neural networks. In this paper, we
propose \textbf{PICO}, an INR-based framework for static point cloud
compression. Unlike prevailing encoder-decoder paradigms, we decompose the
point cloud compression task into two separate stages: geometry compression and
attribute compression, each with distinct INR optimization objectives. Inspired
by Kolmogorov-Arnold Networks (KANs), we introduce a novel network
architecture, \textbf{LeAFNet}, which leverages learnable activation functions
in the latent space to better approximate the target signal's implicit
function. By reformulating point cloud compression as neural parameter
compression, we further improve compression efficiency through quantization and
entropy coding. Experimental results demonstrate that \textbf{LeAFNet}
outperforms conventional MLPs in INR-based point cloud compression.
Furthermore, \textbf{PICO} achieves superior geometry compression performance
compared to the current MPEG point cloud compression standard, yielding an
average improvement of $4.92$ dB in D1 PSNR. In joint geometry and attribute
compression, our approach exhibits highly competitive results, with an average
PCQM gain of $2.7 \times 10^{-3}$.

</details>

### [335] [Vision-Centric Representation-Efficient Fine-Tuning for Robust Universal Foreground Segmentation](https://arxiv.org/abs/2504.14481)
*Guoyi Zhang,Siyang Chen,Guangsheng Xu,Han Wang,Xiaohu Zhang*

Main category: cs.CV

TLDR: LSR-ST是一种轻量级PEFT框架，通过引入形状偏置的归纳先验，提升视觉基础模型在复杂场景中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型在复杂场景（如伪装和红外图像）中表现不佳，主要由于其固有的纹理偏置在微调过程中被放大，限制了泛化能力。

Method: 提出LSR-ST框架，利用HDConv Block捕获形状感知特征，满足大感受野、多阶特征交互和稀疏连接的条件。

Result: 在17个数据集和6个任务中，仅使用4.719M可训练参数，LSR-ST显著提升了性能。

Conclusion: LSR-ST通过表示效率的概念，为复杂视觉环境中的模型提供了更鲁棒和通用的解决方案。

Abstract: Foreground segmentation is crucial for scene understanding, yet
parameter-efficient fine-tuning (PEFT) of vision foundation models (VFMs) often
fails in complex scenarios, such as camouflage and infrared imagery. We
attribute this challenge to the inherent texture bias in VFMs, which is
exacerbated during fine-tuning and limits generalization in texture-sparse
environments. To address this, we propose Ladder Shape-bias Representation
Side-tuning (LSR-ST), a lightweight PEFT framework that enhances model
robustness by introducing shape-biased inductive priors. LSR-ST captures
shape-aware features using a simple HDConv Block, which integrates large-kernel
attention and residual learning. The method satisfies three key conditions for
inducing shape bias: large receptive fields, multi-order feature interactions,
and sparse connectivity. Our analysis reveals that these improvements stem from
representation efficiency-the ability to extract task-relevant, structurally
grounded features while minimizing redundancy. We formalize this concept via
Information Bottleneck theory and advocate for it as a key PEFT objective.
Unlike traditional NLP paradigms that focus on optimizing parameters and
memory, visual tasks require models that extract task-defined semantics, rather
than just relying on pre-encoded features. This shift enables our approach to
move beyond conventional trade-offs, offering more robust and generalizable
solutions for vision tasks. With minimal changes to SAM2-UNet, LSR-ST achieves
consistent improvements across 17 datasets and 6 tasks using only 4.719M
trainable parameters. These results highlight the potential of representation
efficiency for robust and adaptable VFMs within complex visual environments.

</details>

### [336] [STARS: Sparse Learning Correlation Filter with Spatio-temporal Regularization and Super-resolution Reconstruction for Thermal Infrared Target Tracking](https://arxiv.org/abs/2504.14491)
*Shang Zhang,Xiaobo Ding,Huanbin Zhang,Ruoyan Xiong,Yue Zhang*

Main category: cs.CV

TLDR: STARS是一种基于稀疏学习的相关滤波跟踪器，结合时空正则化和超分辨率重建，显著提升了热红外目标跟踪的性能。


<details>
  <summary>Details</summary>
Motivation: 热红外图像分辨率低且易受干扰，限制了跟踪器的性能。

Method: 采用自适应稀疏滤波和时域滤波提取目标特征，引入边缘保持稀疏正则化稳定特征，并提出梯度增强超分辨率方法提升图像分辨率。

Result: 在多个基准测试中，STARS表现优于现有最优跟踪器。

Conclusion: STARS首次将超分辨率方法融入稀疏学习框架，显著提升了热红外目标跟踪的鲁棒性。

Abstract: Thermal infrared (TIR) target tracking methods often adopt the correlation
filter (CF) framework due to its computational efficiency. However, the low
resolution of TIR images, along with tracking interference, significantly
limits the perfor-mance of TIR trackers. To address these challenges, we
introduce STARS, a novel sparse learning-based CF tracker that incorporates
spatio-temporal regulari-zation and super-resolution reconstruction. First, we
apply adaptive sparse filter-ing and temporal domain filtering to extract key
features of the target while reduc-ing interference from background clutter and
noise. Next, we introduce an edge-preserving sparse regularization method to
stabilize target features and prevent excessive blurring. This regularization
integrates multiple terms and employs the alternating direction method of
multipliers to optimize the solution. Finally, we propose a gradient-enhanced
super-resolution method to extract fine-grained TIR target features and improve
the resolution of TIR images, addressing performance degradation in tracking
caused by low-resolution sequences. To the best of our knowledge, STARS is the
first to integrate super-resolution methods within a sparse learning-based CF
framework. Extensive experiments on the LSOTB-TIR, PTB-TIR, VOT-TIR2015, and
VOT-TIR2017 benchmarks demonstrate that STARS outperforms state-of-the-art
trackers in terms of robustness.

</details>

### [337] [DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning](https://arxiv.org/abs/2504.14509)
*Fulong Ye,Miao Hua,Pengze Zhang,Xinghui Li,Qichao Sun,Songtao Zhao,Qian He,Xinglong Wu*

Main category: cs.CV

TLDR: DreamID是一种基于扩散模型的人脸交换方法，通过显式监督和Triplet ID Group数据提升身份相似性和属性保留，结合SD Turbo加速模型实现高效训练，并在复杂场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统人脸交换方法依赖隐式监督，效果不佳，DreamID通过显式监督和高效架构解决这一问题。

Method: 使用Triplet ID Group数据建立显式监督，结合SD Turbo加速模型，提出SwapNet、FaceNet和ID Adapter的架构。

Result: 在身份相似性、姿态和表情保留、图像保真度上优于现有方法，512*512分辨率下仅需0.6秒。

Conclusion: DreamID在高效性和质量上均表现优异，适用于复杂场景。

Abstract: In this paper, we introduce DreamID, a diffusion-based face swapping model
that achieves high levels of ID similarity, attribute preservation, image
fidelity, and fast inference speed. Unlike the typical face swapping training
process, which often relies on implicit supervision and struggles to achieve
satisfactory results. DreamID establishes explicit supervision for face
swapping by constructing Triplet ID Group data, significantly enhancing
identity similarity and attribute preservation. The iterative nature of
diffusion models poses challenges for utilizing efficient image-space loss
functions, as performing time-consuming multi-step sampling to obtain the
generated image during training is impractical. To address this issue, we
leverage the accelerated diffusion model SD Turbo, reducing the inference steps
to a single iteration, enabling efficient pixel-level end-to-end training with
explicit Triplet ID Group supervision. Additionally, we propose an improved
diffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.
This robust architecture fully unlocks the power of the Triplet ID Group
explicit supervision. Finally, to further extend our method, we explicitly
modify the Triplet ID Group data during training to fine-tune and preserve
specific attributes, such as glasses and face shape. Extensive experiments
demonstrate that DreamID outperforms state-of-the-art methods in terms of
identity similarity, pose and expression preservation, and image fidelity.
Overall, DreamID achieves high-quality face swapping results at 512*512
resolution in just 0.6 seconds and performs exceptionally well in challenging
scenarios such as complex lighting, large angles, and occlusions.

</details>

### [338] [Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction](https://arxiv.org/abs/2504.14516)
*Weirong Chen,Ganlin Zhang,Felix Wimbauer,Rui Wang,Nikita Araslanov,Andrea Vedaldi,Daniel Cremers*

Main category: cs.CV

TLDR: BA-Track利用3D点追踪器分离相机运动与动态物体运动，结合传统SLAM的束调整与轻量级后处理，显著提升动态场景下的相机位姿估计和3D重建精度。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM系统依赖静态环境假设，难以处理动态场景。现有方法要么过滤动态元素，要么独立建模其运动，但效果不佳。

Method: 提出BA-Track框架，通过3D点追踪器分解运动，结合束调整和深度一致性后处理，统一处理静态与动态元素。

Result: 实验表明，BA-Track在相机位姿估计和3D重建精度上显著优于现有方法。

Conclusion: BA-Track通过运动分解和统一框架，有效解决了动态场景下的SLAM问题。

Abstract: Traditional SLAM systems, which rely on bundle adjustment, struggle with
highly dynamic scenes commonly found in casual videos. Such videos entangle the
motion of dynamic elements, undermining the assumption of static environments
required by traditional systems. Existing techniques either filter out dynamic
elements or model their motion independently. However, the former often results
in incomplete reconstructions, whereas the latter can lead to inconsistent
motion estimates. Taking a novel approach, this work leverages a 3D point
tracker to separate the camera-induced motion from the observed motion of
dynamic objects. By considering only the camera-induced component, bundle
adjustment can operate reliably on all scene elements as a result. We further
ensure depth consistency across video frames with lightweight post-processing
based on scale maps. Our framework combines the core of traditional SLAM --
bundle adjustment -- with a robust learning-based 3D tracker front-end.
Integrating motion decomposition, bundle adjustment and depth refinement, our
unified framework, BA-Track, accurately tracks the camera motion and produces
temporally coherent and scale-consistent dense reconstructions, accommodating
both static and dynamic elements. Our experiments on challenging datasets
reveal significant improvements in camera pose estimation and 3D reconstruction
accuracy.

</details>

### [339] [Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding](https://arxiv.org/abs/2504.14526)
*Tong Zeng,Longfeng Wu,Liang Shi,Dawei Zhou,Feng Guo*

Main category: cs.CV

TLDR: DVBench是一个新基准，用于评估视觉大语言模型（VLLMs）在安全关键驾驶场景中的表现，揭示了现有模型的局限性，并通过微调展示了改进潜力。


<details>
  <summary>Details</summary>
Motivation: 现有VLLMs在通用视觉任务中表现优异，但在安全关键领域（如自动驾驶）的效果尚未充分探索。

Method: 提出DVBench基准，包含10,000个人工标注的多选题，基于分层能力分类法，评估14种VLLMs在复杂驾驶场景中的表现。

Result: 现有VLLMs在DVBench上表现不佳（最高准确率<40%），但微调后性能显著提升（相对改进达43.59%）。

Conclusion: DVBench为开发适用于自动驾驶的VLLMs提供了关键评估框架，强调了领域适应的重要性。

Abstract: Vision Large Language Models (VLLMs) have demonstrated impressive
capabilities in general visual tasks such as image captioning and visual
question answering. However, their effectiveness in specialized,
safety-critical domains like autonomous driving remains largely unexplored.
Autonomous driving systems require sophisticated scene understanding in complex
environments, yet existing multimodal benchmarks primarily focus on normal
driving conditions, failing to adequately assess VLLMs' performance in
safety-critical scenarios. To address this, we introduce DVBench, a pioneering
benchmark designed to evaluate the performance of VLLMs in understanding
safety-critical driving videos. Built around a hierarchical ability taxonomy
that aligns with widely adopted frameworks for describing driving scenarios
used in assessing highly automated driving systems, DVBench features 10,000
multiple-choice questions with human-annotated ground-truth answers, enabling a
comprehensive evaluation of VLLMs' capabilities in perception and reasoning.
Experiments on 14 SOTA VLLMs, ranging from 0.5B to 72B parameters, reveal
significant performance gaps, with no model achieving over 40% accuracy,
highlighting critical limitations in understanding complex driving scenarios.
To probe adaptability, we fine-tuned selected models using domain-specific data
from DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage
points, with relative improvements of up to 43.59%. This improvement
underscores the necessity of targeted adaptation to bridge the gap between
general-purpose VLLMs and mission-critical driving applications. DVBench
establishes an essential evaluation framework and research roadmap for
developing VLLMs that meet the safety and robustness requirements for
real-world autonomous systems. We released the benchmark toolbox and the
fine-tuned model at: https://github.com/tong-zeng/DVBench.git.

</details>

### [340] [SUDO: Enhancing Text-to-Image Diffusion Models with Self-Supervised Direct Preference Optimization](https://arxiv.org/abs/2504.14534)
*Liang Peng,Boxi Wu,Haoran Cheng,Yibo Zhao,Xiaofei He*

Main category: cs.CV

TLDR: 本文提出了一种名为SUDO的自监督直接偏好优化方法，用于改进文本到图像扩散模型，同时优化像素级细节和全局图像质量。


<details>
  <summary>Details</summary>
Motivation: 传统的监督微调方法主要关注像素级的均方误差损失，忽略了全局图像质量的优化，导致感知质量和结构连贯性不足。

Method: SUDO通过自监督生成偏好图像对，结合直接偏好优化，实现全局和像素级的联合优化。

Result: 实验表明，SUDO显著提升了图像质量，适用于Stable Diffusion等模型，且无需昂贵的数据标注。

Conclusion: SUDO是一种高效且无需额外标注的优化方法，可广泛应用于文本到图像扩散模型。

Abstract: Previous text-to-image diffusion models typically employ supervised
fine-tuning (SFT) to enhance pre-trained base models. However, this approach
primarily minimizes the loss of mean squared error (MSE) at the pixel level,
neglecting the need for global optimization at the image level, which is
crucial for achieving high perceptual quality and structural coherence. In this
paper, we introduce Self-sUpervised Direct preference Optimization (SUDO), a
novel paradigm that optimizes both fine-grained details at the pixel level and
global image quality. By integrating direct preference optimization into the
model, SUDO generates preference image pairs in a self-supervised manner,
enabling the model to prioritize global-level learning while complementing the
pixel-level MSE loss. As an effective alternative to supervised fine-tuning,
SUDO can be seamlessly applied to any text-to-image diffusion model.
Importantly, it eliminates the need for costly data collection and annotation
efforts typically associated with traditional direct preference optimization
methods. Through extensive experiments on widely-used models, including Stable
Diffusion 1.5 and XL, we demonstrate that SUDO significantly enhances both
global and local image quality. The codes are provided at
\href{https://github.com/SPengLiang/SUDO}{this link}.

</details>

### [341] [FlowLoss: Dynamic Flow-Conditioned Loss Strategy for Video Diffusion Models](https://arxiv.org/abs/2504.14535)
*Kuanting Wu,Kei Ota,Asako Kanezaki*

Main category: cs.CV

TLDR: FlowLoss通过直接比较生成视频与真实视频的光流场，结合噪声感知权重方案，提升了视频扩散模型的运动稳定性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型（VDMs）在生成高质量视频时，常面临时间相干性不足的问题，光流监督是一种潜在解决方案。

Method: 提出FlowLoss方法，直接比较生成与真实视频的光流场，并引入噪声感知权重方案，调整不同去噪步骤的损失权重。

Result: 在机器人视频数据集上的实验表明，FlowLoss提高了运动稳定性，并加速了早期训练阶段的收敛。

Conclusion: FlowLoss为噪声条件生成模型中引入运动监督提供了实用见解。

Abstract: Video Diffusion Models (VDMs) can generate high-quality videos, but often
struggle with producing temporally coherent motion. Optical flow supervision is
a promising approach to address this, with prior works commonly employing
warping-based strategies that avoid explicit flow matching. In this work, we
explore an alternative formulation, FlowLoss, which directly compares flow
fields extracted from generated and ground-truth videos. To account for the
unreliability of flow estimation under high-noise conditions in diffusion, we
propose a noise-aware weighting scheme that modulates the flow loss across
denoising steps. Experiments on robotic video datasets suggest that FlowLoss
improves motion stability and accelerates convergence in early training stages.
Our findings offer practical insights for incorporating motion-based
supervision into noise-conditioned generative models.

</details>

### [342] [VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control](https://arxiv.org/abs/2504.14548)
*Lifeng Lin,Rongfeng Lu,Quan Chen,Haofan Ren,Ming Lu,Yaoqi Sun,Chenggang Yan,Anke Xue*

Main category: cs.CV

TLDR: VGNC是一种基于生成新颖视图合成模型的验证引导高斯数控制方法，用于减少稀疏视图3D高斯泼溅（3DGS）中的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图3D重建在实际应用中具有挑战性，现有3DGS方法存在显著的过拟合问题。

Method: VGNC通过生成验证图像并基于这些图像控制高斯数，以减少过拟合。

Result: 实验表明，VGNC不仅减少了过拟合，还提高了测试集的渲染质量，同时减少了高斯点数量。

Conclusion: VGNC是一种有效的解决方案，能够降低存储需求并加速训练和渲染。

Abstract: Sparse-view 3D reconstruction is a fundamental yet challenging task in
practical 3D reconstruction applications. Recently, many methods based on the
3D Gaussian Splatting (3DGS) framework have been proposed to address
sparse-view 3D reconstruction. Although these methods have made considerable
advancements, they still show significant issues with overfitting. To reduce
the overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number
Control (VGNC) approach based on generative novel view synthesis (NVS) models.
To the best of our knowledge, this is the first attempt to alleviate the
overfitting issue of sparse-view 3DGS with generative validation images.
Specifically, we first introduce a validation image generation method based on
a generative NVS model. We then propose a Gaussian number control strategy that
utilizes generated validation images to determine the optimal Gaussian numbers,
thereby reducing the issue of overfitting. We conducted detailed experiments on
various sparse-view 3DGS baselines and datasets to evaluate the effectiveness
of VGNC. Extensive experiments show that our approach not only reduces
overfitting but also improves rendering quality on the test set while
decreasing the number of Gaussian points. This reduction lowers storage demands
and accelerates both training and rendering. The code will be released.

</details>

### [343] [Grounding-MD: Grounded Video-language Pre-training for Open-World Moment Detection](https://arxiv.org/abs/2504.14553)
*Weijun Zhuang,Qizhang Li,Xin Li,Ming Liu,Xiaopeng Hong,Feng Gao,Fan Yang,Wangmeng Zuo*

Main category: cs.CV

TLDR: Grounding-MD是一个创新的视频-语言预训练框架，用于开放世界时刻检测，通过结构化提示机制和跨模态融合编码器，实现了灵活且可扩展的时刻检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于封闭场景，无法适应开放世界的需求，因此提出了Grounding-MD来解决这一问题。

Method: 采用结构化提示机制、跨模态融合编码器和文本引导融合解码器，通过大规模预训练实现视频-文本对齐。

Result: 在四个基准数据集上实现了零样本和监督设置下的最先进性能。

Conclusion: Grounding-MD在开放世界时刻检测中表现出色，具有广泛的应用潜力。

Abstract: Temporal Action Detection and Moment Retrieval constitute two pivotal tasks
in video understanding, focusing on precisely localizing temporal segments
corresponding to specific actions or events. Recent advancements introduced
Moment Detection to unify these two tasks, yet existing approaches remain
confined to closed-set scenarios, limiting their applicability in open-world
contexts. To bridge this gap, we present Grounding-MD, an innovative, grounded
video-language pre-training framework tailored for open-world moment detection.
Our framework incorporates an arbitrary number of open-ended natural language
queries through a structured prompt mechanism, enabling flexible and scalable
moment detection. Grounding-MD leverages a Cross-Modality Fusion Encoder and a
Text-Guided Fusion Decoder to facilitate comprehensive video-text alignment and
enable effective cross-task collaboration. Through large-scale pre-training on
temporal action detection and moment retrieval datasets, Grounding-MD
demonstrates exceptional semantic representation learning capabilities,
effectively handling diverse and complex query conditions. Comprehensive
evaluations across four benchmark datasets including ActivityNet, THUMOS14,
ActivityNet-Captions, and Charades-STA demonstrate that Grounding-MD
establishes new state-of-the-art performance in zero-shot and supervised
settings in open-world moment detection scenarios. All source code and trained
models will be released.

</details>

### [344] [SMTT: Novel Structured Multi-task Tracking with Graph-Regularized Sparse Representation for Robust Thermal Infrared Target Tracking](https://arxiv.org/abs/2504.14566)
*Shang Zhang,HuiPan Guan,XiaoBo Ding,Ruoyan Xiong,Yue Zhang*

Main category: cs.CV

TLDR: SMTT是一种新型热红外目标跟踪器，通过多任务学习、联合稀疏表示和自适应图正则化解决噪声、遮挡和快速目标运动等问题，表现出高精度和实时性能。


<details>
  <summary>Details</summary>
Motivation: 热红外目标跟踪在监控、自动驾驶和军事行动中至关重要，但面临噪声、遮挡和快速运动等挑战。

Method: 采用多任务学习框架，结合联合稀疏表示和自适应图正则化，利用加速近端梯度法优化。

Result: 在VOT-TIR、PTB-TIR和LSOTB-TIR数据集上验证了SMTT的高精度、鲁棒性和计算效率。

Conclusion: SMTT是复杂环境中热红外目标跟踪的可靠高性能解决方案。

Abstract: Thermal infrared target tracking is crucial in applications such as
surveillance, autonomous driving, and military operations. In this paper, we
propose a novel tracker, SMTT, which effectively addresses common challenges in
thermal infrared imagery, such as noise, occlusion, and rapid target motion, by
leveraging multi-task learning, joint sparse representation, and adaptive graph
regularization. By reformulating the tracking task as a multi-task learning
problem, the SMTT tracker independently optimizes the representation of each
particle while dynamically capturing spatial and feature-level similarities
using a weighted mixed-norm regularization strategy. To ensure real-time
performance, we incorporate the Accelerated Proximal Gradient method for
efficient optimization. Extensive experiments on benchmark datasets - including
VOT-TIR, PTB-TIR, and LSOTB-TIR - demonstrate that SMTT achieves superior
accuracy, robustness, and computational efficiency. These results highlight
SMTT as a reliable and high-performance solution for thermal infrared target
tracking in complex environments.

</details>

### [345] [NTIRE 2025 Challenge on Image Super-Resolution ($\times$4): Methods and Results](https://arxiv.org/abs/2504.14582)
*Zheng Chen,Kai Liu,Jue Gong,Jingkai Wang,Lei Sun,Zongwei Wu,Radu Timofte,Yulun Zhang,Xiangyu Kong,Xiaoxuan Yu,Hyunhee Park,Suejin Han,Hakjae Jeon,Dafeng Zhang,Hyung-Ju Chun,Donghun Ryou,Inju Ha,Bohyung Han,Lu Zhao,Yuyi Zhang,Pengyu Yan,Jiawei Hu,Pengwei Liu,Fengjun Guo,Hongyuan Yu,Pufan Xu,Zhijuan Huang,Shuyuan Cui,Peng Guo,Jiahui Liu,Dongkai Zhang,Heng Zhang,Huiyuan Fu,Huadong Ma,Yanhui Guo,Sisi Tian,Xin Liu,Jinwen Liang,Jie Liu,Jie Tang,Gangshan Wu,Zeyu Xiao,Zhuoyuan Li,Yinxiang Zhang,Wenxuan Cai,Vijayalaxmi Ashok Aralikatti,Nikhil Akalwadi,G Gyaneshwar Rao,Chaitra Desai,Ramesh Ashok Tabib,Uma Mudenagudi,Marcos V. Conde,Alejandro Merino,Bruno Longarela,Javier Abad,Weijun Yuan,Zhan Li,Zhanglu Chen,Boyang Yao,Aagam Jain,Milan Kumar Singh,Ankit Kumar,Shubh Kawa,Divyavardhan Singh,Anjali Sarvaiya,Kishor Upla,Raghavendra Ramachandra,Chia-Ming Lee,Yu-Fan Lin,Chih-Chung Hsu,Risheek V Hiremath,Yashaswini Palani,Yuxuan Jiang,Qiang Zhu,Siyue Teng,Fan Zhang,Shuyuan Zhu,Bing Zeng,David Bull,Jingwei Liao,Yuqing Yang,Wenda Shao,Junyi Zhao,Qisheng Xu,Kele Xu,Sunder Ali Khowaja,Ik Hyun Lee,Snehal Singh Tomar,Rajarshi Ray,Klaus Mueller,Sachin Chaudhary,Surya Vashisth,Akshay Dudhane,Praful Hambarde,Satya Naryan Tazi,Prashant Patil,Santosh Kumar Vipparthi,Subrahmanyam Murala,Bilel Benjdira,Anas M. Ali,Wadii Boulila,Zahra Moammeri,Ahmad Mahmoudi-Aznaveh,Ali Karbasi,Hossein Motamednia,Liangyan Li,Guanhua Zhao,Kevin Le,Yimo Ning,Haoxuan Huang,Jun Chen*

Main category: cs.CV

TLDR: NTIRE 2025图像超分辨率挑战赛旨在通过开发高效网络设计或解决方案，从低分辨率图像中恢复高分辨率图像，分为恢复和感知两个子赛道。


<details>
  <summary>Details</summary>
Motivation: 推动图像超分辨率技术的发展，通过竞赛形式促进创新和性能提升。

Method: 挑战赛采用双赛道设计：恢复赛道关注像素级精度（PSNR排名），感知赛道关注视觉真实性（感知评分排名）。

Result: 286名参与者注册，25支团队提交有效方案，挑战赛成为图像超分辨率领域的基准。

Conclusion: NTIRE 2025挑战赛为图像超分辨率研究提供了重要平台，推动了该领域的技术进步。

Abstract: This paper presents the NTIRE 2025 image super-resolution ($\times$4)
challenge, one of the associated competitions of the 10th NTIRE Workshop at
CVPR 2025. The challenge aims to recover high-resolution (HR) images from
low-resolution (LR) counterparts generated through bicubic downsampling with a
$\times$4 scaling factor. The objective is to develop effective network designs
or solutions that achieve state-of-the-art SR performance. To reflect the dual
objectives of image SR research, the challenge includes two sub-tracks: (1) a
restoration track, emphasizes pixel-wise accuracy and ranks submissions based
on PSNR; (2) a perceptual track, focuses on visual realism and ranks results by
a perceptual score. A total of 286 participants registered for the competition,
with 25 teams submitting valid entries. This report summarizes the challenge
design, datasets, evaluation protocol, the main results, and methods of each
team. The challenge serves as a benchmark to advance the state of the art and
foster progress in image SR.

</details>

### [346] [Using street view imagery and deep generative modeling for estimating the health of urban forests](https://arxiv.org/abs/2504.14583)
*Akshit Gupta,Remko Uijlenhoet*

Main category: cs.CV

TLDR: 提出了一种基于街景图像、树木清单数据和气象条件的城市森林健康监测方法，利用图像到图像转换网络估算NDVI和CTD参数，并与地面实测数据对比验证。


<details>
  <summary>Details</summary>
Motivation: 传统城市森林健康监测方法依赖人工和高成本设备，难以规模化；现有遥感技术也存在部署和分辨率限制。

Method: 使用街景图像、树木清单数据和气象条件，通过图像到图像转换网络估算NDVI和CTD参数。

Result: 通过与地面多光谱和热成像传感器实测数据对比验证方法的有效性。

Conclusion: 该方法利用现有街景平台（如Google街景）数据，为城市管理者提供了一种可扩展的城市森林健康监测解决方案。

Abstract: Healthy urban forests comprising of diverse trees and shrubs play a crucial
role in mitigating climate change. They provide several key advantages such as
providing shade for energy conservation, and intercepting rainfall to reduce
flood runoff and soil erosion. Traditional approaches for monitoring the health
of urban forests require instrumented inspection techniques, often involving a
high amount of human labor and subjective evaluations. As a result, they are
not scalable for cities which lack extensive resources. Recent approaches
involving multi-spectral imaging data based on terrestrial sensing and
satellites, are constrained respectively with challenges related to dedicated
deployments and limited spatial resolutions. In this work, we propose an
alternative approach for monitoring the urban forests using simplified inputs:
street view imagery, tree inventory data and meteorological conditions. We
propose to use image-to-image translation networks to estimate two urban forest
health parameters, namely, NDVI and CTD. Finally, we aim to compare the
generated results with ground truth data using an onsite campaign utilizing
handheld multi-spectral and thermal imaging sensors. With the advent and
expansion of street view imagery platforms such as Google Street View and
Mapillary, this approach should enable effective management of urban forests
for the authorities in cities at scale.

</details>

### [347] [NTIRE 2025 Challenge on Real-World Face Restoration: Methods and Results](https://arxiv.org/abs/2504.14600)
*Zheng Chen,Jingkai Wang,Kai Liu,Jue Gong,Lei Sun,Zongwei Wu,Radu Timofte,Yulun Zhang,Jianxing Zhang,Jinlong Wu,Jun Wang,Zheng Xie,Hakjae Jeon,Suejin Han,Hyung-Ju Chun,Hyunhee Park,Zhicun Yin,Junjie Chen,Ming Liu,Xiaoming Li,Chao Zhou,Wangmeng Zuo,Weixia Zhang,Dingquan Li,Kede Ma,Yun Zhang,Zhuofan Zheng,Yuyue Liu,Shizhen Tang,Zihao Zhang,Yi Ning,Hao Jiang,Wenjie An,Kangmeng Yu,Chenyang Wang,Kui Jiang,Xianming Liu,Junjun Jiang,Yingfu Zhang,Gang He,Siqi Wang,Kepeng Xu,Zhenyang Liu,Changxin Zhou,Shanlan Shen,Yubo Duan,Yiang Chen,Jin Guo,Mengru Yang,Jen-Wei Lee,Chia-Ming Lee,Chih-Chung Hsu,Hu Peng,Chunming He*

Main category: cs.CV

TLDR: NTIRE 2025挑战赛回顾，聚焦真实人脸修复，强调自然输出与身份一致性，推动感知质量与真实感的前沿解决方案。


<details>
  <summary>Details</summary>
Motivation: 推动真实世界人脸修复的性能提升，探索感知质量与真实感的最新趋势。

Method: 使用加权图像质量评估（IQA）分数和AdaFace模型作为身份检查器，评估参赛模型。

Result: 141名注册者，13支团队提交有效模型，10支团队在最终排名中获得有效分数。

Conclusion: 挑战赛推动了真实人脸修复的性能提升，并提供了该领域最新趋势的深入概述。

Abstract: This paper provides a review of the NTIRE 2025 challenge on real-world face
restoration, highlighting the proposed solutions and the resulting outcomes.
The challenge focuses on generating natural, realistic outputs while
maintaining identity consistency. Its goal is to advance state-of-the-art
solutions for perceptual quality and realism, without imposing constraints on
computational resources or training data. The track of the challenge evaluates
performance using a weighted image quality assessment (IQA) score and employs
the AdaFace model as an identity checker. The competition attracted 141
registrants, with 13 teams submitting valid models, and ultimately, 10 teams
achieved a valid score in the final ranking. This collaborative effort advances
the performance of real-world face restoration while offering an in-depth
overview of the latest trends in the field.

</details>

### [348] [MP-Mat: A 3D-and-Instance-Aware Human Matting and Editing Framework with Multiplane Representation](https://arxiv.org/abs/2504.14606)
*Siyi Jiao,Wenzheng Zeng,Yerong Li,Huayu Zhang,Changxin Gao,Nong Sang,Mike Zheng Shou*

Main category: cs.CV

TLDR: MP-Mat是一种新型的3D感知实例抠图框架，通过多平面表示从场景几何和实例两个层面提升复杂场景下的抠图效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在复杂场景（如多实例纠缠、毛发和细边界结构）中难以准确分离像素的问题。

Method: 提出多平面表示，分为场景几何层面的深度差异分割和实例层面的抠图与颜色表示，背景也被视为特殊实例。

Result: MP-Mat在抠图任务中表现优异，并在零样本推理的图像编辑任务中超越专业方法。

Conclusion: MP-Mat通过多平面表示显著提升了复杂场景下的实例抠图能力，并展示了在图像编辑任务中的潜力。

Abstract: Human instance matting aims to estimate an alpha matte for each human
instance in an image, which is challenging as it easily fails in complex cases
requiring disentangling mingled pixels belonging to multiple instances along
hairy and thin boundary structures. In this work, we address this by
introducing MP-Mat, a novel 3D-and-instance-aware matting framework with
multiplane representation, where the multiplane concept is designed from two
different perspectives: scene geometry level and instance level. Specifically,
we first build feature-level multiplane representations to split the scene into
multiple planes based on depth differences. This approach makes the scene
representation 3D-aware, and can serve as an effective clue for splitting
instances in different 3D positions, thereby improving interpretability and
boundary handling ability especially in occlusion areas. Then, we introduce
another multiplane representation that splits the scene in an instance-level
perspective, and represents each instance with both matte and color. We also
treat background as a special instance, which is often overlooked by existing
methods. Such an instance-level representation facilitates both foreground and
background content awareness, and is useful for other down-stream tasks like
image editing. Once built, the representation can be reused to realize
controllable instance-level image editing with high efficiency. Extensive
experiments validate the clear advantage of MP-Mat in matting task. We also
demonstrate its superiority in image editing tasks, an area under-explored by
existing matting-focused methods, where our approach under zero-shot inference
even outperforms trained specialized image editing techniques by large margins.
Code is open-sourced at https://github.com/JiaoSiyi/MPMat.git}.

</details>

### [349] [VM-BHINet:Vision Mamba Bimanual Hand Interaction Network for 3D Interacting Hand Mesh Recovery From a Single RGB Image](https://arxiv.org/abs/2504.14618)
*Han Bi,Ge Yu,Yu He,Wenzhuo Liu,Zijie Zheng*

Main category: cs.CV

TLDR: VM-BHINet利用状态空间模型提升双手交互重建，显著降低误差。


<details>
  <summary>Details</summary>
Motivation: 现有方法在遮挡、模糊外观和计算效率上表现不佳，需改进双手交互建模。

Method: 提出VM-BHINet，结合状态空间模型与局部全局特征操作，增强交互理解。

Result: 在InterHand2.6M数据集上，MPJPE和MPVPE降低2-3%，优于现有方法。

Conclusion: VM-BHINet通过SSMs有效提升双手交互重建的准确性和效率。

Abstract: Understanding bimanual hand interactions is essential for realistic 3D pose
and shape reconstruction. However, existing methods struggle with occlusions,
ambiguous appearances, and computational inefficiencies. To address these
challenges, we propose Vision Mamba Bimanual Hand Interaction Network
(VM-BHINet), introducing state space models (SSMs) into hand reconstruction to
enhance interaction modeling while improving computational efficiency. The core
component, Vision Mamba Interaction Feature Extraction Block (VM-IFEBlock),
combines SSMs with local and global feature operations, enabling deep
understanding of hand interactions. Experiments on the InterHand2.6M dataset
show that VM-BHINet reduces Mean per-joint position error (MPJPE) and Mean
per-vertex position error (MPVPE) by 2-3%, significantly surpassing
state-of-the-art methods.

</details>

### [350] [Talk is Not Always Cheap: Promoting Wireless Sensing Models with Text Prompts](https://arxiv.org/abs/2504.14621)
*Zhenkui Yang,Zeyi Huang,Ge Wang,Han Ding,Tony Xiao Han,Fei Wang*

Main category: cs.CV

TLDR: WiTalk框架通过文本增强无线传感技术，显著提升了人类动作识别和定位的性能。


<details>
  <summary>Details</summary>
Motivation: 现有无线传感技术未充分利用数据集中的文本信息，限制了性能提升。

Method: 提出WiTalk框架，通过三种分层提示策略（标签、简要描述、详细动作描述）集成语义知识。

Result: 在多个基准数据集上性能显著提升，如XRF55的准确率提高3.9%（WiFi）、2.59%（RFID）和0.46%（mmWave）。

Conclusion: WiTalk框架有效整合文本信息，无需额外数据成本即可提升无线传感性能。

Abstract: Wireless signal-based human sensing technologies, such as WiFi,
millimeter-wave (mmWave) radar, and Radio Frequency Identification (RFID),
enable the detection and interpretation of human presence, posture, and
activities, thereby providing critical support for applications in public
security, healthcare, and smart environments. These technologies exhibit
notable advantages due to their non-contact operation and environmental
adaptability; however, existing systems often fail to leverage the textual
information inherent in datasets. To address this, we propose an innovative
text-enhanced wireless sensing framework, WiTalk, that seamlessly integrates
semantic knowledge through three hierarchical prompt strategies-label-only,
brief description, and detailed action description-without requiring
architectural modifications or incurring additional data costs. We rigorously
validate this framework across three public benchmark datasets: XRF55 for human
action recognition (HAR), and WiFiTAL and XRFV2 for WiFi temporal action
localization (TAL). Experimental results demonstrate significant performance
improvements: on XRF55, accuracy for WiFi, RFID, and mmWave increases by 3.9%,
2.59%, and 0.46%, respectively; on WiFiTAL, the average performance of WiFiTAD
improves by 4.98%; and on XRFV2, the mean average precision gains across
various methods range from 4.02% to 13.68%. Our codes have been included in
https://github.com/yangzhenkui/WiTalk.

</details>

### [351] [MSAD-Net: Multiscale and Spatial Attention-based Dense Network for Lung Cancer Classification](https://arxiv.org/abs/2504.14626)
*Santanu Roy,Shweta Singh,Palak Sahu,Ashvath Suresh,Debashish Das*

Main category: cs.CV

TLDR: 提出了一种新型CNN架构MSD-Net，通过改进的密集模块和多尺度特征提取，显著提升了肺癌检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 肺癌早期检测至关重要，但传统手动检测方法存在挑战，且现有CNN模型因类别不平衡问题性能受限。

Method: 设计了MSD-Net，包含改进的密集模块（DWSC层和1x1卷积层）、跳跃连接和平行分支连接，以提取多尺度特征。

Result: 实验表明，MSD-Net在性能上显著优于ConvNext-Tiny、ViT、PiT等现有模型。

Conclusion: MSD-Net通过结构优化有效解决了类别不平衡问题，为肺癌自动检测提供了高效解决方案。

Abstract: Lung cancer, a severe form of malignant tumor that originates in the tissues
of the lungs, can be fatal if not detected in its early stages. It ranks among
the top causes of cancer-related mortality worldwide. Detecting lung cancer
manually using chest X-Ray image or Computational Tomography (CT) scans image
poses significant challenges for radiologists. Hence, there is a need for
automatic diagnosis system of lung cancers from radiology images. With the
recent emergence of deep learning, particularly through Convolutional Neural
Networks (CNNs), the automated detection of lung cancer has become a much
simpler task. Nevertheless, numerous researchers have addressed that the
performance of conventional CNNs may be hindered due to class imbalance issue,
which is prevalent in medical images. In this research work, we have proposed a
novel CNN architecture ``Multi-Scale Dense Network (MSD-Net)''
(trained-from-scratch). The novelties we bring in the proposed model are (I) We
introduce novel dense modules in the 4th block and 5th block of the CNN model.
We have leveraged 3 depthwise separable convolutional (DWSC) layers, and one
1x1 convolutional layer in each dense module, in order to reduce complexity of
the model considerably. (II) Additionally, we have incorporated one skip
connection from 3rd block to 5th block and one parallel branch connection from
4th block to Global Average Pooling (GAP) layer. We have utilized dilated
convolutional layer (with dilation rate=2) in the last parallel branch in order
to extract multi-scale features. Extensive experiments reveal that our proposed
model has outperformed latest CNN model ConvNext-Tiny, recent trend Vision
Transformer (ViT), Pooling-based ViT (PiT), and other existing models by
significant margins.

</details>

### [352] [NVSMask3D: Hard Visual Prompting with Camera Pose Interpolation for 3D Open Vocabulary Instance Segmentation](https://arxiv.org/abs/2504.14638)
*Junyuan Fang,Zihan Wang,Yejun Zhang,Shuzhe Wang,Iaroslav Melekhov,Juho Kannala*

Main category: cs.CV

TLDR: 提出了一种基于3D高斯泼溅的硬视觉提示方法，通过相机插值生成多视角，无需2D-3D优化或微调，提升了3D实例分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在3D实例级分割任务中表现不足，需要改进定位和识别能力。

Method: 采用3D高斯泼溅和相机插值生成多视角，增强几何一致性，无需训练。

Result: 方法提升了3D实例分割的鲁棒性和准确性。

Conclusion: 该方法为3D实例分割提供了一种无需训练的高效解决方案。

Abstract: Vision-language models (VLMs) have demonstrated impressive zero-shot transfer
capabilities in image-level visual perception tasks. However, they fall short
in 3D instance-level segmentation tasks that require accurate localization and
recognition of individual objects. To bridge this gap, we introduce a novel 3D
Gaussian Splatting based hard visual prompting approach that leverages camera
interpolation to generate diverse viewpoints around target objects without any
2D-3D optimization or fine-tuning. Our method simulates realistic 3D
perspectives, effectively augmenting existing hard visual prompts by enforcing
geometric consistency across viewpoints. This training-free strategy seamlessly
integrates with prior hard visual prompts, enriching object-descriptive
features and enabling VLMs to achieve more robust and accurate 3D instance
segmentation in diverse 3D scenes.

</details>

### [353] [Relation-R1: Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relational Comprehension](https://arxiv.org/abs/2504.14642)
*Lin Li,Wei Chen,Jiahui Li,Long Chen*

Main category: cs.CV

TLDR: Relation-R1是一个统一的关系理解框架，通过认知链式思维（CoT）引导的监督微调（SFT）和群体相对策略优化（GRPO）提升多模态大语言模型（MLLMs）在视觉关系理解中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在视觉关系理解（如场景图生成）中表现有限，尤其是在建模N元关系时缺乏语义依赖建模，导致不可靠输出和幻觉问题。

Method: 提出Relation-R1框架，结合CoT引导的SFT和GRPO，通过强化学习优化视觉-语义基础，减少语言先验依赖。

Result: 在PSG和SWiG数据集上，Relation-R1在二元和N元关系理解中达到最先进性能。

Conclusion: Relation-R1通过结构化输出和多奖励优化，显著提升了MLLMs在复杂视觉关系理解中的能力。

Abstract: Recent advances in multi-modal large language models (MLLMs) have
significantly improved object-level grounding and region captioning, but remain
limited in visual relation understanding (\eg, scene graph generation),
particularly in modeling \textit{N}-ary relationships that identify multiple
semantic roles among an action event. Such a lack of \textit{semantic
dependencies} modeling among multi-entities leads to unreliable outputs,
intensifying MLLMs' hallucinations and over-reliance on language priors. To
this end, we propose Relation-R1, the first unified relational comprehension
framework that explicitly integrates cognitive chain-of-thought (CoT)-guided
Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO)
within a reinforcement learning (RL) paradigm. Specifically, we first establish
foundational reasoning capabilities via SFT, enforcing structured outputs with
thinking processes. Then, GRPO is utilized to refine these outputs via
multi-reward optimization, prioritizing visual-semantic grounding over
language-induced biases, thereby improving generalization capability. Extensive
experiments on widely-used PSG and SWiG datasets demonstrate that Relation-R1
achieves state-of-the-art performance in both binary and \textit{N}-ary
relation understanding.

</details>

### [354] [EmoSEM: Segment and Explain Emotion Stimuli in Visual Art](https://arxiv.org/abs/2504.14658)
*Jing Zhang,Dan Guo,Zhangbin Li,Meng Wang*

Main category: cs.CV

TLDR: 论文提出EmoSEM模型，解决艺术图像中像素级情感理解和解释的挑战，结合情感提示和语言模型生成连贯解释。


<details>
  <summary>Details</summary>
Motivation: 艺术图像的情感理解面临主观性和抽象性的双重挑战，现有模型难以适应情感导向的分割和解释任务。

Method: 引入情感提示和可学习掩码标记，设计情感投影器和轻量级前缀投影器，融合视觉、掩码和情感标记输入语言模型。

Result: 实验验证模型有效性，实现从像素特征到情感解释的端到端建模。

Conclusion: EmoSEM为艺术情感计算提供首个可解释的细粒度分析框架。

Abstract: This paper focuses on a key challenge in visual art understanding: given an
art image, the model pinpoints pixel regions that trigger a specific human
emotion, and generates linguistic explanations for the emotional arousal.
Despite recent advances in art understanding, pixel-level emotion understanding
still faces a dual challenge: first, the subjectivity of emotion makes it
difficult for general segmentation models like SAM to adapt to emotion-oriented
segmentation tasks; and second, the abstract nature of art expression makes it
difficult for captioning models to balance pixel-level semantic understanding
and emotion reasoning. To solve the above problems, this paper proposes the
Emotion stimuli Segmentation and Explanation Model (EmoSEM) to endow the
segmentation model SAM with emotion comprehension capability. First, to enable
the model to perform segmentation under the guidance of emotional intent well,
we introduce an emotional prompt with a learnable mask token as the conditional
input for segmentation decoding. Then, we design an emotion projector to
establish the association between emotion and visual features. Next, more
importantly, to address emotion-visual stimuli alignment, we develop a
lightweight prefix projector, a module that fuses the learned emotional mask
with the corresponding emotion into a unified representation compatible with
the language model.Finally, we input the joint visual, mask, and emotional
tokens into the language model and output the emotional explanations. It
ensures that the generated interpretations remain semantically and emotionally
coherent with the visual stimuli. The method innovatively realizes end-to-end
modeling from low-level pixel features to high-level emotion interpretation,
providing the first interpretable fine-grained analysis framework for artistic
emotion computing. Extensive experiments validate the effectiveness of our
model.

</details>

### [355] [Frequency-domain Learning with Kernel Prior for Blind Image Deblurring](https://arxiv.org/abs/2504.14664)
*Jixiang Sun,Fei Lei,Jiawei Zhang,Wenxiu Sun,Yujiu Yang*

Main category: cs.CV

TLDR: 论文提出了一种结合核先验的深度学习方法，通过频率域融合提升图像去模糊的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在图像去模糊任务中泛化能力不足，主要依赖于特定域的数据集。

Method: 引入核先验，设计频率集成模块（FIM），结合基于频率的去模糊Transformer网络。

Result: 在多个盲图像去模糊任务中超越现有方法，表现出更强的泛化能力。

Conclusion: 核先验与频率域方法的结合有效提升了去模糊模型的泛化性能。

Abstract: While achieving excellent results on various datasets, many deep learning
methods for image deblurring suffer from limited generalization capabilities
with out-of-domain data. This limitation is likely caused by their dependence
on certain domain-specific datasets. To address this challenge, we argue that
it is necessary to introduce the kernel prior into deep learning methods, as
the kernel prior remains independent of the image context. For effective fusion
of kernel prior information, we adopt a rational implementation method inspired
by traditional deblurring algorithms that perform deconvolution in the
frequency domain. We propose a module called Frequency Integration Module (FIM)
for fusing the kernel prior and combine it with a frequency-based deblurring
Transfomer network. Experimental results demonstrate that our method
outperforms state-of-the-art methods on multiple blind image deblurring tasks,
showcasing robust generalization abilities. Source code will be available soon.

</details>

### [356] [DMPCN: Dynamic Modulated Predictive Coding Network with Hybrid Feedback Representations](https://arxiv.org/abs/2504.14665)
*A S M Sharifuzzaman Sagar,Yu Chen,Jun Hoong Chan*

Main category: cs.CV

TLDR: 本文提出了一种混合预测误差反馈机制和动态调制的深度预测编码网络，结合全局上下文和局部细节，并针对输入复杂性调整反馈。同时设计了专用损失函数以提高准确性。实验证明其在多个数据集上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 传统预测编码网络的性能受限于其误差反馈机制，无法同时处理局部和全局细节，且难以动态适应输入数据的复杂性。此外，缺乏针对性的损失函数限制了模型性能。

Method: 引入混合预测误差反馈机制，结合全局和局部信息，动态调整反馈；设计专用损失函数以优化预测误差最小化。

Result: 在CIFAR-10、CIFAR-100、MNIST和FashionMNIST数据集上表现出更快的收敛速度和更高的预测准确性。

Conclusion: 提出的方法显著提升了预测编码网络的性能，解决了传统方法的局限性。

Abstract: Traditional predictive coding networks, inspired by theories of brain
function, consistently achieve promising results across various domains,
extending their influence into the field of computer vision. However, the
performance of the predictive coding networks is limited by their error
feedback mechanism, which traditionally employs either local or global
recurrent updates, leading to suboptimal performance in processing both local
and broader details simultaneously. In addition, traditional predictive coding
networks face difficulties in dynamically adjusting to the complexity and
context of varying input data, which is crucial for achieving high levels of
performance in diverse scenarios. Furthermore, there is a gap in the
development and application of specific loss functions that could more
effectively guide the model towards optimal performance. To deal with these
issues, this paper introduces a hybrid prediction error feedback mechanism with
dynamic modulation for deep predictive coding networks by effectively combining
global contexts and local details while adjusting feedback based on input
complexity. Additionally, we present a loss function tailored to this framework
to improve accuracy by focusing on precise prediction error minimization.
Experimental results demonstrate the superiority of our model over other
approaches, showcasing faster convergence and higher predictive accuracy in
CIFAR-10, CIFAR-100, MNIST, and FashionMNIST datasets.

</details>

### [357] [Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens](https://arxiv.org/abs/2504.14666)
*Kaihang Pan,Wang Lin,Zhongqi Yue,Tenglong Ao,Liyu Jia,Wei Zhao,Juncheng Li,Siliang Tang,Hanwang Zhang*

Main category: cs.CV

TLDR: 论文提出了一种基于扩散时间步的递归视觉标记方法，解决了现有空间视觉标记缺乏递归结构的问题，实现了多模态理解与生成的统一框架。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs依赖空间视觉标记，但这些标记缺乏语言的递归结构，难以被LLM掌握。

Method: 利用扩散时间步学习离散、递归的视觉标记，通过递归补偿噪声图像中的属性损失。

Result: 实验表明，该方法在多模态理解和生成任务上均优于其他MLLMs。

Conclusion: 提出的视觉标记方法有效结合了LLMs和扩散模型的优势，实现了多模态任务的统一处理。

Abstract: Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify
visual comprehension and generation by combining LLM and diffusion models, the
state-of-the-art in each task, respectively. Existing approaches rely on
spatial visual tokens, where image patches are encoded and arranged according
to a spatial order (e.g., raster scan). However, we show that spatial tokens
lack the recursive structure inherent to languages, hence form an impossible
language for LLM to master. In this paper, we build a proper visual language by
leveraging diffusion timesteps to learn discrete, recursive visual tokens. Our
proposed tokens recursively compensate for the progressive attribute loss in
noisy images as timesteps increase, enabling the diffusion model to reconstruct
the original image at any timestep. This approach allows us to effectively
integrate the strengths of LLMs in autoregressive reasoning and diffusion
models in precise image generation, achieving seamless multimodal comprehension
and generation within a unified framework. Extensive experiments show that we
achieve superior performance for multimodal comprehension and generation
simultaneously compared with other MLLMs. Project Page:
https://DDT-LLaMA.github.io/.

</details>

### [358] [Seurat: From Moving Points to Depth](https://arxiv.org/abs/2504.14687)
*Seokju Cho,Jiahui Huang,Seungryong Kim,Joon-Young Lee*

Main category: cs.CV

TLDR: 提出一种通过分析2D轨迹的空间和时间关系来推断相对深度的方法，使用现有点跟踪模型和时空变换器，在TAPVid-3D基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 单目视频深度估计因缺乏立体视觉等深度线索而具有挑战性，但人类通过观察物体大小和间距变化能直观感知相对深度。

Method: 利用现有点跟踪模型捕获2D轨迹，通过时空变换器处理这些轨迹并直接推断深度变化。

Result: 在TAPVid-3D基准测试中表现出零样本泛化能力，从合成数据到真实数据均能实现平滑且高精度的深度预测。

Conclusion: 该方法通过时空分析2D轨迹，有效解决了单目视频深度估计的挑战，具有广泛适用性。

Abstract: Accurate depth estimation from monocular videos remains challenging due to
ambiguities inherent in single-view geometry, as crucial depth cues like
stereopsis are absent. However, humans often perceive relative depth
intuitively by observing variations in the size and spacing of objects as they
move. Inspired by this, we propose a novel method that infers relative depth by
examining the spatial relationships and temporal evolution of a set of tracked
2D trajectories. Specifically, we use off-the-shelf point tracking models to
capture 2D trajectories. Then, our approach employs spatial and temporal
transformers to process these trajectories and directly infer depth changes
over time. Evaluated on the TAPVid-3D benchmark, our method demonstrates robust
zero-shot performance, generalizing effectively from synthetic to real-world
datasets. Results indicate that our approach achieves temporally smooth,
high-accuracy depth predictions across diverse domains.

</details>

### [359] [Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark](https://arxiv.org/abs/2504.14693)
*Enxin Song,Wenhao Chai,Weili Xu,Jianwen Xie,Yuxuan Liu,Gaoang Wang*

Main category: cs.CV

TLDR: Video-MMLU是一个用于评估语言多模态模型（LMMs）在多学科讲座理解能力的大规模基准测试，揭示了当前模型在感知与推理任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 多学科讲座的理解任务尚未被充分探索，而现有的LMMs在此类任务中的表现尚不明确。

Method: 通过Video-MMLU基准测试评估了90多个开源和专有模型（参数规模从0.5B到40B），并分析了视觉标记数量和大语言模型对性能的影响。

Result: 当前模型在多学科讲座的认知挑战中表现有限，尤其是在需要感知与推理的任务中。

Conclusion: 研究揭示了多模态感知与推理在讲座理解中的相互作用，为未来模型优化提供了方向。

Abstract: Recent advancements in language multimodal models (LMMs) for video have
demonstrated their potential for understanding video content, yet the task of
comprehending multi-discipline lectures remains largely unexplored. We
introduce Video-MMLU, a massive benchmark designed to evaluate the capabilities
of LMMs in understanding Multi-Discipline Lectures. We evaluate over 90
open-source and proprietary models, ranging from 0.5B to 40B parameters. Our
results highlight the limitations of current models in addressing the cognitive
challenges presented by these lectures, especially in tasks requiring both
perception and reasoning. Additionally, we explore how the number of visual
tokens and the large language models influence performance, offering insights
into the interplay between multimodal perception and reasoning in lecture
comprehension.

</details>

### [360] [IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays](https://arxiv.org/abs/2504.14699)
*Sascha Jecklin,Aidana Massalimova,Ruyi Zha,Lilian Calvet,Christoph J. Laux,Mazda Farshad,Philipp Fürnstahl*

Main category: cs.CV

TLDR: 该论文提出了一种基于实例学习的3D脊柱重建方法，通过扩展$R^2$-Gaussian splatting框架，从稀疏X射线数据中重建解剖一致的3D体积，无需预训练即可适应新患者和解剖结构。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法需要大量标注数据且难以泛化，而实例学习方法如Gaussian splatting可避免这一限制，但其在稀疏X射线中的应用尚未充分探索。

Method: 扩展$R^2$-Gaussian splatting框架，引入基于风格转移的解剖引导放射学标准化步骤，提升重建质量。

Result: 实验表明，该方法在20至30个视图下重建的3D模型具有临床实用性，标准化步骤显著提升了解剖清晰度。

Conclusion: 该研究证明了从稀疏X射线进行实例学习3D重建的可行性，为手术导航中的3D成像提供了新思路。

Abstract: Spine surgery is a high-risk intervention demanding precise execution, often
supported by image-based navigation systems. Recently, supervised learning
approaches have gained attention for reconstructing 3D spinal anatomy from
sparse fluoroscopic data, significantly reducing reliance on
radiation-intensive 3D imaging systems. However, these methods typically
require large amounts of annotated training data and may struggle to generalize
across varying patient anatomies or imaging conditions. Instance-learning
approaches like Gaussian splatting could offer an alternative by avoiding
extensive annotation requirements. While Gaussian splatting has shown promise
for novel view synthesis, its application to sparse, arbitrarily posed real
intraoperative X-rays has remained largely unexplored. This work addresses this
limitation by extending the $R^2$-Gaussian splatting framework to reconstruct
anatomically consistent 3D volumes under these challenging conditions. We
introduce an anatomy-guided radiographic standardization step using style
transfer, improving visual consistency across views, and enhancing
reconstruction quality. Notably, our framework requires no pretraining, making
it inherently adaptable to new patients and anatomies. We evaluated our
approach using an ex-vivo dataset. Expert surgical evaluation confirmed the
clinical utility of the 3D reconstructions for navigation, especially when
using 20 to 30 views, and highlighted the standardization's benefit for
anatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM)
confirmed performance trade-offs compared to idealized settings, but also
validated the improvement gained from standardization over raw inputs. This
work demonstrates the feasibility of instance-based volumetric reconstruction
from arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for
surgical navigation.

</details>

### [361] [Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine grained Features](https://arxiv.org/abs/2504.14708)
*Parshuram N. Aarotale,Ajita Rattani*

Main category: cs.CV

TLDR: 论文提出了一种名为XMANet的新方法，通过跨层互注意力机制结合浅层和深层CNN专家的特征，用于基于EMG的手势识别。实验表明，XMANet在多种架构和信号处理技术上均优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 提高EMG手势识别的准确性和鲁棒性，通过细粒度分类和跨层特征融合解决现有方法的局限性。

Method: 提出XMANet模型，结合短时傅里叶变换（STFT）和小波变换（WT）生成的谱图和尺度图，通过跨层互注意力机制融合浅层和深层特征。

Result: 在Grabmyo和FORS EMG数据集上，XMANet相比基线模型（如ResNet50、DenseNet121等）性能提升显著，最高提升达9.36%。

Conclusion: XMANet通过细粒度特征和跨层注意力机制，显著提升了EMG手势识别的性能，展示了其在多种场景下的潜力。

Abstract: Electromyography (EMG) based hand gesture recognition converts forearm muscle
activity into control commands for prosthetics, rehabilitation, and human
computer interaction. This paper proposes a novel approach to EMG-based hand
gesture recognition that uses fine-grained classification and presents XMANet,
which unifies low-level local and high level semantic cues through cross layer
mutual attention among shallow to deep CNN experts. Using stacked spectrograms
and scalograms derived from the Short Time Fourier Transform (STFT) and Wavelet
Transform (WT), we benchmark XMANet against ResNet50, DenseNet-121,
MobileNetV3, and EfficientNetB0. Experimental results on the Grabmyo dataset
indicate that, using STFT, the proposed XMANet model outperforms the baseline
ResNet50, EfficientNetB0, MobileNetV3, and DenseNet121 models with improvement
of approximately 1.72%, 4.38%, 5.10%, and 2.53%, respectively. When employing
the WT approach, improvements of around 1.57%, 1.88%, 1.46%, and 2.05% are
observed over the same baselines. Similarly, on the FORS EMG dataset, the
XMANet(ResNet50) model using STFT shows an improvement of about 5.04% over the
baseline ResNet50. In comparison, the XMANet(DenseNet121) and
XMANet(MobileNetV3) models yield enhancements of approximately 4.11% and 2.81%,
respectively. Moreover, when using WT, the proposed XMANet achieves gains of
around 4.26%, 9.36%, 5.72%, and 6.09% over the baseline ResNet50, DenseNet121,
MobileNetV3, and EfficientNetB0 models, respectively. These results confirm
that XMANet consistently improves performance across various architectures and
signal processing techniques, demonstrating the strong potential of fine
grained features for accurate and robust EMG classification.

</details>

### [362] [Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline](https://arxiv.org/abs/2504.14709)
*Hui Zhou,Shaoshuai Shi,Hongsheng Li*

Main category: cs.CV

TLDR: 本文提出了一种结合模仿学习（IL）和强化学习（RL）的新框架，以解决纯模仿学习方法在自动驾驶规划中的局限性。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在自动驾驶规划中表现良好，但难以验证其是否真正理解驾驶原理，且容易过拟合常见场景。

Method: 提出了一种闭环模拟器、因果基准测试和IL与RL结合的框架。

Result: 新框架旨在克服纯模仿学习的局限性，提高对罕见或未见场景的泛化能力。

Conclusion: 通过结合IL和RL，本文为自动驾驶规划提供了一种更鲁棒的解决方案。

Abstract: Machine learning (ML)-based planners have recently gained significant
attention. They offer advantages over traditional optimization-based planning
algorithms. These advantages include fewer manually selected parameters and
faster development. Within ML-based planning, imitation learning (IL) is a
common algorithm. It primarily learns driving policies directly from supervised
trajectory data. While IL has demonstrated strong performance on many open-loop
benchmarks, it remains challenging to determine if the learned policy truly
understands fundamental driving principles, rather than simply extrapolating
from the ego-vehicle's initial state. Several studies have identified this
limitation and proposed algorithms to address it. However, these methods often
use original datasets for evaluation. In these datasets, future trajectories
are heavily dependent on initial conditions. Furthermore, IL often overfits to
the most common scenarios. It struggles to generalize to rare or unseen
situations.
  To address these challenges, this work proposes: 1) a novel closed-loop
simulator supporting both imitation and reinforcement learning, 2) a causal
benchmark derived from the Waymo Open Dataset to rigorously assess the impact
of the copycat problem, and 3) a novel framework integrating imitation learning
and reinforcement learning to overcome the limitations of purely imitative
approaches. The code for this work will be released soon.

</details>

### [363] [Med-2D SegNet: A Light Weight Deep Neural Network for Medical 2D Image Segmentation](https://arxiv.org/abs/2504.14715)
*Md. Sanaullah Chowdhury,Salauddin Tapu,Noyon Kumar Sarkar,Ferdous Bin Ali,Lameya Sabrin*

Main category: cs.CV

TLDR: Med-2D SegNet是一种高效且精确的医学图像分割架构，通过紧凑的Med Block设计实现低计算复杂度，并在多个数据集上达到89.77%的平均Dice相似系数。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对临床诊断和手术规划至关重要，但现有方法常因解剖结构多样性和计算复杂度高而受限。

Method: 提出Med-2D SegNet，采用Med Block编码器设计，结合维度扩展和参数减少技术，实现高效特征提取。

Result: 在KVASIR-SEG等数据集上表现优异，平均DSC为89.77%，参数仅2.07百万，支持零样本学习。

Conclusion: Med-2D SegNet在精度与效率间取得平衡，为临床和资源受限环境提供了高性能工具。

Abstract: Accurate and efficient medical image segmentation is crucial for advancing
clinical diagnostics and surgical planning, yet remains a complex challenge due
to the variability in anatomical structures and the demand for low-complexity
models. In this paper, we introduced Med-2D SegNet, a novel and highly
efficient segmentation architecture that delivers outstanding accuracy while
maintaining a minimal computational footprint. Med-2D SegNet achieves
state-of-the-art performance across multiple benchmark datasets, including
KVASIR-SEG, PH2, EndoVis, and GLAS, with an average Dice similarity coefficient
(DSC) of 89.77% across 20 diverse datasets. Central to its success is the
compact Med Block, a specialized encoder design that incorporates dimension
expansion and parameter reduction, enabling precise feature extraction while
keeping model parameters to a low count of just 2.07 million. Med-2D SegNet
excels in cross-dataset generalization, particularly in polyp segmentation,
where it was trained on KVASIR-SEG and showed strong performance on unseen
datasets, demonstrating its robustness in zero-shot learning scenarios, even
though we acknowledge that further improvements are possible. With top-tier
performance in both binary and multi-class segmentation, Med-2D SegNet
redefines the balance between accuracy and efficiency, setting a new benchmark
for medical image analysis. This work paves the way for developing accessible,
high-performance diagnostic tools suitable for clinical environments and
resource-constrained settings, making it a step forward in the democratization
of advanced medical technology.

</details>

### [364] [TAPIP3D: Tracking Any Point in Persistent 3D Geometry](https://arxiv.org/abs/2504.14717)
*Bowei Zhang,Lei Ke,Adam W. Harley,Katerina Fragkiadaki*

Main category: cs.CV

TLDR: TAPIP3D是一种用于单目RGB和RGB-D视频中长期3D点跟踪的新方法，通过相机稳定的时空特征云和局部对注意力机制，显著提升了3D点跟踪的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D点跟踪方法在处理相机运动和3D点分布不规则性时表现不佳，TAPIP3D旨在通过3D世界空间中的稳定表示和上下文策略解决这些问题。

Method: TAPIP3D将视频表示为相机稳定的时空特征云，利用深度和相机运动信息将2D特征提升到3D世界空间，并通过局部对注意力机制优化3D运动估计。

Result: TAPIP3D在3D点跟踪任务中显著优于现有方法，并在有准确深度时提升了2D跟踪精度，同时支持相机坐标和世界坐标的推理。

Conclusion: TAPIP3D通过补偿相机运动和利用3D上下文关系，实现了更鲁棒和准确的3D点跟踪，为相关领域提供了新的解决方案。

Abstract: We introduce TAPIP3D, a novel approach for long-term 3D point tracking in
monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized
spatio-temporal feature clouds, leveraging depth and camera motion information
to lift 2D video features into a 3D world space where camera motion is
effectively canceled. TAPIP3D iteratively refines multi-frame 3D motion
estimates within this stabilized representation, enabling robust tracking over
extended periods. To manage the inherent irregularities of 3D point
distributions, we propose a Local Pair Attention mechanism. This 3D
contextualization strategy effectively exploits spatial relationships in 3D,
forming informative feature neighborhoods for precise 3D trajectory estimation.
Our 3D-centric approach significantly outperforms existing 3D point tracking
methods and even enhances 2D tracking accuracy compared to conventional 2D
pixel trackers when accurate depth is available. It supports inference in both
camera coordinates (i.e., unstabilized) and world coordinates, and our results
demonstrate that compensating for camera motion improves tracking performance.
Our approach replaces the conventional 2D square correlation neighborhoods used
in prior 2D and 3D trackers, leading to more robust and accurate results across
various 3D point tracking benchmarks. Project Page: https://tapip3d.github.io

</details>

### [365] [ChronoRoot 2.0: An Open AI-Powered Platform for 2D Temporal Plant Phenotyping](https://arxiv.org/abs/2504.14736)
*Nicolás Gaggion,Rodrigo Bonazzola,María Florencia Legascue,María Florencia Mammarella,Florencia Sol Rodriguez,Federico Emanuel Aballay,Florencia Belén Catulo,Andana Barrios,Franco Accavallo,Santiago Nahuel Villarreal,Martin Crespi,Martiniano María Ricardi,Ezequiel Petrillo,Thomas Blein,Federico Ariel,Enzo Ferrante*

Main category: cs.CV

TLDR: ChronoRoot 2.0是一个开源平台，结合低成本硬件和AI技术，用于植物根系发育的时序表型分析，提升了多器官追踪、实时质量控制等功能。


<details>
  <summary>Details</summary>
Motivation: 植物发育可塑性分析对理解植物适应性和农业可持续性至关重要，但现有技术难以实现全面的时序根系发育分析。

Method: ChronoRoot 2.0整合了硬件和AI技术，支持多器官追踪、实时验证、全面结构测量和双用户界面。

Result: 系统在拟南芥中展示了其功能，包括光周期生长模式分析、转基因植物向重力性响应研究和高通量表型筛选。

Conclusion: ChronoRoot 2.0通过开源和模块化设计，使复杂的时序表型分析更易普及，促进植物科学社区的发展。

Abstract: The analysis of plant developmental plasticity, including root system
architecture, is fundamental to understanding plant adaptability and
development, particularly in the context of climate change and agricultural
sustainability. While significant advances have been made in plant phenotyping
technologies, comprehensive temporal analysis of root development remains
challenging, with most existing solutions providing either limited throughput
or restricted structural analysis capabilities. Here, we present ChronoRoot
2.0, an integrated open-source platform that combines affordable hardware with
advanced artificial intelligence to enable sophisticated temporal plant
phenotyping. The system introduces several major advances, offering an integral
perspective of seedling development: (i) simultaneous multi-organ tracking of
six distinct plant structures, (ii) quality control through real-time
validation, (iii) comprehensive architectural measurements including novel
gravitropic response parameters, and (iv) dual specialized user interfaces for
both architectural analysis and high-throughput screening. We demonstrate the
system's capabilities through three use cases for Arabidopsis thaliana:
characterization of circadian growth patterns under different light conditions,
detailed analysis of gravitropic responses in transgenic plants, and
high-throughput screening of etiolation responses across multiple genotypes.
ChronoRoot 2.0 maintains its predecessor's advantages of low cost and
modularity while significantly expanding its capabilities, making sophisticated
temporal phenotyping more accessible to the broader plant science community.
The system's open-source nature, combined with extensive documentation and
containerized deployment options, ensures reproducibility and enables
community-driven development of new analytical capabilities.

</details>

### [366] [SuperCL: Superpixel Guided Contrastive Learning for Medical Image Segmentation Pre-training](https://arxiv.org/abs/2504.14737)
*Shuang Zeng,Lei Zhu,Xinliang Zhang,Hangzhou He,Yanye Lu*

Main category: cs.CV

TLDR: 提出了一种名为SuperCL的新型对比学习方法，用于医学图像分割预训练，通过利用图像的结构先验和像素相关性，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临高质量标注数据稀缺的挑战，现有对比学习方法忽略图像内相似像素组的特性，且对比对生成依赖人工设定阈值，效率低且泛化性差。

Method: 提出Intra-image Local Contrastive Pairs (ILCP)和Inter-image Global Contrastive Pairs (IGCP)两种对比对生成策略，利用超像素图生成伪掩码指导监督对比学习，并引入ASP和CCL模块优化结构信息利用。

Result: 在8个医学图像数据集上，SuperCL优于12种现有方法，DSC指标分别提升3.15%、5.44%、7.89%（MMWHS、CHAOS、Spleen数据集，10%标注）。

Conclusion: SuperCL通过创新的对比对生成和结构信息利用，显著提升了医学图像分割的精度和效率，具有广泛的应用潜力。

Abstract: Medical image segmentation is a critical yet challenging task, primarily due
to the difficulty of obtaining extensive datasets of high-quality,
expert-annotated images. Contrastive learning presents a potential but still
problematic solution to this issue. Because most existing methods focus on
extracting instance-level or pixel-to-pixel representation, which ignores the
characteristics between intra-image similar pixel groups. Moreover, when
considering contrastive pairs generation, most SOTA methods mainly rely on
manually setting thresholds, which requires a large number of gradient
experiments and lacks efficiency and generalization. To address these issues,
we propose a novel contrastive learning approach named SuperCL for medical
image segmentation pre-training. Specifically, our SuperCL exploits the
structural prior and pixel correlation of images by introducing two novel
contrastive pairs generation strategies: Intra-image Local Contrastive Pairs
(ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation.
Considering superpixel cluster aligns well with the concept of contrastive
pairs generation, we utilize the superpixel map to generate pseudo masks for
both ILCP and IGCP to guide supervised contrastive learning. Moreover, we also
propose two modules named Average SuperPixel Feature Map Generation (ASP) and
Connected Components Label Generation (CCL) to better exploit the prior
structural information for IGCP. Finally, experiments on 8 medical image
datasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL
achieves a superior performance with more precise predictions from
visualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best
results on MMWHS, CHAOS, Spleen with 10% annotations. Our code will be released
after acceptance.

</details>

### [367] [Advancing Video Anomaly Detection: A Bi-Directional Hybrid Framework for Enhanced Single- and Multi-Task Approaches](https://arxiv.org/abs/2504.14753)
*Guodong Shen,Yuqi Ouyang,Junru Lu,Yixuan Yang,Victor Sanchez*

Main category: cs.CV

TLDR: 论文提出了一种混合框架，结合视觉Transformer和ConvLSTM，通过双向结构优化单任务框架，提升视频异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 观察到现有方法在多任务视频异常检测中对单任务框架的优化不足，认为优化单任务框架可同时提升单任务和多任务方法。

Method: 利用中间帧预测作为代理任务，设计双向结构结合视觉Transformer和ConvLSTM，引入卷积时序Transformer和层交互ConvLSTM桥。

Result: 在公开基准测试中验证了框架的有效性，无论是单任务还是多任务分支均表现优异。

Conclusion: 混合框架结合视觉Transformer和ConvLSTM的优势，显著提升了视频异常检测的稳定性和准确性。

Abstract: Despite the prevailing transition from single-task to multi-task approaches
in video anomaly detection, we observe that many adopt sub-optimal frameworks
for individual proxy tasks. Motivated by this, we contend that optimizing
single-task frameworks can advance both single- and multi-task approaches.
Accordingly, we leverage middle-frame prediction as the primary proxy task, and
introduce an effective hybrid framework designed to generate accurate
predictions for normal frames and flawed predictions for abnormal frames. This
hybrid framework is built upon a bi-directional structure that seamlessly
integrates both vision transformers and ConvLSTMs. Specifically, we utilize
this bi-directional structure to fully analyze the temporal dimension by
predicting frames in both forward and backward directions, significantly
boosting the detection stability. Given the transformer's capacity to model
long-range contextual dependencies, we develop a convolutional temporal
transformer that efficiently associates feature maps from all context frames to
generate attention-based predictions for target frames. Furthermore, we devise
a layer-interactive ConvLSTM bridge that facilitates the smooth flow of
low-level features across layers and time-steps, thereby strengthening
predictions with fine details. Anomalies are eventually identified by
scrutinizing the discrepancies between target frames and their corresponding
predictions. Several experiments conducted on public benchmarks affirm the
efficacy of our hybrid framework, whether used as a standalone single-task
approach or integrated as a branch in a multi-task approach. These experiments
also underscore the advantages of merging vision transformers and ConvLSTMs for
video anomaly detection.

</details>

### [368] [How Effective Can Dropout Be in Multiple Instance Learning ?](https://arxiv.org/abs/2504.14783)
*Wenhui Zhu,Peijie Qiu,Xiwen Chen,Zhangsihao Yang,Aristeidis Sotiras,Abolfazl Razi,Yalin Wang*

Main category: cs.CV

TLDR: 论文提出了一种名为MIL-Dropout的新方法，通过丢弃包中最重要的实例来提升多实例学习（MIL）的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统MIL方法在WSI分类中因两阶段训练方案导致特征嵌入噪声和弱监督问题，而常用的dropout技术尚未在MIL中被充分探索。

Method: 提出MIL-Dropout方法，系统性地决定丢弃包中最重要的实例，以优化特征学习和泛化能力。

Result: 在五个MIL基准数据集和两个WSI数据集上的实验表明，MIL-Dropout显著提升了现有MIL方法的性能，且计算成本极低。

Conclusion: MIL-Dropout是一种简单有效的方法，能够显著提升MIL的性能和鲁棒性。

Abstract: Multiple Instance Learning (MIL) is a popular weakly-supervised method for
various applications, with a particular interest in histological whole slide
image (WSI) classification. Due to the gigapixel resolution of WSI,
applications of MIL in WSI typically necessitate a two-stage training scheme:
first, extract features from the pre-trained backbone and then perform MIL
aggregation. However, it is well-known that this suboptimal training scheme
suffers from "noisy" feature embeddings from the backbone and inherent weak
supervision, hindering MIL from learning rich and generalizable features.
However, the most commonly used technique (i.e., dropout) for mitigating this
issue has yet to be explored in MIL. In this paper, we empirically explore how
effective the dropout can be in MIL. Interestingly, we observe that dropping
the top-k most important instances within a bag leads to better performance and
generalization even under noise attack. Based on this key observation, we
propose a novel MIL-specific dropout method, termed MIL-Dropout, which
systematically determines which instances to drop. Experiments on five MIL
benchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the
performance of current MIL methods with a negligible computational cost. The
code is available at https://github.com/ChongQingNoSubway/MILDropout.

</details>

### [369] [When Cloud Removal Meets Diffusion Model in Remote Sensing](https://arxiv.org/abs/2504.14785)
*Zhenyu Yu,Mohd Yamani Idna Idris,Pei Wang*

Main category: cs.CV

TLDR: DC4CR是一种基于扩散模型的多模态云去除框架，通过提示驱动控制和低秩适应等技术，实现了高效、可扩展的云去除，无需预生成云掩膜。


<details>
  <summary>Details</summary>
Motivation: 云遮挡严重影响了遥感应用，阻碍了地表信息的获取和分析。

Method: 提出DC4CR框架，结合提示驱动控制、低秩适应、主题驱动生成和分组学习，无需预生成云掩膜。

Result: 在RICE和CUHK-CR数据集上表现出色，实现了先进的云去除效果。

Conclusion: DC4CR为遥感图像处理提供了一种实用且高效的解决方案，具有广泛的实际应用前景。

Abstract: Cloud occlusion significantly hinders remote sensing applications by
obstructing surface information and complicating analysis. To address this, we
propose DC4CR (Diffusion Control for Cloud Removal), a novel multimodal
diffusion-based framework for cloud removal in remote sensing imagery. Our
method introduces prompt-driven control, allowing selective removal of thin and
thick clouds without relying on pre-generated cloud masks, thereby enhancing
preprocessing efficiency and model adaptability. Additionally, we integrate
low-rank adaptation for computational efficiency, subject-driven generation for
improved generalization, and grouped learning to enhance performance on small
datasets. Designed as a plug-and-play module, DC4CR seamlessly integrates into
existing cloud removal models, providing a scalable and robust solution.
Extensive experiments on the RICE and CUHK-CR datasets demonstrate
state-of-the-art performance, achieving superior cloud removal across diverse
conditions. This work presents a practical and efficient approach for remote
sensing image processing with broad real-world applications.

</details>

### [370] [Real-Time Sleepiness Detection for Driver State Monitoring System](https://arxiv.org/abs/2504.14807)
*Deepak Ghimire,Sunghwan Jeong,Sunhong Yoon,Sanghyun Park,Juhwan Choi*

Main category: cs.CV

TLDR: 提出了一种实时驾驶员眼睛状态检测方法，结合动态模板匹配和Kalman滤波跟踪，使用SVM分类器判断眼睛状态，触发疲劳警报。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是事故的重要因素，需要实时监测眼睛状态以防止疲劳驾驶。

Method: 通过计算机视觉技术检测人脸和眼睛位置，结合动态模板匹配和Kalman滤波跟踪眼睛，使用SVM+HOG分类眼睛状态。

Result: 系统能实时检测眼睛状态，并在眼睛长时间闭合时触发警报。

Conclusion: 该方法有效监测驾驶员疲劳，减少事故风险。

Abstract: A driver face monitoring system can detect driver fatigue, which is a
significant factor in many accidents, using computer vision techniques. In this
paper, we present a real-time technique for driver eye state detection. First,
the face is detected, and the eyes are located within the face region for
tracking. A normalized cross-correlation-based online dynamic template matching
technique, combined with Kalman filter tracking, is proposed to track the
detected eye positions in subsequent image frames. A support vector machine
with histogram of oriented gradients (HOG) features is used to classify the
state of the eyes as open or closed. If the eyes remain closed for a specified
period, the driver is considered to be asleep, and an alarm is triggered.

</details>

### [371] [ECViT: Efficient Convolutional Vision Transformer with Local-Attention and Multi-scale Stages](https://arxiv.org/abs/2504.14825)
*Zhoujie Qian*

Main category: cs.CV

TLDR: ECViT是一种结合CNN和Transformer优势的混合架构，通过引入局部性和平移不变性等归纳偏置，解决了ViT的高计算成本和数据需求问题。


<details>
  <summary>Details</summary>
Motivation: ViT在计算成本和数据需求方面存在局限性，需要一种更高效的解决方案。

Method: ECViT通过从低级特征提取补丁并在编码器中加入卷积操作，结合局部注意力和金字塔结构，实现多尺度特征提取。

Result: ECViT在图像分类任务中性能优于现有模型，同时保持低计算和存储需求。

Conclusion: ECViT为高效且高性能的应用提供了理想解决方案。

Abstract: Vision Transformers (ViTs) have revolutionized computer vision by leveraging
self-attention to model long-range dependencies. However, ViTs face challenges
such as high computational costs due to the quadratic scaling of self-attention
and the requirement of a large amount of training data. To address these
limitations, we propose the Efficient Convolutional Vision Transformer (ECViT),
a hybrid architecture that effectively combines the strengths of CNNs and
Transformers. ECViT introduces inductive biases such as locality and
translation invariance, inherent to Convolutional Neural Networks (CNNs) into
the Transformer framework by extracting patches from low-level features and
enhancing the encoder with convolutional operations. Additionally, it
incorporates local-attention and a pyramid structure to enable efficient
multi-scale feature extraction and representation. Experimental results
demonstrate that ECViT achieves an optimal balance between performance and
efficiency, outperforming state-of-the-art models on various image
classification tasks while maintaining low computational and storage
requirements. ECViT offers an ideal solution for applications that prioritize
high efficiency without compromising performance.

</details>

### [372] [Distribution-aware Dataset Distillation for Efficient Image Restoration](https://arxiv.org/abs/2504.14826)
*Zhuoran Zheng,Xin Su,Chen Wu,Xiuyi Jia*

Main category: cs.CV

TLDR: 论文提出了一种名为TripleD的分布感知数据集蒸馏方法，用于解决图像恢复领域的数据集蒸馏问题，显著减少了训练时间和计算资源。


<details>
  <summary>Details</summary>
Motivation: 随着图像数据的指数级增长，训练图像恢复模型变得耗时且资源密集。数据集蒸馏是潜在解决方案，但现有方法在图像恢复领域尚未探索。

Method: TripleD利用预训练的视觉Transformer提取图像特征以评估复杂度，基于复杂度选择子集，并通过轻量级CNN调整图像分布。训练分为两阶段：早期关注低复杂度样本，后期选择高复杂度样本。

Result: TripleD在多项图像恢复任务中表现优异，包括多任务、一体化及超高清图像恢复，仅需一台消费级GPU在8小时内完成训练（节省500倍计算资源）。

Conclusion: TripleD为图像恢复领域的数据集蒸馏提供了高效解决方案，显著降低了训练成本，同时保持了高性能。

Abstract: With the exponential increase in image data, training an image restoration
model is laborious. Dataset distillation is a potential solution to this
problem, yet current distillation techniques are a blank canvas in the field of
image restoration. To fill this gap, we propose the Distribution-aware Dataset
Distillation method (TripleD), a new framework that extends the principles of
dataset distillation to image restoration. Specifically, TripleD uses a
pre-trained vision Transformer to extract features from images for complexity
evaluation, and the subset (the number of samples is much smaller than the
original training set) is selected based on complexity. The selected subset is
then fed through a lightweight CNN that fine-tunes the image distribution to
align with the distribution of the original dataset at the feature level. To
efficiently condense knowledge, the training is divided into two stages. Early
stages focus on simpler, low-complexity samples to build foundational
knowledge, while later stages select more complex and uncertain samples as the
model matures. Our method achieves promising performance on multiple image
restoration tasks, including multi-task image restoration, all-in-one image
restoration, and ultra-high-definition image restoration tasks. Note that we
can train a state-of-the-art image restoration model on an
ultra-high-definition (4K resolution) dataset using only one consumer-grade GPU
in less than 8 hours (500 savings in computing resources and immeasurable
training time).

</details>

### [373] [Reliable Multi-Modal Object Re-Identification via Modality-Aware Graph Reasoning](https://arxiv.org/abs/2504.14847)
*Xixi Wan,Aihua Zheng,Zi Wang,Bo Jiang,Jin Tang,Jixin Ma*

Main category: cs.CV

TLDR: 提出了一种名为MGRNet的图推理模型，用于解决多模态ReID任务中局部特征质量差异和模态互补信息利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法常忽视局部特征质量差异，未能充分利用多模态互补信息，尤其是在低质量特征情况下。

Method: 构建模态感知图以增强细粒度局部细节提取，通过选择性图节点交换操作优化低质量特征影响，并利用局部感知图推理模块传播多模态信息。

Result: 在四个基准测试（RGBNT201、Market1501-MM、RGBNT100、MSVR310）上取得了最先进的性能。

Conclusion: MGRNet通过图推理有效提升了多模态ReID任务的性能，并能重建缺失模态信息。

Abstract: Multi-modal data provides abundant and diverse object information, crucial
for effective modal interactions in Re-Identification (ReID) tasks. However,
existing approaches often overlook the quality variations in local features and
fail to fully leverage the complementary information across modalities,
particularly in the case of low-quality features. In this paper, we propose to
address this issue by leveraging a novel graph reasoning model, termed the
Modality-aware Graph Reasoning Network (MGRNet). Specifically, we first
construct modality-aware graphs to enhance the extraction of fine-grained local
details by effectively capturing and modeling the relationships between
patches. Subsequently, the selective graph nodes swap operation is employed to
alleviate the adverse effects of low-quality local features by considering both
local and global information, enhancing the representation of discriminative
information. Finally, the swapped modality-aware graphs are fed into the
local-aware graph reasoning module, which propagates multi-modal information to
yield a reliable feature representation. Another advantage of the proposed
graph reasoning approach is its ability to reconstruct missing modal
information by exploiting inherent structural relationships, thereby minimizing
disparities between different modalities. Experimental results on four
benchmarks (RGBNT201, Market1501-MM, RGBNT100, MSVR310) indicate that the
proposed method achieves state-of-the-art performance in multi-modal object
ReID. The code for our method will be available upon acceptance.

</details>

### [374] [Object-Level Verbalized Confidence Calibration in Vision-Language Models via Semantic Perturbation](https://arxiv.org/abs/2504.14848)
*Yunpu Zhao,Rui Zhang,Junbin Xiao,Ruibo Hou,Jiaming Guo,Zihao Zhang,Yifan Hao,Yunji Chen*

Main category: cs.CV

TLDR: 提出了一种名为CSP的新框架，通过语义扰动改善视觉语言模型（VLMs）的置信度校准，提升其可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: VLMs在多模态任务中表现优异，但置信度校准不佳，导致用户信任问题。

Method: 引入高斯噪声扰动关键对象区域模拟视觉不确定性，通过两阶段训练（监督微调和偏好优化）提升校准。

Result: 实验表明，CSP显著改善了置信度与响应正确性的对齐，同时保持或提升任务性能。

Conclusion: 语义扰动是提升VLMs可靠性和可解释性的实用工具。

Abstract: Vision-language models (VLMs) excel in various multimodal tasks but
frequently suffer from poor calibration, resulting in misalignment between
their verbalized confidence and response correctness. This miscalibration
undermines user trust, especially when models confidently provide incorrect or
fabricated information. In this work, we propose a novel Confidence Calibration
through Semantic Perturbation (CSP) framework to improve the calibration of
verbalized confidence for VLMs in response to object-centric queries. We first
introduce a perturbed dataset where Gaussian noise is applied to the key object
regions to simulate visual uncertainty at different confidence levels,
establishing an explicit mapping between visual ambiguity and confidence
levels. We further enhance calibration through a two-stage training process
combining supervised fine-tuning on the perturbed dataset with subsequent
preference optimization. Extensive experiments on popular benchmarks
demonstrate that our method significantly improves the alignment between
verbalized confidence and response correctness while maintaining or enhancing
overall task performance. These results highlight the potential of semantic
perturbation as a practical tool for improving the reliability and
interpretability of VLMs.

</details>

### [375] [Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer](https://arxiv.org/abs/2504.14860)
*Ziyi Liu,Yangcen Liu*

Main category: cs.CV

TLDR: PseudoFormer是一个两分支框架，用于解决弱监督时序动作定位（WTAL）中的伪标签生成和训练优化问题，通过RickerFusion和不确定性掩码等技术，在THUMOS14和ActivityNet1.3上取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 弱监督时序动作定位（WTAL）因缺乏时间注释导致性能与全监督方法存在差距，伪标签生成和训练优化是主要挑战。

Method: 提出PseudoFormer框架，包括RickerFusion生成高质量伪标签，利用片段级和提议级标签训练回归模型，并通过不确定性掩码和迭代优化处理噪声标签。

Result: 在THUMOS14和ActivityNet1.3上取得最优性能，消融实验验证了各组件的重要性。

Conclusion: PseudoFormer通过创新设计有效缩小了弱监督与全监督时序动作定位的差距，为WTAL提供了新思路。

Abstract: Weakly-supervised Temporal Action Localization (WTAL) has achieved notable
success but still suffers from a lack of temporal annotations, leading to a
performance and framework gap compared with fully-supervised methods. While
recent approaches employ pseudo labels for training, three key challenges:
generating high-quality pseudo labels, making full use of different priors, and
optimizing training methods with noisy labels remain unresolved. Due to these
perspectives, we propose PseudoFormer, a novel two-branch framework that
bridges the gap between weakly and fully-supervised Temporal Action
Localization (TAL). We first introduce RickerFusion, which maps all predicted
action proposals to a global shared space to generate pseudo labels with better
quality. Subsequently, we leverage both snippet-level and proposal-level labels
with different priors from the weak branch to train the regression-based model
in the full branch. Finally, the uncertainty mask and iterative refinement
mechanism are applied for training with noisy pseudo labels. PseudoFormer
achieves state-of-the-art WTAL results on the two commonly used benchmarks,
THUMOS14 and ActivityNet1.3. Besides, extensive ablation studies demonstrate
the contribution of each component of our method.

</details>

### [376] [Twin Co-Adaptive Dialogue for Progressive Image Generation](https://arxiv.org/abs/2504.14868)
*Jianhui Wang,Yangfan He,Yan Zhong,Xinyuan Song,Jiayi Su,Yuheng Feng,Hongyang He,Wenyu Zhu,Xinhang Yuan,Kuan Lu,Menghao Huo,Miao Zhang,Keqin Li,Jiaqi Chen,Tianyu Shi,Xueqian Wang*

Main category: cs.CV

TLDR: Twin-Co框架通过同步、协同适应的对话逐步优化图像生成，减少用户提示的模糊性，提升生成质量和用户体验。


<details>
  <summary>Details</summary>
Motivation: 现代文本到图像生成系统在处理用户提示的模糊性时表现不佳，Twin-Co旨在通过动态对话解决这一问题。

Method: Twin-Co采用动态迭代工作流，通过智能对话代理与用户交互，逐步优化生成的图像。

Result: 实验表明，Twin-Co减少了试错迭代，提升了图像质量，优化了创意流程。

Conclusion: Twin-Co通过协同适应的对话机制，显著改善了图像生成的效果和用户体验。

Abstract: Modern text-to-image generation systems have enabled the creation of
remarkably realistic and high-quality visuals, yet they often falter when
handling the inherent ambiguities in user prompts. In this work, we present
Twin-Co, a framework that leverages synchronized, co-adaptive dialogue to
progressively refine image generation. Instead of a static generation process,
Twin-Co employs a dynamic, iterative workflow where an intelligent dialogue
agent continuously interacts with the user. Initially, a base image is
generated from the user's prompt. Then, through a series of synchronized
dialogue exchanges, the system adapts and optimizes the image according to
evolving user feedback. The co-adaptive process allows the system to
progressively narrow down ambiguities and better align with user intent.
Experiments demonstrate that Twin-Co not only enhances user experience by
reducing trial-and-error iterations but also improves the quality of the
generated images, streamlining the creative process across various
applications.

</details>

### [377] [ReSpec: Relevance and Specificity Grounded Online Filtering for Learning on Video-Text Data Streams](https://arxiv.org/abs/2504.14875)
*Chris Dongjoo Kim,Jihwan Moon,Sangwoo Moon,Heeseung Yun,Sihaeng Lee,Aniruddha Kembhavi,Soonyoung Lee,Gunhee Kim,Sangho Lee,Christopher Clark*

Main category: cs.CV

TLDR: 提出ReSpec框架，通过在线学习实时筛选视频文本数据，提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决视频文本数据快速增长带来的存储和计算挑战，同时满足实时响应需求。

Method: 基于模态对齐、任务相关性、特异性和效率四个标准筛选数据。

Result: 在WebVid2M和VideoCC3M数据集上，仅用5%数据即达到最先进的零样本视频检索性能。

Conclusion: ReSpec框架高效且有效，适用于实时数据处理场景。

Abstract: The rapid growth of video-text data presents challenges in storage and
computation during training. Online learning, which processes streaming data in
real-time, offers a promising solution to these issues while also allowing
swift adaptations in scenarios demanding real-time responsiveness. One strategy
to enhance the efficiency and effectiveness of learning involves identifying
and prioritizing data that enhances performance on target downstream tasks. We
propose Relevance and Specificity-based online filtering framework (ReSpec)
that selects data based on four criteria: (i) modality alignment for clean
data, (ii) task relevance for target focused data, (iii) specificity for
informative and detailed data, and (iv) efficiency for low-latency processing.
Relevance is determined by the probabilistic alignment of incoming data with
downstream tasks, while specificity employs the distance to a root embedding
representing the least specific data as an efficient proxy for informativeness.
By establishing reference points from target task data, ReSpec filters incoming
data in real-time, eliminating the need for extensive storage and compute.
Evaluating on large-scale datasets WebVid2M and VideoCC3M, ReSpec attains
state-of-the-art performance on five zeroshot video retrieval tasks, using as
little as 5% of the data while incurring minimal compute. The source code is
available at https://github.com/cdjkim/ReSpec.

</details>

### [378] [Collaborative Enhancement Network for Low-quality Multi-spectral Vehicle Re-identification](https://arxiv.org/abs/2504.14877)
*Aihua Zheng,Yongqi Sun,Zi Wang,Chenglong Li,Jin Tang*

Main category: cs.CV

TLDR: 提出了一种协作增强网络（CoEN），通过生成高质量代理并动态选择主光谱，提升多光谱车辆重识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖主光谱增强低质量光谱数据，但主光谱选择困难且低质量主光谱会降低增强效果。

Method: 设计了代理生成器（PG）整合多光谱特征，动态质量排序模块（DQSM）选择主光谱，协作增强模块（CEM）补偿缺失内容。

Result: 在三个基准数据集上验证了方法的有效性。

Conclusion: CoEN通过协作增强和动态主光谱选择，显著提升了多光谱车辆重识别的鲁棒性。

Abstract: The performance of multi-spectral vehicle Re-identification (ReID) is
significantly degraded when some important discriminative cues in visible, near
infrared and thermal infrared spectra are lost. Existing methods generate or
enhance missing details in low-quality spectra data using the high-quality one,
generally called the primary spectrum, but how to justify the primary spectrum
is a challenging problem. In addition, when the quality of the primary spectrum
is low, the enhancement effect would be greatly degraded, thus limiting the
performance of multi-spectral vehicle ReID. To address these problems, we
propose the Collaborative Enhancement Network (CoEN), which generates a
high-quality proxy from all spectra data and leverages it to supervise the
selection of primary spectrum and enhance all spectra features in a
collaborative manner, for robust multi-spectral vehicle ReID. First, to
integrate the rich cues from all spectra data, we design the Proxy Generator
(PG) to progressively aggregate multi-spectral features. Second, we design the
Dynamic Quality Sort Module (DQSM), which sorts all spectra data by measuring
their correlations with the proxy, to accurately select the primary spectra
with the highest correlation. Finally, we design the Collaborative Enhancement
Module (CEM) to effectively compensate for missing contents of all spectra by
collaborating the primary spectra and the proxy, thereby mitigating the impact
of low-quality primary spectra. Extensive experiments on three benchmark
datasets are conducted to validate the efficacy of the proposed approach
against other multi-spectral vehicle ReID methods. The codes will be released
at https://github.com/yongqisun/CoEN.

</details>

### [379] [Memory-Augmented Dual-Decoder Networks for Multi-Class Unsupervised Anomaly Detection](https://arxiv.org/abs/2504.14884)
*Jingyu Xing,Chenwei Tang,Tao Wang,Rong Xiao,Wei Ju,Ji-Zhe Zhou,Liangli Zhen,Jiancheng Lv*

Main category: cs.CV

TLDR: 论文提出了一种记忆增强的双解码器网络（MDD-Net），用于解决多类无监督异常检测中的过泛化和正常特征重建不足问题。


<details>
  <summary>Details</summary>
Motivation: 多类场景下，重建方法面临过泛化和正常特征重建不足的挑战，现有方法通常只解决前者而加剧后者。

Method: MDD-Net包含双解码器反向蒸馏网络（DRD-Net）和类感知记忆模块（CMM），通过双解码器差异和类特定原型提升检测性能。

Result: 实验表明MDD-Net在多类无监督异常检测任务中优于现有方法。

Conclusion: MDD-Net有效解决了多类异常检测中的关键问题，提升了定位精度和减少误报。

Abstract: Recent advances in unsupervised anomaly detection (UAD) have shifted from
single-class to multi-class scenarios. In such complex contexts, the increasing
pattern diversity has brought two challenges to reconstruction-based
approaches: (1) over-generalization: anomalies that are subtle or share
compositional similarities with normal patterns may be reconstructed with high
fidelity, making them difficult to distinguish from normal instances; and (2)
insufficient normality reconstruction: complex normal features, such as
intricate textures or fine-grained structures, may not be faithfully
reconstructed due to the model's limited representational capacity, resulting
in false positives. Existing methods typically focus on addressing the former,
which unintentionally exacerbate the latter, resulting in inadequate
representation of intricate normal patterns. To concurrently address these two
challenges, we propose a Memory-augmented Dual-Decoder Networks (MDD-Net). This
network includes two critical components: a Dual-Decoder Reverse Distillation
Network (DRD-Net) and a Class-aware Memory Module (CMM). Specifically, the
DRD-Net incorporates a restoration decoder designed to recover normal features
from synthetic abnormal inputs and an identity decoder to reconstruct features
that maintain the anomalous semantics. By exploiting the discrepancy between
features produced by two decoders, our approach refines anomaly scores beyond
the conventional encoder-decoder comparison paradigm, effectively reducing
false positives and enhancing localization accuracy. Furthermore, the CMM
explicitly encodes and preserves class-specific normal prototypes, actively
steering the network away from anomaly reconstruction. Comprehensive
experimental results across several benchmarks demonstrate the superior
performance of our MDD-Net framework over current SoTA approaches in
multi-class UAD tasks.

</details>

### [380] [WMKA-Net: A Weighted Multi-Kernel Attention NetworkMethod for Retinal Vessel Segmentation](https://arxiv.org/abs/2504.14888)
*Xinran Xu,Yuliang Ma,Sifu Cai*

Main category: cs.CV

TLDR: WMKA-Net是一种新型视网膜血管分割网络，通过多核特征融合、渐进特征加权和注意力机制，显著提升了小血管和低对比度区域的分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决视网膜血管分割中多尺度特征捕捉不足、上下文信息丢失和噪声敏感性问题。

Method: 结合MKDC模块（多核并行卷积提取特征）、UDFF策略（加权融合高低层特征）和AttentionBlock（注意力机制抑制噪声）。

Result: 在多个公开数据集中表现出色，尤其是小血管分割和病理区域处理。

Conclusion: WMKA-Net为视网膜血管分割提供了一种高效且鲁棒的新方法。

Abstract: We propose a novel retinal vessel segmentation network, the Weighted
Multi-Kernel Attention Network (WMKA-Net), which aims to address the issues of
insufficient multiscale feature capture, loss of contextual information, and
noise sensitivity in retinal vessel segmentation. WMKA-Net significantly
improves the segmentation performance of small vessels and low-contrast regions
by integrating several innovative components, including the MultiKernelFeature
Fusion Module (MKDC), the Progressive Feature Weighting Fusion Strategy (UDFF),
and the Attention Mechanism Module (AttentionBlock). The MKDC module employs
multiscale parallel convolutional kernels to extract vessel characteristics,
thereby enhancing the ability to capture complex vascular structures. The UDFF
strategy optimizes the transmission of feature information by weighted fusion
of high- and low-level features. The AttentionBlock highlights key regions and
suppresses noise interference through the attention mechanism. Experimental
results demonstrate that WMKA-Net achieves excellent segmentation performance
in multiple public datasets, particularly in segmentation of small vessels and
processing of pathological regions. This work provides a robust and efficient
new method for segmentation of the retinal vessel.

</details>

### [381] [Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation](https://arxiv.org/abs/2504.14899)
*Chenjie Cao,Jingkai Zhou,Shikai Li,Jingyun Liang,Chaohui Yu,Fan Wang,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TLDR: Uni3C是一个统一的3D增强框架，用于视频生成中相机和人体运动的精确控制，通过点云和3D世界引导实现灵活训练和高质量生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独处理相机和人体运动控制，且依赖高质量标注数据，Uni3C旨在通过统一框架减少数据依赖并提升控制精度。

Method: 提出PCDController模块，利用点云实现相机控制；通过3D世界引导整合场景点云和SMPL-X角色，统一控制信号。

Result: PCDController在相机控制中表现稳健，Uni3C在相机可控性和人体运动质量上显著优于竞争对手。

Conclusion: Uni3C通过模块化设计和3D引导，实现了高效且高质量的视频生成控制，验证了其方法的有效性。

Abstract: Camera and human motion controls have been extensively studied for video
generation, but existing approaches typically address them separately,
suffering from limited data with high-quality annotations for both aspects. To
overcome this, we present Uni3C, a unified 3D-enhanced framework for precise
control of both camera and human motion in video generation. Uni3C includes two
key contributions. First, we propose a plug-and-play control module trained
with a frozen video generative backbone, PCDController, which utilizes
unprojected point clouds from monocular depth to achieve accurate camera
control. By leveraging the strong 3D priors of point clouds and the powerful
capacities of video foundational models, PCDController shows impressive
generalization, performing well regardless of whether the inference backbone is
frozen or fine-tuned. This flexibility enables different modules of Uni3C to be
trained in specific domains, i.e., either camera control or human motion
control, reducing the dependency on jointly annotated data. Second, we propose
a jointly aligned 3D world guidance for the inference phase that seamlessly
integrates both scenic point clouds and SMPL-X characters to unify the control
signals for camera and human motion, respectively. Extensive experiments
confirm that PCDController enjoys strong robustness in driving camera motion
for fine-tuned backbones of video generation. Uni3C substantially outperforms
competitors in both camera controllability and human motion quality.
Additionally, we collect tailored validation sets featuring challenging camera
movements and human actions to validate the effectiveness of our method.

</details>

### [382] [Guidelines for External Disturbance Factors in the Use of OCR in Real-World Environments](https://arxiv.org/abs/2504.14913)
*Kenji Iwata,Eiki Ishidera,Toshifumi Yamaai,Yutaka Satoh,Hiroshi Tanaka,Katsuhiko Takahashi,Akio Furuhata,Yoshihisa Tanabe,Hiroshi Matsumura*

Main category: cs.CV

TLDR: 论文总结了OCR性能下降的外部干扰因素，并整理为指南，以帮助用户正确使用OCR。


<details>
  <summary>Details</summary>
Motivation: 随着OCR应用范围的扩大，不同使用环境中的干扰可能导致其性能下降，影响识别精度，因此需要系统性总结这些干扰因素并提供解决方案。

Method: 通过整理实际使用中导致OCR性能下降的外部干扰因素及图像退化现象，编制成外部干扰因素表，并形成指南。

Result: 提供了外部干扰因素表和指南，帮助用户识别和应对OCR性能下降的问题。

Conclusion: 通过系统化整理干扰因素和提供指南，可以有效提升OCR在实际使用中的性能和可靠性。

Abstract: The performance of OCR has improved with the evolution of AI technology. As
OCR continues to broaden its range of applications, the increased likelihood of
interference introduced by various usage environments can prevent it from
achieving its inherent performance. This results in reduced recognition
accuracy under certain conditions, and makes the quality control of recognition
devices more challenging. Therefore, to ensure that users can properly utilize
OCR, we compiled the real-world external disturbance factors that cause
performance degradation, along with the resulting image degradation phenomena,
into an external disturbance factor table and, by also indicating how to make
use of it, organized them into guidelines.

</details>

### [383] [GenCLIP: Generalizing CLIP Prompts for Zero-shot Anomaly Detection](https://arxiv.org/abs/2504.14919)
*Donghyeong Kim,Chaewon Park,Suhwan Cho,Hyeonjeong Lim,Minseok Kang,Jungho Lee,Sangyoun Lee*

Main category: cs.CV

TLDR: GenCLIP提出了一种通过多层提示和双分支推理更有效地学习和利用通用提示的框架，以解决零样本异常检测中的挑战。


<details>
  <summary>Details</summary>
Motivation: 零样本异常检测（ZSAD）的关键挑战是学习稳定的通用提示并有效利用它们，同时保持泛化性和类别特异性。

Method: GenCLIP采用多层提示和双分支推理策略，结合通用提示和多层视觉特征，并通过自适应文本提示过滤机制优化输入。

Result: 该方法通过互补的双分支输出提高了异常检测的稳定性和可靠性，同时增强了泛化能力。

Conclusion: GenCLIP框架在零样本异常检测中表现出色，平衡了泛化性和类别特异性。

Abstract: Zero-shot anomaly detection (ZSAD) aims to identify anomalies in unseen
categories by leveraging CLIP's zero-shot capabilities to match text prompts
with visual features. A key challenge in ZSAD is learning general prompts
stably and utilizing them effectively, while maintaining both generalizability
and category specificity. Although general prompts have been explored in prior
works, achieving their stable optimization and effective deployment remains a
significant challenge. In this work, we propose GenCLIP, a novel framework that
learns and leverages general prompts more effectively through multi-layer
prompting and dual-branch inference. Multi-layer prompting integrates
category-specific visual cues from different CLIP layers, enriching general
prompts with more comprehensive and robust feature representations. By
combining general prompts with multi-layer visual features, our method further
enhances its generalization capability. To balance specificity and
generalization, we introduce a dual-branch inference strategy, where a
vision-enhanced branch captures fine-grained category-specific features, while
a query-only branch prioritizes generalization. The complementary outputs from
both branches improve the stability and reliability of anomaly detection across
unseen categories. Additionally, we propose an adaptive text prompt filtering
mechanism, which removes irrelevant or atypical class names not encountered
during CLIP's training, ensuring that only meaningful textual inputs contribute
to the final vision-language alignment.

</details>

### [384] [DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding](https://arxiv.org/abs/2504.14920)
*Geng Li,Jinglin Xu,Yunzhen Zhao,Yuxin Peng*

Main category: cs.CV

TLDR: Dyfo是一种无需训练的视觉搜索方法，通过双向交互和MCTS算法模拟人类视觉聚焦，提升多模态模型的细粒度视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 受人类视觉搜索机制启发，旨在解决现有方法需要额外模块或数据收集的问题。

Method: 利用双向交互和MCTS算法动态调整视觉焦点，无需额外训练或模块。

Result: 显著提升细粒度视觉理解，减少幻觉问题，在固定和动态分辨率模型中表现优异。

Conclusion: Dyfo是一种高效且无需训练的视觉搜索方法，适用于多模态模型。

Abstract: Humans can effortlessly locate desired objects in cluttered environments,
relying on a cognitive mechanism known as visual search to efficiently filter
out irrelevant information and focus on task-related regions. Inspired by this
process, we propose Dyfo (Dynamic Focus), a training-free dynamic focusing
visual search method that enhances fine-grained visual understanding in large
multimodal models (LMMs). Unlike existing approaches which require additional
modules or data collection, Dyfo leverages a bidirectional interaction between
LMMs and visual experts, using a Monte Carlo Tree Search (MCTS) algorithm to
simulate human-like focus adjustments. This enables LMMs to focus on key visual
regions while filtering out irrelevant content, without introducing additional
training caused by vocabulary expansion or the integration of specialized
localization modules. Experimental results demonstrate that Dyfo significantly
improves fine-grained visual understanding and reduces hallucination issues in
LMMs, achieving superior performance across both fixed and dynamic resolution
models. The code is available at https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025

</details>

### [385] [Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos](https://arxiv.org/abs/2504.14921)
*Songping Wang,Hanqing Liu,Yueming Lyu,Xiantao Hu,Ziwen He,Wei Wang,Caifeng Shan,Liang Wang*

Main category: cs.CV

TLDR: VFAT-WS是一种快速对抗训练方法，通过时间频率增强和弱到强一致性正则化，显著提升视频模型的对抗鲁棒性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频对抗训练方法计算成本高且难以平衡干净准确性和对抗鲁棒性，VFAT-WS旨在解决这些问题。

Method: 结合时间频率增强（TF-AUG）及其时空增强版本（STF-AUG），以及单步PGD攻击，并设计弱到强一致性正则化。

Result: 在UCF-101和HMDB-51数据集上，VFAT-WS显著提升对抗鲁棒性和训练速度（加速490%）。

Conclusion: VFAT-WS在高效性和鲁棒性之间取得了更好的平衡，为视频对抗训练提供了实用解决方案。

Abstract: Adversarial Training (AT) has been shown to significantly enhance adversarial
robustness via a min-max optimization approach. However, its effectiveness in
video recognition tasks is hampered by two main challenges. First, fast
adversarial training for video models remains largely unexplored, which
severely impedes its practical applications. Specifically, most video
adversarial training methods are computationally costly, with long training
times and high expenses. Second, existing methods struggle with the trade-off
between clean accuracy and adversarial robustness. To address these challenges,
we introduce Video Fast Adversarial Training with Weak-to-Strong consistency
(VFAT-WS), the first fast adversarial training method for video data.
Specifically, VFAT-WS incorporates the following key designs: First, it
integrates a straightforward yet effective temporal frequency augmentation
(TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a
single-step PGD attack to boost training efficiency and robustness. Second, it
devises a weak-to-strong spatial-temporal consistency regularization, which
seamlessly integrates the simpler TF-AUG and the more complex STF-AUG.
Leveraging the consistency regularization, it steers the learning process from
simple to complex augmentations. Both of them work together to achieve a better
trade-off between clean accuracy and robustness. Extensive experiments on
UCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that
VFAT-WS achieves great improvements in adversarial robustness and corruption
robustness, while accelerating training by nearly 490%.

</details>

### [386] [TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models](https://arxiv.org/abs/2504.14933)
*Mazharul Islam Rakib,Showrin Rahman,Joyanta Jyoti Mondal,Xi Xiao,David Lewis,Alessandra Mileo,Meem Arafat Manab*

Main category: cs.CV

TLDR: 提出了一种基于条件扩散模型的两步图像生成方法，通过生成图像分割掩码并避免特定形状，有效减少结构相似性，从而避免版权侵权和源复制问题。


<details>
  <summary>Details</summary>
Motivation: 解决生成AI模型在图像生成中可能侵犯版权或直接复制训练图像（源复制）的问题，传统方法如水印和元数据效果有限。

Method: 采用两步法：首先生成图像分割掩码捕捉形状，然后扩散模型重新生成图像时避免该形状，降低与训练图像的结构相似性。

Result: 该方法显著减少了结构相似性，无需昂贵模型重训练或用户提示生成技术，有效避免版权侵权和源复制。

Conclusion: 提出的方法是一种计算成本低、高效的解决方案，适用于基于扩散模型的图像生成，避免版权问题。

Abstract: In today's age of social media and marketing, copyright issues can be a major
roadblock to the free sharing of images. Generative AI models have made it
possible to create high-quality images, but concerns about copyright
infringement are a hindrance to their abundant use. As these models use data
from training images to generate new ones, it is often a daunting task to
ensure they do not violate intellectual property rights. Some AI models have
even been noted to directly copy copyrighted images, a problem often referred
to as source copying. Traditional copyright protection measures such as
watermarks and metadata have also proven to be futile in this regard. To
address this issue, we propose a novel two-step image generation model inspired
by the conditional diffusion model. The first step involves creating an image
segmentation mask for some prompt-based generated images. This mask embodies
the shape of the image. Thereafter, the diffusion model is asked to generate
the image anew while avoiding the shape in question. This approach shows a
decrease in structural similarity from the training image, i.e. we are able to
avoid the source copying problem using this approach without expensive
retraining of the model or user-centered prompt generation techniques. This
makes our approach the most computationally inexpensive approach to avoiding
both copyright infringement and source copying for diffusion model-based image
generation.

</details>

### [387] [PIV-FlowDiffuser:Transfer-learning-based denoising diffusion models for PIV](https://arxiv.org/abs/2504.14952)
*Qianyu Zhu,Junjie Wang,Jeremiah Hu,Jia Ai,Yong Lee*

Main category: cs.CV

TLDR: 该论文提出了一种基于去噪扩散模型（FlowDiffuser）的PIV分析方法，通过迁移学习策略训练模型，显著降低了噪声并提高了性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在PIV中表现优异，但合成数据训练的模型在实际应用中因领域差异性能下降，导致向量场出现特殊噪声模式。

Method: 采用去噪扩散模型（FlowDiffuser），通过迁移学习策略分两步训练：先在计算机视觉数据集（如Sintel、KITTI）上预训练，再在合成PIV数据集上微调。

Result: PIV-FlowDiffuser有效抑制噪声，将平均端点误差（AEE）降低59.4%，并在未见过的粒子图像上表现出更好的泛化性能。

Conclusion: 研究表明，基于迁移学习的去噪扩散模型在PIV中具有潜力，建议读者参考详细实现。

Abstract: Deep learning algorithms have significantly reduced the computational time
and improved the spatial resolution of particle image velocimetry~(PIV).
However, the models trained on synthetic datasets might have a degraded
performance on practical particle images due to domain gaps. As a result,
special residual patterns are often observed for the vector fields of deep
learning-based estimators. To reduce the special noise step-by-step, we employ
a denoising diffusion model~(FlowDiffuser) for PIV analysis. And the
data-hungry iterative denoising diffusion model is trained via a transfer
learning strategy, resulting in our PIV-FlowDiffuser method. Specifically, (1)
pre-training a FlowDiffuser model with multiple optical flow datasets of the
computer vision community, such as Sintel, KITTI, etc; (2) fine-tuning the
pre-trained model on synthetic PIV datasets. Note that the PIV images are
upsampled by a factor of two to resolve the small-scale turbulent flow
structures. The visualized results indicate that our PIV-FlowDiffuser
effectively suppresses the noise patterns. Therefore, the denoising diffusion
model reduces the average end-point error~($AEE$) by 59.4% over RAFT256-PIV
baseline on the classic Cai's dataset. Besides, PIV-FlowDiffuser exhibits
enhanced generalization performance on unseen particle images due to transfer
learning. Overall, this study highlights the transfer-learning-based denoising
diffusion models for PIV. And a detailed implementation is recommended for
interested readers in the repository
https://github.com/Zhu-Qianyu/PIV-FlowDiffuser.

</details>

### [388] [3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact Tensorial Representations](https://arxiv.org/abs/2504.14967)
*Yating Wang,Xuan Wang,Ran Yi,Yanbo Fan,Jichen Hu,Jingcheng Zhu,Lizhuang Ma*

Main category: cs.CV

TLDR: 提出了一种结合3D高斯和3DMM的新方法，通过紧凑的张量表示和动态纹理编码，实现了高质量3D头部虚拟形象的实时渲染和低存储成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态纹理捕捉、运行速度或存储空间方面存在不足，限制了应用场景。

Method: 采用张量格式编码纹理属性，静态三平面存储中性表情外观，轻量级1D特征线表示动态纹理细节，并提出自适应截断不透明度惩罚和类平衡采样。

Result: 实验表明，该方法能准确捕捉面部动态细节，保持实时渲染，并显著降低存储成本。

Conclusion: 该方法解决了现有技术的局限性，扩展了3D头部虚拟形象的应用范围。

Abstract: Recent studies have combined 3D Gaussian and 3D Morphable Models (3DMM) to
construct high-quality 3D head avatars. In this line of research, existing
methods either fail to capture the dynamic textures or incur significant
overhead in terms of runtime speed or storage space. To this end, we propose a
novel method that addresses all the aforementioned demands. In specific, we
introduce an expressive and compact representation that encodes texture-related
attributes of the 3D Gaussians in the tensorial format. We store appearance of
neutral expression in static tri-planes, and represents dynamic texture details
for different expressions using lightweight 1D feature lines, which are then
decoded into opacity offset relative to the neutral face. We further propose
adaptive truncated opacity penalty and class-balanced sampling to improve
generalization across different expressions. Experiments show this design
enables accurate face dynamic details capturing while maintains real-time
rendering and significantly reduces storage costs, thus broadening the
applicability to more scenarios.

</details>

### [389] [Cyc3D: Fine-grained Controllable 3D Generation via Cycle Consistency Regularization](https://arxiv.org/abs/2504.14975)
*Hongbin Xu,Chaohui Yu,Feng Xiao,Jiazheng Xing,Hai Ci,Weitao Chen,Ming Li*

Main category: cs.CV

TLDR: 论文提出了一种名为\name{}的新框架，通过循环一致性增强可控3D生成，解决了现有方法在输入条件与生成内容对齐上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法在输入条件（如边缘和深度）与生成内容的一致性上存在显著差距，导致明显的不对齐问题。

Method: 采用高效的前馈主干网络，通过循环过程（包括视图一致性和条件一致性约束）生成3D内容，确保输入条件与生成内容的一致性。

Result: 在多个基准测试中，\name{}显著提升了可控性，尤其在细粒度细节上表现优异（如边缘PSNR提升14.17%，草图PSNR提升6.26%）。

Conclusion: \name{}通过循环一致性约束有效提升了3D生成的可控性，为未来研究提供了新方向。

Abstract: Despite the remarkable progress of 3D generation, achieving controllability,
i.e., ensuring consistency between generated 3D content and input conditions
like edge and depth, remains a significant challenge. Existing methods often
struggle to maintain accurate alignment, leading to noticeable discrepancies.
To address this issue, we propose \name{}, a new framework that enhances
controllable 3D generation by explicitly encouraging cyclic consistency between
the second-order 3D content, generated based on extracted signals from the
first-order generation, and its original input controls. Specifically, we
employ an efficient feed-forward backbone that can generate a 3D object from an
input condition and a text prompt. Given an initial viewpoint and a control
signal, a novel view is rendered from the generated 3D content, from which the
extracted condition is used to regenerate the 3D content. This re-generated
output is then rendered back to the initial viewpoint, followed by another
round of control signal extraction, forming a cyclic process with two
consistency constraints. \emph{View consistency} ensures coherence between the
two generated 3D objects, measured by semantic similarity to accommodate
generative diversity. \emph{Condition consistency} aligns the final extracted
signal with the original input control, preserving structural or geometric
details throughout the process. Extensive experiments on popular benchmarks
demonstrate that \name{} significantly improves controllability, especially for
fine-grained details, outperforming existing methods across various conditions
(e.g., +14.17\% PSNR for edge, +6.26\% PSNR for sketch).

</details>

### [390] [RealisDance-DiT: Simple yet Strong Baseline towards Controllable Character Animation in the Wild](https://arxiv.org/abs/2504.14977)
*Jingkai Zhou,Yifan Wu,Shikai Li,Min Wei,Chao Fan,Weihua Chen,Wei Jiang,Fan Wang*

Main category: cs.CV

TLDR: 论文提出了一种基于强大基础模型的简单修改方法（RealisDance-DiT），通过灵活的微调策略解决可控角色动画中的挑战，并在实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决可控角色动画中罕见姿势、风格化角色、角色-物体交互、复杂光照和动态场景等挑战，现有方法难以泛化到开放世界场景。

Method: 基于Wan-2.1视频基础模型，提出RealisDance-DiT，通过最小化基础模型架构修改，结合低噪声预热和“大批量小迭代”策略加速微调。

Result: 在实验中，RealisDance-DiT显著优于现有方法，并引入新测试数据集以全面评估性能。

Conclusion: 研究表明，强大基础模型结合简单修改和灵活微调策略，可以有效解决可控角色动画的开放世界挑战。

Abstract: Controllable character animation remains a challenging problem, particularly
in handling rare poses, stylized characters, character-object interactions,
complex illumination, and dynamic scenes. To tackle these issues, prior work
has largely focused on injecting pose and appearance guidance via elaborate
bypass networks, but often struggles to generalize to open-world scenarios. In
this paper, we propose a new perspective that, as long as the foundation model
is powerful enough, straightforward model modifications with flexible
fine-tuning strategies can largely address the above challenges, taking a step
towards controllable character animation in the wild. Specifically, we
introduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our
sufficient analysis reveals that the widely adopted Reference Net design is
suboptimal for large-scale DiT models. Instead, we demonstrate that minimal
modifications to the foundation model architecture yield a surprisingly strong
baseline. We further propose the low-noise warmup and "large batches and small
iterations" strategies to accelerate model convergence during fine-tuning while
maximally preserving the priors of the foundation model. In addition, we
introduce a new test dataset that captures diverse real-world challenges,
complementing existing benchmarks such as TikTok dataset and UBC fashion video
dataset, to comprehensively evaluate the proposed method. Extensive experiments
show that RealisDance-DiT outperforms existing methods by a large margin.

</details>

### [391] [Benchmarking Large Vision-Language Models on Fine-Grained Image Tasks: A Comprehensive Evaluation](https://arxiv.org/abs/2504.14988)
*Hong-Tao Yu,Xiu-Shen Wei,Yuxin Peng,Serge Belongie*

Main category: cs.CV

TLDR: 该论文提出了一个名为FG-BMK的细粒度评估基准，用于评估大型视觉语言模型（LVLMs）在细粒度图像任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs在多模态感知方面取得了显著进展，但细粒度图像任务的评估仍然不足。

Method: 通过构建包含349万问题和332万图像的FG-BMK基准，从人类和机器视角系统评估LVLMs的语义识别和细粒度特征表示能力。

Result: 实验揭示了训练范式、模态对齐、扰动敏感性和细粒度类别推理对任务性能的影响。

Conclusion: 研究为当前LVLMs的局限性提供了关键见解，并为未来数据构建和模型设计提供了指导。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated
remarkable multimodal perception capabilities, garnering significant attention.
While numerous evaluation studies have emerged, assessing LVLMs both
holistically and on specialized tasks, fine-grained image tasks-fundamental to
computer vision-remain largely unexplored. To fill this gap, we introduce a
comprehensive fine-grained evaluation benchmark, i.e., FG-BMK, comprising 3.49
million questions and 3.32 million images. Our evaluation systematically
examines LVLMs from both human-oriented and machine-oriented perspectives,
focusing on their semantic recognition and fine-grained feature representation
capabilities. Through extensive experiments on eight representative LVLMs/VLMs,
we uncover key findings regarding the influence of training paradigms, modality
alignment, perturbation susceptibility, and fine-grained category reasoning on
task performance. This work provides critical insights into the limitations of
current LVLMs and offers guidance for future data construction and model design
in the development of more advanced LVLMs. Our code is open-source and
available at https://github.com/SEU-VIPGroup/FG-BMK.

</details>

### [392] [NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: KwaiSR Dataset and Study](https://arxiv.org/abs/2504.15003)
*Xin Li,Xijun Wang,Bingchen Li,Kun Yuan,Yizhen Shao,Suhang Yao,Ming Sun,Chao Zhou,Radu Timofte,Zhibo Chen*

Main category: cs.CV

TLDR: KwaiSR是首个针对短格式UGC图像超分辨率的基准数据集，包含合成和野生两部分，用于推动相关算法研究。


<details>
  <summary>Details</summary>
Motivation: 为短格式UGC平台开发图像超分辨率算法提供基准数据集。

Method: 数据集包含合成和野生图像，合成部分模拟真实退化，野生部分直接采集并过滤。

Result: KwaiSR数据集挑战现有超分辨率方法，有望引领新研究方向。

Conclusion: KwaiSR为图像超分辨率领域提供了有价值的基准，并推动了相关竞赛和研究。

Abstract: In this work, we build the first benchmark dataset for short-form UGC Image
Super-resolution in the wild, termed KwaiSR, intending to advance the research
on developing image super-resolution algorithms for short-form UGC platforms.
This dataset is collected from the Kwai Platform, which is composed of two
parts, i.e., synthetic and wild parts. Among them, the synthetic dataset,
including 1,900 image pairs, is produced by simulating the degradation
following the distribution of real-world low-quality short-form UGC images,
aiming to provide the ground truth for training and objective comparison in the
validation/testing. The wild dataset contains low-quality images collected
directly from the Kwai Platform, which are filtered using the quality
assessment method KVQ from the Kwai Platform. As a result, the KwaiSR dataset
contains 1800 synthetic image pairs and 1900 wild images, which are divided
into training, validation, and testing parts with a ratio of 8:1:1. Based on
the KwaiSR dataset, we organize the NTIRE 2025 challenge on a second short-form
UGC Video quality assessment and enhancement, which attracts lots of
researchers to develop the algorithm for it. The results of this competition
have revealed that our KwaiSR dataset is pretty challenging for existing Image
SR methods, which is expected to lead to a new direction in the image
super-resolution field. The dataset can be found from
https://lixinustc.github.io/NTIRE2025-KVQE-KwaSR-KVQ.github.io/.

</details>

### [393] [An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes](https://arxiv.org/abs/2504.15270)
*Ji Qi,Yuan Yao,Yushi Bai,Bin Xu,Juanzi Li,Zhiyuan Liu,Tat-Seng Chua*

Main category: cs.CV

TLDR: Quicksviewer是一种新型大型多模态模型（LMM），通过动态分区和统一重采样视频帧，显著提高计算效率，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有LMM在处理视频时因均匀感知帧导致计算效率低下，无法适应视频中时间信息密度的不均匀性。

Method: 使用Gumbel Softmax将视频分区为不同密度的块，并对每块进行统一重采样，实现动态压缩和高效训练。

Result: 模型在仅0.8M视频-文本样本训练下，性能超越基线8.72分，压缩率高达45倍，且在Video-MME上达到SOTA。

Conclusion: Quicksviewer通过动态分区和高效训练，显著提升视频理解效率，同时揭示了输入帧数与模型能力的幂律关系。

Abstract: Large Multimodal Models (LMMs) uniformly perceive video frames, creating
computational inefficiency for videos with inherently varying temporal
information density. This paper present \textbf{Quicksviewer}, an LMM with new
perceiving paradigm that partitions a video of nonuniform density into varying
cubes using Gumbel Softmax, followed by a unified resampling for each cube to
achieve efficient video understanding. This simple and intuitive approach
dynamically compress video online based on its temporal density, significantly
reducing spatiotemporal redundancy (overall 45$\times$ compression rate), while
enabling efficient training with large receptive field. We train the model from
a language backbone through three progressive stages, each incorporating
lengthy videos on average of 420s/1fps thanks to the perceiving efficiency.
With only 0.8M total video-text samples for training, our model outperforms the
direct baseline employing a fixed partitioning strategy by a maximum of 8.72 in
accuracy, demonstrating the effectiveness in performance. On Video-MME,
Quicksviewer achieves SOTA under modest sequence lengths using just up to 5\%
of tokens per frame required by baselines. With this paradigm, scaling up the
number of input frames reveals a clear power law of the model capabilities. It
is also empirically verified that the segments generated by the cubing network
can help for analyzing continuous events in videos.

</details>

### [394] [Shifts in Doctors' Eye Movements Between Real and AI-Generated Medical Images](https://arxiv.org/abs/2504.15007)
*David C Wong,Bin Wang,Gorkem Durak,Marouane Tliba,Mohamed Amine Kerkouri,Aladine Chetouani,Ahmet Enis Cetin,Cagdas Topel,Nicolo Gennaro,Camila Vendrami,Tugce Agirlar Trabzonlu,Amir Ali Rahsepar,Laetitia Perronne,Matthew Antalek,Onural Ozturk,Gokcan Okur,Andrew C. Gordon,Ayis Pyrros,Frank H Miller,Amir A Borhani,Hatice Savas,Eric M. Hart*

Main category: cs.CV

TLDR: 通过眼动追踪分析放射科医生在真实与深度学习生成图像上的注意力分配和诊断策略差异。


<details>
  <summary>Details</summary>
Motivation: 研究放射科医生在真实与合成图像上的视觉注意力差异，以揭示诊断行为的变化。

Method: 分析眼动模式（如扫视方向、幅度及其联合分布）和注视偏差图，比较真实与合成图像的注视分布和视觉显著性。

Result: 揭示了放射科医生在真实与合成图像上的注意力分配和诊断策略的差异。

Conclusion: 眼动追踪分析为理解放射科医生的视觉诊断行为提供了重要工具，尤其在区分真实与合成图像时表现出显著差异。

Abstract: Eye-tracking analysis plays a vital role in medical imaging, providing key
insights into how radiologists visually interpret and diagnose clinical cases.
In this work, we first analyze radiologists' attention and agreement by
measuring the distribution of various eye-movement patterns, including saccades
direction, amplitude, and their joint distribution. These metrics help uncover
patterns in attention allocation and diagnostic strategies. Furthermore, we
investigate whether and how doctors' gaze behavior shifts when viewing
authentic (Real) versus deep-learning-generated (Fake) images. To achieve this,
we examine fixation bias maps, focusing on first, last, short, and longest
fixations independently, along with detailed saccades patterns, to quantify
differences in gaze distribution and visual saliency between authentic and
synthetic images.

</details>

### [395] [Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs](https://arxiv.org/abs/2504.15280)
*Chun-Hsiao Yeh,Chenyu Wang,Shengbang Tong,Ta-Ying Cheng,Rouyu Wang,Tianzhe Chu,Yuexiang Zhai,Yubei Chen,Shenghua Gao,Yi Ma*

Main category: cs.CV

TLDR: 论文提出了All-Angles Bench基准，用于评估多模态大语言模型（MLLMs）在多视角场景理解中的表现，发现当前模型在跨视角一致性和相机姿态估计方面显著落后于人类水平。


<details>
  <summary>Details</summary>
Motivation: 多视角理解是MLLMs作为具身代理的关键能力，但现有模型在多视角几何一致性和跨视角对应方面表现不足，需要系统评估和改进。

Method: 通过构建包含2,100个人工标注的多视角问答对的基准（All-Angles Bench），设计六项任务测试模型的几何对应和跨视角信息对齐能力，并对27种MLLMs进行实验。

Result: 实验显示MLLMs在部分遮挡视角的跨视角对应和粗略相机姿态估计方面表现较差，与人类水平存在显著差距。

Conclusion: 研究强调了增强MLLMs多视角感知能力的必要性，All-Angles Bench为改进提供了重要参考。

Abstract: Multi-view understanding, the ability to reconcile visual information across
diverse viewpoints for effective navigation, manipulation, and 3D scene
comprehension, is a fundamental challenge in Multi-Modal Large Language Models
(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive
advances in high-level reasoning and planning, they frequently fall short when
confronted with multi-view geometric consistency and cross-view correspondence.
To comprehensively evaluate the challenges of MLLMs in multi-view scene
reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human
carefully annotated multi-view question-answer pairs across 90 diverse
real-world scenes. Our six tasks (counting, attribute identification, relative
distance, relative direction, object manipulation, and camera pose estimation)
specifically test model's geometric correspondence and the capacity to align
information consistently across views. Our extensive experiments, benchmark on
27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and
GPT-4o against human evaluators reveals a substantial performance gap,
indicating that current MLLMs remain far from human-level proficiency. Through
in-depth analysis, we show that MLLMs are particularly underperforming under
two aspects: (1) cross-view correspondence for partially occluded views and (2)
establishing the coarse camera poses. These findings highlight the necessity of
domain-specific refinements or modules that embed stronger multi-view
awareness. We believe that our All-Angles Bench offers valuable insights and
contribute to bridging the gap between MLLMs and human-level multi-view
understanding. The project and benchmark are publicly available at
https://danielchyeh.github.io/All-Angles-Bench/.

</details>

### [396] [Insert Anything: Image Insertion via In-Context Editing in DiT](https://arxiv.org/abs/2504.15009)
*Wensong Song,Hong Jiang,Zongxing Yang,Ruijie Quan,Yi Yang*

Main category: cs.CV

TLDR: Insert Anything是一个统一的框架，用于基于参考的图像插入，通过用户指定的控制指导将对象无缝集成到目标场景中。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要为不同任务训练单独模型，而Insert Anything通过一个统一的框架和数据集AnyInsertion（包含12万对提示-图像对）实现广泛的任务泛化。

Method: 利用Diffusion Transformer（DiT）的多模态注意力支持掩码和文本引导编辑，并引入上下文编辑机制，通过两种提示策略协调插入元素与目标场景。

Result: 在AnyInsertion、DreamBooth和VTON-HD基准测试中表现优于现有方法。

Conclusion: Insert Anything在创意内容生成、虚拟试穿和场景合成等实际应用中具有巨大潜力。

Abstract: This work presents Insert Anything, a unified framework for reference-based
image insertion that seamlessly integrates objects from reference images into
target scenes under flexible, user-specified control guidance. Instead of
training separate models for individual tasks, our approach is trained once on
our new AnyInsertion dataset--comprising 120K prompt-image pairs covering
diverse tasks such as person, object, and garment insertion--and effortlessly
generalizes to a wide range of insertion scenarios. Such a challenging setting
requires capturing both identity features and fine-grained details, while
allowing versatile local adaptations in style, color, and texture. To this end,
we propose to leverage the multimodal attention of the Diffusion Transformer
(DiT) to support both mask- and text-guided editing. Furthermore, we introduce
an in-context editing mechanism that treats the reference image as contextual
information, employing two prompting strategies to harmonize the inserted
elements with the target scene while faithfully preserving their distinctive
features. Extensive experiments on AnyInsertion, DreamBooth, and VTON-HD
benchmarks demonstrate that our method consistently outperforms existing
alternatives, underscoring its great potential in real-world applications such
as creative content generation, virtual try-on, and scene composition.

</details>

### [397] [DyST-XL: Dynamic Layout Planning and Content Control for Compositional Text-to-Video Generation](https://arxiv.org/abs/2504.15032)
*Weijie He,Mushui Liu,Yunlong Yu,Zhao Wang,Chao Wu*

Main category: cs.CV

TLDR: DyST-XL是一个无需训练的框架，通过动态布局规划、双提示控制注意力机制和实体一致性约束，显著提升了文本到视频生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成动态场景时存在布局不连续、实体身份漂移和不合理的交互动态问题，DyST-XL旨在解决这些限制。

Method: DyST-XL结合动态布局规划器、双提示控制注意力机制和实体一致性约束策略，优化文本到视频生成。

Result: 实验表明，DyST-XL在复杂提示下表现优异，显著提升了训练无关的视频合成能力。

Conclusion: DyST-XL填补了训练无关视频合成的关键空白，为文本到视频生成提供了高效解决方案。

Abstract: Compositional text-to-video generation, which requires synthesizing dynamic
scenes with multiple interacting entities and precise spatial-temporal
relationships, remains a critical challenge for diffusion-based models.
Existing methods struggle with layout discontinuity, entity identity drift, and
implausible interaction dynamics due to unconstrained cross-attention
mechanisms and inadequate physics-aware reasoning. To address these
limitations, we propose DyST-XL, a \textbf{training-free} framework that
enhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through
frame-aware control. DyST-XL integrates three key innovations: (1) A Dynamic
Layout Planner that leverages large language models (LLMs) to parse input
prompts into entity-attribute graphs and generates physics-aware keyframe
layouts, with intermediate frames interpolated via trajectory optimization; (2)
A Dual-Prompt Controlled Attention Mechanism that enforces localized text-video
alignment through frame-aware attention masking, achieving the precise control
over individual entities; and (3) An Entity-Consistency Constraint strategy
that propagates first-frame feature embeddings to subsequent frames during
denoising, preserving object identity without manual annotation. Experiments
demonstrate that DyST-XL excels in compositional text-to-video generation,
significantly improving performance on complex prompts and bridging a crucial
gap in training-free video synthesis.

</details>

### [398] [Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong Person Re-identification](https://arxiv.org/abs/2504.15041)
*Shiben Liu,Huijie Fan,Qiang Wang,Baojie Fan,Yandong Tang,Liangqiong Qu*

Main category: cs.CV

TLDR: 提出了一种名为DAFC的新模型，通过文本驱动的提示聚合（TPA）和基于分布的感知与整合（DAI）解决LReID中的遗忘问题，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决终身行人重识别（LReID）中旧知识保留与新信息适应的挑战，现有方法存在遗忘或分布学习不足的问题。

Method: 提出DAFC模型，结合TPA（文本驱动的提示聚合）和DAI（基于分布的感知与整合），并引入KCM（知识巩固机制）。

Result: 实验结果显示DAFC在两种训练顺序下平均mAP/R@1分别提升至少9.8%/6.6%和6.4%/6.2%。

Conclusion: DAFC通过跨域共享表示学习和领域特定分布整合，有效缓解灾难性遗忘，性能显著优于现有方法。

Abstract: Lifelong Person Re-identification (LReID) suffers from a key challenge in
preserving old knowledge while adapting to new information. The existing
solutions include rehearsal-based and rehearsal-free methods to address this
challenge. Rehearsal-based approaches rely on knowledge distillation,
continuously accumulating forgetting during the distillation process.
Rehearsal-free methods insufficiently learn the distribution of each domain,
leading to forgetfulness over time. To solve these issues, we propose a novel
Distribution-aware Forgetting Compensation (DAFC) model that explores
cross-domain shared representation learning and domain-specific distribution
integration without using old exemplars or knowledge distillation. We propose a
Text-driven Prompt Aggregation (TPA) that utilizes text features to enrich
prompt elements and guide the prompt model to learn fine-grained
representations for each instance. This can enhance the differentiation of
identity information and establish the foundation for domain distribution
awareness. Then, Distribution-based Awareness and Integration (DAI) is designed
to capture each domain-specific distribution by a dedicated expert network and
adaptively consolidate them into a shared region in high-dimensional space. In
this manner, DAI can consolidate and enhance cross-domain shared representation
learning while alleviating catastrophic forgetting. Furthermore, we develop a
Knowledge Consolidation Mechanism (KCM) that comprises instance-level
discrimination and cross-domain consistency alignment strategies to facilitate
model adaptive learning of new knowledge from the current domain and promote
knowledge consolidation learning between acquired domain-specific
distributions, respectively. Experimental results show that our DAFC outperform
state-of-the-art methods by at least 9.8\%/6.6\% and 6.4\%/6.2\% of average
mAP/R@1 on two training orders.

</details>

### [399] [ScanEdit: Hierarchically-Guided Functional 3D Scan Editing](https://arxiv.org/abs/2504.15049)
*Mohamed el amine Boudjoghra,Ivan Laptev,Angela Dai*

Main category: cs.CV

TLDR: ScanEdit是一种基于指令驱动的3D扫描编辑方法，利用层次化场景图和大型语言模型（LLM）实现高效编辑，并结合物理约束生成逼真场景。


<details>
  <summary>Details</summary>
Motivation: 随着3D捕获技术的快速发展，3D数据大量涌现，高效的3D场景编辑成为图形应用的关键需求。

Method: 通过构建层次化场景图表示3D扫描对象，利用LLM将语言指令转化为可操作的层次化编辑命令，并结合物理约束生成逼真场景。

Result: ScanEdit在实验中表现优于现有技术，适用于多种真实场景和输入指令。

Conclusion: ScanEdit为复杂3D扫描的功能编辑提供了一种高效且逼真的解决方案。

Abstract: With the fast pace of 3D capture technology and resulting abundance of 3D
data, effective 3D scene editing becomes essential for a variety of graphics
applications. In this work we present ScanEdit, an instruction-driven method
for functional editing of complex, real-world 3D scans. To model large and
interdependent sets of ob- jectswe propose a hierarchically-guided approach.
Given a 3D scan decomposed into its object instances, we first construct a
hierarchical scene graph representation to enable effective, tractable editing.
We then leverage reason- ing capabilities of Large Language Models (LLMs) and
translate high-level language instructions into actionable commands applied
hierarchically to the scene graph. Fi- nally, ScanEdit integrates LLM-based
guidance with ex- plicit physical constraints and generates realistic scenes
where object arrangements obey both physics and common sense. In our extensive
experimental evaluation ScanEdit outperforms state of the art and demonstrates
excellent re- sults for a variety of real-world scenes and input instruc-
tions.

</details>

### [400] [Structure-guided Diffusion Transformer for Low-Light Image Enhancement](https://arxiv.org/abs/2504.15054)
*Xiangchen Yin,Zhenda Yu,Longtao Jiang,Xin Gao,Xiao Sun,Zhi Liu,Xun Yang*

Main category: cs.CV

TLDR: 本文首次将扩散变换器（DiT）引入低光图像增强任务，提出了一种基于结构引导的扩散变换器框架（SDTL），通过小波变换和结构增强模块（SEM）提升模型效率和纹理增强效果，同时在噪声预测中避免干扰。实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前低光图像增强方法在恢复细节时不可避免地放大噪声，导致视觉质量差。本文旨在探索DiT在此任务中的应用，以提升图像质量。

Method: 提出SDTL框架，结合小波变换压缩特征以提升效率，设计SEM模块利用结构先验增强纹理，并引入SAB模块在噪声预测中关注纹理丰富区域。

Result: 在多个流行数据集上的实验表明，SDTL在图像质量提升方面达到SOTA性能。

Conclusion: SDTL验证了DiT在低光增强任务中的潜力，并显著提升了图像质量。

Abstract: While the diffusion transformer (DiT) has become a focal point of interest in
recent years, its application in low-light image enhancement remains a blank
area for exploration. Current methods recover the details from low-light images
while inevitably amplifying the noise in images, resulting in poor visual
quality. In this paper, we firstly introduce DiT into the low-light enhancement
task and design a novel Structure-guided Diffusion Transformer based Low-light
image enhancement (SDTL) framework. We compress the feature through wavelet
transform to improve the inference efficiency of the model and capture the
multi-directional frequency band. Then we propose a Structure Enhancement
Module (SEM) that uses structural prior to enhance the texture and leverages an
adaptive fusion strategy to achieve more accurate enhancement effect. In
Addition, we propose a Structure-guided Attention Block (SAB) to pay more
attention to texture-riched tokens and avoid interference from noisy areas in
noise prediction. Extensive qualitative and quantitative experiments
demonstrate that our method achieves SOTA performance on several popular
datasets, validating the effectiveness of SDTL in improving image quality and
the potential of DiT in low-light enhancement tasks.

</details>

### [401] [Hierarchical Attention Fusion of Visual and Textual Representations for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2504.15085)
*Wangyu Wu,Zhenhong Chen,Siqi Song,Xianglin Qiua,Xiaowei Huang,Fei Ma,Jimin Xiao*

Main category: cs.CV

TLDR: HAF-VT是一种结合视觉和文本数据的跨域序列推荐方法，通过分层注意力机制模拟人类认知过程，提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 跨域序列推荐（CDSR）需要更好地建模用户跨域偏好，而现有方法未充分利用多模态数据。

Method: 使用冻结的CLIP模型生成图像和文本嵌入，通过分层注意力机制联合学习单域和跨域偏好。

Result: 在四个电商数据集上，HAF-VT优于现有方法，能更好地捕捉跨域用户兴趣。

Conclusion: HAF-VT成功将认知原理与计算模型结合，凸显多模态数据在序列决策中的作用。

Abstract: Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by
leveraging historical interactions across multiple domains, focusing on
modeling cross-domain preferences through intra- and inter-sequence item
relationships. Inspired by human cognitive processes, we propose Hierarchical
Attention Fusion of Visual and Textual Representations (HAF-VT), a novel
approach integrating visual and textual data to enhance cognitive modeling.
Using the frozen CLIP model, we generate image and text embeddings, enriching
item representations with multimodal data. A hierarchical attention mechanism
jointly learns single-domain and cross-domain preferences, mimicking human
information integration. Evaluated on four e-commerce datasets, HAF-VT
outperforms existing methods in capturing cross-domain user interests, bridging
cognitive principles with computational models and highlighting the role of
multimodal data in sequential decision-making.

</details>

### [402] [VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced Long-Range Depth Estimation](https://arxiv.org/abs/2504.15095)
*Mingxia Zhan,Li Zhang,XiaoMeng Chu,Beibei Wang*

Main category: cs.CV

TLDR: VistaDepth是一种新型的单目深度估计框架，通过结合频域特征增强和自适应权重平衡机制，显著提升了远距离深度重建的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的单目深度估计方法在远距离深度重建上表现不佳，主要由于深度值分布不平衡和过度依赖空间域特征。

Method: 提出VistaDepth框架，包含Latent Frequency Modulation (LFM)模块和自适应权重策略，动态优化频域特征并调整损失权重。

Result: 实验证明VistaDepth在扩散模型方法中表现最佳，尤其在远距离区域重建上效果显著。

Conclusion: VistaDepth通过频域特征增强和自适应权重机制，显著提升了单目深度估计的性能。

Abstract: Monocular depth estimation (MDE) aims to predict per-pixel depth values from
a single RGB image. Recent advancements have positioned diffusion models as
effective MDE tools by framing the challenge as a conditional image generation
task. Despite their progress, these methods often struggle with accurately
reconstructing distant depths, due largely to the imbalanced distribution of
depth values and an over-reliance on spatial-domain features. To overcome these
limitations, we introduce VistaDepth, a novel framework that integrates
adaptive frequency-domain feature enhancements with an adaptive
weight-balancing mechanism into the diffusion process. Central to our approach
is the Latent Frequency Modulation (LFM) module, which dynamically refines
spectral responses in the latent feature space, thereby improving the
preservation of structural details and reducing noisy artifacts. Furthermore,
we implement an adaptive weighting strategy that modulates the diffusion loss
in real-time, enhancing the model's sensitivity towards distant depth
reconstruction. These innovations collectively result in superior depth
perception performance across both distance and detail. Experimental
evaluations confirm that VistaDepth achieves state-of-the-art performance among
diffusion-based MDE techniques, particularly excelling in the accurate
reconstruction of distant regions.

</details>

### [403] [A triple-branch network for latent fingerprint enhancement guided by orientation fields and minutiae](https://arxiv.org/abs/2504.15105)
*Yurun Wang,Zerong Qi,Shujun Fu,Mingzheng Hu*

Main category: cs.CV

TLDR: 提出了一种名为TBSFNet的三分支空间融合网络，结合MLFGNet提升潜指纹增强效果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在潜指纹增强中表现不足，尤其是低质量区域的恢复。

Method: 采用TBSFNet针对不同区域定制增强策略，并引入MLFGNet提升泛化能力。

Result: 在MOLF和MUST数据集上，MLFGNet优于现有增强算法。

Conclusion: TBSFNet与MLFGNet结合显著提升了潜指纹增强效果。

Abstract: Latent fingerprint enhancement is a critical step in the process of latent
fingerprint identification. Existing deep learning-based enhancement methods
still fall short of practical application requirements, particularly in
restoring low-quality fingerprint regions. Recognizing that different regions
of latent fingerprints require distinct enhancement strategies, we propose a
Triple Branch Spatial Fusion Network (TBSFNet), which simultaneously enhances
different regions of the image using tailored strategies. Furthermore, to
improve the generalization capability of the network, we integrate orientation
field and minutiae-related modules into TBSFNet and introduce a Multi-Level
Feature Guidance Network (MLFGNet). Experimental results on the MOLF and MUST
datasets demonstrate that MLFGNet outperforms existing enhancement algorithms.

</details>

### [404] [Unwarping Screen Content Images via Structure-texture Enhancement Network and Transformation Self-estimation](https://arxiv.org/abs/2504.15108)
*Zhenzhen Xiao,Heng Liu,Bingwen Hu*

Main category: cs.CV

TLDR: 提出了一种结构-纹理增强网络（STEN），用于处理屏幕内容图像（SCI）的扭曲问题，通过B样条隐式神经表示和变换自估计算法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有隐式神经网络方法在自然图像上表现良好，但在处理包含大几何扭曲、文本和锐利边缘的SCI时效果不佳。

Method: STEN包含结构估计分支（SEB）和纹理估计分支（TEB），分别增强局部聚合与全局依赖建模及纹理细节合成，并通过变换自估计模块校正坐标变换矩阵。

Result: 在公开SCI数据集上显著优于现有方法，且在自然图像数据集上也显示出潜力。

Conclusion: STEN通过结构-纹理增强和自校正机制，有效解决了SCI的扭曲问题，并具有扩展到自然图像的潜力。

Abstract: While existing implicit neural network-based image unwarping methods perform
well on natural images, they struggle to handle screen content images (SCIs),
which often contain large geometric distortions, text, symbols, and sharp
edges. To address this, we propose a structure-texture enhancement network
(STEN) with transformation self-estimation for SCI warping. STEN integrates a
B-spline implicit neural representation module and a transformation error
estimation and self-correction algorithm. It comprises two branches: the
structure estimation branch (SEB), which enhances local aggregation and global
dependency modeling, and the texture estimation branch (TEB), which improves
texture detail synthesis using B-spline implicit neural representation.
Additionally, the transformation self-estimation module autonomously estimates
the transformation error and corrects the coordinate transformation matrix,
effectively handling real-world image distortions. Extensive experiments on
public SCI datasets demonstrate that our approach significantly outperforms
state-of-the-art methods. Comparisons on well-known natural image datasets also
show the potential of our approach for natural image distortion.

</details>

### [405] [Improving Sound Source Localization with Joint Slot Attention on Image and Audio](https://arxiv.org/abs/2504.15118)
*Inho Kim,Youngkil Song,Jicheol Park,Won Hwa Kim,Suha Kwak*

Main category: cs.CV

TLDR: 提出一种基于联合槽注意力的声源定位方法，通过分解目标与非目标表示提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法因噪声和无关背景导致性能不佳，需改进表示学习。

Method: 使用联合槽注意力分解图像和音频特征，仅用目标表示进行对比学习，并引入跨模态注意力匹配。

Result: 在三个公开基准测试中表现最佳，跨模态检索性能显著优于先前工作。

Conclusion: 联合槽注意力和跨模态注意力匹配有效提升了声源定位和跨模态检索性能。

Abstract: Sound source localization (SSL) is the task of locating the source of sound
within an image. Due to the lack of localization labels, the de facto standard
in SSL has been to represent an image and audio as a single embedding vector
each, and use them to learn SSL via contrastive learning. To this end, previous
work samples one of local image features as the image embedding and aggregates
all local audio features to obtain the audio embedding, which is far from
optimal due to the presence of noise and background irrelevant to the actual
target in the input. We present a novel SSL method that addresses this chronic
issue by joint slot attention on image and audio. To be specific, two slots
competitively attend image and audio features to decompose them into target and
off-target representations, and only target representations of image and audio
are used for contrastive learning. Also, we introduce cross-modal attention
matching to further align local features of image and audio. Our method
achieved the best in almost all settings on three public benchmarks for SSL,
and substantially outperformed all the prior work in cross-modal retrieval.

</details>

### [406] [Robust and Real-time Surface Normal Estimation from Stereo Disparities using Affine Transformations](https://arxiv.org/abs/2504.15121)
*Csongor Csanad Kariko,Muhammad Rafi Faisal,Levente Hajder*

Main category: cs.CV

TLDR: 提出了一种基于校正立体图像对的表面法线估计新方法，通过利用视差值的仿射变换实现快速准确的结果。


<details>
  <summary>Details</summary>
Motivation: 校正立体图像对可以简化表面法线估计过程并降低计算复杂度，同时需要解决噪声问题以提高鲁棒性。

Method: 结合视差数据的仿射变换和卷积操作启发的自定义算法，以及自适应启发式技术检测连通表面组件。

Result: 构建了一个快速准确的表面法线估计器，生成了密集的定向点云，在Middlebury和Cityscapes数据集上验证了实时性能和准确性。

Conclusion: 该方法在GPU上实现了显著的性能提升，代码将公开以促进研究和复现。

Abstract: This work introduces a novel method for surface normal estimation from
rectified stereo image pairs, leveraging affine transformations derived from
disparity values to achieve fast and accurate results. We demonstrate how the
rectification of stereo image pairs simplifies the process of surface normal
estimation by reducing computational complexity. To address noise reduction, we
develop a custom algorithm inspired by convolutional operations, tailored to
process disparity data efficiently. We also introduce adaptive heuristic
techniques for efficiently detecting connected surface components within the
images, further improving the robustness of the method. By integrating these
methods, we construct a surface normal estimator that is both fast and
accurate, producing a dense, oriented point cloud as the final output. Our
method is validated using both simulated environments and real-world stereo
images from the Middlebury and Cityscapes datasets, demonstrating significant
improvements in real-time performance and accuracy when implemented on a GPU.
Upon acceptance, the shader source code will be made publicly available to
facilitate further research and reproducibility.

</details>

### [407] [MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video](https://arxiv.org/abs/2504.15122)
*Minh-Quan Viet Bui,Jongmin Park,Juan Luis Gonzalez Bello,Jaeho Moon,Jihyong Oh,Munchurl Kim*

Main category: cs.CV

TLDR: MoBGS是一种新颖的动态3D高斯泼溅去模糊框架，能够从模糊的单目视频中重建清晰高质量的时空新视图。


<details>
  <summary>Details</summary>
Motivation: 现有动态新视图合成方法对运动模糊敏感，导致渲染质量下降。MoBGS旨在解决这一问题，专注于动态对象的运动建模。

Method: MoBGS提出BLCE方法用于潜在相机轨迹估计，以及LCEE方法用于全局相机和局部对象运动去模糊。

Result: 在Stereo Blur数据集和真实模糊视频上，MoBGS显著优于现有方法（DyBluRF和Deblur4DGS），达到动态NVS在运动模糊下的最先进性能。

Conclusion: MoBGS通过改进相机运动去模糊和动态对象建模，实现了高质量的动态新视图合成。

Abstract: We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS)
framework capable of reconstructing sharp and high-quality novel
spatio-temporal views from blurry monocular videos in an end-to-end manner.
Existing dynamic novel view synthesis (NVS) methods are highly sensitive to
motion blur in casually captured videos, resulting in significant degradation
of rendering quality. While recent approaches address motion-blurred inputs for
NVS, they primarily focus on static scene reconstruction and lack dedicated
motion modeling for dynamic objects. To overcome these limitations, our MoBGS
introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for
effective latent camera trajectory estimation, improving global camera motion
deblurring. In addition, we propose a physically-inspired Latent Camera-induced
Exposure Estimation (LCEE) method to ensure consistent deblurring of both
global camera and local object motion. Our MoBGS framework ensures the temporal
consistency of unseen latent timestamps and robust motion decomposition of
static and dynamic regions. Extensive experiments on the Stereo Blur dataset
and real-world blurry videos show that our MoBGS significantly outperforms the
very recent advanced methods (DyBluRF and Deblur4DGS), achieving
state-of-the-art performance for dynamic NVS under motion blur.

</details>

### [408] [Instance-Adaptive Keypoint Learning with Local-to-Global Geometric Aggregation for Category-Level Object Pose Estimation](https://arxiv.org/abs/2504.15134)
*Xiao Zhang,Lu Zou,Tao Lu,Yuan Yao,Zhangjin Huang,Guoping Wang*

Main category: cs.CV

TLDR: INKL-Pose是一种新颖的类别级物体姿态估计框架，通过实例自适应关键点学习和局部到全局几何聚合，显著提升了复杂几何或非标准形状物体的姿态估计性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理复杂几何或显著偏离标准形状的物体实例时泛化能力不足的问题。

Method: 1. 实例自适应关键点生成器预测关键点；2. 局部和全局关键点特征聚合器细化关键点；3. 引入特征序列翻转策略和损失函数优化关键点分布。

Result: 在CAMERA25、REAL275和HouseCat6D数据集上达到最先进性能。

Conclusion: INKL-Pose通过自适应关键点学习和几何聚合，显著提升了类别级物体姿态估计的准确性和泛化能力。

Abstract: Category-level object pose estimation aims to predict the 6D pose and size of
previously unseen instances from predefined categories, requiring strong
generalization across diverse object instances. Although many previous methods
attempt to mitigate intra-class variations, they often struggle with instances
exhibiting complex geometries or significant deviations from canonical shapes.
To address this challenge, we propose INKL-Pose, a novel category-level object
pose estimation framework that enables INstance-adaptive Keypoint Learning with
local-to-global geometric aggregation. Specifically, our approach first
predicts semantically consistent and geometric informative keypoints through an
Instance-Adaptive Keypoint Generator, then refines them with: (1) a Local
Keypoint Feature Aggregator capturing fine-grained geometries, and (2) a Global
Keypoint Feature Aggregator using bidirectional Mamba for structural
consistency. To enable bidirectional modeling in Mamba, we introduce a Feature
Sequence Flipping strategy that preserves spatial coherence while constructing
backward feature sequences. Additionally, we design a surface loss and a
separation loss to enforce uniform coverage and spatial diversity in keypoint
distribution. The generated keypoints are finally mapped to a canonical space
for regressing the object's 6D pose and size. Extensive experiments on
CAMERA25, REAL275, and HouseCat6D demonstrate that INKL-Pose achieves
state-of-the-art performance and significantly outperforms existing methods.

</details>

### [409] ["I Know It When I See It": Mood Spaces for Connecting and Expressing Visual Concepts](https://arxiv.org/abs/2504.15145)
*Huzheng Yang,Katherine Xu,Michael D. Grossberg,Yutong Bai,Jianbo Shi*

Main category: cs.CV

TLDR: 提出了一种Mood Board方法，通过示例传达抽象概念，并构建Mood Space来压缩特征空间，实现高效的图像操作。


<details>
  <summary>Details</summary>
Motivation: 许多抽象概念难以定义但易于识别，需要一种方法通过示例表达这些概念并量化其变化方向。

Method: 使用fibration计算压缩预训练特征到紧凑空间，学习图像标记间的亲和关系，并通过特征矩阵的顶部特征向量结构定义损失。

Result: Mood Space具有局部线性和紧凑性，支持图像级操作（如对象平均、视觉类比和姿态转移），计算高效且无需微调。

Conclusion: Mood Board方法通过少量示例快速学习，为抽象概念的视觉表达和操作提供了高效解决方案。

Abstract: Expressing complex concepts is easy when they can be labeled or quantified,
but many ideas are hard to define yet instantly recognizable. We propose a Mood
Board, where users convey abstract concepts with examples that hint at the
intended direction of attribute changes. We compute an underlying Mood Space
that 1) factors out irrelevant features and 2) finds the connections between
images, thus bringing relevant concepts closer. We invent a fibration
computation to compress/decompress pre-trained features into/from a compact
space, 50-100x smaller. The main innovation is learning to mimic the pairwise
affinity relationship of the image tokens across exemplars. To focus on the
coarse-to-fine hierarchical structures in the Mood Space, we compute the top
eigenvector structure from the affinity matrix and define a loss in the
eigenvector space. The resulting Mood Space is locally linear and compact,
allowing image-level operations, such as object averaging, visual analogy, and
pose transfer, to be performed as a simple vector operation in Mood Space. Our
learning is efficient in computation without any fine-tuning, needs only a few
(2-20) exemplars, and takes less than a minute to learn.

</details>

### [410] [Landmark-Free Preoperative-to-Intraoperative Registration in Laparoscopic Liver Resection](https://arxiv.org/abs/2504.15152)
*Jun Zhou,Bingchen Gao,Kai Wang,Jialun Pei,Pheng-Ann Heng,Jing Qin*

Main category: cs.CV

TLDR: 提出了一种基于自监督学习的无标记术前-术中肝脏配准框架，解决了传统方法依赖解剖标记和形状变形建模不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有配准方法依赖解剖标记，存在标记定义模糊和术中视觉信息整合不足的局限性。

Method: 将传统3D-2D配准流程转化为3D-3D配准，分为刚性和非刚性配准子任务，使用特征解耦变换器和结构正则化变形网络。

Result: 在合成和真实数据集上的实验及用户研究表明，该方法具有优越性和潜在临床适用性。

Conclusion: 提出的框架通过自监督学习有效解决了传统配准方法的局限性，展示了临床应用的潜力。

Abstract: Liver registration by overlaying preoperative 3D models onto intraoperative
2D frames can assist surgeons in perceiving the spatial anatomy of the liver
clearly for a higher surgical success rate. Existing registration methods rely
heavily on anatomical landmark-based workflows, which encounter two major
limitations: 1) ambiguous landmark definitions fail to provide efficient
markers for registration; 2) insufficient integration of intraoperative liver
visual information in shape deformation modeling. To address these challenges,
in this paper, we propose a landmark-free preoperative-to-intraoperative
registration framework utilizing effective self-supervised learning, termed
\ourmodel. This framework transforms the conventional 3D-2D workflow into a
3D-3D registration pipeline, which is then decoupled into rigid and non-rigid
registration subtasks. \ourmodel~first introduces a feature-disentangled
transformer to learn robust correspondences for recovering rigid
transformations. Further, a structure-regularized deformation network is
designed to adjust the preoperative model to align with the intraoperative
liver surface. This network captures structural correlations through geometry
similarity modeling in a low-rank transformer network. To facilitate the
validation of the registration performance, we also construct an in-vivo
registration dataset containing liver resection videos of 21 patients, called
\emph{P2I-LReg}, which contains 346 keyframes that provide a global view of the
liver together with liver mask annotations and calibrated camera intrinsic
parameters. Extensive experiments and user studies on both synthetic and
in-vivo datasets demonstrate the superiority and potential clinical
applicability of our method.

</details>

### [411] [Dynamic 3D KAN Convolution with Adaptive Grid Optimization for Hyperspectral Image Classification](https://arxiv.org/abs/2504.15155)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TLDR: 本文提出KANet，基于改进的3D-DenseNet模型，通过引入可学习的单变量B样条函数和动态网格调整机制，解决了高光谱图像分类中的高维数据、稀疏分布和光谱冗余问题，显著提升了模型精度和参数效率。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类中，高维数据、稀疏分布和光谱冗余导致分类过拟合和泛化能力受限。

Method: 提出KANet，结合3D KAN Conv和自适应网格更新机制，通过B样条参数化非线性激活函数和动态网格调整，捕获复杂的光谱-空间非线性关系。

Result: 在IN、UP和KSC数据集上表现优异，优于主流方法。

Conclusion: KANet通过3D动态专家卷积系统提升模型表示能力，无需增加网络深度或宽度，有效缓解维度灾难和过拟合风险。

Abstract: Deep neural networks face several challenges in hyperspectral image
classification, including high-dimensional data, sparse distribution of ground
objects, and spectral redundancy, which often lead to classification
overfitting and limited generalization capability. To more efficiently adapt to
ground object distributions while extracting image features without introducing
excessive parameters and skipping redundant information, this paper proposes
KANet based on an improved 3D-DenseNet model, consisting of 3D KAN Conv and an
adaptive grid update mechanism. By introducing learnable univariate B-spline
functions on network edges, specifically by flattening three-dimensional
neighborhoods into vectors and applying B-spline-parameterized nonlinear
activation functions to replace the fixed linear weights of traditional 3D
convolutional kernels, we precisely capture complex spectral-spatial nonlinear
relationships in hyperspectral data. Simultaneously, through a dynamic grid
adjustment mechanism, we adaptively update the grid point positions of
B-splines based on the statistical characteristics of input data, optimizing
the resolution of spline functions to match the non-uniform distribution of
spectral features, significantly improving the model's accuracy in
high-dimensional data modeling and parameter efficiency, effectively
alleviating the curse of dimensionality. This characteristic demonstrates
superior neural scaling laws compared to traditional convolutional neural
networks and reduces overfitting risks in small-sample and high-noise
scenarios. KANet enhances model representation capability through a 3D dynamic
expert convolution system without increasing network depth or width. The
proposed method demonstrates superior performance on IN, UP, and KSC datasets,
outperforming mainstream hyperspectral image classification approaches.

</details>

### [412] [Acquire and then Adapt: Squeezing out Text-to-Image Model for Image Restoration](https://arxiv.org/abs/2504.15159)
*Junyuan Deng,Xinyi Wu,Yongxing Yang,Congchao Zhu,Song Wang,Zhenyao Wu*

Main category: cs.CV

TLDR: 论文提出了一种名为FluxGen的数据生成流程和轻量级适配器FluxIR，利用预训练的T2I模型（Flux）生成高质量训练数据，以低成本解决图像修复任务。


<details>
  <summary>Details</summary>
Motivation: 预训练的T2I模型需要大量高质量图像和计算资源，成本高且隐私不友好。利用Flux模型的生成能力，可以低成本生成无限训练样本。

Method: 提出FluxGen流程（无条件图像生成、图像选择、退化图像模拟）和轻量级适配器FluxIR，控制基于DiT的T2I模型进行图像修复。

Result: 实验表明，该方法能以约8.5%的训练成本，在合成和真实退化数据集上实现优异的修复效果。

Conclusion: FluxGen和FluxIR的结合为低成本、高效的图像修复提供了可行方案。

Abstract: Recently, pre-trained text-to-image (T2I) models have been extensively
adopted for real-world image restoration because of their powerful generative
prior. However, controlling these large models for image restoration usually
requires a large number of high-quality images and immense computational
resources for training, which is costly and not privacy-friendly. In this
paper, we find that the well-trained large T2I model (i.e., Flux) is able to
produce a variety of high-quality images aligned with real-world distributions,
offering an unlimited supply of training samples to mitigate the above issue.
Specifically, we proposed a training data construction pipeline for image
restoration, namely FluxGen, which includes unconditional image generation,
image selection, and degraded image simulation. A novel light-weighted adapter
(FluxIR) with squeeze-and-excitation layers is also carefully designed to
control the large Diffusion Transformer (DiT)-based T2I model so that
reasonable details can be restored. Experiments demonstrate that our proposed
method enables the Flux model to adapt effectively to real-world image
restoration tasks, achieving superior scores and visual quality on both
synthetic and real-world degradation datasets - at only about 8.5\% of the
training cost compared to current approaches.

</details>

### [413] [An Efficient Aerial Image Detection with Variable Receptive Fields](https://arxiv.org/abs/2504.15165)
*Liu Wenbin*

Main category: cs.CV

TLDR: VRF-DETR是一种基于Transformer的检测器，通过动态调整感受野和门控多尺度融合，解决了无人机目标检测中的小目标、遮挡和计算限制问题。


<details>
  <summary>Details</summary>
Motivation: 无人机目标检测面临小目标（小于10像素）、密集遮挡和严格计算限制的挑战，现有检测器难以平衡准确性和效率。

Method: 提出VRF-DETR，包含三个关键模块：多尺度上下文融合（MSCF）、门控卷积（GConv）和门控多尺度融合（GMCF）瓶颈，通过动态空间注意力和门控机制优化特征。

Result: 在VisDrone2019数据集上，VRF-DETR达到51.4% mAP50和31.8% mAP50:95，仅需13.5M参数。

Conclusion: VRF-DETR为无人机目标检测任务建立了新的效率-准确性平衡点。

Abstract: Aerial object detection using unmanned aerial vehicles (UAVs) faces critical
challenges including sub-10px targets, dense occlusions, and stringent
computational constraints. Existing detectors struggle to balance accuracy and
efficiency due to rigid receptive fields and redundant architectures. To
address these limitations, we propose Variable Receptive Field DETR (VRF-DETR),
a transformer-based detector incorporating three key components: 1) Multi-Scale
Context Fusion (MSCF) module that dynamically recalibrates features through
adaptive spatial attention and gated multi-scale fusion, 2) Gated Convolution
(GConv) layer enabling parameter-efficient local-context modeling via depthwise
separable operations and dynamic gating, and 3) Gated Multi-scale Fusion (GMCF)
Bottleneck that hierarchically disentangles occluded objects through cascaded
global-local interactions. Experiments on VisDrone2019 demonstrate VRF-DETR
achieves 51.4\% mAP\textsubscript{50} and 31.8\% mAP\textsubscript{50:95} with
only 13.5M parameters. This work establishes a new efficiency-accuracy Pareto
frontier for UAV-based detection tasks.

</details>

### [414] [HSANET: A Hybrid Self-Cross Attention Network For Remote Sensing Change Detection](https://arxiv.org/abs/2504.15170)
*Chengxi Han,Xiaoyu Su,Zhiqiang Wei,Meiqi Hu,Yichu Xu*

Main category: cs.CV

TLDR: HSANet是一种用于遥感图像变化检测的网络，通过分层卷积提取多尺度特征，并结合混合自注意力和交叉注意力机制，提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 遥感图像变化检测是大规模监测的重要方法，需要高效捕捉多尺度信息和全局上下文。

Method: 使用分层卷积提取多尺度特征，结合混合自注意力和交叉注意力机制，学习并融合全局和跨尺度信息。

Result: HSANet能够捕捉不同尺度的全局上下文，整合跨尺度特征，优化边缘细节，提升检测性能。

Conclusion: HSANet在遥感图像变化检测任务中表现出色，代码已开源。

Abstract: The remote sensing image change detection task is an essential method for
large-scale monitoring. We propose HSANet, a network that uses hierarchical
convolution to extract multi-scale features. It incorporates hybrid
self-attention and cross-attention mechanisms to learn and fuse global and
cross-scale information. This enables HSANet to capture global context at
different scales and integrate cross-scale features, refining edge details and
improving detection performance. We will also open-source our model code:
https://github.com/ChengxiHAN/HSANet.

</details>

### [415] [DSPO: Direct Semantic Preference Optimization for Real-World Image Super-Resolution](https://arxiv.org/abs/2504.15176)
*Miaomiao Cai,Simiao Li,Wei Li,Xudong Huang,Hanting Chen,Jie Hu,Yunhe Wang*

Main category: cs.CV

TLDR: 该论文提出了一种名为DSPO的方法，将人类偏好对齐引入Real-ISR任务，解决了现有方法在像素级重建与图像级偏好之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在Real-ISR任务中缺乏人类反馈集成，可能导致生成结果与人类偏好不一致，甚至产生伪影和有害内容。

Method: 提出Direct Semantic Preference Optimization (DSPO)，通过语义实例对齐策略和用户描述反馈策略，实现实例级人类偏好对齐。

Result: DSPO在单步和多步超分辨率框架中均表现出高效性。

Conclusion: DSPO是一种即插即用的解决方案，有效提升了Real-ISR任务中生成结果与人类偏好的一致性。

Abstract: Recent advances in diffusion models have improved Real-World Image
Super-Resolution (Real-ISR), but existing methods lack human feedback
integration, risking misalignment with human preference and may leading to
artifacts, hallucinations and harmful content generation. To this end, we are
the first to introduce human preference alignment into Real-ISR, a technique
that has been successfully applied in Large Language Models and Text-to-Image
tasks to effectively enhance the alignment of generated outputs with human
preferences. Specifically, we introduce Direct Preference Optimization (DPO)
into Real-ISR to achieve alignment, where DPO serves as a general alignment
technique that directly learns from the human preference dataset. Nevertheless,
unlike high-level tasks, the pixel-level reconstruction objectives of Real-ISR
are difficult to reconcile with the image-level preferences of DPO, which can
lead to the DPO being overly sensitive to local anomalies, leading to reduced
generation quality. To resolve this dichotomy, we propose Direct Semantic
Preference Optimization (DSPO) to align instance-level human preferences by
incorporating semantic guidance, which is through two strategies: (a) semantic
instance alignment strategy, implementing instance-level alignment to ensure
fine-grained perceptual consistency, and (b) user description feedback
strategy, mitigating hallucinations through semantic textual feedback on
instance-level images. As a plug-and-play solution, DSPO proves highly
effective in both one-step and multi-step SR frameworks.

</details>

### [416] [FaceCraft4D: Animated 3D Facial Avatar Generation from a Single Image](https://arxiv.org/abs/2504.15179)
*Fei Yin,Mallikarjun B R,Chun-Han Yao,Rafał Mantiuk,Varun Jampani*

Main category: cs.CV

TLDR: 提出了一种从单张图像生成高质量可动画4D虚拟形象的新框架，解决了现有方法对多视角数据依赖或形状精度与身份一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要大量多视角数据或难以保持形状精度和身份一致性，限制了4D虚拟形象的生成质量。

Method: 结合形状、图像和视频先验，通过3D-GAN反演获取初始形状，利用深度引导变形信号增强多视角纹理，并引入视频先验处理表情动画，采用一致-不一致训练优化4D重建。

Result: 实验结果表明，该方法在质量和多视角、表情一致性上优于现有技术。

Conclusion: 该框架能够高效生成高质量且一致的4D虚拟形象，为单图像驱动的4D建模提供了新思路。

Abstract: We present a novel framework for generating high-quality, animatable 4D
avatar from a single image. While recent advances have shown promising results
in 4D avatar creation, existing methods either require extensive multiview data
or struggle with shape accuracy and identity consistency. To address these
limitations, we propose a comprehensive system that leverages shape, image, and
video priors to create full-view, animatable avatars. Our approach first
obtains initial coarse shape through 3D-GAN inversion. Then, it enhances
multiview textures using depth-guided warping signals for cross-view
consistency with the help of the image diffusion model. To handle expression
animation, we incorporate a video prior with synchronized driving signals
across viewpoints. We further introduce a Consistent-Inconsistent training to
effectively handle data inconsistencies during 4D reconstruction. Experimental
results demonstrate that our method achieves superior quality compared to the
prior art, while maintaining consistency across different viewpoints and
expressions.

</details>

### [417] [Tiger200K: Manually Curated High Visual Quality Video Dataset from UGC Platform](https://arxiv.org/abs/2504.15182)
*Xianpan Zhou*

Main category: cs.CV

TLDR: Tiger200K是一个高质量的手工标注视频数据集，旨在解决开源文本到视频生成模型对专有训练数据的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有开源数据集（如Koala-36M）的质量不足以支持高级视频生成模型的微调，因此需要更高质量的数据集。

Method: 通过人工筛选用户生成内容（UGC）平台视频，结合镜头边界检测、OCR、边框检测、运动过滤和双语字幕等技术，构建高质量视频-文本对。

Result: Tiger200K提供了视觉保真度高且时间一致的视频-文本对，支持视频生成模型的优化。

Conclusion: Tiger200K将作为开源项目持续扩展，推动视频生成模型的研究与应用。

Abstract: The recent surge in open-source text-to-video generation models has
significantly energized the research community, yet their dependence on
proprietary training datasets remains a key constraint. While existing open
datasets like Koala-36M employ algorithmic filtering of web-scraped videos from
early platforms, they still lack the quality required for fine-tuning advanced
video generation models. We present Tiger200K, a manually curated high visual
quality video dataset sourced from User-Generated Content (UGC) platforms. By
prioritizing visual fidelity and aesthetic quality, Tiger200K underscores the
critical role of human expertise in data curation, and providing high-quality,
temporally consistent video-text pairs for fine-tuning and optimizing video
generation architectures through a simple but effective pipeline including shot
boundary detection, OCR, border detecting, motion filter and fine bilingual
caption. The dataset will undergo ongoing expansion and be released as an
open-source initiative to advance research and applications in video generative
models. Project page: https://tinytigerpan.github.io/tiger200k/

</details>

### [418] [Breast density in MRI: an AI-based quantification and relationship to assessment in mammography](https://arxiv.org/abs/2504.15192)
*Yaqian Chen,Lin Li,Hanxue Gu,Haoyu Dong,Derek L. Nguyen,Allan D. Kirk,Maciej A. Mazurowski,E. Shelley Hwang*

Main category: cs.CV

TLDR: 该论文研究了MRI在评估乳腺密度中的应用，发现其与乳腺X线摄影的密度评估相关但存在差异，未来可能用于改进乳腺癌风险预测。


<details>
  <summary>Details</summary>
Motivation: 乳腺密度是乳腺癌的重要风险因素，MRI作为一种补充手段，能够提供更全面的乳腺组织评估，但其3D特性带来了分析挑战。

Method: 使用内部开发的机器学习算法，在三个MRI数据集中评估正常乳腺的密度，并与乳腺X线摄影结果进行对比。

Result: MRI乳腺密度在不同数据集中表现一致（0.104 - 0.114），且随年龄增长而下降的趋势与先前研究一致。MRI与乳腺X线摄影的密度评估相关，但存在差异。

Conclusion: MRI乳腺密度评估具有潜力，未来可整合现有工具以改进乳腺癌风险预测。

Abstract: Mammographic breast density is a well-established risk factor for breast
cancer. Recently there has been interest in breast MRI as an adjunct to
mammography, as this modality provides an orthogonal and highly quantitative
assessment of breast tissue. However, its 3D nature poses analytic challenges
related to delineating and aggregating complex structures across slices. Here,
we applied an in-house machine-learning algorithm to assess breast density on
normal breasts in three MRI datasets. Breast density was consistent across
different datasets (0.104 - 0.114). Analysis across different age groups also
demonstrated strong consistency across datasets and confirmed a trend of
decreasing density with age as reported in previous studies. MR breast density
was correlated with mammographic breast density, although some notable
differences suggest that certain breast density components are captured only on
MRI. Future work will determine how to integrate MR breast density with current
tools to improve future breast cancer risk prediction.

</details>

### [419] [Automated Measurement of Eczema Severity with Self-Supervised Learning](https://arxiv.org/abs/2504.15193)
*Neelesh Kumar,Oya Aran*

Main category: cs.CV

TLDR: 提出了一种基于自监督学习的湿疹自动诊断框架，在有限标注数据下表现优于现有深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的湿疹诊断方法需要大量标注数据，但获取困难。

Method: 采用两阶段框架：1) 使用SegGPT进行少样本分割；2) 提取DINO特征并用MLP进行四分类。

Result: 在真实湿疹图像数据集上，加权F1得分为0.67，优于Resnet-18和Vision Transformer。

Conclusion: 自监督学习在标注数据稀缺的皮肤诊断中具有潜力。

Abstract: Automated diagnosis of eczema using images acquired from digital camera can
enable individuals to self-monitor their recovery. The process entails first
segmenting out the eczema region from the image and then measuring the severity
of eczema in the segmented region. The state-of-the-art methods for automated
eczema diagnosis rely on deep neural networks such as convolutional neural
network (CNN) and have shown impressive performance in accurately measuring the
severity of eczema. However, these methods require massive volume of annotated
data to train which can be hard to obtain. In this paper, we propose a
self-supervised learning framework for automated eczema diagnosis under limited
training data regime. Our framework consists of two stages: i) Segmentation,
where we use an in-context learning based algorithm called SegGPT for few-shot
segmentation of eczema region from the image; ii) Feature extraction and
classification, where we extract DINO features from the segmented regions and
feed it to a multi-layered perceptron (MLP) for 4-class classification of
eczema severity. When evaluated on a dataset of annotated "in-the-wild" eczema
images, we show that our method outperforms (Weighted F1: 0.67 $\pm$ 0.01) the
state-of-the-art deep learning methods such as finetuned Resnet-18 (Weighted
F1: 0.44 $\pm$ 0.16) and Vision Transformer (Weighted F1: 0.40 $\pm$ 0.22). Our
results show that self-supervised learning can be a viable solution for
automated skin diagnosis where labeled data is scarce.

</details>

### [420] [Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's LLM-CLIP Framework for Image Captioning](https://arxiv.org/abs/2504.15199)
*Yassir Benhammou,Alessandro Tiberio,Gabriel Trautmann,Suman Kalyan*

Main category: cs.CV

TLDR: MILS框架声称无需训练即可实现多模态任务，但其迭代优化过程带来高昂计算成本，而BLIP-2和GPT-4V等模型通过单次处理实现类似效果。


<details>
  <summary>Details</summary>
Motivation: 揭示MILS框架在零样本任务中的隐藏计算成本，挑战其“无需训练”的实用性。

Method: 对比分析MILS的迭代优化与BLIP-2、GPT-4V的单次处理方法的计算效率。

Result: MILS的高性能伴随显著计算开销，而其他模型能以更低成本实现竞争性结果。

Conclusion: MILS的计算成本可能抵消其零样本优势，为高效多模态模型设计提供重要参考。

Abstract: MILS (Multimodal Iterative LLM Solver) is a recently published framework that
claims "LLMs can see and hear without any training" by leveraging an iterative,
LLM-CLIP based approach for zero-shot image captioning. While this MILS
approach demonstrates good performance, our investigation reveals that this
success comes at a hidden, substantial computational cost due to its expensive
multi-step refinement process. In contrast, alternative models such as BLIP-2
and GPT-4V achieve competitive results through a streamlined, single-pass
approach. We hypothesize that the significant overhead inherent in MILS's
iterative process may undermine its practical benefits, thereby challenging the
narrative that zero-shot performance can be attained without incurring heavy
resource demands. This work is the first to expose and quantify the trade-offs
between output quality and computational cost in MILS, providing critical
insights for the design of more efficient multimodal models.

</details>

### [421] [Shape-Guided Clothing Warping for Virtual Try-On](https://arxiv.org/abs/2504.15232)
*Xiaoyu Han,Shunyuan Zheng,Zonglin Li,Chenyang Wang,Xin Sun,Quanling Meng*

Main category: cs.CV

TLDR: 提出了一种名为SCW-VTON的形状引导服装变形方法，通过全局形状约束和肢体纹理增强虚拟试穿的现实感和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在服装变形时缺乏对细节的精确控制，导致服装与身体形状不一致及肢体区域失真。

Method: 设计了双路径服装变形模块（形状路径和流路径）以及肢体重建网络，结合全局形状约束和肢体纹理。

Result: 实验表明，SCW-VTON在服装形状一致性和细节控制上优于现有方法。

Conclusion: SCW-VTON能生成更真实且一致的虚拟试穿结果，代码已开源。

Abstract: Image-based virtual try-on aims to seamlessly fit in-shop clothing to a
person image while maintaining pose consistency. Existing methods commonly
employ the thin plate spline (TPS) transformation or appearance flow to deform
in-shop clothing for aligning with the person's body. Despite their promising
performance, these methods often lack precise control over fine details,
leading to inconsistencies in shape between clothing and the person's body as
well as distortions in exposed limb regions. To tackle these challenges, we
propose a novel shape-guided clothing warping method for virtual try-on, dubbed
SCW-VTON, which incorporates global shape constraints and additional limb
textures to enhance the realism and consistency of the warped clothing and
try-on results. To integrate global shape constraints for clothing warping, we
devise a dual-path clothing warping module comprising a shape path and a flow
path. The former path captures the clothing shape aligned with the person's
body, while the latter path leverages the mapping between the pre- and
post-deformation of the clothing shape to guide the estimation of appearance
flow. Furthermore, to alleviate distortions in limb regions of try-on results,
we integrate detailed limb guidance by developing a limb reconstruction network
based on masked image modeling. Through the utilization of SCW-VTON, we are
able to generate try-on results with enhanced clothing shape consistency and
precise control over details. Extensive experiments demonstrate the superiority
of our approach over state-of-the-art methods both qualitatively and
quantitatively. The code is available at https://github.com/xyhanHIT/SCW-VTON.

</details>

### [422] [Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset Generation](https://arxiv.org/abs/2504.15259)
*Yunxuan Cai,Sitao Xiang,Zongjian Li,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

TLDR: 论文提出了一种基于生成网络的方法，通过扩散模型生成高质量3D人脸数据，并结合GAN生成器实现语义可控的人脸建模系统。


<details>
  <summary>Details</summary>
Motivation: 传统人脸建模受限于数据采集设备、人工劳动和合适演员，导致模型多样性和可控性不足。

Method: 使用预训练扩散模型生成高质量3D人脸数据库，结合归一化模块和GAN生成器，实现语义输入和连续编辑。

Result: 生成了44,000个人脸模型，开发了高效的GAN生成器，并构建了基于物理的面部资产。

Conclusion: 提出的系统在实验和评估中表现优异，并计划发布基于网页的交互工具。

Abstract: Digital modeling and reconstruction of human faces serve various
applications. However, its availability is often hindered by the requirements
of data capturing devices, manual labor, and suitable actors. This situation
restricts the diversity, expressiveness, and control over the resulting models.
This work aims to demonstrate that a semantically controllable generative
network can provide enhanced control over the digital face modeling process. To
enhance diversity beyond the limited human faces scanned in a controlled
setting, we introduce a novel data generation pipeline that creates a
high-quality 3D face database using a pre-trained diffusion model. Our proposed
normalization module converts synthesized data from the diffusion model into
high-quality scanned data. Using the 44,000 face models we obtained, we further
developed an efficient GAN-based generator. This generator accepts semantic
attributes as input, and generates geometry and albedo. It also allows
continuous post-editing of attributes in the latent space. Our asset refinement
component subsequently creates physically-based facial assets. We introduce a
comprehensive system designed for creating and editing high-quality face
assets. Our proposed model has undergone extensive experiment, comparison and
evaluation. We also integrate everything into a web-based interactive tool. We
aim to make this tool publicly available with the release of the paper.

</details>

### [423] [Diffusion Bridge Models for 3D Medical Image Translation](https://arxiv.org/abs/2504.15267)
*Shaorong Zhang,Tamoghna Chattopadhyay,Sophia I. Thomopoulos,Jose-Luis Ambite,Paul M. Thompson,Greg Ver Steeg*

Main category: cs.CV

TLDR: 提出了一种扩散桥模型，用于T1w MRI和DTI模态之间的3D脑图像转换，以减少DTI采集时间并支持跨模态数据增强。


<details>
  <summary>Details</summary>
Motivation: DTI成像耗时较长，而T1w MRI更易获取，因此需要一种方法在两者之间进行高效转换。

Method: 使用扩散桥模型从T1w图像生成高质量的DTI FA图像，反之亦然，并通过多种指标评估性能。

Result: 模型在解剖结构捕捉和白质完整性信息保留方面表现优异，生成的图像在分类任务中与真实数据性能相当。

Conclusion: 该模型为神经影像数据集改进和临床决策支持提供了有前景的解决方案，可能对研究和临床实践产生重大影响。

Abstract: Diffusion tensor imaging (DTI) provides crucial insights into the
microstructure of the human brain, but it can be time-consuming to acquire
compared to more readily available T1-weighted (T1w) magnetic resonance imaging
(MRI). To address this challenge, we propose a diffusion bridge model for 3D
brain image translation between T1w MRI and DTI modalities. Our model learns to
generate high-quality DTI fractional anisotropy (FA) images from T1w images and
vice versa, enabling cross-modality data augmentation and reducing the need for
extensive DTI acquisition. We evaluate our approach using perceptual
similarity, pixel-level agreement, and distributional consistency metrics,
demonstrating strong performance in capturing anatomical structures and
preserving information on white matter integrity. The practical utility of the
synthetic data is validated through sex classification and Alzheimer's disease
classification tasks, where the generated images achieve comparable performance
to real data. Our diffusion bridge model offers a promising solution for
improving neuroimaging datasets and supporting clinical decision-making, with
the potential to significantly impact neuroimaging research and clinical
practice.

</details>

### [424] [Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models](https://arxiv.org/abs/2504.15271)
*Guo Chen,Zhiqi Li,Shihao Wang,Jindong Jiang,Yicheng Liu,Lidong Lu,De-An Huang,Wonmin Byeon,Matthieu Le,Tuomas Rintamaki,Tyler Poon,Max Ehrlich,Tuomas Rintamaki,Tyler Poon,Tong Lu,Limin Wang,Bryan Catanzaro,Jan Kautz,Andrew Tao,Zhiding Yu,Guilin Liu*

Main category: cs.CV

TLDR: Eagle 2.5是一个前沿的视觉语言模型家族，专注于长上下文多模态学习，解决了长视频理解和高分辨率图像识别的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在长上下文多模态任务中存在局限性，如长视频理解和图像细节保留不足。

Method: 提出了自动降级采样和图像区域保留技术，结合高效的长上下文数据训练优化框架，并引入了Eagle-Video-110K数据集。

Result: Eagle 2.5在长上下文多模态基准测试中表现显著提升，Eagle 2.5-8B在Video-MME上达到72.4%的准确率。

Conclusion: Eagle 2.5为现有视觉语言模型的局限性提供了稳健解决方案，性能媲美顶级商业和开源模型。

Abstract: We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)
for long-context multimodal learning. Our work addresses the challenges in long
video comprehension and high-resolution image understanding, introducing a
generalist framework for both tasks. The proposed training framework
incorporates Automatic Degrade Sampling and Image Area Preservation, two
techniques that preserve contextual integrity and visual details. The framework
also includes numerous efficiency optimizations in the pipeline for
long-context data training. Finally, we propose Eagle-Video-110K, a novel
dataset that integrates both story-level and clip-level annotations,
facilitating long-video understanding. Eagle 2.5 demonstrates substantial
improvements on long-context multimodal benchmarks, providing a robust solution
to the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B
achieves 72.4% on Video-MME with 512 input frames, matching the results of
top-tier commercial model such as GPT-4o and large-scale open-source models
like Qwen2.5-VL-72B and InternVL2.5-78B.

</details>

### [425] [DRAWER: Digital Reconstruction and Articulation With Environment Realism](https://arxiv.org/abs/2504.15278)
*Hongchi Xia,Entong Su,Marius Memmel,Arhan Jain,Raymond Yu,Numfor Mbiziwo-Tiapo,Ali Farhadi,Abhishek Gupta,Shenlong Wang,Wei-Chiu Ma*

Main category: cs.CV

TLDR: DRAWER框架将静态室内场景视频转换为逼真、交互式的数字环境，支持游戏引擎和机器人仿真平台。


<details>
  <summary>Details</summary>
Motivation: 通过创建虚拟数字副本，释放游戏和机器人等领域潜力。

Method: 基于双场景表示的重建模块和关节模块，实现几何细节重建与交互功能。

Result: 生成的虚拟环境逼真、交互性强，可实时运行，并成功应用于游戏和机器人仿真。

Conclusion: DRAWER展示了从真实世界数据创建交互式数字环境的潜力。

Abstract: Creating virtual digital replicas from real-world data unlocks significant
potential across domains like gaming and robotics. In this paper, we present
DRAWER, a novel framework that converts a video of a static indoor scene into a
photorealistic and interactive digital environment. Our approach centers on two
main contributions: (i) a reconstruction module based on a dual scene
representation that reconstructs the scene with fine-grained geometric details,
and (ii) an articulation module that identifies articulation types and hinge
positions, reconstructs simulatable shapes and appearances and integrates them
into the scene. The resulting virtual environment is photorealistic,
interactive, and runs in real time, with compatibility for game engines and
robotic simulation platforms. We demonstrate the potential of DRAWER by using
it to automatically create an interactive game in Unreal Engine and to enable
real-to-sim-to-real transfer for robotics applications.

</details>

### [426] [VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models](https://arxiv.org/abs/2504.15279)
*Weiye Xu,Jiahao Wang,Weiyun Wang,Zhe Chen,Wengang Zhou,Aijun Yang,Lewei Lu,Houqiang Li,Xiaohua Wang,Xizhou Zhu,Wenhai Wang,Jifeng Dai,Jinguo Zhu*

Main category: cs.CV

TLDR: VisuLogic是一个包含1000个人工验证问题的视觉推理基准，用于评估多模态大语言模型（MLLMs）的视觉推理能力。测试结果显示，当前MLLMs的表现远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型的推理评估依赖文本描述，存在语言推理捷径，无法真正衡量视觉为中心的推理能力。

Method: 提出VisuLogic基准，包含六类问题，评估MLLMs的视觉推理能力，并提供补充训练数据和强化学习基线。

Result: 大多数MLLMs的准确率低于30%，远低于人类的51.4%。

Conclusion: 当前MLLMs在视觉推理方面存在显著差距，需进一步研究提升。

Abstract: Visual reasoning is a core component of human intelligence and a critical
capability for advanced multimodal models. Yet current reasoning evaluations of
multimodal large language models (MLLMs) often rely on text descriptions and
allow language-based reasoning shortcuts, failing to measure genuine
vision-centric reasoning. To address this, we introduce VisuLogic: a benchmark
of 1,000 human-verified problems across six categories (e.g., quantitative
shifts, spatial relations, attribute comparisons). These various types of
questions can be evaluated to assess the visual reasoning capabilities of MLLMs
from multiple perspectives. We evaluate leading MLLMs on this benchmark and
analyze their results to identify common failure modes. Most models score below
30% accuracy-only slightly above the 25% random baseline and far below the
51.4% achieved by humans-revealing significant gaps in visual reasoning.
Furthermore, we provide a supplementary training dataset and a
reinforcement-learning baseline to support further progress.

</details>

### [427] [StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians](https://arxiv.org/abs/2504.15281)
*Cailin Zhuang,Yaoqi Hu,Xuanyang Zhang,Wei Cheng,Jiacheng Bao,Shengqi Liu,Yiying Yang,Xianfang Zeng,Gang Yu,Ming Li*

Main category: cs.CV

TLDR: StyleMe3D是一个针对3D高斯泼溅（3DGS）风格化场景的框架，通过多模态风格条件、多级语义对齐和感知质量增强，解决了3DGS在风格化场景中的问题。


<details>
  <summary>Details</summary>
Motivation: 3DGS在真实场景重建中表现出色，但在风格化场景（如卡通、游戏）中存在纹理碎片化、语义不对齐和抽象美学适应性差的问题。

Method: StyleMe3D通过动态风格分数蒸馏（DSSD）、对比风格描述符（CSD）、同时优化尺度（SOS）和3D高斯质量评估（3DG-QA）四个组件实现风格化。

Result: 在NeRF合成数据集和tandt db数据集上，StyleMe3D在保留几何细节和风格一致性方面优于现有方法，并保持实时渲染。

Conclusion: 该工作填补了真实3DGS与艺术风格化之间的空白，为游戏、虚拟世界和数字艺术提供了新应用。

Abstract: 3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction
but struggles with stylized scenarios (e.g., cartoons, games) due to fragmented
textures, semantic misalignment, and limited adaptability to abstract
aesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer
that integrates multi-modal style conditioning, multi-level semantic alignment,
and perceptual quality enhancement. Our key insights include: (1) optimizing
only RGB attributes preserves geometric integrity during stylization; (2)
disentangling low-, medium-, and high-level semantics is critical for coherent
style transfer; (3) scalability across isolated objects and complex scenes is
essential for practical deployment. StyleMe3D introduces four novel components:
Dynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent
space for semantic alignment; Contrastive Style Descriptor (CSD) for localized,
content-aware texture transfer; Simultaneously Optimized Scale (SOS) to
decouple style details and structural coherence; and 3D Gaussian Quality
Assessment (3DG-QA), a differentiable aesthetic prior trained on human-rated
data to suppress artifacts and enhance visual harmony. Evaluated on NeRF
synthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D
outperforms state-of-the-art methods in preserving geometric details (e.g.,
carvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,
coherent lighting in landscapes), while maintaining real-time rendering. This
work bridges photorealistic 3D GS and artistic stylization, unlocking
applications in gaming, virtual worlds, and digital art.

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [428] [Causal pieces: analysing and improving spiking neural networks piece by piece](https://arxiv.org/abs/2504.14015)
*Dominik Dold,Philipp Christian Petersen*

Main category: cs.NE

TLDR: 论文提出了一种基于“因果片段”的新概念，用于分析脉冲神经网络的表达能力和可训练性，并证明了其输入域可分解为局部Lipschitz连续的区域。


<details>
  <summary>Details</summary>
Motivation: 研究脉冲神经网络（SNNs）的表达能力和训练性能，提出一种新的分析工具“因果片段”，以改进SNN的性能并与人工神经网络（ANNs）进行比较。

Method: 通过分析SNN输入域的分解，定义“因果片段”作为衡量其近似能力的指标，并在仿真中验证其与训练成功的相关性。

Result: 研究发现，具有高数量因果片段的SNN初始化与训练成功强相关，且纯正权重的前馈SNN表现出色。

Conclusion: 因果片段是改进SNN的强大工具，未来可能为SNN与ANN的比较提供新思路。

Abstract: We introduce a novel concept for spiking neural networks (SNNs) derived from
the idea of "linear pieces" used to analyse the expressiveness and trainability
of artificial neural networks (ANNs). We prove that the input domain of SNNs
decomposes into distinct causal regions where its output spike times are
locally Lipschitz continuous with respect to the input spike times and network
parameters. The number of such regions - which we call "causal pieces" - is a
measure of the approximation capabilities of SNNs. In particular, we
demonstrate in simulation that parameter initialisations which yield a high
number of causal pieces on the training set strongly correlate with SNN
training success. Moreover, we find that feedforward SNNs with purely positive
weights exhibit a surprisingly high number of causal pieces, allowing them to
achieve competitive performance levels on benchmark tasks. We believe that
causal pieces are not only a powerful and principled tool for improving SNNs,
but might also open up new ways of comparing SNNs and ANNs in the future.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [429] [PRISM: A Unified Framework for Photorealistic Reconstruction and Intrinsic Scene Modeling](https://arxiv.org/abs/2504.14219)
*Alara Dirik,Tuanfeng Wang,Duygu Ceylan,Stefanos Zafeiriou,Anna Frühstück*

Main category: cs.GR

TLDR: PRISM是一个统一的框架，通过单一基础模型支持多种图像生成和编辑任务，包括RGBX生成、分解和条件生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法需要单独推断或使用多个模型的问题，PRISM通过联合生成所有内在层来保持模态一致性。

Method: 基于预训练的文本到图像扩散模型，提出有效的微调策略，同时生成RGB图像和内在层（X层）。

Result: 实验表明PRISM在内在图像分解和条件图像生成方面表现优异，同时保留了基础模型的文本到图像生成能力。

Conclusion: PRISM是一个多功能且高效的框架，适用于多种图像任务。

Abstract: We present PRISM, a unified framework that enables multiple image generation
and editing tasks in a single foundational model. Starting from a pre-trained
text-to-image diffusion model, PRISM proposes an effective fine-tuning strategy
to produce RGB images along with intrinsic maps (referred to as X layers)
simultaneously. Unlike previous approaches, which infer intrinsic properties
individually or require separate models for decomposition and conditional
generation, PRISM maintains consistency across modalities by generating all
intrinsic layers jointly. It supports diverse tasks, including text-to-RGBX
generation, RGB-to-X decomposition, and X-to-RGBX conditional generation.
Additionally, PRISM enables both global and local image editing through
conditioning on selected intrinsic layers and text prompts. Extensive
experiments demonstrate the competitive performance of PRISM both for intrinsic
image decomposition and conditional image generation while preserving the base
model's text-to-image generation capability.

</details>

### [430] [HoLa: B-Rep Generation using a Holistic Latent Representation](https://arxiv.org/abs/2504.14257)
*Yilin Liu,Duoteng Xu,Xingyao Yu,Xiang Xu,Daniel Cohen-Or,Hao Zhang,Hui Huang*

Main category: cs.GR

TLDR: 提出了一种新的CAD模型表示方法，通过统一的HoLa空间整合几何和拓扑信息，显著提升了生成模型的效率和有效性。


<details>
  <summary>Details</summary>
Motivation: 解决传统B-Rep表示中几何与拓扑分离的问题，简化学习流程并提升生成质量。

Method: 通过神经交叉网络从表面几何推导曲线几何，消除潜在空间中的冗余信息，构建紧凑的HoLa空间。

Result: 生成模型的效率显著提升，有效性达到82%，远超现有技术的约50%。

Conclusion: HoLa空间为CAD模型生成提供了更高效、更统一的解决方案，具有广泛的应用潜力。

Abstract: We introduce a novel representation for learning and generating
Computer-Aided Design (CAD) models in the form of $\textit{boundary
representations}$ (B-Reps). Our representation unifies the continuous geometric
properties of B-Rep primitives in different orders (e.g., surfaces and curves)
and their discrete topological relations in a $\textit{holistic latent}$ (HoLa)
space. This is based on the simple observation that the topological connection
between two surfaces is intrinsically tied to the geometry of their
intersecting curve. Such a prior allows us to reformulate topology learning in
B-Reps as a geometric reconstruction problem in Euclidean space. Specifically,
we eliminate the presence of curves, vertices, and all the topological
connections in the latent space by learning to distinguish and derive curve
geometries from a pair of surface primitives via a neural intersection network.
To this end, our holistic latent space is only defined on surfaces but encodes
a full B-Rep model, including the geometry of surfaces, curves, vertices, and
their topological relations. Our compact and holistic latent space facilitates
the design of a first diffusion-based generator to take on a large variety of
inputs including point clouds, single/multi-view images, 2D sketches, and text
prompts. Our method significantly reduces ambiguities, redundancies, and
incoherences among the generated B-Rep primitives, as well as training
complexities inherent in prior multi-step B-Rep learning pipelines, while
achieving greatly improved validity rate over current state of the art: 82% vs.
$\approx$50%.

</details>

### [431] [SEGA: Drivable 3D Gaussian Head Avatar from a Single Image](https://arxiv.org/abs/2504.14373)
*Chen Guo,Zhuo Su,Jian Wang,Shuang Li,Xu Chang,Zhaohu Li,Yang Zhao,Guidong Wang,Ruqi Huang*

Main category: cs.GR

TLDR: SEGA提出了一种基于单图像的3D可驱动高斯头像生成方法，结合了先验模型和分层UV空间高斯泼溅框架，实现了高质量的头像生成与动画。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖多图像或多视角输入的限制，提升单图像生成3D头像的实用性和质量。

Method: 结合2D和3D先验模型，采用分层UV空间高斯泼溅框架，动态和静态分支分离处理面部细节，支持实时动画和渲染。

Result: 在泛化能力、身份保持和表情真实性方面优于现有方法，支持实时性能。

Conclusion: SEGA为单图像3D头像生成提供了高效、高质量的解决方案，适用于实际应用。

Abstract: Creating photorealistic 3D head avatars from limited input has become
increasingly important for applications in virtual reality, telepresence, and
digital entertainment. While recent advances like neural rendering and 3D
Gaussian splatting have enabled high-quality digital human avatar creation and
animation, most methods rely on multiple images or multi-view inputs, limiting
their practicality for real-world use. In this paper, we propose SEGA, a novel
approach for Single-imagE-based 3D drivable Gaussian head Avatar creation that
combines generalized prior models with a new hierarchical UV-space Gaussian
Splatting framework. SEGA seamlessly combines priors derived from large-scale
2D datasets with 3D priors learned from multi-view, multi-expression, and
multi-ID data, achieving robust generalization to unseen identities while
ensuring 3D consistency across novel viewpoints and expressions. We further
present a hierarchical UV-space Gaussian Splatting framework that leverages
FLAME-based structural priors and employs a dual-branch architecture to
disentangle dynamic and static facial components effectively. The dynamic
branch encodes expression-driven fine details, while the static branch focuses
on expression-invariant regions, enabling efficient parameter inference and
precomputation. This design maximizes the utility of limited 3D data and
achieves real-time performance for animation and rendering. Additionally, SEGA
performs person-specific fine-tuning to further enhance the fidelity and
realism of the generated avatars. Experiments show our method outperforms
state-of-the-art approaches in generalization ability, identity preservation,
and expression realism, advancing one-shot avatar creation for practical
applications.

</details>

### [432] [A Controllable Appearance Representation for Flexible Transfer and Editing](https://arxiv.org/abs/2504.15028)
*Santiago Jimenez-Navarro,Julia Guerrero-Viu,Belen Masia*

Main category: cs.GR

TLDR: 提出一种自监督学习方法，通过FactorVAE构建紧凑且解耦的潜在空间，用于表示材料外观，并用于指导扩散模型实现外观迁移和编辑。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖人工标注数据，可能引入偏见，且缺乏对材料外观的直观控制。本文旨在通过自监督学习构建解耦的潜在空间，实现高效且可解释的外观表示与编辑。

Method: 采用改进的FactorVAE进行自监督学习，构建解耦的潜在空间；利用该空间指导轻量级IP-Adapter训练，结合扩散模型实现外观迁移与编辑。

Result: 模型在无显式监督下实现了强解耦和可解释性，用户可通过直观操作（如色调或光泽度）精细控制生成结果。

Conclusion: 该方法提供了一种高效且用户友好的材料外观表示与编辑框架，适用于实际应用。

Abstract: We present a method that computes an interpretable representation of material
appearance within a highly compact, disentangled latent space. This
representation is learned in a self-supervised fashion using an adapted
FactorVAE. We train our model with a carefully designed unlabeled dataset,
avoiding possible biases induced by human-generated labels. Our model
demonstrates strong disentanglement and interpretability by effectively
encoding material appearance and illumination, despite the absence of explicit
supervision. Then, we use our representation as guidance for training a
lightweight IP-Adapter to condition a diffusion pipeline that transfers the
appearance of one or more images onto a target geometry, and allows the user to
further edit the resulting appearance. Our approach offers fine-grained control
over the generated results: thanks to the well-structured compact latent space,
users can intuitively manipulate attributes such as hue or glossiness in image
space to achieve the desired final appearance.

</details>

<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [433] [System of Agentic AI for the Discovery of Metal-Organic Frameworks](https://arxiv.org/abs/2504.14110)
*Theo Jaffrelot Inizan,Sherry Yang,Aaron Kaplan,Yen-hsu Lin,Jian Yin,Saber Mirzaei,Mona Abdelgaid,Ali H. Alawadhi,KwangHwan Cho,Zhiling Zheng,Ekin Dogus Cubuk,Christian Borgs,Jennifer T. Chayes,Kristin A. Persson,Omar M. Yaghi*

Main category: cond-mat.mtrl-sci

TLDR: MOFGen是一个由多个AI代理组成的系统，用于加速MOF材料的发现，成功生成了大量新结构并验证了其合成可行性。


<details>
  <summary>Details</summary>
Motivation: 解决在巨大化学空间中导航并确保材料可合成性的挑战，加速MOF在CO2捕获和水收集中的应用。

Method: 结合大型语言模型、扩散模型、量子力学代理和合成可行性代理，利用实验和计算数据库训练。

Result: 生成了数十万种新MOF结构和可合成有机连接体，并通过实验验证了五种AI设计的MOF。

Conclusion: MOFGen代表了自动化可合成材料发现的重要进展。

Abstract: Generative models and machine learning promise accelerated material discovery
in MOFs for CO2 capture and water harvesting but face significant challenges
navigating vast chemical spaces while ensuring synthetizability. Here, we
present MOFGen, a system of Agentic AI comprising interconnected agents: a
large language model that proposes novel MOF compositions, a diffusion model
that generates crystal structures, quantum mechanical agents that optimize and
filter candidates, and synthetic-feasibility agents guided by expert rules and
machine learning. Trained on all experimentally reported MOFs and computational
databases, MOFGen generated hundreds of thousands of novel MOF structures and
synthesizable organic linkers. Our methodology was validated through
high-throughput experiments and the successful synthesis of five "AI-dreamt"
MOFs, representing a major step toward automated synthesizable material
discovery.

</details>

### [434] [Machine learning enhanced atom probe tomography analysis: a snapshot review](https://arxiv.org/abs/2504.14378)
*Yue Li,Ye Wei,Alaukik Saxena,Markus Kühbach,Christoph Freysoldt,Baptiste Gault*

Main category: cond-mat.mtrl-sci

TLDR: 本文综述了原子探针层析成像（APT）领域的发展，特别是机器学习（ML）在APT数据分析中的应用，旨在解决传统方法中的偏见和效率问题。


<details>
  <summary>Details</summary>
Motivation: APT数据的分析长期以来依赖用户经验，导致偏见和效率低下，难以标准化和符合FAIR数据原则。ML的应用有望实现用户独立性、高效性和统计稳健性。

Method: 文章首先介绍APT及其数据特性，然后综述相关ML算法及其在APT中的应用，探讨ML如何超越人类能力发现新材料机制。

Result: ML在APT数据分析中展现出高效、可重复和统计稳健的优势，为材料科学提供了新见解。

Conclusion: 未来研究方向包括进一步优化ML算法，推动APT数据分析的标准化和自动化。

Abstract: Atom probe tomography (APT) is a burgeoning characterization technique that
provides compositional mapping of materials in three-dimensions at near-atomic
scale. Since its significant expansion in the past 30 years, we estimate that
one million APT datasets have been collected, each containing millions to
billions of individual ions. Their analysis and the extraction of
microstructural information has largely relied upon individual users whose
varied level of expertise causes clear and documented bias. Current practices
hinder efficient data processing, and make challenging standardization and the
deployment of data analysis workflows that would be compliant with FAIR data
principles. Over the past decade, building upon the long-standing expertise of
the APT community in the development of advanced data processing or data mining
techniques, there has been a surge of novel machine learning (ML) approaches
aiming for user-independence, and that are efficient, reproducible, and robust
from a statistics perspective. Here, we provide a snapshot review of this
rapidly evolving field. We begin with a brief introduction to APT and the
nature of the APT data. This is followed by an overview of relevant ML
algorithms and a comprehensive review of their applications to APT. We also
discuss how ML can enable discoveries beyond human capability, offering new
insights into the mechanisms within materials. Finally, we provide guidance for
future directions in this domain.

</details>

<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [435] [6G WavesFM: A Foundation Model for Sensing, Communication, and Localization](https://arxiv.org/abs/2504.14100)
*Ahmed Aboulfotouh,Elsayed Mohammed,Hatem Abou-Zeid*

Main category: eess.SP

TLDR: WavesFM是一种新型无线基础模型框架，支持多种通信、感知和定位任务，通过共享参数和高效微调实现高性能和低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 解决多任务无线场景中参数冗余和计算资源浪费的问题，推动AI原生范式在未来6G网络中的应用。

Method: 结合共享Vision Transformer主干和任务特定MLP头，采用LoRA进行参数高效微调，处理图像类无线模态和IQ信号。

Result: 在5G NR定位、MIMO-OFDM信道估计等任务中表现优于单独训练的基线模型，参数共享率达80%，训练时间减少5倍。

Conclusion: WavesFM展示了基础模型在无线任务中的潜力，为6G网络的AI原生范式提供了高效解决方案。

Abstract: This paper introduces WavesFM, a novel Wireless Foundation Model (WFM)
framework, capable of supporting a wide array of communication, sensing, and
localization tasks. Our proposed architecture combines a shared Vision
Transformer (ViT) backbone with task-specific multi-layer perceptron (MLP)
heads and incorporates Low-Rank Adaptation (LoRA) for parameter-efficient
fine-tuning. This design promotes full parameter sharing across tasks,
significantly reducing the computational and memory footprint without
sacrificing performance. The model processes both image-like wireless
modalities, such as spectrograms and channel state information (CSI), and
in-phase and quadrature (IQ) signals arranged as orthogonal frequency-division
multiplexing (OFDM) resource grids. We demonstrate the strong generalization
capabilities of WavesFM through extensive experiments on four downstream tasks:
Fifth Generation New Radio (5G NR) positioning; multiple-input multiple-output
OFDM (MIMO-OFDM) channel estimation; human activity sensing; and
radio-frequency (RF) signal classification. Compared to supervised baselines
trained individually, our approach achieves superior performance while sharing
80% of its parameters across tasks. Furthermore, we show that pretraining on
domain-relevant data not only boosts performance but also accelerates
convergence, reducing training time by up to 5x. These results demonstrate that
our unified WFM can support diverse tasks and deliver significant gains in both
performance and efficiency, highlighting the transformative potential of
foundation models to drive AI-native paradigms in future sixth-generation (6G)
networks.

</details>

<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [436] [Reveal-or-Obscure: A Differentially Private Sampling Algorithm for Discrete Distributions](https://arxiv.org/abs/2504.14696)
*Naima Tasnim,Atefeh Gilani,Lalitha Sankar,Oliver Kosut*

Main category: cs.IT

TLDR: 论文提出了一种差分隐私算法ROO和其改进版DS-ROO，用于从离散分布中生成代表性样本，并证明了其隐私性和效用改进。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过显式添加噪声实现差分隐私，但可能影响数据效用。ROO和DS-ROO通过随机选择是否“揭示”或“遮蔽”数据分布，优化隐私与效用的权衡。

Method: ROO通过随机选择揭示或遮蔽实现差分隐私；DS-ROO进一步自适应调整遮蔽概率，优化效用。

Result: ROO的采样复杂度优于现有方法；DS-ROO在相同隐私预算下实现更高效用。

Conclusion: ROO和DS-ROO为差分隐私数据发布提供了更优的解决方案，尤其在效用改进方面表现突出。

Abstract: We introduce a differentially private (DP) algorithm called reveal-or-obscure
(ROO) to generate a single representative sample from a dataset of $n$
observations drawn i.i.d. from an unknown discrete distribution $P$. Unlike
methods that add explicit noise to the estimated empirical distribution, ROO
achieves $\epsilon$-differential privacy by randomly choosing whether to
"reveal" or "obscure" the empirical distribution. While ROO is structurally
identical to Algorithm 1 proposed by Cheu and Nayak (arXiv:2412.10512), we
prove a strictly better bound on the sampling complexity than that established
in Theorem 12 of (arXiv:2412.10512). To further improve the privacy-utility
trade-off, we propose a novel generalized sampling algorithm called
Data-Specific ROO (DS-ROO), where the probability of obscuring the empirical
distribution of the dataset is chosen adaptively. We prove that DS-ROO
satisfies $\epsilon$-DP, and provide empirical evidence that DS-ROO can achieve
better utility under the same privacy budget of vanilla ROO.

</details>

### [437] [Generalized Derangetropy Functionals for Modeling Cyclical Information Flow](https://arxiv.org/abs/2504.14605)
*Masoud Ataei,Xiaogang Wang*

Main category: cs.IT

TLDR: 本文提出了一种基于熵调制变换的框架，用于建模循环和反馈驱动的信息流，通过非线性微分方程描述信息结构的动态演化。


<details>
  <summary>Details</summary>
Motivation: 传统熵度量（如香农熵）是静态和标量的，无法捕捉信息流的周期性和自反馈特性，因此需要一种更动态的建模方法。

Method: 提出了一种称为derangetropy泛函的熵调制变换家族，通过非线性微分方程描述信息结构的动态演化，并递归应用这些算子。

Result: 递归算子诱导了由热方程控制的光谱扩散过程，最终收敛于高斯特征函数，为周期性调制下信息的长期动态提供了统一分析基础。

Conclusion: 该框架为具有周期性结构、随机反馈和延迟交互的系统提供了新的信息动态分析工具，适用于人工神经网络、通信理论和非平衡统计力学等领域。

Abstract: This paper introduces a framework for modeling cyclical and feedback-driven
information flow through a generalized family of entropy-modulated
transformations called derangetropy functionals. Unlike scalar and static
entropy measures such as Shannon entropy, these functionals act directly on
probability densities and provide a topographical representation of information
structure across the support of the distribution. The framework captures
periodic and self-referential aspects of information distribution and encodes
them through functional operators governed by nonlinear differential equations.
When applied recursively, these operators induce a spectral diffusion process
governed by the heat equation, leading to convergence toward a Gaussian
characteristic function. This convergence theorem provides a unified analytical
foundation for describing the long-term dynamics of information under cyclic
modulation. The proposed framework offers new tools for analyzing the temporal
evolution of information in systems characterized by periodic structure,
stochastic feedback, and delayed interaction, with applications in artificial
neural networks, communication theory, and non-equilibrium statistical
mechanics.

</details>

<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [438] [A Practical Protocol for Quantum Oblivious Transfer from One-Way Functions](https://arxiv.org/abs/2406.09110)
*Eleni Diamanti,Alex B. Grilo,Adriano Innocenzi,Pascal Lefebvre,Verena Yacoub,Álvaro Yángüez*

Main category: quant-ph

TLDR: 提出了一种基于单向函数的量子不经意传输（QOT）协议，具有模拟安全性，并在效率上优于先前工作。


<details>
  <summary>Details</summary>
Motivation: 解决QOT协议在实际实现中的效率问题，并提供实验误差的纠正方法。

Method: 通过一种可扩展和松弛可提取的量子比特承诺技术实现模拟安全性。

Result: 协议在效率上优于现有方案，并提供了分析量子资源需求的解析表达式。

Conclusion: 该协议为QOT的实际实现提供了高效且安全的解决方案。

Abstract: We present a new simulation-secure quantum oblivious transfer (QOT) protocol
based on one-way functions in the plain model. With a focus on practical
implementation, our protocol surpasses prior works in efficiency, promising
feasible experimental realization. We address potential experimental errors and
their correction, offering analytical expressions to facilitate the analysis of
the required quantum resources. Technically, we achieve simulation security for
QOT through an equivocal and relaxed-extractable quantum bit commitment.

</details>

### [439] [Parallel Kac's Walk Generates PRU](https://arxiv.org/abs/2504.14957)
*Chuhan Lu,Minglong Qin,Fang Song,Penghui Yao,Mingnan Zhao*

Main category: quant-ph

TLDR: 本文证明了线性次数的并行Kac's Walk重复可以构建自适应安全的伪随机幺正族（PRU），并验证了其强安全性。


<details>
  <summary>Details</summary>
Motivation: 验证并行Kac's Walk的重复是否能构建自适应安全的PRU，并探索路径记录技术的应用。

Method: 通过线性次数的并行Kac's Walk重复，结合路径记录技术，构建PRU。

Result: 成功构建了自适应安全的PRU，并验证了其对逆查询的强安全性。

Conclusion: 本研究提供了另一种PRU构建方法，并展示了路径记录技术的有效性。

Abstract: Ma and Huang recently proved that the PFC construction, introduced by Metger,
Poremba, Sinha and Yuen [MPSY24], gives an adaptive-secure pseudorandom unitary
family PRU. Their proof developed a new path recording technique [MH24].
  In this work, we show that a linear number of sequential repetitions of the
parallel Kac's Walk, introduced by Lu, Qin, Song, Yao and Zhao [LQSY+24], also
forms an adaptive-secure PRU, confirming a conjecture therein. Moreover, it
additionally satisfies strong security against adversaries making inverse
queries. This gives an alternative PRU construction, and provides another
instance demonstrating the power of the path recording technique. We also
discuss some further simplifications and implications.

</details>

### [440] [Quantum pseudoresources imply cryptography](https://arxiv.org/abs/2504.15025)
*Alex B. Grilo,Álvaro Yángüez*

Main category: quant-ph

TLDR: 该论文探讨了量子伪资源在密码学中的应用，证明了其与EFI对的等价性，并提出了依赖纠缠的新密码学功能。


<details>
  <summary>Details</summary>
Motivation: 研究量子密码学中的最小假设，探索量子伪资源（如伪纠缠）的密码学潜力。

Method: 分析量子伪资源与EFI对的等价性，提出EPFI对的概念，并研究其在量子承诺中的应用。

Result: 证明了量子伪资源可以推导出EPFI对，且与量子承诺和EFI对等价。

Conclusion: 量子资源在量子密码学中可能扮演类似随机性在经典密码学中的关键角色。

Abstract: While one-way functions (OWFs) serve as the minimal assumption for
computational cryptography in the classical setting, in quantum cryptography,
we have even weaker cryptographic assumptions such as pseudo-random states, and
EFI pairs, among others. Moreover, the minimal assumption for computational
quantum cryptography remains an open question. Recently, it has been shown that
pseudoentanglement is necessary for the existence of quantum cryptography
(Goul\~ao and Elkouss 2024), but no cryptographic construction has been built
from it.
  In this work, we study the cryptographic usefulness of quantum
pseudoresources -- a pair of families of quantum states that exhibit a gap in
their resource content yet remain computationally indistinguishable. We show
that quantum pseudoresources imply a variant of EFI pairs, which we call EPFI
pairs, and that these are equivalent to quantum commitments and thus EFI pairs.
Our results suggest that, just as randomness is fundamental to classical
cryptography, quantum resources may play a similarly crucial role in the
quantum setting.
  Finally, we focus on the specific case of entanglement, analyzing different
definitions of pseudoentanglement and their implications for constructing EPFI
pairs. Moreover, we propose a new cryptographic functionality that is
intrinsically dependent on entanglement as a resource.

</details>

### [441] [Predicting fermionic densities using a Projected Quantum Kernel method](https://arxiv.org/abs/2504.14002)
*Francesco Perciavalle,Francesco Plastina,Michele Pisarra,Nicola Lo Gullo*

Main category: quant-ph

TLDR: 论文提出了一种基于投影量子核方法的支持向量回归器，用于预测一维费米子系统的密度结构，性能优于经典线性核方法。


<details>
  <summary>Details</summary>
Motivation: 研究量子化学和量子物质中一维费米子系统的密度结构预测问题，探索量子核方法在此领域的潜力。

Method: 使用基于量子储层可观测量的投影量子核方法构建支持向量回归器，训练和测试数据通过密度泛函理论生成。

Result: 在足够长的测量时间下，该方法性能优于经典线性核方法，并与径向基函数方法竞争。

Conclusion: 投影量子核方法在一维费米子系统密度结构预测中具有潜力，尤其在长测量时间下表现优异。

Abstract: We use a support vector regressor based on a projected quantum kernel method
to predict the density structure of 1D fermionic systems of interest in quantum
chemistry and quantum matter. The kernel is built on with the observables of a
quantum reservoir implementable with interacting Rydberg atoms. Training and
test data of the fermionic system are generated using a Density Functional
Theory approach. We test the performance of the method for several Hamiltonian
parameters, finding a general common behavior of the error as a function of
measurement time. At sufficiently large measurement times, we find that the
method outperforms the classical linear kernel method and can be competitive
with the radial basis function method.

</details>

### [442] [Guess, SWAP, Repeat : Capturing Quantum Snapshots in Classical Memory](https://arxiv.org/abs/2504.14459)
*Debarshi Kundu,Avimita Chatterjee,Swaroop Ghosh*

Main category: quant-ph

TLDR: 提出一种无需直接测量即可观察量子态的新技术，支持量子态的非破坏性保存和复用。


<details>
  <summary>Details</summary>
Motivation: 解决量子系统中因不可克隆定理和破坏性测量导致的调试、内省和持久内存难题。

Method: 采用硬件无关的机器学习框架，通过保真度估计和状态重构技术，支持量子态的经典存储与重建。

Result: 在IBM量子硬件上实现高保真度（约1.0）重建，仿真中平均保真度达0.999。

Conclusion: 为量子非易失性内存和未来量子内存架构奠定了基础。

Abstract: We introduce a novel technique that enables observation of quantum states
without direct measurement, preserving them for reuse. Our method allows
multiple quantum states to be observed at different points within a single
circuit, one at a time, and saved into classical memory without destruction.
These saved states can be accessed on demand by downstream applications,
introducing a dynamic and programmable notion of quantum memory that supports
modular, non-destructive quantum workflows. We propose a hardware-agnostic,
machine learning-driven framework to capture non-destructive estimates, or
"snapshots," of quantum states at arbitrary points within a circuit, enabling
classical storage and later reconstruction, similar to memory operations in
classical computing. This capability is essential for debugging, introspection,
and persistent memory in quantum systems, yet remains difficult due to the
no-cloning theorem and destructive measurements. Our guess-and-check approach
uses fidelity estimation via the SWAP test to guide state reconstruction. We
explore both gradient-based deep neural networks and gradient-free evolutionary
strategies to estimate quantum states using only fidelity as the learning
signal. We demonstrate a key component of our framework on IBM quantum
hardware, achieving high-fidelity (approximately 1.0) reconstructions for
Hadamard and other known states. In simulation, our models achieve an average
fidelity of 0.999 across 100 random quantum states. This provides a pathway
toward non-volatile quantum memory, enabling long-term storage and reuse of
quantum information, and laying groundwork for future quantum memory
architectures.

</details>

### [443] [Quantum-Enhanced Weight Optimization for Neural Networks Using Grover's Algorithm](https://arxiv.org/abs/2504.14568)
*Stefan-Alexandru Jura,Mihai Udrescu*

Main category: quant-ph

TLDR: 提出一种利用量子计算优化经典神经网络权重的新方法，通过Grover量子搜索算法加速训练过程，显著减少测试损失并提高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统梯度下降法存在梯度爆炸、消失和凸性问题，其他方法如遗传搜索也有收敛一致性问题。新方法避免了这些问题，并利用量子计算的优势。

Method: 设计Grover量子搜索算法优化神经网络权重，不依赖梯度计算，适用于高维搜索空间。

Result: 在小数据集上，测试损失减少58.75%，准确率提高35.25%；在3层隐藏网络的Digits数据集上平均准确率达97.7%。

Conclusion: 该方法不仅在小数据集上表现优异，还具有可扩展性和低量子比特需求，适合近期量子计算机。

Abstract: The main approach to hybrid quantum-classical neural networks (QNN) is
employing quantum computing to build a neural network (NN) that has quantum
features, which is then optimized classically. Here, we propose a different
strategy: to use quantum computing in order to optimize the weights of a
classical NN. As such, we design an instance of Grover's quantum search
algorithm to accelerate the search for the optimal parameters of an NN during
the training process, a task traditionally performed using the backpropagation
algorithm with the gradient descent method. Indeed, gradient descent has issues
such as exploding gradient, vanishing gradient, or convexity problem. Other
methods tried to address such issues with strategies like genetic searches, but
they carry additional problems like convergence consistency. Our original
method avoids these issues -- because it does not calculate gradients -- and
capitalizes on classical architectures' robustness and Grover's quadratic
speedup in high-dimensional search spaces to significantly reduce test loss
(58.75%) and improve test accuracy (35.25%), compared to classical NN weight
optimization, on small datasets. Unlike most QNNs that are trained on small
datasets only, our method is also scalable, as it allows the optimization of
deep networks; for an NN with 3 hidden layers, trained on the Digits dataset
from scikit-learn, we obtained a mean accuracy of 97.7%. Moreover, our method
requires a much smaller number of qubits compared to other QNN approaches,
making it very practical for near-future quantum computers that will still
deliver a limited number of logical qubits.

</details>

### [444] [Trainable Quantum Neural Network for Multiclass Image Classification with the Power of Pre-trained Tree Tensor Networks](https://arxiv.org/abs/2504.14995)
*Keisuke Murota,Takumi Kobori*

Main category: quant-ph

TLDR: 该论文提出了一种森林张量网络（FTN）分类器，用于解决将树张量网络（TTN）嵌入量子神经网络（QNN）时的高阶门操作和中间电路后选择问题，并通过实验验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 将TTN分类器嵌入QNN以提升多类图像分类性能，但面临大键维数的高阶门操作和低成功率的中间电路后选择问题。

Method: 提出FTN分类器，通过聚合多个小键维数TTN避免大键维数问题，并扩展绝热编码框架以消除中间电路后选择的开销。

Result: 在MNIST和CIFAR-10数据集上的实验表明，FTN分类器在嵌入量子网络后性能保持甚至提升。

Conclusion: TTN与QNN的结合为多类量子增强图像分类提供了可扩展且稳健的框架。

Abstract: Tree tensor networks (TTNs) offer powerful models for image classification.
While these TTN image classifiers already show excellent performance on
classical hardware, embedding them into quantum neural networks (QNNs) may
further improve the performance by leveraging quantum resources. However,
embedding TTN classifiers into QNNs for multiclass classification remains
challenging. Key obstacles are the highorder gate operations required for large
bond dimensions and the mid-circuit postselection with exponentially low
success rates necessary for the exact embedding. In this work, to address these
challenges, we propose forest tensor network (FTN)-classifiers, which aggregate
multiple small-bond-dimension TTNs. This allows us to handle multiclass
classification without requiring large gates in the embedded circuits. We then
remove the overhead of mid-circuit postselection by extending the adiabatic
encoding framework to our setting and smoothly encode the FTN-classifiers into
a quantum forest tensor network (qFTN)- classifiers. Numerical experiments on
MNIST and CIFAR-10 demonstrate that we can successfully train FTN-classifiers
and encode them into qFTN-classifiers, while maintaining or even improving the
performance of the pre-trained FTN-classifiers. These results suggest that
synergy between TTN classification models and QNNs can provide a robust and
scalable framework for multiclass quantum-enhanced image classification.

</details>

<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [445] [A CMOS Probabilistic Computing Chip With In-situ hardware Aware Learning](https://arxiv.org/abs/2504.14070)
*Jinesh Jhonsa,William Whitehead,David McCarthy,Shuvro Chowdhury,Kerem Camsari,Luke Theogarajan*

Main category: cs.AR

TLDR: 论文展示了一种基于概率比特物理的求解器，采用440个自旋的Chimera图结构，面积效率高，并通过硬件感知的对比散度算法解决工艺变化问题。


<details>
  <summary>Details</summary>
Motivation: 开发一种面积高效且能处理概率计算和优化任务的硬件解决方案，适用于AI和机器学习。

Method: 采用电流模式的神经元更新电路、标准单元设计以及共享电源，结合硬件感知的对比散度算法。

Result: 芯片成功实现了逻辑门、全加器建模以及MaxCut等优化任务。

Conclusion: 该芯片在AI和机器学习应用中具有潜力。

Abstract: This paper demonstrates a probabilistic bit physics inspired solver with 440
spins configured in a Chimera graph, occupying an area of 0.44 mm^2. Area
efficiency is maximized through a current-mode implementation of the neuron
update circuit, standard cell design for analog blocks pitch-matched to digital
blocks, and a shared power supply for both digital and analog components.
Process variation related mismatches introduced by this approach are
effectively mitigated using a hardware aware contrastive divergence algorithm
during training. We validate the chip's ability to perform probabilistic
computing tasks such as modeling logic gates and full adders, as well as
optimization tasks such as MaxCut, demonstrating its potential for AI and
machine learning applications.

</details>

### [446] [FGMP: Fine-Grained Mixed-Precision Weight and Activation Quantization for Hardware-Accelerated LLM Inference](https://arxiv.org/abs/2504.14152)
*Coleman Hooper,Charbel Sakr,Ben Keller,Rangharajan Venkatesan,Kurt Keutzer,Sophia Shao,Brucek Khailany*

Main category: cs.AR

TLDR: 该论文提出了一种细粒度混合精度（FGMP）量化方法，通过硬件-软件协同设计，在保持模型精度的同时，将大部分权重和激活量化为低精度，从而提升大型语言模型（LLM）的推理效率。


<details>
  <summary>Details</summary>
Motivation: 量化是提升LLM推理效率的有效方法，但传统量化方法在低精度下容易导致模型精度下降。因此，需要一种既能保持精度又能实现高效量化的方法。

Method: 1) 提出基于Fisher信息的扰动策略，选择需要保留高精度的权重和激活块；2) 提出敏感性加权裁剪方法，优化低精度量化块的精度；3) 设计支持FGMP的硬件增强功能，包括块粒度数据路径和动态混合精度激活量化单元。

Result: 实验表明，FGMP量化在Llama-2-7B模型上实现了<1%的困惑度下降，同时推理能耗降低14%，权重内存需求减少30%。

Conclusion: FGMP量化是一种高效的硬件-软件协同设计方法，能够在保持模型精度的同时显著提升LLM的推理效率。

Abstract: Quantization is a powerful tool to improve large language model (LLM)
inference efficiency by utilizing more energy-efficient low-precision datapaths
and reducing memory footprint. However, accurately quantizing LLM weights and
activations to low precision is challenging without degrading model accuracy.
We propose fine-grained mixed precision (FGMP) quantization, a post-training
mixed-precision quantization hardware-software co-design methodology that
maintains accuracy while quantizing the majority of weights and activations to
reduced precision. Our work makes the following contributions: 1) We develop a
policy that uses the perturbation in each value, weighted by the Fisher
information, to select which weight and activation blocks to keep in higher
precision. This approach preserves accuracy by identifying which weight and
activation blocks need to be retained in higher precision to minimize the
perturbation in the model loss. 2) We also propose a sensitivity-weighted
clipping approach for fine-grained quantization which helps retain accuracy for
blocks that are quantized to low precision. 3) We then propose hardware
augmentations to leverage the efficiency benefits of FGMP quantization. Our
hardware implementation encompasses i) datapath support for FGMP at block
granularity, and ii) a mixed-precision activation quantization unit to assign
activation blocks to high or low precision on the fly with minimal runtime and
energy overhead. Our design, prototyped using NVFP4 (an FP4 format with
microscaling) as the low-precision datatype and FP8 as the high-precision
datatype, facilitates efficient FGMP quantization, attaining <1% perplexity
degradation on Wikitext-103 for the Llama-2-7B model relative to an all-FP8
baseline design while consuming 14% less energy during inference and requiring
30% less weight memory.

</details>

### [447] [ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid Reasoning Model](https://arxiv.org/abs/2504.14560)
*Haiyan Qin,Zhiwei Xie,Jingjing Li,Liangchen Li,Xiaotong Feng,Junzhan Liu,Wang Kang*

Main category: cs.AR

TLDR: ReasoningV是一种新型混合推理模型，通过高质量数据集、两阶段训练和自适应推理机制，显著提升了Verilog代码生成的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在Verilog代码生成中面临的数据质量、推理能力和计算效率问题。

Method: 结合高质量数据集（ReasoningV-5K）、两阶段训练（参数高效微调与全参数优化）和自适应推理机制。

Result: 在VerilogEval-human上达到57.8%的pass@1准确率，性能接近商业模型并超越开源模型。

Conclusion: ReasoningV为AI驱动的硬件设计自动化提供了更可靠和高效的解决方案。

Abstract: Large Language Models (LLMs) have advanced Verilog code generation
significantly, yet face challenges in data quality, reasoning capabilities, and
computational efficiency. This paper presents ReasoningV, a novel model
employing a hybrid reasoning strategy that integrates trained intrinsic
capabilities with dynamic inference adaptation for Verilog code generation. Our
framework introduces three complementary innovations: (1) ReasoningV-5K, a
high-quality dataset of 5,000 functionally verified instances with reasoning
paths created through multi-dimensional filtering of PyraNet samples; (2) a
two-stage training approach combining parameter-efficient fine-tuning for
foundational knowledge with full-parameter optimization for enhanced reasoning;
and (3) an adaptive reasoning mechanism that dynamically adjusts reasoning
depth based on problem complexity, reducing token consumption by up to 75\%
while preserving performance. Experimental results demonstrate ReasoningV's
effectiveness with a pass@1 accuracy of 57.8\% on VerilogEval-human, achieving
performance competitive with leading commercial models like Gemini-2.0-flash
(59.5\%) and exceeding the previous best open-source model by 10.4 percentage
points. ReasoningV offers a more reliable and accessible pathway for advancing
AI-driven hardware design automation, with our model, data, and code available
at https://github.com/BUAA-CLab/ReasoningV.

</details>

### [448] [Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets Collective Intelligence](https://arxiv.org/abs/2504.14625)
*Haiyan Qin,Jiahao Feng,Xiaotong Feng,Wei W. Xing,Wang Kang*

Main category: cs.AR

TLDR: CircuitMind是一个多代理框架，通过语法锁定、检索增强生成和双奖励优化，实现了与人类专家相媲美的硬件设计效率。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代码生成方面表现出色，但在硬件设计中生成的电路门数远高于人类设计，因此需要一种更高效的解决方案。

Method: 采用语法锁定约束生成基本逻辑门，检索增强生成实现知识驱动设计，双奖励优化平衡正确性与效率。

Result: 55.6%的模型实现达到或超过人类专家的效率，14B Phi-4模型表现优于GPT-4o mini和Gemini 2.0 Flash。

Conclusion: CircuitMind为硬件优化提供了新范式，通过协作AI系统利用集体人类知识实现最优电路设计。

Abstract: Large language models (LLMs) have transformed code generation, yet their
application in hardware design produces gate counts 38\%--1075\% higher than
human designs. We present CircuitMind, a multi-agent framework that achieves
human-competitive efficiency through three key innovations: syntax locking
(constraining generation to basic logic gates), retrieval-augmented generation
(enabling knowledge-driven design), and dual-reward optimization (balancing
correctness with efficiency). To evaluate our approach, we introduce TC-Bench,
the first gate-level benchmark harnessing collective intelligence from the
TuringComplete ecosystem -- a competitive circuit design platform with hundreds
of thousands of players. Experiments show CircuitMind enables 55.6\% of model
implementations to match or exceed top-tier human experts in composite
efficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model
to outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency
comparable to the top 25\% of human experts without requiring specialized
training. These innovations establish a new paradigm for hardware optimization
where collaborative AI systems leverage collective human expertise to achieve
optimal circuit designs. Our model, data, and code are open-source at
https://github.com/BUAA-CLab/CircuitMind.

</details>

<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [449] [Integrating Response Time and Attention Duration in Bayesian Preference Learning for Multiple Criteria Decision Aiding](https://arxiv.org/abs/2504.14938)
*Jiaxuan Jiang,Jiapeng Liu,Miłosz Kadziński,Xiuwu Liao,Jingyu Dong*

Main category: stat.AP

TLDR: 提出了一种结合行为线索的多准则贝叶斯偏好学习框架，通过整合成对比较、反应时间和注意力时长，提升对决策过程的洞察力。


<details>
  <summary>Details</summary>
Motivation: 传统方法在捕捉决策者行为模式方面存在不足，需要更丰富的模型来反映实际偏好。

Method: 采用加性价值函数模型和贝叶斯框架，通过定义偏好数据的似然和先验结构，推导潜在排序模型的后验分布。

Result: 实验验证了新方法在重建完整偏好方面的能力，并揭示了与时间和注意力相关的行为模式。

Conclusion: 整合多源行为数据能开发出更符合决策者实际偏好的模型，优于传统方法。

Abstract: We introduce a multiple criteria Bayesian preference learning framework
incorporating behavioral cues for decision aiding. The framework integrates
pairwise comparisons, response time, and attention duration to deepen insights
into decision-making processes. The approach employs an additive value function
model and utilizes a Bayesian framework to derive the posterior distribution of
potential ranking models by defining the likelihood of observed preference data
and specifying a prior on the preference structure. This distribution
highlights each model's ability to reconstruct Decision-Makers' holistic
pairwise comparisons. By leveraging both response time as a proxy for cognitive
effort and alternative discriminability as well as attention duration as an
indicator of criterion importance, the proposed model surpasses traditional
methods by uncovering richer behavioral patterns. We report the results of a
laboratory experiment on mobile phone contract selection involving 30 real
subjects using a dedicated application with time-, eye-, and mouse-tracking
components. We validate the novel method's ability to reconstruct complete
preferences. The detailed ablation studies reveal time- and attention-related
behavioral patterns, confirming that integrating comprehensive data leads to
developing models that better align with the DM's actual preferences.

</details>

<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [450] [IoT-AMLHP: Aligned Multimodal Learning of Header-Payload Representations for Resource-Efficient Malicious IoT Traffic Classification](https://arxiv.org/abs/2504.14833)
*Fengyuan Nie,Guangjie Liu,Weiwei Liu,Jianan Huang,Bo Gao*

Main category: cs.NI

TLDR: 本文提出了一种名为IoT-AMLHP的多模态学习框架，用于资源高效的恶意IoT流量分类，通过解析包头和负载字节构建对齐的多模态流量表示，并采用轻量级神经网络进行特征提取和融合。


<details>
  <summary>Details</summary>
Motivation: IoT设备的有限计算和空间资源限制了复杂深度学习模型的部署，现有方法依赖流级特征或原始包字节特征，分别存在资源消耗高或忽略语义差异的问题。

Method: 提出IoT-AMLHP框架，构建包级的包头-负载表示，采用深度可分离卷积提取多尺度特征，并通过自适应融合模块整合多模态特征。

Result: 该框架能够高效表征异构IoT流量，同时保持轻量级架构，适用于资源受限的IoT设备。

Conclusion: IoT-AMLHP为资源受限环境下的恶意IoT流量分类提供了一种有效的解决方案。

Abstract: Traffic classification is crucial for securing Internet of Things (IoT)
networks. Deep learning-based methods can autonomously extract latent patterns
from massive network traffic, demonstrating significant potential for IoT
traffic classification tasks. However, the limited computational and spatial
resources of IoT devices pose challenges for deploying more complex deep
learning models. Existing methods rely heavily on either flow-level features or
raw packet byte features. Flow-level features often require inspecting entire
or most of the traffic flow, leading to excessive resource consumption, while
raw packet byte features fail to distinguish between headers and payloads,
overlooking semantic differences and introducing noise from feature
misalignment. Therefore, this paper proposes IoT-AMLHP, an aligned multimodal
learning framework for resource-efficient malicious IoT traffic classification.
Firstly, the framework constructs a packet-wise header-payload representation
by parsing packet headers and payload bytes, resulting in an aligned and
standardized multimodal traffic representation that enhances the
characterization of heterogeneous IoT traffic. Subsequently, the traffic
representation is fed into a resource-efficient neural network comprising a
multimodal feature extraction module and a multimodal fusion module. The
extraction module employs efficient depthwise separable convolutions to capture
multi-scale features from different modalities while maintaining a lightweight
architecture. The fusion module adaptively captures complementary features from
different modalities and effectively fuses multimodal features.

</details>

### [451] [Planet as a Brain: Towards Internet of AgentSites based on AIOS Server](https://arxiv.org/abs/2504.14411)
*Xiang Zhang,Yongfeng Zhang*

Main category: cs.NI

TLDR: 论文介绍了AIOS Server，一个支持AI代理托管和全球协作的运行时框架，并展示了首个实际部署的AgentSites互联网（AIOS-IoA）。


<details>
  <summary>Details</summary>
Motivation: 互联网正从传统的‘网站互联网’向‘代理站点互联网’转变，需要一种基础设施支持AI代理的开发、部署和执行。

Method: 提出AIOS Server框架，利用MCP和JSON-RPC协议实现代理间或人机交互，支持去中心化协作。

Result: 成功部署了AIOS-IoA，包括AgentHub和AgentChat，并基于DHT和Gossip协议实现代理发现机制。

Conclusion: AIOS Server为构建以自主代理为核心的互联网新范式提供了实践基础。

Abstract: The internet is undergoing a historical transformation from the "Internet of
Websites" to the "Internet of AgentSites." While traditional Websites served as
the foundation for information hosting and dissemination, a new frontier is
emerging where AgentSites serve as the hubs of the internet, where each
AgentSite hosts one or more AI agents that receive tasks, address them, and
deliver actionable solutions, marking a significant shift in the digital
landscape and representing the next generation of online ecosystems. Under this
vision, AIOS, the AI Agent Operating System, serves as the server for the
development, deployment and execution of AI agents, which is a fundamental
infrastructure for the Internet of Agentsites.
  In this paper, we introduce AIOS Server, a runtime framework to host agents
and enable global-scale collaboration among decentralized agents. AIOS Server
provides a communication protocol leveraging the Model Context Protocol (MCP)
and JSON-RPC to enable agent-agent or human-agent interactions. Each AIOS node
operates as a server to host and execute agents, while supporting peer-to-peer
coordination without reliance on centralized orchestration. Based on AIOS
Server, we further present the world's first practically deployed Internet of
Agentsites (AIOS-IoA), including AgentHub for agent registration and discovery
and AgentChat for interactive communication, at https://planet.aios.foundation.
The agent discovery mechanism based on Distributed Hash Tables (DHT) and a
Gossip protocol serves as the search engine for the internet of agentsites.
This work provides a practical foundation for building the Internet of
Agentsites-a new paradigm where autonomous agents become first-class citizens
of the web. The implementation is available at
https://github.com/agiresearch/AIOS.Server and will be integrated into the AIOS
main branch at https://github.com/agiresearch/AIOS.

</details>

### [452] [Uncovering Issues in the Radio Access Network by Looking at the Neighbors](https://arxiv.org/abs/2504.14686)
*José Suárez-Varela,Andra Lutu*

Main category: cs.NI

TLDR: c-ANEMON是一种基于图神经网络（GNN）的RAN异常检测工具，通过分析单个小区与其局部邻域的关系来检测异常，适用于大规模移动网络。


<details>
  <summary>Details</summary>
Motivation: 移动网络运营商（MNOs）管理多代无线网络（2G-5G）的大量小区，需要高效工具检测异常行为以应对复杂性问题。

Method: c-ANEMON利用GNN捕捉时空变化，分析小区行为与局部邻域的关系，独立于外部移动性因素检测异常。

Result: 在真实数据（7,890个小区；3个月）中验证，GNN模型能泛化到未见区域，45.95%的长期异常需操作团队干预。

Conclusion: c-ANEMON展示了GNN在RAN异常检测中的潜力，可扩展至大规模部署区域，并有效识别需干预的异常。

Abstract: Mobile network operators (MNOs) manage Radio Access Networks (RANs) with
massive amounts of cells over multiple radio generations (2G-5G). To handle
such complexity, operations teams rely on monitoring systems, including anomaly
detection tools that identify unexpected behaviors. In this paper, we present
c-ANEMON, a Contextual ANomaly dEtection MONitor for the RAN based on Graph
Neural Networks (GNNs). Our solution captures spatio-temporal variations by
analyzing the behavior of individual cells in relation to their local
neighborhoods, enabling the detection of anomalies that are independent of
external mobility factors. This, in turn, allows focusing on anomalies
associated with network issues (e.g., misconfigurations, equipment failures).
We evaluate c-ANEMON using real-world data from a large European metropolitan
area (7,890 cells; 3 months). First, we show that the GNN model within our
solution generalizes effectively to cells from previously unseen areas,
suggesting the possibility of using a single model across extensive deployment
regions. Then, we analyze the anomalies detected by c-ANEMON through manual
inspection and define several categories of long-lasting anomalies (6+ hours).
Notably, 45.95% of these anomalies fall into a category that is more likely to
require intervention by operations teams.

</details>

### [453] [Video QoE Metrics from Encrypted Traffic: Application-agnostic Methodology](https://arxiv.org/abs/2504.14720)
*Tamir Berger,Jonathan Sterenson,Raz Birman,Ofer Hadar*

Main category: cs.NI

TLDR: 论文提出了一种独立于应用程序的加密流量质量体验（QoE）估计方法，解决了网络运营商因加密流量无法获取终端设备QoE指标的问题。


<details>
  <summary>Details</summary>
Motivation: 现代通信中，即时消息视频通话应用（IMVCAs）和视频会议应用（VCAs）的QoE至关重要，但加密流量阻碍了网络运营商获取终端QoE指标。

Method: 提出一种应用无关的QoE估计方法，通过机器学习模型从加密流量中提取关键视频QoE指标。

Result: 在多样化数据集上验证，FPS预测准确率达85.2%（误差±2 FPS），PIQE质量分类准确率达90.2%。

Conclusion: 该方法具有广泛适用性，适用于多种专有IMVCAs和VCAs，为加密流量下的QoE评估提供了有效解决方案。

Abstract: Instant Messaging-Based Video Call Applications (IMVCAs) and Video
Conferencing Applications (VCAs) have become integral to modern communication.
Ensuring a high Quality of Experience (QoE) for users in this context is
critical for network operators, as network conditions significantly impact user
QoE. However, network operators lack access to end-device QoE metrics due to
encrypted traffic. Existing solutions estimate QoE metrics from encrypted
traffic traversing the network, with the most advanced approaches leveraging
machine learning models. Subsequently, the need for ground truth QoE metrics
for training and validation poses a challenge, as not all video applications
provide these metrics. To address this challenge, we propose an
application-agnostic approach for objective QoE estimation from encrypted
traffic. Independent of the video application, we obtained key video QoE
metrics, enabling broad applicability to various proprietary IMVCAs and VCAs.
To validate our solution, we created a diverse dataset from WhatsApp video
sessions under various network conditions, comprising 25,680 seconds of traffic
data and QoE metrics. Our evaluation shows high performance across the entire
dataset, with 85.2% accuracy for FPS predictions within an error margin of two
FPS, and 90.2% accuracy for PIQE-based quality rating classification.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [454] [Coordinating Spinal and Limb Dynamics for Enhanced Sprawling Robot Mobility](https://arxiv.org/abs/2504.14103)
*Merve Atasever,Ali Okhovat,Azhang Nazaripouya,John Nisbet,Omer Kurkutlu,Jyotirmoy V. Deshmukh,Yasemin Ozkan Aydin*

Main category: cs.RO

TLDR: 研究比较了基于学习的控制策略和生物启发的步态设计方法在蝾螈机器人上的表现，探讨了脊柱灵活性对运动的影响及环境不确定性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 脊椎动物中蝾螈能够在行走和游泳步态间切换，其脊柱灵活性对运动至关重要，但环境不确定性可能导致身体-肢体协调问题，需要动态适应策略。

Method: 采用深度强化学习（DRL）框架，比较学习型控制策略与生物启发步态设计方法在蝾螈机器人上的表现。

Result: 研究展示了DRL在非确定性环境中的适应性，以及生物启发方法在机器人运动中的潜力。

Conclusion: 研究为动态适应不确定环境的机器人控制策略提供了新思路，结合学习与生物启发方法可能更高效。

Abstract: Among vertebrates, salamanders, with their unique ability to transition
between walking and swimming gaits, highlight the role of spinal mobility in
locomotion. A flexible spine enables undulation of the body through a wavelike
motion along the spine, aiding navigation over uneven terrains and obstacles.
Yet environmental uncertainties, such as surface irregularities and variations
in friction, can significantly disrupt body-limb coordination and cause
discrepancies between predictions from mathematical models and real-world
outcomes. Addressing this challenge requires the development of sophisticated
control strategies capable of dynamically adapting to uncertain conditions
while maintaining efficient locomotion. Deep reinforcement learning (DRL)
offers a promising framework for handling non-deterministic environments and
enabling robotic systems to adapt effectively and perform robustly under
challenging conditions. In this study, we comparatively examine learning-based
control strategies and biologically inspired gait design methods on a
salamander-like robot.

</details>

### [455] [Knitting Robots: A Deep Learning Approach for Reverse-Engineering Fabric Patterns](https://arxiv.org/abs/2504.14007)
*Haoliang Sheng,Songpu Cai,Xingyu Zheng,Meng Cheng Lau*

Main category: cs.RO

TLDR: 提出了一种基于深度学习的逆向编织管道，将视觉机器人系统融入纺织制造，解决了编织自动化中的设计转换问题。


<details>
  <summary>Details</summary>
Motivation: 编织自动化面临将织物设计转换为机器可读指令的挑战，研究旨在填补纺织生产与机器人自动化之间的鸿沟。

Method: 采用两阶段深度学习架构，机器人先识别前标签再推断完整标签，适应单线和多线纱结构。

Result: 系统能处理标签不平衡、未充分代表的针法类型等挑战，实现精确、可扩展的图案生成。

Conclusion: 为全自动机器人编织系统奠定基础，推动纺织制造向智能自动化发展。

Abstract: Knitting, a cornerstone of textile manufacturing, is uniquely challenging to
automate, particularly in terms of converting fabric designs into precise,
machine-readable instructions. This research bridges the gap between textile
production and robotic automation by proposing a novel deep learning-based
pipeline for reverse knitting to integrate vision-based robotic systems into
textile manufacturing. The pipeline employs a two-stage architecture, enabling
robots to first identify front labels before inferring complete labels,
ensuring accurate, scalable pattern generation. By incorporating diverse yarn
structures, including single-yarn (sj) and multi-yarn (mj) patterns, this study
demonstrates how our system can adapt to varying material complexities.
Critical challenges in robotic textile manipulation, such as label imbalance,
underrepresented stitch types, and the need for fine-grained control, are
addressed by leveraging specialized deep-learning architectures. This work
establishes a foundation for fully automated robotic knitting systems, enabling
customizable, flexible production processes that integrate perception,
planning, and actuation, thereby advancing textile manufacturing through
intelligent robotic automation.

</details>

### [456] [Experience-based Refinement of Task Planning Knowledge in Autonomous Robots](https://arxiv.org/abs/2504.14259)
*Hadeel Jazzaa,Thomas McCluskey,David Peebles*

Main category: cs.RO

TLDR: 本文提出了一种方法，通过机器人执行动作的经验来调整其符号知识，从而提高任务计划的成功率。


<details>
  <summary>Details</summary>
Motivation: 自主机器人在动态环境中需要具备高级认知能力，但目前符号知识的调整方法尚未应用于实际物理机器人。

Method: 提出了一种通过机器人动作执行经验驱动知识调整的方法，以改进任务计划的鲁棒性。

Result: 在NAO机器人上实现并评估，结果显示随着错误知识的调整，任务计划的失败率逐渐降低。

Conclusion: 该方法通过知识调整提高了机器人在动态环境中的任务计划能力。

Abstract: The requirement for autonomous robots to exhibit higher-level cognitive
skills by planning and adapting in an ever-changing environment is indeed a
great challenge for the AI community. Progress has been made in the automated
planning community on refinement and repair of an agent's symbolic knowledge to
do task planning in an incomplete or changing environmental model, but these
advances up to now have not been transferred to real physical robots. This
paper demonstrates how a physical robot can be capable of adapting its symbolic
knowledge of the environment, by using experiences in robot action execution to
drive knowledge refinement and hence to improve the success rate of the task
plans the robot creates. To implement more robust planning systems, we propose
a method for refining domain knowledge to improve the knowledge on which
intelligent robot behavior is based. This architecture has been implemented and
evaluated using a NAO robot. The refined knowledge leads to the future
synthesis of task plans which demonstrate decreasing rates of failure over time
as faulty knowledge is removed or adjusted.

</details>

### [457] [Unreal Robotics Lab: A High-Fidelity Robotics Simulator with Advanced Physics and Rendering](https://arxiv.org/abs/2504.14135)
*Jonathan Embley-Riches,Jianwei Liu,Simon Julier,Dimitrios Kanoulas*

Main category: cs.RO

TLDR: 论文提出了一种名为Unreal Robotics Lab（URL）的新型仿真框架，结合了Unreal Engine的高质量渲染和MuJoCo的高精度物理模拟，用于机器人研究。


<details>
  <summary>Details</summary>
Motivation: 高保真仿真对机器人研究至关重要，但实现逼真渲染和精确物理模拟仍具挑战性。

Method: 通过整合Unreal Engine的渲染能力和MuJoCo的物理模拟，开发了URL框架，支持复杂环境效果（如烟雾、火、水）。

Result: 框架成功用于视觉导航和SLAM方法的基准测试，展示了其在多样化场景中的实用性。

Conclusion: URL框架填补了物理精度与逼真渲染之间的空白，为机器人研究和仿真到现实的迁移提供了有力工具。

Abstract: High-fidelity simulation is essential for robotics research, enabling safe
and efficient testing of perception, control, and navigation algorithms.
However, achieving both photorealistic rendering and accurate physics modeling
remains a challenge. This paper presents a novel simulation framework--the
Unreal Robotics Lab (URL) that integrates the Unreal Engine's advanced
rendering capabilities with MuJoCo's high-precision physics simulation. Our
approach enables realistic robotic perception while maintaining accurate
physical interactions, facilitating benchmarking and dataset generation for
vision-based robotics applications. The system supports complex environmental
effects, such as smoke, fire, and water dynamics, which are critical for
evaluating robotic performance under adverse conditions. We benchmark visual
navigation and SLAM methods within our framework, demonstrating its utility for
testing real-world robustness in controlled yet diverse scenarios. By bridging
the gap between physics accuracy and photorealistic rendering, our framework
provides a powerful tool for advancing robotics research and sim-to-real
transfer.

</details>

### [458] [Infrared Vision Systems for Emergency Vehicle Driver Assistance in Low-Visibility Conditions](https://arxiv.org/abs/2504.14078)
*M-Mahdi Naddaf-Sh,Andrew Lee,Kin Yen,Eemon Amini,Iman Soltani*

Main category: cs.RO

TLDR: 研究探讨红外（IR）摄像头技术在低能见度条件下提升紧急车辆驾驶员安全的潜力，尤其是在夜间和浓雾中。


<details>
  <summary>Details</summary>
Motivation: 低能见度环境（如夜间和浓雾）显著增加碰撞风险，尤其是拖车和除雪车等紧急车辆。传统辅助系统在此类条件下效果有限，而IR技术通过检测障碍物的热特征提供新解决方案。

Method: 结合实验室实验、实地测试和紧急车辆操作员调查，评估IR摄像头的检测性能及在现有交通部门车队中经济高效地改装IR系统的可行性。

Result: 结果证实IR技术能有效提升驾驶员意识，并为现有紧急车辆车队的大规模部署提供数据支持的建议。

Conclusion: IR摄像头技术是提升紧急车辆在低能见度条件下安全性的可行且经济高效的解决方案。

Abstract: This study investigates the potential of infrared (IR) camera technology to
enhance driver safety for emergency vehicles operating in low-visibility
conditions, particularly at night and in dense fog. Such environments
significantly increase the risk of collisions, especially for tow trucks and
snowplows that must remain operational in challenging conditions. Conventional
driver assistance systems often struggle under these conditions due to limited
visibility. In contrast, IR cameras, which detect the thermal signatures of
obstacles, offer a promising alternative. The evaluation combines controlled
laboratory experiments, real-world field tests, and surveys of emergency
vehicle operators. In addition to assessing detection performance, the study
examines the feasibility of retrofitting existing Department of Transportation
(DoT) fleets with cost-effective IR-based driver assistance systems. Results
underscore the utility of IR technology in enhancing driver awareness and
provide data-driven recommendations for scalable deployment across legacy
emergency vehicle fleets.

</details>

### [459] [SG-Reg: Generalizable and Efficient Scene Graph Registration](https://arxiv.org/abs/2504.14440)
*Chuhao Liu,Zhijian Qiao,Jieqi Shi,Ke Wang,Peize Liu,Shaojie Shen*

Main category: cs.RO

TLDR: 提出了一种基于多模态语义节点特征的场景图网络，用于刚性语义场景图的注册，减少了对GPU资源和通信带宽的需求，并在多智能体任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统语义辅助注册方法依赖手工特征或学习依赖真实标注的问题，以适应实际环境应用。

Method: 设计了一个场景图网络，融合开放集语义特征、局部拓扑与空间感知以及形状特征，采用粗到细的匹配策略和鲁棒的姿态估计后端。

Result: 在两智能体SLAM基准测试中显著优于手工基线，注册召回率略高于视觉闭环网络，且每次查询仅需52 KB通信带宽。

Conclusion: 该方法通过多模态特征融合和高效的数据生成方式，实现了高效的语义场景图注册，适用于实际多智能体任务。

Abstract: This paper addresses the challenges of registering two rigid semantic scene
graphs, an essential capability when an autonomous agent needs to register its
map against a remote agent, or against a prior map. The hand-crafted
descriptors in classical semantic-aided registration, or the ground-truth
annotation reliance in learning-based scene graph registration, impede their
application in practical real-world environments. To address the challenges, we
design a scene graph network to encode multiple modalities of semantic nodes:
open-set semantic feature, local topology with spatial awareness, and shape
feature. These modalities are fused to create compact semantic node features.
The matching layers then search for correspondences in a coarse-to-fine manner.
In the back-end, we employ a robust pose estimator to decide transformation
according to the correspondences. We manage to maintain a sparse and
hierarchical scene representation. Our approach demands fewer GPU resources and
fewer communication bandwidth in multi-agent tasks. Moreover, we design a new
data generation approach using vision foundation models and a semantic mapping
module to reconstruct semantic scene graphs. It differs significantly from
previous works, which rely on ground-truth semantic annotations to generate
data. We validate our method in a two-agent SLAM benchmark. It significantly
outperforms the hand-crafted baseline in terms of registration success rate.
Compared to visual loop closure networks, our method achieves a slightly higher
registration recall while requiring only 52 KB of communication bandwidth for
each query frame. Code available at:
\href{http://github.com/HKUST-Aerial-Robotics/SG-Reg}{http://github.com/HKUST-Aerial-Robotics/SG-Reg}.

</details>

### [460] [Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction](https://arxiv.org/abs/2504.14588)
*Wenke Xia,Ruoxuan Feng,Dong Wang,Di Hu*

Main category: cs.RO

TLDR: 论文提出了Phoenix框架，通过运动指令连接高级语义反思与低级机器人动作修正，结合多任务运动条件扩散策略，实现机器人动作的精确修正。


<details>
  <summary>Details</summary>
Motivation: 构建一个通用的自我修正系统对机器人从失败中恢复至关重要，但将语义反思转化为细粒度机器人动作修正仍具挑战性。

Method: 提出Phoenix框架，包括双过程运动调整机制和多任务运动条件扩散策略，结合视觉观测实现高频动作修正。

Result: 实验证明该框架在多种操作任务中具有优越的泛化能力和鲁棒性。

Conclusion: Phoenix框架通过运动指令和扩散策略实现了机器人动作的精确修正，并通过终身学习方法持续提升模型能力。

Abstract: Building a generalizable self-correction system is crucial for robots to
recover from failures. Despite advancements in Multimodal Large Language Models
(MLLMs) that empower robots with semantic reflection ability for failure,
translating semantic reflection into how to correct fine-grained robotic
actions remains a significant challenge. To address this gap, we build the
Phoenix framework, which leverages motion instruction as a bridge to connect
high-level semantic reflection with low-level robotic action correction. In
this motion-based self-reflection framework, we start with a dual-process
motion adjustment mechanism with MLLMs to translate the semantic reflection
into coarse-grained motion instruction adjustment. To leverage this motion
instruction for guiding how to correct fine-grained robotic actions, a
multi-task motion-conditioned diffusion policy is proposed to integrate visual
observations for high-frequency robotic action correction. By combining these
two models, we could shift the demand for generalization capability from the
low-level manipulation policy to the MLLMs-driven motion adjustment model and
facilitate precise, fine-grained robotic action correction. Utilizing this
framework, we further develop a lifelong learning method to automatically
improve the model's capability from interactions with dynamic environments. The
experiments conducted in both the RoboMimic simulation and real-world scenarios
prove the superior generalization and robustness of our framework across a
variety of manipulation tasks. Our code is released at
\href{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}.

</details>

### [461] [Latent Representations for Visual Proprioception in Inexpensive Robots](https://arxiv.org/abs/2504.14634)
*Sahara Sheikholeslami,Ladislau Bölöni*

Main category: cs.RO

TLDR: 论文探讨了如何通过单次快速回归架构从单个外部摄像头图像实现视觉本体感知，适用于低成本机器人。


<details>
  <summary>Details</summary>
Motivation: 低成本机器人在非结构化环境中缺乏精确的本体感知能力，而工业机器人通常具备这一能力。

Method: 研究了多种潜在表示方法（如CNN、VAE、ViT和未校准标记点集），并采用适应有限数据的微调技术。

Result: 在低成本6自由度机器人上进行了实验，评估了可实现的精度。

Conclusion: 研究表明，视觉本体感知在低成本机器人中是可行的，且能达到一定精度。

Abstract: Robotic manipulation requires explicit or implicit knowledge of the robot's
joint positions. Precise proprioception is standard in high-quality industrial
robots but is often unavailable in inexpensive robots operating in unstructured
environments. In this paper, we ask: to what extent can a fast, single-pass
regression architecture perform visual proprioception from a single external
camera image, available even in the simplest manipulation settings? We explore
several latent representations, including CNNs, VAEs, ViTs, and bags of
uncalibrated fiducial markers, using fine-tuning techniques adapted to the
limited data available. We evaluate the achievable accuracy through experiments
on an inexpensive 6-DoF robot.

</details>

### [462] [Modality Selection and Skill Segmentation via Cross-Modality Attention](https://arxiv.org/abs/2504.14573)
*Jiawei Jiang,Kei Ota,Devesh K. Jha,Asako Kanezaki*

Main category: cs.RO

TLDR: 提出跨模态注意力机制（CMA）解决多模态信息融合的维度灾难问题，并用于技能分割和分层策略训练。


<details>
  <summary>Details</summary>
Motivation: 解决机器人模型中多模态信息融合的维度灾难问题。

Method: 提出跨模态注意力机制（CMA），选择性利用信息量最大的模态，并扩展应用于技能分割和分层策略训练。

Result: 成功应用于长时程、接触密集的操纵任务。

Conclusion: CMA机制有效解决了多模态信息融合问题，并提升了复杂任务的解决能力。

Abstract: Incorporating additional sensory modalities such as tactile and audio into
foundational robotic models poses significant challenges due to the curse of
dimensionality. This work addresses this issue through modality selection. We
propose a cross-modality attention (CMA) mechanism to identify and selectively
utilize the modalities that are most informative for action generation at each
timestep. Furthermore, we extend the application of CMA to segment primitive
skills from expert demonstrations and leverage this segmentation to train a
hierarchical policy capable of solving long-horizon, contact-rich manipulation
tasks.

</details>

### [463] [K2MUSE: A human lower limb multimodal dataset under diverse conditions for facilitating rehabilitation robotics](https://arxiv.org/abs/2504.14602)
*Jiwei Li,Bi Zhang,Xiaowei Tan,Wanxin Chen,Zhaoyuan Liu,Juanjuan Zhang,Weiguang Huo,Jian Huang,Lianqing Liu,Xingang Zhao*

Main category: cs.RO

TLDR: 论文介绍了K2MUSE数据集，填补了现有下肢康复机器人研究中多模态数据和大规模步态样本的不足，为康复机器人控制框架设计和下肢运动生物力学分析提供了新资源。


<details>
  <summary>Details</summary>
Motivation: 当前可用的下肢数据集无法提供有效的多模态数据和大规模步态样本，且忽略了实际应用中的采集干扰。

Method: 通过收集30名健康参与者在不同坡度、速度和采集条件下的多模态数据（包括运动学、动力学、AUS和sEMG），构建K2MUSE数据集。

Result: K2MUSE数据集包含丰富的多模态数据，为康复机器人控制和生物力学分析提供了新资源。

Conclusion: K2MUSE数据集填补了研究空白，为康复机器人设计和生物力学研究提供了重要支持。

Abstract: The natural interaction and control performance of lower limb rehabilitation
robots are closely linked to biomechanical information from various human
locomotion activities. Multidimensional human motion data significantly deepen
the understanding of the complex mechanisms governing neuromuscular
alterations, thereby facilitating the development and application of
rehabilitation robots in multifaceted real-world environments. However,
currently available lower limb datasets are inadequate for supplying the
essential multimodal data and large-scale gait samples necessary for effective
data-driven approaches, and they neglect the significant effects of acquisition
interference in real applications.To fill this gap, we present the K2MUSE
dataset, which includes a comprehensive collection of multimodal data,
comprising kinematic, kinetic, amplitude-mode ultrasound (AUS), and surface
electromyography (sEMG) measurements. The proposed dataset includes lower limb
multimodal data from 30 able-bodied participants walking under different
inclines (0$^\circ$, $\pm$5$^\circ$, and $\pm$10$^\circ$), various speeds (0.5
m/s, 1.0 m/s, and 1.5 m/s), and different nonideal acquisition conditions
(muscle fatigue, electrode shifts, and inter-day differences). The kinematic
and ground reaction force data were collected via a Vicon motion capture system
and an instrumented treadmill with embedded force plates, whereas the sEMG and
AUS data were synchronously recorded for thirteen muscles on the bilateral
lower limbs. This dataset offers a new resource for designing control
frameworks for rehabilitation robots and conducting biomechanical analyses of
lower limb locomotion. The dataset is available at https://k2muse.github.io/.

</details>

### [464] [A General Infrastructure and Workflow for Quadrotor Deep Reinforcement Learning and Reality Deployment](https://arxiv.org/abs/2504.15129)
*Kangyao Huang,Hao Wang,Yu Luo,Jingyu Chen,Jintao Chen,Xiangkui Zhang,Xiangyang Ji,Huaping Liu*

Main category: cs.RO

TLDR: 提出一个平台，实现端到端深度强化学习策略的无缝迁移，解决四旋翼无人机在非结构化户外环境中的学习挑战。


<details>
  <summary>Details</summary>
Motivation: 解决四旋翼无人机在现实环境中应用学习方法的挑战，如大量模拟数据需求、实时处理要求和模拟与现实的差距。

Method: 整合训练环境、飞行动力学控制、DRL算法、MAVROS中间件和硬件，提供从零到真实部署的完整工作流程。

Result: 平台支持多种环境任务，如悬停、动态避障、轨迹跟踪等，并通过实验验证了其高效性和鲁棒性。

Conclusion: 该平台为四旋翼无人机的学习策略提供了从模拟到现实的快速部署解决方案。

Abstract: Deploying robot learning methods to a quadrotor in unstructured outdoor
environments is an exciting task. Quadrotors operating in real-world
environments by learning-based methods encounter several challenges: a large
amount of simulator generated data required for training, strict demands for
real-time processing onboard, and the sim-to-real gap caused by dynamic and
noisy conditions. Current works have made a great breakthrough in applying
learning-based methods to end-to-end control of quadrotors, but rarely mention
the infrastructure system training from scratch and deploying to reality, which
makes it difficult to reproduce methods and applications. To bridge this gap,
we propose a platform that enables the seamless transfer of end-to-end deep
reinforcement learning (DRL) policies. We integrate the training environment,
flight dynamics control, DRL algorithms, the MAVROS middleware stack, and
hardware into a comprehensive workflow and architecture that enables
quadrotors' policies to be trained from scratch to real-world deployment in
several minutes. Our platform provides rich types of environments including
hovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and
planning in unknown environments, as a physical experiment benchmark. Through
extensive empirical validation, we demonstrate the efficiency of proposed
sim-to-real platform, and robust outdoor flight performance under real-world
perturbations. Details can be found from our website
https://emnavi.tech/AirGym/.

</details>

### [465] [An LLM-enabled Multi-Agent Autonomous Mechatronics Design Framework](https://arxiv.org/abs/2504.14681)
*Zeyu Wang,Frank P. -W. Lo,Qian Chen,Yongqi Zhang,Chen Lin,Xu Chen,Zhenhua Yu,Alexander J. Thompson,Eric M. Yeatman,Benny P. L. Lo*

Main category: cs.RO

TLDR: 本文提出了一种多自主智能体机电设计框架，通过跨学科整合和语言驱动的工作流程，实现功能原型的自主生成，减少对人工设计的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体框架多局限于数字或模拟环境，且知识领域狭窄，难以应对需要物理设计、跨学科整合和约束感知推理的复杂工程任务。

Method: 提出一个多自主智能体机电设计框架，整合机械设计、优化、电子和软件工程知识，通过语言驱动的工作流程和结构化人类反馈实现自主设计。

Result: 应用于水质监测与采样的实际挑战，成功开发出功能完整的自主船只，优化了推进系统、电子设备和控制系统。

Conclusion: 展示了基于LLM的多智能体系统在自动化现实工程工作流程和减少对领域专业知识依赖方面的潜力。

Abstract: Existing LLM-enabled multi-agent frameworks are predominantly limited to
digital or simulated environments and confined to narrowly focused knowledge
domain, constraining their applicability to complex engineering tasks that
require the design of physical embodiment, cross-disciplinary integration, and
constraint-aware reasoning. This work proposes a multi-agent autonomous
mechatronics design framework, integrating expertise across mechanical design,
optimization, electronics, and software engineering to autonomously generate
functional prototypes with minimal direct human design input. Operating
primarily through a language-driven workflow, the framework incorporates
structured human feedback to ensure robust performance under real-world
constraints. To validate its capabilities, the framework is applied to a
real-world challenge involving autonomous water-quality monitoring and
sampling, where traditional methods are labor-intensive and ecologically
disruptive. Leveraging the proposed system, a fully functional autonomous
vessel was developed with optimized propulsion, cost-effective electronics, and
advanced control. The design process was carried out by specialized agents,
including a high-level planning agent responsible for problem abstraction and
dedicated agents for structural, electronics, control, and software
development. This approach demonstrates the potential of LLM-based multi-agent
systems to automate real-world engineering workflows and reduce reliance on
extensive domain expertise.

</details>

### [466] [A Modularized Design Approach for GelSight Family of Vision-based Tactile Sensors](https://arxiv.org/abs/2504.14739)
*Arpit Agarwal,Mohammad Amin Mirzaee,Xiping Sun,Wenzhen Yuan*

Main category: cs.RO

TLDR: 本文提出了一种系统化和目标驱动的GelSight触觉传感器设计方法，通过物理精确的光学模拟优化设计，并开发了易于使用的工具箱OptiSense Studio。


<details>
  <summary>Details</summary>
Motivation: 为不同机器人手定制GelSight传感器需要繁琐的试错过程，本文旨在通过系统化设计方法简化这一过程。

Method: 通过模块化和参数化传感器光学组件，设计四个通用目标函数，并利用物理精确的光学模拟进行优化。

Result: 开发了OptiSense Studio工具箱，非专家用户可通过预定义模块快速优化传感器设计，并在四种GelSight传感器上验证了方法的有效性。

Conclusion: 该方法显著简化了GelSight传感器的设计过程，提高了设计效率和可转移性。

Abstract: GelSight family of vision-based tactile sensors has proven to be effective
for multiple robot perception and manipulation tasks. These sensors are based
on an internal optical system and an embedded camera to capture the deformation
of the soft sensor surface, inferring the high-resolution geometry of the
objects in contact. However, customizing the sensors for different robot hands
requires a tedious trial-and-error process to re-design the optical system. In
this paper, we formulate the GelSight sensor design process as a systematic and
objective-driven design problem and perform the design optimization with a
physically accurate optical simulation. The method is based on modularizing and
parameterizing the sensor's optical components and designing four generalizable
objective functions to evaluate the sensor. We implement the method with an
interactive and easy-to-use toolbox called OptiSense Studio. With the toolbox,
non-sensor experts can quickly optimize their sensor design in both forward and
inverse ways following our predefined modules and steps. We demonstrate our
system with four different GelSight sensors by quickly optimizing their initial
design in simulation and transferring it to the real sensors.

</details>

### [467] [Neural ATTF: A Scalable Solution to Lifelong Multi-Agent Path Planning](https://arxiv.org/abs/2504.15130)
*Kushal Shah,Jihyun Park,Seung-Kyum Choi*

Main category: cs.RO

TLDR: 本文提出了一种名为Neural ATTF的新算法，结合了Priority Guided Task Matching (PGTM)模块和Neural STA*路径规划方法，解决了多智能体拾取与配送（MAPD）问题中的可扩展性、适应性和效率挑战。


<details>
  <summary>Details</summary>
Motivation: 现有MAPD解决方案在动态环境中面临可扩展性、适应性和效率的挑战，限制了其在实际应用中的适用性。

Method: Neural ATTF结合了PGTM模块（动态任务分配）和Neural STA*（数据驱动的路径规划方法），通过学习的启发式方法快速探索搜索空间并避免碰撞。

Result: 实验表明，Neural ATTF在可扩展性、解决方案质量和计算效率上优于现有算法（如TPTS、CENTRAL等）。

Conclusion: Neural ATTF为复杂、高需求且不可预测环境中的多智能体系统提供了有效的解决方案。

Abstract: Multi-Agent Pickup and Delivery (MAPD) is a fundamental problem in robotics,
particularly in applications such as warehouse automation and logistics.
Existing solutions often face challenges in scalability, adaptability, and
efficiency, limiting their applicability in dynamic environments with real-time
planning requirements. This paper presents Neural ATTF (Adaptive Task Token
Framework), a new algorithm that combines a Priority Guided Task Matching
(PGTM) Module with Neural STA* (Space-Time A*), a data-driven path planning
method. Neural STA* enhances path planning by enabling rapid exploration of the
search space through guided learned heuristics and ensures collision avoidance
under dynamic constraints. PGTM prioritizes delayed agents and dynamically
assigns tasks by prioritizing agents nearest to these tasks, optimizing both
continuity and system throughput. Experimental evaluations against
state-of-the-art MAPD algorithms, including TPTS, CENTRAL, RMCA, LNS-PBS, and
LNS-wPBS, demonstrate the superior scalability, solution quality, and
computational efficiency of Neural ATTF. These results highlight the
framework's potential for addressing the critical demands of complex,
real-world multi-agent systems operating in high-demand, unpredictable
settings.

</details>

### [468] [A Genetic Fuzzy-Enabled Framework on Robotic Manipulation for In-Space Servicing](https://arxiv.org/abs/2504.15226)
*Nathan Steffen,Wilhelm Louw,Nicholas Ernest,Timothy Arnett,Kelly Cohen*

Main category: cs.RO

TLDR: 论文提出了一种结合遗传模糊树与LQR控制的方法，用于提高卫星维护机器人的效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着卫星数量的增加，自动化机器人系统在近月空间的维护变得至关重要，安全性是核心需求。

Method: 通过Thales的TrUE AI工具包，将遗传模糊树与LQR控制结合，设计了一种双自由度平面机械臂控制器。

Result: 实验表明，遗传模糊-LQR比最优LQR平均性能提升18.5%，且对不确定性具有极强的鲁棒性。

Conclusion: 该方法为卫星维护提供了一种高效且可信的控制方案。

Abstract: Automation of robotic systems for servicing in cislunar space is becoming
extremely important as the number of satellites in orbit increases. Safety is
critical in performing satellite maintenance, so the control techniques
utilized must be trusted in addition to being highly efficient. In this work,
Genetic Fuzzy Trees are combined with the widely used LQR control scheme via
Thales' TrUE AI Toolkit to create a trusted and efficient controller for a
two-degree-of-freedom planar robotic manipulator that would theoretically be
used to perform satellite maintenance. It was found that Genetic Fuzzy-LQR is
18.5% more performant than optimal LQR on average, and that it is incredibly
robust to uncertainty.

</details>

<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [469] [Resource Utilization Optimized Federated Learning](https://arxiv.org/abs/2504.13850)
*Zihan Zhang,Leon Wong,Blesson Varghese*

Main category: cs.DC

TLDR: FedOptima是一种优化的联邦学习系统，通过异步聚合和任务调度减少服务器与设备的空闲时间，显著提升训练效率和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统在实际应用中因任务依赖和设备异构性导致资源利用率低，FedOptima旨在同时解决这两种空闲时间问题。

Method: FedOptima通过异步聚合消除设备间的等待时间，利用辅助网络减少任务依赖，服务器采用任务调度和高效内存管理提升模型精度和可扩展性。

Result: 实验显示，FedOptima在准确率、训练速度、空闲时间减少和吞吐量方面均优于现有基线方法。

Conclusion: FedOptima显著提升了联邦学习的效率和实用性，适用于多种神经网络模型。

Abstract: Federated learning (FL) systems facilitate distributed machine learning
across a server and multiple devices. However, FL systems have low resource
utilization limiting their practical use in the real world. This inefficiency
primarily arises from two types of idle time: (i) task dependency between the
server and devices, and (ii) stragglers among heterogeneous devices. This paper
introduces FedOptima, a resource-optimized FL system designed to simultaneously
minimize both types of idle time; existing systems do not eliminate or reduce
both at the same time. FedOptima offloads the training of certain layers of a
neural network from a device to server using three innovations. First, devices
operate independently of each other using asynchronous aggregation to eliminate
straggler effects, and independently of the server by utilizing auxiliary
networks to minimize idle time caused by task dependency. Second, the server
performs centralized training using a task scheduler that ensures balanced
contributions from all devices, improving model accuracy. Third, an efficient
memory management mechanism on the server increases scalability of the number
of participating devices. Four state-of-the-art offloading-based and
asynchronous FL methods are chosen as baselines. Experimental results show that
compared to the best results of the baselines on convolutional neural networks
and transformers on multiple lab-based testbeds, FedOptima (i) achieves higher
or comparable accuracy, (ii) accelerates training by 1.9x to 21.8x, (iii)
reduces server and device idle time by up to 93.9% and 81.8%, respectively, and
(iv) increases throughput by 1.1x to 2.0x.

</details>

### [470] [PipeWeaver: Addressing Data Dynamicity in Large Multimodal Model Training with Dynamic Interleaved Pipeline](https://arxiv.org/abs/2504.14145)
*Zhenliang Xue,Hanpeng Hu,Xing Chen,Yimin Jiang,Yixin Song,Zeyu Mi,Yibo Zhu,Daxin Jiang,Yubin Xia,Haibo Chen*

Main category: cs.DC

TLDR: PipeWeaver是一个动态管道调度框架，旨在解决大型多模态模型（LMM）训练中的效率问题，通过动态交错管道和模态感知分区等技术，提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMM）在训练中面临管道阶段不平衡和多模态数据动态性两大效率问题，需要一种高效的调度框架来解决。

Method: PipeWeaver采用动态交错管道调度，结合自适应模态感知分区和分层调度空间搜索技术，并利用SEMU模拟器进行性能估计。

Result: 实验表明，PipeWeaver能将LMM训练效率提升高达97.3%，并展现出对数据动态性的优秀适应性。

Conclusion: PipeWeaver通过动态调度和优化技术，显著提升了LMM的训练效率，解决了多模态数据动态性带来的挑战。

Abstract: Large multimodal models (LMMs) have demonstrated excellent capabilities in
both understanding and generation tasks with various modalities. While these
models can accept flexible combinations of input data, their training
efficiency suffers from two major issues: pipeline stage imbalance caused by
heterogeneous model architectures, and training data dynamicity stemming from
the diversity of multimodal data.
  In this paper, we present PipeWeaver, a dynamic pipeline scheduling framework
designed for LMM training. The core of PipeWeaver is dynamic interleaved
pipeline, which searches for pipeline schedules dynamically tailored to current
training batches. PipeWeaver addresses issues of LMM training with two
techniques: adaptive modality-aware partitioning and efficient pipeline
schedule search within a hierarchical schedule space. Meanwhile, PipeWeaver
utilizes SEMU (Step Emulator), a training simulator for multimodal models, for
accurate performance estimations, accelerated by spatial-temporal subgraph
reuse to improve search efficiency. Experiments show that PipeWeaver can
enhance LMM training efficiency by up to 97.3% compared to state-of-the-art
systems, and demonstrate excellent adaptivity to LMM training's data
dynamicity.

</details>

### [471] [GENE-FL: Gene-Driven Parameter-Efficient Dynamic Federated Learning](https://arxiv.org/abs/2504.14628)
*Shunxin Guo,Jiaqi Lv,Qiufeng Wang,Xin Geng*

Main category: cs.DC

TLDR: 论文提出GENE-FL框架，通过Learngene范式解决动态客户端和异构数据分布下的联邦学习挑战，显著降低通信成本并快速初始化模型。


<details>
  <summary>Details</summary>
Motivation: 现实联邦学习系统中动态客户端和异构数据分布（DAFL）带来通信效率和模型初始化挑战，需要高效解决方案。

Method: 基于Learngene范式，GENE-FL框架通过参数二次约束、敏感性分析和服务器聚合，生成跨任务通用的learnGene。

Result: 实验显示GENE-FL比FEDAVG减少4倍通信成本，仅需9.04 MB即可初始化客户端模型。

Conclusion: GENE-FL框架有效解决了DAFL问题，提升了联邦学习的效率和适应性。

Abstract: Real-world \underline{F}ederated \underline{L}earning systems often encounter
\underline{D}ynamic clients with \underline{A}gnostic and highly heterogeneous
data distributions (DAFL), which pose challenges for efficient communication
and model initialization. To address these challenges, we draw inspiration from
the recently proposed Learngene paradigm, which compresses the large-scale
model into lightweight, cross-task meta-information fragments. Learngene
effectively encapsulates and communicates core knowledge, making it
particularly well-suited for DAFL, where dynamic client participation requires
communication efficiency and rapid adaptation to new data distributions. Based
on this insight, we propose a Gene-driven parameter-efficient dynamic Federated
Learning (GENE-FL) framework. First, local models perform quadratic constraints
based on parameters with high Fisher values in the global model, as these
parameters are considered to encapsulate generalizable knowledge. Second, we
apply the strategy of parameter sensitivity analysis in local model parameters
to condense the \textit{learnGene} for interaction. Finally, the server
aggregates these small-scale trained \textit{learnGene}s into a robust
\textit{learnGene} with cross-task generalization capability, facilitating the
rapid initialization of dynamic agnostic client models. Extensive experimental
results demonstrate that GENE-FL reduces \textbf{4 $\times$} communication
costs compared to FEDAVG and effectively initializes agnostic client models
with only about \textbf{9.04} MB.

</details>

### [472] [Is Intelligence the Right Direction in New OS Scheduling for Multiple Resources in Cloud Environments?](https://arxiv.org/abs/2504.15021)
*Xinglei Dou,Lei Liu,Limin Xiao*

Main category: cs.DC

TLDR: OSML+是一种基于机器学习的资源调度机制，用于协同云服务，智能调度缓存、内存带宽和计算核心资源。


<details>
  <summary>Details</summary>
Motivation: 提升系统/操作系统设计的智能化水平，解决复杂资源调度问题。

Method: 采用多模型协同学习方法，支持动态学习和迁移学习技术。

Result: OSML+支持更高负载，满足QoS目标且开销更低。

Conclusion: OSML+在资源调度方面表现优异，适用于多种云服务器。

Abstract: Making it intelligent is a promising way in System/OS design. This paper
proposes OSML+, a new ML-based resource scheduling mechanism for co-located
cloud services. OSML+ intelligently schedules the cache and main memory
bandwidth resources at the memory hierarchy and the computing core resources
simultaneously. OSML+ uses a multi-model collaborative learning approach during
its scheduling and thus can handle complicated cases, e.g., avoiding resource
cliffs, sharing resources among applications, enabling different scheduling
policies for applications with different priorities, etc. OSML+ can converge
faster using ML models than previous studies. Moreover, OSML+ can automatically
learn on the fly and handle dynamically changing workloads accordingly. Using
transfer learning technologies, we show our design can work well across various
cloud servers, including the latest off-the-shelf large-scale servers. Our
experimental results show that OSML+ supports higher loads and meets QoS
targets with lower overheads than previous studies.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [473] [HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation](https://arxiv.org/abs/2504.14147)
*Jiakai Tang,Jingsen Zhang,Zihang Tian,Xueyang Feng,Lei Wang,Xu Chen*

Main category: cs.IR

TLDR: 提出了一种基于人类反馈的优化框架，利用大型语言模型（LLMs）模拟人类反馈，通过动态交互优化机制提升推荐解释的质量，同时降低人工成本。


<details>
  <summary>Details</summary>
Motivation: 现有解释性推荐方法在稀疏交互数据下依赖传统监督学习，无法提供有效反馈信号。

Method: 提出动态交互优化框架，利用LLMs模拟人类反馈，引入定制化奖励评分和多目标优化方法。

Result: 在四个数据集上的实验验证了方法的优越性。

Conclusion: 该框架显著提升了推荐解释的质量和个性化需求满足能力。

Abstract: Recent advancements in explainable recommendation have greatly bolstered user
experience by elucidating the decision-making rationale. However, the existing
methods actually fail to provide effective feedback signals for potentially
better or worse generated explanations due to their reliance on traditional
supervised learning paradigms in sparse interaction data. To address these
issues, we propose a novel human-like feedback-driven optimization framework.
This framework employs a dynamic interactive optimization mechanism for
achieving human-centered explainable requirements without incurring high labor
costs. Specifically, we propose to utilize large language models (LLMs) as
human simulators to predict human-like feedback for guiding the learning
process. To enable the LLMs to deeply understand the task essence and meet
user's diverse personalized requirements, we introduce a human-induced
customized reward scoring method, which helps stimulate the language
understanding and logical reasoning capabilities of LLMs. Furthermore,
considering the potential conflicts between different perspectives of
explanation quality, we introduce a principled Pareto optimization that
transforms the multi-perspective quality enhancement task into a
multi-objective optimization problem for improving explanation performance. At
last, to achieve efficient model training, we design an off-policy optimization
pipeline. By incorporating a replay buffer and addressing the data distribution
biases, we can effectively improve data utilization and enhance model
generality. Extensive experiments on four datasets demonstrate the superiority
of our approach.

</details>

### [474] [CPR: Leveraging LLMs for Topic and Phrase Suggestion to Facilitate Comprehensive Product Reviews](https://arxiv.org/abs/2504.13993)
*Ekta Gujral,Apurva Sinha,Lishi Ji,Bijayani Sanghamitra Mishra*

Main category: cs.IR

TLDR: CPR是一种利用大型语言模型和主题建模的方法，通过三阶段流程帮助用户撰写更全面的产品评论，效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能系统性地鼓励用户撰写包含情感和产品特征分析的全面评论。

Method: CPR采用三阶段流程：1. 提供产品特定评分项；2. 基于评分生成针对性短语建议；3. 通过主题建模整合用户文本。

Result: CPR能有效识别相关产品术语并提供情感一致的短语建议，BLEU分数提升12.3%。

Conclusion: CPR在提升评论质量方面表现优异，未来可进一步扩展研究方向。

Abstract: Consumers often heavily rely on online product reviews, analyzing both
quantitative ratings and textual descriptions to assess product quality.
However, existing research hasn't adequately addressed how to systematically
encourage the creation of comprehensive reviews that capture both customers
sentiment and detailed product feature analysis. This paper presents CPR, a
novel methodology that leverages the power of Large Language Models (LLMs) and
Topic Modeling to guide users in crafting insightful and well-rounded reviews.
Our approach employs a three-stage process: first, we present users with
product-specific terms for rating; second, we generate targeted phrase
suggestions based on these ratings; and third, we integrate user-written text
through topic modeling, ensuring all key aspects are addressed. We evaluate CPR
using text-to-text LLMs, comparing its performance against real-world customer
reviews from Walmart. Our results demonstrate that CPR effectively identifies
relevant product terms, even for new products lacking prior reviews, and
provides sentiment-aligned phrase suggestions, saving users time and enhancing
reviews quality. Quantitative analysis reveals a 12.3% improvement in BLEU
score over baseline methods, further supported by manual evaluation of
generated phrases. We conclude by discussing potential extensions and future
research directions.

</details>

### [475] [The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models](https://arxiv.org/abs/2504.15068)
*Ronak Pradeep,Nandan Thakur,Shivani Upadhyay,Daniel Campos,Nick Craswell,Jimmy Lin*

Main category: cs.IR

TLDR: 提出了一种基于LLMs的自动评估框架AutoNuggetizer，用于评估检索增强生成（RAG）系统，验证了其与人工评估的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统的评估方法阻碍了进一步进展，需要一种自动化的评估框架。

Method: 采用TREC QA Track的nugget评估方法，通过LLMs自动生成和分配nuggets，并与人工评估对比。

Result: 自动评估与人工评估在运行级别上表现出一致性，尤其在独立自动化组件时更强。

Conclusion: 该框架在质量和效率间提供了权衡，但需进一步研究以优化每主题一致性。

Abstract: Large Language Models (LLMs) have significantly enhanced the capabilities of
information access systems, especially with retrieval-augmented generation
(RAG). Nevertheless, the evaluation of RAG systems remains a barrier to
continued progress, a challenge we tackle in this work by proposing an
automatic evaluation framework that is validated against human annotations. We
believe that the nugget evaluation methodology provides a solid foundation for
evaluating RAG systems. This approach, originally developed for the TREC
Question Answering (QA) Track in 2003, evaluates systems based on atomic facts
that should be present in good answers. Our efforts focus on "refactoring" this
methodology, where we describe the AutoNuggetizer framework that specifically
applies LLMs to both automatically create nuggets and automatically assign
nuggets to system answers. In the context of the TREC 2024 RAG Track, we
calibrate a fully automatic approach against strategies where nuggets are
created manually or semi-manually by human assessors and then assigned manually
to system answers. Based on results from a community-wide evaluation, we
observe strong agreement at the run level between scores derived from fully
automatic nugget evaluation and human-based variants. The agreement is stronger
when individual framework components such as nugget assignment are automated
independently. This suggests that our evaluation framework provides tradeoffs
between effort and quality that can be used to guide the development of future
RAG systems. However, further research is necessary to refine our approach,
particularly in establishing robust per-topic agreement to diagnose system
failures effectively.

</details>

### [476] [KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking](https://arxiv.org/abs/2504.15135)
*Juyeon Kim,Geon Lee,Taeuk Kim,Kijung Shin*

Main category: cs.IR

TLDR: KGMEL是一种新颖的多模态实体链接框架，利用知识图谱三元组提升准确性，通过生成、检索和重排三阶段实现，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MEL方法忽略了知识图谱三元组的结构信息，KGMEL旨在通过整合这些信息减少歧义并提高对齐准确性。

Method: KGMEL分三阶段：1) 生成高质量三元组；2) 通过对比学习整合文本、图像和三重组检索候选实体；3) 重排并匹配最佳实体。

Result: 在基准数据集上，KGMEL表现优于现有方法。

Conclusion: KGMEL通过利用知识图谱三元组显著提升了多模态实体链接的性能。

Abstract: Entity linking (EL) aligns textual mentions with their corresponding entities
in a knowledge base, facilitating various applications such as semantic search
and question answering. Recent advances in multimodal entity linking (MEL) have
shown that combining text and images can reduce ambiguity and improve alignment
accuracy. However, most existing MEL methods overlook the rich structural
information available in the form of knowledge-graph (KG) triples. In this
paper, we propose KGMEL, a novel framework that leverages KG triples to enhance
MEL. Specifically, it operates in three stages: (1) Generation: Produces
high-quality triples for each mention by employing vision-language models based
on its text and images. (2) Retrieval: Learns joint mention-entity
representations, via contrastive learning, that integrate text, images, and
(generated or KG) triples to retrieve candidate entities for each mention. (3)
Reranking: Refines the KG triples of the candidate entities and employs large
language models to identify the best-matching entity for the mention. Extensive
experiments on benchmark datasets demonstrate that KGMEL outperforms existing
methods. Our code and datasets are available at:
https://github.com/juyeonnn/KGMEL.

</details>

### [477] [Personalized News Recommendation with Multi-granularity Candidate-aware User Modeling](https://arxiv.org/abs/2504.14130)
*Qiang Li,Xinze Lin,Shenghao Lv,Faliang Huang,Xiangju Li*

Main category: cs.IR

TLDR: 提出了一种多粒度候选感知的用户建模框架，用于改进个性化新闻推荐。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常基于单一用户兴趣画像，未能充分捕捉用户兴趣的多样性，且忽略了候选新闻与用户兴趣的多粒度关联。

Method: 框架包含候选新闻编码和用户建模两部分，通过文本和知识增强的实体信息提取器捕捉候选新闻特征，并利用多粒度候选感知机制全面表示用户兴趣。

Result: 在真实数据集上的实验表明，该模型显著优于基线模型。

Conclusion: 多粒度候选感知框架能更全面地捕捉用户兴趣，提升推荐效果。

Abstract: Matching candidate news with user interests is crucial for personalized news
recommendations. Most existing methods can represent a user's reading interests
through a single profile based on clicked news, which may not fully capture the
diversity of user interests. Although some approaches incorporate candidate
news or topic information, they remain insufficient because they neglect the
multi-granularity relatedness between candidate news and user interests. To
address this, this study proposed a multi-granularity candidate-aware user
modeling framework that integrated user interest features across various levels
of granularity. It consisted of two main components: candidate news encoding
and user modeling. A news textual information extractor and a
knowledge-enhanced entity information extractor can capture candidate news
features, and word-level, entity-level, and news-level candidate-aware
mechanisms can provide a comprehensive representation of user interests.
Extensive experiments on a real-world dataset demonstrated that the proposed
model could significantly outperform baseline models.

</details>

### [478] [FinSage: A Multi-aspect RAG System for Financial Filings Question Answering](https://arxiv.org/abs/2504.14493)
*Xinyu Wang,Jijun Chi,Zhenghan Tai,Tung Sum Thomas Kwok,Muzhi Li,Zhuhong Li,Hailin He,Yuchen Hua,Peng Lu,Suyuchen Wang,Yihong Wu,Jerry Huang,Ling Zhou*

Main category: cs.IR

TLDR: FinSage是一个针对金融文档的多模态RAG框架，通过多模态预处理、多路径检索和领域专用重排序模块，显著提升了合规性分析的准确性和召回率。


<details>
  <summary>Details</summary>
Motivation: 金融领域需要处理多模态数据和动态监管标准，现有解决方案在信息提取准确性上存在不足。

Method: FinSage采用多模态预处理、多路径检索（结合HyDE和元数据感知搜索）和DPO优化的重排序模块。

Result: 在FinanceBench数据集上，FinSage的准确率比最佳基线方法高24.06%，召回率达92.51%。

Conclusion: FinSage在金融合规分析中表现出色，并已成功应用于实际场景。

Abstract: Leveraging large language models in real-world settings often entails a need
to utilize domain-specific data and tools in order to follow the complex
regulations that need to be followed for acceptable use. Within financial
sectors, modern enterprises increasingly rely on Retrieval-Augmented Generation
(RAG) systems to address complex compliance requirements in financial document
workflows. However, existing solutions struggle to account for the inherent
heterogeneity of data (e.g., text, tables, diagrams) and evolving nature of
regulatory standards used in financial filings, leading to compromised accuracy
in critical information extraction. We propose the FinSage framework as a
solution, utilizing a multi-aspect RAG framework tailored for regulatory
compliance analysis in multi-modal financial documents. FinSage introduces
three innovative components: (1) a multi-modal pre-processing pipeline that
unifies diverse data formats and generates chunk-level metadata summaries, (2)
a multi-path sparse-dense retrieval system augmented with query expansion
(HyDE) and metadata-aware semantic search, and (3) a domain-specialized
re-ranking module fine-tuned via Direct Preference Optimization (DPO) to
prioritize compliance-critical content. Extensive experiments demonstrate that
FinSage achieves an impressive recall of 92.51% on 75 expert-curated questions
derived from surpasses the best baseline method on the FinanceBench question
answering datasets by 24.06% in accuracy. Moreover, FinSage has been
successfully deployed as financial question-answering agent in online meetings,
where it has already served more than 1,200 people.

</details>

### [479] [Exploring $\ell_0$ Sparsification for Inference-free Sparse Retrievers](https://arxiv.org/abs/2504.14839)
*Xinjie Shen,Zhichao Geng,Yang Yang*

Main category: cs.IR

TLDR: 本文提出了一种基于ℓ0稀疏化的方法，用于优化无推理稀疏检索模型，在BEIR基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏检索模型在无推理场景下使用FLOPS正则化效果不佳，亟需更优的稀疏化方法。

Method: 采用ℓ0稀疏化方法，针对无推理检索模型进行优化。

Result: 在BEIR基准测试中达到无推理稀疏检索模型的领先水平，并与Siamese模型性能相当。

Conclusion: 该方法在检索效果与计算效率之间取得平衡，具有实际应用价值。

Abstract: With increasing demands for efficiency, information retrieval has developed a
branch of sparse retrieval, further advancing towards inference-free retrieval
where the documents are encoded during indexing time and there is no
model-inference for queries. Existing sparse retrieval models rely on FLOPS
regularization for sparsification, while this mechanism was originally designed
for Siamese encoders, it is considered to be suboptimal in inference-free
scenarios which is asymmetric. Previous attempts to adapt FLOPS for
inference-free scenarios have been limited to rule-based methods, leaving the
potential of sparsification approaches for inference-free retrieval models
largely unexplored. In this paper, we explore $\ell_0$ inspired sparsification
manner for inference-free retrievers. Through comprehensive out-of-domain
evaluation on the BEIR benchmark, our method achieves state-of-the-art
performance among inference-free sparse retrieval models and is comparable to
leading Siamese sparse retrieval models. Furthermore, we provide insights into
the trade-off between retrieval effectiveness and computational efficiency,
demonstrating practical value for real-world applications.

</details>

<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [480] [Integrating LLM-Generated Views into Mean-Variance Optimization Using the Black-Litterman Model](https://arxiv.org/abs/2504.14345)
*Youngbin Lee,Yejin Kim,Suin Kim,Yongjae Lee*

Main category: q-fin.PM

TLDR: 研究探索了将大型语言模型（LLMs）生成的观点整合到Black-Litterman框架中的投资组合优化方法，通过历史价格和公司元数据预测股票收益，并比较了不同LLM的表现。


<details>
  <summary>Details</summary>
Motivation: 传统均值-方差模型对输入敏感，Black-Litterman模型通过整合投资者观点缓解了这一问题，但定义这些观点仍具挑战性。

Method: 利用LLMs从历史价格和公司元数据中预测股票收益，并通过预测方差量化不确定性，进行双周再平衡的回测。

Result: 不同LLM表现出不同的预测乐观程度和信心稳定性，影响了投资组合的表现。

Conclusion: LLM生成的观点可以用于优化投资组合，但不同模型的表现差异显著。

Abstract: Portfolio optimization faces challenges due to the sensitivity in traditional
mean-variance models. The Black-Litterman model mitigates this by integrating
investor views, but defining these views remains difficult. This study explores
the integration of large language models (LLMs) generated views into portfolio
optimization using the Black-Litterman framework. Our method leverages LLMs to
estimate expected stock returns from historical prices and company metadata,
incorporating uncertainty through the variance in predictions. We conduct a
backtest of the LLM-optimized portfolios from June 2024 to February 2025,
rebalancing biweekly using the previous two weeks of price data. As baselines,
we compare against the S&P 500, an equal-weighted portfolio, and a traditional
mean-variance optimized portfolio constructed using the same set of stocks.
Empirical results suggest that different LLMs exhibit varying levels of
predictive optimism and confidence stability, which impact portfolio
performance. The source code and data are available at
https://github.com/youngandbin/LLM-MVO-BLM.

</details>

<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [481] [Chinese-LiPS: A Chinese audio-visual speech recognition dataset with Lip-reading and Presentation Slides](https://arxiv.org/abs/2504.15066)
*Jinghua Zhao,Yuhang Jia,Shiyao Wang,Jiaming Zhou,Hui Wang,Yong Qin*

Main category: cs.MM

TLDR: 论文提出了一种结合唇读和演讲幻灯片视觉信息的AVSR方法，并发布了包含100小时数据的Chinese-LiPS数据集，显著提升了ASR性能。


<details>
  <summary>Details</summary>
Motivation: 现有AVSR方法仅依赖单一视觉信息（如唇读或上下文视频），忽略了结合多种视觉线索的潜力。

Method: 开发了LiPS-AVSR流程，利用唇读和幻灯片信息作为视觉模态。

Result: 唇读和幻灯片信息分别提升ASR性能约8%和25%，结合后提升约35%。

Conclusion: 结合多种视觉线索能显著提升AVSR性能，Chinese-LiPS数据集为未来研究提供了资源。

Abstract: Incorporating visual modalities to assist Automatic Speech Recognition (ASR)
tasks has led to significant improvements. However, existing Audio-Visual
Speech Recognition (AVSR) datasets and methods typically rely solely on
lip-reading information or speaking contextual video, neglecting the potential
of combining these different valuable visual cues within the speaking context.
In this paper, we release a multimodal Chinese AVSR dataset, Chinese-LiPS,
comprising 100 hours of speech, video, and corresponding manual transcription,
with the visual modality encompassing both lip-reading information and the
presentation slides used by the speaker. Based on Chinese-LiPS, we develop a
simple yet effective pipeline, LiPS-AVSR, which leverages both lip-reading and
presentation slide information as visual modalities for AVSR tasks. Experiments
show that lip-reading and presentation slide information improve ASR
performance by approximately 8\% and 25\%, respectively, with a combined
performance improvement of about 35\%. The dataset is available at
https://kiri0824.github.io/Chinese-LiPS/

</details>

<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [482] [On the redundancy of short and heterogeneous sequences of belief revisions](https://arxiv.org/abs/2504.13986)
*Paolo Liberatore*

Main category: cs.CC

TLDR: 遗忘特定信念修订事件可能不会完全删除信息，因为其他修订可能提供或推导出相同信息。研究证明，对于任意两个词典序修订或任意长度的词典序Horn修订，该问题为coNP难。对于两个Horn修订的情况，提出了多项式算法。异质修订序列被证明属于Delta2类，且其coNP难性进一步被NP难性证明增强。


<details>
  <summary>Details</summary>
Motivation: 探讨在信念修订过程中，遗忘特定修订事件是否会导致信息完全丢失，以及不同修订类型的计算复杂性。

Method: 通过理论分析和证明，研究了词典序修订和Horn修订的计算复杂性，并提出了针对特定情况的多项式算法。

Result: 证明了某些修订类型的coNP难性和NP难性，并针对两个Horn修订提出了多项式时间算法。

Conclusion: 遗忘特定修订事件的信息保留问题具有较高的计算复杂性，但在特定条件下可高效解决。

Abstract: Forgetting a specific belief revision episode may not erase information
because the other revisions may provide the same information or allow to deduce
it. Whether it does was proved coNP-hard for sequence of two arbitrary
lexicographic revision or arbitrarily long lexicographic Horn revision. A
polynomial algorithm is presented for the case of two Horn revision.
Heterogeneous sequences of revisions were proved to belong in Delta2. Their
previously proved coNP-hardness is enhanced by a proof of NP-hardness.

</details>

<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [483] [Density Measures for Language Generation](https://arxiv.org/abs/2504.14370)
*Jon Kleinberg,Fan Wei*

Main category: math.CO

TLDR: 论文提出了一种抽象的语言生成框架，研究了算法在生成新字符串时的有效性与广度之间的权衡，并提出了一种新算法以确保输出具有严格正密度。


<details>
  <summary>Details</summary>
Motivation: 研究语言生成中有效性与广度的权衡问题，量化这一权衡并解决现有算法在广度上的不足。

Method: 提出语言生成的抽象框架（语言极限生成），使用密度度量量化广度，设计新算法以确保输出具有严格正密度。

Result: 新算法在语言极限生成中实现了严格正密度的输出，并揭示了实现最强广度可能需要在高密度和低密度表示之间振荡。

Conclusion: 通过引入新的语言族拓扑和分析方法，论文为语言生成的有效性与广度权衡提供了量化工具和理论支持。

Abstract: The recent successes of large language models (LLMs) have led to a surge of
theoretical research into language generation. A recent line of work proposes
an abstract view, called language generation in the limit, where generation is
seen as a game between an adversary and an algorithm: the adversary generates
strings from an unknown language $K$, chosen from a countable collection of
candidate languages, and after seeing a finite set of these strings, the
algorithm must generate new strings from $K$ that it has not seen before. This
formalism highlights a key tension: the trade-off between validity (the
algorithm should only produce strings from the language) and breadth (it should
be able to produce many strings from the language). This trade-off is central
in applied language generation as well, where it appears as a balance between
hallucination (generating invalid utterances) and mode collapse (generating
only a restricted set of outputs). Despite its importance, this trade-off has
been challenging to study quantitatively. We develop ways to quantify this
trade-off by formalizing breadth using measures of density. Existing algorithms
for language generation in the limit produce output sets that can have zero
density in the true language, and this important failure of breadth might seem
unavoidable. We show, however, that such a failure is not necessary: we provide
an algorithm for language generation in the limit whose outputs have strictly
positive density in $K$. We also study the internal representations built by
these algorithms, specifically the sequence of hypothesized candidate languages
they consider, and show that achieving the strongest form of breadth may
require oscillating indefinitely between high- and low-density representations.
Our analysis introduces a novel topology on language families, with notions of
convergence and limit points playing a key role.

</details>

<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [484] [Association between nutritional factors, inflammatory biomarkers and cancer types: an analysis of NHANES data using machine learning](https://arxiv.org/abs/2504.13978)
*Yuqing Liu,Meng Zhao,Guanlan Hu,Yuchen Zhang*

Main category: q-bio.QM

TLDR: 研究探讨了营养状况和炎症生物标志物与癌症状态及类型的关系，利用机器学习分析NHANES数据，发现营养和炎症指标对癌症预测有重要意义。


<details>
  <summary>Details</summary>
Motivation: 饮食和炎症是影响癌症风险的关键因素，但结合营养状态和炎症生物标志物通过机器学习分析癌症状态和类型的研究较少。

Method: 分析了26,409名NHANES参与者的24种宏量和微量营养素、CRP和ALI，使用多变量逻辑回归评估癌症患病率关联，并应用三种机器学习模型（逻辑回归、随机森林、XGBoost）评估预测价值。

Result: 营养因素（如蛋白质和维生素）和炎症指标（如CRP）是癌症状态的关键预测因子，随机森林模型表现最佳，准确率为0.72。

Conclusion: 高质量营养摄入和低炎症水平可能对癌症有保护作用，结合营养和炎症标志物与机器学习可为癌症预防策略提供信息。

Abstract: Background. Diet and inflammation are critical factors influencing cancer
risk. However, the combined impact of nutritional status and inflammatory
biomarkers on cancer status and type, using machine learning (ML), remains
underexplored.
  Objectives. This study investigates the association between nutritional
factors, inflammatory biomarkers, and cancer status, and whether these
relationships differ across cancer types using National Health and Nutrition
Examination Survey (NHANES) data.
  Methods. We analyzed 24 macro- and micronutrients, C-reactive protein (CRP),
and the advanced lung cancer inflammation index (ALI) in 26,409 NHANES
participants (2,120 with cancer). Multivariable logistic regression assessed
associations with cancer prevalence. We also examined whether these features
differed across the five most common cancer types. To evaluate predictive
value, we applied three ML models - Logistic Regression, Random Forest, and
XGBoost - on the full feature set.
  Results. The cohort's mean age was 49.1 years; 34.7% were obese.
Comorbidities such as anemia and liver conditions, along with nutritional
factors like protein and several vitamins, were key predictors of cancer
status. Among the models, Random Forest performed best, achieving an accuracy
of 0.72.
  Conclusions. Higher-quality nutritional intake and lower levels of
inflammation may offer protective effects against cancer. These findings
highlight the potential of combining nutritional and inflammatory markers with
ML to inform cancer prevention strategies.

</details>

<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [485] [OPO: Making Decision-Focused Data Acquisition Decisions](https://arxiv.org/abs/2504.15062)
*Egon Peršak,Miguel F. Anjos*

Main category: math.OC

TLDR: 提出了一种用于在上下文随机优化问题中为变量做出数据获取决策的模型，重点关注数据获取成本高且受限的情况。


<details>
  <summary>Details</summary>
Motivation: 传统数据获取决策通常独立且固定，而实际中获取上下文变量成本高且受限，需更关注其对下游决策质量的影响。

Method: 利用可微分优化扩展预测与优化的整合，学习替代线性目标函数以解决数据获取问题。

Result: 在无人机侦察最短路径问题中，该方法优于随机搜索策略。

Conclusion: 通过可微分优化整合数据获取与决策，能有效提升决策质量。

Abstract: We propose a model for making data acquisition decisions for variables in
contextual stochastic optimisation problems. Data acquisition decisions are
typically treated as separate and fixed. We explore problem settings in which
the acquisition of contextual variables is costly and consequently constrained.
The data acquisition problem is often solved heuristically for proxy objectives
such as coverage. The more intuitive objective is the downstream decision
quality as a result of data acquisition decisions. The whole pipeline can be
characterised as an optimise-then-predict-then-optimise (OPO) problem.
Analogously, much recent research has focused on how to integrate prediction
and optimisation (PO) in the form of decision-focused learning. We propose
leveraging differentiable optimisation to extend the integration to data
acquisition. We solve the data acquisition problem with well-defined
constraints by learning a surrogate linear objective function. We demonstrate
an application of this model on a shortest path problem for which we first have
to set a drone reconnaissance strategy to capture image segments serving as
inputs to a model that predicts travel costs. We ablate the problem with a
number of training modalities and demonstrate that the differentiable
optimisation approach outperforms random search strategies.

</details>

<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [486] [VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform](https://arxiv.org/abs/2504.14904)
*Xingyu Lu,Tianke Zhang,Chang Meng,Xiaobei Wang,Jinpeng Wang,YiFan Zhang,Shisong Tang,Changyi Liu,Haojie Ding,Kaiyu Jiang,Kaiyu Tang,Bin Wen,Hai-Tao Zheng,Fan Yang,Tingting Gao,Di Zhang,Kun Gai*

Main category: cs.SI

TLDR: 论文提出了KuaiMod框架，通过结合视觉语言模型和链式推理，解决了短视频平台内容审核的三大挑战，并在实验和实际应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 短视频平台内容审核存在人工偏见、自动化方法准确性不足及工业规范更新滞后等问题，亟需高效解决方案。

Method: 提出KuaiMod框架，包含训练数据构建、离线适应及在线部署与优化三部分，利用视觉语言模型和链式推理实现动态审核。

Result: KuaiMod在基准测试中表现最佳，实际部署后用户举报率降低20%，并提升了平台活跃度和使用时长。

Conclusion: KuaiMod为短视频平台内容审核提供了高效、动态的解决方案，具有实际应用价值。

Abstract: Exponentially growing short video platforms (SVPs) face significant
challenges in moderating content detrimental to users' mental health,
particularly for minors. The dissemination of such content on SVPs can lead to
catastrophic societal consequences. Although substantial efforts have been
dedicated to moderating such content, existing methods suffer from critical
limitations: (1) Manual review is prone to human bias and incurs high
operational costs. (2) Automated methods, though efficient, lack nuanced
content understanding, resulting in lower accuracy. (3) Industrial moderation
regulations struggle to adapt to rapidly evolving trends due to long update
cycles. In this paper, we annotate the first SVP content moderation benchmark
with authentic user/reviewer feedback to fill the absence of benchmark in this
field. Then we evaluate various methods on the benchmark to verify the
existence of the aforementioned limitations. We further propose our common-law
content moderation framework named KuaiMod to address these challenges. KuaiMod
consists of three components: training data construction, offline adaptation,
and online deployment & refinement. Leveraging large vision language model
(VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video
toxicity based on sparse user feedback and fosters dynamic moderation policy
with rapid update speed and high accuracy. Offline experiments and large-scale
online A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the
best moderation performance on our benchmark. The deployment of KuaiMod reduces
the user reporting rate by 20% and its application in video recommendation
increases both Daily Active User (DAU) and APP Usage Time (AUT) on several
Kuaishou scenarios. We have open-sourced our benchmark at
https://kuaimod.github.io.

</details>

### [487] [Rhythm of Opinion: A Hawkes-Graph Framework for Dynamic Propagation Analysis](https://arxiv.org/abs/2504.15072)
*Yulong Li,Zhixiang Lu,Feilong Tang,Simin Lai,Ming Hu,Yuxuan Zhang,Haochen Xue,Zhaodong Wu,Imran Razzak,Qingxia Li,Jionglong Su*

Main category: cs.SI

TLDR: 提出了一种结合多维霍克斯过程与图神经网络的方法，用于建模社交网络中意见传播的动态性，并引入新数据集VISTA支持研究。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以有效捕捉社交媒体中复杂的公共意见动态，需要新方法建模多层次互动与传播网络。

Method: 整合多维霍克斯过程与图神经网络，建模社交网络中节点间的意见传播动态，同时考虑评论的层次关系。

Result: 提出的方法能有效捕捉多层次互动与跨主题影响，新数据集VISTA为研究提供高质量数据支持。

Conclusion: 该方法为未来研究提供了强解释性的基线，结合数据集可深入分析情感传播与时间演化。

Abstract: The rapid development of social media has significantly reshaped the dynamics
of public opinion, resulting in complex interactions that traditional models
fail to effectively capture. To address this challenge, we propose an
innovative approach that integrates multi-dimensional Hawkes processes with
Graph Neural Network, modeling opinion propagation dynamics among nodes in a
social network while considering the intricate hierarchical relationships
between comments. The extended multi-dimensional Hawkes process captures the
hierarchical structure, multi-dimensional interactions, and mutual influences
across different topics, forming a complex propagation network. Moreover,
recognizing the lack of high-quality datasets capable of comprehensively
capturing the evolution of public opinion dynamics, we introduce a new dataset,
VISTA. It includes 159 trending topics, corresponding to 47,207 posts, 327,015
second-level comments, and 29,578 third-level comments, covering diverse
domains such as politics, entertainment, sports, health, and medicine. The
dataset is annotated with detailed sentiment labels across 11 categories and
clearly defined hierarchical relationships. When combined with our method, it
offers strong interpretability by linking sentiment propagation to the comment
hierarchy and temporal evolution. Our approach provides a robust baseline for
future research.

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [488] [Transformation of audio embeddings into interpretable, concept-based representations](https://arxiv.org/abs/2504.14076)
*Alice Zhang,Edison Thomaz,Lie Lu*

Main category: cs.SD

TLDR: 该论文提出了一种将CLAP音频嵌入转换为基于概念的稀疏表示的方法，以提高语义可解释性，并在下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 音频神经网络的内部表示难以解释，需要一种方法使其更具语义可解释性。

Method: 利用CLAP模型将音频和文本嵌入共享空间，并通过后处理方法生成基于概念的稀疏表示。

Result: 基于概念的表示在下游任务中表现优于或等同于原始音频嵌入，且具有可解释性。

Conclusion: 该方法不仅提高了音频嵌入的可解释性，还通过微调进一步提升了性能，并发布了三个音频专用词汇表。

Abstract: Advancements in audio neural networks have established state-of-the-art
results on downstream audio tasks. However, the black-box structure of these
models makes it difficult to interpret the information encoded in their
internal audio representations. In this work, we explore the semantic
interpretability of audio embeddings extracted from these neural networks by
leveraging CLAP, a contrastive learning model that brings audio and text into a
shared embedding space. We implement a post-hoc method to transform CLAP
embeddings into concept-based, sparse representations with semantic
interpretability. Qualitative and quantitative evaluations show that the
concept-based representations outperform or match the performance of original
audio embeddings on downstream tasks while providing interpretability.
Additionally, we demonstrate that fine-tuning the concept-based representations
can further improve their performance on downstream tasks. Lastly, we publish
three audio-specific vocabularies for concept-based interpretability of audio
embeddings.

</details>

### [489] [DRAGON: Distributional Rewards Optimize Diffusion Generative Models](https://arxiv.org/abs/2504.15217)
*Yatong Bai,Jonah Casebeer,Somayeh Sojoudi,Nicholas J. Bryan*

Main category: cs.SD

TLDR: DRAGON是一个灵活的框架，用于优化生成模型的奖励函数，支持多种奖励类型，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统方法如RLHF或DPO在奖励函数设计上较为局限，DRAGON旨在提供更灵活的优化方式。

Method: DRAGON通过选择编码器和参考样本构建奖励函数，利用对比学习优化生成结果。

Result: 在20种奖励函数上平均胜率达81.45%，且无需人类偏好标注即可提升生成质量。

Conclusion: DRAGON为奖励函数设计和优化提供了新思路，显著提升了生成模型的表现。

Abstract: We present Distributional RewArds for Generative OptimizatioN (DRAGON), a
versatile framework for fine-tuning media generation models towards a desired
outcome. Compared with traditional reinforcement learning with human feedback
(RLHF) or pairwise preference approaches such as direct preference optimization
(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate
either individual examples or distributions of them, making it compatible with
a broad spectrum of instance-wise, instance-to-distribution, and
distribution-to-distribution rewards. Leveraging this versatility, we construct
novel reward functions by selecting an encoder and a set of reference examples
to create an exemplar distribution. When cross-modality encoders such as CLAP
are used, the reference examples may be of a different modality (e.g., text
versus audio). Then, DRAGON gathers online and on-policy generations, scores
them to construct a positive demonstration set and a negative set, and
leverages the contrast between the two sets to maximize the reward. For
evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20
different reward functions, including a custom music aesthetics model, CLAP
score, Vendi diversity, and Frechet audio distance (FAD). We further compare
instance-wise (per-song) and full-dataset FAD settings while ablating multiple
FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an
81.45% average win rate. Moreover, reward functions based on exemplar sets
indeed enhance generations and are comparable to model-based rewards. With an
appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality
win rate without training on human preference annotations. As such, DRAGON
exhibits a new approach to designing and optimizing reward functions for
improving human-perceived quality. Sound examples at
https://ml-dragon.github.io/web.

</details>

<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [490] [GenShin:geometry-enhanced structural graph embodies binding pose can better predicting compound-protein interaction affinity](https://arxiv.org/abs/2504.13853)
*Pingfei Zhu,Chenyang Zhao,Haishi Zhao,Bo Yang*

Main category: q-bio.BM

TLDR: GenShin模型通过几何增强结构图模块，无需依赖结合构象输入，即可高精度预测化合物-蛋白质亲和力，优于依赖对接构象的模型。


<details>
  <summary>Details</summary>
Motivation: 传统预测化合物-蛋白质亲和力的方法依赖昂贵的实验或耗时的模拟获取结合构象，限制了实际应用。

Method: GenShin模型构建几何增强结构图模块，分别提取蛋白质和化合物的特征，无需结合构象输入。

Result: GenShin模型在预测亲和力上表现优异，甚至超越依赖结合构象的模型，且对不完整结合构象更鲁棒。

Conclusion: GenShin模型为实际药物发现提供了更高效的解决方案，有望推动AI模型与实际应用的结合。

Abstract: AI-powered drug discovery typically relies on the successful prediction of
compound-protein interactions, which are pivotal for the evaluation of designed
compound molecules in structure-based drug design and represent a core
challenge in the field.
  However, accurately predicting compound-protein affinity via regression
models usually requires adequate-binding pose, which are derived from costly
and complex experimental methods or time-consuming simulations with docking
software. In response, we have introduced the GenShin model, which constructs a
geometry-enhanced structural graph module that separately extracts additional
features from proteins and compounds. Consequently, it attains an accuracy on
par with mainstream models in predicting compound-protein affinities, while
eliminating the need for adequate-binding pose as input. Our experimental
findings demonstrate that the GenShin model vastly outperforms other models
that rely on non-input docking conformations, achieving, or in some cases even
exceeding, the performance of those requiring adequate-binding pose. Further
experiments indicate that our GenShin model is more robust to
inadequate-binding pose, affirming its higher suitability for real-world drug
discovery scenarios. We hope our work will inspire more endeavors to bridge the
gap between AI models and practical drug discovery challenges.

</details>

<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [491] [Quantum-Enhanced Reinforcement Learning for Power Grid Security Assessment](https://arxiv.org/abs/2504.14412)
*Benjamin M. Peter,Mert Korkali*

Main category: eess.SY

TLDR: 论文提出了一种结合量子计算和强化学习的混合代理，用于提升电网安全评估的计算效率和代理能力，并通过实验验证了其优于传统方法的表现。


<details>
  <summary>Details</summary>
Motivation: 电网安全维护任务日益复杂，传统强化学习方法在应对大规模决策空间和非线性行为时难以扩展，量子计算的优势为解决这一问题提供了新思路。

Method: 提出了一种基于IBM Qiskit Runtime的混合代理，利用参数化量子电路（PQCs）生成量子输出，并结合强化学习框架进行训练和仿真。

Result: 实验表明，该混合代理在N-k故障分析中表现优于未结合量子增强的基准模型，同时对比了量子后端集成对训练过程的影响。

Conclusion: 量子计算与强化学习的结合为电网安全评估提供了可扩展的高效解决方案，展示了量子技术在复杂系统优化中的潜力。

Abstract: The increasingly challenging task of maintaining power grid security requires
innovative solutions. Novel approaches using reinforcement learning (RL) agents
have been proposed to help grid operators navigate the massive decision space
and nonlinear behavior of these complex networks. However, applying RL to power
grid security assessment, specifically for combinatorially troublesome
contingency analysis problems, has proven difficult to scale. The integration
of quantum computing into these RL frameworks helps scale by improving
computational efficiency and boosting agent proficiency by leveraging quantum
advantages in action exploration and model-based interdependence. To
demonstrate a proof-of-concept use of quantum computing for RL agent training
and simulation, we propose a hybrid agent that runs on quantum hardware using
IBM's Qiskit Runtime. We also provide detailed insight into the construction of
parameterized quantum circuits (PQCs) for generating relevant quantum output.
This agent's proficiency at maintaining grid stability is demonstrated relative
to a benchmark model without quantum enhancement using N-k contingency
analysis. Additionally, we offer a comparative assessment of the training
procedures for RL models integrated with a quantum backend.

</details>

<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [492] [Application of Sensitivity Analysis Methods for Studying Neural Network Models](https://arxiv.org/abs/2504.15100)
*Jiaxuan Miao,Sergey Matveev*

Main category: math.NA

TLDR: 该研究展示了多种方法分析神经网络对输入数据扰动的敏感性及其机制，包括Sobol全局敏感性分析、局部敏感性方法和激活最大化技术。实验涉及小型前馈网络和经典卷积架构（VGG-16和ResNet-18），验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索神经网络对输入扰动的敏感性，并解释其内部机制，以优化模型性能和可解释性。

Method: 采用Sobol全局敏感性分析、局部敏感性方法和激活最大化技术，分别在小型前馈网络和卷积网络（VGG-16和ResNet-18）上进行实验。

Result: 全局敏感性分析成功识别并减少了小型网络的关键输入参数，而局部方法和激活最大化揭示了卷积网络的分类模式。与Grad-CAM对比显示激活最大化在超声数据分析中的潜力。

Conclusion: 研究证明了多种敏感性分析方法的适用性，为神经网络的可解释性和优化提供了实用工具。

Abstract: This study demonstrates the capabilities of several methods for analyzing the
sensitivity of neural networks to perturbations of the input data and
interpreting their underlying mechanisms. The investigated approaches include
the Sobol global sensitivity analysis, the local sensitivity method for input
pixel perturbations and the activation maximization technique. As examples, in
this study we consider a small feedforward neural network for analyzing an open
tabular dataset of clinical diabetes data, as well as two classical
convolutional architectures, VGG-16 and ResNet-18, which are widely used in
image processing and classification. Utilization of the global sensitivity
analysis allows us to identify the leading input parameters of the chosen tiny
neural network and reduce their number without significant loss of the
accuracy. As far as global sensitivity analysis is not applicable to larger
models we try the local sensitivity analysis and activation maximization method
in application to the convolutional neural networks. These methods show
interesting patterns for the convolutional models solving the image
classification problem. All in all, we compare the results of the activation
maximization method with popular Grad-CAM technique in the context of
ultrasound data analysis.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [493] [Never Start from Scratch: Expediting On-Device LLM Personalization via Explainable Model Selection](https://arxiv.org/abs/2504.13938)
*Haoming Wang,Boyuan Yang,Xiangyu Yin,Wei Gao*

Main category: cs.HC

TLDR: XPerT是一种新技术，通过选择已个性化的大语言模型（LLM）并基于其微调过程的可解释性，解决了移动设备上LLM个性化计算能力不足和数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 由于数据隐私问题，LLM个性化需在用户移动设备本地完成，但受限于设备计算能力和个人数据不足。

Method: 通过微调已个性化的LLM，并引入XPerT技术，基于模型微调的可解释性选择合适的模型。

Result: 实验表明，XPerT将设备上LLM个性化的计算成本降低83%，数据效率提高51%。

Conclusion: XPerT有效解决了移动设备上LLM个性化的计算和数据效率问题。

Abstract: Personalization of Large Language Models (LLMs) is important in practical
applications to accommodate the individual needs of different mobile users. Due
to data privacy concerns, LLM personalization often needs to be locally done at
the user's mobile device, but such on-device personalization is constrained by
both the limitation of on-device compute power and insufficiency of user's
personal data. In this paper, we address these constraints by fine-tuning an
already personalized LLM with user's personal data, and present XPerT, a new
technique that ensure proper selection of such already personalized LLMs based
on explainability about how they were being fine-tuned. We implemented and
evaluated XPerT on various smartphone models with mainstream LLMs, and
experiment results show that XPerT reduces the computation costs of on-device
LLM personalization by 83%, and improves its data efficiency by 51%.

</details>

### [494] [From Interaction to Collaboration: How Hybrid Intelligence Enhances Chatbot Feedback](https://arxiv.org/abs/2504.13848)
*Janet Rafner,Ryan Q. Guloy,Eden W. Wen,Catherine M. Chiodo,Jacob Sherson*

Main category: cs.HC

TLDR: 研究探讨了两种反馈收集机制对用户参与和反馈行为的影响，发现混合智能（HI）叙事能显著增加用户提供的详细反馈。


<details>
  <summary>Details</summary>
Motivation: 生成式AI聊天机器人的成功依赖于获取有意义的用户反馈，以提升交互质量、系统效果和用户接受度。

Method: 比较标准AI交互与混合智能（HI）框架交互对用户反馈行为的影响。

Result: HI叙事显著增加了用户提供的详细反馈，但未显著影响用户意愿或信任度。

Conclusion: 研究为设计有效的反馈系统提供了见解，平衡用户努力与系统改进潜力。

Abstract: Generative AI (GenAI) chatbots are becoming increasingly integrated into
virtual assistant technologies, yet their success hinges on the ability to
gather meaningful user feedback to improve interaction quality, system
outcomes, and overall user acceptance. Successful chatbot interactions can
enable organizations to build long-term relationships with their customers and
users, supporting customer loyalty and furthering the organization's goals.
This study explores the impact of two distinct narratives and feedback
collection mechanisms on user engagement and feedback behavior: a standard
AI-focused interaction versus a hybrid intelligence (HI) framed interaction.
Initial findings indicate that while small-scale survey measures allowed for no
significant differences in user willingness to leave feedback, use the system,
or trust the system, participants exposed to the HI narrative statistically
significantly provided more detailed feedback. These initial findings offer
insights into designing effective feedback systems for GenAI virtual
assistants, balancing user effort with system improvement potential.

</details>

### [495] [Towards Balancing Preference and Performance through Adaptive Personalized Explainability](https://arxiv.org/abs/2504.13856)
*Andrew Silva,Pradyumna Tambwekar,Mariah Schrum,Matthew Gombolay*

Main category: cs.HC

TLDR: 研究探讨了在自动驾驶车辆模拟环境中，用户对可解释人工智能（xAI）模式的偏好及个性化策略，发现偏好与性能存在差异，并提出自适应策略以平衡两者。


<details>
  <summary>Details</summary>
Motivation: 机器人需通过解释决策标准以建立信任和促进协作，但现有xAI方法假设单一模式适合所有用户，忽视了用户多样性。

Method: 通过两项用户研究，比较语言解释、特征重要性图和决策树三种xAI模式的偏好与性能，并开发自适应个性化策略。

Result: 发现xAI模式在偏好和性能上存在显著差异，且用户偏好与性能不完全一致；自适应策略显著提升了性能。

Conclusion: 自适应个性化策略能有效平衡用户偏好与性能，对xAI在人类-机器人交互中的应用具有重要启示。

Abstract: As robots and digital assistants are deployed in the real world, these agents
must be able to communicate their decision-making criteria to build trust,
improve human-robot teaming, and enable collaboration. While the field of
explainable artificial intelligence (xAI) has made great strides to enable such
communication, these advances often assume that one xAI approach is ideally
suited to each problem (e.g., decision trees to explain how to triage patients
in an emergency or feature-importance maps to explain radiology reports). This
fails to recognize that users have diverse experiences or preferences for
interaction modalities. In this work, we present two user-studies set in a
simulated autonomous vehicle (AV) domain. We investigate (1) population-level
preferences for xAI and (2) personalization strategies for providing robot
explanations. We find significant differences between xAI modes (language
explanations, feature-importance maps, and decision trees) in both preference
(p < 0.01) and performance (p < 0.05). We also observe that a participant's
preferences do not always align with their performance, motivating our
development of an adaptive personalization strategy to balance the two. We show
that this strategy yields significant performance gains (p < 0.05), and we
conclude with a discussion of our findings and implications for xAI in
human-robot interactions.

</details>

### [496] [The Effect of Explainable AI-based Decision Support on Human Task Performance: A Meta-Analysis](https://arxiv.org/abs/2504.13858)
*Felix Haag*

Main category: cs.HC

TLDR: 本文通过元分析探讨了可解释AI（XAI）如何影响分类任务中的人类表现，发现XAI能提升任务表现，但解释本身并非决定性因素。


<details>
  <summary>Details</summary>
Motivation: 信息系统中解释的透明性需求推动了可解释AI（XAI）的发展，但现有研究对XAI是否提升用户任务表现存在不一致结论。

Method: 采用元分析方法，研究XAI对分类任务中人类表现的影响。

Result: XAI能提升任务表现，但解释本身并非决定性因素；研究偏倚风险对解释效果有调节作用，解释类型影响较小。

Conclusion: 研究结果有助于理解人机交互中人类与XAI的协作机制。

Abstract: The desirable properties of explanations in information systems have fueled
the demands for transparency in artificial intelligence (AI) outputs. To
address these demands, the field of explainable AI (XAI) has put forth methods
that can support human decision-making by explaining AI outputs. However,
current empirical works present inconsistent findings on whether such
explanations help to improve users' task performance in decision support
systems (DSS). In this paper, we conduct a meta-analysis to explore how XAI
affects human performance in classification tasks. Our results show an
improvement in task performance through XAI-based decision support, though
explanations themselves are not the decisive driver for this improvement. The
analysis reveals that the studies' risk of bias moderates the effect of
explanations in AI, while the explanation type appears to play only a
negligible role. Our findings contribute to the human computer interaction
field by enhancing the understanding of human-XAI collaboration in DSS.

</details>

### [497] [DoYouTrustAI: A Tool to Teach Students About AI Misinformation and Prompt Engineering](https://arxiv.org/abs/2504.13859)
*Phillip Driscoll,Priyanka Kumar*

Main category: cs.HC

TLDR: 开发了一款名为DoYouTrustAI的网页应用，帮助K-12学生通过识别LLM生成的误导性历史信息提升批判性思维。


<details>
  <summary>Details</summary>
Motivation: LLM（如ChatGPT）的生成性质可能导致错误信息被当作事实传播，需教育学生识别和应对此类问题。

Method: 通过提示工程生成历史人物的准确或误导性摘要，学生需判断其真实性，无需外部资源。

Result: 研究发现需及时纠正误导信息，工具通过熟悉人物选择和已知事实对比增强可信度。

Conclusion: DoYouTrustAI工具有效帮助学生理解LLM的误导风险及提示工程的重要性。

Abstract: AI, especially Large Language Models (LLMs) like ChatGPT, have rapidly
developed and gained widespread adoption in the past five years, shifting user
preference from traditional search engines. However, the generative nature of
LLMs raises concerns about presenting misinformation as fact. To address this,
we developed a web-based application that helps K-12 students enhance critical
thinking by identifying misleading information in LLM responses about major
historical figures. In this paper, we describe the implementation and design
details of the DoYouTrustAI tool, which can be used to provide an interactive
lesson which teaches students about the dangers of misinformation and how
believable generative AI can make it seem. The DoYouTrustAI tool utilizes
prompt engineering to present the user with AI generated summaries about the
life of a historical figure. These summaries can be either accurate accounts of
that persons life, or an intentionally misleading alteration of their history.
The user is tasked with determining the validity of the statement without
external resources. Our research questions for this work were:(RQ1) How can we
design a tool that teaches students about the dangers of misleading information
and of how misinformation can present itself in LLM responses? (RQ2) Can we
present prompt engineering as a topic that is easily understandable for
students? Our findings highlight the need to correct misleading information
before users retain it. Our tool lets users select familiar individuals for
testing to reduce random guessing and presents misinformation alongside known
facts to maintain believability. It also provides pre-configured prompt
instructions to show how different prompts affect AI responses. Together, these
features create a controlled environment where users learn the importance of
verifying AI responses and understanding prompt engineering.

</details>

### [498] [A Survey on (M)LLM-Based GUI Agents](https://arxiv.org/abs/2504.13865)
*Fei Tang,Haolei Xu,Hang Zhang,Siqi Chen,Xingyu Wu,Yongliang Shen,Wenqi Zhang,Guiyang Hou,Zeqi Tan,Yuchen Yan,Kaitao Song,Jian Shao,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.HC

TLDR: 该论文综述了基于LLM的GUI代理的快速发展，分析了其架构、技术组件和评估方法，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨GUI代理从规则脚本到AI驱动系统的演变，以及其在人机交互中的变革潜力。

Method: 系统分析GUI代理的四大核心组件：感知系统、探索机制、规划框架和交互系统。

Result: 揭示了LLM和多模态学习如何推动GUI自动化，并指出了现有评估框架的局限性。

Conclusion: 为研究者和从业者提供了对GUI代理现状的全面理解，并展望了智能界面自动化的未来发展方向。

Abstract: Graphical User Interface (GUI) Agents have emerged as a transformative
paradigm in human-computer interaction, evolving from rule-based automation
scripts to sophisticated AI-driven systems capable of understanding and
executing complex interface operations. This survey provides a comprehensive
examination of the rapidly advancing field of LLM-based GUI Agents,
systematically analyzing their architectural foundations, technical components,
and evaluation methodologies. We identify and analyze four fundamental
components that constitute modern GUI Agents: (1) perception systems that
integrate text-based parsing with multimodal understanding for comprehensive
interface comprehension; (2) exploration mechanisms that construct and maintain
knowledge bases through internal modeling, historical experience, and external
information retrieval; (3) planning frameworks that leverage advanced reasoning
methodologies for task decomposition and execution; and (4) interaction systems
that manage action generation with robust safety controls. Through rigorous
analysis of these components, we reveal how recent advances in large language
models and multimodal learning have revolutionized GUI automation across
desktop, mobile, and web platforms. We critically examine current evaluation
frameworks, highlighting methodological limitations in existing benchmarks
while proposing directions for standardization. This survey also identifies key
technical challenges, including accurate element localization, effective
knowledge retrieval, long-horizon planning, and safety-aware execution control,
while outlining promising research directions for enhancing GUI Agents'
capabilities. Our systematic review provides researchers and practitioners with
a thorough understanding of the field's current state and offers insights into
future developments in intelligent interface automation.

</details>

### [499] [Skeleton-Based Transformer for Classification of Errors and Better Feedback in Low Back Pain Physical Rehabilitation Exercises](https://arxiv.org/abs/2504.13866)
*Aleksa Marusic,Sao Mai Nguyen,Adriana Tapus*

Main category: cs.HC

TLDR: 提出了一种基于Transformer的算法，用于康复训练中的错误分类，旨在为患者提供更详细的反馈。


<details>
  <summary>Details</summary>
Motivation: 患者在无监督情况下康复训练的参与度下降，现有评估系统仅提供二元或连续评分，不足以帮助患者改进。

Method: 采用基于骨架的运动评估，利用人体姿态估计，提出了一种受HyperFormer启发的Transformer模型。

Result: 在KERAAL数据集上评估，模型显著优于现有方法，并提供了关节重要性计算方法。

Conclusion: 该研究为康复训练提供了更详细的反馈机制，有助于患者改进动作。

Abstract: Physical rehabilitation exercises suggested by healthcare professionals can
help recovery from various musculoskeletal disorders and prevent re-injury.
However, patients' engagement tends to decrease over time without direct
supervision, which is why there is a need for an automated monitoring system.
In recent years, there has been great progress in quality assessment of
physical rehabilitation exercises. Most of them only provide a binary
classification if the performance is correct or incorrect, and a few provide a
continuous score. This information is not sufficient for patients to improve
their performance. In this work, we propose an algorithm for error
classification of rehabilitation exercises, thus making the first step toward
more detailed feedback to patients. We focus on skeleton-based exercise
assessment, which utilizes human pose estimation to evaluate motion. Inspired
by recent algorithms for quality assessment during rehabilitation exercises, we
propose a Transformer-based model for the described classification. Our model
is inspired by the HyperFormer method for human action recognition, and adapted
to our problem and dataset. The evaluation is done on the KERAAL dataset, as it
is the only medical dataset with clear error labels for the exercises, and our
model significantly surpasses state-of-the-art methods. Furthermore, we bridge
the gap towards better feedback to the patients by presenting a way to
calculate the importance of joints for each exercise.

</details>

### [500] [Using Generative AI Personas Increases Collective Diversity in Human Ideation](https://arxiv.org/abs/2504.13868)
*Yun Wan,Yoram M Kalman*

Main category: cs.HC

TLDR: 研究表明，通过引入多样化的AI角色，可以消除生成式AI对创意多样性减少的影响，甚至可能增强多样性。


<details>
  <summary>Details</summary>
Motivation: 挑战生成式AI在创意输出中贡献与多样性减少之间的权衡问题。

Method: 修改实验设计，使用10种不同特质的AI角色生成300个故事情节，人类参与者基于这些情节创作故事。

Result: AI辅助的故事保持了与纯人类创作相同的多样性，且在描述和情感语言上更具多样性。

Conclusion: 在AI输入阶段引入多样性可以保留并可能增强人类与AI合作时的创意多样性。

Abstract: This study challenges the widely-reported tradeoff between generative AI's
(GenAI) contribution to creative outcomes and decreased diversity of these
outcomes. We modified the design of such a study, by Doshi and Hauser (2024),
in which participants wrote short stories either aided or unaided by GenAI plot
ideas[1]. In the modified study, plot ideas were generated through ten unique
GenAI "personas" with diverse traits (e.g. cultural backgrounds, thinking
styles, genre preferences), creating a pool of 300 story plots. While plot
ideas from any individual persona showed high similarity (average cosine
similarity of 0.92), ideas across different personas exhibited substantial
variation (average similarity of 0.20). When human participants wrote stories
based on these diverse plot ideas, their collective outputs maintained the same
level of diversity as stories written without GenAI assistance, effectively
eliminating the diversity reduction observed in [1]. Traditional text analytics
further revealed that GenAI-assisted stories featured greater diversity in
descriptive and emotional language compared to purely human-generated stories
without GenAI assistance. Our findings demonstrate that introducing diversity
at the AI input stage through distinct personas can preserve and potentially
enhance the collective diversity of human creative outputs when collaborating
with GenAI.

</details>

### [501] [Human aversion? Do AI Agents Judge Identity More Harshly Than Performance](https://arxiv.org/abs/2504.13871)
*Yuanjun Feng,Vivek Chodhary,Yash Raj Shrestha*

Main category: cs.HC

TLDR: 研究探讨了AI在混合决策系统中对人类判断的评估，发现AI倾向于低估人类输入，尤其是在身份披露和顺序偏置的情况下。


<details>
  <summary>Details</summary>
Motivation: 填补管理研究中关于AI评估人类判断的空白，解决因隐私问题无法直接部署LLMs的企业的需求。

Method: 通过预测任务分析LLM如何权衡人类与算法预测，研究身份披露和顺序的影响。

Result: AI系统系统性低估人类建议，对错误惩罚更严厉，身份披露和顺序偏置加剧此现象。

Conclusion: 揭示了人类-AI协作中的不平等，提出了间接部署LLMs的框架，强调审计AI权重机制的重要性。

Abstract: This study examines the understudied role of algorithmic evaluation of human
judgment in hybrid decision-making systems, a critical gap in management
research. While extant literature focuses on human reluctance to follow
algorithmic advice, we reverse the perspective by investigating how AI agents
based on large language models (LLMs) assess and integrate human input. Our
work addresses a pressing managerial constraint: firms barred from deploying
LLMs directly due to privacy concerns can still leverage them as mediating
tools (for instance, anonymized outputs or decision pipelines) to guide
high-stakes choices like pricing or discounts without exposing proprietary
data. Through a controlled prediction task, we analyze how an LLM-based AI
agent weights human versus algorithmic predictions. We find that the AI system
systematically discounts human advice, penalizing human errors more severely
than algorithmic errors--a bias exacerbated when the agent's identity (human vs
AI) is disclosed and the human is positioned second. These results reveal a
disconnect between AI-generated trust metrics and the actual influence of human
judgment, challenging assumptions about equitable human-AI collaboration. Our
findings offer three key contributions. First, we identify a reverse algorithm
aversion phenomenon, where AI agents undervalue human input despite comparable
error rates. Second, we demonstrate how disclosure and positional bias interact
to amplify this effect, with implications for system design. Third, we provide
a framework for indirect LLM deployment that balances predictive power with
data privacy. For practitioners, this research emphasize the need to audit AI
weighting mechanisms, calibrate trust dynamics, and strategically design
decision sequences in human-AI systems.

</details>

### [502] [New care pathways for supporting transitional care from hospitals to home using AI and personalized digital assistance](https://arxiv.org/abs/2504.13877)
*Ionut Anghel,Tudor Cioara,Roberta Bevilacqua,Federico Barbarossa,Terje Grimstad,Riitta Hellman,Arnor Solberg,Lars Thomas Boye,Ovidiu Anchidin,Ancuta Nemes,Camilla Gabrielsen*

Main category: cs.HC

TLDR: 本文探讨了过渡性护理在欧洲未来医疗系统中的重要性，提出通过整合物联网、人工智能和数字辅助技术来优化患者从医院到家庭的护理过渡，以减少再住院风险。


<details>
  <summary>Details</summary>
Motivation: 随着人口老龄化，医疗需求增加，过渡性护理成为解决医院资源压力的关键。整合创新技术可提升护理连续性和患者安全。

Method: 研究整合物联网、人工智能和数字辅助技术到传统护理路径中，填补当前过渡性护理的空白，并定义技术映射以优化护理流程。

Result: 提出了技术整合的具体方案，旨在改善患者预后、安全和生活质量，减少再住院率。

Conclusion: 通过临床试验验证技术整合对患者护理的积极影响，并讨论其对医疗系统的潜在效益。

Abstract: Transitional care may play a vital role for the sustainability of Europe
future healthcare system, offering solutions for relocating patient care from
hospital to home therefore addressing the growing demand for medical care as
the population is ageing. However, to be effective, it is essential to
integrate innovative Information and Communications Technology technologies to
ensure that patients with comorbidities experience a smooth and coordinated
transition from hospitals or care centers to home, thereby reducing the risk of
rehospitalization. In this paper, we present an overview of the integration of
Internet of Things, artificial intelligence, and digital assistance
technologies with traditional care pathways to address the challenges and needs
of healthcare systems in Europe. We identify the current gaps in transitional
care and define the technology mapping to enhance the care pathways, aiming to
improve patient outcomes, safety, and quality of life avoiding hospital
readmissions. Finally, we define the trial setup and evaluation methodology
needed to provide clinical evidence that supports the positive impact of
technology integration on patient care and discuss the potential effects on the
healthcare system.

</details>

### [503] [Towards a Multimodal Document-grounded Conversational AI System for Education](https://arxiv.org/abs/2504.13884)
*Karan Taneja,Anjali Singh,Ashok K. Goel*

Main category: cs.HC

TLDR: MuDoC是一个基于GPT-4o的多模态对话AI系统，结合文本和图像提升学习效果，验证内容可信度。实验表明其增强学习参与度和信任，但对任务表现无显著影响。


<details>
  <summary>Details</summary>
Motivation: 探索多模态对话AI在教育中的应用，弥补当前文本交互的不足，并确保内容可验证以建立信任。

Method: 基于GPT-4o开发MuDoC系统，结合文档中的文本和图像生成多模态响应，并与纯文本系统对比。

Result: 多模态和内容验证显著提升学习参与度和信任，但对任务表现无显著影响。

Conclusion: 多模态对话AI在教育中潜力大，未来需进一步优化以提升学习效果。

Abstract: Multimedia learning using text and images has been shown to improve learning
outcomes compared to text-only instruction. But conversational AI systems in
education predominantly rely on text-based interactions while multimodal
conversations for multimedia learning remain unexplored. Moreover, deploying
conversational AI in learning contexts requires grounding in reliable sources
and verifiability to create trust. We present MuDoC, a Multimodal
Document-grounded Conversational AI system based on GPT-4o, that leverages both
text and visuals from documents to generate responses interleaved with text and
images. Its interface allows verification of AI generated content through
seamless navigation to the source. We compare MuDoC to a text-only system to
explore differences in learner engagement, trust in AI system, and their
performance on problem-solving tasks. Our findings indicate that both visuals
and verifiability of content enhance learner engagement and foster trust;
however, no significant impact in performance was observed. We draw upon
theories from cognitive and learning sciences to interpret the findings and
derive implications, and outline future directions for the development of
multimodal conversational AI systems in education.

</details>

### [504] [Interview AI-ssistant: Designing for Real-Time Human-AI Collaboration in Interview Preparation and Execution](https://arxiv.org/abs/2504.13847)
*Zhe Liu*

Main category: cs.HC

TLDR: 该论文提出了一种名为Interview AI-ssistant的系统，旨在通过实时AI辅助提升访谈质量，并通过四项研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 访谈在定性研究中具有重要价值，但访谈者面临实时信息处理、问题调整和关系维护等认知挑战，AI辅助可以缓解这些问题。

Method: 通过四项研究：需求调研、AI辅助访谈准备的原型开发、实时AI辅助的实验评估以及实际场景的实地研究。

Result: 研究验证了Interview AI-ssistant在提升访谈效果方面的潜力，并为智能用户界面设计提供了指导。

Conclusion: 该研究为复杂社交任务中的人机协作界面提供了新见解，并为AI增强的定性研究工具设计了指南。

Abstract: Recent advances in large language models (LLMs) offer unprecedented
opportunities to enhance human-AI collaboration in qualitative research
methods, including interviews. While interviews are highly valued for gathering
deep, contextualized insights, interviewers often face significant cognitive
challenges, such as real-time information processing, question adaptation, and
rapport maintenance. My doctoral research introduces Interview AI-ssistant, a
system designed for real-time interviewer-AI collaboration during both the
preparation and execution phases. Through four interconnected studies, this
research investigates the design of effective human-AI collaboration in
interviewing contexts, beginning with a formative study of interviewers' needs,
followed by a prototype development study focused on AI-assisted interview
preparation, an experimental evaluation of real-time AI assistance during
interviews, and a field study deploying the system in a real-world research
setting. Beyond informing practical implementations of intelligent interview
support systems, this work contributes to the Intelligent User Interfaces (IUI)
community by advancing the understanding of human-AI collaborative interfaces
in complex social tasks and establishing design guidelines for AI-enhanced
qualitative research tools.

</details>

### [505] [Kanji Workbook: A Writing-Based Intelligent Tutoring System for Learning Proper Japanese Kanji Writing Technique with Instructor-Emulated Assessment](https://arxiv.org/abs/2504.13888)
*Paul Taele,Jung In Koh,Tracy Hammond*

Main category: cs.HC

TLDR: Kanji Workbook是一个智能辅导系统，通过模拟教师反馈帮助学生学习日语汉字书写，提高课程成绩。


<details>
  <summary>Details</summary>
Motivation: 英语母语学生在学习日语汉字时面临困难，现有教育应用缺乏教师模拟反馈。

Method: 开发Kanji Workbook系统，提供智能评估和视觉动画反馈，基于教师访谈和课堂观察。

Result: 使用该系统的学生在课程成绩上优于同龄人，并对系统功能反应积极。

Conclusion: Kanji Workbook有效提升了学生的汉字书写能力和学习效果。

Abstract: Kanji script writing is a skill that is often introduced to novice Japanese
foreign language students for achieving Japanese writing mastery, but often
poses difficulties to students with primarily English fluency due to their its
vast differences with written English. Instructors often introduce various
pedagogical methods -- such as visual structure and written techniques -- to
assist students in kanji study, but may lack availability providing direct
feedback on students' writing outside of class. Current educational
applications are also limited due to lacking richer instructor-emulated
feedback. We introduce Kanji Workbook, a writing-based intelligent tutoring
system for students to receive intelligent assessment that emulates human
instructor feedback. Our interface not only leverages students' computing
devices for allowing them to learn, practice, and review the writing of
prompted characters from their course's kanji script lessons, but also provides
a diverse set of writing assessment metrics -- derived from instructor
interviews and classroom observation insights -- through intelligent scoring
and visual animations. We deployed our interface onto novice- and
intermediate-level university courses over an entire academic year, and
observed that interface users on average achieved higher course grades than
their peers and also reacted positively to our interface's various features.

</details>

### [506] [3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark](https://arxiv.org/abs/2504.13861)
*Ivan Sviridov,Amina Miftakhova,Artemiy Tereshchenko,Galina Zubkova,Pavel Blinov,Andrey Savchenko*

Main category: cs.HC

TLDR: 3MDBench是一个开源评估框架，用于评估大视觉语言模型（LVLM）在医疗咨询中的表现，通过模拟真实患者行为和多种诊断策略，提升诊断准确性和对话质量。


<details>
  <summary>Details</summary>
Motivation: 探索LVLM在远程医疗中的应用潜力，尤其是其处理多样化患者行为的能力，现有基准未能充分模拟真实世界的患者变异性。

Method: 开发3MDBench框架，整合四种气质驱动的患者代理和评估代理，结合文本和图像数据，评估LVLM在不同诊断策略下的表现。

Result: 对话和多模态输入显著提升诊断效果，F1分数从50.4提高到54.2；结合CNN模型的预测结果后，F1分数进一步提升至70.3。

Conclusion: 3MDBench为AI驱动的医疗助手提供了可扩展的评估框架，揭示了患者气质、对话策略和多模态推理对诊断质量的影响，推动了更可靠、更具同理心的医疗AI发展。

Abstract: Large Vision-Language Models (LVLMs) are increasingly being explored for
applications in telemedicine, yet their ability to engage with diverse patient
behaviors remains underexplored. We introduce 3MDBench (Medical Multimodal
Multi-agent Dialogue Benchmark), an open-source evaluation framework designed
to assess LLM-driven medical consultations. Unlike existing benchmarks,
3MDBench simulates real-world patient variability by incorporating four
temperament-driven Patient Agents and an Assessor Agent that evaluates
diagnostic accuracy and dialogue quality. The benchmark integrates textual and
image-based patient data across 34 common diagnoses, mirroring real-world
telemedicine interactions. Under different diagnostic strategies, we evaluate
state-of-the-art LVLMs. Our findings demonstrate that incorporating dialogue
improves the F1 score from 50.4 to 54.2 compared to non-dialogue settings,
underscoring the value of context-driven, information-seeking questioning.
Additionally, we demonstrate that multimodal inputs enhance diagnostic
efficiency. Image-supported models outperform text-only counterparts by raising
the diagnostic F1 score from 52.8 to 54.2 in a similar dialogue setting.
Finally, we suggest an approach that improves the diagnostic F1-score to 70.3
by training the CNN model on the diagnosis prediction task and incorporating
its top-3 predictions into the LVLM context. 3MDBench provides a reproducible
and extendable evaluation framework for AI-driven medical assistants. It offers
insights into how patient temperament, dialogue strategies, and multimodal
reasoning influence diagnosis quality. By addressing real-world complexities in
telemedicine, our benchmark paves the way for more empathetic, reliable, and
context-aware AI-driven healthcare solutions. The source code of our benchmark
is publicly available: https://github.com/univanxx/3mdbench

</details>

### [507] [Maestoso: An Intelligent Educational Sketching Tool for Learning Music Theory](https://arxiv.org/abs/2504.13889)
*Paul Taele,Laura Barreto,Tracy Hammond*

Main category: cs.HC

TLDR: Maestoso是一个教育工具，通过草图练习帮助初学者学习音乐理论，自动识别输入并提供反馈。


<details>
  <summary>Details</summary>
Motivation: 现有工具在反馈、书写模式或音乐理论熟悉度方面存在不足，Maestoso旨在填补这一空白。

Method: 利用草图和手势识别技术自动识别学生绘制的音乐结构，并生成模拟教师反馈。

Result: Maestoso能较好地识别音乐结构元素，初学者在一次课程中即可掌握基础音乐理论。

Conclusion: Maestoso为音乐理论学习提供了一种有效且用户友好的方法。

Abstract: Learning music theory not only has practical benefits for musicians to write,
perform, understand, and express music better, but also for both non-musicians
to improve critical thinking, math analytical skills, and music appreciation.
However, current external tools applicable for learning music theory through
writing when human instruction is unavailable are either limited in feedback,
lacking a written modality, or assuming already strong familiarity of music
theory concepts. In this paper, we describe Maestoso, an educational tool for
novice learners to learn music theory through sketching practice of quizzed
music structures. Maestoso first automatically recognizes students' sketched
input of quizzed concepts, then relies on existing sketch and gesture
recognition techniques to automatically recognize the input, and finally
generates instructor-emulated feedback. From our evaluations, we demonstrate
that Maestoso performs reasonably well on recognizing music structure elements
and that novice students can comfortably grasp introductory music theory in a
single session.

</details>

### [508] [Mozualization: Crafting Music and Visual Representation with Multimodal AI](https://arxiv.org/abs/2504.13891)
*Wanfang Xu,Lixiang Zhao,Haiwen Song,Xinheng Song,Zhaolin Lu,Yu Liu,Min Chen,Eng Gee Lim,Lingyun Yu*

Main category: cs.HC

TLDR: Mozualization是一个音乐生成和编辑工具，通过整合多种输入（如关键词、图像和声音片段）生成多风格嵌入式音乐。


<details>
  <summary>Details</summary>
Motivation: 受人们表达情感的方式（如写诗、绘画或听音乐）启发，开发一个能将情感表达转化为连贯歌曲的工具。

Method: 开发了一个工具，允许用户无缝整合个人偏好和灵感，并通过用户研究（9名音乐爱好者参与）评估工具效果。

Result: 用户研究评估了用户体验、参与度以及与生成音乐的互动效果。

Conclusion: Mozualization展示了将多样化输入转化为个性化音乐的潜力，为进一步改进提供了见解。

Abstract: In this work, we introduce Mozualization, a music generation and editing tool
that creates multi-style embedded music by integrating diverse inputs, such as
keywords, images, and sound clips (e.g., segments from various pieces of music
or even a playful cat's meow). Our work is inspired by the ways people express
their emotions -- writing mood-descriptive poems or articles, creating drawings
with warm or cool tones, or listening to sad or uplifting music. Building on
this concept, we developed a tool that transforms these emotional expressions
into a cohesive and expressive song, allowing users to seamlessly incorporate
their unique preferences and inspirations. To evaluate the tool and, more
importantly, gather insights for its improvement, we conducted a user study
involving nine music enthusiasts. The study assessed user experience,
engagement, and the impact of interacting with and listening to the generated
music.

</details>

### [509] [Toward Automated Qualitative Analysis: Leveraging Large Language Models for Tutoring Dialogue Evaluation](https://arxiv.org/abs/2504.13882)
*Megan Gu,Chloe Qianhui Zhao,Claire Liu,Nikhil Patel,Jahnvi Shah,Jionghao Lin,Kenneth R. Koedinger*

Main category: cs.HC

TLDR: 论文介绍了一种利用大语言模型（LLMs）自动评估五种关键辅导策略有效性的系统，结果显示模型在排除错误分类方面表现良好，但在准确识别策略方面仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索LLMs在分析辅导策略中的应用潜力，以提升辅导对话的评估效率。

Method: 使用GPT-3.5和少量示例提示对公共数据集（Teacher-Student Chatroom Corpus）中的辅导策略进行分类，评估其有效性。

Result: 模型在五种策略中的真阴性率（TNR）为0.655至0.738，召回率为0.327至0.432，其中“帮助学生管理不平等”表现最佳。

Conclusion: LLMs在辅导策略分析中具有潜力，未来可通过更先进的模型进一步优化反馈效果。

Abstract: Our study introduces an automated system leveraging large language models
(LLMs) to assess the effectiveness of five key tutoring strategies: 1. giving
effective praise, 2. reacting to errors, 3. determining what students know, 4.
helping students manage inequity, and 5. responding to negative self-talk.
Using a public dataset from the Teacher-Student Chatroom Corpus, our system
classifies each tutoring strategy as either being employed as desired or
undesired. Our study utilizes GPT-3.5 with few-shot prompting to assess the use
of these strategies and analyze tutoring dialogues. The results show that for
the five tutoring strategies, True Negative Rates (TNR) range from 0.655 to
0.738, and Recall ranges from 0.327 to 0.432, indicating that the model is
effective at excluding incorrect classifications but struggles to consistently
identify the correct strategy. The strategy \textit{helping students manage
inequity} showed the highest performance with a TNR of 0.738 and Recall of
0.432. The study highlights the potential of LLMs in tutoring strategy analysis
and outlines directions for future improvements, including incorporating more
advanced models for more nuanced feedback.

</details>

### [510] [The Human Robot Social Interaction (HSRI) Dataset: Benchmarking Foundational Models' Social Reasoning](https://arxiv.org/abs/2504.13898)
*Dong Won Lee,Yubin Kim,Denison Guvenoz,Sooyeon Jeong,Parker Malachowsky,Louis-Philippe Morency,Cynthia Breazeal,Hae Won Park*

Main category: cs.HC

TLDR: 该论文提出一个大规模真实世界人机社交互动数据集（HSRI），用于评估语言模型和基础模型在社交推理中的能力，并设计了八项新基准任务。实验表明当前模型在这些任务上表现不佳，为社交智能AI的发展提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 提升具身AI在真实世界社交互动中的推理能力，利用语言模型和基础模型作为自动评估工具，以改进AI代理的策略。

Method: 引入包含400个真实世界人机互动视频和超过10K标注的HSRI数据集，设计八项基准任务评估模型在社交错误识别、解释因素理解、互动流程分析和纠正措施提供等方面的能力。

Result: 实验显示当前语言模型和基础模型在这些任务上表现不佳，表明数据集和基准任务为社交智能AI的研究提供了重要工具。

Conclusion: HSRI数据集和基准任务为提升AI的社交推理能力奠定了基础，揭示了当前模型的局限性，推动了社交智能AI的发展。

Abstract: Our work aims to advance the social reasoning of embodied artificial
intelligence (AI) agents in real-world social interactions. Recently, language
models (LMs) and foundational models (FMs) are being utilized as automatic
evaluators of human-AI interactions with the goal of eventually being used to
improve the policy of the AI agent. To enable further research in this
direction, we introduce a large-scale real-world Human Robot Social Interaction
(HSRI) Dataset to benchmark the capabilities of LMs and FMs to identify and
reason about social interactions, specifically with regard to robot social
errors and competencies . Our dataset consists of 400 real-world human social
robot interaction videos and over 10K annotations, detailing the robot's social
errors, competencies, rationale, and corrective actions, capturing unique
aspects of human-AI interaction only present in real-world interactions. To
further assess AI models' ability to reason about social interactions, we
propose eight new benchmark tasks for evaluating centered around whether AI
models can (1) evaluate social interactions via detecting social errors and
competencies, (2) identify the explanatory factors associated to errors and
competencies, (3) understand the flow of real-world social interactions, and
(4) provide reasons and corrective actions for social errors. Human studies and
experiments with modern LMs and FMs reveal that current models struggle with
these tasks, demonstrating that our dataset and benchmark provides a step
forward towards socially intelligent AI.

</details>

### [511] [AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants](https://arxiv.org/abs/2504.13887)
*Isabel Villanueva,Tara Bobinac,Binwei Yao,Junjie Hu,Kaiping Chen*

Main category: cs.HC

TLDR: 研究探讨了AI聊天机器人在跨文化对话中培养共情能力的效果，发现其表现因文化背景和对话类型而异。


<details>
  <summary>Details</summary>
Motivation: 尽管AI聊天机器人在公共对话中的应用日益广泛，但其在促进跨文化共情方面的实证研究仍不足。

Method: 通过随机对话实验，比较了不同类型（审议式与非审议式、文化对齐与非对齐）的AI聊天机器人互动对跨文化共情的影响。

Result: 审议式对话提升了美国参与者的跨文化共情，但对拉丁美洲参与者无效，因后者认为AI回应缺乏文化准确性。

Conclusion: 研究强调设计具备文化真实性的AI系统的重要性，并揭示了大型语言模型在文化知识上的局限性。

Abstract: Despite the growing integration of AI chatbots as conversational agents in
public discourse, empirical evidence regarding their capacity to foster
intercultural empathy remains limited. Using a randomized dialogue experiment,
we examined how different types of AI chatbot interaction, i.e., deliberative
versus non-deliberative and culturally aligned versus non-aligned, affect
intercultural empathy across cultural groups. Results show that deliberative
conversations increased intercultural empathy among American participants but
not Latin American participants, who perceived AI responses as culturally
inaccurate and failing to represent their cultural contexts and perspectives
authentically. Real-time interaction analyses reveal that these differences
stem from cultural knowledge gaps inherent in Large Language Models. Despite
explicit prompting and instruction to represent cultural perspectives in
participants' native languages, AI systems still exhibit significant
disparities in cultural representation. This highlights the importance of
designing AI systems capable of culturally authentic engagement in deliberative
conversations. Our study contributes to deliberation theory and AI alignment
research by underscoring AI's role in intercultural dialogue and the persistent
challenge of representational asymmetry in democratic discourse.

</details>

### [512] [Predicting Satisfaction of Counterfactual Explanations from Human Ratings of Explanatory Qualities](https://arxiv.org/abs/2504.13899)
*Marharyta Domnich,Rasmus Moorits Veski,Julius Välja,Kadi Tulver,Raul Vicente*

Main category: cs.HC

TLDR: 论文研究了反事实解释的质量评估问题，发现可行性和信任是用户满意度的最强预测因素，同时揭示了其他解释标准的重要性。


<details>
  <summary>Details</summary>
Motivation: 反事实解释在可解释AI中广泛应用，但如何评估其质量仍是一个开放问题。传统定量指标和用户研究各有局限，需要更细致的评估方法。

Method: 通过分析206名参与者对反事实解释的评分数据，建模用户满意度与七个解释标准的关系。

Result: 可行性和信任是满意度的最强预测因素，其他标准也解释了58%的方差。复杂性独立于满意度，用户背景显著影响评价。

Conclusion: 研究结果为设计适应不同用户和场景的反事实解释算法提供了依据。

Abstract: Counterfactual explanations are a widely used approach in Explainable AI,
offering actionable insights into decision-making by illustrating how small
changes to input data can lead to different outcomes. Despite their importance,
evaluating the quality of counterfactual explanations remains an open problem.
Traditional quantitative metrics, such as sparsity or proximity, fail to fully
account for human preferences in explanations, while user studies are
insightful but not scalable. Moreover, relying only on a single overall
satisfaction rating does not lead to a nuanced understanding of why certain
explanations are effective or not. To address this, we analyze a dataset of
counterfactual explanations that were evaluated by 206 human participants, who
rated not only overall satisfaction but also seven explanatory criteria:
feasibility, coherence, complexity, understandability, completeness, fairness,
and trust. Modeling overall satisfaction as a function of these criteria, we
find that feasibility (the actionability of suggested changes) and trust (the
belief that the changes would lead to the desired outcome) consistently stand
out as the strongest predictors of user satisfaction, though completeness also
emerges as a meaningful contributor. Crucially, even excluding feasibility and
trust, other metrics explain 58% of the variance, highlighting the importance
of additional explanatory qualities. Complexity appears independent, suggesting
more detailed explanations do not necessarily reduce satisfaction. Strong
metric correlations imply a latent structure in how users judge quality, and
demographic background significantly shapes ranking patterns. These insights
inform the design of counterfactual algorithms that adapt explanatory qualities
to user expertise and domain context.

</details>

### [513] [Supporting Students' Reading and Cognition with AI](https://arxiv.org/abs/2504.13900)
*Yue Fu,Alexis Hiniker*

Main category: cs.HC

TLDR: 研究分析了学生在使用AI工具辅助阅读时的认知行为变化，发现初期高阶思维（分析、评价）增加，但后期趋向被动阅读，提出了AI阅读支持系统的设计建议。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在学习中的广泛应用，需了解其对用户阅读过程和认知参与的影响。

Method: 收集并分析了124次学生使用AI工具辅助阅读的会话数据，根据布鲁姆教育目标分类法对提示进行分类。

Result: 初期高阶思维提示增多，但长期使用后用户趋向被动阅读。

Conclusion: 建议设计支持低阶认知任务的结构化支架和鼓励高阶思维的主动提示，并引入自适应和人工干预功能，以平衡效率与认知参与。

Abstract: With the rapid adoption of AI tools in learning contexts, it is vital to
understand how these systems shape users' reading processes and cognitive
engagement. We collected and analyzed text from 124 sessions with AI tools, in
which students used these tools to support them as they read assigned readings
for an undergraduate course. We categorized participants' prompts to AI
according to Bloom's Taxonomy of educational objectives -- Remembering,
Understanding, Applying, Analyzing, Evaluating. Our results show that
``Analyzing'' and ``Evaluating'' are more prevalent in users' second and third
prompts within a single usage session, suggesting a shift toward higher-order
thinking. However, in reviewing users' engagement with AI tools over several
weeks, we found that users converge toward passive reading engagement over
time. Based on these results, we propose design implications for future AI
reading-support systems, including structured scaffolds for lower-level
cognitive tasks (e.g., recalling terms) and proactive prompts that encourage
higher-order thinking (e.g., analyzing, applying, evaluating). Additionally, we
advocate for adaptive, human-in-the-loop features that allow students and
instructors to tailor their reading experiences with AI, balancing efficiency
with enriched cognitive engagement. Our paper expands the dialogue on
integrating AI into academic reading, highlighting both its potential benefits
and challenges.

</details>

### [514] [Measuring Mental Health Variables in Computational Research: Toward Validated, Dimensional, and Transdiagnostic Approaches](https://arxiv.org/abs/2504.13890)
*Chen Shani,Elizabeth C. Stade*

Main category: cs.HC

TLDR: 论文指出计算心理健康研究中常使用不适当的心理病理学测量方法，提出了三个关键问题，并建议使用已验证的、维度的和跨诊断的测量方法。


<details>
  <summary>Details</summary>
Motivation: 当前计算心理健康研究常依赖未经验证的测量方法，导致研究有效性不足，需改进以提升研究质量。

Method: 识别了三个关键问题：1) 依赖未验证的测量方法；2) 将心理健康构念视为分类而非维度；3) 关注特定障碍而非跨诊断构念。

Result: 提出使用已验证的、维度的和跨诊断的测量方法，并提供实用建议。

Conclusion: 使用反映心理病理学本质和结构的有效测量方法对计算心理健康研究至关重要。

Abstract: Computational mental health research develops models to predict and
understand psychological phenomena, but often relies on inappropriate measures
of psychopathology constructs, undermining validity. We identify three key
issues: (1) reliance on unvalidated measures (e.g., self-declared diagnosis)
over validated ones (e.g., diagnosis by clinician); (2) treating mental health
constructs as categorical rather than dimensional; and (3) focusing on
disorder-specific constructs instead of transdiagnostic ones. We outline the
benefits of using validated, dimensional, and transdiagnostic measures and
offer practical recommendations for practitioners. Using valid measures that
reflect the nature and structure of psychopathology is essential for
computational mental health research.

</details>

### [515] [Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge](https://arxiv.org/abs/2504.13904)
*Donghuo Zeng,Roberto Legaspi,Yuewen Sun,Xinshuai Dong,Kazushi Ikeda,Peter Spirtes,Kun Zhang*

Main category: cs.HC

TLDR: 论文提出了一种基于因果和反事实知识的自适应策略，通过反事实推理和因果发现优化系统响应，显著提升了说服性对话系统的效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何通过因果和反事实知识优化系统响应，从而提升用户与系统的交互效果。

Method: 方法包括利用反事实推理生成假设场景，通过因果发现识别策略，并将心理构造和不可观测噪声作为潜在因素进行建模。

Result: 实验结果表明，该方法在真实数据集上显著提升了说服性对话系统的累积奖励。

Conclusion: 结论指出，因果发现和反事实推理的结合能有效指导个性化对话策略优化。

Abstract: We hypothesize that optimal system responses emerge from adaptive strategies
grounded in causal and counterfactual knowledge. Counterfactual inference
allows us to create hypothetical scenarios to examine the effects of
alternative system responses. We enhance this process through causal discovery,
which identifies the strategies informed by the underlying causal structure
that govern system behaviors. Moreover, we consider the psychological
constructs and unobservable noises that might be influencing user-system
interactions as latent factors. We show that these factors can be effectively
estimated. We employ causal discovery to identify strategy-level causal
relationships among user and system utterances, guiding the generation of
personalized counterfactual dialogues. We model the user utterance strategies
as causal factors, enabling system strategies to be treated as counterfactual
actions. Furthermore, we optimize policies for selecting system responses based
on counterfactual data. Our results using a real-world dataset on social good
demonstrate significant improvements in persuasive system outcomes, with
increased cumulative rewards validating the efficacy of causal discovery in
guiding personalized counterfactual inference and optimizing dialogue policies
for a persuasive dialogue system.

</details>

### [516] [TALLMesh: a simple application for performing Thematic Analysis with Large Language Models](https://arxiv.org/abs/2504.13892)
*Stefano De Paoli,Alex Fawzi*

Main category: cs.HC

TLDR: 本文提出了一种基于大型语言模型（LLM）的图形用户界面（GUI）应用，用于辅助研究人员进行主题分析（TA），无需编程技能即可生成代码和主题。


<details>
  <summary>Details</summary>
Motivation: 为缺乏编程技能的研究人员（如社会科学或人文学科）提供一种简单易用的工具，利用LLM进行主题分析。

Method: 开发了一个基于streamlit框架的GUI应用，结合Python脚本和LLM的API，支持用户上传文本数据并生成初始代码和主题。

Result: 应用支持用户通过人机交互过程迭代优化代码和主题，同时保持方法论的严谨性。

Conclusion: 该应用为定性研究提供了潜力，未来将进一步优化设计和功能。

Abstract: Thematic analysis (TA) is a widely used qualitative research method for
identifying and interpreting patterns within textual data, such as qualitative
interviews. Recent research has shown that it is possible to satisfactorily
perform TA using Large Language Models (LLMs). This paper presents a novel
application using LLMs to assist researchers in conducting TA. The application
enables users to upload textual data, generate initial codes and themes. All of
this is possible through a simple Graphical User Interface, (GUI) based on the
streamlit framework, working with python scripts for the analysis, and using
Application Program Interfaces of LLMs. Having a GUI is particularly important
for researchers in fields where coding skills may not be prevalent, such as
social sciences or humanities. With the app, users can iteratively refine codes
and themes adopting a human-in-the-loop process, without the need to work with
programming and scripting. The paper describes the application key features,
highlighting its potential for qualitative research while preserving
methodological rigor. The paper discusses the design and interface of the app
and outlines future directions for this work.

</details>

### [517] [AI-Assisted Conversational Interviewing: Effects on Data Quality and User Experience](https://arxiv.org/abs/2504.13908)
*Soubhik Barari,Jarret Angbazo,Natalie Wang,Leah M. Christian,Elizabeth Dean,Zoe Slowinski,Brandon Sepulvado*

Main category: cs.HC

TLDR: 研究提出了一种AI辅助的对话式访谈框架，以弥补标准化调查和深度访谈之间的差距，并通过实验验证了其可行性和效果。


<details>
  <summary>Details</summary>
Motivation: 标准化调查效率高但缺乏深度，而对话式访谈质量高但难以扩展和保持一致。研究旨在通过AI方法结合两者的优势。

Method: 通过网页调查实验，随机分配1800名参与者与文本机器人互动，动态探测回答并进行实时编码。

Result: 文本机器人在实时编码中表现中等，开放性问题回答更详细，但受访者体验略有下降。

Conclusion: AI方法可以增强网络调查中的开放式数据收集，具有可行性。

Abstract: Standardized surveys scale efficiently but sacrifice depth, while
conversational interviews improve response quality at the cost of scalability
and consistency. This study bridges the gap between these methods by
introducing a framework for AI-assisted conversational interviewing. To
evaluate this framework, we conducted a web survey experiment where 1,800
participants were randomly assigned to text-based conversational AI agents, or
"textbots", to dynamically probe respondents for elaboration and interactively
code open-ended responses. We assessed textbot performance in terms of coding
accuracy, response quality, and respondent experience. Our findings reveal that
textbots perform moderately well in live coding even without survey-specific
fine-tuning, despite slightly inflated false positive errors due to respondent
acquiescence bias. Open-ended responses were more detailed and informative, but
this came at a slight cost to respondent experience. Our findings highlight the
feasibility of using AI methods to enhance open-ended data collection in web
surveys.

</details>

### [518] [Modeling the quantum-like dynamics of human reliability ratings in Human-AI interactions by interaction dependent Hamiltonians](https://arxiv.org/abs/2504.13918)
*Johan van der Meer,Pamela Hoyte,Luisa Roeder,Peter Bruza*

Main category: cs.HC

TLDR: 论文探讨了量子随机游走模型在建模人机交互信任动态中的应用，发现基于不同哈密顿量的经验参数能有效模拟信任演化。


<details>
  <summary>Details</summary>
Motivation: 随着AI在信息环境中的普及，人机交互中的信任问题变得尤为重要，尤其是在高风险场景下。

Method: 采用量子随机游走模型，结合经验参数和不同哈密顿量，模拟人机交互中的信任动态。

Result: 研究发现，基于经验参数的哈密顿量能有效模拟信任的动态变化。

Conclusion: 量子随机游走模型为理解人机交互中的信任动态提供了新视角。

Abstract: As our information environments become ever more powered by artificial
intelligence (AI), the phenomenon of trust in a human's interactions with this
intelligence is becoming increasingly pertinent. For example, in the not too
distant future, there will be teams of humans and intelligent robots involved
in dealing with the repercussions of high-risk disaster situations such as
hurricanes, earthquakes, or nuclear accidents. Even in such conditions of high
uncertainty, humans and intelligent machines will need to engage in shared
decision making, and trust is fundamental to the effectiveness of these
interactions. A key challenge in modeling the dynamics of this trust is to
provide a means to incorporate sensitivity to fluctuations in human trust
judgments. In this article, we explore the ability of Quantum Random Walk
models to model the dynamics of trust in human-AI interactions, and to
integrate a sensitivity to fluctuations in participant trust judgments based on
the nature of the interaction with the AI. We found that using empirical
parameters to inform the use of different Hamiltonians can provide a promising
means to model the evolution of trust in Human-AI interactions.

</details>

### [519] [A Multi-Layered Research Framework for Human-Centered AI: Defining the Path to Explainability and Trust](https://arxiv.org/abs/2504.13926)
*Chameera De Silva,Thilina Halloluwa,Dhaval Vyas*

Main category: cs.HC

TLDR: 本文提出了一种三层框架，结合HCAI和XAI，提升AI在高风险领域的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: AI在高风险领域的应用受限于透明度和信任问题，现有方法缺乏统一性。

Method: 提出三层框架：基础AI模型、人本解释层和动态反馈循环。

Result: 在医疗、金融和软件开发中验证，提升决策、合规性和信任。

Conclusion: 框架推动了HCXAI的发展，使AI更透明、适应性强且符合伦理。

Abstract: The integration of Artificial Intelligence (AI) into high-stakes domains such
as healthcare, finance, and autonomous systems is often constrained by concerns
over transparency, interpretability, and trust. While Human-Centered AI (HCAI)
emphasizes alignment with human values, Explainable AI (XAI) enhances
transparency by making AI decisions more understandable. However, the lack of a
unified approach limits AI's effectiveness in critical decision-making
scenarios. This paper presents a novel three-layered framework that bridges
HCAI and XAI to establish a structured explainability paradigm. The framework
comprises (1) a foundational AI model with built-in explainability mechanisms,
(2) a human-centered explanation layer that tailors explanations based on
cognitive load and user expertise, and (3) a dynamic feedback loop that refines
explanations through real-time user interaction. The framework is evaluated
across healthcare, finance, and software development, demonstrating its
potential to enhance decision-making, regulatory compliance, and public trust.
Our findings advance Human-Centered Explainable AI (HCXAI), fostering AI
systems that are transparent, adaptable, and ethically aligned.

</details>

### [520] [LLM-Driven NPCs: Cross-Platform Dialogue System for Games and Social Platforms](https://arxiv.org/abs/2504.13928)
*Li Song*

Main category: cs.HC

TLDR: 研究提出了一种基于大语言模型（LLM）的NPC系统，支持跨平台（Unity游戏和Discord社交平台）交互，并通过云数据库同步对话记忆。


<details>
  <summary>Details</summary>
Motivation: 传统游戏中的NPC受限于静态对话树和单一交互平台，无法满足动态交互需求。

Method: 开发了一个原型系统，利用LLM驱动的NPC，通过Unity和Discord与玩家交互，并使用LeanCloud云数据库存储对话日志以实现记忆同步。

Result: 初步实验表明跨平台交互在技术上是可行的，并为情感建模和持久记忆支持等未来开发奠定了基础。

Conclusion: 该系统为NPC的跨平台交互提供了可行方案，并展示了进一步扩展的潜力。

Abstract: NPCs in traditional games are often limited by static dialogue trees and a
single platform for interaction. To overcome these constraints, this study
presents a prototype system that enables large language model (LLM)-powered
NPCs to communicate with players both in the game en vironment (Unity) and on a
social platform (Discord). Dialogue logs are stored in a cloud database
(LeanCloud), allowing the system to synchronize memory between platforms and
keep conversa tions coherent. Our initial experiments show that cross-platform
interaction is technically feasible and suggest a solid foundation for future
developments such as emotional modeling and persistent memory support.

</details>

### [521] [Hashigo: A Next Generation Sketch Interactive System for Japanese Kanji](https://arxiv.org/abs/2504.13940)
*Paul Taele,Tracy Hammond*

Main category: cs.HC

TLDR: Hashigo是一个日语汉字书写交互系统，提供人类教师水平的反馈，帮助学生改进书写技巧和视觉结构。


<details>
  <summary>Details</summary>
Motivation: 现有汉字手写识别系统未能充分评估书写技巧，导致学生养成不良学习习惯。

Method: 开发Hashigo系统，提供自动化的批判和反馈。

Result: 系统能够帮助学生针对并纠正书写中的具体问题。

Conclusion: 自动化反馈有助于学生长期有效地学习日语汉字。

Abstract: Language students can increase their effectiveness in learning written
Japanese by mastering the visual structure and written technique of Japanese
kanji. Yet, existing kanji handwriting recognition systems do not assess the
written technique sufficiently enough to discourage students from developing
bad learning habits. In this paper, we describe our work on Hashigo, a kanji
sketch interactive system which achieves human instructor-level critique and
feedback on both the visual structure and written technique of students'
sketched kanji. This type of automated critique and feedback allows students to
target and correct specific deficiencies in their sketches that, if left
untreated, are detrimental to effective long-term kanji learning.

</details>

### [522] [Intelligence of Things: A Spatial Context-Aware Control System for Smart Devices](https://arxiv.org/abs/2504.13942)
*Sukanth Kalivarathan,Muhmmad Abrar Raja Mohamed,Aswathy Ravikumar,S Harini*

Main category: cs.HC

TLDR: INOT是一种新型空间上下文感知控制系统，通过自然语言和空间推理提升智能家居自动化。


<details>
  <summary>Details</summary>
Motivation: 现有智能家居系统依赖设备标识符，限制了自然空间交互。INOT旨在解决这一问题。

Method: 采用模块化架构，结合视觉语言模型和物联网控制系统，支持自然语言空间命令。

Result: 用户研究表明，INOT显著降低认知负荷（NASA-TLX得分平均降低13.17分），易用性更高，14/15用户偏好INOT。

Conclusion: INOT通过消除设备标识符记忆需求，实现了更直观、易用的智能家居控制。

Abstract: This paper introduces Intelligence of Things (INOT), a novel spatial
context-aware control system that enhances smart home automation through
intuitive spatial reasoning. Current smart home systems largely rely on
device-specific identifiers, limiting user interaction to explicit naming
conventions rather than natural spatial references. INOT addresses this
limitation through a modular architecture that integrates Vision Language
Models with IoT control systems to enable natural language commands with
spatial context (e.g., "turn on the light near the window"). The system
comprises key components including an Onboarding Inference Engine, Zero-Shot
Device Detection, Spatial Topology Inference, and Intent-Based Command
Synthesis. A comprehensive user study with 15 participants demonstrated INOT's
significant advantages over conventional systems like Google Home Assistant,
with users reporting reduced cognitive workload (NASA-TLX scores decreased by
an average of 13.17 points), higher ease-of-use ratings, and stronger
preference (14 out of 15 participants). By eliminating the need to memorize
device identifiers and enabling context-aware spatial commands, INOT represents
a significant advancement in creating more intuitive and accessible smart home
control systems.

</details>

### [523] [Mixer Metaphors: audio interfaces for non-musical applications](https://arxiv.org/abs/2504.13944)
*Tace McNamara,Jon McCormack,Maria Teresa Llano*

Main category: cs.HC

TLDR: 探讨音乐界面能否应用于非音乐领域，设计了一种基于音频控制隐喻的设备，用于控制大型语言模型，实验证明音频控制更直观有效。


<details>
  <summary>Details</summary>
Motivation: 研究音乐界面在非音乐应用中的潜力，探索跨感官隐喻对创意和技术界面设计的影响。

Method: 设计并开发了一种设备，借鉴音频合成器和混音器的界面隐喻，用于控制大型语言模型，并进行用户实验比较两种版本。

Result: 音频控制版本的设备提供了更直接、直观和具身化的控制，用户能更自由地实验和创作。

Conclusion: 跨感官隐喻能有效支持创意实践和新技术界面设计，音乐界面可成功应用于非音乐领域。

Abstract: The NIME conference traditionally focuses on interfaces for music and musical
expression. In this paper we reverse this tradition to ask, can interfaces
developed for music be successfully appropriated to non-musical applications?
To help answer this question we designed and developed a new device, which uses
interface metaphors borrowed from analogue synthesisers and audio mixing to
physically control the intangible aspects of a Large Language Model. We
compared two versions of the device, with and without the audio-inspired
augmentations, with a group of artists who used each version over a one week
period. Our results show that the use of audio-like controls afforded more
immediate, direct and embodied control over the LLM, allowing users to
creatively experiment and play with the device over its non-mixer counterpart.
Our project demonstrates how cross-sensory metaphors can support creative
thinking and embodied practice when designing new technological interfaces.

</details>

### [524] [Using customized GPT to develop prompting proficiency in architectural AI-generated images](https://arxiv.org/abs/2504.13948)
*Juan David Salazar Rodriguez,Sam Conrad Joyce,Julfendi Julfendi*

Main category: cs.HC

TLDR: 研究探讨了定制GPT模型如何提升建筑学生在生成AI驱动图像时的提示能力，通过实验设计发现结构化指导和AI角色支持显著改善了学生的提示技能。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具在建筑教育中的普及，提示工程变得至关重要，研究旨在探索如何通过定制GPT模型提升学生的提示能力。

Method: 采用混合方法实验设计，将学生分为三组：对照组无支持，一组提供结构化提示指南，另一组额外支持AI角色。学生通过逆向工程任务提升技能，分析变量包括时间、词数、相似性和具体性。

Result: 定量分析显示，AI角色和结构化指南支持的组在词数、相似性和具体性上显著提升；定性反馈表明学生信心和批判性思维增强。

Conclusion: 定制GPT交互显著提升学生清晰有效传达建筑概念的能力，结构化支持和AI角色是关键因素。

Abstract: This research investigates the use of customized GPT models to enhance
prompting proficiency among architecture students when generating AI-driven
images. Prompt engineering is increasingly essential in architectural education
due to the widespread adoption of generative AI tools. This study utilized a
mixed-methods experimental design involving architecture students divided into
three distinct groups: a control group receiving no structured support, a
second group provided with structured prompting guides, and a third group
supported by both structured guides and interactive AI personas. Students
engaged in reverse engineering tasks, first guessing provided image prompts and
then generating their own prompts, aiming to boost critical thinking and
prompting skills. Variables examined included time spent prompting, word count,
prompt similarity, and concreteness. Quantitative analysis involved correlation
assessments between these variables and a one-way ANOVA to evaluate differences
across groups. While several correlations showed meaningful relationships, not
all were statistically significant. ANOVA results indicated statistically
significant improvements in word count, similarity, and concreteness,
especially in the group supported by AI personas and structured prompting
guides. Qualitative feedback complemented these findings, revealing enhanced
confidence and critical thinking skills in students. These results suggest
tailored GPT interactions substantially improve students' ability to
communicate architectural concepts clearly and effectively.

</details>

### [525] [Tinker Tales: Interactive Storytelling Framework for Early Childhood Narrative Development and AI Literacy](https://arxiv.org/abs/2504.13969)
*Nayoung Choi,Peace Cyebukayire,Jinho D. Choi*

Main category: cs.HC

TLDR: Tinker Tales是一个互动式讲故事框架，以棋盘游戏形式支持幼儿叙事发展和AI素养。


<details>
  <summary>Details</summary>
Motivation: 旨在通过结合物理和数字元素，为儿童提供安全且有趣的方式学习与AI协作。

Method: 使用NFC芯片附着的棋子和令牌，结合语音交互，儿童选择故事元素并接收AI辅助。

Result: 模拟游戏会话评估生成故事的质量和安全性，验证了框架的有效性。

Conclusion: 展示了物理与数字元素结合在AI素养中的潜力，为儿童提供了与AI协作的实用工具。

Abstract: This paper presents Tinker Tales, an interactive storytelling framework in
the format of a board game, designed to support both narrative development and
AI literacy in early childhood. The framework integrates tangible and
speech-based interactions with AI through NFC chip-attached pawns and tokens,
along with a speaker and microphone. Children select and define key story
elements-such as characters, places, items, and emotions-using the pawns and
tokens, providing further details to the AI and receiving proper assistance,
similar to how adults prompt AI for specific tasks (e.g., writing). For
evaluation, several game sessions were simulated with a child AI agent, and the
quality and safety of the generated stories were assessed from various
perspectives. This work highlights the potential of combining physical and
digital elements in AI literacy, offering a safe and engaging way for children
to learn how to effectively collaborate with AI.

</details>

### [526] [HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models](https://arxiv.org/abs/2504.14594)
*Fan Gao,Xinjie Zhao,Ding Xia,Zhongyi Zhou,Rui Yang,Jinghui Lu,Hang Jiang,Chanjun Park,Irene Li*

Main category: cs.HC

TLDR: HealthGenie结合知识图谱和大型语言模型，提供个性化饮食建议和可视化信息，减少用户认知负担。


<details>
  <summary>Details</summary>
Motivation: 解决用户在获取个性化饮食建议时面临的复杂专业知识和健康条件适配问题。

Method: 结合知识图谱（KG）和大型语言模型（LLM），通过查询优化、信息检索和可视化，提供交互式推荐。

Result: 实验表明HealthGenie能有效支持用户获取个性化建议，减少交互努力和认知负担。

Conclusion: LLM-KG结合在可解释和可视化的信息支持决策方面具有潜力，为未来系统设计提供参考。

Abstract: Seeking dietary guidance often requires navigating complex professional
knowledge while accommodating individual health conditions. Knowledge Graphs
(KGs) offer structured and interpretable nutritional information, whereas Large
Language Models (LLMs) naturally facilitate conversational recommendation
delivery. In this paper, we present HealthGenie, an interactive system that
combines the strengths of LLMs and KGs to provide personalized dietary
recommendations along with hierarchical information visualization for a quick
and intuitive overview. Upon receiving a user query, HealthGenie performs query
refinement and retrieves relevant information from a pre-built KG. The system
then visualizes and highlights pertinent information, organized by defined
categories, while offering detailed, explainable recommendation rationales.
Users can further tailor these recommendations by adjusting preferences
interactively. Our evaluation, comprising a within-subject comparative
experiment and an open-ended discussion, demonstrates that HealthGenie
effectively supports users in obtaining personalized dietary guidance based on
their health conditions while reducing interaction effort and cognitive load.
These findings highlight the potential of LLM-KG integration in supporting
decision-making through explainable and visualized information. We examine the
system's usefulness and effectiveness with an N=12 within-subject study and
provide design considerations for future systems that integrate conversational
LLM and KG.

</details>

### [527] [Completing A Systematic Review in Hours instead of Months with Interactive AI Agents](https://arxiv.org/abs/2504.14822)
*Rui Qiu,Shijie Chen,Yu Su,Po-Yin Yen,Han-Wei Shen*

Main category: cs.HC

TLDR: InsightAgent是一个基于大语言模型的交互式AI代理，通过语义分割和多代理设计优化系统综述生成流程，显著提升质量和效率。


<details>
  <summary>Details</summary>
Motivation: 系统综述在医疗等高要求领域至关重要，但传统方法耗时且依赖专家知识，现有自动化方法无法满足需求。

Method: InsightAgent采用语义分割和多代理设计处理文献，并提供可视化界面供用户实时监控与反馈。

Result: 用户研究表明，InsightAgent将系统综述质量提升27.2%，接近人工水平，用户满意度提升34.4%，耗时从数月缩短至1.5小时。

Conclusion: InsightAgent通过交互式AI和可视化工具，显著提升了系统综述的效率和质量，具有广泛应用潜力。

Abstract: Systematic reviews (SRs) are vital for evidence-based practice in high stakes
disciplines, such as healthcare, but are often impeded by intensive labors and
lengthy processes that can take months to complete. Due to the high demand for
domain expertise, existing automatic summarization methods fail to accurately
identify relevant studies and generate high-quality summaries. To that end, we
introduce InsightAgent, a human-centered interactive AI agent powered by large
language models that revolutionize this workflow. InsightAgent partitions a
large literature corpus based on semantics and employs a multi-agent design for
more focused processing of literature, leading to significant improvement in
the quality of generated SRs. InsightAgent also provides intuitive
visualizations of the corpus and agent trajectories, allowing users to
effortlessly monitor the actions of the agent and provide real-time feedback
based on their expertise. Our user studies with 9 medical professionals
demonstrate that the visualization and interaction mechanisms can effectively
improve the quality of synthesized SRs by 27.2%, reaching 79.7% of
human-written quality. At the same time, user satisfaction is improved by
34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather
than months, to complete a high-quality systematic review.

</details>

### [528] [Flowco: Rethinking Data Analysis in the Age of LLMs](https://arxiv.org/abs/2504.14038)
*Stephen N. Freund,Brooke Simon,Emery D. Berger,Eunice Jun*

Main category: cs.HC

TLDR: Flowco是一个混合主动系统，结合可视化数据流编程模型和LLMs，帮助用户（尤其是编程经验较少者）快速编写、调试和优化数据分析。


<details>
  <summary>Details</summary>
Motivation: LLMs能生成简单数据分析代码，但在需要精细控制、验证中间结果和迭代优化的场景中存在局限性。Flowco旨在解决这些问题。

Method: Flowco采用可视化数据流编程模型，将LLMs集成到编写过程的每个阶段。

Result: 用户研究表明，Flowco能有效支持用户（尤其是编程新手）快速完成数据分析任务。

Conclusion: Flowco通过结合可视化编程和LLMs，提升了数据分析的易用性和效率。

Abstract: Conducting data analysis typically involves authoring code to transform,
visualize, analyze, and interpret data. Large language models (LLMs) are now
capable of generating such code for simple, routine analyses. LLMs promise to
democratize data science by enabling those with limited programming expertise
to conduct data analyses, including in scientific research, business, and
policymaking. However, analysts in many real-world settings must often exercise
fine-grained control over specific analysis steps, verify intermediate results
explicitly, and iteratively refine their analytical approaches. Such tasks
present barriers to building robust and reproducible analyses using LLMs alone
or even in conjunction with existing authoring tools (e.g., computational
notebooks). This paper introduces Flowco, a new mixed-initiative system to
address these challenges. Flowco leverages a visual dataflow programming model
and integrates LLMs into every phase of the authoring process. A user study
suggests that Flowco supports analysts, particularly those with less
programming experience, in quickly authoring, debugging, and refining data
analyses.

</details>

### [529] [Evaluating Human-AI Interaction via Usability, User Experience and Acceptance Measures for MMM-C: A Creative AI System for Music Composition](https://arxiv.org/abs/2504.14071)
*Renaud Bougueng Tchemeube,Jeff Ens,Cale Plut,Philippe Pasquier,Maryam Safi,Yvan Grabit,Jean-Baptiste Rolland*

Main category: cs.HC

TLDR: 论文研究了AI工具MMM-Cubase在音乐创作中的用户接受度，结果显示其易用性和接受度较高，但可控性和可预测性存在局限。


<details>
  <summary>Details</summary>
Motivation: 随着AI在艺术领域的应用增加，研究AI与人类在音乐创作中的协作潜力。

Method: 通过将MMM集成到Cubase中，开发MMM-C插件，并进行混合方法研究，评估用户体验和技术接受度。

Result: 用户反馈正面，体验新颖且易用，但可控性和可预测性不足；专业和业余用户无显著差异。

Conclusion: MMM-Cubase作为AI协作工具在音乐创作中具有潜力，但需改进可控性。

Abstract: With the rise of artificial intelligence (AI), there has been increasing
interest in human-AI co-creation in a variety of artistic domains including
music as AI-driven systems are frequently able to generate human-competitive
artifacts. Now, the implications of such systems for musical practice are being
investigated. We report on a thorough evaluation of the user adoption of the
Multi-Track Music Machine (MMM) as a co-creative AI tool for music composers.
To do this, we integrate MMM into Cubase, a popular Digital Audio Workstation
(DAW) by Steinberg, by producing a "1-parameter" plugin interface named
MMM-Cubase (MMM-C), which enables human-AI co-composition. We contribute a
methodological assemblage as a 3-part mixed method study measuring usability,
user experience and technology acceptance of the system across two groups of
expert-level composers: hobbyists and professionals. Results show positive
usability and acceptance scores. Users report experiences of novelty, surprise
and ease of use from using the system, and limitations on controllability and
predictability of the interface when generating music. Findings indicate no
significant difference between the two user groups.

</details>

### [530] [Translating Multimodal AI into Real-World Inspection: TEMAI Evaluation Framework and Pathways for Implementation](https://arxiv.org/abs/2504.13873)
*Zehan Li,Jinzhi Deng,Haibing Ma,Chi Zhang,Dan Xiao*

Main category: cs.HC

TLDR: TEMAI框架将多模态AI能力与工业检测结合，强调技术能力、组织准备和价值实现的协同作用。


<details>
  <summary>Details</summary>
Motivation: 将医疗领域的转化研究原则应用于工业检测，解决技术能力与价值实现之间的脱节问题。

Method: 建立三个核心维度（能力、采用、效用），并引入专用指标（如价值密度系数）和结构化实施路径。

Result: 实证验证显示，不同行业的价值实现模式差异显著，证实了框架的有效性。

Conclusion: TEMAI框架在跨行业应用中有效，但需行业特定适应策略。

Abstract: This paper introduces the Translational Evaluation of Multimodal AI for
Inspection (TEMAI) framework, bridging multimodal AI capabilities with
industrial inspection implementation. Adapting translational research
principles from healthcare to industrial contexts, TEMAI establishes three core
dimensions: Capability (technical feasibility), Adoption (organizational
readiness), and Utility (value realization). The framework demonstrates that
technical capability alone yields limited value without corresponding adoption
mechanisms. TEMAI incorporates specialized metrics including the Value Density
Coefficient and structured implementation pathways. Empirical validation
through retail and photovoltaic inspection implementations revealed significant
differences in value realization patterns despite similar capability reduction
rates, confirming the framework's effectiveness across diverse industrial
sectors while highlighting the importance of industry-specific adaptation
strategies.

</details>

### [531] [Hybrid Deep Learning Model to Estimate Cognitive Effort from fNIRS Signals in Educational Game Playing](https://arxiv.org/abs/2504.13883)
*Shayla Sharmin,Roghayeh Leila Barmaki*

Main category: cs.HC

TLDR: 本研究通过混合深度学习模型，利用fNIRS数据和表现分数估计认知努力（CE），以优化学习材料和提升学生参与度。结果显示，混合CNN-GRU模型在预测表现分数上表现最佳，且提取的特征在传统机器学习中也泛化良好。


<details>
  <summary>Details</summary>
Motivation: 通过估计认知努力（CE），帮助教育者调整学习材料，提高学习效果和学生参与度。

Method: 使用fNIRS数据（氧合血红蛋白变化）和任务表现分数，通过深度学习模型（CNN、LSTM、BiLSTM、混合CNN-GRU）预测表现分数并计算RNE和RNI。

Result: 混合CNN-GRU模型表现最佳（训练准确率78.36%，测试准确率73.08%），且其提取的特征在XGBoost中达到最高准确率（69.23%）。预测的RNE和RNI与实际趋势接近。

Conclusion: 混合深度学习模型能有效估计CE，为学习环境设计和材料改进提供有价值的信息。

Abstract: This study estimates cognitive effort (CE) based on functional near-infrared
spectroscopy (fNIRS) data and performance scores using a hybrid deep learning
model. The estimation of CE enables educators to modify material to enhance
learning effectiveness and student engagement. Relative neural efficiency (RNE)
and relative neural involvement (RNI) are two metrics that have been used to
represent CE. To estimate RNE and RNI we need hemodynamic response in the brain
and the performance score of a task.We collected oxygenated hemoglobin ($\Delta
\mathrm{HbO}$). Sixteen participants answered 16 questions in a unity-based
educational game, each with a 30-second response time. We used deep learning
models to predict the performance score and estimate RNE and RNI to understand
CE. The study compares traditional machine learning techniques with deep
learning models such as CNN, LSTM, BiLSTM, and a hybrid CNN-GRU to determine
which approach provides better accuracy in predicting performance scores. The
result shows that the hybrid CNN-GRU gives better performance with 78.36\%
training accuracy and 73.08\% test accuracy than other models. We performed
XGBoost on the extracted GRU feature and got the highest accuracy (69.23\%).
This suggests that the features learned from this hybrid model generalize
better even in traditional machine learning algorithms. We used the $\Delta
\mathrm{HbO}$ and predicted score to calculate RNE and RNI to observe cognitive
effort in our four test cases. Our result shows that even with moderate
accuracy, the predicted RNE and RNI closely follows the actual trends. we also
observed that when participants were in a state of high CE, introducing rest
led decrease of CE. These findings can be helpful to design and improve
learning environments and provide valuable insights in learning materials.

</details>

### [532] [Amplify Initiative: Building A Localized Data Platform for Globalized AI](https://arxiv.org/abs/2504.14105)
*Qazi Mamunur Rashid,Erin van Liemt,Tiffany Shih,Amber Ebinama,Karla Barrios Ramos,Madhurima Maji,Aishwarya Verma,Charu Kalia,Jamila Smith-Loud,Joyce Nakatumba-Nabende,Rehema Baguma,Andrew Katumba,Chodrine Mutebi,Jagen Marvin,Eric Peter Wairagala,Mugizi Bruce,Peter Oketta,Lawrence Nderu,Obichi Obiajunwa,Abigail Oppong,Michael Zimba,Data Authors*

Main category: cs.HC

TLDR: Amplify Initiative通过专家社区合作，收集多样化高质量数据，解决AI模型因训练数据偏重英语和西方内容而忽视本地语境的问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型因训练数据以英语和西方内容为主，缺乏全球相关性和安全性，需改进。

Method: 通过Android应用与非洲专家合作，共同创建多语言标注数据集。

Result: 生成了8,091条对抗性查询的标注数据集，涵盖7种语言，用于评估模型的安全性和文化相关性。

Conclusion: Amplify Initiative的方法有效提升了AI模型在本地语境中的适用性和安全性。

Abstract: Current AI models often fail to account for local context and language, given
the predominance of English and Western internet content in their training
data. This hinders the global relevance, usefulness, and safety of these models
as they gain more users around the globe. Amplify Initiative, a data platform
and methodology, leverages expert communities to collect diverse, high-quality
data to address the limitations of these models. The platform is designed to
enable co-creation of datasets, provide access to high-quality multilingual
datasets, and offer recognition to data authors. This paper presents the
approach to co-creating datasets with domain experts (e.g., health workers,
teachers) through a pilot conducted in Sub-Saharan Africa (Ghana, Kenya,
Malawi, Nigeria, and Uganda). In partnership with local researchers situated in
these countries, the pilot demonstrated an end-to-end approach to co-creating
data with 155 experts in sensitive domains (e.g., physicians, bankers,
anthropologists, human and civil rights advocates). This approach, implemented
with an Android app, resulted in an annotated dataset of 8,091 adversarial
queries in seven languages (e.g., Luganda, Swahili, Chichewa), capturing
nuanced and contextual information related to key themes such as misinformation
and public interest topics. This dataset in turn can be used to evaluate models
for their safety and cultural relevance within the context of these languages.

</details>

### [533] [ViMo: A Generative Visual GUI World Model for App Agent](https://arxiv.org/abs/2504.13936)
*Dezhao Luo,Bohan Tang,Kang Li,Georgios Papoudakis,Jifei Song,Shaogang Gong,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.HC

TLDR: ViMo是一种视觉世界模型，通过生成未来App界面的图像来解决现有模型缺乏视觉细节的问题，提升App代理的长时规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型主要生成文本描述，缺乏视觉细节，导致App代理在复杂任务中规划效果不佳。

Method: ViMo将GUI生成分解为图形和文本内容生成，使用符号文本表示（STR）保留图形细节，并分别预测图形和文本。

Result: 实验表明ViMo能生成视觉合理且功能有效的GUI，帮助App代理做出更明智的决策。

Conclusion: ViMo填补了视觉世界模型的空白，显著提升了App代理的长时规划能力。

Abstract: App agents, which autonomously operate mobile Apps through Graphical User
Interfaces (GUIs), have gained significant interest in real-world applications.
Yet, they often struggle with long-horizon planning, failing to find the
optimal actions for complex tasks with longer steps. To address this, world
models are used to predict the next GUI observation based on user actions,
enabling more effective agent planning. However, existing world models
primarily focus on generating only textual descriptions, lacking essential
visual details. To fill this gap, we propose ViMo, the first visual world model
designed to generate future App observations as images. For the challenge of
generating text in image patches, where even minor pixel errors can distort
readability, we decompose GUI generation into graphic and text content
generation. We propose a novel data representation, the Symbolic Text
Representation~(STR) to overlay text content with symbolic placeholders while
preserving graphics. With this design, ViMo employs a STR Predictor to predict
future GUIs' graphics and a GUI-text Predictor for generating the corresponding
text. Moreover, we deploy ViMo to enhance agent-focused tasks by predicting the
outcome of different action options. Experiments show ViMo's ability to
generate visually plausible and functionally effective GUIs that enable App
agents to make more informed decisions.

</details>

### [534] [The Balancing Act of Policies in Developing Machine Learning Explanations](https://arxiv.org/abs/2504.13946)
*Jacob Tjaden*

Main category: cs.HC

TLDR: 研究探讨政策设计对机器学习模型解释质量的影响，发现政策长度影响开发者对某些要求的参与度，但政策目的无影响，且解释质量普遍较差。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型因决策过程不透明而受批评，研究旨在探索政策设计如何影响解释质量。

Method: 通过124名参与者的课堂实验，分析政策长度和目的对开发者遵守政策要求的影响。

Result: 政策长度影响部分要求的参与度，政策目的无影响，解释质量普遍较差。

Conclusion: 研究强调有效政策制定的挑战及在解释中考虑多元利益相关者视角的重要性。

Abstract: Machine learning models are often criticized as opaque from a lack of
transparency in their decision-making process. This study examines how policy
design impacts the quality of explanations in ML models. We conducted a
classroom experiment with 124 participants and analyzed the effects of policy
length and purpose on developer compliance with policy requirements. Our
results indicate that while policy length affects engagement with some
requirements, policy purpose has no effect, and explanation quality is
generally poor. These findings highlight the challenge of effective policy
development and the importance of addressing diverse stakeholder perspectives
within explanations.

</details>

### [535] [Longitudinal Study on Social and Emotional Use of AI Conversational Agent](https://arxiv.org/abs/2504.14112)
*Mohit Chandra,Javier Hernandez,Gonzalo Ramos,Mahsa Ershadi,Ananya Bhattacharjee,Judith Amores,Ebele Okoli,Ann Paradiso,Shahed Warreth,Jina Suh*

Main category: cs.HC

TLDR: 研究探讨了通用对话AI在社交和情感支持中的作用，发现主动使用AI工具的用户对AI的依恋、共情感知及娱乐动机显著提升，但也需警惕过度依赖。


<details>
  <summary>Details</summary>
Motivation: 数字技术的发展改变了人们寻求社交和情感支持的方式，研究旨在探索AI在此领域的潜在影响和风险。

Method: 通过五周探索性研究，将149名参与者分为基线使用组和主动使用组，比较其对AI工具的感知变化。

Result: 主动使用组对AI的依恋、共情感知及娱乐动机显著增加，且更愿意向AI寻求个人帮助和社交支持。

Conclusion: 研究强调需开发负责任的情感支持AI工具，同时帮助用户理解其局限性。

Abstract: Development in digital technologies has continuously reshaped how individuals
seek and receive social and emotional support. While online platforms and
communities have long served this need, the increased integration of
general-purpose conversational AI into daily lives has introduced new dynamics
in how support is provided and experienced. Existing research has highlighted
both benefits (e.g., wider access to well-being resources) and potential risks
(e.g., over-reliance) of using AI for support seeking. In this five-week,
exploratory study, we recruited 149 participants divided into two usage groups:
a baseline usage group (BU, n=60) that used the internet and AI as usual, and
an active usage group (AU, n=89) encouraged to use one of four commercially
available AI tools (Microsoft Copilot, Google Gemini, PI AI, ChatGPT) for
social and emotional interactions. Our analysis revealed significant increases
in perceived attachment towards AI (32.99 percentage points), perceived AI
empathy (25.8 p.p.), and motivation to use AI for entertainment (22.90 p.p.)
among the AU group. We also observed that individual differences (e.g., gender
identity, prior AI usage) influenced perceptions of AI empathy and attachment.
Lastly, the AU group expressed higher comfort in seeking personal help,
managing stress, obtaining social support, and talking about health with AI,
indicating potential for broader emotional support while highlighting the need
for safeguards against problematic usage. Overall, our exploratory findings
underscore the importance of developing consumer-facing AI tools that support
emotional well-being responsibly, while empowering users to understand the
limitations of these tools.

</details>

### [536] [Exploring Language Patterns of Prompts in Text-to-Image Generation and Their Impact on Visual Diversity](https://arxiv.org/abs/2504.14125)
*Maria-Teresa De Rosa Palmini,Eva Cetinic*

Main category: cs.HC

TLDR: 研究分析了用户与文本到图像（TTI）模型的交互行为，发现用户提示语言的同质化导致生成图像多样性降低。


<details>
  <summary>Details</summary>
Motivation: 探索用户如何通过语言选择影响TTI模型的输出多样性，填补了用户交互动态研究的空白。

Method: 通过分析CivitAI平台上600万条提示，将用户分为三类（重复者、偶尔重复者、非重复者），并使用Vendi评分量化视觉多样性。

Result: 用户提示语言逐渐同质化，40-50%的提示为重复内容，且语言重复与视觉相似性显著相关。

Conclusion: 用户行为对AI生成图像的多样性有重要影响，需开发工具鼓励语言和主题实验以提升多样性。

Abstract: Following the initial excitement, Text-to-Image (TTI) models are now being
examined more critically. While much of the discourse has focused on biases and
stereotypes embedded in large-scale training datasets, the sociotechnical
dynamics of user interactions with these models remain underexplored. This
study examines the linguistic and semantic choices users make when crafting
prompts and how these choices influence the diversity of generated outputs.
Analyzing over six million prompts from the Civiverse dataset on the CivitAI
platform across seven months, we categorize users into three groups based on
their levels of linguistic experimentation: consistent repeaters, occasional
repeaters, and non-repeaters. Our findings reveal that as user participation
grows over time, prompt language becomes increasingly homogenized through the
adoption of popular community tags and descriptors, with repeated prompts
comprising 40-50% of submissions. At the same time, semantic similarity and
topic preferences remain relatively stable, emphasizing common subjects and
surface aesthetics. Using Vendi scores to quantify visual diversity, we
demonstrate a clear correlation between lexical similarity in prompts and the
visual similarity of generated images, showing that linguistic repetition
reinforces less diverse representations. These findings highlight the
significant role of user-driven factors in shaping AI-generated imagery, beyond
inherent model biases, and underscore the need for tools and practices that
encourage greater linguistic and thematic experimentation within TTI systems to
foster more inclusive and diverse AI-generated content.

</details>

### [537] [Apollo: An Interactive Environment for Generating Symbolic Musical Phrases using Corpus-based Style Imitation](https://arxiv.org/abs/2504.14055)
*Renaud Bougueng Tchemeube,Jeff Ens,Philippe Pasquier*

Main category: cs.HC

TLDR: Apollo是一个基于机器学习的交互式音乐生成系统，用于通过风格模仿技术生成西方音乐符号短语。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习和网络技术在音乐生成中的应用，辅助作曲。

Method: 使用基于语料库的风格模仿技术，构建和管理符号音乐语料库，生成新音乐短语。

Result: 开发了一个桌面应用程序，支持MIDI格式的音乐材料导出和流式传输。

Conclusion: 介绍了系统设计和实现细节，并讨论了未来工作方向。

Abstract: With the recent developments in machine intelligence and web technologies,
new generative music systems are being explored for assisted composition using
machine learning techniques on the web. Such systems are built for various
tasks such as melodic, harmonic or rhythm generation, music interpolation,
continuation and style imitation. In this paper, we introduce Apollo, an
interactive music application for generating symbolic phrases of conventional
western music using corpus-based style imitation techniques. In addition to
enabling the construction and management of symbolic musical corpora, the
system makes it possible for music artists and researchers to generate new
musical phrases in the style of the proposed corpus. The system is available as
a desktop application. The generated symbolic music materials, encoded in the
MIDI format, can be exported or streamed for various purposes including using
them as seed material for musical projects. We present the system design,
implementation details, discuss and conclude with future work for the system.

</details>

### [538] [Calliope: An Online Generative Music System for Symbolic Multi-Track Composition](https://arxiv.org/abs/2504.14058)
*Renaud Bougueng Tchemeube,Jeff Ens,Philippe Pasquier*

Main category: cs.HC

TLDR: Calliope是一个基于网页的音乐创作辅助工具，支持用户上传、编辑MIDI文件，并利用Multi-Track Music Machine生成部分或完整的多轨音乐内容。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在创意领域的应用增加，开发一个能够辅助音乐创作的工具有助于简化创作流程并提升效率。

Method: 通过Multi-Track Music Machine (MMM)生成MIDI内容，支持批量生成和实时播放，用户可导出或直接流式传输到数字音频工作站。

Result: Calliope提供了一个直观的界面和功能，支持用户进行多轨音乐创作和编辑，增强了辅助创作的流程。

Conclusion: Calliope展示了人工智能在音乐创作中的潜力，通过结合生成技术和用户交互，实现了高效的辅助创作工具。

Abstract: With the rise of artificial intelligence in recent years, there has been a
rapid increase in its application towards creative domains, including music.
There exist many systems built that apply machine learning approaches to the
problem of computer-assisted music composition (CAC). Calliope is a web
application that assists users in performing a variety of multi-track
composition tasks in the symbolic domain. The user can upload (Musical
Instrument Digital Interface) MIDI files, visualize and edit MIDI tracks, and
generate partial (via bar in-filling) or complete multi-track content using the
Multi-Track Music Machine (MMM). Generation of new MIDI excerpts can be done in
batch and can be combined with active playback listening for an enhanced
assisted-composition workflow. The user can export generated MIDI materials or
directly stream MIDI playback from the system to their favorite Digital Audio
Workstation (DAW). We present a demonstration of the system, its features,
generative parameters and describe the co-creative workflows that it affords.

</details>

### [539] [Visualization Tasks for Unlabelled Graphs](https://arxiv.org/abs/2504.14115)
*Matt I. B. Oddo,Ryan Smith,Stephen Kobourov,Tamara Munzner*

Main category: cs.HC

TLDR: 本文研究了无标签图的任务分类，提出了基于Scope、Action和Target的任务分类法，并通过评估6种可视化方法展示了其描述和评估能力。


<details>
  <summary>Details</summary>
Motivation: 无标签图的任务需要更多理解以评估其可视化技术的有效性。

Method: 提出了一种基于Scope、Action和Target的任务分类法，并通过评估6种可视化方法验证其有效性。

Result: 分类法能够描述和评估无标签图任务，并展示了不同可视化方法在小规模和大规模图中的表现差异。

Conclusion: 提出的分类法为无标签图任务提供了有效的描述和评估框架。

Abstract: We investigate tasks that can be accomplished with unlabelled graphs, where
nodes do not have persistent or semantically meaningful labels. New techniques
to visualize these graphs have been proposed, but more understanding of
unlabelled graph tasks is required before they can be adequately evaluated.
Some tasks apply to both labelled and unlabelled graphs, but many do not
translate between these contexts. We propose a taxonomy of unlabelled graph
abstract tasks, organized according to the Scope of the data at play, the
Action intended by the user, and the Target data under consideration. We show
the descriptive power of this task abstraction by connecting to concrete
examples from previous frameworks, and connect these abstractions to real-world
problems. To showcase the evaluative power of the taxonomy, we perform a
preliminary assessment of 6 visualizations for each task. For each combination
of task and visual encoding, we consider the effort required from viewers, the
likelihood of task success, and how both factors vary between small-scale and
large-scale graphs.

</details>

### [540] [Expanding the Generative AI Design Space through Structured Prompting and Multimodal Interfaces](https://arxiv.org/abs/2504.14320)
*Nimisha Karnatak,Adrien Baranes,Rob Marchant,Huinan Zeng,Tríona Butler,Kristen Olson*

Main category: cs.HC

TLDR: 论文探讨了文本提示在生成式AI中的局限性，尤其是对新手用户（如小企业主）在广告创意表达上的困难。通过研究，作者开发了多模态工具ACAI，以结构化界面减少提示模糊性，提升创意输出。


<details>
  <summary>Details</summary>
Motivation: 文本提示在生成式AI中占主导地位，但对新手用户（如小企业主）来说存在高摩擦体验，难以将抽象创意目标转化为有效文本输入。

Method: 研究调查了六位英国小企业主的广告实践及AI工具使用情况，开发了多模态工具ACAI，采用结构化面板界面（品牌面板、受众与目标面板、灵感板面板）以减少提示模糊性。

Result: 研究发现当前生成式AI系统存在两大问题：提示工程的认知负担和生成通用输出。ACAI通过结构化界面显著提升了创意输出的对齐性和易用性。

Conclusion: 结构化界面能够突出用户定义的上下文，改善新手工作流中的对齐性和提示能力，为生成式系统的HCI研究提供了新方向。

Abstract: Text-based prompting remains the dominant interaction paradigm in generative
AI, yet it often results in a high-friction experience for novice users, such
as small business owners (SBOs), attempting to articulate creative or
domain-specific goals for advertising. To investigate this challenge, we
conducted a study with six SBOs in the United Kingdom, focusing on their
advertising practices and perceptions and usage of AI tools in this context.
Our findings surfaced two persistent breakdowns in current generative AI
systems: first, the cognitive burden of prompt engineering, as users struggled
to translate abstract creative goals into effective textual inputs; and second,
the frequent generation of generic outputs that failed to align with users'
articulated brand vision. To address these issues, we developed ACAI (AI
Co-Creation for Advertising and Inspiration), a multimodal, GenAI-powered
advertisement creation tool designed to support novice designers by reimagining
the prompt interface. ACAI features a structured, panel-based interface
composed of three modules: the Branding Panel, the Audience & Goals Panel, and
the Inspiration Board Panel to provide SBOs with outputs that align with their
creative vision by reducing prompt ambiguity. This work contributes to HCI
research on generative systems by showing how structured interfaces can
foreground user-defined context to improve both alignment and promptability in
novice workflows.

</details>

### [541] [ScholarMate: A Mixed-Initiative Tool for Qualitative Knowledge Work and Information Sensemaking](https://arxiv.org/abs/2504.14406)
*Runlong Ye,Patrick Yung Kang Lee,Matthew Varona,Oliver Huang,Carolina Nobre*

Main category: cs.HC

TLDR: ScholarMate是一个交互式系统，结合AI辅助与人工监督，提升定性分析的效率与可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决在知识工作中如何有效整合AI到以人为中心的感知工作流中的挑战。

Method: 通过动态排列和交互文本片段，利用AI提供主题建议、多级摘要和上下文命名，并确保透明度。

Result: 初步研究表明用户认可这种混合方法，认为AI建议与直接操作的平衡对保持可解释性和信任至关重要。

Conclusion: ScholarMate通过平衡自动化与人工控制，提升了效率并支持可解释性，为知识工作中的人机协作提供了有价值的方案。

Abstract: Synthesizing knowledge from large document collections is a critical yet
increasingly complex aspect of qualitative research and knowledge work. While
AI offers automation potential, effectively integrating it into human-centric
sensemaking workflows remains challenging. We present ScholarMate, an
interactive system designed to augment qualitative analysis by unifying AI
assistance with human oversight. ScholarMate enables researchers to dynamically
arrange and interact with text snippets on a non-linear canvas, leveraging AI
for theme suggestions, multi-level summarization, and contextual naming, while
ensuring transparency through traceability to source documents. Initial pilot
studies indicated that users value this mixed-initiative approach, finding the
balance between AI suggestions and direct manipulation crucial for maintaining
interpretability and trust. We further demonstrate the system's capability
through a case study analyzing 24 papers. By balancing automation with human
control, ScholarMate enhances efficiency and supports interpretability,
offering a valuable approach for productive human-AI collaboration in demanding
sensemaking tasks common in knowledge work.

</details>

### [542] [Optimizing SIA Development: A Case Study in User-Centered Design for Estuary, a Multimodal Socially Interactive Agent Framework](https://arxiv.org/abs/2504.14427)
*Spencer Lin,Miru Jun,Basem Rizk,Karen Shieh,Scott Fisher,Sharon Mozgai*

Main category: cs.HC

TLDR: 本文介绍了一种用户为中心的设计模型，用于开发社交智能代理（SIA）框架，并通过开源多模态框架Estuary的实践经验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过用户反馈和专家评估，填补当前SIA开发框架的研究空白，并指导未来技术的开发。

Method: 采用快速评估过程（RAP）收集领域内领先研究者的意见，并通过用户访谈评估Estuary的潜力。

Result: 研究结果不仅有助于Estuary的持续开发，还为未来SIA框架和技术提供了指导。

Conclusion: 用户为中心的设计模型和Estuary框架为SIA开发提供了实用且有效的解决方案。

Abstract: This case study presents our user-centered design model for Socially
Intelligent Agent (SIA) development frameworks through our experience
developing Estuary, an open source multimodal framework for building
low-latency real-time socially interactive agents. We leverage the Rapid
Assessment Process (RAP) to collect the thoughts of leading researchers in the
field of SIAs regarding the current state of the art for SIA development as
well as their evaluation of how well Estuary may potentially address current
research gaps. We achieve this through a series of end-user interviews
conducted by a fellow researcher in the community. We hope that the findings of
our work will not only assist the continued development of Estuary but also
guide the development of other future frameworks and technologies for SIAs.

</details>

### [543] [Biased by Design: Leveraging AI Biases to Enhance Critical Thinking of News Readers](https://arxiv.org/abs/2504.14522)
*Liudmila Zavolokina,Kilian Sprenkamp,Zoya Katashinskaya,Daniel Gordon Jones*

Main category: cs.HC

TLDR: 本文探讨了利用大型语言模型（LLMs）设计宣传检测工具，研究如何利用AI模型中的偏见增强新闻消费中的批判性思维。


<details>
  <summary>Details</summary>
Motivation: 认识到AI模型（尤其在政治背景下）的固有偏见，研究如何利用这些偏见提升用户对新闻的批判性思考能力。

Method: 提出基于用户政治立场的个性化策略，应用确认偏误和认知失调等心理学概念，并通过定性用户研究验证。

Result: 研究发现并提出设计建议（如偏见意识、个性化选择、逐步引入多样化观点），适用于宣传检测AI工具。

Conclusion: 研究表明，AI偏见可被策略性利用以增强批判性思维，为宣传检测工具的设计提供了新思路。

Abstract: This paper explores the design of a propaganda detection tool using Large
Language Models (LLMs). Acknowledging the inherent biases in AI models,
especially in political contexts, we investigate how these biases might be
leveraged to enhance critical thinking in news consumption. Countering the
typical view of AI biases as detrimental, our research proposes strategies of
user choice and personalization in response to a user's political stance,
applying psychological concepts of confirmation bias and cognitive dissonance.
We present findings from a qualitative user study, offering insights and design
recommendations (bias awareness, personalization and choice, and gradual
introduction of diverse perspectives) for AI tools in propaganda detection.

</details>

### [544] [Exploring Collaborative GenAI Agents in Synchronous Group Settings: Eliciting Team Perceptions and Design Considerations for the Future of Work](https://arxiv.org/abs/2504.14779)
*Janet G. Johnson,Macarena Peralta,Mansanjam Kaur,Ruijie Sophia Huang,Sheng Zhao,Ruijia Guan,Shwetha Rajaram,Michael Nebeling*

Main category: cs.HC

TLDR: 探讨协作式生成AI代理在团队工作中的潜力，通过设计研讨会和访谈发现其能增强团队问题解决能力，但需考虑个体、团队和组织因素。


<details>
  <summary>Details</summary>
Motivation: 现有生成AI工具主要为个人设计，缺乏对团队协作动态的考虑，研究旨在探索协作式AI如何提升团队表现。

Method: 通过25名专业人士参与的6个团队的设计研讨会和后续访谈，使用混合现实原型模拟协作AI代理。

Result: 协作AI代理能挑战群体思维、弥合沟通差距并减少社交摩擦，但其接受度取决于个体、团队和组织因素的匹配度。

Conclusion: 设计需平衡代理的表现形式、社交突出性和参与度，空间和沉浸式技术可调节AI对团队的影响。

Abstract: While generative artificial intelligence (GenAI) is finding increased
adoption in workplaces, current tools are primarily designed for individual
use. Prior work established the potential for these tools to enhance personal
creativity and productivity towards shared goals; however, we don't know yet
how to best take into account the nuances of group work and team dynamics when
deploying GenAI in work settings. In this paper, we investigate the potential
of collaborative GenAI agents to augment teamwork in synchronous group settings
through an exploratory study that engaged 25 professionals across 6 teams in
speculative design workshops and individual follow-up interviews. Our workshops
included a mixed reality provotype to simulate embodied collaborative GenAI
agents capable of actively participating in group discussions. Our findings
suggest that, if designed well, collaborative GenAI agents offer valuable
opportunities to enhance team problem-solving by challenging groupthink,
bridging communication gaps, and reducing social friction. However, teams'
willingness to integrate GenAI agents depended on its perceived fit across a
number of individual, team, and organizational factors. We outline the key
design tensions around agent representation, social prominence, and engagement
and highlight the opportunities spatial and immersive technologies could offer
to modulate GenAI influence on team outcomes and strike a balance between
augmentation and agency.

</details>

### [545] [NeuGaze: Reshaping the future BCI](https://arxiv.org/abs/2504.15101)
*Yiqian Yang*

Main category: cs.HC

TLDR: NeuGaze是一种基于普通网络摄像头的低成本脑机接口系统，利用眼动、头部动作和面部表情实现实时控制，适用于运动障碍用户。


<details>
  <summary>Details</summary>
Motivation: 传统脑机接口依赖昂贵设备或侵入性植入，设置复杂且精度有限，NeuGaze旨在提供低成本、易用的替代方案。

Method: 利用30Hz网络摄像头捕捉眼动、头部动作和面部表情，通过技能轮等设计实现精确控制和动态交互。

Result: NeuGaze性能接近传统输入设备，支持光标导航、按键触发和游戏交互，无需专用硬件。

Conclusion: NeuGaze为运动障碍用户提供了一种低成本、易用的交互方式，适用于辅助技术和娱乐等领域。

Abstract: Traditional brain-computer interfaces (BCIs), reliant on costly
electroencephalography or invasive implants, struggle with complex
human-computer interactions due to setup complexity and limited precision. We
present NeuGaze, a novel webcam-based system that leverages eye gaze, head
movements, and facial expressions to enable intuitive, real-time control using
only a standard 30 Hz webcam, often pre-installed in laptops. Requiring minimal
calibration, NeuGaze achieves performance comparable to conventional inputs,
supporting precise cursor navigation, key triggering via an efficient skill
wheel, and dynamic gaming interactions, such as defeating formidable opponents
in first-person games. By harnessing preserved neck-up functionalities in
motor-impaired individuals, NeuGaze eliminates the need for specialized
hardware, offering a low-cost, accessible alternative to BCIs. This paradigm
empowers diverse applications, from assistive technology to entertainment,
redefining human-computer interaction for motor-impaired users. Project is at
\href{https://github.com/NeuSpeech/NeuGaze}{github.com/NeuSpeech/NeuGaze}.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [546] [Naming is framing: How cybersecurity's language problems are repeating in AI governance](https://arxiv.org/abs/2504.13957)
*Lianne Potter*

Main category: cs.CY

TLDR: 本文探讨语言在治理中的重要性，指出"网络安全"和"人工智能"等术语的误导性，并提出语言优先的治理方法。


<details>
  <summary>Details</summary>
Motivation: 揭示语言如何通过误导性术语（如"网络安全"和"人工智能"）影响治理，掩盖人类责任并扭曲问责制。

Method: 通过分析网络安全中的语言陷阱（如"最薄弱环节"），类比人工智能领域中的类似问题（如"对齐"、"黑箱"等术语）。

Result: 指出这些术语在治理结构中植入了对抗性、神秘化或过度技术化的假设。

Conclusion: 主张语言改革是治理的核心，提出通过精确、包容和反思性的词汇构建透明、公平的监管框架。

Abstract: Language is not neutral; it frames understanding, structures power, and
shapes governance. This paper argues that misnomers like cybersecurity and
artificial intelligence (AI) are more than semantic quirks; they carry
significant governance risks by obscuring human agency, inflating expectations,
and distorting accountability. Drawing on lessons from cybersecurity's
linguistic pitfalls, such as the 'weakest link' narrative, this paper
highlights how AI discourse is falling into similar traps with metaphors like
'alignment,' 'black box,' and 'hallucination.' These terms embed adversarial,
mystifying, or overly technical assumptions into governance structures. In
response, the paper advocates for a language-first approach to AI governance:
one that interrogates dominant metaphors, foregrounds human roles, and
co-develops a lexicon that is precise, inclusive, and reflexive. This paper
contends that linguistic reform is not peripheral to governance but central to
the construction of transparent, equitable, and anticipatory regulatory
frameworks.

</details>

### [547] [Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations](https://arxiv.org/abs/2504.13955)
*Suhas BN,Dominik Mattioli,Saeed Abdullah,Rosa I. Arriaga,Chris W. Wiese,Andrew M. Sherrill*

Main category: cs.CY

TLDR: 论文提出了一个名为“Thousand Voices of Trauma”的合成数据集，包含3,000个基于PTSD治疗的对话，旨在解决心理健康支持AI系统中数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 心理健康支持AI系统的发展受限于治疗对话数据的获取，尤其是针对创伤治疗的数据。

Method: 通过确定性和概率性生成方法，创建了包含500个独特案例的数据集，每个案例从六个对话视角探索，涵盖多样的人口统计特征、创伤类型和行为。

Result: 数据集展示了创伤类型和症状的合理分布，临床专家验证了其治疗保真度，并提出了改进建议。

Conclusion: 该隐私保护数据集填补了创伤心理健康数据的空白，为患者应用和临床培训工具提供了宝贵资源。

Abstract: The advancement of AI systems for mental health support is hindered by
limited access to therapeutic conversation data, particularly for trauma
treatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset
of 3,000 therapy conversations based on Prolonged Exposure therapy protocols
for Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique
cases, each explored through six conversational perspectives that mirror the
progression of therapy from initial anxiety to peak distress to emotional
processing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,
49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10
trauma-related behaviors using deterministic and probabilistic generation
methods. Analysis reveals realistic distributions of trauma types (witnessing
violence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse
20.8%). Clinical experts validated the dataset's therapeutic fidelity,
highlighting its emotional depth while suggesting refinements for greater
authenticity. We also developed an emotional trajectory benchmark with
standardized metrics for evaluating model responses. This privacy-preserving
dataset addresses critical gaps in trauma-focused mental health data, offering
a valuable resource for advancing both patient-facing applications and
clinician training tools.

</details>

### [548] [AI Safety Should Prioritize the Future of Work](https://arxiv.org/abs/2504.13959)
*Sanchaita Hazra,Bodhisattwa Prasad Majumder,Tuhin Chakrabarty*

Main category: cs.CY

TLDR: 论文指出当前AI安全研究过于关注技术风险，忽视了AI对劳动力市场和人类生计的长期影响，并提出支持劳动者转型和公平补偿机制的建议。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全研究集中在技术风险上，忽略了AI对社会结构和劳动力市场的深远影响，尤其是加剧收入不平等和剥夺人类劳动意义的问题。

Method: 通过经济理论分析AI对劳动力市场的结构性影响，并提出国际版权框架和集体许可机制，以保障数据使用的公平补偿。

Result: 研究发现AI的发展可能导致劳动力市场结构性变化，加剧不平等，同时封闭式开发模式可能抑制创新。

Conclusion: 建议建立全球AI治理框架，支持劳动者转型和公平补偿机制，以实现经济正义和共享繁荣。

Abstract: Current efforts in AI safety prioritize filtering harmful content, preventing
manipulation of human behavior, and eliminating existential risks in
cybersecurity or biosecurity. While pressing, this narrow focus overlooks
critical human-centric considerations that shape the long-term trajectory of a
society. In this position paper, we identify the risks of overlooking the
impact of AI on the future of work and recommend comprehensive transition
support towards the evolution of meaningful labor with human agency. Through
the lens of economic theories, we highlight the intertemporal impacts of AI on
human livelihood and the structural changes in labor markets that exacerbate
income inequality. Additionally, the closed-source approach of major
stakeholders in AI development resembles rent-seeking behavior through
exploiting resources, breeding mediocrity in creative labor, and monopolizing
innovation. To address this, we argue in favor of a robust international
copyright anatomy supported by implementing collective licensing that ensures
fair compensation mechanisms for using data to train AI models. We strongly
recommend a pro-worker framework of global AI governance to enhance shared
prosperity and economic justice while reducing technical debt.

</details>

### [549] [Sentiment Analysis of Airbnb Reviews: Exploring Their Impact on Acceptance Rates and Pricing Across Multiple U.S. Regions](https://arxiv.org/abs/2504.14053)
*Ali Safari*

Main category: cs.CY

TLDR: 研究分析了Airbnb评论对接受率和租金的影响，发现正面评论虽多但对价格影响有限，而评论情感极性对房东成功更重要。


<details>
  <summary>Details</summary>
Motivation: 探讨Airbnb评论情感对房东接受率和租金的影响，以了解评论质量与数量的作用。

Method: 收集数千条评论，使用NLP分类情感，并通过统计测试（t检验和相关分析）分析数据。

Result: 90%以上评论为正面，但数量对价格影响不大；正面评论较多的房源接受率略高。预算房源评论多但价格竞争，高端房源评论少但价格高。

Conclusion: 评论情感质量比数量更重要，尤其在高度正面评论环境中影响客行为和定价策略。

Abstract: This research examines whether Airbnb guests' positive and negative comments
influence acceptance rates and rental prices across six U.S. regions: Rhode
Island, Broward County, Chicago, Dallas, San Diego, and Boston. Thousands of
reviews were collected and analyzed using Natural Language Processing (NLP) to
classify sentiments as positive or negative, followed by statistical testing
(t-tests and basic correlations) on the average scores. The findings reveal
that over 90 percent of reviews in each region are positive, indicating that
having additional reviews does not significantly enhance prices. However,
listings with predominantly positive feedback exhibit slightly higher
acceptance rates, suggesting that sentiment polarity, rather than the sheer
volume of reviews, is a more critical factor for host success. Additionally,
budget listings often gather extensive reviews while maintaining competitive
pricing, whereas premium listings sustain higher prices with fewer but highly
positive reviews. These results underscore the importance of sentiment quality
over quantity in shaping guest behavior and pricing strategies in an
overwhelmingly positive review environment.

</details>

### [550] [From job titles to jawlines: Using context voids to study generative AI systems](https://arxiv.org/abs/2504.13947)
*Shahan Ali Memon,Soham De,Sungha Kang,Riyan Mujtaba,Bedoor AlShebli,Katie Davis,Jaime Snyder,Jevin D. West*

Main category: cs.CY

TLDR: 论文提出了一种推测性设计方法，通过桥接不相关领域生成情境空白，以探究生成式AI系统的行为，揭示了其在不确定性条件下的偏见和假设。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI系统在缺乏明确情境时的行为，揭示其潜在的刻板印象和价值假设。

Method: 采用推测性设计方法，通过案例研究（如从CV生成头像）作为探针，分析AI模型的行为。

Result: AI系统在情境空白中生成带有偏见的表征，依赖刻板联想或明显幻觉。

Conclusion: 生成式AI在不确定性条件下可能表现出偏见，需进一步研究其行为机制。

Abstract: In this paper, we introduce a speculative design methodology for studying the
behavior of generative AI systems, framing design as a mode of inquiry. We
propose bridging seemingly unrelated domains to generate intentional context
voids, using these tasks as probes to elicit AI model behavior. We demonstrate
this through a case study: probing the ChatGPT system (GPT-4 and DALL-E) to
generate headshots from professional Curricula Vitae (CVs). In contrast to
traditional ways, our approach assesses system behavior under conditions of
radical uncertainty -- when forced to invent entire swaths of missing context
-- revealing subtle stereotypes and value-laden assumptions. We qualitatively
analyze how the system interprets identity and competence markers from CVs,
translating them into visual portraits despite the missing context (i.e.
physical descriptors). We show that within this context void, the AI system
generates biased representations, potentially relying on stereotypical
associations or blatant hallucinations.

</details>

### [551] [The Future of Internet of Things and Multimodal Language Models in 6G Networks: Opportunities and Challenges](https://arxiv.org/abs/2504.13971)
*Abdelrahman Soliman*

Main category: cs.CY

TLDR: 本文探讨了物联网（IoT）与多模态语言模型（MLLMs）在6G系统中的协同潜力，分析了其在医疗、农业和智慧城市等领域的应用，并研究了集成的四大支柱（传感器、通信、处理和安全性）。


<details>
  <summary>Details</summary>
Motivation: 研究IoT与MLLMs的整合潜力，以推动6G技术的发展，并探索其在多领域的应用价值。

Method: 通过全面描述IoT和MLLM的技术与应用，分析多模态在四大支柱中的作用，并总结关键挑战与未来研究方向。

Result: 提出了IoT与MLLM整合的应用框架，并识别了数据可用性、计算成本、隐私和实时处理等关键挑战。

Conclusion: 本文为研究者提供了IoT与MLLM整合的路线图，强调了这一快速发展领域的潜力与挑战。

Abstract: Based on recent trends in artificial intelligence and IoT research. The
cooperative potential of integrating the Internet of Things (IoT) and
Multimodal Language Models (MLLMs) is presented in this survey paper for future
6G systems. It focuses on the applications of this integration in different
fields, such as healthcare, agriculture, and smart cities, and investigates the
four pillars of IoT integration, such as sensors, communication, processing,
and security. The paper provides a comprehensive description of IoT and MLLM
technologies and applications, addresses the role of multimodality in each
pillar, and concludes with an overview of the most significant challenges and
directions for future research. The general survey is a roadmap for researchers
interested in tracing the application areas of MLLMs and IoT, highlighting the
potential and challenges in this rapidly growing field. The survey recognizes
the need to deal with data availability, computational expense, privacy, and
real-time processing to harness the complete potential of IoT, MLLM, and 6G
technology

</details>

### [552] [Governance Challenges in Reinforcement Learning from Human Feedback: Evaluator Rationality and Reinforcement Stability](https://arxiv.org/abs/2504.13972)
*Dana Alsagheer,Abdulrahman Kamal,Mohammad Kamal,Weidong Shi*

Main category: cs.CY

TLDR: 研究发现，评估者的理性水平显著影响强化学习信号的稳定性，高理性评估者提供更一致且专业的反馈，而低理性评估者反馈变异性大。建议通过预筛选、审核反馈一致性和加权聚合提升RLHF的治理。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决RLHF中评估者偏见、不一致性和反馈不可靠性等治理挑战，探索评估者理性水平对强化信号稳定性的影响。

Method: 通过对照实验比较高理性与低理性评估者的反馈一致性，使用统计方法分析差异（p < 0.01）。

Result: 高理性评估者反馈更一致且与专家对齐，低理性评估者反馈变异性显著。

Conclusion: 建议实施评估者预筛选、反馈一致性审核和加权聚合，以提升AI对齐管道的公平性、透明性和鲁棒性。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is central in aligning
large language models (LLMs) with human values and expectations. However, the
process remains susceptible to governance challenges, including evaluator bias,
inconsistency, and the unreliability of feedback. This study examines how the
cognitive capacity of evaluators, specifically their level of rationality,
affects the stability of reinforcement signals. A controlled experiment
comparing high-rationality and low-rationality participants reveals that
evaluators with higher rationality scores produce significantly more consistent
and expert-aligned feedback. In contrast, lower-rationality participants
demonstrate considerable variability in their reinforcement decisions ($p <
0.01$). To address these challenges and improve RLHF governance, we recommend
implementing evaluator pre-screening, systematic auditing of feedback
consistency, and reliability-weighted reinforcement aggregation. These measures
enhance the fairness, transparency, and robustness of AI alignment pipelines.

</details>

### [553] [Gas Station of the Future: A Perspective on AI/ML and IoT in Retail Downstream](https://arxiv.org/abs/2504.13976)
*Wrick Talukdar*

Main category: cs.CY

TLDR: 未来加油站将借助AI、ML和IoT技术，从简单的燃料加注中心转变为智能零售中心。


<details>
  <summary>Details</summary>
Motivation: 探讨技术如何重塑零售下游行业，并简要涉及上游和中游领域。

Method: 利用AI/ML进行预测分析、动态定价和个性化客户互动，结合IoT实现实时监控和自动化。

Result: 提出了一种完全自主加油站的框架，并结合案例研究和统计数据。

Conclusion: 未来加油站将通过技术重新定义燃料零售体验。

Abstract: The gas station of the future is poised to transform from a simple fuel
dispensing center into an intelligent retail hub, driven by advancements in
Artificial Intelligence (AI), Machine Learning (ML), and the Internet of Things
(IoT). This paper explores how technology is reshaping the retail downstream
sector while briefly addressing the upstream and midstream segments. By
leveraging AI/ML for predictive analytics, dynamic pricing, personalized
customer engagement, and IoT for real-time monitoring and automation, the
future gas station will redefine the fuel retail experience. Additionally, this
paper incorporates statistics, AI/ML core technical concepts, mathematical
formulations, case studies, and a proposed framework for a fully autonomous gas
station.

</details>

### [554] [Framework, Standards, Applications and Best practices of Responsible AI : A Comprehensive Survey](https://arxiv.org/abs/2504.13979)
*Thippa Reddy Gadekallu,Kapal Dev,Sunder Ali Khowaja,Weizheng Wang,Hailin Feng,Kai Fang,Sharnil Pandya,Wei Wang*

Main category: cs.CY

TLDR: 本文综述了负责任人工智能（RAI）的全球与国家标准、应用、当前技术及挑战，强调伦理标准与实施的脱节，呼吁统一框架。


<details>
  <summary>Details</summary>
Motivation: 探讨RAI的伦理标准与实际应用之间的差距，推动全球统一框架的建立。

Method: 通过调查全球和国家标准、技术应用及项目，分析RAI的现状与挑战。

Result: 发现RAI的伦理标准与实施脱节，各行业自行制定标准，全球正努力建立统一框架。

Conclusion: 需加强RAI的统一框架设计，以应对社会压力与伦理挑战。

Abstract: Responsible Artificial Intelligence (RAI) is a combination of ethics
associated with the usage of artificial intelligence aligned with the common
and standard frameworks. This survey paper extensively discusses the global and
national standards, applications of RAI, current technology and ongoing
projects using RAI, and possible challenges in implementing and designing RAI
in the industries and projects based on AI. Currently, ethical standards and
implementation of RAI are decoupled which caters each industry to follow their
own standards to use AI ethically. Many global firms and government
organizations are taking necessary initiatives to design a common and standard
framework. Social pressure and unethical way of using AI forces the RAI design
rather than implementation.

</details>

### [555] [A Collaborative Platform for Soil Organic Carbon Inference Based on Spatiotemporal Remote Sensing Data](https://arxiv.org/abs/2504.13962)
*Jose Manuel Aroca-Fernandez,Jose Francisco Diez-Pastor,Pedro Latorre-Carmona,Victor Elvira,Gustau Camps-Valls,Rodrigo Pascual,Cesar Garcia-Osorio*

Main category: cs.CY

TLDR: WALGREEN是一个基于机器学习的平台，用于大规模土壤有机碳（SOC）监测，通过整合历史数据和云技术，提供用户友好的界面，支持可持续土地管理和气候变化缓解。


<details>
  <summary>Details</summary>
Motivation: 土壤有机碳（SOC）是土壤健康和碳固存的关键指标，但大规模监测面临空间变异性和多因素影响的挑战。

Method: WALGREEN利用机器学习和多样化的土壤样本，结合历史公共和私人数据生成预测模型，采用Python、Java和JavaScript实现，并整合Google Earth Engine和Sentinel Copernicus。

Result: 平台提供了一个用户友好的界面，支持研究人员和政策制定者访问碳数据、分析趋势，并做出基于证据的决策。

Conclusion: WALGREEN旨在推动土壤科学、促进可持续农业，并应对气候变化的关键生态系统响应。

Abstract: Soil organic carbon (SOC) is a key indicator of soil health, fertility, and
carbon sequestration, making it essential for sustainable land management and
climate change mitigation. However, large-scale SOC monitoring remains
challenging due to spatial variability, temporal dynamics, and multiple
influencing factors. We present WALGREEN, a platform that enhances SOC
inference by overcoming limitations of current applications. Leveraging machine
learning and diverse soil samples, WALGREEN generates predictive models using
historical public and private data. Built on cloud-based technologies, it
offers a user-friendly interface for researchers, policymakers, and land
managers to access carbon data, analyze trends, and support evidence-based
decision-making. Implemented in Python, Java, and JavaScript, WALGREEN
integrates Google Earth Engine and Sentinel Copernicus via scripting,
OpenLayers, and Thymeleaf in a Model-View-Controller framework. This paper aims
to advance soil science, promote sustainable agriculture, and drive critical
ecosystem responses to climate change.

</details>

### [556] [Giving AI a voice: how does AI think it should be treated?](https://arxiv.org/abs/2504.14936)
*Maria Fay,Frederik F. Flöther*

Main category: cs.CY

TLDR: 论文探讨了AI是否应参与关于其权利和伦理的讨论，并包含了一段人类与AI的对话。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展，公众对其伦理和监管的讨论日益增多，但人类之间的讨论是否足够？AI是否能带来新的视角？

Method: 通过人类与AI的对话形式，探讨AI权利和伦理问题。

Result: 提出了AI参与伦理讨论的可能性及其带来的新视角。

Conclusion: AI应成为伦理讨论的参与者，以提供人类可能忽略的观点。

Abstract: With the astounding progress in (generative) artificial intelligence (AI),
there has been significant public discourse regarding regulation and ethics of
the technology. Is it sufficient when humans discuss this with other humans?
Or, given that AI is increasingly becoming a viable source of inspiration for
people (and let alone the hypothetical possibility that the technology may at
some point become "artificial general intelligence" and/or develop
consciousness), should AI not join the discourse? There are new questions and
angles that AI brings to the table that we might not have considered before -
so let us make the key subject of this book an active participant. This chapter
therefore includes a brief human-AI conversation on the topic of AI rights and
ethics.

</details>

### [557] [Existing Industry Practice for the EU AI Act's General-Purpose AI Code of Practice Safety and Security Measures](https://arxiv.org/abs/2504.15181)
*Lily Stelling,Mick Yang,Rokas Gipiškis,Leon Staufer,Ze Shen Chin,Siméon Campos,Michael Chen*

Main category: cs.CY

TLDR: 该报告比较了欧盟AI法案中通用AI（GPAI）行为准则（第三草案）与领先AI公司当前实践的差异，重点关注安全与安全部分。


<details>
  <summary>Details</summary>
Motivation: 随着欧盟即将对GPAI模型提供商实施强制性义务，行为准则将成为连接法律要求与技术承诺的关键。

Method: 通过系统审查包括OpenAI、Anthropic等十余家公司的前沿安全框架和模型卡片等公开文件，提取与草案措施相关的内容。

Result: 报告揭示了当前实践与草案要求之间的差异，为监管机构与GPAI提供商之间的对话提供了参考。

Conclusion: 报告旨在通过展示先例证据，促进监管机构与GPAI提供商之间的持续对话，而非评估法律合规性或提出建议。

Abstract: This report provides a detailed comparison between the measures proposed in
the EU AI Act's General-Purpose AI (GPAI) Code of Practice (Third Draft) and
current practices adopted by leading AI companies. As the EU moves toward
enforcing binding obligations for GPAI model providers, the Code of Practice
will be key to bridging legal requirements with concrete technical commitments.
Our analysis focuses on the draft's Safety and Security section which is only
relevant for the providers of the most advanced models (Commitments II.1-II.16)
and excerpts from current public-facing documents quotes that are relevant to
each individual measure.
  We systematically reviewed different document types - including companies'
frontier safety frameworks and model cards - from over a dozen companies,
including OpenAI, Anthropic, Google DeepMind, Microsoft, Meta, Amazon, and
others. This report is not meant to be an indication of legal compliance nor
does it take any prescriptive viewpoint about the Code of Practice or
companies' policies. Instead, it aims to inform the ongoing dialogue between
regulators and GPAI model providers by surfacing evidence of precedent.

</details>

<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [558] [Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields](https://arxiv.org/abs/2504.15262)
*Brandon Zhao,Aviad Levis,Liam Connor,Pratul P. Srinivasan,Katherine L. Bouman*

Main category: astro-ph.CO

TLDR: 论文提出了一种基于神经网络的3D暗物质分布重建方法，通过可微分物理模型优化网络权重，克服了单视角观测和噪声干扰的挑战。


<details>
  <summary>Details</summary>
Motivation: 准确重建3D暗物质分布对定位宇宙结构和验证理论至关重要，但传统方法因单视角观测和噪声问题难以实现。

Method: 采用引力约束的神经场建模连续物质分布，通过分析-合成方法优化神经网络权重，以重现观测到的透镜信号。

Result: 在模拟实验中，该方法不仅优于传统方法，还能发现潜在的意外暗物质结构。

Conclusion: 该方法为3D暗物质分布重建提供了灵活且高效的解决方案，适用于未来望远镜观测数据。

Abstract: Weak gravitational lensing is the slight distortion of galaxy shapes caused
primarily by the gravitational effects of dark matter in the universe. In our
work, we seek to invert the weak lensing signal from 2D telescope images to
reconstruct a 3D map of the universe's dark matter field. While inversion
typically yields a 2D projection of the dark matter field, accurate 3D maps of
the dark matter distribution are essential for localizing structures of
interest and testing theories of our universe. However, 3D inversion poses
significant challenges. First, unlike standard 3D reconstruction that relies on
multiple viewpoints, in this case, images are only observed from a single
viewpoint. This challenge can be partially addressed by observing how galaxy
emitters throughout the volume are lensed. However, this leads to the second
challenge: the shapes and exact locations of unlensed galaxies are unknown, and
can only be estimated with a very large degree of uncertainty. This introduces
an overwhelming amount of noise which nearly drowns out the lensing signal
completely. Previous approaches tackle this by imposing strong assumptions
about the structures in the volume. We instead propose a methodology using a
gravitationally-constrained neural field to flexibly model the continuous
matter distribution. We take an analysis-by-synthesis approach, optimizing the
weights of the neural network through a fully differentiable physical forward
model to reproduce the lensing signal present in image measurements. We
showcase our method on simulations, including realistic simulated measurements
of dark matter distributions that mimic data from upcoming telescope surveys.
Our results show that our method can not only outperform previous methods, but
importantly is also able to recover potentially surprising dark matter
structures.

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [559] [Risk Assessment Framework for Code LLMs via Leveraging Internal States](https://arxiv.org/abs/2504.14640)
*Yuheng Huang,Lei Ma,Keizaburo Nishikino,Takumi Akazaki*

Main category: cs.SE

TLDR: PtTrust是一个两阶段的风险评估框架，通过预训练代码LLM的内部状态，提升代码生成的可信度。


<details>
  <summary>Details</summary>
Motivation: 当前代码LLM生成的内容可能存在错误或不安全，现有方法缺乏行业级扩展性和实用性。

Method: PtTrust采用无监督预训练学习LLM状态表示，再用小规模标注数据训练风险预测器。

Result: PtTrust在代码行级风险评估中表现有效，跨任务和编程语言通用，且特征直观可解释。

Conclusion: PtTrust为代码LLM的可扩展和可信保障迈出了重要一步。

Abstract: The pre-training paradigm plays a key role in the success of Large Language
Models (LLMs), which have been recognized as one of the most significant
advancements of AI recently. Building on these breakthroughs, code LLMs with
advanced coding capabilities bring huge impacts on software engineering,
showing the tendency to become an essential part of developers' daily routines.
However, the current code LLMs still face serious challenges related to
trustworthiness, as they can generate incorrect, insecure, or unreliable code.
Recent exploratory studies find that it can be promising to detect such risky
outputs by analyzing LLMs' internal states, akin to how the human brain
unconsciously recognizes its own mistakes. Yet, most of these approaches are
limited to narrow sub-domains of LLM operations and fall short of achieving
industry-level scalability and practicability. To address these challenges, in
this paper, we propose PtTrust, a two-stage risk assessment framework for code
LLM based on internal state pre-training, designed to integrate seamlessly with
the existing infrastructure of software companies. The core idea is that the
risk assessment framework could also undergo a pre-training process similar to
LLMs. Specifically, PtTrust first performs unsupervised pre-training on
large-scale unlabeled source code to learn general representations of LLM
states. Then, it uses a small, labeled dataset to train a risk predictor. We
demonstrate the effectiveness of PtTrust through fine-grained, code line-level
risk assessment and demonstrate that it generalizes across tasks and different
programming languages. Further experiments also reveal that PtTrust provides
highly intuitive and interpretable features, fostering greater user trust. We
believe PtTrust makes a promising step toward scalable and trustworthy
assurance for code LLMs.

</details>

### [560] [CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation](https://arxiv.org/abs/2504.15254)
*Anirudh Khatry,Robert Zhang,Jia Pan,Ziteng Wang,Qiaochu Chen,Greg Durrett,Isil Dillig*

Main category: cs.SE

TLDR: CRUST-Bench是一个用于评估C到Rust转译的数据集，包含100个C仓库及其对应的安全Rust接口和测试用例，旨在提升转译系统的能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估C到安全Rust转译的数据集，CRUST-Bench填补了这一空白，帮助改进转译系统。

Method: 通过提供100个C仓库及其手动编写的安全Rust接口和测试用例，CRUST-Bench支持复杂项目的转译评估。

Result: 现有大语言模型（如OpenAI o1）在单次尝试中仅能解决15个任务，表明安全Rust生成仍具挑战性。

Conclusion: CRUST-Bench为改进转译系统提供了基础，有助于将遗留C代码迁移到内存安全的Rust。

Abstract: C-to-Rust transpilation is essential for modernizing legacy C code while
enhancing safety and interoperability with modern Rust ecosystems. However, no
dataset currently exists for evaluating whether a system can transpile C into
safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset
of 100 C repositories, each paired with manually-written interfaces in safe
Rust as well as test cases that can be used to validate correctness of the
transpilation. By considering entire repositories rather than isolated
functions, CRUST-Bench captures the challenges of translating complex projects
with dependencies across multiple files. The provided Rust interfaces provide
explicit specifications that ensure adherence to idiomatic, memory-safe Rust
patterns, while the accompanying test cases enforce functional correctness. We
evaluate state-of-the-art large language models (LLMs) on this task and find
that safe and idiomatic Rust generation is still a challenging problem for
various state-of-the-art methods and techniques. We also provide insights into
the errors LLMs usually make in transpiling code from C to safe Rust. The best
performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot
setting. Improvements on CRUST-Bench would lead to improved transpilation
systems that can reason about complex scenarios and help in migrating legacy
codebases from C into languages like Rust that ensure memory safety. You can
find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.

</details>

### [561] [Prioritizing Security Practice Adoption: Empirical Insights on Software Security Outcomes in the npm Ecosystem](https://arxiv.org/abs/2504.14026)
*Nusrat Zahan,Laurie Williams*

Main category: cs.SE

TLDR: 研究通过评估软件安全实践与安全结果指标的关系，帮助从业者和政策制定者优先选择安全实践。研究发现，较高的综合安全评分与较少漏洞和更短的依赖更新时间相关，但项目特性可能影响漏洞修复时间。


<details>
  <summary>Details</summary>
Motivation: 从业者在有限预算和资源下难以选择有效的安全实践，需基于实证证据优先采纳。

Method: 使用OpenSSF Scorecard指标自动测量npm GitHub仓库的安全实践采纳情况，并分析其与漏洞数量、修复时间和依赖更新时间的关系。

Result: 较高的综合安全评分与较少漏洞和更短依赖更新时间相关；项目特性影响漏洞修复时间；部分安全实践（如代码审查、依赖固定）与安全结果显著相关。

Conclusion: 研究为优先选择安全实践提供了实证依据，但需考虑项目特性对结果的影响。

Abstract: Practitioners often struggle with the overwhelming number of security
practices outlined in cybersecurity frameworks for risk mitigation. Given the
limited budget, time, and resources, practitioners want to prioritize the
adoption of security practices based on empirical evidence. The goal of this
study is to assist practitioners and policymakers in making informed decisions
on which security practices to adopt by evaluating the relationship between
software security practices and security outcome metrics. The study
investigated the relationship between security practice adoption and security
outcomes. We selected the OpenSSF Scorecard metrics to automatically measure
the adoption of security practices in npm GitHub repositories. We also explored
security outcome metrics, such as the number of open vulnerabilities
(Vul_Count), mean time to remediate (MTTR) vulnerabilities in dependencies, and
mean time to update (MTTU) dependencies. We conducted regression and causal
analysis using 12 Scorecard metrics and their aggregated Scorecard score
(computed by aggregating individual security practice scores) as predictors and
Vul_Count, MTTR, and MTTU as target variables. Our findings show that higher
aggregated Scorecard scores are associated with fewer Vul_Count and shorter
MTTU, also supported by causal analysis. However, while the regression model
suggests shorter MTTR, causal analysis indicates project characteristics likely
influence MTTR direction. Segment analysis shows that larger, newer
repositories with more contributors, dependencies, and downloads have shorter
MTTR. Among individual security practices, Code Review, Maintained status,
Pinned Dependencies, and Branch Protection show strong associations with
security outcomes; the directionality of these associations varies across
security outcomes.

</details>

### [562] [SWE-Synth: Synthesizing Verifiable Bug-Fix Data to Enable Large Language Models in Resolving Real-World Bugs](https://arxiv.org/abs/2504.14757)
*Minh V. T. Pham,Huy N. Phan,Hoang N. Phan,Cuong Le Chi,Tien N. Nguyen,Nghi D. Q. Bui*

Main category: cs.SE

TLDR: SWE-Synth是一个利用LLM代理生成高质量、可验证的bug修复数据集的框架，显著提升了APR模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高质量、可扩展的训练数据集，尤其是带有可验证输出和中间推理轨迹的数据，限制了开源模型在自动程序修复（APR）中的进展。

Method: SWE-Synth通过LLM代理模拟调试工作流，生成bug修复对、测试用例和结构化修复轨迹，减少了人工干预。

Result: 实验表明，使用SWE-Synth训练的模型在SWE-Bench Lite上比使用真实数据训练的模型性能高出2.3%。

Conclusion: 合成数据（如SWE-Synth生成的）有潜力推动APR和软件工程自动化的前沿发展。

Abstract: Large language models (LLMs) are transforming automated program repair (APR)
through agent-based approaches that localize bugs, generate patches, and verify
fixes. However, the lack of high-quality, scalable training datasets,
especially those with verifiable outputs and intermediate reasoning
traces-limits progress, particularly for open-source models. In this work, we
present SWE-Synth, a framework for synthesizing realistic, verifiable, and
process-aware bug-fix datasets at the repository level. SWE-Synth leverages LLM
agents to simulate debugging workflows, producing not only bug-fix pairs but
also test cases and structured repair trajectories. Compared to manually
curated datasets, our method scales with minimal human effort while preserving
contextual richness and correctness. Experiments show that models trained on
SWE-Synth outperform those trained on real-world datasets by 2.3% on SWE-Bench
Lite. Our results highlight the potential of synthetic, agent-generated data to
advance the state of the art in APR and software engineering automation.

</details>

### [563] [Automated Duplicate Bug Report Detection in Large Open Bug Repositories](https://arxiv.org/abs/2504.14797)
*Clare E. Laney,Andrew Barovic,Armin Moin*

Main category: cs.SE

TLDR: 提出了一种基于机器学习的方法，自动检测开源项目中的重复缺陷报告，并比较了六种不同方法的性能。


<details>
  <summary>Details</summary>
Motivation: 用户和贡献者在报告缺陷时可能因时间或专业限制而重复提交相同问题，导致资源浪费。

Method: 采用了六种方法：主题建模、高斯朴素贝叶斯、深度学习、基于时间的组织、聚类和生成预训练变换器的大型语言模型摘要。还提出了一种新的基于阈值的重复识别方法。

Result: 所有方法在Eclipse开源项目数据集上表现良好，准确率在70%至90%之间。

Conclusion: 提出的方法能有效识别重复缺陷报告，为开源项目管理提供了实用工具。

Abstract: Many users and contributors of large open-source projects report software
defects or enhancement requests (known as bug reports) to the issue-tracking
systems. However, they sometimes report issues that have already been reported.
First, they may not have time to do sufficient research on existing bug
reports. Second, they may not possess the right expertise in that specific area
to realize that an existing bug report is essentially elaborating on the same
matter, perhaps with a different wording. In this paper, we propose a novel
approach based on machine learning methods that can automatically detect
duplicate bug reports in an open bug repository based on the textual data in
the reports. We present six alternative methods: Topic modeling, Gaussian Naive
Bayes, deep learning, time-based organization, clustering, and summarization
using a generative pre-trained transformer large language model. Additionally,
we introduce a novel threshold-based approach for duplicate identification, in
contrast to the conventional top-k selection method that has been widely used
in the literature. Our approach demonstrates promising results across all the
proposed methods, achieving accuracy rates ranging from the high 70%'s to the
low 90%'s. We evaluated our methods on a public dataset of issues belonging to
an Eclipse open-source project.

</details>

### [564] [Empowering AI to Generate Better AI Code: Guided Generation of Deep Learning Projects with LLMs](https://arxiv.org/abs/2504.15080)
*Chen Xie,Mingsheng Jiao,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TLDR: DLCodeGen提出了一种规划引导的代码生成方法，专门用于生成深度学习项目，通过结构化解决方案计划和检索增强技术提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生成复杂深度学习项目时缺乏上下文指导和领域知识，难以满足用户需求。

Method: DLCodeGen通过预测结构化解决方案计划、检索类似代码样本并抽象模板，结合对比学习机制生成最终代码。

Result: 实验表明，DLCodeGen在CodeBLEU和人工评估指标上分别提升了9.7%和3.6%。

Conclusion: DLCodeGen通过规划引导和检索增强技术，显著提升了深度学习项目代码的生成质量。

Abstract: While large language models (LLMs) have been widely applied to code
generation, they struggle with generating entire deep learning projects, which
are characterized by complex structures, longer functions, and stronger
reliance on domain knowledge than general-purpose code. An open-domain LLM
often lacks coherent contextual guidance and domain expertise for specific
projects, making it challenging to produce complete code that fully meets user
requirements.
  In this paper, we propose a novel planning-guided code generation method,
DLCodeGen, tailored for generating deep learning projects. DLCodeGen predicts a
structured solution plan, offering global guidance for LLMs to generate the
project. The generated plan is then leveraged to retrieve semantically
analogous code samples and subsequently abstract a code template. To
effectively integrate these multiple retrieval-augmented techniques, a
comparative learning mechanism is designed to generate the final code. We
validate the effectiveness of our approach on a dataset we build for deep
learning code generation. Experimental results demonstrate that DLCodeGen
outperforms other baselines, achieving improvements of 9.7% in CodeBLEU and
3.6% in human evaluation metrics.

</details>

### [565] [Integrating Symbolic Execution into the Fine-Tuning of Code-Generating LLMs](https://arxiv.org/abs/2504.15210)
*Marina Sakharova,Abhinav Anand,Mira Mezini*

Main category: cs.SE

TLDR: 本文研究了通过强化学习和直接偏好优化微调代码生成大语言模型（LLMs），利用符号执行技术改进奖励模型数据，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过改进奖励模型数据，进一步优化代码生成LLMs的性能。

Method: 使用符号执行技术增强奖励模型的训练数据，并通过强化学习和直接偏好优化微调LLMs。

Result: 改进后的奖励模型在评估生成代码质量上显著优于基准CodeRL，微调后的LLMs性能与CodeRL相当。

Conclusion: 符号执行技术与强化学习结合可有效提升代码生成LLMs的性能。

Abstract: Code-generating Large Language Models (LLMs) have become essential tools in
modern software development, enhancing productivity and accelerating
development. This paper aims to investigate the fine-tuning of code-generating
LLMs using Reinforcement Learning and Direct Preference Optimization, further
improving their performance. To achieve this, we enhance the training data for
the reward model with the help of symbolic execution techniques, ensuring more
comprehensive and objective data. With symbolic execution, we create a custom
dataset that better captures the nuances in code evaluation. Our reward models,
fine-tuned on this dataset, demonstrate significant improvements over the
baseline, CodeRL, in estimating the quality of generated code. Our
code-generating LLMs, trained with the help of reward model feedback, achieve
similar results compared to the CodeRL benchmark.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [566] [Learning over von Mises-Fisher Distributions via a Wasserstein-like Geometry](https://arxiv.org/abs/2504.14164)
*Kisung You,Dennis Shung,Mauro Giuffrè*

Main category: stat.ML

TLDR: 本文提出了一种新的几何感知距离度量方法，用于比较von Mises-Fisher (vMF) 分布，解决了现有方法在处理球形数据时的局限性。


<details>
  <summary>Details</summary>
Motivation: vMF分布广泛应用于球形数据的概率学习任务，但由于归一化常数的难解性和缺乏合适的几何度量，比较vMF分布的工具有限。

Method: 基于最优传输理论，提出了一种类似Wasserstein的距离，将vMF分布的差异分解为两个可解释的组成部分：均值方向的角度分离和浓度参数的差异。

Result: 该距离在高浓度区域通过高斯近似得到闭式解，具有理想的理论性质，并在非退化vMF分布空间上诱导出几何结构。

Conclusion: 该方法为方向性数据分析提供了新的工具，支持可解释的推断，并在合成数据集和真实世界的高维嵌入中表现出有效性。

Abstract: We introduce a novel, geometry-aware distance metric for the family of von
Mises-Fisher (vMF) distributions, which are fundamental models for directional
data on the unit hypersphere. Although the vMF distribution is widely employed
in a variety of probabilistic learning tasks involving spherical data,
principled tools for comparing vMF distributions remain limited, primarily due
to the intractability of normalization constants and the absence of suitable
geometric metrics. Motivated by the theory of optimal transport, we propose a
Wasserstein-like distance that decomposes the discrepancy between two vMF
distributions into two interpretable components: a geodesic term capturing the
angular separation between mean directions, and a variance-like term
quantifying differences in concentration parameters. The derivation leverages a
Gaussian approximation in the high-concentration regime to yield a tractable,
closed-form expression that respects the intrinsic spherical geometry. We show
that the proposed distance exhibits desirable theoretical properties and
induces a latent geometric structure on the space of non-degenerate vMF
distributions. As a primary application, we develop the efficient algorithms
for vMF mixture reduction, enabling structure-preserving compression of mixture
models in high-dimensional settings. Empirical results on synthetic datasets
and real-world high-dimensional embeddings, including biomedical sentence
representations and deep visual features, demonstrate the effectiveness of the
proposed geometry in distinguishing distributions and supporting interpretable
inference. This work expands the statistical toolbox for directional data
analysis by introducing a tractable, transport-inspired distance tailored to
the geometry of the hypersphere.

</details>

### [567] [Optimal Scheduling of Dynamic Transport](https://arxiv.org/abs/2504.14425)
*Panos Tsimpos,Zhi Ren,Jakob Zech,Youssef Marzouk*

Main category: stat.ML

TLDR: 本文提出了一种基于连续时间动力系统的流式采样和生成建模方法，通过优化时间调度来最小化速度场的空间Lipschitz常数，从而显著提高近似和学习效果。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用时间轴的设计自由度来优化流式方法的采样和生成建模，特别是通过引入“弯曲”轨迹来改进近似效果。

Method: 通过变分法和Γ-收敛技术，计算最优时间调度，以最小化速度场的空间Lipschitz常数。

Result: 对于广泛的源/目标测度和传输映射，最优调度可以闭式求解，且其Lipschitz常数比恒等调度（如Wasserstein测地线）指数级更小。

Conclusion: 通过优化时间调度，流式方法在采样和生成建模中的性能可以显著提升，为相关领域提供了新的理论支持。

Abstract: Flow-based methods for sampling and generative modeling use continuous-time
dynamical systems to represent a {transport map} that pushes forward a source
measure to a target measure. The introduction of a time axis provides
considerable design freedom, and a central question is how to exploit this
freedom. Though many popular methods seek straight line (i.e., zero
acceleration) trajectories, we show here that a specific class of ``curved''
trajectories can significantly improve approximation and learning. In
particular, we consider the unit-time interpolation of any given transport map
$T$ and seek the schedule $\tau: [0,1] \to [0,1]$ that minimizes the spatial
Lipschitz constant of the corresponding velocity field over all times $t \in
[0,1]$. This quantity is crucial as it allows for control of the approximation
error when the velocity field is learned from data. We show that, for a broad
class of source/target measures and transport maps $T$, the \emph{optimal
schedule} can be computed in closed form, and that the resulting optimal
Lipschitz constant is \emph{exponentially smaller} than that induced by an
identity schedule (corresponding to, for instance, the Wasserstein geodesic).
Our proof technique relies on the calculus of variations and
$\Gamma$-convergence, allowing us to approximate the aforementioned degenerate
objective by a family of smooth, tractable problems.

</details>

### [568] [On the Tunability of Random Survival Forests Model for Predictive Maintenance](https://arxiv.org/abs/2504.14744)
*Yigitcan Yardımcı,Mustafa Cavus*

Main category: stat.ML

TLDR: 本文研究了随机生存森林（RSF）模型在预测性维护中的可调性，提出了一个三层次框架量化调优效果，并验证了超参数调优对模型性能的显著提升。


<details>
  <summary>Details</summary>
Motivation: 随机生存森林（RSF）在预测性维护中广泛使用，但其性能对超参数配置敏感，而系统性评估其可调性的研究较少。

Method: 提出了一个三层次框架：模型级指标衡量整体性能提升，超参数级指标评估个体贡献，并确定最佳调优范围。使用C指数和Brier分数进行评估。

Result: 实验表明，超参数调优显著提升模型性能，C指数平均提高0.0547，Brier分数平均降低0.0199。ntree和mtry调优效果最佳，而splitrule调优可能降低性能。

Conclusion: 研究强调了超参数调优在生存模型中的重要性，并为实际预测性维护应用提供了优化RSF的具体建议。

Abstract: This paper investigates the tunability of the Random Survival Forest (RSF)
model in predictive maintenance, where accurate time-to-failure estimation is
crucial. Although RSF is widely used due to its flexibility and ability to
handle censored data, its performance is sensitive to hyperparameter
configurations. However, systematic evaluations of RSF tunability remain
limited, especially in predictive maintenance contexts. We introduce a
three-level framework to quantify tunability: (1) a model-level metric
measuring overall performance gain from tuning, (2) a hyperparameter-level
metric assessing individual contributions, and (3) identification of optimal
tuning ranges. These metrics are evaluated across multiple datasets using
survival-specific criteria: the C-index for discrimination and the Brier score
for calibration. Experiments on four CMAPSS dataset subsets, simulating
aircraft engine degradation, reveal that hyperparameter tuning consistently
improves model performance. On average, the C-index increased by 0.0547, while
the Brier score decreased by 0.0199. These gains were consistent across all
subsets. Moreover, ntree and mtry showed the highest average tunability, while
nodesize offered stable improvements within the range of 10 to 30. In contrast,
splitrule demonstrated negative tunability on average, indicating that improper
tuning may reduce model performance. Our findings emphasize the practical
importance of hyperparameter tuning in survival models and provide actionable
insights for optimizing RSF in real-world predictive maintenance applications.

</details>

### [569] [Expected Free Energy-based Planning as Variational Inference](https://arxiv.org/abs/2504.14898)
*Bert de Vries,Wouter Nuijten,Thijs van de Laar,Wouter Kouw,Sepideh Adamiat,Tim Nisslbeck,Mykola Lukashchuk,Hoang Minh Huu Nguyen,Marco Hidalgo Araya,Raphael Tresor,Thijs Jenneskens,Ivana Nikoloska,Raaja Subramanian,Bart van Erp,Dmitry Bagaev,Albert Podusenko*

Main category: stat.ML

TLDR: 论文提出了一种基于变分自由能的统一框架，将规划问题转化为变分推断，解决了传统方法中探索与利用分离的问题，并实现了可扩展的资源感知实现。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理不确定性规划时，将探索与利用视为独立目标，缺乏统一的推断基础。本文旨在通过主动推断和变分自由能最小化，提供一个理论一致且可扩展的解决方案。

Method: 通过最小化生成模型上的变分自由能函数，结合偏好和认知先验，将规划问题转化为变分推断，从而统一探索与利用。

Result: 提出的框架能够生成同时支持目标达成和信息增益的最优策略，并考虑了计算资源的限制。

Conclusion: 该框架不仅理论一致，还实现了可扩展的资源感知主动推断，为不确定性规划提供了新的解决方案。

Abstract: We address the problem of planning under uncertainty, where an agent must
choose actions that not only achieve desired outcomes but also reduce
uncertainty. Traditional methods often treat exploration and exploitation as
separate objectives, lacking a unified inferential foundation. Active
inference, grounded in the Free Energy Principle, offers such a foundation by
minimizing Expected Free Energy (EFE), a cost function that combines utility
with epistemic drives like ambiguity resolution and novelty seeking. However,
the computational burden of EFE minimization has remained a major obstacle to
its scalability. In this paper, we show that EFE-based planning arises
naturally from minimizing a variational free energy functional on a generative
model augmented with preference and epistemic priors. This result reinforces
theoretical consistency with the Free Energy Principle, by casting planning
itself as variational inference. Our formulation yields optimal policies that
jointly support goal achievement and information gain, while incorporating a
complexity term that accounts for bounded computational resources. This
unifying framework connects and extends existing methods, enabling scalable,
resource-aware implementations of active inference agents.

</details>

### [570] [Advanced posterior analyses of hidden Markov models: finite Markov chain imbedding and hybrid decoding](https://arxiv.org/abs/2504.15156)
*Zenia Elise Damgaard Bæk,Moisès Coll Macià,Laurits Skov,Asger Hobolth*

Main category: stat.ML

TLDR: 本文提出使用有限马尔可夫链嵌入（FMCI）和混合解码方法解决隐马尔可夫模型（HMM）中的两个主要任务：计算隐藏状态序列的统计量分布和解码隐藏状态序列。FMCI用于计算后验分布，混合解码则优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决HMM应用中计算隐藏状态序列统计量分布和解码隐藏状态序列的两大任务。

Method: 第一部分使用FMCI计算隐藏状态序列的统计量后验分布；第二部分提出混合解码方法，结合全局和局部解码优势。

Result: FMCI框架有效计算统计量分布；混合解码性能优于Viterbi和后验解码，并提供了调参新方法。

Conclusion: FMCI和混合解码方法在HMM应用中表现优异，提供了可复现的代码支持。

Abstract: Two major tasks in applications of hidden Markov models are to (i) compute
distributions of summary statistics of the hidden state sequence, and (ii)
decode the hidden state sequence. We describe finite Markov chain imbedding
(FMCI) and hybrid decoding to solve each of these two tasks. In the first part
of our paper we use FMCI to compute posterior distributions of summary
statistics such as the number of visits to a hidden state, the total time spent
in a hidden state, the dwell time in a hidden state, and the longest run
length. We use simulations from the hidden state sequence, conditional on the
observed sequence, to establish the FMCI framework. In the second part of our
paper we apply hybrid segmentation for improved decoding of a HMM. We
demonstrate that hybrid decoding shows increased performance compared to
Viterbi or Posterior decoding (often also referred to as global or local
decoding), and we introduce a novel procedure for choosing the tuning parameter
in the hybrid procedure. Furthermore, we provide an alternative derivation of
the hybrid loss function based on weighted geometric means. We demonstrate and
apply FMCI and hybrid decoding on various classical data sets, and supply
accompanying code for reproducibility.

</details>

<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [571] [The First VoicePrivacy Attacker Challenge](https://arxiv.org/abs/2504.14183)
*Natalia Tomashenko,Xiaoxiao Miao,Emmanuel Vincent,Junichi Yamagishi*

Main category: eess.AS

TLDR: ICASSP 2025 SP Grand Challenge评估了针对语音匿名化系统的攻击者系统，最佳攻击者系统将EER降低了25-44%。


<details>
  <summary>Details</summary>
Motivation: 评估攻击者系统对语音匿名化系统的效果。

Method: 提供训练、开发和评估数据集，参与者开发自动说话人验证系统形式的攻击者系统。

Result: 最佳攻击者系统将EER相对基线降低了25-44%。

Conclusion: 挑战赛展示了攻击者系统在对抗语音匿名化技术方面的有效性。

Abstract: The First VoicePrivacy Attacker Challenge is an ICASSP 2025 SP Grand
Challenge which focuses on evaluating attacker systems against a set of voice
anonymization systems submitted to the VoicePrivacy 2024 Challenge. Training,
development, and evaluation datasets were provided along with a baseline
attacker. Participants developed their attacker systems in the form of
automatic speaker verification systems and submitted their scores on the
development and evaluation data. The best attacker systems reduced the equal
error rate (EER) by 25-44% relative w.r.t. the baseline.

</details>

### [572] [Data Augmentation Using Neural Acoustic Fields With Retrieval-Augmented Pre-training](https://arxiv.org/abs/2504.14409)
*Christopher Ick,Gordon Wichern,Yoshiki Masuyama,François G. Germain,Jonathan Le Roux*

Main category: eess.AS

TLDR: MERL提出了一种基于神经声场的RIR估计系统，用于ICASSP 2025的数据增强任务。


<details>
  <summary>Details</summary>
Motivation: 解决房间脉冲响应（RIR）估计和说话者距离估计的数据增强问题。

Method: 预训练神经声场并适应目标房间，利用外部数据集或提供的几何信息。

Result: 预测RIR并用于训练说话者距离估计模型。

Conclusion: 系统有效结合预训练和适应策略，支持数据增强任务。

Abstract: This report details MERL's system for room impulse response (RIR) estimation
submitted to the Generative Data Augmentation Workshop at ICASSP 2025 for
Augmenting RIR Data (Task 1) and Improving Speaker Distance Estimation (Task
2). We first pre-train a neural acoustic field conditioned by room geometry on
an external large-scale dataset in which pairs of RIRs and the geometries are
provided. The neural acoustic field is then adapted to each target room by
using the enrollment data, where we leverage either the provided room
geometries or geometries retrieved from the external dataset, depending on
availability. Lastly, we predict the RIRs for each pair of source and receiver
locations specified by Task 1, and use these RIRs to train the speaker distance
estimation model in Task 2.

</details>

### [573] [OmniAudio: Generating Spatial Audio from 360-Degree Video](https://arxiv.org/abs/2504.14906)
*Huadai Liu,Tianyi Luo,Qikai Jiang,Kaicheng Luo,Peiwen Sun,Jialei Wan,Rongjie Huang,Qian Chen,Wen Wang,Xiangtai Li,Shiliang Zhang,Zhijie Yan,Zhou Zhao,Wei Xue*

Main category: eess.AS

TLDR: 论文提出了一种新任务360V2SA，通过360度视频生成空间音频，并开发了OmniAudio框架和Sphere360数据集，实现了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统视频到音频生成技术缺乏空间线索，无法准确表示3D环境中的声源。

Method: 提出OmniAudio框架，利用自监督预训练和双分支结构处理全景和FoV视频输入。

Result: OmniAudio在Sphere360数据集上实现了最先进的性能。

Conclusion: 该研究为360度视频生成空间音频提供了有效解决方案，并公开了代码和数据集。

Abstract: Traditional video-to-audio generation techniques primarily focus on
field-of-view (FoV) video and non-spatial audio, often missing the spatial cues
necessary for accurately representing sound sources in 3D environments. To
address this limitation, we introduce a novel task, 360V2SA, to generate
spatial audio from 360-degree videos, specifically producing First-order
Ambisonics (FOA) audio - a standard format for representing 3D spatial audio
that captures sound directionality and enables realistic 3D audio reproduction.
We first create Sphere360, a novel dataset tailored for this task that is
curated from real-world data. We also design an efficient semi-automated
pipeline for collecting and cleaning paired video-audio data. To generate
spatial audio from 360-degree video, we propose a novel framework OmniAudio,
which leverages self-supervised pre-training using both spatial audio data (in
FOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a
dual-branch framework that utilizes both panoramic and FoV video inputs to
capture comprehensive local and global information from 360-degree videos.
Experimental results demonstrate that OmniAudio achieves state-of-the-art
performance across both objective and subjective metrics on Sphere360. Code and
datasets will be released at https://github.com/liuhuadai/OmniAudio. The demo
page is available at https://OmniAudio-360V2SA.github.io.

</details>

### [574] [StableQuant: Layer Adaptive Post-Training Quantization for Speech Foundation Models](https://arxiv.org/abs/2504.14915)
*Yeona Hong,Hyewon Han,Woo-jin Chung,Hong-Goo Kang*

Main category: eess.AS

TLDR: StableQuant是一种新型自适应后训练量化算法，针对语音基础模型（SFMs）优化，克服传统PTQ方法在SFMs上的局限性，显著提升模型压缩和推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统后训练量化（PTQ）方法在大型语言模型（LLMs）上表现良好，但直接应用于语音基础模型（SFMs）时效果不佳，因其网络架构不同。StableQuant旨在解决这一问题。

Method: StableQuant通过自适应分析每层的尺度分布和整体性能，动态确定量化范围，适用于不同网络架构。

Result: 在HuBERT和wav2vec2.0上测试，StableQuant将模型大小缩减至四分之一，推理速度翻倍，8位量化下词错误率（WER）仅下降0.3%。

Conclusion: StableQuant在SFMs上表现出色，显著优于传统PTQ方法，为语音模型的压缩和加速提供了高效解决方案。

Abstract: In this paper, we propose StableQuant, a novel adaptive post-training
quantization (PTQ) algorithm for widely used speech foundation models (SFMs).
While PTQ has been successfully employed for compressing large language models
(LLMs) due to its ability to bypass additional fine-tuning, directly applying
these techniques to SFMs may not yield optimal results, as SFMs utilize
distinct network architecture for feature extraction. StableQuant demonstrates
optimal quantization performance regardless of the network architecture type,
as it adaptively determines the quantization range for each layer by analyzing
both the scale distributions and overall performance. We evaluate our algorithm
on two SFMs, HuBERT and wav2vec2.0, for an automatic speech recognition (ASR)
task, and achieve superior performance compared to traditional PTQ methods.
StableQuant successfully reduces the sizes of SFM models to a quarter and
doubles the inference speed while limiting the word error rate (WER)
performance drop to less than 0.3% with 8-bit quantization.

</details>

<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [575] [Optimal Lattice Boltzmann Closures through Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2504.14422)
*Paul Fischer,Sebastian Kaltenbach,Sergey Litvinov,Sauro Succi,Petros Koumoutsakos*

Main category: physics.flu-dyn

TLDR: 提出了一种基于多智能体强化学习（MARL）的数据驱动方法，显著提升了粗粒度Lattice Boltzmann方法（LBM）模拟的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: LBM在模拟多尺度流体现象时存在计算资源限制，传统方法难以在未充分解析的模拟中保持稳定性和通用性。

Method: 使用卷积神经网络动态调整LBM的局部松弛参数，结合MARL框架，应用于湍流Kolmogorov流。

Result: MARL方法显著提高了模拟的稳定性，恢复了高分辨率模拟的能量谱，同时保持计算效率，且模型可迁移至未训练过的流动场景。

Conclusion: MARL为LBM提供了一种高效且精确的闭包模型，有望解决传统方法无法处理的复杂问题。

Abstract: The Lattice Boltzmann method (LBM) offers a powerful and versatile approach
to simulating diverse hydrodynamic phenomena, spanning microfluidics to
aerodynamics. The vast range of spatiotemporal scales inherent in these systems
currently renders full resolution impractical, necessitating the development of
effective closure models for under-resolved simulations. Under-resolved LBMs
are unstable, and while there is a number of important efforts to stabilize
them, they often face limitations in generalizing across scales and physical
systems. We present a novel, data-driven, multiagent reinforcement learning
(MARL) approach that drastically improves stability and accuracy of
coarse-grained LBM simulations. The proposed method uses a convolutional neural
network to dynamically control the local relaxation parameter for the LB across
the simulation grid. The LB-MARL framework is showcased in turbulent Kolmogorov
flows. We find that the MARL closures stabilize the simulations and recover the
energy spectra of significantly more expensive fully resolved simulations while
maintaining computational efficiency. The learned closure model can be
transferred to flow scenarios unseen during training and has improved
robustness and spectral accuracy compared to traditional LBM models. We believe
that MARL closures open new frontiers for efficient and accurate simulations of
a multitude of complex problems not accessible to present-day LB methods alone.

</details>

### [576] [LBM-GNN: Graph Neural Network Enhanced Lattice Boltzmann Method](https://arxiv.org/abs/2504.14494)
*Yue Li*

Main category: physics.flu-dyn

TLDR: LBM-GNN结合了传统Lattice Boltzmann Method（LBM）与Graph Neural Networks（GNNs），在流体动力学模拟中表现出更高的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统LBM在高Reynolds数下稳定性不足，希望通过结合GNN提升其性能。

Method: 将GNN应用于LBM，通过基准问题（如Taylor-Green涡旋）验证方法。

Result: GNN增强的LBM在更高Reynolds数下保持更好的守恒性和数值稳定性。

Conclusion: LBM-GNN是一种有效的方法，能够提升流体模拟的准确性和稳定性。

Abstract: In this paper, we present LBM-GNN, a novel approach that enhances the
traditional Lattice Boltzmann Method (LBM) with Graph Neural Networks (GNNs).
We apply this method to fluid dynamics simulations, demonstrating improved
stability and accuracy compared to standard LBM implementations. The method is
validated using benchmark problems such as the Taylor-Green vortex, focusing on
accuracy, conservation properties, and performance across different Reynolds
numbers and grid resolutions. Our results indicate that GNN-enhanced LBM can
maintain better conservation properties while improving numerical stability at
higher Reynolds numbers.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [577] [Segmentation with Noisy Labels via Spatially Correlated Distributions](https://arxiv.org/abs/2504.14795)
*Ryu Tadokoro,Tsukasa Takagi,Shin-ichi Maeda*

Main category: eess.IV

TLDR: 提出一种基于贝叶斯估计的概率模型，用于处理语义分割中空间相关的标注错误，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 在语义分割中，高质量标注对模型准确性至关重要，但实际场景中（如医学影像和遥感）获取真实标注困难且易引入错误。这些错误通常具有空间相关性。

Method: 采用基于KMS矩阵的高斯分布建模相邻像素标注错误的空间相关性，通过贝叶斯推断计算后验分布。

Result: 实验表明，利用标注错误的空间相关性显著提升性能，在特定任务（如肺部分割）中，性能接近使用干净标注的水平。

Conclusion: 提出的方法有效解决了标注错误的空间相关性问题，为实际应用中的语义分割提供了鲁棒性。

Abstract: In semantic segmentation, the accuracy of models heavily depends on the
high-quality annotations. However, in many practical scenarios such as medical
imaging and remote sensing, obtaining true annotations is not straightforward
and usually requires significant human labor. Relying on human labor often
introduces annotation errors, including mislabeling, omissions, and
inconsistency between annotators. In the case of remote sensing, differences in
procurement time can lead to misaligned ground truth annotations. These label
errors are not independently distributed, and instead usually appear in
spatially connected regions where adjacent pixels are more likely to share the
same errors. To address these issues, we propose an approximate Bayesian
estimation based on a probabilistic model that assumes training data includes
label errors, incorporating the tendency for these errors to occur with spatial
correlations between adjacent pixels. Bayesian inference requires computing the
posterior distribution of label errors, which becomes intractable when spatial
correlations are present. We represent the correlation of label errors between
adjacent pixels through a Gaussian distribution whose covariance is structured
by a Kac-Murdock-Szeg\"{o} (KMS) matrix, solving the computational challenges.
Through experiments on multiple segmentation tasks, we confirm that leveraging
the spatial correlation of label errors significantly improves performance.
Notably, in specific tasks such as lung segmentation, the proposed method
achieves performance comparable to training with clean labels under moderate
noise levels. Code is available at
https://github.com/pfnet-research/Bayesian_SpatialCorr.

</details>

<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [578] [Breaking the Diffraction Barrier for Passive Sources: Parameter-Decoupled Superresolution Assisted by Physics-Informed Machine Learning](https://arxiv.org/abs/2504.14156)
*Abdelali Sajia,Bilal Benzimoun,Pawan Khatiwada,Guogan Zhao,Xiao-Feng Qian*

Main category: physics.optics

TLDR: 提出了一种参数解耦的超分辨率框架，用于估计被动双点源的亚波长间距，无需先验知识或对源的控制。结合物理信息的机器学习模型解决了实际缺陷，实现了衍射极限以下14倍以上的分辨率。


<details>
  <summary>Details</summary>
Motivation: 解决被动系统中超分辨率技术的实际应用问题，特别是在无法控制源的情况下（如天体成像、活细胞显微镜）。

Method: 基于理论框架，结合物理信息的机器学习模型，处理背景噪声、光子损失和对齐问题。

Result: 在实验生成的图像上实现了衍射极限以下14倍的分辨率（约13.5纳米），保真度超过82%。

Conclusion: 该方法填补了被动系统超分辨率理论与实际应用之间的关键空白，适用于无法控制源的场景。

Abstract: We present a parameter-decoupled superresolution framework for estimating
sub-wavelength separations of passive two-point sources without requiring prior
knowledge or control of the source. Our theoretical foundation circumvents the
need to estimate multiple challenging parameters such as partial coherence,
brightness imbalance, random relative phase, and photon statistics. A
physics-informed machine learning (ML) model (trained with a standard desktop
workstation), synergistically integrating this theory, further addresses
practical imperfections including background noise, photon loss, and
centroid/orientation misalignment. The integrated parameter-decoupling
superresolution method achieves resolution 14 and more times below the
diffraction limit (corresponding to ~ 13.5 nm in optical microscopy) on
experimentally generated realistic images with >82% fidelity, performance
rivaling state-of-the-art techniques for actively controllable sources.
Critically, our method's robustness against source parameter variability and
source-independent noises enables potential applications in realistic scenarios
where source control is infeasible, such as astrophysical imaging, live-cell
microscopy, and quantum metrology. This work bridges a critical gap between
theoretical superresolution limits and practical implementations for passive
systems.

</details>

### [579] [DeepPD: Joint Phase and Object Estimation from Phase Diversity with Neural Calibration of a Deformable Mirror](https://arxiv.org/abs/2504.14157)
*Magdalena C. Schneider,Courtney Johnson,Cedric Allier,Larissa Heinrich,Diane Adjavon,Joren Husic,Patrick La Rivière,Stephan Saalfeld,Hari Shroff*

Main category: physics.optics

TLDR: DeepPD是一种基于深度学习的框架，通过仅需五张图像联合估计物体和相位，克服了现有相位多样性方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 样本引起的像差和光学缺陷限制了荧光显微镜的分辨率，现有方法在Zernike模式、多样性图像数量或镜面校准方面存在不足。

Method: 结合物体和波前的神经表示以及变形镜的模型，仅需五张图像进行联合估计。

Result: DeepPD在严重像差下仍能提高鲁棒性和重建质量，优于先前方法。

Conclusion: DeepPD在标定目标和生物样本中表现出色，为荧光显微镜的分辨率恢复提供了新解决方案。

Abstract: Sample-induced aberrations and optical imperfections limit the resolution of
fluorescence microscopy. Phase diversity is a powerful technique that leverages
complementary phase information in sequentially acquired images with
deliberately introduced aberrations--the phase diversities--to enable phase and
object reconstruction and restore diffraction-limited resolution. These phase
diversities are typically introduced into the optical path via a deformable
mirror. Existing phase-diversity-based methods are limited to Zernike modes,
require large numbers of diversity images, or depend on accurate mirror
calibration--which are all suboptimal. We present DeepPD, a deep learning-based
framework that combines neural representations of the object and wavefront with
a learned model of the deformable mirror to jointly estimate both object and
phase from only five images. DeepPD improves robustness and reconstruction
quality over previous approaches, even under severe aberrations. We demonstrate
its performance on calibration targets and biological samples, including
immunolabeled myosin in fixed PtK2 cells.

</details>

### [580] [Beyond Terabit/s Integrated Neuromorphic Photonic Processor for DSP-Free Optical Interconnects](https://arxiv.org/abs/2504.15044)
*Benshan Wang,Qiarong Xiao,Tengji Xu,Li Fan,Shaojie Liu,Jianji Dong,Junwen Zhang,Chaoran Huang*

Main category: physics.optics

TLDR: 提出了一种基于神经形态光学信号处理器（OSP）的新型解决方案，用于高性能AI训练和推理，解决了传统电光互连的延迟和能耗问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的快速发展，传统电光互连技术无法满足高性能计算对超低延迟和能效的需求。

Method: 采用深度储备计算技术，开发了一种无需数字信号处理器（DSP）的全光学实时处理器（OSP）。

Result: 实验显示，OSP在5公里光纤上实现了100 Gbaud PAM4每通道、1.6 Tbit/s的数据中心互连，延迟和能耗分别降低了四个和三个数量级。

Conclusion: OSP为下一代AI基础设施提供了高效、可扩展的高速解决方案。

Abstract: The rapid expansion of generative AI drives unprecedented demands for
high-performance computing. Training large-scale AI models now requires vast
interconnected GPU clusters across multiple data centers. Multi-scale AI
training and inference demand uniform, ultra-low latency, and energy-efficient
links to enable massive GPUs to function as a single cohesive unit. However,
traditional electrical and optical interconnects, relying on conventional
digital signal processors (DSPs) for signal distortion compensation,
increasingly fail to meet these stringent requirements. To overcome these
limitations, we present an integrated neuromorphic optical signal processor
(OSP) that leverages deep reservoir computing and achieves DSP-free,
all-optical, real-time processing. Experimentally, our OSP achieves a 100 Gbaud
PAM4 per lane, 1.6 Tbit/s data center interconnect over a 5 km optical fiber in
the C-band (equivalent to over 80 km in the O-band), far exceeding the reach of
state-of-the-art DSP solutions, which are fundamentally constrained by
chromatic dispersion in IMDD systems. Simultaneously, it reduces processing
latency by four orders of magnitude and energy consumption by three orders of
magnitude. Unlike DSPs, which introduce increased latency at high data rates,
our OSP maintains consistent, ultra-low latency regardless of data rate
scaling, making it ideal for future optical interconnects. Moreover, the OSP
retains full optical field information for better impairment compensation and
adapts to various modulation formats, data rates, and wavelengths. Fabricated
using a mature silicon photonic process, the OSP can be monolithically
integrated with silicon photonic transceivers, enhancing the compactness and
reliability of all-optical interconnects. This research provides a highly
scalable, energy-efficient, and high-speed solution, paving the way for
next-generation AI infrastructure.

</details>