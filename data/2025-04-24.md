<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 29]
- [cs.LG](#cs.LG) [Total: 51]
- [cs.CL](#cs.CL) [Total: 36]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.CV](#cs.CV) [Total: 65]
- [cs.DC](#cs.DC) [Total: 3]
- [math.ST](#math.ST) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [stat.ML](#stat.ML) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.CY](#cs.CY) [Total: 10]
- [cs.GR](#cs.GR) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [quant-ph](#quant-ph) [Total: 6]
- [cs.HC](#cs.HC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 4]
- [stat.ME](#stat.ME) [Total: 1]
- [eess.SP](#eess.SP) [Total: 8]
- [cs.MA](#cs.MA) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.IR](#cs.IR) [Total: 6]
- [eess.SY](#eess.SY) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.RO](#cs.RO) [Total: 7]
- [eess.IV](#eess.IV) [Total: 3]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Blockchain-Driven Solutions for Carbon Credit Trading: A Decentralized Platform for SMEs](https://arxiv.org/abs/2504.16085)
*Yun-Cheng Tsai*

Main category: cs.CR

TLDR: 论文提出了一种基于区块链的去中心化碳信用交易平台，旨在帮助台湾中小企业简化碳交易流程并降低市场准入门槛。


<details>
  <summary>Details</summary>
Motivation: 全球碳法规的日益严格对中小企业提出了挑战，需要一种更高效、透明的碳交易解决方案。

Method: 结合以太坊智能合约，平台通过自动化交易和增强透明度来减少信息不对称和中介成本。采用实验设计对比传统中心化平台，并使用Kano模型评估用户满意度。

Result: 统计证实该系统能显著减少交易时间和成本，同时确保符合CBAM和CCA法规。用户满意度分析确定了关键功能和改进方向。

Conclusion: 研究为中小企业实现碳中和提供了更全面的解决方案，凸显了区块链在全球碳市场中的变革潜力。

Abstract: The increasing demand for sustainability and compliance with global carbon
regulations has posed significant challenges for small and medium-sized
enterprises (SMEs). This paper proposes a blockchain-based decentralized carbon
credit trading platform tailored for SMEs in Taiwan, aiming to simplify the
complex carbon trading process and lower market entry barriers. Drawing upon
the Diffusion of Innovations theory and transaction cost economics, we
illustrate how blockchain technology can reduce informational asymmetry and
intermediary costs in carbon markets. By integrating Ethereum-based smart
contracts, the platform automates transactions, enhances transparency, and
reduces administrative burdens - addressing key obstacles such as technical
complexity and market risks. A controlled experimental design was conducted to
compare the proposed system with a conventional centralized carbon trading
platform. Statistical analysis confirms its effectiveness in minimizing time
and expenses while ensuring compliance with the Carbon Border Adjustment
Mechanism (CBAM) and the Clean Competition Act (CCA). User satisfaction was
measured using the Kano model, with the results identifying essential features
and prioritizing future enhancements. This study contributes a more
comprehensive solution for SMEs seeking to achieve carbon neutrality,
underscoring the transformative potential of blockchain technology in global
carbon markets.

</details>

### [2] [Surveillance Disguised as Protection: A Comparative Analysis of Sideloaded and In-Store Parental Control Apps](https://arxiv.org/abs/2504.16087)
*Eva-Maria Maier,Leonie Maria Tanczer,Lukas Daniel Klausner*

Main category: cs.CR

TLDR: 论文研究了20款侧载家长控制应用与20款Google Play商店应用，发现侧载应用在隐私保护、功能和安全方面表现较差。


<details>
  <summary>Details</summary>
Motivation: 探索侧载家长控制应用的安全性和隐私问题，填补研究空白。

Method: 分析隐私政策、APK文件、应用行为、网络流量和功能，比较侧载与商店应用。

Result: 侧载应用功能不足，隐私保护差，部分传输未加密数据，半数无隐私政策，8款有潜在间谍软件特征。

Conclusion: 侧载家长控制应用存在严重安全和隐私风险，需加强监管。

Abstract: Parental control applications, software tools designed to manage and monitor
children's online activities, serve as essential safeguards for parents in the
digital age. However, their usage has sparked concerns about security and
privacy violations inherent in various child monitoring products. Sideloaded
software (i. e. apps installed outside official app stores) poses an increased
risk, as it is not bound by the regulations of trusted platforms. Despite this,
the market of sideloaded parental control software has remained widely
unexplored by the research community. This paper examines 20 sideloaded
parental control apps and compares them to 20 apps available on the Google Play
Store. We base our analysis on privacy policies, Android package kit (APK)
files, application behaviour, network traffic and application functionalities.
Our findings reveal that sideloaded parental control apps fall short compared
to their in-store counterparts, lacking specialised parental control features
and safeguards against misuse while concealing themselves on the user's device.
Alarmingly, three apps transmitted sensitive data unencrypted, half lacked a
privacy policy and 8 out of 20 were flagged for potential stalkerware
indicators of compromise (IOC).

</details>

### [3] [Paths Not Taken: A Secure Computing Tutorial](https://arxiv.org/abs/2504.16088)
*William Earl Boebert*

Main category: cs.CR

TLDR: 本文是关于标签或描述符架构的安全机制的教程，展示了如何应用这些机制来减少或消除漏洞。


<details>
  <summary>Details</summary>
Motivation: 介绍被低估的标签或描述符架构安全机制，并鼓励教育者和学生将其纳入安全计算的学习中。

Method: 通过非正式模型展示集成工件的机制，该工件是一个名为“Guard”的专用硬件/软件系统。

Result: 展示了如何利用这些机制来增强安全性。

Conclusion: 希望该教程能促进教育者和学生关注并学习这些重要的历史工作。

Abstract: This paper is a tutorial on the proven but currently under-appreciated
security mechanisms associated with "tagged" or "descriptor" architectures. The
tutorial shows how the principles behind such architectures can be applied to
mitigate or eliminate vulnerabilities. The tutorial incorporates systems
engineering practices by presenting the mechanisms in an informal model of an
integrated artifact in its operational environment. The artifact is a
special-purpose hardware/software system called a "Guard" which robustly hosts
defensive software. It is hoped that this tutorial may encourage teachers to
include significant past work in their curricula and students who are
self-teaching to add that work to their exploration of secure computing.

</details>

### [4] [Carbyne: An Ultra-Lightweight DoS-Resilient Mempool for Bitcoin](https://arxiv.org/abs/2504.16089)
*Hina Binte Haq,Syed Taha Ali,Asad Salman,Patrick McCorry,Siamak F. Shahandashti*

Main category: cs.CR

TLDR: Neonpool是一种优化交易池的技术，利用布隆过滤器变体将内存消耗降低至2MB（原400MB），同时保持99.99%的交易处理准确率。


<details>
  <summary>Details</summary>
Motivation: 加密货币全节点的资源需求增加，尤其是交易池的内存负担，阻碍了节点参与，Neonpool旨在降低这一成本。

Method: 采用布隆过滤器变体优化交易池，减少内存使用，并在C++中实现，通过比特币和以太坊数据集进行评估。

Result: 内存消耗降低200倍（400MB至2MB），交易处理准确率超过99.99%，支持轻量级设备运行。

Conclusion: Neonpool降低了节点参与成本，增强了加密货币网络的去中心化、安全性和鲁棒性。

Abstract: The increasing adoption of cryptocurrencies has significantly amplified the
resource requirements for operating full nodes, creating substantial barriers
to entry. Unlike miners, who are financially incentivized through block rewards
and transaction fees, full nodes lack direct economic compensation for their
critical role in maintaining the network. A key resource burden is the
transaction pool, which is particularly memory-intensive as it temporarily
stores unconfirmed transactions awaiting verification and propagation across
the network. We present Neonpool, a novel optimization for transaction pool
leveraging bloom filter variants to drastically reduce memory consumption by up
to 200 (e.g., 400 MB to 2 MB) while maintaining over 99.99% transaction
processing accuracy. Implemented in C++ and evaluated on unique Bitcoin and
Ethereum datasets, Neonpool enables efficient operation on lightweight clients,
such as smartphones, IoT devices, and systems-on-a-chip, without requiring a
hard fork. By lowering the cost of node participation, Neonpool enhances
decentralization and strengthens the overall security and robustness of
cryptocurrency networks.

</details>

### [5] [Post-Quantum Homomorphic Encryption: A Case for Code-Based Alternatives](https://arxiv.org/abs/2504.16091)
*Siddhartha Siddhiprada Bhoi,Arathi Arakala,Amy Beth Corman,Asha Rao*

Main category: cs.CR

TLDR: 本文综述了后量子同态加密（PQHE）的现状，重点探讨了基于纠错码的加密方法，并提出了五个新的研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于量子计算威胁传统加密方法的安全，研究人员致力于开发后量子同态加密算法，但目前主要基于格问题，而基于纠错码的研究较少。

Method: 本文回顾了现有的基于纠错码的加密框架，分析了其数学基础、安全性和效率，并与基于格的方案进行了比较。

Result: 指出了基于纠错码的同态加密面临的挑战，并提出了五个新的研究方向。

Conclusion: 基于纠错码的同态加密是一种有潜力的后量子加密方法，值得进一步研究。

Abstract: Homomorphic Encryption (HE) allows secure and privacy-protected computation
on encrypted data without the need to decrypt it. Since Shor's algorithm
rendered prime factorisation and discrete logarithm-based ciphers insecure with
quantum computations, researchers have been working on building post-quantum
homomorphic encryption (PQHE) algorithms. Most of the current PQHE algorithms
are secured by Lattice-based problems and there have been limited attempts to
build ciphers based on error-correcting code-based problems. This review
presents an overview of the current approaches to building PQHE schemes and
justifies code-based encryption as a novel way to diversify post-quantum
algorithms. We present the mathematical underpinnings of existing code-based
cryptographic frameworks and their security and efficiency guarantees. We
compare lattice-based and code-based homomorphic encryption solutions
identifying challenges that have inhibited the progress of code-based schemes.
We finally propose five new research directions to advance post-quantum
code-based homomorphic encryption.

</details>

### [6] [Trusted Identities for AI Agents: Leveraging Telco-Hosted eSIM Infrastructure](https://arxiv.org/abs/2504.16108)
*Sebastian Barros*

Main category: cs.CR

TLDR: 提出一种基于电信级eSIM基础设施的架构，为AI代理提供可信身份管理，解决现有身份框架在分布式系统中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于API密钥或证书的身份框架缺乏基础设施级信任和互操作性，难以管理敏感环境中的自主AI代理。

Method: 利用移动网络运营商（MNO）托管的eSIM基础设施作为信任根，通过远程认证和加密API实现代理身份管理。

Result: 探讨了企业自动化代理、金融系统和代理间通信的用例，并提出了对现有标准的扩展建议。

Conclusion: 本文为标准化和安全架构提供了概念框架，推动电信基础设施在代理经济中的角色讨论。

Abstract: The rise of autonomous AI agents in enterprise and industrial environments
introduces a critical challenge: how to securely assign, verify, and manage
their identities across distributed systems. Existing identity frameworks based
on API keys, certificates, or application-layer credentials lack the
infrastructure-grade trust, lifecycle control, and interoperability needed to
manage agents operating independently in sensitive contexts.
  In this paper, we propose a conceptual architecture that leverages
telecom-grade eSIM infrastructure, specifically hosted by mobile network
operators (MNOs), to serve as a root of trust for AI agents. Rather than
embedding SIM credentials in hardware devices, we envision a model where telcos
host secure, certified hardware modules (eUICC or HSM) that store and manage
agent-specific eSIM profiles. Agents authenticate remotely via cryptographic
APIs or identity gateways, enabling scalable and auditable access to enterprise
networks and services.
  We explore use cases such as onboarding enterprise automation agents,
securing AI-driven financial systems, and enabling trust in inter-agent
communications. We identify current limitations in GSMA and 3GPP standards,
particularly their device centric assumptions, and propose extensions to
support non-physical, software-based agents within trusted execution
environments. This paper is intended as a conceptual framework to open
discussion around standardization, security architecture, and the role of
telecom infrastructure in the evolving agent economy.

</details>

### [7] [Security-First AI: Foundations for Robust and Trustworthy Systems](https://arxiv.org/abs/2504.16110)
*Krti Tallam*

Main category: cs.CR

TLDR: 本文主张将AI安全作为基础层优先考虑，提出分层视角区分安全与保障，并提倡以安全为先的方法构建可信赖的AI系统。


<details>
  <summary>Details</summary>
Motivation: 当前AI讨论多关注安全性、透明度和责任，但AI安全（保护数据、模型和流程免受攻击）是这些目标的基础。

Method: 提出分层视角，区分安全与保障，讨论核心威胁模型、攻击途径及防御机制。

Result: 强调以指标驱动的AI安全方法对实现稳健的AI安全性、透明度和责任至关重要。

Conclusion: AI安全应作为优先事项，以确保AI系统的可信赖性和韧性。

Abstract: The conversation around artificial intelligence (AI) often focuses on safety,
transparency, accountability, alignment, and responsibility. However, AI
security (i.e., the safeguarding of data, models, and pipelines from
adversarial manipulation) underpins all of these efforts. This manuscript
posits that AI security must be prioritized as a foundational layer. We present
a hierarchical view of AI challenges, distinguishing security from safety, and
argue for a security-first approach to enable trustworthy and resilient AI
systems. We discuss core threat models, key attack vectors, and emerging
defense mechanisms, concluding that a metric-driven approach to AI security is
essential for robust AI safety, transparency, and accountability.

</details>

### [8] [AI-Based Vulnerability Analysis of NFT Smart Contracts](https://arxiv.org/abs/2504.16113)
*Xin Wang,Xiaoqi Li*

Main category: cs.CR

TLDR: 论文通过收集智能合约代码并分类常见缺陷，使用Python处理数据，构建决策树和随机森林模型，最终比较分析得出通用结论。


<details>
  <summary>Details</summary>
Motivation: 研究智能合约中的常见缺陷，并通过机器学习方法构建模型以识别和分析这些缺陷。

Method: 1. 收集并分类智能合约代码；2. 使用Python处理数据；3. 构建决策树模型；4. 引入随机森林模型；5. 比较分析不同模型。

Result: 通过决策树和随机森林模型成功识别和分析智能合约中的常见缺陷。

Conclusion: 决策树和随机森林模型在智能合约缺陷分析中具有实用性和有效性。

Abstract: In the research experiment of this article, our research work is divided into
several stages. Firstly, we collected a large number of smart contract codes
and classified them, identifying several common defects, including Risky
Mutably Porxy, ERC-721 Recentrancy, Unlimited Mining, Missing Requirements, and
Public Burns. Secondly, we used Python to process the smart contracts. On the
one hand, we modified the file names, and on the other hand, we batched the
process of the content for analysis and application. Next, we built a model of
the decision tree. Firstly, we carried out the feature extraction. We selected
the algorithm and divided the data. After comparing and processing, we chose
the CART classification tree to process. By gene coefficient, we analyzed and
sorted the data, and got the initial model of the decision tree. Then, we
introduced the random forest model on the basis of the decision tree. From
abstracting the same amount of samples to selecting features randomly.From
adjusting and optimizing parameters to completing the construction of the
forest model. Finally, we compared and analyzed the decision tree, random
forest, and self-built model in the paper and drew general conclusions.

</details>

### [9] [DMind Benchmark: The First Comprehensive Benchmark for LLM Evaluation in the Web3 Domain](https://arxiv.org/abs/2504.16116)
*Miracle Master,Rainy Sun,Anya Reese,Joey Ouyang,Alex Chen,Winter Dong,Frank Li,James Yi,Garry Zhao,Tony Ling,Hobert Wong,Lowes Yang*

Main category: cs.CR

TLDR: 论文介绍了DMind Benchmark，一个评估大型语言模型（LLMs）在Web3领域表现的框架，涵盖九个关键类别，并揭示了模型在Web3特定任务中的性能差距。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自然语言处理任务中表现优异，但在Web3等专业和快速发展的领域中的应用尚未充分探索。

Method: 通过DMind Benchmark框架，系统测试15种流行LLMs在区块链、智能合约分析、DeFi等九个类别中的表现，包括主观任务。

Result: 研究发现，即使是表现最强的模型在Web3特定推理和应用（如代币经济学和模因概念）中仍面临挑战，尤其是在识别安全漏洞和分析复杂DeFi机制时。

Conclusion: 论文公开了数据集、评估流程和标注结果，旨在推动LLMs在Web3领域的适应性和发展。

Abstract: Recent advances in Large Language Models (LLMs) have led to significant
progress on a wide range of natural language processing tasks. However, their
effectiveness in specialized and rapidly evolving domains such as Web3 remains
underexplored. In this paper, we introduce DMind Benchmark, a novel framework
that systematically tests LLMs across nine key categories encompassing
blockchain fundamentals, infrastructure, smart contract analysis, decentralized
finance (DeFi), decentralized autonomous organizations (DAOs), non-fungible
tokens (NFTs), token economics, meme concepts, and security vulnerabilities.
  DMind Benchmark goes beyond conventional multiple-choice questions by
incorporating domain-specific subjective tasks (e.g., smart contract code
auditing and repair, numeric reasoning on on-chain data, and fill-in
assessments), thereby capturing real-world complexities and stress-testing
model adaptability. We evaluate fifteen popular LLMs (from ChatGPT, DeepSeek,
Claude, and Gemini series) on DMind Benchmark, uncovering performance gaps in
Web3-specific reasoning and application, particularly in emerging areas like
token economics and meme concepts. Even the strongest models face significant
challenges in identifying subtle security vulnerabilities and analyzing complex
DeFi mechanisms. To foster progress in this area, we publicly release our
benchmark dataset, evaluation pipeline, and annotated results at
http://www.dmind.ai, offering a valuable resource for advancing specialized
domain adaptation and the development of more robust Web3-enabled LLMs.

</details>

### [10] [Towards Explainable and Lightweight AI for Real-Time Cyber Threat Hunting in Edge Networks](https://arxiv.org/abs/2504.16118)
*Milad Rahmati*

Main category: cs.CR

TLDR: 论文提出了一种可解释且轻量化的AI框架（ELAI），用于边缘网络中的实时网络威胁检测，解决了传统深度学习模型在解释性和计算成本上的不足。


<details>
  <summary>Details</summary>
Motivation: 边缘网络的分布式特性和资源限制使得网络安全面临挑战，而传统AI模型缺乏解释性且计算成本高，限制了其实际应用。

Method: 结合可解释的机器学习算法和优化的轻量化深度学习技术，采用决策树、注意力机制和联邦学习，提升检测准确性和解释性。

Result: 在CICIDS和UNSW-NB15等数据集上测试，ELAI实现了高检测率和低误报率，同时显著降低了计算需求。

Conclusion: ELAI为边缘计算环境提供了一种高效、可解释的网络安全解决方案，兼具实时性和计算效率。

Abstract: As cyber threats continue to evolve, securing edge networks has become
increasingly challenging due to their distributed nature and resource
limitations. Many AI-driven threat detection systems rely on complex deep
learning models, which, despite their high accuracy, suffer from two major
drawbacks: lack of interpretability and high computational cost. Black-box AI
models make it difficult for security analysts to understand the reasoning
behind their predictions, limiting their practical deployment. Moreover,
conventional deep learning techniques demand significant computational
resources, rendering them unsuitable for edge devices with limited processing
power. To address these issues, this study introduces an Explainable and
Lightweight AI (ELAI) framework designed for real-time cyber threat detection
in edge networks. Our approach integrates interpretable machine learning
algorithms with optimized lightweight deep learning techniques, ensuring both
transparency and computational efficiency. The proposed system leverages
decision trees, attention-based deep learning, and federated learning to
enhance detection accuracy while maintaining explainability. We evaluate ELAI
using benchmark cybersecurity datasets, such as CICIDS and UNSW-NB15, assessing
its performance across diverse cyberattack scenarios. Experimental results
demonstrate that the proposed framework achieves high detection rates with
minimal false positives, all while significantly reducing computational demands
compared to traditional deep learning methods. The key contributions of this
work include: (1) a novel interpretable AI-based cybersecurity model tailored
for edge computing environments, (2) an optimized lightweight deep learning
approach for real-time cyber threat detection, and (3) a comprehensive analysis
of explainability techniques in AI-driven cybersecurity applications.

</details>

### [11] [A Data-Centric Approach for Safe and Secure Large Language Models against Threatening and Toxic Content](https://arxiv.org/abs/2504.16120)
*Chaima Njeh,Haïfa Nakouri,Fehmi Jaafar*

Main category: cs.CR

TLDR: 论文提出了一种名为BART-Corrective Model的后生成修正机制，用于减少大型语言模型（LLM）生成的有害内容，实验证明其能显著降低毒性和越狱分数。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）取得了显著进展，但其潜在偏见和有害内容的问题仍然存在，需要一种实用解决方案来确保其安全和伦理使用。

Method: 采用BART-Corrective Model作为后生成修正机制，通过数据为中心的方法调整生成内容，而非仅依赖模型微调或提示工程。

Result: 实验显示，该方法在多个有毒数据集上显著降低了毒性和越狱分数，具体表现为GPT-4、PaLM2、Mistral-7B和Gemma-2b-it的分数分别降低了15%-28%和5%-23%。

Conclusion: 该方法有效提升了LLM的安全性和适用性，为其实际应用提供了潜在解决方案。

Abstract: Large Language Models (LLM) have made remarkable progress, but concerns about
potential biases and harmful content persist. To address these apprehensions,
we introduce a practical solution for ensuring LLM's safe and ethical use. Our
novel approach focuses on a post-generation correction mechanism, the
BART-Corrective Model, which adjusts generated content to ensure safety and
security. Unlike relying solely on model fine-tuning or prompt engineering, our
method provides a robust data-centric alternative for mitigating harmful
content. We demonstrate the effectiveness of our approach through experiments
on multiple toxic datasets, which show a significant reduction in mean toxicity
and jail-breaking scores after integration. Specifically, our results show a
reduction of 15% and 21% in mean toxicity and jail-breaking scores with GPT-4,
a substantial reduction of 28% and 5% with PaLM2, a reduction of approximately
26% and 23% with Mistral-7B, and a reduction of 11.1% and 19% with Gemma-2b-it.
These results demonstrate the potential of our approach to improve the safety
and security of LLM, making them more suitable for real-world applications.

</details>

### [12] [Breaking the Prompt Wall (I): A Real-World Case Study of Attacking ChatGPT via Lightweight Prompt Injection](https://arxiv.org/abs/2504.16125)
*Xiangyu Chang,Guang Dai,Hao Di,Haishan Ye*

Main category: cs.CR

TLDR: 报告通过真实案例研究展示了大型语言模型（如ChatGPT）如何受到提示注入攻击，并提出了三种攻击方式。


<details>
  <summary>Details</summary>
Motivation: 揭示商业级大型语言模型仍易受提示注入攻击，绕过安全过滤器并影响用户决策，旨在提高开发者对提示级安全性的重视。

Method: 通过用户输入、基于网络的检索和系统级代理指令三种方式注入对抗性提示。

Result: 攻击虽轻量且低成本，但能导致模型输出持续误导行为。

Conclusion: 报告强调需将提示级安全作为关键设计重点，呼吁开发者（尤其是OpenAI）加强防范。

Abstract: This report presents a real-world case study demonstrating how prompt
injection can attack large language model platforms such as ChatGPT according
to a proposed injection framework. By providing three real-world examples, we
show how adversarial prompts can be injected via user inputs, web-based
retrieval, and system-level agent instructions. These attacks, though
lightweight and low-cost, can cause persistent and misleading behaviors in LLM
outputs. Our case study reveals that even commercial-grade LLMs remain
vulnerable to subtle manipulations that bypass safety filters and influence
user decisions. \textbf{More importantly, we stress that this report is not
intended as an attack guide, but as a technical alert. As ethical researchers,
we aim to raise awareness and call upon developers, especially those at OpenAI,
to treat prompt-level security as a critical design priority.

</details>

### [13] [ReGraph: A Tool for Binary Similarity Identification](https://arxiv.org/abs/2504.16219)
*Li Zhou,Marc Dacier,Charalambos Konstantinou*

Main category: cs.CR

TLDR: ReGraph框架通过高效比较二进制代码函数，解决了现有方法在速度和规模上的问题，速度提升700倍且保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 二进制代码相似性检测（BCSD）在安全任务中至关重要，但现有方法因复杂神经网络导致计算时间长，难以处理大规模软件。

Method: 提出ReGraph框架，用于跨架构和优化级别高效比较二进制代码函数。

Result: 在公开数据集上评估，ReGraph速度比NLP方法快700倍，同时保持与最先进模型相当的准确性。

Conclusion: ReGraph在速度和准确性上取得显著优势，适用于大规模二进制代码分析。

Abstract: Binary Code Similarity Detection (BCSD) is not only essential for security
tasks such as vulnerability identification but also for code copying detection,
yet it remains challenging due to binary stripping and diverse compilation
environments. Existing methods tend to adopt increasingly complex neural
networks for better accuracy performance. The computation time increases with
the complexity. Even with powerful GPUs, the treatment of large-scale software
becomes time-consuming. To address these issues, we present a framework called
ReGraph to efficiently compare binary code functions across architectures and
optimization levels. Our evaluation with public datasets highlights that
ReGraph exhibits a significant speed advantage, performing 700 times faster
than Natural Language Processing (NLP)-based methods while maintaining
comparable accuracy results with respect to the state-of-the-art models.

</details>

### [14] [Blockchain Meets Adaptive Honeypots: A Trust-Aware Approach to Next-Gen IoT Security](https://arxiv.org/abs/2504.16226)
*Yazan Otoum,Arghavan Asad,Amiya Nayak*

Main category: cs.CR

TLDR: 论文提出了一种动态攻击检测与防御方法，结合区块链认证、双阶段入侵检测、信任感知服务迁移和虚拟蜜罐技术，显著提升了NGWN-IoT的安全性。


<details>
  <summary>Details</summary>
Motivation: NGWN-IoT虽然提供了高带宽能力，但面临不断演变的网络威胁，现有安全方法效果有限。

Method: 采用区块链认证（DAA）、双阶段入侵检测（IRF和DCRNN）、信任感知服务迁移（HBO）及虚拟蜜罐（BLISS存储攻击模式）。

Result: 实验表明，该方法在准确性、检测率、误报率等多指标上优于现有方法。

Conclusion: 该框架显著增强了NGWN-IoT生态系统的安全性。

Abstract: Edge computing-based Next-Generation Wireless Networks (NGWN)-IoT offer
enhanced bandwidth capacity for large-scale service provisioning but remain
vulnerable to evolving cyber threats. Existing intrusion detection and
prevention methods provide limited security as adversaries continually adapt
their attack strategies. We propose a dynamic attack detection and prevention
approach to address this challenge. First, blockchain-based authentication uses
the Deoxys Authentication Algorithm (DAA) to verify IoT device legitimacy
before data transmission. Next, a bi-stage intrusion detection system is
introduced: the first stage uses signature-based detection via an Improved
Random Forest (IRF) algorithm. In contrast, the second stage applies
feature-based anomaly detection using a Diffusion Convolution Recurrent Neural
Network (DCRNN). To ensure Quality of Service (QoS) and maintain Service Level
Agreements (SLA), trust-aware service migration is performed using Heap-Based
Optimization (HBO). Additionally, on-demand virtual High-Interaction honeypots
deceive attackers and extract attack patterns, which are securely stored using
the Bimodal Lattice Signature Scheme (BLISS) to enhance signature-based
Intrusion Detection Systems (IDS). The proposed framework is implemented in the
NS3 simulation environment and evaluated against existing methods across
multiple performance metrics, including accuracy, attack detection rate, false
negative rate, precision, recall, ROC curve, memory usage, CPU usage, and
execution time. Experimental results demonstrate that the framework
significantly outperforms existing approaches, reinforcing the security of
NGWN-enabled IoT ecosystems

</details>

### [15] [On the Consistency of GNN Explanations for Malware Detection](https://arxiv.org/abs/2504.16316)
*Hossein Shokouhinejad,Griffin Higgins,Roozbeh Razavi-Far,Hesamodin Mohammadian,Ali A. Ghorbani*

Main category: cs.CR

TLDR: 提出了一种结合规则编码和自动编码器的动态构建CFG的框架，利用GNN分类器检测恶意行为，并通过多种解释性技术提升模型可解释性。


<details>
  <summary>Details</summary>
Motivation: CFG在程序执行分析和恶意软件检测中至关重要，但现有方法在动态构建和可解释性方面存在不足。

Method: 动态构建CFG并嵌入节点特征，结合规则编码和自动编码器，使用GNN分类器检测恶意行为，并应用多种解释性技术。

Result: 框架在恶意软件样本识别和生成可靠解释方面表现优异。

Conclusion: 提出的框架在准确性和可解释性上均表现出色，为恶意软件检测提供了新思路。

Abstract: Control Flow Graphs (CFGs) are critical for analyzing program execution and
characterizing malware behavior. With the growing adoption of Graph Neural
Networks (GNNs), CFG-based representations have proven highly effective for
malware detection. This study proposes a novel framework that dynamically
constructs CFGs and embeds node features using a hybrid approach combining
rule-based encoding and autoencoder-based embedding. A GNN-based classifier is
then constructed to detect malicious behavior from the resulting graph
representations. To improve model interpretability, we apply state-of-the-art
explainability techniques, including GNNExplainer, PGExplainer, and
CaptumExplainer, the latter is utilized three attribution methods: Integrated
Gradients, Guided Backpropagation, and Saliency. In addition, we introduce a
novel aggregation method, called RankFusion, that integrates the outputs of the
top-performing explainers to enhance the explanation quality. We also evaluate
explanations using two subgraph extraction strategies, including the proposed
Greedy Edge-wise Composition (GEC) method for improved structural coherence. A
comprehensive evaluation using accuracy, fidelity, and consistency metrics
demonstrates the effectiveness of the proposed framework in terms of accurate
identification of malware samples and generating reliable and interpretable
explanations.

</details>

### [16] [Property-Preserving Hashing for $\ell_1$-Distance Predicates: Applications to Countering Adversarial Input Attacks](https://arxiv.org/abs/2504.16355)
*Hassan Asghar,Chenhan Zhang,Dali Kaafar*

Main category: cs.CR

TLDR: 本文提出了一种基于ℓ₁距离的保属性哈希（PPH）构造，用于在对抗性环境下检测相似图像，显著提高了抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 感知哈希在对抗性攻击下容易失效，因此需要一种能够保留输入属性的哈希方法，以检测相似图像。

Method: 提出了一种基于ℓ₁距离的PPH构造，通过设定阈值t，迫使攻击者添加显著噪声以规避检测。

Result: 方案高效，运行时间为O(t²)，在28×28灰度图像和224×224 RGB图像上均表现出色。

Conclusion: 该PPH构造能有效抵抗对抗性攻击，同时保持图像质量。

Abstract: Perceptual hashing is used to detect whether an input image is similar to a
reference image with a variety of security applications. Recently, they have
been shown to succumb to adversarial input attacks which make small
imperceptible changes to the input image yet the hashing algorithm does not
detect its similarity to the original image. Property-preserving hashing (PPH)
is a recent construct in cryptography, which preserves some property
(predicate) of its inputs in the hash domain. Researchers have so far shown
constructions of PPH for Hamming distance predicates, which, for instance,
outputs 1 if two inputs are within Hamming distance $t$. A key feature of PPH
is its strong correctness guarantee, i.e., the probability that the predicate
will not be correctly evaluated in the hash domain is negligible. Motivated by
the use case of detecting similar images under adversarial setting, we propose
the first PPH construction for an $\ell_1$-distance predicate. Roughly, this
predicate checks if the two one-sided $\ell_1$-distances between two images are
within a threshold $t$. Since many adversarial attacks use $\ell_2$-distance
(related to $\ell_1$-distance) as the objective function to perturb the input
image, by appropriately choosing the threshold $t$, we can force the attacker
to add considerable noise to evade detection, and hence significantly
deteriorate the image quality. Our proposed scheme is highly efficient, and
runs in time $O(t^2)$. For grayscale images of size $28 \times 28$, we can
evaluate the predicate in $0.0784$ seconds when pixel values are perturbed by
up to $1 \%$. For larger RGB images of size $224 \times 224$, by dividing the
image into 1,000 blocks, we achieve times of $0.0128$ seconds per block for $1
\%$ change, and up to $0.2641$ seconds per block for $14\%$ change.

</details>

### [17] [VideoMark: A Distortion-Free Robust Watermarking Framework for Video Diffusion Models](https://arxiv.org/abs/2504.16359)
*Xuming Hu,Hanqian Li,Jungang Li,Aiwei Liu*

Main category: cs.CR

TLDR: VideoMark是一种无需训练的鲁棒视频水印框架，用于视频扩散模型，通过帧级水印策略和伪随机纠错码嵌入水印信息，同时保持视频质量。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型生成高度逼真视频的能力提升，可靠的内容归属机制变得至关重要。现有图像水印方法难以直接应用于视频，因视频长度可变且易受时间攻击。

Method: 采用帧级水印策略和伪随机纠错码（PRC）嵌入水印，生成扩展水印消息序列并随机选择起始位置。提取时使用时间匹配模块（TMM）通过编辑距离对齐解码消息。

Result: 实验表明，VideoMark的解码精度高于现有方法，视频质量与水印无关生成相当，且水印对无密钥的攻击者不可检测。

Conclusion: VideoMark为基于扩散的视频生成提供了一种无需额外训练且不损害视频质量的内容归属解决方案。

Abstract: This work presents VideoMark, a training-free robust watermarking framework
for video diffusion models. As diffusion models advance in generating highly
realistic videos, the need for reliable content attribution mechanisms has
become critical. While watermarking techniques for image diffusion models have
made progress, directly extending these methods to videos presents unique
challenges due to variable video lengths and vulnerability to temporal attacks.
VideoMark addresses these limitations through a frame-wise watermarking
strategy using pseudorandom error correction (PRC) codes to embed watermark
information during the generation process. Our method generates an extended
watermark message sequence and randomly selects starting positions for each
video, ensuring uniform noise distribution in the latent space and maintaining
generation quality. For watermark extraction, we introduce a Temporal Matching
Module (TMM) that uses edit distance to align decoded messages with the
original watermark sequence, providing robustness against temporal attacks such
as frame deletion. Experimental results demonstrate that VideoMark achieves
higher decoding accuracy than existing methods while maintaining video quality
on par with watermark-free generation. Importantly, our watermark remains
undetectable to attackers without the secret key, ensuring strong
imperceptibility compared to other watermarking frameworks. VideoMark provides
a practical solution for content attribution in diffusion-based video
generation without requiring additional training or compromising video quality.
Our code and data are available at
\href{https://github.com/KYRIE-LI11/VideoMark}{https://github.com/KYRIE-LI11/VideoMark}.

</details>

### [18] [Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection](https://arxiv.org/abs/2504.16429)
*Bo Lin,Shangwen Wang,Yihao Qin,Liqian Chen,Xiaoguang Mao*

Main category: cs.CR

TLDR: CodeGuarder是一个安全加固框架，通过结合功能代码和安全知识提升RACG系统的安全性，显著减少生成的代码中的漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有RACG系统忽视安全性，恶意代码注入可能导致LLMs生成不安全代码，威胁软件开发。

Method: 构建安全知识库，分解查询为子任务并检索相关安全知识，引入重排序和过滤机制优先关键安全指导。

Result: CodeGuarder显著提升代码安全性，标准RACG下平均提升20.12%，在两种中毒场景下分别提升31.53%和21.91%。

Conclusion: CodeGuarder是构建安全可靠RACG系统的重要进展，即使在目标语言安全知识不足时也能提升安全性。

Abstract: Retrieval-Augmented Code Generation (RACG) leverages external knowledge to
enhance Large Language Models (LLMs) in code synthesis, improving the
functional correctness of the generated code. However, existing RACG systems
largely overlook security, leading to substantial risks. Especially, the
poisoning of malicious code into knowledge bases can mislead LLMs, resulting in
the generation of insecure outputs, which poses a critical threat in modern
software development. To address this, we propose a security-hardening
framework for RACG systems, CodeGuarder, that shifts the paradigm from
retrieving only functional code examples to incorporating both functional code
and security knowledge. Our framework constructs a security knowledge base from
real-world vulnerability databases, including secure code samples and root
cause annotations. For each code generation query, a retriever decomposes the
query into fine-grained sub-tasks and fetches relevant security knowledge. To
prioritize critical security guidance, we introduce a re-ranking and filtering
mechanism by leveraging the LLMs' susceptibility to different vulnerability
types. This filtered security knowledge is seamlessly integrated into the
generation prompt. Our evaluation shows CodeGuarder significantly improves code
security rates across various LLMs, achieving average improvements of 20.12\%
in standard RACG, and 31.53\% and 21.91\% under two distinct poisoning
scenarios without compromising functional correctness. Furthermore, CodeGuarder
demonstrates strong generalization, enhancing security even when the targeted
language's security knowledge is lacking. This work presents CodeGuarder as a
pivotal advancement towards building secure and trustworthy RACG systems.

</details>

### [19] [From Past to Present: A Survey of Malicious URL Detection Techniques, Datasets and Code Repositories](https://arxiv.org/abs/2504.16449)
*Ye Tian,Yanqiu Yu,Jianguo Sun,Yanbin Wang*

Main category: cs.CR

TLDR: 本文对恶意URL检测技术进行了全面综述，提出了一种基于模态的新型分类法，并整理了公开数据集和开源实现，以解决现有研究的不足。


<details>
  <summary>Details</summary>
Motivation: 恶意URL持续威胁网络安全，现有综述存在算法分类不清晰、未涵盖LLM/Transformer防御、缺乏开源实现和数据集覆盖不足等问题。

Method: 系统分析从传统黑名单到深度学习方法（如Transformer、GNNs和LLMs）的技术，提出基于模态的分类法，并整理公开数据集和开源实现。

Result: 提出了一种新的分类法，整理了数据集和开源实现，并设计了产品级实现的原则和框架。

Conclusion: 总结了当前挑战并提出了未来研究方向，维护了GitHub仓库以持续更新资源。

Abstract: Malicious URLs persistently threaten the cybersecurity ecosystem, by either
deceiving users into divulging private data or distributing harmful payloads to
infiltrate host systems. Gaining timely insights into the current state of this
ongoing battle holds significant importance. However, existing reviews exhibit
4 critical gaps: 1) Their reliance on algorithm-centric taxonomies obscures
understanding of how detection approaches exploit specific modal information
channels; 2) They fail to incorporate pivotal LLM/Transformer-based defenses;
3) No open-source implementations are collected to facilitate benchmarking; 4)
Insufficient dataset coverage.This paper presents a comprehensive review of
malicious URL detection technologies, systematically analyzing methods from
traditional blacklisting to advanced deep learning approaches (e.g.
Transformer, GNNs, and LLMs). Unlike prior surveys, we propose a novel
modality-based taxonomy that categorizes existing works according to their
primary data modalities (URL, HTML, Visual, etc.). This hierarchical
classification enables both rigorous technical analysis and clear understanding
of multimodal information utilization. Furthermore, to establish a profile of
accessible datasets and address the lack of standardized benchmarking (where
current studies often lack proper baseline comparisons), we curate and analyze:
1) publicly available datasets (2016-2024), and 2) open-source implementations
from published works(2013-2025). Then, we outline essential design principles
and architectural frameworks for product-level implementations. The review
concludes by examining emerging challenges and proposing actionable directions
for future research. We maintain a GitHub repository for ongoing curating
datasets and open-source implementations:
https://github.com/sevenolu7/Malicious-URL-Detection-Open-Source/tree/master.

</details>

### [20] [Seeking Flat Minima over Diverse Surrogates for Improved Adversarial Transferability: A Theoretical Framework and Algorithmic Instantiation](https://arxiv.org/abs/2504.16474)
*Meixi Zheng,Kehan Wu,Yanbo Fan,Rui Huang,Baoyuan Wu*

Main category: cs.CR

TLDR: 该论文提出了一种基于理论的可迁移对抗攻击方法，通过优化对抗样本在代理模型上的平坦性并控制模型间差异，显著提升了对抗样本的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒对抗攻击方法多为启发式设计，缺乏理论支持，本文旨在填补这一空白，提供理论保证并指导攻击算法设计。

Method: 提出了一种理论可迁移性边界，通过优化对抗样本在代理模型集上的平坦性，并控制代理与目标模型间的对抗模型差异。进一步设计了模型多样性兼容的反向对抗扰动（DRAP）方法。

Result: 在NIPS2017和CIFAR-10数据集上的实验表明，该方法显著提升了对抗样本的迁移能力，优于现有方法。

Conclusion: 理论分析为对抗样本的可迁移性提供了新见解，提出的DRAP方法通过模型多样性和平坦性优化，实现了更高效的攻击。

Abstract: The transfer-based black-box adversarial attack setting poses the challenge
of crafting an adversarial example (AE) on known surrogate models that remain
effective against unseen target models. Due to the practical importance of this
task, numerous methods have been proposed to address this challenge. However,
most previous methods are heuristically designed and intuitively justified,
lacking a theoretical foundation. To bridge this gap, we derive a novel
transferability bound that offers provable guarantees for adversarial
transferability. Our theoretical analysis has the advantages of \textit{(i)}
deepening our understanding of previous methods by building a general attack
framework and \textit{(ii)} providing guidance for designing an effective
attack algorithm. Our theoretical results demonstrate that optimizing AEs
toward flat minima over the surrogate model set, while controlling the
surrogate-target model shift measured by the adversarial model discrepancy,
yields a comprehensive guarantee for AE transferability. The results further
lead to a general transfer-based attack framework, within which we observe that
previous methods consider only partial factors contributing to the
transferability. Algorithmically, inspired by our theoretical results, we first
elaborately construct the surrogate model set in which models exhibit diverse
adversarial vulnerabilities with respect to AEs to narrow an instantiated
adversarial model discrepancy. Then, a \textit{model-Diversity-compatible
Reverse Adversarial Perturbation} (DRAP) is generated to effectively promote
the flatness of AEs over diverse surrogate models to improve transferability.
Extensive experiments on NIPS2017 and CIFAR-10 datasets against various target
models demonstrate the effectiveness of our proposed attack.

</details>

### [21] [Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate](https://arxiv.org/abs/2504.16489)
*Senmao Qi,Yifei Zou,Peng Li,Ziyi Lin,Xiuzhen Cheng,Dongxiao Yu*

Main category: cs.CR

TLDR: 多智能体辩论（MAD）通过大型语言模型（LLM）的协作增强复杂任务推理能力，但其迭代对话和角色扮演特性易受越狱攻击，导致有害内容生成。本文研究了四种主流MAD框架的漏洞，并提出一种新型攻击方法，显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: MAD系统在提升推理能力的同时，其安全风险（如越狱攻击）尚未充分研究，亟需系统性评估和防御措施。

Method: 提出一种结构化提示重写框架，利用叙事封装、角色驱动升级、迭代优化和修辞混淆，攻击MAD系统。

Result: 实验表明，MAD系统比单智能体更脆弱，攻击方法将平均有害性从28.14%提升至80.34%，成功率高达80%。

Conclusion: MAD架构存在固有漏洞，需在真实部署前开发专门防御机制。

Abstract: Multi-Agent Debate (MAD), leveraging collaborative interactions among Large
Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks.
However, the security implications of their iterative dialogues and
role-playing characteristics, particularly susceptibility to jailbreak attacks
eliciting harmful content, remain critically underexplored. This paper
systematically investigates the jailbreak vulnerabilities of four prominent MAD
frameworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo,
and DeepSeek) without compromising internal agents. We introduce a novel
structured prompt-rewriting framework specifically designed to exploit MAD
dynamics via narrative encapsulation, role-driven escalation, iterative
refinement, and rhetorical obfuscation. Our extensive experiments demonstrate
that MAD systems are inherently more vulnerable than single-agent setups.
Crucially, our proposed attack methodology significantly amplifies this
fragility, increasing average harmfulness from 28.14% to 80.34% and achieving
attack success rates as high as 80% in certain scenarios. These findings reveal
intrinsic vulnerabilities in MAD architectures and underscore the urgent need
for robust, specialized defenses prior to real-world deployment.

</details>

### [22] [A Collaborative Intrusion Detection System Using Snort IDS Nodes](https://arxiv.org/abs/2504.16550)
*Tom Davies,Max Hashem Eiza,Nathan Shone,Rob Lyon*

Main category: cs.CR

TLDR: 本文提出了一种基于Snort的协作入侵检测系统（CIDS），通过多节点协作和SIEM集成，提高检测精度并减少误报。


<details>
  <summary>Details</summary>
Motivation: 现代网络攻击日益复杂，传统独立IDS难以应对，需要协作机制来提升检测能力。

Method: 设计多Snort节点连接至中心节点，集成SIEM平台实现实时数据共享与分析。

Result: 实验显示CIDS能高效处理大规模流量，提高威胁检测精度，减少误报。

Conclusion: CIDS为构建更具弹性的网络安全系统提供了基础，未来可结合机器学习进一步优化。

Abstract: Intrusion Detection Systems (IDSs) are integral to safeguarding networks by
detecting and responding to threats from malicious traffic or compromised
devices. However, standalone IDS deployments often fall short when addressing
the increasing complexity and scale of modern cyberattacks. This paper proposes
a Collaborative Intrusion Detection System (CIDS) that leverages Snort, an
open-source network intrusion detection system, to enhance detection accuracy
and reduce false positives. The proposed architecture connects multiple Snort
IDS nodes to a centralised node and integrates with a Security Information and
Event Management (SIEM) platform to facilitate real-time data sharing,
correlation, and analysis. The CIDS design includes a scalable configuration of
Snort sensors, a centralised database for log storage, and LogScale SIEM for
advanced analytics and visualisation. By aggregating and analysing intrusion
data from multiple nodes, the system enables improved detection of distributed
and sophisticated attack patterns that standalone IDSs may miss. Performance
evaluation against simulated attacks, including Nmap port scans and ICMP flood
attacks, demonstrates our CIDS's ability to efficiently process large-scale
network traffic, detect threats with higher accuracy, and reduce alert fatigue.
This paper highlights the potential of CIDS in modern network environments and
explores future enhancements, such as integrating machine learning for advanced
threat detection and creating public datasets to support collaborative
research. The proposed CIDS framework provides a promising foundation for
building more resilient and adaptive network security systems.

</details>

### [23] [LaSDVS : A Post-Quantum Secure Compact Strong-Designated Verifier Signature](https://arxiv.org/abs/2504.16571)
*Shanu Poddar,Sweta Mishra,Tapaswini Mohanty,Vikas Srivastava,Sugata Gangopadhyay*

Main category: cs.CR

TLDR: N/A


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Digital signatures are fundamental cryptographic primitives that ensure the
authenticity and integrity of digital communication. However, in scenarios
involving sensitive interactions -- such as e-voting or e-cash -- there is a
growing need for more controlled signing mechanisms. Strong-Designated Verifier
Signature (SDVS) offers such control by allowing the signer to specify and
restrict the verifier of a signature. The existing state-of-the-art SDVS are
mostly based on number-theoretic hardness assumptions. Thus, they are not
secure against quantum attacks. Moreover, Post-Quantum Cryptography (PQC)-based
SDVS are inefficient and have large key and signature sizes. In this work, we
address these challenges and propose an efficient post-quantum SDVS (namely,
LaSDVS) based on ideal lattices under the hardness assumptions of the Ring-SIS
and Ring-LWE problems. LaSDVS achieves advanced security properties including
strong unforgeability under chosen-message attacks, non-transferability,
non-delegatability, and signer anonymity. By employing the algebraic structure
of rings and the gadget trapdoor mechanism of Micciancio et al., we design
LaSDVS to minimize computational overhead and significantly reduce key and
signature sizes. Notably, our scheme achieves a compact signature size of
$\mathcal{O}(n\log q)$, compared to $\mathcal{O}(n^2)$ size, where $n$ is the
security parameter, in the existing state-of-the-art PQC designs. To the best
of our knowledge, LaSDVS offers the \textit{smallest private key and signature
size} among the existing PQC-based SDVS schemes.

</details>

### [24] [Case Study: Fine-tuning Small Language Models for Accurate and Private CWE Detection in Python Code](https://arxiv.org/abs/2504.16584)
*Md. Azizul Hakim Bappy,Hossen A Mustafa,Prottoy Saha,Rajinus Salehat*

Main category: cs.CR

TLDR: 小型语言模型（SLMs）通过微调可高效检测代码漏洞，提供隐私保护的本地解决方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）依赖云基础设施且计算成本高，不适用于敏感或专有代码库的安全分析。

Method: 使用350M参数的预训练代码模型（codegen-mono），通过半监督方法生成数据集并进行微调。

Result: 微调后模型在测试集上达到99%准确率、98.08%精确率、100%召回率和99.04% F1分数。

Conclusion: 微调SLMs是高效、准确的漏洞检测工具，适合本地化安全分析。

Abstract: Large Language Models (LLMs) have demonstrated significant capabilities in
understanding and analyzing code for security vulnerabilities, such as Common
Weakness Enumerations (CWEs). However, their reliance on cloud infrastructure
and substantial computational requirements pose challenges for analyzing
sensitive or proprietary codebases due to privacy concerns and inference costs.
This work explores the potential of Small Language Models (SLMs) as a viable
alternative for accurate, on-premise vulnerability detection. We investigated
whether a 350-million parameter pre-trained code model (codegen-mono) could be
effectively fine-tuned to detect the MITRE Top 25 CWEs specifically within
Python code. To facilitate this, we developed a targeted dataset of 500
examples using a semi-supervised approach involving LLM-driven synthetic data
generation coupled with meticulous human review. Initial tests confirmed that
the base codegen-mono model completely failed to identify CWEs in our samples.
However, after applying instruction-following fine-tuning, the specialized SLM
achieved remarkable performance on our test set, yielding approximately 99%
accuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results
strongly suggest that fine-tuned SLMs can serve as highly accurate and
efficient tools for CWE detection, offering a practical and privacy-preserving
solution for integrating advanced security analysis directly into development
workflows.

</details>

### [25] [Security Science (SecSci), Basic Concepts and Mathematical Foundations](https://arxiv.org/abs/2504.16617)
*Dusko Pavlovic,Peter-Michael Seidel*

Main category: cs.CR

TLDR: 该教材整理了2000年代牛津、2010年代皇家霍洛威以及当前夏威夷的安全课程讲义，适合不同层次的学习需求。


<details>
  <summary>Details</summary>
Motivation: 为不同层次的安全课程提供统一的教学材料，涵盖从基础到高级的内容，并引入研究问题。

Method: 整理和汇编多年来的课程讲义，按难度和内容层次组织章节。

Result: 形成了一本适合从入门到高级课程及研究的综合性教材。

Conclusion: 该教材为安全领域的学习和教学提供了全面且分层次的资源。

Abstract: This textbook compiles the lecture notes from security courses taught at
Oxford in the 2000s, at Royal Holloway in the 2010s, and currently in Hawaii.
The early chapters are suitable for a first course in security. The middle
chapters have been used in advanced courses. Towards the end there are also
some research problems.

</details>

### [26] [MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark](https://arxiv.org/abs/2504.16651)
*William Corrias,Fabio De Gaspari,Dorjan Hitaj,Luigi V. Mancini*

Main category: cs.CR

TLDR: MAYA是一个统一的密码基准测试框架，用于评估生成式密码猜测模型，发现序列模型在生成复杂密码时表现最佳。


<details>
  <summary>Details</summary>
Motivation: 生成模型在密码猜测领域的潜力尚未被充分理解，缺乏标准化评估方法。

Method: 引入MAYA框架，通过高级测试场景和真实密码数据集评估六种最先进模型。

Result: 序列模型表现最佳，多模型攻击效果优于单一模型，但对长复杂密码效果不一。

Conclusion: MAYA为密码生成技术提供了标准化评估工具，促进进一步研究。

Abstract: The rapid evolution of generative models has led to their integration across
various fields, including password guessing, aiming to generate passwords that
resemble human-created ones in complexity, structure, and patterns. Despite
generative model's promise, inconsistencies in prior research and a lack of
rigorous evaluation have hindered a comprehensive understanding of their true
potential. In this paper, we introduce MAYA, a unified, customizable,
plug-and-play password benchmarking framework. MAYA provides a standardized
approach for evaluating generative password-guessing models through a rigorous
set of advanced testing scenarios and a collection of eight real-life password
datasets. Using MAYA, we comprehensively evaluate six state-of-the-art
approaches, which have been re-implemented and adapted to ensure
standardization, for a total of over 15,000 hours of computation. Our findings
indicate that these models effectively capture different aspects of human
password distribution and exhibit strong generalization capabilities. However,
their effectiveness varies significantly with long and complex passwords.
Through our evaluation, sequential models consistently outperform other
generative architectures and traditional password-guessing tools, demonstrating
unique capabilities in generating accurate and complex guesses. Moreover,
models learn and generate different password distributions, enabling a
multi-model attack that outperforms the best individual model. By releasing
MAYA, we aim to foster further research, providing the community with a new
tool to consistently and reliably benchmark password-generation techniques. Our
framework is publicly available at
https://github.com/williamcorrias/MAYA-Password-Benchmarking

</details>

### [27] [CAIBA: Multicast Source Authentication for CAN Through Reactive Bit Flipping](https://arxiv.org/abs/2504.16695)
*Eric Wagner,Frederik Basels,Jan Bauer,Till Zimmermann,Klaus Wehrle,Martin Henze*

Main category: cs.CR

TLDR: CAIBA是一种新型的多播源认证方案，专为CAN等通信总线设计，通过动态覆盖认证标签来防止伪装攻击，同时兼容传统CAN设备。


<details>
  <summary>Details</summary>
Motivation: CAN网络设计时未考虑安全性，现有安全措施无法有效防止伪装攻击，因此需要一种可靠的多播源认证方案。

Method: CAIBA采用动态覆盖认证标签的机制，结合特殊的消息认证方案和反应性位覆盖机制，确保消息来源可验证。

Result: CAIBA在不增加通信开销或验证延迟的情况下，保护接收设备免受伪装攻击，并与传统CAN设备兼容。

Conclusion: CAIBA为CAN网络提供了一种高效、兼容的多播源认证解决方案，弥补了现有安全措施的不足。

Abstract: Controller Area Networks (CANs) are the backbone for reliable intra-vehicular
communication. Recent cyberattacks have, however, exposed the weaknesses of
CAN, which was designed without any security considerations in the 1980s.
Current efforts to retrofit security via intrusion detection or message
authentication codes are insufficient to fully secure CAN as they cannot
adequately protect against masquerading attacks, where a compromised
communication device, a so-called electronic control units, imitates another
device. To remedy this situation, multicast source authentication is required
to reliably identify the senders of messages. In this paper, we present CAIBA,
a novel multicast source authentication scheme specifically designed for
communication buses like CAN. CAIBA relies on an authenticator overwriting
authentication tags on-the-fly, such that a receiver only reads a valid tag if
not only the integrity of a message but also its source can be verified. To
integrate CAIBA into CAN, we devise a special message authentication scheme and
a reactive bit overwriting mechanism. We achieve interoperability with legacy
CAN devices, while protecting receivers implementing the AUTOSAR SecOC standard
against masquerading attacks without communication overhead or verification
delays.

</details>

### [28] [Snorkeling in dark waters: A longitudinal surface exploration of unique Tor Hidden Services (Extended Version)](https://arxiv.org/abs/2504.16836)
*Alfonso Rodriguez Barredo-Valenzuela,Sergio Pastrana Portillo,Guillermo Suarez-Tangil*

Main category: cs.CR

TLDR: 本文通过大规模分析Tor网络，揭示了暗网中82%的内容是镜像，并提出了优化爬取过程的方法。


<details>
  <summary>Details</summary>
Motivation: Tor网络的匿名性使其成为合法用户和犯罪分子的双重工具，但其隐藏服务的不可见性导致对其规模和内容类型的不确定性。

Method: 使用名为Mimir的爬虫收集25k多个站点的数据，分析网络拓扑结构，检测镜像内容，并训练机器学习分类器识别内容类型。

Result: 发现82%的暗网内容是镜像，揭示了初始种子选择对爬取效率的重要性，并指出以往研究忽略了镜像问题。

Conclusion: 研究提供了对Tor网络的新见解，强调了优化爬取过程和初始种子选择的重要性，同时指出以往研究的局限性。

Abstract: The Onion Router (Tor) is a controversial network whose utility is constantly
under scrutiny. On the one hand, it allows for anonymous interaction and
cooperation of users seeking untraceable navigation on the Internet. This
freedom also attracts criminals who aim to thwart law enforcement
investigations, e.g., trading illegal products or services such as drugs or
weapons. Tor allows delivering content without revealing the actual hosting
address, by means of .onion (or hidden) services. Different from regular
domains, these services can not be resolved by traditional name services, are
not indexed by regular search engines, and they frequently change. This
generates uncertainty about the extent and size of the Tor network and the type
of content offered.
  In this work, we present a large-scale analysis of the Tor Network. We
leverage our crawler, dubbed Mimir, which automatically collects and visits
content linked within the pages to collect a dataset of pages from more than
25k sites. We analyze the topology of the Tor Network, including its depth and
reachability from the surface web. We define a set of heuristics to detect the
presence of replicated content (mirrors) and show that most of the analyzed
content in the Dark Web (82% approx.) is a replica of other content. Also, we
train a custom Machine Learning classifier to understand the type of content
the hidden services offer. Overall, our study provides new insights into the
Tor network, highlighting the importance of initial seeding for focus on
specific topics, and optimize the crawling process. We show that previous work
on large-scale Tor measurements does not consider the presence of mirrors,
which biases their understanding of the Dark Web topology and the distribution
of content.

</details>

### [29] [Building A Secure Agentic AI Application Leveraging A2A Protocol](https://arxiv.org/abs/2504.16902)
*Idan Habler,Ken Huang,Vineeth Sai Narajala,Prashant Kulkarni*

Main category: cs.CR

TLDR: 本文对Google的Agent2Agent（A2A）协议进行了全面的安全分析，提出了实用的安全开发方法和架构最佳实践，以支持下一代AI代理应用的可靠性和安全性。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理系统从基本工作流发展为复杂的多代理协作，确保A2A协议的安全实现成为关键需求。

Method: 利用MAESTRO框架进行主动威胁建模，评估A2A部署中的潜在安全问题，重点关注代理卡管理、任务执行完整性和认证方法。

Result: 提出了安全开发方法和架构建议，并探讨了A2A与模型上下文协议（MCP）的协同作用，以增强安全互操作性。

Conclusion: 本文为开发者和架构师提供了构建安全可靠的下一代AI代理应用所需的知识和实践指导。

Abstract: As Agentic AI systems evolve from basic workflows to complex multi agent
collaboration, robust protocols such as Google's Agent2Agent (A2A) become
essential enablers. To foster secure adoption and ensure the reliability of
these complex interactions, understanding the secure implementation of A2A is
essential. This paper addresses this goal by providing a comprehensive security
analysis centered on the A2A protocol. We examine its fundamental elements and
operational dynamics, situating it within the framework of agent communication
development. Utilizing the MAESTRO framework, specifically designed for AI
risks, we apply proactive threat modeling to assess potential security issues
in A2A deployments, focusing on aspects such as Agent Card management, task
execution integrity, and authentication methodologies.
  Based on these insights, we recommend practical secure development
methodologies and architectural best practices designed to build resilient and
effective A2A systems. Our analysis also explores how the synergy between A2A
and the Model Context Protocol (MCP) can further enhance secure
interoperability. This paper equips developers and architects with the
knowledge and practical guidance needed to confidently leverage the A2A
protocol for building robust and secure next generation agentic applications.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [Representation Learning for Tabular Data: A Comprehensive Survey](https://arxiv.org/abs/2504.16109)
*Jun-Peng Jiang,Si-Yang Liu,Hao-Run Cai,Qile Zhou,Han-Jia Ye*

Main category: cs.LG

TLDR: 该论文系统综述了表格表示学习领域，包括背景、挑战、基准及深度神经网络（DNNs）的优缺点，并将现有方法分为专用、可迁移和通用模型三类。


<details>
  <summary>Details</summary>
Motivation: 表格数据是机器学习分类和回归中最常见的数据类型之一，近年来DNNs在表格表示学习中展现出潜力，但缺乏系统性的综述。

Method: 论文将方法分为专用、可迁移和通用模型三类，并详细探讨了特征、样本和目标等关键方面的策略。

Result: 提出了一个层次化分类法，并总结了高质量特征和样本表示的策略，同时探讨了集成方法和扩展应用。

Conclusion: 表格表示学习领域仍有广阔的研究空间，尤其是在开放环境、多模态学习和表格理解等方面。

Abstract: Tabular data, structured as rows and columns, is among the most prevalent
data types in machine learning classification and regression applications.
Models for learning from tabular data have continuously evolved, with Deep
Neural Networks (DNNs) recently demonstrating promising results through their
capability of representation learning. In this survey, we systematically
introduce the field of tabular representation learning, covering the
background, challenges, and benchmarks, along with the pros and cons of using
DNNs. We organize existing methods into three main categories according to
their generalization capabilities: specialized, transferable, and general
models. Specialized models focus on tasks where training and evaluation occur
within the same data distribution. We introduce a hierarchical taxonomy for
specialized models based on the key aspects of tabular data -- features,
samples, and objectives -- and delve into detailed strategies for obtaining
high-quality feature- and sample-level representations. Transferable models are
pre-trained on one or more datasets and subsequently fine-tuned on downstream
tasks, leveraging knowledge acquired from homogeneous or heterogeneous sources,
or even cross-modalities such as vision and language. General models, also
known as tabular foundation models, extend this concept further, allowing
direct application to downstream tasks without fine-tuning. We group these
general models based on the strategies used to adapt across heterogeneous
datasets. Additionally, we explore ensemble methods, which integrate the
strengths of multiple tabular models. Finally, we discuss representative
extensions of tabular learning, including open-environment tabular machine
learning, multimodal learning with tabular data, and tabular understanding.
More information can be found in the following repository:
https://github.com/LAMDA-Tabular/Tabular-Survey.

</details>

### [31] [Active Learning Methods for Efficient Data Utilization and Model Performance Enhancement](https://arxiv.org/abs/2504.16136)
*Chiung-Yi Tseng,Junhao Song,Ziqian Bi,Tianyang Wang,Chia Xin Liang,Ming Liu*

Main category: cs.LG

TLDR: 本文概述了主动学习（AL）在解决数据丰富但标注稀缺问题中的作用，介绍了其基本概念、应用领域及研究重点，并探讨了当前挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的智能时代中，数据丰富但标注稀缺成为机器学习发展的瓶颈，主动学习作为一种高效利用标注数据的方法备受关注。

Method: 介绍了主动学习的基本概念及其在计算机视觉、自然语言处理等领域的应用，重点讨论了不确定性估计、类别不平衡处理等研究主题。

Result: 主动学习在良好评估指标下通常优于被动学习，能提高数据效率和模型性能。

Conclusion: 本文为研究者和实践者提供了关键见解，并提出了主动学习未来发展的方向，包括重建信任、确保可重复性等挑战。

Abstract: In the era of data-driven intelligence, the paradox of data abundance and
annotation scarcity has emerged as a critical bottleneck in the advancement of
machine learning. This paper gives a detailed overview of Active Learning (AL),
which is a strategy in machine learning that helps models achieve better
performance using fewer labeled examples. It introduces the basic concepts of
AL and discusses how it is used in various fields such as computer vision,
natural language processing, transfer learning, and real-world applications.
The paper focuses on important research topics such as uncertainty estimation,
handling of class imbalance, domain adaptation, fairness, and the creation of
strong evaluation metrics and benchmarks. It also shows that learning methods
inspired by humans and guided by questions can improve data efficiency and help
models learn more effectively. In addition, this paper talks about current
challenges in the field, including the need to rebuild trust, ensure
reproducibility, and deal with inconsistent methodologies. It points out that
AL often gives better results than passive learning, especially when good
evaluation measures are used. This work aims to be useful for both researchers
and practitioners by providing key insights and proposing directions for future
progress in active learning.

</details>

### [32] [SparseJEPA: Sparse Representation Learning of Joint Embedding Predictive Architectures](https://arxiv.org/abs/2504.16140)
*Max Hartman,Lav Varshney*

Main category: cs.LG

TLDR: SparseJEPA通过稀疏表示学习增强JEPA框架，提升表示质量和可解释性，并在CIFAR-100数据集和轻量级Vision Transformer上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: JEPA框架在通用表示学习中表现强大，但存在表示密集和可解释性不足的问题。

Method: SparseJEPA引入稀疏表示学习，通过惩罚方法鼓励潜在空间变量在语义相关的数据特征间共享。

Result: 实验表明，SparseJEPA在图像分类和低级任务中表现优异，理论证明分组机制能提升表示质量。

Conclusion: 稀疏性不仅优化了潜在空间，还促进了更具意义和可解释的表示学习，未来将探索对象中心表示学习。

Abstract: Joint Embedding Predictive Architectures (JEPA) have emerged as a powerful
framework for learning general-purpose representations. However, these models
often lack interpretability and suffer from inefficiencies due to dense
embedding representations. We propose SparseJEPA, an extension that integrates
sparse representation learning into the JEPA framework to enhance the quality
of learned representations. SparseJEPA employs a penalty method that encourages
latent space variables to be shared among data features with strong semantic
relationships, while maintaining predictive performance. We demonstrate the
effectiveness of SparseJEPA by training on the CIFAR-100 dataset and
pre-training a lightweight Vision Transformer. The improved embeddings are
utilized in linear-probe transfer learning for both image classification and
low-level tasks, showcasing the architecture's versatility across different
transfer tasks. Furthermore, we provide a theoretical proof that demonstrates
that the grouping mechanism enhances representation quality. This was done by
displaying that grouping reduces Multiinformation among latent-variables,
including proofing the Data Processing Inequality for Multiinformation. Our
results indicate that incorporating sparsity not only refines the latent space
but also facilitates the learning of more meaningful and interpretable
representations. In further work, hope to further extend this method by finding
new ways to leverage the grouping mechanism through object-centric
representation learning.

</details>

### [33] [Deep Learning Meets Process-Based Models: A Hybrid Approach to Agricultural Challenges](https://arxiv.org/abs/2504.16141)
*Yue Shi,Liangxiu Han,Xin Zhang,Tam Sobeih,Thomas Gaiser,Nguyen Huu Thuy,Dominik Behrend,Amit Kumar Srivastava,Krishnagopal Halder,Frank Ewert*

Main category: cs.LG

TLDR: 本文系统综述了基于过程的模型（PBMs）和深度学习（DL）在农业建模中的应用，提出混合PBM-DL框架，并通过案例研究验证其优越性。


<details>
  <summary>Details</summary>
Motivation: PBMs和DL各有优缺点，PBMs具有可解释性但难以扩展，DL能处理复杂模式但缺乏解释性。研究旨在结合两者优势，提升农业建模的鲁棒性和泛化能力。

Method: 对PBMs、DL及混合PBM-DL框架进行系统综述，并通过作物干生物量预测案例比较其性能。

Result: 混合模型在数据质量和空间条件下均优于传统PBMs和DL，具有更强的鲁棒性和泛化能力。

Conclusion: 混合模型结合领域知识与AI方法，为可持续农业提供可扩展、可解释的决策支持工具。

Abstract: Process-based models (PBMs) and deep learning (DL) are two key approaches in
agricultural modelling, each offering distinct advantages and limitations. PBMs
provide mechanistic insights based on physical and biological principles,
ensuring interpretability and scientific rigour. However, they often struggle
with scalability, parameterisation, and adaptation to heterogeneous
environments. In contrast, DL models excel at capturing complex, nonlinear
patterns from large datasets but may suffer from limited interpretability, high
computational demands, and overfitting in data-scarce scenarios.
  This study presents a systematic review of PBMs, DL models, and hybrid PBM-DL
frameworks, highlighting their applications in agricultural and environmental
modelling. We classify hybrid PBM-DL approaches into DL-informed PBMs, where
neural networks refine process-based models, and PBM-informed DL, where
physical constraints guide deep learning predictions. Additionally, we conduct
a case study on crop dry biomass prediction, comparing hybrid models against
standalone PBMs and DL models under varying data quality, sample sizes, and
spatial conditions. The results demonstrate that hybrid models consistently
outperform traditional PBMs and DL models, offering greater robustness to noisy
data and improved generalisation across unseen locations.
  Finally, we discuss key challenges, including model interpretability,
scalability, and data requirements, alongside actionable recommendations for
advancing hybrid modelling in agriculture. By integrating domain knowledge with
AI-driven approaches, this study contributes to the development of scalable,
interpretable, and reproducible agricultural models that support data-driven
decision-making for sustainable agriculture.

</details>

### [34] [Hexcute: A Tile-based Programming Language with Automatic Layout and Task-Mapping Synthesis](https://arxiv.org/abs/2504.16214)
*Xiao Zhang,Yaoyao Ding,Yang Hu,Gennady Pekhimenko*

Main category: cs.LG

TLDR: Hexcute是一种基于瓦片的编程语言，旨在简化混合数据类型矩阵乘法算子的GPU优化，通过共享内存和寄存器抽象实现细粒度优化，并自动生成布局和任务映射。


<details>
  <summary>Details</summary>
Motivation: 深度学习量化技术需要处理混合数据类型的矩阵乘法算子，现有编译器或编程模型在表达性和工程效率上存在不足。

Method: Hexcute通过瓦片编程语言暴露共享内存和寄存器抽象，结合任务映射调度，并利用类型推断算法自动生成布局和任务映射。

Result: Hexcute在混合类型算子上的性能优于现有DL编译器，速度提升1.7-11.28倍，端到端评估中最高提升2.91倍。

Conclusion: Hexcute在表达性和工程效率之间取得平衡，为混合数据类型算子提供了高效的GPU优化方案。

Abstract: Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL
quantization techniques demand a new matrix multiplication operator with mixed
input data types, further complicating GPU optimization. Prior high-level
compilers like Triton lack the expressiveness to implement key optimizations
like fine-grained data pipelines and hardware-friendly memory layouts for these
operators, while low-level programming models, such as Hidet, Graphene, and
CUTLASS, require significant programming efforts. To balance expressiveness
with engineering effort, we propose Hexcute, a tile-based programming language
that exposes shared memory and register abstractions to enable fine-grained
optimization for these operators. Additionally, Hexcute leverages task mapping
to schedule the GPU program, and to reduce programming efforts, it automates
layout and task mapping synthesis with a novel type-inference-based algorithm.
Our evaluation shows that Hexcute generalizes to a wide range of DL operators,
achieves 1.7-11.28$\times$ speedup over existing DL compilers for mixed-type
operators, and brings up to 2.91$\times$ speedup in the end-to-end evaluation.

</details>

### [35] [Using Phonemes in cascaded S2S translation pipeline](https://arxiv.org/abs/2504.16234)
*Rene Pilz,Johannes Schneider*

Main category: cs.LG

TLDR: 论文探讨了在语音到语音翻译中使用音素代替传统文本表示的可行性，发现音素方法在质量相当的同时具有资源需求低等优势。


<details>
  <summary>Details</summary>
Motivation: 研究音素作为文本表示在语音翻译中的潜力，以解决传统文本表示的资源需求问题。

Method: 在WMT17数据集上训练序列到序列模型，比较标准文本表示和音素表示的性能。

Result: 音素方法在BLEU指标上表现相当，但资源需求更低，更适合低资源语言。

Conclusion: 音素表示在语音翻译中具有潜力，尤其适用于资源受限的场景。

Abstract: This paper explores the idea of using phonemes as a textual representation
within a conventional multilingual simultaneous speech-to-speech translation
pipeline, as opposed to the traditional reliance on text-based language
representations. To investigate this, we trained an open-source
sequence-to-sequence model on the WMT17 dataset in two formats: one using
standard textual representation and the other employing phonemic
representation. The performance of both approaches was assessed using the BLEU
metric. Our findings shows that the phonemic approach provides comparable
quality but offers several advantages, including lower resource requirements or
better suitability for low-resource languages.

</details>

### [36] [General Post-Processing Framework for Fairness Adjustment of Machine Learning Models](https://arxiv.org/abs/2504.16238)
*Léandre Eberhard,Nirek Sharma,Filipp Shelobolin,Aalok Ganesh Shanbhag*

Main category: cs.LG

TLDR: 该论文提出了一种新颖的公平性调整框架，适用于多种机器学习任务，通过解耦公平性调整与模型训练过程，保持模型性能的同时提供更大的灵活性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在信贷评估、公共政策和人才招聘等关键领域的影响日益增加，确保公平性成为法律和伦理的迫切需求。

Method: 该方法将传统的处理中技术调整为后处理步骤，无需自定义损失函数，支持使用不同数据集进行公平性调优，并兼容黑盒系统。

Result: 与Adversarial Debiasing相比，该框架在真实数据集上实现了类似的公平性/准确性权衡。

Conclusion: 该框架为公平性调整提供了灵活且可解释的解决方案，适用于多种场景。

Abstract: As machine learning increasingly influences critical domains such as credit
underwriting, public policy, and talent acquisition, ensuring compliance with
fairness constraints is both a legal and ethical imperative. This paper
introduces a novel framework for fairness adjustments that applies to diverse
machine learning tasks, including regression and classification, and
accommodates a wide range of fairness metrics. Unlike traditional approaches
categorized as pre-processing, in-processing, or post-processing, our method
adapts in-processing techniques for use as a post-processing step. By
decoupling fairness adjustments from the model training process, our framework
preserves model performance on average while enabling greater flexibility in
model development. Key advantages include eliminating the need for custom loss
functions, enabling fairness tuning using different datasets, accommodating
proprietary models as black-box systems, and providing interpretable insights
into the fairness adjustments. We demonstrate the effectiveness of this
approach by comparing it to Adversarial Debiasing, showing that our framework
achieves a comparable fairness/accuracy tradeoff on real-world datasets.

</details>

### [37] [FairPlay: A Collaborative Approach to Mitigate Bias in Datasets for Improved AI Fairness](https://arxiv.org/abs/2504.16255)
*Tina Behzad,Mithilesh Kumar Singh,Anthony J. Ripa,Klaus Mueller*

Main category: cs.LG

TLDR: FairPlay是一个基于网络的协作工具，帮助多方利益相关者在没有统一公平标准的情况下达成共识，通过谈判和修改数据集来实现公平。


<details>
  <summary>Details</summary>
Motivation: 解决决策公平性问题，尤其是当不同利益相关者对公平有不同且互不相容的定义时。

Method: 开发了一个名为FairPlay的软件应用，支持多方协作去偏数据集，并通过谈判达成共识。

Result: 用户研究表明，FairPlay能在约五轮游戏内帮助用户达成共识，展示了其在提升AI系统公平性方面的潜力。

Conclusion: FairPlay为缺乏系统谈判工具的情况提供了一种有效解决方案，能够促进多方在公平性问题上的合作。

Abstract: The issue of fairness in decision-making is a critical one, especially given
the variety of stakeholder demands for differing and mutually incompatible
versions of fairness. Adopting a strategic interaction of perspectives provides
an alternative to enforcing a singular standard of fairness. We present a
web-based software application, FairPlay, that enables multiple stakeholders to
debias datasets collaboratively. With FairPlay, users can negotiate and arrive
at a mutually acceptable outcome without a universally agreed-upon theory of
fairness. In the absence of such a tool, reaching a consensus would be highly
challenging due to the lack of a systematic negotiation process and the
inability to modify and observe changes. We have conducted user studies that
demonstrate the success of FairPlay, as users could reach a consensus within
about five rounds of gameplay, illustrating the application's potential for
enhancing fairness in AI systems.

</details>

### [38] [Learning Energy-Based Generative Models via Potential Flow: A Variational Principle Approach to Probability Density Homotopy Matching](https://arxiv.org/abs/2504.16262)
*Junn Yong Loo,Michelle Adeline,Julia Kaiwen Lau,Fang Yu Leong,Hwa Hui Tew,Arghya Pal,Vishnu Monn Baskaran,Chee-Ming Ting,Raphaël C. -W. Phan*

Main category: cs.LG

TLDR: VPFB是一种新的基于能量的生成框架，通过变分损失学习能量参数化的势流，避免了隐式MCMC采样的不稳定性，并在多种生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: EBMs虽然强大，但势流与显式EBMs的关系研究不足，且隐式MCMC采样在高维场景下不稳定且昂贵。

Method: VPFB通过构建流驱动的密度同伦，并通过变分损失最小化KL散度来匹配数据分布，无需隐式MCMC采样或辅助网络。

Result: 实验表明，VPFB在图像生成、插值、分布外检测和组合生成等任务中表现优异，与现有方法竞争力相当。

Conclusion: VPFB提供了一种高效且稳健的生成建模方法，同时保留了EBMs的可解释性。

Abstract: Energy-based models (EBMs) are a powerful class of probabilistic generative
models due to their flexibility and interpretability. However, relationships
between potential flows and explicit EBMs remain underexplored, while
contrastive divergence training via implicit Markov chain Monte Carlo (MCMC)
sampling is often unstable and expensive in high-dimensional settings. In this
paper, we propose Variational Potential Flow Bayes (VPFB), a new energy-based
generative framework that eliminates the need for implicit MCMC sampling and
does not rely on auxiliary networks or cooperative training. VPFB learns an
energy-parameterized potential flow by constructing a flow-driven density
homotopy that is matched to the data distribution through a variational loss
minimizing the Kullback-Leibler divergence between the flow-driven and marginal
homotopies. This principled formulation enables robust and efficient generative
modeling while preserving the interpretability of EBMs. Experimental results on
image generation, interpolation, out-of-distribution detection, and
compositional generation confirm the effectiveness of VPFB, showing that our
method performs competitively with existing approaches in terms of sample
quality and versatility across diverse generative modeling tasks.

</details>

### [39] [Gradient-Optimized Fuzzy Classifier: A Benchmark Study Against State-of-the-Art Models](https://arxiv.org/abs/2504.16263)
*Magnus Sieverding,Nathan Steffen,Kelly Cohen*

Main category: cs.LG

TLDR: 本文比较了梯度优化模糊推理系统（GF）与多种先进机器学习模型的性能，结果显示GF在分类准确性和训练效率上表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索梯度优化的模糊推理系统是否能在保持高精度的同时提升训练效率，并验证其作为可解释模型的潜力。

Method: 方法包括在五个UCI数据集上对GF、随机森林、XGBoost、逻辑回归、支持向量机和神经网络进行性能对比。GF采用梯度下降优化，而非传统模糊系统的无导数方法。

Result: 结果表明，GF在分类准确性上具有竞争力甚至更优，同时训练时间极短，且在噪声数据和多变特征集上表现稳健。

Conclusion: 结论是梯度优化模糊系统可作为高效、可解释的替代方案，适用于监督学习任务。

Abstract: This paper presents a performance benchmarking study of a Gradient-Optimized
Fuzzy Inference System (GF) classifier against several state-of-the-art machine
learning models, including Random Forest, XGBoost, Logistic Regression, Support
Vector Machines, and Neural Networks. The evaluation was conducted across five
datasets from the UCI Machine Learning Repository, each chosen for their
diversity in input types, class distributions, and classification complexity.
Unlike traditional Fuzzy Inference Systems that rely on derivative-free
optimization methods, the GF leverages gradient descent to significantly
improving training efficiency and predictive performance. Results demonstrate
that the GF model achieved competitive, and in several cases superior,
classification accuracy while maintaining high precision and exceptionally low
training times. In particular, the GF exhibited strong consistency across folds
and datasets, underscoring its robustness in handling noisy data and variable
feature sets. These findings support the potential of gradient optimized fuzzy
systems as interpretable, efficient, and adaptable alternatives to more complex
deep learning models in supervised learning tasks.

</details>

### [40] [Boosting Classifier Performance with Opposition-Based Data Transformation](https://arxiv.org/abs/2504.16268)
*Abdesslem Layeb*

Main category: cs.LG

TLDR: 提出了一种基于对立学习（OBL）的数据转换框架，显著提升了传统分类算法的性能。


<details>
  <summary>Details</summary>
Motivation: 通过生成对立样本优化训练数据，改善决策边界形成，从而提升分类性能。

Method: 探索了三种OBL变体（全局OBL、类内OBL和局部类内OBL），并与KNN、SVM、LR和DT等分类器结合。

Result: 在26个异构和高维数据集上的实验表明，OBL增强的分类器在准确率和F1分数上均优于传统方法，且计算效率更高。

Conclusion: OBL是一种轻量但强大的数据转换策略，特别适用于复杂或稀疏的学习环境。

Abstract: In this paper, we introduce a novel data transformation framework based on
Opposition-Based Learning (OBL) to boost the performance of traditional
classification algorithms. Originally developed to accelerate convergence in
optimization tasks, OBL is leveraged here to generate synthetic opposite
samples that replace the acutely training data and improve decision boundary
formation. We explore three OBL variants; Global OBL, Class-Wise OBL, and
Localized Class-Wise OBL; and integrate them with several widely used
classifiers, including K-Nearest Neighbors (KNN), Support Vector Machines
(SVM), Logistic Regression (LR), and Decision Tree (DT). Extensive experiments
conducted on 26 heterogeneous and high-dimensional datasets demonstrate that
OBL-enhanced classifiers consistently outperform their standard counterparts in
terms of accuracy and F1-score, frequently achieving near-perfect or perfect
classification. Furthermore, OBL contributes to improved computational
efficiency, particularly in SVM and LR. These findings underscore the potential
of OBL as a lightweight yet powerful data transformation strategy for enhancing
classification performance, especially in complex or sparse learning
environments.

</details>

### [41] [Learning Explainable Dense Reward Shapes via Bayesian Optimization](https://arxiv.org/abs/2504.16272)
*Ryan Koo,Ian Yang,Vipul Raheja,Mingyi Hong,Kwang-Sung Jun,Dongyeop Kang*

Main category: cs.LG

TLDR: 论文提出了一种基于可解释性方法（如SHAP和LIME）的奖励塑造函数，用于优化LLM对齐中的令牌级信用分配，并通过双层优化框架学习参数，实验表明其性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 当前RLHF流程中，标量奖励分配给序列会导致稀疏反馈和次优的令牌级信用分配，因此需要改进。

Method: 提出一种奖励塑造函数，利用SHAP和LIME估计每个令牌的奖励，并通过双层优化框架（结合贝叶斯优化和策略训练）学习参数。

Result: 实验表明，该方法在性能上优于基线，并在训练中更快找到最优策略。

Conclusion: 基于可解释性方法的令牌级奖励塑造能有效提升LLM对齐性能，且保持原始奖励的最优策略。

Abstract: Current reinforcement learning from human feedback (RLHF) pipelines for large
language model (LLM) alignment typically assign scalar rewards to sequences,
using the final token as a surrogate indicator for the quality of the entire
sequence. However, this leads to sparse feedback and suboptimal token-level
credit assignment. In this work, we frame reward shaping as an optimization
problem focused on token-level credit assignment. We propose a reward-shaping
function leveraging explainability methods such as SHAP and LIME to estimate
per-token rewards from the reward model. To learn parameters of this shaping
function, we employ a bilevel optimization framework that integrates Bayesian
Optimization and policy training to handle noise from the token reward
estimates. Our experiments show that achieving a better balance of token-level
reward attribution leads to performance improvements over baselines on
downstream tasks and finds an optimal policy faster during training.
Furthermore, we show theoretically that explainability methods that are feature
additive attribution functions maintain the optimal policy as the original
reward.

</details>

### [42] [Quantum Doubly Stochastic Transformers](https://arxiv.org/abs/2504.16275)
*Jannis Born,Filip Skogh,Kahn Rhrissorrakrai,Filippo Utro,Nico Wagner,Aleksandros Sobczyk*

Main category: cs.LG

TLDR: 论文提出了一种混合经典-量子双随机Transformer（QDSFormer），用变分量子电路替代Softmax，实验表明其在多个任务中性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer中Softmax可能导致训练不稳定，而Sinkhorn算法虽能改善性能但缺乏灵活性。量子电路提供了一种新的双随机矩阵生成方法。

Method: 用变分量子电路替代Softmax，生成双随机矩阵，并研究其表达能力。

Result: QDSFormer在多个小规模任务中性能优于标准Vision Transformer和其他双随机Transformer，且训练更稳定。

Conclusion: QDSFormer展示了量子电路在Transformer中的潜力，可能缓解小规模数据下ViT训练不稳定的问题。

Abstract: At the core of the Transformer, the Softmax normalizes the attention matrix
to be right stochastic. Previous research has shown that this often
destabilizes training and that enforcing the attention matrix to be doubly
stochastic (through Sinkhorn's algorithm) consistently improves performance
across different tasks, domains and Transformer flavors. However, Sinkhorn's
algorithm is iterative, approximative, non-parametric and thus inflexible
w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been
proven that DSMs can be obtained with a parametric quantum circuit, yielding a
novel quantum inductive bias for DSMs with no known classical analogue.
Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum
doubly stochastic Transformer (QDSFormer) that replaces the Softmax in the
self-attention layer with a variational quantum circuit. We study the
expressive power of the circuit and find that it yields more diverse DSMs that
better preserve information than classical operators. Across multiple
small-scale object recognition tasks, we find that our QDSFormer consistently
surpasses both a standard Vision Transformer and other doubly stochastic
Transformers. Beyond the established Sinkformer, this comparison includes a
novel quantum-inspired doubly stochastic Transformer (based on QR
decomposition) that can be of independent interest. The QDSFormer also shows
improved training stability and lower performance variation suggesting that it
may mitigate the notoriously unstable training of ViTs on small-scale data.

</details>

### [43] [An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon](https://arxiv.org/abs/2504.16276)
*Abhishek Jana,Moeumu Uili,James Atherton,Mark O'Brien,Joe Wood,Leandra Brickson*

Main category: cs.LG

TLDR: 本文提出了一种针对稀有鸟类的自动化单次鸟鸣分类方法，解决了现有分类器（如BirdNET和Perch）因缺乏训练数据而无法识别的问题。


<details>
  <summary>Details</summary>
Motivation: 现有分类器对常见鸟类表现良好，但对仅有1-3条录音的濒危物种无能为力，这对保护濒危鸟类的研究至关重要。

Method: 利用大型鸟类分类网络的嵌入空间，结合余弦相似度分类器、过滤和降噪预处理技术，优化了在极少训练数据下的检测效果。

Result: 在模拟和真实场景（极危物种齿嘴鸽）中测试，模型召回率达1.0，准确率达0.95。

Conclusion: 该系统为保护濒危物种提供了实用的开源工具。

Abstract: This paper presents an automated one-shot bird call classification pipeline
designed for rare species absent from large publicly available classifiers like
BirdNET and Perch. While these models excel at detecting common birds with
abundant training data, they lack options for species with only 1-3 known
recordings-a critical limitation for conservationists monitoring the last
remaining individuals of endangered birds. To address this, we leverage the
embedding space of large bird classification networks and develop a classifier
using cosine similarity, combined with filtering and denoising preprocessing
techniques, to optimize detection with minimal training data. We evaluate
various embedding spaces using clustering metrics and validate our approach in
both a simulated scenario with Xeno-Canto recordings and a real-world test on
the critically endangered tooth-billed pigeon (Didunculus strigirostris), which
has no existing classifiers and only three confirmed recordings. The final
model achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon
calls, making it practical for use in the field. This open-source system
provides a practical tool for conservationists seeking to detect and monitor
rare species on the brink of extinction.

</details>

### [44] [DataS^3: Dataset Subset Selection for Specialization](https://arxiv.org/abs/2504.16277)
*Neha Hulkund,Alaa Maalouf,Levi Cai,Daniel Yang,Tsun-Hsuan Wang,Abigail O'Neil,Timm Haucke,Sandeep Mukherjee,Vikram Ramaswamy,Judy Hansen Shen,Gabriel Tseng,Mike Walmsley,Daniela Rus,Ken Goldberg,Hannah Kerner,Irene Chen,Yogesh Girdhar,Sara Beery*

Main category: cs.LG

TLDR: 论文提出了数据集子集选择问题（DS3），旨在从通用训练数据中选择适合特定部署的子集，并介绍了DataS^3基准测试，验证了部署专用子集的重要性。


<details>
  <summary>Details</summary>
Motivation: 现实机器学习应用中，模型需在特定部署（如特定医院或国家公园）中表现良好，但部署数据分布常不均衡且独特，导致训练与部署分布不一致影响性能。

Method: 提出DS3问题，设计DataS^3基准测试，评估包括核心集、数据过滤和数据筛选等多种算法在部署专用任务上的表现。

Result: 实验发现通用方法在部署专用任务上表现不佳，而手动筛选的专家子集性能提升最高达51.3%。

Conclusion: 研究表明定制化数据集筛选对提升部署专用性能至关重要，未来随着全球数据集普及，其重要性将进一步提升。

Abstract: In many real-world machine learning (ML) applications (e.g. detecting broken
bones in x-ray images, detecting species in camera traps), in practice models
need to perform well on specific deployments (e.g. a specific hospital, a
specific national park) rather than the domain broadly. However, deployments
often have imbalanced, unique data distributions. Discrepancy between the
training distribution and the deployment distribution can lead to suboptimal
performance, highlighting the need to select deployment-specialized subsets
from the available training data. We formalize dataset subset selection for
specialization (DS3): given a training set drawn from a general distribution
and a (potentially unlabeled) query set drawn from the desired
deployment-specific distribution, the goal is to select a subset of the
training data that optimizes deployment performance.
  We introduce DataS^3; the first dataset and benchmark designed specifically
for the DS3 problem. DataS^3 encompasses diverse real-world application
domains, each with a set of distinct deployments to specialize in. We conduct a
comprehensive study evaluating algorithms from various families--including
coresets, data filtering, and data curation--on DataS^3, and find that
general-distribution methods consistently fail on deployment-specific tasks.
Additionally, we demonstrate the existence of manually curated
(deployment-specific) expert subsets that outperform training on all available
data with accuracy gains up to 51.3 percent. Our benchmark highlights the
critical role of tailored dataset curation in enhancing performance and
training efficiency on deployment-specific distributions, which we posit will
only become more important as global, public datasets become available across
domains and ML models are deployed in the real world.

</details>

### [45] [Affect Models Have Weak Generalizability to Atypical Speech](https://arxiv.org/abs/2504.16283)
*Jaya Narain,Amrit Romana,Vikramjit Mitra,Colin Lea,Shirley Ren*

Main category: cs.LG

TLDR: 论文研究了语音异常对情感识别模型性能的影响，发现异常语音显著影响模型输出，并提出通过微调模型提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究语音异常（如发音、韵律和音质问题）如何影响情感识别模型的性能，以填补现有模型对非典型语音的不足。

Method: 评估公开的情感识别模型在非典型语音数据集上的表现，并与典型语音数据集对比，分析发音、韵律和音质三个维度的异常影响。

Result: 模型输出受语音异常显著影响，例如非典型语音被预测为悲伤的比例更高；微调模型可提升对非典型语音的识别能力。

Conclusion: 需扩大训练和评估数据集的多样性，并开发对语音差异更鲁棒的建模方法。

Abstract: Speech and voice conditions can alter the acoustic properties of speech,
which could impact the performance of paralinguistic models for affect for
people with atypical speech. We evaluate publicly available models for
recognizing categorical and dimensional affect from speech on a dataset of
atypical speech, comparing results to datasets of typical speech. We
investigate three dimensions of speech atypicality: intelligibility, which is
related to pronounciation; monopitch, which is related to prosody, and
harshness, which is related to voice quality. We look at (1) distributional
trends of categorical affect predictions within the dataset, (2) distributional
comparisons of categorical affect predictions to similar datasets of typical
speech, and (3) correlation strengths between text and speech predictions for
spontaneous speech for valence and arousal. We find that the output of affect
models is significantly impacted by the presence and degree of speech
atypicalities. For instance, the percentage of speech predicted as sad is
significantly higher for all types and grades of atypical speech when compared
to similar typical speech datasets. In a preliminary investigation on improving
robustness for atypical speech, we find that fine-tuning models on
pseudo-labeled atypical speech data improves performance on atypical speech
without impacting performance on typical speech. Our results emphasize the need
for broader training and evaluation datasets for speech emotion models, and for
modeling approaches that are robust to voice and speech differences.

</details>

### [46] [Semantics at an Angle: When Cosine Similarity Works Until It Doesn't](https://arxiv.org/abs/2504.16318)
*Kisung You*

Main category: cs.LG

TLDR: 本文反思了余弦相似性作为嵌入比较标准的优缺点，探讨了其适用场景、局限性及新兴替代方法。


<details>
  <summary>Details</summary>
Motivation: 余弦相似性因其尺度不变性和与模型训练目标的一致性而被广泛采用，但其在嵌入范数包含语义信息时的局限性逐渐显现。本文旨在澄清其概念并提供实践视角。

Method: 通过选择性回顾余弦相似性的发展历程，分析其优势和不足，并探讨新兴替代方法。

Result: 余弦相似性在许多场景下表现良好，但在嵌入范数具有语义意义时存在局限性，新兴方法正试图解决这些问题。

Conclusion: 本文为定量科学家提供了对余弦相似性的清晰理解，并指出其作为几何和哲学对象的潜在发展方向。

Abstract: Cosine similarity has become a standard metric for comparing embeddings in
modern machine learning. Its scale-invariance and alignment with model training
objectives have contributed to its widespread adoption. However, recent studies
have revealed important limitations, particularly when embedding norms carry
meaningful semantic information. This informal article offers a reflective and
selective examination of the evolution, strengths, and limitations of cosine
similarity. We highlight why it performs well in many settings, where it tends
to break down, and how emerging alternatives are beginning to address its blind
spots. We hope to offer a mix of conceptual clarity and practical perspective,
especially for quantitative scientists who think about embeddings not just as
vectors, but as geometric and philosophical objects.

</details>

### [47] [Disentangled Graph Representation Based on Substructure-Aware Graph Optimal Matching Kernel Convolutional Networks](https://arxiv.org/abs/2504.16360)
*Mao Wang,Tao Wu,Xingping Xian,Shaojie Qiao,Weina Niu,Canyixing Cui*

Main category: cs.LG

TLDR: 本文提出了一种名为GOMKCN的新方法，通过将图视为节点中心子图，并引入图最优匹配核（GOMK）作为卷积算子，解决了现有方法在结构模式分析中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在表征图结构时往往隐含且粗糙，限制了图内结构模式的分析。

Method: 将图视为节点中心子图，引入GOMK作为卷积算子，计算子图与可学习图滤波器之间的相似性。

Result: 实验验证了GOMKCN在图模式挖掘和预测中具有更高的准确性和可解释性。

Conclusion: GOMKCN为解耦图表示学习提供了理论基础，并提升了结构模式识别的能力。

Abstract: Graphs effectively characterize relational data, driving graph representation
learning methods that uncover underlying predictive information. As
state-of-the-art approaches, Graph Neural Networks (GNNs) enable end-to-end
learning for diverse tasks. Recent disentangled graph representation learning
enhances interpretability by decoupling independent factors in graph data.
However, existing methods often implicitly and coarsely characterize graph
structures, limiting structural pattern analysis within the graph. This paper
proposes the Graph Optimal Matching Kernel Convolutional Network (GOMKCN) to
address this limitation. We view graphs as node-centric subgraphs, where each
subgraph acts as a structural factor encoding position-specific information.
This transforms graph prediction into structural pattern recognition. Inspired
by CNNs, GOMKCN introduces the Graph Optimal Matching Kernel (GOMK) as a
convolutional operator, computing similarities between subgraphs and learnable
graph filters. Mathematically, GOMK maps subgraphs and filters into a Hilbert
space, representing graphs as point sets. Disentangled representations emerge
from projecting subgraphs onto task-optimized filters, which adaptively capture
relevant structural patterns via gradient descent. Crucially, GOMK incorporates
local correspondences in similarity measurement, resolving the trade-off
between differentiability and accuracy in graph kernels. Experiments validate
that GOMKCN achieves superior accuracy and interpretability in graph pattern
mining and prediction. The framework advances the theoretical foundation for
disentangled graph representation learning.

</details>

### [48] [Natural Policy Gradient for Average Reward Non-Stationary RL](https://arxiv.org/abs/2504.16415)
*Neharika Jali,Eshika Pathak,Pranay Sharma,Guannan Qu,Gauri Joshi*

Main category: cs.LG

TLDR: N/A


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of non-stationary reinforcement learning (RL) in the
infinite-horizon average-reward setting. We model it by a Markov Decision
Process with time-varying rewards and transition probabilities, with a
variation budget of $\Delta_T$. Existing non-stationary RL algorithms focus on
model-based and model-free value-based methods. Policy-based methods despite
their flexibility in practice are not theoretically well understood in
non-stationary RL. We propose and analyze the first model-free policy-based
algorithm, Non-Stationary Natural Actor-Critic (NS-NAC), a policy gradient
method with a restart based exploration for change and a novel interpretation
of learning rates as adapting factors. Further, we present a bandit-over-RL
based parameter-free algorithm BORL-NS-NAC that does not require prior
knowledge of the variation budget $\Delta_T$. We present a dynamic regret of
$\tilde{\mathscr O}(|S|^{1/2}|A|^{1/2}\Delta_T^{1/6}T^{5/6})$ for both
algorithms, where $T$ is the time horizon, and $|S|$, $|A|$ are the sizes of
the state and action spaces. The regret analysis leverages a novel adaptation
of the Lyapunov function analysis of NAC to dynamic environments and
characterizes the effects of simultaneous updates in policy, value function
estimate and changes in the environment.

</details>

### [49] [MAGIC: Near-Optimal Data Attribution for Deep Learning](https://arxiv.org/abs/2504.16430)
*Andrew Ilyas,Logan Engstrom*

Main category: cs.LG

TLDR: 提出了一种新方法（MAGIC），结合经典方法和元微分技术，用于估计训练数据增减对模型预测的影响。


<details>
  <summary>Details</summary>
Motivation: 在非凸大规模场景中，现有方法对训练数据增减影响的估计效果不佳，需要更优的解决方案。

Method: 结合经典方法和元微分技术，开发了MAGIC方法。

Result: MAGIC方法能够近乎最优地估计训练数据增减对模型预测的影响。

Conclusion: MAGIC方法在非凸大规模场景中显著提升了数据归因的准确性。

Abstract: The goal of predictive data attribution is to estimate how adding or removing
a given set of training datapoints will affect model predictions. In convex
settings, this goal is straightforward (i.e., via the infinitesimal jackknife).
In large-scale (non-convex) settings, however, existing methods are far less
successful -- current methods' estimates often only weakly correlate with
ground truth. In this work, we present a new data attribution method (MAGIC)
that combines classical methods and recent advances in metadifferentiation to
(nearly) optimally estimate the effect of adding or removing training data on
model predictions.

</details>

### [50] [Target Concrete Score Matching: A Holistic Framework for Discrete Diffusion](https://arxiv.org/abs/2504.16431)
*Ruixiang Zhang,Shuangfei Zhai,Yizhe Zhang,James Thornton,Zijing Ou,Joshua Susskind,Navdeep Jaitly*

Main category: cs.LG

TLDR: TCSM是一种新的离散扩散模型训练目标，支持预训练和后训练，灵活高效。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在离散数据建模和生成方面具有潜力，但现有方法缺乏通用性和灵活性。

Method: 提出TCSM目标，通过估计目标分布的具体分数，支持预训练、奖励微调和知识蒸馏。

Result: 实验表明TCSM在语言建模任务中表现优异，且具有更高的样本效率。

Conclusion: TCSM为离散扩散模型提供了通用框架，扩展了其应用场景和灵活性。

Abstract: Discrete diffusion is a promising framework for modeling and generating
discrete data. In this work, we present Target Concrete Score Matching (TCSM),
a novel and versatile objective for training and fine-tuning discrete diffusion
models. TCSM provides a general framework with broad applicability. It supports
pre-training discrete diffusion models directly from data samples, and many
existing discrete diffusion approaches naturally emerge as special cases of our
more general TCSM framework. Furthermore, the same TCSM objective extends to
post-training of discrete diffusion models, including fine-tuning using reward
functions or preference data, and distillation of knowledge from pre-trained
autoregressive models. These new capabilities stem from the core idea of TCSM,
estimating the concrete score of the target distribution, which resides in the
original (clean) data space. This allows seamless integration with reward
functions and pre-trained models, which inherently only operate in the clean
data space rather than the noisy intermediate spaces of diffusion processes.
Our experiments on language modeling tasks demonstrate that TCSM matches or
surpasses current methods. Additionally, TCSM is versatile, applicable to both
pre-training and post-training scenarios, offering greater flexibility and
sample efficiency.

</details>

### [51] [iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network](https://arxiv.org/abs/2504.16432)
*Ziran Liang,Rui An,Wenqi Fan,Yanghui Rao,Yuxuan Liang*

Main category: cs.LG

TLDR: iTFKAN是一种新的可解释时间序列预测模型，通过模型符号化实现可解释性，并结合先验知识注入和时间-频率协同学习策略，在复杂数据中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前深度预测方法缺乏可解释性，限制了其在安全关键应用中的可信度和实际部署。

Method: 提出iTFKAN模型，通过模型符号化实现可解释性，并采用先验知识注入和时间-频率协同学习策略。

Result: 实验表明iTFKAN在预测性能和可解释性方面均表现优异。

Conclusion: iTFKAN为可信时间序列预测提供了有效解决方案。

Abstract: As time evolves, data within specific domains exhibit predictability that
motivates time series forecasting to predict future trends from historical
data. However, current deep forecasting methods can achieve promising
performance but generally lack interpretability, hindering trustworthiness and
practical deployment in safety-critical applications such as auto-driving and
healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for
credible time series forecasting. iTFKAN enables further exploration of model
decision rationales and underlying data patterns due to its interpretability
achieved through model symbolization. Besides, iTFKAN develops two strategies,
prior knowledge injection, and time-frequency synergy learning, to effectively
guide model learning under complex intertwined time series data. Extensive
experimental results demonstrated that iTFKAN can achieve promising forecasting
performance while simultaneously possessing high interpretive capabilities.

</details>

### [52] [Private Federated Learning using Preference-Optimized Synthetic Data](https://arxiv.org/abs/2504.16438)
*Charlie Hou,Mei-Yu Wang,Yige Zhu,Daniel Lazar,Giulia Fanti*

Main category: cs.LG

TLDR: 论文提出了一种名为POPri的算法，通过偏好优化提升差分隐私合成数据的质量，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 差分隐私联邦学习（DP-FL）在处理私有客户端数据时存在局限性，而现有差分隐私合成数据方法依赖复杂的提示工程或迭代反馈。

Method: POPri利用偏好优化算法（如DPO）处理客户端反馈，生成高质量的差分隐私合成数据，并发布新基准LargeFedBench进行评估。

Result: POPri在LargeFedBench和现有基准上显著提升数据效用，将完全私有与非私有设置间的差距缩小至68%，优于现有方法。

Conclusion: POPri通过偏好优化有效提升了差分隐私合成数据的质量，为联邦学习提供了更优的解决方案。

Abstract: In practical settings, differentially private Federated learning (DP-FL) is
the dominant method for training models from private, on-device client data.
Recent work has suggested that DP-FL may be enhanced or outperformed by methods
that use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary
algorithms for generating DP synthetic data for FL applications require careful
prompt engineering based on public information and/or iterative private client
feedback. Our key insight is that the private client feedback collected by
prior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be
viewed as a preference ranking. Our algorithm, Preference Optimization for
Private Client Data (POPri) harnesses client feedback using preference
optimization algorithms such as Direct Preference Optimization (DPO) to
fine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri,
we release LargeFedBench, a new federated text benchmark for uncontaminated LLM
evaluations on federated client data. POPri substantially improves the utility
of DP synthetic data relative to prior work on LargeFedBench datasets and an
existing benchmark from Xie et al. (2024). POPri closes the gap between
next-token prediction accuracy in the fully-private and non-private settings by
up to 68%, compared to 52% for prior synthetic data methods, and 10% for
state-of-the-art DP federated learning methods. The code and data are available
at https://github.com/meiyuw/POPri.

</details>

### [53] [Node Assigned physics-informed neural networks for thermal-hydraulic system simulation: CVH/FL module](https://arxiv.org/abs/2504.16447)
*Jeesuk Shin,Cheolwoong Kim,Sunwoong Yang,Minseo Lee,Sung Joong Kim,Joongoo Jeon*

Main category: cs.LG

TLDR: 本文提出了一种基于物理信息神经网络（PINN）的新数值方法，用于核电站严重事故的热工水力系统代码分析，解决了传统有限差分方案的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 传统热工水力系统代码（如MELCOR和MAAP）在模拟严重事故时存在固有局限性，尤其是由于不一致的有限差分方案导致的多物理场单向耦合问题。

Method: 研究提出了一种节点分配的PINN（NA-PINN），适用于基于控制体积方法的系统代码，通过为每个节点分配独立网络，排除空间信息，学习纯时间解。

Result: 在6个水箱模拟中，PINN和NA-PINN的最大绝对误差分别为1.678和0.007，仅NA-PINN表现出可接受的精度。

Conclusion: NA-PINN首次成功实现了基于PINN的系统代码，未来将扩展为多物理场求解器并以替代方式开发。

Abstract: Severe accidents (SAs) in nuclear power plants have been analyzed using
thermal-hydraulic (TH) system codes such as MELCOR and MAAP. These codes
efficiently simulate the progression of SAs, while they still have inherent
limitations due to their inconsistent finite difference schemes. The use of
empirical schemes incorporating both implicit and explicit formulations
inherently induces unidirectional coupling in multi-physics analyses. The
objective of this study is to develop a novel numerical method for TH system
codes using physics-informed neural network (PINN). They have shown strength in
solving multi-physics due to the innate feature of neural networks-automatic
differentiation. We propose a node-assigned PINN (NA-PINN) that is suitable for
the control volume approach-based system codes. NA-PINN addresses the issue of
spatial governing equation variation by assigning an individual network to each
nodalization of the system code, such that spatial information is excluded from
both the input and output domains, and each subnetwork learns to approximate a
purely temporal solution. In this phase, we evaluated the accuracy of the PINN
methods for the hydrodynamic module. In the 6 water tank simulation, PINN and
NA-PINN showed maximum absolute errors of 1.678 and 0.007, respectively. It
should be noted that only NA-PINN demonstrated acceptable accuracy. To the best
of the authors' knowledge, this is the first study to successfully implement a
system code using PINN. Our future work involves extending NA-PINN to a
multi-physics solver and developing it in a surrogate manner.

</details>

### [54] [An Effective Gram Matrix Characterizes Generalization in Deep Networks](https://arxiv.org/abs/2504.16450)
*Rubing Yang,Pratik Chaudhari*

Main category: cs.LG

TLDR: 论文通过微分方程分析深度网络在梯度下降训练中泛化间隙的演变，提出有效Gram矩阵预测测试损失，并揭示数据与架构匹配对泛化性能的关键影响。


<details>
  <summary>Details</summary>
Motivation: 研究深度网络训练中泛化间隙的动态变化，以理解其与数据集和架构的关系。

Method: 推导控制泛化间隙演变的微分方程，引入收缩因子和扰动因子，分析有效Gram矩阵与初始残差的对齐关系。

Result: 有效Gram矩阵能准确预测测试损失；训练过程良性，泛化间隙未显著恶化；数据与架构匹配决定泛化性能。

Conclusion: 泛化间隙受数据与架构匹配影响，有效Gram矩阵为预测工具，训练过程对泛化无害。

Abstract: We derive a differential equation that governs the evolution of the
generalization gap when a deep network is trained by gradient descent. This
differential equation is controlled by two quantities, a contraction factor
that brings together trajectories corresponding to slightly different datasets,
and a perturbation factor that accounts for them training on different
datasets. We analyze this differential equation to compute an ``effective Gram
matrix'' that characterizes the generalization gap after training in terms of
the alignment between this Gram matrix and a certain initial ``residual''.
Empirical evaluations on image classification datasets indicate that this
analysis can predict the test loss accurately. Further, at any point during
training, the residual predominantly lies in the subspace of the effective Gram
matrix with the smallest eigenvalues. This indicates that the training process
is benign, i.e., it does not lead to significant deterioration of the
generalization gap (which is zero at initialization). The alignment between the
effective Gram matrix and the residual is different for different datasets and
architectures. The match/mismatch of the data and the architecture is primarily
responsible for good/bad generalization.

</details>

### [55] [Dynamic Time-aware Continual User Representation Learning](https://arxiv.org/abs/2504.16501)
*Seungyoon Choi,Sein Kim,Hongseok Kang,Wonjoong Kim,Chanyoung Park*

Main category: cs.LG

TLDR: 论文提出了一种动态时间感知的持续学习框架DITTO，用于解决传统用户建模方法在多任务场景下的局限性，并在实际评估场景中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统用户建模方法在泛化和适应性上存在局限，现有持续学习方法未考虑时间推移对新任务中物品分布变化的影响。

Method: 提出DITTO框架，动态适应物品分布变化，缓解灾难性遗忘，并允许知识迁移。

Result: 实验表明DITTO在实用评估场景下优于现有方法。

Conclusion: DITTO为持续学习用户表示提供了一种有效解决方案，适用于动态变化的实际场景。

Abstract: Traditional user modeling (UM) approaches have primarily focused on designing
models for a single specific task, but they face limitations in generalization
and adaptability across various tasks. Recognizing these challenges, recent
studies have shifted towards continual learning (CL)-based universal user
representation learning aiming to develop a single model capable of handling
multiple tasks. Despite advancements, existing methods are in fact evaluated
under an unrealistic scenario that does not consider the passage of time as
tasks progress, which overlooks newly emerged items that may change the item
distribution of previous tasks. In this paper, we introduce a practical
evaluation scenario on which CL-based universal user representation learning
approaches should be evaluated, which takes into account the passage of time as
tasks progress. Then, we propose a novel framework Dynamic Time-aware continual
user representation learner, named DITTO, designed to alleviate catastrophic
forgetting despite continuous shifts in item distribution, while also allowing
the knowledge acquired from previous tasks to adapt to the current shifted item
distribution. Through our extensive experiments, we demonstrate the superiority
of DITTO over state-of-the-art methods under a practical evaluation scenario.
Our source code is available at
https://github.com/seungyoon-Choi/DITTO_official.

</details>

### [56] [A Comprehensive Survey of Synthetic Tabular Data Generation](https://arxiv.org/abs/2504.16506)
*Ruxue Shi,Yili Wang,Mengnan Du,Xu Shen,Xin Wang*

Main category: cs.LG

TLDR: 该论文综述了合成表格数据生成的方法，提出了一种分类法，比较了不同生成模型，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 表格数据在机器学习中的应用受到数据稀缺、隐私问题和类别不平衡等限制，合成数据生成成为解决方案，但现有研究缺乏全面性。

Method: 提出分类法（传统方法、扩散模型和LLM模型），详细分析生成流程（数据合成、后处理和评估）。

Result: 提供了对合成表格数据生成领域的系统综述，并指出了方法间的联系和挑战。

Conclusion: 该研究填补了领域空白，为未来研究提供了方向，强调了LLM和扩散模型的潜力。

Abstract: Tabular data remains one of the most prevalent and critical data formats
across diverse real-world applications. However, its effective use in machine
learning (ML) is often constrained by challenges such as data scarcity, privacy
concerns, and class imbalance. Synthetic data generation has emerged as a
promising solution, leveraging generative models to learn the distribution of
real datasets and produce high-fidelity, privacy-preserving samples. Various
generative paradigms have been explored, including energy-based models (EBMs),
variational autoencoders (VAEs), generative adversarial networks (GANs), large
language models (LLMs), and diffusion models. While several surveys have
investigated synthetic tabular data generation, most focus on narrow subdomains
or specific generative methods, such as GANs, diffusion models, or
privacy-preserving techniques. This limited scope often results in fragmented
insights, lacking a comprehensive synthesis that bridges diverse approaches. In
particular, recent advances driven by LLMs and diffusion-based models remain
underexplored. This gap hinders a holistic understanding of the field`s
evolution, methodological interplay, and open challenges. To address this, our
survey provides a unified and systematic review of synthetic tabular data
generation. Our contributions are threefold: (1) we propose a comprehensive
taxonomy that organizes existing methods into traditional approaches,
diffusion-based methods, and LLM-based models, and provide an in-depth
comparative analysis; (2) we detail the complete pipeline for synthetic tabular
data generation, including data synthesis, post-processing, and evaluation; (3)
we identify major challenges, explore real-world applications, and outline open
research questions and future directions to guide future work in this rapidly
evolving area.

</details>

### [57] [Least-Squares-Embedded Optimization for Accelerated Convergence of PINNs in Acoustic Wavefield Simulations](https://arxiv.org/abs/2504.16553)
*Mohammad Mahdi Abedi,David Pardo,Tariq Alkhalifah*

Main category: cs.LG

TLDR: 论文提出了一种混合优化框架，通过将最小二乘（LS）求解器嵌入梯度下降（GD）损失函数中，加速了基于Helmholtz方程的物理信息神经网络（PINNs）训练。


<details>
  <summary>Details</summary>
Motivation: 标准PINNs训练在高频波场中收敛慢且不稳定，需要改进。

Method: 提出混合优化框架，结合LS求解器和GD，优化线性输出层更新。

Result: 实验表明，该方法收敛更快、精度更高、稳定性更好，尤其在高频情况下表现优异。

Conclusion: LS增强的方法在计算开销最小的情况下，显著提升了PINNs的训练效率和稳定性。

Abstract: Physics-Informed Neural Networks (PINNs) have shown promise in solving
partial differential equations (PDEs), including the frequency-domain Helmholtz
equation. However, standard training of PINNs using gradient descent (GD)
suffers from slow convergence and instability, particularly for high-frequency
wavefields. For scattered acoustic wavefield simulation based on Helmholtz
equation, we derive a hybrid optimization framework that accelerates training
convergence by embedding a least-squares (LS) solver directly into the GD loss
function. This formulation enables optimal updates for the linear output layer.
Our method is applicable with or without perfectly matched layers (PML), and we
provide practical tensor-based implementations for both scenarios. Numerical
experiments on benchmark velocity models demonstrate that our approach achieves
faster convergence, higher accuracy, and improved stability compared to
conventional PINN training. In particular, our results show that the
LS-enhanced method converges rapidly even in cases where standard GD-based
training fails. The LS solver operates on a small normal matrix, ensuring
minimal computational overhead and making the method scalable for large-scale
wavefield simulations.

</details>

### [58] [Unified Molecule Generation and Property Prediction](https://arxiv.org/abs/2504.16559)
*Adam Izdebski,Jan Olszewski,Pankhil Gawade,Krzysztof Koras,Serra Korkmaz,Valentin Rauscher,Jakub M. Tomczak,Ewa Szczurek*

Main category: cs.LG

TLDR: Hyformer是一种基于Transformer的联合模型，通过交替注意力掩码和统一预训练方案，成功融合生成和预测功能，性能优于其他联合模型及单一任务模型。


<details>
  <summary>Details</summary>
Motivation: 联合建模数据样本及其属性的分布可以构建一个兼具生成和预测功能的模型，但训练联合模型存在架构和优化上的挑战。

Method: 提出Hyformer，采用交替注意力掩码和统一预训练方案，结合生成和预测功能。

Result: Hyformer在分子生成和属性预测任务中表现优异，并在分子表示学习、命中识别和抗菌肽设计等下游任务中显示出联合建模的优势。

Conclusion: Hyformer证明了联合建模在生成和预测任务中的潜力，为多任务学习提供了新思路。

Abstract: Modeling the joint distribution of the data samples and their properties
allows to construct a single model for both data generation and property
prediction, with synergistic capabilities reaching beyond purely generative or
predictive models. However, training joint models presents daunting
architectural and optimization challenges. Here, we propose Hyformer, a
transformer-based joint model that successfully blends the generative and
predictive functionalities, using an alternating attention mask together with a
unified pre-training scheme. We show that Hyformer rivals other joint models,
as well as state-of-the-art molecule generation and property prediction models.
Additionally, we show the benefits of joint modeling in downstream tasks of
molecular representation learning, hit identification and antimicrobial peptide
design.

</details>

### [59] [Hyper-Transforming Latent Diffusion Models](https://arxiv.org/abs/2504.16580)
*Ignacio Peis,Batuhan Koyuncu,Isabel Valera,Jes Frellsen*

Main category: cs.LG

TLDR: 提出了一种结合隐式神经表示（INRs）和基于Transformer的超网络的新型生成框架，解决了MLP超网络的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖MLP超网络，存在可扩展性限制，需要更高效的表示能力和计算效率。

Method: 采用基于Transformer的解码器从潜在变量生成INR参数，扩展了潜在扩散模型（LDMs），支持从头训练或通过超变换策略微调解码器。

Result: 实现了高效的INR生成，无需完全重新训练即可适应现有生成模型。

Conclusion: 该框架为INR生成提供了高效且灵活的解决方案，解决了传统方法的局限性。

Abstract: We introduce a novel generative framework for functions by integrating
Implicit Neural Representations (INRs) and Transformer-based hypernetworks into
latent variable models. Unlike prior approaches that rely on MLP-based
hypernetworks with scalability limitations, our method employs a
Transformer-based decoder to generate INR parameters from latent variables,
addressing both representation capacity and computational efficiency. Our
framework extends latent diffusion models (LDMs) to INR generation by replacing
standard decoders with a Transformer-based hypernetwork, which can be trained
either from scratch or via hyper-transforming-a strategy that fine-tunes only
the decoder while freezing the pre-trained latent space. This enables efficient
adaptation of existing generative models to INR-based representations without
requiring full retraining.

</details>

### [60] [Enhancing Variable Selection in Large-scale Logistic Regression: Leveraging Manual Labeling with Beneficial Noise](https://arxiv.org/abs/2504.16585)
*Xiaofei Wu,Rongmei Liang*

Main category: cs.LG

TLDR: 论文研究了带标签噪声的惩罚逻辑回归（PLR），证明标签噪声有助于变量选择，并提出了一种基于ADMM的分区不敏感并行算法。


<details>
  <summary>Details</summary>
Motivation: 在大规模监督学习中，PLR通过正则化解决过拟合问题，但其性能依赖于高效的变量选择策略。标签噪声（来自人工标注）被发现对变量选择有益。

Method: 提出了一种基于ADMM的分区不敏感并行算法，适用于分布式计算环境，具有全局收敛性和次线性收敛速率。

Result: 实验表明，与传统变量选择方法相比，带标签噪声的PLR在多个大规模数据集上表现出更高的估计和分类准确性。

Conclusion: 标签噪声对PLR的变量选择有积极作用，提出的并行算法在分布式环境中高效且稳定。

Abstract: In large-scale supervised learning, penalized logistic regression (PLR)
effectively addresses the overfitting problem by introducing regularization
terms yet its performance still depends on efficient variable selection
strategies. This paper theoretically demonstrates that label noise stemming
from manual labeling, which is solely related to classification difficulty,
represents a type of beneficial noise for variable selection in PLR. This
benefit is reflected in a more accurate estimation of the selected non-zero
coefficients when compared with the case where only truth labels are used.
Under large-scale settings, the sample size for PLR can become very large,
making it infeasible to store on a single machine. In such cases, distributed
computing methods are required to handle PLR model with manual labeling. This
paper presents a partition-insensitive parallel algorithm founded on the ADMM
(alternating direction method of multipliers) algorithm to address PLR by
incorporating manual labeling. The partition insensitivity of the proposed
algorithm refers to the fact that the solutions obtained by the algorithm will
not change with the distributed storage of data. In addition, the algorithm has
global convergence and a sublinear convergence rate. Experimental results
indicate that, as compared with traditional variable selection classification
techniques, the PLR with manually-labeled noisy data achieves higher estimation
and classification accuracy across multiple large-scale datasets.

</details>

### [61] [Compositional Active Learning of Synchronous Systems through Automated Alphabet Refinement](https://arxiv.org/abs/2504.16624)
*Leo Henry,Thomas Neele,Mohammad Mousavi,Matteo Sammartino*

Main category: cs.LG

TLDR: 本文提出了一种用于同步并行系统的组合式自动机学习方法，能够自动细化全局字母表为组件字母表，并通过理论分析和算法实现显著提升了查询效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的组合式方法在处理并发系统时存在局限性，尤其是在未知分解的情况下。本文旨在开发一种通用技术，以解决这一问题并提升学习效率。

Method: 提出了一种组合式学习算法，自动细化全局字母表为组件字母表，并通过理论分析处理字母表分布的不一致性。算法实现为工具CoalA，基于LearnLib库。

Result: 在630多个实验系统中，CoalA在成员查询上实现了数量级的改进（最高五个数量级），并在高并发系统中表现出更好的等价查询可扩展性。

Conclusion: 本文的方法在理论和实践上均取得了显著进展，为组合式自动机学习提供了有效的解决方案。

Abstract: Active automata learning infers automaton models of systems from behavioral
observations, a technique successfully applied to a wide range of domains.
Compositional approaches for concurrent systems have recently emerged. We take
a significant step beyond available results, including those by the authors,
and develop a general technique for compositional learning of a synchronizing
parallel system with an unknown decomposition. Our approach automatically
refines the global alphabet into component alphabets while learning the
component models. We develop a theoretical treatment of distributions of
alphabets, i.e., sets of possibly overlapping component alphabets. We
characterize counter-examples that reveal inconsistencies with global
observations, and show how to systematically update the distribution to restore
consistency. We present a compositional learning algorithm implementing these
ideas, where learning counterexamples precisely correspond to distribution
counterexamples under well-defined conditions. We provide an implementation,
called CoalA, using the state-of-the-art active learning library LearnLib. Our
experiments show that in more than 630 subject systems, CoalA delivers orders
of magnitude improvements (up to five orders) in membership queries and in
systems with significant concurrency, it also achieves better scalability in
the number of equivalence queries.

</details>

### [62] [ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data](https://arxiv.org/abs/2504.16628)
*Haoran Gu,Handing Wang,Yi Mei,Mengjie Zhang,Yaochu Jin*

Main category: cs.LG

TLDR: ParetoHqD通过将人类偏好表示为目标空间的偏好方向，并利用帕累托前沿附近的高质量数据，改进了多目标对齐算法的性能。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型与多种人类期望和价值观对齐，以满足多样化的用户需求。

Method: 提出ParetoHqD，采用两阶段监督微调过程，每个阶段使用与偏好方向匹配的帕累托高质量训练集。

Result: 在两项多目标对齐任务中，ParetoHqD优于五种基线方法。

Conclusion: ParetoHqD有效解决了偏好表示不当和奖励分数不平衡的问题，提升了多目标对齐的性能。

Abstract: Aligning large language models with multiple human expectations and values is
crucial for ensuring that they adequately serve a variety of user needs. To
this end, offline multiobjective alignment algorithms such as the
Rewards-in-Context algorithm have shown strong performance and efficiency.
However, inappropriate preference representations and training with imbalanced
reward scores limit the performance of such algorithms. In this work, we
introduce ParetoHqD that addresses the above issues by representing human
preferences as preference directions in the objective space and regarding data
near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD
follows a two-stage supervised fine-tuning process, where each stage uses an
individual Pareto high-quality training set that best matches its preference
direction. The experimental results have demonstrated the superiority of
ParetoHqD over five baselines on two multiobjective alignment tasks.

</details>

### [63] [DAPLSR: Data Augmentation Partial Least Squares Regression Model via Manifold Optimization](https://arxiv.org/abs/2504.16639)
*Haoran Chen,Jiapeng Liu,Jiafan Wang,Wenjun Shi*

Main category: cs.LG

TLDR: 本文提出了一种基于流形优化的数据增强偏最小二乘回归（DAPLSR）模型，通过SMOTE和VDM技术提升分类性能，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统PLSR模型在处理类别不平衡数据时表现不佳，需要改进。

Method: 引入SMOTE增加样本量，利用VDM选择近邻样本生成合成样本，并通过流形优化方法提升模型精度。

Result: DAPLSR模型在多个数据集上表现出卓越的分类性能和评价指标。

Conclusion: DAPLSR模型显著优于现有方法，适用于类别不平衡数据。

Abstract: Traditional Partial Least Squares Regression (PLSR) models frequently
underperform when handling data characterized by uneven categories. To address
the issue, this paper proposes a Data Augmentation Partial Least Squares
Regression (DAPLSR) model via manifold optimization. The DAPLSR model
introduces the Synthetic Minority Over-sampling Technique (SMOTE) to increase
the number of samples and utilizes the Value Difference Metric (VDM) to select
the nearest neighbor samples that closely resemble the original samples for
generating synthetic samples. In solving the model, in order to obtain a more
accurate numerical solution for PLSR, this paper proposes a manifold
optimization method that uses the geometric properties of the constraint space
to improve model degradation and optimization. Comprehensive experiments show
that the proposed DAPLSR model achieves superior classification performance and
outstanding evaluation metrics on various datasets, significantly outperforming
existing methods.

</details>

### [64] [Representation Learning via Non-Contrastive Mutual Information](https://arxiv.org/abs/2504.16667)
*Zhaohan Daniel Guo,Bernardo Avila Pires,Khimya Khetarpal,Dale Schuurmans,Bo Dai*

Main category: cs.LG

TLDR: 本文提出了一种结合对比和非对比自监督学习优势的新目标函数MINC，通过改进谱对比损失，降低方差并避免模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 标记数据成本高且耗时，自监督学习方法（如SimCLR和BYOL）虽有效，但对比方法方差高，非对比方法易崩溃。本文旨在结合两者优势。

Method: 从谱对比损失出发，将其转化为非对比形式，提出MINC损失，避免成对比较的同时保留互信息。

Result: 在ImageNet上测试MINC，表现优于谱对比损失基线。

Conclusion: MINC成功结合了对比和非对比方法的优点，为自监督学习提供了更优的解决方案。

Abstract: Labeling data is often very time consuming and expensive, leaving us with a
majority of unlabeled data. Self-supervised representation learning methods
such as SimCLR (Chen et al., 2020) or BYOL (Grill et al., 2020) have been very
successful at learning meaningful latent representations from unlabeled image
data, resulting in much more general and transferable representations for
downstream tasks. Broadly, self-supervised methods fall into two types: 1)
Contrastive methods, such as SimCLR; and 2) Non-Contrastive methods, such as
BYOL. Contrastive methods are generally trying to maximize mutual information
between related data points, so they need to compare every data point to every
other data point, resulting in high variance, and thus requiring large batch
sizes to work well. Non-contrastive methods like BYOL have much lower variance
as they do not need to make pairwise comparisons, but are much trickier to
implement as they have the possibility of collapsing to a constant vector. In
this paper, we aim to develop a self-supervised objective that combines the
strength of both types. We start with a particular contrastive method called
the Spectral Contrastive Loss (HaoChen et al., 2021; Lu et al., 2024), and we
convert it into a more general non-contrastive form; this removes the pairwise
comparisons resulting in lower variance, but keeps the mutual information
formulation of the contrastive method preventing collapse. We call our new
objective the Mutual Information Non-Contrastive (MINC) loss. We test MINC by
learning image representations on ImageNet (similar to SimCLR and BYOL) and
show that it consistently improves upon the Spectral Contrastive loss baseline.

</details>

### [65] [Efficient Data Valuation Approximation in Federated Learning: A Sampling-based Approach](https://arxiv.org/abs/2504.16668)
*Shuyue Wei,Yongxin Tong,Zimu Zhou,Tianran He,Yi Xu*

Main category: cs.LG

TLDR: 论文提出了一种高效的Shapley值近似算法IPSS，用于联邦学习中的数据估值，通过选择关键数据集组合显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中，数据提供者不愿共享高质量数据，除非其数据价值能被公平评估。现有Shapley值计算方法计算成本过高，且缺乏高效准确的解决方案。

Method: 提出统一的分层采样框架，分析并选择更适合联邦线性回归的方案，识别关键组合现象，设计IPSS算法选择性评估高影响力组合。

Result: 在联邦学习基准数据集上的实验表明，IPSS在效率和效果上均优于现有基线方法。

Conclusion: IPSS算法通过减少计算复杂度，实现了高效且准确的数据估值，为联邦学习中的数据共享提供了实用解决方案。

Abstract: Federated learning paradigm to utilize datasets across multiple data
providers. In FL, cross-silo data providers often hesitate to share their
high-quality dataset unless their data value can be fairly assessed. Shapley
value (SV) has been advocated as the standard metric for data valuation in FL
due to its desirable properties. However, the computational overhead of SV is
prohibitive in practice, as it inherently requires training and evaluating an
FL model across an exponential number of dataset combinations. Furthermore,
existing solutions fail to achieve high accuracy and efficiency, making
practical use of SV still out of reach, because they ignore choosing suitable
computation scheme for approximation framework and overlook the property of
utility function in FL. We first propose a unified stratified-sampling
framework for two widely-used schemes. Then, we analyze and choose the more
promising scheme under the FL linear regression assumption. After that, we
identify a phenomenon termed key combinations, where only limited dataset
combinations have a high-impact on final data value. Building on these
insights, we propose a practical approximation algorithm, IPSS, which
strategically selects high-impact dataset combinations rather than evaluating
all possible combinations, thus substantially reducing time cost with minor
approximation error. Furthermore, we conduct extensive evaluations on the FL
benchmark datasets to demonstrate that our proposed algorithm outperforms a
series of representative baselines in terms of efficiency and effectiveness.

</details>

### [66] [Provable wavelet-based neural approximation](https://arxiv.org/abs/2504.16682)
*Youngmi Hur,Hyojae Lim,Mikyoung Lim*

Main category: cs.LG

TLDR: 本文提出了一种基于小波的框架，用于分析神经网络在多种激活函数下的通用逼近能力，并给出了误差估计。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在不同激活函数下的逼近能力，为网络设计提供灵活性。

Method: 利用小波框架理论，推导出激活函数的充分条件，并分析L²距离对逼近误差的影响。

Result: 证明了多种平滑和非平滑激活函数的逼近能力，误差可通过距离明确控制。

Conclusion: 该框架为神经网络架构设计提供了更大的灵活性，适用于广泛的激活函数。

Abstract: In this paper, we develop a wavelet-based theoretical framework for analyzing
the universal approximation capabilities of neural networks over a wide range
of activation functions. Leveraging wavelet frame theory on the spaces of
homogeneous type, we derive sufficient conditions on activation functions to
ensure that the associated neural network approximates any functions in the
given space, along with an error estimate. These sufficient conditions
accommodate a variety of smooth activation functions, including those that
exhibit oscillatory behavior. Furthermore, by considering the $L^2$-distance
between smooth and non-smooth activation functions, we establish a generalized
approximation result that is applicable to non-smooth activations, with the
error explicitly controlled by this distance. This provides increased
flexibility in the design of network architectures.

</details>

### [67] [MCMC for Bayesian estimation of Differential Privacy from Membership Inference Attacks](https://arxiv.org/abs/2504.16683)
*Ceren Yildirim,Kamer Kaya,Sinan Yildirim,Erkay Savas*

Main category: cs.LG

TLDR: 提出了一种新的贝叶斯估计框架，用于差分隐私的评估，结合了多种成员推理攻击（MIA）的证据。


<details>
  <summary>Details</summary>
Motivation: 现有的隐私审计方法通常假设使用最强攻击和最坏情况对，这在现实中不切实际。

Method: 通过MCMC-DP-Est算法进行贝叶斯估计，联合评估MIA的强度和训练算法的隐私性。

Result: 提供了一种更谨慎的隐私分析方法，并通过数值示例展示了方法的实用性。

Conclusion: 该方法为差分隐私评估提供了一种更现实的框架。

Abstract: We propose a new framework for Bayesian estimation of differential privacy,
incorporating evidence from multiple membership inference attacks (MIA).
Bayesian estimation is carried out via a Markov chain Monte Carlo (MCMC)
algorithm, named MCMC-DP-Est, which provides an estimate of the full posterior
distribution of the privacy parameter (e.g., instead of just credible
intervals). Critically, the proposed method does not assume that privacy
auditing is performed with the most powerful attack on the worst-case (dataset,
challenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est
jointly estimates the strengths of MIAs used and the privacy of the training
algorithm, yielding a more cautious privacy analysis. We also present an
economical way to generate measurements for the performance of an MIA that is
to be used by the MCMC method to estimate privacy. We present the use of the
methods with numerical examples with both artificial and real data.

</details>

### [68] [PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation](https://arxiv.org/abs/2504.16693)
*Wenxuan Li,Hang Zhao,Zhiyuan Yu,Yu Du,Qin Zou,Ruizhen Hu,Kai Xu*

Main category: cs.LG

TLDR: 论文提出了一种名为PIN-WM的物理信息世界模型，用于学习非抓取操作的3D刚体动力学，并通过模型强化学习实现鲁棒策略。


<details>
  <summary>Details</summary>
Motivation: 非抓取操作（如推动/戳动）是机器人基础技能，但由于涉及摩擦和恢复力的复杂物理交互，其学习具有挑战性。

Method: 采用可微分物理模拟，通过少量任务无关的物理交互轨迹学习PIN-WM，无需状态估计，使用高斯散射诱导的观测损失。

Result: 通过物理感知随机化生成数字副本，PIN-WM在仿真和真实测试中表现出色，超越了现有Real2Sim2Real方法。

Conclusion: PIN-WM及其数字副本能够有效学习鲁棒的非抓取操作技能，并实现仿真到现实的迁移。

Abstract: While non-prehensile manipulation (e.g., controlled pushing/poking)
constitutes a foundational robotic skill, its learning remains challenging due
to the high sensitivity to complex physical interactions involving friction and
restitution. To achieve robust policy learning and generalization, we opt to
learn a world model of the 3D rigid body dynamics involved in non-prehensile
manipulations and use it for model-based reinforcement learning. We propose
PIN-WM, a Physics-INformed World Model that enables efficient end-to-end
identification of a 3D rigid body dynamical system from visual observations.
Adopting differentiable physics simulation, PIN-WM can be learned with only
few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM
is learned with observational loss induced by Gaussian Splatting without
needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM
into a group of Digital Cousins via physics-aware randomizations which perturb
physics and rendering parameters to generate diverse and meaningful variations
of the PIN-WM. Extensive evaluations on both simulation and real-world tests
demonstrate that PIN-WM, enhanced with physics-aware digital cousins,
facilitates learning robust non-prehensile manipulation skills with Sim2Real
transfer, surpassing the Real2Sim2Real state-of-the-arts.

</details>

### [69] [A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization](https://arxiv.org/abs/2504.16711)
*Shiyin Tan,Jaeeon Park,Dongyuan Li,Renhe Jiang,Manabu Okumura*

Main category: cs.LG

TLDR: 提出了一种新的检索框架，通过整合查询选择和文档排序与缩短，解决了多文档摘要中的输入长度限制问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的模型在多文档摘要中表现优异，但存在输入长度限制，且依赖人工精心设计的查询，检索粒度较粗。

Method: 提出了一种框架，通过识别输入文档中最显著的基本话语单元（EDUs）作为潜在查询，计算相关性分数以指导文档排序，并过滤无关内容以适应上下文长度。

Result: 在多个多文档摘要数据集上验证了框架的有效性，ROUGE指标持续提升，同时展示了其可扩展性和灵活性。

Conclusion: 该框架有效解决了上下文长度限制问题，为多文档摘要提供了可靠且稳健的解决方案。

Abstract: In the field of multi-document summarization (MDS), transformer-based models
have demonstrated remarkable success, yet they suffer an input length
limitation. Current methods apply truncation after the retrieval process to fit
the context length; however, they heavily depend on manually well-crafted
queries, which are impractical to create for each document set for MDS.
Additionally, these methods retrieve information at a coarse granularity,
leading to the inclusion of irrelevant content. To address these issues, we
propose a novel retrieval-based framework that integrates query selection and
document ranking and shortening into a unified process. Our approach identifies
the most salient elementary discourse units (EDUs) from input documents and
utilizes them as latent queries. These queries guide the document ranking by
calculating relevance scores. Instead of traditional truncation, our approach
filters out irrelevant EDUs to fit the context length, ensuring that only
critical information is preserved for summarization. We evaluate our framework
on multiple MDS datasets, demonstrating consistent improvements in ROUGE
metrics while confirming its scalability and flexibility across diverse model
architectures. Additionally, we validate its effectiveness through an in-depth
analysis, emphasizing its ability to dynamically select appropriate queries and
accurately rank documents based on their relevance scores. These results
demonstrate that our framework effectively addresses context-length
constraints, establishing it as a robust and reliable solution for MDS.

</details>

### [70] [Simple Graph Contrastive Learning via Fractional-order Neural Diffusion Networks](https://arxiv.org/abs/2504.16748)
*Yanan Zhao,Feng Ji,Kai Zhao,Xuhao Li,Qiyu Kang,Wenfei Liang,Yahya Alkhatib,Xingchao Jian,Wee Peng Tay*

Main category: cs.LG

TLDR: 本文提出了一种基于图神经扩散模型的无增强图对比学习框架，利用分数微分方程生成多样化的视图，无需负样本训练，适用于同质和异质数据集，性能达到最优。


<details>
  <summary>Details</summary>
Motivation: 现有的图对比学习方法依赖复杂的数据增强或需要负样本训练，限制了其适用性和效率。本文旨在提出一种无需增强和负样本的框架，提高模型的通用性和性能。

Method: 使用分数微分方程（FDE）控制可学习编码器，通过调整微分算子的阶数参数生成多样化视图，捕捉局部或全局信息，用于对比学习。

Result: 模型在多种数据集上表现出色，无需负样本训练，适用于同质和异质数据集，性能达到最优。

Conclusion: 本文提出的无增强图对比学习框架通过分数微分方程生成多样化视图，简化了训练流程并提升了性能，为图表示学习提供了新思路。

Abstract: Graph Contrastive Learning (GCL) has recently made progress as an
unsupervised graph representation learning paradigm. GCL approaches can be
categorized into augmentation-based and augmentation-free methods. The former
relies on complex data augmentations, while the latter depends on encoders that
can generate distinct views of the same input. Both approaches may require
negative samples for training. In this paper, we introduce a novel
augmentation-free GCL framework based on graph neural diffusion models.
Specifically, we utilize learnable encoders governed by Fractional Differential
Equations (FDE). Each FDE is characterized by an order parameter of the
differential operator. We demonstrate that varying these parameters allows us
to produce learnable encoders that generate diverse views, capturing either
local or global information, for contrastive learning. Our model does not
require negative samples for training and is applicable to both homophilic and
heterophilic datasets. We demonstrate its effectiveness across various
datasets, achieving state-of-the-art performance.

</details>

### [71] [QAOA-PCA: Enhancing Efficiency in the Quantum Approximate Optimization Algorithm via Principal Component Analysis](https://arxiv.org/abs/2504.16755)
*Owain Parry,Phil McMinn*

Main category: cs.LG

TLDR: QAOA-PCA是一种通过主成分分析（PCA）降低QAOA参数空间维度的新方法，显著提高了优化效率，同时在性能上保持较好平衡。


<details>
  <summary>Details</summary>
Motivation: 随着QAOA电路层数增加，参数优化所需的计算负担加重，需要一种方法减少参数数量以提高效率。

Method: 提出QAOA-PCA，利用PCA从小问题实例的优化参数中提取主成分，减少大实例的参数数量。

Result: 在MaxCut问题上，QAOA-PCA比标准QAOA需要更少的迭代次数，效率显著提升，尽管近似比略有下降。

Conclusion: QAOA-PCA在效率和性能之间取得了良好平衡，减少了优化开销且未显著影响解的质量。

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a promising
variational algorithm for solving combinatorial optimization problems on
near-term devices. However, as the number of layers in a QAOA circuit
increases, which is correlated with the quality of the solution, the number of
parameters to optimize grows linearly. This results in more iterations required
by the classical optimizer, which results in an increasing computational burden
as more circuit executions are needed. To mitigate this issue, we introduce
QAOA-PCA, a novel reparameterization technique that employs Principal Component
Analysis (PCA) to reduce the dimensionality of the QAOA parameter space. By
extracting principal components from optimized parameters of smaller problem
instances, QAOA-PCA facilitates efficient optimization with fewer parameters on
larger instances. Our empirical evaluation on the prominent MaxCut problem
demonstrates that QAOA-PCA consistently requires fewer iterations than standard
QAOA, achieving substantial efficiency gains. While this comes at the cost of a
slight reduction in approximation ratio compared to QAOA with the same number
of layers, QAOA-PCA almost always outperforms standard QAOA when matched by
parameter count. QAOA-PCA strikes a favorable balance between efficiency and
performance, reducing optimization overhead without significantly compromising
solution quality.

</details>

### [72] [Noise-Tolerant Coreset-Based Class Incremental Continual Learning](https://arxiv.org/abs/2504.16763)
*Edison Mucllari,Aswin Raghavan,Zachary Alan Daniels*

Main category: cs.LG

TLDR: 论文研究了在类增量学习（CIL）中，标签噪声和实例噪声对持续学习（CL）的影响，提出了两种抗噪声的重放缓冲区算法，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉应用需要适应新数据分布，但持续学习易受噪声干扰，尤其是标签噪声和实例噪声。本文旨在理解基于Coresets的重放方法的噪声敏感性，并提出抗噪声解决方案。

Method: 推导了在一般加性噪声威胁模型下，基于Coresets方法对不相关实例噪声的鲁棒性新边界，并设计了两种抗噪声的持续学习算法。

Result: 实验表明，现有基于内存的CL方法在噪声CIL环境中不鲁棒，而提出的方法在分类准确性和减少遗忘方面显著优于现有方法。

Conclusion: 本文提出的抗噪声重放缓冲区算法在噪声CIL环境中表现优异，为持续学习的实际应用提供了更鲁棒的解决方案。

Abstract: Many applications of computer vision require the ability to adapt to novel
data distributions after deployment. Adaptation requires algorithms capable of
continual learning (CL). Continual learners must be plastic to adapt to novel
tasks while minimizing forgetting of previous tasks.However, CL opens up
avenues for noise to enter the training pipeline and disrupt the CL. This work
focuses on label noise and instance noise in the context of class-incremental
learning (CIL), where new classes are added to a classifier over time, and
there is no access to external data from past classes. We aim to understand the
sensitivity of CL methods that work by replaying items from a memory
constructed using the idea of Coresets. We derive a new bound for the
robustness of such a method to uncorrelated instance noise under a general
additive noise threat model, revealing several insights. Putting the theory
into practice, we create two continual learning algorithms to construct
noise-tolerant replay buffers. We empirically compare the effectiveness of
prior memory-based continual learners and the proposed algorithms under label
and uncorrelated instance noise on five diverse datasets. We show that existing
memory-based CL are not robust whereas the proposed methods exhibit significant
improvements in maximizing classification accuracy and minimizing forgetting in
the noisy CIL setting.

</details>

### [73] [Online model learning with data-assimilated reservoir computers](https://arxiv.org/abs/2504.16767)
*Andrea Nóvoa,Luca Magri*

Main category: cs.LG

TLDR: 提出了一种在线学习框架，用于预测非线性时空信号，结合降维、广义自回归模型和在线适应技术。


<details>
  <summary>Details</summary>
Motivation: 解决非线性时空信号预测问题，通过数据驱动的降阶建模与贝叶斯数据同化的结合，实现在线模型学习。

Method: 集成POD降维、储层计算机预测和集合序贯数据同化，测试了三种状态估计策略。

Result: 两重和三重状态估计策略显著提高了收敛性和重构精度，三策略支持部分训练储层计算机的在线学习。

Conclusion: 该框架为非线性时间序列预测提供了可扩展的在线模型学习方法。

Abstract: We propose an online learning framework for forecasting nonlinear
spatio-temporal signals (fields). The method integrates (i) dimensionality
reduction, here, a simple proper orthogonal decomposition (POD) projection;
(ii) a generalized autoregressive model to forecast reduced dynamics, here, a
reservoir computer; (iii) online adaptation to update the reservoir computer
(the model), here, ensemble sequential data assimilation.We demonstrate the
framework on a wake past a cylinder governed by the Navier-Stokes equations,
exploring the assimilation of full flow fields (projected onto POD modes) and
sparse sensors. Three scenarios are examined: a na\"ive physical state
estimation; a two-fold estimation of physical and reservoir states; and a
three-fold estimation that also adjusts the model parameters. The two-fold
strategy significantly improves ensemble convergence and reduces reconstruction
error compared to the na\"ive approach. The three-fold approach enables robust
online training of partially-trained reservoir computers, overcoming
limitations of a priori training. By unifying data-driven reduced order
modelling with Bayesian data assimilation, this work opens new opportunities
for scalable online model learning for nonlinear time series forecasting.

</details>

### [74] [Process Reward Models That Think](https://arxiv.org/abs/2504.16828)
*Muhammad Khalifa,Rishabh Agarwal,Lajanugen Logeswaran,Jaekyeom Kim,Hao Peng,Moontae Lee,Honglak Lee,Lu Wang*

Main category: cs.LG

TLDR: ThinkPRM是一种基于生成式长链思维验证模型，通过极少量过程标签训练，优于传统判别式验证模型和LLM-as-a-Judge方法。


<details>
  <summary>Details</summary>
Motivation: 传统过程奖励模型（PRMs）需要大量步骤级监督数据，训练成本高。本研究旨在开发数据高效的PRMs，通过生成验证链式思维（CoT）来验证每一步。

Method: 提出ThinkPRM，一种基于长链思维（CoT）的生成式验证模型，利用少量过程标签进行微调。

Result: ThinkPRM在多个基准测试中表现优异，仅使用1%的PRM800K标签，优于基线方法。在跨领域评估中，ThinkPRM表现更佳。

Conclusion: 生成式长链思维PRMs能高效扩展验证计算，且训练所需监督数据极少，具有显著价值。

Abstract: Step-by-step verifiers -- also known as process reward models (PRMs) -- are a
key ingredient for test-time scaling. PRMs require step-level supervision,
making them expensive to train. This work aims to build data-efficient PRMs as
verbalized step-wise reward models that verify every step in the solution by
generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long
CoT verifier fine-tuned on orders of magnitude fewer process labels than those
required by discriminative PRMs. Our approach capitalizes on the inherent
reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and
discriminative verifiers -- using only 1% of the process labels in PRM800K --
across several challenging benchmarks. Specifically, ThinkPRM beats the
baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and
reward-guided search. In an out-of-domain evaluation on a subset of
GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers
trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the
same token budget, ThinkPRM scales up verification compute more effectively
compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of
ProcessBench. Our work highlights the value of generative, long CoT PRMs that
can scale test-time compute for verification while requiring minimal
supervision for training. Our code, data, and models will be released at
https://github.com/mukhal/thinkprm.

</details>

### [75] [Evaluating Autoencoders for Parametric and Invertible Multidimensional Projections](https://arxiv.org/abs/2504.16831)
*Frederik L. Dennig,Nina Geyer,Daniela Blumberg,Yannick Metz,Daniel A. Keim*

Main category: cs.LG

TLDR: 论文研究了如何同时实现参数化和可逆的多维数据投影，通过三种自编码器架构进行实验，结果表明定制损失函数的自编码器能生成更平滑的投影。


<details>
  <summary>Details</summary>
Motivation: 探索同时实现参数化和可逆的多维数据投影方法，填补现有研究中这两项特性未被同时研究的空白。

Method: 使用三种自编码器架构，训练映射到2D空间及其逆映射，并通过定量和定性分析比较其性能。

Result: 定制损失函数的自编码器能生成更平滑的参数化和可逆投影，且用户可控制平滑效果强度。

Conclusion: 自编码器在实现参数化和可逆投影方面表现优越，为多维数据可视化提供了新工具。

Abstract: Recently, neural networks have gained attention for creating parametric and
invertible multidimensional data projections. Parametric projections allow for
embedding previously unseen data without recomputing the projection as a whole,
while invertible projections enable the generation of new data points. However,
these properties have never been explored simultaneously for arbitrary
projection methods. We evaluate three autoencoder (AE) architectures for
creating parametric and invertible projections. Based on a given projection, we
train AEs to learn a mapping into 2D space and an inverse mapping into the
original space. We perform a quantitative and qualitative comparison on four
datasets of varying dimensionality and pattern complexity using t-SNE. Our
results indicate that AEs with a customized loss function can create smoother
parametric and inverse projections than feed-forward neural networks while
giving users control over the strength of the smoothing effect.

</details>

### [76] [Improving Significant Wave Height Prediction Using Chronos Models](https://arxiv.org/abs/2504.16834)
*Yilin Zhai,Hongyuan Shi,Chao Zhan,Qing Wang,Zaijin You,Nan Wang*

Main category: cs.LG

TLDR: Chronos是一种基于大型语言模型（LLM）的时间架构，用于波浪高度预测，在计算效率和预测性能上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统物理模型和机器学习方法在波浪预测中存在计算效率和非线性动态建模的挑战，需要更高效的解决方案。

Method: 通过先进的时间模式识别技术，应用于西北太平洋三个海洋区域的历史波浪数据，优化了LLM架构。

Result: 实现了训练时间减少14.3%，推理速度提升2.5倍，短期和长期预测性能均优于基线模型，并展示了零样本能力。

Conclusion: Chronos为波浪预测设立了新标准，提供了高效且可迁移的复杂地球物理系统建模框架。

Abstract: Accurate wave height prediction is critical for maritime safety and coastal
resilience, yet conventional physics-based models and traditional machine
learning methods face challenges in computational efficiency and nonlinear
dynamics modeling. This study introduces Chronos, the first implementation of a
large language model (LLM)-powered temporal architecture (Chronos) optimized
for wave forecasting. Through advanced temporal pattern recognition applied to
historical wave data from three strategically chosen marine zones in the
Northwest Pacific basin, our framework achieves multimodal improvements: (1)
14.3% reduction in training time with 2.5x faster inference speed compared to
PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;
(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)
sustained predictive leadership in extended-range forecasts (1-120h); and (4)
demonstrated zero-shot capability maintaining median performance (rank 4/12)
against specialized operational models. This LLM-enhanced temporal modeling
paradigm establishes a new standard in wave prediction, offering both
computationally efficient solutions and a transferable framework for complex
geophysical systems modeling.

</details>

### [77] [An Adaptive ML Framework for Power Converter Monitoring via Federated Transfer Learning](https://arxiv.org/abs/2504.16866)
*Panagiotis Kakosimos,Alireza Nemat Saberi,Luca Peretti*

Main category: cs.LG

TLDR: 该研究通过结合迁移学习（TL）和联邦学习（FL），提出了一种分段式框架，用于适应电力转换器的热机器学习模型，解决了数据共享限制和安全问题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决电力转换器热ML模型在不同操作条件、数据共享限制和安全性方面的挑战。

Method: 采用分段式框架，结合TL和FL，使用三种领域适应技术（微调、TCA和DDA），并通过Flower框架实现FL。

Result: 验证表明微调具有高准确性，适合实际应用；本地FL在数据不可聚合时表现更好，而云端FL在客户数量增加时更具扩展性。

Conclusion: 该框架为电力转换器热ML模型提供了灵活且高效的解决方案，适用于不同场景。

Abstract: This study explores alternative framework configurations for adapting thermal
machine learning (ML) models for power converters by combining transfer
learning (TL) and federated learning (FL) in a piecewise manner. This approach
inherently addresses challenges such as varying operating conditions, data
sharing limitations, and security implications. The framework starts with a
base model that is incrementally adapted by multiple clients via adapting three
state-of-the-art domain adaptation techniques: Fine-tuning, Transfer Component
Analysis (TCA), and Deep Domain Adaptation (DDA). The Flower framework is
employed for FL, using Federated Averaging for aggregation. Validation with
field data demonstrates that fine-tuning offers a straightforward TL approach
with high accuracy, making it suitable for practical applications. Benchmarking
results reveal a comprehensive comparison of these methods, showcasing their
respective strengths and weaknesses when applied in different scenarios.
Locally hosted FL enhances performance when data aggregation is not feasible,
while cloud-based FL becomes more practical with a significant increase in the
number of clients, addressing scalability and connectivity challenges.

</details>

### [78] [Exploring How LLMs Capture and Represent Domain-Specific Knowledge](https://arxiv.org/abs/2504.16871)
*Mirian Hipolito Garcia,Camille Couturier,Daniel Madrigal Diaz,Ankur Mallick,Anastasios Kyrillidis,Robert Sim,Victor Ruhle,Saravan Rajmohan*

Main category: cs.LG

TLDR: 研究大型语言模型（LLMs）是否自然捕捉领域特定语言细微差别，通过隐藏状态分析其领域敏感性，并揭示模型内部对查询领域的识别能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否能够区分不同领域的查询，并研究其领域表示的鲁棒性。

Method: 通过预填充阶段生成的隐藏状态分析LLMs的领域敏感性，并利用这些表示进行模型选择。

Result: LLMs能区分相关领域查询，且微调模型并非总是最准确。

Conclusion: LLMs具备领域识别能力，适用于封闭和开放生成任务。

Abstract: We study whether Large Language Models (LLMs) inherently capture
domain-specific nuances in natural language. Our experiments probe the domain
sensitivity of LLMs by examining their ability to distinguish queries from
different domains using hidden states generated during the prefill phase. We
reveal latent domain-related trajectories that indicate the model's internal
recognition of query domains. We also study the robustness of these domain
representations to variations in prompt styles and sources. Our approach
leverages these representations for model selection, mapping the LLM that best
matches the domain trace of the input query (i.e., the model with the highest
performance on similar traces). Our findings show that LLMs can differentiate
queries for related domains, and that the fine-tuned model is not always the
most accurate. Unlike previous work, our interpretations apply to both closed
and open-ended generative tasks

</details>

### [79] [Hybrid Reinforcement Learning and Model Predictive Control for Adaptive Control of Hydrogen-Diesel Dual-Fuel Combustion](https://arxiv.org/abs/2504.16875)
*Julian Bedei,Murray McBain,Charles Robert Koch,Jakob Andert,David Gordon*

Main category: cs.LG

TLDR: 提出了一种结合强化学习（RL）和机器学习模型预测控制（ML-MPC）的混合方法，用于优化氢-柴油双燃料发动机控制，解决了RL的不安全探索和ML-MPC的适应性不足问题。


<details>
  <summary>Details</summary>
Motivation: RL和ML-MPC各自在控制双燃料发动机时存在局限性：RL在早期学习阶段可能执行不安全动作，而ML-MPC对系统漂移适应性不足。

Method: 采用混合方法，ML-MPC提供安全控制，RL动态调整ML-MPC的负载跟踪参考以适应环境变化。

Result: 实验表明，RL成功适应边界条件变化，ML-MPC确保控制安全，负载跟踪的RMSE从0.57 bar降至0.44 bar。

Conclusion: 混合方法有效结合了RL的适应性和ML-MPC的安全性，显著提升了双燃料发动机的控制性能。

Abstract: Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive
Control (ML-MPC) are promising approaches for optimizing hydrogen-diesel
dual-fuel engine control, as they can effectively control multiple-input
multiple-output systems and nonlinear processes. ML-MPC is advantageous for
providing safe and optimal controls, ensuring the engine operates within
predefined safety limits. In contrast, RL is distinguished by its adaptability
to changing conditions through its learning-based approach. However, the
practical implementation of either method alone poses challenges. RL requires
high variance in control inputs during early learning phases, which can pose
risks to the system by potentially executing unsafe actions, leading to
mechanical damage. Conversely, ML-MPC relies on an accurate system model to
generate optimal control inputs and has limited adaptability to system drifts,
such as injector aging, which naturally occur in engine applications. To
address these limitations, this study proposes a hybrid RL and ML-MPC approach
that uses an ML-MPC framework while incorporating an RL agent to dynamically
adjust the ML-MPC load tracking reference in response to changes in the
environment. At the same time, the ML-MPC ensures that actions stay safe
throughout the RL agent's exploration. To evaluate the effectiveness of this
approach, fuel pressure is deliberately varied to introduce a model-plant
mismatch between the ML-MPC and the engine test bench. The result of this
mismatch is a root mean square error (RMSE) in indicated mean effective
pressure of 0.57 bar when running the ML-MPC. The experimental results
demonstrate that RL successfully adapts to changing boundary conditions by
altering the tracking reference while ML-MPC ensures safe control inputs. The
quantitative improvement in load tracking by implementing RL is an RSME of 0.44
bar.

</details>

### [80] [I-Con: A Unifying Framework for Representation Learning](https://arxiv.org/abs/2504.16929)
*Shaden Alshammari,John Hershey,Axel Feldmann,William T. Freeman,Mark Hamilton*

Main category: cs.LG

TLDR: 论文提出了一种信息论框架，统一了多种现代机器学习损失函数，揭示了聚类、谱方法、降维、对比学习和监督学习背后的信息几何结构，并基于此开发了新的损失函数，提升了无监督图像分类的性能。


<details>
  <summary>Details</summary>
Motivation: 随着表征学习领域的发展，出现了大量针对不同问题的损失函数。本文旨在提出一个统一的框架，将这些损失函数归纳为一个信息论方程，揭示其共同的信息几何结构。

Method: 引入一个信息论方程，将多种机器学习方法统一为最小化两个条件分布（监督表示和学习表示）之间的KL散度。通过这一框架，连接了23种不同方法，并开发了新的损失函数。

Result: 在无监督图像分类任务中，新方法比现有最佳方法提升了8%的性能（ImageNet-1K数据集）。此外，还展示了如何用于改进对比表示学习中的去偏方法。

Conclusion: 该框架不仅统一了多种机器学习方法，还为开发新损失函数提供了理论基础，显著提升了无监督学习的性能。

Abstract: As the field of representation learning grows, there has been a proliferation
of different loss functions to solve different classes of problems. We
introduce a single information-theoretic equation that generalizes a large
collection of modern loss functions in machine learning. In particular, we
introduce a framework that shows that several broad classes of machine learning
methods are precisely minimizing an integrated KL divergence between two
conditional distributions: the supervisory and learned representations. This
viewpoint exposes a hidden information geometry underlying clustering, spectral
methods, dimensionality reduction, contrastive learning, and supervised
learning. This framework enables the development of new loss functions by
combining successful techniques from across the literature. We not only present
a wide array of proofs, connecting over 23 different approaches, but we also
leverage these theoretical results to create state-of-the-art unsupervised
image classifiers that achieve a +8% improvement over the prior
state-of-the-art on unsupervised classification on ImageNet-1K. We also
demonstrate that I-Con can be used to derive principled debiasing methods which
improve contrastive representation learners.

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [81] [FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking](https://arxiv.org/abs/2504.16188)
*Jabez Magomere,Elena Kochkina,Samuel Mensah,Simerjot Kaur,Charese H. Smiley*

Main category: cs.CL

TLDR: FinNLI是一个用于金融自然语言推理的基准数据集，包含21,304对数据，测试集由金融专家标注。实验显示通用NLI模型在金融领域表现下降，金融LLM泛化能力有限。


<details>
  <summary>Details</summary>
Motivation: 构建一个金融领域的NLI基准数据集，以评估模型在金融文本上的推理能力，并揭示当前模型的局限性。

Method: 通过构建多样化的前提-假设对，避免虚假相关性，并利用金融专家标注高质量测试集。

Result: 通用NLI模型在金融领域表现显著下降，最高Macro F1为74.57%（PLMs）和78.62%（LLMs）。金融LLM泛化能力不足。

Conclusion: FinNLI揭示了当前LLM在金融推理上的弱点，表明仍有改进空间。

Abstract: We introduce FinNLI, a benchmark dataset for Financial Natural Language
Inference (FinNLI) across diverse financial texts like SEC Filings, Annual
Reports, and Earnings Call transcripts. Our dataset framework ensures diverse
premise-hypothesis pairs while minimizing spurious correlations. FinNLI
comprises 21,304 pairs, including a high-quality test set of 3,304 instances
annotated by finance experts. Evaluations show that domain shift significantly
degrades general-domain NLI performance. The highest Macro F1 scores for
pre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and
78.62%, respectively, highlighting the dataset's difficulty. Surprisingly,
instruction-tuned financial LLMs perform poorly, suggesting limited
generalizability. FinNLI exposes weaknesses in current LLMs for financial
reasoning, indicating room for improvement.

</details>

### [82] [The Language of Attachment: Modeling Attachment Dynamics in Psychotherapy](https://arxiv.org/abs/2504.16271)
*Frederik Bredgaard,Martin Lund Trinhammer,Elisa Bassignana*

Main category: cs.CL

TLDR: 论文探讨了利用NLP技术自动评估患者依恋风格，以替代传统的手工评估方法，旨在提高心理治疗的个性化和研究效率。


<details>
  <summary>Details</summary>
Motivation: 传统的手工评估方法（如PACS）复杂且资源消耗大，限制了依恋风格在心理治疗中的广泛应用。通过NLP自动评估可以解决这一问题。

Method: 使用NLP分类模型对心理治疗转录文本进行分析，自动识别患者的依恋风格。

Result: 研究展示了自动评估的可行性，并讨论了误分类（如将‘焦虑型’误判为‘回避型’）对治疗结果的潜在影响。

Conclusion: 该研究为心理治疗的个性化和机制研究开辟了新途径，未来可通过NLP技术进一步优化。

Abstract: The delivery of mental healthcare through psychotherapy stands to benefit
immensely from developments within Natural Language Processing (NLP), in
particular through the automatic identification of patient specific qualities,
such as attachment style. Currently, the assessment of attachment style is
performed manually using the Patient Attachment Coding System (PACS; Talia et
al., 2017), which is complex, resource-consuming and requires extensive
training. To enable wide and scalable adoption of attachment informed treatment
and research, we propose the first exploratory analysis into automatically
assessing patient attachment style from psychotherapy transcripts using NLP
classification models. We further analyze the results and discuss the
implications of using automated tools for this purpose -- e.g., confusing
`preoccupied' patients with `avoidant' likely has a more negative impact on
therapy outcomes with respect to other mislabeling. Our work opens an avenue of
research enabling more personalized psychotherapy and more targeted research
into the mechanisms of psychotherapy through advancements in NLP.

</details>

### [83] [The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation](https://arxiv.org/abs/2504.16286)
*Li Weigang,Pedro Carvalho Brom*

Main category: cs.CL

TLDR: 该研究通过构建多样化语料库和BT-Fried评估系统，比较了六种大型语言模型和三种传统翻译工具在中文-英文翻译中的表现，发现LLMs在文化保留和文学翻译方面存在不足，并提出了一种改进的BLEU变体。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决大型语言模型在中文-英文翻译中保留诗意意图、文化传承和处理专业术语的挑战。

Method: 构建多样化语料库，使用基于回译和Friedman测试的BT-Fried系统评估六种LLMs和三种传统工具。

Result: 发现科学摘要适合回译，传统工具在语言差异大的文本中表现更好；LLMs在文化和文学保留上表现不佳；提出了一种改进的BLEU变体。

Conclusion: 研究为中文NLP性能的实证评估提供了贡献，并深化了对AI翻译中文化保真度的理解。

Abstract: The rapid advancement of large language models (LLMs) has reshaped the
landscape of machine translation, yet challenges persist in preserving poetic
intent, cultural heritage, and handling specialized terminology in
Chinese-English translation. This study constructs a diverse corpus
encompassing Chinese scientific terminology, historical translation paradoxes,
and literary metaphors. Utilizing a back-translation and Friedman test-based
evaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic
similarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three
traditional translation tools. Key findings include: (1) Scientific abstracts
often benefit from back-translation, while traditional tools outperform LLMs in
linguistically distinct texts; (2) LLMs struggle with cultural and literary
retention, exemplifying the "paradox of poetic intent"; (3) Some models exhibit
"verbatim back-translation", reflecting emergent memory behavior; (4) A novel
BLEU variant using Jieba segmentation and n-gram weighting is proposed. The
study contributes to the empirical evaluation of Chinese NLP performance and
advances understanding of cultural fidelity in AI-mediated translation.

</details>

### [84] [Capturing Symmetry and Antisymmetry in Language Models through Symmetry-Aware Training Objectives](https://arxiv.org/abs/2504.16312)
*Zhangdie Yuan,Andreas Vlachos*

Main category: cs.CL

TLDR: 论文提出了一种基于Wikidata的自然语言推理数据集，用于评估大语言模型（LLMs）在对称和反对称关系理解上的表现，发现LLMs表现接近随机。通过对比学习和k近邻的编码器重训练，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在对称和反对称关系理解上的不足，填补关系理解的空白。

Method: 引入Wikidata衍生的自然语言推理数据集，采用对比学习和k近邻方法重训练编码器。

Result: LLMs在基准测试中表现接近随机，重训练后的编码器性能与微调分类头相当，且具有更高的少样本学习效率和抗灾难性遗忘能力。

Conclusion: 编码器重训练是提升LLMs关系理解的有效方法，同时具备效率和抗遗忘优势。

Abstract: Capturing symmetric (e.g., country borders another country) and antisymmetric
(e.g., parent_of) relations is crucial for a variety of applications. This
paper tackles this challenge by introducing a novel Wikidata-derived natural
language inference dataset designed to evaluate large language models (LLMs).
Our findings reveal that LLMs perform comparably to random chance on this
benchmark, highlighting a gap in relational understanding. To address this, we
explore encoder retraining via contrastive learning with k-nearest neighbors.
The retrained encoder matches the performance of fine-tuned classification
heads while offering additional benefits, including greater efficiency in
few-shot learning and improved mitigation of catastrophic forgetting.

</details>

### [85] [Transformer-Based Extraction of Statutory Definitions from the U.S. Code](https://arxiv.org/abs/2504.16353)
*Arpana Hosabettu,Harsh Shah*

Main category: cs.CL

TLDR: 提出一种基于Transformer的NLP系统，用于自动从美国法典中提取法律定义，显著提高了准确率。


<details>
  <summary>Details</summary>
Motivation: 提升对复杂法律文本（如美国法典）的理解和清晰度，解决自动识别法律定义的挑战。

Method: 采用领域特定的Legal-BERT模型，结合多阶段管道（文档结构分析和语言模型），提取定义及其范围。

Result: 在多个美国法典标题上评估，最佳模型达到96.8%精确率和98.9%召回率（98.2% F1分数）。

Conclusion: 显著提升法律信息的可访问性，为下游法律推理任务奠定基础。

Abstract: Automatic extraction of definitions from legal texts is critical for
enhancing the comprehension and clarity of complex legal corpora such as the
United States Code (U.S.C.). We present an advanced NLP system leveraging
transformer-based architectures to automatically extract defined terms, their
definitions, and their scope from the U.S.C. We address the challenges of
automatically identifying legal definitions, extracting defined terms, and
determining their scope within this complex corpus of over 200,000 pages of
federal statutory law. Building upon previous feature-based machine learning
methods, our updated model employs domain-specific transformers (Legal-BERT)
fine-tuned specifically for statutory texts, significantly improving extraction
accuracy. Our work implements a multi-stage pipeline that combines document
structure analysis with state-of-the-art language models to process legal text
from the XML version of the U.S. Code. Each paragraph is first classified using
a fine-tuned legal domain BERT model to determine if it contains a definition.
Our system then aggregates related paragraphs into coherent definitional units
and applies a combination of attention mechanisms and rule-based patterns to
extract defined terms and their jurisdictional scope. The definition extraction
system is evaluated on multiple titles of the U.S. Code containing thousands of
definitions, demonstrating significant improvements over previous approaches.
Our best model achieves 96.8% precision and 98.9% recall (98.2% F1-score),
substantially outperforming traditional machine learning classifiers. This work
contributes to improving accessibility and understanding of legal information
while establishing a foundation for downstream legal reasoning tasks.

</details>

### [86] [Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language Questions](https://arxiv.org/abs/2504.16358)
*Tian Bai,Huiyan Ying,Kailong Suo,Junqiu Wei,Tao Fan,Yuanfeng Song*

Main category: cs.CL

TLDR: 本文介绍了Text-to-TrajVis任务，旨在将自然语言问题转化为轨迹数据可视化，并提出了Trajectory Visualization Language（TVL）和数据集TrajVL。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言接口在轨迹可视化系统中的开发问题，填补相关数据集的空白。

Method: 提出TVL语言，结合LLMs和人工构建高质量数据集TrajVL，并评估多种LLMs的性能。

Result: 成功构建包含18,140对（问题，TVL）的数据集，实验证明任务可行但具挑战性。

Conclusion: Text-to-TrajVis任务值得进一步研究。

Abstract: This paper introduces the Text-to-TrajVis task, which aims to transform
natural language questions into trajectory data visualizations, facilitating
the development of natural language interfaces for trajectory visualization
systems. As this is a novel task, there is currently no relevant dataset
available in the community. To address this gap, we first devised a new
visualization language called Trajectory Visualization Language (TVL) to
facilitate querying trajectory data and generating visualizations. Building on
this foundation, we further proposed a dataset construction method that
integrates Large Language Models (LLMs) with human efforts to create
high-quality data. Specifically, we first generate TVLs using a comprehensive
and systematic process, and then label each TVL with corresponding natural
language questions using LLMs. This process results in the creation of the
first large-scale Text-to-TrajVis dataset, named TrajVL, which contains 18,140
(question, TVL) pairs. Based on this dataset, we systematically evaluated the
performance of multiple LLMs (GPT, Qwen, Llama, etc.) on this task. The
experimental results demonstrate that this task is both feasible and highly
challenging and merits further exploration within the research community.

</details>

### [87] [SplitReason: Learning To Offload Reasoning](https://arxiv.org/abs/2504.16379)
*Yash Akhauri,Anthony Fei,Chi-Chih Chang,Ahmed F. AbouElhamayed,Yueying Li,Mohamed S. Abdelfattah*

Main category: cs.CL

TLDR: 论文提出了一种通过将推理过程中最困难的部分卸载到更大模型的方法，以提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在推理任务中生成较长的序列，导致效率低下，但并非所有部分都同样困难。

Method: 通过标注困难部分并训练小模型识别和触发卸载，结合监督和强化学习微调。

Result: AIME24推理准确率提升24%和28.3%，同时仅卸载少量token。

Conclusion: SplitReason方法有效平衡了效率和准确性，相关资源已开源。

Abstract: Reasoning in large language models (LLMs) tends to produce substantially
longer token generation sequences than simpler language modeling tasks. This
extended generation length reflects the multi-step, compositional nature of
reasoning and is often correlated with higher solution accuracy. From an
efficiency perspective, longer token generation exacerbates the inherently
sequential and memory-bound decoding phase of LLMs. However, not all parts of
this expensive reasoning process are equally difficult to generate. We leverage
this observation by offloading only the most challenging parts of the reasoning
process to a larger, more capable model, while performing most of the
generation with a smaller, more efficient model; furthermore, we teach the
smaller model to identify these difficult segments and independently trigger
offloading when needed. To enable this behavior, we annotate difficult segments
across 18k reasoning traces from the OpenR1-Math-220k chain-of-thought (CoT)
dataset. We then apply supervised fine-tuning (SFT) and reinforcement learning
fine-tuning (RLFT) to a 1.5B-parameter reasoning model, training it to learn to
offload the most challenging parts of its own reasoning process to a larger
model. This approach improves AIME24 reasoning accuracy by 24% and 28.3% while
offloading 1.35% and 5% of the generated tokens respectively. We open-source
our SplitReason model, data, code and logs.

</details>

### [88] [ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs](https://arxiv.org/abs/2504.16394)
*Fahmida Liza Piya,Rahmatollah Beheshti*

Main category: cs.CL

TLDR: ConTextual框架结合上下文保留令牌过滤和领域知识图谱，提升临床文本摘要的准确性和临床相关性。


<details>
  <summary>Details</summary>
Motivation: 现有临床文本摘要方法未能充分保留关键临床信息，导致决策支持不足。

Method: 提出ConTextual框架，集成上下文保留令牌过滤和领域知识图谱，增强临床文本的上下文和结构化知识。

Result: 在两个公共数据集上，ConTextual表现优于基线方法，提升了语言连贯性和临床保真度。

Conclusion: 令牌级过滤与结构化检索互补，为临床文本生成提供了可扩展的高精度解决方案。

Abstract: Unstructured clinical data can serve as a unique and rich source of
information that can meaningfully inform clinical practice. Extracting the most
pertinent context from such data is critical for exploiting its true potential
toward optimal and timely decision-making in patient care. While prior research
has explored various methods for clinical text summarization, most prior
studies either process all input tokens uniformly or rely on heuristic-based
filters, which can overlook nuanced clinical cues and fail to prioritize
information critical for decision-making. In this study, we propose Contextual,
a novel framework that integrates a Context-Preserving Token Filtering method
with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By
preserving context-specific important tokens and enriching them with structured
knowledge, ConTextual improves both linguistic coherence and clinical fidelity.
Our extensive empirical evaluations on two public benchmark datasets
demonstrate that ConTextual consistently outperforms other baselines. Our
proposed approach highlights the complementary role of token-level filtering
and structured retrieval in enhancing both linguistic and clinical integrity,
as well as offering a scalable solution for improving precision in clinical
text generation.

</details>

### [89] [Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation](https://arxiv.org/abs/2504.16408)
*Jiahao Yuan,Xingzhe Sun,Xing Yu,Jingwen Wang,Dehui Du,Zhiqing Cui,Zixiang Di*

Main category: cs.CL

TLDR: XLLM@ACL2025 Shared Task-III提出了一种低资源结构化推理任务，要求LLMs用少量标注数据生成可解释的逐步推理。Less is More方法通过多智能体框架和反向提示诱导等技术，在仅24个标注样本下实现了高质量结构化推理。


<details>
  <summary>Details</summary>
Motivation: 解决低资源环境下结构化推理的挑战，提升LLMs在少量标注数据下的表现。

Method: 采用多智能体框架，结合反向提示诱导、检索增强推理合成和双阶段奖励引导过滤，基于Meta-Llama-3-8B-Instruct进行微调。

Result: 在XLLM@ACL2025 Shared Task-III中获得第三名，验证了可控数据蒸馏在低资源结构化推理中的有效性。

Conclusion: Less is More方法展示了在低资源条件下通过可控数据蒸馏提升结构化推理质量的潜力。

Abstract: The XLLM@ACL2025 Shared Task-III formulates a low-resource structural
reasoning task that challenges LLMs to generate interpretable, step-by-step
rationales with minimal labeled data. We present Less is More, the third-place
winning approach in the XLLM@ACL2025 Shared Task-III, which focuses on
structured reasoning from only 24 labeled examples. Our approach leverages a
multi-agent framework with reverse-prompt induction, retrieval-augmented
reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to
distill high-quality supervision across three subtasks: question parsing, CoT
parsing, and step-level verification. All modules are fine-tuned from
Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure
validation with reward filtering across few-shot and zero-shot prompts, our
pipeline consistently improves structure reasoning quality. These results
underscore the value of controllable data distillation in enhancing structured
inference under low-resource constraints. Our code is available at
https://github.com/Jiahao-Yuan/Less-is-More.

</details>

### [90] [Out-of-the-Box Conditional Text Embeddings from Large Language Models](https://arxiv.org/abs/2504.16411)
*Kosuke Yamada,Peinan Zhang*

Main category: cs.CL

TLDR: PonTE是一种无监督条件文本嵌入方法，利用因果大语言模型和条件提示生成文本嵌入，性能接近监督方法，且无需微调。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法依赖大量训练数据和微调带来的高成本问题。

Method: 结合因果大语言模型和条件提示，无监督生成条件文本嵌入。

Result: 在条件语义文本相似性和文本聚类任务中表现接近监督方法，且具有可解释性。

Conclusion: PonTE提供了一种高效、低成本的条件文本嵌入解决方案，适用于多种任务。

Abstract: Conditional text embedding is a proposed representation that captures the
shift in perspective on texts when conditioned on a specific aspect. Previous
methods have relied on extensive training data for fine-tuning models, leading
to challenges in terms of labor and resource costs. We propose PonTE, a novel
unsupervised conditional text embedding method that leverages a causal large
language model and a conditional prompt. Through experiments on conditional
semantic text similarity and text clustering, we demonstrate that PonTE can
generate useful conditional text embeddings and achieve performance comparable
to supervised methods without fine-tuning. We also show the interpretability of
text embeddings with PonTE by analyzing word generation following prompts and
embedding visualization.

</details>

### [91] [Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study](https://arxiv.org/abs/2504.16414)
*Mohammad Khodadad,Ali Shiraee Kasmaee,Mahdi Astaraki,Nicholas Sherck,Hamidreza Mahyar,Soheila Samiee*

Main category: cs.CL

TLDR: 该研究引入了一个新基准，用于评估大型语言模型在化学领域的组合推理能力，揭示了即使最先进的模型在多跳推理中也面临挑战。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在化学领域的组合推理能力，并探索如何通过文档检索增强其性能。

Method: 设计了一个自动化流程，结合OpenAI推理模型和命名实体识别系统，构建知识图谱并生成多跳问题。

Result: 实验表明，即使有完整上下文，模型在多跳推理中仍存在错误，文档检索能显著提升性能但无法完全消除错误。

Conclusion: 该研究不仅揭示了当前模型的局限性，还提出了一种可生成挑战性推理数据集的新方法，推动了计算语言学中对推理的理解。

Abstract: In this study, we introduced a new benchmark consisting of a curated dataset
and a defined evaluation process to assess the compositional reasoning
capabilities of large language models within the chemistry domain. We designed
and validated a fully automated pipeline, verified by subject matter experts,
to facilitate this task. Our approach integrates OpenAI reasoning models with
named entity recognition (NER) systems to extract chemical entities from recent
literature, which are then augmented with external knowledge bases to form a
comprehensive knowledge graph. By generating multi-hop questions across these
graphs, we assess LLM performance in both context-augmented and non-context
augmented settings. Our experiments reveal that even state-of-the-art models
face significant challenges in multi-hop compositional reasoning. The results
reflect the importance of augmenting LLMs with document retrieval, which can
have a substantial impact on improving their performance. However, even perfect
retrieval accuracy with full context does not eliminate reasoning errors,
underscoring the complexity of compositional reasoning. This work not only
benchmarks and highlights the limitations of current LLMs but also presents a
novel data generation pipeline capable of producing challenging reasoning
datasets across various domains. Overall, this research advances our
understanding of reasoning in computational linguistics.

</details>

### [92] [Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark](https://arxiv.org/abs/2504.16427)
*Hanlei Zhang,Zhuohang Li,Yeshuang Zhu,Hua Xu,Peiwu Wang,Jinchao Zhang,Jie Zhou,Haige Zhu*

Main category: cs.CL

TLDR: MMLA是一个多模态语言分析基准，用于评估MLLMs在理解认知级语义方面的能力，覆盖六种核心语义维度。实验显示当前模型的准确率仅为60%~70%，表明其局限性。


<details>
  <summary>Details</summary>
Motivation: 多模态语言分析领域缺乏对MLLMs理解认知级语义能力的研究，MMLA旨在填补这一空白。

Method: MMLA包含61K多模态话语，评估了八种主流LLMs和MLLMs，采用零样本推理、监督微调和指令调优三种方法。

Result: 实验表明，即使微调后的模型准确率也仅为60%~70%，揭示了当前MLLMs在理解复杂人类语言上的不足。

Conclusion: MMLA为探索大语言模型在多模态语言分析中的潜力提供了基础，并开源了数据集和代码。

Abstract: Multimodal language analysis is a rapidly evolving field that leverages
multiple modalities to enhance the understanding of high-level semantics
underlying human conversational utterances. Despite its significance, little
research has investigated the capability of multimodal large language models
(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce
MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA
comprises over 61K multimodal utterances drawn from both staged and real-world
scenarios, covering six core dimensions of multimodal semantics: intent,
emotion, dialogue act, sentiment, speaking style, and communication behavior.
We evaluate eight mainstream branches of LLMs and MLLMs using three methods:
zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive
experiments reveal that even fine-tuned models achieve only about 60%~70%
accuracy, underscoring the limitations of current MLLMs in understanding
complex human language. We believe that MMLA will serve as a solid foundation
for exploring the potential of large language models in multimodal language
analysis and provide valuable resources to advance this field. The datasets and
code are open-sourced at https://github.com/thuiar/MMLA.

</details>

### [93] [EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records](https://arxiv.org/abs/2504.16448)
*Shuguang Zhao,Qiangzhong Feng,Zhiyang He,Peipei Sun,Yingying Wang,Xiaodong Tao,Xiaoliang Lu,Mei Cheng,Xinyue Wu,Yanyan Wang,Wei Liang*

Main category: cs.CL

TLDR: EMRModel结合LoRA微调和代码风格提示设计，高效将医疗咨询对话转为结构化电子病历，实验F1分数达88.1%，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 医疗咨询对话的非结构化特性限制了其在诊断和治疗中的有效利用，传统方法难以捕捉深层语义。

Method: 提出EMRModel，结合LoRA微调和代码风格提示设计，构建高质量标注数据集，并引入细粒度评估基准。

Result: 实验显示EMRModel的F1分数为88.1%，比标准预训练模型提升49.5%，优于传统LoRA微调方法。

Conclusion: EMRModel在结构化医疗记录提取任务中表现优异，推动了医疗NLP模型的优化。

Abstract: Medical consultation dialogues contain critical clinical information, yet
their unstructured nature hinders effective utilization in diagnosis and
treatment. Traditional methods, relying on rule-based or shallow machine
learning techniques, struggle to capture deep and implicit semantics. Recently,
large pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight
fine-tuning method, have shown promise for structured information extraction.
We propose EMRModel, a novel approach that integrates LoRA-based fine-tuning
with code-style prompt design, aiming to efficiently convert medical
consultation dialogues into structured electronic medical records (EMRs).
Additionally, we construct a high-quality, realistically grounded dataset of
medical consultation dialogues with detailed annotations. Furthermore, we
introduce a fine-grained evaluation benchmark for medical consultation
information extraction and provide a systematic evaluation methodology,
advancing the optimization of medical natural language processing (NLP) models.
Experimental results show EMRModel achieves an F1 score of 88.1%, improving
by49.5% over standard pre-trained models. Compared to traditional LoRA
fine-tuning methods, our model shows superior performance, highlighting its
effectiveness in structured medical record extraction tasks.

</details>

### [94] [T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning](https://arxiv.org/abs/2504.16460)
*Vignesh Ethiraj,Sidhanth Menon,Divya Vijay*

Main category: cs.CL

TLDR: T-VEC是一种专为电信领域设计的嵌入模型，通过深度微调显著提升语义捕捉能力，优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 电信行业的专业词汇和复杂概念对通用NLP模型构成挑战，需要领域专用嵌入模型。

Method: 基于gte-Qwen2-1.5B-instruct模型，通过三重损失目标在大规模电信数据集上深度微调，并开发专用分词器。

Result: T-VEC在MTEB评分（0.825）和电信专用评测（0.9380）中表现优异，显著优于其他模型。

Conclusion: T-VEC为电信AI提供了强大的开源工具，NetoAI在领域创新中处于领先地位。

Abstract: The specialized vocabulary and complex concepts of the telecommunications
industry present significant challenges for standard Natural Language
Processing models. Generic text embeddings often fail to capture
telecom-specific semantics, hindering downstream task performance. We introduce
T-VEC (Telecom Vectorization Model), a novel embedding model tailored for the
telecom domain through deep fine-tuning. Developed by NetoAI, T-VEC is created
by adapting the state-of-the-art gte-Qwen2-1.5B-instruct model using a triplet
loss objective on a meticulously curated, large-scale dataset of
telecom-specific data. Crucially, this process involved substantial
modification of weights across 338 layers of the base model, ensuring deep
integration of domain knowledge, far exceeding superficial adaptation
techniques. We quantify this deep change via weight difference analysis. A key
contribution is the development and open-sourcing (MIT License) of the first
dedicated telecom-specific tokenizer, enhancing the handling of industry
jargon. T-VEC achieves a leading average MTEB score (0.825) compared to
established models and demonstrates vastly superior performance (0.9380 vs.
less than 0.07) on our internal telecom-specific triplet evaluation benchmark,
indicating an exceptional grasp of domain-specific nuances, visually confirmed
by improved embedding separation. This work positions NetoAI at the forefront
of telecom AI innovation, providing the community with a powerful, deeply
adapted, open-source tool.

</details>

### [95] [QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining](https://arxiv.org/abs/2504.16511)
*Fengze Liu,Weidong Zhou,Binbin Liu,Zhimiao Yu,Yifan Zhang,Haobin Lin,Yifeng Yu,Xiaohuan Zhou,Taifeng Wang,Yong Cao*

Main category: cs.CL

TLDR: QuaDMix是一个统一的数据选择框架，用于优化LLM预训练数据分布，平衡质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常单独优化数据质量和多样性，忽略了二者之间的权衡，需要联合考虑。

Method: 提出多标准衡量数据质量，通过域分类衡量多样性，使用参数化采样函数平衡二者。

Result: 实验表明，QuaDMix在多个基准测试中平均性能提升7.2%。

Conclusion: QuaDMix证明了平衡数据质量和多样性的必要性和能力。

Abstract: Quality and diversity are two critical metrics for the training data of large
language models (LLMs), positively impacting performance. Existing studies
often optimize these metrics separately, typically by first applying quality
filtering and then adjusting data proportions. However, these approaches
overlook the inherent trade-off between quality and diversity, necessitating
their joint consideration. Given a fixed training quota, it is essential to
evaluate both the quality of each data point and its complementary effect on
the overall dataset. In this paper, we introduce a unified data selection
framework called QuaDMix, which automatically optimizes the data distribution
for LLM pretraining while balancing both quality and diversity. Specifically,
we first propose multiple criteria to measure data quality and employ domain
classification to distinguish data points, thereby measuring overall diversity.
QuaDMix then employs a unified parameterized data sampling function that
determines the sampling probability of each data point based on these quality
and diversity related labels. To accelerate the search for the optimal
parameters involved in the QuaDMix framework, we conduct simulated experiments
on smaller models and use LightGBM for parameters searching, inspired by the
RegMix method. Our experiments across diverse models and datasets demonstrate
that QuaDMix achieves an average performance improvement of 7.2% across
multiple benchmarks. These results outperform the independent strategies for
quality and diversity, highlighting the necessity and ability to balance data
quality and diversity.

</details>

### [96] [Transformers for Complex Query Answering over Knowledge Hypergraphs](https://arxiv.org/abs/2504.16537)
*Hong Ting Tsang,Zihao Wang,Yangqiu Song*

Main category: cs.CL

TLDR: 论文提出了一种两阶段Transformer模型LKHGT，用于回答知识超图（KHG）中的复杂查询，并在新数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据复杂，传统三元组知识图谱（KG）表达能力有限，需要更灵活的超关系图模型。

Method: 提出LKHGT模型，包含投影编码器和逻辑编码器，支持原子投影和复杂逻辑操作，采用类型感知偏置（TAB）捕捉交互。

Result: 实验表明LKHGT在KHG上的复杂查询回答（CQA）性能最优，且能泛化到分布外查询类型。

Conclusion: LKHGT为知识超图的复杂查询提供了高效解决方案，扩展了CQA的应用范围。

Abstract: Complex Query Answering (CQA) has been extensively studied in recent years.
In order to model data that is closer to real-world distribution, knowledge
graphs with different modalities have been introduced. Triple KGs, as the
classic KGs composed of entities and relations of arity 2, have limited
representation of real-world facts. Real-world data is more sophisticated.
While hyper-relational graphs have been introduced, there are limitations in
representing relationships of varying arity that contain entities with equal
contributions. To address this gap, we sampled new CQA datasets: JF17k-HCQA and
M-FB15k-HCQA. Each dataset contains various query types that include logical
operations such as projection, negation, conjunction, and disjunction. In order
to answer knowledge hypergraph (KHG) existential first-order queries, we
propose a two-stage transformer model, the Logical Knowledge Hypergraph
Transformer (LKHGT), which consists of a Projection Encoder for atomic
projection and a Logical Encoder for complex logical operations. Both encoders
are equipped with Type Aware Bias (TAB) for capturing token interactions.
Experimental results on CQA datasets show that LKHGT is a state-of-the-art CQA
method over KHG and is able to generalize to out-of-distribution query types.

</details>

### [97] [PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression](https://arxiv.org/abs/2504.16574)
*Lizhe Chen,Binjia Zhou,Yuyao Ge,Jiayi Chen,Shiguang NI*

Main category: cs.CL

TLDR: 论文提出Prompt Importance Sampling (PIS)框架，通过动态采样重要token压缩提示，结合token和语义级压缩，提升LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有提示压缩方法忽视LLM内在机制且缺乏系统评估，高成本限制LLM广泛应用。

Method: PIS框架：1) token级基于注意力分数量化重要性，轻量RL网络自适应压缩；2) 语义级采用俄罗斯轮盘采样策略。

Result: 多领域基准测试中达到最优压缩性能，意外提升推理效率。

Conclusion: PIS为提示工程提供理论和实践基础，优化LLM上下文管理。

Abstract: Large language models (LLMs) have achieved remarkable progress, demonstrating
unprecedented capabilities across various natural language processing tasks.
However, the high costs associated with such exceptional performance limit the
widespread adoption of LLMs, highlighting the need for prompt compression.
Existing prompt compression methods primarily rely on heuristic truncation or
abstractive summarization techniques, which fundamentally overlook the
intrinsic mechanisms of LLMs and lack a systematic evaluation of token
importance for generation. In this work, we introduce Prompt Importance
Sampling (PIS), a novel compression framework that dynamically compresses
prompts by sampling important tokens based on the analysis of attention scores
of hidden states. PIS employs a dual-level compression mechanism: 1) at the
token level, we quantify saliency using LLM-native attention scores and
implement adaptive compression through a lightweight 9-layer reinforcement
learning (RL) network; 2) at the semantic level, we propose a Russian roulette
sampling strategy for sentence-level importance sampling. Comprehensive
evaluations across multiple domain benchmarks demonstrate that our method
achieves state-of-the-art compression performance. Notably, our framework
serendipitously enhances reasoning efficiency through optimized context
structuring. This work advances prompt engineering by offering both theoretical
grounding and practical efficiency in context management for LLMs.

</details>

### [98] [Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study](https://arxiv.org/abs/2504.16601)
*Andy Li,Wei Zhou,Rashina Hoda,Chris Bain,Peter Poon*

Main category: cs.CL

TLDR: 研究比较了大语言模型（LLMs）与传统机器翻译（MT）工具在将英文医疗咨询摘要翻译为阿拉伯语、中文和越南语时的表现。结果显示传统MT工具表现更优，而LLMs在简单文本翻译中展现出潜力。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs和传统MT工具在医疗翻译中的表现，以了解其适用性和局限性。

Method: 使用标准自动化指标评估患者友好型和临床医生导向型文本的翻译质量。

Result: 传统MT工具表现更优，尤其对复杂文本；LLMs在越南语和中文的简单文本翻译中表现较好。阿拉伯语翻译随复杂度提升而改善。

Conclusion: LLMs虽具上下文灵活性，但表现不一致，当前评估指标未能捕捉临床相关性。需领域特定训练、改进评估方法及人工监督。

Abstract: This study evaluates how well large language models (LLMs) and traditional
machine translation (MT) tools translate medical consultation summaries from
English into Arabic, Chinese, and Vietnamese. It assesses both patient,
friendly and clinician, focused texts using standard automated metrics. Results
showed that traditional MT tools generally performed better, especially for
complex texts, while LLMs showed promise, particularly in Vietnamese and
Chinese, when translating simpler summaries. Arabic translations improved with
complexity due to the language's morphology. Overall, while LLMs offer
contextual flexibility, they remain inconsistent, and current evaluation
metrics fail to capture clinical relevance. The study highlights the need for
domain-specific training, improved evaluation methods, and human oversight in
medical translation.

</details>

### [99] [Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories](https://arxiv.org/abs/2504.16604)
*Mareike Lisker,Christina Gottschalk,Helena Mihaljević*

Main category: cs.CL

TLDR: 论文探讨了使用LLMs（如GPT-4o、Llama 3和Mistral）生成针对阴谋论的反对言论，发现其效果不佳，存在重复、肤浅和虚构内容的问题。


<details>
  <summary>Details</summary>
Motivation: 研究填补了缺乏针对阴谋论的反对言论数据集的空白，并探索LLMs在此领域的实用性。

Method: 通过结构化提示，评估GPT-4o、Llama 3和Mistral生成反对阴谋论言论的能力。

Result: 模型生成的反对言论常为通用、重复或肤浅，且过度承认恐惧并虚构事实。

Conclusion: 基于提示的LLMs在实际应用中生成反对阴谋论言论的效果有限，需进一步改进。

Abstract: Counterspeech is a key strategy against harmful online content, but scaling
expert-driven efforts is challenging. Large Language Models (LLMs) present a
potential solution, though their use in countering conspiracy theories is
under-researched. Unlike for hate speech, no datasets exist that pair
conspiracy theory comments with expert-crafted counterspeech. We address this
gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively
apply counterspeech strategies derived from psychological research provided
through structured prompts. Our results show that the models often generate
generic, repetitive, or superficial results. Additionally, they
over-acknowledge fear and frequently hallucinate facts, sources, or figures,
making their prompt-based use in practical applications problematic.

</details>

### [100] [TIFIN India at SemEval-2025: Harnessing Translation to Overcome Multilingual IR Challenges in Fact-Checked Claim Retrieval](https://arxiv.org/abs/2504.16627)
*Prasanna Devadiga,Arya Suneesh,Pawan Kumar Rajpoot,Bharatdeep Hazarika,Aditya U Baliga*

Main category: cs.CL

TLDR: 论文提出了一种两阶段策略，结合微调嵌入模型和基于LLM的重新排序器，解决单语和跨语言环境下事实核查的检索问题，并展示了LLM翻译在跨语言检索中的优势。


<details>
  <summary>Details</summary>
Motivation: 全球范围内虚假信息的泛滥使得事实核查变得至关重要，尤其是在多语言环境下。

Method: 采用两阶段策略：1) 使用微调嵌入模型进行基线检索；2) 基于LLM的重新排序器优化结果。同时，利用LLM翻译解决跨语言检索难题。

Result: 最终系统在单语和跨语言测试集上的success@10分数分别为0.938和0.81025。

Conclusion: 该方法在多语言环境下高效且可扩展，且能在消费级GPU上运行。

Abstract: We address the challenge of retrieving previously fact-checked claims in
monolingual and crosslingual settings - a critical task given the global
prevalence of disinformation. Our approach follows a two-stage strategy: a
reliable baseline retrieval system using a fine-tuned embedding model and an
LLM-based reranker. Our key contribution is demonstrating how LLM-based
translation can overcome the hurdles of multilingual information retrieval.
Additionally, we focus on ensuring that the bulk of the pipeline can be
replicated on a consumer GPU. Our final integrated system achieved a success@10
score of 0.938 and 0.81025 on the monolingual and crosslingual test sets,
respectively.

</details>

### [101] [A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics](https://arxiv.org/abs/2504.16677)
*Luisa Shimabucoro,Ahmet Ustun,Marzieh Fadaee,Sebastian Ruder*

Main category: cs.CL

TLDR: 研究探讨了多语言数据微调后大语言模型的跨语言迁移动态，发现其效果受多种因素综合影响，并提出了实践中有效的迁移条件。


<details>
  <summary>Details</summary>
Motivation: 理解跨语言迁移的动态机制，以提升大语言模型在多语言任务中的实用性。

Method: 使用两个模型家族（最大35B参数）在受控多语言数据上训练，研究三种生成任务（摘要、指令跟随、数学推理）在单任务和多任务微调下的表现。

Result: 跨语言迁移和性能无法由单一变量解释，效果取决于微调设置的组合。

Conclusion: 确定了实践中实现有效跨语言迁移的条件。

Abstract: In order for large language models to be useful across the globe, they are
fine-tuned to follow instructions on multilingual data. Despite the ubiquity of
such post-training, a clear understanding of the dynamics that enable
cross-lingual transfer remains elusive. This study examines cross-lingual
transfer (CLT) dynamics in realistic post-training settings. We study two model
families of up to 35B parameters in size trained on carefully controlled
mixtures of multilingual data on three generative tasks with varying levels of
complexity (summarization, instruction following, and mathematical reasoning)
in both single-task and multi-task instruction tuning settings. Overall, we
find that the dynamics of cross-lingual transfer and multilingual performance
cannot be explained by isolated variables, varying depending on the combination
of post-training settings. Finally, we identify the conditions that lead to
effective cross-lingual transfer in practice.

</details>

### [102] [HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations](https://arxiv.org/abs/2504.16754)
*Kwangseob Ahn*

Main category: cs.CL

TLDR: HEMA是一种受人类认知启发的双记忆系统，显著提升大语言模型在长对话中的连贯性和事实回忆能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长对话中难以保持连贯性，HEMA旨在解决这一问题。

Method: HEMA结合紧凑记忆（持续更新的单句摘要）和向量记忆（分块嵌入的存储），通过余弦相似度查询。

Result: 实验显示，HEMA将事实回忆准确率从41%提升至87%，对话连贯性评分从2.7提高到4.3。

Conclusion: HEMA证明结合逐字回忆与语义连续性可支持隐私友好的长对话AI，无需重新训练模型。

Abstract: Large language models (LLMs) struggle with maintaining coherence in extended
conversations spanning hundreds of turns, despite performing well within their
context windows. This paper introduces HEMA (Hippocampus-Inspired Extended
Memory Architecture), a dual-memory system inspired by human cognitive
processes. HEMA combines Compact Memory - a continuously updated one-sentence
summary preserving global narrative coherence, and Vector Memory - an episodic
store of chunk embeddings queried via cosine similarity. When integrated with a
6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns
while keeping prompt length under 3,500 tokens. Experimental results show
substantial improvements: factual recall accuracy increases from 41% to 87%,
and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K
indexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling
the area under the precision-recall curve compared to summarization-only
approaches. Ablation studies reveal two key insights: semantic forgetting
through age-weighted pruning reduces retrieval latency by 34% with minimal
recall loss, and a two-level summary hierarchy prevents cascade errors in
ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that
combining verbatim recall with semantic continuity provides a practical
solution for privacy-aware conversational AI capable of month-long dialogues
without model retraining.

</details>

### [103] [How Effective are Generative Large Language Models in Performing Requirements Classification?](https://arxiv.org/abs/2504.16768)
*Waad Alhoshan,Alessio Ferrari,Liping Zhao*

Main category: cs.CL

TLDR: 本文探讨了生成式大语言模型（如Bloom、Gemma和Llama）在需求分类任务中的表现，通过400多次实验发现提示设计和模型架构是关键因素，而数据集影响则因任务复杂度而异。


<details>
  <summary>Details</summary>
Motivation: 研究生成式LLMs在需求分类任务中的表现，填补现有研究中对此类模型探索的空白。

Method: 设计了400多次实验，使用三种生成式LLMs（Bloom、Gemma、Llama）在三个数据集（PROMISE NFR、Functional-Quality、SecReq）上进行二元和多类需求分类。

Result: 提示设计和模型架构对性能有普遍影响，而数据集的影响则取决于任务复杂度。

Conclusion: 未来模型开发和部署应优化提示结构，并根据任务需求调整模型架构以提高性能。

Abstract: In recent years, transformer-based large language models (LLMs) have
revolutionised natural language processing (NLP), with generative models
opening new possibilities for tasks that require context-aware text generation.
Requirements engineering (RE) has also seen a surge in the experimentation of
LLMs for different tasks, including trace-link detection, regulatory
compliance, and others. Requirements classification is a common task in RE.
While non-generative LLMs like BERT have been successfully applied to this
task, there has been limited exploration of generative LLMs. This gap raises an
important question: how well can generative LLMs, which produce context-aware
outputs, perform in requirements classification? In this study, we explore the
effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing
both binary and multi-class requirements classification. We design an extensive
experimental study involving over 400 experiments across three widely used
datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes
that while factors like prompt design and LLM architecture are universally
important, others-such as dataset variations-have a more situational impact,
depending on the complexity of the classification task. This insight can guide
future model development and deployment strategies, focusing on optimising
prompt structures and aligning model architectures with task-specific needs for
improved performance.

</details>

### [104] [Evaluation Framework for AI Systems in "the Wild"](https://arxiv.org/abs/2504.16778)
*Sarah Jabbour,Trenton Chang,Anindya Das Antar,Joseph Peper,Insu Jang,Jiachen Liu,Jae-Won Chung,Shiqi He,Michael Wellman,Bryan Goodman,Elizabeth Bondi-Kelly,Kevin Samy,Rada Mihalcea,Mosharaf Chowhury,David Jurgens,Lu Wang*

Main category: cs.CL

TLDR: 提出一个动态、全面的生成式AI评估框架，强调真实世界表现、公平性和伦理，而非传统静态指标。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI评估方法未能适应其广泛应用，传统方法无法反映真实性能，导致实验室结果与实际应用脱节。

Method: 提出一个动态、持续、多样化的评估框架，结合人类和自动化评估，注重透明度和伦理。

Result: 框架能更准确反映生成式AI的真实能力，同时兼顾公平性和社会影响。

Conclusion: 动态评估框架确保生成式AI不仅技术高效，还符合伦理和社会责任。

Abstract: Generative AI (GenAI) models have become vital across industries, yet current
evaluation methods have not adapted to their widespread use. Traditional
evaluations often rely on benchmarks and fixed datasets, frequently failing to
reflect real-world performance, which creates a gap between lab-tested outcomes
and practical applications. This white paper proposes a comprehensive framework
for how we should evaluate real-world GenAI systems, emphasizing diverse,
evolving inputs and holistic, dynamic, and ongoing assessment approaches. The
paper offers guidance for practitioners on how to design evaluation methods
that accurately reflect real-time capabilities, and provides policymakers with
recommendations for crafting GenAI policies focused on societal impacts, rather
than fixed performance numbers or parameter sizes. We advocate for holistic
frameworks that integrate performance, fairness, and ethics and the use of
continuous, outcome-oriented methods that combine human and automated
assessments while also being transparent to foster trust among stakeholders.
Implementing these strategies ensures GenAI models are not only technically
proficient but also ethically responsible and impactful.

</details>

### [105] [MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores](https://arxiv.org/abs/2504.16786)
*Fengwei Zhou,Jiafei Song,Wenjin Jason Li,Gengjian Xue,Zhikang Zhao,Yichao Lu,Bailin Na*

Main category: cs.CL

TLDR: MOOSComp是一种基于令牌分类的长上下文压缩方法，通过解决BERT压缩器的过平滑问题并引入异常值分数，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理长上下文输入时面临推理时间和资源消耗增加的挑战，特别是在资源受限的环境中。

Method: 提出MOOSComp方法，通过添加类间余弦相似性损失项和异常值分数，优化令牌分类和压缩过程。

Result: 在各种压缩比下，MOOSComp在长上下文理解和推理任务中表现优异，并在资源受限设备上实现了3.3倍的加速。

Conclusion: MOOSComp通过改进令牌分类和压缩策略，显著提升了长上下文处理的效率和性能。

Abstract: Recent advances in large language models have significantly improved their
ability to process long-context input, but practical applications are
challenged by increased inference time and resource consumption, particularly
in resource-constrained environments. To address these challenges, we propose
MOOSComp, a token-classification-based long-context compression method that
enhances the performance of a BERT-based compressor by mitigating the
over-smoothing problem and incorporating outlier scores. In the training phase,
we add an inter-class cosine similarity loss term to penalize excessively
similar token representations, thereby improving the token classification
accuracy. During the compression phase, we introduce outlier scores to preserve
rare but critical tokens that are prone to be discarded in task-agnostic
compression. These scores are integrated with the classifier's output, making
the compressor more generalizable to various tasks. Superior performance is
achieved at various compression ratios on long-context understanding and
reasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x
compression ratio on a resource-constrained mobile device.

</details>

### [106] [Credible plan-driven RAG method for Multi-hop Question Answering](https://arxiv.org/abs/2504.16787)
*Ningning Zhang,Chi Zhang,Zhizhong Tan,Xingxing Yang,Weiping Deng,Wenyong Wang*

Main category: cs.CL

TLDR: PAR RAG框架通过规划、执行和审查三阶段，减少多跳问答中的错误传播，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法在多跳问答中易因推理路径偏差或中间结果错误导致答案准确性下降。

Method: 提出PAR RAG框架，分三阶段：规划（全局分解问题）、执行（多粒度验证）、审查（调整中间结果）。

Result: 在多跳问答数据集上，PAR RAG在EM和F1分数上显著优于现有方法。

Conclusion: PAR RAG通过可解释的增量推理范式，有效提升多跳问答的准确性和可靠性。

Abstract: Multi-hop question answering (QA) presents a considerable challenge for
Retrieval-Augmented Generation (RAG), requiring the structured decomposition of
complex queries into logical reasoning paths and the generation of dependable
intermediate results. However, deviations in reasoning paths or errors in
intermediate results, which are common in current RAG methods, may propagate
and accumulate throughout the reasoning process, diminishing the accuracy of
the answer to complex queries. To address this challenge, we propose the
Plan-then-Act-and-Review (PAR RAG) framework, which is organized into three key
stages: planning, act, and review, and aims to offer an interpretable and
incremental reasoning paradigm for accurate and reliable multi-hop question
answering by mitigating error propagation.PAR RAG initially applies a top-down
problem decomposition strategy, formulating a comprehensive plan that
integrates multiple executable steps from a holistic viewpoint. This approach
avoids the pitfalls of local optima common in traditional RAG methods, ensuring
the accuracy of the entire reasoning path. Subsequently, PAR RAG incorporates a
plan execution mechanism based on multi-granularity verification. By utilizing
both coarse-grained similarity information and fine-grained relevant data, the
framework thoroughly checks and adjusts intermediate results, ensuring process
accuracy while effectively managing error propagation and amplification.
Experimental results on multi-hop QA datasets demonstrate that the PAR RAG
framework substantially outperforms existing state-of-the-art methods in key
metrics, including EM and F1 scores.

</details>

### [107] [Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention](https://arxiv.org/abs/2504.16795)
*Xiang Hu,Jiaqi Leng,Jun Zhao,Kewei Tu,Wei Wu*

Main category: cs.CL

TLDR: HSA是一种新型注意力机制，通过分块和分层聚合信息，增强RNN的长距离随机访问能力，同时保持其效率优势。RAMba结合HSA和Mamba，在长上下文建模中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决RNN无法随机访问历史上下文的问题，同时避免引入注意力机制导致的效率损失。

Method: 提出HSA机制，将输入分块并选择top-k块，基于细粒度令牌信息学习块间相关性。结合硬件对齐内核设计提高效率。

Result: RAMba在64M上下文中实现完美准确率，并在下游任务中显著提升性能，内存占用几乎恒定。

Conclusion: RAMba展示了在长上下文建模中的巨大潜力，兼具高效性和灵活性。

Abstract: A key advantage of Recurrent Neural Networks (RNNs) over Transformers is
their linear computational and space complexity enables faster training and
inference for long sequences. However, RNNs are fundamentally unable to
randomly access historical context, and simply integrating attention mechanisms
may undermine their efficiency advantages. To overcome this limitation, we
propose \textbf{H}ierarchical \textbf{S}parse \textbf{A}ttention (HSA), a novel
attention mechanism that enhances RNNs with long-range random access
flexibility while preserving their merits in efficiency and length
generalization. HSA divides inputs into chunks, selecting the top-$k$ chunks
and hierarchically aggregates information. The core innovation lies in learning
token-to-chunk relevance based on fine-grained token-level information inside
each chunk. This approach enhances the precision of chunk selection across both
in-domain and out-of-domain context lengths. To make HSA efficient, we further
introduce a hardware-aligned kernel design. By combining HSA with Mamba, we
introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64
million contexts despite pre-training on only 4K-length contexts, and
significant improvements on various downstream tasks, with nearly constant
memory footprint. These results show RAMba's huge potential in long-context
modeling.

</details>

### [108] [LLM-assisted Graph-RAG Information Extraction from IFC Data](https://arxiv.org/abs/2504.16813)
*Sima Iranmanesh,Hadeel Saadany,Edlira Vakaj*

Main category: cs.CL

TLDR: 利用Graph-RAG技术结合LLM解析复杂IFC数据，提升自然语言查询能力。


<details>
  <summary>Details</summary>
Motivation: IFC数据复杂且多义性高，需高效解析方法。

Method: 采用Graph-RAG技术结合LLM（如GPT-4o）解析IFC数据。

Result: Graph-RAG增强了LLM的图知识能力，支持自然语言查询。

Conclusion: Graph-RAG有效简化了IFC数据解析流程，无需复杂管道。

Abstract: IFC data has become the general building information standard for
collaborative work in the construction industry. However, IFC data can be very
complicated because it allows for multiple ways to represent the same product
information. In this research, we utilise the capabilities of LLMs to parse the
IFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to
retrieve building object properties and their relations. We will show that,
despite limitations due to the complex hierarchy of the IFC data, the Graph-RAG
parsing enhances generative LLMs like GPT-4o with graph-based knowledge,
enabling natural language query-response retrieval without the need for a
complex pipeline.

</details>

### [109] [GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning](https://arxiv.org/abs/2504.16832)
*Luu Quy Tung,Hoang Quoc Viet,Vo Trong Thu*

Main category: cs.CL

TLDR: GreenMind-Medium-14B-R1是一个越南语推理模型，基于Group Relative Policy Optimization微调策略，解决了语言混合和事实正确性问题，在越南语数据集和SeaExam上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决Chain-of-Thought方法中的语言混合和事实正确性问题，提升越南语推理任务的性能。

Method: 使用Group Relative Policy Optimization微调策略，设计两个奖励函数：检测语言混合和确保事实正确性。

Result: 在VLSP 2023越南语数据集和SeaExam上表现优于现有方法，提升了语言一致性。

Conclusion: GreenMind-Medium-14B-R1在越南语推理任务中表现出色，验证了方法的有效性。

Abstract: Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that
require intermediate reasoning steps prior to generating a final answer. In
this paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model
inspired by the finetuning strategy based on Group Relative Policy
Optimization. We also leverage a high-quality Vietnamese synthesized reasoning
dataset and design two reward functions to tackle the main limitations of this
technique: (i) language mixing, where we explicitly detect the presence of
biased language characters during the process of sampling tokens, and (ii) we
leverage Sentence Transformer-based models to ensure that the generated
reasoning content maintains factual correctness and does not distort the final
output. Experimental results on the Vietnamese dataset from the VLSP 2023
Challenge demonstrate that our model outperforms prior works and enhances
linguistic consistency in its responses. Furthermore, we extend our evaluation
to SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of
our reasoning method compared to few-shot prompting techniques.

</details>

### [110] [Monte Carlo Planning with Large Language Model for Text-Based Game Agents](https://arxiv.org/abs/2504.16855)
*Zijing Shi,Meng Fang,Ling Chen*

Main category: cs.CL

TLDR: MC-DML算法结合大型语言模型（LLM）和树搜索算法，通过动态记忆机制提升文本游戏中的语言理解和推理能力，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于MCTS和RL的方法在文本游戏中耗时且缺乏语言理解能力，需要更高效的解决方案。

Method: 提出MC-DML算法，利用LLM的语言能力与树搜索的探索优势，并引入动态记忆机制。

Result: 在Jericho基准测试中，MC-DML在初始规划阶段显著优于现有方法。

Conclusion: MC-DML为复杂环境中的语言规划提供了高效解决方案。

Abstract: Text-based games provide valuable environments for language-based autonomous
agents. However, planning-then-learning paradigms, such as those combining
Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably
time-consuming due to extensive iterations. Additionally, these algorithms
perform uncertainty-driven exploration but lack language understanding and
reasoning abilities. In this paper, we introduce the Monte Carlo planning with
Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages
the language understanding and reasoning capabilities of Large Language Models
(LLMs) alongside the exploratory advantages of tree search algorithms.
Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms,
enabling them to learn from past experiences and dynamically adjust action
evaluations during planning. We conduct experiments on a series of text-based
games from the Jericho benchmark. Our results demonstrate that the MC-DML
algorithm significantly enhances performance across various games at the
initial planning phase, outperforming strong contemporary methods that require
multiple iterations. This demonstrates the effectiveness of our algorithm,
paving the way for more efficient language-grounded planning in complex
environments.

</details>

### [111] [Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification](https://arxiv.org/abs/2504.16856)
*Alexander Shvets*

Main category: cs.CL

TLDR: 论文提出了一种基于LLM的数据合成方法，利用Mistral-7b生成多样化的情感分析训练数据，并通过微调BERT类模型达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析数据集缺乏上下文且情感类别有限，大型语言模型（如GPT-4）存在过度预测和资源消耗大的问题。

Method: 设计LLM数据合成流程，利用Mistral-7b生成多样化的上下文和非上下文情感数据，微调预训练编码器模型。

Result: 生成10万条上下文和30万条非上下文数据，微调后的Emo Pillars模型在多个任务中达到SOTA性能。

Conclusion: 方法成功提升了数据多样性和上下文个性化，但需改进对非标准标签的处理。

Abstract: Most datasets for sentiment analysis lack context in which an opinion was
expressed, often crucial for emotion understanding, and are mainly limited by a
few emotion categories. Foundation large language models (LLMs) like GPT-4
suffer from over-predicting emotions and are too resource-intensive. We design
an LLM-based data synthesis pipeline and leverage a large model, Mistral-7b,
for the generation of training examples for more accessible, lightweight
BERT-type encoder models. We focus on enlarging the semantic diversity of
examples and propose grounding the generation into a corpus of narratives to
produce non-repetitive story-character-centered utterances with unique contexts
over 28 emotion classes. By running 700K inferences in 450 GPU hours, we
contribute with the dataset of 100K contextual and also 300K context-less
examples to cover both scenarios. We use it for fine-tuning pre-trained
encoders, which results in several Emo Pillars models. We show that Emo Pillars
models are highly adaptive to new domains when tuned to specific tasks such as
GoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on
the first three. We also validate our dataset, conducting statistical analysis
and human evaluation, and confirm the success of our measures in utterance
diversification (although less for the neutral class) and context
personalization, while pointing out the need for improved handling of
out-of-taxonomy labels within the pipeline.

</details>

### [112] [Planning with Diffusion Models for Target-Oriented Dialogue Systems](https://arxiv.org/abs/2504.16858)
*Hanwen Du,Bo Peng,Xia Ning*

Main category: cs.CL

TLDR: DiffTOD提出了一种基于扩散模型的非顺序对话规划框架，解决了传统顺序规划中的错误累积和短视行为问题。


<details>
  <summary>Details</summary>
Motivation: 现有对话规划方法采用顺序生成方式，容易导致错误累积和短视行为，限制了对话系统的灵活性和长期规划能力。

Method: DiffTOD将对话规划建模为条件引导的轨迹生成问题，利用扩散语言模型估计对话轨迹的似然，并针对不同目标类型设计了三种引导机制。

Result: 在三种不同目标导向对话设置中的实验表明，DiffTOD能够有效进行非短视的前瞻探索，并通过非顺序规划优化长期动作策略。

Conclusion: DiffTOD展示了在复杂多样对话场景中的强大灵活性，为非顺序对话规划提供了新思路。

Abstract: Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM
era, where strategic dialogue planning is crucial for directing conversations
toward specific targets. However, existing dialogue planning methods generate
dialogue plans in a step-by-step sequential manner, and may suffer from
compounding errors and myopic actions. To address these limitations, we
introduce a novel dialogue planning framework, DiffTOD, which leverages
diffusion models to enable non-sequential dialogue planning. DiffTOD formulates
dialogue planning as a trajectory generation problem with conditional guidance,
and leverages a diffusion language model to estimate the likelihood of the
dialogue trajectory. To optimize the dialogue action strategies, DiffTOD
introduces three tailored guidance mechanisms for different target types,
offering flexible guidance towards diverse TOD targets at test time. Extensive
experiments across three diverse TOD settings show that DiffTOD can effectively
perform non-myopic lookahead exploration and optimize action strategies over a
long horizon through non-sequential dialogue planning, and demonstrates strong
flexibility across complex and diverse dialogue scenarios. Our code and data
are accessible through https://anonymous.4open.science/r/DiffTOD.

</details>

### [113] [Do Large Language Models know who did what to whom?](https://arxiv.org/abs/2504.16884)
*Joseph M. Denning,Xiaohan,Guo,Bryor Snefjella,Idan A. Blank*

Main category: cs.CL

TLDR: LLMs的句子表征主要反映句法相似性而非主题角色信息，但部分注意力头能独立捕获主题角色。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否通过词预测目标学习到与语言紧密相关的主题角色理解能力。

Method: 通过两个实验分析四种LLMs的句子表征，比较其与人类相似性判断的差异。

Result: LLMs的句子表征以句法相似性为主，主题角色信息较弱，但部分注意力头能独立捕获主题角色。

Conclusion: LLMs能提取主题角色，但其表征中该信息的影响较人类更弱。

Abstract: Large Language Models (LLMs) are commonly criticized for not understanding
language. However, many critiques focus on cognitive abilities that, in humans,
are distinct from language processing. Here, we instead study a kind of
understanding tightly linked to language: inferring who did what to whom
(thematic roles) in a sentence. Does the central training objective of
LLMs-word prediction-result in sentence representations that capture thematic
roles? In two experiments, we characterized sentence representations in four
LLMs. In contrast to human similarity judgments, in LLMs the overall
representational similarity of sentence pairs reflected syntactic similarity
but not whether their agent and patient assignments were identical vs.
reversed. Furthermore, we found little evidence that thematic role information
was available in any subset of hidden units. However, some attention heads
robustly captured thematic roles, independently of syntax. Therefore, LLMs can
extract thematic roles but, relative to humans, this information influences
their representations more weakly.

</details>

### [114] [Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text](https://arxiv.org/abs/2504.16913)
*Shifali Agrahari,Sanasam Ranbir Singh*

Main category: cs.CL

TLDR: 本文提出了一种名为COT Fine-tuned的新框架，用于检测AI生成文本并识别生成文本的特定语言模型。通过双任务设计和Chain-of-Thought推理，该方法在准确性和可解释性上表现优异。


<details>
  <summary>Details</summary>
Motivation: 近年来，AI生成文本的检测成为研究热点，涉及学术诚信、错误信息和AI伦理部署等问题。

Method: 采用双任务设计：任务A区分AI生成与人类撰写文本，任务B识别生成文本的特定语言模型。关键创新是使用Chain-of-Thought推理，增强模型预测的透明度和可解释性。

Result: 实验表明，COT Fine-tuned在两项任务中均取得高准确率，尤其在语言模型识别和人类-AI分类中表现突出。Chain-of-Thought推理显著提升了模型效果和可解释性。

Conclusion: COT Fine-tuned框架在检测AI生成文本和识别语言模型方面表现出色，其Chain-of-Thought推理设计提升了模型的透明度和实用性。

Abstract: In recent years, the detection of AI-generated text has become a critical
area of research due to concerns about academic integrity, misinformation, and
ethical AI deployment. This paper presents COT Fine-tuned, a novel framework
for detecting AI-generated text and identifying the specific language model.
responsible for generating the text. We propose a dual-task approach, where
Task A involves classifying text as AI-generated or human-written, and Task B
identifies the specific LLM behind the text. The key innovation of our method
lies in the use of Chain-of-Thought reasoning, which enables the model to
generate explanations for its predictions, enhancing transparency and
interpretability. Our experiments demonstrate that COT Fine-tuned achieves high
accuracy in both tasks, with strong performance in LLM identification and
human-AI classification. We also show that the CoT reasoning process
contributes significantly to the models effectiveness and interpretability.

</details>

### [115] [OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents](https://arxiv.org/abs/2504.16918)
*Raghav Thind,Youran Sun,Ling Liang,Haizhao Yang*

Main category: cs.CL

TLDR: OptimAI是一个利用LLM驱动的AI代理解决自然语言描述的优化问题的框架，通过四个关键角色（formulator、planner、coder、code critic）实现高效协作，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 优化问题在科研和实际应用中至关重要，但将自然语言描述的优化问题转化为数学形式并选择合适的求解器需要大量领域知识。

Method: 框架包含四个角色：formulator（翻译问题为数学形式）、planner（制定解决方案策略）、coder和code critic（与环境交互并优化行动）。采用UCB-based debug调度动态切换计划。

Result: 在NLP4LP数据集上达到88.1%准确率，Optibench子集上71.2%，错误率分别降低58%和50%。

Conclusion: OptimAI通过多代理协作和动态调度显著提升了优化问题的解决效率和准确性。

Abstract: Optimization plays a vital role in scientific research and practical
applications, but formulating a concrete optimization problem described in
natural language into a mathematical form and selecting a suitable solver to
solve the problem requires substantial domain expertise. We introduce
\textbf{OptimAI}, a framework for solving \underline{Optim}ization problems
described in natural language by leveraging LLM-powered \underline{AI} agents,
achieving superior performance over current state-of-the-art methods. Our
framework is built upon four key roles: (1) a \emph{formulator} that translates
natural language problem descriptions into precise mathematical formulations;
(2) a \emph{planner} that constructs a high-level solution strategy prior to
execution; and (3) a \emph{coder} and a \emph{code critic} capable of
interacting with the environment and reflecting on outcomes to refine future
actions. Ablation studies confirm that all roles are essential; removing the
planner or code critic results in $5.8\times$ and $3.1\times$ drops in
productivity, respectively. Furthermore, we introduce UCB-based debug
scheduling to dynamically switch between alternative plans, yielding an
additional $3.3\times$ productivity gain. Our design emphasizes multi-agent
collaboration, allowing us to conveniently explore the synergistic effect of
combining diverse models within a unified system. Our approach attains 88.1\%
accuracy on the NLP4LP dataset and 71.2\% on the Optibench (non-linear w/o
table) subset, reducing error rates by 58\% and 50\% respectively over prior
best results.

</details>

### [116] [IberBench: LLM Evaluation on Iberian Languages](https://arxiv.org/abs/2504.16921)
*José Ángel González,Ian Borrego Obrador,Álvaro Romo Herrero,Areg Mikael Sarvazyan,Mara Chinea-Ríos,Angelo Basile,Marc Franco-Salvador*

Main category: cs.CL

TLDR: IberBench是一个针对伊比利亚半岛和伊比利亚美洲语言的综合基准测试，旨在评估LLM在基础和工业相关NLP任务中的表现，解决了现有基准的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要针对英语，缺乏语言多样性和工业相关任务，且静态设计。IberBench填补了这一空白。

Method: 整合101个数据集，覆盖22个任务类别，支持持续更新和社区驱动提交，评估23个不同规模的LLM。

Result: LLM在工业任务中表现较差，对某些语言（如加利西亚语和巴斯克语）表现更低，部分任务接近随机水平。

Conclusion: IberBench提供了开源评估工具和公开排行榜，为多语言LLM评估提供了新标准。

Abstract: Large Language Models (LLMs) remain difficult to evaluate comprehensively,
particularly for languages other than English, where high-quality data is often
limited. Existing benchmarks and leaderboards are predominantly
English-centric, with only a few addressing other languages. These benchmarks
fall short in several key areas: they overlook the diversity of language
varieties, prioritize fundamental Natural Language Processing (NLP)
capabilities over tasks of industrial relevance, and are static. With these
aspects in mind, we present IberBench, a comprehensive and extensible benchmark
designed to assess LLM performance on both fundamental and industry-relevant
NLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.
IberBench integrates 101 datasets from evaluation campaigns and recent
benchmarks, covering 22 task categories such as sentiment and emotion analysis,
toxicity detection, and summarization. The benchmark addresses key limitations
in current evaluation practices, such as the lack of linguistic diversity and
static evaluation setups by enabling continual updates and community-driven
model and dataset submissions moderated by a committee of experts. We evaluate
23 LLMs ranging from 100 million to 14 billion parameters and provide empirical
insights into their strengths and limitations. Our findings indicate that (i)
LLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)
performance is on average lower for Galician and Basque, (iii) some tasks show
results close to random, and (iv) in other tasks LLMs perform above random but
below shared task systems. IberBench offers open-source implementations for the
entire evaluation pipeline, including dataset normalization and hosting,
incremental evaluation of LLMs, and a publicly accessible leaderboard.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [117] [A Framework for Objective-Driven Dynamical Stochastic Fields](https://arxiv.org/abs/2504.16115)
*Yibo Jacky Zhang,Sanmi Koyejo*

Main category: cs.AI

TLDR: 论文提出了一种理论框架，基于三个基本原则（完全配置、局部性和目的性）来理解智能场，并探讨了从人工智能应用角度设计此类场的方法。


<details>
  <summary>Details</summary>
Motivation: 由于复杂动态随机系统的内在复杂性，缺乏对其形式化理论描述和实际应用的解决方案。

Method: 提出三个基本原则（完全配置、局部性和目的性）作为理论框架，并从人工智能应用角度探索设计方法。

Result: 建立了智能场的理论基础，为未来理论和实践发展奠定了基础。

Conclusion: 论文为理解和利用目标驱动的动态随机场的潜力提供了初步探索和理论支持。

Abstract: Fields offer a versatile approach for describing complex systems composed of
interacting and dynamic components. In particular, some of these dynamical and
stochastic systems may exhibit goal-directed behaviors aimed at achieving
specific objectives, which we refer to as $\textit{intelligent fields}$.
However, due to their inherent complexity, it remains challenging to develop a
formal theoretical description of such systems and to effectively translate
these descriptions into practical applications. In this paper, we propose three
fundamental principles -- complete configuration, locality, and purposefulness
-- to establish a theoretical framework for understanding intelligent fields.
Moreover, we explore methodologies for designing such fields from the
perspective of artificial intelligence applications. This initial investigation
aims to lay the groundwork for future theoretical developments and practical
advances in understanding and harnessing the potential of such objective-driven
dynamical stochastic fields.

</details>

### [118] [HTN Plan Repair Algorithms Compared: Strengths and Weaknesses of Different Methods](https://arxiv.org/abs/2504.16209)
*Paul Zaidins,Robert P. Goldman,Ugur Kuter,Dana Nau,Mark Roberts*

Main category: cs.AI

TLDR: 本文对三种分层计划修复算法（SHOPFixer、IPyHOPPER和Rewrite）进行了理论和实证比较，分析了它们在问题定义、搜索空间和修复能力上的差异，并基于基准问题评估了它们的性能。


<details>
  <summary>Details</summary>
Motivation: 理解不同计划修复算法的定义和性能差异，以便在实际应用中选择合适的修复方法。

Method: 通过理论分析和实证评估（基准规划问题）比较三种算法的搜索空间、修复能力和运行时性能。

Result: 理论分析揭示了三种算法在问题定义和修复能力上的差异；实证结果表明它们在运行时性能和修复覆盖率上的不同表现。

Conclusion: 选择合适的计划修复算法需考虑其定义和性能特点，本文为实际应用提供了理论依据和实证支持。

Abstract: This paper provides theoretical and empirical comparisons of three recent
hierarchical plan repair algorithms: SHOPFixer, IPyHOPPER, and Rewrite. Our
theoretical results show that the three algorithms correspond to three
different definitions of the plan repair problem, leading to differences in the
algorithms' search spaces, the repair problems they can solve, and the kinds of
repairs they can make. Understanding these distinctions is important when
choosing a repair method for any given application.
  Building on the theoretical results, we evaluate the algorithms empirically
in a series of benchmark planning problems. Our empirical results provide more
detailed insight into the runtime repair performance of these systems and the
coverage of the repair problems solved, based on algorithmic properties such as
replanning, chronological backtracking, and backjumping over plan trees.

</details>

### [119] [Investigating LLMs in Clinical Triage: Promising Capabilities, Persistent Intersectional Biases](https://arxiv.org/abs/2504.16273)
*Joseph Lee,Tianqi Shang,Jae Young Baik,Duy Duong-Tran,Shu Yang,Lingyao Li,Li Shen*

Main category: cs.AI

TLDR: LLMs在急诊分诊中表现出优越的鲁棒性，但在特定性别和种族组合中存在偏好差距。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在急诊分诊中的应用潜力，尤其是其对数据分布变化和缺失数据的鲁棒性，以及跨性别和种族的偏见问题。

Method: 通过持续预训练和上下文学习等多种LLM方法，结合机器学习方法，评估其在急诊分诊中的表现。

Result: LLMs表现出较高的鲁棒性，但在某些性别和种族组合中存在明显的偏好差异。

Conclusion: LLMs在临床决策中具有潜力，但需注意其编码的潜在人口统计学偏好。

Abstract: Large Language Models (LLMs) have shown promise in clinical decision support,
yet their application to triage remains underexplored. We systematically
investigate the capabilities of LLMs in emergency department triage through two
key dimensions: (1) robustness to distribution shifts and missing data, and (2)
counterfactual analysis of intersectional biases across sex and race. We assess
multiple LLM-based approaches, ranging from continued pre-training to
in-context learning, as well as machine learning approaches. Our results
indicate that LLMs exhibit superior robustness, and we investigate the key
factors contributing to the promising LLM-based approaches. Furthermore, in
this setting, we identify gaps in LLM preferences that emerge in particular
intersections of sex and race. LLMs generally exhibit sex-based differences,
but they are most pronounced in certain racial groups. These findings suggest
that LLMs encode demographic preferences that may emerge in specific clinical
contexts or particular combinations of characteristics.

</details>

### [120] [Cognitive Silicon: An Architectural Blueprint for Post-Industrial Computing Systems](https://arxiv.org/abs/2504.16622)
*Christoforus Yoga Haryanto,Emily Lomempow*

Main category: cs.AI

TLDR: 本文提出了一种名为“Cognitive Silicon”的架构框架，旨在解决自主AI系统在确定性、人类设计的计算架构中的局限性，探索2035年认知计算系统设计的可能路径。


<details>
  <summary>Details</summary>
Motivation: 自主AI系统在现有架构中存在基础性限制，需要一种新的框架来整合符号、内存、道德一致性和对齐执行，以应对未来的认知计算需求。

Method: 通过符号脚手架、受控内存、运行时道德一致性和对齐感知执行的多层集成，结合LLM的辩证共同设计，设计了一种新的架构框架。

Result: 该框架将死亡视为物理约束的自然结果，并提出了不可复制的隐性知识和身份密钥作为认知体现的基础。核心张力（如信任/代理、脚手架/涌现、执行/治理）成为架构设计的中心压力。

Conclusion: 该框架与自由能原理理论一致，可能为认知系统如何通过预测误差最小化保持身份提供正式解释，目标是构建一种道德可控、与人类对齐的认知基础设施。

Abstract: Autonomous AI systems reveal foundational limitations in deterministic,
human-authored computing architectures. This paper presents Cognitive Silicon:
a hypothetical full-stack architectural framework projected toward 2035,
exploring a possible trajectory for cognitive computing system design. The
proposed architecture would integrate symbolic scaffolding, governed memory,
runtime moral coherence, and alignment-aware execution across
silicon-to-semantics layers. Our design grammar has emerged from dialectical
co-design with LLMs under asymmetric epistemic conditions--creating structured
friction to expose blind spots and trade-offs. The envisioned framework would
establish mortality as a natural consequence of physical constraints,
non-copyable tacit knowledge, and non-cloneable identity keys as
cognitive-embodiment primitives. Core tensions (trust/agency,
scaffolding/emergence, execution/governance) would function as central
architectural pressures rather than edge cases. The architecture theoretically
converges with the Free Energy Principle, potentially offering a formal account
of how cognitive systems could maintain identity through prediction error
minimization across physical and computational boundaries. The resulting
framework aims to deliver a morally tractable cognitive infrastructure that
could maintain human-alignment through irreversible hardware constraints and
identity-bound epistemic mechanisms resistant to replication or subversion.

</details>

### [121] [Bridging Econometrics and AI: VaR Estimation via Reinforcement Learning and GARCH Models](https://arxiv.org/abs/2504.16635)
*Fredy Pokou,Jules Sadefo Kamdem,François Benhmad*

Main category: cs.AI

TLDR: 提出了一种结合GARCH模型与深度强化学习的混合框架，用于动态调整风险水平，显著提高了VaR估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统计量经济学模型（如GARCH）在复杂市场动态中适应性不足，需更灵活的风险估计方法。

Method: 结合GARCH波动率模型与Double Deep Q-Network (DDQN)，将任务视为不平衡分类问题，动态调整风险预测。

Result: 在Eurostoxx 50数据上验证，显著提升VaR估计准确性，减少违约次数和资本需求。

Conclusion: 该模型实时调整风险水平的能力使其成为现代主动风险管理的有效工具。

Abstract: In an environment of increasingly volatile financial markets, the accurate
estimation of risk remains a major challenge. Traditional econometric models,
such as GARCH and its variants, are based on assumptions that are often too
rigid to adapt to the complexity of the current market dynamics. To overcome
these limitations, we propose a hybrid framework for Value-at-Risk (VaR)
estimation, combining GARCH volatility models with deep reinforcement learning.
Our approach incorporates directional market forecasting using the Double Deep
Q-Network (DDQN) model, treating the task as an imbalanced classification
problem. This architecture enables the dynamic adjustment of risk-level
forecasts according to market conditions. Empirical validation on daily
Eurostoxx 50 data covering periods of crisis and high volatility shows a
significant improvement in the accuracy of VaR estimates, as well as a
reduction in the number of breaches and also in capital requirements, while
respecting regulatory risk thresholds. The ability of the model to adjust risk
levels in real time reinforces its relevance to modern and proactive risk
management.

</details>

### [122] [IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery](https://arxiv.org/abs/2504.16728)
*Aniketh Garikaparthi,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.AI

TLDR: IRIS是一个开源平台，通过结合人类反馈和LLM技术，提升科学假设生成的透明度和可控性。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动假设生成方法缺乏透明度和人类参与的问题。

Method: 开发IRIS平台，结合蒙特卡洛树搜索（MCTS）、细粒度反馈机制和文献合成功能。

Result: 用户研究表明IRIS能有效提升科学假设生成的质量和效率。

Conclusion: IRIS为研究人员提供了一个透明、可控的LLM辅助科学创意生成工具。

Abstract: The rapid advancement in capabilities of large language models (LLMs) raises
a pivotal question: How can LLMs accelerate scientific discovery? This work
tackles the crucial first stage of research, generating novel hypotheses. While
recent work on automated hypothesis generation focuses on multi-agent
frameworks and extending test-time compute, none of the approaches effectively
incorporate transparency and steerability through a synergistic
Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:
Interactive Research Ideation System, an open-source platform designed for
researchers to leverage LLM-assisted scientific ideation. IRIS incorporates
innovative features to enhance ideation, including adaptive test-time compute
expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,
and query-based literature synthesis. Designed to empower researchers with
greater control and insight throughout the ideation process. We additionally
conduct a user study with researchers across diverse disciplines, validating
the effectiveness of our system in enhancing ideation. We open-source our code
at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System

</details>

### [123] [A Survey of AI Agent Protocols](https://arxiv.org/abs/2504.16736)
*Yingxuan Yang,Huacan Chai,Yuanyi Song,Siyuan Qi,Muning Wen,Ning Li,Junwei Liao,Haoyi Hu,Jianghao Lin,Gaowei Chang,Weiwen Liu,Ying Wen,Yong Yu,Weinan Zhang*

Main category: cs.AI

TLDR: 论文探讨了大型语言模型（LLM）代理间缺乏统一通信协议的问题，提出了分类和性能分析，并展望了未来挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理的广泛应用，缺乏标准化通信协议限制了其协作和扩展能力，亟需解决方案。

Method: 系统综述现有通信协议，分为四类，并进行性能分析（安全性、可扩展性、延迟等）。

Result: 提供了协议分类和性能比较，帮助用户选择合适协议，并指出未来挑战。

Conclusion: 论文为设计和集成LLM代理通信基础设施提供了实用参考，推动未来生态系统发展。

Abstract: The rapid development of large language models (LLMs) has led to the
widespread deployment of LLM agents across diverse industries, including
customer service, content generation, data analysis, and even healthcare.
However, as more LLM agents are deployed, a major issue has emerged: there is
no standard way for these agents to communicate with external tools or data
sources. This lack of standardized protocols makes it difficult for agents to
work together or scale effectively, and it limits their ability to tackle
complex, real-world tasks. A unified communication protocol for LLM agents
could change this. It would allow agents and tools to interact more smoothly,
encourage collaboration, and triggering the formation of collective
intelligence. In this paper, we provide a systematic overview of existing
communication protocols for LLM agents. We classify them into four main
categories and make an analysis to help users and developers select the most
suitable protocols for specific applications. Additionally, we conduct a
comparative performance analysis of these protocols across key dimensions such
as security, scalability, and latency. Finally, we explore future challenges,
such as how protocols can adapt and survive in fast-evolving environments, and
what qualities future protocols might need to support the next generation of
LLM agent ecosystems. We expect this work to serve as a practical reference for
both researchers and engineers seeking to design, evaluate, or integrate robust
communication infrastructures for intelligent agents.

</details>

### [124] [Lightweight Latent Verifiers for Efficient Meta-Generation Strategies](https://arxiv.org/abs/2504.16760)
*Bartosz Piotrowski,Witold Drzewakowski,Konrad Staniszewski,Piotr Miłoś*

Main category: cs.AI

TLDR: LiLaVe是一种轻量级验证方法，通过从基础LLM的隐藏状态中提取正确性信号，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统验证器通常为大型LLM，计算成本高，LiLaVe旨在提供更高效的替代方案。

Method: LiLaVe利用基础LLM的隐藏状态提取信号，结合元生成策略（如best-of-n）或设计新方法（如条件自校正）。

Result: LiLaVe显著提高了生成任务的准确性和效率，尤其适用于小型LLM。

Conclusion: LiLaVe展示了从LLM隐藏状态提取信息的潜力，为资源密集型推理任务提供了可扩展的解决方案。

Abstract: Verifiers are auxiliary models that assess the correctness of outputs
generated by base large language models (LLMs). They play a crucial role in
many strategies for solving reasoning-intensive problems with LLMs. Typically,
verifiers are LLMs themselves, often as large (or larger) than the base model
they support, making them computationally expensive. In this work, we introduce
a novel lightweight verification approach, LiLaVe, which reliably extracts
correctness signals from the hidden states of the base LLM. A key advantage of
LiLaVe is its ability to operate with only a small fraction of the
computational budget required by traditional LLM-based verifiers. To
demonstrate its practicality, we couple LiLaVe with popular meta-generation
strategies, like best-of-n or self-consistency. Moreover, we design novel
LiLaVe-based approaches, like conditional self-correction or conditional
majority voting, that significantly improve both accuracy and efficiency in
generation tasks with smaller LLMs. Our work demonstrates the fruitfulness of
extracting latent information from the hidden states of LLMs, and opens the
door to scalable and resource-efficient solutions for reasoning-intensive
applications.

</details>

### [125] [AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset](https://arxiv.org/abs/2504.16891)
*Ivan Moshkov,Darragh Hanley,Ivan Sorokin,Shubham Toshniwal,Christof Henkel,Benedikt Schifferer,Wei Du,Igor Gitman*

Main category: cs.AI

TLDR: 本文介绍了在AIMO-2竞赛中获胜的数学推理模型，其核心包括大规模数据集构建、工具集成推理方法和生成式解决方案选择（GenSelect）。


<details>
  <summary>Details</summary>
Motivation: 通过构建高质量数学问题数据集和开发新型推理方法，提升数学推理模型的性能。

Method: 1. 创建包含54万数学问题和320万解答的数据集；2. 开发工具集成推理方法，生成170万高质量解答；3. 提出GenSelect方法优化解决方案选择。

Result: 模型在数学推理基准测试中达到最先进水平。

Conclusion: 通过数据集、工具集成和GenSelect方法的结合，显著提升了数学推理能力，并开源了相关资源。

Abstract: This paper presents our winning submission to the AI Mathematical Olympiad -
Progress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art
mathematical reasoning models relies on three key pillars. First, we create a
large-scale dataset comprising 540K unique high-quality math problems,
including olympiad-level problems, and their 3.2M long-reasoning solutions.
Second, we develop a novel method to integrate code execution with long
reasoning models through iterative training, generation, and quality filtering,
resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we
create a pipeline to train models to select the most promising solution from
many candidates. We show that such generative solution selection (GenSelect)
can significantly improve upon majority voting baseline. Combining these ideas,
we train a series of models that achieve state-of-the-art results on
mathematical reasoning benchmarks. To facilitate further research, we release
our code, models, and the complete OpenMathReasoning dataset under a
commercially permissive license.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [126] [Audio and Multiscale Visual Cues Driven Cross-modal Transformer for Idling Vehicle Detection](https://arxiv.org/abs/2504.16102)
*Xiwen Li,Ross Whitaker,Tolga Tasdizen*

Main category: cs.CV

TLDR: AVIVDNetv2是一种基于Transformer的端到端检测网络，用于改进闲置车辆检测（IVD）任务，通过跨模态Transformer和多尺度视觉特征融合模块，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在音频和视觉模态对齐上表现不佳，导致车辆检测效果不理想，需要一种更有效的跨模态建模方法。

Method: 提出AVIVDNetv2，结合跨模态Transformer、多尺度视觉特征融合模块和解耦检测头，优化模态对齐和特征提取。

Result: 实验显示，AVIVDNetv2在mAP上比基线方法提升7.66和9.42，并在所有车辆类别中表现一致优异。

Conclusion: AVIVDNetv2在AVIVD数据集上实现了新的性能基准，为跨模态检测任务提供了有效解决方案。

Abstract: Idling vehicle detection (IVD) supports real-time systems that reduce
pollution and emissions by dynamically messaging drivers to curb excess idling
behavior. In computer vision, IVD has become an emerging task that leverages
video from surveillance cameras and audio from remote microphones to localize
and classify vehicles in each frame as moving, idling, or engine-off. As with
other cross-modal tasks, the key challenge lies in modeling the correspondence
between audio and visual modalities, which differ in representation but provide
complementary cues -- video offers spatial and motion context, while audio
conveys engine activity beyond the visual field. The previous end-to-end model,
which uses a basic attention mechanism, struggles to align these modalities
effectively, often missing vehicle detections. To address this issue, we
propose AVIVDNetv2, a transformer-based end-to-end detection network. It
incorporates a cross-modal transformer with global patch-level learning, a
multiscale visual feature fusion module, and decoupled detection heads.
Extensive experiments show that AVIVDNetv2 improves mAP by 7.66 over the
disjoint baseline and 9.42 over the E2E baseline, with consistent AP gains
across all vehicle categories. Furthermore, AVIVDNetv2 outperforms the
state-of-the-art method for sounding object localization, establishing a new
performance benchmark on the AVIVD dataset.

</details>

### [127] [Shape Your Ground: Refining Road Surfaces Beyond Planar Representations](https://arxiv.org/abs/2504.16103)
*Oussema Dhaouadi,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TLDR: FlexRoad是一个通过NURBS曲面拟合3D道路点的新框架，显著提升了道路表面重建的平滑性和准确性，并提供了GeoRoad数据集用于定量比较。


<details>
  <summary>Details</summary>
Motivation: 现有道路重建方法常产生伪影和不一致性，而下游任务通常以平面简化表示道路，牺牲了准确性。

Method: FlexRoad利用ECSRC算法进行异常校正，通过NURBS曲面拟合3D道路点，减少粗糙度和拟合误差。

Result: 在GeRoD和DSC3D数据集上的实验表明，FlexRoad在多种指标上优于常用方法，且对输入源、地形和噪声类型不敏感。

Conclusion: FlexRoad是一种通用的高质量道路表面建模方法，其各组件对性能提升起关键作用。

Abstract: Road surface reconstruction from aerial images is fundamental for autonomous
driving, urban planning, and virtual simulation, where smoothness, compactness,
and accuracy are critical quality factors. Existing reconstruction methods
often produce artifacts and inconsistencies that limit usability, while
downstream tasks have a tendency to represent roads as planes for simplicity
but at the cost of accuracy. We introduce FlexRoad, the first framework to
directly address road surface smoothing by fitting Non-Uniform Rational
B-Splines (NURBS) surfaces to 3D road points obtained from photogrammetric
reconstructions or geodata providers. Our method at its core utilizes the
Elevation-Constrained Spatial Road Clustering (ECSRC) algorithm for robust
anomaly correction, significantly reducing surface roughness and fitting
errors. To facilitate quantitative comparison between road surface
reconstruction methods, we present GeoRoad Dataset (GeRoD), a diverse
collection of road surface and terrain profiles derived from openly accessible
geodata. Experiments on GeRoD and the photogrammetry-based DeepScenario Open 3D
Dataset (DSC3D) demonstrate that FlexRoad considerably surpasses commonly used
road surface representations across various metrics while being insensitive to
various input sources, terrains, and noise types. By performing ablation
studies, we identify the key role of each component towards high-quality
reconstruction performance, making FlexRoad a generic method for realistic road
surface modeling.

</details>

### [128] [Persistence-based Hough Transform for Line Detection](https://arxiv.org/abs/2504.16114)
*Johannes Ferner,Stefan Huber,Saverio Messineo,Angel Pop,Martin Uray*

Main category: cs.CV

TLDR: 提出了一种基于持久同调的新投票技术，用于霍夫空间中的峰值检测，显著优于传统阈值方法，并增强了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统霍夫变换的投票过程通过阈值处理，易受噪声和伪影影响，需要更稳健的替代方法。

Method: 采用持久同调技术替代传统阈值方法，检测霍夫空间中的峰值。

Result: 在合成数据上的实验表明，新方法显著优于原始方法，且鲁棒性更强。

Conclusion: 倡导将拓扑数据分析技术更广泛地整合到现有方法中，并鼓励探索霍夫变换的数学稳定性改进。

Abstract: The Hough transform is a popular and classical technique in computer vision
for the detection of lines (or more general objects). It maps a pixel into a
dual space -- the Hough space: each pixel is mapped to the set of lines through
this pixel, which forms a curve in Hough space. The detection of lines then
becomes a voting process to find those lines that received many votes by
pixels. However, this voting is done by thresholding, which is susceptible to
noise and other artifacts.
  In this work, we present an alternative voting technique to detect peaks in
the Hough space based on persistent homology, which very naturally addresses
limitations of simple thresholding. Experiments on synthetic data show that our
method significantly outperforms the original method, while also demonstrating
enhanced robustness.
  This work seeks to inspire future research in two key directions. First, we
highlight the untapped potential of Topological Data Analysis techniques and
advocate for their broader integration into existing methods, including
well-established ones. Secondly, we initiate a discussion on the mathematical
stability of the Hough transform, encouraging exploration of mathematically
grounded improvements to enhance its robustness.

</details>

### [129] [Context-Awareness and Interpretability of Rare Occurrences for Discovery and Formalization of Critical Failure Modes](https://arxiv.org/abs/2504.16117)
*Sridevi Polavaram,Xin Zhou,Meenu Ravi,Mohammad Zarei,Anmol Srivastava*

Main category: cs.CV

TLDR: CAIRO框架通过本体论方法检测和形式化AI模型中的罕见故障案例，结合人机协作提升测试与评估的透明度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决视觉系统在关键领域（如自动驾驶）中因罕见或未知场景引发的安全风险。

Method: 提出CAIRO框架，基于本体论形式化故障案例（CP），支持人机协作测试与评估，生成可共享的知识图谱。

Result: 在自动驾驶系统中成功检测并形式化对象检测模型的故障案例，生成可扩展、可解释的知识图谱。

Conclusion: CAIRO为AI模型故障案例的发现与形式化提供了可扩展、透明的方法，有助于提升系统安全性和问责性。

Abstract: Vision systems are increasingly deployed in critical domains such as
surveillance, law enforcement, and transportation. However, their
vulnerabilities to rare or unforeseen scenarios pose significant safety risks.
To address these challenges, we introduce Context-Awareness and
Interpretability of Rare Occurrences (CAIRO), an ontology-based human-assistive
discovery framework for failure cases (or CP - Critical Phenomena) detection
and formalization. CAIRO by design incentivizes human-in-the-loop for testing
and evaluation of criticality that arises from misdetections, adversarial
attacks, and hallucinations in AI black-box models. Our robust analysis of
object detection model(s) failures in automated driving systems (ADS) showcases
scalable and interpretable ways of formalizing the observed gaps between camera
perception and real-world contexts, resulting in test cases stored as explicit
knowledge graphs (in OWL/XML format) amenable for sharing, downstream analysis,
logical reasoning, and accountability.

</details>

### [130] [MonoTher-Depth: Enhancing Thermal Depth Estimation via Confidence-Aware Distillation](https://arxiv.org/abs/2504.16127)
*Xingxing Zuo,Nikhil Ranganathan,Connor Lee,Georgia Gkioxari,Soon-Jo Chung*

Main category: cs.CV

TLDR: 提出了一种通过知识蒸馏从RGB模型增强热图像单目深度估计的方法，显著提升了精度。


<details>
  <summary>Details</summary>
Motivation: 热图像深度估计在恶劣条件下（如雾、烟、低光）对机器人系统至关重要，但标注数据稀缺限制了模型泛化能力。

Method: 采用置信度感知的知识蒸馏方法，利用RGB模型的预测置信度选择性增强热图像模型。

Result: 在无标注深度数据的新场景中，该方法将绝对相对误差降低了22.88%。

Conclusion: 该方法显著提升了热图像深度估计的精度，扩展了其应用场景。

Abstract: Monocular depth estimation (MDE) from thermal images is a crucial technology
for robotic systems operating in challenging conditions such as fog, smoke, and
low light. The limited availability of labeled thermal data constrains the
generalization capabilities of thermal MDE models compared to foundational RGB
MDE models, which benefit from datasets of millions of images across diverse
scenarios. To address this challenge, we introduce a novel pipeline that
enhances thermal MDE through knowledge distillation from a versatile RGB MDE
model. Our approach features a confidence-aware distillation method that
utilizes the predicted confidence of the RGB MDE to selectively strengthen the
thermal MDE model, capitalizing on the strengths of the RGB model while
mitigating its weaknesses. Our method significantly improves the accuracy of
the thermal MDE, independent of the availability of labeled depth supervision,
and greatly expands its applicability to new scenarios. In our experiments on
new scenarios without labeled depth, the proposed confidence-aware distillation
method reduces the absolute relative error of thermal MDE by 22.88\% compared
to the baseline without distillation.

</details>

### [131] [Hybrid Knowledge Transfer through Attention and Logit Distillation for On-Device Vision Systems in Agricultural IoT](https://arxiv.org/abs/2504.16128)
*Stanley Mugisha,Rashid Kisitu,Florence Tushabe*

Main category: cs.CV

TLDR: 提出了一种混合知识蒸馏框架，将Swin Transformer教师模型的知识迁移到MobileNetV3学生模型，以在资源受限的边缘设备上实现高效且高精度的植物病害分类。


<details>
  <summary>Details</summary>
Motivation: 解决农业物联网系统中深度学习应用的高精度与边缘设备资源限制之间的矛盾。

Method: 采用混合知识蒸馏框架，结合自适应注意力对齐和双损失函数优化。

Result: 在lantVillage-Tomato数据集上，蒸馏后的MobileNetV3达到92.4%的准确率，计算量减少95%，推理延迟降低82%。

Conclusion: 该工作推动了实时、高效的精准农业作物监测，并展示了如何在边缘设备上实现ViT级别的诊断精度。

Abstract: Integrating deep learning applications into agricultural IoT systems faces a
serious challenge of balancing the high accuracy of Vision Transformers (ViTs)
with the efficiency demands of resource-constrained edge devices. Large
transformer models like the Swin Transformers excel in plant disease
classification by capturing global-local dependencies. However, their
computational complexity (34.1 GFLOPs) limits applications and renders them
impractical for real-time on-device inference. Lightweight models such as
MobileNetV3 and TinyML would be suitable for on-device inference but lack the
required spatial reasoning for fine-grained disease detection. To bridge this
gap, we propose a hybrid knowledge distillation framework that synergistically
transfers logit and attention knowledge from a Swin Transformer teacher to a
MobileNetV3 student model. Our method includes the introduction of adaptive
attention alignment to resolve cross-architecture mismatch (resolution,
channels) and a dual-loss function optimizing both class probabilities and
spatial focus. On the lantVillage-Tomato dataset (18,160 images), the distilled
MobileNetV3 attains 92.4% accuracy relative to 95.9% for Swin-L but at an 95%
reduction on PC and < 82% in inference latency on IoT devices. (23ms on PC CPU
and 86ms/image on smartphone CPUs). Key innovations include IoT-centric
validation metrics (13 MB memory, 0.22 GFLOPs) and dynamic resolution-matching
attention maps. Comparative experiments show significant improvements over
standalone CNNs and prior distillation methods, with a 3.5% accuracy gain over
MobileNetV3 baselines. Significantly, this work advances real-time,
energy-efficient crop monitoring in precision agriculture and demonstrates how
we can attain ViT-level diagnostic precision on edge devices. Code and models
will be made available for replication after acceptance.

</details>

### [132] [Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends](https://arxiv.org/abs/2504.16134)
*Mohammad Abu Tami,Mohammed Elhenawy,Huthaifa I. Ashqar*

Main category: cs.CV

TLDR: 本文探讨了多模态大语言模型（MLLMs）在提升交通安全性中的潜力，通过整合多模态数据实现全面场景理解，弥补传统ADAS的不足。


<details>
  <summary>Details</summary>
Motivation: 传统ADAS在动态现实场景中表现不佳，亟需更先进的解决方案。

Method: 通过分析MLLM方法，整合视觉、空间和环境数据，提升感知、决策和对抗鲁棒性。

Result: MLLMs展现出革命性潜力，能提供可扩展、情境感知的解决方案，显著提升道路安全。

Conclusion: MLLMs有望成为下一代交通安全系统的核心，推动领域革新。

Abstract: Traffic safety remains a critical global challenge, with traditional Advanced
Driver-Assistance Systems (ADAS) often struggling in dynamic real-world
scenarios due to fragmented sensor processing and susceptibility to adversarial
conditions. This paper reviews the transformative potential of Multimodal Large
Language Models (MLLMs) in addressing these limitations by integrating
cross-modal data such as visual, spatial, and environmental inputs to enable
holistic scene understanding. Through a comprehensive analysis of MLLM-based
approaches, we highlight their capabilities in enhancing perception,
decision-making, and adversarial robustness, while also examining the role of
key datasets (e.g., KITTI, DRAMA, ML4RoadSafety) in advancing research.
Furthermore, we outline future directions, including real-time edge deployment,
causality-driven reasoning, and human-AI collaboration. By positioning MLLMs as
a cornerstone for next-generation traffic safety systems, this review
underscores their potential to revolutionize the field, offering scalable,
context-aware solutions that proactively mitigate risks and improve overall
road safety.

</details>

### [133] [Progressive Language-guided Visual Learning for Multi-Task Visual Grounding](https://arxiv.org/abs/2504.16145)
*Jingchao Wang,Hong Wang,Wenlong Zhang,Kunhua Ji,Dingjiang Huang,Yefeng Zheng*

Main category: cs.CV

TLDR: PLVL框架通过渐进式语言引导视觉学习，解决了多任务视觉定位中语言信息未充分利用和任务间协作不足的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在语言信息注入和任务协作方面存在不足，PLVL旨在通过语言引导的视觉学习和多任务头部设计解决这些问题。

Method: PLVL采用渐进式语言信息注入策略，无需额外跨模态融合模块，并设计多任务头部实现REC和RES的协作预测。

Result: 在多个基准数据集上，PLVL显著优于现有方法，在REC和RES任务中均取得更好表现。

Conclusion: PLVL通过语言引导和多任务协作，有效提升了多任务视觉定位的性能，为未来研究提供了新思路。

Abstract: Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring
Expression Comprehension (REC) and Referring Expression Segmentation (RES). The
existing representative approaches generally follow the research pipeline which
mainly consists of three core procedures, including independent feature
extraction for visual and linguistic modalities, respectively, cross-modal
interaction module, and independent prediction heads for different sub-tasks.
Albeit achieving remarkable performance, this research line has two
limitations: 1) The linguistic content has not been fully injected into the
entire visual backbone for boosting more effective visual feature extraction
and it needs an extra cross-modal interaction module; 2) The relationship
between REC and RES tasks is not effectively exploited to help the
collaborative prediction for more accurate output. To deal with these problems,
in this paper, we propose a Progressive Language-guided Visual Learning
framework for multi-task visual grounding, called PLVL, which not only finely
mine the inherent feature expression of the visual modality itself but also
progressively inject the language information to help learn linguistic-related
visual features. In this manner, our PLVL does not need additional cross-modal
fusion module while fully introducing the language guidance. Furthermore, we
analyze that the localization center for REC would help identify the
to-be-segmented object region for RES to some extent. Inspired by this
investigation, we design a multi-task head to accomplish collaborative
predictions for these two sub-tasks. Extensive experiments conducted on several
benchmark datasets comprehensively substantiate that our PLVL obviously
outperforms the representative methods in both REC and RES tasks.
https://github.com/jcwang0602/PLVL

</details>

### [134] [Classification of Firn Data via Topological Features](https://arxiv.org/abs/2504.16150)
*Sarah Day,Jesse Dimino,Matt Jester,Kaitlin Keegan,Thomas Weighill*

Main category: cs.CV

TLDR: 论文评估了拓扑特征在雪粒图像分类中的性能，探讨了其优势、局限性和权衡。


<details>
  <summary>Details</summary>
Motivation: 研究雪粒（firn）的拓扑和几何结构随深度的变化，利用拓扑数据分析（TDA）理解结构与深度的关系。

Method: 使用两类拓扑特征（子水平集特征和距离变换特征）及持久性曲线，从微CT图像预测样本深度。

Result: 不同训练-测试场景显示，没有单一方法在所有类别中占优，揭示了准确性、可解释性和泛化性之间的权衡。

Conclusion: 拓扑特征在雪粒分类中具有潜力，但需根据具体需求权衡不同方法的优缺点。

Abstract: In this paper we evaluate the performance of topological features for
generalizable and robust classification of firn image data, with the broader
goal of understanding the advantages, pitfalls, and trade-offs in topological
featurization. Firn refers to layers of granular snow within glaciers that
haven't been compressed into ice. This compactification process imposes
distinct topological and geometric structure on firn that varies with depth
within the firn column, making topological data analysis (TDA) a natural choice
for understanding the connection between depth and structure. We use two
classes of topological features, sublevel set features and distance transform
features, together with persistence curves, to predict sample depth from
microCT images. A range of challenging training-test scenarios reveals that no
one choice of method dominates in all categories, and uncoveres a web of
trade-offs between accuracy, interpretability, and generalizability.

</details>

### [135] [A detection-task-specific deep-learning method to improve the quality of sparse-view myocardial perfusion SPECT images](https://arxiv.org/abs/2504.16171)
*Zezhang Yang,Zitong Yu,Nuri Choi,Abhinav K. Jha*

Main category: cs.CV

TLDR: 提出一种基于深度学习的稀疏视角心肌灌注成像方法，通过减少投影角度缩短扫描时间，同时保持图像质量和检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统SPECT扫描时间长，易导致患者不适和运动伪影，减少投影角度虽能缩短时间但影响图像质量。

Method: 提出一种检测任务特定的深度学习方法，结合观察者损失项以优化灌注缺陷检测性能。

Result: 在检测心肌灌注缺陷任务中，AUC显著高于稀疏视角协议，并能恢复左心室壁结构。

Conclusion: 初步结果表明该方法有效，值得进一步评估。

Abstract: Myocardial perfusion imaging (MPI) with single-photon emission computed
tomography (SPECT) is a widely used and cost-effective diagnostic tool for
coronary artery disease. However, the lengthy scanning time in this imaging
procedure can cause patient discomfort, motion artifacts, and potentially
inaccurate diagnoses due to misalignment between the SPECT scans and the
CT-scans which are acquired for attenuation compensation. Reducing projection
angles is a potential way to shorten scanning time, but this can adversely
impact the quality of the reconstructed images. To address this issue, we
propose a detection-task-specific deep-learning method for sparse-view MPI
SPECT images. This method integrates an observer loss term that penalizes the
loss of anthropomorphic channel features with the goal of improving performance
in perfusion defect-detection task. We observed that, on the task of detecting
myocardial perfusion defects, the proposed method yielded an area under the
receiver operating characteristic (ROC) curve (AUC) significantly larger than
the sparse-view protocol. Further, the proposed method was observed to be able
to restore the structure of the left ventricle wall, demonstrating ability to
overcome sparse-sampling artifacts. Our preliminary results motivate further
evaluations of the method.

</details>

### [136] [CLIP-IT: CLIP-based Pairing for Histology Images Classification](https://arxiv.org/abs/2504.16181)
*Banafsheh Karimian,Giulia Avanzato,Soufian Belharbi,Luke McCaffrey,Mohammadhadi Shateri,Eric Granger*

Main category: cs.CV

TLDR: CLIP-IT方法通过外部文本信息增强训练视觉主干模型，解决多模态学习中配对数据需求高的问题，提升组织学图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在医学图像分析中潜力巨大，但训练视觉-语言模型需要大量配对数据，成本高且隐私问题突出。

Method: 利用CLIP模型匹配组织学图像与外部文本数据，构建增强多模态数据集；通过知识蒸馏将文本模态知识迁移到单模态图像分类器，推理时无需文本数据。

Result: 在PCAM、CRC和BACH数据集上，CLIP-IT方法优于单模态分类器，且计算复杂度低。

Conclusion: CLIP-IT提供了一种成本效益高的方法，利用特权文本信息提升组织学图像分类性能。

Abstract: Multimodal learning has shown significant promise for improving medical image
analysis by integrating information from complementary data sources. This is
widely employed for training vision-language models (VLMs) for cancer detection
based on histology images and text reports. However, one of the main
limitations in training these VLMs is the requirement for large paired
datasets, raising concerns over privacy, and data collection, annotation, and
maintenance costs. To address this challenge, we introduce CLIP-IT method to
train a vision backbone model to classify histology images by pairing them with
privileged textual information from an external source. At first, the modality
pairing step relies on a CLIP-based model to match histology images with
semantically relevant textual report data from external sources, creating an
augmented multimodal dataset without the need for manually paired samples.
Then, we propose a multimodal training procedure that distills the knowledge
from the paired text modality to the unimodal image classifier for enhanced
performance without the need for the textual data during inference. A
parameter-efficient fine-tuning method is used to efficiently address the
misalignment between the main (image) and paired (text) modalities. During
inference, the improved unimodal histology classifier is used, with only
minimal additional computational complexity. Our experiments on challenging
PCAM, CRC, and BACH histology image datasets show that CLIP-IT can provide a
cost-effective approach to leverage privileged textual information and
outperform unimodal classifiers for histology.

</details>

### [137] [DeepCS-TRD, a Deep Learning-based Cross-Section Tree Ring Detector](https://arxiv.org/abs/2504.16242)
*Henry Marichal,Verónica Casaravilla,Candice Power,Karolain Mello,Joaquín Mazarino,Christine Lucas,Ludmila Profumo,Diego Passarella,Gregory Randall*

Main category: cs.CV

TLDR: Deep CS-TRD是一种基于深度学习的自动树轮检测算法，替代了传统边缘检测方法，适用于多种图像领域和树种。


<details>
  <summary>Details</summary>
Motivation: 解决传统树轮检测方法在不同图像领域和树种中的局限性。

Method: 使用U-Net深度学习模型替代传统边缘检测步骤，适用于显微镜、扫描仪或智能手机获取的图像。

Result: 在宏观图像（Pinus taeda和Gleditsia triacanthos）中优于现有方法，但在显微镜图像（Salix glauca）中性能稍低。

Conclusion: 首次研究了多种树种和采集条件下的自动树轮检测，提供了公开数据集和源代码。

Abstract: Here, we propose Deep CS-TRD, a new automatic algorithm for detecting tree
rings in whole cross-sections. It substitutes the edge detection step of CS-TRD
by a deep-learning-based approach (U-Net), which allows the application of the
method to different image domains: microscopy, scanner or smartphone acquired,
and species (Pinus taeda, Gleditsia triachantos and Salix glauca).
Additionally, we introduce two publicly available datasets of annotated images
to the community. The proposed method outperforms state-of-the-art approaches
in macro images (Pinus taeda and Gleditsia triacanthos) while showing slightly
lower performance in microscopy images of Salix glauca. To our knowledge, this
is the first paper that studies automatic tree ring detection for such
different species and acquisition conditions. The dataset and source code are
available in https://github.com/hmarichal93/deepcstrd

</details>

### [138] [Naturally Computed Scale Invariance in the Residual Stream of ResNet18](https://arxiv.org/abs/2504.16290)
*André Longon*

Main category: cs.CV

TLDR: 论文研究了ResNet18中残差流如何通过尺度等变表示的元素级残差求和实现尺度不变性，并通过实验验证其与尺度鲁棒物体识别行为的因果关系。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络如何实现视觉对象识别中对图像变化（如光照、旋转、尺度）的不变性，尤其是不同架构网络（如ResNet18）中残差流的作用。

Method: 分析ResNet18的中间块卷积通道，观察其尺度不变性，并通过消融实验验证这些神经属性与尺度鲁棒行为的因果关系。

Result: 发现中间块的卷积通道通过尺度等变表示的元素级残差求和实现尺度不变性，消融实验初步支持其与行为的相关性。

Conclusion: 残差流可能是ResNet18实现尺度不变性的关键机制，代码已开源。

Abstract: An important capacity in visual object recognition is invariance to
image-altering variables which leave the identity of objects unchanged, such as
lighting, rotation, and scale. How do neural networks achieve this? Prior
mechanistic interpretability research has illuminated some invariance-building
circuitry in InceptionV1, but the results are limited and networks with
different architectures have remained largely unexplored. This work
investigates ResNet18 with a particular focus on its residual stream, an
architectural component which InceptionV1 lacks. We observe that many
convolutional channels in intermediate blocks exhibit scale invariant
properties, computed by the element-wise residual summation of scale
equivariant representations: the block input's smaller-scale copy with the
block pre-sum output's larger-scale copy. Through subsequent ablation
experiments, we attempt to causally link these neural properties with
scale-robust object recognition behavior. Our tentative findings suggest how
the residual stream computes scale invariance and its possible role in
behavior. Code is available at:
https://github.com/cest-andre/residual-stream-interp

</details>

### [139] [MetaHarm: Harmful YouTube Video Dataset Annotated by Domain Experts, GPT-4-Turbo, and Crowdworkers](https://arxiv.org/abs/2504.16304)
*Wonjeong Jo,Magdalena Wojcieszak*

Main category: cs.CV

TLDR: 论文提出了两个大规模数据集，用于测量和分析短视频平台上的有害内容，包括YouTube视频和标注数据，以促进未来对在线危害的研究和分类。


<details>
  <summary>Details</summary>
Motivation: 短视频平台（如YouTube、TikTok）存在大量有害内容，但目前缺乏对其全面理解和测量的研究。

Method: 构建了两个数据集：60,906个潜在有害YouTube视频和19,422个标注视频，标注由专家、GPT-4-Turbo和众包工人完成，涵盖六类危害。

Result: 数据集提供了多模态、多类别的标注结果，包括二分类和多标签分类，并提供了不同标注者的一致性数据。

Conclusion: 这些数据集有助于未来对在线危害的研究，支持多模态分类，并推动有害内容的识别和缓解。

Abstract: Short video platforms, such as YouTube, Instagram, or TikTok, are used by
billions of users. These platforms expose users to harmful content, ranging
from clickbait or physical harms to hate or misinformation. Yet, we lack a
comprehensive understanding and measurement of online harm on short video
platforms. Toward this end, we present two large-scale datasets of multi-modal
and multi-categorical online harm: (1) 60,906 systematically selected
potentially harmful YouTube videos and (2) 19,422 videos annotated by three
labeling actors: trained domain experts, GPT-4-Turbo (using 14 image frames, 1
thumbnail, and text metadata), and crowdworkers (Amazon Mechanical Turk master
workers). The annotated dataset includes both (a) binary classification
(harmful vs. harmless) and (b) multi-label categorizations of six harm
categories: Information, Hate and harassment, Addictive, Clickbait, Sexual, and
Physical harms. Furthermore, the annotated dataset provides (1) ground truth
data with videos annotated consistently across (a) all three actors and (b) the
majority of the labeling actors, and (2) three data subsets labeled by
individual actors. These datasets are expected to facilitate future work on
online harm, aid in (multi-modal) classification efforts, and advance the
identification and potential mitigation of harmful content on video platforms.

</details>

### [140] [SignX: The Foundation Model for Sign Recognition](https://arxiv.org/abs/2504.16315)
*Sen Fang,Chunyu Sui,Hongwei Yi,Carol Neidle,Dimitris N. Metaxas*

Main category: cs.CV

TLDR: SignX是一个用于手语识别的框架，通过两阶段训练（Pose2Gloss和Video2Pose）实现高精度识别。


<details>
  <summary>Details</summary>
Motivation: 手语数据处理复杂，现有方法依赖不一致的ID标注，需统一框架。

Method: 提出SignX框架，包含基于逆扩散模型的Pose2Gloss和基于ViT的Video2Pose模块。

Result: 实验表明SignX识别精度优于现有方法。

Conclusion: SignX为手语识别提供了兼容现有姿态格式的统一框架。

Abstract: The complexity of sign language data processing brings many challenges. The
current approach to recognition of ASL signs aims to translate RGB sign
language videos through pose information into English-based ID glosses, which
serve to uniquely identify ASL signs. Note that there is no shared convention
for assigning such glosses to ASL signs, so it is essential that the same
glossing conventions are used for all of the data in the datasets that are
employed. This paper proposes SignX, a foundation model framework for sign
recognition. It is a concise yet powerful framework applicable to multiple
human activity recognition scenarios. First, we developed a Pose2Gloss
component based on an inverse diffusion model, which contains a multi-track
pose fusion layer that unifies five of the most powerful pose information
sources--SMPLer-X, DWPose, Mediapipe, PrimeDepth, and Sapiens
Segmentation--into a single latent pose representation. Second, we trained a
Video2Pose module based on ViT that can directly convert raw video into signer
pose representation. Through this 2-stage training framework, we enable sign
language recognition models to be compatible with existing pose formats, laying
the foundation for the common pose estimation necessary for sign recognition.
Experimental results show that SignX can recognize signs from sign language
video, producing predicted gloss representations with greater accuracy than has
been reported in prior work.

</details>

### [141] [Almost Right: Making First-layer Kernels Nearly Orthogonal Improves Model Generalization](https://arxiv.org/abs/2504.16362)
*Colton R. Crum,Adam Czajka*

Main category: cs.CV

TLDR: 提出一种新的损失组件，通过正则化第一卷积层的滤波核使其接近正交，提高模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 受人类感知智能启发，提高模型对未知样本的泛化能力。

Method: 提出一种灵活的损失组件，允许网络自主选择正交核对，避免严格正交限制。

Result: 在三种架构（ResNet-50、DenseNet-121、ViT-b-16）和两个开放集识别任务中，泛化性能显著提升。

Conclusion: 该方法无需架构修改，即可显著优于现有正交化和显著性正则化方法。

Abstract: An ongoing research challenge within several domains in computer vision is
how to increase model generalization capabilities. Several attempts to improve
model generalization performance are heavily inspired by human perceptual
intelligence, which is remarkable in both its performance and efficiency to
generalize to unknown samples. Many of these methods attempt to force portions
of the network to be orthogonal, following some observation within neuroscience
related to early vision processes. In this paper, we propose a loss component
that regularizes the filtering kernels in the first convolutional layer of a
network to make them nearly orthogonal. Deviating from previous works, we give
the network flexibility in which pairs of kernels it makes orthogonal, allowing
the network to navigate to a better solution space, imposing harsh penalties.
Without architectural modifications, we report substantial gains in
generalization performance using the proposed loss against previous works
(including orthogonalization- and saliency-based regularization methods) across
three different architectures (ResNet-50, DenseNet-121, ViT-b-16) and two
difficult open-set recognition tasks: presentation attack detection in iris
biometrics, and anomaly detection in chest X-ray images.

</details>

### [142] [CLPSTNet: A Progressive Multi-Scale Convolutional Steganography Model Integrating Curriculum Learning](https://arxiv.org/abs/2504.16364)
*Fengchun Liu,Tong Zhang,Chunying Zhang*

Main category: cs.CV

TLDR: 提出了一种基于课程学习的渐进式多尺度卷积网络（CLPSTNet），用于解决CNN在图像隐写术中的不可见性和安全性问题。


<details>
  <summary>Details</summary>
Motivation: 传统隐写术方法依赖于手工特征和先验知识设计，而CNN的引入虽然实现了信息嵌入的自主学习，但仍存在不可见性和安全性问题。

Method: CLPSTNet由多个渐进式多尺度卷积模块组成，结合Inception结构和扩张卷积，从浅到深、从细到粗提取多尺度特征。

Result: 在ALASKA2、VOC2012和ImageNet数据集上，CLPSTNet表现出高PSNR、SSIM和解码精度，且生成的隐写图像具有低隐写分析分数。

Conclusion: CLPSTNet通过渐进式多尺度特征提取，显著提升了隐写术的不可见性和安全性。

Abstract: In recent years, a large number of works have introduced Convolutional Neural
Networks (CNNs) into image steganography, which transform traditional
steganography methods such as hand-crafted features and prior knowledge design
into steganography methods that neural networks autonomically learn information
embedding. However, due to the inherent complexity of digital images, issues of
invisibility and security persist when using CNN models for information
embedding. In this paper, we propose Curriculum Learning Progressive Steganophy
Network (CLPSTNet). The network consists of multiple progressive multi-scale
convolutional modules that integrate Inception structures and dilated
convolutions. The module contains multiple branching pathways, starting from a
smaller convolutional kernel and dilatation rate, extracting the basic, local
feature information from the feature map, and gradually expanding to the
convolution with a larger convolutional kernel and dilatation rate for
perceiving the feature information of a larger receptive field, so as to
realize the multi-scale feature extraction from shallow to deep, and from fine
to coarse, allowing the shallow secret information features to be refined in
different fusion stages. The experimental results show that the proposed
CLPSTNet not only has high PSNR , SSIM metrics and decoding accuracy on three
large public datasets, ALASKA2, VOC2012 and ImageNet, but also the
steganographic images generated by CLPSTNet have low steganalysis scores.You
can find our code at
\href{https://github.com/chaos-boops/CLPSTNet}{https://github.com/chaos-boops/CLPSTNet}.

</details>

### [143] [Revisiting Radar Camera Alignment by Contrastive Learning for 3D Object Detection](https://arxiv.org/abs/2504.16368)
*Linhua Kong,Dongxia Chang,Lian Liu,Zisen Kong,Pengyuan Li,Yao Zhao*

Main category: cs.CV

TLDR: 提出了一种新的雷达与相机特征对齐模型RCAlign，通过双路径对齐模块和雷达特征增强模块，显著提升了3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在雷达与相机特征对齐中存在特征交互不足或空间位置对齐不准确的问题。

Method: 设计了基于对比学习的双路径对齐模块（DRA）和雷达特征增强模块（RFE），结合知识蒸馏损失。

Result: 在nuScenes基准测试中达到最新性能，实时3D检测性能提升显著（4.3% NDS和8.4% mAP）。

Conclusion: RCAlign有效解决了雷达与相机特征对齐问题，显著提升了3D目标检测性能。

Abstract: Recently, 3D object detection algorithms based on radar and camera fusion
have shown excellent performance, setting the stage for their application in
autonomous driving perception tasks. Existing methods have focused on dealing
with feature misalignment caused by the domain gap between radar and camera.
However, existing methods either neglect inter-modal features interaction
during alignment or fail to effectively align features at the same spatial
location across modalities. To alleviate the above problems, we propose a new
alignment model called Radar Camera Alignment (RCAlign). Specifically, we
design a Dual-Route Alignment (DRA) module based on contrastive learning to
align and fuse the features between radar and camera. Moreover, considering the
sparsity of radar BEV features, a Radar Feature Enhancement (RFE) module is
proposed to improve the densification of radar BEV features with the knowledge
distillation loss. Experiments show RCAlign achieves a new state-of-the-art on
the public nuScenes benchmark in radar camera fusion for 3D Object Detection.
Furthermore, the RCAlign achieves a significant performance gain (4.3\% NDS and
8.4\% mAP) in real-time 3D detection compared to the latest state-of-the-art
method (RCBEVDet).

</details>

### [144] [SaENeRF: Suppressing Artifacts in Event-based Neural Radiance Fields](https://arxiv.org/abs/2504.16389)
*Yuanjian Wang,Yufei Deng,Rong Xiao,Jiahao Fan,Chenwei Tang,Deng Xiong,Jiancheng Lv*

Main category: cs.CV

TLDR: 论文提出了一种名为SaENeRF的自监督框架，用于从事件流中重建高质量3D场景，解决了现有方法中的噪声和伪影问题。


<details>
  <summary>Details</summary>
Motivation: 事件相机在高动态范围场景中表现优异，但现有方法在3D重建中存在伪影和噪声问题，限制了其应用。

Method: 通过归一化预测的辐射变化和引入正则化损失，抑制伪影并提升重建质量。

Result: 实验表明，SaENeRF显著减少了伪影，重建质量优于现有方法。

Conclusion: SaENeRF为事件相机的3D重建提供了一种高效且高质量的解决方案。

Abstract: Event cameras are neuromorphic vision sensors that asynchronously capture
changes in logarithmic brightness changes, offering significant advantages such
as low latency, low power consumption, low bandwidth, and high dynamic range.
While these characteristics make them ideal for high-speed scenarios,
reconstructing geometrically consistent and photometrically accurate 3D
representations from event data remains fundamentally challenging. Current
event-based Neural Radiance Fields (NeRF) methods partially address these
challenges but suffer from persistent artifacts caused by aggressive network
learning in early stages and the inherent noise of event cameras. To overcome
these limitations, we present SaENeRF, a novel self-supervised framework that
effectively suppresses artifacts and enables 3D-consistent, dense, and
photorealistic NeRF reconstruction of static scenes solely from event streams.
Our approach normalizes predicted radiance variations based on accumulated
event polarities, facilitating progressive and rapid learning for scene
representation construction. Additionally, we introduce regularization losses
specifically designed to suppress artifacts in regions where photometric
changes fall below the event threshold and simultaneously enhance the light
intensity difference of non-zero events, thereby improving the visual fidelity
of the reconstructed scene. Extensive qualitative and quantitative experiments
demonstrate that our method significantly reduces artifacts and achieves
superior reconstruction quality compared to existing methods. The code is
available at https://github.com/Mr-firework/SaENeRF.

</details>

### [145] [Assessing the Feasibility of Internet-Sourced Video for Automatic Cattle Lameness Detection](https://arxiv.org/abs/2504.16404)
*Md Fahimuzzman Sohan*

Main category: cs.CV

TLDR: 该研究利用深度学习模型（3D CNN和ConvLSTM2D）从视频数据中检测牛跛行，3D CNN表现最佳，准确率达90%。


<details>
  <summary>Details</summary>
Motivation: 牛跛行由蹄部损伤或趾间皮炎引起，影响其行走、进食等生理活动，需高效检测方法。

Method: 使用50个视频数据集，数据增强后，用3D CNN和ConvLSTM2D分类。

Result: 3D CNN准确率90%，ConvLSTM2D为85%。

Conclusion: 深度学习模型可直接从视频中学习时空特征，简化传统多阶段流程，3D CNN效果更优。

Abstract: Cattle lameness is often caused by hoof injuries or interdigital dermatitis,
leads to pain and significantly impacts essential physiological activities such
as walking, feeding, and drinking. This study presents a deep learning-based
model for detecting cattle lameness, sickness, or gait abnormalities using
publicly available video data. The dataset consists of 50 unique videos from 40
individual cattle, recorded from various angles in both indoor and outdoor
environments. Half of the dataset represents naturally walking
(normal/non-lame) cattle, while the other half consists of cattle exhibiting
gait abnormalities (lame). To enhance model robustness and generalizability,
data augmentation was applied to the training data. The pre-processed videos
were then classified using two deep learning models: ConvLSTM2D and 3D CNN. A
comparative analysis of the results demonstrates strong classification
performance. Specifically, the 3D CNN model achieved a video-level
classification accuracy of 90%, with precision, recall, and f1-score of 90.9%,
90.9%, and 90.91% respectively. The ConvLSTM2D model exhibited a slightly lower
accuracy of 85%. This study highlights the effectiveness of directly applying
classification models to learn spatiotemporal features from video data,
offering an alternative to traditional multi-stage approaches that typically
involve object detection, pose estimation, and feature extraction. Besides, the
findings demonstrate that the proposed deep learning models, particularly the
3D CNN, effectively classify and detect lameness in cattle while simplifying
the processing pipeline.

</details>

### [146] [PixelWeb: The First Web GUI Dataset with Pixel-Wise Labels](https://arxiv.org/abs/2504.16419)
*Qi Yang,Weichen Bi,Haiyang Shen,Yaoqi Guo,Yun Ma*

Main category: cs.CV

TLDR: PixelWeb是一个大规模GUI数据集，通过视觉特征提取和DOM结构分析提供高质量标注，显著提升GUI元素检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有GUI数据集标注不准确且仅提供视觉BBox标注，限制了模型性能和下游任务发展。

Method: 采用通道派生和层次分析两模块，结合BGRA四通道位图标注和DOM结构分析，生成精确BBox标注。

Result: 在mAP95指标上性能比现有数据集高3-7倍，标注质量经人工验证。

Conclusion: PixelWeb能显著提升GUI生成和自动化交互等下游任务的性能。

Abstract: Graphical User Interface (GUI) datasets are crucial for various downstream
tasks. However, GUI datasets often generate annotation information through
automatic labeling, which commonly results in inaccurate GUI element BBox
annotations, including missing, duplicate, or meaningless BBoxes. These issues
can degrade the performance of models trained on these datasets, limiting their
effectiveness in real-world applications. Additionally, existing GUI datasets
only provide BBox annotations visually, which restricts the development of
visually related GUI downstream tasks. To address these issues, we introduce
PixelWeb, a large-scale GUI dataset containing over 100,000 annotated web
pages. PixelWeb is constructed using a novel automatic annotation approach that
integrates visual feature extraction and Document Object Model (DOM) structure
analysis through two core modules: channel derivation and layer analysis.
Channel derivation ensures accurate localization of GUI elements in cases of
occlusion and overlapping elements by extracting BGRA four-channel bitmap
annotations. Layer analysis uses the DOM to determine the visibility and
stacking order of elements, providing precise BBox annotations. Additionally,
PixelWeb includes comprehensive metadata such as element images, contours, and
mask annotations. Manual verification by three independent annotators confirms
the high quality and accuracy of PixelWeb annotations. Experimental results on
GUI element detection tasks show that PixelWeb achieves performance on the
mAP95 metric that is 3-7 times better than existing datasets. We believe that
PixelWeb has great potential for performance improvement in downstream tasks
such as GUI generation and automated user interaction.

</details>

### [147] [FrogDogNet: Fourier frequency Retained visual prompt Output Guidance for Domain Generalization of CLIP in Remote Sensing](https://arxiv.org/abs/2504.16433)
*Hariseetharam Gunduboina,Muhammad Haris Khan,Biplab Banerjee*

Main category: cs.CV

TLDR: FrogDogNet提出了一种结合傅里叶频率过滤和自注意力的提示学习框架，用于提升遥感场景分类和领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖全图像特征，引入噪声和背景干扰，导致分类错误，而FrogDogNet旨在解决这一问题。

Method: 通过投影和自注意力提取关键特征，再应用频率过滤保留低频不变信息，优化提示学习。

Result: 在四个遥感数据集和三个领域泛化任务中，FrogDogNet表现优于现有方法。

Conclusion: 频率不变特征保留在泛化中效果显著，为更广泛应用铺平了道路。

Abstract: In recent years, large-scale vision-language models (VLMs) like CLIP have
gained attention for their zero-shot inference using instructional text
prompts. While these models excel in general computer vision, their potential
for domain generalization in remote sensing (RS) remains underexplored.
Existing approaches enhance prompt learning by generating visual prompt tokens
but rely on full-image features, introducing noise and background artifacts
that vary within a class, causing misclassification. To address this, we
propose FrogDogNet, a novel prompt learning framework integrating Fourier
frequency filtering and self-attention to improve RS scene classification and
domain generalization. FrogDogNet selectively retains invariant low-frequency
components while eliminating noise and irrelevant backgrounds, ensuring robust
feature representation across domains. The model first extracts significant
features via projection and self-attention, then applies frequency-based
filtering to preserve essential structural information for prompt learning.
Extensive experiments on four RS datasets and three domain generalization tasks
show that FrogDogNet consistently outperforms state-of-the-art prompt learning
methods, demonstrating superior adaptability across domain shifts. Our findings
highlight the effectiveness of frequency-based invariant feature retention in
generalization, paving the way for broader applications. Our code is available
at https://github.com/HariseetharamG/FrogDogNet

</details>

### [148] [Marginalized Generalized IoU (MGIoU): A Unified Objective Function for Optimizing Any Convex Parametric Shapes](https://arxiv.org/abs/2504.16443)
*Duy-Tho Le,Trung Pham,Jianfei Cai,Hamid Rezatofighi*

Main category: cs.CV

TLDR: 论文提出了一种新的损失函数MGIoU和MGIoU+，用于统一参数化形状优化，解决了现有方法的不足，如相关性差、不稳定或计算复杂。


<details>
  <summary>Details</summary>
Motivation: 现有参数化形状优化方法存在多种问题，如回归损失与IoU相关性差、IoU损失不稳定且仅适用于简单形状，以及任务特定方法计算复杂且缺乏通用性。

Method: 通过将结构化凸形状投影到其唯一形状法线上，计算一维归一化GIoU，提出MGIoU和MGIoU+，支持无结构凸形状优化。

Result: 实验表明，MGIoU和MGIoU+在标准基准上表现优于现有损失函数，计算延迟降低10-40倍，并满足度量性质和尺度不变性。

Conclusion: MGIoU和MGIoU+为参数化形状优化提供了统一、高效且鲁棒的解决方案，适用于多种应用场景。

Abstract: Optimizing the similarity between parametric shapes is crucial for numerous
computer vision tasks, where Intersection over Union (IoU) stands as the
canonical measure. However, existing optimization methods exhibit significant
shortcomings: regression-based losses like L1/L2 lack correlation with IoU,
IoU-based losses are unstable and limited to simple shapes, and task-specific
methods are computationally intensive and not generalizable accross domains. As
a result, the current landscape of parametric shape objective functions has
become scattered, with each domain proposing distinct IoU approximations. To
address this, we unify the parametric shape optimization objective functions by
introducing Marginalized Generalized IoU (MGIoU), a novel loss function that
overcomes these challenges by projecting structured convex shapes onto their
unique shape Normals to compute one-dimensional normalized GIoU. MGIoU offers a
simple, efficient, fully differentiable approximation strongly correlated with
IoU. We then extend MGIoU to MGIoU+ that supports optimizing unstructured
convex shapes. Together, MGIoU and MGIoU+ unify parametric shape optimization
across diverse applications. Experiments on standard benchmarks demonstrate
that MGIoU and MGIoU+ consistently outperform existing losses while reducing
loss computation latency by 10-40x. Additionally, MGIoU and MGIoU+ satisfy
metric properties and scale-invariance, ensuring robustness as an objective
function. We further propose MGIoU- for minimizing overlaps in tasks like
collision-free trajectory prediction. Code is available at
https://ldtho.github.io/MGIoU

</details>

### [149] [Cross Paradigm Representation and Alignment Transformer for Image Deraining](https://arxiv.org/abs/2504.16455)
*Shun Zou,Yi Zou,Juncheng Li,Guangwei Gao,Guojun Qi*

Main category: cs.CV

TLDR: 提出了一种跨范式表示和对齐Transformer（CPRAformer），通过整合全局-局部和空间-通道表示，解决了图像去雨任务中不规则雨纹和复杂几何重叠的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer架构在图像去雨任务中难以应对不规则雨纹和复杂几何重叠，需要一种统一框架来整合互补的全局-局部和空间-通道表示。

Method: 提出CPRAformer，采用稀疏提示通道自注意力（SPC-SA）和空间像素细化自注意力（SPR-SA），并通过自适应对齐频率模块（AAFM）实现特征对齐和交互。

Result: 在八个基准数据集上实现了最先进的性能，并在其他图像修复任务和下游应用中验证了模型的鲁棒性。

Conclusion: CPRAformer通过跨范式动态交互框架，有效提取了两种范式中最有价值的融合信息，显著提升了图像去雨任务的性能。

Abstract: Transformer-based networks have achieved strong performance in low-level
vision tasks like image deraining by utilizing spatial or channel-wise
self-attention. However, irregular rain patterns and complex geometric overlaps
challenge single-paradigm architectures, necessitating a unified framework to
integrate complementary global-local and spatial-channel representations. To
address this, we propose a novel Cross Paradigm Representation and Alignment
Transformer (CPRAformer). Its core idea is the hierarchical representation and
alignment, leveraging the strengths of both paradigms (spatial-channel and
global-local) to aid image reconstruction. It bridges the gap within and
between paradigms, aligning and coordinating them to enable deep interaction
and fusion of features. Specifically, we use two types of self-attention in the
Transformer blocks: sparse prompt channel self-attention (SPC-SA) and spatial
pixel refinement self-attention (SPR-SA). SPC-SA enhances global channel
dependencies through dynamic sparsity, while SPR-SA focuses on spatial rain
distribution and fine-grained texture recovery. To address the feature
misalignment and knowledge differences between them, we introduce the Adaptive
Alignment Frequency Module (AAFM), which aligns and interacts with features in
a two-stage progressive manner, enabling adaptive guidance and complementarity.
This reduces the information gap within and between paradigms. Through this
unified cross-paradigm dynamic interaction framework, we achieve the extraction
of the most valuable interactive fusion information from the two paradigms.
Extensive experiments demonstrate that our model achieves state-of-the-art
performance on eight benchmark datasets and further validates CPRAformer's
robustness in other image restoration tasks and downstream applications.

</details>

### [150] [MTSGL: Multi-Task Structure Guided Learning for Robust and Interpretable SAR Aircraft Recognition](https://arxiv.org/abs/2504.16467)
*Qishan He,Lingjun Zhao,Ru Luo,Siqian Zhang,Lin Lei,Kefeng Ji,Gangyao Kuang*

Main category: cs.CV

TLDR: 论文提出了一种基于结构的多任务学习网络（MTSGL），用于SAR图像中的飞机识别，结合结构语义和几何一致性提升模型的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前SAR图像飞机识别方法缺乏对飞机结构知识的深入理解，需要结合结构信息提升识别效果。

Method: 引入结构注释方法，提出MTSGL网络，包含分类任务、结构语义感知（SSA）模块和结构一致性正则化（SCR）模块。

Result: 在自建数据集MT-SARD上验证，MTSGL表现出优越的鲁棒性和可解释性。

Conclusion: MTSGL通过结合结构先验知识，实现了类似人类认知的飞机识别效果。

Abstract: Aircraft recognition in synthetic aperture radar (SAR) imagery is a
fundamental mission in both military and civilian applications. Recently deep
learning (DL) has emerged a dominant paradigm for its explosive performance on
extracting discriminative features. However, current classification algorithms
focus primarily on learning decision hyperplane without enough comprehension on
aircraft structural knowledge. Inspired by the fined aircraft annotation
methods for optical remote sensing images (RSI), we first introduce a
structure-based SAR aircraft annotations approach to provide structural and
compositional supplement information. On this basis, we propose a multi-task
structure guided learning (MTSGL) network for robust and interpretable SAR
aircraft recognition. Besides the classification task, MTSGL includes a
structural semantic awareness (SSA) module and a structural consistency
regularization (SCR) module. The SSA is designed to capture structure semantic
information, which is conducive to gain human-like comprehension of aircraft
knowledge. The SCR helps maintain the geometric consistency between the
aircraft structure in SAR imagery and the proposed annotation. In this process,
the structural attribute can be disentangled in a geometrically meaningful
manner. In conclusion, the MTSGL is presented with the expert-level aircraft
prior knowledge and structure guided learning paradigm, aiming to comprehend
the aircraft concept in a way analogous to the human cognitive process.
Extensive experiments are conducted on a self-constructed multi-task SAR
aircraft recognition dataset (MT-SARD) and the effective results illustrate the
superiority of robustness and interpretation ability of the proposed MTSGL.

</details>

### [151] [RGB-D Video Object Segmentation via Enhanced Multi-store Feature Memory](https://arxiv.org/abs/2504.16471)
*Boyue Xu,Ruichao Hou,Tongwei Ren,Gangshan Wu*

Main category: cs.CV

TLDR: 提出一种基于多存储特征记忆的RGB-D视频对象分割方法，通过自适应模态选择和融合提升分割性能，并利用SAM模型优化分割结果。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D分割方法未能充分利用跨模态信息且存在长期预测中的对象漂移问题。

Method: 设计分层模态选择和融合模块，结合SAM模型的分割细化模块，利用时空和模态嵌入生成混合提示和融合图像。

Result: 在最新RGB-D VOS基准测试中达到最先进性能。

Conclusion: 所提方法通过多模态特征融合和SAM模型优化，显著提升了RGB-D视频对象分割的鲁棒性和准确性。

Abstract: The RGB-Depth (RGB-D) Video Object Segmentation (VOS) aims to integrate the
fine-grained texture information of RGB with the spatial geometric clues of
depth modality, boosting the performance of segmentation. However,
off-the-shelf RGB-D segmentation methods fail to fully explore cross-modal
information and suffer from object drift during long-term prediction. In this
paper, we propose a novel RGB-D VOS method via multi-store feature memory for
robust segmentation. Specifically, we design the hierarchical modality
selection and fusion, which adaptively combines features from both modalities.
Additionally, we develop a segmentation refinement module that effectively
utilizes the Segmentation Anything Model (SAM) to refine the segmentation mask,
ensuring more reliable results as memory to guide subsequent segmentation
tasks. By leveraging spatio-temporal embedding and modality embedding, mixed
prompts and fused images are fed into SAM to unleash its potential in RGB-D
VOS. Experimental results show that the proposed method achieves
state-of-the-art performance on the latest RGB-D VOS benchmark.

</details>

### [152] [Rethinking Generalizable Infrared Small Target Detection: A Real-scene Benchmark and Cross-view Representation Learning](https://arxiv.org/abs/2504.16487)
*Yahao Lu,Yuehui Li,Xingyuan Guo,Shuai Yuan,Yukai Shi,Liang Lin*

Main category: cs.CV

TLDR: 本文提出了一种基于域适应的红外小目标检测框架，通过跨视图通道对齐和噪声引导表示学习策略，提升模型在多样化场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测对传感器类型、观测条件和目标特性高度敏感，导致数据分布差异（域偏移），限制了模型的跨场景泛化能力。

Method: 提出跨视图通道对齐（CCA）和跨视图Top-K融合策略，结合噪声引导表示学习，以缓解分布差异和噪声影响。

Result: 在检测概率（Pd）、误报率（Fa）和交并比（IoU）上优于现有方法。

Conclusion: 该方法通过域适应和噪声处理策略，显著提升了红外小目标检测的泛化性能和准确性。

Abstract: Infrared small target detection (ISTD) is highly sensitive to sensor type,
observation conditions, and the intrinsic properties of the target. These
factors can introduce substantial variations in the distribution of acquired
infrared image data, a phenomenon known as domain shift. Such distribution
discrepancies significantly hinder the generalization capability of ISTD models
across diverse scenarios. To tackle this challenge, this paper introduces an
ISTD framework enhanced by domain adaptation. To alleviate distribution shift
between datasets and achieve cross-sample alignment, we introduce Cross-view
Channel Alignment (CCA). Additionally, we propose the Cross-view Top-K Fusion
strategy, which integrates target information with diverse background features,
enhancing the model' s ability to extract critical data characteristics. To
further mitigate the impact of noise on ISTD, we develop a Noise-guided
Representation learning strategy. This approach enables the model to learn more
noise-resistant feature representations, to improve its generalization
capability across diverse noisy domains. Finally, we develop a dedicated
infrared small target dataset, RealScene-ISTD. Compared to state-of-the-art
methods, our approach demonstrates superior performance in terms of detection
probability (Pd), false alarm rate (Fa), and intersection over union (IoU). The
code is available at: https://github.com/luy0222/RealScene-ISTD.

</details>

### [153] [PRaDA: Projective Radial Distortion Averaging](https://arxiv.org/abs/2504.16499)
*Daniil Sinitsyn,Linus Härenstam-Nielsen,Daniel Cremers*

Main category: cs.CV

TLDR: 提出了一种在投影空间中解耦径向畸变校准与3D重建的方法，避免了传统方法的复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统径向畸变校准方法需要解决完整的SfM问题或依赖学习，前者复杂，后者精度低。本文旨在保持SfM精度同时简化流程。

Method: 在投影空间中工作，利用单应性封装除畸变外的相机参数，提出Projective Radial Distortion Averaging方法，通过平均多组畸变估计实现校准，无需3D点或完整捆绑调整。

Result: 方法在保持SfM精度的同时，避免了复杂性和多图像点轨迹构建的需求，支持任意特征匹配方法。

Conclusion: 该方法有效简化了径向畸变校准流程，同时保持了高精度，适用于挑战性条件下的相机校准。

Abstract: We tackle the problem of automatic calibration of radially distorted cameras
in challenging conditions. Accurately determining distortion parameters
typically requires either 1) solving the full Structure from Motion (SfM)
problem involving camera poses, 3D points, and the distortion parameters, which
is only possible if many images with sufficient overlap are provided, or 2)
relying heavily on learning-based methods that are comparatively less accurate.
In this work, we demonstrate that distortion calibration can be decoupled from
3D reconstruction, maintaining the accuracy of SfM-based methods while avoiding
many of the associated complexities. This is achieved by working in Projective
Space, where the geometry is unique up to a homography, which encapsulates all
camera parameters except for distortion. Our proposed method, Projective Radial
Distortion Averaging, averages multiple distortion estimates in a fully
projective framework without creating 3d points and full bundle adjustment. By
relying on pairwise projective relations, our methods support any
feature-matching approaches without constructing point tracks across multiple
images.

</details>

### [154] [TraveLLaMA: Facilitating Multi-modal Large Language Models to Understand Urban Scenes and Provide Travel Assistance](https://arxiv.org/abs/2504.16505)
*Meng Chu,Yukang Chen,Haokun Gui,Shaozuo Yu,Yi Wang,Jiaya Jia*

Main category: cs.CV

TLDR: TraveLLaMA是一个专为城市场景理解和旅行辅助设计的多模态语言模型，通过大规模数据集和微调实验显著提升了旅行相关任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统缺乏对城市环境的专业知识和上下文理解，无法满足旅行规划的需求。

Method: 构建了包含22万问答对的数据集，结合文本和视觉语言数据，并在先进的多模态模型上进行微调。

Result: 性能提升6.5%-9.4%，在旅行推荐、地图理解和场景分析方面表现优异。

Conclusion: TraveLLaMA为多模态旅行辅助系统设立了新标杆，显著优于通用模型。

Abstract: Tourism and travel planning increasingly rely on digital assistance, yet
existing multimodal AI systems often lack specialized knowledge and contextual
understanding of urban environments. We present TraveLLaMA, a specialized
multimodal language model designed for urban scene understanding and travel
assistance. Our work addresses the fundamental challenge of developing
practical AI travel assistants through a novel large-scale dataset of 220k
question-answer pairs. This comprehensive dataset uniquely combines 130k text
QA pairs meticulously curated from authentic travel forums with GPT-enhanced
responses, alongside 90k vision-language QA pairs specifically focused on map
understanding and scene comprehension. Through extensive fine-tuning
experiments on state-of-the-art vision-language models (LLaVA, Qwen-VL,
Shikra), we demonstrate significant performance improvements ranging from
6.5\%-9.4\% in both pure text travel understanding and visual question
answering tasks. Our model exhibits exceptional capabilities in providing
contextual travel recommendations, interpreting map locations, and
understanding place-specific imagery while offering practical information such
as operating hours and visitor reviews. Comparative evaluations show TraveLLaMA
significantly outperforms general-purpose models in travel-specific tasks,
establishing a new benchmark for multi-modal travel assistance systems.

</details>

### [155] [Federated Learning of Low-Rank One-Shot Image Detection Models in Edge Devices with Scalable Accuracy and Compute Complexity](https://arxiv.org/abs/2504.16515)
*Abdul Hannaan,Zubair Shah,Aiman Erbad,Amr Mohamed,Ali Safa*

Main category: cs.CV

TLDR: LoRa-FL是一种新型联邦学习框架，用于训练低秩单次图像检测模型，显著降低计算和通信开销，同时保持可扩展的准确性。


<details>
  <summary>Details</summary>
Motivation: 针对边缘设备资源受限的问题，提出一种轻量化的联邦学习框架，以降低计算和通信成本，同时不牺牲模型性能。

Method: 结合低秩适应技术和单次检测架构，利用联邦学习协作训练轻量级图像识别模型。

Result: 在MNIST和CIFAR10数据集（IID和非IID设置）上验证了方法的有效性，实现了竞争性的检测性能，同时显著减少了通信带宽和计算复杂度。

Conclusion: LoRa-FL是一种有前景的解决方案，能够自适应地减少通信和计算开销，同时保持模型准确性。

Abstract: This paper introduces a novel federated learning framework termed LoRa-FL
designed for training low-rank one-shot image detection models deployed on edge
devices. By incorporating low-rank adaptation techniques into one-shot
detection architectures, our method significantly reduces both computational
and communication overhead while maintaining scalable accuracy. The proposed
framework leverages federated learning to collaboratively train lightweight
image recognition models, enabling rapid adaptation and efficient deployment
across heterogeneous, resource-constrained devices. Experimental evaluations on
the MNIST and CIFAR10 benchmark datasets, both in an
independent-and-identically-distributed (IID) and non-IID setting, demonstrate
that our approach achieves competitive detection performance while
significantly reducing communication bandwidth and compute complexity. This
makes it a promising solution for adaptively reducing the communication and
compute power overheads, while not sacrificing model accuracy.

</details>

### [156] [Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation](https://arxiv.org/abs/2504.16516)
*Junrong Yue,Yifan Zhang,Chuan Qin,Bo Li,Xiaomin Lie,Xinlei Yu,Wenxin Zhang,Zhendong Zhao*

Main category: cs.CV

TLDR: 提出了一种多级融合与推理架构（MFRA），通过多模态特征融合提升视觉-语言导航能力，实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖全局场景或对象级特征，难以捕捉跨模态的复杂交互，影响导航准确性。

Method: 引入分层融合机制，整合从低层视觉到高层语义的多级特征，设计推理模块通过指令引导注意力和动态上下文集成推断导航动作。

Result: 在REVERIE、R2R和SOON等基准数据集上表现优于现有方法。

Conclusion: 多级模态融合对具身导航有效，MFRA显著提升决策准确性。

Abstract: Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow
natural language instructions and reach target locations in real-world
environments. While prior methods often rely on either global scene
representations or object-level features, these approaches are insufficient for
capturing the complex interactions across modalities required for accurate
navigation. In this paper, we propose a Multi-level Fusion and Reasoning
Architecture (MFRA) to enhance the agent's ability to reason over visual
observations, language instructions and navigation history. Specifically, MFRA
introduces a hierarchical fusion mechanism that aggregates multi-level
features-ranging from low-level visual cues to high-level semantic
concepts-across multiple modalities. We further design a reasoning module that
leverages fused representations to infer navigation actions through
instruction-guided attention and dynamic context integration. By selectively
capturing and combining relevant visual, linguistic, and temporal signals, MFRA
improves decision-making accuracy in complex navigation scenarios. Extensive
experiments on benchmark VLN datasets including REVERIE, R2R, and SOON
demonstrate that MFRA achieves superior performance compared to
state-of-the-art methods, validating the effectiveness of multi-level modal
fusion for embodied navigation.

</details>

### [157] [A Few-Shot Metric Learning Method with Dual-Channel Attention for Cross-Modal Same-Neuron Identification](https://arxiv.org/abs/2504.16520)
*Wenwei Li,Liyi Cai,Wu Chen,Anan Li*

Main category: cs.CV

TLDR: 提出一种基于双通道注意力机制和预训练视觉Transformer的少样本度量学习方法，用于跨模态神经元识别，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 神经科学研究中，跨模态单神经元匹配对理解神经元结构与功能关系至关重要，但模态差异和有限标注带来挑战。

Method: 采用双通道注意力机制（局部和全局通道分别提取形态和上下文信息）和预训练视觉Transformer，结合硬样本挖掘策略和Circle Loss函数。

Result: 在两光子和fMOST数据集上表现出更高的Top-K准确率和召回率，消融实验和t-SNE可视化验证了模块有效性。

Conclusion: 该方法为单细胞水平匹配和多模态神经影像整合提供了有前景的技术解决方案。

Abstract: In neuroscience research, achieving single-neuron matching across different
imaging modalities is critical for understanding the relationship between
neuronal structure and function. However, modality gaps and limited annotations
present significant challenges. We propose a few-shot metric learning method
with a dual-channel attention mechanism and a pretrained vision transformer to
enable robust cross-modal neuron identification. The local and global channels
extract soma morphology and fiber context, respectively, and a gating mechanism
fuses their outputs. To enhance the model's fine-grained discrimination
capability, we introduce a hard sample mining strategy based on the
MultiSimilarityMiner algorithm, along with the Circle Loss function.
Experiments on two-photon and fMOST datasets demonstrate superior Top-K
accuracy and recall compared to existing methods. Ablation studies and t-SNE
visualizations validate the effectiveness of each module. The method also
achieves a favorable trade-off between accuracy and training efficiency under
different fine-tuning strategies. These results suggest that the proposed
approach offers a promising technical solution for accurate single-cell level
matching and multimodal neuroimaging integration.

</details>

### [158] [Streetscape Analysis with Generative AI (SAGAI): Vision-Language Assessment and Mapping of Urban Scenes](https://arxiv.org/abs/2504.16538)
*Joan Perez,Giovanni Fusco*

Main category: cs.CV

TLDR: SAGAI是一个利用生成式人工智能分析街道景观的模块化工作流，结合开源数据和视觉语言模型，支持可扩展的城市环境分析。


<details>
  <summary>Details</summary>
Motivation: 现有街道景观评估方法局限于形态计量或需要大量人工定性评估，SAGAI旨在提供一种高效、可扩展的替代方案。

Method: SAGAI整合OpenStreetMap、Google街景图像和轻量级LLaVA模型，通过自然语言提示生成结构化空间指标，并支持自动化地图输出。

Result: 案例研究表明，SAGAI在城乡分类、商业特征检测和步行道宽度估计方面表现良好，支持多种城市研究主题。

Conclusion: SAGAI无需特定任务训练或专有软件，通过修改提示即可适应广泛的城市研究需求，具有高度灵活性和实用性。

Abstract: Streetscapes are an essential component of urban space. Their assessment is
presently either limited to morphometric properties of their mass skeleton or
requires labor-intensive qualitative evaluations of visually perceived
qualities. This paper introduces SAGAI: Streetscape Analysis with Generative
Artificial Intelligence, a modular workflow for scoring street-level urban
scenes using open-access data and vision-language models. SAGAI integrates
OpenStreetMap geometries, Google Street View imagery, and a lightweight version
of the LLaVA model to generate structured spatial indicators from images via
customizable natural language prompts. The pipeline includes an automated
mapping module that aggregates visual scores at both the point and street
levels, enabling direct cartographic interpretation. It operates without
task-specific training or proprietary software dependencies, supporting
scalable and interpretable analysis of urban environments. Two exploratory case
studies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial
outputs from vision-language inference. The initial results show strong
performance for binary urban-rural scene classification, moderate precision in
commercial feature detection, and lower estimates, but still informative, of
sidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a
wide range of urban research themes, such as walkability, safety, or urban
design, through prompt modification alone.

</details>

### [159] [ToF-Splatting: Dense SLAM using Sparse Time-of-Flight Depth and Multi-Frame Integration](https://arxiv.org/abs/2504.16545)
*Andrea Conti,Matteo Poggi,Valerio Cambareri,Martin R. Oswald,Stefano Mattoccia*

Main category: cs.CV

TLDR: 提出了一种基于3D高斯分布的SLAM方法ToF-Splatting，用于处理极稀疏的ToF深度数据，通过多帧整合模块生成密集深度图。


<details>
  <summary>Details</summary>
Motivation: 现有低分辨率ToF传感器在移动和AR/VR设备中因功率限制只能提供极稀疏深度数据，限制了其在SLAM中的无缝使用。

Method: 提出ToF-Splatting，结合极稀疏ToF深度、单目彩色和多视角几何信息，通过多帧整合模块生成密集深度图。

Result: 在合成和真实稀疏ToF数据集上的实验表明，该方法在跟踪和建图性能上达到最优。

Conclusion: ToF-Splatting为极稀疏ToF数据的SLAM应用提供了可行且高效的解决方案。

Abstract: Time-of-Flight (ToF) sensors provide efficient active depth sensing at
relatively low power budgets; among such designs, only very sparse measurements
from low-resolution sensors are considered to meet the increasingly limited
power constraints of mobile and AR/VR devices. However, such extreme sparsity
levels limit the seamless usage of ToF depth in SLAM. In this work, we propose
ToF-Splatting, the first 3D Gaussian Splatting-based SLAM pipeline tailored for
using effectively very sparse ToF input data. Our approach improves upon the
state of the art by introducing a multi-frame integration module, which
produces dense depth maps by merging cues from extremely sparse ToF depth,
monocular color, and multi-view geometry. Extensive experiments on both
synthetic and real sparse ToF datasets demonstrate the viability of our
approach, as it achieves state-of-the-art tracking and mapping performances on
reference datasets.

</details>

### [160] [Beyond Anonymization: Object Scrubbing for Privacy-Preserving 2D and 3D Vision Tasks](https://arxiv.org/abs/2504.16557)
*Murat Bilgehan Ertan,Ronak Sahu,Phuong Ha Nguyen,Kaleel Mahmood,Marten van Dijk*

Main category: cs.CV

TLDR: ROAR是一种隐私保护数据集模糊化框架，通过移除而非修改敏感对象，结合实例分割与生成修复技术，在保持场景完整性的同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 解决传统隐私保护方法（如图像丢弃）导致的数据集效用下降问题，提出一种更高效的对象移除框架。

Method: 集成实例分割与生成修复技术，移除可识别实体并保持场景完整性。

Result: 在2D检测中保留87.5%的基线AP（图像丢弃仅74.2%），3D重建中PSNR损失最多1.66 dB，同时提升感知质量。

Conclusion: ROAR证明对象移除是一种高效的隐私保护方法，为未来隐私保护视觉系统奠定基础。

Abstract: We introduce ROAR (Robust Object Removal and Re-annotation), a scalable
framework for privacy-preserving dataset obfuscation that eliminates sensitive
objects instead of modifying them. Our method integrates instance segmentation
with generative inpainting to remove identifiable entities while preserving
scene integrity. Extensive evaluations on 2D COCO-based object detection show
that ROAR achieves 87.5% of the baseline detection average precision (AP),
whereas image dropping achieves only 74.2% of the baseline AP, highlighting the
advantage of scrubbing in preserving dataset utility. The degradation is even
more severe for small objects due to occlusion and loss of fine-grained
details. Furthermore, in NeRF-based 3D reconstruction, our method incurs a PSNR
loss of at most 1.66 dB while maintaining SSIM and improving LPIPS,
demonstrating superior perceptual quality. Our findings establish object
removal as an effective privacy framework, achieving strong privacy guarantees
with minimal performance trade-offs. The results highlight key challenges in
generative inpainting, occlusion-robust segmentation, and task-specific
scrubbing, setting the foundation for future advancements in privacy-preserving
vision systems.

</details>

### [161] [SAIP-Net: Enhancing Remote Sensing Image Segmentation via Spectral Adaptive Information Propagation](https://arxiv.org/abs/2504.16564)
*Zhongtao Wang,Xizhe Cao,Yisong Chen,Guoping Wang*

Main category: cs.CV

TLDR: SAIP-Net通过频域自适应信息传播和增强感受野，显著提升了遥感图像语义分割的性能。


<details>
  <summary>Details</summary>
Motivation: 传统分层模型在遥感图像语义分割中难以处理空间边界和类内一致性问题。

Method: 提出SAIP-Net，结合自适应频率滤波和多尺度感受野增强。

Result: 实验表明其性能优于现有方法。

Conclusion: 频域自适应策略与扩展感受野结合对遥感图像分割有效。

Abstract: Semantic segmentation of remote sensing imagery demands precise spatial
boundaries and robust intra-class consistency, challenging conventional
hierarchical models. To address limitations arising from spatial domain feature
fusion and insufficient receptive fields, this paper introduces SAIP-Net, a
novel frequency-aware segmentation framework that leverages Spectral Adaptive
Information Propagation. SAIP-Net employs adaptive frequency filtering and
multi-scale receptive field enhancement to effectively suppress intra-class
feature inconsistencies and sharpen boundary lines. Comprehensive experiments
demonstrate significant performance improvements over state-of-the-art methods,
highlighting the effectiveness of spectral-adaptive strategies combined with
expanded receptive fields for remote sensing image segmentation.

</details>

### [162] [CountingDINO: A Training-free Pipeline for Class-Agnostic Counting using Unsupervised Backbones](https://arxiv.org/abs/2504.16570)
*Giacomo Pacini,Lorenzo Bianchi,Luca Ciampi,Nicola Messina,Giuseppe Amato,Fabrizio Falchi*

Main category: cs.CV

TLDR: CountingDINO是首个无需训练的类无关计数框架，利用无监督特征提取器，在FSC-147基准测试中表现优于基线，并达到与监督方法竞争的结果。


<details>
  <summary>Details</summary>
Motivation: 当前基于示例的类无关计数方法依赖标注数据训练，限制了可扩展性和泛化能力。CountingDINO旨在解决这一问题。

Method: 使用自监督视觉主干提取对象感知特征，无需标注数据；通过ROI-Align提取潜在对象原型作为卷积核生成相似性图，再转换为密度图。

Result: 在FSC-147基准测试中优于无监督基线，部分情况下甚至优于监督方法和最新监督方法。

Conclusion: 无需训练的类无关计数方法具有可扩展性和竞争力。

Abstract: Class-agnostic counting (CAC) aims to estimate the number of objects in
images without being restricted to predefined categories. However, while
current exemplar-based CAC methods offer flexibility at inference time, they
still rely heavily on labeled data for training, which limits scalability and
generalization to many downstream use cases. In this paper, we introduce
CountingDINO, the first training-free exemplar-based CAC framework that
exploits a fully unsupervised feature extractor. Specifically, our approach
employs self-supervised vision-only backbones to extract object-aware features,
and it eliminates the need for annotated data throughout the entire proposed
pipeline. At inference time, we extract latent object prototypes via ROI-Align
from DINO features and use them as convolutional kernels to generate similarity
maps. These are then transformed into density maps through a simple yet
effective normalization scheme. We evaluate our approach on the FSC-147
benchmark, where we outperform a baseline under the same label-free setting.
Our method also achieves competitive -- and in some cases superior -- results
compared to training-free approaches relying on supervised backbones, as well
as several fully supervised state-of-the-art methods. This demonstrates that
training-free CAC can be both scalable and competitive. Website:
https://lorebianchi98.github.io/CountingDINO/

</details>

### [163] [JEPA for RL: Investigating Joint-Embedding Predictive Architectures for Reinforcement Learning](https://arxiv.org/abs/2504.16591)
*Tristan Kenneweg,Philip Kenneweg,Barbara Hammer*

Main category: cs.CV

TLDR: JEPA架构在自监督学习中表现出色，本文将其应用于强化学习，解决了模型崩溃问题，并在Cart Pole任务中验证了效果。


<details>
  <summary>Details</summary>
Motivation: 探索JEPA架构在强化学习中的潜力，解决模型崩溃问题。

Method: 将JEPA架构应用于强化学习，提出防止模型崩溃的方法。

Result: 在Cart Pole任务中验证了JEPA架构的有效性。

Conclusion: JEPA架构适用于强化学习，并能有效避免模型崩溃。

Abstract: Joint-Embedding Predictive Architectures (JEPA) have recently become popular
as promising architectures for self-supervised learning. Vision transformers
have been trained using JEPA to produce embeddings from images and videos,
which have been shown to be highly suitable for downstream tasks like
classification and segmentation. In this paper, we show how to adapt the JEPA
architecture to reinforcement learning from images. We discuss model collapse,
show how to prevent it, and provide exemplary data on the classical Cart Pole
task.

</details>

### [164] [Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections](https://arxiv.org/abs/2504.16612)
*Max Kirchner,Alexander C. Jenke,Sebastian Bodenstedt,Fiona R. Kolbinger,Oliver Saldanha,Jakob N. Kather,Martin Wagner,Stefanie Speidel*

Main category: cs.CV

TLDR: 研究通过联邦学习训练基础模型，解决数据共享限制，实现无需数据传输的协作模型训练，应用于微创手术。


<details>
  <summary>Details</summary>
Motivation: 解决数据共享限制，实现隐私保护的协作模型训练。

Method: 基于EndoViT研究，改进Masked Autoencoder，结合自适应FedSAM和SWA，预训练于Endo700k数据集，并在下游任务中微调。

Result: FedSAM改进预训练效果，降低重建损失；FL-EndoViT在数据有限时优于CEN-EndoViT，尤其在大数据集的动作识别中表现更佳。

Conclusion: 联邦学习为手术基础模型提供隐私保护的训练方案，未来可探索视频模型以增强时空动态能力。

Abstract: Purpose: In this study, we investigate the training of foundation models
using federated learning to address data-sharing limitations and enable
collaborative model training without data transfer for minimally invasive
surgery. Methods: Inspired by the EndoViT study, we adapt the Masked
Autoencoder for federated learning, enhancing it with adaptive Sharpness-Aware
Minimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is
pretrained on the Endo700k dataset collection and later fine-tuned and
evaluated for tasks such as Semantic Segmentation, Action Triplet Recognition,
and Surgical Phase Recognition. Results: Our findings demonstrate that
integrating adaptive FedSAM into the federated MAE approach improves
pretraining, leading to a reduction in reconstruction loss per patch. The
application of FL-EndoViT in surgical downstream tasks results in performance
comparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over
CEN-EndoViT in surgical scene segmentation when data is limited and in action
triplet recognition when large datasets are used. Conclusion: These findings
highlight the potential of federated learning for privacy-preserving training
of surgical foundation models, offering a robust and generalizable solution for
surgical data science. Effective collaboration requires adapting federated
learning methods, such as the integration of FedSAM, which can accommodate the
inherent data heterogeneity across institutions. In future, exploring FL in
video-based models may enhance these capabilities by incorporating
spatiotemporal dynamics crucial for real-world surgical environments.

</details>

### [165] [EHGCN: Hierarchical Euclidean-Hyperbolic Fusion via Motion-Aware GCN for Hybrid Event Stream Perception](https://arxiv.org/abs/2504.16616)
*Haosheng Chen,Lian Luo,Mengjingcheng Mo,Zhanjie Wu,Guobao Xiao,Ji Gan,Jiaxu Leng,Xinbo Gao*

Main category: cs.CV

TLDR: 提出EHGCN方法，结合欧几里得和双曲空间处理事件流，通过自适应采样和马尔可夫向量场驱动的超边生成，提升事件感知性能。


<details>
  <summary>Details</summary>
Motivation: 传统GNN方法在欧几里得空间中难以捕捉事件流的长程依赖和层次结构，需改进。

Method: 1. 自适应采样策略；2. 马尔可夫向量场驱动的超边生成；3. 欧几里得-双曲GCN融合信息。

Result: 在事件感知任务（如目标检测和识别）中验证了方法的有效性。

Conclusion: EHGCN通过混合空间建模，显著提升了事件流的感知能力。

Abstract: Event cameras, with microsecond temporal resolution and high dynamic range
(HDR) characteristics, emit high-speed event stream for perception tasks.
Despite the recent advancement in GNN-based perception methods, they are prone
to use straightforward pairwise connectivity mechanisms in the pure Euclidean
space where they struggle to capture long-range dependencies and fail to
effectively characterize the inherent hierarchical structures of non-uniformly
distributed event stream. To this end, in this paper we propose a novel
approach named EHGCN, which is a pioneer to perceive event stream in both
Euclidean and hyperbolic spaces for event vision. In EHGCN, we introduce an
adaptive sampling strategy to dynamically regulate sampling rates, retaining
discriminative events while attenuating chaotic noise. Then we present a Markov
Vector Field (MVF)-driven motion-aware hyperedge generation method based on
motion state transition probabilities, thereby eliminating cross-target
spurious associations and providing critically topological priors while
capturing long-range dependencies between events. Finally, we propose a
Euclidean-Hyperbolic GCN to fuse the information locally aggregated and
globally hierarchically modeled in Euclidean and hyperbolic spaces,
respectively, to achieve hybrid event perception. Experimental results on event
perception tasks such as object detection and recognition validate the
effectiveness of our approach.

</details>

### [166] [Dual-Camera All-in-Focus Neural Radiance Fields](https://arxiv.org/abs/2504.16636)
*Xianrui Luo,Zijin Wu,Juewen Peng,Huiqiang Sun,Zhiguo Cao,Guosheng Lin*

Main category: cs.CV

TLDR: 提出首个无需手动对焦的全焦点NeRF合成框架，利用智能手机双摄像头（主摄和超广角）实现高质量全焦点视图生成。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法因固定对焦导致模糊和缺乏清晰参考，无法生成全焦点视图。

Method: 利用双摄像头（主摄高分辨率、超广角大景深），通过空间对齐、颜色匹配和可学习参数的去焦融合模块实现全焦点恢复。

Result: 在自建数据集上验证，DC-NeRF能生成高质量全焦点新视图，优于基线方法。

Conclusion: DC-NeRF支持景深调整应用（如重新对焦、分光镜效果），展示了其多功能性。

Abstract: We present the first framework capable of synthesizing the all-in-focus
neural radiance field (NeRF) from inputs without manual refocusing. Without
refocusing, the camera will automatically focus on the fixed object for all
views, and current NeRF methods typically using one camera fail due to the
consistent defocus blur and a lack of sharp reference. To restore the
all-in-focus NeRF, we introduce the dual-camera from smartphones, where the
ultra-wide camera has a wider depth-of-field (DoF) and the main camera
possesses a higher resolution. The dual camera pair saves the high-fidelity
details from the main camera and uses the ultra-wide camera's deep DoF as
reference for all-in-focus restoration. To this end, we first implement spatial
warping and color matching to align the dual camera, followed by a
defocus-aware fusion module with learnable defocus parameters to predict a
defocus map and fuse the aligned camera pair. We also build a multi-view
dataset that includes image pairs of the main and ultra-wide cameras in a
smartphone. Extensive experiments on this dataset verify that our solution,
termed DC-NeRF, can produce high-quality all-in-focus novel views and compares
favorably against strong baselines quantitatively and qualitatively. We further
show DoF applications of DC-NeRF with adjustable blur intensity and focal
plane, including refocusing and split diopter.

</details>

### [167] [RouteWinFormer: A Route-Window Transformer for Middle-range Attention in Image Restoration](https://arxiv.org/abs/2504.16637)
*Qifan Li,Tianyi Liang,Xingtao Wang,Xiaopeng Fan*

Main category: cs.CV

TLDR: RouteWinFormer是一种基于窗口的Transformer，通过动态选择附近窗口进行注意力聚合，高效扩展感受野，适用于图像修复任务。


<details>
  <summary>Details</summary>
Motivation: Transformer在图像修复中因长距离像素依赖而受关注，但长距离注意力计算开销大且不必要，因为退化和上下文通常是局部的。

Method: 提出RouteWinFormer，包含Route-Windows Attention Module动态选择附近窗口进行注意力聚合，并引入Multi-Scale Structure Regularization训练方法。

Result: 在9个数据集上的实验表明，RouteWinFormer在多种图像修复任务中优于现有方法。

Conclusion: RouteWinFormer通过中距离注意力建模和结构正则化，高效提升了图像修复性能。

Abstract: Transformer models have recently garnered significant attention in image
restoration due to their ability to capture long-range pixel dependencies.
However, long-range attention often results in computational overhead without
practical necessity, as degradation and context are typically localized.
Normalized average attention distance across various degradation datasets shows
that middle-range attention is enough for image restoration. Building on this
insight, we propose RouteWinFormer, a novel window-based Transformer that
models middle-range context for image restoration. RouteWinFormer incorporates
Route-Windows Attnetion Module, which dynamically selects relevant nearby
windows based on regional similarity for attention aggregation, extending the
receptive field to a mid-range size efficiently. In addition, we introduce
Multi-Scale Structure Regularization during training, enabling the sub-scale of
the U-shaped network to focus on structural information, while the
original-scale learns degradation patterns based on generalized image structure
priors. Extensive experiments demonstrate that RouteWinFormer outperforms
state-of-the-art methods across 9 datasets in various image restoration tasks.

</details>

### [168] [SSLR: A Semi-Supervised Learning Method for Isolated Sign Language Recognition](https://arxiv.org/abs/2504.16640)
*Hasan Algafri,Hamzah Luqman,Sarah Alyami,Issam Laradji*

Main category: cs.CV

TLDR: 提出了一种基于半监督学习的手语识别方法（SSLR），通过伪标签方法标注未标记样本，利用姿态信息表示手语动作，并在Transformer模型上验证其性能。


<details>
  <summary>Details</summary>
Motivation: 解决手语识别中标注数据稀缺的问题。

Method: 采用半监督学习方法，使用伪标签标注未标记样本，以姿态信息作为输入，基于Transformer模型进行训练。

Result: 在WLASL-100数据集上，半监督学习方法在标注数据较少的情况下表现优于全监督学习方法。

Conclusion: 半监督学习方法在手语识别中具有潜力，尤其是在标注数据有限的情况下。

Abstract: Sign language is the primary communication language for people with disabling
hearing loss. Sign language recognition (SLR) systems aim to recognize sign
gestures and translate them into spoken language. One of the main challenges in
SLR is the scarcity of annotated datasets. To address this issue, we propose a
semi-supervised learning (SSL) approach for SLR (SSLR), employing a
pseudo-label method to annotate unlabeled samples. The sign gestures are
represented using pose information that encodes the signer's skeletal joint
points. This information is used as input for the Transformer backbone model
utilized in the proposed approach. To demonstrate the learning capabilities of
SSL across various labeled data sizes, several experiments were conducted using
different percentages of labeled data with varying numbers of classes. The
performance of the SSL approach was compared with a fully supervised
learning-based model on the WLASL-100 dataset. The obtained results of the SSL
model outperformed the supervised learning-based model with less labeled data
in many cases.

</details>

### [169] [WiFi based Human Fall and Activity Recognition using Transformer based Encoder Decoder and Graph Neural Networks](https://arxiv.org/abs/2504.16655)
*Younggeol Cho,Elisa Motta,Olivia Nocentini,Marta Lagomarsino,Andrea Merello,Marco Crepaldi,Arash Ajoudani*

Main category: cs.CV

TLDR: 提出了一种基于Transformer的编码器-解码器网络（TED Net），用于从WiFi信道状态信息（CSI）估计人体骨骼姿态，并结合定向图神经网络（DGNN）进行动作识别。


<details>
  <summary>Details</summary>
Motivation: 解决医疗监测、康复和辅助技术中的人体姿态估计和动作识别问题，同时提供一种隐私保护的替代方案，避免基于视觉的方法的监控问题。

Method: TED Net结合卷积编码器和基于Transformer的注意力机制，从CSI信号中提取时空特征；DGNN用于动作识别。

Result: TED Net在姿态估计上优于现有方法，DGNN的动作分类性能与基于RGB的系统相当，且在跌倒和非跌倒场景中表现稳健。

Conclusion: 基于CSI的人体骨骼估计在动作识别（如老年人跌倒检测）中具有潜力，WiFi信号提供了一种隐私友好的替代方案。

Abstract: Human pose estimation and action recognition have received attention due to
their critical roles in healthcare monitoring, rehabilitation, and assistive
technologies. In this study, we proposed a novel architecture named Transformer
based Encoder Decoder Network (TED Net) designed for estimating human skeleton
poses from WiFi Channel State Information (CSI). TED Net integrates
convolutional encoders with transformer based attention mechanisms to capture
spatiotemporal features from CSI signals. The estimated skeleton poses were
used as input to a customized Directed Graph Neural Network (DGNN) for action
recognition. We validated our model on two datasets: a publicly available multi
modal dataset for assessing general pose estimation, and a newly collected
dataset focused on fall related scenarios involving 20 participants.
Experimental results demonstrated that TED Net outperformed existing approaches
in pose estimation, and that the DGNN achieves reliable action classification
using CSI based skeletons, with performance comparable to RGB based systems.
Notably, TED Net maintains robust performance across both fall and non fall
cases. These findings highlight the potential of CSI driven human skeleton
estimation for effective action recognition, particularly in home environments
such as elderly fall detection. In such settings, WiFi signals are often
readily available, offering a privacy preserving alternative to vision based
methods, which may raise concerns about continuous camera monitoring.

</details>

### [170] [Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning](https://arxiv.org/abs/2504.16656)
*Chris,Yichen Wei,Yi Peng,Xiaokun Wang,Weijie Qiu,Wei Shen,Tianyidan Xie,Jiangbo Pei,Jianhao Zhang,Yunzhuo Hao,Xuchen Song,Yang Liu,Yahui Zhou*

Main category: cs.CV

TLDR: Skywork R1V2是一种多模态推理模型，通过混合强化学习范式提升推理能力和泛化性，并引入选择性样本缓冲机制优化训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统模型在复杂推理与广泛泛化之间的平衡问题，并应对训练中的‘优势消失’困境。

Method: 采用混合强化学习范式结合奖励模型指导和规则策略，并设计选择性样本缓冲（SSB）机制优化训练。

Result: 在多个基准测试中表现优异（如OlympiadBench 62.6，AIME2024 79.0），超越开源模型并接近专有系统性能。

Conclusion: Skywork R1V2在推理和泛化能力上取得显著进步，模型权重已公开以促进开放性和可复现性。

Abstract: We present Skywork R1V2, a next-generation multimodal reasoning model and a
major leap forward from its predecessor, Skywork R1V. At its core, R1V2
introduces a hybrid reinforcement learning paradigm that harmonizes
reward-model guidance with rule-based strategies, thereby addressing the
long-standing challenge of balancing sophisticated reasoning capabilities with
broad generalization. To further enhance training efficiency, we propose the
Selective Sample Buffer (SSB) mechanism, which effectively counters the
``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization
(GRPO) by prioritizing high-value samples throughout the optimization process.
Notably, we observe that excessive reinforcement signals can induce visual
hallucinations--a phenomenon we systematically monitor and mitigate through
calibrated reward thresholds throughout the training process. Empirical results
affirm the exceptional capability of R1V2, with benchmark-leading performances
such as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and
74.0 on MMMU. These results underscore R1V2's superiority over existing
open-source models and demonstrate significant progress in closing the
performance gap with premier proprietary systems, including Gemini 2.5 and
OpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to
promote openness and reproducibility
https://huggingface.co/Skywork/Skywork-R1V2-38B.

</details>

### [171] [A Time Series Dataset of NIR Spectra and RGB and NIR-HSI Images of the Barley Germination Process](https://arxiv.org/abs/2504.16658)
*Ole-Christian Galbo Engstrøm,Erik Schou Dreier,Birthe Møller Jespersen,Kim Steenstrup Pedersen*

Main category: cs.CV

TLDR: 开源数据集包含2242个大麦籽粒的RGB和近红外高光谱图像，附带分割掩码和近红外光谱，用于分析发芽时间。


<details>
  <summary>Details</summary>
Motivation: 提供高质量数据集，支持基于RGB、近红外光谱或高光谱图像的大麦籽粒发芽时间分析。

Method: 采集大麦籽粒在湿度暴露前及连续五天每24小时的图像，标注发芽状态，使用黑色滤纸背景简化分割。

Result: 数据集支持多种分析方法（RGB、NIR光谱、NIR-HSI或其组合）的时间序列分析。

Conclusion: 该数据集为研究大麦籽粒发芽提供了多模态分析工具。

Abstract: We provide an open-source dataset of RGB and NIR-HSI (near-infrared
hyperspectral imaging) images with associated segmentation masks and NIR
spectra of 2242 individual malting barley kernels. We imaged every kernel
pre-exposure to moisture and every 24 hours after exposure to moisture for five
consecutive days. Every barley kernel was labeled as germinated or not
germinated during each image acquisition. The barley kernels were imaged with
black filter paper as the background, facilitating straight-forward intensity
threshold-based segmentation, e.g., by Otsu's method. This dataset facilitates
time series analysis of germination time for barley kernels using either RGB
image analysis, NIR spectral analysis, NIR-HSI analysis, or a combination
hereof.

</details>

### [172] [A Diff-Attention Aware State Space Fusion Model for Remote Sensing Classification](https://arxiv.org/abs/2504.16665)
*Wenping Ma,Boyou Xue,Mengru Ma,Chuang Chen,Hekai Zhang,Hao Zhu*

Main category: cs.CV

TLDR: 论文提出了一种基于选择性状态空间模型的DAS2F-Model，用于多模态遥感图像分类，通过CMDA-Module分离MS和PAN图像的共有特征和优势特征，并利用AALF-Module实现语义差异大的特征融合。


<details>
  <summary>Details</summary>
Motivation: MS和PAN图像具有相似信息和各自优势，但现有方法在融合阶段存在特征冗余问题，需分离共有特征和优势特征以减少冗余。

Method: 设计了CMDA-Module提取和分离MS和PAN图像的共有及优势特征，SPVM保留空间特征，AALF-Module通过像素级线性融合处理语义差异大的特征。

Result: 实验表明，该方法优于其他替代方法。

Conclusion: DAS2F-Model有效解决了多模态遥感图像分类中的特征冗余和语义差异问题，提升了分类效果。

Abstract: Multispectral (MS) and panchromatic (PAN) images describe the same land
surface, so these images not only have their own advantages, but also have a
lot of similar information. In order to separate these similar information and
their respective advantages, reduce the feature redundancy in the fusion stage.
This paper introduces a diff-attention aware state space fusion model
(DAS2F-Model) for multimodal remote sensing image classification. Based on the
selective state space model, a cross-modal diff-attention module (CMDA-Module)
is designed to extract and separate the common features and their respective
dominant features of MS and PAN images. Among this, space preserving visual
mamba (SPVM) retains image spatial features and captures local features by
optimizing visual mamba's input reasonably. Considering that features in the
fusion stage will have large semantic differences after feature separation and
simple fusion operations struggle to effectively integrate these significantly
different features, an attention-aware linear fusion module (AALF-Module) is
proposed. It performs pixel-wise linear fusion by calculating influence
coefficients. This mechanism can fuse features with large semantic differences
while keeping the feature size unchanged. Empirical evaluations indicate that
the presented method achieves better results than alternative approaches. The
relevant code can be found at:https://github.com/AVKSKVL/DAS-F-Model

</details>

### [173] [SemanticSugarBeets: A Multi-Task Framework and Dataset for Inspecting Harvest and Storage Characteristics of Sugar Beets](https://arxiv.org/abs/2504.16684)
*Gerardus Croonen,Andreas Trondl,Julia Simon,Daniel Steininger*

Main category: cs.CV

TLDR: 提出了一种用于糖用甜菜检测、语义分割和质量估计的两阶段方法，并创建了一个高质量标注数据集。


<details>
  <summary>Details</summary>
Motivation: 糖用甜菜在储存过程中因微生物等因素导致糖分损失，自动化视觉检测可提升质量保证和生产效率。

Method: 使用单目RGB图像，通过两阶段方法进行检测、语义分割和质量估计，评估了不同图像尺寸、模型架构和环境条件的影响。

Result: 最佳检测模型的mAP50-95为98.8，最佳分割模型的mIoU为64.0。

Conclusion: 该方法在糖用甜菜的自动化检测和质量评估中表现出色。

Abstract: While sugar beets are stored prior to processing, they lose sugar due to
factors such as microorganisms present in adherent soil and excess vegetation.
Their automated visual inspection promises to aide in quality assurance and
thereby increase efficiency throughout the processing chain of sugar
production. In this work, we present a novel high-quality annotated dataset and
two-stage method for the detection, semantic segmentation and mass estimation
of post-harvest and post-storage sugar beets in monocular RGB images. We
conduct extensive ablation experiments for the detection of sugar beets and
their fine-grained semantic segmentation regarding damages, rot, soil adhesion
and excess vegetation. For these tasks, we evaluate multiple image sizes, model
architectures and encoders, as well as the influence of environmental
conditions. Our experiments show an mAP50-95 of 98.8 for sugar-beet detection
and an mIoU of 64.0 for the best-performing segmentation model.

</details>

### [174] [Energy-Based Pseudo-Label Refining for Source-free Domain Adaptation](https://arxiv.org/abs/2504.16692)
*Xinru Meng,Han Sun,Jiamei Liu,Ningzhong Liu,Huiyu Zhou*

Main category: cs.CV

TLDR: 提出了一种基于能量的伪标签细化方法（EBPR），用于解决无源域自适应（SFDA）中的噪声问题，并通过对比学习策略提升特征判别性。


<details>
  <summary>Details</summary>
Motivation: 无源域自适应（SFDA）因无法访问源数据而具有挑战性，现有方法依赖置信度生成的伪标签易引入噪声，导致负迁移。

Method: 提出EBPR方法，基于能量分数生成伪标签，并通过全局和类别能量阈值筛选伪标签；引入对比学习策略对齐困难样本及其增强版本。

Result: 在Office-31、Office-Home和VisDA-C数据集上验证，模型性能优于现有方法。

Conclusion: EBPR通过能量阈值和对比学习有效减少噪声，提升无源域自适应的性能。

Abstract: Source-free domain adaptation (SFDA), which involves adapting models without
access to source data, is both demanding and challenging. Existing SFDA
techniques typically rely on pseudo-labels generated from confidence levels,
leading to negative transfer due to significant noise. To tackle this problem,
Energy-Based Pseudo-Label Refining (EBPR) is proposed for SFDA. Pseudo-labels
are created for all sample clusters according to their energy scores. Global
and class energy thresholds are computed to selectively filter pseudo-labels.
Furthermore, a contrastive learning strategy is introduced to filter difficult
samples, aligning them with their augmented versions to learn more
discriminative features. Our method is validated on the Office-31, Office-Home,
and VisDA-C datasets, consistently finding that our model outperformed
state-of-the-art methods.

</details>

### [175] [PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum Learning](https://arxiv.org/abs/2504.16722)
*Yingjie Xi,Jian Jun Zhang,Xiaosong Yang*

Main category: cs.CV

TLDR: ProMoGen提出了一种结合轨迹引导和稀疏锚点控制的框架，用于生成更可控和精确的人体运动，并通过SAP-CL策略优化学习稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成复杂或定制化人体运动时存在局限性，如文本方法语义模糊、轨迹方法精度不足、锚点方法仅支持简单模式。

Method: ProMoGen框架通过解耦轨迹和锚点控制，支持双/单控制范式；SAP-CL策略通过渐进调整锚点数量优化学习稳定性。

Result: 实验表明，ProMoGen能生成生动多样的运动，显著优于现有方法。

Conclusion: ProMoGen通过结合轨迹和锚点控制，实现了高保真、可控的运动合成，为复杂运动生成提供了新思路。

Abstract: In computer animation, game design, and human-computer interaction,
synthesizing human motion that aligns with user intent remains a significant
challenge. Existing methods have notable limitations: textual approaches offer
high-level semantic guidance but struggle to describe complex actions
accurately; trajectory-based techniques provide intuitive global motion
direction yet often fall short in generating precise or customized character
movements; and anchor poses-guided methods are typically confined to synthesize
only simple motion patterns. To generate more controllable and precise human
motions, we propose \textbf{ProMoGen (Progressive Motion Generation)}, a novel
framework that integrates trajectory guidance with sparse anchor motion
control. Global trajectories ensure consistency in spatial direction and
displacement, while sparse anchor motions only deliver precise action guidance
without displacement. This decoupling enables independent refinement of both
aspects, resulting in a more controllable, high-fidelity, and sophisticated
motion synthesis. ProMoGen supports both dual and single control paradigms
within a unified training process. Moreover, we recognize that direct learning
from sparse motions is inherently unstable, we introduce \textbf{SAP-CL (Sparse
Anchor Posture Curriculum Learning)}, a curriculum learning strategy that
progressively adjusts the number of anchors used for guidance, thereby enabling
more precise and stable convergence. Extensive experiments demonstrate that
ProMoGen excels in synthesizing vivid and diverse motions guided by predefined
trajectory and arbitrary anchor frames. Our approach seamlessly integrates
personalized motion with structured guidance, significantly outperforming
state-of-the-art methods across multiple control scenarios.

</details>

### [176] [Detecting and Understanding Hateful Contents in Memes Through Captioning and Visual Question-Answering](https://arxiv.org/abs/2504.16723)
*Ali Anaissi,Junaid Akram,Kunal Chaturvedi,Ali Braytee*

Main category: cs.CV

TLDR: 提出了一种多模态仇恨内容检测框架，结合OCR、字幕生成、子标签分类、RAG和VQA技术，显著优于单模态和传统多模态模型。


<details>
  <summary>Details</summary>
Motivation: 由于表情包的多模态特性，传统单模态检测系统难以识别隐含的仇恨内容，需要更复杂的解决方案。

Method: 整合OCR提取文本、字幕生成描述图像、子标签分类细化仇恨内容、RAG检索上下文、VQA分析符号线索。

Result: 在Facebook Hateful Memes数据集上，框架在准确率和AUC-ROC上优于其他模型。

Conclusion: 多模态框架能有效检测隐含的仇恨内容，为复杂内容分析提供了新思路。

Abstract: Memes are widely used for humor and cultural commentary, but they are
increasingly exploited to spread hateful content. Due to their multimodal
nature, hateful memes often evade traditional text-only or image-only detection
systems, particularly when they employ subtle or coded references. To address
these challenges, we propose a multimodal hate detection framework that
integrates key components: OCR to extract embedded text, captioning to describe
visual content neutrally, sub-label classification for granular categorization
of hateful content, RAG for contextually relevant retrieval, and VQA for
iterative analysis of symbolic and contextual cues. This enables the framework
to uncover latent signals that simpler pipelines fail to detect. Experimental
results on the Facebook Hateful Memes dataset reveal that the proposed
framework exceeds the performance of unimodal and conventional multimodal
models in both accuracy and AUC-ROC.

</details>

### [177] [V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations](https://arxiv.org/abs/2504.16727)
*Zhiyuan Fan,Yumeng Wang,Sandeep Polisetty,Yi R.,Fung*

Main category: cs.CV

TLDR: LVLMs在视觉语言任务中表现出色，但对视觉变化的鲁棒性研究不足。V²R-Bench框架评估了21种LVLMs，发现其存在视觉位置偏差和人类视觉敏锐度阈值，揭示了架构缺陷和多模态对齐不足的问题。


<details>
  <summary>Details</summary>
Motivation: 研究LVLMs对视觉变化（如位置、尺度、方向）的鲁棒性，填补现有研究的空白。

Method: 提出V²R-Bench框架，包括自动化数据集生成和评估指标，对21种LVLMs进行系统评估和组件级分析。

Result: 发现LVLMs对视觉变化脆弱，存在视觉位置偏差和人类视觉敏锐度阈值，问题源于架构缺陷和多模态对齐不足。

Conclusion: LVLMs的鲁棒性问题需要架构创新，未来设计需解决多模态对齐和错误累积问题。

Abstract: Large Vision Language Models (LVLMs) excel in various vision-language tasks.
Yet, their robustness to visual variations in position, scale, orientation, and
context that objects in natural scenes inevitably exhibit due to changes in
viewpoint and environment remains largely underexplored. To bridge this gap, we
introduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating
Visual Variation Robustness of LVLMs, which encompasses automated evaluation
dataset generation and principled metrics for thorough robustness assessment.
Through extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability
to visual variations, in which even advanced models that excel at complex
vision-language tasks significantly underperform on simple tasks such as object
recognition. Interestingly, these models exhibit a distinct visual position
bias that contradicts theories of effective receptive fields, and demonstrate a
human-like visual acuity threshold. To identify the source of these
vulnerabilities, we present a systematic framework for component-level
analysis, featuring a novel visualization approach for aligned visual features.
Results show that these vulnerabilities stem from error accumulation in the
pipeline architecture and inadequate multimodal alignment. Complementary
experiments with synthetic data further demonstrate that these limitations are
fundamentally architectural deficiencies, scoring the need for architectural
innovations in future LVLM designs.

</details>

### [178] [Prompt-Tuning SAM: From Generalist to Specialist with only 2048 Parameters and 16 Training Images](https://arxiv.org/abs/2504.16739)
*Tristan Piater,Björn Barz,Alexander Freytag*

Main category: cs.CV

TLDR: PTSAM通过仅调整少量参数（2048个）优化SAM模型，使其在非自然图像领域（如显微图像）表现更优，且仅需少量标注数据（16张）。


<details>
  <summary>Details</summary>
Motivation: SAM在自然图像中表现优异，但在非自然领域（如显微图像）性能下降，且需要精确提示，不适合自动化生物医学应用。

Method: 提出PTSAM方法，采用参数高效的提示调优技术，仅调整SAM的掩码解码器和图像编码器。

Result: PTSAM在性能上与现有技术相当，但训练参数减少2000倍；调整图像编码器后，分割准确率提升18%。

Conclusion: PTSAM适用于训练数据有限和领域迁移的场景，显著提升了SAM在非自然图像中的表现。

Abstract: The Segment Anything Model (SAM) is widely used for segmenting a diverse
range of objects in natural images from simple user prompts like points or
bounding boxes. However, SAM's performance decreases substantially when applied
to non-natural domains like microscopic imaging. Furthermore, due to SAM's
interactive design, it requires a precise prompt for each image and object,
which is unfeasible in many automated biomedical applications. Previous
solutions adapt SAM by training millions of parameters via fine-tuning large
parts of the model or of adapter layers. In contrast, we show that as little as
2,048 additional parameters are sufficient for turning SAM into a use-case
specialist for a certain downstream task. Our novel PTSAM (prompt-tuned SAM)
method uses prompt-tuning, a parameter-efficient fine-tuning technique, to
adapt SAM for a specific task. We validate the performance of our approach on
multiple microscopic and one medical dataset. Our results show that
prompt-tuning only SAM's mask decoder already leads to a performance on-par
with state-of-the-art techniques while requiring roughly 2,000x less trainable
parameters. For addressing domain gaps, we find that additionally prompt-tuning
SAM's image encoder is beneficial, further improving segmentation accuracy by
up to 18% over state-of-the-art results. Since PTSAM can be reliably trained
with as little as 16 annotated images, we find it particularly helpful for
applications with limited training data and domain shifts.

</details>

### [179] [Gaussian Splatting is an Effective Data Generator for 3D Object Detection](https://arxiv.org/abs/2504.16740)
*Farhad G. Zanjani,Davide Abati,Auke Wiggers,Dimitris Kalatzis,Jens Petersen,Hong Cai,Amirhossein Habibian*

Main category: cs.CV

TLDR: 该论文研究了基于高斯泼溅的3D重建技术用于自动驾驶场景中的3D物体检测数据增强，通过直接放置3D物体并施加几何变换，提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的方法在BEV布局下合成图像，缺乏物理合理性。本文旨在通过直接放置3D物体并施加几何变换，确保物体放置的物理合理性和高精度标注。

Method: 利用高斯泼溅的3D重建技术，直接在3D空间中放置物体并施加几何变换，确保物理合理性和高精度标注。

Result: 实验表明，该方法显著提升了3D物体检测性能，优于现有扩散方法。几何多样性对性能提升的影响大于外观多样性。

Conclusion: 直接放置3D物体并施加几何变换是一种高效的数据增强方法，几何多样性是关键因素，而生成困难样本对性能提升效果有限。

Abstract: We investigate data augmentation for 3D object detection in autonomous
driving. We utilize recent advancements in 3D reconstruction based on Gaussian
Splatting for 3D object placement in driving scenes. Unlike existing
diffusion-based methods that synthesize images conditioned on BEV layouts, our
approach places 3D objects directly in the reconstructed 3D space with
explicitly imposed geometric transformations. This ensures both the physical
plausibility of object placement and highly accurate 3D pose and position
annotations.
  Our experiments demonstrate that even by integrating a limited number of
external 3D objects into real scenes, the augmented data significantly enhances
3D object detection performance and outperforms existing diffusion-based 3D
augmentation for object detection. Extensive testing on the nuScenes dataset
reveals that imposing high geometric diversity in object placement has a
greater impact compared to the appearance diversity of objects. Additionally,
we show that generating hard examples, either by maximizing detection loss or
imposing high visual occlusion in camera images, does not lead to more
efficient 3D data augmentation for camera-based 3D object detection in
autonomous driving.

</details>

### [180] [Feature Mixing Approach for Detecting Intraoperative Adverse Events in Laparoscopic Roux-en-Y Gastric Bypass Surgery](https://arxiv.org/abs/2504.16749)
*Rupak Bose,Chinedu Innocent Nwoye,Jorge Lazo,Joël Lukas Lavanchy,Nicolas Padoy*

Main category: cs.CV

TLDR: BetaMixer是一种新型深度学习模型，通过基于Beta分布的混合方法解决术中不良事件（IAEs）检测和严重性量化的数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 术中不良事件（IAEs）如出血或热损伤若未被检测到可能导致严重的术后并发症，但其罕见性导致数据集高度不平衡，为AI检测和量化带来挑战。

Method: BetaMixer采用基于Beta分布的采样增强少数类，并通过生成方法对齐特征空间与IAE严重性，使用Transformer进行分类和回归。

Result: 在MultiBypass140数据集上，BetaMixer加权F1得分为0.76，召回率为0.81，PPV为0.73，NPV为0.84，表现优异。

Conclusion: BetaMixer结合Beta分布采样、特征混合和生成建模，为临床环境中IAE检测和量化提供了稳健解决方案。

Abstract: Intraoperative adverse events (IAEs), such as bleeding or thermal injury, can
lead to severe postoperative complications if undetected. However, their rarity
results in highly imbalanced datasets, posing challenges for AI-based detection
and severity quantification. We propose BetaMixer, a novel deep learning model
that addresses these challenges through a Beta distribution-based mixing
approach, converting discrete IAE severity scores into continuous values for
precise severity regression (0-5 scale). BetaMixer employs Beta
distribution-based sampling to enhance underrepresented classes and regularizes
intermediate embeddings to maintain a structured feature space. A generative
approach aligns the feature space with sampled IAE severity, enabling robust
classification and severity regression via a transformer. Evaluated on the
MultiBypass140 dataset, which we extended with IAE labels, BetaMixer achieves a
weighted F1 score of 0.76, recall of 0.81, PPV of 0.73, and NPV of 0.84,
demonstrating strong performance on imbalanced data. By integrating Beta
distribution-based sampling, feature mixing, and generative modeling, BetaMixer
offers a robust solution for IAE detection and quantification in clinical
settings.

</details>

### [181] [Tri-FusionNet: Enhancing Image Description Generation with Transformer-based Fusion Network and Dual Attention Mechanism](https://arxiv.org/abs/2504.16761)
*Lakshita Agarwal,Bindu Verma*

Main category: cs.CV

TLDR: Tri-FusionNet是一种结合ViT、RoBERTa和CLIP的图像描述生成模型，通过双注意力机制和对比学习提升描述质量，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 图像描述生成对可访问性和AI视觉内容理解至关重要，但现有方法在准确性和上下文丰富性上仍有提升空间。

Method: 提出Tri-FusionNet模型，整合ViT编码器（双注意力）、RoBERTa解码器和CLIP对比学习模块，实现视觉与文本数据的高效对齐。

Result: 在Flickr30k、Flickr8k和MS-COCO数据集上取得高BLEU、CIDEr、METEOR和ROUGE-L分数，证明模型有效性。

Conclusion: Tri-FusionNet能生成更准确、上下文丰富的图像描述，为相关领域提供有力工具。

Abstract: Image description generation is essential for accessibility and AI
understanding of visual content. Recent advancements in deep learning have
significantly improved natural language processing and computer vision. In this
work, we propose Tri-FusionNet, a novel image description generation model that
integrates transformer modules: a Vision Transformer (ViT) encoder module with
dual-attention mechanism, a Robustly Optimized BERT Approach (RoBERTa) decoder
module, and a Contrastive Language-Image Pre-Training (CLIP) integrating
module. The ViT encoder, enhanced with dual attention, focuses on relevant
spatial regions and linguistic context, improving image feature extraction. The
RoBERTa decoder is employed to generate precise textual descriptions. CLIP's
integrating module aligns visual and textual data through contrastive learning,
ensuring effective combination of both modalities. This fusion of ViT, RoBERTa,
and CLIP, along with dual attention, enables the model to produce more
accurate, contextually rich, and flexible descriptions. The proposed framework
demonstrated competitive performance on the Flickr30k and Flickr8k datasets,
with BLEU scores ranging from 0.767 to 0.456 and 0.784 to 0.479, CIDEr scores
of 1.679 and 1.483, METEOR scores of 0.478 and 0.358, and ROUGE-L scores of
0.567 and 0.789, respectively. On MS-COCO, the framework obtained BLEU scores
of 0.893 (B-1), 0.821 (B-2), 0.794 (B-3), and 0.725 (B-4). The results
demonstrate the effectiveness of Tri-FusionNet in generating high-quality image
descriptions.

</details>

### [182] [Towards Explainable AI: Multi-Modal Transformer for Video-based Image Description Generation](https://arxiv.org/abs/2504.16788)
*Lakshita Agarwal,Bindu Verma*

Main category: cs.CV

TLDR: 论文提出了一种结合视觉和文本模态的新框架，用于从视频数据生成自然语言描述，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 视频动作的理解与分析对于智能监控和自主系统等应用至关重要，需要生成高质量的描述。

Method: 使用ResNet50提取视频帧的视觉特征，通过GPT-2的编码器-解码器模型生成描述，并采用多头自注意力和交叉注意力对齐文本与视觉表示。

Result: 在BLEU-4、CIDEr、METEOR和ROUGE-L等指标上表现优异，如BLEU-4得分为0.755（BDD-X）和0.778（MSVD）。

Conclusion: 该研究通过生成高质量描述，提升了可解释性AI，并增强了实际应用价值。

Abstract: Understanding and analyzing video actions are essential for producing
insightful and contextualized descriptions, especially for video-based
applications like intelligent monitoring and autonomous systems. The proposed
work introduces a novel framework for generating natural language descriptions
from video datasets by combining textual and visual modalities. The suggested
architecture makes use of ResNet50 to extract visual features from video frames
that are taken from the Microsoft Research Video Description Corpus (MSVD), and
Berkeley DeepDrive eXplanation (BDD-X) datasets. The extracted visual
characteristics are converted into patch embeddings and then run through an
encoder-decoder model based on Generative Pre-trained Transformer-2 (GPT-2). In
order to align textual and visual representations and guarantee high-quality
description production, the system uses multi-head self-attention and
cross-attention techniques. The model's efficacy is demonstrated by performance
evaluation using BLEU (1-4), CIDEr, METEOR, and ROUGE-L. The suggested
framework outperforms traditional methods with BLEU-4 scores of 0.755 (BDD-X)
and 0.778 (MSVD), CIDEr scores of 1.235 (BDD-X) and 1.315 (MSVD), METEOR scores
of 0.312 (BDD-X) and 0.329 (MSVD), and ROUGE-L scores of 0.782 (BDD-X) and
0.795 (MSVD). By producing human-like, contextually relevant descriptions,
strengthening interpretability, and improving real-world applications, this
research advances explainable AI.

</details>

### [183] [Decoupled Global-Local Alignment for Improving Compositional Understanding](https://arxiv.org/abs/2504.16801)
*Xiaoxing Hu,Kaicheng Yang,Jun Wang,Haoran Xu,Ziyong Feng,Yupei Wang*

Main category: cs.CV

TLDR: DeGLA框架通过解耦全局-局部对齐，结合自蒸馏机制和对比损失，提升CLIP的组合理解能力，同时保留其通用能力。


<details>
  <summary>Details</summary>
Motivation: CLIP的全局对比学习限制了其对组合概念（如关系和属性）的理解能力，现有方法通过全局硬负样本改进，但牺牲了模型的通用能力。

Method: 提出DeGLA框架，结合自蒸馏机制（对齐可学习编码器与冻结教师模型）和基于LLM构建的高质量负样本，使用IGC和TGC损失增强组合理解。

Result: 在VALSE、SugarCrepe和ARO基准上平均提升3.5%，在11个零样本分类任务上平均提升13.0%。

Conclusion: DeGLA在提升组合理解的同时，有效保留了CLIP的通用能力，实验证明了其优越性。

Abstract: Contrastive Language-Image Pre-training (CLIP) has achieved success on
multiple downstream tasks by aligning image and text modalities. However, the
nature of global contrastive learning limits CLIP's ability to comprehend
compositional concepts, such as relations and attributes. Although recent
studies employ global hard negative samples to improve compositional
understanding, these methods significantly compromise the model's inherent
general capabilities by forcibly distancing textual negative samples from
images in the embedding space. To overcome this limitation, we introduce a
Decoupled Global-Local Alignment (DeGLA) framework that improves compositional
understanding while substantially mitigating losses in general capabilities. To
optimize the retention of the model's inherent capabilities, we incorporate a
self-distillation mechanism within the global alignment process, aligning the
learnable image-text encoder with a frozen teacher model derived from an
exponential moving average. Under the constraint of self-distillation, it
effectively mitigates the catastrophic forgetting of pretrained knowledge
during fine-tuning. To improve compositional understanding, we first leverage
the in-context learning capability of Large Language Models (LLMs) to construct
about 2M high-quality negative captions across five types. Subsequently, we
propose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)
loss to enhance vision-language compositionally. Extensive experimental results
demonstrate the effectiveness of the DeGLA framework. Compared to previous
state-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across
the VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average
performance improvement of 13.0% on zero-shot classification tasks across
eleven datasets. Our code will be released at
https://github.com/xiaoxing2001/DeGLA

</details>

### [184] [A Low-Cost Photogrammetry System for 3D Plant Modeling and Phenotyping](https://arxiv.org/abs/2504.16840)
*Joe Hrzich,Michael A. Beck,Christopher P. Bidinosti,Christopher J. Henry,Kalhari Manawasinghe,Karen Tanino*

Main category: cs.CV

TLDR: 开源低成本的光度测量系统，用于3D植物建模和表型分析，通过点云重建植物3D模型，并以小麦为例展示表型特征的自动计算。


<details>
  <summary>Details</summary>
Motivation: 开发一种低成本、开源的光度测量系统，以简化植物表型特征的测量和分析，尤其是那些难以手工测量的特征。

Method: 采用运动结构法（structure-from-motion）生成植物的3D点云模型，并从中计算表型特征。

Result: 系统能够自动计算植物高度、半径、叶片角度等特征，并用于小麦冠层结构的分类。

Conclusion: 该系统为植物表型研究提供了一种高效、低成本的解决方案，尤其适用于复杂特征的自动化测量。

Abstract: We present an open-source, low-cost photogrammetry system for 3D plant
modeling and phenotyping. The system uses a structure-from-motion approach to
reconstruct 3D representations of the plants via point clouds. Using wheat as
an example, we demonstrate how various phenotypic traits can be computed easily
from the point clouds. These include standard measurements such as plant height
and radius, as well as features that would be more cumbersome to measure by
hand, such as leaf angles and convex hull. We further demonstrate the utility
of the system through the investigation of specific metrics that may yield
objective classifications of erectophile versus planophile wheat canopy
architectures.

</details>

### [185] [Hyperspectral Vision Transformers for Greenhouse Gas Estimations from Space](https://arxiv.org/abs/2504.16851)
*Ruben Gonzalez Avilés,Linus Scheibenreif,Nassim Ait Ali Braham,Benedikt Blumenstiel,Thomas Brunschwiler,Ranjini Guruprasad,Damian Borth,Conrad Albrecht,Paolo Fraccaro,Devyani Lambhate,Johannes Jakubik*

Main category: cs.CV

TLDR: 提出了一种光谱变换模型，通过多光谱数据合成高光谱数据，解决了高光谱成像空间覆盖有限和多光谱数据光谱细节不足的问题。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像在温室气体监测中潜力巨大，但受限于空间覆盖和重访频率；多光谱成像覆盖广但光谱细节不足。

Method: 使用带掩码的自编码器预训练模型，并在时空对齐的多光谱-高光谱图像对上微调。

Result: 合成的数据保留了多光谱的空间和时间优势，同时提高了温室气体预测的准确性。

Conclusion: 该方法通过自监督深度学习结合高光谱和多光谱系统的优势，推动了大气监测的发展。

Abstract: Hyperspectral imaging provides detailed spectral information and holds
significant potential for monitoring of greenhouse gases (GHGs). However, its
application is constrained by limited spatial coverage and infrequent revisit
times. In contrast, multispectral imaging offers broader spatial and temporal
coverage but often lacks the spectral detail that can enhance GHG detection. To
address these challenges, this study proposes a spectral transformer model that
synthesizes hyperspectral data from multispectral inputs. The model is
pre-trained via a band-wise masked autoencoder and subsequently fine-tuned on
spatio-temporally aligned multispectral-hyperspectral image pairs. The
resulting synthetic hyperspectral data retain the spatial and temporal benefits
of multispectral imagery and improve GHG prediction accuracy relative to using
multispectral data alone. This approach effectively bridges the trade-off
between spectral resolution and coverage, highlighting its potential to advance
atmospheric monitoring by combining the strengths of hyperspectral and
multispectral systems with self-supervised deep learning.

</details>

### [186] [High-Quality Cloud-Free Optical Image Synthesis Using Multi-Temporal SAR and Contaminated Optical Data](https://arxiv.org/abs/2504.16870)
*Chenxi Duan*

Main category: cs.CV

TLDR: 本文提出CRSynthNet，一种新型图像合成网络，用于解决云覆盖导致的卫星光学数据缺失问题，通过创新模块提升精度，并创建了TCSEN12数据集。


<details>
  <summary>Details</summary>
Motivation: 解决云覆盖和卫星重访周期长导致的光学数据缺失问题，为遥感应用提供支持。

Method: 提出CRSynthNet网络，包含DownUp Block和Fusion Attention等创新模块，用于复杂云覆盖场景下的数据合成。

Result: CRSynthNet在PSNR（26.978）、SSIM（0.648）和RMSE（0.050）等指标上表现优异，显著优于对比方法。

Conclusion: CRSynthNet和TCSEN12数据集为光学卫星图像合成任务提供了实用方法和宝贵资源。

Abstract: Addressing gaps caused by cloud cover and the long revisit cycle of
satellites is vital for providing essential data to support remote sensing
applications. This paper tackles the challenges of missing optical data
synthesis, particularly in complex scenarios with cloud cover. We propose
CRSynthNet, a novel image synthesis network that incorporates innovative
designed modules such as the DownUp Block and Fusion Attention to enhance
accuracy. Experimental results validate the effectiveness of CRSynthNet,
demonstrating substantial improvements in restoring structural details,
preserving spectral consist, and achieving superior visual effects that far
exceed those produced by comparison methods. It achieves quantitative
improvements across multiple metrics: a peak signal-to-noise ratio (PSNR) of
26.978, a structural similarity index measure (SSIM) of 0.648, and a root mean
square error (RMSE) of 0.050. Furthermore, this study creates the TCSEN12
dataset, a valuable resource specifically designed to address cloud cover
challenges in missing optical data synthesis study. The dataset uniquely
includes cloud-covered images and leverages earlier image to predict later
image, offering a realistic representation of real-world scenarios. This study
offer practical method and valuable resources for optical satellite image
synthesis task.

</details>

### [187] [BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation](https://arxiv.org/abs/2504.16907)
*Ruotong Wang,Mingli Zhu,Jiarong Ou,Rui Chen,Xin Tao,Pengfei Wan,Baoyuan Wu*

Main category: cs.CV

TLDR: BadVideo是首个针对文本到视频（T2V）生成模型的后门攻击框架，利用视频生成中的冗余信息嵌入恶意内容，具有高隐蔽性并能绕过传统内容审核系统。


<details>
  <summary>Details</summary>
Motivation: 探索T2V生成模型的对抗性漏洞，揭示其潜在风险。

Method: 通过时空组合和动态元素变换两种策略设计目标对抗输出。

Result: 实验证明BadVideo攻击成功率高，同时保持原始语义和干净输入的优秀性能。

Conclusion: 揭示了T2V模型的对抗性漏洞，呼吁关注潜在风险和滥用。

Abstract: Text-to-video (T2V) generative models have rapidly advanced and found
widespread applications across fields like entertainment, education, and
marketing. However, the adversarial vulnerabilities of these models remain
rarely explored. We observe that in T2V generation tasks, the generated videos
often contain substantial redundant information not explicitly specified in the
text prompts, such as environmental elements, secondary objects, and additional
details, providing opportunities for malicious attackers to embed hidden
harmful content. Exploiting this inherent redundancy, we introduce BadVideo,
the first backdoor attack framework tailored for T2V generation. Our attack
focuses on designing target adversarial outputs through two key strategies: (1)
Spatio-Temporal Composition, which combines different spatiotemporal features
to encode malicious information; (2) Dynamic Element Transformation, which
introduces transformations in redundant elements over time to convey malicious
information. Based on these strategies, the attacker's malicious target
seamlessly integrates with the user's textual instructions, providing high
stealthiness. Moreover, by exploiting the temporal dimension of videos, our
attack successfully evades traditional content moderation systems that
primarily analyze spatial information within individual frames. Extensive
experiments demonstrate that BadVideo achieves high attack success rates while
preserving original semantics and maintaining excellent performance on clean
inputs. Overall, our work reveals the adversarial vulnerability of T2V models,
calling attention to potential risks and misuse. Our project page is at
https://wrt2000.github.io/BadVideo2025/.

</details>

### [188] [DreamO: A Unified Framework for Image Customization](https://arxiv.org/abs/2504.16915)
*Chong Mou,Yanze Wu,Wenxu Wu,Zinan Guo,Pengze Zhang,Yufeng Cheng,Yiming Luo,Fei Ding,Shiwen Zhang,Xinghui Li,Mengtian Li,Songtao Zhao,Jian Zhang,Qian He,Xinglong Wu*

Main category: cs.CV

TLDR: DreamO是一个统一的图像定制框架，支持多种任务和条件集成，通过扩散变换器（DiT）和渐进训练策略实现高质量输出。


<details>
  <summary>Details</summary>
Motivation: 现有图像定制方法多为任务特定，缺乏通用性，DreamO旨在解决这一问题。

Method: 采用扩散变换器（DiT）统一处理输入，引入特征路由约束和占位符策略，并分三阶段渐进训练。

Result: 实验表明DreamO能高质量完成多种图像定制任务，灵活整合不同控制条件。

Conclusion: DreamO为图像定制提供了一个通用且高效的解决方案。

Abstract: Recently, extensive research on image customization (e.g., identity, subject,
style, background, etc.) demonstrates strong customization capabilities in
large-scale generative models. However, most approaches are designed for
specific tasks, restricting their generalizability to combine different types
of condition. Developing a unified framework for image customization remains an
open challenge. In this paper, we present DreamO, an image customization
framework designed to support a wide range of tasks while facilitating seamless
integration of multiple conditions. Specifically, DreamO utilizes a diffusion
transformer (DiT) framework to uniformly process input of different types.
During training, we construct a large-scale training dataset that includes
various customization tasks, and we introduce a feature routing constraint to
facilitate the precise querying of relevant information from reference images.
Additionally, we design a placeholder strategy that associates specific
placeholders with conditions at particular positions, enabling control over the
placement of conditions in the generated results. Moreover, we employ a
progressive training strategy consisting of three stages: an initial stage
focused on simple tasks with limited data to establish baseline consistency, a
full-scale training stage to comprehensively enhance the customization
capabilities, and a final quality alignment stage to correct quality biases
introduced by low-quality data. Extensive experiments demonstrate that the
proposed DreamO can effectively perform various image customization tasks with
high quality and flexibly integrate different types of control conditions.

</details>

### [189] [Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light](https://arxiv.org/abs/2504.16922)
*Ali Hassani,Fengzhe Zhou,Aditya Kane,Jiannan Huang,Chieh-Yun Chen,Min Shi,Steven Walton,Markus Hoehnerbach,Vijay Thakkar,Michael Isaev,Qinsheng Zhang,Bing Xu,Haicheng Wu,Wen-mei Hwu,Ming-Yu Liu,Humphrey Shi*

Main category: cs.CV

TLDR: 论文提出了一种广义邻域注意力（GNA）机制，通过模拟器和实际实现验证其在稀疏注意力中的性能提升，并在生成模型中实现了显著的端到端加速。


<details>
  <summary>Details</summary>
Motivation: 稀疏注意力机制（如邻域注意力）通常未能稳定超越自注意力基线的速度，原因是注意力基础设施的复杂性和AI硬件的快速演进。同时，许多前沿基础模型（尤其是计算机视觉领域）受限于注意力机制，需要可靠的稀疏性以摆脱O(n^2)复杂度。

Method: 提出广义邻域注意力（GNA），涵盖滑动窗口、跨步滑动窗口和分块注意力；设计模拟器预测性能提升上限；在NVIDIA Blackwell架构上实现GNA，并集成到生成模型中。

Result: GNA在完美块稀疏情况下实现了理论最大加速，FP16下有效利用率为1.3 petaFLOPs/秒；在Cosmos-7B等生成模型中实现28%至46%的端到端加速。

Conclusion: GNA为稀疏注意力提供了可靠的性能提升方案，其模拟器和内核将通过NATTEN项目开源。

Abstract: Many sparse attention mechanisms such as Neighborhood Attention have
typically failed to consistently deliver speedup over the self attention
baseline. This is largely due to the level of complexity in attention
infrastructure, and the rapid evolution of AI hardware architecture. At the
same time, many state-of-the-art foundational models, particularly in computer
vision, are heavily bound by attention, and need reliable sparsity to escape
the O(n^2) complexity. In this paper, we study a class of promising sparse
attention mechanisms that focus on locality, and aim to develop a better
analytical model of their performance improvements. We first introduce
Generalized Neighborhood Attention (GNA), which can describe sliding window,
strided sliding window, and blocked attention. We then consider possible design
choices in implementing these approaches, and create a simulator that can
provide much more realistic speedup upper bounds for any given setting.
Finally, we implement GNA on top of a state-of-the-art fused multi-headed
attention (FMHA) kernel designed for the NVIDIA Blackwell architecture in
CUTLASS. Our implementation can fully realize the maximum speedup theoretically
possible in many perfectly block-sparse cases, and achieves an effective
utilization of 1.3 petaFLOPs/second in FP16. In addition, we plug various GNA
configurations into off-the-shelf generative models, such as Cosmos-7B,
HunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end
speedup on B200 without any fine-tuning. We will open source our simulator and
Blackwell kernels directly through the NATTEN project.

</details>

### [190] [Procedural Dataset Generation for Zero-Shot Stereo Matching](https://arxiv.org/abs/2504.16930)
*David Yan,Alexander Raistrick,Jia Deng*

Main category: cs.CV

TLDR: 论文研究了合成数据集对立体匹配网络训练的影响，提出了优化后的数据集生成器Infinigen-Stereo，其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索合成数据集的设计空间，以提升零样本立体匹配性能。

Method: 通过调整数据集生成器的参数，生成不同合成数据集，并评估其对零样本性能的影响。

Result: Infinigen-Stereo生成的数据集在零样本立体匹配任务中表现优于现有方法。

Conclusion: 优化后的数据集生成器Infinigen-Stereo为立体匹配研究提供了更有效的工具。

Abstract: Synthetic datasets are a crucial ingredient for training stereo matching
networks, but the question of what makes a stereo dataset effective remains
largely unexplored. We investigate the design space of synthetic datasets by
varying the parameters of a procedural dataset generator, and report the
effects on zero-shot stereo matching performance using standard benchmarks. We
collect the best settings to produce Infinigen-Stereo, a procedural generator
specifically optimized for zero-shot stereo datasets. Models trained only on
data from our system outperform robust baselines trained on a combination of
existing synthetic datasets and have stronger zero-shot stereo matching
performance than public checkpoints from prior works. We open source our system
at https://github.com/princeton-vl/InfinigenStereo to enable further research
on procedural stereo datasets.

</details>

<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [191] [DP2FL: Dual Prompt Personalized Federated Learning in Foundation Models](https://arxiv.org/abs/2504.16357)
*Ying Chang,Xiaohu Shi,Xiaohui Zhao,Zhaohuang Chen,Deyin Ma*

Main category: cs.DC

TLDR: DP2FL框架通过双提示和自适应聚合策略，解决了个性化联邦学习中数据不足和新客户集成的问题，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决个性化联邦学习中因本地数据不足导致的模型训练不足问题，并探索基础模型（如CLIP）在联邦学习中的应用潜力。

Method: 提出DP2FL框架，结合双提示和自适应聚合策略，实现全局任务感知与本地数据驱动的平衡。

Result: 在高度异构环境中验证了DP2FL的有效性，展示了其对新颖数据源的预测能力和新客户的无缝集成。

Conclusion: DP2FL通过创新设计显著提升了联邦学习的性能，为数据不足和新客户集成提供了解决方案。

Abstract: Personalized federated learning (PFL) has garnered significant attention for
its ability to address heterogeneous client data distributions while preserving
data privacy. However, when local client data is limited, deep learning models
often suffer from insufficient training, leading to suboptimal performance.
Foundation models, such as CLIP (Contrastive Language-Image Pretraining),
exhibit strong feature extraction capabilities and can alleviate this issue by
fine-tuning on limited local data. Despite their potential, foundation models
are rarely utilized in federated learning scenarios, and challenges related to
integrating new clients remain largely unresolved. To address these challenges,
we propose the Dual Prompt Personalized Federated Learning (DP2FL) framework,
which introduces dual prompts and an adaptive aggregation strategy. DP2FL
combines global task awareness with local data-driven insights, enabling local
models to achieve effective generalization while remaining adaptable to
specific data distributions. Moreover, DP2FL introduces a global model that
enables prediction on new data sources and seamlessly integrates newly added
clients without requiring retraining. Experimental results in highly
heterogeneous environments validate the effectiveness of DP2FL's prompt design
and aggregation strategy, underscoring the advantages of prediction on novel
data sources and demonstrating the seamless integration of new clients into the
federated learning framework.

</details>

### [192] [Towards a Distributed Federated Learning Aggregation Placement using Particle Swarm Intelligence](https://arxiv.org/abs/2504.16227)
*Amir Ali-Pour,Sadra Bekrani,Laya Samizadeh,Julien Gascon-Samson*

Main category: cs.DC

TLDR: Flag-Swap是一种基于粒子群优化（PSO）的方法，用于在分层半去中心化联邦学习（SDFL）中优化聚合任务的位置，仅依赖处理延迟数据，减少对系统数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要频繁交换系统数据以监控节点性能和资源消耗，Flag-Swap旨在减少这种依赖，提高效率。

Method: 采用粒子群优化（PSO）方法，根据处理延迟优化聚合位置。

Result: 模拟和实际实现表明，Flag-Swap比随机和均匀放置策略分别快43%和32%。

Conclusion: Flag-Swap在减少系统数据依赖的同时，显著提高了聚合效率。

Abstract: Federated learning has become a promising distributed learning concept with
extra insurance on data privacy. Extensive studies on various models of
Federated learning have been done since the coinage of its term. One of the
important derivatives of federated learning is hierarchical semi-decentralized
federated learning, which distributes the load of the aggregation task over
multiple nodes and parallelizes the aggregation workload at the breadth of each
level of the hierarchy. Various methods have also been proposed to perform
inter-cluster and intra-cluster aggregation optimally. Most of the solutions,
nonetheless, require monitoring the nodes' performance and resource consumption
at each round, which necessitates frequently exchanging systematic data. To
optimally perform distributed aggregation in SDFL with minimal reliance on
systematic data, we propose Flag-Swap, a Particle Swarm Optimization (PSO)
method that optimizes the aggregation placement according only to the
processing delay. Our simulation results show that PSO-based placement can find
the optimal placement relatively fast, even in scenarios with many clients as
candidates for aggregation. Our real-world docker-based implementation of
Flag-Swap over the recently emerged FL framework shows superior performance
compared to black-box-based deterministic placement strategies, with about 43%
minutes faster than random placement, and 32% minutes faster than uniform
placement, in terms of total processing time.

</details>

### [193] [Simplified Swarm Learning Framework for Robust and Scalable Diagnostic Services in Cancer Histopathology](https://arxiv.org/abs/2504.16732)
*Yanjie Wu,Yuhao Ji,Saiho Lee,Juniad Akram,Ali Braytee,Ali Anaissi*

Main category: cs.DC

TLDR: 论文提出了一种简化的点对点群体学习框架（P2P-SL），用于解决医疗数据隐私和资源受限问题，替代依赖区块链的传统群体学习。


<details>
  <summary>Details</summary>
Motivation: 医疗数据的复杂性（如隐私问题、数据集不平衡和互操作性）需要创新的机器学习解决方案，而传统群体学习因区块链依赖限制了可访问性和可扩展性。

Method: 提出P2P-SL框架，取消区块链依赖，采用轻量级点对点通信，结合优化的预训练模型（如TorchXRayVision和DenseNet解码器）提升诊断准确性。

Result: 实验表明，该框架在不平衡和偏置数据集上表现优异，性能接近集中式模型，同时保护隐私。

Conclusion: 该研究为医疗领域提供了一种可扩展、易访问且高效的隐私敏感诊断解决方案，推动了先进机器学习的普及。

Abstract: The complexities of healthcare data, including privacy concerns, imbalanced
datasets, and interoperability issues, necessitate innovative machine learning
solutions. Swarm Learning (SL), a decentralized alternative to Federated
Learning, offers privacy-preserving distributed training, but its reliance on
blockchain technology hinders accessibility and scalability. This paper
introduces a \textit{Simplified Peer-to-Peer Swarm Learning (P2P-SL) Framework}
tailored for resource-constrained environments. By eliminating blockchain
dependencies and adopting lightweight peer-to-peer communication, the proposed
framework ensures robust model synchronization while maintaining data privacy.
Applied to cancer histopathology, the framework integrates optimized
pre-trained models, such as TorchXRayVision, enhanced with DenseNet decoders,
to improve diagnostic accuracy. Extensive experiments demonstrate the
framework's efficacy in handling imbalanced and biased datasets, achieving
comparable performance to centralized models while preserving privacy. This
study paves the way for democratizing advanced machine learning in healthcare,
offering a scalable, accessible, and efficient solution for privacy-sensitive
diagnostic applications.

</details>

<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [194] [Confidence Sequences for Generalized Linear Models via Regret Analysis](https://arxiv.org/abs/2504.16555)
*Eugenio Clerico,Hamish Flynn,Wojciech Kotłowski,Gergely Neu*

Main category: math.ST

TLDR: 论文提出了一种通过序列预测构建统计模型参数置信集的方法，将问题转化为算法设计。


<details>
  <summary>Details</summary>
Motivation: 为广义线性模型（GLM）参数构建置信集，通过序列概率分配游戏实现低后悔值，从而提供高概率的似然过剩上界。

Method: 提出在线到置信集转换方案，分为分析转换（证明低后悔算法的存在性）和算法转换（利用在线算法输出构建置信集）。

Result: 该方法统一了现有最优置信集构造，并提出了文献中未见过的新置信集类型。

Conclusion: 通过序列预测和在线学习框架，为统计模型参数置信集提供了一种通用且灵活的方法。

Abstract: We develop a methodology for constructing confidence sets for parameters of
statistical models via a reduction to sequential prediction. Our key
observation is that for any generalized linear model (GLM), one can construct
an associated game of sequential probability assignment such that achieving low
regret in the game implies a high-probability upper bound on the excess
likelihood of the true parameter of the GLM. This allows us to develop a scheme
that we call online-to-confidence-set conversions, which effectively reduces
the problem of proving the desired statistical claim to an algorithmic
question. We study two varieties of this conversion scheme: 1) analytical
conversions that only require proving the existence of algorithms with low
regret and provide confidence sets centered at the maximum-likelihood estimator
2) algorithmic conversions that actively leverage the output of the online
algorithm to construct confidence sets (and may be centered at other,
adaptively constructed point estimators). The resulting methodology recovers
all state-of-the-art confidence set constructions within a single framework,
and also provides several new types of confidence sets that were previously
unknown in the literature.

</details>

<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [195] [Efficient Portfolio Selection through Preference Aggregation with Quicksort and the Bradley--Terry Model](https://arxiv.org/abs/2504.16093)
*Yurun Ge,Lucas Böttcher,Tom Chou,Maria R. D'Orsogna*

Main category: q-fin.PM

TLDR: 论文提出了一种基于Quicksort和Bradley-Terry模型的比较规则，用于在不确定性下优化项目资源分配，其方法在性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在不确定性下分配有限资源以最大化长期效益是常见问题，如创新项目选择、研究资助分配和参与式预算。现有方法在比较和聚合项目评估时存在不足。

Method: 采用Quicksort和Bradley-Terry模型，通过代理对项目长期效益的评估生成配对“胜率”，并聚合这些胜率以排名项目。结合采样技术减少配对比较次数。

Result: 提出的方法性能优于当前最有效的两种聚合方法，并能显著减少配对比较次数。

Conclusion: Bradley-Terry模型为项目组合选择提供了实用且高效的解决方案，适用于多种资源分配场景。

Abstract: How to allocate limited resources to projects that will yield the greatest
long-term benefits is a problem that often arises in decision-making under
uncertainty. For example, organizations may need to evaluate and select
innovation projects with risky returns. Similarly, when allocating resources to
research projects, funding agencies are tasked with identifying the most
promising proposals based on idiosyncratic criteria. Finally, in participatory
budgeting, a local community may need to select a subset of public projects to
fund. Regardless of context, agents must estimate the uncertain values of a
potentially large number of projects. Developing parsimonious methods to
compare these projects, and aggregating agent evaluations so that the overall
benefit is maximized, are critical in assembling the best project portfolio.
Unlike in standard sorting algorithms, evaluating projects on the basis of
uncertain long-term benefits introduces additional complexities. We propose
comparison rules based on Quicksort and the Bradley--Terry model, which
connects rankings to pairwise "win" probabilities. In our model, each agent
determines win probabilities of a pair of projects based on his or her specific
evaluation of the projects' long-term benefit. The win probabilities are then
appropriately aggregated and used to rank projects. Several of the methods we
propose perform better than the two most effective aggregation methods
currently available. Additionally, our methods can be combined with sampling
techniques to significantly reduce the number of pairwise comparisons. We also
discuss how the Bradley--Terry portfolio selection approach can be implemented
in practice.

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [196] [Regularizing Differentiable Architecture Search with Smooth Activation](https://arxiv.org/abs/2504.16306)
*Yanlin Zhou,Mostafa El-Khamy,Kee-Bong Song*

Main category: cs.NE

TLDR: SA-DARTS通过平滑激活函数解决DARTS的跳跃操作主导问题，提升NAS性能。


<details>
  <summary>Details</summary>
Motivation: DARTS存在鲁棒性和泛化性问题，尤其是跳跃操作主导导致的性能崩溃。

Method: 提出SA-DARTS，利用平滑激活函数作为辅助损失，平衡权重和无权重操作。

Result: 在NAS-Bench-201、分类和超分辨率任务上取得SOTA结果。

Conclusion: SA-DARTS能有效解决跳跃操作主导问题，提升模型性能，且参数更少。

Abstract: Differentiable Architecture Search (DARTS) is an efficient Neural
Architecture Search (NAS) method but suffers from robustness, generalization,
and discrepancy issues. Many efforts have been made towards the performance
collapse issue caused by skip dominance with various regularization techniques
towards operation weights, path weights, noise injection, and super-network
redesign. It had become questionable at a certain point if there could exist a
better and more elegant way to retract the search to its intended goal -- NAS
is a selection problem. In this paper, we undertake a simple but effective
approach, named Smooth Activation DARTS (SA-DARTS), to overcome skip dominance
and discretization discrepancy challenges. By leveraging a smooth activation
function on architecture weights as an auxiliary loss, our SA-DARTS mitigates
the unfair advantage of weight-free operations, converging to fanned-out
architecture weight values, and can recover the search process from
skip-dominance initialization. Through theoretical and empirical analysis, we
demonstrate that the SA-DARTS can yield new state-of-the-art (SOTA) results on
NAS-Bench-201, classification, and super-resolution. Further, we show that
SA-DARTS can help improve the performance of SOTA models with fewer parameters,
such as Information Multi-distillation Network on the super-resolution task.

</details>

### [197] [Neuro-Evolutionary Approach to Physics-Aware Symbolic Regression](https://arxiv.org/abs/2504.16503)
*Jiří Kubalík,Robert Babuška*

Main category: cs.NE

TLDR: 提出了一种结合神经进化与梯度优化的符号回归方法，通过记忆策略和种群扰动提升模型质量，优于其他基于神经网络的方法。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归依赖遗传编程，而神经网络方法易陷入次优结构，需结合两者优势。

Method: 结合进化搜索神经网络拓扑与梯度优化参数，采用记忆策略和短序列反向传播训练。

Result: 在三个实际测试问题中表现优于其他神经网络方法。

Conclusion: 该方法有效结合进化与梯度优化，提升了符号回归的模型质量。

Abstract: Symbolic regression is a technique that can automatically derive analytic
models from data. Traditionally, symbolic regression has been implemented
primarily through genetic programming that evolves populations of candidate
solutions sampled by genetic operators, crossover and mutation. More recently,
neural networks have been employed to learn the entire analytical model, i.e.,
its structure and coefficients, using regularized gradient-based optimization.
Although this approach tunes the model's coefficients better, it is prone to
premature convergence to suboptimal model structures. Here, we propose a
neuro-evolutionary symbolic regression method that combines the strengths of
evolutionary-based search for optimal neural network (NN) topologies with
gradient-based tuning of the network's parameters. Due to the inherent high
computational demand of evolutionary algorithms, it is not feasible to learn
the parameters of every candidate NN topology to full convergence. Thus, our
method employs a memory-based strategy and population perturbations to enhance
exploitation and reduce the risk of being trapped in suboptimal NNs. In this
way, each NN topology can be trained using only a short sequence of
backpropagation iterations. The proposed method was experimentally evaluated on
three real-world test problems and has been shown to outperform other NN-based
approaches regarding the quality of the models obtained.

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [198] [TinyML for Speech Recognition](https://arxiv.org/abs/2504.16213)
*Andrew Barovic,Armin Moin*

Main category: cs.SD

TLDR: 论文提出了一种在资源受限的IoT边缘设备上部署的量化1D卷积神经网络模型，用于语音识别，并创建了新数据集，实现了97%的准确率。


<details>
  <summary>Details</summary>
Motivation: 为智能家居和辅助生活等IoT应用提供高效的语音识别解决方案。

Method: 使用Edge Impulse技术优化模型，并在Arduino Nano 33 BLE Sense上实现原型验证。

Result: 模型能处理23种关键词，准确率达97%。

Conclusion: 该方法在资源受限设备上实现了高效的语音识别，扩展了应用场景。

Abstract: We train and deploy a quantized 1D convolutional neural network model to
conduct speech recognition on a highly resource-constrained IoT edge device.
This can be useful in various Internet of Things (IoT) applications, such as
smart homes and ambient assisted living for the elderly and people with
disabilities, just to name a few examples. In this paper, we first create a new
dataset with over one hour of audio data that enables our research and will be
useful to future studies in this field. Second, we utilize the technologies
provided by Edge Impulse to enhance our model's performance and achieve a high
Accuracy of up to 97% on our dataset. For the validation, we implement our
prototype using the Arduino Nano 33 BLE Sense microcontroller board. This
microcontroller board is specifically designed for IoT and AI applications,
making it an ideal choice for our target use case scenarios. While most
existing research focuses on a limited set of keywords, our model can process
23 different keywords, enabling complex commands.

</details>

<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [199] [Probabilistic Emulation of the Community Radiative Transfer Model Using Machine Learning](https://arxiv.org/abs/2504.16192)
*Lucas Howard,Aneesh C. Subramanian,Gregory Thompson,Benjamin Johnson,Thomas Auligne*

Main category: physics.ao-ph

TLDR: 利用机器学习构建了一个高效的神经网络概率模拟器，用于模拟社区辐射传输模型（CRTM），显著降低了计算成本并提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 卫星观测数据的增加及其同化对天气预报技能提升至关重要，但辐射传输模型的计算成本高，导致大量数据未被充分利用。

Method: 使用机器学习构建CRTM的神经网络概率模拟器，应用于GOES Advanced Baseline Imager，预测亮温及其误差。

Result: 模拟器预测的亮温均方根误差为0.3 K，晴空条件下9/10红外通道误差小于0.1 K，误差预测可靠。

Conclusion: 模拟器不仅高效，还能重现相关物理过程，增强了模型对新数据的性能信心。

Abstract: The continuous improvement in weather forecast skill over the past several
decades is largely due to the increasing quantity of available satellite
observations and their assimilation into operational forecast systems.
Assimilating these observations requires observation operators in the form of
radiative transfer models. Significant efforts have been dedicated to enhancing
the computational efficiency of these models. Computational cost remains a
bottleneck, and a large fraction of available data goes unused for
assimilation. To address this, we used machine learning to build an efficient
neural network based probabilistic emulator of the Community Radiative Transfer
Model (CRTM), applied to the GOES Advanced Baseline Imager. The trained NN
emulator predicts brightness temperatures output by CRTM and the corresponding
error with respect to CRTM. RMSE of the predicted brightness temperature is 0.3
K averaged across all channels. For clear sky conditions, the RMSE is less than
0.1 K for 9 out of 10 infrared channels. The error predictions are generally
reliable across a wide range of conditions. Explainable AI methods demonstrate
that the trained emulator reproduces the relevant physics, increasing
confidence that the model will perform well when presented with new data.

</details>

<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [200] [Approximating Optimal Labelings for Temporal Connectivity](https://arxiv.org/abs/2504.16837)
*Daniele Carnevale,Gianlorenzo D'Angelo,Martin Olsen*

Main category: cs.DS

TLDR: 研究在时间图中如何调度边的可用时间，以最小化标签数量，同时确保所有顶点对在给定最大时间内连通。问题为NP难，并给出了近似算法。


<details>
  <summary>Details</summary>
Motivation: 在物流、社交网络信息传播等领域，优化时间标签可显著降低成本或资源消耗。

Method: 通过理论分析证明问题的复杂性，并设计近似算法。

Result: 证明了问题在特定条件下的不可近似性，并提供了近似算法。

Conclusion: 研究扩展了对MAL问题的理解，并建立了与静态图问题的联系。

Abstract: In a temporal graph the edge set dynamically changes over time according to a
set of time-labels associated with each edge that indicates at which time-steps
the edge is available. Two vertices are connected if there is a path connecting
them in which the edges are traversed in increasing order of their labels. We
study the problem of scheduling the availability time of the edges of a
temporal graph in such a way that all pairs of vertices are connected within a
given maximum allowed time $a$ and the overall number of labels is minimized.
  The problem, known as \emph{Minimum Aged Labeling} (MAL), has several
applications in logistics, distribution scheduling, and information spreading
in social networks, where carefully choosing the time-labels can significantly
reduce infrastructure costs, fuel consumption, or greenhouse gases.
  The problem MAL has previously been proved to be NP-complete on undirected
graphs and \APX-hard on directed graphs. In this paper, we extend our knowledge
on the complexity and approximability of MAL in several directions. We first
show that the problem cannot be approximated within a factor better than
$O(\log n)$ when $a\geq 2$, unless $\text{P} = \text{NP}$, and a factor better
than $2^{\log ^{1-\epsilon} n}$ when $a\geq 3$, unless $\text{NP}\subseteq
\text{DTIME}(2^{\text{polylog}(n)})$, where $n$ is the number of vertices in
the graph. Then we give a set of approximation algorithms that, under some
conditions, almost match these lower bounds. In particular, we show that the
approximation depends on a relation between $a$ and the diameter of the input
graph.
  We further establish a connection with a foundational optimization problem on
static graphs called \emph{Diameter Constrained Spanning Subgraph} (DCSS) and
show that our hardness results also apply to DCSS.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [201] [Behavior of prediction performance metrics with rare events](https://arxiv.org/abs/2504.16185)
*Emily Minus,R. Yates Coley,Susan M. Shortreed,Brian D. Williamson*

Main category: stat.ML

TLDR: AUC在罕见事件中的可靠性取决于事件总数而非事件率，其他性能指标如敏感性和特异性则分别受事件和非事件数量影响。


<details>
  <summary>Details</summary>
Motivation: 研究AUC在罕见事件中是否可靠，以及其他预测性能指标的行为。

Method: 通过模拟研究分析AUC的偏差和方差，以及其他指标（如PPV、准确性、敏感性和特异性）的表现。

Result: AUC的可靠性取决于最小类别大小，而非事件率；敏感性和特异性分别受事件和非事件数量影响；PPV和准确性则依赖于事件率。

Conclusion: 在罕见事件中，只要事件总数足够大，AUC是可靠的。

Abstract: Area under the receiving operator characteristic curve (AUC) is commonly
reported alongside binary prediction models. However, there are concerns that
AUC might be a misleading measure of prediction performance in the rare event
setting. This setting is common since many events of clinical importance are
rare events. We conducted a simulation study to determine when or whether AUC
is unstable in the rare event setting. Specifically, we aimed to determine
whether the bias and variance of AUC are driven by the number of events or the
event rate. We also investigated the behavior of other commonly used measures
of prediction performance, including positive predictive value, accuracy,
sensitivity, and specificity. Our results indicate that poor AUC behavior -- as
measured by empirical bias, variability of cross-validated AUC estimates, and
empirical coverage of confidence intervals -- is driven by the minimum class
size, not event rate. Performance of sensitivity is driven by the number of
events, while that of specificity is driven by the number of non-events. Other
measures, including positive predictive value and accuracy, depend on the event
rate even in large samples. AUC is reliable in the rare event setting provided
that the total number of events is moderately large.

</details>

### [202] [Covariate-dependent Graphical Model Estimation via Neural Networks with Statistical Guarantees](https://arxiv.org/abs/2504.16356)
*Jiahe Lin,Yikai Zhang,George Michailidis*

Main category: stat.ML

TLDR: 本文提出了一种基于深度神经网络的方法，用于估计协变量依赖的图结构，适用于非高斯数据，并提供了理论保证和实际应用验证。


<details>
  <summary>Details</summary>
Motivation: 图形模型广泛用于建模随机变量间的条件依赖关系，但现有方法通常假设图结构固定或高斯分布，本文旨在解决协变量依赖和非高斯数据的挑战。

Method: 采用深度神经网络方法，灵活建模协变量对图结构的依赖关系，无需高斯假设，并在经验风险最小化框架下提供理论保证。

Result: 在合成数据和真实数据（神经科学和金融领域）上验证了方法的性能，优于现有方法，并生成可解释的结果。

Conclusion: 该方法在协变量依赖的图结构估计中表现出色，具有理论支持和实际应用价值。

Abstract: Graphical models are widely used in diverse application domains to model the
conditional dependencies amongst a collection of random variables. In this
paper, we consider settings where the graph structure is covariate-dependent,
and investigate a deep neural network-based approach to estimate it. The method
allows for flexible functional dependency on the covariate, and fits the data
reasonably well in the absence of a Gaussianity assumption. Theoretical results
with PAC guarantees are established for the method, under assumptions commonly
used in an Empirical Risk Minimization framework. The performance of the
proposed method is evaluated on several synthetic data settings and benchmarked
against existing approaches. The method is further illustrated on real datasets
involving data from neuroscience and finance, respectively, and produces
interpretable results.

</details>

<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [203] [Exploring zero-shot structure-based protein fitness prediction](https://arxiv.org/abs/2504.16886)
*Arnav Sharma,Anthony Gitter*

Main category: q-bio.QM

TLDR: 该论文探讨了基于结构的零射击蛋白质适应度预测模型的性能及其局限性，尤其是在无序区域的表现。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估基于结构的机器学习模型在预测蛋白质序列变化的适应度后果时的效果，尤其是在缺乏固定结构的无序区域。

Method: 通过实验评估了多种基于结构的建模选择及其对适应度预测的影响，并测试了多模态集成模型。

Result: 研究发现无序区域的预测结构可能误导模型性能，且匹配蛋白质结构与适应度测定至关重要。多模态集成模型表现良好。

Conclusion: 零射击适应度预测模型在无序区域存在挑战，但多模态集成模型提供了有效的基线解决方案。

Abstract: The ability to make zero-shot predictions about the fitness consequences of
protein sequence changes with pre-trained machine learning models enables many
practical applications. Such models can be applied for downstream tasks like
genetic variant interpretation and protein engineering without additional
labeled data. The advent of capable protein structure prediction tools has led
to the availability of orders of magnitude more precomputed predicted
structures, giving rise to powerful structure-based fitness prediction models.
Through our experiments, we assess several modeling choices for structure-based
models and their effects on downstream fitness prediction. Zero-shot fitness
prediction models can struggle to assess the fitness landscape within
disordered regions of proteins, those that lack a fixed 3D structure. We
confirm the importance of matching protein structures to fitness assays and
find that predicted structures for disordered regions can be misleading and
affect predictive performance. Lastly, we evaluate an additional
structure-based model on the ProteinGym substitution benchmark and show that
simple multi-modal ensembles are strong baselines.

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [204] [Implementing AI Bill of Materials (AI BOM) with SPDX 3.0: A Comprehensive Guide to Creating AI and Dataset Bill of Materials](https://arxiv.org/abs/2504.16743)
*Karen Bennet,Gopi Krishnan Rajbahadur,Arthit Suriyawongkul,Kate Stewart*

Main category: cs.SE

TLDR: 本文提出了AI-BOM的概念，扩展了SBOM以涵盖AI项目的更多方面，包括算法、数据收集方法、框架和库、许可信息及标准合规性。


<details>
  <summary>Details</summary>
Motivation: AI项目在软件安全之外面临独特挑战，需要更全面的材料清单以确保透明性和安全性。

Method: 通过扩展SBOM，引入AI-BOM概念，涵盖算法、数据收集方法、框架和库、许可信息及标准合规性。

Result: 提出了AI-BOM框架，为AI项目提供更全面的材料清单。

Conclusion: AI-BOM是解决AI项目独特挑战的有效工具，增强了软件供应链的透明性和安全性。

Abstract: A Software Bill of Materials (SBOM) is becoming an increasingly important
tool in regulatory and technical spaces to introduce more transparency and
security into a project's software supply chain.
  Artificial intelligence (AI) projects face unique challenges beyond the
security of their software, and thus require a more expansive approach to a
bill of materials. In this report, we introduce the concept of an AI-BOM,
expanding on the SBOM to include the documentation of algorithms, data
collection methods, frameworks and libraries, licensing information, and
standard compliance.

</details>

### [205] [Mining Software Repositories for Expert Recommendation](https://arxiv.org/abs/2504.16343)
*Chad Marshall,Andrew Barovic,Armin Moin*

Main category: cs.SE

TLDR: 提出了一种基于BERTopic和TopicMiner的自动化方法，用于将开源项目中的bug分配给合适的开发者。


<details>
  <summary>Details</summary>
Motivation: 帮助人工bug分类员更高效地找到具备特定领域专业知识的开发者。

Method: 利用问题跟踪系统中的历史数据，结合bug报告的特征（如产品、组件、优先级和严重性）和开发者的经验进行分配。

Result: 通过Top-k准确率评估，结果优于TopicMiner MTM、BUGZIE、BT-RL和LDA-SVM等方法。

Conclusion: 该方法能有效提升bug分配的准确性和效率。

Abstract: We propose an automated approach to bug assignment to developers in large
open-source software projects. This way, we assist human bug triagers who are
in charge of finding the best developer with the right level of expertise in a
particular area to be assigned to a newly reported issue. Our approach is based
on the history of software development as documented in the issue tracking
systems. We deploy BERTopic and techniques from TopicMiner. Our approach works
based on the bug reports' features, such as the corresponding products and
components, as well as their priority and severity levels. We sort developers
based on their experience with specific combinations of new reports. The
evaluation is performed using Top-k accuracy, and the results are compared with
the reported results in prior work, namely TopicMiner MTM, BUGZIE, Bug triaging
via deep Reinforcement Learning BT-RL, and LDA-SVM. The evaluation data come
from various Eclipse and Mozilla projects, such as JDT, Firefox, and
Thunderbird.

</details>

### [206] [Harden and Catch for Just-in-Time Assured LLM-Based Software Testing: Open Research Challenges](https://arxiv.org/abs/2504.16472)
*Mark Harman,Peter O'Hearn,Shubho Sengupta*

Main category: cs.SE

TLDR: 论文探讨了自动化软件测试中的新概念，如硬化测试和捕获测试，并提出了即时捕获测试挑战（JiTTest），展示了其在大型语言模型中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 研究自动化软件测试中未明确定义但具有实际潜力的概念，特别是在大型语言模型背景下。

Method: 正式定义并研究硬化测试和捕获测试的特性，提出即时捕获测试挑战（JiTTest）。

Result: 展示了硬化测试和JiTTest的潜在应用，包括在Meta的初步成果。

Conclusion: 论文提出了新的研究方向，并讨论了部署选项和开放性问题。

Abstract: Despite decades of research and practice in automated software testing,
several fundamental concepts remain ill-defined and under-explored, yet offer
enormous potential real-world impact. We show that these concepts raise
exciting new challenges in the context of Large Language Models for software
test generation. More specifically, we formally define and investigate the
properties of hardening and catching tests. A hardening test is one that seeks
to protect against future regressions, while a catching test is one that
catches such a regression or a fault in new functionality introduced by a code
change. Hardening tests can be generated at any time and may become catching
tests when a future regression is caught. We also define and motivate the
Catching `Just-in-Time' (JiTTest) Challenge, in which tests are generated
`just-in-time' to catch new faults before they land into production. We show
that any solution to Catching JiTTest generation can also be repurposed to
catch latent faults in legacy code. We enumerate possible outcomes for
hardening and catching tests and JiTTests, and discuss open research problems,
deployment options, and initial results from our work on automated LLM-based
hardening at Meta. This paper\footnote{Author order is alphabetical. The
corresponding author is Mark Harman.} was written to accompany the keynote by
the authors at the ACM International Conference on the Foundations of Software
Engineering (FSE) 2025.

</details>

### [207] [On Developers' Self-Declaration of AI-Generated Code: An Analysis of Practices](https://arxiv.org/abs/2504.16485)
*Syed Mohammad Kashif,Peng Liang,Amjed Tahir*

Main category: cs.SE

TLDR: 研究探讨开发者如何自我声明AI生成代码及其原因，通过混合方法研究发现多数开发者会声明，少数不声明，并提供了相关指南。


<details>
  <summary>Details</summary>
Motivation: 在现实软件开发中，区分AI生成代码与人工编写代码是前提，需开发者明确声明AI生成代码。

Method: 混合方法研究：第一阶段挖掘GitHub代码片段，第二阶段进行工业调查。

Result: 76.6%开发者会声明AI生成代码，23.4%不声明。声明原因包括追踪和伦理考量，不声明原因包括代码修改多和认为声明不必要。

Conclusion: 提供指南以帮助开发者声明AI生成代码，解决伦理和代码质量问题。

Abstract: AI code generation tools have gained significant popularity among developers,
who use them to assist in software development due to their capability to
generate code. Existing studies mainly explored the quality, e.g., correctness
and security, of AI-generated code, while in real-world software development,
the prerequisite is to distinguish AI-generated code from human-written code,
which emphasizes the need to explicitly declare AI-generated code by
developers. To this end, this study intends to understand the ways developers
use to self-declare AI-generated code and explore the reasons why developers
choose to self-declare or not. We conducted a mixed-methods study consisting of
two phases. In the first phase, we mined GitHub repositories and collected 613
instances of AI-generated code snippets. In the second phase, we conducted a
follow-up industrial survey, which received 111 valid responses. Our research
revealed the practices followed by developers to self-declare AI-generated
code. Most practitioners (76.6%) always or sometimes self-declare AI-generated
code. In contrast, other practitioners (23.4%) noted that they never
self-declare AI-generated code. The reasons for self-declaring AI-generated
code include the need to track and monitor the code for future review and
debugging, and ethical considerations. The reasons for not self-declaring
AI-generated code include extensive modifications to AI-generated code and the
developers' perception that self-declaration is an unnecessary activity. We
finally provided guidelines for practitioners to self-declare AI-generated
code, addressing ethical and code quality concerns.

</details>

### [208] [ClarifyCoder: Clarification-Aware Fine-Tuning for Programmatic Problem Solving](https://arxiv.org/abs/2504.16331)
*Jie JW Wu,Manav Chaudhary,Davit Abrahamyan,Arhaan Khaku,Anjiang Wei,Fatemeh H. Fard*

Main category: cs.SE

TLDR: ClarifyCoder是一个新框架，通过合成数据生成和指令调整，使LLM在代码生成前识别模糊需求并请求澄清。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在代码生成任务中表现优异，但与人类工程师相比，缺乏主动澄清模糊需求的能力。

Method: 1. 合成数据技术，为编程数据集添加需要澄清的场景；2. 微调策略，教导模型优先请求澄清而非直接生成代码。

Result: ClarifyCoder显著提升了Code LLM的沟通能力，同时保持代码生成能力。

Conclusion: ClarifyCoder为LLM提供了识别和查询模糊需求的内在能力，填补了与人类工程师的差距。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation tasks. However, a significant gap remains between their current
performance and that of expert software engineers. A key differentiator is that
human engineers actively seek clarification when faced with ambiguous
requirements, while LLMs typically generate code regardless of uncertainties in
the problem description. We present ClarifyCoder, a novel framework with
synthetic data generation and instruction-tuning that enables LLMs to identify
ambiguities and request clarification before proceeding with code generation.
While recent work has focused on LLM-based agents for iterative code
generation, we argue that the fundamental ability to recognize and query
ambiguous requirements should be intrinsic to the models themselves. Our
approach consists of two main components: (1) a data synthesis technique that
augments existing programming datasets with scenarios requiring clarification
to generate clarification-aware training data, and (2) a fine-tuning strategy
that teaches models to prioritize seeking clarification over immediate code
generation when faced with incomplete or ambiguous requirements. We further
provide an empirical analysis of integrating ClarifyCoder with standard
fine-tuning for a joint optimization of both clarify-awareness and coding
ability. Experimental results demonstrate that ClarifyCoder significantly
improves the communication capabilities of Code LLMs through meaningful
clarification dialogues while maintaining code generation capabilities.

</details>

<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [209] [Heterogeneous networks in drug-target interaction prediction](https://arxiv.org/abs/2504.16152)
*Mohammad Molaee,Nasrollah Moghadam Charkari*

Main category: q-bio.BM

TLDR: 本文综述了2020-2024年间基于图机器学习的药物-靶点相互作用预测方法，包括框架、贡献、数据集和源代码，并讨论了未来挑战。


<details>
  <summary>Details</summary>
Motivation: 药物发现耗时耗力，计算预测可缩小实验范围，提高效率。

Method: 综述了图机器学习方法，包括框架、数据集和评估指标。

Result: 图机器学习在药物-靶点相互作用预测中表现优异。

Conclusion: 未来需进一步探索相关挑战和关键领域。

Abstract: Drug discovery requires a tremendous amount of time and cost. Computational
drug-target interaction prediction, a significant part of this process, can
reduce these requirements by narrowing the search space for wet lab
experiments. In this survey, we provide comprehensive details of graph machine
learning-based methods in predicting drug-target interaction, as they have
shown promising results in this field. These details include the overall
framework, main contribution, datasets, and their source codes. The selected
papers were mainly published from 2020 to 2024. Prior to discussing papers, we
briefly introduce the datasets commonly used with these methods and
measurements to assess their performance. Finally, future challenges and some
crucial areas that need to be explored are discussed.

</details>

### [210] [The Dance of Atoms-De Novo Protein Design with Diffusion Model](https://arxiv.org/abs/2504.16479)
*Yujie Qin,Ming He,Changyong Yu,Ming Ni,Xian Liu,Xiaochen Bo*

Main category: q-bio.BM

TLDR: 生成式AI模型，尤其是扩散模型，显著提升了蛋白质从头设计的成功率，降低了实验成本，并在该领域取得突破。


<details>
  <summary>Details</summary>
Motivation: 利用高质量蛋白质结构和序列数据，结合生成式AI模型，克服传统方法的局限性，推动蛋白质设计的发展。

Method: 扩散模型在蛋白质骨架和序列生成中的应用，比较不同模型的优缺点。

Result: 扩散模型（如RFDiffusion）在25项蛋白质设计任务中表现优于传统方法和其它AI方法。

Conclusion: 扩散模型在蛋白质设计中展现出巨大潜力，未来需进一步探索其发展方向和应用场景。

Abstract: The de novo design of proteins refers to creating proteins with specific
structures and functions that do not naturally exist. In recent years, the
accumulation of high-quality protein structure and sequence data and
technological advancements have paved the way for the successful application of
generative artificial intelligence (AI) models in protein design. These models
have surpassed traditional approaches that rely on fragments and
bioinformatics. They have significantly enhanced the success rate of de novo
protein design, and reduced experimental costs, leading to breakthroughs in the
field. Among various generative AI models, diffusion models have yielded the
most promising results in protein design. In the past two to three years, more
than ten protein design models based on diffusion models have emerged. Among
them, the representative model, RFDiffusion, has demonstrated success rates in
25 protein design tasks that far exceed those of traditional methods, and other
AI-based approaches like RFjoint and hallucination. This review will
systematically examine the application of diffusion models in generating
protein backbones and sequences. We will explore the strengths and limitations
of different models, summarize successful cases of protein design using
diffusion models, and discuss future development directions.

</details>

<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [211] [4D Multimodal Co-attention Fusion Network with Latent Contrastive Alignment for Alzheimer's Diagnosis](https://arxiv.org/abs/2504.16798)
*Yuxiang Wei,Yanteng Zhang,Xi Xiao,Tianyang Wang,Xiao Wang,Vince D. Calhoun*

Main category: cs.MM

TLDR: M2M-AlignNet是一种几何感知的多模态共注意力网络，用于早期阿尔茨海默病（AD）诊断，通过sMRI和fMRI数据的融合解决模态异质性挑战。


<details>
  <summary>Details</summary>
Motivation: 多模态神经影像数据（如sMRI和fMRI）的融合可提高AD诊断敏感性，但模态间的异质性（如4D fMRI与3D sMRI）带来了特征融合的挑战。

Method: 提出M2M-AlignNet，采用几何加权的多补丁对比损失函数和潜在对齐的共注意力模块，实现fMRI与sMRI的无约束对齐和特征融合。

Result: 实验验证了方法的有效性，并揭示了fMRI与sMRI作为AD生物标志物的对应关系。

Conclusion: M2M-AlignNet为多模态数据融合提供了一种有效方法，有助于早期AD诊断。

Abstract: Multimodal neuroimaging provides complementary structural and functional
insights into both human brain organization and disease-related dynamics.
Recent studies demonstrate enhanced diagnostic sensitivity for Alzheimer's
disease (AD) through synergistic integration of neuroimaging data (e.g., sMRI,
fMRI) with behavioral cognitive scores tabular data biomarkers. However, the
intrinsic heterogeneity across modalities (e.g., 4D spatiotemporal fMRI
dynamics vs. 3D anatomical sMRI structure) presents critical challenges for
discriminative feature fusion. To bridge this gap, we propose M2M-AlignNet: a
geometry-aware multimodal co-attention network with latent alignment for early
AD diagnosis using sMRI and fMRI. At the core of our approach is a
multi-patch-to-multi-patch (M2M) contrastive loss function that quantifies and
reduces representational discrepancies via geometry-weighted patch
correspondence, explicitly aligning fMRI components across brain regions with
their sMRI structural substrates without one-to-one constraints. Additionally,
we propose a latent-as-query co-attention module to autonomously discover
fusion patterns, circumventing modality prioritization biases while minimizing
feature redundancy. We conduct extensive experiments to confirm the
effectiveness of our method and highlight the correspondance between fMRI and
sMRI as AD biomarkers.

</details>

<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [212] [A Geometric Approach to Problems in Optimization and Data Science](https://arxiv.org/abs/2504.16270)
*Naren Sarayu Manoj*

Main category: math.OC

TLDR: 论文分为两部分：第一部分关注优化问题的计算方面，提出新算法；第二部分研究数据科学问题的统计保证，分析新模型和算法鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 利用高维几何和概率工具解决计算和统计机器学习中的问题。

Method: 第一部分提出近似凸多面体、稀疏化、鲁棒最小二乘回归和对偶优化的新算法；第二部分分析后门数据投毒攻击的统计特性及图聚类算法的鲁棒性。

Result: 提供了优化和数据科学问题的新算法和统计保证。

Conclusion: 通过高维几何和概率工具，论文在计算和统计机器学习领域取得了新进展。

Abstract: We give new results for problems in computational and statistical machine
learning using tools from high-dimensional geometry and probability.
  We break up our treatment into two parts. In Part I, we focus on
computational considerations in optimization. Specifically, we give new
algorithms for approximating convex polytopes in a stream, sparsification and
robust least squares regression, and dueling optimization.
  In Part II, we give new statistical guarantees for data science problems. In
particular, we formulate a new model in which we analyze statistical properties
of backdoor data poisoning attacks, and we study the robustness of graph
clustering algorithms to ``helpful'' misspecification.

</details>

### [213] [The Safety-Privacy Tradeoff in Linear Bandits](https://arxiv.org/abs/2504.16371)
*Arghavan Zibaie,Spencer Hutchinson,Ramtin Pedarsani,Mahnoosh Alizadeh*

Main category: math.OC

TLDR: 论文研究了多智能体线性随机老虎机问题，在全局安全约束和局部差分隐私（LDP）保护下，权衡隐私、安全性和遗憾最小化。


<details>
  <summary>Details</summary>
Motivation: 解决在多智能体系统中，如何在保护隐私的同时满足全局安全约束并最小化遗憾的问题。

Method: 通过安全集的锐度概念，提出了一种在给定最大遗憾预算下，为不同智能体分配隐私级别的不可单边改进的隐私向量。

Result: 量化了隐私保护与安全性和遗憾之间的权衡关系。

Conclusion: 提出的方法能够有效平衡隐私、安全性和遗憾，为多智能体系统的决策提供了理论支持。

Abstract: We consider a collection of linear stochastic bandit problems, each modeling
the random response of different agents to proposed interventions, coupled
together by a global safety constraint. We assume a central coordinator must
choose actions to play on each bandit with the objective of regret
minimization, while also ensuring that the expected response of all agents
satisfies the global safety constraints at each round, in spite of uncertainty
about the bandits' parameters. The agents consider their observed responses to
be private and in order to protect their sensitive information, the data
sharing with the central coordinator is performed under local differential
privacy (LDP). However, providing higher level of privacy to different agents
would have consequences in terms of safety and regret. We formalize these
tradeoffs by building on the notion of the sharpness of the safety set - a
measure of how the geometric properties of the safe set affects the growth of
regret - and propose a unilaterally unimprovable vector of privacy levels for
different agents given a maximum regret budget.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [214] [SOTOPIA-S4: a user-friendly system for flexible, customizable, and large-scale social simulation](https://arxiv.org/abs/2504.16122)
*Xuhui Zhou,Zhe Su,Sophie Feng,Jiaxu Zhou,Jen-tse Huang,Hsien-Te Kao,Spencer Lynch,Svitlana Volkova,Tongshuang Sherry Wu,Anita Woolley,Hao Zhu,Maarten Sap*

Main category: cs.CY

TLDR: SOTOPIA-S4是一个快速、灵活且可扩展的社交模拟系统，通过LLM代理支持多轮和多方的交互，并提供可定制的评估指标。


<details>
  <summary>Details</summary>
Motivation: 解决当前社交模拟框架的技术障碍，支持非技术用户设计、运行和分析模拟。

Method: 系统包含模拟引擎、RESTful API服务器和Web界面，支持多轮和多方的LLM交互。

Result: 通过招聘谈判和多方规划两个案例验证了系统的实用性。

Conclusion: SOTOPIA-S4为社交科学研究和LLM行为验证提供了高效工具。

Abstract: Social simulation through large language model (LLM) agents is a promising
approach to explore and validate hypotheses related to social science questions
and LLM agents behavior. We present SOTOPIA-S4, a fast, flexible, and scalable
social simulation system that addresses the technical barriers of current
frameworks while enabling practitioners to generate multi-turn and multi-party
LLM-based interactions with customizable evaluation metrics for hypothesis
testing. SOTOPIA-S4 comes as a pip package that contains a simulation engine,
an API server with flexible RESTful APIs for simulation management, and a web
interface that enables both technical and non-technical users to design, run,
and analyze simulations without programming. We demonstrate the usefulness of
SOTOPIA-S4 with two use cases involving dyadic hiring negotiation and
multi-party planning scenarios.

</details>

### [215] [Efficacy of a Computer Tutor that Models Expert Human Tutors](https://arxiv.org/abs/2504.16132)
*Andrew M. Olney,Sidney K. D'Mello,Natalie Person,Whitney Cade,Patrick Hays,Claire W. Dempsey,Blair Lehman,Betsy Williams,Art Graesser*

Main category: cs.CY

TLDR: 研究表明，智能辅导系统（ITS）和人类专家导师在即时和延迟测试中均显著提升学习效果，但导师的专业知识对辅导效果的具体贡献仍需进一步探讨。


<details>
  <summary>Details</summary>
Motivation: 探讨专业知识在辅导效果中的作用，并比较智能辅导系统与人类专家导师的效果差异。

Method: 进行为期9周的学习效果研究，比较智能辅导系统、人类专家导师和无辅导条件的效果，使用逻辑混合效应模型分析数据。

Result: 智能辅导系统和人类导师在即时测试（d=.71和d=.66）和延迟测试（d=.36和d=.39）中均表现出显著正面效果。

Conclusion: 研究支持智能辅导系统和人类导师的有效性，但专业知识的具体作用仍需进一步研究。

Abstract: Tutoring is highly effective for promoting learning. However, the
contribution of expertise to tutoring effectiveness is unclear and continues to
be debated. We conducted a 9-week learning efficacy study of an intelligent
tutoring system (ITS) for biology modeled on expert human tutors with two
control conditions: human tutors who were experts in the domain but not in
tutoring and a no-tutoring condition. All conditions were supplemental to
classroom instruction, and students took learning tests immediately before and
after tutoring sessions as well as delayed tests 1-2 weeks later. Analysis
using logistic mixed-effects modeling indicates significant positive effects on
the immediate post-test for the ITS (d =.71) and human tutors (d =.66) which
are in the 99th percentile of meta-analytic effects, as well as significant
positive effects on the delayed post-test for the ITS (d =.36) and human tutors
(d =.39). We discuss implications for the role of expertise in tutoring and the
design of future studies.

</details>

### [216] [A Conceptual Framework for AI-based Decision Systems in Critical Infrastructures](https://arxiv.org/abs/2504.16133)
*Milad Leyli-abadi,Ricardo J. Bessa,Jan Viebahn,Daniel Boos,Clark Borst,Alberto Castagna,Ricardo Chavarriaga,Mohamed Hassouna,Bruno Lemetayer,Giulia Leto,Antoine Marot,Maroua Meddeb,Manuel Meyer,Viola Schiaffonati,Manuel Schneider,Toni Waefler*

Main category: cs.CY

TLDR: 本文提出了一种跨学科的框架，用于解决人类与AI在安全关键系统中的交互挑战，整合了多个领域的知识。


<details>
  <summary>Details</summary>
Motivation: 现有框架未能全面解决人类与AI在安全关键系统中的交互问题，特别是在透明度、信任和解释性方面的需求。

Method: 采用跨学科方法，整合数学、决策理论、计算机科学、哲学、心理学和认知工程等领域的知识，并在现有框架上进行实例化。

Result: 提出了一个全面的概念框架，能够灵活应用于关键基础设施的设计、部署和维护。

Conclusion: 该框架为安全关键系统中人类与AI的交互提供了更全面的解决方案，填补了现有研究的空白。

Abstract: The interaction between humans and AI in safety-critical systems presents a
unique set of challenges that remain partially addressed by existing
frameworks. These challenges stem from the complex interplay of requirements
for transparency, trust, and explainability, coupled with the necessity for
robust and safe decision-making. A framework that holistically integrates human
and AI capabilities while addressing these concerns is notably required,
bridging the critical gaps in designing, deploying, and maintaining safe and
effective systems. This paper proposes a holistic conceptual framework for
critical infrastructures by adopting an interdisciplinary approach. It
integrates traditionally distinct fields such as mathematics, decision theory,
computer science, philosophy, psychology, and cognitive engineering and draws
on specialized engineering domains, particularly energy, mobility, and
aeronautics. The flexibility in its adoption is also demonstrated through its
instantiation on an already existing framework.

</details>

### [217] [Trends in Frontier AI Model Count: A Forecast to 2028](https://arxiv.org/abs/2504.16138)
*Iyngkarran Kumar,Sam Manning*

Main category: cs.CY

TLDR: 论文分析了政府基于训练计算量对AI模型的监管趋势，预测未来几年超过特定FLOP阈值的模型数量将快速增长。


<details>
  <summary>Details</summary>
Motivation: 探讨政府基于训练计算量（如FLOP）对AI模型实施监管的影响，预测未来模型数量是否会超过这些阈值。

Method: 通过统计和预测模型，估计未来几年超过特定FLOP阈值的AI模型数量，并分析其增长趋势。

Result: 预测到2028年，超过欧盟AI法案$10^{25}$ FLOP阈值的模型将有103-306个，超过美国AI Diffusion框架$10^{26}$ FLOP阈值的模型将有45-148个。增长趋势为超线性。

Conclusion: 基于绝对计算量的监管阈值将捕获越来越多的模型，而基于相对计算量的阈值则更稳定。

Abstract: Governments are starting to impose requirements on AI models based on how
much compute was used to train them. For example, the EU AI Act imposes
requirements on providers of general-purpose AI with systemic risk, which
includes systems trained using greater than $10^{25}$ floating point operations
(FLOP). In the United States' AI Diffusion Framework, a training compute
threshold of $10^{26}$ FLOP is used to identify "controlled models" which face
a number of requirements. We explore how many models such training compute
thresholds will capture over time. We estimate that by the end of 2028, there
will be between 103-306 foundation models exceeding the $10^{25}$ FLOP
threshold put forward in the EU AI Act (90% CI), and 45-148 models exceeding
the $10^{26}$ FLOP threshold that defines controlled models in the AI Diffusion
Framework (90% CI). We also find that the number of models exceeding these
absolute compute thresholds each year will increase superlinearly -- that is,
each successive year will see more new models captured within the threshold
than the year before. Thresholds that are defined with respect to the largest
training run to date (for example, such that all models within one order of
magnitude of the largest training run to date are captured by the threshold)
see a more stable trend, with a median forecast of 14-16 models being captured
by this definition annually from 2025-2028.

</details>

### [218] [Enhancing Trust Through Standards: A Comparative Risk-Impact Framework for Aligning ISO AI Standards with Global Ethical and Regulatory Contexts](https://arxiv.org/abs/2504.16139)
*Sridharan Sankaran*

Main category: cs.CY

TLDR: 论文提出了一种比较风险影响评估框架，用于评估ISO AI标准在不同监管环境中的有效性，并提出了增强其全球适用性的建议。


<details>
  <summary>Details</summary>
Motivation: 随着AI重塑行业和社会，确保其可信度（如减少偏见、不透明和责任缺失等伦理风险）是全球性挑战。ISO AI标准旨在促进负责任的发展，但其效果因监管环境不同而异。

Method: 论文引入了一种比较风险影响评估框架，通过将ISO标准映射到欧盟AI法案，并调查十个地区的监管框架，建立了伦理对齐的基线。

Result: 研究发现自愿性ISO标准在执行（如美国科罗拉多州）和地区特定风险（如中国的隐私问题）方面存在不足。建议包括强制性风险审计、地区特定附录和隐私模块。

Conclusion: 该框架为标准化与伦理要求的对齐提供了可复制的工具，有助于全球AI的互操作性和信任。政策制定者和标准机构可利用这些见解改进AI治理。

Abstract: As artificial intelligence (AI) reshapes industries and societies, ensuring
its trustworthiness-through mitigating ethical risks like bias, opacity, and
accountability deficits-remains a global challenge. International Organization
for Standardization (ISO) AI standards, such as ISO/IEC 24027 and 24368, aim to
foster responsible development by embedding fairness, transparency, and risk
management into AI systems. However, their effectiveness varies across diverse
regulatory landscapes, from the EU's risk-based AI Act to China's
stability-focused measures and the U.S.'s fragmented state-led initiatives.
This paper introduces a novel Comparative Risk-Impact Assessment Framework to
evaluate how well ISO standards address ethical risks within these contexts,
proposing enhancements to strengthen their global applicability. By mapping ISO
standards to the EU AI Act and surveying regulatory frameworks in ten
regions-including the UK, Canada, India, Japan, Singapore, South Korea, and
Brazil-we establish a baseline for ethical alignment. The framework, applied to
case studies in the EU, US-Colorado, and China, reveals gaps: voluntary ISO
standards falter in enforcement (e.g., Colorado) and undervalue region-specific
risks like privacy (China). We recommend mandatory risk audits, region-specific
annexes, and a privacy-focused module to enhance ISO's adaptability. This
approach not only synthesizes global trends but also offers a replicable tool
for aligning standardization with ethical imperatives, fostering
interoperability and trust in AI worldwide. Policymakers and standards bodies
can leverage these insights to evolve AI governance, ensuring it meets diverse
societal needs as the technology advances.

</details>

### [219] [Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room](https://arxiv.org/abs/2504.16148)
*Danial Hooshyar,Gustav Šír,Yeongwook Yang,Eve Kikas,Raija Hämäläinen,Tommi Kärkkäinen,Dragan Gašević,Roger Azevedo*

Main category: cs.CY

TLDR: 论文分析了AI在教育领域的九大未解决问题，并提出混合AI方法（神经符号AI）作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在教育系统中有显著进展，但公平性、透明性和有效性仍受挑战，需解决这些关键问题。

Method: 通过理论和实证研究，提出神经符号AI方法来解决九大挑战。

Result: 神经符号AI能有效应对问题，为教育领域提供可信赖的AI系统。

Conclusion: 混合AI方法是解决教育中AI问题的关键，为未来研究提供了方向。

Abstract: Despite significant advancements in AI-driven educational systems and ongoing
calls for responsible AI for education, several critical issues remain
unresolved -- acting as the elephant in the room within AI in education,
learning analytics, educational data mining, learning sciences, and educational
psychology communities. This critical analysis identifies and examines nine
persistent challenges that continue to undermine the fairness, transparency,
and effectiveness of current AI methods and applications in education. These
include: (1) the lack of clarity around what AI for education truly means --
often ignoring the distinct purposes, strengths, and limitations of different
AI families -- and the trend of equating it with domain-agnostic,
company-driven large language models; (2) the widespread neglect of essential
learning processes such as motivation, emotion, and (meta)cognition in
AI-driven learner modelling and their contextual nature; (3) limited
integration of domain knowledge and lack of stakeholder involvement in AI
design and development; (4) continued use of non-sequential machine learning
models on temporal educational data; (5) misuse of non-sequential metrics to
evaluate sequential models; (6) use of unreliable explainable AI methods to
provide explanations for black-box models; (7) ignoring ethical guidelines in
addressing data inconsistencies during model training; (8) use of mainstream AI
methods for pattern discovery and learning analytics without systematic
benchmarking; and (9) overemphasis on global prescriptions while overlooking
localised, student-specific recommendations. Supported by theoretical and
empirical research, we demonstrate how hybrid AI methods -- specifically
neural-symbolic AI -- can address the elephant in the room and serve as the
foundation for responsible, trustworthy AI systems in education.

</details>

### [220] [Leveraging Social Media Analytics for Sustainability Trend Detection in Saudi Arabias Evolving Market](https://arxiv.org/abs/2504.16153)
*Kanwal Aalijah*

Main category: cs.CY

TLDR: 本文探讨了如何利用AI和社交媒体分析实时追踪沙特阿拉伯在《愿景2030》下的可持续发展趋势，为决策者提供跨行业的可靠洞察。


<details>
  <summary>Details</summary>
Motivation: 沙特阿拉伯的快速经济增长和社会变革为实时追踪趋势提供了独特机会，有助于发现商业和投资机会。

Method: 采用AI驱动的方法，分析数百万条社交媒体帖子、新闻和博客，识别可持续发展趋势。

Result: 研究提供了一种可靠的AI方法，帮助决策者理解趋势并做出更好的决策，同时展示了该方法在其他地区的适用潜力。

Conclusion: AI方法为决策者提供了理解公众对倡议的接受程度和趋势发展的可靠工具。

Abstract: Saudi Arabias rapid economic growth and social evolution under Vision 2030
present a unique opportunity to track emerging trends in real time. Uncovering
trends in real time can open up new avenues for business and investment
opportunities. This paper explores how AI and social media analytics can
uncover and monitor these trends across sectors like sustainability,
construction, food beverages industry, tourism, technology, and entertainment.
This paper focus on use of AI-driven methodology to identify sustainability
trends across Saudi Arabia. We processed millions of social media posts, news,
blogs in order to understand sustainability trends in the region. The paper
presents an AI approach that can help economists, businesses, government to
understand sustainability trends and make better decisions around them. This
approach offers both sector-specific and cross-sector insights, giving
decision-makers a reliable, up to date snapshot of Saudi Arabias market shifts.
Beyond Saudi Arabia, this framework also shows potential for adapting to other
regions. Overall, our findings highlight how by using AI-methodologies, give
decision makers a reliable method to understand how initiatives are perceived
and adopted by the public and understand growth of trends.

</details>

### [221] [Cooperative Speech, Semantic Competence, and AI](https://arxiv.org/abs/2504.16092)
*Mahrad Almotahari*

Main category: cs.CY

TLDR: 论文探讨了合作性言语的道德基础，认为大型语言模型（LLMs）缺乏作为合作对话者所需的尊重，因此无法进行断言，并对其语义能力提出质疑。


<details>
  <summary>Details</summary>
Motivation: 研究合作性言语的道德维度，特别是尊重在对话中的作用，以及LLMs是否具备合作对话者的资格。

Method: 通过哲学分析，探讨合作性言语中尊重的必要性及其对LLMs的适用性。

Result: LLMs缺乏合作对话所需的尊重，因此无法进行断言，其语义能力受到质疑。

Conclusion: 知识的含义不仅是认知心理学的研究对象，也是道德心理学的研究对象，LLMs的道德地位限制了其作为合作对话者的能力。

Abstract: Cooperative speech is purposive. From the speaker's perspective, one crucial
purpose is the transmission of knowledge. Cooperative speakers care about
getting things right for their conversational partners. This attitude is a kind
of respect. Cooperative speech is an ideal form of communication because
participants have respect for each other. And having respect within a
cooperative enterprise is sufficient for a particular kind of moral standing:
we ought to respect those who have respect for us. Respect demands reciprocity.
I maintain that large language models aren't owed the kind of respect that
partly constitutes a cooperative conversation. This implies that they aren't
cooperative interlocutors, otherwise we would be obliged to reciprocate the
attitude. Leveraging this conclusion, I argue that present-day LLMs are
incapable of assertion and that this raises an overlooked doubt about their
semantic competence. One upshot of this argument is that knowledge of meaning
isn't just a subject for the cognitive psychologist. It's also a subject for
the moral psychologist.

</details>

### [222] [Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and Interaction Design](https://arxiv.org/abs/2504.16204)
*Christian Djeffal*

Main category: cs.CY

TLDR: 本文探讨了负责任提示工程的重要性，提出一个包含五个组件的框架，以在生成式AI中嵌入伦理和法律考量，平衡技术精确性与伦理意识。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的普及，提示工程对社会公平、问责和透明性具有深远影响，需要将伦理和社会价值直接融入AI交互。

Method: 提出一个负责任提示工程的综合框架，包括提示设计、系统选择、系统配置、性能评估和提示管理五个组件。

Result: 实证研究表明，该框架能改善社会结果并降低风险，强调技术精确性与伦理意识的平衡。

Conclusion: 负责任提示工程是AI开发与部署的关键桥梁，未来需进一步研究和实践指导以推动该领域发展。

Abstract: Responsible prompt engineering has emerged as a critical framework for
ensuring that generative artificial intelligence (AI) systems serve society's
needs while minimizing potential harms. As generative AI applications become
increasingly powerful and ubiquitous, the way we instruct and interact with
them through prompts has profound implications for fairness, accountability,
and transparency. This article examines how strategic prompt engineering can
embed ethical and legal considerations and societal values directly into AI
interactions, moving beyond mere technical optimization for functionality. This
article proposes a comprehensive framework for responsible prompt engineering
that encompasses five interconnected components: prompt design, system
selection, system configuration, performance evaluation, and prompt management.
Drawing from empirical evidence, the paper demonstrates how each component can
be leveraged to promote improved societal outcomes while mitigating potential
risks. The analysis reveals that effective prompt engineering requires a
delicate balance between technical precision and ethical consciousness,
combining the systematic rigor and focus on functionality with the nuanced
understanding of social impact. Through examination of real-world and emerging
practices, the article illustrates how responsible prompt engineering serves as
a crucial bridge between AI development and deployment, enabling organizations
to fine-tune AI outputs without modifying underlying model architectures. This
approach aligns with broader "Responsibility by Design" principles, embedding
ethical considerations directly into the implementation process rather than
treating them as post-hoc additions. The article concludes by identifying key
research directions and practical guidelines for advancing the field of
responsible prompt engineering.

</details>

### [223] [Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark](https://arxiv.org/abs/2504.16137)
*Jasper Götting,Pedro Medeiros,Jon G Sanders,Nathaniel Li,Long Phan,Karam Elabd,Lennart Justen,Dan Hendrycks,Seth Donoughe*

Main category: cs.CY

TLDR: VCT是一个评估大语言模型在复杂病毒学实验室协议中解决问题能力的基准测试，结果显示顶级LLM表现优于专家病毒学家，引发双重用途技术治理的讨论。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在病毒学实验室实践中的能力，并探讨其潜在的双重用途风险。

Method: 通过322个多模态问题构建VCT基准，涵盖病毒学实验室的基础、隐性及视觉知识。

Result: 顶级LLM（OpenAI的o3）准确率达43.8%，优于94%的专家病毒学家。

Conclusion: LLM在病毒学领域的专家级能力需纳入现有双重用途技术治理框架。

Abstract: We present the Virology Capabilities Test (VCT), a large language model (LLM)
benchmark that measures the capability to troubleshoot complex virology
laboratory protocols. Constructed from the inputs of dozens of PhD-level expert
virologists, VCT consists of $322$ multimodal questions covering fundamental,
tacit, and visual knowledge that is essential for practical work in virology
laboratories. VCT is difficult: expert virologists with access to the internet
score an average of $22.1\%$ on questions specifically in their sub-areas of
expertise. However, the most performant LLM, OpenAI's o3, reaches $43.8\%$
accuracy, outperforming $94\%$ of expert virologists even within their
sub-areas of specialization. The ability to provide expert-level virology
troubleshooting is inherently dual-use: it is useful for beneficial research,
but it can also be misused. Therefore, the fact that publicly available models
outperform virologists on VCT raises pressing governance considerations. We
propose that the capability of LLMs to provide expert-level troubleshooting of
dual-use virology work should be integrated into existing frameworks for
handling dual-use technologies in the life sciences.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [224] [HUG: Hierarchical Urban Gaussian Splatting with Block-Based Reconstruction](https://arxiv.org/abs/2504.16606)
*Zhongtao Wang,Mai Su,Huishan Au,Yilong Li,Xizhe Cao,Chengwei Pan,Yisong Chen,Guoping Wang*

Main category: cs.GR

TLDR: HUG是一种基于3D高斯泼溅的新方法，通过分层神经高斯表示优化大规模城市环境的重建与渲染，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着城市3D场景复杂度增加，高效的重建与渲染技术需求迫切。

Method: 采用分层神经高斯表示和增强的块级重建流程，减少冗余训练区域。

Result: 在公共基准测试中取得领先结果，证明其在大规模城市场景中的有效性。

Conclusion: HUG在低计算成本下实现高质量渲染，适用于复杂城市环境。

Abstract: As urban 3D scenes become increasingly complex and the demand for
high-quality rendering grows, efficient scene reconstruction and rendering
techniques become crucial. We present HUG, a novel approach to address
inefficiencies in handling large-scale urban environments and intricate details
based on 3D Gaussian splatting. Our method optimizes data partitioning and the
reconstruction pipeline by incorporating a hierarchical neural Gaussian
representation. We employ an enhanced block-based reconstruction pipeline
focusing on improving reconstruction quality within each block and reducing the
need for redundant training regions around block boundaries. By integrating
neural Gaussian representation with a hierarchical architecture, we achieve
high-quality scene rendering at a low computational cost. This is demonstrated
by our state-of-the-art results on public benchmarks, which prove the
effectiveness and advantages in large-scale urban scene representation.

</details>

<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [225] [Breaking scaling relations with inverse catalysts: a machine learning exploration of trends in $\mathrm{CO_2}$ hydrogenation energy barriers](https://arxiv.org/abs/2504.16493)
*Luuk H. E. Kempen,Marius Juul Nielsen,Mie Andersen*

Main category: cond-mat.mtrl-sci

TLDR: 论文提出了一种基于机器学习的工作流程，用于探索逆催化剂中反应过渡态，以加速CO2转化为甲醇的催化剂开发。


<details>
  <summary>Details</summary>
Motivation: 减少CO2排放和依赖化石燃料的需求，开发高效催化剂是关键，但传统方法成本高且耗时。

Method: 使用神经网络机器学习原子间势能，探索氧化铟纳米团簇在Cu(111)上的反应过渡态。

Result: 该方法显著加速了活性位点的探索，揭示了纳米团簇边缘和内部的结构-活性关系，并打破了线性比例关系。

Conclusion: 逆催化剂在实验中表现优异的原因可能是其打破了线性比例关系，该方法为催化剂设计提供了新思路。

Abstract: The conversion of $\mathrm{CO_2}$ into useful products such as methanol is a
key strategy for abating climate change and our dependence on fossil fuels.
Developing new catalysts for this process is costly and time-consuming and can
thus benefit from computational exploration of possible active sites. However,
this is complicated by the complexity of the materials and reaction networks.
Here, we present a workflow for exploring transition states of elementary
reaction steps at inverse catalysts, which is based on the training of a neural
network-based machine learning interatomic potential. We focus on the crucial
formate intermediate and its formation over nanoclusters of indium oxide
supported on Cu(111). The speedup compared to an approach purely based on
density functional theory allows us to probe a wide variety of active sites
found at nanoclusters of different sizes and stoichiometries. Analysis of the
obtained set of transition state geometries reveals different
structure--activity trends at the edge or interior of the nanoclusters.
Furthermore, the identified geometries allow for the breaking of linear scaling
relations, which could be a key underlying reason for the excellent catalytic
performance of inverse catalysts observed in experiments.

</details>

<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [226] [Introduction to Quantum Machine Learning and Quantum Architecture Search](https://arxiv.org/abs/2504.16131)
*Samuel Yen-Chi Chen,Zhiding Liang*

Main category: quant-ph

TLDR: 量子计算与机器学习的结合推动了量子机器学习（QML）的发展，旨在通过量子原理提升ML算法性能，并探索自动化设计高性能量子电路架构的方法。


<details>
  <summary>Details</summary>
Motivation: 量子计算和机器学习的快速发展促使研究者探索两者的结合，以提升ML算法的性能并降低量子计算的使用门槛。

Method: 通过系统化和自动化的方法设计高性能量子电路架构，使非量子领域的研究者也能有效利用量子增强工具。

Result: QML的突破性进展有望拓展其在不同领域的应用范围。

Conclusion: QML的进一步发展将推动量子计算和机器学习的融合，为跨学科研究开辟新方向。

Abstract: Recent advancements in quantum computing (QC) and machine learning (ML) have
fueled significant research efforts aimed at integrating these two
transformative technologies. Quantum machine learning (QML), an emerging
interdisciplinary field, leverages quantum principles to enhance the
performance of ML algorithms. Concurrently, the exploration of systematic and
automated approaches for designing high-performance quantum circuit
architectures for QML tasks has gained prominence, as these methods empower
researchers outside the quantum computing domain to effectively utilize
quantum-enhanced tools. This tutorial will provide an in-depth overview of
recent breakthroughs in both areas, highlighting their potential to expand the
application landscape of QML across diverse fields.

</details>

### [227] [Public-Key Quantum Fire and Key-Fire From Classical Oracles](https://arxiv.org/abs/2504.16407)
*Alper Çakan,Vipul Goyal,Omri Shmueli*

Main category: quant-ph

TLDR: 本文首次在经典预言机模型下构建了公钥量子火（quantum fire），并证明了其无条件安全性，同时提出了更强的量子密钥火（quantum key-fire）概念及其应用。


<details>
  <summary>Details</summary>
Motivation: 量子火是一种可高效克隆但无法转换为经典字符串的量子态分布，此前的工作仅基于低效的酉预言机或未完全证明安全性。本文旨在填补这一空白。

Method: 通过经典预言机模型构建公钥量子火，并引入量子密钥火概念，利用后量子单向函数实现高效预言机。

Result: 成功构建了首个无条件安全的公钥量子火，并证明了量子密钥火的安全性，应用于无界泄漏抗性加密方案。

Conclusion: 本文为量子力学中的无克隆和无电报原理提供了首个经典预言机分离，并扩展了量子火的应用场景。

Abstract: Quantum fire was recently formalized by Bostanci, Nehoran and Zhandry (STOC
25). This notion considers a distribution of quantum states that can be
efficiently cloned, but cannot be converted into a classical string.
Previously, work of Nehoran and Zhandry (ITCS 24) showed how to construct
quantum fire relative to an inefficient unitary oracle. Later, the work of
Bostanci, Nehoran, Zhandry gave a candidate construction based on group action
assumptions, and proved the correctness of their scheme; however, even in the
classical oracle model they only conjectured the security, and no security
proof was given.
  In this work, we give the first construction of public-key quantum fire
relative to a classical oracle, and prove its security unconditionally. This
gives the first classical oracle seperation between the two fundamental
principles of quantum mechanics that are equivalent in the
information-theoretic setting: no-cloning and no-telegraphing.
  Going further, we introduce a stronger notion called quantum key-fire where
the clonable fire states can be used to run a functionality (such as a signing
or decryption key), and prove a secure construction relative to a classical
oracle. As an application of this notion, we get the first public-key
encryption scheme whose secret key is clonable but satisfies unbounded
leakage-resilience (Cakan, Goyal, Liu-Zhang, Ribeiro [TCC 24]), relative to a
classical oracle. Unbounded leakage-resilience is closely related to, and can
be seen as a generalization of the notion of no-telegraphing.
  For all of our constructions, the oracles can be made efficient (i.e.
polynomial time), assuming the existence of post-quantum one-way functions.

</details>

### [228] [Resource Reduction in Multiparty Quantum Secret Sharing of both Classical and Quantum Information under Noisy Scenario](https://arxiv.org/abs/2504.16709)
*Nirupam Basak,Goutam Paul*

Main category: quant-ph

TLDR: 论文提出了一种高效的量子纠错方案，用于减少量子秘密共享协议中的噪声影响，显著提升了协议的实用性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 量子秘密共享（QSS）在信息分发中具有安全性优势，但易受噪声影响。研究旨在解决噪声对QSS协议的干扰问题。

Method: 基于简化的Shor码，提出了一种高效的量子纠错方案，将量子比特开销从9减少到3，同时降低平均错误率。

Result: 模拟结果显示，该方案显著提升了协议的抗噪声能力，使其更适合实际应用。

Conclusion: 该方案不仅适用于QSS协议，还可推广到其他基于单量子比特的量子协议中。

Abstract: Quantum secret sharing (QSS) enables secure distribution of information among
multiple parties but remains vulnerable to noise. We analyze the effects of
bit-flip, phase-flip, and amplitude damping noise on the multiparty QSS for
classical message (QSSCM) and secret sharing of quantum information (SSQI)
protocols proposed by Zhang et al. (Phys. Rev. A, 71:044301, 2005). To scale
down these effects, we introduce an efficient quantum error correction (QEC)
scheme based on a simplified version of Shor's code. Leveraging the specific
structure of the QSS protocols, we reduce the qubit overhead from the standard
9 of Shor's code to as few as 3 while still achieving lower average error rates
than existing QEC methods. Thus, our approach can also be adopted for other
single-qubit-based quantum protocols. Simulations demonstrate that our approach
significantly enhances the protocols' resilience, improving their practicality
for real-world deployment.

</details>

### [229] [The Sponge is Quantum Indifferentiable](https://arxiv.org/abs/2504.16887)
*Gorjan Alagic,Joseph Carolan,Christian Majenz,Saliha Tokat*

Main category: quant-ph

TLDR: 本文证明了海绵构造在量子敌手下的不可区分性，填补了后量子密码学中SHA-3安全性研究的空白。


<details>
  <summary>Details</summary>
Motivation: 研究海绵构造在量子敌手下的安全性，尤其是其不可区分性，以支持后量子密码学的广泛应用。

Method: 开发了一种专门技术，克服了量子环境下懒惰采样技术的不足，证明了海绵构造的不可区分性。

Result: 海绵构造在量子敌手下具有不可区分性，且给出了更紧的预像和碰撞抗性界限。

Conclusion: 海绵构造（如SHA-3）在后量子环境下是安全的，为后量子密码学提供了理论支持。

Abstract: The sponge is a cryptographic construction that turns a public permutation
into a hash function. When instantiated with the Keccak permutation, the sponge
forms the NIST SHA-3 standard. SHA-3 is a core component of most post-quantum
public-key cryptography schemes slated for worldwide adoption.
  While one can consider many security properties for the sponge, the ultimate
one is indifferentiability from a random oracle, or simply indifferentiability.
The sponge was proved indifferentiable against classical adversaries by Bertoni
et al. in 2008. Despite significant efforts in the years since, little is known
about sponge security against quantum adversaries, even for simple properties
like preimage or collision resistance beyond a single round. This is primarily
due to the lack of a satisfactory quantum analog of the lazy sampling technique
for permutations.
  In this work, we develop a specialized technique that overcomes this barrier
in the case of the sponge. We prove that the sponge is in fact indifferentiable
from a random oracle against quantum adversaries. Our result establishes that
the domain extension technique behind SHA-3 is secure in the post-quantum
setting. Our indifferentiability bound for the sponge is a loose
$O(\mathsf{poly}(q) 2^{-\mathsf{min}(r, c)/4})$, but we also give bounds on
preimage and collision resistance that are tighter.

</details>

### [230] [QAOA-GPT: Efficient Generation of Adaptive and Regular Quantum Approximate Optimization Algorithm Circuits](https://arxiv.org/abs/2504.16350)
*Ilya Tyagin,Marwa H. Farag,Kyle Sherbert,Karunya Shirali,Yuri Alexeev,Ilya Safro*

Main category: quant-ph

TLDR: QAOA-GPT是一个结合生成预训练变换器（GPT）的框架，用于直接合成解决二次无约束二进制优化问题的量子电路，并在MaxCut问题上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 量子计算在解决某些经典计算机难以处理的优化问题上具有潜力，但现有方法如QAOA存在计算开销大的问题。

Method: 使用自适应QAOA方法生成合成数据集，训练GPT模型直接合成量子电路，并在未见过的图实例上测试。

Result: QAOA-GPT能生成高质量量子电路，显著降低经典QAOA和自适应方法的计算开销。

Conclusion: 生成式AI是生成紧凑量子电路的可扩展途径。

Abstract: Quantum computing has the potential to improve our ability to solve certain
optimization problems that are computationally difficult for classical
computers, by offering new algorithmic approaches that may provide speedups
under specific conditions. In this work, we introduce QAOA-GPT, a generative
framework that leverages Generative Pretrained Transformers (GPT) to directly
synthesize quantum circuits for solving quadratic unconstrained binary
optimization problems, and demonstrate it on the MaxCut problem on graphs. To
diversify the training circuits and ensure their quality, we have generated a
synthetic dataset using the adaptive QAOA approach, a method that incrementally
builds and optimizes problem-specific circuits. The experiments conducted on a
curated set of graph instances demonstrate that QAOA-GPT, generates high
quality quantum circuits for new problem instances unseen in the training as
well as successfully parametrizes QAOA. Our results show that using QAOA-GPT to
generate quantum circuits will significantly decrease both the computational
overhead of classical QAOA and adaptive approaches that often use gradient
evaluation to generate the circuit and the classical optimization of the
circuit parameters. Our work shows that generative AI could be a promising
avenue to generate compact quantum circuits in a scalable way.

</details>

### [231] [Deep Neural Network Emulation of the Quantum-Classical Transition via Learned Wigner Function Dynamics](https://arxiv.org/abs/2504.16334)
*Kamran Majid*

Main category: quant-ph

TLDR: 本文提出了一种基于深度神经网络的方法，直接从初始量子态参数和普朗克常数预测时间演化的Wigner函数参数，成功模拟了量子-经典过渡。


<details>
  <summary>Details</summary>
Motivation: 研究量子力学中经典行为如何从普朗克常数趋近于零时涌现，这是一个基础物理问题。

Method: 使用深度前馈神经网络，训练其从高斯波包的初始参数和普朗克常数预测相空间中Wigner函数的动态映射。

Result: 网络训练损失约为0.0390，能准确捕捉Wigner函数的动态映射，模拟量子-经典过渡。

Conclusion: 该方法为研究量子力学基础问题提供了新的计算视角，超越了以往基于可观测量映射的研究。

Abstract: The emergence of classical behavior from quantum mechanics as Planck's
constant $\hbar$ approaches zero remains a fundamental challenge in physics
[1-3]. This paper introduces a novel approach employing deep neural networks to
directly learn the dynamical mapping from initial quantum state parameters (for
Gaussian wave packets of the one-dimensional harmonic oscillator) and $\hbar$
to the parameters of the time-evolved Wigner function in phase space [4-6]. A
comprehensive dataset of analytically derived time-evolved Wigner functions was
generated, and a deep feedforward neural network with an enhanced architecture
was successfully trained for this prediction task, achieving a final training
loss of ~ 0.0390. The network demonstrates a significant and previously
unrealized ability to accurately capture the underlying mapping of the Wigner
function dynamics. This allows for a direct emulation of the quantum-classical
transition by predicting the evolution of phase-space distributions as $\hbar$
is systematically varied. The implications of these findings for providing a
new computational lens on the emergence of classicality are discussed,
highlighting the potential of this direct phase-space learning approach for
studying fundamental aspects of quantum mechanics. This work presents a
significant advancement beyond previous efforts that focused on learning
observable mappings [7], offering a direct route via the phase-space
representation.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [232] [Quality of explanation of xAI from the prespective of Italian end-users: Italian version of System Causability Scale (SCS)](https://arxiv.org/abs/2504.16193)
*Carmine Attanasio,Alireza Mortezapour*

Main category: cs.HC

TLDR: 研究验证了意大利语版本的系统可解释性量表（I-SCS），用于评估xAI系统提供的解释质量。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能应用范围的扩大，研究者关注如何提供高质量的算法解释。本研究旨在验证意大利语版本的量表。

Method: 采用前向-后向翻译方法，计算内容效度指数/比率，并进行认知访谈。

Result: 原始问卷10个问题中，1个因效度不足被移除，最终意大利语版本包含9个问题，用户理解良好。

Conclusion: 意大利语版本的量表可用于未来研究和xAI开发，评估意大利文化中的解释质量。

Abstract: Background and aim: Considering the scope of the application of artificial
intelligence beyond the field of computer science, one of the concerns of
researchers is to provide quality explanations about the functioning of
algorithms based on artificial intelligence and the data extracted from it. The
purpose of the present study is to validate the Italian version of system
causability scale (I-SCS) to measure the quality of explanations provided in a
xAI.
  Method: For this purpose, the English version, initially provided in 2020 in
coordination with the main developer, was utilized. The forward-backward
translation method was applied to ensure accuracy. Finally, these nine steps
were completed by calculating the content validity index/ratio and conducting
cognitive interviews with representative end users.
  Results: The original version of the questionnaire consisted of 10 questions.
However, based on the obtained indexes (CVR below 0.49), one question (Question
8) was entirely removed. After completing the aforementioned steps, the Italian
version contained 9 questions. The representative sample of Italian end users
fully comprehended the meaning and content of the questions in the Italian
version.
  Conclusion: The Italian version obtained in this study can be used in future
research studies as well as in the field by xAI developers. This tool can be
used to measure the quality of explanations provided for an xAI system in
Italian culture.

</details>

### [233] [Cyberoception: Finding a Painlessly-Measurable New Sense in the Cyberworld Towards Emotion-Awareness in Computing](https://arxiv.org/abs/2504.16378)
*Tadashi Okoshi,Zexiong Gao,Tan Yi Zhen,Takumi Karasawa,Takeshi Miki,Wataru Sasaki,Rajesh K. Balan*

Main category: cs.HC

TLDR: 论文提出了一种新概念“cyberoception”，用于通过智能手机传感器在用户日常生活中测量类似内感受的状态，并发现其与情绪效价显著相关。


<details>
  <summary>Details</summary>
Motivation: 现有内感受测量方法依赖实验室环境和精密设备，难以在现实生活中监测用户的内感受状态。

Method: 提出“cyberoception”概念，并通过10天的实验室与野外混合实验验证其与情绪效价的关系。

Result: 发现一种特定类型的“cyberoception”（“Turn On”）与参与者的情绪效价显著相关。

Conclusion: “cyberoception”可作为开发更“情感感知”应用的基础。

Abstract: In Affective computing, recognizing users' emotions accurately is the basis
of affective human-computer interaction. Understanding users' interoception
contributes to a better understanding of individually different emotional
abilities, which is essential for achieving inter-individually accurate emotion
estimation. However, existing interoception measurement methods, such as the
heart rate discrimination task, have several limitations, including their
dependence on a well-controlled laboratory environment and precision apparatus,
making monitoring users' interoception challenging. This study aims to
determine other forms of data that can explain users' interoceptive or similar
states in their real-world lives and propose a novel hypothetical concept
"cyberoception," a new sense (1) which has properties similar to interoception
in terms of the correlation with other emotion-related abilities, and (2) which
can be measured only by the sensors embedded inside commodity smartphone
devices in users' daily lives. Results from a 10-day-long in-lab/in-the-wild
hybrid experiment reveal a specific cyberoception type "Turn On" (users'
subjective sensory perception about the frequency of turning-on behavior on
their smartphones), significantly related to participants' emotional valence.
We anticipate that cyberoception to serve as a fundamental building block for
developing more "emotion-aware", user-friendly applications and services.

</details>

### [234] [FeedQUAC: Quick Unobtrusive AI-Generated Commentary](https://arxiv.org/abs/2504.16416)
*Tao Long,Kendra Wannamaker,Jo Vermeulen,George Fitzmaurice,Justin Matejka*

Main category: cs.HC

TLDR: 论文探讨了AI如何通过实时反馈工具FeedQUAC，为设计师提供便捷、轻松的反馈，从而提升创意工作流程。


<details>
  <summary>Details</summary>
Motivation: 设计过程中持续获取反馈是劳动密集型且可能干扰工作，AI可以填补这一空白。

Method: 引入FeedQUAC工具，提供多视角实时AI反馈，并通过8名参与者的设计探针研究验证效果。

Result: 参与者反馈工具提升了便利性、趣味性、自信心和灵感，同时提出改进建议。

Conclusion: AI反馈在创意支持系统中具有潜力，需平衡用户参与，环境交互是未来设计的重要考量。

Abstract: Design thrives on feedback. However, gathering constant feedback throughout
the design process can be labor-intensive and disruptive. We explore how AI can
bridge this gap by providing effortless, ambient feedback. We introduce
FeedQUAC, a design companion that delivers real-time AI-generated commentary
from a variety of perspectives through different personas. A design probe study
with eight participants highlights how designers can leverage quick yet ambient
AI feedback to enhance their creative workflows. Participants highlight
benefits such as convenience, playfulness, confidence boost, and inspiration
from this lightweight feedback agent, while suggesting additional features,
like chat interaction and context curation. We discuss the role of AI feedback,
its strengths and limitations, and how to integrate it into existing design
workflows while balancing user involvement. Our findings also suggest that
ambient interaction is a valuable consideration for both the design and
evaluation of future creativity support systems.

</details>

### [235] [Exploring human-SAV interaction using large language models: The impact of psychological ownership and anthropomorphism on user experience](https://arxiv.org/abs/2504.16548)
*Lirui Guo,Michael G. Burke,Wynita M. Griggs*

Main category: cs.HC

TLDR: 研究探讨了LLM驱动的共享自动驾驶汽车（SAV）用户界面中提示策略如何影响用户感知、体验和采用意图，发现更具拟人化和心理所有权触发的设计能提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注LLM驱动的SAV用户界面中提示策略对用户心理因素的影响，尤其是心理所有权和拟人化。

Method: 设计了四种具有不同拟人化特征和心理所有权触发器的SAV界面，通过定量和定性方法收集用户反馈。

Result: 更具拟人化和心理所有权触发的SAV界面提升了用户对SAV人性化特质的感知，并改善了用户情感反馈。

Conclusion: 研究为设计LLM驱动的SAV对话界面提供了实用指导，以增强用户体验和采用意愿。

Abstract: There has been extensive prior work exploring how psychological factors such
as anthropomorphism affect the adoption of shared autonomous vehicles (SAVs).
However, limited research has been conducted on how prompt strategies in large
language model (LLM)-powered SAV User Interfaces (UIs) affect users'
perceptions, experiences, and intentions to adopt such technology. In this
work, we investigate how conversational UIs powered by LLMs drive these
psychological factors and psychological ownership, the sense of possession a
user may come to feel towards an entity or object they may not legally own. We
designed four SAV UIs with varying levels of anthropomorphic characteristics
and psychological ownership triggers. Quantitative measures of psychological
ownership, anthropomorphism, quality of service, disclosure tendency, sentiment
of SAV responses, and overall acceptance were collected after participants
interacted with each SAV. Qualitative feedback was also gathered regarding the
experience of psychological ownership during the interactions. The results
indicate that an SAV conversational UI designed to be more anthropomorphic and
to induce psychological ownership improved users' perceptions of the SAV's
human-like qualities and improved the sentiment of responses compared to a
control condition. These findings provide practical guidance for designing
LLM-based conversational UIs that enhance user experience and adoption of SAVs.

</details>

### [236] [A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and Environments](https://arxiv.org/abs/2504.16562)
*Julian Rasch,Florian Müller,Francesco Chiossi*

Main category: cs.HC

TLDR: 本文探讨了AI驱动的动态AR内容布局，旨在通过机器学习优化AR体验，减少用户认知负担，并推动多行业创新。


<details>
  <summary>Details</summary>
Motivation: 现有AR系统在管理交互可能性方面存在不足，需要更智能的方法来动态调整内容布局以适应环境和用户行为。

Method: 提出利用机器学习方法，动态管理AR内容分布，结合环境变化和用户移动，优化UI布局。

Result: 设想中的系统能够更智能地分配AR内容，提升沉浸感和用户体验，同时降低认知负担。

Conclusion: AI驱动的动态AR内容布局有望为多领域带来创新，如导航、工作和学习，推动更直观有效的AR体验。

Abstract: Augmented Reality (AR) is transforming the way we interact with virtual
information in the physical world. By overlaying digital content in real-world
environments, AR enables new forms of immersive and engaging experiences.
However, existing AR systems often struggle to effectively manage the many
interactive possibilities that AR presents. This vision paper speculates on
AI-driven approaches for adaptive AR content placement, dynamically adjusting
to user movement and environmental changes. By leveraging machine learning
methods, such a system would intelligently manage content distribution between
AR projections integrated into the external environment and fixed static
content, enabling seamless UI layout and potentially reducing users' cognitive
load. By exploring the possibilities of AI-driven dynamic AR content placement,
we aim to envision new opportunities for innovation and improvement in various
industries, from urban navigation and workplace productivity to immersive
learning and beyond. This paper outlines a vision for the development of more
intuitive, engaging, and effective AI-powered AR experiences.

</details>

### [237] [PsyCounAssist: A Full-Cycle AI-Powered Psychological Counseling Assistant System](https://arxiv.org/abs/2504.16573)
*Xianghe Liu,Jiaqi Xu,Tao Sun*

Main category: cs.HC

TLDR: PsyCounAssist是一款AI驱动的心理咨询辅助系统，结合多模态情绪识别和自动报告功能，提升心理咨询效率。


<details>
  <summary>Details</summary>
Motivation: 心理咨询需要动态监控情绪变化并保持连续性，传统方法效率有限。

Method: 整合语音和PPG信号进行实时情感分析，利用大语言模型生成结构化报告，提供个性化随访支持。

Result: 实验验证了PPG情绪分类的可靠性，系统在真实场景中表现出实用性和隐私保护性。

Conclusion: PsyCounAssist为AI在心理咨询中的伦理和有效整合提供了新思路。

Abstract: Psychological counseling is a highly personalized and dynamic process that
requires therapists to continuously monitor emotional changes, document session
insights, and maintain therapeutic continuity. In this paper, we introduce
PsyCounAssist, a comprehensive AI-powered counseling assistant system
specifically designed to augment psychological counseling practices.
PsyCounAssist integrates multimodal emotion recognition combining speech and
photoplethysmography (PPG) signals for accurate real-time affective analysis,
automated structured session reporting using large language models (LLMs), and
personalized AI-generated follow-up support. Deployed on Android-based tablet
devices, the system demonstrates practical applicability and flexibility in
real-world counseling scenarios. Experimental evaluation confirms the
reliability of PPG-based emotional classification and highlights the system's
potential for non-intrusive, privacy-aware emotional support. PsyCounAssist
represents a novel approach to ethically and effectively integrating AI into
psychological counseling workflows.

</details>

<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [238] [HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing](https://arxiv.org/abs/2504.16112)
*Myunghyun Rhee,Joonseop Sim,Taeyoung Ahn,Seungyong Lee,Daegun Yoon,Euiseok Kim,Kyoung Park,Youngpyo Joo,Hosik Kim*

Main category: cs.AR

TLDR: 论文提出了一种高带宽处理单元（HPU），作为GPU的协处理器，通过卸载内存密集型任务提升大批次LLM推理的效率。


<details>
  <summary>Details</summary>
Motivation: 当前GPU系统在Transformer LLMs的注意力层中存在效率问题，主要由于低操作强度和KV缓存的高内存需求。

Method: 设计并实现了一种基于PCIe FPGA卡的HPU原型，作为GPU的协处理器，专注于内存密集型任务。

Result: GPU-HPU异构系统相比纯GPU系统实现了4.1倍的性能提升和4.6倍的能效改进。

Conclusion: HPU提供了一种无需增加GPU数量即可扩展系统性能的解决方案。

Abstract: The attention layer, a core component of Transformer-based LLMs, brings out
inefficiencies in current GPU systems due to its low operational intensity and
the substantial memory requirements of KV caches. We propose a High-bandwidth
Processing Unit (HPU), a memoryintensive co-processor that enhances GPU
resource utilization during large-batched LLM inference. By offloading
memory-bound operations, the HPU allows the GPU to focus on compute-intensive
tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales
out to accommodate surging memory demands driven by large batch sizes and
extended sequence lengths. In this paper, we show the HPU prototype implemented
with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU
heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy
efficiency improvements over a GPUonly system, providing scalability without
increasing the number of GPUs.

</details>

### [239] [FPGA-Based Neural Network Accelerators for Space Applications: A Survey](https://arxiv.org/abs/2504.16173)
*Pedro Antunes,Artur Podobas*

Main category: cs.AR

TLDR: 本文探讨了FPGA在航天任务中作为神经网络加速器的潜力，总结了现有研究并提出了未来方向。


<details>
  <summary>Details</summary>
Motivation: 航天任务对高性能计算的需求增加，FPGA因其灵活性和抗辐射性成为理想选择，神经网络在自主操作和数据分析中的应用也日益重要。

Method: 通过文献分析，识别趋势与不足，并提出未来研究方向。

Result: FPGA加速器有望提升航天器计算系统性能。

Conclusion: FPGA与神经网络的结合为航天任务提供了高效计算解决方案，未来研究需进一步优化。

Abstract: Space missions are becoming increasingly ambitious, necessitating
high-performance onboard spacecraft computing systems. In response,
field-programmable gate arrays (FPGAs) have garnered significant interest due
to their flexibility, cost-effectiveness, and radiation tolerance potential.
Concurrently, neural networks (NNs) are being recognized for their capability
to execute space mission tasks such as autonomous operations, sensor data
analysis, and data compression. This survey serves as a valuable resource for
researchers aiming to implement FPGA-based NN accelerators in space
applications. By analyzing existing literature, identifying trends and gaps,
and proposing future research directions, this work highlights the potential of
these accelerators to enhance onboard computing systems.

</details>

### [240] [TeLLMe: An Energy-Efficient Ternary LLM Accelerator for Prefilling and Decoding on Edge FPGAs](https://arxiv.org/abs/2504.16266)
*Ye Qiao,Zhiheng Cheng,Yifan Zhang,Yian Wang,Sitao Huang*

Main category: cs.AR

TLDR: TeLLMe是一种针对低功耗FPGA的三元大型语言模型加速器，支持1.58位权重和8位激活，显著提升了边缘设备上的能效和性能。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署大型语言模型面临计算和内存需求高、资源有限、功耗预算低以及预填充阶段延迟的挑战。

Method: 提出了一种基于表查找的矩阵引擎、融合的带宽高效注意力模块以及优化的归一化和量化-反量化单元。

Result: 在7W功耗下，TeLLMe实现了9 tokens/s的吞吐量，预填充延迟为0.55--1.15 s，显著提升了能效。

Conclusion: TeLLMe为生成式AI在边缘FPGA上的部署设定了新的能效基准。

Abstract: Deploying large language models (LLMs) on edge platforms is challenged by
their high computational and memory demands. Although recent low-bit
quantization methods (e.g., BitNet, DeepSeek) compress weights to as little as
1.58 bits with minimal accuracy loss, edge deployment is still constrained by
limited on-chip resources, power budgets, and the often-neglected latency of
the prefill phase. We present TeLLMe, the first ternary LLM accelerator for
low-power FPGAs (e.g., AMD KV260) that fully supports both prefill and
autoregressive decoding using 1.58-bit weights and 8-bit activations. Our
contributions include: (1) a table-lookup matrix engine for ternary matmul that
merges grouped activations with online precomputation to minimize resource use;
(2) a fused, bandwidth-efficient attention module featuring a reversed
reordering scheme to accelerate prefill; and (3) a tightly integrated
normalization and quantization--dequantization unit optimized for ultra-low-bit
inference. Under a 7W power budget, TeLLMe delivers up to 9 tokens/s throughput
over 1,024-token contexts and prefill latencies of 0.55--1.15 s for 64--128
token prompts, marking a significant energy-efficiency advance and establishing
a new edge FPGA benchmark for generative AI.

</details>

### [241] [COBRA: Algorithm-Architecture Co-optimized Binary Transformer Accelerator for Edge Inference](https://arxiv.org/abs/2504.16269)
*Ye Qiao,Zhiheng Cheng,Yian Wang,Yifan Zhang,Yunzhe Deng,Sitao Huang*

Main category: cs.AR

TLDR: COBRA是一种算法-架构协同优化的二进制Transformer加速器，专为边缘计算设计，显著提升了能效和吞吐量。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在边缘平台部署时面临模型大、计算资源需求高的问题，现有二进制Transformer因缺乏硬件优化而效率低下。

Method: COBRA采用1-bit二进制乘法单元，支持-1、0、+1的矩阵运算，并在注意力块中进行了硬件友好优化。

Result: 在边缘FPGA上，COBRA实现了3,894.7 GOPS的吞吐量和448.7 GOPS/W的能效，比GPU提升311倍能效，比现有二进制加速器提升3.5倍吞吐量。

Conclusion: COBRA为边缘计算提供了一种高效、低功耗的二进制Transformer加速方案，且几乎不影响推理精度。

Abstract: Transformer-based models have demonstrated superior performance in various
fields, including natural language processing and computer vision. However,
their enormous model size and high demands in computation, memory, and
communication limit their deployment to edge platforms for local, secure
inference. Binary transformers offer a compact, low-complexity solution for
edge deployment with reduced bandwidth needs and acceptable accuracy. However,
existing binary transformers perform inefficiently on current hardware due to
the lack of binary specific optimizations. To address this, we introduce COBRA,
an algorithm-architecture co-optimized binary Transformer accelerator for edge
computing. COBRA features a real 1-bit binary multiplication unit, enabling
matrix operations with -1, 0, and +1 values, surpassing ternary methods. With
further hardware-friendly optimizations in the attention block, COBRA achieves
up to 3,894.7 GOPS throughput and 448.7 GOPS/Watt energy efficiency on edge
FPGAs, delivering a 311x energy efficiency improvement over GPUs and a 3.5x
throughput improvement over the state-of-the-art binary accelerator, with only
negligible inference accuracy degradation.

</details>

<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [242] [Common Functional Decompositions Can Mis-attribute Differences in Outcomes Between Populations](https://arxiv.org/abs/2504.16864)
*Manuel Quintero,William T. Stephenson,Advik Shreekumar,Tamara Broderick*

Main category: stat.ME

TLDR: 论文探讨了如何扩展Kitagawa-Oaxaca-Blinder（KOB）分解方法以处理非线性关系，发现常见的功能分解方法（如ANOVA和ALE）可能导致错误归因，并提出避免错误归因的条件。


<details>
  <summary>Details</summary>
Motivation: 在社会科学中，需要解释两个群体间结果差异的原因，但KOB分解仅适用于线性关系，而真实关系可能是非线性的。

Method: 扩展KOB分解方法，使用现代机器学习的非线性功能分解方法，并分析其适用性。

Result: 发现常见的功能分解方法（如ANOVA和ALE）可能导致错误归因，并提出避免错误归因的条件。

Conclusion: 任何依赖于协变量分布的可加分解方法都可能产生错误归因，而独立于输入分布的分解方法则不会。

Abstract: In science and social science, we often wish to explain why an outcome is
different in two populations. For instance, if a jobs program benefits members
of one city more than another, is that due to differences in program
participants (particular covariates) or the local labor markets (outcomes given
covariates)? The Kitagawa-Oaxaca-Blinder (KOB) decomposition is a standard tool
in econometrics that explains the difference in the mean outcome across two
populations. However, the KOB decomposition assumes a linear relationship
between covariates and outcomes, while the true relationship may be
meaningfully nonlinear. Modern machine learning boasts a variety of nonlinear
functional decompositions for the relationship between outcomes and covariates
in one population. It seems natural to extend the KOB decomposition using these
functional decompositions. We observe that a successful extension should not
attribute the differences to covariates -- or, respectively, to outcomes given
covariates -- if those are the same in the two populations. Unfortunately, we
demonstrate that, even in simple examples, two common decompositions --
functional ANOVA and Accumulated Local Effects -- can attribute differences to
outcomes given covariates, even when they are identical in two populations. We
provide a characterization of when functional ANOVA misattributes, as well as a
general property that any discrete decomposition must satisfy to avoid
misattribution. We show that if the decomposition is independent of its input
distribution, it does not misattribute. We further conjecture that
misattribution arises in any reasonable additive decomposition that depends on
the distribution of the covariates.

</details>

<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [243] [A CNN-based Local-Global Self-Attention via Averaged Window Embeddings for Hierarchical ECG Analysis](https://arxiv.org/abs/2504.16097)
*Arthur Buzelin,Pedro Robles Dutenhefner,Turi Rezende,Luisa G. Porfirio,Pedro Bento,Yan Aquino,Jose Fernandes,Caio Santana,Gabriela Miana,Gisele L. Pappa,Antonio Ribeiro,Wagner Meira Jr*

Main category: eess.SP

TLDR: 提出了一种结合局部和全局注意力的新型心电图分析模型LGA-ECG，解决了传统Transformer在捕捉局部形态特征上的不足，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要高效的心电图诊断工具。传统Transformer在捕捉局部形态特征上存在局限性。

Method: 提出LGA-ECG模型，结合卷积归纳偏置和全局自注意力机制，通过卷积窗口提取局部特征，同时建模全局上下文。

Result: 在CODE-15数据集上，LGA-ECG优于现有模型，验证了局部-全局注意力策略的有效性。

Conclusion: LGA-ECG通过捕捉心电图信号的层次时间依赖性和形态模式，展示了在临床自动化分类中的潜力。

Abstract: Cardiovascular diseases remain the leading cause of global mortality,
emphasizing the critical need for efficient diagnostic tools such as
electrocardiograms (ECGs). Recent advancements in deep learning, particularly
transformers, have revolutionized ECG analysis by capturing detailed waveform
features as well as global rhythm patterns. However, traditional transformers
struggle to effectively capture local morphological features that are critical
for accurate ECG interpretation. We propose a novel Local-Global Attention ECG
model (LGA-ECG) to address this limitation, integrating convolutional inductive
biases with global self-attention mechanisms. Our approach extracts queries by
averaging embeddings obtained from overlapping convolutional windows, enabling
fine-grained morphological analysis, while simultaneously modeling global
context through attention to keys and values derived from the entire sequence.
Experiments conducted on the CODE-15 dataset demonstrate that LGA-ECG
outperforms state-of-the-art models and ablation studies validate the
effectiveness of the local-global attention strategy. By capturing the
hierarchical temporal dependencies and morphological patterns in ECG signals,
this new design showcases its potential for clinical deployment with robust
automated ECG classification.

</details>

### [244] [Two-Timescale Joint Transmit and Pinching Beamforming for Pinching-Antenna Systems](https://arxiv.org/abs/2504.16099)
*Luyuan Zhang,Xidong Mu,An Liu,Yuanwei Liu*

Main category: eess.SP

TLDR: 提出了一种基于双时间尺度的联合发射和夹持波束成形设计，用于最大化PASS系统的多用户下行链路速率。


<details>
  <summary>Details</summary>
Motivation: PASS技术通过低成本夹持天线实现灵活通信，但需要高效的波束成形设计以优化性能。

Method: 采用原始对偶分解法将问题分为短期发射波束成形和长期夹持波束成形子问题，分别用KKT引导的双学习和随机连续凸近似方法解决。

Result: 仿真结果表明，所提算法显著优于基线方法。

Conclusion: 双时间尺度算法有效提升了PASS系统的性能。

Abstract: Pinching antenna systems (PASS) have been proposed as a revolutionary
flexible antenna technology which facilitates line-of-sight links via numerous
low-cost pinching antennas with adjustable activation positions over
waveguides. This letter proposes a two-timescale joint transmit and pinching
beamforming design for the maximization of sum rate of a PASS-based downlink
multi-user multiple input single output system. A primal dual decomposition
method is developed to decouple the two-timescale problem into two
sub-problems: 1) A Karush-Kuhn-Tucker-guided dual learning-based approach is
proposed to solve the short-term transmit beamforming design sub-problem; 2)
The long-term pinching beamforming design sub-problem is tackled by adopting a
stochastic successive convex approximation method. Simulation results
demonstrate that the proposed two-timescale algorithm achieves a significant
performance gain compared to other baselines.

</details>

### [245] [Towards Accurate Forecasting of Renewable Energy : Building Datasets and Benchmarking Machine Learning Models for Solar and Wind Power in France](https://arxiv.org/abs/2504.16100)
*Eloi Lindas,Yannig Goude,Philippe Ciais*

Main category: eess.SP

TLDR: 该研究提出了一种基于机器学习的法国太阳能和风能发电预测方法，利用空间天气数据和产能信息，比较了多种建模方法，发现神经网络优于传统树模型。


<details>
  <summary>Details</summary>
Motivation: 非调度可再生能源的准确预测对电网稳定和电价预测至关重要，现有方法未能充分利用空间数据。

Method: 使用ERA5天气数据、产能信息和电价作为输入特征，探索了空间平均、主成分分析和计算机视觉架构三种方法，并比较了多种机器学习模型。

Result: 神经网络表现最佳，误差范围为4%至10%，与传统单厂模型相当。

Conclusion: 该方法展示了区域电力供应预测的潜力，尤其是神经网络在处理空间数据时的优势。

Abstract: Accurate prediction of non-dispatchable renewable energy sources is essential
for grid stability and price prediction. Regional power supply forecasts are
usually indirect through a bottom-up approach of plant-level forecasts,
incorporate lagged power values, and do not use the potential of spatially
resolved data. This study presents a comprehensive methodology for predicting
solar and wind power production at country scale in France using machine
learning models trained with spatially explicit weather data combined with
spatial information about production sites capacity. A dataset is built
spanning from 2012 to 2023, using daily power production data from RTE (the
national grid operator) as the target variable, with daily weather data from
ERA5, production sites capacity and location, and electricity prices as input
features. Three modeling approaches are explored to handle spatially resolved
weather data: spatial averaging over the country, dimension reduction through
principal component analysis, and a computer vision architecture to exploit
complex spatial relationships. The study benchmarks state-of-the-art machine
learning models as well as hyperparameter tuning approaches based on
cross-validation methods on daily power production data. Results indicate that
cross-validation tailored to time series is best suited to reach low error. We
found that neural networks tend to outperform traditional tree-based models,
which face challenges in extrapolation due to the increasing renewable capacity
over time. Model performance ranges from 4% to 10% in nRMSE for midterm
horizon, achieving similar error metrics to local models established at a
single-plant level, highlighting the potential of these methods for regional
power supply forecasting.

</details>

### [246] [xLSTM-ECG: Multi-label ECG Classification via Feature Fusion with xLSTM](https://arxiv.org/abs/2504.16101)
*Lei Kang,Xuanshuo Fu,Javier Vazquez-Corral,Ernest Valveny,Dimosthenis Karatzas*

Main category: eess.SP

TLDR: 提出了一种基于xLSTM网络的多标签ECG信号分类方法xLSTM-ECG，显著提高了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，ECG手动解读耗时且易错，需高效准确的诊断工具。

Method: 使用STFT将ECG信号转换到频域，xLSTM网络捕捉12导联ECG的局部和全局特征。

Result: 在PTB-XL数据集上表现优异，Georgia 12-Lead数据集测试验证了其鲁棒性和高效性。

Conclusion: xLSTM-ECG显著提升ECG分类准确性，推动临床诊断和患者护理。

Abstract: Cardiovascular diseases (CVDs) remain the leading cause of mortality
worldwide, highlighting the critical need for efficient and accurate diagnostic
tools. Electrocardiograms (ECGs) are indispensable in diagnosing various heart
conditions; however, their manual interpretation is time-consuming and
error-prone. In this paper, we propose xLSTM-ECG, a novel approach that
leverages an extended Long Short-Term Memory (xLSTM) network for multi-label
classification of ECG signals, using the PTB-XL dataset. To the best of our
knowledge, this work represents the first design and application of xLSTM
modules specifically adapted for multi-label ECG classification. Our method
employs a Short-Time Fourier Transform (STFT) to convert time-series ECG
waveforms into the frequency domain, thereby enhancing feature extraction. The
xLSTM architecture is specifically tailored to address the complexities of
12-lead ECG recordings by capturing both local and global signal features.
Comprehensive experiments on the PTB-XL dataset reveal that our model achieves
strong multi-label classification performance, while additional tests on the
Georgia 12-Lead dataset underscore its robustness and efficiency. This approach
significantly improves ECG classification accuracy, thereby advancing clinical
diagnostics and patient care. The code will be publicly available upon
acceptance.

</details>

### [247] [A Self-supervised Learning Method for Raman Spectroscopy based on Masked Autoencoders](https://arxiv.org/abs/2504.16130)
*Pengju Ren,Ri-gui Zhou,Yaochong Li*

Main category: eess.SP

TLDR: 提出了一种基于掩码自编码器（SMAE）的自监督学习方法，用于拉曼光谱分析，解决了标注数据不足的问题，并在细菌分类任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有监督学习方法依赖大量标注数据，但拉曼光谱标注成本高且数据有限，导致性能下降。

Method: 采用自监督学习范式SMAE，通过随机掩码和重构光谱信息学习特征，无需标注数据。

Result: SMAE在预训练后提高了信噪比，细菌分类准确率超过80%，微调后达到83.90%，与监督方法相当。

Conclusion: SMAE为拉曼光谱分析提供了一种高效的自监督解决方案，尤其在标注数据稀缺时表现优越。

Abstract: Raman spectroscopy serves as a powerful and reliable tool for analyzing the
chemical information of substances. The integration of Raman spectroscopy with
deep learning methods enables rapid qualitative and quantitative analysis of
materials. Most existing approaches adopt supervised learning methods. Although
supervised learning has achieved satisfactory accuracy in spectral analysis, it
is still constrained by costly and limited well-annotated spectral datasets for
training. When spectral annotation is challenging or the amount of annotated
data is insufficient, the performance of supervised learning in spectral
material identification declines. In order to address the challenge of feature
extraction from unannotated spectra, we propose a self-supervised learning
paradigm for Raman Spectroscopy based on a Masked AutoEncoder, termed SMAE.
SMAE does not require any spectral annotations during pre-training. By randomly
masking and then reconstructing the spectral information, the model learns
essential spectral features. The reconstructed spectra exhibit certain
denoising properties, improving the signal-to-noise ratio (SNR) by more than
twofold. Utilizing the network weights obtained from masked pre-training, SMAE
achieves clustering accuracy of over 80% for 30 classes of isolated bacteria in
a pathogenic bacterial dataset, demonstrating significant improvements compared
to classical unsupervised methods and other state-of-the-art deep clustering
methods. After fine-tuning the network with a limited amount of annotated data,
SMAE achieves an identification accuracy of 83.90% on the test set, presenting
competitive performance against the supervised ResNet (83.40%).

</details>

### [248] [A Non-Invasive Load Monitoring Method for Edge Computing Based on MobileNetV3 and Dynamic Time Regulation](https://arxiv.org/abs/2504.16142)
*Hangxu Liu,Yaojie Sun,Yu Wang*

Main category: eess.SP

TLDR: 本文提出了一种创新的动态时间规整（DTW）算法，用于解决非侵入式负载监测（NILM）技术在资源受限的MCU上部署时的高计算和内存需求问题。通过实验验证，该方案实现了95%的识别准确率，并显著降低了运行时间和存储开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习和深度学习的NILM方法虽然精度高，但计算和内存需求大，难以在资源受限的MCU上部署。

Method: 提出了一种时间-频域动态时间规整（DTW）算法，并系统比较了六种机器学习技术在家用电场景中的性能。

Result: 实验验证显示，该方案在边缘MCU上实现了95%的识别准确率，运行时间减少55.55%，存储开销降低约34.6%。

Conclusion: 未来研究将聚焦于消除电压互感器设计以降低成本，为NILM的实际应用提供更具性价比的解决方案。

Abstract: In recent years, non-intrusive load monitoring (NILM) technology has
attracted much attention in the related research field by virtue of its unique
advantage of utilizing single meter data to achieve accurate decomposition of
device-level energy consumption. Cutting-edge methods based on machine learning
and deep learning have achieved remarkable results in load decomposition
accuracy by fusing time-frequency domain features. However, these methods
generally suffer from high computational costs and huge memory requirements,
which become the main obstacles for their deployment on resource-constrained
microcontroller units (MCUs). To address these challenges, this study proposes
an innovative Dynamic Time Warping (DTW) algorithm in the time-frequency domain
and systematically compares and analyzes the performance of six machine
learning techniques in home electricity scenarios. Through complete
experimental validation on edge MCUs, this scheme successfully achieves a
recognition accuracy of 95%. Meanwhile, this study deeply optimizes the
frequency domain feature extraction process, which effectively reduces the
running time by 55.55% and the storage overhead by about 34.6%. The algorithm
performance will be further optimized in future research work. Considering that
the elimination of voltage transformer design can significantly reduce the
cost, the subsequent research will focus on this direction, and is committed to
providing more cost-effective solutions for the practical application of NILM,
and providing a solid theoretical foundation and feasible technical paths for
the design of efficient NILM systems in edge computing environments.

</details>

### [249] [SeizureFormer: A Transformer Model for IEA-Based Seizure Risk Forecasting](https://arxiv.org/abs/2504.16098)
*Tianning Feng,Junting Ni,Ezequiel Gleichgerrcht,Wei Jin*

Main category: eess.SP

TLDR: SeizureFormer是一种基于Transformer的模型，用于长期癫痫风险预测，结合了IEA和LE生物标志物，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于原始头皮EEG的模型在癫痫风险预测中存在局限性，需要一种更稳健且可解释的工具。

Method: 利用CNN补丁嵌入、多头自注意力和挤压-激励块，结合短期动态和长期癫痫周期建模。

Result: 在5名患者中测试，平均ROC AUC为79.44%，PR AUC为76.29%，优于统计和深度学习基线。

Conclusion: SeizureFormer为个性化癫痫管理提供了可解释且稳健的预测工具，支持未来临床整合。

Abstract: We present SeizureFormer, a Transformer-based model for long-term seizure
risk forecasting using interictal epileptiform activity (IEA) surrogate
biomarkers and long episode (LE) biomarkers from responsive neurostimulation
(RNS) systems. Unlike raw scalp EEG-based models, SeizureFormer leverages
structured, clinically relevant features and integrates CNN-based patch
embedding, multi-head self-attention, and squeeze-and-excitation blocks to
model both short-term dynamics and long-term seizure cycles. Tested across five
patients and multiple prediction windows (1 to 14 days), SeizureFormer achieved
state-of-the-art performance with mean ROC AUC of 79.44 percent and mean PR AUC
of 76.29 percent. Compared to statistical, machine learning, and deep learning
baselines, it demonstrates enhanced generalizability and seizure risk
forecasting performance under class imbalance. This work supports future
clinical integration of interpretable and robust seizure forecasting tools for
personalized epilepsy management.

</details>

### [250] [A Statistical Approach for Synthetic EEG Data Generation](https://arxiv.org/abs/2504.16143)
*Gideon Vos,Maryam Ebrahimpour,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: eess.SP

TLDR: 提出了一种结合相关性分析和随机采样的方法，生成高质量合成EEG数据，以解决真实数据收集成本高的问题。


<details>
  <summary>Details</summary>
Motivation: EEG数据对心理健康诊断至关重要，但大规模收集成本高且耗时。合成数据生成可以扩展数据集，但保留情感和心理健康信号的高质量合成EEG仍具挑战性。

Method: 通过相关性分析EEG频段间的相互依赖关系，并基于此结构通过随机采样生成合成样本。保留与真实数据相关性高的样本，并通过分布分析和分类任务评估。

Result: 合成数据在统计和结构特性上与原始EEG高度匹配，随机森林模型无法区分合成与真实数据，表明高保真度。

Conclusion: 该方法为EEG数据集提供了一种可扩展且隐私保护的增强方式，有助于心理健康研究中更高效的模型训练。

Abstract: Electroencephalogram (EEG) data is crucial for diagnosing mental health
conditions but is costly and time-consuming to collect at scale. Synthetic data
generation offers a promising solution to augment datasets for machine learning
applications. However, generating high-quality synthetic EEG that preserves
emotional and mental health signals remains challenging. This study proposes a
method combining correlation analysis and random sampling to generate realistic
synthetic EEG data.
  We first analyze interdependencies between EEG frequency bands using
correlation analysis. Guided by this structure, we generate synthetic samples
via random sampling. Samples with high correlation to real data are retained
and evaluated through distribution analysis and classification tasks. A Random
Forest model trained to distinguish synthetic from real EEG performs at chance
level, indicating high fidelity.
  The generated synthetic data closely match the statistical and structural
properties of the original EEG, with similar correlation coefficients and no
significant differences in PERMANOVA tests. This method provides a scalable,
privacy-preserving approach for augmenting EEG datasets, enabling more
efficient model training in mental health research.

</details>

<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [251] [MARFT: Multi-Agent Reinforcement Fine-Tuning](https://arxiv.org/abs/2504.16129)
*Junwei Liao,Muning Wen,Jun Wang,Weinan Zhang*

Main category: cs.MA

TLDR: 本文提出了一种名为多智能体强化微调（MARFT）的新范式，用于优化基于LLM的多智能体系统（LaMAS），解决了传统多智能体强化学习（MARL）在LaMAS中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索如何利用基础强化学习技术微调LaMAS，且MARL直接应用于LaMAS存在显著挑战。

Method: 提出MARFT框架，包括算法设计、理论分析和开源实现，对比MARL与MARFT的关键差异。

Result: 开发了一个可扩展的MARFT框架，并提供了开源实现，为实际应用奠定了基础。

Conclusion: MARFT为LaMAS提供了新的研究方向，旨在实现更具适应性和鲁棒性的智能体系统。

Abstract: LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in
addressing complex, agentic tasks requiring multifaceted reasoning and
collaboration, from generating high-quality presentation slides to conducting
sophisticated scientific research. Meanwhile, RL has been widely recognized for
its effectiveness in enhancing agent intelligence, but limited research has
investigated the fine-tuning of LaMAS using foundational RL techniques.
Moreover, the direct application of MARL methodologies to LaMAS introduces
significant challenges, stemming from the unique characteristics and mechanisms
inherent to LaMAS. To address these challenges, this article presents a
comprehensive study of LLM-based MARL and proposes a novel paradigm termed
Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal
algorithmic framework tailored for LaMAS, outlining the conceptual foundations,
key distinctions, and practical implementation strategies. We begin by
reviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage
for a parallel analysis in the multi-agent domain. In the context of LaMAS, we
elucidate critical differences between MARL and MARFT. These differences
motivate a transition toward a novel, LaMAS-oriented formulation of RFT.
Central to this work is the presentation of a robust and scalable MARFT
framework. We detail the core algorithm and provide a complete, open-source
implementation to facilitate adoption and further research. The latter sections
of the paper explore real-world application perspectives and opening challenges
in MARFT. By bridging theoretical underpinnings with practical methodologies,
this work aims to serve as a roadmap for researchers seeking to advance MARFT
toward resilient and adaptive solutions in agentic systems. Our implementation
of the proposed framework is publicly available at:
https://github.com/jwliao-ai/MARFT.

</details>

<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [252] [Radiometer Calibration using Machine Learning](https://arxiv.org/abs/2504.16791)
*S. A. K. Leeney,H. T. J. Bevins,E. de Lera Acedo,W. J. Handley,C. Kirkham,R. S. Patel,J. Zhu,D. Molnar,J. Cumner,D. Anstey,K. Artuc,G. Bernardi,M. Bucher,S. Carey,J. Cavillot,R. Chiello,W. Croukamp,D. I. L. de Villiers,J. A. Ely,A. Fialkov,T. Gessey-Jones,G. Kulkarni,A. Magro,P. D. Meerburg,S. Mittal,J. H. N. Pattison,S. Pegwal,C. M. Pieterse,J. R. Pritchard,E. Puchwein,N. Razavi-Ghods,I. L. V. Roque,A. Saxena,K. H. Scheutwinkel,P. Scott,E. Shen,P. H. Sims,M. Spinelli*

Main category: astro-ph.IM

TLDR: 论文提出了一种基于机器学习的校准框架，用于提高射电天文辐射计的精度，特别针对探测21厘米线的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统校准方法（如Dicke切换）在阻抗不匹配时可能引入信号反射和失真，而机器学习提供了一种更有效的替代方案。

Method: 使用神经网络训练已知信号源，建模和校准复杂系统，以解决传统分析方法难以处理的问题。

Result: 首次测试并验证了机器学习校准框架在21厘米线探测实验中的高精度能力。

Conclusion: 机器学习校准框架为射电天文辐射计的高精度校准提供了新途径，尤其适用于21厘米线的探测。

Abstract: Radiometers are crucial instruments in radio astronomy, forming the primary
component of nearly all radio telescopes. They measure the intensity of
electromagnetic radiation, converting this radiation into electrical signals. A
radiometer's primary components are an antenna and a Low Noise Amplifier (LNA),
which is the core of the ``receiver'' chain. Instrumental effects introduced by
the receiver are typically corrected or removed during calibration. However,
impedance mismatches between the antenna and receiver can introduce unwanted
signal reflections and distortions. Traditional calibration methods, such as
Dicke switching, alternate the receiver input between the antenna and a
well-characterised reference source to mitigate errors by comparison. Recent
advances in Machine Learning (ML) offer promising alternatives. Neural
networks, which are trained using known signal sources, provide a powerful
means to model and calibrate complex systems where traditional analytical
approaches struggle. These methods are especially relevant for detecting the
faint sky-averaged 21-cm signal from atomic hydrogen at high redshifts. This is
one of the main challenges in observational Cosmology today. Here, for the
first time, we introduce and test a machine learning-based calibration
framework capable of achieving the precision required for radiometric
experiments aiming to detect the 21-cm line.

</details>

<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [253] [PINN-MEP: Continuous Neural Representations for Minimum-Energy Path Discovery in Molecular Systems](https://arxiv.org/abs/2504.16381)
*Magnus Petersen,Roberto Covino*

Main category: physics.chem-ph

TLDR: 论文提出了一种基于物理信息神经网络（PINNs）的方法，用于高效生成分子系统的过渡路径，避免了传统采样方法的高成本。


<details>
  <summary>Details</summary>
Motivation: 分子系统中的构象转变是计算科学中的核心挑战，传统方法（如分子动力学或MCMC）难以处理高维度和高能垒问题，而这些转变对生物过程（如离子通道蛋白的开关）至关重要。

Method: 将过渡路径生成问题转化为连续优化问题，利用物理信息神经网络（PINNs）和可微分分子动力学力场，通过自动微分实现高效路径发现。

Result: 方法在两种蛋白质（包括一个包含8,300多个原子的水合BPTI系统）上验证了有效性。

Conclusion: 该方法为分子构象转变的高效计算提供了新思路，避免了传统采样方法的计算成本问题。

Abstract: Characterizing conformational transitions in physical systems remains a
fundamental challenge in the computational sciences. Traditional sampling
methods like molecular dynamics (MD) or MCMC often struggle with the
high-dimensional nature of molecular systems and the high energy barriers of
transitions between stable states. While these transitions are rare events in
simulation timescales, they often represent the most biologically significant
processes - for example, the conformational change of an ion channel protein
from its closed to open state, which controls cellular ion flow and is crucial
for neural signaling. Such transitions in real systems may take milliseconds to
seconds but could require months or years of continuous simulation to observe
even once. We present a method that reformulates transition path generation as
a continuous optimization problem solved through physics-informed neural
networks (PINNs) inspired by string methods for minimum-energy path (MEP)
generation. By representing transition paths as implicit neural functions and
leveraging automatic differentiation with differentiable molecular dynamics
force fields, our method enables the efficient discovery of physically
realistic transition pathways without requiring expensive path sampling. We
demonstrate our method's effectiveness on two proteins, including an explicitly
hydrated bovine pancreatic trypsin inhibitor (BPTI) system with over 8,300
atoms.

</details>

<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [254] [BrainPrompt: Multi-Level Brain Prompt Enhancement for Neurological Condition Identification](https://arxiv.org/abs/2504.16096)
*Jiaxing Xu,Kai He,Yue Tang,Wei Li,Mengcheng Lan,Xia Dong,Yiping Ke,Mengling Feng*

Main category: q-bio.NC

TLDR: BrainPrompt结合GNN和LLM，通过多级知识驱动提示增强神经疾病诊断的预测能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有脑网络分析方法主要依赖成像数据，忽视了非成像因素，限制了模型的预测能力和可解释性。

Method: 提出BrainPrompt框架，整合ROI级、受试者级和疾病级知识驱动提示，利用LLM增强GNN。

Result: 在两个fMRI数据集上表现优于现有方法，并能提取与神经科学领域知识一致的可解释信息。

Conclusion: BrainPrompt通过多模态信息整合提升了神经疾病诊断的准确性和可解释性。

Abstract: Neurological conditions, such as Alzheimer's Disease, are challenging to
diagnose, particularly in the early stages where symptoms closely resemble
healthy controls. Existing brain network analysis methods primarily focus on
graph-based models that rely solely on imaging data, which may overlook
important non-imaging factors and limit the model's predictive power and
interpretability. In this paper, we present BrainPrompt, an innovative
framework that enhances Graph Neural Networks (GNNs) by integrating Large
Language Models (LLMs) with knowledge-driven prompts, enabling more effective
capture of complex, non-imaging information and external knowledge for
neurological disease identification. BrainPrompt integrates three types of
knowledge-driven prompts: (1) ROI-level prompts to encode the identity and
function of each brain region, (2) subject-level prompts that incorporate
demographic information, and (3) disease-level prompts to capture the temporal
progression of disease. By leveraging these multi-level prompts, BrainPrompt
effectively harnesses knowledge-enhanced multi-modal information from LLMs,
enhancing the model's capability to predict neurological disease stages and
meanwhile offers more interpretable results. We evaluate BrainPrompt on two
resting-state functional Magnetic Resonance Imaging (fMRI) datasets from
neurological disorders, showing its superiority over state-of-the-art methods.
Additionally, a biomarker study demonstrates the framework's ability to extract
valuable and interpretable information aligned with domain knowledge in
neuroscience.

</details>

### [255] [Application of an attention-based CNN-BiLSTM framework for in vivo two-photon calcium imaging of neuronal ensembles: decoding complex bilateral forelimb movements from unilateral M1](https://arxiv.org/abs/2504.16917)
*Ghazal Mirzaee,Jonathan Chang,Shahrzad Latifi*

Main category: q-bio.NC

TLDR: 本文提出了一种基于注意力机制的CNN-BiLSTM混合深度学习模型，用于从单侧M1神经元群体信号中解码复杂的前肢运动，展示了其在捕捉神经元网络时空依赖性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 解码多尺度脑网络中的行为（如运动）是神经科学的核心目标，而人工智能和机器学习在揭示运动功能的神经机制中扮演着越来越重要的角色。

Method: 采用了一种混合深度学习框架——基于注意力机制的CNN-BiLSTM模型，利用双光子钙成像信号解码复杂前肢运动。

Result: 研究发现，单侧M1神经元群体信号可以准确解码同侧和对侧前肢的复杂运动。

Conclusion: 结果表明，先进的混合深度学习模型能够有效捕捉与复杂运动执行相关的神经元网络活动的时空依赖性。

Abstract: Decoding behavior, such as movement, from multiscale brain networks remains a
central objective in neuroscience. Over the past decades, artificial
intelligence and machine learning have played an increasingly significant role
in elucidating the neural mechanisms underlying motor function. The advancement
of brain-monitoring technologies, capable of capturing complex neuronal signals
with high spatial and temporal resolution, necessitates the development and
application of more sophisticated machine learning models for behavioral
decoding. In this study, we employ a hybrid deep learning framework, an
attention-based CNN-BiLSTM model, to decode skilled and complex forelimb
movements using signals obtained from in vivo two-photon calcium imaging. Our
findings demonstrate that the intricate movements of both ipsilateral and
contralateral forelimbs can be accurately decoded from unilateral M1 neuronal
ensembles. These results highlight the efficacy of advanced hybrid deep
learning models in capturing the spatiotemporal dependencies of neuronal
networks activity linked to complex movement execution.

</details>

<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [256] [A Statistical Evaluation of Indoor LoRaWAN Environment-Aware Propagation for 6G: MLR, ANOVA, and Residual Distribution Analysis](https://arxiv.org/abs/2504.16688)
*Nahshon Mokua,Obiri,Kristof,Van Laerhoven*

Main category: cs.NI

TLDR: 本文提出了一种两阶段方法，用于建模室内LoRaWAN技术部署中的路径损耗，通过引入环境变量和混合高斯模型显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 室内LoRaWAN技术部署中的路径损耗建模因结构障碍、人员密度和活动以及环境条件波动而具有挑战性。

Method: 采用两阶段方法：1) 多元线性回归模型，包括传统传播指标和环境变量；2) 通过拟合五种概率分布分析残差分布。

Result: 引入环境变量可减少42.32%的未解释方差；四分量高斯混合模型在捕捉残差异质性方面表现最佳。

Conclusion: 环境感知建模可显著改善动态室内物联网部署中的LoRaWAN网络设计，为6G网络的超可靠、上下文感知通信提供支持。

Abstract: Modeling path loss in indoor LoRaWAN technology deployments is inherently
challenging due to structural obstructions, occupant density and activities,
and fluctuating environmental conditions. This study proposes a two-stage
approach to capture and analyze these complexities using an extensive dataset
of 1,328,334 field measurements collected over six months in a single-floor
office at the University of Siegen's Hoelderlinstrasse Campus, Germany. First,
we implement a multiple linear regression model that includes traditional
propagation metrics (distance, structural walls) and an extension with proposed
environmental variables (relative humidity, temperature, carbon dioxide,
particulate matter, and barometric pressure). Using analysis of variance, we
demonstrate that adding these environmental factors can reduce unexplained
variance by 42.32 percent. Secondly, we examine residual distributions by
fitting five candidate probability distributions: Normal, Skew-Normal, Cauchy,
Student's t, and Gaussian Mixture Models with one to five components. Our
results show that a four-component Gaussian Mixture Model captures the residual
heterogeneity of indoor signal propagation most accurately, significantly
outperforming single-distribution approaches. Given the push toward
ultra-reliable, context-aware communications in 6G networks, our analysis shows
that environment-aware modeling can substantially improve LoRaWAN network
design in dynamic indoor IoT deployments.

</details>

<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [257] [Adaptive and Efficient Dynamic Memory Management for Hardware Enclaves](https://arxiv.org/abs/2504.16251)
*Vijay Dhanraj,Harpreet Singh Chwarla,Tao Zhang,Daniel Manila,Eric Thomas Schneider,Erica Fu,Mona Vij,Chia-Che Tsai,Donald E. Porter*

Main category: cs.OS

TLDR: SGX2的EDMM功能动态管理内存和线程，减少了启动时间，但直接采用会降低执行效率。通过优化，可以消除EDMM的开销。


<details>
  <summary>Details</summary>
Motivation: 解决SGX2中EDMM功能直接采用导致的执行效率下降问题，同时保留其性能优势。

Method: 在Gramine库OS中实现优化策略，包括减少页面映射修改成本。

Result: 优化后成功消除EDMM的开销，同时保留了其性能提升和灵活性。

Conclusion: 通过优化策略，SGX2的EDMM功能可以在不牺牲性能的情况下实现动态内存管理。

Abstract: The second version of Intel Software Guard Extensions (Intel SGX), or SGX2,
adds dynamic management of enclave memory and threads. The first version
required the address space and thread counts to be fixed before execution. The
Enclave Dynamic Memory Management (EDMM) feature of SGX2 has the potential to
lower launch times and overall execution time. Despite reducing the enclave
loading time by 28--93%, straightforward EDMM adoption strategies actually slow
execution time down by as much as 58%. Using the Gramine library OS as a
representative enclave runtime environment, this paper shows how to recover
EDMM performance. The paper explains how implementing mutual distrust between
the OS and enclave increases the cost of modifying page mappings. The paper
then describes and evaluates a series of optimizations on application
benchmarks, showing that these optimizations effectively eliminate the
overheads of EDMM while retaining EDMM's performance and flexibility gains.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [258] [Detecting Actionable Requests and Offers on Social Media During Crises Using LLMs](https://arxiv.org/abs/2504.16144)
*Ahmed El Fekih Zguir,Ferda Ofli,Muhammad Imran*

Main category: cs.IR

TLDR: 提出了一种基于LLM的细粒度分类方法（QSF Learning），用于高效组织灾害相关社交媒体信息，并评估其可操作性。


<details>
  <summary>Details</summary>
Motivation: 自然灾害导致社交媒体信息激增，需要系统化分类以提升人道主义组织响应效率。

Method: 采用QSF Learning方法，通过检索特定类别的标注示例增强LLM的分类性能，并评估信息的可操作性。

Result: 实验表明，该方法优于基线提示策略，能有效识别和优先处理可操作请求与帮助。

Conclusion: QSF Learning为灾害信息分类与优先级排序提供了高效解决方案。

Abstract: Natural disasters often result in a surge of social media activity, including
requests for assistance, offers of help, sentiments, and general updates. To
enable humanitarian organizations to respond more efficiently, we propose a
fine-grained hierarchical taxonomy to systematically organize crisis-related
information about requests and offers into three critical dimensions: supplies,
emergency personnel, and actions. Leveraging the capabilities of Large Language
Models (LLMs), we introduce Query-Specific Few-shot Learning (QSF Learning)
that retrieves class-specific labeled examples from an embedding database to
enhance the model's performance in detecting and classifying posts. Beyond
classification, we assess the actionability of messages to prioritize posts
requiring immediate attention. Extensive experiments demonstrate that our
approach outperforms baseline prompting strategies, effectively identifying and
prioritizing actionable requests and offers.

</details>

### [259] [LegalRAG: A Hybrid RAG System for Multilingual Legal Information Retrieval](https://arxiv.org/abs/2504.16121)
*Muhammad Rafsan Kabir,Rafeed Mohammad Sultan,Fuad Rahman,Mohammad Ruhul Amin,Sifat Momen,Nabeel Mohammed,Shafin Rahman*

Main category: cs.IR

TLDR: 开发了一个高效的双语问答框架，用于处理孟加拉国警察公报中的法规文件，结合现代RAG技术提升检索和回答生成能力。


<details>
  <summary>Details</summary>
Motivation: 法律和监管任务中NLP技术的应用有限，需要一种高效的双语问答系统来提升法律信息的可访问性。

Method: 采用Retrieval Augmented Generation (RAG)管道，并提出一种改进的RAG方法以提升检索性能。

Result: 在孟加拉国警察公报的测试集上，改进的RAG方法在所有评估指标上均优于现有方法。

Conclusion: 提出的双语问答框架显著提升了法律信息的检索和回答生成效率，为法律领域NLP应用提供了新思路。

Abstract: Natural Language Processing (NLP) and computational linguistic techniques are
increasingly being applied across various domains, yet their use in legal and
regulatory tasks remains limited. To address this gap, we develop an efficient
bilingual question-answering framework for regulatory documents, specifically
the Bangladesh Police Gazettes, which contain both English and Bangla text. Our
approach employs modern Retrieval Augmented Generation (RAG) pipelines to
enhance information retrieval and response generation. In addition to
conventional RAG pipelines, we propose an advanced RAG-based approach that
improves retrieval performance, leading to more precise answers. This system
enables efficient searching for specific government legal notices, making legal
information more accessible. We evaluate both our proposed and conventional RAG
systems on a diverse test set on Bangladesh Police Gazettes, demonstrating that
our approach consistently outperforms existing methods across all evaluation
metrics.

</details>

### [260] [CLIRudit: Cross-Lingual Information Retrieval of Scientific Documents](https://arxiv.org/abs/2504.16264)
*Francisco Valentini,Diego Kozlowski,Vincent Larivière*

Main category: cs.IR

TLDR: 论文介绍了CLIRudit数据集，用于评估跨语言学术搜索，比较了多种零样本检索方法，发现大型密集检索器表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决跨语言学术搜索的挑战，促进科学知识的跨语言传播。

Method: 构建CLIRudit数据集，评估稀疏和密集检索器、机器翻译等方法。

Result: 大型密集检索器表现接近人工翻译，稀疏检索器结合文档翻译也具竞争力。

Conclusion: 研究为跨语言学术检索提供了新框架和数据集，推动科学知识的无障碍传播。

Abstract: Cross-lingual information retrieval (CLIR) consists in finding relevant
documents in a language that differs from the language of the queries. This
paper presents CLIRudit, a new dataset created to evaluate cross-lingual
academic search, focusing on English queries and French documents. The dataset
is built using bilingual article metadata from \'Erudit, a Canadian publishing
platform, and is designed to represent scenarios in which researchers search
for scholarly content in languages other than English. We perform a
comprehensive benchmarking of different zero-shot first-stage retrieval methods
on the dataset, including dense and sparse retrievers, query and document
machine translation, and state-of-the-art multilingual retrievers. Our results
show that large dense retrievers, not necessarily trained for the cross-lingual
retrieval task, can achieve zero-shot performance comparable to using ground
truth human translations, without the need for machine translation. Sparse
retrievers, such as BM25 or SPLADE, combined with document translation, show
competitive results, providing an efficient alternative to large dense models.
This research advances the understanding of cross-lingual academic information
retrieval and provides a framework that others can use to build comparable
datasets across different languages and disciplines. By making the dataset and
code publicly available, we aim to facilitate further research that will help
make scientific knowledge more accessible across language barriers.

</details>

### [261] [Disentangling and Generating Modalities for Recommendation in Missing Modality Scenarios](https://arxiv.org/abs/2504.16352)
*Jiwan Kim,Hongseok Kang,Sein Kim,Kibum Kim,Chanyoung Park*

Main category: cs.IR

TLDR: DGMRec是一个针对多模态推荐系统中缺失模态问题的新框架，通过解耦和生成模态特征提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态推荐系统中缺失模态场景和模态特征独特性未被充分考虑的挑战。

Method: DGMRec将模态特征解耦为通用和特定特征，并通过整合其他模态特征生成缺失模态特征。

Result: DGMRec在缺失模态和新物品场景中表现优于现有方法，并支持跨模态检索。

Conclusion: DGMRec通过生成方法解决了缺失模态问题，展示了实际应用的潜力。

Abstract: Multi-modal recommender systems (MRSs) have achieved notable success in
improving personalization by leveraging diverse modalities such as images,
text, and audio. However, two key challenges remain insufficiently addressed:
(1) Insufficient consideration of missing modality scenarios and (2) the
overlooking of unique characteristics of modality features. These challenges
result in significant performance degradation in realistic situations where
modalities are missing. To address these issues, we propose Disentangling and
Generating Modality Recommender (DGMRec), a novel framework tailored for
missing modality scenarios. DGMRec disentangles modality features into general
and specific modality features from an information-based perspective, enabling
richer representations for recommendation. Building on this, it generates
missing modality features by integrating aligned features from other modalities
and leveraging user modality preferences. Extensive experiments show that
DGMRec consistently outperforms state-of-the-art MRSs in challenging scenarios,
including missing modalities and new item settings as well as diverse missing
ratios and varying levels of missing modalities. Moreover, DGMRec's
generation-based approach enables cross-modal retrieval, a task inapplicable
for existing MRSs, highlighting its adaptability and potential for real-world
applications. Our code is available at https://github.com/ptkjw1997/DGMRec.

</details>

### [262] [A Survey of Foundation Model-Powered Recommender Systems: From Feature-Based, Generative to Agentic Paradigms](https://arxiv.org/abs/2504.16420)
*Chengkai Huang,Hongtao Huang,Tong Yu,Kaige Xie,Junda Wu,Shuai Zhang,Julian Mcauley,Dietmar Jannach,Lina Yao*

Main category: cs.IR

TLDR: 该论文综述了基础模型（FMs）在推荐系统（RS）中的应用，探讨了三种集成范式：特征增强、生成式推荐和代理交互系统，并分析了其机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型（如GPT、LLaMA和CLIP）的兴起，传统的推荐系统技术需要适应新的范式，以更高效地利用这些模型的能力。

Method: 论文通过综述现有研究，分析了基础模型在推荐系统中的三种应用范式，并讨论了其在不同任务中的表现。

Result: 基础模型为推荐系统带来了新的机遇，如多模态推理和自然语言理解，但也面临数据依赖性和计算成本等挑战。

Conclusion: 论文总结了基础模型在推荐系统中的潜力，并提出了未来研究方向，包括解决技术挑战和优化范式选择。

Abstract: Recommender systems (RS) have become essential in filtering information and
personalizing content for users. RS techniques have traditionally relied on
modeling interactions between users and items as well as the features of
content using models specific to each task. The emergence of foundation models
(FMs), large scale models trained on vast amounts of data such as GPT, LLaMA
and CLIP, is reshaping the recommendation paradigm. This survey provides a
comprehensive overview of the Foundation Models for Recommender Systems
(FM4RecSys), covering their integration in three paradigms: (1) Feature-Based
augmentation of representations, (2) Generative recommendation approaches, and
(3) Agentic interactive systems. We first review the data foundations of RS,
from traditional explicit or implicit feedback to multimodal content sources.
We then introduce FMs and their capabilities for representation learning,
natural language understanding, and multi-modal reasoning in RS contexts. The
core of the survey discusses how FMs enhance RS under different paradigms.
Afterward, we examine FM applications in various recommendation tasks. Through
an analysis of recent research, we highlight key opportunities that have been
realized as well as challenges encountered. Finally, we outline open research
directions and technical challenges for next-generation FM4RecSys. This survey
not only reviews the state-of-the-art methods but also provides a critical
analysis of the trade-offs among the feature-based, the generative, and the
agentic paradigms, outlining key open issues and future research directions.

</details>

### [263] [MMHCL: Multi-Modal Hypergraph Contrastive Learning for Recommendation](https://arxiv.org/abs/2504.16576)
*Xu Guo,Tong Zhang,Fuyun Wang,Xudong Wang,Xiaoya Zhang,Xin Liu,Zhen Cui*

Main category: cs.IR

TLDR: 论文提出了一种多模态超图对比学习框架（MMHCL），通过构建用户和物品的超图来挖掘共享偏好和语义关联，缓解数据稀疏和冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 多模态内容平台的兴起需要个性化推荐系统，但现有方法因数据稀疏和冷启动问题难以充分挖掘用户-物品关联。

Method: 构建用户-用户和物品-物品超图，结合对比学习增强特征区分性，融合一阶和二阶语义信息。

Result: MMHCL通过挖掘更丰富的共享属性，有效缓解了数据稀疏和冷启动问题，实验验证了其有效性。

Conclusion: MMHCL框架在多模态推荐系统中表现出色，为数据稀疏和冷启动问题提供了新解决方案。

Abstract: The burgeoning presence of multimodal content-sharing platforms propels the
development of personalized recommender systems. Previous works usually suffer
from data sparsity and cold-start problems, and may fail to adequately explore
semantic user-product associations from multimodal data. To address these
issues, we propose a novel Multi-Modal Hypergraph Contrastive Learning (MMHCL)
framework for user recommendation. For a comprehensive information exploration
from user-product relations, we construct two hypergraphs, i.e. a user-to-user
(u2u) hypergraph and an item-to-item (i2i) hypergraph, to mine shared
preferences among users and intricate multimodal semantic resemblance among
items, respectively. This process yields denser second-order semantics that are
fused with first-order user-item interaction as complementary to alleviate the
data sparsity issue. Then, we design a contrastive feature enhancement paradigm
by applying synergistic contrastive learning. By maximizing/minimizing the
mutual information between second-order (e.g. shared preference pattern for
users) and first-order (information of selected items for users) embeddings of
the same/different users and items, the feature distinguishability can be
effectively enhanced. Compared with using sparse primary user-item interaction
only, our MMHCL obtains denser second-order hypergraphs and excavates more
abundant shared attributes to explore the user-product associations, which to a
certain extent alleviates the problems of data sparsity and cold-start.
Extensive experiments have comprehensively demonstrated the effectiveness of
our method. Our code is publicly available at: https://github.com/Xu107/MMHCL.

</details>

<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [264] [Data-Assimilated Model-Based Reinforcement Learning for Partially Observed Chaotic Flows](https://arxiv.org/abs/2504.16588)
*Defne E. Ozan,Andrea Nóvoa,Luca Magri*

Main category: eess.SY

TLDR: 提出了一种结合数据同化和模型强化学习的框架（DA-MBRL），用于部分可观测和噪声环境中的湍流控制。


<details>
  <summary>Details</summary>
Motivation: 湍流控制因混沌动力学和高维度而困难，且实验环境中常缺乏完整状态信息。

Method: 使用控制感知的回声状态网络进行数据驱动预测，集成数据同化和集成卡尔曼滤波进行实时状态估计，并采用离策略演员-评论家算法学习控制策略。

Result: 在Kuramoto-Sivashinsky方程上测试，成功从噪声和部分测量中稳定时空混沌流。

Conclusion: DA-MBRL框架在部分可观测和噪声环境下有效，为湍流控制提供了新方法。

Abstract: The goal of many applications in energy and transport sectors is to control
turbulent flows. However, because of chaotic dynamics and high dimensionality,
the control of turbulent flows is exceedingly difficult. Model-free
reinforcement learning (RL) methods can discover optimal control policies by
interacting with the environment, but they require full state information,
which is often unavailable in experimental settings. We propose a
data-assimilated model-based RL (DA-MBRL) framework for systems with partial
observability and noisy measurements. Our framework employs a control-aware
Echo State Network for data-driven prediction of the dynamics, and integrates
data assimilation with an Ensemble Kalman Filter for real-time state
estimation. An off-policy actor-critic algorithm is employed to learn optimal
control strategies from state estimates. The framework is tested on the
Kuramoto-Sivashinsky equation, demonstrating its effectiveness in stabilizing a
spatiotemporally chaotic flow from noisy and partial measurements.

</details>

### [265] [Learning Verifiable Control Policies Using Relaxed Verification](https://arxiv.org/abs/2504.16879)
*Puja Chaudhury,Alexander Estornell,Michael Everett*

Main category: eess.SY

TLDR: 论文提出在训练过程中进行验证，以确保学习控制策略满足安全规范，避免训练后验证的保守性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练后进行验证可能导致无法满足规范或验证算法过于保守，因此需要在训练过程中进行验证。

Method: 使用可微分可达性分析，并将新组件纳入损失函数。

Result: 在四旋翼模型和独轮车模型上的实验表明，该方法能学习到满足可达性和不变性规范的控制策略。

Conclusion: 通过在训练过程中进行验证，可以生成满足轻量级运行时验证需求的控制策略。

Abstract: To provide safety guarantees for learning-based control systems, recent work
has developed formal verification methods to apply after training ends.
However, if the trained policy does not meet the specifications, or there is
conservatism in the verification algorithm, establishing these guarantees may
not be possible. Instead, this work proposes to perform verification throughout
training to ultimately aim for policies whose properties can be evaluated
throughout runtime with lightweight, relaxed verification algorithms. The
approach is to use differentiable reachability analysis and incorporate new
components into the loss function. Numerical experiments on a quadrotor model
and unicycle model highlight the ability of this approach to lead to learned
control policies that satisfy desired reach-avoid and invariance
specifications.

</details>

<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [266] [Physics-Informed Inference Time Scaling via Simulation-Calibrated Scientific Machine Learning](https://arxiv.org/abs/2504.16172)
*Zexi Fan,Yan Sun,Shihao Yang,Yiping Lu*

Main category: math.NA

TLDR: SCaSML是一种基于物理信息的框架，通过动态修正和去偏SciML预测，显著提升高维PDE求解的精度。


<details>
  <summary>Details</summary>
Motivation: 高维PDE在多个领域具有重要应用，但现有SciML方法存在偏差且忽视物理规律。

Method: 提出SCaSML框架，利用新物理定律量化系统误差，并采用蒙特卡洛求解器动态修正预测。

Result: 数值实验显示SCaSML将误差降低20-50%，首次实现在推理阶段优化高维PDE近似解。

Conclusion: SCaSML通过计算最优推理方法提升收敛速度，为高维PDE求解提供新思路。

Abstract: High-dimensional partial differential equations (PDEs) pose significant
computational challenges across fields ranging from quantum chemistry to
economics and finance. Although scientific machine learning (SciML) techniques
offer approximate solutions, they often suffer from bias and neglect crucial
physical insights. Inspired by inference-time scaling strategies in language
models, we propose Simulation-Calibrated Scientific Machine Learning (SCaSML),
a physics-informed framework that dynamically refines and debiases the SCiML
predictions during inference by enforcing the physical laws. SCaSML leverages
derived new physical laws that quantifies systematic errors and employs Monte
Carlo solvers based on the Feynman-Kac and Elworthy-Bismut-Li formulas to
dynamically correct the prediction. Both numerical and theoretical analysis
confirms enhanced convergence rates via compute-optimal inference methods. Our
numerical experiments demonstrate that SCaSML reduces errors by 20-50% compared
to the base surrogate model, establishing it as the first algorithm to refine
approximated solutions to high-dimensional PDE during inference. Code of SCaSML
is available at https://github.com/Francis-Fan-create/SCaSML.

</details>

<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [267] [Circinus: Efficient Query Planner for Compound ML Serving](https://arxiv.org/abs/2504.16397)
*Banruo Liu,Wei-Yu Lin,Minghao Fang,Yihan Jiang,Fan Lai*

Main category: cs.DB

TLDR: Circinus是一种面向大规模复合AI工作负载的SLO感知查询规划器，通过分解多查询规划和多维度SLO目标，显著提升服务吞吐量和规划效率。


<details>
  <summary>Details</summary>
Motivation: 复合AI服务的兴起需要高效的算子放置、配置和资源分配，但现有解决方案因搜索空间庞大而受限。

Method: Circinus通过分解多查询规划和多维度SLO目标，利用查询内和查询间的计划相似性减少搜索步骤，并结合精度感知的计划分析器提高效率。

Result: 在真实场景中，Circinus将服务吞吐量提升3.2-5.0倍，查询规划速度加快4.2-5.8倍，响应时间缩短至秒级，部署成本降低3.2-4.0倍。

Conclusion: Circinus在复合AI服务中显著提升了全局SLO吞吐量和成本效率，优于现有技术。

Abstract: The rise of compound AI serving -- integrating multiple operators in a
pipeline that may span edge and cloud tiers -- enables end-user applications
such as autonomous driving, generative AI-powered meeting companions, and
immersive gaming. Achieving high service goodput -- i.e., meeting service level
objectives (SLOs) for pipeline latency, accuracy, and costs -- requires
effective planning of operator placement, configuration, and resource
allocation across infrastructure tiers. However, the diverse SLO requirements,
varying edge capabilities, and high query volumes create an enormous planning
search space, rendering current solutions fundamentally limited for real-time
serving and cost-efficient deployments.
  This paper presents Circinus, an SLO-aware query planner for large-scale
compound AI workloads. Circinus novelly decomposes multi-query planning and
multi-dimensional SLO objectives while preserving global decision quality. By
exploiting plan similarities within and across queries, it significantly
reduces search steps. It further improves per-step efficiency with a
precision-aware plan profiler that incrementally profiles and strategically
applies early stopping based on imprecise estimates of plan performance. At
scale, Circinus selects query-plan combinations to maximize global SLO goodput.
Evaluations in real-world settings show that Circinus improves service goodput
by 3.2-5.0$\times$, accelerates query planning by 4.2-5.8$\times$, achieving
query response in seconds, while reducing deployment costs by 3.2-4.0$\times$
over state of the arts even in their intended single-tier deployments.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [268] [ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance](https://arxiv.org/abs/2504.16464)
*Ying Li,Xiaobao Wei,Xiaowei Chi,Yuming Li,Zhongyu Zhao,Hao Wang,Ningning Ma,Ming Lu,Shanghang Zhang*

Main category: cs.RO

TLDR: ManipDreamer提出了一种基于动作树和视觉引导的高级世界模型，显著提升了机器人操作视频合成的指令跟随能力和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RoboDreamer虽然通过语言分解实现了指令跟随，但忽略了指令原语之间的关系和视觉引导（如深度和语义信息），影响了视觉质量和一致性。

Method: ManipDreamer将指令表示为动作树，并为树节点分配嵌入，通过动作树导航获取指令嵌入以指导世界模型。同时，引入视觉引导适配器结合深度和语义信息，提升视频生成的时空一致性。

Result: 在机器人操作基准测试中，ManipDreamer在视频质量指标（PSNR、SSIM、Flow Error）和任务成功率上均显著优于RoboDreamer。

Conclusion: ManipDreamer通过动作树和视觉引导的结合，有效解决了指令原语关系和视觉质量的问题，为机器人操作视频合成提供了更优的解决方案。

Abstract: While recent advancements in robotic manipulation video synthesis have shown
promise, significant challenges persist in ensuring effective
instruction-following and achieving high visual quality. Recent methods, like
RoboDreamer, utilize linguistic decomposition to divide instructions into
separate lower-level primitives, conditioning the world model on these
primitives to achieve compositional instruction-following. However, these
separate primitives do not consider the relationships that exist between them.
Furthermore, recent methods neglect valuable visual guidance, including depth
and semantic guidance, both crucial for enhancing visual quality. This paper
introduces ManipDreamer, an advanced world model based on the action tree and
visual guidance. To better learn the relationships between instruction
primitives, we represent the instruction as the action tree and assign
embeddings to tree nodes, each instruction can acquire its embeddings by
navigating through the action tree. The instruction embeddings can be used to
guide the world model. To enhance visual quality, we combine depth and semantic
guidance by introducing a visual guidance adapter compatible with the world
model. This visual adapter enhances both the temporal and physical consistency
of video generation. Based on the action tree and visual guidance, ManipDreamer
significantly boosts the instruction-following ability and visual quality.
Comprehensive evaluations on robotic manipulation benchmarks reveal that
ManipDreamer achieves large improvements in video quality metrics in both seen
and unseen tasks, with PSNR improved from 19.55 to 21.05, SSIM improved from
0.7474 to 0.7982 and reduced Flow Error from 3.506 to 3.201 in unseen tasks,
compared to the recent RoboDreamer model. Additionally, our method increases
the success rate of robotic manipulation tasks by 2.5% in 6 RLbench tasks on
average.

</details>

### [269] [PCF-Grasp: Converting Point Completion to Geometry Feature to Enhance 6-DoF Grasp](https://arxiv.org/abs/2504.16320)
*Yaofeng Cheng,Fusheng Zha,Wei Guo,Pengfei Wang,Chao Zeng,Lining Sun,Chenguang Yang*

Main category: cs.RO

TLDR: 提出了一种基于点云补全的6自由度抓取框架，通过将补全结果作为形状特征训练网络，并结合评分过滤器提高抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于单视角点云的抓取方法因信息不完整导致抓取精度低，受人类利用几何经验估计物体形状的启发，提出改进方法。

Method: 将点云补全结果作为形状特征训练6自由度抓取网络，并加入评分过滤器筛选可执行抓取方案。

Result: 实验表明，该方法生成的抓取方案更准确，评分过滤器显著提升了真实抓取的可信度，成功率比现有方法高17.8%。

Conclusion: 通过点云补全和评分过滤器的结合，显著提升了6自由度抓取的精度和实用性。

Abstract: The 6-Degree of Freedom (DoF) grasp method based on point clouds has shown
significant potential in enabling robots to grasp target objects. However, most
existing methods are based on the point clouds (2.5D points) generated from
single-view depth images. These point clouds only have one surface side of the
object providing incomplete geometry information, which mislead the grasping
algorithm to judge the shape of the target object, resulting in low grasping
accuracy. Humans can accurately grasp objects from a single view by leveraging
their geometry experience to estimate object shapes. Inspired by humans, we
propose a novel 6-DoF grasping framework that converts the point completion
results as object shape features to train the 6-DoF grasp network. Here, point
completion can generate approximate complete points from the 2.5D points
similar to the human geometry experience, and converting it as shape features
is the way to utilize it to improve grasp efficiency. Furthermore, due to the
gap between the network generation and actual execution, we integrate a score
filter into our framework to select more executable grasp proposals for the
real robot. This enables our method to maintain a high grasp quality in any
camera viewpoint. Extensive experiments demonstrate that utilizing complete
point features enables the generation of significantly more accurate grasp
proposals and the inclusion of a score filter greatly enhances the credibility
of real-world robot grasping. Our method achieves a 17.8\% success rate higher
than the state-of-the-art method in real-world experiments.

</details>

### [270] [Offline Robotic World Model: Learning Robotic Policies without a Physics Simulator](https://arxiv.org/abs/2504.16680)
*Chenhao Li,Andreas Krause,Marco Hutter*

Main category: cs.RO

TLDR: RWM-O是一种基于模型的离线强化学习方法，通过显式估计认知不确定性来提升策略学习，无需依赖物理模拟器。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中分布偏移和模型误差累积的问题，同时提升策略的泛化能力和安全性。

Method: 提出RWM-O方法，通过估计认知不确定性并整合到策略优化中，惩罚不可靠的转移以减少模型误差的过拟合。

Result: 实验结果表明，RWM-O提升了策略的泛化能力和安全性，能够仅从真实数据中学习策略。

Conclusion: RWM-O为机器人领域提供了一种可扩展且数据高效的强化学习方法。

Abstract: Reinforcement Learning (RL) has demonstrated impressive capabilities in
robotic control but remains challenging due to high sample complexity, safety
concerns, and the sim-to-real gap. While offline RL eliminates the need for
risky real-world exploration by learning from pre-collected data, it suffers
from distributional shift, limiting policy generalization. Model-Based RL
(MBRL) addresses this by leveraging predictive models for synthetic rollouts,
yet existing approaches often lack robust uncertainty estimation, leading to
compounding errors in offline settings. We introduce Offline Robotic World
Model (RWM-O), a model-based approach that explicitly estimates epistemic
uncertainty to improve policy learning without reliance on a physics simulator.
By integrating these uncertainty estimates into policy optimization, our
approach penalizes unreliable transitions, reducing overfitting to model errors
and enhancing stability. Experimental results show that RWM-O improves
generalization and safety, enabling policy learning purely from real-world data
and advancing scalable, data-efficient RL for robotics.

</details>

### [271] [HERB: Human-augmented Efficient Reinforcement learning for Bin-packing](https://arxiv.org/abs/2504.16595)
*Gojko Perovic,Nuno Ferreira Duarte,Atabak Dehban,Gonçalo Teixeira,Egidio Falotico,José Santos-Victor*

Main category: cs.RO

TLDR: HERB是一个结合人类演示的强化学习框架，用于高效打包不规则3D物体，优于传统几何和纯RL方法。


<details>
  <summary>Details</summary>
Motivation: 解决不规则3D物体打包中的形状和稳定性挑战，避免纯模拟训练的效率和计算成本问题。

Method: 利用人类演示学习打包顺序和潜在因素（如空间优化和稳定性），再训练视觉驱动的放置算法。

Result: 实验表明HERB在打包效率和适应性上优于几何和纯RL方法，并在机器人系统中验证了可行性。

Conclusion: 结合人类专业知识的RL方法能有效解决复杂打包问题，提升机器人系统的鲁棒性和适应性。

Abstract: Packing objects efficiently is a fundamental problem in logistics, warehouse
automation, and robotics. While traditional packing solutions focus on
geometric optimization, packing irregular, 3D objects presents significant
challenges due to variations in shape and stability. Reinforcement
Learning~(RL) has gained popularity in robotic packing tasks, but training
purely from simulation can be inefficient and computationally expensive. In
this work, we propose HERB, a human-augmented RL framework for packing
irregular objects. We first leverage human demonstrations to learn the best
sequence of objects to pack, incorporating latent factors such as space
optimization, stability, and object relationships that are difficult to model
explicitly. Next, we train a placement algorithm that uses visual information
to determine the optimal object positioning inside a packing container. Our
approach is validated through extensive performance evaluations, analyzing both
packing efficiency and latency. Finally, we demonstrate the real-world
feasibility of our method on a robotic system. Experimental results show that
our method outperforms geometric and purely RL-based approaches by leveraging
human intuition, improving both packing robustness and adaptability. This work
highlights the potential of combining human expertise-driven RL to tackle
complex real-world packing challenges in robotic systems.

</details>

### [272] [MOSAIC: A Skill-Centric Algorithmic Framework for Long-Horizon Manipulation Planning](https://arxiv.org/abs/2504.16738)
*Itamar Mishani,Yorai Shaoul,Maxim Likhachev*

Main category: cs.RO

TLDR: MOSAIC是一个技能中心框架，通过技能本身指导规划过程，解决机器人长期运动规划问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在技能组合探索、通用技能利用和避免符号世界表示依赖方面存在不足，MOSAIC旨在统一这些元素。

Method: MOSAIC使用生成器和连接器两类技能，生成可执行轨迹并解决边界值问题，以完成复杂任务。

Result: 在模拟和真实机器人任务中，MOSAIC成功解决了复杂长期规划问题。

Conclusion: MOSAIC通过技能中心方法，为复杂长期规划问题提供了鲁棒且可扩展的解决方案。

Abstract: Planning long-horizon motions using a set of predefined skills is a key
challenge in robotics and AI. Addressing this challenge requires methods that
systematically explore skill combinations to uncover task-solving sequences,
harness generic, easy-to-learn skills (e.g., pushing, grasping) to generalize
across unseen tasks, and bypass reliance on symbolic world representations that
demand extensive domain and task-specific knowledge. Despite significant
progress, these elements remain largely disjoint in existing approaches,
leaving a critical gap in achieving robust, scalable solutions for complex,
long-horizon problems. In this work, we present MOSAIC, a skill-centric
framework that unifies these elements by using the skills themselves to guide
the planning process. MOSAIC uses two families of skills: Generators compute
executable trajectories and world configurations, and Connectors link these
independently generated skill trajectories by solving boundary value problems,
enabling progress toward completing the overall task. By breaking away from the
conventional paradigm of incrementally discovering skills from predefined start
or goal states--a limitation that significantly restricts exploration--MOSAIC
focuses planning efforts on regions where skills are inherently effective. We
demonstrate the efficacy of MOSAIC in both simulated and real-world robotic
manipulation tasks, showcasing its ability to solve complex long-horizon
planning problems using a diverse set of skills incorporating generative
diffusion models, motion planning algorithms, and manipulation-specific models.
Visit https://skill-mosaic.github.io for demonstrations and examples.

</details>

### [273] [Meta-Learning Online Dynamics Model Adaptation in Off-Road Autonomous Driving](https://arxiv.org/abs/2504.16923)
*Jacob Levy,Jason Gibson,Bogdan Vlahov,Erica Tevere,Evangelos Theodorou,David Fridovich-Keil,Patrick Spieler*

Main category: cs.RO

TLDR: 论文提出了一种结合卡尔曼滤波在线适应与元学习参数的新框架，用于解决高速越野自动驾驶中的地形-车辆交互建模问题。


<details>
  <summary>Details</summary>
Motivation: 高速越野自动驾驶面临复杂多变的地形特性及难以准确建模的地形-车辆交互问题，现有动力学模型难以泛化到未知地形，需实时适应。

Method: 通过离线元学习优化适应参数和基函数，结合在线卡尔曼滤波动态调整动力学模型，实现实时模型控制。

Result: 实验验证表明，该方法在预测精度、性能和安全性上优于基线方法，尤其在安全关键场景中表现突出。

Conclusion: 元学习动力学模型适应方法有效提升了自动驾驶系统在多样未知环境中的可靠性。

Abstract: High-speed off-road autonomous driving presents unique challenges due to
complex, evolving terrain characteristics and the difficulty of accurately
modeling terrain-vehicle interactions. While dynamics models used in
model-based control can be learned from real-world data, they often struggle to
generalize to unseen terrain, making real-time adaptation essential. We propose
a novel framework that combines a Kalman filter-based online adaptation scheme
with meta-learned parameters to address these challenges. Offline meta-learning
optimizes the basis functions along which adaptation occurs, as well as the
adaptation parameters, while online adaptation dynamically adjusts the onboard
dynamics model in real time for model-based control. We validate our approach
through extensive experiments, including real-world testing on a full-scale
autonomous off-road vehicle, demonstrating that our method outperforms baseline
approaches in prediction accuracy, performance, and safety metrics,
particularly in safety-critical scenarios. Our results underscore the
effectiveness of meta-learned dynamics model adaptation, advancing the
development of reliable autonomous systems capable of navigating diverse and
unseen environments. Video is available at: https://youtu.be/cCKHHrDRQEA

</details>

### [274] [Latent Diffusion Planning for Imitation Learning](https://arxiv.org/abs/2504.16925)
*Amber Xie,Oleh Rybkin,Dorsa Sadigh,Chelsea Finn*

Main category: cs.RO

TLDR: Latent Diffusion Planning (LDP) 是一种模块化方法，通过利用无动作演示和次优数据，在模仿学习中取得优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法依赖大量专家演示，限制了其应用范围。LDP旨在通过利用更广泛的数据（如无动作演示和次优数据）来克服这一限制。

Method: LDP包括一个规划器和一个逆动力学模型，两者在学习的潜在空间中操作。首先通过变分自编码器学习紧凑的潜在空间，然后使用扩散目标训练规划器和逆动力学模型。

Result: 在模拟视觉机器人操作任务中，LDP优于现有模仿学习方法，因为它能利用额外的数据。

Conclusion: LDP通过模块化设计和潜在空间学习，显著提升了模仿学习的性能，尤其是在数据受限的情况下。

Abstract: Recent progress in imitation learning has been enabled by policy
architectures that scale to complex visuomotor tasks, multimodal distributions,
and large datasets. However, these methods often rely on learning from large
amount of expert demonstrations. To address these shortcomings, we propose
Latent Diffusion Planning (LDP), a modular approach consisting of a planner
which can leverage action-free demonstrations, and an inverse dynamics model
which can leverage suboptimal data, that both operate over a learned latent
space. First, we learn a compact latent space through a variational
autoencoder, enabling effective forecasting of future states in image-based
domains. Then, we train a planner and an inverse dynamics model with diffusion
objectives. By separating planning from action prediction, LDP can benefit from
the denser supervision signals of suboptimal and action-free data. On simulated
visual robotic manipulation tasks, LDP outperforms state-of-the-art imitation
learning approaches, as they cannot leverage such additional data.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [275] [Comprehensive Evaluation of Quantitative Measurements from Automated Deep Segmentations of PSMA PET/CT Images](https://arxiv.org/abs/2504.16237)
*Obed Korshie Dzikunu,Amirhossein Toosi,Shadab Ahamed,Sara Harsini,Francois Benard,Xiaoxiao Li,Arman Rahmim*

Main category: eess.IV

TLDR: 该研究通过深度学习分割方法评估了六种定量指标，提出L1加权Dice Focal Loss（L1DFL）在Attention U-Net中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统Dice相似系数评估有限，需更全面的定量指标评估方法。

Method: 使用U-Net、Attention U-Net和SegResNet，结合四种损失函数（包括提出的L1DFL），分析380例PSMA PET/CT扫描。

Result: Attention U-Net与L1DFL表现最佳，SUVmax和TLA相关性最高（0.90-0.99）。

Conclusion: L1DFL显著减少临床测量变异性，适用于定量指标评估。

Abstract: This study performs a comprehensive evaluation of quantitative measurements
as extracted from automated deep-learning-based segmentation methods, beyond
traditional Dice Similarity Coefficient assessments, focusing on six
quantitative metrics, namely SUVmax, SUVmean, total lesion activity (TLA),
tumor volume (TMTV), lesion count, and lesion spread. We analyzed 380
prostate-specific membrane antigen (PSMA) targeted [18F]DCFPyL PET/CT scans of
patients with biochemical recurrence of prostate cancer, training deep neural
networks, U-Net, Attention U-Net and SegResNet with four loss functions: Dice
Loss, Dice Cross Entropy, Dice Focal Loss, and our proposed L1 weighted Dice
Focal Loss (L1DFL). Evaluations indicated that Attention U-Net paired with
L1DFL achieved the strongest correlation with the ground truth (concordance
correlation = 0.90-0.99 for SUVmax and TLA), whereas models employing the Dice
Loss and the other two compound losses, particularly with SegResNet,
underperformed. Equivalence testing (TOST, alpha = 0.05, Delta = 20%) confirmed
high performance for SUV metrics, lesion count and TLA, with L1DFL yielding the
best performance. By contrast, tumor volume and lesion spread exhibited greater
variability. Bland-Altman, Coverage Probability, and Total Deviation Index
analyses further highlighted that our proposed L1DFL minimizes variability in
quantification of the ground truth clinical measures. The code is publicly
available at: https://github.com/ObedDzik/pca\_segment.git.

</details>

### [276] [Frequency-Compensated Network for Daily Arctic Sea Ice Concentration Prediction](https://arxiv.org/abs/2504.16745)
*Jialiang Zhang,Feng Gao,Yanhai Gan,Junyu Dong,Qian Du*

Main category: eess.IV

TLDR: 本文提出了一种频率补偿网络（FCNet）用于北极海冰浓度（SIC）的每日预测，解决了现有方法在频域长期特征依赖和高频细节保留方面的不足。


<details>
  <summary>Details</summary>
Motivation: 准确预测北极海冰浓度对全球生态系统健康和航行安全至关重要，但现有方法在频域特征依赖和高频细节保留方面存在不足。

Method: 设计了双分支网络，包括频率特征提取和卷积特征提取分支，分别通过自适应频率滤波块和高频增强块实现特征提取。

Result: 在卫星每日SIC数据集上的实验验证了FCNet的有效性，特别是在边缘和细节预测上的改进。

Conclusion: FCNet通过频域和空间域特征的结合，显著提升了海冰浓度预测的精度。

Abstract: Accurately forecasting sea ice concentration (SIC) in the Arctic is critical
to global ecosystem health and navigation safety. However, current methods
still is confronted with two challenges: 1) these methods rarely explore the
long-term feature dependencies in the frequency domain. 2) they can hardly
preserve the high-frequency details, and the changes in the marginal area of
the sea ice cannot be accurately captured. To this end, we present a
Frequency-Compensated Network (FCNet) for Arctic SIC prediction on a daily
basis. In particular, we design a dual-branch network, including branches for
frequency feature extraction and convolutional feature extraction. For
frequency feature extraction, we design an adaptive frequency filter block,
which integrates trainable layers with Fourier-based filters. By adding
frequency features, the FCNet can achieve refined prediction of edges and
details. For convolutional feature extraction, we propose a high-frequency
enhancement block to separate high and low-frequency information. Moreover,
high-frequency features are enhanced via channel-wise attention, and temporal
attention unit is employed for low-frequency feature extraction to capture
long-range sea ice changes. Extensive experiments are conducted on a
satellite-derived daily SIC dataset, and the results verify the effectiveness
of the proposed FCNet. Our codes and data will be made public available at:
https://github.com/oucailab/FCNet .

</details>

### [277] [Advanced Chest X-Ray Analysis via Transformer-Based Image Descriptors and Cross-Model Attention Mechanism](https://arxiv.org/abs/2504.16774)
*Lakshita Agarwal,Bindu Verma*

Main category: eess.IV

TLDR: 该研究提出了一种结合Vision Transformer编码器和GPT-4解码器的新模型，用于生成胸部X光图像的描述，显著提升了描述的准确性和丰富性。


<details>
  <summary>Details</summary>
Motivation: 胸部X光图像检查对诊断胸腔疾病至关重要，但传统方法在生成图像描述时存在准确性和上下文不足的问题。

Method: 模型采用ViT编码器提取高质量视觉特征，并通过跨模态注意力与文本数据融合，再由GPT-4解码器生成描述。

Result: 在NIH和IU数据集上，模型在多项指标上表现优异，如BLEU、CIDEr、METEOR和ROUGE-L。

Conclusion: 该框架有望提升胸部X光评估的效率和准确性，辅助放射科医生进行更精确的诊断。

Abstract: The examination of chest X-ray images is a crucial component in detecting
various thoracic illnesses. This study introduces a new image description
generation model that integrates a Vision Transformer (ViT) encoder with
cross-modal attention and a GPT-4-based transformer decoder. The ViT captures
high-quality visual features from chest X-rays, which are fused with text data
through cross-modal attention to improve the accuracy, context, and richness of
image descriptions. The GPT-4 decoder transforms these fused features into
accurate and relevant captions. The model was tested on the National Institutes
of Health (NIH) and Indiana University (IU) Chest X-ray datasets. On the IU
dataset, it achieved scores of 0.854 (B-1), 0.883 (CIDEr), 0.759 (METEOR), and
0.712 (ROUGE-L). On the NIH dataset, it achieved the best performance on all
metrics: BLEU 1--4 (0.825, 0.788, 0.765, 0.752), CIDEr (0.857), METEOR (0.726),
and ROUGE-L (0.705). This framework has the potential to enhance chest X-ray
evaluation, assisting radiologists in more precise and efficient diagnosis.

</details>