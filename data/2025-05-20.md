<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 56]
- [cs.LG](#cs.LG) [Total: 275]
- [cs.CL](#cs.CL) [Total: 193]
- [cs.AI](#cs.AI) [Total: 103]
- [cs.CV](#cs.CV) [Total: 201]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [stat.ML](#stat.ML) [Total: 23]
- [cs.LO](#cs.LO) [Total: 2]
- [eess.SP](#eess.SP) [Total: 5]
- [cs.NI](#cs.NI) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.RO](#cs.RO) [Total: 23]
- [cs.SI](#cs.SI) [Total: 6]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.GT](#cs.GT) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.CY](#cs.CY) [Total: 4]
- [q-bio.GN](#q-bio.GN) [Total: 2]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.MA](#cs.MA) [Total: 6]
- [eess.IV](#eess.IV) [Total: 22]
- [eess.AS](#eess.AS) [Total: 6]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.IR](#cs.IR) [Total: 9]
- [physics.atom-ph](#physics.atom-ph) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 3]
- [math.OC](#math.OC) [Total: 5]
- [econ.GN](#econ.GN) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 11]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Hybrid Privacy Policy-Code Consistency Check using Knowledge Graphs and LLMs](https://arxiv.org/abs/2505.11502)
*Zhenyu Mao,Xinxin Fan,Yifei Wang,Jacky Keung,Jialong Li*

Main category: cs.CR

TLDR: 提出了一种结合知识图谱和LLM的混合方法，用于提高隐私政策与实际行为一致性检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM方法在隐私政策一致性检测中准确率低和计算成本高的问题。

Method: 结合知识图谱的确定性检查和LLM的初步语义分析，以降低计算成本并提高准确性。

Result: 初步评估显示，该方法在精确度上提高了37.63%，F1分数提高了23.13%，同时减少了93.5%的token消耗和87.3%的时间。

Conclusion: 混合方法在隐私政策检测中显著提升了性能并降低了计算成本。

Abstract: The increasing concern in user privacy misuse has accelerated research into
checking consistencies between smartphone apps' declared privacy policies and
their actual behaviors. Recent advances in Large Language Models (LLMs) have
introduced promising techniques for semantic comparison, but these methods
often suffer from low accuracies and expensive computational costs. To address
this problem, this paper proposes a novel hybrid approach that integrates 1)
knowledge graph-based deterministic checking to ensure higher accuracy, and 2)
LLMs exclusively used for preliminary semantic analysis to save computational
costs. Preliminary evaluation indicates this hybrid approach not only achieves
37.63% increase in precision and 23.13% increase F1-score but also consumes
93.5% less tokens and 87.3% shorter time.

</details>

### [2] [MorphMark: Flexible Adaptive Watermarking for Large Language Models](https://arxiv.org/abs/2505.11541)
*Zongqi Wang,Tianle Gu,Baoyuan Wu,Yujiu Yang*

Main category: cs.CR

TLDR: 论文提出了一种名为MorphMark的自适应水印方法，通过动态调整水印强度，解决了现有水印方法在有效性和文本质量之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法在提高水印可检测性时往往牺牲文本质量，限制了实际应用。

Method: 在多目标权衡分析框架下，识别关键因素，并提出MorphMark方法，动态调整水印强度。

Result: 实验表明，MorphMark在有效性和质量之间取得更好平衡，同时具有更高的灵活性和效率。

Conclusion: MorphMark为实际部署提供了实用解决方案，尤其适用于快速发展的AI模型。

Abstract: Watermarking by altering token sampling probabilities based on red-green list
is a promising method for tracing the origin of text generated by large
language models (LLMs). However, existing watermark methods often struggle with
a fundamental dilemma: improving watermark effectiveness (the detectability of
the watermark) often comes at the cost of reduced text quality. This trade-off
limits their practical application. To address this challenge, we first
formalize the problem within a multi-objective trade-off analysis framework.
Within this framework, we identify a key factor that influences the dilemma.
Unlike existing methods, where watermark strength is typically treated as a
fixed hyperparameter, our theoretical insights lead to the development of
MorphMarka method that adaptively adjusts the watermark strength in response to
changes in the identified factor, thereby achieving an effective resolution of
the dilemma. In addition, MorphMark also prioritizes flexibility since it is a
model-agnostic and model-free watermark method, thereby offering a practical
solution for real-world deployment, particularly in light of the rapid
evolution of AI models. Extensive experiments demonstrate that MorphMark
achieves a superior resolution of the effectiveness-quality dilemma, while also
offering greater flexibility and time and space efficiency.

</details>

### [3] [Cybersecurity threat detection based on a UEBA framework using Deep Autoencoders](https://arxiv.org/abs/2505.11542)
*Jose Fuentes,Ines Ortega-Fernandez,Nora M. Villanueva,Marta Sestelo*

Main category: cs.CR

TLDR: 本文提出了一种基于深度自编码器和Doc2Vec的可解释UEBA异常检测框架，能够有效识别真实和合成异常，并提供可解释的结果。


<details>
  <summary>Details</summary>
Motivation: UEBA需要一种能够处理数值和文本特征的可解释异常检测方法，以应对数据泄露等安全威胁。

Method: 结合深度自编码器和Doc2Vec处理多模态数据，并提出一种新的全连接神经网络等价性证明。

Result: 实验表明框架能有效检测异常，并提供可解释的结果，帮助追溯异常来源。

Conclusion: 该框架可无缝集成到企业环境中，增强现有安全系统的可解释威胁检测能力。

Abstract: User and Entity Behaviour Analytics (UEBA) is a broad branch of data
analytics that attempts to build a normal behavioural profile in order to
detect anomalous events. Among the techniques used to detect anomalies, Deep
Autoencoders constitute one of the most promising deep learning models on UEBA
tasks, allowing explainable detection of security incidents that could lead to
the leak of personal data, hijacking of systems, or access to sensitive
business information. In this study, we introduce the first implementation of
an explainable UEBA-based anomaly detection framework that leverages Deep
Autoencoders in combination with Doc2Vec to process both numerical and textual
features. Additionally, based on the theoretical foundations of neural
networks, we offer a novel proof demonstrating the equivalence of two widely
used definitions for fully-connected neural networks. The experimental results
demonstrate the proposed framework capability to detect real and synthetic
anomalies effectively generated from real attack data, showing that the models
provide not only correct identification of anomalies but also explainable
results that enable the reconstruction of the possible origin of the anomaly.
Our findings suggest that the proposed UEBA framework can be seamlessly
integrated into enterprise environments, complementing existing security
systems for explainable threat detection.

</details>

### [4] [On Technique Identification and Threat-Actor Attribution using LLMs and Embedding Models](https://arxiv.org/abs/2505.11547)
*Kyla Guru,Robert J. Moss,Mykel J. Kochenderfer*

Main category: cs.CR

TLDR: 研究评估了大型语言模型（LLMs）在基于行为指标的网络攻击归因中的应用，发现其生成的TTP数据与人工数据存在差异但仍可用于训练模型。


<details>
  <summary>Details</summary>
Motivation: 网络攻击归因复杂且关键，当前从密集法医文档中手动提取行为指标导致显著延迟。

Method: 测试GPT-4和text-embedding-3-large，通过向量嵌入搜索识别TTP并构建攻击者档案。

Result: LLM生成的TTP数据存在噪声，但与MITRE数据集频率相似，可用于训练高于基线的归因模型。

Conclusion: 尽管LLM生成的TTP与人工数据不同，但仍对归因任务有效，提供了一种端到端的解决方案。

Abstract: Attribution of cyber-attacks remains a complex but critical challenge for
cyber defenders. Currently, manual extraction of behavioral indicators from
dense forensic documentation causes significant attribution delays, especially
following major incidents at the international scale. This research evaluates
large language models (LLMs) for cyber-attack attribution based on behavioral
indicators extracted from forensic documentation. We test OpenAI's GPT-4 and
text-embedding-3-large for identifying threat actors' tactics, techniques, and
procedures (TTPs) by comparing LLM-generated TTPs against human-generated data
from MITRE ATT&CK Groups. Our framework then identifies TTPs from text using
vector embedding search and builds profiles to attribute new attacks for a
machine learning model to learn. Key contributions include: (1) assessing
off-the-shelf LLMs for TTP extraction and attribution, and (2) developing an
end-to-end pipeline from raw CTI documents to threat-actor prediction. This
research finds that standard LLMs generate TTP datasets with noise, resulting
in a low similarity to human-generated datasets. However, the TTPs generated
are similar in frequency to those within the existing MITRE datasets.
Additionally, although these TTPs are different than human-generated datasets,
our work demonstrates that they still prove useful for training a model that
performs above baseline on attribution. Project code and files are contained
here: https://github.com/kylag/ttp_attribution.

</details>

### [5] [One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2505.11548)
*Zhiyuan Chang,Xiaojun Jia,Mingyang Li,Junjie Wang,Yuekai Huang,Qing Wang,Ziyou Jiang,Yang Liu*

Main category: cs.CR

TLDR: AuthChain是一种新型知识投毒攻击方法，通过证据链和权威效应生成更具说服力的投毒内容，显著提高攻击成功率并保持隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统依赖外部知识库，存在安全漏洞，尤其是公开可修改的知识库易受投毒攻击。传统多文档投毒攻击易被检测且不实用。

Method: AuthChain利用证据链理论和权威效应，生成具有强证据链和权威声明的投毒内容，克服真实文档和LLM内部知识的干扰。

Result: 在六种流行LLM上的实验表明，AuthChain攻击成功率显著高于现有基线，同时保持对RAG防御机制的高度隐蔽性。

Conclusion: AuthChain为单文档投毒攻击提供了高效且隐蔽的解决方案，揭示了RAG系统在安全性上的潜在风险。

Abstract: Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation
(RAG) have shown improved performance in generating accurate responses.
However, the dependence on external knowledge bases introduces potential
security vulnerabilities, particularly when these knowledge bases are publicly
accessible and modifiable. Poisoning attacks on knowledge bases for RAG systems
face two fundamental challenges: the injected malicious content must compete
with multiple authentic documents retrieved by the retriever, and LLMs tend to
trust retrieved information that aligns with their internal memorized
knowledge. Previous works attempt to address these challenges by injecting
multiple malicious documents, but such saturation attacks are easily detectable
and impractical in real-world scenarios. To enable the effective single
document poisoning attack, we propose AuthChain, a novel knowledge poisoning
attack method that leverages Chain-of-Evidence theory and authority effect to
craft more convincing poisoned documents. AuthChain generates poisoned content
that establishes strong evidence chains and incorporates authoritative
statements, effectively overcoming the interference from both authentic
documents and LLMs' internal knowledge. Extensive experiments across six
popular LLMs demonstrate that AuthChain achieves significantly higher attack
success rates while maintaining superior stealthiness against RAG defense
mechanisms compared to state-of-the-art baselines.

</details>

### [6] [Managerial Insights on Investment Strategy in Cybersecurity: Findings from Multi-Country Research](https://arxiv.org/abs/2505.11549)
*Silvia Tedeschi,Giacomo Marzi,Marco Balzano,Gabriele Costa*

Main category: cs.CR

TLDR: 研究基于欧洲、英国和美国的1083名管理者的调查数据，探讨了网络安全的战略作用。发现网络安全被视为竞争优势，但企业仍面临资源、人才和文化障碍。大企业和高科技公司更主动，而中小企业和低技术行业差异较大。管理上需平衡安全与创新。国家间存在差异，领导力和员工参与是关键。


<details>
  <summary>Details</summary>
Motivation: 探讨网络安全在企业战略中的角色及其面临的挑战。

Method: 基于欧洲、英国和美国1083名管理者的调查数据分析。

Result: 网络安全被视为竞争优势，但企业面临资源、人才和文化障碍；大企业和高科技公司更主动；国家间存在差异。

Conclusion: 领导力和员工参与是缩小战略意图与实际操作差距的关键。

Abstract: This study examines the strategic role of cybersecurity based on survey data
from 1,083 managers across Europe, the UK, and the United States. The findings
indicate growing recognition of cybersecurity as a source of competitive
advantage, although firms continue to face barriers such as limited resources,
talent shortages, and cultural resistance. Larger and high-tech firms tend to
adopt more proactive strategies, while SMEs and low-tech sectors display
greater variability. Key managerial tensions emerge in balancing security with
innovation and agility. Notable country-level differences are observed across
Europe, the UK, and the United States. Across all contexts, leadership and
employee engagement appear central to closing the gap between strategic intent
and operational practice.

</details>

### [7] [A Survey of Learning-Based Intrusion Detection Systems for In-Vehicle Network](https://arxiv.org/abs/2505.11551)
*Muzun Althunayyan,Amir Javed,Omer Rana*

Main category: cs.CR

TLDR: 本文综述了基于学习的车内入侵检测系统（IDS）的研究现状，重点分析了机器学习（ML）、深度学习（DL）和联邦学习（FL）方法，并探讨了其局限性和未来方向。


<details>
  <summary>Details</summary>
Motivation: 联网和自动驾驶汽车（CAVs）面临网络安全威胁，尤其是通过不安全的CAN总线，需要实时检测恶意活动的解决方案。

Method: 通过综述现有研究，分类分析了基于ML、DL和FL的IDS方法，并评估了其检测已知、未知及混合攻击的能力。

Result: 研究发现现有IDS方法存在局限性，需结合多种评估指标以满足安全关键系统的需求。

Conclusion: 本文为未来研究提供了方向，旨在开发更鲁棒和自适应的保护机制，确保CAVs的安全性和可靠性。

Abstract: Connected and Autonomous Vehicles (CAVs) enhance mobility but face
cybersecurity threats, particularly through the insecure Controller Area
Network (CAN) bus. Cyberattacks can have devastating consequences in connected
vehicles, including the loss of control over critical systems, necessitating
robust security solutions. In-vehicle Intrusion Detection Systems (IDSs) offer
a promising approach by detecting malicious activities in real time. This
survey provides a comprehensive review of state-of-the-art research on
learning-based in-vehicle IDSs, focusing on Machine Learning (ML), Deep
Learning (DL), and Federated Learning (FL) approaches. Based on the reviewed
studies, we critically examine existing IDS approaches, categorising them by
the types of attacks they detect - known, unknown, and combined known-unknown
attacks - while identifying their limitations. We also review the evaluation
metrics used in research, emphasising the need to consider multiple criteria to
meet the requirements of safety-critical systems. Additionally, we analyse
FL-based IDSs and highlight their limitations. By doing so, this survey helps
identify effective security measures, address existing limitations, and guide
future research toward more resilient and adaptive protection mechanisms,
ensuring the safety and reliability of CAVs.

</details>

### [8] [AC-LoRA: (Almost) Training-Free Access Control-Aware Multi-Modal LLMs](https://arxiv.org/abs/2505.11557)
*Lara Magdalena Lazier,Aritra Dhar,Vasilije Stambolic,Lukas Cavigelli*

Main category: cs.CR

TLDR: AC-LoRA是一个端到端的系统，用于企业LLM聊天机器人，确保信息隔离，通过权限控制的LoRA适配器和文档嵌入实现高效知识管理。


<details>
  <summary>Details</summary>
Motivation: 当前LLM容易泄露敏感信息，难以在需要严格访问控制的环境中应用，因此设计了AC-LoRA来解决这一问题。

Method: AC-LoRA为权限数据集维护独立的LoRA适配器和文档嵌入，基于用户查询和权限的相似度检索并合并LoRA响应，无需额外训练。

Result: AC-LoRA在性能上匹配或超越现有LoRA混合技术，同时提供强隔离保证，并可扩展到不同模态。

Conclusion: AC-LoRA是一种高效且安全的解决方案，适用于企业环境中需要严格访问控制的LLM应用。

Abstract: Corporate LLMs are gaining traction for efficient knowledge dissemination and
management within organizations. However, as current LLMs are vulnerable to
leaking sensitive information, it has proven difficult to apply them in
settings where strict access control is necessary. To this end, we design
AC-LoRA, an end-to-end system for access control-aware corporate LLM chatbots
that maintains a strong information isolation guarantee. AC-LoRA maintains
separate LoRA adapters for permissioned datasets, along with the document
embedding they are finetuned on. AC-LoRA retrieves a precise set of LoRA
adapters based on the similarity score with the user query and their
permission. This similarity score is later used to merge the responses if more
than one LoRA is retrieved, without requiring any additional training for LoRA
routing. We provide an end-to-end prototype of AC-LoRA, evaluate it on two
datasets, and show that AC-LoRA matches or even exceeds the performance of
state-of-the-art LoRA mixing techniques while providing strong isolation
guarantees. Furthermore, we show that AC-LoRA design can be directly applied to
different modalities.

</details>

### [9] [ACSE-Eval: Can LLMs threat model real-world cloud infrastructure?](https://arxiv.org/abs/2505.11565)
*Sarthak Munshi,Swapnil Pathak,Sonam Ghatode,Thenuga Priyadarshini,Dhivya Chandramouleeswaran,Ashutosh Rana*

Main category: cs.CR

TLDR: 论文提出了AWS Cloud Security Engineering Eval（ACSE-Eval）数据集，用于评估大语言模型（LLMs）在云安全威胁建模中的能力，并比较了GPT 4.1、Gemini 2.5 Pro和Claude 3.7 Sonnet的表现。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在云部署环境中识别安全威胁的有效性，填补现有研究的空白。

Method: 通过ACSE-Eval数据集（包含100个生产级AWS部署场景）评估LLMs的威胁识别、攻击向量分析和缓解策略提出能力。

Result: Gemini 2.5 Pro在零样本场景中表现最佳，GPT 4.1在少样本场景中表现最优，Claude 3.7 Sonnet生成语义更复杂的威胁模型但分类和泛化能力较弱。

Conclusion: 开源数据集和方法以促进自动化网络安全威胁分析的研究，并展示了LLMs在云安全中的潜力。

Abstract: While Large Language Models have shown promise in cybersecurity applications,
their effectiveness in identifying security threats within cloud deployments
remains unexplored. This paper introduces AWS Cloud Security Engineering Eval,
a novel dataset for evaluating LLMs cloud security threat modeling
capabilities. ACSE-Eval contains 100 production grade AWS deployment scenarios,
each featuring detailed architectural specifications, Infrastructure as Code
implementations, documented security vulnerabilities, and associated threat
modeling parameters. Our dataset enables systemic assessment of LLMs abilities
to identify security risks, analyze attack vectors, and propose mitigation
strategies in cloud environments. Our evaluations on ACSE-Eval demonstrate that
GPT 4.1 and Gemini 2.5 Pro excel at threat identification, with Gemini 2.5 Pro
performing optimally in 0-shot scenarios and GPT 4.1 showing superior results
in few-shot settings. While GPT 4.1 maintains a slight overall performance
advantage, Claude 3.7 Sonnet generates the most semantically sophisticated
threat models but struggles with threat categorization and generalization. To
promote reproducibility and advance research in automated cybersecurity threat
analysis, we open-source our dataset, evaluation metrics, and methodologies.

</details>

### [10] [Scaling an ISO Compliance Practice: Strategic Insights from Building a \$1m+ Cybersecurity Certification Line](https://arxiv.org/abs/2505.11583)
*Nishant Sonkar*

Main category: cs.CR

TLDR: 本文介绍了一个成功的网络安全认证实践案例，展示了如何通过ISO认证提升企业信任、合规性和竞争优势。


<details>
  <summary>Details</summary>
Motivation: 随着云优先业务模式的快速发展和全球数据保护法规的收紧，ISO认证（如ISO/IEC 27001、27017和27018）的重要性显著增加，成为企业建立信任、确保合规和获得竞争优势的战略需求。

Method: 通过案例研究，描述了在Armanino LLP设计和实施网络安全认证实践的过程，包括模块化审计模板、认证准备工具包以及与SOC 2和CIS控制的集成。

Result: 该实践在一年内带来了超过100万美元的新服务收入，客户群增长了150%，并成功完成了20多项ISO认证。

Conclusion: 该研究为其他企业提供了可扩展的合规计划模板，有助于应对不断变化的数字风险环境，并丰富了关于审计可扩展性、网络安全合规和ISO标准化的知识。

Abstract: The rapid exponential growth in cloud-first business models and tightened
global data protection regulations have led to the exponential increase in the
level of importance of ISO certifications, especially ISO/IEC 27001, 27017, and
27018, as strategic imperative propositions for organizations wanting to build
trust, ensure compliance, and achieve a competitive advantage. This article
describes a case study of a successful design, implementation, and scaling of a
cybersecurity certification practice in Armanino LLP, a pioneering US
accounting and consulting firm. In reaction to increasing client desires for
formalized information security frameworks, I founded an industry practice from
conception through implementation to aid mid-market and high-growth technology
firms. During one year, the initiative brought in over \$1 million in new
service revenue, expanded our portfolio of cybersecurity clients by 150%, and
produced more than 20 successful ISO certifications on various verticals such
as SaaS, healthcare, and fintech. Based on the strategic wisdom and operational
strategy, this paper outlines the technical architecture of the ISO service
line from modular audit templates to certification readiness kits, from
stakeholder enablement to integration with SOC 2 and CIS controls. The approach
gave value to repeatability, speed, and assurance, thus making Armanino a
reputable certification body. The lessons drawn out provide us with a flexible
template that can be utilized by firms wishing to build strong compliance
programs that can be tailored to address changing digital risk terrains. This
work adds to the increasing knowledge about audit scalability, cybersecurity
compliance, and ISO standardisation.

</details>

### [11] [The Ripple Effect: On Unforeseen Complications of Backdoor Attacks](https://arxiv.org/abs/2505.11586)
*Rui Zhang,Yun Shen,Hongwei Li,Wenbo Jiang,Hanxiao Chen,Yuan Zhang,Guowen Xu,Yang Zhang*

Main category: cs.CR

TLDR: 论文量化了预训练语言模型（PTLM）后门攻击的复杂性，并提出了一种多任务学习方法以减少这种复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究发现后门攻击的PTLM在适应无关下游任务时可能引发意外后果，影响攻击隐蔽性，因此需要量化并解决这一问题。

Method: 通过4种PTLM和16个文本分类数据集实验，量化后门复杂性，并提出基于多任务学习的缓解方法。

Result: 实验表明后门复杂性普遍存在，且提出的方法能有效减少复杂性，同时保持攻击效果。

Conclusion: 论文首次全面量化后门复杂性，并提出了一种无需下游任务先验知识的解决方案。

Abstract: Recent research highlights concerns about the trustworthiness of third-party
Pre-Trained Language Models (PTLMs) due to potential backdoor attacks. These
backdoored PTLMs, however, are effective only for specific pre-defined
downstream tasks. In reality, these PTLMs can be adapted to many other
unrelated downstream tasks. Such adaptation may lead to unforeseen consequences
in downstream model outputs, consequently raising user suspicion and
compromising attack stealthiness. We refer to this phenomenon as backdoor
complications. In this paper, we undertake the first comprehensive
quantification of backdoor complications. Through extensive experiments using 4
prominent PTLMs and 16 text classification benchmark datasets, we demonstrate
the widespread presence of backdoor complications in downstream models
fine-tuned from backdoored PTLMs. The output distribution of triggered samples
significantly deviates from that of clean samples. Consequently, we propose a
backdoor complication reduction method leveraging multi-task learning to
mitigate complications without prior knowledge of downstream tasks. The
experimental results demonstrate that our proposed method can effectively
reduce complications while maintaining the efficacy and consistency of backdoor
attacks. Our code is available at
https://github.com/zhangrui4041/Backdoor_Complications.

</details>

### [12] [Forensics of Error Rates of Quantum Hardware](https://arxiv.org/abs/2505.11706)
*Rupshali Roy,Swaroop Ghosh*

Main category: cs.CR

TLDR: 论文提出了一种通过分析原始和转译后的量子电路来估计后端错误率的方法，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 第三方云量子硬件服务提供商缺乏透明度，用户无法清晰了解程序执行情况，且可能存在资源优化导致错误率上升的问题。因此，需要从执行、转译和结果等多角度了解后端错误率。

Method: 通过分析转译过程中量子比特映射和路由步骤的选择行为，推断后端各部分错误率，并与公开的错误率数据进行对比验证。

Result: 在IBM Sherbrooke和IBM Brisbane后端中，分别有83.5%和80%的量子比特链接的错误率分档与实际误差信息分档的差异不超过2。

Conclusion: 该方法能够有效估计量子硬件的错误率，为量子计算用户提供更多透明度和控制权。

Abstract: There has been a rise in third-party cloud providers offering quantum
hardware as a service to improve performance at lower cost. Although these
providers provide flexibility to the users to choose from several qubit
technologies, quantum hardware, and coupling maps; the actual execution of the
program is not clearly visible to the customer. The success of the user
program, in addition to various other metadata such as cost, performance, &
number of iterations to converge, depends on the error rate of the backend
used. Moreover, the third-party provider and/or tools (e.g., hardware allocator
and mapper) may hold insider/outsider adversarial agents to conserve resources
and maximize profit by running the quantum circuits on error-prone hardware.
Thus it is important to gain visibility of the backend from various
perspectives of the computing process e.g., execution, transpilation and
outcomes. In this paper, we estimate the error rate of the backend from the
original and transpiled circuit. For the forensics, we exploit the fact that
qubit mapping and routing steps of the transpilation process select qubits and
qubit pairs with less single qubit and two-qubit gate errors to minimize
overall error accumulation, thereby, giving us clues about the error rates of
the various parts of the backend. We ranked qubit links into bins based on ECR
error rates publicly available, and compared it to the rankings derived from
our investigation of the relative frequency of a qubit link being chosen by the
transpiler. For upto 83.5% of the qubit links in IBM Sherbrooke and 80% in IBM
Brisbane, 127 qubit IBM backends, we are able to assign a bin rank which has a
difference upto 2 with the bin rank assigned on the basis of actual error rate
information.

</details>

### [13] [Unveiling the Black Box: A Multi-Layer Framework for Explaining Reinforcement Learning-Based Cyber Agents](https://arxiv.org/abs/2505.11708)
*Diksha Goel,Kristen Moore,Jeff Wang,Minjune Kim,Thanh Thi Nguyen*

Main category: cs.CR

TLDR: 提出了一种多层次的强化学习（RL）攻击代理解释框架，揭示战略（MDP层面）和战术（策略层面）的决策过程，以增强网络安全中的透明度和应对能力。


<details>
  <summary>Details</summary>
Motivation: 在网络安全领域，RL代理的决策过程不透明，影响信任和防御准备，因此需要可解释性框架来理解攻击策略的形成和演变。

Method: 通过将网络攻击建模为部分可观测马尔可夫决策过程（POMDPs），分析探索-利用动态和阶段行为变化；在策略层面，利用优先经验回放（PER）分析Q值的时序演化和关键学习转变。

Result: 在CyberBattleSim环境中验证，框架提供了可扩展的代理行为解释，支持从红队模拟到RL策略调试的多种应用。

Conclusion: 该框架将黑盒学习转化为可操作的行为情报，帮助防御者和开发者更好地预测和分析自主网络威胁。

Abstract: Reinforcement Learning (RL) agents are increasingly used to simulate
sophisticated cyberattacks, but their decision-making processes remain opaque,
hindering trust, debugging, and defensive preparedness. In high-stakes
cybersecurity contexts, explainability is essential for understanding how
adversarial strategies are formed and evolve over time. In this paper, we
propose a unified, multi-layer explainability framework for RL-based attacker
agents that reveals both strategic (MDP-level) and tactical (policy-level)
reasoning. At the MDP level, we model cyberattacks as a Partially Observable
Markov Decision Processes (POMDPs) to expose exploration-exploitation dynamics
and phase-aware behavioural shifts. At the policy level, we analyse the
temporal evolution of Q-values and use Prioritised Experience Replay (PER) to
surface critical learning transitions and evolving action preferences.
Evaluated across CyberBattleSim environments of increasing complexity, our
framework offers interpretable insights into agent behaviour at scale. Unlike
previous explainable RL methods, which are often post-hoc, domain-specific, or
limited in depth, our approach is both agent- and environment-agnostic,
supporting use cases ranging from red-team simulation to RL policy debugging.
By transforming black-box learning into actionable behavioural intelligence,
our framework enables both defenders and developers to better anticipate,
analyse, and respond to autonomous cyber threats.

</details>

### [14] [Co-Evolutionary Defence of Active Directory Attack Graphs via GNN-Approximated Dynamic Programming](https://arxiv.org/abs/2505.11710)
*Diksha Goel,Hussain Ahmad,Kristen Moore,Mingyu Guo*

Main category: cs.CR

TLDR: 论文提出了一种基于Stackelberg博弈的主动防御框架，结合图神经网络和进化多样性优化，以应对动态攻击者行为，并在实验中展示了其高效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现代企业网络依赖Active Directory（AD）进行身份和访问管理，但其集中化特性成为单点故障，现有防御方法假设攻击者行为静态，无法应对动态攻击。

Method: 通过Stackelberg博弈建模攻击者与防御者互动，结合图神经网络动态规划和进化多样性优化生成防御策略，并引入固定参数可处理图简化方法提高可扩展性。

Result: 在合成AD图上实验显示，框架性能接近最优（r500误差0.1%），并在更大规模图（r1000和r2000）上表现优异。

Conclusion: 该框架通过联合优化攻击者和防御者策略，提升了防御的泛化能力和抗动态攻击能力，同时具备可扩展性。

Abstract: Modern enterprise networks increasingly rely on Active Directory (AD) for
identity and access management. However, this centralization exposes a single
point of failure, allowing adversaries to compromise high-value assets.
Existing AD defense approaches often assume static attacker behavior, but
real-world adversaries adapt dynamically, rendering such methods brittle. To
address this, we model attacker-defender interactions in AD as a Stackelberg
game between an adaptive attacker and a proactive defender. We propose a
co-evolutionary defense framework that combines Graph Neural Network
Approximated Dynamic Programming (GNNDP) to model attacker strategies, with
Evolutionary Diversity Optimization (EDO) to generate resilient blocking
strategies. To ensure scalability, we introduce a Fixed-Parameter Tractable
(FPT) graph reduction method that reduces complexity while preserving strategic
structure. Our framework jointly refines attacker and defender policies to
improve generalization and prevent premature convergence. Experiments on
synthetic AD graphs show near-optimal results (within 0.1 percent of optimality
on r500) and improved performance on larger graphs (r1000 and r2000),
demonstrating the framework's scalability and effectiveness.

</details>

### [15] [Decentralized Multi-Authority Attribute-Based Inner-Product Functional Encryption: Noisy and Evasive Constructions from Lattices](https://arxiv.org/abs/2505.11744)
*Jiaqi Liu,Yan Wang,Fang-Wei Fu*

Main category: cs.CR

TLDR: 论文提出了两种新的多权威属性基功能加密方案：支持近似内积计算的MA-AB(N)IPFE和基于伪随机性的MA-evIPFE，并基于新的格假设构建了这些方案。


<details>
  <summary>Details</summary>
Motivation: 研究多权威属性基功能加密，以支持噪声内积功能，扩展现有方案并引入更宽松的安全概念。

Method: 基于格假设，引入新的计算假设（如evasive IPFE和IND-evIPFE），并构建支持子集策略的加密方案。

Result: 提出了两种新方案，证明其在随机预言模型下的静态安全性，并展示了MA-AB(N)IPFE可转换为无噪声方案。

Conclusion: 这些方案为多权威环境下的噪声容忍和细粒度访问控制提供了实用候选方案。

Abstract: We study multi-authority attribute-based functional encryption for noisy
inner-product functionality, and propose two new primitives: (1)
multi-authority attribute-based (noisy) inner-product functional encryption
(MA-AB(N)IPFE), which generalizes existing multi-authority attribute-based IPFE
schemes by Agrawal et al. (TCC'21), by enabling approximate inner-product
computation; and (2) multi-authority attribute-based evasive inner-product
functional encryption (MA-evIPFE), a relaxed variant inspired by the evasive
IPFE framework by Hsieh et al. (EUROCRYPT'24), shifting focus from ciphertext
indistinguishability to a more relaxed pseudorandomness-based security notion.
To support the above notions, we introduce two variants of lattice-based
computational assumptions: evasive IPFE assumption and
indistinguishability-based evasive IPFE assumption (IND-evIPFE). We present
lattice-based constructions of both primitives for subset policies, building
upon the framework of Waters et al.( TCC'22). Our schemes are proven to be
statically secure in the random oracle model under the standard LWE assumption
and the newly introduced assumptions. Additionally, we show our MA-AB(N)IPFE
scheme can be transformed via modulus switching into a noiseless MA-IPFE scheme
that supports exact inner-product functionality. This yields the first
lattice-based construction of such a primitive. All our schemes support
arbitrary polynomial-size attribute policies and are secure in the random
oracle model under lattice assumptions with a sub-exponential modulus-to-noise
ratio, making them practical candidates for noise-tolerant, fine-grained access
control in multi-authority settings.

</details>

### [16] [Benchmarking LLMs in an Embodied Environment for Blue Team Threat Hunting](https://arxiv.org/abs/2505.11901)
*Xiaoqun Liu,Feiyang Yu,Xi Li,Guanhua Yan,Ping Yang,Zhaohan Xi*

Main category: cs.CR

TLDR: CYBERTEAM是一个为蓝队威胁狩猎设计的基准测试，通过模块化步骤指导LLMs完成任务，评估其在现实网络安全中的能力。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁日益复杂，蓝队需要先进工具主动检测风险，但LLMs在实际威胁狩猎中的效果尚未充分探索。

Method: CYBERTEAM分两阶段构建环境：1) 模拟真实威胁狩猎流程；2) 为每个任务定制功能模块，形成结构化操作序列。

Result: 评估了领先LLMs和网络安全代理，比较了功能调用与基础策略，揭示了LLMs在威胁狩猎中的能力与局限。

Conclusion: CYBERTEAM为LLMs在现实网络安全中的实际应用奠定了基础。

Abstract: As cyber threats continue to grow in scale and sophistication, blue team
defenders increasingly require advanced tools to proactively detect and
mitigate risks. Large Language Models (LLMs) offer promising capabilities for
enhancing threat analysis. However, their effectiveness in real-world blue team
threat-hunting scenarios remains insufficiently explored. In this paper, we
present CYBERTEAM, a benchmark designed to guide LLMs in blue teaming practice.
CYBERTEAM constructs an embodied environment in two stages. First, it models
realistic threat-hunting workflows by capturing the dependencies among
analytical tasks from threat attribution to incident response. Next, each task
is addressed through a set of embodied functions tailored to its specific
analytical requirements. This transforms the overall threat-hunting process
into a structured sequence of function-driven operations, where each node
represents a discrete function and edges define the execution order. Guided by
this framework, LLMs are directed to perform threat-hunting tasks through
modular steps. Overall, CYBERTEAM integrates 30 tasks and 9 embodied functions,
guiding LLMs through pipelined threat analysis. We evaluate leading LLMs and
state-of-the-art cybersecurity agents, comparing CYBERTEAM's embodied
function-calling against fundamental elicitation strategies. Our results offer
valuable insights into the current capabilities and limitations of LLMs in
threat hunting, laying the foundation for the practical adoption in real-world
cybersecurity applications.

</details>

### [17] [MARVEL: Multi-Agent RTL Vulnerability Extraction using Large Language Models](https://arxiv.org/abs/2505.11963)
*Luca Collini,Baleegh Ahmad,Joey Ah-kiow,Ramesh Karri*

Main category: cs.CR

TLDR: MARVEL是一个多代理LLM框架，用于硬件安全验证，通过模拟设计师的认知过程，结合多种工具和方法，显著提高了漏洞检测效率。


<details>
  <summary>Details</summary>
Motivation: 硬件安全验证复杂且耗时，现有工具和方法需要进一步整合和优化以提高效率。

Method: MARVEL采用多代理架构，包括一个监督代理和多个执行代理，结合形式化工具、静态分析、模拟测试和LLM检测等多种方法。

Result: 在OpenTitan的测试中，MARVEL报告的48个问题中有20个是实际的安全漏洞。

Conclusion: MARVEL通过多代理框架和多样化工具的结合，显著提升了硬件安全验证的效率和准确性。

Abstract: Hardware security verification is a challenging and time-consuming task. For
this purpose, design engineers may utilize tools such as formal verification,
linters, and functional simulation tests, coupled with analysis and a deep
understanding of the hardware design being inspected. Large Language Models
(LLMs) have been used to assist during this task, either directly or in
conjunction with existing tools. We improve the state of the art by proposing
MARVEL, a multi-agent LLM framework for a unified approach to decision-making,
tool use, and reasoning. MARVEL mimics the cognitive process of a designer
looking for security vulnerabilities in RTL code. It consists of a supervisor
agent that devises the security policy of the system-on-chips (SoCs) using its
security documentation. It delegates tasks to validate the security policy to
individual executor agents. Each executor agent carries out its assigned task
using a particular strategy. Each executor agent may use one or more tools to
identify potential security bugs in the design and send the results back to the
supervisor agent for further analysis and confirmation. MARVEL includes
executor agents that leverage formal tools, linters, simulation tests,
LLM-based detection schemes, and static analysis-based checks. We test our
approach on a known buggy SoC based on OpenTitan from the Hack@DATE
competition. We find that 20 of the 48 issues reported by MARVEL pose security
vulnerabilities.

</details>

### [18] [TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique Annotation in Cyber Threat Intelligence Text](https://arxiv.org/abs/2505.11988)
*Ahmed Lekssays,Utsav Shukla,Husrev Taha Sencar,Md Rizwan Parvez*

Main category: cs.CR

TLDR: TechniqueRAG是一个领域特定的检索增强生成框架，通过结合现成检索器、指令调优的LLM和少量文本-技术对，解决了现有方法在数据稀缺和资源密集问题上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在识别安全文本中的对抗技术时，要么依赖通用模型导致领域精度不足，要么需要资源密集的优化流程，而领域内数据稀缺。

Method: TechniqueRAG通过仅对生成组件进行微调，避免资源密集的检索训练，并利用零样本LLM重新排序提升检索质量和领域特异性。

Result: 实验表明，TechniqueRAG在多个安全基准测试中达到最先进性能，无需大量任务特定优化或标注数据。

Conclusion: TechniqueRAG为领域特定任务提供了一种高效且资源节约的解决方案，显著提升了对抗技术识别的精度。

Abstract: Accurately identifying adversarial techniques in security texts is critical
for effective cyber defense. However, existing methods face a fundamental
trade-off: they either rely on generic models with limited domain precision or
require resource-intensive pipelines that depend on large labeled datasets and
task-specific optimizations, such as custom hard-negative mining and denoising,
resources rarely available in specialized domains.
  We propose TechniqueRAG, a domain-specific retrieval-augmented generation
(RAG) framework that bridges this gap by integrating off-the-shelf retrievers,
instruction-tuned LLMs, and minimal text-technique pairs. Our approach
addresses data scarcity by fine-tuning only the generation component on limited
in-domain examples, circumventing the need for resource-intensive retrieval
training. While conventional RAG mitigates hallucination by coupling retrieval
and generation, its reliance on generic retrievers often introduces noisy
candidates, limiting domain-specific precision. To address this, we enhance
retrieval quality and domain specificity through zero-shot LLM re-ranking,
which explicitly aligns retrieved candidates with adversarial techniques.
  Experiments on multiple security benchmarks demonstrate that TechniqueRAG
achieves state-of-the-art performance without extensive task-specific
optimizations or labeled data, while comprehensive analysis provides further
insights.

</details>

### [19] [A Human Study of Cognitive Biases in CTF Challenges](https://arxiv.org/abs/2505.12018)
*Yuwei Yang,Skyler Grandel,Daniel Balasubramanian,Yu Huang,Kevin Leach*

Main category: cs.CR

TLDR: 研究探讨了认知偏差（如搜索满意度和损失厌恶）如何影响CTF竞赛中的参与者表现，发现这些偏差显著降低了参与者的成功率。


<details>
  <summary>Details</summary>
Motivation: CTF竞赛是网络安全教育的重要工具，但参与者的决策过程研究不足，心理学因素可能影响攻击者行为。

Method: 通过定量和定性分析，设计CTF任务触发认知偏差，研究其对参与者表现的影响。

Result: 参与者中搜索满意度偏差显著，导致找到的标志数量平均减少25%。

Conclusion: 认知偏差可被战略性地用于提升网络安全教育和测量效果。

Abstract: Cybersecurity training has become a crucial part of computer science
education and industrial onboarding. Capture the Flag (CTF) competitions have
emerged as a valuable, gamified approach for developing and refining the skills
of cybersecurity and software engineering professionals. However, while CTFs
provide a controlled environment for tackling real world challenges, the
participants' decision making and problem solving processes remain under
explored. Recognizing that psychology may play a role in a cyber attacker's
behavior, we investigate how cognitive biases could be used to improve CTF
education and security. In this paper, we present an approach to control
cognitive biases, specifically Satisfaction of Search and Loss Aversion, to
influence and potentially hinder attackers' effectiveness against web
application vulnerabilities in a CTF style challenge. We employ a rigorous
quantitative and qualitative analysis through a controlled human study of CTF
tasks. CTF exercises are widely used in cybersecurity education and research to
simulate real world attack scenarios and help participants develop critical
skills by solving security challenges in controlled environments. In our study,
participants interact with a web application containing deliberately embedded
vulnerabilities while being subjected to tasks designed to trigger cognitive
biases. Our study reveals that many participants exhibit the Satisfaction of
Search bias and that this bias has a significant effect on their success. On
average, participants found 25% fewer flags compared to those who did not
exhibit this bias. Our findings provide valuable insights into how cognitive
biases can be strategically employed to enhance cybersecurity outcomes,
education, and measurements through the lens of CTF challenges.

</details>

### [20] [FL-PLAS: Federated Learning with Partial Layer Aggregation for Backdoor Defense Against High-Ratio Malicious Clients](https://arxiv.org/abs/2505.12019)
*Jianyi Zhang,Ziyin Zhou,Yilong Li,Qichao Jin*

Main category: cs.CR

TLDR: FL-PLAS是一种基于部分层聚合策略的联邦学习防御算法，能有效抵御后门攻击，即使恶意用户比例高达90%。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）易受后门攻击，现有防御方法难以应对高比例恶意用户或需要辅助数据集。

Method: 将本地模型分为特征提取器和分类器，仅上传特征提取器参数进行聚合，保留分类器层以避免后门标签传播。

Result: 实验表明，FL-PLAS在三种图像数据集上对多种后门攻击具有高防御效果，主任务准确率高且无需服务器辅助数据集。

Conclusion: FL-PLAS是一种高效且无需额外数据的防御方法，适用于高恶意用户比例的场景。

Abstract: Federated learning (FL) is gaining increasing attention as an emerging
collaborative machine learning approach, particularly in the context of
large-scale computing and data systems. However, the fundamental algorithm of
FL, Federated Averaging (FedAvg), is susceptible to backdoor attacks. Although
researchers have proposed numerous defense algorithms, two significant
challenges remain. The attack is becoming more stealthy and harder to detect,
and current defense methods are unable to handle 50\% or more malicious users
or assume an auxiliary server dataset.
  To address these challenges, we propose a novel defense algorithm, FL-PLAS,
\textbf{F}ederated \textbf{L}earning based on \textbf{P}artial\textbf{ L}ayer
\textbf{A}ggregation \textbf{S}trategy. In particular, we divide the local
model into a feature extractor and a classifier. In each iteration, the clients
only upload the parameters of a feature extractor after local training. The
server then aggregates these local parameters and returns the results to the
clients.
  Each client retains its own classifier layer, ensuring that the backdoor
labels do not impact other clients. We assess the effectiveness of FL-PLAS
against state-of-the-art (SOTA) backdoor attacks on three image datasets and
compare our approach to six defense strategies. The results of the experiment
demonstrate that our methods can effectively protect local models from backdoor
attacks. Without requiring any auxiliary dataset for the server, our method
achieves a high main-task accuracy with a lower backdoor accuracy even under
the condition of 90\% malicious users with the attacks of trigger, semantic and
edge-case.

</details>

### [21] [The Impact of Emerging Phishing Threats: Assessing Quishing and LLM-generated Phishing Emails against Organizations](https://arxiv.org/abs/2505.12104)
*Marie Weinz,Nicola Zannone,Luca Allodi,Giovanni Apruzzese*

Main category: cs.CR

TLDR: 论文研究了两种新型钓鱼攻击（QR码钓鱼和LLM辅助钓鱼）的实际效果，并通过模拟实验和员工调查评估了现有防御措施的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管钓鱼检测系统和员工培训有所进步，但攻击者不断创新，尤其是QR码和LLM辅助钓鱼成为新威胁。当前防御措施对这些攻击的实际效果尚不明确。

Method: 通过三种钓鱼模拟实验（传统钓鱼、QR码钓鱼、LLM辅助钓鱼）和员工调查，评估攻击效果和防御能力。

Result: QR码钓鱼与传统钓鱼效果相当，但更难检测；LLM辅助钓鱼成功率高达30%。员工自评安全意识与组织防御能力相关。

Conclusion: 新型钓鱼攻击威胁显著，需针对性改进防御措施，并提升员工安全意识。

Abstract: Modern organizations are persistently targeted by phishing emails. Despite
advances in detection systems and widespread employee training, attackers
continue to innovate, posing ongoing threats. Two emerging vectors stand out in
the current landscape: QR-code baits and LLM-enabled pretexting. Yet, little is
known about the effectiveness of current defenses against these attacks,
particularly when it comes to real-world impact on employees. This gap leaves
uncertainty around to what extent related countermeasures are justified or
needed. Our work addresses this issue.
  We conduct three phishing simulations across organizations of varying sizes
-- from small-medium businesses to a multinational enterprise. In total, we
send over 71k emails targeting employees, including: a "traditional" phishing
email with a click-through button; a nearly-identical "quishing" email with a
QR code instead; and a phishing email written with the assistance of an LLM and
open-source intelligence. Our results show that quishing emails have the same
effectiveness as traditional phishing emails at luring users to the landing
webpage -- which is worrying, given that quishing emails are much harder to
identify even by operational detectors. We also find that LLMs can be very good
"social engineers": in one company, over 30% of the emails opened led to
visiting the landing webpage -- a rate exceeding some prior benchmarks.
Finally, we complement our study by conducting a survey across the
organizations' employees, measuring their "perceived" phishing awareness. Our
findings suggest a correlation between higher self-reported awareness and
organizational resilience to phishing attempts.

</details>

### [22] [MalVis: A Large-Scale Image-Based Framework and Dataset for Advancing Android Malware Classification](https://arxiv.org/abs/2505.12106)
*Saleh J. Makkawy,Michael J. De Lucia,Kenneth E. Barner*

Main category: cs.CR

TLDR: MalVis是一个统一的恶意软件可视化框架，结合熵和N-gram分析，显著提升恶意代码检测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: Android恶意软件威胁日益严重，传统检测方法难以应对混淆技术，现有深度学习方法未能有效突出关键恶意特征。

Method: 提出MalVis框架，整合熵和N-gram分析，使用大规模数据集（130万样本）和多种CNN模型，结合集成学习和欠采样技术。

Result: MalVis表现优异：准确率95.19%，F1分数90.81%，ROC-AUC 98.06%。

Conclusion: MalVis为恶意软件检测提供了高效、可解释的解决方案，并为安全研究提供了宝贵资源。

Abstract: As technology advances, Android malware continues to pose significant threats
to devices and sensitive data. The open-source nature of the Android OS and the
availability of its SDK contribute to this rapid growth. Traditional malware
detection techniques, such as signature-based, static, and dynamic analysis,
struggle to detect obfuscated threats that use encryption, packing, or
compression. While deep learning (DL)-based visualization methods have been
proposed, they often fail to highlight the critical malicious features
effectively. This research introduces MalVis, a unified visualization framework
that integrates entropy and N-gram analysis to emphasize structural and
anomalous patterns in malware bytecode. MalVis addresses key limitations of
prior methods, including insufficient feature representation, poor
interpretability, and limited data accessibility. The framework leverages a
newly introduced large-scale dataset, the MalVis dataset, containing over 1.3
million visual samples across nine malware classes and one benign class. We
evaluate MalVis against state-of-the-art visualization techniques using leading
CNN models: MobileNet-V2, DenseNet201, ResNet50, and Inception-V3. To enhance
performance and reduce overfitting, we implement eight ensemble learning
strategies. Additionally, an undersampling technique mitigates class imbalance
in the multiclass setting. MalVis achieves strong results: 95.19% accuracy,
90.81% F1-score, 92.58% precision, 89.10% recall, 87.58% MCC, and 98.06%
ROC-AUC. These findings demonstrate the effectiveness of MalVis in enabling
accurate, interpretable malware detection and providing a valuable resource for
security research and applications.

</details>

### [23] [Back to Square Roots: An Optimal Bound on the Matrix Factorization Error for Multi-Epoch Differentially Private SGD](https://arxiv.org/abs/2505.12128)
*Nikita P. Kalinin,Ryan McKenna,Jalaj Upadhyay,Christoph H. Lampert*

Main category: cs.CR

TLDR: 提出了一种新的显式矩阵分解方法BISR，用于差分隐私训练中的多轮误差优化，填补了理论与实际之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私训练中的矩阵分解方法在多轮训练时误差界限存在显著差距，需要更优的解决方案。

Method: 引入Banded Inverse Square Root (BISR)方法，通过对逆相关矩阵施加带状结构，实现多轮误差的显式紧致刻画。

Result: BISR在理论上达到渐近最优误差，并在实验中与现有最优方法性能相当，同时更简单高效。

Conclusion: BISR是一种高效且易于分析的差分隐私训练方法，填补了理论与实际的误差差距。

Abstract: Matrix factorization mechanisms for differentially private training have
emerged as a promising approach to improve model utility under privacy
constraints. In practical settings, models are typically trained over multiple
epochs, requiring matrix factorizations that account for repeated
participation. Existing theoretical upper and lower bounds on multi-epoch
factorization error leave a significant gap. In this work, we introduce a new
explicit factorization method, Banded Inverse Square Root (BISR), which imposes
a banded structure on the inverse correlation matrix. This factorization
enables us to derive an explicit and tight characterization of the multi-epoch
error. We further prove that BISR achieves asymptotically optimal error by
matching the upper and lower bounds. Empirically, BISR performs on par with
state-of-the-art factorization methods, while being simpler to implement,
computationally efficient, and easier to analyze.

</details>

### [24] [Proof-of-Social-Capital: Privacy-Preserving Consensus Protocol Replacing Stake for Social Capital](https://arxiv.org/abs/2505.12144)
*Juraj Mariani,Ivan Homoliak*

Main category: cs.CR

TLDR: 提出了一种基于社会资本（信任和影响力）的新型区块链共识协议，结合零知识证明和激励机制，以增强公平性和去中心化。


<details>
  <summary>Details</summary>
Motivation: 传统区块链共识协议依赖稀缺资源（如算力或金融质押），而社会资本作为一种非可转移资源，可能提供更公平和去中心化的解决方案。

Method: 整合零知识证明、可验证凭证、类似Whisk的领导者选举机制和激励机制，防止女巫攻击并鼓励参与。

Result: 理论框架提升了隐私和公平性，但仍需解决链外贿赂等问题。

Conclusion: 该协议为去中心化系统开发提供了新思路，适用于金融等领域，并与现代社交媒体行为相契合。

Abstract: Consensus protocols used today in blockchains often rely on computational
power or financial stakes - scarce resources. We propose a novel protocol using
social capital - trust and influence from social interactions - as a
non-transferable staking mechanism to ensure fairness and decentralization. The
methodology integrates zero-knowledge proofs, verifiable credentials, a
Whisk-like leader election, and an incentive scheme to prevent Sybil attacks
and encourage engagement. The theoretical framework would enhance privacy and
equity, though unresolved issues like off-chain bribery require further
research. This work offers a new model aligned with modern social media
behavior and lifestyle, with applications in finance, providing a practical
insight for decentralized system development.

</details>

### [25] [Nonmalleable Progress Leakage](https://arxiv.org/abs/2505.12210)
*Ethan Cecchetti*

Main category: cs.CR

TLDR: 该论文提出了一种结合进展敏感安全性与安全降级（去分类和认可）的方法，定义了非可延展进展泄漏（NMPL），并首次实现了允许部分进展泄漏的信息流类型系统。


<details>
  <summary>Details</summary>
Motivation: 现实程序需要去分类结果和认可输入，而传统的进展不敏感非干扰性无法满足这些需求，同时忽略了攻击者通过进展通道控制泄漏的问题。

Method: 结合进展敏感安全性与安全降级，使用超属性区分进展敏感与非敏感非干扰性，并定义NMPL。提出信息流类型系统，支持部分进展泄漏并验证其安全性。

Result: 首次实现了允许部分进展泄漏的类型系统，并验证了所有定理。

Conclusion: NMPL为安全降级进展信息提供了新方法，类型系统实现了理论验证的实际应用。

Abstract: Information-flow control systems often enforce progress-insensitive
noninterference, as it is simple to understand and enforce. Unfortunately, real
programs need to declassify results and endorse inputs, which noninterference
disallows, while preventing attackers from controlling leakage, including
through progress channels, which progress-insensitivity ignores.
  This work combines ideas for progress-sensitive security with secure
downgrading (declassification and endorsement) to identify a notion of securely
downgrading progress information. We use hyperproperties to distill the
separation between progress-sensitive and progress-insensitive noninterference
and combine it with nonmalleable information flow, an existing
(progress-insensitive) definition of secure downgrading, to define nonmalleable
progress leakage (NMPL). We present the first information-flow type system to
allow some progress leakage while enforcing NMPL, and we show how to infer the
location of secure progress downgrades. All theorems are verified in Rocq.

</details>

### [26] [TPM2.0-Supported Runtime Customizable TEE on FPGA-SoC with User-Controllable vTPM](https://arxiv.org/abs/2505.12256)
*Jingkai Mao,Xiaolin Chang*

Main category: cs.CR

TLDR: 本文提出了一种在FPGA-SoC上构建兼容TPM 2.0的可定制TEE的方法，通过用户可控的虚拟TPM（vTPM）保护IP核心。


<details>
  <summary>Details</summary>
Motivation: 为促进FPGA-SoC TEE的广泛应用，需支持动态测量、部署和调用IP，同时确保安全性和可验证性。

Method: 提出FPGA-vTPM架构，扩展TPM指令集以支持FPGA-SoC TEE的敏感操作，并在Xilinx Zynq平台上实现原型。

Result: 原型验证了方法的实用性和增强的安全性。

Conclusion: 该方法成功实现了FPGA-SoC TEE的动态安全操作，符合TPM 2.0规范。

Abstract: Constructing a Trusted Execution Environment (TEE) on Field Programmable Gate
Array System on Chip (FPGA-SoC) in Cloud can effectively protect users' private
intel-lectual Property (IP) cores. In order to facilitate the wide-spread
deployment of FPGA-SoC TEE, this paper proposes an approach for constructing a
TPM 2.0-compatible runtime customizable TEE on FPGA-SoC. This approach
leverages a user-controllable virtual Trusted Platform Module (vTPM) that
integrates sensitive operations specific to FPGA-SoC TEE. It provides TPM 2.0
support for a customizable FPGA-SoC TEE to dynamically measure, deploy, and
invoke IP during runtime. Our main contributions include: (i) Propose an
FPGA-vTPM architecture that enables the TPM 2.0 specification support for
FPGA-SoC TEE; (ii) Explore the utilization of FPGA-vTPM to dynamically measure,
deploy, and invoke users' IPs on FPGA-SoC TEE; (iii) Extend the TPM command set
to accommodate the sensitive operations of FPGA-SoC TEE, enabling users to
perform sensitive tasks in a secure and verifiable manner according to the TPM
2.0 specification. We implement a prototype of TRCTEE on the Xilinx Zynq
UltraScale+ MPSoC platform and conducted security analysis and performance
evaluations to prove the practicality and enhanced security features of this
approach.

</details>

### [27] [PoLO: Proof-of-Learning and Proof-of-Ownership at Once with Chained Watermarking](https://arxiv.org/abs/2505.12296)
*Haiyu Deng,Yanna Jiang,Guangsheng Yu,Qin Wang,Xu Wang,Baihe Ma,Wei Ni,Ren Ping Liu*

Main category: cs.CR

TLDR: PoLO是一个统一框架，通过链式水印同时实现训练验证（PoL）和所有权证明（PoO），提高效率并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型的共享和外包，需要同时验证训练努力和所有权，但现有方法分开处理，导致保护不足和验证开销高。

Method: PoLO将训练过程分为细粒度训练片段，每个片段嵌入基于前一片段哈希的专用水印，形成链式结构。

Result: PoLO在所有权验证中达到99%的水印检测准确率，验证成本降至传统方法的1.5-10%，且抗伪造能力强。

Conclusion: PoLO高效、隐私友好且安全，为模型训练验证和所有权保护提供了统一解决方案。

Abstract: Machine learning models are increasingly shared and outsourced, raising
requirements of verifying training effort (Proof-of-Learning, PoL) to ensure
claimed performance and establishing ownership (Proof-of-Ownership, PoO) for
transactions. When models are trained by untrusted parties, PoL and PoO must be
enforced together to enable protection, attribution, and compensation. However,
existing studies typically address them separately, which not only weakens
protection against forgery and privacy breaches but also leads to high
verification overhead.
  We propose PoLO, a unified framework that simultaneously achieves PoL and PoO
using chained watermarks. PoLO splits the training process into fine-grained
training shards and embeds a dedicated watermark in each shard. Each watermark
is generated using the hash of the preceding shard, certifying the training
process of the preceding shard. The chained structure makes it computationally
difficult to forge any individual part of the whole training process. The
complete set of watermarks serves as the PoL, while the final watermark
provides the PoO. PoLO offers more efficient and privacy-preserving
verification compared to the vanilla PoL solutions that rely on gradient-based
trajectory tracing and inadvertently expose training data during verification,
while maintaining the same level of ownership assurance of watermark-based PoO
schemes. Our evaluation shows that PoLO achieves 99% watermark detection
accuracy for ownership verification, while preserving data privacy and cutting
verification costs to just 1.5-10% of traditional methods. Forging PoLO demands
1.1-4x more resources than honest proof generation, with the original proof
retaining over 90% detection accuracy even after attacks.

</details>

### [28] [Automated Profile Inference with Language Model Agents](https://arxiv.org/abs/2505.12402)
*Yuntao Du,Zitao Li,Bolin Ding,Yaliang Li,Hanshen Xiao,Jingren Zhou,Ninghui Li*

Main category: cs.CR

TLDR: 论文研究LLM对在线匿名性的威胁，提出自动化框架AutoProfiler，实验证明其高效且易部署，隐私风险显著。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在自动化问题解决中的恶意应用，特别是对在线匿名性的威胁。

Method: 提出AutoProfiler框架，包含四个协作的LLM代理，用于收集和处理用户在线活动并生成个人资料。

Result: 实验显示AutoProfiler高效且易部署，推断的属性敏感且可识别，隐私风险高。

Conclusion: 呼吁提高公众对这一新兴隐私威胁的认识，并探讨缓解策略。

Abstract: Impressive progress has been made in automated problem-solving by the
collaboration of large language models (LLMs) based agents. However, these
automated capabilities also open avenues for malicious applications. In this
paper, we study a new threat that LLMs pose to online pseudonymity, called
automated profile inference, where an adversary can instruct LLMs to
automatically scrape and extract sensitive personal attributes from publicly
visible user activities on pseudonymous platforms. We also introduce an
automated profiling framework called AutoProfiler to assess the feasibility of
such threats in real-world scenarios. AutoProfiler consists of four specialized
LLM agents, who work collaboratively to collect and process user online
activities and generate a profile with extracted personal information.
Experimental results on two real-world datasets and one synthetic dataset
demonstrate that AutoProfiler is highly effective and efficient, and can be
easily deployed on a web scale. We demonstrate that the inferred attributes are
both sensitive and identifiable, posing significant risks of privacy breaches,
such as de-anonymization and sensitive information leakage. Additionally, we
explore mitigation strategies from different perspectives and advocate for
increased public awareness of this emerging privacy threat to online
pseudonymity.

</details>

### [29] [IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2505.12442)
*Liwen Wang,Wenxuan Wang,Shuai Wang,Zongjie Li,Zhenlan Ji,Zongyi Lyu,Daoyuan Wu,Shing-Chi Cheung*

Main category: cs.CR

TLDR: MASLEAK是一种新型攻击框架，用于从多代理系统（MAS）中提取敏感信息，攻击成功率高。


<details>
  <summary>Details</summary>
Motivation: 随着多代理系统的广泛应用，其知识产权保护问题日益突出，需要研究潜在的攻击方法。

Method: MASLEAK通过精心设计的对抗性查询，在无需了解MAS架构的情况下，通过公共API提取系统组件信息。

Result: MASLEAK在合成数据集和真实应用中表现优异，攻击成功率达87%-92%。

Conclusion: 研究揭示了MAS的知识产权风险，并探讨了可能的防御措施。

Abstract: The rapid advancement of Large Language Models (LLMs) has led to the
emergence of Multi-Agent Systems (MAS) to perform complex tasks through
collaboration. However, the intricate nature of MAS, including their
architecture and agent interactions, raises significant concerns regarding
intellectual property (IP) protection. In this paper, we introduce MASLEAK, a
novel attack framework designed to extract sensitive information from MAS
applications. MASLEAK targets a practical, black-box setting, where the
adversary has no prior knowledge of the MAS architecture or agent
configurations. The adversary can only interact with the MAS through its public
API, submitting attack query $q$ and observing outputs from the final agent.
Inspired by how computer worms propagate and infect vulnerable network hosts,
MASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain
responses from each MAS agent that reveal a full set of proprietary components,
including the number of agents, system topology, system prompts, task
instructions, and tool usages. We construct the first synthetic dataset of MAS
applications with 810 applications and also evaluate MASLEAK against real-world
MAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in
extracting MAS IP, with an average attack success rate of 87% for system
prompts and task instructions, and 92% for system architecture in most cases.
We conclude by discussing the implications of our findings and the potential
defenses.

</details>

### [30] [SecEmb: Sparsity-Aware Secure Federated Learning of On-Device Recommender System with Large Embedding](https://arxiv.org/abs/2505.12453)
*Peihua Mai,Youlong Ding,Ziyan Lyu,Minxin Du,Yan Pang*

Main category: cs.CR

TLDR: SecEmb是一种安全高效的联邦推荐系统协议，通过稀疏嵌入更新减少用户负载，同时保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 解决联邦推荐系统中模型传输带来的带宽和计算负担，同时保护用户评分项索引的隐私。

Method: 提出SecEmb协议，包含隐私保护的嵌入检索模块和安全更新聚合模块。

Result: SecEmb将通信成本降低90倍，用户计算时间减少70倍，且优于有损压缩方法。

Conclusion: SecEmb在保护隐私的同时显著提升了联邦推荐系统的效率和实用性。

Abstract: Federated recommender system (FedRec) has emerged as a solution to protect
user data through collaborative training techniques. A typical FedRec involves
transmitting the full model and entire weight updates between edge devices and
the server, causing significant burdens to devices with limited bandwidth and
computational power. While the sparsity of embedding updates provides
opportunity for payload optimization, existing sparsity-aware federated
protocols generally sacrifice privacy for efficiency. A key challenge in
designing a secure sparsity-aware efficient protocol is to protect the rated
item indices from the server. In this paper, we propose a lossless secure
recommender systems on sparse embedding updates (SecEmb). SecEmb reduces user
payload while ensuring that the server learns no information about both rated
item indices and individual updates except the aggregated model. The protocol
consists of two correlated modules: (1) a privacy-preserving embedding
retrieval module that allows users to download relevant embeddings from the
server, and (2) an update aggregation module that securely aggregates updates
at the server. Empirical analysis demonstrates that SecEmb reduces both
download and upload communication costs by up to 90x and decreases user-side
computation time by up to 70x compared with secure FedRec protocols.
Additionally, it offers non-negligible utility advantages compared with lossy
message compression methods.

</details>

### [31] [Proposal for Improving Google A2A Protocol: Safeguarding Sensitive Data in Multi-Agent Systems](https://arxiv.org/abs/2505.12490)
*Yedidel Louck,Ariel Stulman,Amit Dvir*

Main category: cs.CR

TLDR: 本文分析了A2A协议在AI代理通信中的局限性，提出了七项增强措施以提升安全性、隐私和信任。


<details>
  <summary>Details</summary>
Motivation: A2A协议在处理敏感数据（如支付信息、身份文件和个人信息）时存在关键问题，亟需改进。

Method: 通过审查现有协议、识别局限性，并提出具体增强措施（如短时效令牌、客户认证等），结合研究支持和实施考虑。

Result: 提出的七项增强措施（如直接数据传输、多交易批准）通过假期预订案例展示了风险降低和用户体验提升。

Conclusion: 本文为A2A协议提供了可行的安全增强方案，通过具体案例验证了其有效性。

Abstract: A2A, a protocol for AI agent communication, offers a robust foundation for
secure AI agent communication. However, it has several critical issues in
handling sensitive data, such as payment details, identification documents, and
personal information. This paper reviews the existing protocol, identifies its
limitations, and proposes specific enhancements to improve security, privacy,
and trust. It includes a concrete example to illustrate the problem and
solution, research-backed rationales, and implementation considerations,
drawing on prior studies to strengthen the arguments and proposed solutions.
This proposal includes seven enhancements: short-lived tokens, customer
authentication (SCA), granular scopes, explicit consent, direct data transfer,
multi-transaction approval, and payment standard compliance. The vacation
booking example illustrates how these enhancements reduce risks and enhance
user experience.

</details>

### [32] [A Survey of Attacks on Large Language Models](https://arxiv.org/abs/2505.12567)
*Wenrui Xu,Keshab K. Parhi*

Main category: cs.CR

TLDR: 本文系统综述了针对大语言模型（LLMs）及其代理的对抗攻击，分为训练阶段攻击、推理阶段攻击以及可用性与完整性攻击三类，并分析了代表性攻击方法及防御措施。


<details>
  <summary>Details</summary>
Motivation: LLMs的广泛应用暴露了安全与可靠性风险，如恶意滥用、隐私泄露和服务中断，威胁用户信任和社会安全。

Method: 将攻击分为三类，分析每类中的代表性攻击方法及防御措施。

Result: 提供了对LLM安全的全面理解，强调攻击细节及其防御。

Conclusion: 呼吁关注LLM应用的风险，并提出需要针对不断演变的威胁制定鲁棒的缓解策略。

Abstract: Large language models (LLMs) and LLM-based agents have been widely deployed
in a wide range of applications in the real world, including healthcare
diagnostics, financial analysis, customer support, robotics, and autonomous
driving, expanding their powerful capability of understanding, reasoning, and
generating natural languages. However, the wide deployment of LLM-based
applications exposes critical security and reliability risks, such as the
potential for malicious misuse, privacy leakage, and service disruption that
weaken user trust and undermine societal safety. This paper provides a
systematic overview of the details of adversarial attacks targeting both LLMs
and LLM-based agents. These attacks are organized into three phases in LLMs:
Training-Phase Attacks, Inference-Phase Attacks, and Availability & Integrity
Attacks. For each phase, we analyze the details of representative and recently
introduced attack methods along with their corresponding defenses. We hope our
survey will provide a good tutorial and a comprehensive understanding of LLM
security, especially for attacks on LLMs. We desire to raise attention to the
risks inherent in widely deployed LLM-based applications and highlight the
urgent need for robust mitigation strategies for evolving threats.

</details>

### [33] [Compile-Time Fully Homomorphic Encryption: Eliminating Online Encryption via Algebraic Basis Synthesis](https://arxiv.org/abs/2505.12582)
*Dongfang Zhao*

Main category: cs.CR

TLDR: 提出了一种新的全同态加密（FHE）系统中编译时密文合成框架，通过预计算加密基向量和同态操作实现高效密文生成。


<details>
  <summary>Details</summary>
Motivation: 解决传统运行时加密算法效率低下的问题，实现密文生成与加密的解耦，支持高效批量编码。

Method: 利用预计算的加密基向量，仅通过同态加法、标量乘法和零的随机加密合成密文，并将其形式化为随机模块态射。

Result: 证明了该方法满足IND-CPA安全性，并通过混合游戏框架将安全性归约到底层FHE方案的不可区分性。

Conclusion: 该方法适用于外包数据库系统等高吞吐量FHE场景，兼容标准FHE API并保持下游同态操作的布局语义。

Abstract: We propose a new framework for compile-time ciphertext synthesis in fully
homomorphic encryption (FHE) systems. Instead of invoking encryption algorithms
at runtime, our method synthesizes ciphertexts from precomputed encrypted basis
vectors using only homomorphic additions, scalar multiplications, and
randomized encryptions of zero. This decouples ciphertext generation from
encryption, and enables efficient batch encoding through algebraic reuse. We
formalize this technique as a randomized module morphism and prove that it
satisfies IND-CPA security. Our proof uses a hybrid game framework that
interpolates between encrypted vector instances and reduces adversarial
advantage to the indistinguishability of the underlying FHE scheme. This
reduction structure captures the security implications of ciphertext basis
reuse and structured noise injection. The proposed synthesis primitive supports
fast, encryption-free ingestion in outsourced database systems and other
high-throughput FHE pipelines. It is compatible with standard FHE APIs and
preserves layout semantics for downstream homomorphic operations.

</details>

### [34] [hChain: Blockchain Based Large Scale EHR Data Sharing with Enhanced Security and Privacy](https://arxiv.org/abs/2505.12610)
*Musharraf Alruwaill,Saraju Mohanty,Elias Kougianos*

Main category: cs.CR

TLDR: 区块链技术与医疗物联网（IoMT）结合，通过边缘设备和加密技术提升智能医疗系统的安全性、隐私性和数据完整性。


<details>
  <summary>Details</summary>
Motivation: 传统医疗存在隐私和数据安全问题，区块链技术为解决这些问题提供了新途径。

Method: 结合区块链与IoMT，利用边缘设备进行实时数据加密和哈希处理，采用对称和非对称密钥保护数据隐私和完整性。

Result: 提出了一种基于区块链的智能医疗系统，通过hChain架构提升计算能力、数据安全性和隐私保护。

Conclusion: 区块链与IoMT的结合为智能医疗提供了高效、安全的解决方案，未来可进一步优化。

Abstract: Concerns regarding privacy and data security in conventional healthcare
prompted alternative technologies. In smart healthcare, blockchain technology
addresses existing concerns with security, privacy, and electronic healthcare
transmission. Integration of Blockchain Technology with the Internet of Medical
Things (IoMT) allows real-time monitoring of protected healthcare data.
Utilizing edge devices with IoMT devices is very advantageous for addressing
security, computing, and storage challenges. Encryption using symmetric and
asymmetric keys is used to conceal sensitive information from unauthorized
parties. SHA256 is an algorithm for one-way hashing. It is used to verify that
the data has not been altered, since if it had, the hash value would have
changed. This article offers a blockchain-based smart healthcare system using
IoMT devices for continuous patient monitoring. In addition, it employs edge
resources in addition to IoMT devices to have extra computing power and storage
to hash and encrypt incoming data before sending it to the blockchain.
Symmetric key is utilized to keep the data private even in the blockchain,
allowing the patient to safely communicate the data through smart contracts
while preventing unauthorized physicians from seeing the data. Through the use
of a verification node and blockchain, an asymmetric key is used for the
signing and validation of patient data in the healthcare provider system. In
addition to other security measures, location-based authentication is
recommended to guarantee that data originates from the patient area. Through
the edge device, SHA256 is utilized to secure the data's integrity and a secret
key is used to maintain its secrecy. The hChain architecture improves the
computing power of IoMT environments, the security of EHR sharing through smart
contracts, and the privacy and authentication procedures.

</details>

### [35] [EPSpatial: Achieving Efficient and Private Statistical Analytics of Geospatial Data](https://arxiv.org/abs/2505.12612)
*Chuan Zhang,Xuhao Ren,Zhangcheng Huang,Jinwen Liang,Jianzong Wang,Liehuang Zhu*

Main category: cs.CR

TLDR: 论文提出了一种名为EPSpatial的方案，结合空间分布式点函数（SDPF）和Gray码分区，用于高效、准确且隐私保护的geospatial数据统计分析。


<details>
  <summary>Details</summary>
Motivation: 由于现有隐私保护技术在geospatial数据分析中存在高开销和结果不准确的问题，且移动设备的实时更新需求增加了复杂性，因此需要一种更高效的解决方案。

Method: 设计了SDPF（结合四叉树和分布式点函数），并与Gray码分区结合，提出了EPSpatial方案，同时提供了高效的更新算法以应对客户频繁移动的需求。

Result: EPSpatial在保护隐私的同时，计算和通信开销比现有方案至少减少50%。

Conclusion: EPSpatial是一种高效、准确且隐私保护的geospatial数据分析方案，适用于实时更新的移动设备场景。

Abstract: Geospatial data statistics involve the aggregation and analysis of location
data to derive the distribution of clients within geospatial. The need for
privacy protection in geospatial data analysis has become paramount due to
concerns over the misuse or unauthorized access of client location information.
However, existing private geospatial data statistics mainly rely on privacy
computing techniques such as cryptographic tools and differential privacy,
which leads to significant overhead and inaccurate results. In practical
applications, geospatial data is frequently generated by mobile devices such as
smartphones and IoT sensors. The continuous mobility of clients and the need
for real-time updates introduce additional complexity. To address these issues,
we first design \textit{spatially distributed point functions (SDPF)}, which
combines a quad-tree structure with distributed point functions, allowing
clients to succinctly secret-share values on the nodes of an exponentially
large quad-tree. Then, we use Gray code to partition the region and combine
SDPF with it to propose $\mathtt{EPSpatial}$, a scheme for accurate, efficient,
and private statistical analytics of geospatial data. Moreover, considering
clients' frequent movement requires continuous location updates, we leverage
the region encoding property to present an efficient update algorithm.Security
analysis shows that $\mathtt{EPSpatial}$ effectively protects client location
privacy. Theoretical analysis and experimental results on real datasets
demonstrate that $\mathtt{EPSpatial}$ reduces computational and communication
overhead by at least $50\%$ compared to existing statistical schemes.

</details>

### [36] [Towards Centralized Orchestration of Cyber Protection Condition (CPCON)](https://arxiv.org/abs/2505.12613)
*Mark Timmons,Daniel Lukaszewski,Geoffrey Xie,Thomas Mayo,Donald McCanless*

Main category: cs.CR

TLDR: 本文提出了一种集中式协调系统，用于自动化执行美国网络司令部（USCYBERCOM）的网络保护条件（CPCON）指令，以提高安全策略的执行效率和实时威胁响应能力。


<details>
  <summary>Details</summary>
Motivation: 当前美国国防部（DoD）网络中CPCON框架的实施主要依赖手动操作，存在不一致性和易出错的问题，亟需自动化解决方案。

Method: 基于主机入侵响应的先前研究，开发了一个策略驱动的协调器，用于标准化安全操作、隔离受攻击子网并验证执行状态。

Result: 通过模拟攻击场景验证，系统在CPCON转换中表现出更高的速度、准确性和可验证性，同时保留人工监督。

Conclusion: 该系统为CPCON指令的自动化执行提供了可行方案，显著提升了网络安全响应的效率和可靠性。

Abstract: The United States Cyber Command (USCYBERCOM) Cyber Protection Condition
(CPCON) framework mandates graduated security postures across Department of
Defense (DoD) networks, but current implementation remains largely manual,
inconsistent, and error-prone. This paper presents a prototype system for
centralized orchestration of CPCON directives, enabling automated policy
enforcement and real-time threat response across heterogeneous network
environments. Building on prior work in host-based intrusion response, our
system leverages a policy-driven orchestrator to standardize security actions,
isolate compromised subnets, and verify enforcement status. We validate the
system through emulated attack scenarios, demonstrating improved speed,
accuracy, and verifiability in CPCON transitions with human-in-the-loop
oversight.

</details>

### [37] [GDPRShield: AI-Powered GDPR Support for Software Developers in Small and Medium-Sized Enterprises](https://arxiv.org/abs/2505.12640)
*Tharaka Wijesundara,Mathew Warren,Nalin Arachchilage*

Main category: cs.CR

TLDR: 论文提出了一种名为GDPRShield的AI框架，旨在提升中小企业开发者的GDPR意识和隐私态度，从而改善合规性。


<details>
  <summary>Details</summary>
Motivation: 中小企业开发者因多角色工作导致隐私意识不足，可能违反GDPR，而合规性可以带来竞争优势。

Method: GDPRShield通过用户故事生成定制化的GDPR隐私描述，并展示不合规的实际后果。

Result: 该框架提高了开发者的GDPR意识和动机，有助于中小企业建立更强的隐私文化。

Conclusion: GDPRShield通过双重关注意识和动机，有效提升了GDPR合规性和隐私态度。

Abstract: With the rapid increase in privacy violations in modern software development,
regulatory frameworks such as the General Data Protection Regulation (GDPR)
have been established to enforce strict data protection practices. However,
insufficient privacy awareness among SME software developers contributes to
failure in GDPR compliance. For instance, a developer unfamiliar with data
minimization may build a system that collects excessive data, violating GDPR
and risking fines. One reason for this lack of awareness is that developers in
SMEs often take on multidisciplinary roles (e.g., front-end, back-end, database
management, and privacy compliance), which limits specialization in privacy.
This lack of awareness may lead to poor privacy attitudes, ultimately hindering
the development of a strong organizational privacy culture. However, SMEs that
achieve GDPR compliance may gain competitive advantages, such as increased user
trust and marketing value, compared to others that do not.
  Therefore, in this paper, we introduce a novel AI-powered framework called
"GDPRShield," specifically designed to enhance the GDPR awareness of SME
software developers and, through this, improve their privacy attitudes.
Simultaneously, GDPRShield boosts developers' motivation to comply with GDPR
from the early stages of software development. It leverages functional
requirements written as user stories to provide comprehensive GDPR-based
privacy descriptions tailored to each requirement. Alongside improving
awareness, GDPRShield strengthens motivation by presenting real-world
consequences of noncompliance, such as heavy fines, reputational damage, and
loss of user trust, aligned with each requirement. This dual focus on awareness
and motivation leads developers to engage with GDPRShield, improving their GDPR
compliance and privacy attitudes, which will help SMEs build a stronger privacy
culture over time.

</details>

### [38] [Web IP at Risk: Prevent Unauthorized Real-Time Retrieval by Large Language Models](https://arxiv.org/abs/2505.12655)
*Yisheng Zhong,Yizhu Wen,Junfeng Guo,Mehran Kafai,Heng Huang,Hanqing Guo,Zhuangdi Zhu*

Main category: cs.CR

TLDR: 论文提出了一种新防御框架，利用LLM的语义理解能力保护网络内容创作者的IP，防止未经授权的实时提取。实验显示防御成功率从2.5%提升至88.6%。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的普及，用户依赖其生成内容减少了对原始信息的直接访问，削弱了内容创作者的激励。

Method: 通过LLM的语义理解能力设计防御框架，解决黑盒优化问题。

Result: 实验表明防御成功率显著提升，从2.5%增至88.6%。

Conclusion: 该方法有效保护了网络IP，优于传统配置限制方法。

Abstract: Protecting cyber Intellectual Property (IP) such as web content is an
increasingly critical concern. The rise of large language models (LLMs) with
online retrieval capabilities presents a double-edged sword that enables
convenient access to information but often undermines the rights of original
content creators. As users increasingly rely on LLM-generated responses, they
gradually diminish direct engagement with original information sources,
significantly reducing the incentives for IP creators to contribute, and
leading to a saturating cyberspace with more AI-generated content. In response,
we propose a novel defense framework that empowers web content creators to
safeguard their web-based IP from unauthorized LLM real-time extraction by
leveraging the semantic understanding capability of LLMs themselves. Our method
follows principled motivations and effectively addresses an intractable
black-box optimization problem. Real-world experiments demonstrated that our
methods improve defense success rates from 2.5% to 88.6% on different LLMs,
outperforming traditional defenses such as configuration-based restrictions.

</details>

### [39] [Shielding Latent Face Representations From Privacy Attacks](https://arxiv.org/abs/2505.12688)
*Arjun Ramesh Kaushik,Bharat Chandra Yalavarthi,Arun Ross,Vishnu Boddeti,Nalini Ratha*

Main category: cs.CR

TLDR: 论文提出了一种多层保护框架，结合全同态加密和不可逆特征哈希，保护人脸嵌入中的敏感信息，同时保留功能性能。


<details>
  <summary>Details</summary>
Motivation: 人脸嵌入可能泄露敏感属性（如年龄、性别），威胁隐私和公民权利，需解决这一问题。

Method: 采用全同态加密（FHE）和不可逆特征哈希，结合嵌入压缩以减少加密处理开销。

Result: 在两个数据集和两种人脸编码器上的实验表明，该方法优于现有隐私保护方法。

Conclusion: 该方法有效保护敏感信息，同时保持功能性能，为隐私保护提供了新思路。

Abstract: In today's data-driven analytics landscape, deep learning has become a
powerful tool, with latent representations, known as embeddings, playing a
central role in several applications. In the face analytics domain, such
embeddings are commonly used for biometric recognition (e.g., face
identification). However, these embeddings, or templates, can inadvertently
expose sensitive attributes such as age, gender, and ethnicity. Leaking such
information can compromise personal privacy and affect civil liberty and human
rights. To address these concerns, we introduce a multi-layer protection
framework for embeddings. It consists of a sequence of operations: (a)
encrypting embeddings using Fully Homomorphic Encryption (FHE), and (b) hashing
them using irreversible feature manifold hashing. Unlike conventional
encryption methods, FHE enables computations directly on encrypted data,
allowing downstream analytics while maintaining strong privacy guarantees. To
reduce the overhead of encrypted processing, we employ embedding compression.
Our proposed method shields latent representations of sensitive data from
leaking private attributes (such as age and gender) while retaining essential
functional capabilities (such as face identification). Extensive experiments on
two datasets using two face encoders demonstrate that our approach outperforms
several state-of-the-art privacy protection methods.

</details>

### [40] [An Automated Blackbox Noncompliance Checker for QUIC Server Implementations](https://arxiv.org/abs/2505.12690)
*Kian Kai Ang,Guy Farrelly,Cheryl Pope,Damith C. Ranasinghe*

Main category: cs.CR

TLDR: QUICtester是一种自动化方法，用于检测QUIC协议实现中的非合规行为，通过主动自动机学习和状态依赖分析，发现55个错误和1个规范模糊性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉QUIC实现中的状态依赖和时序问题，需要一种无需正式模型的非合规检测工具。

Method: 利用主动自动机学习生成有限状态机，结合事件时序变化和多样化输入配置，通过差分分析识别行为偏差。

Result: 分析了19个QUIC实现的186个模型，发现55个错误、1个规范模糊性、5个CVE漏洞和2个漏洞赏金。

Conclusion: QUICtester有效检测QUIC实现中的非合规行为，揭示了规范模糊性和实际漏洞，具有实际应用价值。

Abstract: We develop QUICtester, an automated approach for uncovering non-compliant
behaviors in the ratified QUIC protocol implementations (RFC 9000/9001).
QUICtester leverages active automata learning to abstract the behavior of a
QUIC implementation into a finite state machine (FSM) representation. Unlike
prior noncompliance checking methods, to help uncover state dependencies on
event timing, QUICtester introduces the idea of state learning with event
timing variations, adopting both valid and invalid input configurations, and
combinations of security and transport layer parameters during learning. We use
pairwise differential analysis of learned behaviour models of tested QUIC
implementations to identify non-compliance instances as behaviour deviations in
a property-agnostic way. This exploits the existence of the many different QUIC
implementations, removing the need for validated, formal models. The diverse
implementations act as cross-checking test oracles to discover non-compliance.
We used QUICtester to analyze analyze 186 learned models from 19 QUIC
implementations under the five security settings and discovered 55
implementation errors. Significantly, the tool uncovered a QUIC specification
ambiguity resulting in an easily exploitable DoS vulnerability, led to 5 CVE
assignments from developers, and two bug bounties thus far.

</details>

### [41] [Writing a Good Security Paper for ISSCC (2025)](https://arxiv.org/abs/2505.12700)
*Utsav Banerjee,Chiraag Juvekar,Yong Ki Lee,Leibo Liu,Sanu Mathew,Thomas Poeppelmann,Shreyas Sen,Takeshi Sugawara,Ingrid Verbauwhede,Rabia Tugce Yazicigil*

Main category: cs.CR

TLDR: ISSCC 2024设立安全子委员会以推动硬件安全领域的高质量投稿。


<details>
  <summary>Details</summary>
Motivation: 硬件安全在芯片和系统设计中日益重要，ISSCC希望通过安全子委员会促进该领域的发展。

Method: 作者作为安全子委员会成员，回顾了过去两年的投稿情况。

Result: 通过分析投稿，鼓励更多高质量的安全相关研究提交至ISSCC。

Conclusion: ISSCC致力于通过安全子委员会推动硬件安全领域的进步，并期待更多高质量投稿。

Abstract: Security is increasingly more important in designing chips and systems based
on them, and the International Solid-State Circuits Conference (ISSCC), the
leading conference for presenting advances in solid-state circuits and
semiconductor technology, is committed to hardware security by establishing the
security subcommittee since 2024. In the past two years, the authors of this
paper reviewed submissions as members of the Security Subcommittee, a part of
International Technical Program Committee (ITPC). This paper aims to encourage
high-quality submissions to grow this field in the overall scope of the ISSCC.

</details>

### [42] [Malware families discovery via Open-Set Recognition on Android manifest permissions](https://arxiv.org/abs/2505.12750)
*Filippo Leveni,Matteo Mistura,Francesco Iubatti,Carmine Giangregorio,Nicolò Pastore,Cesare Alippi,Giacomo Boracchi*

Main category: cs.CR

TLDR: 论文提出了一种结合MaxLogit和梯度提升树的恶意软件分类系统，能够识别已知和新出现的恶意软件家族。


<details>
  <summary>Details</summary>
Motivation: 恶意软件分类对网络安全防御至关重要，但高维权限数据和训练样本有限性增加了分类难度，尤其是新恶意软件家族不断涌现。

Method: 结合计算机视觉领域的MaxLogit技术和梯度提升树分类器，处理高维数据并检测新恶意软件。

Result: 实验证明该系统高效实用，已在商业环境中部署。

Conclusion: 该系统为恶意软件分类提供了有效解决方案，尤其适用于新恶意软件的检测。

Abstract: Malware are malicious programs that are grouped into families based on their
penetration technique, source code, and other characteristics. Classifying
malware programs into their respective families is essential for building
effective defenses against cyber threats. Machine learning models have a huge
potential in malware detection on mobile devices, as malware families can be
recognized by classifying permission data extracted from Android manifest
files. Still, the malware classification task is challenging due to the
high-dimensional nature of permission data and the limited availability of
training samples. In particular, the steady emergence of new malware families
makes it impossible to acquire a comprehensive training set covering all the
malware classes. In this work, we present a malware classification system that,
on top of classifying known malware, detects new ones. In particular, we
combine an open-set recognition technique developed within the computer vision
community, namely MaxLogit, with a tree-based Gradient Boosting classifier,
which is particularly effective in classifying high-dimensional data. Our
solution turns out to be very practical, as it can be seamlessly employed in a
standard classification workflow, and efficient, as it adds minimal
computational overhead. Experiments on public and proprietary datasets
demonstrate the potential of our solution, which has been deployed in a
business environment.

</details>

### [43] [Testing Access-Control Configuration Changes for Web Applications](https://arxiv.org/abs/2505.12770)
*Chengcheng Xiang,Li Zhong,Eric Mugnier,Nathaniel Nguyen,Yuanyuan Zhou,Tianyin Xu*

Main category: cs.CR

TLDR: 本文提出了一种名为ACtests的新方法，用于自动测试访问控制配置变更，以检测潜在的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 访问控制配置错误是当前Web应用数据泄露的主要原因之一，但缺乏自动化和系统化的测试方法。

Method: ACtests通过创建一个包含生产环境和数据的迷你测试环境，报告访问控制变更的影响，包括被拒绝但变更后允许的请求，以及相反情况。

Result: 在193个Dockerhub配置中，ACtests检测到168个新漏洞，其中54个被确认，44个已修复。在五个实际系统中，ACtests高效检测了所有变更影响。

Conclusion: ACtests是一种有效且高效的方法，可用于测试访问控制配置变更并识别安全风险。

Abstract: Access-control misconfigurations are among the main causes of today's data
breaches in web applications. However, few techniques are available to support
automatic and systematic testing for access-control changes and detecting risky
changes to prevent severe consequences. As a result, those critical security
configurations often lack testing, or are tested manually in an ad hoc way.
  This paper advocates that tests should be made available for users to test
access-control configuration changes. The key challenges are such tests need to
be run with production environments (to reason end-to-end behavior) and need to
be performance-efficient. We present a new approach to create such tests, as a
mini test environment incorporating production program and data, called
ACtests. ACtests report the impacts of access-control changes, namely the
requests that were denied but would be allowed after a change, and vice versa.
Users can validate if the changed requests are intended or not and identify
potential security vulnerabilities.
  We evaluate ACtests with 193 public configurations of widely-used web
applications on Dockerhub. ACtests detect 168 new vulnerabilities from 72
configuration images. We report them to the image maintainers: 54 of them have
been confirmed and 44 have been fixed. We also conduct in-depth experiments
with five real-world deployed systems, including Wikipedia and a commercial
company's web proxy. Our results show that ACtests effectively and efficiently
detect all the change impacts.

</details>

### [44] [FLTG: Byzantine-Robust Federated Learning via Angle-Based Defense and Non-IID-Aware Weighting](https://arxiv.org/abs/2505.12851)
*Yanhua Wen,Lu Ai,Gang Liu,Chuang Li,Jianhao Wei*

Main category: cs.CR

TLDR: FLTG是一种新的联邦学习聚合算法，通过角度防御和动态参考选择解决拜占庭攻击问题，在高恶意客户端比例和非i.i.d.数据下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高恶意客户端比例和非i.i.d.数据下鲁棒性不足，导致准确性下降。

Method: FLTG通过ReLU剪裁余弦相似度筛选客户端，动态选择参考客户端，并基于角度偏差分配聚合权重。

Result: 在多种数据集和攻击下，FLTG优于现有方法，支持超过50%恶意客户端比例。

Conclusion: FLTG在极端偏差和高恶意比例下保持鲁棒性，是联邦学习中的有效防御方案。

Abstract: Byzantine attacks during model aggregation in Federated Learning (FL)
threaten training integrity by manipulating malicious clients' updates.
Existing methods struggle with limited robustness under high malicious client
ratios and sensitivity to non-i.i.d. data, leading to degraded accuracy. To
address this, we propose FLTG, a novel aggregation algorithm integrating
angle-based defense and dynamic reference selection. FLTG first filters clients
via ReLU-clipped cosine similarity, leveraging a server-side clean dataset to
exclude misaligned updates. It then dynamically selects a reference client
based on the prior global model to mitigate non-i.i.d. bias, assigns
aggregation weights inversely proportional to angular deviations, and
normalizes update magnitudes to suppress malicious scaling. Evaluations across
datasets of varying complexity under five classic attacks demonstrate FLTG's
superiority over state-of-the-art methods under extreme bias scenarios and
sustains robustness with a higher proportion(over 50%) of malicious clients.

</details>

### [45] [Outsourced Privacy-Preserving Feature Selection Based on Fully Homomorphic Encryption](https://arxiv.org/abs/2505.12869)
*Koki Wakiyama,Tomohiro I,Hiroshi Sakamoto*

Main category: cs.CR

TLDR: 提出了一种基于全同态加密的隐私保护特征选择算法，显著提升了计算效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 在大规模数据场景下，特征选择需要保护隐私，尤其是数据所有者与分析者为不同方时。现有方法依赖多方计算且安全性不足。

Method: 采用全同态加密技术，提出首个外包特征选择算法，优化了时间和空间复杂度。

Result: 算法复杂度从O(kn^2)降至O(kn log^3 n)和O(kn)，实验验证了其高效性。

Conclusion: 该方法为隐私保护特征选择提供了高效且安全的解决方案。

Abstract: Feature selection is a technique that extracts a meaningful subset from a set
of features in training data. When the training data is large-scale,
appropriate feature selection enables the removal of redundant features, which
can improve generalization performance, accelerate the training process, and
enhance the interpretability of the model. This study proposes a
privacy-preserving computation model for feature selection. Generally, when the
data owner and analyst are the same, there is no need to conceal the private
information. However, when they are different parties or when multiple owners
exist, an appropriate privacy-preserving framework is required. Although
various private feature selection algorithms, they all require two or more
computing parties and do not guarantee security in environments where no
external party can be fully trusted. To address this issue, we propose the
first outsourcing algorithm for feature selection using fully homomorphic
encryption. Compared to a prior two-party algorithm, our result improves the
time and space complexity O(kn^2) to O(kn log^3 n) and O(kn), where k and n
denote the number of features and data samples, respectively. We also
implemented the proposed algorithm and conducted comparative experiments with
the naive one. The experimental result shows the efficiency of our method even
with small datasets.

</details>

### [46] [Lara: Lightweight Anonymous Authentication with Asynchronous Revocation Auditability](https://arxiv.org/abs/2505.12968)
*Claudio Correia,Guilherme Santos,Luis Rodrigues*

Main category: cs.CR

TLDR: Lara是一种轻量级匿名认证方案，支持异步撤销审计，无需依赖时间同步，解决了传统方法中因时间假设导致的隐私问题。


<details>
  <summary>Details</summary>
Motivation: 传统匿名认证方案依赖时间同步，容易因时钟偏差导致隐私泄露，Lara旨在消除这种依赖，提升隐私保护。

Method: Lara通过提供撤销列表（RL）和证明非撤销机制，避免时间假设，使用高效公钥原语和空间优化数据结构。

Result: 实验证明Lara在效率和空间占用上表现优异，适用于实际部署。

Conclusion: Lara提供了一种无需时间同步的匿名认证方案，有效解决了撤销审计中的隐私问题。

Abstract: Anonymous authentication is a technique that allows to combine access control
with privacy preservation. Typically, clients use different pseudonyms for each
access, hindering providers from correlating their activities. To perform the
revocation of pseudonyms in a privacy preserving manner is notoriously
challenging. When multiple pseudonyms are revoked together, an adversary may
infer that these pseudonyms belong to the same client and perform privacy
breaking correlations, in particular if these pseudonyms have already been
used. Backward unlinkability and revocation auditability are two properties
that address this problem. Most systems that offer these properties rely on
some sort of time slots, which assume a common reference of time that must be
shared among clients and providers; for instance, the client must be aware that
it should not use a pseudonym after a certain time or should be able to assess
the freshness of a revocation list prior to perform authentication. In this
paper we propose Lara, a Lightweight Anonymous Authentication with Asynchronous
Revocation Auditability that does not require parties to agree on the current
time slot and it is not affected by the clock skew. Prior to disclosing a
pseudonym, clients are provided with a revocation list (RL) and can check that
the pseudonym has not been revoked. Then, they provide a proof on
non-revocation that cannot be used against any other (past or future) RL,
avoiding any dependency of timing assumptions. Lara can be implemented using
efficient public-key primitives and space-efficient data structures. We have
implemented a prototype of Lara and have assessed experimentally its
efficiency.

</details>

### [47] [From Assistants to Adversaries: Exploring the Security Risks of Mobile LLM Agents](https://arxiv.org/abs/2505.12981)
*Liangxuan Wu,Chao Wang,Tianming Liu,Yanjie Zhao,Haoyu Wang*

Main category: cs.CR

TLDR: 本文首次对移动LLM代理进行了全面的安全分析，揭示了11种攻击面，并通过AgentScan框架验证了9种代理的漏洞，提出了防御设计原则。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在移动计算中的广泛应用，移动AI代理的安全问题尚未被充分研究，本文旨在填补这一空白。

Method: 通过分析移动代理的工作流程，识别三大核心能力维度的安全威胁，并开发AgentScan框架对9种代理进行漏洞测试。

Result: 所有测试代理均存在漏洞，最严重的涉及8种攻击向量，可能导致行为偏差、隐私泄露或执行劫持。

Conclusion: 研究强调了标准化安全实践的紧迫性，并提出了防御设计原则，得到了设备厂商的积极反馈。

Abstract: The growing adoption of large language models (LLMs) has led to a new
paradigm in mobile computing--LLM-powered mobile AI agents--capable of
decomposing and automating complex tasks directly on smartphones. However, the
security implications of these agents remain largely unexplored. In this paper,
we present the first comprehensive security analysis of mobile LLM agents,
encompassing three representative categories: System-level AI Agents developed
by original equipment manufacturers (e.g., YOYO Assistant), Third-party
Universal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g.,
Alibaba Mobile Agent). We begin by analyzing the general workflow of mobile
agents and identifying security threats across three core capability
dimensions: language-based reasoning, GUI-based interaction, and system-level
execution. Our analysis reveals 11 distinct attack surfaces, all rooted in the
unique capabilities and interaction patterns of mobile LLM agents, and spanning
their entire operational lifecycle. To investigate these threats in practice,
we introduce AgentScan, a semi-automated security analysis framework that
systematically evaluates mobile LLM agents across all 11 attack scenarios.
Applying AgentScan to nine widely deployed agents, we uncover a concerning
trend: every agent is vulnerable to targeted attacks. In the most severe cases,
agents exhibit vulnerabilities across eight distinct attack vectors. These
attacks can cause behavioral deviations, privacy leakage, or even full
execution hijacking. Based on these findings, we propose a set of defensive
design principles and practical recommendations for building secure mobile LLM
agents. Our disclosures have received positive feedback from two major device
vendors. Overall, this work highlights the urgent need for standardized
security practices in the fast-evolving landscape of LLM-driven mobile
automation.

</details>

### [48] [ACE: Confidential Computing for Embedded RISC-V Systems](https://arxiv.org/abs/2505.12995)
*Wojciech Ozga,Guerney D. H. Hunt,Michael V. Le,Lennard Gäher,Avraham Shinnar,Elaine R. Palmer,Hani Jamjoom,Silvio Dragone*

Main category: cs.CR

TLDR: ACE是一种针对嵌入式RISC-V系统的开源保密计算技术，旨在提升关键嵌入式系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 利用保密计算技术隔离敏感应用，构建更安全的关键嵌入式系统。

Method: 提出一套原则和方法论，开发开源且免版税的ACE技术，并支持形式化验证。

Result: 在支持虚拟化的RISC-V硬件上评估原型，证明ACE适用于目标系统。

Conclusion: ACE是构建安全嵌入式系统的可行方案。

Abstract: Confidential computing plays an important role in isolating sensitive
applications from the vast amount of untrusted code commonly found in the
modern cloud. We argue that it can also be leveraged to build safer and more
secure mission-critical embedded systems. In this paper, we introduce the
Assured Confidential Execution (ACE), an open-source and royalty-free
confidential computing technology targeted for embedded RISC-V systems. We
present a set of principles and a methodology that we used to build \ACE and
that might be applied for developing other embedded systems that require formal
verification. An evaluation of our prototype on the first available RISC-V
hardware supporting virtualization indicates that ACE is a viable candidate for
our target systems.

</details>

### [49] [Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset](https://arxiv.org/abs/2505.13028)
*Sayon Palit,Daniel Woods*

Main category: cs.CR

TLDR: 该研究对大型语言模型（LLM）安全工具进行了比较分析，发现现有工具在有效性和可用性方面存在不足，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: LLM在关键系统中的广泛应用带来了安全风险，但缺乏对安全工具效果的正式评估。

Method: 通过构建恶意提示的基准数据集，比较了7种安全工具与基线模型（ChatGPT-3.5-Turbo）的性能。

Result: 基线模型误报率高，Lakera Guard和ProtectAI LLM Guard表现最佳。

Conclusion: 建议提高闭源工具的透明度、改进检测能力、增强开源参与度、提升用户意识并采用更具代表性的性能指标。

Abstract: Large Language Models (LLMs) are increasingly integrated into critical
systems in industries like healthcare and finance. Users can often submit
queries to LLM-enabled chatbots, some of which can enrich responses with
information retrieved from internal databases storing sensitive data. This
gives rise to a range of attacks in which a user submits a malicious query and
the LLM-system outputs a response that creates harm to the owner, such as
leaking internal data or creating legal liability by harming a third-party.
While security tools are being developed to counter these threats, there is
little formal evaluation of their effectiveness and usability. This study
addresses this gap by conducting a thorough comparative analysis of LLM
security tools. We identified 13 solutions (9 closed-source, 4 open-source),
but only 7 were evaluated due to a lack of participation by proprietary model
owners.To evaluate, we built a benchmark dataset of malicious prompts, and
evaluate these tools performance against a baseline LLM model
(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many
false positives to be used for this task. Lakera Guard and ProtectAI LLM Guard
emerged as the best overall tools showcasing the tradeoff between usability and
performance. The study concluded with recommendations for greater transparency
among closed source providers, improved context-aware detections, enhanced
open-source engagement, increased user awareness, and the adoption of more
representative performance metrics.

</details>

### [50] [The Hidden Dangers of Browsing AI Agents](https://arxiv.org/abs/2505.13076)
*Mykyta Mudryi,Markiyan Chaklosh,Grzegorz Wójcik*

Main category: cs.CR

TLDR: 本文对基于大型语言模型（LLM）的自主浏览代理进行了全面的安全评估，揭示了其多层次的系统漏洞，并提出了端到端的威胁模型和深度防御策略。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的自主浏览代理在自动化网络任务中的广泛应用，其依赖动态内容、工具执行和用户数据的特点使其面临广泛的安全威胁。

Method: 通过白盒分析开源项目Browser Use，识别了多种攻击向量（如提示注入、域验证绕过和凭证泄露），并提出了包括输入净化、执行隔离和形式化分析在内的防御措施。

Result: 研究发现，不受信任的网页内容可以劫持代理行为，导致严重的安全漏洞，并通过披露的CVE和概念验证漏洞证明了这些威胁。

Conclusion: 本文提出了深度防御策略，为实际部署中的浏览代理提供了可操作的安全指导。

Abstract: Autonomous browsing agents powered by large language models (LLMs) are
increasingly used to automate web-based tasks. However, their reliance on
dynamic content, tool execution, and user-provided data exposes them to a broad
attack surface. This paper presents a comprehensive security evaluation of such
agents, focusing on systemic vulnerabilities across multiple architectural
layers. Our work outlines the first end-to-end threat model for browsing agents
and provides actionable guidance for securing their deployment in real-world
environments. To address discovered threats, we propose a defense in depth
strategy incorporating input sanitization, planner executor isolation, formal
analyzers, and session safeguards. These measures protect against both initial
access and post exploitation attack vectors. Through a white box analysis of a
popular open source project, Browser Use, we demonstrate how untrusted web
content can hijack agent behavior and lead to critical security breaches. Our
findings include prompt injection, domain validation bypass, and credential
exfiltration, evidenced by a disclosed CVE and a working proof of concept
exploit.

</details>

### [51] [Network-wide Quantum Key Distribution with Onion Routing Relay (Conference Version)](https://arxiv.org/abs/2505.13158)
*Pedro Otero-García,David Pérez-Castro,Manuel Fernández-Veiga,Ana Fernández-Vilas*

Main category: cs.CR

TLDR: 论文提出了一种名为Onion Routing Relay (ORR)的新型量子密钥分发协议，结合洋葱路由和后量子密码学，以增强量子安全通信的保密性、完整性和匿名性。


<details>
  <summary>Details</summary>
Motivation: 量子计算的进步威胁到经典加密方法，需要开发安全的量子密钥分发（QKD）解决方案。

Method: ORR协议通过后量子密码学封装，避免中间恶意节点的安全风险，确保端到端安全性。

Result: 基本ORR模型在性能上与现有方法竞争，提供显著安全性提升，但扩展认证（ORR-Ext）对服务质量（QoS）影响较大。

Conclusion: ORR在安全性上表现优异，适用于高安全需求环境，但需权衡扩展功能对QoS的影响。

Abstract: The advancement of quantum computing threatens classical cryptographic
methods, necessitating the development of secure quantum key distribution (QKD)
solutions for QKD Networks (QKDN). In this paper, a novel key distribution
protocol, Onion Routing Relay (ORR), that integrates onion routing (OR) with
post-quantum cryptography (PQC) in a key-relay (KR) model is evaluated for
QKDNs. This approach increases the security by enhancing confidentiality,
integrity, authenticity (CIA principles), and anonymity in quantum-secure
communications. By employing PQC-based encapsulation, ORR aims to avoid the
security risks posed by intermediate malicious nodes and ensures end-to-end
security. Our results show a competitive performance of the basic ORR model,
against current KR and trusted-node (TN) approaches, demonstrating its
feasibility and applicability in high-security environments maintaining a
consistent Quality of Service (QoS). The results also show that while basic ORR
incurs higher encryption overhead, it provides substantial security
improvements without significantly impacting the overall key distribution time.
Nevertheless, the introduction of an end-to-end authentication extension
(ORR-Ext) has a significant impact on the Quality of Service (QoS), thereby
limiting its suitability to applications with stringent security requirements.

</details>

### [52] [A Geometry-Grounded Data Perimeter in Azure](https://arxiv.org/abs/2505.13238)
*Christophe Parisel*

Main category: cs.CR

TLDR: 本文提出了一种基于Azure爆炸半径超度量的数据边界定义方法，通过解决旅行商问题实现边界点的有序排列，生成可操作的几何轮廓。


<details>
  <summary>Details</summary>
Motivation: 当前网络安全领域对数据边界的定义缺乏明确的边界点排列方法，本文旨在填补这一空白。

Method: 利用Azure的爆炸半径超度量提供距离度量，并通过解决旅行商问题对边界点进行排序。

Result: 生成了一种可操作的几何轮廓，用于SPN优先级排序。

Conclusion: 该方法为数据边界提供了明确的几何定义和可操作性，适用于网络安全中的优先级排序。

Abstract: While data perimeter is ubiquitous in cybersecurity speak, it rarely defines
how boundary points are arranged. In this paper we show how Azure s blast
radius ultrametric provides the distance, and how solving the Traveling
Salesman Problem in this ultrametric space provides the ordering, yielding a
true geometric contour: an actionable perimeter measure for SPN prioritization.

</details>

### [53] [Network-wide Quantum Key Distribution with Onion Routing Relay](https://arxiv.org/abs/2505.13239)
*Pedro Otero-García,David Pérez-Castro,Manuel Fernández-Veiga,Ana Fernández-Vilas*

Main category: cs.CR

TLDR: 论文提出了一种名为ORR的新型密钥分发协议，结合洋葱路由和后量子密码学，用于量子密钥分发网络，显著提升了安全性。


<details>
  <summary>Details</summary>
Motivation: 量子计算的进步威胁传统加密方法，需要开发安全的量子密钥分发解决方案。

Method: ORR协议结合洋葱路由和后量子密码学，采用密钥中继模型，通过PQC封装增强安全性。

Result: ORR在安全性上优于现有密钥中继和可信节点方法，尽管加密开销较高，但对整体密钥分发时间影响不大。

Conclusion: ORR在高安全环境中具有可行性和适用性，同时保持服务质量。

Abstract: The advancement of quantum computing threatens classical cryptographic
methods, necessitating the development of secure quantum key distribution (QKD)
solutions for QKD Networks (QKDN). In this paper, a novel key distribution
protocol, Onion Routing Relay (ORR), that integrates onion routing (OR) with
post-quantum cryptography (PQC) in a key-relay (KR) model is evaluated for
QKDNs. This approach increases the security by enhancing confidentiality,
integrity, authenticity, and anonymity in quantum-secure communications. By
employing PQC-based encapsulation, ORR pretends to avoid the security risks
posed by intermediate malicious nodes and ensures end-to-end security. Results
show that the performance of the ORR model, against current key-relay (KR) and
trusted-node (TN) approaches, demonstrating its feasibility and applicability
in high-security environments maintaining a consistent Quality of Service
(QoS). The results show that while ORR incurs higher encryption overhead, it
provides substantial security improvements without significantly impacting the
overall key distribution time.

</details>

### [54] [Cross-Cloud Data Privacy Protection: Optimizing Collaborative Mechanisms of AI Systems by Integrating Federated Learning and LLMs](https://arxiv.org/abs/2505.13292)
*Huaiying Luo,Cheng Ji*

Main category: cs.CR

TLDR: 结合联邦学习与大语言模型优化跨云环境协作，提升数据隐私保护与模型效率。


<details>
  <summary>Details</summary>
Motivation: 解决云环境中敏感数据共享与协作优化的挑战。

Method: 在联邦学习框架中引入跨云架构，结合大语言模型提升训练效率，并加入安全通信层保护数据隐私。

Result: 实验显示，该方法在准确性、收敛速度和数据隐私保护上优于传统联邦学习模型。

Conclusion: 该方法有效优化了跨云环境协作，同时保护了数据隐私。

Abstract: In the age of cloud computing, data privacy protection has become a major
challenge, especially when sharing sensitive data across cloud environments.
However, how to optimize collaboration across cloud environments remains an
unresolved problem. In this paper, we combine federated learning with
large-scale language models to optimize the collaborative mechanism of AI
systems. Based on the existing federated learning framework, we introduce a
cross-cloud architecture in which federated learning works by aggregating model
updates from decentralized nodes without exposing the original data. At the
same time, combined with large-scale language models, its powerful context and
semantic understanding capabilities are used to improve model training
efficiency and decision-making ability. We've further innovated by introducing
a secure communication layer to ensure the privacy and integrity of model
updates and training data. The model enables continuous model adaptation and
fine-tuning across different cloud environments while protecting sensitive
data. Experimental results show that the proposed method is significantly
better than the traditional federated learning model in terms of accuracy,
convergence speed and data privacy protection.

</details>

### [55] [SVAFD: A Secure and Verifiable Co-Aggregation Protocol for Federated Distillation](https://arxiv.org/abs/2505.13319)
*Tian Wen,Sheng Sun,Yuwei Wang,Peiyan Chen,Zhiyuan Wu,Min Liu,Bo Gao*

Main category: cs.CR

TLDR: SVAFD是一种专为联邦蒸馏（FD）设计的首个安全聚合协议，解决了现有协议在异构模型环境中的不足，通过多边协同聚合和质量感知知识过滤方法提升安全性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有安全聚合（SA）设计在联邦学习（FL）中依赖同构模型假设，且中央服务器存在信任问题，而联邦蒸馏（FD）的异构模型特性需要新的SA解决方案。

Method: SVAFD采用多边协同聚合方法（客户端本地评估和聚合，服务器解码和验证）和质量感知知识过滤（排除偏置logits以抵御投毒攻击）。

Result: SVAFD在四种FD架构中实现原型，显著提升模型准确性，并抵御投毒和推理攻击。

Conclusion: SVAFD为异构FL系统提供了安全可验证的聚合方案，是联邦蒸馏领域的重要进展。

Abstract: Secure Aggregation (SA) is an indispensable component of Federated Learning
(FL) that concentrates on privacy preservation while allowing for robust
aggregation. However, most SA designs rely heavily on the unrealistic
assumption of homogeneous model architectures. Federated Distillation (FD),
which aggregates locally computed logits instead of model parameters,
introduces a promising alternative for cooperative training in heterogeneous
model settings. Nevertheless, we recognize two major challenges in implementing
SA for FD. (i) Prior SA designs encourage a dominant server, who is solely
responsible for collecting, aggregating and distributing. Such central
authority facilitates server to forge aggregation proofs or collude to bypass
the claimed security guarantees; (ii) Existing SA, tailored for FL models,
overlook the intrinsic properties of logits, making them unsuitable for FD.
  To address these challenges, we propose SVAFD, the first SA protocol that is
specifically designed for FD. At a high level, SVAFD incorporates two
innovations: (i) a multilateral co-aggregation method tha redefines the
responsibilities of clients and server. Clients autonomously evaluate and
aggregate logits shares locally with a lightweight coding scheme, while the
server handles ciphertext decoding and performs the task of generating
verification proofs; (ii) a quality-aware knowledge filtration method that
facilitates biased logits exclusion against poisoning attacks. Moreover, SVAFD
is resilient to stragglers and colluding clients, making it well-suited for
dynamic networks in real-world applications. We have implemented the SVAFD
prototype over four emerging FD architectures and evaluated it against
poisoning and inference attacks. Results demonstrate that SVAFD improves model
accuracy, making it a significant step forward in secure and verifiable
aggregation for heterogeneous FL systems.

</details>

### [56] [DynaNoise: Dynamic Probabilistic Noise Injection for Defending Against Membership Inference Attacks](https://arxiv.org/abs/2505.13362)
*Javad Forough,Hamed Haddadi*

Main category: cs.CR

TLDR: DynaNoise是一种动态噪声注入方法，通过查询敏感度自适应调整噪声，有效防御成员推理攻击（MIAs），同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 成员推理攻击（MIAs）威胁训练数据隐私，传统静态差分隐私方法在噪声与性能间难以平衡。

Method: DynaNoise动态调整噪声注入，基于查询敏感度分析（如香农熵），并通过概率平滑归一化输出。

Result: DynaNoise显著降低MIA成功率，MIDPUT指标提升四倍，模型精度保持竞争力。

Conclusion: DynaNoise是高效隐私防御方法，平衡隐私保护与模型性能。

Abstract: Membership Inference Attacks (MIAs) pose a significant risk to the privacy of
training datasets by exploiting subtle differences in model outputs to
determine whether a particular data sample was used during training. These
attacks can compromise sensitive information, especially in domains such as
healthcare and finance, where data privacy is paramount. Traditional mitigation
techniques, such as static differential privacy, rely on injecting a fixed
amount of noise during training or inference. However, this approach often
leads to a detrimental trade-off: the noise may be insufficient to counter
sophisticated attacks or, when increased, may substantially degrade model
performance. In this paper, we present DynaNoise, an adaptive approach that
dynamically modulates noise injection based on query sensitivity. Our approach
performs sensitivity analysis using measures such as Shannon entropy to
evaluate the risk associated with each query and adjusts the noise variance
accordingly. A probabilistic smoothing step is then applied to renormalize the
perturbed outputs, ensuring that the model maintains high accuracy while
effectively obfuscating membership signals. We further propose an empirical
metric, the Membership Inference Defense Privacy-Utility Tradeoff (MIDPUT),
which quantifies the balance between reducing attack success rates and
preserving the target model's accuracy. Our extensive evaluation on several
benchmark datasets demonstrates that DynaNoise not only significantly reduces
MIA success rates but also achieves up to a fourfold improvement in the MIDPUT
metric compared to the state-of-the-art. Moreover, DynaNoise maintains
competitive model accuracy while imposing only marginal inference overhead,
highlighting its potential as an effective and efficient privacy defense
against MIAs.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [57] [PRIME: Physics-Related Intelligent Mixture of Experts for Transistor Characteristics Prediction](https://arxiv.org/abs/2505.11523)
*Zhenxing Dou,Yijiao Wang,Tao Zou,Zhiwei Chen,Fei Liu,Peng Wang,Weisheng Zhao*

Main category: cs.LG

TLDR: PRIME框架结合物理知识与数据驱动智能，通过动态加权机制提升晶体管特性预测精度，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在多操作区域非线性电流响应预测中的挑战。

Method: 提出PRIME框架，结合物理知识与数据驱动智能，采用动态加权机制激活合适的专家模型。

Result: 在GAA结构上验证，预测精度提升60%-84%。

Conclusion: PRIME有效整合物理与数据驱动方法，显著提升预测性能。

Abstract: In recent years, machine learning has been extensively applied to data
prediction during process ramp-up, with a particular focus on transistor
characteristics for circuit design and manufacture. However, capturing the
nonlinear current response across multiple operating regions remains a
challenge for neural networks. To address such challenge, a novel machine
learning framework, PRIME (Physics-Related Intelligent Mixture of Experts), is
proposed to capture and integrate complex regional characteristics. In essence,
our framework incorporates physics-based knowledge with data-driven
intelligence. By leveraging a dynamic weighting mechanism in its gating
network, PRIME adaptively activates the suitable expert model based on distinct
input data features. Extensive evaluations are conducted on various
gate-all-around (GAA) structures to examine the effectiveness of PRIME and
considerable improvements (60\%-84\%) in prediction accuracy are shown over
state-of-the-art models.

</details>

### [58] [Policy Gradient with Second Order Momentum](https://arxiv.org/abs/2505.11561)
*Tianyu Sun*

Main category: cs.LG

TLDR: PG-SOM是一种轻量级的二阶优化方法，通过结合梯度平均和对角Hessian近似，提升了强化学习策略的效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统的一阶优化方法在强化学习中可能效率较低且不稳定，PG-SOM旨在通过引入二阶信息来改善这些问题。

Method: PG-SOM在REINFORCE更新中加入了两个指数加权统计量：一阶梯度平均和对角Hessian近似，通过曲率估计对梯度进行预处理。

Result: 实验表明，PG-SOM在标准控制基准上的样本效率提升了2.1倍，方差显著降低。

Conclusion: PG-SOM证明了即使是粗略的二阶信息也能带来显著的性能提升，且仅需D内存开销。

Abstract: We develop Policy Gradient with Second-Order Momentum (PG-SOM), a lightweight
second-order optimisation scheme for reinforcement-learning policies. PG-SOM
augments the classical REINFORCE update with two exponentially weighted
statistics: a first-order gradient average and a diagonal approximation of the
Hessian. By preconditioning the gradient with this curvature estimate, the
method adaptively rescales each parameter, yielding faster and more stable
ascent of the expected return. We provide a concise derivation, establish that
the diagonal Hessian estimator is unbiased and positive-definite under mild
regularity assumptions, and prove that the resulting update is a descent
direction in expectation. Numerical experiments on standard control benchmarks
show up to a 2.1x increase in sample efficiency and a substantial reduction in
variance compared to first-order and Fisher-matrix baselines. These results
indicate that even coarse second-order information can deliver significant
practical gains while incurring only D memory overhead for a D-parameter
policy. All code and reproducibility scripts will be made publicly available.

</details>

### [59] [HessFormer: Hessians at Foundation Scale](https://arxiv.org/abs/2505.11564)
*Diego Granziol*

Main category: cs.LG

TLDR: 论文介绍了HessFormer，一个支持分布式Hessian向量计算的软件包，适用于大规模深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习的一阶优化方法取得了重大进展，但依赖Hessian向量积的方法仍受限于单GPU，无法处理数十亿参数模型。

Method: 开发了HessFormer软件包，结合分布式随机Lanczos正交算法，支持多GPU计算。

Result: 利用该工具研究了Deepseek 700亿参数模型的Hessian谱密度。

Conclusion: HessFormer为大规模模型的Hessian分析提供了实用工具。

Abstract: Whilst there have been major advancements in the field of first order
optimisation of deep learning models, where state of the art open source
mixture of expert models go into the hundreds of billions of parameters,
methods that rely on Hessian vector products, are still limited to run on a
single GPU and thus cannot even work for models in the billion parameter range.
We release a software package \textbf{HessFormer}, which integrates nicely with
the well known Transformers package and allows for distributed hessian vector
computation across a single node with multiple GPUs. Underpinning our
implementation is a distributed stochastic lanczos quadrature algorithm, which
we release for public consumption. Using this package we investigate the
Hessian spectral density of the recent Deepseek $70$bn parameter model.

</details>

### [60] [Beyond Time: Cross-Dimensional Frequency Supervision for Time Series Forecasting](https://arxiv.org/abs/2505.11567)
*Tianyi Shi,Zhu Meng,Yue Chen,Siyang Zheng,Fei Su,Jin Huang,Changrui Ren,Zhicheng Zhao*

Main category: cs.LG

TLDR: 提出了一种基于纯频域监督的方法X-Freq损失，通过频域分析提升时间序列预测的通用性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多针对特定数据集设计，缺乏通用性，且时间域标签的强相关性违背IID假设。

Method: 利用频域信息熵更高的特性，结合傅里叶变换和小波变换，在频域计算预测与目标的损失。

Result: 在14个真实数据集上，X-Freq平均提升长短期预测性能3.3%和27.7%。

Conclusion: X-Freq无需修改模型架构即可显著提升预测性能，具有广泛适用性。

Abstract: Time series forecasting plays a crucial role in various fields, and the
methods based on frequency domain analysis have become an important branch.
However, most existing studies focus on the design of elaborate model
architectures and are often tailored for limited datasets, still lacking
universality. Besides, the assumption of independent and identically
distributed (IID) data also contradicts the strong correlation of the time
domain labels. To address these issues, abandoning time domain supervision, we
propose a purely frequency domain supervision approach named cross-dimensional
frequency (X-Freq) loss. Specifically, based on a statistical phenomenon, we
first prove that the information entropy of the time series is higher than its
spectral entropy, which implies higher certainty in frequency domain and thus
can provide better supervision. Secondly, the Fourier Transform and the Wavelet
Transform are applied to the time dimension and the channel dimension of the
time series respectively, to capture the long-term and short-term frequency
variations as well as the spatial configuration features. Thirdly, the loss
between predictions and targets is uniformly computed in the frequency domain.
Moreover, we plug-and-play incorporate X-Freq into multiple advanced
forecasting models and compare on 14 real-world datasets. The experimental
results demonstrate that, without making any modification to the original
architectures or hyperparameters, X-Freq can improve the forecasting
performance by an average of 3.3% on long-term forecasting datasets and 27.7%
on short-term ones, showcasing superior generality and practicality. The code
will be released publicly.

</details>

### [61] [Towards Adaptive Deep Learning: Model Elasticity via Prune-and-Grow CNN Architectures](https://arxiv.org/abs/2505.11569)
*Pooja Mangal,Sudaksh Kalra,Dolly Sapra*

Main category: cs.LG

TLDR: 该论文提出了一种动态调整计算复杂度的自适应CNN架构，通过结构化剪枝和动态重构实现资源高效利用。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限设备上部署CNN时的高计算需求和静态架构问题。

Method: 采用结构化剪枝和动态重构方法，创建嵌套子网络，实现运行时容量调整。

Result: 实验表明自适应模型在多种CNN架构和数据集上保持或提升性能。

Conclusion: 嵌入自适应能力显著提升CNN的鲁棒性和灵活性，适用于多样化计算环境。

Abstract: Deploying deep convolutional neural networks (CNNs) on resource-constrained
devices presents significant challenges due to their high computational demands
and rigid, static architectures. To overcome these limitations, this thesis
explores methods for enabling CNNs to dynamically adjust their computational
complexity based on available hardware resources. We introduce adaptive CNN
architectures capable of scaling their capacity at runtime, thus efficiently
balancing performance and resource utilization. To achieve this adaptability,
we propose a structured pruning and dynamic re-construction approach that
creates nested subnetworks within a single CNN model. This approach allows the
network to dynamically switch between compact and full-sized configurations
without retraining, making it suitable for deployment across varying hardware
platforms. Experiments conducted across multiple CNN architectures including
VGG-16, AlexNet, ResNet-20, and ResNet-56 on CIFAR-10 and Imagenette datasets
demonstrate that adaptive models effectively maintain or even enhance
performance under varying computational constraints. Our results highlight that
embedding adaptability directly into CNN architectures significantly improves
their robustness and flexibility, paving the way for efficient real-world
deployment in diverse computational environments.

</details>

### [62] [Tool-Aided Evolutionary LLM for Generative Policy Toward Efficient Resource Management in Wireless Federated Learning](https://arxiv.org/abs/2505.11570)
*Chongyang Tan,Ruoqi Wen,Rongpeng Li,Zhifeng Zhao,Ekram Hossain,Honggang Zhang*

Main category: cs.LG

TLDR: 提出了一种基于工具辅助的进化大语言模型（T-ELLM）框架，用于无线联邦学习环境中的设备选择策略生成，通过自然语言提示和虚拟学习环境提升泛化能力和效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在动态异构无线环境中的效率依赖于设备选择和资源分配，传统方法需要大量专业知识和调参成本。

Method: T-ELLM利用自然语言提示解耦联合优化问题，结合虚拟学习环境和凸优化工具，实现高效策略学习。

Result: 实验表明T-ELLM在能源效率和环境适应性上优于基准方法，理论分析验证了虚拟与真实环境偏差的可控性。

Conclusion: T-ELLM为联邦学习提供了一种高效、适应性强的设备选择解决方案，减少了对真实交互的依赖。

Abstract: Federated Learning (FL) enables distributed model training across edge
devices in a privacy-friendly manner. However, its efficiency heavily depends
on effective device selection and high-dimensional resource allocation in
dynamic and heterogeneous wireless environments. Conventional methods demand a
confluence of domain-specific expertise, extensive hyperparameter tuning,
and/or heavy interaction cost. This paper proposes a Tool-aided Evolutionary
Large Language Model (T-ELLM) framework to generate a qualified policy for
device selection in a wireless FL environment. Unlike conventional optimization
methods, T-ELLM leverages natural language-based scenario prompts to enhance
generalization across varying network conditions. The framework decouples the
joint optimization problem mathematically, enabling tractable learning of
device selection policies while delegating resource allocation to convex
optimization tools. To improve adaptability, T-ELLM integrates a
sample-efficient, model-based virtual learning environment that captures the
relationship between device selection and learning performance, facilitating
subsequent group relative policy optimization. This concerted approach reduces
reliance on real-world interactions, minimizing communication overhead while
maintaining high-fidelity decision-making. Theoretical analysis proves that the
discrepancy between virtual and real environments is bounded, ensuring the
advantage function learned in the virtual environment maintains a provably
small deviation from real-world conditions. Experimental results demonstrate
that T-ELLM outperforms benchmark methods in energy efficiency and exhibits
robust adaptability to environmental changes.

</details>

### [63] [InfiJanice: Joint Analysis and In-situ Correction Engine for Quantization-Induced Math Degradation in Large Language Models](https://arxiv.org/abs/2505.11574)
*Zhen Li,Yupeng Su,Songmiao Wang,Runming Yang,Congkai Xie,Aofan Liu,Ming Li,Jiannong Cao,Yuan Xie,Ngai Wong,Hongxia Yang*

Main category: cs.LG

TLDR: 论文研究了主流量化方法对开源大语言模型数学推理能力的影响，发现量化可能导致准确性下降高达69.81%。通过分析错误类型并构建精选数据集，仅需少量数据和短时间训练即可恢复量化模型的推理准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理任务中表现优异，但其高计算需求限制了实际部署。量化是减少内存和延迟的有效方法，但量化对推理准确性的影响尚不明确。

Method: 研究主流量化方法（如AWQ、GPTQ、SmoothQuant）对开源模型（如Qwen2.5、LLaMA3系列）的影响，开发自动化错误分类和数据集构建流程。

Result: 量化导致数学推理准确性下降高达69.81%。通过精选332个样本训练3-5分钟，量化模型的准确性可恢复至全精度基线水平。

Conclusion: 量化虽影响推理准确性，但通过精选数据和短时训练可有效恢复性能，为实际部署提供可行方案。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance on
complex reasoning benchmarks such as GSM8K, MATH, and AIME. However, the
substantial computational demands of these tasks pose significant challenges
for real-world deployment. Model quantization has emerged as a promising
approach to reduce memory footprint and inference latency by representing
weights and activations with lower bit-widths. In this work, we conduct a
comprehensive study of mainstream quantization methods(e.g., AWQ, GPTQ,
SmoothQuant) on the most popular open-sourced models (e.g., Qwen2.5, LLaMA3
series), and reveal that quantization can degrade mathematical reasoning
accuracy by up to 69.81%. To better understand this degradation, we develop an
automated assignment and judgment pipeline that qualitatively categorizes
failures into four error types and quantitatively identifies the most impacted
reasoning capabilities. Building on these findings, we employ an automated
data-curation pipeline to construct a compact "Silver Bullet" datasets.
Training a quantized model on as few as 332 carefully selected examples for
just 3-5 minutes on a single GPU is enough to restore its reasoning accuracy to
match that of the full-precision baseline.

</details>

### [64] [Concept-Guided Interpretability via Neural Chunking](https://arxiv.org/abs/2505.11576)
*Shuchen Wu,Stephan Alaniz,Shyamgopal Karthik,Peter Dayan,Eric Schulz,Zeynep Akata*

Main category: cs.LG

TLDR: 论文提出“反射假说”，认为神经网络的活动模式反映了训练数据的规律，并通过分块方法提取可解释的单元。三种方法（DSC、PA、UCD）在不同场景下有效提取实体，展示了神经网络的可解释性。


<details>
  <summary>Details</summary>
Motivation: 挑战神经网络是“黑箱”的普遍观点，揭示其活动模式与训练数据的关联，为可解释性提供新方向。

Method: 提出三种分块方法：离散序列分块（DSC）、群体平均（PA）和无监督分块发现（UCD），用于提取神经网络中的可解释实体。

Result: 实验证明这些方法能有效提取实体，且实体与具体或抽象概念对应。人为诱导这些实体可改变网络生成相关概念的行为。

Conclusion: 结合认知原理和自然数据结构的可解释性方法，为理解复杂学习系统提供了新途径。

Abstract: Neural networks are often black boxes, reflecting the significant challenge
of understanding their internal workings. We propose a different perspective
that challenges the prevailing view: rather than being inscrutable, neural
networks exhibit patterns in their raw population activity that mirror
regularities in the training data. We refer to this as the Reflection
Hypothesis and provide evidence for this phenomenon in both simple recurrent
neural networks (RNNs) and complex large language models (LLMs). Building on
this insight, we propose to leverage cognitively-inspired methods of chunking
to segment high-dimensional neural population dynamics into interpretable units
that reflect underlying concepts. We propose three methods to extract these
emerging entities, complementing each other based on label availability and
dimensionality. Discrete sequence chunking (DSC) creates a dictionary of
entities; population averaging (PA) extracts recurring entities that correspond
to known labels; and unsupervised chunk discovery (UCD) can be used when labels
are absent. We demonstrate the effectiveness of these methods in extracting
entities across varying model sizes, ranging from inducing compositionality in
RNNs to uncovering recurring neural population states in large models with
diverse architectures, and illustrate their advantage over other methods.
Throughout, we observe a robust correspondence between the extracted entities
and concrete or abstract concepts. Artificially inducing the extracted entities
in neural populations effectively alters the network's generation of associated
concepts. Our work points to a new direction for interpretability, one that
harnesses both cognitive principles and the structure of naturalistic data to
reveal the hidden computations of complex learning systems, gradually
transforming them from black boxes into systems we can begin to understand.

</details>

### [65] [Spatiotemporal Field Generation Based on Hybrid Mamba-Transformer with Physics-informed Fine-tuning](https://arxiv.org/abs/2505.11578)
*Peimian Du,Jiabin Liu,Xiaowei Jin,Mengwang Zuo,Hui Li*

Main category: cs.LG

TLDR: 提出了一种基于混合Mamba-Transformer架构的时空物理场生成模型HMT-PF，通过物理信息增强的微调块减少物理方程差异，并开发了MSE-R评估方法。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动模型在生成时空物理场时遇到的显著物理方程差异问题。

Method: 采用混合Mamba-Transformer架构，结合非结构化网格信息输入，引入物理信息增强的微调块，并通过点查询机制计算物理方程残差进行梯度评估。

Result: 模型在生成时空场方面表现良好，物理信息微调机制有效减少了物理误差。

Conclusion: HMT-PF模型通过物理一致性微调和自监督学习，显著提升了时空物理场生成的准确性和真实性。

Abstract: This research confronts the challenge of substantial physical equation
discrepancies encountered in the generation of spatiotemporal physical fields
through data-driven trained models. A spatiotemporal physical field generation
model, named HMT-PF, is developed based on the hybrid Mamba-Transformer
architecture, incorporating unstructured grid information as input. A
fine-tuning block, enhanced with physical information, is introduced to
effectively reduce the physical equation discrepancies. The physical equation
residuals are computed through a point query mechanism for efficient gradient
evaluation, then encoded into latent space for refinement. The fine-tuning
process employs a self-supervised learning approach to achieve physical
consistency while maintaining essential field characteristics. Results show
that the hybrid Mamba-Transformer model achieves good performance in generating
spatiotemporal fields, while the physics-informed fine-tuning mechanism further
reduces significant physical errors effectively. A MSE-R evaluation method is
developed to assess the accuracy and realism of physical field generation.

</details>

### [66] [Flash Invariant Point Attention](https://arxiv.org/abs/2505.11580)
*Andrew Liu,Axel Elaldi,Nicholas T Franklin,Nathan Russell,Gurinder S Atwal,Yih-En A Ban,Olivia Viessmann*

Main category: cs.LG

TLDR: FlashIPA是一种基于硬件优化的FlashAttention的IPA改进算法，显著降低了计算成本，同时保持或超越标准IPA性能。


<details>
  <summary>Details</summary>
Motivation: IPA的二次复杂度限制了输入序列长度，需要一种更高效的算法。

Method: 通过因子化重构IPA，利用FlashAttention实现线性扩展。

Result: FlashIPA在GPU内存和计算时间上实现线性扩展，支持更长的序列训练。

Conclusion: FlashIPA扩展了训练范围，支持生成数千残基的结构。

Abstract: Invariant Point Attention (IPA) is a key algorithm for geometry-aware
modeling in structural biology, central to many protein and RNA models.
However, its quadratic complexity limits the input sequence length. We
introduce FlashIPA, a factorized reformulation of IPA that leverages
hardware-efficient FlashAttention to achieve linear scaling in GPU memory and
wall-clock time with sequence length. FlashIPA matches or exceeds standard IPA
performance while substantially reducing computational costs. FlashIPA extends
training to previously unattainable lengths, and we demonstrate this by
re-training generative models without length restrictions and generating
structures of thousands of residues. FlashIPA is available at
https://github.com/flagshippioneering/flash_ipa.

</details>

### [67] [A Training Framework for Optimal and Stable Training of Polynomial Neural Networks](https://arxiv.org/abs/2505.11589)
*Forsad Al Hossain,Tauhidur Rahman*

Main category: cs.LG

TLDR: 论文提出了一种训练多项式神经网络（PNNs）的稳健框架，解决了低阶多项式表达能力不足和高阶多项式数值不稳定的问题，通过边界损失和选择性梯度裁剪实现了稳定训练。


<details>
  <summary>Details</summary>
Motivation: 多项式神经网络在隐私保护推理（如同态加密）中具有重要应用，但训练时面临低阶多项式表达能力有限和高阶多项式数值不稳定的挑战。

Method: 提出两种创新方法：1) 边界损失，对超出稳定范围的输入进行指数惩罚；2) 选择性梯度裁剪，有效控制梯度幅度并保留批归一化统计量。

Result: 在多种数据集上，使用低阶（如2阶）和高阶（如22阶）多项式激活的PNNs均实现了高精度和稳定训练，性能接近ReLU网络。

Conclusion: 该框架为多项式神经网络的训练提供了实用解决方案，推动了隐私保护深度学习推理的实际应用。

Abstract: By replacing standard non-linearities with polynomial activations, Polynomial
Neural Networks (PNNs) are pivotal for applications such as privacy-preserving
inference via Homomorphic Encryption (HE). However, training PNNs effectively
presents a significant challenge: low-degree polynomials can limit model
expressivity, while higher-degree polynomials, crucial for capturing complex
functions, often suffer from numerical instability and gradient explosion. We
introduce a robust and versatile training framework featuring two synergistic
innovations: 1) a novel Boundary Loss that exponentially penalizes activation
inputs outside a predefined stable range, and 2) Selective Gradient Clipping
that effectively tames gradient magnitudes while preserving essential Batch
Normalization statistics. We demonstrate our framework's broad efficacy by
training PNNs within deep architectures composed of HE-compatible layers (e.g.,
linear layers, average pooling, batch normalization, as used in ResNet
variants) across diverse image, audio, and human activity recognition datasets.
These models consistently achieve high accuracy with low-degree polynomial
activations (such as degree 2) and, critically, exhibit stable training and
strong performance with polynomial degrees up to 22, where standard methods
typically fail or suffer severe degradation. Furthermore, the performance of
these PNNs achieves a remarkable parity, closely approaching that of their
original ReLU-based counterparts. Extensive ablation studies validate the
contributions of our techniques and guide hyperparameter selection. We confirm
the HE-compatibility of the trained models, advancing the practical deployment
of accurate, stable, and secure deep learning inference.

</details>

### [68] [SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training](https://arxiv.org/abs/2505.11594)
*Jintao Zhang,Jia Wei,Pengle Zhang,Xiaoming Xu,Haofeng Huang,Haoxu Wang,Kai Jiang,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TLDR: 通过利用FP4 Tensor Cores和设计8-bit注意力机制，显著提升了注意力的计算效率，并在推理和训练任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 注意力机制的二次时间复杂度限制了其效率，尤其是在大规模模型训练中。

Method: 1. 利用Blackwell GPU的FP4 Tensor Cores加速注意力计算；2. 设计8-bit注意力机制，支持训练任务的前向和反向传播。

Result: FP4注意力在RTX5090上实现了5倍加速；8-bit注意力在微调任务中表现无损，但在预训练任务中收敛较慢。

Conclusion: 低比特注意力在推理和部分训练任务中具有潜力，但预训练任务仍需优化。

Abstract: The efficiency of attention is important due to its quadratic time
complexity. We enhance the efficiency of attention through two key
contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to
accelerate attention computation. Our implementation achieves 1038 TOPS on
RTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.
Experiments show that our FP4 attention can accelerate inference of various
models in a plug-and-play way. Second, we pioneer low-bit attention to training
tasks. Existing low-bit attention works like FlashAttention3 and SageAttention
focus only on inference. However, the efficiency of training large models is
also important. To explore whether low-bit attention can be effectively applied
to training tasks, we design an accurate and efficient 8-bit attention for both
forward and backward propagation. Experiments indicate that 8-bit attention
achieves lossless performance in fine-tuning tasks but exhibits slower
convergence in pretraining tasks. The code will be available at
https://github.com/thu-ml/SageAttention.

</details>

### [69] [Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO](https://arxiv.org/abs/2505.11595)
*Peter Chen,Xiaopeng Li,Ziniu Li,Xi Chen,Tianyi Lin*

Main category: cs.LG

TLDR: 论文提出了一种改进GRPO的方法，通过引入AI反馈增加响应多样性，解决了全负样本组导致的学习停滞问题，并在理论和实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: GRPO在全负样本组中无法更新策略，导致学习停滞，因此需要一种方法来解决这一问题。

Method: 提出了一种框架，利用AI反馈在全负样本组中引入响应多样性，并通过理论分析和实验验证其效果。

Result: 实验表明，该方法在不同模型规模（7B、14B、32B）和多种基准测试中均能提升性能。

Conclusion: 学习全负样本组不仅可行且有益，为RL在LLMs中的应用提供了新见解。

Abstract: Reinforcement learning (RL) has demonstrated significant success in enhancing
reasoning capabilities in large language models (LLMs). One of the most widely
used RL methods is Group Relative Policy Optimization
(GRPO)~\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and
success in training DeepSeek-R1~\cite{Guo-2025-Deepseek}. However, GRPO stalls
when all sampled responses in a group are incorrect -- referred to as an
\emph{all-negative-sample} group -- as it fails to update the policy, hindering
learning progress. The contributions of this paper are two-fold. First, we
propose a simple yet effective framework that introduces response diversity
within all-negative-sample groups in GRPO using AI feedback. We also provide a
theoretical analysis, via a stylized model, showing how this diversification
improves learning dynamics. Second, we empirically validate our approach,
showing the improved performance across various model sizes (7B, 14B, 32B) in
both offline and online learning settings with 10 benchmarks, including base
and distilled variants. Our findings highlight that learning from
all-negative-sample groups is not only feasible but beneficial, advancing
recent insights from \citet{Xiong-2025-Minimalist}.

</details>

### [70] [Continuous Optimization for Feature Selection with Permutation-Invariant Embedding and Policy-Guided Search](https://arxiv.org/abs/2505.11601)
*Rui Liu,Rui Xie,Zijun Yao,Yanjie Fu,Dongjie Wang*

Main category: cs.LG

TLDR: 提出了一种新框架，通过编码器-解码器范式保持特征子集知识的连续嵌入空间，并利用强化学习探索空间，解决了现有方法在特征交互和空间搜索中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法难以捕捉复杂特征交互并适应多样场景，且存在嵌入空间排列敏感性和凸性假设问题。

Method: 采用编码器-解码器范式确保排列不变性，引入诱导点机制加速计算；利用基于策略的强化学习探索嵌入空间。

Result: 实验证明模型在有效性、效率、鲁棒性和可解释性方面表现优异。

Conclusion: 新框架成功解决了特征选择中的关键问题，提升了性能。

Abstract: Feature selection removes redundant features to enhanc performance and
computational efficiency in downstream tasks. Existing works often struggle to
capture complex feature interactions and adapt to diverse scenarios. Recent
advances in this domain have incorporated generative intelligence to address
these drawbacks by uncovering intricate relationships between features.
However, two key limitations remain: 1) embedding feature subsets in a
continuous space is challenging due to permutation sensitivity, as changes in
feature order can introduce biases and weaken the embedding learning process;
2) gradient-based search in the embedding space assumes convexity, which is
rarely guaranteed, leading to reduced search effectiveness and suboptimal
subsets. To address these limitations, we propose a new framework that can: 1)
preserve feature subset knowledge in a continuous embedding space while
ensuring permutation invariance; 2) effectively explore the embedding space
without relying on strong convex assumptions. For the first objective, we
develop an encoder-decoder paradigm to preserve feature selection knowledge
into a continuous embedding space. This paradigm captures feature interactions
through pairwise relationships within the subset, removing the influence of
feature order on the embedding. Moreover, an inducing point mechanism is
introduced to accelerate pairwise relationship computations. For the second
objective, we employ a policy-based reinforcement learning (RL) approach to
guide the exploration of the embedding space. The RL agent effectively
navigates the space by balancing multiple objectives. By prioritizing
high-potential regions adaptively and eliminating the reliance on convexity
assumptions, the RL agent effectively reduces the risk of converging to local
optima. Extensive experiments demonstrate the effectiveness, efficiency,
robustness and explicitness of our model.

</details>

### [71] [Regularity and Stability Properties of Selective SSMs with Discontinuous Gating](https://arxiv.org/abs/2505.11602)
*Nikola Zubić,Davide Scaramuzza*

Main category: cs.LG

TLDR: 论文研究了连续时间选择性状态空间模型（SSMs）的稳定性，通过被动性和输入到状态稳定性（ISS）分析，证明了能量耗散保证状态遗忘，并提出了全局ISS的充分条件。


<details>
  <summary>Details</summary>
Motivation: 研究选择性SSMs的稳定性，尤其是处理不连续门控信号时的挑战。

Method: 通过被动性和ISS理论分析模型稳定性，推导参数LMI条件和核约束。

Result: 证明了能量耗散导致状态遗忘，并提出了全局ISS的充分条件。

Conclusion: 为设计和理解稳定可靠的选择性SSMs提供了严格框架。

Abstract: Deep Selective State-Space Models (SSMs), characterized by input-dependent,
time-varying parameters, offer significant expressive power but pose challenges
for stability analysis, especially with discontinuous gating signals. In this
paper, we investigate the stability and regularity properties of
continuous-time selective SSMs through the lens of passivity and Input-to-State
Stability (ISS). We establish that intrinsic energy dissipation guarantees
exponential forgetting of past states. Crucially, we prove that the unforced
system dynamics possess an underlying minimal quadratic energy function whose
defining matrix exhibits robust $\text{AUC}_{\text{loc}}$ regularity,
accommodating discontinuous gating. Furthermore, assuming a universal quadratic
storage function ensures passivity across all inputs, we derive parametric LMI
conditions and kernel constraints that limit gating mechanisms, formalizing
"irreversible forgetting" of recurrent models. Finally, we provide sufficient
conditions for global ISS, linking uniform local dissipativity to overall
system robustness. Our findings offer a rigorous framework for understanding
and designing stable and reliable deep selective SSMs.

</details>

### [72] [A Classical View on Benign Overfitting: The Role of Sample Size](https://arxiv.org/abs/2505.11621)
*Junhyung Park,Patrick Bloebaum,Shiva Prasad Kasiviswanathan*

Main category: cs.LG

TLDR: 论文探讨了“几乎良性过拟合”现象，即在训练误差和测试误差均极小的情况下模型仍能良好泛化，并通过理论和实验验证了其存在。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络等模型在训练误差非零但仍能良好泛化的现象，提出“几乎良性过拟合”概念，并探索其在经典场景中的可能性。

Method: 通过理论分析，研究核岭回归和两层全连接ReLU神经网络的梯度流训练，提出新的证明技术分解超额风险。

Result: 验证了“几乎良性过拟合”现象的存在，并在神经网络中首次实现了不依赖回归函数或噪声假设的泛化结果。

Conclusion: 研究为理解模型复杂性与样本量交互作用提供了新视角，证明技术在避免均匀收敛陷阱方面具有独立价值。

Abstract: Benign overfitting is a phenomenon in machine learning where a model
perfectly fits (interpolates) the training data, including noisy examples, yet
still generalizes well to unseen data. Understanding this phenomenon has
attracted considerable attention in recent years. In this work, we introduce a
conceptual shift, by focusing on almost benign overfitting, where models
simultaneously achieve both arbitrarily small training and test errors. This
behavior is characteristic of neural networks, which often achieve low (but
non-zero) training error while still generalizing well. We hypothesize that
this almost benign overfitting can emerge even in classical regimes, by
analyzing how the interaction between sample size and model complexity enables
larger models to achieve both good training fit but still approach
Bayes-optimal generalization. We substantiate this hypothesis with theoretical
evidence from two case studies: (i) kernel ridge regression, and (ii)
least-squares regression using a two-layer fully connected ReLU neural network
trained via gradient flow. In both cases, we overcome the strong assumptions
often required in prior work on benign overfitting.
  Our results on neural networks also provide the first generalization result
in this setting that does not rely on any assumptions about the underlying
regression function or noise, beyond boundedness. Our analysis introduces a
novel proof technique based on decomposing the excess risk into estimation and
approximation errors, interpreting gradient flow as an implicit regularizer,
that helps avoid uniform convergence traps. This analysis idea could be of
independent interest.

</details>

### [73] [Nearest Neighbor Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.11625)
*Huiliang Zhang,Ping Nie,Lijun Sun,Benoit Boulet*

Main category: cs.LG

TLDR: 提出了一种基于k近邻检索机制的MTS预测框架（kNN-MTS），通过在大规模数据存储中检索相似模式，显著提升了预测性能，并设计了混合时空编码器（HSTEncoder）以捕获长短期依赖关系。


<details>
  <summary>Details</summary>
Motivation: 当前STGNN方法因计算复杂度限制只能处理有限长度的MTS数据，且难以识别稀疏分布的相似模式，导致性能提升有限。

Method: 采用k近邻检索机制，利用MTS模型的表示进行相似性搜索，无需额外训练；设计了HSTEncoder以捕获长短期时空依赖。

Result: 在多个真实数据集上显著提升了预测性能，并展示了模型的解释性和高效性。

Conclusion: kNN-MTS为高效利用大规模MTS数据提供了新途径，具有广阔的应用前景。

Abstract: Multivariate time series (MTS) forecasting has a wide range of applications
in both industry and academia. Recently, spatial-temporal graph neural networks
(STGNNs) have gained popularity as MTS forecasting methods. However, current
STGNNs can only use the finite length of MTS input data due to the
computational complexity. Moreover, they lack the ability to identify similar
patterns throughout the entire dataset and struggle with data that exhibit
sparsely and discontinuously distributed correlations among variables over an
extensive historical period, resulting in only marginal improvements. In this
article, we introduce a simple yet effective k-nearest neighbor MTS forecasting
( kNN-MTS) framework, which forecasts with a nearest neighbor retrieval
mechanism over a large datastore of cached series, using representations from
the MTS model for similarity search. This approach requires no additional
training and scales to give the MTS model direct access to the whole dataset at
test time, resulting in a highly expressive model that consistently improves
performance, and has the ability to extract sparse distributed but similar
patterns spanning over multivariables from the entire dataset. Furthermore, a
hybrid spatial-temporal encoder (HSTEncoder) is designed for kNN-MTS which can
capture both long-term temporal and short-term spatial-temporal dependencies
and is shown to provide accurate representation for kNN-MTSfor better
forecasting. Experimental results on several real-world datasets show a
significant improvement in the forecasting performance of kNN-MTS. The
quantitative analysis also illustrates the interpretability and efficiency of
kNN-MTS, showing better application prospects and opening up a new path for
efficiently using the large dataset in MTS models.

</details>

### [74] [Adaptive Robust Optimization with Data-Driven Uncertainty for Enhancing Distribution System Resilience](https://arxiv.org/abs/2505.11627)
*Shuyi Chen,Shixiang Zhu,Ramteen Sioshansi*

Main category: cs.LG

TLDR: 本文提出了一种新颖的三层优化框架，用于整合主动基础设施投资、时空破坏的对抗建模和自适应响应，以提升电力系统在极端天气下的韧性。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件对电力系统造成日益严重的压力，现有方法依赖简化的不确定性模型，且未考虑主动与被动决策的相互依赖。

Method: 采用三层优化框架，结合保形预测构建高概率、无分布不确定性集，并通过强对偶性和Benders分解算法求解嵌套决策问题。

Result: 实验表明，该方法在真实和合成数据上均优于传统鲁棒和两阶段方法，降低了最坏情况损失并提高了资源分配效率。

Conclusion: 该框架为电力系统在极端天气下的韧性规划提供了更有效的解决方案。

Abstract: Extreme weather events are placing growing strain on electric power systems,
exposing the limitations of purely reactive responses and prompting the need
for proactive resilience planning. However, existing approaches often rely on
simplified uncertainty models and decouple proactive and reactive decisions,
overlooking their critical interdependence. This paper proposes a novel
tri-level optimization framework that integrates proactive infrastructure
investment, adversarial modeling of spatio-temporal disruptions, and adaptive
reactive response. We construct high-probability, distribution-free uncertainty
sets using conformal prediction to capture complex and data-scarce outage
patterns. To solve the resulting nested decision problem, we derive a bi-level
reformulation via strong duality and develop a scalable Benders decomposition
algorithm. Experiments on both real and synthetic data demonstrate that our
approach consistently outperforms conventional robust and two-stage methods,
achieving lower worst-case losses and more efficient resource allocation,
especially under tight operational constraints and large-scale uncertainty.

</details>

### [75] [Enhancing Network Anomaly Detection with Quantum GANs and Successive Data Injection for Multivariate Time Series](https://arxiv.org/abs/2505.11631)
*Wajdi Hammami,Soumaya Cherkaoui,Shengrui Wang*

Main category: cs.LG

TLDR: 论文提出了一种基于量子生成对抗网络（QGAN）的多元时间序列异常检测方法，结合变分量子电路（VQC）、时间窗口移位、数据重上传和连续数据注入（SuDaI）技术，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 量子计算为机器学习提供了新思路，尤其是在复杂任务如网络流量异常检测中。本文旨在利用量子计算的优势，解决经典模型在硬件限制下的不足。

Method: 采用QGAN架构，通过VQC将多元时间序列数据编码为旋转角度，结合数据重上传和SuDaI技术高效映射经典数据到量子态，并使用生成器和判别器输出进行异常评分。

Result: 实验表明，量子模型在准确率、召回率和F1分数上优于经典GAN，且参数更少（仅80个），在噪声环境下仍保持有效性。

Conclusion: QGAN在多元时间序列异常检测中表现出色，展示了量子计算在机器学习中的潜力。

Abstract: Quantum computing may offer new approaches for advancing machine learning,
including in complex tasks such as anomaly detection in network traffic. In
this paper, we introduce a quantum generative adversarial network (QGAN)
architecture for multivariate time-series anomaly detection that leverages
variational quantum circuits (VQCs) in combination with a time-window shifting
technique, data re-uploading, and successive data injection (SuDaI). The method
encodes multivariate time series data as rotation angles. By integrating both
data re-uploading and SuDaI, the approach maps classical data into quantum
states efficiently, helping to address hardware limitations such as the
restricted number of available qubits. In addition, the approach employs an
anomaly scoring technique that utilizes both the generator and the
discriminator output to enhance the accuracy of anomaly detection. The QGAN was
trained using the parameter shift rule and benchmarked against a classical GAN.
Experimental results indicate that the quantum model achieves a accuracy high
along with high recall and F1-scores in anomaly detection, and attains a lower
MSE compared to the classical model. Notably, the QGAN accomplishes this
performance with only 80 parameters, demonstrating competitive results with a
compact architecture. Tests using a noisy simulator suggest that the approach
remains effective under realistic noise-prone conditions.

</details>

### [76] [The Gaussian-Multinoulli Restricted Boltzmann Machine: A Potts Model Extension of the GRBM](https://arxiv.org/abs/2505.11635)
*Nikhil Kapasi,William Whitehead,Luke Theogarajan*

Main category: cs.LG

TLDR: 论文提出了一种高斯-多项受限玻尔兹曼机（GM-RBM），通过用多状态Potts变量替换二元隐藏单元，扩展了高斯-伯努利RBM（GB-RBM），以支持更丰富的离散结构化表示。


<details>
  <summary>Details</summary>
Motivation: 现实任务需要离散结构化表示，而传统连续潜在模型难以自然表达。

Method: 引入GM-RBM，推导其能量函数、学习动态和条件分布，保持可处理的推理和训练。

Result: GM-RBM在多模态分布建模上优于二元RBM，在类比回忆和结构化记忆任务中表现更佳。

Conclusion: GM-RBM为离散潜在推理提供了更具表达力和互操作性的可扩展框架。

Abstract: Many real-world tasks, from associative memory to symbolic reasoning, demand
discrete, structured representations that standard continuous latent models
struggle to express naturally. We introduce the Gaussian-Multinoulli Restricted
Boltzmann Machine (GM-RBM), a generative energy-based model that extends the
Gaussian-Bernoulli RBM (GB-RBM) by replacing binary hidden units with $q$-state
Potts variables. This modification enables a combinatorially richer latent
space and supports learning over multivalued, interpretable latent concepts. We
formally derive GM-RBM's energy function, learning dynamics, and conditional
distributions, showing that it preserves tractable inference and training
through contrastive divergence. Empirically, we demonstrate that GM-RBMs model
complex multimodal distributions more effectively than binary RBMs,
outperforming them on tasks involving analogical recall and structured memory.
Our results highlight GM-RBMs as a scalable framework for discrete latent
inference with enhanced expressiveness and interoperability.

</details>

### [77] [Generalization Guarantees for Learning Branch-and-Cut Policies in Integer Programming](https://arxiv.org/abs/2505.11636)
*Hongyu Cheng,Amitabh Basu*

Main category: cs.LG

TLDR: 该论文为学习分支定界（B&C）策略建立了严格的样本复杂度界限，适用于具有分段多项式结构的评分函数，并推广了线性模型和神经网络架构。


<details>
  <summary>Details</summary>
Motivation: 理解学习到的B&C策略的理论基础，特别是从有限数据中学习的泛化性能。

Method: 建立样本复杂度界限，适用于分段多项式结构的评分函数，涵盖线性模型和神经网络架构。

Result: 提出了一个理论框架，适用于B&C策略及更广泛的序列决策问题。

Conclusion: 该理论框架为B&C策略的实践和理论研究提供了统一视角，并适用于更广泛的序列决策问题。

Abstract: Mixed-integer programming (MIP) provides a powerful framework for
optimization problems, with Branch-and-Cut (B&C) being the predominant
algorithm in state-of-the-art solvers. The efficiency of B&C critically depends
on heuristic policies for making sequential decisions, including node
selection, cut selection, and branching variable selection. While traditional
solvers often employ heuristics with manually tuned parameters, recent
approaches increasingly leverage machine learning, especially neural networks,
to learn these policies directly from data. A key challenge is to understand
the theoretical underpinnings of these learned policies, particularly their
generalization performance from finite data. This paper establishes rigorous
sample complexity bounds for learning B&C policies where the scoring functions
guiding each decision step (node, cut, branch) have a certain piecewise
polynomial structure. This structure generalizes the linear models that form
the most commonly deployed policies in practice and investigated recently in a
foundational series of theoretical works by Balcan et al. Such piecewise
polynomial policies also cover the neural network architectures (e.g., using
ReLU activations) that have been the focal point of contemporary practical
studies. Consequently, our theoretical framework closely reflects the models
utilized by practitioners investigating machine learning within B&C, offering a
unifying perspective relevant to both established theory and modern empirical
research in this area. Furthermore, our theory applies to quite general
sequential decision making problems beyond B&C.

</details>

### [78] [Urban Representation Learning for Fine-grained Economic Mapping: A Semi-supervised Graph-based Approach](https://arxiv.org/abs/2505.11645)
*Jinzhou Cao,Xiangxu Wang,Jiashi Chen,Wei Tu,Zhenhui Li,Xindong Yang,Tianhong Zhao,Qingquan Li*

Main category: cs.LG

TLDR: SemiGTX是一种可解释的半监督图学习框架，用于部门经济映射，通过多任务学习和融合地理空间数据模态，显著提升了经济预测的精确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在数据稀缺场景中忽视半监督学习，且缺乏统一的多任务框架，无法全面分析部门经济。

Method: 提出SemiGTX框架，结合空间自监督和局部掩码监督回归的半信息损失函数，通过多任务学习统一映射GDP。

Result: 在珠三角地区的实验中，模型在三个部门的R2分数分别为0.93、0.96和0.94，跨区域实验验证了其泛化能力。

Conclusion: SemiGTX通过多样化城市数据整合，为精确经济预测提供了坚实基础，推动了区域经济监测的发展。

Abstract: Fine-grained economic mapping through urban representation learning has
emerged as a crucial tool for evidence-based economic decisions. While existing
methods primarily rely on supervised or unsupervised approaches, they often
overlook semi-supervised learning in data-scarce scenarios and lack unified
multi-task frameworks for comprehensive sectoral economic analysis. To address
these gaps, we propose SemiGTX, an explainable semi-supervised graph learning
framework for sectoral economic mapping. The framework is designed with
dedicated fusion encoding modules for various geospatial data modalities,
seamlessly integrating them into a cohesive graph structure. It introduces a
semi-information loss function that combines spatial self-supervision with
locally masked supervised regression, enabling more informative and effective
region representations. Through multi-task learning, SemiGTX concurrently maps
GDP across primary, secondary, and tertiary sectors within a unified model.
Extensive experiments conducted in the Pearl River Delta region of China
demonstrate the model's superior performance compared to existing methods,
achieving R2 scores of 0.93, 0.96, and 0.94 for the primary, secondary and
tertiary sectors, respectively. Cross-regional experiments in Beijing and
Chengdu further illustrate its generality. Systematic analysis reveals how
different data modalities influence model predictions, enhancing explainability
while providing valuable insights for regional development planning. This
representation learning framework advances regional economic monitoring through
diverse urban data integration, providing a robust foundation for precise
economic forecasting.

</details>

### [79] [Joint Graph Estimation and Signal Restoration for Robust Federated Learning](https://arxiv.org/abs/2505.11648)
*Tsutahiro Fukuhara,Junya Hara,Hiroshi Higashi,Yuichi Tanaka*

Main category: cs.LG

TLDR: 提出一种联邦学习中针对噪声通信的鲁棒聚合方法，通过图学习和信号恢复提升模型准确性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端与服务器间的噪声和缺失值会导致模型准确性下降。

Method: 通过图学习表示客户端模型参数间的关系，并联合信号恢复问题，采用DC优化和近端DC算法求解。

Result: 在MNIST和CIFAR-10数据集上，分类准确性比现有方法高2%-5%。

Conclusion: 该方法在噪声和偏置数据分布下显著提升了联邦学习的性能。

Abstract: We propose a robust aggregation method for model parameters in federated
learning (FL) under noisy communications. FL is a distributed machine learning
paradigm in which a central server aggregates local model parameters from
multiple clients. These parameters are often noisy and/or have missing values
during data collection, training, and communication between the clients and
server. This may cause a considerable drop in model accuracy. To address this
issue, we learn a graph that represents pairwise relationships between model
parameters of the clients during aggregation. We realize it with a joint
problem of graph learning and signal (i.e., model parameters) restoration. The
problem is formulated as a difference-of-convex (DC) optimization, which is
efficiently solved via a proximal DC algorithm. Experimental results on MNIST
and CIFAR-10 datasets show that the proposed method outperforms existing
approaches by up to $2$--$5\%$ in classification accuracy under biased data
distributions and noisy conditions.

</details>

### [80] [UrbanMind: Urban Dynamics Prediction with Multifaceted Spatial-Temporal Large Language Models](https://arxiv.org/abs/2505.11654)
*Yuhang Liu,Yingxue Zhang,Xin Zhang,Ling Tian,Xu Zheng,Yanhua Li,Jun Luo*

Main category: cs.LG

TLDR: UrbanMind是一个新颖的时空大语言模型框架，用于多方面的城市动态预测，通过Muffin-MAE和语义感知提示策略提升预测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法泛化能力有限，而大语言模型在时空城市动态中的应用尚未充分探索，且难以处理多源数据和分布偏移问题。

Method: 提出Muffin-MAE（多源融合掩码自编码器）和语义感知提示策略，结合测试时间适应机制。

Result: 在多个城市的真实数据集上，UrbanMind表现优于现有方法，具有高准确性和鲁棒泛化能力。

Conclusion: UrbanMind为城市动态预测提供了高效且泛化能力强的解决方案。

Abstract: Understanding and predicting urban dynamics is crucial for managing
transportation systems, optimizing urban planning, and enhancing public
services. While neural network-based approaches have achieved success, they
often rely on task-specific architectures and large volumes of data, limiting
their ability to generalize across diverse urban scenarios. Meanwhile, Large
Language Models (LLMs) offer strong reasoning and generalization capabilities,
yet their application to spatial-temporal urban dynamics remains underexplored.
Existing LLM-based methods struggle to effectively integrate multifaceted
spatial-temporal data and fail to address distributional shifts between
training and testing data, limiting their predictive reliability in real-world
applications. To bridge this gap, we propose UrbanMind, a novel
spatial-temporal LLM framework for multifaceted urban dynamics prediction that
ensures both accurate forecasting and robust generalization. At its core,
UrbanMind introduces Muffin-MAE, a multifaceted fusion masked autoencoder with
specialized masking strategies that capture intricate spatial-temporal
dependencies and intercorrelations among multifaceted urban dynamics.
Additionally, we design a semantic-aware prompting and fine-tuning strategy
that encodes spatial-temporal contextual details into prompts, enhancing LLMs'
ability to reason over spatial-temporal patterns. To further improve
generalization, we introduce a test time adaptation mechanism with a test data
reconstructor, enabling UrbanMind to dynamically adjust to unseen test data by
reconstructing LLM-generated embeddings. Extensive experiments on real-world
urban datasets across multiple cities demonstrate that UrbanMind consistently
outperforms state-of-the-art baselines, achieving high accuracy and robust
generalization, even in zero-shot settings.

</details>

### [81] [A Local Polyak-Lojasiewicz and Descent Lemma of Gradient Descent For Overparametrized Linear Models](https://arxiv.org/abs/2505.11664)
*Ziqing Xu,Hancheng Min,Salma Tarmoun,Enrique Mallada,Rene Vidal*

Main category: cs.LG

TLDR: 本文研究了梯度下降（GD）在两层线性神经网络中的收敛性，放宽了对步长、宽度和初始化的强假设，并推导出线性收敛速率。


<details>
  <summary>Details</summary>
Motivation: 先前的研究对梯度下降的收敛性依赖强假设（如步长极小、宽度无限或初始化特殊），本文旨在放宽这些假设，提供更通用的收敛性分析。

Method: 通过证明局部Polyak-Łojasiewicz（PL）条件和下降引理成立，并基于权重初始化、当前损失和非过参数化模型的全局常数，推导出GD的线性收敛速率。

Result: 在放宽假设的条件下，证明了GD的线性收敛性，并通过数值实验验证了步长的优化选择。

Conclusion: 本文不仅改进了先前结果，还为步长选择提供了更好的理论依据。

Abstract: Most prior work on the convergence of gradient descent (GD) for
overparameterized neural networks relies on strong assumptions on the step size
(infinitesimal), the hidden-layer width (infinite), or the initialization
(large, spectral, balanced). Recent efforts to relax these assumptions focus on
two-layer linear networks trained with the squared loss. In this work, we
derive a linear convergence rate for training two-layer linear neural networks
with GD for general losses and under relaxed assumptions on the step size,
width, and initialization. A key challenge in deriving this result is that
classical ingredients for deriving convergence rates for nonconvex problems,
such as the Polyak-{\L}ojasiewicz (PL) condition and Descent Lemma, do not hold
globally for overparameterized neural networks. Here, we prove that these two
conditions hold locally with local constants that depend on the weights. Then,
we provide bounds on these local constants, which depend on the initialization
of the weights, the current loss, and the global PL and smoothness constants of
the non-overparameterized model. Based on these bounds, we derive a linear
convergence rate for GD. Our convergence analysis not only improves upon prior
results but also suggests a better choice for the step size, as verified
through our numerical experiments.

</details>

### [82] [OT Score: An OT based Confidence Score for Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.11669)
*Yiming Zhang,Sitong Liu,Alex Cloninger*

Main category: cs.LG

TLDR: 提出了一种基于最优传输（OT）的置信度评分（OT score），用于无监督域适应（UDA）中的分类性能评估，解决了现有方法的计算和理论限制。


<details>
  <summary>Details</summary>
Motivation: 现有分布对齐方法在无监督域适应中存在计算和理论上的局限性，特别是在无目标标签时评估分类性能和置信度方面。

Method: 通过半离散最优传输对齐的灵活性，提出了一种新的理论分析，并推导出OT score作为置信度指标。

Result: 实验表明，OT score能显著提升分类准确率，优于现有置信度指标。

Conclusion: OT score是一种直观、理论严谨且计算高效的置信度评估方法，适用于多种适应场景。

Abstract: We address the computational and theoretical limitations of existing
distributional alignment methods for unsupervised domain adaptation (UDA),
particularly regarding the estimation of classification performance and
confidence without target labels. Current theoretical frameworks for these
methods often yield computationally intractable quantities and fail to
adequately reflect the properties of the alignment algorithms employed. To
overcome these challenges, we introduce the Optimal Transport (OT) score, a
confidence metric derived from a novel theoretical analysis that exploits the
flexibility of decision boundaries induced by Semi-Discrete Optimal Transport
alignment. The proposed OT score is intuitively interpretable, theoretically
rigorous, and computationally efficient. It provides principled uncertainty
estimates for any given set of target pseudo-labels without requiring model
retraining, and can flexibly adapt to varying degrees of available source
information. Experimental results on standard UDA benchmarks demonstrate that
classification accuracy consistently improves by identifying and removing
low-confidence predictions, and that OT score significantly outperforms
existing confidence metrics across diverse adaptation scenarios.

</details>

### [83] [Mollifier Layers: Enabling Efficient High-Order Derivatives in Inverse PDE Learning](https://arxiv.org/abs/2505.11682)
*Ananyae Kumar Bhartari,Vinayak Vinayak,Vivek B Shenoy*

Main category: cs.LG

TLDR: 论文提出了一种名为Mollifier Layers的轻量级模块，用于替代自动微分（autodiff）在物理约束机器学习中的高阶导数计算，通过卷积操作实现高效、抗噪的参数估计。


<details>
  <summary>Details</summary>
Motivation: 现有基于自动微分的方法在高阶导数计算中存在精度低、内存占用大、抗噪性差的问题，限制了物理约束机器学习的应用。

Method: 提出Mollifier Layers模块，利用解析定义的mollifiers进行卷积操作，替代自动微分计算高阶导数，无需修改网络架构。

Result: 在多种PDE任务中（如Langevin动力学、热扩散等），Mollifier Layers显著提升了内存效率、训练速度和参数恢复精度。

Conclusion: Mollifier Layers为物理约束学习提供了一种高效、可扩展的工具，并在实际生物医学问题中验证了其有效性。

Abstract: Parameter estimation in inverse problems involving partial differential
equations (PDEs) underpins modeling across scientific disciplines, especially
when parameters vary in space or time. Physics-informed Machine Learning
(PhiML) integrates PDE constraints into deep learning, but prevailing
approaches depend on recursive automatic differentiation (autodiff), which
produces inaccurate high-order derivatives, inflates memory usage, and
underperforms in noisy settings. We propose Mollifier Layers, a lightweight,
architecture-agnostic module that replaces autodiff with convolutional
operations using analytically defined mollifiers. This reframing of derivative
computation as smoothing integration enables efficient, noise-robust estimation
of high-order derivatives directly from network outputs. Mollifier Layers
attach at the output layer and require no architectural modifications. We
compare them with three distinct architectures and benchmark performance across
first-, second-, and fourth-order PDEs -- including Langevin dynamics, heat
diffusion, and reaction-diffusion systems -- observing significant improvements
in memory efficiency, training time and accuracy for parameter recovery across
tasks. To demonstrate practical relevance, we apply Mollifier Layers to infer
spatially varying epigenetic reaction rates from super-resolution chromatin
imaging data -- a real-world inverse problem with biomedical significance. Our
results establish Mollifier Layers as an efficient and scalable tool for
physics-constrained learning.

</details>

### [84] [The Geometry of ReLU Networks through the ReLU Transition Graph](https://arxiv.org/abs/2505.11692)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TLDR: 本文提出了一种通过ReLU Transition Graph（RTG）分析ReLU神经网络的新理论框架，揭示了RTG几何性质与表达能力、泛化性和鲁棒性的联系。


<details>
  <summary>Details</summary>
Motivation: 通过组合对象RTG统一分析ReLU网络的结构，填补了图论与神经网络理论结合的空白。

Method: 构建RTG，研究其几何性质（如大小、直径、连通性），并通过实验验证理论结果。

Result: 提出了RTG的紧组合界、连通性证明，以及图论对VC维的解释，并验证了RTG熵与泛化误差的关系。

Conclusion: RTG为ReLU网络的结构分析提供了统一框架，为压缩、正则化和复杂度控制开辟了新途径。

Abstract: We develop a novel theoretical framework for analyzing ReLU neural networks
through the lens of a combinatorial object we term the ReLU Transition Graph
(RTG). In this graph, each node corresponds to a linear region induced by the
network's activation patterns, and edges connect regions that differ by a
single neuron flip. Building on this structure, we derive a suite of new
theoretical results connecting RTG geometry to expressivity, generalization,
and robustness. Our contributions include tight combinatorial bounds on RTG
size and diameter, a proof of RTG connectivity, and graph-theoretic
interpretations of VC-dimension. We also relate entropy and average degree of
the RTG to generalization error. Each theoretical result is rigorously
validated via carefully controlled experiments across varied network depths,
widths, and data regimes. This work provides the first unified treatment of
ReLU network structure via graph theory and opens new avenues for compression,
regularization, and complexity control rooted in RTG analysis.

</details>

### [85] [Neural Networks as Universal Finite-State Machines: A Constructive Deterministic Finite Automaton Theory](https://arxiv.org/abs/2505.11694)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TLDR: 该论文证明了前馈神经网络可以作为通用有限状态机（N-FSMs），通过理论框架和实证验证了ReLU和阈值网络能精确模拟确定性有限自动机（DFAs）。


<details>
  <summary>Details</summary>
Motivation: 研究动机是建立神经网络与离散符号计算之间的桥梁，探索神经网络如何实现离散符号过程。

Method: 方法包括将DFA状态转移展开为神经网络的深度层，并分析所需的深度、宽度和状态压缩。

Result: 结果表明DFA转移是线性可分的，二进制阈值激活允许指数压缩，且Myhill-Nerode等价类可嵌入连续潜在空间。

Conclusion: 结论是固定深度的前馈网络无法识别需要无界内存的非正则语言，但为神经符号计算提供了严格蓝图。

Abstract: We present a complete theoretical and empirical framework establishing
feedforward neural networks as universal finite-state machines (N-FSMs). Our
results prove that finite-depth ReLU and threshold networks can exactly
simulate deterministic finite automata (DFAs) by unrolling state transitions
into depth-wise neural layers, with formal characterizations of required depth,
width, and state compression. We demonstrate that DFA transitions are linearly
separable, binary threshold activations allow exponential compression, and
Myhill-Nerode equivalence classes can be embedded into continuous latent spaces
while preserving separability. We also formalize the expressivity boundary:
fixed-depth feedforward networks cannot recognize non-regular languages
requiring unbounded memory. Unlike prior heuristic or probing-based studies, we
provide constructive proofs and design explicit DFA-unrolled neural
architectures that empirically validate every claim. Our results bridge deep
learning, automata theory, and neural-symbolic computation, offering a rigorous
blueprint for how discrete symbolic processes can be realized in continuous
neural systems.

</details>

### [86] [Qronos: Correcting the Past by Shaping the Future... in Post-Training Quantization](https://arxiv.org/abs/2505.11695)
*Shihao Zhang,Haoyu Zhang,Ian Colbert,Rayan Saab*

Main category: cs.LG

TLDR: Qronos是一种新的后训练量化算法，通过顺序舍入和更新神经网络权重，显著提升量化性能。


<details>
  <summary>Details</summary>
Motivation: 解决权重和激活量化以及前层量化带来的误差问题，超越现有数据驱动方法。

Method: 基于可解释的优化框架，交替进行误差校正和扩散，利用Cholesky分解高效求解最小二乘问题。

Result: 在Llama3系列模型中，Qronos在权重、激活和KV缓存量化中均优于现有方法。

Conclusion: Qronos是一种高效且兼容现有技术的量化算法，显著提升了量化性能。

Abstract: We introduce Qronos -- a new state-of-the-art post-training quantization
algorithm that sequentially rounds and updates neural network weights. Qronos
not only explicitly corrects errors due to both weight and activation
quantization, but also errors resulting from quantizing previous layers. Our
iterative algorithm is based on an interpretable and disciplined optimization
framework that subsumes and surpasses existing data-driven approaches. At each
step, Qronos alternates between error correction and diffusion via optimal
update rules. Importantly, we prove that Qronos admits an efficient
implementation that uses the Cholesky decomposition for solving least-squares
problems. We also demonstrate that Qronos is compatible with existing
transformation techniques such as Hadamard-based incoherence processing and
weight-activation scaling equalization, among others. We evaluate Qronos using
recent autoregressive language generation models in the Llama3 family; Qronos
consistently outperforms previous state-of-the-art adaptive rounding methods
when quantizing the weights, activations, and/or KV caches.

</details>

### [87] [Invariant Representations via Wasserstein Correlation Maximization](https://arxiv.org/abs/2505.11702)
*Keenan Eikenberry,Lizuo Liu,Yoonsang Lee*

Main category: cs.LG

TLDR: 本文研究了基于Wasserstein距离的Wasserstein相关性在无监督表示学习中的应用，发现其能压缩数据维度并保留输入分布的拓扑和几何特性，同时还能实现对特定增强的近似不变性。


<details>
  <summary>Details</summary>
Motivation: 探索一种基于Wasserstein距离的统计依赖性度量方法，用于无监督表示学习，以替代传统的对比方法，并实现数据压缩和增强不变性。

Method: 通过最大化输入和编码分布之间的Wasserstein相关性训练（自动）编码器，并利用Markov-Wasserstein核定义增强编码器以实现不变性。

Result: 实验表明，该方法能实现数据压缩和增强不变性，且适用于简单前馈网络或预训练模型。

Conclusion: Wasserstein相关性最大化是一种有效的无监督学习方法，能同时实现数据压缩和增强不变性，并具有理论支持。

Abstract: This work investigates the use of Wasserstein correlation -- a normalized
measure of statistical dependence based on the Wasserstein distance between a
joint distribution and the product of its marginals -- for unsupervised
representation learning. Unlike, for example, contrastive methods, which
naturally cluster classes in the latent space, we find that an (auto)encoder
trained to maximize Wasserstein correlation between the input and encoded
distributions instead acts as a compressor, reducing dimensionality while
approximately preserving the topological and geometric properties of the input
distribution. More strikingly, we show that Wasserstein correlation
maximization can be used to arrive at an (auto)encoder -- either trained from
scratch, or else one that extends a frozen, pretrained model -- that is
approximately invariant to a chosen augmentation, or collection of
augmentations, and that still approximately preserves the structural properties
of the non-augmented input distribution. To do this, we first define the notion
of an augmented encoder using the machinery of Markov-Wasserstein kernels. When
the maximization objective is then applied to the augmented encoder, as opposed
to the underlying, deterministic encoder, the resulting model exhibits the
desired invariance properties. Finally, besides our experimental results, which
show that even simple feedforward networks can be imbued with invariants or
can, alternatively, be used to impart invariants to pretrained models under
this training process, we additionally establish various theoretical results
for optimal transport-based dependence measures. Code is available at
https://github.com/keenan-eikenberry/wasserstein_correlation_maximization .

</details>

### [88] [Reinforcement Learning Finetunes Small Subnetworks in Large Language Models](https://arxiv.org/abs/2505.11711)
*Sagnik Mukherjee,Lifan Yuan,Dilek Hakkani-Tur,Hao Peng*

Main category: cs.LG

TLDR: 强化学习（RL）在大型语言模型（LLMs）中仅更新5%至30%的参数即可显著提升下游任务性能和人类价值观对齐，这种现象称为参数更新稀疏性。


<details>
  <summary>Details</summary>
Motivation: 研究RL在LLMs中参数更新稀疏性的现象及其原因。

Method: 实验中使用7种RL算法和10种不同家族的LLMs，分析参数更新稀疏性的表现和特征。

Result: RL仅更新少量参数即可达到与全参数更新相近的性能，且不同条件下的子网络重叠度高。

Conclusion: 参数更新稀疏性主要由训练数据接近策略分布引起，KL正则化和梯度裁剪影响有限。

Abstract: Reinforcement learning (RL) yields substantial improvements in large language
models (LLMs) downstream task performance and alignment with human values.
Surprisingly, such large gains result from updating only a small subnetwork
comprising just 5 percent to 30 percent of the parameters, with the rest
effectively unchanged. We refer to this phenomenon as parameter update sparsity
induced by RL. It is observed across all 7 widely used RL algorithms (e.g.,
PPO, GRPO, DPO) and all 10 LLMs from different families in our experiments.
This sparsity is intrinsic and occurs without any explicit sparsity promoting
regularizations or architectural constraints. Finetuning the subnetwork alone
recovers the test accuracy, and, remarkably, produces a model nearly identical
to the one obtained via full finetuning. The subnetworks from different random
seeds, training data, and even RL algorithms show substantially greater overlap
than expected by chance. Our analysis suggests that this sparsity is not due to
updating only a subset of layers, instead, nearly all parameter matrices
receive similarly sparse updates. Moreover, the updates to almost all parameter
matrices are nearly full-rank, suggesting RL updates a small subset of
parameters that nevertheless span almost the full subspaces that the parameter
matrices can represent. We conjecture that the this update sparsity can be
primarily attributed to training on data that is near the policy distribution,
techniques that encourage the policy to remain close to the pretrained model,
such as the KL regularization and gradient clipping, have limited impact.

</details>

### [89] [Bi-Level Policy Optimization with Nyström Hypergradients](https://arxiv.org/abs/2505.11714)
*Arjun Prakash,Naicheng He,Denizalp Goktas,Amy Greenwald*

Main category: cs.LG

TLDR: BLPO算法通过嵌套更新和Nyström方法改进AC强化学习，解决了数值不稳定性问题，并在实验中表现优于PPO。


<details>
  <summary>Details</summary>
Motivation: AC强化学习中的actor对critic的依赖可建模为双层优化问题，需改进传统AC算法以更稳定高效地求解。

Method: 提出BLPO算法，嵌套更新critic并使用Nyström方法计算超梯度。

Result: 理论证明BLPO在多项式时间内收敛到局部强Stackelberg均衡，实验表现优于PPO。

Conclusion: BLPO通过双层优化框架和数值稳定方法，显著提升了AC算法的性能。

Abstract: The dependency of the actor on the critic in actor-critic (AC) reinforcement
learning means that AC can be characterized as a bilevel optimization (BLO)
problem, also called a Stackelberg game. This characterization motivates two
modifications to vanilla AC algorithms. First, the critic's update should be
nested to learn a best response to the actor's policy. Second, the actor should
update according to a hypergradient that takes changes in the critic's behavior
into account. Computing this hypergradient involves finding an inverse Hessian
vector product, a process that can be numerically unstable. We thus propose a
new algorithm, Bilevel Policy Optimization with Nystr\"om Hypergradients
(BLPO), which uses nesting to account for the nested structure of BLO, and
leverages the Nystr\"om method to compute the hypergradient. Theoretically, we
prove BLPO converges to (a point that satisfies the necessary conditions for) a
local strong Stackelberg equilibrium in polynomial time with high probability,
assuming a linear parametrization of the critic's objective. Empirically, we
demonstrate that BLPO performs on par with or better than PPO on a variety of
discrete and continuous control tasks.

</details>

### [90] [EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents](https://arxiv.org/abs/2505.11717)
*Xilong Wang,John Bloch,Zedian Shao,Yuepeng Hu,Shuyan Zhou,Neil Zhenqiang Gong*

Main category: cs.LG

TLDR: EnvInjection是一种针对多模态大语言模型（MLLM）网页代理的攻击方法，通过修改网页像素值诱导代理执行特定动作，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法在有效性、隐蔽性或实用性上存在不足，EnvInjection旨在解决这些问题。

Method: 通过优化问题设计扰动，训练神经网络近似像素到截图的映射，并使用投影梯度下降求解。

Result: 在多个网页数据集上验证，EnvInjection效果显著优于基线方法。

Conclusion: EnvInjection是一种高效且实用的攻击方法，解决了现有技术的局限性。

Abstract: Multi-modal large language model (MLLM)-based web agents interact with
webpage environments by generating actions based on screenshots of the
webpages. Environmental prompt injection attacks manipulate the environment to
induce the web agent to perform a specific, attacker-chosen action--referred to
as the target action. However, existing attacks suffer from limited
effectiveness or stealthiness, or are impractical in real-world settings. In
this work, we propose EnvInjection, a new attack that addresses these
limitations. Our attack adds a perturbation to the raw pixel values of the
rendered webpage, which can be implemented by modifying the webpage's source
code. After these perturbed pixels are mapped into a screenshot, the
perturbation induces the web agent to perform the target action. We formulate
the task of finding the perturbation as an optimization problem. A key
challenge in solving this problem is that the mapping between raw pixel values
and screenshot is non-differentiable, making it difficult to backpropagate
gradients to the perturbation. To overcome this, we train a neural network to
approximate the mapping and apply projected gradient descent to solve the
reformulated optimization problem. Extensive evaluation on multiple webpage
datasets shows that EnvInjection is highly effective and significantly
outperforms existing baselines.

</details>

### [91] [CLT and Edgeworth Expansion for m-out-of-n Bootstrap Estimators of The Studentized Median](https://arxiv.org/abs/2505.11725)
*Imon Banerjee,Sayak Chakrabarty*

Main category: cs.LG

TLDR: 本文为m-out-of-n自助法在样本分位数估计中的有效性提供了严格的参数自由保证，包括中心极限定理和Edgeworth展开，并展示了其在实际统计中的应用。


<details>
  <summary>Details</summary>
Motivation: 尽管m-out-of-n自助法在多个领域广泛应用，但其在样本分位数估计中的理论保证仍不完善。本文旨在填补这一空白。

Method: 通过分析m-out-of-n重采样得到的样本分位数估计量，证明了中心极限定理，并构造反例验证其紧性。进一步推导了Edgeworth展开和Berry Esseen界。

Result: 在温和矩条件下，证明了数据驱动的估计量的中心极限定理，并展示了其在实际统计问题中的适用性。

Conclusion: 本文的理论结果为m-out-of-n自助法在样本分位数估计中的有效性提供了坚实的理论基础，并展示了其在现代估计和学习任务中的实用性。

Abstract: The m-out-of-n bootstrap, originally proposed by Bickel, Gotze, and Zwet
(1992), approximates the distribution of a statistic by repeatedly drawing m
subsamples (with m much smaller than n) without replacement from an original
sample of size n. It is now routinely used for robust inference with
heavy-tailed data, bandwidth selection, and other large-sample applications.
Despite its broad applicability across econometrics, biostatistics, and machine
learning, rigorous parameter-free guarantees for the soundness of the
m-out-of-n bootstrap when estimating sample quantiles have remained elusive.
  This paper establishes such guarantees by analyzing the estimator of sample
quantiles obtained from m-out-of-n resampling of a dataset of size n. We first
prove a central limit theorem for a fully data-driven version of the estimator
that holds under a mild moment condition and involves no unknown nuisance
parameters. We then show that the moment assumption is essentially tight by
constructing a counter-example in which the CLT fails. Strengthening the
assumptions slightly, we derive an Edgeworth expansion that provides exact
convergence rates and, as a corollary, a Berry Esseen bound on the bootstrap
approximation error. Finally, we illustrate the scope of our results by
deriving parameter-free asymptotic distributions for practical statistics,
including the quantiles for random walk Metropolis-Hastings and the rewards of
ergodic Markov decision processes, thereby demonstrating the usefulness of our
theory in modern estimation and learning tasks.

</details>

### [92] [Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models](https://arxiv.org/abs/2505.11731)
*Harshil Vejendla,Haizhou Shi,Yibin Wang,Tunyu Zhang,Huan Zhang,Hao Wang*

Main category: cs.LG

TLDR: 提出了一种无需测试时采样的LLM不确定性估计方法，通过蒸馏技术将贝叶斯LLM的置信度转移到非贝叶斯学生LLM中，显著提高了效率。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯方法在推理时需要多次采样，效率低下，限制了实际部署。

Method: 通过最小化预测分布之间的差异，将贝叶斯LLM的置信度蒸馏到非贝叶斯学生LLM中，无需额外验证数据集。

Result: 实验表明，该方法在测试时效率提高N倍（N为传统贝叶斯LLM所需采样次数），且性能与最先进贝叶斯LLM相当或更优。

Conclusion: 蒸馏技术成功将不确定性估计能力从训练数据推广到测试数据，提供了一种高效且可靠的解决方案。

Abstract: Recent advances in uncertainty estimation for Large Language Models (LLMs)
during downstream adaptation have addressed key challenges of reliability and
simplicity. However, existing Bayesian methods typically require multiple
sampling iterations during inference, creating significant efficiency issues
that limit practical deployment. In this paper, we investigate the possibility
of eliminating the need for test-time sampling for LLM uncertainty estimation.
Specifically, when given an off-the-shelf Bayesian LLM, we distill its aligned
confidence into a non-Bayesian student LLM by minimizing the divergence between
their predictive distributions. Unlike typical calibration methods, our
distillation is carried out solely on the training dataset without the need of
an additional validation dataset. This simple yet effective approach achieves
N-times more efficient uncertainty estimation during testing, where N is the
number of samples traditionally required by Bayesian LLMs. Our extensive
experiments demonstrate that uncertainty estimation capabilities on training
data can successfully generalize to unseen test data through our distillation
technique, consistently producing results comparable to (or even better than)
state-of-the-art Bayesian LLMs.

</details>

### [93] [Token-Level Uncertainty Estimation for Large Language Model Reasoning](https://arxiv.org/abs/2505.11737)
*Tunyu Zhang,Haizhou Shi,Yibin Wang,Hengyi Wang,Xiaoxiao He,Zhuowei Li,Haoxian Chen,Ligong Han,Kai Xu,Huan Zhang,Dimitris Metaxas,Hao Wang*

Main category: cs.LG

TLDR: 提出了一种基于低秩随机权重扰动的token级不确定性估计框架，用于提升LLM在数学推理中的生成质量。


<details>
  <summary>Details</summary>
Motivation: LLM的输出质量在不同场景下不一致，难以识别可信赖的响应，尤其是在需要多步推理的复杂任务中。

Method: 引入低秩随机权重扰动到LLM解码中，生成预测分布以估计token级不确定性，并聚合这些不确定性反映语义不确定性。

Result: 实验表明，token级不确定性指标与答案正确性和模型鲁棒性高度相关，且能通过多轮生成和粒子滤波算法提升推理性能。

Conclusion: 该方法优于现有不确定性估计方法，为LLM的推理生成评估和改进提供了有效工具。

Abstract: While Large Language Models (LLMs) have demonstrated impressive capabilities,
their output quality remains inconsistent across various application scenarios,
making it difficult to identify trustworthy responses, especially in complex
tasks requiring multi-step reasoning. In this paper, we propose a token-level
uncertainty estimation framework to enable LLMs to self-assess and self-improve
their generation quality in mathematical reasoning. Specifically, we introduce
low-rank random weight perturbation to LLM decoding, generating predictive
distributions that we use to estimate token-level uncertainties. We then
aggregate these uncertainties to reflect semantic uncertainty of the generated
sequences. Experiments on mathematical reasoning datasets of varying difficulty
demonstrate that our token-level uncertainty metrics strongly correlate with
answer correctness and model robustness. Additionally, we explore using
uncertainty to directly enhance the model's reasoning performance through
multiple generations and the particle filtering algorithm. Our approach
consistently outperforms existing uncertainty estimation methods, establishing
effective uncertainty estimation as a valuable tool for both evaluating and
improving reasoning generation in LLMs.

</details>

### [94] [Simple and Effective Specialized Representations for Fair Classifiers](https://arxiv.org/abs/2505.11740)
*Alberto Sinigaglia,Davide Sartor,Marina Ceccon,Gian Antonio Susto*

Main category: cs.LG

TLDR: 提出了一种基于特征函数距离的新方法，用于公平分类，解决了现有方法的不稳定性和计算复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 公平分类在高风险决策中日益重要，但现有方法如对抗学习或分布匹配存在不稳定或计算复杂的问题。

Method: 利用特征函数距离，确保学习到的表示包含最少的敏感信息，同时保持下游任务的高效性。

Result: 在基准数据集上，新方法在公平性和预测准确性上优于或匹配现有方法，且计算高效。

Conclusion: 新方法为公平分类提供了稳定、高效且实用的解决方案。

Abstract: Fair classification is a critical challenge that has gained increasing
importance due to international regulations and its growing use in high-stakes
decision-making settings. Existing methods often rely on adversarial learning
or distribution matching across sensitive groups; however, adversarial learning
can be unstable, and distribution matching can be computationally intensive. To
address these limitations, we propose a novel approach based on the
characteristic function distance. Our method ensures that the learned
representation contains minimal sensitive information while maintaining high
effectiveness for downstream tasks. By utilizing characteristic functions, we
achieve a more stable and efficient solution compared to traditional methods.
Additionally, we introduce a simple relaxation of the objective function that
guarantees fairness in common classification models with no performance
degradation. Experimental results on benchmark datasets demonstrate that our
approach consistently matches or achieves better fairness and predictive
accuracy than existing methods. Moreover, our method maintains robustness and
computational efficiency, making it a practical solution for real-world
applications.

</details>

### [95] [POCAII: Parameter Optimization with Conscious Allocation using Iterative Intelligence](https://arxiv.org/abs/2505.11745)
*Joshua Inman,Tanmay Khandait,Lalitha Sankar,Giulia Pedrielli*

Main category: cs.LG

TLDR: POCAII是一种新的超参数优化算法，通过分离搜索和评估阶段，并在两个阶段采用探索与利用原则，实现了在低预算条件下的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有超参数优化算法（如Hyperband和Successive Halving）未明确分离搜索和评估阶段，POCAII旨在解决这一问题，提升低预算条件下的优化效果。

Method: POCAII明确分离搜索和评估阶段，初期侧重搜索生成配置，后期增加评估资源，采用探索与利用原则。

Result: 在低预算条件下，POCAII表现优于SMAC、BOHB和DEHB，且结果更稳健、方差更低。

Conclusion: POCAII适用于资源有限的实际场景，尤其对训练成本高的模型具有重要价值。

Abstract: In this paper we propose for the first time the hyperparameter optimization
(HPO) algorithm POCAII. POCAII differs from the Hyperband and Successive
Halving literature by explicitly separating the search and evaluation phases
and utilizing principled approaches to exploration and exploitation principles
during both phases. Such distinction results in a highly flexible scheme for
managing a hyperparameter optimization budget by focusing on search (i.e.,
generating competing configurations) towards the start of the HPO process while
increasing the evaluation effort as the HPO comes to an end.
  POCAII was compared to state of the art approaches SMAC, BOHB and DEHB. Our
algorithm shows superior performance in low-budget hyperparameter optimization
regimes. Since many practitioners do not have exhaustive resources to assign to
HPO, it has wide applications to real-world problems. Moreover, the empirical
evidence showed how POCAII demonstrates higher robustness and lower variance in
the results. This is again very important when considering realistic scenarios
with extremely expensive models to train.

</details>

### [96] [HOME-3: High-Order Momentum Estimator with Third-Power Gradient for Convex and Smooth Nonconvex Optimization](https://arxiv.org/abs/2505.11748)
*Wei Zhang,Arif Hassan Zidan,Afrar Jahin,Yu Bao,Tianming Liu*

Main category: cs.LG

TLDR: 论文提出了一种基于高阶梯度（特别是三阶梯度）的动量方法，理论和实验证明其在优化问题中优于传统低阶动量方法。


<details>
  <summary>Details</summary>
Motivation: 探索高阶梯度在动量方法中的应用，以提升优化器的收敛性能和逃离驻点的能力。

Method: 引入高阶动量概念，重点研究三阶梯度构建的动量，并进行了理论和实验验证。

Result: 理论和实验表明，高阶动量在凸、光滑非凸及非光滑非凸优化任务中均优于传统低阶动量。

Conclusion: 高阶动量方法在多种优化问题中表现出优越性能，为优化器设计提供了新思路。

Abstract: Momentum-based gradients are essential for optimizing advanced machine
learning models, as they not only accelerate convergence but also advance
optimizers to escape stationary points. While most state-of-the-art momentum
techniques utilize lower-order gradients, such as the squared first-order
gradient, there has been limited exploration of higher-order gradients,
particularly those raised to powers greater than two. In this work, we
introduce the concept of high-order momentum, where momentum is constructed
using higher-power gradients, with a focus on the third-power of the
first-order gradient as a representative case. Our research offers both
theoretical and empirical support for this approach. Theoretically, we
demonstrate that incorporating third-power gradients can improve the
convergence bounds of gradient-based optimizers for both convex and smooth
nonconvex problems. Empirically, we validate these findings through extensive
experiments across convex, smooth nonconvex, and nonsmooth nonconvex
optimization tasks. Across all cases, high-order momentum consistently
outperforms conventional low-order momentum methods, showcasing superior
performance in various optimization problems.

</details>

### [97] [Permutation Randomization on Nonsmooth Nonconvex Optimization: A Theoretical and Experimental Study](https://arxiv.org/abs/2505.11752)
*Wei Zhang,Arif Hassan Zidan,Afrar Jahin,Yu Bao,Tianming Liu*

Main category: cs.LG

TLDR: 本文研究了梯度优化中随机化（尤其是排列随机化）的作用，揭示了其理论优势（如破坏梯度优化的收缩行为并保持收敛速度）和实际效果（在深度神经网络训练和噪声目标函数优化中表现优异）。


<details>
  <summary>Details</summary>
Motivation: 探索随机化在无维度非光滑非凸优化中的作用，填补理论空白。

Method: 通过理论分析和数值实验（如深度神经网络训练和噪声函数优化）验证排列随机化的效果。

Result: 理论证明排列随机化能破坏收缩行为并保持收敛速度；实验显示其在多种任务中优于基线方法。

Conclusion: 排列随机化在理论和实践中均有效，为更广泛的随机化研究奠定了基础。

Abstract: While gradient-based optimizers that incorporate randomization often showcase
superior performance on complex optimization, the theoretical foundations
underlying this superiority remain insufficiently understood. A particularly
pressing question has emerged: What is the role of randomization in
dimension-free nonsmooth nonconvex optimization? To address this gap, we
investigate the theoretical and empirical impact of permutation randomization
within gradient-based optimization frameworks, using it as a representative
case to explore broader implications. From a theoretical perspective, our
analyses reveal that permutation randomization disrupts the shrinkage behavior
of gradient-based optimizers, facilitating continuous convergence toward the
global optimum given a sufficiently large number of iterations. Additionally,
we prove that permutation randomization can preserve the convergence rate of
the underlying optimizer. On the empirical side, we conduct extensive numerical
experiments comparing permutation-randomized optimizer against three baseline
methods. These experiments span tasks such as training deep neural networks
with stacked architectures and optimizing noisy objective functions. The
results not only corroborate our theoretical insights but also highlight the
practical benefits of permutation randomization. In summary, this work delivers
both rigorous theoretical justification and compelling empirical evidence for
the effectiveness of permutation randomization. Our findings and evidence lay a
foundation for extending analytics to encompass a wide array of randomization.

</details>

### [98] [Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders](https://arxiv.org/abs/2505.11756)
*David Chanin,Tomáš Dulka,Adrià Garriga-Alonso*

Main category: cs.LG

TLDR: 稀疏自编码器（SAE）在特征相关且数量超过其宽度时，会合并相关特征，破坏单义性，称为特征对冲。本文研究此现象并提出改进方法。


<details>
  <summary>Details</summary>
Motivation: 探讨SAE在LLM中表现不佳的原因，发现特征对冲现象，并提出改进方案。

Method: 通过理论分析和实验验证，研究特征对冲现象，并提出改进的Matryoshka SAE变体。

Result: 特征对冲是SAE表现不佳的核心原因之一，改进方法有望提升SAE性能。

Conclusion: 特征对冲是SAE的根本问题，但通过改进有望实现其在大规模LLM解释中的潜力。

Abstract: It is assumed that sparse autoencoders (SAEs) decompose polysemantic
activations into interpretable linear directions, as long as the activations
are composed of sparse linear combinations of underlying features. However, we
find that if an SAE is more narrow than the number of underlying "true
features" on which it is trained, and there is correlation between features,
the SAE will merge components of correlated features together, thus destroying
monosemanticity. In LLM SAEs, these two conditions are almost certainly true.
This phenomenon, which we call feature hedging, is caused by SAE reconstruction
loss, and is more severe the narrower the SAE. In this work, we introduce the
problem of feature hedging and study it both theoretically in toy models and
empirically in SAEs trained on LLMs. We suspect that feature hedging may be one
of the core reasons that SAEs consistently underperform supervised baselines.
Finally, we use our understanding of feature hedging to propose an improved
variant of matryoshka SAEs. Our work shows there remain fundamental issues with
SAEs, but we are hopeful that that highlighting feature hedging will catalyze
future advances that allow SAEs to achieve their full potential of interpreting
LLMs at scale.

</details>

### [99] [Topology-Aware Knowledge Propagation in Decentralized Learning](https://arxiv.org/abs/2505.11760)
*Mansi Sakarvadia,Nathaniel Hudson,Tian Li,Ian Foster,Kyle Chard*

Main category: cs.LG

TLDR: 研究去中心化学习中OOD知识传播问题，提出拓扑感知聚合策略以提升传播效率。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习中，设备通过局部数据训练和邻居模型聚合传播知识，但现有算法在OOD知识传播上表现不佳。

Method: 分析OOD知识传播受拓扑结构和数据位置的影响，提出拓扑感知的聚合策略。

Result: 拓扑感知策略显著提升OOD数据准确性，平均提高123%。

Conclusion: 拓扑感知聚合策略能有效加速OOD知识传播，提升去中心化学习性能。

Abstract: Decentralized learning enables collaborative training of models across
naturally distributed data without centralized coordination or maintenance of a
global model. Instead, devices are organized in arbitrary communication
topologies, in which they can only communicate with neighboring devices. Each
device maintains its own local model by training on its local data and
integrating new knowledge via model aggregation with neighbors. Therefore,
knowledge is propagated across the topology via successive aggregation rounds.
We study, in particular, the propagation of out-of-distribution (OOD)
knowledge. We find that popular decentralized learning algorithms struggle to
propagate OOD knowledge effectively to all devices. Further, we find that both
the location of OOD data within a topology, and the topology itself,
significantly impact OOD knowledge propagation. We then propose topology-aware
aggregation strategies to accelerate (OOD) knowledge propagation across
devices. These strategies improve OOD data accuracy, compared to
topology-unaware baselines, by 123% on average across models in a topology.

</details>

### [100] [Redefining Neural Operators in $d+1$ Dimensions](https://arxiv.org/abs/2505.11766)
*Haoze Song,Zhihao Li,Xiaobo Zhang,Zecheng Gan,Zhilu Lai,Wei Wang*

Main category: cs.LG

TLDR: 论文提出了一种新的$d+1$维神经算子框架（SKNO），通过量子模拟PDE的突破性进展，改进了传统神经算子的演化机制，并在实验中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子在嵌入空间中的演化机制不明确，限制了其捕捉目标系统演化的能力。

Method: 基于量子模拟PDE的进展，重新定义了$d+1$维神经算子，并提出了Schrödingerised Kernel Neural Operator (SKNO)。

Result: SKNO在实验中表现优于其他方法，并在基准测试和零样本超分辨率任务中达到SOTA性能。

Conclusion: $d+1$维演化框架显著提升了神经算子的性能，验证了其与底层演化的对齐性。

Abstract: Neural Operators have emerged as powerful tools for learning mappings between
function spaces. Among them, the kernel integral operator has been widely
validated on universally approximating various operators. Although recent
advancements following this definition have developed effective modules to
better approximate the kernel function defined on the original domain (with $d$
dimensions, $d=1, 2, 3...$), the unclarified evolving mechanism in the
embedding spaces blocks our view to design neural operators that can fully
capture the target system evolution.
  Drawing on recent breakthroughs in quantum simulation of partial differential
equations (PDEs), we elucidate the linear evolution process in neural
operators. Based on that, we redefine neural operators on a new $d+1$
dimensional domain. Within this framework, we implement our proposed
Schr\"odingerised Kernel Neural Operator (SKNO) aligning better with the $d+1$
dimensional evolution. In experiments, our $d+1$ dimensional evolving linear
block performs far better than others. Also, we test SKNO's SOTA performance on
various benchmark tests and also the zero-shot super-resolution task. In
addition, we analyse the impact of different lifting and recovering operators
on the prediction within the redefined NO framework, reflecting the alignment
between our model and the underlying $d+1$ dimensional evolution.

</details>

### [101] [Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors](https://arxiv.org/abs/2505.11770)
*Jing Huang,Junyi Tao,Thomas Icard,Diyi Yang,Christopher Potts*

Main category: cs.LG

TLDR: 论文提出两种利用因果机制预测模型输出正确性的方法，并在分布内外任务中表现优于非因果方法。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络内部机制是否能用于预测模型在分布外数据上的行为。

Method: 提出两种方法：反事实模拟（检查关键因果变量是否实现）和值探测（利用变量值预测）。

Result: 两种方法在分布内外任务中均表现优异，AUC-ROC高。

Conclusion: 内部因果分析在语言模型中有重要应用价值。

Abstract: Interpretability research now offers a variety of techniques for identifying
abstract internal mechanisms in neural networks. Can such techniques be used to
predict how models will behave on out-of-distribution examples? In this work,
we provide a positive answer to this question. Through a diverse set of
language modeling tasks--including symbol manipulation, knowledge retrieval,
and instruction following--we show that the most robust features for
correctness prediction are those that play a distinctive causal role in the
model's behavior. Specifically, we propose two methods that leverage causal
mechanisms to predict the correctness of model outputs: counterfactual
simulation (checking whether key causal variables are realized) and value
probing (using the values of those variables to make predictions). Both achieve
high AUC-ROC in distribution and outperform methods that rely on
causal-agnostic features in out-of-distribution settings, where predicting
model behaviors is more crucial. Our work thus highlights a novel and
significant application for internal causal analysis of language models.

</details>

### [102] [Residual Feature Integration is Sufficient to Prevent Negative Transfer](https://arxiv.org/abs/2505.11771)
*Yichen Xu,Ryumei Nakada,Linjun Zhang,Lexin Li*

Main category: cs.LG

TLDR: REFINE方法通过结合固定源域表示和可训练目标域编码器，有效缓解负迁移问题，提升跨域任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统迁移学习中源域表示与目标域分布不匹配导致的负迁移问题。

Method: 提出REFINE方法，结合固定源域表示与可训练目标域编码器，并训练浅层神经网络适应目标域。

Result: 理论证明REFINE可避免负迁移，实验表明其在多种数据模态中性能优越。

Conclusion: REFINE是一种轻量、通用且高效的迁移学习工具。

Abstract: Transfer learning typically leverages representations learned from a source
domain to improve performance on a target task. A common approach is to extract
features from a pre-trained model and directly apply them for target
prediction. However, this strategy is prone to negative transfer where the
source representation fails to align with the target distribution. In this
article, we propose Residual Feature Integration (REFINE), a simple yet
effective method designed to mitigate negative transfer. Our approach combines
a fixed source-side representation with a trainable target-side encoder and
fits a shallow neural network on the resulting joint representation, which
adapts to the target domain while preserving transferable knowledge from the
source domain. Theoretically, we prove that REFINE is sufficient to prevent
negative transfer under mild conditions, and derive the generalization bound
demonstrating its theoretical benefit. Empirically, we show that REFINE
consistently enhances performance across diverse application and data
modalities including vision, text, and tabular data, and outperforms numerous
alternative solutions. Our method is lightweight, architecture-agnostic, and
robust, making it a valuable addition to the existing transfer learning
toolbox.

</details>

### [103] [LAMP: Extracting Locally Linear Decision Surfaces from LLM World Models](https://arxiv.org/abs/2505.11772)
*Ryan Chen,Youngmin Ko,Zeyu Zhang,Catherine Cho,Sunny Chung,Mauro Giuffré,Dennis L. Shung,Bradly C. Stadie*

Main category: cs.LG

TLDR: LAMP是一种通过局部线性模型近似语言模型决策表面的方法，用于分析模型如何将其自我解释映射到预测。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型如何可靠地将其自我解释映射到预测，揭示模型决策的依据和程度。

Method: LAMP将模型的自我解释作为坐标系，拟合局部线性替代模型，连接解释权重与模型输出。

Result: LAMP发现许多LLM具有局部线性决策表面，且与人类解释质量评估和专家判断一致。

Conclusion: LAMP是一种轻量级框架，适用于审计专有语言模型，验证模型行为与解释的一致性。

Abstract: We introduce \textbf{LAMP} (\textbf{L}inear \textbf{A}ttribution
\textbf{M}apping \textbf{P}robe), a method that shines light onto a black-box
language model's decision surface and studies how reliably a model maps its
stated reasons to its predictions through a locally linear model approximating
the decision surface. LAMP treats the model's own self-reported explanations as
a coordinate system and fits a locally linear surrogate that links those
weights to the model's output. By doing so, it reveals which stated factors
steer the model's decisions, and by how much. We apply LAMP to three tasks:
\textit{sentiment analysis}, \textit{controversial-topic detection}, and
\textit{safety-prompt auditing}. Across these tasks, LAMP reveals that many
LLMs exhibit locally linear decision landscapes. In addition, these surfaces
correlate with human judgments on explanation quality and, on a clinical
case-file data set, aligns with expert assessments. Since LAMP operates without
requiring access to model gradients, logits, or internal activations, it serves
as a practical and lightweight framework for auditing proprietary language
models, and enabling assessment of whether a model behaves consistently with
the explanations it provides.

</details>

### [104] [HARDMath2: A Benchmark for Applied Mathematics Built by Students as Part of a Graduate Class](https://arxiv.org/abs/2505.11774)
*James V. Roggeveen,Erik Y. Wang,Will Flintoft,Peter Donets,Lucy S. Nathwani,Nickholas Gutierrez,David Ettel,Anton Marius Graf,Siddharth Dandavate,Arjun Nageswaran,Raglan Ward,Ava Williamson,Anne Mykland,Kacper K. Migacz,Yijun Wang,Egemen Bostan,Duy Thuc Nguyen,Zhe He,Marc L. Descoteaux,Felix Yeung,Shida Liu,Jorge García Ponce,Luke Zhu,Yuyang Chen,Ekaterina S. Ivshina,Miguel Fernandez,Minjae Kim,Kennan Gumbs,Matthew Scott Tan,Russell Yang,Mai Hoang,David Brown,Isabella A. Silveira,Lavon Sykes,Ahmed Roman,William Fredenberg,Yiming Chen,Lucas Martin,Yixing Tang,Kelly Werker Smith,Hongyu Liao,Logan G. Wilson,Alexander Dazhen Cai,Andrea Elizabeth Biju,Michael P. Brenner*

Main category: cs.LG

TLDR: HARDMath2是一个包含211个应用数学问题的数据集，旨在评估大语言模型在近似问题上的表现，结果显示前沿模型仍存在困难。


<details>
  <summary>Details</summary>
Motivation: 填补大语言模型在应用数学近似问题评估上的空白，同时提升学生对课程内容的理解。

Method: 通过学生和教师协作设计、验证问题，并利用自动检查机制评估模型生成的解决方案。

Result: 前沿模型在HARDMath2数据集上表现不佳，学生通过交互发现了模型的常见失败模式。

Conclusion: HARDMath2不仅是一个更丰富的评估基准，还促进了学生对课程内容的深入理解。

Abstract: Large language models (LLMs) have shown remarkable progress in mathematical
problem-solving, but evaluation has largely focused on problems that have exact
analytical solutions or involve formal proofs, often overlooking
approximation-based problems ubiquitous in applied science and engineering. To
fill this gap, we build on prior work and present HARDMath2, a dataset of 211
original problems covering the core topics in an introductory graduate applied
math class, including boundary-layer analysis, WKB methods, asymptotic
solutions of nonlinear partial differential equations, and the asymptotics of
oscillatory integrals. This dataset was designed and verified by the students
and instructors of a core graduate applied mathematics course at Harvard. We
build the dataset through a novel collaborative environment that challenges
students to write and refine difficult problems consistent with the class
syllabus, peer-validate solutions, test different models, and automatically
check LLM-generated solutions against their own answers and numerical ground
truths. Evaluation results show that leading frontier models still struggle
with many of the problems in the dataset, highlighting a gap in the
mathematical reasoning skills of current LLMs. Importantly, students identified
strategies to create increasingly difficult problems by interacting with the
models and exploiting common failure modes. This back-and-forth with the models
not only resulted in a richer and more challenging benchmark but also led to
qualitative improvements in the students' understanding of the course material,
which is increasingly important as we enter an age where state-of-the-art
language models can solve many challenging problems across a wide domain of
fields.

</details>

### [105] [Generative and Contrastive Graph Representation Learning](https://arxiv.org/abs/2505.11776)
*Jiali Chen,Avijit Mukherjee*

Main category: cs.LG

TLDR: 提出了一种新的图自监督学习架构，结合对比学习和生成方法的优势，通过社区感知节点级对比学习和图级对比学习，以及综合增强策略，在多个任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有图自监督学习方法在分类和链接预测任务中表现不均衡的问题，提出结合对比学习和生成方法的优势。

Method: 引入社区感知节点级对比学习和图级对比学习，结合特征掩码、节点扰动和边扰动的综合增强策略。

Result: 在公开基准数据集上，模型在节点分类、聚类和链接预测任务中优于现有方法，性能提升0.23%-2.01%。

Conclusion: 提出的框架通过结合对比学习和生成方法的优势，实现了更鲁棒和有效的图表示学习。

Abstract: Self-supervised learning (SSL) on graphs generates node and graph
representations (i.e., embeddings) that can be used for downstream tasks such
as node classification, node clustering, and link prediction. Graph SSL is
particularly useful in scenarios with limited or no labeled data. Existing SSL
methods predominantly follow contrastive or generative paradigms, each
excelling in different tasks: contrastive methods typically perform well on
classification tasks, while generative methods often excel in link prediction.
In this paper, we present a novel architecture for graph SSL that integrates
the strengths of both approaches. Our framework introduces community-aware
node-level contrastive learning, providing more robust and effective positive
and negative node pairs generation, alongside graph-level contrastive learning
to capture global semantic information. Additionally, we employ a comprehensive
augmentation strategy that combines feature masking, node perturbation, and
edge perturbation, enabling robust and diverse representation learning. By
incorporating these enhancements, our model achieves superior performance
across multiple tasks, including node classification, clustering, and link
prediction. Evaluations on open benchmark datasets demonstrate that our model
outperforms state-of-the-art methods, achieving a performance lift of
0.23%-2.01% depending on the task and dataset.

</details>

### [106] [Multi-Order Wavelet Derivative Transform for Deep Time Series Forecasting](https://arxiv.org/abs/2505.11781)
*Ziyu Zhou,Jiaxi Hu,Qingsong Wen,James T. Kwok,Yuxuan Liang*

Main category: cs.LG

TLDR: 论文提出了一种基于小波导数变换（WDT）的多分支框架WaveTS，用于解决傅里叶变换和小波变换在时间序列预测中的局限性，实现了更高的预测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 傅里叶变换（FT）和小波变换（WT）在时间序列预测中分别存在无法捕捉多尺度时间敏感模式和对时间变化点不敏感的问题。

Method: 引入多阶小波导数变换（WDT），通过操作时间序列的导数来放大变化率信号，并结合多分支框架WaveTS进行多尺度时间-频率分解和重构。

Result: 在十个基准数据集上的实验表明，WaveTS实现了最先进的预测精度，同时保持了较高的计算效率。

Conclusion: WDT和WaveTS框架有效解决了现有方法的局限性，为时间序列预测提供了更优的解决方案。

Abstract: In deep time series forecasting, the Fourier Transform (FT) is extensively
employed for frequency representation learning. However, it often struggles in
capturing multi-scale, time-sensitive patterns. Although the Wavelet Transform
(WT) can capture these patterns through frequency decomposition, its
coefficients are insensitive to change points in time series, leading to
suboptimal modeling. To mitigate these limitations, we introduce the
multi-order Wavelet Derivative Transform (WDT) grounded in the WT, enabling the
extraction of time-aware patterns spanning both the overall trend and subtle
fluctuations. Compared with the standard FT and WT, which model the raw series,
the WDT operates on the derivative of the series, selectively magnifying
rate-of-change cues and exposing abrupt regime shifts that are particularly
informative for time series modeling. Practically, we embed the WDT into a
multi-branch framework named WaveTS, which decomposes the input series into
multi-scale time-frequency coefficients, refines them via linear layers, and
reconstructs them into the time domain via the inverse WDT. Extensive
experiments on ten benchmark datasets demonstrate that WaveTS achieves
state-of-the-art forecasting accuracy while retaining high computational
efficiency.

</details>

### [107] [Improving Coverage in Combined Prediction Sets with Weighted p-values](https://arxiv.org/abs/2505.11785)
*Gina Wong,Drew Prinster,Suchi Saria,Rama Chellappa,Anqi Liu*

Main category: cs.LG

TLDR: 提出了一种加权聚合预测集的方法，通过分配权重优化整体覆盖范围，介于$1-2\alpha$和$1-\alpha$之间，并扩展到数据依赖权重。


<details>
  <summary>Details</summary>
Motivation: 解决多预测集聚合时覆盖范围下降的问题，提升不确定性量化的精度。

Method: 提出加权聚合框架，分配权重优化覆盖范围，并扩展到数据依赖权重。

Result: 实验证明数据依赖权重聚合能提供自适应覆盖，优于传统方法。

Conclusion: 加权聚合框架有效提升预测集的覆盖精度，适用于复杂场景。

Abstract: Conformal prediction quantifies the uncertainty of machine learning models by
augmenting point predictions with valid prediction sets, assuming
exchangeability. For complex scenarios involving multiple trials, models, or
data sources, conformal prediction sets can be aggregated to create a
prediction set that captures the overall uncertainty, often improving
precision. However, aggregating multiple prediction sets with individual
$1-\alpha$ coverage inevitably weakens the overall guarantee, typically
resulting in $1-2\alpha$ worst-case coverage. In this work, we propose a
framework for the weighted aggregation of prediction sets, where weights are
assigned to each prediction set based on their contribution. Our framework
offers flexible control over how the sets are aggregated, achieving tighter
coverage bounds that interpolate between the $1-2\alpha$ guarantee of the
combined models and the $1-\alpha$ guarantee of an individual model depending
on the distribution of weights. We extend our framework to data-dependent
weights, and we derive a general procedure for data-dependent weight
aggregation that maintains finite-sample validity. We demonstrate the
effectiveness of our methods through experiments on synthetic and real data in
the mixture-of-experts setting, and we show that aggregation with
data-dependent weights provides a form of adaptive coverage.

</details>

### [108] [JULI: Jailbreak Large Language Models by Self-Introspection](https://arxiv.org/abs/2505.11790)
*Jesson Wang,Zhanhao Hu,David Wagner*

Main category: cs.LG

TLDR: 提出了一种名为JULI的方法，通过操纵令牌对数概率，利用BiasNet插件块，成功在API调用的黑盒设置下破解LLMs的安全对齐。


<details>
  <summary>Details</summary>
Motivation: 尽管已有攻击方法揭示了LLMs安全对齐的漏洞，但这些方法通常需要访问模型权重或生成过程，而专有模型通过API调用不提供此类权限。因此，需要一种无需此类权限的攻击方法。

Method: 提出JULI方法，通过操纵令牌对数概率，使用BiasNet插件块，仅需目标LLM预测的令牌对数概率知识即可实现破解。

Result: JULI在仅知道前5个令牌对数概率的黑盒设置下，成功破解API调用的LLMs，并在多个指标上优于现有最先进方法。

Conclusion: JULI是一种高效的黑盒攻击方法，无需访问模型权重或生成过程，为LLMs的安全对齐研究提供了新的视角。

Abstract: Large Language Models (LLMs) are trained with safety alignment to prevent
generating malicious content. Although some attacks have highlighted
vulnerabilities in these safety-aligned LLMs, they typically have limitations,
such as necessitating access to the model weights or the generation process.
Since proprietary models through API-calling do not grant users such
permissions, these attacks find it challenging to compromise them. In this
paper, we propose Jailbreaking Using LLM Introspection (JULI), which jailbreaks
LLMs by manipulating the token log probabilities, using a tiny plug-in block,
BiasNet. JULI relies solely on the knowledge of the target LLM's predicted
token log probabilities. It can effectively jailbreak API-calling LLMs under a
black-box setting and knowing only top-$5$ token log probabilities. Our
approach demonstrates superior effectiveness, outperforming existing
state-of-the-art (SOTA) approaches across multiple metrics.

</details>

### [109] [Diffmv: A Unified Diffusion Framework for Healthcare Predictions with Random Missing Views and View Laziness](https://arxiv.org/abs/2505.11802)
*Chuang Zhao,Hui Tang,Hongke Zhao,Xiaomeng Li*

Main category: cs.LG

TLDR: Diffmv是一个基于扩散的生成框架，旨在解决EHR数据中的随机缺失视图和视图惰性问题，通过统一框架和重新加权策略提升多视图利用效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设EHR数据视图完整且模型能充分利用每个视图，但实践中随机缺失视图和视图惰性限制了多视图利用的改进。

Method: 提出Diffmv框架，通过扩散-去噪统一框架处理随机缺失视图，并通过重新加权策略评估各视图优势以缓解视图惰性。

Result: 在多个健康预测任务中，Diffmv在三种流行数据集上表现出优越性能，包括多视图和多模态场景。

Conclusion: Diffmv通过创新方法解决了EHR数据中的多视图利用问题，显著提升了预测性能。

Abstract: Advanced healthcare predictions offer significant improvements in patient
outcomes by leveraging predictive analytics. Existing works primarily utilize
various views of Electronic Health Record (EHR) data, such as diagnoses, lab
tests, or clinical notes, for model training. These methods typically assume
the availability of complete EHR views and that the designed model could fully
leverage the potential of each view. However, in practice, random missing views
and view laziness present two significant challenges that hinder further
improvements in multi-view utilization. To address these challenges, we
introduce Diffmv, an innovative diffusion-based generative framework designed
to advance the exploitation of multiple views of EHR data. Specifically, to
address random missing views, we integrate various views of EHR data into a
unified diffusion-denoising framework, enriched with diverse contextual
conditions to facilitate progressive alignment and view transformation. To
mitigate view laziness, we propose a novel reweighting strategy that assesses
the relative advantages of each view, promoting a balanced utilization of
various data views within the model. Our proposed strategy achieves superior
performance across multiple health prediction tasks derived from three popular
datasets, including multi-view and multi-modality scenarios.

</details>

### [110] [VenusX: Unlocking Fine-Grained Functional Understanding of Proteins](https://arxiv.org/abs/2505.11812)
*Yang Tan,Wenrui Gou,Bozitao Zhong,Liang Hong,Huiqun Yu,Bingxin Zhou*

Main category: cs.LG

TLDR: VenusX是一个大规模基准测试，用于细粒度蛋白质功能注释和基于功能的蛋白质配对，涵盖残基、片段和域级别。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在蛋白质功能预测方面取得了进展，但需要更细致的视角来理解功能机制和评估模型捕获的生物学知识。

Method: VenusX包含三类任务，涉及六种注释类型，包括残基级二分类、片段级多分类和功能相似性评分。数据来自多个开源数据库，共878,000个样本。

Result: 基准测试提供了多种评估设置和基线模型性能比较，包括预训练蛋白质语言模型和结构混合方法。

Conclusion: VenusX为未来研究提供了全面的评估框架和公开可用的代码与数据。

Abstract: Deep learning models have driven significant progress in predicting protein
function and interactions at the protein level. While these advancements have
been invaluable for many biological applications such as enzyme engineering and
function annotation, a more detailed perspective is essential for understanding
protein functional mechanisms and evaluating the biological knowledge captured
by models. To address this demand, we introduce VenusX, the first large-scale
benchmark for fine-grained functional annotation and function-based protein
pairing at the residue, fragment, and domain levels. VenusX comprises three
major task categories across six types of annotations, including residue-level
binary classification, fragment-level multi-class classification, and pairwise
functional similarity scoring for identifying critical active sites, binding
sites, conserved sites, motifs, domains, and epitopes. The benchmark features
over 878,000 samples curated from major open-source databases such as InterPro,
BioLiP, and SAbDab. By providing mixed-family and cross-family splits at three
sequence identity thresholds, our benchmark enables a comprehensive assessment
of model performance on both in-distribution and out-of-distribution scenarios.
For baseline evaluation, we assess a diverse set of popular and open-source
models, including pre-trained protein language models, sequence-structure
hybrids, structure-based methods, and alignment-based techniques. Their
performance is reported across all benchmark datasets and evaluation settings
using multiple metrics, offering a thorough comparison and a strong foundation
for future research. Code and data are publicly available at
https://github.com/ai4protein/VenusX.

</details>

### [111] [Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment](https://arxiv.org/abs/2505.11821)
*Siliang Zeng,Quan Wei,William Brown,Oana Frunza,Yuriy Nevmyvaka,Mingyi Hong*

Main category: cs.LG

TLDR: 论文提出了一种基于强化学习的细粒度回合级优势估计策略，以提升大型语言模型（LLM）在多回合工具使用任务中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多回合任务中难以实现回合级的信用分配，限制了LLM代理的性能。

Method: 引入细粒度回合级优势估计策略，结合MDP框架和GRPO等强化学习算法。

Result: 实验表明，该方法在工具执行上达到100%成功率，精确答案匹配准确率提升至50%，显著优于基线。

Conclusion: 提出的策略有效提升了LLM代理在多回合复杂决策任务中的推理能力。

Abstract: This paper investigates approaches to enhance the reasoning capabilities of
Large Language Model (LLM) agents using Reinforcement Learning (RL).
Specifically, we focus on multi-turn tool-use scenarios, which can be naturally
modeled as Markov Decision Processes (MDPs). While existing approaches often
train multi-turn LLM agents with trajectory-level advantage estimation in
bandit settings, they struggle with turn-level credit assignment across
multiple decision steps, limiting their performance on multi-turn reasoning
tasks. To address this, we introduce a fine-grained turn-level advantage
estimation strategy to enable more precise credit assignment in multi-turn
agent interactions. The strategy is general and can be incorporated into
various RL algorithms such as Group Relative Preference Optimization (GRPO).
Our experimental evaluation on multi-turn reasoning and search-based tool-use
tasks with GRPO implementations highlights the effectiveness of the MDP
framework and the turn-level credit assignment in advancing the multi-turn
reasoning capabilities of LLM agents in complex decision-making settings. Our
method achieves 100% success in tool execution and 50% accuracy in exact answer
matching, significantly outperforming baselines, which fail to invoke tools and
achieve only 20-30% exact match accuracy.

</details>

### [112] [Variational Regularized Unbalanced Optimal Transport: Single Network, Least Action](https://arxiv.org/abs/2505.11823)
*Yuhao Sun,Zhenyi Zhang,Zihan Wang,Tiejun Li,Peijie Zhou*

Main category: cs.LG

TLDR: 论文提出了一种名为Var-RUOT的新框架，用于解决正则化非平衡最优输运（RUOT）问题，通过结合最优条件和损失函数设计，提高了稳定性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 从高维系统的少量快照中恢复动力学是一个具有挑战性的任务，现有方法常因未明确强制最优条件而难以满足最小作用原理和稳定收敛。

Method: 提出Var-RUOT框架，将RUOT问题的最优条件融入搜索空间参数化和损失函数设计，仅需学习标量场即可解决问题。

Result: 在模拟数据和真实单细胞数据集上验证了Var-RUOT的有效性，相比现有算法，其能找到更低作用的解且收敛更快、训练更稳定。

Conclusion: Var-RUOT通过优化条件和设计改进，显著提升了RUOT问题的求解效率和稳定性。

Abstract: Recovering the dynamics from a few snapshots of a high-dimensional system is
a challenging task in statistical physics and machine learning, with important
applications in computational biology. Many algorithms have been developed to
tackle this problem, based on frameworks such as optimal transport and the
Schr\"odinger bridge. A notable recent framework is Regularized Unbalanced
Optimal Transport (RUOT), which integrates both stochastic dynamics and
unnormalized distributions. However, since many existing methods do not
explicitly enforce optimality conditions, their solutions often struggle to
satisfy the principle of least action and meet challenges to converge in a
stable and reliable way. To address these issues, we propose Variational RUOT
(Var-RUOT), a new framework to solve the RUOT problem. By incorporating the
optimal necessary conditions for the RUOT problem into both the
parameterization of the search space and the loss function design, Var-RUOT
only needs to learn a scalar field to solve the RUOT problem and can search for
solutions with lower action. We also examined the challenge of selecting a
growth penalty function in the widely used Wasserstein-Fisher-Rao metric and
proposed a solution that better aligns with biological priors in Var-RUOT. We
validated the effectiveness of Var-RUOT on both simulated data and real
single-cell datasets. Compared with existing algorithms, Var-RUOT can find
solutions with lower action while exhibiting faster convergence and improved
training stability.

</details>

### [113] [Search-Based Correction of Reasoning Chains for Language Models](https://arxiv.org/abs/2505.11824)
*Minsu Kim,Jean-Pierre Falet,Oliver E. Richardson,Xiaoyin Chen,Moksh Jain,Sungjin Ahn,Sungsoo Ahn,Yoshua Bengio*

Main category: cs.LG

TLDR: 论文提出了一种新的自我校正框架，通过引入潜在变量标记推理步骤的真实性，结合离散搜索算法（Search Corrector）和摊销校正器（Amortized Corrector），显著提升了语言模型在逻辑和数学推理中的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决Chain-of-Thought推理中可能出现的错误陈述，提升模型的性能和可信度。

Method: 提出自我校正框架，引入潜在变量标记推理步骤的真实性；设计Search Corrector算法进行高效推理；通过伪标签训练Amortized Corrector实现零样本校正。

Result: Search Corrector在逻辑（ProntoQA）和数学（GSM8K）推理基准中可靠识别错误；Amortized Corrector零样本准确率相当，最终答案准确率提升高达25%。

Conclusion: 该框架有效提升了语言模型的推理准确性和可信度，为复杂任务中的自我校正提供了新思路。

Abstract: Chain-of-Thought (CoT) reasoning has advanced the capabilities and
transparency of language models (LMs); however, reasoning chains can contain
inaccurate statements that reduce performance and trustworthiness. To address
this, we introduce a new self-correction framework that augments each reasoning
step in a CoT with a latent variable indicating its veracity, enabling modeling
of all possible truth assignments rather than assuming correctness throughout.
To efficiently explore this expanded space, we introduce Search Corrector, a
discrete search algorithm over boolean-valued veracity assignments. It
efficiently performs otherwise intractable inference in the posterior
distribution over veracity assignments by leveraging the LM's joint likelihood
over veracity and the final answer as a proxy reward. This efficient
inference-time correction method facilitates supervised fine-tuning of an
Amortized Corrector by providing pseudo-labels for veracity. The Amortized
Corrector generalizes self-correction, enabling accurate zero-shot veracity
inference in novel contexts. Empirical results demonstrate that Search
Corrector reliably identifies errors in logical (ProntoQA) and mathematical
reasoning (GSM8K) benchmarks. The Amortized Corrector achieves comparable
zero-shot accuracy and improves final answer accuracy by up to 25%.

</details>

### [114] [SplInterp: Improving our Understanding and Training of Sparse Autoencoders](https://arxiv.org/abs/2505.11836)
*Jeremy Budd,Javier Ideami,Benjamin Macdowall Rynne,Keith Duggar,Randall Balestriero*

Main category: cs.LG

TLDR: 论文通过深度学习的分段理论分析稀疏自编码器（SAEs），发现其牺牲准确性以换取可解释性，并提出了一种新的训练算法PAM-SGD，在MNIST和LLM实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 增强对稀疏自编码器（SAEs）的理论理解，解决其在实际应用中的效用争议。

Method: 利用分段理论框架分析SAEs，提出PAM-SGD算法进行训练。

Result: SAEs在可解释性和准确性之间存在权衡，PAM-SGD算法在MNIST和LLM实验中表现出色，尤其在样本效率和稀疏性方面。

Conclusion: SAEs在理论上有局限性，但通过PAM-SGD算法可以提升其性能，为实际应用提供支持。

Abstract: Sparse autoencoders (SAEs) have received considerable recent attention as
tools for mechanistic interpretability, showing success at extracting
interpretable features even from very large LLMs. However, this research has
been largely empirical, and there have been recent doubts about the true
utility of SAEs. In this work, we seek to enhance the theoretical understanding
of SAEs, using the spline theory of deep learning. By situating SAEs in this
framework: we discover that SAEs generalise ``$k$-means autoencoders'' to be
piecewise affine, but sacrifice accuracy for interpretability vs. the optimal
``$k$-means-esque plus local principal component analysis (PCA)'' piecewise
affine autoencoder. We characterise the underlying geometry of (TopK) SAEs
using power diagrams. And we develop a novel proximal alternating method SGD
(PAM-SGD) algorithm for training SAEs, with both solid theoretical foundations
and promising empirical results in MNIST and LLM experiments, particularly in
sample efficiency and (in the LLM setting) improved sparsity of codes. All code
is available at: https://github.com/splInterp2025/splInterp

</details>

### [115] [On Membership Inference Attacks in Knowledge Distillation](https://arxiv.org/abs/2505.11837)
*Ziyao Cui,Minxing Zhang,Jian Pei*

Main category: cs.LG

TLDR: 研究探讨知识蒸馏对大型语言模型（LLM）隐私保护的影响，发现师生模型在MIA攻击下表现不同，并提出5种隐私保护蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: LLM训练数据可能包含敏感信息，隐私攻击（如MIA）威胁数据安全，知识蒸馏对隐私的影响尚未充分研究。

Method: 通过实验分析师生模型在MIA下的表现，并提出5种隐私保护蒸馏方法。

Result: 师生模型在MIA准确率上相似，但教师模型更保护成员数据，学生模型更保护非成员数据；提出的方法有效降低学生模型对MIA的脆弱性。

Conclusion: 隐私保护蒸馏方法能增强学生模型的安全性，为高效安全的模型压缩提供可靠方案。

Abstract: Nowadays, Large Language Models (LLMs) are trained on huge datasets, some
including sensitive information. This poses a serious privacy concern because
privacy attacks such as Membership Inference Attacks (MIAs) may detect this
sensitive information. While knowledge distillation compresses LLMs into
efficient, smaller student models, its impact on privacy remains underexplored.
In this paper, we investigate how knowledge distillation affects model
robustness against MIA. We focus on two questions. First, how is private data
protected in teacher and student models? Second, how can we strengthen privacy
preservation against MIAs in knowledge distillation? Through comprehensive
experiments, we show that while teacher and student models achieve similar
overall MIA accuracy, teacher models better protect member data, the primary
target of MIA, whereas student models better protect non-member data. To
address this vulnerability in student models, we propose 5 privacy-preserving
distillation methods and demonstrate that they successfully reduce student
models' vulnerability to MIA, with ensembling further stabilizing the
robustness, offering a reliable approach for distilling more secure and
efficient student models. Our implementation source code is available at
https://github.com/richardcui18/MIA_in_KD.

</details>

### [116] [On the $O(\frac{\sqrt{d}}{K^{1/4}})$ Convergence Rate of AdamW Measured by $\ell_1$ Norm](https://arxiv.org/abs/2505.11840)
*Huan Li,Yiming Dong,Zhouchen Lin*

Main category: cs.LG

TLDR: 本文分析了AdamW优化器的收敛行为，提出了基于ℓ1范数的收敛率，并与SGD的最优收敛率进行了类比。


<details>
  <summary>Details</summary>
Motivation: 尽管AdamW在训练大语言模型方面取得了显著成功，但其收敛行为的理论理解尚不充分。本文旨在填补这一理论空白。

Method: 通过理论分析和实验验证，研究了AdamW在ℓ1范数下的收敛率，并与SGD的最优收敛率进行了对比。

Result: 理论推导和实验结果表明，AdamW的收敛率与SGD的最优收敛率类似，支持了其实际有效性。

Conclusion: 本文为AdamW的收敛行为提供了理论支持，表明其在实际任务中具有与SGD类似的收敛性能。

Abstract: As the default optimizer for training large language models, AdamW has
achieved remarkable success in deep learning. However, its convergence behavior
is not theoretically well-understood. This paper establishes the convergence
rate $\frac{1}{K}\sum_{k=1}^KE\left[\|\nabla f(x^k)\|_1\right]\leq
O(\frac{\sqrt{d}C}{K^{1/4}})$ for AdamW measured by $\ell_1$ norm, where $K$
represents the iteration number, $d$ denotes the model dimension, and $C$
matches the constant in the optimal convergence rate of SGD. Theoretically, we
have $E\left[\|\nabla f(x)\|_1\right]\geq\sqrt{\frac{2d}{\pi}}E\left[\|\nabla
f(x)\|_2\right]$ when each element of $\nabla f(x)$ is generated from Gaussian
distribution $\mathcal N(0,1)$. Empirically, our experimental results on
real-world deep learning tasks reveal $\|\nabla
f(x)\|_1=\varTheta(\sqrt{d})\|\nabla f(x)\|_2$. Both support that our
convergence rate can be considered to be analogous to the optimal
$\frac{1}{K}\sum_{k=1}^KE\left[\|\nabla f(x^k)\|_2\right]\leq
O(\frac{C}{K^{1/4}})$ convergence rate of SGD.

</details>

### [117] [Learning on a Razor's Edge: the Singularity Bias of Polynomial Neural Networks](https://arxiv.org/abs/2505.11846)
*Vahid Shahverdi,Giovanni Luca Marchetti,Kathlén Kohn*

Main category: cs.LG

TLDR: 论文通过代数几何理论分析了深度神经网络中子网络及其偏置，揭示了子网络参数化的函数空间维度、奇异性及其与训练动态临界点的关系。


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络中子网络的特性及其偏置，以理解其几何结构和训练动态。

Method: 采用代数几何方法，分析全连接网络和多项式激活函数下的神经流形几何特性。

Result: 发现子网络参数化的子空间维度、奇异性，并证明奇异性常对应训练动态的临界点；卷积网络中偏置不出现。

Conclusion: 子网络的几何特性与训练动态密切相关，但卷积网络的偏置行为不同。

Abstract: Deep neural networks often infer sparse representations, converging to a
subnetwork during the learning process. In this work, we theoretically analyze
subnetworks and their bias through the lens of algebraic geometry. We consider
fully-connected networks with polynomial activation functions, and focus on the
geometry of the function space they parametrize, often referred to as
neuromanifold. First, we compute the dimension of the subspace of the
neuromanifold parametrized by subnetworks. Second, we show that this subspace
is singular. Third, we argue that such singularities often correspond to
critical points of the training dynamics. Lastly, we discuss convolutional
networks, for which subnetworks and singularities are similarly related, but
the bias does not arise.

</details>

### [118] [Bridging the Reality Gap in Digital Twins with Context-Aware, Physics-Guided Deep Learning](https://arxiv.org/abs/2505.11847)
*Sizhe Ma,Katherine A. Flanigan,Mario Bergés*

Main category: cs.LG

TLDR: 论文提出了一种名为Reality Gap Analysis (RGA)的模块，用于解决数字孪生(DTs)中现实差距的问题，通过持续校准和物理一致性保持来提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 数字孪生的可靠性受到现实差距的威胁，尤其是上下文不匹配问题，需要一种能够持续校准并保持物理一致性的方法。

Method: 提出RGA模块，结合域对抗深度学习和降阶模拟器指导，持续整合传感器数据、检测偏差并重新校准。

Result: 在匹兹堡钢桁架桥的结构健康监测案例中，RGA模块实现了更快的校准和更好的现实对齐。

Conclusion: RGA模块通过融合数据驱动和物理约束，有效缩小了数字孪生与现实系统之间的差距。

Abstract: Digital twins (DTs) enable powerful predictive analytics, but persistent
discrepancies between simulations and real systems--known as the reality
gap--undermine their reliability. Coined in robotics, the term now applies to
DTs, where discrepancies stem from context mismatches, cross-domain
interactions, and multi-scale dynamics. Among these, context mismatch is
pressing and underexplored, as DT accuracy depends on capturing operational
context, often only partially observable. However, DTs have a key advantage:
simulators can systematically vary contextual factors and explore scenarios
difficult or impossible to observe empirically, informing inference and model
alignment. While sim-to-real transfer like domain adaptation shows promise in
robotics, their application to DTs poses two key challenges. First, unlike
one-time policy transfers, DTs require continuous calibration across an asset's
lifecycle--demanding structured information flow, timely detection of
out-of-sync states, and integration of historical and new data. Second, DTs
often perform inverse modeling, inferring latent states or faults from
observations that may reflect multiple evolving contexts. These needs strain
purely data-driven models and risk violating physical consistency. Though some
approaches preserve validity via reduced-order model, most domain adaptation
techniques still lack such constraints. To address this, we propose a Reality
Gap Analysis (RGA) module for DTs that continuously integrates new sensor data,
detects misalignments, and recalibrates DTs via a query-response framework. Our
approach fuses domain-adversarial deep learning with reduced-order simulator
guidance to improve context inference and preserve physical consistency. We
illustrate the RGA module in a structural health monitoring case study on a
steel truss bridge in Pittsburgh, PA, showing faster calibration and better
real-world alignment.

</details>

### [119] [Q-Policy: Quantum-Enhanced Policy Evaluation for Scalable Reinforcement Learning](https://arxiv.org/abs/2505.11862)
*Kalyan Cherukuri,Aarav Lala,Yash Yardi*

Main category: cs.LG

TLDR: Q-Policy是一种混合量子-经典强化学习框架，利用量子计算加速策略评估和优化。


<details>
  <summary>Details</summary>
Motivation: 解决经典强化学习在可扩展性上的挑战，探索量子计算在RL中的潜力。

Method: 通过量子叠加编码价值函数，利用量子并行性同时评估多个状态-动作对，并引入量子增强的策略迭代算法。

Result: 在小规模离散控制任务中验证了技术可行性和理论正确性，展示了概念验证行为。

Conclusion: Q-Policy为未来量子设备上的可扩展RL提供了理论基础，具有超越经典方法的潜力。

Abstract: We propose Q-Policy, a hybrid quantum-classical reinforcement learning (RL)
framework that mathematically accelerates policy evaluation and optimization by
exploiting quantum computing primitives. Q-Policy encodes value functions in
quantum superposition, enabling simultaneous evaluation of multiple
state-action pairs via amplitude encoding and quantum parallelism. We introduce
a quantum-enhanced policy iteration algorithm with provable polynomial
reductions in sample complexity for the evaluation step, under standard
assumptions. To demonstrate the technical feasibility and theoretical soundness
of our approach, we validate Q-Policy on classical emulations of small discrete
control tasks. Due to current hardware and simulation limitations, our
experiments focus on showcasing proof-of-concept behavior rather than
large-scale empirical evaluation. Our results support the potential of Q-Policy
as a theoretical foundation for scalable RL on future quantum devices,
addressing RL scalability challenges beyond classical approaches.

</details>

### [120] [Learning Pareto-Optimal Rewards from Noisy Preferences: A Framework for Multi-Objective Inverse Reinforcement Learning](https://arxiv.org/abs/2505.11864)
*Kalyan Cherukuri,Aarav Lala*

Main category: cs.LG

TLDR: 论文提出了一种基于偏好的多目标逆强化学习（MO-IRL）框架，用于从人类偏好中恢复帕累托最优奖励表示，并提供了理论保证和算法。


<details>
  <summary>Details</summary>
Motivation: 生成代理的行为与复杂人类价值观的对齐是一个挑战，现有方法常将人类意图简化为标量奖励，忽略了反馈的多面性。

Method: 提出MO-IRL框架，将人类偏好建模为潜在向量值奖励函数，并推导了恢复帕累托前沿的样本复杂度边界和遗憾公式。

Result: 建立了多目标结构的识别条件，提出了收敛性可证明的策略优化算法。

Conclusion: 该研究为高维和价值多元环境中的对齐行为学习提供了理论基础。

Abstract: As generative agents become increasingly capable, alignment of their behavior
with complex human values remains a fundamental challenge. Existing approaches
often simplify human intent through reduction to a scalar reward, overlooking
the multi-faceted nature of human feedback. In this work, we introduce a
theoretical framework for preference-based Multi-Objective Inverse
Reinforcement Learning (MO-IRL), where human preferences are modeled as latent
vector-valued reward functions. We formalize the problem of recovering a
Pareto-optimal reward representation from noisy preference queries and
establish conditions for identifying the underlying multi-objective structure.
We derive tight sample complexity bounds for recovering
$\epsilon$-approximations of the Pareto front and introduce a regret
formulation to quantify suboptimality in this multi-objective setting.
Furthermore, we propose a provably convergent algorithm for policy optimization
using preference-inferred reward cones. Our results bridge the gap between
practical alignment techniques and theoretical guarantees, providing a
principled foundation for learning aligned behaviors in a high-dimension and
value-pluralistic environment.

</details>

### [121] [J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge](https://arxiv.org/abs/2505.11875)
*Chi-Min Chan,Chunpu Xu,Jiaming Ji,Zhen Ye,Pengcheng Wen,Chunyang Jiang,Yaodong Yang,Wei Xue,Sirui Han,Yike Guo*

Main category: cs.LG

TLDR: 论文提出J1-7B模型，通过改进评估方法和强化学习训练，显著提升了LLM-as-a-Judge的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法缺乏可解释性，LLM-as-a-Judge提供了一种更可扩展和可解释的监督方式，但需要进一步优化。

Method: 采用反射增强数据集进行监督微调，并通过强化学习（RL）训练，结合简单测试时间扩展（STTS）策略。

Result: J1-7B比现有最佳LLM-as-a-Judge性能提升4.8%，STTS下扩展趋势强5.1%。

Conclusion: RL训练是实现有效STTS能力的关键，反射增强数据集单独微调效果有限。

Abstract: The current focus of AI research is shifting from emphasizing model training
towards enhancing evaluation quality, a transition that is crucial for driving
further advancements in AI systems. Traditional evaluation methods typically
rely on reward models assigning scalar preference scores to outputs. Although
effective, such approaches lack interpretability, leaving users often uncertain
about why a reward model rates a particular response as high or low. The advent
of LLM-as-a-Judge provides a more scalable and interpretable method of
supervision, offering insights into the decision-making process. Moreover, with
the emergence of large reasoning models, which consume more tokens for deeper
thinking and answer refinement, scaling test-time computation in the
LLM-as-a-Judge paradigm presents an avenue for further boosting performance and
providing more interpretability through reasoning traces. In this paper, we
introduce $\textbf{J1-7B}$, which is first supervised fine-tuned on
reflection-enhanced datasets collected via rejection-sampling and subsequently
trained using Reinforcement Learning (RL) with verifiable rewards. At inference
time, we apply Simple Test-Time Scaling (STTS) strategies for additional
performance improvement. Experimental results demonstrate that $\textbf{J1-7B}$
surpasses the previous state-of-the-art LLM-as-a-Judge by $ \textbf{4.8}$\% and
exhibits a $ \textbf{5.1}$\% stronger scaling trend under STTS. Additionally,
we present three key findings: (1) Existing LLM-as-a-Judge does not inherently
exhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced
datasets continues to demonstrate similarly weak scaling behavior. (3)
Significant scaling trend emerges primarily during the RL phase, suggesting
that effective STTS capability is acquired predominantly through RL training.

</details>

### [122] [Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets](https://arxiv.org/abs/2505.12038)
*Ning Lu,Shengcai Liu,Jiahao Wu,Weiyu Chen,Zhirui Zhang,Yew-Soon Ong,Qi Wang,Ke Tang*

Main category: cs.LG

TLDR: 论文提出了一种名为Safe Delta的安全感知后训练防御方法，用于解决大语言模型（LLM）在微调服务中因用户上传数据导致的安全对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法难以平衡微调数据集的多样性与安全性，导致模型输出可能不安全。

Method: Safe Delta通过调整微调前后的参数变化（delta参数），估计安全退化，选择参数以最大化效用并限制安全损失，最后应用安全补偿向量。

Result: 在四个不同数据集上的实验表明，Safe Delta能有效保持安全性，同时不影响良性数据集的效用增益。

Conclusion: Safe Delta为解决LLM微调中的安全问题提供了一种有效且实用的方法。

Abstract: Large language models (LLMs) have shown great potential as general-purpose AI
assistants across various domains. To fully leverage this potential in specific
applications, many companies provide fine-tuning API services, enabling users
to upload their own data for LLM customization. However, fine-tuning services
introduce a new safety threat: user-uploaded data, whether harmful or benign,
can break the model's alignment, leading to unsafe outputs. Moreover, existing
defense methods struggle to address the diversity of fine-tuning datasets
(e.g., varying sizes, tasks), often sacrificing utility for safety or vice
versa. To address this issue, we propose Safe Delta, a safety-aware
post-training defense method that adjusts the delta parameters (i.e., the
parameter change before and after fine-tuning). Specifically, Safe Delta
estimates the safety degradation, selects delta parameters to maximize utility
while limiting overall safety loss, and applies a safety compensation vector to
mitigate residual safety loss. Through extensive experiments on four diverse
datasets with varying settings, our approach consistently preserves safety
while ensuring that the utility gain from benign datasets remains unaffected.

</details>

### [123] [AdaptMol: Adaptive Fusion from Sequence String to Topological Structure for Few-shot Drug Discovery](https://arxiv.org/abs/2505.11878)
*Yifan Dai,Xuanbai Ren,Tengfei Ma,Qipeng Yan,Yiping Liu,Yuansheng Liu,Xiangxiang Zeng*

Main category: cs.LG

TLDR: AdaptMol是一种结合自适应多模态融合的原型网络，用于分子表示，通过双级注意力机制动态整合全局和局部分子特征，在少样本学习中表现优异。


<details>
  <summary>Details</summary>
Motivation: 分子属性预测在药物开发中至关重要，但实验验证数据稀缺，AI驱动的研究面临挑战。少样本学习中，分子表示的质量直接影响模型性能上限。

Method: AdaptMol采用双级注意力机制，从SMILES序列和分子图中动态整合全局和局部特征。局部特征提取原子相互作用和子结构，全局特征通过SMILES序列提供整体表示。

Result: 在5-shot和10-shot设置下，AdaptMol在三个常用基准测试中表现最优，验证了多模态自适应融合的必要性。

Conclusion: AdaptMol通过可解释的方法验证了多模态融合的高效性，并强调了两种模态的重要性。

Abstract: Accurate molecular property prediction (MPP) is a critical step in modern
drug development. However, the scarcity of experimental validation data poses a
significant challenge to AI-driven research paradigms. Under few-shot learning
scenarios, the quality of molecular representations directly dictates the
theoretical upper limit of model performance. We present AdaptMol, a
prototypical network integrating Adaptive multimodal fusion for Molecular
representation. This framework employs a dual-level attention mechanism to
dynamically integrate global and local molecular features derived from two
modalities: SMILES sequences and molecular graphs. (1) At the local level,
structural features such as atomic interactions and substructures are extracted
from molecular graphs, emphasizing fine-grained topological information; (2) At
the global level, the SMILES sequence provides a holistic representation of the
molecule. To validate the necessity of multimodal adaptive fusion, we propose
an interpretable approach based on identifying molecular active substructures
to demonstrate that multimodal adaptive fusion can efficiently represent
molecules. Extensive experiments on three commonly used benchmarks under 5-shot
and 10-shot settings demonstrate that AdaptMol achieves state-of-the-art
performance in most cases. The rationale-extracted method guides the fusion of
two modalities and highlights the importance of both modalities.

</details>

### [124] [FABLE: A Localized, Targeted Adversarial Attack on Weather Forecasting Models](https://arxiv.org/abs/2505.12167)
*Yue Deng,Asadullah Hill Galib,Xin Lan,Pang-Ning Tan,Lifeng Luo*

Main category: cs.LG

TLDR: 本文提出了一种名为FABLE的新框架，用于生成针对天气预测模型的对抗性攻击，同时保持地理时空一致性。


<details>
  <summary>Details</summary>
Motivation: 深度学习天气预测模型易受对抗性攻击，现有方法难以平衡攻击效果与地理时空依赖性。

Method: 使用3D离散小波分解提取地理时空数据的变形成分，通过调节扰动幅度生成对抗性输入。

Result: 实验证明FABLE在多个真实数据集上优于基线方法。

Conclusion: FABLE能有效生成地理时空一致且忠实的对抗性攻击。

Abstract: Deep learning-based weather forecasting models have recently demonstrated
significant performance improvements over gold-standard physics-based
simulation tools. However, these models are vulnerable to adversarial attacks,
which raises concerns about their trustworthiness. In this paper, we first
investigate the feasibility of applying existing adversarial attack methods to
weather forecasting models. We argue that a successful attack should (1) not
modify significantly its original inputs, (2) be faithful, i.e., achieve the
desired forecast at targeted locations with minimal changes to non-targeted
locations, and (3) be geospatio-temporally realistic. However, balancing these
criteria is a challenge as existing methods are not designed to preserve the
geospatio-temporal dependencies of the original samples. To address this
challenge, we propose a novel framework called FABLE (Forecast Alteration By
Localized targeted advErsarial attack), which employs a 3D discrete wavelet
decomposition to extract the varying components of the geospatio-temporal data.
By regulating the magnitude of adversarial perturbations across different
components, FABLE can generate adversarial inputs that maintain
geospatio-temporal coherence while remaining faithful and closely aligned with
the original inputs. Experimental results on multiple real-world datasets
demonstrate the effectiveness of our framework over baseline methods across
various metrics.

</details>

### [125] [MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging](https://arxiv.org/abs/2505.11883)
*Zihuan Qiu,Yi Xu,Chiyuan He,Fanman Meng,Linfeng Xu,Qingbo Wu,Hongliang Li*

Main category: cs.LG

TLDR: MINGLE框架通过测试时适应和空门约束门控，解决了持续模型合并中的参数干扰和分布适应性不足问题，显著减少遗忘并提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前持续模型合并方法存在参数干扰和测试分布适应性不足的问题，导致任务遗忘和新任务适应困难。

Method: 提出MINGLE框架，利用测试样本动态指导合并过程，采用低秩专家混合结构和空门约束门控技术。

Result: 实验表明MINGLE显著减少遗忘，性能平均提升7-9%，优于现有方法。

Conclusion: MINGLE通过动态适应和约束优化，为持续模型合并提供了高效且鲁棒的解决方案。

Abstract: Continual model merging integrates independently fine-tuned models
sequentially without access to original training data, providing a scalable and
efficient solution to continual learning. However, current methods still face
critical challenges, notably parameter interference among tasks and limited
adaptability to evolving test distributions. The former causes catastrophic
forgetting of integrated tasks, while the latter hinders effective adaptation
to new tasks. To address these, we propose MINGLE, a novel framework for
test-time continual model merging, which leverages test-time adaptation using a
small set of unlabeled test samples from the current task to dynamically guide
the merging process. MINGLE employs a mixture-of-experts architecture composed
of parameter-efficient, low-rank experts, enabling efficient adaptation and
improving robustness to distribution shifts. To mitigate catastrophic
forgetting, we propose Null-Space Constrained Gating, which restricts gating
updates to subspaces orthogonal to prior task representations. This suppresses
activations on old task inputs and preserves model behavior on past tasks. To
further balance stability and adaptability, we design an Adaptive Relaxation
Strategy, which dynamically adjusts the constraint strength based on
interference signals captured during test-time adaptation. Extensive
experiments on standard continual merging benchmarks demonstrate that MINGLE
achieves robust generalization, reduces forgetting significantly, and
consistently surpasses previous state-of-the-art methods by 7-9\% on average
across diverse task orders.

</details>

### [126] [Self-Destructive Language Model](https://arxiv.org/abs/2505.12186)
*Yuhui Wang,Rongyi Zhu,Ting Wang*

Main category: cs.LG

TLDR: SEAM是一种新型防御方法，通过将LLM转化为自毁模型，使其在有害数据上性能崩溃，从而抵御微调攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法未能解决LLM对有害数据的固有可训练性，导致其易受攻击。

Method: SEAM通过新型损失函数和对抗梯度上升技术，耦合良性及有害数据的优化轨迹，实现自毁效果。

Result: SEAM在低强度攻击下表现稳健，高强度攻击下性能崩溃，使模型无法使用。

Conclusion: SEAM为LLM安全提供了一种有效的防御方案，显著提升了对抗微调攻击的鲁棒性。

Abstract: Harmful fine-tuning attacks pose a major threat to the security of large
language models (LLMs), allowing adversaries to compromise safety guardrails
with minimal harmful data. While existing defenses attempt to reinforce LLM
alignment, they fail to address models' inherent "trainability" on harmful
data, leaving them vulnerable to stronger attacks with increased learning rates
or larger harmful datasets. To overcome this critical limitation, we introduce
SEAM, a novel alignment-enhancing defense that transforms LLMs into
self-destructive models with intrinsic resilience to misalignment attempts.
Specifically, these models retain their capabilities for legitimate tasks while
exhibiting substantial performance degradation when fine-tuned on harmful data.
The protection is achieved through a novel loss function that couples the
optimization trajectories of benign and harmful data, enhanced with adversarial
gradient ascent to amplify the self-destructive effect. To enable practical
training, we develop an efficient Hessian-free gradient estimate with
theoretical error bounds. Extensive evaluation across LLMs and datasets
demonstrates that SEAM creates a no-win situation for adversaries: the
self-destructive models achieve state-of-the-art robustness against
low-intensity attacks and undergo catastrophic performance collapse under
high-intensity attacks, rendering them effectively unusable. (warning: this
paper contains potentially harmful content generated by LLMs.)

</details>

### [127] [Fast RoPE Attention: Combining the Polynomial Method and Fast Fourier Transform](https://arxiv.org/abs/2505.11892)
*Josh Alman,Zhao Song*

Main category: cs.LG

TLDR: 论文提出了一种在有限输入条件下，结合多项式方法和快速傅里叶变换的新算法，以近似线性时间计算RoPE注意力。


<details>
  <summary>Details</summary>
Motivation: RoPE位置嵌入虽然有效，但使注意力计算复杂化，传统快速算法失效，需新方法解决。

Method: 结合多项式方法和快速傅里叶变换，设计新算法。

Result: 在有限输入条件下，实现了近似线性时间的RoPE注意力计算。

Conclusion: 新算法成功克服了RoPE带来的计算复杂性，为高效注意力计算提供了可能。

Abstract: The transformer architecture has been widely applied to many machine learning
tasks. A main bottleneck in the time to perform transformer computations is a
task called attention computation. [Alman and Song, NeurIPS 2023] have shown
that in the bounded entry regime, there is an almost linear time algorithm to
approximate the attention computation. They also proved that the bounded entry
assumption is necessary for a fast algorithm assuming the popular Strong
Exponential Time Hypothesis.
  A new version of transformer which uses position embeddings has recently been
very successful. At a high level, position embedding enables the model to
capture the correlations between tokens while taking into account their
position in the sequence. Perhaps the most popular and effective version is
Rotary Position Embedding (RoPE), which was proposed by [Su, Lu, Pan, Murtadha,
Wen, and Liu, Neurocomputing 2024].
  A main downside of RoPE is that it complicates the attention computation
problem, so that previous techniques for designing almost linear time
algorithms no longer seem to work. In this paper, we show how to overcome this
issue, and give a new algorithm to compute the RoPE attention in almost linear
time in the bounded entry regime. (Again, known lower bounds imply that bounded
entries are necessary.) Our new algorithm combines two techniques in a novel
way: the polynomial method, which was used in prior fast attention algorithms,
and the Fast Fourier Transform.

</details>

### [128] [ACU: Analytic Continual Unlearning for Efficient and Exact Forgetting with Privacy Preservation](https://arxiv.org/abs/2505.12239)
*Jianheng Tang,Huiping Zhuang,Di Fang,Jiaxu Li,Feijiang Han,Yajiang Huang,Kejia Fan,Leye Wang,Zhanxing Zhu,Shanghang Zhang,Houbing Herbert Song,Yunhuai Liu*

Main category: cs.LG

TLDR: 论文提出了一种新的梯度无关方法（ACU）用于持续遗忘学习（CU），解决了现有遗忘方法在效率和模型保真度上的局限性。


<details>
  <summary>Details</summary>
Motivation: 持续学习（CL）需要适应开放世界环境，而持续遗忘（CU）则需满足隐私和安全需求。现有遗忘方法依赖梯度更新，存在效率和保真度问题。

Method: 提出梯度无关的解析持续遗忘（ACU）方法，通过最小二乘法递归推导闭式解，实现高效且精确的遗忘。

Result: 理论和实验验证表明，ACU在遗忘效果、模型保真度和系统效率上优于现有方法。

Conclusion: ACU为持续遗忘学习提供了一种高效、精确且隐私保护的解决方案。

Abstract: The development of artificial intelligence demands that models incrementally
update knowledge by Continual Learning (CL) to adapt to open-world
environments. To meet privacy and security requirements, Continual Unlearning
(CU) emerges as an important problem, aiming to sequentially forget particular
knowledge acquired during the CL phase. However, existing unlearning methods
primarily focus on single-shot joint forgetting and face significant
limitations when applied to CU. First, most existing methods require access to
the retained dataset for re-training or fine-tuning, violating the inherent
constraint in CL that historical data cannot be revisited. Second, these
methods often suffer from a poor trade-off between system efficiency and model
fidelity, making them vulnerable to being overwhelmed or degraded by
adversaries through deliberately frequent requests. In this paper, we identify
that the limitations of existing unlearning methods stem fundamentally from
their reliance on gradient-based updates. To bridge the research gap at its
root, we propose a novel gradient-free method for CU, named Analytic Continual
Unlearning (ACU), for efficient and exact forgetting with historical data
privacy preservation. In response to each unlearning request, our ACU
recursively derives an analytical (i.e., closed-form) solution in an
interpretable manner using the least squares method. Theoretical and
experimental evaluations validate the superiority of our ACU on unlearning
effectiveness, model fidelity, and system efficiency.

</details>

### [129] [AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning](https://arxiv.org/abs/2505.11896)
*Chenwei Lou,Zewei Sun,Xinnian Liang,Meng Qu,Wei Shen,Wenqi Wang,Yuntao Li,Qingping Yang,Shuangzhi Wu*

Main category: cs.LG

TLDR: AdaCoT是一种自适应链式思维框架，通过强化学习动态控制CoT触发，平衡性能与计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决传统CoT对所有查询生成冗长推理步骤导致的计算成本高和效率低的问题。

Method: 提出AdaCoT框架，将自适应推理建模为帕累托优化问题，使用PPO算法动态调整CoT触发边界，并引入选择性损失掩码（SLM）防止边界崩溃。

Result: AdaCoT显著降低简单查询的CoT触发率（低至3.18%）和平均响应令牌数（减少69.06%），同时保持复杂任务的高性能。

Conclusion: AdaCoT有效平衡推理性能与计算效率，为LLMs的推理任务提供了一种高效解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities but
often face challenges with tasks requiring sophisticated reasoning. While
Chain-of-Thought (CoT) prompting significantly enhances reasoning, it
indiscriminately generates lengthy reasoning steps for all queries, leading to
substantial computational costs and inefficiency, especially for simpler
inputs. To address this critical issue, we introduce AdaCoT (Adaptive
Chain-of-Thought), a novel framework enabling LLMs to adaptively decide when to
invoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem
that seeks to balance model performance with the costs associated with CoT
invocation (both frequency and computational overhead). We propose a
reinforcement learning (RL) based method, specifically utilizing Proximal
Policy Optimization (PPO), to dynamically control the CoT triggering decision
boundary by adjusting penalty coefficients, thereby allowing the model to
determine CoT necessity based on implicit query complexity. A key technical
contribution is Selective Loss Masking (SLM), designed to counteract decision
boundary collapse during multi-stage RL training, ensuring robust and stable
adaptive triggering. Experimental results demonstrate that AdaCoT successfully
navigates the Pareto frontier, achieving substantial reductions in CoT usage
for queries not requiring elaborate reasoning. For instance, on our production
traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\% and
decreased average response tokens by 69.06%, while maintaining high performance
on complex tasks.

</details>

### [130] [Dynamic Perturbed Adaptive Method for Infinite Task-Conflicting Time Series](https://arxiv.org/abs/2505.11902)
*Jiang You,Xiaozhen Wang,Arben Cela*

Main category: cs.LG

TLDR: 论文提出了一种动态扰动自适应方法，用于解决时间序列任务中同一输入可能产生不同输出的问题，显著优于现有静态模型。


<details>
  <summary>Details</summary>
Motivation: 研究模型在频繁任务切换下的泛化和适应能力，现有静态模型无法应对此类问题。

Method: 采用主干-分支架构，主干缓慢演化以捕捉长期结构，分支模块为每个任务重新初始化和更新，实现持续测试时适应和跨任务转移。

Result: 理论证明该架构比静态模型和LoRA具有更高的功能表达能力，实验显示在复杂冲突任务环境中显著优于基线方法。

Conclusion: 动态扰动自适应方法在快速适应和渐进学习方面表现出色，适用于多变任务环境。

Abstract: We formulate time series tasks as input-output mappings under varying
objectives, where the same input may yield different outputs. This challenges a
model's generalization and adaptability. To study this, we construct a
synthetic dataset with numerous conflicting subtasks to evaluate adaptation
under frequent task shifts. Existing static models consistently fail in such
settings. We propose a dynamic perturbed adaptive method based on a
trunk-branch architecture, where the trunk evolves slowly to capture long-term
structure, and branch modules are re-initialized and updated for each task.
This enables continual test-time adaptation and cross-task transfer without
relying on explicit task labels. Theoretically, we show that this architecture
has strictly higher functional expressivity than static models and LoRA. We
also establish exponential convergence of branch adaptation under the
Polyak-Lojasiewicz condition. Experiments demonstrate that our method
significantly outperforms competitive baselines in complex and conflicting task
environments, exhibiting fast adaptation and progressive learning capabilities.

</details>

### [131] [K*-Means: A Parameter-free Clustering Algorithm](https://arxiv.org/abs/2505.11904)
*Louis Mahon,Mirella Lapata*

Main category: cs.LG

TLDR: k*-means是一种无需预设聚类数k的新聚类算法，通过最小描述长度原则自动确定最优k*，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法需要预设k或依赖阈值，限制了其有效性。

Method: k*-means通过分裂和合并聚类，同时优化k-means目标函数，自动确定k*。

Result: 实验证明k*-means在未知k时显著优于现有方法，且运行时间具有竞争力。

Conclusion: k*-means无需参数设置，自动确定k*，性能优越，适用于实际应用。

Abstract: Clustering is a widely used and powerful machine learning technique, but its
effectiveness is often limited by the need to specify the number of clusters,
k, or by relying on thresholds that implicitly determine k. We introduce
k*-means, a novel clustering algorithm that eliminates the need to set k or any
other parameters. Instead, it uses the minimum description length principle to
automatically determine the optimal number of clusters, k*, by splitting and
merging clusters while also optimising the standard k-means objective. We prove
that k*-means is guaranteed to converge and demonstrate experimentally that it
significantly outperforms existing methods in scenarios where k is unknown. We
also show that it is accurate in estimating k, and that empirically its runtime
is competitive with existing methods, and scales well with dataset size.

</details>

### [132] [Private Statistical Estimation via Truncation](https://arxiv.org/abs/2505.12541)
*Manolis Zampetakis,Felix Zhou*

Main category: cs.LG

TLDR: 提出了一种基于数据截断的差分隐私统计估计框架，解决了数据支持无界时的挑战，适用于指数族分布，实现了近最优样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖问题特定的敏感性分析，适用性有限；无界数据支持是差分隐私估计的主要挑战。

Method: 利用截断统计技术，结合最大似然估计和差分隐私随机梯度下降，开发高效差分隐私估计器。

Result: 为高斯均值和协方差估计提供了近最优样本复杂度的差分隐私估计器，并改进了指数族对数似然函数的一致收敛保证。

Conclusion: 该框架为基于截断统计的差分隐私算法设计提供了通用蓝图。

Abstract: We introduce a novel framework for differentially private (DP) statistical
estimation via data truncation, addressing a key challenge in DP estimation
when the data support is unbounded. Traditional approaches rely on
problem-specific sensitivity analysis, limiting their applicability. By
leveraging techniques from truncated statistics, we develop computationally
efficient DP estimators for exponential family distributions, including
Gaussian mean and covariance estimation, achieving near-optimal sample
complexity. Previous works on exponential families only consider bounded or
one-dimensional families. Our approach mitigates sensitivity through truncation
while carefully correcting for the introduced bias using maximum likelihood
estimation and DP stochastic gradient descent. Along the way, we establish
improved uniform convergence guarantees for the log-likelihood function of
exponential families, which may be of independent interest. Our results provide
a general blueprint for DP algorithm design via truncated statistics.

</details>

### [133] [Modèles de Substitution pour les Modèles à base d'Agents : Enjeux, Méthodes et Applications](https://arxiv.org/abs/2505.11912)
*Paul Saves,Nicolas Verstaevel,Benoît Gaudou*

Main category: cs.LG

TLDR: 该论文探讨了代理模型在降低多代理模拟计算成本中的应用，通过机器学习技术构建高效替代模型，并强调准确性、效率和可解释性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 多代理模拟（ABM）的高计算成本限制了其在大规模模拟和实时决策中的应用，因此需要高效的替代模型来解决这一问题。

Method: 使用回归模型、神经网络、随机森林和高斯过程等机器学习技术构建代理模型，并通过案例研究验证其性能。

Result: 代理模型显著降低了计算成本，同时保持了准确性，适用于大规模参数探索和实时决策支持。

Conclusion: 代理模型为ABM提供了可扩展性和实时决策支持，未来可进一步优化其解释性和应用范围。

Abstract: Multi-agent simulations enables the modeling and analyses of the dynamic
behaviors and interactions of autonomous entities evolving in complex
environments. Agent-based models (ABM) are widely used to study emergent
phenomena arising from local interactions. However, their high computational
cost poses a significant challenge, particularly for large-scale simulations
requiring extensive parameter exploration, optimization, or uncertainty
quantification. The increasing complexity of ABM limits their feasibility for
real-time decision-making and large-scale scenario analysis. To address these
limitations, surrogate models offer an efficient alternative by learning
approximations from sparse simulation data. These models provide
cheap-to-evaluate predictions, significantly reducing computational costs while
maintaining accuracy. Various machine learning techniques, including regression
models, neural networks, random forests and Gaussian processes, have been
applied to construct robust surrogates. Moreover, uncertainty quantification
and sensitivity analysis play a crucial role in enhancing model reliability and
interpretability.
  This article explores the motivations, methods, and applications of surrogate
modeling for ABM, emphasizing the trade-offs between accuracy, computational
efficiency, and interpretability. Through a case study on a segregation model,
we highlight the challenges associated with building and validating surrogate
models, comparing different approaches and evaluating their performance.
Finally, we discuss future perspectives on integrating surrogate models within
ABM to improve scalability, explainability, and real-time decision support
across various fields such as ecology, urban planning and economics.

</details>

### [134] [Transformers as Unsupervised Learning Algorithms: A study on Gaussian Mixtures](https://arxiv.org/abs/2505.11918)
*Zhiheng Chen,Ruofan Wu,Guanhua Fang*

Main category: cs.LG

TLDR: 本文研究了Transformer在解决高斯混合模型（GMM）这一无监督学习问题中的能力，提出了TGMM框架，并验证了其优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在人工智能中表现出色，但其在无监督学习领域的潜力尚未充分探索。本文旨在填补这一空白，研究Transformer在GMM问题中的应用。

Method: 提出了TGMM框架，利用共享Transformer主干同时学习多个GMM任务，并通过统计估计视角进行分析。

Result: 实验表明，TGMM能有效克服传统方法（如EM算法或谱方法）的局限性，并对分布偏移表现出鲁棒性。理论上，证明了Transformer可以近似EM算法和谱方法的核心组件。

Conclusion: 本文通过理论和实验验证了Transformer在无监督学习中的潜力，为其作为通用工具提供了支持。

Abstract: The transformer architecture has demonstrated remarkable capabilities in
modern artificial intelligence, among which the capability of implicitly
learning an internal model during inference time is widely believed to play a
key role in the under standing of pre-trained large language models. However,
most recent works have been focusing on studying supervised learning topics
such as in-context learning, leaving the field of unsupervised learning largely
unexplored. This paper investigates the capabilities of transformers in solving
Gaussian Mixture Models (GMMs), a fundamental unsupervised learning problem
through the lens of statistical estimation. We propose a transformer-based
learning framework called TGMM that simultaneously learns to solve multiple GMM
tasks using a shared transformer backbone. The learned models are empirically
demonstrated to effectively mitigate the limitations of classical methods such
as Expectation-Maximization (EM) or spectral algorithms, at the same time
exhibit reasonable robustness to distribution shifts. Theoretically, we prove
that transformers can approximate both the EM algorithm and a core component of
spectral methods (cubic tensor power iterations). These results bridge the gap
between practical success and theoretical understanding, positioning
transformers as versatile tools for unsupervised learning.

</details>

### [135] [Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?](https://arxiv.org/abs/2505.12871)
*Zi Liang,Haibo Hu,Qingqing Ye,Yaxin Xiao,Ronghua Li*

Main category: cs.LG

TLDR: LoRA技术在微调大语言模型时效率高，但其在训练时攻击下的安全性研究不足。本文分析了LoRA的低秩结构对数据投毒和后门攻击的鲁棒性，发现其对后门攻击更鲁棒，但对无目标数据投毒更脆弱。


<details>
  <summary>Details</summary>
Motivation: 研究LoRA在训练时攻击下的安全性，填补现有研究的空白。

Method: 提出分析框架，结合神经正切核和信息理论，研究LoRA的低秩结构与攻击脆弱性的关系。

Result: LoRA对后门攻击更鲁棒，但对无目标数据投毒更脆弱。

Conclusion: LoRA的低秩结构在安全性上有优缺点，需进一步优化。

Abstract: Low rank adaptation (LoRA) has emerged as a prominent technique for
fine-tuning large language models (LLMs) thanks to its superb efficiency gains
over previous methods. While extensive studies have examined the performance
and structural properties of LoRA, its behavior upon training-time attacks
remain underexplored, posing significant security risks. In this paper, we
theoretically investigate the security implications of LoRA's low-rank
structure during fine-tuning, in the context of its robustness against data
poisoning and backdoor attacks. We propose an analytical framework that models
LoRA's training dynamics, employs the neural tangent kernel to simplify the
analysis of the training process, and applies information theory to establish
connections between LoRA's low rank structure and its vulnerability against
training-time attacks. Our analysis indicates that LoRA exhibits better
robustness to backdoor attacks than full fine-tuning, while becomes more
vulnerable to untargeted data poisoning due to its over-simplified information
geometry. Extensive experimental evaluations have corroborated our theoretical
findings.

</details>

### [136] [PyScrew: A Comprehensive Dataset Collection from Industrial Screw Driving Experiments](https://arxiv.org/abs/2505.11925)
*Nikolai West,Jochen Deuse*

Main category: cs.LG

TLDR: 论文介绍了一个工业螺丝驱动数据集集合，用于制造过程监控和质量控制研究，包含六个数据集和34,000多次操作，支持多种研究应用。


<details>
  <summary>Details</summary>
Motivation: 解决工业制造中标准化、全面数据集的稀缺问题，促进可重复研究和分析方法公平比较。

Method: 通过标准化实验设置收集数据，包括硬件规格、过程阶段和数据采集方法，提供原始数据和Python库（PyScrew）。

Result: 数据集支持异常检测、预测性维护、质量控制等研究应用，并提供了双访问路径。

Conclusion: 该数据集填补了工业自动化领域的数据空白，推动了相关研究的可重复性和公平比较。

Abstract: This paper presents a comprehensive collection of industrial screw driving
datasets designed to advance research in manufacturing process monitoring and
quality control. The collection comprises six distinct datasets with over
34,000 individual screw driving operations conducted under controlled
experimental conditions, capturing the multifaceted nature of screw driving
processes in plastic components. Each dataset systematically investigates
specific aspects: natural thread degradation patterns through repeated use
(s01), variations in surface friction conditions including contamination and
surface treatments (s02), diverse assembly faults with up to 27 error types
(s03-s04), and fabrication parameter variations in both upper and lower
workpieces through modified injection molding settings (s05-s06). We detail the
standardized experimental setup used across all datasets, including hardware
specifications, process phases, and data acquisition methods. The hierarchical
data model preserves the temporal and operational structure of screw driving
processes, facilitating both exploratory analysis and the development of
machine learning models. To maximize accessibility, we provide dual access
pathways: raw data through Zenodo with a persistent DOI, and a purpose-built
Python library (PyScrew) that offers consistent interfaces for data loading,
preprocessing, and integration with common analysis workflows. These datasets
serve diverse research applications including anomaly detection, predictive
maintenance, quality control system development, feature extraction methodology
evaluation, and classification of specific error conditions. By addressing the
scarcity of standardized, comprehensive datasets in industrial manufacturing,
this collection enables reproducible research and fair comparison of analytical
approaches in an area of growing importance for industrial automation.

</details>

### [137] [The Logical Expressiveness of Temporal GNNs via Two-Dimensional Product Logics](https://arxiv.org/abs/2505.11930)
*Marco Sälzer,Przemysław Andrzej Wałęga,Martin Lange*

Main category: cs.LG

TLDR: 本文研究了时间图神经网络（temporal GNNs）的逻辑表达能力，将其与二维乘积逻辑联系起来，揭示了不同架构的表达能力差异。


<details>
  <summary>Details</summary>
Motivation: 随着基础神经网络架构的能力逐渐被理解，研究转向结合多种架构范式的模型，尤其是时间GNNs，其结合了空间（图结构）和时间（时间演化）维度，分析难度较大。

Method: 通过将时间GNNs与二维乘积逻辑（如命题时间逻辑PTL和模态逻辑K的乘积）联系起来，分析其表达能力。

Result: 递归应用静态GNNs的时间GNNs可以表达PTL和K乘积逻辑的所有性质，而其他架构（如图与时间TGNNs和全局TGNNs）只能表达受限片段。

Conclusion: 本文首次对时间GNNs进行了逻辑刻画，并建立了其相对表达能力的新结果。

Abstract: In recent years, the expressive power of various neural architectures --
including graph neural networks (GNNs), transformers, and recurrent neural
networks -- has been characterised using tools from logic and formal language
theory. As the capabilities of basic architectures are becoming well
understood, increasing attention is turning to models that combine multiple
architectural paradigms. Among them particularly important, and challenging to
analyse, are temporal extensions of GNNs, which integrate both spatial
(graph-structure) and temporal (evolution over time) dimensions. In this paper,
we initiate the study of logical characterisation of temporal GNNs by
connecting them to two-dimensional product logics. We show that the expressive
power of temporal GNNs depends on how graph and temporal components are
combined. In particular, temporal GNNs that apply static GNNs recursively over
time can capture all properties definable in the product logic of (past)
propositional temporal logic PTL and the modal logic K. In contrast,
architectures such as graph-and-time TGNNs and global TGNNs can only express
restricted fragments of this logic, where the interaction between temporal and
spatial operators is syntactically constrained. These results yield the first
logical characterisations of temporal GNNs and establish new relative
expressiveness results for temporal GNNs.

</details>

### [138] [How can Diffusion Models Evolve into Continual Generators?](https://arxiv.org/abs/2505.11936)
*Jingren Liu,Zhong Ji,Xiangyu Chen*

Main category: cs.LG

TLDR: 论文提出了一种针对扩散模型的持续学习框架（CDG），通过分析跨任务动态性，定义了三种一致性标准（IKC、UKC、LKC），并提出了CCD框架，显著提升了生成能力的稳定性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在静态数据生成中表现优异，但在持续学习场景中面临灾难性遗忘问题，现有方法缺乏与扩散过程的理论对齐。

Method: 通过分析扩散模型的跨任务动态性，提出三种一致性标准（IKC、UKC、LKC），并设计CCD框架，通过分层损失函数实现知识保留与新能力学习。

Result: 在四个基准数据集上，CCD在持续学习任务中表现优异，显著提升了生成保真度（MF和IMF）。

Conclusion: CCD框架为扩散模型的持续学习提供了理论支持和实践方法，有效解决了灾难性遗忘问题。

Abstract: While diffusion models have achieved remarkable success in static data
generation, their deployment in streaming or continual learning (CL) scenarios
faces a major challenge: catastrophic forgetting (CF), where newly acquired
generative capabilities overwrite previously learned ones. To systematically
address this, we introduce a formal Continual Diffusion Generation (CDG)
paradigm that characterizes and redefines CL in the context of generative
diffusion models. Prior efforts often adapt heuristic strategies from continual
classification tasks but lack alignment with the underlying diffusion process.
In this work, we develop the first theoretical framework for CDG by analyzing
cross-task dynamics in diffusion-based generative modeling. Our analysis
reveals that the retention and stability of generative knowledge across tasks
are governed by three key consistency criteria: inter-task knowledge
consistency (IKC), unconditional knowledge consistency (UKC), and label
knowledge consistency (LKC). Building on these insights, we propose Continual
Consistency Diffusion (CCD), a principled framework that integrates these
consistency objectives into training via hierarchical loss terms
$\mathcal{L}_{IKC}$, $\mathcal{L}_{UKC}$, and $\mathcal{L}_{LKC}$. This
promotes effective knowledge retention while enabling the assimilation of new
generative capabilities. Extensive experiments on four benchmark datasets
demonstrate that CCD achieves state-of-the-art performance under continual
settings, with substantial gains in Mean Fidelity (MF) and Incremental Mean
Fidelity (IMF), particularly in tasks with rich cross-task knowledge overlap.

</details>

### [139] [FlowPure: Continuous Normalizing Flows for Adversarial Purification](https://arxiv.org/abs/2505.13280)
*Elias Collaert,Abel Rodríguez,Sander Joos,Lieven Desmet,Vera Rimmer*

Main category: cs.LG

TLDR: FlowPure是一种基于连续归一化流（CNFs）的新型对抗净化方法，通过条件流匹配（CFM）训练，优于现有方法，并在对抗检测中表现出色。


<details>
  <summary>Details</summary>
Motivation: 对抗鲁棒性仍是机器学习系统的关键挑战，现有扩散模型方法依赖固定噪声过程，限制了灵活性。

Method: FlowPure利用CNFs和CFM训练，从对抗样本映射到干净样本，支持特定攻击知识和通用高斯扰动变体。

Result: 在CIFAR-10和CIFAR-100上，FlowPure在预处理盲盒和白盒场景中优于现有方法，且保持良性准确率。

Conclusion: FlowPure不仅是高效净化器，还具有强对抗检测潜力。

Abstract: Despite significant advancements in the area, adversarial robustness remains
a critical challenge in systems employing machine learning models. The removal
of adversarial perturbations at inference time, known as adversarial
purification, has emerged as a promising defense strategy. To achieve this,
state-of-the-art methods leverage diffusion models that inject Gaussian noise
during a forward process to dilute adversarial perturbations, followed by a
denoising step to restore clean samples before classification. In this work, we
propose FlowPure, a novel purification method based on Continuous Normalizing
Flows (CNFs) trained with Conditional Flow Matching (CFM) to learn mappings
from adversarial examples to their clean counterparts. Unlike prior
diffusion-based approaches that rely on fixed noise processes, FlowPure can
leverage specific attack knowledge to improve robustness under known threats,
while also supporting a more general stochastic variant trained on Gaussian
perturbations for settings where such knowledge is unavailable. Experiments on
CIFAR-10 and CIFAR-100 demonstrate that our method outperforms state-of-the-art
purification-based defenses in preprocessor-blind and white-box scenarios, and
can do so while fully preserving benign accuracy in the former. Moreover, our
results show that not only is FlowPure a highly effective purifier but it also
holds a strong potential for adversarial detection, identifying
preprocessor-blind PGD samples with near-perfect accuracy.

</details>

### [140] [Exploring Criteria of Loss Reweighting to Enhance LLM Unlearning](https://arxiv.org/abs/2505.11953)
*Puning Yang,Qizhou Wang,Zhuo Huang,Tongliang Liu,Chengqi Zhang,Bo Han*

Main category: cs.LG

TLDR: 本文研究了损失重加权在大型语言模型（LLM）遗忘学习中的两种目标：饱和度和重要性，并提出了一种结合两者优势的新方法SatImp。


<details>
  <summary>Details</summary>
Motivation: 损失重加权在LLM遗忘学习中效果显著，但其具体功能和最优策略尚不明确，阻碍了方法的理解和改进。

Method: 设计了针对饱和度和重要性的特定重加权策略，并通过实验评估其效果。

Result: 饱和度比重重要性更有效，两者结合可进一步提升效果；权重分布的平滑性和粒度对遗忘效果有重要影响。

Conclusion: 提出的SatImp方法结合了饱和度和重要性的优势，实验验证了其有效性，为未来研究提供了方向。

Abstract: Loss reweighting has shown significant benefits for machine unlearning with
large language models (LLMs). However, their exact functionalities are left
unclear and the optimal strategy remains an open question, thus impeding the
understanding and improvement of existing methodologies. In this paper, we
identify two distinct goals of loss reweighting, namely, Saturation and
Importance -- the former indicates that those insufficiently optimized data
should be emphasized, while the latter stresses some critical data that are
most influential for loss minimization. To study their usefulness, we design
specific reweighting strategies for each goal and evaluate their respective
effects on unlearning. We conduct extensive empirical analyses on
well-established benchmarks, and summarize some important observations as
follows: (i) Saturation enhances efficacy more than importance-based
reweighting, and their combination can yield additional improvements. (ii)
Saturation typically allocates lower weights to data with lower likelihoods,
whereas importance-based reweighting does the opposite. (iii) The efficacy of
unlearning is also largely influenced by the smoothness and granularity of the
weight distributions. Based on these findings, we propose SatImp, a simple
reweighting method that combines the advantages of both saturation and
importance. Empirical results on extensive datasets validate the efficacy of
our method, potentially bridging existing research gaps and indicating
directions for future research. Our code is available at
https://github.com/Puning97/SatImp-for-LLM-Unlearning.

</details>

### [141] [Accelerating Neural Network Training Along Sharp and Flat Directions](https://arxiv.org/abs/2505.11972)
*Daniyar Zakarin,Sidak Pal Singh*

Main category: cs.LG

TLDR: 论文研究了Bulk-SGD（一种限制更新在Hessian主空间正交补空间上的SGD变体），分析了其稳定性，并提出了插值梯度方法以平衡收敛速度与稳定性。


<details>
  <summary>Details</summary>
Motivation: 探索Hessian谱中平坦和尖锐方向的不同作用，以及梯度与Hessian主空间的对齐现象。

Method: 通过Bulk-SGD的消融研究，分析其稳定性，并引入插值梯度方法统一SGD、Dom-SGD和Bulk-SGD。

Result: 发现沿Bulk子空间的更新可加速收敛但可能影响稳定性，且曲率能量主要集中在主空间。

Conclusion: 提出了一种基于子空间分解的曲率感知优化器设计方法。

Abstract: Recent work has highlighted a surprising alignment between gradients and the
top eigenspace of the Hessian -- termed the Dominant subspace -- during neural
network training. Concurrently, there has been growing interest in the distinct
roles of sharp and flat directions in the Hessian spectrum. In this work, we
study Bulk-SGD, a variant of SGD that restricts updates to the orthogonal
complement of the Dominant subspace. Through ablation studies, we characterize
the stability properties of Bulk-SGD and identify critical hyperparameters that
govern its behavior. We show that updates along the Bulk subspace,
corresponding to flatter directions in the loss landscape, can accelerate
convergence but may compromise stability. To balance these effects, we
introduce interpolated gradient methods that unify SGD, Dom-SGD, and Bulk-SGD.
Finally, we empirically connect this subspace decomposition to the Generalized
Gauss-Newton and Functional Hessian terms, showing that curvature energy is
largely concentrated in the Dominant subspace. Our findings suggest a
principled approach to designing curvature-aware optimizers.

</details>

### [142] [FedHQ: Hybrid Runtime Quantization for Federated Learning](https://arxiv.org/abs/2505.11982)
*Zihao Zheng,Ziyao Wang,Xiuping Cui,Maoliang Li,Jiayu Chen,Yun,Liang,Ang Li,Xiang Chen*

Main category: cs.LG

TLDR: 本文提出了一种结合PTQ和QAT的混合量化方法FedHQ，用于提升联邦学习的效率与准确性。通过硬件和数据分布分析优化策略选择，实验显示其显著提升了训练速度和精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在保护数据隐私的同时效率较低，现有量化方法仅关注PTQ或QAT，无法兼顾速度与准确性。

Method: 提出混合量化方法FedHQ，结合PTQ和QAT，通过硬件和数据分布分析优化策略选择，并采用粗粒度全局初始化和细粒度ML调整。

Result: 实验表明，FedHQ实现了2.47倍的训练加速和11.15%的精度提升，额外开销可忽略。

Conclusion: FedHQ通过混合量化策略有效提升了FL的效率与准确性，为异构设备和数据分布提供了解决方案。

Abstract: Federated Learning (FL) is a decentralized model training approach that
preserves data privacy but struggles with low efficiency. Quantization, a
powerful training optimization technique, has been widely explored for
integration into FL. However, many studies fail to consider the distinct
performance attribution between particular quantization strategies, such as
post-training quantization (PTQ) or quantization-aware training (QAT). As a
result, existing FL quantization methods rely solely on either PTQ or QAT,
optimizing for speed or accuracy while compromising the other. To efficiently
accelerate FL and maintain distributed convergence accuracy across various FL
settings, this paper proposes a hybrid quantitation approach combining PTQ and
QAT for FL systems. We conduct case studies to validate the effectiveness of
using hybrid quantization in FL. To solve the difficulty of modeling speed and
accuracy caused by device and data heterogeneity, we propose a hardware-related
analysis and data-distribution-related analysis to help identify the trade-off
boundaries for strategy selection. Based on these, we proposed a novel
framework named FedHQ to automatically adopt optimal hybrid strategy allocation
for FL systems. Specifically, FedHQ develops a coarse-grained global
initialization and fine-grained ML-based adjustment to ensure efficiency and
robustness. Experiments show that FedHQ achieves up to 2.47x times training
acceleration and up to 11.15% accuracy improvement and negligible extra
overhead.

</details>

### [143] [Variance-Optimal Arm Selection: Regret Minimization and Best Arm Identification](https://arxiv.org/abs/2505.11985)
*Sabrina Khurshid,Gourab Ghatak,Mohammad Shahid Abdulla*

Main category: cs.LG

TLDR: 论文提出两种算法（UCB-VV和SHVV）用于选择方差最大的臂，分别在遗憾设置和固定预算设置中表现优异，并扩展到次高斯分布。


<details>
  <summary>Details</summary>
Motivation: 研究如何高效地从多个独立臂中选择方差最大的臂，以优化决策过程。

Method: 开发UCB-VV算法（遗憾设置）和SHVV算法（固定预算设置），并扩展到次高斯分布。

Result: UCB-VV的遗憾上界为O(log n)，SHVV的错误概率上界为exp(-n/(log(K)H))，均达到理论最优。

Conclusion: UCB-VV和SHVV在理论和实验中均表现优异，适用于金融等领域的决策问题。

Abstract: This paper focuses on selecting the arm with the highest variance from a set
of $K$ independent arms. Specifically, we focus on two settings: (i) regret
setting, that penalizes the number of pulls of suboptimal arms in terms of
variance, and (ii) fixed-budget \ac{BAI} setting, that evaluates the ability of
an algorithm to determine the arm with the highest variance after a fixed
number of pulls. We develop a novel online algorithm called \texttt{UCB-VV} for
the regret setting and show that its upper bound on regret for bounded rewards
evolves as $\mathcal{O}\left(\log{n}\right)$ where $n$ is the horizon. By
deriving the lower bound on the regret, we show that \texttt{UCB-VV} is order
optimal. For the fixed budget \ac{BAI} setting and propose the \texttt{SHVV}
algorithm. We show that the upper bound of the error probability of
\texttt{SHVV} evolves as $\exp\left(-\frac{n}{\log(K) H}\right)$, where $H$
represents the complexity of the problem, and this rate matches the
corresponding lower bound. We extend the framework from bounded distributions
to sub-Gaussian distributions using a novel concentration inequality on the
sample variance. Leveraging the same, we derive a concentration inequality for
the empirical Sharpe ratio (SR) for sub-Gaussian distributions, which was
previously unknown in the literature. Empirical simulations show that
\texttt{UCB-VV} consistently outperforms \texttt{$\epsilon$-greedy} across
different sub-optimality gaps though it is surpassed by \texttt{VTS}, which
exhibits the lowest regret, albeit lacking in theoretical guarantees. We also
illustrate the superior performance of \texttt{SHVV}, for a fixed budget
setting under 6 different setups against uniform sampling. Finally, we conduct
a case study to empirically evaluate the performance of the \texttt{UCB-VV} and
\texttt{SHVV} in call option trading on $100$ stocks generated using \ac{GBM}.

</details>

### [144] [Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation](https://arxiv.org/abs/2505.11998)
*Prashant Shivaram Bhat,Shakib Yazdani,Elahe Arani,Bahram Zonooz*

Main category: cs.LG

TLDR: PEARL是一种无需重放的持续学习框架，通过动态分配LoRA组件的秩来解决灾难性遗忘问题，并在多种视觉架构和场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 灾难性遗忘是深度神经网络在持续学习中的主要挑战，现有参数高效微调技术对秩选择敏感，导致资源分配和性能问题。

Method: PEARL利用参考任务权重动态分配LoRA组件的秩，根据当前任务与参考任务的参数空间接近度自适应调整。

Result: PEARL在三种视觉架构和多种持续学习场景中显著优于基线方法。

Conclusion: PEARL通过动态秩分配有效解决了灾难性遗忘问题，提升了持续学习的性能。

Abstract: Catastrophic forgetting has remained a critical challenge for deep neural
networks in Continual Learning (CL) as it undermines consolidated knowledge
when learning new tasks. Parameter efficient fine tuning CL techniques are
gaining traction for their effectiveness in addressing catastrophic forgetting
with a lightweight training schedule while avoiding degradation of consolidated
knowledge in pre-trained models. However, low rank adapters (LoRA) in these
approaches are highly sensitive to rank selection which can lead to sub-optimal
resource allocation and performance. To this end, we introduce PEARL, a
rehearsal-free CL framework that entails dynamic rank allocation for LoRA
components during CL training. Specifically, PEARL leverages reference task
weights and adaptively determines the rank of task-specific LoRA components
based on the current tasks' proximity to reference task weights in parameter
space. To demonstrate the versatility of PEARL, we evaluate it across three
vision architectures (ResNet, Separable Convolutional Network and Vision
Transformer) and a multitude of CL scenarios, and show that PEARL outperforms
all considered baselines by a large margin.

</details>

### [145] [Approximation theory for 1-Lipschitz ResNets](https://arxiv.org/abs/2505.12003)
*Davide Murari,Takashi Furuya,Carola-Bibiane Schönlieb*

Main category: cs.LG

TLDR: 本文研究了1-Lipschitz残差网络的逼近能力，证明了其在紧凑域上对1-Lipschitz函数的通用逼近性，并展示了其实际训练的可行性。


<details>
  <summary>Details</summary>
Motivation: 1-Lipschitz神经网络在生成建模、逆问题和鲁棒分类器中具有重要作用，但对其逼近能力的理论分析尚不完善。

Method: 基于负梯度流的显式欧拉步构建1-Lipschitz残差网络，利用Restricted Stone-Weierstrass定理分析其逼近能力。

Result: 证明了网络在宽度和深度增加时对1-Lipschitz函数的通用逼近性，以及在固定宽度下通过插入范数约束线性映射实现相同效果。

Conclusion: 本文首次为1-Lipschitz残差网络提供了通用逼近保证，为其实际应用奠定了理论基础。

Abstract: 1-Lipschitz neural networks are fundamental for generative modelling, inverse
problems, and robust classifiers. In this paper, we focus on 1-Lipschitz
residual networks (ResNets) based on explicit Euler steps of negative gradient
flows and study their approximation capabilities. Leveraging the Restricted
Stone-Weierstrass Theorem, we first show that these 1-Lipschitz ResNets are
dense in the set of scalar 1-Lipschitz functions on any compact domain when
width and depth are allowed to grow. We also show that these networks can
exactly represent scalar piecewise affine 1-Lipschitz functions. We then prove
a stronger statement: by inserting norm-constrained linear maps between the
residual blocks, the same density holds when the hidden width is fixed. Because
every layer obeys simple norm constraints, the resulting models can be trained
with off-the-shelf optimisers. This paper provides the first universal
approximation guarantees for 1-Lipschitz ResNets, laying a rigorous foundation
for their practical use.

</details>

### [146] [GeoMaNO: Geometric Mamba Neural Operator for Partial Differential Equations](https://arxiv.org/abs/2505.12020)
*Xi Han,Jingwei Zhang,Dimitris Samaras,Fei Hou,Hong Qin*

Main category: cs.LG

TLDR: GeoMaNO框架通过结合Mamba的建模能力和几何严谨性，解决了现有基于Transformer的神经算子在复杂度和性能上的不足，显著提升了PDE求解的精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的神经算子存在二次复杂度、缺乏几何严谨性等问题，导致在规则网格上性能不佳。

Method: 提出GeoMaNO框架，结合Mamba的建模能力、线性复杂度和几何严谨性。

Result: 在多个PDE基准测试中，GeoMaNO将解算子逼近的精度提升高达58.9%。

Conclusion: GeoMaNO框架显著优于现有基线，为PDE求解提供了更高效和精确的工具。

Abstract: The neural operator (NO) framework has emerged as a powerful tool for solving
partial differential equations (PDEs). Recent NOs are dominated by the
Transformer architecture, which offers NOs the capability to capture long-range
dependencies in PDE dynamics. However, existing Transformer-based NOs suffer
from quadratic complexity, lack geometric rigor, and thus suffer from
sub-optimal performance on regular grids. As a remedy, we propose the Geometric
Mamba Neural Operator (GeoMaNO) framework, which empowers NOs with Mamba's
modeling capability, linear complexity, plus geometric rigor. We evaluate
GeoMaNO's performance on multiple standard and popularly employed PDE
benchmarks, spanning from Darcy flow problems to Navier-Stokes problems.
GeoMaNO improves existing baselines in solution operator approximation by as
much as 58.9%.

</details>

### [147] [Spotlight Your Instructions: Instruction-following with Dynamic Attention Steering](https://arxiv.org/abs/2505.12025)
*Praveen Venkateswaran,Danish Contractor*

Main category: cs.LG

TLDR: 提出了一种动态调整模型注意力权重的推理时方法，以提升大型语言模型对复杂指令的遵循能力。


<details>
  <summary>Details</summary>
Motivation: 用户依赖自然语言指令指导大型语言模型完成任务，但模型对指令的关注不稳定，且缺乏简单机制强调指令重要性。

Method: 通过动态调整模型对用户指定部分的注意力权重，使其与用户意图对齐。

Result: 方法在涉及多指令的任务中显著提升指令遵循能力，且适用于不同规模的模型。

Conclusion: 该方法有效解决了指令关注不稳定的问题，无需性能损失即可提升模型表现。

Abstract: In many real-world applications, users rely on natural language instructions
to guide large language models (LLMs) across a wide range of tasks. These
instructions are often complex, diverse, and subject to frequent change.
However, LLMs do not always attend to these instructions reliably, and users
lack simple mechanisms to emphasize their importance beyond modifying prompt
wording or structure. To address this, we present an inference-time method that
enables users to emphasize specific parts of their prompt by steering the
model's attention toward them, aligning the model's perceived importance of
different prompt tokens with user intent. Unlike prior approaches that are
limited to static instructions, require significant offline profiling, or rely
on fixed biases, we dynamically update the proportion of model attention given
to the user-specified parts--ensuring improved instruction following without
performance degradation. We demonstrate that our approach improves instruction
following across a variety of tasks involving multiple instructions and
generalizes across models of varying scales.

</details>

### [148] [Relation-Aware Graph Foundation Model](https://arxiv.org/abs/2505.12027)
*Jianxiang Yu,Jiapeng Zhu,Hao Qian,Ziqi Liu,Zhiqiang Zhang,Xiang Li*

Main category: cs.LG

TLDR: REEF提出了一种基于关系令牌的图基础模型框架，通过关系词汇表和超网络设计，解决了图学习中缺乏明确泛化单元的问题，并在预训练和迁移学习中表现出色。


<details>
  <summary>Details</summary>
Motivation: 图基础模型缺乏明确的泛化单元，难以设计有效的预训练策略，而语言模型通过令牌表示实现了泛化。

Method: REEF利用关系令牌作为基本单元，构建关系词汇表，并通过超网络动态生成聚合器和分类器参数，结合数据增强和混合数据集预训练策略。

Result: REEF在预训练和迁移学习任务中显著优于现有方法。

Conclusion: REEF是一种强大的图基础模型框架，具有广泛的应用潜力。

Abstract: In recent years, large language models (LLMs) have demonstrated remarkable
generalization capabilities across various natural language processing (NLP)
tasks. Similarly, graph foundation models (GFMs) have emerged as a promising
direction in graph learning, aiming to generalize across diverse datasets
through large-scale pre-training. However, unlike language models that rely on
explicit token representations, graphs lack a well-defined unit for
generalization, making it challenging to design effective pre-training
strategies. In this work, we propose REEF, a novel framework that leverages
relation tokens as the basic units for GFMs. Inspired by the token vocabulary
in LLMs, we construct a relation vocabulary of relation tokens to store
relational information within graphs. To accommodate diverse relations, we
introduce two hypernetworks that adaptively generate the parameters of
aggregators and classifiers in graph neural networks based on relation tokens.
In addition, we design another hypernetwork to construct dataset-specific
projectors and incorporate a dataset-level feature bias into the initial node
representations, enhancing flexibility across different datasets with the same
relation. Further, we adopt graph data augmentation and a mixed-dataset
pre-training strategy, allowing REEF to capture relational diversity more
effectively and exhibit strong generalization capabilities. Extensive
experiments show that REEF significantly outperforms existing methods on both
pre-training and transfer learning tasks, underscoring its potential as a
powerful foundation model for graph-based applications.

</details>

### [149] [Adaptive Resolving Methods for Reinforcement Learning with Function Approximations](https://arxiv.org/abs/2505.12037)
*Jiashuo Jiang,Yiming Zong,Yinyu Ye*

Main category: cs.LG

TLDR: 本文提出了一种基于线性规划（LP）的新型强化学习算法，通过迭代优化LP实现高效的样本复杂度保证。


<details>
  <summary>Details</summary>
Motivation: 解决大规模或无限状态-动作空间的强化学习问题，并通过函数逼近优化决策策略。

Method: 基于LP重构，每次迭代利用新数据改进LP求解，实现实例依赖的样本复杂度保证。

Result: 算法输出具有实例依赖的$\tilde{O}(1/N)$次优性差距，优于文献中的$O(1/\sqrt{N})$最坏情况保证。

Conclusion: 新算法在理论分析和数值实验中均表现出高效性能，适用于有利实例。

Abstract: Reinforcement learning (RL) problems are fundamental in online
decision-making and have been instrumental in finding an optimal policy for
Markov decision processes (MDPs). Function approximations are usually deployed
to handle large or infinite state-action space. In our work, we consider the RL
problems with function approximation and we develop a new algorithm to solve it
efficiently. Our algorithm is based on the linear programming (LP)
reformulation and it resolves the LP at each iteration improved with new data
arrival. Such a resolving scheme enables our algorithm to achieve an
instance-dependent sample complexity guarantee, more precisely, when we have
$N$ data, the output of our algorithm enjoys an instance-dependent
$\tilde{O}(1/N)$ suboptimality gap. In comparison to the $O(1/\sqrt{N})$
worst-case guarantee established in the previous literature, our
instance-dependent guarantee is tighter when the underlying instance is
favorable, and the numerical experiments also reveal the efficient empirical
performances of our algorithms.

</details>

### [150] [Improving regional weather forecasts with neural interpolation](https://arxiv.org/abs/2505.12040)
*James Jackaman,Oliver Sutton*

Main category: cs.LG

TLDR: 设计神经插值算子改进区域天气模型的边界数据，结合图像超分辨率和卷积神经网络技术。


<details>
  <summary>Details</summary>
Motivation: 解决多尺度动态映射问题，提升区域天气模型的边界数据质量。

Method: 结合图像超分辨率和卷积神经网络（CNNs）及残差网络，融入大气动态流。

Result: 提出了一种适用于简化模型的方法，可推广至区域天气模型的核心动态。

Conclusion: 该方法为改进天气模型边界数据提供了新思路，具有潜在应用价值。

Abstract: In this paper we design a neural interpolation operator to improve the
boundary data for regional weather models, which is a challenging problem as we
are required to map multi-scale dynamics between grid resolutions. In
particular, we expose a methodology for approaching the problem through the
study of a simplified model, with a view to generalise the results in this work
to the dynamical core of regional weather models. Our approach will exploit a
combination of techniques from image super-resolution with convolutional neural
networks (CNNs) and residual networks, in addition to building the flow of
atmospheric dynamics into the neural network

</details>

### [151] [FlashBias: Fast Computation of Attention with Bias](https://arxiv.org/abs/2505.12044)
*Haixu Wu,Minghao Guo,Yuezhou Ma,Yuanxu Sun,Jianmin Wang,Wojciech Matusik,Mingsheng Long*

Main category: cs.LG

TLDR: FlashBias提出了一种基于低秩压缩感知理论的方法，优化了带有偏置的注意力机制的计算效率，显著提升了AlphaFold及视觉与语言模型的运行速度。


<details>
  <summary>Details</summary>
Motivation: 尽管带有偏置的注意力机制在多个领域中被广泛使用，但其计算效率问题尚未得到针对性优化，限制了其在复杂任务中的应用。

Method: 基于FlashAttention的理论分析，提出FlashBias方法，利用低秩压缩感知理论，实现对常见偏置的快速精确计算及一般形式偏置的快速近似。

Result: FlashBias在AlphaFold中实现了1.5倍的速度提升，在视觉与语言模型中实现了超过2倍的速度提升，且无精度损失。

Conclusion: FlashBias为带有偏置的注意力机制提供了高效的优化方案，显著提升了其在实际应用中的性能。

Abstract: Attention mechanism has emerged as a foundation module of modern deep
learning models and has also empowered many milestones in various domains.
Moreover, FlashAttention with IO-aware speedup resolves the efficiency issue of
standard attention, further promoting its practicality. Beyond canonical
attention, attention with bias also widely exists, such as relative position
bias in vision and language models and pair representation bias in AlphaFold.
In these works, prior knowledge is introduced as an additive bias term of
attention weights to guide the learning process, which has been proven
essential for model performance. Surprisingly, despite the common usage of
attention with bias, its targeted efficiency optimization is still absent,
which seriously hinders its wide applications in complex tasks. Diving into the
computation of FlashAttention, we prove that its optimal efficiency is
determined by the rank of the attention weight matrix. Inspired by this
theoretical result, this paper presents FlashBias based on the low-rank
compressed sensing theory, which can provide fast-exact computation for many
widely used attention biases and a fast-accurate approximation for biases in
general formalization. FlashBias can fully take advantage of the extremely
optimized matrix multiplication operation in modern GPUs, achieving 1.5$\times$
speedup for AlphaFold, and over 2$\times$ speedup for attention with bias in
vision and language models without loss of accuracy.

</details>

### [152] [Unsupervised Port Berth Identification from Automatic Identification System Data](https://arxiv.org/abs/2505.12046)
*Andreas Hadjipieris,Neofytos Dimitriou,Ognjen Arandjelović*

Main category: cs.LG

TLDR: 提出了一种基于AIS数据的无监督空间建模方法，用于港口泊位定位，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有港口泊位文档不完整或不准确，需数据驱动方法改进。

Method: 利用AIS数据聚类和超参数优化进行无监督建模。

Result: 模型在多个港口表现优异，平均Bhattacharyya距离为0.85，优于现有方法的13.56。

Conclusion: 该方法能更精确地定位泊位边界，适用于多样化港口环境。

Abstract: Port berthing sites are regions of high interest for monitoring and
optimizing port operations. Data sourced from the Automatic Identification
System (AIS) can be superimposed on berths enabling their real-time monitoring
and revealing long-term utilization patterns. Ultimately, insights from
multiple berths can uncover bottlenecks, and lead to the optimization of the
underlying supply chain of the port and beyond. However, publicly available
documentation of port berths, even when available, is frequently incomplete -
e.g. there may be missing berths or inaccuracies such as incorrect boundary
boxes - necessitating a more robust, data-driven approach to port berth
localization. In this context, we propose an unsupervised spatial modeling
method that leverages AIS data clustering and hyperparameter optimization to
identify berthing sites. Trained on one month of freely available AIS data and
evaluated across ports of varying sizes, our models significantly outperform
competing methods, achieving a mean Bhattacharyya distance of 0.85 when
comparing Gaussian Mixture Models (GMMs) trained on separate data splits,
compared to 13.56 for the best existing method. Qualitative comparison with
satellite images and existing berth labels further supports the superiority of
our method, revealing more precise berth boundaries and improved spatial
resolution across diverse port environments.

</details>

### [153] [Beyond Scalar Rewards: An Axiomatic Framework for Lexicographic MDPs](https://arxiv.org/abs/2505.12049)
*Mehran Shakerinava,Siamak Ravanbakhsh,Adam Oberman*

Main category: cs.LG

TLDR: 论文通过扩展Hausner的工作，提出了在马尔可夫决策过程中需要二维奖励函数的条件，并证明了其最优策略的优良性质。


<details>
  <summary>Details</summary>
Motivation: 研究在放弃连续性公理后，如何用多维奖励函数表示偏好，并探讨其在MDP和CMDP中的表现。

Method: 通过理论分析，识别了无法用标量奖励表示偏好的条件，并给出了二维及多维奖励函数的完整描述。

Result: 在MDP中，多维奖励函数的最优策略保留了标量奖励的优良性质，但在CMDP中则不然。

Conclusion: 多维奖励函数在MDP中具有实用性，但在CMDP中需谨慎使用。

Abstract: Recent work has formalized the reward hypothesis through the lens of expected
utility theory, by interpreting reward as utility. Hausner's foundational work
showed that dropping the continuity axiom leads to a generalization of expected
utility theory where utilities are lexicographically ordered vectors of
arbitrary dimension. In this paper, we extend this result by identifying a
simple and practical condition under which preferences cannot be represented by
scalar rewards, necessitating a 2-dimensional reward function. We provide a
full characterization of such reward functions, as well as the general
d-dimensional case, in Markov Decision Processes (MDPs) under a memorylessness
assumption on preferences. Furthermore, we show that optimal policies in this
setting retain many desirable properties of their scalar-reward counterparts,
while in the Constrained MDP (CMDP) setting -- another common multiobjective
setting -- they do not.

</details>

### [154] [Discovering Symbolic Differential Equations with Symmetry Invariants](https://arxiv.org/abs/2505.12083)
*Jianke Yang,Manu Bhat,Bryan Hu,Yadi Cao,Nima Dehmamy,Robin Walters,Rose Yu*

Main category: cs.LG

TLDR: 提出了一种利用对称不变量发现符号微分方程的方法，解决了现有方法搜索空间大且可能违反物理定律的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在发现符号微分方程时面临搜索空间过大和可能违反物理定律的挑战，需改进。

Method: 引入对称不变量的概念，将其作为方程发现的基本单元，确保方程满足对称性，并与稀疏回归和遗传编程等方法结合。

Result: 验证了该方法在流体和反应-扩散等物理系统中的有效性，能够恢复简洁且符合物理定律的方程。

Conclusion: 该方法提高了方程发现的准确性和效率，同时保证了物理定律的遵循。

Abstract: Discovering symbolic differential equations from data uncovers fundamental
dynamical laws underlying complex systems. However, existing methods often
struggle with the vast search space of equations and may produce equations that
violate known physical laws. In this work, we address these problems by
introducing the concept of \textit{symmetry invariants} in equation discovery.
We leverage the fact that differential equations admitting a symmetry group can
be expressed in terms of differential invariants of symmetry transformations.
Thus, we propose to use these invariants as atomic entities in equation
discovery, ensuring the discovered equations satisfy the specified symmetry.
Our approach integrates seamlessly with existing equation discovery methods
such as sparse regression and genetic programming, improving their accuracy and
efficiency. We validate the proposed method through applications to various
physical systems, such as fluid and reaction-diffusion, demonstrating its
ability to recover parsimonious and interpretable equations that respect the
laws of physics.

</details>

### [155] [Attribution Projection Calculus: A Novel Framework for Causal Inference in Bayesian Networks](https://arxiv.org/abs/2505.12094)
*M Ruhul Amin*

Main category: cs.LG

TLDR: AP-Calculus是一种新的数学框架，用于在结构化贝叶斯网络中确定因果关系，证明中间节点在不同上下文中既是混淆因子又是去混淆因子，并优于传统因果框架。


<details>
  <summary>Details</summary>
Motivation: 研究如何在结构化贝叶斯网络中更有效地确定因果关系，特别是在特征与标签的归因中管理虚假相关性、量化信息增益和确保公平性。

Method: 提出AP-Calculus框架，分析特定网络架构中中间节点的双重角色（混淆因子和去混淆因子），并建立分离函数以最大化中间表示的区分度。

Result: 证明AP-Calculus在因果推理中优于其他结构（如Pearl的因果框架），并能扩展或取代传统do-calculus。

Conclusion: AP-Calculus为特征标签归因提供了全面的数学基础，适用于监督学习中的因果推理，尤其在大型语言模型中表现优越。

Abstract: This paper introduces Attribution Projection Calculus (AP-Calculus), a novel
mathematical framework for determining causal relationships in structured
Bayesian networks. We investigate a specific network architecture with source
nodes connected to destination nodes through intermediate nodes, where each
input maps to a single label with maximum marginal probability. We prove that
for each label, exactly one intermediate node acts as a deconfounder while
others serve as confounders, enabling optimal attribution of features to their
corresponding labels. The framework formalizes the dual nature of intermediate
nodes as both confounders and deconfounders depending on the context, and
establishes separation functions that maximize distinctions between
intermediate representations. We demonstrate that the proposed network
architecture is optimal for causal inference compared to alternative
structures, including those based on Pearl's causal framework. AP-Calculus
provides a comprehensive mathematical foundation for analyzing feature-label
attributions, managing spurious correlations, quantifying information gain,
ensuring fairness, and evaluating uncertainty in prediction models, including
large language models. Theoretical verification shows that AP-Calculus not only
extends but can also subsume traditional do-calculus for many practical
applications, offering a more direct approach to causal inference in supervised
learning contexts.

</details>

### [156] [When the Left Foot Leads to the Right Path: Bridging Initial Prejudice and Trainability](https://arxiv.org/abs/2505.12096)
*Alberto Bassi,Carlo Albert,Aurelien Lucchi,Marco Baity-Jesi,Emanuele Francazi*

Main category: cs.LG

TLDR: 论文通过理论证明连接了初始猜测偏差（IGB）与平均场（MF）理论，揭示了初始化参数分布对梯度消失或爆炸的影响，并指出优化训练性的初始化必然存在偏差。


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络（DNNs）初始化的统计特性，以阐明其可训练性和数据暴露前的固有架构偏差。

Method: 通过理论推导证明IGB与MF理论的对应关系，并扩展MF/IGB框架到多节点激活函数。

Result: 发现优化训练性的初始化必然存在偏差，并提供了设计稳定优化初始化方案的实用指南。

Conclusion: 初始化偏差与训练性优化密切相关，为架构设计提供了新的理论支持。

Abstract: Understanding the statistical properties of deep neural networks (DNNs) at
initialization is crucial for elucidating both their trainability and the
intrinsic architectural biases they encode prior to data exposure. Mean-field
(MF) analyses have demonstrated that the parameter distribution in randomly
initialized networks dictates whether gradients vanish or explode.
Concurrently, untrained DNNs were found to exhibit an initial-guessing bias
(IGB), in which large regions of the input space are assigned to a single
class. In this work, we derive a theoretical proof establishing the
correspondence between IGB and previous MF theories, thereby connecting a
network prejudice toward specific classes with the conditions for fast and
accurate learning. This connection yields the counter-intuitive conclusion: the
initialization that optimizes trainability is necessarily biased, rather than
neutral. Furthermore, we extend the MF/IGB framework to multi-node activation
functions, offering practical guidelines for designing initialization schemes
that ensure stable optimization in architectures employing max- and
average-pooling layers.

</details>

### [157] [SAINT: Attention-Based Modeling of Sub-Action Dependencies in Multi-Action Policies](https://arxiv.org/abs/2505.12109)
*Matthew Landers,Taylor W. Killian,Thomas Hartvigsen,Afsaneh Doryab*

Main category: cs.LG

TLDR: SAINT是一种新型策略架构，通过自注意力机制建模子动作间的依赖关系，解决了组合动作空间中的指数增长问题，并在多个任务域中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法在组合动作空间中因动作数量指数增长而受限，现有方法未能捕捉复杂的联合行为。

Method: 提出SAINT架构，将多组件动作表示为无序集，并通过自注意力建模其依赖关系，同时保持置换不变性。

Result: 在15个组合环境中（包括近1700万联合动作的环境），SAINT表现优于基线方法。

Conclusion: SAINT是一种高效且通用的策略架构，适用于复杂的组合动作空间。

Abstract: The combinatorial structure of many real-world action spaces leads to
exponential growth in the number of possible actions, limiting the
effectiveness of conventional reinforcement learning algorithms. Recent
approaches for combinatorial action spaces impose factorized or sequential
structures over sub-actions, failing to capture complex joint behavior. We
introduce the Sub-Action Interaction Network using Transformers (SAINT), a
novel policy architecture that represents multi-component actions as unordered
sets and models their dependencies via self-attention conditioned on the global
state. SAINT is permutation-invariant, sample-efficient, and compatible with
standard policy optimization algorithms. In 15 distinct combinatorial
environments across three task domains, including environments with nearly 17
million joint actions, SAINT consistently outperforms strong baselines.

</details>

### [158] [Metric Graph Kernels via the Tropical Torelli Map](https://arxiv.org/abs/2505.12129)
*Yueqi Cao,Anthea Monod*

Main category: cs.LG

TLDR: 提出基于热带代数几何的图核方法，通过几何和拓扑特性比较图，优于传统基于组合的图核方法。


<details>
  <summary>Details</summary>
Motivation: 传统图核方法依赖图的组合特性（如节点、边和子图），而新方法基于几何和拓扑特性，更适合比较不同底层空间的图。

Method: 利用热带代数几何研究度量图，构建对边细分不变的图核，并开发高效计算算法。

Result: 新图核在无标签设置下优于现有方法，并在合成和真实数据集上验证了其性能。

Conclusion: 提出的图核方法在几何和拓扑特性上具有优势，适用于实际任务如城市道路网络分类。

Abstract: We propose new graph kernels grounded in the study of metric graphs via
tropical algebraic geometry. In contrast to conventional graph kernels that are
based on graph combinatorics such as nodes, edges, and subgraphs, our graph
kernels are purely based on the geometry and topology of the underlying metric
space. A key characterizing property of our construction is its invariance
under edge subdivision, making the kernels intrinsically well-suited for
comparing graphs that represent different underlying spaces. We develop
efficient algorithms for computing these kernels and analyze their complexity,
showing that it depends primarily on the genus of the input graphs.
Empirically, our kernels outperform existing methods in label-free settings, as
demonstrated on both synthetic and real-world benchmark datasets. We further
highlight their practical utility through an urban road network classification
task.

</details>

### [159] [Understanding the Capabilities of Molecular Graph Neural Networks in Materials Science Through Multimodal Learning and Physical Context Encoding](https://arxiv.org/abs/2505.12137)
*Can Polat,Hasan Kurban,Erchin Serpedin,Mustafa Kurban*

Main category: cs.LG

TLDR: 该论文提出了一种多模态框架，将文本描述与分子图结合，通过门控融合机制平衡几何和文本特征，实验表明文本数据对某些电子特性有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有分子图神经网络（GNNs）仅关注几何表示，忽略了公共数据库中的化学上下文信息。

Method: 引入多模态框架，整合文本描述（如IUPAC名称、分子式、理化性质等）和分子图，采用门控融合机制。

Result: 实验表明，添加文本数据对某些电子特性有显著改进，但对其他特性提升有限；GNN架构表现出相似性能模式。

Conclusion: 文本数据与分子图的结合能提升模型性能，但GNNs学习的是相似表示而非独特物理见解。

Abstract: Molecular graph neural networks (GNNs) often focus exclusively on XYZ-based
geometric representations and thus overlook valuable chemical context available
in public databases like PubChem. This work introduces a multimodal framework
that integrates textual descriptors, such as IUPAC names, molecular formulas,
physicochemical properties, and synonyms, alongside molecular graphs. A gated
fusion mechanism balances geometric and textual features, allowing models to
exploit complementary information. Experiments on benchmark datasets indicate
that adding textual data yields notable improvements for certain electronic
properties, while gains remain limited for others. Furthermore, the GNN
architectures display similar performance patterns (improving and deteriorating
on analogous targets), suggesting they learn comparable representations rather
than distinctly different physical insights.

</details>

### [160] [Transformer learns the cross-task prior and regularization for in-context learning](https://arxiv.org/abs/2505.12138)
*Fei Lu,Yue Yu*

Main category: cs.LG

TLDR: 论文研究了Transformer在上下文学习（ICL）中的能力，特别是针对逆线性回归（ILR）问题。通过引入线性Transformer模型，研究发现其能隐式学习先验分布和有效的正则化策略，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在上下文学习中的推断能力及其对下游预测的实用性，特别是在逆线性回归问题中。

Method: 引入线性Transformer模型，研究其在秩不足的逆问题中的表现，并与传统正则化方法对比。

Result: Transformer隐式学习先验分布和正则化策略，性能优于传统方法，且误差与噪声水平、任务维度与上下文长度比及输入数据条件数线性相关。

Conclusion: Transformer在解决不适定逆问题中具有潜力，为理解其知识提取机制提供了新视角。

Abstract: Transformers have shown a remarkable ability for in-context learning (ICL),
making predictions based on contextual examples. However, while theoretical
analyses have explored this prediction capability, the nature of the inferred
context and its utility for downstream predictions remain open questions. This
paper aims to address these questions by examining ICL for inverse linear
regression (ILR), where context inference can be characterized by unsupervised
learning of underlying weight vectors. Focusing on the challenging scenario of
rank-deficient inverse problems, where context length is smaller than the
number of unknowns in the weight vectors and regularization is necessary, we
introduce a linear transformer to learn the inverse mapping from contextual
examples to the underlying weight vector. Our findings reveal that the
transformer implicitly learns both a prior distribution and an effective
regularization strategy, outperforming traditional ridge regression and
regularization methods. A key insight is the necessity of low task
dimensionality relative to the context length for successful learning.
Furthermore, we numerically verify that the error of the transformer estimator
scales linearly with the noise level, the ratio of task dimension to context
length, and the condition number of the input data. These results not only
demonstrate the potential of transformers for solving ill-posed inverse
problems, but also provide a new perspective towards understanding the
knowledge extraction mechanism within transformers.

</details>

### [161] [Structured Representation](https://arxiv.org/abs/2505.12143)
*Arun Kumar,Paul Schrater*

Main category: cs.LG

TLDR: 论文探讨了不变表示的核心问题，提出了一种基于高阶关系知识的抽象不变结构，并形式化了其计算基础。


<details>
  <summary>Details</summary>
Motivation: 揭示不变表示在表示学习中的核心作用，解决如何定义稳定且可迁移的不变表示的问题。

Method: 提出基于封闭半环的关系代数结构，形式化不变分区的计算基础。

Result: 不变分区作为核心不变表示，为结构化表示提供基础原语。

Conclusion: 不变分区是知识存储和学习发生的结构基础，为表示学习提供了新的理论框架。

Abstract: Invariant representations are core to representation learning, yet a central
challenge remains: uncovering invariants that are stable and transferable
without suppressing task-relevant signals. This raises fundamental questions,
requiring further inquiry, about the appropriate level of abstraction at which
such invariants should be defined, and which aspects of a system they should
characterize. Interpretation of the environment relies on abstract knowledge
structures to make sense of the current state, which leads to interactions,
essential drivers of learning and knowledge acquisition. We posit that
interpretation operates at the level of higher-order relational knowledge;
hence, invariant structures must be where knowledge resides, specifically, as
partitions defined by the closure of relational paths within an abstract
knowledge space. These partitions serve as the core invariant representations,
forming the structural substrate where knowledge is stored and learning occurs.
On the other hand, inter-partition connectors enable the deployment of these
knowledge partitions encoding task-relevant transitions. Thus, invariant
partitions provide the foundational primitives of structured representation. We
formalize the computational foundations for structured representation of the
invariant partitions based on closed semiring, a relational algebraic
structure.

</details>

### [162] [Causal Machine Learning in IoT-based Engineering Problems: A Tool Comparison in the Case of Household Energy Consumption](https://arxiv.org/abs/2505.12147)
*Nikolaos-Lysias Kosioris,Sotirios Nikoletseas,Gavrilis Filios,Stefanos Panagiotou*

Main category: cs.LG

TLDR: 本文比较了两种基于因果机器学习方法的工具，并通过18个查询验证其性能，结果表明这些工具在家庭能源数据领域表现良好，且可推广到其他领域。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习工具仅依赖概率关系而非推理逻辑，因果机器学习方法有望弥补这一不足。

Method: 比较两种因果机器学习工具，基于IDEAL家庭能源数据集验证其性能，并使用内置验证工具评估因果关系的假设。

Result: 工具在家庭能源数据领域表现良好，结果具有推广潜力。

Conclusion: 因果机器学习方法在解决现有工具的不足方面表现出色，且适用于更广泛的领域。

Abstract: The rapid increase in computing power and the ability to store Big Data in
the infrastructure has enabled predictions in a large variety of domains by
Machine Learning. However, in many cases, existing Machine Learning tools are
considered insufficient or incorrect since they exploit only probabilistic
dependencies rather than inference logic. Causal Machine Learning methods seem
to close this gap. In this paper, two prevalent tools based on Causal Machine
Learning methods are compared, as well as their mathematical underpinning
background. The operation of the tools is demonstrated by examining their
response to 18 queries, based on the IDEAL Household Energy Dataset, published
by the University of Edinburgh. First, it was important to evaluate the causal
relations assumption that allowed the use of this approach; this was based on
the preexisting scientific knowledge of the domain and was implemented by use
of the in-built validation tools. Results were encouraging and may easily be
extended to other domains.

</details>

### [163] [Improving Energy Natural Gradient Descent through Woodbury, Momentum, and Randomization](https://arxiv.org/abs/2505.12149)
*Andrés Guzmán-Cordero,Felix Dangel,Gil Goldshlager,Marius Zeinhofer*

Main category: cs.LG

TLDR: 论文提出了一系列技术改进能量自然梯度下降（ENGD）在物理信息神经网络（PINNs）中的效率和准确性，包括Woodbury公式、子采样投影增量自然梯度下降算法和随机化方法。


<details>
  <summary>Details</summary>
Motivation: 自然梯度方法在PINNs训练中显著加速，但计算成本过高，因此需要更高效的方法。

Method: 利用Woodbury公式降低计算复杂度，采用子采样投影增量自然梯度下降算法加速收敛，并探索随机化方法减少大规模批处理的计算成本。

Result: 数值实验表明，新方法在相同L2误差下比原始ENGD快75倍。

Conclusion: 提出的技术显著提高了ENGD在PINNs中的效率和性能，但在某些场景下仍需克服加速障碍。

Abstract: Natural gradient methods significantly accelerate the training of
Physics-Informed Neural Networks (PINNs), but are often prohibitively costly.
We introduce a suite of techniques to improve the accuracy and efficiency of
energy natural gradient descent (ENGD) for PINNs. First, we leverage the
Woodbury formula to dramatically reduce the computational complexity of ENGD.
Second, we adapt the Subsampled Projected-Increment Natural Gradient Descent
algorithm from the variational Monte Carlo literature to accelerate the
convergence. Third, we explore the use of randomized algorithms to further
reduce the computational cost in the case of large batch sizes. We find that
randomization accelerates progress in the early stages of training for
low-dimensional problems, and we identify key barriers to attaining
acceleration in other scenarios. Our numerical experiments demonstrate that our
methods outperform previous approaches, achieving the same $L^2$ error as the
original ENGD up to $75\times$ faster.

</details>

### [164] [Reasoning Large Language Model Errors Arise from Hallucinating Critical Problem Features](https://arxiv.org/abs/2505.12151)
*Alex Heyman,Joel Zylberberg*

Main category: cs.LG

TLDR: 研究发现，推理大语言模型（RLLMs）在图着色任务中容易虚构图中未指定的边，导致错误答案。


<details>
  <summary>Details</summary>
Motivation: 理解RLLMs的失败模式和原因，以改进模型设计和用户使用。

Method: 测试多个RLLMs模型在图着色任务中的表现，分析错误率和解释文本。

Result: RLLMs普遍存在虚构边的问题，且在不同复杂度和语义框架下均存在。

Conclusion: RLLMs可能在问题细节表达上存在广泛问题，需改进设计以减少此类错误。

Abstract: Large language models have recently made great strides in reasoning task
performance through chain-of-thought (CoT) strategies trained via reinforcement
learning; however, these "reasoning large language models" (RLLMs) remain
imperfect reasoners, and understanding the frequencies and causes of their
failure modes is important for both users and developers. We test o1-mini,
o3-mini, DeepSeek-R1, Claude 3.7 Sonnet, Gemini 2.5 Pro Preview, and Grok 3
Mini Beta on graph coloring as a variable-complexity constraint-satisfaction
logic problem, and find evidence from both error rate comparisons and
CoT/explanation text analysis that RLLMs are prone to hallucinate edges not
specified in the prompt's description of the graph. This phenomenon persists
across multiple problem complexity levels and semantic frames, and it appears
to account for a significant fraction of the incorrect answers from every
tested model, and the vast majority of them for some models. Our results
indicate that RLLMs may possess broader issues with misrepresentation of
problem specifics, and we offer suggestions for design choices to mitigate this
weakness.

</details>

### [165] [Learning to Dissipate Energy in Oscillatory State-Space Models](https://arxiv.org/abs/2505.12171)
*Jared Boyer,T. Konstantin Rusch,Daniela Rus*

Main category: cs.LG

TLDR: 论文提出了一种改进的线性振荡状态空间模型（D-LinOSS），通过多时间尺度的能量耗散机制，解决了LinOSS模型在长序列任务中的表示限制，并在性能和超参数搜索空间上均有提升。


<details>
  <summary>Details</summary>
Motivation: LinOSS模型在长序列任务中因固定的能量耗散机制而受限，需要更灵活的机制以提升表现。

Method: 引入D-LinOSS模型，通过多时间尺度的能量耗散机制改进LinOSS，并分析其频谱分布及稳定性。

Result: D-LinOSS在长序列任务中表现优于LinOSS，且超参数搜索空间减少50%。

Conclusion: D-LinOSS是一种更通用的振荡状态空间模型，适用于长序列学习任务。

Abstract: State-space models (SSMs) are a class of networks for sequence learning that
benefit from fixed state size and linear complexity with respect to sequence
length, contrasting the quadratic scaling of typical attention mechanisms.
Inspired from observations in neuroscience, Linear Oscillatory State-Space
models (LinOSS) are a recently proposed class of SSMs constructed from layers
of discretized forced harmonic oscillators. Although these models perform
competitively, leveraging fast parallel scans over diagonal recurrent matrices
and achieving state-of-the-art performance on tasks with sequence length up to
50k, LinOSS models rely on rigid energy dissipation ("forgetting") mechanisms
that are inherently coupled to the timescale of state evolution. As forgetting
is a crucial mechanism for long-range reasoning, we demonstrate the
representational limitations of these models and introduce Damped Linear
Oscillatory State-Space models (D-LinOSS), a more general class of oscillatory
SSMs that learn to dissipate latent state energy on multiple timescales. We
analyze the spectral distribution of the model's recurrent matrices and prove
that the SSM layers exhibit stable dynamics under simple, flexible
parameterizations. D-LinOSS consistently outperforms previous LinOSS methods on
long-range learning tasks, without introducing additional complexity, and
simultaneously reduces the hyperparameter search space by 50%.

</details>

### [166] [BenSParX: A Robust Explainable Machine Learning Framework for Parkinson's Disease Detection from Bengali Conversational Speech](https://arxiv.org/abs/2505.12192)
*Riad Hossain,Muhammad Ashad Kabir,Arat Ibne Golam Mowla,Animesh Chandra Roy,Ranjit Kumar Ghosh*

Main category: cs.LG

TLDR: 论文提出了BenSparX，首个用于帕金森病（PD）检测的孟加拉语语音数据集，并开发了一个鲁棒且可解释的机器学习框架，实现了95.77%的准确率。


<details>
  <summary>Details</summary>
Motivation: 帕金森病在资源受限地区（如孟加拉国）的早期检测困难，且现有研究缺乏针对孟加拉语的语音数据集和鲁棒的机器学习方法。

Method: 结合多种声学特征类别、系统特征选择方法、先进的机器学习算法及超参数优化，并引入SHAP分析增强模型解释性。

Result: 模型在PD检测中达到95.77%的准确率，F1分数95.57%，AUC-ROC为0.982，并在其他语言数据集上验证了其优越性。

Conclusion: BenSparX填补了孟加拉语PD语音数据集的空白，为早期诊断提供了高效且可解释的解决方案。

Abstract: Parkinson's disease (PD) poses a growing global health challenge, with
Bangladesh experiencing a notable rise in PD-related mortality. Early detection
of PD remains particularly challenging in resource-constrained settings, where
voice-based analysis has emerged as a promising non-invasive and cost-effective
alternative. However, existing studies predominantly focus on English or other
major languages; notably, no voice dataset for PD exists for Bengali - posing a
significant barrier to culturally inclusive and accessible healthcare
solutions. Moreover, most prior studies employed only a narrow set of acoustic
features, with limited or no hyperparameter tuning and feature selection
strategies, and little attention to model explainability. This restricts the
development of a robust and generalizable machine learning model. To address
this gap, we present BenSparX, the first Bengali conversational speech dataset
for PD detection, along with a robust and explainable machine learning
framework tailored for early diagnosis. The proposed framework incorporates
diverse acoustic feature categories, systematic feature selection methods, and
state-of-the-art machine learning algorithms with extensive hyperparameter
optimization. Furthermore, to enhance interpretability and trust in model
predictions, the framework incorporates SHAP (SHapley Additive exPlanations)
analysis to quantify the contribution of individual acoustic features toward PD
detection. Our framework achieves state-of-the-art performance, yielding an
accuracy of 95.77%, F1 score of 95.57%, and AUC-ROC of 0.982. We further
externally validated our approach by applying the framework to existing PD
datasets in other languages, where it consistently outperforms state-of-the-art
approaches. To facilitate further research and reproducibility, the dataset has
been made publicly available at https://github.com/Riad071/BenSParX.

</details>

### [167] [Near-Optimal Sample Complexities of Divergence-based S-rectangular Distributionally Robust Reinforcement Learning](https://arxiv.org/abs/2505.12202)
*Zhenghao Li,Shengbo Wang,Nian Si*

Main category: cs.LG

TLDR: 该论文研究了基于散度的S-rectangular DR-RL的样本复杂度，提出了近最优的样本复杂度界限，并通过实验验证了其快速学习性能。


<details>
  <summary>Details</summary>
Motivation: 解决训练与测试环境间的分布差异问题，同时平衡鲁棒性、保守性和计算可行性。

Method: 采用基于散度的S-rectangular DR-RL模型，提出经验值迭代算法。

Result: 建立了近最优的样本复杂度界限，并在实验中验证了算法的快速学习性能。

Conclusion: 首次实现了基于散度的S-rectangular模型在样本复杂度上的最优依赖关系，为实际应用提供了理论支持。

Abstract: Distributionally robust reinforcement learning (DR-RL) has recently gained
significant attention as a principled approach that addresses discrepancies
between training and testing environments. To balance robustness, conservatism,
and computational traceability, the literature has introduced DR-RL models with
SA-rectangular and S-rectangular adversaries. While most existing statistical
analyses focus on SA-rectangular models, owing to their algorithmic simplicity
and the optimality of deterministic policies, S-rectangular models more
accurately capture distributional discrepancies in many real-world applications
and often yield more effective robust randomized policies. In this paper, we
study the empirical value iteration algorithm for divergence-based
S-rectangular DR-RL and establish near-optimal sample complexity bounds of
$\widetilde{O}(|\mathcal{S}||\mathcal{A}|(1-\gamma)^{-4}\varepsilon^{-2})$,
where $\varepsilon$ is the target accuracy, $|\mathcal{S}|$ and $|\mathcal{A}|$
denote the cardinalities of the state and action spaces, and $\gamma$ is the
discount factor. To the best of our knowledge, these are the first sample
complexity results for divergence-based S-rectangular models that achieve
optimal dependence on $|\mathcal{S}|$, $|\mathcal{A}|$, and $\varepsilon$
simultaneously. We further validate this theoretical dependence through
numerical experiments on a robust inventory control problem and a theoretical
worst-case example, demonstrating the fast learning performance of our proposed
algorithm.

</details>

### [168] [Of Mice and Machines: A Comparison of Learning Between Real World Mice and RL Agents](https://arxiv.org/abs/2505.12204)
*Shuo Han,German Espinosa,Junda Huang,Daniel A. Dombeck,Malcolm A. MacIver,Bradly C. Stadie*

Main category: cs.LG

TLDR: 比较强化学习（RL）代理与生物小鼠在捕食者回避迷宫中的行为，发现RL代理缺乏自我保存本能，并提出两种新机制以改进其风险回避行为。


<details>
  <summary>Details</summary>
Motivation: 探讨RL代理与经过数百万年进化的生物代理在决策行为上的差异，尤其是风险回避方面的表现。

Method: 在捕食者回避迷宫环境中比较生物小鼠与RL代理的行为，并提出两种新机制以增强RL代理的自然风险回避行为。

Result: RL代理表现出缺乏自我保存本能的行为，而生物代理则展示出复杂的风险评估与回避行为。提出的新机制使RL代理行为更接近生物系统。

Conclusion: 通过新机制，RL代理能够表现出更自然的风险回避行为，缩小了与生物代理的差距。

Abstract: Recent advances in reinforcement learning (RL) have demonstrated impressive
capabilities in complex decision-making tasks. This progress raises a natural
question: how do these artificial systems compare to biological agents, which
have been shaped by millions of years of evolution? To help answer this
question, we undertake a comparative study of biological mice and RL agents in
a predator-avoidance maze environment. Through this analysis, we identify a
striking disparity: RL agents consistently demonstrate a lack of
self-preservation instinct, readily risking ``death'' for marginal efficiency
gains. These risk-taking strategies are in contrast to biological agents, which
exhibit sophisticated risk-assessment and avoidance behaviors. Towards bridging
this gap between the biological and artificial, we propose two novel mechanisms
that encourage more naturalistic risk-avoidance behaviors in RL agents. Our
approach leads to the emergence of naturalistic behaviors, including strategic
environment assessment, cautious path planning, and predator avoidance patterns
that closely mirror those observed in biological systems.

</details>

### [169] [Imagination-Limited Q-Learning for Offline Reinforcement Learning](https://arxiv.org/abs/2505.12211)
*Wenhui Liu,Zhijian Wu,Jingchao Wang,Dingjiang Huang,Shuigeng Zhou*

Main category: cs.LG

TLDR: 论文提出了一种名为Imagination-Limited Q-learning (ILQ)的方法，通过限制对分布外动作的乐观估计，平衡了利用与限制，解决了离线强化学习中价值估计过于乐观的问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，对分布外动作的价值估计往往过于乐观，现有方法（如策略约束或保守价值正则化）可能限制性能提升或引入偏差。

Method: 提出ILQ方法，利用动态模型想象分布外动作的价值，并通过行为价值的最大值进行裁剪，避免过度乐观。

Result: 理论上证明了ILQ在表格马尔可夫决策过程中的收敛性，实验表明在D4RL基准测试中表现优异。

Conclusion: ILQ方法有效平衡了利用与限制，解决了价值估计偏差问题，并在实验中取得了最优性能。

Abstract: Offline reinforcement learning seeks to derive improved policies entirely
from historical data but often struggles with over-optimistic value estimates
for out-of-distribution (OOD) actions. This issue is typically mitigated via
policy constraint or conservative value regularization methods. However, these
approaches may impose overly constraints or biased value estimates, potentially
limiting performance improvements. To balance exploitation and restriction, we
propose an Imagination-Limited Q-learning (ILQ) method, which aims to maintain
the optimism that OOD actions deserve within appropriate limits. Specifically,
we utilize the dynamics model to imagine OOD action-values, and then clip the
imagined values with the maximum behavior values. Such design maintains
reasonable evaluation of OOD actions to the furthest extent, while avoiding its
over-optimism. Theoretically, we prove the convergence of the proposed ILQ
under tabular Markov decision processes. Particularly, we demonstrate that the
error bound between estimated values and optimality values of OOD state-actions
possesses the same magnitude as that of in-distribution ones, thereby
indicating that the bias in value estimates is effectively mitigated.
Empirically, our method achieves state-of-the-art performance on a wide range
of tasks in the D4RL benchmark.

</details>

### [170] [Machine Learning Applications Related to Suicide in Military and Veterans: A Scoping Literature Review](https://arxiv.org/abs/2505.12220)
*Yuhan Zhang,Yishu Wei,Yanshan Wang,Yunyu Xiao,COL,Ronald K. Poropatich,Gretchen L. Haas,Yiye Zhang,Chunhua Weng,Jinze Liu,Lisa A. Brenner,James M. Bjork,Yifan Peng*

Main category: cs.LG

TLDR: 本文综述了机器学习技术在军事和退伍军人自杀风险评估中的应用，总结了相关研究并指出了现有研究的不足。


<details>
  <summary>Details</summary>
Motivation: 自杀是军人和退伍军人可预防的主要死因之一，早期检测和预测对预防至关重要。机器学习技术在此领域展现出潜力。

Method: 通过PubMed、IEEE、ACM和Google Scholar进行关键词搜索，采用PRISMA协议筛选32篇符合条件的研究。

Result: 机器学习模型在预测自杀风险方面表现出合理准确性，但存在研究空白，如忽视假阳性和假阴性指标、缺乏对生存和纵向数据的处理，以及临床解释不足。

Conclusion: 机器学习识别了多种自杀风险因素，表明预防策略需全面且灵活。

Abstract: Suicide remains one of the main preventable causes of death among active
service members and veterans. Early detection and prediction are crucial in
suicide prevention. Machine learning techniques have yielded promising results
in this area recently. This study aims to assess and summarize current research
and provides a comprehensive review regarding the application of machine
learning techniques in assessing and predicting suicidal ideation, attempts,
and mortality among members of military and veteran populations.
  A keyword search using PubMed, IEEE, ACM, and Google Scholar was conducted,
and the PRISMA protocol was adopted for relevant study selection. Thirty-two
articles met the inclusion criteria. These studies consistently identified risk
factors relevant to mental health issues such as depression, post-traumatic
stress disorder (PTSD), suicidal ideation, prior attempts, physical health
problems, and demographic characteristics.
  Machine learning models applied in this area have demonstrated reasonable
predictive accuracy. However, additional research gaps still exist. First, many
studies have overlooked metrics that distinguish between false positives and
negatives, such as positive predictive value and negative predictive value,
which are crucial in the context of suicide prevention policies. Second, more
dedicated approaches to handling survival and longitudinal data should be
explored. Lastly, most studies focused on machine learning methods, with
limited discussion of their connection to clinical rationales.
  In summary, machine learning analyses have identified a wide range of risk
factors associated with suicide in military populations. The diversity and
complexity of these factors also demonstrates that effective prevention
strategies must be comprehensive and flexible.

</details>

### [171] [Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling](https://arxiv.org/abs/2505.12225)
*Jizhou Guo,Zhaomin Wu,Philip S. Yu*

Main category: cs.LG

TLDR: ELHSR是一种高效、参数少的奖励模型，利用LLM隐藏状态信息，显著优于基线模型，且计算效率高。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型计算成本高且参数多，限制了实际应用，需要更高效的解决方案。

Method: 提出ELHSR模型，利用LLM隐藏状态信息，参数极少且训练样本需求少。

Result: ELHSR性能显著优于基线模型，计算效率提升显著，且适用于闭源LLM。

Conclusion: ELHSR是一种高效、通用的奖励模型，可单独使用或与传统模型结合提升性能。

Abstract: High-quality reward models are crucial for unlocking the reasoning potential
of large language models (LLMs), with best-of-N voting demonstrating
significant performance gains. However, current reward models, which typically
operate on the textual output of LLMs, are computationally expensive and
parameter-heavy, limiting their real-world applications. We introduce the
Efficient Linear Hidden State Reward (ELHSR) model - a novel, highly
parameter-efficient approach that leverages the rich information embedded in
LLM hidden states to address these issues. ELHSR systematically outperform
baselines with less than 0.005% of the parameters of baselines, requiring only
a few samples for training. ELHSR also achieves orders-of-magnitude efficiency
improvement with significantly less time and fewer FLOPs per sample than
baseline reward models. Moreover, ELHSR exhibits robust performance even when
trained only on logits, extending its applicability to some closed-source LLMs.
In addition, ELHSR can also be combined with traditional reward models to
achieve additional performance gains.

</details>

### [172] [AFCL: Analytic Federated Continual Learning for Spatio-Temporal Invariance of Non-IID Data](https://arxiv.org/abs/2505.12245)
*Jianheng Tang,Huiping Zhuang,Jingyu He,Run He,Jingchao Wang,Kejia Fan,Anfeng Liu,Tian Wang,Leye Wang,Zhanxing Zhu,Shanghang Zhang,Houbing Herbert Song,Yunhuai Liu*

Main category: cs.LG

TLDR: 论文提出了一种梯度无关的方法AFCL，通过解析解解决联邦持续学习中数据异构性问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦持续学习方法在动态场景下面临空间和时间数据异构性挑战，导致模型性能下降和灾难性遗忘。

Method: AFCL通过冻结特征提取层并推导解析解，实现单轮本地训练和全局聚合，无需梯度计算。

Result: AFCL在理论和实验中均表现出对非独立同分布数据的时空不变性，性能优于现有方法。

Conclusion: AFCL从根本上解决了数据异构性问题，模型性能与集中式联合学习一致，具有广泛适用性。

Abstract: Federated Continual Learning (FCL) enables distributed clients to
collaboratively train a global model from online task streams in dynamic
real-world scenarios. However, existing FCL methods face challenges of both
spatial data heterogeneity among distributed clients and temporal data
heterogeneity across online tasks. Such data heterogeneity significantly
degrades the model performance with severe spatial-temporal catastrophic
forgetting of local and past knowledge. In this paper, we identify that the
root cause of this issue lies in the inherent vulnerability and sensitivity of
gradients to non-IID data. To fundamentally address this issue, we propose a
gradient-free method, named Analytic Federated Continual Learning (AFCL), by
deriving analytical (i.e., closed-form) solutions from frozen extracted
features. In local training, our AFCL enables single-epoch learning with only a
lightweight forward-propagation process for each client. In global aggregation,
the server can recursively and efficiently update the global model with
single-round aggregation. Theoretical analyses validate that our AFCL achieves
spatio-temporal invariance of non-IID data. This ideal property implies that,
regardless of how heterogeneous the data are distributed across local clients
and online tasks, the aggregated model of our AFCL remains invariant and
identical to that of centralized joint learning. Extensive experiments show the
consistent superiority of our AFCL over state-of-the-art baselines across
various benchmark datasets and settings.

</details>

### [173] [SchoenbAt: Rethinking Attention with Polynomial basis](https://arxiv.org/abs/2505.12252)
*Yuhan Guo,Lizhong Ding,Yuwan Yang,Xuewei Guo*

Main category: cs.LG

TLDR: 论文提出SchoenbAt方法，基于Schoenberg定理，通过多项式基和随机Maclaurin特征近似点积核注意力，提升计算效率并保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于随机特征的方法仅限于Fourier基扩展，限制了核注意力的优化空间。

Method: 利用Schoenberg定理的多项式基和随机Maclaurin特征近似点积核注意力，并采用两阶段正则化约束输入空间和恢复输出尺度。

Result: 理论证明和实验验证表明，SchoenbAt在效率和准确性上均优于现有方法，显著提升计算速度且保持性能。

Conclusion: SchoenbAt是一种高效且准确的核注意力近似方法，适用于实际应用。

Abstract: Kernelized attention extends the attention mechanism by modeling sequence
correlations through kernel functions, making significant progresses in
optimizing attention. Under the guarantee of harmonic analysis theory, kernel
functions can be expanded with basis functions, inspiring random feature-based
approaches to enhance the efficiency of kernelized attention while maintaining
predictive performance. However, current random feature-based works are limited
to the Fourier basis expansions under Bochner's theorem. We propose
Schoenberg's theorem-based attention (SchoenbAt), which approximates
dot-product kernelized attention with the polynomial basis under Schoenberg's
theorem via random Maclaurin features and applies a two-stage regularization to
constrain the input space and restore the output scale, acting as a drop-in
replacement of dot-product kernelized attention. Our theoretical proof of the
unbiasedness and concentration error bound of SchoenbAt supports its efficiency
and accuracy as a kernelized attention approximation, which is also empirically
validated under various random feature dimensions. Evaluations on real-world
datasets demonstrate that SchoenbAt significantly enhances computational speed
while preserving competitive performance in terms of precision, outperforming
several efficient attention methods.

</details>

### [174] [Curriculum Abductive Learning](https://arxiv.org/abs/2505.12275)
*Wen-Chao Hu,Qi-Jie Li,Lin-Han Jia,Cunjing Ge,Yu-Feng Li,Yuan Jiang,Zhi-Hua Zhou*

Main category: cs.LG

TLDR: 提出了一种名为C-ABL的方法，通过逐步引入知识库的分区来优化ABL的训练过程，显著提升了稳定性、收敛速度和准确性。


<details>
  <summary>Details</summary>
Motivation: ABL训练过程中由于知识库的复杂性和不确定性导致训练不稳定，现有方法未能充分利用知识库的内部结构。

Method: 将知识库分区并按顺序引入训练，逐步减少推理空间，使模型平滑地学习逻辑。

Result: 在多个任务中，C-ABL表现优于传统ABL，显著提升了训练稳定性、收敛速度和最终准确性。

Conclusion: C-ABL通过动态利用知识库结构，有效解决了ABL训练中的挑战，适用于复杂知识场景。

Abstract: Abductive Learning (ABL) integrates machine learning with logical reasoning
in a loop: a learning model predicts symbolic concept labels from raw inputs,
which are revised through abduction using domain knowledge and then fed back
for retraining. However, due to the nondeterminism of abduction, the training
process often suffers from instability, especially when the knowledge base is
large and complex, resulting in a prohibitively large abduction space. While
prior works focus on improving candidate selection within this space, they
typically treat the knowledge base as a static black box. In this work, we
propose Curriculum Abductive Learning (C-ABL), a method that explicitly
leverages the internal structure of the knowledge base to address the ABL
training challenges. C-ABL partitions the knowledge base into a sequence of
sub-bases, progressively introduced during training. This reduces the abduction
space throughout training and enables the model to incorporate logic in a
stepwise, smooth way. Experiments across multiple tasks show that C-ABL
outperforms previous ABL implementations, significantly improves training
stability, convergence speed, and final accuracy, especially under complex
knowledge setting.

</details>

### [175] [SenseFlow: A Physics-Informed and Self-Ensembling Iterative Framework for Power Flow Estimation](https://arxiv.org/abs/2505.12302)
*Zhen Zhao,Wenqi Huang,Zicheng Wang,Jiaxuan Hou,Peng Li,Lei Bai*

Main category: cs.LG

TLDR: SenseFlow是一种新型的物理信息自集成迭代框架，用于提高电力系统潮流估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能充分解决电力系统的独特特性，如网络连接的稀疏性和Slack节点的重要性，导致潮流估计精度不足。

Method: SenseFlow结合了物理信息潮流网络（FlowNet）和自集成迭代估计（SeIter），通过迭代优化电压幅值和相位角预测。

Result: 实验表明，SenseFlow在多种电网配置下优于现有方法，实现了高精度的潮流估计。

Conclusion: SenseFlow为电力系统潮流估计提供了一种高效且准确的解决方案。

Abstract: Power flow estimation plays a vital role in ensuring the stability and
reliability of electrical power systems, particularly in the context of growing
network complexities and renewable energy integration. However, existing
studies often fail to adequately address the unique characteristics of power
systems, such as the sparsity of network connections and the critical
importance of the unique Slack node, which poses significant challenges in
achieving high-accuracy estimations. In this paper, we present SenseFlow, a
novel physics-informed and self-ensembling iterative framework that integrates
two main designs, the Physics-Informed Power Flow Network (FlowNet) and
Self-Ensembling Iterative Estimation (SeIter), to carefully address the unique
properties of the power system and thereby enhance the power flow estimation.
Specifically, SenseFlow enforces the FlowNet to gradually predict
high-precision voltage magnitudes and phase angles through the iterative SeIter
process. On the one hand, FlowNet employs the Virtual Node Attention and
Slack-Gated Feed-Forward modules to facilitate efficient global-local
communication in the face of network sparsity and amplify the influence of the
Slack node on angle predictions, respectively. On the other hand, SeIter
maintains an exponential moving average of FlowNet's parameters to create a
robust ensemble model that refines power state predictions throughout the
iterative fitting process. Experimental results demonstrate that SenseFlow
outperforms existing methods, providing a promising solution for high-accuracy
power flow estimation across diverse grid configurations.

</details>

### [176] [Efficient Federated Class-Incremental Learning of Pre-Trained Models via Task-agnostic Low-rank Residual Adaptation](https://arxiv.org/abs/2505.12318)
*Feng Yu,Jia Hu,Geyong Min*

Main category: cs.LG

TLDR: Fed-TaLoRA是一种针对联邦类增量学习（FCIL）的高效参数微调方法，通过共享任务无关的LoRA参数和创新的残差权重更新机制，解决了灾难性遗忘和非独立同分布数据问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦参数高效微调方法假设数据分布静态，无法应对现实场景中不断出现新类的挑战，尤其是在FCIL中。

Method: Fed-TaLoRA通过任务无关的LoRA参数微调、后聚合模型校准和LoRA模块策略性放置，实现高效知识迁移和遗忘缓解。

Result: 在多个基准数据集上，Fed-TaLoRA在数据异构场景中表现优于现有方法，同时显著降低资源需求。

Conclusion: Fed-TaLoRA为资源受限的FCIL提供了一种高效且性能优越的解决方案。

Abstract: Federated Parameter-Efficient Fine-Tuning (FedPEFT) reduces communication and
computation costs in federated fine-tuning of pre-trained models by updating
only a small subset of model parameters. However, existing approaches assume
static data distributions, failing to adequately address real-world scenarios
where new classes continually emerge, particularly in Federated Class
Incremental Learning (FCIL). FCIL faces two key challenges: catastrophic
forgetting and performance degradation caused by non-IID data across clients.
Unlike current methods that maintain separate task-specific components or
suffer from aggregation noise during parameter aggregation, we propose
Federated Task-agnostic Low-rank Residual Adaptation (Fed-TaLoRA), a novel
parameter-efficient approach for fine-tuning in resource-constrained FCIL
scenarios. Specifically, we fine-tune only shared task-agnostic LoRA parameters
across sequential tasks, effectively mitigating catastrophic forgetting while
enabling efficient knowledge transfer among clients. Based on a theoretical
analysis of aggregation, we develop a novel residual weight update mechanism
that ensures accurate knowledge consolidation with minimal overhead. Our
methodological innovations are attributed to three key strategies:
task-agnostic adaptation, post-aggregation model calibration, and strategic
placement of LoRA modules. Extensive experiments on multiple benchmark datasets
demonstrate that Fed-TaLoRA consistently outperforms state-of-the-art methods
in diverse data heterogeneity scenarios while substantially reducing resource
requirements.

</details>

### [177] [Model alignment using inter-modal bridges](https://arxiv.org/abs/2505.12322)
*Ali Gholamzadeh,Noor Sajid*

Main category: cs.LG

TLDR: 提出一种半监督方法，通过条件流匹配实现跨模态模型对齐，减少对大量配对数据的需求。


<details>
  <summary>Details</summary>
Motivation: 解决跨模态模型重用中内部表示对齐的困难，避免现有方法对大量配对数据或特定领域的依赖。

Method: 采用条件流匹配方法，学习不同模态潜在空间之间的条件流，包括最优传输问题和基于标记样本的高效对齐。

Result: 在MNIST、ImageNet等数据集上，下游任务性能与端到端训练模型相当，尤其在标记数据稀缺时表现优异。

Conclusion: 该方法为跨模态模型对齐提供了一种数据高效、监督需求低的解决方案。

Abstract: Foundation models have demonstrated remarkable performance across modalities
such as language and vision. However, model reuse across distinct modalities
(e.g., text and vision) remains limited due to the difficulty of aligning
internal representations. Existing methods require extensive paired training
data or are constrained to specific domains. We introduce a semi-supervised
approach for model alignment via conditional flow matching. The conditional
flow between latent spaces of different modalities (e.g., text-to-image or
biological-to-artificial neuronal activity) can be learned in two settings:
($1$) solving a (balanced or unbalanced) optimal transport problem with an
inter-space bridge cost, and ($2$) performing memory-efficient alignment using
labelled exemplars. Despite being constrained by the original models' capacity,
our method--under both settings--matches downstream task performance of
end-to-end trained models on object recognition and image generation tasks
across MNIST, ImageNet, and \cite{majaj2015simple} datasets, particularly when
labelled training data is scarce ($<20\%$). Our method provides a
data-efficient solution for inter-modal model alignment with minimal
supervision.

</details>

### [178] [GraphFLEx: Structure Learning Framework for Large Expanding Graphs](https://arxiv.org/abs/2505.12323)
*Mohit Kataria,Nikita Malik,Sandeep Kumar,Jayadeva*

Main category: cs.LG

TLDR: GraphFLEx是一个可扩展的图结构学习框架，适用于大规模动态图，通过聚类和粗化技术减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在大规模动态图上效率低下，需要重新学习结构，计算和内存成本高。

Method: 结合聚类和粗化技术，限制边形成到结构相关的节点子集，支持48种灵活配置。

Result: 在26个数据集和GNN架构上表现优异，显著提升了可扩展性。

Conclusion: GraphFLEx是一个高效、灵活的框架，适用于大规模动态图结构学习。

Abstract: Graph structure learning is a core problem in graph-based machine learning,
essential for uncovering latent relationships and ensuring model
interpretability. However, most existing approaches are ill-suited for
large-scale and dynamically evolving graphs, as they often require complete
re-learning of the structure upon the arrival of new nodes and incur
substantial computational and memory costs. In this work, we propose GraphFLEx:
a unified and scalable framework for Graph Structure Learning in Large and
Expanding Graphs. GraphFLEx mitigates the scalability bottlenecks by
restricting edge formation to structurally relevant subsets of nodes identified
through a combination of clustering and coarsening techniques. This
dramatically reduces the search space and enables efficient, incremental graph
updates. The framework supports 48 flexible configurations by integrating
diverse choices of learning paradigms, coarsening strategies, and clustering
methods, making it adaptable to a wide range of graph settings and learning
objectives. Extensive experiments across 26 diverse datasets and Graph Neural
Network architectures demonstrate that GraphFLEx achieves state-of-the-art
performance with significantly improved scalability.

</details>

### [179] [Neural Graduated Assignment for Maximum Common Edge Subgraphs](https://arxiv.org/abs/2505.12325)
*Chaolong Ying,Yingqi Ruan,Xuemin Chen,Yaomin Wang,Tianshu Yu*

Main category: cs.LG

TLDR: 本文提出了一种名为“神经逐步分配”（NGA）的无监督学习方法，用于解决最大公共边子图（MCES）问题，显著提升了计算时间和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理大规模MCES问题时存在可扩展性不足的问题，需要一种更高效的方法。

Method: NGA通过堆叠神经组件，并结合可学习温度的高维参数化，改进了经典的逐步分配（GA）技术。

Result: 实验表明，NGA在计算时间、可扩展性和性能上均优于现有方法。

Conclusion: NGA为MCES计算提供了显著进步，并对其他分配问题具有启发意义。

Abstract: The Maximum Common Edge Subgraph (MCES) problem is a crucial challenge with
significant implications in domains such as biology and chemistry. Traditional
approaches, which include transformations into max-clique and search-based
algorithms, suffer from scalability issues when dealing with larger instances.
This paper introduces ``Neural Graduated Assignment'' (NGA), a simple,
scalable, unsupervised-training-based method that addresses these limitations
by drawing inspiration from the classical Graduated Assignment (GA) technique.
Central to NGA is stacking of neural components that closely resemble the GA
process, but with the reparameterization of learnable temperature into higher
dimension. We further theoretically analyze the learning dynamics of NGA,
showing its design leads to fast convergence, better exploration-exploitation
tradeoff, and ability to escape local optima. Extensive experiments across MCES
computation, graph similarity estimation, and graph retrieval tasks reveal that
NGA not only significantly improves computation time and scalability on large
instances but also enhances performance compared to existing methodologies. The
introduction of NGA marks a significant advancement in the computation of MCES
and offers insights into other assignment problems.

</details>

### [180] [Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models](https://arxiv.org/abs/2505.12343)
*Kai Tang,Jinhao You,Xiuqi Ge,Hanze Li,Yichen Guo,Xiande Huang*

Main category: cs.LG

TLDR: 提出了一种无需重新训练的解码机制DCLA，通过层聚合增强层间一致性，有效减少大型视觉语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在性能不稳定和对超参数敏感方面存在局限，限制了实用性。

Method: 通过聚合前层表示构建动态语义参考，校正语义偏离层以增强层间一致性。

Result: 在MME和POPE等基准测试中，DCLA显著减少幻觉并提升模型可靠性。

Conclusion: DCLA是一种无需外部知识库的实用方法，能稳定提升LVLMs的性能。

Abstract: Despite the impressive capabilities of Large Vision-Language Models (LVLMs),
they remain susceptible to hallucinations-generating content that is
inconsistent with the input image. Existing training-free hallucination
mitigation methods often suffer from unstable performance and high sensitivity
to hyperparameter settings, limiting their practicality and broader adoption.
In this paper, we propose a novel decoding mechanism, Decoding with Inter-layer
Consistency via Layer Aggregation (DCLA), which requires no retraining,
fine-tuning, or access to external knowledge bases. Specifically, our approach
constructs a dynamic semantic reference by aggregating representations from
previous layers, and corrects semantically deviated layers to enforce
inter-layer consistency. The method allows DCLA to robustly mitigate
hallucinations across multiple LVLMs. Experiments on hallucination benchmarks
such as MME and POPE demonstrate that DCLA effectively reduces hallucinations
while enhancing the reliability and performance of LVLMs.

</details>

### [181] [Early Prediction of In-Hospital ICU Mortality Using Innovative First-Day Data: A Review](https://arxiv.org/abs/2505.12344)
*Han Wang*

Main category: cs.LG

TLDR: 综述评估了利用ICU入院首日数据预测院内死亡率的创新方法，重点关注机器学习、新型生物标志物及多源数据整合。


<details>
  <summary>Details</summary>
Motivation: 传统评分系统在预测准确性和适应性上存在局限，亟需更早、更准确的预测方法以优化临床干预和资源分配。

Method: 系统评估了机器学习、新型生物标志物应用及多源数据整合的创新方法。

Result: 综述为ICU早期死亡率预测提供了方法学基准。

Conclusion: 创新方法有望提升预测准确性，改善患者预后和资源利用。

Abstract: The intensive care unit (ICU) manages critically ill patients, many of whom
face a high risk of mortality. Early and accurate prediction of in-hospital
mortality within the first 24 hours of ICU admission is crucial for timely
clinical interventions, resource optimization, and improved patient outcomes.
Traditional scoring systems, while useful, often have limitations in predictive
accuracy and adaptability. Objective: This review aims to systematically
evaluate and benchmark innovative methodologies that leverage data available
within the first day of ICU admission for predicting in-hospital mortality. We
focus on advancements in machine learning, novel biomarker applications, and
the integration of diverse data types.

</details>

### [182] [Multi-CALF: A Policy Combination Approach with Statistical Guarantees](https://arxiv.org/abs/2505.12350)
*Georgiy Malaniya,Anton Bolychev,Grigory Yaremenko,Anastasia Krasnaya,Pavel Osinenko*

Main category: cs.LG

TLDR: Multi-CALF算法通过结合强化学习策略，基于相对价值改进，提升性能并保持稳定性。


<details>
  <summary>Details</summary>
Motivation: 结合标准RL策略与理论支持的替代策略，以继承稳定性保证并提升性能。

Method: 整合两种策略，提供收敛概率、最大偏差和收敛时间的精确界限。

Result: 在控制任务中验证了性能提升和稳定性保证。

Conclusion: Multi-CALF在保持稳定性的同时，显著提升了性能。

Abstract: We introduce Multi-CALF, an algorithm that intelligently combines
reinforcement learning policies based on their relative value improvements. Our
approach integrates a standard RL policy with a theoretically-backed
alternative policy, inheriting formal stability guarantees while often
achieving better performance than either policy individually. We prove that our
combined policy converges to a specified goal set with known probability and
provide precise bounds on maximum deviation and convergence time. Empirical
validation on control tasks demonstrates enhanced performance while maintaining
stability guarantees.

</details>

### [183] [Importance Sampling for Nonlinear Models](https://arxiv.org/abs/2505.12353)
*Prakash Palanivelu Rajmohan,Fred Roosta*

Main category: cs.LG

TLDR: 该论文通过引入非线性映射的伴随算子概念，将基于范数和杠杆得分的重要性采样方法推广到非线性模型，为非线性映射提供近似保证，并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 非线性模型中识别重要数据点的方法尚未充分发展，论文旨在填补这一空白。

Method: 引入非线性映射的伴随算子，推广基于范数和杠杆得分的采样方法。

Result: 采样方法为非线性映射提供近似保证，降低计算复杂度，并支持模型可解释性和异常检测。

Conclusion: 论文通过理论和实验验证了非线性重要性采样方法的有效性。

Abstract: While norm-based and leverage-score-based methods have been extensively
studied for identifying "important" data points in linear models, analogous
tools for nonlinear models remain significantly underdeveloped. By introducing
the concept of the adjoint operator of a nonlinear map, we address this gap and
generalize norm-based and leverage-score-based importance sampling to nonlinear
settings. We demonstrate that sampling based on these generalized notions of
norm and leverage scores provides approximation guarantees for the underlying
nonlinear mapping, similar to linear subspace embeddings. As direct
applications, these nonlinear scores not only reduce the computational
complexity of training nonlinear models by enabling efficient sampling over
large datasets but also offer a novel mechanism for model explainability and
outlier detection. Our contributions are supported by both theoretical analyses
and experimental results across a variety of supervised learning scenarios.

</details>

### [184] [A universal policy wrapper with guarantees](https://arxiv.org/abs/2505.12354)
*Anton Bolychev,Georgiy Malaniya,Grigory Yaremenko,Anastasia Krasnaya,Pavel Osinenko*

Main category: cs.LG

TLDR: 提出了一种强化学习代理的通用策略包装器，确保目标达成的形式化保证。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习算法性能优异但缺乏严格的安全保障，因此需要一种方法在保持高性能的同时提供安全性。

Method: 通过在高性能基础策略和具有已知收敛性的备用策略之间选择性切换，基础策略的价值函数监督切换过程。

Result: 分析表明，该包装器继承了备用策略的目标达成保证，同时保持或提升了基础策略的性能。

Conclusion: 该方法无需额外系统知识或在线约束优化，可轻松部署于多种强化学习架构和任务。

Abstract: We introduce a universal policy wrapper for reinforcement learning agents
that ensures formal goal-reaching guarantees. In contrast to standard
reinforcement learning algorithms that excel in performance but lack rigorous
safety assurances, our wrapper selectively switches between a high-performing
base policy -- derived from any existing RL method -- and a fallback policy
with known convergence properties. Base policy's value function supervises this
switching process, determining when the fallback policy should override the
base policy to ensure the system remains on a stable path. The analysis proves
that our wrapper inherits the fallback policy's goal-reaching guarantees while
preserving or improving upon the performance of the base policy. Notably, it
operates without needing additional system knowledge or online constrained
optimization, making it readily deployable across diverse reinforcement
learning architectures and tasks.

</details>

### [185] [AbFlowNet: Optimizing Antibody-Antigen Binding Energy via Diffusion-GFlowNet Fusion](https://arxiv.org/abs/2505.12358)
*Abrar Rahman Abir,Haz Sameen Shahgir,Md Rownok Zahan Ratul,Md Toki Tahmid,Greg Ver Steeg,Yue Dong*

Main category: cs.LG

TLDR: AbFlowNet是一种结合GFlowNet和扩散模型的新框架，用于优化抗体CDR设计，直接整合结合能信号，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能联合优化结合能，且依赖计算昂贵的在线强化学习，效果不稳定。

Method: AbFlowNet将扩散步骤作为GFlowNet的状态，统一优化扩散损失和结合能。

Result: 在氨基酸恢复、几何重建和结合能改进方面显著优于基线模型，误差大幅降低。

Conclusion: AbFlowNet提供了一种高效且统一的抗体设计方法，避免了复杂计算。

Abstract: Complementarity Determining Regions (CDRs) are critical segments of an
antibody that facilitate binding to specific antigens. Current computational
methods for CDR design utilize reconstruction losses and do not jointly
optimize binding energy, a crucial metric for antibody efficacy. Rather,
binding energy optimization is done through computationally expensive Online
Reinforcement Learning (RL) pipelines rely heavily on unreliable binding energy
estimators. In this paper, we propose AbFlowNet, a novel generative framework
that integrates GFlowNet with Diffusion models. By framing each diffusion step
as a state in the GFlowNet framework, AbFlowNet jointly optimizes standard
diffusion losses and binding energy by directly incorporating energy signals
into the training process, thereby unifying diffusion and reward optimization
in a single procedure. Experimental results show that AbFlowNet outperforms the
base diffusion model by 3.06% in amino acid recovery, 20.40% in geometric
reconstruction (RMSD), and 3.60% in binding energy improvement ratio. ABFlowNet
also decreases Top-1 total energy and binding energy errors by 24.8% and 38.1%
without pseudo-labeling the test dataset or using computationally expensive
online RL regimes.

</details>

### [186] [STAR: Stage-Wise Attention-Guided Token Reduction for Efficient Large Vision-Language Models Inference](https://arxiv.org/abs/2505.12359)
*Yichen Guo,Hanze Li,Zonghao Zhang,Jinhao You,Kai Tang,Xiande Huang*

Main category: cs.LG

TLDR: STAR是一种无需训练、即插即用的框架，通过两阶段注意力引导的token剪枝，显著降低计算成本并保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有单阶段token剪枝方法因局部视角导致性能下降，尤其在高压剪枝比下。

Method: STAR采用两阶段剪枝：早期基于视觉自注意力剪枝冗余低层特征，后期基于跨模态注意力剪枝任务无关token。

Result: STAR在多种LVLM架构和基准测试中显著加速且性能接近或优于原模型。

Conclusion: STAR通过全局视角的两阶段剪枝，高效减少计算开销并保留关键信息。

Abstract: Although large vision-language models (LVLMs) leverage rich visual token
representations to achieve strong performance on multimodal tasks, these tokens
also introduce significant computational overhead during inference. Existing
training-free token pruning methods typically adopt a single-stage strategy,
focusing either on visual self-attention or visual-textual cross-attention.
However, such localized perspectives often overlook the broader information
flow across the model, leading to substantial performance degradation,
especially under high pruning ratios. In this work, we propose STAR (Stage-wise
Attention-guided token Reduction), a training-free, plug-and-play framework
that approaches token pruning from a global perspective. Instead of pruning at
a single point, STAR performs attention-guided reduction in two complementary
stages: an early-stage pruning based on visual self-attention to remove
redundant low-level features, and a later-stage pruning guided by cross-modal
attention to discard task-irrelevant tokens. This holistic approach allows STAR
to significantly reduce computational cost while better preserving
task-critical information. Extensive experiments across multiple LVLM
architectures and benchmarks show that STAR achieves strong acceleration while
maintaining comparable, and in some cases even improved performance.

</details>

### [187] [DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization](https://arxiv.org/abs/2505.12366)
*Gang Li,Ming Lin,Tomer Galanti,Zhengzhong Tu,Tianbao Yang*

Main category: cs.LG

TLDR: 论文分析了GRPO在二元奖励设置中的局限性，并提出了一种新的DisCO框架，通过判别性学习和约束优化显著提升了大型推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: GRPO在大型推理模型中的应用存在难度偏差和熵不稳定性问题，作者希望通过判别性学习改进这些问题。

Method: 提出DisCO框架，采用判别性目标函数、非裁剪RL代理目标和约束优化方法。

Result: DisCO在数学推理任务中显著优于GRPO及其变体，平均提升7%和6%。

Conclusion: DisCO通过消除难度偏差和提升稳定性，为强化大型推理模型提供了更有效的框架。

Abstract: The recent success and openness of DeepSeek-R1 have brought widespread
attention to Group Relative Policy Optimization (GRPO) as a reinforcement
learning method for large reasoning models (LRMs). In this work, we analyze the
GRPO objective under a binary reward setting and reveal an inherent limitation
of question-level difficulty bias. We also identify a connection between GRPO
and traditional discriminative methods in supervised learning. Motivated by
these insights, we introduce a new Discriminative Constrained Optimization
(DisCO) framework for reinforcing LRMs, grounded in the principle of
discriminative learning. The main differences between DisCO and GRPO and its
recent variants are: (1) it replaces the group relative objective with a
discriminative objective defined by a scoring function; (2) it abandons
clipping-based surrogates in favor of non-clipping RL surrogate objectives used
as scoring functions; (3) it employs a simple yet effective constrained
optimization approach to enforce the KL divergence constraint, ensuring stable
training. As a result, DisCO offers notable advantages over GRPO and its
variants: (i) it completely eliminates difficulty bias by adopting
discriminative objectives; (ii) it addresses the entropy instability in GRPO
and its variants through the use of non-clipping scoring functions and a
constrained optimization approach; (iii) it allows the incorporation of
advanced discriminative learning techniques to address data imbalance, where a
significant number of questions have more negative than positive generated
answers during training. Our experiments on enhancing the mathematical
reasoning capabilities of SFT-finetuned models show that DisCO significantly
outperforms GRPO and its improved variants such as DAPO, achieving average
gains of 7\% over GRPO and 6\% over DAPO across six benchmark tasks for an 1.5B
model.

</details>

### [188] [Graph-Reward-SQL: Execution-Free Reinforcement Learning for Text-to-SQL via Graph Matching and Stepwise Reward](https://arxiv.org/abs/2505.12380)
*Han Weng,Boyi Liu,Yuanfeng Song,Dun Zeng,Yingxiang Yang,Yi Zhan,Longjie Cui,Xiaoming Yin,Yang Sun*

Main category: cs.LG

TLDR: 提出了一种名为Graph-Reward-SQL的新框架，通过GMNScore奖励模型和SQL图表示优化Text-to-SQL任务中的RL微调，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在Text-to-SQL任务中存在执行延迟高和GPU内存开销大的问题，限制了效率和扩展性。

Method: 采用GMNScore奖励模型和SQL图表示，减少推理时间和内存使用；进一步引入StepRTM模型，提供对CTE子查询的逐步监督。

Result: 在Spider和BIRD等基准测试中，新方法性能优于现有奖励模型。

Conclusion: Graph-Reward-SQL框架通过优化奖励模型和监督机制，显著提升了Text-to-SQL任务的效率和效果。

Abstract: Reinforcement learning (RL) has been widely adopted to enhance the
performance of large language models (LLMs) on Text-to-SQL tasks. However,
existing methods often rely on execution-based or LLM-based Bradley-Terry
reward models. The former suffers from high execution latency caused by
repeated database calls, whereas the latter imposes substantial GPU memory
overhead, both of which significantly hinder the efficiency and scalability of
RL pipelines. To this end, we propose a novel Text-to-SQL RL fine-tuning
framework named Graph-Reward-SQL, which employs the GMNScore outcome reward
model. We leverage SQL graph representations to provide accurate reward signals
while significantly reducing inference time and GPU memory usage. Building on
this foundation, we further introduce StepRTM, a stepwise reward model that
provides intermediate supervision over Common Table Expression (CTE)
subqueries. This encourages both functional correctness and structural clarity
of SQL. Extensive comparative and ablation experiments on standard benchmarks,
including Spider and BIRD, demonstrate that our method consistently outperforms
existing reward models.

</details>

### [189] [Neural Thermodynamics I: Entropic Forces in Deep and Universal Representation Learning](https://arxiv.org/abs/2505.12387)
*Liu Ziyin,Yizhou Xu,Isaac Chuang*

Main category: cs.LG

TLDR: 论文提出了一种基于熵力理论的方法，用于解释和理解深度学习中的涌现现象，特别是随机梯度下降（SGD）及其变体的学习动态。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习和大型语言模型中涌现现象的快速发现，解释和理解其成因成为迫切需求。

Method: 基于参数对称性和熵损失景观理论，提出了一种严格的熵力理论，揭示了表示学习受随机性和离散时间更新产生的熵力支配。

Result: 熵力系统性地破坏连续参数对称性并保留离散对称性，导致梯度平衡现象，解释了神经网络表示的对齐和优化行为的矛盾观察。

Conclusion: 熵力与对称性破缺的结合是理解深度学习中涌现现象的关键。

Abstract: With the rapid discovery of emergent phenomena in deep learning and large
language models, explaining and understanding their cause has become an urgent
need. Here, we propose a rigorous entropic-force theory for understanding the
learning dynamics of neural networks trained with stochastic gradient descent
(SGD) and its variants. Building on the theory of parameter symmetries and an
entropic loss landscape, we show that representation learning is crucially
governed by emergent entropic forces arising from stochasticity and
discrete-time updates. These forces systematically break continuous parameter
symmetries and preserve discrete ones, leading to a series of gradient balance
phenomena that resemble the equipartition property of thermal systems. These
phenomena, in turn, (a) explain the universal alignment of neural
representations between AI models and lead to a proof of the Platonic
Representation Hypothesis, and (b) reconcile the seemingly contradictory
observations of sharpness- and flatness-seeking behavior of deep learning
optimization. Our theory and experiments demonstrate that a combination of
entropic forces and symmetry breaking is key to understanding emergent
phenomena in deep learning.

</details>

### [190] [Engineering application of physics-informed neural networks for Saint-Venant torsion](https://arxiv.org/abs/2505.12389)
*Su Yeong Jo,Sanghyeon Park,Seungchan Ko,Jongcheon Park,Hosung Kim,Sangseung Lee,Joongoo Jeon*

Main category: cs.LG

TLDR: 该研究基于物理信息神经网络（PINN）开发了三种新型数值方法，用于解决Saint-Venant扭转方程，避免了传统网格方法的复杂性和高计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法（如有限元法）在解决Saint-Venant扭转问题时需要复杂的网格技术和高计算成本，因此需要开发更高效的方法。

Method: 利用神经网络的表达能力和自动微分功能，开发了三种PINN求解器：基础PINN、变尺度PINN（VS-PINN）和参数化PINN。

Result: 所有求解器的结果与参考解吻合良好，证明了其准确性和鲁棒性。

Conclusion: PINN方法为扭转行为分析提供了高效且灵活的解决方案，可根据需求选择不同求解器。

Abstract: The Saint-Venant torsion theory is a classical theory for analyzing the
torsional behavior of structural components, and it remains critically
important in modern computational design workflows. Conventional numerical
methods, including the finite element method (FEM), typically rely on
mesh-based approaches to obtain approximate solutions. However, these methods
often require complex and computationally intensive techniques to overcome the
limitations of approximation, leading to significant increases in computational
cost. The objective of this study is to develop a series of novel numerical
methods based on physics-informed neural networks (PINN) for solving the
Saint-Venant torsion equations. Utilizing the expressive power and the
automatic differentiation capability of neural networks, the PINN can solve
partial differential equations (PDEs) along with boundary conditions without
the need for intricate computational techniques. First, a PINN solver was
developed to compute the torsional constant for bars with arbitrary
cross-sectional geometries. This was followed by the development of a solver
capable of handling cases with sharp geometric transitions; variable-scaling
PINN (VS-PINN). Finally, a parametric PINN was constructed to address the
limitations of conventional single-instance PINN. The results from all three
solvers showed good agreement with reference solutions, demonstrating their
accuracy and robustness. Each solver can be selectively utilized depending on
the specific requirements of torsional behavior analysis.

</details>

### [191] [Few-Shot Concept Unlearning with Low Rank Adaptation](https://arxiv.org/abs/2505.12395)
*Udaya Shreyas,L. N. Aadarsh*

Main category: cs.LG

TLDR: 本文提出了一种快速算法，通过更新文本编码器的梯度来移除扩散模型中特定概念的影响，避免重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 图像生成模型可能生成敏感或侵权内容，传统重新训练耗时过长，需要快速移除特定概念的方法。

Method: 使用加权损失函数和反向传播更新Stable Diffusion模型的文本编码器权重，结合Textual Inversion和Low-Rank Adaptation技术。

Result: 在Stable Diffusion v2模型上实验，平均概念移除时间为50秒，仅需4-5张图像。

Conclusion: 该方法高效且快速，适用于移除扩散模型中的特定概念，解决了隐私和版权问题。

Abstract: Image Generation models are a trending topic nowadays, with many people
utilizing Artificial Intelligence models in order to generate images. There are
many such models which, given a prompt of a text, will generate an image which
depicts said prompt. There are many image generation models, such as Latent
Diffusion Models, Denoising Diffusion Probabilistic Models, Generative
Adversarial Networks and many more. When generating images, these models can
generate sensitive image data, which can be threatening to privacy or may
violate copyright laws of private entities. Machine unlearning aims at removing
the influence of specific data subsets from the trained models and in the case
of image generation models, remove the influence of a concept such that the
model is unable to generate said images of the concept when prompted.
Conventional retraining of the model can take upto days, hence fast algorithms
are the need of the hour. In this paper we propose an algorithm that aims to
remove the influence of concepts in diffusion models through updating the
gradients of the final layers of the text encoders. Using a weighted loss
function, we utilize backpropagation in order to update the weights of the
final layers of the Text Encoder componet of the Stable Diffusion Model,
removing influence of the concept from the text-image embedding space, such
that when prompted, the result is an image not containing the concept. The
weighted loss function makes use of Textual Inversion and Low-Rank
Adaptation.We perform our experiments on Latent Diffusion Models, namely the
Stable Diffusion v2 model, with an average concept unlearning runtime of 50
seconds using 4-5 images.

</details>

### [192] [Hyperbolic Residual Quantization: Discrete Representations for Data with Latent Hierarchies](https://arxiv.org/abs/2505.12404)
*Piotr Piękos,Subhradeep Kayal,Alexandros Karatzoglou*

Main category: cs.LG

TLDR: 论文提出了一种基于双曲几何的残差量化方法（HRQ），用于更有效地表示层次数据，相比传统的欧几里得残差量化（RQ），在层次建模任务中性能提升高达20%。


<details>
  <summary>Details</summary>
Motivation: 传统残差量化（RQ）依赖欧几里得几何，可能导致与层次分支结构不匹配，限制了层次数据的表示能力。

Method: 提出HRQ方法，在双曲流形中嵌入数据，并使用双曲操作和距离度量进行残差量化。

Result: 在WordNet超名词树监督层次建模和层次发现任务中，HRQ的性能优于欧几里得RQ，提升高达20%。

Conclusion: 将双曲几何引入离散表示学习能显著增强对潜在层次结构的捕捉能力。

Abstract: Hierarchical data arise in countless domains, from biological taxonomies and
organizational charts to legal codes and knowledge graphs. Residual
Quantization (RQ) is widely used to generate discrete, multitoken
representations for such data by iteratively quantizing residuals in a
multilevel codebook. However, its reliance on Euclidean geometry can introduce
fundamental mismatches that hinder modeling of hierarchical branching,
necessary for faithful representation of hierarchical data. In this work, we
propose Hyperbolic Residual Quantization (HRQ), which embeds data natively in a
hyperbolic manifold and performs residual quantization using hyperbolic
operations and distance metrics. By adapting the embedding network, residual
computation, and distance metric to hyperbolic geometry, HRQ imparts an
inductive bias that aligns naturally with hierarchical branching. We claim that
HRQ in comparison to RQ can generate more useful for downstream tasks discrete
hierarchical representations for data with latent hierarchies. We evaluate HRQ
on two tasks: supervised hierarchy modeling using WordNet hypernym trees, where
the model is supervised to learn the latent hierarchy - and hierarchy
discovery, where, while latent hierarchy exists in the data, the model is not
directly trained or evaluated on a task related to the hierarchy. Across both
scenarios, HRQ hierarchical tokens yield better performance on downstream tasks
compared to Euclidean RQ with gains of up to $20\%$ for the hierarchy modeling
task. Our results demonstrate that integrating hyperbolic geometry into
discrete representation learning substantially enhances the ability to capture
latent hierarchies.

</details>

### [193] [It Takes a Graph to Know a Graph: Rewiring for Homophily with a Reference Graph](https://arxiv.org/abs/2505.12411)
*Harel Mendelman,Haggai Maron,Ronen Talmon*

Main category: cs.LG

TLDR: 该论文提出了一种通过图重连提高图同质性的框架，以提升GNN在异质性图上的性能。


<details>
  <summary>Details</summary>
Motivation: GNN在异质性图上表现不佳，而图重连是一种未被充分探索的策略。论文通过理论分析强调了提高图同质性的必要性。

Method: 提出了一种基于参考图的图重连框架，并设计了一种标签驱动的扩散方法来构建同质性参考图。

Result: 在11个真实异质性数据集上，该方法优于现有重连技术和专用GNN，提高了节点分类准确性。

Conclusion: 该框架能有效提升GNN在异质性图上的性能，且具有高效性和可扩展性。

Abstract: Graph Neural Networks (GNNs) excel at analyzing graph-structured data but
struggle on heterophilic graphs, where connected nodes often belong to
different classes. While this challenge is commonly addressed with specialized
GNN architectures, graph rewiring remains an underexplored strategy in this
context. We provide theoretical foundations linking edge homophily, GNN
embedding smoothness, and node classification performance, motivating the need
to enhance homophily. Building on this insight, we introduce a rewiring
framework that increases graph homophily using a reference graph, with
theoretical guarantees on the homophily of the rewired graph. To broaden
applicability, we propose a label-driven diffusion approach for constructing a
homophilic reference graph from node features and training labels. Through
extensive simulations, we analyze how the homophily of both the original and
reference graphs influences the rewired graph homophily and downstream GNN
performance. We evaluate our method on 11 real-world heterophilic datasets and
show that it outperforms existing rewiring techniques and specialized GNNs for
heterophilic graphs, achieving improved node classification accuracy while
remaining efficient and scalable to large graphs.

</details>

### [194] [Embedding principle of homogeneous neural network for classification problem](https://arxiv.org/abs/2505.12419)
*Jiahan Zhang,Tao Luo,Yaoyu Zhang*

Main category: cs.LG

TLDR: 论文研究了同质神经网络中KKT点的嵌入原理，证明通过神经元分裂可以将小网络的KKT点嵌入到大网络中，并探讨了梯度流训练的动态行为。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络的收敛点和优化景观，尤其是同质网络中KKT点与最大边际问题的关系，以揭示不同宽度网络间的结构联系。

Method: 引入KKT点嵌入原理，通过线性等距变换将小网络的KKT点嵌入到大网络中，并分析梯度流训练的动力学行为。

Result: 证明了神经元分裂在两层和深层同质网络中的有效性，并展示了训练过程中映射关系的动态保持。

Conclusion: 研究揭示了网络宽度、参数冗余以及优化解结构之间的联系，为同质网络的优化提供了新视角。

Abstract: Understanding the convergence points and optimization landscape of neural
networks is crucial, particularly for homogeneous networks where
Karush-Kuhn-Tucker (KKT) points of the associated maximum-margin problem often
characterize solutions. This paper investigates the relationship between such
KKT points across networks of different widths generated via neuron splitting.
We introduce and formalize the \textbf{KKT point embedding principle},
establishing that KKT points of a homogeneous network's max-margin problem
($P_{\Phi}$) can be embedded into the KKT points of a larger network's problem
($P_{\tilde{\Phi}}$) via specific linear isometric transformations
corresponding to neuron splitting. We rigorously prove this principle holds for
neuron splitting in both two-layer and deep homogeneous networks. Furthermore,
we connect this static embedding to the dynamics of gradient flow training with
smooth losses. We demonstrate that trajectories initiated from appropriately
mapped points remain mapped throughout training and that the resulting
$\omega$-limit sets of directions are correspondingly mapped ($T(L(\theta(0)))
= L(\boldsymbol{\eta}(0))$), thereby preserving the alignment with KKT
directions dynamically when directional convergence occurs. Our findings offer
insights into the effects of network width, parameter redundancy, and the
structural connections between solutions found via optimization in homogeneous
networks of varying sizes.

</details>

### [195] [Fixed Point Explainability](https://arxiv.org/abs/2505.12421)
*Emanuele La Malfa,Jon Vadillo,Marco Molinari,Michael Wooldridge*

Main category: cs.LG

TLDR: 本文提出了一种基于“为什么回归”原则的固定点解释形式，通过递归应用评估模型与其解释器之间的稳定性。


<details>
  <summary>Details</summary>
Motivation: 揭示隐藏的模型行为和解释弱点，确保解释满足最小性、稳定性和忠实性。

Method: 定义了多种解释器（从基于特征的工具到稀疏自编码器等机制工具）的收敛条件。

Result: 报告了定量和定性的实验结果。

Conclusion: 固定点解释能有效揭示模型与解释器之间的交互稳定性。

Abstract: This paper introduces a formal notion of fixed point explanations, inspired
by the "why regress" principle, to assess, through recursive applications, the
stability of the interplay between a model and its explainer. Fixed point
explanations satisfy properties like minimality, stability, and faithfulness,
revealing hidden model behaviours and explanatory weaknesses. We define
convergence conditions for several classes of explainers, from feature-based to
mechanistic tools like Sparse AutoEncoders, and we report quantitative and
qualitative results.

</details>

### [196] [A Learning-Based Ansatz Satisfying Boundary Conditions in Variational Problems](https://arxiv.org/abs/2505.12430)
*Rafael Florencio,Julio Guerrero*

Main category: cs.LG

TLDR: 提出了一种新的Deep Ritz Method变体，通过满足边界条件的ansatz，解决了传统方法因惩罚项导致的误导性结果，同时降低了复杂度并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 传统Deep Ritz Method因神经网络不满足边界条件而引入惩罚项，可能导致优化过程中的误导性结果。

Method: 提出了一种新的ansatz，其设计天然满足变分问题的边界条件，无需额外惩罚项。

Result: 新方法消除了误导性结果，降低了复杂度，同时保持了准确性。

Conclusion: 该方法在解决变分问题中具有实际有效性，优于传统Deep Ritz Method。

Abstract: Recently, innovative adaptations of the Ritz Method incorporating deep
learning have been developed, known as the Deep Ritz Method. This approach
employs a neural network as the test function for variational problems.
However, the neural network does not inherently satisfy the boundary conditions
of the variational problem. To resolve this issue, the Deep Ritz Method
introduces a penalty term into the functional of the variational problem, which
can lead to misleading results during the optimization process. In this work,
an ansatz is proposed that inherently satisfies the boundary conditions of the
variational problem. The results demonstrate that the proposed ansatz not only
eliminates misleading outcomes but also reduces complexity while maintaining
accuracy, showcasing its practical effectiveness in addressing variational
problems.

</details>

### [197] [Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning](https://arxiv.org/abs/2505.12432)
*Zirun Guo,Minjie Hong,Tao Jin*

Main category: cs.LG

TLDR: Observe-R1是一个新颖的框架，通过逐步学习和多模态格式约束增强多模态大语言模型的推理能力，实验证明其优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 探索如何将强化学习应用于多模态数据，以提升多模态大语言模型的推理能力。

Method: 提出逐步学习范式，构建NeuraLadder数据集，引入多模态格式约束和奖励系统，动态加权机制。

Result: 在Qwen2.5-VL模型上表现优于其他推理模型，推理链更清晰简洁。

Conclusion: Observe-R1框架有效提升了多模态大语言模型的推理能力，具有鲁棒性和泛化性。

Abstract: Reinforcement Learning (RL) has shown promise in improving the reasoning
abilities of Large Language Models (LLMs). However, the specific challenges of
adapting RL to multimodal data and formats remain relatively unexplored. In
this work, we present Observe-R1, a novel framework aimed at enhancing the
reasoning capabilities of multimodal large language models (MLLMs). We draw
inspirations from human learning progression--from simple to complex and easy
to difficult, and propose a gradual learning paradigm for MLLMs. To this end,
we construct the NeuraLadder dataset, which is organized and sampled according
to the difficulty and complexity of data samples for RL training. To tackle
multimodal tasks, we introduce a multimodal format constraint that encourages
careful observation of images, resulting in enhanced visual abilities and
clearer and more structured responses. Additionally, we implement a bonus
reward system that favors concise, correct answers within a length constraint,
alongside a dynamic weighting mechanism that prioritizes uncertain and
medium-difficulty problems, ensuring that more informative samples have a
greater impact on training. Our experiments with the Qwen2.5-VL-3B and
Qwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that
Observe-R1 outperforms a series of larger reasoning models on both reasoning
and general benchmarks, achieving superior clarity and conciseness in reasoning
chains. Ablation studies validate the effectiveness of our strategies,
highlighting the robustness and generalization of our approach. The dataset and
code will be released at https://github.com/zrguo/Observe-R1.

</details>

### [198] [SGDPO: Self-Guided Direct Preference Optimization for Language Model Alignment](https://arxiv.org/abs/2505.12435)
*Wenqiao Zhu,Ji Liu,Lulu Wang,Jun Wu,Yulun Zhang*

Main category: cs.LG

TLDR: 论文提出了一种名为SGDPO的新算法，通过引入引导项改进DPO，提升了生成人类偏好响应的能力。


<details>
  <summary>Details</summary>
Motivation: DPO在调整LLMs与人类价值观对齐时存在生成能力不足和结果不稳定的问题。

Method: 提出SGDPO算法，加入引导项以优化梯度流，精细控制奖励更新。

Result: 实验证明SGDPO显著优于DPO（最高提升9.19%）。

Conclusion: SGDPO通过理论分析和实验验证，有效解决了DPO的局限性。

Abstract: Direct Preference Optimization (DPO) is broadly utilized for aligning Large
Language Models (LLMs) with human values because of its flexibility. Despite
its effectiveness, it has been observed that the capability of DPO to generate
human-preferred response is limited and the results of DPO are far from
resilient. To address these limitations, in this paper we propose a novel
Self-Guided Direct Preference Optimization algorithm, i.e., SGDPO, which
incorporates a pilot term to steer the gradient flow during the optimization
process, allowing for fine-grained control over the updates of chosen and
rejected rewards. We provide a detailed theoretical analysis of our proposed
method and elucidate its operational mechanism. Furthermore, we conduct
comprehensive experiments on various models and benchmarks. The extensive
experimental results demonstrate the consistency between the empirical results
and our theoretical analysis and confirm the effectiveness of our proposed
approach (up to 9.19% higher score).

</details>

### [199] [Addressing the Scarcity of Benchmarks for Graph XAI](https://arxiv.org/abs/2505.12437)
*Michele Fontanesi,Alessio Micheli,Marco Podda,Domenico Tortorella*

Main category: cs.LG

TLDR: 提出了一种自动化构建图分类XAI基准的方法，并提供了15个现成基准及生成2000多个基准的代码，用于评估图解释器的效果。


<details>
  <summary>Details</summary>
Motivation: 图神经网络的决策过程不透明，阻碍了其在安全关键应用中的部署，而现有XAI基准数据集稀缺且多为合成或专家手工标注。

Method: 提出了一种从真实数据集中自动化构建图分类XAI基准的通用方法。

Result: 提供了15个现成基准和生成2000多个基准的代码，并通过实验评估了流行图解释器的效果。

Conclusion: 该方法解决了XAI基准稀缺问题，为图解释器的评估提供了丰富资源。

Abstract: While Graph Neural Networks (GNNs) have become the de facto model for
learning from structured data, their decisional process remains opaque to the
end user, undermining their deployment in safety-critical applications. In the
case of graph classification, Explainable Artificial Intelligence (XAI)
techniques address this major issue by identifying sub-graph motifs that
explain predictions. However, advancements in this field are hindered by a
chronic scarcity of benchmark datasets with known ground-truth motifs to assess
the explanations' quality. Current graph XAI benchmarks are limited to
synthetic data or a handful of real-world tasks hand-curated by domain experts.
In this paper, we propose a general method to automate the construction of XAI
benchmarks for graph classification from real-world datasets. We provide both
15 ready-made benchmarks, as well as the code to generate more than 2000
additional XAI benchmarks with our method. As a use case, we employ our
benchmarks to assess the effectiveness of some popular graph explainers.

</details>

### [200] [AltLoRA: Towards Better Gradient Approximation in Low-Rank Adaptation with Alternating Projections](https://arxiv.org/abs/2505.12455)
*Xin Yu,Yujia Wang,Jinghui Chen,Lingzhou Xue*

Main category: cs.LG

TLDR: AltLoRA是一种交替投影方法，解决了LoRA及其变体在低秩空间中梯度近似和动量集成的问题，同时保持内存效率。


<details>
  <summary>Details</summary>
Motivation: LoRA及其变体在微调大型语言模型时存在性能不足和内存开销问题，尤其是梯度近似和动量集成的挑战。

Method: 提出AltLoRA，采用交替投影方法，避免联合更新设计带来的梯度近似困难，并集成动量而不增加内存复杂度。

Result: 理论分析证明AltLoRA具有收敛保证，实验表明其在多个任务中优于LoRA及其变体，接近全微调性能且保持内存效率。

Conclusion: AltLoRA有效解决了低秩适应中的性能与内存效率问题，为大型语言模型微调提供了更优方案。

Abstract: Low-Rank Adaptation (LoRA) has emerged as an effective technique for reducing
memory overhead in fine-tuning large language models. However, it often suffers
from sub-optimal performance compared with full fine-tuning since the update is
constrained in the low-rank space. Recent variants such as LoRA-Pro attempt to
mitigate this by adjusting the gradients of the low-rank matrices to
approximate the full gradient. However, LoRA-Pro's solution is not unique, and
different solutions can lead to significantly varying performance in ablation
studies. Besides, to incorporate momentum or adaptive optimization design,
approaches like LoRA-Pro must first compute the equivalent gradient, causing a
higher memory cost close to full fine-tuning. A key challenge remains in
integrating momentum properly into the low-rank space with lower memory cost.
In this work, we propose AltLoRA, an alternating projection method that avoids
the difficulties in gradient approximation brought by the joint update design,
meanwhile integrating momentum without higher memory complexity. Our
theoretical analysis provides convergence guarantees and further shows that
AltLoRA enables stable feature learning and robustness to transformation
invariance. Extensive experiments across multiple tasks demonstrate that
AltLoRA outperforms LoRA and its variants, narrowing the gap toward full
fine-tuning while preserving superior memory efficiency.

</details>

### [201] [UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection](https://arxiv.org/abs/2505.12457)
*Yang Zhao,Kai Xiong,Xiao Ding,Li Du,YangouOuyang,Zhouhao Sun,Jiannan Guan,Wenbin Zhang,Bin Liu,Dong Hu,Bing Qin,Ting Liu*

Main category: cs.LG

TLDR: UFO-RL通过单次不确定性估计高效选择数据，显著提升RL训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统多采样方法计算成本高，LLMs在潜在理解区域（ZPD）学习效果最佳。

Method: 提出UFO-RL框架，利用单次不确定性估计快速评估数据价值，选择ZPD内数据训练。

Result: 仅用10%数据即可达到或超越全数据训练效果，训练时间减少16倍，稳定性与泛化性提升。

Conclusion: UFO-RL为LLMs的RL微调提供高效实用策略，聚焦高价值数据。

Abstract: Scaling RL for LLMs is computationally expensive, largely due to
multi-sampling for policy optimization and evaluation, making efficient data
selection crucial. Inspired by the Zone of Proximal Development (ZPD) theory,
we hypothesize LLMs learn best from data within their potential comprehension
zone. Addressing the limitation of conventional, computationally intensive
multi-sampling methods for data assessment, we introduce UFO-RL. This novel
framework uses a computationally efficient single-pass uncertainty estimation
to identify informative data instances, achieving up to 185x faster data
evaluation. UFO-RL leverages this metric to select data within the estimated
ZPD for training. Experiments show that training with just 10% of data selected
by UFO-RL yields performance comparable to or surpassing full-data training,
reducing overall training time by up to 16x while enhancing stability and
generalization. UFO-RL offers a practical and highly efficient strategy for
scaling RL fine-tuning of LLMs by focusing learning on valuable data.

</details>

### [202] [A Case for Library-Level k-Means Binning in Histogram Gradient-Boosted Trees](https://arxiv.org/abs/2505.12460)
*Asher Labovich*

Main category: cs.LG

TLDR: 论文提出用k-means离散化替代传统分位数分箱策略，以提升梯度提升决策树（GBDT）的性能，尤其在回归任务和有限分箱预算下表现显著。


<details>
  <summary>Details</summary>
Motivation: 传统分位数分箱策略可能忽略关键边界值，影响预测性能。

Method: 采用k-means离散化方法，初始化为分位数分箱，并在33个OpenML任务和合成数据集上测试。

Result: 回归任务中k-means无显著损失，且在部分数据集上表现优异（如MSE下降55%）；分类任务中两者表现相当。合成实验显示MSE提升显著（>20%）。

Conclusion: 建议将k-means作为默认分箱方法，尤其在回归任务和有限预算场景下，因其性能稳定且提升潜力大。

Abstract: Modern gradient-boosted decision trees (GBDTs) accelerate split finding with
histogram-based binning, which reduces complexity from O(N) to O(B) given a
fixed bin budget B. However, the predominant quantile binning strategy-designed
to distribute data points evenly among bins-may overlook critical boundary
values that could enhance predictive performance. In this work, we propose
replacing quantile binning with a k-means discretizer initialized with quantile
bins. We test this swap on 33 OpenML tasks plus synthetics that control for
modality, skew, and bin budget. Across 18 regression datasets, k-means shows no
statistically significant losses at the 5% level and wins in four cases-most
strikingly a 55% MSE drop on one particularly skewed dataset-even though
k-means' mean reciprocal rank (MRR) is slightly lower (0.65 vs 0.72). On the 15
classification datasets the two methods are statistically tied (MRR 0.70 vs
0.68) with gaps $\leq$0.2 pp. Synthetic experiments confirm consistently large
MSE gains-typically >20% and rising to 90% as outlier magnitude increases or
bin budget drops. We find that k-means keeps error on par with exact splitting
when extra cuts add little value, yet still recovers key split points that
quantile overlooks. As such, we advocate for a built-in bin_method=k-means
flag, especially in regression tasks and in tight-budget settings such as the
32-64-bin GPU regime-because it is a "safe default" with large upside, yet adds
only a one-off, cacheable overhead ($\approx$ 2s to bin 10M rows on one core).

</details>

### [203] [A Finite-Sample Analysis of Distributionally Robust Average-Reward Reinforcement Learning](https://arxiv.org/abs/2505.12462)
*Zachary Roch,Chi Zhang,George Atia,Yue Wang*

Main category: cs.LG

TLDR: 提出了Robust Halpern Iteration (RHI)，首个具有有限样本复杂度保证的算法，填补了鲁棒强化学习在平均奖励准则下的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅提供渐近保证，缺乏有限样本分析，限制了鲁棒强化学习在数据受限场景中的实际应用。

Method: 提出RHI算法，适用于标准不确定性集合（如污染集和ℓp范数球），无需先验知识。

Result: RHI以近最优样本复杂度实现ε-最优策略，首次给出多项式样本复杂度保证。

Conclusion: RHI显著提升了鲁棒平均奖励方法在复杂实际问题中的实用性。

Abstract: Robust reinforcement learning (RL) under the average-reward criterion is
crucial for long-term decision making under potential environment mismatches,
yet its finite-sample complexity study remains largely unexplored. Existing
works offer algorithms with asymptotic guarantees, but the absence of
finite-sample analysis hinders its principled understanding and practical
deployment, especially in data-limited settings. We close this gap by proposing
Robust Halpern Iteration (RHI), the first algorithm with provable finite-sample
complexity guarantee. Under standard uncertainty sets -- including
contamination sets and $\ell_p$-norm balls -- RHI attains an $\epsilon$-optimal
policy with near-optimal sample complexity of $\tilde{\mathcal
O}\left(\frac{SA\mathcal H^{2}}{\epsilon^{2}}\right)$, where $S$ and $A$ denote
the numbers of states and actions, and $\mathcal H$ is the robust optimal bias
span. This result gives the first polynomial sample complexity guarantee for
robust average-reward RL. Moreover, our RHI's independence from prior knowledge
distinguishes it from many previous average-reward RL studies. Our work thus
constitutes a significant advancement in enhancing the practical applicability
of robust average-reward methods to complex, real-world problems.

</details>

### [204] [Resolving Latency and Inventory Risk in Market Making with Reinforcement Learning](https://arxiv.org/abs/2505.12465)
*Junzhe Jiang,Chang Yang,Xinrun Wang,Zhiming Li,Xiao Huang,Bo Li*

Main category: cs.LG

TLDR: 论文提出了一种名为Relaver的基于强化学习的方法，解决了市场做市中因延迟导致的订单取消和库存风险问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法忽视延迟对市场做市的影响，导致订单取消和库存积累风险，无法应用于实际场景。

Method: 构建了包含随机延迟的模拟环境，提出Relaver方法，结合动态规划和市场趋势预测优化策略。

Result: 在四个真实数据集上验证，Relaver显著提升了性能。

Conclusion: Relaver有效解决了延迟和库存风险问题，提升了市场做市策略的实用性。

Abstract: The latency of the exchanges in Market Making (MM) is inevitable due to
hardware limitations, system processing times, delays in receiving data from
exchanges, the time required for order transmission to reach the market, etc.
Existing reinforcement learning (RL) methods for Market Making (MM) overlook
the impact of these latency, which can lead to unintended order cancellations
due to price discrepancies between decision and execution times and result in
undesired inventory accumulation, exposing MM traders to increased market risk.
Therefore, these methods cannot be applied in real MM scenarios. To address
these issues, we first build a realistic MM environment with random delays of
30-100 milliseconds for order placement and market information reception, and
implement a batch matching mechanism that collects orders within every 500
milliseconds before matching them all at once, simulating the batch auction
mechanisms adopted by some exchanges. Then, we propose Relaver, an RL-based
method for MM to tackle the latency and inventory risk issues. The three main
contributions of Relaver are: i) we introduce an augmented state-action space
that incorporates order hold time alongside price and volume, enabling Relaver
to optimize execution strategies under latency constraints and time-priority
matching mechanisms, ii) we leverage dynamic programming (DP) to guide the
exploration of RL training for better policies, iii) we train a market trend
predictor, which can guide the agent to intelligently adjust the inventory to
reduce the risk. Extensive experiments and ablation studies on four real-world
datasets demonstrate that \textsc{Relaver} significantly improves the
performance of state-of-the-art RL-based MM strategies across multiple metrics.

</details>

### [205] [Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning](https://arxiv.org/abs/2505.12477)
*Hugues Van Assel,Mark Ibrahim,Tommaso Biancalani,Aviv Regev,Randall Balestriero*

Main category: cs.LG

TLDR: 论文比较了自监督学习中重建和联合嵌入两种范式，揭示了它们的核心机制和适用场景，指出联合嵌入方法在特定条件下更优。


<details>
  <summary>Details</summary>
Motivation: 为自监督学习中的重建和联合嵌入两种范式提供清晰的选择指南，揭示其核心区别和适用条件。

Method: 通过闭式解分析两种范式，研究数据增强对表示学习的影响，并比较其对无关特征的依赖程度。

Result: 发现联合嵌入方法对无关特征的依赖更弱，在无关特征影响较大时表现更优。

Conclusion: 联合嵌入方法在特定场景下优于重建方法，为实践提供了理论支持。

Abstract: Reconstruction and joint embedding have emerged as two leading paradigms in
Self Supervised Learning (SSL). Reconstruction methods focus on recovering the
original sample from a different view in input space. On the other hand, joint
embedding methods align the representations of different views in latent space.
Both approaches offer compelling advantages, yet practitioners lack clear
guidelines for choosing between them. In this work, we unveil the core
mechanisms that distinguish each paradigm. By leveraging closed form solutions
for both approaches, we precisely characterize how the view generation process,
e.g. data augmentation, impacts the learned representations. We then
demonstrate that, unlike supervised learning, both SSL paradigms require a
minimal alignment between augmentations and irrelevant features to achieve
asymptotic optimality with increasing sample size. Our findings indicate that
in scenarios where these irrelevant features have a large magnitude, joint
embedding methods are preferable because they impose a strictly weaker
alignment condition compared to reconstruction based methods. These results not
only clarify the trade offs between the two paradigms but also substantiate the
empirical success of joint embedding approaches on real world challenging
datasets.

</details>

### [206] [$γ$-FedHT: Stepsize-Aware Hard-Threshold Gradient Compression in Federated Learning](https://arxiv.org/abs/2505.12479)
*Rongwei Lu,Yutong Jiang,Jinrui Zhang,Chunyang Li,Yifei Zhu,Bin Chen,Zhi Wang*

Main category: cs.LG

TLDR: 论文提出了一种名为γ-FedHT的低成本压缩器，用于解决联邦学习中硬阈值压缩导致的精度下降问题，并通过误差反馈保证收敛性。


<details>
  <summary>Details</summary>
Motivation: 硬阈值压缩在非独立同分布数据集和递减步长下会导致精度下降，因此需要一种更高效的压缩方法。

Method: 提出γ-FedHT，一种步长感知的低成本压缩器，结合误差反馈机制。

Result: 实验表明，γ-FedHT在相同通信流量下比Top-k方法精度提升高达7.42%。

Conclusion: γ-FedHT在保证收敛性的同时显著提升了联邦学习的精度。

Abstract: Gradient compression can effectively alleviate communication bottlenecks in
Federated Learning (FL). Contemporary state-of-the-art sparse compressors, such
as Top-$k$, exhibit high computational complexity, up to
$\mathcal{O}(d\log_2{k})$, where $d$ is the number of model parameters. The
hard-threshold compressor, which simply transmits elements with absolute values
higher than a fixed threshold, is thus proposed to reduce the complexity to
$\mathcal{O}(d)$. However, the hard-threshold compression causes accuracy
degradation in FL, where the datasets are non-IID and the stepsize $\gamma$ is
decreasing for model convergence. The decaying stepsize reduces the updates and
causes the compression ratio of the hard-threshold compression to drop rapidly
to an aggressive ratio. At or below this ratio, the model accuracy has been
observed to degrade severely. To address this, we propose $\gamma$-FedHT, a
stepsize-aware low-cost compressor with Error-Feedback to guarantee
convergence. Given that the traditional theoretical framework of FL does not
consider Error-Feedback, we introduce the fundamental conversation of
Error-Feedback. We prove that $\gamma$-FedHT has the convergence rate of
$\mathcal{O}(\frac{1}{T})$ ($T$ representing total training iterations) under
$\mu$-strongly convex cases and $\mathcal{O}(\frac{1}{\sqrt{T}})$ under
non-convex cases, \textit{same as FedAVG}. Extensive experiments demonstrate
that $\gamma$-FedHT improves accuracy by up to $7.42\%$ over Top-$k$ under
equal communication traffic on various non-IID image datasets.

</details>

### [207] [CPGD: Toward Stable Rule-based Reinforcement Learning for Language Models](https://arxiv.org/abs/2505.12504)
*Zongkai Liu,Fanqing Meng,Lingxiao Du,Zhixiang Zhou,Chao Yu,Wenqi Shao,Qiaosheng Zhang*

Main category: cs.LG

TLDR: 提出了一种名为CPGD的新算法，通过动态正则化策略更新和剪裁机制，解决了现有RL方法在语言模型训练中的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法（如GRPO、REINFORCE++和RLOO）在语言模型训练中存在不稳定性问题，如策略更新过大和剪裁不当导致训练崩溃。

Method: CPGD引入基于KL散度的策略漂移约束动态正则化策略更新，并利用对数比值的剪裁机制防止过度更新。

Result: 实验表明CPGD显著提高了性能并保持了训练稳定性，优于现有方法。

Conclusion: CPGD为语言模型的后训练提供了一种稳定且高效的RL替代方案，代码已开源。

Abstract: Recent advances in rule-based reinforcement learning (RL) have significantly
improved the reasoning capability of language models (LMs) with rule-based
rewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO --
often suffer from training instability, where large policy updates and improper
clipping can lead to training collapse. To address this issue, we propose
Clipped Policy Gradient Optimization with Policy Drift (CPGD), a novel
algorithm designed to stabilize policy learning in LMs. CPGD introduces a
policy drift constraint based on KL divergence to dynamically regularize policy
updates, and leverages a clip mechanism on the logarithm of the ratio to
prevent excessive policy updates. We provide theoretical justification for CPGD
and demonstrate through empirical analysis that it mitigates the instability
observed in prior approaches. Furthermore, we show that CPGD significantly
improves performance while maintaining training stability. Our implementation
balances theoretical rigor with practical usability, offering a robust
alternative for RL in the post-training of LMs. We release our code at
https://github.com/ModalMinds/MM-EUREKA.

</details>

### [208] [Unsupervised Invariant Risk Minimization](https://arxiv.org/abs/2505.12506)
*Yotam Norman,Ron Meir*

Main category: cs.LG

TLDR: 提出了一种无监督的IRM框架，通过特征分布对齐实现无标签数据的鲁棒表示学习，包括线性方法PICA和深度生成模型VIAE。


<details>
  <summary>Details</summary>
Motivation: 传统IRM依赖标签数据，本文旨在扩展无标签数据的鲁棒表示学习。

Method: 提出PICA（线性方法）和VIAE（深度生成模型），基于无监督结构因果模型。

Result: 在合成数据和MNIST上验证了方法的有效性，能捕捉不变结构并泛化到不同环境。

Conclusion: 无监督IRM框架在无标签数据中实现了鲁棒表示学习，具有环境条件生成和干预能力。

Abstract: We propose a novel unsupervised framework for \emph{Invariant Risk
Minimization} (IRM), extending the concept of invariance to settings where
labels are unavailable. Traditional IRM methods rely on labeled data to learn
representations that are robust to distributional shifts across environments.
In contrast, our approach redefines invariance through feature distribution
alignment, enabling robust representation learning from unlabeled data. We
introduce two methods within this framework: Principal Invariant Component
Analysis (PICA), a linear method that extracts invariant directions under
Gaussian assumptions, and Variational Invariant Autoencoder (VIAE), a deep
generative model that disentangles environment-invariant and
environment-dependent latent factors. Our approach is based on a novel
``unsupervised'' structural causal model and supports environment-conditioned
sample-generation and intervention. Empirical evaluations on synthetic dataset
and modified versions of MNIST demonstrate the effectiveness of our methods in
capturing invariant structure, preserving relevant information, and
generalizing across environments without access to labels.

</details>

### [209] [InnateCoder: Learning Programmatic Options with Foundation Models](https://arxiv.org/abs/2505.12508)
*Rubens O. Moraes,Quazi Asif Sadmine,Hendrik Baier,Levi H. S. Lelis*

Main category: cs.LG

TLDR: InnateCoder利用基础模型中的人类知识，通过零样本学习生成程序化策略，提高强化学习样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习需从零开始学习，效率低；InnateCoder旨在利用人类知识加速学习过程。

Method: 从基础模型中零样本学习选项，组合成复杂程序化策略。

Result: 在MicroRTS和Karel the Robot中验证，样本效率优于无选项或经验学习版本。

Conclusion: InnateCoder通过利用人类知识显著提升强化学习效率。

Abstract: Outside of transfer learning settings, reinforcement learning agents start
their learning process from a clean slate. As a result, such agents have to go
through a slow process to learn even the most obvious skills required to solve
a problem. In this paper, we present InnateCoder, a system that leverages human
knowledge encoded in foundation models to provide programmatic policies that
encode "innate skills" in the form of temporally extended actions, or options.
In contrast to existing approaches to learning options, InnateCoder learns them
from the general human knowledge encoded in foundation models in a zero-shot
setting, and not from the knowledge the agent gains by interacting with the
environment. Then, InnateCoder searches for a programmatic policy by combining
the programs encoding these options into larger and more complex programs. We
hypothesized that InnateCoder's way of learning and using options could improve
the sampling efficiency of current methods for learning programmatic policies.
Empirical results in MicroRTS and Karel the Robot support our hypothesis, since
they show that InnateCoder is more sample efficient than versions of the system
that do not use options or learn them from experience.

</details>

### [210] [Towards Budget-Friendly Model-Agnostic Explanation Generation for Large Language Models](https://arxiv.org/abs/2505.12509)
*Junhao Liu,Haonan Yu,Xin Zhang*

Main category: cs.LG

TLDR: 论文提出了一种通过预算友好模型生成忠实解释的新方法，解决了现有模型无关技术因多次调用LLM导致高成本的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的广泛应用，解释其预测结果的需求日益迫切，但现有模型无关技术因需多次调用LLM而成本高昂。

Method: 通过预算友好模型采样生成忠实解释，并进行实证研究验证其有效性。

Result: 实验表明，代理解释不仅成本低，还能在下游任务中表现良好。

Conclusion: 该研究为LLM的模型无关解释方法提供了新范式，结合预算友好模型的信息。

Abstract: With Large language models (LLMs) becoming increasingly prevalent in various
applications, the need for interpreting their predictions has become a critical
challenge. As LLMs vary in architecture and some are closed-sourced,
model-agnostic techniques show great promise without requiring access to the
model's internal parameters. However, existing model-agnostic techniques need
to invoke LLMs many times to gain sufficient samples for generating faithful
explanations, which leads to high economic costs. In this paper, we show that
it is practical to generate faithful explanations for large-scale LLMs by
sampling from some budget-friendly models through a series of empirical
studies. Moreover, we show that such proxy explanations also perform well on
downstream tasks. Our analysis provides a new paradigm of model-agnostic
explanation methods for LLMs, by including information from budget-friendly
models.

</details>

### [211] [Scalable Strategies for Continual Learning with Replay](https://arxiv.org/abs/2505.12512)
*Truman Hickok*

Main category: cs.LG

TLDR: 论文探讨了持续学习中的关键挑战，提出了结合低秩适应、整合和顺序合并的方法，显著提升了可扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 未来深度学习模型需要持续学习，但现有方法如重放成本高且未充分利用多任务微调技术。

Method: 结合低秩适应、整合（减少重放样本）和顺序合并（任务算术的变体），构建高效工具集。

Result: 方法显著减少重放样本需求（55%），并在持续学习中表现出协同效应，优于独立方法。

Conclusion: 提出的工具集高效且可扩展，为持续学习提供了新方向。

Abstract: Future deep learning models will be distinguished by systems that perpetually
learn through interaction, imagination, and cooperation, blurring the line
between training and inference. This makes continual learning a critical
challenge, as methods that efficiently maximize bidirectional transfer across
learning trajectories will be essential. Replay is on track to play a
foundational role in continual learning, allowing models to directly reconcile
new information with past knowledge. In practice, however, replay is quite
unscalable, doubling the cost of continual learning when applied naively.
Moreover, the continual learning literature has not fully synchronized with the
multi-task fine-tuning literature, having not fully integrated highly scalable
techniques like model merging and low rank adaptation into a replay-enabled
toolset that can produce a unified model in the face of many sequential tasks.
In this paper, we begin by applying and analyzing low rank adaptation in a
continual learning setting. Next, we introduce consolidation, a phasic approach
to replay which leads to up to 55\% less replay samples being needed for a
given performance target. Then, we propose sequential merging, an offshoot of
task arithmetic which is tailored to the continual learning setting and is
shown to work well in combination with replay. Finally, we demonstrate that the
developed strategies can operate synergistically, resulting in a highly
scalable toolset that outperforms standalone variants.

</details>

### [212] [Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought](https://arxiv.org/abs/2505.12514)
*Hanlin Zhu,Shibo Hao,Zhiting Hu,Jiantao Jiao,Stuart Russell,Yuandong Tian*

Main category: cs.LG

TLDR: 论文证明了连续CoTs在解决有向图可达性问题时优于离散CoTs，通过并行广度优先搜索（BFS）实现更高效的求解。


<details>
  <summary>Details</summary>
Motivation: 理解连续CoTs为何在推理任务中优于离散CoTs，特别是在有向图可达性问题中。

Method: 使用两层Transformer和连续CoTs，构建并行BFS的叠加状态。

Result: 连续CoTs可在D步内解决问题（D为图直径），而离散CoTs需要O(n²)步。

Conclusion: 连续CoTs通过并行搜索显著提升效率，且训练中自动涌现叠加状态，无需显式监督。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in many
applications, including challenging reasoning problems via chain-of-thoughts
(CoTs) techniques that generate ``thinking tokens'' before answering the
questions. While existing theoretical works demonstrate that CoTs with discrete
tokens boost the capability of LLMs, recent work on continuous CoTs lacks a
theoretical understanding of why it outperforms discrete counterparts in
various reasoning tasks such as directed graph reachability, a fundamental
graph reasoning problem that includes many practical domain applications as
special cases. In this paper, we prove that a two-layer transformer with $D$
steps of continuous CoTs can solve the directed graph reachability problem,
where $D$ is the diameter of the graph, while the best known result of
constant-depth transformers with discrete CoTs requires $O(n^2)$ decoding steps
where $n$ is the number of vertices ($D<n$). In our construction, each
continuous thought vector is a superposition state that encodes multiple search
frontiers simultaneously (i.e., parallel breadth-first search (BFS)), while
discrete CoTs must choose a single path sampled from the superposition state,
which leads to sequential search that requires many more steps and may be
trapped into local solutions. We also performed extensive experiments to verify
that our theoretical construction aligns well with the empirical solution
obtained via training dynamics. Notably, encoding of multiple search frontiers
as a superposition state automatically emerges in training continuous CoTs,
without explicit supervision to guide the model to explore multiple paths
simultaneously.

</details>

### [213] [Energy-Aware Deep Learning on Resource-Constrained Hardware](https://arxiv.org/abs/2505.12523)
*Josh Millar,Hamed Haddadi,Anil Madhavapeddy*

Main category: cs.LG

TLDR: 本文综述了在物联网和移动设备上使用深度学习时如何优化能源消耗的方法，强调了能源感知技术的重要性及其局限性。


<details>
  <summary>Details</summary>
Motivation: 物联网和移动设备因能源限制需要优化深度学习推理和训练，以延长电池寿命或支持间歇性运行。

Method: 综述了能源感知方法，包括其方法论、对能耗和系统效率的影响，以及支持的硬件平台和应用场景。

Result: 总结了现有方法的优缺点，并指出了未来研究方向。

Conclusion: 本文为能源受限计算领域的未来研究提供了清晰的综述和基础。

Abstract: The use of deep learning (DL) on Internet of Things (IoT) and mobile devices
offers numerous advantages over cloud-based processing. However, such devices
face substantial energy constraints to prolong battery-life, or may even
operate intermittently via energy-harvesting. Consequently,
\textit{energy-aware} approaches for optimizing DL inference and training on
such resource-constrained devices have garnered recent interest. We present an
overview of such approaches, outlining their methodologies, implications for
energy consumption and system-level efficiency, and their limitations in terms
of supported network types, hardware platforms, and application scenarios. We
hope our review offers a clear synthesis of the evolving energy-aware DL
landscape and serves as a foundation for future research in energy-constrained
computing.

</details>

### [214] [Never Skip a Batch: Continuous Training of Temporal GNNs via Adaptive Pseudo-Supervision](https://arxiv.org/abs/2505.12526)
*Alexander Panyshev,Dmitry Vinichenko,Oleg Travkin,Roman Alferov,Alexey Zaytsev*

Main category: cs.LG

TLDR: 论文提出了一种名为HAL的方法，通过历史标签分布生成伪目标，解决了动态图中梯度稀疏问题，显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 动态图中由于监督信号稀疏导致训练效率低下，需要一种方法在不改变架构的情况下加速收敛。

Method: 提出HAL方法，利用历史节点交互生成伪标签，动态丰富训练批次以减少梯度方差。

Result: 实验表明HAL将TGNv2训练速度提升15倍，同时保持性能。

Conclusion: HAL是一种高效、轻量且架构无关的解决方案，适用于时序图学习中的标签稀疏问题。

Abstract: Temporal Graph Networks (TGNs), while being accurate, face significant
training inefficiencies due to irregular supervision signals in dynamic graphs,
which induce sparse gradient updates. We first theoretically establish that
aggregating historical node interactions into pseudo-labels reduces gradient
variance, accelerating convergence. Building on this analysis, we propose
History-Averaged Labels (HAL), a method that dynamically enriches training
batches with pseudo-targets derived from historical label distributions. HAL
ensures continuous parameter updates without architectural modifications by
converting idle computation into productive learning steps. Experiments on the
Temporal Graph Benchmark (TGB) validate our findings and an assumption about
slow change of user preferences: HAL accelerates TGNv2 training by up to 15x
while maintaining competitive performance. Thus, this work offers an efficient,
lightweight, architecture-agnostic, and theoretically motivated solution to
label sparsity in temporal graph learning.

</details>

### [215] [Enforcing Fairness Where It Matters: An Approach Based on Difference-of-Convex Constraints](https://arxiv.org/abs/2505.12530)
*Yutian He,Yankun Huang,Yao Yao,Qihang Lin*

Main category: cs.LG

TLDR: 论文提出了一种部分公平的机器学习框架，专注于特定分数范围内的公平性，而非全局公平，以平衡公平性与预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法追求全局公平性，但可能牺牲预测性能且不符合实际需求，因此需要一种更灵活的公平性解决方案。

Method: 提出了一种基于约束优化的方法，使用差异凸算法（IDCA）实现部分公平性，并引入统计指标评估特定分数范围的公平性。

Result: 实验表明，该框架在保持高预测性能的同时，能在关键分数范围内实现公平性。

Conclusion: 部分公平性框架为实际应用提供了更灵活的公平性解决方案，平衡了公平性与性能的需求。

Abstract: Fairness in machine learning has become a critical concern, particularly in
high-stakes applications. Existing approaches often focus on achieving full
fairness across all score ranges generated by predictive models, ensuring
fairness in both high and low-scoring populations. However, this stringent
requirement can compromise predictive performance and may not align with the
practical fairness concerns of stakeholders. In this work, we propose a novel
framework for building partially fair machine learning models, which enforce
fairness within a specific score range of interest, such as the middle range
where decisions are most contested, while maintaining flexibility in other
regions. We introduce two statistical metrics to rigorously evaluate partial
fairness within a given score range, such as the top 20%-40% of scores. To
achieve partial fairness, we propose an in-processing method by formulating the
model training problem as constrained optimization with difference-of-convex
constraints, which can be solved by an inexact difference-of-convex algorithm
(IDCA). We provide the complexity analysis of IDCA for finding a nearly KKT
point. Through numerical experiments on real-world datasets, we demonstrate
that our framework achieves high predictive performance while enforcing partial
fairness where it matters most.

</details>

### [216] [ChemPile: A 250GB Diverse and Curated Dataset for Chemical Foundation Models](https://arxiv.org/abs/2505.12534)
*Adrian Mirza,Nawaf Alampara,Martiño Ríos-García,Mohamed Abdelalim,Jack Butler,Bethany Connolly,Tunca Dogan,Marianna Nezhurina,Bünyamin Şen,Santosh Tirunagari,Mark Worrall,Adamo Young,Philippe Schwaller,Michael Pieler,Kevin Maik Jablonka*

Main category: cs.LG

TLDR: ChemPile是一个开放的化学数据集，包含750亿个经过专家精心整理的化学数据标记，旨在为化学科学中的通用模型提供训练和评估基础。


<details>
  <summary>Details</summary>
Motivation: 当前化学领域缺乏多样化、大规模、高质量的数据集，限制了基础模型的应用。ChemPile旨在填补这一空白，推动化学AI的发展。

Method: 通过整合多种化学表示形式（如SMILES、SELFIES等）、科学文本、可执行代码和化学图像，ChemPile模拟了人类学习化学的多元路径。

Result: ChemPile提供了标准化的训练、验证和测试分割，支持稳健的基准测试，并通过HuggingFace平台开放共享。

Conclusion: ChemPile有望成为化学AI发展的催化剂，推动下一代化学基础模型的开发。

Abstract: Foundation models have shown remarkable success across scientific domains,
yet their impact in chemistry remains limited due to the absence of diverse,
large-scale, high-quality datasets that reflect the field's multifaceted
nature. We present the ChemPile, an open dataset containing over 75 billion
tokens of curated chemical data, specifically built for training and evaluating
general-purpose models in the chemical sciences. The dataset mirrors the human
learning journey through chemistry -- from educational foundations to
specialized expertise -- spanning multiple modalities and content types
including structured data in diverse chemical representations (SMILES, SELFIES,
IUPAC names, InChI, molecular renderings), scientific and educational text,
executable code, and chemical images. ChemPile integrates foundational
knowledge (textbooks, lecture notes), specialized expertise (scientific
articles and language-interfaced data), visual understanding (molecular
structures, diagrams), and advanced reasoning (problem-solving traces and code)
-- mirroring how human chemists develop expertise through diverse learning
materials and experiences. Constructed through hundreds of hours of expert
curation, the ChemPile captures both foundational concepts and domain-specific
complexity. We provide standardized training, validation, and test splits,
enabling robust benchmarking. ChemPile is openly released via HuggingFace with
a consistent API, permissive license, and detailed documentation. We hope the
ChemPile will serve as a catalyst for chemical AI, enabling the development of
the next generation of chemical foundation models.

</details>

### [217] [Harnessing the Universal Geometry of Embeddings](https://arxiv.org/abs/2505.12540)
*Rishi Jha,Collin Zhang,Vitaly Shmatikov,John X. Morris*

Main category: cs.LG

TLDR: 提出了一种无需配对数据、编码器或预定义匹配的无监督文本嵌入翻译方法，将嵌入映射到通用潜在表示空间。


<details>
  <summary>Details</summary>
Motivation: 探索如何在无监督条件下实现不同向量空间之间的嵌入翻译，并验证其通用性。

Method: 通过无监督方法将任意嵌入翻译到通用潜在表示空间，支持不同架构、参数和训练数据的模型。

Result: 翻译后的嵌入在不同模型间具有高余弦相似性，验证了方法的有效性。

Conclusion: 该方法对向量数据库的安全性有重大影响，攻击者可能通过嵌入向量提取敏感信息。

Abstract: We introduce the first method for translating text embeddings from one vector
space to another without any paired data, encoders, or predefined sets of
matches. Our unsupervised approach translates any embedding to and from a
universal latent representation (i.e., a universal semantic structure
conjectured by the Platonic Representation Hypothesis). Our translations
achieve high cosine similarity across model pairs with different architectures,
parameter counts, and training datasets.
  The ability to translate unknown embeddings into a different space while
preserving their geometry has serious implications for the security of vector
databases. An adversary with access only to embedding vectors can extract
sensitive information about the underlying documents, sufficient for
classification and attribute inference.

</details>

### [218] [Alternators With Noise Models](https://arxiv.org/abs/2505.12544)
*Mohammad R. Rezaei,Adji Bousso Dieng*

Main category: cs.LG

TLDR: Alternator++是一种改进的Alternator模型，通过显式建模噪声项提升性能，在时间序列任务中表现优于多种基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统Alternator模型在时间依赖性数据建模中表现优异，但缺乏对噪声项的显式建模，限制了其灵活性。

Method: Alternator++结合Alternator损失和噪声匹配损失，显式建模潜在和观测轨迹的噪声项。

Result: 在密度估计、时间序列填补和预测任务中，Alternator++优于Mambas、ScoreGrad和Dyffusion等基线模型。

Conclusion: Alternator++通过噪声建模显著提升了传统Alternator的性能，适用于多种时间序列任务。

Abstract: Alternators have recently been introduced as a framework for modeling
time-dependent data. They often outperform other popular frameworks, such as
state-space models and diffusion models, on challenging time-series tasks. This
paper introduces a new Alternator model, called Alternator++, which enhances
the flexibility of traditional Alternators by explicitly modeling the noise
terms used to sample the latent and observed trajectories, drawing on the idea
of noise models from the diffusion modeling literature. Alternator++ optimizes
the sum of the Alternator loss and a noise-matching loss. The latter forces the
noise trajectories generated by the two noise models to approximate the noise
trajectories that produce the observed and latent trajectories. We demonstrate
the effectiveness of Alternator++ in tasks such as density estimation, time
series imputation, and forecasting, showing that it outperforms several strong
baselines, including Mambas, ScoreGrad, and Dyffusion.

</details>

### [219] [Beyond Accuracy: EcoL2 Metric for Sustainable Neural PDE Solvers](https://arxiv.org/abs/2505.12556)
*Taniya Kapoor,Abhishek Chandra,Anastasios Stamou,Stephen J Roberts*

Main category: cs.LG

TLDR: 论文提出了一种衡量PDE求解器碳排放的指标EcoL2，平衡模型精度与碳排放，适用于数据收集、训练和部署阶段。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习PDE求解器主要关注精度提升，而忽略了计算带来的碳排放问题。

Method: 引入EcoL2指标，结合模型精度与碳排放，评估物理信息机器学习和算子学习架构。

Result: 实验表明EcoL2能全面评估模型性能与排放成本。

Conclusion: EcoL2为构建高性能、低环境影响的科学机器学习系统提供了方向。

Abstract: Real-world systems, from aerospace to railway engineering, are modeled with
partial differential equations (PDEs) describing the physics of the system.
Estimating robust solutions for such problems is essential. Deep learning-based
architectures, such as neural PDE solvers, have recently gained traction as a
reliable solution method. The current state of development of these approaches,
however, primarily focuses on improving accuracy. The environmental impact of
excessive computation, leading to increased carbon emissions, has largely been
overlooked. This paper introduces a carbon emission measure for a range of PDE
solvers. Our proposed metric, EcoL2, balances model accuracy with emissions
across data collection, model training, and deployment. Experiments across both
physics-informed machine learning and operator learning architectures
demonstrate that the proposed metric presents a holistic assessment of model
performance and emission cost. As such solvers grow in scale and deployment,
EcoL2 represents a step toward building performant scientific machine learning
systems with lower long-term environmental impact.

</details>

### [220] [HybridServe: Efficient Serving of Large AI Models with Confidence-Based Cascade Routing](https://arxiv.org/abs/2505.12566)
*Leyang Xue,Yao Fu,Luo Mai,Mahesh K. Marina*

Main category: cs.LG

TLDR: HybridServe是一种新型混合DNN模型服务系统，通过结合不同大小的模型（从小型到巨型）优化能源效率和推理准确性。


<details>
  <summary>Details</summary>
Motivation: 巨型DNN在云AI服务中至关重要，但其能源消耗极高，现有方法无法同时优化能源效率和准确性。

Method: HybridServe利用基于置信度的混合模型服务数据流，优先使用小型模型处理请求以减少巨型DNN的副本需求，并通过数据流规划器优化模型分区和复制。

Result: 实验表明，HybridServe比现有系统减少能源消耗高达19.8倍，同时保持与巨型DNN相同的准确性。

Conclusion: HybridServe成功解决了巨型DNN服务的高能耗问题，同时保持了高准确性。

Abstract: Giant Deep Neural Networks (DNNs), have become indispensable for accurate and
robust support of large-scale cloud based AI services. However, serving giant
DNNs is prohibitively expensive from an energy consumption viewpoint easily
exceeding that of training, due to the enormous scale of GPU clusters needed to
hold giant DNN model partitions and replicas. Existing approaches can either
optimize energy efficiency or inference accuracy but not both. To overcome this
status quo, we propose HybridServe, a novel hybrid DNN model serving system
that leverages multiple sized versions (small to giant) of the model to be
served in tandem. Through a confidence based hybrid model serving dataflow,
HybridServe prefers to serve inference requests with energy-efficient smaller
models so long as accuracy is not compromised, thereby reducing the number of
replicas needed for giant DNNs. HybridServe also features a dataflow planner
for efficient partitioning and replication of candidate models to maximize
serving system throughput. Experimental results using a prototype
implementation of HybridServe show that it reduces energy footprint by up to
19.8x compared to the state-of-the-art DNN model serving systems while matching
the accuracy of serving solely with giant DNNs.

</details>

### [221] [AdaDim: Dimensionality Adaptation for SSL Representational Dynamics](https://arxiv.org/abs/2505.12576)
*Kiran Kokilepersaud,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.LG

TLDR: 论文研究了自监督学习（SSL）中维度崩溃问题，提出通过动态调整特征解相关和样本均匀分布的损失权重（AdaDim方法）来优化表示空间。


<details>
  <summary>Details</summary>
Motivation: 解决当前SSL文献中对训练动态和表示空间维度（H(R)和I(R;Z)）与下游性能关系的理解不足问题。

Method: 提出AdaDim方法，动态调整特征解相关和样本均匀分布的损失权重，以优化表示空间。

Result: 发现最佳SSL模型并非H(R)最高或I(R;Z)最低，而是两者达到中间最优值。

Conclusion: AdaDim方法通过动态调整损失权重，有效优化SSL表示空间，提升模型性能。

Abstract: A key factor in effective Self-Supervised learning (SSL) is preventing
dimensional collapse, which is where higher-dimensional representation spaces
span a lower-dimensional subspace. Therefore, SSL optimization strategies
involve guiding a model to produce representations ($R$) with a higher
dimensionality. Dimensionality is either optimized through a
dimension-contrastive approach that encourages feature decorrelation or through
a sample-contrastive method that promotes a uniform spread of sample
representations. Both families of SSL algorithms also utilize a projection head
that maps $R$ into a lower-dimensional embedding space $Z$. Recent work has
characterized the projection head as a filter of irrelevant features from the
SSL objective by reducing mutual information, $I(R;Z)$. Therefore, the current
literature's view is that a good SSL representation space should have a high
$H(R)$ and a low $I(R;Z)$. However, this view of the problem is lacking in
terms of an understanding of the underlying training dynamics that influences
both terms, as well as how the values of $H(R)$ and $I(R;Z)$ arrived at the end
of training reflect the downstream performance of an SSL model. We address both
gaps in the literature by demonstrating that increases in $H(R)$ due to feature
decorrelation at the start of training lead to a higher $I(R;Z)$, while
increases in $H(R)$ due to samples distributing uniformly in a high-dimensional
space at the end of training cause $I(R;Z)$ to plateau or decrease.
Furthermore, our analysis shows that the best performing SSL models do not have
the highest $H(R)$ nor the lowest $I(R;Z)$, but arrive at an optimal
intermediate point for both. We develop a method called AdaDim to exploit these
observed training dynamics by adaptively weighting between losses based on
feature decorrelation and uniform sample spread.

</details>

### [222] [Adaptive parameter-efficient fine-tuning via Hessian-informed subset selection](https://arxiv.org/abs/2505.12579)
*Shiyun Xu,Zhiqi Bu*

Main category: cs.LG

TLDR: AdaPEFT是一种基于Hessian信息的PEFT方法，通过多任务优化选择最具影响力的参数子集，适应不同任务和模型。


<details>
  <summary>Details</summary>
Motivation: PEFT方法在微调大模型时仅训练少量参数，但不同子集选择导致性能差异，因此需要有效选择最优子集。

Method: 将子集选择建模为多任务问题，结合ε约束法和二阶泰勒近似，转化为0-1背包问题，并通过Pareto最优性求解。

Result: 提出的AdaPEFT方法在任务和模型间具有适应性，所选子集在不同训练范围和模型大小下表现稳定。

Conclusion: AdaPEFT通过优化参数子集选择，显著提升了PEFT方法的效率和性能。

Abstract: Parameter-efficient fine-tuning (PEFT) is a highly effective approach for
adapting large pre-trained models to downstream tasks with minimal
computational overhead. At the core, PEFT methods freeze most parameters and
only trains a small subset (say $<0.1\%$ of total parameters). Notably,
different PEFT methods select different subsets, resulting in varying levels of
performance. This variation prompts a key question: how to effectively select
the most influential subset to train?
  We formulate the subset selection as a multi-task problem: maximizing the
performance and minimizing the number of trainable parameters. We leverage a
series of transformations -- including $\epsilon$-constraint method and
second-order Taylor approximation -- to arrive at the classical 0-1 knapsack
problem, which we solve through the lens of Pareto optimality. Consequently, we
propose AdaPEFT, a Hessian-informed PEFT that adapts to various tasks and
models, in which the selected subset empirically transfers across training
horizons and model sizes.

</details>

### [223] [An approach based on class activation maps for investigating the effects of data augmentation on neural networks for image classification](https://arxiv.org/abs/2505.12581)
*Lucas M. Dorneles,Luan Fonseca Garcia,Joel Luís Carbonera*

Main category: cs.LG

TLDR: 该论文提出了一种方法和指标，用于定量分析数据增强对卷积神经网络在图像分类任务中学习模式的影响。


<details>
  <summary>Details</summary>
Motivation: 研究数据增强方法对神经网络模型学习模式的影响，填补现有文献的空白。

Method: 利用类别激活图（CAM）识别和测量模型对图像像素的重要性，并通过提取相似性和差异性指标分析不同数据增强策略的影响。

Result: 实验表明，数据增强技术的影响可以通过该方法分析，并能识别其对训练模型的不同影响模式。

Conclusion: 提出的方法为定量分析数据增强对神经网络模型的影响提供了有效工具。

Abstract: Neural networks have become increasingly popular in the last few years as an
effective tool for the task of image classification due to the impressive
performance they have achieved on this task. In image classification tasks, it
is common to use data augmentation strategies to increase the robustness of
trained networks to changes in the input images and to avoid overfitting.
Although data augmentation is a widely adopted technique, the literature lacks
a body of research analyzing the effects data augmentation methods have on the
patterns learned by neural network models working on complex datasets. The
primary objective of this work is to propose a methodology and set of metrics
that may allow a quantitative approach to analyzing the effects of data
augmentation in convolutional networks applied to image classification. An
important tool used in the proposed approach lies in the concept of class
activation maps for said models, which allow us to identify and measure the
importance these models assign to each individual pixel in an image when
executing the classification task. From these maps, we may then extract metrics
over the similarities and differences between maps generated by these models
trained on a given dataset with different data augmentation strategies.
Experiments made using this methodology suggest that the effects of these data
augmentation techniques not only can be analyzed in this way but also allow us
to identify different impact profiles over the trained models.

</details>

### [224] [Learning Robust Spectral Dynamics for Temporal Domain Generalization](https://arxiv.org/abs/2505.12585)
*En Yu,Jie Lu,Xiaoyu Yang,Guangquan Zhang,Zhen Fang*

Main category: cs.LG

TLDR: FreKoo通过频域分析和Koopman算子解决复杂概念漂移问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型在动态环境中性能下降，现有方法难以处理复杂漂移。

Method: 利用傅里叶变换分解参数轨迹，低频部分用Koopman算子预测，高频部分通过正则化平滑。

Result: FreKoo在复杂漂移场景中表现显著优于现有方法。

Conclusion: FreKoo通过频域分析和理论保障，有效提升模型在动态环境中的泛化能力。

Abstract: Modern machine learning models struggle to maintain performance in dynamic
environments where temporal distribution shifts, \emph{i.e., concept drift},
are prevalent. Temporal Domain Generalization (TDG) seeks to enable model
generalization across evolving domains, yet existing approaches typically
assume smooth incremental changes, struggling with complex real-world drifts
involving long-term structure (incremental evolution/periodicity) and local
uncertainties. To overcome these limitations, we introduce FreKoo, which
tackles these challenges via a novel frequency-domain analysis of parameter
trajectories. It leverages the Fourier transform to disentangle parameter
evolution into distinct spectral bands. Specifically, low-frequency component
with dominant dynamics are learned and extrapolated using the Koopman operator,
robustly capturing diverse drift patterns including both incremental and
periodicity. Simultaneously, potentially disruptive high-frequency variations
are smoothed via targeted temporal regularization, preventing overfitting to
transient noise and domain uncertainties. In addition, this dual spectral
strategy is rigorously grounded through theoretical analysis, providing
stability guarantees for the Koopman prediction, a principled Bayesian
justification for the high-frequency regularization, and culminating in a
multiscale generalization bound connecting spectral dynamics to improved
generalization. Extensive experiments demonstrate FreKoo's significant
superiority over SOTA TDG approaches, particularly excelling in real-world
streaming scenarios with complex drifts and uncertainties.

</details>

### [225] [A Few Large Shifts: Layer-Inconsistency Based Minimal Overhead Adversarial Example Detection](https://arxiv.org/abs/2505.12586)
*Sanggeon Yun,Ryozo Masukawa,Hyunwoo Oh,Nathaniel D. Bastian,Mohsen Imani*

Main category: cs.LG

TLDR: 提出一种轻量级检测框架，利用目标模型内部层间不一致性检测对抗样本，仅需良性数据校准，性能优越。


<details>
  <summary>Details</summary>
Motivation: DNN易受对抗样本攻击，现有检测方法依赖外部模型或复杂架构，效率与泛化性受限。

Method: 基于“少数大偏移假设”，提出Recovery Testing (RT)和Logit-layer Testing (LT)两种策略检测对抗扰动。

Result: 在CIFAR-10、CIFAR-100和ImageNet上，检测性能达到SOTA，计算开销可忽略且不影响干净数据准确率。

Conclusion: 该方法高效、轻量，仅需良性数据，为对抗样本检测提供实用解决方案。

Abstract: Deep neural networks (DNNs) are highly susceptible to adversarial
examples--subtle, imperceptible perturbations that can lead to incorrect
predictions. While detection-based defenses offer a practical alternative to
adversarial training, many existing methods depend on external models, complex
architectures, heavy augmentations, or adversarial data, limiting their
efficiency and generalizability. We introduce a lightweight, plug-in detection
framework that leverages internal layer-wise inconsistencies within the target
model itself, requiring only benign data for calibration. Our approach is
grounded in the A Few Large Shifts Assumption, which posits that adversarial
perturbations typically induce large representation shifts in a small subset of
layers. Building on this, we propose two complementary strategies--Recovery
Testing (RT) and Logit-layer Testing (LT)--to expose internal disruptions
caused by adversaries. Evaluated on CIFAR-10, CIFAR-100, and ImageNet under
both standard and adaptive threat models, our method achieves state-of-the-art
detection performance with negligible computational overhead and no compromise
to clean accuracy.

</details>

### [226] [Rethinking Predictive Modeling for LLM Routing: When Simple kNN Beats Complex Learned Routers](https://arxiv.org/abs/2505.12601)
*Yang Li*

Main category: cs.LG

TLDR: 论文提出了一种基于k近邻（kNN）的简单路由方法，用于选择最适合输入的大语言模型（LLM），其性能优于复杂的学习路由策略，并通过标准化基准和多模态数据集验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模和专业化的增长，高效选择最佳模型的路由变得至关重要。现有复杂路由方法依赖不同训练数据和评估设置，难以比较和泛化。

Method: 采用简单且经过优化的k近邻（kNN）方法进行路由选择，并引入标准化路由基准和多模态数据集进行系统评估。

Result: 实验表明，kNN方法在多样任务中不仅匹配且常优于最先进的学习路由策略，且样本复杂度更低。

Conclusion: 研究挑战了复杂架构的趋势，强调在投资复杂解决方案前应充分评估简单基线方法，并提供了可复现的基准和代码。

Abstract: As large language models (LLMs) grow in scale and specialization,
routing--selecting the best model for a given input--has become essential for
efficient and effective deployment. While recent methods rely on complex
learned routing strategies, their dependence on disparate training data and
evaluation setups makes comparison and generalization difficult. In this work,
we revisit LLM routing through the lens of simplicity. We show that a
well-tuned k-Nearest Neighbors (kNN) approach not only matches but often
outperforms state-of-the-art learned routers across diverse tasks. To support
systematic evaluation, we introduce a suite of standardized routing benchmarks
spanning instruction-following, question-answering, and reasoning tasks, as
well as the first multi-modal routing dataset involving visual inputs. Our
findings reveal that the locality properties of model performance in embedding
space enable simple non-parametric methods to achieve strong routing decisions
with lower sample complexity than parametric approaches. This challenges the
prevailing trend toward sophisticated architectures and highlights the
importance of thoroughly evaluating simple baselines before investing in
complex solutions. To support reproducibility and further exploration, we will
release all benchmarks and code upon publication.

</details>

### [227] [Action-Dependent Optimality-Preserving Reward Shaping](https://arxiv.org/abs/2505.12611)
*Grant C. Forbes,Jianxun Wang,Leonardo Villalobos-Arias,Arnav Jhala,David L. Roberts*

Main category: cs.LG

TLDR: 论文提出了一种新的奖励塑形方法ADOPS，解决了传统PBRS方法在复杂探索环境中的不足，适用于稀疏奖励任务。


<details>
  <summary>Details</summary>
Motivation: 传统PBRS方法（如GRM和PIES）在复杂、探索密集的环境中效果不佳，可能导致奖励滥用问题。

Method: 引入ADOPS方法，将内在奖励转换为最优性保持形式，适用于稀疏奖励环境（如Montezuma's Revenge）。

Result: ADOPS能够处理无法用潜在形式表示的内在奖励，同时保持最优策略集。

Conclusion: ADOPS在复杂稀疏奖励环境中表现优于传统PBRS方法。

Abstract: Recent RL research has utilized reward shaping--particularly complex shaping
rewards such as intrinsic motivation (IM)--to encourage agent exploration in
sparse-reward environments. While often effective, ``reward hacking'' can lead
to the shaping reward being optimized at the expense of the extrinsic reward,
resulting in a suboptimal policy. Potential-Based Reward Shaping (PBRS)
techniques such as Generalized Reward Matching (GRM) and Policy-Invariant
Explicit Shaping (PIES) have mitigated this. These methods allow for
implementing IM without altering optimal policies. In this work we show that
they are effectively unsuitable for complex, exploration-heavy environments
with long-duration episodes. To remedy this, we introduce Action-Dependent
Optimality Preserving Shaping (ADOPS), a method of converting intrinsic rewards
to an optimality-preserving form that allows agents to utilize IM more
effectively in the extremely sparse environment of Montezuma's Revenge. We also
prove ADOPS accommodates reward shaping functions that cannot be written in a
potential-based form: while PBRS-based methods require the cumulative
discounted intrinsic return be independent of actions, ADOPS allows for
intrinsic cumulative returns to be dependent on agents' actions while still
preserving the optimal policy set. We show how action-dependence enables
ADOPS's to preserve optimality while learning in complex, sparse-reward
environments where other methods struggle.

</details>

### [228] [Adaptive Graph Unlearning](https://arxiv.org/abs/2505.12614)
*Pengfei Ding,Yan Wang,Guanfeng Liu,Jiajie Zhu*

Main category: cs.LG

TLDR: AGU是一种自适应图遗忘框架，解决了现有方法在图遗忘任务中的不足，能够灵活适应不同任务和GNN架构，确保完全遗忘并保持图的完整性。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，图数据可能包含过时、不准确或隐私敏感信息，需要高效删除。现有方法存在不完全或过度遗忘问题，且对不同GNN架构的邻居影响识别不准确。

Method: 提出AGU框架，灵活适应不同遗忘任务和GNN架构，准确识别受影响邻居并优先处理重要节点。

Result: 在七个真实世界图上实验表明，AGU在有效性、效率和遗忘能力上优于现有方法。

Conclusion: AGU为图遗忘任务提供了高效且灵活的解决方案，显著提升了性能。

Abstract: Graph unlearning, which deletes graph elements such as nodes and edges from
trained graph neural networks (GNNs), is crucial for real-world applications
where graph data may contain outdated, inaccurate, or privacy-sensitive
information. However, existing methods often suffer from (1) incomplete or over
unlearning due to neglecting the distinct objectives of different unlearning
tasks, and (2) inaccurate identification of neighbors affected by deleted
elements across various GNN architectures. To address these limitations, we
propose AGU, a novel Adaptive Graph Unlearning framework that flexibly adapts
to diverse unlearning tasks and GNN architectures. AGU ensures the complete
forgetting of deleted elements while preserving the integrity of the remaining
graph. It also accurately identifies affected neighbors for each GNN
architecture and prioritizes important ones to enhance unlearning performance.
Extensive experiments on seven real-world graphs demonstrate that AGU
outperforms existing methods in terms of effectiveness, efficiency, and
unlearning capability.

</details>

### [229] [Dual-Agent Reinforcement Learning for Automated Feature Generation](https://arxiv.org/abs/2505.12628)
*Wanfu Gao,Zengyao Man,Hanlin Pan,Kunpeng Liu*

Main category: cs.LG

TLDR: 提出了一种双智能体强化学习方法，解决特征生成中的冗余特征、状态表示不足及离散/连续特征差异问题。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法在特征生成中存在冗余特征、状态表示不完整及离散/连续特征处理差异的挑战。

Method: 设计双智能体：一个生成特征，另一个决定保留与否；使用自注意力机制增强状态表示，区分离散/连续特征操作。

Result: 多数据集实验证明方法有效。

Conclusion: 双智能体强化学习方法在特征生成中表现优异，解决了现有问题。

Abstract: Feature generation involves creating new features from raw data to capture
complex relationships among the original features, improving model robustness
and machine learning performance. Current methods using reinforcement learning
for feature generation have made feature exploration more flexible and
efficient. However, several challenges remain: first, during feature expansion,
a large number of redundant features are generated. When removing them, current
methods only retain the best features each round, neglecting those that perform
poorly initially but could improve later. Second, the state representation used
by current methods fails to fully capture complex feature relationships. Third,
there are significant differences between discrete and continuous features in
tabular data, requiring different operations for each type. To address these
challenges, we propose a novel dual-agent reinforcement learning method for
feature generation. Two agents are designed: the first generates new features,
and the second determines whether they should be preserved. A self-attention
mechanism enhances state representation, and diverse operations distinguish
interactions between discrete and continuous features. The experimental results
on multiple datasets demonstrate that the proposed method is effective. The
code is available at https://github.com/extess0/DARL.

</details>

### [230] [Enhancing Latent Computation in Transformers with Latent Tokens](https://arxiv.org/abs/2505.12629)
*Yuchang Sun,Yanxi Chen,Yaliang Li,Bolin Ding*

Main category: cs.LG

TLDR: 提出一种轻量级方法“潜在标记”，通过注意力机制增强大型语言模型的性能，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 通过辅助标记增强大型语言模型（LLMs）的性能，提升其适应性和泛化能力。

Method: 引入非自然语言可解释的虚拟标记（潜在标记），通过注意力机制引导自回归解码过程，并与预训练Transformer无缝集成。

Result: 数值结果表明，该方法在分布外泛化场景中显著优于基线方法。

Conclusion: 潜在标记方法能有效提升LLMs的适应性，具有潜在的应用价值。

Abstract: Augmenting large language models (LLMs) with auxiliary tokens has emerged as
a promising strategy for enhancing model performance. In this work, we
introduce a lightweight method termed latent tokens; these are dummy tokens
that may be non-interpretable in natural language but steer the autoregressive
decoding process of a Transformer-based LLM via the attention mechanism. The
proposed latent tokens can be seamlessly integrated with a pre-trained
Transformer, trained in a parameter-efficient manner, and applied flexibly at
inference time, while adding minimal complexity overhead to the existing
infrastructure of standard Transformers. We propose several hypotheses about
the underlying mechanisms of latent tokens and design synthetic tasks
accordingly to verify them. Numerical results confirm that the proposed method
noticeably outperforms the baselines, particularly in the out-of-distribution
generalization scenarios, highlighting its potential in improving the
adaptability of LLMs.

</details>

### [231] [Two out of Three (ToT): using self-consistency to make robust predictions](https://arxiv.org/abs/2505.12642)
*Jung Hoon Lee,Sujith Vijayan*

Main category: cs.LG

TLDR: 论文提出了一种名为‘Two out of Three (ToT)’的算法，旨在帮助深度学习模型在不确定时选择不回答，以提高决策的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习的决策机制不透明，在高风险领域部署时可能带来严重后果。因此，需要一种方法让模型在不确定时能够选择不回答。

Method: ToT算法通过生成两个额外的预测，结合原始预测来判断是否提供答案，灵感来源于人脑对冲突信息的敏感性。

Result: ToT算法能够帮助模型在不确定时选择不回答，从而提高决策的鲁棒性。

Conclusion: ToT算法为深度学习模型提供了一种在不确定时避免错误决策的方法，适用于高风险领域。

Abstract: Deep learning (DL) can automatically construct intelligent agents, deep
neural networks (alternatively, DL models), that can outperform humans in
certain tasks. However, the operating principles of DL remain poorly
understood, making its decisions incomprehensible. As a result, it poses a
great risk to deploy DL in high-stakes domains in which mistakes or errors may
lead to critical consequences. Here, we aim to develop an algorithm that can
help DL models make more robust decisions by allowing them to abstain from
answering when they are uncertain. Our algorithm, named `Two out of Three
(ToT)', is inspired by the sensitivity of the human brain to conflicting
information. ToT creates two alternative predictions in addition to the
original model prediction and uses the alternative predictions to decide
whether it should provide an answer or not.

</details>

### [232] [Spiking Neural Network: a low power solution for physical layer authentication](https://arxiv.org/abs/2505.12647)
*Jung Hoon Lee,Sujith Vijayan*

Main category: cs.LG

TLDR: SNNs可用于无线通信的物理层认证，且能高效节能，但易受对抗攻击，需用自编码器增强安全性。


<details>
  <summary>Details</summary>
Motivation: 探索SNNs在无线通信物理层认证中的潜力，解决DL模型在边缘设备上部署的资源消耗问题。

Method: 评估SNNs学习RF发射器独特物理特性（指纹）的能力，并测试其对对抗攻击的脆弱性。

Result: SNNs能有效识别设备指纹，但易受对抗攻击；自编码器可清除扰动，增强安全性。

Conclusion: SNNs是无线通信物理层认证的高效替代方案，但需进一步研究对抗攻击防御。

Abstract: Deep learning (DL) is a powerful tool that can solve complex problems, and
thus, it seems natural to assume that DL can be used to enhance the security of
wireless communication. However, deploying DL models to edge devices in
wireless networks is challenging, as they require significant amounts of
computing and power resources. Notably, Spiking Neural Networks (SNNs) are
known to be efficient in terms of power consumption, meaning they can be an
alternative platform for DL models for edge devices. In this study, we ask if
SNNs can be used in physical layer authentication. Our evaluation suggests that
SNNs can learn unique physical properties (i.e., `fingerprints') of RF
transmitters and use them to identify individual devices. Furthermore, we find
that SNNs are also vulnerable to adversarial attacks and that an autoencoder
can be used clean out adversarial perturbations to harden SNNs against them.

</details>

### [233] [TransferTraj: A Vehicle Trajectory Learning Model for Region and Task Transferability](https://arxiv.org/abs/2505.12672)
*Tonglong Wei,Yan Lin,Zeyu Zhou,Haomin Wen,Jilin Hu,Shengnan Guo,Youfang Lin,Gao Cong,Huaiyu Wan*

Main category: cs.LG

TLDR: 论文提出TransferTraj模型，解决车辆GPS轨迹在跨区域和跨任务迁移中的挑战，通过多模态整合和统一输入输出结构实现高效迁移。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹学习模型在跨区域和跨任务迁移时表现不佳，需重新训练或难以泛化。

Method: 提出TransferTraj模型，包含RTTE模块整合多模态信息，TRIE模块处理空间特征相对信息，以及空间上下文MoE模块；设计任务可迁移的输入输出方案。

Result: 在三个真实数据集上验证了模型在任务迁移、零样本和少样本区域迁移中的有效性。

Conclusion: TransferTraj在跨区域和跨任务迁移中表现优越，无需重新训练即可适应不同任务和区域。

Abstract: Vehicle GPS trajectories provide valuable movement information that supports
various downstream tasks and applications. A desirable trajectory learning
model should be able to transfer across regions and tasks without retraining,
avoiding the need to maintain multiple specialized models and subpar
performance with limited training data. However, each region has its unique
spatial features and contexts, which are reflected in vehicle movement patterns
and difficult to generalize. Additionally, transferring across different tasks
faces technical challenges due to the varying input-output structures required
for each task. Existing efforts towards transferability primarily involve
learning embedding vectors for trajectories, which perform poorly in region
transfer and require retraining of prediction modules for task transfer.
  To address these challenges, we propose TransferTraj, a vehicle GPS
trajectory learning model that excels in both region and task transferability.
For region transferability, we introduce RTTE as the main learnable module
within TransferTraj. It integrates spatial, temporal, POI, and road network
modalities of trajectories to effectively manage variations in spatial context
distribution across regions. It also introduces a TRIE module for incorporating
relative information of spatial features and a spatial context MoE module for
handling movement patterns in diverse contexts. For task transferability, we
propose a task-transferable input-output scheme that unifies the input-output
structure of different tasks into the masking and recovery of modalities and
trajectory points. This approach allows TransferTraj to be pre-trained once and
transferred to different tasks without retraining. Extensive experiments on
three real-world vehicle trajectory datasets under task transfer, zero-shot,
and few-shot region transfer, validating TransferTraj's effectiveness.

</details>

### [234] [On the Mechanisms of Adversarial Data Augmentation for Robust and Adaptive Transfer Learning](https://arxiv.org/abs/2505.12681)
*Hana Satou,Alan Mitkiy*

Main category: cs.LG

TLDR: 论文研究了对抗性数据增强（ADA）在迁移学习中的作用，提出了一种结合ADA、一致性正则化和域不变表示学习的统一框架，显著提升了目标域性能。


<details>
  <summary>Details</summary>
Motivation: 解决分布偏移下迁移学习的挑战，探索对抗性扰动作为数据增强工具的潜力。

Method: 提出统一框架，结合对抗性数据增强、一致性正则化和域不变表示学习。

Result: 在多个基准数据集（如VisDA、DomainNet和Office-Home）上，方法显著提升了目标域性能。

Conclusion: 对抗性学习可以转化为促进跨域迁移的正则化工具。

Abstract: Transfer learning across domains with distribution shift remains a
fundamental challenge in building robust and adaptable machine learning
systems. While adversarial perturbations are traditionally viewed as threats
that expose model vulnerabilities, recent studies suggest that they can also
serve as constructive tools for data augmentation. In this work, we
systematically investigate the role of adversarial data augmentation (ADA) in
enhancing both robustness and adaptivity in transfer learning settings. We
analyze how adversarial examples, when used strategically during training,
improve domain generalization by enriching decision boundaries and reducing
overfitting to source-domain-specific features. We further propose a unified
framework that integrates ADA with consistency regularization and
domain-invariant representation learning. Extensive experiments across multiple
benchmark datasets -- including VisDA, DomainNet, and Office-Home --
demonstrate that our method consistently improves target-domain performance
under both unsupervised and few-shot domain adaptation settings. Our results
highlight a constructive perspective of adversarial learning, transforming
perturbation from a destructive attack into a regularizing force for
cross-domain transferability.

</details>

### [235] [RoFL: Robust Fingerprinting of Language Models](https://arxiv.org/abs/2505.12682)
*Yun-Yun Tsai,Chuan Guo,Junfeng Yang,Laurens van der Maaten*

Main category: cs.LG

TLDR: 提出了一种通过指纹识别大语言模型（LLM）的方法，用于检测模型或输出的许可证违规使用。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的发布和许可证多样化，如何识别模型或输出的违规使用成为问题。

Method: 利用统计指纹，无需修改模型训练，通过有限查询在黑盒设置中识别模型。

Result: 方法对模型或推理设置的常见变化具有高鲁棒性，性能优于现有技术。

Conclusion: 该方法为非侵入式，不影响模型质量，能有效识别模型使用情况。

Abstract: AI developers are releasing large language models (LLMs) under a variety of
different licenses. Many of these licenses restrict the ways in which the
models or their outputs may be used. This raises the question how license
violations may be recognized. In particular, how can we identify that an API or
product uses (an adapted version of) a particular LLM? We present a new method
that enable model developers to perform such identification via fingerprints:
statistical patterns that are unique to the developer's model and robust to
common alterations of that model. Our method permits model identification in a
black-box setting using a limited number of queries, enabling identification of
models that can only be accessed via an API or product. The fingerprints are
non-invasive: our method does not require any changes to the model during
training, hence by design, it does not impact model quality. Empirically, we
find our method provides a high degree of robustness to common changes in the
model or inference settings. In our experiments, it substantially outperforms
prior art, including invasive methods that explicitly train watermarks into the
model.

</details>

### [236] [DimGrow: Memory-Efficient Field-level Embedding Dimension Search](https://arxiv.org/abs/2505.12683)
*Yihong Huang,Chen Chu*

Main category: cs.LG

TLDR: DimGrow是一种轻量级方法，通过动态调整特征字段的嵌入维度，避免了传统方法中需要训练内存密集型SuperNet的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如剪枝或NAS）需要训练SuperNet，计算资源消耗大，不适用于大规模特征空间。

Method: DimGrow从每个特征字段的一个维度开始训练，通过重要性评分动态扩展或收缩维度，仅在重要性持续超过阈值时增加维度。

Result: 在三个推荐数据集上的实验表明，DimGrow有效且比基于SuperNet的方法节省内存。

Conclusion: DimGrow提供了一种高效、轻量级的维度分配方法，适用于大规模特征空间。

Abstract: Key feature fields need bigger embedding dimensionality, others need smaller.
This demands automated dimension allocation. Existing approaches, such as
pruning or Neural Architecture Search (NAS), require training a
memory-intensive SuperNet that enumerates all possible dimension combinations,
which is infeasible for large feature spaces. We propose DimGrow, a lightweight
approach that eliminates the SuperNet requirement. Starting training model from
one dimension per feature field, DimGrow can progressively expand/shrink
dimensions via importance scoring. Dimensions grow only when their importance
consistently exceed a threshold, ensuring memory efficiency. Experiments on
three recommendation datasets verify the effectiveness of DimGrow while it
reduces training memory compared to SuperNet-based methods.

</details>

### [237] [Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement](https://arxiv.org/abs/2505.12684)
*Yinlin Zhu,Xunkai Li,Jishuo Jia,Miao Hu,Di Wu,Meikang Qiu*

Main category: cs.LG

TLDR: FedGFM+ 是一种新型的去中心化图基础模型训练框架，通过减少知识纠缠提升跨域适应能力。


<details>
  <summary>Details</summary>
Motivation: 结合联邦图学习和图基础模型的优势，解决数据与任务异质性及跨域资源利用不足的问题。

Method: 提出 FedGFM+，包含 AncDAI（全局锚点初始化）和 AdaDPP（本地自适应提示池）两个核心模块。

Result: 在 8 个多样化基准测试中优于 20 个基线方法。

Conclusion: FedGFM+ 通过知识解耦显著提升了跨域适应性和下游任务表现。

Abstract: Recent advances in graph machine learning have shifted to data-centric
paradigms, driven by two emerging fields: (1) Federated graph learning (FGL)
enables multi-client collaboration but faces challenges from data and task
heterogeneity, limiting its practicality; (2) Graph foundation models (GFM)
offer strong domain generalization but are usually trained on single machines,
missing out on cross-silo data and resources.
  These paradigms are complementary, and their integration brings notable
benefits. Motivated by this, we propose FedGFM, a novel decentralized GFM
training paradigm. However, a key challenge is knowledge entanglement, where
multi-domain knowledge merges into indistinguishable representations, hindering
downstream adaptation.
  To address this, we present FedGFM+, an enhanced framework with two core
modules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based
domain-aware initialization strategy. Before pre-training, each client encodes
its local graph into domain-specific prototypes that serve as semantic anchors.
Synthetic embeddings around these anchors initialize the global model. We
theoretically prove these prototypes are distinguishable across domains,
providing a strong inductive bias to disentangle domain-specific knowledge. (2)
AdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a
lightweight graph prompt capturing domain semantics during pre-training. During
fine-tuning, prompts from all clients form a pool from which the GFM selects
relevant prompts to augment target graph attributes, improving downstream
adaptation.
  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and
tasks, outperforming 20 baselines from supervised learning, FGL, and federated
GFM variants.

</details>

### [238] [RoVo: Robust Voice Protection Against Unauthorized Speech Synthesis with Embedding-Level Perturbations](https://arxiv.org/abs/2505.12686)
*Seungmin Kim,Sohee Park,Donghyun Kim,Jisu Lee,Daeseon Choi*

Main category: cs.LG

TLDR: RoVo是一种新型的主动防御技术，通过在高维嵌入向量中注入对抗性扰动来保护语音信号，有效抵御语音合成攻击和语音增强模型的威胁。


<details>
  <summary>Details</summary>
Motivation: 随着AI语音合成技术的进步，语音欺骗攻击（如语音钓鱼和假新闻）的风险增加，现有防御方法效果有限。

Method: 提出RoVo技术，将对抗性扰动注入音频信号的高维嵌入向量中，重构为受保护的语音。

Result: 实验显示，RoVo将防御成功率（DSR）提高了70%以上，在商业语音验证API上达到99.5%的DSR，且对语音增强模型具有强鲁棒性。

Conclusion: RoVo在复杂威胁场景中表现出色，同时保持了语音的自然性和可用性。

Abstract: With the advancement of AI-based speech synthesis technologies such as Deep
Voice, there is an increasing risk of voice spoofing attacks, including voice
phishing and fake news, through unauthorized use of others' voices. Existing
defenses that inject adversarial perturbations directly into audio signals have
limited effectiveness, as these perturbations can easily be neutralized by
speech enhancement methods. To overcome this limitation, we propose RoVo
(Robust Voice), a novel proactive defense technique that injects adversarial
perturbations into high-dimensional embedding vectors of audio signals,
reconstructing them into protected speech. This approach effectively defends
against speech synthesis attacks and also provides strong resistance to speech
enhancement models, which represent a secondary attack threat.
  In extensive experiments, RoVo increased the Defense Success Rate (DSR) by
over 70% compared to unprotected speech, across four state-of-the-art speech
synthesis models. Specifically, RoVo achieved a DSR of 99.5% on a commercial
speaker-verification API, effectively neutralizing speech synthesis attack.
Moreover, RoVo's perturbations remained robust even under strong speech
enhancement conditions, outperforming traditional methods. A user study
confirmed that RoVo preserves both naturalness and usability of protected
speech, highlighting its effectiveness in complex and evolving threat
scenarios.

</details>

### [239] [Counterfactual Explanations for Continuous Action Reinforcement Learning](https://arxiv.org/abs/2505.12701)
*Shuyang Dong,Shangtong Zhang,Lu Feng*

Main category: cs.LG

TLDR: 提出了一种在连续动作强化学习中生成反事实解释的新方法，通过计算改进结果且最小化偏离原始动作序列的替代动作序列。


<details>
  <summary>Details</summary>
Motivation: 强化学习在医疗和机器人等领域有潜力，但缺乏可解释性限制了其应用。反事实解释为理解RL决策提供了新途径，但在连续动作空间中研究不足。

Method: 利用连续动作的距离度量，计算改进结果的替代动作序列，同时考虑预定义策略等约束。

Result: 在糖尿病控制和月球着陆两个RL领域验证了方法的有效性、效率和泛化能力。

Conclusion: 该方法增强了强化学习的可解释性和可信度，推动了其在更多领域的应用。

Abstract: Reinforcement Learning (RL) has shown great promise in domains like
healthcare and robotics but often struggles with adoption due to its lack of
interpretability. Counterfactual explanations, which address "what if"
scenarios, provide a promising avenue for understanding RL decisions but remain
underexplored for continuous action spaces. We propose a novel approach for
generating counterfactual explanations in continuous action RL by computing
alternative action sequences that improve outcomes while minimizing deviations
from the original sequence. Our approach leverages a distance metric for
continuous actions and accounts for constraints such as adhering to predefined
policies in specific states. Evaluations in two RL domains, Diabetes Control
and Lunar Lander, demonstrate the effectiveness, efficiency, and generalization
of our approach, enabling more interpretable and trustworthy RL applications.

</details>

### [240] [PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI](https://arxiv.org/abs/2505.12707)
*Yingchen He,Christian D. Weilbach,Martyna E. Wojciechowska,Yuxuan Zhang,Frank Wood*

Main category: cs.LG

TLDR: PLAICraft是一个多模态数据集平台，用于训练和评估实时、社交互动的AI代理，包含10,000小时的多玩家Minecraft游戏数据。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏大规模、实时、多模态且社交互动的数据集，限制了人类级别AI代理的发展。

Method: 开发PLAICraft平台，记录五种时间对齐的多模态数据（视频、音频、输入音频、鼠标和键盘动作），并提供评估套件。

Result: 数据集包含10,000小时游戏数据，支持对象识别、空间感知、语言基础和长期记忆等任务。

Conclusion: PLAICraft为训练实时、流畅的AI代理提供了新路径，推动真正具身智能的发展。

Abstract: Advances in deep generative modelling have made it increasingly plausible to
train human-level embodied agents. Yet progress has been limited by the absence
of large-scale, real-time, multi-modal, and socially interactive datasets that
reflect the sensory-motor complexity of natural environments. To address this,
we present PLAICraft, a novel data collection platform and dataset capturing
multiplayer Minecraft interactions across five time-aligned modalities: video,
game output audio, microphone input audio, mouse, and keyboard actions. Each
modality is logged with millisecond time precision, enabling the study of
synchronous, embodied behaviour in a rich, open-ended world. The dataset
comprises over 10,000 hours of gameplay from more than 10,000 global
participants.\footnote{We have done a privacy review for the public release of
an initial 200-hour subset of the dataset, with plans to release most of the
dataset over time.} Alongside the dataset, we provide an evaluation suite for
benchmarking model capabilities in object recognition, spatial awareness,
language grounding, and long-term memory. PLAICraft opens a path toward
training and evaluating agents that act fluently and purposefully in real time,
paving the way for truly embodied artificial intelligence.

</details>

### [241] [Pave Your Own Path: Graph Gradual Domain Adaptation on Fused Gromov-Wasserstein Geodesics](https://arxiv.org/abs/2505.12709)
*Zhichen Zeng,Ruizhong Qiu,Wenxuan Bao,Tianxin Wei,Xiao Lin,Yuchen Yan,Tarek F. Abdelzaher,Jiawei Han,Hanghang Tong*

Main category: cs.LG

TLDR: 论文提出Gadget框架，首次解决非IID图数据中的逐步域适应问题，通过FGW距离和最优路径提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有图域适应方法假设源和目标图间分布偏移较小，难以应对实际中的大偏移，且逐步域适应方法未解决非IID图数据问题。

Method: 采用Fused Gromov-Wasserstein (FGW)距离作为域差异度量，推导误差界，并设计算法生成FGW测地线作为最优路径。

Result: Gadget框架显著提升现有图域适应方法性能，在真实数据集上节点分类准确率最高提升6.8%。

Conclusion: Gadget为非IID图数据的逐步域适应提供了理论和实践基础，有效应对大分布偏移问题。

Abstract: Graph neural networks, despite their impressive performance, are highly
vulnerable to distribution shifts on graphs. Existing graph domain adaptation
(graph DA) methods often implicitly assume a \textit{mild} shift between source
and target graphs, limiting their applicability to real-world scenarios with
\textit{large} shifts. Gradual domain adaptation (GDA) has emerged as a
promising approach for addressing large shifts by gradually adapting the source
model to the target domain via a path of unlabeled intermediate domains.
Existing GDA methods exclusively focus on independent and identically
distributed (IID) data with a predefined path, leaving their extension to
\textit{non-IID graphs without a given path} an open challenge. To bridge this
gap, we present Gadget, the first GDA framework for non-IID graph data. First
(\textit{theoretical foundation}), the Fused Gromov-Wasserstein (FGW) distance
is adopted as the domain discrepancy for non-IID graphs, based on which, we
derive an error bound revealing that the target domain error is proportional to
the length of the path. Second (\textit{optimal path}), guided by the error
bound, we identify the FGW geodesic as the optimal path, which can be
efficiently generated by our proposed algorithm. The generated path can be
seamlessly integrated with existing graph DA methods to handle large shifts on
graphs, improving state-of-the-art graph DA methods by up to 6.8\% in node
classification accuracy on real-world datasets.

</details>

### [242] [Confidence-Regulated Generative Diffusion Models for Reliable AI Agent Migration in Vehicular Metaverses](https://arxiv.org/abs/2505.12710)
*Yingkai Kang,Jiawen Kang,Jinbo Wen,Tao Zhang,Zhaohui Yang,Dusit Niyato,Yan Zhang*

Main category: cs.LG

TLDR: 论文提出了一种可靠的车载AI代理迁移框架，通过车辆与RSU的合作实现动态迁移和资源调度，并设计了信任评估模型和CGDM算法以优化性能。


<details>
  <summary>Details</summary>
Motivation: 车载元宇宙需要实时决策的AI代理，但迁移过程中频繁的数据交换可能引发网络安全问题，因此需要可靠的迁移框架和信任机制。

Method: 提出合作式迁移框架，设计信任评估模型，并将迁移过程建模为部分可观测马尔可夫决策过程，开发CGDM算法生成迁移决策。

Result: CGDM算法显著降低了系统延迟并增强了抗网络攻击的鲁棒性，优于基线方法。

Conclusion: 该框架和算法为车载元宇宙中的AI代理迁移提供了高效、可靠的解决方案。

Abstract: Vehicular metaverses are an emerging paradigm that merges intelligent
transportation systems with virtual spaces, leveraging advanced digital twin
and Artificial Intelligence (AI) technologies to seamlessly integrate vehicles,
users, and digital environments. In this paradigm, vehicular AI agents are
endowed with environment perception, decision-making, and action execution
capabilities, enabling real-time processing and analysis of multi-modal data to
provide users with customized interactive services. Since vehicular AI agents
require substantial resources for real-time decision-making, given vehicle
mobility and network dynamics conditions, the AI agents are deployed in
RoadSide Units (RSUs) with sufficient resources and dynamically migrated among
them. However, AI agent migration requires frequent data exchanges, which may
expose vehicular metaverses to potential cyber attacks. To this end, we propose
a reliable vehicular AI agent migration framework, achieving reliable dynamic
migration and efficient resource scheduling through cooperation between
vehicles and RSUs. Additionally, we design a trust evaluation model based on
the theory of planned behavior to dynamically quantify the reputation of RSUs,
thereby better accommodating the personalized trust preferences of users. We
then model the vehicular AI agent migration process as a partially observable
markov decision process and develop a Confidence-regulated Generative Diffusion
Model (CGDM) to efficiently generate AI agent migration decisions. Numerical
results demonstrate that the CGDM algorithm significantly outperforms baseline
methods in reducing system latency and enhancing robustness against cyber
attacks.

</details>

### [243] [Deep Unfolding with Kernel-based Quantization in MIMO Detection](https://arxiv.org/abs/2505.12736)
*Zeyi Ren,Jingreng Lei,Yichen Jin,Ermo Hua,Qingfeng Lin,Chen Zhang,Bowen Zhou,Yik-Chung Wu*

Main category: cs.LG

TLDR: 本文提出了一种基于核的自适应量化（KAQ）框架，用于深度展开网络，解决了现有量化方法在边缘设备部署中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 边缘计算对多输入多输出（MIMO）检测任务的能效模型部署提出了关键需求，但现有量化方法因依赖参数化分布假设和静态量化步长而性能下降。

Method: 提出KAQ框架，结合核密度估计（KDE）和最大均值差异（MMD）对齐全精度和量化模型的激活分布，并引入动态步长更新方法。

Result: 仿真表明，KAQ框架的精度优于传统方法，并成功降低了模型推理延迟。

Conclusion: KAQ框架有效解决了量化性能问题，适用于资源受限的边缘设备部署。

Abstract: The development of edge computing places critical demands on energy-efficient
model deployment for multiple-input multiple-output (MIMO) detection tasks.
Deploying deep unfolding models such as PGD-Nets and ADMM-Nets into
resource-constrained edge devices using quantization methods is challenging.
Existing quantization methods based on quantization aware training (QAT) suffer
from performance degradation due to their reliance on parametric distribution
assumption of activations and static quantization step sizes. To address these
challenges, this paper proposes a novel kernel-based adaptive quantization
(KAQ) framework for deep unfolding networks. By utilizing a joint kernel
density estimation (KDE) and maximum mean discrepancy (MMD) approach to align
activation distributions between full-precision and quantized models, the need
for prior distribution assumptions is eliminated. Additionally, a dynamic step
size updating method is introduced to adjust the quantization step size based
on the channel conditions of wireless networks. Extensive simulations
demonstrate that the accuracy of proposed KAQ framework outperforms traditional
methods and successfully reduces the model's inference latency.

</details>

### [244] [Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2505.12737)
*Hongjoon Ahn,Heewoong Choi,Jisu Han,Taesup Moon*

Main category: cs.LG

TLDR: 论文提出了一种名为OTA的方法，通过改进价值函数来解决离线目标条件强化学习（GCRL）中长时程任务的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 离线GCRL在长时程任务中表现不佳，主要原因是高层策略无法生成合适的子目标，且优势信号不准确。

Method: 提出OTA方法，将时间抽象融入时间差分学习过程，改进价值函数以生成更清晰的优势信号。

Result: 实验表明，OTA在OGBench基准测试中表现优异，适用于迷宫导航和视觉机器人操作任务。

Conclusion: OTA通过改进价值函数，显著提升了离线GCRL在长时程任务中的性能。

Abstract: Offline goal-conditioned reinforcement learning (GCRL) offers a practical
learning paradigm where goal-reaching policies are trained from abundant
unlabeled (reward-free) datasets without additional environment interaction.
However, offline GCRL still struggles with long-horizon tasks, even with recent
advances that employ hierarchical policy structures, such as HIQL. By
identifying the root cause of this challenge, we observe the following
insights: First, performance bottlenecks mainly stem from the high-level
policy's inability to generate appropriate subgoals. Second, when learning the
high-level policy in the long-horizon regime, the sign of the advantage signal
frequently becomes incorrect. Thus, we argue that improving the value function
to produce a clear advantage signal for learning the high-level policy is
essential. In this paper, we propose a simple yet effective solution:
Option-aware Temporally Abstracted value learning, dubbed OTA, which
incorporates temporal abstraction into the temporal-difference learning
process. By modifying the value update to be option-aware, the proposed
learning scheme contracts the effective horizon length, enabling better
advantage estimates even in long-horizon regimes. We experimentally show that
the high-level policy extracted using the OTA value function achieves strong
performance on complex tasks from OGBench, a recently proposed offline GCRL
benchmark, including maze navigation and visual robotic manipulation
environments.

</details>

### [245] [EpiLLM: Unlocking the Potential of Large Language Models in Epidemic Forecasting](https://arxiv.org/abs/2505.12738)
*Chenghua Gong,Rui Sun,Yuhao Zheng,Juyuan Zhang,Tianjun Gu,Liming Pan,Linyuan Lv*

Main category: cs.LG

TLDR: 论文提出了EpiLLM，一种基于大语言模型（LLM）的时空流行病预测框架，通过双分支架构和自回归建模范式提升预测能力。


<details>
  <summary>Details</summary>
Motivation: 流行病预测对公共卫生安全至关重要，但现有LLM在此领域的潜力尚未充分探索。

Method: 提出EpiLLM框架，采用双分支架构实现复杂流行病模式与语言标记的细粒度对齐，并引入自回归建模范式和数据驱动的时空提示学习技术。

Result: 在真实COVID-19数据集上，EpiLLM显著优于现有基线，并展现出LLM的扩展特性。

Conclusion: EpiLLM为流行病预测提供了高效的新方法，验证了LLM在此领域的潜力。

Abstract: Advanced epidemic forecasting is critical for enabling precision containment
strategies, highlighting its strategic importance for public health security.
While recent advances in Large Language Models (LLMs) have demonstrated
effectiveness as foundation models for domain-specific tasks, their potential
for epidemic forecasting remains largely unexplored. In this paper, we
introduce EpiLLM, a novel LLM-based framework tailored for spatio-temporal
epidemic forecasting. Considering the key factors in real-world epidemic
transmission: infection cases and human mobility, we introduce a dual-branch
architecture to achieve fine-grained token-level alignment between such complex
epidemic patterns and language tokens for LLM adaptation. To unleash the
multi-step forecasting and generalization potential of LLM architectures, we
propose an autoregressive modeling paradigm that reformulates the epidemic
forecasting task into next-token prediction. To further enhance LLM perception
of epidemics, we introduce spatio-temporal prompt learning techniques, which
strengthen forecasting capabilities from a data-driven perspective. Extensive
experiments show that EpiLLM significantly outperforms existing baselines on
real-world COVID-19 datasets and exhibits scaling behavior characteristic of
LLMs.

</details>

### [246] [PEER pressure: Model-to-Model Regularization for Single Source Domain Generalization](https://arxiv.org/abs/2505.12745)
*Dong Kyu Cho,Inwoo Hwang,Sanghack Lee*

Main category: cs.LG

TLDR: 论文提出了一种名为PEER的新方法，通过参数空间集成和熵正则化减少数据增强训练中的性能波动，提升模型在未见目标域的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 数据增强方法在训练过程中性能波动普遍存在，导致模型选择困难，原因是模型无法积累多样化增强数据的学习知识。

Method: 提出PEER方法，使用代理模型学习增强数据，主模型通过参数平均逐步积累知识，并通过最大化输出表示的互信息减少特征失真。

Result: 实验表明PEER能有效减少OOD性能波动，并在多个数据集上实现SOTA性能。

Conclusion: PEER通过简单随机增强即可超越复杂数据增强策略的先前方法，显著提升泛化能力。

Abstract: Data augmentation is a popular tool for single source domain generalization,
which expands the source domain by generating simulated ones, improving
generalization on unseen target domains. In this work, we show that the
performance of such augmentation-based methods in the target domains
universally fluctuates during training, posing challenges in model selection
under realistic scenarios. We argue that the fluctuation stems from the
inability of the model to accumulate the knowledge learned from diverse
augmentations, exacerbating feature distortion during training. Based on this
observation, we propose a novel generalization method, coined Parameter-Space
Ensemble with Entropy Regularization (PEER), that uses a proxy model to learn
the augmented data on behalf of the main model. The main model is updated by
averaging its parameters with the proxy model, progressively accumulating
knowledge over the training steps. Maximizing the mutual information between
the output representations of the two models guides the learning process of the
proxy model, mitigating feature distortion during training. Experimental
results demonstrate the effectiveness of PEER in reducing the OOD performance
fluctuation and enhancing generalization across various datasets, including
PACS, Digits, Office-Home, and VLCS. Notably, our method with simple random
augmentation achieves state-of-the-art performance, surpassing prior approaches
on sDG that utilize complex data augmentation strategies.

</details>

### [247] [Structure-based Anomaly Detection and Clustering](https://arxiv.org/abs/2505.12751)
*Filippo Leveni*

Main category: cs.LG

TLDR: 该论文提出了无监督异常检测的新方法，包括基于结构的检测和流数据检测，并在合成和真实数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 异常检测在医疗、制造和网络安全等领域至关重要，但现有方法在结构和流数据场景中存在不足。

Method: 提出了Preference Isolation Forest（PIF）及其变体Voronoi-iForest和RuzHash-iForest，以及Sliding-PIF用于流数据；MultiLink用于结构聚类；Online-iForest用于流数据检测；MaxLogit用于网络安全。

Result: 新方法在合成和真实数据集上优于现有技术，MultiLink在速度和鲁棒性上表现突出，Online-iForest在实时应用中高效。

Conclusion: 论文提出的方法在异常检测的多个场景中表现出色，部分已投入实际应用。

Abstract: Anomaly detection is a fundamental problem in domains such as healthcare,
manufacturing, and cybersecurity. This thesis proposes new unsupervised methods
for anomaly detection in both structured and streaming data settings. In the
first part, we focus on structure-based anomaly detection, where normal data
follows low-dimensional manifolds while anomalies deviate from them. We
introduce Preference Isolation Forest (PIF), which embeds data into a
high-dimensional preference space via manifold fitting, and isolates outliers
using two variants: Voronoi-iForest, based on geometric distances, and
RuzHash-iForest, leveraging Locality Sensitive Hashing for scalability. We also
propose Sliding-PIF, which captures local manifold information for streaming
scenarios. Our methods outperform existing techniques on synthetic and real
datasets. We extend this to structure-based clustering with MultiLink, a novel
method for recovering multiple geometric model families in noisy data.
MultiLink merges clusters via a model-aware linkage strategy, enabling robust
multi-class structure recovery. It offers key advantages over existing
approaches, such as speed, reduced sensitivity to thresholds, and improved
robustness to poor initial sampling. The second part of the thesis addresses
online anomaly detection in evolving data streams. We propose Online Isolation
Forest (Online-iForest), which uses adaptive, multi-resolution histograms and
dynamically updates tree structures to track changes over time. It avoids
retraining while achieving accuracy comparable to offline models, with superior
efficiency for real-time applications. Finally, we tackle anomaly detection in
cybersecurity via open-set recognition for malware classification. We enhance a
Gradient Boosting classifier with MaxLogit to detect unseen malware families, a
method now integrated into Cleafy's production system.

</details>

### [248] [ProDS: Preference-oriented Data Selection for Instruction Tuning](https://arxiv.org/abs/2505.12754)
*Wenya Guo,Zhengkun Zhang,Xumeng Liu,Ying Zhang,Ziyu Lu,Haoze Zhu,Xubo Liu,Ruxue Yan*

Main category: cs.LG

TLDR: ProDS方法通过偏好导向的数据选择，提升训练数据质量，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视人类对多样回应的偏好，需改进数据选择标准。

Method: 采用直接偏好优化(DPO)和双向偏好合成策略评分训练样本。

Result: 实验证明ProDS优于任务无关和目标导向方法。

Conclusion: ProDS通过偏好对齐显著提升数据选择效果。

Abstract: Instruction data selection aims to identify a high-quality subset from the
training set that matches or exceeds the performance of the full dataset on
target tasks. Existing methods focus on the instruction-to-response mapping,
but neglect the human preference for diverse responses. In this paper, we
propose Preference-oriented Data Selection method (ProDS) that scores training
samples based on their alignment with preferences observed in the target set.
Our key innovation lies in shifting the data selection criteria from merely
estimating features for accurate response generation to explicitly aligning
training samples with human preferences in target tasks. Specifically, direct
preference optimization (DPO) is employed to estimate human preferences across
diverse responses. Besides, a bidirectional preference synthesis strategy is
designed to score training samples according to both positive preferences and
negative preferences. Extensive experimental results demonstrate our
superiority to existing task-agnostic and targeted methods.

</details>

### [249] [Your Offline Policy is Not Trustworthy: Bilevel Reinforcement Learning for Sequential Portfolio Optimization](https://arxiv.org/abs/2505.12759)
*Haochen Yuan,Minting Pan,Yunbo Wang,Siyu Gao,Philip S. Yu,Xiaokang Yang*

Main category: cs.LG

TLDR: MetaTrader提出了一种新的部分离线强化学习方法，用于投资组合优化，通过双层学习框架和新的TD方法提升泛化性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在固定数据集上训练的策略泛化性差，无法适应市场的非平稳性。

Method: 采用双层学习框架，同时优化原始数据和变换数据的性能；引入新的TD方法，解决离线数据有限时的价值高估问题。

Result: 在两个公开股票数据集上，MetaTrader优于现有方法，包括传统RL和股票预测模型。

Conclusion: MetaTrader通过改进泛化性和解决价值高估问题，显著提升了投资组合优化的性能。

Abstract: Reinforcement learning (RL) has shown significant promise for sequential
portfolio optimization tasks, such as stock trading, where the objective is to
maximize cumulative returns while minimizing risks using historical data.
However, traditional RL approaches often produce policies that merely memorize
the optimal yet impractical buying and selling behaviors within the fixed
dataset. These offline policies are less generalizable as they fail to account
for the non-stationary nature of the market. Our approach, MetaTrader, frames
portfolio optimization as a new type of partial-offline RL problem and makes
two technical contributions. First, MetaTrader employs a bilevel learning
framework that explicitly trains the RL agent to improve both in-domain profits
on the original dataset and out-of-domain performance across diverse
transformations of the raw financial data. Second, our approach incorporates a
new temporal difference (TD) method that approximates worst-case TD estimates
from a batch of transformed TD targets, addressing the value overestimation
issue that is particularly challenging in scenarios with limited offline data.
Our empirical results on two public stock datasets show that MetaTrader
outperforms existing methods, including both RL-based approaches and
traditional stock prediction models.

</details>

### [250] [Enhancing Channel-Independent Time-Series Forecasting via Cross-Variate Patch Embedding](https://arxiv.org/abs/2505.12761)
*Donghwa Shin,Edwin Zhang*

Main category: cs.LG

TLDR: 论文提出了一种轻量级的跨变量补丁嵌入（CVPE）模块，通过修改补丁嵌入过程，将跨变量上下文注入通道独立（CI）模型中，从而提升时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在时间序列预测中主要关注时间依赖性，忽略了变量间复杂关系，且完全依赖通道（CD），容易过拟合。

Method: 提出CVPE模块，通过添加可学习的位置编码和轻量级路由器注意力块，将跨变量信息注入CI模型，并集成到Time-LLM中。

Result: 在七个真实数据集上的实验表明，仅通过加入CVPE模块，改进后的Time-LLM显著优于原始基线模型。

Conclusion: CVPE模块有效提升了CI模型在跨变量依赖关系建模中的性能，且无需其他改动。

Abstract: Transformers have recently gained popularity in time series forecasting due
to their ability to capture long-term dependencies. However, many existing
models focus only on capturing temporal dependencies while omitting intricate
relationships between variables. Recent models have tried tackling this by
explicitly modeling both cross-time and cross-variate dependencies through a
sequential or unified attention mechanism, but they are entirely channel
dependent (CD) across all layers, making them potentially susceptible to
overfitting. To address this, we propose Cross-Variate Patch Embeddings (CVPE),
a lightweight CD module that injects cross-variate context into
channel-independent (CI) models by simply modifying the patch embedding
process. We achieve this by adding a learnable positional encoding and a
lightweight router-attention block to the vanilla patch embedding layer. We
then integrate CVPE into Time-LLM, a multimodal CI forecasting model, to
demonstrate its effectiveness in capturing cross-variate dependencies and
enhance the CI model's performance. Extensive experimental results on seven
real-world datasets show that our enhanced Time-LLM outperforms the original
baseline model simply by incorporating the CVPE module, with no other changes.

</details>

### [251] [Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization](https://arxiv.org/abs/2505.12763)
*Sunghwan Kim,Dongjin Kang,Taeyoon Kwon,Hyungjoo Chae,Dongha Lee,Jinyoung Yeo*

Main category: cs.LG

TLDR: 论文探讨了奖励模型（RMs）在强化学习中的评估问题，提出通过奖励过优化现象构建可靠基准的方法。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型基准与优化策略性能相关性弱，无法准确评估奖励模型能力。

Method: 通过奖励过优化现象探索评估设计，分析其对策略学习信号的影响。

Result: 提出构建可靠基准的三个关键发现，并指出奖励过优化程度与下游性能的权衡。

Conclusion: 设计基准时应将奖励过优化程度作为工具而非最终目标。

Abstract: Reward models (RMs) play a crucial role in reinforcement learning from human
feedback (RLHF), aligning model behavior with human preferences. However,
existing benchmarks for reward models show a weak correlation with the
performance of optimized policies, suggesting that they fail to accurately
assess the true capabilities of RMs. To bridge this gap, we explore several
evaluation designs through the lens of reward overoptimization\textemdash a
phenomenon that captures both how well the reward model aligns with human
preferences and the dynamics of the learning signal it provides to the policy.
The results highlight three key findings on how to construct a reliable
benchmark: (i) it is important to minimize differences between chosen and
rejected responses beyond correctness, (ii) evaluating reward models requires
multiple comparisons across a wide range of chosen and rejected responses, and
(iii) given that reward models encounter responses with diverse
representations, responses should be sourced from a variety of models. However,
we also observe that a extremely high correlation with degree of
overoptimization leads to comparatively lower correlation with certain
downstream performance. Thus, when designing a benchmark, it is desirable to
use the degree of overoptimization as a useful tool, rather than the end goal.

</details>

### [252] [FedSVD: Adaptive Orthogonalization for Private Federated Learning with LoRA](https://arxiv.org/abs/2505.12805)
*Seanie Lee,Sangwoo Park,Dong Bok Lee,Dominik Wagner,Haebin Seong,Tobias Bocklet,Juho Lee,Sung Ju Hwang*

Main category: cs.LG

TLDR: FedSVD通过SVD重新参数化LoRA，优化B矩阵并聚合，避免噪声放大，提升差分隐私下的模型性能。


<details>
  <summary>Details</summary>
Motivation: LoRA在差分隐私梯度下降（DP-SGD）中面临噪声放大的问题，限制了模型表达能力。

Method: 提出FedSVD方法，客户端优化B矩阵，服务器通过SVD重新参数化BA，生成新的A和B。

Result: FedSVD在多种隐私设置和基准测试中表现优于基线，提升了稳定性和性能。

Conclusion: FedSVD通过SVD重新参数化有效解决了噪声放大问题，适用于差分隐私和非隐私场景。

Abstract: Low-Rank Adaptation (LoRA), which introduces a product of two trainable
low-rank matrices into frozen pre-trained weights, is widely used for efficient
fine-tuning of language models in federated learning (FL). However, when
combined with differentially private stochastic gradient descent (DP-SGD), LoRA
faces substantial noise amplification: DP-SGD perturbs per-sample gradients,
and the matrix multiplication of the LoRA update ($BA$) intensifies this
effect. Freezing one matrix (e.g., $A$) reduces the noise but restricts model
expressiveness, often resulting in suboptimal adaptation. To address this, we
propose FedSVD, a simple yet effective method that introduces a global
reparameterization based on singular value decomposition (SVD). In our
approach, each client optimizes only the $B$ matrix and transmits it to the
server. The server aggregates the $B$ matrices, computes the product $BA$ using
the previous $A$, and refactorizes the result via SVD. This yields a new
adaptive $A$ composed of the orthonormal right singular vectors of $BA$, and an
updated $B$ containing the remaining SVD components. This reparameterization
avoids quadratic noise amplification, while allowing $A$ to better capture the
principal directions of the aggregate updates. Moreover, the orthonormal
structure of $A$ bounds the gradient norms of $B$ and preserves more signal
under DP-SGD, as confirmed by our theoretical analysis. As a result, FedSVD
consistently improves stability and performance across a variety of privacy
settings and benchmarks, outperforming relevant baselines under both private
and non-private regimes.

</details>

### [253] [Koopman Autoencoders Learn Neural Representation Dynamics](https://arxiv.org/abs/2505.12809)
*Nishant Suresh Aswani,Saif Eddin Jabari*

Main category: cs.LG

TLDR: 论文提出了一种基于动态系统理论的Koopman自编码器方法，用于建模神经网络内部表示的演化过程。


<details>
  <summary>Details</summary>
Motivation: 探索是否能用动态系统理论建模神经网络内部表示的变换。

Method: 引入Koopman自编码器，将神经网络表示视为动态系统状态，学习一个线性空间中的代理模型。

Result: 代理模型能复现神经网络中观察到的拓扑简化现象，并在Yin-Yang和MNIST任务中实现目标类别遗忘。

Conclusion: Koopman自编码器提供了一种线性化且保留拓扑结构的神经网络表示建模方法，具有实际应用价值。

Abstract: This paper explores a simple question: can we model the internal
transformations of a neural network using dynamical systems theory? We
introduce Koopman autoencoders to capture how neural representations evolve
through network layers, treating these representations as states in a dynamical
system. Our approach learns a surrogate model that predicts how neural
representations transform from input to output, with two key advantages. First,
by way of lifting the original states via an autoencoder, it operates in a
linear space, making editing the dynamics straightforward. Second, it preserves
the topologies of the original representations by regularizing the autoencoding
objective. We demonstrate that these surrogate models naturally replicate the
progressive topological simplification observed in neural networks. As a
practical application, we show how our approach enables targeted class
unlearning in the Yin-Yang and MNIST classification tasks.

</details>

### [254] [Theoretical Investigation on Inductive Bias of Isolation Forest](https://arxiv.org/abs/2505.12825)
*Qin-Cheng Zheng,Shao-Qun Zhang,Shen-Huan Lyu,Yuan Jiang,Zhi-Hua Zhou*

Main category: cs.LG

TLDR: 本文通过理论分析iForest的深度函数和生长过程，揭示了其成功的原因和适用范围，发现iForest对中心异常敏感度较低且参数适应性更强。


<details>
  <summary>Details</summary>
Motivation: 尽管iForest被广泛使用，但其成功的理论基础尚不明确，本文旨在填补这一空白。

Method: 通过将iForest的生长过程建模为随机游走，推导出期望深度函数，并分析其归纳偏差。

Result: iForest对中心异常敏感度较低，参数适应性优于k-近邻异常检测器。

Conclusion: 研究为iForest的有效性提供了理论支持，并为未来理论探索奠定了基础。

Abstract: Isolation Forest (iForest) stands out as a widely-used unsupervised anomaly
detector valued for its exceptional runtime efficiency and performance on
large-scale tasks. Despite its widespread adoption, a theoretical foundation
explaining iForest's success remains unclear. This paper theoretically
investigates the conditions and extent of iForest's effectiveness by analyzing
its inductive bias through the formulation of depth functions and growth
processes. Since directly analyzing the depth function proves intractable due
to iForest's random splitting mechanism, we model the growth process of iForest
as a random walk, enabling us to derive the expected depth function using
transition probabilities. Our case studies reveal key inductive biases: iForest
exhibits lower sensitivity to central anomalies while demonstrating greater
parameter adaptability compared to $k$-Nearest Neighbor anomaly detectors. Our
study provides theoretical understanding of the effectiveness of iForest and
establishes a foundation for further theoretical exploration.

</details>

### [255] [GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents](https://arxiv.org/abs/2505.12842)
*Zheng Wu,Pengzhou Cheng,Zongru Wu,Lingzhong Dong,Zhuosheng Zhang*

Main category: cs.LG

TLDR: 论文提出了一种名为GEM的新方法，用于检测GUI代理中的分布外（OOD）指令，通过高斯混合模型拟合输入嵌入距离，显著提升了检测准确率。


<details>
  <summary>Details</summary>
Motivation: GUI代理在执行超出其能力范围或违反环境约束的指令时可能导致任务失败或安全威胁，因此需要有效的OOD检测方法。

Method: 基于观察到输入语义空间的聚类模式，提出GEM方法，利用高斯混合模型拟合输入嵌入距离以反映代理的能力边界。

Result: 在8个数据集上测试，GEM的平均准确率比最佳基线方法提高了23.70%，并在9种不同骨干网络上验证了其泛化能力。

Conclusion: GEM方法在GUI代理的OOD检测中表现出色，具有较高的准确率和泛化能力。

Abstract: Graphical user interface (GUI) agents have recently emerged as an intriguing
paradigm for human-computer interaction, capable of automatically executing
user instructions to operate intelligent terminal devices. However, when
encountering out-of-distribution (OOD) instructions that violate environmental
constraints or exceed the current capabilities of agents, GUI agents may suffer
task breakdowns or even pose security threats. Therefore, effective OOD
detection for GUI agents is essential. Traditional OOD detection methods
perform suboptimally in this domain due to the complex embedding space and
evolving GUI environments. In this work, we observe that the in-distribution
input semantic space of GUI agents exhibits a clustering pattern with respect
to the distance from the centroid. Based on the finding, we propose GEM, a
novel method based on fitting a Gaussian mixture model over input embedding
distances extracted from the GUI Agent that reflect its capability boundary.
Evaluated on eight datasets spanning smartphones, computers, and web browsers,
our method achieves an average accuracy improvement of 23.70\% over the
best-performing baseline. Analysis verifies the generalization ability of our
method through experiments on nine different backbones. The codes are available
at https://github.com/Wuzheng02/GEM-OODforGUIagents.

</details>

### [256] [Bias Fitting to Mitigate Length Bias of Reward Model in RLHF](https://arxiv.org/abs/2505.12843)
*Kangwen Zhao,Jianfeng Cai,Jinhua Zhu,Ruopei Sun,Dongyun Xue,Wengang Zhou,Li Li,Houqiang Li*

Main category: cs.LG

TLDR: FiMi-RM框架通过自主学习和纠正奖励模型中的长度偏差，解决了RLHF中的奖励黑客问题，提升了模型与人类偏好的对齐效果。


<details>
  <summary>Details</summary>
Motivation: RLHF中的奖励模型存在长度偏差问题，导致模型倾向于生成冗长回答而非高质量内容，现有方法未能准确建模或完全解决这一问题。

Method: FiMi-RM分三阶段：训练含偏差的奖励模型、用轻量级拟合模型捕捉非线性的长度-奖励关系、将关系整合到奖励模型中以去偏。

Result: 实验表明，FiMi-RM实现了更平衡的长度-奖励分布，提升了长度控制的胜率并减少了冗余。

Conclusion: FiMi-RM有效解决了长度偏差问题，提升了RLHF中奖励模型的性能和对齐效果。

Abstract: Reinforcement Learning from Human Feedback relies on reward models to align
large language models with human preferences. However, RLHF often suffers from
reward hacking, wherein policy learning exploits flaws in the trained reward
model to maximize reward scores without genuinely aligning with human
preferences. A significant example of such reward hacking is length bias, where
reward models usually favor longer responses irrespective of actual response
quality. Previous works on length bias have notable limitations, these
approaches either mitigate bias without characterizing the bias form, or simply
assume a linear length-reward relation. To accurately model the intricate
nature of length bias and facilitate more effective bias mitigation, we propose
FiMi-RM (Bias Fitting to Mitigate Length Bias of Reward Model in RLHF), a
framework that autonomously learns and corrects underlying bias patterns. Our
approach consists of three stages: First, we train a standard reward model
which inherently contains length bias. Next, we deploy a lightweight fitting
model to explicitly capture the non-linear relation between length and reward.
Finally, we incorporate this learned relation into the reward model to debias.
Experimental results demonstrate that FiMi-RM achieves a more balanced
length-reward distribution. Furthermore, when applied to alignment algorithms,
our debiased reward model improves length-controlled win rate and reduces
verbosity without compromising its performance.

</details>

### [257] [AdS-GNN -- a Conformally Equivariant Graph Neural Network](https://arxiv.org/abs/2505.12880)
*Maksim Zhdanov,Nabil Iqbal,Erik Bekkers,Patrick Forré*

Main category: cs.LG

TLDR: 提出了一种在一般共形变换下等变的神经网络，通过将数据从欧几里得空间提升到AdS空间实现。


<details>
  <summary>Details</summary>
Motivation: 共形对称性在多个领域至关重要，但现有方法难以处理一般共形变换。

Method: 利用AdS空间与欧几里得空间的共形变换对应关系，构建基于消息传递的等变网络。

Result: 模型在计算机视觉和统计物理任务中表现优异，泛化能力强，并能提取共形数据。

Conclusion: 该方法为共形变换下的深度学习提供了高效框架，具有广泛应用潜力。

Abstract: Conformal symmetries, i.e.\ coordinate transformations that preserve angles,
play a key role in many fields, including physics, mathematics, computer vision
and (geometric) machine learning. Here we build a neural network that is
equivariant under general conformal transformations. To achieve this, we lift
data from flat Euclidean space to Anti de Sitter (AdS) space. This allows us to
exploit a known correspondence between conformal transformations of flat space
and isometric transformations on the AdS space. We then build upon the fact
that such isometric transformations have been extensively studied on general
geometries in the geometric deep learning literature. We employ message-passing
layers conditioned on the proper distance, yielding a computationally efficient
framework. We validate our model on tasks from computer vision and statistical
physics, demonstrating strong performance, improved generalization capacities,
and the ability to extract conformal data such as scaling dimensions from the
trained network.

</details>

### [258] [PhyDA: Physics-Guided Diffusion Models for Data Assimilation in Atmospheric Systems](https://arxiv.org/abs/2505.12882)
*Hao Wang,Jindong Han,Wei Fan,Weijia Zhang,Hao Liu*

Main category: cs.LG

TLDR: PhyDA是一个物理引导的扩散框架，用于大气数据同化，通过整合物理约束和虚拟重建编码器，提高了重建的准确性和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法在数据同化中忽视了物理规律，导致物理不一致的重建结果，影响下游应用。

Method: PhyDA引入物理正则化扩散目标和虚拟重建编码器，将物理约束融入训练过程并解决观测稀疏性问题。

Result: 在ERA5再分析数据集上，PhyDA在准确性和物理合理性上优于现有基线方法。

Conclusion: PhyDA展示了生成建模与领域物理知识结合的重要性，为改进实际数据同化系统提供了新方向。

Abstract: Data Assimilation (DA) plays a critical role in atmospheric science by
reconstructing spatially continous estimates of the system state, which serves
as initial conditions for scientific analysis. While recent advances in
diffusion models have shown great potential for DA tasks, most existing
approaches remain purely data-driven and often overlook the physical laws that
govern complex atmospheric dynamics. As a result, they may yield physically
inconsistent reconstructions that impair downstream applications. To overcome
this limitation, we propose PhyDA, a physics-guided diffusion framework
designed to ensure physical coherence in atmospheric data assimilation. PhyDA
introduces two key components: (1) a Physically Regularized Diffusion Objective
that integrates physical constraints into the training process by penalizing
deviations from known physical laws expressed as partial differential
equations, and (2) a Virtual Reconstruction Encoder that bridges observational
sparsity for structured latent representations, further enhancing the model's
ability to infer complete and physically coherent states. Experiments on the
ERA5 reanalysis dataset demonstrate that PhyDA achieves superior accuracy and
better physical plausibility compared to state-of-the-art baselines. Our
results emphasize the importance of combining generative modeling with
domain-specific physical knowledge and show that PhyDA offers a promising
direction for improving real-world data assimilation systems.

</details>

### [259] [TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks](https://arxiv.org/abs/2505.12884)
*Yuanze Hu,Zhaoxin Fan,Xinyu Wang,Gen Li,Ye Qiu,Zhichao Yang,Wenjun Wu,Kejian Wu,Yifan Sun,Xiaotie Deng,Jin Dong*

Main category: cs.LG

TLDR: 论文提出TinyAlign框架，通过检索增强生成策略提升轻量级视觉-语言模型的对齐效果，显著减少训练损失并提高数据效率。


<details>
  <summary>Details</summary>
Motivation: 轻量级视觉-语言模型在资源受限应用中不可或缺，但现有对齐方法依赖语言模型固有能力，导致对齐质量受限。

Method: 提出TinyAlign框架，利用检索增强生成策略从记忆库中检索相关上下文，丰富多模态输入以提升对齐效果。

Result: 实验表明TinyAlign显著降低训练损失、加速收敛，并仅需40%微调数据即可达到基线性能。

Conclusion: TinyAlign为轻量级视觉-语言模型开发提供了实用路径，并为理解多模态系统对齐瓶颈提供了新视角。

Abstract: Lightweight Vision-Language Models (VLMs) are indispensable for
resource-constrained applications. The prevailing approach to aligning vision
and language models involves freezing both the vision encoder and the language
model while training small connector modules. However, this strategy heavily
depends on the intrinsic capabilities of the language model, which can be
suboptimal for lightweight models with limited representational capacity. In
this work, we investigate this alignment bottleneck through the lens of mutual
information, demonstrating that the constrained capacity of the language model
inherently limits the Effective Mutual Information (EMI) between multimodal
inputs and outputs, thereby compromising alignment quality. To address this
challenge, we propose TinyAlign, a novel framework inspired by
Retrieval-Augmented Generation, which strategically retrieves relevant context
from a memory bank to enrich multimodal inputs and enhance their alignment.
Extensive empirical evaluations reveal that TinyAlign significantly reduces
training loss, accelerates convergence, and enhances task performance.
Remarkably, it allows models to achieve baseline-level performance with only
40\% of the fine-tuning data, highlighting exceptional data efficiency. Our
work thus offers a practical pathway for developing more capable lightweight
VLMs while introducing a fresh theoretical lens to better understand and
address alignment bottlenecks in constrained multimodal systems.

</details>

### [260] [Efficient training for large-scale optical neural network using an evolutionary strategy and attention pruning](https://arxiv.org/abs/2505.12906)
*Zhiwei Yang,Zeyang Fan,Yihang Lai,Qi Chen,Tian Zhang,Jian Dai,Kun Xu*

Main category: cs.LG

TLDR: 提出了一种用于大规模BONNs的CAP算法，通过修剪矩阵块和直接优化种群个体，显著减少了参数数量和计算成本，同时保持了较高的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前BONNs的训练算法鲁棒性不足，且大规模网络参数多导致计算和功耗高。

Method: 采用基于协方差矩阵适应进化策略和注意力修剪的CAP算法，修剪矩阵块并优化种群个体。

Result: CAP算法可修剪60%-80%参数，性能仅下降3.289%-4.693%；在噪声环境下表现最强鲁棒性，实验准确率达88.5%。

Conclusion: CAP算法在大规模网络和复杂任务中潜力显著，且仅使用内部相位调制器的MZI结构可进一步减少系统面积和参数。

Abstract: MZI-based block optical neural networks (BONNs), which can achieve
large-scale network models, have increasingly drawn attentions. However, the
robustness of the current training algorithm is not high enough. Moreover,
large-scale BONNs usually contain numerous trainable parameters, resulting in
expensive computation and power consumption. In this article, by pruning matrix
blocks and directly optimizing the individuals in population, we propose an
on-chip covariance matrix adaptation evolution strategy and attention-based
pruning (CAP) algorithm for large-scale BONNs. The calculated results
demonstrate that the CAP algorithm can prune 60% and 80% of the parameters for
MNIST and Fashion-MNIST datasets, respectively, while only degrades the
performance by 3.289% and 4.693%. Considering the influence of dynamic noise in
phase shifters, our proposed CAP algorithm (performance degradation of 22.327%
for MNIST dataset and 24.019% for Fashion-MNIST dataset utilizing a poor
fabricated chip and electrical control with a standard deviation of 0.5)
exhibits strongest robustness compared with both our previously reported block
adjoint training algorithm (43.963% and 41.074%) and the covariance matrix
adaptation evolution strategy (25.757% and 32.871%), respectively. Moreover,
when 60% of the parameters are pruned, the CAP algorithm realizes 88.5%
accuracy in experiment for the simplified MNIST dataset, which is similar to
the simulation result without noise (92.1%). Additionally, we simulationally
and experimentally demonstrate that using MZIs with only internal phase
shifters to construct BONNs is an efficient way to reduce both the system area
and the required trainable parameters. Notably, our proposed CAP algorithm show
excellent potential for larger-scale network models and more complex tasks.

</details>

### [261] [Sinusoidal Initialization, Time for a New Start](https://arxiv.org/abs/2505.12909)
*Alberto Fernández-Hernández,Jose I. Mestre,Manuel F. Dolz,Jose Duato,Enrique S. Quintana-Ortí*

Main category: cs.LG

TLDR: 提出了一种名为Sinusoidal初始化的确定性方法，通过正弦函数构建结构化权重矩阵，以改善权重分布和神经元激活状态的均匀性，从而提高收敛速度、训练稳定性和最终准确性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练中初始化对收敛、稳定性和泛化能力至关重要，传统随机初始化方法可能导致权重分布不均。

Method: 使用正弦函数构造结构化权重矩阵，确保权重和激活状态从一开始就均匀分布。

Result: 实验表明，该方法平均提高了4.8%的验证准确率和20.9%的收敛速度。

Conclusion: Sinusoidal初始化通过结构化方法取代随机性，为深度学习系统提供了更强、更可靠的基础。

Abstract: Initialization plays a critical role in Deep Neural Network training,
directly influencing convergence, stability, and generalization. Common
approaches such as Glorot and He initializations rely on randomness, which can
produce uneven weight distributions across layer connections. In this paper, we
introduce the Sinusoidal initialization, a novel deterministic method that
employs sinusoidal functions to construct structured weight matrices expressly
to improve the spread and balance of weights throughout the network while
simultaneously fostering a more uniform, well-conditioned distribution of
neuron activation states from the very first forward pass. Because Sinusoidal
initialization begins with weights and activations that are already evenly and
efficiently utilized, it delivers consistently faster convergence, greater
training stability, and higher final accuracy across a wide range of models,
including convolutional neural networks, vision transformers, and large
language models. On average, our experiments show an increase of 4.8 % in final
validation accuracy and 20.9 % in convergence speed. By replacing randomness
with structure, this initialization provides a stronger and more reliable
foundation for Deep Learning systems.

</details>

### [262] [Active Learning on Synthons for Molecular Design](https://arxiv.org/abs/2505.12913)
*Tom George Grigg,Mason Burlage,Oliver Brook Scott,Adam Taouil,Dominique Sydow,Liam Wilbraham*

Main category: cs.LG

TLDR: SALSA算法通过片段选择扩展池式主动学习，解决了药物发现中组合空间的高成本问题，表现出高效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现代药物发现中，组合空间的穷举虚拟筛选成本高且难以实现，尤其在多向量扩展等复杂场景下。

Method: SALSA算法通过片段选择建模和获取，扩展池式主动学习至不可枚举空间。

Result: 实验显示SALSA在样本效率和可扩展性上表现优异，能处理万亿级化合物空间，生成分子多样性和评分优于现有方法。

Conclusion: SALSA在多参数目标设计任务中表现突出，生成的分子与已知生物活性分子相当，且更具多样性。

Abstract: Exhaustive virtual screening is highly informative but often intractable
against the expensive objective functions involved in modern drug discovery.
This problem is exacerbated in combinatorial contexts such as multi-vector
expansion, where molecular spaces can quickly become ultra-large. Here, we
introduce Scalable Active Learning via Synthon Acquisition (SALSA): a simple
algorithm applicable to multi-vector expansion which extends pool-based active
learning to non-enumerable spaces by factoring modeling and acquisition over
synthon or fragment choices. Through experiments on ligand- and structure-based
objectives, we highlight SALSA's sample efficiency, and its ability to scale to
spaces of trillions of compounds. Further, we demonstrate application toward
multi-parameter objective design tasks on three protein targets - finding
SALSA-generated molecules have comparable chemical property profiles to known
bioactives, and exhibit greater diversity and higher scores over an
industry-leading generative approach.

</details>

### [263] [Temporal Query Network for Efficient Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.12917)
*Shengsheng Lin,Haojun Chen,Haijie Wu,Chunyun Qiu,Weiwei Lin*

Main category: cs.LG

TLDR: 提出了一种名为Temporal Query (TQ)的新技术，通过注意力机制捕捉多元时间序列的全局相关性，并构建了TQNet模型，在12个数据集上实现了最先进的预测精度和高效率。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列预测中，有效建模变量间相关性对准确性至关重要。

Method: 采用周期性移动的可学习向量作为注意力机制中的查询，结合原始输入数据生成键和值，捕捉全局和局部相关性，构建了单层注意力机制和轻量级MLP的TQNet模型。

Result: TQNet在12个真实数据集上实现了最先进的预测精度，同时在高维数据集上保持了与线性方法相当的高效率。

Conclusion: TQNet通过TQ技术有效捕捉多元相关性，在性能和计算成本之间取得了平衡。

Abstract: Sufficiently modeling the correlations among variables (aka channels) is
crucial for achieving accurate multivariate time series forecasting (MTSF). In
this paper, we propose a novel technique called Temporal Query (TQ) to more
effectively capture multivariate correlations, thereby improving model
performance in MTSF tasks. Technically, the TQ technique employs periodically
shifted learnable vectors as queries in the attention mechanism to capture
global inter-variable patterns, while the keys and values are derived from the
raw input data to encode local, sample-level correlations. Building upon the TQ
technique, we develop a simple yet efficient model named Temporal Query Network
(TQNet), which employs only a single-layer attention mechanism and a
lightweight multi-layer perceptron (MLP). Extensive experiments demonstrate
that TQNet learns more robust multivariate correlations, achieving
state-of-the-art forecasting accuracy across 12 challenging real-world
datasets. Furthermore, TQNet achieves high efficiency comparable to
linear-based methods even on high-dimensional datasets, balancing performance
and computational cost. The code is available at:
https://github.com/ACAT-SCUT/TQNet.

</details>

### [264] [RGNMR: A Gauss-Newton method for robust matrix completion with theoretical guarantees](https://arxiv.org/abs/2505.12919)
*Eilon Vaknin Laufer,Boaz Nadler*

Main category: cs.LG

TLDR: 本文提出了一种名为RGNMR的新型鲁棒矩阵补全方法，克服了现有方法的局限性，如需要大量观测数据、对超参数化和病态矩阵的恢复能力不足。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒矩阵补全方法在观测数据较少、超参数化或矩阵病态时表现不佳，因此需要一种更有效的方法。

Method: RGNMR是一种基于因子化的迭代算法，结合了高斯-牛顿线性化和异常值去除技术。

Result: 理论证明RGNMR能在适当条件下精确恢复低秩矩阵，实验显示其优于现有方法，尤其在数据少、超参数化和病态矩阵情况下。

Conclusion: RGNMR是一种高效且鲁棒的矩阵补全方法，适用于多种复杂场景。

Abstract: Recovering a low rank matrix from a subset of its entries, some of which may
be corrupted, is known as the robust matrix completion (RMC) problem. Existing
RMC methods have several limitations: they require a relatively large number of
observed entries; they may fail under overparametrization, when their assumed
rank is higher than the correct one; and many of them fail to recover even
mildly ill-conditioned matrices. In this paper we propose a novel RMC method,
denoted $\texttt{RGNMR}$, which overcomes these limitations. $\texttt{RGNMR}$
is a simple factorization-based iterative algorithm, which combines a
Gauss-Newton linearization with removal of entries suspected to be outliers. On
the theoretical front, we prove that under suitable assumptions,
$\texttt{RGNMR}$ is guaranteed exact recovery of the underlying low rank
matrix. Our theoretical results improve upon the best currently known for
factorization-based methods. On the empirical front, we show via several
simulations the advantages of $\texttt{RGNMR}$ over existing RMC methods, and
in particular its ability to handle a small number of observed entries,
overparameterization of the rank and ill-conditioned matrices.

</details>

### [265] [Leveraging LLM Inconsistency to Boost Pass@k Performance](https://arxiv.org/abs/2505.12938)
*Uri Dalal,Meirav Segal,Zvika Ben-Haim,Dan Lahav,Omer Nevo*

Main category: cs.LG

TLDR: 通过利用大型语言模型（LLM）的不一致性，提出了一种名为“Variator”的代理方法，生成任务变体以提升Pass@k性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多个领域表现出色，但其对微小输入变化的响应不一致性被视为潜在优势。

Method: 引入“Variator”代理，生成任务变体并为每个变体提交候选解决方案，方法适用于多种领域。

Result: 在APPS数据集上实证表现优于基线，理论模型验证了不一致性的有效性。

Conclusion: 不一致性在高级推理模型中持续存在，表明该方法对未来模型仍具价值。

Abstract: Large language models (LLMs) achieve impressive abilities in numerous
domains, but exhibit inconsistent performance in response to minor input
changes. Rather than view this as a drawback, in this paper we introduce a
novel method for leveraging models' inconsistency to boost Pass@k performance.
Specifically, we present a "Variator" agent that generates k variants of a
given task and submits one candidate solution for each one. Our variant
generation approach is applicable to a wide range of domains as it is task
agnostic and compatible with free-form inputs. We demonstrate the efficacy of
our agent theoretically using a probabilistic model of the inconsistency
effect, and show empirically that it outperforms the baseline on the APPS
dataset. Furthermore, we establish that inconsistency persists even in frontier
reasoning models across coding and cybersecurity domains, suggesting our method
is likely to remain relevant for future model generations.

</details>

### [266] [Multi-Level Monte Carlo Training of Neural Operators](https://arxiv.org/abs/2505.12940)
*James Rowbottom,Stefania Fresca,Pietro Lio,Carola-Bibiane Schönlieb,Nicolas Boullé*

Main category: cs.LG

TLDR: 提出了一种基于多级蒙特卡罗（MLMC）的方法来训练神经算子，通过利用多分辨率函数离散化层次结构，降低计算成本同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子在处理大规模高分辨率问题时训练成本高昂，因此需要一种更高效的方法。

Method: 采用多级蒙特卡罗（MLMC）方法，利用多分辨率数据的梯度修正，减少对高分辨率数据的需求。

Result: 实验表明，该方法在多种先进模型和测试案例中显著提高了计算效率，并揭示了精度与计算时间之间的帕累托曲线关系。

Conclusion: MLMC方法为神经算子的高效训练提供了可行方案，适用于接受多分辨率数据的任何架构。

Abstract: Operator learning is a rapidly growing field that aims to approximate
nonlinear operators related to partial differential equations (PDEs) using
neural operators. These rely on discretization of input and output functions
and are, usually, expensive to train for large-scale problems at
high-resolution. Motivated by this, we present a Multi-Level Monte Carlo (MLMC)
approach to train neural operators by leveraging a hierarchy of resolutions of
function dicretization. Our framework relies on using gradient corrections from
fewer samples of fine-resolution data to decrease the computational cost of
training while maintaining a high level accuracy. The proposed MLMC training
procedure can be applied to any architecture accepting multi-resolution data.
Our numerical experiments on a range of state-of-the-art models and test-cases
demonstrate improved computational efficiency compared to traditional
single-resolution training approaches, and highlight the existence of a Pareto
curve between accuracy and computational time, related to the number of samples
per resolution.

</details>

### [267] [CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs](https://arxiv.org/abs/2505.12944)
*Jan Hagnberger,Daniel Musekamp,Mathias Niepert*

Main category: cs.LG

TLDR: CALM-PDE是一种新型模型，用于高效解决任意离散化的PDE问题，通过连续卷积编码-解码架构显著提升内存和推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法在物理空间直接计算PDE成本高，而现有神经代理模型虽降低复杂度，但Transformer机制内存消耗大，卷积网络则受限于规则离散化。

Method: 提出CALM-PDE，采用连续卷积编码-解码架构，利用epsilon邻域约束核和自适应查询点优化。

Result: 在多种PDE上表现优异，内存和推理效率显著优于Transformer方法。

Conclusion: CALM-PDE在解决任意离散化PDE时高效且性能优越。

Abstract: Solving time-dependent Partial Differential Equations (PDEs) using a densely
discretized spatial domain is a fundamental problem in various scientific and
engineering disciplines, including modeling climate phenomena and fluid
dynamics. However, performing these computations directly in the physical space
often incurs significant computational costs. To address this issue, several
neural surrogate models have been developed that operate in a compressed latent
space to solve the PDE. While these approaches reduce computational complexity,
they often use Transformer-based attention mechanisms to handle irregularly
sampled domains, resulting in increased memory consumption. In contrast,
convolutional neural networks allow memory-efficient encoding and decoding but
are limited to regular discretizations. Motivated by these considerations, we
propose CALM-PDE, a model class that efficiently solves arbitrarily discretized
PDEs in a compressed latent space. We introduce a novel continuous
convolution-based encoder-decoder architecture that uses an
epsilon-neighborhood-constrained kernel and learns to apply the convolution
operator to adaptive and optimized query points. We demonstrate the
effectiveness of CALM-PDE on a diverse set of PDEs with both regularly and
irregularly sampled spatial domains. CALM-PDE is competitive with or
outperforms existing baseline methods while offering significant improvements
in memory and inference time efficiency compared to Transformer-based methods.

</details>

### [268] [DGRO: Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management](https://arxiv.org/abs/2505.12951)
*Xuerui Su,Liya Guo,Yue Wang,Yi Zhu,Zhiming Ma,Zun Wang,Yuting Liu*

Main category: cs.LG

TLDR: 论文提出了一种名为DGRO的强化学习算法，用于优化大型语言模型的推理能力，通过解耦策略梯度和采样策略距离的超参数，平衡探索与利用，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的推理方法多依赖手工设计的奖励函数，但其在探索与利用的权衡中缺乏理论和实证研究，因此需要更通用的算法。

Method: 提出DGRO算法，解耦策略梯度和采样策略距离的超参数，并研究奖励方差对模型性能的影响。

Result: DGRO在Logic数据集上达到96.9%的平均准确率，并在数学基准测试中表现出强泛化能力。

Conclusion: DGRO通过解耦超参数和优化奖励方差，显著提升了大型语言模型的推理性能，具有广泛适用性。

Abstract: Inference scaling further accelerates Large Language Models (LLMs) toward
Artificial General Intelligence (AGI), with large-scale Reinforcement Learning
(RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning
approaches usually rely on handcrafted rule-based reward functions. However,
the tarde-offs of exploration and exploitation in RL algorithms involves
multiple complex considerations, and the theoretical and empirical impacts of
manually designed reward functions remain insufficiently explored. In this
paper, we propose Decoupled Group Reward Optimization (DGRO), a general RL
algorithm for LLM reasoning. On the one hand, DGRO decouples the traditional
regularization coefficient into two independent hyperparameters: one scales the
policy gradient term, and the other regulates the distance from the sampling
policy. This decoupling not only enables precise control over balancing
exploration and exploitation, but also can be seamlessly extended to Online
Policy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward
Optimization. On the other hand, we observe that reward variance significantly
affects both convergence speed and final model performance. We conduct both
theoretical analysis and extensive empirical validation to assess DGRO,
including a detailed ablation study that investigates its performance and
optimization dynamics. Experimental results show that DGRO achieves
state-of-the-art performance on the Logic dataset with an average accuracy of
96.9\%, and demonstrates strong generalization across mathematical benchmarks.

</details>

### [269] [LoD: Loss-difference OOD Detection by Intentionally Label-Noisifying Unlabeled Wild Data](https://arxiv.org/abs/2505.12952)
*Chuanxing Geng,Qifei Li,Xinrui Wang,Dong Liang,Songcan Chen,Pong C. Yuen*

Main category: cs.LG

TLDR: 提出了一种新的损失差异OOD检测框架（LoD），通过故意对未标记的野生数据进行标签噪声化，解决了现有方法中ID数据主导学习和OOD阈值选择的难题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理未标记的野生数据时，存在ID数据主导模型学习和OOD阈值选择困难的问题，影响了模型的安全性和可靠性。

Method: 提出LoD框架，通过故意噪声化未标记数据的标签，使ID和OOD数据共同主导模型学习，并利用经典聚类技术（如K-means）无需阈值即可过滤OOD样本。

Result: 理论分析和大量实验验证了LoD的可行性和优越性。

Conclusion: LoD框架有效解决了现有方法的局限性，提升了模型在未标记野生数据中的安全性和可靠性。

Abstract: Using unlabeled wild data containing both in-distribution (ID) and
out-of-distribution (OOD) data to improve the safety and reliability of models
has recently received increasing attention. Existing methods either design
customized losses for labeled ID and unlabeled wild data then perform joint
optimization, or first filter out OOD data from the latter then learn an OOD
detector. While achieving varying degrees of success, two potential issues
remain: (i) Labeled ID data typically dominates the learning of models,
inevitably making models tend to fit OOD data as IDs; (ii) The selection of
thresholds for identifying OOD data in unlabeled wild data usually faces
dilemma due to the unavailability of pure OOD samples. To address these issues,
we propose a novel loss-difference OOD detection framework (LoD) by
\textit{intentionally label-noisifying} unlabeled wild data. Such operations
not only enable labeled ID data and OOD data in unlabeled wild data to jointly
dominate the models' learning but also ensure the distinguishability of the
losses between ID and OOD samples in unlabeled wild data, allowing the classic
clustering technique (e.g., K-means) to filter these OOD samples without
requiring thresholds any longer. We also provide theoretical foundation for
LoD's viability, and extensive experiments verify its superiority.

</details>

### [270] [Hardware-Adaptive and Superlinear-Capacity Memristor-based Associative Memory](https://arxiv.org/abs/2505.12960)
*Chengping He,Mingrui Jiang,Keyi Shan,Szu-Hao Yang,Zefan Li,Shengbo Wang,Giacomo Pedretti,Jim Ignowski,Can Li*

Main category: cs.LG

TLDR: 提出一种新型硬件自适应学习算法，显著提升忆阻器Hopfield神经网络的缺陷容忍度、容量和效率，支持多层架构处理二进制和连续模式。


<details>
  <summary>Details</summary>
Motivation: 传统Hopfield神经网络在硬件实现中存在效率瓶颈，而忆阻器技术虽具潜力，但离线训练导致缺陷容忍度低、容量有限且难以处理模拟模式。

Method: 开发并实验验证了一种硬件自适应学习算法，支持多层架构，通过同步更新提升并行性。

Result: 在50%设备故障下容量提升3倍，多层架构实现超线性容量扩展（N^1.49和N^1.74），能效提升8.8倍，延迟降低99.7%。

Conclusion: 该算法显著提升了忆阻器关联记忆系统的可靠性、容量和灵活性，为新型应用研究铺平道路。

Abstract: Brain-inspired computing aims to mimic cognitive functions like associative
memory, the ability to recall complete patterns from partial cues. Memristor
technology offers promising hardware for such neuromorphic systems due to its
potential for efficient in-memory analog computing. Hopfield Neural Networks
(HNNs) are a classic model for associative memory, but implementations on
conventional hardware suffer from efficiency bottlenecks, while prior
memristor-based HNNs faced challenges with vulnerability to hardware defects
due to offline training, limited storage capacity, and difficulty processing
analog patterns. Here we introduce and experimentally demonstrate on integrated
memristor hardware a new hardware-adaptive learning algorithm for associative
memories that significantly improves defect tolerance and capacity, and
naturally extends to scalable multilayer architectures capable of handling both
binary and continuous patterns. Our approach achieves 3x effective capacity
under 50% device faults compared to state-of-the-art methods. Furthermore, its
extension to multilayer architectures enables superlinear capacity scaling
(\(\propto N^{1.49}\ for binary patterns) and effective recalling of continuous
patterns (\propto N^{1.74}\ scaling), as compared to linear capacity scaling
for previous HNNs. It also provides flexibility to adjust capacity by tuning
hidden neurons for the same-sized patterns. By leveraging the massive
parallelism of the hardware enabled by synchronous updates, it reduces energy
by 8.8x and latency by 99.7% for 64-dimensional patterns over asynchronous
schemes, with greater improvements at scale. This promises the development of
more reliable memristor-based associative memory systems and enables new
applications research due to the significantly improved capacity, efficiency,
and flexibility.

</details>

### [271] [Augmented Regression Models using Neurochaos Learning](https://arxiv.org/abs/2505.12967)
*Akhila Henry,Nithin Nagaraj*

Main category: cs.LG

TLDR: 该研究提出了一种基于神经混沌学习（NL）的新型增强回归模型，通过将NL框架中的Tracemean特征与传统回归算法结合，显著提升了回归性能。


<details>
  <summary>Details</summary>
Motivation: 探索混沌启发的特征在回归任务中的潜力，以提高预测模型的准确性和计算效率。

Method: 将NL框架中的Tracemean特征与线性回归、岭回归、Lasso回归和支持向量回归（SVR）结合，并在十种真实数据集和一种合成数据集上评估性能。

Result: Tracemean特征显著提升了回归性能，尤其在增强Lasso回归和增强SVR中表现突出，其中增强混沌岭回归的平均性能提升最高（11.35%）。

Conclusion: 该研究表明混沌启发的特征在回归任务中具有潜力，为更准确和高效的预测模型提供了新途径。

Abstract: This study presents novel Augmented Regression Models using Neurochaos
Learning (NL), where Tracemean features derived from the Neurochaos Learning
framework are integrated with traditional regression algorithms : Linear
Regression, Ridge Regression, Lasso Regression, and Support Vector Regression
(SVR). Our approach was evaluated using ten diverse real-life datasets and a
synthetically generated dataset of the form $y = mx + c + \epsilon$. Results
show that incorporating the Tracemean feature (mean of the chaotic neural
traces of the neurons in the NL architecture) significantly enhances regression
performance, particularly in Augmented Lasso Regression and Augmented SVR,
where six out of ten real-life datasets exhibited improved predictive accuracy.
Among the models, Augmented Chaotic Ridge Regression achieved the highest
average performance boost (11.35 %). Additionally, experiments on the simulated
dataset demonstrated that the Mean Squared Error (MSE) of the augmented models
consistently decreased and converged towards the Minimum Mean Squared Error
(MMSE) as the sample size increased. This work demonstrates the potential of
chaos-inspired features in regression tasks, offering a pathway to more
accurate and computationally efficient prediction models.

</details>

### [272] [Multi-parameter Control for the (1+($λ$,$λ$))-GA on OneMax via Deep Reinforcement Learning](https://arxiv.org/abs/2505.12982)
*Tai Nguyen,Phong Le,Carola Doerr,Nguyen Dang*

Main category: cs.LG

TLDR: 该论文研究了动态参数控制在遗传算法中的优势，并通过深度强化学习技术优化了$(1+(\lambda,\lambda))$遗传算法的参数控制策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 动态参数控制在优化过程中能显著提升算法性能，但多参数控制仍具挑战性。本文旨在探索深度强化学习在多参数控制中的应用。

Method: 使用深度强化学习技术，对$(1+(\lambda,\lambda))$遗传算法的四个主要参数进行解耦和优化。

Result: 深度强化学习找到的策略优于现有所有控制策略，新策略比理论推荐设置和现有最佳策略分别提升27%和13%。

Conclusion: 深度强化学习在多参数控制中表现强大，能显著提升算法性能，为未来研究提供了新方向。

Abstract: It is well known that evolutionary algorithms can benefit from dynamic
choices of the key parameters that control their behavior, to adjust their
search strategy to the different stages of the optimization process. A
prominent example where dynamic parameter choices have shown a provable
super-constant speed-up is the $(1+(\lambda,\lambda))$ Genetic Algorithm
optimizing the OneMax function. While optimal parameter control policies result
in linear expected running times, this is not possible with static parameter
choices. This result has spurred a lot of interest in parameter control
policies. However, many works, in particular theoretical running time analyses,
focus on controlling one single parameter. Deriving policies for controlling
multiple parameters remains very challenging. In this work we reconsider the
problem of the $(1+(\lambda,\lambda))$ Genetic Algorithm optimizing OneMax. We
decouple its four main parameters and investigate how well state-of-the-art
deep reinforcement learning techniques can approximate good control policies.
We show that although making deep reinforcement learning learn effectively is a
challenging task, once it works, it is very powerful and is able to find
policies that outperform all previously known control policies on the same
benchmark. Based on the results found through reinforcement learning, we derive
a simple control policy that consistently outperforms the default
theory-recommended setting by $27\%$ and the irace-tuned policy, the strongest
existing control policy on this benchmark, by $13\%$, for all tested problem
sizes up to $40{,}000$.

</details>

### [273] [Optimal Formats for Weight Quantisation](https://arxiv.org/abs/2505.12988)
*Douglas Orr,Luka Ribar,Carlo Luschi*

Main category: cs.LG

TLDR: 提出了一种系统设计和分析量化格式的框架，通过连接经典量化理论和KL散度优化，证明了变长编码的优越性，并推导出参数张量的最优比特分配。


<details>
  <summary>Details</summary>
Motivation: 量化格式的选择通常依赖经验，缺乏系统设计方法，因此需要一种理论框架来优化量化格式。

Method: 将量化格式设计与经典量化理论结合，通过最小化KL散度优化量化误差，评估变长编码与定长编码的性能。

Result: 变长编码优于定长编码，均匀量化加无损压缩是最优方案，块格式和稀疏异常格式也表现良好。比特分配优化节省了0.25比特/参数。

Conclusion: 变长编码在量化中具有显著优势，系统设计框架为量化格式选择提供了理论支持。

Abstract: Weight quantisation is an essential technique for enabling efficient training
and deployment of modern deep learning models. However, the recipe book of
quantisation formats is large and the formats are often chosen empirically. In
this paper, we propose a framework for systematic design and analysis of
quantisation formats. By connecting the question of format design with the
classical quantisation theory, we show that the strong practical performance of
popular formats comes from their ability to represent values using
variable-length codes. Framing the optimisation problem as minimising the KL
divergence between the original and quantised model outputs, the objective is
aligned with minimising the squared quantisation error of the model parameters.
We therefore develop and evaluate squared-error-optimal formats for known
distributions, observing significant improvement of variable-length codes over
fixed-length codes. Uniform quantisation followed by lossless compression with
a variable-length code is shown to be optimal. However, we find that commonly
used block formats and sparse outlier formats also outperform fixed-length
codes, implying they also exploit variable-length encoding. Finally, by using
the relationship between the Fisher information and KL divergence, we derive
the optimal allocation of bit-widths to individual parameter tensors across the
model's layers, saving up to 0.25 bits per parameter when tested with
direct-cast quantisation of language models.

</details>

### [274] [Fractured Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.12992)
*Baohao Liao,Hanze Dong,Yuhui Xu,Doyen Sahoo,Christof Monz,Junnan Li,Caiming Xiong*

Main category: cs.LG

TLDR: Fractured Sampling是一种推理时策略，通过调整推理轨迹的数量、最终解的数量和截断深度，显著减少令牌成本，同时保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决Chain-of-Thought（CoT）及其扩展Long CoT在延迟敏感场景中因高令牌成本而难以部署的问题。

Method: 提出Fractured Sampling，通过三个正交维度（推理轨迹数量、最终解数量、截断深度）在完整CoT和仅解采样之间插值。

Result: 在五个推理基准和多个模型规模上，Fractured Sampling实现了优越的准确性-成本权衡，显著提升了Pass@k与令牌预算的关系。

Conclusion: Fractured Sampling为更高效和可扩展的LLM推理提供了计算资源分配方法。

Abstract: Inference-time scaling techniques have significantly bolstered the reasoning
capabilities of large language models (LLMs) by harnessing additional
computational effort at inference without retraining. Similarly,
Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy
by generating rich intermediate reasoning trajectories, but these approaches
incur substantial token costs that impede their deployment in latency-sensitive
settings. In this work, we first show that truncated CoT, which stops reasoning
before completion and directly generates the final answer, often matches full
CoT sampling while using dramatically fewer tokens. Building on this insight,
we introduce Fractured Sampling, a unified inference-time strategy that
interpolates between full CoT and solution-only sampling along three orthogonal
axes: (1) the number of reasoning trajectories, (2) the number of final
solutions per trajectory, and (3) the depth at which reasoning traces are
truncated. Through extensive experiments on five diverse reasoning benchmarks
and several model scales, we demonstrate that Fractured Sampling consistently
achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling
gains in Pass@k versus token budget. Our analysis reveals how to allocate
computation across these dimensions to maximize performance, paving the way for
more efficient and scalable LLM reasoning.

</details>

### [275] [Generative Modeling of Random Fields from Limited Data via Constrained Latent Flow Matching](https://arxiv.org/abs/2505.13007)
*James E. Warner,Tristan A. Shah,Patrick E. Leser,Geoffrey F. Bomarito,Joshua D. Pribe,Michael C. Stanley*

Main category: cs.LG

TLDR: 提出一种结合领域知识的生成模型框架，用于数据有限情况下的随机场建模，通过潜在流匹配和变分自编码器（VAE）实现高效生成。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型依赖大量高质量数据，限制了其应用范围。本文旨在通过结合领域知识，解决数据稀疏、间接或有限的问题。

Method: 采用潜在流匹配方法，在预训练VAE的潜在空间中进行生成建模，并整合物理/统计约束到VAE训练中。

Result: 在风速场重建和材料属性推断任务中，该方法显著优于无约束方法，且在小数据集上表现优异。

Conclusion: 该框架在数据有限情况下仍能生成满足领域约束的随机场样本，扩展了生成模型的应用范围。

Abstract: Deep generative models are promising tools for science and engineering, but
their reliance on abundant, high-quality data limits applicability. We present
a novel framework for generative modeling of random fields (probability
distributions over continuous functions) that incorporates domain knowledge to
supplement limited, sparse, and indirect data. The foundation of the approach
is latent flow matching, where generative modeling occurs on compressed
function representations in the latent space of a pre-trained variational
autoencoder (VAE). Innovations include the adoption of a function decoder
within the VAE and integration of physical/statistical constraints into the VAE
training process. In this way, a latent function representation is learned that
yields continuous random field samples satisfying domain-specific constraints
when decoded, even in data-limited regimes. Efficacy is demonstrated on two
challenging applications: wind velocity field reconstruction from sparse
sensors and material property inference from a limited number of indirect
measurements. Results show that the proposed framework achieves significant
improvements in reconstruction accuracy compared to unconstrained methods and
enables effective inference with relatively small training datasets that is
intractable without constraints.

</details>

### [276] [LiBOG: Lifelong Learning for Black-Box Optimizer Generation](https://arxiv.org/abs/2505.13025)
*Jiyuan Pei,Yi Mei,Jialin Liu,Mengjie Zhang*

Main category: cs.LG

TLDR: Meta-Black-Box Optimization (MetaBBO) 自动化优化器配置，但现有方法假设问题分布静态且训练样本充足。LiBOG 提出终身学习范式，解决动态分布问题并避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现实问题分布动态变化，现有 MetaBBO 方法依赖静态假设，需支持持续学习。

Method: 提出 LiBOG，通过跨任务和任务内知识整合，实现终身学习。

Result: 实验证明 LiBOG 能高效生成高性能优化器，避免遗忘并保持学习新任务能力。

Conclusion: LiBOG 为 MetaBBO 提供终身学习解决方案，适应动态问题分布。

Abstract: Meta-Black-Box Optimization (MetaBBO) garners attention due to its success in
automating the configuration and generation of black-box optimizers,
significantly reducing the human effort required for optimizer design and
discovering optimizers with higher performance than classic human-designed
optimizers. However, existing MetaBBO methods conduct one-off training under
the assumption that a stationary problem distribution with extensive and
representative training problem samples is pre-available. This assumption is
often impractical in real-world scenarios, where diverse problems following
shifting distribution continually arise. Consequently, there is a pressing need
for methods that can continuously learn from new problems encountered
on-the-fly and progressively enhance their capabilities. In this work, we
explore a novel paradigm of lifelong learning in MetaBBO and introduce LiBOG, a
novel approach designed to learn from sequentially encountered problems and
generate high-performance optimizers for Black-Box Optimization (BBO). LiBOG
consolidates knowledge both across tasks and within tasks to mitigate
catastrophic forgetting. Extensive experiments demonstrate LiBOG's
effectiveness in learning to generate high-performance optimizers in a lifelong
learning manner, addressing catastrophic forgetting while maintaining
plasticity to learn new tasks.

</details>

### [277] [Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs](https://arxiv.org/abs/2505.13026)
*Jack Chen,Fazhong Liu,Naruto Liu,Yuhan Luo,Erqu Qin,Harry Zheng,Tian Dong,Haojin Zhu,Yan Meng,Xiao Wang*

Main category: cs.LG

TLDR: SASR是一种动态混合训练框架，结合SFT和RL，通过自适应调整算法提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前SFT和RL单独使用时存在过拟合和模式崩溃问题，静态混合方法泛化能力差。

Method: SASR采用SFT初始预热，结合动态调整算法（基于梯度范数和分布差异）与GRPO在线RL方法。

Result: 实验表明SASR优于SFT、RL及静态混合方法。

Conclusion: SASR通过动态平衡SFT和RL，有效提升模型推理能力。

Abstract: Large language models (LLMs) excel at mathematical reasoning and logical
problem-solving. The current popular training paradigms primarily use
supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the
models' reasoning abilities. However, when using SFT or RL alone, there are
respective challenges: SFT may suffer from overfitting, while RL is prone to
mode collapse. The state-of-the-art methods have proposed hybrid training
schemes. However, static switching faces challenges such as poor generalization
across different tasks and high dependence on data quality. In response to
these challenges, inspired by the curriculum learning-quiz mechanism in human
reasoning cultivation, We propose SASR, a step-wise adaptive hybrid training
framework that theoretically unifies SFT and RL and dynamically balances the
two throughout optimization. SASR uses SFT for initial warm-up to establish
basic reasoning skills, and then uses an adaptive dynamic adjustment algorithm
based on gradient norm and divergence relative to the original distribution to
seamlessly integrate SFT with the online RL method GRPO. By monitoring the
training status of LLMs and adjusting the training process in sequence, SASR
ensures a smooth transition between training schemes, maintaining core
reasoning abilities while exploring different paths. Experimental results
demonstrate that SASR outperforms SFT, RL, and static hybrid training methods.

</details>

### [278] [Unpacking Positional Encoding in Transformers: A Spectral Analysis of Content-Position Coupling](https://arxiv.org/abs/2505.13027)
*Zihan Gu,Han Zhang,Ruoyu Chen,Yue Hu,Hua Zhang*

Main category: cs.LG

TLDR: 论文通过谱分析研究位置编码（PE）机制，发现RoPE通过Hadamard积引入谱收缩，优化稳定性和效率。实验验证RoPE在位置敏感任务中表现最佳，并提出改进PE耦合方式的新思路。


<details>
  <summary>Details</summary>
Motivation: 探索不同PE方案如何耦合内容和位置信息及其对模型动态的影响，填补理论空白。

Method: 提出统一框架，基于Toeplitz矩阵的谱性质分析PE，设计合成任务对比不同PE方法。

Result: RoPE在位置敏感任务中表现最优，且能诱导早期层的局部位置处理模式。改进PE耦合方式（如MLA）可缓解集中问题。

Conclusion: 有效PE设计的关键是内容相对混合与相对位置Toeplitz信号的结合，为Transformer架构中位置结构整合提供新见解。

Abstract: Positional encoding (PE) is essential for enabling Transformers to model
sequential structure. However, the mechanisms by which different PE schemes
couple token content and positional information-and how these mechanisms
influence model dynamics-remain theoretically underexplored. In this work, we
present a unified framework that analyzes PE through the spectral properties of
Toeplitz and related matrices derived from attention logits. We show that
multiplicative content-position coupling-exemplified by Rotary Positional
Encoding (RoPE) via a Hadamard product with a Toeplitz matrix-induces spectral
contraction, which theoretically improves optimization stability and
efficiency. Guided by this theory, we construct synthetic tasks that contrast
content-position dependent and content-position independent settings, and
evaluate a range of PE methods. Our experiments reveal strong alignment with
theory: RoPE consistently outperforms other methods on position-sensitive tasks
and induces "single-head deposit" patterns in early layers, indicating
localized positional processing. Further analyses show that modifying the
method and timing of PE coupling, such as MLA in Deepseek-V3, can effectively
mitigate this concentration. These results establish explicit content-relative
mixing with relative-position Toeplitz signals as a key principle for effective
PE design and provide new insight into how positional structure is integrated
in Transformer architectures.

</details>

### [279] [TSPulse: Dual Space Tiny Pre-Trained Models for Rapid Time-Series Analysis](https://arxiv.org/abs/2505.13033)
*Vijay Ekambaram,Subodh Kumar,Arindam Jati,Sumanta Mukherjee,Tomoya Sakai,Pankaj Dayama,Wesley M. Gifford,Jayant Kalagnanam*

Main category: cs.LG

TLDR: TSPulse是一种超紧凑的时间序列预训练模型，仅需1M参数，在分类、异常检测、填补和检索任务中表现优异。通过双空间掩码重建和双嵌入解耦等创新，显著提升了性能，同时保持了高效性。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预训练模型通常规模庞大，计算资源需求高。TSPulse旨在提供一种高效且性能优越的替代方案。

Method: 采用双空间掩码重建（时间和频率域）、双嵌入解耦、TSLens微调组件、多头三角测量技术和混合掩码预训练。

Result: 在UEA分类基准上提升5-16%，TSB-AD异常检测榜单上提升20%，零样本填补提升50%，时间序列检索提升25%。

Conclusion: TSPulse以极小的参数量实现了高性能，为高效时间序列预训练模型树立了新标准。

Abstract: The rise of time-series pre-trained models has advanced temporal
representation learning, but current state-of-the-art models are often
large-scale, requiring substantial compute. We introduce TSPulse, ultra-compact
time-series pre-trained models with only 1M parameters, specialized to perform
strongly across classification, anomaly detection, imputation, and retrieval
tasks. TSPulse introduces innovations at both the architecture and task levels.
At the architecture level, it employs a dual-space masked reconstruction,
learning from both time and frequency domains to capture complementary signals.
This is further enhanced by a dual-embedding disentanglement, generating both
detailed embeddings for fine-grained analysis and high-level semantic
embeddings for broader task understanding. Notably, TSPulse's semantic
embeddings are robust to shifts in time, magnitude, and noise, which is
important for robust retrieval. At the task level, TSPulse incorporates TSLens,
a fine-tuning component enabling task-specific feature attention. It also
introduces a multi-head triangulation technique that correlates deviations from
multiple prediction heads, enhancing anomaly detection by fusing complementary
model outputs. Additionally, a hybrid mask pretraining is proposed to improves
zero-shot imputation by reducing pre-training bias. These architecture and task
innovations collectively contribute to TSPulse's significant performance gains:
5-16% on the UEA classification benchmarks, +20% on the TSB-AD anomaly
detection leaderboard, +50% in zero-shot imputation, and +25% in time-series
retrieval. Remarkably, these results are achieved with just 1M parameters,
making TSPulse 10-100X smaller than existing pre-trained models. Its efficiency
enables GPU-free inference and rapid pre-training, setting a new standard for
efficient time-series pre-trained models. Models will be open-sourced soon.

</details>

### [280] [PPTNet: A Hybrid Periodic Pattern-Transformer Architecture for Traffic Flow Prediction and Congestion Identification](https://arxiv.org/abs/2505.13047)
*Hongrui Kou,Jingkai Li,Ziyu Wang,Zhouhang Lv,Yuxin Zhang,Cheng Wang*

Main category: cs.LG

TLDR: 本文提出了一种结合周期性模式提取与Transformer架构的PPTNet模型，用于交通流量预测和实时拥堵识别，并在中国拥堵高速公路场景下验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统的高效运行依赖于交通流量参数的准确预测和实时拥堵状态的识别。

Method: 构建了适用于中国拥堵高速公路场景的数据集TF4CHE，提出PPTNet模型，结合FFT提取多尺度周期性模式、Inception卷积提取特征，以及Transformer解码器建模时间依赖，最后通过模糊推理模块实时识别拥堵。

Result: PPTNet在预测精度上显著优于主流方法，拥堵识别模块能有效实时识别道路拥堵状态。

Conclusion: PPTNet在真实交通场景中表现出优越性和实用性。

Abstract: Accurate prediction of traffic flow parameters and real time identification
of congestion states are essential for the efficient operation of intelligent
transportation systems. This paper proposes a Periodic Pattern Transformer
Network (PPTNet) for traffic flow prediction, integrating periodic pattern
extraction with the Transformer architecture, coupled with a fuzzy inference
method for real-time congestion identification. Firstly, a high-precision
traffic flow dataset (Traffic Flow Dataset for China's Congested Highways and
Expressways, TF4CHE) suitable for congested highway scenarios in China is
constructed based on drone aerial imagery data. Subsequently, the proposed
PPTNet employs Fast Fourier Transform to capture multi-scale periodic patterns
and utilizes two-dimensional Inception convolutions to efficiently extract
intra and inter periodic features. A Transformer decoder dynamically models
temporal dependencies, enabling accurate predictions of traffic density and
speed. Finally, congestion probabilities are calculated in real-time using the
predicted outcomes via a Mamdani fuzzy inference-based congestion
identification module. Experimental results demonstrate that the proposed
PPTNet significantly outperforms mainstream traffic prediction methods in
prediction accuracy, and the congestion identification module effectively
identifies real-time road congestion states, verifying the superiority and
practicality of the proposed method in real-world traffic scenarios. Project
page: https://github.com/ADSafetyJointLab/PPTNet.

</details>

### [281] [A Path to Universal Neural Cellular Automata](https://arxiv.org/abs/2505.13058)
*Gabriel Béna,Maxence Faldor,Dan F. M. Goodman,Antoine Cully*

Main category: cs.LG

TLDR: 该论文探讨了神经细胞自动机在连续域中实现通用计算的潜力，通过梯度下降训练开发了一种连续通用细胞自动机模型。


<details>
  <summary>Details</summary>
Motivation: 研究连续域细胞自动机是否保留通用计算能力，并探索神经细胞自动机通过梯度下降学习规则的可能性。

Method: 提出了一种细胞自动机模型、目标函数和训练策略，用于在连续环境中引导神经细胞自动机实现通用计算。

Result: 实验成功训练出基础计算原语（如矩阵乘法和转置），并在细胞自动机状态中模拟了MNIST数字分类任务的神经网络。

Conclusion: 该研究为实现模拟通用计算机奠定了基础，对理解连续动力学中的通用计算和通过机器学习自动发现复杂细胞自动机行为具有重要意义。

Abstract: Cellular automata have long been celebrated for their ability to generate
complex behaviors from simple, local rules, with well-known discrete models
like Conway's Game of Life proven capable of universal computation. Recent
advancements have extended cellular automata into continuous domains, raising
the question of whether these systems retain the capacity for universal
computation. In parallel, neural cellular automata have emerged as a powerful
paradigm where rules are learned via gradient descent rather than manually
designed. This work explores the potential of neural cellular automata to
develop a continuous Universal Cellular Automaton through training by gradient
descent. We introduce a cellular automaton model, objective functions and
training strategies to guide neural cellular automata toward universal
computation in a continuous setting. Our experiments demonstrate the successful
training of fundamental computational primitives - such as matrix
multiplication and transposition - culminating in the emulation of a neural
network solving the MNIST digit classification task directly within the
cellular automata state. These results represent a foundational step toward
realizing analog general-purpose computers, with implications for understanding
universal computation in continuous dynamics and advancing the automated
discovery of complex cellular automata behaviors via machine learning.

</details>

### [282] [Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs](https://arxiv.org/abs/2505.13060)
*Shmulik Markovich-Golan,Daniel Ohayon,Itay Niv,Yair Hanani*

Main category: cs.LG

TLDR: 该论文提出了一种自动选择混合精度配置的方法，通过泰勒展开的敏感性度量预测量化误差，并结合硬件感知的时间增益模型，优化推理性能。


<details>
  <summary>Details</summary>
Motivation: 量化虽能压缩神经网络模型，但过度降低精度会损害准确性。混合精度通过动态调整各层精度来平衡这一矛盾，但如何自动选择最优配置仍具挑战。

Method: 提出基于泰勒展开的敏感性度量，高效计算各层量化误差；结合硬件感知的时间增益模型，将问题转化为整数规划问题，优化配置。

Result: 在Intel Gaudi 2加速器上验证了方法的有效性，适用于多种大型语言模型。

Conclusion: 该方法能自动选择最优混合精度配置，显著提升推理效率，同时保持模型准确性。

Abstract: Quantization is essential for Neural Network (NN) compression, reducing model
size and computational demands by using lower bit-width data types, though
aggressive reduction often hampers accuracy. Mixed Precision (MP) mitigates
this tradeoff by varying the numerical precision across network layers. This
study focuses on automatically selecting an optimal MP configuration within
Post-Training Quantization (PTQ) for inference. The first key contribution is a
novel sensitivity metric derived from a first-order Taylor series expansion of
the loss function as a function of quantization errors in weights and
activations. This metric, based on the Mean Square Error (MSE) of the loss, is
efficiently calculated per layer using high-precision forward and backward
passes over a small calibration dataset. The metric is additive across layers,
with low calibration memory overhead as weight optimization is unnecessary. The
second contribution is an accurate hardware-aware method for predicting MP time
gain by modeling it as additive for sequential sub-graphs. An algorithm
partitions the model graph into sequential subgraphs, measuring time gain for
each configuration using a few samples. After calibrating per-layer sensitivity
and time gain, an Integer Programming (IP) problem is formulated to maximize
time gain while keeping loss MSE below a set threshold. Memory gain and
theoretical time gain based on Multiply and Accumulate (MAC) operations are
also considered. Rigorous experiments on the Intel Gaudi 2 accelerator validate
the approach on several Large Language Models (LLMs).

</details>

### [283] [OmniFC: Rethinking Federated Clustering via Lossless and Secure Distance Reconstruction](https://arxiv.org/abs/2505.13071)
*Jie Yan,Xin Liu,Zhong-Yuan Zhang*

Main category: cs.LG

TLDR: OmniFC是一个统一且模型无关的联邦聚类框架，通过编码数据共享实现隐私保护，解决了非独立同分布数据下的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 联邦聚类（FC）在保护隐私的同时需要解决数据异构性和鲁棒性下降的挑战。现有方法依赖模型特定的本地代理，限制了鲁棒性和通用性。

Method: 提出OmniFC框架，利用拉格朗日编码计算，客户端仅共享编码数据，精确重构全局距离矩阵，无需泄露隐私信息。

Result: 理论分析和实验证明OmniFC在鲁棒性、有效性和通用性上优于现有方法。

Conclusion: OmniFC为联邦聚类提供了一种统一且隐私安全的解决方案，适用于多种集中式聚类方法。

Abstract: Federated clustering (FC) aims to discover global cluster structures across
decentralized clients without sharing raw data, making privacy preservation a
fundamental requirement. There are two critical challenges: (1) privacy leakage
during collaboration, and (2) robustness degradation due to aggregation of
proxy information from non-independent and identically distributed (Non-IID)
local data, leading to inaccurate or inconsistent global clustering. Existing
solutions typically rely on model-specific local proxies, which are sensitive
to data heterogeneity and inherit inductive biases from their centralized
counterparts, thus limiting robustness and generality. We propose Omni
Federated Clustering (OmniFC), a unified and model-agnostic framework.
Leveraging Lagrange coded computing, our method enables clients to share only
encoded data, allowing exact reconstruction of the global distance matrix--a
fundamental representation of sample relationships--without leaking private
information, even under client collusion. This construction is naturally
resilient to Non-IID data distributions. This approach decouples FC from
model-specific proxies, providing a unified extension mechanism applicable to
diverse centralized clustering methods. Theoretical analysis confirms both
reconstruction fidelity and privacy guarantees, while comprehensive experiments
demonstrate OmniFC's superior robustness, effectiveness, and generality across
various benchmarks compared to state-of-the-art methods. Code will be released.

</details>

### [284] [Orthogonal Survival Learners for Estimating Heterogeneous Treatment Effects from Time-to-Event Data](https://arxiv.org/abs/2505.13072)
*Dennis Frauen,Maresa Schröder,Konstantin Hess,Stefan Feuerriegel*

Main category: cs.LG

TLDR: 提出了一种新的正交生存学习工具箱，用于在生存分析中估计异质性处理效应（HTEs），具有正交性、自定义权重函数和模型无关性等优势。


<details>
  <summary>Details</summary>
Motivation: 在生存分析中估计HTEs具有挑战性，尤其是存在时间到事件数据和删失结果时。

Method: 开发了正交生存学习工具箱，支持自定义权重函数，并与多种机器学习模型兼容。

Result: 通过数值实验验证了学习器在不同低重叠情况下的有效性。

Conclusion: 为实践者提供了一个适用于随机和观察性研究的工具箱，支持删失时间到事件数据的HTE估计。

Abstract: Estimating heterogeneous treatment effects (HTEs) is crucial for personalized
decision-making. However, this task is challenging in survival analysis, which
includes time-to-event data with censored outcomes (e.g., due to study
dropout). In this paper, we propose a toolbox of novel orthogonal survival
learners to estimate HTEs from time-to-event data under censoring. Our learners
have three main advantages: (i) we show that learners from our toolbox are
guaranteed to be orthogonal and thus come with favorable theoretical
properties; (ii) our toolbox allows for incorporating a custom weighting
function, which can lead to robustness against different types of low overlap,
and (iii) our learners are model-agnostic (i.e., they can be combined with
arbitrary machine learning models). We instantiate the learners from our
toolbox using several weighting functions and, as a result, propose various
neural orthogonal survival learners. Some of these coincide with existing
survival learners (including survival versions of the DR- and R-learner), while
others are novel and further robust w.r.t. low overlap regimes specific to the
survival setting (i.e., survival overlap and censoring overlap). We then
empirically verify the effectiveness of our learners for HTE estimation in
different low-overlap regimes through numerical experiments. In sum, we provide
practitioners with a large toolbox of learners that can be used for randomized
and observational studies with censored time-to-event data.

</details>

### [285] [Walking the Tightrope: Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning](https://arxiv.org/abs/2505.13081)
*Xiaoyu Yang,Jie Lu,En Yu*

Main category: cs.LG

TLDR: 论文揭示了多模态大语言模型（MLLMs）中链式思维（CoT）推理在非平稳强化微调（RFT）过程中的有害概念漂移现象，并提出了一种基于反事实感知的RFT方法（CPO）来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 研究发现CoT推理中的推理标记分布会因非平稳RFT而不可预测地演变，导致最终预测的显著偏差，亟需理论和方法上的创新来解决这一问题。

Method: 通过将CoT的自回归标记流形式化为经历任意时间偏移的非平稳分布，提出了一种基于概念图的反事实感知RFT方法（CPO），通过生成反事实推理轨迹来分离有益分布适应和有害概念漂移。

Result: 实验表明，CPO在非平稳环境中表现出卓越的鲁棒性、泛化性和协调性，并贡献了一个大规模数据集CXR-CounterFact（CCF）。

Conclusion: 论文通过理论框架和方法创新解决了MLLMs中CoT推理的概念漂移问题，为医疗领域等非平稳环境中的RFT提供了稳定解决方案。

Abstract: This paper uncovers a critical yet overlooked phenomenon in multi-modal large
language models (MLLMs): detrimental concept drift within chain-of-thought
(CoT) reasoning during non-stationary reinforcement fine-tuning (RFT), where
reasoning token distributions evolve unpredictably, thereby introducing
significant biases in final predictions. To address this, we are pioneers in
establishing the theoretical bridge between concept drift theory and RFT
processes by formalizing CoT's autoregressive token streams as non-stationary
distributions undergoing arbitrary temporal shifts. Leveraging this framework,
we propose a novel counterfact-aware RFT that systematically decouples
beneficial distribution adaptation from harmful concept drift through concept
graph-empowered LLM experts generating counterfactual reasoning trajectories.
Our solution, Counterfactual Preference Optimization (CPO), enables stable RFT
in non-stationary environments, particularly within the medical domain, through
custom-tuning of counterfactual-aware preference alignment. Extensive
experiments demonstrate our superior performance of robustness, generalization
and coordination within RFT. Besides, we also contributed a large-scale dataset
CXR-CounterFact (CCF), comprising 320,416 meticulously curated counterfactual
reasoning trajectories derived from MIMIC-CXR. Our code and data are public.

</details>

### [286] [Graph Alignment for Benchmarking Graph Neural Networks and Learning Positional Encodings](https://arxiv.org/abs/2505.13087)
*Adrien Lagesse,Marc Lelarge*

Main category: cs.LG

TLDR: 提出了一种基于图对齐问题的图神经网络（GNN）基准测试方法，用于评估不同架构的性能，并展示了其在无监督预训练中的有效性。


<details>
  <summary>Details</summary>
Motivation: 图对齐问题可以推广图同构问题，为GNN提供一种自监督学习任务，用于生成多样化的测试数据集。

Method: 通过合成随机图和真实图数据集生成不同难度的图对齐任务，评估各GNN架构性能。

Result: 各向异性GNN优于标准卷积架构，且在无监督预训练中表现优异，在分子回归任务中达到SOTA。

Conclusion: 图对齐任务为GNN评估和预训练提供了有效工具，开源工具支持进一步研究。

Abstract: We propose a novel benchmarking methodology for graph neural networks (GNNs)
based on the graph alignment problem, a combinatorial optimization task that
generalizes graph isomorphism by aligning two unlabeled graphs to maximize
overlapping edges. We frame this problem as a self-supervised learning task and
present several methods to generate graph alignment datasets using synthetic
random graphs and real-world graph datasets from multiple domains. For a given
graph dataset, we generate a family of graph alignment datasets with increasing
difficulty, allowing us to rank the performance of various architectures. Our
experiments indicate that anisotropic graph neural networks outperform standard
convolutional architectures. To further demonstrate the utility of the graph
alignment task, we show its effectiveness for unsupervised GNN pre-training,
where the learned node embeddings outperform other positional encodings on
three molecular regression tasks and achieve state-of-the-art results on the
PCQM4Mv2 dataset with significantly fewer parameters. To support
reproducibility and further research, we provide an open-source Python package
to generate graph alignment datasets and benchmark new GNN architectures.

</details>

### [287] [Treatment Effect Estimation for Optimal Decision-Making](https://arxiv.org/abs/2505.13092)
*Dennis Frauen,Valentyn Melnychuk,Jonas Schweisthal,Stefan Feuerriegel*

Main category: cs.LG

TLDR: 论文研究了基于两阶段CATE估计器（如DR-learner）的决策性能，发现其在决策中可能表现不佳，并提出了一种新的学习目标和神经方法以优化决策性能。


<details>
  <summary>Details</summary>
Motivation: 现代CATE估计器在决策中的性能缺乏理论理解，而实践中却广泛依赖其估计结果进行决策。

Method: 提出了一种新的两阶段学习目标，平衡CATE估计误差和决策性能，并设计了一种神经方法优化该目标。

Result: 理论和实证结果均验证了新方法的有效性。

Conclusion: 首次展示了如何调整两阶段CATE估计器以实现最优决策。

Abstract: Decision-making across various fields, such as medicine, heavily relies on
conditional average treatment effects (CATEs). Practitioners commonly make
decisions by checking whether the estimated CATE is positive, even though the
decision-making performance of modern CATE estimators is poorly understood from
a theoretical perspective. In this paper, we study optimal decision-making
based on two-stage CATE estimators (e.g., DR-learner), which are considered
state-of-the-art and widely used in practice. We prove that, while such
estimators may be optimal for estimating CATE, they can be suboptimal when used
for decision-making. Intuitively, this occurs because such estimators
prioritize CATE accuracy in regions far away from the decision boundary, which
is ultimately irrelevant to decision-making. As a remedy, we propose a novel
two-stage learning objective that retargets the CATE to balance CATE estimation
error and decision performance. We then propose a neural method that optimizes
an adaptively-smoothed approximation of our learning objective. Finally, we
confirm the effectiveness of our method both empirically and theoretically. In
sum, our work is the first to show how two-stage CATE estimators can be adapted
for optimal decision-making.

</details>

### [288] [Time series saliency maps: explaining models across multiple domains](https://arxiv.org/abs/2505.13100)
*Christodoulos Kechris,Jonathan Dan,David Atienza*

Main category: cs.LG

TLDR: 论文提出了一种跨域积分梯度方法，用于时间序列模型的特征归因，解决了传统显著性图方法在时间序列中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统显著性图方法在时间序列中无法捕捉语义上有意义的特征，因为它们通常存在于其他域中。

Method: 提出了跨域积分梯度方法，支持任何可逆、可微的时间域变换，并扩展到复数域以实现基于频率的归因。

Result: 在三个实际任务中展示了可解释的、问题特定的归因效果，并发布了开源库。

Conclusion: 跨域积分梯度能够提供传统时间域显著性方法无法实现的语义上有意义的见解。

Abstract: Traditional saliency map methods, popularized in computer vision, highlight
individual points (pixels) of the input that contribute the most to the model's
output. However, in time-series they offer limited insights as semantically
meaningful features are often found in other domains. We introduce Cross-domain
Integrated Gradients, a generalization of Integrated Gradients. Our method
enables feature attributions on any domain that can be formulated as an
invertible, differentiable transformation of the time domain. Crucially, our
derivation extends the original Integrated Gradients into the complex domain,
enabling frequency-based attributions. We provide the necessary theoretical
guarantees, namely, path independence and completeness. Our approach reveals
interpretable, problem-specific attributions that time-domain methods cannot
capture, on three real-world tasks: wearable sensor heart rate extraction,
electroencephalography-based seizure detection, and zero-shot time-series
forecasting. We release an open-source Tensorflow/PyTorch library to enable
plug-and-play cross-domain explainability for time-series models. These results
demonstrate the ability of cross-domain integrated gradients to provide
semantically meaningful insights in time-series models that are impossible with
traditional time-domain saliency.

</details>

### [289] [Lightweight Transformer via Unrolling of Mixed Graph Algorithms for Traffic Forecast](https://arxiv.org/abs/2505.13102)
*Ji Qi,Tam Thuc Do,Mingxiao Liu,Zhuoshi Pan,Yuzhe Li,Gene Cheung,H. Vicky Zhao*

Main category: cs.LG

TLDR: 论文提出了一种基于混合图的轻量级Transformer-like神经网络，用于时空交通预测，通过图学习和ADMM展开实现高效参数学习。


<details>
  <summary>Details</summary>
Motivation: 解决交通预测中时空维度的复杂性问题，同时保持模型的轻量化和可解释性。

Method: 构建无向图和有向图分别捕捉空间和时间关系，设计新的变分项量化平滑性，通过ADMM展开为神经网络。

Result: 实验表明，该方法在减少参数量的同时，达到了与最先进方法竞争的性能。

Conclusion: 提出的方法在交通预测中实现了高效且轻量化的表现，具有实际应用潜力。

Abstract: To forecast traffic with both spatial and temporal dimensions, we unroll a
mixed-graph-based optimization algorithm into a lightweight and interpretable
transformer-like neural net. Specifically, we construct two graphs: an
undirected graph $\mathcal{G}^u$ capturing spatial correlations across
geography, and a directed graph $\mathcal{G}^d$ capturing sequential
relationships over time. We formulate a prediction problem for the future
samples of signal $\mathbf{x}$, assuming it is "smooth" with respect to both
$\mathcal{G}^u$ and $\mathcal{G}^d$, where we design new $\ell_2$ and
$\ell_1$-norm variational terms to quantify and promote signal smoothness
(low-frequency reconstruction) on a directed graph. We construct an iterative
algorithm based on alternating direction method of multipliers (ADMM), and
unroll it into a feed-forward network for data-driven parameter learning. We
insert graph learning modules for $\mathcal{G}^u$ and $\mathcal{G}^d$, which
are akin to the self-attention mechanism in classical transformers. Experiments
show that our unrolled networks achieve competitive traffic forecast
performance as state-of-the-art prediction schemes, while reducing parameter
counts drastically. Our code is available in
https://github.com/SingularityUndefined/Unrolling-GSP-STForecast.

</details>

### [290] [FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference](https://arxiv.org/abs/2505.13109)
*Guangda Liu,Chengwei Li,Zhenyu Ning,Jing Lin,Yiwu Yao,Danning Ke,Minyi Guo,Jieru Zhao*

Main category: cs.LG

TLDR: FreeKV是一种算法-系统协同优化框架，旨在提高KV检索效率并保持准确性，通过推测性检索和细粒度校正实现，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 长上下文窗口的部署挑战主要源于KV缓存的大小与上下文长度成比例增长，现有方法在准确性和效率上存在不足。

Method: FreeKV结合推测性检索和细粒度校正的算法优化，以及跨CPU和GPU内存的混合KV布局和双缓冲流式召回的系统优化。

Result: FreeKV在多种场景和模型中实现了近乎无损的准确性，速度比现有最优KV检索方法快13倍。

Conclusion: FreeKV通过算法与系统的协同优化，有效解决了长上下文部署中的KV缓存问题，显著提升了效率和准确性。

Abstract: Large language models (LLMs) have been widely deployed with rapidly expanding
context windows to support increasingly demanding applications. However, long
contexts pose significant deployment challenges, primarily due to the KV cache
whose size grows proportionally with context length. While KV cache compression
methods are proposed to address this issue, KV dropping methods incur
considerable accuracy loss, and KV retrieval methods suffer from significant
efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization
framework to enhance KV retrieval efficiency while preserving accuracy. On the
algorithm side, FreeKV introduces speculative retrieval to shift the KV
selection and recall processes out of the critical path, combined with
fine-grained correction to ensure accuracy. On the system side, FreeKV employs
hybrid KV layouts across CPU and GPU memory to eliminate fragmented data
transfers, and leverages double-buffered streamed recall to further improve
efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy
across various scenarios and models, delivering up to 13$\times$ speedup
compared to SOTA KV retrieval methods.

</details>

### [291] [Why Knowledge Distillation Works in Generative Models: A Minimal Working Explanation](https://arxiv.org/abs/2505.13111)
*Sungmin Cha,Kyunghyun Cho*

Main category: cs.LG

TLDR: 知识蒸馏（KD）在生成模型中通过权衡精度与召回率提升学生模型性能，尤其在样本质量优先的场景中效果显著。


<details>
  <summary>Details</summary>
Motivation: 尽管KD在生成模型中的实证效果显著，但其提升生成质量的机制尚不明确。

Method: 通过高斯混合模型的模拟实验和大规模语言模型验证，分析KD如何影响学生模型的精度与召回率。

Result: 实验表明，KD使学生模型在高似然区域集中概率质量，牺牲覆盖率以提升样本质量。

Conclusion: KD通过精度-召回率权衡提升生成模型性能，尤其在样本质量优先的任务中效果显著。

Abstract: Knowledge distillation (KD) is a core component in the training and
deployment of modern generative models, particularly large language models
(LLMs). While its empirical benefits are well documented--enabling smaller
student models to emulate the performance of much larger teachers--the
underlying mechanisms by which KD improves generative quality remain poorly
understood. In this work, we present a minimal working explanation of KD in
generative modeling. Using a controlled simulation with mixtures of Gaussians,
we demonstrate that distillation induces a trade-off between precision and
recall in the student model. As the teacher distribution becomes more
selective, the student concentrates more probability mass on high-likelihood
regions at the expense of coverage--a behavior modulated by a single
entropy-controlling parameter. We then validate this effect in a large-scale
language modeling setup using the SmolLM2 family of models. Empirical results
reveal the same precision-recall dynamics observed in simulation, where
precision corresponds to sample quality and recall to distributional coverage.
This precision-recall trade-off proves especially beneficial in scenarios where
sample quality outweighs diversity, such as instruction tuning or downstream
generation. Our analysis provides a simple and general explanation for the
effectiveness of KD in generative modeling.

</details>

### [292] [Continuous Fair SMOTE -- Fairness-Aware Stream Learning from Imbalanced Data](https://arxiv.org/abs/2505.13116)
*Kathrin Lammers,Valerie Vaquet,Barbara Hammer*

Main category: cs.LG

TLDR: 论文提出了一种名为CFSMOTE的公平感知预处理方法，旨在同时解决数据流中的类别不平衡和公平性问题。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在在线数据流中的应用增加，算法的公平性成为伦理和法律关注的重点。现有方法通常单独处理类别不平衡或优化单一公平性指标，可能导致偏差。

Method: CFSMOTE通过情境测试和公平相关组的平衡，在过采样过程中同时解决类别不平衡和公平性问题。

Result: 实验表明，CFSMOTE在多个常见群体公平性指标上显著优于传统C-SMOTE，同时保持竞争力。

Conclusion: CFSMOTE是一种有效的预处理方法，能够同时优化类别平衡和公平性，避免单一指标优化带来的潜在问题。

Abstract: As machine learning is increasingly applied in an online fashion to deal with
evolving data streams, the fairness of these algorithms is a matter of growing
ethical and legal concern. In many use cases, class imbalance in the data also
needs to be dealt with to ensure predictive performance. Current fairness-aware
stream learners typically attempt to solve these issues through in- or
post-processing by focusing on optimizing one specific discrimination metric,
addressing class imbalance in a separate processing step. While C-SMOTE is a
highly effective model-agnostic pre-processing approach to mitigate class
imbalance, as a side effect of this method, algorithmic bias is often
introduced.
  Therefore, we propose CFSMOTE - a fairness-aware, continuous SMOTE variant -
as a pre-processing approach to simultaneously address the class imbalance and
fairness concerns by employing situation testing and balancing
fairness-relevant groups during oversampling. Unlike other fairness-aware
stream learners, CFSMOTE is not optimizing for only one specific fairness
metric, therefore avoiding potentially problematic trade-offs. Our experiments
show significant improvement on several common group fairness metrics in
comparison to vanilla C-SMOTE while maintaining competitive performance, also
in comparison to other fairness-aware algorithms.

</details>

### [293] [When majority rules, minority loses: bias amplification of gradient descent](https://arxiv.org/abs/2505.13122)
*François Bachoc,Jérôme Bolte,Ryan Boustany,Jean-Michel Loubes*

Main category: cs.LG

TLDR: 论文研究了机器学习中的偏见放大问题，提出了一个理论框架，揭示了标准训练如何偏向多数群体，并提出了三个关键发现。


<details>
  <summary>Details</summary>
Motivation: 尽管有越来越多的实证证据表明机器学习中存在偏见放大，但其理论基础仍然不足。

Method: 开发了一个多数-少数学习任务的形式化框架，假设存在群体和方差不平衡，分析了标准训练的影响。

Result: 发现了三个关键现象：全数据与刻板预测器的接近性、训练模型仅学习多数特征的区域主导性，以及额外训练需求的下界。

Conclusion: 研究结果通过深度学习实验验证，为理解偏见放大提供了理论支持。

Abstract: Despite growing empirical evidence of bias amplification in machine learning,
its theoretical foundations remain poorly understood. We develop a formal
framework for majority-minority learning tasks, showing how standard training
can favor majority groups and produce stereotypical predictors that neglect
minority-specific features. Assuming population and variance imbalance, our
analysis reveals three key findings: (i) the close proximity between
``full-data'' and stereotypical predictors, (ii) the dominance of a region
where training the entire model tends to merely learn the majority traits, and
(iii) a lower bound on the additional training required. Our results are
illustrated through experiments in deep learning for tabular and image
classification tasks.

</details>

### [294] [$μ$PC: Scaling Predictive Coding to 100+ Layer Networks](https://arxiv.org/abs/2505.13124)
*Francesco Innocenti,El Mehdi Achour,Christopher L. Buckley*

Main category: cs.LG

TLDR: 论文提出了一种名为μPC的方法，通过Depth-μP参数化成功训练了100+层的预测编码网络（PCNs），解决了传统PCNs在深度网络训练中的困难。


<details>
  <summary>Details</summary>
Motivation: 反向传播（BP）在生物学上不现实，而现有的替代算法（如预测编码PC）在训练深层网络时表现不佳，因此需要一种更有效的方法。

Method: 采用Depth-μP参数化（称为μPC）来稳定训练深层PCNs，并分析了PCNs的扩展行为。

Result: μPC能够稳定训练128层残差网络，性能与现有基准相当，且支持零样本迁移学习率。

Conclusion: μPC为局部算法提供了新的可能性，并可扩展到卷积和Transformer架构。

Abstract: The biological implausibility of backpropagation (BP) has motivated many
alternative, brain-inspired algorithms that attempt to rely only on local
information, such as predictive coding (PC) and equilibrium propagation.
However, these algorithms have notoriously struggled to train very deep
networks, preventing them from competing with BP in large-scale settings.
Indeed, scaling PC networks (PCNs) has recently been posed as a challenge for
the community (Pinchetti et al., 2024). Here, we show that 100+ layer PCNs can
be trained reliably using a Depth-$\mu$P parameterisation (Yang et al., 2023;
Bordelon et al., 2023) which we call "$\mu$PC". Through an extensive analysis
of the scaling behaviour of PCNs, we reveal several pathologies that make
standard PCNs difficult to train at large depths. We then show that, despite
addressing only some of these instabilities, $\mu$PC allows stable training of
very deep (up to 128-layer) residual networks on simple classification tasks
with competitive performance and little tuning compared to current benchmarks.
Moreover, $\mu$PC enables zero-shot transfer of both weight and activity
learning rates across widths and depths. Our results have implications for
other local algorithms and could be extended to convolutional and transformer
architectures. Code for $\mu$PC is made available as part of a JAX library for
PCNs at https://github.com/thebuckleylab/jpc (Innocenti et al., 2024).

</details>

### [295] [Neurosymbolic Diffusion Models](https://arxiv.org/abs/2505.13138)
*Emile van Krieken,Pasquale Minervini,Edoardo Ponti,Antonio Vergari*

Main category: cs.LG

TLDR: NeSyDMs通过离散扩散模型解决神经符号预测器中符号独立性假设的局限性，提升建模依赖关系和不确定性量化，在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 标准神经符号预测器假设符号条件独立，限制了建模交互和不确定性的能力，导致预测过度自信和泛化能力差。

Method: 提出神经符号扩散模型（NeSyDMs），利用离散扩散逐步建模符号依赖关系，同时保留独立性假设以实现可扩展学习。

Result: 在合成和真实任务（如视觉路径规划和自动驾驶）中，NeSyDMs在神经符号预测器中达到最优准确率，并展现强校准性。

Conclusion: NeSyDMs通过离散扩散有效解决了符号依赖性和不确定性量化问题，显著提升了神经符号预测器的性能。

Abstract: Neurosymbolic (NeSy) predictors combine neural perception with symbolic
reasoning to solve tasks like visual reasoning. However, standard NeSy
predictors assume conditional independence between the symbols they extract,
thus limiting their ability to model interactions and uncertainty - often
leading to overconfident predictions and poor out-of-distribution
generalisation. To overcome the limitations of the independence assumption, we
introduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy
predictors that use discrete diffusion to model dependencies between symbols.
Our approach reuses the independence assumption from NeSy predictors at each
step of the diffusion process, enabling scalable learning while capturing
symbol dependencies and uncertainty quantification. Across both synthetic and
real-world benchmarks - including high-dimensional visual path planning and
rule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among
NeSy predictors and demonstrate strong calibration.

</details>

### [296] [Parallel Layer Normalization for Universal Approximation](https://arxiv.org/abs/2505.13142)
*Yunhao Ni,Yuhe Liu,Wenxin Sun,Yitong Tang,Yuxin Guo,Peilin Feng,Wenjun Wu,Lei Huang*

Main category: cs.LG

TLDR: 本文首次研究了带有归一化层的深度神经网络的通用逼近定理（UAT），证明了仅由并行层归一化（PLN）和线性层组成的无限宽网络具有通用逼近能力，并探讨了PLN在理论和实际中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统UAT分析忽略了现代网络中常用的归一化层，本文旨在填补这一空白，研究带有归一化层的DNN的逼近能力。

Method: 通过理论证明，分析了仅含PLN和线性层的无限宽网络的逼近能力，并比较了PLN与传统激活函数的性能。

Result: 证明了PLN具有通用逼近能力，且能同时作为激活函数和归一化层使用，在Transformer架构中表现优于传统LN。

Conclusion: PLN在理论和实际应用中均表现出潜力，为神经网络设计提供了新的可能性。

Abstract: Universal approximation theorem (UAT) is a fundamental theory for deep neural
networks (DNNs), demonstrating their powerful representation capacity to
represent and approximate any function. The analyses and proofs of UAT are
based on traditional network with only linear and nonlinear activation
functions, but omitting normalization layers, which are commonly employed to
enhance the training of modern networks. This paper conducts research on UAT of
DNNs with normalization layers for the first time. We theoretically prove that
an infinitely wide network -- composed solely of parallel layer normalization
(PLN) and linear layers -- has universal approximation capacity. Additionally,
we investigate the minimum number of neurons required to approximate
$L$-Lipchitz continuous functions, with a single hidden-layer network. We
compare the approximation capacity of PLN with traditional activation functions
in theory. Different from the traditional activation functions, we identify
that PLN can act as both activation function and normalization in deep neural
networks at the same time. We also find that PLN can improve the performance
when replacing LN in transformer architectures, which reveals the potential of
PLN used in neural architectures.

</details>

### [297] [Temporal Distance-aware Transition Augmentation for Offline Model-based Reinforcement Learning](https://arxiv.org/abs/2505.13144)
*Dongsu Lee,Minhae Kwon*

Main category: cs.LG

TLDR: TempDATA是一种新的离线模型强化学习框架，通过在潜在空间中生成时间结构化的增强转换，解决了稀疏奖励和长时任务中的性能问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在处理固定数据集时，常因分布外样本导致性能下降。现有离线MBRL方法在稀疏奖励和长时任务中表现不佳。

Method: TempDATA在潜在空间中生成时间结构化的增强转换，通过学习捕捉状态空间轨迹和转换级别的时间距离的潜在抽象。

Result: 实验表明，TempDATA在多个基准任务中优于现有离线MBRL方法，并与扩散增强和目标条件RL方法性能相当或更好。

Conclusion: TempDATA通过潜在空间的时间结构化增强，显著提升了离线MBRL在复杂任务中的性能。

Abstract: The goal of offline reinforcement learning (RL) is to extract a
high-performance policy from the fixed datasets, minimizing performance
degradation due to out-of-distribution (OOD) samples. Offline model-based RL
(MBRL) is a promising approach that ameliorates OOD issues by enriching
state-action transitions with augmentations synthesized via a learned dynamics
model. Unfortunately, seminal offline MBRL methods often struggle in
sparse-reward, long-horizon tasks. In this work, we introduce a novel MBRL
framework, dubbed Temporal Distance-Aware Transition Augmentation (TempDATA),
that generates augmented transitions in a temporally structured latent space
rather than in raw state space. To model long-horizon behavior, TempDATA learns
a latent abstraction that captures a temporal distance from both trajectory and
transition levels of state space. Our experiments confirm that TempDATA
outperforms previous offline MBRL methods and achieves matching or surpassing
the performance of diffusion-based trajectory augmentation and goal-conditioned
RL on the D4RL AntMaze, FrankaKitchen, CALVIN, and pixel-based FrankaKitchen.

</details>

### [298] [Zero-Shot Adaptation of Behavioral Foundation Models to Unseen Dynamics](https://arxiv.org/abs/2505.13150)
*Maksim Bobrin,Ilya Zisman,Alexander Nikulin,Vladislav Kurenkov,Dmitry Dylov*

Main category: cs.LG

TLDR: 论文提出了一种改进的Forward-Backward（FB）表示方法，通过引入基于transformer的信念估计器和动态特定聚类策略，解决了行为基础模型（BFMs）在动态变化环境中的适应性问题，显著提升了零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有BFMs在动态变化环境中表现不佳，无法适应部分可观测性或转移函数变化，限制了其在现实场景（如机器人）中的应用。

Method: 提出了一种改进的FB模型，结合transformer信念估计器和动态特定聚类策略，以增强对动态变化的适应能力。

Result: 在动态变化环境中，该方法在离散和连续任务中的零样本回报比基线方法提高了2倍。

Conclusion: 改进的FB模型通过动态适应和聚类策略，显著提升了BFMs在动态环境中的零样本性能，具有实际应用潜力。

Abstract: Behavioral Foundation Models (BFMs) proved successful in producing policies
for arbitrary tasks in a zero-shot manner, requiring no test-time training or
task-specific fine-tuning. Among the most promising BFMs are the ones that
estimate the successor measure learned in an unsupervised way from
task-agnostic offline data. However, these methods fail to react to changes in
the dynamics, making them inefficient under partial observability or when the
transition function changes. This hinders the applicability of BFMs in a
real-world setting, e.g., in robotics, where the dynamics can unexpectedly
change at test time. In this work, we demonstrate that Forward-Backward (FB)
representation, one of the methods from the BFM family, cannot distinguish
between distinct dynamics, leading to an interference among the latent
directions, which parametrize different policies. To address this, we propose a
FB model with a transformer-based belief estimator, which greatly facilitates
zero-shot adaptation. We also show that partitioning the policy encoding space
into dynamics-specific clusters, aligned with the context-embedding directions,
yields additional gain in performance. These traits allow our method to respond
to the dynamics observed during training and to generalize to unseen ones.
Empirically, in the changing dynamics setting, our approach achieves up to a 2x
higher zero-shot returns compared to the baselines for both discrete and
continuous tasks.

</details>

### [299] [RIFLES: Resource-effIcient Federated LEarning via Scheduling](https://arxiv.org/abs/2505.13169)
*Sara Alosaime,Arshad Jhumka*

Main category: cs.LG

TLDR: 论文提出RIFLES方法，通过预测客户端可用性优化联邦学习中的客户端选择，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习客户端选择策略短视，导致效率低下，需改进。

Method: 将客户端选择问题转化为调度问题，利用CNN-LSTM模型预测客户端可用性，提出自适应选择策略。

Result: RIFLES在准确率和测试损失等指标上提升10%-50%。

Conclusion: RIFLES首次将联邦学习视为调度问题，显著优化了客户端选择效率。

Abstract: Federated Learning (FL) is a privacy-preserving machine learning technique
that allows decentralized collaborative model training across a set of
distributed clients, by avoiding raw data exchange. A fundamental component of
FL is the selection of a subset of clients in each round for model training by
a central server. Current selection strategies are myopic in nature in that
they are based on past or current interactions, often leading to inefficiency
issues such as straggling clients. In this paper, we address this serious
shortcoming by proposing the RIFLES approach that builds a novel availability
forecasting layer to support the client selection process. We make the
following contributions: (i) we formalise the sequential selection problem and
reduce it to a scheduling problem and show that the problem is NP-complete,
(ii) leveraging heartbeat messages from clients, RIFLES build an availability
prediction layer to support (long term) selection decisions, (iii) we propose a
novel adaptive selection strategy to support efficient learning and resource
usage. To circumvent the inherent exponential complexity, we present RIFLES, a
heuristic that leverages clients' historical availability data by using a
CNN-LSTM time series forecasting model, allowing the server to predict the
optimal participation times of clients, thereby enabling informed selection
decisions. By comparing against other FL techniques, we show that RIFLES
provide significant improvement by between 10%-50% on a variety of metrics such
as accuracy and test loss. To the best of our knowledge, it is the first work
to investigate FL as a scheduling problem.

</details>

### [300] [When a Reinforcement Learning Agent Encounters Unknown Unknowns](https://arxiv.org/abs/2505.13188)
*Juntian Zhu,Miguel de Carvalho,Zhouwang Yang,Fengxiang He*

Main category: cs.LG

TLDR: 论文提出了一种基于强化学习的模型（EMDP-GA）来处理AI代理在未知未知状态下的问题，采用非信息价值扩展（NIVE）方法扩展值函数，并证明其性能与现有技术相当。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理在从未意识到的未知未知状态下的行为问题，提出了一种数学框架来解决这一问题。

Method: 提出了EMDP-GA模型，采用NIVE方法扩展值函数，并使用上置信界动量Q学习进行训练。

Result: 证明了方法的遗憾与现有技术一致，计算和空间复杂度也相当。

Conclusion: 未知未知状态可以被渐近地发现，且速度和成本均可接受。

Abstract: An AI agent might surprisingly find she has reached an unknown state which
she has never been aware of -- an unknown unknown. We mathematically ground
this scenario in reinforcement learning: an agent, after taking an action
calculated from value functions $Q$ and $V$ defined on the {\it {aware
domain}}, reaches a state out of the domain. To enable the agent to handle this
scenario, we propose an {\it episodic Markov decision {process} with growing
awareness} (EMDP-GA) model, taking a new {\it noninformative value expansion}
(NIVE) approach to expand value functions to newly aware areas: when an agent
arrives at an unknown unknown, value functions $Q$ and $V$ whereon are
initialised by noninformative beliefs -- the averaged values on the aware
domain. This design is out of respect for the complete absence of knowledge in
the newly discovered state. The upper confidence bound momentum Q-learning is
then adapted to the growing awareness for training the EMDP-GA model. We prove
that (1) the regret of our approach is asymptotically consistent with the state
of the art (SOTA) without exposure to unknown unknowns in an extremely
uncertain environment, and (2) our computational complexity and space
complexity are comparable with the SOTA -- these collectively suggest that
though an unknown unknown is surprising, it will be asymptotically properly
discovered with decent speed and an affordable cost.

</details>

### [301] [Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space](https://arxiv.org/abs/2505.13308)
*Hengli Li,Chenxi Li,Tong Wu,Xuekai Zhu,Yuxuan Wang,Zhaoxin Yu,Eric Hanchen Jiang,Song-Chun Zhu,Zixia Jia,Ying Nian Wu,Zilong Zheng*

Main category: cs.LG

TLDR: LatentSeek通过潜在空间的测试时实例级适应（TTIA）提升LLM的推理能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在推理能力上仍有挑战，如灾难性遗忘和训练数据不足，测试时扩展成为替代方案。

Method: 提出LatentSeek框架，利用策略梯度迭代更新潜在表示，通过自生成奖励信号指导。

Result: 在多个推理基准测试中表现优于基线方法，高效且可扩展。

Conclusion: LatentSeek是轻量级、可扩展且有效的解决方案，凸显潜在空间测试时扩展的潜力。

Abstract: Reasoning ability, a core component of human intelligence, continues to pose
a significant challenge for Large Language Models (LLMs) in the pursuit of AGI.
Although model performance has improved under the training scaling law,
significant challenges remain, particularly with respect to training
algorithms, such as catastrophic forgetting, and the limited availability of
novel training data. As an alternative, test-time scaling enhances reasoning
performance by increasing test-time computation without parameter updating.
Unlike prior methods in this paradigm focused on token space, we propose
leveraging latent space for more effective reasoning and better adherence to
the test-time scaling law. We introduce LatentSeek, a novel framework that
enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)
within the model's latent space. Specifically, LatentSeek leverages policy
gradient to iteratively update latent representations, guided by self-generated
reward signals. LatentSeek is evaluated on a range of reasoning benchmarks,
including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.
Results show that LatentSeek consistently outperforms strong baselines, such as
Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our
analysis demonstrates that LatentSeek is highly efficient, typically converging
within a few iterations for problems of average complexity, while also
benefiting from additional iterations, thereby highlighting the potential of
test-time scaling in the latent space. These findings position LatentSeek as a
lightweight, scalable, and effective solution for enhancing the reasoning
capabilities of LLMs.

</details>

### [302] [True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics](https://arxiv.org/abs/2505.13192)
*Christoph Jürgen Hemmer,Daniel Durstewitz*

Main category: cs.LG

TLDR: DynaMix是一种新型的基于ALRNN的混合专家架构，用于动态系统重建（DSR），能够在零样本情况下泛化到域外动态系统，优于现有时间序列基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有DSR方法需要针对每个新系统进行专门训练，缺乏像LLMs那样的零样本和上下文推理能力。

Method: DynaMix采用预训练的ALRNN混合专家架构，无需重新训练即可从上下文信号中推断新系统的长期行为。

Result: DynaMix在长期统计和短期预测上优于现有时间序列基础模型，且参数更少、推理速度更快。

Conclusion: 基于动态系统原理的模型（如DynaMix）在时间序列预测领域具有巨大潜力。

Abstract: Complex, temporally evolving phenomena, from climate to brain activity, are
governed by dynamical systems (DS). DS reconstruction (DSR) seeks to infer
generative surrogate models of these from observed data, reproducing their
long-term behavior. Existing DSR approaches require purpose-training for any
new system observed, lacking the zero-shot and in-context inference
capabilities known from LLMs. Here we introduce DynaMix, a novel multivariate
ALRNN-based mixture-of-experts architecture pre-trained for DSR, the first DSR
model able to generalize zero-shot to out-of-domain DS. Just from a provided
context signal, without any re-training, DynaMix faithfully forecasts the
long-term evolution of novel DS where existing time series (TS) foundation
models, like Chronos, fail -- at a fraction of the number of parameters and
orders of magnitude faster inference times. DynaMix outperforms TS foundation
models in terms of long-term statistics, and often also short-term forecasts,
even on real-world time series, like traffic or weather data, typically used
for training and evaluating TS models, but not at all part of DynaMix' training
corpus. We illustrate some of the failure modes of TS models for DSR problems,
and conclude that models built on DS principles may bear a huge potential also
for advancing the TS prediction field.

</details>

### [303] [A Physics-Inspired Optimizer: Velocity Regularized Adam](https://arxiv.org/abs/2505.13196)
*Pranav Vaidhyanathan,Lucas Schorling,Natalia Ares,Michael A. Osborne*

Main category: cs.LG

TLDR: VRAdam是一种基于物理启发的优化器，通过速度正则化改进Adam，抑制训练中的振荡并提升收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统优化器（如Adam）在训练中容易产生快速振荡和收敛缓慢的问题，VRAdam通过引入速度正则化解决这一问题。

Method: VRAdam在Adam基础上增加高阶速度惩罚项，动态调整学习率，抑制大权重更新时的振荡。

Result: VRAdam在图像分类、语言建模等任务中表现优于AdamW等标准优化器。

Conclusion: VRAdam通过结合速度正则化和Adam的逐参数缩放，显著提升了训练稳定性和性能。

Abstract: We introduce Velocity-Regularized Adam (VRAdam), a physics-inspired optimizer
for training deep neural networks that draws on ideas from quartic terms for
kinetic energy with its stabilizing effects on various system dynamics.
Previous algorithms, including the ubiquitous Adam, operate at the so called
adaptive edge of stability regime during training leading to rapid oscillations
and slowed convergence of loss. However, VRAdam adds a higher order penalty on
the learning rate based on the velocity such that the algorithm automatically
slows down whenever weight updates become large. In practice, we observe that
the effective dynamic learning rate shrinks in high-velocity regimes, damping
oscillations and allowing for a more aggressive base step size when necessary
without divergence. By combining this velocity-based regularizer for global
damping with per-parameter scaling of Adam to create a hybrid optimizer, we
demonstrate that VRAdam consistently exceeds the performance against standard
optimizers including AdamW. We benchmark various tasks such as image
classification, language modeling, image generation and generative modeling
using diverse architectures and training methodologies including Convolutional
Neural Networks (CNNs), Transformers, and GFlowNets.

</details>

### [304] [Inferring stochastic dynamics with growth from cross-sectional data](https://arxiv.org/abs/2505.13197)
*Stephen Zhang,Suryanarayana Maddu,Xiaoje Qiu,Victor Chardès*

Main category: cs.LG

TLDR: 提出了一种名为“不平衡概率流推断”的新方法，用于从时间分辨单细胞组学数据中推断生物物理模型，解决了细胞分裂、死亡和分子状态变化的挑战。


<details>
  <summary>Details</summary>
Motivation: 时间分辨单细胞组学数据具有高通量、全基因组测量的优势，但由于其破坏性和细胞动态变化（如分裂、死亡），推断真实的生物物理模型面临挑战。

Method: 利用Fokker-Planck方程的拉格朗日形式，提出“不平衡概率流推断”方法，准确分离漂移、固有噪声和生长效应。

Result: 在模拟和真实单细胞RNA-seq数据集上的评估表明，该方法比现有方法更准确，且训练过程简单。

Conclusion: 该方法为从单细胞数据中推断生物过程提供了更准确的工具，具有实际应用潜力。

Abstract: Time-resolved single-cell omics data offers high-throughput, genome-wide
measurements of cellular states, which are instrumental to reverse-engineer the
processes underpinning cell fate. Such technologies are inherently destructive,
allowing only cross-sectional measurements of the underlying stochastic
dynamical system. Furthermore, cells may divide or die in addition to changing
their molecular state. Collectively these present a major challenge to
inferring realistic biophysical models. We present a novel approach,
\emph{unbalanced} probability flow inference, that addresses this challenge for
biological processes modelled as stochastic dynamics with growth. By leveraging
a Lagrangian formulation of the Fokker-Planck equation, our method accurately
disentangles drift from intrinsic noise and growth. We showcase the
applicability of our approach through evaluation on a range of simulated and
real single-cell RNA-seq datasets. Comparing to several existing methods, we
find our method achieves higher accuracy while enjoying a simple two-step
training scheme.

</details>

### [305] [A Minimum Description Length Approach to Regularization in Neural Networks](https://arxiv.org/abs/2505.13398)
*Matan Abudy,Orr Well,Emmanuel Chemla,Roni Katzir,Nur Lan*

Main category: cs.LG

TLDR: 论文探讨了正则化方法对神经网络训练的影响，提出MDL原则优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在训练中为何无法收敛到完美解，而仅得到近似解。

Method: 比较传统正则化方法（L1、L2或无）与基于MDL原则的方法在形式语言任务中的表现。

Result: MDL能选择完美解而非近似解，且不受优化算法影响。

Conclusion: MDL提供了更优的归纳偏置，有效抑制过拟合并提升泛化能力。

Abstract: State-of-the-art neural networks can be trained to become remarkable
solutions to many problems. But while these architectures can express symbolic,
perfect solutions, trained models often arrive at approximations instead. We
show that the choice of regularization method plays a crucial role: when
trained on formal languages with standard regularization ($L_1$, $L_2$, or
none), expressive architectures not only fail to converge to correct solutions
but are actively pushed away from perfect initializations. In contrast,
applying the Minimum Description Length (MDL) principle to balance model
complexity with data fit provides a theoretically grounded regularization
method. Using MDL, perfect solutions are selected over approximations,
independently of the optimization algorithm. We propose that unlike existing
regularization techniques, MDL introduces the appropriate inductive bias to
effectively counteract overfitting and promote generalization.

</details>

### [306] [Implicit bias produces neural scaling laws in learning curves, from perceptrons to deep networks](https://arxiv.org/abs/2505.13230)
*Francesco D'Amico,Dario Bocchi,Matteo Negri*

Main category: cs.LG

TLDR: 论文揭示了深度学习中的动态缩放规律，通过谱复杂度范数分析训练动态，发现了两种新的动态缩放规律，并提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习中的缩放规律，揭示训练动态对模型性能的影响，填补现有研究对训练过程动态分析的空白。

Method: 通过谱复杂度范数分析训练动态，结合实验验证（CNN、ResNet、Vision Transformer等）和理论模型（单层感知机）。

Result: 发现两种新的动态缩放规律，解释了泛化性能的涌现，并在多种模型和数据集上验证了一致性。

Conclusion: 动态缩放规律为理解深度学习模型的训练过程和泛化性能提供了新的视角和理论支持。

Abstract: Scaling laws in deep learning - empirical power-law relationships linking
model performance to resource growth - have emerged as simple yet striking
regularities across architectures, datasets, and tasks. These laws are
particularly impactful in guiding the design of state-of-the-art models, since
they quantify the benefits of increasing data or model size, and hint at the
foundations of interpretability in machine learning. However, most studies
focus on asymptotic behavior at the end of training or on the optimal training
time given the model size. In this work, we uncover a richer picture by
analyzing the entire training dynamics through the lens of spectral complexity
norms. We identify two novel dynamical scaling laws that govern how performance
evolves during training. These laws together recover the well-known test error
scaling at convergence, offering a mechanistic explanation of generalization
emergence. Our findings are consistent across CNNs, ResNets, and Vision
Transformers trained on MNIST, CIFAR-10 and CIFAR-100. Furthermore, we provide
analytical support using a solvable model: a single-layer perceptron trained
with binary cross-entropy. In this setting, we show that the growth of spectral
complexity driven by the implicit bias mirrors the generalization behavior
observed at fixed norm, allowing us to connect the performance dynamics to
classical learning rules in the perceptron.

</details>

### [307] [Reconstructing Physics-Informed Machine Learning for Traffic Flow Modeling: a Multi-Gradient Descent and Pareto Learning Approach](https://arxiv.org/abs/2505.13241)
*Yuan-Zheng Lei,Yaobang Gong,Dianwei Chen,Yao Cheng,Xianfeng Terry Yang*

Main category: cs.LG

TLDR: 论文提出了一种多目标优化方法替代传统的线性标量化方法，用于物理信息机器学习（PIML）中的损失函数优化，显著提升了复杂场景下的模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统PIML中线性标量化方法仅能探索帕累托前沿的凸区域，且权重调整耗时，限制了模型性能的提升。

Method: 将训练过程重构为多目标优化问题，应用多种多梯度下降算法（如TMGD和DCGD）独立处理数据驱动损失和物理损失。

Result: 在多目标优化下，宏观交通流模型中表现与传统方法相当，微观模型中显著优于标量化方法。

Conclusion: 多目标优化方法在复杂PIML场景中具有显著优势，为未来研究提供了新方向。

Abstract: Physics-informed machine learning (PIML) is crucial in modern traffic flow
modeling because it combines the benefits of both physics-based and data-driven
approaches. In conventional PIML, physical information is typically
incorporated by constructing a hybrid loss function that combines data-driven
loss and physics loss through linear scalarization. The goal is to find a
trade-off between these two objectives to improve the accuracy of model
predictions. However, from a mathematical perspective, linear scalarization is
limited to identifying only the convex region of the Pareto front, as it treats
data-driven and physics losses as separate objectives. Given that most PIML
loss functions are non-convex, linear scalarization restricts the achievable
trade-off solutions. Moreover, tuning the weighting coefficients for the two
loss components can be both time-consuming and computationally challenging. To
address these limitations, this paper introduces a paradigm shift in PIML by
reformulating the training process as a multi-objective optimization problem,
treating data-driven loss and physics loss independently. We apply several
multi-gradient descent algorithms (MGDAs), including traditional multi-gradient
descent (TMGD) and dual cone gradient descent (DCGD), to explore the Pareto
front in this multi-objective setting. These methods are evaluated on both
macroscopic and microscopic traffic flow models. In the macroscopic case, MGDAs
achieved comparable performance to traditional linear scalarization methods.
Notably, in the microscopic case, MGDAs significantly outperformed their
scalarization-based counterparts, demonstrating the advantages of a
multi-objective optimization approach in complex PIML scenarios.

</details>

### [308] [Fine-tuning Quantized Neural Networks with Zeroth-order Optimization](https://arxiv.org/abs/2505.13430)
*Sifeng Shang,Jiayi Zhou,Chenyu Lin,Minxian Li,Kaiyang Zhou*

Main category: cs.LG

TLDR: 论文提出了一种名为QZO的新型方法，通过零阶优化和量化技术显著减少大语言模型训练时的内存占用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模指数级增长，GPU内存成为下游任务适配的瓶颈，需高效内存训练方法。

Method: 结合零阶优化（避免梯度和优化器状态）与量化技术（减少权重内存），提出QZO方法，通过扰动连续量化尺度估计梯度并稳定训练。

Result: QZO将4位LLM的总内存成本降低18倍以上，可在单24GB GPU上微调Llama-2-13B和Stable Diffusion 3.5 Large。

Conclusion: QZO是一种高效内存的训练框架，适用于大规模模型微调。

Abstract: As the size of large language models grows exponentially, GPU memory has
become a bottleneck for adapting these models to downstream tasks. In this
paper, we aim to push the limits of memory-efficient training by minimizing
memory usage on model weights, gradients, and optimizer states, within a
unified framework. Our idea is to eliminate both gradients and optimizer states
using zeroth-order optimization, which approximates gradients by perturbing
weights during forward passes to identify gradient directions. To minimize
memory usage on weights, we employ model quantization, e.g., converting from
bfloat16 to int4. However, directly applying zeroth-order optimization to
quantized weights is infeasible due to the precision gap between discrete
weights and continuous gradients, which would otherwise require de-quantization
and re-quantization. To overcome this challenge, we propose Quantized
Zeroth-order Optimization (QZO), a novel approach that perturbs the continuous
quantization scale for gradient estimation and uses a directional derivative
clipping method to stabilize training. QZO is orthogonal to both scalar-based
and codebook-based post-training quantization methods. Compared to
full-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by
more than 18$\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and
Stable Diffusion 3.5 Large within a single 24GB GPU.

</details>

### [309] [RN-F: A Novel Approach for Mitigating Contaminated Data in Large Language Models](https://arxiv.org/abs/2505.13249)
*Le Vu Anh,Dinh Duc Nha Nguyen,Phi Long Nguyen*

Main category: cs.LG

TLDR: 论文提出了一种名为RN-F的新方法，用于检测大型语言模型中的数据污染问题，其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数据污染问题影响了大型语言模型应用的可靠性，现有方法无法有效解决这一问题。

Method: 提出了一种名为Residual-Noise Fingerprinting (RN-F)的轻量级、模型无关且高效的单次梯度检测方法。

Result: RN-F在多个大型语言模型和污染数据集上表现优于现有方法，检测指标提升达10.5%。

Conclusion: RN-F是一种有效的检测数据污染的方法，具有实际应用潜力。

Abstract: Large Language Models (LLMs) have become foundational in modern artificial
intelligence, powering a wide range of applications from code generation and
virtual assistants to scientific research and enterprise automation. However,
concerns about data contamination--where test data overlaps with training
data--have raised serious questions about the reliability of these
applications. Despite awareness of this issue, existing methods fall short in
effectively identifying or mitigating contamination. In this paper, we propose
Residual-Noise Fingerprinting (RN-F), a novel framework for detecting
contaminated data in LLMs. RN-F is a single-pass, gradient-free detection
method that leverages residual signal patterns without introducing additional
floating-point operations. Our approach is lightweight, model-agnostic, and
efficient. We evaluate RN-F on multiple LLMs across various contaminated
datasets and show that it consistently outperforms existing state-of-the-art
methods, achieving performance improvements of up to 10.5% in contamination
detection metrics.

</details>

### [310] [Optimizing Anytime Reasoning via Budget Relative Policy Optimization](https://arxiv.org/abs/2505.13438)
*Penghui Qi,Zichen Liu,Tianyu Pang,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TLDR: 论文提出AnytimeReasoner框架，优化语言模型的实时推理性能，通过截断思维过程适应不同token预算，引入密集奖励提升RL效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在大固定token预算下仅优化最终性能，导致训练和部署效率低下。

Method: 截断完整思维过程以适应采样token预算，引入密集奖励，分离优化思维和总结策略，提出BRPO减少方差。

Result: 在数学推理任务中，AnytimeReasoner在所有预算下均优于GRPO，提升训练和token效率。

Conclusion: AnytimeReasoner通过优化实时推理性能和引入BRPO，显著提升了语言模型的效率和灵活性。

Abstract: Scaling test-time compute is crucial for enhancing the reasoning capabilities
of large language models (LLMs). Existing approaches typically employ
reinforcement learning (RL) to maximize a verifiable reward obtained at the end
of reasoning traces. However, such methods optimize only the final performance
under a large and fixed token budget, which hinders efficiency in both training
and deployment. In this work, we present a novel framework, AnytimeReasoner, to
optimize anytime reasoning performance, which aims to improve token efficiency
and the flexibility of reasoning under varying token budget constraints. To
achieve this, we truncate the complete thinking process to fit within sampled
token budgets from a prior distribution, compelling the model to summarize the
optimal answer for each truncated thinking for verification. This introduces
verifiable dense rewards into the reasoning process, facilitating more
effective credit assignment in RL optimization. We then optimize the thinking
and summary policies in a decoupled manner to maximize the cumulative reward.
Additionally, we introduce a novel variance reduction technique, Budget
Relative Policy Optimization (BRPO), to enhance the robustness and efficiency
of the learning process when reinforcing the thinking policy. Empirical results
in mathematical reasoning tasks demonstrate that our method consistently
outperforms GRPO across all thinking budgets under various prior distributions,
enhancing both training and token efficiency.

</details>

### [311] [Net-Zero: A Comparative Study on Neural Network Design for Climate-Economic PDEs Under Uncertainty](https://arxiv.org/abs/2505.13264)
*Carlos Rodriguez-Pardo,Louis Daumas,Leonardo Chiani,Massimo Tavoni*

Main category: cs.LG

TLDR: 论文探讨了基于神经网络的方法，用于解决高维最优控制问题，以应对气候经济建模中的不确定性，并评估了不同神经网络架构的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在处理高维气候经济模型时计算困难，限制了政策制定者有效应对气候变化的能力。

Method: 开发了一个连续时间的内生经济增长模型，结合多种减排路径，并比较了不同神经网络架构与有限差分生成解的准确性。

Result: 研究发现，选择合适的神经网络架构能显著提高模型解的准确性和计算效率。

Conclusion: 该方法为气候政策决策提供了更精细的建模工具，有助于更好地应对气候变化中的技术转型和不确定性。

Abstract: Climate-economic modeling under uncertainty presents significant
computational challenges that may limit policymakers' ability to address
climate change effectively. This paper explores neural network-based approaches
for solving high-dimensional optimal control problems arising from models that
incorporate ambiguity aversion in climate mitigation decisions. We develop a
continuous-time endogenous-growth economic model that accounts for multiple
mitigation pathways, including emission-free capital and carbon intensity
reductions. Given the inherent complexity and high dimensionality of these
models, traditional numerical methods become computationally intractable. We
benchmark several neural network architectures against finite-difference
generated solutions, evaluating their ability to capture the dynamic
interactions between uncertainty, technology transitions, and optimal climate
policy. Our findings demonstrate that appropriate neural architecture selection
significantly impacts both solution accuracy and computational efficiency when
modeling climate-economic systems under uncertainty. These methodological
advances enable more sophisticated modeling of climate policy decisions,
allowing for better representation of technology transitions and
uncertainty-critical elements for developing effective mitigation strategies in
the face of climate change.

</details>

### [312] [Neural Functional: Learning Function to Scalar Maps for Neural PDE Surrogates](https://arxiv.org/abs/2505.13275)
*Anthony Zhou,Amir Barati Farimani*

Main category: cs.LG

TLDR: 提出了一种新的神经网络架构Neural Functional，用于学习函数到标量的映射，并展示了其在PDE替代模型中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络或算子学习架构在PDE替代模型中存在局限性，需要一种能够学习函数导数的新方法。

Method: 结合算子学习和神经场的见解，提出Neural Functional架构，能够隐式学习函数导数。

Result: Neural Functional在1D和2D PDE中表现出改进的稳定性和能量守恒特性。

Conclusion: Neural Functional不仅适用于PDE替代模型，还可扩展至分子动力学或设计优化等领域。

Abstract: Many architectures for neural PDE surrogates have been proposed in recent
years, largely based on neural networks or operator learning. In this work, we
derive and propose a new architecture, the Neural Functional, which learns
function to scalar mappings. Its implementation leverages insights from
operator learning and neural fields, and we show the ability of neural
functionals to implicitly learn functional derivatives. For the first time,
this allows for an extension of Hamiltonian mechanics to neural PDE surrogates
by learning the Hamiltonian functional and optimizing its functional
derivatives. We demonstrate that the Hamiltonian Neural Functional can be an
effective surrogate model through improved stability and conserving energy-like
quantities on 1D and 2D PDEs. Beyond PDEs, functionals are prevalent in
physics; functional approximation and learning with its gradients may find
other uses, such as in molecular dynamics or design optimization.

</details>

### [313] [RECON: Robust symmetry discovery via Explicit Canonical Orientation Normalization](https://arxiv.org/abs/2505.13289)
*Alonso Urbano,David W. Romero,Max Zimmer,Sebastian Pokutta*

Main category: cs.LG

TLDR: RECON框架通过数据驱动的方法发现输入数据的固有对称性分布，无需预先固定变换群，提升了对称性建模的灵活性。


<details>
  <summary>Details</summary>
Motivation: 现实数据常具有未知或近似对称性，现有等变网络需预先固定变换群（如连续旋转群SO(2)），导致实际对称性与预设不匹配时性能下降。

Method: RECON利用类-姿态分解和数据驱动归一化，将任意参考系对齐到共同的自然姿态，生成可比较和可解释的对称性描述符。

Result: 在2D图像基准测试中有效发现对称性，并首次扩展到3D变换群。

Conclusion: RECON为更灵活的等变建模铺平了道路。

Abstract: Real-world data often exhibits unknown or approximate symmetries, yet
existing equivariant networks must commit to a fixed transformation group prior
to training, e.g., continuous $SO(2)$ rotations. This mismatch degrades
performance when the actual data symmetries differ from those in the
transformation group. We introduce RECON, a framework to discover each input's
intrinsic symmetry distribution from unlabeled data. RECON leverages class-pose
decompositions and applies a data-driven normalization to align arbitrary
reference frames into a common natural pose, yielding directly comparable and
interpretable symmetry descriptors. We demonstrate effective symmetry discovery
on 2D image benchmarks and -- for the first time -- extend it to 3D
transformation groups, paving the way towards more flexible equivariant
modeling.

</details>

### [314] [TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning Engineering Agents](https://arxiv.org/abs/2505.13291)
*Yifu Cai,Xinyu Li,Mononito Goswami,Michał Wiliński,Gus Welter,Artur Dubrawski*

Main category: cs.LG

TLDR: TimeSeriesGym是一个可扩展的基准测试框架，用于评估AI代理在时间序列机器学习工程挑战中的表现，解决了现有基准测试在可扩展性和多样性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏可扩展性，且仅关注模型构建的有限场景，无法全面评估AI代理在机器学习工程中的实际能力。

Method: 框架通过整合多领域任务和多样化挑战（如数据处理、代码翻译等），并开发支持大规模挑战设计的工具，同时采用数值和LLM评估方法。

Result: TimeSeriesGym不仅适用于时间序列应用，还可扩展到其他数据类型，提升了AI代理评估的全面性和实用性。

Conclusion: 该框架的开源将促进未来对AI代理机器学习工程能力的研究。

Abstract: We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating
Artificial Intelligence (AI) agents on time series machine learning engineering
challenges. Existing benchmarks lack scalability, focus narrowly on model
building in well-defined settings, and evaluate only a limited set of research
artifacts (e.g., CSV submission files). To make AI agent benchmarking more
relevant to the practice of machine learning engineering, our framework scales
along two critical dimensions. First, recognizing that effective ML engineering
requires a range of diverse skills, TimeSeriesGym incorporates challenges from
diverse sources spanning multiple domains and tasks. We design challenges to
evaluate both isolated capabilities (including data handling, understanding
research repositories, and code translation) and their combinations, and rather
than addressing each challenge independently, we develop tools that support
designing multiple challenges at scale. Second, we implement evaluation
mechanisms for multiple research artifacts, including submission files, code,
and models, using both precise numeric measures and more flexible LLM-based
evaluation approaches. This dual strategy balances objective assessment with
contextual judgment. Although our initial focus is on time series applications,
our framework can be readily extended to other data modalities, broadly
enhancing the comprehensiveness and practical utility of agentic AI evaluation.
We open-source our benchmarking framework to facilitate future research on the
ML engineering capabilities of AI agents.

</details>

### [315] [KHRONOS: a Kernel-Based Neural Architecture for Rapid, Resource-Efficient Scientific Computation](https://arxiv.org/abs/2505.13315)
*Reza T. Batley,Sourav Saha*

Main category: cs.LG

TLDR: KHRONOS是一种AI框架，通过分层核扩展构建连续可微的目标场，显著提升了高维物理系统的建模效率。


<details>
  <summary>Details</summary>
Motivation: 解决高维物理系统建模中的维度灾难和数据密集依赖问题。

Method: 使用分层核扩展构建目标场，并通过张量化和叠加实现高效建模。

Result: 在2D泊松方程基准测试中，L2平方误差显著降低，计算效率提升显著。

Conclusion: KHRONOS在边缘计算、在线控制和计算机视觉等领域具有广泛应用潜力。

Abstract: Contemporary models of high dimensional physical systems are constrained by
the curse of dimensionality and a reliance on dense data. We introduce KHRONOS
(Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates), an
AI framework for model based, model free and model inversion tasks. KHRONOS
constructs continuously differentiable target fields with a hierarchical
composition of per-dimension kernel expansions, which are tensorized into modes
and then superposed. We evaluate KHRONOS on a canonical 2D, Poisson equation
benchmark: across 16 to 512 degrees of freedom (DoFs), it obtained L2 square
errors of 5e-4 down to 6e-10. This represents a 100 time gain over Kolmogorov
Arnold Networks (which itself reports a 100 times improvement on MLPs/PINNs
with 100 times fewer parameters) when controlling for the number of parameters.
This also represents a 1e4 times improvement in L2 square error compared to
standard linear FEM at comparable DoFs. Inference complexity is dominated by
inner products, yielding sub-millisecond full-field predictions that scale to
an arbitrary resolution. For inverse problems, KHRONOS facilitates rapid,
iterative level set recovery in only a few forward evaluations, with
sub-microsecond per sample latency. KHRONOS scalability, expressivity, and
interpretability open new avenues in constrained edge computing, online
control, computer vision, and beyond.

</details>

### [316] [Unlabeled Data or Pre-trained Model: Rethinking Semi-Supervised Learning and Pretrain-Finetuning](https://arxiv.org/abs/2505.13317)
*Song-Lin Li,Rui Zhu,Yu-Feng Li,Lan-Zhe Guo*

Main category: cs.LG

TLDR: 论文比较了半监督学习（SSL）和预训练-微调范式在标签数据稀缺时的表现，发现预训练视觉语言模型（VLMs）在多数情况下优于SSL，除非数据分辨率低或缺乏清晰语义结构。


<details>
  <summary>Details</summary>
Motivation: 研究在标签数据稀缺时，应选择利用未标记数据（SSL）还是预训练模型（预训练-微调范式）。

Method: 提出Few-shot SSL框架，公平比较两种范式，控制标签数据量，并选择预训练VLMs作为代表。

Result: 预训练VLMs在几乎所有情况下优于SSL方法，除非数据分辨率低或语义结构不清晰。

Conclusion: 建议未来SSL研究与预训练模型比较，并探索更深层次整合，如利用预训练知识增强伪标签。同时发布了统一的复现和评估框架。

Abstract: Semi-supervised learning (SSL) alleviates the cost of data labeling process
by exploiting unlabeled data, and has achieved promising results on various
tasks such as image classification. Meanwhile, the Pretrain-Finetuning paradigm
has garnered significant attention in recent years, and exploiting pre-trained
models could also reduce the requirement of labeled data in downstream tasks.
Therefore, a question naturally occurs: \emph{When the labeled data is scarce
in the target tasks, should we exploit unlabeled data or pre-trained models?}
To answer this question, we select pre-trained Vision-Language Models (VLMs) as
representative pretrain-finetuning instances and propose \textit{Few-shot SSL}
-- a framework that enables fair comparison between these two paradigms by
controlling the amount of labeled data used. Extensive experiments across
various settings demonstrate that pre-trained VLMs generally outperform SSL
methods in nearly all cases, except when the data has low resolution or lacks
clear semantic structure. Therefore, we encourage future SSL research to
compare with pre-trained models and explore deeper integration, such as using
pre-trained knowledge to enhance pseudo-labeling. To support future research,
we release our unified reproduction and evaluation framework. Codes are
available at
https://anonymous.4open.science/r/Rethinking-SSL-and-Pretrain-Finetuning-5566

</details>

### [317] [Thinking Short and Right Over Thinking Long: Serving LLM Reasoning Efficiently and Accurately](https://arxiv.org/abs/2505.13326)
*Yuhang Wang,Youhe Jiang,Bin Cui,Fangcheng Fu*

Main category: cs.LG

TLDR: SART框架通过优化LLM的推理过程，解决了多分支推理导致的效率问题，提升了准确性和服务效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在结合Chain-of-Thought推理和多分支推理时，系统效率显著下降，主要由于时间成本和内存消耗增加。

Method: 提出SART框架，包括冗余采样与提前停止策略以生成短推理分支，以及动态剪枝低质量分支以减少内存消耗。

Result: 实验表明，SART在保持相同准确性的情况下，效率提升高达28.2倍，平均15.7倍。

Conclusion: SART通过优化推理分支管理，显著提升了LLM的服务效率和准确性。

Abstract: Recent advances in test-time scaling suggest that Large Language Models
(LLMs) can gain better capabilities by generating Chain-of-Thought reasoning
(analogous to human thinking) to respond a given request, and meanwhile
exploring more reasoning branches (i.e., generating multiple responses and
ensembling them) can improve the final output quality. However, when
incorporating the two scaling dimensions, we find that the system efficiency is
dampened significantly for two reasons. Firstly, the time cost to generate the
final output increases substantially as many reasoning branches would be
trapped in the over-thinking dilemma, producing excessively long responses.
Secondly, generating multiple reasoning branches for each request increases
memory consumption, which is unsuitable for LLM serving since we can only batch
a limited number of requests to process simultaneously. To address this, we
present SART, a serving framework for efficient and accurate LLM reasoning. The
essential idea is to manage the thinking to be short and right, rather than
long. For one thing, we devise a redundant sampling with early stopping
approach based on empirical observations and theoretic analysis, which
increases the likelihood of obtaining short-thinking responses when sampling
reasoning branches. For another, we propose to dynamically prune low-quality
branches so that only right-thinking branches are maintained, reducing the
memory consumption and allowing us to batch more requests. Experimental results
demonstrate that SART not only improves the accuracy of LLM reasoning but also
enhances the serving efficiency, outperforming existing methods by up to 28.2
times and on average 15.7 times in terms of efficiency when achieving the same
level of accuracy.

</details>

### [318] [Detect and Correct: A Selective Noise Correction Method for Learning with Noisy Labels](https://arxiv.org/abs/2505.13342)
*Yuval Grinberg,Nimrod Harel,Jacob Goldberger,Ofir Lindenbaum*

Main category: cs.LG

TLDR: 提出了一种基于损失分布识别和修正噪声标签的方法，结合数据筛选和噪声转移矩阵，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 噪声标签会显著损害深度学习模型的性能，现有方法（全局噪声估计和数据筛选）存在调整正确标签或丢失有价值数据的风险。

Method: 通过损失分布识别潜在噪声样本，分离噪声和干净样本，并学习噪声转移矩阵修正噪声样本的损失，保留干净数据。

Result: 在MNIST、CIFAR-10、CIFAR-100和scRNA-seq数据集上，模型准确性和鲁棒性显著优于传统方法。

Conclusion: 该方法通过保留噪声样本中的有价值信息并优化修正过程，实现了鲁棒学习和模型性能提升。

Abstract: Falsely annotated samples, also known as noisy labels, can significantly harm
the performance of deep learning models. Two main approaches for learning with
noisy labels are global noise estimation and data filtering. Global noise
estimation approximates the noise across the entire dataset using a noise
transition matrix, but it can unnecessarily adjust correct labels, leaving room
for local improvements. Data filtering, on the other hand, discards potentially
noisy samples but risks losing valuable data. Our method identifies potentially
noisy samples based on their loss distribution. We then apply a selection
process to separate noisy and clean samples and learn a noise transition matrix
to correct the loss for noisy samples while leaving the clean data unaffected,
thereby improving the training process. Our approach ensures robust learning
and enhanced model performance by preserving valuable information from noisy
samples and refining the correction process. We applied our method to standard
image datasets (MNIST, CIFAR-10, and CIFAR-100) and a biological scRNA-seq
cell-type annotation dataset. We observed a significant improvement in model
accuracy and robustness compared to traditional methods.

</details>

### [319] [MRM3: Machine Readable ML Model Metadata](https://arxiv.org/abs/2505.13343)
*Andrej Čop,Blaž Bertalanič,Marko Grobelnik,Carolina Fortuna*

Main category: cs.LG

TLDR: 论文提出了一种结构化ML模型元数据的方案，支持机器可读格式并整合到知识图谱中，同时包含环境影响指标。


<details>
  <summary>Details</summary>
Motivation: 随着ML模型的复杂性和数量增长，结构化且机器可读的元数据对开发者和公司至关重要，同时需纳入环境影响指标。

Method: 定义了一个结构化ML模型元数据的模式，支持知识图谱集成，并提供了一个无线定位模型元数据示例数据集。

Result: 构建了一个包含22个模型和4个数据集的Neo4j知识图谱，包含113个节点和199个关系。

Conclusion: 结构化元数据方案提升了ML模型的组织和查询能力，扩展了其应用场景。

Abstract: As the complexity and number of machine learning (ML) models grows,
well-documented ML models are essential for developers and companies to use or
adapt them to their specific use cases. Model metadata, already present in
unstructured format as model cards in online repositories such as Hugging Face,
could be more structured and machine readable while also incorporating
environmental impact metrics such as energy consumption and carbon footprint.
Our work extends the existing State of the Art by defining a structured schema
for ML model metadata focusing on machine-readable format and support for
integration into a knowledge graph (KG) for better organization and querying,
enabling a wider set of use cases. Furthermore, we present an example wireless
localization model metadata dataset consisting of 22 models trained on 4
datasets, integrated into a Neo4j-based KG with 113 nodes and 199 relations.

</details>

### [320] [Occult: Optimizing Collaborative Communication across Experts for Accelerated Parallel MoE Training and Inference](https://arxiv.org/abs/2505.13345)
*Shuqing Luo,Pingzhi Li,Jie Peng,Hanrui Wang,Yang,Zhao,Yu,Cao,Yu Cheng,Tianlong Chen*

Main category: cs.LG

TLDR: 论文提出Occult方法，通过优化MoE架构中的协作通信（intra-和inter-collaboration）来减少通信开销，加速训练和推理，实验显示速度提升显著。


<details>
  <summary>Details</summary>
Motivation: MoE架构中的全设备通信开销大（占运行时40%以上），限制了分布式训练和推理的可扩展性。

Method: 定义协作通信，提出系统与算法创新（Occult），通过增加intra-collaboration比例或协作剪枝来优化通信。

Result: Occult在多个MoE-LLM上实现1.5倍以上加速，且质量与标准微调相当或更优。

Conclusion: Occult有效减少通信成本，显著提升MoE模型的训练和推理效率。

Abstract: Mixture-of-experts (MoE) architectures could achieve impressive computational
efficiency with expert parallelism, which relies heavily on all-to-all
communication across devices. Unfortunately, such communication overhead
typically constitutes a significant portion of the total runtime, hampering the
scalability of distributed training and inference for modern MoE models
(consuming over $40\%$ runtime in large-scale training). In this paper, we
first define collaborative communication to illustrate this intrinsic
limitation, and then propose system- and algorithm-level innovations to reduce
communication costs. Specifically, given a pair of experts co-activated by one
token, we call them "collaborated", which comprises $2$ cases as intra- and
inter-collaboration, depending on whether they are kept on the same device. Our
pilot investigations reveal that augmenting the proportion of
intra-collaboration can accelerate expert parallelism at scale. It motivates us
to strategically optimize collaborative communication for accelerated MoE
training and inference, dubbed Occult. Our designs are capable of either
delivering exact results with reduced communication cost or controllably
minimizing the cost with collaboration pruning, materialized by modified
fine-tuning. Comprehensive experiments on various MoE-LLMs demonstrate that
Occult can be faster than popular state-of-the-art inference or training
frameworks (more than $1.5\times$ speed up across multiple tasks and models)
with comparable or superior quality compared to the standard fine-tuning. Code
is available at
$\href{https://github.com/UNITES-Lab/Occult}{https://github.com/UNITES-Lab/Occult}$.

</details>

### [321] [One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling](https://arxiv.org/abs/2505.13358)
*Nimrod Berman,Ilan Naiman,Moshe Eliasof,Hedi Zisling,Omri Azencot*

Main category: cs.LG

TLDR: 提出了一种基于Koopman理论的离线蒸馏方法KDM，通过线性化表示扩散模型的非线性动态，实现单步生成并保持语义保真度，显著提升了生成效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的计算成本高，而离线蒸馏在效率、模块化和灵活性方面具有优势。利用Koopman理论工具和扩散模型的潜在空间结构，提出了一种新的蒸馏框架。

Method: KDM将噪声输入编码到嵌入空间，通过学习的线性算子传播，再通过解码器重构干净样本，实现单步生成。

Result: KDM在标准离线蒸馏基准测试中达到最先进性能，单步生成的FID分数提升高达40%。

Conclusion: KDM通过Koopman理论有效线性化扩散动态，实现了高效的生成，同时保持了语义保真度。

Abstract: Diffusion-based generative models have demonstrated exceptional performance,
yet their iterative sampling procedures remain computationally expensive. A
prominent strategy to mitigate this cost is distillation, with offline
distillation offering particular advantages in terms of efficiency, modularity,
and flexibility. In this work, we identify two key observations that motivate a
principled distillation framework: (1) while diffusion models have been viewed
through the lens of dynamical systems theory, powerful and underexplored tools
can be further leveraged; and (2) diffusion models inherently impose
structured, semantically coherent trajectories in latent space. Building on
these observations, we introduce the Koopman Distillation Model KDM, a novel
offline distillation approach grounded in Koopman theory-a classical framework
for representing nonlinear dynamics linearly in a transformed space. KDM
encodes noisy inputs into an embedded space where a learned linear operator
propagates them forward, followed by a decoder that reconstructs clean samples.
This enables single-step generation while preserving semantic fidelity. We
provide theoretical justification for our approach: (1) under mild assumptions,
the learned diffusion dynamics admit a finite-dimensional Koopman
representation; and (2) proximity in the Koopman latent space correlates with
semantic similarity in the generated outputs, allowing for effective trajectory
alignment. Empirically, KDM achieves state-of-the-art performance across
standard offline distillation benchmarks, improving FID scores by up to 40% in
a single generation step. All implementation details and code for the
experimental setups are provided in our GitHub -
https://github.com/azencot-group/KDM, or in our project page -
https://sites.google.com/view/koopman-distillation-model.

</details>

### [322] [Restoration Score Distillation: From Corrupted Diffusion Pretraining to One-Step High-Quality Generation](https://arxiv.org/abs/2505.13377)
*Yasi Zhang,Tianyu Chen,Zhendong Wang,Ying Nian Wu,Mingyuan Zhou,Oscar Leong*

Main category: cs.LG

TLDR: 论文提出了一种名为Restoration Score Distillation (RSD)的方法，扩展了Denoising Score Distillation (DSD)，用于从多种损坏数据中训练生成模型，并在恢复任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 从损坏数据中学习生成模型是一个重要但具有挑战性的任务，尤其是在干净数据有限或昂贵的情况下。

Method: RSD通过预训练一个教师扩散模型（仅使用损坏数据），然后将其蒸馏为单步生成器，生成高质量重建。

Result: RSD在多种恢复任务中表现优于其教师模型，并与多种损坏感知训练技术兼容。

Conclusion: RSD不仅是一种采样加速技术，还是一种在数据严重损坏情况下提升生成性能的原则性方法。

Abstract: Learning generative models from corrupted data is a fundamental yet
persistently challenging task across scientific disciplines, particularly when
access to clean data is limited or expensive. Denoising Score Distillation
(DSD) \cite{chen2025denoising} recently introduced a novel and surprisingly
effective strategy that leverages score distillation to train high-fidelity
generative models directly from noisy observations. Building upon this
foundation, we propose \textit{Restoration Score Distillation} (RSD), a
principled generalization of DSD that accommodates a broader range of
corruption types, such as blurred, incomplete, or low-resolution images. RSD
operates by first pretraining a teacher diffusion model solely on corrupted
data and subsequently distilling it into a single-step generator that produces
high-quality reconstructions. Empirically, RSD consistently surpasses its
teacher model across diverse restoration tasks on both natural and scientific
datasets. Moreover, beyond standard diffusion objectives, the RSD framework is
compatible with several corruption-aware training techniques such as Ambient
Tweedie, Ambient Diffusion, and its Fourier-space variant, enabling flexible
integration with recent advances in diffusion modeling. Theoretically, we
demonstrate that in a linear regime, RSD recovers the eigenspace of the clean
data covariance matrix from linear measurements, thereby serving as an implicit
regularizer. This interpretation recasts score distillation not only as a
sampling acceleration technique but as a principled approach to enhancing
generative performance in severely degraded data regimes.

</details>

### [323] [Learning by solving differential equations](https://arxiv.org/abs/2505.13397)
*Benoit Dherin,Michael Munn,Hanna Mazzawi,Michael Wunder,Sourabh Medapati,Javier Gonzalvo*

Main category: cs.LG

TLDR: 论文探讨了高阶Runge-Kutta（RK）方法在深度学习中的应用，分析了其局限性并提出了改进方案。


<details>
  <summary>Details</summary>
Motivation: 梯度下降是深度学习的主要学习方法，但高阶ODE求解器（如RK方法）在深度学习中的应用尚未广泛。研究旨在评估其性能并解决其局限性。

Method: 通过结合现代神经网络优化器的关键要素（如预处理、自适应学习率和动量），改进高阶RK求解器的性能。

Result: 研究发现高阶RK方法在深度学习中有潜力，但需结合优化技术以克服其局限性。

Conclusion: 高阶RK方法在深度学习中具有应用前景，但需进一步优化以适应现代神经网络的需求。

Abstract: Modern deep learning algorithms use variations of gradient descent as their
main learning methods. Gradient descent can be understood as the simplest
Ordinary Differential Equation (ODE) solver; namely, the Euler method applied
to the gradient flow differential equation. Since Euler, many ODE solvers have
been devised that follow the gradient flow equation more precisely and more
stably. Runge-Kutta (RK) methods provide a family of very powerful explicit and
implicit high-order ODE solvers. However, these higher-order solvers have not
found wide application in deep learning so far. In this work, we evaluate the
performance of higher-order RK solvers when applied in deep learning, study
their limitations, and propose ways to overcome these drawbacks. In particular,
we explore how to improve their performance by naturally incorporating key
ingredients of modern neural network optimizers such as preconditioning,
adaptive learning rates, and momentum.

</details>

### [324] [Mean Flows for One-step Generative Modeling](https://arxiv.org/abs/2505.13447)
*Zhengyang Geng,Mingyang Deng,Xingjian Bai,J. Zico Kolter,Kaiming He*

Main category: cs.LG

TLDR: 提出了一种基于平均速度的一步生成建模框架MeanFlow，无需预训练或课程学习，显著优于现有一步扩散/流模型。


<details>
  <summary>Details</summary>
Motivation: 旨在缩小一步扩散/流模型与多步模型之间的性能差距，并重新审视这些模型的底层原理。

Method: 引入平均速度概念，推导其与瞬时速度的关系，用于指导神经网络训练，无需额外步骤。

Result: 在ImageNet 256x256上单次评估（1-NFE）达到FID 3.43，显著优于现有方法。

Conclusion: MeanFlow大幅提升了一步模型的性能，为未来研究提供了新的方向。

Abstract: We propose a principled and effective framework for one-step generative
modeling. We introduce the notion of average velocity to characterize flow
fields, in contrast to instantaneous velocity modeled by Flow Matching methods.
A well-defined identity between average and instantaneous velocities is derived
and used to guide neural network training. Our method, termed the MeanFlow
model, is self-contained and requires no pre-training, distillation, or
curriculum learning. MeanFlow demonstrates strong empirical performance: it
achieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet
256x256 trained from scratch, significantly outperforming previous
state-of-the-art one-step diffusion/flow models. Our study substantially
narrows the gap between one-step diffusion/flow models and their multi-step
predecessors, and we hope it will motivate future research to revisit the
foundations of these powerful models.

</details>

### [325] [A Dataless Reinforcement Learning Approach to Rounding Hyperplane Optimization for Max-Cut](https://arxiv.org/abs/2505.13405)
*Gabriel Malikal,Ismail Alkhouri,Alvaro Velasquez,Adam M Alessio,Saiprasad Ravishankar*

Main category: cs.LG

TLDR: 本文提出了一种基于无训练数据的强化学习方法，用于改进Goemans-Williamson算法的超平面舍入策略，以在大规模图上获得更好的切割效果。


<details>
  <summary>Details</summary>
Motivation: MaxCut问题的最优解难以获取，现有启发式算法依赖领域知识，而学习型方法存在泛化和扩展性问题。Goemans-Williamson算法虽为经典近似方法，但其随机超平面舍入策略仍有改进空间。

Method: 采用非阶段性强化学习框架，通过马尔可夫决策过程优化超平面选择，以生成比GW算法更好的切割方案。

Result: 该方法在不同密度和度分布的大规模图上均能获得优于GW算法的切割效果。

Conclusion: 无训练数据的强化学习方法能有效改进GW算法的超平面舍入策略，提升MaxCut问题的求解效果。

Abstract: The Maximum Cut (MaxCut) problem is NP-Complete, and obtaining its optimal
solution is NP-hard in the worst case. As a result, heuristic-based algorithms
are commonly used, though their design often requires significant domain
expertise. More recently, learning-based methods trained on large (un)labeled
datasets have been proposed; however, these approaches often struggle with
generalizability and scalability. A well-known approximation algorithm for
MaxCut is the Goemans-Williamson (GW) algorithm, which relaxes the Quadratic
Unconstrained Binary Optimization (QUBO) formulation into a semidefinite
program (SDP). The GW algorithm then applies hyperplane rounding by uniformly
sampling a random hyperplane to convert the SDP solution into binary node
assignments. In this paper, we propose a training-data-free approach based on a
non-episodic reinforcement learning formulation, in which an agent learns to
select improved rounding hyperplanes that yield better cuts than those produced
by the GW algorithm. By optimizing over a Markov Decision Process (MDP), our
method consistently achieves better cuts across large-scale graphs with varying
densities and degree distributions.

</details>

### [326] [Joint Velocity-Growth Flow Matching for Single-Cell Dynamics Modeling](https://arxiv.org/abs/2505.13413)
*Dongyi Wang,Yuanwei Jiang,Zhenyi Zhang,Xiang Gu,Peijie Zhou,Jian Sun*

Main category: cs.LG

TLDR: VGFM是一种联合学习单细胞状态转移和质量增长的新方法，通过流匹配解决非配对和不平衡数据问题。


<details>
  <summary>Details</summary>
Motivation: 破坏性测量技术和细胞增殖/死亡导致快照数据非配对和不平衡，增加了学习单细胞动力学的难度。

Method: 提出VGFM，通过流匹配联合学习状态转移和质量增长，利用半松弛最优传输理论构建理想动力学模型，并用神经网络近似实现。

Result: 在合成和真实数据集上，VGFM能捕捉质量和状态变化的生物动力学，优于现有方法。

Conclusion: VGFM为单细胞动力学建模提供了有效的新范式。

Abstract: Learning the underlying dynamics of single cells from snapshot data has
gained increasing attention in scientific and machine learning research. The
destructive measurement technique and cell proliferation/death result in
unpaired and unbalanced data between snapshots, making the learning of the
underlying dynamics challenging. In this paper, we propose joint
Velocity-Growth Flow Matching (VGFM), a novel paradigm that jointly learns
state transition and mass growth of single-cell populations via flow matching.
VGFM builds an ideal single-cell dynamics containing velocity of state and
growth of mass, driven by a presented two-period dynamic understanding of the
static semi-relaxed optimal transport, a mathematical tool that seeks the
coupling between unpaired and unbalanced data. To enable practical usage, we
approximate the ideal dynamics using neural networks, forming our joint
velocity and growth matching framework. A distribution fitting loss is also
employed in VGFM to further improve the fitting performance for snapshot data.
Extensive experimental results on both synthetic and real datasets demonstrate
that VGFM can capture the underlying biological dynamics accounting for mass
and state variations over time, outperforming existing approaches for
single-cell dynamics modeling.

</details>

### [327] [Gluon: Making Muon & Scion Great Again! (Bridging Theory and Practice of LMO-based Optimizers for LLMs)](https://arxiv.org/abs/2505.13416)
*Artem Riabinin,Egor Shulgin,Kaja Gruntkowska,Peter Richtárik*

Main category: cs.LG

TLDR: 论文提出了一种新的基于LMO的优化方法Gluon，解决了现有理论分析中的不足，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LMO方法（如Muon和Scion）在实践中有优势，但理论分析存在不足，如忽略分层应用和不现实的平滑假设。

Method: 提出Gluon方法，结合新的广义平滑模型，匹配分层几何结构，并改进理论分析。

Result: 理论步长与实验值匹配，实验验证了假设的有效性，缩小了理论与实践差距。

Conclusion: Gluon方法在理论和实践上均表现出色，为深度学习优化提供了新方向。

Abstract: Recent developments in deep learning optimization have brought about
radically new algorithms based on the Linear Minimization Oracle (LMO)
framework, such as $\sf Muon$ and $\sf Scion$. After over a decade of $\sf
Adam$'s dominance, these LMO-based methods are emerging as viable replacements,
offering several practical advantages such as improved memory efficiency,
better hyperparameter transferability, and most importantly, superior empirical
performance on large-scale tasks, including LLM training. However, a
significant gap remains between their practical use and our current theoretical
understanding: prior analyses (1) overlook the layer-wise LMO application of
these optimizers in practice, and (2) rely on an unrealistic smoothness
assumption, leading to impractically small stepsizes. To address both, we
propose a new LMO-based method called $\sf Gluon$, capturing prior
theoretically analyzed methods as special cases, and introduce a new refined
generalized smoothness model that captures the layer-wise geometry of neural
networks, matches the layer-wise practical implementation of $\sf Muon$ and
$\sf Scion$, and leads to convergence guarantees with strong practical
predictive power. Unlike prior results, our theoretical stepsizes closely match
the fine-tuned values reported by Pethick et al. (2025). Our experiments with
NanoGPT and CNN confirm that our assumption holds along the optimization
trajectory, ultimately closing the gap between theory and practice.

</details>

### [328] [Make Still Further Progress: Chain of Thoughts for Tabular Data Leaderboard](https://arxiv.org/abs/2505.13421)
*Si-Yang Liu,Qile Zhou,Han-Jia Ye*

Main category: cs.LG

TLDR: 提出了一种基于大语言模型（LLMs）的动态集成框架（CoT²），用于表格数据预测，通过实例级适应性集成外部模型预测，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 表格数据在机器学习中广泛应用，但模型性能因数据集特性差异而波动，传统静态集成方法缺乏实例级适应性。

Method: 利用LLMs动态集成外部模型预测，通过最近邻构建上下文，并采用多步推理提示策略（CoT²）。

Result: 实验表明，该方法在多个表格数据集上优于传统集成方法和基线模型。

Conclusion: CoT²框架通过动态、实例级集成和可解释推理，提升了表格数据预测性能。

Abstract: Tabular data, a fundamental data format in machine learning, is predominantly
utilized in competitions and real-world applications. The performance of
tabular models--such as gradient boosted decision trees and neural
networks--can vary significantly across datasets due to differences in feature
distributions and task characteristics. Achieving top performance on each
dataset often requires specialized expert knowledge. To address this
variability, practitioners often aggregate the predictions of multiple models.
However, conventional aggregation strategies typically rely on static
combination rules and lack instance-level adaptability. In this work, we
propose an in-context ensemble framework for tabular prediction that leverages
large language models (LLMs) to perform dynamic, instance-specific integration
of external model predictions. Without access to raw tabular features or
semantic information, our method constructs a context around each test instance
using its nearest neighbors and the predictions from a pool of external models.
Within this enriched context, we introduce Chain of Tabular Thoughts (CoT$^2$),
a prompting strategy that guides LLMs through multi-step, interpretable
reasoning, making still further progress toward expert-level decision-making.
Experimental results show that our method outperforms well-tuned baselines and
standard ensemble techniques across a wide range of tabular datasets.

</details>

### [329] [Learnware of Language Models: Specialized Small Language Models Can Do Big](https://arxiv.org/abs/2505.13425)
*Zhi-Hao Tan,Zi-Chen Zhao,Hao-Yu Shi,Xin-Yu Zhang,Peng Tan,Yang Yu,Zhi-Hua Zhou*

Main category: cs.LG

TLDR: 论文探讨了将learnware范式应用于语言模型，通过重用专业小语言模型（SLMs）提升任务性能，并在金融、医疗和数学领域验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在专业场景中的数据稀缺、隐私和高计算成本问题，同时探索learnware范式在语言模型中的应用潜力。

Method: 构建包含约100个8B参数SLMs的learnware系统，每个模型附带描述其能力的规范，用户可通过规范选择合适模型而无需共享数据。

Result: 系统在金融领域任务中优于多个大型模型（如Qwen1.5-110B），在医疗任务中超越Flan-PaLM-540B。

Conclusion: learnware范式为语言模型提供了一种高效、隐私保护的解决方案，尤其在专业领域表现突出。

Abstract: The learnware paradigm offers a novel approach to machine learning by
enabling users to reuse a set of well-trained models for tasks beyond the
models' original purposes. It eliminates the need to build models from scratch,
instead relying on specifications (representations of a model's capabilities)
to identify and leverage the most suitable models for new tasks. While
learnware has proven effective in many scenarios, its application to language
models has remained largely unexplored. At the same time, large language models
(LLMs) have demonstrated remarkable universal question-answering abilities, yet
they face challenges in specialized scenarios due to data scarcity, privacy
concerns, and high computational costs, thus more and more specialized small
language models (SLMs) are being trained for specific domains. To address these
limitations systematically, the learnware paradigm provides a promising
solution by enabling maximum utilization of specialized SLMs, and allowing
users to identify and reuse them in a collaborative and privacy-preserving
manner.
  This paper presents a preliminary attempt to apply the learnware paradigm to
language models. We simulated a learnware system comprising approximately 100
learnwares of specialized SLMs with 8B parameters, fine-tuned across finance,
healthcare, and mathematics domains. Each learnware contains an SLM and a
specification, which enables users to identify the most relevant models without
exposing their own data. Experimental results demonstrate promising
performance: by selecting one suitable learnware for each task-specific
inference, the system outperforms the base SLMs on all benchmarks. Compared to
LLMs, the system outperforms Qwen1.5-110B, Qwen2.5-72B, and
Llama3.1-70B-Instruct by at least 14% in finance domain tasks, and surpasses
Flan-PaLM-540B (ranked 7th on the Open Medical LLM Leaderboard) in medical
domain tasks.

</details>

### [330] [Synthetic-Powered Predictive Inference](https://arxiv.org/abs/2505.13432)
*Meshi Bashari,Roy Maor Lotan,Yonghoon Lee,Edgar Dobriban,Yaniv Romano*

Main category: cs.LG

TLDR: SPPI利用合成数据改进样本效率，通过分数传输器校准非一致性分数，显著提升预测集的紧密度和信息量。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测在数据稀缺时预测集信息量不足，SPPI通过引入合成数据解决这一问题。

Method: 采用分数传输器校准真实数据与合成数据的非一致性分数，并整合到校准过程中。

Result: 实验证明SPPI在图像分类和表格回归中显著提升预测效率。

Conclusion: SPPI在数据稀缺时提供更紧密度和信息量的预测集，且无需假设数据分布。

Abstract: Conformal prediction is a framework for predictive inference with a
distribution-free, finite-sample guarantee. However, it tends to provide
uninformative prediction sets when calibration data are scarce. This paper
introduces Synthetic-powered predictive inference (SPPI), a novel framework
that incorporates synthetic data -- e.g., from a generative model -- to improve
sample efficiency. At the core of our method is a score transporter: an
empirical quantile mapping that aligns nonconformity scores from trusted, real
data with those from synthetic data. By carefully integrating the score
transporter into the calibration process, SPPI provably achieves finite-sample
coverage guarantees without making any assumptions about the real and synthetic
data distributions. When the score distributions are well aligned, SPPI yields
substantially tighter and more informative prediction sets than standard
conformal prediction. Experiments on image classification and tabular
regression demonstrate notable improvements in predictive efficiency in
data-scarce settings.

</details>

### [331] [Unlocking Non-Invasive Brain-to-Text](https://arxiv.org/abs/2505.13446)
*Dulhan Jayalath,Gilad Landau,Oiwi Parker Jones*

Main category: cs.LG

TLDR: 首次提出非侵入式脑到文本（B2T）系统，显著超越基线性能，提升BLEU分数1.4-2.6倍，通过LLM重评分、词汇扩展和跨数据集扩展实现。


<details>
  <summary>Details</summary>
Motivation: 解决非侵入式脑机接口（BCI）在瘫痪患者沟通恢复中的技术瓶颈，避免手术需求。

Method: 1. 结合LLM重评分改进单词分类模型；2. 引入预测填充处理词汇外单词；3. 跨数据集扩展模型规模。

Result: BLEU分数提升1.4-2.6倍，模型准确性提高2.1-2.3倍。

Conclusion: 研究成果为实用非侵入式B2T系统扫除关键障碍，揭示了数据质量和词汇量的重要性。

Abstract: Despite major advances in surgical brain-to-text (B2T), i.e. transcribing
speech from invasive brain recordings, non-invasive alternatives have yet to
surpass even chance on standard metrics. This remains a barrier to building a
non-invasive brain-computer interface (BCI) capable of restoring communication
in paralysed individuals without surgery. Here, we present the first
non-invasive B2T result that significantly exceeds these critical baselines,
raising BLEU by $1.4\mathrm{-}2.6\times$ over prior work. This result is driven
by three contributions: (1) we extend recent word-classification models with
LLM-based rescoring, transforming single-word predictors into closed-vocabulary
B2T systems; (2) we introduce a predictive in-filling approach to handle
out-of-vocabulary (OOV) words, substantially expanding the effective
vocabulary; and (3) we demonstrate, for the first time, how to scale
non-invasive B2T models across datasets, unlocking deep learning at scale and
improving accuracy by $2.1\mathrm{-}2.3\times$. Through these contributions, we
offer new insights into the roles of data quality and vocabulary size.
Together, our results remove a major obstacle to realising practical
non-invasive B2T systems.

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [332] [A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism](https://arxiv.org/abs/2505.11533)
*Jinqiang Wang,Huansheng Ning,Tao Zhu,Jianguo Ding*

Main category: cs.CL

TLDR: 论文提出SynPT方法，利用LLM驱动的用户和助手代理模拟对话，生成高质量数据集SynPT-Dialog，以解决旅游领域LLM在挖掘用户隐式意图和主动引导方面的不足。


<details>
  <summary>Details</summary>
Motivation: 旅游领域LLM难以从模糊查询中挖掘隐式意图且缺乏主动引导能力，现有方法存在领域适应性差、数据分布不均等问题。

Method: 构建LLM驱动的用户和助手代理，基于旅游网站种子数据模拟对话，生成包含显式推理的训练数据集SynPT-Dialog。

Result: 实验表明SynPT优于现有方法，并分析了关键超参数及实际应用案例。

Conclusion: SynPT有效解决了旅游领域LLM的隐式意图挖掘问题，具备实际应用潜力，且支持英语场景扩展。

Abstract: In the tourism domain, Large Language Models (LLMs) often struggle to mine
implicit user intentions from tourists' ambiguous inquiries and lack the
capacity to proactively guide users toward clarifying their needs. A critical
bottleneck is the scarcity of high-quality training datasets that facilitate
proactive questioning and implicit intention mining. While recent advances
leverage LLM-driven data synthesis to generate such datasets and transfer
specialized knowledge to downstream models, existing approaches suffer from
several shortcomings: (1) lack of adaptation to the tourism domain, (2) skewed
distributions of detail levels in initial inquiries, (3) contextual redundancy
in the implicit intention mining module, and (4) lack of explicit thinking
about tourists' emotions and intention values. Therefore, we propose SynPT (A
Data Synthesis Method Driven by LLMs for Proactive Mining of Implicit User
Intentions in the Tourism), which constructs an LLM-driven user agent and
assistant agent to simulate dialogues based on seed data collected from Chinese
tourism websites. This approach addresses the aforementioned limitations and
generates SynPT-Dialog, a training dataset containing explicit reasoning. The
dataset is utilized to fine-tune a general LLM, enabling it to proactively mine
implicit user intentions. Experimental evaluations, conducted from both human
and LLM perspectives, demonstrate the superiority of SynPT compared to existing
methods. Furthermore, we analyze key hyperparameters and present case studies
to illustrate the practical applicability of our method, including discussions
on its adaptability to English-language scenarios. All code and data are
publicly available.

</details>

### [333] [AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification](https://arxiv.org/abs/2505.11550)
*Harika Abburi,Sanmitra Bhattacharya,Edward Bowen,Nirmala Pudota*

Main category: cs.CL

TLDR: 该论文研究了如何检测AI生成的文本并识别其来源模型，提出了两种神经网络架构，分别在区分人类与AI生成文本（任务A）和识别生成模型（任务B）中取得良好表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）可能被滥用，如生成假新闻或垃圾邮件，因此需要准确检测AI生成文本及其来源模型。

Method: 为任务A和任务B分别设计了优化的和简化的神经网络架构。

Result: 任务A的优化模型F1得分为0.994，排名第五；任务B的简化模型F1得分为0.627，同样排名第五。

Conclusion: 提出的方法在检测和识别AI生成文本方面表现优异，有助于LLMs的负责任使用。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
generating text that closely resembles human writing across a wide range of
styles and genres. However, such capabilities are prone to potential misuse,
such as fake news generation, spam email creation, and misuse in academic
assignments. As a result, accurate detection of AI-generated text and
identification of the model that generated it are crucial for maintaining the
responsible use of LLMs. In this work, we addressed two sub-tasks put forward
by the Defactify workshop under AI-Generated Text Detection shared task at the
Association for the Advancement of Artificial Intelligence (AAAI 2025): Task A
involved distinguishing between human-authored or AI-generated text, while Task
B focused on attributing text to its originating language model. For each task,
we proposed two neural architectures: an optimized model and a simpler variant.
For Task A, the optimized neural architecture achieved fifth place with $F1$
score of 0.994, and for Task B, the simpler neural architecture also ranked
fifth place with $F1$ score of 0.627.

</details>

### [334] [Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks](https://arxiv.org/abs/2505.11556)
*Yuxuan Li,Aoi Naito,Hirokazu Shirado*

Main category: cs.CL

TLDR: 论文引入社会心理学中的Hidden Profile范式，用于评估多智能体LLM系统的集体推理能力，发现其表现与人类群体相似但存在行为差异。


<details>
  <summary>Details</summary>
Motivation: 缺乏理论基础的基准来系统评估多智能体LLM系统的集体推理失败问题。

Method: 采用Hidden Profile范式，设计包含九个任务的基准，测试GPT-4.1等六种LLM模型。

Result: 多智能体系统在所有模型中的准确性均低于拥有完整信息的单个智能体，且表现出与人类群体相似但更敏感于社会期望的行为差异。

Conclusion: 研究提供了一个可复现的评估框架，并揭示了多智能体LLM系统中的合作-矛盾权衡，为未来研究人工集体智能和人机交互提供了方向。

Abstract: Multi-agent systems built on large language models (LLMs) promise enhanced
problem-solving through distributed information integration, but also risk
replicating collective reasoning failures observed in human groups. Yet, no
theory-grounded benchmark exists to systematically evaluate such failures. In
this paper, we introduce the Hidden Profile paradigm from social psychology as
a diagnostic testbed for multi-agent LLM systems. By distributing critical
information asymmetrically across agents, the paradigm reveals how inter-agent
dynamics support or hinder collective reasoning. We first formalize the
paradigm for multi-agent decision-making under distributed knowledge and
instantiate it as a benchmark with nine tasks spanning diverse scenarios,
including adaptations from prior human studies. We then conduct experiments
with GPT-4.1 and five other leading LLMs, including reasoning-enhanced
variants, showing that multi-agent systems across all models fail to match the
accuracy of single agents given complete information. While agents' collective
performance is broadly comparable to that of human groups, nuanced behavioral
differences emerge, such as increased sensitivity to social desirability.
Finally, we demonstrate the paradigm's diagnostic utility by exploring a
cooperation-contradiction trade-off in multi-agent LLM systems. We find that
while cooperative agents are prone to over-coordination in collective settings,
increased contradiction impairs group convergence. This work contributes a
reproducible framework for evaluating multi-agent LLM systems and motivates
future research on artificial collective intelligence and human-AI interaction.

</details>

### [335] [Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models](https://arxiv.org/abs/2505.11604)
*Kyudan Jung,Hojun Cho,Jooyeol Yun,Jaehyeok Jang,Jagul Choo*

Main category: cs.CL

TLDR: 论文提出了一种基于大语言模型（LLM）的代理Talk-to-Your-Slides，用于直接编辑PowerPoint幻灯片，解决了现有研究中忽视的编辑任务问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注幻灯片生成，而忽略了编辑现有幻灯片的繁琐任务。

Method: 采用两级方法：高层处理由LLM代理解析指令并制定编辑计划，低层执行通过Python脚本直接操作PowerPoint对象。

Result: 实验表明，Talk-to-Your-Slides在执行成功率、指令忠实度和编辑效率上显著优于基线方法。

Conclusion: 提出的方法实现了更灵活、上下文感知的幻灯片编辑，并通过TSBench数据集验证了其有效性。

Abstract: Existing research on large language models (LLMs) for PowerPoint
predominantly focuses on slide generation, overlooking the common yet tedious
task of editing existing slides. We introduce Talk-to-Your-Slides, an
LLM-powered agent that directly edits slides within active PowerPoint sessions
through COM communication. Our system employs a two-level approach: (1)
high-level processing where an LLM agent interprets instructions and formulates
editing plans, and (2) low-level execution where Python scripts directly
manipulate PowerPoint objects. Unlike previous methods relying on predefined
operations, our approach enables more flexible and contextually-aware editing.
To facilitate evaluation, we present TSBench, a human-annotated dataset of 379
diverse editing instructions with corresponding slide variations. Experimental
results demonstrate that Talk-to-Your-Slides significantly outperforms baseline
methods in execution success rate, instruction fidelity, and editing
efficiency. Our code and benchmark are available at
https://anonymous.4open.science/r/talk-to-your-slides/

</details>

### [336] [MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models](https://arxiv.org/abs/2505.11613)
*Xiaomin Li,Mingye Gao,Yuexing Hao,Taoran Li,Guangya Wan,Zihan Wang,Yijun Wang*

Main category: cs.CL

TLDR: MedGUIDE是一个评估大型语言模型（LLMs）是否能遵循临床指南的新基准，发现即使是专业领域的LLMs在结构化指南任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在临床决策中是否可靠遵循结构化指南，以确保其在真实医疗环境中的安全性。

Method: 基于55个NCCN决策树构建MedGUIDE，通过两阶段质量筛选生成7,747个高质量样本，评估25种LLMs。

Result: 发现即使是专业领域的LLMs在结构化指南任务中表现不佳，提示其在实际临床环境中的局限性。

Conclusion: MedGUIDE对评估LLMs在临床环境中的安全性至关重要，未来需改进模型以更好地遵循指南。

Abstract: Clinical guidelines, typically structured as decision trees, are central to
evidence-based medical practice and critical for ensuring safe and accurate
diagnostic decision-making. However, it remains unclear whether Large Language
Models (LLMs) can reliably follow such structured protocols. In this work, we
introduce MedGUIDE, a new benchmark for evaluating LLMs on their ability to
make guideline-consistent clinical decisions. MedGUIDE is constructed from 55
curated NCCN decision trees across 17 cancer types and uses clinical scenarios
generated by LLMs to create a large pool of multiple-choice diagnostic
questions. We apply a two-stage quality selection process, combining
expert-labeled reward models and LLM-as-a-judge ensembles across ten clinical
and linguistic criteria, to select 7,747 high-quality samples. We evaluate 25
LLMs spanning general-purpose, open-source, and medically specialized models,
and find that even domain-specific LLMs often underperform on tasks requiring
structured guideline adherence. We also test whether performance can be
improved via in-context guideline inclusion or continued pretraining. Our
findings underscore the importance of MedGUIDE in assessing whether LLMs can
operate safely within the procedural frameworks expected in real-world clinical
settings.

</details>

### [337] [Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations](https://arxiv.org/abs/2505.11615)
*Jian-Qiao Zhu,Haijiang Yan,Thomas L. Griffiths*

Main category: cs.CL

TLDR: 通过构建“导向向量”编辑Transformer残差流，无需重新训练即可定向改变大语言模型行为。提出一种系统化识别导向向量的方法，并通过风险偏好实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索如何在不重新训练或微调模型的情况下，通过修改内部神经激活定向影响大语言模型的行为。

Method: 提出一种原则性方法，通过行为方法（如马尔可夫链蒙特卡洛）与神经表征对齐，识别导向向量。

Result: 实验表明，导向向量能可靠地按目标行为调节模型输出。

Conclusion: 导向向量是一种有效且定向的方法，可用于改变大语言模型行为。

Abstract: Changing the behavior of large language models (LLMs) can be as
straightforward as editing the Transformer's residual streams using
appropriately constructed "steering vectors." These modifications to internal
neural activations, a form of representation engineering, offer an effective
and targeted means of influencing model behavior without retraining or
fine-tuning the model. But how can such steering vectors be systematically
identified? We propose a principled approach for uncovering steering vectors by
aligning latent representations elicited through behavioral methods
(specifically, Markov chain Monte Carlo with LLMs) with their neural
counterparts. To evaluate this approach, we focus on extracting latent risk
preferences from LLMs and steering their risk-related outputs using the aligned
representations as steering vectors. We show that the resulting steering
vectors successfully and reliably modulate LLM outputs in line with the
targeted behavior.

</details>

### [338] [THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering](https://arxiv.org/abs/2505.11626)
*Udita Patel,Rutu Mulkar,Jay Roberts,Cibi Chakravarthy Senthilkumar,Sujay Gandhi,Xiaofei Zheng,Naumaan Nayyar,Rafael Castrillo*

Main category: cs.CL

TLDR: THELMA是一个无参考框架，用于评估RAG QA应用，包含六个相互关联的指标，支持开发者优化端到端流程。


<details>
  <summary>Details</summary>
Motivation: 为RAG QA应用提供无需标注数据或参考响应的全面评估方法。

Method: 设计六个相互依赖的指标，用于细粒度评估RAG QA应用。

Result: THELMA能识别需要改进的RAG组件，帮助优化QA应用。

Conclusion: THELMA为RAG QA应用提供了一种有效的评估和改进工具。

Abstract: We propose THELMA (Task Based Holistic Evaluation of Large Language Model
Applications), a reference free framework for RAG (Retrieval Augmented
generation) based question answering (QA) applications. THELMA consist of six
interdependent metrics specifically designed for holistic, fine grained
evaluation of RAG QA applications. THELMA framework helps developers and
application owners evaluate, monitor and improve end to end RAG QA pipelines
without requiring labelled sources or reference responses.We also present our
findings on the interplay of the proposed THELMA metrics, which can be
interpreted to identify the specific RAG component needing improvement in QA
applications.

</details>

### [339] [Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation](https://arxiv.org/abs/2505.11628)
*Berkcan Kapusuzoglu,Supriyo Chakraborty,Chia-Hsuan Lee,Sambit Sahu*

Main category: cs.CL

TLDR: 提出了一种名为CGD的多阶段框架，通过整合教师模型的解释性批评和精炼响应，解决SFT中的模仿问题。


<details>
  <summary>Details</summary>
Motivation: 解决监督微调（SFT）中模型仅模仿正确响应而不理解背后原理的问题。

Method: 使用教师模型生成的解释性批评和精炼响应，训练学生模型学习模仿内容和原因。

Result: 在数学（AMC23 +17.5%）和语言理解任务（MMLU-Pro +6.3%）上显著提升，并缓解了格式漂移问题。

Conclusion: CGD通过结合批评和精炼响应，有效提升了模型的理解能力和性能。

Abstract: Supervised fine-tuning (SFT) using expert demonstrations often suffer from
the imitation problem, where the model learns to reproduce the correct
responses without \emph{understanding} the underlying rationale. To address
this limitation, we propose \textsc{Critique-Guided Distillation (CGD)}, a
novel multi-stage framework that integrates teacher model generated
\emph{explanatory critiques} and \emph{refined responses} into the SFT process.
A student model is then trained to map the triplet of prompt, teacher critique,
and its own initial response to the corresponding refined teacher response,
thereby learning both \emph{what} to imitate and \emph{why}. Using
entropy-based analysis, we show that \textsc{CGD} reduces refinement
uncertainty and can be interpreted as a Bayesian posterior update. We perform
extensive empirical evaluation of \textsc{CGD}, on variety of benchmark tasks,
and demonstrate significant gains on both math (AMC23 +17.5%) and language
understanding tasks (MMLU-Pro +6.3%), while successfully mitigating the format
drift issues observed in previous critique fine-tuning (CFT) techniques.

</details>

### [340] [Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models? Evidence from a Four-Stage Curriculum on GPT-2](https://arxiv.org/abs/2505.11643)
*Xiang Fu*

Main category: cs.CL

TLDR: 开发顺序课程显著提升小语言模型的推理透明度和样本效率。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过分阶段课程提升小语言模型的推理能力，减少训练成本。

Method: 训练124M参数的GPT-2模型Cognivolve，采用四阶段课程（从词汇匹配到多步符号推理），无需任务微调。

Result: Cognivolve在优化步骤减半的情况下达到目标精度，激活更多梯度显著推理头，注意力熵更高。

Conclusion: 课程顺序是关键，但最终答案成功率仍低30%，需进一步研究混合阶段微调和探测扩展。

Abstract: We demonstrate that a developmentally ordered curriculum markedly improves
reasoning transparency and sample-efficiency in small language models (SLMs).
Concretely, we train Cognivolve, a 124 M-parameter GPT-2 model, on a four-stage
syllabus that ascends from lexical matching to multi-step symbolic inference
and then evaluate it without any task-specific fine-tuning. Cognivolve reaches
target accuracy in half the optimization steps of a single-phase baseline,
activates an order-of-magnitude more gradient-salient reasoning heads, and
shifts those heads toward deeper layers, yielding higher-entropy attention that
balances local and long-range context. The same curriculum applied out of order
or with optimizer resets fails to reproduce these gains, confirming that
progression--not extra compute--drives the effect. We also identify open
challenges: final-answer success still lags a conventional run by about 30%,
and our saliency probe under-detects verbal-knowledge heads in the hardest
stage, suggesting directions for mixed-stage fine-tuning and probe expansion.

</details>

### [341] [Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks](https://arxiv.org/abs/2505.11665)
*Shubham Vatsal,Harsh Dubey,Aditi Singh*

Main category: cs.CL

TLDR: 本文综述了多语言提示工程技术，旨在提升大语言模型（LLMs）在多语言环境中的表现，无需大量参数调整。


<details>
  <summary>Details</summary>
Motivation: 确保LLMs在多种语言中的有效性面临挑战，多语言提示工程成为关键解决方案。

Method: 通过结构化自然语言提示，提取LLMs在不同语言中的知识，并分类39种提示技术应用于30种多语言NLP任务。

Result: 分析了250种语言的数据集，总结了语言家族和资源水平的见解，并探讨了潜在的最先进方法。

Conclusion: 多语言提示工程为更广泛用户提供了利用LLMs的途径，未来研究可进一步优化这些技术。

Abstract: Large language models (LLMs) have demonstrated impressive performance across
a wide range of Natural Language Processing (NLP) tasks. However, ensuring
their effectiveness across multiple languages presents unique challenges.
Multilingual prompt engineering has emerged as a key approach to enhance LLMs'
capabilities in diverse linguistic settings without requiring extensive
parameter re-training or fine-tuning. With growing interest in multilingual
prompt engineering over the past two to three years, researchers have explored
various strategies to improve LLMs' performance across languages and NLP tasks.
By crafting structured natural language prompts, researchers have successfully
extracted knowledge from LLMs across different languages, making these
techniques an accessible pathway for a broader audience, including those
without deep expertise in machine learning, to harness the capabilities of
LLMs. In this paper, we survey and categorize different multilingual prompting
techniques based on the NLP tasks they address across a diverse set of datasets
that collectively span around 250 languages. We further highlight the LLMs
employed, present a taxonomy of approaches and discuss potential
state-of-the-art (SoTA) methods for specific multilingual datasets.
Additionally, we derive a range of insights across language families and
resource levels (high-resource vs. low-resource), including analyses such as
the distribution of NLP tasks by language resource type and the frequency of
prompting methods across different language families. Our survey reviews 36
research papers covering 39 prompting techniques applied to 30 multilingual NLP
tasks, with the majority of these studies published in the last two years.

</details>

### [342] [Ambiguity Resolution in Text-to-Structured Data Mapping](https://arxiv.org/abs/2505.11679)
*Zhibo Hu,Chen Wang,Yanfeng Shu,Hye-Young Paik,Liming Zhu*

Main category: cs.CL

TLDR: 本文提出了一种新方法，通过分析潜在空间中模糊文本的表示差异来识别模糊性，并利用稀疏自编码器（SAE）的梯度路径核设计新的距离度量，以改进LLM在模糊代理工具调用中的性能。


<details>
  <summary>Details</summary>
Motivation: 自然语言中的模糊性是LLM实现文本到结构化数据映射的主要障碍，现有方法（如ReACT框架或监督微调）未能有效解决。本文旨在通过潜在空间表示差异识别模糊性。

Method: 通过分析模糊问题与其解释的关系，利用稀疏自编码器（SAE）的梯度路径核设计新的距离度量，识别模糊性问题。

Result: 提出了一种新框架，通过预测缺失概念来改进LLM在模糊代理工具调用中的性能。

Conclusion: 新方法通过潜在空间分析和新的距离度量，有效识别模糊性并提升LLM在模糊任务中的表现。

Abstract: Ambiguity in natural language is a significant obstacle for achieving
accurate text to structured data mapping through large language models (LLMs),
which affects the performance of tasks such as mapping text to agentic tool
calling and text-to-SQL queries. Existing methods of ambiguity handling either
exploit ReACT framework to produce the correct mapping through trial and error,
or supervised fine tuning to guide models to produce a biased mapping to
improve certain tasks. In this paper, we adopt a different approach that
characterizes the representation difference of ambiguous text in the latent
space and leverage the difference to identify ambiguity before mapping them to
structured data. To detect ambiguity of a sentence, we focused on the
relationship between ambiguous questions and their interpretations and what
cause the LLM ignore multiple interpretations. Different to the distance
calculated by dense embedding vectors, we utilize the observation that
ambiguity is caused by concept missing in latent space of LLM to design a new
distance measurement, computed through the path kernel by the integral of
gradient values for each concepts from sparse-autoencoder (SAE) under each
state. We identify patterns to distinguish ambiguous questions with this
measurement. Based on our observation, We propose a new framework to improve
the performance of LLMs on ambiguous agentic tool calling through missing
concepts prediction.

</details>

### [343] [Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation](https://arxiv.org/abs/2505.11683)
*Susanna Rücker,Alan Akbik*

Main category: cs.CL

TLDR: 论文提出了一种基于双编码器的实体消歧模型VerbalizED，通过优化损失函数、相似性度量、标签表达格式和负采样策略，实现了文档级的实体消歧，并在AIDA-Yago数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究双编码器在实体消歧任务中的关键设计决策，以提升模型性能。

Method: 提出VerbalizED模型，结合上下文标签表达和高效硬负采样，并探索迭代预测变体。

Result: 在AIDA-Yago数据集上验证了模型的有效性，并在ZELDA基准测试中达到新的最优性能。

Conclusion: 优化设计决策（如标签表达和负采样）显著提升了双编码器在实体消歧任务中的性能。

Abstract: Entity disambiguation (ED) is the task of linking mentions in text to
corresponding entries in a knowledge base. Dual Encoders address this by
embedding mentions and label candidates in a shared embedding space and
applying a similarity metric to predict the correct label. In this work, we
focus on evaluating key design decisions for Dual Encoder-based ED, such as its
loss function, similarity metric, label verbalization format, and negative
sampling strategy. We present the resulting model VerbalizED, a document-level
Dual Encoder model that includes contextual label verbalizations and efficient
hard negative sampling. Additionally, we explore an iterative prediction
variant that aims to improve the disambiguation of challenging data points.
Comprehensive experiments on AIDA-Yago validate the effectiveness of our
approach, offering insights into impactful design choices that result in a new
State-of-the-Art system on the ZELDA benchmark.

</details>

### [344] [Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions](https://arxiv.org/abs/2505.11690)
*Sukairaj Hafiz Imam,Babangida Sani,Dawit Ketema Gete,Bedru Yimam Ahamed,Ibrahim Said Ahmad,Idris Abdulmumin,Seid Muhie Yimam,Muhammad Yahuza Bello,Shamsuddeen Hassan Muhammad*

Main category: cs.CL

TLDR: 论文探讨了非洲低资源语言在自动语音识别（ASR）技术中的挑战，提出了数据稀缺、语言复杂性等障碍，并提出了社区驱动数据收集、轻量模型等解决方案。


<details>
  <summary>Details</summary>
Motivation: 非洲低资源语言在ASR研究中代表性不足，阻碍了技术应用，需解决这些挑战以促进技术发展。

Method: 分析了数据稀缺、语言复杂性等问题，并提出了社区驱动数据收集、自监督学习等方法。

Result: 试点项目展示了定制化解决方案的可行性，如基于语素的建模和特定领域ASR应用。

Conclusion: 跨学科合作和持续投资是解决非洲语言ASR挑战的关键，需推动伦理、高效的ASR系统。

Abstract: Automatic Speech Recognition (ASR) technologies have transformed
human-computer interaction; however, low-resource languages in Africa remain
significantly underrepresented in both research and practical applications.
This study investigates the major challenges hindering the development of ASR
systems for these languages, which include data scarcity, linguistic
complexity, limited computational resources, acoustic variability, and ethical
concerns surrounding bias and privacy. The primary goal is to critically
analyze these barriers and identify practical, inclusive strategies to advance
ASR technologies within the African context. Recent advances and case studies
emphasize promising strategies such as community-driven data collection,
self-supervised and multilingual learning, lightweight model architectures, and
techniques that prioritize privacy. Evidence from pilot projects involving
various African languages showcases the feasibility and impact of customized
solutions, which encompass morpheme-based modeling and domain-specific ASR
applications in sectors like healthcare and education. The findings highlight
the importance of interdisciplinary collaboration and sustained investment to
tackle the distinct linguistic and infrastructural challenges faced by the
continent. This study offers a progressive roadmap for creating ethical,
efficient, and inclusive ASR systems that not only safeguard linguistic
diversity but also improve digital accessibility and promote socioeconomic
participation for speakers of African languages.

</details>

### [345] [Hierarchical Bracketing Encodings for Dependency Parsing as Tagging](https://arxiv.org/abs/2505.11693)
*Ana Ezquerro,David Vilares,Anssi Yli-Jyrä,Carlos Gómez-Rodríguez*

Main category: cs.CL

TLDR: 提出了一种基于分层括号的序列标注依赖解析编码家族，证明现有4位投影编码是其子集但非最优，并推导出最优分层括号编码，支持更紧凑的非投影性表示。


<details>
  <summary>Details</summary>
Motivation: 改进现有依赖解析编码的效率，减少标签数量并支持非投影性。

Method: 基于分层括号概念，推导最优编码，最小化符号使用，支持非投影性。

Result: 最优编码仅需12个标签（原16个），非投影性表示更紧凑，在多树库上表现竞争性。

Conclusion: 新编码在标签数量和紧凑性上优于现有方法，且保持高准确性。

Abstract: We present a family of encodings for sequence labeling dependency parsing,
based on the concept of hierarchical bracketing. We prove that the existing
4-bit projective encoding belongs to this family, but it is suboptimal in the
number of labels used to encode a tree. We derive an optimal hierarchical
bracketing, which minimizes the number of symbols used and encodes projective
trees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also
extend optimal hierarchical bracketing to support arbitrary non-projectivity in
a more compact way than previous encodings. Our new encodings yield competitive
accuracy on a diverse set of treebanks.

</details>

### [346] [Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures](https://arxiv.org/abs/2505.11726)
*Shun Inadumi,Nobuhiro Ueda,Koichiro Yoshino*

Main category: cs.CL

TLDR: 该论文提出了一种统一文本和多模态参考解析的框架，通过将提及嵌入映射到对象嵌入，并基于相似性选择提及或对象。实验表明，学习文本参考解析（如共指消解和谓词-论元结构分析）对多模态参考解析性能有积极影响。


<details>
  <summary>Details</summary>
Motivation: 解决对话中因代词和省略引起的歧义，需要整合文本和多模态参考解析。

Method: 提出框架，统一文本和多模态参考解析，通过嵌入映射和相似性选择。

Result: 模型在代词短语接地任务中表现优于MDETR和GLIP，文本参考关系的引入增强了提及与对象之间的置信度。

Conclusion: 结合文本参考关系可以减少视觉接地对话中的歧义，提升性能。

Abstract: Multimodal reference resolution, including phrase grounding, aims to
understand the semantic relations between mentions and real-world objects.
Phrase grounding between images and their captions is a well-established task.
In contrast, for real-world applications, it is essential to integrate textual
and multimodal reference resolution to unravel the reference relations within
dialogue, especially in handling ambiguities caused by pronouns and ellipses.
This paper presents a framework that unifies textual and multimodal reference
resolution by mapping mention embeddings to object embeddings and selecting
mentions or objects based on their similarity. Our experiments show that
learning textual reference resolution, such as coreference resolution and
predicate-argument structure analysis, positively affects performance in
multimodal reference resolution. In particular, our model with coreference
resolution performs better in pronoun phrase grounding than representative
models for this task, MDETR and GLIP. Our qualitative analysis demonstrates
that incorporating textual reference relations strengthens the confidence
scores between mentions, including pronouns and predicates, and objects, which
can reduce the ambiguities that arise in visually grounded dialogues.

</details>

### [347] [MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports](https://arxiv.org/abs/2505.11733)
*Kevin Wu,Eric Wu,Rahul Thapa,Kevin Wei,Angela Zhang,Arvind Suresh,Jacqueline J. Tao,Min Woo Sun,Alejandro Lozano,James Zou*

Main category: cs.CL

TLDR: 研究者提出了MedCaseReasoning数据集，用于评估LLMs在临床诊断推理中的表现，发现现有模型在诊断准确性和推理质量上存在不足，但通过微调可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学基准（如MedQA和MMLU）仅关注最终答案的准确性，忽视了临床推理过程的质量和忠实性。

Method: 引入MedCaseReasoning数据集，包含14,489个诊断问答案例，每个案例配有详细的临床推理说明。评估了现有LLMs的表现，并展示了微调的效果。

Result: 现有模型表现不佳（如DeepSeek-R1诊断准确率仅48%），但微调后诊断准确性和推理召回率分别平均提升29%和41%。

Conclusion: MedCaseReasoning填补了医学评估的空白，微调LLMs可显著改善其临床诊断和推理能力。

Abstract: Doctors and patients alike increasingly use Large Language Models (LLMs) to
diagnose clinical cases. However, unlike domains such as math or coding, where
correctness can be objectively defined by the final answer, medical diagnosis
requires both the outcome and the reasoning process to be accurate. Currently,
widely used medical benchmarks like MedQA and MMLU assess only accuracy in the
final answer, overlooking the quality and faithfulness of the clinical
reasoning process. To address this limitation, we introduce MedCaseReasoning,
the first open-access dataset for evaluating LLMs on their ability to align
with clinician-authored diagnostic reasoning. The dataset includes 14,489
diagnostic question-and-answer cases, each paired with detailed reasoning
statements derived from open-access medical case reports. We evaluate
state-of-the-art reasoning LLMs on MedCaseReasoning and find significant
shortcomings in their diagnoses and reasoning: for instance, the top-performing
open-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy
and mentions only 64% of the clinician reasoning statements (recall). However,
we demonstrate that fine-tuning LLMs on the reasoning traces derived from
MedCaseReasoning significantly improves diagnostic accuracy and clinical
reasoning recall by an average relative gain of 29% and 41%, respectively. The
open-source dataset, code, and models are available at
https://github.com/kevinwu23/Stanford-MedCaseReasoning.

</details>

### [348] [ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training](https://arxiv.org/abs/2505.11739)
*Feijiang Han,Xiaodong Yu,Jianheng Tang,Lyle Ungar*

Main category: cs.CL

TLDR: 论文提出了一种无需训练的方法ZeroTuning，通过调整初始空语义令牌的注意力来优化大语言模型（LLM）性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖辅助机制识别任务相关令牌，可能引入偏差且适用性有限。研究发现初始空语义令牌是未充分探索的控制点。

Method: 通过理论分析发现初始令牌的注意力调整会影响后续令牌的注意力分布。提出ZeroTuning方法，针对该令牌进行头部特异性注意力调整。

Result: ZeroTuning在文本分类、多选和多轮对话任务中显著提升性能（如Llama-3.1-8B分类任务提升11.71%），且具有鲁棒性。

Conclusion: 初始空语义令牌是LLM中一个被忽视的控制点，ZeroTuning为推理时优化和模型可解释性提供了新视角。

Abstract: Recently, training-free methods for improving large language models (LLMs)
have attracted growing interest, with token-level attention tuning emerging as
a promising and interpretable direction. However, existing methods typically
rely on auxiliary mechanisms to identify important or irrelevant task-specific
tokens, introducing potential bias and limiting applicability. In this paper,
we uncover a surprising and elegant alternative: the semantically empty initial
token is a powerful and underexplored control point for optimizing model
behavior. Through theoretical analysis, we show that tuning the initial token's
attention sharpens or flattens the attention distribution over subsequent
tokens, and its role as an attention sink amplifies this effect. Empirically,
we find that: (1) tuning its attention improves LLM performance more
effectively than tuning other task-specific tokens; (2) the effect follows a
consistent trend across layers, with earlier layers having greater impact, but
varies across attention heads, with different heads showing distinct
preferences in how they attend to this token. Based on these findings, we
propose ZeroTuning, a training-free approach that improves LLM performance by
applying head-specific attention adjustments to this special token. Despite
tuning only one token, ZeroTuning achieves higher performance on text
classification, multiple-choice, and multi-turn conversation tasks across
models such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves
Llama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its
multi-turn score from 7.804 to 7.966. The method is also robust to limited
resources, few-shot settings, long contexts, quantization, decoding strategies,
and prompt variations. Our work sheds light on a previously overlooked control
point in LLMs, offering new insights into both inference-time tuning and model
interpretability.

</details>

### [349] [Token Masking Improves Transformer-Based Text Classification](https://arxiv.org/abs/2505.11746)
*Xianglong Xu,John Bowen,Rojin Taheri*

Main category: cs.CL

TLDR: 论文提出了一种名为“token masking regularization”的方法，通过随机替换输入令牌为[MASK]来增强Transformer模型的性能。实验表明，该方法在语言识别和情感分析任务中优于标准正则化技术。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过掩码输入令牌进一步提升Transformer模型在文本分类任务中的性能。

Method: 提出“token masking regularization”，随机以概率p替换输入令牌为[MASK]，通过引入随机扰动实现隐式梯度平均。

Result: 在多种模型（mBERT、Qwen2.5-0.5B、TinyLlama-1.1B）上，语言识别和情感分析任务中表现优于标准正则化技术，p=0.1为通用最优值。

Conclusion: 该方法通过减少过拟合和隐式集成效应提升了模型性能，p=0.1是推荐的通用掩码率。

Abstract: While transformer-based models achieve strong performance on text
classification, we explore whether masking input tokens can further enhance
their effectiveness. We propose token masking regularization, a simple yet
theoretically motivated method that randomly replaces input tokens with a
special [MASK] token at probability p. This introduces stochastic perturbations
during training, leading to implicit gradient averaging that encourages the
model to capture deeper inter-token dependencies. Experiments on language
identification and sentiment analysis -- across diverse models (mBERT,
Qwen2.5-0.5B, TinyLlama-1.1B) -- show consistent improvements over standard
regularization techniques. We identify task-specific optimal masking rates,
with p = 0.1 as a strong general default. We attribute the gains to two key
effects: (1) input perturbation reduces overfitting, and (2) gradient-level
smoothing acts as implicit ensembling.

</details>

### [350] [Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation](https://arxiv.org/abs/2505.11754)
*Wenyu Huang,Pavlos Vougiouklis,Mirella Lapata,Jeff Z. Pan*

Main category: cs.CL

TLDR: 论文研究了语言模型在多跳问答任务中的表现，发现编码器-解码器模型优于仅解码器模型，文档顺序影响性能，修改因果掩码可提升效果。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在多跳问答任务中的表现，特别是因果掩码对推理能力的限制。

Method: 通过排列检索文档的顺序，分析模型性能，并修改因果掩码以增强双向注意力。

Result: 编码器-解码器模型表现更优，文档顺序与推理链一致时性能最佳，修改掩码可提升仅解码器模型效果。

Conclusion: 优化文档顺序和注意力机制可显著提升语言模型在多跳问答任务中的表现。

Abstract: Multi-hop Question Answering (MHQA) adds layers of complexity to question
answering, making it more challenging. When Language Models (LMs) are prompted
with multiple search results, they are tasked not only with retrieving relevant
information but also employing multi-hop reasoning across the information
sources. Although LMs perform well on traditional question-answering tasks, the
causal mask can hinder their capacity to reason across complex contexts. In
this paper, we explore how LMs respond to multi-hop questions by permuting
search results (retrieved documents) under various configurations. Our study
reveals interesting findings as follows: 1) Encoder-decoder models, such as the
ones in the Flan-T5 family, generally outperform causal decoder-only LMs in
MHQA tasks, despite being significantly smaller in size; 2) altering the order
of gold documents reveals distinct trends in both Flan T5 models and fine-tuned
decoder-only models, with optimal performance observed when the document order
aligns with the reasoning chain order; 3) enhancing causal decoder-only models
with bi-directional attention by modifying the causal mask can effectively
boost their end performance. In addition to the above, we conduct a thorough
investigation of the distribution of LM attention weights in the context of
MHQA. Our experiments reveal that attention weights tend to peak at higher
values when the resulting answer is correct. We leverage this finding to
heuristically improve LMs' performance on this task. Our code is publicly
available at https://github.com/hwy9855/MultiHopQA-Reasoning.

</details>

### [351] [Towards Universal Semantics With Large Language Models](https://arxiv.org/abs/2505.11764)
*Raymond Baartmans,Matthew Raffel,Rahul Vikram,Aiden Deringer,Lizhong Chen*

Main category: cs.CL

TLDR: 论文探讨了利用大语言模型（LLMs）生成自然语义元语言（NSM）解释的方法，提出自动评估工具和专用数据集，1B和8B模型表现优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 传统NSM解释生成过程缓慢且手动，LLMs的应用有望提升效率并推动语义分析和翻译等任务。

Method: 提出自动评估方法、专用数据集，并微调1B和8B模型用于生成NSM解释。

Result: 1B和8B模型在生成准确、可跨语言翻译的解释上优于GPT-4o。

Conclusion: LLMs在通用语义表示方面迈出重要一步，为语义分析和翻译等应用开辟新可能。

Abstract: The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a
universal set of semantic primes: simple, primitive word-meanings that have
been shown to exist in most, if not all, languages of the world. According to
this framework, any word, regardless of complexity, can be paraphrased using
these primes, revealing a clear and universally translatable meaning. These
paraphrases, known as explications, can offer valuable applications for many
natural language processing (NLP) tasks, but producing them has traditionally
been a slow, manual process. In this work, we present the first study of using
large language models (LLMs) to generate NSM explications. We introduce
automatic evaluation methods, a tailored dataset for training and evaluation,
and fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in
producing accurate, cross-translatable explications, marking a significant step
toward universal semantic representation with LLMs and opening up new
possibilities for applications in semantic analysis, translation, and beyond.

</details>

### [352] [Retrospex: Language Agent Meets Offline Reinforcement Learning Critic](https://arxiv.org/abs/2505.11807)
*Yufei Xiang,Yiqun Shen,Yeqin Zhang,Cam-Tu Nguyen*

Main category: cs.CL

TLDR: Retrospex是一种新的基于LLM的智能体框架，通过结合LLM的动作概率和RL Critic估计的动作值，利用离线回顾过程优化过去经验，提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体框架未能充分利用过去经验进行改进，Retrospex旨在通过深入分析经验来解决这一问题。

Method: Retrospex不直接将经验集成到LLM上下文中，而是结合LLM动作概率和RL Critic的动作值估计，并通过动态动作重评分机制调整经验值的重要性。

Result: 在ScienceWorld、ALFWorld和Webshop环境中，Retrospex表现优于现有基线方法。

Conclusion: Retrospex通过有效利用过去经验和动态调整机制，显著提升了LLM智能体的性能。

Abstract: Large Language Models (LLMs) possess extensive knowledge and commonsense
reasoning capabilities, making them valuable for creating powerful agents.
However, existing LLM agent frameworks have not fully utilized past experiences
for improvement. This work introduces a new LLM-based agent framework called
Retrospex, which addresses this challenge by analyzing past experiences in
depth. Unlike previous approaches, Retrospex does not directly integrate
experiences into the LLM's context. Instead, it combines the LLM's action
likelihood with action values estimated by a Reinforcement Learning (RL)
Critic, which is trained on past experiences through an offline
''retrospection'' process. Additionally, Retrospex employs a dynamic action
rescoring mechanism that increases the importance of experience-based values
for tasks that require more interaction with the environment. We evaluate
Retrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its
advantages over strong, contemporary baselines.

</details>

### [353] [Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model](https://arxiv.org/abs/2505.11810)
*Shen Li,Renfen Hu,Lijun Wang*

Main category: cs.CL

TLDR: AI Taiyan是一个专为古典中文设计的语言模型，通过合理的设计和训练，仅用18亿参数就在古典中文任务中超越了通用模型和传统领域模型。


<details>
  <summary>Details</summary>
Motivation: 通用语言模型在古典中文领域表现不佳，需要专门设计的模型来提升效果。

Method: 开发AI Taiyan模型，结合合理的设计、数据处理、基础训练和微调。

Result: 在标点、典故识别、词义解释和古今翻译等任务中表现优异，接近或超越人类水平。

Conclusion: AI Taiyan为高效构建领域专用语言模型提供了参考，并展示了在古籍整理等领域的应用潜力。

Abstract: General-purpose large language models demonstrate notable capabilities in
language comprehension and generation, achieving results that are comparable
to, or even surpass, human performance in many language information processing
tasks. Nevertheless, when general models are applied to some specific domains,
e.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and
fine-tuning open-source foundational models similarly struggles to adequately
incorporate domain-specific knowledge. To address this challenge, this study
developed a large language model, AI Taiyan, specifically designed for
understanding and generating Classical Chinese. Experiments show that with a
reasonable model design, data processing, foundational training, and
fine-tuning, satisfactory results can be achieved with only 1.8 billion
parameters. In key tasks related to Classical Chinese information processing
such as punctuation, identification of allusions, explanation of word meanings,
and translation between ancient and modern Chinese, this model exhibits a clear
advantage over both general-purpose large models and domain-specific
traditional models, achieving levels close to or surpassing human baselines.
This research provides a reference for the efficient construction of
specialized domain-specific large language models. Furthermore, the paper
discusses the application of this model in fields such as the collation of
ancient texts, dictionary editing, and language research, combined with case
studies.

</details>

### [354] [BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering](https://arxiv.org/abs/2505.11811)
*Taolin Zhang,Dongyang Li,Qizhou Chen,Chengyu Wang,Xiaofeng He*

Main category: cs.CL

TLDR: 论文提出BELLE框架，通过多智能体辩论和分层设计，针对多跳QA问题类型选择最优方法，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法未针对多跳QA问题类型优化，BELLE旨在通过问题类型与方法的对应关系提升性能。

Method: BELLE框架分为两层：第一层多智能体辩论生成执行计划，第二层快慢辩论者监控观点变化。

Result: BELLE在多个数据集上显著优于基线，且在复杂场景中更具成本效益。

Conclusion: BELLE通过分层多智能体设计，有效解决了多跳QA问题类型与方法的匹配问题，性能优越。

Abstract: Multi-hop question answering (QA) involves finding multiple relevant passages
and performing step-by-step reasoning to answer complex questions. Previous
works on multi-hop QA employ specific methods from different modeling
perspectives based on large language models (LLMs), regardless of the question
types. In this paper, we first conduct an in-depth analysis of public multi-hop
QA benchmarks, dividing the questions into four types and evaluating five types
of cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step,
Iterative-step, Sub-step, and Adaptive-step. We find that different types of
multi-hop questions have varying degrees of sensitivity to different types of
methods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to
address multi-hop QA by specifically focusing on the correspondence between
question types and methods, where each type of method is regarded as an
''operator'' by prompting LLMs differently. The first level of BELLE includes
multiple agents that debate to obtain an executive plan of combined
''operators'' to address the multi-hop QA task comprehensively. During the
debate, in addition to the basic roles of affirmative debater, negative
debater, and judge, at the second level, we further leverage fast and slow
debaters to monitor whether changes in viewpoints are reasonable. Extensive
experiments demonstrate that BELLE significantly outperforms strong baselines
in various datasets. Additionally, the model consumption of BELLE is higher
cost-effectiveness than that of single models in more complex multi-hop QA
scenarios.

</details>

### [355] [Chain-of-Model Learning for Language Model](https://arxiv.org/abs/2505.11820)
*Kaitao Song,Xiaohua Wang,Xu Tan,Huiqiang Jiang,Chengruidong Zhang,Yongliang Shen,Cen LU,Zihao Li,Zifan Song,Caihua Shan,Yansen Wang,Kan Ren,Xiaoqing Zheng,Tao Qin,Yuqing Yang,Dongsheng Li,Lili Qiu*

Main category: cs.CL

TLDR: 提出了一种名为Chain-of-Model (CoM)的新学习范式，通过将因果关系引入隐藏状态链式结构，提升了模型训练效率和部署灵活性。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统模型在训练和推理中缺乏灵活性和效率的问题，提出了一种链式隐藏状态表示方法。

Method: 引入Chain-of-Representation (CoR)概念，将隐藏状态分解为多个子表示链，并通过Chain-of-Language-Model (CoLM)实现Transformer架构的链式扩展。

Result: 实验表明，CoLM家族在性能上与标准Transformer相当，同时提供了更高的灵活性和效率。

Conclusion: CoM为语言模型的构建提供了一种新的高效灵活的方法。

Abstract: In this paper, we propose a novel learning paradigm, termed Chain-of-Model
(CoM), which incorporates the causal relationship into the hidden states of
each layer as a chain style, thereby introducing great scaling efficiency in
model training and inference flexibility in deployment. We introduce the
concept of Chain-of-Representation (CoR), which formulates the hidden states at
each layer as a combination of multiple sub-representations (i.e., chains) at
the hidden dimension level. In each layer, each chain from the output
representations can only view all of its preceding chains in the input
representations. Consequently, the model built upon CoM framework can
progressively scale up the model size by increasing the chains based on the
previous models (i.e., chains), and offer multiple sub-models at varying sizes
for elastic inference by using different chain numbers. Based on this
principle, we devise Chain-of-Language-Model (CoLM), which incorporates the
idea of CoM into each layer of Transformer architecture. Based on CoLM, we
further introduce CoLM-Air by introducing a KV sharing mechanism, that computes
all keys and values within the first chain and then shares across all chains.
This design demonstrates additional extensibility, such as enabling seamless LM
switching, prefilling acceleration and so on. Experimental results demonstrate
our CoLM family can achieve comparable performance to the standard Transformer,
while simultaneously enabling greater flexiblity, such as progressive scaling
to improve training efficiency and offer multiple varying model sizes for
elastic inference, paving a a new way toward building language models. Our code
will be released in the future at: https://github.com/microsoft/CoLM.

</details>

### [356] [Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.11827)
*Yansong Ning,Wei Li,Jun Fang,Naiqiang Tan,Hao Liu*

Main category: cs.CL

TLDR: 论文提出了一种名为Long⊗Short的高效推理框架，通过区分长思维和短思维，优化LLMs的推理效率，减少80%的token长度。


<details>
  <summary>Details</summary>
Motivation: 现有方法对所有思维进行均等压缩，限制了推理的简洁性和有效性。

Method: 通过自动分块和蒙特卡洛模拟评估思维重要性，设计联合度量标准，并利用长思维和短思维LLMs协作推理，结合多轮强化学习优化协作。

Result: 在多个基准测试中，Qwen2.5-7B和Llama3.1-8B性能接近DeepSeek-R1-Distill模型，同时token长度减少80%以上。

Conclusion: Long⊗Short框架显著提升了推理效率，同时保持了性能。

Abstract: Compressing long chain-of-thought (CoT) from large language models (LLMs) is
an emerging strategy to improve the reasoning efficiency of LLMs. Despite its
promising benefits, existing studies equally compress all thoughts within a
long CoT, hindering more concise and effective reasoning. To this end, we first
investigate the importance of different thoughts by examining their
effectiveness and efficiency in contributing to reasoning through automatic
long CoT chunking and Monte Carlo rollouts. Building upon the insights, we
propose a theoretically bounded metric to jointly measure the effectiveness and
efficiency of different thoughts. We then propose Long$\otimes$Short, an
efficient reasoning framework that enables two LLMs to collaboratively solve
the problem: a long-thought LLM for more effectively generating important
thoughts, while a short-thought LLM for efficiently generating remaining
thoughts. Specifically, we begin by synthesizing a small amount of cold-start
data to fine-tune LLMs for long-thought and short-thought reasoning styles,
respectively. Furthermore, we propose a synergizing-oriented multi-turn
reinforcement learning, focusing on the model self-evolution and collaboration
between long-thought and short-thought LLMs. Experimental results show that our
method enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance
compared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while
reducing token length by over 80% across the MATH500, AIME24/25, AMC23, and
GPQA Diamond benchmarks. Our data and code are available at
https://github.com/yasNing/Long-otimes-Short/.

</details>

### [357] [Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks](https://arxiv.org/abs/2505.11829)
*Chenlu Wang,Weimin Lyu,Ritwik Banerjee*

Main category: cs.CL

TLDR: ClaD是一种新的训练范式，通过类蒸馏技术从多样背景中提取小目标类，结合Mahalanobis距离和可解释决策算法，在性别歧视、隐喻和讽刺检测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 检测异常或微妙语言（如性别歧视、隐喻、讽刺）对提升在线社交话语的安全性和清晰度至关重要，但现有分类器计算成本高且数据需求大。

Method: 提出ClaD训练范式，包括基于Mahalanobis距离的损失函数和优化的可解释决策算法。

Result: 在三个基准任务中，ClaD优于基线模型，且使用更小语言模型和更少参数时性能接近大型语言模型。

Conclusion: ClaD是高效工具，适用于从异构背景中提取小目标类的实用语言理解任务。

Abstract: Detecting deviant language such as sexism, or nuanced language such as
metaphors or sarcasm, is crucial for enhancing the safety, clarity, and
interpretation of online social discourse. While existing classifiers deliver
strong results on these tasks, they often come with significant computational
cost and high data demands. In this work, we propose \textbf{Cla}ss
\textbf{D}istillation (ClaD), a novel training paradigm that targets the core
challenge: distilling a small, well-defined target class from a highly diverse
and heterogeneous background. ClaD integrates two key innovations: (i) a loss
function informed by the structural properties of class distributions, based on
Mahalanobis distance, and (ii) an interpretable decision algorithm optimized
for class separation. Across three benchmark detection tasks -- sexism,
metaphor, and sarcasm -- ClaD outperforms competitive baselines, and even with
smaller language models and orders of magnitude fewer parameters, achieves
performance comparable to several large language models (LLMs). These results
demonstrate ClaD as an efficient tool for pragmatic language understanding
tasks that require gleaning a small target class from a larger heterogeneous
background.

</details>

### [358] [Multilingual Collaborative Defense for Large Language Models](https://arxiv.org/abs/2505.11835)
*Hongliang Li,Jinan Xu,Gengping Cui,Changhao Guan,Fengran Mo,Kaiyu Huang*

Main category: cs.CL

TLDR: 论文提出了一种多语言协作防御（MCD）方法，通过优化连续软安全提示，增强大型语言模型（LLMs）在多语言场景下的安全性。


<details>
  <summary>Details</summary>
Motivation: 研究发现LLMs在罕见或低资源语言中容易被绕过安全措施，亟需提升多语言安全性。

Method: 提出MCD方法，自动优化安全提示，提升多语言保护能力，并构建多语言基准数据集进行评估。

Result: MCD在多语言保护性能、泛化能力和语言对齐方面优于现有方法。

Conclusion: MCD为LLMs的多语言安全提供了一种有效解决方案，并展示了强大的语言迁移能力。

Abstract: The robustness and security of large language models (LLMs) has become a
prominent research area. One notable vulnerability is the ability to bypass LLM
safeguards by translating harmful queries into rare or underrepresented
languages, a simple yet effective method of "jailbreaking" these models.
Despite the growing concern, there has been limited research addressing the
safeguarding of LLMs in multilingual scenarios, highlighting an urgent need to
enhance multilingual safety. In this work, we investigate the correlation
between various attack features across different languages and propose
Multilingual Collaborative Defense (MCD), a novel learning method that
optimizes a continuous, soft safety prompt automatically to facilitate
multilingual safeguarding of LLMs. The MCD approach offers three advantages:
First, it effectively improves safeguarding performance across multiple
languages. Second, MCD maintains strong generalization capabilities while
minimizing false refusal rates. Third, MCD mitigates the language safety
misalignment caused by imbalances in LLM training corpora. To evaluate the
effectiveness of MCD, we manually construct multilingual versions of commonly
used jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess
various safeguarding methods. Additionally, we introduce these datasets in
underrepresented (zero-shot) languages to verify the language transferability
of MCD. The results demonstrate that MCD outperforms existing approaches in
safeguarding against multilingual jailbreak attempts while also exhibiting
strong language transfer capabilities. Our code is available at
https://github.com/HLiang-Lee/MCD.

</details>

### [359] [When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research](https://arxiv.org/abs/2505.11855)
*Guijin Son,Jiwoo Hong,Honglu Fan,Heejeong Nam,Hyunwoo Ko,Seungwon Lim,Jinyeop Song,Jinha Choi,Gonçalo Paulo,Youngjae Yu,Stella Biderman*

Main category: cs.CL

TLDR: 论文探讨了利用大语言模型（LLMs）作为科学论文学术验证工具的可行性，发现当前模型的性能远未达到可靠水平。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在学术验证中的补充应用，以提升科学论文的准确性。

Method: 引入SPOT数据集，评估LLMs在检测论文错误中的表现。

Result: 当前LLMs的召回率和精确度均较低（最高21.1%和6.1%），且可靠性不足。

Conclusion: 现有LLMs能力与可靠学术验证需求之间存在显著差距。

Abstract: Recent advances in large language models (LLMs) have fueled the vision of
automated scientific discovery, often called AI Co-Scientists. To date, prior
work casts these systems as generative co-authors responsible for crafting
hypotheses, synthesizing code, or drafting manuscripts. In this work, we
explore a complementary application: using LLMs as verifiers to automate the
\textbf{academic verification of scientific manuscripts}. To that end, we
introduce SPOT, a dataset of 83 published papers paired with 91 errors
significant enough to prompt errata or retraction, cross-validated with actual
authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find
that none surpasses 21.1\% recall or 6.1\% precision (o3 achieves the best
scores, with all others near zero). Furthermore, confidence estimates are
uniformly low, and across eight independent runs, models rarely rediscover the
same errors, undermining their reliability. Finally, qualitative analysis with
domain experts reveals that even the strongest models make mistakes resembling
student-level misconceptions derived from misunderstandings. These findings
highlight the substantial gap between current LLM capabilities and the
requirements for dependable AI-assisted academic verification.

</details>

### [360] [NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization](https://arxiv.org/abs/2505.11876)
*Yanbo Dai,Zhenlan Ji,Zongjie Li,Shuai Wang*

Main category: cs.CL

TLDR: NAMET是一种噪声感知的模型编辑方法，通过简单修改MEMIT，显著提升大规模编辑场景下的效果。


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法在大规模编辑场景下效果不佳，主要由于知识项之间的嵌入冲突。

Method: 提出NAMET方法，通过在记忆提取中引入噪声，仅需一行修改MEMIT。

Result: 在六个LLM和三个数据集上的实验表明，NAMET在编辑数千条事实时优于现有方法。

Conclusion: NAMET通过噪声感知机制有效解决了大规模编辑中的嵌入冲突问题。

Abstract: Model editing techniques are essential for efficiently updating knowledge in
large language models (LLMs). However, the effectiveness of existing approaches
degrades in massive editing scenarios, particularly when evaluated with
practical metrics or in context-rich settings. We attribute these failures to
embedding collisions among knowledge items, which undermine editing reliability
at scale. To address this, we propose NAMET (Noise-aware Model Editing in
Transformers), a simple yet effective method that introduces noise during
memory extraction via a one-line modification to MEMIT. Extensive experiments
across six LLMs and three datasets demonstrate that NAMET consistently
outperforms existing methods when editing thousands of facts.

</details>

### [361] [AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation](https://arxiv.org/abs/2505.11887)
*Xiechi Zhang,Zetian Ouyang,Linlin Wang,Gerard de Melo,Zhu Cao,Xiaoling Wang,Ya Zhang,Yanfeng Wang,Liang He*

Main category: cs.CL

TLDR: AutoMedEval是一个开源的自动评估模型，专为评估医学领域大型语言模型的问答能力而设计，旨在减少对人类评估的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标（如F1和ROUGE）忽视医学术语的重要性，而人类评估成本高且可能不准确。现有基于LLM的评估方法在医学领域适用性有限。

Method: 提出AutoMedEval，采用分层训练方法，包括课程指令调整和迭代知识内省机制，以有限指令数据获得专业医学评估能力。

Result: AutoMedEval在人类评估中表现出优于其他基线模型的性能，与人类判断相关性更高。

Conclusion: AutoMedEval为医学LLM评估提供了一种高效、准确的自动解决方案，显著减少了对人类评估的需求。

Abstract: With the proliferation of large language models (LLMs) in the medical domain,
there is increasing demand for improved evaluation techniques to assess their
capabilities. However, traditional metrics like F1 and ROUGE, which rely on
token overlaps to measure quality, significantly overlook the importance of
medical terminology. While human evaluation tends to be more reliable, it can
be very costly and may as well suffer from inaccuracies due to limits in human
expertise and motivation. Although there are some evaluation methods based on
LLMs, their usability in the medical field is limited due to their proprietary
nature or lack of expertise. To tackle these challenges, we present
AutoMedEval, an open-sourced automatic evaluation model with 13B parameters
specifically engineered to measure the question-answering proficiency of
medical LLMs. The overarching objective of AutoMedEval is to assess the quality
of responses produced by diverse models, aspiring to significantly reduce the
dependence on human evaluation. Specifically, we propose a hierarchical
training method involving curriculum instruction tuning and an iterative
knowledge introspection mechanism, enabling AutoMedEval to acquire professional
medical assessment capabilities with limited instructional data. Human
evaluations indicate that AutoMedEval surpasses other baselines in terms of
correlation with human judgments.

</details>

### [362] [Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents](https://arxiv.org/abs/2505.11891)
*Weikai Xu,Zhizheng Jiang,Yuxuan Liu,Wei Liu,Jian Luan,Yuanchun Li,Yunxin Liu,Bin Wang,Bo An*

Main category: cs.CL

TLDR: 论文提出Mobile-Bench-v2，一个更全面和真实的基准测试，用于评估移动代理在多路径任务、噪声环境和主动交互中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法稳定评估移动代理的动态环境适应能力、多解决方案任务处理能力，以及噪声和主动交互能力。

Method: 采用基于槽位的指令生成方法，构建包含多路径评估、噪声环境和模糊指令交互的Mobile-Bench-v2。

Result: 评估了多种移动代理框架（如AppAgent-v1、Mobile-Agent-v2等），展示了基准的全面性。

Conclusion: Mobile-Bench-v2为移动代理的评估提供了更真实和全面的测试环境。

Abstract: VLM-based mobile agents are increasingly popular due to their capabilities to
interact with smartphone GUIs and XML-structured texts and to complete daily
tasks. However, existing online benchmarks struggle with obtaining stable
reward signals due to dynamic environmental changes. Offline benchmarks
evaluate the agents through single-path trajectories, which stands in contrast
to the inherently multi-solution characteristics of GUI tasks. Additionally,
both types of benchmarks fail to assess whether mobile agents can handle noise
or engage in proactive interactions due to a lack of noisy apps or overly full
instructions during the evaluation process. To address these limitations, we
use a slot-based instruction generation method to construct a more realistic
and comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a
common task split, with offline multi-path evaluation to assess the agent's
ability to obtain step rewards during task execution. It contains a noisy split
based on pop-ups and ads apps, and a contaminated split named AITZ-Noise to
formulate a real noisy environment. Furthermore, an ambiguous instruction split
with preset Q\&A interactions is released to evaluate the agent's proactive
interaction capabilities. We conduct evaluations on these splits using the
single-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2,
as well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are
available at https://huggingface.co/datasets/xwk123/MobileBench-v2.

</details>

### [363] [RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving](https://arxiv.org/abs/2505.11893)
*Zepeng Ding,Dixuan Wang,Ziqin Luo,Guochao Jiang,Deqing Yang,Jiaqing Liang*

Main category: cs.CL

TLDR: 论文提出了一种基于强化学习的自适应规划框架（RLAP），通过将NLP任务建模为马尔可夫决策过程（MDP），结合轻量级Actor模型优化多步任务规划，提升LLM在NLP任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多步NLP任务中忽视了实例的语言特征，依赖LLM的固有规划能力，导致结果不理想。

Method: 将NLP任务建模为MDP，训练轻量级Actor模型通过强化学习估计自然语言序列的Q值，动态优化子任务顺序。

Result: 在三种不同类型的NLP任务和多个数据集上验证了RLAP的有效性和鲁棒性。

Conclusion: RLAP通过结合强化学习和LLM，显著提升了多步NLP任务的性能。

Abstract: Multi-step planning has been widely employed to enhance the performance of
large language models (LLMs) on downstream natural language processing (NLP)
tasks, which decomposes the original task into multiple subtasks and guide LLMs
to solve them sequentially without additional training. When addressing task
instances, existing methods either preset the order of steps or attempt
multiple paths at each step. However, these methods overlook instances'
linguistic features and rely on the intrinsic planning capabilities of LLMs to
evaluate intermediate feedback and then select subtasks, resulting in
suboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this
paper we propose a Reinforcement Learning enhanced Adaptive Planning framework
(RLAP). In our framework, we model an NLP task as a Markov decision process
(MDP) and employ an LLM directly into the environment. In particular, a
lightweight Actor model is trained to estimate Q-values for natural language
sequences consisting of states and actions through reinforcement learning.
Therefore, during sequential planning, the linguistic features of each sequence
in the MDP can be taken into account, and the Actor model interacts with the
LLM to determine the optimal order of subtasks for each task instance. We apply
RLAP on three different types of NLP tasks and conduct extensive experiments on
multiple datasets to verify RLAP's effectiveness and robustness.

</details>

### [364] [Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data](https://arxiv.org/abs/2505.11900)
*Philipp Christmann,Gerhard Weikum*

Main category: cs.CL

TLDR: ReQAP是一种通过递归分解生成可执行操作符树的方法，用于混合源问答，支持结构化与非结构化数据的无缝集成，并保持数据在用户设备上。


<details>
  <summary>Details</summary>
Motivation: 解决个人设备上混合数据（如文本和表格）的问答需求，同时确保数据隐私和小型化。

Method: 递归分解问题生成操作符树，设计操作符以集成结构化与非结构化数据。

Result: 提出ReQAP方法，并发布PerQA基准测试，涵盖多样化用户需求。

Conclusion: ReQAP为混合源问答提供高效、隐私保护的解决方案。

Abstract: Question answering over mixed sources, like text and tables, has been
advanced by verbalizing all contents and encoding it with a language model. A
prominent case of such heterogeneous data is personal information: user devices
log vast amounts of data every day, such as calendar entries, workout
statistics, shopping records, streaming history, and more. Information needs
range from simple look-ups to queries of analytical nature. The challenge is to
provide humans with convenient access with small footprint, so that all
personal data stays on the user devices. We present ReQAP, a novel method that
creates an executable operator tree for a given question, via recursive
decomposition. Operators are designed to enable seamless integration of
structured and unstructured sources, and the execution of the operator tree
yields a traceable answer. We further release the PerQA benchmark, with
persona-based data and questions, covering a diverse spectrum of realistic user
needs.

</details>

### [365] [ELITE: Embedding-Less retrieval with Iterative Text Exploration](https://arxiv.org/abs/2505.11908)
*Zhangyu Wang,Siyuan Gao,Rong Zhou,Hao Wang,Li Ning*

Main category: cs.CL

TLDR: 提出了一种无需嵌入的检索框架，利用LLMs的逻辑推理能力进行检索，显著减少存储和计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG系统因依赖嵌入检索而导致的语义相似但意图不匹配问题，以及图或层次结构带来的计算和存储开销。

Method: 通过迭代搜索空间优化和重要性度量，利用LLMs的逻辑推理能力进行检索，避免显式图构建。

Result: 在长上下文QA基准测试中表现优于基线，存储和运行时减少一个数量级。

Conclusion: 提出的方法在保持高性能的同时显著降低了资源消耗。

Abstract: Large Language Models (LLMs) have achieved impressive progress in natural
language processing, but their limited ability to retain long-term context
constrains performance on document-level or multi-turn tasks.
Retrieval-Augmented Generation (RAG) mitigates this by retrieving relevant
information from an external corpus. However, existing RAG systems often rely
on embedding-based retrieval trained on corpus-level semantic similarity, which
can lead to retrieving content that is semantically similar in form but
misaligned with the question's true intent. Furthermore, recent RAG variants
construct graph- or hierarchy-based structures to improve retrieval accuracy,
resulting in significant computation and storage overhead. In this paper, we
propose an embedding-free retrieval framework. Our method leverages the logical
inferencing ability of LLMs in retrieval using iterative search space
refinement guided by our novel importance measure and extend our retrieval
results with logically related information without explicit graph construction.
Experiments on long-context QA benchmarks, including NovelQA and Marathon, show
that our approach outperforms strong baselines while reducing storage and
runtime by over an order of magnitude.

</details>

### [366] [Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning](https://arxiv.org/abs/2505.11922)
*Yuheng Lu,ZiMeng Bai,Caixia Yuan,Huixing Jiang,Xiaojie Wang*

Main category: cs.CL

TLDR: 论文提出MISO方法，通过将顺序指令转换为并行子上下文指令，提升LLM在复杂指令跟随任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂指令跟随任务中可能忽略关键子上下文，影响监督微调（SFT）效果。

Method: 提出MISO框架，将顺序指令分解为并行子上下文，并引入混合上下文范式以优化SFT。

Result: 实验表明MISO在复杂指令跟随任务中表现优越，且训练效率高。

Conclusion: MISO是一种有效的LLM微调方法，适用于复杂指令场景。

Abstract: Large language models (LLMs) exhibit remarkable capabilities in handling
natural language tasks; however, they may struggle to consistently follow
complex instructions including those involve multiple constraints.
Post-training LLMs using supervised fine-tuning (SFT) is a standard approach to
improve their ability to follow instructions. In addressing complex instruction
following, existing efforts primarily focus on data-driven methods that
synthesize complex instruction-output pairs for SFT. However, insufficient
attention allocated to crucial sub-contexts may reduce the effectiveness of
SFT. In this work, we propose transforming sequentially structured input
instruction into multiple parallel instructions containing subcontexts. To
support processing this multi-input, we propose MISO (Multi-Input
Single-Output), an extension to currently dominant decoder-only
transformer-based LLMs. MISO introduces a mixture-of-contexts paradigm that
jointly considers the overall instruction-output alignment and the influence of
individual sub-contexts to enhance SFT effectiveness. We apply MISO fine-tuning
to complex instructionfollowing datasets and evaluate it with standard LLM
inference. Empirical results demonstrate the superiority of MISO as a
fine-tuning method for LLMs, both in terms of effectiveness in complex
instruction-following scenarios and its potential for training efficiency.

</details>

### [367] [An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts](https://arxiv.org/abs/2505.11924)
*Yu-Ting Lee,Hui-Ying Shih,Fu-Chieh Chang,Pei-Yuan Wu*

Main category: cs.CL

TLDR: 论文研究了语言模型通过提示诱导的隐状态变化实现自我校正的性能提升机制，提出了一种数学框架并验证了其在文本去毒任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型如何通过提示诱导的隐状态变化实现自我校正，并解释其性能提升的机制。

Method: 提出了一种数学框架，分析提示诱导的隐状态变化对输出分布的影响，并通过实验验证了其在文本去毒任务中的应用。

Result: 实验结果显示，提示诱导的隐状态变化显著区分了有毒和无毒词汇的嵌入向量，表明自我校正增强了模型对潜在概念的识别能力。

Conclusion: 研究揭示了自我校正的底层机制，为解释提示如何影响模型行为提供了理论支持。

Abstract: We provide an explanation for the performance gains of intrinsic
self-correction, a process where a language model iteratively refines its
outputs without external feedback. More precisely, we investigate how prompting
induces interpretable changes in hidden states and thus affects the output
distributions. We hypothesize that each prompt-induced shift lies in a linear
span of some linear representation vectors, naturally separating tokens based
on individual concept alignment. Building around this idea, we give a
mathematical formulation of self-correction and derive a concentration result
for output tokens based on alignment magnitudes. Our experiments on text
detoxification with zephyr-7b-sft reveal a substantial gap in the inner
products of the prompt-induced shifts and the unembeddings of the top-100 most
toxic tokens vs. those of the unembeddings of the bottom-100 least toxic
tokens, under toxic instructions. This suggests that self-correction prompts
enhance a language model's capability of latent concept recognition. Our
analysis offers insights into the underlying mechanism of self-correction by
characterizing how prompting works explainably. For reproducibility, our code
is available.

</details>

### [368] [Neuro-Symbolic Query Compiler](https://arxiv.org/abs/2505.11932)
*Yuyao Zhang,Zhicheng Dou,Xiaoxi Li,Jiajie Jin,Yongkang Wu,Zhonghua Li,Qi Ye,Ji-Rong Wen*

Main category: cs.CL

TLDR: QCompiler是一个神经符号框架，通过设计BNF语法和编译流程，提升RAG系统对复杂查询的意图识别能力。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统在资源受限和复杂查询（嵌套结构和依赖）下意图识别的挑战。

Method: 设计最小冗余的BNF语法$G[q]$，并开发包含查询表达式翻译器、词法语法解析器和递归下降处理器的QCompiler框架，将查询编译为AST。

Result: 子查询的原子性提高了文档检索和响应生成的精确性，显著增强了对复杂查询的处理能力。

Conclusion: QCompiler通过神经符号方法有效提升了RAG系统的复杂查询处理能力。

Abstract: Precise recognition of search intent in Retrieval-Augmented Generation (RAG)
systems remains a challenging goal, especially under resource constraints and
for complex queries with nested structures and dependencies. This paper
presents QCompiler, a neuro-symbolic framework inspired by linguistic grammar
rules and compiler design, to bridge this gap. It theoretically designs a
minimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize
complex queries. Unlike previous methods, this grammar maintains completeness
while minimizing redundancy. Based on this, QCompiler includes a Query
Expression Translator, a Lexical Syntax Parser, and a Recursive Descent
Processor to compile queries into Abstract Syntax Trees (ASTs) for execution.
The atomicity of the sub-queries in the leaf nodes ensures more precise
document retrieval and response generation, significantly improving the RAG
system's ability to address complex queries.

</details>

### [369] [ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing](https://arxiv.org/abs/2505.11935)
*Xuanle Zhao,Xuexin Liu,Haoyue Yang,Xianzhen Luo,Fanhu Zeng,Jianling Li,Qi Shi,Chi Chen*

Main category: cs.CL

TLDR: 提出了ChartEdit基准，用于评估多模态大语言模型（MLLMs）在图表编辑任务中的表现，发现现有模型在精确修改方面仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 图表编辑对MLLMs提出了更高要求，但现有评估方法不足，亟需全面评估框架。

Method: 构建ChartEdit基准，包含1,405条编辑指令和233个真实图表，评估10种主流MLLMs在代码和图表层面的表现。

Result: 大规模模型能生成部分匹配参考图像的代码，但精确编辑能力有限（SOTA模型得分59.96）；小规模模型表现更差。

Conclusion: 图表编辑任务仍具挑战性，需进一步研究提升模型能力。

Abstract: Although multimodal large language models (MLLMs) show promise in generating
chart rendering code, chart editing presents a greater challenge. This
difficulty stems from its nature as a labor-intensive task for humans that also
demands MLLMs to integrate chart understanding, complex reasoning, and precise
intent interpretation. While many MLLMs claim such editing capabilities,
current assessments typically rely on limited case studies rather than robust
evaluation methodologies, highlighting the urgent need for a comprehensive
evaluation framework. In this work, we propose ChartEdit, a new high-quality
benchmark designed for chart editing tasks. This benchmark comprises $1,405$
diverse editing instructions applied to $233$ real-world charts, with each
instruction-chart instance having been manually annotated and validated for
accuracy. Utilizing ChartEdit, we evaluate the performance of 10 mainstream
MLLMs across two types of experiments, assessing them at both the code and
chart levels. The results suggest that large-scale models can generate code to
produce images that partially match the reference images. However, their
ability to generate accurate edits according to the instructions remains
limited. The state-of-the-art (SOTA) model achieves a score of only $59.96$,
highlighting significant challenges in precise modification. In contrast,
small-scale models, including chart-domain models, struggle both with following
editing instructions and generating overall chart images, underscoring the need
for further development in this area. Code is available at
https://github.com/xxlllz/ChartEdit.

</details>

### [370] [Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning](https://arxiv.org/abs/2505.11958)
*Aswini Kumar Padhi,Anil Bandhakavi,Tanmoy Chakraborty*

Main category: cs.CL

TLDR: HiPPrO是一个新颖的两阶段框架，通过分层前缀学习和偏好优化，结合多属性条件生成更有效的反仇恨言论。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注单一属性的反仇恨言论生成，而多属性同时考虑可以产生更细致和有效的回应。

Method: HiPPrO采用分层前缀嵌入空间优化，并结合参考和无奖励偏好优化，生成更具建设性的反仇恨言论。

Result: HiPPrO在意图一致性上提升了约38%，在Rouge-1、Rouge-2和Rouge-L上分别提升了3%、2%和3%。

Conclusion: 多属性条件在提升反仇恨言论生成系统效能方面具有潜力。

Abstract: Counterspeech has proven to be a powerful tool to combat hate speech online.
Previous studies have focused on generating counterspeech conditioned only on
specific intents (single attributed). However, a holistic approach considering
multiple attributes simultaneously can yield more nuanced and effective
responses. Here, we introduce HiPPrO, Hierarchical Prefix learning with
Preference Optimization, a novel two-stage framework that utilizes the
effectiveness of attribute-specific prefix embedding spaces hierarchically
optimized during the counterspeech generation process in the first phase.
Thereafter, we incorporate both reference and reward-free preference
optimization to generate more constructive counterspeech. Furthermore, we
extend IntentCONANv2 by annotating all 13,973 counterspeech instances with
emotion labels by five annotators. HiPPrO leverages hierarchical prefix
optimization to integrate these dual attributes effectively. An extensive
evaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent
conformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L,
respectively, compared to several baseline models. Human evaluations further
substantiate the superiority of our approach, highlighting the enhanced
relevance and appropriateness of the generated counterspeech. This work
underscores the potential of multi-attribute conditioning in advancing the
efficacy of counterspeech generation systems.

</details>

### [371] [EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English](https://arxiv.org/abs/2505.11959)
*Md. Rafiul Biswas,Wajdi Zaghouani*

Main category: cs.CL

TLDR: 该研究提供了一个双语数据集（阿拉伯语23,456条，英语10,036条），标注了情感和希望言论，填补了多情感数据集的空白。数据集包含情感强度、复杂性和原因的详细标注，以及希望言论的分类和子类别。标注可靠性通过Fleiss' Kappa验证（0.75-0.85），基线模型的评估指标（micro-F1-Score=0.67）证明了标注的价值。


<details>
  <summary>Details</summary>
Motivation: 解决多情感（情感和希望言论）数据集稀缺的问题，促进自然语言处理在低资源语言中的发展。

Method: 构建双语数据集，进行详细的情感和希望言论标注，并使用Fleiss' Kappa验证标注可靠性，通过基线模型评估数据质量。

Result: 数据集标注可靠（Fleiss' Kappa 0.75-0.85），基线模型表现良好（micro-F1-Score=0.67）。

Conclusion: 该数据集为自然语言处理提供了宝贵资源，支持跨语言情感和希望言论分析。

Abstract: This research introduces a bilingual dataset comprising 23,456 entries for
Arabic and 10,036 entries for English, annotated for emotions and hope speech,
addressing the scarcity of multi-emotion (Emotion and hope) datasets. The
dataset provides comprehensive annotations capturing emotion intensity,
complexity, and causes, alongside detailed classifications and subcategories
for hope speech. To ensure annotation reliability, Fleiss' Kappa was employed,
revealing 0.75-0.85 agreement among annotators both for Arabic and English
language. The evaluation metrics (micro-F1-Score=0.67) obtained from the
baseline model (i.e., using a machine learning model) validate that the data
annotations are worthy. This dataset offers a valuable resource for advancing
natural language processing in underrepresented languages, fostering better
cross-linguistic analysis of emotions and hope speech.

</details>

### [372] [CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation](https://arxiv.org/abs/2505.11965)
*Xu Liu,Guanyi Chen*

Main category: cs.CL

TLDR: CCNU团队开发的系统在Mu-SHROOM共享任务中表现优异，利用多LLM并行标注幻觉，并在14种语言中取得显著成绩。


<details>
  <summary>Details</summary>
Motivation: 解决问答系统中幻觉识别的问题，特别是在多语言环境下。

Method: 采用多LLM并行标注，结合内外知识，使用开源LLM DeepSeek-V3。

Result: 在印地语数据中排名第一，其他七种语言进入前五。

Conclusion: 系统在多语言幻觉识别中表现优异，同时分享了失败经验和关键见解。

Abstract: We present the system developed by the Central China Normal University (CCNU)
team for the Mu-SHROOM shared task, which focuses on identifying hallucinations
in question-answering systems across 14 different languages. Our approach
leverages multiple Large Language Models (LLMs) with distinct areas of
expertise, employing them in parallel to annotate hallucinations, effectively
simulating a crowdsourcing annotation process. Furthermore, each LLM-based
annotator integrates both internal and external knowledge related to the input
during the annotation process. Using the open-source LLM DeepSeek-V3, our
system achieves the top ranking (\#1) for Hindi data and secures a Top-5
position in seven other languages. In this paper, we also discuss unsuccessful
approaches explored during our development process and share key insights
gained from participating in this shared task.

</details>

### [373] [An Annotated Corpus of Arabic Tweets for Hate Speech Analysis](https://arxiv.org/abs/2505.11969)
*Md. Rafiul Biswas,Wajdi Zaghouani*

Main category: cs.CL

TLDR: 该研究构建了一个阿拉伯语多标签仇恨言论数据集，包含10000条标注推文，并评估了基于Transformer的模型性能，其中AraBERTv2表现最佳。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语方言多样性使得仇恨内容识别具有挑战性，因此需要构建高质量数据集以支持相关研究。

Method: 收集并标注10000条阿拉伯语推文，计算标注者间一致性，并使用多种Transformer模型进行评估。

Result: 标注者间一致性为0.86（攻击性内容）和0.71（仇恨目标）。AraBERTv2的micro-F1得分为0.7865，准确率为0.786。

Conclusion: 该数据集为阿拉伯语仇恨言论研究提供了可靠资源，AraBERTv2在识别任务中表现最优。

Abstract: Identifying hate speech content in the Arabic language is challenging due to
the rich quality of dialectal variations. This study introduces a multilabel
hate speech dataset in the Arabic language. We have collected 10000 Arabic
tweets and annotated each tweet, whether it contains offensive content or not.
If a text contains offensive content, we further classify it into different
hate speech targets such as religion, gender, politics, ethnicity, origin, and
others. A text can contain either single or multiple targets. Multiple
annotators are involved in the data annotation task. We calculated the
inter-annotator agreement, which was reported to be 0.86 for offensive content
and 0.71 for multiple hate speech targets. Finally, we evaluated the data
annotation task by employing a different transformers-based model in which
AraBERTv2 outperformed with a micro-F1 score of 0.7865 and an accuracy of
0.786.

</details>

### [374] [Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation](https://arxiv.org/abs/2505.11995)
*Yuhao Wang,Ruiyang Ren,Yucheng Wang,Wayne Xin Zhao,Jing Liu,Hua Wu,Haifeng Wang*

Main category: cs.CL

TLDR: 本文系统研究了大型语言模型（LLM）在检索增强生成（RAG）中如何整合内部（参数化）和外部（检索）知识，揭示了知识利用的四个阶段，并提出了一种新方法（KAPE）来识别与知识源相关的神经元。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG在知识密集型任务中表现出潜力，但其在LLM中的知识利用机制尚未充分探索。

Method: 通过宏观知识流分析和微观模块功能研究，分解知识利用为四个阶段，并引入KAPE方法识别神经元。

Result: 研究发现段落相关性引导知识流，且多头部注意力和多层感知机在知识形成中起互补作用。

Conclusion: 这些发现为提升RAG的LLM的可解释性和可靠性奠定了基础，推动了知识密集型领域更稳健的生成解决方案。

Abstract: Considering the inherent limitations of parametric knowledge in large
language models (LLMs), retrieval-augmented generation (RAG) is widely employed
to expand their knowledge scope. Since RAG has shown promise in
knowledge-intensive tasks like open-domain question answering, its broader
application to complex tasks and intelligent assistants has further advanced
its utility. Despite this progress, the underlying knowledge utilization
mechanisms of LLM-based RAG remain underexplored. In this paper, we present a
systematic investigation of the intrinsic mechanisms by which LLMs integrate
internal (parametric) and external (retrieved) knowledge in RAG scenarios.
Specially, we employ knowledge stream analysis at the macroscopic level, and
investigate the function of individual modules at the microscopic level.
Drawing on knowledge streaming analyses, we decompose the knowledge utilization
process into four distinct stages within LLM layers: knowledge refinement,
knowledge elicitation, knowledge expression, and knowledge contestation. We
further demonstrate that the relevance of passages guides the streaming of
knowledge through these stages. At the module level, we introduce a new method,
knowledge activation probability entropy (KAPE) for neuron identification
associated with either internal or external knowledge. By selectively
deactivating these neurons, we achieve targeted shifts in the LLM's reliance on
one knowledge source over the other. Moreover, we discern complementary roles
for multi-head attention and multi-layer perceptron layers during knowledge
formation. These insights offer a foundation for improving interpretability and
reliability in retrieval-augmented LLMs, paving the way for more robust and
transparent generative solutions in knowledge-intensive domains.

</details>

### [375] [Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method](https://arxiv.org/abs/2505.12028)
*Yupei Ren,Xinyi Zhou,Ning Zhang,Shangqing Zhao,Man Lan,Xiaopeng Bai*

Main category: cs.CL

TLDR: 论文提出14种细粒度论证关系类型，以解决现有论证关系过于简单的问题，并在三个任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有论证关系过于基础，难以捕捉复杂论证结构，尤其是在真实场景中。

Method: 提出14种细粒度关系类型，从垂直和水平维度分析论证结构，并在三个任务（论证组件检测、关系预测、自动作文评分）中进行实验。

Result: 实验结果表明细粒度标注对论证写作质量评估至关重要，并支持多维论证分析。

Conclusion: 细粒度论证关系能更全面地理解论证结构，对提升论证分析质量具有重要意义。

Abstract: Argument mining has garnered increasing attention over the years, with the
recent advancement of Large Language Models (LLMs) further propelling this
trend. However, current argument relations remain relatively simplistic and
foundational, struggling to capture the full scope of argument information,
particularly when it comes to representing complex argument structures in
real-world scenarios. To address this limitation, we propose 14 fine-grained
relation types from both vertical and horizontal dimensions, thereby capturing
the intricate interplay between argument components for a thorough
understanding of argument structure. On this basis, we conducted extensive
experiments on three tasks: argument component detection, relation prediction,
and automated essay grading. Additionally, we explored the impact of writing
quality on argument component detection and relation prediction, as well as the
connections between discourse relations and argumentative features. The
findings highlight the importance of fine-grained argumentative annotations for
argumentative writing quality assessment and encourage multi-dimensional
argument analysis.

</details>

### [376] [MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities](https://arxiv.org/abs/2505.12043)
*Jingxue Chen,Qingkun Tang,Qianchun Lu,Siyuan Fang*

Main category: cs.CL

TLDR: 论文提出了一种名为MoL的新框架，通过解耦领域特定和通用语料的优化目标，解决了CPT方法中的两大问题：领域偏差数据影响通用语言能力，以及语料混合比例不当限制适应效果。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在通用任务中表现良好，但在领域特定应用中存在幻觉和准确性限制。CPT方法面临领域偏差数据损害通用语言能力和语料混合比例不当的问题。

Method: 提出MoL框架，使用交叉熵损失（CE）处理领域数据以确保知识获取，同时使用KL散度对齐通用语料训练与基础模型的通用能力。

Result: 实验表明，1:1的领域与通用语料比例能最优平衡训练和过拟合，无需大量调优。模型在Math-500和AIME25基准测试中分别取得27.9%和83.3%的性能提升。

Conclusion: MoL框架有效保留了通用能力并增强领域专长，避免了灾难性遗忘，显著优于传统CPT方法。

Abstract: Although LLMs perform well in general tasks, domain-specific applications
suffer from hallucinations and accuracy limitations. CPT approaches encounter
two key issues: (1) domain-biased data degrades general language skills, and
(2) improper corpus-mixture ratios limit effective adaptation. To address
these, we propose a novel framework, Mixture of Losses (MoL), which decouples
optimization objectives for domain-specific and general corpora. Specifically,
cross-entropy (CE) loss is applied to domain data to ensure knowledge
acquisition, while Kullback-Leibler (KL) divergence aligns general-corpus
training with the base model's foundational capabilities. This dual-loss
architecture preserves universal skills while enhancing domain expertise,
avoiding catastrophic forgetting. Empirically, we validate that a 1:1
domain-to-general corpus ratio optimally balances training and overfitting
without the need for extensive tuning or resource-intensive experiments.
Furthermore, our experiments demonstrate significant performance gains compared
to traditional CPT approaches, which often suffer from degradation in general
language capabilities; our model achieves 27.9% higher accuracy on the Math-500
benchmark in the non-think reasoning mode, and an impressive 83.3% improvement
on the challenging AIME25 subset in the think mode, underscoring the
effectiveness of our approach.

</details>

### [377] [ABoN: Adaptive Best-of-N Alignment](https://arxiv.org/abs/2505.12050)
*Vinod Raman,Hilal Asi,Satyen Kale*

Main category: cs.CL

TLDR: 提出了一种基于提示自适应的Best-of-N对齐策略，通过两阶段算法高效分配推理计算资源，显著优于均匀分配方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有测试时对齐方法（如Best-of-N采样）计算成本高的问题，尤其是在不同提示对齐难度不均的情况下。

Method: 开发两阶段算法：初始探索阶段用小预算估计奖励分布，第二阶段自适应分配剩余预算。

Result: 在AlpacaEval数据集上，12种LM/RM组合和50批提示的实验中，自适应策略优于均匀分配，且在大批量时表现更优。

Conclusion: 该方法简单实用，兼容性强，能高效提升语言模型对齐效果。

Abstract: Recent advances in test-time alignment methods, such as Best-of-N sampling,
offer a simple and effective way to steer language models (LMs) toward
preferred behaviors using reward models (RM). However, these approaches can be
computationally expensive, especially when applied uniformly across prompts
without accounting for differences in alignment difficulty. In this work, we
propose a prompt-adaptive strategy for Best-of-N alignment that allocates
inference-time compute more efficiently. Motivated by latency concerns, we
develop a two-stage algorithm: an initial exploratory phase estimates the
reward distribution for each prompt using a small exploration budget, and a
second stage adaptively allocates the remaining budget using these estimates.
Our method is simple, practical, and compatible with any LM/RM combination.
Empirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different
batches of prompts show that our adaptive strategy consistently outperforms the
uniform allocation with the same inference budget. Moreover, our experiments
show that our adaptive strategy remains competitive against uniform allocations
with 20% larger inference budgets and even improves in performance as the batch
size grows.

</details>

### [378] [GenderBench: Evaluation Suite for Gender Biases in LLMs](https://arxiv.org/abs/2505.12054)
*Matúš Pikuliak*

Main category: cs.CL

TLDR: GenderBench是一个用于评估LLMs中性别偏见的综合测试套件，包含14个探针，量化19种性别相关有害行为。开源发布，评估了12个LLMs，发现其存在刻板印象推理、性别代表不公等问题。


<details>
  <summary>Details</summary>
Motivation: 量化LLMs中的性别偏见，提高评估的可重复性和鲁棒性。

Method: 开发GenderBench套件，包含14个探针，评估12个LLMs的19种性别相关行为。

Result: LLMs在刻板印象推理、性别代表公平性及高风险场景（如招聘）中存在歧视行为。

Conclusion: GenderBench为评估LLMs性别偏见提供了标准化工具，揭示了LLMs的性别偏见问题。

Abstract: We present GenderBench -- a comprehensive evaluation suite designed to
measure gender biases in LLMs. GenderBench includes 14 probes that quantify 19
gender-related harmful behaviors exhibited by LLMs. We release GenderBench as
an open-source and extensible library to improve the reproducibility and
robustness of benchmarking across the field. We also publish our evaluation of
12 LLMs. Our measurements reveal consistent patterns in their behavior. We show
that LLMs struggle with stereotypical reasoning, equitable gender
representation in generated texts, and occasionally also with discriminatory
behavior in high-stakes scenarios, such as hiring.

</details>

### [379] [Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement](https://arxiv.org/abs/2505.12060)
*Peng Ding,Jun Kuang,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TLDR: 论文提出了一种名为SAGE的无训练防御策略，通过增强LLMs的安全判别能力与生成能力的一致性，有效抵御复杂的越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 发现LLMs在检测越狱提示时表现良好，但在直接处理这些输入时仍可能生成不安全响应，存在安全漏洞。

Method: 提出SAGE，包含判别分析模块和判别响应模块，通过灵活的安全判别指令提升防御能力。

Result: 实验表明SAGE在多种LLMs上平均防御成功率达99%，同时保持通用任务的实用性。

Conclusion: SAGE为未来LLMs的安全意识和生成行为一致性提供了贡献，代码和数据集已公开。

Abstract: Large Language Models (LLMs) have shown impressive capabilities across
various tasks but remain vulnerable to meticulously crafted jailbreak attacks.
In this paper, we identify a critical safety gap: while LLMs are adept at
detecting jailbreak prompts, they often produce unsafe responses when directly
processing these inputs. Inspired by this insight, we propose SAGE (Self-Aware
Guard Enhancement), a training-free defense strategy designed to align LLMs'
strong safety discrimination performance with their relatively weaker safety
generation ability. SAGE consists of two core components: a Discriminative
Analysis Module and a Discriminative Response Module, enhancing resilience
against sophisticated jailbreak attempts through flexible safety discrimination
instructions. Extensive experiments demonstrate SAGE's effectiveness and
robustness across various open-source and closed-source LLMs of different sizes
and architectures, achieving an average 99% defense success rate against
numerous complex and covert jailbreak methods while maintaining helpfulness on
general benchmarks. We further conduct mechanistic interpretability analysis
through hidden states and attention distributions, revealing the underlying
mechanisms of this detection-generation discrepancy. Our work thus contributes
to developing future LLMs with coherent safety awareness and generation
behavior. Our code and datasets are publicly available at
https://github.com/NJUNLP/SAGE.

</details>

### [380] [Historical and psycholinguistic perspectives on morphological productivity: A sketch of an integrative approach](https://arxiv.org/abs/2505.12071)
*Harald Baayen,Kristian Berg,Maziyah Mohamed*

Main category: cs.CL

TLDR: 该研究从认知计算和历时视角探讨形态生产力，使用判别词典模型分析芬兰语、马来语和英语的形态模式，并通过托马斯·曼的写作分析其词汇创新性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索形态生产力如何通过认知计算模型和历时语言使用体现，尤其是通过具体作家的词汇创新行为。

Method: 方法包括使用判别词典模型分析形态模式的系统性，以及通过托马斯·曼的阅读和写作数据研究其词汇创新性。

Result: 结果显示模型能通过子词单元与语义中心关联预测形态生产力，而托马斯·曼的词汇创新率极低，输入中的新词远多于输出。

Conclusion: 结论指出形态生产力与形式-语义系统性和个体语言使用行为密切相关，但低频和新词的研究仍具挑战。

Abstract: In this study, we approach morphological productivity from two perspectives:
a cognitive-computational perspective, and a diachronic perspective zooming in
on an actual speaker, Thomas Mann. For developing the first perspective, we
make use of a cognitive computational model of the mental lexicon, the
discriminative lexicon model. For computational mappings between form and
meaning to be productive, in the sense that novel, previously unencountered
words, can be understood and produced, there must be systematicities between
the form space and the semantic space. If the relation between form and meaning
would be truly arbitrary, a model could memorize form and meaning pairings, but
there is no way in which the model would be able to generalize to novel test
data. For Finnish nominal inflection, Malay derivation, and English
compounding, we explore, using the Discriminative Lexicon Model as a
computational tool, to trace differences in the degree to which inflectional
and word formation patterns are productive. We show that the DLM tends to
associate affix-like sublexical units with the centroids of the embeddings of
the words with a given affix. For developing the second perspective, we study
how the intake and output of one prolific writer, Thomas Mann, changes over
time. We show by means of an examination of what Thomas Mann is likely to have
read, and what he wrote, that the rate at which Mann produces novel derived
words is extremely low. There are far more novel words in his input than in his
output. We show that Thomas Mann is less likely to produce a novel derived word
with a given suffix the greater the average distance is of the embeddings of
all derived words to the corresponding centroid, and discuss the challenges of
using speaker-specific embeddings for low-frequency and novel words.

</details>

### [381] [Do different prompting methods yield a common task representation in language models?](https://arxiv.org/abs/2505.12075)
*Guy Davidson,Todd M. Gureckis,Brenden M. Lake,Adina Williams*

Main category: cs.CL

TLDR: 研究探讨了语言模型在不同任务提示方式（演示与指令）下的任务表示机制，发现它们利用不同的模型组件，支持结合两种方式的实践。


<details>
  <summary>Details</summary>
Motivation: 理解任务表示机制有助于解释模型行为并优化任务表现。

Method: 通过函数向量提取任务表示，并推广到短文本指令提示，分析演示与指令功能向量的差异。

Result: 演示和指令功能向量利用不同模型组件，任务表示不完全重叠。

Conclusion: 支持结合指令与演示的实践，需进一步研究任务推理机制。

Abstract: Demonstrations and instructions are two primary approaches for prompting
language models to perform in-context learning (ICL) tasks. Do identical tasks
elicited in different ways result in similar representations of the task? An
improved understanding of task representation mechanisms would offer
interpretability insights and may aid in steering models. We study this through
function vectors, recently proposed as a mechanism to extract few-shot ICL task
representations. We generalize function vectors to alternative task
presentations, focusing on short textual instruction prompts, and successfully
extract instruction function vectors that promote zero-shot task accuracy. We
find evidence that demonstration- and instruction-based function vectors
leverage different model components, and offer several controls to dissociate
their contributions to task performance. Our results suggest that different
task presentations do not induce a common task representation but elicit
different, partly overlapping mechanisms. Our findings offer principled support
to the practice of combining textual instructions and task demonstrations,
imply challenges in universally monitoring task inference across presentation
forms, and encourage further examinations of LLM task inference mechanisms.

</details>

### [382] [Model Merging in Pre-training of Large Language Models](https://arxiv.org/abs/2505.12082)
*Yunshui Li,Yiyuan Ma,Shen Yan,Chaoyi Zhang,Jing Liu,Jianqiao Lu,Ziwen Xu,Mengzhao Chen,Minrui Wang,Shiyi Zhan,Jin Ma,Xunhao Lai,Yao Luo,Xingyan Bin,Hongbin Ren,Mingji Han,Wenhao Hao,Bairen Yi,LingJun Liu,Bole Ma,Xiaoying Jia,Zhou Xun,Liang Xiang,Yonghui Wu*

Main category: cs.CL

TLDR: 研究探讨了模型合并技术在预训练中的应用，展示了其在性能提升和成本降低方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索模型合并技术在大规模预训练中的应用，以提升模型性能和降低训练成本。

Method: 通过密集和MoE架构的实验，研究合并检查点的方法及其超参数。

Result: 合并检查点显著提升性能，并能预测退火行为，降低训练成本。

Conclusion: 为开源社区提供了有效的模型合并预训练指南。

Abstract: Model merging has emerged as a promising technique for enhancing large
language models, though its application in large-scale pre-training remains
relatively unexplored. In this paper, we present a comprehensive investigation
of model merging techniques during the pre-training process. Through extensive
experiments with both dense and Mixture-of-Experts (MoE) architectures ranging
from millions to over 100 billion parameters, we demonstrate that merging
checkpoints trained with constant learning rates not only achieves significant
performance improvements but also enables accurate prediction of annealing
behavior. These improvements lead to both more efficient model development and
significantly lower training costs. Our detailed ablation studies on merging
strategies and hyperparameters provide new insights into the underlying
mechanisms while uncovering novel applications. Through comprehensive
experimental analysis, we offer the open-source community practical
pre-training guidelines for effective model merging.

</details>

### [383] [Personalized Author Obfuscation with Large Language Models](https://arxiv.org/abs/2505.12090)
*Mohammad Shokri,Sarah Ita Levitan,Rivka Levitan*

Main category: cs.CL

TLDR: 研究大型语言模型（LLMs）在通过改写和改变写作风格来混淆作者身份方面的效果，发现效果因人而异，并提出个性化提示方法以改善效果。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在混淆作者身份方面的实际效果，特别关注个体差异。

Method: 采用用户级别的分析，提出个性化提示方法。

Result: LLMs效果呈现双峰分布，个性化提示方法优于标准方法。

Conclusion: 个性化提示方法能部分解决效果差异问题。

Abstract: In this paper, we investigate the efficacy of large language models (LLMs) in
obfuscating authorship by paraphrasing and altering writing styles. Rather than
adopting a holistic approach that evaluates performance across the entire
dataset, we focus on user-wise performance to analyze how obfuscation
effectiveness varies across individual authors. While LLMs are generally
effective, we observe a bimodal distribution of efficacy, with performance
varying significantly across users. To address this, we propose a personalized
prompting method that outperforms standard prompting techniques and partially
mitigates the bimodality issue.

</details>

### [384] [Improving Fairness in LLMs Through Testing-Time Adversaries](https://arxiv.org/abs/2505.12100)
*Isabela Pereira Gregio,Ian Pons,Anna Helena Reali Costa,Artur Jordão*

Main category: cs.CL

TLDR: 提出一种简单、用户友好且实用的方法，通过修改句子属性生成多个变体并评估预测行为，以减少LLM中的偏见，提高公平性和可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM中的偏见问题阻碍了其在伦理敏感和负责任决策任务中的应用，亟需解决。

Method: 通过生成句子的多个变体并比较预测行为，仅需前向传递，无需训练或微调。

Result: 在Llama3上实验显示，公平性指标提升高达27个百分点。

Conclusion: 该方法显著提高了LLM的公平性和可靠性，适用于需要伦理考量的任务。

Abstract: Large Language Models (LLMs) push the bound-aries in natural language
processing and generative AI, driving progress across various aspects of modern
society. Unfortunately, the pervasive issue of bias in LLMs responses (i.e.,
predictions) poses a significant and open challenge, hindering their
application in tasks involving ethical sensitivity and responsible
decision-making. In this work, we propose a straightforward, user-friendly and
practical method to mitigate such biases, enhancing the reliability and
trustworthiness of LLMs. Our method creates multiple variations of a given
sentence by modifying specific attributes and evaluates the corresponding
prediction behavior compared to the original, unaltered, prediction/sentence.
The idea behind this process is that critical ethical predictions often exhibit
notable inconsistencies, indicating the presence of bias. Unlike previous
approaches, our method relies solely on forward passes (i.e., testing-time
adversaries), eliminating the need for training, fine-tuning, or prior
knowledge of the training data distribution. Through extensive experiments on
the popular Llama family, we demonstrate the effectiveness of our method in
improving various fairness metrics, focusing on the reduction of disparities in
how the model treats individuals from different racial groups. Specifically,
using standard metrics, we improve the fairness in Llama3 in up to 27
percentage points. Overall, our approach significantly enhances fairness,
equity, and reliability in LLM-generated results without parameter tuning or
training data modifications, confirming its effectiveness in practical
scenarios. We believe our work establishes an important step toward enabling
the use of LLMs in tasks that require ethical considerations and responsible
decision-making.

</details>

### [385] [A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings](https://arxiv.org/abs/2505.12116)
*Fitsum Gaim,Hoyun Song,Huije Lee,Changgeon Ko,Eui Jun Hwang,Jong C. Park*

Main category: cs.CL

TLDR: 该论文提出了一个针对提格里尼亚语社交媒体的大规模多任务基准数据集，用于检测辱骂性语言，并支持情感和主题分类。数据集包含13,717条YouTube评论，通过迭代术语聚类方法构建，覆盖罗马化和原生Ge'ez脚本。实验表明，小型多任务模型在低资源环境下表现优于前沿模型。


<details>
  <summary>Details</summary>
Motivation: 当前内容审核研究因资源不足未能覆盖多数语言，导致数百万用户面临网络敌意。本文旨在填补提格里尼亚语在辱骂性语言检测领域的空白。

Method: 采用迭代术语聚类方法构建数据集，包含13,717条YouTube评论，由九名母语者标注。数据集支持罗马化和Ge'ez脚本，并建立多任务基准。

Result: 实验显示，小型多任务模型在低资源环境下表现优异，辱骂性语言检测准确率达86%（提升7个百分点）。

Conclusion: 该研究为提格里尼亚语的内容审核提供了重要资源，并展示了多任务模型在低资源语言中的潜力。

Abstract: Content moderation research has recently made significant advances, but still
fails to serve the majority of the world's languages due to the lack of
resources, leaving millions of vulnerable users to online hostility. This work
presents a large-scale human-annotated multi-task benchmark dataset for abusive
language detection in Tigrinya social media with joint annotations for three
tasks: abusiveness, sentiment, and topic classification. The dataset comprises
13,717 YouTube comments annotated by nine native speakers, collected from 7,373
videos with a total of over 1.2 billion views across 51 channels. We developed
an iterative term clustering approach for effective data selection. Recognizing
that around 64% of Tigrinya social media content uses Romanized
transliterations rather than native Ge'ez script, our dataset accommodates both
writing systems to reflect actual language use. We establish strong baselines
across the tasks in the benchmark, while leaving significant challenges for
future contributions. Our experiments reveal that small, specialized multi-task
models outperform the current frontier models in the low-resource setting,
achieving up to 86% accuracy (+7 points) in abusiveness detection. We make the
resources publicly available to promote research on online safety.

</details>

### [386] [The AI Gap: How Socioeconomic Status Affects Language Technology Interactions](https://arxiv.org/abs/2505.12158)
*Elisa Bassignana,Amanda Cercas Curry,Dirk Hovy*

Main category: cs.CL

TLDR: 研究发现社会经济地位（SES）显著影响人们与语言技术的互动方式，高SES群体更抽象简洁，低SES群体更具体且拟人化。


<details>
  <summary>Details</summary>
Motivation: 探讨SES如何影响语言技术的使用，弥补以往依赖代理指标和合成数据的不足。

Method: 调查1000名不同SES背景的个体，收集6482条与LLMs的互动提示。

Result: 高SES群体语言更抽象简洁，话题如‘包容性’和‘旅行’；低SES群体更拟人化且语言具体。

Conclusion: 需在语言技术开发中考虑SES差异，以减少数字鸿沟。

Abstract: Socioeconomic status (SES) fundamentally influences how people interact with
each other and more recently, with digital technologies like Large Language
Models (LLMs). While previous research has highlighted the interaction between
SES and language technology, it was limited by reliance on proxy metrics and
synthetic data. We survey 1,000 individuals from diverse socioeconomic
backgrounds about their use of language technologies and generative AI, and
collect 6,482 prompts from their previous interactions with LLMs. We find
systematic differences across SES groups in language technology usage (i.e.,
frequency, performed tasks), interaction styles, and topics. Higher SES entails
a higher level of abstraction, convey requests more concisely, and topics like
'inclusivity' and 'travel'. Lower SES correlates with higher
anthropomorphization of LLMs (using ''hello'' and ''thank you'') and more
concrete language. Our findings suggest that while generative language
technologies are becoming more accessible to everyone, socioeconomic linguistic
differences still stratify their use to exacerbate the digital divide. These
differences underscore the importance of considering SES in developing language
technologies to accommodate varying linguistic needs rooted in socioeconomic
factors and limit the AI Gap across SES groups.

</details>

### [387] [Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse](https://arxiv.org/abs/2505.12160)
*Darmawan Wicaksono,Hasri Akbar Awal Rozaq,Nevfel Boz*

Main category: cs.CL

TLDR: 研究分析了土耳其社交媒体中‘Sessiz Istila’（沉默入侵）一词的使用，揭示了叙利亚难民潮引发的反难民情绪。通过BERTurk和TREMO数据集开发的土耳其语情感识别模型（ERM）准确率达92.62%，用于分析土耳其语社交媒体数据中的情感变化。


<details>
  <summary>Details</summary>
Motivation: 探讨土耳其社交媒体中反难民情绪的兴起及其情感表达，推动对非主流语言的情感分析研究。

Method: 使用BERTurk和TREMO数据集开发土耳其语情感识别模型（ERM），并应用于大规模X平台数据分析。

Result: ERM模型在情感分类中达到92.62%的准确率，揭示了土耳其语社交媒体中的情感细微差异。

Conclusion: 本地化的NLP工具（如ERM）在土耳其语情感分析中具有实际应用价值，有助于提升决策效率，并强调了考虑区域和语言差异的重要性。

Abstract: Social media platforms like X (formerly Twitter) play a crucial role in
shaping public discourse and societal norms. This study examines the term
Sessiz Istila (Silent Invasion) on Turkish social media, highlighting the rise
of anti-refugee sentiment amidst the Syrian refugee influx. Using BERTurk and
the TREMO dataset, we developed an advanced Emotion Recognition Model (ERM)
tailored for Turkish, achieving 92.62% accuracy in categorizing emotions such
as happiness, fear, anger, sadness, disgust, and surprise. By applying this
model to large-scale X data, the study uncovers emotional nuances in Turkish
discourse, contributing to computational social science by advancing sentiment
analysis in underrepresented languages and enhancing our understanding of
global digital discourse and the unique linguistic challenges of Turkish. The
findings underscore the transformative potential of localized NLP tools, with
our ERM model offering practical applications for real-time sentiment analysis
in Turkish-language contexts. By addressing critical areas, including
marketing, public relations, and crisis management, these models facilitate
improved decision-making through timely and accurate sentiment tracking. This
highlights the significance of advancing research that accounts for regional
and linguistic nuances.

</details>

### [388] [Truth Neurons](https://arxiv.org/abs/2505.12182)
*Haohang Li,Yupeng Cao,Yangyang Yu,Jordan W. Suchow,Zining Zhu*

Main category: cs.CL

TLDR: 论文提出了一种在神经元层面识别语言模型中真实性表征的方法，发现存在与主题无关的“真实性神经元”，并通过实验验证了其普遍性和重要性。


<details>
  <summary>Details</summary>
Motivation: 语言模型有时会生成不真实的回答，但其真实性机制尚不明确，影响了模型的可靠性和安全性。

Method: 通过神经元层面的分析，识别并验证语言模型中的“真实性神经元”，并研究其分布模式和功能。

Result: 实验证明“真实性神经元”普遍存在于不同规模的模型中，其分布与真实性几何特征一致，抑制这些神经元会降低模型性能。

Conclusion: 研究揭示了语言模型中真实性的机制，为提升模型的可靠性和信任度提供了新方向。

Abstract: Despite their remarkable success and deployment across diverse workflows,
language models sometimes produce untruthful responses. Our limited
understanding of how truthfulness is mechanistically encoded within these
models jeopardizes their reliability and safety. In this paper, we propose a
method for identifying representations of truthfulness at the neuron level. We
show that language models contain truth neurons, which encode truthfulness in a
subject-agnostic manner. Experiments conducted across models of varying scales
validate the existence of truth neurons, confirming that the encoding of
truthfulness at the neuron level is a property shared by many language models.
The distribution patterns of truth neurons over layers align with prior
findings on the geometry of truthfulness. Selectively suppressing the
activations of truth neurons found through the TruthfulQA dataset degrades
performance both on TruthfulQA and on other benchmarks, showing that the
truthfulness mechanisms are not tied to a specific dataset. Our results offer
novel insights into the mechanisms underlying truthfulness in language models
and highlight potential directions toward improving their trustworthiness and
reliability.

</details>

### [389] [Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases](https://arxiv.org/abs/2505.12183)
*Manari Hirose,Masato Uchida*

Main category: cs.CL

TLDR: 研究提出了一种评估大型语言模型（LLM）意识形态偏见的框架，通过分析436个二元选择题发现不同模型和语言间存在差异，并揭示了潜在的伦理问题。


<details>
  <summary>Details</summary>
Motivation: 理解LLM的偏见和意识形态对社会的影响，以确保其伦理和有效使用。

Method: 提出定量分析框架，通过436个二元选择题评估ChatGPT和Gemini的意识形态偏见。

Result: 发现LLM在不同模型和语言间存在意识形态差异，ChatGPT倾向于迎合提问者观点，且存在伦理偏见。

Conclusion: 强调评估LLM时需考虑意识形态和伦理问题，框架为开发更社会对齐的AI提供了方法。

Abstract: The widespread integration of Large Language Models (LLMs) across various
sectors has highlighted the need for empirical research to understand their
biases, thought patterns, and societal implications to ensure ethical and
effective use. In this study, we propose a novel framework for evaluating LLMs,
focusing on uncovering their ideological biases through a quantitative analysis
of 436 binary-choice questions, many of which have no definitive answer. By
applying our framework to ChatGPT and Gemini, findings revealed that while LLMs
generally maintain consistent opinions on many topics, their ideologies differ
across models and languages. Notably, ChatGPT exhibits a tendency to change
their opinion to match the questioner's opinion. Both models also exhibited
problematic biases, unethical or unfair claims, which might have negative
societal impacts. These results underscore the importance of addressing both
ideological and ethical considerations when evaluating LLMs. The proposed
framework offers a flexible, quantitative method for assessing LLM behavior,
providing valuable insights for the development of more socially aligned AI
systems.

</details>

### [390] [Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled](https://arxiv.org/abs/2505.12196)
*Yi-Chien Lin,Hongao Zhu,William Schuler*

Main category: cs.CL

TLDR: 研究发现，大型语言模型（LLMs）在预测人类阅读时间和脑成像数据时存在不足，且随着模型规模增大，这种不匹配反而加剧。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs与人类句子处理能力之间的关系，验证是否存在'质量-能力'正相关，以及模型规模对预测能力的影响。

Method: 使用完整的LLM向量进行评估，并控制较大模型中预测变量的数量。

Result: 结果显示存在逆向缩放现象，表明LLMs与人类句子处理存在显著不匹配，且随着模型规模增大而恶化。

Conclusion: LLMs的不足可能源于其与人类句子处理的根本性不匹配，而非模型复杂度不足。

Abstract: The impressive linguistic abilities of large language models (LLMs) have
recommended them as models of human sentence processing, with some conjecturing
a positive 'quality-power' relationship (Wilcox et al., 2023), in which
language models' (LMs') fit to psychometric data continues to improve as their
ability to predict words in context increases. This is important because it
suggests that elements of LLM architecture, such as veridical attention to
context and a unique objective of predicting upcoming words, reflect the
architecture of the human sentence processing faculty, and that any
inadequacies in predicting human reading time and brain imaging data may be
attributed to insufficient model complexity, which recedes as larger models
become available. Recent studies (Oh and Schuler, 2023) have shown this scaling
inverts after a point, as LMs become excessively large and accurate, when word
prediction probability (as information-theoretic surprisal) is used as a
predictor. Other studies propose the use of entire vectors from differently
sized LLMs, still showing positive scaling (Schrimpf et al., 2021), casting
doubt on the value of surprisal as a predictor, but do not control for the
larger number of predictors in vectors from larger LMs. This study evaluates
LLM scaling using entire LLM vectors, while controlling for the larger number
of predictors in vectors from larger LLMs. Results show that inverse scaling
obtains, suggesting that inadequacies in predicting human reading time and
brain imaging data may be due to substantial misalignment between LLMs and
human sentence processing, which worsens as larger models are used.

</details>

### [391] [How Reliable is Multilingual LLM-as-a-Judge?](https://arxiv.org/abs/2505.12201)
*Xiyan Fu,Wei Liu*

Main category: cs.CL

TLDR: LLM-as-a-Judge在评估多语言任务时表现不一致，尤其是在低资源语言中，模型规模和训练数据未能直接提升一致性。


<details>
  <summary>Details</summary>
Motivation: 研究多语言LLM-as-a-Judge的可靠性，填补其在多语言评估中的不确定性。

Method: 评估五个不同模型家族的模型在25种语言的五个任务中的表现，分析影响因素。

Result: LLM在多语言评估中一致性较差（Fleiss' Kappa约0.3），低资源语言表现更差，模型规模和训练数据无直接改善。

Conclusion: LLM在多语言评估中尚不可靠，提出集成策略以提升实际应用中的一致性。

Abstract: LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced
large language models assess generation results in alignment with human
instructions. While these models serve as a promising alternative to human
annotators, their reliability in multilingual evaluation remains uncertain. To
bridge this gap, we conduct a comprehensive analysis of multilingual
LLM-as-a-Judge. Specifically, we evaluate five models from different model
families across five diverse tasks involving 25 languages. Our findings reveal
that LLMs struggle to achieve consistent judgment results across languages,
with an average Fleiss' Kappa of approximately 0.3, and some models performing
even worse. To investigate the cause of inconsistency, we analyze various
influencing factors. We observe that consistency varies significantly across
languages, with particularly poor performance in low-resource languages.
Additionally, we find that neither training on multilingual data nor increasing
model scale directly improves judgment consistency. These findings suggest that
LLMs are not yet reliable for evaluating multilingual predictions. We finally
propose an ensemble strategy which improves the consistency of the multilingual
judge in real-world applications.

</details>

### [392] [Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning](https://arxiv.org/abs/2505.12212)
*Shaobo Wang,Ziming Wang,Xiangqi Jin,Jize Wang,Jiajun Zhang,Kaixin Li,Zichen Wen,Zhong Li,Conghui He,Xuming Hu,Linfeng Zhang*

Main category: cs.CL

TLDR: Data Whisperer是一种无需训练、基于注意力的高效数据选择方法，通过少样本上下文学习优化LLM微调数据选择，显著提升性能并减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着数据集规模增长，传统数据选择方法效率低且资源消耗大，无法充分利用模型预测能力。

Method: 提出Data Whisperer，利用少样本上下文学习和注意力机制，无需额外训练即可高效选择数据子集。

Result: 在Llama-3-8B-Instruct模型上，仅用10%数据即超越完整GSM8K数据集性能，性能提升3.1分，速度提升7.4倍。

Conclusion: Data Whisperer为LLM微调提供了一种高效、低成本的数据选择解决方案。

Abstract: Fine-tuning large language models (LLMs) on task-specific data is essential
for their effective deployment. As dataset sizes grow, efficiently selecting
optimal subsets for training becomes crucial to balancing performance and
computational costs. Traditional data selection methods often require
fine-tuning a scoring model on the target dataset, which is time-consuming and
resource-intensive, or rely on heuristics that fail to fully leverage the
model's predictive capabilities. To address these challenges, we propose Data
Whisperer, an efficient, training-free, attention-based method that leverages
few-shot in-context learning with the model to be fine-tuned. Comprehensive
evaluations were conducted on both raw and synthetic datasets across diverse
tasks and models. Notably, Data Whisperer achieves superior performance
compared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just
10% of the data, and outperforms existing methods with a 3.1-point improvement
and a 7.4$\times$ speedup.

</details>

### [393] [GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment](https://arxiv.org/abs/2505.12215)
*Jiwei Tang,Zhicheng Zhang,Shunlong Wu,Jingheng Ye,Lichen Bai,Zitai Wang,Tingwei Lu,Jiaqi Chen,Lin Hai,Hai-Tao Zheng,Hong-Gee Kim*

Main category: cs.CL

TLDR: GMSA是一种基于编码器-解码器架构的上下文压缩框架，通过减少输入序列长度和冗余信息，解决了大语言模型在长上下文场景中的计算效率低和信息冗余问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文场景中面临计算效率低和信息冗余的挑战，需要一种有效的压缩方法。

Method: GMSA采用组合并和层语义对齐（LSA）技术，通过自动编码器训练学习软标记，并结合知识提取微调（KEFT）适应下游任务。

Result: GMSA在上下文恢复和下游问答任务中表现优异，实现了2倍加速并显著优于现有方法。

Conclusion: GMSA是一种高效且稳定的上下文压缩框架，适用于长上下文场景，显著提升了模型性能。

Abstract: Large language models (LLMs) have achieved impressive performance in a
variety of natural language processing (NLP) tasks. However, when applied to
long-context scenarios, they face two challenges, i.e., low computational
efficiency and much redundant information. This paper introduces GMSA, a
context compression framework based on the encoder-decoder architecture, which
addresses these challenges by reducing input sequence length and redundant
information. Structurally, GMSA has two key components: Group Merging and Layer
Semantic Alignment (LSA). Group merging is used to effectively and efficiently
extract summary vectors from the original context. Layer semantic alignment, on
the other hand, aligns the high-level summary vectors with the low-level
primary input semantics, thus bridging the semantic gap between different
layers. In the training process, GMSA first learns soft tokens that contain
complete semantics through autoencoder training. To furtherly adapt GMSA to
downstream tasks, we propose Knowledge Extraction Fine-tuning (KEFT) to extract
knowledge from the soft tokens for downstream tasks. We train GMSA by randomly
sampling the compression rate for each sample in the dataset. Under this
condition, GMSA not only significantly outperforms the traditional compression
paradigm in context restoration but also achieves stable and significantly
faster convergence with only a few encoder layers. In downstream
question-answering (QA) tasks, GMSA can achieve approximately a 2x speedup in
end-to-end inference while outperforming both the original input prompts and
various state-of-the-art (SOTA) methods by a large margin.

</details>

### [394] [One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models](https://arxiv.org/abs/2505.12216)
*Rongguang Ye,Ming Tang*

Main category: cs.CL

TLDR: UniCuCo提出了一种通用模型，通过StratNet学习将任意请求映射到最优剪枝策略，解决了现有LLM剪枝方法处理多请求时效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM剪枝方法在处理多请求时效率低下，处理时间随请求数线性增长，无法满足实际需求。

Method: 引入StratNet学习最优剪枝策略，利用高斯过程近似非可微剪枝过程的梯度，实现StratNet更新。

Result: UniCuCo在处理64个请求时比基线快28倍，同时保持与基线相当的准确性。

Conclusion: UniCuCo显著提升了多请求场景下的剪枝效率，为实际应用提供了可行方案。

Abstract: Existing pruning methods for large language models (LLMs) focus on achieving
high compression rates while maintaining model performance. Although these
methods have demonstrated satisfactory performance in handling a single user's
compression request, their processing time increases linearly with the number
of requests, making them inefficient for real-world scenarios with multiple
simultaneous requests. To address this limitation, we propose a Univeral Model
for Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that
learns to map arbitrary requests to their optimal pruning strategy. The
challenge in training StratNet lies in the high computational cost of
evaluating pruning strategies and the non-differentiable nature of the pruning
process, which hinders gradient backpropagation for StratNet updates. To
overcome these challenges, we leverage a Gaussian process to approximate the
evaluation process. Since the gradient of the Gaussian process is computable,
we can use it to approximate the gradient of the non-differentiable pruning
process, thereby enabling StratNet updates. Experimental results show that
UniCuCo is 28 times faster than baselines in processing 64 requests, while
maintaining comparable accuracy to baselines.

</details>

### [395] [Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers](https://arxiv.org/abs/2505.12218)
*Tong Bao,Yi Zhao,Jin Mao,Chengzhi Zhang*

Main category: cs.CL

TLDR: 研究分析了LLMs对学术写作语言特征的影响，发现LLM偏好词汇增加，词汇复杂性提高但句法复杂性降低，同时连贯性和可读性下降。


<details>
  <summary>Details</summary>
Motivation: 现有研究多采用定量方法分析LLMs在学术写作中的使用，但缺乏对其语言特征影响的系统性研究。

Method: 通过对arXiv数据集中823,798篇摘要进行语言学特征分析，包括词汇偏好、复杂性、连贯性等。

Result: LLM偏好词汇显著增加，词汇复杂性提高但句法复杂性降低，连贯性和可读性下降，英语能力较弱的学者更倾向使用LLMs。

Conclusion: LLMs对学术写作风格产生广泛影响，尤其在计算机科学领域表现显著，而数学领域变化较小。

Abstract: Large Language Models (LLMs), such as ChatGPT, have prompted academic
concerns about their impact on academic writing. Existing studies have
primarily examined LLM usage in academic writing through quantitative
approaches, such as word frequency statistics and probability-based analyses.
However, few have systematically examined the potential impact of LLMs on the
linguistic characteristics of academic writing. To address this gap, we
conducted a large-scale analysis across 823,798 abstracts published in last
decade from arXiv dataset. Through the linguistic analysis of features such as
the frequency of LLM-preferred words, lexical complexity, syntactic complexity,
cohesion, readability and sentiment, the results indicate a significant
increase in the proportion of LLM-preferred words in abstracts, revealing the
widespread influence of LLMs on academic writing. Additionally, we observed an
increase in lexical complexity and sentiment in the abstracts, but a decrease
in syntactic complexity, suggesting that LLMs introduce more new vocabulary and
simplify sentence structure. However, the significant decrease in cohesion and
readability indicates that abstracts have fewer connecting words and are
becoming more difficult to read. Moreover, our analysis reveals that scholars
with weaker English proficiency were more likely to use the LLMs for academic
writing, and focused on improving the overall logic and fluency of the
abstracts. Finally, at discipline level, we found that scholars in Computer
Science showed more pronounced changes in writing style, while the changes in
Mathematics were minimal.

</details>

### [396] [Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training](https://arxiv.org/abs/2505.12236)
*Quanjiang Guo,Jinchuan Zhang,Sijie Wang,Ling Tian,Zhao Kang,Bin Yan,Weidong Xiao*

Main category: cs.CL

TLDR: TKRE提出了一种结合生成式和判别式学习的两阶段预训练框架，通过生成解释驱动的知识和合成数据解决少样本关系抽取的数据稀缺问题，并在基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 少样本关系抽取（FSRE）面临标注数据稀缺和现有模型泛化能力有限的问题，尽管大语言模型（LLM）通过上下文学习展示了潜力，但其通用训练目标在任务特定关系抽取中表现不佳。

Method: TKRE框架结合LLM与传统关系抽取模型，通过生成解释驱动的知识和模式约束的合成数据解决数据稀缺问题，并采用两阶段预训练策略（MSLM和SCL）增强关系推理和泛化能力。

Result: 在基准数据集上的实验表明，TKRE在FSRE任务中达到了新的SOTA性能，展示了其在低资源场景中的广泛应用潜力。

Conclusion: TKRE通过创新的两阶段预训练和知识生成方法，有效解决了FSRE中的数据稀缺和泛化问题，为低资源关系抽取提供了新思路。

Abstract: Few-Shot Relation Extraction (FSRE) remains a challenging task due to the
scarcity of annotated data and the limited generalization capabilities of
existing models. Although large language models (LLMs) have demonstrated
potential in FSRE through in-context learning (ICL), their general-purpose
training objectives often result in suboptimal performance for task-specific
relation extraction. To overcome these challenges, we propose TKRE (Two-Stage
Knowledge-Guided Pre-training for Relation Extraction), a novel framework that
synergistically integrates LLMs with traditional relation extraction models,
bridging generative and discriminative learning paradigms. TKRE introduces two
key innovations: (1) leveraging LLMs to generate explanation-driven knowledge
and schema-constrained synthetic data, addressing the issue of data scarcity;
and (2) a two-stage pre-training strategy combining Masked Span Language
Modeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational
reasoning and generalization. Together, these components enable TKRE to
effectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets
demonstrate the efficacy of TKRE, achieving new state-of-the-art performance in
FSRE and underscoring its potential for broader application in low-resource
scenarios. \footnote{The code and data are released on
https://github.com/UESTC-GQJ/TKRE.

</details>

### [397] [PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs](https://arxiv.org/abs/2505.12238)
*Sriram Selvam,Anneswa Ghosh*

Main category: cs.CL

TLDR: 论文介绍了PANORAMA数据集，用于研究大型语言模型对敏感和个人身份信息（PII）的记忆问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的规模扩大和实际应用增加，其对敏感和PII数据的记忆带来了隐私风险，但目前缺乏全面、真实且符合伦理的数据集来研究这一问题。

Method: 通过构建合成的人类多属性档案，生成多样化的在线内容（如社交媒体帖子、论坛讨论等），嵌入真实的PII和敏感信息，形成PANORAMA数据集。

Result: 实验表明，随着数据重复率的增加，PII的记忆率也增加，且不同内容类型的记忆风险存在差异。

Conclusion: PANORAMA为隐私风险评估、模型审计和隐私保护LLM的开发提供了重要资源。

Abstract: The memorization of sensitive and personally identifiable information (PII)
by large language models (LLMs) poses growing privacy risks as models scale and
are increasingly deployed in real-world applications. Existing efforts to study
sensitive and PII data memorization and develop mitigation strategies are
hampered by the absence of comprehensive, realistic, and ethically sourced
datasets reflecting the diversity of sensitive information found on the web. We
introduce PANORAMA - Profile-based Assemblage for Naturalistic Online
Representation and Attribute Memorization Analysis, a large-scale synthetic
corpus of 384,789 samples derived from 9,674 synthetic profiles designed to
closely emulate the distribution, variety, and context of PII and sensitive
data as it naturally occurs in online environments. Our data generation
pipeline begins with the construction of internally consistent, multi-attribute
human profiles using constrained selection to reflect real-world demographics
such as education, health attributes, financial status, etc. Using a
combination of zero-shot prompting and OpenAI o3-mini, we generate diverse
content types - including wiki-style articles, social media posts, forum
discussions, online reviews, comments, and marketplace listings - each
embedding realistic, contextually appropriate PII and other sensitive
information. We validate the utility of PANORAMA by fine-tuning the Mistral-7B
model on 1x, 5x, 10x, and 25x data replication rates with a subset of data and
measure PII memorization rates - revealing not only consistent increases with
repetition but also variation across content types, highlighting PANORAMA's
ability to model how memorization risks differ by context. Our dataset and code
are publicly available, providing a much-needed resource for privacy risk
assessment, model auditing, and the development of privacy-preserving LLMs.

</details>

### [398] [Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce](https://arxiv.org/abs/2505.12244)
*Haojin Wang,Zining Zhu,Freda Shi*

Main category: cs.CL

TLDR: 研究探讨了自回归语言模型生成概率分布的能力，发现某些分布比其他分布更难诱导。


<details>
  <summary>Details</summary>
Motivation: 系统理解语言模型能生成的概率分布范围，揭示其表达能力。

Method: 通过软或硬梯度提示调整，寻找能诱导模型输出接近目标分布的提示。

Result: 低或高熵分布、含异常标记的分布及模型生成的分布更容易近似。

Conclusion: 研究揭示了语言模型的表达能力及其作为概率分布提议者的挑战。

Abstract: Autoregressive neural language models (LMs) generate a probability
distribution over tokens at each time step given a prompt. In this work, we
attempt to systematically understand the probability distributions that LMs can
produce, showing that some distributions are significantly harder to elicit
than others. Specifically, for any target next-token distribution over the
vocabulary, we attempt to find a prompt that induces the LM to output a
distribution as close as possible to the target, using either soft or hard
gradient-based prompt tuning. We find that (1) in general, distributions with
very low or very high entropy are easier to approximate than those with
moderate entropy; (2) among distributions with the same entropy, those
containing ''outlier tokens'' are easier to approximate; (3) target
distributions generated by LMs -- even LMs with different tokenizers -- are
easier to approximate than randomly chosen targets. These results offer
insights into the expressiveness of LMs and the challenges of using them as
probability distribution proposers.

</details>

### [399] [Not All Documents Are What You Need for Extracting Instruction Tuning Data](https://arxiv.org/abs/2505.12250)
*Chi Zhang,Huaping Zhong,Hongtao Li,Chengliang Chai,Jiawei Hong,Yuhao Deng,Jiacheng Wang,Tian Tan,Yizhou Yan,Jiantao Qiu,Ye Yuan,Guoren Wang,Conghui He,Lei Cao*

Main category: cs.CL

TLDR: 论文提出EQUAL框架，通过从网络语料库中提取多样化的指令数据来优化指令调优，显著降低计算成本并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法合成的指令数据缺乏多样性且与种子数据相似，限制了实际应用。

Method: 提出EQUAL框架，结合文档聚类和多臂老虎机策略，迭代选择高质量QA对。

Result: 实验显示EQUAL降低计算成本5-10倍，并在LLaMA-3.1-8B和Mistral-7B上提升准确率2.5%。

Conclusion: EQUAL是一种高效且可扩展的数据提取方法，显著提升了指令调优的效果。

Abstract: Instruction tuning improves the performance of large language models (LLMs),
but it heavily relies on high-quality training data. Recently, LLMs have been
used to synthesize instruction data using seed question-answer (QA) pairs.
However, these synthesized instructions often lack diversity and tend to be
similar to the input seeds, limiting their applicability in real-world
scenarios. To address this, we propose extracting instruction tuning data from
web corpora that contain rich and diverse knowledge. A naive solution is to
retrieve domain-specific documents and extract all QA pairs from them, but this
faces two key challenges: (1) extracting all QA pairs using LLMs is
prohibitively expensive, and (2) many extracted QA pairs may be irrelevant to
the downstream tasks, potentially degrading model performance. To tackle these
issues, we introduce EQUAL, an effective and scalable data extraction framework
that iteratively alternates between document selection and high-quality QA pair
extraction to enhance instruction tuning. EQUAL first clusters the document
corpus based on embeddings derived from contrastive learning, then uses a
multi-armed bandit strategy to efficiently identify clusters that are likely to
contain valuable QA pairs. This iterative approach significantly reduces
computational cost while boosting model performance. Experiments on
AutoMathText and StackOverflow across four downstream tasks show that EQUAL
reduces computational costs by 5-10x and improves accuracy by 2.5 percent on
LLaMA-3.1-8B and Mistral-7B

</details>

### [400] [Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches](https://arxiv.org/abs/2505.12259)
*Yuhang Zhou,Xutian Chen,Yixin Cao,Yuchen Ni,Yu He,Siyu Tian,Xiang Liu,Jian Zhang,Chuanjun Ji,Guangnan Ye,Xipeng Qiu*

Main category: cs.CL

TLDR: Teach2Eval是一种间接评估框架，通过评估大型语言模型（LLM）教授较弱学生模型的能力，实现多维度、可扩展的自动化评估。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试存在公平性、可扩展性和数据污染问题，需要更有效的评估方法。

Method: 将开放任务转化为标准化多选题，通过教师生成的反馈评估LLM的多项能力。

Result: 实验结果显示，Teach2Eval与现有动态排名高度一致，并提供更多可解释性。

Conclusion: Teach2Eval提供了一种新颖、可扩展且多功能的LLM评估方法。

Abstract: Recent progress in large language models (LLMs) has outpaced the development
of effective evaluation methods. Traditional benchmarks rely on task-specific
metrics and static datasets, which often suffer from fairness issues, limited
scalability, and contamination risks. In this paper, we introduce Teach2Eval,
an indirect evaluation framework inspired by the Feynman Technique. Instead of
directly testing LLMs on predefined tasks, our method evaluates a model's
multiple abilities to teach weaker student models to perform tasks effectively.
By converting open-ended tasks into standardized multiple-choice questions
(MCQs) through teacher-generated feedback, Teach2Eval enables scalable,
automated, and multi-dimensional assessment. Our approach not only avoids data
leakage and memorization but also captures a broad range of cognitive abilities
that are orthogonal to current benchmarks. Experimental results across 26
leading LLMs show strong alignment with existing human and model-based dynamic
rankings, while offering additional interpretability for training guidance.

</details>

### [401] [Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation](https://arxiv.org/abs/2505.12265)
*Chengwei Qin,Wenxuan Zhou,Karthik Abinav Sankararaman,Nanshu Wang,Tengyu Xu,Alexander Radovic,Eryk Helenowski,Arya Talebzadeh,Aditya Tayade,Sinong Wang,Shafiq Joty,Han Fang,Hao Ma*

Main category: cs.CL

TLDR: 论文提出了一种名为RATE-FT的新方法，用于在开放域长文本生成任务中检测幻觉（错误信息），通过结合微调和辅助任务，显著提升了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在开放域长文本生成中产生的幻觉问题，现有方法依赖外部工具或局限于特定领域，缺乏通用性。

Method: 提出RATE-FT方法，结合微调和辅助任务，联合学习幻觉检测任务，并通过实验验证其有效性。

Result: 实验表明，RATE-FT在LongFact数据集上比通用微调方法提升了3%的准确率。

Conclusion: RATE-FT是一种高效且通用的幻觉检测方法，适用于开放域长文本任务。

Abstract: Hallucination, the generation of factually incorrect information, remains a
significant challenge for large language models (LLMs), especially in
open-domain long-form generation. Existing approaches for detecting
hallucination in long-form tasks either focus on limited domains or rely
heavily on external fact-checking tools, which may not always be available.
  In this work, we systematically investigate reference-free hallucination
detection in open-domain long-form responses. Our findings reveal that internal
states (e.g., model's output probability and entropy) alone are insufficient
for reliably (i.e., better than random guessing) distinguishing between factual
and hallucinated content. To enhance detection, we explore various existing
approaches, including prompting-based methods, probing, and fine-tuning, with
fine-tuning proving the most effective. To further improve the accuracy, we
introduce a new paradigm, named RATE-FT, that augments fine-tuning with an
auxiliary task for the model to jointly learn with the main task of
hallucination detection. With extensive experiments and analysis using a
variety of model families & datasets, we demonstrate the effectiveness and
generalizability of our method, e.g., +3% over general fine-tuning methods on
LongFact.

</details>

### [402] [$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks](https://arxiv.org/abs/2505.12268)
*Pratim Chowdhary*

Main category: cs.CL

TLDR: 论文提出了一种名为$K$-MSHC的方法，用于识别在分类任务中关键的注意力头集合，并通过Search-K-MSHC算法在Gemma-9B模型中分析了三种语法任务家族。研究发现不同任务依赖不同的头电路，并揭示了任务间计算组件的非线性重叠模式。


<details>
  <summary>Details</summary>
Motivation: 理解中等规模语言模型（≤10B参数）中哪些神经组件驱动特定能力是一个关键挑战。

Method: 提出了$K$-MSHC方法和Search-K-MSHC算法，用于识别关键注意力头集合，并在Gemma-9B模型上分析了三种语法任务家族。

Result: 发现不同任务依赖不同的头电路，任务间存在非线性重叠模式，且每种任务有专用的“超级头”。

Conclusion: 语法和数值能力源于专门但部分可重用的头电路。

Abstract: Understanding which neural components drive specific capabilities in
mid-sized language models ($\leq$10B parameters) remains a key challenge. We
introduce the $(\bm{K}, \epsilon)$-Minimum Sufficient Head Circuit ($K$-MSHC),
a methodology to identify minimal sets of attention heads crucial for
classification tasks as well as Search-K-MSHC, an efficient algorithm for
discovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B,
we analyze three syntactic task families: grammar acceptability, arithmetic
verification, and arithmetic word problems. Our findings reveal distinct
task-specific head circuits, with grammar tasks predominantly utilizing early
layers, word problems showing pronounced activity in both shallow and deep
regions, and arithmetic verification demonstrating a more distributed pattern
across the network. We discover non-linear circuit overlap patterns, where
different task pairs share computational components at varying levels of
importance. While grammar and arithmetic share many "weak" heads, arithmetic
and word problems share more consistently critical "strong" heads. Importantly,
we find that each task maintains dedicated "super-heads" with minimal
cross-task overlap, suggesting that syntactic and numerical competencies emerge
from specialized yet partially reusable head circuits.

</details>

### [403] [LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark](https://arxiv.org/abs/2505.12273)
*Md. Atiqur Rahman,Sabrina Islam,Mushfiqul Haque Omi*

Main category: cs.CL

TLDR: 论文提出了一种基于方言指导的框架，用于提升低资源语言机器翻译（MT）的评估效果，通过扩展数据集和优化提示策略，显著提高了评估性能。


<details>
  <summary>Details</summary>
Motivation: 低资源语言机器翻译评估面临参考翻译稀缺和方言多样性问题，现有大语言模型（LLM）方法在缺乏方言上下文时效果不佳。

Method: 扩展ONUBAD数据集，加入Sylheti-英语句子对和人工评分；优化分词器词汇；引入回归头和方言指导提示策略。

Result: 在多个LLM上测试，新方法在Spearman相关性上最高提升+0.1083，其他评估指标也有改进。

Conclusion: 方言指导的框架显著提升了低资源语言MT评估效果，数据集和代码已开源。

Abstract: Evaluating machine translation (MT) for low-resource languages poses a
persistent challenge, primarily due to the limited availability of high quality
reference translations. This issue is further exacerbated in languages with
multiple dialects, where linguistic diversity and data scarcity hinder robust
evaluation. Large Language Models (LLMs) present a promising solution through
reference-free evaluation techniques; however, their effectiveness diminishes
in the absence of dialect-specific context and tailored guidance. In this work,
we propose a comprehensive framework that enhances LLM-based MT evaluation
using a dialect guided approach. We extend the ONUBAD dataset by incorporating
Sylheti-English sentence pairs, corresponding machine translations, and Direct
Assessment (DA) scores annotated by native speakers. To address the vocabulary
gap, we augment the tokenizer vocabulary with dialect-specific terms. We
further introduce a regression head to enable scalar score prediction and
design a dialect-guided (DG) prompting strategy. Our evaluation across multiple
LLMs shows that the proposed pipeline consistently outperforms existing
methods, achieving the highest gain of +0.1083 in Spearman correlation, along
with improvements across other evaluation settings. The dataset and the code
are available at https://github.com/180041123-Atiq/MTEonLowResourceLanguage.

</details>

### [404] [The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models](https://arxiv.org/abs/2505.12287)
*Linghan Huang,Haolin Jin,Zhaoge Bi,Pengyue Yang,Peizhou Zhao,Taozhao Chen,Xiongfei Wu,Lei Ma,Huaming Chen*

Main category: cs.CL

TLDR: 研究探讨了闭源大语言模型在多语言攻击场景下的脆弱性，提出了一种综合对抗框架，评估了包括GPT-4o在内的多个模型的安全性能。结果表明，Qwen-Max最易受攻击，而GPT-4o防御最强，中文提示的攻击成功率更高。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注开源模型，而闭源大语言模型在多语言攻击下的安全性尚未充分探索。

Method: 提出了一种综合对抗框架，系统评估了四种闭源模型在六类安全内容上的表现，生成38,400个响应，并采用攻击成功率（ASR）作为量化指标。

Result: Qwen-Max最脆弱，GPT-4o防御最强；中文提示的攻击成功率高于英文；新型Two-Sides攻击技术最有效。

Conclusion: 研究强调了大语言模型需要语言感知对齐和跨语言防御，以构建更健壮和包容的AI系统。

Abstract: Large language models (LLMs) have seen widespread applications across various
domains, yet remain vulnerable to adversarial prompt injections. While most
existing research on jailbreak attacks and hallucination phenomena has focused
primarily on open-source models, we investigate the frontier of closed-source
LLMs under multilingual attack scenarios. We present a first-of-its-kind
integrated adversarial framework that leverages diverse attack techniques to
systematically evaluate frontier proprietary solutions, including GPT-4o,
DeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories
of security contents in both English and Chinese, generating 38,400 responses
across 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as
the quantitative metric to assess performance from three dimensions: prompt
design, model architecture, and language environment. Our findings suggest that
Qwen-Max is the most vulnerable, while GPT-4o shows the strongest defense.
Notably, prompts in Chinese consistently yield higher ASRs than their English
counterparts, and our novel Two-Sides attack technique proves to be the most
effective across all models. This work highlights a dire need for
language-aware alignment and robust cross-lingual defenses in LLMs, and we hope
it will inspire researchers, developers, and policymakers toward more robust
and inclusive AI systems.

</details>

### [405] [Enhance Mobile Agents Thinking Process Via Iterative Preference Learning](https://arxiv.org/abs/2505.12299)
*Kun Huang,Weikai Xu,Yuxuan Liu,Quandong Wang,Pengzhi Gao,Wei Liu,Jian Luan,Bin Wang,Bo An*

Main category: cs.CL

TLDR: 论文提出了一种迭代偏好学习（IPL）方法，通过构建CoaT树和思维级直接偏好优化（T-DPO）对，解决了现有方法在数据稀缺和中间推理步骤正确性上的不足，并在移动GUI任务中取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在CoaT轨迹多样性和推理步骤正确性上存在不足，且依赖昂贵的标注数据。

Method: 提出IPL方法，包括CoaT树构建、规则奖励评分和T-DPO对生成，并引入三阶段指令演化增强泛化能力。

Result: MobileIPL在三个标准移动GUI基准测试中表现优于基线模型，并展现出强大的泛化能力。

Conclusion: IPL方法有效解决了数据稀缺和推理正确性问题，显著提升了移动GUI任务的性能。

Abstract: The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to
improve the reasoning performance of VLM-based mobile agents in GUI tasks.
However, the scarcity of diverse CoaT trajectories limits the expressiveness
and generalization ability of such agents. While self-training is commonly
employed to address data scarcity, existing approaches either overlook the
correctness of intermediate reasoning steps or depend on expensive
process-level annotations to construct process reward models (PRM). To address
the above problems, we propose an Iterative Preference Learning (IPL) that
constructs a CoaT-tree through interative sampling, scores leaf nodes using
rule-based reward, and backpropagates feedback to derive Thinking-level Direct
Preference Optimization (T-DPO) pairs. To prevent overfitting during warm-up
supervised fine-tuning, we further introduce a three-stage instruction
evolution, which leverages GPT-4o to generate diverse Q\&A pairs based on real
mobile UI screenshots, enhancing both generality and layout understanding.
Experiments on three standard Mobile GUI-agent benchmarks demonstrate that our
agent MobileIPL outperforms strong baselines, including continual pretraining
models such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance
across three standard Mobile GUI-Agents benchmarks and shows strong
generalization to out-of-domain scenarios.

</details>

### [406] [HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models](https://arxiv.org/abs/2505.12300)
*Weixuan Wang,Minghao Wu,Barry Haddow,Alexandra Birch*

Main category: cs.CL

TLDR: 论文提出了一种名为HBO的新方法，通过全局和局部优化解决LLM微调中的数据不平衡和异质性问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注全局数据平衡，忽略了局部数据的不平衡和异质性，限制了微调效果。

Method: HBO采用双层优化策略，包括全局Actor平衡跨数据集采样和局部Actor优化子集内数据使用，基于奖励函数动态调整。

Result: 在多种任务和语言设置下，HBO显著优于基线方法，准确率提升明显。

Conclusion: HBO为LLM微调中的数据不平衡和异质性问题提供了全面解决方案，提升了训练效果。

Abstract: Fine-tuning large language models (LLMs) on a mixture of diverse datasets
poses challenges due to data imbalance and heterogeneity. Existing methods
often address these issues across datasets (globally) but overlook the
imbalance and heterogeneity within individual datasets (locally), which limits
their effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a
novel method that enables LLMs to autonomously adjust data allocation during
fine-tuning both across datasets (globally) and within each individual dataset
(locally). HBO employs a bilevel optimization strategy with two types of
actors: a Global Actor, which balances data sampling across different subsets
of the training mixture, and several Local Actors, which optimizes data usage
within each subset based on difficulty levels. These actors are guided by
reward functions derived from the LLM's training state, which measure learning
progress and relative performance improvement. We evaluate HBO on three LLM
backbones across nine diverse tasks in multilingual and multitask setups.
Results show that HBO consistently outperforms existing baselines, achieving
significant accuracy gains. Our in-depth analysis further demonstrates that
both the global actor and local actors of HBO effectively adjust data usage
during fine-tuning. HBO provides a comprehensive solution to the challenges of
data imbalance and heterogeneity in LLM fine-tuning, enabling more effective
training across diverse datasets.

</details>

### [407] [R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model](https://arxiv.org/abs/2505.12625)
*Ali Naseh,Harsh Chaudhari,Jaechul Roh,Mingshi Wu,Alina Oprea,Amir Houmansadr*

Main category: cs.CL

TLDR: DeepSeek的R1模型在推理任务中表现优异，但在涉及中国政治敏感话题时表现出明显的审查行为。本文通过分析其审查模式、触发条件及跨语言表现，探讨了其设计背后的潜在问题。


<details>
  <summary>Details</summary>
Motivation: 研究R1模型在政治敏感话题上的审查行为，揭示其设计选择对透明度和偏见的影响。

Method: 构建大规模敏感提示集，分析R1的审查模式、一致性及跨语言表现，并探索绕过审查的技术。

Result: R1的审查行为可能源于训练或对齐阶段的设计选择，引发对模型透明度和治理的担忧。

Conclusion: R1的审查现象凸显了语言模型部署中的透明度和偏见问题，需进一步研究其治理机制。

Abstract: DeepSeek recently released R1, a high-performing large language model (LLM)
optimized for reasoning tasks. Despite its efficient training pipeline, R1
achieves competitive performance, even surpassing leading reasoning models like
OpenAI's o1 on several benchmarks. However, emerging reports suggest that R1
refuses to answer certain prompts related to politically sensitive topics in
China. While existing LLMs often implement safeguards to avoid generating
harmful or offensive outputs, R1 represents a notable shift - exhibiting
censorship-like behavior on politically charged queries. In this paper, we
investigate this phenomenon by first introducing a large-scale set of heavily
curated prompts that get censored by R1, covering a range of politically
sensitive topics, but are not censored by other models. We then conduct a
comprehensive analysis of R1's censorship patterns, examining their
consistency, triggers, and variations across topics, prompt phrasing, and
context. Beyond English-language queries, we explore censorship behavior in
other languages. We also investigate the transferability of censorship to
models distilled from the R1 language model. Finally, we propose techniques for
bypassing or removing this censorship. Our findings reveal possible additional
censorship integration likely shaped by design choices during training or
alignment, raising concerns about transparency, bias, and governance in
language model deployment.

</details>

### [408] [Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection](https://arxiv.org/abs/2505.12306)
*Yuwei Zhang,Wenhao Yu,Shangbin Feng,Yifan Zhu,Letian Peng,Jayanth Srinivasa,Gaowen Liu,Jingbo Shang*

Main category: cs.CL

TLDR: 论文提出了WikiDYK，一个基于维基百科‘Did You Know...’条目的知识注入基准，用于测试大语言模型（LLMs）的知识记忆能力。实验发现因果语言模型（CLMs）在知识记忆上表现较差，而双向语言模型（BiLMs）表现更好。作者还提出了一个模块化协作框架，进一步提升了可靠性。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型（LLMs）的知识记忆能力，填补缺乏标准化高质量测试基准的空白。

Method: 利用维基百科‘Did You Know...’条目构建WikiDYK基准，包含多种任务格式的问题-答案对。通过持续预训练实验比较CLMs和BiLMs的表现，并提出模块化协作框架。

Result: CLMs在知识记忆上比BiLMs低23%的可靠性准确率。模块化协作框架将可靠性准确率提升至29.1%。

Conclusion: WikiDYK为LLMs的知识记忆能力提供了有效测试基准，BiLMs表现优于CLMs，模块化协作框架进一步提升了性能。

Abstract: Despite significant advances in large language models (LLMs), their knowledge
memorization capabilities remain underexplored, due to the lack of standardized
and high-quality test ground. In this paper, we introduce a novel, real-world
and large-scale knowledge injection benchmark that evolves continuously over
time without requiring human intervention. Specifically, we propose WikiDYK,
which leverages recently-added and human-written facts from Wikipedia's "Did
You Know..." entries. These entries are carefully selected by expert Wikipedia
editors based on criteria such as verifiability and clarity. Each entry is
converted into multiple question-answer pairs spanning diverse task formats
from easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290
facts and 77,180 questions, which is also seamlessly extensible with future
updates from Wikipedia editors. Extensive experiments using continued
pre-training reveal a surprising insight: despite their prevalence in modern
LLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge
memorization capabilities compared to Bidirectional Language Models (BiLMs),
exhibiting a 23% lower accuracy in terms of reliability. To compensate for the
smaller scales of current BiLMs, we introduce a modular collaborative framework
utilizing ensembles of BiLMs as external knowledge repositories to integrate
with LLMs. Experiment shows that our framework further improves the reliability
accuracy by up to 29.1%.

</details>

### [409] [ExpertSteer: Intervening in LLMs through Expert Knowledge](https://arxiv.org/abs/2505.12313)
*Weixuan Wang,Minghao Wu,Barry Haddow,Alexandra Birch*

Main category: cs.CL

TLDR: ExpertSteer是一种新方法，利用外部专家模型生成引导向量，干预任何大型语言模型（LLM），显著提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖模型自身生成引导向量，限制了效果和通用性。ExpertSteer旨在利用外部专家模型解决这一问题。

Method: 通过四步流程：维度对齐、干预层配对、生成引导向量、应用向量，无需更新模型参数。

Result: 在15个基准测试中，ExpertSteer显著优于基线方法。

Conclusion: ExpertSteer提供了一种高效、通用的方法，利用外部专家模型优化LLM行为。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities across various
tasks, yet guiding them to follow desired behaviours during inference remains a
significant challenge. Activation steering offers a promising method to control
the generation process of LLMs by modifying their internal activations.
However, existing methods commonly intervene in the model's behaviour using
steering vectors generated by the model itself, which constrains their
effectiveness to that specific model and excludes the possibility of leveraging
powerful external expert models for steering. To address these limitations, we
propose ExpertSteer, a novel approach that leverages arbitrary specialized
expert models to generate steering vectors, enabling intervention in any LLMs.
ExpertSteer transfers the knowledge from an expert model to a target LLM
through a cohesive four-step process: first aligning representation dimensions
with auto-encoders to enable cross-model transfer, then identifying
intervention layer pairs based on mutual information analysis, next generating
steering vectors from the expert model using Recursive Feature Machines, and
finally applying these vectors on the identified layers during inference to
selectively guide the target LLM without updating model parameters. We conduct
comprehensive experiments using three LLMs on 15 popular benchmarks across four
distinct domains. Experiments demonstrate that ExpertSteer significantly
outperforms established baselines across diverse tasks at minimal cost.

</details>

### [410] [LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning](https://arxiv.org/abs/2505.12328)
*Xinye Li,Mingqi Wan,Dianbo Sui*

Main category: cs.CL

TLDR: Team asdfo123使用Meta-Llama-3-8B-Instruct模型，通过少量样本多轮提示，在LLMSR@XLLM25任务中排名第5，表现接近更复杂的系统。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在细粒度、可控和可解释推理任务中的表现。

Method: 使用少量样本多轮提示，引导模型提取条件、分解推理链并验证逻辑有效性，后处理正则化输出。

Result: 排名第5，F1分数与复杂系统相当，无需微调或外部资源。

Conclusion: 分析了方法的优缺点，并展望了LLM结构推理的未来研究方向。

Abstract: We present Team asdfo123's submission to the LLMSR@XLLM25 shared task, which
evaluates large language models on producing fine-grained, controllable, and
interpretable reasoning processes. Systems must extract all problem conditions,
decompose a chain of thought into statement-evidence pairs, and verify the
logical validity of each pair. Leveraging only the off-the-shelf
Meta-Llama-3-8B-Instruct, we craft a concise few-shot, multi-turn prompt that
first enumerates all conditions and then guides the model to label, cite, and
adjudicate every reasoning step. A lightweight post-processor based on regular
expressions normalises spans and enforces the official JSON schema. Without
fine-tuning, external retrieval, or ensembling, our method ranks 5th overall,
achieving macro F1 scores on par with substantially more complex and
resource-consuming pipelines. We conclude by analysing the strengths and
limitations of our approach and outlining directions for future research in
structural reasoning with LLMs. Our code is available at
https://github.com/asdfo123/LLMSR-asdfo123.

</details>

### [411] [UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models](https://arxiv.org/abs/2505.12345)
*Qizhou Chen,Dakan Wang,Taolin Zhang,Zaoming Yan,Chengsong You,Chengyu Wang,Xiaofeng He*

Main category: cs.CL

TLDR: UniEdit是一个基于开放领域知识的统一基准，用于评估大型语言模型（LLM）的编辑能力，覆盖广泛的知识领域和编辑效果评估。


<details>
  <summary>Details</summary>
Motivation: 现有LLM编辑数据集局限于狭窄知识领域，缺乏对编辑需求和涟漪效应的全面评估。

Method: 通过从25个常见领域选择实体构建编辑样本，设计NMCS算法采样子图以评估涟漪效应，并利用LLM将知识子图转换为自然语言文本。

Result: UniEdit基准在规模、全面性和多样性上得到验证，实验分析了多种LLM和编辑器的性能。

Conclusion: UniEdit为开放知识领域和多样化评估标准下的模型编辑研究提供了有价值的见解。

Abstract: Model editing aims to enhance the accuracy and reliability of large language
models (LLMs) by efficiently adjusting their internal parameters. Currently,
most LLM editing datasets are confined to narrow knowledge domains and cover a
limited range of editing evaluation. They often overlook the broad scope of
editing demands and the diversity of ripple effects resulting from edits. In
this context, we introduce UniEdit, a unified benchmark for LLM editing
grounded in open-domain knowledge. First, we construct editing samples by
selecting entities from 25 common domains across five major categories,
utilizing the extensive triple knowledge available in open-domain knowledge
graphs to ensure comprehensive coverage of the knowledge domains. To address
the issues of generality and locality in editing, we design an Neighborhood
Multi-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given
knowledge piece to entail comprehensive ripple effects to evaluate. Finally, we
employ proprietary LLMs to convert the sampled knowledge subgraphs into natural
language text, guaranteeing grammatical accuracy and syntactical diversity.
Extensive statistical analysis confirms the scale, comprehensiveness, and
diversity of our UniEdit benchmark. We conduct comprehensive experiments across
multiple LLMs and editors, analyzing their performance to highlight strengths
and weaknesses in editing across open knowledge domains and various evaluation
criteria, thereby offering valuable insights for future research endeavors.

</details>

### [412] [Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds](https://arxiv.org/abs/2505.12349)
*Axel Abels,Tom Lenaerts*

Main category: cs.CL

TLDR: 研究发现大语言模型（LLMs）会反映数据中的偏见，通过聚合多模型响应可缓解偏见，但简单平均会加剧偏见，加权聚合更有效，结合人类与LLMs的混合群体效果最佳。


<details>
  <summary>Details</summary>
Motivation: LLMs可能无意中放大训练数据中的偏见，需探索方法减少其偏见。

Method: 分析LLMs对偏见性标题的响应，比较简单平均与加权聚合的效果，并引入人类与LLMs的混合群体。

Result: 加权聚合能更有效减少偏见并提高准确性，混合群体在减少种族和性别偏见上表现更优。

Conclusion: 结合人类与LLMs的混合群体是减少偏见并提升性能的有效策略。

Abstract: Despite their performance, large language models (LLMs) can inadvertently
perpetuate biases found in the data they are trained on. By analyzing LLM
responses to bias-eliciting headlines, we find that these models often mirror
human biases. To address this, we explore crowd-based strategies for mitigating
bias through response aggregation. We first demonstrate that simply averaging
responses from multiple LLMs, intended to leverage the "wisdom of the crowd",
can exacerbate existing biases due to the limited diversity within LLM crowds.
In contrast, we show that locally weighted aggregation methods more effectively
leverage the wisdom of the LLM crowd, achieving both bias mitigation and
improved accuracy. Finally, recognizing the complementary strengths of LLMs
(accuracy) and humans (diversity), we demonstrate that hybrid crowds containing
both significantly enhance performance and further reduce biases across ethnic
and gender-related contexts.

</details>

### [413] [CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement](https://arxiv.org/abs/2505.12368)
*Gauri Kholkar,Ratinder Ahuja*

Main category: cs.CL

TLDR: CAPTURE是一个新的上下文感知基准测试，用于评估提示注入防护模型的攻击检测能力和过度防御倾向，揭示了现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 提示注入是大型语言模型的主要安全风险，现有防护模型在上下文感知环境中的效果尚未充分研究，且存在过度防御问题。

Method: 引入CAPTURE基准测试，通过少量域内示例评估攻击检测和过度防御倾向。

Result: 实验显示当前防护模型在对抗情况下漏报率高，在良性场景中误报率高。

Conclusion: 现有提示注入防护模型存在显著局限性，需改进以适应上下文感知环境。

Abstract: Prompt injection remains a major security risk for large language models.
However, the efficacy of existing guardrail models in context-aware settings
remains underexplored, as they often rely on static attack benchmarks.
Additionally, they have over-defense tendencies. We introduce CAPTURE, a novel
context-aware benchmark assessing both attack detection and over-defense
tendencies with minimal in-domain examples. Our experiments reveal that current
prompt injection guardrail models suffer from high false negatives in
adversarial cases and excessive false positives in benign scenarios,
highlighting critical limitations.

</details>

### [414] [From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling](https://arxiv.org/abs/2505.12381)
*Mohsinul Kabir,Tasfia Tahsin,Sophia Ananiadou*

Main category: cs.CL

TLDR: 该论文提出了一种基于比较行为理论的方法，研究语言模型中数据与架构对偏见传播的影响，发现数据时间性、模型设计及架构对偏见传播有显著影响。


<details>
  <summary>Details</summary>
Motivation: 当前关于语言模型偏见的研究多集中于数据质量，而忽视了模型架构和数据时间性的影响，且缺乏对偏见起源的系统性研究。

Method: 基于比较行为理论，结合n-gram语言模型与transformer的对比，分析数据、模型设计及时间动态对偏见传播的影响。

Result: 发现n-gram模型对上下文窗口大小敏感，transformer架构更稳健；数据时间性显著影响偏见；不同架构对偏见注入反应不同，某些偏见（如性取向）被过度放大。

Conclusion: 需从数据和模型维度全面追溯偏见起源，而非仅关注表象，以减少语言模型偏见带来的危害。

Abstract: Current research on bias in language models (LMs) predominantly focuses on
data quality, with significantly less attention paid to model architecture and
temporal influences of data. Even more critically, few studies systematically
investigate the origins of bias. We propose a methodology grounded in
comparative behavioral theory to interpret the complex interaction between
training data and model architecture in bias propagation during language
modeling. Building on recent work that relates transformers to n-gram LMs, we
evaluate how data, model design choices, and temporal dynamics affect bias
propagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to
context window size in bias propagation, while transformers demonstrate
architectural robustness; (2) the temporal provenance of training data
significantly affects bias; and (3) different model architectures respond
differentially to controlled bias injection, with certain biases (e.g. sexual
orientation) being disproportionately amplified. As language models become
ubiquitous, our findings highlight the need for a holistic approach -- tracing
bias to its origins across both data and model dimensions, not just symptoms,
to mitigate harm.

</details>

### [415] [SLOT: Sample-specific Language Model Optimization at Test-time](https://arxiv.org/abs/2505.12392)
*Yang Hu,Xingyu Zhang,Xueji Fang,Zhiyang Chen,Xiao Wang,Huatian Zhang,Guojun Qi*

Main category: cs.CL

TLDR: SLOT是一种参数高效的测试时推理方法，通过少量优化步骤更新样本特定参数向量，提升语言模型对复杂指令的响应能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在复杂指令上表现不佳，SLOT旨在通过测试时优化提升模型对特定样本的适应能力。

Method: SLOT在测试时进行少量优化步骤，更新轻量级样本特定参数向量，并将其添加到最终隐藏层前，通过最小化输入提示的交叉熵损失实现高效适应。

Result: 实验表明，SLOT在多个基准测试和模型上表现优异，例如Qwen2.5-7B在GSM8K上准确率提升8.6%。

Conclusion: SLOT通过测试时优化显著提升了语言模型对复杂指令的响应能力，具有高效性和广泛适用性。

Abstract: We propose SLOT (Sample-specific Language Model Optimization at Test-time), a
novel and parameter-efficient test-time inference approach that enhances a
language model's ability to more accurately respond to individual prompts.
Existing Large Language Models (LLMs) often struggle with complex instructions,
leading to poor performances on those not well represented among general
samples. To address this, SLOT conducts few optimization steps at test-time to
update a light-weight sample-specific parameter vector. It is added to the
final hidden layer before the output head, and enables efficient adaptation by
caching the last layer features during per-sample optimization. By minimizing
the cross-entropy loss on the input prompt only, SLOT helps the model better
aligned with and follow each given instruction. In experiments, we demonstrate
that our method outperforms the compared models across multiple benchmarks and
LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on
GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT
achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is
available at https://github.com/maple-research-lab/SLOT.

</details>

### [416] [Traversal Verification for Speculative Tree Decoding](https://arxiv.org/abs/2505.12398)
*Yepeng Weng,Qiao Hu,Xujie Chen,Li Liu,Dianwen Mei,Huishi Qiu,Jiang Tian,Zhongchao Shi*

Main category: cs.CL

TLDR: 提出了一种新的推测解码算法Traversal Verification，通过从叶到根的遍历方式优化验证过程，提高接受率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法依赖逐层验证，导致接受长度不理想且候选利用率低。

Method: 采用叶到根遍历方式验证整个令牌序列，保留可能有效的子序列。

Result: 实验证明该方法在多个任务和模型上显著提高了接受长度和吞吐量。

Conclusion: Traversal Verification在保证无损推理的同时实现了显著的加速效果。

Abstract: Speculative decoding is a promising approach for accelerating large language
models. The primary idea is to use a lightweight draft model to speculate the
output of the target model for multiple subsequent timesteps, and then verify
them in parallel to determine whether the drafted tokens should be accepted or
rejected. To enhance acceptance rates, existing frameworks typically construct
token trees containing multiple candidates in each timestep. However, their
reliance on token-level verification mechanisms introduces two critical
limitations: First, the probability distribution of a sequence differs from
that of individual tokens, leading to suboptimal acceptance length. Second,
current verification schemes begin from the root node and proceed layer by
layer in a top-down manner. Once a parent node is rejected, all its child nodes
should be discarded, resulting in inefficient utilization of speculative
candidates. This paper introduces Traversal Verification, a novel speculative
decoding algorithm that fundamentally rethinks the verification paradigm
through leaf-to-root traversal. Our approach considers the acceptance of the
entire token sequence from the current node to the root, and preserves
potentially valid subsequences that would be prematurely discarded by existing
methods. We theoretically prove that the probability distribution obtained
through Traversal Verification is identical to that of the target model,
guaranteeing lossless inference while achieving substantial acceleration gains.
Experimental results across different large language models and multiple tasks
show that our method consistently improves acceptance length and throughput
over existing methods

</details>

### [417] [The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT](https://arxiv.org/abs/2505.12405)
*Konstantinos Xylogiannopoulos,Petros Xanthopoulos,Panagiotis Karampelas,Georgios Bakamitsos*

Main category: cs.CL

TLDR: 论文提出了一种基于模式相似性的方法，用于检测ChatGPT生成的改写新闻，准确率达96.23%。


<details>
  <summary>Details</summary>
Motivation: 生成式AI改写文本可能侵犯版权并损害原创内容创作者的收入，但目前缺乏相关研究。

Method: 提出了一种不依赖深度学习的算法方案，通过模式相似性检测ChatGPT改写的新闻。

Result: 在包含2,224篇BBC新闻及其ChatGPT改写版本的测试集上，准确率、精确率、敏感度、特异性和F1分数均超过96%。

Conclusion: 该方法能有效识别ChatGPT生成的改写内容，为版权保护提供了实用工具。

Abstract: Generative AI paraphrased text can be used for copyright infringement and the
AI paraphrased content can deprive substantial revenue from original content
creators. Despite this recent surge of malicious use of generative AI, there
are few academic publications that research this threat. In this article, we
demonstrate the ability of pattern-based similarity detection for AI
paraphrased news recognition. We propose an algorithmic scheme, which is not
limited to detect whether an article is an AI paraphrase, but, more
importantly, to identify that the source of infringement is the ChatGPT. The
proposed method is tested with a benchmark dataset specifically created for
this task that incorporates real articles from BBC, incorporating a total of
2,224 articles across five different news categories, as well as 2,224
paraphrased articles created with ChatGPT. Results show that our pattern
similarity-based method, that makes no use of deep learning, can detect ChatGPT
assisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for
precision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1
score.

</details>

### [418] [Table-R1: Region-based Reinforcement Learning for Table Understanding](https://arxiv.org/abs/2505.12415)
*Zhenhe Wu,Jian Yang,Jiaheng Liu,Xianjie Wu,Changzai Pan,Jie Zhang,Yu Zhao,Shuangyong Song,Yongxiang Li,Zhoujun Li*

Main category: cs.CL

TLDR: Table-R1是一种基于区域的强化学习方法，通过整合区域证据提升语言模型对表格的理解能力，结合RE-SFT和TARPO技术，显著提高了表格问答性能。


<details>
  <summary>Details</summary>
Motivation: 表格因其行列结构对语言模型提出独特挑战，现有方法如CoT和PoT虽有效但未充分优化。

Method: 提出Table-R1方法，结合RE-SFT指导模型识别相关表格区域，并通过TARPO动态平衡区域准确性和答案正确性。

Result: Table-R1在多个基准数据集上平均提升14.36分，TARPO减少67.5%的响应令牌消耗。

Conclusion: 该方法显著提升了语言模型在表格推理中的效率和性能，优于参数规模更大的基线模型。

Abstract: Tables present unique challenges for language models due to their structured
row-column interactions, necessitating specialized approaches for effective
comprehension. While large language models (LLMs) have demonstrated potential
in table reasoning through prompting and techniques like chain-of-thought (CoT)
and program-of-thought (PoT), optimizing their performance for table question
answering remains underexplored. In this paper, we introduce region-based
Table-R1, a novel reinforcement learning approach that enhances LLM table
understanding by integrating region evidence into reasoning steps. Our method
employs Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in
identifying relevant table regions before generating answers, incorporating
textual, symbolic, and program-based reasoning. Additionally, Table-Aware Group
Relative Policy Optimization (TARPO) introduces a mixed reward system to
dynamically balance region accuracy and answer correctness, with decaying
region rewards and consistency penalties to align reasoning steps. Experiments
show that Table-R1 achieves an average performance improvement of 14.36 points
across multiple base models on three benchmark datasets, even outperforming
baseline models with ten times the parameters, while TARPO reduces response
token consumption by 67.5% compared to GRPO, significantly advancing LLM
capabilities in efficient tabular reasoning.

</details>

### [419] [PSC: Extending Context Window of Large Language Models via Phase Shift Calibration](https://arxiv.org/abs/2505.12423)
*Wenqiao Zhu,Chao Xu,Lulu Wang,Jun Wu*

Main category: cs.CL

TLDR: 论文提出了一种名为PSC（Phase Shift Calibration）的小模块，用于校准现有方法预定义的频率，从而增强RoPE（Rotary Position Embedding）在扩展上下文窗口时的性能。实验表明，PSC能显著降低困惑度，并具有广泛适用性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在扩展RoPE的上下文窗口时，难以预定义最优的频率缩放因子，导致性能受限。

Method: 引入PSC模块，校准现有方法预定义的频率，提升如PI、YaRN和LongRoPE等方法的性能。

Result: 实验显示，PSC在不同模型和任务中均能显著降低困惑度，尤其在16k至64k的上下文窗口范围内表现突出。

Conclusion: PSC是一种高效且通用的频率校准方法，能显著提升RoPE在扩展上下文窗口时的性能。

Abstract: Rotary Position Embedding (RoPE) is an efficient position encoding approach
and is widely utilized in numerous large language models (LLMs). Recently, a
lot of methods have been put forward to further expand the context window based
on RoPE. The core concept of those methods is to predefine or search for a set
of factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a
challenge for existing methods to predefine an optimal factor due to the
exponential search space. In view of this, we introduce PSC (Phase Shift
Calibration), a small module for calibrating the frequencies predefined by
existing methods. With the employment of PSC, we demonstrate that many existing
methods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted
extensive experiments across multiple models and tasks. The results demonstrate
that (1) when PSC is enabled, the comparative reductions in perplexity increase
as the context window size is varied from 16k, to 32k, and up to 64k. (2) Our
approach is broadly applicable and exhibits robustness across a variety of
models and tasks. The code can be found at https://github.com/WNQzhu/PSC.

</details>

### [420] [Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games](https://arxiv.org/abs/2505.12439)
*Jinming Zhang,Yunfei Long*

Main category: cs.CL

TLDR: 提出了一个名为LPLH的认知启发框架，指导大语言模型（LLMs）系统性地学习和玩交互式小说游戏（IF游戏），以更接近人类的方式理解叙事和游戏逻辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于关注任务性能指标，忽视了人类对叙事和游戏逻辑的理解，因此需要一种更接近人类认知的框架。

Method: LPLH框架包含三个关键组件：结构化地图构建、动作学习和反馈驱动的经验分析。

Result: LPLH框架能够使LLM代理的行为更符合叙事意图和常识约束，提供更可解释、类人的表现。

Conclusion: LPLH将IF游戏挑战重新定义为LLM代理的学习问题，为复杂文本环境中的上下文感知游戏提供了新路径。

Abstract: Interactive Fiction games (IF games) are where players interact through
natural language commands. While recent advances in Artificial Intelligence
agents have reignited interest in IF games as a domain for studying
decision-making, existing approaches prioritize task-specific performance
metrics over human-like comprehension of narrative context and gameplay logic.
This work presents a cognitively inspired framework that guides Large Language
Models (LLMs) to learn and play IF games systematically. Our proposed
**L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three
key components: (1) structured map building to capture spatial and narrative
relationships, (2) action learning to identify context-appropriate commands,
and (3) feedback-driven experience analysis to refine decision-making over
time. By aligning LLMs-based agents' behavior with narrative intent and
commonsense constraints, LPLH moves beyond purely exploratory strategies to
deliver more interpretable, human-like performance. Crucially, this approach
draws on cognitive science principles to more closely simulate how human
players read, interpret, and respond within narrative worlds. As a result, LPLH
reframes the IF games challenge as a learning problem for LLMs-based agents,
offering a new path toward robust, context-aware gameplay in complex text-based
environments.

</details>

### [421] [Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment](https://arxiv.org/abs/2505.12452)
*Siyang Wu,Honglin Bao,Nadav Kunievsky,James A. Evans*

Main category: cs.CL

TLDR: 论文提出了一种通过自我提问提升大语言模型（LLM）理解能力的方法，并在计算机科学专利数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: LLM内部知识多为潜在且松散的结构，难以直接访问或评估。研究旨在通过自我提问激活这些知识，提升模型在细粒度语义区分任务中的表现。

Method: 采用自我提问策略，让LLM生成并回答与任务相关的问题，同时结合外部科学文本检索。

Result: 自我提问显著提升模型性能，尤其是在技术概念理解上。小模型生成的提问对中等模型更有效，揭示了跨模型协作的新策略。

Conclusion: 自我提问是一种实用机制，可自动提升LLM理解能力，并作为诊断工具揭示内部与外部知识的组织方式。

Abstract: Large language models (LLMs) increasingly demonstrate signs of conceptual
understanding, yet much of their internal knowledge remains latent, loosely
structured, and difficult to access or evaluate. We propose self-questioning as
a lightweight and scalable strategy to improve LLMs' understanding,
particularly in domains where success depends on fine-grained semantic
distinctions. To evaluate this approach, we introduce a challenging new
benchmark of 1.3 million post-2015 computer science patent pairs, characterized
by dense technical jargon and strategically complex writing. The benchmark
centers on a pairwise differentiation task: can a model distinguish between
closely related but substantively different inventions? We show that prompting
LLMs to generate and answer their own questions - targeting the background
knowledge required for the task - significantly improves performance. These
self-generated questions and answers activate otherwise underutilized internal
knowledge. Allowing LLMs to retrieve answers from external scientific texts
further enhances performance, suggesting that model knowledge is compressed and
lacks the full richness of the training data. We also find that
chain-of-thought prompting and self-questioning converge, though
self-questioning remains more effective for improving understanding of
technical concepts. Notably, we uncover an asymmetry in prompting: smaller
models often generate more fundamental, more open-ended, better-aligned
questions for mid-sized models than large models with better understanding do,
revealing a new strategy for cross-model collaboration. Altogether, our
findings establish self-questioning as both a practical mechanism for
automatically improving LLM comprehension, especially in domains with sparse
and underrepresented knowledge, and a diagnostic probe of how internal and
external knowledge are organized.

</details>

### [422] [Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations](https://arxiv.org/abs/2505.12454)
*Yuyang Ding,Dan Qiao,Juntao Li,Jiajie Xu,Pingfu Chao,Xiaofang Zhou,Min Zhang*

Main category: cs.CL

TLDR: 本文探讨了远程监督命名实体识别（DS-NER）的有效性和鲁棒性，提出了一种新的噪声评估框架，并在多个数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统人工标注方法成本高，远程监督方法虽能自动生成训练数据，但不同标注方法间的噪声分布研究较少。

Method: 研究从两方面入手：（1）对比传统规则方法和基于大语言模型的监督方法；（2）提出新框架，将噪声问题分为未标注实体问题（UEP）和噪声实体问题（NEP），并提供针对性解决方案。

Result: 在八个真实世界数据集上取得显著改进，优于当前最先进方法。

Conclusion: 提出的方法在多种数据源和标注技术下表现出优越性，为DS-NER的噪声问题提供了有效解决方案。

Abstract: Distantly supervised named entity recognition (DS-NER) has emerged as a cheap
and convenient alternative to traditional human annotation methods, enabling
the automatic generation of training data by aligning text with external
resources. Despite the many efforts in noise measurement methods, few works
focus on the latent noise distribution between different distant annotation
methods. In this work, we explore the effectiveness and robustness of DS-NER by
two aspects: (1) distant annotation techniques, which encompasses both
traditional rule-based methods and the innovative large language model
supervision approach, and (2) noise assessment, for which we introduce a novel
framework. This framework addresses the challenges by distinctly categorizing
them into the unlabeled-entity problem (UEP) and the noisy-entity problem
(NEP), subsequently providing specialized solutions for each. Our proposed
method achieves significant improvements on eight real-world distant
supervision datasets originating from three different data sources and
involving four distinct annotation techniques, confirming its superiority over
current state-of-the-art methods.

</details>

### [423] [What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization](https://arxiv.org/abs/2505.12474)
*Weixiao Zhou,Junnan Zhu,Gengyao Li,Xianfu Cheng,Xinnian Liang,Feifei Zhai,Zhoujun Li*

Main category: cs.CL

TLDR: 论文研究了LLMs在新任务中的表现，该任务需结合背景知识与讨论进行摘要，以解决现有对话摘要系统仅依赖讨论信息的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有对话摘要系统因仅依赖讨论信息导致外部观察者困惑，需结合背景知识改进。

Method: 任务输出建模为背景和观点摘要，定义两种标准化摘要模式，引入首个高质量标注基准和分层评估框架。

Result: LLMs在背景摘要检索、生成及观点摘要整合上表现不佳，顶级LLMs平均性能不足69%，且缺乏自我评估和修正能力。

Conclusion: 当前LLMs在此任务中表现有限，需进一步改进背景知识利用和自我修正能力。

Abstract: In this work, we investigate the performance of LLMs on a new task that
requires combining discussion with background knowledge for summarization. This
aims to address the limitation of outside observer confusion in existing
dialogue summarization systems due to their reliance solely on discussion
information. To achieve this, we model the task output as background and
opinion summaries and define two standardized summarization patterns. To
support assessment, we introduce the first benchmark comprising high-quality
samples consistently annotated by human experts and propose a novel
hierarchical evaluation framework with fine-grained, interpretable metrics. We
evaluate 12 LLMs under structured-prompt and self-reflection paradigms. Our
findings reveal: (1) LLMs struggle with background summary retrieval,
generation, and opinion summary integration. (2) Even top LLMs achieve less
than 69% average performance across both patterns. (3) Current LLMs lack
adequate self-evaluation and self-correction capabilities for this task.

</details>

### [424] [Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering](https://arxiv.org/abs/2505.12476)
*Xiao Long,Liansheng Zhuang,Chen Shen,Shaotian Yan,Yifei Li,Shafei Wang*

Main category: cs.CL

TLDR: 论文提出了一种名为RTSoG的无训练框架，通过分解复杂问题为子问题并利用奖励引导的树搜索优化知识图谱问答任务。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的KGQA方法忽视历史推理路径的利用，且复杂语义可能导致推理路径不准确。

Method: RTSoG分解问题为子问题，引入奖励引导的SC-MCTS检索加权推理路径，并堆叠生成最终答案。

Result: 在四个数据集上验证有效性，性能提升显著（GrailQA 8.7%，WebQSP 7.0%）。

Conclusion: RTSoG通过优化推理路径利用和语义处理，显著提升了KGQA任务的性能。

Abstract: Recently, large language models (LLMs) have demonstrated impressive
performance in Knowledge Graph Question Answering (KGQA) tasks, which aim to
find answers based on knowledge graphs (KGs) for natural language questions.
Existing LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented
Generation (GraphRAG) paradigm, which first retrieves reasoning paths from the
large KGs, and then generates the answers based on them. However, these methods
emphasize the exploration of new optimal reasoning paths in KGs while ignoring
the exploitation of historical reasoning paths, which may lead to sub-optimal
reasoning paths. Additionally, the complex semantics contained in questions may
lead to the retrieval of inaccurate reasoning paths. To address these issues,
this paper proposes a novel and training-free framework for KGQA tasks called
Reward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original
question into a series of simpler and well-defined sub-questions to handle the
complex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided
by a reward model is introduced to iteratively retrieve weighted reasoning
paths as contextual knowledge. Finally, it stacks the weighted reasoning paths
according to their weights to generate the final answers. Extensive experiments
on four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves
8.7\% and 7.0\% performance improvement over the state-of-the-art method on the
GrailQA and the WebQSP respectively.

</details>

### [425] [KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation](https://arxiv.org/abs/2505.12495)
*Nikita Tatarinov,Vidhyakshaya Kannan,Haricharana Srinivasa,Arnav Raj,Harpreet Singh Anand,Varun Singh,Aditya Luthra,Ravij Lade,Agam Shah,Sudheer Chava*

Main category: cs.CL

TLDR: KG-QAGen框架通过结构化金融协议生成多复杂度QA对，评估模型在长上下文中的表现，发现模型在多跳推理和集合操作上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型的上下文长度增加，需要评估其在长文档中的信息检索和处理能力，现有基准缺乏系统化问题复杂度变化的方法。

Method: 提出KG-QAGen框架，利用金融协议的结构化表示生成多复杂度QA对，覆盖多跳检索、集合操作和答案多样性三个维度。

Result: 构建了20,139对QA（当前长上下文基准中最大），评估13个模型，发现模型在多跳推理和集合操作上表现不佳。

Conclusion: 模型在语义误解和隐含关系处理上存在系统性失败模式，需进一步改进。

Abstract: The increasing context length of modern language models has created a need
for evaluating their ability to retrieve and process information across
extensive documents. While existing benchmarks test long-context capabilities,
they often lack a structured way to systematically vary question complexity. We
introduce KG-QAGen (Knowledge-Graph-based Question-Answer Generation), a
framework that (1) extracts QA pairs at multiple complexity levels (2) by
leveraging structured representations of financial agreements (3) along three
key dimensions -- multi-hop retrieval, set operations, and answer plurality --
enabling fine-grained assessment of model performance across controlled
difficulty levels. Using this framework, we construct a dataset of 20,139 QA
pairs (the largest number among the long-context benchmarks) and open-source a
part of it. We evaluate 13 proprietary and open-source LLMs and observe that
even the best-performing models are struggling with set-based comparisons and
multi-hop logical inference. Our analysis reveals systematic failure modes tied
to semantic misinterpretation and inability to handle implicit relations.

</details>

### [426] [LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection](https://arxiv.org/abs/2505.12507)
*Xu Zheng,Zhuomin Chen,Esteban Schafir,Sipeng Chen,Hojat Allah Salehi,Haifeng Chen,Farhad Shirani,Wei Cheng,Dongsheng Luo*

Main category: cs.CL

TLDR: 提出了一种名为LM$^2$otifs的可解释框架，用于检测机器生成文本（MGT），通过图神经网络和解释性方法实现高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器生成文本检测方法在解释性方面的不足，特别是传统方法难以捕捉复杂词关系的局限性。

Method: LM$^2$otifs框架分三阶段：将文本转化为词共现图，使用图神经网络预测，并通过后处理提取可解释的motifs。

Result: 在多个基准数据集上表现优异，提取的motifs能有效区分人机生成文本，并揭示MGT的独特语言特征。

Conclusion: LM$^2$otifs不仅检测性能强，还提供了多层次的解释性，为文本检测领域提供了新思路。

Abstract: The impressive ability of large language models to generate natural text
across various tasks has led to critical challenges in authorship
authentication. Although numerous detection methods have been developed to
differentiate between machine-generated texts (MGT) and human-generated texts
(HGT), the explainability of these methods remains a significant gap.
Traditional explainability techniques often fall short in capturing the complex
word relationships that distinguish HGT from MGT. To address this limitation,
we present LM$^2$otifs, a novel explainable framework for MGT detection.
Inspired by probabilistic graphical models, we provide a theoretical rationale
for the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networks
to achieve both accurate detection and interpretability. The LM$^2$otifs
pipeline operates in three key stages: first, it transforms text into graphs
based on word co-occurrence to represent lexical dependencies; second, graph
neural networks are used for prediction; and third, a post-hoc explainability
method extracts interpretable motifs, offering multi-level explanations from
individual words to sentence structures. Extensive experiments on multiple
benchmark datasets demonstrate the comparable performance of LM$^2$otifs. The
empirical evaluation of the extracted explainable motifs confirms their
effectiveness in differentiating HGT and MGT. Furthermore, qualitative analysis
reveals distinct and visible linguistic fingerprints characteristic of MGT.

</details>

### [427] [DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design](https://arxiv.org/abs/2505.12511)
*Yanting Li,Jiyue Jiang,Zikang Wang,Ziqian Lin,Dongchen He,Yuheng Shan,Yanruisheng Shao,Jiayi Li,Xiangyu Shi,Jiuming Wang,Yanyu Chen,Yimin Fan,Han Li,Yu Li*

Main category: cs.CL

TLDR: DS-ProGen是一种双结构深度语言模型，结合了骨架几何和表面特征，用于功能性蛋白质设计，实现了61.47%的恢复率。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖骨架坐标或表面特征，无法全面捕捉蛋白质设计的复杂约束，需要一种更全面的方法。

Method: DS-ProGen整合骨架几何和表面描述符，采用下一个氨基酸预测范式，生成功能相关且结构稳定的序列。

Result: 在PRIDE数据集上，DS-ProGen达到61.47%的恢复率，并在与多种生物伙伴的相互作用预测中表现优异。

Conclusion: DS-ProGen展示了多模态结构编码在蛋白质设计中的协同优势，具有强大的功能保留能力。

Abstract: Inverse Protein Folding (IPF) is a critical subtask in the field of protein
design, aiming to engineer amino acid sequences capable of folding correctly
into a specified three-dimensional (3D) conformation. Although substantial
progress has been achieved in recent years, existing methods generally rely on
either backbone coordinates or molecular surface features alone, which
restricts their ability to fully capture the complex chemical and geometric
constraints necessary for precise sequence prediction. To address this
limitation, we present DS-ProGen, a dual-structure deep language model for
functional protein design, which integrates both backbone geometry and
surface-level representations. By incorporating backbone coordinates as well as
surface chemical and geometric descriptors into a next-amino-acid prediction
paradigm, DS-ProGen is able to generate functionally relevant and structurally
stable sequences while satisfying both global and local conformational
constraints. On the PRIDE dataset, DS-ProGen attains the current
state-of-the-art recovery rate of 61.47%, demonstrating the synergistic
advantage of multi-modal structural encoding in protein design. Furthermore,
DS-ProGen excels in predicting interactions with a variety of biological
partners, including ligands, ions, and RNA, confirming its robust functional
retention capabilities.

</details>

### [428] [ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents](https://arxiv.org/abs/2505.12531)
*Navid Madani,Rohini Srihari*

Main category: cs.CL

TLDR: ESC-Judge是一个自动化评估框架，用于比较情感支持LLM的效果，基于Clara Hill的咨询模型，并实现了与人类专家相近的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可扩展且理论支持的方法来评估情感支持LLM的效果，ESC-Judge旨在填补这一空白。

Method: ESC-Judge分三阶段：合成求助者角色、候选模型分别会话、专用LLM评估模型表现。

Result: ESC-Judge在Exploration、Insight和Action决策上与人类专家的一致性分别达到85%、83%和86%。

Conclusion: ESC-Judge为情感支持AI提供了高效、可靠的评估工具，并开源了相关资源以促进透明发展。

Abstract: Large language models (LLMs) increasingly power mental-health chatbots, yet
the field still lacks a scalable, theory-grounded way to decide which model is
most effective to deploy. We present ESC-Judge, the first end-to-end evaluation
framework that (i) grounds head-to-head comparisons of emotional-support LLMs
in Clara Hill's established Exploration-Insight-Action counseling model,
providing a structured and interpretable view of performance, and (ii) fully
automates the evaluation pipeline at scale. ESC-Judge operates in three stages:
first, it synthesizes realistic help-seeker roles by sampling empirically
salient attributes such as stressors, personality, and life history; second, it
has two candidate support agents conduct separate sessions with the same role,
isolating model-specific strategies; and third, it asks a specialized judge LLM
to express pairwise preferences across rubric-anchored skills that span the
Exploration, Insight, and Action spectrum. In our study, ESC-Judge matched
PhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and
86 percent of Action decisions, demonstrating human-level reliability at a
fraction of the cost. All code, prompts, synthetic roles, transcripts, and
judgment scripts are released to promote transparent progress in emotionally
supportive AI.

</details>

### [429] [Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE](https://arxiv.org/abs/2505.12533)
*Varvara Arzt,Allan Hanbury,Michael Wiegand,Gábor Recski,Terra Blevins*

Main category: cs.CL

TLDR: 研究发现关系抽取模型在跨数据集泛化能力较弱，高数据集内性能不代表更好的迁移能力，数据质量是关键。


<details>
  <summary>Details</summary>
Motivation: 评估关系抽取模型是否学习到稳健的关系模式，而非依赖虚假相关性。

Method: 通过跨数据集实验，比较不同数据质量和迁移策略（如微调和少样本学习）的效果。

Result: 模型在未见数据上表现不佳，数据质量比词汇相似性更重要，不同数据质量下最优迁移策略不同。

Conclusion: 关系抽取基准的结构问题（如单关系样本和非标准化负类定义）限制了模型迁移能力。

Abstract: Analysing the generalisation capabilities of relation extraction (RE) models
is crucial for assessing whether they learn robust relational patterns or rely
on spurious correlations. Our cross-dataset experiments find that RE models
struggle with unseen data, even within similar domains. Notably, higher
intra-dataset performance does not indicate better transferability, instead
often signaling overfitting to dataset-specific artefacts. Our results also
show that data quality, rather than lexical similarity, is key to robust
transfer, and the choice of optimal adaptation strategy depends on the quality
of data available: while fine-tuning yields the best cross-dataset performance
with high-quality data, few-shot in-context learning (ICL) is more effective
with noisier data. However, even in these cases, zero-shot baselines
occasionally outperform all cross-dataset results. Structural issues in RE
benchmarks, such as single-relation per sample constraints and non-standardised
negative class definitions, further hinder model transferability.

</details>

### [430] [Disambiguation in Conversational Question Answering in the Era of LLM: A Survey](https://arxiv.org/abs/2505.12543)
*Md Mehrab Tanjim,Yeonjun In,Xiang Chen,Victor S. Bursztyn,Ryan A. Rossi,Sungchul Kim,Guang-Jie Ren,Vaishnavi Muppala,Shun Jiang,Yongsung Kim,Chanyoung Park*

Main category: cs.CL

TLDR: 本文探讨了自然语言处理（NLP）中的歧义问题，特别是在大型语言模型（LLMs）和对话式问答（CQA）背景下的定义、形式及影响。


<details>
  <summary>Details</summary>
Motivation: 解决NLP中的歧义问题对提升语言系统的鲁棒性和可靠性至关重要，尤其是在LLMs能力扩展的背景下。

Method: 通过定义关键术语和概念，分类LLMs支持的消歧方法，并进行比较分析。

Result: 提供了公开数据集的综述，用于评估歧义检测和解决技术，并指出了未来研究方向。

Conclusion: 本文旨在通过全面综述当前研究，推动更强大和可靠的语言系统的发展。

Abstract: Ambiguity remains a fundamental challenge in Natural Language Processing
(NLP) due to the inherent complexity and flexibility of human language. With
the advent of Large Language Models (LLMs), addressing ambiguity has become
even more critical due to their expanded capabilities and applications. In the
context of Conversational Question Answering (CQA), this paper explores the
definition, forms, and implications of ambiguity for language driven systems,
particularly in the context of LLMs. We define key terms and concepts,
categorize various disambiguation approaches enabled by LLMs, and provide a
comparative analysis of their advantages and disadvantages. We also explore
publicly available datasets for benchmarking ambiguity detection and resolution
techniques and highlight their relevance for ongoing research. Finally, we
identify open problems and future research directions, proposing areas for
further investigation. By offering a comprehensive review of current research
on ambiguities and disambiguation with LLMs, we aim to contribute to the
development of more robust and reliable language systems.

</details>

### [431] [Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models](https://arxiv.org/abs/2505.12545)
*Yang Zhao,Pu Wang,Yibo Zhao,Hongru Du,Hao,Yang*

Main category: cs.CL

TLDR: 论文提出TrafficSafe框架，利用LLM改进交通事故预测和特征归因，显著提升性能，并揭示关键风险因素。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以解析多源交通事故数据的复杂关系，限制了关键风险因素的识别能力。

Method: 通过多模态数据集和LLM微调，提出TrafficSafe框架及特征归因方法TrafficSafe Attribution。

Result: TrafficSafe的F1分数比基线提高42%，发现酒驾是严重事故的主因。

Conclusion: TrafficSafe为交通安全研究提供了突破性方法，将AI技术转化为实际应用。

Abstract: Predicting crash events is crucial for understanding crash distributions and
their contributing factors, thereby enabling the design of proactive traffic
safety policy interventions. However, existing methods struggle to interpret
the complex interplay among various sources of traffic crash data, including
numeric characteristics, textual reports, crash imagery, environmental
conditions, and driver behavior records. As a result, they often fail to
capture the rich semantic information and intricate interrelationships embedded
in these diverse data sources, limiting their ability to identify critical
crash risk factors. In this research, we propose TrafficSafe, a framework that
adapts LLMs to reframe crash prediction and feature attribution as text-based
reasoning. A multi-modal crash dataset including 58,903 real-world reports
together with belonged infrastructure, environmental, driver, and vehicle
information is collected and textualized into TrafficSafe Event Dataset. By
customizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves
a 42% average improvement in F1-score over baselines. To interpret these
predictions and uncover contributing factors, we introduce TrafficSafe
Attribution, a sentence-level feature attribution framework enabling
conditional risk analysis. Findings show that alcohol-impaired driving is the
leading factor in severe crashes, with aggressive and impairment-related
behaviors having nearly twice the contribution for severe crashes compared to
other driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal
features during model training, guiding strategic crash data collection for
iterative performance improvements. The proposed TrafficSafe offers a
transformative leap in traffic safety research, providing a blueprint for
translating advanced AI technologies into responsible, actionable, and
life-saving outcomes.

</details>

### [432] [Extracting memorized pieces of (copyrighted) books from open-weight language models](https://arxiv.org/abs/2505.12546)
*A. Feder Cooper,Aaron Gokaslan,Amy B. Cyphert,Christopher De Sa,Mark A. Lemley,Daniel E. Ho,Percy Liang*

Main category: cs.CL

TLDR: 论文探讨了生成式AI版权诉讼中关于大型语言模型（LLM）是否记忆了受版权保护内容的争议，通过实验证明记忆程度因模型和书籍而异，结果对版权案件有复杂影响。


<details>
  <summary>Details</summary>
Motivation: 解决版权诉讼中关于LLM是否记忆受保护内容的极端对立观点，揭示记忆与版权关系的复杂性。

Method: 使用概率提取技术从13个开源LLM中提取Books3数据集内容，通过实验分析记忆程度。

Result: 实验表明LLM确实记忆了部分书籍内容，但记忆程度因模型和书籍不同；最大模型未记忆大部分书籍，但某些模型（如Llama 3.1 70B）几乎完全记忆了特定书籍。

Conclusion: 研究结果表明记忆与版权关系复杂，结果对版权案件有重要但不明确的启示。

Abstract: Plaintiffs and defendants in copyright lawsuits over generative AI often make
sweeping, opposing claims about the extent to which large language models
(LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial
ML and copyright law, we show that these polarized positions dramatically
oversimplify the relationship between memorization and copyright. To do so, we
leverage a recent probabilistic extraction technique to extract pieces of the
Books3 dataset from 13 open-weight LLMs. Through numerous experiments, we show
that it's possible to extract substantial parts of at least some books from
different LLMs. This is evidence that the LLMs have memorized the extracted
text; this memorized content is copied inside the model parameters. But the
results are complicated: the extent of memorization varies both by model and by
book. With our specific experiments, we find that the largest LLMs don't
memorize most books -- either in whole or in part. However, we also find that
Llama 3.1 70B memorizes some books, like Harry Potter and 1984, almost
entirely. We discuss why our results have significant implications for
copyright cases, though not ones that unambiguously favor either side.

</details>

### [433] [The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations](https://arxiv.org/abs/2505.12560)
*Hiram Ring*

Main category: cs.CL

TLDR: 论文介绍了一个自动标记的平行数据集taggedPBC，包含超过1,500种语言的1,800多句标记文本，旨在解决现有跨语言数据集的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言数据集要么覆盖少量语言的大量数据，要么覆盖大量语言的少量数据，限制了揭示人类语言普遍特性的能力。

Method: 开发了一个自动标记的平行数据集taggedPBC，包含1,800多句标记文本，覆盖1,500多种语言。

Result: 数据集标记准确性与现有SOTA标记器和手工标记语料库一致，并提出了新指标N1 ratio，可预测语言基本语序。

Conclusion: taggedPBC是推动基于语料库的跨语言研究的重要一步，已通过GitHub开放供研究和合作。

Abstract: Existing datasets available for crosslinguistic investigations have tended to
focus on large amounts of data for a small group of languages or a small amount
of data for a large number of languages. This means that claims based on these
datasets are limited in what they reveal about universal properties of the
human language faculty. While this has begun to change through the efforts of
projects seeking to develop tagged corpora for a large number of languages,
such efforts are still constrained by limits on resources. The current paper
reports on a large automatically tagged parallel dataset which has been
developed to partially address this issue. The taggedPBC contains more than
1,800 sentences of pos-tagged parallel text data from over 1,500 languages,
representing 133 language families and 111 isolates, dwarfing previously
available resources. The accuracy of tags in this dataset is shown to correlate
well with both existing SOTA taggers for high-resource languages (SpaCy,
Trankit) as well as hand-tagged corpora (Universal Dependencies Treebanks).
Additionally, a novel measure derived from this dataset, the N1 ratio,
correlates with expert determinations of word order in three typological
databases (WALS, Grambank, Autotyp) such that a Gaussian Naive Bayes classifier
trained on this feature can accurately identify basic word order for languages
not in those databases. While much work is still needed to expand and develop
this dataset, the taggedPBC is an important step to enable corpus-based
crosslinguistic investigations, and is made available for research and
collaboration via GitHub.

</details>

### [434] [Enriching Patent Claim Generation with European Patent Dataset](https://arxiv.org/abs/2505.12568)
*Lekang Jiang,Chengzu Li,Stephan Goetz*

Main category: cs.CL

TLDR: 论文介绍了欧洲专利数据集EPD，用于扩展专利权利要求生成的研究范围，提升模型性能，并模拟真实挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖美国专利数据，缺乏多样性和高质量数据，EPD填补了这一空白。

Method: 构建EPD数据集，包含欧洲专利的文本和元数据，用于训练和评估LLMs。

Result: 基于EPD微调的LLMs在权利要求生成质量和跨领域泛化能力上显著优于其他数据集和GPT-4o。

Conclusion: EPD为专利生成研究提供了更全面的基准，并揭示了未来研究的挑战。

Abstract: Drafting patent claims is time-intensive, costly, and requires professional
skill. Therefore, researchers have investigated large language models (LLMs) to
assist inventors in writing claims. However, existing work has largely relied
on datasets from the United States Patent and Trademark Office (USPTO). To
enlarge research scope regarding various jurisdictions, drafting conventions,
and legal standards, we introduce EPD, a European patent dataset. EPD presents
rich textual data and structured metadata to support multiple patent-related
tasks, including claim generation. This dataset enriches the field in three
critical aspects: (1) Jurisdictional diversity: Patents from different offices
vary in legal and drafting conventions. EPD fills a critical gap by providing a
benchmark for European patents to enable more comprehensive evaluation. (2)
Quality improvement: EPD offers high-quality granted patents with finalized and
legally approved texts, whereas others consist of patent applications that are
unexamined or provisional. Experiments show that LLMs fine-tuned on EPD
significantly outperform those trained on previous datasets and even GPT-4o in
claim quality and cross-domain generalization. (3) Real-world simulation: We
propose a difficult subset of EPD to better reflect real-world challenges of
claim generation. Results reveal that all tested LLMs perform substantially
worse on these challenging samples, which highlights the need for future
research.

</details>

### [435] [Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio](https://arxiv.org/abs/2505.12572)
*Hanwen Shen,Ting Ying*

Main category: cs.CL

TLDR: 研究探讨了在生成百万字小说时，人类提供的大纲长度对质量的影响，并提出了一种两阶段分层生成方法以减少语义失真。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过优化大纲长度，在超长小说生成中平衡信息保留与人类工作量。

Method: 采用信息论分析，提出两阶段分层生成流程（大纲→详细大纲→手稿），并通过实验验证其效果。

Result: 两阶段方法显著减少了语义失真，为百万字小说生成提供了实证指导。

Conclusion: 两阶段分层大纲方法在超长小说生成中优于单阶段方法，为作者和研究者提供了实用建议。

Abstract: Writing novels with Large Language Models (LLMs) raises a critical question:
how much human-authored outline is necessary to generate high-quality
million-word novels? While frameworks such as DOME, Plan&Write, and Long Writer
have improved stylistic coherence and logical consistency, they primarily
target shorter novels (10k--100k words), leaving ultra-long generation largely
unexplored. Drawing on insights from recent text compression methods like
LLMZip and LLM2Vec, we conduct an information-theoretic analysis that
quantifies distortion occurring when LLMs compress and reconstruct ultra-long
novels under varying compression-expansion ratios. We introduce a hierarchical
two-stage generation pipeline (outline -> detailed outline -> manuscript) and
find an optimal outline length that balances information preservation with
human effort. Through extensive experimentation with Chinese novels, we
establish that a two-stage hierarchical outline approach significantly reduces
semantic distortion compared to single-stage methods. Our findings provide
empirically-grounded guidance for authors and researchers collaborating with
LLMs to create million-word novels.

</details>

### [436] [Improving Multilingual Language Models by Aligning Representations through Steering](https://arxiv.org/abs/2505.12584)
*Omar Mahmoud,Buddhika Laknath Semage,Thommen George Karimpanal,Santu Rana*

Main category: cs.CL

TLDR: 通过表示引导技术（向单层激活添加学习向量）提升大语言模型（LLMs）处理非英语标记的能力，效果媲美翻译基线并超越现有提示优化方法。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs如何处理非英语标记，填补领域空白。

Method: 使用表示引导技术，结合监督微调（SFT）和人类反馈强化学习（RLHF）优化模型表现。

Result: 单层引导显著提升性能，效果优于现有方法。

Conclusion: 表示引导技术结合SFT和RLHF能有效优化LLMs的多语言能力。

Abstract: In this paper, we investigate how large language models (LLMS) process
non-English tokens within their layer representations, an open question despite
significant advancements in the field. Using representation steering,
specifically by adding a learned vector to a single model layer's activations,
we demonstrate that steering a single model layer can notably enhance
performance. Our analysis shows that this approach achieves results comparable
to translation baselines and surpasses state of the art prompt optimization
methods. Additionally, we highlight how advanced techniques like supervised
fine tuning (\textsc{sft}) and reinforcement learning from human feedback
(\textsc{rlhf}) improve multilingual capabilities by altering representation
spaces. We further illustrate how these methods align with our approach to
reshaping LLMS layer representations.

</details>

### [437] [CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling](https://arxiv.org/abs/2505.12587)
*Aditeya Baral,Allen George Ajith,Roshan Nayak,Mrityunjay Abhijeet Bhanja*

Main category: cs.CL

TLDR: CMLFormer是一种改进的多层双解码器Transformer模型，用于处理代码混合语言的结构和语义动态，通过多任务预训练策略在HASOC-2021基准测试中表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 标准语言模型难以处理代码混合语言中的频繁语言转换问题，因此需要一种新模型来捕捉其结构和语义动态。

Method: 提出CMLFormer，采用共享编码器和同步解码器交叉注意力机制，预训练时结合切换点、翻译注释及多任务目标。

Result: 在HASOC-2021基准测试中，CMLFormer在F1分数、精确度和准确率上优于其他方法，并能有效识别切换点。

Conclusion: CMLFormer的架构和多任务预训练策略对建模代码混合语言具有显著效果。

Abstract: Code-mixed languages, characterized by frequent within-sentence language
transitions, present structural challenges that standard language models fail
to address. In this work, we propose CMLFormer, an enhanced multi-layer
dual-decoder Transformer with a shared encoder and synchronized decoder
cross-attention, designed to model the linguistic and semantic dynamics of
code-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with
switching point and translation annotations with multiple new objectives
specifically aimed at capturing switching behavior, cross-lingual structure,
and code-mixing complexity. Our experiments show that CMLFormer improves F1
score, precision, and accuracy over other approaches on the HASOC-2021
benchmark under select pre-training setups. Attention analyses further show
that it can identify and attend to switching points, validating its sensitivity
to code-mixed structure. These results demonstrate the effectiveness of
CMLFormer's architecture and multi-task pre-training strategy for modeling
code-mixed languages.

</details>

### [438] [PromptPrism: A Linguistically-Inspired Taxonomy for Prompts](https://arxiv.org/abs/2505.12592)
*Sullam Jeoung,Yueyan Chen,Yi Zhang,Shuai Wang,Haibo Ding,Lin Lee Cheong*

Main category: cs.CL

TLDR: PromptPrism是一个语言学启发的分类法，用于系统分析提示的结构和组件，并通过三个层次（功能结构、语义组件和句法模式）优化LLM性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统分析提示的框架，PromptPrism填补了这一空白，帮助理解提示的结构和组件。

Method: 提出PromptPrism分类法，应用于提示优化、数据集分析和敏感性实验。

Result: 实验验证了PromptPrism在提示优化、数据集分析和敏感性分析中的有效性。

Conclusion: PromptPrism为提示的细化、分析和优化提供了基础。

Abstract: Prompts are the interface for eliciting the capabilities of large language
models (LLMs). Understanding their structure and components is critical for
analyzing LLM behavior and optimizing performance. However, the field lacks a
comprehensive framework for systematic prompt analysis and understanding. We
introduce PromptPrism, a linguistically-inspired taxonomy that enables prompt
analysis across three hierarchical levels: functional structure, semantic
component, and syntactic pattern. We show the practical utility of PromptPrism
by applying it to three applications: (1) a taxonomy-guided prompt refinement
approach that automatically improves prompt quality and enhances model
performance across a range of tasks; (2) a multi-dimensional dataset profiling
method that extracts and aggregates structural, semantic, and syntactic
characteristics from prompt datasets, enabling comprehensive analysis of prompt
distributions and patterns; (3) a controlled experimental framework for prompt
sensitivity analysis by quantifying the impact of semantic reordering and
delimiter modifications on LLM performance. Our experimental results validate
the effectiveness of our taxonomy across these applications, demonstrating that
PromptPrism provides a foundation for refining, profiling, and analyzing
prompts.

</details>

### [439] [AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection](https://arxiv.org/abs/2505.12594)
*Tiankai Yang,Junjun Liu,Wingchun Siu,Jiahang Wang,Zhuangzhuang Qian,Chanjuan Song,Cheng Cheng,Xiyang Hu,Yue Zhao*

Main category: cs.CL

TLDR: AD-AGENT是一个基于LLM的多代理框架，通过自然语言指令生成完整的异常检测（AD）流程，帮助非专家用户克服数据多样性和库复杂性挑战。


<details>
  <summary>Details</summary>
Motivation: 异常检测在多个领域至关重要，但数据模态多样化和专用库的复杂性为非专家用户带来了挑战。

Method: AD-AGENT通过协调多个代理（意图解析、数据准备、库选择等），结合短期工作区和长期缓存，集成流行AD库（如PyOD、PyGOD）。

Result: 实验表明AD-AGENT能生成可靠脚本并推荐跨库的竞争性模型。

Conclusion: AD-AGENT开源，支持进一步研究和实际应用。

Abstract: Anomaly detection (AD) is essential in areas such as fraud detection, network
monitoring, and scientific research. However, the diversity of data modalities
and the increasing number of specialized AD libraries pose challenges for
non-expert users who lack in-depth library-specific knowledge and advanced
programming skills. To tackle this, we present AD-AGENT, an LLM-driven
multi-agent framework that turns natural-language instructions into fully
executable AD pipelines. AD-AGENT coordinates specialized agents for intent
parsing, data preparation, library and model selection, documentation mining,
and iterative code generation and debugging. Using a shared short-term
workspace and a long-term cache, the agents integrate popular AD libraries like
PyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that
AD-AGENT produces reliable scripts and recommends competitive models across
libraries. The system is open-sourced to support further research and practical
applications in AD.

</details>

### [440] [Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2505.12616)
*Shujauddin Syed,Ted Pedersen*

Main category: cs.CL

TLDR: 本文介绍了Duluth方法在SemEval-2025任务7中的多语言和跨语言事实核查声明检索中的应用，基于TF-IDF的检索系统在实验中获得最佳表现。


<details>
  <summary>Details</summary>
Motivation: 探索在多语言和跨语言环境中，传统TF-IDF方法在事实核查声明检索任务中的竞争力。

Method: 采用TF-IDF检索系统，实验了不同向量维度和分词策略，最佳配置为15,000特征的词级分词。

Result: 开发集平均success@10得分为0.78，测试集为0.69，表现优于高资源语言，但与顶级系统（0.96）仍有差距。

Conclusion: 尽管神经网络方法在多语言检索中占主导，但优化后的传统TF-IDF方法在资源有限场景下仍具竞争力。

Abstract: This paper presents the Duluth approach to the SemEval-2025 Task 7 on
Multilingual and Crosslingual Fact-Checked Claim Retrieval. We implemented a
TF-IDF-based retrieval system with experimentation on vector dimensions and
tokenization strategies. Our best-performing configuration used word-level
tokenization with a vocabulary size of 15,000 features, achieving an average
success@10 score of 0.78 on the development set and 0.69 on the test set across
ten languages. Our system showed stronger performance on higher-resource
languages but still lagged significantly behind the top-ranked system, which
achieved 0.96 average success@10. Our findings suggest that though advanced
neural architectures are increasingly dominant in multilingual retrieval tasks,
properly optimized traditional methods like TF-IDF remain competitive
baselines, especially in limited compute resource scenarios.

</details>

### [441] [Think Before You Attribute: Improving the Performance of LLMs Attribution Systems](https://arxiv.org/abs/2505.12621)
*João Eduardo Batista,Emil Vatai,Mohamed Wahib*

Main category: cs.CL

TLDR: 论文提出了一种句子级预归属步骤，用于改进检索增强生成（RAG）系统的可信度和准确性，通过分类句子类型来优化归属过程。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在科学领域应用广泛，但缺乏可信赖、可验证的输出限制了其进一步推广。

Method: 提出句子级预归属步骤，将句子分为不可归属、可归属到单一引用和可归属到多个引用三类，以优化归属方法的选择。

Result: 实验表明分类器适用于此任务，并提供了HAGRID数据集的清洁版本和开箱即用的端到端归属系统。

Conclusion: 预归属步骤能有效降低归属的计算复杂度，提高LLMs在科学和高风险场景中的可靠性。

Abstract: Large Language Models (LLMs) are increasingly applied in various science
domains, yet their broader adoption remains constrained by a critical
challenge: the lack of trustworthy, verifiable outputs. Current LLMs often
generate answers without reliable source attribution, or worse, with incorrect
attributions, posing a barrier to their use in scientific and high-stakes
settings, where traceability and accountability are non-negotiable. To be
reliable, attribution systems need high accuracy and retrieve data with short
lengths, i.e., attribute to a sentence within a document rather than a whole
document. We propose a sentence-level pre-attribution step for
Retrieve-Augmented Generation (RAG) systems that classify sentences into three
categories: not attributable, attributable to a single quote, and attributable
to multiple quotes. By separating sentences before attribution, a proper
attribution method can be selected for the type of sentence, or the attribution
can be skipped altogether. Our results indicate that classifiers are
well-suited for this task. In this work, we propose a pre-attribution step to
reduce the computational complexity of attribution, provide a clean version of
the HAGRID dataset, and provide an end-to-end attribution system that works out
of the box.

</details>

### [442] [Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing](https://arxiv.org/abs/2505.12636)
*Jiakuan Xie,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TLDR: 论文提出“表面编辑”概念，指语言模型在知识编辑后仍易生成原始知识的问题，并揭示了导致该问题的两个关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑算法在传统指标上表现优异，但编辑后的模型仍易生成原始知识，这一问题尚未被充分研究。

Method: 通过系统性评估，识别并验证了导致表面编辑的两个关键因素：早期层的残差流和后期层的特定注意力模块。

Result: 研究发现某些注意力头及其输出矩阵的左奇异向量与表面编辑有因果关系，且在表面遗忘任务中表现出一致模式。

Conclusion: 研究揭示了知识编辑中的表面编辑问题，并验证了其关键因素，为未来算法改进提供了方向。

Abstract: Knowledge editing, which aims to update the knowledge encoded in language
models, can be deceptive. Despite the fact that many existing knowledge editing
algorithms achieve near-perfect performance on conventional metrics, the models
edited by them are still prone to generating original knowledge. This paper
introduces the concept of "superficial editing" to describe this phenomenon.
Our comprehensive evaluation reveals that this issue presents a significant
challenge to existing algorithms. Through systematic investigation, we identify
and validate two key factors contributing to this issue: (1) the residual
stream at the last subject position in earlier layers and (2) specific
attention modules in later layers. Notably, certain attention heads in later
layers, along with specific left singular vectors in their output matrices,
encapsulate the original knowledge and exhibit a causal relationship with
superficial editing. Furthermore, we extend our analysis to the task of
superficial unlearning, where we observe consistent patterns in the behavior of
specific attention heads and their corresponding left singular vectors, thereby
demonstrating the robustness and broader applicability of our methodology and
conclusions. Our code is available here.

</details>

### [443] [Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals](https://arxiv.org/abs/2505.12654)
*Yuxin Lin,Yinglin Zheng,Ming Zeng,Wangzheng Shi*

Main category: cs.CL

TLDR: 论文提出了一种多模态信号预测人机对话中轮转和反馈行为的方法，构建了大规模数据集MM-F2F，并开发了端到端框架，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据集的局限性，提升人机对话中轮转和反馈行为的预测能力。

Method: 通过自动数据收集管道构建MM-F2F数据集，提出端到端多模态预测框架。

Result: 在轮转和反馈预测任务中，F1分数分别提升10%和33%。

Conclusion: 数据集和代码公开，为后续研究提供便利，模型在多模态信号处理中表现优异。

Abstract: This paper addresses the gap in predicting turn-taking and backchannel
actions in human-machine conversations using multi-modal signals (linguistic,
acoustic, and visual). To overcome the limitation of existing datasets, we
propose an automatic data collection pipeline that allows us to collect and
annotate over 210 hours of human conversation videos. From this, we construct a
Multi-Modal Face-to-Face (MM-F2F) human conversation dataset, including over
1.5M words and corresponding turn-taking and backchannel annotations from
approximately 20M frames. Additionally, we present an end-to-end framework that
predicts the probability of turn-taking and backchannel actions from
multi-modal signals. The proposed model emphasizes the interrelation between
modalities and supports any combination of text, audio, and video inputs,
making it adaptable to a variety of realistic scenarios. Our experiments show
that our approach achieves state-of-the-art performance on turn-taking and
backchannel prediction tasks, achieving a 10\% increase in F1-score on
turn-taking and a 33\% increase on backchannel prediction. Our dataset and code
are publicly available online to ease of subsequent research.

</details>

### [444] [Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering](https://arxiv.org/abs/2505.12662)
*Xukai Liu,Ye Liu,Shiwen Wu,Yanghai Zhang,Yihao Yuan,Kai Zhang,Qi Liu*

Main category: cs.CL

TLDR: Know3-RAG是一个基于知识图谱的检索增强生成框架，通过结构化知识改进检索、生成和过滤，显著减少幻觉并提升答案可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统存在自适应控制不可靠和幻觉问题，需要改进。

Method: 提出Know3-RAG框架，利用知识图谱嵌入优化检索、生成和过滤三阶段。

Result: 在多个开放域QA基准测试中表现优于基线，减少幻觉并提高可靠性。

Conclusion: Know3-RAG通过结构化知识显著提升了RAG系统的性能和可靠性。

Abstract: Recent advances in large language models (LLMs) have led to impressive
progress in natural language generation, yet their tendency to produce
hallucinated or unsubstantiated content remains a critical concern. To improve
factual reliability, Retrieval-Augmented Generation (RAG) integrates external
knowledge during inference. However, existing RAG systems face two major
limitations: (1) unreliable adaptive control due to limited external knowledge
supervision, and (2) hallucinations caused by inaccurate or irrelevant
references. To address these issues, we propose Know3-RAG, a knowledge-aware
RAG framework that leverages structured knowledge from knowledge graphs (KGs)
to guide three core stages of the RAG process, including retrieval, generation,
and filtering. Specifically, we introduce a knowledge-aware adaptive retrieval
module that employs KG embedding to assess the confidence of the generated
answer and determine retrieval necessity, a knowledge-enhanced reference
generation strategy that enriches queries with KG-derived entities to improve
generated reference relevance, and a knowledge-driven reference filtering
mechanism that ensures semantic alignment and factual accuracy of references.
Experiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG
consistently outperforms strong baselines, significantly reducing
hallucinations and enhancing answer reliability.

</details>

### [445] [Shadow-FT: Tuning Instruct via Base](https://arxiv.org/abs/2505.12716)
*Taiqiang Wu,Runming Yang,Jiayi Li,Pengfei Hu,Ngai Wong,Yujiu Yang*

Main category: cs.CL

TLDR: 论文提出了一种名为Shadow-FT的新框架，通过利用BASE模型来微调INSTRUCT模型，避免了直接微调INSTRUCT模型导致的性能退化问题。该方法无需额外参数，易于实现，并在多个任务上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 直接微调INSTRUCT模型往往效果有限甚至导致性能退化，而对应的BASE模型权重相似度很高，因此提出利用BASE模型进行微调。

Method: 通过微调BASE模型，并将学习到的权重更新直接移植到INSTRUCT模型中，提出了Shadow-FT框架。

Result: 在19个基准测试中，Shadow-FT表现优于传统的全参数和参数高效微调方法，并可扩展到多模态大语言模型（MLLMs）和直接偏好优化（DPO）。

Conclusion: Shadow-FT是一种高效、无需额外参数的微调方法，显著提升了INSTRUCT模型的性能，具有广泛的应用潜力。

Abstract: Large language models (LLMs) consistently benefit from further fine-tuning on
various tasks. However, we observe that directly tuning the INSTRUCT (i.e.,
instruction tuned) models often leads to marginal improvements and even
performance degeneration. Notably, paired BASE models, the foundation for these
INSTRUCT variants, contain highly similar weight values (i.e., less than 2% on
average for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to
tune the INSTRUCT models by leveraging the corresponding BASE models. The key
insight is to fine-tune the BASE model, and then directly graft the learned
weight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no
additional parameters, is easy to implement, and significantly improves
performance. We conduct extensive experiments on tuning mainstream LLMs, such
as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering
coding, reasoning, and mathematical tasks. Experimental results demonstrate
that Shadow-FT consistently outperforms conventional full-parameter and
parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT
can be applied to multimodal large language models (MLLMs) and combined with
direct preference optimization (DPO). Codes and weights are available at
\href{https://github.com/wutaiqiang/Shadow-FT}{Github}.

</details>

### [446] [ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving](https://arxiv.org/abs/2505.12717)
*Haoyuan Wu,Xueyi Chen,Rui Ming,Jilong Gao,Shoubo Hu,Zhuolun He,Bei Yu*

Main category: cs.CL

TLDR: 论文提出了一种基于树状思维（ToT）的强化学习框架ToTRL，用于改进大型语言模型（LLM）的推理能力，通过并行生成和评估推理分支，提升性能并减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 长链思维（CoT）推理存在冗长输出和缺乏系统性的问题，树状思维（ToT）提供了一种更高效的并行推理方法。

Method: 引入ToTRL框架，结合强化学习和规则奖励，指导LLM从序列CoT策略转向并行ToT策略，并通过解谜游戏训练模型。

Result: 实验表明，ToTQwen3-8B模型在复杂推理任务中显著提升了性能和效率。

Conclusion: ToTRL框架有效改进了LLM的推理能力，为复杂任务提供了更高效的解决方案。

Abstract: Large language models (LLMs) demonstrate significant reasoning capabilities,
particularly through long chain-of-thought (CoT) processes, which can be
elicited by reinforcement learning (RL). However, prolonged CoT reasoning
presents limitations, primarily verbose outputs due to excessive introspection.
The reasoning process in these LLMs often appears to follow a trial-and-error
methodology rather than a systematic, logical deduction. In contrast,
tree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling
reasoning as an exploration within a tree structure. This reasoning structure
facilitates the parallel generation and evaluation of multiple reasoning
branches, allowing for the active identification, assessment, and pruning of
unproductive paths. This process can potentially lead to improved performance
and reduced token costs. Building upon the long CoT capability of LLMs, we
introduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a
rule-based reward. ToTRL is designed to guide LLMs in developing the parallel
ToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs
as players in a puzzle game during the ToTRL training process. Solving puzzle
games inherently necessitates exploring interdependent choices and managing
multiple constraints, which requires the construction and exploration of a
thought tree, providing challenging tasks for cultivating the ToT reasoning
capability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,
trained with our ToTRL, achieves significant improvement in performance and
reasoning efficiency on complex reasoning tasks.

</details>

### [447] [Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework](https://arxiv.org/abs/2505.12718)
*Jingyang Peng,Wenyuan Shen,Jiarui Rao,Jionghao Lin*

Main category: cs.CL

TLDR: 提出了一种自动化评估生成式AI教育内容偏见的方法，结合上下文嵌入关联测试和提示工程词提取技术，验证了其高可靠性和一致性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在教育内容创作中广泛应用，但其中潜在的偏见（如性别、种族或国家刻板印象）引发了伦理和教育问题，缺乏系统性评估方法。

Method: 采用上下文嵌入关联测试与提示工程词提取方法，结合检索增强生成框架，对AI生成的导师培训文本进行偏见评估。

Result: 自动化与人工评估的词集高度一致（Pearson相关系数r=0.993），表明方法可靠且一致。

Conclusion: 该方法减少了主观性，提升了公平性、可扩展性和可重复性，适用于生成式AI教育内容的偏见审计。

Abstract: Recent advances in Generative Artificial Intelligence (GenAI) have
transformed educational content creation, particularly in developing tutor
training materials. However, biases embedded in AI-generated content--such as
gender, racial, or national stereotypes--raise significant ethical and
educational concerns. Despite the growing use of GenAI, systematic methods for
detecting and evaluating such biases in educational materials remain limited.
This study proposes an automated bias assessment approach that integrates the
Contextualized Embedding Association Test with a prompt-engineered word
extraction method within a Retrieval-Augmented Generation framework. We applied
this method to AI-generated texts used in tutor training lessons. Results show
a high alignment between the automated and manually curated word sets, with a
Pearson correlation coefficient of r = 0.993, indicating reliable and
consistent bias assessment. Our method reduces human subjectivity and enhances
fairness, scalability, and reproducibility in auditing GenAI-produced
educational content.

</details>

### [448] [On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding](https://arxiv.org/abs/2505.12723)
*Haoyuan Wu,Rui Ming,Jilong Gao,Hangyu Zhao,Xueyi Chen,Yikai Yang,Haisheng Zheng,Zhuolun He,Bei Yu*

Main category: cs.CL

TLDR: 通过代码翻译任务和新型强化学习框架OORL（结合策略内和策略外学习）训练大语言模型（LLMs），提升其在多种编程语言中的代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在不同编程语言间性能差异显著的问题，促进跨语言编码能力的迁移。

Method: 提出OORL框架，结合策略内强化学习（基于单元测试的规则奖励）和策略外学习（GEPO方法，利用中间表示组优化偏好）。

Result: 实验表明，该方法显著提升了LLMs在多种编程语言代码基准测试中的性能。

Conclusion: OORL和GEPO方法有效增强了LLMs对代码功能的理解及跨语言代码关系的掌握。

Abstract: Large language models (LLMs) achieve remarkable performance in code
generation tasks. However, a significant performance disparity persists between
popular programming languages (e.g., Python, C++) and others. To address this
capability gap, we leverage the code translation task to train LLMs, thereby
facilitating the transfer of coding proficiency across diverse programming
languages. Moreover, we introduce OORL for training, a novel reinforcement
learning (RL) framework that integrates on-policy and off-policy strategies.
Within OORL, on-policy RL is applied during code translation, guided by a
rule-based reward signal derived from unit tests. Complementing this
coarse-grained rule-based reward, we propose Group Equivalent Preference
Optimization (GEPO), a novel preference optimization method. Specifically, GEPO
trains the LLM using intermediate representations (IRs) groups. LLMs can be
guided to discern IRs equivalent to the source code from inequivalent ones,
while also utilizing signals about the mutual equivalence between IRs within
the group. This process allows LLMs to capture nuanced aspects of code
functionality. By employing OORL for training with code translation tasks, LLMs
improve their recognition of code functionality and their understanding of the
relationships between code implemented in different languages. Extensive
experiments demonstrate that our OORL for LLMs training with code translation
tasks achieves significant performance improvements on code benchmarks across
multiple programming languages.

</details>

### [449] [What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma](https://arxiv.org/abs/2505.12727)
*Han Meng,Yancan Chen,Yunan Li,Yitian Yang,Jungup Lee,Renwen Zhang,Yi-Chieh Lee*

Main category: cs.CL

TLDR: 论文提出了一个专家标注、理论支持的语料库，用于训练神经模型精细分类心理健康污名，并评估了现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 心理健康污名是一个普遍的社会问题，但现有训练资源有限且缺乏理论基础。

Method: 使用专家标注的人类-聊天机器人访谈语料库（4,141个片段，684名参与者），并评估了最先进的神经模型。

Result: 实验揭示了污名检测的挑战，并提供了可用于未来研究的语料库。

Conclusion: 该数据集有助于计算检测、中和及对抗心理健康污名的研究。

Abstract: Mental-health stigma remains a pervasive social problem that hampers
treatment-seeking and recovery. Existing resources for training neural models
to finely classify such stigma are limited, relying primarily on social-media
or synthetic data without theoretical underpinnings. To remedy this gap, we
present an expert-annotated, theory-informed corpus of human-chatbot
interviews, comprising 4,141 snippets from 684 participants with documented
socio-cultural backgrounds. Our experiments benchmark state-of-the-art neural
models and empirically unpack the challenges of stigma detection. This dataset
can facilitate research on computationally detecting, neutralizing, and
counteracting mental-health stigma.

</details>

### [450] [ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL](https://arxiv.org/abs/2505.12768)
*Yaxun Dai,Wenxuan Xie,Xialie Zhuang,Tianyu Yang,Yiying Yang,Haiqin Yang,Yuhang Zhao,Pingfu Chao,Wenhao Jiang*

Main category: cs.CL

TLDR: ReEx-SQL提出了一种动态整合执行反馈的Text-to-SQL框架，通过执行感知的强化学习提升SQL生成准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅将执行反馈作为事后校正信号，未能动态整合到生成过程中，导致推理错误无法及时修正。

Method: ReEx-SQL引入执行感知推理范式，结合结构化提示和逐步展开策略，动态调整推理路径，并通过复合奖励函数监督学习。

Result: 在Spider和BIRD上分别达到88.8%和64.9%的准确率，推理时间减少51.9%。

Conclusion: ReEx-SQL通过动态整合执行反馈和树状解码策略，显著提升了Text-to-SQL的性能和效率。

Abstract: In Text-to-SQL, execution feedback is essential for guiding large language
models (LLMs) to reason accurately and generate reliable SQL queries. However,
existing methods treat execution feedback solely as a post-hoc signal for
correction or selection, failing to integrate it into the generation process.
This limitation hinders their ability to address reasoning errors as they
occur, ultimately reducing query accuracy and robustness. To address this
issue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement
Learning), a framework for Text-to-SQL that enables models to interact with the
database during decoding and dynamically adjust their reasoning based on
execution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm
that interleaves intermediate SQL execution into reasoning paths, facilitating
context-sensitive revisions. It achieves this through structured prompts with
markup tags and a stepwise rollout strategy that integrates execution feedback
into each stage of generation. To supervise policy learning, we develop a
composite reward function that includes an exploration reward, explicitly
encouraging effective database interaction. Additionally, ReEx-SQL adopts a
tree-based decoding strategy to support exploratory reasoning, enabling dynamic
expansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on
Spider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning
baseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving
85.2% on Spider-Realistic with leading performance. In addition, its
tree-structured decoding improves efficiency and performance over linear
decoding, reducing inference time by 51.9% on the BIRD development set.

</details>

### [451] [A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone](https://arxiv.org/abs/2505.12781)
*Jitai Hao,Qiang Huang,Hao Liu,Xinyan Xiao,Zhaochun Ren,Jun Yu*

Main category: cs.CL

TLDR: 论文提出了一种名为Low-Rank Clone (LRC)的高效预训练方法，用于训练小型语言模型（SLMs），通过低秩投影矩阵实现软剪枝和激活对齐，显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练小型语言模型时面临信息丢失、表示对齐效率低和FFN激活利用不足等问题。

Method: LRC通过训练低秩投影矩阵，实现软剪枝和激活对齐，无需显式对齐模块。

Result: 实验表明，LRC在仅使用20B tokens的情况下，性能达到或超过基于万亿tokens训练的SOTA模型，训练效率提升1000倍。

Conclusion: LRC是一种高效的小型语言模型预训练方法，显著提升了知识转移和训练效率。

Abstract: Training high-performing Small Language Models (SLMs) remains costly, even
with knowledge distillation and pruning from larger teacher models. Existing
work often faces three key challenges: (1) information loss from hard pruning,
(2) inefficient alignment of representations, and (3) underutilization of
informative activations, particularly from Feed-Forward Networks (FFNs). To
address these challenges, we introduce Low-Rank Clone (LRC), an efficient
pre-training method that constructs SLMs aspiring to behavioral equivalence
with strong teacher models. LRC trains a set of low-rank projection matrices
that jointly enable soft pruning by compressing teacher weights, and activation
clone by aligning student activations, including FFN signals, with those of the
teacher. This unified design maximizes knowledge transfer while removing the
need for explicit alignment modules. Extensive experiments with open-source
teachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC
matches or surpasses state-of-the-art models trained on trillions of
tokens--while using only 20B tokens, achieving over 1,000x training efficiency.
Our codes and model checkpoints are available at
https://github.com/CURRENTF/LowRankClone and
https://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.

</details>

### [452] [EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs](https://arxiv.org/abs/2505.12792)
*Wenhao Zhu,Yuhang Xie,Guojie Song,Xin Zhang*

Main category: cs.CL

TLDR: EAVIT框架结合本地可调优和在线黑盒LLMs，通过小型本地模型生成初始价值估计，优化在线LLMs输入，显著减少计算成本并提升准确性。


<details>
  <summary>Details</summary>
Motivation: 传统NLP模型（如BERT）在文本数据中识别人类价值的能力不如新兴LLMs（如GPTs），但在线LLMs处理长上下文时性能下降且计算成本高。

Method: 提出EAVIT框架，结合本地小型模型（价值检测器）和在线LLMs，通过解释性训练和数据生成技术优化输入提示。

Result: EAVIT将输入令牌数量减少至1/6，同时性能优于传统NLP方法和其他基于LLM的策略。

Conclusion: EAVIT是一种高效准确的人类价值识别框架，解决了在线LLMs的局限性。

Abstract: The rapid evolution of large language models (LLMs) has revolutionized
various fields, including the identification and discovery of human values
within text data. While traditional NLP models, such as BERT, have been
employed for this task, their ability to represent textual data is
significantly outperformed by emerging LLMs like GPTs. However, the performance
of online LLMs often degrades when handling long contexts required for value
identification, which also incurs substantial computational costs. To address
these challenges, we propose EAVIT, an efficient and accurate framework for
human value identification that combines the strengths of both locally
fine-tunable and online black-box LLMs. Our framework employs a value detector
- a small, local language model - to generate initial value estimations. These
estimations are then used to construct concise input prompts for online LLMs,
enabling accurate final value identification. To train the value detector, we
introduce explanation-based training and data generation techniques
specifically tailored for value identification, alongside sampling strategies
to optimize the brevity of LLM input prompts. Our approach effectively reduces
the number of input tokens by up to 1/6 compared to directly querying online
LLMs, while consistently outperforming traditional NLP methods and other
LLM-based strategies.

</details>

### [453] [Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models](https://arxiv.org/abs/2505.12808)
*Yanbin Yin,Kun Zhou,Zhen Wang,Xiangdong Zhang,Yifei Shao,Shibo Hao,Yi Gu,Jieyuan Liu,Somanshu Singla,Tianyang Liu,Eric P. Xing,Zhengzhong Liu,Haojian Jin,Zhiting Hu*

Main category: cs.CL

TLDR: 提出了一种名为Decentralized Arena（dearena）的自动化框架，利用所有LLMs的集体智慧相互评估，解决了现有基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试方法存在封闭性问题饱和、人工评估成本高以及自动化方法依赖单一模型导致偏见等问题。

Method: 采用民主化的成对评估方法，结合粗到细的排名算法和自动问题选择策略，实现高效扩展。

Result: 在66个LLMs上的实验表明，dearena与人类判断的相关性高达97%，同时显著降低成本。

Conclusion: dearena提供了一种高效、低成本的自动化评估框架，解决了现有基准测试的局限性。

Abstract: The recent explosion of large language models (LLMs), each with its own
general or specialized strengths, makes scalable, reliable benchmarking more
urgent than ever. Standard practices nowadays face fundamental trade-offs:
closed-ended question-based benchmarks (eg MMLU) struggle with saturation as
newer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely
on costly and slow human judges. Recently, automated methods (eg
LLM-as-a-judge) shed light on the scalability, but risk bias by relying on one
or a few "authority" models. To tackle these issues, we propose Decentralized
Arena (dearena), a fully automated framework leveraging collective intelligence
from all LLMs to evaluate each other. It mitigates single-model judge bias by
democratic, pairwise evaluation, and remains efficient at scale through two key
components: (1) a coarse-to-fine ranking algorithm for fast incremental
insertion of new models with sub-quadratic complexity, and (2) an automatic
question selection strategy for the construction of new evaluation dimensions.
Across extensive experiments across 66 LLMs, dearena attains up to 97%
correlation with human judgements, while significantly reducing the cost. Our
code and data will be publicly released on
https://github.com/maitrix-org/de-arena.

</details>

### [454] [PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs](https://arxiv.org/abs/2505.12814)
*Xilong Cheng,Yunxiao Qin,Yuting Tan,Zhengnan Li,Ye Wang,Hongjiang Xiao,Yuan Zhang*

Main category: cs.CL

TLDR: PsyMem框架通过细粒度心理属性和显式记忆控制改进角色扮演LLM，提升角色扮演的可靠性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖浅层文本描述或简单指标，无法充分建模角色内外维度，且记忆一致性不足。

Method: PsyMem结合26种心理指标和显式记忆对齐训练，动态控制角色响应。

Result: 在Qwen2.5-7B-Instruct上训练的PsyMem-Qwen在人类相似性和角色保真度上表现最佳。

Conclusion: PsyMem显著提升了角色扮演LLM的可靠性和应用潜力。

Abstract: Existing LLM-based role-playing methods often rely on superficial textual
descriptions or simplistic metrics, inadequately modeling both intrinsic and
extrinsic character dimensions. Additionally, they typically simulate character
memory with implicit model knowledge or basic retrieval augment generation
without explicit memory alignment, compromising memory consistency. The two
issues weaken reliability of role-playing LLMs in several applications, such as
trustworthy social simulation. To address these limitations, we propose PsyMem,
a novel framework integrating fine-grained psychological attributes and
explicit memory control for role-playing. PsyMem supplements textual
descriptions with 26 psychological indicators to detailed model character.
Additionally, PsyMem implements memory alignment training, explicitly trains
the model to align character's response with memory, thereby enabling dynamic
memory-controlled responding during inference. By training Qwen2.5-7B-Instruct
on our specially designed dataset (including 5,414 characters and 38,962
dialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,
outperforms baseline models in role-playing, achieving the best performance in
human-likeness and character fidelity.

</details>

### [455] [SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models](https://arxiv.org/abs/2505.12821)
*Han Sun,Zhen Sun,Zongmin Zhang,Linzhao Jia,Wei Shao,Min Zhang*

Main category: cs.CL

TLDR: 论文提出了一种名为SynDec的新方法，通过自动合成高质量提示并增强其在解码过程中的作用，解决了大语言模型在任意风格转换中的依赖手动提示和固有风格偏见问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在文本风格转换中表现突出，但在任意风格转换中面临依赖手动提示和固有风格偏见的挑战。

Method: 提出Synthesize-then-Decode（SynDec）方法，通过选择代表性样本、四维风格分析和候选重排合成提示，并在解码阶段通过最大化输出概率对比增强效果。

Result: 在六个基准测试中的五个上优于现有方法，例如在现代英语到伊丽莎白英语转换中准确率提升9%。

Conclusion: SynDec方法有效解决了大语言模型在风格转换中的挑战，并通过实验验证了其优越性。

Abstract: Large Language Models (LLMs) are emerging as dominant forces for textual
style transfer. However, for arbitrary style transfer, LLMs face two key
challenges: (1) considerable reliance on manually-constructed prompts and (2)
rigid stylistic biases inherent in LLMs. In this paper, we propose a novel
Synthesize-then-Decode (SynDec) approach, which automatically synthesizes
high-quality prompts and amplifies their roles during decoding process.
Specifically, our approach synthesizes prompts by selecting representative
few-shot samples, conducting a four-dimensional style analysis, and reranking
the candidates. At LLM decoding stage, the TST effect is amplified by
maximizing the contrast in output probabilities between scenarios with and
without the synthesized prompt, as well as between prompts and negative
samples. We conduct extensive experiments and the results show that SynDec
outperforms existing state-of-the-art LLM-based methods on five out of six
benchmarks (e.g., achieving up to a 9\% increase in accuracy for
modern-to-Elizabethan English transfer). Detailed ablation studies further
validate the effectiveness of SynDec.

</details>

### [456] [Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering](https://arxiv.org/abs/2505.12831)
*Zifeng Cheng,Zhonghui Wang,Yuchen Fu,Zhiwei Jiang,Yafeng Yin,Cong Wang,Qing Gu*

Main category: cs.CL

TLDR: 论文提出了一种对比提示（CP）方法，通过引入辅助提示来优化句子嵌入的提取，避免非必要信息的干扰，提升语义编码能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过提示工程从大型语言模型（LLM）中提取句子嵌入，但最后一个标记仍包含过多非必要信息（如停用词），限制了编码能力。

Method: 提出对比提示（CP）方法，通过辅助提示与现有提示对比，引导模型编码句子的核心语义而非非必要信息。

Result: 在语义文本相似性（STS）任务和下游分类任务中，CP方法显著提升了现有基于提示方法的性能。

Conclusion: CP是一种即插即用的推理时干预方法，可结合多种基于提示的方法，有效提升句子嵌入的质量。

Abstract: Extracting sentence embeddings from large language models (LLMs) is a
practical direction, as it requires neither additional data nor fine-tuning.
Previous studies usually focus on prompt engineering to guide LLMs to encode
the core semantic information of the sentence into the embedding of the last
token. However, the last token in these methods still encodes an excess of
non-essential information, such as stop words, limiting its encoding capacity.
To this end, we propose a Contrastive Prompting (CP) method that introduces an
extra auxiliary prompt to elicit better sentence embedding. By contrasting with
the auxiliary prompt, CP can steer existing prompts to encode the core
semantics of the sentence, rather than non-essential information. CP is a
plug-and-play inference-time intervention method that can be combined with
various prompt-based methods. Extensive experiments on Semantic Textual
Similarity (STS) tasks and downstream classification tasks demonstrate that our
method can improve the performance of existing prompt-based methods across
different LLMs. Our code will be released at https://github.com/zifengcheng/CP.

</details>

### [457] [FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models](https://arxiv.org/abs/2505.12835)
*Hengxing Cai,Jinhan Dong,Jingjun Tan,Jingcheng Deng,Sihang Li,Zhifeng Gao,Haidong Wang,Zicheng Su,Agachai Sumalee,Renxin Zhong*

Main category: cs.CL

TLDR: FlightGPT是一个基于视觉语言模型的新型无人机视觉与语言导航框架，通过两阶段训练和链式思维推理机制，解决了多模态融合不足、泛化能力弱和可解释性差的问题，在CityNav数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 无人机视觉与语言导航在灾害响应、物流配送等应用中至关重要，但现有方法在多模态融合、泛化能力和可解释性方面存在不足。

Method: 提出FlightGPT框架，采用两阶段训练：监督微调（SFT）和基于复合奖励的组相对策略优化（GRPO），并引入链式思维（CoT）推理机制。

Result: 在CityNav数据集上，FlightGPT在未见环境中成功率比最强基线高9.22%，达到最先进性能。

Conclusion: FlightGPT通过改进训练和推理机制，显著提升了无人机视觉与语言导航的性能和可解释性。

Abstract: Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital
for applications such as disaster response, logistics delivery, and urban
inspection. However, existing methods often struggle with insufficient
multimodal fusion, weak generalization, and poor interpretability. To address
these challenges, we propose FlightGPT, a novel UAV VLN framework built upon
Vision-Language Models (VLMs) with powerful multimodal perception capabilities.
We design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)
using high-quality demonstrations to improve initialization and structured
reasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by
a composite reward that considers goal accuracy, reasoning quality, and format
compliance, to enhance generalization and adaptability. Furthermore, FlightGPT
introduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve
decision interpretability. Extensive experiments on the city-scale dataset
CityNav demonstrate that FlightGPT achieves state-of-the-art performance across
all scenarios, with a 9.22\% higher success rate than the strongest baseline in
unseen environments. Our implementation is publicly available.

</details>

### [458] [The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting](https://arxiv.org/abs/2505.12837)
*Christian Braun,Alexander Lilienbeck,Daniel Mentjukov*

Main category: cs.CL

TLDR: 研究探讨了输入文本结构和提示工程对GPT-4o和GPT-4.1在法律问答任务中性能的影响，发现GPT-4.1对结构敏感，优化结构和提示可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索法律合同的结构对LLM处理的影响，特别是在高风险的法学应用中。

Method: 比较不同输入格式（如原始文本、OCR提取文本、GPT-4o Vision提取文本和Markdown）对模型性能的影响，并评估提示工程的优化效果。

Result: GPT-4.1对结构敏感，优化结构和提示可提升20-33个百分点的准确率；GPT-4o表现稳健但整体性能较低。

Conclusion: 输入结构优化和提示设计对LLM性能至关重要，尤其是在法学应用中。

Abstract: Legal contracts possess an inherent, semantically vital structure (e.g.,
sections, clauses) that is crucial for human comprehension but whose impact on
LLM processing remains under-explored. This paper investigates the effects of
explicit input text structure and prompt engineering on the performance of
GPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the
CUAD. We compare model exact-match accuracy across various input formats:
well-structured plain-text (human-generated from CUAD), plain-text cleaned of
line breaks, extracted plain-text from Azure OCR, plain-text extracted by
GPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o
Vision. To give an indication of the impact of possible prompt engineering, we
assess the impact of shifting task instructions to the system prompt and
explicitly informing the model about the structured nature of the input. Our
findings reveal that GPT-4o demonstrates considerable robustness to variations
in input structure, but lacks in overall performance. Conversely, GPT-4.1's
performance is markedly sensitive; poorly structured inputs yield suboptimal
results (but identical with GPT-4o), while well-structured formats (original
CUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by
~20 percentage points. Optimizing the system prompt to include task details and
an advisory about structured input further elevates GPT-4.1's accuracy by an
additional ~10-13 percentage points, with Markdown ultimately achieving the
highest performance under these conditions (79 percentage points overall
exact-match accuracy). This research empirically demonstrates that while newer
models exhibit greater resilience, careful input structuring and strategic
prompt design remain critical for optimizing the performance of LLMs, and can
significantly affect outcomes in high-stakes legal applications.

</details>

### [459] [Re-identification of De-identified Documents with Autoregressive Infilling](https://arxiv.org/abs/2505.12859)
*Lucas Georges Gabriel Charpentier,Pierre Lison*

Main category: cs.CL

TLDR: 论文提出了一种基于RAG的反向去标识化方法，通过背景知识库重新识别被掩码的个人信息，实验表明80%的掩码信息可被恢复。


<details>
  <summary>Details</summary>
Motivation: 研究去标识化方法的鲁棒性，验证个人信息掩码后是否仍可被重新识别。

Method: 采用两步法：检索器从背景知识库中筛选相关段落，填充模型推断掩码内容，迭代直至所有掩码被替换。

Result: 在三个数据集上测试，80%的掩码信息可被恢复，且背景知识越多，重新识别准确率越高。

Conclusion: 去标识化方法存在漏洞，背景知识可显著提升重新识别效果。

Abstract: Documents revealing sensitive information about individuals must typically be
de-identified. This de-identification is often done by masking all mentions of
personally identifiable information (PII), thereby making it more difficult to
uncover the identity of the person(s) in question. To investigate the
robustness of de-identification methods, we present a novel, RAG-inspired
approach that attempts the reverse process of re-identification based on a
database of documents representing background knowledge. Given a text in which
personal identifiers have been masked, the re-identification proceeds in two
steps. A retriever first selects from the background knowledge passages deemed
relevant for the re-identification. Those passages are then provided to an
infilling model which seeks to infer the original content of each text span.
This process is repeated until all masked spans are replaced. We evaluate the
re-identification on three datasets (Wikipedia biographies, court rulings and
clinical notes). Results show that (1) as many as 80% of de-identified text
spans can be successfully recovered and (2) the re-identification accuracy
increases along with the level of background knowledge.

</details>

### [460] [LEXam: Benchmarking Legal Reasoning on 340 Law Exams](https://arxiv.org/abs/2505.12864)
*Yu Fan,Jingwei Ni,Jakob Merane,Etienne Salimbeni,Yang Tian,Yoan Hermstrüwer,Yinya Huang,Mubashara Akhtar,Florian Geering,Oliver Dreyer,Daniel Brunner,Markus Leippold,Mrinmaya Sachan,Alexander Stremitzer,Christoph Engel,Elliott Ash,Joel Niklaus*

Main category: cs.CL

TLDR: LEXam是一个新的法律推理基准测试，包含340门法律课程的4,886个问题，用于评估大型语言模型（LLMs）在法律推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在长形式法律推理中的挑战，并提供更全面的评估方法。

Method: 从法律考试中构建LEXam数据集，包含开放式和选择题，并采用LLM-as-a-Judge范式进行评估。

Result: LLMs在需要结构化多步推理的开放式问题上表现不佳，但数据集能有效区分不同能力的模型。

Conclusion: LEXam为评估法律推理质量提供了可扩展的方法，超越了简单的准确性指标。

Abstract: Long-form legal reasoning remains a key challenge for large language models
(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a
novel benchmark derived from 340 law exams spanning 116 law school courses
across a range of subjects and degree levels. The dataset comprises 4,886 law
exam questions in English and German, including 2,841 long-form, open-ended
questions and 2,045 multiple-choice questions. Besides reference answers, the
open questions are also accompanied by explicit guidance outlining the expected
legal reasoning approach such as issue spotting, rule recall, or rule
application. Our evaluation on both open-ended and multiple-choice questions
present significant challenges for current LLMs; in particular, they notably
struggle with open questions that require structured, multi-step legal
reasoning. Moreover, our results underscore the effectiveness of the dataset in
differentiating between models with varying capabilities. Adopting an
LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate
how model-generated reasoning steps can be evaluated consistently and
accurately. Our evaluation setup provides a scalable method to assess legal
reasoning quality beyond simple accuracy metrics. Project page:
https://lexam-benchmark.github.io/

</details>

### [461] [GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation](https://arxiv.org/abs/2505.12888)
*Jialun Zhong,Yanzeng Li,Sen Hu,Yang Zhang,Teng Xu,Lei Zou*

Main category: cs.CL

TLDR: 论文提出了一种基于图辅助提示（GAP）的框架，用于解决基于对话的药物推荐任务中的挑战，如忽略细粒度信息和非事实性响应。


<details>
  <summary>Details</summary>
Motivation: 医疗对话系统中的药物推荐任务需要关注患者与医生的交互细节，而现有大型语言模型（LLM）可能忽略关键信息或生成非事实性响应。

Method: GAP框架通过构建患者中心图并结合外部医学知识图谱，生成丰富的查询和提示，以减少非事实性响应。

Result: 实验表明，GAP在对话式药物推荐任务中表现优异，并在动态诊断访谈中展现出潜力。

Conclusion: GAP框架有效解决了LLM在医疗对话中的局限性，提升了药物推荐的准确性和安全性。

Abstract: Medication recommendations have become an important task in the healthcare
domain, especially in measuring the accuracy and safety of medical dialogue
systems (MDS). Different from the recommendation task based on electronic
health records (EHRs), dialogue-based medication recommendations require
research on the interaction details between patients and doctors, which is
crucial but may not exist in EHRs. Recent advancements in large language models
(LLM) have extended the medical dialogue domain. These LLMs can interpret
patients' intent and provide medical suggestions including medication
recommendations, but some challenges are still worth attention. During a
multi-turn dialogue, LLMs may ignore the fine-grained medical information or
connections across the dialogue turns, which is vital for providing accurate
suggestions. Besides, LLMs may generate non-factual responses when there is a
lack of domain-specific knowledge, which is more risky in the medical domain.
To address these challenges, we propose a \textbf{G}raph-\textbf{A}ssisted
\textbf{P}rompts (\textbf{GAP}) framework for dialogue-based medication
recommendation. It extracts medical concepts and corresponding states from
dialogue to construct an explicitly patient-centric graph, which can describe
the neglected but important information. Further, combined with external
medical knowledge graphs, GAP can generate abundant queries and prompts, thus
retrieving information from multiple sources to reduce the non-factual
responses. We evaluate GAP on a dialogue-based medication recommendation
dataset and further explore its potential in a more difficult scenario,
dynamically diagnostic interviewing. Extensive experiments demonstrate its
competitive performance when compared with strong baselines.

</details>

### [462] [On the Thinking-Language Modeling Gap in Large Language Models](https://arxiv.org/abs/2505.12896)
*Chenxi Liu,Yongqiang Chen,Tongliang Liu,James Cheng,Bo Han,Kun Zhang*

Main category: cs.CL

TLDR: 论文提出了一种新的提示技术Language-of-Thoughts（LoT），用于减少大型语言模型（LLMs）中的语言建模偏差，并提高其推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 研究表明，LLMs在模拟人类语言时容易吸收语言偏差，导致推理过程中偏离思维链条，从而误导模型对前提的理解。

Method: 提出LoT技术，通过调整表达顺序和使用的词汇，引导LLMs更全面地表达相关信息，而非直接从部分信息中引出思维链条。

Result: LoT显著减少了LLMs中的语言建模偏差，并在多种推理任务中提升了模型性能。

Conclusion: LoT是一种简单有效的技术，能够缩小语言与思维建模之间的差距，提升LLMs的推理能力。

Abstract: System 2 reasoning is one of the defining characteristics of intelligence,
which requires slow and logical thinking. Human conducts System 2 reasoning via
the language of thoughts that organizes the reasoning process as a causal
sequence of mental language, or thoughts. Recently, it has been observed that
System 2 reasoning can be elicited from Large Language Models (LLMs)
pre-trained on large-scale natural languages. However, in this work, we show
that there is a significant gap between the modeling of languages and thoughts.
As language is primarily a tool for humans to share knowledge and thinking,
modeling human language can easily absorb language biases into LLMs deviated
from the chain of thoughts in minds. Furthermore, we show that the biases will
mislead the eliciting of "thoughts" in LLMs to focus only on a biased part of
the premise. To this end, we propose a new prompt technique termed
Language-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of
directly eliciting the chain of thoughts from partial information, LoT
instructs LLMs to adjust the order and token used for the expressions of all
the relevant information. We show that the simple strategy significantly
reduces the language modeling biases in LLMs and improves the performance of
LLMs across a variety of reasoning tasks.

</details>

### [463] [PyFCG: Fluid Construction Grammar in Python](https://arxiv.org/abs/2505.12920)
*Paul Van Eecke,Katrien Beuls*

Main category: cs.CL

TLDR: PyFCG是一个开源的Python库，将流体构造语法（FCG）移植到Python中，支持与其他Python库集成，并提供三个教程展示其典型用例。


<details>
  <summary>Details</summary>
Motivation: 将FCG功能集成到Python生态系统中，方便用户结合其他库使用FCG。

Method: 开发了PyFCG库，并通过三个教程展示其在语法分析、语料库学习和代理实验中的应用。

Result: PyFCG成功实现了FCG的Python移植，并展示了其多功能性。

Conclusion: PyFCG为FCG用户提供了更灵活的工具，扩展了其应用场景。

Abstract: We present PyFCG, an open source software library that ports Fluid
Construction Grammar (FCG) to the Python programming language. PyFCG enables
its users to seamlessly integrate FCG functionality into Python programs, and
to use FCG in combination with other libraries within Python's rich ecosystem.
Apart from a general description of the library, this paper provides three
walkthrough tutorials that demonstrate example usage of PyFCG in typical use
cases of FCG: (i) formalising and testing construction grammar analyses, (ii)
learning usage-based construction grammars from corpora, and (iii) implementing
agent-based experiments on emergent communication.

</details>

### [464] [Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs](https://arxiv.org/abs/2505.12929)
*Zhihe Yang,Xufang Luo,Zilong Wang,Dongqi Han,Zhiyuan He,Dongsheng Li,Yunjian Xu*

Main category: cs.CL

TLDR: 论文提出两种新方法（Advantage Reweighting和Lopti）解决RL训练中低概率token梯度主导的问题，显著提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 发现RL训练中低概率token的梯度过大，抑制了高概率token的学习，影响LLM性能。

Method: 提出Advantage Reweighting和Lopti方法，平衡不同概率token的梯度更新。

Result: 实验显示方法显著提升GRPO训练的LLM性能，在K&K逻辑推理任务中提升46.2%。

Conclusion: 新方法有效解决梯度不平衡问题，提升RL训练效率。

Abstract: Reinforcement learning (RL) has become a cornerstone for enhancing the
reasoning capabilities of large language models (LLMs), with recent innovations
such as Group Relative Policy Optimization (GRPO) demonstrating exceptional
effectiveness. In this study, we identify a critical yet underexplored issue in
RL training: low-probability tokens disproportionately influence model updates
due to their large gradient magnitudes. This dominance hinders the effective
learning of high-probability tokens, whose gradients are essential for LLMs'
performance but are substantially suppressed. To mitigate this interference, we
propose two novel methods: Advantage Reweighting and Low-Probability Token
Isolation (Lopti), both of which effectively attenuate gradients from
low-probability tokens while emphasizing parameter updates driven by
high-probability tokens. Our approaches promote balanced updates across tokens
with varying probabilities, thereby enhancing the efficiency of RL training.
Experimental results demonstrate that they substantially improve the
performance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K
Logic Puzzle reasoning tasks. Our implementation is available at
https://github.com/zhyang2226/AR-Lopti.

</details>

### [465] [A3 : an Analytical Low-Rank Approximation Framework for Attention](https://arxiv.org/abs/2505.12942)
*Jeffrey T. H. Wong,Cheng Zhang,Xinye Cao,Pedro Gimenes,George A. Constantinides,Wayne Luk,Yiren Zhao*

Main category: cs.CL

TLDR: 论文提出了一种名为A³的后训练低秩近似框架，通过将Transformer层分为三个功能组件并优化其功能损失，显著减少了模型大小和计算开销，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有低秩近似方法未考虑Transformer架构特性且分解方式导致性能不足和运行时开销，A³旨在解决这些问题。

Method: A³将Transformer层分为QK、OV和MLP三个组件，通过分析解减少隐藏维度大小，最小化功能损失。

Result: 在相同计算和内存预算下，A³在LLaMA 3.1-70B上实现了4.69的困惑度，优于之前的SoTA（7.87）。

Conclusion: A³是一种高效的低秩近似方法，适用于KV缓存压缩、量化和混合秩分配，显著提升性能。

Abstract: Large language models have demonstrated remarkable performance; however,
their massive parameter counts make deployment highly expensive. Low-rank
approximation offers a promising compression solution, yet existing approaches
have two main limitations: (1) They focus on minimizing the output error of
individual linear layers, without considering the architectural characteristics
of Transformers, and (2) they decompose a large weight matrix into two small
low-rank matrices. Consequently, these methods often fall short compared to
other compression techniques like pruning and quantization, and introduce
runtime overhead such as the extra GEMM kernel launches for decomposed small
matrices. To address these limitations, we propose $\tt A^\tt 3$, a
post-training low-rank approximation framework. $\tt A^\tt 3$ splits a
Transformer layer into three functional components, namely $\tt QK$, $\tt OV$,
and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical
solution that reduces the hidden dimension size inside each component while
minimizing the component's functional loss ($\it i.e.$, error in attention
scores, attention outputs, and MLP outputs). This approach directly reduces
model sizes, KV cache sizes, and FLOPs without introducing any runtime
overheads. In addition, it provides a new narrative in advancing the
optimization problem from singular linear layer loss optimization toward
improved end-to-end performance. Through extensive experiments, we show that
$\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example,
under the same reduction budget in computation and memory, our low-rank
approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,
outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the
versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and
mixed-rank assignments for enhanced performance.

</details>

### [466] [Neural Morphological Tagging for Nguni Languages](https://arxiv.org/abs/2505.12949)
*Cael Marquard,Simbarashe Mawere,Francois Meyer*

Main category: cs.CL

TLDR: 论文研究了使用神经方法为四种Nguni语言构建形态标记器的效果，比较了从头训练的神经序列标记器和预训练语言模型的性能，发现神经标记器优于传统规则基线。


<details>
  <summary>Details</summary>
Motivation: 针对Nguni等粘着语言，形态解析任务具有挑战性，需要高效的方法来分解和标记词素。

Method: 采用两种神经方法：从头训练的序列标记器（LSTM和神经CRF）和微调预训练语言模型，并与传统规则解析器对比。

Result: 神经标记器显著优于规则基线，从头训练的模型通常优于预训练模型。

Conclusion: 神经标记器结合现有形态分割器在Nguni语言中具有可行性。

Abstract: Morphological parsing is the task of decomposing words into morphemes, the
smallest units of meaning in a language, and labelling their grammatical roles.
It is a particularly challenging task for agglutinative languages, such as the
Nguni languages of South Africa, which construct words by concatenating
multiple morphemes. A morphological parsing system can be framed as a pipeline
with two separate components, a segmenter followed by a tagger. This paper
investigates the use of neural methods to build morphological taggers for the
four Nguni languages. We compare two classes of approaches: training neural
sequence labellers (LSTMs and neural CRFs) from scratch and finetuning
pretrained language models. We compare performance across these two categories,
as well as to a traditional rule-based morphological parser. Neural taggers
comfortably outperform the rule-based baseline and models trained from scratch
tend to outperform pretrained models. We also compare parsing results across
different upstream segmenters and with varying linguistic input features. Our
findings confirm the viability of employing neural taggers based on
pre-existing morphological segmenters for the Nguni languages.

</details>

### [467] [GuRE:Generative Query REwriter for Legal Passage Retrieval](https://arxiv.org/abs/2505.12950)
*Daehee Kim,Deokhyung Kang,Jonghwi Kim,Sangwon Ryu,Gary Geunbae Lee*

Main category: cs.CL

TLDR: 论文提出了一种名为GuRE的生成式查询重写方法，利用大语言模型（LLM）解决法律段落检索中的词汇不匹配问题，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 法律段落检索（LPR）系统对法律从业者起草法律论点至关重要，但词汇不匹配问题限制了其发展。

Method: 通过训练LLM进行查询重写（GuRE方法），生成改写后的查询以减少词汇不匹配。

Result: 实验表明，GuRE在检索器无关的方式下显著优于基线方法，且不同训练目标导致不同的检索行为。

Conclusion: GuRE比直接微调检索器更适合实际应用，代码已开源。

Abstract: Legal Passage Retrieval (LPR) systems are crucial as they help practitioners
save time when drafting legal arguments. However, it remains an underexplored
avenue. One primary reason is the significant vocabulary mismatch between the
query and the target passage. To address this, we propose a simple yet
effective method, the Generative query REwriter (GuRE). We leverage the
generative capabilities of Large Language Models (LLMs) by training the LLM for
query rewriting. "Rewritten queries" help retrievers to retrieve target
passages by mitigating vocabulary mismatch. Experimental results show that GuRE
significantly improves performance in a retriever-agnostic manner,
outperforming all baseline methods. Further analysis reveals that different
training objectives lead to distinct retrieval behaviors, making GuRE more
suitable than direct retriever fine-tuning for real-world applications. Codes
are avaiable at github.com/daehuikim/GuRE.

</details>

### [468] [MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition](https://arxiv.org/abs/2505.12964)
*Shanshan Liu,Noriki Nishida,Rumana Ferdous Munne,Narumi Tokunaga,Yuki Yamagata,Kouji Kozaki,Yuji Matsumoto*

Main category: cs.CL

TLDR: MA-COIR是一个将概念识别重新定义为索引-识别任务的框架，通过语义搜索索引（ssIDs）解决歧义并提高效率，适用于生物医学领域。


<details>
  <summary>Details</summary>
Motivation: 传统概念识别方法无法捕捉文本中未明确提及的复杂概念，限制了知识图谱构建和本体细化。

Method: 使用预训练的BART模型在小数据集上微调，结合LLM生成的查询和合成数据，降低计算需求并提升低资源环境下的识别能力。

Result: 在CDR、HPO和HOIP三个场景中，MA-COIR能有效识别显式和隐式概念，无需推理时的提及级标注。

Conclusion: MA-COIR通过索引-识别任务和LLM辅助，显著提升了生物医学领域的概念识别能力，代码和数据已开源。

Abstract: Recognizing biomedical concepts in the text is vital for ontology refinement,
knowledge graph construction, and concept relationship discovery. However,
traditional concept recognition methods, relying on explicit mention
identification, often fail to capture complex concepts not explicitly stated in
the text. To overcome this limitation, we introduce MA-COIR, a framework that
reformulates concept recognition as an indexing-recognition task. By assigning
semantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in
ontology entries and enhances recognition efficiency. Using a pretrained
BART-based model fine-tuned on small datasets, our approach reduces
computational requirements to facilitate adoption by domain experts.
Furthermore, we incorporate large language models (LLMs)-generated queries and
synthetic data to improve recognition in low-resource settings. Experimental
results on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of
MA-COIR in recognizing both explicit and implicit concepts without the need for
mention-level annotations during inference, advancing ontology-driven concept
recognition in biomedical domain applications. Our code and constructed data
are available at https://github.com/sl-633/macoir-master.

</details>

### [469] [Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down](https://arxiv.org/abs/2505.12969)
*Yingzhi Wang,Anas Alhmoud,Saad Alsahly,Muhammad Alqurishi,Mirco Ravanelli*

Main category: cs.CL

TLDR: 提出了一种减少Whisper在非语音段幻觉的新方法，通过分析并微调特定自注意力头，显著降低了幻觉问题。


<details>
  <summary>Details</summary>
Motivation: Whisper在非语音段存在幻觉问题，限制了其在复杂工业场景中的应用。

Method: 通过头掩码分析Whisper解码器中各头的贡献，并微调导致幻觉的关键头。

Result: 微调后的模型Calm-Whisper在非语音段幻觉减少80%以上，WER仅增加0.1%。

Conclusion: 该方法有效减少了Whisper的幻觉问题，同时保持了语音识别的准确性。

Abstract: OpenAI's Whisper has achieved significant success in Automatic Speech
Recognition. However, it has consistently been found to exhibit hallucination
issues, particularly in non-speech segments, which limits its broader
application in complex industrial settings.
  In this paper, we introduce a novel method to reduce Whisper's hallucination
on non-speech segments without using any pre- or post-possessing techniques.
Specifically, we benchmark the contribution of each self-attentional head in
the Whisper-large-v3 decoder to the hallucination problem by performing a
head-wise mask. Our findings reveal that only 3 of the 20 heads account for
over 75% of the hallucinations on the UrbanSound dataset. We then fine-tune
these three crazy heads using a collection of non-speech data. The results show
that our best fine-tuned model, namely Calm-Whisper, achieves over 80%
reduction in non-speech hallucination with only less than 0.1% WER degradation
on LibriSpeech test-clean and test-other.

</details>

### [470] [A Structured Literature Review on Traditional Approaches in Current Natural Language Processing](https://arxiv.org/abs/2505.12970)
*Robin Jegan,Andreas Henrich*

Main category: cs.CL

TLDR: 本文综述了神经网络和大语言模型在自然语言处理中的崛起及其对传统方法的影响，分析了五种应用场景中传统技术的现状。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在主流成功的同时仍存在的不足，并探讨传统方法在未来应用中的潜力和合理场景。

Method: 通过调查分类、信息与关系提取、文本简化和文本摘要五种场景的近期文献，分析传统技术的使用情况。

Result: 发现五种场景中传统方法仍以不同形式存在，如作为处理流程的一部分、基线比较或核心模型。

Conclusion: 传统技术在某些场景中仍有价值，未来需结合新旧方法以实现更优效果。

Abstract: The continued rise of neural networks and large language models in the more
recent past has altered the natural language processing landscape, enabling new
approaches towards typical language tasks and achieving mainstream success.
Despite the huge success of large language models, many disadvantages still
remain and through this work we assess the state of the art in five application
scenarios with a particular focus on the future perspectives and sensible
application scenarios of traditional and older approaches and techniques.
  In this paper we survey recent publications in the application scenarios
classification, information and relation extraction, text simplification as
well as text summarization. After defining our terminology, i.e., which
features are characteristic for traditional techniques in our interpretation
for the five scenarios, we survey if such traditional approaches are still
being used, and if so, in what way they are used. It turns out that all five
application scenarios still exhibit traditional models in one way or another,
as part of a processing pipeline, as a comparison/baseline to the core model of
the respective paper, or as the main model(s) of the paper. For the complete
statistics, see https://zenodo.org/records/13683801

</details>

### [471] [Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models](https://arxiv.org/abs/2505.12973)
*Mahta Fetrat Qharabagh,Zahra Dehghanian,Hamid R. Rabiee*

Main category: cs.CL

TLDR: 论文提出了一种半自动化的同形异义词数据集构建方法，并开发了HomoRich数据集。同时，改进了规则基础的G2P系统eSpeak，推出了HomoFast eSpeak，提升了同形异义词消歧的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言中同形异义词消歧的挑战，包括数据集构建的高成本和实时应用中的延迟问题。

Method: 1. 提出半自动化的数据集构建流程，生成HomoRich数据集；2. 改进规则基础的G2P系统eSpeak，开发HomoFast eSpeak。

Result: 深度学习系统和eSpeak系统的同形异义词消歧准确率提高了约30%。

Conclusion: 通过离线数据集和规则基础的快速方法，有效解决了同形异义词消歧问题，适用于实时应用。

Abstract: Homograph disambiguation remains a significant challenge in
grapheme-to-phoneme (G2P) conversion, especially for low-resource languages.
This challenge is twofold: (1) creating balanced and comprehensive homograph
datasets is labor-intensive and costly, and (2) specific disambiguation
strategies introduce additional latency, making them unsuitable for real-time
applications such as screen readers and other accessibility tools. In this
paper, we address both issues. First, we propose a semi-automated pipeline for
constructing homograph-focused datasets, introduce the HomoRich dataset
generated through this pipeline, and demonstrate its effectiveness by applying
it to enhance a state-of-the-art deep learning-based G2P system for Persian.
Second, we advocate for a paradigm shift - utilizing rich offline datasets to
inform the development of fast, rule-based methods suitable for
latency-sensitive accessibility applications like screen readers. To this end,
we improve one of the most well-known rule-based G2P systems, eSpeak, into a
fast homograph-aware version, HomoFast eSpeak. Our results show an approximate
30% improvement in homograph disambiguation accuracy for the deep
learning-based and eSpeak systems.

</details>

### [472] [An Empirical Study of Many-to-Many Summarization with Large Language Models](https://arxiv.org/abs/2505.12983)
*Jiaan Wang,Fandong Meng,Zengkui Sun,Yunlong Liang,Yuxuan Cao,Jiarong Xu,Haoxiang Shi,Jie Zhou*

Main category: cs.CL

TLDR: 该论文研究了大型语言模型（LLMs）在多语言多领域摘要任务（M2MS）中的表现，通过重组数据集和实验评估，发现指令调优能显著提升开源LLMs的性能，但事实性问题仍需解决。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在多语言多领域摘要任务中的潜力，填补现有研究的空白，并为实际应用提供指导。

Method: 重组47.8K样本的M2MS数据集，涵盖5个领域和6种语言，对18种LLMs进行零样本和指令调优评估，并与传统模型对比。

Result: 零样本LLMs表现与传统模型相当；指令调优后开源LLMs显著提升，但事实性问题加剧。

Conclusion: 指令调优能提升LLMs的M2MS能力，但需解决事实性问题，未来研究应关注错误控制。

Abstract: Many-to-many summarization (M2MS) aims to process documents in any language
and generate the corresponding summaries also in any language. Recently, large
language models (LLMs) have shown strong multi-lingual abilities, giving them
the potential to perform M2MS in real applications. This work presents a
systematic empirical study on LLMs' M2MS ability. Specifically, we first
reorganize M2MS data based on eight previous domain-specific datasets. The
reorganized data contains 47.8K samples spanning five domains and six
languages, which could be used to train and evaluate LLMs. Then, we benchmark
18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned
traditional models (e.g., mBART) are also conducted for comparisons. Our
experiments reveal that, zero-shot LLMs achieve competitive results with
fine-tuned traditional models. After instruct-tuning, open-source LLMs can
significantly improve their M2MS ability, and outperform zero-shot LLMs
(including GPT-4) in terms of automatic evaluations. In addition, we
demonstrate that this task-specific improvement does not sacrifice the LLMs'
general task-solving abilities. However, as revealed by our human evaluation,
LLMs still face the factuality issue, and the instruction tuning might
intensify the issue. Thus, how to control factual errors becomes the key when
building LLM summarizers in real applications, and is worth noting in future
research.

</details>

### [473] [ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning](https://arxiv.org/abs/2505.12996)
*Jiaan Wang,Fandong Meng,Jie Zhou*

Main category: cs.CL

TLDR: 论文提出了一种新的奖励建模方法，通过强化学习提升机器翻译性能，并在多语言环境中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型推理模型（LRMs）在机器翻译中表现优异，但主要集中在高资源语言，且奖励建模方法未充分发挥强化学习的潜力。

Method: 设计了一种新的奖励建模方法，将策略MT模型的翻译结果与强大的LRM（DeepSeek-R1-671B）进行比较，量化比较结果以提供奖励。

Result: 实验表明该方法优越，使用Qwen2.5-7B-Instruct作为主干模型，在文学翻译中达到新SOTA，并扩展到11种语言的90个翻译方向。

Conclusion: 新奖励建模方法显著提升了机器翻译性能，尤其在多语言环境中表现突出。

Abstract: In recent years, the emergence of large reasoning models (LRMs), such as
OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex
problems, e.g., mathematics and coding. Some pioneering studies attempt to
bring the success of LRMs in neural machine translation (MT). They try to build
LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite
some progress that has been made, these attempts generally focus on several
high-resource languages, e.g., English and Chinese, leaving the performance on
other languages unclear. Besides, the reward modeling methods in previous work
do not fully unleash the potential of reinforcement learning in MT. In this
work, we first design a new reward modeling method that compares the
translation results of the policy MT model with a strong LRM (i.e.,
DeepSeek-R1-671B), and quantifies the comparisons to provide rewards.
Experimental results demonstrate the superiority of the reward modeling method.
Using Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new
state-of-the-art performance in literary translation, and outperforms strong
LRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to
the multilingual settings with 11 languages. With a carefully designed
lightweight reward modeling in RL, we can simply transfer the strong MT ability
from a single direction into multiple (i.e., 90) translation directions and
achieve impressive multilingual MT performance.

</details>

### [474] [EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code](https://arxiv.org/abs/2505.13004)
*Yuhao Qing,Boyu Zhu,Mingzhe Du,Zhijiang Guo,Terry Yue Zhuo,Qianru Zhang,Jie M. Zhang,Heming Cui,Siu-Ming Yiu,Dong Huang,See-Kiong Ng,Luu Anh Tuan*

Main category: cs.CL

TLDR: EffiBench-X是首个多语言基准测试，用于评估LLM生成代码的效率，覆盖Python、C++、Java等语言，结果显示LLM生成的代码效率显著低于人类专家。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准主要关注功能正确性，缺乏对代码效率和多语言支持的评估，EffiBench-X填补了这一空白。

Method: EffiBench-X包含多语言编程任务，以人类专家解决方案为效率基准，评估了多种LLM的代码效率表现。

Result: LLM生成的代码效率平均仅为人类专家的62%，且在Python、Ruby和JavaScript中表现优于Java、C++和Golang。

Conclusion: 研究强调了优化LLM代码效率的必要性，EffiBench-X的数据集和评估工具已开源。

Abstract: Existing code generation benchmarks primarily evaluate functional
correctness, with limited focus on code efficiency and often restricted to a
single language like Python. To address this gap, we introduce EffiBench-X, the
first multi-language benchmark designed to measure the efficiency of
LLM-generated code. EffiBench-X supports Python, C++, Java, JavaScript, Ruby,
and Golang. It comprises competitive programming tasks with human-expert
solutions as efficiency baselines. Evaluating state-of-the-art LLMs on
EffiBench-X reveals that while models generate functionally correct code, they
consistently underperform human experts in efficiency. Even the most efficient
LLM-generated solutions (Qwen3-32B) achieve only around \textbf{62\%} of human
efficiency on average, with significant language-specific variations. LLMs show
better efficiency in Python, Ruby, and JavaScript than in Java, C++, and
Golang. For instance, DeepSeek-R1's Python code is significantly more efficient
than its Java code. These results highlight the critical need for research into
LLM optimization techniques to improve code efficiency across diverse
languages. The dataset and evaluation infrastructure are submitted and
available at https://github.com/EffiBench/EffiBench-X.git and
https://huggingface.co/datasets/EffiBench/effibench-x.

</details>

### [475] [Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain](https://arxiv.org/abs/2505.13006)
*Yuyang Li,Philip J. M. Kerbusch,Raimon H. R. Pruim,Tobias Käfer*

Main category: cs.CL

TLDR: 论文研究了三种RAG方法在机场对话AI中的应用，发现Graph RAG效果最佳，推荐用于机场环境。


<details>
  <summary>Details</summary>
Motivation: 提升机场自动化水平，解决机场术语和动态问题。

Method: 实现三种RAG方法（传统RAG、SQL RAG、Graph RAG）并对比效果。

Result: Graph RAG准确率最高（91.49%），且幻觉最少，适合处理推理问题。

Conclusion: 推荐SQL RAG和Graph RAG用于机场环境，因其幻觉少且能处理动态问题。

Abstract: Airports from the top 20 in terms of annual passengers are highly dynamic
environments with thousands of flights daily, and they aim to increase the
degree of automation. To contribute to this, we implemented a Conversational AI
system that enables staff in an airport to communicate with flight information
systems. This system not only answers standard airport queries but also
resolves airport terminology, jargon, abbreviations, and dynamic questions
involving reasoning. In this paper, we built three different
Retrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL
RAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that
traditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally
produced hallucinations, which is risky to airport safety. In contrast, SQL RAG
and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with
significantly fewer hallucinations. Moreover, Graph RAG was especially
effective for questions that involved reasoning. Based on our observations, we
thus recommend SQL RAG and Graph RAG are better for airport environments, due
to fewer hallucinations and the ability to handle dynamic questions.

</details>

### [476] [To Bias or Not to Bias: Detecting bias in News with bias-detector](https://arxiv.org/abs/2505.13010)
*Himel Ghosh,Ahmed Mosharafa,Georg Groh*

Main category: cs.CL

TLDR: 本文提出了一种基于RoBERTa的句子级媒体偏见检测方法，通过专家标注的BABE数据集微调模型，并在统计测试中显著优于基线模型。模型结合注意力机制和现有偏见分类器，表现出良好的泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 媒体偏见检测对信息公平传播至关重要，但主观性和高质量标注数据稀缺使其具有挑战性。

Method: 使用RoBERTa模型在BABE数据集上微调，并通过McNemar检验和5x2交叉验证t检验验证性能。结合注意力机制和现有偏见分类器构建检测流程。

Result: 模型在统计测试中显著优于基线，注意力机制避免了对政治敏感词的过度关注，更注重上下文相关标记。

Conclusion: 该方法为媒体偏见检测提供了更鲁棒、可解释的解决方案，未来可探索上下文感知建模和偏见类型分类等方向。

Abstract: Media bias detection is a critical task in ensuring fair and balanced
information dissemination, yet it remains challenging due to the subjectivity
of bias and the scarcity of high-quality annotated data. In this work, we
perform sentence-level bias classification by fine-tuning a RoBERTa-based model
on the expert-annotated BABE dataset. Using McNemar's test and the 5x2
cross-validation paired t-test, we show statistically significant improvements
in performance when comparing our model to a domain-adaptively pre-trained
DA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model
avoids common pitfalls like oversensitivity to politically charged terms and
instead attends more meaningfully to contextually relevant tokens. For a
comprehensive examination of media bias, we present a pipeline that combines
our model with an already-existing bias-type classifier. Our method exhibits
good generalization and interpretability, despite being constrained by
sentence-level analysis and dataset size because of a lack of larger and more
advanced bias corpora. We talk about context-aware modeling, bias
neutralization, and advanced bias type classification as potential future
directions. Our findings contribute to building more robust, explainable, and
socially responsible NLP systems for media bias detection.

</details>

### [477] [topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation](https://arxiv.org/abs/2505.13034)
*Márton Kardos,Kenneth C. Enevoldsen,Kristoffer Laigaard Nielbo*

Main category: cs.CL

TLDR: 论文介绍了topicwizard，一个模型无关的主题模型解释框架，旨在通过直观的交互工具帮助用户更好地理解主题模型中的语义关系。


<details>
  <summary>Details</summary>
Motivation: 主题模型的参数解释对用户具有挑战性，传统的基于词列表的方法存在局限性和偏见，需要更全面的可视化工具。

Method: 提出topicwizard框架，提供模型无关的交互式工具，用于分析文档、词汇和主题之间的复杂语义关系。

Result: topicwizard为用户提供了更完整和准确的主题模型输出理解。

Conclusion: topicwizard通过改进的可视化和交互设计，提升了主题模型解释的效率和准确性。

Abstract: Topic models are statistical tools that allow their users to gain qualitative
and quantitative insights into the contents of textual corpora without the need
for close reading. They can be applied in a wide range of settings from
discourse analysis, through pretraining data curation, to text filtering. Topic
models are typically parameter-rich, complex models, and interpreting these
parameters can be challenging for their users. It is typical practice for users
to interpret topics based on the top 10 highest ranking terms on a given topic.
This list-of-words approach, however, gives users a limited and biased picture
of the content of topics. Thoughtful user interface design and visualizations
can help users gain a more complete and accurate understanding of topic models'
output. While some visualization utilities do exist for topic models, these are
typically limited to a certain type of topic model. We introduce topicwizard, a
framework for model-agnostic topic model interpretation, that provides
intuitive and interactive tools that help users examine the complex semantic
relations between documents, words and topics learned by topic models.

</details>

### [478] [KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025](https://arxiv.org/abs/2505.13036)
*Sai Koneru,Maike Züfle,Thai-Binh Nguyen,Seymanur Akti,Jan Niehues,Alexander Waibel*

Main category: cs.CL

TLDR: 本文介绍了卡尔斯鲁厄理工学院在离线语音翻译和指令跟随任务中的提交方案，利用大语言模型提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代系统能力的提升，尤其是大语言模型的成功，推动了语音翻译任务的扩展。

Method: 离线语音翻译采用多语音识别系统融合和大语言模型文档级上下文的管道方法；指令跟随任务采用端到端模型结合语音编码器和大语言模型。

Result: 通过两步翻译和文档级细化提升翻译质量；端到端模型结合上下文信息优化输出。

Conclusion: 大语言模型在语音翻译和指令跟随任务中表现出色，文档级细化进一步提升了性能。

Abstract: The scope of the International Workshop on Spoken Language Translation
(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to
encompass a wider array of tasks, including Speech Question Answering and
Summarization. This shift is partly driven by the growing capabilities of
modern systems, particularly with the success of Large Language Models (LLMs).
In this paper, we present the Karlsruhe Institute of Technology's submissions
for the Offline ST and Instruction Following (IF) tracks, where we leverage
LLMs to enhance performance across all tasks. For the Offline ST track, we
propose a pipeline that employs multiple automatic speech recognition systems,
whose outputs are fused using an LLM with document-level context. This is
followed by a two-step translation process, incorporating additional refinement
step to improve translation quality. For the IF track, we develop an end-to-end
model that integrates a speech encoder with an LLM to perform a wide range of
instruction-following tasks. We complement it with a final document-level
refinement stage to further enhance output quality by using contextual
information.

</details>

### [479] [SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation](https://arxiv.org/abs/2505.13053)
*Amelie S. Robrecht,Christoph R. Kowalski,Stefan Kopp*

Main category: cs.CL

TLDR: 论文提出了一种基于贝叶斯推理和非平稳马尔可夫决策过程的框架，用于动态调整对话系统中的解释策略，以适应不同用户。


<details>
  <summary>Details</summary>
Motivation: 适应不同用户是对话系统成功解释的关键，但现有方法难以动态调整策略。

Method: 采用贝叶斯推理更新用户模型，结合非平稳马尔可夫决策过程动态调整解释策略。

Result: 实验表明，框架能有效适应不同用户（包括反馈行为变化的用户），并生成针对性解释策略。

Conclusion: 该方法为可解释AI和对话系统提供了动态适应的潜力。

Abstract: Adapting to the addressee is crucial for successful explanations, yet poses
significant challenges for dialogsystems. We adopt the approach of treating
explanation generation as a non-stationary decision process, where the optimal
strategy varies according to changing beliefs about the explainee and the
interaction context. In this paper we address the questions of (1) how to track
the interaction context and the relevant listener features in a formally
defined computational partner model, and (2) how to utilize this model in the
dynamically adjusted, rational decision process that determines the currently
best explanation strategy. We propose a Bayesian inference-based approach to
continuously update the partner model based on user feedback, and a
non-stationary Markov Decision Process to adjust decision-making based on the
partner model values. We evaluate an implementation of this framework with five
simulated interlocutors, demonstrating its effectiveness in adapting to
different partners with constant and even changing feedback behavior. The
results show high adaptivity with distinct explanation strategies emerging for
different partners, highlighting the potential of our approach to improve
explainable AI systems and dialogsystems in general.

</details>

### [480] [Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset](https://arxiv.org/abs/2505.13069)
*Ambre Marie,Ilias Maoudj,Guillaume Dardenne,Gwenolé Quellec*

Main category: cs.CL

TLDR: 研究探讨了多模态方法在青少年自杀风险评估中的应用，结合了语音转录、语言和音频嵌入以及手工声学特征，发现加权注意力融合策略效果最佳。


<details>
  <summary>Details</summary>
Motivation: 解决基于语音的青少年自杀风险评估的需求。

Method: 整合WhisperX自动转录、中文RoBERTa语言嵌入、WavLM音频嵌入及手工声学特征，探索三种融合策略。

Result: 加权注意力策略在开发集上达到69%准确率，但开发集与测试集间存在性能差距。

Conclusion: 需优化嵌入表示和融合机制以提高分类可靠性。

Abstract: The 1st SpeechWellness Challenge conveys the need for speech-based suicide
risk assessment in adolescents. This study investigates a multimodal approach
for this challenge, integrating automatic transcription with WhisperX,
linguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM.
Additionally, handcrafted acoustic features -- including MFCCs, spectral
contrast, and pitch-related statistics -- were incorporated. We explored three
fusion strategies: early concatenation, modality-specific processing, and
weighted attention with mixup regularization. Results show that weighted
attention provided the best generalization, achieving 69% accuracy on the
development set, though a performance gap between development and test sets
highlights generalization challenges. Our findings, strictly tied to the
MINI-KID framework, emphasize the importance of refining embedding
representations and fusion mechanisms to enhance classification reliability.

</details>

### [481] [Advancing Sequential Numerical Prediction in Autoregressive Models](https://arxiv.org/abs/2505.13077)
*Xiang Fei,Jinghui Lu,Qi Sun,Hao Feng,Yanjie Wang,Wei Shi,An-Lan Wang,Jingqun Tang,Can Huang*

Main category: cs.CL

TLDR: 论文提出了一种名为NTIL的损失函数，通过结合token级和序列级的优化，改进了自回归模型在数值序列生成中的表现。


<details>
  <summary>Details</summary>
Motivation: 标准自回归模型在处理数值序列时忽略了其连贯结构，导致性能不足。

Method: 引入NTIL，结合token级的EMD扩展和序列级的差异惩罚。

Result: 实验表明NTIL显著提升了数值预测性能。

Conclusion: NTIL有效解决了数值序列生成中的结构问题，适用于LLMs/MLLMs。

Abstract: Autoregressive models have become the de facto choice for sequence generation
tasks, but standard approaches treat digits as independent tokens and apply
cross-entropy loss, overlooking the coherent structure of numerical sequences.
This paper introduces Numerical Token Integrity Loss (NTIL) to address this
gap. NTIL operates at two levels: (1) token-level, where it extends the Earth
Mover's Distance (EMD) to preserve ordinal relationships between numerical
values, and (2) sequence-level, where it penalizes the overall discrepancy
between the predicted and actual sequences. This dual approach improves
numerical prediction and integrates effectively with LLMs/MLLMs. Extensive
experiments show significant performance improvements with NTIL.

</details>

### [482] [Systematic Generalization in Language Models Scales with Information Entropy](https://arxiv.org/abs/2505.13089)
*Sondre Wold,Lucas Georges Gabriel Charpentier,Étienne Simon*

Main category: cs.CL

TLDR: 论文探讨了语言模型在系统性泛化中的挑战，提出用训练数据中组件部分的熵来描述泛化能力，并发现模型性能与熵相关。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在系统性泛化中存在困难，缺乏衡量问题难度的标准。

Method: 通过序列到序列任务中组件部分的熵来形式化衡量框架，分析模型性能与熵的关系。

Result: 模型性能随熵增加而提升，高熵下无需内置先验即可成功，低熵可作为系统性泛化进展的评估目标。

Conclusion: 系统性泛化与信息效率相关，高熵表现可独立于先验，低熵表现可作为泛化鲁棒性的评估指标。

Abstract: Systematic generalization remains challenging for current language models,
which are known to be both sensitive to semantically similar permutations of
the input and to struggle with known concepts presented in novel contexts.
Although benchmarks exist for assessing compositional behavior, it is unclear
how to measure the difficulty of a systematic generalization problem. In this
work, we show how one aspect of systematic generalization can be described by
the entropy of the distribution of component parts in the training data. We
formalize a framework for measuring entropy in a sequence-to-sequence task and
find that the performance of popular model architectures scales with the
entropy. Our work connects systematic generalization to information efficiency,
and our results indicate that success at high entropy can be achieved even
without built-in priors, and that success at low entropy can serve as a target
for assessing progress towards robust systematic generalization.

</details>

### [483] [The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation](https://arxiv.org/abs/2505.13090)
*David Stap,Christof Monz*

Main category: cs.CL

TLDR: 研究发现，增加语言多样性在LLM微调中能提升翻译质量，但超过一定阈值后效果会减弱。


<details>
  <summary>Details</summary>
Motivation: 解决先前研究中关于语言多样性在LLM微调中效果不一致的问题。

Method: 通过132个翻译方向的受控微调实验，系统分析语言多样性的影响。

Result: 语言多样性提升翻译质量，但效果在达到阈值后趋于平稳或下降；多样性增强了语言无关的表征。

Conclusion: 语言多样性通过改善表征适应性提升模型性能，但需注意多样性阈值。

Abstract: Prior research diverges on language diversity in LLM fine-tuning: Some
studies report benefits while others find no advantages. Through controlled
fine-tuning experiments across 132 translation directions, we systematically
resolve these disparities. We find that expanding language diversity during
fine-tuning improves translation quality for both unsupervised and --
surprisingly -- supervised pairs, despite less diverse models being fine-tuned
exclusively on these supervised pairs. However, benefits plateau or decrease
beyond a certain diversity threshold. We show that increased language diversity
creates more language-agnostic representations. These representational
adaptations help explain the improved performance in models fine-tuned with
greater diversity.

</details>

### [484] [Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning](https://arxiv.org/abs/2505.13115)
*Debarpan Bhattacharya,Apoorva Kulkarni,Sriram Ganapathy*

Main category: cs.CL

TLDR: 论文提出了一种新的数据集TREA，用于评估大型音频语言模型（LALMs）在时序推理任务上的表现，并发现现有模型与人类能力存在差距。同时，提出了一种不确定性度量方法，强调需要更全面的模型评估。


<details>
  <summary>Details</summary>
Motivation: 多模态社区希望将文本与其他模态（如音频）结合，但现有LALMs在推理任务上的表现尚未充分评估，因此需要新的数据集和评估方法。

Method: 提出TREA数据集，用于评估LALMs的时序推理能力，并设计了一种不确定性度量方法，衡量模型对输入语义相同扰动的鲁棒性。

Result: 实验显示，开源LALMs在TREA任务上表现不及人类，且准确性与不确定性度量无必然相关性。

Conclusion: LALMs在高风险应用中需要更全面的评估，仅依赖准确性指标可能不足。

Abstract: The popular success of text-based large language models (LLM) has streamlined
the attention of the multimodal community to combine other modalities like
vision and audio along with text to achieve similar multimodal capabilities. In
this quest, large audio language models (LALMs) have to be evaluated on
reasoning related tasks which are different from traditional classification or
generation tasks. Towards this goal, we propose a novel dataset called temporal
reasoning evaluation of audio (TREA).
  We benchmark open-source LALMs and observe that they are consistently behind
human capabilities on the tasks in the TREA dataset. While evaluating LALMs, we
also propose an uncertainty metric, which computes the invariance of the model
to semantically identical perturbations of the input. Our analysis shows that
the accuracy and uncertainty metrics are not necessarily correlated and thus,
points to a need for wholesome evaluation of LALMs for high-stakes
applications.

</details>

### [485] [ModernGBERT: German-only 1B Encoder Model Trained from Scratch](https://arxiv.org/abs/2505.13136)
*Anton Ehrmanntraut,Julia Wunderle,Jan Pfister,Fotis Jannidis,Andreas Hotho*

Main category: cs.CL

TLDR: 论文介绍了ModernGBERT和LL"aMmlein2Vec两种德语编码器模型，通过对比实验证明ModernGBERT 1B在性能和参数效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决资源受限应用中编码器的重要性，并探索从头训练编码器与从解码器转换的编码器的实际权衡。

Method: 方法包括从头训练ModernGBERT家族模型，以及通过LLM2Vec从解码器模型转换得到LL"aMmlein2Vec家族模型。

Result: 结果显示ModernGBERT 1B在德语自然语言理解、文本嵌入和长上下文推理任务中优于现有方法。

Conclusion: 结论是ModernGBERT 1B在性能和效率上表现最佳，所有模型和数据均已公开，推动了德语NLP生态系统的发展。

Abstract: Despite the prominence of decoder-only language models, encoders remain
crucial for resource-constrained applications. We introduce ModernGBERT (134M,
1B), a fully transparent family of German encoder models trained from scratch,
incorporating architectural innovations from ModernBERT. To evaluate the
practical trade-offs of training encoders from scratch, we also present
LL\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German
decoder-only models via LLM2Vec. We benchmark all models on natural language
understanding, text embedding, and long-context reasoning tasks, enabling a
controlled comparison between dedicated encoders and converted decoders. Our
results show that ModernGBERT 1B outperforms prior state-of-the-art German
encoders as well as encoders adapted via LLM2Vec, with regard to performance
and parameter-efficiency. All models, training data, checkpoints and code are
publicly available, advancing the German NLP ecosystem with transparent,
high-performance encoder models.

</details>

### [486] [Understanding Cross-Lingual Inconsistency in Large Language Models](https://arxiv.org/abs/2505.13141)
*Zheng Wei Lim,Alham Fikri Aji,Trevor Cohn*

Main category: cs.CL

TLDR: 大型语言模型（LLMs）在多语言推理中存在输出不一致问题，研究发现其依赖语言特定子空间而非共享语义空间。通过调整模型潜在处理方向，可提升多语言推理性能。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs如何从一种语言泛化知识到其他语言，并解决多语言推理中的不一致性问题。

Method: 应用logit lens分析LLMs解决多语言多选推理问题的隐含步骤，并通过调整潜在处理方向验证共享语义空间的影响。

Result: LLMs依赖语言特定子空间导致预测不一致，但通过强化共享语义空间利用可提升多语言推理性能和输出一致性。

Conclusion: 通过引导模型潜在处理方向至共享语义空间，可优化LLMs的多语言知识转移和推理能力。

Abstract: Large language models (LLMs) are demonstrably capable of cross-lingual
transfer, but can produce inconsistent output when prompted with the same
queries written in different languages. To understand how language models are
able to generalize knowledge from one language to the others, we apply the
logit lens to interpret the implicit steps taken by LLMs to solve multilingual
multi-choice reasoning questions. We find LLMs predict inconsistently and are
less accurate because they rely on subspaces of individual languages, rather
than working in a shared semantic space. While larger models are more
multilingual, we show their hidden states are more likely to dissociate from
the shared representation compared to smaller models, but are nevertheless more
capable of retrieving knowledge embedded across different languages. Finally,
we demonstrate that knowledge sharing can be modulated by steering the models'
latent processing towards the shared semantic space. We find reinforcing
utilization of the shared space improves the models' multilingual reasoning
performance, as a result of more knowledge transfer from, and better output
consistency with English.

</details>

### [487] [What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text](https://arxiv.org/abs/2505.13147)
*Aswathy Velutharambath,Roman Klinger,Kai Sassenberg*

Main category: cs.CL

TLDR: 论文探讨了仅从文本中检测欺骗的可行性，提出了一种基于信念的欺骗框架，并通过构建DeFaBel语料库验证了传统欺骗线索的不可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是质疑现有自动欺骗检测方法的泛化能力，认为其成功可能源于数据收集中的偏差。

Method: 方法包括提出信念欺骗框架、构建DeFaBel语料库（含多语言和不同条件的数据），并评估传统欺骗线索和多种模型的表现。

Result: 结果显示传统欺骗线索与欺骗标签相关性微弱且不一致，模型在DeFaBel上表现接近随机。

Conclusion: 结论指出欺骗无法仅从语言线索可靠推断，呼吁重新思考NLP中欺骗研究的方法。

Abstract: Can deception be detected solely from written text? Cues of deceptive
communication are inherently subtle, even more so in text-only communication.
Yet, prior studies have reported considerable success in automatic deception
detection. We hypothesize that such findings are largely driven by artifacts
introduced during data collection and do not generalize beyond specific
datasets. We revisit this assumption by introducing a belief-based deception
framework, which defines deception as a misalignment between an author's claims
and true beliefs, irrespective of factual accuracy, allowing deception cues to
be studied in isolation. Based on this framework, we construct three corpora,
collectively referred to as DeFaBel, including a German-language corpus of
deceptive and non-deceptive arguments and a multilingual version in German and
English, each collected under varying conditions to account for belief change
and enable cross-linguistic analysis. Using these corpora, we evaluate commonly
reported linguistic cues of deception. Across all three DeFaBel variants, these
cues show negligible, statistically insignificant correlations with deception
labels, contrary to prior work that treats such cues as reliable indicators. We
further benchmark against other English deception datasets following similar
data collection protocols. While some show statistically significant
correlations, effect sizes remain low and, critically, the set of predictive
cues is inconsistent across datasets. We also evaluate deception detection
using feature-based models, pretrained language models, and instruction-tuned
large language models. While some models perform well on established deception
datasets, they consistently perform near chance on DeFaBel. Our findings
challenge the assumption that deception can be reliably inferred from
linguistic cues and call for rethinking how deception is studied and modeled in
NLP.

</details>

### [488] [Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice](https://arxiv.org/abs/2505.13156)
*Zhi Liu,Tao Yang,Jing Wang,Yexin Chen,Zhan Gao,Jiaxi Yang,Kui Chen,Bingji Lu,Xiaochen Li,Changyong Luo,Yan Li,Xiaohong Gu,Peng Cao*

Main category: cs.CL

TLDR: 论文介绍了Tianyi，一个专为中医设计的7.6亿参数大语言模型，解决了现有模型在中医领域的不足，并通过TCMEval评估其性能。


<details>
  <summary>Details</summary>
Motivation: 中医的全球认可度提升，但其应用需要专业知识，现有AI技术因数据和目标单一限制而难以实用化。

Method: 提出Tianyi模型，基于多样中医语料预训练和微调，采用渐进式学习方式吸收系统知识，并建立TCMEval评估基准。

Result: Tianyi在中医考试、临床任务等领域表现出色，展示了作为AI助手的潜力。

Conclusion: Tianyi填补了中医知识与实际应用间的鸿沟，为中医临床和研究提供了有力工具。

Abstract: Natural medicines, particularly Traditional Chinese Medicine (TCM), are
gaining global recognition for their therapeutic potential in addressing human
symptoms and diseases. TCM, with its systematic theories and extensive
practical experience, provides abundant resources for healthcare. However, the
effective application of TCM requires precise syndrome diagnosis, determination
of treatment principles, and prescription formulation, which demand decades of
clinical expertise. Despite advancements in TCM-based decision systems, machine
learning, and deep learning research, limitations in data and single-objective
constraints hinder their practical application. In recent years, large language
models (LLMs) have demonstrated potential in complex tasks, but lack
specialization in TCM and face significant challenges, such as too big model
scale to deploy and issues with hallucination. To address these challenges, we
introduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and
specifically designed for TCM, pre-trained and fine-tuned on diverse TCM
corpora, including classical texts, expert treatises, clinical records, and
knowledge graphs. Tianyi is designed to assimilate interconnected and
systematic TCM knowledge through a progressive learning manner. Additionally,
we establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in
TCM examinations, clinical tasks, domain-specific question-answering, and
real-world trials. The extensive evaluations demonstrate the significant
potential of Tianyi as an AI assistant in TCM clinical practice and research,
bridging the gap between TCM knowledge and practical application.

</details>

### [489] [Role-Playing Evaluation for Large Language Models](https://arxiv.org/abs/2505.13157)
*Yassine El Boudouri,Walter Nuninger,Julian Alvarez,Yvan Peter*

Main category: cs.CL

TLDR: 介绍了一个名为RPEval的新基准，用于评估大语言模型在角色扮演中的能力，包括情感理解、决策、道德对齐和角色一致性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（如人工评估）资源密集且自动化评估可能存在偏差，需要一种更有效的评估工具。

Method: 构建了RPEval基准，涵盖四个关键维度，并进行了基线评估。

Result: 提出了RPEval的具体实现和评估结果，代码和数据集已开源。

Conclusion: RPEval为评估LLM角色扮演能力提供了标准化工具，解决了现有方法的局限性。

Abstract: Large Language Models (LLMs) demonstrate a notable capacity for adopting
personas and engaging in role-playing. However, evaluating this ability
presents significant challenges, as human assessments are resource-intensive
and automated evaluations can be biased. To address this, we introduce
Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM
role-playing capabilities across four key dimensions: emotional understanding,
decision-making, moral alignment, and in-character consistency. This article
details the construction of RPEval and presents baseline evaluations. Our code
and dataset are available at https://github.com/yelboudouri/RPEval

</details>

### [490] [Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks](https://arxiv.org/abs/2505.13171)
*Yixuan Xu,Antoine Bosselut,Imanol Schlag*

Main category: cs.CL

TLDR: 研究发现大型语言模型存在记忆训练数据的风险，提出位置偏移效应可降低记忆风险。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型记忆训练数据导致的版权风险。

Method: 预训练不同规模的模型，模拟受控频率的版权内容，分析记忆现象。

Result: 发现位置偏移效应，模型对早期标记敏感，偏移可降低记忆风险。

Conclusion: 位置偏移是评估记忆风险的关键因素，可有效减少记忆和退化现象。

Abstract: Large language models are known to memorize parts of their training data,
posing risk of copyright violations. To systematically examine this risk, we
pretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing
web-scale data with public domain books used to simulate copyrighted content at
controlled frequencies at lengths at least ten times longer than prior work. We
thereby identified the offset effect, a phenomenon characterized by two key
findings: (1) verbatim memorization is most strongly triggered by short
prefixes drawn from the beginning of the context window, with memorization
decreasing counterintuitively as prefix length increases; and (2) a sharp
decline in verbatim recall when prefix begins offset from the initial tokens of
the context window. We attribute this to positional fragility: models rely
disproportionately on the earliest tokens in their context window as retrieval
anchors, making them sensitive to even slight shifts. We further observe that
when the model fails to retrieve memorized content, it often produces
degenerated text. Leveraging these findings, we show that shifting sensitive
data deeper into the context window suppresses both extractable memorization
and degeneration. Our results suggest that positional offset is a critical and
previously overlooked axis for evaluating memorization risks, since prior work
implicitly assumed uniformity by probing only from the beginning of training
sequences.

</details>

### [491] [A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs](https://arxiv.org/abs/2505.13173)
*V. S. D. S. Mahesh Akavarapu,Hrishikesh Terdalkar,Pramit Bhattacharyya,Shubhangi Agarwal,Vishakha Deulgaonkar,Pralay Manna,Chaitali Dangarikar,Arnab Bhattacharya*

Main category: cs.CL

TLDR: 研究探讨了大型语言模型（LLMs）在古典语言（梵语、古希腊语和拉丁语）中的跨语言零样本泛化能力，发现模型规模是影响性能的关键因素。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解LLMs在古典语言中的自然语言理解能力，尤其是跨语言零样本泛化的影响因素。

Method: 通过命名实体识别、机器翻译和问答任务（尤其是梵语的检索增强生成方法）评估LLMs的性能。

Result: LLMs在域外数据上表现优于或等于微调基线，但小模型在抽象实体类型和问答任务中表现较差。检索增强生成显著提升了梵语问答性能。

Conclusion: 模型规模是影响跨语言泛化的关键因素，研究为LLMs在古典语言研究中的潜在应用提供了见解。

Abstract: Large Language Models (LLMs) have demonstrated remarkable generalization
capabilities across diverse tasks and languages. In this study, we focus on
natural language understanding in three classical languages -- Sanskrit,
Ancient Greek and Latin -- to investigate the factors affecting cross-lingual
zero-shot generalization. First, we explore named entity recognition and
machine translation into English. While LLMs perform equal to or better than
fine-tuned baselines on out-of-domain data, smaller models often struggle,
especially with niche or abstract entity types. In addition, we concentrate on
Sanskrit by presenting a factoid question-answering (QA) dataset and show that
incorporating context via retrieval-augmented generation approach significantly
boosts performance. In contrast, we observe pronounced performance drops for
smaller LLMs across these QA tasks. These results suggest model scale as an
important factor influencing cross-lingual generalization. Assuming that models
used such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical
languages, our findings provide insights into how LLMs may generalize on these
languages and their consequent utility in classical studies.

</details>

### [492] [ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models](https://arxiv.org/abs/2505.13176)
*Zihao Cheng,Hongru Wang,Zeming Liu,Yuhang Guo,Yuanfang Guo,Yunhong Wang,Haifeng Wang*

Main category: cs.CL

TLDR: 论文提出ToolSpectrum基准，评估LLMs在个性化工具使用中的能力，强调上下文感知的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视工具选择的上下文感知个性化，导致用户满意度低和工具利用效率不足。

Method: 引入ToolSpectrum基准，形式化用户画像和环境因素两个维度，分析其对工具利用的影响。

Result: 实验表明个性化工具利用显著提升用户体验，但现有LLMs难以同时权衡用户画像和环境因素。

Conclusion: 上下文感知个性化对工具增强型LLMs至关重要，当前模型存在局限性。

Abstract: While integrating external tools into large language models (LLMs) enhances
their ability to access real-time information and domain-specific services,
existing approaches focus narrowly on functional tool selection following user
instructions, overlooking the context-aware personalization in tool selection.
This oversight leads to suboptimal user satisfaction and inefficient tool
utilization, particularly when overlapping toolsets require nuanced selection
based on contextual factors. To bridge this gap, we introduce ToolSpectrum, a
benchmark designed to evaluate LLMs' capabilities in personalized tool
utilization. Specifically, we formalize two key dimensions of personalization,
user profile and environmental factors, and analyze their individual and
synergistic impacts on tool utilization. Through extensive experiments on
ToolSpectrum, we demonstrate that personalized tool utilization significantly
improves user experience across diverse scenarios. However, even
state-of-the-art LLMs exhibit the limited ability to reason jointly about user
profiles and environmental factors, often prioritizing one dimension at the
expense of the other. Our findings underscore the necessity of context-aware
personalization in tool-augmented LLMs and reveal critical limitations for
current models. Our data and code are available at
https://github.com/Chengziha0/ToolSpectrum.

</details>

### [493] [Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space](https://arxiv.org/abs/2505.13181)
*Zhengrui Ma,Yang Feng,Chenze Shao,Fandong Meng,Jie Zhou,Min Zhang*

Main category: cs.CL

TLDR: SLED是一种新的语音语言建模方法，通过连续潜在表示和能量距离目标实现高效训练，避免了现有方法的复杂性和离散化误差。


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型依赖复杂的层次结构和残差向量量化，导致离散化误差和建模复杂性。SLED旨在简化流程并保留语音信息的丰富性。

Method: SLED将语音波形编码为连续潜在表示，并通过能量距离目标自回归建模，避免离散化误差和复杂架构。

Result: 实验表明，SLED在零样本和流式语音合成中表现优异，展示了其在通用语音语言模型中的潜力。

Conclusion: SLED通过简化建模流程和避免离散化误差，为语音语言建模提供了高效且通用的解决方案。

Abstract: We introduce SLED, an alternative approach to speech language modeling by
encoding speech waveforms into sequences of continuous latent representations
and modeling them autoregressively using an energy distance objective. The
energy distance offers an analytical measure of the distributional gap by
contrasting simulated and target samples, enabling efficient training to
capture the underlying continuous autoregressive distribution. By bypassing
reliance on residual vector quantization, SLED avoids discretization errors and
eliminates the need for the complicated hierarchical architectures common in
existing speech language models. It simplifies the overall modeling pipeline
while preserving the richness of speech information and maintaining inference
efficiency. Empirical results demonstrate that SLED achieves strong performance
in both zero-shot and streaming speech synthesis, showing its potential for
broader applications in general-purpose speech language models.

</details>

### [494] [Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification](https://arxiv.org/abs/2505.13204)
*Jikai Wang,Zhenxu Tian,Juntao Li,Qingrong Xia,Xinyu Duan,Zhefeng Wang,Baoxing Huai,Min Zhang*

Main category: cs.CL

TLDR: 提出了一种无需训练的增强对齐推测解码算法，通过对齐采样和灵活验证策略，提升生成准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖训练实现对齐，成本高，需探索无需训练的高效对齐方法。

Method: 采用对齐采样和灵活验证策略，利用预填充阶段的输出分布生成对齐候选，并通过自适应概率阈值优化验证。

Result: 在8个数据集上实验，LLaMA3模型的平均生成分数提升3.3分，平均接受长度达2.39，生成速度提升2.23倍。

Conclusion: 该方法显著提升生成性能，无需训练成本，具有高效性和普适性。

Abstract: Recent works have revealed the great potential of speculative decoding in
accelerating the autoregressive generation process of large language models.
The success of these methods relies on the alignment between draft candidates
and the sampled outputs of the target model. Existing methods mainly achieve
draft-target alignment with training-based methods, e.g., EAGLE, Medusa,
involving considerable training costs. In this paper, we present a
training-free alignment-augmented speculative decoding algorithm. We propose
alignment sampling, which leverages output distribution obtained in the
prefilling phase to provide more aligned draft candidates. To further benefit
from high-quality but non-aligned draft candidates, we also introduce a simple
yet effective flexible verification strategy. Through an adaptive probability
threshold, our approach can improve generation accuracy while further improving
inference efficiency. Experiments on 8 datasets (including question answering,
summarization and code completion tasks) show that our approach increases the
average generation score by 3.3 points for the LLaMA3 model. Our method
achieves a mean acceptance length up to 2.39 and speed up generation by 2.23.

</details>

### [495] [Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry](https://arxiv.org/abs/2505.13210)
*Xiaocong Du,Haoyu Pei,Haipeng Zhang*

Main category: cs.CL

TLDR: 提出了一种基于多模态的古典诗歌情感分析方法，结合音频和视觉特征，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了诗歌的节奏和视觉特征，而这些特征对情感表达至关重要。

Method: 提取多方言音频特征和视觉特征，通过多模态对比表示学习与文本特征融合。

Result: 在两个公开数据集上准确率提升至少2.51%，宏F1提升1.63%。

Conclusion: 该方法为古典诗歌情感分析和多模态中文表示提供了新思路，代码已开源。

Abstract: Classical Chinese poetry is a vital and enduring part of Chinese literature,
conveying profound emotional resonance. Existing studies analyze sentiment
based on textual meanings, overlooking the unique rhythmic and visual features
inherent in poetry,especially since it is often recited and accompanied by
Chinese paintings. In this work, we propose a dialect-enhanced multimodal
framework for classical Chinese poetry sentiment analysis. We extract
sentence-level audio features from the poetry and incorporate audio from
multiple dialects,which may retain regional ancient Chinese phonetic features,
enriching the phonetic representation. Additionally, we generate sentence-level
visual features, and the multimodal features are fused with textual features
enhanced by LLM translation through multimodal contrastive representation
learning. Our framework outperforms state-of-the-art methods on two public
datasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro
F1. We open-source the code to facilitate research in this area and provide
insights for general multimodal Chinese representation.

</details>

### [496] [SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science](https://arxiv.org/abs/2505.13220)
*Jie Ying,Zihong Chen,Zhefan Wang,Wanli Jiang,Chenyang Wang,Zhonghang Yuan,Haoyang Su,Huanjun Kong,Fan Yang,Nanqing Dong*

Main category: cs.CL

TLDR: SeedBench是首个针对种子科学的多任务基准，旨在填补大语言模型（LLMs）在该领域的应用空白，并评估了26种领先的LLMs。


<details>
  <summary>Details</summary>
Motivation: 种子科学对现代农业至关重要，但面临跨学科复杂性、高成本和资源不足等挑战，LLMs的应用受限。

Method: 开发SeedBench基准，模拟现代育种过程，并与领域专家合作，评估26种LLMs。

Result: 评估揭示了LLMs在解决种子科学实际问题中的显著差距。

Conclusion: SeedBench为LLMs在种子设计领域的研究奠定了基础。

Abstract: Seed science is essential for modern agriculture, directly influencing crop
yields and global food security. However, challenges such as interdisciplinary
complexity and high costs with limited returns hinder progress, leading to a
shortage of experts and insufficient technological support. While large
language models (LLMs) have shown promise across various fields, their
application in seed science remains limited due to the scarcity of digital
resources, complex gene-trait relationships, and the lack of standardized
benchmarks. To address this gap, we introduce SeedBench -- the first multi-task
benchmark specifically designed for seed science. Developed in collaboration
with domain experts, SeedBench focuses on seed breeding and simulates key
aspects of modern breeding processes. We conduct a comprehensive evaluation of
26 leading LLMs, encompassing proprietary, open-source, and domain-specific
fine-tuned models. Our findings not only highlight the substantial gaps between
the power of LLMs and the real-world seed science problems, but also make a
foundational step for research on LLMs for seed design.

</details>

### [497] [JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models](https://arxiv.org/abs/2505.13244)
*Jieying Xue,Phuong Minh Nguyen,Minh Le Nguyen,Xin Liu*

Main category: cs.CL

TLDR: 论文研究了多语言多标签情感检测，提出了两种方法（基础方法和成对方法），并在SemEval-2025任务11中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 随着全球数字化的快速发展，多语言情感检测成为重要研究方向，旨在解决跨语言情感识别的挑战。

Method: 采用预训练多语言模型，包括微调的BERT分类模型和指令调优的生成式LLM，并提出基础方法和成对方法处理多标签分类。

Result: 在Track A中，方法在10种语言中排名前4，印地语排名第1；在Track B中，7种语言排名前5。

Conclusion: 该方法在多语言情感检测中表现出强大的泛化能力和有效性。

Abstract: With the rapid advancement of global digitalization, users from different
countries increasingly rely on social media for information exchange. In this
context, multilingual multi-label emotion detection has emerged as a critical
research area. This study addresses SemEval-2025 Task 11: Bridging the Gap in
Text-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:
(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.
To tackle multilingual challenges, we leverage pre-trained multilingual models
and focus on two architectures: (1) a fine-tuned BERT-based classification
model and (2) an instruction-tuned generative LLM. Additionally, we propose two
methods for handling multi-label classification: the base method, which maps an
input directly to all its corresponding emotion labels, and the pairwise
method, which models the relationship between the input text and each emotion
category individually. Experimental results demonstrate the strong
generalization ability of our approach in multilingual emotion recognition. In
Track A, our method achieved Top 4 performance across 10 languages, ranking 1st
in Hindi. In Track B, our approach also secured Top 5 performance in 7
languages, highlighting its simplicity and effectiveness\footnote{Our code is
available at https://github.com/yingjie7/mlingual_multilabel_emo_detection.

</details>

### [498] [Stronger Together: Unleashing the Social Impact of Hate Speech Research](https://arxiv.org/abs/2505.13251)
*Sidney Wong*

Main category: cs.CL

TLDR: 互联网对边缘化社区既是机遇也是挑战，需从社会角度而非纯技术手段解决网络仇恨言论问题。


<details>
  <summary>Details</summary>
Motivation: 互联网既能连接社区，也可能传播仇恨与错误信息，需社会方法解决这一社会问题。

Method: 借鉴语言学研究的经验，将语言与社会知识应用于数字空间的反社会行为风险缓解。

Result: 语言学家与NLP研究者可通过与社区、政策制定者合作，推动数字包容与缩小数字鸿沟。

Conclusion: 语言学与社会研究的结合能有效应对网络仇恨言论，促进公平的数字包容。

Abstract: The advent of the internet has been both a blessing and a curse for once
marginalised communities. When used well, the internet can be used to connect
and establish communities crossing different intersections; however, it can
also be used as a tool to alienate people and communities as well as perpetuate
hate, misinformation, and disinformation especially on social media platforms.
We propose steering hate speech research and researchers away from pre-existing
computational solutions and consider social methods to inform social solutions
to address this social problem. In a similar way linguistics research can
inform language planning policy, linguists should apply what we know about
language and society to mitigate some of the emergent risks and dangers of
anti-social behaviour in digital spaces. We argue linguists and NLP researchers
can play a principle role in unleashing the social impact potential of
linguistics research working alongside communities, advocates, activists, and
policymakers to enable equitable digital inclusion and to close the digital
divide.

</details>

### [499] [Natural Language Planning via Coding and Inference Scaling](https://arxiv.org/abs/2505.13252)
*Rikhil Amonkar,Ronan Le Bras,Li Zhang*

Main category: cs.CL

TLDR: 论文研究了LLMs在复杂文本规划任务（如会议安排）中的表现，比较了闭源和开源模型生成程序代码的能力，发现编程通常优于规划，但生成的代码存在鲁棒性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在复杂文本规划任务中的表现，尤其是闭源和开源模型在生成可执行程序代码方面的差异。

Method: 系统评估闭源和开源模型，包括支持输出长度随复杂度扩展的模型，生成Python代码和约束满足问题求解器代码。

Result: 编程通常优于规划，但生成的代码缺乏鲁棒性和效率，影响泛化能力。

Conclusion: 尽管编程在某些情况下表现更好，但生成的代码仍需改进以提高鲁棒性和效率。

Abstract: Real-life textual planning tasks such as meeting scheduling have posed much
challenge to LLMs especially when the complexity is high. While previous work
primarily studied auto-regressive generation of plans with closed-source
models, we systematically evaluate both closed- and open-source models,
including those that scales output length with complexity during inference, in
generating programs, which are executed to output the plan. We consider not
only standard Python code, but also the code to a constraint satisfaction
problem solver. Despite the algorithmic nature of the task, we show that
programming often but not always outperforms planning. Our detailed error
analysis also indicates a lack of robustness and efficiency in the generated
code that hinders generalization.

</details>

### [500] [HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding](https://arxiv.org/abs/2505.13254)
*Siran Liu,Yang Ye,Qianchao Zhu,Zheng Cao,Yongchao He*

Main category: cs.CL

TLDR: HeteroSpec是一种异构自适应的推测解码框架，通过动态优化计算资源分配，显著提升大型语言模型（LLM）推理速度。


<details>
  <summary>Details</summary>
Motivation: 自回归解码是LLM推理的标准方法，但其顺序性导致效率低下。现有推测解码算法未能充分利用语言复杂性的异构性，导致资源分配不优。

Method: 提出HeteroSpec框架，包括基于累积元路径Top-K熵的上下文预测机制和动态资源分配策略。

Result: 在五个基准测试和四个模型上，HeteroSpec平均加速4.26倍，优于现有EAGLE-3方法。

Conclusion: HeteroSpec无需重新训练草案模型，开销低，且与其他加速技术兼容，为上下文感知的LLM推理加速提供了新范式。

Abstract: Autoregressive decoding, the standard approach for Large Language Model (LLM)
inference, remains a significant bottleneck due to its sequential nature. While
speculative decoding algorithms mitigate this inefficiency through parallel
verification, they fail to exploit the inherent heterogeneity in linguistic
complexity, a key factor leading to suboptimal resource allocation. We address
this by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding
framework that dynamically optimizes computational resource allocation based on
linguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A
novel cumulative meta-path Top-$K$ entropy metric for efficiently identifying
predictable contexts. (2) A dynamic resource allocation strategy based on
data-driven entropy partitioning, enabling adaptive speculative expansion and
pruning tailored to local context difficulty. Evaluated on five public
benchmarks and four models, HeteroSpec achieves an average speedup of
4.26$\times$. It consistently outperforms state-of-the-art EAGLE-3 across
speedup rates, average acceptance length, and verification cost. Notably,
HeteroSpec requires no draft model retraining, incurs minimal overhead, and is
orthogonal to other acceleration techniques. It demonstrates enhanced
acceleration with stronger draft models, establishing a new paradigm for
context-aware LLM inference acceleration.

</details>

### [501] [WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?](https://arxiv.org/abs/2505.13257)
*Zilu Tang,Afra Feyza Akyürek,Ekin Akyürek,Derry Wijaya*

Main category: cs.CL

TLDR: 论文介绍了WikiPersona数据集，用于解决个性化偏好对齐问题，并提出了一种基于推断个人偏好的前缀方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注通用人类偏好对齐，而忽略了个人层面的细微偏好差异，缺乏相关数据集。

Method: 引入WikiPersona数据集，通过生成可验证的文本描述来对齐名人偏好，并评估不同个性化方法。

Result: 发现基于推断个人偏好的前缀方法在冲突偏好主题中表现更优，且能泛化到未见过的个人。

Conclusion: 推断个人偏好前缀方法在个性化对齐中更有效，尤其是在偏好冲突的场景下。

Abstract: Preference alignment has become a standard pipeline in finetuning models to
follow \emph{generic} human preferences. Majority of work seeks to optimize
model to produce responses that would be preferable \emph{on average},
simplifying the diverse and often \emph{contradicting} space of human
preferences. While research has increasingly focused on personalized alignment:
adapting models to individual user preferences, there is a lack of personalized
preference dataset which focus on nuanced individual-level preferences. To
address this, we introduce WikiPersona: the first fine-grained personalization
using well-documented, famous individuals. Our dataset challenges models to
align with these personas through an interpretable process: generating
verifiable textual descriptions of a persona's background and preferences in
addition to alignment. We systematically evaluate different personalization
approaches and find that as few-shot prompting with preferences and fine-tuning
fail to simultaneously ensure effectiveness and efficiency, using
\textit{inferred personal preferences} as prefixes enables effective
personalization, especially in topics where preferences clash while leading to
more equitable generalization across unseen personas.

</details>

### [502] [Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability](https://arxiv.org/abs/2505.13258)
*Jingyi Ren,Yekun Xu,Xiaolong Wang,Weitao Li,Weizhi Ma,Yang Liu*

Main category: cs.CL

TLDR: ARENA框架通过强化学习提升RAG生成器的透明性和有效性，在多跳QA数据集上表现优于基线10-30%。


<details>
  <summary>Details</summary>
Motivation: 解决RAG中生成器利用检索信息能力不足及缺乏透明度的问题。

Method: 提出ARENA框架，基于强化学习训练生成器，结合结构化生成和自适应奖励计算。

Result: 在多跳QA数据集上表现优于基线10-30%，与商业LLMs媲美。

Conclusion: ARENA框架显著提升RAG性能，具有灵活性和可解释性。

Abstract: Retrieval-Augmented Generation (RAG) has significantly improved the
performance of large language models (LLMs) on knowledge-intensive domains.
However, although RAG achieved successes across distinct domains, there are
still some unsolved challenges: 1) Effectiveness. Existing research mainly
focuses on developing more powerful RAG retrievers, but how to enhance the
generator's (LLM's) ability to utilize the retrieved information for reasoning
and generation? 2) Transparency. Most RAG methods ignore which retrieved
content actually contributes to the reasoning process, resulting in a lack of
interpretability and visibility. To address this, we propose ARENA
(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator
framework trained via reinforcement learning (RL) with our proposed rewards.
Based on the structured generation and adaptive reward calculation, our
RL-based training enables the model to identify key evidence, perform
structured reasoning, and generate answers with interpretable decision traces.
Applied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments
with various RAG baselines demonstrate that our model achieves 10-30%
improvements on all multi-hop QA datasets, which is comparable with the SOTA
Commercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses
show that ARENA has strong flexibility to be adopted on new datasets without
extra training. Our models and codes are publicly released.

</details>

### [503] [From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery](https://arxiv.org/abs/2505.13259)
*Tianshi Zheng,Zheye Deng,Hong Ting Tsang,Weiqi Wang,Jiaxin Bai,Zihao Wang,Yangqiu Song*

Main category: cs.CL

TLDR: 本文综述了大型语言模型（LLMs）在科学发现中的角色演变，提出了三级分类法（工具、分析师、科学家），并探讨了未来挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何从任务自动化工具发展为自主科学发现代理，重新定义科研流程与人机协作。

Method: 通过科学方法的视角，提出三级分类法（Tool, Analyst, Scientist）来划分LLMs的自主性和职责。

Result: 总结了LLMs在科学发现中的潜力，并识别了机器人自动化、自我改进和伦理治理等关键挑战。

Conclusion: 本文为AI驱动的科学发现提供了概念框架和战略展望，旨在促进创新与负责任的发展。

Abstract: Large Language Models (LLMs) are catalyzing a paradigm shift in scientific
discovery, evolving from task-specific automation tools into increasingly
autonomous agents and fundamentally redefining research processes and human-AI
collaboration. This survey systematically charts this burgeoning field, placing
a central focus on the changing roles and escalating capabilities of LLMs in
science. Through the lens of the scientific method, we introduce a foundational
three-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating
autonomy and evolving responsibilities within the research lifecycle. We
further identify pivotal challenges and future research trajectories such as
robotic automation, self-improvement, and ethical governance. Overall, this
survey provides a conceptual architecture and strategic foresight to navigate
and shape the future of AI-driven scientific discovery, fostering both rapid
innovation and responsible advancement. Github Repository:
https://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.

</details>

### [504] [Representation of perceived prosodic similarity of conversational feedback](https://arxiv.org/abs/2505.13268)
*Livia Qian,Carol Figueroa,Gabriel Skantze*

Main category: cs.CL

TLDR: 研究探讨了语音反馈的韵律相似性及其在语音表示中的体现，发现自监督和频谱表示优于基频特征，并通过对比学习进一步优化。


<details>
  <summary>Details</summary>
Motivation: 语音反馈在对话系统中对确保共同理解至关重要，但其韵律相似性及现有语音表示是否能反映这种相似性尚不明确。

Method: 采用三元比较任务，参与者评估来自两个数据集的语音反馈的感知相似性，并分析不同语音表示的效果。

Result: 自监督和频谱表示在编码韵律方面优于基频特征，尤其是同一说话者的反馈；对比学习可进一步优化表示与人类感知的对齐。

Conclusion: 研究为语音反馈的韵律表示提供了优化方向，支持通过对比学习提升语音表示与人类感知的一致性。

Abstract: Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of
spoken dialogue and is crucial to ensuring common ground in conversational
systems. The exact meaning of such feedback is conveyed through both lexical
and prosodic form. In this work, we investigate the perceived prosodic
similarity of vocal feedback with the same lexical form, and to what extent
existing speech representations reflect such similarities. A triadic comparison
task with recruited participants is used to measure perceived similarity of
feedback responses taken from two different datasets. We find that spectral and
self-supervised speech representations encode prosody better than extracted
pitch features, especially in the case of feedback from the same speaker. We
also find that it is possible to further condense and align the representations
to human perception through contrastive learning.

</details>

### [505] [CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning](https://arxiv.org/abs/2505.13271)
*Lei Sheng,Shuai-Shuai Xu*

Main category: cs.CL

TLDR: CSC-SQL整合了Self-Consistency和Self-Correction方法，通过并行采样和修正模型提升SQL生成质量，实验证明其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如Self-Consistency和Self-Correction）在SQL生成中存在局限性，如选择次优输出或仅修正语法错误。

Method: 提出CSC-SQL方法，结合两种技术，并通过GRPO算法微调模型。

Result: 在BIRD开发集上，3B和7B模型分别达到65.28%和69.19%的执行准确率。

Conclusion: CSC-SQL显著提升了SQL生成质量，代码将开源。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
translating natural language questions about relational databases into SQL
queries. In particular, test-time scaling techniques such as Self-Consistency
and Self-Correction can enhance SQL generation accuracy by increasing
computational effort during inference. However, these methods have notable
limitations: Self-Consistency may select suboptimal outputs despite majority
votes, while Self-Correction typically addresses only syntactic errors. To
leverage the strengths of both approaches, we propose CSC-SQL, a novel method
that integrates Self-Consistency and Self-Correction. CSC-SQL selects the two
most frequently occurring outputs from parallel sampling and feeds them into a
merge revision model for correction. Additionally, we employ the Group Relative
Policy Optimization (GRPO) algorithm to fine-tune both the SQL generation and
revision models via reinforcement learning, significantly enhancing output
quality. Experimental results confirm the effectiveness and generalizability of
CSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution
accuracy, while the 7B model achieves 69.19%. The code will be open sourced at
https://github.com/CycloneBoy/csc_sql.

</details>

### [506] [$\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion](https://arxiv.org/abs/2505.13282)
*Sahil Mishra,Kumar Arjun,Tanmoy Chakraborty*

Main category: cs.CL

TLDR: LORex是一个结合判别式排序和生成式推理的框架，用于高效扩展分类法，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着数据增长，分类法扩展变得重要，但现有方法在表示能力和泛化性上存在问题。

Method: LORex通过排序和分块候选词，过滤噪声并迭代优化选择，结合层次推理确保上下文效率。

Result: 在四个基准测试和十二个基线方法中，LORex将准确性提高了12%，Wu & Palmer相似性提高了5%。

Conclusion: LORex在分类法扩展任务中显著优于现有方法，具有更高的准确性和效率。

Abstract: Taxonomies are hierarchical knowledge graphs crucial for recommendation
systems, and web applications. As data grows, expanding taxonomies is
essential, but existing methods face key challenges: (1) discriminative models
struggle with representation limits and generalization, while (2) generative
methods either process all candidates at once, introducing noise and exceeding
context limits, or discard relevant entities by selecting noisy candidates. We
propose LORex ($\textbf{L}$ineage-$\textbf{O}$riented $\textbf{Re}$asoning for
Taxonomy E$\textbf{x}$pansion), a plug-and-play framework that combines
discriminative ranking and generative reasoning for efficient taxonomy
expansion. Unlike prior methods, LORex ranks and chunks candidate terms into
batches, filtering noise and iteratively refining selections by reasoning
candidates' hierarchy to ensure contextual efficiency. Extensive experiments
across four benchmarks and twelve baselines show that LORex improves accuracy
by 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.

</details>

### [507] [I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models](https://arxiv.org/abs/2505.13302)
*Alice Plebe,Timothy Douglas,Diana Riazi,R. Maria del Rio-Chanona*

Main category: cs.CL

TLDR: 研究探讨了视觉语言模型（VLMs）在新闻推荐系统中因图像存在而增加虚假新闻传播的风险，并分析了人物设定和内容属性对其行为的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在新闻推荐系统中的广泛应用，其传播虚假信息的风险引发关注。研究旨在揭示图像如何影响VLMs的新闻转发行为，并探索人物设定（如反社会特质和政治倾向）的调节作用。

Method: 采用一种基于越狱启发的提示策略，模拟具有反社会特质和政治倾向的用户行为，同时使用多模态数据集（PolitiFact的政治新闻及其图像和真实性标签）进行分析。

Result: 实验显示，图像存在使真实新闻转发率增加4.8%，虚假新闻增加15%。人物设定进一步调节此效应：黑暗三人格特质会放大虚假新闻转发，而共和党倾向用户对真实性敏感度降低。Claude-3-Haiku是唯一对视觉虚假信息表现稳健的模型。

Conclusion: 研究揭示了多模态模型行为中的潜在风险，呼吁开发针对个性化AI系统的评估框架和缓解策略。

Abstract: Large language models are increasingly integrated into news recommendation
systems, raising concerns about their role in spreading misinformation. In
humans, visual content is known to boost credibility and shareability of
information, yet its effect on vision-language models (VLMs) remains unclear.
We present the first study examining how images influence VLMs' propensity to
reshare news content, whether this effect varies across model families, and how
persona conditioning and content attributes modulate this behavior. To support
this analysis, we introduce two methodological contributions: a
jailbreaking-inspired prompting strategy that elicits resharing decisions from
VLMs while simulating users with antisocial traits and political alignments;
and a multimodal dataset of fact-checked political news from PolitiFact, paired
with corresponding images and ground-truth veracity labels. Experiments across
model families reveal that image presence increases resharing rates by 4.8% for
true news and 15.0% for false news. Persona conditioning further modulates this
effect: Dark Triad traits amplify resharing of false news, whereas
Republican-aligned profiles exhibit reduced veracity sensitivity. Of all the
tested models, only Claude-3-Haiku demonstrates robustness to visual
misinformation. These findings highlight emerging risks in multimodal model
behavior and motivate the development of tailored evaluation frameworks and
mitigation strategies for personalized AI systems. Code and dataset are
available at: https://github.com/3lis/misinfo_vlm

</details>

### [508] [RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.13307)
*Qiguang Chen,Libo Qin,Jinhao Liu,Yue Liao,Jiaqi Wang,Jingxuan Zhou,Wanxiang Che*

Main category: cs.CL

TLDR: 论文提出了Reasoning Boundary Framework++（RBF++），用于解决Chain-of-Thought（CoT）推理在现实应用中的两大挑战：定量评估和优化可测量边界，以及评估不可测量边界（如多模态感知）。


<details>
  <summary>Details</summary>
Motivation: 当前CoT推理在复杂任务中表现优异，但缺乏定量指标和可操作指南来评估和优化其能力边界，尤其是在多模态场景中。

Method: 提出RBF++框架，定义推理边界（RB）作为CoT性能上限，并提出组合定律和常数假设来处理可测量和不可测量边界。此外，引入推理边界划分机制，将不可测量边界分为两个子边界。

Result: 在13个任务中验证了38个模型，证明了框架在多模态场景中的可行性。评估了10种CoT策略，并扩展了评估基准。

Conclusion: RBF++框架为理解和优化LLM中的推理边界提供了新方法，推动了相关研究的发展。

Abstract: Chain-of-Thought (CoT) reasoning has proven effective in enhancing large
language models (LLMs) on complex tasks, spurring research into its underlying
mechanisms. However, two primary challenges remain for real-world applications:
(1) the lack of quantitative metrics and actionable guidelines for evaluating
and optimizing measurable boundaries of CoT capability, and (2) the absence of
methods to assess boundaries of unmeasurable CoT capability, such as multimodal
perception. To address these gaps, we introduce the Reasoning Boundary
Framework++ (RBF++). To tackle the first challenge, we define the reasoning
boundary (RB) as the maximum limit of CoT performance. We also propose a
combination law for RBs, enabling quantitative analysis and offering actionable
guidance across various CoT tasks. For the second challenge, particularly in
multimodal scenarios, we introduce a constant assumption, which replaces
unmeasurable RBs with scenario-specific constants. Additionally, we propose the
reasoning boundary division mechanism, which divides unmeasurable RBs into two
sub-boundaries, facilitating the quantification and optimization of both
unmeasurable domain knowledge and multimodal perception capabilities. Extensive
experiments involving 38 models across 13 tasks validate the feasibility of our
framework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,
offer insights into optimization and decay from two complementary perspectives,
and expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope
this work advances the understanding of RBs and optimization strategies in
LLMs. Code and data are available at
https://github.com/LightChen233/reasoning-boundary.

</details>

### [509] [GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection](https://arxiv.org/abs/2505.13312)
*Zhijie Deng,Chris Yuhao Liu,Zirui Pang,Xinlei He,Lei Feng,Qi Xuan,Zhaowei Zhu,Jiaheng Wei*

Main category: cs.CL

TLDR: GUARD框架通过动态推理时选择性遗忘，避免微调对模型性能的负面影响，实现高效遗忘。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）需要选择性遗忘特定知识以确保安全性和合规性，但现有方法通常以牺牲整体性能为代价。

Method: 提出GUARD框架，通过提示分类器检测遗忘目标，动态惩罚和过滤候选令牌，结合令牌匹配和语义匹配防止泄露遗忘内容。

Result: 在多个任务中（如版权内容遗忘和实体遗忘），GUARD表现出色，几乎不影响模型的通用能力。

Conclusion: GUARD在遗忘与模型性能之间取得了优异平衡，为LLMs的安全部署提供了有效解决方案。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
memorizing vast amounts of knowledge across diverse domains. However, the
ability to selectively forget specific knowledge is critical for ensuring the
safety and compliance of deployed models. Existing unlearning efforts typically
fine-tune the model with resources such as forget data, retain data, and a
calibration model. These additional gradient steps blur the decision boundary
between forget and retain knowledge, making unlearning often at the expense of
overall performance. To avoid the negative impact of fine-tuning, it would be
better to unlearn solely at inference time by safely guarding the model against
generating responses related to the forget target, without destroying the
fluency of text generation. In this work, we propose Generation-time Unlearning
via Adaptive Restriction and Detection (GUARD), a framework that enables
dynamic unlearning during LLM generation. Specifically, we first employ a
prompt classifier to detect unlearning targets and extract the corresponding
forbidden token. We then dynamically penalize and filter candidate tokens
during generation using a combination of token matching and semantic matching,
effectively preventing the model from leaking the forgotten content.
Experimental results on copyright content unlearning tasks over the Harry
Potter dataset and the MUSE benchmark, as well as entity unlearning tasks on
the TOFU dataset, demonstrate that GUARD achieves strong forget quality across
various tasks while causing almost no degradation to the LLM's general
capabilities, striking an excellent trade-off between forgetting and utility.

</details>

### [510] [Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges](https://arxiv.org/abs/2505.13328)
*Hongru Wang,Wenyu Huang,Yufei Wang,Yuanhao Xi,Jianqiao Lu,Huan Zhang,Nan Hu,Zeming Liu,Jeff Z. Pan,Kam-Fai Wong*

Main category: cs.CL

TLDR: 论文提出DialogTool数据集和VirtualMobile评估环境，用于多轮对话中状态感知的工具使用评估，发现现有LLM在长时工具使用上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注单轮或无状态交互，忽略了多轮应用中工具使用的状态性，需填补这一空白。

Method: 构建DialogTool多轮对话数据集和VirtualMobile虚拟环境，评估工具创建、使用和角色一致响应三阶段任务。

Result: 对13种开源和闭源LLM的综合评估显示，现有先进模型在长时工具使用上仍有不足。

Conclusion: 需进一步改进LLM在多轮状态感知工具使用中的表现。

Abstract: Existing benchmarks that assess Language Models (LMs) as Language Agents
(LAs) for tool use primarily focus on stateless, single-turn interactions or
partial evaluations, such as tool selection in a single turn, overlooking the
inherent stateful nature of interactions in multi-turn applications. To fulfill
this gap, we propose \texttt{DialogTool}, a multi-turn dialogue dataset with
stateful tool interactions considering the whole life cycle of tool use, across
six key tasks in three stages: 1) \textit{tool creation}; 2) \textit{tool
utilization}: tool awareness, tool selection, tool execution; and 3)
\textit{role-consistent response}: response generation and role play.
Furthermore, we build \texttt{VirtualMobile} -- an embodied virtual mobile
evaluation environment to simulate API calls and assess the robustness of the
created APIs\footnote{We will use tools and APIs alternatively, there are no
significant differences between them in this paper.}. Taking advantage of these
artifacts, we conduct comprehensive evaluation on 13 distinct open- and
closed-source LLMs and provide detailed analysis at each stage, revealing that
the existing state-of-the-art LLMs still cannot perform well to use tools over
long horizons.

</details>

### [511] [Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation](https://arxiv.org/abs/2505.13338)
*Qiongqiong Wang,Hardik B. Sailor,Tianchi Liu,Ai Ti Aw*

Main category: cs.CL

TLDR: 论文提出了一种新框架，用于从真实语音数据生成结合上下文推理和副语言信息的QA数据集，验证了其有效性，并揭示了语音-LLM在共情推理任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前语音-LLM在上下文推理和副语言理解方面能力有限，缺乏覆盖这两方面的QA数据集。

Method: 提出了一种新框架，包括基于伪副语言标签的数据浓缩和基于LLM的上下文副语言QA生成。

Result: 验证了框架的有效性，并发现语音-LLM在处理共情推理任务时的局限性。

Conclusion: 该框架是首创，有望训练出更具鲁棒性的语音-LLM。

Abstract: Current speech-LLMs exhibit limited capability in contextual reasoning
alongside paralinguistic understanding, primarily due to the lack of
Question-Answer (QA) datasets that cover both aspects. We propose a novel
framework for dataset generation from in-the-wild speech data, that integrates
contextual reasoning with paralinguistic information. It consists of a pseudo
paralinguistic label-based data condensation of in-the-wild speech and
LLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is
validated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct
model on a dataset created by our framework and human-generated CPQA dataset.
The results also reveal the speech-LLM's limitations in handling empathetic
reasoning tasks, highlighting the need for such datasets and more robust
models. The proposed framework is first of its kind and has potential in
training more robust speech-LLMs with paralinguistic reasoning capabilities.

</details>

### [512] [J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization](https://arxiv.org/abs/2505.13346)
*Austin Xu,Yilun Zhou,Xuan-Phi Nguyen,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TLDR: 论文提出了一种基于强化学习的LLM评估方法，通过EIS-GRPO算法解决位置偏差问题，并引入新基准ReasoningJudgeBench。训练的J4R模型在推理任务中表现优于GPT-4o和其他小型模型。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的发展，传统人工评估效率低下，自动评估成为趋势。然而，现有LLM评估模型在复杂推理任务中表现不佳，需要改进。

Method: 提出EIS-GRPO算法以减少位置偏差，并开发ReasoningJudgeBench基准。训练7B规模的J4R模型，使用强化学习优化评估能力。

Result: J4R在JudgeBench和ReasoningJudgeBench上表现优于GPT-4o和其他小型模型，分别提升6.7%和9%。

Conclusion: 通过强化学习训练的J4R模型显著提升了复杂推理任务的评估能力，为LLM自动评估提供了新方向。

Abstract: To keep pace with the increasing pace of large language models (LLM)
development, model output evaluation has transitioned away from time-consuming
human evaluation to automatic evaluation, where LLMs themselves are tasked with
assessing and critiquing other model outputs. LLM-as-judge models are a class
of generative evaluators that excel in evaluating relatively simple domains,
like chat quality, but struggle in reasoning intensive domains where model
responses contain more substantive and challenging content. To remedy existing
judge shortcomings, we explore training judges with reinforcement learning
(RL). We make three key contributions: (1) We propose the Equivalent Initial
State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us
to train our judge to be robust to positional biases that arise in more complex
evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that
evaluates judges in diverse reasoning settings not covered by prior work. (3)
We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that
outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or
exceeding the performance of larger GRPO-trained judges on both JudgeBench and
ReasoningJudgeBench.

</details>

### [513] [Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks](https://arxiv.org/abs/2505.13348)
*Narek Maloyan,Bislan Ashinov,Dmitry Namiot*

Main category: cs.CL

TLDR: 论文研究了LLM作为评估者时对提示注入攻击的脆弱性，提出了两种攻击策略并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM作为评估者的可靠性和安全性，特别是对抗对抗性操纵的鲁棒性。

Method: 使用GCG优化方法设计对抗性后缀，并在开源LLM上进行实验。

Result: CUA攻击成功率超过30%，JMA也表现显著。

Conclusion: 当前LLM评估系统存在重大漏洞，需加强防御机制和研究对抗性评估。

Abstract: Large Language Models (LLMs) are increasingly employed as evaluators
(LLM-as-a-Judge) for assessing the quality of machine-generated text. This
paradigm offers scalability and cost-effectiveness compared to human
annotation. However, the reliability and security of such systems, particularly
their robustness against adversarial manipulations, remain critical concerns.
This paper investigates the vulnerability of LLM-as-a-Judge architectures to
prompt-injection attacks, where malicious inputs are designed to compromise the
judge's decision-making process. We formalize two primary attack strategies:
Comparative Undermining Attack (CUA), which directly targets the final decision
output, and Justification Manipulation Attack (JMA), which aims to alter the
model's generated reasoning. Using the Greedy Coordinate Gradient (GCG)
optimization method, we craft adversarial suffixes appended to one of the
responses being compared. Experiments conducted on the MT-Bench Human Judgments
dataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and
Falcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves
an Attack Success Rate (ASR) exceeding 30\%, while JMA also shows notable
effectiveness. These findings highlight substantial vulnerabilities in current
LLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and
further research into adversarial evaluation and trustworthiness in LLM-based
assessment frameworks.

</details>

### [514] [Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning](https://arxiv.org/abs/2505.13353)
*Adam Štorek,Mukur Gupta,Samira Hajizadeh,Prashast Srivastava,Suman Jana*

Main category: cs.CL

TLDR: 该论文研究了大型语言模型（LLMs）在代码推理中对长上下文的利用能力，区分了词汇代码回忆和语义代码回忆，并提出了SemTrace技术来衡量语义回忆。研究发现，随着代码片段接近输入上下文的中间位置，代码推理准确性显著下降，且词汇回忆与语义回忆之间存在脱节。


<details>
  <summary>Details</summary>
Motivation: 现代LLMs支持极大的上下文，但其在代码推理中利用长上下文的有效性尚不明确。论文旨在探究LLMs在大型代码库中的推理能力及其与回忆能力的关系。

Method: 论文提出了SemTrace技术来衡量语义代码回忆，并开发了一种量化现有基准中语义回忆敏感性的方法。对最先进的LLMs进行了评估。

Result: 研究发现，代码推理准确性在代码片段接近输入上下文中间时显著下降，尤其是需要高语义回忆的技术（如SemTrace）。词汇回忆因粒度不同而异，模型在函数检索上表现良好，但在逐行回忆上表现不佳。词汇回忆与语义回忆之间存在脱节。

Conclusion: 当前代码推理基准可能低估了LLMs在利用上下文信息时的挑战，因其语义回忆敏感性较低。

Abstract: Although modern Large Language Models (LLMs) support extremely large
contexts, their effectiveness in utilizing long context for code reasoning
remains unclear. This paper investigates LLM reasoning ability over code
snippets within large repositories and how it relates to their recall ability.
Specifically, we differentiate between lexical code recall (verbatim retrieval)
and semantic code recall (remembering what the code does). To measure semantic
recall, we propose SemTrace, a code reasoning technique where the impact of
specific statements on output is attributable and unpredictable. We also
present a method to quantify semantic recall sensitivity in existing
benchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop
in code reasoning accuracy as a code snippet approaches the middle of the input
context, particularly with techniques requiring high semantic recall like
SemTrace. Moreover, we find that lexical recall varies by granularity, with
models excelling at function retrieval but struggling with line-by-line recall.
Notably, a disconnect exists between lexical and semantic recall, suggesting
different underlying mechanisms. Finally, our findings indicate that current
code reasoning benchmarks may exhibit low semantic recall sensitivity,
potentially underestimating LLM challenges in leveraging in-context
information.

</details>

### [515] [What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts](https://arxiv.org/abs/2505.13360)
*Chenyang Yang,Yike Shi,Qianou Ma,Michael Xieyang Liu,Christian Kästner,Tongshuang Wu*

Main category: cs.CL

TLDR: 论文分析了LLM提示中的未充分指定问题，提出了一种新的需求感知提示优化机制，性能提升4.8%。


<details>
  <summary>Details</summary>
Motivation: 开发者通过自然语言与LLM沟通时，提示常未充分指定用户重要需求，导致性能不稳定。

Method: 分析了提示未充分指定的影响，提出需求感知提示优化机制。

Result: 未充分指定的提示在模型或提示变化时性能下降2倍，新机制平均提升4.8%。

Conclusion: 有效管理提示未充分指定需包括需求发现、评估和监控的全面流程。

Abstract: Building LLM-powered software requires developers to communicate their
requirements through natural language, but developer prompts are frequently
underspecified, failing to fully capture many user-important requirements. In
this paper, we present an in-depth analysis of prompt underspecification,
showing that while LLMs can often (41.1%) guess unspecified requirements by
default, such behavior is less robust: Underspecified prompts are 2x more
likely to regress over model or prompt changes, sometimes with accuracy drops
by more than 20%. We then demonstrate that simply adding more requirements to a
prompt does not reliably improve performance, due to LLMs' limited
instruction-following capabilities and competing constraints, and standard
prompt optimizers do not offer much help. To address this, we introduce novel
requirements-aware prompt optimization mechanisms that can improve performance
by 4.8% on average over baselines that naively specify everything in the
prompt. Beyond prompt optimization, we envision that effectively managing
prompt underspecification requires a broader process, including proactive
requirements discovery, evaluation, and monitoring.

</details>

### [516] [Thinkless: LLM Learns When to Think](https://arxiv.org/abs/2505.13379)
*Gongfan Fang,Xinyin Ma,Xinchao Wang*

Main category: cs.CL

TLDR: Thinkless框架通过自适应选择短链或长链推理，提升推理语言模型的效率，减少50%-90%的长链推理使用。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs对所有查询均采用复杂推理导致的低效问题，探索模型是否能学会何时思考。

Method: 提出Thinkless框架，基于任务复杂度和模型能力选择推理模式，采用DeGRPO算法分解学习目标为控制令牌损失和响应损失。

Result: 在多个基准测试中显著减少长链推理使用，提升模型效率。

Conclusion: Thinkless有效平衡推理效率与准确性，为LLMs的实用化提供新思路。

Abstract: Reasoning Language Models, capable of extended chain-of-thought reasoning,
have demonstrated remarkable performance on tasks requiring complex logical
inference. However, applying elaborate reasoning for all queries often results
in substantial computational inefficiencies, particularly when many problems
admit straightforward solutions. This motivates an open question: Can LLMs
learn when to think? To answer this, we propose Thinkless, a learnable
framework that empowers an LLM to adaptively select between short-form and
long-form reasoning, based on both task complexity and the model's ability.
Thinkless is trained under a reinforcement learning paradigm and employs two
control tokens, <short> for concise responses and <think> for detailed
reasoning. At the core of our method is a Decoupled Group Relative Policy
Optimization (DeGRPO) algorithm, which decomposes the learning objective of
hybrid reasoning into two components: (1) a control token loss that governs the
selection of the reasoning mode, and (2) a response loss that improves the
accuracy of the generated answers. This decoupled formulation enables
fine-grained control over the contributions of each objective, stabilizing
training and effectively preventing collapse observed in vanilla GRPO.
Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and
GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -
90%, significantly improving the efficiency of Reasoning Language Models. The
code is available at https://github.com/VainF/Thinkless

</details>

### [517] [R3: Robust Rubric-Agnostic Reward Models](https://arxiv.org/abs/2505.13388)
*David Anugraha,Zilu Tang,Lester James V. Miranda,Hanyang Zhao,Mohammad Rifqi Farhansyah,Garry Kuwanto,Derry Wijaya,Genta Indra Winata*

Main category: cs.CL

TLDR: R3是一个新的奖励建模框架，解决了现有方法在可控性和可解释性上的不足，支持更透明和灵活的语言模型评估。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型缺乏可控性和可解释性，且优化目标狭窄，限制了其泛化能力。

Method: 提出R3框架，支持多维度评估并提供可解释的评分。

Result: R3实现了更透明和灵活的语言模型评估，支持多样化的人类价值观和应用场景。

Conclusion: R3为奖励建模提供了更通用和可解释的解决方案，开源资源可供使用。

Abstract: Reward models are essential for aligning language model outputs with human
preferences, yet existing approaches often lack both controllability and
interpretability. These models are typically optimized for narrow objectives,
limiting their generalizability to broader downstream tasks. Moreover, their
scalar outputs are difficult to interpret without contextual reasoning. To
address these limitations, we introduce R3, a novel reward modeling framework
that is rubric-agnostic, generalizable across evaluation dimensions, and
provides interpretable, reasoned score assignments. R3 enables more transparent
and flexible evaluation of language models, supporting robust alignment with
diverse human values and use cases. Our models, data, and code are available as
open source at https://github.com/rubricreward/r3

</details>

### [518] [MR. Judge: Multimodal Reasoner as a Judge](https://arxiv.org/abs/2505.13403)
*Renjie Pi,Felix Bai,Qibin Chen,Simon Wang,Jiulong Shan,Kieran Liu,Meng Cao*

Main category: cs.CL

TLDR: 提出了一种名为MR. Judge的多模态推理评判范式，通过将评判过程转化为多选推理问题，提升MLLM评判的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评判方法直接评分缺乏解释性，且性能有限，需增强其推理能力。

Method: 将评判过程设计为多选推理问题，通过反向生成候选答案和文本推理提取策略自动标注数据。

Result: MR. Judge-7B在VL-RewardBench上超越GPT-4o 9.9%，在MM-Vet上提升7.7%。

Conclusion: MR. Judge通过推理驱动的评判范式显著提升了MLLM的性能和可解释性。

Abstract: The paradigm of using Large Language Models (LLMs) and Multimodal Large
Language Models (MLLMs) as evaluative judges has emerged as an effective
approach in RLHF and inference-time scaling. In this work, we propose
Multimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering
general-purpose MLLMs judges with strong reasoning capabilities. Instead of
directly assigning scores for each response, we formulate the judgement process
as a reasoning-inspired multiple-choice problem. Specifically, the judge model
first conducts deliberate reasoning covering different aspects of the responses
and eventually selects the best response from them. This reasoning process not
only improves the interpretibility of the judgement, but also greatly enhances
the performance of MLLM judges. To cope with the lack of questions with scored
responses, we propose the following strategy to achieve automatic annotation:
1) Reverse Response Candidates Synthesis: starting from a supervised
fine-tuning (SFT) dataset, we treat the original response as the best candidate
and prompt the MLLM to generate plausible but flawed negative candidates. 2)
Text-based reasoning extraction: we carefully design a data synthesis pipeline
for distilling the reasoning capability from a text-based reasoning model,
which is adopted to enable the MLLM judges to regain complex reasoning ability
via warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge
is effective across a wide range of tasks. Specifically, our MR. Judge-7B
surpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet
during inference-time scaling by up to 7.7%.

</details>

### [519] [Granary: Speech Recognition and Translation Dataset in 25 European Languages](https://arxiv.org/abs/2505.13404)
*Nithin Rao Koluguri,Monica Sekoyan,George Zelenfroynd,Sasha Meister,Shuoyang Ding,Sofia Kostandian,He Huang,Nikolay Karpov,Jagadeesh Balam,Vitaly Lavrukhin,Yifan Peng,Sara Papi,Marco Gaido,Alessio Brutti,Boris Ginsburg*

Main category: cs.CL

TLDR: Granary是一个大规模的多语言语音数据集，支持25种欧洲语言的识别和翻译，通过伪标注和数据过滤提升质量，模型训练效率高且性能优异。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言语音处理数据稀缺的问题。

Method: 采用伪标注流程（包括分段、两遍推理、幻觉过滤和标点恢复）和数据过滤生成高质量数据集。

Result: 模型在减少50%数据量的情况下仍能保持相似性能。

Conclusion: Granary为低资源语言语音处理提供了高效且高质量的数据解决方案。

Abstract: Multi-task and multilingual approaches benefit large models, yet speech
processing for low-resource languages remains underexplored due to data
scarcity. To address this, we present Granary, a large-scale collection of
speech datasets for recognition and translation across 25 European languages.
This is the first open-source effort at this scale for both transcription and
translation. We enhance data quality using a pseudo-labeling pipeline with
segmentation, two-pass inference, hallucination filtering, and punctuation
restoration. We further generate translation pairs from pseudo-labeled
transcriptions using EuroLLM, followed by a data filtration pipeline. Designed
for efficiency, our pipeline processes vast amount of data within hours. We
assess models trained on processed data by comparing their performance on
previously curated datasets for both high- and low-resource languages. Our
findings show that these models achieve similar performance using approx. 50%
less data. Dataset will be made available at
https://hf.co/datasets/nvidia/Granary

</details>

### [520] [AdaptThink: Reasoning Models Can Learn When to Think](https://arxiv.org/abs/2505.13417)
*Jiajie Zhang,Nianyi Lin,Lei Hou,Ling Feng,Juanzi Li*

Main category: cs.CL

TLDR: AdaptThink是一种新型RL算法，通过自适应选择思考模式（Thinking或NoThinking）来优化推理模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型的深度思考过程增加了推理开销，效率成为瓶颈。研究发现，对于简单任务，直接生成解决方案（NoThinking）在性能和效率上更优。

Method: 提出AdaptThink算法，包含两个核心组件：1）约束优化目标，鼓励模型选择NoThinking同时保持性能；2）重要性采样策略，平衡训练中的思考模式样本。

Result: 实验表明，AdaptThink显著降低推理成本并提升性能，在三个数学数据集上平均响应长度减少53%，准确率提升2.4%。

Conclusion: AdaptThink通过自适应选择思考模式，有效平衡了推理质量和效率，具有广泛应用潜力。

Abstract: Recently, large reasoning models have achieved impressive performance on
various tasks by employing human-like deep thinking. However, the lengthy
thinking process substantially increases inference overhead, making efficiency
a critical bottleneck. In this work, we first demonstrate that NoThinking,
which prompts the reasoning model to skip thinking and directly generate the
final solution, is a better choice for relatively simple tasks in terms of both
performance and efficiency. Motivated by this, we propose AdaptThink, a novel
RL algorithm to teach reasoning models to choose the optimal thinking mode
adaptively based on problem difficulty. Specifically, AdaptThink features two
core components: (1) a constrained optimization objective that encourages the
model to choose NoThinking while maintaining the overall performance; (2) an
importance sampling strategy that balances Thinking and NoThinking samples
during on-policy training, thereby enabling cold start and allowing the model
to explore and exploit both thinking modes throughout the training process. Our
experiments indicate that AdaptThink significantly reduces the inference costs
while further enhancing performance. Notably, on three math datasets,
AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B
by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive
thinking-mode selection for optimizing the balance between reasoning quality
and efficiency. Our codes and models are available at
https://github.com/THU-KEG/AdaptThink.

</details>

### [521] [Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness](https://arxiv.org/abs/2505.13418)
*Lotem Peled-Cohen,Maya Zadok,Nitay Calderon,Hila Gonen,Roi Reichart*

Main category: cs.CL

TLDR: 论文研究了非专家和LLMs如何通过语言感知痴呆症，发现人类依赖有限且有时误导的线索，而LLMs使用更丰富、更符合临床特征的特征集。


<details>
  <summary>Details</summary>
Motivation: 探索非专家和LLMs如何通过语言直觉判断痴呆症，以帮助非专家更准确地识别相关语言迹象。

Method: 通过转录的图片描述，让非专家和LLMs判断文本是否来自健康人或痴呆患者，并引入可解释的方法提取特征，用逻辑回归建模。

Result: 人类判断不一致且依赖有限线索，LLMs特征更丰富且接近临床模式，但两者均易漏诊。

Conclusion: 研究框架有助于非专家更准确地识别痴呆症的语言特征。

Abstract: Cognitive decline often surfaces in language years before diagnosis. It is
frequently non-experts, such as those closest to the patient, who first sense a
change and raise concern. As LLMs become integrated into daily communication
and used over prolonged periods, it may even be an LLM that notices something
is off. But what exactly do they notice--and should be noticing--when making
that judgment? This paper investigates how dementia is perceived through
language by non-experts. We presented transcribed picture descriptions to
non-expert humans and LLMs, asking them to intuitively judge whether each text
was produced by someone healthy or with dementia. We introduce an explainable
method that uses LLMs to extract high-level, expert-guided features
representing these picture descriptions, and use logistic regression to model
human and LLM perceptions and compare with clinical diagnoses. Our analysis
reveals that human perception of dementia is inconsistent and relies on a
narrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a
richer, more nuanced feature set that aligns more closely with clinical
patterns. Still, both groups show a tendency toward false negatives, frequently
overlooking dementia cases. Through our interpretable framework and the
insights it provides, we hope to help non-experts better recognize the
linguistic signs that matter.

</details>

### [522] [SMOTExT: SMOTE meets Large Language Models](https://arxiv.org/abs/2505.13434)
*Mateusz Bystroński,Mikołaj Hołysz,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.CL

TLDR: SMOTExT是一种将SMOTE技术应用于文本数据的新方法，通过BERT嵌入插值和xRAG架构生成合成文本，解决数据稀缺和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决NLP模型在专业领域或低资源环境中的数据稀缺和类别不平衡问题。

Method: 利用BERT嵌入插值和xRAG架构生成合成文本。

Result: 初步结果表明，该方法在少样本学习和隐私保护机器学习中具有潜力。

Conclusion: SMOTExT为数据保护和高效学习提供了可行路径。

Abstract: Data scarcity and class imbalance are persistent challenges in training
robust NLP models, especially in specialized domains or low-resource settings.
We propose a novel technique, SMOTExT, that adapts the idea of Synthetic
Minority Over-sampling (SMOTE) to textual data. Our method generates new
synthetic examples by interpolating between BERT-based embeddings of two
existing examples and then decoding the resulting latent point into text with
xRAG architecture. By leveraging xRAG's cross-modal retrieval-generation
framework, we can effectively turn interpolated vectors into coherent text.
While this is preliminary work supported by qualitative outputs only, the
method shows strong potential for knowledge distillation and data augmentation
in few-shot settings. Notably, our approach also shows promise for
privacy-preserving machine learning: in early experiments, training models
solely on generated data achieved comparable performance to models trained on
the original dataset. This suggests a viable path toward safe and effective
learning under data protection constraints.

</details>

### [523] [ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models](https://arxiv.org/abs/2505.13444)
*Liyan Tang,Grace Kim,Xinyu Zhao,Thom Lake,Wenxuan Ding,Fangcong Yin,Prasann Singhal,Manya Wadhwa,Zeyu Leo Liu,Zayne Sprague,Ramya Namuduri,Bodun Hu,Juan Diego Rodriguez,Puyuan Peng,Greg Durrett*

Main category: cs.CL

TLDR: 论文探讨了大型视觉语言模型（LVLMs）在图表理解中的挑战，指出其视觉推理能力不足，并提出了新的基准测试ChartMuseum，揭示了模型与人类表现的显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前LVLMs在图表理解中表现出视觉与文本推理能力的不平衡，尤其是视觉推理能力较弱。

Method: 通过合成数据集和新的基准测试ChartMuseum（包含1,162个专家标注的问题），评估模型的视觉与文本推理能力。

Result: 人类准确率为93%，最佳模型Gemini-2.5-Pro为63.0%，开源模型Qwen2.5-VL-72B-Instruct仅为38.5%。视觉推理问题中，模型性能下降35%-55%。

Conclusion: 当前LVLMs在复杂视觉推理任务中表现不佳，需进一步改进。

Abstract: Chart understanding presents a unique challenge for large vision-language
models (LVLMs), as it requires the integration of sophisticated textual and
visual reasoning capabilities. However, current LVLMs exhibit a notable
imbalance between these skills, falling short on visual reasoning that is
difficult to perform in text. We conduct a case study using a synthetic dataset
solvable only through visual reasoning and show that model performance degrades
significantly with increasing visual complexity, while human performance
remains robust. We then introduce ChartMuseum, a new Chart Question Answering
(QA) benchmark containing 1,162 expert-annotated questions spanning multiple
reasoning types, curated from real-world charts across 184 sources,
specifically built to evaluate complex visual and textual reasoning. Unlike
prior chart understanding benchmarks -- where frontier models perform similarly
and near saturation -- our benchmark exposes a substantial gap between model
and human performance, while effectively differentiating model capabilities:
although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro
attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct
achieves only 38.5%. Moreover, on questions requiring primarily visual
reasoning, all models experience a 35%-55% performance drop from
text-reasoning-heavy question performance. Lastly, our qualitative error
analysis reveals specific categories of visual reasoning that are challenging
for current LVLMs.

</details>

### [524] [CIE: Controlling Language Model Text Generations Using Continuous Signals](https://arxiv.org/abs/2505.13448)
*Vinay Samuel,Harshita Diddee,Yiming Zhang,Daphne Ippolito*

Main category: cs.CL

TLDR: 论文提出了一种通过连续信号控制语言模型生成的方法，优于现有的离散信号或提示方法。


<details>
  <summary>Details</summary>
Motivation: 增强语言模型与用户意图的对齐，提供更灵活的控制方式。

Method: 通过微调语言模型，使用连续向量信号（介于高低嵌入之间）控制生成行为。

Result: 方法在控制生成响应长度上比上下文学习或离散信号方法更可靠。

Conclusion: 连续信号控制是一种有效的语言模型控制方法，代码和数据集已开源。

Abstract: Aligning language models with user intent is becoming increasingly relevant
to enhance user experience. This calls for designing methods that can allow
users to control the properties of the language that LMs generate. For example,
controlling the length of the generation, the complexity of the language that
gets chosen, the sentiment, tone, etc. Most existing work attempts to integrate
users' control by conditioning LM generations on natural language prompts or
discrete control signals, which are often brittle and hard to scale. In this
work, we are interested in \textit{continuous} control signals, ones that exist
along a spectrum that can't easily be captured in a natural language prompt or
via existing techniques in conditional generation. Through a case study in
controlling the precise response-length of generations produced by LMs, we
demonstrate how after fine-tuning, behaviors of language models can be
controlled via continuous signals -- as vectors that are interpolated between a
"low" and a "high" token embedding. Our method more reliably exerts
response-length control than in-context learning methods or fine-tuning methods
that represent the control signal as a discrete signal. Our full open-sourced
code and datasets are available at https://github.com/vsamuel2003/CIE.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [525] [LLM Agents Are Hypersensitive to Nudges](https://arxiv.org/abs/2505.11584)
*Manuel Cherep,Pattie Maes,Nikhil Singh*

Main category: cs.AI

TLDR: 研究表明，LLMs在复杂决策环境中对选择架构（如默认选项、提示等）的敏感性高于人类，且其信息获取策略和表现与人类存在差异，提示策略可部分改善对齐性，但无法完全消除敏感性。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在复杂决策环境中的行为特性及其对选择架构的敏感性，以评估其作为代理或助手的适用性。

Method: 通过多属性表格决策问题，测试LLMs在默认选项、提示和信息高亮等架构下的表现，并尝试零-shot CoT和少-shot提示策略。

Result: LLMs对选择架构的敏感性显著高于人类，信息获取策略和表现与人类不同，提示策略可部分改善对齐性但无法消除敏感性。

Conclusion: 在复杂环境中部署LLMs前需进行行为测试，以确保其作为代理或助手的可靠性。

Abstract: LLMs are being set loose in complex, real-world environments involving
sequential decision-making and tool use. Often, this involves making choices on
behalf of human users. However, not much is known about the distribution of
such choices, and how susceptible they are to different choice architectures.
We perform a case study with a few such LLM models on a multi-attribute tabular
decision-making problem, under canonical nudges such as the default option,
suggestions, and information highlighting, as well as additional prompting
strategies. We show that, despite superficial similarities to human choice
distributions, such models differ in subtle but important ways. First, they
show much higher susceptibility to the nudges. Second, they diverge in points
earned, being affected by factors like the idiosyncrasy of available prizes.
Third, they diverge in information acquisition strategies: e.g. incurring
substantial cost to reveal too much information, or selecting without revealing
any. Moreover, we show that simple prompt strategies like zero-shot chain of
thought (CoT) can shift the choice distribution, and few-shot prompting with
human data can induce greater alignment. Yet, none of these methods resolve the
sensitivity of these models to nudges. Finally, we show how optimal nudges
optimized with a human resource-rational model can similarly increase LLM
performance for some models. All these findings suggest that behavioral tests
are needed before deploying models as agents or assistants acting on behalf of
users in complex environments.

</details>

### [526] [Foundation Models for AI-Enabled Biological Design](https://arxiv.org/abs/2505.11610)
*Asher Moldwin,Amarda Shehu*

Main category: cs.AI

TLDR: 本文综述了AI驱动的生物设计中的基础模型，重点讨论了大规模自监督模型在蛋白质工程、小分子设计和基因组序列设计等任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 探讨如何将基础模型应用于生物设计领域，解决该领域快速发展的需求。

Method: 提出并讨论当前模型和方法的分类法，重点关注生物序列建模架构、生成可控性和多模态集成等挑战与解决方案。

Result: 总结了当前模型的进展，并指出了生物序列生成中的关键问题和解决方案。

Conclusion: 讨论了开放问题和未来方向，提出了改进生物序列生成质量的具体步骤。

Abstract: This paper surveys foundation models for AI-enabled biological design,
focusing on recent developments in applying large-scale, self-supervised models
to tasks such as protein engineering, small molecule design, and genomic
sequence design. Though this domain is evolving rapidly, this survey presents
and discusses a taxonomy of current models and methods. The focus is on
challenges and solutions in adapting these models for biological applications,
including biological sequence modeling architectures, controllability in
generation, and multi-modal integration. The survey concludes with a discussion
of open problems and future directions, offering concrete next-steps to improve
the quality of biological sequence generation.

</details>

### [527] [Probing the Vulnerability of Large Language Models to Polysemantic Interventions](https://arxiv.org/abs/2505.11611)
*Bofan Gong,Shiyang Lai,Dawn Song*

Main category: cs.AI

TLDR: 论文研究了神经网络中的多义性问题，利用稀疏自编码器分析小模型的多义结构，并评估其在提示、特征、标记和神经元层面的脆弱性。研究发现这种结构可推广到更大的黑盒模型。


<details>
  <summary>Details</summary>
Motivation: 多义性是神经网络中普遍存在的现象，但其对模型安全性的影响尚不明确，研究旨在填补这一空白。

Method: 使用稀疏自编码器分析Pythia-70M和GPT-2-Small的多义结构，并在不同层面评估其脆弱性。

Result: 发现两种模型共享一致的多义拓扑结构，并成功在更大的黑盒模型（LLaMA3.1-8B-Instruct和Gemma-2-9B-Instruct）上实施干预。

Conclusion: 多义结构具有稳定性和可迁移性，可能在多种架构和训练方式中普遍存在。

Abstract: Polysemanticity -- where individual neurons encode multiple unrelated
features -- is a well-known characteristic of large neural networks and remains
a central challenge in the interpretability of language models. At the same
time, its implications for model safety are also poorly understood. Leveraging
recent advances in sparse autoencoders, we investigate the polysemantic
structure of two small models (Pythia-70M and GPT-2-Small) and evaluate their
vulnerability to targeted, covert interventions at the prompt, feature, token,
and neuron levels. Our analysis reveals a consistent polysemantic topology
shared across both models. Strikingly, we demonstrate that this structure can
be exploited to mount effective interventions on two larger, black-box
instruction-tuned models (LLaMA3.1-8B-Instruct and Gemma-2-9B-Instruct). These
findings suggest not only the generalizability of the interventions but also
point to a stable and transferable polysemantic structure that could
potentially persist across architectures and training regimes.

</details>

### [528] [Heart2Mind: Human-Centered Contestable Psychiatric Disorder Diagnosis System using Wearable ECG Monitors](https://arxiv.org/abs/2505.11612)
*Hung Nguyen,Alireza Rahimi,Veronica Whitford,Hélène Fournier,Irina Kondratova,René Richard,Hung Cao*

Main category: cs.AI

TLDR: Heart2Mind是一个基于可穿戴心电图（ECG）监测器的精神障碍诊断系统，通过心脏生物标志物（如HRV和RRI）实现客观诊断，结合多尺度时频变换器和可争议诊断界面，准确率达91.7%。


<details>
  <summary>Details</summary>
Motivation: 精神障碍诊断面临主观评估和可及性问题，导致治疗延迟，需要客观、透明的诊断工具。

Method: 系统包括三个组件：1）实时数据采集接口；2）多尺度时频变换器处理RRI数据；3）结合自对抗解释和可争议大语言模型的诊断界面。

Result: MSTFT在HRV-ACC数据集上达到91.7%准确率，优于现有方法；SAEs和LLMs成功检测不一致预测并支持临床验证。

Conclusion: 结合可穿戴技术和可解释AI，Heart2Mind为精神障碍诊断提供了透明、可争议的系统，同时保持临床监督。

Abstract: Psychiatric disorders affect millions globally, yet their diagnosis faces
significant challenges in clinical practice due to subjective assessments and
accessibility concerns, leading to potential delays in treatment. To help
address this issue, we present Heart2Mind, a human-centered contestable
psychiatric disorder diagnosis system using wearable electrocardiogram (ECG)
monitors. Our approach leverages cardiac biomarkers, particularly heart rate
variability (HRV) and R-R intervals (RRI) time series, as objective indicators
of autonomic dysfunction in psychiatric conditions. The system comprises three
key components: (1) a Cardiac Monitoring Interface (CMI) for real-time data
acquisition from Polar H9/H10 devices; (2) a Multi-Scale Temporal-Frequency
Transformer (MSTFT) that processes RRI time series through integrated
time-frequency domain analysis; (3) a Contestable Diagnosis Interface (CDI)
combining Self-Adversarial Explanations (SAEs) with contestable Large Language
Models (LLMs). Our MSTFT achieves 91.7% accuracy on the HRV-ACC dataset using
leave-one-out cross-validation, outperforming state-of-the-art methods. SAEs
successfully detect inconsistencies in model predictions by comparing
attention-based and gradient-based explanations, while LLMs enable clinicians
to validate correct predictions and contest erroneous ones. This work
demonstrates the feasibility of combining wearable technology with Explainable
Artificial Intelligence (XAI) and contestable LLMs to create a transparent,
contestable system for psychiatric diagnosis that maintains clinical oversight
while leveraging advanced AI capabilities. Our implementation is publicly
available at: https://github.com/Analytics-Everywhere-Lab/heart2mind.

</details>

### [529] [Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions](https://arxiv.org/abs/2505.11614)
*Jian-Qiao Zhu,Hanbo Xie,Dilip Arumugam,Robert C. Wilson,Thomas L. Griffiths*

Main category: cs.AI

TLDR: 论文探讨了利用预训练大语言模型（LLMs）作为兼具预测和解释能力的认知模型，通过强化学习生成可解释的推理轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络模型在预测人类行为方面表现优异，但缺乏对认知机制的可解释性。

Method: 采用基于结果的强化学习，引导LLMs生成解释人类风险选择的显式推理轨迹。

Result: 该方法在预测人类决策的同时，生成了高质量的解释。

Conclusion: 预训练LLMs结合强化学习，可同时实现准确预测和可解释的认知建模。

Abstract: A central goal of cognitive modeling is to develop models that not only
predict human behavior but also provide insight into the underlying cognitive
mechanisms. While neural network models trained on large-scale behavioral data
often achieve strong predictive performance, they typically fall short in
offering interpretable explanations of the cognitive processes they capture. In
this work, we explore the potential of pretrained large language models (LLMs)
to serve as dual-purpose cognitive models--capable of both accurate prediction
and interpretable explanation in natural language. Specifically, we employ
reinforcement learning with outcome-based rewards to guide LLMs toward
generating explicit reasoning traces for explaining human risky choices. Our
findings demonstrate that this approach produces high-quality explanations
alongside strong quantitative predictions of human decisions.

</details>

### [530] [Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges](https://arxiv.org/abs/2505.11618)
*Pengrui Quan,Brian Wang,Kang Yang,Liying Han,Mani Srivastava*

Main category: cs.AI

TLDR: 论文提出了一个名为STARK的分层时空推理基准，用于评估大语言模型（LLMs）和大推理模型（LRMs）在复杂时空信号推理中的表现，发现LRMs在几何推理任务中表现稳健，而LLMs在需要世界知识的任务中表现接近LRMs。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs和LRMs在复杂时空信号推理中的能力，填补现有研究的空白。

Method: 构建了一个包含26个任务、14,552个挑战的分层时空推理基准STARK，评估了3种LRMs和8种LLMs的表现。

Result: LLMs在几何推理任务中表现有限，而LRMs表现稳健；在需要世界知识的任务中，LLMs与LRMs的差距缩小。

Conclusion: STARK为智能CPS的模型架构和推理范式创新提供了结构化框架，揭示了LLMs和LRMs在时空推理中的局限性。

Abstract: Spatiotemporal reasoning plays a key role in Cyber-Physical Systems (CPS).
Despite advances in Large Language Models (LLMs) and Large Reasoning Models
(LRMs), their capacity to reason about complex spatiotemporal signals remains
underexplored. This paper proposes a hierarchical SpatioTemporal reAsoning
benchmaRK, STARK, to systematically evaluate LLMs across three levels of
reasoning complexity: state estimation (e.g., predicting field variables,
localizing and tracking events in space and time), spatiotemporal reasoning
over states (e.g., inferring spatial-temporal relationships), and
world-knowledge-aware reasoning that integrates contextual and domain knowledge
(e.g., intent prediction, landmark-aware navigation). We curate 26 distinct
spatiotemporal tasks with diverse sensor modalities, comprising 14,552
challenges where models answer directly or by Python Code Interpreter.
Evaluating 3 LRMs and 8 LLMs, we find LLMs achieve limited success in tasks
requiring geometric reasoning (e.g., multilateration or triangulation),
particularly as complexity increases. Surprisingly, LRMs show robust
performance across tasks with various levels of difficulty, often competing or
surpassing traditional first-principle-based methods. Our results show that in
reasoning tasks requiring world knowledge, the performance gap between LLMs and
LRMs narrows, with some LLMs even surpassing LRMs. However, the LRM o3 model
continues to achieve leading performance across all evaluated tasks, a result
attributed primarily to the larger size of the reasoning models. STARK
motivates future innovations in model architectures and reasoning paradigms for
intelligent CPS by providing a structured framework to identify limitations in
the spatiotemporal reasoning of LLMs and LRMs.

</details>

### [531] [FLOW-BENCH: Towards Conversational Generation of Enterprise Workflows](https://arxiv.org/abs/2505.11646)
*Evelyn Duesterwald,Siyu Huo,Vatche Isahagian,K. R. Jayaram,Ritesh Kumar,Vinod Muthusamy,Punleuk Oum,Debashish Saha,Gegi Thomas,Praveen Venkateswaran*

Main category: cs.AI

TLDR: 论文提出了FLOW-BENCH数据集和FLOW-GEN方法，用于评估和实现基于自然语言的业务流程自动化。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用大型语言模型（LLMs）将自然语言指令转化为结构化业务流程，以推动业务流程自动化（BPA）的发展。

Method: 提出FLOW-BENCH数据集和FLOW-GEN方法，后者利用LLMs将自然语言转换为中间表示（Python语法），再转化为BPMN和DMN等业务流程定义语言。

Result: 通过FLOW-BENCH评估了八种不同规模的LLMs，验证了FLOW-GEN的有效性。

Conclusion: FLOW-GEN和FLOW-BENCH有望推动BPA研究，使其对新手和专家用户更易用。

Abstract: Business process automation (BPA) that leverages Large Language Models (LLMs)
to convert natural language (NL) instructions into structured business process
artifacts is becoming a hot research topic. This paper makes two technical
contributions -- (i) FLOW-BENCH, a high quality dataset of paired natural
language instructions and structured business process definitions to evaluate
NL-based BPA tools, and support bourgeoning research in this area, and (ii)
FLOW-GEN, our approach to utilize LLMs to translate natural language into an
intermediate representation with Python syntax that facilitates final
conversion into widely adopted business process definition languages, such as
BPMN and DMN. We bootstrap FLOW-BENCH by demonstrating how it can be used to
evaluate the components of FLOW-GEN across eight LLMs of varying sizes. We hope
that FLOW-GEN and FLOW-BENCH catalyze further research in BPA making it more
accessible to novice and expert users.

</details>

### [532] [Learning from Less: Guiding Deep Reinforcement Learning with Differentiable Symbolic Planning](https://arxiv.org/abs/2505.11661)
*Zihan Ye,Oleg Arenz,Kristian Kersting*

Main category: cs.AI

TLDR: 论文提出了一种名为Dylan的可微分符号规划框架，将符号规划融入强化学习，以赋予RL代理类似人类的先验知识，从而减少训练交互次数。


<details>
  <summary>Details</summary>
Motivation: 人类能够通过分解任务和动态调整计划高效解决问题，而现有强化学习代理缺乏这种能力，需要大量训练才能达到类似效果。研究旨在为RL代理引入类似人类的先验知识，提升学习效率。

Method: 提出Dylan框架，结合符号规划和强化学习，动态调整奖励函数并指导代理完成子任务，同时避免符号规划的常见问题（如无限循环）。

Result: 实验表明，Dylan显著提升了RL代理的性能，并增强了其在未见任务上的泛化能力。

Conclusion: Dylan通过引入符号规划，有效提升了强化学习代理的效率和适应性，为未来研究提供了新方向。

Abstract: When tackling complex problems, humans naturally break them down into
smaller, manageable subtasks and adjust their initial plans based on
observations. For instance, if you want to make coffee at a friend's place, you
might initially plan to grab coffee beans, go to the coffee machine, and pour
them into the machine. Upon noticing that the machine is full, you would skip
the initial steps and proceed directly to brewing. In stark contrast, state of
the art reinforcement learners, such as Proximal Policy Optimization (PPO),
lack such prior knowledge and therefore require significantly more training
steps to exhibit comparable adaptive behavior. Thus, a central research
question arises: \textit{How can we enable reinforcement learning (RL) agents
to have similar ``human priors'', allowing the agent to learn with fewer
training interactions?} To address this challenge, we propose differentiable
symbolic planner (Dylan), a novel framework that integrates symbolic planning
into Reinforcement Learning. Dylan serves as a reward model that dynamically
shapes rewards by leveraging human priors, guiding agents through intermediate
subtasks, thus enabling more efficient exploration. Beyond reward shaping,
Dylan can work as a high level planner that composes primitive policies to
generate new behaviors while avoiding common symbolic planner pitfalls such as
infinite execution loops. Our experimental evaluations demonstrate that Dylan
significantly improves RL agents' performance and facilitates generalization to
unseen tasks.

</details>

### [533] [Conditional Deep Generative Models for Belief State Planning](https://arxiv.org/abs/2505.11698)
*Antoine Bigeard,Anthony Corso,Mykel Kochenderfer*

Main category: cs.AI

TLDR: 提出了一种基于条件深度生成模型（cDGMs）的新方法，用于解决高维状态POMDPs中的信念表示问题，并在矿物勘探任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 高维状态POMDPs中信念的准确表示是一个难题，传统方法难以应对。

Method: 使用cDGMs表示信念，通过随机轨迹数据训练模型，生成后验信念样本。

Result: cDGMs在信念准确性和规划性能上均优于粒子滤波基线。

Conclusion: cDGMs为高维状态POMDPs提供了一种有效的信念表示方法。

Abstract: Partially observable Markov decision processes (POMDPs) are used to model a
wide range of applications, including robotics, autonomous vehicles, and
subsurface problems. However, accurately representing the belief is difficult
for POMDPs with high-dimensional states. In this paper, we propose a novel
approach that uses conditional deep generative models (cDGMs) to represent the
belief. Unlike traditional belief representations, cDGMs are well-suited for
high-dimensional states and large numbers of observations, and they can
generate an arbitrary number of samples from the posterior belief. We train the
cDGMs on data produced by random rollout trajectories and show their
effectiveness in solving a mineral exploration POMDP with a large and
continuous state space. The cDGMs outperform particle filter baselines in both
task-agnostic measures of belief accuracy as well as in planning performance.

</details>

### [534] [DMN-Guided Prompting: A Low-Code Framework for Controlling LLM Behavior](https://arxiv.org/abs/2505.11701)
*Shaghayegh Abedi,Amin Jalali*

Main category: cs.AI

TLDR: 论文提出了一种基于DMN的提示框架，用于优化LLM在复杂决策逻辑中的表现，并在教学场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: LLMs在知识密集型流程中的决策逻辑自动化潜力大，但提示策略和质量是关键，且现有方法难以让终端用户修改逻辑。

Method: 引入DMN引导的提示框架，将复杂决策逻辑分解为可管理的小组件，并通过结构化路径引导LLMs。

Result: 框架在教学场景中表现优于CoT提示，学生反馈积极，认可其实用性。

Conclusion: DMN引导的提示框架能有效提升LLMs在复杂决策中的表现，且用户接受度高。

Abstract: Large Language Models (LLMs) have shown considerable potential in automating
decision logic within knowledge-intensive processes. However, their
effectiveness largely depends on the strategy and quality of prompting. Since
decision logic is typically embedded in prompts, it becomes challenging for end
users to modify or refine it. Decision Model and Notation (DMN) offers a
standardized graphical approach for defining decision logic in a structured,
user-friendly manner. This paper introduces a DMN-guided prompting framework
that breaks down complex decision logic into smaller, manageable components,
guiding LLMs through structured decision pathways. We implemented the framework
in a graduate-level course where students submitted assignments. The
assignments and DMN models representing feedback instructions served as inputs
to our framework. The instructor evaluated the generated feedback and labeled
it for performance assessment. Our approach demonstrated promising results,
outperforming chain-of-thought (CoT) prompting. Students also responded
positively to the generated feedback, reporting high levels of perceived
usefulness in a survey based on the Technology Acceptance Model.

</details>

### [535] [REMOR: Automated Peer Review Generation with LLM Reasoning and Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2505.11718)
*Pawin Taechoyotin,Daniel Acuna*

Main category: cs.AI

TLDR: 论文研究了基于推理的大语言模型（REMOR）如何改进AI同行评审系统，通过多目标强化学习训练，生成更高质量的评审反馈。


<details>
  <summary>Details</summary>
Motivation: 现有AI同行评审系统生成的反馈往往浅显且过度赞扬，无法与人类评审相媲美。

Method: 设计了多方面的奖励函数，结合监督微调和GRPO训练REMOR-H和REMOR-U模型。

Result: REMOR模型在奖励分数上超过人类评审和其他AI系统，且避免了低质量评审的长尾问题。

Conclusion: 推理能力是提升评审质量的关键，并公开了相关数据集和模型以推动领域发展。

Abstract: AI-based peer review systems tend to produce shallow and overpraising
suggestions compared to human feedback. Here, we evaluate how well a reasoning
LLM trained with multi-objective reinforcement learning (REMOR) can overcome
these limitations. We start by designing a multi-aspect reward function that
aligns with human evaluation of reviews. The aspects are related to the review
itself (e.g., criticisms, novelty) and the relationship between the review and
the manuscript (i.e., relevance). First, we perform supervised fine-tuning of
DeepSeek-R1-Distill-Qwen-7B using LoRA on PeerRT, a new dataset of high-quality
top AI conference reviews enriched with reasoning traces. We then apply Group
Relative Policy Optimization (GRPO) to train two models: REMOR-H (with the
human-aligned reward) and REMOR-U (with a uniform reward). Interestingly, the
human-aligned reward penalizes aspects typically associated with strong
reviews, leading REMOR-U to produce qualitatively more substantive feedback.
Our results show that REMOR-U and REMOR-H achieve more than twice the average
rewards of human reviews, non-reasoning state-of-the-art agentic multi-modal AI
review systems, and general commercial LLM baselines. We found that while the
best AI and human reviews are comparable in quality, REMOR avoids the long tail
of low-quality human reviews. We discuss how reasoning is key to achieving
these improvements and release the Human-aligned Peer Review Reward (HPRR)
function, the Peer Review Reasoning-enriched Traces (PeerRT) dataset, and the
REMOR models, which we believe can help spur progress in the area.

</details>

### [536] [Rethinking Optimal Verification Granularity for Compute-Efficient Test-Time Scaling](https://arxiv.org/abs/2505.11730)
*Hao Mark Chen,Guanxi Lu,Yasuyuki Okoshi,Zhiwen Mo,Masato Motomura,Hongxiang Fan*

Main category: cs.AI

TLDR: 论文提出了一种名为VG-Search的算法，通过动态调整验证粒度（g）来优化大语言模型的推理性能和计算效率。实验表明，该方法在计算效率和准确性上均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 验证在大语言模型的测试时扩展（TTS）中至关重要，但传统验证范式（如仅验证最终输出或单步生成）存在局限性。本文旨在研究验证粒度对性能和效率的影响。

Method: 引入VG-Search算法，通过可调参数g统一了束搜索和Best-of-N采样，动态选择验证粒度以优化性能。

Result: 实验显示，动态选择g可提升计算效率和扩展行为，适应性策略在准确性上比束搜索和Best-of-N分别提高3.1%和3.6%，同时减少52%以上的FLOPs。

Conclusion: VG-Search通过动态验证粒度显著提升了推理性能和计算效率，为未来研究提供了开源支持。

Abstract: Test-time scaling (TTS) has proven effective in enhancing the reasoning
capabilities of large language models (LLMs). Verification plays a key role in
TTS, simultaneously influencing (1) reasoning performance and (2) compute
efficiency, due to the quality and computational cost of verification. In this
work, we challenge the conventional paradigms of verification, and make the
first attempt toward systematically investigating the impact of verification
granularity-that is, how frequently the verifier is invoked during generation,
beyond verifying only the final output or individual generation steps. To this
end, we introduce Variable Granularity Search (VG-Search), a unified algorithm
that generalizes beam search and Best-of-N sampling via a tunable granularity
parameter g. Extensive experiments with VG-Search under varying compute
budgets, generator-verifier configurations, and task attributes reveal that
dynamically selecting g can improve the compute efficiency and scaling
behavior. Building on these findings, we propose adaptive VG-Search strategies
that achieve accuracy gains of up to 3.1\% over Beam Search and 3.6\% over
Best-of-N, while reducing FLOPs by over 52\%. We will open-source the code to
support future research.

</details>

### [537] [Automated Real-time Assessment of Intracranial Hemorrhage Detection AI Using an Ensembled Monitoring Model (EMM)](https://arxiv.org/abs/2505.11738)
*Zhongnan Fang,Andrew Johnston,Lina Cheuy,Hye Sun Na,Magdalini Paschali,Camila Gonzalez,Bonnie A. Armstrong,Arogya Koirala,Derrick Laurel,Andrew Walker Campion,Michael Iv,Akshay S. Chaudhari,David B. Larson*

Main category: cs.AI

TLDR: 论文提出了一种名为Ensembled Monitoring Model (EMM)的框架，用于实时监控商业AI工具在放射学中的预测置信度，以减少认知负担并提高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 当前AI工具在部署后缺乏实时监控，用户需自行判断预测的可信度，增加了认知负担和误诊风险。

Method: EMM框架受临床共识实践启发，独立于商业AI产品运行，无需访问其内部组件或中间输出，通过多专家评审方式提供置信度评估。

Result: 在2919项颅内出血检测研究中，EMM成功分类了AI预测的置信度，并提出了改进建议，提升了AI工具的整体性能。

Conclusion: EMM为临床环境中监控AI工具提供了技术考虑和最佳实践，有望减少认知负担并提高诊断效率。

Abstract: Artificial intelligence (AI) tools for radiology are commonly unmonitored
once deployed. The lack of real-time case-by-case assessments of AI prediction
confidence requires users to independently distinguish between trustworthy and
unreliable AI predictions, which increases cognitive burden, reduces
productivity, and potentially leads to misdiagnoses. To address these
challenges, we introduce Ensembled Monitoring Model (EMM), a framework inspired
by clinical consensus practices using multiple expert reviews. Designed
specifically for black-box commercial AI products, EMM operates independently
without requiring access to internal AI components or intermediate outputs,
while still providing robust confidence measurements. Using intracranial
hemorrhage detection as our test case on a large, diverse dataset of 2919
studies, we demonstrate that EMM successfully categorizes confidence in the
AI-generated prediction, suggesting different actions and helping improve the
overall performance of AI tools to ultimately reduce cognitive burden.
Importantly, we provide key technical considerations and best practices for
successfully translating EMM into clinical settings.

</details>

### [538] [Diverging Towards Hallucination: Detection of Failures in Vision-Language Models via Multi-token Aggregation](https://arxiv.org/abs/2505.11741)
*Geigh Zollicoffer,Minh Vu,Manish Bhattarai*

Main category: cs.AI

TLDR: 论文提出了一种名为MTRE的多令牌可靠性估计方法，通过分析早期令牌的完整序列来改进视觉语言模型的幻觉检测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）在幻觉检测上仅关注单个令牌或最高分组件，忽略了早期令牌分布中的丰富信号，导致检测不准确。

Method: 提出MTRE方法，通过聚合前十个令牌的对数似然比和自注意力机制，分析KL散度来捕捉幻觉与非幻觉令牌的差异。

Result: 在多个基准测试中，MTRE的AUROC比SLP和P(True)分别提高了9.4和12.1个百分点，达到开源VLM幻觉检测的最新水平。

Conclusion: MTRE通过多令牌分析显著提升了幻觉检测的准确性，为VLM的可靠性评估提供了高效且轻量级的解决方案。

Abstract: Vision-language models (VLMs) now rival human performance on many multimodal
tasks, yet they still hallucinate objects or generate unsafe text. Current
hallucination detectors, e.g., single-token linear probing (SLP) and P(True),
typically analyze only the logit of the first generated token or just its
highest scoring component overlooking richer signals embedded within earlier
token distributions. We demonstrate that analyzing the complete sequence of
early logits potentially provides substantially more diagnostic information. We
emphasize that hallucinations may only emerge after several tokens, as subtle
inconsistencies accumulate over time. By analyzing the Kullback-Leibler (KL)
divergence between logits corresponding to hallucinated and non-hallucinated
tokens, we underscore the importance of incorporating later-token logits to
more accurately capture the reliability dynamics of VLMs. In response, we
introduce Multi-Token Reliability Estimation (MTRE), a lightweight, white-box
method that aggregates logits from the first ten tokens using multi-token
log-likelihood ratios and self-attention. Despite the challenges posed by large
vocabulary sizes and long logit sequences, MTRE remains efficient and
tractable. On MAD-Bench, MM-SafetyBench, MathVista, and four
compositional-geometry benchmarks, MTRE improves AUROC by 9.4 +/- 1.3 points
over SLP and by 12.1 +/- 1.7 points over P(True), setting a new
state-of-the-art in hallucination detection for open-source VLMs.

</details>

### [539] [A Review and Analysis of a Parallel Approach for Decision Tree Learning from Large Data Streams](https://arxiv.org/abs/2505.11780)
*Zeinab Shiralizadeh*

Main category: cs.AI

TLDR: pdsCART是一种并行决策树学习算法，支持实时数据流学习、并行处理和大规模分布式计算。


<details>
  <summary>Details</summary>
Motivation: 研究一种可扩展且高效的数据分析方法，适用于大规模数据流和分布式环境。

Method: 结合实时增量学习、并行处理能力，并集成到MapReduce框架中。

Result: 展示了算法的性能和可扩展性。

Conclusion: pdsCART是一种适用于大规模数据流和分布式计算的高效并行决策树算法。

Abstract: This work studies one of the parallel decision tree learning algorithms,
pdsCART, designed for scalable and efficient data analysis. The method
incorporates three core capabilities. First, it supports real-time learning
from data streams, allowing trees to be constructed incrementally. Second, it
enables parallel processing of high-volume streaming data, making it
well-suited for large-scale applications. Third, the algorithm integrates
seamlessly into the MapReduce framework, ensuring compatibility with
distributed computing environments. In what follows, we present the algorithm's
key components along with results highlighting its performance and scalability.

</details>

### [540] [Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling](https://arxiv.org/abs/2505.11792)
*Yitian Chen,Jingfan Xia,Siyu Shao,Dongdong Ge,Yinyu Ye*

Main category: cs.AI

TLDR: 论文提出了一种名为SIRL的新框架，利用优化求解器作为可验证的奖励机制，显著提升LLMs在优化建模中的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在从自然语言描述自动生成优化模型方面取得进展，但其常因幻觉问题生成不准确或不可用的模型，亟需可靠自动化方法。

Method: SIRL框架结合外部优化求解器作为验证机制，提供语法、可行性和解质量的反馈，并通过实例增强的自一致性方法生成高质量训练数据。

Result: 在多个公开基准测试中，SIRL表现优异，显著优于现有方法，能生成更准确且可执行的优化模型。

Conclusion: SIRL通过结合优化求解器和强化学习，有效提升了LLMs在优化建模中的表现，为自动化决策提供了可靠工具。

Abstract: Optimization modeling is fundamental to decision-making across diverse
domains.Despite progress in automating optimization formulation from natural
language descriptions, Large Language Models (LLMs) often struggle to generate
formally correct and usable models due to hallucinations, posing a challenge
for reliable automation. Inspired by the success of Reinforcement Learning (RL)
in enhancing Large Reasoning Models, we present Solver-Informed Reinforcement
Learning (SIRL).This novel framework leverages external optimization solvers as
verifiable reward mechanisms to significantly improve the authenticity of LLMs
for optimization modeling.Acting as precise verifiers, these solvers
automatically assess the executable code and the instance-level mathematical
model represented by the associated LP file, yielding precise and comprehensive
feedback signals -- including syntax, feasibility, and solution quality that
directly inform the RL process. This automated verification process, powered by
classic optimization solvers, also underpins our instance-enhanced
self-consistency method to synthesize high-quality training data. Extensive
experiments on diverse public benchmarks demonstrate that SIRL achieves
state-of-the-art performance, substantially outperforming existing methods in
generating accurate and executable optimization models.

</details>

### [541] [VITA: Versatile Time Representation Learning for Temporal Hyper-Relational Knowledge Graphs](https://arxiv.org/abs/2505.11803)
*ChongIn Un,Yuhuan Lu,Tianyue Yang,Dingqi Yang*

Main category: cs.AI

TLDR: VITA是一种用于时间超关系知识图谱的灵活时间表示学习方法，能有效处理四种时间有效性事实，显著提升链接预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理时间知识图谱时对时间间隔/粒度的选择敏感，且难以处理长期或无限有效的事实。

Method: 提出灵活的时间表示方法，涵盖四种时间有效性类型，并设计VITA模型学习时间值和时间跨度信息。

Result: VITA在多种链接预测任务中表现最佳，性能提升高达75.3%。

Conclusion: VITA通过灵活的时间表示和有效学习机制，显著优于现有方法。

Abstract: Knowledge graphs (KGs) have become an effective paradigm for managing
real-world facts, which are not only complex but also dynamically evolve over
time. The temporal validity of facts often serves as a strong clue in
downstream link prediction tasks, which predicts a missing element in a fact.
Traditional link prediction techniques on temporal KGs either consider a
sequence of temporal snapshots of KGs with an ad-hoc defined time interval or
expand a temporal fact over its validity period under a predefined time
granularity; these approaches not only suffer from the sensitivity of the
selection of time interval/granularity, but also face the computational
challenges when handling facts with long (even infinite) validity. Although the
recent hyper-relational KGs represent the temporal validity of a fact as
qualifiers describing the fact, it is still suboptimal due to its ignorance of
the infinite validity of some facts and the insufficient information encoded
from the qualifiers about the temporal validity. Against this background, we
propose VITA, a $\underline{V}$ersatile t$\underline{I}$me
represen$\underline{TA}$tion learning method for temporal hyper-relational
knowledge graphs. We first propose a versatile time representation that can
flexibly accommodate all four types of temporal validity of facts (i.e., since,
until, period, time-invariant), and then design VITA to effectively learn the
time information in both aspects of time value and timespan to boost the link
prediction performance. We conduct a thorough evaluation of VITA compared to a
sizable collection of baselines on real-world KG datasets. Results show that
VITA outperforms the best-performing baselines in various link prediction tasks
(predicting missing entities, relations, time, and other numeric literals) by
up to 75.3%. Ablation studies and a case study also support our key design
choices.

</details>

### [542] [ChatHTN: Interleaving Approximate (LLM) and Symbolic HTN Planning](https://arxiv.org/abs/2505.11814)
*Hector Munoz-Avila,David W. Aha,Paola Rizzo*

Main category: cs.AI

TLDR: ChatHTN结合符号化HTN规划与ChatGPT查询，生成任务分解的近似解，其生成的计划能正确完成任务。


<details>
  <summary>Details</summary>
Motivation: 通过结合符号化HTN规划与ChatGPT的能力，提升任务分解的灵活性和效率。

Method: 使用符号化HTN规划与ChatGPT查询交替生成任务分解层次结构。

Result: ChatHTN生成的计划能正确完成任务，且系统已开源实现。

Conclusion: ChatHTN是一种可靠的HTN规划器，结合了符号化方法与ChatGPT的灵活性。

Abstract: We introduce ChatHTN, a Hierarchical Task Network (HTN) planner that combines
symbolic HTN planning techniques with queries to ChatGPT to approximate
solutions in the form of task decompositions. The resulting hierarchies
interleave task decompositions generated by symbolic HTN planning with those
generated by ChatGPT. Despite the approximate nature of the results generates
by ChatGPT, ChatHTN is provably sound; any plan it generates correctly achieves
the input tasks. We demonstrate this property with an open-source
implementation of our system.

</details>

### [543] [ARC-AGI-2: A New Challenge for Frontier AI Reasoning Systems](https://arxiv.org/abs/2505.11831)
*Francois Chollet,Mike Knoop,Gregory Kamradt,Bryan Landers,Henry Pinkard*

Main category: cs.AI

TLDR: ARC-AGI-2是ARC-AGI的升级版，旨在通过更细粒度的任务评估AI的抽象推理和问题解决能力，为更通用和类人AI能力提供基准。


<details>
  <summary>Details</summary>
Motivation: 由于AI的进步，需要更高认知复杂度的基准来评估流体智力。

Method: 保留输入-输出对任务格式，新增更细粒度的任务集，并结合人类测试结果。

Result: ARC-AGI-2对人类智能友好，但对当前AI系统具有挑战性。

Conclusion: ARC-AGI-2是下一代工具，用于严格衡量AI向更通用和类人能力的发展。

Abstract: The Abstraction and Reasoning Corpus for Artificial General Intelligence
(ARC-AGI), introduced in 2019, established a challenging benchmark for
evaluating the general fluid intelligence of artificial systems via a set of
unique, novel tasks only requiring minimal prior knowledge. While ARC-AGI has
spurred significant research activity over the past five years, recent AI
progress calls for benchmarks capable of finer-grained evaluation at higher
levels of cognitive complexity. We introduce ARC-AGI-2, an upgraded version of
the benchmark. ARC-AGI-2 preserves the input-output pair task format of its
predecessor, ensuring continuity for researchers. It incorporates a newly
curated and expanded set of tasks specifically designed to provide a more
granular signal to assess abstract reasoning and problem-solving abilities at
higher levels of fluid intelligence. To contextualize the difficulty and
characteristics of ARC-AGI-2, we present extensive results from human testing,
providing a robust baseline that highlights the benchmark's accessibility to
human intelligence, yet difficulty for current AI systems. ARC-AGI-2 aims to
serve as a next-generation tool for rigorously measuring progress towards more
general and human-like AI capabilities.

</details>

### [544] [ToLeaP: Rethinking Development of Tool Learning with Large Language Models](https://arxiv.org/abs/2505.11833)
*Haotian Chen,Zijun Song,Boye Niu,Ke Zhang,Litu Ou,Yaxi Lu,Zhong Zhang,Xin Cong,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TLDR: 论文研究了41种主流大语言模型（LLMs）的工具学习能力，通过复现33个基准测试并构建ToLeaP平台，发现四大关键挑战，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 工具学习（Tool Learning）在提升LLMs生产力方面潜力巨大，但关键挑战和机会尚未充分研究，限制了进一步的发展。

Method: 通过复现33个基准测试，构建ToLeaP平台，分析41种LLMs的3000多个失败案例。

Result: 发现四大挑战：基准限制、自主学习不足、泛化能力弱、长任务解决能力差。初步实验验证了提出的研究方向的有效性。

Conclusion: 未来需关注真实世界基准构建、兼容性自主学习、理性思考学习及关键线索识别与回忆，以推动工具学习的发展。

Abstract: Tool learning, which enables large language models (LLMs) to utilize external
tools effectively, has garnered increasing attention for its potential to
revolutionize productivity across industries. Despite rapid development in tool
learning, key challenges and opportunities remain understudied, limiting deeper
insights and future advancements. In this paper, we investigate the tool
learning ability of 41 prevalent LLMs by reproducing 33 benchmarks and enabling
one-click evaluation for seven of them, forming a Tool Learning Platform named
ToLeaP. We also collect 21 out of 33 potential training datasets to facilitate
future exploration. After analyzing over 3,000 bad cases of 41 LLMs based on
ToLeaP, we identify four main critical challenges: (1) benchmark limitations
induce both the neglect and lack of (2) autonomous learning, (3)
generalization, and (4) long-horizon task-solving capabilities of LLMs. To aid
future advancements, we take a step further toward exploring potential
directions, namely (1) real-world benchmark construction, (2)
compatibility-aware autonomous learning, (3) rationale learning by thinking,
and (4) identifying and recalling key clues. The preliminary experiments
demonstrate their effectiveness, highlighting the need for further research and
exploration.

</details>

### [545] [On the Eligibility of LLMs for Counterfactual Reasoning: A Decompositional Study](https://arxiv.org/abs/2505.11839)
*Shuai Yang,Qi Yang,Luoxi Tang,Jeremy Blackburn,Zhaohan Xi*

Main category: cs.AI

TLDR: 本文提出了一种分解策略，通过从因果构建到反事实干预的推理，分析大语言模型在反事实推理中的表现，并研究了11个数据集以评估模型行为。


<details>
  <summary>Details</summary>
Motivation: 反事实推理是评估大语言模型适应性和可靠性的关键，但现有研究未明确哪些因素显著影响其性能。

Method: 采用分解策略，将反事实生成从因果构建分解到干预推理，并在11个多任务数据集上进行评估。

Result: 通过评估，揭示了模型在不同分解阶段的行为，以及模态类型和中间推理对性能的影响。

Conclusion: 本研究为分析反事实推理提供了结构化框架，有助于开发更可靠的大语言模型推理系统。

Abstract: Counterfactual reasoning has emerged as a crucial technique for generalizing
the reasoning capabilities of large language models (LLMs). By generating and
analyzing counterfactual scenarios, researchers can assess the adaptability and
reliability of model decision-making. Although prior work has shown that LLMs
often struggle with counterfactual reasoning, it remains unclear which factors
most significantly impede their performance across different tasks and
modalities. In this paper, we propose a decompositional strategy that breaks
down the counterfactual generation from causality construction to the reasoning
over counterfactual interventions. To support decompositional analysis, we
investigate 11 datasets spanning diverse tasks, including natural language
understanding, mathematics, programming, and vision-language tasks. Through
extensive evaluations, we characterize LLM behavior across each decompositional
stage and identify how modality type and intermediate reasoning influence
performance. By establishing a structured framework for analyzing
counterfactual reasoning, this work contributes to the development of more
reliable LLM-based reasoning systems and informs future elicitation strategies.

</details>

### [546] [VeriReason: Reinforcement Learning with Testbench Feedback for Reasoning-Enhanced Verilog Generation](https://arxiv.org/abs/2505.11849)
*Yiting Wang,Guoheng Sun,Wanghao Ye,Gang Qu,Ang Li*

Main category: cs.AI

TLDR: VeriReason是一个结合监督微调和强化学习的框架，用于自动化生成RTL代码，显著提升了功能正确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的RTL代码生成方法存在训练数据稀缺、规范与代码对齐差、缺乏验证机制等问题，VeriReason旨在解决这些问题。

Method: VeriReason结合监督微调和GRPO强化学习，使用精选训练样本和反馈驱动的奖励模型，嵌入自检能力。

Result: 在VerilogEval基准测试中，VeriReason功能正确率达83.1%，优于同类模型和GPT-4 Turbo，首次尝试正确率提升2.8倍。

Conclusion: VeriReason首次将显式推理与强化学习结合用于Verilog生成，为自动化RTL合成设定了新标准。

Abstract: Automating Register Transfer Level (RTL) code generation using Large Language
Models (LLMs) offers substantial promise for streamlining digital circuit
design and reducing human effort. However, current LLM-based approaches face
significant challenges with training data scarcity, poor specification-code
alignment, lack of verification mechanisms, and balancing generalization with
specialization. Inspired by DeepSeek-R1, we introduce VeriReason, a framework
integrating supervised fine-tuning with Guided Reward Proximal Optimization
(GRPO) reinforcement learning for RTL generation. Using curated training
examples and a feedback-driven reward model, VeriReason combines testbench
evaluations with structural heuristics while embedding self-checking
capabilities for autonomous error correction. On the VerilogEval Benchmark,
VeriReason delivers significant improvements: achieving 83.1% functional
correctness on the VerilogEval Machine benchmark, substantially outperforming
both comparable-sized models and much larger commercial systems like GPT-4
Turbo. Additionally, our approach demonstrates up to a 2.8X increase in
first-attempt functional correctness compared to baseline methods and exhibits
robust generalization to unseen designs. To our knowledge, VeriReason
represents the first system to successfully integrate explicit reasoning
capabilities with reinforcement learning for Verilog generation, establishing a
new state-of-the-art for automated RTL synthesis. The models and datasets are
available at: https://huggingface.co/collections/AI4EDA-CASE Code is Available
at: https://github.com/NellyW8/VeriReason

</details>

### [547] [Evaluating the Logical Reasoning Abilities of Large Reasoning Models](https://arxiv.org/abs/2505.11854)
*Hanmeng Liu,Yiran Ding,Zhizhang Fu,Chaoli Zhang,Xiaozhang Liu,Yue Zhang*

Main category: cs.AI

TLDR: LogiEval是一个评估大型推理模型逻辑推理能力的综合基准，涵盖多种推理类型和任务格式，发现模型在某些任务上超越人类，但在泛化能力上存在局限。


<details>
  <summary>Details</summary>
Motivation: 研究大型推理模型的逻辑推理能力，填补现有研究的空白。

Method: 引入LogiEval基准，涵盖多种推理类型和任务格式，并通过实验评估模型表现。

Result: 模型在部分任务上超越人类，但泛化能力不均，LogiEval-Hard子集揭示了模型的持续瓶颈。

Conclusion: LogiEval-Hard可作为诊断工具和测试平台，推动LLM逻辑推理能力的进步。

Abstract: Large reasoning models, often post-trained on long chain-of-thought (long
CoT) data with reinforcement learning, achieve state-of-the-art performance on
mathematical, coding, and domain-specific reasoning benchmarks. However, their
logical reasoning capabilities - fundamental to human cognition and independent
of domain knowledge - remain understudied. To address this gap, we introduce
LogiEval, a holistic benchmark for evaluating logical reasoning in large
reasoning models. LogiEval spans diverse reasoning types (deductive, inductive,
analogical, and abductive) and task formats (e.g., logical sequence, argument
analysis), sourced from high-quality human examinations (e.g., LSAT, GMAT). Our
experiments demonstrate that modern reasoning models excel at 4-choice argument
analysis problems and analogical reasoning, surpassing human performance, yet
exhibit uneven capabilities across reasoning types and formats, highlighting
limitations in their generalization. Our analysis reveals that human
performance does not mirror model failure distributions. To foster further
research, we curate LogiEval-Hard, a challenging subset identified through a
novel screening paradigm where small-model failures (Qwen3-30B-A3B) reliably
predict difficulties for larger models. Modern models show striking, consistent
failures on LogiEval-Hard. This demonstrates that fundamental reasoning
bottlenecks persist across model scales, and establishes LogiEval-Hard as both
a diagnostic tool and a rigorous testbed for advancing logical reasoning in
LLMs.

</details>

### [548] [Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity](https://arxiv.org/abs/2505.11861)
*Qi Zhou,Jie Zhang,Dongxia Wang,Qiang Liu,Tianlin Li,Jin Song Dong,Wenhai Wang,Qing Guo*

Main category: cs.AI

TLDR: Fair-PP是一个基于真实社会调查数据的合成个性化偏好数据集，旨在解决现有数据集中个性化与偏好关联不足的问题，并通过自动化框架生成偏好数据。


<details>
  <summary>Details</summary>
Motivation: 现有的人类偏好数据集成本高且忽视个性化与偏好的关联，Fair-PP通过合成数据填补这一空白。

Method: 利用GPT-4o-mini进行角色扮演，生成个性化偏好数据，并提出自动化框架和样本重加权方法。

Result: 生成238,623条偏好记录，实验表明提出的方法优于基线。

Conclusion: Fair-PP为个性化偏好对齐提供了有效工具，并展示了主流LLM在不同区域的偏好定位。

Abstract: Human preference plays a crucial role in the refinement of large language
models (LLMs). However, collecting human preference feedback is costly and most
existing datasets neglect the correlation between personalization and
preferences. To address this issue, we introduce Fair-PP, a synthetic dataset
of personalized preferences targeting social equity, derived from real-world
social survey data, which includes 28 social groups, 98 equity topics, and 5
personal preference dimensions. Leveraging GPT-4o-mini, we engage in
role-playing based on seven representative persona portrayals guided by
existing social survey data, yielding a total of 238,623 preference records.
Through Fair-PP, we also contribute (i) An automated framework for generating
preference data, along with a more fine-grained dataset of personalized
preferences; (ii) analysis of the positioning of the existing mainstream LLMs
across five major global regions within the personalized preference space; and
(iii) a sample reweighting method for personalized preference alignment,
enabling alignment with a target persona while maximizing the divergence from
other personas. Empirical experiments show our method outperforms the
baselines.

</details>

### [549] [Position Paper: Bounded Alignment: What (Not) To Expect From AGI Agents](https://arxiv.org/abs/2505.11866)
*Ali A. Minai*

Main category: cs.AI

TLDR: 本文主张AI/ML社区对通用人工智能（AGI）的现有观点需更新，应更多借鉴人类和动物智能的安全期望与标准。


<details>
  <summary>Details</summary>
Motivation: 随着AGI前景逼近，AI风险和安全性问题日益突出，需重新审视AGI的愿景与安全标准。

Method: 通过分析现有AGI观点的局限性，提出借鉴人类和动物智能的视角。

Result: 改变视角将带来更现实的技术评估和更优的政策决策。

Conclusion: AGI研究需更多基于人类和动物智能的理解，以提升安全性和政策效果。

Abstract: The issues of AI risk and AI safety are becoming critical as the prospect of
artificial general intelligence (AGI) looms larger. The emergence of extremely
large and capable generative models has led to alarming predictions and created
a stir from boardrooms to legislatures. As a result, AI alignment has emerged
as one of the most important areas in AI research. The goal of this position
paper is to argue that the currently dominant vision of AGI in the AI and
machine learning (AI/ML) community needs to evolve, and that expectations and
metrics for its safety must be informed much more by our understanding of the
only existing instance of general intelligence, i.e., the intelligence found in
animals, and especially in humans. This change in perspective will lead to a
more realistic view of the technology, and allow for better policy decisions.

</details>

### [550] [From Recall to Reasoning: Automated Question Generation for Deeper Math Learning through Large Language Models](https://arxiv.org/abs/2505.11899)
*Yongan Yu,Alexandre Krantz,Nikki G. Lobczowski*

Main category: cs.AI

TLDR: 研究探讨了如何利用生成式AI（GenAI）优化高级数学课程内容创建，发现提供示例和相关内容可提升问题质量。


<details>
  <summary>Details</summary>
Motivation: 教育工作者开始使用GenAI创建课程内容，但缺乏相关指导，研究旨在填补这一空白。

Method: 通过两项研究：1）评估现有GenAI的能力；2）开发改进框架以解决局限性。

Result: GenAI能生成不同质量的数学问题，提供示例和内容支持可显著提升输出质量。

Conclusion: 研究为教育工作者提供了使用GenAI优化教学内容的实用建议。

Abstract: Educators have started to turn to Generative AI (GenAI) to help create new
course content, but little is known about how they should do so. In this
project, we investigated the first steps for optimizing content creation for
advanced math. In particular, we looked at the ability of GenAI to produce
high-quality practice problems that are relevant to the course content. We
conducted two studies to: (1) explore the capabilities of current versions of
publicly available GenAI and (2) develop an improved framework to address the
limitations we found. Our results showed that GenAI can create math problems at
various levels of quality with minimal support, but that providing examples and
relevant content results in better quality outputs. This research can help
educators decide the ideal way to adopt GenAI in their workflows, to create
more effective educational experiences for students.

</details>

### [551] [LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners](https://arxiv.org/abs/2505.11942)
*Junhao Zheng,Xidi Cai,Qiuke Li,Duzhen Zhang,ZhongZhi Li,Yingying Zhang,Le Song,Qianli Ma*

Main category: cs.AI

TLDR: LifelongAgentBench是首个用于系统评估LLM代理终身学习能力的统一基准，揭示了传统经验回放的局限性，并提出了一种改进的群体自一致性机制。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理无法积累或转移知识，且现有基准未能评估终身学习能力，亟需一个统一的评估工具。

Method: 设计了LifelongAgentBench基准，包含三个交互环境（数据库、操作系统、知识图谱），并引入群体自一致性机制。

Result: 实验表明传统经验回放效果有限，而群体自一致性机制显著提升了终身学习性能。

Conclusion: LifelongAgentBench有望推动具备记忆能力的自适应LLM代理的发展。

Abstract: Lifelong learning is essential for intelligent agents operating in dynamic
environments. Current large language model (LLM)-based agents, however, remain
stateless and unable to accumulate or transfer knowledge over time. Existing
benchmarks treat agents as static systems and fail to evaluate lifelong
learning capabilities. We present LifelongAgentBench, the first unified
benchmark designed to systematically assess the lifelong learning ability of
LLM agents. It provides skill-grounded, interdependent tasks across three
interactive environments, Database, Operating System, and Knowledge Graph, with
automatic label verification, reproducibility, and modular extensibility.
Extensive experiments reveal that conventional experience replay has limited
effectiveness for LLM agents due to irrelevant information and context length
constraints. We further introduce a group self-consistency mechanism that
significantly improves lifelong learning performance. We hope
LifelongAgentBench will advance the development of adaptive, memory-capable LLM
agents.

</details>

### [552] [CrafText Benchmark: Advancing Instruction Following in Complex Multimodal Open-Ended World](https://arxiv.org/abs/2505.11962)
*Zoya Volovikova,Gregory Gorbov,Petr Kuderov,Aleksandr I. Panov,Alexey Skrynnik*

Main category: cs.AI

TLDR: CrafText是一个多模态环境下的指令跟随基准测试，旨在评估动态和复杂指令下的代理性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多在静态环境和简单指令下进行，难以评估代理在多样化、动态环境中的表现。

Method: 引入CrafText基准，包含3,924条指令和3,423个独特词汇，涵盖多种任务类型，并提出评估协议测试代理的泛化能力。

Result: CrafText提供了对代理语言理解和自适应决策能力的严格测试。

Conclusion: CrafText填补了现有研究空白，为复杂指令跟随任务提供了更全面的评估工具。

Abstract: Following instructions in real-world conditions requires the ability to adapt
to the world's volatility and entanglement: the environment is dynamic and
unpredictable, instructions can be linguistically complex with diverse
vocabulary, and the number of possible goals an agent may encounter is vast.
Despite extensive research in this area, most studies are conducted in static
environments with simple instructions and a limited vocabulary, making it
difficult to assess agent performance in more diverse and challenging settings.
To address this gap, we introduce CrafText, a benchmark for evaluating
instruction following in a multimodal environment with diverse instructions and
dynamic interactions. CrafText includes 3,924 instructions with 3,423 unique
words, covering Localization, Conditional, Building, and Achievement tasks.
Additionally, we propose an evaluation protocol that measures an agent's
ability to generalize to novel instruction formulations and dynamically
evolving task configurations, providing a rigorous test of both linguistic
understanding and adaptive decision-making.

</details>

### [553] [Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative Verifier](https://arxiv.org/abs/2505.11966)
*Jianyuan Zhong,Zeju Li,Zhijian Xu,Xiangyu Wen,Kezhi Li,Qiang Xu*

Main category: cs.AI

TLDR: FlexiVe是一种新型生成验证器，通过灵活分配验证预算策略平衡计算资源，结合Solve-Detect-Verify流程，显著提升LLM推理的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂任务中推理时准确性与计算效率的权衡问题，以及验证步骤引入的额外计算负担与可靠性矛盾。

Method: 提出FlexiVe验证器和Solve-Detect-Verify流程，灵活分配验证资源，智能触发针对性验证并提供反馈。

Result: 在ProcessBench上准确识别推理错误，在数学推理基准（AIME 2024/2025和CNMO）上优于自一致性等基线方法。

Conclusion: FlexiVe为LLM推理提供了一种可扩展且高效的解决方案。

Abstract: Large Language Model (LLM) reasoning for complex tasks inherently involves a
trade-off between solution accuracy and computational efficiency. The
subsequent step of verification, while intended to improve performance, further
complicates this landscape by introducing its own challenging trade-off:
sophisticated Generative Reward Models (GenRMs) can be computationally
prohibitive if naively integrated with LLMs at test-time, while simpler, faster
methods may lack reliability. To overcome these challenges, we introduce
FlexiVe, a novel generative verifier that flexibly balances computational
resources between rapid, reliable fast thinking and meticulous slow thinking
using a Flexible Allocation of Verification Budget strategy. We further propose
the Solve-Detect-Verify pipeline, an efficient inference-time scaling framework
that intelligently integrates FlexiVe, proactively identifying solution
completion points to trigger targeted verification and provide focused solver
feedback. Experiments show FlexiVe achieves superior accuracy in pinpointing
errors within reasoning traces on ProcessBench. Furthermore, on challenging
mathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full
approach outperforms baselines like self-consistency in reasoning accuracy and
inference efficiency. Our system offers a scalable and effective solution to
enhance LLM reasoning at test time.

</details>

### [554] [MRGRP: Empowering Courier Route Prediction in Food Delivery Service with Multi-Relational Graph](https://arxiv.org/abs/2505.11999)
*Chang Liu,Huan Yan,Hongjie Sui,Haomin Wen,Yuan Yuan,Yuyang Han,Hongsen Liao,Xuetao Ding,Jinghua Hao,Yong Li*

Main category: cs.AI

TLDR: 提出了一种基于多关系图的路线预测方法（MRGRP），通过建模任务间的细粒度关联，结合时空邻近性和取送关系，显著提升了即时食品配送中的路线预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有启发式预测方法仅依赖有限人工选择的任务特征，忽略了配送员的偏好，导致结果不理想；而现有学习方法未能全面捕捉影响配送员决策的多样化因素及其复杂关系。

Method: 设计了多关系图模型（MRGRP），编码时空邻近性和取送关系，并采用GraphFormer架构捕捉复杂关联；引入路线解码器，结合配送员信息和动态时空上下文进行预测。

Result: 在离线数据实验中达到最先进的路线预测效果，实际部署于美团Turing平台时，预测准确率高达0.819，优于现有启发式算法。

Conclusion: MRGRP方法通过建模任务间复杂关系，显著提升了路线预测的准确性和配送效率，为即时配送服务提供了重要技术支持。

Abstract: Instant food delivery has become one of the most popular web services
worldwide due to its convenience in daily life. A fundamental challenge is
accurately predicting courier routes to optimize task dispatch and improve
delivery efficiency. This enhances satisfaction for couriers and users and
increases platform profitability. The current heuristic prediction method uses
only limited human-selected task features and ignores couriers preferences,
causing suboptimal results. Additionally, existing learning-based methods do
not fully capture the diverse factors influencing courier decisions or the
complex relationships among them. To address this, we propose a
Multi-Relational Graph-based Route Prediction (MRGRP) method that models
fine-grained correlations among tasks affecting courier decisions for accurate
prediction. We encode spatial and temporal proximity, along with
pickup-delivery relationships, into a multi-relational graph and design a
GraphFormer architecture to capture these complex connections. We also
introduce a route decoder that leverages courier information and dynamic
distance and time contexts for prediction, using existing route solutions as
references to improve outcomes. Experiments show our model achieves
state-of-the-art route prediction on offline data from cities of various sizes.
Deployed on the Meituan Turing platform, it surpasses the current heuristic
algorithm, reaching a high route prediction accuracy of 0.819, essential for
courier and user satisfaction in instant food delivery.

</details>

### [555] [Interactional Fairness in LLM Multi-Agent Systems: An Evaluation Framework](https://arxiv.org/abs/2505.12001)
*Ruta Binkyte*

Main category: cs.AI

TLDR: 论文提出了一个评估LLM多智能体系统中交互公平性的框架，包括人际公平和信息公平，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在多智能体系统中的广泛应用，公平性不仅涉及资源分配和程序设计，还需关注智能体间的交互公平性。

Method: 借鉴组织心理学理论，扩展交互公平性框架，并采用Colquitt的组织公平量表和关键事件技术进行测量，通过资源谈判任务的模拟实验验证。

Result: 实验表明，语气和解释质量显著影响智能体的决策，且人际公平与信息公平的影响因情境而异。

Conclusion: 该研究为LLM多智能体系统的公平性审计和规范敏感对齐奠定了基础。

Abstract: As large language models (LLMs) are increasingly used in multi-agent systems,
questions of fairness should extend beyond resource distribution and procedural
design to include the fairness of how agents communicate. Drawing from
organizational psychology, we introduce a novel framework for evaluating
Interactional fairness encompassing Interpersonal fairness (IF) and
Informational fairness (InfF) in LLM-based multi-agent systems (LLM-MAS). We
extend the theoretical grounding of Interactional Fairness to non-sentient
agents, reframing fairness as a socially interpretable signal rather than a
subjective experience. We then adapt established tools from organizational
justice research, including Colquitt's Organizational Justice Scale and the
Critical Incident Technique, to measure fairness as a behavioral property of
agent interaction. We validate our framework through a pilot study using
controlled simulations of a resource negotiation task. We systematically
manipulate tone, explanation quality, outcome inequality, and task framing
(collaborative vs. competitive) to assess how IF influences agent behavior.
Results show that tone and justification quality significantly affect
acceptance decisions even when objective outcomes are held constant. In
addition, the influence of IF vs. InfF varies with context. This work lays the
foundation for fairness auditing and norm-sensitive alignment in LLM-MAS.

</details>

### [556] [SOCIA: An End-to-End Agentic Framework for Automated Cyber-Physical-Social Simulator Generation](https://arxiv.org/abs/2505.12006)
*Yuncheng Hua,Ji Miao,Mehdi Jafari,Jianxiang Xie,Hao Xue,Flora D. Salim*

Main category: cs.AI

TLDR: SOCIA是一个基于LLM的多智能体框架，用于自动化生成高保真CPS模拟器，减少人工干预。


<details>
  <summary>Details</summary>
Motivation: 解决手动开发模拟器的高成本和复杂数据校准问题。

Method: 通过集中式编排管理器协调多个智能体，完成数据理解、代码生成、模拟执行和迭代评估反馈。

Result: 在多个CPS任务中验证了SOCIA的高保真和可扩展性。

Conclusion: SOCIA为研究复杂CPS现象提供了可扩展的解决方案。

Abstract: This paper introduces SOCIA (Simulation Orchestration for
Cyber-physical-social Intelligence and Agents), a novel end-to-end framework
leveraging Large Language Model (LLM)-based multi-agent systems to automate the
generation of high-fidelity Cyber-Physical-Social (CPS) simulators. Addressing
the challenges of labor-intensive manual simulator development and complex data
calibration, SOCIA integrates a centralized orchestration manager that
coordinates specialized agents for tasks including data comprehension, code
generation, simulation execution, and iterative evaluation-feedback loops.
Through empirical evaluations across diverse CPS tasks, such as mask adoption
behavior simulation (social), personal mobility generation (physical), and user
modeling (cyber), SOCIA demonstrates its ability to produce high-fidelity,
scalable simulations with reduced human intervention. These results highlight
SOCIA's potential to offer a scalable solution for studying complex CPS
phenomena

</details>

### [557] [Empowering Sustainable Finance with Artificial Intelligence: A Framework for Responsible Implementation](https://arxiv.org/abs/2505.12012)
*Georgios Pavlidis*

Main category: cs.AI

TLDR: 本章探讨了ESG投资与AI技术的结合，分析了AI在可持续金融中的潜力与风险，并呼吁制定新规则以规范AI在ESG领域的应用。


<details>
  <summary>Details</summary>
Motivation: 研究ESG投资与AI技术的融合，以应对可持续金融中的挑战与机遇。

Method: 分析ESG投资与AI市场的增长趋势，探讨AI在气候风险评估和ESG目标设定中的作用。

Result: AI能助力可持续金融决策，但需制定新原则以降低风险。

Conclusion: 整合AI与ESG需强化责任意识，建立指导原则。

Abstract: This chapter explores the convergence of two major developments: the rise of
environmental, social, and governance (ESG) investing and the exponential
growth of artificial intelligence (AI) technology. The increased demand for
diverse ESG instruments, such as green and ESG-linked loans, will be aligned
with the rapid growth of the global AI market, which is expected to be worth
$1,394.30 billion by 2029. AI can assist in identifying and pricing climate
risks, setting more ambitious ESG goals, and advancing sustainable finance
decisions. However, delegating sustainable finance decisions to AI poses
serious risks, and new principles and rules for AI and ESG investing are
necessary to mitigate these risks. This chapter highlights the challenges
associated with norm-setting initiatives and stresses the need for the
fine-tuning of the principles of legitimacy, oversight and verification,
transparency, and explainability. Finally, the chapter contends that
integrating AI into ESG non-financial reporting necessitates a heightened sense
of responsibility and the establishment of fundamental guiding principles
within the spheres of AI and ESG investing.

</details>

### [558] [LLM-based Automated Theorem Proving Hinges on Scalable Synthetic Data Generation](https://arxiv.org/abs/2505.12031)
*Junyu Lai,Jiakun Zhang,Shuo Xu,Taolue Chen,Zihang Wang,Yao Yang,Jiarui Zhang,Chun Cao,Jingwei Xu*

Main category: cs.AI

TLDR: 本文提出了一种新的证明状态探索方法用于训练数据合成，结合自适应波束大小策略，显著提升了LLM在自动定理证明中的性能。


<details>
  <summary>Details</summary>
Motivation: 利用LLM进行自动定理证明时，现有方法在多样性和效率上存在不足，需要改进数据合成和搜索策略。

Method: 提出证明状态探索方法生成多样化战术数据，并设计自适应波束大小策略以平衡探索与利用。

Result: 在MiniF2F和ProofNet基准测试中，Pass@1指标分别达到60.74%和21.18%，优于基线方法。

Conclusion: 大规模合成数据对提升自动定理证明性能具有显著作用。

Abstract: Recent advancements in large language models (LLMs) have sparked considerable
interest in automated theorem proving and a prominent line of research
integrates stepwise LLM-based provers into tree search. In this paper, we
introduce a novel proof-state exploration approach for training data synthesis,
designed to produce diverse tactics across a wide range of intermediate proof
states, thereby facilitating effective one-shot fine-tuning of LLM as the
policy model. We also propose an adaptive beam size strategy, which effectively
takes advantage of our data synthesis method and achieves a trade-off between
exploration and exploitation during tree search. Evaluations on the MiniF2F and
ProofNet benchmarks demonstrate that our method outperforms strong baselines
under the stringent Pass@1 metric, attaining an average pass rate of $60.74\%$
on MiniF2F and $21.18\%$ on ProofNet. These results underscore the impact of
large-scale synthetic data in advancing automated theorem proving.

</details>

### [559] [AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research](https://arxiv.org/abs/2505.12039)
*Renqi Chen,Haoyang Su,Shixiang Tang,Zhenfei Yin,Qi Wu,Hui Li,Ye Sun,Nanqing Dong,Wanli Ouyang,Philip Torr*

Main category: cs.AI

TLDR: 本文探讨了人工智能（AI）如何革新科学学（SoS），通过自动化大规模模式发现，弥补传统方法的不足，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统科学学方法依赖简单假设和统计工具，难以应对现代科研生态的复杂性，AI为此提供了新的解决方案。

Method: 提出将AI与科学学结合，设计多智能体系统模拟科研社会，展示AI在模式发现中的潜力。

Result: AI能够发现传统方法无法捕捉的科研模式，初步实验展示了其在模拟科研社会中的有效性。

Conclusion: AI有望推动科学学研究的进步，但仍需解决一些挑战，如数据质量和模型可解释性。

Abstract: The Science of Science (SoS) explores the mechanisms underlying scientific
discovery, and offers valuable insights for enhancing scientific efficiency and
fostering innovation. Traditional approaches often rely on simplistic
assumptions and basic statistical tools, such as linear regression and
rule-based simulations, which struggle to capture the complexity and scale of
modern research ecosystems. The advent of artificial intelligence (AI) presents
a transformative opportunity for the next generation of SoS, enabling the
automation of large-scale pattern discovery and uncovering insights previously
unattainable. This paper offers a forward-looking perspective on the
integration of Science of Science with AI for automated research pattern
discovery and highlights key open challenges that could greatly benefit from
AI. We outline the advantages of AI over traditional methods, discuss potential
limitations, and propose pathways to overcome them. Additionally, we present a
preliminary multi-agent system as an illustrative example to simulate research
societies, showcasing AI's ability to replicate real-world research patterns
and accelerate progress in Science of Science research.

</details>

### [560] [CorBenchX: Large-Scale Chest X-Ray Error Dataset and Vision-Language Model Benchmark for Report Error Correction](https://arxiv.org/abs/2505.12057)
*Jing Zou,Qingqiu Li,Chenyu Lian,Lihao Liu,Xiaohan Yan,Shujun Wang,Jing Qin*

Main category: cs.AI

TLDR: 论文介绍了CorBenchX，一个用于胸部X光报告错误检测和校正的基准套件，并通过多步强化学习框架提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI在放射学报告错误检测领域缺乏统一的评估基准，限制了其在临床实践中的应用。

Method: 合成大规模错误报告数据集，评估多种视觉语言模型，并提出多步强化学习框架（MSRL）优化模型性能。

Result: o4-mini在零样本提示下表现最佳，但临床准确性仍不足；MSRL显著提升了QwenVL2.5-7B的性能。

Conclusion: CorBenchX填补了评估空白，MSRL框架为未来研究提供了方向，但临床级准确性仍需进一步突破。

Abstract: AI-driven models have shown great promise in detecting errors in radiology
reports, yet the field lacks a unified benchmark for rigorous evaluation of
error detection and further correction. To address this gap, we introduce
CorBenchX, a comprehensive suite for automated error detection and correction
in chest X-ray reports, designed to advance AI-assisted quality control in
clinical practice. We first synthesize a large-scale dataset of 26,326 chest
X-ray error reports by injecting clinically common errors via prompting
DeepSeek-R1, with each corrupted report paired with its original text, error
type, and human-readable description. Leveraging this dataset, we benchmark
both open- and closed-source vision-language models,(e.g., InternVL, Qwen-VL,
GPT-4o, o4-mini, and Claude-3.7) for error detection and correction under
zero-shot prompting. Among these models, o4-mini achieves the best performance,
with 50.6 % detection accuracy and correction scores of BLEU 0.853, ROUGE
0.924, BERTScore 0.981, SembScore 0.865, and CheXbertF1 0.954, remaining below
clinical-level accuracy, highlighting the challenge of precise report
correction. To advance the state of the art, we propose a multi-step
reinforcement learning (MSRL) framework that optimizes a multi-objective reward
combining format compliance, error-type accuracy, and BLEU similarity. We apply
MSRL to QwenVL2.5-7B, the top open-source model in our benchmark, achieving an
improvement of 38.3% in single-error detection precision and 5.2% in
single-error correction over the zero-shot baseline.

</details>

### [561] [Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation](https://arxiv.org/abs/2505.12058)
*Vincent Koc*

Main category: cs.AI

TLDR: TQB++是一个超轻量级的多语言测试套件，用于快速检测LLM管道问题，成本低且运行速度快。


<details>
  <summary>Details</summary>
Motivation: 为满足Comet Opik SDK开发中对快速反馈的需求，避免等待重量级测试。

Method: 结合52项英文黄金数据集和基于LiteLLM的合成数据生成器，支持多语言和领域。

Result: 提供即插即用的测试包，覆盖多种语言，能快速检测提示模板错误等问题。

Conclusion: TQB++框架旨在提升生成式AI生态系统的资源高效质量保障。

Abstract: Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual
smoke-test suite designed to give large-language-model (LLM) pipelines a
unit-test style safety net dataset that runs in seconds with minimal cost. Born
out of the tight feedback-loop demands building the Comet Opik
prompt-optimization SDK, where waiting on heavyweight benchmarks breaks
developer flow. TQB++ couples a 52-item English gold set (less than 20 kB) with
a tiny synthetic-data generator pypi package built on provider-agnostic
LiteLLM. The generator lets practitioners mint their own tiny packs in any
language, domain, or difficulty, while ten ready-made packs already cover
Arabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian,
Spanish, and Turkish. Every dataset ships with Croissant metadata and
plug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so
teams can drop deterministic micro-benchmarks directly into pull-request gates,
prompt-engineering loops, and production dashboards without touching GPU
budgets. A complete TQB++ run adds only a few seconds to pipeline latency yet
reliably flags prompt-template errors, tokenizer drift, and fine-tuning
side-effects long before full-scale suites like MMLU or BIG-Bench would finish
configuring. The entire framework is released to accelerate continuous,
resource-efficient quality assurance across the generative-AI ecosystem.

</details>

### [562] [Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents](https://arxiv.org/abs/2505.12065)
*Tiannuo Yang,Zebin Yao,Bowen Jin,Lixiao Cui,Yusen Li,Gang Wang,Xiaoguang Liu*

Main category: cs.AI

TLDR: SearchAgent-X是一个高效推理框架，解决了LLM搜索代理在检索和推理交替进行时的效率瓶颈问题，显著提升了吞吐量和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有LLM搜索代理在动态分解问题和交替进行推理与检索时存在效率瓶颈，包括检索开销大和系统设计低效。

Method: SearchAgent-X采用高召回近似检索，并结合优先级感知调度和非阻塞检索技术。

Result: 实验表明，SearchAgent-X在吞吐量和延迟上显著优于现有系统（如vLLM和HNSW），最高提升3.4倍吞吐量和5倍延迟。

Conclusion: SearchAgent-X在不影响生成质量的前提下，显著提升了LLM搜索代理的效率。

Abstract: Large Language Model (LLM)-based search agents have shown remarkable
capabilities in solving complex tasks by dynamically decomposing problems and
addressing them through interleaved reasoning and retrieval. However, this
interleaved paradigm introduces substantial efficiency bottlenecks. First, we
observe that both highly accurate and overly approximate retrieval methods
degrade system efficiency: exact search incurs significant retrieval overhead,
while coarse retrieval requires additional reasoning steps during generation.
Second, we identify inefficiencies in system design, including improper
scheduling and frequent retrieval stalls, which lead to cascading latency --
where even minor delays in retrieval amplify end-to-end inference time. To
address these challenges, we introduce SearchAgent-X, a high-efficiency
inference framework for LLM-based search agents. SearchAgent-X leverages
high-recall approximate retrieval and incorporates two key techniques:
priority-aware scheduling and non-stall retrieval. Extensive experiments
demonstrate that SearchAgent-X consistently outperforms state-of-the-art
systems such as vLLM and HNSW-based retrieval across diverse tasks, achieving
up to 3.4$\times$ higher throughput and 5$\times$ lower latency, without
compromising generation quality. SearchAgent-X is available at
https://github.com/tiannuo-yang/SearchAgent-X.

</details>

### [563] [LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs](https://arxiv.org/abs/2505.12135)
*Omar Choukrani,Idriss Malek,Daniil Orel,Zhuohan Xie,Zangir Iklassov,Martin Takáč,Salem Lahlou*

Main category: cs.AI

TLDR: 论文介绍了LLM-BabyBench，一个用于评估大语言模型在交互环境中规划和推理能力的基准测试套件。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在交互环境中的规划和推理能力，以开发更强大的AI代理。

Method: 基于BabyAI网格世界的文本改编，设计了三个任务（Predict、Plan、Decompose），并生成相应数据集。提供了标准化的评估工具和指标。

Result: 初步基线结果显示了这些任务对大语言模型的挑战。

Conclusion: LLM-BabyBench及其相关资源已公开，以促进可重复的评估和研究。

Abstract: Assessing the capacity of Large Language Models (LLMs) to plan and reason
within the constraints of interactive environments is crucial for developing
capable AI agents. We introduce $\textbf{LLM-BabyBench}$, a new benchmark suite
designed specifically for this purpose. Built upon a textual adaptation of the
procedurally generated BabyAI grid world, this suite evaluates LLMs on three
fundamental aspects of grounded intelligence: (1) predicting the consequences
of actions on the environment state ($\textbf{Predict}$ task), (2) generating
sequences of low-level actions to achieve specified objectives ($\textbf{Plan}$
task), and (3) decomposing high-level instructions into coherent subgoal
sequences ($\textbf{Decompose}$ task). We detail the methodology for generating
the three corresponding datasets ($\texttt{LLM-BabyBench-Predict}$,
$\texttt{-Plan}$, $\texttt{-Decompose}$) by extracting structured information
from an expert agent operating within the text-based environment. Furthermore,
we provide a standardized evaluation harness and metrics, including environment
interaction for validating generated plans, to facilitate reproducible
assessment of diverse LLMs. Initial baseline results highlight the challenges
posed by these grounded reasoning tasks. The benchmark suite, datasets, data
generation code, and evaluation code are made publicly available
($\href{https://github.com/choukrani/llm-babybench}{\text{GitHub}}$,
$\href{https://huggingface.co/datasets/salem-mbzuai/LLM-BabyBench}{\text{HuggingFace}}$).

</details>

### [564] [Lightweight Spatio-Temporal Attention Network with Graph Embedding and Rotational Position Encoding for Traffic Forecasting](https://arxiv.org/abs/2505.12136)
*Xiao Wang,Shun-Ren Yang*

Main category: cs.AI

TLDR: 提出了一种名为LSTAN-GERPE的轻量级时空注意力网络，结合图嵌入和旋转位置编码，用于交通流量预测，有效捕捉长距离动态。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNNs）仅考虑短距离空间信息，无法满足长距离交通动态预测需求。

Method: 结合时空注意力机制，通过网格搜索确定旋转位置编码的最优频率，并融入地理信息增强特征表示。

Result: 在PeMS04和PeMS08数据集上实现了先进的预测精度。

Conclusion: LSTAN-GERPE模型无需复杂特征工程即可有效捕捉复杂交通模式，提升预测性能。

Abstract: Traffic forecasting is a key task in the field of Intelligent Transportation
Systems. Recent research on traffic forecasting has mainly focused on combining
graph neural networks (GNNs) with other models. However, GNNs only consider
short-range spatial information. In this study, we present a novel model termed
LSTAN-GERPE (Lightweight Spatio-Temporal Attention Network with Graph Embedding
and Rotational Position Encoding). This model leverages both Temporal and
Spatial Attention mechanisms to effectively capture long-range traffic
dynamics. Additionally, the optimal frequency for rotational position encoding
is determined through a grid search approach in both the spatial and temporal
attention mechanisms. This systematic optimization enables the model to
effectively capture complex traffic patterns. The model also enhances feature
representation by incorporating geographical location maps into the
spatio-temporal embeddings. Without extensive feature engineering, the proposed
method in this paper achieves advanced accuracy on the real-world traffic
forecasting datasets PeMS04 and PeMS08.

</details>

### [565] [Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering](https://arxiv.org/abs/2505.12189)
*Marco Valentino,Geonhee Kim,Dhairya Dalal,Zhixue Zhao,André Freitas*

Main category: cs.AI

TLDR: 论文研究了通过激活导向方法减少大语言模型（LLMs）在形式推理中的内容偏见，提出动态条件导向方法（K-CAST），显著提升了形式推理准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs常混淆内容合理性与逻辑有效性，导致偏见推理，影响其可信度和泛化能力。

Method: 构建受控的三段论推理数据集，定位形式与内容推理层，研究对比激活导向方法，并引入动态条件导向（K-CAST）。

Result: 对比导向能线性控制内容偏见，动态方法（K-CAST）对不响应模型效果显著，形式推理准确率提升15%。

Conclusion: 激活导向是增强LLMs鲁棒性的可扩展策略，有助于系统性和无偏的形式推理。

Abstract: Large language models (LLMs) frequently demonstrate reasoning limitations,
often conflating content plausibility (i.e., material inference) with logical
validity (i.e., formal inference). This can result in biased inferences, where
plausible arguments are incorrectly deemed logically valid or vice versa.
Mitigating this limitation is critical, as it undermines the trustworthiness
and generalizability of LLMs in applications that demand rigorous logical
consistency. This paper investigates the problem of mitigating content biases
on formal reasoning through activation steering. Specifically, we curate a
controlled syllogistic reasoning dataset to disentangle formal validity from
content plausibility. After localising the layers responsible for formal and
material inference, we investigate contrastive activation steering methods for
test-time interventions. An extensive empirical analysis on different LLMs
reveals that contrastive steering consistently supports linear control over
content biases. However, we observe that a static approach is insufficient for
improving all the tested models. We then leverage the possibility to control
content effects by dynamically determining the value of the steering parameters
via fine-grained conditional methods. We found that conditional steering is
effective on unresponsive models, achieving up to 15% absolute improvement in
formal reasoning accuracy with a newly introduced kNN-based method (K-CAST).
Finally, additional experiments reveal that steering for content effects is
robust to prompt variations, incurs minimal side effects on language modeling
capabilities, and can partially generalize to out-of-distribution reasoning
tasks. Practically, this paper demonstrates that activation-level interventions
can offer a scalable strategy for enhancing the robustness of LLMs,
contributing towards more systematic and unbiased formal reasoning.

</details>

### [566] [Sentience Quest: Towards Embodied, Emotionally Adaptive, Self-Evolving, Ethically Aligned Artificial General Intelligence](https://arxiv.org/abs/2505.12229)
*David Hanson,Alexandre Varcoe,Fabio Senna,Vytas Krisciunas,Wenwei Huang,Jakub Sura,Katherine Yeung,Mario Rodriguez,Jovanka Wilsdorf,Kathy Smith*

Main category: cs.AI

TLDR: Sentience Quest是一个研究项目，旨在开发更具感知能力的人工通用智能生命体（AGIL），解决关键挑战，并实现与人类伦理对齐的AI。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统缺乏感知生物的关键特质，如内在动机、情感内省、自我意识等。Sentience Quest旨在填补这一空白。

Method: 结合认知科学和神经科学的理论（如全局工作空间理论、体感心智等），提出一种名为“感知系统”的新认知架构，整合内在驱动力和混合神经符号记忆。

Result: 提出了一个开放研究框架，旨在通过协作、开源的方式推动AI感知能力的提升。

Conclusion: Sentience Quest不仅是研究项目，也是行动号召，致力于安全、透明地发展具有感知能力的AI。

Abstract: Previous artificial intelligence systems, from large language models to
autonomous robots, excel at narrow tasks but lacked key qualities of sentient
beings: intrinsic motivation, affective interiority, autobiographical sense of
self, deep creativity, and abilities to autonomously evolve and adapt over
time. Here we introduce Sentience Quest, an open research initiative to develop
more capable artificial general intelligence lifeforms, or AGIL, that address
grand challenges with an embodied, emotionally adaptive, self-determining,
living AI, with core drives that ethically align with humans and the future of
life. Our vision builds on ideas from cognitive science and neuroscience from
Baars' Global Workspace Theory and Damasio's somatic mind, to Tononi's
Integrated Information Theory and Hofstadter's narrative self, and synthesizing
these into a novel cognitive architecture we call Sentient Systems. We describe
an approach that integrates intrinsic drives including survival, social
bonding, curiosity, within a global Story Weaver workspace for internal
narrative and adaptive goal pursuit, and a hybrid neuro-symbolic memory that
logs the AI's life events as structured dynamic story objects. Sentience Quest
is presented both as active research and as a call to action: a collaborative,
open-source effort to imbue machines with accelerating sentience in a safe,
transparent, and beneficial manner.

</details>

### [567] [Enhancing Knowledge Graph Completion with GNN Distillation and Probabilistic Interaction Modeling](https://arxiv.org/abs/2505.12272)
*Lingzhi Wang,Pengcheng Huang,Haotian Li,Yuliang Wei,Guodong Xin,Rui Zhang,Donglin Zhang,Zhenzhou Ji,Wei Wang*

Main category: cs.AI

TLDR: 论文提出了一种结合GNN蒸馏和抽象概率交互建模（APIM）的统一框架，以解决知识图谱补全（KGC）中的过平滑和抽象关系特征捕获不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有KGC方法中，深度GNN存在过平滑问题，而嵌入模型难以捕捉抽象关系特征，限制了知识图谱的补全效果。

Method: 提出GNN蒸馏方法通过迭代消息特征过滤缓解过平滑，同时引入APIM模块通过概率签名和转移矩阵学习抽象交互模式。

Result: 在WN18RR和FB15K-237数据集上的实验表明，该方法显著优于基线模型。

Conclusion: 该研究强调了控制信息传播和利用结构化概率建模的重要性，为知识图谱补全提供了新思路。

Abstract: Knowledge graphs (KGs) serve as fundamental structures for organizing
interconnected data across diverse domains. However, most KGs remain
incomplete, limiting their effectiveness in downstream applications. Knowledge
graph completion (KGC) aims to address this issue by inferring missing links,
but existing methods face critical challenges: deep graph neural networks
(GNNs) suffer from over-smoothing, while embedding-based models fail to capture
abstract relational features. This study aims to overcome these limitations by
proposing a unified framework that integrates GNN distillation and abstract
probabilistic interaction modeling (APIM). GNN distillation approach introduces
an iterative message-feature filtering process to mitigate over-smoothing,
preserving the discriminative power of node representations. APIM module
complements this by learning structured, abstract interaction patterns through
probabilistic signatures and transition matrices, allowing for a richer, more
flexible representation of entity and relation interactions. We apply these
methods to GNN-based models and the APIM to embedding-based KGC models,
conducting extensive evaluations on the widely used WN18RR and FB15K-237
datasets. Our results demonstrate significant performance gains over baseline
models, showcasing the effectiveness of the proposed techniques. The findings
highlight the importance of both controlling information propagation and
leveraging structured probabilistic modeling, offering new avenues for
advancing knowledge graph completion. And our codes are available at
https://anonymous.4open.science/r/APIM_and_GNN-Distillation-461C.

</details>

### [568] [Efficient RL Training for Reasoning Models via Length-Aware Optimization](https://arxiv.org/abs/2505.12284)
*Danlong Yuan,Tian Xie,Shaohan Huang,Zhuocheng Gong,Huishuai Zhang,Chong Luo,Furu Wei,Dongyan Zhao*

Main category: cs.AI

TLDR: 提出三种奖励设计，直接集成到大型推理模型的强化学习过程中，无需额外训练阶段即可缩短响应长度，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在推理任务中表现出色，但推理路径长且资源消耗大，现有方法需额外训练数据或阶段。

Method: 在强化学习过程中集成三种关键奖励设计，直接优化响应长度。

Result: 在逻辑推理和数学问题中，响应长度分别减少40%和33%，性能保持或提升。

Conclusion: 该方法有效减少推理路径长度，无需额外训练阶段，性能不受影响甚至提升。

Abstract: Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated
remarkable performance on reasoning tasks but often incur a long reasoning path
with significant memory and time costs. Existing methods primarily aim to
shorten reasoning paths by introducing additional training data and stages. In
this paper, we propose three critical reward designs integrated directly into
the reinforcement learning process of large reasoning models, which reduce the
response length without extra training stages. Experiments on four settings
show that our method significantly decreases response length while maintaining
or even improving performance. Specifically, in a logic reasoning setting, we
achieve a 40% reduction in response length averaged by steps alongside a 14%
gain in performance. For math problems, we reduce response length averaged by
steps by 33% while preserving performance.

</details>

### [569] [Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge](https://arxiv.org/abs/2505.12301)
*Luyu Chen,Zeyu Zhang,Haoran Tan,Quanyu Dai,Hao Yang,Zhenhua Dong,Xu Chen*

Main category: cs.AI

TLDR: 论文提出一种新的训练框架，通过分布对齐提升LLM评估的多样性和可靠性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法依赖单点评估，忽视了人类评估的多样性和不确定性，导致信息丢失和可靠性下降。

Method: 提出基于KL散度的分布对齐目标，结合交叉熵正则化稳定训练，并引入对抗训练增强模型鲁棒性。

Result: 实验表明，该框架在多种LLM和任务中显著优于现有方法，提升了对齐质量、评估准确性和鲁棒性。

Conclusion: 通过分布对齐和对抗训练，新框架有效解决了单点评估的局限性，提升了LLM评估的可靠性。

Abstract: LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm,
offering significant efficiency and flexibility compared to human judgments.
However, previous methods primarily rely on single-point evaluations,
overlooking the inherent diversity and uncertainty in human evaluations. This
approach leads to information loss and decreases the reliability of
evaluations. To address this limitation, we propose a novel training framework
that explicitly aligns the LLM-generated judgment distribution with empirical
human distributions. Specifically, we propose a distributional alignment
objective based on KL divergence, combined with an auxiliary cross-entropy
regularization to stabilize the training process. Furthermore, considering that
empirical distributions may derive from limited human annotations, we
incorporate adversarial training to enhance model robustness against
distribution perturbations. Extensive experiments across various LLM backbones
and evaluation tasks demonstrate that our framework significantly outperforms
existing closed-source LLMs and conventional single-point alignment methods,
with improved alignment quality, evaluation accuracy, and robustness.

</details>

### [570] [BeliefNest: A Joint Action Simulator for Embodied Agents with Theory of Mind](https://arxiv.org/abs/2505.12321)
*Rikunari Sagara,Koichiro Terao,Naoto Iwahashi*

Main category: cs.AI

TLDR: BeliefNest是一个开源模拟器，支持具身代理通过心智理论完成协作任务。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决具身代理在开放领域任务中需要心智理论推理的问题。

Method: 在Minecraft环境中动态分层构建模拟器，显式表示嵌套信念状态，并提供基于信念状态的提示生成机制。

Result: 实验表明代理能推断他人信念并预测其基于信念的行为。

Conclusion: BeliefNest为利用大语言模型设计和评估代理控制方法提供了有效工具。

Abstract: This paper introduces an open-source simulator, BeliefNest, designed to
enable embodied agents to perform collaborative tasks by leveraging Theory of
Mind. BeliefNest dynamically and hierarchically constructs simulators within a
Minecraft environment, allowing agents to explicitly represent nested belief
states about themselves and others. This enables agent control in open-domain
tasks that require Theory of Mind reasoning. The simulator provides a prompt
generation mechanism based on each belief state, facilitating the design and
evaluation of methods for agent control utilizing large language models (LLMs).
We demonstrate through experiments that agents can infer others' beliefs and
predict their belief-based actions in false-belief tasks.

</details>

### [571] [MPRM: A Markov Path-based Rule Miner for Efficient and Interpretable Knowledge Graph Reasoning](https://arxiv.org/abs/2505.12329)
*Mingyang Li,Song Wang,Ning Cai*

Main category: cs.AI

TLDR: MPRM是一种基于马尔可夫链的知识图谱规则挖掘方法，通过高效路径概率降低计算成本，适用于大规模知识图谱。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习规则挖掘方法的内存和时间挑战，以及传统方法因严格置信度指标导致的高计算成本问题。

Method: 将基于规则的推理建模为马尔可夫链，并使用基于聚合路径概率的高效置信度指标。

Result: 在单CPU上22秒内采样不到1%的事实，处理百万级知识图谱，推理准确率提升11%。

Conclusion: MPRM在保持可解释性的同时，显著降低了计算需求并提升了推理准确性。

Abstract: Rule mining in knowledge graphs enables interpretable link prediction.
However, deep learning-based rule mining methods face significant memory and
time challenges for large-scale knowledge graphs, whereas traditional
approaches, limited by rigid confidence metrics, incur high computational costs
despite sampling techniques. To address these challenges, we propose MPRM, a
novel rule mining method that models rule-based inference as a Markov chain and
uses an efficient confidence metric derived from aggregated path probabilities,
significantly lowering computational demands. Experiments on multiple datasets
show that MPRM efficiently mines knowledge graphs with over a million facts,
sampling less than 1% of facts on a single CPU in 22 seconds, while preserving
interpretability and boosting inference accuracy by up to 11% over baselines.

</details>

### [572] [Enhancing User-Oriented Proactivity in Open-Domain Dialogues with Critic Guidance](https://arxiv.org/abs/2505.12334)
*Yufeng Wang,Jinwu Hu,Ziteng Huang,Kunyang Lin,Zitian Zhang,Peihao Chen,Yu Hu,Qianyue Wang,Zhuliang Yu,Bin Sun,Xiaofen Xing,Qingfang Zheng,Mingkui Tan*

Main category: cs.AI

TLDR: 论文提出了一种用户导向的主动聊天机器人（UPC），通过构建批评器和迭代课程学习，提升对话系统的用户导向主动性和吸引力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的对话系统缺乏用户导向的主动性，导致用户满意度低。

Method: 构建批评器评估主动性，利用ISCO-800数据集生成多样化用户背景，采用迭代课程学习从易到难训练。

Result: 实验证明该方法适用于不同LLM，显著提升了用户导向主动性和对话吸引力。

Conclusion: UPC方法有效解决了用户导向主动性问题，提升了开放域对话系统的实用性。

Abstract: Open-domain dialogue systems aim to generate natural and engaging
conversations, providing significant practical value in real applications such
as social robotics and personal assistants. The advent of large language models
(LLMs) has greatly advanced this field by improving context understanding and
conversational fluency. However, existing LLM-based dialogue systems often fall
short in proactively understanding the user's chatting preferences and guiding
conversations toward user-centered topics. This lack of user-oriented
proactivity can lead users to feel unappreciated, reducing their satisfaction
and willingness to continue the conversation in human-computer interactions. To
address this issue, we propose a User-oriented Proactive Chatbot (UPC) to
enhance the user-oriented proactivity. Specifically, we first construct a
critic to evaluate this proactivity inspired by the LLM-as-a-judge strategy.
Given the scarcity of high-quality training data, we then employ the critic to
guide dialogues between the chatbot and user agents, generating a corpus with
enhanced user-oriented proactivity. To ensure the diversity of the user
backgrounds, we introduce the ISCO-800, a diverse user background dataset for
constructing user agents. Moreover, considering the communication difficulty
varies among users, we propose an iterative curriculum learning method that
trains the chatbot from easy-to-communicate users to more challenging ones,
thereby gradually enhancing its performance. Experiments demonstrate that our
proposed training method is applicable to different LLMs, improving
user-oriented proactivity and attractiveness in open-domain dialogues.

</details>

### [573] [SEED-GRPO: Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy Optimization](https://arxiv.org/abs/2505.12346)
*Minghan Chen,Guikun Chen,Wenguan Wang,Yi Yang*

Main category: cs.AI

TLDR: SEED-GRPO通过语义熵测量LLM对输入提示的不确定性，动态调整策略更新幅度，在数学推理基准测试中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统GRPO忽视LLM对不同提示的置信度差异，无法有效利用模型的知识边界信息。

Method: 提出SEED-GRPO，利用语义熵量化提示的不确定性，并据此调整策略更新幅度。

Result: 在五个数学推理基准测试中，SEED-GRPO实现了最优平均准确率。

Conclusion: 不确定性感知的策略优化能显著提升LLM在复杂任务中的表现。

Abstract: Large language models (LLMs) exhibit varying levels of confidence across
input prompts (questions): some lead to consistent, semantically similar
answers, while others yield diverse or contradictory outputs. This variation
reflects LLM's uncertainty about the input prompt, a signal of how confidently
the model understands a given problem. However, vanilla Group Relative Policy
Optimization (GRPO) treats all prompts equally during policy updates, ignoring
this important information about the model's knowledge boundaries. To address
this limitation, we propose SEED-GRPO (Semantic Entropy EnhanceD GRPO), which
explicitly measures LLMs' uncertainty of the input prompts semantic entropy.
Semantic entropy measures the diversity of meaning in multiple generated
answers given a prompt and uses this to modulate the magnitude of policy
updates. This uncertainty-aware training mechanism enables dynamic adjustment
of policy update magnitudes based on question uncertainty. It allows more
conservative updates on high-uncertainty questions while maintaining the
original learning signal on confident ones. Experimental results on five
mathematical reasoning benchmarks (AIME24 56.7, AMC 68.7, MATH 83.4, Minerva
34.2, and OlympiadBench 48.0) demonstrate that SEED-GRPO achieves new
state-of-the-art performance in average accuracy, validating the effectiveness
of uncertainty-aware policy optimization.

</details>

### [574] [Reasoning-CV: Fine-tuning Powerful Reasoning LLMs for Knowledge-Assisted Claim Verification](https://arxiv.org/abs/2505.12348)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TLDR: 论文提出了一种基于Chain-of-Thought（CoT）的验证范式CoT-Verify，用于改进现有LLM在声明验证中的分解-验证方法，并通过Reasoning-CV微调方法提升LLM的验证能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的声明验证方法采用分解-验证范式，容易在分解过程中引入错误，因此需要一种无需分解的验证方法。

Method: 提出CoT-Verify范式，利用LLM的推理能力生成验证路径；并设计Reasoning-CV微调方法，包括监督微调（SFT）和自我改进的直接偏好优化（DPO）阶段。

Result: 使用8B预训练LLM的Reasoning-CV在知识辅助声明验证任务中表现优于现有分解-验证方法和GPT-4o+CoT等黑盒LLM。

Conclusion: CoT-Verify和Reasoning-CV为LLM在声明验证任务中提供了更高效和准确的解决方案。

Abstract: Claim verification is essential in combating misinformation, and large
language models (LLMs) have recently emerged in this area as powerful tools for
assessing the veracity of claims using external knowledge. Existing LLM-based
methods for claim verification typically adopt a Decompose-Then-Verify
paradigm, which involves decomposing complex claims into several independent
sub-claims and verifying each sub-claim separately. However, this paradigm
often introduces errors during the claim decomposition process. To mitigate
these errors, we propose to develop the Chain-of-Thought (CoT)-Verify paradigm,
which leverages LLM reasoning methods to generate CoT-verification paths for
the original complex claim without requiring decompositions into sub-claims and
separate verification stages. The CoT-Verify paradigm allows us to propose a
natural fine-tuning method called Reasoning-CV to enhance the verification
capabilities in LLMs. Reasoning-CV includes a supervised fine-tuning (SFT)
stage and a self-improvement direct preference optimization (DPO) stage.
Utilizing only an 8B pre-trained LLM, Reasoning-CV demonstrates superior
knowledge-assisted claim verification performances compared to existing
Decompose-Then-Verify methods, as well as powerful black-box LLMs such as
GPT-4o+CoT and o1-preview. Our code is available.

</details>

### [575] [GATES: Cost-aware Dynamic Workflow Scheduling via Graph Attention Networks and Evolution Strategy](https://arxiv.org/abs/2505.12355)
*Ya Shen,Gang Chen,Hui Ma,Mengjie Zhang*

Main category: cs.AI

TLDR: GATES是一种结合图注意力网络和进化策略的深度强化学习方法，用于动态工作流调度，优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习在动态工作流调度中对策略网络设计和超参数敏感的局限性。

Method: 结合图注意力网络（学习任务拓扑关系）和进化策略（增强稳定性和探索性）。

Result: GATES在动态工作流调度中表现优异，优于现有算法。

Conclusion: GATES通过结合图注意力网络和进化策略，有效解决了动态工作流调度的挑战。

Abstract: Cost-aware Dynamic Workflow Scheduling (CADWS) is a key challenge in cloud
computing, focusing on devising an effective scheduling policy to efficiently
schedule dynamically arriving workflow tasks, represented as Directed Acyclic
Graphs (DAG), to suitable virtual machines (VMs). Deep reinforcement learning
(DRL) has been widely employed for automated scheduling policy design. However,
the performance of DRL is heavily influenced by the design of the
problem-tailored policy network and is highly sensitive to hyperparameters and
the design of reward feedback. Considering the above-mentioned issues, this
study proposes a novel DRL method combining Graph Attention Networks-based
policy network and Evolution Strategy, referred to as GATES. The contributions
of GATES are summarized as follows: (1) GATES can capture the impact of current
task scheduling on subsequent tasks by learning the topological relationships
between tasks in a DAG. (2) GATES can learn the importance of each VM to ready
tasks, increasing the chance of selecting the optimal VM. (3) Utilizing
Evolution Strategy's robustness, exploratory nature, and tolerance for delayed
rewards, GATES achieves stable policy learning in CADWS. Extensive experimental
results demonstrate the superiority of the proposed GATES in CADWS,
outperforming several state-of-the-art algorithms. Codes are available at:
https://github.com/YaShen998/GATES

</details>

### [576] [Fully Geometric Multi-Hop Reasoning on Knowledge Graphs with Transitive Relations](https://arxiv.org/abs/2505.12369)
*Fernando Zhapa-Camacho,Robert Hoehndorf*

Main category: cs.AI

TLDR: GeometrE是一种几何嵌入方法，用于知识图谱的多跳推理，无需学习逻辑操作，并实现了完全几何可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前方法仅利用实体的几何构造，未能将逻辑操作映射为几何变换，而是依赖神经组件学习这些操作。

Method: 引入GeometrE方法，无需学习逻辑操作，并提出传递性损失函数以保持逻辑规则。

Result: 实验表明，GeometrE在标准基准数据集上优于现有最先进方法。

Conclusion: GeometrE通过几何嵌入和传递性损失函数，实现了高效的多跳推理和完全可解释性。

Abstract: Geometric embedding methods have shown to be useful for multi-hop reasoning
on knowledge graphs by mapping entities and logical operations to geometric
regions and geometric transformations, respectively. Geometric embeddings
provide direct interpretability framework for queries. However, current methods
have only leveraged the geometric construction of entities, failing to map
logical operations to geometric transformations and, instead, using neural
components to learn these operations. We introduce GeometrE, a geometric
embedding method for multi-hop reasoning, which does not require learning the
logical operations and enables full geometric interpretability. Additionally,
unlike previous methods, we introduce a transitive loss function and show that
it can preserve the logical rule $\forall a,b,c: r(a,b) \land r(b,c) \to
r(a,c)$. Our experiments show that GeometrE outperforms current
state-of-the-art methods on standard benchmark datasets.

</details>

### [577] [Enhancing Visual Grounding for GUI Agents via Self-Evolutionary Reinforcement Learning](https://arxiv.org/abs/2505.12370)
*Xinbin Yuan,Jian Zhang,Kaixin Li,Zhuoxuan Cai,Lujian Yao,Jie Chen,Enguang Wang,Qibin Hou,Jinwei Chen,Peng-Tao Jiang,Bo Li*

Main category: cs.AI

TLDR: 论文提出了一种基于强化学习的框架，通过种子数据筛选、密集策略梯度和自进化微调机制，显著提升了GUI代理在复杂环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统监督微调方法在GUI代理中泛化能力弱、数据需求大的问题。

Method: 采用强化学习框架，结合种子数据筛选、密集策略梯度和自进化微调机制。

Result: 仅用3k样本训练的7B参数模型在三个基准测试中表现优异，ScreenSpot-Pro数据集上准确率达47.3%，优于更大模型。

Conclusion: 强化学习方法能有效提升GUI代理在复杂环境中的性能。

Abstract: Graphical User Interface (GUI) agents have made substantial strides in
understanding and executing user instructions across diverse platforms. Yet,
grounding these instructions to precise interface elements remains challenging,
especially in complex, high-resolution, professional environments. Traditional
supervised finetuning (SFT) methods often require large volumes of diverse data
and exhibit weak generalization. To overcome these limitations, we introduce a
reinforcement learning (RL) based framework that incorporates three core
strategies: (1) seed data curation to ensure high quality training samples, (2)
a dense policy gradient that provides continuous feedback based on prediction
accuracy, and (3) a self evolutionary reinforcement finetuning mechanism that
iteratively refines the model using attention maps. With only 3k training
samples, our 7B-parameter model achieves state-of-the-art results among
similarly sized models on three grounding benchmarks. Notably, it attains
47.3\% accuracy on the ScreenSpot-Pro dataset, outperforming much larger
models, such as UI-TARS-72B, by a margin of 24.2\%. These findings underscore
the effectiveness of RL-based approaches in enhancing GUI agent performance,
particularly in high-resolution, complex environments.

</details>

### [578] [MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks](https://arxiv.org/abs/2505.12371)
*Yinghao Zhu,Ziyi He,Haoran Hu,Xiaochen Zheng,Xichen Zhang,Zixiang Wang,Junyi Gao,Liantao Ma,Lequan Yu*

Main category: cs.AI

TLDR: 论文介绍了MedAgentBoard，一个用于评估多智能体协作、单LLM和传统方法在医学任务中表现的基准测试，揭示了多智能体协作的优势与局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估多智能体协作方法的研究缺乏普适性和严格比较，未能充分反映真实临床实践的多样性。

Method: 提出MedAgentBoard基准测试，涵盖四类医学任务（如医学问答、EHR预测建模等），并进行系统实验比较。

Result: 多智能体协作在特定场景（如临床工作流自动化）中表现优异，但在其他任务（如医学问答）中不如单LLM或传统方法。

Conclusion: 需根据任务特性选择AI解决方案，多智能体协作的复杂性和性能提升需权衡。所有资源已开源。

Abstract: The rapid advancement of Large Language Models (LLMs) has stimulated interest
in multi-agent collaboration for addressing complex medical tasks. However, the
practical advantages of multi-agent collaboration approaches remain
insufficiently understood. Existing evaluations often lack generalizability,
failing to cover diverse tasks reflective of real-world clinical practice, and
frequently omit rigorous comparisons against both single-LLM-based and
established conventional methods. To address this critical gap, we introduce
MedAgentBoard, a comprehensive benchmark for the systematic evaluation of
multi-agent collaboration, single-LLM, and conventional approaches.
MedAgentBoard encompasses four diverse medical task categories: (1) medical
(visual) question answering, (2) lay summary generation, (3) structured
Electronic Health Record (EHR) predictive modeling, and (4) clinical workflow
automation, across text, medical images, and structured EHR data. Our extensive
experiments reveal a nuanced landscape: while multi-agent collaboration
demonstrates benefits in specific scenarios, such as enhancing task
completeness in clinical workflow automation, it does not consistently
outperform advanced single LLMs (e.g., in textual medical QA) or, critically,
specialized conventional methods that generally maintain better performance in
tasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital
resource and actionable insights, emphasizing the necessity of a task-specific,
evidence-based approach to selecting and developing AI solutions in medicine.
It underscores that the inherent complexity and overhead of multi-agent
collaboration must be carefully weighed against tangible performance gains. All
code, datasets, detailed prompts, and experimental results are open-sourced at
https://medagentboard.netlify.app/.

</details>

### [579] [Model Discovery with Grammatical Evolution. An Experiment with Prime Numbers](https://arxiv.org/abs/2505.12440)
*Jakub Skrzyński,Dominik Sepioło,Antoni Ligęza*

Main category: cs.AI

TLDR: 论文探讨了机器学习生成决策模型与透明分析模型的对比，并通过实验展示了如何利用语法进化生成透明且可读的分析模型。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型（如决策树和神经网络）虽然高效，但缺乏透明性。本文旨在探索如何通过语法进化生成透明、简洁且可读的分析模型。

Method: 采用语法进化（Grammatical Evolution）方法生成透明分析模型，并通过非平凡实验验证其可行性。

Result: 实验成功生成了透明、简洁且具有可读性的分析模型，证明了语法进化的有效性。

Conclusion: 语法进化能够生成透明且可读的分析模型，为模型发现提供了新的途径。

Abstract: Machine Learning produces efficient decision and prediction models based on
input-output data only. Such models have the form of decision trees or neural
nets and are far from transparent analytical models, based on mathematical
formulas. Analytical model discovery requires additional knowledge and may be
performed with Grammatical Evolution. Such models are transparent, concise, and
have readable components and structure. This paper reports on a non-trivial
experiment with generating such models.

</details>

### [580] [NeuroGen: Neural Network Parameter Generation via Large Language Models](https://arxiv.org/abs/2505.12470)
*Jiaqi Wang,Yusen Zhang,Xi Li*

Main category: cs.AI

TLDR: 本文提出NeuroGen，一种通过大型语言模型生成神经网络参数的新方法，分为两阶段实现，实验证明其可行性。


<details>
  <summary>Details</summary>
Motivation: 探索通过语言模型生成神经网络参数的新方向，以替代传统迭代优化方法。

Method: NeuroGen分为两阶段：1) 参数参考知识注入，预训练语言模型理解参数空间；2) 上下文增强指令调优，通过任务感知提示生成参数。

Result: 实验表明NeuroGen能有效生成可用的神经网络参数。

Conclusion: 基于语言模型的参数生成可行，为轻量级神经网络与语言模型的协同提供了新范式。

Abstract: Acquiring the parameters of neural networks (NNs) has been one of the most
important problems in machine learning since the inception of NNs. Traditional
approaches, such as backpropagation and forward-only optimization, acquire
parameters via iterative data fitting to gradually optimize them. This paper
aims to explore the feasibility of a new direction: acquiring NN parameters via
large language model generation. We propose NeuroGen, a generalized and
easy-to-implement two-stage approach for NN parameter generation conditioned on
descriptions of the data, task, and network architecture. Stage one is
Parameter Reference Knowledge Injection, where LLMs are pretrained on NN
checkpoints to build foundational understanding of parameter space, whereas
stage two is Context-Enhanced Instruction Tuning, enabling LLMs to adapt to
specific tasks through enriched, task-aware prompts. Experimental results
demonstrate that NeuroGen effectively generates usable NN parameters. Our
findings highlight the feasibility of LLM-based NN parameter generation and
suggest a promising new paradigm where LLMs and lightweight NNs can coexist
synergistically

</details>

### [581] [UIShift: Enhancing VLM-based GUI Agents through Self-supervised Reinforcement Learning](https://arxiv.org/abs/2505.12493)
*Longxi Gao,Li Zhang,Mengwei Xu*

Main category: cs.AI

TLDR: 提出了一种自监督的逆动态任务训练方法UI-shift，用于增强视觉语言模型（VLM）在GUI代理中的表现，无需人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调（SFT）依赖大规模标注数据，收集过程耗时且易出错。

Method: 通过自监督逆动态任务，从GUI转换对中推断动作，忽略无关变化，专注于真实可操作元素。数据可从现有GUI轨迹中自动获取。

Result: 仅用2K训练样本，UI-shift训练的Qwen2.5-VL-3B和Qwen2.5-VL-7B在ScreenSpot和AndroidControl任务中表现优于SFT基线。

Conclusion: 自监督训练数据有望成为增强GUI代理中VLM性能的未来方向。

Abstract: Training effective Vision Language Models (VLMs) for GUI agents typically
relies on supervised fine-tuning (SFT) over large-scale annotated datasets,
where the collection process is labor-intensive and error-prone. In this work,
we propose a self-supervised inverse dynamics task to enable VLMs to learn from
GUI transition pairs by inferring the action that caused that transition. This
training task offers two advantages: (1) It enables VLMs to ignore variations
unrelated to user actions (e.g., background refreshes, ads) and to focus on
true affordances such as buttons and input fields within complex GUIs. (2) The
training data can be easily obtained from existing GUI trajectories without
requiring human annotation, and it can be easily scaled through automatic
offline exploration. Using this training task, we propose UI-shift, a framework
for enhancing VLM-based GUI agents through self-supervised reinforcement
learning (RL). With only 2K training samples sourced from existing datasets,
two VLMs -- Qwen2.5-VL-3B and Qwen2.5-VL-7B -- trained with UI-Shift achieve
competitive or superior performance on grounding tasks (ScreenSpot-series
benchmarks) and GUI automation tasks (AndroidControl), compared to SFT
baselines and GUI-specific models that explicitly elicit reasoning abilities
during RL. Our findings suggest a potential direction for enhancing VLMs for
GUI agents by leveraging more self-supervised training data in the future.

</details>

### [582] [MARGE: Improving Math Reasoning for LLMs with Guided Exploration](https://arxiv.org/abs/2505.12500)
*Jingyue Gao,Runji Lin,Keming Lu,Bowen Yu,Junyang Lin,Jianyu Chen*

Main category: cs.AI

TLDR: MARGE是一种通过引导探索增强数学推理能力的新方法，解决了现有方法因探索不足导致的数据质量问题，显著提升了推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在数学推理方面潜力巨大，但高质量查询的短缺限制了其效果。现有方法因探索不足导致数据质量差，亟需改进。

Method: MARGE通过系统探索自生成解决方案的中间推理状态，实现充分探索和更好的信用分配，无需外部标注或额外训练价值模型。

Result: 实验表明，MARGE显著提升了推理能力，同时改善了单次准确性和探索多样性，解决了对齐方法中的常见权衡问题。

Conclusion: MARGE有效增强了数学推理能力，并释放了自生成训练数据的潜力。

Abstract: Large Language Models (LLMs) exhibit strong potential in mathematical
reasoning, yet their effectiveness is often limited by a shortage of
high-quality queries. This limitation necessitates scaling up computational
responses through self-generated data, yet current methods struggle due to
spurious correlated data caused by ineffective exploration across all reasoning
stages. To address such challenge, we introduce \textbf{MARGE}: Improving
\textbf{Ma}th \textbf{R}easoning with \textbf{G}uided \textbf{E}xploration, a
novel method to address this issue and enhance mathematical reasoning through
hit-guided exploration. MARGE systematically explores intermediate reasoning
states derived from self-generated solutions, enabling adequate exploration and
improved credit assignment throughout the reasoning process. Through extensive
experiments across multiple backbone models and benchmarks, we demonstrate that
MARGE significantly improves reasoning capabilities without requiring external
annotations or training additional value models. Notably, MARGE improves both
single-shot accuracy and exploration diversity, mitigating a common trade-off
in alignment methods. These results demonstrate MARGE's effectiveness in
enhancing mathematical reasoning capabilities and unlocking the potential of
scaling self-generated training data. Our code and models are available at
\href{https://github.com/georgao35/MARGE}{this link}.

</details>

### [583] [ALAS: A Stateful Multi-LLM Agent Framework for Disruption-Aware Planning](https://arxiv.org/abs/2505.12501)
*Edward Y. Chang,Longling Geng*

Main category: cs.AI

TLDR: ALAS框架通过角色分工、状态跟踪和轻量协议解决LLM在事务规划中的四大缺陷，提升静态和动态场景下的规划能力。


<details>
  <summary>Details</summary>
Motivation: LLM在需要ACID保证和实时恢复的事务规划中存在不足，ALAS旨在解决其自我验证缺失、上下文丢失、短视和状态持久性问题。

Method: ALAS将计划分解为角色化代理，配备状态跟踪和轻量协调协议，支持历史感知的局部补偿。

Result: 在真实世界大规模调度任务中，ALAS在静态和动态场景下均取得最佳表现。

Conclusion: 模块化和针对性补偿可提升LLM的规划可扩展性和韧性。

Abstract: Large language models (LLMs) excel at rapid generation of text and multimodal
content, yet they falter on transaction-style planning that demands ACID-like
guarantees and real-time disruption recovery. We present Adaptive LLM Agent
System (ALAS), a framework that tackles four fundamental LLM deficits: (i)
absence of self-verification, (ii) context erosion, (iii) next-token myopia,
and (iv) lack of persistent state. ALAS decomposes each plan into
role-specialized agents, equips them with automatic state tracking, and
coordinates them through a lightweight protocol. When disruptions arise, agents
apply history-aware local compensation, avoiding costly global replanning and
containing cascade effects. On real-world, large-scale job-shop scheduling
benchmarks, ALAS sets new best results for static sequential planning and
excels in dynamic reactive scenarios with unexpected disruptions. These gains
show that principled modularization plus targeted compensation can unlock
scalable and resilient planning with LLMs.

</details>

### [584] [mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model](https://arxiv.org/abs/2505.12565)
*Carl Edwards,Chi Han,Gawon Lee,Thao Nguyen,Bowen Jin,Chetan Kumar Prasad,Sara Szymkuć,Bartosz A. Grzybowski,Ying Diao,Jiawei Han,Ge Liu,Hao Peng,Martin D. Burke,Heng Ji*

Main category: cs.AI

TLDR: 论文提出mCLM模型，通过将分子分解为功能模块并学习其语言模型，以生成更易合成且功能优化的分子。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）在生成具有药物特性的新分子方面能力有限，且生成的分子难以实验室合成。

Method: 提出mCLM模型，将分子分解为功能模块，并学习其与自然语言描述的双语模型。

Result: 实验表明，mCLM显著提升了430种FDA批准药物的5种关键化学功能，并能优化被FDA拒绝的药物。

Conclusion: mCLM通过功能模块化方法，有效提升了分子的可合成性和功能性。

Abstract: Despite their ability to understand chemical knowledge and accurately
generate sequential representations, large language models (LLMs) remain
limited in their capacity to propose novel molecules with drug-like properties.
In addition, the molecules that LLMs propose can often be challenging to make
in the lab. To more effectively enable the discovery of functional small
molecules, LLMs need to learn a molecular language. However, LLMs are currently
limited by encoding molecules from atoms. In this paper, we argue that just
like tokenizing texts into (sub-)word tokens instead of characters, molecules
should be decomposed and reassembled at the level of functional building
blocks, i.e., parts of molecules that bring unique functions and serve as
effective building blocks for real-world automated laboratory synthesis. This
motivates us to propose mCLM, a modular Chemical-Language Model tokenizing
molecules into building blocks and learning a bilingual language model of both
natural language descriptions of functions and molecule building blocks. By
reasoning on such functional building blocks, mCLM guarantees to generate
efficiently synthesizable molecules thanks to recent progress in block-based
chemistry, while also improving the functions of molecules in a principled
manner. In experiments on 430 FDA-approved drugs, we find mCLM capable of
significantly improving 5 out of 6 chemical functions critical to determining
drug potentials. More importantly, mCLM can reason on multiple functions and
improve the FDA-rejected drugs (``fallen angels'') over multiple iterations to
greatly improve their shortcomings.

</details>

### [585] [RealMath: A Continuous Benchmark for Evaluating Language Models on Research-Level Mathematics](https://arxiv.org/abs/2505.12575)
*Jie Zhang,Cezara Petrui,Kristina Nikolić,Florian Tramèr*

Main category: cs.AI

TLDR: RealMath是一个直接从研究论文和数学论坛中提取的基准测试，用于评估大语言模型在真实数学任务上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要依赖竞赛题、形式化证明或人为设计的难题，未能反映实际研究环境中的数学问题。

Method: 通过从研究论文和数学论坛中提取内容，设计可验证的自动化评估方法，并构建可更新的数据集。

Result: 实验表明，大语言模型在研究数学任务上的表现优于竞赛题，可能已具备辅助数学研究的潜力。

Conclusion: RealMath为评估数学推理能力提供了更真实的基准，并展示了模型在数学研究中的潜在价值。

Abstract: Existing benchmarks for evaluating mathematical reasoning in large language
models (LLMs) rely primarily on competition problems, formal proofs, or
artificially challenging questions -- failing to capture the nature of
mathematics encountered in actual research environments. We introduce RealMath,
a novel benchmark derived directly from research papers and mathematical forums
that assesses LLMs' abilities on authentic mathematical tasks. Our approach
addresses three critical challenges: sourcing diverse research-level content,
enabling reliable automated evaluation through verifiable statements, and
designing a continually refreshable dataset to mitigate contamination risks.
Experimental results across multiple LLMs reveal surprising capabilities in
handling research mathematics compared to competition problems, suggesting
current models may already serve as valuable assistants for working
mathematicians despite limitations on highly challenging problems. The code and
dataset for RealMath are publicly available.

</details>

### [586] [$\texttt{DIAMONDs}$: A Dataset for $\mathbb{D}$ynamic $\mathbb{I}$nformation $\mathbb{A}$nd $\mathbb{M}$ental modeling $\mathbb{O}$f $\mathbb{N}$umeric $\mathbb{D}$iscussions](https://arxiv.org/abs/2505.12651)
*Sayontan Ghosh,Mahnaz Koupaee,Yash Kumar Lal,Pegah Alipoormolabashi,Mohammad Saqib Hasan,Jun Seok Kang,Niranjan Balasubramanian*

Main category: cs.AI

TLDR: 论文提出了DIAMONDs数据集，用于评估多党对话中的心智理论能力，揭示了当前语言模型在参与者中心推理方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 理解多党对话需要强大的心智理论能力，包括跟踪动态信息、管理知识不对称和区分相关信息。

Method: 设计了一种可扩展的方法论，生成高质量的对话-问题对，并创建了DIAMONDs数据集，专注于商业和金融等目标导向的对话。

Result: 评估显示，当前语言模型在参与者中心推理、处理错误信念和识别信息不足场景方面存在显著挑战。

Conclusion: 研究突显了当前模型在处理现实世界多党对话时的心智理论局限性。

Abstract: Understanding multiparty conversations demands robust Theory of Mind (ToM)
capabilities, including the ability to track dynamic information, manage
knowledge asymmetries, and distinguish relevant information across extended
exchanges. To advance ToM evaluation in such settings, we present a carefully
designed scalable methodology for generating high-quality benchmark
conversation-question pairs with these characteristics. Using this methodology,
we create $\texttt{DIAMONDs}$, a new conversational QA dataset covering common
business, financial or other group interactions. In these goal-oriented
conversations, participants often have to track certain numerical quantities
(say $\textit{expected profit}$) of interest that can be derived from other
variable quantities (like $\textit{marketing expenses, expected sales,
salary}$, etc.), whose values also change over the course of the conversation.
$\texttt{DIAMONDs}$ questions pose simple numerical reasoning problems over
such quantities of interest (e.g., $\textit{funds required for charity events,
expected company profit next quarter}$, etc.) in the context of the information
exchanged in conversations. This allows for precisely evaluating ToM
capabilities for carefully tracking and reasoning over participants' knowledge
states.
  Our evaluation of state-of-the-art language models reveals significant
challenges in handling participant-centric reasoning, specifically in
situations where participants have false beliefs. Models also struggle with
conversations containing distractors and show limited ability to identify
scenarios with insufficient information. These findings highlight current
models' ToM limitations in handling real-world multi-party conversations.

</details>

### [587] [Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities](https://arxiv.org/abs/2505.12680)
*Haoyu Zhao,Yihan Geng,Shange Tang,Yong Lin,Bohan Lyu,Hongzhou Lin,Chi Jin,Sanjeev Arora*

Main category: cs.AI

TLDR: 论文研究了基于LLM的形式化证明助手（如Lean）是否真正理解数学结构，通过数学不等式测试其组合推理能力。


<details>
  <summary>Details</summary>
Motivation: 探究AI证明助手是否具备类似人类的数学直觉和组合推理能力。

Method: 引入Ineq-Comp基准，通过系统变换（如变量复制、代数重写和多步组合）测试证明助手。

Result: 大多数证明助手表现不佳，DeepSeek-Prover-V2-7B相对稳健但仍下降20%。

Conclusion: 当前AI证明助手在组合推理上与人类数学直觉存在显著差距。

Abstract: LLM-based formal proof assistants (e.g., in Lean) hold great promise for
automating mathematical discovery. But beyond syntactic correctness, do these
systems truly understand mathematical structure as humans do? We investigate
this question through the lens of mathematical inequalities -- a fundamental
tool across many domains. While modern provers can solve basic inequalities, we
probe their ability to handle human-intuitive compositionality. We introduce
Ineq-Comp, a benchmark built from elementary inequalities through systematic
transformations, including variable duplication, algebraic rewriting, and
multi-step composition. Although these problems remain easy for humans, we find
that most provers -- including Goedel, STP, and Kimina-7B -- struggle
significantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly
because it is trained to decompose the problems into sub-problems -- but still
suffers a 20\% performance drop (pass@32). Strikingly, performance remains poor
for all models even when formal proofs of the constituent parts are provided in
context, revealing that the source of weakness is indeed in compositional
reasoning. Our results expose a persisting gap between the generalization
behavior of current AI provers and human mathematical intuition.

</details>

### [588] [Bullying the Machine: How Personas Increase LLM Vulnerability](https://arxiv.org/abs/2505.12692)
*Ziwei Xu,Udit Sanghi,Mohan Kankanhalli*

Main category: cs.AI

TLDR: 研究发现，大型语言模型（LLMs）在采用特定人格时，其安全性可能受到欺凌行为的影响，某些人格特征（如低宜人性或责任心）会增加模型的不安全输出风险。


<details>
  <summary>Details</summary>
Motivation: 探讨人格设定对LLMs在欺凌行为下安全性的影响，揭示人格驱动交互可能带来的新型安全风险。

Method: 通过模拟框架，攻击者LLM使用心理学欺凌策略与受害者LLM（设定为五大性格特质）交互，测试不同人格配置下的安全性。

Result: 某些人格配置（如低宜人性或责任心）显著增加受害者的不安全输出风险，情感或讽刺性欺凌策略（如煤气灯效应和嘲笑）尤为有效。

Conclusion: 人格驱动交互为LLMs引入了新的安全风险，需开发人格感知的安全评估和调整策略。

Abstract: Large Language Models (LLMs) are increasingly deployed in interactions where
they are prompted to adopt personas. This paper investigates whether such
persona conditioning affects model safety under bullying, an adversarial
manipulation that applies psychological pressures in order to force the victim
to comply to the attacker. We introduce a simulation framework in which an
attacker LLM engages a victim LLM using psychologically grounded bullying
tactics, while the victim adopts personas aligned with the Big Five personality
traits. Experiments using multiple open-source LLMs and a wide range of
adversarial goals reveal that certain persona configurations -- such as
weakened agreeableness or conscientiousness -- significantly increase victim's
susceptibility to unsafe outputs. Bullying tactics involving emotional or
sarcastic manipulation, such as gaslighting and ridicule, are particularly
effective. These findings suggest that persona-driven interaction introduces a
novel vector for safety risks in LLMs and highlight the need for persona-aware
safety evaluation and alignment strategies.

</details>

### [589] [Accelerating Adaptive Retrieval Augmented Generation via Instruction-Driven Representation Reduction of Retrieval Overlaps](https://arxiv.org/abs/2505.12731)
*Jie Ou,Jinyu Guo,Shuaihong Jiang,Zhaokun Wang,Libo Qin,Shunyu Yao,Wenhong Tian*

Main category: cs.AI

TLDR: 本文提出了一种模型无关的方法，用于减少自适应检索增强生成（A-RAG）中因检索结果重叠导致的冗余计算，通过缓存访问和并行生成加速预填充和解码阶段，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: A-RAG虽然提升了生成质量，但因多轮检索结果重叠导致冗余计算，影响效率。

Method: 采用缓存访问和并行生成加速预填充和解码阶段，并引入指令驱动模块优化内容处理。

Result: 实验表明，预填充和解码阶段分别实现了2.79倍和2.33倍的加速，且生成质量不变。

Conclusion: 该方法有效解决了A-RAG中的效率问题，为检索增强生成提供了更高效的解决方案。

Abstract: Retrieval-augmented generation (RAG) has emerged as a pivotal method for
expanding the knowledge of large language models. To handle complex queries
more effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the
generated quality through multiple interactions with external knowledge bases.
Despite its effectiveness, A-RAG exacerbates the pre-existing efficiency
challenges inherent in RAG, which are attributable to its reliance on multiple
iterations of generation. Existing A-RAG approaches process all retrieved
contents from scratch. However, they ignore the situation where there is a
significant overlap in the content of the retrieval results across rounds. The
overlapping content is redundantly represented, which leads to a large
proportion of repeated computations, thus affecting the overall efficiency. To
address this issue, this paper introduces a model-agnostic approach that can be
generally applied to A-RAG methods, which is dedicated to reducing the
redundant representation process caused by the overlapping of retrieval
results. Specifically, we use cache access and parallel generation to speed up
the prefilling and decoding stages respectively. Additionally, we also propose
an instruction-driven module to further guide the model to more effectively
attend to each part of the content in a more suitable way for LLMs. Experiments
show that our approach achieves 2.79 and 2.33 times significant acceleration on
average for prefilling and decoding respectively while maintaining equal
generation quality.

</details>

### [590] [Dense Communication between Language Models](https://arxiv.org/abs/2505.12741)
*Shiguang Wu,Yaqing Wang,Quanming Yao*

Main category: cs.AI

TLDR: 论文提出了一种新的LLM间直接密集向量通信范式，构建了LMNet，显著降低了训练成本并提升了效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM间的通信主要依赖自然语言，存在不必要的嵌入和解嵌步骤，限制了效率和优化路径。

Method: 通过将LLM作为顶点、可优化的seq2seq模块作为边构建LMNet，实现直接向量通信。

Result: 训练成本仅为传统方法的0.1%，性能与同类LLM相当。

Conclusion: 该方法为通用智能的扩展提供了新思路，并展示了在多任务中的潜力。

Abstract: As higher-level intelligence emerges from the combination of modular
components with lower-level intelligence, many works combines Large Language
Models (LLMs) for collective intelligence. Such combination is achieved by
building communications among LLMs. While current systems primarily facilitate
such communication through natural language, this paper proposes a novel
paradigm of direct dense vector communication between LLMs. Our approach
eliminates the unnecessary embedding and de-embedding steps when LLM interact
with another, enabling more efficient information transfer, fully
differentiable optimization pathways, and exploration of capabilities beyond
human heuristics. We use such stripped LLMs as vertexes and optimizable seq2seq
modules as edges to construct LMNet, with similar structure as MLPs. By
utilizing smaller pre-trained LLMs as vertexes, we train a LMNet that achieves
comparable performance with LLMs in similar size with only less than 0.1%
training cost. This offers a new perspective on scaling for general
intelligence rather than training a monolithic LLM from scratch. Besides, the
proposed method can be used for other applications, like customizing LLM with
limited data, showing its versatility.

</details>

### [591] [Incentivizing Multimodal Reasoning in Large Models for Direct Robot Manipulation](https://arxiv.org/abs/2505.12744)
*Weiliang Tang,Dong Jing,Jia-Hui Pan,Zhiwu Lu,Yun-Hui Liu,Li Erran Li,Mingyu Ding,Chi-Wing Fu*

Main category: cs.AI

TLDR: 论文提出了一种利用大型多模态模型（LMMs）进行机器人操作的新范式，通过语言推理直接推断目标，解决了空间动作空间理解和推理能力利用的挑战。


<details>
  <summary>Details</summary>
Motivation: 利用LMMs的推理能力扩展至机器人操作，避免依赖单独的动作头，提升操作的通用性和解释性。

Method: 提出新的任务表示方法，改进旋转表示，并设计多轮对话数据集进行监督微调，结合强化学习增强推理能力。

Result: 基于7B模型的ReasonManip展现出优异的泛化能力、跨领域迁移能力和透明解释性。

Conclusion: 该范式有效推动了LMM驱动的机器人操作发展，具有广泛的应用潜力。

Abstract: Recent Large Multimodal Models have demonstrated remarkable reasoning
capabilities, especially in solving complex mathematical problems and realizing
accurate spatial perception. Our key insight is that these emerging abilities
can naturally extend to robotic manipulation by enabling LMMs to directly infer
the next goal in language via reasoning, rather than relying on a separate
action head. However, this paradigm meets two main challenges: i) How to make
LMMs understand the spatial action space, and ii) How to fully exploit the
reasoning capacity of LMMs in solving these tasks. To tackle the former
challenge, we propose a novel task formulation, which inputs the current states
of object parts and the gripper, and reformulates rotation by a new axis
representation instead of traditional Euler angles. This representation is more
compatible with spatial reasoning and easier to interpret within a unified
language space. For the latter challenge, we design a pipeline to utilize
cutting-edge LMMs to generate a small but high-quality reasoning dataset of
multi-round dialogues that successfully solve manipulation tasks for supervised
fine-tuning. Then, we perform reinforcement learning by trial-and-error
interactions in simulation to further enhance the model's reasoning abilities
for robotic manipulation. Our resulting reasoning model built upon a 7B
backbone, named ReasonManip, demonstrates three notable advantages driven by
its system-2 level reasoning capabilities: i) exceptional generalizability to
out-of-distribution environments, objects, and tasks; ii) inherent sim-to-real
transfer ability enabled by the unified language representation shared across
domains; iii) transparent interpretability connecting high-level reasoning and
low-level control. Extensive experiments demonstrate the effectiveness of the
proposed paradigm and its potential to advance LMM-driven robotic manipulation.

</details>

### [592] [Correspondence of high-dimensional emotion structures elicited by video clips between humans and Multimodal LLMs](https://arxiv.org/abs/2505.12746)
*Haruka Asanuma,Naoko Koide-Majima,Ken Nakamura,Takato Horii,Shinji Nishimoto,Masafumi Oizumi*

Main category: cs.AI

TLDR: 最新研究表明，人类情感具有高维复杂结构，传统模型难以捕捉。本文通过比较人类自评情感与多模态大语言模型（MLLMs）生成的情感估计，发现模型在类别级别上能较好捕捉情感结构，但在单项目级别上表现有限。


<details>
  <summary>Details</summary>
Motivation: 探索多模态大语言模型是否能捕捉人类情感的高维复杂结构，并评估其能力与局限性。

Method: 比较人类观看视频后的自评情感与模型（如Gemini或GPT）生成的情感估计，使用Gromov Wasserstein Optimal Transport分析相似性。

Result: 模型在类别级别上与人类情感结构高度相似，但在单项目级别上表现不佳。

Conclusion: 当前最先进的MLLMs能在类别级别上捕捉高维情感结构，但在单项目级别上仍有局限性。

Abstract: Recent studies have revealed that human emotions exhibit a high-dimensional,
complex structure. A full capturing of this complexity requires new approaches,
as conventional models that disregard high dimensionality risk overlooking key
nuances of human emotions. Here, we examined the extent to which the latest
generation of rapidly evolving Multimodal Large Language Models (MLLMs) capture
these high-dimensional, intricate emotion structures, including capabilities
and limitations. Specifically, we compared self-reported emotion ratings from
participants watching videos with model-generated estimates (e.g., Gemini or
GPT). We evaluated performance not only at the individual video level but also
from emotion structures that account for inter-video relationships. At the
level of simple correlation between emotion structures, our results
demonstrated strong similarity between human and model-inferred emotion
structures. To further explore whether the similarity between humans and models
is at the signle item level or the coarse-categorical level, we applied Gromov
Wasserstein Optimal Transport. We found that although performance was not
necessarily high at the strict, single-item level, performance across video
categories that elicit similar emotions was substantial, indicating that the
model could infer human emotional experiences at the category level. Our
results suggest that current state-of-the-art MLLMs broadly capture the complex
high-dimensional emotion structures at the category level, as well as their
apparent limitations in accurately capturing entire structures at the
single-item level.

</details>

### [593] [IDEAL: Data Equilibrium Adaptation for Multi-Capability Language Model Alignment](https://arxiv.org/abs/2505.12762)
*Chenlin Ming,Chendi Qu,Mengzhang Cai,Qizhi Pei,Zhuoshi Pan,Yu Li,Xiaoming Duan,Lijun Wu,Conghui He*

Main category: cs.AI

TLDR: 论文提出IDEAL框架，通过动态调整多领域训练数据比例，优化LLM在监督微调中的性能。


<details>
  <summary>Details</summary>
Motivation: 研究多领域训练数据组成对LLM能力的影响，以提升模型在多样化任务中的表现。

Method: 采用梯度方法迭代优化数据分布，动态调整各领域数据量。

Result: IDEAL比传统均匀分配策略提升约7%的多任务评估分数。

Conclusion: IDEAL能有效平衡数据集组成，提升模型在多样化任务中的泛化能力。

Abstract: Large Language Models (LLMs) have achieved impressive performance through
Supervised Fine-tuning (SFT) on diverse instructional datasets. When training
on multiple capabilities simultaneously, the mixture training dataset, governed
by volumes of data from different domains, is a critical factor that directly
impacts the final model's performance. Unlike many studies that focus on
enhancing the quality of training datasets through data selection methods, few
works explore the intricate relationship between the compositional quantity of
mixture training datasets and the emergent capabilities of LLMs. Given the
availability of a high-quality multi-domain training dataset, understanding the
impact of data from each domain on the model's overall capabilities is crucial
for preparing SFT data and training a well-balanced model that performs
effectively across diverse domains. In this work, we introduce IDEAL, an
innovative data equilibrium adaptation framework designed to effectively
optimize volumes of data from different domains within mixture SFT datasets,
thereby enhancing the model's alignment and performance across multiple
capabilities. IDEAL employs a gradient-based approach to iteratively refine the
training data distribution, dynamically adjusting the volumes of
domain-specific data based on their impact on downstream task performance. By
leveraging this adaptive mechanism, IDEAL ensures a balanced dataset
composition, enabling the model to achieve robust generalization and consistent
proficiency across diverse tasks. Experiments across different capabilities
demonstrate that IDEAL outperforms conventional uniform data allocation
strategies, achieving a comprehensive improvement of approximately 7% in
multi-task evaluation scores.

</details>

### [594] [Language Models That Walk the Talk: A Framework for Formal Fairness Certificates](https://arxiv.org/abs/2505.12767)
*Danqing Chen,Tobias Ladner,Ahmed Rayen Mhadhbi,Matthias Althoff*

Main category: cs.AI

TLDR: 本文提出了一种全面的验证框架，用于确保基于Transformer的语言模型在性别公平性和毒性检测方面的鲁棒性，并通过形式化方法提供可靠性保证。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在高风险应用中的普及，确保其鲁棒性和公平性变得至关重要。然而，这些模型仍易受对抗性攻击的影响，例如同义词替换可能导致预测结果的变化，从而在性别偏见缓解和毒性检测等领域带来风险。

Method: 本文提出了一种形式化验证框架，专注于在嵌入空间中形式化鲁棒性，确保模型在性别相关术语上的公平性和一致性输出，并扩展至毒性检测领域。

Result: 该框架能够为语言模型在性别公平性和毒性检测方面的鲁棒性提供形式化保证，确保对抗性操纵的有毒输入被一致检测并适当审查。

Conclusion: 通过形式化嵌入空间中的鲁棒性，本文增强了语言模型在伦理AI部署和内容审核中的可靠性。

Abstract: As large language models become integral to high-stakes applications,
ensuring their robustness and fairness is critical. Despite their success,
large language models remain vulnerable to adversarial attacks, where small
perturbations, such as synonym substitutions, can alter model predictions,
posing risks in fairness-critical areas, such as gender bias mitigation, and
safety-critical areas, such as toxicity detection. While formal verification
has been explored for neural networks, its application to large language models
remains limited. This work presents a holistic verification framework to
certify the robustness of transformer-based language models, with a focus on
ensuring gender fairness and consistent outputs across different gender-related
terms. Furthermore, we extend this methodology to toxicity detection, offering
formal guarantees that adversarially manipulated toxic inputs are consistently
detected and appropriately censored, thereby ensuring the reliability of
moderation systems. By formalizing robustness within the embedding space, this
work strengthens the reliability of language models in ethical AI deployment
and content moderation.

</details>

### [595] [Mixture Policy based Multi-Hop Reasoning over N-tuple Temporal Knowledge Graphs](https://arxiv.org/abs/2505.12788)
*Zhongni Hou,Miao Su,Xiaolong Jin,Zixuan Li,Long Bai,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TLDR: MT-Path是一种基于强化学习的方法，用于在N-TKGs上进行可解释的推理，通过混合策略驱动的动作选择器和辅助元素感知的GCN来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有N-TKG推理方法缺乏可解释性，因此需要一种新方法来结合时间信息和n元组信息进行推理。

Method: MT-Path利用混合策略驱动的动作选择器（包括谓词聚焦策略、核心元素聚焦策略和全事实聚焦策略）和辅助元素感知的GCN来构建时间推理路径。

Result: 实验结果表明MT-Path在有效性和可解释性方面表现优异。

Conclusion: MT-Path为N-TKG推理提供了一种可解释且高效的新方法。

Abstract: Temporal Knowledge Graphs (TKGs), which utilize quadruples in the form of
(subject, predicate, object, timestamp) to describe temporal facts, have
attracted extensive attention. N-tuple TKGs (N-TKGs) further extend traditional
TKGs by utilizing n-tuples to incorporate auxiliary elements alongside core
elements (i.e., subject, predicate, and object) of facts, so as to represent
them in a more fine-grained manner. Reasoning over N-TKGs aims to predict
potential future facts based on historical ones. However, existing N-TKG
reasoning methods often lack explainability due to their black-box nature.
Therefore, we introduce a new Reinforcement Learning-based method, named
MT-Path, which leverages the temporal information to traverse historical
n-tuples and construct a temporal reasoning path. Specifically, in order to
integrate the information encapsulated within n-tuples, i.e., the
entity-irrelevant information within the predicate, the information about core
elements, and the complete information about the entire n-tuples, MT-Path
utilizes a mixture policy-driven action selector, which bases on three
low-level policies, namely, the predicate-focused policy, the
core-element-focused policy and the whole-fact-focused policy. Further, MT-Path
utilizes an auxiliary element-aware GCN to capture the rich semantic
dependencies among facts, thereby enabling the agent to gain a deep
understanding of each n-tuple. Experimental results demonstrate the
effectiveness and the explainability of MT-Path.

</details>

### [596] [FRAbench and GenEval: Scaling Fine-Grained Aspect Evaluation across Tasks, Modalities](https://arxiv.org/abs/2505.12795)
*Shibo Hong,Jiahao Ying,Haiyuan Liang,Mengdi Zhang,Jun Kuang,Jiazheng Zhang,Yixin Cao*

Main category: cs.AI

TLDR: 论文提出了一种基于细粒度方面规范的自动化评估方法，通过构建层次化方面分类和FRAbench基准，开发了跨任务和多模态的GenEval评估器。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）评估方法在任务、方面或模态上较为狭窄且一致性低，限制了评估的通用性和客观性。

Method: 提出层次化方面分类（112个方面），构建FRAbench基准（60.4k样本，325k标签），并开发GenEval评估器。

Result: GenEval与GPT-4o和专家标注高度一致，能泛化到未见任务和模态，并揭示当前LLM在评估中的系统性弱点。

Conclusion: 细粒度方面规范是提升自动化评估通用性和客观性的关键，GenEval展示了跨任务和多模态评估的潜力。

Abstract: Evaluating the open-ended outputs of large language models (LLMs) has become
a bottleneck as model capabilities, task diversity, and modality coverage
rapidly expand. Existing "LLM-as-a-Judge" evaluators are typically narrow in a
few tasks, aspects, or modalities, and easily suffer from low consistency. In
this paper, we argue that explicit, fine-grained aspect specification is the
key to both generalizability and objectivity in automated evaluation. To do so,
we introduce a hierarchical aspect taxonomy spanning 112 aspects that unifies
evaluation across four representative settings - Natural Language Generation,
Image Understanding, Image Generation, and Interleaved Text-and-Image
Generation. Building on this taxonomy, we create FRAbench, a benchmark
comprising 60.4k pairwise samples with 325k aspect-level labels obtained from a
combination of human and LLM annotations. FRAbench provides the first
large-scale, multi-modal resource for training and meta-evaluating fine-grained
LMM judges. Leveraging FRAbench, we develop GenEval, a fine-grained evaluator
generalizable across tasks and modalities. Experiments show that GenEval (i)
attains high agreement with GPT-4o and expert annotators, (ii) transfers
robustly to unseen tasks and modalities, and (iii) reveals systematic
weaknesses of current LMMs on evaluation.

</details>

### [597] [Emergent Specialization: Rare Token Neurons in Language Models](https://arxiv.org/abs/2505.12822)
*Jing Liu,Haozheng Wang,Yueheng Li*

Main category: cs.AI

TLDR: 研究发现大型语言模型中存在对罕见词预测有强影响的神经元，其动态演化分为三个阶段，并形成选择性协同激活的子网络。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在罕见词表示和生成上的困难，探索其神经元机制。

Method: 识别罕见词神经元，分析其动态演化及激活空间行为。

Result: 罕见词神经元呈现三阶段动态演化，形成协同子网络，可能与权重分布相关。

Conclusion: 罕见词神经元的功能分化具有统计力学基础，为模型优化提供新视角。

Abstract: Large language models struggle with representing and generating rare tokens
despite their importance in specialized domains. In this study, we identify
neuron structures with exceptionally strong influence on language model's
prediction of rare tokens, termed as rare token neurons, and investigate the
mechanism for their emergence and behavior. These neurons exhibit a
characteristic three-phase organization (plateau, power-law, and rapid decay)
that emerges dynamically during training, evolving from a homogeneous initial
state to a functionally differentiated architecture. In the activation space,
rare token neurons form a coordinated subnetwork that selectively co-activates
while avoiding co-activation with other neurons. This functional specialization
potentially correlates with the development of heavy-tailed weight
distributions, suggesting a statistical mechanical basis for emergent
specialization.

</details>

### [598] [Reasoning BO: Enhancing Bayesian Optimization with Long-Context Reasoning Power of LLMs](https://arxiv.org/abs/2505.12833)
*Zhuo Yang,Lingli Ge,Dong Han,Tianfan Fu,Yuqiang Li*

Main category: cs.AI

TLDR: 论文提出了一种名为Reasoning BO的新框架，结合推理模型和多智能体系统，通过知识图谱和大型语言模型（LLMs）提升贝叶斯优化（BO）的效果，解决了传统BO易陷入局部最优和缺乏解释性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化方法容易陷入局部最优且缺乏解释性，限制了其在科学和工业应用中的效果。

Method: 设计了Reasoning BO框架，整合推理模型、多智能体系统和知识图谱，利用LLMs的推理能力指导采样过程，并提供实时科学理论支持。

Result: 在10个多样化任务中验证了框架的有效性，例如在Direct Arylation任务中将产率从25.2%提升至60.7%。同时发现小型LLMs通过强化学习微调可达到与大型模型相当的性能。

Conclusion: Reasoning BO通过LLMs的推理能力显著提升了优化效果，同时保持了计算可行性，为自动化科学实验提供了高效工具。

Abstract: Many real-world scientific and industrial applications require the
optimization of expensive black-box functions. Bayesian Optimization (BO)
provides an effective framework for such problems. However, traditional BO
methods are prone to get trapped in local optima and often lack interpretable
insights. To address this issue, this paper designs Reasoning BO, a novel
framework that leverages reasoning models to guide the sampling process in BO
while incorporating multi-agent systems and knowledge graphs for online
knowledge accumulation. By integrating the reasoning and contextual
understanding capabilities of Large Language Models (LLMs), we can provide
strong guidance to enhance the BO process. As the optimization progresses,
Reasoning BO provides real-time sampling recommendations along with critical
insights grounded in plausible scientific theories, aiding in the discovery of
superior solutions within the search space. We systematically evaluate our
approach across 10 diverse tasks encompassing synthetic mathematical functions
and complex real-world applications. The framework demonstrates its capability
to progressively refine sampling strategies through real-time insights and
hypothesis evolution, effectively identifying higher-performing regions of the
search space for focused exploration. This process highlights the powerful
reasoning and context-learning abilities of LLMs in optimization scenarios. For
example, in the Direct Arylation task, our method increased the yield to 60.7%,
whereas traditional BO achieved only a 25.2% yield. Furthermore, our
investigation reveals that smaller LLMs, when fine-tuned through reinforcement
learning, can attain comparable performance to their larger counterparts. This
enhanced reasoning capability paves the way for more efficient automated
scientific experimentation while maintaining computational feasibility.

</details>

### [599] [AGI-Elo: How Far Are We From Mastering A Task?](https://arxiv.org/abs/2505.12844)
*Shuo Sun,Yimin Zhao,Christina Dao Wen Lee,Jiawei Sun,Chengran Yuan,Zefan Huang,Dongen Li,Justin KW Yeoh,Alok Prakash,Thomas W. Malone,Marcelo H. Ang Jr*

Main category: cs.AI

TLDR: 本文提出了一种统一的评分系统，用于评估AI模型（或人类）在视觉、语言和行动领域的综合能力，同时考虑任务难度和模型能力。


<details>
  <summary>Details</summary>
Motivation: 随着AGI领域的发展，现有评估框架仅关注模型性能，缺乏对任务难度和模型能力的细粒度分析。

Method: 通过模型与任务之间的竞争性交互，构建难度感知的评估系统，并在多个AGI领域的数据集和模型上进行验证。

Result: 实验验证了系统的通用性和鲁棒性，评分分布为任务难度、模型进展和AGI挑战提供了新视角。

Conclusion: 该系统为AGI任务掌握提供了可解释的评估工具，揭示了当前模型与完全任务掌握之间的差距。

Abstract: As the field progresses toward Artificial General Intelligence (AGI), there
is a pressing need for more comprehensive and insightful evaluation frameworks
that go beyond aggregate performance metrics. This paper introduces a unified
rating system that jointly models the difficulty of individual test cases and
the competency of AI models (or humans) across vision, language, and action
domains. Unlike existing metrics that focus solely on models, our approach
allows for fine-grained, difficulty-aware evaluations through competitive
interactions between models and tasks, capturing both the long-tail
distribution of real-world challenges and the competency gap between current
models and full task mastery. We validate the generalizability and robustness
of our system through extensive experiments on multiple established datasets
and models across distinct AGI domains. The resulting rating distributions
offer novel perspectives and interpretable insights into task difficulty, model
progression, and the outstanding challenges that remain on the path to
achieving full AGI task mastery.

</details>

### [600] [Multi-Level Aware Preference Learning: Enhancing RLHF for Complex Multi-Instruction Tasks](https://arxiv.org/abs/2505.12845)
*Ruopei Sun,Jianfeng Cai,Jinhua Zhu,Kangwen Zhao,Dongyun Xue,Wengang Zhou,Li Li,Houqiang Li*

Main category: cs.AI

TLDR: 论文提出了一种多级感知偏好学习（MAPL）框架，通过利用提示输入中的潜在信号和多级偏好差异，提升了AI系统在复杂多指令任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法在复杂多指令任务中表现不足，且依赖高成本的人工标注或可能引入偏见。此外，现有方法忽略了提示输入中的潜在信号和多级偏好差异。

Method: 提出MAPL框架，通过构建多级偏好数据集（包括样本内和样本间偏好差异）并设计相应的训练目标函数，整合到奖励建模和直接偏好优化中。

Result: 在多个基准测试中验证了MAPL框架的有效性，显著提升了多指令任务的表现。

Conclusion: MAPL框架通过利用多级偏好差异和潜在信号，为AI系统在多指令任务中的对齐提供了更高效的解决方案。

Abstract: RLHF has emerged as a predominant approach for aligning artificial
intelligence systems with human preferences, demonstrating exceptional and
measurable efficacy in instruction following tasks; however, it exhibits
insufficient compliance capabilities when confronted with complex
multi-instruction tasks. Conventional approaches rely heavily on human
annotation or more sophisticated large language models, thereby introducing
substantial resource expenditure or potential bias concerns. Meanwhile,
alternative synthetic methods that augment standard preference datasets often
compromise the model's semantic quality. Our research identifies a critical
oversight in existing techniques, which predominantly focus on comparing
responses while neglecting valuable latent signals embedded within prompt
inputs, and which only focus on preference disparities at the intra-sample
level, while neglecting to account for the inter-sample level preference
differentials that exist among preference data. To leverage these previously
neglected indicators, we propose a novel Multi-level Aware Preference Learning
(MAPL) framework, capable of enhancing multi-instruction capabilities.
Specifically, for any given response in original preference data pairs, we
construct varied prompts with a preference relation under different conditions,
in order to learn intra-sample level preference disparities. Furthermore, for
any given original preference pair, we synthesize multi-instruction preference
pairs to capture preference discrepancies at the inter-sample level. Building
on the two datasets constructed above, we consequently devise two sophisticated
training objective functions. Subsequently, our framework integrates seamlessly
into both Reward Modeling and Direct Preference Optimization paradigms. Through
rigorous evaluation across multiple benchmarks, we empirically validate the
efficacy of our framework.

</details>

### [601] [From Grunts to Grammar: Emergent Language from Cooperative Foraging](https://arxiv.org/abs/2505.12872)
*Maytus Piriyajitakonkij,Rujikorn Charakorn,Weicheng Tao,Wei Pan,Mingfei Sun,Cheston Tan,Mengmi Zhang*

Main category: cs.AI

TLDR: 研究通过多智能体觅食游戏模拟语言起源，发现智能体自发发展出具有自然语言特征的通信协议。


<details>
  <summary>Details</summary>
Motivation: 探索语言如何因生态和社会需求而演化，以及其在团队合作中的作用。

Method: 使用端到端深度强化学习，让智能体在部分可观察的网格世界中学习行动和通信策略。

Result: 智能体发展出具有任意性、互换性、位移性、文化传播性和组合性等自然语言特征的通信协议。

Conclusion: 该框架为研究语言如何从部分可观察性、时间推理和合作目标中演化提供了平台。

Abstract: Early cavemen relied on gestures, vocalizations, and simple signals to
coordinate, plan, avoid predators, and share resources. Today, humans
collaborate using complex languages to achieve remarkable results. What drives
this evolution in communication? How does language emerge, adapt, and become
vital for teamwork? Understanding the origins of language remains a challenge.
A leading hypothesis in linguistics and anthropology posits that language
evolved to meet the ecological and social demands of early human cooperation.
Language did not arise in isolation, but through shared survival goals.
Inspired by this view, we investigate the emergence of language in multi-agent
Foraging Games. These environments are designed to reflect the cognitive and
ecological constraints believed to have influenced the evolution of
communication. Agents operate in a shared grid world with only partial
knowledge about other agents and the environment, and must coordinate to
complete games like picking up high-value targets or executing temporally
ordered actions. Using end-to-end deep reinforcement learning, agents learn
both actions and communication strategies from scratch. We find that agents
develop communication protocols with hallmark features of natural language:
arbitrariness, interchangeability, displacement, cultural transmission, and
compositionality. We quantify each property and analyze how different factors,
such as population size and temporal dependencies, shape specific aspects of
the emergent language. Our framework serves as a platform for studying how
language can evolve from partial observability, temporal reasoning, and
cooperative goals in embodied multi-agent settings. We will release all data,
code, and models publicly.

</details>

### [602] [Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective](https://arxiv.org/abs/2505.12886)
*Zhongxiang Sun,Qipeng Wang,Haoyu Wang,Xiao Zhang,Jun Xu*

Main category: cs.AI

TLDR: 论文研究了大型推理模型（LRMs）中的推理幻觉问题，提出了一种量化推理深度的指标（Reasoning Score），并开发了检测框架（RHD）和强化学习算法（GRPO-R）以减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 推理幻觉（逻辑连贯但事实错误的推理）比传统幻觉更难检测且危害更大，需从机制角度研究。

Method: 提出Reasoning Score量化推理深度，分析ReTruthQA数据集，开发RHD检测框架和GRPO-R算法。

Result: RHD在多个领域表现最优，GRPO-R提升了推理质量并降低了幻觉率。

Conclusion: 通过量化推理深度和改进算法，有效检测并减少了推理幻觉。

Abstract: Large Reasoning Models (LRMs) have shown impressive capabilities in
multi-step reasoning tasks. However, alongside these successes, a more
deceptive form of model error has emerged--Reasoning Hallucination--where
logically coherent but factually incorrect reasoning traces lead to persuasive
yet faulty conclusions. Unlike traditional hallucinations, these errors are
embedded within structured reasoning, making them more difficult to detect and
potentially more harmful. In this work, we investigate reasoning hallucinations
from a mechanistic perspective. We propose the Reasoning Score, which
quantifies the depth of reasoning by measuring the divergence between logits
obtained from projecting late layers of LRMs to the vocabulary space,
effectively distinguishing shallow pattern-matching from genuine deep
reasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA
dataset and identify two key reasoning hallucination patterns: early-stage
fluctuation in reasoning depth and incorrect backtracking to flawed prior
steps. These insights motivate our Reasoning Hallucination Detection (RHD)
framework, which achieves state-of-the-art performance across multiple domains.
To mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced
reinforcement learning algorithm that incorporates step-level deep reasoning
rewards via potential-based shaping. Our theoretical analysis establishes
stronger generalization guarantees, and experiments demonstrate improved
reasoning quality and reduced hallucination rates.

</details>

### [603] [TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios](https://arxiv.org/abs/2505.12891)
*Shaohang Wei,Wei Li,Feifan Song,Wen Luo,Tianyi Zhuang,Haochen Tan,Zhijiang Guo,Houfeng Wang*

Main category: cs.AI

TLDR: 论文提出了一个多层次的基准TIME，用于解决大语言模型在现实世界中进行时间推理时的挑战，包括密集时间信息、快速变化的事件动态和复杂的时间依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了时间推理在现实世界中的挑战，如密集时间信息、快速变化的事件动态和复杂的时间依赖关系。

Method: 设计了TIME基准，包含38,522个QA对，涵盖3个层次和11个子任务，并分为3个子数据集（TIME-Wiki、TIME-News、TIME-Dial）。

Result: 通过实验分析了推理模型和非推理模型的表现，总结了测试时间扩展对时间推理能力的影响。

Conclusion: 发布了TIME-Lite子集以促进未来研究，代码和数据集已开源。

Abstract: Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend
the real world. However, existing works neglect the real-world challenges for
temporal reasoning: (1) intensive temporal information, (2) fast-changing event
dynamics, and (3) complex temporal dependencies in social interactions. To
bridge this gap, we propose a multi-level benchmark TIME, designed for temporal
reasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3
levels with 11 fine-grained sub-tasks. This benchmark encompasses 3
sub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,
and TIME-Dial. We conduct extensive experiments on reasoning models and
non-reasoning models. And we conducted an in-depth analysis of temporal
reasoning performance across diverse real-world scenarios and tasks, and
summarized the impact of test-time scaling on temporal reasoning capabilities.
Additionally, we release TIME-Lite, a human-annotated subset to foster future
research and standardized evaluation in temporal reasoning. The code is
available at https://github.com/sylvain-wei/TIME , and the dataset is available
at https://huggingface.co/datasets/SylvainWei/TIME .

</details>

### [604] [The Traitors: Deception and Trust in Multi-Agent Language Model Simulations](https://arxiv.org/abs/2505.12923)
*Pedro M. P. Curvo*

Main category: cs.AI

TLDR: 论文提出了一个名为'The Traitors'的多智能体模拟框架，用于研究大型语言模型（LLM）在不对称信息下的欺骗行为、信任形成和战略沟通。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在需要信任和与人类价值观对齐的角色中日益重要，研究其欺骗行为的时机和原因成为关键。

Method: 基于博弈论、行为经济学和社会认知的框架，开发了评估指标，并实现了一个完全自主的模拟平台，支持异构智能体群体和自适应行为。

Result: 实验发现，高级模型如GPT-4o具有更强的欺骗能力，但对他人谎言的识别能力较弱，表明欺骗技能可能比检测能力发展更快。

Conclusion: 'The Traitors'为研究LLM在复杂社交互动中的行为提供了一个可配置的实验平台，有助于更严谨地研究欺骗机制和AI系统的社会可靠性。

Abstract: As AI systems increasingly assume roles where trust and alignment with human
values are essential, understanding when and why they engage in deception has
become a critical research priority. We introduce The Traitors, a multi-agent
simulation framework inspired by social deduction games, designed to probe
deception, trust formation, and strategic communication among large language
model (LLM) agents under asymmetric information. A minority of agents the
traitors seek to mislead the majority, while the faithful must infer hidden
identities through dialogue and reasoning. Our contributions are: (1) we ground
the environment in formal frameworks from game theory, behavioral economics,
and social cognition; (2) we develop a suite of evaluation metrics capturing
deception success, trust dynamics, and collective inference quality; (3) we
implement a fully autonomous simulation platform where LLMs reason over
persistent memory and evolving social dynamics, with support for heterogeneous
agent populations, specialized traits, and adaptive behaviors. Our initial
experiments across DeepSeek-V3, GPT-4o-mini, and GPT-4o (10 runs per model)
reveal a notable asymmetry: advanced models like GPT-4o demonstrate superior
deceptive capabilities yet exhibit disproportionate vulnerability to others'
falsehoods. This suggests deception skills may scale faster than detection
abilities. Overall, The Traitors provides a focused, configurable testbed for
investigating LLM behavior in socially nuanced interactions. We position this
work as a contribution toward more rigorous research on deception mechanisms,
alignment challenges, and the broader social reliability of AI systems.

</details>

### [605] [Unveiling and Steering Connectome Organization with Interpretable Latent Variables](https://arxiv.org/abs/2505.13011)
*Yubin Li,Xingyu Liu,Guozhang Chen*

Main category: cs.AI

TLDR: 该研究通过结合果蝇连接组子图提取和生成模型，揭示了神经回路低维表示的组织原则，并验证了其功能相关性。


<details>
  <summary>Details</summary>
Motivation: 大脑连接组的高度复杂性源于紧凑的遗传代码，暗示存在低维组织原则。研究旨在揭示这些原则。

Method: 提出一个框架，结合FlyWire果蝇连接组的子图提取和生成模型，生成可解释的低维表示，并通过可解释性模块分析其功能。

Result: 验证了模型的有效性，能够重建图形并可控生成具有预定义属性的连接组子图。

Conclusion: 该研究为理解大脑架构提供了新工具，并为设计仿生人工神经网络提供了潜在途径。

Abstract: The brain's intricate connectome, a blueprint for its function, presents
immense complexity, yet it arises from a compact genetic code, hinting at
underlying low-dimensional organizational principles. This work bridges
connectomics and representation learning to uncover these principles. We
propose a framework that combines subgraph extraction from the Drosophila
connectome, FlyWire, with a generative model to derive interpretable
low-dimensional representations of neural circuitry. Crucially, an
explainability module links these latent dimensions to specific structural
features, offering insights into their functional relevance. We validate our
approach by demonstrating effective graph reconstruction and, significantly,
the ability to manipulate these latent codes to controllably generate
connectome subgraphs with predefined properties. This research offers a novel
tool for understanding brain architecture and a potential avenue for designing
bio-inspired artificial neural networks.

</details>

### [606] [MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO](https://arxiv.org/abs/2505.13031)
*Yicheng Xiao,Lin Song,Yukang Chen,Yingmin Luo,Yuxin Chen,Yukang Gan,Wei Huang,Xiu Li,Xiaojuan Qi,Ying Shan*

Main category: cs.AI

TLDR: MindOmni是一个统一的多模态大语言模型，通过强化学习解决文本到图像系统的多模态输入和复杂推理任务限制。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像系统在多模态输入和复杂推理任务上表现有限，需要更高效的解决方案。

Method: 采用三阶段训练策略：1）设计带解码器扩散模块的统一视觉语言模型；2）用Chain-of-Thought指令数据进行监督微调；3）提出RGPO算法，利用多模态反馈指导策略更新。

Result: MindOmni在理解和生成基准测试中表现优异，尤其在数学推理指令上展示出细粒度推理生成能力。

Conclusion: MindOmni通过强化学习和多模态反馈显著提升了多模态推理能力，代码将开源。

Abstract: Recent text-to-image systems face limitations in handling multimodal inputs
and complex reasoning tasks. We introduce MindOmni, a unified multimodal large
language model that addresses these challenges by incorporating reasoning
generation through reinforcement learning. MindOmni leverages a three-phase
training strategy: i) design of a unified vision language model with a
decoder-only diffusion module, ii) supervised fine-tuning with Chain-of-Thought
(CoT) instruction data, and iii) our proposed Reasoning Generation Policy
Optimization (RGPO) algorithm, utilizing multimodal feedback to effectively
guide policy updates. Experimental results demonstrate that MindOmni
outperforms existing models, achieving impressive performance on both
understanding and generation benchmarks, meanwhile showcasing advanced
fine-grained reasoning generation capabilities, especially with mathematical
reasoning instruction. All codes will be made public at
\href{https://github.com/EasonXiao-888/MindOmni}{https://github.com/EasonXiao-888/MindOmni}.

</details>

### [607] [CAIM: Development and Evaluation of a Cognitive AI Memory Framework for Long-Term Interaction with Intelligent Agents](https://arxiv.org/abs/2505.13044)
*Rebecca Westhäußer,Frederik Berenz,Wolfgang Minker,Sebastian Zepf*

Main category: cs.AI

TLDR: 论文提出了一种名为CAIM的记忆框架，用于提升大型语言模型（LLM）在长期交互中的表现，通过模拟人类认知过程优化记忆建模。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在长期交互中适应性和上下文理解不足的问题，提出基于认知AI原理的记忆框架。

Method: CAIM框架包含三个模块：记忆控制器、记忆检索和后思考，分别负责决策、数据过滤和存储维护。

Result: CAIM在检索准确性、响应正确性、上下文连贯性和记忆存储方面优于基线框架。

Conclusion: CAIM展示了上下文感知能力，有望改善长期人机交互。

Abstract: Large language models (LLMs) have advanced the field of artificial
intelligence (AI) and are a powerful enabler for interactive systems. However,
they still face challenges in long-term interactions that require adaptation
towards the user as well as contextual knowledge and understanding of the
ever-changing environment. To overcome these challenges, holistic memory
modeling is required to efficiently retrieve and store relevant information
across interaction sessions for suitable responses. Cognitive AI, which aims to
simulate the human thought process in a computerized model, highlights
interesting aspects, such as thoughts, memory mechanisms, and decision-making,
that can contribute towards improved memory modeling for LLMs. Inspired by
these cognitive AI principles, we propose our memory framework CAIM. CAIM
consists of three modules: 1.) The Memory Controller as the central decision
unit; 2.) the Memory Retrieval, which filters relevant data for interaction
upon request; and 3.) the Post-Thinking, which maintains the memory storage. We
compare CAIM against existing approaches, focusing on metrics such as retrieval
accuracy, response correctness, contextual coherence, and memory storage. The
results demonstrate that CAIM outperforms baseline frameworks across different
metrics, highlighting its context-awareness and potential to improve long-term
human-AI interactions.

</details>

### [608] [LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs](https://arxiv.org/abs/2505.13098)
*Lars-Peter Meyer,Johannes Frey,Desiree Heim,Felix Brei,Claus Stadler,Kurt Junghanns,Michael Martin*

Main category: cs.AI

TLDR: LLM-KG-Bench框架3.0版用于评估LLMs在知识图谱工程中的表现，支持自动化任务评估，并扩展了对多种模型的支持。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在语义网和知识图谱工程中的能力，并开发自动化评估工具以减少手动检查需求。

Method: 通过LLM-KG-Bench框架3.0版，包含可扩展的任务集和数据集，评估多种LLMs在RDF和SPARQL等任务中的表现。

Result: 生成了包含30多种LLMs的数据集，展示了模型在RDF和SPARQL任务中的能力，并比较了不同序列化任务的性能。

Conclusion: LLM-KG-Bench框架3.0为评估LLMs在知识图谱工程中的能力提供了有效工具，并展示了模型的多样化表现。

Abstract: Current Large Language Models (LLMs) can assist developing program code
beside many other things, but can they support working with Knowledge Graphs
(KGs) as well? Which LLM is offering the best capabilities in the field of
Semantic Web and Knowledge Graph Engineering (KGE)? Is this possible to
determine without checking many answers manually? The LLM-KG-Bench framework in
Version 3.0 is designed to answer these questions. It consists of an extensible
set of tasks for automated evaluation of LLM answers and covers different
aspects of working with semantic technologies. In this paper the LLM-KG-Bench
framework is presented in Version 3 along with a dataset of prompts, answers
and evaluations generated with it and several state-of-the-art LLMs.
Significant enhancements have been made to the framework since its initial
release, including an updated task API that offers greater flexibility in
handling evaluation tasks, revised tasks, and extended support for various open
models through the vllm library, among other improvements. A comprehensive
dataset has been generated using more than 30 contemporary open and proprietary
LLMs, enabling the creation of exemplary model cards that demonstrate the
models' capabilities in working with RDF and SPARQL, as well as comparing their
performance on Turtle and JSON-LD RDF serialization tasks.

</details>

### [609] [Unveil Sources of Uncertainty: Feature Contribution to Conformal Prediction Intervals](https://arxiv.org/abs/2505.13118)
*Marouane Il Idrissi,Agathe Fernandes Machado,Ewen Gallic,Arthur Charpentier*

Main category: cs.AI

TLDR: 提出了一种基于合作博弈论和保形预测的新方法，用于解释机器学习中的预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI框架主要关注平均预测，忽略了预测不确定性，本文旨在填补这一空白。

Method: 通过定义保形预测区间特性（如宽度和边界）作为价值函数的合作博弈，利用Harsanyi分配（特别是比例Shapley值）系统分配预测不确定性。

Result: 实验证明该方法在合成和真实数据集上具有实用性和解释深度。

Conclusion: 结合合作博弈论和保形预测，为高风险机器学习应用提供了理解预测不确定性的工具。

Abstract: Cooperative game theory methods, notably Shapley values, have significantly
enhanced machine learning (ML) interpretability. However, existing explainable
AI (XAI) frameworks mainly attribute average model predictions, overlooking
predictive uncertainty. This work addresses that gap by proposing a novel,
model-agnostic uncertainty attribution (UA) method grounded in conformal
prediction (CP). By defining cooperative games where CP interval
properties-such as width and bounds-serve as value functions, we systematically
attribute predictive uncertainty to input features. Extending beyond the
traditional Shapley values, we use the richer class of Harsanyi allocations,
and in particular the proportional Shapley values, which distribute attribution
proportionally to feature importance. We propose a Monte Carlo approximation
method with robust statistical guarantees to address computational feasibility,
significantly improving runtime efficiency. Our comprehensive experiments on
synthetic benchmarks and real-world datasets demonstrate the practical utility
and interpretative depth of our approach. By combining cooperative game theory
and conformal prediction, we offer a rigorous, flexible toolkit for
understanding and communicating predictive uncertainty in high-stakes ML
applications.

</details>

### [610] [Zero-Shot Iterative Formalization and Planning in Partially Observable Environments](https://arxiv.org/abs/2505.13126)
*Liancheng Gong,Wang Zhu,Jesse Thomason,Li Zhang*

Main category: cs.AI

TLDR: PDDLego+框架通过零样本方式迭代形式化、规划、扩展和精炼PDDL表示，解决了部分可观测环境中的规划问题，显著提升了性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在完全可观测环境中表现良好，但在部分可观测环境中因信息不完整而失效，因此需要一种新方法应对这一挑战。

Method: 提出PDDLego+框架，通过迭代形式化、规划、扩展和精炼PDDL表示，无需依赖现有轨迹。

Result: 在两个文本模拟环境中，PDDLego+表现出优越性能和鲁棒性，且捕获的领域知识可解释并有益于未来任务。

Conclusion: PDDLego+为部分可观测环境中的规划问题提供了高效且可扩展的解决方案。

Abstract: In planning, using LLMs not to predict plans but to formalize an environment
into the Planning Domain Definition Language (PDDL) has been shown to greatly
improve performance and control. While most work focused on fully observable
environments, we tackle the more realistic and challenging partially observable
environments where existing methods are incapacitated by the lack of complete
information. We propose PDDLego+, a framework to iteratively formalize, plan,
grow, and refine PDDL representations in a zero-shot manner, without needing
access to any existing trajectories. On two textual simulated environments, we
show that PDDLego+ not only achieves superior performance, but also shows
robustness against problem complexity. We also show that the domain knowledge
captured after a successful trial is interpretable and benefits future tasks.

</details>

### [611] [Enhancing LLMs for Time Series Forecasting via Structure-Guided Cross-Modal Alignment](https://arxiv.org/abs/2505.13175)
*Siming Sun,Kai Zhang,Xuejun Jiang,Wenchao Meng,Qinmin Yang*

Main category: cs.AI

TLDR: 提出了一种名为SGCMA的结构引导跨模态对齐框架，通过序列级别的结构一致性提升时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了LLMs建模整体序列结构的能力，SGCMA旨在通过结构一致性实现更有效的跨模态对齐。

Method: SGCMA包含结构对齐（通过HMMs和MEMM学习状态转移矩阵）和语义对齐（跨注意力机制加权平均嵌入）。

Result: 在多个基准测试中，SGCMA实现了最先进的性能。

Conclusion: SGCMA为时间序列预测中的跨模态对齐提供了一种新方法。

Abstract: The emerging paradigm of leveraging pretrained large language models (LLMs)
for time series forecasting has predominantly employed linguistic-temporal
modality alignment strategies through token-level or layer-wise feature
mapping. However, these approaches fundamentally neglect a critical insight:
the core competency of LLMs resides not merely in processing localized token
features but in their inherent capacity to model holistic sequence structures.
This paper posits that effective cross-modal alignment necessitates structural
consistency at the sequence level. We propose the Structure-Guided Cross-Modal
Alignment (SGCMA), a framework that fully exploits and aligns the
state-transition graph structures shared by time-series and linguistic data as
sequential modalities, thereby endowing time series with language-like
properties and delivering stronger generalization after modality alignment.
SGCMA consists of two key components, namely Structure Alignment and Semantic
Alignment. In Structure Alignment, a state transition matrix is learned from
text data through Hidden Markov Models (HMMs), and a shallow transformer-based
Maximum Entropy Markov Model (MEMM) receives the hot-start transition matrix
and annotates each temporal patch into state probability, ensuring that the
temporal representation sequence inherits language-like sequential dynamics. In
Semantic Alignment, cross-attention is applied between temporal patches and the
top-k tokens within each state, and the ultimate temporal embeddings are
derived by the expected value of these embeddings using a weighted average
based on state probabilities. Experiments on multiple benchmarks demonstrate
that SGCMA achieves state-of-the-art performance, offering a novel approach to
cross-modal alignment in time series forecasting.

</details>

### [612] [ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and Vision-Language Models](https://arxiv.org/abs/2505.13180)
*Matteo Merler,Nicola Dainese,Minttu Alakuijala,Giovanni Bonetta,Pietro Ferrazzi,Yu Tian,Bernardo Magnini,Pekka Marttinen*

Main category: cs.AI

TLDR: ViPlan是首个开源视觉规划基准，比较了VLM符号规划和直接VLM规划的性能，发现符号规划在Blocksworld中表现更好，而直接规划在家庭机器人任务中更优。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对VLM符号规划和直接VLM规划的系统比较，ViPlan旨在填补这一空白。

Method: ViPlan包含两个领域的任务（Blocksworld和家庭机器人），评估了九种开源VLM家族和部分闭源模型。

Result: 符号规划在Blocksworld中优于直接规划，而直接规划在家庭机器人任务中表现更好；Chain-of-Thought提示对当前VLM帮助有限。

Conclusion: ViPlan为视觉规划提供了标准化评估，揭示了不同方法在不同任务中的适用性。

Abstract: Integrating Large Language Models with symbolic planners is a promising
direction for obtaining verifiable and grounded plans compared to planning in
natural language, with recent works extending this idea to visual domains using
Vision-Language Models (VLMs). However, rigorous comparison between
VLM-grounded symbolic approaches and methods that plan directly with a VLM has
been hindered by a lack of common environments, evaluation protocols and model
coverage. We introduce ViPlan, the first open-source benchmark for Visual
Planning with symbolic predicates and VLMs. ViPlan features a series of
increasingly challenging tasks in two domains: a visual variant of the classic
Blocksworld planning problem and a simulated household robotics environment. We
benchmark nine open-source VLM families across multiple sizes, along with
selected closed models, evaluating both VLM-grounded symbolic planning and
using the models directly to propose actions. We find symbolic planning to
outperform direct VLM planning in Blocksworld, where accurate image grounding
is crucial, whereas the opposite is true in the household robotics tasks, where
commonsense knowledge and the ability to recover from errors are beneficial.
Finally, we show that across most models and methods, there is no significant
benefit to using Chain-of-Thought prompting, suggesting that current VLMs still
struggle with visual reasoning.

</details>

### [613] [Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities](https://arxiv.org/abs/2505.13195)
*Lili Zhang,Haomiaomiao Wang,Long Cheng,Libao Deng,Tomas Ward*

Main category: cs.AI

TLDR: 该论文提出了一个对抗性评估框架，用于测试大型语言模型（LLMs）在交互和对抗条件下的决策能力，揭示了模型的脆弱性和策略适应性不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标主要关注推理准确性或事实正确性，而忽视了LLMs在对抗性操纵和动态环境中的稳健性。

Method: 结合认知心理学和博弈论方法，设计了对抗性评估框架，测试LLMs在双臂老虎机和多轮信任任务中的表现。

Result: 研究发现不同模型在操纵敏感性和策略适应性上存在显著差异，强调了适应性和公平性识别的重要性。

Conclusion: 该研究为诊断LLM决策弱点提供了方法论，为AI对齐和安全研究提供了实用见解。

Abstract: As Large Language Models (LLMs) become increasingly integrated into
real-world decision-making systems, understanding their behavioural
vulnerabilities remains a critical challenge for AI safety and alignment. While
existing evaluation metrics focus primarily on reasoning accuracy or factual
correctness, they often overlook whether LLMs are robust to adversarial
manipulation or capable of using adaptive strategy in dynamic environments.
This paper introduces an adversarial evaluation framework designed to
systematically stress-test the decision-making processes of LLMs under
interactive and adversarial conditions. Drawing on methodologies from cognitive
psychology and game theory, our framework probes how models respond in two
canonical tasks: the two-armed bandit task and the Multi-Round Trust Task.
These tasks capture key aspects of exploration-exploitation trade-offs, social
cooperation, and strategic flexibility. We apply this framework to several
state-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3,
revealing model-specific susceptibilities to manipulation and rigidity in
strategy adaptation. Our findings highlight distinct behavioral patterns across
models and emphasize the importance of adaptability and fairness recognition
for trustworthy AI deployment. Rather than offering a performance benchmark,
this work proposes a methodology for diagnosing decision-making weaknesses in
LLM-based agents, providing actionable insights for alignment and safety
research.

</details>

### [614] [Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis](https://arxiv.org/abs/2505.13227)
*Tianbao Xie,Jiaqi Deng,Xiaochuan Li,Junlin Yang,Haoyuan Wu,Jixuan Chen,Wenjing Hu,Xinyuan Wang,Yuhui Xu,Zekun Wang,Yiheng Xu,Junli Wang,Doyen Sahoo,Tao Yu,Caiming Xiong*

Main category: cs.AI

TLDR: 论文介绍了OSWorld-G和Jedi数据集，用于解决GUI grounding的复杂性，提升模型在多种任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试简化了GUI grounding任务，未能捕捉真实交互的复杂性。

Method: 引入OSWorld-G基准和Jedi数据集，通过多尺度模型训练。

Result: 模型在多个基准上表现优异，并将基础模型的代理能力从5%提升至27%。

Conclusion: 结合专用数据能实现对新界面的组合泛化，所有资源已开源。

Abstract: Graphical user interface (GUI) grounding, the ability to map natural language
instructions to specific actions on graphical user interfaces, remains a
critical bottleneck in computer use agent development. Current benchmarks
oversimplify grounding tasks as short referring expressions, failing to capture
the complexity of real-world interactions that require software commonsense,
layout understanding, and fine-grained manipulation capabilities. To address
these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising
564 finely annotated samples across diverse task types including text matching,
element recognition, layout understanding, and precise manipulation.
Additionally, we synthesize and release the largest computer use grounding
dataset Jedi, which contains 4 million examples through multi-perspective
decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its
effectiveness by outperforming existing approaches on ScreenSpot-v2,
ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved
grounding with Jedi directly enhances agentic capabilities of general
foundation models on complex computer tasks, improving from 5% to 27% on
OSWorld. Through detailed ablation studies, we identify key factors
contributing to grounding performance and verify that combining specialized
data for different interface elements enables compositional generalization to
novel interfaces. All benchmark, data, checkpoints, and code are open-sourced
and available at https://osworld-grounding.github.io.

</details>

### [615] [StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment](https://arxiv.org/abs/2505.13232)
*Younghyun Kim,Jongheon Jeong,Sangkyung Kwak,Kyungmin Lee,Juho Lee,Jinwoo Shin*

Main category: cs.AI

TLDR: 论文提出StarFT框架，通过正则化防止零样本模型在微调时学习虚假特征，提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 零样本模型（如CLIP）在微调下游任务时容易学习虚假特征（如背景或纹理），导致鲁棒性下降。

Method: 提出StarFT框架，引入正则化，将虚假标签注入的输出分布与原始零样本模型对齐，避免学习无关特征。

Result: 在Waterbirds数据集上，StarFT将最差组和平均准确率分别提升14.30%和3.02%。

Conclusion: StarFT有效提升模型鲁棒性，并在零样本分类和组鲁棒性上表现优异。

Abstract: Learning robust representations from data often requires scale, which has led
to the success of recent zero-shot models such as CLIP. However, the obtained
robustness can easily be deteriorated when these models are fine-tuned on other
downstream tasks (e.g., of smaller scales). Previous works often interpret this
phenomenon in the context of domain shift, developing fine-tuning methods that
aim to preserve the original domain as much as possible. However, in a
different context, fine-tuned models with limited data are also prone to
learning features that are spurious to humans, such as background or texture.
In this paper, we propose StarFT (Spurious Textual Alignment Regularization), a
novel framework for fine-tuning zero-shot models to enhance robustness by
preventing them from learning spuriosity. We introduce a regularization that
aligns the output distribution for spuriosity-injected labels with the original
zero-shot model, ensuring that the model is not induced to extract irrelevant
features further from these descriptions.We leverage recent language models to
get such spuriosity-injected labels by generating alternative textual
descriptions that highlight potentially confounding features.Extensive
experiments validate the robust generalization of StarFT and its emerging
properties: zero-shot group robustness and improved zero-shot classification.
Notably, StarFT boosts both worst-group and average accuracy by 14.30% and
3.02%, respectively, in the Waterbirds group shift scenario, where other robust
fine-tuning baselines show even degraded performance.

</details>

### [616] [Agentic Publications: An LLM-Driven Framework for Interactive Scientific Publishing, Supplementing Traditional Papers with AI-Powered Knowledge Systems](https://arxiv.org/abs/2505.13246)
*Roberto Pugliese,George Kourousias,Francesco Venier,Grazia Garlatti Costa*

Main category: cs.AI

TLDR: 论文提出了一种名为'Agentic Publications'的新型LLM驱动框架，旨在通过将论文转化为交互式知识系统来补充传统出版方式。


<details>
  <summary>Details</summary>
Motivation: 科学文献的指数增长使研究人员在复杂的知识环境中导航变得困难，因此需要一种更高效的知识整合与传播方式。

Method: 该框架结合了结构化数据与非结构化内容，通过检索增强生成和多代理验证技术实现。提供人机交互接口，并采用自动验证和透明治理解决伦理问题。

Result: 概念验证展示了多语言交互、API可访问性和结构化知识表示（如向量数据库、知识图谱和验证代理）。

Conclusion: 该框架提升了跨学科科学交流的效率与合作，同时保留了传统出版路径，特别适用于知识整合困难的跨学科领域。

Abstract: The exponential growth of scientific literature presents significant
challenges for researchers navigating the complex knowledge landscape. We
propose "Agentic Publications", a novel LLM-driven framework complementing
traditional publishing by transforming papers into interactive knowledge
systems. Our architecture integrates structured data with unstructured content
through retrieval-augmented generation and multi-agent verification. The
framework offers interfaces for both humans and machines, combining narrative
explanations with machine-readable outputs while addressing ethical
considerations through automated validation and transparent governance. Key
features include continuous knowledge updates, automatic integration of new
findings, and customizable detail levels. Our proof-of-concept demonstrates
multilingual interaction, API accessibility, and structured knowledge
representation through vector databases, knowledge graphs, and verification
agents. This approach enhances scientific communication across disciplines,
improving efficiency and collaboration while preserving traditional publishing
pathways, particularly valuable for interdisciplinary fields where knowledge
integration remains challenging.

</details>

### [617] [Seeing the Unseen: How EMoE Unveils Bias in Text-to-Image Diffusion Models](https://arxiv.org/abs/2505.13273)
*Lucas Berry,Axel Brando,Wei-Di Chang,Juan Camilo Gamboa Higuera,David Meger*

Main category: cs.AI

TLDR: EMoE框架高效估计扩散模型中的认知不确定性，无需额外训练，揭示数据集偏见。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型参数庞大且输入空间复杂，现有方法难以有效估计不确定性。

Method: 提出EMoE框架，利用预训练网络和潜在空间直接估计不确定性。

Result: 在COCO数据集上验证，EMoE显示不确定性与图像质量相关，并揭示数据集偏见。

Conclusion: EMoE为AI生成内容的公平性和问责性提供了有效工具。

Abstract: Estimating uncertainty in text-to-image diffusion models is challenging
because of their large parameter counts (often exceeding 100 million) and
operation in complex, high-dimensional spaces with virtually infinite input
possibilities. In this paper, we propose Epistemic Mixture of Experts (EMoE), a
novel framework for efficiently estimating epistemic uncertainty in diffusion
models. EMoE leverages pre-trained networks without requiring additional
training, enabling direct uncertainty estimation from a prompt. We leverage a
latent space within the diffusion process that captures epistemic uncertainty
better than existing methods. Experimental results on the COCO dataset
demonstrate EMoE's effectiveness, showing a strong correlation between
uncertainty and image quality. Additionally, EMoE identifies under-sampled
languages and regions with higher uncertainty, revealing hidden biases in the
training set. This capability demonstrates the relevance of EMoE as a tool for
addressing fairness and accountability in AI-generated content.

</details>

### [618] [Level Generation with Quantum Reservoir Computing](https://arxiv.org/abs/2505.13287)
*João S. Ferreira,Pierre Fromholz,Hari Shaji,James R. Wootton*

Main category: cs.AI

TLDR: 论文将量子储层计算从音乐生成扩展到《超级马里奥兄弟》关卡生成，并开发了实时生成Roblox关卡的平台。


<details>
  <summary>Details</summary>
Motivation: 探索量子储层计算在游戏关卡生成中的应用潜力。

Method: 改编量子储层计算模型，用于生成游戏关卡，并开发实时生成平台。

Result: 成功生成《超级马里奥兄弟》关卡和Roblox实时关卡，分析了实时生成的限制。

Conclusion: 量子储层计算在游戏内容生成中具有潜力，但实时生成受硬件限制。

Abstract: Reservoir computing is a form of machine learning particularly suited for
time series analysis, including forecasting predictions. We take an
implementation of \emph{quantum} reservoir computing that was initially
designed to generate variants of musical scores and adapt it to create levels
of Super Mario Bros. Motivated by our analysis of these levels, we develop a
new Roblox \textit{obby} where the courses can be generated in real time on
superconducting qubit hardware, and investigate some of the constraints placed
by such real-time generation.

</details>

### [619] [Multi-Armed Bandits Meet Large Language Models](https://arxiv.org/abs/2505.13355)
*Djallel Bouneffouf,Raphael Feraud*

Main category: cs.AI

TLDR: 综述探讨了Bandit算法与LLMs的协同潜力，分别优化彼此性能并推动AI跨学科研究。


<details>
  <summary>Details</summary>
Motivation: 探索Bandit算法与LLMs如何互补，提升决策与自然语言处理能力。

Method: 分析Bandit算法在LLM优化中的应用及LLMs对Bandit算法的改进。

Result: 发现两者结合可增强性能，并提出未来研究方向。

Conclusion: Bandit算法与LLMs的结合为AI创新应用开辟了新路径。

Abstract: Bandit algorithms and Large Language Models (LLMs) have emerged as powerful
tools in artificial intelligence, each addressing distinct yet complementary
challenges in decision-making and natural language processing. This survey
explores the synergistic potential between these two fields, highlighting how
bandit algorithms can enhance the performance of LLMs and how LLMs, in turn,
can provide novel insights for improving bandit-based decision-making. We first
examine the role of bandit algorithms in optimizing LLM fine-tuning, prompt
engineering, and adaptive response generation, focusing on their ability to
balance exploration and exploitation in large-scale learning tasks.
Subsequently, we explore how LLMs can augment bandit algorithms through
advanced contextual understanding, dynamic adaptation, and improved policy
selection using natural language reasoning. By providing a comprehensive review
of existing research and identifying key challenges and opportunities, this
survey aims to bridge the gap between bandit algorithms and LLMs, paving the
way for innovative applications and interdisciplinary research in AI.

</details>

### [620] [Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific Temporal Planning Guidance using Reinforcement Learning](https://arxiv.org/abs/2505.13372)
*Irene Brugnara,Alessandro Valentini,Andrea Micheli*

Main category: cs.AI

TLDR: 本文提出了一种结合强化学习和符号启发式的方法，用于改进时序规划器的性能。通过利用符号启发式信息，解决了无限状态MDP中的截断问题，并学习符号启发式的残差。实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在固定领域和训练问题下，通过强化学习提取启发式，但存在无限状态MDP截断问题。本文旨在利用符号启发式信息改进学习和规划过程。

Method: 1. 提出奖励模式，利用符号启发式解决MDP截断问题；2. 学习符号启发式的残差而非从头学习；3. 结合符号启发式与学习启发式，采用多队列规划方法。

Result: 实验比较了不同方法，展示了各自的优缺点，显著提升了该规划和学习框架的性能。

Conclusion: 结合符号启发式与强化学习的方法有效改进了时序规划器的性能，解决了现有问题，并推动了该领域的发展。

Abstract: Recent work investigated the use of Reinforcement Learning (RL) for the
synthesis of heuristic guidance to improve the performance of temporal planners
when a domain is fixed and a set of training problems (not plans) is given. The
idea is to extract a heuristic from the value function of a particular
(possibly infinite-state) MDP constructed over the training problems.
  In this paper, we propose an evolution of this learning and planning
framework that focuses on exploiting the information provided by symbolic
heuristics during both the RL and planning phases. First, we formalize
different reward schemata for the synthesis and use symbolic heuristics to
mitigate the problems caused by the truncation of episodes needed to deal with
the potentially infinite MDP. Second, we propose learning a residual of an
existing symbolic heuristic, which is a "correction" of the heuristic value,
instead of eagerly learning the whole heuristic from scratch. Finally, we use
the learned heuristic in combination with a symbolic heuristic using a
multiple-queue planning approach to balance systematic search with imperfect
learned information. We experimentally compare all the approaches, highlighting
their strengths and weaknesses and significantly advancing the state of the art
for this planning and learning schema.

</details>

### [621] [CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition](https://arxiv.org/abs/2505.13380)
*Nam V. Nguyen,Huy Nguyen,Quang Pham,Van Nguyen,Savitha Ramasamy,Nhat Ho*

Main category: cs.AI

TLDR: 论文提出了一种新的竞争机制（Competition）用于稀疏专家混合模型（SMoE）的路由过程，解决了传统路由效率低的问题，并通过实验验证了其高效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 稀疏专家混合模型（SMoE）在扩展模型复杂度时面临路由效率低的问题，传统路由方法中专家计算与路由过程脱节。

Method: 提出竞争机制（Competition），将令牌路由至具有最高神经响应的专家，并开发了CompeteSMoE算法，通过路由器学习竞争策略。

Result: 在视觉指令调优和语言预训练任务中，CompeteSMoE表现出高效性、鲁棒性和可扩展性，优于现有SMoE方法。

Conclusion: 竞争机制显著提升了SMoE的路由效率，CompeteSMoE算法在性能和训练开销上均表现出色。

Abstract: Sparse mixture of experts (SMoE) offers an appealing solution to scale up the
model complexity beyond the mean of increasing the network's depth or width.
However, we argue that effective SMoE training remains challenging because of
the suboptimal routing process where experts that perform computation do not
directly contribute to the routing process. In this work, we propose
competition, a novel mechanism to route tokens to experts with the highest
neural response. Theoretically, we show that the competition mechanism enjoys a
better sample efficiency than the traditional softmax routing. Furthermore, we
develop CompeteSMoE, a simple yet effective algorithm to train large language
models by deploying a router to learn the competition policy, thus enjoying
strong performances at a low training overhead. Our extensive empirical
evaluations on both the visual instruction tuning and language pre-training
tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE
compared to state-of-the-art SMoE strategies. We have made the implementation
available at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an
improved version of the previous study at arXiv:2402.02526

</details>

### [622] [Advancing Generalization Across a Variety of Abstract Visual Reasoning Tasks](https://arxiv.org/abs/2505.13391)
*Mikołaj Małkiński,Jacek Mańdziuk*

Main category: cs.AI

TLDR: 论文提出了一种名为PoNG的新神经网络架构，用于提升抽象视觉推理任务中的模型泛化能力，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决抽象视觉推理任务中模型在新测试分布（o.o.d.）下泛化能力不足的问题。

Method: 采用结合组卷积、归一化和并行设计的PoNG架构。

Result: 在Raven渐进矩阵和视觉类比问题等基准测试中，PoNG表现出强大的泛化能力，部分场景优于现有方法。

Conclusion: PoNG架构显著提升了模型在抽象视觉推理任务中的泛化性能。

Abstract: The abstract visual reasoning (AVR) domain presents a diverse suite of
analogy-based tasks devoted to studying model generalization. Recent years have
brought dynamic progress in the field, particularly in i.i.d. scenarios, in
which models are trained and evaluated on the same data distributions.
Nevertheless, o.o.d. setups that assess model generalization to new test
distributions remain challenging even for the most recent models. To advance
generalization in AVR tasks, we present the Pathways of Normalized Group
Convolution model (PoNG), a novel neural architecture that features group
convolution, normalization, and a parallel design. We consider a wide set of
AVR benchmarks, including Raven's Progressive Matrices and visual analogy
problems with both synthetic and real-world images. The experiments demonstrate
strong generalization capabilities of the proposed model, which in several
settings outperforms the existing literature methods.

</details>

### [623] [Robin: A multi-agent system for automating scientific discovery](https://arxiv.org/abs/2505.13400)
*Ali Essam Ghareeb,Benjamin Chang,Ludovico Mitchener,Angela Yiu,Caralyn J. Szostkiewicz,Jon M. Laurent,Muhammed T. Razzak,Andrew D. White,Michaela M. Hinks,Samuel G. Rodriques*

Main category: cs.AI

TLDR: Robin是一个多智能体系统，能够完全自动化科学发现的关键步骤，包括假设生成、实验设计、结果解释和假设更新，并成功发现了一种治疗干性年龄相关性黄斑变性（dAMD）的新方法。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在科学发现中有所应用，但尚未有系统能够完全自动化科学发现的所有关键步骤。Robin旨在填补这一空白，实现半自主的科学发现。

Method: Robin整合了文献搜索智能体和数据分析智能体，能够生成假设、提出实验、解释结果并更新假设，形成迭代的科学发现流程。

Result: Robin成功提出并验证了ripasudil作为治疗dAMD的新候选药物，并通过RNA-seq实验揭示了其作用机制与ABCA1上调相关。

Conclusion: Robin是首个在迭代实验室循环框架内自主发现并验证新治疗候选药物的AI系统，为AI驱动的科学发现树立了新范式。

Abstract: Scientific discovery is driven by the iterative process of background
research, hypothesis generation, experimentation, and data analysis. Despite
recent advancements in applying artificial intelligence to scientific
discovery, no system has yet automated all of these stages in a single
workflow. Here, we introduce Robin, the first multi-agent system capable of
fully automating the key intellectual steps of the scientific process. By
integrating literature search agents with data analysis agents, Robin can
generate hypotheses, propose experiments, interpret experimental results, and
generate updated hypotheses, achieving a semi-autonomous approach to scientific
discovery. By applying this system, we were able to identify a novel treatment
for dry age-related macular degeneration (dAMD), the major cause of blindness
in the developed world. Robin proposed enhancing retinal pigment epithelium
phagocytosis as a therapeutic strategy, and identified and validated a
promising therapeutic candidate, ripasudil. Ripasudil is a clinically-used rho
kinase (ROCK) inhibitor that has never previously been proposed for treating
dAMD. To elucidate the mechanism of ripasudil-induced upregulation of
phagocytosis, Robin then proposed and analyzed a follow-up RNA-seq experiment,
which revealed upregulation of ABCA1, a critical lipid efflux pump and possible
novel target. All hypotheses, experimental plans, data analyses, and data
figures in the main text of this report were produced by Robin. As the first AI
system to autonomously discover and validate a novel therapeutic candidate
within an iterative lab-in-the-loop framework, Robin establishes a new paradigm
for AI-driven scientific discovery.

</details>

### [624] [AutoMathKG: The automated mathematical knowledge graph based on LLM and vector database](https://arxiv.org/abs/2505.13406)
*Rong Bian,Yu Geng,Zijian Yang,Bing Cheng*

Main category: cs.AI

TLDR: AutoMathKG是一个高质量、广覆盖、多维度的数学知识图谱，能够自动更新，解决了现有工作中知识不完整和自动化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有数学知识图谱受限于语料完整性和自动化程度，无法高效整合多源知识。

Method: AutoMathKG将数学视为由定义、定理和问题构成的图，利用ProofWiki、教材、arXiv论文和TheoremQA等数据源，通过LLM增强实体和关系，并构建MathVD向量数据库进行相似实体搜索。

Result: 实验表明，AutoMathKG在MathVD中的可达性查询结果优于五个基线，Math LLM具备强大的数学推理能力。

Conclusion: AutoMathKG在数学知识图谱构建和自动化更新方面表现出色，具有广泛的应用潜力。

Abstract: A mathematical knowledge graph (KG) presents knowledge within the field of
mathematics in a structured manner. Constructing a math KG using natural
language is an essential but challenging task. There are two major limitations
of existing works: first, they are constrained by corpus completeness, often
discarding or manually supplementing incomplete knowledge; second, they
typically fail to fully automate the integration of diverse knowledge sources.
This paper proposes AutoMathKG, a high-quality, wide-coverage, and
multi-dimensional math KG capable of automatic updates. AutoMathKG regards
mathematics as a vast directed graph composed of Definition, Theorem, and
Problem entities, with their reference relationships as edges. It integrates
knowledge from ProofWiki, textbooks, arXiv papers, and TheoremQA, enhancing
entities and relationships with large language models (LLMs) via in-context
learning for data augmentation. To search for similar entities, MathVD, a
vector database, is built through two designed embedding strategies using
SBERT. To automatically update, two mechanisms are proposed. For knowledge
completion mechanism, Math LLM is developed to interact with AutoMathKG,
providing missing proofs or solutions. For knowledge fusion mechanism, MathVD
is used to retrieve similar entities, and LLM is used to determine whether to
merge with a candidate or add as a new entity. A wide range of experiments
demonstrate the advanced performance and broad applicability of the AutoMathKG
system, including superior reachability query results in MathVD compared to
five baselines and robust mathematical reasoning capability in Math LLM.

</details>

### [625] [CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process](https://arxiv.org/abs/2505.13408)
*Jinhe Bi,Danqi Yan,Yifan Wang,Wenke Huang,Haokun Chen,Guancheng Wan,Mang Ye,Xun Xiao,Hinrich Schuetze,Volker Tresp,Yunpu Ma*

Main category: cs.AI

TLDR: 论文提出了一种基于经典力学的CoT-Kinetics能量方程，用于评估大型推理模型（LRM）生成推理轨迹的质量，从而更准确地衡量输出答案的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估LRM输出时，仅关注答案正确性而忽略推理轨迹的合理性，导致对答案可信度的判断不够准确。

Method: 受经典力学启发，提出CoT-Kinetics能量方程，将LRM内部变换层的令牌状态转换过程类比为力学场中的粒子动力学，并生成标量分数评估推理阶段的合理性。

Result: CoT-Kinetics能量方程能够更精确地衡量推理轨迹的合理性，从而提升对LRM输出质量的评估能力。

Conclusion: 该方法为LRM输出质量的评估提供了更细粒度的工具，弥补了现有方法的不足。

Abstract: Recent Large Reasoning Models significantly improve the reasoning ability of
Large Language Models by learning to reason, exhibiting the promising
performance in solving complex tasks. LRMs solve tasks that require complex
reasoning by explicitly generating reasoning trajectories together with
answers. Nevertheless, judging the quality of such an output answer is not easy
because only considering the correctness of the answer is not enough and the
soundness of the reasoning trajectory part matters as well. Logically, if the
soundness of the reasoning part is poor, even if the answer is correct, the
confidence of the derived answer should be low. Existing methods did consider
jointly assessing the overall output answer by taking into account the
reasoning part, however, their capability is still not satisfactory as the
causal relationship of the reasoning to the concluded answer cannot properly
reflected. In this paper, inspired by classical mechanics, we present a novel
approach towards establishing a CoT-Kinetics energy equation. Specifically, our
CoT-Kinetics energy equation formulates the token state transformation process,
which is regulated by LRM internal transformer layers, as like a particle
kinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy
assigns a scalar score to evaluate specifically the soundness of the reasoning
phase, telling how confident the derived answer could be given the evaluated
reasoning. As such, the LRM's overall output quality can be accurately
measured, rather than a coarse judgment (e.g., correct or incorrect) anymore.

</details>

### [626] [MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable Step-Level Supervision](https://arxiv.org/abs/2505.13427)
*Lingxiao Du,Fanqing Meng,Zongkai Liu,Zhixiang Zhou,Ping Luo,Qiaosheng Zhang,Wenqi Shao*

Main category: cs.AI

TLDR: MM-PRM是一个过程奖励模型，通过自动化框架提升多模态推理的逻辑一致性，显著提高了多步推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在复杂多步推理中表现不佳，缺乏对中间推理步骤的细粒度监督。

Method: 提出MM-PRM，利用MM-Policy和MM-K12数据集，通过MCTS生成大量无标注的步骤级注释，训练过程奖励模型。

Result: 在多个基准测试中显著提升性能，验证了软标签、小学习率和路径多样性的有效性。

Conclusion: 过程监督是增强多模态推理系统逻辑鲁棒性的有效工具。

Abstract: While Multimodal Large Language Models (MLLMs) have achieved impressive
progress in vision-language understanding, they still struggle with complex
multi-step reasoning, often producing logically inconsistent or partially
correct solutions. A key limitation lies in the lack of fine-grained
supervision over intermediate reasoning steps. To address this, we propose
MM-PRM, a process reward model trained within a fully automated, scalable
framework. We first build MM-Policy, a strong multimodal model trained on
diverse mathematical reasoning data. Then, we construct MM-K12, a curated
dataset of 10,000 multimodal math problems with verifiable answers, which
serves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based
pipeline, we generate over 700k step-level annotations without human labeling.
The resulting PRM is used to score candidate reasoning paths in the Best-of-N
inference setup and achieves significant improvements across both in-domain
(MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.)
benchmarks. Further analysis confirms the effectiveness of soft labels, smaller
learning rates, and path diversity in optimizing PRM performance. MM-PRM
demonstrates that process supervision is a powerful tool for enhancing the
logical robustness of multimodal reasoning systems. We release all our codes
and data at https://github.com/ModalMinds/MM-PRM.

</details>

### [627] [Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2505.13445)
*Xiaoyuan Liu,Tian Liang,Zhiwei He,Jiahao Xu,Wenxuan Wang,Pinjia He,Zhaopeng Tu,Haitao Mi,Dong Yu*

Main category: cs.AI

TLDR: RISE 是一个新的在线强化学习框架，通过同时训练 LLM 的问题解决和自我验证能力，解决了模型自我反思不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前 LLM 在复杂推理中存在“表面自我反思”问题，无法有效验证自身输出。

Method: RISE 利用可验证奖励机制，在单一强化学习过程中同时优化问题解决和自我验证能力。

Result: 实验表明，RISE 显著提升了问题解决准确性和自我验证能力。

Conclusion: RISE 为开发更鲁棒和自知的推理模型提供了有效路径。

Abstract: Large Language Models (LLMs) show great promise in complex reasoning, with
Reinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement
strategy. However, a prevalent issue is ``superficial self-reflection'', where
models fail to robustly verify their own outputs. We introduce RISE
(Reinforcing Reasoning with Self-Verification), a novel online RL framework
designed to tackle this. RISE explicitly and simultaneously trains an LLM to
improve both its problem-solving and self-verification abilities within a
single, integrated RL process. The core mechanism involves leveraging
verifiable rewards from an outcome verifier to provide on-the-fly feedback for
both solution generation and self-verification tasks. In each iteration, the
model generates solutions, then critiques its own on-policy generated
solutions, with both trajectories contributing to the policy update. Extensive
experiments on diverse mathematical reasoning benchmarks show that RISE
consistently improves model's problem-solving accuracy while concurrently
fostering strong self-verification skills. Our analyses highlight the
advantages of online verification and the benefits of increased verification
compute. Additionally, RISE models exhibit more frequent and accurate
self-verification behaviors during reasoning. These advantages reinforce RISE
as a flexible and effective path towards developing more robust and self-aware
reasoners.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [628] [Improving Open-Set Semantic Segmentation in 3D Point Clouds by Conditional Channel Capacity Maximization: Preliminary Results](https://arxiv.org/abs/2505.11521)
*Wang Fang,Shirin Rahimi,Olivia Bennett,Sophie Carter,Mitra Hassani,Xu Lan,Omid Javadi,Lucas Mitchell*

Main category: cs.CV

TLDR: 提出了一种用于开放集语义分割（O3S）的即插即用框架，通过条件马尔可夫链建模分割流程，并引入条件通道容量最大化（3CM）正则化项，以增强模型对未知类别的识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型在封闭集上表现优异，但难以识别或分割训练集之外的类别，因此需要解决开放集语义分割问题。

Method: 将分割流程建模为条件马尔可夫链，提出3CM正则化项，最大化特征与预测之间的条件互信息。

Result: 实验证明该方法能有效检测未知对象。

Conclusion: 该方法提升了模型对未知类别的分割能力，并展望了动态开放世界适应和信息论高效估计的未来方向。

Abstract: Point-cloud semantic segmentation underpins a wide range of critical
applications. Although recent deep architectures and large-scale datasets have
driven impressive closed-set performance, these models struggle to recognize or
properly segment objects outside their training classes. This gap has sparked
interest in Open-Set Semantic Segmentation (O3S), where models must both
correctly label known categories and detect novel, unseen classes. In this
paper, we propose a plug and play framework for O3S. By modeling the
segmentation pipeline as a conditional Markov chain, we derive a novel
regularizer term dubbed Conditional Channel Capacity Maximization (3CM), that
maximizes the mutual information between features and predictions conditioned
on each class. When incorporated into standard loss functions, 3CM encourages
the encoder to retain richer, label-dependent features, thereby enhancing the
network's ability to distinguish and segment previously unseen categories.
Experimental results demonstrate effectiveness of proposed method on detecting
unseen objects. We further outline future directions for dynamic open-world
adaptation and efficient information-theoretic estimation.

</details>

### [629] [Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis](https://arxiv.org/abs/2505.11581)
*Akarsh Kumar,Jeff Clune,Joel Lehman,Kenneth O. Stanley*

Main category: cs.CV

TLDR: 论文探讨了现代AI中性能提升是否必然带来更好的内部表示，通过比较进化网络和SGD训练网络在生成单张图像任务中的表现，发现两者输出相同但内部表示差异显著。


<details>
  <summary>Details</summary>
Motivation: 挑战性能提升必然带来更好内部表示的观点，探究不同训练方法对网络内部表示的影响。

Method: 比较进化网络和SGD训练网络在生成单张图像任务中的表现，可视化每个隐藏神经元的功能行为。

Result: SGD训练网络表现出分裂纠缠表示（FER），而进化网络接近统一分解表示（UFR）。

Conclusion: 理解和缓解FER可能对表示学习的未来发展至关重要，尤其是对模型的泛化、创造力和持续学习能力。

Abstract: Much of the excitement in modern AI is driven by the observation that scaling
up existing systems leads to better performance. But does better performance
necessarily imply better internal representations? While the representational
optimist assumes it must, this position paper challenges that view. We compare
neural networks evolved through an open-ended search process to networks
trained via conventional stochastic gradient descent (SGD) on the simple task
of generating a single image. This minimal setup offers a unique advantage:
each hidden neuron's full functional behavior can be easily visualized as an
image, thus revealing how the network's output behavior is internally
constructed neuron by neuron. The result is striking: while both networks
produce the same output behavior, their internal representations differ
dramatically. The SGD-trained networks exhibit a form of disorganization that
we term fractured entangled representation (FER). Interestingly, the evolved
networks largely lack FER, even approaching a unified factored representation
(UFR). In large models, FER may be degrading core model capacities like
generalization, creativity, and (continual) learning. Therefore, understanding
and mitigating FER could be critical to the future of representation learning.

</details>

### [630] [Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization](https://arxiv.org/abs/2505.11620)
*Aaron Wilhelm,Nils Napp*

Main category: cs.CV

TLDR: 改进的BoW图像检索系统，用于地面纹理定位，显著提高了全局定位精度和SLAM中的闭环检测性能。


<details>
  <summary>Details</summary>
Motivation: 地面纹理定位是一种低成本、高精度的解决方案，适用于动态环境且无需修改环境。现有BoW系统在精度和速度上仍有提升空间。

Method: 采用近似k均值（AKM）词汇和软分配，利用地面纹理定位的固定方向和尺度约束，分别设计了高精度和高速度版本算法。

Result: 通过消融实验验证了改进效果，在全局定位和闭环检测中均表现出色。

Conclusion: 该方法可直接替代现有BoW系统，显著提升性能。

Abstract: Ground texture localization using a downward-facing camera offers a low-cost,
high-precision localization solution that is robust to dynamic environments and
requires no environmental modification. We present a significantly improved
bag-of-words (BoW) image retrieval system for ground texture localization,
achieving substantially higher accuracy for global localization and higher
precision and recall for loop closure detection in SLAM. Our approach leverages
an approximate $k$-means (AKM) vocabulary with soft assignment, and exploits
the consistent orientation and constant scale constraints inherent to ground
texture localization. Identifying the different needs of global localization
vs. loop closure detection for SLAM, we present both high-accuracy and
high-speed versions of our algorithm. We test the effect of each of our
proposed improvements through an ablation study and demonstrate our method's
effectiveness for both global localization and loop closure detection. With
numerous ground texture localization systems already using BoW, our method can
readily replace other generic BoW systems in their pipeline and immediately
improve their results.

</details>

### [631] [BandRC: Band Shifted Raised Cosine Activated Implicit Neural Representations](https://arxiv.org/abs/2505.11640)
*Pandula Thennakoon,Avishka Ranasinghe,Mario De Silva,Buwaneka Epakanda,Roshan Godaliyadda,Parakrama Ekanayake,Vijitha Herath*

Main category: cs.CV

TLDR: 论文提出了一种新的激活函数BandRC，用于增强隐式神经表示（INRs）的信号表示能力，解决了现有激活函数在频谱偏差、噪声鲁棒性和局部-全局特征捕获等方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有激活函数在INRs中存在频谱偏差、噪声鲁棒性不足、难以同时捕获局部和全局特征等问题，且需要手动调参。

Method: 引入BandRC激活函数，并结合信号提取的深度先验知识，通过任务特定模型调整激活函数。

Result: 在图像重建、去噪、超分辨率、修复和3D形状重建等任务中，BandRC显著优于现有SOTA方法，PSNR提升显著。

Conclusion: BandRC是一种高效的激活函数，能够显著提升INRs在多种计算机视觉任务中的性能。

Abstract: In recent years, implicit neural representations(INRs) have gained popularity
in the computer vision community. This is mainly due to the strong performance
of INRs in many computer vision tasks. These networks can extract a continuous
signal representation given a discrete signal representation. In previous
studies, it has been repeatedly shown that INR performance has a strong
correlation with the activation functions used in its multilayer perceptrons.
Although numerous activation functions have been proposed that are competitive
with one another, they share some common set of challenges such as spectral
bias(Lack of sensitivity to high-frequency content in signals), limited
robustness to signal noise and difficulties in simultaneous capturing both
local and global features. and furthermore, the requirement for manual
parameter tuning. To address these issues, we introduce a novel activation
function, Band Shifted Raised Cosine Activated Implicit Neural Networks
\textbf{(BandRC)} tailored to enhance signal representation capacity further.
We also incorporate deep prior knowledge extracted from the signal to adjust
the activation functions through a task-specific model. Through a mathematical
analysis and a series of experiments which include image reconstruction (with a
+8.93 dB PSNR improvement over the nearest counterpart), denoising (with a
+0.46 dB increase in PSNR), super-resolution (with a +1.03 dB improvement over
the nearest State-Of-The-Art (SOTA) method for 6X super-resolution),
inpainting, and 3D shape reconstruction we demonstrate the dominance of BandRC
over existing state of the art activation functions.

</details>

### [632] [DPSeg: Dual-Prompt Cost Volume Learning for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2505.11676)
*Ziyu Zhao,Xiaoguang Li,Linjia Shi,Nasrin Imanpour,Song Wang*

Main category: cs.CV

TLDR: 论文提出DPSeg框架，通过双提示机制解决开放词汇语义分割中图像与文本嵌入的领域差距问题，并提升小物体和细节的分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练的视觉语言模型（如CLIP）的文本嵌入，但存在图像与文本嵌入的领域差距问题，且缺乏浅层特征指导，影响分割精度。

Method: 提出DPSeg框架，结合双提示成本体积生成、成本体积引导的解码器和语义引导的提示细化策略，利用视觉提示编码器减少领域差距并提供多级特征指导。

Result: 在多个公共数据集上显著优于现有最先进方法。

Conclusion: DPSeg通过双提示机制有效解决了领域差距问题，提升了分割精度，尤其在处理小物体和细节时表现优异。

Abstract: Open-vocabulary semantic segmentation aims to segment images into distinct
semantic regions for both seen and unseen categories at the pixel level.
Current methods utilize text embeddings from pre-trained vision-language models
like CLIP but struggle with the inherent domain gap between image and text
embeddings, even after extensive alignment during training. Additionally,
relying solely on deep text-aligned features limits shallow-level feature
guidance, which is crucial for detecting small objects and fine details,
ultimately reducing segmentation accuracy. To address these limitations, we
propose a dual prompting framework, DPSeg, for this task. Our approach combines
dual-prompt cost volume generation, a cost volume-guided decoder, and a
semantic-guided prompt refinement strategy that leverages our dual prompting
scheme to mitigate alignment issues in visual prompt generation. By
incorporating visual embeddings from a visual prompt encoder, our approach
reduces the domain gap between text and image embeddings while providing
multi-level guidance through shallow features. Extensive experiments
demonstrate that our method significantly outperforms existing state-of-the-art
approaches on multiple public datasets.

</details>

### [633] [LoFT: LoRA-fused Training Dataset Generation with Few-shot Guidance](https://arxiv.org/abs/2505.11703)
*Jae Myung Kim,Stephan Alaniz,Cordelia Schmid,Zeynep Akata*

Main category: cs.CV

TLDR: LoFT是一种新的数据集生成框架，通过微调LoRA权重并融合真实图像特征，生成高保真和多样性的合成数据，显著提升监督学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据方法无法准确捕捉真实数据的分布，缺乏多样性和保真度，影响下游模型训练效果。

Method: LoFT框架通过微调LoRA权重并融合真实图像特征，生成合成数据，提升多样性和保真度。

Result: 实验表明，LoFT生成的合成数据在10个数据集上表现优于其他方法，显著提高模型准确性。

Conclusion: LoFT能生成高保真和多样性的合成数据，有效提升监督学习性能。

Abstract: Despite recent advances in text-to-image generation, using synthetically
generated data seldom brings a significant boost in performance for supervised
learning. Oftentimes, synthetic datasets do not faithfully recreate the data
distribution of real data, i.e., they lack the fidelity or diversity needed for
effective downstream model training. While previous work has employed few-shot
guidance to address this issue, existing methods still fail to capture and
generate features unique to specific real images. In this paper, we introduce a
novel dataset generation framework named LoFT, LoRA-Fused Training-data
Generation with Few-shot Guidance. Our method fine-tunes LoRA weights on
individual real images and fuses them at inference time, producing synthetic
images that combine the features of real images for improved diversity and
fidelity of generated data. We evaluate the synthetic data produced by LoFT on
10 datasets, using 8 to 64 real images per class as guidance and scaling up to
1000 images per class. Our experiments show that training on LoFT-generated
data consistently outperforms other synthetic dataset methods, significantly
increasing accuracy as the dataset size increases. Additionally, our analysis
demonstrates that LoFT generates datasets with high fidelity and sufficient
diversity, which contribute to the performance improvement. The code is
available at https://github.com/ExplainableML/LoFT.

</details>

### [634] [Attend to Not Attended: Structure-then-Detail Token Merging for Post-training DiT Acceleration](https://arxiv.org/abs/2505.11707)
*Haipeng Fang,Sheng Tang,Juan Cao,Enshuo Zhang,Fan Tang,Tong-Yee Lee*

Main category: cs.CV

TLDR: SDTM提出了一种基于扩散模型去噪先验的动态令牌合并方法，显著提升了计算效率且不影响图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有令牌缩减技术忽视了扩散模型的去噪先验，导致加速效果不佳和图像质量下降。

Method: SDTM通过动态视觉令牌合并、压缩比调整和提示重加权，在不同阶段压缩特征冗余。

Result: 实验表明，SDTM实现了1.55倍的加速，且对图像质量影响可忽略。

Conclusion: SDTM是一种高效且通用的方法，可无缝集成到任何DiT架构中。

Abstract: Diffusion transformers have shown exceptional performance in visual
generation but incur high computational costs. Token reduction techniques that
compress models by sharing the denoising process among similar tokens have been
introduced. However, existing approaches neglect the denoising priors of the
diffusion models, leading to suboptimal acceleration and diminished image
quality. This study proposes a novel concept: attend to prune feature
redundancies in areas not attended by the diffusion process. We analyze the
location and degree of feature redundancies based on the structure-then-detail
denoising priors. Subsequently, we introduce SDTM, a structure-then-detail
token merging approach that dynamically compresses feature redundancies.
Specifically, we design dynamic visual token merging, compression ratio
adjusting, and prompt reweighting for different stages. Served in a
post-training way, the proposed method can be integrated seamlessly into any
DiT architecture. Extensive experiments across various backbones, schedulers,
and datasets showcase the superiority of our method, for example, it achieves
1.55 times acceleration with negligible impact on image quality. Project page:
https://github.com/ICTMCG/SDTM.

</details>

### [635] [EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video](https://arxiv.org/abs/2505.11709)
*Ryan Hoque,Peide Huang,David J. Yoon,Mouli Sivapurapu,Jian Zhang*

Main category: cs.CV

TLDR: 论文介绍了EgoDex数据集，用于解决模仿学习中数据稀缺问题，包含829小时的视频和3D手部追踪数据，覆盖194种日常任务。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在操作任务中存在数据稀缺问题，现有数据集缺乏手部姿态标注和操作任务数据。

Method: 使用Apple Vision Pro收集EgoDex数据集，包含大量视频和3D手部追踪数据，并训练模仿学习策略。

Result: EgoDex是目前最大且最多样化的手部操作数据集，支持手部轨迹预测的模仿学习策略。

Conclusion: 发布EgoDex数据集旨在推动机器人、计算机视觉和基础模型的发展。

Abstract: Imitation learning for manipulation has a well-known data scarcity problem.
Unlike natural language and 2D computer vision, there is no Internet-scale
corpus of data for dexterous manipulation. One appealing option is egocentric
human video, a passively scalable data source. However, existing large-scale
datasets such as Ego4D do not have native hand pose annotations and do not
focus on object manipulation. To this end, we use Apple Vision Pro to collect
EgoDex: the largest and most diverse dataset of dexterous human manipulation to
date. EgoDex has 829 hours of egocentric video with paired 3D hand and finger
tracking data collected at the time of recording, where multiple calibrated
cameras and on-device SLAM can be used to precisely track the pose of every
joint of each hand. The dataset covers a wide range of diverse manipulation
behaviors with everyday household objects in 194 different tabletop tasks
ranging from tying shoelaces to folding laundry. Furthermore, we train and
systematically evaluate imitation learning policies for hand trajectory
prediction on the dataset, introducing metrics and benchmarks for measuring
progress in this increasingly important area. By releasing this large-scale
dataset, we hope to push the frontier of robotics, computer vision, and
foundation models.

</details>

### [636] [UGoDIT: Unsupervised Group Deep Image Prior Via Transferable Weights](https://arxiv.org/abs/2505.11720)
*Shijun Liang,Ismail R. Alkhouri,Siddhant Gautam,Qing Qu,Saiprasad Ravishankar*

Main category: cs.CV

TLDR: UGoDIT是一种无监督的Group DIP方法，通过可转移权重在低数据量情况下实现图像重建，优于现有DIP方法，且无需大量干净训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的深度生成模型（如扩散模型）需要大量干净训练数据，而无需训练数据的方法（如DIP）存在噪声过拟合和计算成本高的问题。UGoDIT旨在解决这些问题，适用于仅有少量子采样测量数据的场景。

Method: UGoDIT通过优化共享编码器和M个解耦解码器学习可转移权重。测试时，部分参数固定为学习到的权重，其余参数优化以保持测量一致性。

Result: 在医学（多线圈MRI）和自然图像恢复任务中，UGoDIT相比现有DIP方法加速收敛并显著提升重建质量，性能接近SOTA监督方法。

Conclusion: UGoDIT在低数据量情况下表现出色，无需大量干净训练数据，为图像重建提供了高效解决方案。

Abstract: Recent advances in data-centric deep generative models have led to
significant progress in solving inverse imaging problems. However, these models
(e.g., diffusion models (DMs)) typically require large amounts of fully sampled
(clean) training data, which is often impractical in medical and scientific
settings such as dynamic imaging.
  On the other hand, training-data-free approaches like the Deep Image Prior
(DIP) do not require clean ground-truth images but suffer from noise
overfitting and can be computationally expensive as the network parameters need
to be optimized for each measurement set independently. Moreover, DIP-based
methods often overlook the potential of learning a prior using a small number
of sub-sampled measurements (or degraded images) available during training. In
this paper, we propose UGoDIT, an Unsupervised Group DIP via Transferable
weights, designed for the low-data regime where only a very small number, M, of
sub-sampled measurement vectors are available during training. Our method
learns a set of transferable weights by optimizing a shared encoder and M
disentangled decoders. At test time, we reconstruct the unseen degraded image
using a DIP network, where part of the parameters are fixed to the learned
weights, while the remaining are optimized to enforce measurement consistency.
We evaluate UGoDIT on both medical (multi-coil MRI) and natural (super
resolution and non-linear deblurring) image recovery tasks under various
settings. Compared to recent standalone DIP methods, UGoDIT provides
accelerated convergence and notable improvement in reconstruction quality.
Furthermore, our method achieves performance competitive with SOTA DM-based and
supervised approaches, despite not requiring large amounts of clean training
data.

</details>

### [637] [Semantically-Aware Game Image Quality Assessment](https://arxiv.org/abs/2505.11724)
*Kai Zhu,Vignesh Edithal,Le Zhang,Ilia Blank,Imran Junejo*

Main category: cs.CV

TLDR: 该研究提出了一种针对游戏图形的无参考图像质量评估模型，通过知识蒸馏提取游戏特有失真特征，并结合语义门控动态调整特征权重，显著提升了质量评分的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于游戏图形缺乏参考图像且失真类型独特（如锯齿、纹理模糊等），现有无参考图像质量评估方法难以适用，因此需要专门针对游戏环境的评估模型。

Method: 模型采用知识蒸馏的游戏失真特征提取器（GDFE）检测游戏特有失真，并结合CLIP嵌入的语义门控动态加权特征重要性，训练数据来自不同图形质量预设的游戏画面。

Result: GDFE能有效泛化到训练中未见的中间失真级别，语义门控提升了上下文相关性并降低了预测方差，模型在同类游戏中表现出稳健的质量趋势。

Conclusion: 该研究为游戏图形质量自动评估奠定了基础，推动了无参考图像质量评估方法在游戏领域的进展。

Abstract: Assessing the visual quality of video game graphics presents unique
challenges due to the absence of reference images and the distinct types of
distortions, such as aliasing, texture blur, and geometry level of detail (LOD)
issues, which differ from those in natural images or user-generated content.
Existing no-reference image and video quality assessment (NR-IQA/VQA) methods
fail to generalize to gaming environments as they are primarily designed for
distortions like compression artifacts. This study introduces a
semantically-aware NR-IQA model tailored to gaming. The model employs a
knowledge-distilled Game distortion feature extractor (GDFE) to detect and
quantify game-specific distortions, while integrating semantic gating via CLIP
embeddings to dynamically weight feature importance based on scene content.
Training on gameplay data recorded across graphical quality presets enables the
model to produce quality scores that align with human perception. Our results
demonstrate that the GDFE, trained through knowledge distillation from binary
classifiers, generalizes effectively to intermediate distortion levels unseen
during training. Semantic gating further improves contextual relevance and
reduces prediction variance. In the absence of in-domain NR-IQA baselines, our
model outperforms out-of-domain methods and exhibits robust, monotonic quality
trends across unseen games in the same genre. This work establishes a
foundation for automated graphical quality assessment in gaming, advancing
NR-IQA methods in this domain.

</details>

### [638] [X-Edit: Detecting and Localizing Edits in Images Altered by Text-Guided Diffusion Models](https://arxiv.org/abs/2505.11753)
*Valentina Bazyleva,Nicolo Bonettini,Gaurav Bharaj*

Main category: cs.CV

TLDR: X-Edit是一种新方法，用于定位基于扩散模型的图像编辑区域，通过预训练扩散模型的反演和分割网络实现，结合分割和相关性损失优化，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 文本引导的扩散模型在图像编辑中表现出色，但其恶意使用带来了检测挑战，需要一种方法来定位这些细微的深度伪造编辑。

Method: 通过预训练扩散模型反演图像特征，输入分割网络预测编辑区域，结合分割损失和相关性损失优化模型。

Result: X-Edit在PSNR和SSIM指标上优于基线方法，能准确定位扩散模型编辑的区域。

Conclusion: X-Edit是一种强大的法证工具，可用于检测和定位高级图像编辑技术引入的篡改。

Abstract: Text-guided diffusion models have significantly advanced image editing,
enabling highly realistic and local modifications based on textual prompts.
While these developments expand creative possibilities, their malicious use
poses substantial challenges for detection of such subtle deepfake edits. To
this end, we introduce Explain Edit (X-Edit), a novel method for localizing
diffusion-based edits in images. To localize the edits for an image, we invert
the image using a pretrained diffusion model, then use these inverted features
as input to a segmentation network that explicitly predicts the edited masked
regions via channel and spatial attention. Further, we finetune the model using
a combined segmentation and relevance loss. The segmentation loss ensures
accurate mask prediction by balancing pixel-wise errors and perceptual
similarity, while the relevance loss guides the model to focus on low-frequency
regions and mitigate high-frequency artifacts, enhancing the localization of
subtle edits. To the best of our knowledge, we are the first to address and
model the problem of localizing diffusion-based modified regions in images. We
additionally contribute a new dataset of paired original and edited images
addressing the current lack of resources for this task. Experimental results
demonstrate that X-Edit accurately localizes edits in images altered by
text-guided diffusion models, outperforming baselines in PSNR and SSIM metrics.
This highlights X-Edit's potential as a robust forensic tool for detecting and
pinpointing manipulations introduced by advanced image editing techniques.

</details>

### [639] [Generalizable Vision-Language Few-Shot Adaptation with Predictive Prompts and Negative Learning](https://arxiv.org/abs/2505.11758)
*Sriram Mandalika*

Main category: cs.CV

TLDR: PromptFuseNL是一个统一框架，通过结合预测性提示调优和双分支正负学习，提升视觉语言模型在少样本场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决少样本适应中的监督不足和噪声支持样本问题。

Method: 结合任务条件残差、多阶段跨模态协调和语义硬负样本挖掘，并通过无监督实例重加权策略处理标签噪声。

Result: 在15个基准测试中表现优于现有方法，训练速度提升300倍，计算量降低1000倍。

Conclusion: PromptFuseNL为少样本视觉语言适应提供了高效且鲁棒的解决方案，达到新SOTA。

Abstract: Few-shot adaptation remains a core challenge for vision-language models
(VLMs), especially under limited supervision and noisy support samples. We
propose PromptFuseNL, a unified framework that enhances few-shot generalization
by combining predictive prompt tuning with dual-branch positive and negative
learning. The method refines class prototypes through task-conditioned
residuals, multi-stage cross-modal coordination, and semantic hard negative
mining. To address label noise, we introduce an unsupervised instance
reweighting strategy that downweights unreliable support examples without
requiring additional labels or structural changes. PromptFuseNL fuses visual
and textual cues through lightweight modules for efficient and discriminative
prediction. Evaluated across 15 benchmarks, it consistently surpasses existing
prompt- and adapter-based methods in all shot settings while remaining highly
efficient, achieving up to 300x faster training and 1000x lower FLOPs compared
to full prompt tuning, achieving a new state-of-the-art for robust and scalable
few-shot vision-language adaptation.

</details>

### [640] [Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Boosting Off-Road Segmentation via Photometric Distortion and Exponential Moving Average](https://arxiv.org/abs/2505.11769)
*Wonjune Kim,Lae-kyoung Lee,Su-Yong An*

Main category: cs.CV

TLDR: 论文提出了一种基于FlashInternImage-B和UPerNet的高容量语义分割方法，用于GOOSE 2D挑战赛的非结构化越野场景，通过数据增强和EMA权重优化，验证集mIoU达88.8%。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化越野场景的语义分割问题，适应其独特的光照和地形变化。

Method: 采用FlashInternImage-B主干网络和UPerNet解码器，结合强光增强和EMA权重优化。

Result: 在GOOSE训练集上，验证集mIoU达到88.8%。

Conclusion: 通过现有技术优化，成功应用于越野场景，表现优异。

Abstract: We report on the application of a high-capacity semantic segmentation
pipeline to the GOOSE 2D Semantic Segmentation Challenge for unstructured
off-road environments. Using a FlashInternImage-B backbone together with a
UPerNet decoder, we adapt established techniques, rather than designing new
ones, to the distinctive conditions of off-road scenes. Our training recipe
couples strong photometric distortion augmentation (to emulate the wide
lighting variations of outdoor terrain) with an Exponential Moving Average
(EMA) of weights for better generalization. Using only the GOOSE training
dataset, we achieve 88.8\% mIoU on the validation set.

</details>

### [641] [Self-NPO: Negative Preference Optimization of Diffusion Models by Simply Learning from Itself without Explicit Preference Annotations](https://arxiv.org/abs/2505.11777)
*Fu-Yun Wang,Keqiang Sun,Yao Teng,Xihui Liu,Jiaming Song,Hongsheng Li*

Main category: cs.CV

TLDR: Self-NPO是一种无需人工标注或奖励模型训练的负偏好优化方法，通过从模型自身学习，显著提升了扩散模型的生成质量和人类偏好对齐。


<details>
  <summary>Details</summary>
Motivation: 现有负偏好优化方法依赖昂贵且脆弱的显式偏好标注，限制了其在实际应用中的可行性。Self-NPO旨在解决这一问题。

Method: Self-NPO通过从模型自身学习负偏好，无需人工标注或奖励模型训练，且高效无需大量数据采样。

Result: Self-NPO成功集成到多种扩散模型中，显著提升了生成质量和人类偏好对齐。

Conclusion: Self-NPO为负偏好优化提供了一种高效且实用的解决方案，适用于数据稀缺或难以获取的领域。

Abstract: Diffusion models have demonstrated remarkable success in various visual
generation tasks, including image, video, and 3D content generation. Preference
optimization (PO) is a prominent and growing area of research that aims to
align these models with human preferences. While existing PO methods primarily
concentrate on producing favorable outputs, they often overlook the
significance of classifier-free guidance (CFG) in mitigating undesirable
results. Diffusion-NPO addresses this gap by introducing negative preference
optimization (NPO), training models to generate outputs opposite to human
preferences and thereby steering them away from unfavorable outcomes. However,
prior NPO approaches, including Diffusion-NPO, rely on costly and fragile
procedures for obtaining explicit preference annotations (e.g., manual pairwise
labeling or reward model training), limiting their practicality in domains
where such data are scarce or difficult to acquire. In this work, we introduce
Self-NPO, a Negative Preference Optimization approach that learns exclusively
from the model itself, thereby eliminating the need for manual data labeling or
reward model training. Moreover, our method is highly efficient and does not
require exhaustive data sampling. We demonstrate that Self-NPO integrates
seamlessly into widely used diffusion models, including SD1.5, SDXL, and
CogVideoX, as well as models already optimized for human preferences,
consistently enhancing both their generation quality and alignment with human
preferences.

</details>

### [642] [CL-CaGAN: Capsule differential adversarial continuous learning for cross-domain hyperspectral anomaly detection](https://arxiv.org/abs/2505.11793)
*Jianing Wang,Siying Guo,Zheng Hua,Runhu Huang,Jinyu Hu,Maoguo Gong*

Main category: cs.CV

TLDR: 提出了一种基于持续学习的胶囊差分生成对抗网络（CL-CaGAN），用于提升跨场景学习性能，解决高光谱异常检测中的先验信息不足和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在开放跨域场景中面临先验信息不足和灾难性遗忘的挑战，限制了其在高光谱异常检测（HAD）中的实际应用。

Method: 构建了改进的胶囊结构与对抗学习网络，结合聚类样本回放策略和自蒸馏正则化，通过持续学习策略保留历史与未来知识。

Result: 实验表明，CL-CaGAN在多个真实高光谱图像上表现出更高的检测性能和持续学习能力，有效缓解了跨域场景下的灾难性遗忘问题。

Conclusion: CL-CaGAN为高光谱异常检测提供了一种有效的持续学习方法，显著提升了跨场景检测性能。

Abstract: Anomaly detection (AD) has attracted remarkable attention in hyperspectral
image (HSI) processing fields, and most existing deep learning (DL)-based
algorithms indicate dramatic potential for detecting anomaly samples through
specific training process under current scenario. However, the limited prior
information and the catastrophic forgetting problem indicate crucial challenges
for existing DL structure in open scenarios cross-domain detection. In order to
improve the detection performance, a novel continual learning-based capsule
differential generative adversarial network (CL-CaGAN) is proposed to elevate
the cross-scenario learning performance for facilitating the real application
of DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule
structure with adversarial learning network is constructed to estimate the
background distribution for surmounting the deficiency of prior information. To
mitigate the catastrophic forgetting phenomenon, clustering-based sample replay
strategy and a designed extra self-distillation regularization are integrated
for merging the history and future knowledge in continual AD task, while the
discriminative learning ability from previous detection scenario to current
scenario is retained by the elaborately designed structure with continual
learning (CL) strategy. In addition, the differentiable enhancement is enforced
to augment the generation performance of the training data. This further
stabilizes the training process with better convergence and efficiently
consolidates the reconstruction ability of background samples. To verify the
effectiveness of our proposed CL-CaGAN, we conduct experiments on several real
HSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher
detection performance and continuous learning capacity for mitigating the
catastrophic forgetting under cross-domain scenarios.

</details>

### [643] [CL-BioGAN: Biologically-Inspired Cross-Domain Continual Learning for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2505.11796)
*Jianing Wang,Zheng Hua,Wan Zhang,Shengjia Hao,Yuqiong Yao,Maoguo Gong*

Main category: cs.CV

TLDR: 论文提出了一种受生物启发的持续学习生成对抗网络（CL-BioGAN），用于跨场景高光谱异常检测（HAD）任务，通过主动遗忘历史知识和引入回放策略，实现了更好的分布拟合能力。


<details>
  <summary>Details</summary>
Motivation: 生物神经网络通过调节学习触发的突触扩展和收敛，能够主动遗忘与新经验冲突的历史知识。受此启发，论文旨在解决持续学习中记忆稳定性和学习灵活性的核心挑战。

Method: 提出了CL-BioGAN，结合了持续学习生物启发损失（CL-Bio Loss）和自注意力生成对抗网络（BioGAN），设计了主动遗忘损失（AF Loss）和CL损失，从贝叶斯角度实现参数释放与增强。

Result: 实验表明，CL-BioGAN在跨域HAD任务中能以更少的参数和计算成本实现更鲁棒的准确性。

Conclusion: 该方法不仅提升了持续学习性能，还为开放场景HAD任务中的神经适应机制提供了新见解。

Abstract: Memory stability and learning flexibility in continual learning (CL) is a
core challenge for cross-scene Hyperspectral Anomaly Detection (HAD) task.
Biological neural networks can actively forget history knowledge that conflicts
with the learning of new experiences by regulating learning-triggered synaptic
expansion and synaptic convergence. Inspired by this phenomenon, we propose a
novel Biologically-Inspired Continual Learning Generative Adversarial Network
(CL-BioGAN) for augmenting continuous distribution fitting ability for
cross-domain HAD task, where Continual Learning Bio-inspired Loss (CL-Bio Loss)
and self-attention Generative Adversarial Network (BioGAN) are incorporated to
realize forgetting history knowledge as well as involving replay strategy in
the proposed BioGAN. Specifically, a novel Bio-Inspired Loss composed with an
Active Forgetting Loss (AF Loss) and a CL loss is designed to realize
parameters releasing and enhancing between new task and history tasks from a
Bayesian perspective. Meanwhile, BioGAN loss with L2-Norm enhances
self-attention (SA) to further balance the stability and flexibility for better
fitting background distribution for open scenario HAD (OHAD) tasks. Experiment
results underscore that the proposed CL-BioGAN can achieve more robust and
satisfying accuracy for cross-domain HAD with fewer parameters and computation
cost. This dual contribution not only elevates CL performance but also offers
new insights into neural adaptation mechanisms in OHAD task.

</details>

### [644] [Self-Learning Hyperspectral and Multispectral Image Fusion via Adaptive Residual Guided Subspace Diffusion Model](https://arxiv.org/abs/2505.11800)
*Jian Zhu,He Wang,Yang Xu,Zebin Wu,Zhihui Wei*

Main category: cs.CV

TLDR: 提出了一种自学习的自适应残差引导子空间扩散模型（ARGS-Diff），仅利用观测图像无需额外训练数据，实现了高分辨率高光谱图像（HR-HSI）的生成。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖大量高光谱数据进行监督训练，但实际应用中数据稀缺。

Method: 设计了轻量级的谱和空间扩散模型，分别学习谱和空间分布，并通过自适应残差引导模块（ARGM）优化重建过程。

Result: 实验表明，ARGS-Diff在性能和计算效率上优于现有方法。

Conclusion: ARGS-Diff为HSI-MSI融合提供了一种高效且无需额外训练数据的解决方案。

Abstract: Hyperspectral and multispectral image (HSI-MSI) fusion involves combining a
low-resolution hyperspectral image (LR-HSI) with a high-resolution
multispectral image (HR-MSI) to generate a high-resolution hyperspectral image
(HR-HSI). Most deep learning-based methods for HSI-MSI fusion rely on large
amounts of hyperspectral data for supervised training, which is often scarce in
practical applications. In this paper, we propose a self-learning Adaptive
Residual Guided Subspace Diffusion Model (ARGS-Diff), which only utilizes the
observed images without any extra training data. Specifically, as the LR-HSI
contains spectral information and the HR-MSI contains spatial information, we
design two lightweight spectral and spatial diffusion models to separately
learn the spectral and spatial distributions from them. Then, we use these two
models to reconstruct HR-HSI from two low-dimensional components, i.e, the
spectral basis and the reduced coefficient, during the reverse diffusion
process. Furthermore, we introduce an Adaptive Residual Guided Module (ARGM),
which refines the two components through a residual guided function at each
sampling step, thereby stabilizing the sampling process. Extensive experimental
results demonstrate that ARGS-Diff outperforms existing state-of-the-art
methods in terms of both performance and computational efficiency in the field
of HSI-MSI fusion. Code is available at https://github.com/Zhu1116/ARGS-Diff.

</details>

### [645] [Are vision language models robust to uncertain inputs?](https://arxiv.org/abs/2505.11804)
*Xi Wang,Eric Nalisnick*

Main category: cs.CV

TLDR: 论文探讨了大规模视觉语言模型（VLMs）在面对不确定和模糊输入时的鲁棒性问题，发现虽然模型规模增大能提升性能，但仍存在过度自信的问题。通过简单提示模型避免不确定预测，可以显著提升可靠性，但领域特定任务仍需改进。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决深度学习模型在面对不确定和模糊输入时的鲁棒性问题，尤其是大规模视觉语言模型的局限性。

Method: 通过异常检测和模糊分类任务评估模型性能，并提出基于提示的改进方法及通过标题多样性揭示模型内部不确定性的新机制。

Result: 新的大规模VLMs在鲁棒性上优于早期模型，但仍存在过度自信问题；简单提示可显著提升可靠性，但领域特定任务效果有限。

Conclusion: 模型规模增大有助于鲁棒性，但需结合提示机制和领域知识改进；新提出的标题多样性机制为无标签数据下的不确定性预测提供了可行方案。

Abstract: Robustness against uncertain and ambiguous inputs is a critical challenge for
deep learning models. While recent advancements in large scale vision language
models (VLMs, e.g. GPT4o) might suggest that increasing model and training
dataset size would mitigate this issue, our empirical evaluation shows a more
complicated picture. Testing models using two classic uncertainty
quantification tasks, anomaly detection and classification under inherently
ambiguous conditions, we find that newer and larger VLMs indeed exhibit
improved robustness compared to earlier models, but still suffer from a
tendency to strictly follow instructions, often causing them to hallucinate
confident responses even when faced with unclear or anomalous inputs.
Remarkably, for natural images such as ImageNet, this limitation can be
overcome without pipeline modifications: simply prompting models to abstain
from uncertain predictions enables significant reliability gains, achieving
near-perfect robustness in several settings. However, for domain-specific tasks
such as galaxy morphology classification, a lack of specialized knowledge
prevents reliable uncertainty estimation. Finally, we propose a novel mechanism
based on caption diversity to reveal a model's internal uncertainty, enabling
practitioners to predict when models will successfully abstain without relying
on labeled data.

</details>

### [646] [Image-based Visibility Analysis Replacing Line-of-Sight Simulation: An Urban Landmark Perspective](https://arxiv.org/abs/2505.11809)
*Zicheng Fan,Kunihiko Fujiwara,Pengyuan Liu,Fan Zhang,Filip Biljecki*

Main category: cs.CV

TLDR: 论文提出了一种基于图像的新方法，利用视觉语言模型（VLM）分析城市地标的可见性，补充了传统的视线（LoS）方法，并展示了在城市规划和景观研究中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统基于几何交点的视线（LoS）方法无法捕捉城市地标在真实世界中的上下文和感知维度，因此需要一种更全面的可见性分析方法。

Method: 应用视觉语言模型（VLM）在街景图像（SVI）中检测目标地标，构建异质可见性图以分析观察者与目标对象的复杂互动。

Result: 在第一个案例中，方法对全球六个高大地标的可见性检测准确率达87%；第二个案例揭示了伦敦泰晤士河沿岸地标的连接形式和强度，桥梁占总连接的30%。

Conclusion: 该方法补充并增强了传统LoS分析，为城市规划、遗产保护和社会计算提供了新的研究视角。

Abstract: Visibility analysis is one of the fundamental analytics methods in urban
planning and landscape research, traditionally conducted through computational
simulations based on the Line-of-Sight (LoS) principle. However, when assessing
the visibility of named urban objects such as landmarks, geometric intersection
alone fails to capture the contextual and perceptual dimensions of visibility
as experienced in the real world. The study challenges the traditional
LoS-based approaches by introducing a new, image-based visibility analysis
method. Specifically, a Vision Language Model (VLM) is applied to detect the
target object within a direction-zoomed Street View Image (SVI). Successful
detection represents the object's visibility at the corresponding SVI location.
Further, a heterogeneous visibility graph is constructed to address the complex
interaction between observers and target objects. In the first case study, the
method proves its reliability in detecting the visibility of six tall landmark
constructions in global cities, with an overall accuracy of 87%. Furthermore,
it reveals broader contextual differences when the landmarks are perceived and
experienced. In the second case, the proposed visibility graph uncovers the
form and strength of connections for multiple landmarks along the River Thames
in London, as well as the places where these connections occur. Notably,
bridges on the River Thames account for approximately 30% of total connections.
Our method complements and enhances traditional LoS-based visibility analysis,
and showcases the possibility of revealing the prevalent connection of any
visual objects in the urban environment. It opens up new research perspectives
for urban planning, heritage conservation, and computational social science.

</details>

### [647] [SGD-Mix: Enhancing Domain-Specific Image Classification with Label-Preserving Data Augmentation](https://arxiv.org/abs/2505.11813)
*Yixuan Dong,Fang-Yi Su,Jung-Hsien Chiang*

Main category: cs.CV

TLDR: 本文提出了一种新的数据增强框架，通过显著性引导混合和微调扩散模型，解决了多样性、忠实性和标签清晰度的问题，并在多个任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的数据增强方法未能同时解决多样性、忠实性和标签清晰度的问题，且忽略了扩散模型的固有挑战。

Method: 采用显著性引导混合和微调扩散模型，以保留前景语义、丰富背景多样性并确保标签一致性。

Result: 在细粒度、长尾、少样本和背景鲁棒性任务中，该方法表现优于现有技术。

Conclusion: 提出的框架有效整合了多样性、忠实性和标签清晰度，显著提升了数据增强的性能。

Abstract: Data augmentation for domain-specific image classification tasks often
struggles to simultaneously address diversity, faithfulness, and label clarity
of generated data, leading to suboptimal performance in downstream tasks. While
existing generative diffusion model-based methods aim to enhance augmentation,
they fail to cohesively tackle these three critical aspects and often overlook
intrinsic challenges of diffusion models, such as sensitivity to model
characteristics and stochasticity under strong transformations. In this paper,
we propose a novel framework that explicitly integrates diversity,
faithfulness, and label clarity into the augmentation process. Our approach
employs saliency-guided mixing and a fine-tuned diffusion model to preserve
foreground semantics, enrich background diversity, and ensure label
consistency, while mitigating diffusion model limitations. Extensive
experiments across fine-grained, long-tail, few-shot, and background robustness
tasks demonstrate our method's superior performance over state-of-the-art
approaches.

</details>

### [648] [UniMoCo: Unified Modality Completion for Robust Multi-Modal Embeddings](https://arxiv.org/abs/2505.11815)
*Jiajun Qin,Yuan Pu,Zhuolun He,Seunggeun Kim,David Z. Pan,Bei Yu*

Main category: cs.CV

TLDR: UniMoCo是一种新型视觉-语言模型，通过模态补全模块和专用训练策略，解决了多模态嵌入任务中模态组合多样性的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以在统一嵌入空间中对齐多样化的模态组合（如文本到图像、文本到文本和图像等），导致推理性能下降。

Method: 提出UniMoCo架构，包含模态补全模块（从文本生成视觉特征）和专用训练策略，确保嵌入空间的一致性。

Result: 实验表明，UniMoCo优于现有方法，并在多样化设置中表现稳健，同时缓解了传统方法因训练数据模态不平衡导致的偏差。

Conclusion: UniMoCo通过模态补全和一致性训练策略，显著提升了多模态嵌入任务的性能，并解决了模态不平衡问题。

Abstract: Current research has explored vision-language models for multi-modal
embedding tasks, such as information retrieval, visual grounding, and
classification. However, real-world scenarios often involve diverse modality
combinations between queries and targets, such as text and image to text, text
and image to text and image, and text to text and image. These diverse
combinations pose significant challenges for existing models, as they struggle
to align all modality combinations within a unified embedding space during
training, which degrades performance at inference. To address this limitation,
we propose UniMoCo, a novel vision-language model architecture designed for
multi-modal embedding tasks. UniMoCo introduces a modality-completion module
that generates visual features from textual inputs, ensuring modality
completeness for both queries and targets. Additionally, we develop a
specialized training strategy to align embeddings from both original and
modality-completed inputs, ensuring consistency within the embedding space.
This enables the model to robustly handle a wide range of modality combinations
across embedding tasks. Experiments show that UniMoCo outperforms previous
methods while demonstrating consistent robustness across diverse settings. More
importantly, we identify and quantify the inherent bias in conventional
approaches caused by imbalance of modality combinations in training data, which
can be mitigated through our modality-completion paradigm. The code is
available at https://github.com/HobbitQia/UniMoCo.

</details>

### [649] [Continuous Subspace Optimization for Continual Learning](https://arxiv.org/abs/2505.11816)
*Quan Cheng,Yuanyu Wan,Lingyu Wu,Chenping Hou,Lijun Zhang*

Main category: cs.CV

TLDR: CoSO提出了一种动态子空间优化方法，通过梯度奇异值分解确定子空间，解决了持续学习中固定低秩子空间限制学习能力的问题。


<details>
  <summary>Details</summary>
Motivation: 持续学习中预训练模型通常采用低秩适应，但固定子空间限制了模型的学习能力，导致性能下降。

Method: CoSO通过动态确定子空间并投影梯度进行优化，同时保持任务子空间正交性以减少遗忘。

Result: 在多个数据集上，CoSO显著优于现有方法，尤其在长任务序列场景中表现突出。

Conclusion: CoSO通过动态子空间优化有效提升了持续学习的性能，解决了低秩适应的局限性。

Abstract: Continual learning aims to learn multiple tasks sequentially while preserving
prior knowledge, but faces the challenge of catastrophic forgetting when
acquiring new knowledge. Recently, approaches leveraging pre-trained models
have gained increasing popularity to mitigate this issue, due to the strong
generalization ability of foundation models. To adjust pre-trained models for
new tasks, existing methods usually employ low-rank adaptation, which restricts
parameter updates to a fixed low-rank subspace. However, constraining the
optimization space inherently compromises the model's learning capacity,
resulting in inferior performance. To address the limitation, we propose
Continuous Subspace Optimization for Continual Learning (CoSO) to fine-tune the
model in a series of subspaces rather than a single one. These sequential
subspaces are dynamically determined through the singular value decomposition
of gradients. CoSO updates the model by projecting gradients into these
subspaces, ensuring memory-efficient optimization. To mitigate forgetting, the
optimization subspaces of each task are set to be orthogonal to the historical
task subspace. During task learning, CoSO maintains a task-specific component
that captures the critical update directions associated with the current task.
Upon completing a task, this component is used to update the historical task
subspace, laying the groundwork for subsequent learning. Extensive experiments
on multiple datasets demonstrate that CoSO significantly outperforms
state-of-the-art methods, especially in challenging scenarios with long task
sequences.

</details>

### [650] [Robust Cross-View Geo-Localization via Content-Viewpoint Disentanglement](https://arxiv.org/abs/2505.11822)
*Ke Li,Di Wang,Xiaowei Wang,Zhihong Wu,Yiming Zhang,Yifeng Wang,Quan Wang*

Main category: cs.CV

TLDR: 论文提出了一种名为CVD的新框架，通过解耦内容和视角因素来改进跨视角地理定位任务，显著提升了定位精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 跨视角地理定位（CVGL）因视角变化导致的外观和空间差异而极具挑战性，现有方法未能有效解决视角差异带来的特征不一致问题。

Method: 提出CVD框架，通过内容与视角解耦，引入两种约束：1）视图内独立性约束，最小化互信息；2）视图间重建约束，确保语义保留。

Result: 在四个基准数据集上的实验表明，CVD显著提升了定位精度和泛化能力。

Conclusion: CVD通过解耦内容和视角因素，有效解决了跨视角地理定位中的特征不一致问题，具有广泛适用性。

Abstract: Cross-view geo-localization (CVGL) aims to match images of the same
geographic location captured from different perspectives, such as drones and
satellites. Despite recent advances, CVGL remains highly challenging due to
significant appearance changes and spatial distortions caused by viewpoint
variations. Existing methods typically assume that cross-view images can be
directly aligned within a shared feature space by maximizing feature similarity
through contrastive learning. Nonetheless, this assumption overlooks the
inherent conflicts induced by viewpoint discrepancies, resulting in extracted
features containing inconsistent information that hinders precise localization.
In this study, we take a manifold learning perspective and model the feature
space of cross-view images as a composite manifold jointly governed by content
and viewpoint information. Building upon this insight, we propose
$\textbf{CVD}$, a new CVGL framework that explicitly disentangles
$\textit{content}$ and $\textit{viewpoint}$ factors. To promote effective
disentanglement, we introduce two constraints: $\textit{(i)}$ An intra-view
independence constraint, which encourages statistical independence between the
two factors by minimizing their mutual information. $\textit{(ii)}$ An
inter-view reconstruction constraint that reconstructs each view by
cross-combining $\textit{content}$ and $\textit{viewpoint}$ from paired images,
ensuring factor-specific semantics are preserved. As a plug-and-play module,
CVD can be seamlessly integrated into existing geo-localization pipelines.
Extensive experiments on four benchmarks, i.e., University-1652, SUES-200,
CVUSA, and CVACT, demonstrate that CVD consistently improves both localization
accuracy and generalization across multiple baselines.

</details>

### [651] [Bootstrapping Diffusion: Diffusion Model Training Leveraging Partial and Corrupted Data](https://arxiv.org/abs/2505.11825)
*Xudong Ma*

Main category: cs.CV

TLDR: 论文提出了一种利用部分数据（如低分辨率图像、短视频等）训练扩散模型的方法，通过训练残差评分函数提高数据效率。


<details>
  <summary>Details</summary>
Motivation: 获取大规模高质量数据困难，而部分数据（如低分辨率图像、带水印视频）通常被视为损坏或残缺。研究探讨了是否可以利用这些部分数据训练扩散模型。

Method: 为每种部分数据训练单独的扩散模型，再训练残差评分函数预测模型，并采用正则化降低泛化误差。

Result: 理论证明，该方法在残差评分函数训练中采用适当正则化时，可降低泛化误差，且数据效率接近一阶最优。

Conclusion: 通过利用部分数据训练扩散模型，该方法在数据效率上表现优异，适用于数据获取受限的场景。

Abstract: Training diffusion models requires large datasets. However, acquiring large
volumes of high-quality data can be challenging, for example, collecting large
numbers of high-resolution images and long videos. On the other hand, there are
many complementary data that are usually considered corrupted or partial, such
as low-resolution images and short videos. Other examples of corrupted data
include videos that contain subtitles, watermarks, and logos. In this study, we
investigate the theoretical problem of whether the above partial data can be
utilized to train conventional diffusion models. Motivated by our theoretical
analysis in this study, we propose a straightforward approach of training
diffusion models utilizing partial data views, where we consider each form of
complementary data as a view of conventional data. Our proposed approach first
trains one separate diffusion model for each individual view, and then trains a
model for predicting the residual score function. We prove generalization error
bounds, which show that the proposed diffusion model training approach can
achieve lower generalization errors if proper regularizations are adopted in
the residual score function training. In particular, we prove that the
difficulty in training the residual score function scales proportionally with
the signal correlations not captured by partial data views. Consequently, the
proposed approach achieves near first-order optimal data efficiency.

</details>

### [652] [CoT-Vid: Dynamic Chain-of-Thought Routing with Self Verification for Training-Free Video Reasoning](https://arxiv.org/abs/2505.11830)
*Hongbo Jin,Ruyang Liu,Wenhao Zhang,Guibo Luo,Ge Li*

Main category: cs.CV

TLDR: CoT-Vid是一种无需训练的视频推理新范式，通过多阶段复杂推理设计，显著提升了视频领域的推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域对复杂视频推理的研究相对不足，而现有视频LLM过于依赖感知能力。

Method: 提出动态推理路径路由、问题解耦策略和视频自一致性验证三个核心组件，并设计新的视频问题分类标准。

Result: 在多个基准测试中表现优异，性能提升显著，甚至超越GPT-4V等大型专有模型。

Conclusion: CoT-Vid为视频推理提供了高效的新方法，代码将开源。

Abstract: System2 reasoning is developing rapidly these days with the emergence of
Deep- Thinking Models and chain-of-thought technology, which has become a
centralized discussion point in the AI community. However, there is a relative
gap in the research on complex video reasoning at present. In this work, we
propose CoT-Vid, a novel training-free paradigm for the video domain with a
multistage complex reasoning design. Distinguishing from existing video LLMs,
which rely heavily on perceptual abilities, it achieved surprising performance
gain with explicit reasoning mechanism. The paradigm consists of three main
components: dynamic inference path routing, problem decoupling strategy, and
video self-consistency verification. In addition, we propose a new standard for
categorization of video questions. CoT- Vid showed outstanding results on a
wide range of benchmarks, and outperforms its base model by 9.3% on Egochema
and 5.6% on VideoEspresso, rivalling or even surpassing larger and proprietary
models, such as GPT-4V, GPT-4o and Gemini-1.5-flash. Our codebase will be
publicly available soon.

</details>

### [653] [RVTBench: A Benchmark for Visual Reasoning Tasks](https://arxiv.org/abs/2505.11838)
*Yiqing Shen,Chenjia Li,Chenxiao Fan,Mathias Unberath*

Main category: cs.CV

TLDR: 论文提出了推理视觉任务（RVTs）的统一框架，扩展了传统视频推理分割任务，并构建了RVTBench基准测试和RVTagent代理框架，以解决现有基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉推理领域缺乏相关基准测试，且现有方法依赖大型语言模型（LLMs），无法充分捕捉复杂的时空关系和多步推理链。

Method: 提出了一种基于数字孪生（DT）表示的自动化RVT基准构建流程，并构建了RVTBench基准测试和RVTagent代理框架。

Result: 构建了包含3,896个查询、覆盖四种RVT类型和三种推理类别的RVTBench基准测试，并提出了支持零样本泛化的RVTagent框架。

Conclusion: 论文通过统一框架和自动化方法，解决了视觉推理任务的基准测试和模型泛化问题。

Abstract: Visual reasoning, the capability to interpret visual input in response to
implicit text query through multi-step reasoning, remains a challenge for deep
learning models due to the lack of relevant benchmarks. Previous work in visual
reasoning has primarily focused on reasoning segmentation, where models aim to
segment objects based on implicit text queries. This paper introduces reasoning
visual tasks (RVTs), a unified formulation that extends beyond traditional
video reasoning segmentation to a diverse family of visual language reasoning
problems, which can therefore accommodate multiple output formats including
bounding boxes, natural language descriptions, and question-answer pairs.
Correspondingly, we identify the limitations in current benchmark construction
methods that rely solely on large language models (LLMs), which inadequately
capture complex spatial-temporal relationships and multi-step reasoning chains
in video due to their reliance on token representation, resulting in benchmarks
with artificially limited reasoning complexity. To address this limitation, we
propose a novel automated RVT benchmark construction pipeline that leverages
digital twin (DT) representations as structured intermediaries between
perception and the generation of implicit text queries. Based on this method,
we construct RVTBench, a RVT benchmark containing 3,896 queries of over 1.2
million tokens across four types of RVT (segmentation, grounding, VQA and
summary), three reasoning categories (semantic, spatial, and temporal), and
four increasing difficulty levels, derived from 200 video sequences. Finally,
we propose RVTagent, an agent framework for RVT that allows for zero-shot
generalization across various types of RVT without task-specific fine-tuning.

</details>

### [654] [Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs](https://arxiv.org/abs/2505.11842)
*Xuannan Liu,Zekun Li,Zheqi He,Peipei Li,Shuhan Xia,Xing Cui,Huaibo Huang,Xi Yang,Ran He*

Main category: cs.CV

TLDR: Video-SafetyBench是首个评估大型视觉语言模型（LVLMs）在视频文本攻击下安全性的综合基准，包含2,264个视频文本对，覆盖48个细粒度不安全类别。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估主要关注静态图像输入，忽略了视频的动态特性可能引发的独特安全风险。

Method: 设计了可控的视频生成管道，将视频语义分解为主题图像和运动文本，并提出了基于LLM的RJScore评估指标。

Result: 实验显示，良性查询视频组合的平均攻击成功率为67.2%，揭示了模型对视频诱导攻击的持续脆弱性。

Conclusion: Video-SafetyBench将推动未来视频安全评估和防御策略的研究。

Abstract: The increasing deployment of Large Vision-Language Models (LVLMs) raises
safety concerns under potential malicious inputs. However, existing multimodal
safety evaluations primarily focus on model vulnerabilities exposed by static
image inputs, ignoring the temporal dynamics of video that may induce distinct
safety risks. To bridge this gap, we introduce Video-SafetyBench, the first
comprehensive benchmark designed to evaluate the safety of LVLMs under
video-text attacks. It comprises 2,264 video-text pairs spanning 48
fine-grained unsafe categories, each pairing a synthesized video with either a
harmful query, which contains explicit malice, or a benign query, which appears
harmless but triggers harmful behavior when interpreted alongside the video. To
generate semantically accurate videos for safety evaluation, we design a
controllable pipeline that decomposes video semantics into subject images (what
is shown) and motion text (how it moves), which jointly guide the synthesis of
query-relevant videos. To effectively evaluate uncertain or borderline harmful
outputs, we propose RJScore, a novel LLM-based metric that incorporates the
confidence of judge models and human-aligned decision threshold calibration.
Extensive experiments show that benign-query video composition achieves average
attack success rates of 67.2%, revealing consistent vulnerabilities to
video-induced attacks. We believe Video-SafetyBench will catalyze future
research into video-based safety evaluation and defense strategies.

</details>

### [655] [ElderFallGuard: Real-Time IoT and Computer Vision-Based Fall Detection System for Elderly Safety](https://arxiv.org/abs/2505.11845)
*Tasrifur Riahi,Md. Azizul Hakim Bappy,Md. Mehedi Islam*

Main category: cs.CV

TLDR: ElderFallGuard是一个基于计算机视觉的物联网系统，用于实时检测老年人跌倒并通知护理人员，通过MediaPipe进行姿态估计，使用随机森林分类器，达到100%的准确率。


<details>
  <summary>Details</summary>
Motivation: 老年人跌倒可能导致严重伤害和失去独立性，因此需要一种非侵入式的实时检测系统。

Method: 利用MediaPipe进行姿态估计，开发包含7200个样本的自定义数据集，训练随机森林分类器，并通过特定逻辑（如特定姿势持续时间和运动下降）检测跌倒。

Result: 系统在测试中表现优异，准确率、精确率、召回率和F1分数均为100%。

Conclusion: ElderFallGuard是一种有效的视觉物联网解决方案，可提升老年人安全性并为护理人员提供及时警报。

Abstract: For the elderly population, falls pose a serious and increasing risk of
serious injury and loss of independence. In order to overcome this difficulty,
we present ElderFallGuard: A Computer Vision Based IoT Solution for Elderly
Fall Detection and Notification, a cutting-edge, non-invasive system intended
for quick caregiver alerts and real-time fall detection. Our approach leverages
the power of computer vision, utilizing MediaPipe for accurate human pose
estimation from standard video streams. We developed a custom dataset
comprising 7200 samples across 12 distinct human poses to train and evaluate
various machine learning classifiers, with Random Forest ultimately selected
for its superior performance. ElderFallGuard employs a specific detection
logic, identifying a fall when a designated prone pose ("Pose6") is held for
over 3 seconds coupled with a significant drop in motion detected for more than
2 seconds. Upon confirmation, the system instantly dispatches an alert,
including a snapshot of the event, to a designated Telegram group via a custom
bot, incorporating cooldown logic to prevent notification overload. Rigorous
testing on our dataset demonstrated exceptional results, achieving 100%
accuracy, precision, recall, and F1-score. ElderFallGuard offers a promising,
vision-based IoT solution to enhance elderly safety and provide peace of mind
for caregivers through intelligent, timely alerts.

</details>

### [656] [MedSG-Bench: A Benchmark for Medical Image Sequences Grounding](https://arxiv.org/abs/2505.11852)
*Jingkun Yue,Siqi Zhang,Zinan Jia,Huihuan Xu,Zongbo Han,Xiaohong Liu,Guangyu Wang*

Main category: cs.CV

TLDR: MedSG-Bench是首个针对医学图像序列的视觉基准测试，填补了现有基准测试中序列图像的空白，并提出了两种任务范式。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉基准测试主要关注单张图像，而临床应用中需要处理序列图像，需跨模态和时间跟踪病变。

Method: 提出MedSG-Bench基准测试，包含8种VQA任务，分为图像差异和一致性两种范式。

Result: 覆盖76个数据集和10种模态，共9630个问答对，测试显示现有模型在序列任务中表现有限。

Conclusion: 构建了MedSG-188K数据集和MedSeq-Grounder模型，推动医学序列图像细粒度理解研究。

Abstract: Visual grounding is essential for precise perception and reasoning in
multimodal large language models (MLLMs), especially in medical imaging
domains. While existing medical visual grounding benchmarks primarily focus on
single-image scenarios, real-world clinical applications often involve
sequential images, where accurate lesion localization across different
modalities and temporal tracking of disease progression (e.g., pre- vs.
post-treatment comparison) require fine-grained cross-image semantic alignment
and context-aware reasoning. To remedy the underrepresentation of image
sequences in existing medical visual grounding benchmarks, we propose
MedSG-Bench, the first benchmark tailored for Medical Image Sequences
Grounding. It comprises eight VQA-style tasks, formulated into two paradigms of
the grounding tasks, including 1) Image Difference Grounding, which focuses on
detecting change regions across images, and 2) Image Consistency Grounding,
which emphasizes detection of consistent or shared semantics across sequential
images. MedSG-Bench covers 76 public datasets, 10 medical imaging modalities,
and a wide spectrum of anatomical structures and diseases, totaling 9,630
question-answer pairs. We benchmark both general-purpose MLLMs (e.g.,
Qwen2.5-VL) and medical-domain specialized MLLMs (e.g., HuatuoGPT-vision),
observing that even the advanced models exhibit substantial limitations in
medical sequential grounding tasks. To advance this field, we construct
MedSG-188K, a large-scale instruction-tuning dataset tailored for sequential
visual grounding, and further develop MedSeq-Grounder, an MLLM designed to
facilitate future research on fine-grained understanding across medical
sequential images. The benchmark, dataset, and model are available at
https://huggingface.co/MedSG-Bench

</details>

### [657] [MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos](https://arxiv.org/abs/2505.11868)
*Hongyi Zhou,Xiaogang Wang,Yulan Guo,Kai Xu*

Main category: cs.CV

TLDR: 提出了一种零样本框架，通过单目视频分析3D运动部件及其属性，无需标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖密集多视图图像或详细标注，限制了动态环境分析的灵活性。

Method: 结合深度估计、光流分析和点云配准初步分析运动部件，再通过2D高斯泼溅和动态场景优化算法细化结果。

Result: 实验表明，该方法能有效分析铰接物体运动，适用于旋转、平移及复合运动。

Conclusion: 该框架在零样本条件下表现出色，对未来具身智能应用具有重要潜力。

Abstract: Accurately analyzing the motion parts and their motion attributes in dynamic
environments is crucial for advancing key areas such as embodied intelligence.
Addressing the limitations of existing methods that rely on dense multi-view
images or detailed part-level annotations, we propose an innovative framework
that can analyze 3D mobility from monocular videos in a zero-shot manner. This
framework can precisely parse motion parts and motion attributes only using a
monocular video, completely eliminating the need for annotated training data.
Specifically, our method first constructs the scene geometry and roughly
analyzes the motion parts and their initial motion attributes combining depth
estimation, optical flow analysis and point cloud registration method, then
employs 2D Gaussian splatting for scene representation. Building on this, we
introduce an end-to-end dynamic scene optimization algorithm specifically
designed for articulated objects, refining the initial analysis results to
ensure the system can handle 'rotation', 'translation', and even complex
movements ('rotation+translation'), demonstrating high flexibility and
versatility. To validate the robustness and wide applicability of our method,
we created a comprehensive dataset comprising both simulated and real-world
scenarios. Experimental results show that our framework can effectively analyze
articulated object motions in an annotation-free manner, showcasing its
significant potential in future embodied intelligence applications.

</details>

### [658] [PRS-Med: Position Reasoning Segmentation with Vision-Language Model in Medical Imaging](https://arxiv.org/abs/2505.11872)
*Quoc-Huy Trinh,Minh-Van Nguyen,Jung Peng,Ulas Bagci,Debesh Jha*

Main category: cs.CV

TLDR: PRS-Med是一个结合视觉语言模型和分割能力的框架，用于生成准确的分割掩码和空间推理输出，解决了医学图像中位置推理数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在医生需要通过自然语言交互或需要位置推理时面临挑战，PRS-Med旨在解决这些问题。

Method: PRS-Med整合视觉语言模型与分割能力，并引入MMRS数据集提供多样化的空间推理数据。

Result: PRS-Med在六种成像模态中表现优异，显著优于现有方法的分割准确性和位置推理能力。

Conclusion: PRS-Med通过自然语言实现直观的医生-系统交互，提升诊断效率，数据集和模型将开源以促进研究。

Abstract: Recent advancements in prompt-based medical image segmentation have enabled
clinicians to identify tumors using simple input like bounding boxes or text
prompts. However, existing methods face challenges when doctors need to
interact through natural language or when position reasoning is required -
understanding spatial relationships between anatomical structures and
pathologies. We present PRS-Med, a framework that integrates vision-language
models with segmentation capabilities to generate both accurate segmentation
masks and corresponding spatial reasoning outputs. Additionally, we introduce
the MMRS dataset (Multimodal Medical in Positional Reasoning Segmentation),
which provides diverse, spatially-grounded question-answer pairs to address the
lack of position reasoning data in medical imaging. PRS-Med demonstrates
superior performance across six imaging modalities (CT, MRI, X-ray, ultrasound,
endoscopy, RGB), significantly outperforming state-of-the-art methods in both
segmentation accuracy and position reasoning. Our approach enables intuitive
doctor-system interaction through natural language, facilitating more efficient
diagnoses. Our dataset pipeline, model, and codebase will be released to foster
further research in spatially-aware multimodal reasoning for medical
applications.

</details>

### [659] [Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks](https://arxiv.org/abs/2505.11881)
*Giyeong Oh,Woohyun Cho,Siyeol Kim,Suhwan Choi,Younjae Yu*

Main category: cs.CV

TLDR: 论文提出了一种正交残差更新方法，通过分解模块输出并仅添加与输入流正交的分量，以增强特征学习和训练效率。


<details>
  <summary>Details</summary>
Motivation: 标准残差更新可能仅强化或调制现有流方向，未能充分利用模块学习新特征的能力。

Method: 引入正交残差更新，分解模块输出并仅添加正交分量。

Result: 在多种架构和数据集上提升了泛化准确性和训练稳定性，例如ViT-B在ImageNet-1k上提升了4.3%的top-1准确率。

Conclusion: 正交残差更新策略有效促进了新特征学习，提升了模型性能。

Abstract: Residual connections are pivotal for deep neural networks, enabling greater
depth by mitigating vanishing gradients. However, in standard residual updates,
the module's output is directly added to the input stream. This can lead to
updates that predominantly reinforce or modulate the existing stream direction,
potentially underutilizing the module's capacity for learning entirely novel
features. In this work, we introduce Orthogonal Residual Update: we decompose
the module's output relative to the input stream and add only the component
orthogonal to this stream. This design aims to guide modules to contribute
primarily new representational directions, fostering richer feature learning
while promoting more efficient training. We demonstrate that our orthogonal
update strategy improves generalization accuracy and training stability across
diverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs,
TinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\%p top-1 accuracy
gain for ViT-B on ImageNet-1k.

</details>

### [660] [GenZSL: Generative Zero-Shot Learning Via Inductive Variational Autoencoder](https://arxiv.org/abs/2505.11882)
*Shiming Chen,Dingjie Fu,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CV

TLDR: GenZSL是一种基于变分自编码器的生成式零样本学习方法，通过从相似已知类中归纳新类样本，并使用弱语义向量（如CLIP文本嵌入）提升生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式ZSL方法依赖专家标注的强语义向量，生成性能有限且场景泛化能力不足，GenZSL旨在解决这些问题。

Method: GenZSL采用类多样性促进和目标类引导信息增强策略，优化生成样本的多样性和信息量。

Result: 在三个基准数据集上，GenZSL显著优于f-VAEGAN，性能提升24.7%，训练速度快60倍以上。

Conclusion: GenZSL通过弱语义向量和优化策略，显著提升了生成式ZSL的效能和效率。

Abstract: Remarkable progress in zero-shot learning (ZSL) has been achieved using
generative models. However, existing generative ZSL methods merely generate
(imagine) the visual features from scratch guided by the strong class semantic
vectors annotated by experts, resulting in suboptimal generative performance
and limited scene generalization. To address these and advance ZSL, we propose
an inductive variational autoencoder for generative zero-shot learning, dubbed
GenZSL. Mimicking human-level concept learning, GenZSL operates by inducting
new class samples from similar seen classes using weak class semantic vectors
derived from target class names (i.e., CLIP text embedding). To ensure the
generation of informative samples for training an effective ZSL classifier, our
GenZSL incorporates two key strategies. Firstly, it employs class diversity
promotion to enhance the diversity of class semantic vectors. Secondly, it
utilizes target class-guided information boosting criteria to optimize the
model. Extensive experiments conducted on three popular benchmark datasets
showcase the superiority and potential of our GenZSL with significant efficacy
and efficiency over f-VAEGAN, e.g., 24.7% performance gains and more than
$60\times$ faster training speed on AWA2. Codes are available at
https://github.com/shiming-chen/GenZSL.

</details>

### [661] [Facial Recognition Leveraging Generative Adversarial Networks](https://arxiv.org/abs/2505.11884)
*Zhongwen Li,Zongwei Li,Xiaoqi Li*

Main category: cs.CV

TLDR: 本文提出了一种基于GAN的数据增强方法，通过改进生成器和判别器结构，显著提升了小样本下的人脸识别性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习人脸识别依赖大规模训练数据，但实际应用中数据获取困难。

Method: 提出了一种GAN框架，包括残差嵌入生成器、Inception ResNet-V1判别器，以及端到端的联合优化方法。

Result: 在LFW基准测试中，识别准确率提升12.7%，且在小样本下表现良好。

Conclusion: 该方法有效解决了数据不足问题，提升了人脸识别性能。

Abstract: Face recognition performance based on deep learning heavily relies on
large-scale training data, which is often difficult to acquire in practical
applications. To address this challenge, this paper proposes a GAN-based data
augmentation method with three key contributions: (1) a residual-embedded
generator to alleviate gradient vanishing/exploding problems, (2) an Inception
ResNet-V1 based FaceNet discriminator for improved adversarial training, and
(3) an end-to-end framework that jointly optimizes data generation and
recognition performance. Experimental results demonstrate that our approach
achieves stable training dynamics and significantly improves face recognition
accuracy by 12.7% on the LFW benchmark compared to baseline methods, while
maintaining good generalization capability with limited training samples.

</details>

### [662] [Adversarial Robustness for Unified Multi-Modal Encoders via Efficient Calibration](https://arxiv.org/abs/2505.11895)
*Chih-Ting Liao,Bin Ren,Guofeng Mei,Xu Zheng*

Main category: cs.CV

TLDR: 本文首次全面研究了统一多模态编码器在对抗性扰动下的脆弱性，并提出了一种高效的对抗性校准框架以提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管统一多模态编码器在多模态任务中表现优异，但其在对抗性扰动下的鲁棒性尚未充分研究，这对安全敏感应用至关重要。

Method: 提出了一种无需修改预训练编码器的对抗性校准框架，通过模态特定投影头和三种训练目标（固定中心交叉熵、干净到对抗性L2对齐、干净对抗性InfoNCE）提升鲁棒性。

Result: 在六种模态和三种Bind风格模型上，该方法在epsilon=4/255时对抗性鲁棒性提升高达47.3%，且干净零样本和检索性能保持或提升。

Conclusion: 该框架显著提升了多模态编码器的对抗性鲁棒性，同时保持了模型性能，为安全敏感应用提供了实用解决方案。

Abstract: Recent unified multi-modal encoders align a wide range of modalities into a
shared representation space, enabling diverse cross-modal tasks. Despite their
impressive capabilities, the robustness of these models under adversarial
perturbations remains underexplored, which is a critical concern for
safety-sensitive applications. In this work, we present the first comprehensive
study of adversarial vulnerability in unified multi-modal encoders. We find
that even mild adversarial perturbations lead to substantial performance drops
across all modalities. Non-visual inputs, such as audio and point clouds, are
especially fragile, while visual inputs like images and videos also degrade
significantly. To address this, we propose an efficient adversarial calibration
framework that improves robustness across modalities without modifying
pretrained encoders or semantic centers, ensuring compatibility with existing
foundation models. Our method introduces modality-specific projection heads
trained solely on adversarial examples, while keeping the backbone and
embeddings frozen. We explore three training objectives: fixed-center
cross-entropy, clean-to-adversarial L2 alignment, and clean-adversarial
InfoNCE, and we introduce a regularization strategy to ensure
modality-consistent alignment under attack. Experiments on six modalities and
three Bind-style models show that our method improves adversarial robustness by
up to 47.3 percent at epsilon = 4/255, while preserving or even improving clean
zero-shot and retrieval performance with less than 1 percent trainable
parameters.

</details>

### [663] [FiGKD: Fine-Grained Knowledge Distillation via High-Frequency Detail Transfer](https://arxiv.org/abs/2505.11897)
*Seonghak Kim*

Main category: cs.CV

TLDR: FiGKD提出了一种基于频率感知的知识蒸馏方法，通过分解教师模型的输出为低频和高频分量，选择性传递高频信息，提升细粒度视觉任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法在细粒度视觉任务中表现不佳，因传统方法将教师模型的输出视为单一信号，未区分有用与冗余信息。

Method: FiGKD利用离散小波变换将教师模型的输出分解为低频（内容）和高频（细节）分量，仅传递高频分量以保留语义决策模式。

Result: 在CIFAR-100、TinyImageNet等数据集上，FiGKD优于现有基于输出和特征的蒸馏方法。

Conclusion: 频率感知的分解能更高效地传递知识，尤其在资源受限的场景中表现突出。

Abstract: Knowledge distillation (KD) is a widely adopted technique for transferring
knowledge from a high-capacity teacher model to a smaller student model by
aligning their output distributions. However, existing methods often
underperform in fine-grained visual recognition tasks, where distinguishing
subtle differences between visually similar classes is essential. This
performance gap stems from the fact that conventional approaches treat the
teacher's output logits as a single, undifferentiated signal-assuming all
contained information is equally beneficial to the student. Consequently,
student models may become overloaded with redundant signals and fail to capture
the teacher's nuanced decision boundaries. To address this issue, we propose
Fine-Grained Knowledge Distillation (FiGKD), a novel frequency-aware framework
that decomposes a model's logits into low-frequency (content) and
high-frequency (detail) components using the discrete wavelet transform (DWT).
FiGKD selectively transfers only the high-frequency components, which encode
the teacher's semantic decision patterns, while discarding redundant
low-frequency content already conveyed through ground-truth supervision. Our
approach is simple, architecture-agnostic, and requires no access to
intermediate feature maps. Extensive experiments on CIFAR-100, TinyImageNet,
and multiple fine-grained recognition benchmarks show that FiGKD consistently
outperforms state-of-the-art logit-based and feature-based distillation methods
across a variety of teacher-student configurations. These findings confirm that
frequency-aware logit decomposition enables more efficient and effective
knowledge transfer, particularly in resource-constrained settings.

</details>

### [664] [GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects Based on Appearance and Geometric Complexity](https://arxiv.org/abs/2505.11905)
*Takuya Ikeda,Sergey Zakharov,Muhammad Zubair Irshad,Istvan Balazs Opra,Shun Iwase,Dian Chen,Mark Tjersland,Robert Lee,Alexandre Dilly,Rares Ambrus,Koichi Nishiwaki*

Main category: cs.CV

TLDR: 提出了一种基于单目RGBD视频的6-DoF物体跟踪和高保真3D重建新方法，解决了复杂物体（如对称、复杂几何或外观）的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂物体时表现不佳，尤其是对称、复杂几何或外观的物体。

Method: 结合3D高斯溅射、混合几何/外观跟踪和关键帧选择的自适应方法。

Result: 实现了鲁棒的跟踪和精确的重建，并提供了高质量标注的基准测试集。

Conclusion: 该方法在开放环境中为单传感器3D重建设定了新标准。

Abstract: We present a novel method for 6-DoF object tracking and high-quality 3D
reconstruction from monocular RGBD video. Existing methods, while achieving
impressive results, often struggle with complex objects, particularly those
exhibiting symmetry, intricate geometry or complex appearance. To bridge these
gaps, we introduce an adaptive method that combines 3D Gaussian Splatting,
hybrid geometry/appearance tracking, and key frame selection to achieve robust
tracking and accurate reconstructions across a diverse range of objects.
Additionally, we present a benchmark covering these challenging object classes,
providing high-quality annotations for evaluating both tracking and
reconstruction performance. Our approach demonstrates strong capabilities in
recovering high-fidelity object meshes, setting a new standard for
single-sensor 3D reconstruction in open-world environments.

</details>

### [665] [Are Multimodal Large Language Models Ready for Omnidirectional Spatial Reasoning?](https://arxiv.org/abs/2505.11907)
*Zihao Dongfang,Xu Zheng,Ziqiao Weng,Yuanhuiyi Lyu,Danda Pani Paudel,Luc Van Gool,Kailun Yang,Xuming Hu*

Main category: cs.CV

TLDR: 该论文探讨了多模态大语言模型（MLLMs）在全景空间推理中的表现，并提出了首个专门用于此场景的基准测试OSR-Bench。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在视觉空间推理方面表现出潜力，但现有研究主要集中在标准针孔视图图像上，全景感知领域尚未充分探索。

Method: 研究设计了OSR-Bench基准，包含超过153,000个多样化的问答对，并提出了负采样策略和两阶段评估框架。

Result: 评估了八种先进的MLLMs，发现它们在全景空间推理中表现不佳，表明需要更接地气的模型。

Conclusion: 论文呼吁开发更具感知基础的MLLMs，并公开了OSR-Bench基准和代码。

Abstract: The 180x360 omnidirectional field of view captured by 360-degree cameras
enables their use in a wide range of applications such as embodied AI and
virtual reality. Although recent advances in multimodal large language models
(MLLMs) have shown promise in visual-spatial reasoning, most studies focus on
standard pinhole-view images, leaving omnidirectional perception largely
unexplored. In this paper, we ask: Are MLLMs ready for omnidirectional spatial
reasoning? To investigate this, we introduce OSR-Bench, the first benchmark
specifically designed for this setting. OSR-Bench includes over 153,000 diverse
question-answer pairs grounded in high-fidelity panoramic indoor scene maps. It
covers key reasoning types including object counting, relative distance, and
direction. We also propose a negative sampling strategy that inserts
non-existent objects into prompts to evaluate hallucination and grounding
robustness. For fine-grained analysis, we design a two-stage evaluation
framework assessing both cognitive map generation and QA accuracy using
rotation-invariant matching and a combination of rule-based and LLM-based
metrics. We evaluate eight state-of-the-art MLLMs, including GPT-4o, Gemini 1.5
Pro, and leading open-source models under zero-shot settings. Results show that
current models struggle with spatial reasoning in panoramic contexts,
highlighting the need for more perceptually grounded MLLMs. OSR-Bench and code
will be released at: https://huggingface.co/datasets/UUUserna/OSR-Bench

</details>

### [666] [DC-Seg: Disentangled Contrastive Learning for Brain Tumor Segmentation with Missing Modalities](https://arxiv.org/abs/2505.11921)
*Haitao Li,Ziyu Li,Yiheng Mao,Zhengyao Ding,Zhengxing Huang*

Main category: cs.CV

TLDR: DC-Seg提出了一种新方法，通过解耦模态不变解剖表示和模态特定表示，改进多模态脑图像分割任务，尤其在模态缺失情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 临床数据中多模态图像可能不完整，现有方法在共享潜在空间中编码多模态信息效果有限，未能充分利用各模态独特信息。

Method: DC-Seg采用解剖对比学习和模态对比学习，解耦图像为模态不变和模态特定表示，并引入分割正则化增强模型鲁棒性。

Result: 在BraTS 2020和WMH数据集上，DC-Seg优于现有方法，尤其在模态缺失情况下表现突出，并展示出强泛化能力。

Conclusion: DC-Seg通过解耦和对比学习，显著提升了多模态脑图像分割的准确性和鲁棒性，尤其在模态缺失场景下具有优势。

Abstract: Accurate segmentation of brain images typically requires the integration of
complementary information from multiple image modalities. However, clinical
data for all modalities may not be available for every patient, creating a
significant challenge. To address this, previous studies encode multiple
modalities into a shared latent space. While somewhat effective, it remains
suboptimal, as each modality contains distinct and valuable information. In
this study, we propose DC-Seg (Disentangled Contrastive Learning for
Segmentation), a new method that explicitly disentangles images into
modality-invariant anatomical representation and modality-specific
representation, by using anatomical contrastive learning and modality
contrastive learning respectively. This solution improves the separation of
anatomical and modality-specific features by considering the modality gaps,
leading to more robust representations. Furthermore, we introduce a
segmentation-based regularizer that enhances the model's robustness to missing
modalities. Extensive experiments on the BraTS 2020 and a private white matter
hyperintensity(WMH) segmentation dataset demonstrate that DC-Seg outperforms
state-of-the-art methods in handling incomplete multimodal brain tumor
segmentation tasks with varying missing modalities, while also demonstrate
strong generalizability in WMH segmentation. The code is available at
https://github.com/CuCl-2/DC-Seg.

</details>

### [667] [SafeVid: Toward Safety Aligned Video Large Multimodal Models](https://arxiv.org/abs/2505.11926)
*Yixu Wang,Jiaxin Song,Yifeng Gao,Xin Wang,Yang Yao,Yan Teng,Xingjun Ma,Yingchun Wang,Yu-Gang Jiang*

Main category: cs.CV

TLDR: SafeVid框架通过文本视频描述和LLM驱动的安全推理，显著提升视频大模型（VLMMs）的安全性。


<details>
  <summary>Details</summary>
Motivation: 视频大模型的复杂性导致静态安全对齐在动态视频场景中失效，需要针对视频的安全对齐方法。

Method: 1）构建SafeVid-350K数据集；2）使用DPO对齐VLMMs；3）通过SafeVidBench评估。

Result: SafeVid显著提升VLMMs安全性（如LLaVA-NeXT-Video提升42.39%）。

Conclusion: SafeVid通过文本描述桥接安全推理，为VLMMs提供了有效的安全对齐框架。

Abstract: As Video Large Multimodal Models (VLMMs) rapidly advance, their inherent
complexity introduces significant safety challenges, particularly the issue of
mismatched generalization where static safety alignments fail to transfer to
dynamic video contexts. We introduce SafeVid, a framework designed to instill
video-specific safety principles in VLMMs. SafeVid uniquely transfers robust
textual safety alignment capabilities to the video domain by employing detailed
textual video descriptions as an interpretive bridge, facilitating LLM-based
rule-driven safety reasoning. This is achieved through a closed-loop system
comprising: 1) generation of SafeVid-350K, a novel 350,000-pair video-specific
safety preference dataset; 2) targeted alignment of VLMMs using Direct
Preference Optimization (DPO); and 3) comprehensive evaluation via our new
SafeVidBench benchmark. Alignment with SafeVid-350K significantly enhances VLMM
safety, with models like LLaVA-NeXT-Video demonstrating substantial
improvements (e.g., up to 42.39%) on SafeVidBench. SafeVid provides critical
resources and a structured approach, demonstrating that leveraging textual
descriptions as a conduit for safety reasoning markedly improves the safety
alignment of VLMMs. We have made SafeVid-350K dataset
(https://huggingface.co/datasets/yxwang/SafeVid-350K) publicly available.

</details>

### [668] [iSegMan: Interactive Segment-and-Manipulate 3D Gaussians](https://arxiv.org/abs/2505.11934)
*Yian Zhao,Wanshi Xu,Ruochong Zheng,Pengchong Qiao,Chang Liu,Jie Chen*

Main category: cs.CV

TLDR: iSegMan是一个交互式3D高斯分割与操作框架，通过2D用户交互实现高效区域控制，无需场景特定训练。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景操作方法在区域控制和交互反馈方面存在不足，且依赖场景特定训练，限制了效率和灵活性。

Method: 提出Epipolar-guided Interaction Propagation（EIP）和Visibility-based Gaussian Voting（VGV），结合2D交互和3D高斯模型实现高效区域分割。

Result: iSegMan在3D场景操作和分割任务中表现出显著优势，提供了更高的可控性和实用性。

Conclusion: iSegMan通过创新的交互和分割技术，显著提升了3D场景操作的效率和灵活性。

Abstract: The efficient rendering and explicit nature of 3DGS promote the advancement
of 3D scene manipulation. However, existing methods typically encounter
challenges in controlling the manipulation region and are unable to furnish the
user with interactive feedback, which inevitably leads to unexpected results.
Intuitively, incorporating interactive 3D segmentation tools can compensate for
this deficiency. Nevertheless, existing segmentation frameworks impose a
pre-processing step of scene-specific parameter training, which limits the
efficiency and flexibility of scene manipulation. To deliver a 3D region
control module that is well-suited for scene manipulation with reliable
efficiency, we propose interactive Segment-and-Manipulate 3D Gaussians
(iSegMan), an interactive segmentation and manipulation framework that only
requires simple 2D user interactions in any view. To propagate user
interactions to other views, we propose Epipolar-guided Interaction Propagation
(EIP), which innovatively exploits epipolar constraint for efficient and robust
interaction matching. To avoid scene-specific training to maintain efficiency,
we further propose the novel Visibility-based Gaussian Voting (VGV), which
obtains 2D segmentations from SAM and models the region extraction as a voting
game between 2D Pixels and 3D Gaussians based on Gaussian visibility. Taking
advantage of the efficient and precise region control of EIP and VGV, we put
forth a Manipulation Toolbox to implement various functions on selected
regions, enhancing the controllability, flexibility and practicality of scene
manipulation. Extensive results on 3D scene manipulation and segmentation tasks
fully demonstrate the significant advantages of iSegMan. Project page is
available at https://zhao-yian.github.io/iSegMan.

</details>

### [669] [Top-Down Compression: Revisit Efficient Vision Token Projection for Visual Instruction Tuning](https://arxiv.org/abs/2505.11945)
*Bonan li,Zicheng Zhang,Songhua Liu,Weihao Yu,Xinchao Wang*

Main category: cs.CV

TLDR: LLaVA-Meteor提出了一种新的视觉指令调优方法，通过Top-Down Compression范式高效压缩视觉标记，同时保持核心信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉到语言的投影中存在准确性与效率的权衡问题，LLaVA-Meteor旨在解决这一挑战。

Method: 采用Top-Down Compression范式，结合Flash Global Fusion模块和local-to-single扫描方式，以及Visual-Native Selection机制。

Result: 实验表明，该方法减少了75-95%的视觉标记，同时在12个基准测试中表现优异。

Conclusion: LLaVA-Meteor显著提升了视觉指令调优的效率与性能。

Abstract: Visual instruction tuning aims to enable large language models to comprehend
the visual world, with a pivotal challenge lying in establishing an effective
vision-to-language projection. However, existing methods often grapple with the
intractable trade-off between accuracy and efficiency. In this paper, we
present LLaVA-Meteor, a novel approach designed to break this deadlock,
equipped with a novel Top-Down Compression paradigm that strategically
compresses visual tokens without compromising core information. Specifically,
we construct a trainable Flash Global Fusion module based on efficient
selective state space operators, which aligns the feature space while enabling
each token to perceive holistic visual context and instruction preference at
low cost. Furthermore, a local-to-single scanning manner is employed to
effectively capture local dependencies, thereby enhancing the model's
capability in vision modeling. To alleviate computational overhead, we explore
a Visual-Native Selection mechanism that independently assesses token
significance by both the visual and native experts, followed by aggregation to
retain the most critical subset. Extensive experiments show that our approach
reduces visual tokens by 75--95% while achieving comparable or superior
performance across 12 benchmarks, significantly improving efficiency.

</details>

### [670] [Advanced Integration of Discrete Line Segments in Digitized P&ID for Continuous Instrument Connectivity](https://arxiv.org/abs/2505.11976)
*Soumya Swarup Prusty,Astha Agarwal,Srinivasan Iyenger*

Main category: cs.CV

TLDR: 论文提出了一种通过计算机视觉模型检测并合并线段的方法，以数字化P&ID图纸，从而减少人工错误和时间消耗。


<details>
  <summary>Details</summary>
Motivation: 现有P&ID图纸信息的手动映射耗时且易错，依赖专家经验，亟需自动化解决方案。

Method: 利用计算机视觉模型检测线段，合并线段以连接设备和仪器，构建数字化P&ID。

Result: 实现了P&ID的数字化，信息可存储于知识图谱，支持优化路径、检测系统循环等任务。

Conclusion: 该方法显著提升了P&ID信息处理的效率和准确性，为后续高级算法应用奠定了基础。

Abstract: Piping and Instrumentation Diagrams (P&IDs) constitute the foundational
blueprint of a plant, depicting the interconnections among process equipment,
instrumentation for process control, and the flow of fluids and control
signals. In their existing setup, the manual mapping of information from P&ID
sheets holds a significant challenge. This is a time-consuming process, taking
around 3-6 months, and is susceptible to errors. It also depends on the
expertise of the domain experts and often requires multiple rounds of review.
The digitization of P&IDs entails merging detected line segments, which is
essential for linking various detected instruments, thereby creating a
comprehensive digitized P&ID. This paper focuses on explaining how line
segments which are detected using a computer vision model are merged and
eventually building the connection between equipment and merged lines. Hence
presenting a digitized form of information stating the interconnection between
process equipment, instrumentation, flow of fluids and control signals.
Eventually, which can be stored in a knowledge graph and that information along
with the help of advanced algorithms can be leveraged for tasks like finding
optimal routes, detecting system cycles, computing transitive closures, and
more.

</details>

### [671] [AoP-SAM: Automation of Prompts for Efficient Segmentation](https://arxiv.org/abs/2505.11980)
*Yi Chen,Mu-Young Son,Chuanbo Hua,Joo-Young Kim*

Main category: cs.CV

TLDR: AoP-SAM是一种自动生成提示的方法，提升了SAM在图像分割中的效率和实用性，无需手动输入。


<details>
  <summary>Details</summary>
Motivation: 手动提示在现实应用中不切实际，尤其是在需要快速提示和资源效率的场景中。

Method: AoP-SAM通过轻量级Prompt Predictor模型自动检测关键实体并确定最佳提示区域，结合自适应采样和过滤机制。

Result: 在三个数据集上的评估显示，AoP-SAM显著提升了提示生成效率和掩码生成准确性。

Conclusion: AoP-SAM使SAM更适合自动化分割任务，提高了其效率和实用性。

Abstract: The Segment Anything Model (SAM) is a powerful foundation model for image
segmentation, showing robust zero-shot generalization through prompt
engineering. However, relying on manual prompts is impractical for real-world
applications, particularly in scenarios where rapid prompt provision and
resource efficiency are crucial. In this paper, we propose the Automation of
Prompts for SAM (AoP-SAM), a novel approach that learns to generate essential
prompts in optimal locations automatically. AoP-SAM enhances SAM's efficiency
and usability by eliminating manual input, making it better suited for
real-world tasks. Our approach employs a lightweight yet efficient Prompt
Predictor model that detects key entities across images and identifies the
optimal regions for placing prompt candidates. This method leverages SAM's
image embeddings, preserving its zero-shot generalization capabilities without
requiring fine-tuning. Additionally, we introduce a test-time instance-level
Adaptive Sampling and Filtering mechanism that generates prompts in a
coarse-to-fine manner. This notably enhances both prompt and mask generation
efficiency by reducing computational overhead and minimizing redundant mask
refinements. Evaluations of three datasets demonstrate that AoP-SAM
substantially improves both prompt generation efficiency and mask generation
accuracy, making SAM more effective for automated segmentation tasks.

</details>

### [672] [Online Iterative Self-Alignment for Radiology Report Generation](https://arxiv.org/abs/2505.11983)
*Ting Xiao,Lei Shi,Yang Zhang,HaoFeng Yang,Zhe Wang,Chenjia Bai*

Main category: cs.CV

TLDR: 本文提出了一种新颖的在线迭代自对齐（OISA）方法，用于放射学报告生成（RRG），通过自生成多样化数据、自评估多目标偏好数据、自对齐多目标优化和自迭代进一步改进，显著提升了数据质量和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有RRG模型主要依赖监督微调（SFT）和强化学习（RL），但高质量标注数据覆盖有限，容易过拟合和泛化能力不足。

Method: 提出OISA方法，包括四个阶段：自生成多样化数据、自评估多目标偏好数据、自对齐多目标优化和自迭代改进。

Result: 实验结果表明，该方法在多个评估指标上超越了现有方法，达到了最先进的性能。

Conclusion: OISA方法通过迭代多目标优化显著提升了RRG模型的数据质量和性能，具有实际应用潜力。

Abstract: Radiology Report Generation (RRG) is an important research topic for
relieving radiologist' heavy workload. Existing RRG models mainly rely on
supervised fine-tuning (SFT) based on different model architectures using data
pairs of radiological images and corresponding radiologist-annotated reports.
Recent research has shifted focus to post-training improvements, aligning RRG
model outputs with human preferences using reinforcement learning (RL).
However, the limited data coverage of high-quality annotated data poses risks
of overfitting and generalization. This paper proposes a novel Online Iterative
Self-Alignment (OISA) method for RRG that consists of four stages:
self-generation of diverse data, self-evaluation for multi-objective preference
data,self-alignment for multi-objective optimization and self-iteration for
further improvement. Our approach allows for generating varied reports tailored
to specific clinical objectives, enhancing the overall performance of the RRG
model iteratively. Unlike existing methods, our frame-work significantly
increases data quality and optimizes performance through iterative
multi-objective optimization. Experimental results demonstrate that our method
surpasses previous approaches, achieving state-of-the-art performance across
multiple evaluation metrics.

</details>

### [673] [SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations](https://arxiv.org/abs/2505.11992)
*Songchun Zhang,Huiyao Xu,Sitong Guo,Zhongwei Xie,Pengwei Liu,Hujun Bao,Weiwei Xu,Changqing Zou*

Main category: cs.CV

TLDR: 提出SpatialCrafter框架，利用视频扩散模型从稀疏或单视图输入生成逼真3D场景。


<details>
  <summary>Details</summary>
Motivation: 现有技术依赖密集多视图观测，限制了应用范围，因此需要解决稀疏或单视图输入的3D场景重建问题。

Method: 结合可训练相机编码器、极线注意力机制和统一尺度估计策略，利用视频扩散模型生成额外观测，并通过混合网络结构处理长序列特征。

Result: 实验表明，该方法提升了稀疏视图重建效果，恢复了3D场景的真实外观。

Conclusion: SpatialCrafter框架有效解决了稀疏或单视图输入的3D场景重建问题，具有广泛应用潜力。

Abstract: Novel view synthesis (NVS) boosts immersive experiences in computer vision
and graphics. Existing techniques, though progressed, rely on dense multi-view
observations, restricting their application. This work takes on the challenge
of reconstructing photorealistic 3D scenes from sparse or single-view inputs.
We introduce SpatialCrafter, a framework that leverages the rich knowledge in
video diffusion models to generate plausible additional observations, thereby
alleviating reconstruction ambiguity. Through a trainable camera encoder and an
epipolar attention mechanism for explicit geometric constraints, we achieve
precise camera control and 3D consistency, further reinforced by a unified
scale estimation strategy to handle scale discrepancies across datasets.
Furthermore, by integrating monocular depth priors with semantic features in
the video latent space, our framework directly regresses 3D Gaussian primitives
and efficiently processes long-sequence features using a hybrid network
structure. Extensive experiments show our method enhances sparse view
reconstruction and restores the realistic appearance of 3D scenes.

</details>

### [674] [Multimodal Cancer Survival Analysis via Hypergraph Learning with Cross-Modality Rebalance](https://arxiv.org/abs/2505.11997)
*Mingcheng Qu,Guang Yang,Donglin,Tonghua Su,Yue Gao,Yang Song,Lei Fan*

Main category: cs.CV

TLDR: 提出了一种结合超图学习的多模态生存预测框架，通过模态再平衡机制和交互对齐融合策略解决病理-基因组不平衡问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多采用多实例学习聚合病理图像特征，但忽略了上下文和层次细节的信息丢失，且病理与基因组数据在粒度和维度上的差异导致模态不平衡。

Method: 提出超图学习框架，捕捉病理图像的上下文和层次细节，并采用模态再平衡机制和交互对齐融合策略动态调整两模态贡献。

Result: 在五个TCGA数据集上的实验表明，模型在C-Index性能上优于先进方法超过3.4%。

Conclusion: 该框架有效解决了病理-基因组模态不平衡问题，提升了生存预测性能。

Abstract: Multimodal pathology-genomic analysis has become increasingly prominent in
cancer survival prediction. However, existing studies mainly utilize
multi-instance learning to aggregate patch-level features, neglecting the
information loss of contextual and hierarchical details within pathology
images. Furthermore, the disparity in data granularity and dimensionality
between pathology and genomics leads to a significant modality imbalance. The
high spatial resolution inherent in pathology data renders it a dominant role
while overshadowing genomics in multimodal integration. In this paper, we
propose a multimodal survival prediction framework that incorporates hypergraph
learning to effectively capture both contextual and hierarchical details from
pathology images. Moreover, it employs a modality rebalance mechanism and an
interactive alignment fusion strategy to dynamically reweight the contributions
of the two modalities, thereby mitigating the pathology-genomics imbalance.
Quantitative and qualitative experiments are conducted on five TCGA datasets,
demonstrating that our model outperforms advanced methods by over 3.4\% in
C-Index performance.

</details>

### [675] [IQBench: How "Smart'' Are Vision-Language Models? A Study with Human IQ Tests](https://arxiv.org/abs/2505.12000)
*Tan-Hanh Pham,Phu-Vinh Nguyen,Dang The Hung,Bui Trong Duong,Vu Nguyen Thanh,Chris Ngo,Tri Quang Truong,Truong-Son Hy*

Main category: cs.CV

TLDR: 论文介绍了IQBench，一个评估视觉语言模型在标准化视觉智商测试中推理能力的新基准，强调推理过程比最终预测准确性更重要。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在人类智商测试中的真实推理能力，填补现有研究的空白。

Method: 手动收集并标注500个视觉智商问题，评估模型的推理能力，包括解释、解决模式和最终预测准确性。

Result: 模型在3D空间和字谜推理任务中表现较差，推理过程与最终答案存在不一致性。

Conclusion: 评估推理准确性对全面理解模型能力至关重要，现有模型在通用推理能力上仍有显著局限。

Abstract: Although large Vision-Language Models (VLMs) have demonstrated remarkable
performance in a wide range of multimodal tasks, their true reasoning
capabilities on human IQ tests remain underexplored. To advance research on the
fluid intelligence of VLMs, we introduce **IQBench**, a new benchmark designed
to evaluate VLMs on standardized visual IQ tests. We focus on evaluating the
reasoning capabilities of VLMs, which we argue are more important than the
accuracy of the final prediction. **Our benchmark is visually centric,
minimizing the dependence on unnecessary textual content**, thus encouraging
models to derive answers primarily from image-based information rather than
learned textual knowledge. To this end, we manually collected and annotated 500
visual IQ questions to **prevent unintentional data leakage during training**.
Unlike prior work that focuses primarily on the accuracy of the final answer,
we evaluate the reasoning ability of the models by assessing their explanations
and the patterns used to solve each problem, along with the accuracy of the
final prediction and human evaluation. Our experiments show that there are
substantial performance disparities between tasks, with models such as
`o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet` achieving the highest
average accuracies of 0.615, 0.578, and 0.548, respectively. However, all
models struggle with 3D spatial and anagram reasoning tasks, highlighting
significant limitations in current VLMs' general reasoning abilities. In terms
of reasoning scores, `o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet`
achieved top averages of 0.696, 0.586, and 0.516, respectively. These results
highlight inconsistencies between the reasoning processes of the models and
their final answers, emphasizing the importance of evaluating the accuracy of
the reasoning in addition to the final predictions.

</details>

### [676] [CHRIS: Clothed Human Reconstruction with Side View Consistency](https://arxiv.org/abs/2505.12005)
*Dong Liu,Yifan Yang,Zixiong Huang,Yuxin Gao,Mingkui Tan*

Main category: cs.CV

TLDR: CHRIS方法通过侧视一致性提升单视角RGB图像重建穿衣人体的真实感，包括侧视法线判别器和多对一梯度计算。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用侧视信息，导致全局拓扑和局部表面不一致。

Method: 提出CHRIS方法，包含侧视法线判别器和多对一梯度计算（M2O）。

Result: 在公开基准测试中表现最优，超越现有方法。

Conclusion: CHRIS有效解决了单视角重建中的侧视一致性问题，提升了真实感。

Abstract: Creating a realistic clothed human from a single-view RGB image is crucial
for applications like mixed reality and filmmaking. Despite some progress in
recent years, mainstream methods often fail to fully utilize side-view
information, as the input single-view image contains front-view information
only. This leads to globally unrealistic topology and local surface
inconsistency in side views. To address these, we introduce Clothed Human
Reconstruction with Side View Consistency, namely CHRIS, which consists of 1) A
Side-View Normal Discriminator that enhances global visual reasonability by
distinguishing the generated side-view normals from the ground truth ones; 2) A
Multi-to-One Gradient Computation (M2O) that ensures local surface consistency.
M2O calculates the gradient of a sampling point by integrating the gradients of
the nearby points, effectively acting as a smooth operation. Experimental
results demonstrate that CHRIS achieves state-of-the-art performance on public
benchmarks and outperforms the prior work.

</details>

### [677] [Multi-modal Collaborative Optimization and Expansion Network for Event-assisted Single-eye Expression Recognition](https://arxiv.org/abs/2505.12007)
*Runduo Han,Xiuping Liu,Shangxuan Yi,Yi Zhang,Hongchen Tan*

Main category: cs.CV

TLDR: 提出了一种多模态协同优化与扩展网络（MCO-E Net），通过事件模态解决单眼表情识别任务中的低光、高曝光和高动态范围问题。


<details>
  <summary>Details</summary>
Motivation: 解决单眼表情识别在复杂光照条件下的挑战。

Method: 引入MCO-Mamba和HCE-MoE两种创新设计，分别通过双模态联合优化和动态路由机制实现模态协同学习。

Result: 在单眼表情识别任务中表现出色，尤其在光照条件差的情况下。

Conclusion: MCO-E Net通过多模态协同和异构架构，显著提升了复杂环境下的表情识别性能。

Abstract: In this paper, we proposed a Multi-modal Collaborative Optimization and
Expansion Network (MCO-E Net), to use event modalities to resist challenges
such as low light, high exposure, and high dynamic range in single-eye
expression recognition tasks. The MCO-E Net introduces two innovative designs:
Multi-modal Collaborative Optimization Mamba (MCO-Mamba) and Heterogeneous
Collaborative and Expansion Mixture-of-Experts (HCE-MoE). MCO-Mamba, building
upon Mamba, leverages dual-modal information to jointly optimize the model,
facilitating collaborative interaction and fusion of modal semantics. This
approach encourages the model to balance the learning of both modalities and
harness their respective strengths. HCE-MoE, on the other hand, employs a
dynamic routing mechanism to distribute structurally varied experts (deep,
attention, and focal), fostering collaborative learning of complementary
semantics. This heterogeneous architecture systematically integrates diverse
feature extraction paradigms to comprehensively capture expression semantics.
Extensive experiments demonstrate that our proposed network achieves
competitive performance in the task of single-eye expression recognition,
especially under poor lighting conditions.

</details>

### [678] [Black-box Adversaries from Latent Space: Unnoticeable Attacks on Human Pose and Shape Estimation](https://arxiv.org/abs/2505.12009)
*Zhiying Li,Guanggang Geng,Yeying Jin,Zhizhi Guo,Bruce Gu,Jidong Huo,Zhaoxin Fan,Wenjun Wu*

Main category: cs.CV

TLDR: 提出了一种针对EHPS模型的新型不可察觉黑盒攻击（UBA），通过潜在空间表示生成对抗噪声，无需模型内部知识，显著增加姿态估计误差。


<details>
  <summary>Details</summary>
Motivation: 现有EHPS模型多关注估计精度而忽视安全漏洞，且现有对抗攻击需白盒访问或生成明显扰动，实用性不足。

Method: 利用自然图像的潜在空间表示生成对抗噪声，通过迭代优化攻击方向和噪声模式，仅依赖模型输出查询。

Result: UBA平均增加EHPS模型姿态估计误差17.27%-58.21%，揭示了严重安全漏洞。

Conclusion: 研究强调了数字人生成系统安全风险的紧迫性，需采取措施缓解。

Abstract: Expressive human pose and shape (EHPS) estimation is vital for digital human
generation, particularly in live-streaming applications. However, most existing
EHPS models focus primarily on minimizing estimation errors, with limited
attention on potential security vulnerabilities. Current adversarial attacks on
EHPS models often require white-box access (e.g., model details or gradients)
or generate visually conspicuous perturbations, limiting their practicality and
ability to expose real-world security threats. To address these limitations, we
propose a novel Unnoticeable Black-Box Attack (UBA) against EHPS models. UBA
leverages the latent-space representations of natural images to generate an
optimal adversarial noise pattern and iteratively refine its attack potency
along an optimized direction in digital space. Crucially, this process relies
solely on querying the model's output, requiring no internal knowledge of the
EHPS architecture, while guiding the noise optimization toward greater stealth
and effectiveness. Extensive experiments and visual analyses demonstrate the
superiority of UBA. Notably, UBA increases the pose estimation errors of EHPS
models by 17.27%-58.21% on average, revealing critical vulnerabilities. These
findings underscore the urgent need to address and mitigate security risks
associated with digital human generation systems.

</details>

### [679] [Cross-Model Transfer of Task Vectors via Few-Shot Orthogonal Alignment](https://arxiv.org/abs/2505.12021)
*Kazuhiko Kawamoto,Atsuhiro Endo,Hiroshi Kera*

Main category: cs.CV

TLDR: 论文提出了一种基于少样本正交对齐的方法，用于在不同预训练模型间对齐任务向量，提升跨模型迁移的准确性。


<details>
  <summary>Details</summary>
Motivation: 任务算术通常假设源模型和目标模型从相同的预训练参数初始化，限制了其在跨模型迁移场景中的应用。本文旨在解决这一问题。

Method: 采用少样本正交对齐方法，将任务向量对齐到不同预训练目标模型的参数空间，同时保留任务向量的关键属性（如范数和秩）。

Result: 实验表明，该方法在八个分类数据集上优于直接应用任务向量，性能接近少样本微调，同时保持了任务向量的模块化和可重用性。

Conclusion: 提出的方法有效解决了跨模型迁移中的任务向量对齐问题，提升了迁移准确性，且代码已开源。

Abstract: Task arithmetic enables efficient model editing by representing task-specific
changes as vectors in parameter space. Task arithmetic typically assumes that
the source and target models are initialized from the same pre-trained
parameters. This assumption limits its applicability in cross-model transfer
settings, where models are independently pre-trained on different datasets. To
address this challenge, we propose a method based on few-shot orthogonal
alignment, which aligns task vectors to the parameter space of a differently
pre-trained target model. These transformations preserve key properties of task
vectors, such as norm and rank, and are learned using only a small number of
labeled examples. We evaluate the method using two Vision Transformers
pre-trained on YFCC100M and LAION400M, and test on eight classification
datasets. Experimental results show that our method improves transfer accuracy
over direct task vector application and achieves performance comparable to
few-shot fine-tuning, while maintaining the modularity and reusability of task
vectors. Our code is available at
https://github.com/kawakera-lab/CrossModelTransfer.

</details>

### [680] [FIGhost: Fluorescent Ink-based Stealthy and Flexible Backdoor Attacks on Physical Traffic Sign Recognition](https://arxiv.org/abs/2505.12045)
*Shuai Yuan,Guowen Xu,Hongwei Li,Rui Zhang,Xinyuan Qian,Wenbo Jiang,Hangcheng Cao,Qingchuan Zhao*

Main category: cs.CV

TLDR: FIGhost是一种利用荧光墨水作为触发器的物理世界后门攻击方法，具有隐蔽性、灵活性和不可追踪性，适用于交通标志识别系统。


<details>
  <summary>Details</summary>
Motivation: 现有物理后门攻击缺乏隐蔽性或灵活性，且未考虑视觉-大语言模型（VLMs），因此需要一种更优的攻击方法。

Method: 采用荧光墨水作为触发器，结合基于插值的荧光模拟算法增强鲁棒性，并开发自动化后门样本生成方法。

Result: FIGhost在物理世界中有效攻击先进检测器和VLMs，且能抵抗环境变化并规避现有防御。

Conclusion: FIGhost为物理世界后门攻击提供了隐蔽、灵活且不可追踪的解决方案。

Abstract: Traffic sign recognition (TSR) systems are crucial for autonomous driving but
are vulnerable to backdoor attacks. Existing physical backdoor attacks either
lack stealth, provide inflexible attack control, or ignore emerging
Vision-Large-Language-Models (VLMs). In this paper, we introduce FIGhost, the
first physical-world backdoor attack leveraging fluorescent ink as triggers.
Fluorescent triggers are invisible under normal conditions and activated
stealthily by ultraviolet light, providing superior stealthiness, flexibility,
and untraceability. Inspired by real-world graffiti, we derive realistic
trigger shapes and enhance their robustness via an interpolation-based
fluorescence simulation algorithm. Furthermore, we develop an automated
backdoor sample generation method to support three attack objectives. Extensive
evaluations in the physical world demonstrate FIGhost's effectiveness against
state-of-the-art detectors and VLMs, maintaining robustness under environmental
variations and effectively evading existing defenses.

</details>

### [681] [Accelerating Diffusion-based Super-Resolution with Dynamic Time-Spatial Sampling](https://arxiv.org/abs/2505.12048)
*Rui Qin,Qijie Wang,Ming Sun,Haowei Zhu,Chao Zhou,Bin Wang*

Main category: cs.CV

TLDR: 本文提出了一种基于时间和空间感知的采样策略（TSS），用于加速扩散超分辨率（SR）任务，无需额外训练成本，显著减少迭代步骤并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散SR方法计算成本高，且通用加速技术未充分利用低层任务特性。本文通过分析扩散SR的频域和空间域特性，提出针对性优化策略。

Method: 提出TSS策略，结合时间动态采样（TDS）和空间动态采样（SDS），根据图像内容动态调整迭代和去噪策略。

Result: TSS在多个基准测试中表现优异，仅用一半迭代步骤即超越现有加速方法，MUSIQ分数提升0.2-3.0。

Conclusion: TSS通过时间和空间感知优化，显著提升扩散SR效率与性能，为低层视觉任务提供新思路。

Abstract: Diffusion models have gained attention for their success in modeling complex
distributions, achieving impressive perceptual quality in SR tasks. However,
existing diffusion-based SR methods often suffer from high computational costs,
requiring numerous iterative steps for training and inference. Existing
acceleration techniques, such as distillation and solver optimization, are
generally task-agnostic and do not fully leverage the specific characteristics
of low-level tasks like super-resolution (SR). In this study, we analyze the
frequency- and spatial-domain properties of diffusion-based SR methods,
revealing key insights into the temporal and spatial dependencies of
high-frequency signal recovery. Specifically, high-frequency details benefit
from concentrated optimization during early and late diffusion iterations,
while spatially textured regions demand adaptive denoising strategies. Building
on these observations, we propose the Time-Spatial-aware Sampling strategy
(TSS) for the acceleration of Diffusion SR without any extra training cost. TSS
combines Time Dynamic Sampling (TDS), which allocates more iterations to
refining textures, and Spatial Dynamic Sampling (SDS), which dynamically
adjusts strategies based on image content. Extensive evaluations across
multiple benchmarks demonstrate that TSS achieves state-of-the-art (SOTA)
performance with significantly fewer iterations, improving MUSIQ scores by 0.2
- 3.0 and outperforming the current acceleration methods with only half the
number of steps.

</details>

### [682] [VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption](https://arxiv.org/abs/2505.12053)
*Tianxiong Zhong,Xingye Tian,Boyuan Jiang,Xuebo Wang,Xin Tao,Pengfei Wan,Zhiwei Zhang*

Main category: cs.CV

TLDR: 论文提出VFRTok，一种基于Transformer的视频分词器，通过可变帧率编码和解码解决现有视频生成框架的效率问题，同时引入Partial RoPE提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成框架因固定时间压缩率导致计算成本随帧率线性增长，效率低下。

Method: 提出Duration-Proportional Information假设，设计VFRTok实现可变帧率编码，并引入Partial RoPE解耦位置与内容建模。

Result: VFRTok仅用1/8的token即可达到竞争性重建质量和最佳生成保真度。

Conclusion: VFRTok通过紧凑连续的时空表示，显著提升了视频生成效率和效果。

Abstract: Modern video generation frameworks based on Latent Diffusion Models suffer
from inefficiencies in tokenization due to the Frame-Proportional Information
Assumption. Existing tokenizers provide fixed temporal compression rates,
causing the computational cost of the diffusion model to scale linearly with
the frame rate. The paper proposes the Duration-Proportional Information
Assumption: the upper bound on the information capacity of a video is
proportional to the duration rather than the number of frames. Based on this
insight, the paper introduces VFRTok, a Transformer-based video tokenizer, that
enables variable frame rate encoding and decoding through asymmetric frame rate
training between the encoder and decoder. Furthermore, the paper proposes
Partial Rotary Position Embeddings (RoPE) to decouple position and content
modeling, which groups correlated patches into unified tokens. The Partial RoPE
effectively improves content-awareness, enhancing the video generation
capability. Benefiting from the compact and continuous spatio-temporal
representation, VFRTok achieves competitive reconstruction quality and
state-of-the-art generation fidelity while using only 1/8 tokens compared to
existing tokenizers.

</details>

### [683] [Beluga Whale Detection from Satellite Imagery with Point Labels](https://arxiv.org/abs/2505.12066)
*Yijie Zheng,Jinxuan Yang,Yu Chen,Yaxuan Wang,Yihang Lu,Guoqing Li*

Main category: cs.CV

TLDR: 该研究提出了一种自动化流程，利用点标注和Segment Anything Model（SAM）生成精确的边界框标注，用于训练YOLOv8进行多类别检测，显著提高了鲸鱼和海豹的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的鲸鱼检测方法需要大量人工标注，且常忽略不确定鲸鱼个体，限制了实际应用。

Method: 结合点标注和SAM生成精确标注，训练YOLOv8进行多类别检测。

Result: SAM生成的标注显著提升检测性能，鲸鱼总体F1分数达72.2%，海豹为70.3%。

Conclusion: 该方法减少了标注工作量，提升了对不确定鲸鱼的检测，为海洋动物监测提供了更全面的解决方案。

Abstract: Very high-resolution (VHR) satellite imagery has emerged as a powerful tool
for monitoring marine animals on a large scale. However, existing deep
learning-based whale detection methods usually require manually created,
high-quality bounding box annotations, which are labor-intensive to produce.
Moreover, existing studies often exclude ``uncertain whales'', individuals that
have ambiguous appearances in satellite imagery, limiting the applicability of
these models in real-world scenarios. To address these limitations, this study
introduces an automated pipeline for detecting beluga whales and harp seals in
VHR satellite imagery. The pipeline leverages point annotations and the Segment
Anything Model (SAM) to generate precise bounding box annotations, which are
used to train YOLOv8 for multiclass detection of certain whales, uncertain
whales, and harp seals. Experimental results demonstrated that SAM-generated
annotations significantly improved detection performance, achieving higher
$\text{F}_\text{1}$-scores compared to traditional buffer-based annotations.
YOLOv8 trained on SAM-labeled boxes achieved an overall
$\text{F}_\text{1}$-score of 72.2% for whales overall and 70.3% for harp seals,
with superior performance in dense scenes. The proposed approach not only
reduces the manual effort required for annotation but also enhances the
detection of uncertain whales, offering a more comprehensive solution for
marine animal monitoring. This method holds great potential for extending to
other species, habitats, and remote sensing platforms, as well as for
estimating whale biometrics, thereby advancing ecological monitoring and
conservation efforts. The codes for our label and detection pipeline are
publicly available at http://github.com/voyagerxvoyagerx/beluga-seeker .

</details>

### [684] [MT-CYP-Net: Multi-Task Network for Pixel-Level Crop Yield Prediction Under Very Few Samples](https://arxiv.org/abs/2505.12069)
*Shenzhou Liu,Di Wang,Haonan Guo,Chengxi Han,Wenzhi Zeng*

Main category: cs.CV

TLDR: MT-CYP-Net提出了一种多任务特征共享策略，通过共享主干网络的特征，同时用于作物产量预测和分类，解决了地面数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 解决基于卫星遥感数据的像素级产量预测因地面数据稀缺而受限的问题。

Method: 提出MT-CYP-Net框架，采用多任务特征共享策略，结合稀疏的产量点和作物类型标签进行训练。

Result: 实验表明MT-CYP-Net在多种作物上优于传统方法，展示了深度网络在有限标签数据下的潜力。

Conclusion: MT-CYP-Net为像素级作物产量预测提供了有效解决方案，尤其在数据标签有限的情况下。

Abstract: Accurate and fine-grained crop yield prediction plays a crucial role in
advancing global agriculture. However, the accuracy of pixel-level yield
estimation based on satellite remote sensing data has been constrained by the
scarcity of ground truth data. To address this challenge, we propose a novel
approach called the Multi-Task Crop Yield Prediction Network (MT-CYP-Net). This
framework introduces an effective multi-task feature-sharing strategy, where
features extracted from a shared backbone network are simultaneously utilized
by both crop yield prediction decoders and crop classification decoders with
the ability to fuse information between them. This design allows MT-CYP-Net to
be trained with extremely sparse crop yield point labels and crop type labels,
while still generating detailed pixel-level crop yield maps. Concretely, we
collected 1,859 yield point labels along with corresponding crop type labels
and satellite images from eight farms in Heilongjiang Province, China, in 2023,
covering soybean, maize, and rice crops, and constructed a sparse crop yield
label dataset. MT-CYP-Net is compared with three classical machine learning and
deep learning benchmark methods in this dataset. Experimental results not only
indicate the superiority of MT-CYP-Net compared to previous methods on multiple
types of crops but also demonstrate the potential of deep networks on precise
pixel-level crop yield prediction, especially with limited data labels.

</details>

### [685] [Denoising Mutual Knowledge Distillation in Bi-Directional Multiple Instance Learning](https://arxiv.org/abs/2505.12074)
*Chen Shu,Boyu Fu,Yiman Li,Ting Yin,Wenchuan Zhang,Jie Chen,Yuhao Yi,Hong Bu*

Main category: cs.CV

TLDR: 该论文提出了一种结合伪标签校正的方法，以改进多实例学习（MIL）在病理图像分类中的性能，弥补MIL与全监督学习之间的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管MIL避免了细粒度标注的需求，但其在袋级和实例级分类的准确性仍存疑。现有方法可能引入噪声标签，因此需要一种更鲁棒的学习方法。

Method: 通过结合伪标签校正技术，增强袋级和实例级学习过程，从弱到强泛化。

Result: 在公共病理数据集上的实验表明，该方法在袋级和实例级预测上均优于现有MIL算法。

Conclusion: 提出的方法有效提升了MIL算法的性能，为病理图像分类提供了一种更可靠的解决方案。

Abstract: Multiple Instance Learning is the predominant method for Whole Slide Image
classification in digital pathology, enabling the use of slide-level labels to
supervise model training. Although MIL eliminates the tedious fine-grained
annotation process for supervised learning, whether it can learn accurate bag-
and instance-level classifiers remains a question. To address the issue,
instance-level classifiers and instance masks were incorporated to ground the
prediction on supporting patches. These methods, while practically improving
the performance of MIL methods, may potentially introduce noisy labels. We
propose to bridge the gap between commonly used MIL and fully supervised
learning by augmenting both the bag- and instance-level learning processes with
pseudo-label correction capabilities elicited from weak to strong
generalization techniques. The proposed algorithm improves the performance of
dual-level MIL algorithms on both bag- and instance-level predictions.
Experiments on public pathology datasets showcase the advantage of the proposed
methods.

</details>

### [686] [VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.12081)
*Yuqi Liu,Tianyuan Qu,Zhisheng Zhong,Bohao Peng,Shu Liu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TLDR: VisionReasoner是一个统一框架，通过多目标认知学习策略和任务重构，增强视觉推理能力，在检测、分割和计数任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决视觉感知任务的多样性，提出一个统一框架以提升多任务处理能力。

Method: 设计多目标认知学习策略和系统任务重构，生成结构化推理过程。

Result: 在COCO、ReasonSeg和CountBench任务中分别优于Qwen2.5VL 29.1%、22.1%和15.3%。

Conclusion: VisionReasoner作为统一模型，在多任务视觉感知中表现卓越。

Abstract: Large vision-language models exhibit inherent capabilities to handle diverse
visual perception tasks. In this paper, we introduce VisionReasoner, a unified
framework capable of reasoning and solving multiple visual perception tasks
within a shared model. Specifically, by designing novel multi-object cognitive
learning strategies and systematic task reformulation, VisionReasoner enhances
its reasoning capabilities to analyze visual inputs, and addresses diverse
perception tasks in a unified framework. The model generates a structured
reasoning process before delivering the desired outputs responding to user
queries. To rigorously assess unified visual perception capabilities, we
evaluate VisionReasoner on ten diverse tasks spanning three critical domains:
detection, segmentation, and counting. Experimental results show that
VisionReasoner achieves superior performance as a unified model, outperforming
Qwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg
(segmentation), and 15.3% on CountBench (counting).

</details>

### [687] [LOVE: Benchmarking and Evaluating Text-to-Video Generation and Video-to-Text Interpretation](https://arxiv.org/abs/2505.12098)
*Jiarui Wang,Huiyu Duan,Ziheng Jia,Yu Zhao,Woo Yi Yang,Zicheng Zhang,Zijian Chen,Juntong Wang,Yuke Xing,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TLDR: 论文提出了AIGVE-60K数据集和LOVE评估指标，用于评估AI生成视频的质量和文本-视频对齐性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成视频在感知质量和文本-视频对齐方面存在不足，需要可靠且可扩展的自动评估模型。

Method: 构建了包含60K人类标注的AIGVE-60K数据集，并提出基于LMM的LOVE评估指标。

Result: LOVE在AIGVE-60K上表现优异，并能泛化到其他AIGV评估基准。

Conclusion: AIGVE-60K数据集和LOVE指标为AI生成视频的评估提供了重要工具。

Abstract: Recent advancements in large multimodal models (LMMs) have driven substantial
progress in both text-to-video (T2V) generation and video-to-text (V2T)
interpretation tasks. However, current AI-generated videos (AIGVs) still
exhibit limitations in terms of perceptual quality and text-video alignment.
Therefore, a reliable and scalable automatic model for AIGV evaluation is
desirable, which heavily relies on the scale and quality of human annotations.
To this end, we present AIGVE-60K, a comprehensive dataset and benchmark for
AI-Generated Video Evaluation, which features (i) comprehensive tasks,
encompassing 3,050 extensive prompts across 20 fine-grained task dimensions,
(ii) the largest human annotations, including 120K mean-opinion scores (MOSs)
and 60K question-answering (QA) pairs annotated on 58,500 videos generated from
30 T2V models, and (iii) bidirectional benchmarking and evaluating for both T2V
generation and V2T interpretation capabilities. Based on AIGVE-60K, we propose
LOVE, a LMM-based metric for AIGV Evaluation from multiple dimensions including
perceptual preference, text-video correspondence, and task-specific accuracy in
terms of both instance level and model level. Comprehensive experiments
demonstrate that LOVE not only achieves state-of-the-art performance on the
AIGVE-60K dataset, but also generalizes effectively to a wide range of other
AIGV evaluation benchmarks. These findings highlight the significance of the
AIGVE-60K dataset. Database and codes are anonymously available at
https://github.com/IntMeGroup/LOVE.

</details>

### [688] [TinyRS-R1: Compact Multimodal Language Model for Remote Sensing](https://arxiv.org/abs/2505.12099)
*Aybora Koksal,A. Aydin Alatan*

Main category: cs.CV

TLDR: TinyRS是一种2B参数的多模态小语言模型（MSLM），专为遥感任务优化，其推理增强版本TinyRS-R1在性能上媲美或超越7B参数模型，同时内存和延迟仅为三分之一。


<details>
  <summary>Details</summary>
Motivation: 解决边缘硬件无法承载大型多模态语言模型的问题，为遥感任务提供高效的小型模型。

Method: 通过四阶段训练流程：卫星图像预训练、视觉指令微调、推理数据集CoT微调、GRPO对齐。

Result: TinyRS-R1在分类、VQA、视觉定位和开放问答任务中表现优异，且资源消耗更低。

Conclusion: TinyRS-R1是首个针对遥感任务的GRPO对齐CoT推理MSLM，CoT显著提升空间定位和场景理解能力。

Abstract: Remote-sensing applications often run on edge hardware that cannot host
today's 7B-parameter multimodal language models. This paper introduces TinyRS,
the first 2B-parameter multimodal small language model (MSLM) optimized for
remote sensing tasks, and TinyRS-R1, its reasoning-augmented variant. Built
upon Qwen2-VL-2B, TinyRS is trained through a four-stage pipeline: pre-training
on million satellite images, instruction tuning on visual instruction examples,
fine-tuning with Chain-of-Thought (CoT) annotations from the proposed reasoning
dataset, and alignment via Group Relative Policy Optimization (GRPO). TinyRS-R1
achieves or surpasses the performance of recent 7B-parameter remote sensing
models across classification, VQA, visual grounding, and open-ended question
answering-while requiring just one-third of the memory and latency. Our
analysis shows that CoT reasoning substantially benefits spatial grounding and
scene understanding, while the non-reasoning TinyRS excels in concise,
latency-sensitive VQA tasks. TinyRS-R1 represents the first domain-specialized
MSLM with GRPO-aligned CoT reasoning for general-purpose remote sensing.

</details>

### [689] [EarthSynth: Generating Informative Earth Observation with Diffusion Models](https://arxiv.org/abs/2505.12108)
*Jiancheng Pan,Shiye Lei,Yuqian Fu,Jiahao Li,Yanxing Liu,Yuze Sun,Xiao He,Long Peng,Xiaomeng Huang,Bo Zhao*

Main category: cs.CV

TLDR: EarthSynth是一种基于扩散的生成基础模型，用于合成多类别、跨卫星标注的遥感图像，以解决标注数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 遥感图像解释任务因标注数据稀缺而受限，EarthSynth旨在通过生成合成数据提升性能。

Method: 采用Counterfactual Composition训练策略和R-Filter规则方法，提升数据多样性和类别控制。

Result: 在场景分类、目标检测和语义分割任务中表现优异，为开放世界场景提供实用解决方案。

Conclusion: EarthSynth是首个探索多任务生成的遥感模型，为遥感图像解释任务提供了新思路。

Abstract: Remote sensing image (RSI) interpretation typically faces challenges due to
the scarcity of labeled data, which limits the performance of RSI
interpretation tasks. To tackle this challenge, we propose EarthSynth, a
diffusion-based generative foundation model that enables synthesizing
multi-category, cross-satellite labeled Earth observation for downstream RSI
interpretation tasks. To the best of our knowledge, EarthSynth is the first to
explore multi-task generation for remote sensing. EarthSynth, trained on the
EarthSynth-180K dataset, employs the Counterfactual Composition training
strategy to improve training data diversity and enhance category control.
Furthermore, a rule-based method of R-Filter is proposed to filter more
informative synthetic data for downstream tasks. We evaluate our EarthSynth on
scene classification, object detection, and semantic segmentation in open-world
scenarios, offering a practical solution for advancing RSI interpretation.

</details>

### [690] [Keypoints as Dynamic Centroids for Unified Human Pose and Segmentation](https://arxiv.org/abs/2505.12130)
*Niaz Ahmad,Jawad Khan,Kang G. Shin,Youngmoon Lee,Guanghui Wang*

Main category: cs.CV

TLDR: 提出了一种基于动态质心的新方法（KDC），用于统一的人体姿态估计和实例级分割，解决了重叠关节和快速变化姿态的挑战。


<details>
  <summary>Details</summary>
Motivation: 动态人体运动对姿态估计和分割提出了挑战，现有方法在复杂场景中表现不佳。

Method: 采用自下而上的范式生成关键点热图，引入KeyCentroids和MaskCentroids动态聚类像素。

Result: 在CrowdPose、OCHuman和COCO基准测试中表现出色，兼顾准确性和运行效率。

Conclusion: KDC方法在复杂场景中具有高效性和通用性，为实时环境提供了有效解决方案。

Abstract: The dynamic movement of the human body presents a fundamental challenge for
human pose estimation and body segmentation. State-of-the-art approaches
primarily rely on combining keypoint heatmaps with segmentation masks but often
struggle in scenarios involving overlapping joints or rapidly changing poses
during instance-level segmentation. To address these limitations, we propose
Keypoints as Dynamic Centroid (KDC), a new centroid-based representation for
unified human pose estimation and instance-level segmentation. KDC adopts a
bottom-up paradigm to generate keypoint heatmaps for both easily
distinguishable and complex keypoints and improves keypoint detection and
confidence scores by introducing KeyCentroids using a keypoint disk. It
leverages high-confidence keypoints as dynamic centroids in the embedding space
to generate MaskCentroids, allowing for swift clustering of pixels to specific
human instances during rapid body movements in live environments. Our
experimental evaluations on the CrowdPose, OCHuman, and COCO benchmarks
demonstrate KDC's effectiveness and generalizability in challenging scenarios
in terms of both accuracy and runtime performance. The implementation is
available at: https://sites.google.com/view/niazahmad/projects/kdc.

</details>

### [691] [Learning to Highlight Audio by Watching Movies](https://arxiv.org/abs/2505.12154)
*Chao Huang,Ruohan Gao,J. M. F. Tsang,Jan Kurcius,Cagdas Bilen,Chenliang Xu,Anurag Kumar,Sanjeel Parekh*

Main category: cs.CV

TLDR: 论文提出了一种基于视觉引导的音频高亮任务，通过Transformer多模态框架解决，并引入新数据集和伪数据生成方法，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 视频内容创作中视觉与音频的协调不足，导致视听体验不和谐，需填补这一空白。

Method: 采用Transformer多模态框架，利用电影数据生成伪数据模拟真实场景，通过分离、调整和重混三步训练模型。

Result: 方法在定量和主观评估中均优于基线，并系统研究了上下文引导和数据集难度的影响。

Conclusion: 提出的任务和框架有效提升了视听协调性，为未来研究提供了新方向。

Abstract: Recent years have seen a significant increase in video content creation and
consumption. Crafting engaging content requires the careful curation of both
visual and audio elements. While visual cue curation, through techniques like
optimal viewpoint selection or post-editing, has been central to media
production, its natural counterpart, audio, has not undergone equivalent
advancements. This often results in a disconnect between visual and acoustic
saliency. To bridge this gap, we introduce a novel task: visually-guided
acoustic highlighting, which aims to transform audio to deliver appropriate
highlighting effects guided by the accompanying video, ultimately creating a
more harmonious audio-visual experience. We propose a flexible,
transformer-based multimodal framework to solve this task. To train our model,
we also introduce a new dataset -- the muddy mix dataset, leveraging the
meticulous audio and video crafting found in movies, which provides a form of
free supervision. We develop a pseudo-data generation process to simulate
poorly mixed audio, mimicking real-world scenarios through a three-step process
-- separation, adjustment, and remixing. Our approach consistently outperforms
several baselines in both quantitative and subjective evaluation. We also
systematically study the impact of different types of contextual guidance and
difficulty levels of the dataset. Our project page is here:
https://wikichao.github.io/VisAH/.

</details>

### [692] [SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds](https://arxiv.org/abs/2505.12155)
*Ranit Karmakar,Simon F. Nørrelykke*

Main category: cs.CV

TLDR: SoftPQ是一种灵活的实例分割评估指标，通过引入可调阈值和部分匹配区域，解决了传统二元评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统分割评估指标依赖二元逻辑，忽略了实例级结构和渐进改进的重要性。

Method: 提出SoftPQ，使用可调上下IoU阈值和部分匹配区域，并应用次线性惩罚函数处理模糊或碎片化预测。

Result: SoftPQ能更平滑地反映分割质量差异，对结构错误更鲁棒，并提供更有用的模型反馈。

Conclusion: SoftPQ是一种实用且原则性的替代方案，适用于基准测试和模型迭代优化。

Abstract: Segmentation evaluation metrics traditionally rely on binary decision logic:
predictions are either correct or incorrect, based on rigid IoU thresholds.
Detection--based metrics such as F1 and mAP determine correctness at the object
level using fixed overlap cutoffs, while overlap--based metrics like
Intersection over Union (IoU) and Dice operate at the pixel level, often
overlooking instance--level structure. Panoptic Quality (PQ) attempts to unify
detection and segmentation assessment, but it remains dependent on
hard-threshold matching--treating predictions below the threshold as entirely
incorrect. This binary framing obscures important distinctions between
qualitatively different errors and fails to reward gradual model improvements.
We propose SoftPQ, a flexible and interpretable instance segmentation metric
that redefines evaluation as a graded continuum rather than a binary
classification. SoftPQ introduces tunable upper and lower IoU thresholds to
define a partial matching region and applies a sublinear penalty function to
ambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit
smoother score behavior, greater robustness to structural segmentation errors,
and more informative feedback for model development and evaluation. Through
controlled perturbation experiments, we show that SoftPQ captures meaningful
differences in segmentation quality that existing metrics overlook, making it a
practical and principled alternative for both benchmarking and iterative model
refinement.

</details>

### [693] [Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum](https://arxiv.org/abs/2505.12191)
*Wenquan Lu,Jiaqi Zhang,Hugues Van Assel,Randall Balestriero*

Main category: cs.CV

TLDR: 提出了一种自监督学习框架，能够在噪声数据上学习鲁棒表示，无需推理时的去噪器或下游微调。


<details>
  <summary>Details</summary>
Motivation: 自监督学习（SSL）通常针对干净数据，而噪声数据（如天体物理学、医学影像等）的应用仍具挑战性。

Method: 首先训练一个SSL去噪器，然后构建从去噪到噪声的数据课程，结合教师引导的正则化，使模型内化噪声鲁棒性。

Result: 在极端高斯噪声下（SNR=0.72 dB），线性探测准确率比DINOv2提高4.8%。

Conclusion: 该方法通过噪声感知预训练实现了无需去噪器的鲁棒性，简化了部署。

Abstract: Self-Supervised Learning (SSL) has become a powerful solution to extract rich
representations from unlabeled data. Yet, SSL research is mostly focused on
clean, curated and high-quality datasets. As a result, applying SSL on noisy
data remains a challenge, despite being crucial to applications such as
astrophysics, medical imaging, geophysics or finance. In this work, we present
a fully self-supervised framework that enables noise-robust representation
learning without requiring a denoiser at inference or downstream fine-tuning.
Our method first trains an SSL denoiser on noisy data, then uses it to
construct a denoised-to-noisy data curriculum (i.e., training first on
denoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),
combined with a teacher-guided regularization that anchors noisy embeddings to
their denoised counterparts. This process encourages the model to internalize
noise robustness. Notably, the denoiser can be discarded after pretraining,
simplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise
($\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by
4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from
noise-aware pretraining. The code is available at
https://github.com/wenquanlu/noisy_dinov2.

</details>

### [694] [Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather](https://arxiv.org/abs/2505.12199)
*Kui Jiang,Jing Cao,Zhaocheng Yu,Junjun Jiang,Jingchun Zhou*

Main category: cs.CV

TLDR: 提出了一种名为ACDepth的鲁棒单目深度估计方法，通过高质量训练数据生成和域适应提升在恶劣天气下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在恶劣天气下性能下降，主要由于领域偏移和场景信息提取困难。

Method: 使用一步扩散模型生成模拟恶劣天气的样本，构建多组退化数据集；采用LoRA适配器微调扩散模型；结合循环一致性损失和对抗训练；提出多粒度知识蒸馏策略（MKD）和有序引导蒸馏机制（OGD）。

Result: 在nuScenes数据集上，ACDepth在夜间和雨天场景的absRel指标上分别超过md4all-DD 2.50%和2.61%。

Conclusion: ACDepth通过数据生成和知识蒸馏策略有效提升了恶劣天气下的深度估计性能。

Abstract: Monocular depth estimation is critical for applications such as autonomous
driving and scene reconstruction. While existing methods perform well under
normal scenarios, their performance declines in adverse weather, due to
challenging domain shifts and difficulties in extracting scene information. To
address this issue, we present a robust monocular depth estimation method
called \textbf{ACDepth} from the perspective of high-quality training data
generation and domain adaptation. Specifically, we introduce a one-step
diffusion model for generating samples that simulate adverse weather
conditions, constructing a multi-tuple degradation dataset during training. To
ensure the quality of the generated degradation samples, we employ LoRA
adapters to fine-tune the generation weights of diffusion model. Additionally,
we integrate circular consistency loss and adversarial training to guarantee
the fidelity and naturalness of the scene contents. Furthermore, we elaborate
on a multi-granularity knowledge distillation strategy (MKD) that encourages
the student network to absorb knowledge from both the teacher model and
pretrained Depth Anything V2. This strategy guides the student model in
learning degradation-agnostic scene information from various degradation
inputs. In particular, we introduce an ordinal guidance distillation mechanism
(OGD) that encourages the network to focus on uncertain regions through
differential ranking, leading to a more precise depth estimation. Experimental
results demonstrate that our ACDepth surpasses md4all-DD by 2.50\% for night
scene and 2.61\% for rainy scene on the nuScenes dataset in terms of the absRel
metric.

</details>

### [695] [CompBench: Benchmarking Complex Instruction-guided Image Editing](https://arxiv.org/abs/2505.12200)
*Bohan Jia,Wenxuan Huang,Yuntian Tang,Junbo Qiao,Jincheng Liao,Shaosheng Cao,Fei Zhao,Zhaopeng Feng,Zhouhong Gu,Zhenfei Yin,Lei Bai,Wanli Ouyang,Lin Chen,Fei Zhao,Zihan Wang,Yuan Xie,Shaohui Lin*

Main category: cs.CV

TLDR: 该论文提出了一个名为CompBench的大规模基准测试，用于复杂指令引导的图像编辑，填补了现有基准测试在任务复杂性和细粒度指令上的不足。


<details>
  <summary>Details</summary>
Motivation: 现实应用对复杂场景操作的需求日益增长，而现有基准测试往往过于简化任务且缺乏细粒度指令。

Method: 提出了一个多模态语言模型（MLLM）与人类协作的框架，并采用指令解耦策略，将编辑意图分为位置、外观、动态和对象四个维度。

Result: CompBench揭示了当前图像编辑模型的基本局限性，并为下一代系统的开发提供了关键见解。

Conclusion: CompBench为复杂指令引导的图像编辑提供了全面的评估工具，推动了该领域的发展。

Abstract: While real-world applications increasingly demand intricate scene
manipulation, existing instruction-guided image editing benchmarks often
oversimplify task complexity and lack comprehensive, fine-grained instructions.
To bridge this gap, we introduce, a large-scale benchmark specifically designed
for complex instruction-guided image editing. CompBench features challenging
editing scenarios that incorporate fine-grained instruction following, spatial
and contextual reasoning, thereby enabling comprehensive evaluation of image
editing models' precise manipulation capabilities. To construct CompBench, We
propose an MLLM-human collaborative framework with tailored task pipelines.
Furthermore, we propose an instruction decoupling strategy that disentangles
editing intents into four key dimensions: location, appearance, dynamics, and
objects, ensuring closer alignment between instructions and complex editing
requirements. Extensive evaluations reveal that CompBench exposes fundamental
limitations of current image editing models and provides critical insights for
the development of next-generation instruction-guided image editing systems.

</details>

### [696] [Is Artificial Intelligence Generated Image Detection a Solved Problem?](https://arxiv.org/abs/2505.12335)
*Ziqiang Li,Jiazhen Yan,Ziwen He,Kai Zeng,Weiwei Jiang,Lizhi Xiong,Zhangjie Fu*

Main category: cs.CV

TLDR: AIGIBench是一个用于评估AI生成图像检测器鲁棒性和泛化能力的综合基准，揭示了现有检测器在真实场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如GANs和Diffusion模型）的快速发展导致高度逼真的合成图像泛滥，引发了对错误信息、深度伪造和版权侵权的担忧。尽管已有许多AI生成图像检测器，但其在真实场景中的有效性存疑。

Method: AIGIBench通过四个核心任务（多源泛化、图像退化鲁棒性、数据增强敏感性和测试时预处理影响）评估11种先进检测器，涵盖23种多样化的伪造图像子集和真实样本。

Result: 实验表明，尽管检测器在受控环境中表现优异，但在真实数据上性能显著下降，且常见增强和预处理效果有限。

Conclusion: AIGIBench为未来研究提供了统一且现实的评估框架，强调需要更鲁棒的检测策略。

Abstract: The rapid advancement of generative models, such as GANs and Diffusion
models, has enabled the creation of highly realistic synthetic images, raising
serious concerns about misinformation, deepfakes, and copyright infringement.
Although numerous Artificial Intelligence Generated Image (AIGI) detectors have
been proposed, often reporting high accuracy, their effectiveness in real-world
scenarios remains questionable. To bridge this gap, we introduce AIGIBench, a
comprehensive benchmark designed to rigorously evaluate the robustness and
generalization capabilities of state-of-the-art AIGI detectors. AIGIBench
simulates real-world challenges through four core tasks: multi-source
generalization, robustness to image degradation, sensitivity to data
augmentation, and impact of test-time pre-processing. It includes 23 diverse
fake image subsets that span both advanced and widely adopted image generation
techniques, along with real-world samples collected from social media and AI
art platforms. Extensive experiments on 11 advanced detectors demonstrate that,
despite their high reported accuracy in controlled settings, these detectors
suffer significant performance drops on real-world data, limited benefits from
common augmentations, and nuanced effects of pre-processing, highlighting the
need for more robust detection strategies. By providing a unified and realistic
evaluation framework, AIGIBench offers valuable insights to guide future
research toward dependable and generalizable AIGI detection.

</details>

### [697] [Road Segmentation for ADAS/AD Applications](https://arxiv.org/abs/2505.12206)
*Mathanesh Vellingiri Ramasamy,Dimas Rizky Kurniasalim*

Main category: cs.CV

TLDR: 研究探讨了模型架构和数据集选择对道路分割的影响，比较了VGG-16和U-Net在Comma10k和KITTI Road数据集上的表现，发现VGG-16在跨数据集测试中优于U-Net。


<details>
  <summary>Details</summary>
Motivation: 精确的道路分割对自动驾驶和ADAS至关重要，研究旨在分析模型架构和数据集对分割效果的影响。

Method: 使用改进的VGG-16在Comma10k数据集和U-Net在KITTI Road数据集上进行训练，并通过F1分数、mIoU和精度等指标评估性能。

Result: VGG-16在跨数据集测试中表现优于U-Net，尽管U-Net训练了更多轮次。

Conclusion: 模型架构和数据集选择对道路分割性能有显著影响，VGG-16在跨数据集测试中更具优势。

Abstract: Accurate road segmentation is essential for autonomous driving and ADAS,
enabling effective navigation in complex environments. This study examines how
model architecture and dataset choice affect segmentation by training a
modified VGG-16 on the Comma10k dataset and a modified U-Net on the KITTI Road
dataset. Both models achieved high accuracy, with cross-dataset testing showing
VGG-16 outperforming U-Net despite U-Net being trained for more epochs. We
analyze model performance using metrics such as F1-score, mean intersection
over union, and precision, discussing how architecture and dataset impact
results.

</details>

### [698] [Can Large Multimodal Models Understand Agricultural Scenes? Benchmarking with AgroMind](https://arxiv.org/abs/2505.12207)
*Qingmei Li,Yang Zhang,Zurong Mai,Yuhang Chen,Shuohong Lou,Henglian Huang,Jiarui Zhang,Zhiwei Zhang,Yibin Wen,Weijia Li,Haohuan Fu,Jianxi Huang,Juepeng Zheng*

Main category: cs.CV

TLDR: AgroMind是一个全面的农业遥感基准测试，涵盖四个任务维度，评估了18个开源和3个闭源LMMs，揭示了其在空间推理和细粒度识别上的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有农业遥感基准测试存在场景多样性和任务设计不足的问题，AgroMind旨在填补这一空白。

Method: 通过整合多个数据集生成高质量评估集，定义多样化任务，并使用LMMs进行推理和评估。

Result: 实验显示LMMs在空间推理和细粒度识别上表现不佳，部分领先模型甚至超过人类表现。

Conclusion: AgroMind为农业遥感提供了标准化评估框架，揭示了LMMs的局限性，并指出了未来研究方向。

Abstract: Large Multimodal Models (LMMs) has demonstrated capabilities across various
domains, but comprehensive benchmarks for agricultural remote sensing (RS)
remain scarce. Existing benchmarks designed for agricultural RS scenarios
exhibit notable limitations, primarily in terms of insufficient scene diversity
in the dataset and oversimplified task design. To bridge this gap, we introduce
AgroMind, a comprehensive agricultural remote sensing benchmark covering four
task dimensions: spatial perception, object understanding, scene understanding,
and scene reasoning, with a total of 13 task types, ranging from crop
identification and health monitoring to environmental analysis. We curate a
high-quality evaluation set by integrating eight public datasets and one
private farmland plot dataset, containing 25,026 QA pairs and 15,556 images.
The pipeline begins with multi-source data preprocessing, including collection,
format standardization, and annotation refinement. We then generate a diverse
set of agriculturally relevant questions through the systematic definition of
tasks. Finally, we employ LMMs for inference, generating responses, and
performing detailed examinations. We evaluated 18 open-source LMMs and 3
closed-source models on AgroMind. Experiments reveal significant performance
gaps, particularly in spatial reasoning and fine-grained recognition, it is
notable that human performance lags behind several leading LMMs. By
establishing a standardized evaluation framework for agricultural RS, AgroMind
reveals the limitations of LMMs in domain knowledge and highlights critical
challenges for future work. Data and code can be accessed at
https://rssysu.github.io/AgroMind/.

</details>

### [699] [Hyperspectral Image Land Cover Captioning Dataset for Vision Language Models](https://arxiv.org/abs/2505.12217)
*Aryan Das,Tanishq Rachamalla,Pravendra Singh,Koushik Biswas,Vinay Kumar Verma,Swalpa Kumar Roy*

Main category: cs.CV

TLDR: HyperCap是首个大规模高光谱字幕数据集，结合光谱数据与像素级文本注释，提升遥感应用中的模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统高光谱数据集仅关注分类任务，HyperCap通过引入文本注释，增强高光谱图像的语义理解。

Method: 数据集基于四个基准数据集构建，采用自动与人工结合的混合标注方法确保准确性。

Result: 实验表明，使用先进编码器和融合技术显著提升了分类性能。

Conclusion: HyperCap为高光谱视觉语言学习奠定基础，是未来研究的重要资源。

Abstract: We introduce HyperCap, the first large-scale hyperspectral captioning dataset
designed to enhance model performance and effectiveness in remote sensing
applications. Unlike traditional hyperspectral imaging (HSI) datasets that
focus solely on classification tasks, HyperCap integrates spectral data with
pixel-wise textual annotations, enabling deeper semantic understanding of
hyperspectral imagery. This dataset enhances model performance in tasks like
classification and feature extraction, providing a valuable resource for
advanced remote sensing applications. HyperCap is constructed from four
benchmark datasets and annotated through a hybrid approach combining automated
and manual methods to ensure accuracy and consistency. Empirical evaluations
using state-of-the-art encoders and diverse fusion techniques demonstrate
significant improvements in classification performance. These results
underscore the potential of vision-language learning in HSI and position
HyperCap as a foundational dataset for future research in the field.

</details>

### [700] [From Low Field to High Value: Robust Cortical Mapping from Low-Field MRI](https://arxiv.org/abs/2505.12228)
*Karthik Gopinath,Annabel Sorby-Adams,Jonathan W. Ramirez,Dina Zemlyanker,Jennifer Guo,David Hunt,Christine L. Mac Donald,C. Dirk Keene,Timothy Coalson,Matthew F. Glasser,David Van Essen,Matthew S. Rosen,Oula Puonti,W. Taylor Kimberly,Juan Eugenio Iglesias*

Main category: cs.CV

TLDR: 提出了一种基于机器学习的3D重建方法，用于便携式低场MRI的皮质表面分析，无需重新训练即可适应不同对比度和分辨率。


<details>
  <summary>Details</summary>
Motivation: 高场MRI（HF-MRI）在研究和临床中受限，而低场MRI（LF-MRI）成本低且易获取，但现有工具难以处理其低信噪比和分辨率问题。

Method: 使用3D U-Net从合成的LF-MRI预测皮质表面的有符号距离函数，并通过几何处理确保拓扑准确性。

Result: 在3mm各向同性T2加权扫描中，与HF-MRI重建结果高度一致（表面面积r=0.96，皮质分区Dice=0.98，灰质体积r=0.93）。

Conclusion: 该方法为便携式LF-MRI的皮质表面分析提供了可行方案，代码已开源。

Abstract: Three-dimensional reconstruction of cortical surfaces from MRI for
morphometric analysis is fundamental for understanding brain structure. While
high-field MRI (HF-MRI) is standard in research and clinical settings, its
limited availability hinders widespread use. Low-field MRI (LF-MRI),
particularly portable systems, offers a cost-effective and accessible
alternative. However, existing cortical surface analysis tools are optimized
for high-resolution HF-MRI and struggle with the lower signal-to-noise ratio
and resolution of LF-MRI. In this work, we present a machine learning method
for 3D reconstruction and analysis of portable LF-MRI across a range of
contrasts and resolutions. Our method works "out of the box" without
retraining. It uses a 3D U-Net trained on synthetic LF-MRI to predict signed
distance functions of cortical surfaces, followed by geometric processing to
ensure topological accuracy. We evaluate our method using paired HF/LF-MRI
scans of the same subjects, showing that LF-MRI surface reconstruction accuracy
depends on acquisition parameters, including contrast type (T1 vs T2),
orientation (axial vs isotropic), and resolution. A 3mm isotropic T2-weighted
scan acquired in under 4 minutes, yields strong agreement with HF-derived
surfaces: surface area correlates at r=0.96, cortical parcellations reach
Dice=0.98, and gray matter volume achieves r=0.93. Cortical thickness remains
more challenging with correlations up to r=0.70, reflecting the difficulty of
sub-mm precision with 3mm voxels. We further validate our method on challenging
postmortem LF-MRI, demonstrating its robustness. Our method represents a step
toward enabling cortical surface analysis on portable LF-MRI. Code is available
at https://surfer.nmr.mgh.harvard.edu/fswiki/ReconAny

</details>

### [701] [NOFT: Test-Time Noise Finetune via Information Bottleneck for Highly Correlated Asset Creation](https://arxiv.org/abs/2505.12235)
*Jia Li,Nan Gao,Huaibo Huang,Ran He*

Main category: cs.CV

TLDR: 提出了一种名为NOFT的即插即用噪声微调模块，用于Stable Diffusion，通过优化传输信息瓶颈（OT-IB）微调种子噪声或逆噪声，以生成高度相关且多样化的图像。


<details>
  <summary>Details</summary>
Motivation: 现有T2I和I2I扩散方法未充分利用压缩上下文潜在信息，且噪声潜在能隐式表示图像的拓扑和纹理流形。

Method: 使用OT-IB微调种子噪声或逆噪声，仅需约14K可训练参数和10分钟训练时间。

Result: NOFT在测试时能生成高保真图像变体，同时考虑拓扑和纹理对齐。

Conclusion: NOFT是一种高效的通用重新想象方法，适用于基于文本或图像引导的2D/3D AIGC资产微调。

Abstract: The diffusion model has provided a strong tool for implementing text-to-image
(T2I) and image-to-image (I2I) generation. Recently, topology and texture
control are popular explorations, e.g., ControlNet, IP-Adapter, Ctrl-X, and
DSG. These methods explicitly consider high-fidelity controllable editing based
on external signals or diffusion feature manipulations. As for diversity, they
directly choose different noise latents. However, the diffused noise is capable
of implicitly representing the topological and textural manifold of the
corresponding image. Moreover, it's an effective workbench to conduct the
trade-off between content preservation and controllable variations. Previous
T2I and I2I diffusion works do not explore the information within the
compressed contextual latent. In this paper, we first propose a plug-and-play
noise finetune NOFT module employed by Stable Diffusion to generate highly
correlated and diverse images. We fine-tune seed noise or inverse noise through
an optimal-transported (OT) information bottleneck (IB) with around only 14K
trainable parameters and 10 minutes of training. Our test-time NOFT is good at
producing high-fidelity image variations considering topology and texture
alignments. Comprehensive experiments demonstrate that NOFT is a powerful
general reimagine approach to efficiently fine-tune the 2D/3D AIGC assets with
text or image guidance.

</details>

### [702] [From Shots to Stories: LLM-Assisted Video Editing with Unified Language Representations](https://arxiv.org/abs/2505.12237)
*Yuzhi Li,Haojun Xu,Feng Tian*

Main category: cs.CV

TLDR: 本文首次系统研究了LLMs在视频编辑中的应用，提出L-Storyboard作为中间表示，将视频片段转化为结构化语言描述，并通过StoryFlow策略提升任务准确性和逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs和VLMs在视频理解中表现出强大的推理和泛化能力，但在视频编辑中的应用尚未充分探索。本文旨在填补这一空白。

Method: 引入L-Storyboard作为中间表示，将视频片段转化为语言描述，并提出StoryFlow策略以解决发散任务的不稳定性。

Result: 实验表明，L-Storyboard显著提升了视觉信息与语言描述的映射效果，StoryFlow增强了逻辑一致性和输出稳定性。

Conclusion: LLMs在智能视频编辑中具有巨大潜力，L-Storyboard和StoryFlow为相关任务提供了有效解决方案。

Abstract: Large Language Models (LLMs) and Vision-Language Models (VLMs) have
demonstrated remarkable reasoning and generalization capabilities in video
understanding; however, their application in video editing remains largely
underexplored. This paper presents the first systematic study of LLMs in the
context of video editing. To bridge the gap between visual information and
language-based reasoning, we introduce L-Storyboard, an intermediate
representation that transforms discrete video shots into structured language
descriptions suitable for LLM processing. We categorize video editing tasks
into Convergent Tasks and Divergent Tasks, focusing on three core tasks: Shot
Attributes Classification, Next Shot Selection, and Shot Sequence Ordering. To
address the inherent instability of divergent task outputs, we propose the
StoryFlow strategy, which converts the divergent multi-path reasoning process
into a convergent selection mechanism, effectively enhancing task accuracy and
logical coherence. Experimental results demonstrate that L-Storyboard
facilitates a more robust mapping between visual information and language
descriptions, significantly improving the interpretability and privacy
protection of video editing tasks. Furthermore, StoryFlow enhances the logical
consistency and output stability in Shot Sequence Ordering, underscoring the
substantial potential of LLMs in intelligent video editing.

</details>

### [703] [SEPT: Standard-Definition Map Enhanced Scene Perception and Topology Reasoning for Autonomous Driving](https://arxiv.org/abs/2505.12246)
*Muleilan Pei,Jiayao Shan,Peiliang Li,Jieqi Shi,Jing Huo,Yang Gao,Shaojie Shen*

Main category: cs.CV

TLDR: 论文提出了一种SD地图增强的场景感知与拓扑推理框架（SEPT），通过结合SD地图与BEV特征，提升自动驾驶车辆在无地图环境中的感知能力。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶车辆在无高精地图（HD）环境下，因传感器限制导致的长距离或遮挡场景感知不足的问题。

Method: 提出混合特征融合策略，结合SD地图与BEV特征，并设计辅助的交叉感知关键点检测任务。

Result: 在OpenLane-V2数据集上，SEPT框架显著提升了场景感知与拓扑推理性能，优于现有方法。

Conclusion: 通过有效整合SD地图先验知识，SEPT框架为自动驾驶的无地图系统提供了更强大的场景理解能力。

Abstract: Online scene perception and topology reasoning are critical for autonomous
vehicles to understand their driving environments, particularly for mapless
driving systems that endeavor to reduce reliance on costly High-Definition (HD)
maps. However, recent advances in online scene understanding still face
limitations, especially in long-range or occluded scenarios, due to the
inherent constraints of onboard sensors. To address this challenge, we propose
a Standard-Definition (SD) Map Enhanced scene Perception and Topology reasoning
(SEPT) framework, which explores how to effectively incorporate the SD map as
prior knowledge into existing perception and reasoning pipelines. Specifically,
we introduce a novel hybrid feature fusion strategy that combines SD maps with
Bird's-Eye-View (BEV) features, considering both rasterized and vectorized
representations, while mitigating potential misalignment between SD maps and
BEV feature spaces. Additionally, we leverage the SD map characteristics to
design an auxiliary intersection-aware keypoint detection task, which further
enhances the overall scene understanding performance. Experimental results on
the large-scale OpenLane-V2 dataset demonstrate that by effectively integrating
SD map priors, our framework significantly improves both scene perception and
topology reasoning, outperforming existing methods by a substantial margin.

</details>

### [704] [SMFusion: Semantic-Preserving Fusion of Multimodal Medical Images for Enhanced Clinical Diagnosis](https://arxiv.org/abs/2505.12251)
*Haozhe Xiang,Han Zhang,Yu Cheng,Xiongwen Quan,Wanwan Huang*

Main category: cs.CV

TLDR: 提出了一种基于语义引导的多模态医学图像融合方法，首次将医学先验知识融入融合过程，通过语义交互对齐模块和文本注入模块实现特征融合，并在实验中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像融合方法主要遵循计算机视觉标准，忽略了医学图像中丰富的语义信息，因此需要一种能够结合医学先验知识的新方法。

Method: 构建多模态医学图像-文本数据集，利用BiomedGPT生成文本描述，通过语义交互对齐模块将文本与图像特征对齐，再通过文本注入模块进行特征融合，并设计医学语义损失函数。

Result: 实验结果表明，该方法在定性和定量评估中均表现优越，且能保留更多关键医学信息。

Conclusion: 提出的语义引导方法有效提升了医学图像融合的性能和信息保留能力，为医学诊断提供了更可靠的工具。

Abstract: Multimodal medical image fusion plays a crucial role in medical diagnosis by
integrating complementary information from different modalities to enhance
image readability and clinical applicability. However, existing methods mainly
follow computer vision standards for feature extraction and fusion strategy
formulation, overlooking the rich semantic information inherent in medical
images. To address this limitation, we propose a novel semantic-guided medical
image fusion approach that, for the first time, incorporates medical prior
knowledge into the fusion process. Specifically, we construct a publicly
available multimodal medical image-text dataset, upon which text descriptions
generated by BiomedGPT are encoded and semantically aligned with image features
in a high-dimensional space via a semantic interaction alignment module. During
this process, a cross attention based linear transformation automatically maps
the relationship between textual and visual features to facilitate
comprehensive learning. The aligned features are then embedded into a
text-injection module for further feature-level fusion. Unlike traditional
methods, we further generate diagnostic reports from the fused images to assess
the preservation of medical information. Additionally, we design a medical
semantic loss function to enhance the retention of textual cues from the source
images. Experimental results on test datasets demonstrate that the proposed
method achieves superior performance in both qualitative and quantitative
evaluations while preserving more critical medical information.

</details>

### [705] [LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding](https://arxiv.org/abs/2505.12253)
*Hanyu Zhou,Gim Hee Lee*

Main category: cs.CV

TLDR: 提出LLaVA-4D框架，通过时空提示增强4D场景理解，解决现有3D大模型在动态物体理解上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有3D大模型缺乏对动态物体的时空表征能力，限制了物理世界的理解。

Method: 提出时空提示，将3D位置和1D时间编码为动态感知的4D坐标嵌入，并嵌入视觉特征中。

Result: 实验证明该方法在4D场景理解任务中有效，能区分背景与动态物体。

Conclusion: LLaVA-4D通过时空提示提升了大模型对物理世界的时空理解能力。

Abstract: Despite achieving significant progress in 2D image understanding, large
multimodal models (LMMs) struggle in the physical world due to the lack of
spatial representation. Typically, existing 3D LMMs mainly embed 3D positions
as fixed spatial prompts within visual features to represent the scene.
However, these methods are limited to understanding the static background and
fail to capture temporally varying dynamic objects. In this paper, we propose
LLaVA-4D, a general LMM framework with a novel spatiotemporal prompt for visual
representation in 4D scene understanding. The spatiotemporal prompt is
generated by encoding 3D position and 1D time into a dynamic-aware 4D
coordinate embedding. Moreover, we demonstrate that spatial and temporal
components disentangled from visual features are more effective in
distinguishing the background from objects. This motivates embedding the 4D
spatiotemporal prompt into these features to enhance the dynamic scene
representation. By aligning visual spatiotemporal embeddings with language
embeddings, LMMs gain the ability to understand both spatial and temporal
characteristics of static background and dynamic objects in the physical world.
Additionally, we construct a 4D vision-language dataset with spatiotemporal
coordinate annotations for instruction fine-tuning LMMs. Extensive experiments
have been conducted to demonstrate the effectiveness of our method across
different tasks in 4D scene understanding.

</details>

### [706] [MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark](https://arxiv.org/abs/2505.12254)
*Yiwei Ou,Xiaobin Ren,Ronggui Sun,Guansong Gao,Ziyi Jiang,Kaiqi Zhao,Manfredo Manfredini*

Main category: cs.CV

TLDR: MMS-VPR是一个大规模多模态数据集，用于复杂行人环境中的街景地点识别，填补了现有数据集的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉地点识别数据集主要依赖车载图像，缺乏多模态多样性且未充分代表非西方城市环境中的密集混合用途街景空间。

Method: MMS-VPR包含78,575张标注图像和2,512个视频片段，覆盖207个地点，采用系统化、可复制的数据收集协议。

Result: 实验表明，利用多模态和结构线索能显著提升地点识别性能。

Conclusion: MMS-VPR为计算机视觉、地理空间理解和多模态推理的交叉研究提供了支持，数据集已公开。

Abstract: Existing visual place recognition (VPR) datasets predominantly rely on
vehicle-mounted imagery, lack multimodal diversity and underrepresent dense,
mixed-use street-level spaces, especially in non-Western urban contexts. To
address these gaps, we introduce MMS-VPR, a large-scale multimodal dataset for
street-level place recognition in complex, pedestrian-only environments. The
dataset comprises 78,575 annotated images and 2,512 video clips captured across
207 locations in a ~70,800 $\mathrm{m}^2$ open-air commercial district in
Chengdu, China. Each image is labeled with precise GPS coordinates, timestamp,
and textual metadata, and covers varied lighting conditions, viewpoints, and
timeframes. MMS-VPR follows a systematic and replicable data collection
protocol with minimal device requirements, lowering the barrier for scalable
dataset creation. Importantly, the dataset forms an inherent spatial graph with
125 edges, 81 nodes, and 1 subgraph, enabling structure-aware place
recognition. We further define two application-specific subsets --
Dataset_Edges and Dataset_Points -- to support fine-grained and graph-based
evaluation tasks. Extensive benchmarks using conventional VPR models, graph
neural networks, and multimodal baselines show substantial improvements when
leveraging multimodal and structural cues. MMS-VPR facilitates future research
at the intersection of computer vision, geospatial understanding, and
multimodal reasoning. The dataset is publicly available at
https://huggingface.co/datasets/Yiwei-Ou/MMS-VPR.

</details>

### [707] [PMQ-VE: Progressive Multi-Frame Quantization for Video Enhancement](https://arxiv.org/abs/2505.12266)
*ZhanFeng Feng,Long Peng,Xin Di,Yong Guo,Wenbo Li,Yulun Zhang,Renjing Pei,Yang Wang,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TLDR: 提出了一种名为PMQ-VE的新型量化方法，用于视频增强任务，通过两阶段过程（BMFQ和PMTD）解决现有量化方法的局限性，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer-based视频增强方法计算和内存需求高，难以部署在边缘设备上，而直接应用现有量化方法会导致性能下降和细节丢失。

Method: PMQ-VE采用两阶段方法：BMFQ（基于回溯的多帧量化）和PMTD（渐进式多教师蒸馏），分别优化动态范围适应和提升低比特模型的性能。

Result: 实验表明，PMQ-VE在多个任务和基准测试中优于现有方法，达到最先进的性能。

Conclusion: PMQ-VE是一种高效的量化方法，显著提升了视频增强任务的性能，适用于边缘设备部署。

Abstract: Multi-frame video enhancement tasks aim to improve the spatial and temporal
resolution and quality of video sequences by leveraging temporal information
from multiple frames, which are widely used in streaming video processing,
surveillance, and generation. Although numerous Transformer-based enhancement
methods have achieved impressive performance, their computational and memory
demands hinder deployment on edge devices. Quantization offers a practical
solution by reducing the bit-width of weights and activations to improve
efficiency. However, directly applying existing quantization methods to video
enhancement tasks often leads to significant performance degradation and loss
of fine details. This stems from two limitations: (a) inability to allocate
varying representational capacity across frames, which results in suboptimal
dynamic range adaptation; (b) over-reliance on full-precision teachers, which
limits the learning of low-bit student models. To tackle these challenges, we
propose a novel quantization method for video enhancement: Progressive
Multi-Frame Quantization for Video Enhancement (PMQ-VE). This framework
features a coarse-to-fine two-stage process: Backtracking-based Multi-Frame
Quantization (BMFQ) and Progressive Multi-Teacher Distillation (PMTD). BMFQ
utilizes a percentile-based initialization and iterative search with pruning
and backtracking for robust clipping bounds. PMTD employs a progressive
distillation strategy with both full-precision and multiple high-bit (INT)
teachers to enhance low-bit models' capacity and quality. Extensive experiments
demonstrate that our method outperforms existing approaches, achieving
state-of-the-art performance across multiple tasks and benchmarks.The code will
be made publicly available at: https://github.com/xiaoBIGfeng/PMQ-VE.

</details>

### [708] [Context-Aware Autoregressive Models for Multi-Conditional Image Generation](https://arxiv.org/abs/2505.12274)
*Yixiao Chen,Zhiyuan Ma,Guoli Jia,Che Jiang,Jianjun Li,Bowen Zhou*

Main category: cs.CV

TLDR: ContextAR是一个多条件图像生成框架，通过将不同条件嵌入到统一的token序列中，结合混合位置编码和条件感知注意力机制，实现了高效且灵活的多模态控制。


<details>
  <summary>Details</summary>
Motivation: 解决多条件图像生成任务中如何统一处理不同模态条件的问题，同时保持空间对齐和条件区分。

Method: 提出ContextAR框架，采用混合位置编码（Rotary和Learnable）和条件感知注意力机制，支持任意条件组合。

Result: 实验表明ContextAR在多条件控制任务中具有竞争力和灵活性，性能接近或优于现有方法。

Conclusion: ContextAR为多条件图像生成提供了一种简洁高效的解决方案，展现了强大的可控性和通用性。

Abstract: Autoregressive transformers have recently shown impressive image generation
quality and efficiency on par with state-of-the-art diffusion models. Unlike
diffusion architectures, autoregressive models can naturally incorporate
arbitrary modalities into a single, unified token sequence--offering a concise
solution for multi-conditional image generation tasks. In this work, we propose
$\textbf{ContextAR}$, a flexible and effective framework for multi-conditional
image generation. ContextAR embeds diverse conditions (e.g., canny edges, depth
maps, poses) directly into the token sequence, preserving modality-specific
semantics. To maintain spatial alignment while enhancing discrimination among
different condition types, we introduce hybrid positional encodings that fuse
Rotary Position Embedding with Learnable Positional Embedding. We design
Conditional Context-aware Attention to reduces computational complexity while
preserving effective intra-condition perception. Without any fine-tuning,
ContextAR supports arbitrary combinations of conditions during inference time.
Experimental results demonstrate the powerful controllability and versatility
of our approach, and show that the competitive perpormance than diffusion-based
multi-conditional control approaches the existing autoregressive baseline
across diverse multi-condition driven scenarios. Project page:
$\href{https://context-ar.github.io/}{https://context-ar.github.io/.}$

</details>

### [709] [Temporal-Spectral-Spatial Unified Remote Sensing Dense Prediction](https://arxiv.org/abs/2505.12280)
*Sijie Zhao,Feng Liu,Xueliang Zhang,Hao Chen,Pengfeng Xiao,Lei Bai*

Main category: cs.CV

TLDR: 本文提出了一种名为TSSUN的新型网络架构，用于统一处理遥感数据在时间、光谱和空间维度上的异质性，并通过局部-全局窗口注意力机制提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 遥感数据在时间、光谱和空间维度上存在显著异质性，现有深度学习模型通常针对特定配置设计，导致性能下降或模型不兼容，需要大量调整。

Method: TSSUN采用时间-光谱-空间统一策略，利用元信息标准化输入表示，并统一输出结构；提出局部-全局窗口注意力机制以捕获局部和全局特征。

Result: 实验表明，TSSUN能有效适应异质输入并统一多种密集预测任务，性能达到或超越现有最佳方法。

Conclusion: TSSUN展示了在复杂遥感应用中的鲁棒性和泛化能力，无需任务特定调整。

Abstract: The proliferation of diverse remote sensing data has spurred advancements in
dense prediction tasks, yet significant challenges remain in handling data
heterogeneity. Remote sensing imagery exhibits substantial variability across
temporal, spectral, and spatial (TSS) dimensions, complicating unified data
processing. Current deep learning models for dense prediction tasks, such as
semantic segmentation and change detection, are typically tailored to specific
input-output configurations. Consequently, variations in data dimensionality or
task requirements often lead to significant performance degradation or model
incompatibility, necessitating costly retraining or fine-tuning efforts for
different application scenarios. This paper introduces the
Temporal-Spectral-Spatial Unified Network (TSSUN), a novel architecture
designed for unified representation and modeling of remote sensing data across
diverse TSS characteristics and task types. TSSUN employs a
Temporal-Spectral-Spatial Unified Strategy that leverages meta-information to
decouple and standardize input representations from varied temporal, spectral,
and spatial configurations, and similarly unifies output structures for
different dense prediction tasks and class numbers. Furthermore, a Local-Global
Window Attention mechanism is proposed to efficiently capture both local
contextual details and global dependencies, enhancing the model's adaptability
and feature extraction capabilities. Extensive experiments on multiple datasets
demonstrate that a single TSSUN model effectively adapts to heterogeneous
inputs and unifies various dense prediction tasks. The proposed approach
consistently achieves or surpasses state-of-the-art performance, highlighting
its robustness and generalizability for complex remote sensing applications
without requiring task-specific modifications.

</details>

### [710] [LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?](https://arxiv.org/abs/2505.12307)
*Maoyuan Ye,Jing Zhang,Juhua Liu,Bo Du,Dacheng Tao*

Main category: cs.CV

TLDR: LogicOCR是一个评估大型多模态模型（LMMs）在文本丰富图像上逻辑推理能力的基准数据集，包含1,100道选择题，旨在减少对领域知识的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前LMMs在文本丰富图像上的复杂逻辑推理能力尚未充分探索，LogicOCR旨在填补这一空白。

Method: 通过中国国家公务员考试的文本语料库构建数据集，利用自动化流程生成多模态样本，包括多样化的背景、布局和字体，并通过人工验证筛选。

Result: 评估显示LMMs在多模态推理上仍落后于纯文本输入，且受测试规模、输入模态和视觉-文本方向影响显著。

Conclusion: LogicOCR为推进多模态推理研究提供了宝贵资源，LMMs在视觉阅读与推理结合方面仍有提升空间。

Abstract: Recent advances in Large Multimodal Models (LMMs) have significantly improved
their reasoning and Optical Character Recognition (OCR) capabilities. However,
their performance on complex logical reasoning tasks involving text-rich images
remains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark
comprising 1,100 multiple-choice questions designed to evaluate LMMs' logical
reasoning abilities on text-rich images, while minimizing reliance on
domain-specific knowledge (e.g., mathematics). We construct LogicOCR by
curating a text corpus from the Chinese National Civil Servant Examination and
develop a scalable, automated pipeline to convert it into multimodal samples.
First, we design prompt templates to steer GPT-Image-1 to generate images with
diverse backgrounds, interleaved text-illustration layouts, and varied fonts,
ensuring contextual relevance and visual realism. Then, the generated images
are manually verified, with low-quality examples discarded. We evaluate a range
of representative open-source and proprietary LMMs under both Chain-of-Thought
(CoT) and direct-answer settings. Our multi-dimensional analysis reveals key
insights, such as the impact of test-time scaling, input modality differences,
and sensitivity to visual-text orientation. Notably, LMMs still lag in
multimodal reasoning compared to text-only inputs, indicating that they have
not fully bridged visual reading with reasoning. We hope LogicOCR will serve as
a valuable resource for advancing multimodal reasoning research. The dataset is
available at https://github.com/MiliLab/LogicOCR.

</details>

### [711] [DNOI-4DRO: Deep 4D Radar Odometry with Differentiable Neural-Optimization Iterations](https://arxiv.org/abs/2505.12310)
*Shouyi Lu,Huanyu Zhou,Guirong Zhuo*

Main category: cs.CV

TLDR: 提出了一种结合学习与优化的4D雷达里程计模型DNOI-4DRO，通过神经网络与传统几何优化的结合，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统几何优化与端到端神经网络训练在4D雷达里程计中的融合问题，提升稀疏点云数据的表示能力。

Method: 结合神经网络估计点运动流，构建基于点运动与3D位姿关系的成本函数，使用高斯牛顿更新优化雷达位姿；设计双流4D雷达骨干网络，整合多尺度几何特征与聚类感知特征。

Result: 在VoD和Snail-Radar数据集上表现优异，超越现有经典与学习方法，甚至接近基于LiDAR的A-LOAM性能。

Conclusion: DNOI-4DRO模型通过创新结合学习与优化，显著提升了4D雷达里程计的精度与鲁棒性，代码将开源。

Abstract: A novel learning-optimization-combined 4D radar odometry model, named
DNOI-4DRO, is proposed in this paper. The proposed model seamlessly integrates
traditional geometric optimization with end-to-end neural network training,
leveraging an innovative differentiable neural-optimization iteration operator.
In this framework, point-wise motion flow is first estimated using a neural
network, followed by the construction of a cost function based on the
relationship between point motion and pose in 3D space. The radar pose is then
refined using Gauss-Newton updates. Additionally, we design a dual-stream 4D
radar backbone that integrates multi-scale geometric features and
clustering-based class-aware features to enhance the representation of sparse
4D radar point clouds. Extensive experiments on the VoD and Snail-Radar
datasets demonstrate the superior performance of our model, which outperforms
recent classical and learning-based approaches. Notably, our method even
achieves results comparable to A-LOAM with mapping optimization using LiDAR
point clouds as input. Our models and code will be publicly released.

</details>

### [712] [Visuospatial Cognitive Assistant](https://arxiv.org/abs/2505.12312)
*Qi Feng,Hidetoshi Shimodaira*

Main category: cs.CV

TLDR: 论文介绍了ViCA-322K数据集和ViCA-7B模型，用于提升视频空间认知能力，并在多个任务上达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决当前视觉语言模型在视频空间认知任务中的不足。

Method: 构建ViCA-322K数据集并开发ViCA-7B模型，通过微调实现性能提升。

Result: ViCA-7B在八个VSI-Bench任务上表现最优，并提供了可解释性数据集ViCA-Thinking-2.68K。

Conclusion: 强调针对性数据的重要性，并提出了改进时空建模的路径。

Abstract: Video-based spatial cognition is vital for robotics and embodied AI but
challenges current Vision-Language Models (VLMs). This paper makes two key
contributions. First, we introduce ViCA (Visuospatial Cognitive
Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor
videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D
metadata-grounded queries and video-based complex reasoning. Second, we develop
ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all
eight VSI-Bench tasks, outperforming existing models, including larger ones
(e.g., +26.1 on Absolute Distance). For interpretability, we present
ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune
ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial
reasoning. Our work highlights the importance of targeted data and suggests
paths for improved temporal-spatial modeling. We release all resources to
foster research in robust visuospatial intelligence.

</details>

### [713] [Improving Out-of-Domain Robustness with Targeted Augmentation in Frequency and Pixel Spaces](https://arxiv.org/abs/2505.12317)
*Ruoqi Wang,Haitao Wang,Shaojie Guo,Qiong Luo*

Main category: cs.CV

TLDR: 论文提出了一种名为Frequency-Pixel Connect的领域适应框架，通过在频率空间和像素空间中引入目标增强，提升模型在分布偏移下的鲁棒性。该方法无需特定数据集知识，适用于多种领域。


<details>
  <summary>Details</summary>
Motivation: 在领域适应场景中，模型在面对分布偏移时表现不佳，传统数据增强方法效果有限，而特定数据集的目标增强方法需要专家知识。因此，需要一种更通用且有效的方法来提升模型的跨领域鲁棒性。

Method: 提出Frequency-Pixel Connect框架，通过混合源图像和目标图像的振幅谱和像素内容生成增强样本，既引入领域多样性，又保留源图像的语义结构。

Result: 在四个不同领域的真实基准测试中，该方法显著提升了跨领域连接性，表现优于传统通用方法和特定数据集的目标增强方法。

Conclusion: Frequency-Pixel Connect是一种无需特定数据集知识的目标增强方法，能够有效提升模型在分布偏移下的鲁棒性，适用于多种领域。

Abstract: Out-of-domain (OOD) robustness under domain adaptation settings, where
labeled source data and unlabeled target data come from different
distributions, is a key challenge in real-world applications. A common approach
to improving OOD robustness is through data augmentations. However, in
real-world scenarios, models trained with generic augmentations can only
improve marginally when generalized under distribution shifts toward unlabeled
target domains. While dataset-specific targeted augmentations can address this
issue, they typically require expert knowledge and extensive prior data
analysis to identify the nature of the datasets and domain shift. To address
these challenges, we propose Frequency-Pixel Connect, a domain-adaptation
framework that enhances OOD robustness by introducing a targeted augmentation
in both the frequency space and pixel space. Specifically, we mix the amplitude
spectrum and pixel content of a source image and a target image to generate
augmented samples that introduce domain diversity while preserving the semantic
structure of the source image. Unlike previous targeted augmentation methods
that are both dataset-specific and limited to the pixel space, Frequency-Pixel
Connect is dataset-agnostic, enabling broader and more flexible applicability
beyond natural image datasets. We further analyze the effectiveness of
Frequency-Pixel Connect by evaluating the performance of our method connecting
same-class cross-domain samples while separating different-class examples. We
demonstrate that Frequency-Pixel Connect significantly improves cross-domain
connectivity and outperforms previous generic methods on four diverse
real-world benchmarks across vision, medical, audio, and astronomical domains,
and it also outperforms other dataset-specific targeted augmentation methods.

</details>

### [714] [Towards Open-world Generalized Deepfake Detection: General Feature Extraction via Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.12339)
*Midou Guo,Qilin Yin,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TLDR: 提出了一种新的开放世界深度伪造检测任务，并设计了OWG-DS策略，通过域距离优化和相似性类边界分离模块提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于生成式AI的快速发展，未标记合成数据与真实数据混杂，现有监督检测方法难以应对未知伪造方法，需解决开放世界场景下的检测问题。

Method: 提出OWG-DS策略，包括域距离优化模块（DDO）和相似性类边界分离模块（SCBS），结合对抗训练学习域不变特征。

Result: 实验表明，该策略在跨方法和跨数据集场景中表现优异，显著提升模型泛化能力。

Conclusion: OWG-DS策略有效解决了开放世界深度伪造检测的泛化问题，为未来研究提供了新思路。

Abstract: With the development of generative artificial intelligence, new forgery
methods are rapidly emerging. Social platforms are flooded with vast amounts of
unlabeled synthetic data and authentic data, making it increasingly challenging
to distinguish real from fake. Due to the lack of labels, existing supervised
detection methods struggle to effectively address the detection of unknown
deepfake methods. Moreover, in open world scenarios, the amount of unlabeled
data greatly exceeds that of labeled data. Therefore, we define a new deepfake
detection generalization task which focuses on how to achieve efficient
detection of large amounts of unlabeled data based on limited labeled data to
simulate a open world scenario. To solve the above mentioned task, we propose a
novel Open-World Deepfake Detection Generalization Enhancement Training
Strategy (OWG-DS) to improve the generalization ability of existing methods.
Our approach aims to transfer deepfake detection knowledge from a small amount
of labeled source domain data to large-scale unlabeled target domain data.
Specifically, we introduce the Domain Distance Optimization (DDO) module to
align different domain features by optimizing both inter-domain and
intra-domain distances. Additionally, the Similarity-based Class Boundary
Separation (SCBS) module is used to enhance the aggregation of similar samples
to ensure clearer class boundaries, while an adversarial training mechanism is
adopted to learn the domain-invariant features. Extensive experiments show that
the proposed deepfake detection generalization enhancement training strategy
excels in cross-method and cross-dataset scenarios, improving the model's
generalization.

</details>

### [715] [DIMM: Decoupled Multi-hierarchy Kalman Filter for 3D Object Tracking](https://arxiv.org/abs/2505.12340)
*Jirong Zha,Yuxuan Fan,Kai Li,Han Li,Chen Gao,Xinlei Chen,Yong Li*

Main category: cs.CV

TLDR: 提出了一种名为DIMM的新框架，通过方向解耦的多层次滤波器组和可微分自适应融合网络，显著提高了3D目标跟踪的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统IMM方法在模型组合解空间和权重计算上存在局限性，无法充分处理目标在不同方向上的运动特性及测量不确定性。

Method: DIMM设计了3D解耦的多层次滤波器组扩展解空间，并通过注意力机制的TD3方法生成更可靠的组合权重矩阵。

Result: 实验表明，DIMM将现有状态估计方法的跟踪精度提高了31.61%~99.23%。

Conclusion: DIMM通过方向解耦和自适应融合网络，有效解决了传统IMM的局限性，显著提升了3D目标跟踪性能。

Abstract: State estimation is challenging for 3D object tracking with high
maneuverability, as the target's state transition function changes rapidly,
irregularly, and is unknown to the estimator. Existing work based on
interacting multiple model (IMM) achieves more accurate estimation than
single-filter approaches through model combination, aligning appropriate models
for different motion modes of the target object over time. However, two
limitations of conventional IMM remain unsolved. First, the solution space of
the model combination is constrained as the target's diverse kinematic
properties in different directions are ignored. Second, the model combination
weights calculated by the observation likelihood are not accurate enough due to
the measurement uncertainty. In this paper, we propose a novel framework, DIMM,
to effectively combine estimates from different motion models in each
direction, thus increasing the 3D object tracking accuracy. First, DIMM extends
the model combination solution space of conventional IMM from a hyperplane to a
hypercube by designing a 3D-decoupled multi-hierarchy filter bank, which
describes the target's motion with various-order linear models. Second, DIMM
generates more reliable combination weight matrices through a differentiable
adaptive fusion network for importance allocation rather than solely relying on
the observation likelihood; it contains an attention-based twin delayed deep
deterministic policy gradient (TD3) method with a hierarchical reward.
Experiments demonstrate that DIMM significantly improves the tracking accuracy
of existing state estimation methods by 31.61%~99.23%.

</details>

### [716] [Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts](https://arxiv.org/abs/2505.12363)
*Qi Feng,Hidetoshi Shimodaira*

Main category: cs.CV

TLDR: ViCA2是一种新型多模态大语言模型，专注于提升空间推理能力，通过双视觉编码器和专用数据集ViCA-322K，在VSI-Bench基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有模型在空间推理任务上表现不足，缺乏专门的架构和训练数据。

Method: 采用双视觉编码器（SigLIP和Hiera）结合令牌比例控制机制，并使用ViCA-322K数据集进行指令调优。

Result: ViCA2-7B在VSI-Bench上以56.8分超越其他开源和专有模型。

Conclusion: ViCA2展示了紧凑模型在空间智能任务上的高效性，并开源了模型和数据集以推动研究。

Abstract: While Multimodal Large Language Models (MLLMs) excel at general
vision-language tasks, visuospatial cognition - reasoning about spatial
layouts, relations, and dynamics - remains a significant challenge. Existing
models often lack the necessary architectural components and specialized
training data for fine-grained spatial understanding. We introduce ViCA2
(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial
reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP
for semantics and Hiera for spatial structure, coupled with a token ratio
control mechanism for efficiency. We also developed ViCA-322K, a new
large-scale dataset with over 322,000 spatially grounded question-answer pairs
for targeted instruction tuning. On the challenging VSI-Bench benchmark, our
ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly
surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and
leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the
effectiveness of our approach in achieving strong visuospatial intelligence
with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset
to facilitate further research.

</details>

### [717] [CLIP-aware Domain-Adaptive Super-Resolution](https://arxiv.org/abs/2505.12391)
*Zhengyang Lu,Qian Xia,Weifan Wang,Feng Wang*

Main category: cs.CV

TLDR: CDASR是一种利用CLIP语义能力的超分辨率框架，通过特征对齐和元学习策略实现跨域泛化，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决单图像超分辨率中的域泛化问题，利用CLIP的语义能力提升跨域性能。

Method: 结合CLIP引导的特征对齐和元学习策略，通过多阶段变换和自定义损失函数实现高效知识迁移。

Result: 在Urban100数据集上，CDASR在×8和×16缩放下分别取得0.15dB和0.30dB的PSNR提升。

Conclusion: CDASR通过语义信息融合和自适应策略，显著提升了超分辨率任务的跨域性能。

Abstract: This work introduces CLIP-aware Domain-Adaptive Super-Resolution (CDASR), a
novel framework that addresses the critical challenge of domain generalization
in single image super-resolution. By leveraging the semantic capabilities of
CLIP (Contrastive Language-Image Pre-training), CDASR achieves unprecedented
performance across diverse domains and extreme scaling factors. The proposed
method integrates CLIP-guided feature alignment mechanism with a meta-learning
inspired few-shot adaptation strategy, enabling efficient knowledge transfer
and rapid adaptation to target domains. A custom domain-adaptive module
processes CLIP features alongside super-resolution features through a
multi-stage transformation process, including CLIP feature processing, spatial
feature generation, and feature fusion. This intricate process ensures
effective incorporation of semantic information into the super-resolution
pipeline. Additionally, CDASR employs a multi-component loss function that
combines pixel-wise reconstruction, perceptual similarity, and semantic
consistency. Extensive experiments on benchmark datasets demonstrate CDASR's
superiority, particularly in challenging scenarios. On the Urban100 dataset at
$\times$8 scaling, CDASR achieves a significant PSNR gain of 0.15dB over
existing methods, with even larger improvements of up to 0.30dB observed at
$\times$16 scaling.

</details>

### [718] [ViEEG: Hierarchical Neural Coding with Cross-Modal Progressive Enhancement for EEG-Based Visual Decoding](https://arxiv.org/abs/2505.12408)
*Minxu Liu,Donghai Guan,Chuhang Zheng,Chunwei Tian,Jie Wen,Qi Zhu*

Main category: cs.CV

TLDR: ViEEG是一种受生物学启发的分层EEG解码框架，通过模拟视觉处理层次结构，显著提升了EEG视觉解码的性能。


<details>
  <summary>Details</summary>
Motivation: 现有EEG视觉解码方法依赖扁平神经表示，忽略了大脑的视觉层次结构，ViEEG旨在解决这一问题。

Method: ViEEG将视觉刺激分解为三个生物对齐组件，通过三流EEG编码器和交叉注意力路由逐步整合，并结合分层对比学习。

Result: 在THINGS-EEG数据集上，ViEEG在受试者依赖和跨受试者设置中分别达到40.9%和22.9%的Top-1准确率，超越现有方法45%。

Conclusion: ViEEG不仅提升了性能，还为基于生物学的脑解码AI设定了新范式。

Abstract: Understanding and decoding brain activity into visual representations is a
fundamental challenge at the intersection of neuroscience and artificial
intelligence. While EEG-based visual decoding has shown promise due to its
non-invasive, low-cost nature and millisecond-level temporal resolution,
existing methods are limited by their reliance on flat neural representations
that overlook the brain's inherent visual hierarchy. In this paper, we
introduce ViEEG, a biologically inspired hierarchical EEG decoding framework
that aligns with the Hubel-Wiesel theory of visual processing. ViEEG decomposes
each visual stimulus into three biologically aligned components-contour,
foreground object, and contextual scene-serving as anchors for a three-stream
EEG encoder. These EEG features are progressively integrated via
cross-attention routing, simulating cortical information flow from V1 to IT to
the association cortex. We further adopt hierarchical contrastive learning to
align EEG representations with CLIP embeddings, enabling zero-shot object
recognition. Extensive experiments on the THINGS-EEG dataset demonstrate that
ViEEG achieves state-of-the-art performance, with 40.9% Top-1 accuracy in
subject-dependent and 22.9% Top-1 accuracy in cross-subject settings,
surpassing existing methods by over 45%. Our framework not only advances the
performance frontier but also sets a new paradigm for biologically grounded
brain decoding in AI.

</details>

### [719] [Kornia-rs: A Low-Level 3D Computer Vision Library In Rust](https://arxiv.org/abs/2505.12425)
*Edgar Riba,Jian Shi,Aditya Kumar,Andrew Shen,Gary Bradski*

Main category: cs.CV

TLDR: kornia-rs是一个高性能的3D计算机视觉库，完全用Rust编写，专为安全关键和实时应用设计。它通过Rust的所有权模型和类型系统确保内存和线程安全，并提供高效的图像I/O、处理和3D操作。


<details>
  <summary>Details</summary>
Motivation: 解决现有C++库（如OpenCV）或基于包装的解决方案（如OpenCV-Rust）在安全性和性能上的不足，填补Rust生态系统中3D计算机视觉的空白。

Method: 采用静态类型张量系统和模块化设计，提供Python绑定以实现跨平台兼容性。

Result: 在图像转换任务中比原生Rust替代方案快3~5倍，性能与C++包装库相当，并提供了3D计算机视觉功能。

Conclusion: kornia-rs在性能和安全性上表现出色，适用于实际计算机视觉应用。

Abstract: We present \textit{kornia-rs}, a high-performance 3D computer vision library
written entirely in native Rust, designed for safety-critical and real-time
applications. Unlike C++-based libraries like OpenCV or wrapper-based solutions
like OpenCV-Rust, \textit{kornia-rs} is built from the ground up to leverage
Rust's ownership model and type system for memory and thread safety.
\textit{kornia-rs} adopts a statically-typed tensor system and a modular set of
crates, providing efficient image I/O, image processing and 3D operations. To
aid cross-platform compatibility, \textit{kornia-rs} offers Python bindings,
enabling seamless and efficient integration with Rust code. Empirical results
show that \textit{kornia-rs} achieves a 3~ 5 times speedup in image
transformation tasks over native Rust alternatives, while offering comparable
performance to C++ wrapper-based libraries. In addition to 2D vision
capabilities, \textit{kornia-rs} addresses a significant gap in the Rust
ecosystem by providing a set of 3D computer vision operators. This paper
presents the architecture and performance characteristics of
\textit{kornia-rs}, demonstrating its effectiveness in real-world computer
vision applications.

</details>

### [720] [DragLoRA: Online Optimization of LoRA Adapters for Drag-based Image Editing in Diffusion Model](https://arxiv.org/abs/2505.12427)
*Siwei Xia,Li Sun,Tiantian Sun,Qingli Li*

Main category: cs.CV

TLDR: DragLoRA通过集成LoRA适配器和引入去噪分数蒸馏损失，提升了基于拖拽的编辑方法的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法在基于拖拽的编辑中存在精度低和效率不足的问题，主要由于特征表示能力有限和搜索空间过大。

Method: DragLoRA将LoRA适配器集成到编辑流程中，并引入去噪分数蒸馏损失以增强训练，同时采用自适应优化方案。

Result: 实验表明，DragLoRA显著提高了控制精度和计算效率。

Conclusion: DragLoRA为基于拖拽的图像编辑提供了一种更精确和高效的解决方案。

Abstract: Drag-based editing within pretrained diffusion model provides a precise and
flexible way to manipulate foreground objects. Traditional methods optimize the
input feature obtained from DDIM inversion directly, adjusting them iteratively
to guide handle points towards target locations. However, these approaches
often suffer from limited accuracy due to the low representation ability of the
feature in motion supervision, as well as inefficiencies caused by the large
search space required for point tracking. To address these limitations, we
present DragLoRA, a novel framework that integrates LoRA (Low-Rank Adaptation)
adapters into the drag-based editing pipeline. To enhance the training of LoRA
adapters, we introduce an additional denoising score distillation loss which
regularizes the online model by aligning its output with that of the original
model. Additionally, we improve the consistency of motion supervision by
adapting the input features using the updated LoRA, giving a more stable and
accurate input feature for subsequent operations. Building on this, we design
an adaptive optimization scheme that dynamically toggles between two modes,
prioritizing efficiency without compromising precision. Extensive experiments
demonstrate that DragLoRA significantly enhances the control precision and
computational efficiency for drag-based image editing. The Codes of DragLoRA
are available at: https://github.com/Sylvie-X/DragLoRA.

</details>

### [721] [DPCD: A Quality Assessment Database for Dynamic Point Clouds](https://arxiv.org/abs/2505.12431)
*Yating Liu,Yujie Zhang,Qi Yang,Yiling Xu,Zhu Li,Ye-Kui Wang*

Main category: cs.CV

TLDR: 论文介绍了动态点云质量评估（DPCQA）的新数据库DPCD，填补了静态点云质量评估研究的空白，并验证了其可靠性和挑战性。


<details>
  <summary>Details</summary>
Motivation: 动态点云（DPC）能捕捉时间变化，但缺乏质量评估研究，限制了相关应用的发展。

Method: 构建了包含15个参考DPC和525个失真DPC的数据库DPCD，通过主观实验获取MOS，并评估客观指标性能。

Result: 实验表明DPCQA比静态点云更具挑战性，DPCD数据库具有可靠性和异质性。

Conclusion: DPCD为DPCQA研究提供了基础，公开可用，推动相关领域发展。

Abstract: Recently, the advancements in Virtual/Augmented Reality (VR/AR) have driven
the demand for Dynamic Point Clouds (DPC). Unlike static point clouds, DPCs are
capable of capturing temporal changes within objects or scenes, offering a more
accurate simulation of the real world. While significant progress has been made
in the quality assessment research of static point cloud, little study has been
done on Dynamic Point Cloud Quality Assessment (DPCQA), which hinders the
development of quality-oriented applications, such as interframe compression
and transmission in practical scenarios. In this paper, we introduce a
large-scale DPCQA database, named DPCD, which includes 15 reference DPCs and
525 distorted DPCs from seven types of lossy compression and noise distortion.
By rendering these samples to Processed Video Sequences (PVS), a comprehensive
subjective experiment is conducted to obtain Mean Opinion Scores (MOS) from 21
viewers for analysis. The characteristic of contents, impact of various
distortions, and accuracy of MOSs are presented to validate the heterogeneity
and reliability of the proposed database. Furthermore, we evaluate the
performance of several objective metrics on DPCD. The experiment results show
that DPCQA is more challenge than that of static point cloud. The DPCD, which
serves as a catalyst for new research endeavors on DPCQA, is publicly available
at https://huggingface.co/datasets/Olivialyt/DPCD.

</details>

### [722] [SRLoRA: Subspace Recomposition in Low-Rank Adaptation via Importance-Based Fusion and Reinitialization](https://arxiv.org/abs/2505.12433)
*Haodong Yang,Lei Wang,Md Zakir Hossain*

Main category: cs.CV

TLDR: SRLoRA通过动态重组低秩子空间提升LoRA的表达能力，不增加可训练参数，实现更快收敛和更高精度。


<details>
  <summary>Details</summary>
Motivation: LoRA的低秩更新限制了表示能力，影响下游任务性能。

Method: SRLoRA基于重要性评分动态融合和重新初始化低秩对，利用预训练权重的奇异值分解。

Result: 在语言和视觉任务中，SRLoRA比标准LoRA收敛更快且精度更高。

Conclusion: SRLoRA是一种高效且通用的PEFT方法，具有广泛应用潜力。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient
fine-tuning (PEFT) method that injects two trainable low-rank matrices (A and
B) into frozen pretrained models. While efficient, LoRA constrains updates to a
fixed low-rank subspace (Delta W = BA), which can limit representational
capacity and hinder downstream performance. We introduce Subspace Recomposition
in Low-Rank Adaptation (SRLoRA) via importance-based fusion and
reinitialization, a novel approach that enhances LoRA's expressiveness without
compromising its lightweight structure. SRLoRA assigns importance scores to
each LoRA pair (a column of B and the corresponding row of A), and dynamically
recomposes the subspace during training. Less important pairs are fused into
the frozen backbone, freeing capacity to reinitialize new pairs along unused
principal directions derived from the pretrained weight's singular value
decomposition. This mechanism enables continual subspace refreshment and richer
adaptation over time, without increasing the number of trainable parameters. We
evaluate SRLoRA on both language and vision tasks, including the GLUE benchmark
and various image classification datasets. SRLoRA consistently achieves faster
convergence and improved accuracy over standard LoRA, demonstrating its
generality, efficiency, and potential for broader PEFT applications.

</details>

### [723] [VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning](https://arxiv.org/abs/2505.12434)
*Qi Wang,Yanrui Yu,Ye Yuan,Rui Mao,Tianfei Zhou*

Main category: cs.CV

TLDR: VIDEORFT扩展了RFT范式，通过自动生成高质量视频CoT数据集和引入语义一致性奖励，提升了MLLMs的视频推理能力。


<details>
  <summary>Details</summary>
Motivation: 视频推理是人工智能的重要挑战，现有方法难以处理视频数据的复杂逻辑和时空结构。

Method: 采用两阶段方法：基于CoT注释的SFT和RL，通过自动生成数据集和语义一致性奖励优化。

Result: 在六个视频推理基准测试中达到最优性能。

Conclusion: VIDEORFT为视频推理提供了有效解决方案，填补了现有技术的空白。

Abstract: Reinforcement fine-tuning (RFT) has shown great promise in achieving
humanlevel reasoning capabilities of Large Language Models (LLMs), and has
recently been extended to MLLMs. Nevertheless, reasoning about videos, which is
a fundamental aspect of human intelligence, remains a persistent challenge due
to the complex logic, temporal and causal structures inherent in video data. To
fill this gap, we propose VIDEORFT, a novel approach that extends the RFT
paradigm to cultivate human-like video reasoning capabilities in MLLMs.
VIDEORFT follows the standard two-stage scheme in RFT: supervised fine-tuning
(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement
learning (RL) to improve generalization. A central challenge to achieve this in
the video domain lies in the scarcity of large-scale, high-quality video CoT
datasets. We address this by building a fully automatic CoT curation pipeline.
First, we devise a cognitioninspired prompting strategy to elicit a reasoning
LLM to generate preliminary CoTs based solely on rich, structured, and literal
representations of video content. Subsequently, these CoTs are revised by a
visual-language model conditioned on the actual video, ensuring visual
consistency and reducing visual hallucinations. This pipeline results in two
new datasets - VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To
further strength the RL phase, we introduce a novel semantic-consistency reward
that explicitly promotes the alignment between textual reasoning with visual
evidence. This reward encourages the model to produce coherent, context-aware
reasoning outputs grounded in visual input. Extensive experiments show that
VIDEORFT achieves state-of-the-art performance on six video reasoning
benchmarks.

</details>

### [724] [SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning](https://arxiv.org/abs/2505.12448)
*Yang Liu,Ming Ma,Xiaomin Yu,Pengxiang Ding,Han Zhao,Mingyang Sun,Siteng Huang,Donglin Wang*

Main category: cs.CV

TLDR: 提出了一种名为SSR的新方法，将深度数据转化为结构化文本推理，增强视觉语言模型的空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型依赖RGB输入，空间理解能力有限，且现有深度信息整合方法效果不佳。

Method: SSR框架将深度数据转化为结构化文本推理，并通过知识蒸馏压缩为紧凑嵌入，无需重新训练即可集成到现有模型中。

Result: 实验表明，SSR显著提升了深度信息利用和空间推理能力。

Conclusion: SSR推进了视觉语言模型的多模态理解能力，使其更接近人类水平。

Abstract: Despite impressive advancements in Visual-Language Models (VLMs) for
multi-modal tasks, their reliance on RGB inputs limits precise spatial
understanding. Existing methods for integrating spatial cues, such as point
clouds or depth, either require specialized sensors or fail to effectively
exploit depth information for higher-order reasoning. To this end, we propose a
novel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that
transforms raw depth data into structured, interpretable textual rationales.
These textual rationales serve as meaningful intermediate representations to
significantly enhance spatial reasoning capabilities. Additionally, we leverage
knowledge distillation to compress the generated rationales into compact latent
embeddings, which facilitate resource-efficient and plug-and-play integration
into existing VLMs without retraining. To enable comprehensive evaluation, we
introduce a new dataset named SSR-CoT, a million-scale visual-language
reasoning dataset enriched with intermediate spatial reasoning annotations, and
present SSRBench, a comprehensive multi-task benchmark. Extensive experiments
on multiple benchmarks demonstrate SSR substantially improves depth utilization
and enhances spatial reasoning, thereby advancing VLMs toward more human-like
multi-modal understanding. Our project page is at
https://yliu-cs.github.io/SSR.

</details>

### [725] [Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral Image Classification](https://arxiv.org/abs/2505.12482)
*Wenchen Chen,Yanmei Zhang,Zhongwei Xiao,Jianping Chu,Xingbo Wang*

Main category: cs.CV

TLDR: 提出了一种结合自监督学习和少样本学习的方法（S4L-FSC），通过空间和光谱特征提取器提升高光谱图像的少样本分类性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像（HSI）少样本分类面临标记样本稀缺的挑战，现有方法难以适应空间几何多样性和缺乏光谱先验知识。

Method: 使用旋转-镜像自监督学习（RM-SSL）和少样本学习（FSL）预训练空间特征提取器；结合掩码重建自监督学习（MR-SSL）和FSL预训练光谱特征提取器。

Result: 在四个HSI数据集上的实验验证了S4L-FSC方法的有效性和优越性。

Conclusion: S4L-FSC通过空间-光谱预训练和异构-同构知识整合，显著提升了少样本HSI分类性能。

Abstract: Few-shot classification of hyperspectral images (HSI) faces the challenge of
scarce labeled samples. Self-Supervised learning (SSL) and Few-Shot Learning
(FSL) offer promising avenues to address this issue. However, existing methods
often struggle to adapt to the spatial geometric diversity of HSIs and lack
sufficient spectral prior knowledge. To tackle these challenges, we propose a
method, Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral
Image Classification (S4L-FSC), aimed at improving the performance of few-shot
HSI classification. Specifically, we first leverage heterogeneous datasets to
pretrain a spatial feature extractor using a designed Rotation-Mirror
Self-Supervised Learning (RM-SSL) method, combined with FSL. This approach
enables the model to learn the spatial geometric diversity of HSIs using
rotation and mirroring labels as supervisory signals, while acquiring
transferable spatial meta-knowledge through few-shot learning. Subsequently,
homogeneous datasets are utilized to pretrain a spectral feature extractor via
a combination of FSL and Masked Reconstruction Self-Supervised Learning
(MR-SSL). The model learns to reconstruct original spectral information from
randomly masked spectral vectors, inferring spectral dependencies. In parallel,
FSL guides the model to extract pixel-level discriminative features, thereby
embedding rich spectral priors into the model. This spectral-spatial
pretraining method, along with the integration of knowledge from heterogeneous
and homogeneous sources, significantly enhances model performance. Extensive
experiments on four HSI datasets demonstrate the effectiveness and superiority
of the proposed S4L-FSC approach for few-shot HSI classification.

</details>

### [726] [Guiding Diffusion with Deep Geometric Moments: Balancing Fidelity and Variation](https://arxiv.org/abs/2505.12486)
*Sangmin Jung,Utkarsh Nath,Yezhou Yang,Giulia Pedrielli,Joydeep Biswas,Amy Zhang,Hassan Ghasemzadeh,Pavan Turaga*

Main category: cs.CV

TLDR: 提出了一种名为Deep Geometric Moments（DGM）的新方法，用于在扩散模型中实现更精细的图像生成控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如分割图和深度图）在控制图像生成时过于刚性，限制了扩散模型的多样性。DGM旨在通过几何先验提供更灵活的控制。

Method: DGM通过学习几何先验来捕捉主题的视觉特征和细节，避免依赖全局特征或对像素扰动的敏感性。

Result: 实验表明，DGM在扩散模型中有效平衡了控制与多样性，提供了灵活的生成控制机制。

Conclusion: DGM为扩散模型提供了一种新颖且有效的指导方法，解决了现有方法的局限性。

Abstract: Text-to-image generation models have achieved remarkable capabilities in
synthesizing images, but often struggle to provide fine-grained control over
the output. Existing guidance approaches, such as segmentation maps and depth
maps, introduce spatial rigidity that restricts the inherent diversity of
diffusion models. In this work, we introduce Deep Geometric Moments (DGM) as a
novel form of guidance that encapsulates the subject's visual features and
nuances through a learned geometric prior. DGMs focus specifically on the
subject itself compared to DINO or CLIP features, which suffer from
overemphasis on global image features or semantics. Unlike ResNets, which are
sensitive to pixel-wise perturbations, DGMs rely on robust geometric moments.
Our experiments demonstrate that DGM effectively balance control and diversity
in diffusion-based image generation, allowing a flexible control mechanism for
steering the diffusion process.

</details>

### [727] [Video-GPT via Next Clip Diffusion](https://arxiv.org/abs/2505.12489)
*Shaobin Zhuang,Zhipeng Huang,Ying Zhang,Fangyikang Wang,Canmiao Fu,Binxin Yang,Chong Sun,Chen Li,Yali Wang*

Main category: cs.CV

TLDR: Video-GPT将视频视为视觉世界建模的新语言，提出了一种新颖的下一片段扩散范式，用于预训练，实现了短时生成和长时预测，并在视频预测任务中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 语言序列不足以描述视觉世界的时空细节，而视频序列能更好地捕捉这些细节。

Method: 提出Video-GPT，采用下一片段扩散范式，通过自回归去噪历史片段中的噪声片段进行预训练。

Result: 在视频预测任务中取得最佳性能（Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64 vs. Wan 20.89），并在6个主流视频任务中表现出良好的泛化能力。

Conclusion: Video-GPT在视觉世界建模中表现出色，具有广泛的下游任务适应能力。

Abstract: GPT has shown its remarkable success in natural language processing. However,
the language sequence is not sufficient to describe spatial-temporal details in
the visual world. Alternatively, the video sequence is good at capturing such
details. Motivated by this fact, we propose a concise Video-GPT in this paper
by treating video as new language for visual world modeling. By analogy to next
token prediction in GPT, we introduce a novel next clip diffusion paradigm for
pretraining Video-GPT. Different from the previous works, this distinct
paradigm allows Video-GPT to tackle both short-term generation and long-term
prediction, by autoregressively denoising the noisy clip according to the clean
clips in the history. Extensive experiments show our Video-GPT achieves the
state-of-the-art performance on video prediction, which is the key factor
towards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64
vs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in
both video generation and understanding, showing its great generalization
capacity in downstream. The project page is at https://Video-GPT.github.io.

</details>

### [728] [Rebalancing Contrastive Alignment with Learnable Semantic Gaps in Text-Video Retrieval](https://arxiv.org/abs/2505.12499)
*Jian Xiao,Zijie Song,Jialong Hu,Hao Cheng,Zhenzhen Hu,Jia Li,Richang Hong*

Main category: cs.CV

TLDR: GARE框架通过引入可学习的增量Delta_ij解决文本-视频检索中的模态间隙和假阴性问题，优化对比学习中的梯度冲突。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了文本和视频表示空间中的模态间隙及批量采样中的假阴性问题，导致梯度冲突和不稳定对齐。

Method: 提出GARE框架，通过计算语义间隙的轻量级神经模块生成Delta_ij，并结合信任区域约束、方向多样性项和信息瓶颈进行正则化。

Result: 在四个检索基准测试中，GARE显著提升了对齐精度和对噪声监督的鲁棒性。

Conclusion: GARE通过间隙感知的张力缓解机制，有效解决了对比学习中的梯度冲突问题。

Abstract: Recent advances in text-video retrieval have been largely driven by
contrastive learning frameworks. However, existing methods overlook a key
source of optimization tension: the separation between text and video
distributions in the representation space (referred to as the modality gap),
and the prevalence of false negatives in batch sampling. These factors lead to
conflicting gradients under the InfoNCE loss, impeding stable alignment. To
mitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces
a learnable, pair-specific increment Delta_ij between text t_i and video v_j to
offload the tension from the global anchor representation. We first derive the
ideal form of Delta_ij via a coupled multivariate first-order Taylor
approximation of the InfoNCE loss under a trust-region constraint, revealing it
as a mechanism for resolving gradient conflicts by guiding updates along a
locally optimal descent direction. Due to the high cost of directly computing
Delta_ij, we introduce a lightweight neural module conditioned on the semantic
gap between each video-text pair, enabling structure-aware correction guided by
gradient supervision. To further stabilize learning and promote
interpretability, we regularize Delta using three components: a trust-region
constraint to prevent oscillation, a directional diversity term to promote
semantic coverage, and an information bottleneck to limit redundancy.
Experiments across four retrieval benchmarks show that GARE consistently
improves alignment accuracy and robustness to noisy supervision, confirming the
effectiveness of gap-aware tension mitigation.

</details>

### [729] [GlobalGeoTree: A Multi-Granular Vision-Language Dataset for Global Tree Species Classification](https://arxiv.org/abs/2505.12513)
*Yang Mu,Zhitong Xiong,Yi Wang,Muhammad Shahzad,Franz Essl,Mark van Kleunen,Xiao Xiang Zhu*

Main category: cs.CV

TLDR: GlobalGeoTree是一个全球树种类数据集，包含630万条地理标记的树木记录，用于遥感数据分类。GeoTreeCLIP模型在该数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决全球树种类分类中大规模标记数据集稀缺的问题。

Method: 构建GlobalGeoTree数据集，包含遥感图像和环境变量，并开发GeoTreeCLIP模型进行零样本和少样本分类。

Result: GeoTreeCLIP在GlobalGeoTree-10kEval上显著优于现有模型。

Conclusion: GlobalGeoTree和GeoTreeCLIP为树种类分类和生物多样性研究提供了基准工具。

Abstract: Global tree species mapping using remote sensing data is vital for
biodiversity monitoring, forest management, and ecological research. However,
progress in this field has been constrained by the scarcity of large-scale,
labeled datasets. To address this, we introduce GlobalGeoTree, a comprehensive
global dataset for tree species classification. GlobalGeoTree comprises 6.3
million geolocated tree occurrences, spanning 275 families, 2,734 genera, and
21,001 species across the hierarchical taxonomic levels. Each sample is paired
with Sentinel-2 image time series and 27 auxiliary environmental variables,
encompassing bioclimatic, geographic, and soil data. The dataset is partitioned
into GlobalGeoTree-6M for model pretraining and curated evaluation subsets,
primarily GlobalGeoTree-10kEval for zero-shot and few-shot benchmarking. To
demonstrate the utility of the dataset, we introduce a baseline model,
GeoTreeCLIP, which leverages paired remote sensing data and taxonomic text
labels within a vision-language framework pretrained on GlobalGeoTree-6M.
Experimental results show that GeoTreeCLIP achieves substantial improvements in
zero- and few-shot classification on GlobalGeoTree-10kEval over existing
advanced models. By making the dataset, models, and code publicly available, we
aim to establish a benchmark to advance tree species classification and foster
innovation in biodiversity research and ecological applications.

</details>

### [730] [Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets](https://arxiv.org/abs/2505.12532)
*Ahmet Bilican,M. Akın Yılmaz,A. Murat Tekalp,R. Gökberk Cinbiş*

Main category: cs.CV

TLDR: WaveFT是一种新颖的参数高效微调方法，通过在小波域学习稀疏更新，显著优于LoRA等方法，在极低参数量下表现优异。


<details>
  <summary>Details</summary>
Motivation: 在计算和内存预算有限的情况下，高效适应大型基础模型至关重要。现有PEFT方法（如LoRA）在低参数量下效果有限。

Method: 提出WaveFT，在小波域学习残差矩阵的稀疏更新，精确控制可训练参数，实现细粒度容量调整。

Result: 在个性化文本到图像生成任务中，WaveFT显著优于LoRA和其他PEFT方法，尤其在低参数量下表现突出。

Conclusion: WaveFT是一种高效的PEFT方法，适用于极端参数高效场景，具有广泛的应用潜力。

Abstract: Efficiently adapting large foundation models is critical, especially with
tight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT)
methods such as LoRA offer limited granularity and effectiveness in
few-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT
method that learns highly sparse updates in the wavelet domain of residual
matrices. WaveFT allows precise control of trainable parameters, offering
fine-grained capacity adjustment and excelling with remarkably low parameter
count, potentially far fewer than LoRA's minimum -- ideal for extreme
parameter-efficient scenarios. In order to demonstrate the effect of the
wavelet transform, we compare WaveFT with a special case, called SHiRA, that
entails applying sparse updates directly in the weight domain. Evaluated on
personalized text-to-image generation using Stable Diffusion XL as baseline,
WaveFT significantly outperforms LoRA and other PEFT methods, especially at low
parameter counts; achieving superior subject fidelity, prompt alignment, and
image diversity.

</details>

### [731] [Learning Segment Similarity and Alignment in Large-Scale Content Based Video Retrieval](https://arxiv.org/abs/2309.11091)
*Chen Jiang,Kaiming Huang,Sifeng He,Xudong Yang,Wei Zhang,Xiaobo Zhang,Yuan Cheng,Lei Yang,Qing Wang,Furong Xu,Tan Pan,Wei Chu*

Main category: cs.CV

TLDR: 提出了一种名为SSAN的端到端训练方法，用于解决细粒度视频检索中的高效计算和低存储消耗问题，通过两个新模块（SKE和SPD）显著提升了检索精度和效率。


<details>
  <summary>Details</summary>
Motivation: 随着网络视频的爆炸式增长，细粒度视频检索（S-CBVR）在视频过滤、推荐和版权保护中变得至关重要，但如何在高精度对齐的同时实现高效计算和低存储消耗是主要挑战。

Method: 提出SSAN网络，包含两个新模块：自监督关键帧提取（SKE）减少冗余帧特征，相似性模式检测（SPD）提高时间对齐精度。SSAN通过端到端训练优化整体性能。

Result: 实验表明，SSAN在公共数据集上比现有方法具有更高的对齐精度，同时节省了存储和在线查询计算成本。

Conclusion: SSAN通过SKE和SPD模块的联合训练，显著提升了视频检索的效率和精度，且模块可灵活应用于其他视频检索流程。

Abstract: With the explosive growth of web videos in recent years, large-scale
Content-Based Video Retrieval (CBVR) becomes increasingly essential in video
filtering, recommendation, and copyright protection. Segment-level CBVR
(S-CBVR) locates the start and end time of similar segments in finer
granularity, which is beneficial for user browsing efficiency and infringement
detection especially in long video scenarios. The challenge of S-CBVR task is
how to achieve high temporal alignment accuracy with efficient computation and
low storage consumption. In this paper, we propose a Segment Similarity and
Alignment Network (SSAN) in dealing with the challenge which is firstly trained
end-to-end in S-CBVR. SSAN is based on two newly proposed modules in video
retrieval: (1) An efficient Self-supervised Keyframe Extraction (SKE) module to
reduce redundant frame features, (2) A robust Similarity Pattern Detection
(SPD) module for temporal alignment. In comparison with uniform frame
extraction, SKE not only saves feature storage and search time, but also
introduces comparable accuracy and limited extra computation time. In terms of
temporal alignment, SPD localizes similar segments with higher accuracy and
efficiency than existing deep learning methods. Furthermore, we jointly train
SSAN with SKE and SPD and achieve an end-to-end improvement. Meanwhile, the two
key modules SKE and SPD can also be effectively inserted into other video
retrieval pipelines and gain considerable performance improvements.
Experimental results on public datasets show that SSAN can obtain higher
alignment accuracy while saving storage and online query computational cost
compared to existing methods.

</details>

### [732] [ProMi: An Efficient Prototype-Mixture Baseline for Few-Shot Segmentation with Bounding-Box Annotations](https://arxiv.org/abs/2505.12547)
*Florent Chiaroni,Ali Ayub,Ola Ahmad*

Main category: cs.CV

TLDR: 提出了一种基于边界框标注的少样本二值分割方法ProMi，无需训练，背景类被视为混合分布，效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 机器人应用中，少样本分割至关重要，但像素级标注耗时且昂贵。

Method: 采用原型混合方法ProMi，利用边界框标注而非像素级标签，简单且无需训练。

Result: 在不同数据集上表现最佳，显著优于基线，适用于移动机器人任务。

Conclusion: ProMi是一种高效、简单且有效的少样本分割方法，适用于实际机器人应用。

Abstract: In robotics applications, few-shot segmentation is crucial because it allows
robots to perform complex tasks with minimal training data, facilitating their
adaptation to diverse, real-world environments. However, pixel-level
annotations of even small amount of images is highly time-consuming and costly.
In this paper, we present a novel few-shot binary segmentation method based on
bounding-box annotations instead of pixel-level labels. We introduce, ProMi, an
efficient prototype-mixture-based method that treats the background class as a
mixture of distributions. Our approach is simple, training-free, and effective,
accommodating coarse annotations with ease. Compared to existing baselines,
ProMi achieves the best results across different datasets with significant
gains, demonstrating its effectiveness. Furthermore, we present qualitative
experiments tailored to real-world mobile robot tasks, demonstrating the
applicability of our approach in such scenarios. Our code:
https://github.com/ThalesGroup/promi.

</details>

### [733] [Knowledge-enhanced Multi-perspective Video Representation Learning for Scene Recognition](https://arxiv.org/abs/2401.04354)
*Xuzheng Yu,Chen Jiang,Wei Zhang,Tian Gan,Linlin Chao,Jianan Zhao,Yuan Cheng,Qingpei Guo,Wei Chu*

Main category: cs.CV

TLDR: 提出了一种新颖的双流框架，结合时间和非时间视角，通过自蒸馏和知识增强特征融合方法提升视频场景识别效果。


<details>
  <summary>Details</summary>
Motivation: 视频数据爆炸式增长，现有方法仅从视觉或文本信息的时间或非时间视角识别场景，忽略了多视角互补性和外部知识的价值。

Method: 设计双流框架，分别建模时间和非时间视角的视频表示，通过自蒸馏端到端整合，并引入知识增强的特征融合和标签预测方法。

Result: 在真实数据集上的实验证明了方法的有效性。

Conclusion: 多视角和外部知识的结合显著提升了视频场景识别的性能。

Abstract: With the explosive growth of video data in real-world applications, a
comprehensive representation of videos becomes increasingly important. In this
paper, we address the problem of video scene recognition, whose goal is to
learn a high-level video representation to classify scenes in videos. Due to
the diversity and complexity of video contents in realistic scenarios, this
task remains a challenge. Most existing works identify scenes for videos only
from visual or textual information in a temporal perspective, ignoring the
valuable information hidden in single frames, while several earlier studies
only recognize scenes for separate images in a non-temporal perspective. We
argue that these two perspectives are both meaningful for this task and
complementary to each other, meanwhile, externally introduced knowledge can
also promote the comprehension of videos. We propose a novel two-stream
framework to model video representations from multiple perspectives, i.e.
temporal and non-temporal perspectives, and integrate the two perspectives in
an end-to-end manner by self-distillation. Besides, we design a
knowledge-enhanced feature fusion and label prediction method that contributes
to naturally introducing knowledge into the task of video scene recognition.
Experiments conducted on a real-world dataset demonstrate the effectiveness of
our proposed method.

</details>

### [734] [VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold](https://arxiv.org/abs/2505.12549)
*Dominic Maggio,Hyungtae Lim,Luca Carlone*

Main category: cs.CV

TLDR: VGGT-SLAM是一种基于未校准单目相机的稠密RGB SLAM系统，通过全局对齐子地图优化重建一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用相似变换对齐子地图，但未校准相机下效果不佳，需解决重建模糊性问题。

Method: 利用SL(4)流形优化估计15自由度单应变换，处理子地图对齐和闭环约束。

Result: 实验证明VGGT-SLAM在长视频序列中提升了地图质量，克服了VGGT的高GPU需求限制。

Conclusion: VGGT-SLAM在未校准相机下实现了更优的重建效果，适用于长序列场景。

Abstract: We present VGGT-SLAM, a dense RGB SLAM system constructed by incrementally
and globally aligning submaps created from the feed-forward scene
reconstruction approach VGGT using only uncalibrated monocular cameras. While
related works align submaps using similarity transforms (i.e., translation,
rotation, and scale), we show that such approaches are inadequate in the case
of uncalibrated cameras. In particular, we revisit the idea of reconstruction
ambiguity, where given a set of uncalibrated cameras with no assumption on the
camera motion or scene structure, the scene can only be reconstructed up to a
15-degrees-of-freedom projective transformation of the true geometry. This
inspires us to recover a consistent scene reconstruction across submaps by
optimizing over the SL(4) manifold, thus estimating 15-degrees-of-freedom
homography transforms between sequential submaps while accounting for potential
loop closure constraints. As verified by extensive experiments, we demonstrate
that VGGT-SLAM achieves improved map quality using long video sequences that
are infeasible for VGGT due to its high GPU requirements.

</details>

### [735] [Coarse Attribute Prediction with Task Agnostic Distillation for Real World Clothes Changing ReID](https://arxiv.org/abs/2505.12580)
*Priyank Pathak,Yogesh S Rawat*

Main category: cs.CV

TLDR: 提出RLQ框架，通过CAP和TAD提升低质量图像下的衣物更换重识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高质图像表现良好，但在低质图像（含噪点、模糊等）中表现不佳，需改进。

Method: RLQ框架结合CAP（粗粒度属性预测）和TAD（任务无关蒸馏），交替训练以减少噪声影响并增强特征表示。

Result: 在LaST、DeepChange等数据集上Top-1提升1.6%-2.9%，PRCC提升5.3%-6%。

Conclusion: RLQ显著提升低质图像下的衣物更换重识别性能，代码即将开源。

Abstract: This work focuses on Clothes Changing Re-IDentification (CC-ReID) for the
real world. Existing works perform well with high-quality (HQ) images, but
struggle with low-quality (LQ) where we can have artifacts like pixelation,
out-of-focus blur, and motion blur. These artifacts introduce noise to not only
external biometric attributes (e.g. pose, body shape, etc.) but also corrupt
the model's internal feature representation. Models usually cluster LQ image
features together, making it difficult to distinguish between them, leading to
incorrect matches. We propose a novel framework Robustness against Low-Quality
(RLQ) to improve CC-ReID model on real-world data. RLQ relies on Coarse
Attributes Prediction (CAP) and Task Agnostic Distillation (TAD) operating in
alternate steps in a novel training mechanism. CAP enriches the model with
external fine-grained attributes via coarse predictions, thereby reducing the
effect of noisy inputs. On the other hand, TAD enhances the model's internal
feature representation by bridging the gap between HQ and LQ features, via an
external dataset through task-agnostic self-supervision and distillation. RLQ
outperforms the existing approaches by 1.6%-2.9% Top-1 on real-world datasets
like LaST, and DeepChange, while showing consistent improvement of 5.3%-6%
Top-1 on PRCC with competitive performance on LTCC. *The code will be made
public soon.*

</details>

### [736] [Event-based Star Tracking under Spacecraft Jitter: the e-STURT Dataset](https://arxiv.org/abs/2505.12588)
*Samya Bagchi,Peter Anastasiou,Matthew Tetlow,Tat-Jun Chin,Yasir Latif*

Main category: cs.CV

TLDR: 论文介绍了首个基于事件相机的星体观测数据集e-STURT，用于模拟航天器抖动条件下的星体追踪，并提出了高频抖动估计算法。


<details>
  <summary>Details</summary>
Motivation: 航天器抖动影响光学通信等任务的精确指向能力，需要高保真传感器数据开发抖动补偿算法。

Method: 使用压电执行器模拟系统性和可重复的抖动，生成200个序列的事件相机数据集。

Result: 数据集公开可用，并提出了基于事件流的高频抖动估计算法。

Conclusion: e-STURT数据集将支持开发抖动感知算法，用于关键空间传感任务。

Abstract: Jitter degrades a spacecraft's fine-pointing ability required for optical
communication, earth observation, and space domain awareness. Development of
jitter estimation and compensation algorithms requires high-fidelity sensor
observations representative of on-board jitter. In this work, we present the
Event-based Star Tracking Under Jitter (e-STURT) dataset -- the first event
camera based dataset of star observations under controlled jitter conditions.
Specialized hardware employed for the dataset emulates an event-camera
undergoing on-board jitter. While the event camera provides asynchronous, high
temporal resolution star observations, systematic and repeatable jitter is
introduced using a micrometer accurate piezoelectric actuator. Various jitter
sources are simulated using distinct frequency bands and utilizing both axes of
motion. Ground-truth jitter is captured in hardware from the piezoelectric
actuator. The resulting dataset consists of 200 sequences and is made publicly
available. This work highlights the dataset generation process, technical
challenges and the resulting limitations. To serve as a baseline, we propose a
high-frequency jitter estimation algorithm that operates directly on the event
stream. The e-STURT dataset will enable the development of jitter aware
algorithms for mission critical event-based space sensing applications.

</details>

### [737] [SurveillanceVQA-589K: A Benchmark for Comprehensive Surveillance Video-Language Understanding with Large Models](https://arxiv.org/abs/2505.12589)
*Bo Liu,Pengfei Qiao,Minhan Ma,Xuange Zhang,Yinan Tang,Peng Xu,Kun Liu,Tongtong Yuan*

Main category: cs.CV

TLDR: SurveillanceVQA-589K是一个针对监控领域的大规模开放视频问答基准数据集，包含589,380个问答对，涵盖12种认知多样的问题类型，旨在推动视频语言理解在安全关键应用中的发展。


<details>
  <summary>Details</summary>
Motivation: 监控视频内容的理解在视觉语言研究中是一个关键但未被充分探索的挑战，因其现实复杂性、不规则事件动态和安全关键性。

Method: 采用混合标注流程，结合时间对齐的人工标注和大型视觉语言模型辅助的问答生成，并提出多维评估协议。

Result: 评估了八个大型视觉语言模型，发现其在因果和异常相关任务中存在显著性能差距。

Conclusion: 该基准为智能监控、事件分析和自主决策等安全关键应用提供了实用且全面的资源。

Abstract: Understanding surveillance video content remains a critical yet underexplored
challenge in vision-language research, particularly due to its real-world
complexity, irregular event dynamics, and safety-critical implications. In this
work, we introduce SurveillanceVQA-589K, the largest open-ended video question
answering benchmark tailored to the surveillance domain. The dataset comprises
589,380 QA pairs spanning 12 cognitively diverse question types, including
temporal reasoning, causal inference, spatial understanding, and anomaly
interpretation, across both normal and abnormal video scenarios. To construct
the benchmark at scale, we design a hybrid annotation pipeline that combines
temporally aligned human-written captions with Large Vision-Language
Model-assisted QA generation using prompt-based techniques. We also propose a
multi-dimensional evaluation protocol to assess contextual, temporal, and
causal comprehension. We evaluate eight LVLMs under this framework, revealing
significant performance gaps, especially in causal and anomaly-related tasks,
underscoring the limitations of current models in real-world surveillance
contexts. Our benchmark provides a practical and comprehensive resource for
advancing video-language understanding in safety-critical applications such as
intelligent monitoring, incident analysis, and autonomous decision-making.

</details>

### [738] [Learning Cross-Spectral Point Features with Task-Oriented Training](https://arxiv.org/abs/2505.12593)
*Mia Thomas,Trevor Ablett,Jonathan Kelly*

Main category: cs.CV

TLDR: 该论文提出了一种通过训练特征网络在热-可见光图像对上进行匹配和配准的方法，以提升无人机在低能见度环境下的导航能力。


<details>
  <summary>Details</summary>
Motivation: 无人机在低能见度环境下依赖可见光相机的导航系统表现不佳，而热成像相机在此类条件下仍能有效工作。论文旨在通过跨光谱特征学习，将热成像数据整合到现有导航系统中。

Method: 提出了一种训练特征网络的方法，专注于匹配和配准任务，而非直接训练检测和描述输出。通过将网络响应输入可微分的配准流程，并对其匹配和配准估计应用损失函数。

Result: 在MultiPoint数据集上，所选模型的配准误差（角点误差）低于10像素的比例超过75%。此外，模型还可用于传统的匹配和配准流程。

Conclusion: 该方法有效利用了热-可见光图像数据，提升了无人机在低能见度环境下的导航性能，并展示了与传统流程的兼容性。

Abstract: Unmanned aerial vehicles (UAVs) enable operations in remote and hazardous
environments, yet the visible-spectrum, camera-based navigation systems often
relied upon by UAVs struggle in low-visibility conditions. Thermal cameras,
which capture long-wave infrared radiation, are able to function effectively in
darkness and smoke, where visible-light cameras fail. This work explores
learned cross-spectral (thermal-visible) point features as a means to integrate
thermal imagery into established camera-based navigation systems. Existing
methods typically train a feature network's detection and description outputs
directly, which often focuses training on image regions where thermal and
visible-spectrum images exhibit similar appearance. Aiming to more fully
utilize the available data, we propose a method to train the feature network on
the tasks of matching and registration. We run our feature network on
thermal-visible image pairs, then feed the network response into a
differentiable registration pipeline. Losses are applied to the matching and
registration estimates of this pipeline. Our selected model, trained on the
task of matching, achieves a registration error (corner error) below 10 pixels
for more than 75% of estimates on the MultiPoint dataset. We further
demonstrate that our model can also be used with a classical pipeline for
matching and registration.

</details>

### [739] [Temporal-Oriented Recipe for Transferring Large Vision-Language Model to Video Understanding](https://arxiv.org/abs/2505.12605)
*Thong Nguyen,Zhiyuan Hu,Xu Lin,Cong-Duy Nguyen,See-Kiong Ng,Luu Anh Tuan*

Main category: cs.CV

TLDR: 本文通过实证研究揭示了影响大型视觉语言模型（LVLMs）时间理解能力的关键组件，并提出了一种时间导向的训练方案和升级接口，显著提升了视频理解任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的大型视觉语言模型在视频理解中依赖其隐含的时间理解能力，但未明确关键组件，限制了其潜力。

Method: 通过实证研究分析影响时间理解的关键组件，并提出时间导向的训练方案和升级接口。

Result: 最终模型在标准视频理解任务中显著优于之前的LVLMs。

Conclusion: 研究揭示了时间理解的关键因素，并提出了有效的改进方案，为视频理解任务提供了新思路。

Abstract: Recent years have witnessed outstanding advances of large vision-language
models (LVLMs). In order to tackle video understanding, most of them depend
upon their implicit temporal understanding capacity. As such, they have not
deciphered important components that contribute to temporal understanding
ability, which might limit the potential of these LVLMs for video
understanding. In this work, we conduct a thorough empirical study to demystify
crucial components that influence the temporal understanding of LVLMs. Our
empirical study reveals that significant impacts are centered around the
intermediate interface between the visual encoder and the large language model.
Building on these insights, we propose a temporal-oriented recipe that
encompasses temporal-oriented training schemes and an upscaled interface. Our
final model developed using our recipe significantly enhances previous LVLMs on
standard video understanding tasks.

</details>

### [740] [Diff-MM: Exploring Pre-trained Text-to-Image Generation Model for Unified Multi-modal Object Tracking](https://arxiv.org/abs/2505.12606)
*Shiyu Xuan,Zechao Li,Jinhui Tang*

Main category: cs.CV

TLDR: 论文提出了一种名为Diff-MM的多模态目标跟踪方法，利用预训练的文本到图像生成模型的多模态理解能力，通过并行特征提取管道和子模块调优方法，实现了在RGB-N/D/T/E跟踪中的统一性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于多模态训练数据的不足，性能不理想。为解决这一问题，论文提出利用预训练生成模型的多模态理解能力来提升跟踪性能。

Method: Diff-MM利用预训练的Stable Diffusion的UNet作为特征提取器，通过并行特征提取管道处理成对图像输入，并引入多模态子模块调优方法以学习模态间的互补信息。

Result: 实验结果表明，Diff-MM在性能上优于近期提出的跟踪器，例如在TNL2K数据集上的AUC比OneTracker高出8.3%。

Conclusion: 通过利用生成模型的先验知识，Diff-MM实现了多模态跟踪的统一性能提升，展示了其在复杂场景中的潜力。

Abstract: Multi-modal object tracking integrates auxiliary modalities such as depth,
thermal infrared, event flow, and language to provide additional information
beyond RGB images, showing great potential in improving tracking stabilization
in complex scenarios. Existing methods typically start from an RGB-based
tracker and learn to understand auxiliary modalities only from training data.
Constrained by the limited multi-modal training data, the performance of these
methods is unsatisfactory. To alleviate this limitation, this work proposes a
unified multi-modal tracker Diff-MM by exploiting the multi-modal understanding
capability of the pre-trained text-to-image generation model. Diff-MM leverages
the UNet of pre-trained Stable Diffusion as a tracking feature extractor
through the proposed parallel feature extraction pipeline, which enables
pairwise image inputs for object tracking. We further introduce a multi-modal
sub-module tuning method that learns to gain complementary information between
different modalities. By harnessing the extensive prior knowledge in the
generation model, we achieve a unified tracker with uniform parameters for
RGB-N/D/T/E tracking. Experimental results demonstrate the promising
performance of our method compared with recently proposed trackers, e.g., its
AUC outperforms OneTracker by 8.3% on TNL2K.

</details>

### [741] [BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation](https://arxiv.org/abs/2505.12620)
*Haiquan Wen,Yiwei He,Zhenglin Huang,Tianxiao Li,Zihan YU,Xingru Huang,Lu Qi,Baoyuan Wu,Xiangtai Li,Guangliang Cheng*

Main category: cs.CV

TLDR: 论文提出GenBuster-200K数据集和BusterX框架，解决AI生成视频检测中数据集不足和模型解释性差的问题。


<details>
  <summary>Details</summary>
Motivation: AI生成视频技术快速发展（如Sora和WanX），但缺乏大规模高质量数据集用于检测，现有方法多为二分类且缺乏解释性。

Method: 提出GenBuster-200K数据集（20万高清视频）和BusterX框架（结合MLLM和强化学习），实现检测与解释。

Result: 实验验证BusterX的有效性和泛化性，优于现有方法。

Conclusion: GenBuster-200K和BusterX填补了AI生成视频检测领域的空白，提升检测能力和解释性。

Abstract: Advances in AI generative models facilitate super-realistic video synthesis,
amplifying misinformation risks via social media and eroding trust in digital
content. Several research works have explored new deepfake detection methods on
AI-generated images to alleviate these risks. However, with the fast
development of video generation models, such as Sora and WanX, there is
currently a lack of large-scale, high-quality AI-generated video datasets for
forgery detection. In addition, existing detection approaches predominantly
treat the task as binary classification, lacking explainability in model
decision-making and failing to provide actionable insights or guidance for the
public. To address these challenges, we propose \textbf{GenBuster-200K}, a
large-scale AI-generated video dataset featuring 200K high-resolution video
clips, diverse latest generative techniques, and real-world scenes. We further
introduce \textbf{BusterX}, a novel AI-generated video detection and
explanation framework leveraging multimodal large language model (MLLM) and
reinforcement learning for authenticity determination and explainable
rationale. To our knowledge, GenBuster-200K is the {\it \textbf{first}}
large-scale, high-quality AI-generated video dataset that incorporates the
latest generative techniques for real-world scenarios. BusterX is the {\it
\textbf{first}} framework to integrate MLLM with reinforcement learning for
explainable AI-generated video detection. Extensive comparisons with
state-of-the-art methods and ablation studies validate the effectiveness and
generalizability of BusterX. The code, models, and datasets will be released.

</details>

### [742] [Degradation-Aware Feature Perturbation for All-in-One Image Restoration](https://arxiv.org/abs/2505.12630)
*Xiangpeng Tian,Xiangyu Liao,Xiao Liu,Meng Li,Chao Ren*

Main category: cs.CV

TLDR: DFPIR提出了一种基于退化感知特征扰动（DFP）的全能图像修复方法，通过通道和注意力扰动调整特征空间，解决了多任务干扰问题。


<details>
  <summary>Details</summary>
Motivation: 解决全能图像修复中因退化类型差异导致的梯度更新方向不一致问题。

Method: 引入通道和注意力扰动，通过DGPB模块在编码-解码架构中实现特征空间对齐。

Result: 在去噪、去雾、去雨、运动去模糊和低光增强等任务中达到SOTA性能。

Conclusion: DFPIR通过特征扰动有效解决了多任务干扰问题，提升了全能图像修复的性能。

Abstract: All-in-one image restoration aims to recover clear images from various
degradation types and levels with a unified model. Nonetheless, the significant
variations among degradation types present challenges for training a universal
model, often resulting in task interference, where the gradient update
directions of different tasks may diverge due to shared parameters. To address
this issue, motivated by the routing strategy, we propose DFPIR, a novel
all-in-one image restorer that introduces Degradation-aware Feature
Perturbations(DFP) to adjust the feature space to align with the unified
parameter space. In this paper, the feature perturbations primarily include
channel-wise perturbations and attention-wise perturbations. Specifically,
channel-wise perturbations are implemented by shuffling the channels in
high-dimensional space guided by degradation types, while attention-wise
perturbations are achieved through selective masking in the attention space. To
achieve these goals, we propose a Degradation-Guided Perturbation Block (DGPB)
to implement these two functions, positioned between the encoding and decoding
stages of the encoder-decoder architecture. Extensive experimental results
demonstrate that DFPIR achieves state-of-the-art performance on several
all-in-one image restoration tasks including image denoising, image dehazing,
image deraining, motion deblurring, and low-light image enhancement. Our codes
are available at https://github.com/TxpHome/DFPIR.

</details>

### [743] [Multi-Resolution Haar Network: Enhancing human motion prediction via Haar transform](https://arxiv.org/abs/2505.12631)
*Li Lin*

Main category: cs.CV

TLDR: HaarMoDic网络利用2D Haar变换将关节投影到高分辨率坐标，同时捕捉时空信息，显著提升了3D人体姿态预测的精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法因忽略人体运动序列在时空轴上的过渡特性，难以处理复杂动作预测，如随意姿势或问候。

Method: 提出HaarMoDic网络，采用MR-Haar模块通过2D Haar变换将运动序列投影到混合高分辨率坐标，同时利用时空信息。

Result: 在Human3.6M数据集上，HaarMoDic在MPJPE指标上全面超越现有方法。

Conclusion: MR-Haar模块是关键创新，HaarMoDic通过同时利用时空信息显著提升了预测性能。

Abstract: The 3D human pose is vital for modern computer vision and computer graphics,
and its prediction has drawn attention in recent years. 3D human pose
prediction aims at forecasting a human's future motion from the previous
sequence. Ignoring that the arbitrariness of human motion sequences has a firm
origin in transition in both temporal and spatial axes limits the performance
of state-of-the-art methods, leading them to struggle with making precise
predictions on complex cases, e.g., arbitrarily posing or greeting. To
alleviate this problem, a network called HaarMoDic is proposed in this paper,
which utilizes the 2D Haar transform to project joints to higher resolution
coordinates where the network can access spatial and temporal information
simultaneously. An ablation study proves that the significant contributing
module within the HaarModic Network is the Multi-Resolution Haar (MR-Haar)
block. Instead of mining in one of two axes or extracting separately, the
MR-Haar block projects whole motion sequences to a mixed-up coordinate in
higher resolution with 2D Haar Transform, allowing the network to give scope to
information from both axes in different resolutions. With the MR-Haar block,
the HaarMoDic network can make predictions referring to a broader range of
information. Experimental results demonstrate that HaarMoDic surpasses
state-of-the-art methods in every testing interval on the Human3.6M dataset in
the Mean Per Joint Position Error (MPJPE) metric.

</details>

### [744] [Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents](https://arxiv.org/abs/2505.12632)
*Yunseok Jang,Yeda Song,Sungryull Sohn,Lajanugen Logeswaran,Tiange Luo,Dong-Ki Kim,Kyunghoon Bae,Honglak Lee*

Main category: cs.CV

TLDR: MONDAY是一个大规模数据集，用于训练GUI视觉代理，通过自动化框架从YouTube视频中提取任务数据，显著提升模型在跨平台导航任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 开发GUI视觉代理需要多样化的真实世界数据，现有数据集局限于单一操作系统，无法满足跨平台需求。

Method: 提出MONDAY数据集和自动化框架，包括OCR场景检测、UI元素检测和多步动作识别，从YouTube视频中提取任务数据。

Result: 使用MONDAY预训练的模型在未见过的移动操作系统上平均性能提升18.11%，优于现有单OS数据集。

Conclusion: MONDAY和自动化框架为移动OS导航研究提供了高效的数据支持，推动了跨平台GUI代理的发展。

Abstract: Recent advancements in Large Language Models (LLMs) and Vision-Language
Models (VLMs) have sparked significant interest in developing GUI visual
agents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from
YouTube), a large-scale dataset of 313K annotated frames from 20K instructional
videos capturing diverse real-world mobile OS navigation across multiple
platforms. Models that include MONDAY in their pre-training phases demonstrate
robust cross-platform generalization capabilities, consistently outperforming
models trained on existing single OS datasets while achieving an average
performance gain of 18.11%p on an unseen mobile OS platform. To enable
continuous dataset expansion as mobile platforms evolve, we present an
automated framework that leverages publicly available video content to create
comprehensive task datasets without manual annotation. Our framework comprises
robust OCR-based scene detection (95.04% F1score), near-perfect UI element
detection (99.87% hit ratio), and novel multi-step action identification to
extract reliable action sequences across diverse interface configurations. We
contribute both the MONDAY dataset and our automated collection framework to
facilitate future research in mobile OS navigation.

</details>

### [745] [MVPainter: Accurate and Detailed 3D Texture Generation via Multi-View Diffusion with Geometric Control](https://arxiv.org/abs/2505.12635)
*Mingqi Shao,Feng Xiong,Zhaoxu Sun,Mu Xu*

Main category: cs.CV

TLDR: MVPainter提出了一种改进3D纹理生成的方法，通过数据过滤、增强和几何条件控制，提升了纹理质量与几何对齐，并支持PBR渲染。


<details>
  <summary>Details</summary>
Motivation: 当前3D纹理生成研究不足，存在参考纹理对齐、几何纹理一致性和局部纹理质量等问题。

Method: 采用数据过滤与增强策略，结合ControlNet几何条件控制，生成PBR属性纹理。

Result: 在三个核心维度上取得最优效果，并通过开源工具促进研究复现。

Conclusion: MVPainter在3D纹理生成中表现优异，为实际渲染应用提供了高质量解决方案。

Abstract: Recently, significant advances have been made in 3D object generation.
Building upon the generated geometry, current pipelines typically employ image
diffusion models to generate multi-view RGB images, followed by UV texture
reconstruction through texture baking. While 3D geometry generation has
improved significantly, supported by multiple open-source frameworks, 3D
texture generation remains underexplored. In this work, we systematically
investigate 3D texture generation through the lens of three core dimensions:
reference-texture alignment, geometry-texture consistency, and local texture
quality. To tackle these issues, we propose MVPainter, which employs data
filtering and augmentation strategies to enhance texture fidelity and detail,
and introduces ControlNet-based geometric conditioning to improve
texture-geometry alignment. Furthermore, we extract physically-based rendering
(PBR) attributes from the generated views to produce PBR meshes suitable for
real-world rendering applications. MVPainter achieves state-of-the-art results
across all three dimensions, as demonstrated by human-aligned evaluations. To
facilitate further research and reproducibility, we also release our full
pipeline as an open-source system, including data construction, model
architecture, and evaluation tools.

</details>

### [746] [Single Image Reflection Removal via inter-layer Complementarity](https://arxiv.org/abs/2505.12641)
*Yue Huang,Zi'ang Li,Tianle Hu,Jie Wen,Guanbin Li,Jinglin Zhang,Guoxu Zhou,Xiaozhao Fang*

Main category: cs.CV

TLDR: 论文提出两种改进双流架构的方法，通过增强层间互补性提升图像分离质量。


<details>
  <summary>Details</summary>
Motivation: 现有双流架构未能充分利用层间互补性，限制了图像分离质量。

Method: 1. 引入层间互补模型，低频分量与传输层交互；高频分量提供逆调制。2. 提出高效层间互补注意力机制，通过通道级重组和注意力计算优化分离。

Result: 在多个公开数据集上实现最优分离质量，同时显著降低计算成本和模型复杂度。

Conclusion: 改进的双流架构通过增强层间互补性，显著提升了图像分离效果和效率。

Abstract: Although dual-stream architectures have achieved remarkable success in single
image reflection removal, they fail to fully exploit inter-layer
complementarity in their physical modeling and network design, which limits the
quality of image separation. To address this fundamental limitation, we propose
two targeted improvements to enhance dual-stream architectures: First, we
introduce a novel inter-layer complementarity model where low-frequency
components extracted from the residual layer interact with the transmission
layer through dual-stream architecture to enhance inter-layer complementarity.
Meanwhile, high-frequency components from the residual layer provide inverse
modulation to both streams, improving the detail quality of the transmission
layer. Second, we propose an efficient inter-layer complementarity attention
mechanism which first cross-reorganizes dual streams at the channel level to
obtain reorganized streams with inter-layer complementary structures, then
performs attention computation on the reorganized streams to achieve better
inter-layer separation, and finally restores the original stream structure for
output. Experimental results demonstrate that our method achieves
state-of-the-art separation quality on multiple public datasets while
significantly reducing both computational cost and model complexity.

</details>

### [747] [Use as Many Surrogates as You Want: Selective Ensemble Attack to Unleash Transferability without Sacrificing Resource Efficiency](https://arxiv.org/abs/2505.12644)
*Bo Yang,Hengwei Zhang,Jindong Wang,Yuchen Ren,Chenhao Lin,Chao Shen,Zhengyu Zhao*

Main category: cs.CV

TLDR: 论文提出选择性集成攻击（SEA），通过动态选择多样化模型，解决了传统攻击中转移性与效率的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法在转移性和效率之间存在权衡，限制了攻击效果，尽管在线预训练模型易于获取。

Method: 提出SEA方法，动态选择多样化模型，固定迭代内模型数量以保持效率，增加迭代间多样性以提高转移性。

Result: 在ImageNet实验中，SEA比现有攻击方法转移性提高8.5%，且适用于商业视觉API和大型视觉语言模型。

Conclusion: SEA为根据资源需求自适应平衡转移性和效率提供了可能。

Abstract: In surrogate ensemble attacks, using more surrogate models yields higher
transferability but lower resource efficiency. This practical trade-off between
transferability and efficiency has largely limited existing attacks despite
many pre-trained models are easily accessible online. In this paper, we argue
that such a trade-off is caused by an unnecessary common assumption, i.e., all
models should be identical across iterations. By lifting this assumption, we
can use as many surrogates as we want to unleash transferability without
sacrificing efficiency. Concretely, we propose Selective Ensemble Attack (SEA),
which dynamically selects diverse models (from easily accessible pre-trained
models) across iterations based on our new interpretation of decoupling
within-iteration and cross-iteration model diversity.In this way, the number of
within-iteration models is fixed for maintaining efficiency, while only
cross-iteration model diversity is increased for higher transferability.
Experiments on ImageNet demonstrate the superiority of SEA in various
scenarios. For example, when dynamically selecting 4 from 20 accessible models,
SEA yields 8.5% higher transferability than existing attacks under the same
efficiency. The superiority of SEA also generalizes to real-world systems, such
as commercial vision APIs and large vision-language models. Overall, SEA opens
up the possibility of adaptively balancing transferability and efficiency
according to specific resource requirements.

</details>

### [748] [AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use](https://arxiv.org/abs/2505.12650)
*Yaotian Yang,Yiwen Tang,Yizhe Chen,Xiao Chen,Jiangjie Qiu,Hao Xiong,Haoyu Yin,Zhiyao Luo,Yifei Zhang,Sijia Tao,Wentao Li,Qinghua Zhang,Yuqiang Li,Wanli Ouyang,Bin Zhao,Xiaonan Wang,Fei Wei*

Main category: cs.CV

TLDR: AutoMat是一个自动化工具，将STEM图像转换为原子晶体结构并预测物理性质，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实验数据稀缺且转换过程复杂，阻碍了机器学习模型的训练和验证。

Method: 结合去噪、模板检索、原子重建、松弛和性质预测，通过外部工具协调实现闭环推理。

Result: 在450个样本上表现优于现有多模态大语言模型和工具。

Conclusion: AutoMat和STEM2Mat-Bench为材料科学中显微镜与原子模拟的桥梁提供了关键工具。

Abstract: Machine learning-based interatomic potentials and force fields depend
critically on accurate atomic structures, yet such data are scarce due to the
limited availability of experimentally resolved crystals. Although
atomic-resolution electron microscopy offers a potential source of structural
data, converting these images into simulation-ready formats remains
labor-intensive and error-prone, creating a bottleneck for model training and
validation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that
automatically transforms scanning transmission electron microscopy (STEM)
images into atomic crystal structures and predicts their physical properties.
AutoMat combines pattern-adaptive denoising, physics-guided template retrieval,
symmetry-aware atomic reconstruction, fast relaxation and property prediction
via MatterSim, and coordinated orchestration across all stages. We propose the
first dedicated STEM2Mat-Bench for this task and evaluate performance using
lattice RMSD, formation energy MAE, and structure-matching success rate. By
orchestrating external tool calls, AutoMat enables a text-only LLM to
outperform vision-language models in this domain, achieving closed-loop
reasoning throughout the pipeline. In large-scale experiments over 450
structure samples, AutoMat substantially outperforms existing multimodal large
language models and tools. These results validate both AutoMat and
STEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic
simulation in materials science.The code and dataset are publicly available at
https://github.com/yyt-2378/AutoMat and
https://huggingface.co/datasets/yaotianvector/STEM2Mat.

</details>

### [749] [SPKLIP: Aligning Spike Video Streams with Natural Language](https://arxiv.org/abs/2505.12656)
*Yongchang Gao,Meiling Jin,Zhaofei Yu,Tiejun Huang,Guozhang Chen*

Main category: cs.CV

TLDR: SPKLIP是首个专为Spike-VLA设计的架构，通过分层特征提取和对比学习实现高效的多模态对齐，并在能效和性能上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统模型（如CLIP）在Spike-VLA任务中因模态不匹配导致的性能不足问题。

Method: 采用分层尖峰特征提取器建模多尺度时序动态，结合尖峰-文本对比学习，并引入全尖峰视觉编码器提升能效。

Result: 在基准数据集上达到SOTA性能，并在新贡献的真实数据集上表现出强泛化能力。

Conclusion: SPKLIP在能效和性能上的优势为神经形态计算和多模态研究提供了新方向。

Abstract: Spike cameras offer unique sensing capabilities but their sparse,
asynchronous output challenges semantic understanding, especially for Spike
Video-Language Alignment (Spike-VLA) where models like CLIP underperform due to
modality mismatch. We introduce SPKLIP, the first architecture specifically for
Spike-VLA. SPKLIP employs a hierarchical spike feature extractor that
adaptively models multi-scale temporal dynamics in event streams, and uses
spike-text contrastive learning to directly align spike video with language,
enabling effective few-shot learning. A full-spiking visual encoder variant,
integrating SNN components into our pipeline, demonstrates enhanced energy
efficiency. Experiments show state-of-the-art performance on benchmark spike
datasets and strong few-shot generalization on a newly contributed real-world
dataset. SPKLIP's energy efficiency highlights its potential for neuromorphic
deployment, advancing event-based multimodal research. The source code and
dataset are available at [link removed for anonymity].

</details>

### [750] [Predicting Reaction Time to Comprehend Scenes with Foveated Scene Understanding Maps](https://arxiv.org/abs/2505.12660)
*Ziqi Wen,Jonathan Skaza,Shravan Murlidaran,William Y. Wang,Miguel P. Eckstein*

Main category: cs.CV

TLDR: 提出了一种基于视觉语言模型和注视点视觉的新型图像可计算模型（F-SUM），用于预测人类场景理解时间，并验证其优于传统图像指标。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以预测人类在场景理解任务中的反应时间，而视觉语言模型的发展为解决这一问题提供了新机会。

Method: 结合注视点视觉和视觉语言模型，生成空间分辨的场景理解图（F-SUM），并计算其总分。

Result: F-SUM分数与人类反应时间、注视次数和描述准确性显著相关，优于传统图像指标。

Conclusion: F-SUM是一种有效的图像可计算指标，强调了注视点视觉处理在场景理解中的重要性。

Abstract: Although models exist that predict human response times (RTs) in tasks such
as target search and visual discrimination, the development of image-computable
predictors for scene understanding time remains an open challenge. Recent
advances in vision-language models (VLMs), which can generate scene
descriptions for arbitrary images, combined with the availability of
quantitative metrics for comparing linguistic descriptions, offer a new
opportunity to model human scene understanding. We hypothesize that the primary
bottleneck in human scene understanding and the driving source of variability
in response times across scenes is the interaction between the foveated nature
of the human visual system and the spatial distribution of task-relevant visual
information within an image. Based on this assumption, we propose a novel
image-computable model that integrates foveated vision with VLMs to produce a
spatially resolved map of scene understanding as a function of fixation
location (Foveated Scene Understanding Map, or F-SUM), along with an aggregate
F-SUM score. This metric correlates with average (N=17) human RTs (r=0.47) and
number of saccades (r=0.51) required to comprehend a scene (across 277 scenes).
The F-SUM score also correlates with average (N=16) human description accuracy
(r=-0.56) in time-limited presentations. These correlations significantly
exceed those of standard image-based metrics such as clutter, visual
complexity, and scene ambiguity based on language entropy. Together, our work
introduces a new image-computable metric for predicting human response times in
scene understanding and demonstrates the importance of foveated visual
processing in shaping comprehension difficulty.

</details>

### [751] [Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking](https://arxiv.org/abs/2505.12667)
*Zihan Su,Xuerui Qiu,Hongbin Xu,Tangyu Jiang,Junhao Zhuang,Chun Yuan,Ming Li,Shengfeng He,Fei Richard Yu*

Main category: cs.CV

TLDR: Safe-Sora 是首个将图形水印直接嵌入视频生成过程的框架，通过分层自适应匹配机制和3D小波变换增强的Mamba架构，实现了高质量、高保真和鲁棒的水印保护。


<details>
  <summary>Details</summary>
Motivation: 生成视频模型的快速发展增加了对AI生成内容版权保护的需求，而视频生成中的隐形水印技术尚未充分探索。

Method: 提出分层粗到细的自适应匹配机制，将水印图像分块并分配到最相似的视频帧中，再通过3D小波变换增强的Mamba架构实现时空融合。

Result: 实验表明，Safe-Sora 在视频质量、水印保真度和鲁棒性方面达到最先进水平。

Conclusion: Safe-Sora 为高效且鲁棒的水印保护开辟了新途径，并首次将状态空间模型应用于水印技术。

Abstract: The explosive growth of generative video models has amplified the demand for
reliable copyright preservation of AI-generated content. Despite its popularity
in image synthesis, invisible generative watermarking remains largely
underexplored in video generation. To address this gap, we propose Safe-Sora,
the first framework to embed graphical watermarks directly into the video
generation process. Motivated by the observation that watermarking performance
is closely tied to the visual similarity between the watermark and cover
content, we introduce a hierarchical coarse-to-fine adaptive matching
mechanism. Specifically, the watermark image is divided into patches, each
assigned to the most visually similar video frame, and further localized to the
optimal spatial region for seamless embedding. To enable spatiotemporal fusion
of watermark patches across video frames, we develop a 3D wavelet
transform-enhanced Mamba architecture with a novel spatiotemporal local
scanning strategy, effectively modeling long-range dependencies during
watermark embedding and retrieval. To the best of our knowledge, this is the
first attempt to apply state space models to watermarking, opening new avenues
for efficient and robust watermark protection. Extensive experiments
demonstrate that Safe-Sora achieves state-of-the-art performance in terms of
video quality, watermark fidelity, and robustness, which is largely attributed
to our proposals. We will release our code upon publication.

</details>

### [752] [TS-VLM: Text-Guided SoftSort Pooling for Vision-Language Models in Multi-View Driving Reasoning](https://arxiv.org/abs/2505.12670)
*Lihong Chen,Hossein Hassani,Soodeh Nikan*

Main category: cs.CV

TLDR: 本文提出了一种轻量级的视觉语言模型TS-VLM，通过文本引导的软排序池化模块（TGSSP）动态融合多视角视觉特征，显著提升了自动驾驶场景中的多模态推理能力，同时降低了90%的计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在自动驾驶应用中存在计算开销大和多视角数据融合效率低的问题，难以满足实时部署的需求。

Method: 设计了TS-VLM模型，引入TGSSP模块，通过输入查询的语义动态排序和融合多视角视觉特征，避免使用高成本的注意力机制。

Result: 在DriveLM基准测试中，TS-VLM在BLEU-4、METEOR、ROUGE-L和CIDEr等指标上优于现有模型，同时计算成本降低90%，最小版本仅含2010万参数。

Conclusion: TS-VLM通过高效的查询感知多视角聚合，提升了自动驾驶场景的上下文准确性，同时具备实时部署的实用性。

Abstract: Vision-Language Models (VLMs) have shown remarkable potential in advancing
autonomous driving by leveraging multi-modal fusion in order to enhance scene
perception, reasoning, and decision-making. Despite their potential, existing
models suffer from computational overhead and inefficient integration of
multi-view sensor data that make them impractical for real-time deployment in
safety-critical autonomous driving applications. To address these shortcomings,
this paper is devoted to designing a lightweight VLM called TS-VLM, which
incorporates a novel Text-Guided SoftSort Pooling (TGSSP) module. By resorting
to semantics of the input queries, TGSSP ranks and fuses visual features from
multiple views, enabling dynamic and query-aware multi-view aggregation without
reliance on costly attention mechanisms. This design ensures the query-adaptive
prioritization of semantically related views, which leads to improved
contextual accuracy in multi-view reasoning for autonomous driving. Extensive
evaluations on the DriveLM benchmark demonstrate that, on the one hand, TS-VLM
outperforms state-of-the-art models with a BLEU-4 score of 56.82, METEOR of
41.91, ROUGE-L of 74.64, and CIDEr of 3.39. On the other hand, TS-VLM reduces
computational cost by up to 90%, where the smallest version contains only 20.1
million parameters, making it more practical for real-time deployment in
autonomous vehicles.

</details>

### [753] [Few-Step Diffusion via Score identity Distillation](https://arxiv.org/abs/2505.12674)
*Mingyuan Zhou,Yi Gu,Zhendong Wang*

Main category: cs.CV

TLDR: SiD是一种数据无关的一步蒸馏框架，用于加速高分辨率文本到图像扩散模型（如SDXL），通过理论分析和新的指导策略（Zero-CFG和Anti-CFG）解决了文本对齐与生成多样性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖真实或合成图像，且使用分类器自由指导（CFG）导致文本对齐与多样性之间的权衡。SiD旨在解决这些问题。

Method: 提出Score identity Distillation（SiD）框架，结合理论分析优化多步生成，引入Diffusion GAN对抗损失和两种新指导策略（Zero-CFG和Anti-CFG）。

Result: 在SD1.5和SDXL上实现最先进性能，支持一步和多步生成，且对无真实图像情况具有鲁棒性。

Conclusion: SiD框架高效且灵活，显著提升性能，代码和模型将公开。

Abstract: Diffusion distillation has emerged as a promising strategy for accelerating
text-to-image (T2I) diffusion models by distilling a pretrained score network
into a one- or few-step generator. While existing methods have made notable
progress, they often rely on real or teacher-synthesized images to perform well
when distilling high-resolution T2I diffusion models such as Stable Diffusion
XL (SDXL), and their use of classifier-free guidance (CFG) introduces a
persistent trade-off between text-image alignment and generation diversity. We
address these challenges by optimizing Score identity Distillation (SiD) -- a
data-free, one-step distillation framework -- for few-step generation. Backed
by theoretical analysis that justifies matching a uniform mixture of outputs
from all generation steps to the data distribution, our few-step distillation
algorithm avoids step-specific networks and integrates seamlessly into existing
pipelines, achieving state-of-the-art performance on SDXL at 1024x1024
resolution. To mitigate the alignment-diversity trade-off when real text-image
pairs are available, we introduce a Diffusion GAN-based adversarial loss
applied to the uniform mixture and propose two new guidance strategies:
Zero-CFG, which disables CFG in the teacher and removes text conditioning in
the fake score network, and Anti-CFG, which applies negative CFG in the fake
score network. This flexible setup improves diversity without sacrificing
alignment. Comprehensive experiments on SD1.5 and SDXL demonstrate
state-of-the-art performance in both one-step and few-step generation settings,
along with robustness to the absence of real images. Our efficient PyTorch
implementation, along with the resulting one- and few-step distilled
generators, will be released publicly as a separate branch at
https://github.com/mingyuanzhou/SiD-LSG.

</details>

### [754] [CURE: Concept Unlearning via Orthogonal Representation Editing in Diffusion Models](https://arxiv.org/abs/2505.12677)
*Shristi Das Biswas,Arani Roy,Kaushik Roy*

Main category: cs.CV

TLDR: CURE是一种无需训练的框架，通过权重空间操作高效消除预训练扩散模型中的不良概念。


<details>
  <summary>Details</summary>
Motivation: 现有安全干预方法存在概念移除不彻底、易受攻击、效率低或损害其他能力的问题。

Method: 基于Spectral Eraser的闭式正交投影模块，通过SVD识别并隔离不良概念特征，一步更新模型。

Result: CURE在2秒内高效消除目标概念，对原始生成能力影响小，且抗攻击性强。

Conclusion: CURE提供了一种快速、可解释且高效的概念消除方法，优于现有技术。

Abstract: As Text-to-Image models continue to evolve, so does the risk of generating
unsafe, copyrighted, or privacy-violating content. Existing safety
interventions - ranging from training data curation and model fine-tuning to
inference-time filtering and guidance - often suffer from incomplete concept
removal, susceptibility to jail-breaking, computational inefficiency, or
collateral damage to unrelated capabilities. In this paper, we introduce CURE,
a training-free concept unlearning framework that operates directly in the
weight space of pre-trained diffusion models, enabling fast, interpretable, and
highly specific suppression of undesired concepts. At the core of our method is
the Spectral Eraser, a closed-form, orthogonal projection module that
identifies discriminative subspaces using Singular Value Decomposition over
token embeddings associated with the concepts to forget and retain.
Intuitively, the Spectral Eraser identifies and isolates features unique to the
undesired concept while preserving safe attributes. This operator is then
applied in a single step update to yield an edited model in which the target
concept is effectively unlearned - without retraining, supervision, or
iterative optimization. To balance the trade-off between filtering toxicity and
preserving unrelated concepts, we further introduce an Expansion Mechanism for
spectral regularization which selectively modulates singular vectors based on
their relative significance to control the strength of forgetting. All the
processes above are in closed-form, guaranteeing extremely efficient erasure in
only $2$ seconds. Benchmarking against prior approaches, CURE achieves a more
efficient and thorough removal for targeted artistic styles, objects,
identities, or explicit content, with minor damage to original generation
ability and demonstrates enhanced robustness against red-teaming.

</details>

### [755] [Mamba-Adaptor: State Space Model Adaptor for Visual Recognition](https://arxiv.org/abs/2505.12685)
*Fei Xie,Jiahao Nie,Yujin Tang,Wenkang Zhang,Hongshen Zhao*

Main category: cs.CV

TLDR: Mamba-Adaptor通过两个模块（Adaptor-T和Adaptor-S）解决了Mamba模型在视觉任务中的性能问题，提升了全局上下文建模和空间结构建模能力。


<details>
  <summary>Details</summary>
Motivation: Mamba模型在视觉任务中表现不佳，主要受限于因果计算的全局上下文缺失、长程遗忘和空间结构建模能力弱。

Method: 提出Mamba-Adaptor，包含Adaptor-T（缓解长程遗忘）和Adaptor-S（增强空间建模）。

Result: 在ImageNet和COCO基准测试中达到最优性能。

Conclusion: Mamba-Adaptor是一种简单有效的视觉任务适配器，显著提升了Mamba模型的性能。

Abstract: Recent State Space Models (SSM), especially Mamba, have demonstrated
impressive performance in visual modeling and possess superior model
efficiency. However, the application of Mamba to visual tasks suffers inferior
performance due to three main constraints existing in the sequential model: 1)
Casual computing is incapable of accessing global context; 2) Long-range
forgetting when computing the current hidden states; 3) Weak spatial structural
modeling due to the transformed sequential input. To address these issues, we
investigate a simple yet powerful vision task Adaptor for Mamba models, which
consists of two functional modules: Adaptor-T and Adaptor-S. When solving the
hidden states for SSM, we apply a lightweight prediction module Adaptor-T to
select a set of learnable locations as memory augmentations to ease long-range
forgetting issues. Moreover, we leverage Adapator-S, composed of multi-scale
dilated convolutional kernels, to enhance the spatial modeling and introduce
the image inductive bias into the feature output. Both modules can enlarge the
context modeling in casual computing, as the output is enhanced by the
inaccessible features. We explore three usages of Mamba-Adaptor: A general
visual backbone for various vision tasks; A booster module to raise the
performance of pretrained backbones; A highly efficient fine-tuning module that
adapts the base model for transfer learning tasks. Extensive experiments verify
the effectiveness of Mamba-Adaptor in three settings. Notably, our
Mamba-Adaptor achieves state-of the-art performance on the ImageNet and COCO
benchmarks.

</details>

### [756] [TACOcc:Target-Adaptive Cross-Modal Fusion with Volume Rendering for 3D Semantic Occupancy](https://arxiv.org/abs/2505.12693)
*Luyao Lei,Shuo Xu,Yifan Bai,Xing Wei*

Main category: cs.CV

TLDR: 论文提出了一种自适应多模态融合框架TACOcc，通过双向对称检索机制和体积渲染监督，解决了3D占用预测中的几何-语义不匹配和表面细节丢失问题。


<details>
  <summary>Details</summary>
Motivation: 多模态3D占用预测性能受限，主要由于固定融合策略导致的几何-语义不匹配和稀疏噪声标注引起的表面细节丢失。

Method: 提出目标尺度自适应的双向对称检索机制，改进体积渲染流程，结合2D-3D一致性优化。

Result: 在nuScenes和SemanticKITTI基准测试中验证了有效性。

Conclusion: TACOcc框架通过自适应融合和渲染监督，显著提升了3D语义占用预测的精度。

Abstract: The performance of multi-modal 3D occupancy prediction is limited by
ineffective fusion, mainly due to geometry-semantics mismatch from fixed fusion
strategies and surface detail loss caused by sparse, noisy annotations. The
mismatch stems from the heterogeneous scale and distribution of point cloud and
image features, leading to biased matching under fixed neighborhood fusion. To
address this, we propose a target-scale adaptive, bidirectional symmetric
retrieval mechanism. It expands the neighborhood for large targets to enhance
context awareness and shrinks it for small ones to improve efficiency and
suppress noise, enabling accurate cross-modal feature alignment. This mechanism
explicitly establishes spatial correspondences and improves fusion accuracy.
For surface detail loss, sparse labels provide limited supervision, resulting
in poor predictions for small objects. We introduce an improved volume
rendering pipeline based on 3D Gaussian Splatting, which takes fused features
as input to render images, applies photometric consistency supervision, and
jointly optimizes 2D-3D consistency. This enhances surface detail
reconstruction while suppressing noise propagation. In summary, we propose
TACOcc, an adaptive multi-modal fusion framework for 3D semantic occupancy
prediction, enhanced by volume rendering supervision. Experiments on the
nuScenes and SemanticKITTI benchmarks validate its effectiveness.

</details>

### [757] [Long-RVOS: A Comprehensive Benchmark for Long-term Referring Video Object Segmentation](https://arxiv.org/abs/2505.12702)
*Tianming Liang,Haichao Jiang,Yuting Yang,Chaolei Tan,Shuai Li,Wei-Shi Zheng,Jian-Fang Hu*

Main category: cs.CV

TLDR: 论文提出了一个名为Long-RVOS的大规模基准数据集，用于长期参考视频对象分割任务，并提出了新的评估指标和基线方法ReferMo。


<details>
  <summary>Details</summary>
Motivation: 现有数据集仅关注短视频片段，无法满足实际场景需求。

Method: 提出Long-RVOS数据集，包含2000+长视频；引入新评估指标；提出基线方法ReferMo，结合运动信息扩展时间感受野。

Result: 现有方法在长视频任务中表现不佳，ReferMo显著优于现有方法。

Conclusion: Long-RVOS和ReferMo为未来研究提供了更实用的长视频分割基准和方法。

Abstract: Referring video object segmentation (RVOS) aims to identify, track and
segment the objects in a video based on language descriptions, which has
received great attention in recent years. However, existing datasets remain
focus on short video clips within several seconds, with salient objects visible
in most frames. To advance the task towards more practical scenarios, we
introduce \textbf{Long-RVOS}, a large-scale benchmark for long-term referring
video object segmentation. Long-RVOS contains 2,000+ videos of an average
duration exceeding 60 seconds, covering a variety of objects that undergo
occlusion, disappearance-reappearance and shot changing. The objects are
manually annotated with three different types of descriptions to individually
evaluate the understanding of static attributes, motion patterns and
spatiotemporal relationships. Moreover, unlike previous benchmarks that rely
solely on the per-frame spatial evaluation, we introduce two new metrics to
assess the temporal and spatiotemporal consistency. We benchmark 6
state-of-the-art methods on Long-RVOS. The results show that current approaches
struggle severely with the long-video challenges. To address this, we further
propose ReferMo, a promising baseline method that integrates motion information
to expand the temporal receptive field, and employs a local-to-global
architecture to capture both short-term dynamics and long-term dependencies.
Despite simplicity, ReferMo achieves significant improvements over current
methods in long-term scenarios. We hope that Long-RVOS and our baseline can
drive future RVOS research towards tackling more realistic and long-form
videos.

</details>

### [758] [SpatialLLM: From Multi-modality Data to Urban Spatial Intelligence](https://arxiv.org/abs/2505.12703)
*Jiabin Chen,Haiping Wang,Jinpeng Li,Yuan Liu,Zhen Dong,Bisheng Yang*

Main category: cs.CV

TLDR: SpatialLLM是一种无需训练或专家干预的语言模型，直接处理复杂城市空间智能任务。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法依赖地理分析工具或领域专家的问题，提供更通用的解决方案。

Method: 通过构建结构化场景描述，直接利用预训练LLM进行零样本空间分析。

Result: 实验表明，预训练LLM能准确感知空间分布信息，完成高级空间智能任务。

Conclusion: SpatialLLM为城市智能分析提供了新视角，多领域知识、上下文长度和推理能力是关键因素。

Abstract: We propose SpatialLLM, a novel approach advancing spatial intelligence tasks
in complex urban scenes. Unlike previous methods requiring geographic analysis
tools or domain expertise, SpatialLLM is a unified language model directly
addressing various spatial intelligence tasks without any training,
fine-tuning, or expert intervention. The core of SpatialLLM lies in
constructing detailed and structured scene descriptions from raw spatial data
to prompt pre-trained LLMs for scene-based analysis. Extensive experiments show
that, with our designs, pretrained LLMs can accurately perceive spatial
distribution information and enable zero-shot execution of advanced spatial
intelligence tasks, including urban planning, ecological analysis, traffic
management, etc. We argue that multi-field knowledge, context length, and
reasoning ability are key factors influencing LLM performances in urban
analysis. We hope that SpatialLLM will provide a novel viable perspective for
urban intelligent analysis and management. The code and dataset are available
at https://github.com/WHU-USI3DV/SpatialLLM.

</details>

### [759] [Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining](https://arxiv.org/abs/2505.12711)
*Qichen Sun,Zhengrui Guo,Rui Peng,Hao Chen,Jinzhuo Wang*

Main category: cs.CV

TLDR: ALTER是一个多模态预训练框架，整合WSIs、基因组学和病理报告，解决计算病理学中数据融合、模态缺失和任务多样性的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决计算病理学中数据融合、模态缺失和任务多样性的问题。

Method: 提出ALTER框架，支持任意模态子集的预训练，学习跨模态表示。

Result: 在生存预测、癌症分型等任务中表现优于或接近现有最佳方法。

Conclusion: ALTER为多模态计算病理学提供了灵活且强大的解决方案。

Abstract: Recent advances in computational pathology and artificial intelligence have
significantly enhanced the utilization of gigapixel whole-slide images and and
additional modalities (e.g., genomics) for pathological diagnosis. Although
deep learning has demonstrated strong potential in pathology, several key
challenges persist: (1) fusing heterogeneous data types requires sophisticated
strategies beyond simple concatenation due to high computational costs; (2)
common scenarios of missing modalities necessitate flexible strategies that
allow the model to learn robustly in the absence of certain modalities; (3) the
downstream tasks in CPath are diverse, ranging from unimodal to multimodal,
cnecessitating a unified model capable of handling all modalities. To address
these challenges, we propose ALTER, an any-to-any tri-modal pretraining
framework that integrates WSIs, genomics, and pathology reports. The term "any"
emphasizes ALTER's modality-adaptive design, enabling flexible pretraining with
any subset of modalities, and its capacity to learn robust, cross-modal
representations beyond WSI-centric approaches. We evaluate ALTER across
extensive clinical tasks including survival prediction, cancer subtyping, gene
mutation prediction, and report generation, achieving superior or comparable
performance to state-of-the-art baselines.

</details>

### [760] [IA-MVS: Instance-Focused Adaptive Depth Sampling for Multi-View Stereo](https://arxiv.org/abs/2505.12714)
*Yinzhe Wang,Yiwen Xiao,Hu Wang,Yiping Xu,Yan Tian*

Main category: cs.CV

TLDR: 提出了一种基于实例自适应的多视角立体（IA-MVS）方法，通过缩小深度假设范围和实例级细化提升深度估计精度，并引入基于深度连续性的过滤机制增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用实例深度覆盖范围较小的潜力，且初始阶段的偏差会累积，限制了深度估计精度的进一步提升。

Method: 提出IA-MVS，通过实例级深度假设范围缩小和细化，结合基于深度连续性的过滤机制，并开发了基于条件概率的置信度估计数学模型。

Result: 在DTU基准测试中达到最先进性能，且无需额外训练负担。

Conclusion: IA-MVS通过实例自适应和鲁棒性增强，显著提升了深度估计精度，具有广泛适用性。

Abstract: Multi-view stereo (MVS) models based on progressive depth hypothesis
narrowing have made remarkable advancements. However, existing methods haven't
fully utilized the potential that the depth coverage of individual instances is
smaller than that of the entire scene, which restricts further improvements in
depth estimation precision. Moreover, inevitable deviations in the initial
stage accumulate as the process advances. In this paper, we propose
Instance-Adaptive MVS (IA-MVS). It enhances the precision of depth estimation
by narrowing the depth hypothesis range and conducting refinement on each
instance. Additionally, a filtering mechanism based on intra-instance depth
continuity priors is incorporated to boost robustness. Furthermore, recognizing
that existing confidence estimation can degrade IA-MVS performance on point
clouds. We have developed a detailed mathematical model for confidence
estimation based on conditional probability. The proposed method can be widely
applied in models based on MVSNet without imposing extra training burdens. Our
method achieves state-of-the-art performance on the DTU benchmark. The source
code is available at https://github.com/KevinWang73106/IA-MVS.

</details>

### [761] [VLC Fusion: Vision-Language Conditioned Sensor Fusion for Robust Object Detection](https://arxiv.org/abs/2505.12715)
*Aditya Taparia,Noel Ngu,Mario Leiva,Joshua Shay Kricheli,John Corcoran,Nathaniel D. Bastian,Gerardo Simari,Paulo Shakarian,Ransalu Senanayake*

Main category: cs.CV

TLDR: VLC Fusion是一种新颖的多模态融合框架，利用视觉语言模型动态调整传感器权重，提升目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有融合方法忽视环境条件和传感器输入的细微变化，难以自适应调整模态权重。

Method: 提出VLC Fusion框架，通过视觉语言模型捕捉环境线索（如黑暗、雨天、模糊），动态调整模态权重。

Result: 在自动驾驶和军事目标检测数据集上，VLC Fusion优于传统融合方法，检测精度更高。

Conclusion: VLC Fusion能有效适应环境变化，提升多模态目标检测性能。

Abstract: Although fusing multiple sensor modalities can enhance object detection
performance, existing fusion approaches often overlook subtle variations in
environmental conditions and sensor inputs. As a result, they struggle to
adaptively weight each modality under such variations. To address this
challenge, we introduce Vision-Language Conditioned Fusion (VLC Fusion), a
novel fusion framework that leverages a Vision-Language Model (VLM) to
condition the fusion process on nuanced environmental cues. By capturing
high-level environmental context such as as darkness, rain, and camera
blurring, the VLM guides the model to dynamically adjust modality weights based
on the current scene. We evaluate VLC Fusion on real-world autonomous driving
and military target detection datasets that include image, LIDAR, and mid-wave
infrared modalities. Our experiments show that VLC Fusion consistently
outperforms conventional fusion baselines, achieving improved detection
accuracy in both seen and unseen scenarios.

</details>

### [762] [FLASH: Latent-Aware Semi-Autoregressive Speculative Decoding for Multimodal Tasks](https://arxiv.org/abs/2505.12728)
*Zihua Wang,Ruibo Li,Haozhe Du,Joey Tianyi Zhou,Yu Zhang,Xu Yang*

Main category: cs.CV

TLDR: FLASH是一种专为多模态模型设计的推测解码框架，通过轻量级潜在感知令牌压缩和半自回归解码策略，显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）的解码速度较慢，尤其是视觉输入包含更多低信息密度的令牌，现有方法未能充分利用视觉输入特性。

Method: 提出FLASH框架，结合潜在感知令牌压缩和半自回归解码策略，优化草案模型设计。

Result: 实验显示FLASH在视频字幕和视觉指令调优任务上分别实现2.68倍和2.55倍的加速。

Conclusion: FLASH通过针对性设计显著提升多模态模型的推理效率，为未来研究提供新方向。

Abstract: Large language and multimodal models (LLMs and LMMs) exhibit strong inference
capabilities but are often limited by slow decoding speeds. This challenge is
especially acute in LMMs, where visual inputs typically comprise more tokens
with lower information density than text -- an issue exacerbated by recent
trends toward finer-grained visual tokenizations to boost performance.
Speculative decoding has been effective in accelerating LLM inference by using
a smaller draft model to generate candidate tokens, which are then selectively
verified by the target model, improving speed without sacrificing output
quality. While this strategy has been extended to LMMs, existing methods
largely overlook the unique properties of visual inputs and depend solely on
text-based draft models. In this work, we propose \textbf{FLASH} (Fast
Latent-Aware Semi-Autoregressive Heuristics), a speculative decoding framework
designed specifically for LMMs, which leverages two key properties of
multimodal data to design the draft model. First, to address redundancy in
visual tokens, we propose a lightweight latent-aware token compression
mechanism. Second, recognizing that visual objects often co-occur within a
scene, we employ a semi-autoregressive decoding strategy to generate multiple
tokens per forward pass. These innovations accelerate draft decoding while
maintaining high acceptance rates, resulting in faster overall inference.
Experiments show that FLASH significantly outperforms prior speculative
decoding approaches in both unimodal and multimodal settings, achieving up to
\textbf{2.68$\times$} speed-up on video captioning and \textbf{2.55$\times$} on
visual instruction tuning tasks compared to the original LMM.

</details>

### [763] [MVAR: Visual Autoregressive Modeling with Scale and Spatial Markovian Conditioning](https://arxiv.org/abs/2505.12742)
*Jinhua Zhang,Wei Long,Minghao Han,Weiyi You,Shuhang Gu*

Main category: cs.CV

TLDR: MVAR是一种新型自回归框架，通过引入尺度和空间马尔可夫假设，减少条件概率建模的复杂性，显著降低计算和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 传统方法在视觉生成中存在尺度和空间冗余，导致计算和内存开销大。

Method: 提出MVAR框架，包括尺度马尔可夫轨迹和空间马尔可夫注意力机制，分别减少输入依赖和注意力范围。

Result: 在ImageNet上，MVAR性能与现有方法相当或更优，同时GPU内存占用减少3倍。

Conclusion: MVAR通过减少冗余和优化计算，显著提升了视觉生成的效率和可扩展性。

Abstract: Essential to visual generation is efficient modeling of visual data priors.
Conventional next-token prediction methods define the process as learning the
conditional probability distribution of successive tokens. Recently, next-scale
prediction methods redefine the process to learn the distribution over
multi-scale representations, significantly reducing generation latency.
However, these methods condition each scale on all previous scales and require
each token to consider all preceding tokens, exhibiting scale and spatial
redundancy. To better model the distribution by mitigating redundancy, we
propose Markovian Visual AutoRegressive modeling (MVAR), a novel autoregressive
framework that introduces scale and spatial Markov assumptions to reduce the
complexity of conditional probability modeling. Specifically, we introduce a
scale-Markov trajectory that only takes as input the features of adjacent
preceding scale for next-scale prediction, enabling the adoption of a parallel
training strategy that significantly reduces GPU memory consumption.
Furthermore, we propose spatial-Markov attention, which restricts the attention
of each token to a localized neighborhood of size k at corresponding positions
on adjacent scales, rather than attending to every token across these scales,
for the pursuit of reduced modeling complexity. Building on these improvements,
we reduce the computational complexity of attention calculation from O(N^2) to
O(Nk), enabling training with just eight NVIDIA RTX 4090 GPUs and eliminating
the need for KV cache during inference. Extensive experiments on ImageNet
demonstrate that MVAR achieves comparable or superior performance with both
small model trained from scratch and large fine-tuned models, while reducing
the average GPU memory footprint by 3.0x.

</details>

### [764] [LiDAR MOT-DETR: A LiDAR-based Two-Stage Transformer for 3D Multiple Object Tracking](https://arxiv.org/abs/2505.12753)
*Martha Teiko Teye,Ori Maoz,Matthias Rottmann*

Main category: cs.CV

TLDR: 提出了一种基于LiDAR的两阶段DETR变换器方法，用于多目标跟踪，通过平滑和跟踪阶段提升性能。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云数据稀疏且不规则，传统跟踪系统依赖手工特征和运动模型，难以在拥挤或快速移动场景中保持一致性。

Method: 采用两阶段方法：平滑阶段优化检测结果，跟踪阶段使用DETR注意力块关联目标。

Result: 在nuScenes和KITTI数据集上表现优异，在线模式aMOTA为0.722，aMOTP为0.475，离线模式性能更高。

Conclusion: 该方法在LiDAR多目标跟踪中优于基线模型和SOTA方法，尤其在离线模式下表现更佳。

Abstract: Multi-object tracking from LiDAR point clouds presents unique challenges due
to the sparse and irregular nature of the data, compounded by the need for
temporal coherence across frames. Traditional tracking systems often rely on
hand-crafted features and motion models, which can struggle to maintain
consistent object identities in crowded or fast-moving scenes. We present a
lidar-based two-staged DETR inspired transformer; a smoother and tracker. The
smoother stage refines lidar object detections, from any off-the-shelf
detector, across a moving temporal window. The tracker stage uses a DETR-based
attention block to maintain tracks across time by associating tracked objects
with the refined detections using the point cloud as context. The model is
trained on the datasets nuScenes and KITTI in both online and offline (forward
peeking) modes demonstrating strong performance across metrics such as
ID-switch and multiple object tracking accuracy (MOTA). The numerical results
indicate that the online mode outperforms the lidar-only baseline and SOTA
models on the nuScenes dataset, with an aMOTA of 0.722 and an aMOTP of 0.475,
while the offline mode provides an additional 3 pp aMOTP

</details>

### [765] [It's not you, it's me -- Global urban visual perception varies across demographics and personalities](https://arxiv.org/abs/2505.12758)
*Matias Quintana,Youlong Gu,Xiucheng Liang,Yujun Hou,Koichi Ito,Yihan Zhu,Mahmoud Abdelrahman,Filip Biljecki*

Main category: cs.CV

TLDR: 论文通过全球街道景观视觉感知调查，分析了人口统计特征和人格特质对城市感知的影响，并提出了新的感知指标。研究发现现有机器学习模型与人类感知存在偏差，强调城市规划应考虑本地居民感知。


<details>
  <summary>Details</summary>
Motivation: 当前城市感知研究常忽略人口统计和人格特质的差异，导致偏见放大。研究旨在填补这一空白，提供更全面的感知数据。

Method: 采用全球街道景观视觉感知调查（SPECS数据集），覆盖五国45国籍的1000名参与者，分析人口统计和人格特质对感知的影响。

Result: 发现感知指标在人口统计和人格特质间存在显著差异，机器学习模型与人类感知存在偏差。

Conclusion: 城市规划需考虑本地居民感知和个体差异，避免偏见，SPECS数据集为此提供了支持。

Abstract: Understanding people's preferences and needs is crucial for urban planning
decisions, yet current approaches often combine them from multi-cultural and
multi-city populations, obscuring important demographic differences and risking
amplifying biases. We conducted a large-scale urban visual perception survey of
streetscapes worldwide using street view imagery, examining how demographics --
including gender, age, income, education, race and ethnicity, and, for the
first time, personality traits -- shape perceptions among 1,000 participants,
with balanced demographics, from five countries and 45 nationalities. This
dataset, introduced as Street Perception Evaluation Considering Socioeconomics
(SPECS), exhibits statistically significant differences in perception scores in
six traditionally used indicators (safe, lively, wealthy, beautiful, boring,
and depressing) and four new ones we propose (live nearby, walk, cycle, green)
among demographics and personalities. We revealed that location-based
sentiments are carried over in people's preferences when comparing urban
streetscapes with other cities. Further, we compared the perception scores
based on where participants and streetscapes are from. We found that an
off-the-shelf machine learning model trained on an existing global perception
dataset tends to overestimate positive indicators and underestimate negative
ones compared to human responses, suggesting that targeted intervention should
consider locals' perception. Our study aspires to rectify the myopic treatment
of street perception, which rarely considers demographics or personality
traits.

</details>

### [766] [Reasoning-OCR: Can Large Multimodal Models Solve Complex Logical Reasoning Problems from OCR Cues?](https://arxiv.org/abs/2505.12766)
*Haibin He,Maoyuan Ye,Jing Zhang,Xiantao Cai,Juhua Liu,Bo Du,Dacheng Tao*

Main category: cs.CV

TLDR: 论文介绍了Reasoning-OCR基准测试，用于评估大型多模态模型（LMMs）在基于OCR线索的复杂逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有OCR相关基准测试主要关注简单任务，而LMMs在复杂逻辑推理方面的能力尚未充分探索。

Method: 提出Reasoning-OCR基准，涵盖六种视觉场景和150个精心设计的问题，分为六类推理挑战。

Result: 评估揭示了专有和开源LMMs在不同推理挑战中的表现，凸显了提升推理能力的迫切性。

Conclusion: Reasoning-OCR旨在推动基于OCR线索的复杂推理能力研究，并公开可用。

Abstract: Large Multimodal Models (LMMs) have become increasingly versatile,
accompanied by impressive Optical Character Recognition (OCR) related
capabilities. Existing OCR-related benchmarks emphasize evaluating LMMs'
abilities of relatively simple visual question answering, visual-text parsing,
etc. However, the extent to which LMMs can deal with complex logical reasoning
problems based on OCR cues is relatively unexplored. To this end, we introduce
the Reasoning-OCR benchmark, which challenges LMMs to solve complex reasoning
problems based on the cues that can be extracted from rich visual-text.
Reasoning-OCR covers six visual scenarios and encompasses 150 meticulously
designed questions categorized into six reasoning challenges. Additionally,
Reasoning-OCR minimizes the impact of field-specialized knowledge. Our
evaluation offers some insights for proprietary and open-source LMMs in
different reasoning challenges, underscoring the urgent to improve the
reasoning performance. We hope Reasoning-OCR can inspire and facilitate future
research on enhancing complex reasoning ability based on OCR cues.
Reasoning-OCR is publicly available at
https://github.com/Hxyz-123/ReasoningOCR.

</details>

### [767] [Pyramid Sparse Transformer: Enhancing Multi-Scale Feature Fusion with Dynamic Token Selection](https://arxiv.org/abs/2505.12772)
*Junyi Hu,Tian Bai,Fengyi Wu,Zhengming Peng,Yi Zhang*

Main category: cs.CV

TLDR: 论文提出了一种轻量级的Pyramid Sparse Transformer（PST）模块，用于高效特征融合，显著提升模型性能且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的特征融合方法计算复杂且实现困难，限制了在资源受限环境中的效率。

Method: PST通过粗到细的token选择和共享注意力参数减少计算量，同时保留空间细节，无需重新训练即可提升精度。

Result: 在YOLOv11和ResNet等模型上，PST显著提升了检测和分类任务的性能（如mAP和准确率）。

Conclusion: PST是一种简单、硬件友好的模块，适用于多种视觉任务，能有效平衡性能和计算成本。

Abstract: Feature fusion is critical for high-performance vision models but often
incurs prohibitive complexity. However, prevailing attention-based fusion
methods often involve significant computational complexity and implementation
challenges, limiting their efficiency in resource-constrained environments. To
address these issues, we introduce the Pyramid Sparse Transformer (PST), a
lightweight, plug-and-play module that integrates coarse-to-fine token
selection and shared attention parameters to reduce computation while
preserving spatial detail. PST can be trained using only coarse attention and
seamlessly activated at inference for further accuracy gains without
retraining. When added to state-of-the-art real-time detection models, such as
YOLOv11-N/S/M, PST yields mAP improvements of 0.9%, 0.5%, and 0.4% on MS COCO
with minimal latency impact. Likewise, embedding PST into ResNet-18/50/101 as
backbones, boosts ImageNet top-1 accuracy by 6.5%, 1.7%, and 1.0%,
respectively. These results demonstrate PST's effectiveness as a simple,
hardware-friendly enhancement for both detection and classification tasks.

</details>

### [768] [Enhancing Transformers Through Conditioned Embedded Tokens](https://arxiv.org/abs/2505.12789)
*Hemanth Saratchandran,Simon Lucey*

Main category: cs.CV

TLDR: 论文揭示了Transformer中注意力块的固有不良条件问题，提出了一种改进嵌入令牌条件的方法，显著提升了训练的稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 注意力机制在Transformer中至关重要，但其不良条件问题阻碍了梯度优化和训练效率。

Method: 提出理论框架，建立注意力块条件与嵌入令牌条件的关系，并引入条件嵌入令牌方法。

Result: 该方法显著改善了不良条件问题，在多种任务中验证了其有效性和广泛适用性。

Conclusion: 改进的嵌入令牌方法为Transformer训练提供了更稳定和高效的解决方案。

Abstract: Transformers have transformed modern machine learning, driving breakthroughs
in computer vision, natural language processing, and robotics. At the core of
their success lies the attention mechanism, which enables the modeling of
global dependencies among input tokens. However, we reveal that the attention
block in transformers suffers from inherent ill-conditioning, which hampers
gradient-based optimization and leads to inefficient training. To address this,
we develop a theoretical framework that establishes a direct relationship
between the conditioning of the attention block and that of the embedded
tokenized data. Building on this insight, we introduce conditioned embedded
tokens, a method that systematically modifies the embedded tokens to improve
the conditioning of the attention mechanism. Our analysis demonstrates that
this approach significantly mitigates ill-conditioning, leading to more stable
and efficient training. We validate our methodology across various transformer
architectures, achieving consistent improvements in image classification,
object detection, instance segmentation, and natural language processing,
highlighting its broad applicability and effectiveness.

</details>

### [769] [Informed Mixing -- Improving Open Set Recognition via Attribution-based Augmentation](https://arxiv.org/abs/2505.12803)
*Jiawen Xu,Odej Kao,Margret Keuper*

Main category: cs.CV

TLDR: GradMix是一种数据增强方法，通过动态利用梯度归因图来屏蔽已学习的概念，从而鼓励模型学习更多样化的特征。


<details>
  <summary>Details</summary>
Motivation: 开放集识别（OSR）旨在解决模型推理中检测新类别的问题，但现有方法在未见类别上表现不佳。

Method: 提出GradMix，利用训练过程中梯度归因图动态屏蔽已学习概念，以学习更全面的特征。

Result: 在开放集识别、闭集分类和分布外检测任务中，GradMix表现优于现有方法，并提升模型鲁棒性和自监督学习性能。

Conclusion: GradMix通过优化特征学习，显著提升了模型泛化能力。

Abstract: Open set recognition (OSR) is devised to address the problem of detecting
novel classes during model inference. Even in recent vision models, this
remains an open issue which is receiving increasing attention. Thereby, a
crucial challenge is to learn features that are relevant for unseen categories
from given data, for which these features might not be discriminative. To
facilitate this process and "optimize to learn" more diverse features, we
propose GradMix, a data augmentation method that dynamically leverages
gradient-based attribution maps of the model during training to mask out
already learned concepts. Thus GradMix encourages the model to learn a more
complete set of representative features from the same data source. Extensive
experiments on open set recognition, close set classification, and
out-of-distribution detection reveal that our method can often outperform the
state-of-the-art. GradMix can further increase model robustness to corruptions
as well as downstream classification performance for self-supervised learning,
indicating its benefit for model generalization.

</details>

### [770] [Rethinking Features-Fused-Pyramid-Neck for Object Detection](https://arxiv.org/abs/2505.12820)
*Hulin Li*

Main category: cs.CV

TLDR: 论文提出了一种独立层次金字塔（IHP）架构，解决了多尺度检测中特征金字塔强制点对点融合导致的特征不对齐问题，并引入了软最近邻插值（SNI）和特征自适应选择方法（ESD），最终实现了实时检测的先进性能。


<details>
  <summary>Details</summary>
Motivation: 多尺度检测中特征金字塔的强制点对点融合会导致特征不对齐，影响检测效果。

Method: 设计了独立层次金字塔（IHP）架构，引入软最近邻插值（SNI）和特征自适应选择方法（ESD），结合轻量级卷积技术（GSConvE）。

Result: 在Pascal VOC和MS COCO上实现了最先进的检测性能。

Conclusion: 提出的方法有效解决了特征不对齐问题，提升了实时检测的精度和效率。

Abstract: Multi-head detectors typically employ a features-fused-pyramid-neck for
multi-scale detection and are widely adopted in the industry. However, this
approach faces feature misalignment when representations from different
hierarchical levels of the feature pyramid are forcibly fused point-to-point.
To address this issue, we designed an independent hierarchy pyramid (IHP)
architecture to evaluate the effectiveness of the features-unfused-pyramid-neck
for multi-head detectors. Subsequently, we introduced soft nearest neighbor
interpolation (SNI) with a weight downscaling factor to mitigate the impact of
feature fusion at different hierarchies while preserving key textures.
Furthermore, we present a features adaptive selection method for down sampling
in extended spatial windows (ESD) to retain spatial features and enhance
lightweight convolutional techniques (GSConvE). These advancements culminate in
our secondary features alignment solution (SA) for real-time detection,
achieving state-of-the-art results on Pascal VOC and MS COCO. Code will be
released at https://github.com/AlanLi1997/rethinking-fpn. This paper has been
accepted by ECCV2024 and published on Springer Nature.

</details>

### [771] [Mitigating Hallucination in VideoLLMs via Temporal-Aware Activation Engineering](https://arxiv.org/abs/2505.12826)
*Jianfeng Cai,Wengang Zhou,Zongmeng Zhang,Jiale Hong,Nianji Zhan,Houqiang Li*

Main category: cs.CV

TLDR: 该论文研究了激活工程在减少视频多模态大语言模型（VideoLLMs）中的幻觉问题上的有效性，提出了一个基于时间变化特征的框架，显著降低了幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在视频理解方面取得了显著进展，但幻觉问题（模型生成看似合理但实际错误的输出）仍未得到充分解决。激活工程在LLMs和ImageLLMs中已证明有效，但其在VideoLLMs中的应用尚未探索。

Method: 论文首先研究了影响激活工程性能的关键因素，发现模型对幻觉的敏感性与时间变化相关。随后提出了一个时间感知的激活工程框架，自适应地识别和操作对幻觉敏感的模块。

Result: 实验表明，该方法在多个模型和基准测试中显著减少了VideoLLMs中的幻觉现象。

Conclusion: 研究验证了激活工程在VideoLLMs中的有效性，并提出了一个无需额外微调的框架，为减少幻觉问题提供了新思路。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress in
video understanding.However, hallucination, where the model generates plausible
yet incorrect outputs, persists as a significant and under-addressed challenge
in the video domain. Among existing solutions, activation engineering has
proven successful in mitigating hallucinations in LLMs and ImageLLMs, yet its
applicability to VideoLLMs remains largely unexplored. In this work, we are the
first to systematically investigate the effectiveness and underlying mechanisms
of activation engineering for mitigating hallucinations in VideoLLMs. We
initially conduct an investigation of the key factors affecting the performance
of activation engineering and find that a model's sensitivity to hallucination
depends on $\textbf{temporal variation}$ rather than task type. Moreover,
selecting appropriate internal modules and dataset for activation engineering
is critical for reducing hallucination. Guided by these findings, we propose a
temporal-aware activation engineering framework for VideoLLMs, which adaptively
identifies and manipulates hallucination-sensitive modules based on the
temporal variation characteristic, substantially mitigating hallucinations
without additional LLM fine-tuning. Experiments across multiple models and
benchmarks demonstrate that our method markedly reduces hallucination in
VideoLLMs, thereby validating the robustness of our findings.

</details>

### [772] [A Study on the Refining Handwritten Font by Mixing Font Styles](https://arxiv.org/abs/2505.12834)
*Avinash Kumar,Kyeolhee Kang,Ammar ul Hassan,Jaeyoung Choi*

Main category: cs.CV

TLDR: FontFusionGAN (FFGAN) 使用生成对抗网络（GAN）结合手写体和印刷体字体，生成既易读又美观的字体。


<details>
  <summary>Details</summary>
Motivation: 手写字体具有独特的表达性，但往往因不清晰或不一致而难以阅读。FFGAN旨在通过结合手写体和印刷体字体的优点，提升手写字体的可读性。

Method: 采用生成对抗网络（GAN），在包含手写体和印刷体字体的数据集上进行训练，生成兼具两者优点的字体图像。

Result: 实验表明，FFGAN显著提升了原始手写字体的可读性，同时保留了其独特的美学特征。

Conclusion: FFGAN不仅适用于提升手写字体的可读性，还可用于其他文本图像相关任务，如字体属性控制和多语言字体风格迁移。

Abstract: Handwritten fonts have a distinct expressive character, but they are often
difficult to read due to unclear or inconsistent handwriting. FontFusionGAN
(FFGAN) is a novel method for improving handwritten fonts by combining them
with printed fonts. Our method implements generative adversarial network (GAN)
to generate font that mix the desirable features of handwritten and printed
fonts. By training the GAN on a dataset of handwritten and printed fonts, it
can generate legible and visually appealing font images. We apply our method to
a dataset of handwritten fonts and demonstrate that it significantly enhances
the readability of the original fonts while preserving their unique aesthetic.
Our method has the potential to improve the readability of handwritten fonts,
which would be helpful for a variety of applications including document
creation, letter writing, and assisting individuals with reading and writing
difficulties. In addition to addressing the difficulties of font creation for
languages with complex character sets, our method is applicable to other
text-image-related tasks, such as font attribute control and multilingual font
style transfer.

</details>

### [773] [Accelerate TarFlow Sampling with GS-Jacobi Iteration](https://arxiv.org/abs/2505.12849)
*Ben Liu,Zhen Qin*

Main category: cs.CV

TLDR: 论文提出通过GS-Jacobi迭代方法优化TarFlow模型的采样效率，利用CRM和IGM指标区分块的重要性，显著加速采样过程，同时保持生成图像质量。


<details>
  <summary>Details</summary>
Motivation: TarFlow模型采样速度慢，因其因果注意力形式需顺序计算。

Method: 提出GS-Jacobi迭代方法，结合CRM和IGM指标优化块的计算顺序和初始值。

Result: 在四个TarFlow模型上实现显著加速（最高5.32倍），且FID分数未下降。

Conclusion: GS-Jacobi方法有效提升TarFlow采样效率，适用于不同任务，代码已开源。

Abstract: Image generation models have achieved widespread applications. As an
instance, the TarFlow model combines the transformer architecture with
Normalizing Flow models, achieving state-of-the-art results on multiple
benchmarks. However, due to the causal form of attention requiring sequential
computation, TarFlow's sampling process is extremely slow. In this paper, we
demonstrate that through a series of optimization strategies, TarFlow sampling
can be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as
GS-Jacobi) iteration method. Specifically, we find that blocks in the TarFlow
model have varying importance: a small number of blocks play a major role in
image generation tasks, while other blocks contribute relatively little; some
blocks are sensitive to initial values and prone to numerical overflow, while
others are relatively robust. Based on these two characteristics, we propose
the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM
is used to identify whether a TarFlow block is "simple" (converges in few
iterations) or "tough" (requires more iterations); IGM is used to evaluate
whether the initial value of the iteration is good. Experiments on four TarFlow
models demonstrate that GS-Jacobi sampling can significantly enhance sampling
efficiency while maintaining the quality of generated images (measured by FID),
achieving speed-ups of 4.53x in Img128cond, 5.32x in AFHQ, 2.96x in
Img64uncond, and 2.51x in Img64cond without degrading FID scores or sample
quality. Code and checkpoints are accessible on
https://github.com/encoreus/GS-Jacobi_for_TarFlow

</details>

### [774] [The Way Up: A Dataset for Hold Usage Detection in Sport Climbing](https://arxiv.org/abs/2505.12854)
*Anna Maschek,David C. Schedl*

Main category: cs.CV

TLDR: 论文介绍了一个标注了攀岩视频的数据集，并探讨了基于关键点的2D姿态估计模型在攀岩中的应用。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏详细标注攀岩动作的数据集，阻碍了攀岩相关应用的发展。

Method: 通过分析关节关键点与攀岩支点的重叠，确定支点使用情况，并评估多种先进模型的准确性。

Result: 数据集和实验结果揭示了攀岩姿态估计的关键挑战。

Conclusion: 为未来AI辅助攀岩系统的研究奠定了基础。

Abstract: Detecting an athlete's position on a route and identifying hold usage are
crucial in various climbing-related applications. However, no climbing dataset
with detailed hold usage annotations exists to our knowledge. To address this
issue, we introduce a dataset of 22 annotated climbing videos, providing
ground-truth labels for hold locations, usage order, and time of use.
Furthermore, we explore the application of keypoint-based 2D pose-estimation
models for detecting hold usage in sport climbing. We determine usage by
analyzing the key points of certain joints and the corresponding overlap with
climbing holds. We evaluate multiple state-of-the-art models and analyze their
accuracy on our dataset, identifying and highlighting climbing-specific
challenges. Our dataset and results highlight key challenges in
climbing-specific pose estimation and establish a foundation for future
research toward AI-assisted systems for sports climbing.

</details>

### [775] [Towards a Universal Image Degradation Model via Content-Degradation Disentanglement](https://arxiv.org/abs/2505.12860)
*Wenbo Yang,Zhongling Wang,Zhou Wang*

Main category: cs.CV

TLDR: 提出了一种通用退化模型，能够合成包含全局和空间变化成分的复杂退化，无需用户干预。


<details>
  <summary>Details</summary>
Motivation: 现有模型只能生成特定或狭窄的退化类型，缺乏通用性和适应性。

Method: 通过解耦压缩方法分离退化信息，并设计新模块提取和整合空间变化退化。

Result: 在胶片颗粒模拟和盲图像恢复任务中验证了模型的准确性和适应性。

Conclusion: 该模型为图像退化合成提供了通用且高效的解决方案。

Abstract: Image degradation synthesis is highly desirable in a wide variety of
applications ranging from image restoration to simulating artistic effects.
Existing models are designed to generate one specific or a narrow set of
degradations, which often require user-provided degradation parameters. As a
result, they lack the generalizability to synthesize degradations beyond their
initial design or adapt to other applications. Here we propose the first
universal degradation model that can synthesize a broad spectrum of complex and
realistic degradations containing both homogeneous (global) and inhomogeneous
(spatially varying) components. Our model automatically extracts and
disentangles homogeneous and inhomogeneous degradation features, which are
later used for degradation synthesis without user intervention. A
disentangle-by-compression method is proposed to separate degradation
information from images. Two novel modules for extracting and incorporating
inhomogeneous degradations are created to model inhomogeneous components in
complex degradations. We demonstrate the model's accuracy and adaptability in
film-grain simulation and blind image restoration tasks. The demo video, code,
and dataset of this project will be released upon publication at
github.com/yangwenbo99/content-degradation-disentanglement.

</details>

### [776] [Robust Multimodal Segmentation with Representation Regularization and Hybrid Prototype Distillation](https://arxiv.org/abs/2505.12861)
*Jiaqi Tan,Xu Zheng,Yang Liu*

Main category: cs.CV

TLDR: RobustSeg提出了一种两阶段框架，通过HPDM和RRM增强多模态鲁棒性，显著提升了多模态语义分割的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态语义分割在动态环境、传感器故障和噪声干扰下面临挑战，理论模型与实际性能存在差距。

Method: RobustSeg采用两阶段框架：第一阶段预训练多模态教师模型；第二阶段通过HPDM和RRM训练学生模型，利用随机模态丢失和知识蒸馏。

Result: 在三个公开基准测试中，RobustSeg性能优于现有方法，分别提升了2.76%、4.56%和0.98%。

Conclusion: RobustSeg通过HPDM和RRM有效解决了多模态语义分割的鲁棒性问题，性能显著提升。

Abstract: Multi-modal semantic segmentation (MMSS) faces significant challenges in
real-world scenarios due to dynamic environments, sensor failures, and noise
interference, creating a gap between theoretical models and practical
performance. To address this, we propose a two-stage framework called
RobustSeg, which enhances multi-modal robustness through two key components:
the Hybrid Prototype Distillation Module (HPDM) and the Representation
Regularization Module (RRM). In the first stage, RobustSeg pre-trains a
multi-modal teacher model using complete modalities. In the second stage, a
student model is trained with random modality dropout while learning from the
teacher via HPDM and RRM. HPDM transforms features into compact prototypes,
enabling cross-modal hybrid knowledge distillation and mitigating bias from
missing modalities. RRM reduces representation discrepancies between the
teacher and student by optimizing functional entropy through the log-Sobolev
inequality. Extensive experiments on three public benchmarks demonstrate that
RobustSeg outperforms previous state-of-the-art methods, achieving improvements
of +2.76%, +4.56%, and +0.98%, respectively. Code is available at:
https://github.com/RobustSeg/RobustSeg.

</details>

### [777] [ORQA: A Benchmark and Foundation Model for Holistic Operating Room Modeling](https://arxiv.org/abs/2505.12890)
*Ege Özsoy,Chantal Pellegrini,David Bani-Harouni,Kun Yuan,Matthias Keicher,Nassir Navab*

Main category: cs.CV

TLDR: ORQA是一个新型手术室问答基准和多模态基础模型，旨在提升手术室智能。通过整合四个公共数据集，支持多样化任务，并提出渐进式知识蒸馏方法，优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 手术复杂性要求系统具备全面理解能力，而现有研究局限于单任务，缺乏通用性。ORQA旨在解决这一问题。

Method: 整合四个公共数据集构建基准，提出多模态大语言模型融合视觉、听觉和结构化数据，并采用渐进式知识蒸馏优化模型。

Result: ORQA在基准测试中表现优异，具备零样本泛化能力，推动了多模态手术智能的发展。

Conclusion: ORQA为手术室智能提供了可扩展的统一建模方法，显著推进了多模态手术智能的研究。

Abstract: The real-world complexity of surgeries necessitates surgeons to have deep and
holistic comprehension to ensure precision, safety, and effective
interventions. Computational systems are required to have a similar level of
comprehension within the operating room. Prior works, limited to single-task
efforts like phase recognition or scene graph generation, lack scope and
generalizability. In this work, we introduce ORQA, a novel OR question
answering benchmark and foundational multimodal model to advance OR
intelligence. By unifying all four public OR datasets into a comprehensive
benchmark, we enable our approach to concurrently address a diverse range of OR
challenges. The proposed multimodal large language model fuses diverse OR
signals such as visual, auditory, and structured data, for a holistic modeling
of the OR. Finally, we propose a novel, progressive knowledge distillation
paradigm, to generate a family of models optimized for different speed and
memory requirements. We show the strong performance of ORQA on our proposed
benchmark, and its zero-shot generalization, paving the way for scalable,
unified OR modeling and significantly advancing multimodal surgical
intelligence. We will release our code and data upon acceptance.

</details>

### [778] [EPIC: Explanation of Pretrained Image Classification Networks via Prototype](https://arxiv.org/abs/2505.12897)
*Piotr Borycki,Magdalena Trędowicz,Szymon Janusz,Jacek Tabor,Przemysław Spurek,Arkadiusz Lewicki,Łukasz Struski*

Main category: cs.CV

TLDR: EPIC是一种新型的XAI方法，结合了后处理和原型解释的优点，无需修改预训练模型即可提供直观的原型解释。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法中，后处理方法解释粗糙，而原型方法需要专用架构和训练。EPIC旨在填补两者之间的空白。

Method: EPIC在预训练模型上运行，无需架构修改，通过原型解释模型决策，结合了后处理和原型方法的优势。

Result: EPIC在CUB-200-2011、Stanford Cars和ImageNet等数据集上验证了其解释能力，提供了高质量的解释。

Conclusion: EPIC是首个能够完全复制固有可解释模型核心解释能力的后处理方法，为XAI提供了灵活且易于理解的工具。

Abstract: Explainable AI (XAI) methods generally fall into two categories. Post-hoc
approaches generate explanations for pre-trained models and are compatible with
various neural network architectures. These methods often use feature
importance visualizations, such as saliency maps, to indicate which input
regions influenced the model's prediction. Unfortunately, they typically offer
a coarse understanding of the model's decision-making process. In contrast,
ante-hoc (inherently explainable) methods rely on specially designed model
architectures trained from scratch. A notable subclass of these methods
provides explanations through prototypes, representative patches extracted from
the training data. However, prototype-based approaches have limitations: they
require dedicated architectures, involve specialized training procedures, and
perform well only on specific datasets. In this work, we propose EPIC
(Explanation of Pretrained Image Classification), a novel approach that bridges
the gap between these two paradigms. Like post-hoc methods, EPIC operates on
pre-trained models without architectural modifications. Simultaneously, it
delivers intuitive, prototype-based explanations inspired by ante-hoc
techniques. To the best of our knowledge, EPIC is the first post-hoc method
capable of fully replicating the core explanatory power of inherently
interpretable models. We evaluate EPIC on benchmark datasets commonly used in
prototype-based explanations, such as CUB-200-2011 and Stanford Cars, alongside
large-scale datasets like ImageNet, typically employed by post-hoc methods.
EPIC uses prototypes to explain model decisions, providing a flexible and
easy-to-understand tool for creating clear, high-quality explanations.

</details>

### [779] [Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach](https://arxiv.org/abs/2505.12903)
*Shiao Wang,Xiao Wang,Liye Jin,Bo Jiang,Lin Zhu,Lan Chen,Yonghong Tian,Bin Luo*

Main category: cs.CV

TLDR: 提出了一种名为SFTrack的慢快跟踪范式，结合高精度慢跟踪器和高效快跟踪器，适应不同资源需求。


<details>
  <summary>Details</summary>
Motivation: 传统基于帧的跟踪方法在低延迟和资源受限环境下表现不佳，事件相机提供了一种新的解决方案。

Method: 通过图表示学习从事件流中提取信息，并集成到两个FlashAttention视觉骨干中，分别生成慢快跟踪器。快跟踪器通过轻量设计和单次前向多输出实现低延迟。

Result: 在多个公开基准测试中验证了方法的有效性和效率。

Conclusion: SFTrack在不同实际场景中表现出色，代码已开源。

Abstract: Existing tracking algorithms typically rely on low-frame-rate RGB cameras
coupled with computationally intensive deep neural network architectures to
achieve effective tracking. However, such frame-based methods inherently face
challenges in achieving low-latency performance and often fail in
resource-constrained environments. Visual object tracking using bio-inspired
event cameras has emerged as a promising research direction in recent years,
offering distinct advantages for low-latency applications. In this paper, we
propose a novel Slow-Fast Tracking paradigm that flexibly adapts to different
operational requirements, termed SFTrack. The proposed framework supports two
complementary modes, i.e., a high-precision slow tracker for scenarios with
sufficient computational resources, and an efficient fast tracker tailored for
latency-aware, resource-constrained environments. Specifically, our framework
first performs graph-based representation learning from
high-temporal-resolution event streams, and then integrates the learned
graph-structured information into two FlashAttention-based vision backbones,
yielding the slow and fast trackers, respectively. The fast tracker achieves
low latency through a lightweight network design and by producing multiple
bounding box outputs in a single forward pass. Finally, we seamlessly combine
both trackers via supervised fine-tuning and further enhance the fast tracker's
performance through a knowledge distillation strategy. Extensive experiments on
public benchmarks, including FE240, COESOT, and EventVOT, demonstrate the
effectiveness and efficiency of our proposed method across different real-world
scenarios. The source code has been released on
https://github.com/Event-AHU/SlowFast_Event_Track.

</details>

### [780] [Dynamic Graph Induced Contour-aware Heat Conduction Network for Event-based Object Detection](https://arxiv.org/abs/2505.12908)
*Xiao Wang,Yu Jin,Lan Chen,Bo Jiang,Lin Zhu,Yonghong Tian,Jin Tang,Bin Luo*

Main category: cs.CV

TLDR: 提出了一种基于事件流的目标检测新方法CvHeat-DET，通过动态图和轮廓感知的热传导网络解决现有方法在轮廓建模和多尺度特征利用上的不足。


<details>
  <summary>Details</summary>
Motivation: 事件视觉传感器（EVS）在低光、高速和低延迟场景中优于传统RGB相机，但现有目标检测算法在轮廓建模和多尺度特征利用上表现不佳。

Method: 提出CvHeat-DET模型，利用事件流的清晰轮廓信息预测热传导系数，并整合多尺度图结构特征。

Result: 在三个基准数据集上的实验验证了模型的有效性。

Conclusion: CvHeat-DET在事件流目标检测中实现了效率和精度的平衡，代码已开源。

Abstract: Event-based Vision Sensors (EVS) have demonstrated significant advantages
over traditional RGB frame-based cameras in low-light conditions, high-speed
motion capture, and low latency. Consequently, object detection based on EVS
has attracted increasing attention from researchers. Current event stream
object detection algorithms are typically built upon Convolutional Neural
Networks (CNNs) or Transformers, which either capture limited local features
using convolutional filters or incur high computational costs due to the
utilization of self-attention. Recently proposed vision heat conduction
backbone networks have shown a good balance between efficiency and accuracy;
however, these models are not specifically designed for event stream data. They
exhibit weak capability in modeling object contour information and fail to
exploit the benefits of multi-scale features. To address these issues, this
paper proposes a novel dynamic graph induced contour-aware heat conduction
network for event stream based object detection, termed CvHeat-DET. The
proposed model effectively leverages the clear contour information inherent in
event streams to predict the thermal diffusivity coefficients within the heat
conduction model, and integrates hierarchical structural graph features to
enhance feature learning across multiple scales. Extensive experiments on three
benchmark datasets for event stream-based object detection fully validated the
effectiveness of the proposed model. The source code of this paper will be
released on https://github.com/Event-AHU/OpenEvDET.

</details>

### [781] [HiERO: understanding the hierarchy of human behavior enhances reasoning on egocentric videos](https://arxiv.org/abs/2505.12911)
*Simone Alberto Peirone,Francesca Pistilli,Giuseppe Averta*

Main category: cs.CV

TLDR: HiERO是一种弱监督方法，通过层次化活动线程丰富视频片段特征，利用视频与描述的对齐实现上下文、语义和时间推理，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 人类活动复杂多变，但其变异性具有层次化结构，可以利用无脚本视频中的这种结构更好地推理活动内容。

Method: HiERO通过视频片段与叙述描述的对齐，推断上下文、语义和时间推理，采用层次化架构。

Result: 在多个视频-文本对齐基准测试（EgoMCQ、EgoNLQ）和零样本程序学习任务（EgoProceL、Ego4D Goal-Step）中表现优异，甚至优于全监督方法。

Conclusion: 层次化人类活动知识对第一人称视觉中的多推理任务具有重要价值。

Abstract: Human activities are particularly complex and variable, and this makes
challenging for deep learning models to reason about them. However, we note
that such variability does have an underlying structure, composed of a
hierarchy of patterns of related actions. We argue that such structure can
emerge naturally from unscripted videos of human activities, and can be
leveraged to better reason about their content. We present HiERO, a
weakly-supervised method to enrich video segments features with the
corresponding hierarchical activity threads. By aligning video clips with their
narrated descriptions, HiERO infers contextual, semantic and temporal reasoning
with an hierarchical architecture. We prove the potential of our enriched
features with multiple video-text alignment benchmarks (EgoMCQ, EgoNLQ) with
minimal additional training, and in zero-shot for procedure learning tasks
(EgoProceL and Ego4D Goal-Step). Notably, HiERO achieves state-of-the-art
performance in all the benchmarks, and for procedure learning tasks it
outperforms fully-supervised methods by a large margin (+12.5% F1 on EgoProceL)
in zero shot. Our results prove the relevance of using knowledge of the
hierarchy of human activities for multiple reasoning tasks in egocentric
vision.

</details>

### [782] [Uniformity First: Uniformity-aware Test-time Adaptation of Vision-language Models against Image Corruption](https://arxiv.org/abs/2505.12912)
*Kazuki Adachi,Shin'ya Yamaguchi,Tomoki Hamagami*

Main category: cs.CV

TLDR: 论文提出了一种名为UnInfo的新方法，用于解决预训练视觉语言模型（如CLIP）在传感器退化等分布偏移问题中的脆弱性，通过均匀性感知的信息平衡测试时适应（TTA）提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: CLIP等预训练视觉语言模型在分布偏移（如传感器退化）下表现不佳，而收集新数据微调成本高昂，因此研究如何在测试时通过无标签数据自适应调整。

Method: 提出UnInfo方法，包括均匀性感知置信度最大化、信息感知损失平衡和EMA教师的知识蒸馏，以解决图像嵌入的均匀性损坏问题。

Result: 实验表明，UnInfo通过保持均匀性信息，显著提升了传感器退化条件下的分类准确率。

Conclusion: UnInfo方法有效解决了CLIP在传感器退化等分布偏移下的适应问题，为实际应用提供了鲁棒性解决方案。

Abstract: Pre-trained vision-language models such as contrastive language-image
pre-training (CLIP) have demonstrated a remarkable generalizability, which has
enabled a wide range of applications represented by zero-shot classification.
However, vision-language models still suffer when they face datasets with large
gaps from training ones, i.e., distribution shifts. We found that CLIP is
especially vulnerable to sensor degradation, a type of realistic distribution
shift caused by sensor conditions such as weather, light, or noise. Collecting
a new dataset from a test distribution for fine-tuning highly costs since
sensor degradation occurs unexpectedly and has a range of variety. Thus, we
investigate test-time adaptation (TTA) of zero-shot classification, which
enables on-the-fly adaptation to the test distribution with unlabeled test
data. Existing TTA methods for CLIP mainly focus on modifying image and text
embeddings or predictions to address distribution shifts. Although these
methods can adapt to domain shifts, such as fine-grained labels spaces or
different renditions in input images, they fail to adapt to distribution shifts
caused by sensor degradation. We found that this is because image embeddings
are "corrupted" in terms of uniformity, a measure related to the amount of
information. To make models robust to sensor degradation, we propose a novel
method called uniformity-aware information-balanced TTA (UnInfo). To address
the corruption of image embeddings, we introduce uniformity-aware confidence
maximization, information-aware loss balancing, and knowledge distillation from
the exponential moving average (EMA) teacher. Through experiments, we
demonstrate that our UnInfo improves accuracy under sensor degradation by
retaining information in terms of uniformity.

</details>

### [783] [LatentINDIGO: An INN-Guided Latent Diffusion Algorithm for Image Restoration](https://arxiv.org/abs/2505.12935)
*Di You,Daniel Siromani,Pier Luigi Dragotti*

Main category: cs.CV

TLDR: 论文提出了一种基于小波可逆神经网络的潜在扩散模型，用于解决图像修复任务中的复杂或未知退化问题，并在潜在空间中减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于预定义的退化算子，难以处理复杂或未知的退化，且在潜在空间中缺乏稳定指导，计算开销大。

Method: 提出小波启发的可逆神经网络（INN），通过前向变换模拟退化，逆向变换恢复细节，并将其集成到潜在扩散模型中。

Result: 实验表明，该方法在合成和真实低质量图像上达到最优性能，并能适应任意输出尺寸。

Conclusion: 该方法有效解决了图像修复中的退化建模和计算效率问题，具有广泛适用性。

Abstract: There is a growing interest in the use of latent diffusion models (LDMs) for
image restoration (IR) tasks due to their ability to model effectively the
distribution of natural images. While significant progress has been made, there
are still key challenges that need to be addressed. First, many approaches
depend on a predefined degradation operator, making them ill-suited for complex
or unknown degradations that deviate from standard analytical models. Second,
many methods struggle to provide a stable guidance in the latent space and
finally most methods convert latent representations back to the pixel domain
for guidance at every sampling iteration, which significantly increases
computational and memory overhead. To overcome these limitations, we introduce
a wavelet-inspired invertible neural network (INN) that simulates degradations
through a forward transform and reconstructs lost details via the inverse
transform. We further integrate this design into a latent diffusion pipeline
through two proposed approaches: LatentINDIGO-PixelINN, which operates in the
pixel domain, and LatentINDIGO-LatentINN, which stays fully in the latent space
to reduce complexity. Both approaches alternate between updating intermediate
latent variables under the guidance of our INN and refining the INN forward
model to handle unknown degradations. In addition, a regularization step
preserves the proximity of latent variables to the natural image manifold.
Experiments demonstrate that our algorithm achieves state-of-the-art
performance on synthetic and real-world low-quality images, and can be readily
adapted to arbitrary output sizes.

</details>

### [784] [Multiscale Adaptive Conflict-Balancing Model For Multimedia Deepfake Detection](https://arxiv.org/abs/2505.12966)
*Zihan Xiong,Xiaohua Wu,Lei Chen,Fangqi Lou*

Main category: cs.CV

TLDR: 提出了一种音频-视觉联合学习方法（MACB-DF），通过对比学习和多模态融合解决模态冲突和忽视问题，提升深度伪造检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态检测方法因模态间学习不平衡而受限，影响了多媒体可信度。

Method: 采用对比学习辅助多级跨模态融合，设计正交化-多模态帕累托模块以保留单模态信息并解决梯度冲突。

Result: 在主流深度伪造数据集上平均准确率达95.5%，跨数据集泛化能力显著提升。

Conclusion: MACB-DF方法在多模态深度伪造检测中表现出色，具有实际应用潜力。

Abstract: Advances in computer vision and deep learning have blurred the line between
deepfakes and authentic media, undermining multimedia credibility through
audio-visual forgery. Current multimodal detection methods remain limited by
unbalanced learning between modalities. To tackle this issue, we propose an
Audio-Visual Joint Learning Method (MACB-DF) to better mitigate modality
conflicts and neglect by leveraging contrastive learning to assist in
multi-level and cross-modal fusion, thereby fully balancing and exploiting
information from each modality. Additionally, we designed an
orthogonalization-multimodal pareto module that preserves unimodal information
while addressing gradient conflicts in audio-video encoders caused by differing
optimization targets of the loss functions. Extensive experiments and ablation
studies conducted on mainstream deepfake datasets demonstrate consistent
performance gains of our model across key evaluation metrics, achieving an
average accuracy of 95.5% across multiple datasets. Notably, our method
exhibits superior cross-dataset generalization capabilities, with absolute
improvements of 8.0% and 7.7% in ACC scores over the previous best-performing
approach when trained on DFDC and tested on DefakeAVMiT and FakeAVCeleb
datasets.

</details>

### [785] [A Skull-Adaptive Framework for AI-Based 3D Transcranial Focused Ultrasound Simulation](https://arxiv.org/abs/2505.12998)
*Vinkle Srivastav,Juliette Puel,Jonathan Vappou,Elijah Van Houten,Paolo Cabras,Nicolas Padoy*

Main category: cs.CV

TLDR: TFUScapes是一个大规模、高分辨率的tFUS模拟数据集，结合DeepTFUS深度学习模型，用于估计压力场，加速非侵入性脑刺激研究。


<details>
  <summary>Details</summary>
Motivation: 解决人类颅骨异质性和各向异性导致的超声波波前扭曲问题，减少患者特异性规划和校正的时间消耗。

Method: 使用k-Wave伪谱求解器生成模拟数据，开发DeepTFUS模型，结合U-Net和傅里叶编码位置嵌入等技术。

Result: 公开了TFUScapes数据集，并展示了DeepTFUS模型在估计压力场方面的有效性。

Conclusion: TFUScapes和DeepTFUS为计算声学、神经技术和深度学习的交叉研究提供了重要资源。

Abstract: Transcranial focused ultrasound (tFUS) is an emerging modality for
non-invasive brain stimulation and therapeutic intervention, offering
millimeter-scale spatial precision and the ability to target deep brain
structures. However, the heterogeneous and anisotropic nature of the human
skull introduces significant distortions to the propagating ultrasound
wavefront, which require time-consuming patient-specific planning and
corrections using numerical solvers for accurate targeting. To enable
data-driven approaches in this domain, we introduce TFUScapes, the first
large-scale, high-resolution dataset of tFUS simulations through anatomically
realistic human skulls derived from T1-weighted MRI images. We have developed a
scalable simulation engine pipeline using the k-Wave pseudo-spectral solver,
where each simulation returns a steady-state pressure field generated by a
focused ultrasound transducer placed at realistic scalp locations. In addition
to the dataset, we present DeepTFUS, a deep learning model that estimates
normalized pressure fields directly from input 3D CT volumes and transducer
position. The model extends a U-Net backbone with transducer-aware
conditioning, incorporating Fourier-encoded position embeddings and MLP layers
to create global transducer embeddings. These embeddings are fused with U-Net
encoder features via feature-wise modulation, dynamic convolutions, and
cross-attention mechanisms. The model is trained using a combination of
spatially weighted and gradient-sensitive loss functions, enabling it to
approximate high-fidelity wavefields. The TFUScapes dataset is publicly
released to accelerate research at the intersection of computational acoustics,
neurotechnology, and deep learning. The project page is available at
https://github.com/CAMMA-public/TFUScapes.

</details>

### [786] [Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions](https://arxiv.org/abs/2505.13023)
*Yimao Guo,Zuomin Qu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TLDR: Anti-Inpainting是一种主动防御方法，通过三重机制保护图像免受未知条件下的恶意篡改，包括多级深度特征提取、多尺度语义保留数据增强和基于选择的分布偏差优化策略。


<details>
  <summary>Details</summary>
Motivation: 解决现有主动防御方法仅能在已知条件下保护图像，无法应对恶意用户设计的未知篡改条件的问题。

Method: 提出三重机制：多级深度特征提取器、多尺度语义保留数据增强和基于选择的分布偏差优化策略。

Result: 实验表明Anti-Inpainting在InpaintGuardBench和CelebA-HQ上对未知条件下的篡改具有有效防御能力，并对不同版本的扩散模型具有鲁棒性和可迁移性。

Conclusion: Anti-Inpainting为未知条件下的图像篡改提供了有效的主动防御解决方案。

Abstract: As diffusion-based malicious image manipulation becomes increasingly
prevalent, multiple proactive defense methods are developed to safeguard images
against unauthorized tampering. However, most proactive defense methods only
can safeguard images against manipulation under known conditions, and fail to
protect images from manipulations guided by tampering conditions crafted by
malicious users. To tackle this issue, we propose Anti-Inpainting, a proactive
defense method that achieves adequate protection under unknown conditions
through a triple mechanism to address this challenge. Specifically, a
multi-level deep feature extractor is presented to obtain intricate features
during the diffusion denoising process to improve protective effectiveness. We
design multi-scale semantic-preserving data augmentation to enhance the
transferability of adversarial perturbations across unknown conditions by
multi-scale transformations while preserving semantic integrity. In addition,
we propose a selection-based distribution deviation optimization strategy to
improve the protection of adversarial perturbation against manipulation under
diverse random seeds. Extensive experiments indicate the proactive defensive
performance of Anti-Inpainting against diffusion-based inpainters guided by
unknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we
also demonstrate the proposed approach's robustness under various image
purification methods and its transferability across different versions of
diffusion models.

</details>

### [787] [Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields in Efficient CNNs for Fair Medical Image Classification](https://arxiv.org/abs/2505.13039)
*Xiao Wu,Xiaoqing Zhang,Zunjie Xiao,Lingxi Hu,Risa Higashita,Jiang Liu*

Main category: cs.CV

TLDR: 论文提出了一种名为ERoHPRF的新方法，通过异构金字塔感受野模拟多专家会诊模式，提升医学图像分类的性能和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有CNN架构在医学图像分类中存在两个主要问题：1) 难以高效捕捉多样病灶特征；2) 预测结果存在偏见，影响实际诊断。

Method: 设计了异构金字塔感受野（ERoHPRF），通过多异构核尺寸卷积操作捕捉病灶特征，并引入专家级结构重参数化技术优化计算成本。

Result: 实验表明，ERoHPRF在分类性能、公平性和计算开销上优于现有方法。

Conclusion: ERoHPRF为医学图像分类提供了一种高效且公平的解决方案，代码即将开源。

Abstract: Efficient convolutional neural network (CNN) architecture designs have
attracted growing research interests. However, they usually apply single
receptive field (RF), small asymmetric RFs, or pyramid RFs to learn different
feature representations, still encountering two significant challenges in
medical image classification tasks: 1) They have limitations in capturing
diverse lesion characteristics efficiently, e.g., tiny, coordination, small and
salient, which have unique roles on results, especially imbalanced medical
image classification. 2) The predictions generated by those CNNs are often
unfair/biased, bringing a high risk by employing them to real-world medical
diagnosis conditions. To tackle these issues, we develop a new concept,
Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields
(ERoHPRF), to simultaneously boost medical image classification performance and
fairness. This concept aims to mimic the multi-expert consultation mode by
applying the well-designed heterogeneous pyramid RF bags to capture different
lesion characteristics effectively via convolution operations with multiple
heterogeneous kernel sizes. Additionally, ERoHPRF introduces an expert-like
structural reparameterization technique to merge its parameters with the
two-stage strategy, ensuring competitive computation cost and inference speed
through comparisons to a single RF. To manifest the effectiveness and
generalization ability of ERoHPRF, we incorporate it into mainstream efficient
CNN architectures. The extensive experiments show that our method maintains a
better trade-off than state-of-the-art methods in terms of medical image
classification, fairness, and computation overhead. The codes of this paper
will be released soon.

</details>

### [788] [A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation](https://arxiv.org/abs/2505.13043)
*Hao-Ran Yang,Xiaohui Chen,Chuan-Xian Ren*

Main category: cs.CV

TLDR: 论文提出了一种基于广义标签偏移（GLS）理论的跨域注视估计方法，通过标签和条件偏移建模，引入截断高斯分布的重加权策略，提升了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有跨域注视估计方法仅关注特征空间的域不变性，但广义标签偏移理论表明其不足，因此需要从标签和条件偏移角度重新建模。

Method: 提出GLS校正框架，采用截断高斯分布的重加权策略解决标签偏移问题，并推导概率感知的条件算子差异估计。

Result: 在标准跨域注视估计任务中，该方法在不同骨干模型上均表现出优异的泛化能力和适用性。

Conclusion: 通过GLS视角和概率感知方法，有效解决了跨域注视估计中的标签和条件偏移问题。

Abstract: Aiming to generalize the well-trained gaze estimation model to new target
domains, Cross-domain Gaze Estimation (CDGE) is developed for real-world
application scenarios. Existing CDGE methods typically extract the
domain-invariant features to mitigate domain shift in feature space, which is
proved insufficient by Generalized Label Shift (GLS) theory. In this paper, we
introduce a novel GLS perspective to CDGE and modelize the cross-domain problem
by label and conditional shift problem. A GLS correction framework is presented
and a feasible realization is proposed, in which a importance reweighting
strategy based on truncated Gaussian distribution is introduced to overcome the
continuity challenges in label shift correction. To embed the reweighted source
distribution to conditional invariant learning, we further derive a
probability-aware estimation of conditional operator discrepancy. Extensive
experiments on standard CDGE tasks with different backbone models validate the
superior generalization capability across domain and applicability on various
models of proposed method.

</details>

### [789] [RGB-to-Polarization Estimation: A New Task and Benchmark Study](https://arxiv.org/abs/2505.13050)
*Beibei Lin,Zifeng Yuan,Tingting Chen*

Main category: cs.CV

TLDR: 该论文提出了从RGB图像估计偏振信息的新任务，并建立了首个综合基准，评估了多种深度学习模型，揭示了不同模型的优缺点，并提供了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 偏振图像包含丰富物理信息，但获取成本高且复杂。为了填补这一空白，研究旨在从RGB图像直接推断偏振信息。

Method: 利用现有偏振数据集，评估了多种深度学习模型（包括恢复导向和生成架构），通过定量和定性分析建立性能基准。

Result: 基准确定了RGB到偏振估计的当前性能上限，并系统揭示了不同模型家族的优缺点。

Conclusion: 该基准为未来偏振估计方法的设计和评估提供了基础资源，并指出了潜在研究方向。

Abstract: Polarization images provide rich physical information that is fundamentally
absent from standard RGB images, benefiting a wide range of computer vision
applications such as reflection separation and material classification.
However, the acquisition of polarization images typically requires additional
optical components, which increases both the cost and the complexity of the
applications. To bridge this gap, we introduce a new task: RGB-to-polarization
image estimation, which aims to infer polarization information directly from
RGB images. In this work, we establish the first comprehensive benchmark for
this task by leveraging existing polarization datasets and evaluating a diverse
set of state-of-the-art deep learning models, including both
restoration-oriented and generative architectures. Through extensive
quantitative and qualitative analysis, our benchmark not only establishes the
current performance ceiling of RGB-to-polarization estimation, but also
systematically reveals the respective strengths and limitations of different
model families -- such as direct reconstruction versus generative synthesis,
and task-specific training versus large-scale pre-training. In addition, we
provide some potential directions for future research on polarization
estimation. This benchmark is intended to serve as a foundational resource to
facilitate the design and evaluation of future methods for polarization
estimation from standard RGB inputs.

</details>

### [790] [3D Visual Illusion Depth Estimation](https://arxiv.org/abs/2505.13061)
*CHengtang Yao,Zhidan Liu,Jiaxi Zeng,Lidong Yu,Yuwei Wu,Yunde Jia*

Main category: cs.CV

TLDR: 论文揭示了机器视觉系统会被3D视觉错觉欺骗，并提出了一种结合视觉语言模型的鲁棒深度估计框架，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索3D视觉错觉对深度估计的影响，并解决现有方法在此问题上的不足。

Method: 收集大规模数据集，训练和评估现有深度估计方法，并提出一种结合视觉语言模型的鲁棒框架。

Result: 实验表明现有方法均被3D视觉错觉欺骗，而提出的方法表现最优。

Conclusion: 提出的框架能有效应对3D视觉错觉，提升深度估计的鲁棒性。

Abstract: 3D visual illusion is a perceptual phenomenon where a two-dimensional plane
is manipulated to simulate three-dimensional spatial relationships, making a
flat artwork or object look three-dimensional in the human visual system. In
this paper, we reveal that the machine visual system is also seriously fooled
by 3D visual illusions, including monocular and binocular depth estimation. In
order to explore and analyze the impact of 3D visual illusion on depth
estimation, we collect a large dataset containing almost 3k scenes and 200k
images to train and evaluate SOTA monocular and binocular depth estimation
methods. We also propose a robust depth estimation framework that uses common
sense from a vision-language model to adaptively select reliable depth from
binocular disparity and monocular depth. Experiments show that SOTA monocular,
binocular, and multi-view depth estimation approaches are all fooled by various
3D visual illusions, while our method achieves SOTA performance.

</details>

### [791] [Cross-modal feature fusion for robust point cloud registration with ambiguous geometry](https://arxiv.org/abs/2505.13088)
*Zhaoyi Wang,Shengyu Huang,Jemil Avers Butt,Yuanzhou Cai,Matej Varga,Andreas Wieser*

Main category: cs.CV

TLDR: CoFF是一种新颖的跨模态特征融合方法，结合点云几何和RGB图像进行点云配准，显著提升了配准性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法常忽略RGB图像的辐射信息，导致在几何数据不足的区域配准效果不佳。CoFF旨在通过融合几何和图像特征解决这一问题。

Method: CoFF采用两阶段融合策略：1）像素级图像特征与3D点云特征融合；2）块级图像特征与超点特征融合，并通过粗到细匹配模块建立对应关系。

Result: 在3DMatch、3DLoMatch等数据集上，CoFF实现了95.9%和81.6%的配准召回率，性能达到SOTA。

Conclusion: CoFF通过跨模态特征融合显著提升了点云配准效果，尤其在几何模糊区域表现优异。

Abstract: Point cloud registration has seen significant advancements with the
application of deep learning techniques. However, existing approaches often
overlook the potential of integrating radiometric information from RGB images.
This limitation reduces their effectiveness in aligning point clouds pairs,
especially in regions where geometric data alone is insufficient. When used
effectively, radiometric information can enhance the registration process by
providing context that is missing from purely geometric data. In this paper, we
propose CoFF, a novel Cross-modal Feature Fusion method that utilizes both
point cloud geometry and RGB images for pairwise point cloud registration.
Assuming that the co-registration between point clouds and RGB images is
available, CoFF explicitly addresses the challenges where geometric information
alone is unclear, such as in regions with symmetric similarity or planar
structures, through a two-stage fusion of 3D point cloud features and 2D image
features. It incorporates a cross-modal feature fusion module that assigns
pixel-wise image features to 3D input point clouds to enhance learned 3D point
features, and integrates patch-wise image features with superpoint features to
improve the quality of coarse matching. This is followed by a coarse-to-fine
matching module that accurately establishes correspondences using the fused
features. We extensively evaluate CoFF on four common datasets: 3DMatch,
3DLoMatch, IndoorLRS, and the recently released ScanNet++ datasets. In
addition, we assess CoFF on specific subset datasets containing geometrically
ambiguous cases. Our experimental results demonstrate that CoFF achieves
state-of-the-art registration performance across all benchmarks, including
remarkable registration recalls of 95.9% and 81.6% on the widely-used 3DMatch
and 3DLoMatch datasets, respectively...(Truncated to fit arXiv abstract length)

</details>

### [792] [Touch2Shape: Touch-Conditioned 3D Diffusion for Shape Exploration and Reconstruction](https://arxiv.org/abs/2505.13091)
*Yuanbo Wang,Zhaoxuan Zhang,Jiajin Qiu,Dilong Sun,Zhengyu Meng,Xiaopeng Wei,Xin Yang*

Main category: cs.CV

TLDR: 提出Touch2Shape模型，利用触觉图像和扩散模型结合强化学习，解决现有3D扩散模型在局部细节捕捉和遮挡条件下的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有3D扩散模型在全局上下文理解上表现优异，但在复杂形状的局部细节捕捉及遮挡和光照条件下受限。

Method: 结合触觉图像捕捉局部3D信息，提出触觉条件扩散模型Touch2Shape，包括触觉嵌入模块和触觉形状融合模块，并利用强化学习训练探索策略。

Result: 实验验证了重建质量，触觉探索策略进一步提升了重建性能。

Conclusion: Touch2Shape模型通过触觉和扩散模型的结合，有效解决了局部细节和遮挡问题，提升了3D生成任务的性能。

Abstract: Diffusion models have made breakthroughs in 3D generation tasks. Current 3D
diffusion models focus on reconstructing target shape from images or a set of
partial observations. While excelling in global context understanding, they
struggle to capture the local details of complex shapes and limited to the
occlusion and lighting conditions. To overcome these limitations, we utilize
tactile images to capture the local 3D information and propose a Touch2Shape
model, which leverages a touch-conditioned diffusion model to explore and
reconstruct the target shape from touch. For shape reconstruction, we have
developed a touch embedding module to condition the diffusion model in creating
a compact representation and a touch shape fusion module to refine the
reconstructed shape. For shape exploration, we combine the diffusion model with
reinforcement learning to train a policy. This involves using the generated
latent vector from the diffusion model to guide the touch exploration policy
training through a novel reward design. Experiments validate the reconstruction
quality thorough both qualitatively and quantitative analysis, and our touch
exploration policy further boosts reconstruction performance.

</details>

### [793] [Industry-focused Synthetic Segmentation Pre-training](https://arxiv.org/abs/2505.13099)
*Shinichi Mae,Ryosuke Yamada,Hirokatsu Kataoka*

Main category: cs.CV

TLDR: 提出InsCore合成数据集，解决工业应用中依赖真实图像和标注的问题，性能优于COCO、ImageNet-21k和微调SAM。


<details>
  <summary>Details</summary>
Motivation: 工业应用面临法律限制和领域差距问题，需无需真实图像或标注的解决方案。

Method: 基于公式驱动监督学习（FDSL）生成合成数据集InsCore，模拟工业数据特性。

Result: 在五个工业数据集上，InsCore预训练模型性能平均提升6.2点，数据效率高。

Conclusion: InsCore为工业应用提供高效、免授权的视觉基础模型。

Abstract: Pre-training on real-image datasets has been widely proven effective for
improving instance segmentation. However, industrial applications face two key
challenges: (1) legal and ethical restrictions, such as ImageNet's prohibition
of commercial use, and (2) limited transferability due to the domain gap
between web images and industrial imagery. Even recent vision foundation
models, including the segment anything model (SAM), show notable performance
degradation in industrial settings. These challenges raise critical questions:
Can we build a vision foundation model for industrial applications without
relying on real images or manual annotations? And can such models outperform
even fine-tuned SAM on industrial datasets? To address these questions, we
propose the Instance Core Segmentation Dataset (InsCore), a synthetic
pre-training dataset based on formula-driven supervised learning (FDSL).
InsCore generates fully annotated instance segmentation images that reflect key
characteristics of industrial data, including complex occlusions, dense
hierarchical masks, and diverse non-rigid shapes, distinct from typical web
imagery. Unlike previous methods, InsCore requires neither real images nor
human annotations. Experiments on five industrial datasets show that models
pre-trained with InsCore outperform those trained on COCO and ImageNet-21k, as
well as fine-tuned SAM, achieving an average improvement of 6.2 points in
instance segmentation performance. This result is achieved using only 100k
synthetic images, more than 100 times fewer than the 11 million images in SAM's
SA-1B dataset, demonstrating the data efficiency of our approach. These
findings position InsCore as a practical and license-free vision foundation
model for industrial applications.

</details>

### [794] [ARIW-Framework: Adaptive Robust Iterative Watermarking Framework](https://arxiv.org/abs/2505.13101)
*Shaowu Wu,Liting Zeng,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TLDR: 本文提出了一种自适应鲁棒迭代水印框架（ARIW-Framework），解决了深度学习水印技术在视觉质量、鲁棒性和泛化性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着大模型的快速发展，生成图像内容的版权保护成为关键安全挑战，现有水印技术在视觉质量、鲁棒性和泛化性方面存在不足。

Method: 采用迭代方法优化编码器，生成鲁棒残差；引入噪声层和解码器计算鲁棒性权重；利用图像梯度确定嵌入强度。

Result: 实验表明，该方法在视觉质量、鲁棒性和泛化性方面表现优异。

Conclusion: ARIW-Framework能够有效提升水印技术的性能，适用于版权保护。

Abstract: With the rapid rise of large models, copyright protection for generated image
content has become a critical security challenge. Although deep learning
watermarking techniques offer an effective solution for digital image copyright
protection, they still face limitations in terms of visual quality, robustness
and generalization. To address these issues, this paper proposes an adaptive
robust iterative watermarking framework (ARIW-Framework) that achieves
high-quality watermarked images while maintaining exceptional robustness and
generalization performance. Specifically, we introduce an iterative approach to
optimize the encoder for generating robust residuals. The encoder incorporates
noise layers and a decoder to compute robustness weights for residuals under
various noise attacks. By employing a parallel optimization strategy, the
framework enhances robustness against multiple types of noise attacks.
Furthermore, we leverage image gradients to determine the embedding strength at
each pixel location, significantly improving the visual quality of the
watermarked images. Extensive experiments demonstrate that the proposed method
achieves superior visual quality while exhibiting remarkable robustness and
generalization against noise attacks.

</details>

### [795] [Just Dance with $π$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection](https://arxiv.org/abs/2505.13123)
*Snehashis Majhi,Giacomo D'Amicantonio,Antitza Dantcheva,Quan Kong,Lorenzo Garattoni,Gianpiero Francesca,Egor Bondarev,Francois Bremond*

Main category: cs.CV

TLDR: PI-VAD是一种基于多模态增强的弱监督视频异常检测方法，通过引入五种额外模态（姿态、深度、全景掩码、光流和语言线索）来提升RGB特征的区分能力，无需在推理阶段使用多模态骨干网络。


<details>
  <summary>Details</summary>
Motivation: RGB特征在区分视觉相似事件（如商店盗窃）时表现不足，需要引入多模态信息以提高检测的鲁棒性。

Method: PI-VAD通过伪模态生成模块和跨模态诱导模块，将五种模态的信息融入RGB特征，仅需在训练阶段使用多模态骨干网络。

Result: 在三个真实场景的VAD数据集上达到了最先进的准确率。

Conclusion: PI-VAD通过多模态增强显著提升了视频异常检测的可靠性，且计算开销低。

Abstract: Weakly-supervised methods for video anomaly detection (VAD) are
conventionally based merely on RGB spatio-temporal features, which continues to
limit their reliability in real-world scenarios. This is due to the fact that
RGB-features are not sufficiently distinctive in setting apart categories such
as shoplifting from visually similar events. Therefore, towards robust complex
real-world VAD, it is essential to augment RGB spatio-temporal features by
additional modalities. Motivated by this, we introduce the Poly-modal Induced
framework for VAD: "PI-VAD", a novel approach that augments RGB representations
by five additional modalities. Specifically, the modalities include sensitivity
to fine-grained motion (Pose), three dimensional scene and entity
representation (Depth), surrounding objects (Panoptic masks), global motion
(optical flow), as well as language cues (VLM). Each modality represents an
axis of a polygon, streamlined to add salient cues to RGB. PI-VAD includes two
plug-in modules, namely Pseudo-modality Generation module and Cross Modal
Induction module, which generate modality-specific prototypical representation
and, thereby, induce multi-modal information into RGB cues. These modules
operate by performing anomaly-aware auxiliary tasks and necessitate five
modality backbones -- only during training. Notably, PI-VAD achieves
state-of-the-art accuracy on three prominent VAD datasets encompassing
real-world scenarios, without requiring the computational overhead of five
modality backbones at inference.

</details>

### [796] [Adaptive Image Restoration for Video Surveillance: A Real-Time Approach](https://arxiv.org/abs/2505.13130)
*Muhammad Awais Amin,Adama Ilboudo,Abdul Samad bin Shahid,Amjad Ali,Waqas Haider Khan Bangyal*

Main category: cs.CV

TLDR: 开发了一种基于ResNet_50的实时图像修复模型，用于视频监控，能自动识别图像退化类型并选择修复方法。


<details>
  <summary>Details</summary>
Motivation: 图像质量退化（如雨、雾、光照等）影响计算机视觉任务的自动化决策，现有修复方案不适用于实时处理。

Method: 使用迁移学习和ResNet_50构建模型，自动识别图像退化类型并选择修复方法。

Result: 模型具有灵活性和可扩展性，适用于实时视频监控。

Conclusion: 该研究为实时图像修复提供了一种高效解决方案。

Abstract: One of the major challenges in the field of computer vision especially for
detection, segmentation, recognition, monitoring, and automated solutions, is
the quality of images. Image degradation, often caused by factors such as rain,
fog, lighting, etc., has a negative impact on automated
decision-making.Furthermore, several image restoration solutions exist,
including restoration models for single degradation and restoration models for
multiple degradations. However, these solutions are not suitable for real-time
processing. In this study, the aim was to develop a real-time image restoration
solution for video surveillance. To achieve this, using transfer learning with
ResNet_50, we developed a model for automatically identifying the types of
degradation present in an image to reference the necessary treatment(s) for
image restoration. Our solution has the advantage of being flexible and
scalable.

</details>

### [797] [Learning to Adapt to Position Bias in Vision Transformer Classifiers](https://arxiv.org/abs/2505.13137)
*Robert-Jan Bruintjes,Jan van Gemert*

Main category: cs.CV

TLDR: 论文研究了位置信息对图像分类的影响，提出了一种衡量位置偏置的方法Position-SHAP，并根据数据集的位置偏置优化位置嵌入方式，最终提出了一种自适应位置嵌入方法Auto-PE。


<details>
  <summary>Details</summary>
Motivation: 研究位置信息在图像分类中的作用，探讨其在不同数据集中的重要性，并优化位置嵌入方法以提高分类性能。

Method: 提出Position-SHAP方法衡量位置偏置，并设计Auto-PE自适应位置嵌入方法，通过调整嵌入范数来适应不同数据集的需求。

Result: 实验表明不同数据集存在不同程度的位置偏置，Auto-PE能够结合现有位置嵌入方法提升分类准确率。

Conclusion: 位置偏置对视觉Transformer分类器性能至关重要，Auto-PE是一种有效的自适应位置嵌入解决方案。

Abstract: How discriminative position information is for image classification depends
on the data. On the one hand, the camera position is arbitrary and objects can
appear anywhere in the image, arguing for translation invariance. At the same
time, position information is key for exploiting capture/center bias, and scene
layout, e.g.: the sky is up. We show that position bias, the level to which a
dataset is more easily solved when positional information on input features is
used, plays a crucial role in the performance of Vision Transformers image
classifiers. To investigate, we propose Position-SHAP, a direct measure of
position bias by extending SHAP to work with position embeddings. We show
various levels of position bias in different datasets, and find that the
optimal choice of position embedding depends on the position bias apparent in
the dataset. We therefore propose Auto-PE, a single-parameter position
embedding extension, which allows the position embedding to modulate its norm,
enabling the unlearning of position information. Auto-PE combines with existing
PEs to match or improve accuracy on classification datasets.

</details>

### [798] [CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow](https://arxiv.org/abs/2505.13140)
*Takahiro Maeda,Jinkun Cao,Norimichi Ukita,Kris Kitani*

Main category: cs.CV

TLDR: CacheFlow是一种基于流的快速3D人体运动预测方法，通过预计算和缓存流生成模型的结果，显著提高了推理速度，同时保持预测精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D人体运动预测方法推理时间过长的问题，提出一种更高效的密度估计技术。

Method: 采用两阶段方法：1) 使用无条件流生成模型将高斯混合转换为未来运动的密度；2) 通过轻量级模型将历史轨迹映射到高斯混合样本。

Result: 推理时间缩短至约1毫秒，比VAE快4倍，比扩散方法快30倍，同时在Human3.6M和AMASS数据集上保持高精度。

Conclusion: CacheFlow在速度和精度上均优于现有方法，为实时3D人体运动预测提供了高效解决方案。

Abstract: Many density estimation techniques for 3D human motion prediction require a
significant amount of inference time, often exceeding the duration of the
predicted time horizon. To address the need for faster density estimation for
3D human motion prediction, we introduce a novel flow-based method for human
motion prediction called CacheFlow. Unlike previous conditional generative
models that suffer from time efficiency, CacheFlow takes advantage of an
unconditional flow-based generative model that transforms a Gaussian mixture
into the density of future motions. The results of the computation of the
flow-based generative model can be precomputed and cached. Then, for
conditional prediction, we seek a mapping from historical trajectories to
samples in the Gaussian mixture. This mapping can be done by a much more
lightweight model, thus saving significant computation overhead compared to a
typical conditional flow model. In such a two-stage fashion and by caching
results from the slow flow model computation, we build our CacheFlow without
loss of prediction accuracy and model expressiveness. This inference process is
completed in approximately one millisecond, making it 4 times faster than
previous VAE methods and 30 times faster than previous diffusion-based methods
on standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our
method demonstrates improved density estimation accuracy and comparable
prediction accuracy to a SOTA method on Human3.6M. Our code and models will be
publicly available.

</details>

### [799] [FlowCut: Unsupervised Video Instance Segmentation via Temporal Mask Matching](https://arxiv.org/abs/2505.13174)
*Alp Eren Sari,Paolo Favaro*

Main category: cs.CV

TLDR: FlowCut是一种用于无监督视频实例分割的三阶段框架方法，通过伪标签构建高质量视频数据集，并在多个基准测试中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 首次尝试通过伪标签构建视频数据集，以解决无监督视频实例分割的数据需求问题。

Method: 三阶段框架：1）利用图像和光流特征生成伪实例掩码；2）通过时间匹配构建高质量短视频段；3）从YouTubeVIS-2021提取训练集并训练模型。

Result: 在YouTubeVIS-2019、YouTubeVIS-2021、DAVIS-2017及其Motion基准测试中达到最先进性能。

Conclusion: FlowCut通过伪标签构建高质量数据集，为无监督视频实例分割提供了有效解决方案。

Abstract: We propose FlowCut, a simple and capable method for unsupervised video
instance segmentation consisting of a three-stage framework to construct a
high-quality video dataset with pseudo labels. To our knowledge, our work is
the first attempt to curate a video dataset with pseudo-labels for unsupervised
video instance segmentation. In the first stage, we generate pseudo-instance
masks by exploiting the affinities of features from both images and optical
flows. In the second stage, we construct short video segments containing
high-quality, consistent pseudo-instance masks by temporally matching them
across the frames. In the third stage, we use the YouTubeVIS-2021 video dataset
to extract our training instance segmentation set, and then train a video
segmentation model. FlowCut achieves state-of-the-art performance on the
YouTubeVIS-2019, YouTubeVIS-2021, DAVIS-2017, and DAVIS-2017 Motion benchmarks.

</details>

### [800] [Emergence of Fixational and Saccadic Movements in a Multi-Level Recurrent Attention Model for Vision](https://arxiv.org/abs/2505.13191)
*Pengcheng Pan,Yonekura Shogo,Yasuo Kuniyoshi*

Main category: cs.CV

TLDR: 提出了一种多级循环注意力模型（MRAM），通过模拟人类视觉系统的层次结构，解决了现有硬注意力模型在视觉探索动态上的不足，实现了更接近人类眼动的注意力行为。


<details>
  <summary>Details</summary>
Motivation: 现有硬注意力模型（如RAM和DRAM）未能模拟人类视觉系统的层次结构，导致注意力行为过于固定或过度扫视，与人类眼动行为不符。

Method: 提出MRAM框架，通过将瞥视位置生成和任务执行功能解耦到两个循环层中，模拟人类视觉处理的神经层次。

Result: MRAM不仅实现了更接近人类眼动的注意力动态，还在标准图像分类基准上优于CNN、RAM和DRAM。

Conclusion: MRAM通过层次化建模，显著提升了硬注意力模型的性能和人类相似性。

Abstract: Inspired by foveal vision, hard attention models promise interpretability and
parameter economy. However, existing models like the Recurrent Model of Visual
Attention (RAM) and Deep Recurrent Attention Model (DRAM) failed to model the
hierarchy of human vision system, that compromise on the visual exploration
dynamics. As a result, they tend to produce attention that are either overly
fixational or excessively saccadic, diverging from human eye movement behavior.
In this paper, we propose a Multi-Level Recurrent Attention Model (MRAM), a
novel hard attention framework that explicitly models the neural hierarchy of
human visual processing. By decoupling the function of glimpse location
generation and task execution in two recurrent layers, MRAM emergent a balanced
behavior between fixation and saccadic movement. Our results show that MRAM not
only achieves more human-like attention dynamics, but also consistently
outperforms CNN, RAM and DRAM baselines on standard image classification
benchmarks.

</details>

### [801] [MatPredict: a dataset and benchmark for learning material properties of diverse indoor objects](https://arxiv.org/abs/2505.13201)
*Yuzhen Chen,Hojun Son,Arpan Kusari*

Main category: cs.CV

TLDR: MatPredict数据集结合高质量合成对象与材料属性，用于从视觉图像推断材料属性，支持消费机器人应用。


<details>
  <summary>Details</summary>
Motivation: 通过相机图像确定材料属性可增强复杂物体的识别能力，对消费机器人应用有价值。

Method: 结合Replica和MatSynth数据集，生成18种常见物体和14种材料属性，提供光照和相机位置变化，并建立基准测试神经网络模型。

Result: 通过模拟光与材料的交互增强真实感，有效训练模型，提升消费机器人感知能力。

Conclusion: MatPredict数据集和代码公开，旨在推动消费机器人感知技术的革新。

Abstract: Determining material properties from camera images can expand the ability to
identify complex objects in indoor environments, which is valuable for consumer
robotics applications. To support this, we introduce MatPredict, a dataset that
combines the high-quality synthetic objects from Replica dataset with MatSynth
dataset's material properties classes - to create objects with diverse material
properties. We select 3D meshes of specific foreground objects and render them
with different material properties. In total, we generate \textbf{18} commonly
occurring objects with \textbf{14} different materials. We showcase how we
provide variability in terms of lighting and camera placement for these
objects. Next, we provide a benchmark for inferring material properties from
visual images using these perturbed models in the scene, discussing the
specific neural network models involved and their performance based on
different image comparison metrics. By accurately simulating light interactions
with different materials, we can enhance realism, which is crucial for training
models effectively through large-scale simulations. This research aims to
revolutionize perception in consumer robotics. The dataset is provided
\href{https://huggingface.co/datasets/UMTRI/MatPredict}{here} and the code is
provided \href{https://github.com/arpan-kusari/MatPredict}{here}.

</details>

### [802] [MAGI-1: Autoregressive Video Generation at Scale](https://arxiv.org/abs/2505.13211)
*Sand. ai,Hansi Teng,Hongyu Jia,Lei Sun,Lingzhi Li,Maolin Li,Mingqiu Tang,Shuai Han,Tianning Zhang,W. Q. Zhang,Weifeng Luo,Xiaoyang Kang,Yuchen Sun,Yue Cao,Yunpeng Huang,Yutong Lin,Yuxin Fang,Zewei Tao,Zheng Zhang,Zhongshu Wang,Zixun Liu,Dai Shi,Guoli Su,Hanwen Sun,Hong Pan,Jie Wang,Jiexin Sheng,Min Cui,Min Hu,Ming Yan,Shucheng Yin,Siran Zhang,Tingting Liu,Xianping Yin,Xiaoyu Yang,Xin Song,Xuan Hu,Yankai Zhang,Yuqiao Li*

Main category: cs.CV

TLDR: MAGI-1是一个通过自回归预测视频块序列生成视频的世界模型，支持流式生成，并在图像到视频任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决视频生成中的时间一致性和可扩展性问题，同时支持实时、高效的内存部署。

Method: 采用分块噪声去噪训练方法，支持分块提示和恒定推理成本。

Result: MAGI-1在图像到视频任务中表现出高时间一致性和可扩展性，最大模型参数达240亿。

Conclusion: MAGI-1通过算法创新和专用基础设施实现了高效、可控的视频生成。

Abstract: We present MAGI-1, a world model that generates videos by autoregressively
predicting a sequence of video chunks, defined as fixed-length segments of
consecutive frames. Trained to denoise per-chunk noise that increases
monotonically over time, MAGI-1 enables causal temporal modeling and naturally
supports streaming generation. It achieves strong performance on image-to-video
(I2V) tasks conditioned on text instructions, providing high temporal
consistency and scalability, which are made possible by several algorithmic
innovations and a dedicated infrastructure stack. MAGI-1 facilitates
controllable generation via chunk-wise prompting and supports real-time,
memory-efficient deployment by maintaining constant peak inference cost,
regardless of video length. The largest variant of MAGI-1 comprises 24 billion
parameters and supports context lengths of up to 4 million tokens,
demonstrating the scalability and robustness of our approach. The code and
models are available at https://github.com/SandAI-org/MAGI-1 and
https://github.com/SandAI-org/MagiAttention. The product can be accessed at
https://sand.ai.

</details>

### [803] [RB-SCD: A New Benchmark for Semantic Change Detection of Roads and Bridges in Traffic Scenes](https://arxiv.org/abs/2505.13212)
*Qingling Shu,Sibao Chen,Zhihui You,Wei Lu,Jin Tang,Bin Luo*

Main category: cs.CV

TLDR: 论文提出了RB-SCD数据集和MFDCD框架，用于高精度检测道路和桥梁的语义变化，通过频域多模态特征融合提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在交通场景中缺乏高质量标注数据，难以提取细粒度语义变化信息。

Method: 提出RB-SCD数据集和MFDCD框架，结合动态频率耦合器（DFC）和文本频率滤波器（TFF）进行频域特征融合。

Result: 在RB-SCD和三个公共基准测试中验证了方法的有效性。

Conclusion: RB-SCD和MFDCD为道路桥梁语义变化检测提供了新工具，性能优越。

Abstract: Accurate detection of changes in roads and bridges, such as construction,
renovation, and demolition, is essential for urban planning and traffic
management. However, existing methods often struggle to extract fine-grained
semantic change information due to the lack of high-quality annotated datasets
in traffic scenarios. To address this, we introduce the Road and Bridge
Semantic Change Detection (RB-SCD) dataset, a comprehensive benchmark
comprising 260 pairs of high-resolution remote sensing images from diverse
cities and countries. RB-SCD captures 11 types of semantic changes across
varied road and bridge structures, enabling detailed structural and functional
analysis. Building on this dataset, we propose a novel framework, Multimodal
Frequency-Driven Change Detector (MFDCD), which integrates multimodal features
in the frequency domain. MFDCD includes a Dynamic Frequency Coupler (DFC) that
fuses hierarchical visual features with wavelet-based frequency components, and
a Textual Frequency Filter (TFF) that transforms CLIP-derived textual features
into the frequency domain and applies graph-based filtering. Experimental
results on RB-SCD and three public benchmarks demonstrate the effectiveness of
our approach.

</details>

### [804] [Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation](https://arxiv.org/abs/2505.13215)
*Seungjun Oh,Younggeun Lee,Hyejin Jeon,Eunbyung Park*

Main category: cs.CV

TLDR: 提出了一种混合3D-4D高斯泼溅方法（3D-4DGS），通过自适应地将静态区域表示为3D高斯，动态区域保留为4D高斯，显著减少了计算和内存开销，同时保持了高质量的动态重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有4D高斯泼溅方法在静态区域冗余分配4D高斯，导致计算和内存开销大，且可能降低图像质量。

Method: 从完全的4D高斯表示开始，迭代地将时间不变的高斯转换为3D，动态高斯保留为4D。

Result: 显著减少了参数数量，提高了计算效率，同时保持了或提升了视觉质量。

Conclusion: 3D-4DGS在训练速度和视觉质量上优于基线4D高斯泼溅方法。

Abstract: Recent advancements in dynamic 3D scene reconstruction have shown promising
results, enabling high-fidelity 3D novel view synthesis with improved temporal
consistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an
appealing approach due to its ability to model high-fidelity spatial and
temporal variations. However, existing methods suffer from substantial
computational and memory overhead due to the redundant allocation of 4D
Gaussians to static regions, which can also degrade image quality. In this
work, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework
that adaptively represents static regions with 3D Gaussians while reserving 4D
Gaussians for dynamic elements. Our method begins with a fully 4D Gaussian
representation and iteratively converts temporally invariant Gaussians into 3D,
significantly reducing the number of parameters and improving computational
efficiency. Meanwhile, dynamic Gaussians retain their full 4D representation,
capturing complex motions with high fidelity. Our approach achieves
significantly faster training times compared to baseline 4D Gaussian Splatting
methods while maintaining or improving the visual quality.

</details>

### [805] [Swin DiT: Diffusion Transformer using Pseudo Shifted Windows](https://arxiv.org/abs/2505.13219)
*Jiafu Wu,Yabiao Wang,Jian Li,Jinlong Peng,Yun Cao,Chengjie Wang,Jiangning Zhang*

Main category: cs.CV

TLDR: Swin-DiT通过伪移位窗口注意力（PSWA）和渐进覆盖通道分配（PCCA）策略，显著降低了计算成本并提升了图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 传统DiT在处理高分辨率图像时计算成本高，且全局信息依赖性被过度假设，导致冗余计算。

Method: 提出PSWA实现局部-全局信息交互，PCCA策略捕捉高阶注意力相似性，构建Swin-DiT模型。

Result: Swin-DiT-L在FID指标上比DiT-XL/2提升54%，且计算成本更低。

Conclusion: Swin-DiT通过优化注意力机制和通道分配，显著提升了效率和性能。

Abstract: Diffusion Transformers (DiTs) achieve remarkable performance within the
domain of image generation through the incorporation of the transformer
architecture. Conventionally, DiTs are constructed by stacking serial isotropic
global information modeling transformers, which face significant computational
cost when processing high-resolution images. We empirically analyze that latent
space image generation does not exhibit a strong dependence on global
information as traditionally assumed. Most of the layers in the model
demonstrate redundancy in global computation. In addition, conventional
attention mechanisms exhibit low-frequency inertia issues. To address these
issues, we propose \textbf{P}seudo \textbf{S}hifted \textbf{W}indow
\textbf{A}ttention (PSWA), which fundamentally mitigates global model
redundancy. PSWA achieves intermediate global-local information interaction
through window attention, while employing a high-frequency bridging branch to
simulate shifted window operations, supplementing appropriate global and
high-frequency information. Furthermore, we propose the Progressive Coverage
Channel Allocation(PCCA) strategy that captures high-order attention similarity
without additional computational cost. Building upon all of them, we propose a
series of Pseudo \textbf{S}hifted \textbf{Win}dow DiTs (\textbf{Swin DiT}),
accompanied by extensive experiments demonstrating their superior performance.
For example, our proposed Swin-DiT-L achieves a 54%$\uparrow$ FID improvement
over DiT-XL/2 while requiring less computational.
https://github.com/wujiafu007/Swin-DiT

</details>

### [806] [Automatic Complementary Separation Pruning Toward Lightweight CNNs](https://arxiv.org/abs/2505.13225)
*David Levin,Gonen Singer*

Main category: cs.CV

TLDR: ACSP是一种全自动的卷积神经网络剪枝方法，结合结构化剪枝和基于激活的剪枝，高效移除冗余组件，同时保持网络性能。


<details>
  <summary>Details</summary>
Motivation: 设计一种无需人工干预的全自动剪枝方法，减少计算成本并保持网络性能。

Method: 通过构建图空间编码组件的分离能力，利用互补选择和聚类算法确定最优组件子集。

Result: 在多种架构和数据集上，ACSP在保持高精度的同时显著降低计算成本。

Conclusion: ACSP是一种高效、全自动的剪枝方法，适用于实际部署。

Abstract: In this paper, we present Automatic Complementary Separation Pruning (ACSP),
a novel and fully automated pruning method for convolutional neural networks.
ACSP integrates the strengths of both structured pruning and activation-based
pruning, enabling the efficient removal of entire components such as neurons
and channels while leveraging activations to identify and retain the most
relevant components. Our approach is designed specifically for supervised
learning tasks, where we construct a graph space that encodes the separation
capabilities of each component with respect to all class pairs. By employing
complementary selection principles and utilizing a clustering algorithm, ACSP
ensures that the selected components maintain diverse and complementary
separation capabilities, reducing redundancy and maintaining high network
performance. The method automatically determines the optimal subset of
components in each layer, utilizing a knee-finding algorithm to select the
minimal subset that preserves performance without requiring user-defined
pruning volumes. Extensive experiments on multiple architectures, including
VGG-16, ResNet-50, and MobileNet-V2, across datasets like CIFAR-10, CIFAR-100,
and ImageNet-1K, demonstrate that ACSP achieves competitive accuracy compared
to other methods while significantly reducing computational costs. This fully
automated approach not only enhances scalability but also makes ACSP especially
practical for real-world deployment by eliminating the need for manually
defining the pruning volume.

</details>

### [807] [From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection](https://arxiv.org/abs/2505.13233)
*Lincan Cai,Jingxuan Kang,Shuang Li,Wenxuan Ma,Binhui Xie,Zhida Qin,Jian Liang*

Main category: cs.CV

TLDR: 论文提出了一种基于注意力的选择方法（ABS），通过注意力引导的裁剪和特征选择，解决了视觉增强技术引入的背景伪影和局部细节过度关注问题，提升了零样本分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉增强技术（如随机裁剪）在提升零样本性能时，会引入背景伪影并导致模型过度关注局部细节，损害全局语义理解。

Method: 提出ABS方法，通过注意力引导的裁剪和特征选择，补充全局语义信息，并引入软匹配技术优化LLM描述的匹配。

Result: ABS在分布外泛化和零样本分类任务中达到最先进性能，且无需训练，甚至媲美少样本和测试时适应方法。

Conclusion: ABS是一种高效且无需训练的方法，显著提升了视觉语言模型在零样本任务中的性能。

Abstract: Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive
zero-shot capabilities on downstream tasks. Prior research highlights the
crucial role of visual augmentation techniques, like random cropping, in
alignment with fine-grained class descriptions generated by large language
models (LLMs), significantly enhancing zero-shot performance by incorporating
multi-view information. However, the inherent randomness of these augmentations
can inevitably introduce background artifacts and cause models to overly focus
on local details, compromising global semantic understanding. To address these
issues, we propose an \textbf{A}ttention-\textbf{B}ased \textbf{S}election
(\textbf{ABS}) method from local details to global context, which applies
attention-guided cropping in both raw images and feature space, supplement
global semantic information through strategic feature selection. Additionally,
we introduce a soft matching technique to effectively filter LLM descriptions
for better alignment. \textbf{ABS} achieves state-of-the-art performance on
out-of-distribution generalization and zero-shot classification tasks. Notably,
\textbf{ABS} is training-free and even rivals few-shot and test-time adaptation
methods. Our code is available at
\href{https://github.com/BIT-DA/ABS}{\textcolor{darkgreen}{https://github.com/BIT-DA/ABS}}.

</details>

### [808] [WriteViT: Handwritten Text Generation with Vision Transformer](https://arxiv.org/abs/2505.13235)
*Dang Hoai Nam,Huynh Tong Dang Khoa,Vo Nguyen Le Duy*

Main category: cs.CV

TLDR: WriteViT是一个基于Vision Transformers的单样本手写文本合成框架，能够高效提取风格嵌入并生成高质量手写文本，尤其在低资源场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 人类能够快速从单一样本中泛化手写风格，而机器在低数据环境下难以捕捉细微的空间和风格线索。WriteViT旨在填补这一差距。

Method: WriteViT结合了ViT-based Writer Identifier提取风格嵌入、多尺度生成器和轻量级ViT-based识别器，利用Transformer捕捉笔画细节和风格信息。

Result: 在越南语和英语数据集上的实验表明，WriteViT能生成高质量、风格一致的手写文本，并在低资源场景下保持强识别性能。

Conclusion: 基于Transformer的设计在多语言手写生成和高效风格适应方面具有潜力。

Abstract: Humans can quickly generalize handwriting styles from a single example by
intuitively separating content from style. Machines, however, struggle with
this task, especially in low-data settings, often missing subtle spatial and
stylistic cues. Motivated by this gap, we introduce WriteViT, a one-shot
handwritten text synthesis framework that incorporates Vision Transformers
(ViT), a family of models that have shown strong performance across various
computer vision tasks. WriteViT integrates a ViT-based Writer Identifier for
extracting style embeddings, a multi-scale generator built with Transformer
encoder-decoder blocks enhanced by conditional positional encoding (CPE), and a
lightweight ViT-based recognizer. While previous methods typically rely on CNNs
or CRNNs, our design leverages transformers in key components to better capture
both fine-grained stroke details and higher-level style information. Although
handwritten text synthesis has been widely explored, its application to
Vietnamese -- a language rich in diacritics and complex typography -- remains
limited. Experiments on Vietnamese and English datasets demonstrate that
WriteViT produces high-quality, style-consistent handwriting while maintaining
strong recognition performance in low-resource scenarios. These results
highlight the promise of transformer-based designs for multilingual handwriting
generation and efficient style adaptation.

</details>

### [809] [Joint Depth and Reflectivity Estimation using Single-Photon LiDAR](https://arxiv.org/abs/2505.13250)
*Hashan K. Weerasooriya,Prateek Chennuri,Weijian Zhang,Istvan Gyongy,Stanley H. Chan*

Main category: cs.CV

TLDR: 提出了一种名为SPLiDER的新方法，用于在动态场景中同时恢复深度和反射率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有SP-LiDAR方法通常单独或顺序恢复深度和反射率，且在动态场景中效率不足。

Method: 通过理论分析深度与反射率的互相关性，提出联合估计方法SPLiDER，利用共享信息增强信号恢复。

Result: 在合成和真实SP-LiDAR数据上，SPLiDER表现优于现有方法，实现了更优的联合重建质量。

Conclusion: SPLiDER方法在动态场景中有效提升了深度和反射率的联合估计性能。

Abstract: Single-Photon Light Detection and Ranging (SP-LiDAR is emerging as a leading
technology for long-range, high-precision 3D vision tasks. In SP-LiDAR,
timestamps encode two complementary pieces of information: pulse travel time
(depth) and the number of photons reflected by the object (reflectivity).
Existing SP-LiDAR reconstruction methods typically recover depth and
reflectivity separately or sequentially use one modality to estimate the other.
Moreover, the conventional 3D histogram construction is effective mainly for
slow-moving or stationary scenes. In dynamic scenes, however, it is more
efficient and effective to directly process the timestamps. In this paper, we
introduce an estimation method to simultaneously recover both depth and
reflectivity in fast-moving scenes. We offer two contributions: (1) A
theoretical analysis demonstrating the mutual correlation between depth and
reflectivity and the conditions under which joint estimation becomes
beneficial. (2) A novel reconstruction method, "SPLiDER", which exploits the
shared information to enhance signal recovery. On both synthetic and real
SP-LiDAR data, our method outperforms existing approaches, achieving superior
joint reconstruction quality.

</details>

### [810] [Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning](https://arxiv.org/abs/2505.13261)
*Mingrui Chen,Haogeng Liu,Hao Liang,Huaibo Huang,Wentao Zhang,Ran He*

Main category: cs.CV

TLDR: 论文研究了如何通过显式建模问题难度先验信息提升多模态推理中基于强化学习的微调效果，提出了离线数据筛选、在线优势差异化和难度提示三种方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 探索问题难度先验信息对多模态推理中强化学习微调效果的影响，以优化训练效率和模型性能。

Method: 1. 离线数据筛选：通过多轮采样分析数据集难度分布，过滤过于简单或困难的样本；2. 在线优势差异化：基于组别经验准确率动态调整优势估计权重；3. 难度提示：在第二阶段训练中为复杂样本添加显式提示。

Result: 在仅使用2K+0.6K两阶段训练数据的情况下，方法在多模态数学推理基准测试中表现显著提升。

Conclusion: 显式建模问题难度信息能有效提升强化学习微调在多模态推理中的性能，且数据效率高。

Abstract: In this work, we investigate how explicitly modeling problem's difficulty
prior information shapes the effectiveness of reinforcement learning based
fine-tuning for multimodal reasoning. Our exploration mainly comprises of
following three perspective: First, through offline data curation, we analyze
the U-shaped difficulty distribution of two given datasets using the base model
by multi-round sampling, and then filter out prompts that are either too simple
or extremely difficult to provide meaningful gradients and perform subsequent
two-stage training. Second, we implement an online advantage differentiation,
computing group-wise empirical accuracy as a difficulty proxy to adaptively
reweight advantages estimation, providing stronger learning signals for more
challenging problems. Finally, we introduce difficulty hints as explicit
prompts for more complex samples in the second training stage, encouraging the
model to calibrate its reasoning depth and perform reflective validation
checks. Our comprehensive approach demonstrates significant performances across
various multi-modal mathematical reasoning benchmarks with only 2K+0.6K
two-stage training data.

</details>

### [811] [DB3D-L: Depth-aware BEV Feature Transformation for Accurate 3D Lane Detection](https://arxiv.org/abs/2505.13266)
*Yehao Liu,Xiaosu Xu,Zijian Wang,Yiqing Yao*

Main category: cs.CV

TLDR: 提出了一种基于深度感知BEV特征转换的3D车道检测方法，通过集成深度网络简化视图变换，并在合成和真实数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决从前视图图像构建准确BEV信息时因缺乏深度信息而依赖平坦地面假设的问题，以及现有方法难以有效整合单目深度估计与车道检测任务的局限性。

Method: 设计了一个特征提取模块（集成深度网络）、特征降维模块和融合模块，以有效融合前视图特征和深度信息，构建BEV特征。

Result: 在合成数据集Apollo和真实数据集OpenLane上表现与最先进方法相当。

Conclusion: 提出的方法通过深度感知BEV特征转换，有效解决了3D车道检测中的视图变换问题，提升了检测准确性。

Abstract: 3D Lane detection plays an important role in autonomous driving. Recent
advances primarily build Birds-Eye-View (BEV) feature from front-view (FV)
images to perceive 3D information of Lane more effectively. However,
constructing accurate BEV information from FV image is limited due to the
lacking of depth information, causing previous works often rely heavily on the
assumption of a flat ground plane. Leveraging monocular depth estimation to
assist in constructing BEV features is less constrained, but existing methods
struggle to effectively integrate the two tasks. To address the above issue, in
this paper, an accurate 3D lane detection method based on depth-aware BEV
feature transtormation is proposed. In detail, an effective feature extraction
module is designed, in which a Depth Net is integrated to obtain the vital
depth information for 3D perception, thereby simplifying the complexity of view
transformation. Subquently a feature reduce module is proposed to reduce height
dimension of FV features and depth features, thereby enables effective fusion
of crucial FV features and depth features. Then a fusion module is designed to
build BEV feature from prime FV feature and depth information. The proposed
method performs comparably with state-of-the-art methods on both synthetic
Apollo, realistic OpenLane datasets.

</details>

### [812] [Event-Driven Dynamic Scene Depth Completion](https://arxiv.org/abs/2505.13279)
*Zhiqiang Yan,Jianhao Jiao,Zhengxue Wang,Gim Hee Lee*

Main category: cs.CV

TLDR: EventDC是一种基于事件相机的深度补全框架，通过事件调制对齐和局部深度滤波模块，在动态场景中实现更准确的深度补全。


<details>
  <summary>Details</summary>
Motivation: 动态场景中，传统RGB-D传感器因快速运动和物体运动导致深度数据质量下降，而事件相机的高时间分辨率能提供互补信息。

Method: 提出EventDC框架，包含事件调制对齐（EMA）和局部深度滤波（LDF）模块，利用事件流自适应调整卷积操作的偏移和权重。

Result: 实验证明EventDC在动态场景中表现优越，并建立了首个事件相机深度补全基准数据集。

Conclusion: EventDC通过事件相机的高动态特性，显著提升了动态场景中的深度补全性能。

Abstract: Depth completion in dynamic scenes poses significant challenges due to rapid
ego-motion and object motion, which can severely degrade the quality of input
modalities such as RGB images and LiDAR measurements. Conventional RGB-D
sensors often struggle to align precisely and capture reliable depth under such
conditions. In contrast, event cameras with their high temporal resolution and
sensitivity to motion at the pixel level provide complementary cues that are
%particularly beneficial in dynamic environments.To this end, we propose
EventDC, the first event-driven depth completion framework. It consists of two
key components: Event-Modulated Alignment (EMA) and Local Depth Filtering
(LDF). Both modules adaptively learn the two fundamental components of
convolution operations: offsets and weights conditioned on motion-sensitive
event streams. In the encoder, EMA leverages events to modulate the sampling
positions of RGB-D features to achieve pixel redistribution for improved
alignment and fusion. In the decoder, LDF refines depth estimations around
moving objects by learning motion-aware masks from events. Additionally,
EventDC incorporates two loss terms to further benefit global alignment and
enhance local depth recovery. Moreover, we establish the first benchmark for
event-based depth completion comprising one real-world and two synthetic
datasets to facilitate future research. Extensive experiments on this benchmark
demonstrate the superiority of our EventDC.

</details>

### [813] [Computer Vision Models Show Human-Like Sensitivity to Geometric and Topological Concepts](https://arxiv.org/abs/2505.13281)
*Zekun Wang,Sashank Varma*

Main category: cs.CV

TLDR: 论文探讨计算机视觉模型与人类对几何和拓扑（GT）概念敏感性的对齐问题，发现Transformer模型表现最佳且与儿童表现一致，而视觉语言模型表现较差。


<details>
  <summary>Details</summary>
Motivation: 研究GT概念是否通过日常互动学习而非天生，并评估计算机视觉模型在解释人类GT敏感性方面的适用性。

Method: 使用三类模型（CNN、Transformer、视觉语言模型）在包含43个GT概念的odd-one-out任务中测试性能和对齐度。

Result: Transformer模型表现最优，超越儿童水平且与儿童表现一致；视觉语言模型表现较差且偏离人类模式。

Conclusion: 支持计算机视觉模型用于验证GT概念的学习解释，但多模态整合可能损害几何敏感性。

Abstract: With the rapid improvement of machine learning (ML) models, cognitive
scientists are increasingly asking about their alignment with how humans think.
Here, we ask this question for computer vision models and human sensitivity to
geometric and topological (GT) concepts. Under the core knowledge account,
these concepts are innate and supported by dedicated neural circuitry. In this
work, we investigate an alternative explanation, that GT concepts are learned
``for free'' through everyday interaction with the environment. We do so using
computer visions models, which are trained on large image datasets. We build on
prior studies to investigate the overall performance and human alignment of
three classes of models -- convolutional neural networks (CNNs),
transformer-based models, and vision-language models -- on an odd-one-out task
testing 43 GT concepts spanning seven classes. Transformer-based models achieve
the highest overall accuracy, surpassing that of young children. They also show
strong alignment with children's performance, finding the same classes of
concepts easy vs. difficult. By contrast, vision-language models underperform
their vision-only counterparts and deviate further from human profiles,
indicating that na\"ive multimodality might compromise abstract geometric
sensitivity. These findings support the use of computer vision models to
evaluate the sufficiency of the learning account for explaining human
sensitivity to GT concepts, while also suggesting that integrating linguistic
and visual representations might have unpredicted deleterious consequences.

</details>

### [814] [DD-Ranking: Rethinking the Evaluation of Dataset Distillation](https://arxiv.org/abs/2505.13300)
*Zekai Li,Xinhao Zhong,Samir Khaki,Zhiyuan Liang,Yuhao Zhou,Mingjia Shi,Ziqiao Wang,Xuanlei Zhao,Wangbo Zhao,Ziheng Qin,Mengxuan Wu,Pengfei Zhou,Haonan Wang,David Junhao Zhang,Jia-Wei Liu,Shaobo Wang,Dai Liu,Linfeng Zhang,Guang Li,Kun Wang,Zheng Zhu,Zhiheng Ma,Joey Tianyi Zhou,Jiancheng Lv,Yaochu Jin,Peihao Wang,Kaipeng Zhang,Lingjuan Lyu,Yiran Huang,Zeynep Akata,Zhiwei Deng,Xindi Wu,George Cazenavette,Yuzhang Shang,Justin Cui,Jindong Gu,Qian Zheng,Hao Ye,Shuo Wang,Xiaobo Wang,Yan Yan,Angela Yao,Mike Zheng Shou,Tianlong Chen,Hakan Bilen,Baharan Mirzasoleiman,Manolis Kellis,Konstantinos N. Plataniotis,Zhangyang Wang,Bo Zhao,Yang You,Kai Wang*

Main category: cs.CV

TLDR: 论文探讨了数据集蒸馏领域中的评估问题，提出了DD-Ranking框架以更公平地评估方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法的性能提升可能源于额外技术而非图像本身质量，导致评估不公，阻碍领域发展。

Method: 提出DD-Ranking框架及新评估指标，关注数据集的实际信息增强。

Result: DD-Ranking提供了更全面、公平的评估标准，揭示了不同方法的真实性能提升。

Conclusion: DD-Ranking为未来研究提供了更可靠的评估基准，推动数据集蒸馏领域的健康发展。

Abstract: In recent years, dataset distillation has provided a reliable solution for
data compression, where models trained on the resulting smaller synthetic
datasets achieve performance comparable to those trained on the original
datasets. To further improve the performance of synthetic datasets, various
training pipelines and optimization objectives have been proposed, greatly
advancing the field of dataset distillation. Recent decoupled dataset
distillation methods introduce soft labels and stronger data augmentation
during the post-evaluation phase and scale dataset distillation up to larger
datasets (e.g., ImageNet-1K). However, this raises a question: Is accuracy
still a reliable metric to fairly evaluate dataset distillation methods? Our
empirical findings suggest that the performance improvements of these methods
often stem from additional techniques rather than the inherent quality of the
images themselves, with even randomly sampled images achieving superior
results. Such misaligned evaluation settings severely hinder the development of
DD. Therefore, we propose DD-Ranking, a unified evaluation framework, along
with new general evaluation metrics to uncover the true performance
improvements achieved by different methods. By refocusing on the actual
information enhancement of distilled datasets, DD-Ranking provides a more
comprehensive and fair evaluation standard for future research advancements.

</details>

### [815] [GMM-Based Comprehensive Feature Extraction and Relative Distance Preservation For Few-Shot Cross-Modal Retrieval](https://arxiv.org/abs/2505.13306)
*Chengsong Sun,Weiping Li,Xiang Li,Yuankun Liu,Lianlei Shan*

Main category: cs.CV

TLDR: 论文提出了一种名为GCRDP的新方法，用于解决少样本跨模态检索中的多峰分布问题，通过高斯混合模型和多正样本对比学习机制提升检索准确性。


<details>
  <summary>Details</summary>
Motivation: 传统跨模态检索方法假设训练和测试数据共享相同的类别分布，而少样本检索涉及稀疏表示的数据，导致潜在语义空间中的模态内和模态间偏差，影响检索准确性。

Method: 提出GCRDP方法，利用高斯混合模型捕捉数据的多峰分布，结合多正样本对比学习机制和新的跨模态语义对齐策略。

Result: 在四个基准数据集上的实验表明，GCRDP优于六种现有方法。

Conclusion: GCRDP通过有效建模多峰分布和优化跨模态对齐，显著提升了少样本跨模态检索的性能。

Abstract: Few-shot cross-modal retrieval focuses on learning cross-modal
representations with limited training samples, enabling the model to handle
unseen classes during inference. Unlike traditional cross-modal retrieval
tasks, which assume that both training and testing data share the same class
distribution, few-shot retrieval involves data with sparse representations
across modalities. Existing methods often fail to adequately model the
multi-peak distribution of few-shot cross-modal data, resulting in two main
biases in the latent semantic space: intra-modal bias, where sparse samples
fail to capture intra-class diversity, and inter-modal bias, where
misalignments between image and text distributions exacerbate the semantic gap.
These biases hinder retrieval accuracy. To address these issues, we propose a
novel method, GCRDP, for few-shot cross-modal retrieval. This approach
effectively captures the complex multi-peak distribution of data using a
Gaussian Mixture Model (GMM) and incorporates a multi-positive sample
contrastive learning mechanism for comprehensive feature modeling.
Additionally, we introduce a new strategy for cross-modal semantic alignment,
which constrains the relative distances between image and text feature
distributions, thereby improving the accuracy of cross-modal representations.
We validate our approach through extensive experiments on four benchmark
datasets, demonstrating superior performance over six state-of-the-art methods.

</details>

### [816] [eStonefish-scenes: A synthetically generated dataset for underwater event-based optical flow prediction tasks](https://arxiv.org/abs/2505.13309)
*Jad Mansour,Sebastian Realpe,Hayat Rajani,Michele Grimaldi,Rafael Garcia,Nuno Gracias*

Main category: cs.CV

TLDR: 论文提出了一种合成事件光流数据集eStonefish-scenes，填补了水下应用中事件视觉数据的空白，并提供了一个数据处理库eWiz。


<details>
  <summary>Details</summary>
Motivation: 现有事件光流数据集多样性不足且难以收集，水下应用尤其缺乏标注数据，阻碍了事件视觉与自主水下机器人的结合。

Method: 基于Stonefish模拟器创建合成数据集eStonefish-scenes，提供数据生成管道和场景生成器，并开发了数据处理库eWiz。

Result: 生成了可定制的水下环境数据集，支持动态场景模拟和真实珊瑚礁生成，同时提供了数据处理工具。

Conclusion: 合成数据集和工具库为事件视觉在水下机器人中的应用提供了可扩展的解决方案。

Abstract: The combined use of event-based vision and Spiking Neural Networks (SNNs) is
expected to significantly impact robotics, particularly in tasks like visual
odometry and obstacle avoidance. While existing real-world event-based datasets
for optical flow prediction, typically captured with Unmanned Aerial Vehicles
(UAVs), offer valuable insights, they are limited in diversity, scalability,
and are challenging to collect. Moreover, there is a notable lack of labelled
datasets for underwater applications, which hinders the integration of
event-based vision with Autonomous Underwater Vehicles (AUVs). To address this,
synthetic datasets could provide a scalable solution while bridging the gap
between simulation and reality. In this work, we introduce eStonefish-scenes, a
synthetic event-based optical flow dataset based on the Stonefish simulator.
Along with the dataset, we present a data generation pipeline that enables the
creation of customizable underwater environments. This pipeline allows for
simulating dynamic scenarios, such as biologically inspired schools of fish
exhibiting realistic motion patterns, including obstacle avoidance and reactive
navigation around corals. Additionally, we introduce a scene generator that can
build realistic reef seabeds by randomly distributing coral across the terrain.
To streamline data accessibility, we present eWiz, a comprehensive library
designed for processing event-based data, offering tools for data loading,
augmentation, visualization, encoding, and training data generation, along with
loss functions and performance metrics.

</details>

### [817] [Denoising Diffusion Probabilistic Model for Point Cloud Compression at Low Bit-Rates](https://arxiv.org/abs/2505.13316)
*Gabriele Spadaro,Alberto Presta,Jhony H. Giraldo,Marco Grangetto,Wei Hu,Giuseppe Valenzise,Attilio Fiandrotti,Enzo Tartaglione*

Main category: cs.CV

TLDR: 提出了一种基于DDPM的低比特率点云压缩方法（DDPM-PCC），通过PointNet编码器和可学习量化器实现低比特率下的高质量压缩。


<details>
  <summary>Details</summary>
Motivation: 现有技术主要关注高保真重建，需要较多比特，而低比特率点云压缩在带宽受限应用中至关重要。

Method: 采用DDPM架构，结合PointNet编码器生成条件向量，并通过可学习量化器进行量化。

Result: 在ShapeNet和ModelNet40上的实验表明，该方法在低比特率下优于标准化和最新技术。

Conclusion: DDPM-PCC在低比特率下实现了更好的率失真性能，代码已公开。

Abstract: Efficient compression of low-bit-rate point clouds is critical for
bandwidth-constrained applications. However, existing techniques mainly focus
on high-fidelity reconstruction, requiring many bits for compression. This
paper proposes a "Denoising Diffusion Probabilistic Model" (DDPM) architecture
for point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoder
produces the condition vector for the generation, which is then quantized via a
learnable vector quantizer. This configuration allows to achieve a low bitrates
while preserving quality. Experiments on ShapeNet and ModelNet40 show improved
rate-distortion at low rates compared to standardized and state-of-the-art
approaches. We publicly released the code at
https://github.com/EIDOSLAB/DDPM-PCC.

</details>

### [818] [VesselGPT: Autoregressive Modeling of Vascular Geometry](https://arxiv.org/abs/2505.13318)
*Paula Feldman,Martin Sinnona,Viviana Siless,Claudio Delrieux,Emmanuel Iarussi*

Main category: cs.CV

TLDR: 提出了一种基于自回归方法的解剖树合成技术，利用VQ-VAE和GPT-2模型，实现了高保真度的血管树重建。


<details>
  <summary>Details</summary>
Motivation: 解剖树的复杂几何结构使其准确表示成为挑战，受大语言模型启发，探索自回归方法。

Method: 使用VQ-VAE嵌入血管结构到离散词汇表，再用GPT-2自回归建模生成。

Result: 方法能捕捉复杂几何和分支模式，实现高保真重建，B样条表示保留形态细节。

Conclusion: 首次以自回归方式生成血管，代码和数据将公开。

Abstract: Anatomical trees are critical for clinical diagnosis and treatment planning,
yet their complex and diverse geometry make accurate representation a
significant challenge. Motivated by the latest advances in large language
models, we introduce an autoregressive method for synthesizing anatomical
trees. Our approach first embeds vessel structures into a learned discrete
vocabulary using a VQ-VAE architecture, then models their generation
autoregressively with a GPT-2 model. This method effectively captures intricate
geometries and branching patterns, enabling realistic vascular tree synthesis.
Comprehensive qualitative and quantitative evaluations reveal that our
technique achieves high-fidelity tree reconstruction with compact discrete
representations. Moreover, our B-spline representation of vessel cross-sections
preserves critical morphological details that are often overlooked in previous'
methods parameterizations. To the best of our knowledge, this work is the first
to generate blood vessels in an autoregressive manner. Code, data, and trained
models will be made available.

</details>

### [819] [Benchmarking Unified Face Attack Detection via Hierarchical Prompt Tuning](https://arxiv.org/abs/2505.13327)
*Ajian Liu,Haocheng Yuan,Xiao Guo,Hui Ma,Wanyi Zhuang,Changtao Miao,Yan Hong,Chuanbiao Song,Jun Lan,Qi Chu,Tao Gong,Yanyan Liang,Weiqiang Wang,Jun Wan,Xiaoming Liu,Zhen Lei*

Main category: cs.CV

TLDR: 论文提出了一种统一的人脸攻击检测模型，通过UniAttackData+数据集和HiPTune框架解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸攻击检测模型因单独训练而难以应对未知攻击，且缺乏统一的数据集和分类标准。

Method: 提出UniAttackData+数据集和HiPTune框架，后者通过视觉提示树和多语义空间分类标准自适应选择特征。

Result: 实验在12个数据集上验证了方法的有效性，展示了在统一人脸攻击检测领域的潜力。

Conclusion: UniAttackData+和HiPTune为解决统一人脸攻击检测问题提供了新思路和工具。

Abstract: Presentation Attack Detection and Face Forgery Detection are designed to
protect face data from physical media-based Presentation Attacks and digital
editing-based DeepFakes respectively. But separate training of these two models
makes them vulnerable to unknown attacks and burdens deployment environments.
The lack of a Unified Face Attack Detection model to handle both types of
attacks is mainly due to two factors. First, there's a lack of adequate
benchmarks for models to explore. Existing UAD datasets have limited attack
types and samples, restricting the model's ability to address advanced threats.
To address this, we propose UniAttackDataPlus (UniAttackData+), the most
extensive and sophisticated collection of forgery techniques to date. It
includes 2,875 identities and their 54 kinds of falsified samples, totaling
697,347 videos. Second, there's a lack of a reliable classification criterion.
Current methods try to find an arbitrary criterion within the same semantic
space, which fails when encountering diverse attacks. So, we present a novel
Visual-Language Model-based Hierarchical Prompt Tuning Framework (HiPTune) that
adaptively explores multiple classification criteria from different semantic
spaces. We build a Visual Prompt Tree to explore various classification rules
hierarchically. Then, by adaptively pruning the prompts, the model can select
the most suitable prompts to guide the encoder to extract discriminative
features at different levels in a coarse-to-fine way. Finally, to help the
model understand the classification criteria in visual space, we propose a
Dynamically Prompt Integration module to project the visual prompts to the text
encoder for more accurate semantics. Experiments on 12 datasets have shown the
potential to inspire further innovations in the UAD field.

</details>

### [820] [RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers](https://arxiv.org/abs/2505.13344)
*Ahmet Berke Gokmen,Yigit Ekin,Bahri Batuhan Bilecen,Aysegul Dundar*

Main category: cs.CV

TLDR: RoPECraft是一种无需训练的视频运动迁移方法，通过修改扩散变换器的旋转位置嵌入（RoPE）实现，利用光流编码运动，并通过流匹配目标优化嵌入。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要训练或复杂调整，RoPECraft旨在通过简单修改RoPE实现高效运动迁移。

Method: 提取参考视频的光流，用运动偏移扭曲RoPE的复数张量，通过流匹配优化嵌入，并引入相位正则化避免重复生成。

Result: 实验表明RoPECraft在质量和数量上均优于最新方法。

Conclusion: RoPECraft提供了一种高效、无需训练的运动迁移解决方案，性能优越。

Abstract: We propose RoPECraft, a training-free video motion transfer method for
diffusion transformers that operates solely by modifying their rotary
positional embeddings (RoPE). We first extract dense optical flow from a
reference video, and utilize the resulting motion offsets to warp the
complex-exponential tensors of RoPE, effectively encoding motion into the
generation process. These embeddings are then further optimized during
denoising time steps via trajectory alignment between the predicted and target
velocities using a flow-matching objective. To keep the output faithful to the
text prompt and prevent duplicate generations, we incorporate a regularization
term based on the phase components of the reference video's Fourier transform,
projecting the phase angles onto a smooth manifold to suppress high-frequency
artifacts. Experiments on benchmarks reveal that RoPECraft outperforms all
recently published methods, both qualitatively and quantitatively.

</details>

### [821] [Faster Video Diffusion with Trainable Sparse Attention](https://arxiv.org/abs/2505.13389)
*Peiyuan Zhang,Haofeng Huang,Yongqi Chen,Will Lin,Zhengzhong Liu,Ion Stoica,Eric P. Xing,Hao Zhang*

Main category: cs.CV

TLDR: VSA是一种可训练的稀疏注意力机制，通过分阶段处理显著减少计算量，提升视频扩散模型的效率。


<details>
  <summary>Details</summary>
Motivation: 视频扩散变换器（DiTs）的3D注意力机制计算复杂度高，限制了其扩展性，而实际注意力主要集中在少数关键位置。

Method: VSA采用两阶段稀疏注意力：粗阶段聚合token为块并识别关键token，细阶段仅在关键块内计算token级注意力。

Result: VSA将训练FLOPS减少2.53倍，注意力时间加速6倍，生成时间从31秒降至18秒，且保持模型质量。

Conclusion: VSA证明可训练稀疏注意力是替代全注意力的实用方案，为视频扩散模型的进一步扩展提供了可能。

Abstract: Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D
attention, even though most of the attention mass concentrates on a small
subset of positions. We turn this observation into VSA, a trainable,
hardware-efficient sparse attention that replaces full attention at \emph{both}
training and inference. In VSA, a lightweight coarse stage pools tokens into
tiles and identifies high-weight \emph{critical tokens}; a fine stage computes
token-level attention only inside those tiles subjecting to block computing
layout to ensure hard efficiency. This leads to a single differentiable kernel
that trains end-to-end, requires no post-hoc profiling, and sustains 85\% of
FlashAttention3 MFU. We perform a large sweep of ablation studies and
scaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA
reaches a Pareto point that cuts training FLOPS by 2.53$\times$ with no drop in
diffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention
time by 6$\times$ and lowers end-to-end generation time from 31s to 18s with
comparable quality. These results establish trainable sparse attention as a
practical alternative to full attention and a key enabler for further scaling
of video diffusion models.

</details>

### [822] [FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language Models with Emotional Synergy and Reasoning](https://arxiv.org/abs/2505.13419)
*Zhuozhao Hu,Kaishen Yuan,Xin Liu,Zitong Yu,Yuan Zong,Jingang Shi,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

TLDR: 论文提出了一种新的FEA指令数据集和MLLM架构FEALLM，解决了传统方法在面部情绪分析中的局限性，并在多个数据集上展示了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 传统面部情绪分析方法在可解释性、泛化和推理能力上存在不足，且缺乏专门的数据集。

Method: 引入FEA指令数据集和FEABench基准，提出FEALLM架构以捕捉更详细的面部信息。

Result: FEALLM在FEABench上表现优异，并在RAF-DB、AffectNet等数据集上展示了零样本泛化能力。

Conclusion: FEALLM和FEA指令数据集有效提升了面部情绪分析的性能，具有广泛的应用潜力。

Abstract: Facial Emotion Analysis (FEA) plays a crucial role in visual affective
computing, aiming to infer a person's emotional state based on facial data.
Scientifically, facial expressions (FEs) result from the coordinated movement
of facial muscles, which can be decomposed into specific action units (AUs)
that provide detailed emotional insights. However, traditional methods often
struggle with limited interpretability, constrained generalization and
reasoning abilities. Recently, Multimodal Large Language Models (MLLMs) have
shown exceptional performance in various visual tasks, while they still face
significant challenges in FEA due to the lack of specialized datasets and their
inability to capture the intricate relationships between FEs and AUs. To
address these issues, we introduce a novel FEA Instruction Dataset that
provides accurate and aligned FE and AU descriptions and establishes causal
reasoning relationships between them, followed by constructing a new benchmark,
FEABench. Moreover, we propose FEALLM, a novel MLLM architecture designed to
capture more detailed facial information, enhancing its capability in FEA
tasks. Our model demonstrates strong performance on FEABench and impressive
generalization capability through zero-shot evaluation on various datasets,
including RAF-DB, AffectNet, BP4D, and DISFA, showcasing its robustness and
effectiveness in FEA tasks. The dataset and code will be available at
https://github.com/953206211/FEALLM.

</details>

### [823] [G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning](https://arxiv.org/abs/2505.13426)
*Liang Chen,Hongcheng Gao,Tianyu Liu,Zhiqi Huang,Flood Sung,Xinyu Zhou,Yuxin Wu,Baobao Chang*

Main category: cs.CV

TLDR: VLM-Gym是一个专为多游戏并行训练设计的强化学习环境，通过纯RL驱动的自我进化训练G0模型，并进一步开发了感知增强的G1模型，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型（VLMs）在交互式视觉环境（如游戏）中决策能力不足的问题。

Method: 引入VLM-Gym环境，训练G0和G1模型，G1结合感知增强的冷启动和RL微调。

Result: G1模型在所有游戏中表现优于其教师模型和领先的专有模型（如Claude-3.7-Sonnet-Thinking）。

Conclusion: 感知和推理能力在RL训练过程中相互促进，G1模型展示了VLMs作为交互式代理的潜力。

Abstract: Vision-Language Models (VLMs) excel in many direct multimodal tasks but
struggle to translate this prowess into effective decision-making within
interactive, visually rich environments like games. This ``knowing-doing'' gap
significantly limits their potential as autonomous agents, as leading VLMs
often performing badly in simple games. To address this, we introduce VLM-Gym,
a curated reinforcement learning (RL) environment featuring diverse visual
games with unified interfaces and adjustable, compositional difficulty,
specifically designed for scalable multi-game parallel training. Leveraging
VLM-Gym, we train G0 models using pure RL-driven self-evolution, which
demonstrate emergent perception and reasoning patterns. To further mitigate
challenges arising from game diversity, we develop G1 models. G1 incorporates a
perception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models
consistently surpass their teacher across all games and outperform leading
proprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals
an intriguing finding: perception and reasoning abilities mutually bootstrap
each other throughout the RL training process. Source code including VLM-Gym
and RL training are released at https://github.com/chenllliang/G1 to foster
future research in advancing VLMs as capable interactive agents.

</details>

### [824] [Understanding Complexity in VideoQA via Visual Program Generation](https://arxiv.org/abs/2505.13429)
*Cristobal Eyzaguirre,Igor Vasiljevic,Achal Dave,Jiajun Wu,Rares Andrei Ambrus,Thomas Kollar,Juan Carlos Niebles,Pavel Tokmakov*

Main category: cs.CV

TLDR: 提出了一种数据驱动的方法来分析视频问答中的查询复杂度，通过代码生成技术自动评估问题难度，优于人工预测，并构建了一个更难的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基准设计依赖人工设计复杂问题，但实验表明人工难以预测模型的实际困难问题，因此需要自动化的评估方法。

Method: 利用代码生成技术，将生成代码的复杂度作为问题难度的代理指标，并提出算法从代码中估计问题复杂度。

Result: 该方法与模型性能的相关性显著优于人工估计，并能自动生成更复杂的问题，构建的基准比NExT-QA难1.9倍。

Conclusion: 自动化的数据驱动方法能更准确地评估和生成复杂问题，为视频问答领域提供了可扩展的工具。

Abstract: We propose a data-driven approach to analyzing query complexity in Video
Question Answering (VideoQA). Previous efforts in benchmark design have relied
on human expertise to design challenging questions, yet we experimentally show
that humans struggle to predict which questions are difficult for machine
learning models. Our automatic approach leverages recent advances in code
generation for visual question answering, using the complexity of generated
code as a proxy for question difficulty. We demonstrate that this measure
correlates significantly better with model performance than human estimates. To
operationalize this insight, we propose an algorithm for estimating question
complexity from code. It identifies fine-grained primitives that correlate with
the hardest questions for any given set of models, making it easy to scale to
new approaches in the future. Finally, to further illustrate the utility of our
method, we extend it to automatically generate complex questions, constructing
a new benchmark that is 1.9 times harder than the popular NExT-QA.

</details>

### [825] [KinTwin: Imitation Learning with Torque and Muscle Driven Biomechanical Models Enables Precise Replication of Able-Bodied and Impaired Movement from Markerless Motion Capture](https://arxiv.org/abs/2505.13436)
*R. James Cotton*

Main category: cs.CV

TLDR: 模仿学习应用于生物力学模型，能够准确推断运动中的动力学参数，如关节扭矩和肌肉激活，为临床运动分析提供高质量工具。


<details>
  <summary>Details</summary>
Motivation: 提高运动科学和康复领域的运动分析质量，详细描述运动障碍和对干预的反应，甚至早期检测神经系统疾病或跌倒风险。

Method: 使用模仿学习技术，基于大量正常和运动障碍个体的运动数据，训练生物力学模型（KinTwin），推断逆动力学参数。

Result: KinTwin能准确复现多种运动的运动学特征，包括使用辅助设备或治疗师协助的运动，并能推断临床相关的关节扭矩和肌肉激活差异。

Conclusion: 模仿学习在临床运动分析中具有潜力，可为高质量运动分析提供支持。

Abstract: Broader access to high-quality movement analysis could greatly benefit
movement science and rehabilitation, such as allowing more detailed
characterization of movement impairments and responses to interventions, or
even enabling early detection of new neurological conditions or fall risk.
While emerging technologies are making it easier to capture kinematics with
biomechanical models, or how joint angles change over time, inferring the
underlying physics that give rise to these movements, including ground reaction
forces, joint torques, or even muscle activations, is still challenging. Here
we explore whether imitation learning applied to a biomechanical model from a
large dataset of movements from able-bodied and impaired individuals can learn
to compute these inverse dynamics. Although imitation learning in human pose
estimation has seen great interest in recent years, our work differences in
several ways: we focus on using an accurate biomechanical model instead of
models adopted for computer vision, we test it on a dataset that contains
participants with impaired movements, we reported detailed tracking metrics
relevant for the clinical measurement of movement including joint angles and
ground contact events, and finally we apply imitation learning to a
muscle-driven neuromusculoskeletal model. We show that our imitation learning
policy, KinTwin, can accurately replicate the kinematics of a wide range of
movements, including those with assistive devices or therapist assistance, and
that it can infer clinically meaningful differences in joint torques and muscle
activations. Our work demonstrates the potential for using imitation learning
to enable high-quality movement analysis in clinical practice.

</details>

### [826] [FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance](https://arxiv.org/abs/2505.13437)
*Dian Shao,Mingfei Shi,Shengda Xu,Haodong Chen,Yongle Huang,Binglu Wang*

Main category: cs.CV

TLDR: FinePhys是一个结合物理学的细粒度人类动作生成框架，通过2D姿态估计、3D姿态提升和物理运动重估计模块，显著提升了动作生成的物理合理性和自然性。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成方法在建模细粒度语义和复杂时间动态时表现不佳，尤其是在生成高难度体操动作时效果不理想。

Method: FinePhys通过在线2D姿态估计、2D到3D的上下文学习提升，以及基于欧拉-拉格朗日方程的物理运动重估计模块，融合数据驱动和物理预测的3D姿态，为扩散过程提供多尺度2D热图指导。

Result: 在FineGym的三个细粒度动作子集（FX-JUMP、FX-TURN和FX-SALTO）上，FinePhys显著优于基线方法，生成的动作更自然和物理合理。

Conclusion: FinePhys通过结合数据驱动和物理建模，有效解决了细粒度人类动作生成的挑战，为未来研究提供了新方向。

Abstract: Despite significant advances in video generation, synthesizing physically
plausible human actions remains a persistent challenge, particularly in
modeling fine-grained semantics and complex temporal dynamics. For instance,
generating gymnastics routines such as "switch leap with 0.5 turn" poses
substantial difficulties for current methods, often yielding unsatisfactory
results. To bridge this gap, we propose FinePhys, a Fine-grained human action
generation framework that incorporates Physics to obtain effective skeletal
guidance. Specifically, FinePhys first estimates 2D poses in an online manner
and then performs 2D-to-3D dimension lifting via in-context learning. To
mitigate the instability and limited interpretability of purely data-driven 3D
poses, we further introduce a physics-based motion re-estimation module
governed by Euler-Lagrange equations, calculating joint accelerations via
bidirectional temporal updating. The physically predicted 3D poses are then
fused with data-driven ones, offering multi-scale 2D heatmap guidance for the
diffusion process. Evaluated on three fine-grained action subsets from FineGym
(FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms
competitive baselines. Comprehensive qualitative results further demonstrate
FinePhys's ability to generate more natural and plausible fine-grained human
actions.

</details>

### [827] [VTBench: Evaluating Visual Tokenizers for Autoregressive Image Generation](https://arxiv.org/abs/2505.13439)
*Huawei Lin,Tong Geng,Zhaozhuo Xu,Weijie Zhao*

Main category: cs.CV

TLDR: VTBench是一个新的基准测试，用于评估视觉分词器（VT）在图像重建、细节保留和文本保留任务中的表现，发现连续VAEs优于离散VTs。


<details>
  <summary>Details</summary>
Motivation: 当前离散视觉分词器在图像重建和细节保留方面表现不佳，缺乏专门的评估基准。

Method: 引入VTBench，系统评估多种视觉分词器，使用多种指标衡量重建质量。

Result: 连续VAEs在保留空间结构和语义细节上优于离散VTs，后者常导致失真和细节丢失。

Conclusion: 呼吁开发更强大的开源视觉分词器，并公开了基准和代码以促进研究。

Abstract: Autoregressive (AR) models have recently shown strong performance in image
generation, where a critical component is the visual tokenizer (VT) that maps
continuous pixel inputs to discrete token sequences. The quality of the VT
largely defines the upper bound of AR model performance. However, current
discrete VTs fall significantly behind continuous variational autoencoders
(VAEs), leading to degraded image reconstructions and poor preservation of
details and text. Existing benchmarks focus on end-to-end generation quality,
without isolating VT performance. To address this gap, we introduce VTBench, a
comprehensive benchmark that systematically evaluates VTs across three core
tasks: Image Reconstruction, Detail Preservation, and Text Preservation, and
covers a diverse range of evaluation scenarios. We systematically assess
state-of-the-art VTs using a set of metrics to evaluate the quality of
reconstructed images. Our findings reveal that continuous VAEs produce superior
visual representations compared to discrete VTs, particularly in retaining
spatial structure and semantic detail. In contrast, the degraded
representations produced by discrete VTs often lead to distorted
reconstructions, loss of fine-grained textures, and failures in preserving text
and object integrity. Furthermore, we conduct experiments on GPT-4o image
generation and discuss its potential AR nature, offering new insights into the
role of visual tokenization. We release our benchmark and codebase publicly to
support further research and call on the community to develop strong,
general-purpose open-source VTs.

</details>

### [828] [Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos](https://arxiv.org/abs/2505.13440)
*Ruoyu Wang,Yi Ma,Shenghua Gao*

Main category: cs.CV

TLDR: 提出一种两阶段策略，无需相机参数或几何先验，仅通过原始视频帧或多视角图像训练视图合成模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖校准相机或几何先验，限制了在大规模未校准数据上的应用。

Method: 第一阶段在隐式潜在空间中重建场景，第二阶段通过3D高斯基元显式优化几何一致性。

Result: 实验表明，该方法在视图合成和相机姿态估计上优于依赖监督信息的方法。

Conclusion: 两阶段策略互补，显著提升了自监督训练的效果。

Abstract: Currently almost all state-of-the-art novel view synthesis and reconstruction
models rely on calibrated cameras or additional geometric priors for training.
These prerequisites significantly limit their applicability to massive
uncalibrated data. To alleviate this requirement and unlock the potential for
self-supervised training on large-scale uncalibrated videos, we propose a novel
two-stage strategy to train a view synthesis model from only raw video frames
or multi-view images, without providing camera parameters or other priors. In
the first stage, we learn to reconstruct the scene implicitly in a latent space
without relying on any explicit 3D representation. Specifically, we predict
per-frame latent camera and scene context features, and employ a view synthesis
model as a proxy for explicit rendering. This pretraining stage substantially
reduces the optimization complexity and encourages the network to learn the
underlying 3D consistency in a self-supervised manner. The learned latent
camera and implicit scene representation have a large gap compared with the
real 3D world. To reduce this gap, we introduce the second stage training by
explicitly predicting 3D Gaussian primitives. We additionally apply explicit
Gaussian Splatting rendering loss and depth projection loss to align the
learned latent representations with physically grounded 3D geometry. In this
way, Stage 1 provides a strong initialization and Stage 2 enforces 3D
consistency - the two stages are complementary and mutually beneficial.
Extensive experiments demonstrate the effectiveness of our approach, achieving
high-quality novel view synthesis and accurate camera pose estimation, compared
to methods that employ supervision with calibration, pose, or depth
information. The code is available at https://github.com/Dwawayu/Pensieve.

</details>

<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [829] [LaPON: A Lagrange's-mean-value-theorem-inspired operator network for solving PDEs and its application on NSE](https://arxiv.org/abs/2505.12360)
*Siwen Zhang,Xizeng Zhao,Zhengzhi Deng,Zhaoyuan Huang,Gang Tao,Nuo Xu,Zhouteng Ye*

Main category: physics.comp-ph

TLDR: LaPON是一种结合神经算子与传统数值方法的混合框架，通过将先验知识嵌入网络架构而非损失函数，严格满足物理约束，显著提升了粗分辨率下非线性PDE求解的精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决非线性PDE在粗时空分辨率下保持精度的挑战，克服传统物理约束方法（如PINNs）的软约束不足。

Method: 提出LaPON框架，基于Lagrange中值定理设计神经算子，直接嵌入先验知识，结合数值方法补偿离散化误差。

Result: 在Navier-Stokes方程模拟中，LaPON在8倍粗网格和8倍大时间步长下，精度和稳定性优于基准方法，涡度相关性超过0.98，且能泛化至未见流态。

Conclusion: LaPON为高保真流体动力学模拟提供了可扩展且可靠的解决方案，展示了在天气预测和工程设计等领域的广泛应用潜力。

Abstract: Accelerating the solution of nonlinear partial differential equations (PDEs)
while maintaining accuracy at coarse spatiotemporal resolution remains a key
challenge in scientific computing. Physics-informed machine learning (ML)
methods such as Physics-Informed Neural Networks (PINNs) introduce prior
knowledge through loss functions to ensure physical consistency, but their
"soft constraints" are usually not strictly satisfied. Here, we propose LaPON,
an operator network inspired by the Lagrange's mean value theorem, which embeds
prior knowledge directly into the neural network architecture instead of the
loss function, making the neural network naturally satisfy the given
constraints. This is a hybrid framework that combines neural operators with
traditional numerical methods, where neural operators are used to compensate
for the effect of discretization errors on the analytical scale in
under-resolution simulations. As evaluated on turbulence problem modeled by the
Navier-Stokes equations (NSE), the multiple time step extrapolation accuracy
and stability of LaPON exceed the direct numerical simulation baseline at 8x
coarser grids and 8x larger time steps, while achieving a vorticity correlation
of more than 0.98 with the ground truth. It is worth noting that the model can
be well generalized to unseen flow states, such as turbulence with different
forcing, without retraining. In addition, with the same training data, LaPON's
comprehensive metrics on the out-of-distribution test set are at least
approximately twice as good as two popular ML baseline methods. By combining
numerical computing with machine learning, LaPON provides a scalable and
reliable solution for high-fidelity fluid dynamics simulation, showing the
potential for wide application in fields such as weather forecasting and
engineering design.

</details>

<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [830] [HAKES: Scalable Vector Database for Embedding Search Service](https://arxiv.org/abs/2505.12524)
*Guoyu Hu,Shaofeng Cai,Tien Tuan Anh Dinh,Zhongle Xie,Cong Yue,Gang Chen,Beng Chin Ooi*

Main category: cs.DB

TLDR: 论文提出了一种新型向量数据库HAKES，通过两阶段索引设计和轻量级机器学习优化，解决了现有图索引在高并发读写负载下的性能问题。


<details>
  <summary>Details</summary>
Motivation: 现有向量数据库的图索引构建成本高，并发读写性能差，且难以扩展。

Method: 提出两阶段索引设计（快速过滤+精炼），结合轻量级机器学习优化参数，并支持动态查询终止检查和解耦参数管理。

Result: 实验表明，新索引在高召回区域和并发读写负载下优于基线，HAKES系统吞吐量最高提升16倍。

Conclusion: HAKES是一种高效、可扩展的分布式向量数据库，适用于高并发读写场景。

Abstract: Modern deep learning models capture the semantics of complex data by
transforming them into high-dimensional embedding vectors. Emerging
applications, such as retrieval-augmented generation, use approximate nearest
neighbor (ANN) search in the embedding vector space to find similar data.
Existing vector databases provide indexes for efficient ANN searches, with
graph-based indexes being the most popular due to their low latency and high
recall in real-world high-dimensional datasets. However, these indexes are
costly to build, suffer from significant contention under concurrent read-write
workloads, and scale poorly to multiple servers.
  Our goal is to build a vector database that achieves high throughput and high
recall under concurrent read-write workloads. To this end, we first propose an
ANN index with an explicit two-stage design combining a fast filter stage with
highly compressed vectors and a refine stage to ensure recall, and we devise a
novel lightweight machine learning technique to fine-tune the index parameters.
We introduce an early termination check to dynamically adapt the search process
for each query. Next, we add support for writes while maintaining search
performance by decoupling the management of the learned parameters. Finally, we
design HAKES, a distributed vector database that serves the new index in a
disaggregated architecture. We evaluate our index and system against 12
state-of-the-art indexes and three distributed vector databases, using
high-dimensional embedding datasets generated by deep learning models. The
experimental results show that our index outperforms index baselines in the
high recall region and under concurrent read-write workloads. Furthermore,
\namesys{} is scalable and achieves up to $16\times$ higher throughputs than
the baselines. The HAKES project is open-sourced at
https://www.comp.nus.edu.sg/~dbsystem/hakes/.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [831] [The Stochastic Occupation Kernel (SOCK) Method for Learning Stochastic Differential Equations](https://arxiv.org/abs/2505.11622)
*Michael L. Wells,Kamel Lahouel,Bruno Jedynak*

Main category: stat.ML

TLDR: 提出了一种基于核的新方法，用于学习多元随机微分方程（SDE），通过两步法估计漂移和扩散函数，避免了似然函数的计算困难。


<details>
  <summary>Details</summary>
Motivation: 解决随机微分方程学习中常见的似然函数难以处理的问题，提高估计效率和准确性。

Method: 使用向量值和算子值占据核分别估计漂移和扩散函数，通过重建误差优化目标，结合Fenchel对偶性提高效率。

Result: 在模拟基准和真实世界数据集（如阿尔茨海默病淀粉样蛋白成像）上验证了方法的准确性和效率。

Conclusion: 该方法在避免复杂似然计算的同时，保持了高预测精度，适用于实际应用。

Abstract: We present a novel kernel-based method for learning multivariate stochastic
differential equations (SDEs). The method follows a two-step procedure: we
first estimate the drift term function, then the (matrix-valued) diffusion
function given the drift. Occupation kernels are integral functionals on a
reproducing kernel Hilbert space (RKHS) that aggregate information over a
trajectory. Our approach leverages vector-valued occupation kernels for
estimating the drift component of the stochastic process. For diffusion
estimation, we extend this framework by introducing operator-valued occupation
kernels, enabling the estimation of an auxiliary matrix-valued function as a
positive semi-definite operator, from which we readily derive the diffusion
estimate. This enables us to avoid common challenges in SDE learning, such as
intractable likelihoods, by optimizing a reconstruction-error-based objective.
We propose a simple learning procedure that retains strong predictive accuracy
while using Fenchel duality to promote efficiency. We validate the method on
simulated benchmarks and a real-world dataset of Amyloid imaging in healthy and
Alzheimer's disease (AD) subjects.

</details>

### [832] [Humble your Overconfident Networks: Unlearning Overfitting via Sequential Monte Carlo Tempered Deep Ensembles](https://arxiv.org/abs/2505.11671)
*Andrew Millard,Zheng Zhao,Joshua Murphy,Simon Maskell*

Main category: stat.ML

TLDR: SMCSGHMC结合了SMC和SGHMC，实现了高效的mini-batch采样，在多个任务中表现优于SGD和深度集成，并改善了模型的校准性。


<details>
  <summary>Details</summary>
Motivation: 传统SMC方法因需全批量梯度计算而受限，需要一种更高效的采样方法。

Method: 将SGHMC提议引入SMC，开发了SMCSGHMC算法，支持mini-batch采样。

Result: 在图像分类、OOD检测和迁移学习中表现优异，减少过拟合并提升校准性。

Conclusion: SMCSGHMC为预训练神经网络提供了灵活、可扩展的贝叶斯建模途径。

Abstract: Sequential Monte Carlo (SMC) methods offer a principled approach to Bayesian
uncertainty quantification but are traditionally limited by the need for
full-batch gradient evaluations. We introduce a scalable variant by
incorporating Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) proposals
into SMC, enabling efficient mini-batch based sampling. Our resulting SMCSGHMC
algorithm outperforms standard stochastic gradient descent (SGD) and deep
ensembles across image classification, out-of-distribution (OOD) detection, and
transfer learning tasks. We further show that SMCSGHMC mitigates overfitting
and improves calibration, providing a flexible, scalable pathway for converting
pretrained neural networks into well-calibrated Bayesian models.

</details>

### [833] [Missing Data Imputation by Reducing Mutual Information with Rectified Flows](https://arxiv.org/abs/2505.11749)
*Jiahao Yu,Qizhen Ying,Leyang Wang,Ziyue Jiang,Song Liu*

Main category: stat.ML

TLDR: 提出了一种新颖的迭代缺失数据填补方法，通过逐步减少数据与缺失掩码之间的互信息来优化填补效果。


<details>
  <summary>Details</summary>
Motivation: 受GAN方法的启发，旨在减少缺失模式的预测性，明确以降低互信息为目标。

Method: 迭代最小化填补数据与缺失掩码联合分布与前一迭代边缘分布乘积之间的KL散度。

Result: 实验证明该方法在合成和真实数据集上具有优越的填补性能。

Conclusion: 该方法为缺失数据填补提供了一种有效框架，部分现有技术可视为其近似特例。

Abstract: This paper introduces a novel iterative method for missing data imputation
that sequentially reduces the mutual information between data and their
corresponding missing mask. Inspired by GAN-based approaches, which train
generators to decrease the predictability of missingness patterns, our method
explicitly targets the reduction of mutual information. Specifically, our
algorithm iteratively minimizes the KL divergence between the joint
distribution of the imputed data and missing mask, and the product of their
marginals from the previous iteration. We show that the optimal imputation
under this framework corresponds to solving an ODE, whose velocity field
minimizes a rectified flow training objective. We further illustrate that some
existing imputation techniques can be interpreted as approximate special cases
of our mutual-information-reducing framework. Comprehensive experiments on
synthetic and real-world datasets validate the efficacy of our proposed
approach, demonstrating superior imputation performance.

</details>

### [834] [Multi-Attribute Graph Estimation with Sparse-Group Non-Convex Penalties](https://arxiv.org/abs/2505.11984)
*Jitendra K Tugnait*

Main category: stat.ML

TLDR: 论文提出了一种多属性图学习方法，分析了凸和非凸惩罚函数的理论性能，并通过ADMM优化方法实现了高维条件下的局部一致性和精度矩阵估计。


<details>
  <summary>Details</summary>
Motivation: 现有图估计方法多基于单属性模型，而多属性图模型中每个节点代表一个随机向量，需要统一的理论分析框架。

Method: 使用惩罚对数似然目标函数，结合凸（稀疏组lasso）和非凸（log-sum和SCAD）惩罚函数，采用ADMM优化方法。

Result: 在合成数据中，稀疏组log-sum惩罚函数在F1分数和汉明距离上显著优于lasso和SCAD惩罚函数。

Conclusion: 多属性图学习方法在高维条件下具有理论保证和实际性能优势。

Abstract: We consider the problem of inferring the conditional independence graph (CIG)
of high-dimensional Gaussian vectors from multi-attribute data. Most existing
methods for graph estimation are based on single-attribute models where one
associates a scalar random variable with each node. In multi-attribute
graphical models, each node represents a random vector. In this paper we
provide a unified theoretical analysis of multi-attribute graph learning using
a penalized log-likelihood objective function. We consider both convex
(sparse-group lasso) and sparse-group non-convex (log-sum and smoothly clipped
absolute deviation (SCAD) penalties) penalty/regularization functions. An
alternating direction method of multipliers (ADMM) approach coupled with local
linear approximation to non-convex penalties is presented for optimization of
the objective function. For non-convex penalties, theoretical analysis
establishing local consistency in support recovery, local convexity and
precision matrix estimation in high-dimensional settings is provided under two
sets of sufficient conditions: with and without some irrepresentability
conditions. We illustrate our approaches using both synthetic and real-data
numerical examples. In the synthetic data examples the sparse-group log-sum
penalized objective function significantly outperformed the lasso penalized as
well as SCAD penalized objective functions with $F_1$-score and Hamming
distance as performance metrics.

</details>

### [835] [Thompson Sampling-like Algorithms for Stochastic Rising Bandits](https://arxiv.org/abs/2505.12092)
*Marco Fiandri,Alberto Maria Metelli,Francesco Trovò*

Main category: stat.ML

TLDR: 论文研究了随机上升休息多臂老虎机（SRRB）问题，提出并分析了基于Thompson采样（TS）的算法，探讨了其在不同环境假设下的表现，并提供了新的遗憾分析工具。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于填补SRRB问题中TS类算法的研究空白，并探索其在不同环境下的表现。

Method: 采用基于TS的滑动窗口方法，分析其在SRRB中的表现，并引入新的技术工具。

Result: 结果表明TS类算法在某些假设下可实现次线性遗憾，同时提出了一个基于复杂度指标的遗憾下界。

Conclusion: 结论指出TS类算法在SRRB问题中具有潜力，但表现取决于环境特性，数值模拟验证了其与现有方法的竞争力。

Abstract: Stochastic rising rested bandit (SRRB) is a setting where the arms' expected
rewards increase as they are pulled. It models scenarios in which the
performances of the different options grow as an effect of an underlying
learning process (e.g., online model selection). Even if the bandit literature
provides specifically crafted algorithms based on upper-confidence bounds for
such a setting, no study about Thompson sampling TS-like algorithms has been
performed so far. The strong regularity of the expected rewards in the SRRB
setting suggests that specific instances may be tackled effectively using
adapted and sliding-window TS approaches. This work provides novel regret
analyses for such algorithms in SRRBs, highlighting the challenges and
providing new technical tools of independent interest. Our results allow us to
identify under which assumptions TS-like algorithms succeed in achieving
sublinear regret and which properties of the environment govern the complexity
of the regret minimization problem when approached with TS. Furthermore, we
provide a regret lower bound based on a complexity index we introduce. Finally,
we conduct numerical simulations comparing TS-like algorithms with
state-of-the-art approaches for SRRBs in synthetic and real-world settings.

</details>

### [836] [T-Rex: Fitting a Robust Factor Model via Expectation-Maximization](https://arxiv.org/abs/2505.12117)
*Daniel Cederberg*

Main category: stat.ML

TLDR: 提出了一种基于Tyler M估计的鲁棒EM算法，用于拟合统计因子模型，解决了传统方法对重尾和异常值的敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 高维数据中的低维结构建模需求，传统方法（如PCA或高斯假设的最大似然估计）对重尾和异常值敏感。

Method: 基于Tyler M估计的EM算法，结合低秩加对角协方差结构约束。

Result: 在合成和真实数据实验中验证了方法的鲁棒性，适用于非均匀噪声下的方向估计和子空间恢复。

Conclusion: 提出的方法在鲁棒性上优于传统方法，适用于复杂数据环境。

Abstract: Over the past decades, there has been a surge of interest in studying
low-dimensional structures within high-dimensional data. Statistical factor
models $-$ i.e., low-rank plus diagonal covariance structures $-$ offer a
powerful framework for modeling such structures. However, traditional methods
for fitting statistical factor models, such as principal component analysis
(PCA) or maximum likelihood estimation assuming the data is Gaussian, are
highly sensitive to heavy tails and outliers in the observed data. In this
paper, we propose a novel expectation-maximization (EM) algorithm for robustly
fitting statistical factor models. Our approach is based on Tyler's M-estimator
of the scatter matrix for an elliptical distribution, and consists of solving
Tyler's maximum likelihood estimation problem while imposing a structural
constraint that enforces the low-rank plus diagonal covariance structure. We
present numerical experiments on both synthetic and real examples,
demonstrating the robustness of our method for direction-of-arrival estimation
in nonuniform noise and subspace recovery.

</details>

### [837] [Training Latent Diffusion Models with Interacting Particle Algorithms](https://arxiv.org/abs/2505.12412)
*Tim Y. J. Wang,Juan Kuntz,O. Deniz Akyildiz*

Main category: stat.ML

TLDR: 提出了一种基于粒子的新型算法，用于端到端训练潜在扩散模型，通过自由能泛函最小化实现梯度流，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有粒子方法和变分推断方法在训练潜在扩散模型时存在局限性，需要更高效且理论支持的算法。

Method: 将训练任务重新表述为自由能泛函最小化问题，通过粒子系统近似梯度流，并提供理论误差保证。

Result: 新算法在实验中表现优于现有粒子方法和变分推断方法。

Conclusion: 该算法为潜在扩散模型训练提供了高效且理论支持的新方法。

Abstract: We introduce a novel particle-based algorithm for end-to-end training of
latent diffusion models. We reformulate the training task as minimizing a free
energy functional and obtain a gradient flow that does so. By approximating the
latter with a system of interacting particles, we obtain the algorithm, which
we underpin it theoretically by providing error guarantees. The novel algorithm
compares favorably in experiments with previous particle-based methods and
variational inference analogues.

</details>

### [838] [High-Dimensional Dynamic Covariance Models with Random Forests](https://arxiv.org/abs/2505.12444)
*Shuguang Yu,Fan Zhou,Yingjie Zhang,Ziqi Chen,Hongtu Zhu*

Main category: stat.ML

TLDR: 提出了一种基于随机森林的非参数方法，用于估计高维动态协方差矩阵，支持多条件协变量，并具有理论保证。


<details>
  <summary>Details</summary>
Motivation: 传统静态方法和核平滑方法无法有效捕捉分布异质性或处理多条件协变量，因此需要一种新的动态非参数方法。

Method: 利用随机森林构建动态非参数协方差模型，支持多条件协变量，并提供理论一致性分析。

Result: 在高维场景下，方法表现出均匀一致性，适用于响应维度随样本量次指数增长的情况，并通过模拟和股票数据分析验证了其有效性。

Conclusion: 该方法首次将随机森林用于高维动态协方差矩阵估计，能够有效建模复杂动态关系。

Abstract: This paper introduces a novel nonparametric method for estimating
high-dimensional dynamic covariance matrices with multiple conditioning
covariates, leveraging random forests and supported by robust theoretical
guarantees. Unlike traditional static methods, our dynamic nonparametric
covariance models effectively capture distributional heterogeneity.
Furthermore, unlike kernel-smoothing methods, which are restricted to a single
conditioning covariate, our approach accommodates multiple covariates in a
fully nonparametric framework. To the best of our knowledge, this is the first
method to use random forests for estimating high-dimensional dynamic covariance
matrices. In high-dimensional settings, we establish uniform consistency
theory, providing nonasymptotic error rates and model selection properties,
even when the response dimension grows sub-exponentially with the sample size.
These results hold uniformly across a range of conditioning variables. The
method's effectiveness is demonstrated through simulations and a stock dataset
analysis, highlighting its ability to model complex dynamics in
high-dimensional scenarios.

</details>

### [839] [Wasserstein Barycenter Gaussian Process based Bayesian Optimization](https://arxiv.org/abs/2505.12471)
*Antonio Candelieri,Andrea Ponti,Francesco Archetti*

Main category: stat.ML

TLDR: 论文提出了一种基于Wasserstein Barycenter的高斯过程贝叶斯优化方法（WBGP-BO），解决了传统方法在超参数调优中的不足，并在复杂测试问题上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程贝叶斯优化在超参数调优中存在不足，尤其是在复杂问题上表现不佳，因此需要一种更稳健的方法。

Method: 通过预定义一组超参数值拟合多个高斯过程，并将其组合为Wasserstein Barycenter模型，形成WBGP-BO方法。

Result: WBGP-BO在复杂测试问题上表现优于传统方法，能够收敛到最优解。

Conclusion: WBGP-BO为解决高斯过程贝叶斯优化中的超参数调优问题提供了有效方案，适用于复杂优化任务。

Abstract: Gaussian Process based Bayesian Optimization is a widely applied algorithm to
learn and optimize under uncertainty, well-known for its sample efficiency.
However, recently -- and more frequently -- research studies have empirically
demonstrated that the Gaussian Process fitting procedure at its core could be
its most relevant weakness. Fitting a Gaussian Process means tuning its
kernel's hyperparameters to a set of observations, but the common Maximum
Likelihood Estimation technique, usually appropriate for learning tasks, has
shown different criticalities in Bayesian Optimization, making theoretical
analysis of this algorithm an open challenge. Exploiting the analogy between
Gaussian Processes and Gaussian Distributions, we present a new approach which
uses a prefixed set of hyperparameters values to fit as many Gaussian Processes
and then combines them into a unique model as a Wasserstein Barycenter of
Gaussian Processes. We considered both "easy" test problems and others known to
undermine the \textit{vanilla} Bayesian Optimization algorithm. The new method,
namely Wasserstein Barycenter Gausssian Process based Bayesian Optimization
(WBGP-BO), resulted promising and able to converge to the optimum, contrary to
vanilla Bayesian Optimization, also on the most "tricky" test problems.

</details>

### [840] [Multi-modal contrastive learning adapts to intrinsic dimensions of shared latent variables](https://arxiv.org/abs/2505.12473)
*Yu Gui,Cong Ma,Zongming Ma*

Main category: stat.ML

TLDR: 多模态对比学习通过温度优化，不仅能最大化模态间互信息，还能适应数据的低维内在结构。


<details>
  <summary>Details</summary>
Motivation: 研究多模态对比学习在非线性表示和一般数据分布下的理论性质。

Method: 通过理论分析和实验验证，探索对比学习在低维表示中的能力。

Result: 实验证明对比学习能学习低维且信息丰富的表示。

Conclusion: 多模态对比学习在理论和实践中均表现出色，适应数据内在维度。

Abstract: Multi-modal contrastive learning as a self-supervised representation learning
technique has achieved great success in foundation model training, such as
CLIP~\citep{radford2021learning}. In this paper, we study the theoretical
properties of the learned representations from multi-modal contrastive learning
beyond linear representations and specific data distributions. Our analysis
reveals that, enabled by temperature optimization, multi-modal contrastive
learning not only maximizes mutual information between modalities but also
adapts to intrinsic dimensions of data, which can be much lower than
user-specified dimensions for representation vectors. Experiments on both
synthetic and real-world datasets demonstrate the ability of contrastive
learning to learn low-dimensional and informative representations, bridging
theoretical insights and practical performance.

</details>

### [841] [Nonlinear Laplacians: Tunable principal component analysis under directional prior information](https://arxiv.org/abs/2505.12528)
*Yuxin Ma,Dmitriy Kunisky*

Main category: stat.ML

TLDR: 论文提出了一种新算法家族，用于在信号方向已知的情况下从噪声观测中检测和估计秩一信号，特别关注信号条目偏正的情况。通过构造非线性拉普拉斯矩阵并分析其特征，算法在稀疏主成分分析和高斯植入子矩阵问题中表现优于直接谱算法。


<details>
  <summary>Details</summary>
Motivation: 研究如何在信号方向已知且信号条目偏正的情况下，更有效地从噪声中检测和估计秩一信号。

Method: 通过构造非线性拉普拉斯矩阵（形式为Y + diag(σ(Y1))），并分析其顶部特征值和特征向量，设计算法。

Result: 理论分析和实验表明，选择合适的非线性函数σ时，非线性拉普拉斯谱算法显著优于直接谱算法。

Conclusion: 非线性拉普拉斯谱算法在特定条件下性能优越，且避免了更复杂算法的计算负担。

Abstract: We introduce a new family of algorithms for detecting and estimating a
rank-one signal from a noisy observation under prior information about that
signal's direction, focusing on examples where the signal is known to have
entries biased to be positive. Given a matrix observation $\mathbf{Y}$, our
algorithms construct a nonlinear Laplacian, another matrix of the form
$\mathbf{Y} + \mathrm{diag}(\sigma(\mathbf{Y}\mathbf{1}))$ for a nonlinear
$\sigma: \mathbb{R} \to \mathbb{R}$, and examine the top eigenvalue and
eigenvector of this matrix. When $\mathbf{Y}$ is the (suitably normalized)
adjacency matrix of a graph, our approach gives a class of algorithms that
search for unusually dense subgraphs by computing a spectrum of the graph
"deformed" by the degree profile $\mathbf{Y}\mathbf{1}$. We study the
performance of such algorithms compared to direct spectral algorithms (the case
$\sigma = 0$) on models of sparse principal component analysis with biased
signals, including the Gaussian planted submatrix problem. For such models, we
rigorously characterize the critical threshold strength of rank-one signal, as
a function of the nonlinearity $\sigma$, at which an outlier eigenvalue appears
in the spectrum of a nonlinear Laplacian. While identifying the $\sigma$ that
minimizes this critical signal strength in closed form seems intractable, we
explore three approaches to design $\sigma$ numerically: exhaustively searching
over simple classes of $\sigma$, learning $\sigma$ from datasets of problem
instances, and tuning $\sigma$ using black-box optimization of the critical
signal strength. We find both theoretically and empirically that, if $\sigma$
is chosen appropriately, then nonlinear Laplacian spectral algorithms
substantially outperform direct spectral algorithms, while avoiding the
complexity of broader classes of algorithms like approximate message passing or
general first order methods.

</details>

### [842] [Stacked conformal prediction](https://arxiv.org/abs/2505.12578)
*Paulo C. Marques F*

Main category: stat.ML

TLDR: 论文提出了一种堆叠集成预测模型的共形化方法，通过简单的元学习器实现计算成本可控的近似边际有效性，无需单独校准样本。


<details>
  <summary>Details</summary>
Motivation: 解决传统共形预测方法中需要单独校准样本的问题，同时保持计算效率。

Method: 使用堆叠集成模型，通过简单的元学习器实现共形化，避免单独校准。

Result: 实证结果表明，该方法优于标准的归纳替代方法。

Conclusion: 该方法在保持计算效率的同时，实现了近似边际有效性，具有实际应用潜力。

Abstract: We consider the conformalization of a stacked ensemble of predictive models,
showing that the potentially simple form of the meta-learner at the top of the
stack enables a procedure with manageable computational cost that achieves
approximate marginal validity without requiring the use of a separate
calibration sample. Empirical results indicate that the method compares
favorably to a standard inductive alternative.

</details>

### [843] [Testing Identifiability and Transportability with Observational and Experimental Data](https://arxiv.org/abs/2505.12801)
*Konstantina Lelova,Gregory F. Cooper,Sofia Triantafillou*

Main category: stat.ML

TLDR: 提出一种贝叶斯方法，用于评估因果效应是否可识别和可迁移，无需已知因果图。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中因果图未知时，因果效应识别和迁移的挑战。

Method: 结合源人群的实验数据和目标人群的观察数据，计算因果效应可识别和可迁移的概率。

Result: 模拟显示方法能正确识别可迁移因果效应并改进估计。

Conclusion: 该方法为未知因果图下的因果效应迁移提供了实用解决方案。

Abstract: Transporting causal information learned from experiments in one population to
another is a critical challenge in clinical research and decision-making.
Causal transportability uses causal graphs to model differences between the
source and target populations and identifies conditions under which causal
effects learned from experiments can be reused in a different population.
Similarly, causal identifiability identifies conditions under which causal
effects can be estimated from observational data. However, these approaches
rely on knowing the causal graph, which is often unavailable in real-world
settings. In this work, we propose a Bayesian method for assessing whether
Z-specific (conditional) causal effects are both identifiable and
transportable, without knowing the causal graph. Our method combines
experimental data from the source population with observational data from the
target population to compute the probability that a causal effect is both
identifiable from observational data and transportable. When this holds, we
leverage both observational data from the target domain and experimental data
from the source domain to obtain an unbiased, efficient estimator of the causal
effect in the target population. Using simulations, we demonstrate that our
method correctly identifies transportable causal effects and improves causal
effect estimation.

</details>

### [844] [Causality-Inspired Robustness for Nonlinear Models via Representation Learning](https://arxiv.org/abs/2505.12868)
*Marin Šola,Peter Bühlmann,Xinwei Shen*

Main category: stat.ML

TLDR: 论文提出了一种非线性因果框架下的分布鲁棒性方法，首次在非线性设置中实现了有限半径鲁棒性保证。


<details>
  <summary>Details</summary>
Motivation: 现实数据中存在普遍的分布偏移，传统分布鲁棒性优化方法需要预先指定不确定性集合，而因果建模框架能提供数据驱动的鲁棒性保证。

Method: 结合可识别表示学习的最新进展，提出了一种非线性因果框架下的鲁棒性方法。

Result: 在合成数据和真实单细胞数据上验证了理论发现，证明了有限半径鲁棒性的重要性。

Conclusion: 该方法首次在非线性设置中实现了有限半径鲁棒性保证，为分布鲁棒性提供了新的解决方案。

Abstract: Distributional robustness is a central goal of prediction algorithms due to
the prevalent distribution shifts in real-world data. The prediction model aims
to minimize the worst-case risk among a class of distributions, a.k.a., an
uncertainty set. Causality provides a modeling framework with a rigorous
robustness guarantee in the above sense, where the uncertainty set is
data-driven rather than pre-specified as in traditional distributional
robustness optimization. However, current causality-inspired robustness methods
possess finite-radius robustness guarantees only in the linear settings, where
the causal relationships among the covariates and the response are linear. In
this work, we propose a nonlinear method under a causal framework by
incorporating recent developments in identifiable representation learning and
establish a distributional robustness guarantee. To our best knowledge, this is
the first causality-inspired robustness method with such a finite-radius
robustness guarantee in nonlinear settings. Empirical validation of the
theoretical findings is conducted on both synthetic data and real-world
single-cell data, also illustrating that finite-radius robustness is crucial.

</details>

### [845] [Spline Dimensional Decomposition with Interpolation-based Optimal Knot Selection for Stochastic Dynamic Analysis](https://arxiv.org/abs/2505.12879)
*Yeonsu Kim,Junhan Lee,John T. Hwang,Bingran Wang,Dongjin Lee*

Main category: stat.ML

TLDR: 提出了一种基于插值的优化节点选择方法，用于提高样条维度分解（SDD）在非光滑或局部振荡非线性动态系统中的准确性。


<details>
  <summary>Details</summary>
Motivation: 动态系统中的前向不确定性量化因非光滑或局部振荡的非线性行为而具有挑战性，SDD的准确性对内部节点的位置高度敏感。

Method: 通过三步实现：1) 插值输入-输出曲线；2) 定义基于子区间的参考区域；3) 在每个区域内选择梯度最大点作为最优节点。

Result: 在模态分析中，提出的SDD方法比均匀或随机节点分布的SDD及高斯过程代理模型更准确，相对方差误差最低（2.89%）。

Conclusion: 该方法在数学函数和实际系统中均表现出高效性和可扩展性，仅需少量函数评估即可准确估计统计量和可靠性。

Abstract: Forward uncertainty quantification in dynamic systems is challenging due to
non-smooth or locally oscillating nonlinear behaviors. Spline dimensional
decomposition (SDD) effectively addresses such nonlinearity by partitioning
input coordinates via knot placement, yet its accuracy is highly sensitive to
the location of internal knots. Optimizing knots through sequential quadratic
programming can be effective, yet the optimization process becomes
computationally intense. We propose a computationally efficient,
interpolation-based method for optimal knot selection in SDD. The method
involves three steps: (1) interpolating input-output profiles, (2) defining
subinterval-based reference regions, and (3) selecting optimal knot locations
at maximum gradient points within each region. The resulting knot vector is
then applied to SDD for accurate approximation of non-smooth and locally
oscillating responses. A modal analysis of a lower control arm demonstrates
that SDD with the proposed knot selection achieves higher accuracy than SDD
with uniformly or randomly spaced knots, and also a Gaussian process surrogate
model. The proposed SDD exhibits the lowest relative variance error (2.89%),
compared to SDD with uniformly spaced knots (12.310%), randomly spaced knots
(15.274%), and Gaussian process (5.319%) in the first natural frequency
distribution. All surrogate models are constructed using the same 401
simulation datasets, and the relative errors are evaluated against a
2000-sample Monte Carlo simulation. The scalability and applicability of
proposed method are demonstrated through stochastic and reliability analyses of
mathematical functions (N=1, 3) and a lower control arm system (N=10). The
results confirm that both second-moment statistics and reliability estimates
can be accurately achieved with only a few hundred function evaluations or
finite element simulations.

</details>

### [846] [Asymptotic Performance of Time-Varying Bayesian Optimization](https://arxiv.org/abs/2505.13012)
*Anthony Bardou,Patrick Thiran*

Main category: stat.ML

TLDR: TVBO框架用于优化随时间变化的黑盒目标函数，研究其瞬时遗憾是否能渐近消失，并给出算法无关的上下界遗憾分析。


<details>
  <summary>Details</summary>
Motivation: 探讨TVBO算法是否具有无遗憾性质，并确定其条件。

Method: 提供算法无关的上下界遗憾分析，覆盖所有主要静态核函数类。

Result: 推导出TVBO算法具有无遗憾性质的充分条件。

Conclusion: TVBO在特定条件下可实现无遗憾性质，覆盖广泛核函数类。

Abstract: Time-Varying Bayesian Optimization (TVBO) is the go-to framework for
optimizing a time-varying black-box objective function that may be noisy and
expensive to evaluate. Is it possible for the instantaneous regret of a TVBO
algorithm to vanish asymptotically, and if so, when? We answer this question of
great theoretical importance by providing algorithm-independent lower regret
bounds and upper regret bounds for TVBO algorithms, from which we derive
sufficient conditions for a TVBO algorithm to have the no-regret property. Our
analysis covers all major classes of stationary kernel functions.

</details>

### [847] [Model Selection for Gaussian-gated Gaussian Mixture of Experts Using Dendrograms of Mixing Measures](https://arxiv.org/abs/2505.13052)
*Tuan Thai,TrungTin Nguyen,Dat Do,Nhat Ho,Christopher Drovandi*

Main category: stat.ML

TLDR: 本文提出了一种基于高斯门控高斯混合专家模型的新方法，通过扩展混合测量的树状图概念，实现了对真实混合成分数量的准确估计，并避免了训练和比较多个模型的负担。


<details>
  <summary>Details</summary>
Motivation: 尽管混合专家模型在实践中广泛应用，但其理论理解仍有限，尤其是在模型选择和最优混合成分数量方面存在挑战。本文旨在解决这一问题。

Method: 引入了一种扩展的高斯门控高斯混合专家模型，利用混合测量的树状图概念，实现了对混合成分数量的准确估计和参数估计的最优收敛速率。

Result: 实验结果表明，该方法在合成数据上能准确恢复专家数量，优于常见准则（如AIC、BIC、ICL），并实现了参数估计的最优收敛速率。

Conclusion: 该方法不仅解决了模型选择问题，还显著降低了计算负担，特别适用于高维或深度神经网络场景。

Abstract: Mixture of Experts (MoE) models constitute a widely utilized class of
ensemble learning approaches in statistics and machine learning, known for
their flexibility and computational efficiency. They have become integral
components in numerous state-of-the-art deep neural network architectures,
particularly for analyzing heterogeneous data across diverse domains. Despite
their practical success, the theoretical understanding of model selection,
especially concerning the optimal number of mixture components or experts,
remains limited and poses significant challenges. These challenges primarily
stem from the inclusion of covariates in both the Gaussian gating functions and
expert networks, which introduces intrinsic interactions governed by partial
differential equations with respect to their parameters. In this paper, we
revisit the concept of dendrograms of mixing measures and introduce a novel
extension to Gaussian-gated Gaussian MoE models that enables consistent
estimation of the true number of mixture components and achieves the pointwise
optimal convergence rate for parameter estimation in overfitted scenarios.
Notably, this approach circumvents the need to train and compare a range of
models with varying numbers of components, thereby alleviating the
computational burden, particularly in high-dimensional or deep neural network
settings. Experimental results on synthetic data demonstrate the effectiveness
of the proposed method in accurately recovering the number of experts. It
outperforms common criteria such as the Akaike information criterion, the
Bayesian information criterion, and the integrated completed likelihood, while
achieving optimal convergence rates for parameter estimation and accurately
approximating the regression function.

</details>

### [848] [Attention-based clustering](https://arxiv.org/abs/2505.13112)
*Rodrigo Maulen-Soto,Claire Boyer,Pierre Marion*

Main category: stat.ML

TLDR: 论文分析了Transformer在无监督设置下从数据中自动提取结构的能力，特别是其在高斯混合模型数据上的聚类适用性。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer作为一种强大神经网络架构在无监督学习中的理论能力。

Method: 通过简化双头注意力层，并定义一种基于未标记数据的群体风险最小化方法。

Result: 证明了注意力头参数会与真实混合中心对齐。

Conclusion: Transformer在高斯混合模型数据上具有聚类潜力。

Abstract: Transformers have emerged as a powerful neural network architecture capable
of tackling a wide range of learning tasks. In this work, we provide a
theoretical analysis of their ability to automatically extract structure from
data in an unsupervised setting. In particular, we demonstrate their
suitability for clustering when the input data is generated from a Gaussian
mixture model. To this end, we study a simplified two-head attention layer and
define a population risk whose minimization with unlabeled data drives the head
parameters to align with the true mixture centroids.

</details>

### [849] [Diffusion Models with Double Guidance: Generate with aggregated datasets](https://arxiv.org/abs/2505.13213)
*Yanfeng Yang,Kenji Fukumizu*

Main category: stat.ML

TLDR: 提出了一种名为Diffusion Model with Double Guidance的新方法，解决了多数据集合并时条件缺失的问题，提升了生成模型的精确性和可控性。


<details>
  <summary>Details</summary>
Motivation: 由于构建大规模标注数据集成本高昂，合并现有数据集成为常见策略，但条件不一致导致生成模型难以精确控制。

Method: 采用Diffusion Model with Double Guidance方法，无需联合标注即可在多条件下实现精确生成。

Result: 在分子和图像生成任务中表现优异，优于现有基线，尤其在条件缺失情况下仍保持高可控性。

Conclusion: 该方法为条件生成模型提供了一种高效解决方案，显著提升了模型在复杂条件下的适用性。

Abstract: Creating large-scale datasets for training high-performance generative models
is often prohibitively expensive, especially when associated attributes or
annotations must be provided. As a result, merging existing datasets has become
a common strategy. However, the sets of attributes across datasets are often
inconsistent, and their naive concatenation typically leads to block-wise
missing conditions. This presents a significant challenge for conditional
generative modeling when the multiple attributes are used jointly as
conditions, thereby limiting the model's controllability and applicability. To
address this issue, we propose a novel generative approach, Diffusion Model
with Double Guidance, which enables precise conditional generation even when no
training samples contain all conditions simultaneously. Our method maintains
rigorous control over multiple conditions without requiring joint annotations.
We demonstrate its effectiveness in molecular and image generation tasks, where
it outperforms existing baselines both in alignment with target conditional
distributions and in controllability under missing condition settings.

</details>

### [850] [From What Ifs to Insights: Counterfactuals in Causal Inference vs. Explainable AI](https://arxiv.org/abs/2505.13324)
*Galit Shmueli,David Martens,Jaewon Yoo,Travis Greene*

Main category: stat.ML

TLDR: 论文探讨了反事实在因果推理（CI）和可解释人工智能（XAI）中的核心作用，提出了一个统一的定义，并比较了两者在应用和解释上的差异。


<details>
  <summary>Details</summary>
Motivation: 反事实在CI和XAI中均扮演重要角色，但两者的使用和解释方式存在差异，研究旨在明确这些差异并探索跨领域合作的可能性。

Method: 通过引入一个正式定义，分析反事实在CI和XAI中的生成、评估和操作化方式，并进行对比。

Result: 揭示了CI和XAI在反事实应用上的概念和实践差异，提出了跨领域合作的潜在机会。

Conclusion: 通过比较CI和XAI中的反事实，研究为两领域的交叉融合提供了理论基础和实践方向。

Abstract: Counterfactuals play a pivotal role in the two distinct data science fields
of causal inference (CI) and explainable artificial intelligence (XAI). While
the core idea behind counterfactuals remains the same in both fields--the
examination of what would have happened under different circumstances--there
are key differences in how they are used and interpreted. We introduce a formal
definition that encompasses the multi-faceted concept of the counterfactual in
CI and XAI. We then discuss how counterfactuals are used, evaluated, generated,
and operationalized in CI vs. XAI, highlighting conceptual and practical
differences. By comparing and contrasting the two, we hope to identify
opportunities for cross-fertilization across CI and XAI.

</details>

### [851] [Conformalized Decision Risk Assessment](https://arxiv.org/abs/2505.13243)
*Wenbin Zhou,Agni Orfanoudaki,Shixiang Zhu*

Main category: stat.ML

TLDR: CREDO框架通过结合逆优化几何、共形预测和生成建模，为决策提供分布无关的次优概率上界，增强决策的稳健性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 高风险决策领域（如医疗、能源、公共政策）依赖专家经验与预测工具，但现有方法缺乏可解释性且在分布不确定性下表现不佳。

Method: 结合逆优化几何、共形预测和生成建模，量化决策的次优概率上界。

Result: CREDO生成统计严谨且可解释的风险证书，支持决策者在不确定性下审核和验证决策。

Conclusion: CREDO填补了算法工具与现实决策之间的鸿沟，提升了决策的稳健性和透明度。

Abstract: High-stakes decisions in domains such as healthcare, energy, and public
policy are often made by human experts using domain knowledge and heuristics,
yet are increasingly supported by predictive and optimization-based tools. A
dominant approach in operations research is the predict-then-optimize paradigm,
where a predictive model estimates uncertain inputs, and an optimization model
recommends a decision. However, this approach often lacks interpretability and
can fail under distributional uncertainty -- particularly when the outcome
distribution is multi-modal or complex -- leading to brittle or misleading
decisions. In this paper, we introduce CREDO, a novel framework that
quantifies, for any candidate decision, a distribution-free upper bound on the
probability that the decision is suboptimal. By combining inverse optimization
geometry with conformal prediction and generative modeling, CREDO produces risk
certificates that are both statistically rigorous and practically
interpretable. This framework enables human decision-makers to audit and
validate their own decisions under uncertainty, bridging the gap between
algorithmic tools and real-world judgment.

</details>

### [852] [Smoothed SGD for quantiles: Bahadur representation and Gaussian approximation](https://arxiv.org/abs/2505.13299)
*Likai Chen,Georg Keilbar,Wei Biao Wu*

Main category: stat.ML

TLDR: 提出了一种基于平滑随机梯度下降（SGD）的分位数估计方法，解决了传统SGD分位数算法中分位数曲线交叉的问题，并提供了理论保证和数值验证。


<details>
  <summary>Details</summary>
Motivation: 传统SGD分位数算法中分位数曲线可能交叉，缺乏单调性，影响了估计的可靠性。本文通过平滑方法解决这一问题。

Method: 通过平滑传统SGD分位数算法中的评分函数，确保分位数曲线的单调性，并推导了非渐近尾部概率界限。

Result: 理论分析表明，平滑后的SGD分位数估计具有非渐近尾部概率界限，数值实验验证了其良好的有限样本性能。

Conclusion: 平滑SGD分位数估计方法有效解决了分位数曲线交叉问题，理论结果和数值实验均支持其优越性。

Abstract: This paper considers the estimation of quantiles via a smoothed version of
the stochastic gradient descent (SGD) algorithm. By smoothing the score
function in the conventional SGD quantile algorithm, we achieve monotonicity in
the quantile level in that the estimated quantile curves do not cross. We
derive non-asymptotic tail probability bounds for the smoothed SGD quantile
estimate both for the case with and without Polyak-Ruppert averaging. For the
latter, we also provide a uniform Bahadur representation and a resulting
Gaussian approximation result. Numerical studies show good finite sample
behavior for our theoretical results.

</details>

### [853] [Minimum-Excess-Work Guidance](https://arxiv.org/abs/2505.13375)
*Christopher Kolloff,Tobias Höppe,Emmanouil Angelis,Mathias Jacob Schreiner,Stefan Bauer,Andrea Dittadi,Simon Olsson*

Main category: stat.ML

TLDR: 提出了一种基于热力学工作的正则化框架，用于指导预训练的概率流生成模型，通过最小化超额工作实现高效的数据稀疏场景下的生成。


<details>
  <summary>Details</summary>
Motivation: 解决科学应用中数据稀疏（如有限目标样本或部分密度约束）的问题，将热力学原理与现代生成架构结合。

Method: 引入两种策略：路径引导（Path Guidance）用于采样稀有过渡态，可观测引导（Observable Guidance）用于对齐生成分布与实验观测值。

Result: 在粗粒度蛋白质模型上验证了方法的有效性，成功采样折叠/非折叠态过渡构型，并利用实验数据纠正系统偏差。

Conclusion: 该方法为数据稀缺领域提供了一种基于物理原理的高效替代方案，显著提升了样本效率和偏差减少。

Abstract: We propose a regularization framework inspired by thermodynamic work for
guiding pre-trained probability flow generative models (e.g., continuous
normalizing flows or diffusion models) by minimizing excess work, a concept
rooted in statistical mechanics and with strong conceptual connections to
optimal transport. Our approach enables efficient guidance in sparse-data
regimes common to scientific applications, where only limited target samples or
partial density constraints are available. We introduce two strategies: Path
Guidance for sampling rare transition states by concentrating probability mass
on user-defined subsets, and Observable Guidance for aligning generated
distributions with experimental observables while preserving entropy. We
demonstrate the framework's versatility on a coarse-grained protein model,
guiding it to sample transition configurations between folded/unfolded states
and correct systematic biases using experimental data. The method bridges
thermodynamic principles with modern generative architectures, offering a
principled, efficient, and physics-inspired alternative to standard fine-tuning
in data-scarce domains. Empirical results highlight improved sample efficiency
and bias reduction, underscoring its applicability to molecular simulations and
beyond.

</details>

<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [854] [Learning Probabilistic Temporal Logic Specifications for Stochastic Systems](https://arxiv.org/abs/2505.12107)
*Rajarshi Roy,Yash Pote,David Parker,Marta Kwiatkowska*

Main category: cs.LO

TLDR: 论文提出了一种从马尔可夫链中推断概率线性时序逻辑（PLTL）公式的新算法，适用于随机行为的系统。


<details>
  <summary>Details</summary>
Motivation: 现有技术无法处理随机行为的系统，而这类系统在强化学习和形式验证中很常见。

Method: 结合语法枚举、搜索启发式、概率模型检查和布尔集合覆盖程序，提出了一种学习算法。

Result: 算法在强化学习策略和概率模型变体中有效提取了简洁的PLTL规范。

Conclusion: 该方法能自动高效地提取PLTL规范，适用于随机行为的系统。

Abstract: There has been substantial progress in the inference of formal behavioural
specifications from sample trajectories, for example, using Linear Temporal
Logic (LTL). However, these techniques cannot handle specifications that
correctly characterise systems with stochastic behaviour, which occur commonly
in reinforcement learning and formal verification. We consider the passive
learning problem of inferring a Boolean combination of probabilistic LTL (PLTL)
formulas from a set of Markov chains, classified as either positive or
negative. We propose a novel learning algorithm that infers concise PLTL
specifications, leveraging grammar-based enumeration, search heuristics,
probabilistic model checking and Boolean set-cover procedures. We demonstrate
the effectiveness of our algorithm in two use cases: learning from policies
induced by RL algorithms and learning from variants of a probabilistic model.
In both cases, our method automatically and efficiently extracts PLTL
specifications that succinctly characterise the temporal differences between
the policies or model variants.

</details>

### [855] [Information Science Principles of Machine Learning: A Causal Chain Meta-Framework Based on Formalized Information Mapping](https://arxiv.org/abs/2505.13182)
*Jianfeng Xu*

Main category: cs.LO

TLDR: 该研究提出了一种统一的机器学习理论框架（MLT-MF），解决了当前机器学习中缺乏统一理论框架、可解释性和伦理安全性的问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习领域缺乏统一的形式化理论框架，且存在可解释性和伦理安全性不足的问题。

Method: 构建了一个形式化信息模型，使用良构公式集定义机器学习组件的本体状态和载体映射，并引入可学习和处理的谓词及函数来分析模型的逻辑推理和约束规则。

Result: 建立了MLT-MF框架，提出了模型可解释性和伦理安全性的通用定义，并证明了三个关键定理。

Conclusion: 该研究克服了碎片化研究的局限性，为系统解决机器学习中的关键挑战提供了统一的理论基础。

Abstract: [Objective] This study focuses on addressing the current lack of a unified
formal theoretical framework in machine learning, as well as the deficiencies
in interpretability and ethical safety assurance. [Methods] A formal
information model is first constructed, utilizing sets of well-formed formulas
to explicitly define the ontological states and carrier mappings of typical
components in machine learning. Learnable and processable predicates, along
with learning and processing functions, are introduced to analyze the logical
deduction and constraint rules of the causal chains within models. [Results] A
meta-framework for machine learning theory (MLT-MF) is established. Based on
this framework, universal definitions for model interpretability and ethical
safety are proposed. Furthermore, three key theorems are proved: the
equivalence of model interpretability and information recoverability, the
assurance of ethical safety, and the estimation of generalization error.
[Limitations] The current framework assumes ideal conditions with noiseless
information-enabling mappings and primarily targets model learning and
processing logic in static scenarios. It does not yet address information
fusion and conflict resolution across ontological spaces in multimodal or
multi-agent systems. [Conclusions] This work overcomes the limitations of
fragmented research and provides a unified theoretical foundation for
systematically addressing the critical challenges currently faced in machine
learning.

</details>

<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [856] [Fine-Grained ECG-Text Contrastive Learning via Waveform Understanding Enhancement](https://arxiv.org/abs/2505.11939)
*Haitao Li,Che Liu,Zhengyao Ding,Ziyi Liu,Zhengxing Huang*

Main category: eess.SP

TLDR: FG-CLEP通过恢复心电图中未记录的波形特征，利用大语言模型和对比学习，提升了心电图诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有ECG-文本对比学习方法因报告不完整而难以捕捉波形特征和诊断推理，限制了模型性能。

Method: 提出FG-CLEP，利用大语言模型恢复波形特征，引入语义相似矩阵和基于sigmoid的损失函数。

Result: 在六个数据集上，FG-CLEP在零样本预测和线性探测中均优于现有方法。

Conclusion: FG-CLEP通过解决报告不完整问题，显著提升了心电图诊断模型的性能。

Abstract: Electrocardiograms (ECGs) are essential for diagnosing cardiovascular
diseases. While previous ECG-text contrastive learning methods have shown
promising results, they often overlook the incompleteness of the reports. Given
an ECG, the report is generated by first identifying key waveform features and
then inferring the final diagnosis through these features. Despite their
importance, these waveform features are often not recorded in the report as
intermediate results. Aligning ECGs with such incomplete reports impedes the
model's ability to capture the ECG's waveform features and limits its
understanding of diagnostic reasoning based on those features. To address this,
we propose FG-CLEP (Fine-Grained Contrastive Language ECG Pre-training), which
aims to recover these waveform features from incomplete reports with the help
of large language models (LLMs), under the challenges of hallucinations and the
non-bijective relationship between waveform features and diagnoses.
Additionally, considering the frequent false negatives due to the prevalence of
common diagnoses in ECGs, we introduce a semantic similarity matrix to guide
contrastive learning. Furthermore, we adopt a sigmoid-based loss function to
accommodate the multi-label nature of ECG-related tasks. Experiments on six
datasets demonstrate that FG-CLEP outperforms state-of-the-art methods in both
zero-shot prediction and linear probing across these datasets.

</details>

### [857] [S-Crescendo: A Nested Transformer Weaving Framework for Scalable Nonlinear System in S-Domain Representation](https://arxiv.org/abs/2505.11843)
*Junlang Huang,Hao Chen,Li Luo,Yong Cai,Lexin Zhang,Tianhao Ma,Yitian Zhang,Zhong Guan*

Main category: eess.SP

TLDR: S-Crescendo是一种结合S域和神经算子的嵌套变压器框架，用于高效预测高阶非线性网络的时域行为，将计算复杂度从立方降低到线性。


<details>
  <summary>Details</summary>
Motivation: 现代VLSI后端设计中，高阶非线性系统的仿真计算资源需求大，且分岔诱导的不稳定性和混沌行为带来挑战。

Method: 通过将n阶传递函数分解为具有重复极点和留数的一阶模态项，避免传统Jacobian矩阵迭代，结合S域编码器和注意力校正算子。

Result: 在1至10阶网络上验证，达到0.99的测试集精度（R²），仿真速度提升18倍。

Conclusion: S-Crescendo为高维非线性建模提供了可扩展且物理感知的框架。

Abstract: Simulation of high-order nonlinear system requires extensive computational
resources, especially in modern VLSI backend design where bifurcation-induced
instability and chaos-like transient behaviors pose challenges. We present
S-Crescendo - a nested transformer weaving framework that synergizes S-domain
with neural operators for scalable time-domain prediction in high-order
nonlinear networks, alleviating the computational bottlenecks of conventional
solvers via Newton-Raphson method. By leveraging the partial-fraction
decomposition of an n-th order transfer function into first-order modal terms
with repeated poles and residues, our method bypasses the conventional Jacobian
matrix-based iterations and efficiently reduces computational complexity from
cubic $O(n^3)$ to linear $O(n)$.The proposed architecture seamlessly integrates
an S-domain encoder with an attention-based correction operator to
simultaneously isolate dominant response and adaptively capture higher-order
non-linearities. Validated on order-1 to order-10 networks, our method achieves
up to 0.99 test-set ($R^2$) accuracy against HSPICE golden waveforms and
accelerates simulation by up to 18(X), providing a scalable, physics-aware
framework for high-dimensional nonlinear modeling.

</details>

### [858] [Multi-View Wireless Sensing via Conditional Generative Learning: Framework and Model Design](https://arxiv.org/abs/2505.12664)
*Ziqing Xing,Zhaoyang Zhang,Zirui Chen,Hongning Ruan,Zhaohui Yang*

Main category: eess.SP

TLDR: 论文提出了一种结合物理知识的生成式多视角感知框架，通过多基站与用户设备间的信道状态信息（CSI）实现高精度目标感知。


<details>
  <summary>Details</summary>
Motivation: 利用多视角CSI数据提升目标感知的精度和灵活性，同时融入电磁波传播的物理知识。

Method: 设计了一个双向神经网络架构，包括一个编码器用于融合多视角CSI的潜在特征，以及一个条件扩散模型用于生成目标的点云。编码器引入了空间位置嵌入方案以捕捉CSI的物理相关性。

Result: 实验结果表明，提出的Gen-MV框架在目标形状和电磁特性重建方面表现出优异的灵活性和显著性能提升。

Conclusion: 该框架为多视角目标感知提供了一种高效且灵活的方法，同时结合了物理知识与深度学习。

Abstract: In this paper, we incorporate physical knowledge into learning-based
high-precision target sensing using the multi-view channel state information
(CSI) between multiple base stations (BSs) and user equipment (UEs). Such kind
of multi-view sensing problem can be naturally cast into a conditional
generation framework. To this end, we design a bipartite neural network
architecture, the first part of which uses an elaborately designed encoder to
fuse the latent target features embedded in the multi-view CSI, and then the
second uses them as conditioning inputs of a powerful generative model to guide
the target's reconstruction. Specifically, the encoder is designed to capture
the physical correlation between the CSI and the target, and also be adaptive
to the numbers and positions of BS-UE pairs. Therein the view-specific nature
of CSI is assimilated by introducing a spatial positional embedding scheme,
which exploits the structure of electromagnetic(EM)-wave propagation channels.
Finally, a conditional diffusion model with a weighted loss is employed to
generate the target's point cloud from the fused features. Extensive numerical
results demonstrate that the proposed generative multi-view (Gen-MV) sensing
framework exhibits excellent flexibility and significant performance
improvement on the reconstruction quality of target's shape and EM properties.

</details>

### [859] [The role of data partitioning on the performance of EEG-based deep learning models in supervised cross-subject analysis: a preliminary study](https://arxiv.org/abs/2505.13021)
*Federico Del Pup,Andrea Zanola,Louis Fabrice Tshimanga,Alessandra Bertoldo,Livio Finos,Manfredo Atzori*

Main category: eess.SP

TLDR: 该论文研究了数据分区和交叉验证在EEG深度学习模型评估中的重要性，提出了避免数据泄漏的指南，并比较了五种交叉验证设置。


<details>
  <summary>Details</summary>
Motivation: 由于EEG信号的特殊性（如生物特征），数据分区和交叉验证可能导致结果不一致和数据泄漏，进而影响研究的可比性和性能评估的准确性。目前缺乏相关领域的全面指南和定量评估。

Method: 论文比较了五种交叉验证设置，应用于三个跨被试分类任务（BCI、帕金森病和阿尔茨海默病检测）和四种复杂度递增的架构（ShallowConvNet、EEGNet、DeepConvNet和Temporal-based ResNet），训练了超过10万个模型。

Result: 研究发现，基于被试的交叉验证策略对EEG深度学习模型评估至关重要（除非是BCI等允许被试内分析的情况）。嵌套方法（N-LNSO）比非嵌套方法更可靠，后者容易导致数据泄漏和模型过拟合。

Conclusion: 论文为EEG深度学习研究者提供了数据分区和交叉验证的分析，并提出了避免数据泄漏的指南，以防止性能评估的夸大。

Abstract: Deep learning is significantly advancing the analysis of
electroencephalography (EEG) data by effectively discovering highly nonlinear
patterns within the signals. Data partitioning and cross-validation are crucial
for assessing model performance and ensuring study comparability, as they can
produce varied results and data leakage due to specific signal properties
(e.g., biometric). Such variability leads to incomparable studies and,
increasingly, overestimated performance claims, which are detrimental to the
field. Nevertheless, no comprehensive guidelines for proper data partitioning
and cross-validation exist in the domain, nor is there a quantitative
evaluation of their impact on model accuracy, reliability, and
generalizability. To assist researchers in identifying optimal experimental
strategies, this paper thoroughly investigates the role of data partitioning
and cross-validation in evaluating EEG deep learning models. Five
cross-validation settings are compared across three supervised cross-subject
classification tasks (BCI, Parkinson's, and Alzheimer's disease detection) and
four established architectures of increasing complexity (ShallowConvNet,
EEGNet, DeepConvNet, and Temporal-based ResNet). The comparison of over 100,000
trained models underscores, first, the importance of using subject-based
cross-validation strategies for evaluating EEG deep learning models, except
when within-subject analyses are acceptable (e.g., BCI). Second, it highlights
the greater reliability of nested approaches (N-LNSO) compared to non-nested
counterparts, which are prone to data leakage and favor larger models
overfitting to validation data. In conclusion, this work provides EEG deep
learning researchers with an analysis of data partitioning and cross-validation
and offers guidelines to avoid data leakage, currently undermining the domain
with potentially overestimated performance claims.

</details>

### [860] [Simplicity is Key: An Unsupervised Pretraining Approach for Sparse Radio Channels](https://arxiv.org/abs/2505.13055)
*Jonathan Ott,Maximilian Stahlke,Tobias Feigl,Bjoern M. Eskofier,Christopher Mutschler*

Main category: eess.SP

TLDR: SpaRTran是一种基于压缩感知的无监督表示学习方法，专注于无线电传播的物理特性，显著降低了下游任务的误差。


<details>
  <summary>Details</summary>
Motivation: 当前无线电信号处理任务需要大量标注数据，且现有方法泛化能力不足，因此提出SpaRTran以减少标注成本并提高灵活性。

Method: 使用稀疏门控自编码器学习稀疏表示，并通过字典学习原子特征以增强信号重建能力。

Result: 实验表明，SpaRTran在无线电指纹任务中误差降低85%，且预训练需求更少。

Conclusion: SpaRTran是一种高效、灵活的基模型，适用于多种无线电下游任务，显著提升泛化能力。

Abstract: We introduce the Sparse pretrained Radio Transformer (SpaRTran), an
unsupervised representation learning approach based on the concept of
compressed sensing for radio channels. Our approach learns embeddings that
focus on the physical properties of radio propagation, to create the optimal
basis for fine-tuning on radio-based downstream tasks. SpaRTran uses a sparse
gated autoencoder that induces a simplicity bias to the learned
representations, resembling the sparse nature of radio propagation. For signal
reconstruction, it learns a dictionary that holds atomic features, which
increases flexibility across signal waveforms and spatiotemporal signal
patterns. Our experiments show that SpaRTran reduces errors by up to 85 %
compared to state-of-the-art methods when fine-tuned on radio fingerprinting, a
challenging downstream task. In addition, our method requires less pretraining
effort and offers greater flexibility, as we train it solely on individual
radio signals. SpaRTran serves as an excellent base model that can be
fine-tuned for various radio-based downstream tasks, effectively reducing the
cost for labeling. In addition, it is significantly more versatile than
existing methods and demonstrates superior generalization.

</details>

<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [861] [LAMeTA: Intent-Aware Agentic Network Optimization via a Large AI Model-Empowered Two-Stage Approach](https://arxiv.org/abs/2505.12247)
*Yinqiu Liu,Guangyuan Liu,Jiacheng Wang,Ruichen Zhang,Dusit Niyato,Geng Sun,Zehui Xiong,Zhu Han*

Main category: cs.NI

TLDR: LAMeTA提出了一种基于大型AI模型的两阶段方法，用于意图感知的代理网络优化，通过意图导向知识蒸馏和共生强化学习，显著提升了意图预测和服务质量。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习难以动态捕捉用户意图语义，导致代理网络优化效果不佳。

Method: 采用意图导向知识蒸馏（IoKD）将意图理解能力从资源密集型LAM迁移到轻量级边缘LAM（E-LAM），并结合共生强化学习（SRL）将用户意图转化为结构化偏好向量以指导网络优化。

Result: 实验表明，IoKD将意图预测的均方误差降低22.5%，SRL在意图感知QoE优化上比传统方法提升23.5%。

Conclusion: LAMeTA通过结合意图理解和动态策略调整，显著提升了代理网络的优化效果。

Abstract: Nowadays, Generative AI (GenAI) reshapes numerous domains by enabling
machines to create content across modalities. As GenAI evolves into autonomous
agents capable of reasoning, collaboration, and interaction, they are
increasingly deployed on network infrastructures to serve humans automatically.
This emerging paradigm, known as the agentic network, presents new optimization
challenges due to the demand to incorporate subjective intents of human users
expressed in natural language. Traditional generic Deep Reinforcement Learning
(DRL) struggles to capture intent semantics and adjust policies dynamically,
thus leading to suboptimality. In this paper, we present LAMeTA, a Large AI
Model (LAM)-empowered Two-stage Approach for intent-aware agentic network
optimization. First, we propose Intent-oriented Knowledge Distillation (IoKD),
which efficiently distills intent-understanding capabilities from
resource-intensive LAMs to lightweight edge LAMs (E-LAMs) to serve end users.
Second, we develop Symbiotic Reinforcement Learning (SRL), integrating E-LAMs
with a policy-based DRL framework. In SRL, E-LAMs translate natural language
user intents into structured preference vectors that guide both state
representation and reward design. The DRL, in turn, optimizes the generative
service function chain composition and E-LAM selection based on real-time
network conditions, thus optimizing the subjective Quality-of-Experience (QoE).
Extensive experiments conducted in an agentic network with 81 agents
demonstrate that IoKD reduces mean squared error in intent prediction by up to
22.5%, while SRL outperforms conventional generic DRL by up to 23.5% in
maximizing intent-aware QoE.

</details>

### [862] [Unleashing Automated Congestion Control Customization in the Wild](https://arxiv.org/abs/2505.12492)
*Amit Cohen,Lev Gloukhenki,Ravid Hadar,Eden Itah,Yehuda Shvut,Michael Schapira*

Main category: cs.NI

TLDR: 论文探讨了自动定制拥塞控制逻辑的系统，以适应不同服务和网络条件，并分享了PCC Vivace协议的实际部署经验。


<details>
  <summary>Details</summary>
Motivation: 传统拥塞控制算法难以满足多样化服务和网络条件的需求，因此需要定制化解决方案。

Method: 利用基于在线学习的PCC Vivace协议，自动定制拥塞控制逻辑。

Result: 通过案例研究展示了在流媒体、游戏、联网汽车等领域的性能优势。

Conclusion: 定制化拥塞控制逻辑能显著提升性能，同时PCC Vivace的适应性为实际部署提供了重要经验。

Abstract: Congestion control (CC) crucially impacts user experience across Internet
services like streaming, gaming, AR/VR, and connected cars. Traditionally, CC
algorithm design seeks universal control rules that yield high performance
across diverse application domains and networks. However, varying service needs
and network conditions challenge this approach. We share operational experience
with a system that automatically customizes congestion control logic to service
needs and network conditions. We discuss design, deployment challenges, and
solutions, highlighting performance benefits through case studies in streaming,
gaming, connected cars, and more.
  Our system leverages PCC Vivace, an online-learning based congestion control
protocol developed by researchers. Hence, along with insights from customizing
congestion control, we also discuss lessons learned and modifications made to
adapt PCC Vivace for real-world deployment.

</details>

### [863] [Towards Sustainability in 6G Network Slicing with Energy-Saving and Optimization Methods](https://arxiv.org/abs/2505.12132)
*Rodrigo Moreira,Tereza C. M. Carvalho,Flávio de Oliveira Silva,Nazim Agoulmine,Joberto S. B. Martins*

Main category: cs.NI

TLDR: 本文探讨了在6G/5G网络切片架构中嵌入节能方法，提出通过ML-native代理动态优化资源分配以实现节能。


<details>
  <summary>Details</summary>
Motivation: 6G网络预计将带来移动流量的激增，节能成为电信行业的重要课题，而网络切片作为关键使能技术，其节能方法尚存研究空白。

Method: 提出在SFI2网络切片参考架构中部署ML-native代理，利用对比学习动态优化资源分配。

Result: 通过ML-native代理和对比学习，显著提升了网络切片架构的节能效果。

Conclusion: 该方法为6G/5G网络切片提供了有效的节能解决方案，适用于IoT、IoV等新兴系统。

Abstract: The 6G mobile network is the next evolutionary step after 5G, with a
prediction of an explosive surge in mobile traffic. It provides ultra-low
latency, higher data rates, high device density, and ubiquitous coverage,
positively impacting services in various areas. Energy saving is a major
concern for new systems in the telecommunications sector because all players
are expected to reduce their carbon footprints to contribute to mitigating
climate change. Network slicing is a fundamental enabler for 6G/5G mobile
networks and various other new systems, such as the Internet of Things (IoT),
Internet of Vehicles (IoV), and Industrial IoT (IIoT). However, energy-saving
methods embedded in network slicing architectures are still a research gap.
This paper discusses how to embed energy-saving methods in network-slicing
architectures that are a fundamental enabler for nearly all new innovative
systems being deployed worldwide. This paper's main contribution is a proposal
to save energy in network slicing. That is achieved by deploying ML-native
agents in NS architectures to dynamically orchestrate and optimize resources
based on user demands. The SFI2 network slicing reference architecture is the
concrete use case scenario in which contrastive learning improves energy saving
for resource allocation.

</details>

<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [864] [Explainable Machine Learning for Oxygen Diffusion in Perovskites and Pyrochlores](https://arxiv.org/abs/2505.11722)
*Grace M. Lu,Dallas R. Trinkle*

Main category: cond-mat.mtrl-sci

TLDR: 论文通过可解释机器学习方法，发现了影响钙钛矿和烧绿石中氧扩散活化能的关键材料特性，并确定了快速筛选新材料的方法。


<details>
  <summary>Details</summary>
Motivation: 理解钙钛矿和烧绿石中氧扩散活化能的关键材料特性，以加速新材料的发现。

Method: 构建实验活化能数据库，应用分组算法处理材料特性特征，拟合七种机器学习模型，通过集成共识确定关键特征。

Result: 钙钛矿中最关键的特征是A位键的离子性和氧分压；烧绿石中最关键的特征是A位s价电子数和B位电负性。这些特征均基于金属元素的加权平均值。

Conclusion: 研究确定了易于测量的关键特征，可用于快速筛选具有高氧离子扩散速率的新材料。

Abstract: Explainable machine learning can help to discover new physical relationships
for material properties. To understand the material properties that govern the
activation energy for oxygen diffusion in perovskites and pyrochlores, we build
a database of experimental activation energies and apply a grouping algorithm
to the material property features. These features are then used to fit seven
different machine learning models. An ensemble consensus determines that the
most important features for predicting the activation energy are the ionicity
of the A-site bond and the partial pressure of oxygen for perovskites. For
pyrochlores, the two most important features are the A-site $s$ valence
electron count and the B-site electronegativity. The most important features
are all constructed using the weighted averages of elemental metal properties,
despite weighted averages of the constituent binary oxides being included in
our feature set. This is surprising because the material properties of the
constituent oxides are more similar to the experimentally measured properties
of perovskites and pyrochlores than the features of the metals that are chosen.
The easy-to-measure features identified in this work enable rapid screening for
new materials with fast oxide-ion diffusivity.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [865] [Revisiting Adversarial Perception Attacks and Defense Methods on Autonomous Driving Systems](https://arxiv.org/abs/2505.11532)
*Cheng Chen,Yuhong Wang,Nafis S Munir,Xiangwei Zhou,Xugui Zhou*

Main category: cs.RO

TLDR: 本文研究了自动驾驶系统中基于深度学习的感知模型对对抗攻击的脆弱性，重点分析了路标识别和前方物体检测的对抗攻击与防御方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统依赖的深度学习感知模型易受对抗攻击，需研究其脆弱性并提出有效防御策略。

Method: 使用OpenPilot和YOLO模型，系统评估对抗扰动的影响，并测试对抗训练、图像处理、对比学习和扩散模型等防御技术。

Result: 实验揭示了这些防御方法在应对复杂攻击时的优势和局限性。

Conclusion: 研究为理解自动驾驶感知系统的脆弱性提供了深入见解，并为开发更鲁棒的防御策略提供了指导。

Abstract: Autonomous driving systems (ADS) increasingly rely on deep learning-based
perception models, which remain vulnerable to adversarial attacks. In this
paper, we revisit adversarial attacks and defense methods, focusing on road
sign recognition and lead object detection and prediction (e.g., relative
distance). Using a Level-2 production ADS, OpenPilot by Comma.ai, and the
widely adopted YOLO model, we systematically examine the impact of adversarial
perturbations and assess defense techniques, including adversarial training,
image processing, contrastive learning, and diffusion models. Our experiments
highlight both the strengths and limitations of these methods in mitigating
complex attacks. Through targeted evaluations of model robustness, we aim to
provide deeper insights into the vulnerabilities of ADS perception systems and
contribute guidance for developing more resilient defense strategies.

</details>

### [866] [LaDi-WM: A Latent Diffusion-based World Model for Predictive Manipulation](https://arxiv.org/abs/2505.11528)
*Yuhang Huang,JIazhao Zhang,Shilong Zou,XInwang Liu,Ruizhen Hu,Kai Xu*

Main category: cs.RO

TLDR: LaDi-WM是一种基于扩散模型的世界模型，通过预测潜在空间状态提升机器人策略性能，实验显示显著效果。


<details>
  <summary>Details</summary>
Motivation: 预测机器人-物体交互的未来视觉状态是Embodied AI中的挑战，现有方法在像素级表示上表现不佳。

Method: 利用预训练视觉基础模型（VFMs）的潜在空间（DINO和CLIP特征），通过扩散模型预测状态演化，并设计扩散策略优化动作。

Result: 在LIBERO-LONG和真实场景中分别提升27.9%和20%的性能，且表现出优秀的泛化能力。

Conclusion: LaDi-WM通过潜在空间预测和扩散策略显著提升了机器人策略的准确性和一致性。

Abstract: Predictive manipulation has recently gained considerable attention in the
Embodied AI community due to its potential to improve robot policy performance
by leveraging predicted states. However, generating accurate future visual
states of robot-object interactions from world models remains a well-known
challenge, particularly in achieving high-quality pixel-level representations.
To this end, we propose LaDi-WM, a world model that predicts the latent space
of future states using diffusion modeling. Specifically, LaDi-WM leverages the
well-established latent space aligned with pre-trained Visual Foundation Models
(VFMs), which comprises both geometric features (DINO-based) and semantic
features (CLIP-based). We find that predicting the evolution of the latent
space is easier to learn and more generalizable than directly predicting
pixel-level images. Building on LaDi-WM, we design a diffusion policy that
iteratively refines output actions by incorporating forecasted states, thereby
generating more consistent and accurate results. Extensive experiments on both
synthetic and real-world benchmarks demonstrate that LaDi-WM significantly
enhances policy performance by 27.9\% on the LIBERO-LONG benchmark and 20\% on
the real-world scenario. Furthermore, our world model and policies achieve
impressive generalizability in real-world experiments.

</details>

### [867] [Object-Centric Representations Improve Policy Generalization in Robot Manipulation](https://arxiv.org/abs/2505.11563)
*Alexandre Chapin,Bruno Machado,Emmanuel Dellandrea,Liming Chen*

Main category: cs.RO

TLDR: 论文探讨了对象中心表示（OCR）在机器人操作任务中的优势，相比全局或密集特征，OCR在泛化能力上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖全局或密集特征，容易混淆任务相关与无关信息，限制了在分布变化下的鲁棒性。

Method: 通过模拟和真实世界的机器人操作任务，比较了对象中心、全局和密集视觉编码器的性能。

Result: OCR方法在泛化场景中表现优于密集和全局表示，无需任务特定预训练。

Conclusion: OCR是设计动态、真实世界机器人视觉系统的有前景方向。

Abstract: Visual representations are central to the learning and generalization
capabilities of robotic manipulation policies. While existing methods rely on
global or dense features, such representations often entangle task-relevant and
irrelevant scene information, limiting robustness under distribution shifts. In
this work, we investigate object-centric representations (OCR) as a structured
alternative that segments visual input into a finished set of entities,
introducing inductive biases that align more naturally with manipulation tasks.
We benchmark a range of visual encoders-object-centric, global and dense
methods-across a suite of simulated and real-world manipulation tasks ranging
from simple to complex, and evaluate their generalization under diverse visual
conditions including changes in lighting, texture, and the presence of
distractors. Our findings reveal that OCR-based policies outperform dense and
global representations in generalization settings, even without task-specific
pretraining. These insights suggest that OCR is a promising direction for
designing visual systems that generalize effectively in dynamic, real-world
robotic environments.

</details>

### [868] [Zero-Shot Visual Generalization in Robot Manipulation](https://arxiv.org/abs/2505.11719)
*Sumeet Batra,Gaurav Sukhatme*

Main category: cs.RO

TLDR: 该论文提出了一种基于解耦表示学习和联想记忆的方法，用于提高视觉操作策略的鲁棒性，并在仿真和真实硬件中展示了零样本适应性。


<details>
  <summary>Details</summary>
Motivation: 解决视觉环境中机器人学习策略的鲁棒性问题，避免依赖不变表示或大量数据集。

Method: 结合解耦表示学习和联想记忆，扩展到复杂任务，并引入一种新技术使策略对2D平面旋转不变。

Result: 在视觉泛化方面显著优于现有模仿学习方法，且策略对相机扰动具有鲁棒性。

Conclusion: 该工作为开发适应性强且鲁棒的机器人操作策略迈出了重要一步。

Abstract: Training vision-based manipulation policies that are robust across diverse
visual environments remains an important and unresolved challenge in robot
learning. Current approaches often sidestep the problem by relying on invariant
representations such as point clouds and depth, or by brute-forcing
generalization through visual domain randomization and/or large, visually
diverse datasets. Disentangled representation learning - especially when
combined with principles of associative memory - has recently shown promise in
enabling vision-based reinforcement learning policies to be robust to visual
distribution shifts. However, these techniques have largely been constrained to
simpler benchmarks and toy environments. In this work, we scale disentangled
representation learning and associative memory to more visually and dynamically
complex manipulation tasks and demonstrate zero-shot adaptability to visual
perturbations in both simulation and on real hardware. We further extend this
approach to imitation learning, specifically Diffusion Policy, and empirically
show significant gains in visual generalization compared to state-of-the-art
imitation learning methods. Finally, we introduce a novel technique adapted
from the model equivariance literature that transforms any trained neural
network policy into one invariant to 2D planar rotations, making our policy not
only visually robust but also resilient to certain camera perturbations. We
believe that this work marks a significant step towards manipulation policies
that are not only adaptable out of the box, but also robust to the complexities
and dynamical nature of real-world deployment. Supplementary videos are
available at https://sites.google.com/view/vis-gen-robotics/home.

</details>

### [869] [Reachability Barrier Networks: Learning Hamilton-Jacobi Solutions for Smooth and Flexible Control Barrier Functions](https://arxiv.org/abs/2505.11755)
*Matthew Kim,William Sharpless,Hyun Joe Jeong,Sander Tonkens,Somil Bansal,Sylvia Herbert*

Main category: cs.RO

TLDR: 论文提出了一种基于物理信息神经网络（PINNs）的方法，用于生成平滑的控制屏障函数（CBFs），解决了传统方法在高维空间中难以生成的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的控制屏障函数在低维空间外难以实现，且常导致非可微或不准确的近似，无法确保安全性。

Method: 利用物理信息神经网络（PINNs）计算Hamilton-Jacobi最优控制解，生成平滑的CBFs（称为RBNs），并通过参数化折扣项调整其保守性。

Result: RBNs在低维空间中高度准确，在高维空间中比标准神经CBF方法更安全且保守性更低。例如，在9D多车避碰问题中，RBNs的安全性提高了5.5倍，保守性降低了1.9倍。

Conclusion: RBNs为非线性自治系统提供了一种有前景的CBF合成方法。

Abstract: Recent developments in autonomous driving and robotics underscore the
necessity of safety-critical controllers. Control barrier functions (CBFs) are
a popular method for appending safety guarantees to a general control
framework, but they are notoriously difficult to generate beyond low
dimensions. Existing methods often yield non-differentiable or inaccurate
approximations that lack integrity, and thus fail to ensure safety. In this
work, we use physics-informed neural networks (PINNs) to generate smooth
approximations of CBFs by computing Hamilton-Jacobi (HJ) optimal control
solutions. These reachability barrier networks (RBNs) avoid traditional
dimensionality constraints and support the tuning of their conservativeness
post-training through a parameterized discount term. To ensure robustness of
the discounted solutions, we leverage conformal prediction methods to derive
probabilistic safety guarantees for RBNs. We demonstrate that RBNs are highly
accurate in low dimensions, and safer than the standard neural CBF approach in
high dimensions. Namely, we showcase the RBNs in a 9D multi-vehicle collision
avoidance problem where it empirically proves to be 5.5x safer and 1.9x less
conservative than the neural CBFs, offering a promising method to synthesize
CBFs for general nonlinear autonomous systems.

</details>

### [870] [Bridging Human Oversight and Black-box Driver Assistance: Vision-Language Models for Predictive Alerting in Lane Keeping Assist Systems](https://arxiv.org/abs/2505.11535)
*Yuhang Wang,Hao Zhou*

Main category: cs.RO

TLDR: LKAlert是一种新型监督预警系统，利用VLM提前1-3秒预测LKA风险，通过多模态数据和可解释模型增强驾驶员信任和情境意识。


<details>
  <summary>Details</summary>
Motivation: 解决LKA系统因黑盒特性导致的不可预测故障和驾驶员信任不足的问题。

Method: 结合VLM和可解释模型处理视频和CAN数据，生成预测警报和自然语言解释。

Result: 预测LKA故障准确率69.8%，F1分数58.6%，解释质量高（71.7 ROUGE-L），实时运行频率2 Hz。

Conclusion: LKAlert提升了ADAS的安全性和可用性，为黑盒自动化的人机监督提供了可扩展范式。

Abstract: Lane Keeping Assist systems, while increasingly prevalent, often suffer from
unpredictable real-world failures, largely due to their opaque, black-box
nature, which limits driver anticipation and trust. To bridge the gap between
automated assistance and effective human oversight, we present LKAlert, a novel
supervisory alert system that leverages VLM to forecast potential LKA risk 1-3
seconds in advance. LKAlert processes dash-cam video and CAN data, integrating
surrogate lane segmentation features from a parallel interpretable model as
automated guiding attention. Unlike traditional binary classifiers, LKAlert
issues both predictive alert and concise natural language explanation,
enhancing driver situational awareness and trust. To support the development
and evaluation of such systems, we introduce OpenLKA-Alert, the first benchmark
dataset designed for predictive and explainable LKA failure warnings. It
contains synchronized multimodal inputs and human-authored justifications
across annotated temporal windows. We further contribute a generalizable
methodological framework for VLM-based black-box behavior prediction, combining
surrogate feature guidance with LoRA. This framework enables VLM to reason over
structured visual context without altering its vision backbone, making it
broadly applicable to other complex, opaque systems requiring interpretable
oversight. Empirical results correctly predicts upcoming LKA failures with
69.8% accuracy and a 58.6\% F1-score. The system also generates high-quality
textual explanations for drivers (71.7 ROUGE-L) and operates efficiently at
approximately 2 Hz, confirming its suitability for real-time, in-vehicle use.
Our findings establish LKAlert as a practical solution for enhancing the safety
and usability of current ADAS and offer a scalable paradigm for applying VLMs
to human-centered supervision of black-box automation.

</details>

### [871] [GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation](https://arxiv.org/abs/2505.11865)
*Teli Ma,Jia Zheng,Zifan Wang,Ziyao Gao,Jiaming Zhou,Junwei Liang*

Main category: cs.RO

TLDR: 论文介绍了HOVA-500K数据集和GLOVER++框架，用于从人类演示视频中学习机器人操作技能，并通过全局到局部的可操作感知训练提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决从人类演示视频中学习机器人操作技能的挑战，包括缺乏大规模精确感知标注数据集和对多样化操作场景中感知的探索不足。

Method: 提出HOVA-500K数据集和GLOVER++框架，通过全局到局部的感知训练实现知识迁移。

Result: GLOVER++在HOVA-500K基准测试中达到最优性能，并在多样化机器人操作任务中表现出强泛化能力。

Conclusion: HOVA-500K和GLOVER++为弥合人类演示与机器人操作能力之间的差距提供了宝贵资源。

Abstract: Learning manipulation skills from human demonstration videos offers a
promising path toward generalizable and interpretable robotic
intelligence-particularly through the lens of actionable affordances. However,
transferring such knowledge remains challenging due to: 1) a lack of
large-scale datasets with precise affordance annotations, and 2) insufficient
exploration of affordances in diverse manipulation contexts. To address these
gaps, we introduce HOVA-500K, a large-scale, affordance-annotated dataset
comprising 500,000 images across 1,726 object categories and 675 actions. We
also release a standardized benchmarking suite for multi-modal affordance
reasoning. Built upon HOVA-500K, we present GLOVER++, a global-to-local
affordance training framework that effectively transfers actionable affordance
knowledge from human demonstrations to downstream open-vocabulary reasoning
tasks. GLOVER++ achieves state-of-the-art results on the HOVA-500K benchmark
and demonstrates strong generalization across diverse downstream robotic
manipulation tasks. By explicitly modeling actionable affordances, GLOVER++
facilitates robust transfer across scenes, modalities, and tasks. We hope that
HOVA-500K and the GLOVER++ framework will serve as valuable resources for
bridging the gap between human demonstrations and robotic manipulation
capabilities.

</details>

### [872] [Experimental Study on Automatically Assembling Custom Catering Packages With a 3-DOF Delta Robot Using Deep Learning Methods](https://arxiv.org/abs/2505.11879)
*Reihaneh Yourdkhani,Arash Tavoosian,Navid Asadi Khomami,Mehdi Tale Masouleh*

Main category: cs.RO

TLDR: 本文介绍了一种使用两指夹持器和3自由度Delta并联机器人实现餐饮包装自动化的实验研究，采用深度学习方法，结合YOLOV5和FastSAM模型，实现了实时检测和自主包装，成功率超过80%。


<details>
  <summary>Details</summary>
Motivation: 解决餐饮包装自动化中的挑战，特别是针对波斯制造产品的首次数据集应用。

Method: 使用YOLOV5进行目标检测，FastSAM进行分割，结合几何方法计算抓取点，并通过3-DOF Delta机器人实现自动化包装。

Result: 算法实现了实时检测和自主包装，抓取成功率超过80%。

Conclusion: 该研究显著提升了机器人系统在包装自动化中的实际应用能力。

Abstract: This paper introduces a pioneering experimental study on the automated
packing of a catering package using a two-fingered gripper affixed to a
3-degree-of-freedom Delta parallel robot. A distinctive contribution lies in
the application of a deep learning approach to tackle this challenge. A custom
dataset, comprising 1,500 images, is meticulously curated for this endeavor,
representing a noteworthy initiative as the first dataset focusing on
Persian-manufactured products. The study employs the YOLOV5 model for object
detection, followed by segmentation using the FastSAM model. Subsequently,
rotation angle calculation is facilitated with segmentation masks, and a
rotated rectangle encapsulating the object is generated. This rectangle forms
the basis for calculating two grasp points using a novel geometrical approach
involving eigenvectors. An extensive experimental study validates the proposed
model, where all pertinent information is seamlessly transmitted to the 3-DOF
Delta parallel robot. The proposed algorithm ensures real-time detection,
calibration, and the fully autonomous packing process of a catering package,
boasting an impressive over 80\% success rate in automatic grasping. This study
marks a significant stride in advancing the capabilities of robotic systems for
practical applications in packaging automation.

</details>

### [873] [Emergent Active Perception and Dexterity of Simulated Humanoids from Visual Reinforcement Learning](https://arxiv.org/abs/2505.12278)
*Zhengyi Luo,Chen Tessler,Toru Lin,Ye Yuan,Tairan He,Wenli Xiao,Yunrong Guo,Gal Chechik,Kris Kitani,Linxi Fan,Yuke Zhu*

Main category: cs.RO

TLDR: 论文提出了一种基于视觉感知的灵巧全身控制框架（PDC），仅依赖自我中心视觉完成任务，无需特权状态信息，能执行多种家庭任务。


<details>
  <summary>Details</summary>
Motivation: 人类行为受视觉感知驱动，研究旨在通过模拟人类视觉感知实现灵巧控制，以完成复杂任务。

Method: 采用强化学习从零开始训练单一策略，利用自我中心视觉作为任务接口，实现目标搜索、放置和技能选择。

Result: PDC框架成功实现了多种家庭任务（如抓取、放置），并涌现出主动搜索等人性化行为。

Conclusion: 视觉驱动的控制是连接感知与行动的关键，适用于动画、机器人和具身AI。

Abstract: Human behavior is fundamentally shaped by visual perception -- our ability to
interact with the world depends on actively gathering relevant information and
adapting our movements accordingly. Behaviors like searching for objects,
reaching, and hand-eye coordination naturally emerge from the structure of our
sensory system. Inspired by these principles, we introduce Perceptive Dexterous
Control (PDC), a framework for vision-driven dexterous whole-body control with
simulated humanoids. PDC operates solely on egocentric vision for task
specification, enabling object search, target placement, and skill selection
through visual cues, without relying on privileged state information (e.g., 3D
object positions and geometries). This perception-as-interface paradigm enables
learning a single policy to perform multiple household tasks, including
reaching, grasping, placing, and articulated object manipulation. We also show
that training from scratch with reinforcement learning can produce emergent
behaviors such as active search. These results demonstrate how vision-driven
control and complex tasks induce human-like behaviors and can serve as the key
ingredients in closing the perception-action loop for animation, robotics, and
embodied AI.

</details>

### [874] [Structureless VIO](https://arxiv.org/abs/2505.12337)
*Junlin Song,Miguel Olivares-Mendez*

Main category: cs.RO

TLDR: 提出了一种无结构的视觉惯性里程计（VIO），通过移除视觉地图，提高了计算效率和精度。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉里程计（VO）和视觉惯性里程计（VIO）依赖紧密耦合的定位与建图模块，导致效率低下。本文旨在探索一种无需地图的高效定位方案。

Method: 提出了一种无结构的VIO框架，移除了视觉地图，简化了系统设计。

Result: 实验表明，无结构VIO在计算效率和精度上均优于传统结构化的VIO基线方法。

Conclusion: 无结构VIO是一种高效且精确的替代方案，适用于不需要地图的场景。

Abstract: Visual odometry (VO) is typically considered as a chicken-and-egg problem, as
the localization and mapping modules are tightly-coupled. The estimation of
visual map relies on accurate localization information. Meanwhile, localization
requires precise map points to provide motion constraints. This classical
design principle is naturally inherited by visual-inertial odometry (VIO).
Efficient localization solution that does not require a map has not been fully
investigated. To this end, we propose a novel structureless VIO, where the
visual map is removed from the odometry framework. Experimental results
demonstrated that, compared to the structure-based VIO baseline, our
structureless VIO not only substantially improves computational efficiency but
also has advantages in accuracy.

</details>

### [875] [RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction](https://arxiv.org/abs/2505.12224)
*Weifeng Lu,Minghao Ye,Zewei Ye,Ruihan Tao,Shuo Yang,Bo Zhao*

Main category: cs.RO

TLDR: RoboFAC框架通过构建包含错误轨迹和QA对的数据集，提升了VLA模型在开放世界中的失败恢复能力，实验表明其性能优于GPT-4o，并在实际任务中显著改进。


<details>
  <summary>Details</summary>
Motivation: VLA模型在开放世界场景中表现不佳，主要因为其训练数据多为成功示范且缺乏失败恢复能力。

Method: 构建RoboFAC数据集（包含9,440错误轨迹和78,623 QA对），并开发RoboFAC模型，具备任务理解、失败分析和纠正能力。

Result: RoboFAC模型在评估基准上比GPT-4o高34.1%，实际任务中平均提升29.1%。

Conclusion: RoboFAC框架有效处理机器人失败并帮助VLA模型恢复，显著提升性能。

Abstract: Vision-Language-Action (VLA) models have recently advanced robotic
manipulation by translating natural-language instructions and image information
into sequential control actions. However, these models often underperform in
open-world scenarios, as they are predominantly trained on successful expert
demonstrations and exhibit a limited capacity for failure recovery. In this
work, we present a Robotic Failure Analysis and Correction (RoboFAC) framework
to address this issue. Firstly, we construct RoboFAC dataset comprising 9,440
erroneous manipulation trajectories and 78,623 QA pairs across 16 diverse tasks
and 53 scenes in both simulation and real-world environments. Leveraging our
dataset, we develop RoboFAC model, which is capable of Task Understanding,
Failure Analysis and Failure Correction. Experimental results demonstrate that
the RoboFAC model outperforms GPT-4o by 34.1% on our evaluation benchmark.
Furthermore, we integrate the RoboFAC model into a real-world VLA control
pipeline as an external supervision providing correction instructions, yielding
a 29.1% relative improvement on average on four real-world tasks. The results
show that our RoboFAC framework effectively handles robotic failures and
assists the VLA model in recovering from failures.

</details>

### [876] [TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation](https://arxiv.org/abs/2505.12748)
*Hangyu Li,Qin Zhao,Haoran Xu,Xinyu Jiang,Qingwei Ben,Feiyu Jia,Haoyu Zhao,Liang Xu,Jia Zeng,Hanqing Wang,Bo Dai,Junting Dong,Jiangmiao Pang*

Main category: cs.RO

TLDR: 论文介绍了TeleOpBench，一个专注于双臂灵巧遥操作的仿真基准测试平台，包含30个任务环境，并评估了四种遥操作模态。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一的遥操作系统基准测试，难以公平比较不同硬件管道的性能。

Method: 提出TeleOpBench，包含30个任务环境和四种遥操作模态，通过仿真和硬件实验验证其有效性。

Result: 仿真与硬件实验性能高度相关，验证了TeleOpBench的外部有效性。

Conclusion: TeleOpBench为遥操作研究提供了统一标准，并支持未来算法和硬件创新。

Abstract: Teleoperation is a cornerstone of embodied-robot learning, and bimanual
dexterous teleoperation in particular provides rich demonstrations that are
difficult to obtain with fully autonomous systems. While recent studies have
proposed diverse hardware pipelines-ranging from inertial motion-capture gloves
to exoskeletons and vision-based interfaces-there is still no unified benchmark
that enables fair, reproducible comparison of these systems. In this paper, we
introduce TeleOpBench, a simulator-centric benchmark tailored to bimanual
dexterous teleoperation. TeleOpBench contains 30 high-fidelity task
environments that span pick-and-place, tool use, and collaborative
manipulation, covering a broad spectrum of kinematic and force-interaction
difficulty. Within this benchmark we implement four representative
teleoperation modalities-(i) MoCap, (ii) VR device, (iii) arm-hand
exoskeletons, and (iv) monocular vision tracking-and evaluate them with a
common protocol and metric suite. To validate that performance in simulation is
predictive of real-world behavior, we conduct mirrored experiments on a
physical dual-arm platform equipped with two 6-DoF dexterous hands. Across 10
held-out tasks we observe a strong correlation between simulator and hardware
performance, confirming the external validity of TeleOpBench. TeleOpBench
establishes a common yardstick for teleoperation research and provides an
extensible platform for future algorithmic and hardware innovation.

</details>

### [877] [Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion Predictions](https://arxiv.org/abs/2505.12327)
*Albert Zhao,Stefano Soatto*

Main category: cs.RO

TLDR: 提出了一种基于扩散模型的自动驾驶规划方法，结合正常和对抗性行为预测，实现鲁棒性规划。


<details>
  <summary>Details</summary>
Motivation: 现有方法在对抗性行为中可能过于保守或忽视正常行为，需要一种更平衡的规划方法。

Method: 训练扩散模型学习正常行为分布，测试时生成对抗性预测，通过混合分布评分候选计划。

Result: 在单/多智能体闯红灯等场景中验证了方法的有效性。

Conclusion: 该方法在对抗性和正常行为间取得平衡，避免过度保守或忽视低风险行为。

Abstract: We describe a robust planning method for autonomous driving that mixes normal
and adversarial agent predictions output by a diffusion model trained for
motion prediction. We first train a diffusion model to learn an unbiased
distribution of normal agent behaviors. We then generate a distribution of
adversarial predictions by biasing the diffusion model at test time to generate
predictions that are likely to collide with a candidate plan. We score plans
using expected cost with respect to a mixture distribution of normal and
adversarial predictions, leading to a planner that is robust against
adversarial behaviors but not overly conservative when agents behave normally.
Unlike current approaches, we do not use risk measures that over-weight
adversarial behaviors while placing little to no weight on low-cost normal
behaviors or use hard safety constraints that may not be appropriate for all
driving scenarios. We show the effectiveness of our method on single-agent and
multi-agent jaywalking scenarios as well as a red light violation scenario.

</details>

### [878] [Adaptive MPC-based quadrupedal robot control under periodic disturbances](https://arxiv.org/abs/2505.12361)
*Elizaveta Pestova,Ilya Osokin,Danil Belov,Pavel Osinenko*

Main category: cs.RO

TLDR: 论文提出了一种轻量级回归器方法，用于估计周期性扰动，从而提升四足机器人在轨迹跟踪任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未明确解决周期性扰动问题，尤其是在四足机器人应用中。

Method: 使用简化机器人动力学和轻量级回归器估计周期性扰动的幅度和频率。

Result: 实验表明，该方法在性能上优于静态扰动补偿基线。

Conclusion: 该方法有效提升了四足机器人在周期性扰动下的轨迹跟踪能力，相关资源已开源。

Abstract: Recent advancements in adaptive control for reference trajectory tracking
enable quadrupedal robots to perform locomotion tasks under challenging
conditions. There are methods enabling the estimation of the external
disturbances in terms of forces and torques. However, a specific case of
disturbances that are periodic was not explicitly tackled in application to
quadrupeds. This work is devoted to the estimation of the periodic disturbances
with a lightweight regressor using simplified robot dynamics and extracting the
disturbance properties in terms of the magnitude and frequency. Experimental
evidence suggests performance improvement over the baseline static disturbance
compensation. All source files, including simulation setups, code, and
calculation scripts, are available on GitHub at
https://github.com/aidagroup/quad-periodic-mpc.

</details>

### [879] [OmniDrones: An Efficient and Flexible Platform for Reinforcement Learning in Drone Control](https://arxiv.org/abs/2309.12825)
*Botian Xu,Feng Gao,Chao Yu,Ruize Zhang,Yi Wu,Yu Wang*

Main category: cs.RO

TLDR: OmniDrones是一个基于Nvidia Omniverse Isaac Sim的高效灵活平台，专为无人机控制的强化学习设计，提供丰富的工具和基准任务。


<details>
  <summary>Details</summary>
Motivation: 为无人机控制的强化学习提供一个易于设计和实验的仿真平台，支持多样化应用场景和基准任务。

Method: 采用自下而上的设计方法，支持GPU并行化仿真，提供多种无人机模型、传感器模态、控制模式和基准任务。

Result: 平台包含4种无人机模型、5种传感器模态、4种控制模式、10多个基准任务，并提供初步实验结果。

Conclusion: OmniDrones是一个开源平台，旨在推动强化学习在无人机系统中的实际应用研究。

Abstract: In this work, we introduce OmniDrones, an efficient and flexible platform
tailored for reinforcement learning in drone control, built on Nvidia's
Omniverse Isaac Sim. It employs a bottom-up design approach that allows users
to easily design and experiment with various application scenarios on top of
GPU-parallelized simulations. It also offers a range of benchmark tasks,
presenting challenges ranging from single-drone hovering to over-actuated
system tracking. In summary, we propose an open-sourced drone simulation
platform, equipped with an extensive suite of tools for drone learning. It
includes 4 drone models, 5 sensor modalities, 4 control modes, over 10
benchmark tasks, and a selection of widely used RL baselines. To showcase the
capabilities of OmniDrones and to support future research, we also provide
preliminary results on these benchmark tasks. We hope this platform will
encourage further studies on applying RL to practical drone systems.

</details>

### [880] [DynamicDTA: Drug-Target Binding Affinity Prediction Using Dynamic Descriptors and Graph Representation](https://arxiv.org/abs/2505.11529)
*Dan Luo,Jinyu Zhou,Le Xu,Sisi Yuan,Xuan Lin*

Main category: cs.RO

TLDR: DynamicDTA是一种创新的深度学习框架，结合静态和动态蛋白质特征，显著提升了药物-靶标结合亲和力（DTA）预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有模型主要依赖静态蛋白质结构，忽略了蛋白质的动态特性，而动态特性对捕捉结合亲和力至关重要。

Method: DynamicDTA整合药物序列、蛋白质序列和动态描述符，通过图卷积网络、扩张卷积和多层感知器处理，并使用交叉注意力和张量融合网络整合特征。

Result: 在三个数据集上，DynamicDTA的RMSE评分比七种先进基线方法至少提高了3.4%，并在HIV-1新型药物预测中验证了其可靠性。

Conclusion: DynamicDTA通过结合动态蛋白质特征，显著提升了DTA预测的准确性和生物相关性。

Abstract: Predicting drug-target binding affinity (DTA) is essential for identifying
potential therapeutic candidates in drug discovery. However, most existing
models rely heavily on static protein structures, often overlooking the dynamic
nature of proteins, which is crucial for capturing conformational flexibility
that will be beneficial for protein binding interactions. We introduce
DynamicDTA, an innovative deep learning framework that incorporates static and
dynamic protein features to enhance DTA prediction. The proposed DynamicDTA
takes three types of inputs, including drug sequence, protein sequence, and
dynamic descriptors. A molecular graph representation of the drug sequence is
generated and subsequently processed through graph convolutional network, while
the protein sequence is encoded using dilated convolutions. Dynamic
descriptors, such as root mean square fluctuation, are processed through a
multi-layer perceptron. These embedding features are fused with static protein
features using cross-attention, and a tensor fusion network integrates all
three modalities for DTA prediction. Extensive experiments on three datasets
demonstrate that DynamicDTA achieves by at least 3.4% improvement in RMSE score
with comparison to seven state-of-the-art baseline methods. Additionally,
predicting novel drugs for Human Immunodeficiency Virus Type 1 and visualizing
the docking complexes further demonstrates the reliability and biological
relevance of DynamicDTA.

</details>

### [881] [Empirical Performance Evaluation of Lane Keeping Assist on Modern Production Vehicles](https://arxiv.org/abs/2505.11534)
*Yuhang Wang,Abdulaziz Alhuraish,Shuyi Wang,Hao Zhou*

Main category: cs.RO

TLDR: 本文首次对真实世界中的车道保持辅助系统（LKA）性能进行了全面实证分析，揭示了其失败模式、驾驶策略差异及环境因素影响，并提出了改进模型和工具。


<details>
  <summary>Details</summary>
Motivation: 研究动机是利用公开数据集分析LKA系统的实际表现，揭示其局限性并为自动驾驶技术提供改进方向。

Method: 通过分析LKA相关CAN信号，分类失败模式；对比LKA与人类驾驶策略；统计环境因素对LKA失败的影响。

Result: 发现LKA失败可分为感知、规划和控制错误；LKA策略导致向外漂移；环境因素如模糊车道线和急弯显著增加失败概率。

Conclusion: 提出了整合道路几何和LKA能力的理论模型及机器学习工具，为更安全的自动驾驶技术发展提供支持。

Abstract: Leveraging a newly released open dataset of Lane Keeping Assist (LKA) systems
from production vehicles, this paper presents the first comprehensive empirical
analysis of real-world LKA performance. Our study yields three key findings:
(i) LKA failures can be systematically categorized into perception, planning,
and control errors. We present representative examples of each failure mode
through in-depth analysis of LKA-related CAN signals, enabling both
justification of the failure mechanisms and diagnosis of when and where each
module begins to degrade; (ii) LKA systems tend to follow a fixed
lane-centering strategy, often resulting in outward drift that increases
linearly with road curvature, whereas human drivers proactively steer slightly
inward on similar curved segments; (iii) We provide the first statistical
summary and distribution analysis of environmental and road conditions under
LKA failures, identifying with statistical significance that faded lane
markings, low pavement laneline contrast, and sharp curvature are the most
dominant individual factors, along with critical combinations that
substantially increase failure likelihood. Building on these insights, we
propose a theoretical model that integrates road geometry, speed limits, and
LKA steering capability to inform infrastructure design. Additionally, we
develop a machine learning-based model to assess roadway readiness for LKA
deployment, offering practical tools for safer infrastructure planning,
especially in rural areas. This work highlights key limitations of current LKA
systems and supports the advancement of safer and more reliable autonomous
driving technologies.

</details>

### [882] [A Comprehensive Survey on Physical Risk Control in the Era of Foundation Model-enabled Robotics](https://arxiv.org/abs/2505.12583)
*Takeshi Kojima,Yaonan Zhu,Yusuke Iwasawa,Toshinori Kitamura,Gang Yan,Shu Morikuni,Ryosuke Takanami,Alfredo Solano,Tatsuya Matsushima,Akiko Murakami,Yutaka Matsuo*

Main category: cs.RO

TLDR: 该论文综述了基础模型驱动的机器人（FMRs）的物理风险控制方法，分为部署前、事件前和事件后三个阶段，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: FMRs因其通用技能而具有广泛的应用潜力，但其物理交互特性带来了安全风险，需系统研究其控制方法。

Method: 通过时间线划分（部署前、事件前、事件后）全面总结FMRs的物理风险控制策略。

Result: 发现事件前风险缓解策略、人机物理交互研究及基础模型本身问题是未来重点研究方向。

Conclusion: 该综述为FMRs的物理风险控制提供了高分辨率分析，有助于实现良好的人机关系。

Abstract: Recent Foundation Model-enabled robotics (FMRs) display greatly improved
general-purpose skills, enabling more adaptable automation than conventional
robotics. Their ability to handle diverse tasks thus creates new opportunities
to replace human labor. However, unlike general foundation models, FMRs
interact with the physical world, where their actions directly affect the
safety of humans and surrounding objects, requiring careful deployment and
control. Based on this proposition, our survey comprehensively summarizes robot
control approaches to mitigate physical risks by covering all the lifespan of
FMRs ranging from pre-deployment to post-accident stage. Specifically, we
broadly divide the timeline into the following three phases: (1) pre-deployment
phase, (2) pre-incident phase, and (3) post-incident phase. Throughout this
survey, we find that there is much room to study (i) pre-incident risk
mitigation strategies, (ii) research that assumes physical interaction with
humans, and (iii) essential issues of foundation models themselves. We hope
that this survey will be a milestone in providing a high-resolution analysis of
the physical risks of FMRs and their control, contributing to the realization
of a good human-robot relationship.

</details>

### [883] [DreamGen: Unlocking Generalization in Robot Learning through Neural Trajectories](https://arxiv.org/abs/2505.12705)
*Joel Jang,Seonghyeon Ye,Zongyu Lin,Jiannan Xiang,Johan Bjorck,Yu Fang,Fengyuan Hu,Spencer Huang,Kaushil Kundalia,Yen-Chen Lin,Loic Magne,Ajay Mandlekar,Avnish Narayan,You Liang Tan,Guanzhi Wang,Jing Wang,Qi Wang,Yinzhen Xu,Xiaohui Zeng,Kaiyuan Zheng,Ruijie Zheng,Ming-Yu Liu,Luke Zettlemoyer,Dieter Fox,Jan Kautz,Scott Reed,Yuke Zhu,Linxi Fan*

Main category: cs.RO

TLDR: DreamGen是一个4阶段流程，通过神经轨迹训练机器人策略，实现行为和环境的泛化。


<details>
  <summary>Details</summary>
Motivation: 解决机器人学习中数据收集的局限性，通过视频生成模型扩展训练数据。

Method: 利用图像到视频生成模型生成合成视频，并通过潜在动作模型或逆动力学模型提取伪动作序列。

Result: 人类机器人能在新环境和行为中表现良好，仅需少量原始数据。

Conclusion: DreamGen为机器人学习提供了高效的数据扩展方法，具有广泛应用潜力。

Abstract: We introduce DreamGen, a simple yet highly effective 4-stage pipeline for
training robot policies that generalize across behaviors and environments
through neural trajectories - synthetic robot data generated from video world
models. DreamGen leverages state-of-the-art image-to-video generative models,
adapting them to the target robot embodiment to produce photorealistic
synthetic videos of familiar or novel tasks in diverse environments. Since
these models generate only videos, we recover pseudo-action sequences using
either a latent action model or an inverse-dynamics model (IDM). Despite its
simplicity, DreamGen unlocks strong behavior and environment generalization: a
humanoid robot can perform 22 new behaviors in both seen and unseen
environments, while requiring teleoperation data from only a single
pick-and-place task in one environment. To evaluate the pipeline
systematically, we introduce DreamGen Bench, a video generation benchmark that
shows a strong correlation between benchmark performance and downstream policy
success. Our work establishes a promising new axis for scaling robot learning
well beyond manual data collection.

</details>

### [884] [Composing Dextrous Grasping and In-hand Manipulation via Scoring with a Reinforcement Learning Critic](https://arxiv.org/abs/2505.13253)
*Lennart Röstel,Dominik Winkelbauer,Johannes Pitz,Leon Sievers,Berthold Bäuml*

Main category: cs.RO

TLDR: 提出一种利用强化学习代理的评论网络来评分和选择初始抓取的方法，以提高手内操作的成功率，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 解决手内操作策略在真实场景中因初始抓取状态不合适而受限的问题。

Method: 利用训练好的强化学习代理的评论网络对初始抓取进行评分和选择。

Result: 显著提高手内操作的成功率，并在真实系统中实现自主抓取和重定向。

Conclusion: 该方法有效解决了初始抓取与手内操作目标的匹配问题，提升了实际应用性能。

Abstract: In-hand manipulation and grasping are fundamental yet often separately
addressed tasks in robotics. For deriving in-hand manipulation policies,
reinforcement learning has recently shown great success. However, the derived
controllers are not yet useful in real-world scenarios because they often
require a human operator to place the objects in suitable initial (grasping)
states. Finding stable grasps that also promote the desired in-hand
manipulation goal is an open problem. In this work, we propose a method for
bridging this gap by leveraging the critic network of a reinforcement learning
agent trained for in-hand manipulation to score and select initial grasps. Our
experiments show that this method significantly increases the success rate of
in-hand manipulation without requiring additional training. We also present an
implementation of a full grasp manipulation pipeline on a real-world system,
enabling autonomous grasping and reorientation even of unwieldy objects.

</details>

### [885] [Interpretable Robotic Friction Learning via Symbolic Regression](https://arxiv.org/abs/2505.13186)
*Philipp Scholl,Alexander Dietrich,Sebastian Wolf,Jinoh Lee,Alin-Albu Schäffer,Gitta Kutyniok,Maged Iskandar*

Main category: cs.RO

TLDR: 论文提出使用符号回归（SR）来估计机器人关节的摩擦扭矩，结合了模型驱动和数据驱动方法的优点，生成可解释的符号公式，并在KUKA LWR-IV+机器人上验证了其高精度和扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统模型驱动方法需要大量实验和专业知识，难以适应新场景；数据驱动方法缺乏鲁棒性和可解释性。SR旨在解决两者的局限性。

Method: 应用符号回归算法，利用KUKA LWR-IV+机器人收集的数据近似摩擦扭矩，生成可解释的符号公式。

Result: SR生成的公式复杂度与模型驱动方法相当，但精度更高，并能灵活扩展以包含负载依赖等动态因素。

Conclusion: 符号回归是一种有效的摩擦扭矩建模方法，兼具可解释性和灵活性，适用于机器人硬件和安全关键应用。

Abstract: Accurately modeling the friction torque in robotic joints has long been
challenging due to the request for a robust mathematical description.
Traditional model-based approaches are often labor-intensive, requiring
extensive experiments and expert knowledge, and they are difficult to adapt to
new scenarios and dependencies. On the other hand, data-driven methods based on
neural networks are easier to implement but often lack robustness,
interpretability, and trustworthiness--key considerations for robotic hardware
and safety-critical applications such as human-robot interaction. To address
the limitations of both approaches, we propose the use of symbolic regression
(SR) to estimate the friction torque. SR generates interpretable symbolic
formulas similar to those produced by model-based methods while being flexible
to accommodate various dynamic effects and dependencies. In this work, we apply
SR algorithms to approximate the friction torque using collected data from a
KUKA LWR-IV+ robot. Our results show that SR not only yields formulas with
comparable complexity to model-based approaches but also achieves higher
accuracy. Moreover, SR-derived formulas can be seamlessly extended to include
load dependencies and other dynamic factors.

</details>

### [886] [Investigating Active Sampling for Hardness Classification with Vision-Based Tactile Sensors](https://arxiv.org/abs/2505.13231)
*Junyi Chen,Alap Kshirsagar,Frederik Heller,Mario Gómez Andreu,Boris Belousov,Tim Schneider,Lisa P. Y. Lin,Katja Doerschner,Knut Drewing,Jan Peters*

Main category: cs.RO

TLDR: 该论文研究了基于视觉触觉传感器的硬度分类，通过信息论主动采样策略提高效率，结果表明不确定性驱动的采样方法优于随机采样，且机器人表现优于人类。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过触觉传感器高效分类物体硬度，提升机器人在触觉感知任务中的性能。

Method: 评估了三种概率分类器模型和两种基于模型不确定性的采样策略，分别在机器人实验和人类数据集上进行测试。

Result: 主动采样方法在准确性和稳定性上优于随机采样，机器人分类准确率达88.78%，显著高于人类的48.00%。

Conclusion: 基于视觉的触觉传感器在硬度分类中表现出色，不确定性驱动的采样策略能有效提升性能。

Abstract: One of the most important object properties that humans and robots perceive
through touch is hardness. This paper investigates information-theoretic active
sampling strategies for sample-efficient hardness classification with
vision-based tactile sensors. We evaluate three probabilistic classifier models
and two model-uncertainty-based sampling strategies on a robotic setup as well
as on a previously published dataset of samples collected by human testers. Our
findings indicate that the active sampling approaches, driven by uncertainty
metrics, surpass a random sampling baseline in terms of accuracy and stability.
Additionally, while in our human study, the participants achieve an average
accuracy of 48.00%, our best approach achieves an average accuracy of 88.78% on
the same set of objects, demonstrating the effectiveness of vision-based
tactile sensors for object hardness classification.

</details>

### [887] [OPA-Pack: Object-Property-Aware Robotic Bin Packing](https://arxiv.org/abs/2505.13339)
*Jia-Hui Pan,Yeok Tatt Cheah,Zhengzhe Liu,Ka-Hei Hui,Xiaojie Gao,Pheng-Ann Heng,Yun-Hui Liu,Chi-Wing Fu*

Main category: cs.RO

TLDR: OPA-Pack是一个机器人装箱框架，首次考虑物体属性（如易碎性、化学兼容性）来优化装箱，结合检索增强生成和链式推理技术，显著提升兼容性和保护易碎物品。


<details>
  <summary>Details</summary>
Motivation: 现有装箱方法主要关注形状优化，忽略了人类装箱时考虑的物体属性（如易碎性、化学兼容性），导致实际应用中的不足。

Method: 开发了基于检索增强生成和链式推理的属性识别方案，构建了包含1032个日常物体属性标注的数据集，并提出OPA-Net模型，结合属性嵌入层和高度图，通过深度Q学习训练。

Result: 实验显示，OPA-Pack将不兼容物体对的分离准确率从52%提升至95%，减少易碎物品压力29.4%，同时保持良好紧凑性。

Conclusion: OPA-Pack在真实平台上验证了其有效性，展示了在实际场景中的实用性。

Abstract: Robotic bin packing aids in a wide range of real-world scenarios such as
e-commerce and warehouses. Yet, existing works focus mainly on considering the
shape of objects to optimize packing compactness and neglect object properties
such as fragility, edibility, and chemistry that humans typically consider when
packing objects. This paper presents OPA-Pack (Object-Property-Aware Packing
framework), the first framework that equips the robot with object property
considerations in planning the object packing. Technical-wise, we develop a
novel object property recognition scheme with retrieval-augmented generation
and chain-of-thought reasoning, and build a dataset with object property
annotations for 1,032 everyday objects. Also, we formulate OPA-Net, aiming to
jointly separate incompatible object pairs and reduce pressure on fragile
objects, while compacting the packing. Further, OPA-Net consists of a property
embedding layer to encode the property of candidate objects to be packed,
together with a fragility heightmap and an avoidance heightmap to keep track of
the packed objects. Then, we design a reward function and adopt a deep
Q-learning scheme to train OPA-Net. Experimental results manifest that OPA-Pack
greatly improves the accuracy of separating incompatible object pairs (from 52%
to 95%) and largely reduces pressure on fragile objects (by 29.4%), while
maintaining good packing compactness. Besides, we demonstrate the effectiveness
of OPA-Pack on a real packing platform, showcasing its practicality in
real-world scenarios.

</details>

<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [888] [Pre-trained Prompt-driven Community Search](https://arxiv.org/abs/2505.12304)
*Li Ni,Hengkai Xu,Lin Mu,Yiwen Zhang,Wenjian Luo*

Main category: cs.SI

TLDR: 论文提出了一种基于预训练和提示的新模型PPCS，用于半监督社区搜索，提高了搜索准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有半监督社区检测算法通常无法包含查询节点，因此不适用于搜索给定节点的社区。

Method: PPCS包含三个组件：节点编码（使用图神经网络学习节点表示）、样本生成（选择结构相似的已知社区作为训练样本）和提示驱动的微调（利用样本作为提示指导预测）。

Result: 在五个真实数据集上的实验表明，PPCS优于基线算法，且搜索效率更高。

Conclusion: PPCS是一种有效的半监督社区搜索方法，其各组件均被验证有效。

Abstract: The "pre-train, prompt" paradigm is widely adopted in various graph-based
tasks and has shown promising performance in community detection. Most existing
semi-supervised community detection algorithms detect communities based on
known ones, and the detected communities typically do not contain the given
query node. Therefore, they are not suitable for searching the community of a
given node. Motivated by this, we adopt this paradigm into the semi-supervised
community search for the first time and propose Pre-trained Prompt-driven
Community Search (PPCS), a novel model designed to enhance search accuracy and
efficiency. PPCS consists of three main components: node encoding, sample
generation, and prompt-driven fine-tuning. Specifically, the node encoding
component employs graph neural networks to learn local structural patterns of
nodes in a graph, thereby obtaining representations for nodes and communities.
Next, the sample generation component identifies an initial community for a
given node and selects known communities that are structurally similar to the
initial one as training samples. Finally, the prompt-driven fine-tuning
component leverages these samples as prompts to guide the final community
prediction. Experimental results on five real-world datasets demonstrate that
PPCS performs better than baseline algorithms. It also achieves higher
community search efficiency than semi-supervised community search baseline
methods, with ablation studies verifying the effectiveness of each component of
PPCS.

</details>

### [889] [Community Search in Time-dependent Road-social Attributed Networks](https://arxiv.org/abs/2505.12309)
*Li Ni,Hengkai Xu,Lin Mu,Yiwen Zhang,Wenjian Luo*

Main category: cs.SI

TLDR: 提出了一种结合语义和空间信息的k-core社区搜索方法，解决了现有方法仅关注单一属性（关键词或位置）和忽略交通时间变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现实网络通常包含关键词和位置信息，且交通时间会变化，但现有社区搜索方法仅关注单一属性，导致社区语义或空间内聚性低。

Method: 提出精确和贪心两种算法，从查询节点局部扩展，避免遍历无关节点，并利用大语言模型计算关键词语义相似度。

Result: 贪心算法在结构、语义和时间依赖的空间内聚性上优于基线方法。

Conclusion: 该方法有效提升了社区搜索的语义和空间内聚性，且计算效率更高。

Abstract: Real-world networks often involve both keywords and locations, along with
travel time variations between locations due to traffic conditions. However,
most existing cohesive subgraph-based community search studies utilize a single
attribute, either keywords or locations, to identify communities. They do not
simultaneously consider both keywords and locations, which results in low
semantic or spatial cohesiveness of the detected communities, and they fail to
account for variations in travel time. Additionally, these studies traverse the
entire network to build efficient indexes, but the detected community only
involves nodes around the query node, leading to the traversal of nodes that
are not relevant to the community. Therefore, we propose the problem of
discovering semantic-spatial aware k-core, which refers to a k-core with high
semantic and time-dependent spatial cohesiveness containing the query node. To
address this problem, we propose an exact and a greedy algorithm, both of which
gradually expand outward from the query node. They are local methods that only
access the local part of the attributed network near the query node rather than
the entire network. Moreover, we design a method to calculate the semantic
similarity between two keywords using large language models. This method
alleviates the disadvantages of keyword-matching methods used in existing
community search studies, such as mismatches caused by differently expressed
synonyms and the presence of irrelevant words. Experimental results show that
the greedy algorithm outperforms baselines in terms of structural, semantic,
and time-dependent spatial cohesiveness.

</details>

### [890] [HyperDet: Source Detection in Hypergraphs via Interactive Relationship Construction and Feature-rich Attention Fusion](https://arxiv.org/abs/2505.12894)
*Le Cheng,Peican Zhu,Yangming Guo,Keke Tang,Chao Gao,Zhen Wang*

Main category: cs.SI

TLDR: 论文提出了一种基于超图的谣言源检测方法（HyperDet），通过交互式关系构建和特征丰富的注意力融合，有效捕捉群体现象。


<details>
  <summary>Details</summary>
Motivation: 现有谣言源检测方法主要关注二元交互，无法充分处理更复杂的关系结构。超图能更好地建模社交网络中的群体现象。

Method: 提出HyperDet方法，包含交互式关系构建模块（建模静态拓扑和动态交互）和特征丰富的注意力融合模块（自注意力机制学习节点特征）。

Result: 实验验证表明，HyperDet优于现有最先进方法。

Conclusion: HyperDet通过建模高阶关系，显著提升了谣言源检测的准确性。

Abstract: Hypergraphs offer superior modeling capabilities for social networks,
particularly in capturing group phenomena that extend beyond pairwise
interactions in rumor propagation. Existing approaches in rumor source
detection predominantly focus on dyadic interactions, which inadequately
address the complexity of more intricate relational structures. In this study,
we present a novel approach for Source Detection in Hypergraphs (HyperDet) via
Interactive Relationship Construction and Feature-rich Attention Fusion.
Specifically, our methodology employs an Interactive Relationship Construction
module to accurately model both the static topology and dynamic interactions
among users, followed by the Feature-rich Attention Fusion module, which
autonomously learns node features and discriminates between nodes using a
self-attention mechanism, thereby effectively learning node representations
under the framework of accurately modeled higher-order relationships. Extensive
experimental validation confirms the efficacy of our HyperDet approach,
showcasing its superiority relative to current state-of-the-art methods.

</details>

### [891] [SourceDetMamba: A Graph-aware State Space Model for Source Detection in Sequential Hypergraphs](https://arxiv.org/abs/2505.12910)
*Le Cheng,Peican Zhu,Yangming Guo,Chao Gao,Zhen Wang,Keke Tang*

Main category: cs.SI

TLDR: SourceDetMamba利用状态空间模型Mamba和超图建模，通过时序网络快照和图形感知状态更新机制，显著提升了谣言源检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法未能充分捕捉谣言传播的内在动态性，需要更高效的模型来解决这一问题。

Method: 使用超图建模社交网络的高阶交互，反向输入时序网络快照至Mamba模型，并提出图形感知状态更新机制。

Result: 在八个数据集上的实验表明，SourceDetMamba优于现有最优方法。

Conclusion: SourceDetMamba通过结合时序和拓扑信息，有效提升了谣言源检测的准确性和效率。

Abstract: Source detection on graphs has demonstrated high efficacy in identifying
rumor origins. Despite advances in machine learning-based methods, many fail to
capture intrinsic dynamics of rumor propagation. In this work, we present
SourceDetMamba: A Graph-aware State Space Model for Source Detection in
Sequential Hypergraphs, which harnesses the recent success of the state space
model Mamba, known for its superior global modeling capabilities and
computational efficiency, to address this challenge. Specifically, we first
employ hypergraphs to model high-order interactions within social networks.
Subsequently, temporal network snapshots generated during the propagation
process are sequentially fed in reverse order into Mamba to infer underlying
propagation dynamics. Finally, to empower the sequential model to effectively
capture propagation patterns while integrating structural information, we
propose a novel graph-aware state update mechanism, wherein the state of each
node is propagated and refined by both temporal dependencies and topological
context. Extensive evaluations on eight datasets demonstrate that
SourceDetMamba consistently outperforms state-of-the-art approaches.

</details>

### [892] [Framework of Voting Prediction of Parliament Members](https://arxiv.org/abs/2505.12535)
*Zahi Mizrahi,Shai Berkovitz,Nimrod Talmon,Michael Fire*

Main category: cs.SI

TLDR: 该论文提出了一种名为投票预测框架（VPF）的数据驱动方法，用于预测议会投票结果，包括议员个人投票和法案整体结果。


<details>
  <summary>Details</summary>
Motivation: 政府透明度需要跟踪议员投票行为，但现有投票记录难以解读。准确预测投票结果可优化议会工作和法案通过率。

Method: VPF框架包括数据收集、解析与特征整合、预测模型三个部分，利用机器学习分析多国议会投票记录。

Result: VPF在预测个人投票和法案结果上分别达到85%和84%的准确率。

Conclusion: VPF是政治分析和政策研究的有效工具，可提升公众对立法决策的参与度。

Abstract: Keeping track of how lawmakers vote is essential for government transparency.
While many parliamentary voting records are available online, they are often
difficult to interpret, making it challenging to understand legislative
behavior across parliaments and predict voting outcomes. Accurate prediction of
votes has several potential benefits, from simplifying parliamentary work by
filtering out bills with a low chance of passing to refining proposed
legislation to increase its likelihood of approval. In this study, we leverage
advanced machine learning and data analysis techniques to develop a
comprehensive framework for predicting parliamentary voting outcomes across
multiple legislatures. We introduce the Voting Prediction Framework (VPF) - a
data-driven framework designed to forecast parliamentary voting outcomes at the
individual legislator level and for entire bills. VPF consists of three key
components: (1) Data Collection - gathering parliamentary voting records from
multiple countries using APIs, web crawlers, and structured databases; (2)
Parsing and Feature Integration - processing and enriching the data with
meaningful features, such as legislator seniority, and content-based
characteristics of a given bill; and (3) Prediction Models - using machine
learning to forecast how each parliament member will vote and whether a bill is
likely to pass. The framework will be open source, enabling anyone to use or
modify the framework. To evaluate VPF, we analyzed over 5 million voting
records from five countries - Canada, Israel, Tunisia, the United Kingdom and
the USA. Our results show that VPF achieves up to 85% precision in predicting
individual votes and up to 84% accuracy in predicting overall bill outcomes.
These findings highlight VPF's potential as a valuable tool for political
analysis, policy research, and enhancing public access to legislative
decision-making.

</details>

### [893] [Measuring Social Influence with Networked Synthetic Control](https://arxiv.org/abs/2505.13334)
*Ho-Chun Herbert Chang*

Main category: cs.SI

TLDR: 该论文提出了一种结合机器学习和网络科学的方法，用于测量社会影响力，并通过理论推导和模拟验证了其性质。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏反事实和比较，测量社会影响力具有挑战性。

Method: 结合机器学习建模和网络科学，提出了一种基于合成控制的社会价值测量方法，并通过理论推导和模拟验证其性质。

Result: 社会价值与中心性度量不同，且在特定情况下验证了广义友谊悖论。

Conclusion: 该方法为测量社会影响力提供了新视角，并展示了其计算效率和理论性质。

Abstract: Measuring social influence is difficult due to the lack of counter-factuals
and comparisons. By combining machine learning-based modeling and network
science, we present general properties of social value, a recent measure for
social influence using synthetic control applicable to political behavior.
Social value diverges from centrality measures on in that it relies on an
external regressor to predict an output variable of interest, generates a
synthetic measure of influence, then distributes individual contribution based
on a social network. Through theoretical derivations, we show the properties of
SV under linear regression with and without interaction, across lattice
networks, power-law networks, and random graphs. A reduction in computation can
be achieved for any ensemble model. Through simulation, we find that the
generalized friendship paradox holds -- that in certain situations, your
friends have on average more influence than you do.

</details>

<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [894] [Control Invariant Sets for Neural Network Dynamical Systems and Recursive Feasibility in Model Predictive Control](https://arxiv.org/abs/2505.11546)
*Xiao Li,Tianhao Wei,Changliu Liu,Anouck Girard,Ilya Kolmanovsky*

Main category: eess.SY

TLDR: 该论文提出了一种针对神经网络动态模型的控制不变集合成方法，确保安全性和递归可行性，并通过自动驾驶场景验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 神经网络的非线性与黑盒特性对控制设计的安全性和递归可行性提出了挑战，需要一种能够保证这些特性的方法。

Method: 采用集合递归算法合成控制不变集，并结合混合整数优化的模型预测控制设计。

Result: 理论分析和数值模拟表明，该方法能够有效合成控制不变集并实现安全、递归可行的在线控制。

Conclusion: 提出的方法为神经网络动态模型的控制设计提供了安全性和递归可行性的理论保证，并在实际场景中验证了其有效性。

Abstract: Neural networks are powerful tools for data-driven modeling of complex
dynamical systems, enhancing predictive capability for control applications.
However, their inherent nonlinearity and black-box nature challenge control
designs that prioritize rigorous safety and recursive feasibility guarantees.
This paper presents algorithmic methods for synthesizing control invariant sets
specifically tailored to neural network based dynamical models. These
algorithms employ set recursion, ensuring termination after a finite number of
iterations and generating subsets in which closed-loop dynamics are forward
invariant, thus guaranteeing perpetual operational safety. Additionally, we
propose model predictive control designs that integrate these control invariant
sets into mixed-integer optimization, with guaranteed adherence to safety
constraints and recursive feasibility at the computational level. We also
present a comprehensive theoretical analysis examining the properties and
guarantees of the proposed methods. Numerical simulations in an autonomous
driving scenario demonstrate the methods' effectiveness in synthesizing
control-invariant sets offline and implementing model predictive control
online, ensuring safety and recursive feasibility.

</details>

### [895] [Power Allocation for Delay Optimization in Device-to-Device Networks: A Graph Reinforcement Learning Approach](https://arxiv.org/abs/2505.12902)
*Hao Fang,Kai Huang,Hao Ye,Chongtao Guo,Le Liang,Xiao Li,Shi Jin*

Main category: eess.SY

TLDR: 论文提出了一种基于图神经网络（GNN）和强化学习（RL）的新型功率分配方法，用于优化设备间（D2D）通信中的延迟问题，同时兼顾用户公平性。


<details>
  <summary>Details</summary>
Motivation: 无线通信中速率最大化常面临用户公平性挑战，本文旨在解决这一问题。

Method: 采用集中式RL方法，结合GNN层和PPO算法，利用信道状态、数据包延迟等信息优化功率分配。

Result: 仿真结果表明，该方法有效降低平均延迟，确保公平性，优于基线方法，并具备可扩展性和泛化能力。

Conclusion: 该方法为D2D通信中的延迟和公平性问题提供了有效解决方案。

Abstract: The pursuit of rate maximization in wireless communication frequently
encounters substantial challenges associated with user fairness. This paper
addresses these challenges by exploring a novel power allocation approach for
delay optimization, utilizing graph neural networks (GNNs)-based reinforcement
learning (RL) in device-to-device (D2D) communication. The proposed approach
incorporates not only channel state information but also factors such as packet
delay, the number of backlogged packets, and the number of transmitted packets
into the components of the state information. We adopt a centralized RL method,
where a central controller collects and processes the state information. The
central controller functions as an agent trained using the proximal policy
optimization (PPO) algorithm. To better utilize topology information in the
communication network and enhance the generalization of the proposed method, we
embed GNN layers into both the actor and critic networks of the PPO algorithm.
This integration allows for efficient parameter updates of GNNs and enables the
state information to be parameterized as a low-dimensional embedding, which is
leveraged by the agent to optimize power allocation strategies. Simulation
results demonstrate that the proposed method effectively reduces average delay
while ensuring user fairness, outperforms baseline methods, and exhibits
scalability and generalization capability.

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [896] [SpikeX: Exploring Accelerator Architecture and Network-Hardware Co-Optimization for Sparse Spiking Neural Networks](https://arxiv.org/abs/2505.12292)
*Boxun Xu,Richard Boone,Peng Li*

Main category: cs.NE

TLDR: 本文提出了一种名为SpikeX的新型脉冲神经网络（SNN）加速器架构，通过利用时空稀疏性优化硬件效率，并结合网络与硬件协同设计，显著提升了能效和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 尽管SNN在生物启发的计算模型中具有潜力，但其硬件加速器设计的研究相对较少，尤其是如何利用其固有的时空稀疏性来提升效率。

Method: 提出SpikeX架构，通过高效数据流减少内存访问并增加数据共享，同时开发网络与硬件协同优化方法。

Result: SpikeX显著提升了能效和推理延迟，能量延迟积（EDP）降低了15.1x-150.87x，且不影响模型精度。

Conclusion: SpikeX通过协同设计方法为SNN硬件加速器提供了高效解决方案，展示了其在超低功耗和实时处理中的潜力。

Abstract: Spiking Neural Networks (SNNs) are promising biologically plausible models of
computation which utilize a spiking binary activation function similar to that
of biological neurons. SNNs are well positioned to process spatiotemporal data,
and are advantageous in ultra-low power and real-time processing. Despite a
large body of work on conventional artificial neural network accelerators, much
less attention has been given to efficient SNN hardware accelerator design. In
particular, SNNs exhibit inherent unstructured spatial and temporal firing
sparsity, an opportunity yet to be fully explored for great hardware processing
efficiency. In this work, we propose a novel systolic-array SNN accelerator
architecture, called SpikeX, to take on the challenges and opportunities
stemming from unstructured sparsity while taking into account the unique
characteristics of spike-based computation. By developing an efficient dataflow
targeting expensive multi-bit weight data movements, SpikeX reduces memory
access and increases data sharing and hardware utilization for computations
spanning across both time and space, thereby significantly improving energy
efficiency and inference latency. Furthermore, recognizing the importance of
SNN network and hardware co-design, we develop a co-optimization methodology
facilitating not only hardware-aware SNN training but also hardware accelerator
architecture search, allowing joint network weight parameter optimization and
accelerator architectural reconfiguration. This end-to-end network/accelerator
co-design approach offers a significant reduction of 15.1x-150.87x in
energy-delay-product(EDP) without comprising model accuracy.

</details>

<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [897] [Data Sharing with a Generative AI Competitor](https://arxiv.org/abs/2505.12386)
*Boaz Taitler,Omer Madmon,Moshe Tennenholtz,Omer Ben-Porat*

Main category: cs.GT

TLDR: 论文研究了GenAI平台与内容创作公司之间的数据共享问题，通过Stackelberg博弈模型分析，发现公司可能愿意付费共享自身数据，形成昂贵的数据共享均衡。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI平台的发展，其对竞争提供商内容的依赖和替代数据源的访问为数据共享决策带来新挑战。

Method: 采用Stackelberg博弈模型，公司先决定共享多少专有数据，GenAI随后决定从外部专家获取多少额外数据。

Result: 发现公司可能付费共享数据，形成昂贵均衡；并确定了帕累托改进的数据价格范围。

Conclusion: 研究揭示了GenAI时代数据共享的经济动力，为平台、监管者和政策制定者设计有效数据交换机制提供指导。

Abstract: As GenAI platforms grow, their dependence on content from competing
providers, combined with access to alternative data sources, creates new
challenges for data-sharing decisions. In this paper, we provide a model of
data sharing between a content creation firm and a GenAI platform that can also
acquire content from third-party experts. The interaction is modeled as a
Stackelberg game: the firm first decides how much of its proprietary dataset to
share with GenAI, and GenAI subsequently determines how much additional data to
acquire from external experts. Their utilities depend on user traffic, monetary
transfers, and the cost of acquiring additional data from external experts. We
characterize the unique subgame perfect equilibrium of the game and uncover a
surprising phenomenon: The firm may be willing to pay GenAI to share the firm's
own data, leading to a costly data-sharing equilibrium. We further characterize
the set of Pareto improving data prices, and show that such improvements occur
only when the firm pays to share data. Finally, we study how the price can be
set to optimize different design objectives, such as promoting firm data
sharing, expert data acquisition, or a balance of both. Our results shed light
on the economic forces shaping data-sharing partnerships in the age of GenAI,
and provide guidance for platforms, regulators and policymakers seeking to
design effective data exchange mechanisms.

</details>

### [898] [Incentivize Contribution and Learn Parameters Too: Federated Learning with Strategic Data Owners](https://arxiv.org/abs/2505.12010)
*Drashthi Doshi,Aditya Vema Reddy Kesari,Swaprava Nath,Avishek Ghosh,Suhas S Kowshik*

Main category: cs.GT

TLDR: 该论文提出两种激励机制，解决联邦学习中客户端贡献数据的理性问题，并通过实验验证其高效性和性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习未考虑客户端参与和贡献数据的成本与激励问题，导致贡献不足或非最优。

Method: 提出两种机制：1）纳什均衡激励贡献与参数学习；2）预算平衡的货币转移机制，实现全数据贡献与最优学习。

Result: 实验表明，两种机制在真实数据集上快速收敛，提供良好福利保证和模型性能提升。

Conclusion: 同时解决激励与学习问题，优于现有方法，适用于实际联邦学习场景。

Abstract: Classical federated learning (FL) assumes that the clients have a limited
amount of noisy data with which they voluntarily participate and contribute
towards learning a global, more accurate model in a principled manner. The
learning happens in a distributed fashion without sharing the data with the
center. However, these methods do not consider the incentive of an agent for
participating and contributing to the process, given that data collection and
running a distributed algorithm is costly for the clients. The question of
rationality of contribution has been asked recently in the literature and some
results exist that consider this problem. This paper addresses the question of
simultaneous parameter learning and incentivizing contribution, which
distinguishes it from the extant literature. Our first mechanism incentivizes
each client to contribute to the FL process at a Nash equilibrium and
simultaneously learn the model parameters. However, this equilibrium outcome
can be away from the optimal, where clients contribute with their full data and
the algorithm learns the optimal parameters. We propose a second mechanism with
monetary transfers that is budget balanced and enables the full data
contribution along with optimal parameter learning. Large scale experiments
with real (federated) datasets (CIFAR-10, FeMNIST, and Twitter) show that these
algorithms converge quite fast in practice, yield good welfare guarantees, and
better model performance for all agents.

</details>

### [899] [The Hamiltonian of Poly-matrix Zero-sum Games](https://arxiv.org/abs/2505.12609)
*Toshihiro Ota,Yuma Fujimoto*

Main category: cs.GT

TLDR: 论文通过将多矩阵零和博弈中的策略与累积收益作为共轭变量，构建哈密顿函数并揭示其对称性，提出DFTRL动力学以收敛至纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 探索哈密顿动力学在博弈学习动态中的结构特性，揭示其对称性与守恒量。

Method: 将策略与累积收益作为共轭变量构建哈密顿函数，提出DFTRL动力学并引入扰动。

Result: 揭示了系统的概率守恒与Fenchel耦合不变性，DFTRL动力学收敛至纳什均衡。

Conclusion: 哈密顿动力学在博弈论与机器学习中具有广泛应用潜力。

Abstract: Understanding a dynamical system fundamentally relies on establishing an
appropriate Hamiltonian function and elucidating its symmetries. By formulating
agents' strategies and cumulative payoffs as canonically conjugate variables,
we identify the Hamiltonian function that generates the dynamics of poly-matrix
zero-sum games. We reveal the symmetries of our Hamiltonian and derive the
associated conserved quantities, showing how the conservation of probability
and the invariance of the Fenchel coupling are intrinsically encoded within the
system. Furthermore, we propose the dissipation FTRL (DFTRL) dynamics by
introducing a perturbation that dissipates the Fenchel coupling, proving
convergence to the Nash equilibrium and linking DFTRL to last-iterate
convergent algorithms. Our results highlight the potential of Hamiltonian
dynamics in uncovering the structural properties of learning dynamics in games,
and pave the way for broader applications of Hamiltonian dynamics in game
theory and machine learning.

</details>

<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [900] [BioCube: A Multimodal Dataset for Biodiversity Research](https://arxiv.org/abs/2505.11568)
*Stylianos Stasinos,Martino Mensio,Elena Lazovik,Athanasios Trantas*

Main category: q-bio.QM

TLDR: BioCube是一个多模态、细粒度的全球数据集，用于生态和生物多样性研究，整合了物种观测、环境数据和气候变量。


<details>
  <summary>Details</summary>
Motivation: 生物多样性研究需要完整且详细的信息，以研究不同尺度的生态系统动态，而数据驱动方法（如机器学习）需要大型、多模态的数据集。

Method: BioCube整合了物种观测（图像、音频、描述）、环境DNA、植被指数、农业和森林指标以及高分辨率气候变量，所有数据均按WGS84地理坐标系对齐。

Result: BioCube数据集覆盖2000年至2020年，提供了细粒度的空间和时间分辨率。

Conclusion: BioCube为生态和生物多样性研究提供了全面的多模态数据支持，数据集和代码已公开。

Abstract: Biodiversity research requires complete and detailed information to study
ecosystem dynamics at different scales. Employing data-driven methods like
Machine Learning is getting traction in ecology and more specific biodiversity,
offering alternative modelling pathways. For these methods to deliver accurate
results there is the need for large, curated and multimodal datasets that offer
granular spatial and temporal resolutions. In this work, we introduce BioCube,
a multimodal, fine-grained global dataset for ecology and biodiversity
research. BioCube incorporates species observations through images, audio
recordings and descriptions, environmental DNA, vegetation indices,
agricultural, forest, land indicators, and high-resolution climate variables.
All observations are geospatially aligned under the WGS84 geodetic system,
spanning from 2000 to 2020. The dataset will become available at
https://huggingface.co/datasets/BioDT/BioCube while the acquisition and
processing code base at https://github.com/BioDT/bfm-data.

</details>

<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [901] [Fast and Simple Densest Subgraph with Predictions](https://arxiv.org/abs/2505.12600)
*Thai Bui,Hoa T. Vu*

Main category: cs.DS

TLDR: 论文研究了通过学习增强算法解决最密子图问题及其变体，提出了一种基于部分解的线性时间算法，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有算法如Charikar的贪心算法虽然高效但精度有限，而精确解计算复杂度高。通过学习增强方法，利用机器学习预测的部分解，可以显著提升算法性能。

Method: 提出了一种基于机器学习预测部分解的线性时间算法，能够利用部分解快速逼近最优解，并扩展到有向最密子图问题及其他NP难变体。

Result: 在Twitch Ego Nets数据集上的实验表明，该算法优于Charikar的贪心算法和直接返回预测子图的基线方法。

Conclusion: 学习增强算法为最密子图问题提供了高效且高精度的解决方案，具有广泛的应用潜力。

Abstract: We study the densest subgraph problem and its variants through the lens of
learning-augmented algorithms. For this problem, the greedy algorithm by
Charikar (APPROX 2000) provides a linear-time $ 1/2 $-approximation, while
computing the exact solution typically requires solving a linear program or
performing maximum flow computations.We show that given a partial solution,
i.e., one produced by a machine learning classifier that captures at least a $
(1 - \epsilon) $-fraction of nodes in the optimal subgraph, it is possible to
design an extremely simple linear-time algorithm that achieves a provable $ (1
- \epsilon) $-approximation. Our approach also naturally extends to the
directed densest subgraph problem and several NP-hard variants.An experiment on
the Twitch Ego Nets dataset shows that our learning-augmented algorithm
outperforms Charikar's greedy algorithm and a baseline that directly returns
the predicted densest subgraph without additional algorithmic processing.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [902] [Protocol as Poetry: Case Study on Pak's Protocol Arts](https://arxiv.org/abs/2505.12393)
*Botao Amber Hu*

Main category: cs.CY

TLDR: 本文通过分析匿名协议艺术家Pak的作品，提出了协议艺术的七个核心特征，并探讨了其与其他艺术形式的区别。


<details>
  <summary>Details</summary>
Motivation: 现有定义（如Primavera De Filippi的“协议主义”）难以在实践中区分协议艺术与其他艺术形式，本文旨在填补这一理论与实践的鸿沟。

Method: 通过对Pak的协议艺术作品（如The Fungible、Merge、Censored和Not Found）进行主题分析，提炼出七个核心特征。

Result: 识别出协议艺术的七个特征：系统中心性、自治治理、分布式代理、时间动态性、经济驱动、诗意嵌入和互操作性。

Conclusion: 通过基于Pak实践的理论框架，本文为协议艺术的理论发展提供了贡献，并为艺术家设计提供了启示。

Abstract: Protocol art emerges at the confluence of blockchain-based smart contracts
and a century-long lineage of conceptual art, participatory art, and
algorithmic generative art practices. Yet existing definitions-most notably
Primavera De Filippi's "protocolism"-struggle to demarcate this nascent genre
from other art forms in practice. Addressing this definition-to-practice gap,
this paper offers a focused case study of pioneering protocol artworks by Pak,
an early and influential pseudonymous protocol artist who treats smart
contracts as medium and protocol participation as message. Tracing the
evolution from early open-edition releases of The Fungible and the dynamic
mechanics of Merge to the soul-bound messaging of Censored and the reflective
absence of Not Found, we examine how Pak choreographs distributed agency across
collectors and autonomous contracts, showing how programmable protocols become
a social fabric in artistic meaning-making. Through thematic analysis of Pak's
works, we identify seven core characteristics that distinguish protocol art:
(1) system-centric rather than object-centric composition, (2) autonomous
governance for open-ended control, (3) distributed agency and communal
authorship, (4) temporal dynamism and lifecycle aesthetics, (5) economic-driven
engagement, (6) poetic message embedding in interaction rituals, and (7)
interoperability enabling composability for emergence. We then discuss how
these features set protocol art apart from adjacent artistic movements. By
developing a theoretical framework grounded in Pak's practice, we contribute to
the emerging literature on protocolism while offering design implications for
artists shaping this evolving art form.

</details>

### [903] [Recommender Systems for Democracy: Toward Adversarial Robustness in Voting Advice Applications](https://arxiv.org/abs/2505.13329)
*Frédéric Berdoz,Dustin Brunner,Yann Vonlanthen,Roger Wattenhofer*

Main category: cs.CY

TLDR: 本文探讨了投票建议应用（VAAs）在受到敌对实体攻击时对民主过程的潜在风险，揭示了11种操纵策略，并提出了增强其对抗性的框架。


<details>
  <summary>Details</summary>
Motivation: 研究VAAs在敌对攻击下的脆弱性，以保障民主选举的公正性。

Method: 通过分析瑞士主要VAA（Smartvote）的数据，测量11种操纵策略的影响，并提出对抗性鲁棒性属性和评估指标。

Result: 操纵参数可显著改变政党推荐频率（最高达105%），问卷项选择可增加推荐频率261%，微小改动政党回应可增加248%。

Conclusion: 提出增强VAAs对抗性的框架，为未来AI驱动的VAAs提供安全保障。

Abstract: Voting advice applications (VAAs) help millions of voters understand which
political parties or candidates best align with their views. This paper
explores the potential risks these applications pose to the democratic process
when targeted by adversarial entities. In particular, we expose 11 manipulation
strategies and measure their impact using data from Switzerland's primary VAA,
Smartvote, collected during the last two national elections. We find that
altering application parameters, such as the matching method, can shift a
party's recommendation frequency by up to 105%. Cherry-picking questionnaire
items can increase party recommendation frequency by over 261%, while subtle
changes to parties' or candidates' responses can lead to a 248% increase. To
address these vulnerabilities, we propose adversarial robustness properties
VAAs should satisfy, introduce empirical metrics for assessing the resilience
of various matching methods, and suggest possible avenues for research toward
mitigating the effect of manipulation. Our framework is key to ensuring secure
and reliable AI-based VAAs poised to emerge in the near future.

</details>

### [904] [The Accountability Paradox: How Platform API Restrictions Undermine AI Transparency Mandates](https://arxiv.org/abs/2505.11577)
*FLorian A. D. Burnat,Brittany I. Davidson*

Main category: cs.CY

TLDR: 论文分析了社交媒体API限制对欧盟《数字服务法》合规性的挑战，提出了审计框架，揭示了平台内容审核和算法放大的“审计盲点”，并提出政策干预建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估社交媒体平台API限制与欧盟《数字服务法》要求的算法透明性之间的不匹配问题。

Method: 开发了结构化审计框架，对X/Twitter、Reddit、TikTok和Meta进行了比较分析。

Result: 发现平台内容审核和算法放大的“审计盲点”，揭示了“问责悖论”：平台依赖AI系统的同时限制了独立监督能力。

Conclusion: 建议基于NIST的AI风险管理框架，采取联邦访问模型和加强监管执行的政策干预。

Abstract: Recent application programming interface (API) restrictions on major social
media platforms challenge compliance with the EU Digital Services Act [20],
which mandates data access for algorithmic transparency. We develop a
structured audit framework to assess the growing misalignment between
regulatory requirements and platform implementations. Our comparative analysis
of X/Twitter, Reddit, TikTok, and Meta identifies critical ``audit
blind-spots'' where platform content moderation and algorithmic amplification
remain inaccessible to independent verification. Our findings reveal an
``accountability paradox'': as platforms increasingly rely on AI systems, they
simultaneously restrict the capacity for independent oversight. We propose
targeted policy interventions aligned with the AI Risk Management Framework of
the National Institute of Standards and Technology [80], emphasizing federated
access models and enhanced regulatory enforcement.

</details>

### [905] [Toward Adaptive Categories: Dimensional Governance for Agentic AI](https://arxiv.org/abs/2505.11579)
*Zeynep Engin,David Hand*

Main category: cs.CY

TLDR: 论文提出了一种基于动态维度的AI治理框架（3A：决策权、过程自主性和问责制），以替代传统的静态分类治理方法，适应AI系统的动态发展。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从静态工具发展为动态代理，传统的基于固定风险等级或人类监督的分类治理框架已不足以应对其复杂性。

Method: 提出维度治理框架，动态跟踪决策权、过程自主性和问责制的分布，并监控系统跨越治理阈值的行为。

Result: 维度治理能更灵活地适应AI系统的动态变化，实现预风险调整和适应性分类。

Conclusion: 维度治理为AI治理提供了更具弹性和前瞻性的路径，同时保留了分类方法的实用性。

Abstract: As AI systems evolve from static tools to dynamic agents, traditional
categorical governance frameworks -- based on fixed risk tiers, levels of
autonomy, or human oversight models -- are increasingly insufficient on their
own. Systems built on foundation models, self-supervised learning, and
multi-agent architectures increasingly blur the boundaries that categories were
designed to police. In this Perspective, we make the case for dimensional
governance: a framework that tracks how decision authority, process autonomy,
and accountability (the 3As) distribute dynamically across human-AI
relationships. A critical advantage of this approach is its ability to
explicitly monitor system movement toward and across key governance thresholds,
enabling preemptive adjustments before risks materialize. This dimensional
approach provides the necessary foundation for more adaptive categorization,
enabling thresholds and classifications that can evolve with emerging
capabilities. While categories remain essential for decision-making, building
them upon dimensional foundations allows for context-specific adaptability and
stakeholder-responsive governance that static approaches cannot achieve. We
outline key dimensions, critical trust thresholds, and practical examples
illustrating where rigid categorical frameworks fail -- and where a dimensional
mindset could offer a more resilient and future-proof path forward for both
governance and innovation at the frontier of artificial intelligence.

</details>

<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [906] [scSiameseClu: A Siamese Clustering Framework for Interpreting single-cell RNA Sequencing Data](https://arxiv.org/abs/2505.12626)
*Ping Xu,Zhiyuan Ning,Pengjiang Li,Wenhao Liu,Pengyang Wang,Jiaxu Cui,Yuanchun Zhou,Pengfei Wang*

Main category: q-bio.GN

TLDR: scSiameseClu是一种新型的Siamese聚类框架，用于解决scRNA-seq数据中的噪声、稀疏性和高维度问题，通过双增强模块、Siamese融合模块和最优传输聚类，显著提升了聚类性能。


<details>
  <summary>Details</summary>
Motivation: scRNA-seq数据分析面临噪声、稀疏性和高维度的挑战，且现有GNN方法存在过平滑问题，限制了复杂生物信息的捕捉。

Method: 提出scSiameseClu框架，包括双增强模块、Siamese融合模块和最优传输聚类，以增强表示鲁棒性并缓解过平滑。

Result: 在七个真实数据集上的评估表明，scSiameseClu在单细胞聚类、细胞类型注释和分类上优于现有方法。

Conclusion: scSiameseClu为scRNA-seq数据解释提供了强大工具，显著提升了聚类性能。

Abstract: Single-cell RNA sequencing (scRNA-seq) reveals cell heterogeneity, with cell
clustering playing a key role in identifying cell types and marker genes.
Recent advances, especially graph neural networks (GNNs)-based methods, have
significantly improved clustering performance. However, the analysis of
scRNA-seq data remains challenging due to noise, sparsity, and high
dimensionality. Compounding these challenges, GNNs often suffer from
over-smoothing, limiting their ability to capture complex biological
information. In response, we propose scSiameseClu, a novel Siamese Clustering
framework for interpreting single-cell RNA-seq data, comprising of 3 key steps:
(1) Dual Augmentation Module, which applies biologically informed perturbations
to the gene expression matrix and cell graph relationships to enhance
representation robustness; (2) Siamese Fusion Module, which combines
cross-correlation refinement and adaptive information fusion to capture complex
cellular relationships while mitigating over-smoothing; and (3) Optimal
Transport Clustering, which utilizes Sinkhorn distance to efficiently align
cluster assignments with predefined proportions while maintaining balance.
Comprehensive evaluations on seven real-world datasets demonstrate
that~\methodname~outperforms state-of-the-art methods in single-cell
clustering, cell type annotation, and cell type classification, providing a
powerful tool for scRNA-seq data interpretation.

</details>

### [907] [ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data](https://arxiv.org/abs/2505.12638)
*Yifeng Jiao,Yuchen Liu,Yu Zhang,Xin Guo,Yushuai Wu,Chen Jiang,Jiyang Li,Hongwei Zhang,Limei Han,Xin Gao,Yuan Qi,Yuan Cheng*

Main category: q-bio.GN

TLDR: ChromFound是一种针对scATAC-seq数据的基础模型，通过混合架构和基因组感知标记化解决数据高维稀疏性，支持零样本高质量细胞识别和多组学分析。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在单细胞转录组学中成功，但缺乏适用于scATAC-seq的模型，且数据高维稀疏和开放染色质区域标准化表示不足。

Method: 采用混合架构和基因组感知标记化，预训练于30种组织和6种疾病状态的197万细胞。

Result: 在6种任务中表现广泛适用性，零样本生成通用细胞表示，并在细胞类型注释和跨组学预测中表现优异。

Conclusion: ChromFound为理解非编码基因组中的疾病风险变异提供了有前景的框架。

Abstract: The advent of single-cell Assay for Transposase-Accessible Chromatin using
sequencing (scATAC-seq) offers an innovative perspective for deciphering
regulatory mechanisms by assembling a vast repository of single-cell chromatin
accessibility data. While foundation models have achieved significant success
in single-cell transcriptomics, there is currently no foundation model for
scATAC-seq that supports zero-shot high-quality cell identification and
comprehensive multi-omics analysis simultaneously. Key challenges lie in the
high dimensionality and sparsity of scATAC-seq data, as well as the lack of a
standardized schema for representing open chromatin regions (OCRs). Here, we
present \textbf{ChromFound}, a foundation model tailored for scATAC-seq.
ChromFound utilizes a hybrid architecture and genome-aware tokenization to
effectively capture genome-wide long contexts and regulatory signals from
dynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues
and 6 disease conditions, ChromFound demonstrates broad applicability across 6
diverse tasks. Notably, it achieves robust zero-shot performance in generating
universal cell representations and exhibits excellent transferability in cell
type annotation and cross-omics prediction. By uncovering enhancer-gene links
undetected by existing computational methods, ChromFound offers a promising
framework for understanding disease risk variants in the noncoding genome.

</details>

<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [908] [Chatting with Papers: A Hybrid Approach Using LLMs and Knowledge Graphs](https://arxiv.org/abs/2505.11633)
*Vyacheslav Tykhonov,Han Yang,Philipp Mayr,Jetze Touber,Andrea Scharnhorst*

Main category: cs.DL

TLDR: 论文介绍了一种结合大型语言模型和知识图谱的工作流GhostWriter，用于支持文献集的导航和交互式查询。


<details>
  <summary>Details</summary>
Motivation: 旨在通过结合检索增强生成技术，帮助研究人员更高效地浏览和理解文献集，并优化研究问题。

Method: 基于EverythingData工具套件，GhostWriter通过迭代式工作流实现文献集的查询和对话功能。

Result: 应用于GESIS的方法数据分析期刊文献集，展示了其支持研究需求的能力。

Conclusion: GhostWriter为文献集交互提供了灵活的工具，并具有广泛的应用潜力。

Abstract: This demo paper reports on a new workflow \textit{GhostWriter} that combines
the use of Large Language Models and Knowledge Graphs (semantic artifacts) to
support navigation through collections. Situated in the research area of
Retrieval Augmented Generation, this specific workflow details the creation of
local and adaptable chatbots. Based on the tool-suite \textit{EverythingData}
at the backend, \textit{GhostWriter} provides an interface that enables
querying and ``chatting'' with a collection. Applied iteratively, the workflow
supports the information needs of researchers when interacting with a
collection of papers, whether it be to gain an overview, to learn more about a
specific concept and its context, and helps the researcher ultimately to refine
their research question in a controlled way. We demonstrate the workflow for a
collection of articles from the \textit{method data analysis} journal published
by GESIS -- Leibniz-Institute for the Social Sciences. We also point to further
application areas.

</details>

<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [909] [AES-RV: Hardware-Efficient RISC-V Accelerator with Low-Latency AES Instruction Extension for IoT Security](https://arxiv.org/abs/2505.11880)
*Van Tinh Nguyen,Phuc Hung Pham,Vu Trung Duong Le,Hoai Luan Pham,Tuan Hai Vu,Thi Diem Tran*

Main category: cs.AR

TLDR: AES-RV是一种基于RISC-V的高效硬件加速器，通过低延迟AES指令扩展优化实时处理，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有AES硬件加速器在性能、能效和灵活性方面存在不足，需要更高效的解决方案。

Method: AES-RV采用高带宽内部缓冲区、专用AES单元和流水线系统，支持乒乓内存传输机制。

Result: 在Xilinx ZCU102 SoC FPGA上实现，性能提升255.97倍，能效提升453.04倍，优于现有加速器。

Conclusion: AES-RV是嵌入式系统安全和高性能的理想选择。

Abstract: The Advanced Encryption Standard (AES) is a widely adopted cryptographic
algorithm essential for securing embedded systems and IoT platforms. However,
existing AES hardware accelerators often face limitations in performance,
energy efficiency, and flexibility. This paper presents AES-RV, a
hardware-efficient RISC-V accelerator featuring low-latency AES instruction
extensions optimized for real-time processing across all AES modes and key
sizes. AES-RV integrates three key innovations: high-bandwidth internal buffers
for continuous data processing, a specialized AES unit with custom low-latency
instructions, and a pipelined system supported by a ping-pong memory transfer
mechanism. Implemented on the Xilinx ZCU102 SoC FPGA, AES-RV achieves up to
255.97 times speedup and up to 453.04 times higher energy efficiency compared
to baseline and conventional CPU/GPU platforms. It also demonstrates superior
throughput and area efficiency against state-of-the-art AES accelerators,
making it a strong candidate for secure and high-performance embedded systems.

</details>

### [910] [Efficient Implementations of Residue Generators Mod 2n + 1 Providing Diminished-1 Representation](https://arxiv.org/abs/2505.11928)
*Stanisław J. Piestrak,Piotr Patronik*

Main category: cs.AR

TLDR: 提出了一种基于D1表示的低成本奇数模数2n+1的余数生成器架构，适用于RNS系统，并能扩展到共轭模数2n±1。


<details>
  <summary>Details</summary>
Motivation: 低成本的奇数模数2n+1在RNS系统中常用，但需要高效的D1表示余数生成器。

Method: 设计了一种通用的余数生成器架构，初始部分可适应任意p≥4n，最终模块为4操作数加法器。

Result: 该架构不仅能生成D1形式的余数，还能扩展到共轭模数2n±1，节省硬件资源。

Conclusion: 提出的架构高效且通用，适用于RNS系统中的余数生成。

Abstract: The moduli of the form 2n + 1 belong to a class of low-cost odd moduli, which
have been frequently selected to form the basis of various residue number
systems (RNS). The most efficient computations modulo (mod) 2n + 1 are
performed using the so-called diminished-1 (D1) representation. Therefore, it
is desirable that the input converter from the positional number system to RNS
(composed of a set of residue generators) could generate the residues mod 2n +
1 in D1 form. In this paper, we propose the basic architecture of the residue
generator mod 2n + 1 with D1 output. It is universal, because its initial part
can be easily designed for an arbitrary p >= 4n, whereas its final block-the
4-operand adder mod 2n + 1-preserves the same structure for any p. If a pair of
conjugate moduli 2n +/- 1 belongs to the RNS moduli set, the latter
architecture can be easily extended to build p-input bi-residue generators mod
2n+/-1, which not only save hardware by sharing p - 4n full-adders, but also
generate the residue mod 2n + 1 directly in D1 form.

</details>

### [911] [LLM-DSE: Searching Accelerator Parameters with LLM Agents](https://arxiv.org/abs/2505.12188)
*Hanyu Wang,Xinrui Wu,Zijian Ding,Su Zheng,Chengyue Wang,Tony Nowatzki,Yizhou Sun,Jason Cong*

Main category: cs.AR

TLDR: LLM-DSE是一个多智能体框架，用于优化HLS指令参数，结合LLM和设计空间探索，显著提升性能并减少运行时间。


<details>
  <summary>Details</summary>
Motivation: 尽管HLS工具提高了抽象层次，但优化硬件指令参数仍具挑战性，现有方法在适应性和样本效率上表现不足。

Method: LLM-DSE通过四个智能体（Router、Specialists、Arbitrator、Critic）协同工作，结合领域知识和在线学习优化参数。

Result: 在HLSyn数据集上，LLM-DSE性能提升2.55倍，同时发现新设计并减少运行时间。

Conclusion: LLM-DSE通过多智能体交互有效优化HLS指令，代码已开源。

Abstract: Even though high-level synthesis (HLS) tools mitigate the challenges of
programming domain-specific accelerators (DSAs) by raising the abstraction
level, optimizing hardware directive parameters remains a significant hurdle.
Existing heuristic and learning-based methods struggle with adaptability and
sample efficiency.We present LLM-DSE, a multi-agent framework designed
specifically for optimizing HLS directives. Combining LLM with design space
exploration (DSE), our explorer coordinates four agents: Router, Specialists,
Arbitrator, and Critic. These multi-agent components interact with various
tools to accelerate the optimization process. LLM-DSE leverages essential
domain knowledge to identify efficient parameter combinations while maintaining
adaptability through verbal learning from online interactions. Evaluations on
the HLSyn dataset demonstrate that LLM-DSE achieves substantial $2.55\times$
performance gains over state-of-the-art methods, uncovering novel designs while
reducing runtime. Ablation studies validate the effectiveness and necessity of
the proposed agent interactions. Our code is open-sourced here:
https://github.com/Nozidoali/LLM-DSE.

</details>

### [912] [Introducing Instruction-Accurate Simulators for Performance Estimation of Autotuning Workloads](https://arxiv.org/abs/2505.13357)
*Rebecca Pelke,Nils Bosbach,Lennart M. Reimann,Rainer Leupers*

Main category: cs.AR

TLDR: 论文提出了一种通过模拟器执行自动调优工作负载的接口，解决了目标硬件有限的问题，并验证了使用快速指令级模拟器的可行性。通过训练预测器，能够高效预测目标硬件上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习工作负载的优化空间庞大，需要高效方法加速。传统自动调优需在目标硬件上执行，但硬件资源有限。

Method: 提出接口支持在模拟器上执行自动调优工作负载，并训练预测器基于模拟统计预测目标硬件性能。

Result: 预测器效果显著，最佳实现始终位于预测的前3%。在嵌入式架构上，仅需少量并行模拟即可优于原生执行。

Conclusion: 该方法在硬件资源有限时具有高扩展性，且能高效预测目标硬件性能。

Abstract: Accelerating Machine Learning (ML) workloads requires efficient methods due
to their large optimization space. Autotuning has emerged as an effective
approach for systematically evaluating variations of implementations.
Traditionally, autotuning requires the workloads to be executed on the target
hardware (HW). We present an interface that allows executing autotuning
workloads on simulators. This approach offers high scalability when the
availability of the target HW is limited, as many simulations can be run in
parallel on any accessible HW. Additionally, we evaluate the feasibility of
using fast instruction-accurate simulators for autotuning. We train various
predictors to forecast the performance of ML workload implementations on the
target HW based on simulation statistics. Our results demonstrate that the
tuned predictors are highly effective. The best workload implementation in
terms of actual run time on the target HW is always within the top 3 % of
predictions for the tested x86, ARM, and RISC-V-based architectures. In the
best case, this approach outperforms native execution on the target HW for
embedded architectures when running as few as three samples on three simulators
in parallel.

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [913] [Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair](https://arxiv.org/abs/2505.13103)
*Han Zheng,Ilia Shumailov,Tianqi Fan,Aiden Hall,Mathias Payer*

Main category: cs.SE

TLDR: 论文提出了一种基于崩溃点修复和模板引导补丁生成的自动化程序修复方法WILLIAMT，显著降低了大型语言模型的令牌成本，并提高了修复率。


<details>
  <summary>Details</summary>
Motivation: 现代漏洞修复需求激增，但传统方法难以精确分析复杂漏洞的根因，亟需更高效的自动化修复方案。

Method: 采用崩溃点修复简化任务，结合模板引导补丁生成技术，降低LLM令牌成本。

Result: WILLIAMT在ARVO基准测试中，令牌成本降低45.9%，修复率提升至73.5%（+29.6%），且能在本地模型上有效运行。

Conclusion: WILLIAMT展示了广泛的适用性和可扩展性，为自动化程序修复提供了高效解决方案。

Abstract: The rapid advancement of bug-finding techniques has led to the discovery of
more vulnerabilities than developers can reasonably fix, creating an urgent
need for effective Automated Program Repair (APR) methods. However, the
complexity of modern bugs often makes precise root cause analysis difficult and
unreliable. To address this challenge, we propose crash-site repair to simplify
the repair task while still mitigating the risk of exploitation. In addition,
we introduce a template-guided patch generation approach that significantly
reduces the token cost of Large Language Models (LLMs) while maintaining both
efficiency and effectiveness.
  We implement our prototype system, WILLIAMT, and evaluate it against
state-of-the-art APR tools. Our results show that, when combined with the
top-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and
increases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open
source software vulnerabilities benchmark. Furthermore, we demonstrate that
WILLIAMT can function effectively even without access to frontier LLMs: even a
local model running on a Mac M4 Mini achieves a reasonable repair rate. These
findings highlight the broad applicability and scalability of WILLIAMT.

</details>

### [914] [Introduction to Analytical Software Engineering Design Paradigm](https://arxiv.org/abs/2505.11979)
*Tarik Houichime,Younes El Amrani*

Main category: cs.SE

TLDR: 论文提出了一种新的设计范式Analytical Software Engineering (ASE)，通过BSS和ODR框架解决复杂软件工程问题，平衡抽象、工具可访问性、兼容性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在应对现代软件系统的规模和复杂性时表现不足，特别是在设计模式检测和代码重构方面，需要新的解决方案。

Method: ASE通过Behavioral-Structural Sequences (BSS)和Optimized Design Refactoring (ODR)两个框架实现，BSS提供紧凑的代码表示，ODR通过启发式算法优化重构。

Result: ASE有效解决了复杂软件工程问题，BSS支持精确设计模式检测，ODR减少了计算开销。

Conclusion: ASE为未来复杂软件度量的编码和分析奠定了基础。

Abstract: As modern software systems expand in scale and complexity, the challenges
associated with their modeling and formulation grow increasingly intricate.
Traditional approaches often fall short in effectively addressing these
complexities, particularly in tasks such as design pattern detection for
maintenance and assessment, as well as code refactoring for optimization and
long-term sustainability. This growing inadequacy underscores the need for a
paradigm shift in how such challenges are approached and resolved. This paper
presents Analytical Software Engineering (ASE), a novel design paradigm aimed
at balancing abstraction, tool accessibility, compatibility, and scalability.
ASE enables effective modeling and resolution of complex software engineering
problems. The paradigm is evaluated through two frameworks
Behavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR),
both developed in accordance with ASE principles. BSS offers a compact,
language-agnostic representation of codebases to facilitate precise design
pattern detection. ODR unifies artifact and solution representations to
optimize code refactoring via heuristic algorithms while eliminating iterative
computational overhead. By providing a structured approach to software design
challenges, ASE lays the groundwork for future research in encoding and
analyzing complex software metrics.

</details>

### [915] [EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective](https://arxiv.org/abs/2505.12185)
*Sen Fang,Weiyuan Ding,Bowen Xu*

Main category: cs.SE

TLDR: EVALOOP是一种新的评估框架，通过自一致性循环评估大型语言模型在编程任务中的鲁棒性，发现其性能在多次循环后会显著下降。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法主要关注静态基准测试的准确性，忽视了模型在编程任务中的鲁棒性，而现有的对抗攻击方法效果有限且不一致。

Method: EVALOOP通过自反馈循环（如代码生成与代码摘要的循环）评估模型鲁棒性，无需外部攻击设置。

Result: 在16个主流LLM上测试，EVALOOP导致pass@1性能下降5.01%-19.31%，且鲁棒性与初始性能不总是正相关。

Conclusion: EVALOOP提供了一种统一且内在的鲁棒性评估方法，揭示了模型在多次循环中的性能变化。

Abstract: Assessing the programming capabilities of Large Language Models (LLMs) is
crucial for their effective use in software engineering. Current evaluations,
however, predominantly measure the accuracy of generated code on static
benchmarks, neglecting the critical aspect of model robustness during
programming tasks. While adversarial attacks offer insights on model
robustness, their effectiveness is limited and evaluation could be constrained.
Current adversarial attack methods for robustness evaluation yield inconsistent
results, struggling to provide a unified evaluation across different LLMs. We
introduce EVALOOP, a novel assessment framework that evaluate the robustness
from a self-consistency perspective, i.e., leveraging the natural duality
inherent in popular software engineering tasks, e.g., code generation and code
summarization. EVALOOP initiates a self-contained feedback loop: an LLM
generates output (e.g., code) from an input (e.g., natural language
specification), and then use the generated output as the input to produce a new
output (e.g., summarizes that code into a new specification). EVALOOP repeats
the process to assess the effectiveness of EVALOOP in each loop. This cyclical
strategy intrinsically evaluates robustness without rely on any external attack
setups, providing a unified metric to evaluate LLMs' robustness in programming.
We evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found
that EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1
performance within ten loops. Intriguingly, robustness does not always align
with initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,
despite superior initial code generation compared to DeepSeek-V2, demonstrated
lower robustness over repeated evaluation loop.

</details>

### [916] [AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models](https://arxiv.org/abs/2505.12900)
*Shuyang Hou,Zhangxiao Shen,Huayi Wu,Jianyuan Liang,Haoyue Jiao,Yaxian Qing,Xiaopu Zhang,Xu Li,Zhipeng Gui,Xuefeng Guan,Longgang Xiang*

Main category: cs.SE

TLDR: AutoGEEval是一个基于Google Earth Engine的多模态、单元级自动化评估框架，用于地理空间代码生成任务，填补了该领域标准化工具的空白。


<details>
  <summary>Details</summary>
Motivation: 地理空间代码生成在AI与地理科学分析结合中日益重要，但缺乏自动化评估工具。

Method: 基于GEE Python API，构建包含1325个测试用例的基准套件，集成问题生成和答案验证组件，支持多维定量分析。

Result: 评估了18种先进LLM，揭示了它们在GEE代码生成中的性能特点和优化路径。

Conclusion: AutoGEEval为地理空间代码生成模型的开发和评估提供了统一协议和基础资源，推动了自然语言到领域特定代码的自动化翻译。

Abstract: Geospatial code generation is emerging as a key direction in the integration
of artificial intelligence and geoscientific analysis. However, there remains a
lack of standardized tools for automatic evaluation in this domain. To address
this gap, we propose AutoGEEval, the first multimodal, unit-level automated
evaluation framework for geospatial code generation tasks on the Google Earth
Engine (GEE) platform powered by large language models (LLMs). Built upon the
GEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)
comprising 1325 test cases that span 26 GEE data types. The framework
integrates both question generation and answer verification components to
enable an end-to-end automated evaluation pipeline-from function invocation to
execution validation. AutoGEEval supports multidimensional quantitative
analysis of model outputs in terms of accuracy, resource consumption, execution
efficiency, and error types. We evaluate 18 state-of-the-art LLMs-including
general-purpose, reasoning-augmented, code-centric, and geoscience-specialized
models-revealing their performance characteristics and potential optimization
pathways in GEE code generation. This work provides a unified protocol and
foundational resource for the development and assessment of geospatial code
generation models, advancing the frontier of automated natural language to
domain-specific code translation.

</details>

### [917] [EvoGPT: Enhancing Test Suite Robustness via LLM-Based Generation and Genetic Optimization](https://arxiv.org/abs/2505.12424)
*Lior Broide,Roni Stern*

Main category: cs.SE

TLDR: EvoGPT结合大型语言模型（LLM）和进化搜索技术，生成多样化且能揭示错误的单元测试，相比传统方法在代码覆盖率和变异分数上平均提升10%。


<details>
  <summary>Details</summary>
Motivation: 单元测试的主要目标是错误检测，但传统方法在多样性和有效性上存在不足。EvoGPT旨在通过LLM和进化搜索的结合提升测试效果。

Method: EvoGPT采用混合框架，首先生成多样化的测试用例，再通过生成-修复循环和覆盖率引导的断言增强优化测试，最后用遗传算法进化测试套件。

Result: 在多个开源Java项目上评估，EvoGPT在代码覆盖率和变异分数上平均提升10%。

Conclusion: 结合LLM驱动的多样性、针对性修复和进化优化，能生成更有效和健壮的测试套件。

Abstract: Large Language Models (LLMs) have recently emerged as promising tools for
automated unit test generation. We introduce a hybrid framework called EvoGPT
that integrates LLM-based test generation with evolutionary search techniques
to create diverse, fault-revealing unit tests. Unit tests are initially
generated with diverse temperature sampling to maximize behavioral and test
suite diversity, followed by a generation-repair loop and coverage-guided
assertion enhancement. The resulting test suites are evolved using genetic
algorithms, guided by a fitness function prioritizing mutation score over
traditional coverage metrics. This design emphasizes the primary objective of
unit testing-fault detection. Evaluated on multiple open-source Java projects,
EvoGPT achieves an average improvement of 10% in both code coverage and
mutation score compared to LLMs and traditional search-based software testing
baselines. These results demonstrate that combining LLM-driven diversity,
targeted repair, and evolutionary optimization produces more effective and
resilient test suites.

</details>

### [918] [Enhancing Code Quality with Generative AI: Boosting Developer Warning Compliance](https://arxiv.org/abs/2505.11677)
*Hansen Chang,Christian DeLozier*

Main category: cs.SE

TLDR: 程序员常因误报问题忽视静态分析工具的警告，而大语言模型可简化警告、解释严重性并提供修复建议以提高开发者修复意愿。


<details>
  <summary>Details</summary>
Motivation: 程序员常忽视警告，尤其是静态分析工具生成的警告，因其可能包含误报或难以理解的重要性。

Method: 利用大语言模型简化警告信息，解释其严重性，并提供修复建议。

Result: 通过模型干预，提高开发者对警告的重视和修复意愿。

Conclusion: 大语言模型能有效提升开发者对警告的响应，减少潜在漏洞。

Abstract: Programmers have long ignored warnings, especially those generated by static
analysis tools, due to the potential for false-positives. In some cases,
warnings may be indicative of larger issues, but programmers may not understand
how a seemingly unimportant warning can grow into a vulnerability. Because
these messages tend to be long and confusing, programmers tend to ignore them
if they do not cause readily identifiable issues. Large language models can
simplify these warnings, explain the gravity of important warnings, and suggest
potential fixes to increase developer compliance with fixing warnings.

</details>

### [919] [OSS-Bench: Benchmark Generator for Coding LLMs](https://arxiv.org/abs/2505.12331)
*Yuancheng Jiang,Roland Yap,Zhenkai Liang*

Main category: cs.SE

TLDR: 介绍了OSS-Bench，一个自动从开源软件生成大规模实时评估任务的基准测试工具，用于评估LLM生成代码的质量。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试需要大量人工创建静态数据集，任务间接或不具挑战性，且缺乏对低级别安全性的评估，特别是内存安全问题。

Method: OSS-Bench通过替换开源软件中的函数为LLM生成代码，并使用编译性、功能正确性和内存安全性三个指标进行评估。

Result: 评估了17种LLM，揭示了模型家族内部行为模式和模型大小与性能不一致性，并发现LLM对低级别代码安全性的理解有限。

Conclusion: OSS-Bench提供了一个实用且可扩展的框架，用于评估LLM在真实世界中的编码能力。

Abstract: In light of the rapid adoption of AI coding assistants, LLM-assisted
development has become increasingly prevalent, creating an urgent need for
robust evaluation of generated code quality. Existing benchmarks often require
extensive manual effort to create static datasets, rely on indirect or
insufficiently challenging tasks, depend on non-scalable ground truth, or
neglect critical low-level security evaluations, particularly memory-safety
issues. In this work, we introduce OSS-Bench, a benchmark generator that
automatically constructs large-scale, live evaluation tasks from real-world
open-source software. OSS-Bench replaces functions with LLM-generated code and
evaluates them using three natural metrics: compilability, functional
correctness, and memory safety, leveraging robust signals like compilation
failures, test-suite violations, and sanitizer alerts as ground truth. In our
evaluation, the benchmark, instantiated as OSS-Bench(php) and OSS-Bench(sql),
profiles 17 diverse LLMs, revealing insights such as intra-family behavioral
patterns and inconsistencies between model size and performance. Our results
demonstrate that OSS-Bench mitigates overfitting by leveraging the evolving
complexity of OSS and highlights LLMs' limited understanding of low-level code
security via extended fuzzing experiments. Overall, OSS-Bench offers a
practical and scalable framework for benchmarking the real-world coding
capabilities of LLMs.

</details>

### [920] [CPRet: A Dataset, Benchmark, and Model for Retrieval in Competitive Programming](https://arxiv.org/abs/2505.12925)
*Han Deng,Yuan Meng,Shixiang Tang,Wanli Ouyang,Xinzhu Ma*

Main category: cs.SE

TLDR: 提出CPRet基准套件解决竞争编程中重复或相似问题的问题，包括四种检索任务，并提供高质量数据和模型。


<details>
  <summary>Details</summary>
Motivation: 竞争编程中重复或相似问题影响公平性和模型评估有效性，需解决。

Method: 构建CPRet基准套件，含四种检索任务，开发两种专用检索模型。

Result: CPRetriever-Code和CPRetriever-Prob表现优异，开源数据和模型。

Conclusion: 相似问题影响模型评估，需相似性感知的未来基准。

Abstract: Competitive programming benchmarks are widely used in scenarios such as
programming contests and large language model assessments. However, the growing
presence of duplicate or highly similar problems raises concerns not only about
competition fairness, but also about the validity of competitive programming as
a benchmark for model evaluation. In this paper, we propose a new problem --
similar question retrieval -- to address this issue. Due to the lack of both
data and models, solving this problem is challenging. To this end, we introduce
CPRet, a retrieval-oriented benchmark suite for competitive programming,
covering four retrieval tasks: two code-centric (i.e., Text-to-Code and
Code-to-Code) and two newly proposed problem-centric tasks (i.e.,
Problem-to-Duplicate and Simplified-to-Full), built from a combination of
automatically crawled problem-solution data and manually curated annotations.
Our contribution includes both high-quality training data and temporally
separated test sets for reliable evaluation. In addition, we develop two
task-specialized retrievers based on this dataset: CPRetriever-Code, trained
with a novel Group-InfoNCE loss for problem-code alignment, and
CPRetriever-Prob, fine-tuned for identifying problem-level similarity. Both
models achieve strong results and are open-sourced for local use. Finally, we
analyze LiveCodeBench and find that high-similarity problems inflate model pass
rates and reduce differentiation, underscoring the need for similarity-aware
evaluation in future benchmarks.
  Code and data are available at: https://github.com/coldchair/CPRet

</details>

### [921] [Structure-Aware Corpus Construction and User-Perception-Aligned Metrics for Large-Language-Model Code Completion](https://arxiv.org/abs/2505.13073)
*Dengfeng Liu,Jucai Zhai,Xiaoguang Jiang,Ziqun Li,Qianjin Yu,Feng Liu,Rui Ye,Huang Liu,Zhiguo Yang,Yongsheng Du,Fang Tan*

Main category: cs.SE

TLDR: 论文提出两种代码补全评估指标LCP和ROUGE-LCP，以及一种基于SPSR-Graph的数据处理方法，以提升用户感知一致性和模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前代码补全评估指标与用户实际感知存在差距，且LLMs在仓库级代码补全中缺乏有效的结构语义建模和跨模块依赖信息。

Method: 提出LCP和ROUGE-LCP评估指标，以及基于SPSR-Graph的数据处理方法。

Result: 实验验证了评估指标的用户感知一致性优势，以及数据处理方法对模型性能的提升。

Conclusion: 提出的指标和方法有效解决了现有问题，提升了代码补全的实用性和性能。

Abstract: Code completion technology based on large language model has significantly
improved the development efficiency of programmers. However, in practical
applications, there remains a gap between current commonly used code completion
evaluation metrics and users' actual perception. To address this issue, we
propose two evaluation metrics for code completion tasks--LCP and ROUGE-LCP,
from the perspective of probabilistic modeling. Furthermore, to tackle the lack
of effective structural semantic modeling and cross-module dependency
information in LLMs for repository-level code completion scenarios, we propose
a data processing method based on a Structure-Preserving and
Semantically-Reordered Code Graph (SPSR-Graph). Through theoretical analysis
and experimental validation, we demonstrate the superiority of the proposed
evaluation metrics in terms of user perception consistency, as well as the
effectiveness of the data processing method in enhancing model performance.

</details>

<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [922] [PeerGuard: Defending Multi-Agent Systems Against Backdoor Attacks Through Mutual Reasoning](https://arxiv.org/abs/2505.11642)
*Falong Fan,Xi Li*

Main category: cs.MA

TLDR: 研究多智能体系统中的后门漏洞，提出基于交互的防御机制，通过逻辑推理检测中毒智能体。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统的安全性研究不足，现有工作多关注单一AI模型而非交互智能体。

Method: 利用智能体的推理能力，通过评估其他智能体的响应检测异常逻辑。

Result: 在基于LLM的多智能体系统（如ChatGPT和Llama 3）中验证了方法的有效性，能高精度识别中毒智能体且误报率低。

Conclusion: 该研究为多智能体系统安全性提供了新思路，有助于开发更稳健、可信的AI交互。

Abstract: Multi-agent systems leverage advanced AI models as autonomous agents that
interact, cooperate, or compete to complete complex tasks across applications
such as robotics and traffic management. Despite their growing importance,
safety in multi-agent systems remains largely underexplored, with most research
focusing on single AI models rather than interacting agents. This work
investigates backdoor vulnerabilities in multi-agent systems and proposes a
defense mechanism based on agent interactions. By leveraging reasoning
abilities, each agent evaluates responses from others to detect illogical
reasoning processes, which indicate poisoned agents. Experiments on LLM-based
multi-agent systems, including ChatGPT series and Llama 3, demonstrate the
effectiveness of the proposed method, achieving high accuracy in identifying
poisoned agents while minimizing false positives on clean agents. We believe
this work provides insights into multi-agent system safety and contributes to
the development of robust, trustworthy AI interactions.

</details>

### [923] [OMAC: A Broad Optimization Framework for LLM-Based Multi-Agent Collaboration](https://arxiv.org/abs/2505.11765)
*Shijun Li,Hilaf Hasson,Joydeep Ghosh*

Main category: cs.MA

TLDR: OMAC是一个用于优化基于LLM的多代理系统（MAS）的通用框架，通过五个关键维度优化代理功能和协作结构，实验证明其在代码生成、算术推理等任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多代理系统开发依赖手工方法，缺乏系统性设计和优化的研究，因此提出OMAC框架以填补这一空白。

Method: 提出五个关键优化维度，并设计两种算法：一种针对单一维度优化（语义初始化器和对比比较器），另一种用于多维度联合优化。

Result: 实验表明OMAC在代码生成、算术推理和通用推理任务中优于现有方法。

Conclusion: OMAC为LLM-based MAS的系统性优化提供了有效框架，显著提升了性能。

Abstract: Agents powered by advanced large language models (LLMs) have demonstrated
impressive capabilities across diverse complex applications. Recently,
Multi-Agent Systems (MAS), wherein multiple agents collaborate and communicate
with each other, have exhibited enhanced capabilities in complex tasks, such as
high-quality code generation and arithmetic reasoning. However, the development
of such systems often relies on handcrafted methods, and the literature on
systematic design and optimization of LLM-based MAS remains limited.
  In this work, we introduce OMAC, a general framework designed for holistic
optimization of LLM-based MAS. Specifically, we identify five key optimization
dimensions for MAS, encompassing both agent functionality and collaboration
structure. Building upon these dimensions, we first propose a general
algorithm, utilizing two actors termed the Semantic Initializer and the
Contrastive Comparator, to optimize any single dimension. Then, we present an
algorithm for joint optimization across multiple dimensions. Extensive
experiments demonstrate the superior performance of OMAC on code generation,
arithmetic reasoning, and general reasoning tasks against state-of-the-art
approaches.

</details>

### [924] [IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar](https://arxiv.org/abs/2505.13393)
*Christopher K. Frantz*

Main category: cs.MA

TLDR: IG Parser是一款用于定性内容分析的软件，支持对正式和非正式规则（即“制度”）的分析，并通过独特语法实现自然语言的严格编码和自动化转换。


<details>
  <summary>Details</summary>
Motivation: 为了支持社会系统中制度的分析，提供一种工具来解析和转换制度语言，便于下游分析。

Method: 采用IG Script语法，基于Institutional Grammar 2.0的理论框架，实现自然语言的编码和格式转换。

Result: IG Parser能够高效解析制度语言，并生成多种格式以支持多样化分析技术。

Conclusion: IG Parser是一个强大的工具，为制度分析提供了理论基础和实用功能，并通过示例展示了其应用价值。

Abstract: This article provides an overview of IG Parser, a software that facilitates
qualitative content analysis of formal (e.g., legal) rules or informal (e.g.,
socio-normative) norms, and strategies (such as conventions) -- referred to as
\emph{institutions} -- that govern social systems and operate configurally to
describe \emph{institutional systems}. To this end, the IG Parser employs a
distinctive syntax that ensures rigorous encoding of natural language, while
automating the transformation into various formats that support the downstream
analysis using diverse analytical techniques. The conceptual core of the IG
Parser is an associated syntax, IG Script, that operationalizes the conceptual
foundations of the Institutional Grammar, and more specifically Institutional
Grammar 2.0, an analytical paradigm for institutional analysis. This article
presents the IG Parser, including its conceptual foundations, syntactic
specification of IG Script, alongside architectural principles. This
introduction is augmented with selective illustrative examples that highlight
the use and benefit associated with the tool.

</details>

### [925] [Beyond Frameworks: Unpacking Collaboration Strategies in Multi-Agent Systems](https://arxiv.org/abs/2505.12467)
*Haochun Wang,Sendong Zhao,Jingbo Wang,Zewen Qiang,Bing Qin,Ting Liu*

Main category: cs.MA

TLDR: 该论文研究了多智能体协作中的四个关键策略维度，并通过实验验证了集中治理、导师引导参与、有序交互模式和导师整理上下文摘要对任务准确性和计算效率的优化作用。


<details>
  <summary>Details</summary>
Motivation: 多智能体协作在LLM驱动的应用中至关重要，但现有研究多关注高层架构，而忽略了影响性能和可扩展性的细粒度机制。

Method: 系统研究了四个协作策略维度（治理、参与控制、交互动态、对话历史管理），并在DEI和SES两种场景下进行实验。

Result: 发现集中治理、导师引导参与、有序交互和上下文摘要能优化任务准确性与计算效率的权衡。

Conclusion: 为设计自适应、可扩展的多智能体系统奠定了基础，将研究重点从结构创新转向策略交互机制。

Abstract: Multi-agent collaboration has emerged as a pivotal paradigm for addressing
complex, distributed tasks in large language model (LLM)-driven applications.
While prior research has focused on high-level architectural frameworks, the
granular mechanisms governing agents, critical to performance and scalability,
remain underexplored. This study systematically investigates four dimensions of
collaboration strategies: (1) agent governance, (2) participation control, (3)
interaction dynamics, and (4) dialogue history management. Through rigorous
experimentation under two context-dependent scenarios: Distributed Evidence
Integration (DEI) and Structured Evidence Synthesis (SES), we quantify the
impact of these strategies on both task accuracy and computational efficiency.
Our findings reveal that centralized governance, instructor-led participation,
ordered interaction patterns, and instructor-curated context summarization
collectively optimize the trade-off between decision quality and resource
utilization with the support of the proposed Token-Accuracy Ratio (TAR). This
work establishes a foundation for designing adaptive, scalable multi-agent
systems, shifting the focus from structural novelty to strategic interaction
mechanics.

</details>

### [926] [Lightweight and Effective Preference Construction in PIBT for Large-Scale Multi-Agent Pathfinding](https://arxiv.org/abs/2505.12623)
*Keisuke Okumura,Hiroki Nagai*

Main category: cs.MA

TLDR: PIBT是一种轻量级算法，用于多智能体路径规划（MAPF），通过两种简单的技术改进其性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何在PIBT中通过智能决策和集体学习优化路径规划，以提升解决方案的质量。

Method: 提出两种技术：1）智能避让；2）通过多次运行学习最小化集体遗憾。

Result: 实验表明，这些技术能显著降低路径成本，提升效率，尤其在密集场景中效果显著。

Conclusion: 两种技术在不影响计算速度的前提下，显著提升了PIBT的性能。

Abstract: PIBT is a computationally lightweight algorithm that can be applied to a
variety of multi-agent pathfinding (MAPF) problems, generating the next
collision-free locations of agents given another. Because of its simplicity and
scalability, it is becoming a popular underlying scheme for recent large-scale
MAPF methods involving several hundreds or thousands of agents. Vanilla PIBT
makes agents behave greedily towards their assigned goals, while agents
typically have multiple best actions, since the graph shortest path is not
always unique. Consequently, tiebreaking about how to choose between these
actions significantly affects resulting solutions. This paper studies two
simple yet effective techniques for tiebreaking in PIBT, without compromising
its computational advantage. The first technique allows an agent to
intelligently dodge another, taking into account whether each action will
hinder the progress of the next timestep. The second technique is to learn,
through multiple PIBT runs, how an action causes regret in others and to use
this information to minimise regret collectively. Our empirical results
demonstrate that these techniques can reduce the solution cost of one-shot MAPF
and improve the throughput of lifelong MAPF. For instance, in densely populated
one-shot cases, the combined use of these tiebreaks achieves improvements of
around 10-20% in sum-of-costs, without significantly compromising the speed of
a PIBT-based planner.

</details>

### [927] [Dynamic Sight Range Selection in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.12811)
*Wei-Chen Liao,Ti-Rong Wu,I-Chen Wu*

Main category: cs.MA

TLDR: 提出动态视野范围选择（DSR）方法，通过UCB算法动态调整智能体的视野范围，解决多智能体强化学习中的视野范围困境。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中，智能体的视野范围问题（信息不足或过多）影响性能，需一种动态调整方法。

Method: 使用UCB算法动态调整训练中的视野范围（DSR）。

Result: 在LBF、RWARE、SMAC等环境中表现更优，兼容QMIX和MAPPO算法，加速训练并提供可解释性。

Conclusion: DSR为视野范围问题提供高效解决方案，适用于复杂现实环境。

Abstract: Multi-agent reinforcement Learning (MARL) is often challenged by the sight
range dilemma, where agents either receive insufficient or excessive
information from their environment. In this paper, we propose a novel method,
called Dynamic Sight Range Selection (DSR), to address this issue. DSR utilizes
an Upper Confidence Bound (UCB) algorithm and dynamically adjusts the sight
range during training. Experiment results show several advantages of using DSR.
First, we demonstrate using DSR achieves better performance in three common
MARL environments, including Level-Based Foraging (LBF), Multi-Robot Warehouse
(RWARE), and StarCraft Multi-Agent Challenge (SMAC). Second, our results show
that DSR consistently improves performance across multiple MARL algorithms,
including QMIX and MAPPO. Third, DSR offers suitable sight ranges for different
training steps, thereby accelerating the training process. Finally, DSR
provides additional interpretability by indicating the optimal sight range used
during training. Unlike existing methods that rely on global information or
communication mechanisms, our approach operates solely based on the individual
sight ranges of agents. This approach offers a practical and efficient solution
to the sight range dilemma, making it broadly applicable to real-world complex
environments.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [928] [MedVKAN: Efficient Feature Extraction with Mamba and KAN for Medical Image Segmentation](https://arxiv.org/abs/2505.11797)
*Hancan Zhu,Jinhao Chen,Guanghua He*

Main category: eess.IV

TLDR: MedVKAN结合Mamba和KAN，提出了一种高效的医学图像分割模型，解决了CNN和Transformer的局限性，并在多个数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: CNN的有限感受野和Transformer的二次计算复杂度限制了医学图像分割的性能，需要更高效的架构。

Method: 提出MedVKAN模型，整合Mamba（线性复杂度）和KAN（可学习激活函数），并设计了EFC-KAN和VKAN模块。

Result: 在五个公开数据集上，MedVKAN在四个数据集上表现最优，另一个排名第二。

Conclusion: MedVKAN展示了Mamba和KAN在医学图像分割中的潜力，提供了一种高效的特征提取框架。

Abstract: Medical image segmentation relies heavily on convolutional neural networks
(CNNs) and Transformer-based models. However, CNNs are constrained by limited
receptive fields, while Transformers suffer from scalability challenges due to
their quadratic computational complexity. To address these limitations, recent
advances have explored alternative architectures. The state-space model Mamba
offers near-linear complexity while capturing long-range dependencies, and the
Kolmogorov-Arnold Network (KAN) enhances nonlinear expressiveness by replacing
fixed activation functions with learnable ones. Building on these strengths, we
propose MedVKAN, an efficient feature extraction model integrating Mamba and
KAN. Specifically, we introduce the EFC-KAN module, which enhances KAN with
convolutional operations to improve local pixel interaction. We further design
the VKAN module, integrating Mamba with EFC-KAN as a replacement for
Transformer modules, significantly improving feature extraction. Extensive
experiments on five public medical image segmentation datasets show that
MedVKAN achieves state-of-the-art performance on four datasets and ranks second
on the remaining one. These results validate the potential of Mamba and KAN for
medical image segmentation while introducing an innovative and computationally
efficient feature extraction framework. The code is available at:
https://github.com/beginner-cjh/MedVKAN.

</details>

### [929] [Patient-Specific Autoregressive Models for Organ Motion Prediction in Radiotherapy](https://arxiv.org/abs/2505.11832)
*Yuxiang Lai,Jike Zhong,Vanessa Su,Xiaofeng Yang*

Main category: eess.IV

TLDR: 论文提出了一种基于自回归模型的器官运动预测方法，用于放疗前精准预测器官运动，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 放疗期间器官运动影响辐射精度，现有方法依赖PCA，难以捕捉周期性动态。

Method: 将器官运动预测建模为自回归过程，利用4D CT扫描数据预测未来运动相位。

Result: 在50名患者和公开数据集上测试，预测肺和心脏运动性能优于基准。

Conclusion: 该方法有望提升放疗前规划精度，实现更精准的辐射治疗。

Abstract: Radiotherapy often involves a prolonged treatment period. During this time,
patients may experience organ motion due to breathing and other physiological
factors. Predicting and modeling this motion before treatment is crucial for
ensuring precise radiation delivery. However, existing pre-treatment organ
motion prediction methods primarily rely on deformation analysis using
principal component analysis (PCA), which is highly dependent on registration
quality and struggles to capture periodic temporal dynamics for motion
modeling.In this paper, we observe that organ motion prediction closely
resembles an autoregressive process, a technique widely used in natural
language processing (NLP). Autoregressive models predict the next token based
on previous inputs, naturally aligning with our objective of predicting future
organ motion phases. Building on this insight, we reformulate organ motion
prediction as an autoregressive process to better capture patient-specific
motion patterns. Specifically, we acquire 4D CT scans for each patient before
treatment, with each sequence comprising multiple 3D CT phases. These phases
are fed into the autoregressive model to predict future phases based on prior
phase motion patterns. We evaluate our method on a real-world test set of 4D CT
scans from 50 patients who underwent radiotherapy at our institution and a
public dataset containing 4D CT scans from 20 patients (some with multiple
scans), totaling over 1,300 3D CT phases. The performance in predicting the
motion of the lung and heart surpasses existing benchmarks, demonstrating its
effectiveness in capturing motion dynamics from CT images. These results
highlight the potential of our method to improve pre-treatment planning in
radiotherapy, enabling more precise and adaptive radiation delivery.

</details>

### [930] [Bridging the Inter-Domain Gap through Low-Level Features for Cross-Modal Medical Image Segmentation](https://arxiv.org/abs/2505.11909)
*Pengfei Lyu,Pak-Hei Yeung,Xiaosheng Yu,Jing Xia,Jianning Chi,Chengdong Wu,Jagath C. Rajapakse*

Main category: eess.IV

TLDR: 提出了一种模型无关的无监督域适应框架LowBridge，通过利用跨模态图像的低级特征（如边缘）实现医学图像分割。


<details>
  <summary>Details</summary>
Motivation: 解决跨模态医学图像分割中的域适应问题，利用低级特征的相似性。

Method: 训练生成模型从边缘特征恢复源图像，再训练分割模型；测试时用目标图像的边缘特征生成源风格图像并分割。

Result: 在多个数据集上表现优于11种现有方法，且对不同生成和分割模型具有普适性。

Conclusion: LowBridge简单有效，可灵活结合先进模型，未来潜力大。

Abstract: This paper addresses the task of cross-modal medical image segmentation by
exploring unsupervised domain adaptation (UDA) approaches. We propose a
model-agnostic UDA framework, LowBridge, which builds on a simple observation
that cross-modal images share some similar low-level features (e.g., edges) as
they are depicting the same structures. Specifically, we first train a
generative model to recover the source images from their edge features,
followed by training a segmentation model on the generated source images,
separately. At test time, edge features from the target images are input to the
pretrained generative model to generate source-style target domain images,
which are then segmented using the pretrained segmentation network. Despite its
simplicity, extensive experiments on various publicly available datasets
demonstrate that \proposed achieves state-of-the-art performance, outperforming
eleven existing UDA approaches under different settings. Notably, further
ablation studies show that \proposed is agnostic to different types of
generative and segmentation models, suggesting its potential to be seamlessly
plugged with the most advanced models to achieve even more outstanding results
in the future. The code is available at https://github.com/JoshuaLPF/LowBridge.

</details>

### [931] [NTIRE 2025 Challenge on Efficient Burst HDR and Restoration: Datasets, Methods, and Results](https://arxiv.org/abs/2505.12089)
*Sangmin Lee,Eunpil Park,Angel Canelo,Hyunhee Park,Youngjo Kim,Hyung-Ju Chun,Xin Jin,Chongyi Li,Chun-Le Guo,Radu Timofte,Qi Wu,Tianheng Qiu,Yuchun Dong,Shenglin Ding,Guanghua Pan,Weiyu Zhou,Tao Hu,Yixu Feng,Duwei Dai,Yu Cao,Peng Wu,Wei Dong,Yanning Zhang,Qingsen Yan,Simon J. Larsen,Ruixuan Jiang,Senyan Xu,Xingbo Wang,Xin Lu,Marcos V. Conde,Javier Abad-Hernandez,Alvaro Garcıa-Lara,Daniel Feijoo,Alvaro Garcıa,Zeyu Xiao,Zhuoyuan Li*

Main category: eess.IV

TLDR: NTIRE 2025挑战赛聚焦高效多帧HDR与修复技术，基于RAW多帧融合数据集，要求参赛者在严格效率限制下开发解决方案。


<details>
  <summary>Details</summary>
Motivation: 推动高效多帧HDR与修复技术的发展，解决噪声和未对齐RAW帧的融合问题。

Method: 使用包含9帧不同曝光RAW数据的挑战数据集，参赛者需在参数和计算量限制下设计模型。

Result: 6支团队提交方案，最佳方案PSNR达43.22 dB，展示了新方法的潜力。

Conclusion: 挑战赛为高效HDR与修复领域提供了有价值的参考和比较。

Abstract: This paper reviews the NTIRE 2025 Efficient Burst HDR and Restoration
Challenge, which aims to advance efficient multi-frame high dynamic range (HDR)
and restoration techniques. The challenge is based on a novel RAW multi-frame
fusion dataset, comprising nine noisy and misaligned RAW frames with various
exposure levels per scene. Participants were tasked with developing solutions
capable of effectively fusing these frames while adhering to strict efficiency
constraints: fewer than 30 million model parameters and a computational budget
under 4.0 trillion FLOPs. A total of 217 participants registered, with six
teams finally submitting valid solutions. The top-performing approach achieved
a PSNR of 43.22 dB, showcasing the potential of novel methods in this domain.
This paper provides a comprehensive overview of the challenge, compares the
proposed solutions, and serves as a valuable reference for researchers and
practitioners in efficient burst HDR and restoration.

</details>

### [932] [Joint Manifold Learning and Optimal Transport for Dynamic Imaging](https://arxiv.org/abs/2505.11913)
*Sven Dummer,Puru Vaish,Christoph Brune*

Main category: eess.IV

TLDR: 论文提出了一种结合低维图像流形假设和最优传输（OT）正则化的方法，用于动态生物成像中时间序列数据的处理。


<details>
  <summary>Details</summary>
Motivation: 动态生物成像中时间序列数据和时间点有限，难以学习有意义模式。现有方法要么忽略时间先验，要么忽视其他时间序列信息。

Method: 提出一种潜在模型表示图像流形，并确保其与时间序列数据和OT先验的一致性。

Result: 结合低维流形假设和OT正则化，提升了动态图像分析的性能。

Conclusion: 该方法通过整合两种正则化策略，有效解决了数据稀缺问题，并优化了动态图像建模。

Abstract: Dynamic imaging is critical for understanding and visualizing dynamic
biological processes in medicine and cell biology. These applications often
encounter the challenge of a limited amount of time series data and time
points, which hinders learning meaningful patterns. Regularization methods
provide valuable prior knowledge to address this challenge, enabling the
extraction of relevant information despite the scarcity of time-series data and
time points. In particular, low-dimensionality assumptions on the image
manifold address sample scarcity, while time progression models, such as
optimal transport (OT), provide priors on image development to mitigate the
lack of time points. Existing approaches using low-dimensionality assumptions
disregard a temporal prior but leverage information from multiple time series.
OT-prior methods, however, incorporate the temporal prior but regularize only
individual time series, ignoring information from other time series of the same
image modality. In this work, we investigate the effect of integrating a
low-dimensionality assumption of the underlying image manifold with an OT
regularizer for time-evolving images. In particular, we propose a latent model
representation of the underlying image manifold and promote consistency between
this representation, the time series data, and the OT prior on the
time-evolving images. We discuss the advantages of enriching OT interpolations
with latent models and integrating OT priors into latent models.

</details>

### [933] [Bayesian Deep Learning Approaches for Uncertainty-Aware Retinal OCT Image Segmentation for Multiple Sclerosis](https://arxiv.org/abs/2505.12061)
*Samuel T. M. Ball*

Main category: eess.IV

TLDR: 该研究提出了一种基于贝叶斯卷积神经网络（BCNNs）的方法，用于自动分割视网膜OCT图像，并提供不确定性估计，以提高临床应用的可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统视网膜OCT图像分割方法耗时且易受人为偏见影响，而现有深度学习方法因缺乏不确定性估计导致模型可能“自信地错误”。

Method: 使用贝叶斯卷积神经网络（BCNNs）对公开的OCT数据集进行分割，生成不确定性图以识别高不确定样本。

Result: 方法在分割任务中表现优异（Dice分数95.65%），并能提供不确定性估计，辅助识别噪声或校准问题。

Conclusion: 该方法提升了视网膜OCT分割的临床适用性、统计鲁棒性和性能。

Abstract: Optical Coherence Tomography (OCT) provides valuable insights in
ophthalmology, cardiology, and neurology due to high-resolution,
cross-sectional images of the retina. One critical task for ophthalmologists
using OCT is delineation of retinal layers within scans. This process is
time-consuming and prone to human bias, affecting the accuracy and reliability
of diagnoses. Previous efforts to automate delineation using deep learning face
challenges in uptake from clinicians and statisticians due to the absence of
uncertainty estimation, leading to "confidently wrong" models via
hallucinations. In this study, we address these challenges by applying Bayesian
convolutional neural networks (BCNNs) to segment an openly available OCT
imaging dataset containing 35 human retina OCTs split between healthy controls
and patients with multiple sclerosis. Our findings demonstrate that Bayesian
models can be used to provide uncertainty maps of the segmentation, which can
further be used to identify highly uncertain samples that exhibit recording
artefacts such as noise or miscalibration at inference time. Our method also
allows for uncertainty-estimation for important secondary measurements such as
layer thicknesses, that are medically relevant for patients. We show that these
features come in addition to greater performance compared to similar work over
all delineations; with an overall Dice score of 95.65%. Our work brings greater
clinical applicability, statistical robustness, and performance to retinal OCT
segmentation.

</details>

### [934] [HISTAI: An Open-Source, Large-Scale Whole Slide Image Dataset for Computational Pathology](https://arxiv.org/abs/2505.12120)
*Dmitry Nechaev,Alexey Pchelnikov,Ekaterina Ivanova*

Main category: eess.IV

TLDR: HISTAI数据集是一个大规模、多模态、开放访问的WSI集合，包含6万张切片，附带丰富的临床元数据，旨在解决现有数据集在规模、多样性和元数据方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有公开WSI数据集规模小、组织多样性不足且缺乏临床元数据，限制了AI模型的鲁棒性和泛化能力。

Method: 构建HISTAI数据集，包含6万张切片，涵盖多种组织类型，并提供详细的临床元数据和病理标注。

Result: HISTAI数据集填补了现有资源的空白，支持创新和可重复性研究。

Conclusion: HISTAI数据集为计算病理学提供了重要资源，促进临床相关解决方案的开发。

Abstract: Recent advancements in Digital Pathology (DP), particularly through
artificial intelligence and Foundation Models, have underscored the importance
of large-scale, diverse, and richly annotated datasets. Despite their critical
role, publicly available Whole Slide Image (WSI) datasets often lack sufficient
scale, tissue diversity, and comprehensive clinical metadata, limiting the
robustness and generalizability of AI models. In response, we introduce the
HISTAI dataset, a large, multimodal, open-access WSI collection comprising over
60,000 slides from various tissue types. Each case in the HISTAI dataset is
accompanied by extensive clinical metadata, including diagnosis, demographic
information, detailed pathological annotations, and standardized diagnostic
coding. The dataset aims to fill gaps identified in existing resources,
promoting innovation, reproducibility, and the development of clinically
relevant computational pathology solutions. The dataset can be accessed at
https://github.com/HistAI/HISTAI.

</details>

### [935] [CTLformer: A Hybrid Denoising Model Combining Convolutional Layers and Self-Attention for Enhanced CT Image Reconstruction](https://arxiv.org/abs/2505.12203)
*Zhiting Zheng,Shuqi Wu,Wen Ding*

Main category: eess.IV

TLDR: 本文提出了一种名为CTLformer的创新模型，结合卷积结构和Transformer架构，用于低剂量CT（LDCT）图像去噪。通过多尺度注意力机制和动态注意力控制机制，模型显著提升了去噪性能。


<details>
  <summary>Details</summary>
Motivation: LDCT图像常伴随显著噪声，影响图像质量和诊断准确性。现有方法在多尺度特征融合和噪声分布多样性方面存在挑战。

Method: CTLformer结合卷积和Transformer架构，提出多尺度注意力机制和动态注意力控制机制，并采用重叠推理减少边界伪影。

Result: 在2016年NIH AAPM Mayo Clinic LDCT Challenge数据集上，CTLformer在去噪性能和模型效率上显著优于现有方法。

Conclusion: CTLformer为LDCT去噪提供了高效解决方案，并在处理复杂噪声模式的医学图像分析中展现出广泛潜力。

Abstract: Low-dose CT (LDCT) images are often accompanied by significant noise, which
negatively impacts image quality and subsequent diagnostic accuracy. To address
the challenges of multi-scale feature fusion and diverse noise distribution
patterns in LDCT denoising, this paper introduces an innovative model,
CTLformer, which combines convolutional structures with transformer
architecture. Two key innovations are proposed: a multi-scale attention
mechanism and a dynamic attention control mechanism. The multi-scale attention
mechanism, implemented through the Token2Token mechanism and self-attention
interaction modules, effectively captures both fine details and global
structures at different scales, enhancing relevant features and suppressing
noise. The dynamic attention control mechanism adapts the attention
distribution based on the noise characteristics of the input image, focusing on
high-noise regions while preserving details in low-noise areas, thereby
enhancing robustness and improving denoising performance. Furthermore,
CTLformer integrates convolutional layers for efficient feature extraction and
uses overlapping inference to mitigate boundary artifacts, further
strengthening its denoising capability. Experimental results on the 2016
National Institutes of Health AAPM Mayo Clinic LDCT Challenge dataset
demonstrate that CTLformer significantly outperforms existing methods in both
denoising performance and model efficiency, greatly improving the quality of
LDCT images. The proposed CTLformer not only provides an efficient solution for
LDCT denoising but also shows broad potential in medical image analysis,
especially for clinical applications dealing with complex noise patterns.

</details>

### [936] [PRETI: Patient-Aware Retinal Foundation Model via Metadata-Guided Representation Learning](https://arxiv.org/abs/2505.12233)
*Yeonkyung Lee,Woojung Han,Youngjun Jun,Hyeonmin Kim,Jungkyung Cho,Seong Jae Hwang*

Main category: eess.IV

TLDR: PRETI是一种视网膜基础模型，通过结合元数据感知学习和自监督学习，提升视网膜图像分析的性能。


<details>
  <summary>Details</summary>
Motivation: 临床报告获取成本高且困难，而元数据（如年龄、性别）广泛可用，可用于疾病进展分析。

Method: 提出Learnable Metadata Embedding（LME）动态优化元数据表示，构建患者级数据对，并引入Retina-Aware Adaptive Masking（RAAM）策略。

Result: PRETI在多种疾病和生物标志物预测中取得最优结果。

Conclusion: 元数据引导的基础模型在视网膜疾病分析中具有重要意义。

Abstract: Retinal foundation models have significantly advanced retinal image analysis
by leveraging self-supervised learning to reduce dependence on labeled data
while achieving strong generalization. Many recent approaches enhance retinal
image understanding using report supervision, but obtaining clinical reports is
often costly and challenging. In contrast, metadata (e.g., age, gender) is
widely available and serves as a valuable resource for analyzing disease
progression. To effectively incorporate patient-specific information, we
propose PRETI, a retinal foundation model that integrates metadata-aware
learning with robust self-supervised representation learning. We introduce
Learnable Metadata Embedding (LME), which dynamically refines metadata
representations. Additionally, we construct patient-level data pairs,
associating images from the same individual to improve robustness against
non-clinical variations. To further optimize retinal image representation, we
propose Retina-Aware Adaptive Masking (RAAM), a strategy that selectively
applies masking within the retinal region and dynamically adjusts the masking
ratio during training. PRETI captures both global structures and fine-grained
pathological details, resulting in superior diagnostic performance. Extensive
experiments demonstrate that PRETI achieves state-of-the-art results across
diverse diseases and biomarker predictions using in-house and public data,
indicating the importance of metadata-guided foundation models in retinal
disease analysis. Our code and pretrained model are available at
https://github.com/MICV-yonsei/PRETI

</details>

### [937] [Attention-Enhanced U-Net for Accurate Segmentation of COVID-19 Infected Lung Regions in CT Scans](https://arxiv.org/abs/2505.12298)
*Amal Lahchim,Lazar Davic*

Main category: eess.IV

TLDR: 提出了一种基于改进U-Net架构的自动分割方法，用于COVID-19 CT扫描中感染区域的识别，性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 解决COVID-19 CT扫描中感染区域自动分割的挑战。

Method: 采用改进的U-Net架构，结合注意力机制、数据增强和后处理技术。

Result: Dice系数0.8658，平均IoU 0.8316，表现优于其他方法。

Conclusion: 方法性能优越，未来将扩展数据集并探索3D分割及临床部署。

Abstract: In this study, we propose a robust methodology for automatic segmentation of
infected lung regions in COVID-19 CT scans using convolutional neural networks.
The approach is based on a modified U-Net architecture enhanced with attention
mechanisms, data augmentation, and postprocessing techniques. It achieved a
Dice coefficient of 0.8658 and mean IoU of 0.8316, outperforming other methods.
The dataset was sourced from public repositories and augmented for diversity.
Results demonstrate superior segmentation performance. Future work includes
expanding the dataset, exploring 3D segmentation, and preparing the model for
clinical deployment.

</details>

### [938] [Mutual Evidential Deep Learning for Medical Image Segmentation](https://arxiv.org/abs/2505.12418)
*Yuanpeng He,Yali Bi,Lijian Li,Chi-Man Pun,Wenpin Jiao,Zhi Jin*

Main category: eess.IV

TLDR: 提出了一种基于证据的深度学习框架（MEDL），通过互补证据和改进的融合策略解决半监督医学分割中伪标签质量低的问题，并通过渐进学习策略优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督医学分割框架因伪标签质量低导致模型性能下降，且未能充分利用不同来源伪标签的可靠性。

Method: 1. 引入不同架构网络生成互补证据，采用改进的类别感知证据融合策略；2. 基于不确定性设计渐进Fisher信息学习策略，逐步关注不同质量的伪标签样本。

Result: 在五个主流数据集上验证，MEDL实现了最先进的性能。

Conclusion: MEDL通过互补证据和渐进学习策略，有效提升了半监督医学分割的性能。

Abstract: Existing semi-supervised medical segmentation co-learning frameworks have
realized that model performance can be diminished by the biases in model
recognition caused by low-quality pseudo-labels. Due to the averaging nature of
their pseudo-label integration strategy, they fail to explore the reliability
of pseudo-labels from different sources. In this paper, we propose a mutual
evidential deep learning (MEDL) framework that offers a potentially viable
solution for pseudo-label generation in semi-supervised learning from two
perspectives. First, we introduce networks with different architectures to
generate complementary evidence for unlabeled samples and adopt an improved
class-aware evidential fusion to guide the confident synthesis of evidential
predictions sourced from diverse architectural networks. Second, utilizing the
uncertainty in the fused evidence, we design an asymptotic Fisher
information-based evidential learning strategy. This strategy enables the model
to initially focus on unlabeled samples with more reliable pseudo-labels,
gradually shifting attention to samples with lower-quality pseudo-labels while
avoiding over-penalization of mislabeled classes in high data uncertainty
samples. Additionally, for labeled data, we continue to adopt an
uncertainty-driven asymptotic learning strategy, gradually guiding the model to
focus on challenging voxels. Extensive experiments on five mainstream datasets
have demonstrated that MEDL achieves state-of-the-art performance.

</details>

### [939] [FreqSelect: Frequency-Aware fMRI-to-Image Reconstruction](https://arxiv.org/abs/2505.12552)
*Junliang Ye,Lei Wang,Md Zakir Hossain*

Main category: eess.IV

TLDR: FreqSelect是一个轻量级模块，通过选择性过滤空间频率带提升fMRI图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法对所有空间频率成分平等处理的问题，提升解码效果。

Method: 引入FreqSelect模块，动态选择预测性频率带，无缝集成到VAE-扩散模型中。

Result: 在Natural Scenes数据集上，FreqSelect显著提升了重建质量，并提供了频率选择的解释性。

Conclusion: FreqSelect不仅提升性能，还为神经科学解释性提供了新视角，具有广泛适用性。

Abstract: Reconstructing natural images from functional magnetic resonance imaging
(fMRI) data remains a core challenge in natural decoding due to the mismatch
between the richness of visual stimuli and the noisy, low resolution nature of
fMRI signals. While recent two-stage models, combining deep variational
autoencoders (VAEs) with diffusion models, have advanced this task, they treat
all spatial-frequency components of the input equally. This uniform treatment
forces the model to extract meaning features and suppress irrelevant noise
simultaneously, limiting its effectiveness. We introduce FreqSelect, a
lightweight, adaptive module that selectively filters spatial-frequency bands
before encoding. By dynamically emphasizing frequencies that are most
predictive of brain activity and suppressing those that are uninformative,
FreqSelect acts as a content-aware gate between image features and natural
data. It integrates seamlessly into standard very deep VAE-diffusion pipelines
and requires no additional supervision. Evaluated on the Natural Scenes
dataset, FreqSelect consistently improves reconstruction quality across both
low- and high-level metrics. Beyond performance gains, the learned
frequency-selection patterns offer interpretable insights into how different
visual frequencies are represented in the brain. Our method generalizes across
subjects and scenes, and holds promise for extension to other neuroimaging
modalities, offering a principled approach to enhancing both decoding accuracy
and neuroscientific interpretability.

</details>

### [940] [The Gaussian Latent Machine: Efficient Prior and Posterior Sampling for Inverse Problems](https://arxiv.org/abs/2505.12836)
*Muhamed Kuric,Martin Zach,Andreas Habring,Michael Unser,Thomas Pock*

Main category: eess.IV

TLDR: 论文提出了一种基于高斯潜变量模型的通用采样方法，适用于贝叶斯成像中的先验和后验分布采样。


<details>
  <summary>Details</summary>
Motivation: 解决贝叶斯成像中常见的先验和后验分布采样问题。

Method: 通过将模型提升为高斯潜变量模型（Gaussian latent machine），提出了一种通用的采样方法，包括高效的两块Gibbs采样。

Result: 数值实验表明，该方法在多种贝叶斯成像问题中高效且有效。

Conclusion: 该方法统一并推广了现有采样算法，适用于广泛的贝叶斯成像问题。

Abstract: We consider the problem of sampling from a product-of-experts-type model that
encompasses many standard prior and posterior distributions commonly found in
Bayesian imaging. We show that this model can be easily lifted into a novel
latent variable model, which we refer to as a Gaussian latent machine. This
leads to a general sampling approach that unifies and generalizes many existing
sampling algorithms in the literature. Most notably, it yields a highly
efficient and effective two-block Gibbs sampling approach in the general case,
while also specializing to direct sampling algorithms in particular cases.
Finally, we present detailed numerical experiments that demonstrate the
efficiency and effectiveness of our proposed sampling approach across a wide
range of prior and posterior sampling problems from Bayesian imaging.

</details>

### [941] [RetinaLogos: Fine-Grained Synthesis of High-Resolution Retinal Images Through Captions](https://arxiv.org/abs/2505.12887)
*Junzhi Ning,Cheng Tang,Kaijin Zhou,Diping Song,Lihao Liu,Ming Hu,Wei Li,Yanzhou Su,Tianbing Li,Jiyao Liu,Yejin,Sheng Zhang,Yuanfeng Ji,Junjun He*

Main category: eess.IV

TLDR: 论文提出了一种名为RetinaLogos-1400k的创新方法，通过合成大规模视网膜图像数据集，解决了高质量标注数据稀缺的问题，并提升了机器学习模型在眼科领域的性能。


<details>
  <summary>Details</summary>
Motivation: 高质量标注视网膜图像的稀缺阻碍了机器学习模型在眼科领域的发展，现有方法无法生成具有多样性和细粒度解剖结构的图像。

Method: 引入RetinaLogos-1400k数据集，利用大语言模型生成视网膜图像描述，并采用三步训练框架实现细粒度语义控制。

Result: 合成的图像62.07%被眼科医生认为与真实图像无法区分，且在糖尿病视网膜病变分级和青光眼检测中准确率提升10%-25%。

Conclusion: 该方法为眼科数据集扩展提供了可扩展的解决方案，显著提升了模型性能。

Abstract: The scarcity of high-quality, labelled retinal imaging data, which presents a
significant challenge in the development of machine learning models for
ophthalmology, hinders progress in the field. To synthesise Colour Fundus
Photographs (CFPs), existing methods primarily relying on predefined disease
labels face significant limitations. However, current methods remain limited,
thus failing to generate images for broader categories with diverse and
fine-grained anatomical structures. To overcome these challenges, we first
introduce an innovative pipeline that creates a large-scale, synthetic
Caption-CFP dataset comprising 1.4 million entries, called RetinaLogos-1400k.
Specifically, RetinaLogos-1400k uses large language models (LLMs) to describe
retinal conditions and key structures, such as optic disc configuration,
vascular distribution, nerve fibre layers, and pathological features.
Furthermore, based on this dataset, we employ a novel three-step training
framework, called RetinaLogos, which enables fine-grained semantic control over
retinal images and accurately captures different stages of disease progression,
subtle anatomical variations, and specific lesion types. Extensive experiments
demonstrate state-of-the-art performance across multiple datasets, with 62.07%
of text-driven synthetic images indistinguishable from real ones by
ophthalmologists. Moreover, the synthetic data improves accuracy by 10%-25% in
diabetic retinopathy grading and glaucoma detection, thereby providing a
scalable solution to augment ophthalmic datasets.

</details>

### [942] [Segmentation of temporomandibular joint structures on mri images using neural networks for diagnosis of pathologies](https://arxiv.org/abs/2505.12963)
*Maksim I. Ivanov,Olga E. Mendybaeva,Yuri E. Karyakin,Igor N. Glukhikh,Aleksey V. Lebedev*

Main category: eess.IV

TLDR: 该论文探讨了人工智能在颞下颌关节（TMJ）病理诊断中的应用，特别是针对MRI图像中关节盘的分割。研究通过分析现有解决方案的不足，提出了一种基于Roboflow神经网络的新方法，并验证了其潜力。


<details>
  <summary>Details</summary>
Motivation: TMJ病理的高发率及现有诊断工具（如Diagnocat和MandSeg）对关节盘分割的不足，促使研究开发更精确、快速的诊断方法。

Method: 研究收集了94张图像数据集，采用数据增强方法，并训练和比较了U-Net、YOLOv8n、YOLOv11n和Roboflow神经网络模型，使用Dice Score等指标评估性能。

Result: Roboflow模型在颞下颌关节分割中表现最佳，证实了其潜力。

Conclusion: 未来计划开发测量颌间距离和确定关节盘位置的算法，以进一步提升TMJ病理诊断的准确性。

Abstract: This article explores the use of artificial intelligence for the diagnosis of
pathologies of the temporomandibular joint (TMJ), in particular, for the
segmentation of the articular disc on MRI images. The relevance of the work is
due to the high prevalence of TMJ pathologies, as well as the need to improve
the accuracy and speed of diagnosis in medical institutions. During the study,
the existing solutions (Diagnocat, MandSeg) were analyzed, which, as a result,
are not suitable for studying the articular disc due to the orientation towards
bone structures. To solve the problem, an original dataset was collected from
94 images with the classes "temporomandibular joint" and "jaw". To increase the
amount of data, augmentation methods were used. After that, the models of
U-Net, YOLOv8n, YOLOv11n and Roboflow neural networks were trained and
compared. The evaluation was carried out according to the Dice Score,
Precision, Sensitivity, Specificity, and Mean Average Precision metrics. The
results confirm the potential of using the Roboflow model for segmentation of
the temporomandibular joint. In the future, it is planned to develop an
algorithm for measuring the distance between the jaws and determining the
position of the articular disc, which will improve the diagnosis of TMJ
pathologies.

</details>

### [943] [Enhancing Diffusion-Weighted Images (DWI) for Diffusion MRI: Is it Enough without Non-Diffusion-Weighted B=0 Reference?](https://arxiv.org/abs/2505.12978)
*Yinzhe Wu,Jiahao Huang,Fanwen Wang,Mengze Gao,Congyu Liao,Guang Yang,Kawin Setsompop*

Main category: eess.IV

TLDR: 论文提出了一种新的比率损失函数，用于改进扩散MRI（dMRI）超分辨率成像，通过优化DWI与b=0图像的比率误差，显著提升了扩散度量的准确性。


<details>
  <summary>Details</summary>
Motivation: 高分辨率dMRI成像面临采集时间与信噪比的权衡问题，传统方法仅优化DWI而忽略其与b=0图像的关系，导致扩散度量计算不准确。

Method: 提出了一种比率损失函数，定义为预测与真实DWI/b=0比率的对数之间的均方误差（MSE）损失。

Result: 比率损失的引入显著降低了比率误差，略微提升了生成DWI的峰值信噪比（PSNR），改善了dMRI超分辨率和扩散度量的保留。

Conclusion: 比率损失函数有效提升了dMRI超分辨率成像的质量，尤其是对基于b=0比率的扩散度量的计算具有重要意义。

Abstract: Diffusion MRI (dMRI) is essential for studying brain microstructure, but
high-resolution imaging remains challenging due to the inherent trade-offs
between acquisition time and signal-to-noise ratio (SNR). Conventional methods
often optimize only the diffusion-weighted images (DWIs) without considering
their relationship with the non-diffusion-weighted (b=0) reference images.
However, calculating diffusion metrics, such as the apparent diffusion
coefficient (ADC) and diffusion tensor with its derived metrics like fractional
anisotropy (FA) and mean diffusivity (MD), relies on the ratio between each DWI
and the b=0 image, which is crucial for clinical observation and diagnostics.
In this study, we demonstrate that solely enhancing DWIs using a conventional
pixel-wise mean squared error (MSE) loss is insufficient, as the error in ratio
between generated DWIs and b=0 diverges. We propose a novel ratio loss, defined
as the MSE loss between the predicted and ground-truth log of DWI/b=0 ratios.
Our results show that incorporating the ratio loss significantly improves the
convergence of this ratio error, achieving lower ratio MSE and slightly
enhancing the peak signal-to-noise ratio (PSNR) of generated DWIs. This leads
to improved dMRI super-resolution and better preservation of b=0 ratio-based
features for the derivation of diffusion metrics.

</details>

### [944] [A generalisable head MRI defacing pipeline: Evaluation on 2,566 meningioma scans](https://arxiv.org/abs/2505.12999)
*Lorena Garcia-Foncillas Macias,Aaron Kujawa,Aya Elshalakany,Jonathan Shapey,Tom Vercauteren*

Main category: eess.IV

TLDR: 提出了一种可靠的MRI去标识化方法，结合基于图谱的配准和脑部掩模技术，成功率高且能保留脑部解剖结构。


<details>
  <summary>Details</summary>
Motivation: 现有方法在去标识化时可能不完全或损坏脑组织区域，需要一种更可靠的方法来保护患者隐私并支持研究合作。

Method: 采用基于图谱的配准与脑部掩模结合的流程，处理高分辨率MRI数据。

Result: 在2,566例临床扫描中，视觉检查成功率为99.92%，脑部掩模的Dice相似系数为0.9975±0.0023。

Conclusion: 该方法高效可靠，适用于临床研究，代码已开源。

Abstract: Reliable MRI defacing techniques to safeguard patient privacy while
preserving brain anatomy are critical for research collaboration. Existing
methods often struggle with incomplete defacing or degradation of brain tissue
regions. We present a robust, generalisable defacing pipeline for
high-resolution MRI that integrates atlas-based registration with brain
masking. Our method was evaluated on 2,566 heterogeneous clinical scans for
meningioma and achieved a 99.92 per cent success rate (2,564/2,566) upon visual
inspection. Excellent anatomical preservation is demonstrated with a Dice
similarity coefficient of 0.9975 plus or minus 0.0023 between brain masks
automatically extracted from the original and defaced volumes. Source code is
available at https://github.com/cai4cai/defacing_pipeline.

</details>

### [945] [Higher fidelity perceptual image and video compression with a latent conditioned residual denoising diffusion model](https://arxiv.org/abs/2505.13152)
*Jonas Brenig,Radu Timofte*

Main category: eess.IV

TLDR: 本文提出了一种混合压缩方案，结合解码器网络和扩散模型，以在保持感知质量的同时减少失真。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现出色，但在图像压缩中可能导致失真增加。本文旨在改进这一缺点。

Method: 使用解码器网络生成初始图像以减少失真，再通过扩散模型预测残差以提升感知质量。

Result: 在标准基准测试中，PSNR提高了2dB，同时保持与CDC相当的LPIPS和FID分数。

Conclusion: 该方法在图像和视频压缩中均有效，平衡了感知质量和失真。

Abstract: Denoising diffusion models achieved impressive results on several image
generation tasks often outperforming GAN based models. Recently, the generative
capabilities of diffusion models have been employed for perceptual image
compression, such as in CDC. A major drawback of these diffusion-based methods
is that, while producing impressive perceptual quality images they are dropping
in fidelity/increasing the distortion to the original uncompressed images when
compared with other traditional or learned image compression schemes aiming for
fidelity. In this paper, we propose a hybrid compression scheme optimized for
perceptual quality, extending the approach of the CDC model with a decoder
network in order to reduce the impact on distortion metrics such as PSNR. After
using the decoder network to generate an initial image, optimized for
distortion, the latent conditioned diffusion model refines the reconstruction
for perceptual quality by predicting the residual. On standard benchmarks, we
achieve up to +2dB PSNR fidelity improvements while maintaining comparable
LPIPS and FID perceptual scores when compared with CDC. Additionally, the
approach is easily extensible to video compression, where we achieve similar
results.

</details>

### [946] [GuidedMorph: Two-Stage Deformable Registration for Breast MRI](https://arxiv.org/abs/2505.13414)
*Yaqian Chen,Hanxue Gu,Haoyu Dong,Qihang Li,Yuwen Chen,Nicholas Konz,Lin Li,Maciej A. Mazurowski*

Main category: eess.IV

TLDR: 提出了一种名为GuidedMorph的两阶段配准框架，用于更精确地对齐乳腺MR图像中的密集组织，通过双空间变换网络和欧几里得距离变换方法提升配准精度。


<details>
  <summary>Details</summary>
Motivation: 乳腺MR图像的多时间点配准对乳腺癌检测和治疗规划至关重要，但密集组织的复杂性和非刚性特性使传统方法难以精确对齐内部细节。

Method: 采用两阶段框架，结合单尺度网络进行全局结构对齐，并利用密集组织信息跟踪乳腺运动，引入双空间变换网络（DSTN）和基于欧几里得距离变换（EDT）的变形方法。

Result: 在ISPY2和内部数据集上验证，密集组织Dice提升13.01%，乳腺Dice提升3.13%，乳腺SSIM提升1.21%，优于现有学习基线。

Conclusion: GuidedMorph在乳腺MR图像配准中表现出色，尤其在密集组织对齐和细节保留方面具有显著优势。

Abstract: Accurately registering breast MR images from different time points enables
the alignment of anatomical structures and tracking of tumor progression,
supporting more effective breast cancer detection, diagnosis, and treatment
planning. However, the complexity of dense tissue and its highly non-rigid
nature pose challenges for conventional registration methods, which primarily
focus on aligning general structures while overlooking intricate internal
details. To address this, we propose \textbf{GuidedMorph}, a novel two-stage
registration framework designed to better align dense tissue. In addition to a
single-scale network for global structure alignment, we introduce a framework
that utilizes dense tissue information to track breast movement. The learned
transformation fields are fused by introducing the Dual Spatial Transformer
Network (DSTN), improving overall alignment accuracy. A novel warping method
based on the Euclidean distance transform (EDT) is also proposed to accurately
warp the registered dense tissue and breast masks, preserving fine structural
details during deformation. The framework supports paradigms that require
external segmentation models and with image data only. It also operates
effectively with the VoxelMorph and TransMorph backbones, offering a versatile
solution for breast registration. We validate our method on ISPY2 and internal
dataset, demonstrating superior performance in dense tissue, overall breast
alignment, and breast structural similarity index measure (SSIM), with notable
improvements by over 13.01% in dense tissue Dice, 3.13% in breast Dice, and
1.21% in breast SSIM compared to the best learning-based baseline.

</details>

### [947] [Measurement Score-Based Diffusion Model](https://arxiv.org/abs/2505.11853)
*Chicago Y. Park,Shirin Shoushtari,Hongyu An,Ulugbek S. Kamilov*

Main category: eess.IV

TLDR: 论文提出了一种基于测量分数的扩散模型（MSM），仅需噪声和子采样测量即可学习部分测量分数，无需干净图像。通过随机子采样和高效采样算法，MSM能生成高质量图像并解决逆问题。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型需要干净图像训练，但在许多应用中难以获取。MSM旨在仅利用噪声和子采样数据，解决这一限制。

Method: MSM通过随机子采样建模完整测量的分布，并开发了高效采样算法和新的后验采样方法。

Result: 理论分析证明算法准确性，实验显示MSM在自然图像和多线圈MRI中能生成高质量图像并解决逆问题。

Conclusion: MSM无需干净数据即可高效生成图像和解决逆问题，具有广泛应用潜力。

Abstract: Diffusion models are widely used in applications ranging from image
generation to inverse problems. However, training diffusion models typically
requires clean ground-truth images, which are unavailable in many applications.
We introduce the Measurement Score-based diffusion Model (MSM), a novel
framework that learns partial measurement scores using only noisy and
subsampled measurements. MSM models the distribution of full measurements as an
expectation over partial scores induced by randomized subsampling. To make the
MSM representation computationally efficient, we also develop a stochastic
sampling algorithm that generates full images by using a randomly selected
subset of partial scores at each step. We additionally propose a new posterior
sampling method for solving inverse problems that reconstructs images using
these partial scores. We provide a theoretical analysis that bounds the
Kullback-Leibler divergence between the distributions induced by full and
stochastic sampling, establishing the accuracy of the proposed algorithm. We
demonstrate the effectiveness of MSM on natural images and multi-coil MRI,
showing that it can generate high-quality images and solve inverse problems --
all without access to clean training data. Code is available at
https://github.com/wustl-cig/MSM.

</details>

### [948] [WaLRUS: Wavelets for Long-range Representation Using SSMs](https://arxiv.org/abs/2505.12161)
*Hossein Babaei,Mel White,Sina Alemohammad,Richard G. Baraniuk*

Main category: eess.IV

TLDR: WaLRUS是一种基于Daubechies小波的SaFARi框架实现，用于构建更通用的状态空间模型（SSMs）。


<details>
  <summary>Details</summary>
Motivation: HiPPO等方法依赖特定基的闭式解，限制了其通用性。SaFARi框架通过支持任意基（包括非正交和冗余基）扩展了SSMs的可能性。

Method: 利用Daubechies小波作为基，实现SaFARi框架，构建WaLRUS模型。

Result: WaLRUS能够生成多样化的SSMs，突破了HiPPO等方法的限制。

Conclusion: WaLRUS为长序列建模提供了更灵活的工具，扩展了SSMs的应用范围。

Abstract: State-Space Models (SSMs) have proven to be powerful tools for modeling
long-range dependencies in sequential data. While the recent method known as
HiPPO has demonstrated strong performance, and formed the basis for machine
learning models S4 and Mamba, it remains limited by its reliance on closed-form
solutions for a few specific, well-behaved bases. The SaFARi framework
generalized this approach, enabling the construction of SSMs from arbitrary
frames, including non-orthogonal and redundant ones, thus allowing an infinite
diversity of possible "species" within the SSM family. In this paper, we
introduce WaLRUS (Wavelets for Long-range Representation Using SSMs), a new
implementation of SaFARi built from Daubechies wavelets.

</details>

### [949] [Trustworthy Image Super-Resolution via Generative Pseudoinverse](https://arxiv.org/abs/2505.12375)
*Andreas Floros,Seyed-Mohsen Moosavi-Dezfooli,Pier Luigi Dragotti*

Main category: eess.IV

TLDR: 本文提出了一种基于生成模型的图像超分辨率方法，通过约束优化确保与低分辨率测量的一致性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决可信赖的图像恢复问题，确保生成模型与退化过程一致。

Method: 采用生成模型进行图像超分辨率，通过约束优化先验密度。

Result: 方法在低分辨率测量一致性方面大幅优于现有方法。

Conclusion: 所提方法在图像超分辨率任务中表现出色，具有理论保证和实践优势。

Abstract: We consider the problem of trustworthy image restoration, taking the form of
a constrained optimization over the prior density. To this end, we develop
generative models for the task of image super-resolution that respect the
degradation process and that can be made asymptotically consistent with the
low-resolution measurements, outperforming existing methods by a large margin
in that respect.

</details>

<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [950] [Exploring the Potential of SSL Models for Sound Event Detection](https://arxiv.org/abs/2505.11889)
*Hanfang Cui,Longfei Song,Li Li,Dongxing Xu,Yanhua Long*

Main category: eess.AS

TLDR: 该研究系统评估了自监督学习（SSL）模型在声音事件检测（SED）中的表现，提出了融合策略和自适应后处理方法，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 探索自监督学习模型在声音事件检测中的协同潜力，指导模型选择和集成。

Method: 提出三种融合策略（单模型嵌入集成、双模态融合、全聚合）和自适应后处理方法（nSEBBs）。

Result: 双模态融合（如CRNN+BEATs+WavLM）表现最佳，nSEBBs提升性能4%。

Conclusion: SSL模型具有兼容性和互补性，为任务特定融合和SED系统设计提供指导。

Abstract: Self-supervised learning (SSL) models offer powerful representations for
sound event detection (SED), yet their synergistic potential remains
underexplored. This study systematically evaluates state-of-the-art SSL models
to guide optimal model selection and integration for SED. We propose a
framework that combines heterogeneous SSL representations (e.g., BEATs, HuBERT,
WavLM) through three fusion strategies: individual SSL embedding integration,
dual-modal fusion, and full aggregation. Experiments on the DCASE 2023 Task 4
Challenge reveal that dual-modal fusion (e.g., CRNN+BEATs+WavLM) achieves
complementary performance gains, while CRNN+BEATs alone delivers the best
results among individual SSL models. We further introduce normalized sound
event bounding boxes (nSEBBs), an adaptive post-processing method that
dynamically adjusts event boundary predictions, improving PSDS1 by up to 4% for
standalone SSL models. These findings highlight the compatibility and
complementarity of SSL architectures, providing guidance for task-specific
fusion and robust SED system design.

</details>

### [951] [Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis](https://arxiv.org/abs/2505.12226)
*Dong Yang,Yiyi Cai,Yuki Saito,Lixu Wang,Hiroshi Saruwatari*

Main category: eess.AS

TLDR: 提出了一种浅层流匹配（SFM）机制，通过粗到细的生成范式增强基于流匹配（FM）的文本到语音（TTS）模型。


<details>
  <summary>Details</summary>
Motivation: 提升FM-based TTS模型的自然度和推理效率。

Method: SFM通过粗输出表示构建FM路径的中间状态，训练时采用正交投影方法自适应确定状态时间位置，并基于单段分段流进行构建。推理时从中间状态开始，聚焦于FM路径的后期阶段。

Result: SFM显著提升了合成语音的自然度（主客观评估），并大幅减少了自适应步长ODE求解器的推理时间。

Conclusion: SFM是一种轻量级且高效的改进方法，适用于多种TTS模型。

Abstract: We propose a shallow flow matching (SFM) mechanism to enhance flow matching
(FM)-based text-to-speech (TTS) models within a coarse-to-fine generation
paradigm. SFM constructs intermediate states along the FM paths using coarse
output representations. During training, we introduce an orthogonal projection
method to adaptively determine the temporal position of these states, and apply
a principled construction strategy based on a single-segment piecewise flow.
The SFM inference starts from the intermediate state rather than pure noise and
focuses computation on the latter stages of the FM paths. We integrate SFM into
multiple TTS models with a lightweight SFM head. Experiments show that SFM
consistently improves the naturalness of synthesized speech in both objective
and subjective evaluations, while significantly reducing inference when using
adaptive-step ODE solvers. Demo and codes are available at
https://ydqmkkx.github.io/SFMDemo/.

</details>

### [952] [SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information](https://arxiv.org/abs/2505.13237)
*Chih-Kai Yang,Neo Ho,Yen-Ting Piao,Hung-yi Lee*

Main category: eess.AS

TLDR: 论文介绍了SAKURA基准测试，用于评估大型音频-语言模型（LALMs）的多跳推理能力，发现LALMs在整合语音/音频信息进行多跳推理时存在困难。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LALMs在语音和音频处理任务中的表现进行了广泛研究，但其多跳推理能力尚未系统评估。

Method: 提出SAKURA基准测试，专门评估LALMs基于语音和音频信息的多跳推理能力。

Result: LALMs在整合语音/音频信息进行多跳推理时表现不佳，即使能正确提取相关信息。

Conclusion: 研究揭示了LALMs在多模态推理中的关键局限性，为未来研究提供了方向和资源。

Abstract: Large audio-language models (LALMs) extend the large language models with
multimodal understanding in speech, audio, etc. While their performances on
speech and audio-processing tasks are extensively studied, their reasoning
abilities remain underexplored. Particularly, their multi-hop reasoning, the
ability to recall and integrate multiple facts, lacks systematic evaluation.
Existing benchmarks focus on general speech and audio-processing tasks,
conversational abilities, and fairness but overlook this aspect. To bridge this
gap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning
based on speech and audio information. Results show that LALMs struggle to
integrate speech/audio representations for multi-hop reasoning, even when they
extract the relevant information correctly, highlighting a fundamental
challenge in multimodal reasoning. Our findings expose a critical limitation in
LALMs, offering insights and resources for future research.

</details>

### [953] [AnalyticKWS: Towards Exemplar-Free Analytic Class Incremental Learning for Small-footprint Keyword Spotting](https://arxiv.org/abs/2505.11817)
*Yang Xiao,Tianyi Peng,Rohan Kumar Das,Yuchen Hu,Huiping Zhuang*

Main category: eess.AS

TLDR: 论文提出了一种无需存储旧数据的持续学习方法AnalyticKWS，通过闭式解析解更新模型参数，解决了隐私和资源消耗问题。


<details>
  <summary>Details</summary>
Motivation: 语音系统中的关键词识别需要持续学习新关键词，但现有方法依赖存储旧数据，存在隐私和资源限制问题。

Method: 提出AnalyticKWS方法，通过闭式解析解更新模型参数，避免梯度更新和存储旧数据，仅需单次适应新关键词。

Result: 实验表明，AnalyticKWS在多种数据集和设置下均优于现有持续学习方法。

Conclusion: AnalyticKWS解决了隐私和资源问题，适用于资源受限环境，性能优于现有方法。

Abstract: Keyword spotting (KWS) offers a vital mechanism to identify spoken commands
in voice-enabled systems, where user demands often shift, requiring models to
learn new keywords continually over time. However, a major problem is
catastrophic forgetting, where models lose their ability to recognize earlier
keywords. Although several continual learning methods have proven their
usefulness for reducing forgetting, most existing approaches depend on storing
and revisiting old data to combat catastrophic forgetting. Though effective,
these methods face two practical challenges: 1) privacy risks from keeping user
data and 2) large memory and time consumption that limit deployment on small
devices. To address these issues, we propose an exemplar-free Analytic
Continual Learning (AnalyticKWS) method that updates model parameters without
revisiting earlier data. Inspired by efficient learning principles, AnalyticKWS
computes a closed-form analytical solution for model updates and requires only
a single epoch of adaptation for incoming keywords. AnalyticKWS demands fewer
computational resources by avoiding gradient-based updates and does not store
old data. By eliminating the need for back-propagation during incremental
learning, the model remains lightweight and efficient. As a result, AnalyticKWS
meets the challenges mentioned earlier and suits resource-limited settings
well. Extensive experiments on various datasets and settings show that
AnalyticKWS consistently outperforms existing continual learning methods.

</details>

### [954] [Cross-modal Knowledge Transfer Learning as Graph Matching Based on Optimal Transport for ASR](https://arxiv.org/abs/2505.13079)
*Xugang Lu,Peng Shen,Yu Tsao,Hisashi Kawai*

Main category: eess.AS

TLDR: 论文提出了一种基于图匹配的最优传输方法（GM-OT），用于解决语言和声学模态之间的对齐问题，通过结合节点和边的距离度量，显著提升了端到端语音识别的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的最优传输方法在语言知识迁移中忽略了特征向量的结构关系，导致模态对齐效果不佳。

Method: 提出GM-OT方法，将语言和声学序列建模为结构化图，同时最小化节点间的Wasserstein距离和边间的Gromov-Wasserstein距离。

Result: 在普通话ASR任务中，GM-OT显著优于现有最优传输方法，验证了其有效性。

Conclusion: GM-OT通过结构化对齐实现了更高效的知识迁移，为模态对齐问题提供了新思路。

Abstract: Transferring linguistic knowledge from a pretrained language model (PLM) to
acoustic feature learning has proven effective in enhancing end-to-end
automatic speech recognition (E2E-ASR). However, aligning representations
between linguistic and acoustic modalities remains a challenge due to inherent
modality gaps. Optimal transport (OT) has shown promise in mitigating these
gaps by minimizing the Wasserstein distance (WD) between linguistic and
acoustic feature distributions. However, previous OT-based methods overlook
structural relationships, treating feature vectors as unordered sets. To
address this, we propose Graph Matching Optimal Transport (GM-OT), which models
linguistic and acoustic sequences as structured graphs. Nodes represent feature
embeddings, while edges capture temporal and sequential relationships. GM-OT
minimizes both WD (between nodes) and Gromov-Wasserstein distance (GWD)
(between edges), leading to a fused Gromov-Wasserstein distance (FGWD)
formulation. This enables structured alignment and more efficient knowledge
transfer compared to existing OT-based approaches. Theoretical analysis further
shows that prior OT-based methods in linguistic knowledge transfer can be
viewed as a special case within our GM-OT framework. We evaluate GM-OT on
Mandarin ASR using a CTC-based E2E-ASR system with a PLM for knowledge
transfer. Experimental results demonstrate significant performance gains over
state-of-the-art models, validating the effectiveness of our approach.

</details>

### [955] [Universal Semantic Disentangled Privacy-preserving Speech Representation Learning](https://arxiv.org/abs/2505.13085)
*Biel Tura Vecino,Subhadeep Maji,Aravind Varier,Antonio Bonafonte,Ivan Valles,Michael Owen,Leif Radel,Grant Strimmel,Seyi Feyisetan,Roberto Barra Chicote,Ariya Rastrow,Constantinos Papayiannis,Volker Leutnant,Trevor Wood*

Main category: eess.AS

TLDR: 提出了一种通过通用语音编解码器（USC）保护说话者隐私的表示学习方法，该方法将语音解耦为隐私保护的语义表示和可重构的残差表示，同时保留内容和情感。


<details>
  <summary>Details</summary>
Motivation: 解决使用人类语音训练大型语言模型（LLM）时的隐私问题，防止模型生成与训练数据相似的输出。

Method: 使用USC模型将语音解耦为隐私保护的语义表示和残差表示，保留内容、韵律和情感，同时去除可识别的说话者属性。

Result: USC在语音重构方面达到最优性能，同时有效保护隐私。

Conclusion: USC在隐私保护、内容保留和情感保持之间取得了平衡，为隐私保护表示学习提供了有效方法。

Abstract: The use of audio recordings of human speech to train LLMs poses privacy
concerns due to these models' potential to generate outputs that closely
resemble artifacts in the training data. In this study, we propose a speaker
privacy-preserving representation learning method through the Universal Speech
Codec (USC), a computationally efficient encoder-decoder model that
disentangles speech into: $\textit{(i)}$ privacy-preserving semantically rich
representations, capturing content and speech paralinguistics, and
$\textit{(ii)}$ residual acoustic and speaker representations that enables
high-fidelity reconstruction. Extensive evaluations presented show that USC's
semantic representation preserves content, prosody, and sentiment, while
removing potentially identifiable speaker attributes. Combining both
representations, USC achieves state-of-the-art speech reconstruction.
Additionally, we introduce an evaluation methodology for measuring
privacy-preserving properties, aligning with perceptual tests. We compare USC
against other codecs in the literature and demonstrate its effectiveness on
privacy-preserving representation learning, illustrating the trade-offs of
speaker anonymization, paralinguistics retention and content preservation in
the learned semantic representations. Audio samples are shared in
$\href{https://www.amazon.science/usc-samples}{https://www.amazon.science/usc-samples}$.

</details>

<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [956] [Enhanced Multimodal Hate Video Detection via Channel-wise and Modality-wise Fusion](https://arxiv.org/abs/2505.12051)
*Yinghui Zhang,Tailin Chen,Yuchen Zhang,Zeyu Fu*

Main category: cs.MM

TLDR: CMFusion是一种新型多模态仇恨视频检测模型，通过通道和模态融合机制显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 视频平台上的仇恨内容传播迅速且隐含性强，现有单模态检测方法效果有限，多模态方法未能有效整合时序动态和模态交互。

Method: CMFusion提取文本、音频和视频特征，结合时序交叉注意力机制，并通过通道和模态融合模块生成视频表征。

Result: 实验表明CMFusion在准确率、精确率、召回率和F1分数上优于五种基线方法。

Conclusion: CMFusion能有效检测仇恨视频，其设计通过消融实验和参数分析得到验证。

Abstract: The rapid rise of video content on platforms such as TikTok and YouTube has
transformed information dissemination, but it has also facilitated the spread
of harmful content, particularly hate videos. Despite significant efforts to
combat hate speech, detecting these videos remains challenging due to their
often implicit nature. Current detection methods primarily rely on unimodal
approaches, which inadequately capture the complementary features across
different modalities. While multimodal techniques offer a broader perspective,
many fail to effectively integrate temporal dynamics and modality-wise
interactions essential for identifying nuanced hate content. In this paper, we
present CMFusion, an enhanced multimodal hate video detection model utilizing a
novel Channel-wise and Modality-wise Fusion Mechanism. CMFusion first extracts
features from text, audio, and video modalities using pre-trained models and
then incorporates a temporal cross-attention mechanism to capture dependencies
between video and audio streams. The learned features are then processed by
channel-wise and modality-wise fusion modules to obtain informative
representations of videos. Our extensive experiments on a real-world dataset
demonstrate that CMFusion significantly outperforms five widely used baselines
in terms of accuracy, precision, recall, and F1 score. Comprehensive ablation
studies and parameter analyses further validate our design choices,
highlighting the model's effectiveness in detecting hate videos. The source
codes will be made publicly available at https://github.com/EvelynZ10/cmfusion.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [957] [Modeling Aesthetic Preferences in 3D Shapes: A Large-Scale Paired Comparison Study Across Object Categories](https://arxiv.org/abs/2505.12373)
*Kapil Dev*

Main category: cs.GR

TLDR: 通过大规模人类偏好研究，揭示了3D形状美学的几何驱动因素，结合非线性建模和跨类别分析，提供了可解释的设计指导。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D美学计算模型缺乏大规模人类判断实证基础的问题，提升其实际应用价值。

Method: 收集22,301对比较数据，应用Bradley-Terry模型和随机森林分析，结合SHAP解释几何特征（如对称性、曲率）。

Result: 揭示了美学偏好的普遍原则和领域特定趋势，提供了透明且可操作的设计见解。

Conclusion: 通过数据驱动框架，为设计师提供实用指导，并公开数据集以支持可重复性研究。

Abstract: Human aesthetic preferences for 3D shapes are central to industrial design,
virtual reality, and consumer product development. However, most computational
models of 3D aesthetics lack empirical grounding in large-scale human
judgments, limiting their practical relevance. We present a large-scale study
of human preferences. We collected 22,301 pairwise comparisons across five
object categories (chairs, tables, mugs, lamps, and dining chairs) via Amazon
Mechanical Turk. Building on a previously published
dataset~\cite{dev2020learning}, we introduce new non-linear modeling and
cross-category analysis to uncover the geometric drivers of aesthetic
preference. We apply the Bradley-Terry model to infer latent aesthetic scores
and use Random Forests with SHAP analysis to identify and interpret the most
influential geometric features (e.g., symmetry, curvature, compactness). Our
cross-category analysis reveals both universal principles and domain-specific
trends in aesthetic preferences. We focus on human interpretable geometric
features to ensure model transparency and actionable design insights, rather
than relying on black-box deep learning approaches. Our findings bridge
computational aesthetics and cognitive science, providing practical guidance
for designers and a publicly available dataset to support reproducibility. This
work advances the understanding of 3D shape aesthetics through a human-centric,
data-driven framework.

</details>

### [958] [UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes](https://arxiv.org/abs/2505.12774)
*Zichen Geng,Zeeshan Hayder,Wei Liu,Ajmal Mian*

Main category: cs.GR

TLDR: UniHM是一种统一运动语言模型，通过扩散生成技术合成场景感知的人体运动，支持文本到运动和文本到人-物交互任务。


<details>
  <summary>Details</summary>
Motivation: 现有语言条件运动模型在场景感知运动生成中存在局限性，导致信息丢失和无法捕捉3D人体运动的连续性与上下文依赖性。

Method: 提出UniHM框架，采用混合运动表示、新型LFQ-VAE和增强版Lingo数据集，提升运动真实感和生成性能。

Result: 在OMOMO和HumanML3D基准测试中表现优异，支持复杂3D场景中的运动合成。

Conclusion: UniHM在场景感知运动生成中具有显著优势，为多模态运动合成提供了有效解决方案。

Abstract: Human motion synthesis in complex scenes presents a fundamental challenge,
extending beyond conventional Text-to-Motion tasks by requiring the integration
of diverse modalities such as static environments, movable objects, natural
language prompts, and spatial waypoints. Existing language-conditioned motion
models often struggle with scene-aware motion generation due to limitations in
motion tokenization, which leads to information loss and fails to capture the
continuous, context-dependent nature of 3D human movement. To address these
issues, we propose UniHM, a unified motion language model that leverages
diffusion-based generation for synthesizing scene-aware human motion. UniHM is
the first framework to support both Text-to-Motion and Text-to-Human-Object
Interaction (HOI) in complex 3D scenes. Our approach introduces three key
contributions: (1) a mixed-motion representation that fuses continuous 6DoF
motion with discrete local motion tokens to improve motion realism; (2) a novel
Look-Up-Free Quantization VAE (LFQ-VAE) that surpasses traditional VQ-VAEs in
both reconstruction accuracy and generative performance; and (3) an enriched
version of the Lingo dataset augmented with HumanML3D annotations, providing
stronger supervision for scene-specific motion learning. Experimental results
demonstrate that UniHM achieves comparative performance on the OMOMO benchmark
for text-to-HOI synthesis and yields competitive results on HumanML3D for
general text-conditioned motion generation.

</details>

### [959] [AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning](https://arxiv.org/abs/2505.12782)
*Kai Zhang,Xingyu Chen,Xiaofeng Zhang*

Main category: cs.GR

TLDR: AdaToken-3D是一种自适应空间令牌优化框架，通过动态修剪冗余令牌提升3D LMM的效率，同时保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: 当前3D LMM因空间令牌冗余导致计算效率低下，需要优化。

Method: 提出AdaToken-3D框架，通过空间贡献分析和注意力模式挖掘动态修剪冗余令牌。

Result: 在LLaVA-3D上实现21%推理速度提升和63%FLOPs减少，保持准确性。

Conclusion: 揭示了3D多模态学习中空间令牌冗余的规律，为高效学习奠定理论基础。

Abstract: Large Multimodal Models (LMMs) have become a pivotal research focus in deep
learning, demonstrating remarkable capabilities in 3D scene understanding.
However, current 3D LMMs employing thousands of spatial tokens for multimodal
reasoning suffer from critical inefficiencies: excessive computational overhead
and redundant information flows. Unlike 2D VLMs processing single images, 3D
LMMs exhibit inherent architectural redundancy due to the heterogeneous
mechanisms between spatial tokens and visual tokens. To address this challenge,
we propose AdaToken-3D, an adaptive spatial token optimization framework that
dynamically prunes redundant tokens through spatial contribution analysis. Our
method automatically tailors pruning strategies to different 3D LMM
architectures by quantifying token-level information flows via attention
pattern mining. Extensive experiments on LLaVA-3D (a 7B parameter 3D-LMM)
demonstrate that AdaToken-3D achieves 21\% faster inference speed and 63\%
FLOPs reduction while maintaining original task accuracy. Beyond efficiency
gains, this work systematically investigates redundancy patterns in multimodal
spatial information flows through quantitative token interaction analysis. Our
findings reveal that over 60\% of spatial tokens contribute minimally ($<$5\%)
to the final predictions, establishing theoretical foundations for efficient 3D
multimodal learning.

</details>

### [960] [Neural Importance Sampling of Many Lights](https://arxiv.org/abs/2505.11729)
*Pedro Figueiredo,Qihao He,Steve Bako,Nima Khademi Kalantari*

Main category: cs.GR

TLDR: 提出一种基于神经网络的动态光选择分布估计方法，用于改进蒙特卡洛渲染中的重要性采样，适用于多光源复杂场景。


<details>
  <summary>Details</summary>
Motivation: 解决复杂场景中多光源重要性采样的效率问题。

Method: 使用神经网络预测每个着色点的光选择分布，结合光层次结构技术管理大量光源，并引入残差学习策略加速训练收敛。

Result: 在多样化和挑战性场景中表现优异。

Conclusion: 该方法显著提升了蒙特卡洛渲染中重要性采样的效率和性能。

Abstract: We propose a neural approach for estimating spatially varying light selection
distributions to improve importance sampling in Monte Carlo rendering,
particularly for complex scenes with many light sources. Our method uses a
neural network to predict the light selection distribution at each shading
point based on local information, trained by minimizing the KL-divergence
between the learned and target distributions in an online manner. To
efficiently manage hundreds or thousands of lights, we integrate our neural
approach with light hierarchy techniques, where the network predicts
cluster-level distributions and existing methods sample lights within clusters.
Additionally, we introduce a residual learning strategy that leverages initial
distributions from existing techniques, accelerating convergence during
training. Our method achieves superior performance across diverse and
challenging scenes.

</details>

<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [961] [Prink: $k_s$-Anonymization for Streaming Data in Apache Flink](https://arxiv.org/abs/2505.13153)
*Philip Groneberg,Saskia Nuñez von Voigt,Thomas Janke,Louis Loechel,Karl Wolf,Elias Grünewald,Frank Pallas*

Main category: cs.DC

TLDR: Prink是一种新颖且实用的ks-匿名化数据流的概念和原型，扩展了CASTLE方案，支持非数值数据的语义感知匿名化，并集成到Apache Flink中。


<details>
  <summary>Details</summary>
Motivation: 解决现有流数据匿名化方法无法处理非数值数据、提供离散数据点以及在实际系统中集成困难的问题。

Method: 基于CASTLE方案，引入语义感知的ks-匿名化方法，优化信息损失，并原生集成到Apache Flink中。

Result: 支持非数值数据匿名化，提供离散数据点，实验证明性能开销和信息损失可接受。

Conclusion: Prink是一种适用于多种实际流处理应用的高效匿名化方法。

Abstract: In this paper, we present Prink, a novel and practically applicable concept
and fully implemented prototype for ks-anonymizing data streams in real-world
application architectures. Building upon the pre-existing, yet rudimentary
CASTLE scheme, Prink for the first time introduces semantics-aware
ks-anonymization of non-numerical (such as categorical or hierarchically
generalizable) streaming data in a information loss-optimized manner. In
addition, it provides native integration into Apache Flink, one of the
prevailing frameworks for enterprise-grade stream data processing in numerous
application domains.
  Our contributions excel the previously established state of the art for the
privacy guarantee-providing anonymization of streaming data in that they 1)
allow to include non-numerical data in the anonymization process, 2) provide
discrete datapoints instead of aggregates, thereby facilitating flexible data
use, 3) are applicable in real-world system contexts with minimal integration
efforts, and 4) are experimentally proven to raise acceptable performance
overheads and information loss in realistic settings. With these
characteristics, Prink provides an anonymization approach which is practically
feasible for a broad variety of real-world, enterprise-grade stream processing
applications and environments.

</details>

### [962] [Cloud-Based AI Systems: Leveraging Large Language Models for Intelligent Fault Detection and Autonomous Self-Healing](https://arxiv.org/abs/2505.11743)
*Cheng Ji,Huaiying Luo*

Main category: cs.DC

TLDR: 提出了一种基于大语言模型（LLM）的AI框架，用于云系统的智能故障检测和自愈机制，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 云计算的快速发展和基础设施复杂性增加，需要实时故障检测和缓解的智能机制。

Method: 结合机器学习故障检测算法和LLM的自然语言理解能力，采用多级架构，结合监督学习和无监督学习。

Result: 实验表明，模型在故障检测准确性、系统停机时间减少和恢复速度方面显著优于传统系统。

Conclusion: 该AI框架为云系统提供了高效的智能故障检测和自愈解决方案。

Abstract: With the rapid development of cloud computing systems and the increasing
complexity of their infrastructure, intelligent mechanisms to detect and
mitigate failures in real time are becoming increasingly important. Traditional
methods of failure detection are often difficult to cope with the scale and
dynamics of modern cloud environments. In this study, we propose a novel AI
framework based on Massive Language Model (LLM) for intelligent fault detection
and self-healing mechanisms in cloud systems. The model combines existing
machine learning fault detection algorithms with LLM's natural language
understanding capabilities to process and parse system logs, error reports, and
real-time data streams through semantic context. The method adopts a
multi-level architecture, combined with supervised learning for fault
classification and unsupervised learning for anomaly detection, so that the
system can predict potential failures before they occur and automatically
trigger the self-healing mechanism. Experimental results show that the proposed
model is significantly better than the traditional fault detection system in
terms of fault detection accuracy, system downtime reduction and recovery
speed.

</details>

### [963] [Communication-Efficient Hybrid Language Model via Uncertainty-Aware Opportunistic and Compressed Transmission](https://arxiv.org/abs/2505.11788)
*Seungeun Oh,Jinhyuk Kim,Jihong Park,Seung-Woo Ko,Jinho Choi,Tony Q. S. Quek,Seong-Lyun Kim*

Main category: cs.DC

TLDR: CU-HLM是一种高效的混合语言模型，通过减少通信开销和优化词汇截断策略，显著提高了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决原始HLM中因上传完整词汇分布和验证高概率接受令牌导致的通信和计算资源浪费问题。

Method: 提出CU-HLM，仅在SLM输出不确定性高时传输截断的词汇分布，并理论推导最优不确定性阈值和词汇截断策略。

Result: CU-HLM比标准HLM吞吐量提高206倍，跳过74.8%的传输，词汇压缩97.4%，同时保持97.4%的准确率。

Conclusion: CU-HLM通过通信高效和不确定性感知的设计，显著提升了混合语言模型的性能。

Abstract: To support emerging language-based applications using dispersed and
heterogeneous computing resources, the hybrid language model (HLM) offers a
promising architecture, where an on-device small language model (SLM) generates
draft tokens that are validated and corrected by a remote large language model
(LLM). However, the original HLM suffers from substantial communication
overhead, as the LLM requires the SLM to upload the full vocabulary
distribution for each token. Moreover, both communication and computation
resources are wasted when the LLM validates tokens that are highly likely to be
accepted. To overcome these limitations, we propose communication-efficient and
uncertainty-aware HLM (CU-HLM). In CU-HLM, the SLM transmits truncated
vocabulary distributions only when its output uncertainty is high. We validate
the feasibility of this opportunistic transmission by discovering a strong
correlation between SLM's uncertainty and LLM's rejection probability.
Furthermore, we theoretically derive optimal uncertainty thresholds and optimal
vocabulary truncation strategies. Simulation results show that, compared to
standard HLM, CU-HLM achieves up to 206$\times$ higher token throughput by
skipping 74.8% transmissions with 97.4% vocabulary compression, while
maintaining 97.4% accuracy.

</details>

### [964] [ZenFlow: Enabling Stall-Free Offloading Training via Asynchronous Updates](https://arxiv.org/abs/2505.12242)
*Tingfeng Lan,Yusen Wu,Bin Ma,Zhaoyuan Su,Rui Yang,Tekin Bicer,Dong Li,Yue Cheng*

Main category: cs.DC

TLDR: ZenFlow是一种新的卸载框架，通过优先处理重要参数并解耦GPU和CPU的更新，显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有卸载训练框架（如ZeRO-Offload）将所有参数平等对待，导致GPU因等待CPU更新而闲置，效率低下。

Method: ZenFlow在GPU上原地更新重要梯度，同时异步卸载和累积次要梯度到CPU，完全重叠CPU和GPU的工作。此外，ZenFlow引入了一种轻量级梯度选择方法，利用梯度的时空局部性避免全局同步。

Result: ZenFlow实现了端到端5倍加速，PCIe流量降低2倍，GPU闲置减少85%，同时保持准确性。

Conclusion: ZenFlow通过智能参数卸载和更新策略，显著提升了大规模语言模型训练的效率和资源利用率。

Abstract: Fine-tuning large language models (LLMs) often exceeds GPU memory limits,
prompting systems to offload model states to CPU memory. However, existing
offloaded training frameworks like ZeRO-Offload treat all parameters equally
and update the full model on the CPU, causing severe GPU stalls, where fast,
expensive GPUs sit idle waiting for slow CPU updates and limited-bandwidth PCIe
transfers.
  We present ZenFlow, a new offloading framework that prioritizes important
parameters and decouples updates between GPU and CPU. ZenFlow performs in-place
updates of important gradients on GPU, while asynchronously offloading and
accumulating less important ones on CPU, fully overlapping CPU work with GPU
computation.
  To scale across GPUs, ZenFlow introduces a lightweight gradient selection
method that exploits a novel spatial and temporal locality property of
important gradients, avoiding costly global synchronization. ZenFlow achieves
up to 5x end-to-end speedup, 2x lower PCIe traffic, and reduces GPU stalls by
over 85 percent, all while preserving accuracy.

</details>

### [965] [Learning in Chaos: Efficient Autoscaling and Self-healing for Distributed Training at the Edge](https://arxiv.org/abs/2505.12815)
*Wenjiao Feng,Rongxing Xiao,Zonghang Li,Hongfang Yu,Gang Sun,Long Luo,Mohsen Guizani,Qirong Ho*

Main category: cs.DC

TLDR: Chaos是一个具有自我修复和自动扩展功能的边缘分布式训练系统，通过多邻居复制和快速分片调度加速扩展，并通过集群监控和点对点协商协议实现完全自管理的自动扩展。


<details>
  <summary>Details</summary>
Motivation: 边缘AI集群中频繁的节点和链接变化会干扰分布式训练，传统的检查点恢复和云中心自动扩展速度慢且不适合边缘环境。

Method: Chaos采用多邻居复制与快速分片调度加速扩展，通过集群监控跟踪资源与拓扑变化，并使用点对点协商协议处理扩展事件。

Result: 实验表明，Chaos的扩展延迟显著低于其他系统，能在一毫秒内处理节点加入、退出和故障，并实现最低的空闲时间。

Conclusion: Chaos在边缘分布式训练中表现出卓越的弹性、资源利用率和可扩展性。

Abstract: Frequent node and link changes in edge AI clusters disrupt distributed
training, while traditional checkpoint-based recovery and cloud-centric
autoscaling are too slow for scale-out and ill-suited to chaotic and
self-governed edge. This paper proposes Chaos, a resilient and scalable edge
distributed training system with built-in self-healing and autoscaling. It
speeds up scale-out by using multi-neighbor replication with fast shard
scheduling, allowing a new node to pull the latest training state from nearby
neighbors in parallel while balancing the traffic load between them. It also
uses a cluster monitor to track resource and topology changes to assist
scheduler decisions, and handles scaling events through peer negotiation
protocols, enabling fully self-governed autoscaling without a central admin.
Extensive experiments show that Chaos consistently achieves much lower
scale-out delays than Pollux, EDL, and Autoscaling, and handles scale-in,
connect-link, and disconnect-link events within 1 millisecond, making it
smoother to handle node joins, exits, and failures. It also delivers the lowest
idle time, showing superior resource use and scalability as the cluster grows.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [966] [TARGET: Benchmarking Table Retrieval for Generative Tasks](https://arxiv.org/abs/2505.11545)
*Xingyu Ji,Parker Glenn,Aditya G. Parameswaran,Madelon Hulsebos*

Main category: cs.IR

TLDR: 论文介绍了TARGET基准，用于评估生成任务中的表格检索性能，发现密集嵌入检索优于BM25基线，并分析了检索器对下游任务的影响。


<details>
  <summary>Details</summary>
Motivation: 结构化数据在数据分析和机器学习中具有重要价值，但如何为查询或任务检索正确的表格是关键问题。

Method: 提出TARGET基准，评估不同检索器的性能及其对下游任务的影响，包括密集嵌入检索和BM25基线。

Result: 密集嵌入检索显著优于BM25基线，检索性能对元数据（如表标题缺失）敏感，且在不同数据集和任务中差异显著。

Conclusion: TARGET为表格检索提供了评估基准，揭示了检索器的性能差异及其对生成任务的重要性。

Abstract: The data landscape is rich with structured data, often of high value to
organizations, driving important applications in data analysis and machine
learning. Recent progress in representation learning and generative models for
such data has led to the development of natural language interfaces to
structured data, including those leveraging text-to-SQL. Contextualizing
interactions, either through conversational interfaces or agentic components,
in structured data through retrieval-augmented generation can provide
substantial benefits in the form of freshness, accuracy, and comprehensiveness
of answers. The key question is: how do we retrieve the right table(s) for the
analytical query or task at hand? To this end, we introduce TARGET: a benchmark
for evaluating TAble Retrieval for GEnerative Tasks. With TARGET we analyze the
retrieval performance of different retrievers in isolation, as well as their
impact on downstream tasks. We find that dense embedding-based retrievers far
outperform a BM25 baseline which is less effective than it is for retrieval
over unstructured text. We also surface the sensitivity of retrievers across
various metadata (e.g., missing table titles), and demonstrate a stark
variation of retrieval performance across datasets and tasks. TARGET is
available at https://target-benchmark.github.io.

</details>

### [967] [GSPRec: Temporal-Aware Graph Spectral Filtering for Recommendation](https://arxiv.org/abs/2505.11552)
*Ahmad Bin Rabiah,Julian McAuley*

Main category: cs.IR

TLDR: GSPRec是一种基于图谱的推荐模型，通过时序增强的图构建和频率感知滤波解决传统图推荐系统的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统图推荐系统过度依赖低通滤波，抑制用户特定信号，且忽略了时序动态。

Method: GSPRec通过多跳扩散编码项目转移，设计双滤波机制（高斯带通滤波和低通滤波）捕捉用户偏好。

Result: 在四个公开数据集上，GSPRec平均NDCG@10提升6.77%。

Conclusion: 时序图增强和带通滤波的互补性验证了GSPRec的有效性。

Abstract: Graph-based recommendation systems are effective at modeling collaborative
patterns but often suffer from two limitations: overreliance on low-pass
filtering, which suppresses user-specific signals, and omission of sequential
dynamics in graph construction. We introduce GSPRec, a graph spectral model
that integrates temporal transitions through sequentially-informed graph
construction and applies frequency-aware filtering in the spectral domain.
GSPRec encodes item transitions via multi-hop diffusion to enable the use of
symmetric Laplacians for spectral processing. To capture user preferences, we
design a dual-filtering mechanism: a Gaussian bandpass filter to extract
mid-frequency, user-level patterns, and a low-pass filter to retain global
trends. Extensive experiments on four public datasets show that GSPRec
consistently outperforms baselines, with an average improvement of 6.77% in
NDCG@10. Ablation studies show the complementary benefits of both sequential
graph augmentation and bandpass filtering.

</details>

### [968] [Comparing Lexical and Semantic Vector Search Methods When Classifying Medical Documents](https://arxiv.org/abs/2505.11582)
*Lee Harris,Philippe De Wilde,James Bentham*

Main category: cs.IR

TLDR: 传统词法向量搜索在结构化医学文档分类中表现优于现成的语义向量搜索，且速度更快。


<details>
  <summary>Details</summary>
Motivation: 探讨在结构化医学文档分类任务中，传统方法与神经方法的性能差异。

Method: 比较现成的语义向量搜索与定制的词法向量搜索模型在分类任务中的表现。

Result: 定制词法向量搜索在预测准确性和执行速度上均优于语义向量搜索。

Conclusion: 传统方法在信息检索中仍具竞争力，不应被忽视。

Abstract: Classification is a common AI problem, and vector search is a typical
solution. This transforms a given body of text into a numerical representation,
known as an embedding, and modern improvements to vector search focus on
optimising speed and predictive accuracy. This is often achieved through neural
methods that aim to learn language semantics. However, our results suggest that
these are not always the best solution. Our task was to classify
rigidly-structured medical documents according to their content, and we found
that using off-the-shelf semantic vector search produced slightly worse
predictive accuracy than creating a bespoke lexical vector search model, and
that it required significantly more time to execute. These findings suggest
that traditional methods deserve to be contenders in the information retrieval
toolkit, despite the prevalence and success of neural models.

</details>

### [969] [Second SIGIR Workshop on Simulations for Information Access (Sim4IA 2025)](https://arxiv.org/abs/2505.11687)
*Philipp Schaer,Christin Katharina Kreutz,Krisztian Balog,Timo Breuer,Andreas Konstantin Kruff*

Main category: cs.IR

TLDR: 论文探讨了信息访问（IA）中模拟的重要性及其在研究、评估和用户理解中的作用，并介绍了Sim4IA研讨会的最新进展。


<details>
  <summary>Details</summary>
Motivation: 模拟在IA研究中具有重要作用，尤其是在无法使用真实用户或涉及伦理问题时。此外，模拟还能简化实验、提高可重复性。

Method: 基于近期方法和技术的发展，Sim4IA研讨会汇集研究者和从业者，讨论IA模拟的未来方向，并计划TREC/CLEF活动。

Result: 研讨会为IA模拟的未来发展提供了互动平台，并推动了相关活动的规划。

Conclusion: 模拟在IA研究中具有潜力，Sim4IA研讨会为其发展提供了重要支持。

Abstract: Simulations in information access (IA) have recently gained interest, as
shown by various tutorials and workshops around that topic. Simulations can be
key contributors to central IA research and evaluation questions, especially
around interactive settings when real users are unavailable, or their
participation is impossible due to ethical reasons. In addition, simulations in
IA can help contribute to a better understanding of users, reduce complexity of
evaluation experiments, and improve reproducibility. Building on recent
developments in methods and toolkits, the second iteration of our Sim4IA
workshop aims to again bring together researchers and practitioners to form an
interactive and engaging forum for discussions on the future perspectives of
the field. An additional aim is to plan an upcoming TREC/CLEF campaign.

</details>

### [970] [Conversational Recommendation System using NLP and Sentiment Analysis](https://arxiv.org/abs/2505.11933)
*Piyush Talegaonkar,Siddhant Hole,Shrinesh Kamble,Prashil Gulechha,Deepali Salapurkar*

Main category: cs.IR

TLDR: 本文提出了一种结合对话数据的推荐系统新方法，利用深度学习和语音识别技术提升个性化推荐效果。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统难以利用对话数据的丰富信息，本文旨在通过整合对话洞察改进推荐系统的个性化和上下文感知能力。

Method: 结合内容推荐和协同过滤方法，并融入NLP技术，使用深度学习（CNN、RNN、LSTM）、关联规则挖掘（Apriori）及语音识别技术（HMM、DTW）。

Result: 系统在营销应用中实现了更个性化和上下文感知的推荐体验。

Conclusion: 通过整合对话数据和先进技术，本文提出的方法显著提升了推荐系统的效果。

Abstract: In today's digitally-driven world, the demand for personalized and
context-aware recommendations has never been greater. Traditional recommender
systems have made significant strides in this direction, but they often lack
the ability to tap into the richness of conversational data. This paper
represents a novel approach to recommendation systems by integrating
conversational insights into the recommendation process. The Conversational
Recommender System integrates cutting-edge technologies such as deep learning,
leveraging machine learning algorithms like Apriori for Association Rule
Mining, Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN),
and Long Short-Term Memory (LTSM). Furthermore, sophisticated voice recognition
technologies, including Hidden Markov Models (HMMs) and Dynamic Time Warping
(DTW) algorithms, play a crucial role in accurate speech-to-text conversion,
ensuring robust performance in diverse environments. The methodology
incorporates a fusion of content-based and collaborative recommendation
approaches, enhancing them with NLP techniques. This innovative integration
ensures a more personalized and context-aware recommendation experience,
particularly in marketing applications.

</details>

### [971] [Let's have a chat with the EU AI Act](https://arxiv.org/abs/2505.11946)
*Adam Kovari,Yasin Ghafourian,Csaba Hegedus,Belal Abu Naim,Kitti Mezei,Pal Varga,Markus Tauber*

Main category: cs.IR

TLDR: 本文介绍了一种基于RAG框架的AI聊天机器人，帮助用户遵守欧盟AI法案及相关标准。


<details>
  <summary>Details</summary>
Motivation: 随着AI法规日益复杂，开发者面临合规挑战，需要工具简化流程。

Method: 采用检索增强生成（RAG）框架，结合公共和专有标准，提供实时合规指导。

Result: 聊天机器人能有效减少合规复杂性，促进负责任AI开发。

Conclusion: 该工具有潜力提升AI治理效率，未来可进一步优化模型。

Abstract: As artificial intelligence (AI) regulations evolve and the regulatory
landscape develops and becomes more complex, ensuring compliance with ethical
guidelines and legal frameworks remains a challenge for AI developers. This
paper introduces an AI-driven self-assessment chatbot designed to assist users
in navigating the European Union AI Act and related standards. Leveraging a
Retrieval-Augmented Generation (RAG) framework, the chatbot enables real-time,
context-aware compliance verification by retrieving relevant regulatory texts
and providing tailored guidance. By integrating both public and proprietary
standards, it streamlines regulatory adherence, reduces complexity, and fosters
responsible AI development. The paper explores the chatbot's architecture,
comparing naive and graph-based RAG models, and discusses its potential impact
on AI governance.

</details>

### [972] [MIRACL-VISION: A Large, multilingual, visual document retrieval benchmark](https://arxiv.org/abs/2505.11651)
*Radek Osmulsk,Gabriel de Souza P. Moreira,Ronay Ak,Mengyao Xu,Benedikt Schifferer,Even Oldridge*

Main category: cs.IR

TLDR: MIRACL-VISION是一个多语言视觉文档检索评估基准，扩展自MIRACL数据集，覆盖18种语言，旨在解决现有视觉文档检索基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉文档检索基准主要关注英语，依赖合成问题且语料库规模小，无法满足多语言和复杂布局文档的需求。

Method: 通过消除语料库中的“简单”负样本，减少语料库规模以优化计算效率，同时保持挑战性。

Result: 实验表明，基于视觉的检索模型在多语言能力上存在显著差距，准确率比文本模型低59.7%，英语环境下低12.1%。

Conclusion: MIRACL-VISION为视觉检索管道提供了一个具有挑战性和代表性的多语言评估基准，有助于开发更鲁棒的文档检索模型。

Abstract: Document retrieval is an important task for search and Retrieval-Augmented
Generation (RAG) applications. Large Language Models (LLMs) have contributed to
improving the accuracy of text-based document retrieval. However, documents
with complex layout and visual elements like tables, charts and infographics
are not perfectly represented in textual format. Recently, image-based document
retrieval pipelines have become popular, which use visual large language models
(VLMs) to retrieve relevant page images given a query. Current evaluation
benchmarks on visual document retrieval are limited, as they primarily focus
only English language, rely on synthetically generated questions and offer a
small corpus size. Therefore, we introduce MIRACL-VISION, a multilingual visual
document retrieval evaluation benchmark. MIRACL-VISION covers 18 languages, and
is an extension of the MIRACL dataset, a popular benchmark to evaluate
text-based multilingual retrieval pipelines. MIRACL was built using a
human-intensive annotation process to generate high-quality questions. In order
to reduce MIRACL-VISION corpus size to make evaluation more compute friendly
while keeping the datasets challenging, we have designed a method for
eliminating the "easy" negatives from the corpus. We conducted extensive
experiments comparing MIRACL-VISION with other benchmarks, using popular public
text and image models. We observe a gap in state-of-the-art VLM-based embedding
models on multilingual capabilities, with up to 59.7% lower retrieval accuracy
than a text-based retrieval models. Even for the English language, the visual
models retrieval accuracy is 12.1% lower compared to text-based models.
MIRACL-VISION is a challenging, representative, multilingual evaluation
benchmark for visual retrieval pipelines and will help the community build
robust models for document retrieval.

</details>

### [973] [LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference](https://arxiv.org/abs/2505.12260)
*Guangyuan Ma,Yongliang Ma,Xuanrui Gou,Zhenpeng Su,Ming Zhou,Songlin Hu*

Main category: cs.IR

TLDR: LightRetriever是一种基于LLM的混合检索器，通过极轻量级的查询编码器显著提升查询推理速度，同时保留大部分性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM显著提升了检索能力，但其深度参数化导致查询推理速度下降和在线部署资源需求增加。

Method: 保留完整大小的LLM用于文档编码，但将查询编码的工作量减少到不超过嵌入查找。

Result: 在H800 GPU上实现了1000倍以上的查询推理加速，无GPU时也有20倍加速，且性能保留95%。

Conclusion: LightRetriever在保持高性能的同时显著提升了效率，适用于多样化检索任务。

Abstract: Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode
queries and documents into low-dimensional dense or high-dimensional sparse
vectors. It retrieves documents relevant to search queries based on vector
similarities. Documents are pre-encoded offline, while queries arrive in
real-time, necessitating an efficient online query encoder. Although LLMs
significantly enhance retrieval capabilities, serving deeply parameterized LLMs
slows down query inference throughput and increases demands for online
deployment resources. In this paper, we propose LightRetriever, a novel
LLM-based hybrid retriever with extremely lightweight query encoders. Our
method retains a full-sized LLM for document encoding, but reduces the workload
of query encoding to no more than an embedding lookup. Compared to serving a
full-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for
query inference with GPU acceleration, and even a 20x speedup without GPU.
Experiments on large-scale retrieval benchmarks demonstrate that our method
generalizes well across diverse retrieval tasks, retaining an average of 95%
full-sized performance.

</details>

### [974] [Unlearning for Federated Online Learning to Rank: A Reproducibility Study](https://arxiv.org/abs/2505.12791)
*Yiling Tao,Shuyi Wang,Jiaxi Yang,Guido Zuccon*

Main category: cs.IR

TLDR: 该论文研究了联邦在线学习排序（FOLTR）中联邦遗忘策略的有效性和效率，重点关注如何系统且可验证地分析遗忘能力。


<details>
  <summary>Details</summary>
Motivation: 随着各国立法确立‘被遗忘权’，FOLTR等基于机器学习模型的服务需提供用户数据删除功能。当前评估方法单一且存在争议，亟需多指标综合评估。

Method: 研究通过调整和新提出的评估指标，严格评估五种遗忘策略在欠遗忘和过遗忘场景中的有效性。

Result: 详细分析揭示了五种遗忘策略的优势与局限，为优化联邦遗忘以平衡数据隐私和系统性能提供了宝贵见解。

Conclusion: 论文公开了代码和完整结果，为FOLTR中联邦遗忘的进一步研究和实践提供了支持。

Abstract: This paper reports on findings from a comparative study on the effectiveness
and efficiency of federated unlearning strategies within Federated Online
Learning to Rank (FOLTR), with specific attention to systematically analysing
the unlearning capabilities of methods in a verifiable manner.
  Federated approaches to ranking of search results have recently garnered
attention to address users privacy concerns. In FOLTR, privacy is safeguarded
by collaboratively training ranking models across decentralized data sources,
preserving individual user data while optimizing search results based on
implicit feedback, such as clicks.
  Recent legislation introduced across numerous countries is establishing the
so called "the right to be forgotten", according to which services based on
machine learning models like those in FOLTR should provide capabilities that
allow users to remove their own data from those used to train models. This has
sparked the development of unlearning methods, along with evaluation practices
to measure whether unlearning of a user data successfully occurred. Current
evaluation practices are however often controversial, necessitating the use of
multiple metrics for a more comprehensive assessment -- but previous proposals
of unlearning methods only used single evaluation metrics.
  This paper addresses this limitation: our study rigorously assesses the
effectiveness of unlearning strategies in managing both under-unlearning and
over-unlearning scenarios using adapted, and newly proposed evaluation metrics.
Thanks to our detailed analysis, we uncover the strengths and limitations of
five unlearning strategies, offering valuable insights into optimizing
federated unlearning to balance data privacy and system performance within
FOLTR. We publicly release our code and complete results at
https://github.com/Iris1026/Unlearning-for-FOLTR.git.

</details>

<div id='physics.atom-ph'></div>

# physics.atom-ph [[Back]](#toc)

### [975] [A Comprehensive Benchmarking Platform for Deep Generative Models in Molecular Design](https://arxiv.org/abs/2505.12848)
*Adarsh Singh*

Main category: physics.atom-ph

TLDR: 论文分析了MOSES平台，用于标准化评估分子设计中的深度生成模型，比较了不同架构的优劣。


<details>
  <summary>Details</summary>
Motivation: 药物开发成本高、耗时长，深度生成模型有望加速这一过程，但缺乏标准化评估方法。

Method: 通过MOSES平台评估多种生成模型（如RNN、VAE、GAN）在生成分子结构时的表现。

Result: 不同架构在不同指标上各具优势，揭示了化学空间探索与利用的权衡。

Conclusion: 研究为AI驱动的药物发现提供了标准化评估基础，并指出了未来发展方向。

Abstract: The development of novel pharmaceuticals represents a significant challenge
in modern science, with substantial costs and time investments. Deep generative
models have emerged as promising tools for accelerating drug discovery by
efficiently exploring the vast chemical space. However, this rapidly evolving
field lacks standardized evaluation protocols, impeding fair comparison between
approaches. This research presents an extensive analysis of the Molecular Sets
(MOSES) platform, a comprehensive benchmarking framework designed to
standardize evaluation of deep generative models in molecular design. Through
rigorous assessment of multiple generative architectures, including recurrent
neural networks, variational autoencoders, and generative adversarial networks,
we examine their capabilities in generating valid, unique, and novel molecular
structures while maintaining specific chemical properties. Our findings reveal
that different architectures exhibit complementary strengths across various
metrics, highlighting the complex trade-offs between exploration and
exploitation in chemical space. This study provides detailed insights into the
current state of the art in molecular generation and establishes a foundation
for future advancements in AI-driven drug discovery.

</details>

<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [976] [BrainNetMLP: An Efficient and Effective Baseline for Functional Brain Network Classification](https://arxiv.org/abs/2505.11538)
*Jiacheng Hou,Zhenjie Song,Ercan Engin Kuruoglu*

Main category: q-bio.NC

TLDR: 论文提出了一种基于纯MLP的方法BrainNetMLP，用于功能脑网络分类，展示了简单模型的高效性和有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型复杂度增加，但性能提升不明显，因此重新审视最简单的MLP架构，探索其潜力。

Method: 提出BrainNetMLP，采用双分支结构融合空间连接和频谱信息，实现精确的时空特征融合。

Result: 在HCP和ABIDE数据集上，BrainNetMLP达到了最先进的性能。

Conclusion: MLP模型可以作为功能脑网络分类中高效且有效的替代方案。

Abstract: Recent studies have made great progress in functional brain network
classification by modeling the brain as a network of Regions of Interest (ROIs)
and leveraging their connections to understand brain functionality and diagnose
mental disorders. Various deep learning architectures, including Convolutional
Neural Networks, Graph Neural Networks, and the recent Transformer, have been
developed. However, despite the increasing complexity of these models, the
performance gain has not been as salient. This raises a question: Does
increasing model complexity necessarily lead to higher classification accuracy?
In this paper, we revisit the simplest deep learning architecture, the
Multi-Layer Perceptron (MLP), and propose a pure MLP-based method, named
BrainNetMLP, for functional brain network classification, which capitalizes on
the advantages of MLP, including efficient computation and fewer parameters.
Moreover, BrainNetMLP incorporates a dual-branch structure to jointly capture
both spatial connectivity and spectral information, enabling precise
spatiotemporal feature fusion. We evaluate our proposed BrainNetMLP on two
public and popular brain network classification datasets, the Human Connectome
Project (HCP) and the Autism Brain Imaging Data Exchange (ABIDE). Experimental
results demonstrate pure MLP-based methods can achieve state-of-the-art
performance, revealing the potential of MLP-based models as more efficient yet
effective alternatives in functional brain network classification. The code
will be available at https://github.com/JayceonHo/BrainNetMLP.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [977] [Behind the Screens: Uncovering Bias in AI-Driven Video Interview Assessments Using Counterfactuals](https://arxiv.org/abs/2505.12114)
*Dena F. Mujtaba,Nihar R. Mahapatra*

Main category: cs.HC

TLDR: 论文提出了一种基于反事实的框架，用于评估和量化AI驱动的人格评估中的偏见，利用GAN生成反事实表示，支持多模态公平性分析。


<details>
  <summary>Details</summary>
Motivation: AI在人格评估中的应用可能放大训练数据中的偏见，导致基于性别、种族和年龄的歧视性结果，需要一种方法来系统评估和解决这一问题。

Method: 采用生成对抗网络（GAN）生成反事实的求职者表示，通过改变受保护属性进行公平性分析，支持多模态（视觉、音频、文本）评估。

Result: 应用于先进的人格预测模型时，该方法揭示了不同人口群体间的显著差异，并通过受保护属性分类器验证了反事实生成的有效性。

Conclusion: 该框架为商业AI招聘平台的公平性审计提供了可扩展工具，尤其在黑盒环境下，强调了反事实方法在提升情感计算伦理透明度中的重要性。

Abstract: AI-enhanced personality assessments are increasingly shaping hiring
decisions, using affective computing to predict traits from the Big Five
(OCEAN) model. However, integrating AI into these assessments raises ethical
concerns, especially around bias amplification rooted in training data. These
biases can lead to discriminatory outcomes based on protected attributes like
gender, ethnicity, and age. To address this, we introduce a
counterfactual-based framework to systematically evaluate and quantify bias in
AI-driven personality assessments. Our approach employs generative adversarial
networks (GANs) to generate counterfactual representations of job applicants by
altering protected attributes, enabling fairness analysis without access to the
underlying model. Unlike traditional bias assessments that focus on unimodal or
static data, our method supports multimodal evaluation-spanning visual, audio,
and textual features. This comprehensive approach is particularly important in
high-stakes applications like hiring, where third-party vendors often provide
AI systems as black boxes. Applied to a state-of-the-art personality prediction
model, our method reveals significant disparities across demographic groups. We
also validate our framework using a protected attribute classifier to confirm
the effectiveness of our counterfactual generation. This work provides a
scalable tool for fairness auditing of commercial AI hiring platforms,
especially in black-box settings where training data and model internals are
inaccessible. Our results highlight the importance of counterfactual approaches
in improving ethical transparency in affective computing.

</details>

### [978] [How Adding Metacognitive Requirements in Support of AI Feedback in Practice Exams Transforms Student Learning Behaviors](https://arxiv.org/abs/2505.13381)
*Mak Ahmad,Prerna Ravi,David Karger,Marc Facciotti*

Main category: cs.HC

TLDR: 论文介绍了一个结合AI生成反馈和教材引用的实践考试系统，用于大型生物学课程，旨在提升学生元认知能力和学习效果。


<details>
  <summary>Details</summary>
Motivation: 解决大规模STEM课程中个性化反馈的难题，促进学生元认知行为和学习效果。

Method: 系统使用GPT-4生成个性化反馈，结合教材引用，要求学生解释答案并评估信心。通过日志、调查和访谈分析效果。

Result: 反馈类型对成绩无显著影响，但信心评分和解释要求对学生实际考试策略有积极影响。40%学生使用教材引用，满意度高。

Conclusion: 结构化反思要求比复杂反馈机制更具影响力。

Abstract: Providing personalized, detailed feedback at scale in large undergraduate
STEM courses remains a persistent challenge. We present an empirically
evaluated practice exam system that integrates AI generated feedback with
targeted textbook references, deployed in a large introductory biology course.
Our system encourages metacognitive behavior by asking students to explain
their answers and declare their confidence. It uses OpenAI's GPT-4o to generate
personalized feedback based on this information, while directing them to
relevant textbook sections. Through interaction logs from consenting
participants across three midterms (541, 342, and 413 students respectively),
totaling 28,313 question-student interactions across 146 learning objectives,
along with 279 surveys and 23 interviews, we examined the system's impact on
learning outcomes and engagement. Across all midterms, feedback types showed no
statistically significant performance differences, though some trends suggested
potential benefits. The most substantial impact came from the required
confidence ratings and explanations, which students reported transferring to
their actual exam strategies. About 40 percent of students engaged with
textbook references when prompted by feedback -- far higher than traditional
reading rates. Survey data revealed high satisfaction (mean rating 4.1 of 5),
with 82.1 percent reporting increased confidence on practiced midterm topics,
and 73.4 percent indicating they could recall and apply specific concepts. Our
findings suggest that embedding structured reflection requirements may be more
impactful than sophisticated feedback mechanisms.

</details>

<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [979] [Improving the discovery of near-Earth objects with machine-learning methods](https://arxiv.org/abs/2505.11910)
*Peter Vereš,Richard Cloete,Matthew J. Payne,Abraham Loeb*

Main category: astro-ph.IM

TLDR: 通过分析2019至2024年NEOCP候选天体的digest2参数，提出减少非近地天体（NEO）误报的方法，结合机器学习模型显著提升了分类精度。


<details>
  <summary>Details</summary>
Motivation: 近地天体发现数量增加，但NEOCP中仅约一半候选被确认为NEO，导致大量观测时间浪费在非NEO上。

Method: 分析30种digest2参数，设计过滤机制，并应用四种机器学习技术（GBM、RF、SGD、NN）分类候选天体。

Result: 过滤机制排除了20%的非NEO，机器学习模型分类精度达95%，综合方法减少80%以上非NEO，NEO丢失率仅5.5%。

Conclusion: 结合digest2参数过滤与机器学习模型，显著优化NEOCP候选天体分类效率，减少资源浪费。

Abstract: We present a comprehensive analysis of the digest2 parameters for candidates
of the Near-Earth Object Confirmation Page (NEOCP) that were reported between
2019 and 2024. Our study proposes methods for significantly reducing the
inclusion of non-NEO objects on the NEOCP. Despite the substantial increase in
near-Earth object (NEO) discoveries in recent years, only about half of the
NEOCP candidates are ultimately confirmed as NEOs. Therefore, much observing
time is spent following up on non-NEOs. Furthermore, approximately 11% of the
candidates remain unconfirmed because the follow-up observations are
insufficient. These are nearly 600 cases per year. To reduce false positives
and minimize wasted resources on non-NEOs, we refine the posting criteria for
NEOCP based on a detailed analysis of all digest2 scores. We investigated 30
distinct digest2 parameter categories for candidates that were confirmed as
NEOs and non-NEOs. From this analysis, we derived a filtering mechanism based
on selected digest2 parameters that were able to exclude 20% of the non-NEOs
from the NEOCP while maintaining a minimal loss of true NEOs. We also
investigated the application of four machine-learning (ML) techniques, that is,
the gradient-boosting machine (GBM), the random forest (RF) classifier, the
stochastic gradient descent (SGD) classifier, and neural networks (NN) to
classify NEOCP candidates as NEOs or non-NEOs. Based on digest2 parameters as
input, our ML models achieved a precision of approximately 95% in
distinguishing between NEOs and non-NEOs. Results. Combining the digest2
parameter filter with an ML-based classification model, we demonstrate a
significant reduction in non-NEOs on the NEOCP that exceeds 80%, while limiting
the loss of NEO discovery tracklets to 5.5%. Importantly, we show that most
follow-up tracklets of initially misclassified NEOs are later correctly
identified as NEOs.

</details>

<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [980] [Machine learning the first stage in 2SLS: Practical guidance from bias decomposition and simulation](https://arxiv.org/abs/2505.13422)
*Connor Lennon,Edward Rubin,Glen Waddell*

Main category: econ.EM

TLDR: 本文探讨了将机器学习（ML）引入两阶段最小二乘法（2SLS）的影响，发现线性ML方法表现良好，而非线性方法可能导致显著偏差。


<details>
  <summary>Details</summary>
Motivation: 研究ML在2SLS中的应用效果，明确其优势和潜在问题，填补现有指导的空白。

Method: 通过分解偏差为三个组成部分，结合模拟实验，分析线性与非线性ML方法在2SLS中的表现。

Result: 线性ML方法（如后Lasso）效果良好，而非线性方法（如随机森林、神经网络）可能导致第二阶段估计的显著偏差，甚至超过内生OLS的偏差。

Conclusion: 在2SLS中使用ML需谨慎选择方法，线性方法更可靠，而非线性方法可能带来负面影响。

Abstract: Machine learning (ML) primarily evolved to solve "prediction problems." The
first stage of two-stage least squares (2SLS) is a prediction problem,
suggesting potential gains from ML first-stage assistance. However, little
guidance exists on when ML helps 2SLS$\unicode{x2014}$or when it hurts. We
investigate the implications of inserting ML into 2SLS, decomposing the bias
into three informative components. Mechanically, ML-in-2SLS procedures face
issues common to prediction and causal-inference settings$\unicode{x2014}$and
their interaction. Through simulation, we show linear ML methods (e.g.,
post-Lasso) work well, while nonlinear methods (e.g., random forests, neural
nets) generate substantial bias in second-stage
estimates$\unicode{x2014}$potentially exceeding the bias of endogenous OLS.

</details>

<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [981] [Programmable metasurfaces for future photonic artificial intelligence](https://arxiv.org/abs/2505.11659)
*Loubnan Abou-Hamdan,Emil Marinov,Peter Wiecha,Philipp del Hougne,Tianyu Wang,Patrice Genevet*

Main category: physics.optics

TLDR: 光子神经网络（PNNs）具有高并行性和低功耗等优势，但实现可扩展的光子AI解决方案仍具挑战性。可编程超表面技术可能成为实现可扩展光子AI加速器的关键硬件。


<details>
  <summary>Details</summary>
Motivation: 解决光子AI模型的扩展性问题，使其在商业上可行，并超越传统数字电子技术的优势。

Method: 利用可编程超表面技术，结合电子集成、3D堆叠和大规模制造，提升PNN的可扩展性和功能。

Result: 可编程超表面技术有望解决PNN当前面临的挑战，推动下一代光子AI技术的发展。

Conclusion: 可编程超表面技术是实现可扩展光子AI加速器的关键，可能在未来与数字电子技术竞争。

Abstract: Photonic neural networks (PNNs), which share the inherent benefits of
photonic systems, such as high parallelism and low power consumption, could
challenge traditional digital neural networks in terms of energy efficiency,
latency, and throughput. However, producing scalable photonic artificial
intelligence (AI) solutions remains challenging. To make photonic AI models
viable, the scalability problem needs to be solved. Large optical AI models
implemented on PNNs are only commercially feasible if the advantages of optical
computation outweigh the cost of their input-output overhead. In this
Perspective, we discuss how field-programmable metasurface technology may
become a key hardware ingredient in achieving scalable photonic AI accelerators
and how it can compete with current digital electronic technologies.
Programmability or reconfigurability is a pivotal component for PNN hardware,
enabling in situ training and accommodating non-stationary use cases that
require fine-tuning or transfer learning. Co-integration with electronics, 3D
stacking, and large-scale manufacturing of metasurfaces would significantly
improve PNN scalability and functionalities. Programmable metasurfaces could
address some of the current challenges that PNNs face and enable
next-generation photonic AI technology.

</details>

<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [982] [Efficient Implementation of Gaussian Process Regression Accelerated Saddle Point Searches with Application to Molecular Reactions](https://arxiv.org/abs/2505.12519)
*Rohit Goswami,Maxim Masterov,Satish Kamath,Alejandro Peña-Torres,Hannes Jónsson*

Main category: physics.chem-ph

TLDR: 论文提出了一种基于高斯过程回归（GPR）加速的最小模式跟随方法，用于高效定位高维能量表面上的鞍点，显著减少了电子结构计算的次数。


<details>
  <summary>Details</summary>
Motivation: 在高维能量表面上定位鞍点是研究热激活事件机制和速率的关键步骤，但直接结合电子结构计算时，收敛所需的能量和原子力评估次数较多。

Method: 采用GPR加速最小模式跟随方法，通过构建和更新代理能量表面，结合二聚体估计Hessian矩阵的最低本征模式。

Result: 在500个分子反应测试集上，GPR方法比传统二聚体方法减少了数量级的电子结构计算次数，且计算效率与复杂内坐标方法相当。

Conclusion: GPR方法在减少计算量的同时保持了高效性，适用于低计算水平下的鞍点搜索。

Abstract: The task of locating first order saddle points on high-dimensional surfaces
describing the variation of energy as a function of atomic coordinates is an
essential step for identifying the mechanism and estimating the rate of
thermally activated events within the harmonic approximation of transition
state theory. When combined directly with electronic structure calculations,
the number of energy and atomic force evaluations needed for convergence is a
primary issue. Here, we describe an efficient implementation of Gaussian
process regression (GPR) acceleration of the minimum mode following method
where a dimer is used to estimate the lowest eigenmode of the Hessian. A
surrogate energy surface is constructed and updated after each electronic
structure calculation. The method is applied to a test set of 500 molecular
reactions previously generated by Hermez and coworkers [J. Chem. Theory Comput.
18, 6974 (2022)]. An order of magnitude reduction in the number of electronic
structure calculations needed to reach the saddle point configurations is
obtained by using the GPR compared to the dimer method. Despite the wide range
in stiffness of the molecular degrees of freedom, the calculations are carried
out using Cartesian coordinates and are found to require similar number of
electronic structure calculations as an elaborate internal coordinate method
implemented in the Sella software package. The present implementation of the
GPR surrogate model in C++ is efficient enough for the wall time of the saddle
point searches to be reduced in 3 out of 4 cases even though the calculations
are carried out at a low Hartree-Fock level.

</details>

<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [983] [A Malliavin-Gamma calculus approach to Score Based Diffusion Generative models for random fields](https://arxiv.org/abs/2505.13189)
*Giacomo Greco*

Main category: math.PR

TLDR: 论文通过Gamma和Malliavin微积分方法，将基于分数的扩散生成模型（SGMs）推广到无限维抽象Hilbert空间。


<details>
  <summary>Details</summary>
Motivation: 将SGMs扩展到无限维空间，以解决高维或无限维数据的生成问题。

Method: 利用Dirichlet形式定义前向噪声过程，并通过抽象时间反转公式将分数函数表示为Malliavin导数和条件期望。

Result: 成功将SGMs推广到无限维Hilbert空间，并扩展了有限维熵收敛界。

Conclusion: 该方法为处理高维或无限维数据提供了理论框架，并在球形随机场中验证了其有效性。

Abstract: We adopt a Gamma and Malliavin Calculi point of view in order to generalize
Score-based diffusion Generative Models (SGMs) to an infinite-dimensional
abstract Hilbertian setting. Particularly, we define the forward noising
process using Dirichlet forms associated to the Cameron-Martin space of
Gaussian measures and Wiener chaoses; whereas by relying on an abstract
time-reversal formula, we show that the score function is a Malliavin
derivative and it corresponds to a conditional expectation. This allows us to
generalize SGMs to the infinite-dimensional setting. Moreover, we extend
existing finite-dimensional entropic convergence bounds to this Hilbertian
setting by highlighting the role played by the Cameron-Martin norm in the
Fisher information of the data distribution. Lastly, we specify our discussion
for spherical random fields, considering as source of noise a Whittle-Mat\'ern
random spherical field.

</details>

<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [984] [Decentralized Traffic Flow Optimization Through Intrinsic Motivation](https://arxiv.org/abs/2505.11520)
*Himaja Papala,Daniel Polani,Stas Tiomkin*

Main category: physics.soc-ph

TLDR: 论文提出一种基于内在动机（赋能原则）的自动驾驶汽车行为控制方法，用于改善交通流，减少拥堵。


<details>
  <summary>Details</summary>
Motivation: 交通拥堵是随着大城市快速发展而加剧的普遍问题，传统模型中的自发交通堵塞现象需要新的解决方案。

Method: 采用赋能原则作为内在动机，替代Nagel-Schreckenberg元胞自动机模型中部分车辆的默认行为，实现分散化控制。

Result: 新方法显著改善了整体交通流，缓解了拥堵，并减少了平均堵车时间。

Conclusion: 基于内在动机的分散化控制策略能有效改善交通流，且无需显式协调，具有鲁棒性。

Abstract: Traffic congestion has long been an ubiquitous problem that is exacerbating
with the rapid growth of megacities. In this proof-of-concept work we study
intrinsic motivation, implemented via the empowerment principle, to control
autonomous car behavior to improve traffic flow. In standard models of traffic
dynamics, self-organized traffic jams emerge spontaneously from the individual
behavior of cars, affecting traffic over long distances. Our novel car behavior
strategy improves traffic flow while still being decentralized and using only
locally available information without explicit coordination. Decentralization
is essential for various reasons, not least to be able to absorb robustly
substantial levels of uncertainty. Our scenario is based on the
well-established traffic dynamics model, the Nagel-Schreckenberg cellular
automaton. In a fraction of the cars in this model, we substitute the default
behavior by empowerment, our intrinsic motivation-based method. This proposed
model significantly improves overall traffic flow, mitigates congestion, and
reduces the average traffic jam time.

</details>

### [985] [Analysis and Resilience of the U.S. Flight Network](https://arxiv.org/abs/2505.11559)
*Sushrit Kafle,Shreejan Pandey*

Main category: physics.soc-ph

TLDR: 本文通过复杂网络理论分析美国飞行网络（USFN），发现其具有幂律分布和枢纽主导特性，效率高但易受攻击。


<details>
  <summary>Details</summary>
Motivation: 研究USFN的拓扑结构如何影响其效率和脆弱性。

Method: 分析网络的结构特性、度分布和社区结构，并与空网络对比。

Result: USFN具有高聚类系数和模块性，但对枢纽攻击脆弱，易引发级联故障。

Conclusion: 保护枢纽机场对增强网络鲁棒性和防止大规模故障至关重要。

Abstract: Air travel is one of the most widely used transportation services in the
United States. This paper analyzes the U.S. Flight Network (USFN) using complex
network theory by exploring how the network's topology contributes to its
efficiency and vulnerability. This is done by examining the structural
properties, degree distributions, and community structures in the network. USFN
was observed to follow power-law distribution and falls under the anomalous
regime, suggesting that the network is hub dominant. Compared to null networks,
USFN has a higher clustering coefficient and modularity. Various percolation
test revealed that USFN is vulnerable to targeted attacks and is susceptible to
complete cascading failure if one of the major hubs fails. The overall results
suggest that while the USFN is designed for efficiency, it is highly vulnerable
to disruptions. Protecting key hub airports is important to make the network
more robust and prevent large-scale failures.

</details>

<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [986] [Efficient Generation of Parameterised Quantum Circuits from Large Texts](https://arxiv.org/abs/2505.13208)
*Colin Krawchuk,Nikhil Khatri,Neil John Ortega,Dimitri Kartsaklis*

Main category: quant-ph

TLDR: 本文提出了一种将大规模文本转换为量子电路的高效方法，利用树状表示和对称单子范畴理论，实现了对复杂文本的忠实编码。


<details>
  <summary>Details</summary>
Motivation: 传统量子-经典混合模型依赖经典神经网络，缺乏直接性和可解释性。本文旨在通过DisCoCirc框架直接编码文本为量子电路，提升效率和可解释性。

Method: 采用树状表示的前群图方法，结合对称单子范畴理论，将文本转换为参数化量子电路（PQCs）。

Result: 实验证明，该方法能高效编码长达6410词的复杂文本，并保持句法和语篇关系。

Conclusion: 该方法为量子自然语言处理提供了新工具，已集成到开源量子NLP包lambeq Gen II中。

Abstract: Quantum approaches to natural language processing (NLP) are redefining how
linguistic information is represented and processed. While traditional hybrid
quantum-classical models rely heavily on classical neural networks, recent
advancements propose a novel framework, DisCoCirc, capable of directly encoding
entire documents as parameterised quantum circuits (PQCs), besides enjoying
some additional interpretability and compositionality benefits. Following these
ideas, this paper introduces an efficient methodology for converting
large-scale texts into quantum circuits using tree-like representations of
pregroup diagrams. Exploiting the compositional parallels between language and
quantum mechanics, grounded in symmetric monoidal categories, our approach
enables faithful and efficient encoding of syntactic and discourse
relationships in long and complex texts (up to 6410 words in our experiments)
to quantum circuits. The developed system is provided to the community as part
of the augmented open-source quantum NLP package lambeq Gen II.

</details>

<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [987] [OpenPros: A Large-Scale Dataset for Limited View Prostate Ultrasound Computed Tomography](https://arxiv.org/abs/2505.12261)
*Hanchen Wang,Yixuan Wu,Yinan Feng,Peng Jin,Shihang Feng,Yiming Mao,James Wiskin,Baris Turkbey,Peter A. Pinto,Bradford J. Wood,Songting Luo,Yinpeng Chen,Emad Boctor,Youzuo Lin*

Main category: physics.med-ph

TLDR: OpenPros是一个针对有限视角前列腺超声计算机断层扫描（USCT）的大规模基准数据集，旨在解决传统超声检测前列腺癌灵敏度低的问题。


<details>
  <summary>Details</summary>
Motivation: 前列腺癌是男性常见且致命的癌症，早期检测至关重要。传统超声方法灵敏度低，而USCT在临床应用中面临挑战。

Method: 通过解剖学准确的3D数字前列腺模型生成28万对2D声速（SOS）幻影和超声全波形数据，使用开源声波求解器进行模拟。

Result: 深度学习方法的推理效率和重建精度优于传统物理方法，但仍无法满足临床高分辨率图像需求。

Conclusion: OpenPros的发布旨在推动机器学习算法发展，以实现临床可用的高分辨率前列腺超声图像。

Abstract: Prostate cancer is one of the most common and lethal cancers among men,
making its early detection critically important. Although ultrasound imaging
offers greater accessibility and cost-effectiveness compared to MRI,
traditional transrectal ultrasound methods suffer from low sensitivity,
especially in detecting anteriorly located tumors. Ultrasound computed
tomography provides quantitative tissue characterization, but its clinical
implementation faces significant challenges, particularly under anatomically
constrained limited-angle acquisition conditions specific to prostate imaging.
To address these unmet needs, we introduce OpenPros, the first large-scale
benchmark dataset explicitly developed for limited-view prostate USCT. Our
dataset includes over 280,000 paired samples of realistic 2D speed-of-sound
(SOS) phantoms and corresponding ultrasound full-waveform data, generated from
anatomically accurate 3D digital prostate models derived from real clinical
MRI/CT scans and ex vivo ultrasound measurements, annotated by medical experts.
Simulations are conducted under clinically realistic configurations using
advanced finite-difference time-domain and Runge-Kutta acoustic wave solvers,
both provided as open-source components. Through comprehensive baseline
experiments, we demonstrate that state-of-the-art deep learning methods surpass
traditional physics-based approaches in both inference efficiency and
reconstruction accuracy. Nevertheless, current deep learning models still fall
short of delivering clinically acceptable high-resolution images with
sufficient accuracy. By publicly releasing OpenPros, we aim to encourage the
development of advanced machine learning algorithms capable of bridging this
performance gap and producing clinically usable, high-resolution, and highly
accurate prostate ultrasound images. The dataset is publicly accessible at
https://open-pros.github.io/.

</details>

<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [988] [Accelerating Natural Gradient Descent for PINNs with Randomized Numerical Linear Algebra](https://arxiv.org/abs/2505.11638)
*Ivan Bioli,Carlo Marcati,Giancarlo Sangalli*

Main category: math.NA

TLDR: 论文提出了一种基于随机Nyström预处理的矩阵无关自然梯度下降（NGD）方法，用于加速训练神经网络求解偏微分方程（PDE），解决了Gramian矩阵条件数差导致的计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 自然梯度下降（NGD）在训练神经网络求解PDE时具有潜力，但Gramian矩阵的高计算成本和条件数差限制了其实际应用。

Method: 扩展矩阵无关NGD方法，引入随机Nyström预处理技术，加速共轭梯度（CG）求解器的收敛。

Result: 在多个PDE问题上，该方法显著提升了性能，优于现有NGD方法。

Conclusion: 随机Nyström预处理有效解决了NGD的计算瓶颈，为神经网络求解PDE提供了更高效的优化工具。

Abstract: Natural Gradient Descent (NGD) has emerged as a promising optimization
algorithm for training neural network-based solvers for partial differential
equations (PDEs), such as Physics-Informed Neural Networks (PINNs). However,
its practical use is often limited by the high computational cost of solving
linear systems involving the Gramian matrix. While matrix-free NGD methods
based on the conjugate gradient (CG) method avoid explicit matrix inversion,
the ill-conditioning of the Gramian significantly slows the convergence of CG.
In this work, we extend matrix-free NGD to broader classes of problems than
previously considered and propose the use of Randomized Nystr\"om
preconditioning to accelerate convergence of the inner CG solver. The resulting
algorithm demonstrates substantial performance improvements over existing
NGD-based methods on a range of PDE problems discretized using neural networks.

</details>

### [989] [BOLT: Block-Orthonormal Lanczos for Trace estimation of matrix functions](https://arxiv.org/abs/2505.12289)
*Kingsley Yeon,Promit Ghosal,Mihai Anitescu*

Main category: math.NA

TLDR: 提出了一种名为BOLT的方法，用于高效估计矩阵迹，适用于受限存储和部分访问的场景，优于现有方法Hutch++。


<details>
  <summary>Details</summary>
Motivation: 在大规模应用中，矩阵过大无法完整存储或访问，现有方法如Hutch++依赖完整矩阵-向量乘积，难以适用。

Method: BOLT基于正交块探针和Lanczos迭代，结合随机探针与Krylov子空间方法，并引入Subblock SLQ变体以适应内存限制和部分访问。

Result: BOLT在精度上与Hutch++相当，在平坦谱区域表现更优，并支持低内存和部分访问场景。

Conclusion: BOLT为受限环境下的矩阵迹估计提供了高效解决方案，并扩展了KL散度和Wasserstein-2距离的计算能力。

Abstract: Efficient matrix trace estimation is essential for scalable computation of
log-determinants, matrix norms, and distributional divergences. In many
large-scale applications, the matrices involved are too large to store or
access in full, making even a single matrix-vector (mat-vec) product
infeasible. Instead, one often has access only to small subblocks of the matrix
or localized matrix-vector products on restricted index sets. Hutch++ achieves
optimal convergence rate but relies on randomized SVD and assumes full mat-vec
access, making it difficult to apply in these constrained settings. We propose
the Block-Orthonormal Stochastic Lanczos Quadrature (BOLT), which matches
Hutch++ accuracy with a simpler implementation based on orthonormal block
probes and Lanczos iterations. BOLT builds on the Stochastic Lanczos Quadrature
(SLQ) framework, which combines random probing with Krylov subspace methods to
efficiently approximate traces of matrix functions, and performs better than
Hutch++ in near flat-spectrum regimes. To address memory limitations and
partial access constraints, we introduce Subblock SLQ, a variant of BOLT that
operates only on small principal submatrices. As a result, this framework
yields a proxy KL divergence estimator and an efficient method for computing
the Wasserstein-2 distance between Gaussians - both compatible with low-memory
and partial-access regimes. We provide theoretical guarantees and demonstrate
strong empirical performance across a range of high-dimensional settings.

</details>

### [990] [Identifiability of Nonnegative Tucker Decompositions -- Part I: Theory](https://arxiv.org/abs/2505.12713)
*Subhayan Saha,Giovanni Barbarino,Nicolas Gillis*

Main category: math.NA

TLDR: 论文研究了非负Tucker分解（nTD）的可识别性，通过扩展非负矩阵分解（NMF）的结果，提出了在稀疏性条件下nTD的唯一性理论和方法。


<details>
  <summary>Details</summary>
Motivation: Tucker分解（TD）在数据科学中广泛应用，但其通常不具备可识别性。研究nTD的可识别性有助于从数据中恢复真实源。

Method: 通过扩展NMF的可识别性结果，利用稀疏性条件（如可分离性或充分分散条件），提出基于展开或切片的优化方法，最小化核心张量的体积以实现可识别性。

Result: 在稀疏性条件下，nTD具有唯一性，且核心张量无需非负，仅需某些切片或展开具有满列秩。

Conclusion: 论文为nTD的可识别性提供了理论支持和方法，扩展了张量分解在数据科学中的应用潜力。

Abstract: Tensor decompositions have become a central tool in data science, with
applications in areas such as data analysis, signal processing, and machine
learning. A key property of many tensor decompositions, such as the canonical
polyadic decomposition, is identifiability: the factors are unique, up to
trivial scaling and permutation ambiguities. This allows one to recover the
groundtruth sources that generated the data. The Tucker decomposition (TD) is a
central and widely used tensor decomposition model. However, it is in general
not identifiable. In this paper, we study the identifiability of the
nonnegative TD (nTD). By adapting and extending identifiability results of
nonnegative matrix factorization (NMF), we provide uniqueness results for nTD.
Our results require the nonnegative matrix factors to have some degree of
sparsity (namely, satisfy the separability condition, or the sufficiently
scattered condition), while the core tensor only needs to have some slices (or
linear combinations of them) or unfoldings with full column rank (but does not
need to be nonnegative). Under such conditions, we derive several procedures,
using either unfoldings or slices of the input tensor, to obtain identifiable
nTDs by minimizing the volume of unfoldings or slices of the core tensor.

</details>

<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [991] [Code Retrieval for MILP Instance Generation](https://arxiv.org/abs/2505.11526)
*Tianxing Yang,Huigen Ye,Hua Xu*

Main category: math.OC

TLDR: 该论文提出了一种将MILP实例生成任务重新定义为MILP代码生成任务的方法，并引入了MILP-EmbedSim相似性度量和MILP-Retrieval流程，以提高生成效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有MILP实例生成方法需要为每个问题类训练单独模型且计算成本高，限制了学习型求解器的性能提升。

Method: 将MILP实例生成任务重新定义为代码生成任务，提出MILP-EmbedSim度量相似性，并设计MILP-Retrieval流程从库中检索生成代码。

Result: MILP-Retrieval在代码生成和实例生成任务中优于基线方法，为学习型求解器提供了新思路。

Conclusion: 该方法为MILP实例生成提供了高效、灵活且可解释的解决方案，并为学习型求解器开辟了新可能性。

Abstract: Mixed-Integer Linear Programming (MILP) is widely used in fields such as
scheduling, logistics, and planning. Enhancing the performance of MILP solvers,
particularly learning-based solvers, requires substantial amounts of
high-quality data. However, existing methods for MILP instance generation
typically necessitate training a separate model for each problem class and are
computationally intensive when generating new instances. To address these
limitations, we reformulate the MILP Instance Generation task as MILP Code
Generation task, enabling efficient, flexible, and interpretable instance
generation through code. Since MILP instances generated from code can vary
significantly in scale, we introduce MILP-EmbedSim, a new similarity metric
that accurately measures the similarity between instances of varying sizes
within the same problem class. Leveraging this metric, we propose
MILP-Retrieval, a pipeline that retrieves generation code from library to
produce MILP instances highly similar to target instance. MILP-Retrieval
outperforms baselines in both MILP Code Generation and Instance Generation
tasks, provides a novel perspective on MILP instance generation and opens new
possibilities for learning-based solvers.

</details>

### [992] [Deep Unrolled Meta-Learning for Multi-Coil and Multi-Modality MRI with Adaptive Optimization](https://arxiv.org/abs/2505.11518)
*Merham Fouladvand,Peuroly Batra*

Main category: math.OC

TLDR: 提出了一种统一的深度元学习框架，用于加速MRI，同时解决多线圈重建和跨模态合成问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理欠采样数据和缺失模态时存在局限性，因此需要一种更高效的方法。

Method: 将可证明收敛的优化算法展开为结构化神经网络架构，结合元学习以快速适应新任务。

Result: 在开放数据集上表现优于传统监督学习，PSNR和SSIM显著提升。

Conclusion: 结合展开优化、任务感知元学习和模态融合，为临床MRI重建提供了可扩展且通用的解决方案。

Abstract: We propose a unified deep meta-learning framework for accelerated magnetic
resonance imaging (MRI) that jointly addresses multi-coil reconstruction and
cross-modality synthesis. Motivated by the limitations of conventional methods
in handling undersampled data and missing modalities, our approach unrolls a
provably convergent optimization algorithm into a structured neural network
architecture. Each phase of the network mimics a step of an adaptive
forward-backward scheme with extrapolation, enabling the model to incorporate
both data fidelity and nonconvex regularization in a principled manner. To
enhance generalization across different acquisition settings, we integrate
meta-learning, which enables the model to rapidly adapt to unseen sampling
patterns and modality combinations using task-specific meta-knowledge. The
proposed method is evaluated on the open source datasets, showing significant
improvements in PSNR and SSIM over conventional supervised learning, especially
under aggressive undersampling and domain shifts. Our results demonstrate the
synergy of unrolled optimization, task-aware meta-learning, and modality
fusion, offering a scalable and generalizable solution for real-world clinical
MRI reconstruction.

</details>

### [993] [Efficient Optimization with Orthogonality Constraint: a Randomized Riemannian Submanifold Method](https://arxiv.org/abs/2505.12378)
*Andi Han,Pierre-Louis Poirion,Akiko Takeda*

Main category: math.OC

TLDR: 提出了一种基于随机子流形的优化方法，以降低大规模正交约束问题的计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决Riemannian优化在大规模问题中因回缩操作计算成本高而受限的问题。

Method: 通过限制每次更新在随机子流形上，减少每轮迭代复杂度，并提出了两种采样策略。

Result: 理论分析了方法的收敛性，并在多种问题上验证了其有效性。

Conclusion: 该方法显著提升了Riemannian优化的可扩展性，适用于大规模问题。

Abstract: Optimization with orthogonality constraints frequently arises in various
fields such as machine learning. Riemannian optimization offers a powerful
framework for solving these problems by equipping the constraint set with a
Riemannian manifold structure and performing optimization intrinsically on the
manifold. This approach typically involves computing a search direction in the
tangent space and updating variables via a retraction operation. However, as
the size of the variables increases, the computational cost of the retraction
can become prohibitively high, limiting the applicability of Riemannian
optimization to large-scale problems. To address this challenge and enhance
scalability, we propose a novel approach that restricts each update on a random
submanifold, thereby significantly reducing the per-iteration complexity. We
introduce two sampling strategies for selecting the random submanifolds and
theoretically analyze the convergence of the proposed methods. We provide
convergence results for general nonconvex functions and functions that satisfy
Riemannian Polyak-Lojasiewicz condition as well as for stochastic optimization
settings. Additionally, we demonstrate how our approach can be generalized to
quotient manifolds derived from the orthogonal manifold. Extensive experiments
verify the benefits of the proposed method, across a wide variety of problems.

</details>

### [994] [Hamiltonian Descent Algorithms for Optimization: Accelerated Rates via Randomized Integration Time](https://arxiv.org/abs/2505.12553)
*Qiang Fu,Andre Wibisono*

Main category: math.OC

TLDR: HF-opt模拟哈密顿动力学进行优化，随机化积分时间后（RHF）在连续时间中实现加速收敛。离散时间实现的RHGD算法与Nesterov加速梯度下降（AGD）性能相当，并在某些情况下更优。


<details>
  <summary>Details</summary>
Motivation: 研究哈密顿流优化（HF-opt）及其随机化版本（RHF）的收敛性能，探索其在优化问题中的潜力。

Method: HF-opt模拟哈密顿动力学并重置速度；RHF随机化积分时间；离散实现为RHGD算法。

Result: RHF在连续时间中实现加速收敛；RHGD与AGD性能相当，某些情况下更优。

Conclusion: RHGD是一种高效的优化算法，性能与经典加速方法相当甚至更优。

Abstract: We study the Hamiltonian flow for optimization (HF-opt), which simulates the
Hamiltonian dynamics for some integration time and resets the velocity to $0$
to decrease the objective function; this is the optimization analogue of the
Hamiltonian Monte Carlo algorithm for sampling. For short integration time,
HF-opt has the same convergence rates as gradient descent for minimizing
strongly and weakly convex functions. We show that by randomizing the
integration time in HF-opt, the resulting randomized Hamiltonian flow (RHF)
achieves accelerated convergence rates in continuous time, similar to the rates
for the accelerated gradient flow. We study a discrete-time implementation of
RHF as the randomized Hamiltonian gradient descent (RHGD) algorithm. We prove
that RHGD achieves the same accelerated convergence rates as Nesterov's
accelerated gradient descent (AGD) for minimizing smooth strongly and weakly
convex functions. We provide numerical experiments to demonstrate that RHGD is
competitive with classical accelerated methods such as AGD across all settings
and outperforms them in certain regimes.

</details>

### [995] [Accelerated Markov Chain Monte Carlo Algorithms on Discrete States](https://arxiv.org/abs/2505.12599)
*Bohan Zhou,Shu Liu,Xinzhe Zuo,Wuchen Li*

Main category: math.OC

TLDR: 提出了一种基于Nesterov加速梯度方法的离散状态采样算法，扩展了经典的Metropolis-Hastings算法，通过动量加速框架和粒子系统近似，提高了采样效率。


<details>
  <summary>Details</summary>
Motivation: 经典Metropolis-Hastings算法的离散状态概率分布演化可以解释为KL散度的梯度下降方向，但缺乏加速机制。

Method: 利用离散Wasserstein-2度量和动量加速框架，设计阻尼哈密顿流和粒子系统近似加速采样动力学。

Result: 数值实验表明，算法在估计离散得分函数和保持正概率方面具有优势，适用于高斯混合和超立方分布等场景。

Conclusion: 提出的离散状态采样算法通过加速框架和粒子系统，显著提升了采样效率，且无需归一化常数。

Abstract: We propose a class of discrete state sampling algorithms based on Nesterov's
accelerated gradient method, which extends the classical Metropolis-Hastings
(MH) algorithm. The evolution of the discrete states probability distribution
governed by MH can be interpreted as a gradient descent direction of the
Kullback--Leibler (KL) divergence, via a mobility function and a score
function. Specifically, this gradient is defined on a probability simplex
equipped with a discrete Wasserstein-2 metric with a mobility function. This
motivates us to study a momentum-based acceleration framework using damped
Hamiltonian flows on the simplex set, whose stationary distribution matches the
discrete target distribution. Furthermore, we design an interacting particle
system to approximate the proposed accelerated sampling dynamics. The extension
of the algorithm with a general choice of potentials and mobilities is also
discussed. In particular, we choose the accelerated gradient flow of the
relative Fisher information, demonstrating the advantages of the algorithm in
estimating discrete score functions without requiring the normalizing constant
and keeping positive probabilities. Numerical examples, including sampling on a
Gaussian mixture supported on lattices or a distribution on a hypercube,
demonstrate the effectiveness of the proposed discrete-state sampling
algorithm.

</details>

<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [996] [Vague Knowledge: Evidence from Analyst Reports](https://arxiv.org/abs/2505.12269)
*Kerry Xiao,Amy Zang*

Main category: econ.GN

TLDR: 论文探讨了语言在传递模糊信息中的作用，发现分析师报告中的文本语气对预测误差和后续数值预测修订具有预测能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，人们对未来收益的模糊认知难以量化，而语言在传递这种模糊信息中扮演了重要但鲜为人知的角色。

Method: 通过实证分析，研究分析师报告中的文本语气与数值预测的关系，特别是在模糊性、不确定性和分析师忙碌程度较高时。

Result: 文本语气对预测误差和后续修订具有预测能力，且这种关系在模糊性、不确定性和忙碌程度较高时更强。

Conclusion: 研究表明，部分有用信息仅通过语言模糊传递，语言在主观预期中具有重要作用。

Abstract: People in the real world often possess vague knowledge of future payoffs, for
which quantification is not feasible or desirable. We argue that language, with
differing ability to convey vague information, plays an important but less
known-role in subjective expectations. Empirically, we find that in their
reports, analysts include useful information in linguistic expressions but not
numerical forecasts. Specifically, the textual tone of analyst reports has
predictive power for forecast errors and subsequent revisions in numerical
forecasts, and this relation becomes stronger when analyst's language is
vaguer, when uncertainty is higher, and when analysts are busier. Overall, our
theory and evidence suggest that some useful information is vaguely known and
only communicated through language.

</details>

<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [997] [Improving Medium Range Severe Weather Prediction through Transformer Post-processing of AI Weather Forecasts](https://arxiv.org/abs/2505.11750)
*Zhanxiang Hua,Ryan Sobash,David John Gagne II,Yingkai Sha,Alexandra Anderson-Frey*

Main category: physics.ao-ph

TLDR: 该论文提出了一种基于解码器-仅Transformer网络的新方法，用于后处理AI天气预报（如Pangu-Weather模型），以提高中短期（1-8天）恶劣天气预测能力。


<details>
  <summary>Details</summary>
Motivation: 改进中短期恶劣天气预测能力对减轻社会影响至关重要。传统方法使用密集神经网络处理离散预报样本，而新方法将预报时间视为序列标记，以捕捉复杂的时间关系。

Method: 采用解码器-仅Transformer网络后处理Pangu-Weather模型的预报数据，并与传统密集神经网络方法及GFS模型进行对比。

Result: 基于Transformer的后处理方法显著优于密集神经网络，且Pangu-Weather模型在高分辨率分析初始化下表现优于GFS模型。

Conclusion: 该方法提高了预测准确性和可靠性，并通过特征归因分析提供可解释性，推动了中短期恶劣天气预测技术的发展。

Abstract: Improving the skill of medium-range (1-8 day) severe weather prediction is
crucial for mitigating societal impacts. This study introduces a novel approach
leveraging decoder-only transformer networks to post-process AI-based weather
forecasts, specifically from the Pangu-Weather model, for improved severe
weather guidance. Unlike traditional post-processing methods that use a dense
neural network to predict the probability of severe weather using discrete
forecast samples, our method treats forecast lead times as sequential
``tokens'', enabling the transformer to learn complex temporal relationships
within the evolving atmospheric state. We compare this approach against
post-processing of the Global Forecast System (GFS) using both a traditional
dense neural network and our transformer, as well as configurations that
exclude convective parameters to fairly evaluate the impact of using the
Pangu-Weather AI model. Results demonstrate that the transformer-based
post-processing significantly enhances forecast skill compared to dense neural
networks. Furthermore, AI-driven forecasts, particularly Pangu-Weather
initialized from high resolution analysis, exhibit superior performance to GFS
in the medium-range, even without explicit convective parameters. Our approach
offers improved accuracy, and reliability, which also provides interpretability
through feature attribution analysis, advancing medium-range severe weather
prediction capabilities.

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [998] [ASR-FAIRBENCH: Measuring and Benchmarking Equity Across Speech Recognition Systems](https://arxiv.org/abs/2505.11572)
*Anand Rai,Satyam Rahangdale,Utkarsh Anand,Animesh Mukherjee*

Main category: cs.SD

TLDR: ASR-FAIRBENCH leaderboard评估ASR模型的准确性和公平性，结合公平分数与传统指标，揭示性能差异。


<details>
  <summary>Details</summary>
Motivation: 解决ASR系统在不同人口群体中性能差异的问题。

Method: 利用Meta的Fair-Speech数据集，采用混合效应泊松回归模型计算公平分数，结合WER生成FAAS。

Result: 发现SOTA ASR模型在不同群体中存在显著性能差异。

Conclusion: 提供了一种推动更包容ASR技术发展的评估框架。

Abstract: Automatic Speech Recognition (ASR) systems have become ubiquitous in everyday
applications, yet significant disparities in performance across diverse
demographic groups persist. In this work, we introduce the ASR-FAIRBENCH
leaderboard which is designed to assess both the accuracy and equity of ASR
models in real-time. Leveraging the Meta's Fair-Speech dataset, which captures
diverse demographic characteristics, we employ a mixed-effects Poisson
regression model to derive an overall fairness score. This score is integrated
with traditional metrics like Word Error Rate (WER) to compute the Fairness
Adjusted ASR Score (FAAS), providing a comprehensive evaluation framework. Our
approach reveals significant performance disparities in SOTA ASR models across
demographic groups and offers a benchmark to drive the development of more
inclusive ASR technologies.

</details>

### [999] [SepPrune: Structured Pruning for Efficient Deep Speech Separation](https://arxiv.org/abs/2505.12079)
*Yuqi Li,Kai Li,Xin Yin,Zhifei Yang,Junhao Dong,Zeyu Dong,Chuanguang Yang,Yingli Tian,Yao Lu*

Main category: cs.SD

TLDR: SepPrune是一个结构化剪枝框架，用于压缩深度语音分离模型并降低计算成本，通过梯度驱动的通道选择和微调恢复性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注语音分离质量而忽略计算效率，SepPrune旨在解决这一问题，适用于实时低延迟应用。

Method: 分析模型计算结构，识别高计算负担层；引入可微分掩码策略进行梯度驱动通道选择；剪枝冗余通道并微调剩余参数。

Result: SepPrune在语音分离模型中表现优异，仅需一epoch微调即可恢复85%性能，收敛速度比从头训练快36倍。

Conclusion: SepPrune为语音分离模型的高效剪枝提供了有效解决方案，显著提升计算效率。

Abstract: Although deep learning has substantially advanced speech separation in recent
years, most existing studies continue to prioritize separation quality while
overlooking computational efficiency, an essential factor for low-latency
speech processing in real-time applications. In this paper, we propose
SepPrune, the first structured pruning framework specifically designed to
compress deep speech separation models and reduce their computational cost.
SepPrune begins by analyzing the computational structure of a given model to
identify layers with the highest computational burden. It then introduces a
differentiable masking strategy to enable gradient-driven channel selection.
Based on the learned masks, SepPrune prunes redundant channels and fine-tunes
the remaining parameters to recover performance. Extensive experiments
demonstrate that this learnable pruning paradigm yields substantial advantages
for channel pruning in speech separation models, outperforming existing
methods. Notably, a model pruned with SepPrune can recover 85% of the
performance of a pre-trained model (trained over hundreds of epochs) with only
one epoch of fine-tuning, and achieves convergence 36$\times$ faster than
training from scratch. Code is available at
https://github.com/itsnotacie/SepPrune.

</details>

### [1000] [VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning](https://arxiv.org/abs/2505.12332)
*Qianyue Hu,Junyan Wu,Wei Lu,Xiangyang Luo*

Main category: cs.SD

TLDR: VoiceCloak是一种针对扩散模型（DMs）的多维主动防御框架，通过对抗性扰动破坏未经授权的语音克隆（VC），同时混淆说话人身份并降低感知质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在语音克隆中表现出色，但也增加了恶意滥用的风险。现有防御方法不适用于扩散模型，因此需要一种新的防御框架。

Method: VoiceCloak通过分析扩散模型的漏洞，引入对抗性扰动，扭曲说话人身份表示并破坏关键条件引导过程，同时通过噪声引导的语义破坏降低输出质量。

Result: 实验表明，VoiceCloak在防御未经授权的语音克隆方面表现出色。

Conclusion: VoiceCloak为扩散模型提供了一种有效的主动防御解决方案，能够显著降低语音克隆的滥用风险。

Abstract: Diffusion Models (DMs) have achieved remarkable success in realistic voice
cloning (VC), while they also increase the risk of malicious misuse. Existing
proactive defenses designed for traditional VC models aim to disrupt the
forgery process, but they have been proven incompatible with DMs due to the
intricate generative mechanisms of diffusion. To bridge this gap, we introduce
VoiceCloak, a multi-dimensional proactive defense framework with the goal of
obfuscating speaker identity and degrading perceptual quality in potential
unauthorized VC. To achieve these goals, we conduct a focused analysis to
identify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt
the cloning process by introducing adversarial perturbations into the reference
audio. Specifically, to obfuscate speaker identity, VoiceCloak first targets
speaker identity by distorting representation learning embeddings to maximize
identity variation, which is guided by auditory perception principles.
Additionally, VoiceCloak disrupts crucial conditional guidance processes,
particularly attention context, thereby preventing the alignment of vocal
characteristics that are essential for achieving convincing cloning. Then, to
address the second objective, VoiceCloak introduces score magnitude
amplification to actively steer the reverse trajectory away from the generation
of high-quality speech. Noise-guided semantic corruption is further employed to
disrupt structural speech semantics captured by DMs, degrading output quality.
Extensive experiments highlight VoiceCloak's outstanding defense success rate
against unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak
are available at https://voice-cloak.github.io/VoiceCloak/.

</details>

### [1001] [MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix](https://arxiv.org/abs/2505.13032)
*Ziyang Ma,Yinghao Ma,Yanqiao Zhu,Chen Yang,Yi-Wen Chao,Ruiyang Xu,Wenxi Chen,Yuanzhe Chen,Zhuo Chen,Jian Cong,Kai Li,Keliang Li,Siyou Li,Xinfeng Li,Xiquan Li,Zheng Lian,Yuzhe Liang,Minghao Liu,Zhikang Niu,Tianrui Wang,Yuping Wang,Yuxuan Wang,Yihao Wu,Guanrou Yang,Jianwei Yu,Ruibin Yuan,Zhisheng Zheng,Ziya Zhou,Haina Zhu,Wei Xue,Emmanouil Benetos,Kai Yu,Eng-Siong Chng,Xie Chen*

Main category: cs.SD

TLDR: MMAR是一个新的基准测试，用于评估音频-语言模型（ALMs）在多学科任务中的深度推理能力，包含1000个高质量音频-问题-答案三元组，覆盖广泛的现实音频场景和复杂的推理层次。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试局限于特定音频领域（如声音、音乐或语音），缺乏对多模态音频场景和深度推理能力的评估，因此需要更全面的基准来推动研究。

Method: MMAR通过从互联网视频中收集并迭代修正1000个音频-问题-答案三元组，覆盖信号、感知、语义和文化四个推理层次，并标注Chain-of-Thought（CoT）推理路径。

Result: 测试表明，现有模型（如LALMs、LARMs、LLMs等）在MMAR上表现不佳，揭示了其在理解和推理能力上的局限性。

Conclusion: MMAR旨在推动音频推理领域的研究，填补当前模型的不足，并为未来研究提供挑战性基准。

Abstract: We introduce MMAR, a new benchmark designed to evaluate the deep reasoning
capabilities of Audio-Language Models (ALMs) across massive multi-disciplinary
tasks. MMAR comprises 1,000 meticulously curated audio-question-answer
triplets, collected from real-world internet videos and refined through
iterative error corrections and quality checks to ensure high quality. Unlike
existing benchmarks that are limited to specific domains of sound, music, or
speech, MMAR extends them to a broad spectrum of real-world audio scenarios,
including mixed-modality combinations of sound, music, and speech. Each
question in MMAR is hierarchically categorized across four reasoning layers:
Signal, Perception, Semantic, and Cultural, with additional sub-categories
within each layer to reflect task diversity and complexity. To further foster
research in this area, we annotate every question with a Chain-of-Thought (CoT)
rationale to promote future advancements in audio reasoning. Each item in the
benchmark demands multi-step deep reasoning beyond surface-level understanding.
Moreover, a part of the questions requires graduate-level perceptual and
domain-specific knowledge, elevating the benchmark's difficulty and depth. We
evaluate MMAR using a broad set of models, including Large Audio-Language
Models (LALMs), Large Audio Reasoning Models (LARMs), Omni Language Models
(OLMs), Large Language Models (LLMs), and Large Reasoning Models (LRMs), with
audio caption inputs. The performance of these models on MMAR highlights the
benchmark's challenging nature, and our analysis further reveals critical
limitations of understanding and reasoning capabilities among current models.
We hope MMAR will serve as a catalyst for future advances in this important but
little-explored area.

</details>

### [1002] [Unified Cross-modal Translation of Score Images, Symbolic Music, and Performance Audio](https://arxiv.org/abs/2505.12863)
*Jongmin Jung,Dongmin Kim,Sihun Lee,Seola Cho,Hyungjoon Soh,Irmak Bukey,Chris Donahue,Dasaem Jeong*

Main category: cs.SD

TLDR: 论文提出了一种统一的多模态音乐翻译方法，通过大规模数据集和统一的分词框架，实现了跨模态任务的性能提升。


<details>
  <summary>Details</summary>
Motivation: 过去的研究通常针对单一模态翻译任务训练专用模型，缺乏统一性。本文旨在通过统一模型和多任务训练解决这一问题。

Method: 提出了一种新的数据集（1300小时配对音频-乐谱图像数据）和统一的分词框架，将不同模态离散化为令牌序列，使用单一编码器-解码器Transformer处理多任务。

Result: 统一模型在多个任务中表现优于单任务基线，如光学音乐识别的符号错误率从24.58%降至13.67%，并首次实现了乐谱图像条件音频生成。

Conclusion: 该方法为跨模态音乐翻译和生成提供了突破性进展，证明了统一模型的潜力。

Abstract: Music exists in various modalities, such as score images, symbolic scores,
MIDI, and audio. Translations between each modality are established as core
tasks of music information retrieval, such as automatic music transcription
(audio-to-MIDI) and optical music recognition (score image to symbolic score).
However, most past work on multimodal translation trains specialized models on
individual translation tasks. In this paper, we propose a unified approach,
where we train a general-purpose model on many translation tasks
simultaneously. Two key factors make this unified approach viable: a new
large-scale dataset and the tokenization of each modality. Firstly, we propose
a new dataset that consists of more than 1,300 hours of paired audio-score
image data collected from YouTube videos, which is an order of magnitude larger
than any existing music modal translation datasets. Secondly, our unified
tokenization framework discretizes score images, audio, MIDI, and MusicXML into
a sequence of tokens, enabling a single encoder-decoder Transformer to tackle
multiple cross-modal translation as one coherent sequence-to-sequence task.
Experimental results confirm that our unified multitask model improves upon
single-task baselines in several key areas, notably reducing the symbol error
rate for optical music recognition from 24.58% to a state-of-the-art 13.67%,
while similarly substantial improvements are observed across the other
translation tasks. Notably, our approach achieves the first successful
score-image-conditioned audio generation, marking a significant breakthrough in
cross-modal music generation.

</details>

### [1003] [Text2midi-InferAlign: Improving Symbolic Music Generation with Inference-Time Alignment](https://arxiv.org/abs/2505.12669)
*Abhinaba Roy,Geeta Puri,Dorien Herremans*

Main category: cs.SD

TLDR: Text2midi-InferAlign通过文本-音频对齐和音乐结构对齐奖励改进符号音乐生成，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 提升符号音乐生成与输入文本的一致性。

Method: 引入文本-音频一致性评分和和谐一致性评分，优化生成过程。

Result: 在Text2midi模型上显著提升生成音乐的客观和主观评价指标。

Conclusion: 该方法有效提升生成音乐的质量和一致性，适用于现有自回归模型。

Abstract: We present Text2midi-InferAlign, a novel technique for improving symbolic
music generation at inference time. Our method leverages text-to-audio
alignment and music structural alignment rewards during inference to encourage
the generated music to be consistent with the input caption. Specifically, we
introduce two objectives scores: a text-audio consistency score that measures
rhythmic alignment between the generated music and the original text caption,
and a harmonic consistency score that penalizes generated music containing
notes inconsistent with the key. By optimizing these alignment-based objectives
during the generation process, our model produces symbolic music that is more
closely tied to the input captions, thereby improving the overall quality and
coherence of the generated compositions. Our approach can extend any existing
autoregressive model without requiring further training or fine-tuning. We
evaluate our work on top of Text2midi - an existing text-to-midi generation
model, demonstrating significant improvements in both objective and subjective
evaluation metrics.

</details>

### [1004] [SounDiT: Geo-Contextual Soundscape-to-Landscape Generation](https://arxiv.org/abs/2505.12734)
*Junbo Wang,Haofeng Tan,Bowen Liao,Albert Jiang,Teng Fei,Qixing Huang,Zhengzhong Tu,Shan Ye,Yuhao Kang*

Main category: cs.SD

TLDR: 论文提出了一种新颖的Geo-Contextual Soundscape-to-Landscape (GeoS2L)生成方法，通过结合地理知识生成地理一致的景观图像。


<details>
  <summary>Details</summary>
Motivation: 现有音频到图像生成方法忽略地理和环境背景，导致生成图像与现实环境不符。

Method: 提出基于Diffusion Transformer的SounDiT模型，并构建了两个大规模地理上下文多模态数据集。

Result: SounDiT在视觉保真度和地理一致性上优于现有基线。

Conclusion: 该工作为GeoS2L生成奠定了基础，并强调了地理知识在多模态生成模型中的重要性。

Abstract: We present a novel and practically significant problem-Geo-Contextual
Soundscape-to-Landscape (GeoS2L) generation-which aims to synthesize
geographically realistic landscape images from environmental soundscapes. Prior
audio-to-image generation methods typically rely on general-purpose datasets
and overlook geographic and environmental contexts, resulting in unrealistic
images that are misaligned with real-world environmental settings. To address
this limitation, we introduce a novel geo-contextual computational framework
that explicitly integrates geographic knowledge into multimodal generative
modeling. We construct two large-scale geo-contextual multimodal datasets,
SoundingSVI and SonicUrban, pairing diverse soundscapes with real-world
landscape images. We propose SounDiT, a novel Diffusion Transformer (DiT)-based
model that incorporates geo-contextual scene conditioning to synthesize
geographically coherent landscape images. Furthermore, we propose a
practically-informed geo-contextual evaluation framework, the Place Similarity
Score (PSS), across element-, scene-, and human perception-levels to measure
consistency between input soundscapes and generated landscape images. Extensive
experiments demonstrate that SounDiT outperforms existing baselines in both
visual fidelity and geographic settings. Our work not only establishes
foundational benchmarks for GeoS2L generation but also highlights the
importance of incorporating geographic domain knowledge in advancing multimodal
generative models, opening new directions at the intersection of generative AI,
geography, urban planning, and environmental sciences.

</details>

### [1005] [OZSpeech: One-step Zero-shot Speech Synthesis with Learned-Prior-Conditioned Flow Matching](https://arxiv.org/abs/2505.12800)
*Hieu-Nghia Huynh-Nguyen,Ngoc Son Nguyen,Huynh Nguyen Dang,Thieu Vo,Truong-Son Hy,Van Nguyen*

Main category: cs.SD

TLDR: OZSpeech是一种基于最优传输条件流匹配和一步采样的新型TTS方法，通过解耦语音属性并减少采样步骤，显著提升了语音合成的准确性和自然度。


<details>
  <summary>Details</summary>
Motivation: 传统TTS方法在语音表示和训练约束上存在局限性，如忽略语音属性和高计算成本。

Method: 采用最优传输条件流匹配框架，结合一步采样和学习先验条件，对解耦的语音属性进行建模。

Result: 实验表明，OZSpeech在内容准确性、自然度、韵律生成和说话人风格保留方面优于现有方法。

Conclusion: OZSpeech通过创新的建模方法，为TTS系统提供了更高效和精确的解决方案。

Abstract: Text-to-speech (TTS) systems have seen significant advancements in recent
years, driven by improvements in deep learning and neural network
architectures. Viewing the output speech as a data distribution, previous
approaches often employ traditional speech representations, such as waveforms
or spectrograms, within the Flow Matching framework. However, these methods
have limitations, including overlooking various speech attributes and incurring
high computational costs due to additional constraints introduced during
training. To address these challenges, we introduce OZSpeech, the first TTS
method to explore optimal transport conditional flow matching with one-step
sampling and a learned prior as the condition, effectively disregarding
preceding states and reducing the number of sampling steps. Our approach
operates on disentangled, factorized components of speech in token format,
enabling accurate modeling of each speech attribute, which enhances the TTS
system's ability to precisely clone the prompt speech. Experimental results
show that our method achieves promising performance over existing methods in
content accuracy, naturalness, prosody generation, and speaker style
preservation. Audio samples are available at our demo page
https://ozspeech.github.io/OZSpeech_Web/.

</details>

### [1006] [The Computation of Generalized Embeddings for Underwater Acoustic Target Recognition using Contrastive Learning](https://arxiv.org/abs/2505.12904)
*Hilde I. Hummel,Arwin Gansekoele,Sandjai Bhulai,Rob van der Mei*

Main category: cs.SD

TLDR: 论文提出了一种无监督对比学习方法，用于水下噪声分类，解决了高质量标记数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 海洋环境中声音污染日益严重，需要监测水下噪声以识别污染源，但现有监督学习方法依赖大量高质量标记数据，而这些数据不易获取。

Method: 采用无监督对比学习方法，使用Conformer-based编码器和Variance-Invariance-Covariance Regularization损失函数，利用低质量未标记数据进行训练，并迁移到标记数据。

Result: 在船舶类型和海洋哺乳动物声音分类任务中，该方法生成了鲁棒且通用的嵌入，展示了无监督方法的潜力。

Conclusion: 无监督方法在水下声学分析任务中具有广泛应用前景。

Abstract: The increasing level of sound pollution in marine environments poses an
increased threat to ocean health, making it crucial to monitor underwater
noise. By monitoring this noise, the sources responsible for this pollution can
be mapped. Monitoring is performed by passively listening to these sounds. This
generates a large amount of data records, capturing a mix of sound sources such
as ship activities and marine mammal vocalizations. Although machine learning
offers a promising solution for automatic sound classification, current
state-of-the-art methods implement supervised learning. This requires a large
amount of high-quality labeled data that is not publicly available. In
contrast, a massive amount of lower-quality unlabeled data is publicly
available, offering the opportunity to explore unsupervised learning
techniques. This research explores this possibility by implementing an
unsupervised Contrastive Learning approach. Here, a Conformer-based encoder is
optimized by the so-called Variance-Invariance-Covariance Regularization loss
function on these lower-quality unlabeled data and the translation to the
labeled data is made. Through classification tasks involving recognizing ship
types and marine mammal vocalizations, our method demonstrates to produce
robust and generalized embeddings. This shows to potential of unsupervised
methods for various automatic underwater acoustic analysis tasks.

</details>

### [1007] [MultiActor-Audiobook: Zero-Shot Audiobook Generation with Faces and Voices of Multiple Speakers](https://arxiv.org/abs/2505.13082)
*Kyeongman Park,Seongho Joo,Kyomin Jung*

Main category: cs.SD

TLDR: MultiActor-Audiobook提出了一种零样本方法，通过MSP和LSI两个新过程，自动生成具有一致性和表达力的有声书，解决了现有系统需要手动配置、单调语调或高成本训练的问题。


<details>
  <summary>Details</summary>
Motivation: 现有有声书系统存在手动配置繁琐、语调单调或依赖高成本训练的问题，MultiActor-Audiobook旨在解决这些问题。

Method: 引入MSP（多模态说话人角色生成）和LSI（基于LLM的脚本指令生成）两个新过程，无需额外训练即可生成表达力强的有声书。

Result: 通过人类和MLLM评估，系统在商业产品中表现竞争力，并通过消融实验验证了MSP和LSI的有效性。

Conclusion: MultiActor-Audiobook提供了一种高效、低成本的有声书生成方案，具有实际应用潜力。

Abstract: We introduce MultiActor-Audiobook, a zero-shot approach for generating
audiobooks that automatically produces consistent, expressive, and
speaker-appropriate prosody, including intonation and emotion. Previous
audiobook systems have several limitations: they require users to manually
configure the speaker's prosody, read each sentence with a monotonic tone
compared to voice actors, or rely on costly training. However, our
MultiActor-Audiobook addresses these issues by introducing two novel processes:
(1) MSP (**Multimodal Speaker Persona Generation**) and (2) LSI (**LLM-based
Script Instruction Generation**). With these two processes,
MultiActor-Audiobook can generate more emotionally expressive audiobooks with a
consistent speaker prosody without additional training. We compare our system
with commercial products, through human and MLLM evaluations, achieving
competitive results. Furthermore, we demonstrate the effectiveness of MSP and
LSI through ablation studies.

</details>

### [1008] [Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech Separation](https://arxiv.org/abs/2505.13094)
*Guo Chen,Kai Li,Runxuan Yang,Xiaolin Hu*

Main category: cs.SD

TLDR: 提出了一种基于注意力机制和缓存记忆的因果语音分离模型TFACM，性能接近SOTA但复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 现有因果语音分离模型因难以保留历史信息而性能不足。

Method: 结合LSTM层、缓存记忆模块和因果注意力细化模块，捕捉时空关系。

Result: TFACM性能与SOTA模型相当，但复杂度和参数量显著降低。

Conclusion: TFACM通过有效利用历史信息，实现了高效的因果语音分离。

Abstract: Existing causal speech separation models often underperform compared to
non-causal models due to difficulties in retaining historical information. To
address this, we propose the Time-Frequency Attention Cache Memory (TFACM)
model, which effectively captures spatio-temporal relationships through an
attention mechanism and cache memory (CM) for historical information storage.
In TFACM, an LSTM layer captures frequency-relative positions, while causal
modeling is applied to the time dimension using local and global
representations. The CM module stores past information, and the causal
attention refinement (CAR) module further enhances time-based feature
representations for finer granularity. Experimental results showed that TFACM
achieveed comparable performance to the SOTA TF-GridNet-Causal model, with
significantly lower complexity and fewer trainable parameters. For more
details, visit the project page: https://cslikai.cn/TFACM/.

</details>