<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 31]
- [cs.LG](#cs.LG) [Total: 87]
- [cs.CL](#cs.CL) [Total: 41]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.CV](#cs.CV) [Total: 65]
- [math.NA](#math.NA) [Total: 1]
- [cs.SE](#cs.SE) [Total: 14]
- [cs.RO](#cs.RO) [Total: 7]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [cs.IR](#cs.IR) [Total: 12]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 3]
- [cs.DC](#cs.DC) [Total: 6]
- [math.OC](#math.OC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Multi-Party Private Set Operations from Predicative Zero-Sharing](https://arxiv.org/abs/2504.20050)
*Minglang Dong,Yu Chen,Cong Zhang,Yujie Bai,Yang Cao*

Main category: cs.CR

TLDR: 本文提出了一种多党私有集合操作（MPSO）的统一框架，支持任意集合公式的安全计算，具有高度的灵活性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有MPSO协议功能有限且缺乏统一框架，本文旨在填补这一空白。

Method: 设计了一个基于对称密钥技术的MPSO框架，支持任意集合操作组合。

Result: 框架在理论和实践上表现优异，计算和通信复杂度与集合大小线性相关，且在某些功能上达到最优复杂度。

Conclusion: 该框架是首个在MPSO领域实现高度灵活性和通用性的工作，无需依赖通用安全多方计算技术。

Abstract: Typical protocols in the multi-party private set operations (MPSO) setting
enable m > 2 parties to perform certain secure computation on the intersection
or union of their private sets, realizing a very limited range of MPSO
functionalities. Most works in this field focus on just one or two specific
functionalities, resulting in a large variety of isolated schemes and a lack of
a unified framework in MPSO research. In this work, we present an MPSO
framework, which allows m parties, each holding a set, to securely compute any
set formulas (arbitrary compositions of a finite number of binary set
operations, including intersection, union and difference) on their private
sets. Our framework is highly versatile and can be instantiated to accommodate
a broad spectrum of MPSO functionalities. To the best of our knowledge, this is
the first framework to achieve such a level of flexibility and generality in
MPSO, without relying on generic secure multi-party computation (MPC)
techniques.
  Our framework exhibits favorable theoretical and practical performance. The
computation and communication complexity scale linearly with the set size n,
and it achieves optimal complexity that is on par with the naive solution for
widely used functionalities, such as multi-party private set intersection
(MPSI), MPSI with cardinality output (MPSI-card), and MPSI with cardinality and
sum (MPSI-card-sum), in the standard semi-honest model. Furthermore, the
instantiations of our framework mainly from symmetric-key techniques yield
efficient protocols for MPSI, MPSI-card, MPSI-card-sum, and multi-party private
set union (MPSU), with online performance surpassing or matching the state of
the art.

</details>

### [2] [Cybersecurity for Autonomous Vehicles](https://arxiv.org/abs/2504.20180)
*Sai varun reddy Bhemavarapu*

Main category: cs.CR

TLDR: 论文探讨了自动驾驶车辆面临的网络安全威胁及其重要性，分析了挑战和现有解决方案，并强调了监管和行业合作的作用。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆的普及，网络安全威胁成为重要问题，需要保护系统安全和完整性以避免危害。

Method: 分析了自动驾驶车辆的网络安全挑战（如软硬件漏洞、无线通信风险等），并回顾了现有解决方案（如安全软件开发、入侵检测系统等）。

Result: 提出了加强网络安全的措施，包括监管、行业合作和标准制定，以支持自动驾驶技术的发展。

Conclusion: 通过分析当前网络安全形势并提出对策，论文旨在促进自动驾驶技术的安全发展和公众信任。

Abstract: The increasing adoption of autonomous vehicles is bringing a major shift in
the automotive industry. However, as these vehicles become more connected,
cybersecurity threats have emerged as a serious concern. Protecting the
security and integrity of autonomous systems is essential to prevent malicious
activities that can harm passengers, other road users, and the overall
transportation network. This paper focuses on addressing the cybersecurity
issues in autonomous vehicles by examining the challenges and risks involved,
which are important for building a secure future. Since autonomous vehicles
depend on the communication between sensors, artificial intelligence, external
infrastructure, and other systems, they are exposed to different types of cyber
threats. A cybersecurity breach in an autonomous vehicle can cause serious
problems, including a loss of public trust and safety. Therefore, it is very
important to develop and apply strong cybersecurity measures to support the
growth and acceptance of self-driving cars. This paper discusses major
cybersecurity challenges like vulnerabilities in software and hardware, risks
from wireless communication, and threats through external interfaces. It also
reviews existing solutions such as secure software development, intrusion
detection systems, cryptographic protocols, and anomaly detection methods.
Additionally, the paper highlights the role of regulatory bodies, industry
collaborations, and cybersecurity standards in creating a secure environment
for autonomous vehicles. Setting clear rules and best practices is necessary
for consistent protection across manufacturers and regions. By analyzing the
current cybersecurity landscape and suggesting practical countermeasures, this
paper aims to contribute to the safe development and public trust of autonomous
vehicle technology.

</details>

### [3] [A Case Study on the Use of Representativeness Bias as a Defense Against Adversarial Cyber Threats](https://arxiv.org/abs/2504.20245)
*Briland Hitaj,Grit Denker,Laura Tinnel,Michael McAnally,Bruce DeBruhl,Nathan Bunting,Alex Fafard,Daniel Aaron,Richard D. Roberts,Joshua Lawson,Greg McCain,Dylan Starink*

Main category: cs.CR

TLDR: 论文提出了一种基于心理学认知偏差（代表性偏差）的主动防御策略，通过实验证明其能有效引导黑客远离易受攻击路径。


<details>
  <summary>Details</summary>
Motivation: 现有网络安全防御策略多关注软硬件层面，忽视了人类因素。本文旨在探索利用人类认知偏差（如代表性偏差）来设计更有效的主动防御机制。

Method: 通过捕获旗帜（CTF）实验，设计了两类挑战以利用代表性偏差，观察其对黑客攻击路径选择的影响。

Result: 实验表明，利用代表性偏差的挑战能显著引导黑客远离易受攻击路径，验证了基于偏差的防御机制的有效性。

Conclusion: 该研究为未来利用更多人类认知偏差设计网络安全防御策略提供了新思路。

Abstract: Cyberspace is an ever-evolving battleground involving adversaries seeking to
circumvent existing safeguards and defenders aiming to stay one step ahead by
predicting and mitigating the next threat. Existing mitigation strategies have
focused primarily on solutions that consider software or hardware aspects,
often ignoring the human factor. This paper takes a first step towards
psychology-informed, active defense strategies, where we target biases that
human beings are susceptible to under conditions of uncertainty.
  Using capture-the-flag events, we create realistic challenges that tap into a
particular cognitive bias: representativeness. This study finds that this bias
can be triggered to thwart hacking attempts and divert hackers into
non-vulnerable attack paths. Participants were exposed to two different
challenges designed to exploit representativeness biases. One of the
representativeness challenges significantly thwarted attackers away from
vulnerable attack vectors and onto non-vulnerable paths, signifying an
effective bias-based defense mechanism. This work paves the way towards cyber
defense strategies that leverage additional human biases to thwart future,
sophisticated adversarial attacks.

</details>

### [4] [SA2FE: A Secure, Anonymous, Auditable, and Fair Edge Computing Service Offloading Framework](https://arxiv.org/abs/2504.20260)
*Xiaojian Wang,Huayue Gu,Zhouyu Li,Fangtong Zhou,Ruozhou Yu,Dejun Yang,Guoliang Xue*

Main category: cs.CR

TLDR: SA2FE框架解决了边缘计算中数据安全和隐私问题，通过重新随机化谜题和盲令牌方案保护用户隐私并确保公平卸载。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中用户任务卸载到异构设备时面临数据安全和隐私风险，现有解决方案依赖云服务器或未能保护敏感信息。

Method: 设计重新随机化谜题和盲令牌方案，保护敏感信息并确保公平卸载。

Result: 在通用可组合框架下证明安全性，并在移动设备和边缘服务器上验证性能与可扩展性。

Conclusion: SA2FE有效解决了边缘计算中的安全和隐私挑战，同时保持高性能和可扩展性。

Abstract: The inclusion of pervasive computing devices in a democratized edge computing
ecosystem can significantly expand the capability and coverage of near-end
computing for large-scale applications. However, offloading user tasks to
heterogeneous and decentralized edge devices comes with the dual risk of both
endangered user data security and privacy due to the curious base station or
malicious edge servers, and unfair offloading and malicious attacks targeting
edge servers from other edge servers and/or users. Existing solutions to edge
access control and offloading either rely on "always-on" cloud servers with
reduced edge benefits or fail to protect sensitive user service information. To
address these challenges, this paper presents SA2FE, a novel framework for edge
access control, offloading and accounting. We design a rerandomizable puzzle
primitive and a corresponding scheme to protect sensitive service information
from eavesdroppers and ensure fair offloading decisions, while a blind
token-based scheme safeguards user privacy, prevents double spending, and
ensures usage accountability. The security of SA2FE is proved under the
Universal Composability framework, and its performance and scalability are
demonstrated with implementation on commodity mobile devices and edge servers.

</details>

### [5] [A Virtual Cybersecurity Department for Securing Digital Twins in Water Distribution Systems](https://arxiv.org/abs/2504.20266)
*Mohammadhossein Homaei,Agustin Di Bartolo,Oscar Mogollon-Gutierrez,Fernando Broncano Morgado,Pablo Garcia Rodriguez*

Main category: cs.CR

TLDR: 论文提出了一种名为VCD的自动化网络安全框架，旨在为中小企业提供低成本、易管理的水务系统网络安全解决方案。


<details>
  <summary>Details</summary>
Motivation: 数字孪生在水务系统中广泛应用，但易受网络攻击，中小企业因预算和人员限制难以建立强大的网络安全团队。

Method: VCD框架结合开源工具（如Zabbix、Suricata、Fail2Ban）和基于机器学习的入侵检测系统（IDS），使用改进的集成模型训练于OD-IDS2022数据集。

Result: 该模型能检测多种网络威胁（如暴力破解、远程代码执行等），准确率达92%，误报率低。

Conclusion: VCD为中小企业提供了一种实用、高效且低成本的网络安全解决方案。

Abstract: Digital twins (DTs) help improve real-time monitoring and decision-making in
water distribution systems. However, their connectivity makes them easy targets
for cyberattacks such as scanning, denial-of-service (DoS), and unauthorized
access. Small and medium-sized enterprises (SMEs) that manage these systems
often do not have enough budget or staff to build strong cybersecurity teams.
To solve this problem, we present a Virtual Cybersecurity Department (VCD), an
affordable and automated framework designed for SMEs. The VCD uses open-source
tools like Zabbix for real-time monitoring, Suricata for network intrusion
detection, Fail2Ban to block repeated login attempts, and simple firewall
settings. To improve threat detection, we also add a machine-learning-based IDS
trained on the OD-IDS2022 dataset using an improved ensemble model. This model
detects cyber threats such as brute-force attacks, remote code execution (RCE),
and network flooding, with 92\% accuracy and fewer false alarms. Our solution
gives SMEs a practical and efficient way to secure water systems using low-cost
and easy-to-manage tools.

</details>

### [6] [Smart Water Security with AI and Blockchain-Enhanced Digital Twins](https://arxiv.org/abs/2504.20275)
*Mohammadhossein Homaei,Victor Gonzalez Morales,Oscar Mogollon Gutierrez,Ruben Molano Gomez,Andres Caro*

Main category: cs.CR

TLDR: 本文提出了一种结合LoRaWAN数据采集、机器学习驱动的入侵检测系统（IDS）和区块链数字孪生（BC-DT）平台的集成框架，用于农村地区安全透明的供水管理。


<details>
  <summary>Details</summary>
Motivation: 农村供水系统面临实时监控缺失、易受网络攻击和数据不可靠等问题，亟需一种安全且高效的解决方案。

Method: 采用LSTM自编码器和Isolation Forest过滤异常数据，通过私有以太坊区块链（PoA共识）记录验证数据，并构建实时数字孪生模型支持泄漏检测和预测维护。

Result: 实验结果显示，系统每秒处理80笔以上交易，延迟低于2秒，且支持1000个智能水表的扩展。

Conclusion: 该框架为农村分散式供水基础设施提供了一种实用且安全的解决方案。

Abstract: Water distribution systems in rural areas face serious challenges such as a
lack of real-time monitoring, vulnerability to cyberattacks, and unreliable
data handling. This paper presents an integrated framework that combines
LoRaWAN-based data acquisition, a machine learning-driven Intrusion Detection
System (IDS), and a blockchain-enabled Digital Twin (BC-DT) platform for secure
and transparent water management. The IDS filters anomalous or spoofed data
using a Long Short-Term Memory (LSTM) Autoencoder and Isolation Forest before
validated data is logged via smart contracts on a private Ethereum blockchain
using Proof of Authority (PoA) consensus. The verified data feeds into a
real-time DT model supporting leak detection, consumption forecasting, and
predictive maintenance. Experimental results demonstrate that the system
achieves over 80 transactions per second (TPS) with under 2 seconds of latency
while remaining cost-effective and scalable for up to 1,000 smart meters. This
work demonstrates a practical and secure architecture for decentralized water
infrastructure in under-connected rural environments.

</details>

### [7] [SoK: A Survey of Mixing Techniques and Mixers for Cryptocurrencies](https://arxiv.org/abs/2504.20296)
*Juraj Mariani,Ivan Homoliak*

Main category: cs.CR

TLDR: 本文综述了区块链加密货币中的混币技术及其实现，探讨了分类、技术、优缺点、隐私影响及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 区块链的伪匿名性引发隐私问题，促使混币技术的发展，本文旨在全面调查相关技术与实现。

Method: 分类回顾混币技术，调查实际实现及其组合，分析优缺点，评估隐私影响及漏洞。

Result: 总结了混币技术的现状、有效性及潜在风险，并探讨了智能合约和跨链领域的扩展挑战。

Conclusion: 混币技术对隐私保护至关重要，未来需解决智能合约和跨链应用中的挑战。

Abstract: Blockchain technologies have overturned the digital finance industry by
introducing a decentralized pseudonymous means of monetary transfer. The
pseudonymous nature introduced privacy concerns, enabling various
deanonymization techniques, which in turn spurred development of stronger
anonymity-preserving measures. The purpose of this paper is to create a
comprehensive survey of mixing techniques and implementations within the vast
ecosystem surrounding anonymization tools and mechanisms available in
blockchain cryptocurrencies. First, we begin by reviewing classifications used
in the field. Then, we survey various obfuscation techniques, helping to delve
into actual implementations and combinations of these techniques. Next, we
identify the positive and negative attributes of the approaches and
implementations included. Moreover, we examine the implications of
anonymization tools for user privacy, including their effectiveness in
preserving anonymity and susceptibility to attacks and vulnerabilities.
Finally, we discuss the challenges and innovations for extending mixing
services into the realm of smart contracts or cross-chain space.

</details>

### [8] [Enhancing Leakage Attacks on Searchable Symmetric Encryption Using LLM-Based Synthetic Data Generation](https://arxiv.org/abs/2504.20414)
*Joshua Chiu,Partha Protim Paul,Zahin Wahab*

Main category: cs.CR

TLDR: 论文研究了在最小数据泄露的威胁模型下，利用大型语言模型（如GPT-4）生成合成数据以增强可搜索对称加密（SSE）的泄漏攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有SSE方案易受泄漏攻击，但通常假设攻击者拥有大量加密数据集，这在现实中不成立。本文探讨了在更现实的威胁模型下增强攻击的可行性。

Method: 提出利用GPT-4生成统计和语义上类似真实数据（如Enron邮件）的合成文档，并通过随机采样和层次聚类方法评估其对SAP关键词推断攻击的效果。

Result: 结果表明，增加数据集大小和基于聚类的生成方法显著提高了攻击准确性，效果接近使用大量真实数据的攻击。

Conclusion: 研究强调了大型语言模型在对抗性场景中的重要性，并展示了合成数据在增强泄漏攻击中的潜力。

Abstract: Searchable Symmetric Encryption (SSE) enables efficient search capabilities
over encrypted data, allowing users to maintain privacy while utilizing cloud
storage. However, SSE schemes are vulnerable to leakage attacks that exploit
access patterns, search frequency, and volume information. Existing studies
frequently assume that adversaries possess a substantial fraction of the
encrypted dataset to mount effective inference attacks, implying there is a
database leakage of such documents, thus, an assumption that may not hold in
real-world scenarios. In this work, we investigate the feasibility of enhancing
leakage attacks under a more realistic threat model in which adversaries have
access to minimal leaked data. We propose a novel approach that leverages large
language models (LLMs), specifically GPT-4 variants, to generate synthetic
documents that statistically and semantically resemble the real-world dataset
of Enron emails. Using the email corpus as a case study, we evaluate the
effectiveness of synthetic data generated via random sampling and hierarchical
clustering methods on the performance of the SAP (Search Access Pattern)
keyword inference attack restricted to token volumes only. Our results
demonstrate that, while the choice of LLM has limited effect, increasing
dataset size and employing clustering-based generation significantly improve
attack accuracy, achieving comparable performance to attacks using larger
amounts of real data. We highlight the growing relevance of LLMs in adversarial
contexts.

</details>

### [9] [Network Attack Traffic Detection With Hybrid Quantum-Enhanced Convolution Neural Network](https://arxiv.org/abs/2504.20436)
*Zihao Wang,Kar Wai Fok,Vrizlynn L. L. Thing*

Main category: cs.CR

TLDR: 量子机器学习（QML）结合量子计算与机器学习（ML），提升算法速度和数据处理能力，特别关注网络流量中未知攻击的检测。通过设计量子卷积神经网络（QCNN），实验证明QML在未知攻击检测上优于传统ML。


<details>
  <summary>Details</summary>
Motivation: 传统ML在检测训练数据中未出现的恶意流量（未知攻击）时表现不佳，QML利用量子特性有望提升检测性能。

Method: 设计并评估新型混合结构的量子卷积神经网络（QCNN），与传统ML方法对比检测性能、泛化能力和鲁棒性。

Result: 实验结果显示，QCNN模型在未知攻击检测上优于传统ML方法。

Conclusion: QML在网络流量检测，尤其是未知攻击检测方面具有显著优势，为未来研究提供了新方向。

Abstract: The emerging paradigm of Quantum Machine Learning (QML) combines features of
quantum computing and machine learning (ML). QML enables the generation and
recognition of statistical data patterns that classical computers and classical
ML methods struggle to effectively execute. QML utilizes quantum systems to
enhance algorithmic computation speed and real-time data processing
capabilities, making it one of the most promising tools in the field of ML.
Quantum superposition and entanglement features also hold the promise to
potentially expand the potential feature representation capabilities of ML.
Therefore, in this study, we explore how quantum computing affects ML and
whether it can further improve the detection performance on network traffic
detection, especially on unseen attacks which are types of malicious traffic
that do not exist in the ML training dataset. Classical ML models often perform
poorly in detecting these unseen attacks because they have not been trained on
such traffic. Hence, this paper focuses on designing and proposing novel hybrid
structures of Quantum Convolutional Neural Network (QCNN) to achieve the
detection of malicious traffic. The detection performance, generalization, and
robustness of the QML solutions are evaluated and compared with classical ML
running on classical computers. The emphasis lies in assessing whether the
QML-based malicious traffic detection outperforms classical solutions. Based on
experiment results, QCNN models demonstrated superior performance compared to
classical ML approaches on unseen attack detection.

</details>

### [10] [Robustness via Referencing: Defending against Prompt Injection Attacks by Referencing the Executed Instruction](https://arxiv.org/abs/2504.20472)
*Yulin Chen,Haoran Li,Yuan Sui,Yue Liu,Yufei He,Yangqiu Song,Bryan Hooi*

Main category: cs.CR

TLDR: 论文提出了一种新方法，利用而非抑制大语言模型（LLM）的指令遵循能力，通过生成包含指令引用的响应来防御提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: LLM因无法区分指令和数据内容而容易受到提示注入攻击，现有防御方法难以有效抑制其指令遵循倾向。

Method: 提出一种新防御方法，要求LLM生成包含答案及对应指令引用的响应，并过滤掉与原始指令无关的答案。

Result: 实验表明，该方法优于提示工程基线，性能接近微调方法，某些场景下攻击成功率降至0%，且对整体效用影响极小。

Conclusion: 新方法有效解决了提示注入攻击问题，同时保持了LLM的高效性。

Abstract: Large language models (LLMs) have demonstrated impressive performance and
have come to dominate the field of natural language processing (NLP) across
various tasks. However, due to their strong instruction-following capabilities
and inability to distinguish between instructions and data content, LLMs are
vulnerable to prompt injection attacks. These attacks manipulate LLMs into
deviating from the original input instructions and executing maliciously
injected instructions within data content, such as web documents retrieved from
search engines. Existing defense methods, including prompt-engineering and
fine-tuning approaches, typically instruct models to follow the original input
instructions while suppressing their tendencies to execute injected
instructions. However, our experiments reveal that suppressing
instruction-following tendencies is challenging. Through analyzing failure
cases, we observe that although LLMs tend to respond to any recognized
instructions, they are aware of which specific instructions they are executing
and can correctly reference them within the original prompt. Motivated by these
findings, we propose a novel defense method that leverages, rather than
suppresses, the instruction-following abilities of LLMs. Our approach prompts
LLMs to generate responses that include both answers and their corresponding
instruction references. Based on these references, we filter out answers not
associated with the original input instructions. Comprehensive experiments
demonstrate that our method outperforms prompt-engineering baselines and
achieves performance comparable to fine-tuning methods, reducing the attack
success rate (ASR) to 0 percent in some scenarios. Moreover, our approach has
minimal impact on overall utility.

</details>

### [11] [Sleeping Giants -- Activating Dormant Java Deserialization Gadget Chains through Stealthy Code Changes](https://arxiv.org/abs/2504.20485)
*Bruno Kreyssig,Sabine Houy,Timothée Riom,Alexandre Bartel*

Main category: cs.CR

TLDR: 该论文研究了Java反序列化小工具链的潜在风险，特别是依赖项中小代码修改如何激活或引入这些链，并验证了休眠小工具链作为供应链攻击的可行性。


<details>
  <summary>Details</summary>
Motivation: Java反序列化小工具链是已知的严重软件漏洞，且多数依赖依赖项中的小工具。研究表明，依赖项中的小代码修改可能激活这些链，使得检测成为被动行为。论文旨在评估小代码修改激活小工具链的可能性，并探索其作为供应链攻击的潜力。

Method: 研究分析了依赖项中类可序列化性的波动性，并提出了三种攻击者可能用于隐蔽引入小工具链的修改模式。对533个依赖项应用这些模式，并使用三种先进的小工具链检测工具进行测试。

Result: 在26.08%的依赖项中，修改模式可激活或注入小工具链。最终验证了53个依赖项中存在休眠小工具链，表明其作为供应链攻击的潜在威胁。

Conclusion: Java反序列化小工具链是广泛存在的软件漏洞，休眠小工具链为供应链攻击提供了新的攻击途径。

Abstract: Java deserialization gadget chains are a well-researched critical software
weakness. The vast majority of known gadget chains rely on gadgets from
software dependencies. Furthermore, it has been shown that small code changes
in dependencies have enabled these gadget chains. This makes gadget chain
detection a purely reactive endeavor. Even if one dependency's deployment
pipeline employs gadget chain detection, a gadget chain can still result from
gadgets in other dependencies. In this work, we assess how likely small code
changes are to enable a gadget chain. These changes could either be accidental
or intentional as part of a supply chain attack. Specifically, we show that
class serializability is a strongly fluctuating property over a dependency's
evolution. Then, we investigate three change patterns by which an attacker
could stealthily introduce gadgets into a dependency. We apply these patterns
to 533 dependencies and run three state-of-the-art gadget chain detectors both
on the original and the modified dependencies. The tools detect that applying
the modification patterns can activate/inject gadget chains in 26.08% of the
dependencies we selected. Finally, we verify the newly detected chains. As
such, we identify dormant gadget chains in 53 dependencies that could be added
through minor code modifications. This both shows that Java deserialization
gadget chains are a broad liability to software and proves dormant gadget
chains as a lucrative supply chain attack vector.

</details>

### [12] [Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression](https://arxiv.org/abs/2504.20493)
*Yu Cui,Yujun Cai,Yiwei Wang*

Main category: cs.CR

TLDR: 论文提出了一种名为“推理中断攻击”的新型提示注入攻击方法，通过自适应令牌压缩技术，显著降低了触发LLM安全漏洞的提示长度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在推理任务中表现出色，但存在安全漏洞，如“思考停止”漏洞。现有触发方法需要复杂且冗长的提示，成本高。

Method: 提出基于自适应令牌压缩的“推理中断攻击”，利用简单算术任务触发漏洞，并开发系统化方法收集攻击提示和压缩框架。

Result: 实验表明压缩框架显著减少提示长度，同时保持攻击有效性，并分析了漏洞的底层原因。

Conclusion: 研究为提升推理LLM的安全性提供了重要见解，展示了简单任务触发漏洞的潜力。

Abstract: While reasoning large language models (LLMs) demonstrate remarkable
performance across various tasks, they also contain notable security
vulnerabilities. Recent research has uncovered a "thinking-stopped"
vulnerability in DeepSeek-R1, where model-generated reasoning tokens can
forcibly interrupt the inference process, resulting in empty responses that
compromise LLM-integrated applications. However, existing methods triggering
this vulnerability require complex mathematical word problems with long
prompts--even exceeding 5,000 tokens. To reduce the token cost and formally
define this vulnerability, we propose a novel prompt injection attack named
"Reasoning Interruption Attack", based on adaptive token compression. We
demonstrate that simple standalone arithmetic tasks can effectively trigger
this vulnerability, and the prompts based on such tasks exhibit simpler logical
structures than mathematical word problems. We develop a systematic approach to
efficiently collect attack prompts and an adaptive token compression framework
that utilizes LLMs to automatically compress these prompts. Experiments show
our compression framework significantly reduces prompt length while maintaining
effective attack capabilities. We further investigate the attack's performance
via output prefix and analyze the underlying causes of the vulnerability,
providing valuable insights for improving security in reasoning LLMs.

</details>

### [13] [Starfish: Rebalancing Multi-Party Off-Chain Payment Channels](https://arxiv.org/abs/2504.20536)
*Minghui Xu,Wenxuan Yu,Guangyong Shang,Guangpeng Qi,Dongliang Duan,Shan Wang,Kun Li,Yue Zhang,Xiuzhen Cheng*

Main category: cs.CR

TLDR: Starfish是一种基于星形网络结构的PCN再平衡方法，高效且支持大容量通道，仅需N次链上操作。


<details>
  <summary>Details</summary>
Motivation: 解决PCN中因支付偏斜导致的通道余额耗尽问题，提升交易成功率。

Method: 利用星形网络结构连接独立通道并聚合预算，仅需N次链上操作。

Result: Starfish在效率和容量上优于现有再平衡技术，并通过了安全性验证。

Conclusion: Starfish为PCN再平衡提供了高效且可扩展的解决方案。

Abstract: Blockchain technology has revolutionized the way transactions are executed,
but scalability remains a major challenge. Payment Channel Network (PCN), as a
Layer-2 scaling solution, has been proposed to address this issue. However,
skewed payments can deplete the balance of one party within a channel,
restricting the ability of PCNs to transact through a path and subsequently
reducing the transaction success rate. To address this issue, the technology of
rebalancing has been proposed. However, existing rebalancing strategies in PCNs
are limited in their capacity and efficiency. Cycle-based approaches only
address rebalancing within groups of nodes that form a cycle network, while
non-cycle-based approaches face high complexity of on-chain operations and
limitations on rebalancing capacity. In this study, we propose Starfish, a
rebalancing approach that captures the star-shaped network structure to provide
high rebalancing efficiency and large channel capacity. Starfish requires only
$N$-time on-chain operations to connect independent channels and aggregate the
total budget of all channels. To demonstrate the correctness and advantages of
our method, we provide a formal security proof of the Starfish protocol and
conduct comparative experiments with existing rebalancing techniques.

</details>

### [14] [Mutual Information Minimization for Side-Channel Attack Resistance via Optimal Noise Injection](https://arxiv.org/abs/2504.20556)
*Jiheon Woo,Daewon Seo,Young-Sik Kim,Namyoon Lee,Yuval Cassuto,Yongjune Kim*

Main category: cs.CR

TLDR: 论文提出了一种基于互信息最小化的最优人工噪声注入方法，用于减少侧信道攻击的信息泄漏，适用于资源受限系统。


<details>
  <summary>Details</summary>
Motivation: 侧信道攻击（SCA）通过物理泄漏（如功耗、时间变化等）提取密钥，现有的人工噪声注入方法功耗高，不适用于物联网等资源受限系统。

Method: 将SCA建模为通信信道，通过最小化秘密信息与侧信道观测之间的互信息（受限于噪声功率约束），提出两种凸优化问题：1）最小化总互信息；2）最小化观测间的最大互信息。

Result: 数值结果表明，与传统方法相比，所提方法显著降低了总互信息和最大互信息。

Conclusion: 该方法为资源受限的安全关键系统提供了一种高效的保护方案。

Abstract: Side-channel attacks (SCAs) pose a serious threat to system security by
extracting secret keys through physical leakages such as power consumption,
timing variations, and electromagnetic emissions. Among existing
countermeasures, artificial noise injection is recognized as one of the most
effective techniques. However, its high power consumption poses a major
challenge for resource-constrained systems such as Internet of Things (IoT)
devices, motivating the development of more efficient protection schemes. In
this paper, we model SCAs as a communication channel and aim to suppress
information leakage by minimizing the mutual information between the secret
information and side-channel observations, subject to a power constraint on the
artificial noise. We propose an optimal artificial noise injection method to
minimize the mutual information in systems with Gaussian inputs. Specifically,
we formulate two convex optimization problems: 1) minimizing the total mutual
information, and 2) minimizing the maximum mutual information across
observations. Numerical results show that the proposed methods significantly
reduce both total and maximum mutual information compared to conventional
techniques, confirming their effectiveness for resource-constrained,
security-critical systems.

</details>

### [15] [VIMU: Effective Physics-based Realtime Detection and Recovery against Stealthy Attacks on UAVs](https://arxiv.org/abs/2504.20569)
*Yunbo Wang,Cong Sun,Qiaosen Liu,Bingnan Su,Zongxu Zhang,Michael Norris,Gang Tan,Jianfeng Ma*

Main category: cs.CR

TLDR: VIMU系统通过CS-EMA算法和精细物理模型，有效检测和缓解无人机传感器攻击，提升飞行稳定性。


<details>
  <summary>Details</summary>
Motivation: 传感器攻击日益复杂且隐蔽，现有物理模型方法检测效果有限，需改进。

Method: 提出CS-EMA检测算法和精细非线性物理模型，结合FIFO缓冲区保护。

Result: VIMU在PX4自动驾驶仪上实现，能有效检测和缓解多种隐蔽攻击。

Conclusion: VIMU系统显著提升传感器攻击的检测和恢复能力，保障无人机飞行安全。

Abstract: Sensor attacks on robotic vehicles have become pervasive and manipulative.
Their latest advancements exploit sensor and detector characteristics to bypass
detection. Recent security efforts have leveraged the physics-based model to
detect or mitigate sensor attacks. However, these approaches are only resilient
to a few sensor attacks and still need improvement in detection effectiveness.
We present VIMU, an efficient sensor attack detection and resilience system for
unmanned aerial vehicles. We propose a detection algorithm, CS-EMA, that
leverages low-pass filtering to identify stealthy gyroscope attacks while
achieving an overall effective sensor attack detection. We develop a
fine-grained nonlinear physical model with precise aerodynamic and propulsion
wrench modeling. We also augment the state estimation with a FIFO buffer
safeguard to mitigate the impact of high-rate IMU attacks. The proposed
physical model and buffer safeguard provide an effective system state recovery
toward maintaining flight stability. We implement VIMU on PX4 autopilot. The
evaluation results demonstrate the effectiveness of VIMU in detecting and
mitigating various realistic sensor attacks, especially stealthy attacks.

</details>

### [16] [ReCIT: Reconstructing Full Private Data from Gradient in Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2504.20570)
*Jin Xie,Ruishi He,Songze Li,Xiaojun Jia,Shouling Ji*

Main category: cs.CR

TLDR: ReCIT是一种新型隐私攻击方法，能够从参数高效微调（PEFT）的梯度中高保真地恢复完整私有数据，解决了现有方法在同时恢复上下文前缀和个人身份信息（PII）时的挑战。


<details>
  <summary>Details</summary>
Motivation: 在协作学习场景（如联邦学习）中，PEFT的梯度交换可能导致数据隐私泄露，现有方法无法同时高精度恢复上下文和PII。

Method: ReCIT通过恶意微调增强预训练模型的记忆能力，并提出基于过滤的令牌提取技术和令牌配对机制，以解决高维令牌空间和大批量问题。

Result: ReCIT在多种PEFT范式中表现优于现有梯度反转和基于记忆的攻击，PII恢复率提高10倍，且在大批量下仍有效。

Conclusion: 研究强调了在去中心化或共享训练环境中重新评估PEFT隐私保障的紧迫性。

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a practical solution
for adapting large language models (LLMs) to custom datasets with significantly
reduced computational cost. When carrying out PEFT under collaborative learning
scenarios (e.g., federated learning), it is often required to exchange model
updates (or gradients) across parties. These gradients, even with limited
dimensions, can cause severe breach of data privacy. Recent works have shown
that both contextual prefixes and personally identifiable information (PII) can
be exposed through gradients. However, \emph{simultaneously} and
\emph{accurately} recovering both components from the same training instance
remains infeasible due to the following challenges: 1) limited number of PEFT
parameters; 2) high-dimensional token spaces; and 3) large batch sizes. We
propose ReCIT, a novel privacy attack that addresses all challenges, and
achieves recovery of \emph{full} private data from PEFT gradients with high
fidelity. Specifically, ReCIT proposes to enhance the memorization capability
of the pre-trained model through malicious fine-tuning with Personal Notes;
ReCIT also proposes a novel filter-based token extraction technique and a token
pairing mechanism, to accurately reconstruct tokens from the training sequences
with large batch sizes. Extensive evaluations show that ReCIT consistently
outperforms state-of-the-art gradient inversion and memorization-based attacks
across different PEFT paradigms. It achieves up to 10$\times$ higher PII
recovery rates and remains effective across varying batch sizes, especially in
settings where prefix reconstruction is intractable for conventional
approaches. These findings highlight an urgent need to reassess the privacy
guarantees of PEFT, especially in decentralized or shared training
environments.

</details>

### [17] [The Hidden Risks of LLM-Generated Web Application Code: A Security-Centric Evaluation of Code Generation Capabilities in Large Language Models](https://arxiv.org/abs/2504.20612)
*Swaroop Dora,Deven Lunkad,Naziya Aslam,S. Venkatesan,Sandeep Kumar Shukla*

Main category: cs.CR

TLDR: LLMs提升开发效率但生成代码存在安全隐患，研究发现多模型代码未完全符合安全标准，需人工审核和更强安全框架。


<details>
  <summary>Details</summary>
Motivation: 评估LLM生成代码的安全性，揭示其在真实应用中的潜在风险。

Method: 使用预定义安全参数测试多模型（如ChatGPT、DeepSeek等）生成代码的安全性。

Result: 发现认证、会话管理等关键漏洞，模型安全措施不足。

Conclusion: 需人工审核和更强安全框架以确保LLM生成代码的可靠性。

Abstract: The rapid advancement of Large Language Models (LLMs) has enhanced software
development processes, minimizing the time and effort required for coding and
enhancing developer productivity. However, despite their potential benefits,
code generated by LLMs has been shown to generate insecure code in controlled
environments, raising critical concerns about their reliability and security in
real-world applications. This paper uses predefined security parameters to
evaluate the security compliance of LLM-generated code across multiple models,
such as ChatGPT, DeepSeek, Claude, Gemini and Grok. The analysis reveals
critical vulnerabilities in authentication mechanisms, session management,
input validation and HTTP security headers. Although some models implement
security measures to a limited extent, none fully align with industry best
practices, highlighting the associated risks in automated software development.
Our findings underscore that human expertise is crucial to ensure secure
software deployment or review of LLM-generated code. Also, there is a need for
robust security assessment frameworks to enhance the reliability of
LLM-generated code in real-world applications.

</details>

### [18] [A Novel Cipher for Enhancing MAVLink Security: Design, Security Analysis, and Performance Evaluation Using a Drone Testbed](https://arxiv.org/abs/2504.20626)
*Bhavya Dixit,Ananthapadmanabhan A.,Adheeba Thahsin,Saketh Pathak,Gaurav S. Kasbekar,Arnab Maity*

Main category: cs.CR

TLDR: MAVShield是一种新型轻量级密码，用于保护无人机（UAV）中MAVLink协议的通信，性能优于现有算法。


<details>
  <summary>Details</summary>
Motivation: MAVLink协议默认未加密，现有研究多为理论或模拟，缺乏实际实现。

Method: 在真实无人机测试平台上实现MAVShield，并与AES-CTR、ChaCha20等算法对比。

Result: MAVShield通过NIST和Diehard测试，安全性强，且在内存、CPU和电池消耗上表现更优。

Conclusion: MAVShield为MAVLink通信提供了高效的实际解决方案。

Abstract: We present MAVShield, a novel lightweight cipher designed to secure
communications in Unmanned Aerial Vehicles (UAVs) using the MAVLink protocol,
which by default transmits unencrypted messages between UAVs and Ground Control
Stations (GCS). While existing studies propose encryption for MAVLink, most
remain theoretical or simulation-based. We implement MAVShield alongside
AES-CTR, ChaCha20, Speck-CTR, and Rabbit, and evaluate them on a real drone
testbed. A comprehensive security analysis using statistical test suites (NIST
and Diehard) demonstrates strong resistance of the novel cipher to
cryptanalysis. Performance evaluation across key metrics including memory
usage, CPU load, and battery power consumption, demonstrates that MAVShield
outperforms existing algorithms and offers an efficient, real-world solution
for securing MAVLink communications in UAVs.

</details>

### [19] [Protocol Dialects as Formal Patterns: A Composable Theory of Lingos -- Technical report](https://arxiv.org/abs/2504.20637)
*Víctor García,Santiago Escobar,Catherine Meadows,Jose Meseguer*

Main category: cs.CR

TLDR: 论文提出了一种通过协议方言（protocol dialects）增强轻量级安全性的方法，重点是通过“行话”（lingo）动态变化使攻击者难以破解。


<details>
  <summary>Details</summary>
Motivation: 针对易受攻击的协议提供轻量级安全保护，防止简单攻击演变为严重威胁。

Method: 提出了多种行话变换和组合方法，从简单的行话生成更强大的行话，从而增强方言的安全性。

Result: 通过动态变化的行话使攻击者难以掌握协议方言，提升了安全性。

Conclusion: 协议方言和行话的动态变化是提升轻量级安全性的有效方法。

Abstract: Protocol dialects are methods for modifying protocols that provide
light-weight security, especially against easy attacks that can lead to more
serious ones. A lingo is a dialect's key security component by making attackers
unable to "speak" the lingo. A lingo's "talk" changes all the time, becoming a
moving target for attackers. We present several kinds of lingo transformations
and compositions to generate stronger lingos from simpler ones, thus making
dialects more secure.

</details>

### [20] [Data Encryption Battlefield: A Deep Dive into the Dynamic Confrontations in Ransomware Attacks](https://arxiv.org/abs/2504.20681)
*Arash Mahboubi,Hamed Aboutorab,Seyit Camtepe,Hang Thanh Bui,Khanh Luong,Keyvan Ansari,Shenlu Wang,Bazara Barry*

Main category: cs.CR

TLDR: 论文研究了对抗勒索软件加密策略的动态防御方法，重点评估了在线增量机器学习算法在检测不同加密技术中的效果。


<details>
  <summary>Details</summary>
Motivation: 随着勒索软件攻击者采用更复杂的加密方法（如Base64编码和间歇性加密），传统检测方法失效，需要开发新的防御技术。

Method: 研究使用在线增量机器学习算法（如Hoeffding Tree和Random Forest）分析包含11,928个文件的32.6GB数据集，覆盖多种文件格式和75种勒索软件家族。

Result: Hoeffding Tree在检测传统和AES-Base64加密中表现优异，而Random Forest在识别间歇性加密方面更有效。

Conclusion: 针对勒索软件的不同加密策略，需要定制化的机器学习解决方案以提升防御效果。

Abstract: In the rapidly evolving landscape of cybersecurity threats, ransomware
represents a significant challenge. Attackers increasingly employ sophisticated
encryption methods, such as entropy reduction through Base64 encoding, and
partial or intermittent encryption to evade traditional detection methods. This
study explores the dynamic battle between adversaries who continuously refine
encryption strategies and defenders developing advanced countermeasures to
protect vulnerable data. We investigate the application of online incremental
machine learning algorithms designed to predict file encryption activities
despite adversaries evolving obfuscation techniques. Our analysis utilizes an
extensive dataset of 32.6 GB, comprising 11,928 files across multiple formats,
including Microsoft Word documents (doc), PowerPoint presentations (ppt), Excel
spreadsheets (xlsx), image formats (jpg, jpeg, png, tif, gif), PDFs (pdf),
audio (mp3), and video (mp4) files. These files were encrypted by 75 distinct
ransomware families, facilitating a robust empirical evaluation of machine
learning classifiers effectiveness against diverse encryption tactics. Results
highlight the Hoeffding Tree algorithms superior incremental learning
capability, particularly effective in detecting traditional and AES-Base64
encryption methods employed to lower entropy. Conversely, the Random Forest
classifier with warm-start functionality excels at identifying intermittent
encryption methods, demonstrating the necessity of tailored machine learning
solutions to counter sophisticated ransomware strategies.

</details>

### [21] [DICOM Compatible, 3D Multimodality Image Encryption using Hyperchaotic Signal](https://arxiv.org/abs/2504.20689)
*Anandik N Anand,Sishu Shankar Muni,Abhishek Kaushik*

Main category: cs.CR

TLDR: 提出了一种基于超混沌信号和多级扩散的医学图像加密方法，结合验证码认证，实现双重安全保护。


<details>
  <summary>Details</summary>
Motivation: 保护医学图像中的敏感健康信息免受网络攻击和未经授权的访问。

Method: 利用逻辑映射生成混沌信号进行像素随机置换，结合4位、8位、径向和相邻扩散的多级扩散方法，并引入验证码认证。

Result: 实验结果表明，该方法在保持图像完整性的同时提供了强大的安全性，显著减少了未经授权的数据泄露。

Conclusion: 双重加密方法不仅保护了医学图像的机密性，还通过验证码增强了认证安全性。

Abstract: Medical image encryption plays an important role in protecting sensitive
health information from cyberattacks and unauthorized access. In this paper, we
introduce a secure and robust encryption scheme that is multi-modality
compatible and works with MRI, CT, X-Ray and Ultrasound images for different
anatomical region of interest. The method utilizes hyperchaotic signals and
multi-level diffusion methods. The encryption starts by taking DICOM image as
input, then padding to increase the image area. Chaotic signals are produced by
a logistic map and are used to carry out pixel random permutation. Then,
multi-level diffusion is carried out by 4-bit, 8-bit, radial and adjacent
diffusion to provide high randomness and immunity against statistical attacks.
In addition, we propose a captcha-based authentication scheme to further
improve security. An algorithm generates alphanumeric captcha-based image which
is encrypted with the same chaotic and diffusion methods as the medical image.
Both encrypted images(DICOM image and captcha image) are then superimposed to
create a final encrypted output, essentially integrating dual-layer security.
Upon decryption, the superimposed image is again decomposed back to original
medical and captcha images, and inverse operations are performed to obtain the
original unencrypted data. Experimental results show that the proposed method
provides strong protection with no loss in image integrity, thereby reducing
unauthorized data breaches to a significant level. The dual-encryption approach
not only protects the confidentiality of the medical images but also enhances
authentication by incorporating captcha.

</details>

### [22] [Building Trust in Healthcare with Privacy Techniques: Blockchain in the Cloud](https://arxiv.org/abs/2504.20700)
*Ferhat Ozgur Catak,Chunming Rong,Øyvind Meinich-Bache,Sara Brunner,Kjersti Engan*

Main category: cs.CR

TLDR: 论文提出了一种结合以太坊区块链与云计算的架构，用于新生儿护理中的视频数据分析，重点解决患者同意、数据安全和信任问题。


<details>
  <summary>Details</summary>
Motivation: 通过结合区块链与云计算技术，提升新生儿护理中的数据安全性和透明度，同时简化患者同意的管理。

Method: 采用以太坊区块链和智能合约机制，结合云计算，确保数据保护和可控共享。

Result: 展示了区块链与云计算在医疗数据管理中的潜力，强调了数据完整性的重要性。

Conclusion: 该研究为计算机科学和医疗创新提供了新思路，展示了区块链与云计算在医疗领域的应用前景。

Abstract: This study introduces a cutting-edge architecture developed for the
NewbornTime project, which uses advanced AI to analyze video data at birth and
during newborn resuscitation, with the aim of improving newborn care. The
proposed architecture addresses the crucial issues of patient consent, data
security, and investing trust in healthcare by integrating Ethereum blockchain
with cloud computing. Our blockchain-based consent application simplifies
patient consent's secure and transparent management. We explain the smart
contract mechanisms and privacy measures employed, ensuring data protection
while permitting controlled data sharing among authorized parties. This work
demonstrates the potential of combining blockchain and cloud technologies in
healthcare, emphasizing their role in maintaining data integrity, with
implications for computer science and healthcare innovation.

</details>

### [23] [Enhancing Vulnerability Reports with Automated and Augmented Description Summarization](https://arxiv.org/abs/2504.20726)
*Hattan Althebeiti,Mohammed Alkinoon,Manar Mohaisen,Saeed Salem,DaeHun Nyang,David Mohaisen*

Main category: cs.CR

TLDR: Zad系统通过外部资源丰富NVD漏洞描述，解决描述简短和信息不足的问题。


<details>
  <summary>Details</summary>
Motivation: 公共漏洞数据库（如NVD）的描述通常简短且信息不足，需要改进。

Method: Zad包含两个流程：一是收集和过滤补充数据构建详细数据集，二是微调预训练模型生成丰富描述。

Result: 评估显示Zad能显著提升漏洞描述的全面性和连贯性。

Conclusion: Zad有效增强了漏洞信息的质量，为威胁信息共享提供了更全面的支持。

Abstract: Public vulnerability databases, such as the National Vulnerability Database
(NVD), document vulnerabilities and facilitate threat information sharing.
However, they often suffer from short descriptions and outdated or insufficient
information. In this paper, we introduce Zad, a system designed to enrich NVD
vulnerability descriptions by leveraging external resources. Zad consists of
two pipelines: one collects and filters supplementary data using two encoders
to build a detailed dataset, while the other fine-tunes a pre-trained model on
this dataset to generate enriched descriptions. By addressing brevity and
improving content quality, Zad produces more comprehensive and cohesive
vulnerability descriptions. We evaluate Zad using standard summarization
metrics and human assessments, demonstrating its effectiveness in enhancing
vulnerability information.

</details>

### [24] [did:self A registry-less DID method](https://arxiv.org/abs/2504.20767)
*Nikos Fotiou,George C. Polyzos,Vasilios A. Siris*

Main category: cs.CR

TLDR: did:self是一种去中心化标识符（DID）方法，无需依赖可信注册表存储DID文档，支持通过任意方式传播认证信息，具有轻量级、可控委托、高安全性和隐私性等特点。


<details>
  <summary>Details</summary>
Motivation: 解决传统DID方法依赖可信注册表的问题，提供更灵活、安全的身份认证方案。

Method: 通过去中心化方式传播DID文档信息，支持隐式DID文档重建，利用JSON Web Tokens和X.509证书等认证材料。

Result: 实现了轻量级、高安全性和隐私性的身份认证，适用于人、内容和物联网设备。

Conclusion: did:self为去中心化身份认证提供了高效、灵活的解决方案。

Abstract: We introduce did:self, a Decentralized Identifier (DID) method that does not
depend on any trusted registry for storing the corresponding DID documents.
Information for authenticating a did:self subject can be disseminated using any
means and without making any security assumption about the delivery method.
did:self is lightweight, it allows controlled delegation, it offers increased
security and privacy, and it can be used for identifying people, content, as
well as IoT devices. Furthermore, DID documents in did:self can be implicit,
allowing re-construction of DID documents based on other authentication
material, such as JSON Web Tokens and X.509 certificates.

</details>

### [25] [Unlocking User-oriented Pages: Intention-driven Black-box Scanner for Real-world Web Applications](https://arxiv.org/abs/2504.20801)
*Weizhe Wang,Yao Zhang,Kaitai Liang,Guangquan Xu,Hongpeng Bai,Qingyang Yan,Xi Zheng,Bin Wu*

Main category: cs.CR

TLDR: Hoyen是一种基于大型语言模型的黑盒扫描器，通过预测用户意图扩展扫描范围，显著提高测试覆盖率和漏洞检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒扫描器难以访问需要通过复杂用户交互才能进入的深层页面，限制了漏洞检测的全面性。

Method: 利用网页中的语义信息，通过大型语言模型预测用户意图，指导扫描范围的扩展。

Result: 在12个开源Web应用上评估，Hoyen的覆盖率平均是其他工具的2倍，检测到更多漏洞，包括知名应用中的独特漏洞。

Conclusion: Hoyen通过语义理解和意图预测，显著提升了黑盒扫描的覆盖率和漏洞检测效果。

Abstract: Black-box scanners have played a significant role in detecting
vulnerabilities for web applications. A key focus in current black-box scanning
is increasing test coverage (i.e., accessing more web pages). However, since
many web applications are user-oriented, some deep pages can only be accessed
through complex user interactions, which are difficult to reach by existing
black-box scanners. To fill this gap, a key insight is that web pages contain a
wealth of semantic information that can aid in understanding potential user
intention. Based on this insight, we propose Hoyen, a black-box scanner that
uses the Large Language Model to predict user intention and provide guidance
for expanding the scanning scope. Hoyen has been rigorously evaluated on 12
popular open-source web applications and compared with 6 representative tools.
The results demonstrate that Hoyen performs a comprehensive exploration of web
applications, expanding the attack surface while achieving about 2x than the
coverage of other scanners on average, with high request accuracy. Furthermore,
Hoyen detected over 90% of its requests towards the core functionality of the
application, detecting more vulnerabilities than other scanners, including
unique vulnerabilities in well-known web applications. Our data/code is
available at https://hoyen.tjunsl.com/

</details>

### [26] [DP-SMOTE: Integrating Differential Privacy and Oversampling Technique to Preserve Privacy in Smart Homes](https://arxiv.org/abs/2504.20827)
*Amr Tarek Elsayed,Almohammady Sobhi Alsharkawy,Mohamed Sayed Farag,Shaban Ebrahim Abu Yusuf*

Main category: cs.CR

TLDR: 本文提出了一种基于差分隐私的智能家居数据安全共享方法，结合SMOTe和高斯噪声生成合成数据，同时保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 智能家居设备收集用户数据以提升生活质量，但数据共享需确保隐私安全。

Method: 结合SMOTe算法和高斯噪声生成合成数据，并应用k-匿名函数评估重识别风险。

Result: 方法在隐私保护和准确性（90%-98%）方面表现优异，重识别风险为30%。

Conclusion: 该方法有效平衡了数据共享与隐私保护，适用于智能家居场景。

Abstract: Smart homes represent intelligent environments where interconnected devices
gather information, enhancing users living experiences by ensuring comfort,
safety, and efficient energy management. To enhance the quality of life,
companies in the smart device industry collect user data, including activities,
preferences, and power consumption. However, sharing such data necessitates
privacy-preserving practices. This paper introduces a robust method for secure
sharing of data to service providers, grounded in differential privacy (DP).
This empowers smart home residents to contribute usage statistics while
safeguarding their privacy. The approach incorporates the Synthetic Minority
Oversampling technique (SMOTe) and seamlessly integrates Gaussian noise to
generate synthetic data, enabling data and statistics sharing while preserving
individual privacy. The proposed method employs the SMOTe algorithm and applies
Gaussian noise to generate data. Subsequently, it employs a k-anonymity
function to assess reidentification risk before sharing the data. The
simulation outcomes demonstrate that our method delivers strong performance in
safeguarding privacy and in accuracy, recall, and f-measure metrics. This
approach is particularly effective in smart homes, offering substantial utility
in privacy at a reidentification risk of 30%, with Gaussian noise set to 0.3,
SMOTe at 500%, and the application of a k-anonymity function with k = 2.
Additionally, it shows a high classification accuracy, ranging from 90% to 98%,
across various classification techniques.

</details>

### [27] [Dual Explanations via Subgraph Matching for Malware Detection](https://arxiv.org/abs/2504.20904)
*Hossein Shokouhinejad,Roozbeh Razavi-Far,Griffin Higgins,Ali A. Ghorbani*

Main category: cs.CR

TLDR: 提出了一种基于双原型驱动的可解释框架，用于GNN恶意软件检测，结合子图匹配技术提升解释性。


<details>
  <summary>Details</summary>
Motivation: 传统GNN解释方法无法将重要区域与已知恶意或良性行为模式关联，限制了其在安全场景中的实用性。

Method: 引入双解释框架，结合基础解释器和基于子图匹配的SubMatch解释器，为节点分配可解释分数。

Result: 实验表明，该方法在保持高检测性能的同时显著提升了恶意软件分析的可解释性。

Conclusion: 原型驱动的评分机制提供了更行为对齐的解释，增强了GNN在安全领域的实用性。

Abstract: Interpretable malware detection is crucial for understanding harmful
behaviors and building trust in automated security systems. Traditional
explainable methods for Graph Neural Networks (GNNs) often highlight important
regions within a graph but fail to associate them with known benign or
malicious behavioral patterns. This limitation reduces their utility in
security contexts, where alignment with verified prototypes is essential. In
this work, we introduce a novel dual prototype-driven explainable framework
that interprets GNN-based malware detection decisions. This dual explainable
framework integrates a base explainer (a state-of-the-art explainer) with a
novel second-level explainer which is designed by subgraph matching technique,
called SubMatch explainer. The proposed explainer assigns interpretable scores
to nodes based on their association with matched subgraphs, offering a
fine-grained distinction between benign and malicious regions. This
prototype-guided scoring mechanism enables more interpretable, behavior-aligned
explanations. Experimental results demonstrate that our method preserves high
detection performance while significantly improving interpretability in malware
analysis.

</details>

### [28] [GiBy: A Giant-Step Baby-Step Classifier For Anomaly Detection In Industrial Control Systems](https://arxiv.org/abs/2504.20906)
*Sarad Venugopalan,Sridhar Adepu*

Main category: cs.CR

TLDR: 提出一种基于线性化和降维的异常检测方法，用于工业控制系统（ICS），实现毫秒级响应并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 确保工业控制系统的安全运行，通过及时检测异常（攻击、故障等）保护设备和人员安全。

Method: 通过线性化非线性传感器-执行器关系，并利用降维技术降低时间复杂性。

Result: 实验证明该方法能快速（毫秒级）检测异常，并提供其他AI/ML模型无法同时实现的可解释性。

Conclusion: 该方法在异常检测速度和可解释性方面优于现有技术，适用于工业控制系统的安全监控。

Abstract: The continuous monitoring of the interactions between cyber-physical
components of any industrial control system (ICS) is required to secure
automation of the system controls, and to guarantee plant processes are
fail-safe and remain in an acceptably safe state. Safety is achieved by
managing actuation (where electric signals are used to trigger physical
movement), dependent on corresponding sensor readings; used as ground truth in
decision making. Timely detection of anomalies (attacks, faults and
unascertained states) in ICSs is crucial for the safe running of a plant, the
safety of its personnel, and for the safe provision of any services provided.
We propose an anomaly detection method that involves accurate linearization of
the non-linear forms arising from sensor-actuator(s) relationships, primarily
because solving linear models is easier and well understood. Further, the time
complexity of the anomaly detection scenario/problem at hand is lowered using
dimensionality reduction of the actuator(s) in relationship with a sensor. We
accomplish this by using a well-known water treatment testbed as a use case.
Our experiments show millisecond time response to detect anomalies and provide
explainability; that are not simultaneously achieved by other state of the art
AI/ML models with eXplainable AI (XAI) used for the same purpose. Further, we
pin-point the sensor(s) and its actuation state for which anomaly was detected.

</details>

### [29] [Bipartite Randomized Response Mechanism for Local Differential Privacy](https://arxiv.org/abs/2504.20926)
*Shun Zhang,Hai Zhu,Zhili Chen,Neal N. Xiong*

Main category: cs.CR

TLDR: 论文研究了在本地差分隐私（LDP）约束下最大化数据效用的优化问题，提出了一种自适应LDP机制Bipartite Randomized Response（BRR），并通过实验证明其优于现有LDP机制。


<details>
  <summary>Details</summary>
Motivation: 随着数据隐私的重要性增加，LDP成为保护用户隐私的有效手段，但如何在隐私约束下最大化数据效用是一个关键问题。

Method: 提出BRR机制，通过全局视角解决隐私-效用最大化问题，并将其转化为线性规划问题。

Result: BRR在理论和实验上均优于现有LDP机制，适用于多种效用函数和应用场景。

Conclusion: BRR是一种高效且通用的LDP机制，能够在隐私约束下显著提升数据效用。

Abstract: With the increasing importance of data privacy, Local Differential Privacy
(LDP) has recently become a strong measure of privacy for protecting each
user's privacy from data analysts without relying on a trusted third party. In
many cases, both data providers and data analysts hope to maximize the utility
of released data. In this paper, we study the fundamental trade-off formulated
as a constrained optimization problem: maximizing data utility subject to the
constraint of LDP budgets. In particular, the Generalized Randomized Response
(GRR) treats all discrete data equally except for the true data. For this, we
introduce an adaptive LDP mechanism called Bipartite Randomized Response (BRR),
which solves the above privacy-utility maximization problem from the global
standpoint. We prove that for any utility function and any privacy level,
solving the maximization problem is equivalent to confirming how many
high-utility data to be treated equally as the true data on release
probability, the outcome of which gives the optimal randomized response.
Further, solving this linear program can be computationally cheap in theory.
Several examples of utility functions defined by distance metrics and
applications in decision trees and deep learning are presented. The results of
various experiments show that our BRR significantly outperforms the
state-of-the-art LDP mechanisms of both continuous and distributed types.

</details>

### [30] [Conformal-DP: Differential Privacy on Riemannian Manifolds via Conformal Transformation](https://arxiv.org/abs/2504.20941)
*Peilin He,Liou Tang,M. Amin Rahimian,James Joshi*

Main category: cs.CR

TLDR: 提出了一种名为Conformal-DP的新机制，通过共形变换在黎曼流形上均衡局部样本密度，重新定义测地距离，解决了现有流形感知DP方法对数据均匀分布的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有流形感知DP方法假设数据在流形上均匀分布，但实际数据密度不均，导致噪声分布偏差，影响隐私-效用权衡。

Method: 利用共形变换均衡局部样本密度，重新定义测地距离，同时保留流形的内在几何结构。

Result: 理论证明共形因子显式感知数据密度，新机制满足ε-差分隐私，且测地误差上界仅依赖于最大密度比。实验验证了其在均匀和非均匀流形数据上的高效用。

Conclusion: Conformal-DP在保持ε-差分隐私的同时，显著提升了非均匀流形数据的隐私-效用权衡。

Abstract: Differential Privacy (DP) has been established as a safeguard for private
data sharing by adding perturbations to information release. Prior research on
DP has extended beyond data in the flat Euclidean space and addressed data on
curved manifolds, e.g., diffusion tensor MRI, social networks, or organ shape
analysis, by adding perturbations along geodesic distances. However, existing
manifold-aware DP methods rely on the assumption that samples are uniformly
distributed across the manifold. In reality, data densities vary, leading to a
biased noise imbalance across manifold regions, weakening the privacy-utility
trade-offs. To address this gap, we propose a novel mechanism: Conformal-DP,
utilizing conformal transformations on the Riemannian manifold to equalize
local sample density and to redefine geodesic distances accordingly while
preserving the intrinsic geometry of the manifold. Our theoretical analysis
yields two main results. First, we prove that the conformal factor computed
from local kernel-density estimates is explicitly data-density-aware; Second,
under the conformal metric, the mechanism satisfies $ \varepsilon
$-differential privacy on any complete Riemannian manifold and admits a
closed-form upper bound on the expected geodesic error that depends only on the
maximal density ratio, not on global curvatureof the manifold. Our experimental
results validate that the mechanism achieves high utility while providing the $
\varepsilon $-DP guarantee for both homogeneous and especially heterogeneous
manifold data.

</details>

### [31] [ACE: A Security Architecture for LLM-Integrated App Systems](https://arxiv.org/abs/2504.20984)
*Evan Li,Tushin Mallick,Evan Rose,William Robertson,Alina Oprea,Cristina Nita-Rotaru*

Main category: cs.CR

TLDR: 论文提出了一种新的安全架构ACE，用于保护LLM集成应用系统免受恶意应用攻击，确保规划和执行的完整性。


<details>
  <summary>Details</summary>
Motivation: LLM集成系统存在新的攻击向量，可能导致规划或执行的完整性破坏、可用性中断或隐私泄露。

Method: 提出Abstract-Concrete-Execute (ACE)架构，将规划分为抽象计划和具体计划两阶段，并通过静态分析和执行屏障确保安全。

Result: 实验证明ACE能抵御INJECAGENT基准和新攻击，确保系统安全。

Conclusion: ACE是提升LLM系统安全性的重要进展，适用于信任度不同的系统设施。

Abstract: LLM-integrated app systems extend the utility of Large Language Models (LLMs)
with third-party apps that are invoked by a system LLM using interleaved
planning and execution phases to answer user queries. These systems introduce
new attack vectors where malicious apps can cause integrity violation of
planning or execution, availability breakdown, or privacy compromise during
execution.
  In this work, we identify new attacks impacting the integrity of planning, as
well as the integrity and availability of execution in LLM-integrated apps, and
demonstrate them against IsolateGPT, a recent solution designed to mitigate
attacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new
secure architecture for LLM-integrated app systems that provides security
guarantees for system planning and execution. Specifically, ACE decouples
planning into two phases by first creating an abstract execution plan using
only trusted information, and then mapping the abstract plan to a concrete plan
using installed system apps. We verify that the plans generated by our system
satisfy user-specified secure information flow constraints via static analysis
on the structured plan output. During execution, ACE enforces data and
capability barriers between apps, and ensures that the execution is conducted
according to the trusted abstract plan. We show experimentally that our system
is secure against attacks from the INJECAGENT benchmark, a standard benchmark
for control flow integrity in the face of indirect prompt injection attacks,
and our newly introduced attacks. Our architecture represents a significant
advancement towards hardening LLM-based systems containing system facilities of
varying levels of trustworthiness.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [A constraints-based approach to fully interpretable neural networks for detecting learner behaviors](https://arxiv.org/abs/2504.20055)
*Juan D. Pinto,Luc Paquette*

Main category: cs.LG

TLDR: 提出了一种设计上可解释的神经网络行为检测模型，通过约束实现模型的可解释性，并在检测“钻系统空子”行为中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着复杂机器学习模型在教育中的应用增加，其可解释性问题引发关注，需要开发既忠实于模型内部机制又易于人类理解的解释技术。

Method: 设计了一种完全可解释的神经网络模型，通过约束简化推理过程并使其更接近人类对任务的理解，用于检测“钻系统空子”行为。

Result: 模型成功学习了“钻系统空子”行为的模式，并提供了完全可解释的证据。

Conclusion: 该方法为可解释性提供了新思路，建议通过人类基础方法评估解释性。

Abstract: The increasing use of complex machine learning models in education has led to
concerns about their interpretability, which in turn has spurred interest in
developing explainability techniques that are both faithful to the model's
inner workings and intelligible to human end-users. In this paper, we describe
a novel approach to creating a neural-network-based behavior detection model
that is interpretable by design. Our model is fully interpretable, meaning that
the parameters we extract for our explanations have a clear interpretation,
fully capture the model's learned knowledge about the learner behavior of
interest, and can be used to create explanations that are both faithful and
intelligible. We achieve this by implementing a series of constraints to the
model that both simplify its inference process and bring it closer to a human
conception of the task at hand. We train the model to detect gaming-the-system
behavior, evaluate its performance on this task, and compare its learned
patterns to those identified by human experts. Our results show that the model
is successfully able to learn patterns indicative of gaming-the-system behavior
while providing evidence for fully interpretable explanations. We discuss the
implications of our approach and suggest ways to evaluate explainability using
a human-grounded approach.

</details>

### [33] [A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives](https://arxiv.org/abs/2504.20069)
*Junhong Lai,Jiyu Wei,Lin Yao,Yueming Wang*

Main category: cs.LG

TLDR: 综述近期EEG基础模型（EEG-FMs）的发展，包括架构、预训练策略、数据集等，并探讨挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: EEG信号对理解脑活动和诊断神经疾病至关重要，EEG-FMs在EEG数据处理中潜力巨大。

Method: 讨论多种EEG-FMs的架构、预训练策略及数据集。

Result: EEG-FMs在EEG分析中展现出显著潜力。

Conclusion: 综述为研究者提供了EEG-FMs的全面概述，并指出未来研究方向。

Abstract: Electroencephalogram (EEG) signals play a crucial role in understanding brain
activity and diagnosing neurological disorders. This review focuses on the
recent development of EEG foundation models(EEG-FMs), which have shown great
potential in processing and analyzing EEG data. We discuss various EEG-FMs,
including their architectures, pre-training strategies, their pre-training and
downstream datasets and other details. The review also highlights the
challenges and future directions in this field, aiming to provide a
comprehensive overview for researchers and practitioners interested in EEG
analysis and related EEG-FMs.

</details>

### [34] [Improving Deep Knowledge Tracing via Gated Architectures and Adaptive Optimization](https://arxiv.org/abs/2504.20070)
*Altun Shukurlu*

Main category: cs.LG

TLDR: 论文改进了深度知识追踪（DKT）模型，使用LSTM和GRU增强长时依赖捕捉能力，并通过PyTorch重新实现以提高可扩展性和可复现性。实验表明优化算法和架构改进显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 原始DKT模型基于Lua的Torch框架，限制了可扩展性和可复现性，且标准RNN在捕捉长时依赖和梯度消失问题上表现不佳。

Method: 采用LSTM和GRU改进模型架构，并使用PyTorch重新实现；比较了多种优化算法（SGD、RMSProp、Adagrad、Adam、AdamW）的性能。

Result: 在Synthetic-5和Khan Academy数据集上，LSTM和GRU比标准RNN表现更好，自适应优化器（如Adam和AdamW）在收敛速度和预测准确性上优于SGD。

Conclusion: 改进后的DKT模型在性能和稳定性上显著提升，开源PyTorch实现为未来研究提供了可扩展和可复现的基础。

Abstract: Deep Knowledge Tracing (DKT) models student learning behavior by using
Recurrent Neural Networks (RNNs) to predict future performance based on
historical interaction data. However, the original implementation relied on
standard RNNs in the Lua-based Torch framework, which limited extensibility and
reproducibility. In this work, we revisit the DKT model from two perspectives:
architectural improvements and optimization efficiency. First, we enhance the
model using gated recurrent units, specifically Long Short-Term Memory (LSTM)
networks and Gated Recurrent Units (GRU), which better capture long-term
dependencies and help mitigate vanishing gradient issues. Second, we
re-implement DKT using the PyTorch framework, enabling a modular and accessible
infrastructure compatible with modern deep learning workflows. We also
benchmark several optimization algorithms SGD, RMSProp, Adagrad, Adam, and
AdamW to evaluate their impact on convergence speed and predictive accuracy in
educational modeling tasks. Experiments on the Synthetic-5 and Khan Academy
datasets show that GRUs and LSTMs achieve higher accuracy and improved training
stability compared to basic RNNs, while adaptive optimizers such as Adam and
AdamW consistently outperform SGD in both early-stage learning and final model
performance. Our open-source PyTorch implementation provides a reproducible and
extensible foundation for future research in neural knowledge tracing and
personalized learning systems.

</details>

### [35] [RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2504.20073)
*Zihan Wang,Kangrui Wang,Qineng Wang,Pingyue Zhang,Linjie Li,Zhengyuan Yang,Kefan Yu,Minh Nhat Nguyen,Licheng Liu,Eli Gottlieb,Monica Lam,Yiping Lu,Kyunghyun Cho,Jiajun Wu,Li Fei-Fei,Lijuan Wang,Yejin Choi,Manling Li*

Main category: cs.LG

TLDR: 论文提出了StarPO框架和RAGEN系统，用于训练和评估LLM代理，解决了多轮RL训练中的Echo Trap问题，并探讨了RL rollout的优化方法。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）作为交互代理在长时决策和随机环境反馈中的挑战，尤其是多轮RL训练的不足。

Method: 提出StarPO框架和RAGEN系统，通过轨迹级RL训练代理，并引入StarPO-S变体解决Echo Trap问题。

Result: 发现Echo Trap现象，提出优化RL rollout的方法，并强调细粒度奖励信号对代理推理的重要性。

Conclusion: StarPO和RAGEN为LLM代理的RL训练提供了有效框架，但需细粒度奖励信号以促进推理能力。

Abstract: Training large language models (LLMs) as interactive agents presents unique
challenges including long-horizon decision making and interacting with
stochastic environment feedback. While reinforcement learning (RL) has enabled
progress in static tasks, multi-turn agent RL training remains underexplored.
We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a
general framework for trajectory-level agent RL, and introduce RAGEN, a modular
system for training and evaluating LLM agents. Our study on three stylized
environments reveals three core findings. First, our agent RL training shows a
recurring mode of Echo Trap where reward variance cliffs and gradient spikes;
we address this with StarPO-S, a stabilized variant with trajectory filtering,
critic incorporation, and decoupled clipping. Second, we find the shaping of RL
rollouts would benefit from diverse initial states, medium interaction
granularity and more frequent sampling. Third, we show that without
fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge
through multi-turn RL and they may show shallow strategies or hallucinated
thoughts. Code and environments are available at
https://github.com/RAGEN-AI/RAGEN.

</details>

### [36] [Low-Rank Matrix Approximation for Neural Network Compression](https://arxiv.org/abs/2504.20078)
*Kalyan Cherukuri,Aarav Lala*

Main category: cs.LG

TLDR: 论文提出了一种自适应秩奇异值分解（ARSVD）方法，动态调整全连接层的秩以减少内存和计算需求，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNNs）常受限于高内存和计算需求，传统SVD压缩方法采用固定秩减少，无法灵活平衡压缩与性能。

Method: 通过能量分布动态选择每层的秩，在能量消耗阈值内最大化利用能量并最小化精度损失。

Result: 在MNIST、CIFAR-10和CIFAR-100数据集上测试，ARSVD实现了显著模型压缩且未降低分类精度。

Conclusion: ARSVD在资源受限场景下有效平衡了压缩与性能，优于传统静态方法。

Abstract: Deep Neural Networks (DNNs) are often constrained by their large memories and
computational restrictions. In this paper, we introduce a novel adaptive-rank
Singular Value Decomposition (ARSVD) that dynamically chooses the rank increase
of the fully connected layers below a certain threshold in energy expenditure.
Unlike conventional SVD compression methods that apply a fixed rank reduction
in all layers, our ARSVD method uses energy distribution to adaptively select
rank per layer while retaining accuracy. This is done for each layer in an
effort to use as much energy as possible while maintaining the lowest accuracy
loss. Such accuracy-adaptive approaches outperform traditional static rank
reduction methods by providing an improved balance between compression and
model performance. We first train a simple Multi-Layer Perceptron (MLP) on the
MNIST, CIFAR-10, and CIFAR-100 dataset and evaluate its performance using
accuracy and F1-score. After applying ARSVD, our results demonstrate that the
technique can achieve substantial model compression without compromising
classification accuracy. These results illustrate the usefulness of ARSVD in
computing scenarios where both computational and memory resources are scarce.

</details>

### [37] [FX-DARTS: Designing Topology-unconstrained Architectures with Differentiable Architecture Search and Entropy-based Super-network Shrinking](https://arxiv.org/abs/2504.20079)
*Xuan Rao,Bo Zhao,Derong Liu,Cesare Alippi*

Main category: cs.LG

TLDR: FX-DARTS通过消除DARTS中的强先验约束，提出了一种基于熵的超网络收缩框架（ESS），以增强架构搜索的灵活性，同时保持稳定性。


<details>
  <summary>Details</summary>
Motivation: 强先验约束限制了DARTS的架构搜索空间，阻碍了Auto-ML的发展和更强大神经网络的探索。

Method: 提出FX-DARTS方法，通过消除单元拓扑限制和改进超网络的离散化机制，利用ESS框架解决约束消除带来的挑战。

Result: 实验表明，FX-DARTS能在单一搜索过程中探索性能与计算复杂度平衡的神经网络架构。

Conclusion: FX-DARTS成功减少了先验约束，提升了架构搜索的灵活性和稳定性，为Auto-ML提供了更广阔的发展空间。

Abstract: Strong priors are imposed on the search space of Differentiable Architecture
Search (DARTS), such that cells of the same type share the same topological
structure and each intermediate node retains two operators from distinct nodes.
While these priors reduce optimization difficulties and improve the
applicability of searched architectures, they hinder the subsequent development
of automated machine learning (Auto-ML) and prevent the optimization algorithm
from exploring more powerful neural networks through improved architectural
flexibility. This paper aims to reduce these prior constraints by eliminating
restrictions on cell topology and modifying the discretization mechanism for
super-networks. Specifically, the Flexible DARTS (FX-DARTS) method, which
leverages an Entropy-based Super-Network Shrinking (ESS) framework, is
presented to address the challenges arising from the elimination of prior
constraints. Notably, FX-DARTS enables the derivation of neural architectures
without strict prior rules while maintaining the stability in the enlarged
search space. Experimental results on image classification benchmarks
demonstrate that FX-DARTS is capable of exploring a set of neural architectures
with competitive trade-offs between performance and computational complexity
within a single search procedure.

</details>

### [38] [DNAD: Differentiable Neural Architecture Distillation](https://arxiv.org/abs/2504.20080)
*Xuan Rao,Bo Zhao,Derong Liu*

Main category: cs.LG

TLDR: 论文提出了DNAD算法，通过结合搜索删除和模仿搜索两种方法，设计高效神经网络，平衡模型性能和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 设计高效神经网络，平衡模型性能和计算复杂度。

Method: 开发了DNAD算法，包括搜索删除（SNPS）和模仿搜索（结合知识蒸馏）。SNPS基于DARTS框架，动态生成灵活的Pareto最优架构。

Result: 在CIFAR-10和ImageNet上，DNAD和SNPS均能生成参数和计算量更少、性能相当的架构。DNAD在ImageNet上达到23.7%的top-1错误率。

Conclusion: DNAD算法通过结合搜索删除和模仿搜索，显著提升了神经网络设计的效率和性能。

Abstract: To meet the demand for designing efficient neural networks with appropriate
trade-offs between model performance (e.g., classification accuracy) and
computational complexity, the differentiable neural architecture distillation
(DNAD) algorithm is developed based on two cores, namely search by deleting and
search by imitating. Primarily, to derive neural architectures in a space where
cells of the same type no longer share the same topology, the super-network
progressive shrinking (SNPS) algorithm is developed based on the framework of
differentiable architecture search (DARTS), i.e., search by deleting. Unlike
conventional DARTS-based approaches which yield neural architectures with
simple structures and derive only one architecture during the search procedure,
SNPS is able to derive a Pareto-optimal set of architectures with flexible
structures by forcing the dynamic super-network shrink from a dense structure
to a sparse one progressively. Furthermore, since knowledge distillation (KD)
has shown great effectiveness to train a compact network with the assistance of
an over-parameterized model, we integrate SNPS with KD to formulate the DNAD
algorithm, i.e., search by imitating. By minimizing behavioral differences
between the super-network and teacher network, the over-fitting of one-level
DARTS is avoided and well-performed neural architectures are derived.
Experiments on CIFAR-10 and ImageNet classification tasks demonstrate that both
SNPS and DNAD are able to derive a set of architectures which achieve similar
or lower error rates with fewer parameters and FLOPs. Particularly, DNAD
achieves the top-1 error rate of 23.7% on ImageNet classification with a model
of 6.0M parameters and 598M FLOPs, which outperforms most DARTS-based methods.

</details>

### [39] [Towards Practical Second-Order Optimizers in Deep Learning: Insights from Fisher Information Analysis](https://arxiv.org/abs/2504.20096)
*Damien Martins Gomes*

Main category: cs.LG

TLDR: AdaFisher是一种新型自适应二阶优化器，通过近似Fisher信息矩阵的块Kronecker对角矩阵来预条件梯度，旨在平衡二阶方法的收敛性与计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管二阶优化算法在收敛性上优于一阶方法（如Adam和SGD），但其高计算成本限制了在深度神经网络训练中的实用性。AdaFisher试图解决这一矛盾。

Method: AdaFisher利用Fisher信息矩阵的块Kronecker对角近似来预条件梯度，结合了二阶方法的优势与计算效率。

Result: AdaFisher在图像分类和语言建模任务中表现出色，稳定且鲁棒，且在准确性和收敛速度上优于现有优化器。

Conclusion: AdaFisher成功弥合了二阶方法的收敛优势与计算效率之间的差距，为深度神经网络训练提供了实用且高效的优化方案。

Abstract: First-order optimization methods remain the standard for training deep neural
networks (DNNs). Optimizers like Adam incorporate limited curvature information
by preconditioning the stochastic gradient with a diagonal matrix. Despite the
widespread adoption of first-order methods, second-order optimization
algorithms often exhibit superior convergence compared to methods like Adam and
SGD. However, their practicality in training DNNs is still limited by a
significantly higher per-iteration computational cost compared to first-order
methods. In this thesis, we present AdaFisher, a novel adaptive second-order
optimizer that leverages a diagonal block-Kronecker approximation of the Fisher
information matrix to adaptively precondition gradients. AdaFisher aims to
bridge the gap between the improved convergence and generalization of
second-order methods and the computational efficiency needed for training DNNs.
Despite the traditionally slower speed of second-order optimizers, AdaFisher is
effective for tasks such as image classification and language modeling,
exhibiting remarkable stability and robustness during hyperparameter tuning. We
demonstrate that AdaFisher outperforms state-of-the-art optimizers in both
accuracy and convergence speed. The code is available from
https://github.com/AtlasAnalyticsLab/AdaFisher.

</details>

### [40] [Decoding Latent Spaces: Assessing the Interpretability of Time Series Foundation Models for Visual Analytics](https://arxiv.org/abs/2504.20099)
*Inmaculada Santamaria-Valenzuela,Victor Rodriguez-Fernandez,Javier Huertas-Tato,Jong Hyuk Park,David Camacho*

Main category: cs.LG

TLDR: 研究了时间序列基础模型（如MOMENT）潜在空间的可解释性，发现其虽在性能上有提升，但潜在空间的解释性仍需改进。


<details>
  <summary>Details</summary>
Motivation: 探索时间序列基础模型潜在空间的可解释性，以支持可视化分析任务。

Method: 评估MOMENT模型家族在五个数据集上的表现，分析其潜在空间结构，并验证微调对嵌入空间清晰度的影响。

Result: 微调后性能提升，但潜在空间的可解释性改进有限，需进一步优化。

Conclusion: 时间序列基础模型虽高效，但潜在空间解释性需额外方法改进，如投影技术或损失函数调整。

Abstract: The present study explores the interpretability of latent spaces produced by
time series foundation models, focusing on their potential for visual analysis
tasks. Specifically, we evaluate the MOMENT family of models, a set of
transformer-based, pre-trained architectures for multivariate time series tasks
such as: imputation, prediction, classification, and anomaly detection. We
evaluate the capacity of these models on five datasets to capture the
underlying structures in time series data within their latent space projection
and validate whether fine tuning improves the clarity of the resulting
embedding spaces. Notable performance improvements in terms of loss reduction
were observed after fine tuning. Visual analysis shows limited improvement in
the interpretability of the embeddings, requiring further work. Results suggest
that, although Time Series Foundation Models such as MOMENT are robust, their
latent spaces may require additional methodological refinements to be
adequately interpreted, such as alternative projection techniques, loss
functions, or data preprocessing strategies. Despite the limitations of MOMENT,
foundation models supose a big reduction in execution time and so a great
advance for interactive visual analytics.

</details>

### [41] [HyboWaveNet: Hyperbolic Graph Neural Networks with Multi-Scale Wavelet Transform for Protein-Protein Interaction Prediction](https://arxiv.org/abs/2504.20102)
*Qingzhi Yu,Shuai Yan,Wenfeng Dai,Xiang Cheng*

Main category: cs.LG

TLDR: HyboWaveNet结合双曲图神经网络和多尺度图小波变换，提升蛋白质相互作用预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏因果解释且难以捕捉蛋白质的多尺度动态交互模式。

Method: 提出HyboWaveNet框架，利用双曲距离度量模拟生物分子层次关系，结合多尺度图小波变换提取特征。

Result: 实验显示HyboWaveNet优于现有方法，小波变换模块提升预测性能和泛化能力。

Conclusion: 该工作将几何深度学习与信号处理结合，为复杂生物系统分析提供新思路。

Abstract: Protein-protein interactions (PPIs) are fundamental for deciphering cellular
functions,disease pathways,and drug discovery.Although existing neural networks
and machine learning methods have achieved high accuracy in PPI
prediction,their black-box nature leads to a lack of causal interpretation of
the prediction results and difficulty in capturing hierarchical geometries and
multi-scale dynamic interaction patterns among proteins.To address these
challenges, we propose HyboWaveNet,a novel deep learning framework that
collaborates with hyperbolic graphical neural networks (HGNNs) and multiscale
graphical wavelet transform for robust PPI prediction. Mapping protein features
to Lorentz space simulates hierarchical topological relationships among
biomolecules via a hyperbolic distance metric,enabling node feature
representations that better fit biological a priori.HyboWaveNet inherently
simulates hierarchical and scale-free biological relationships, while the
integration of wavelet transforms enables adaptive extraction of local and
global interaction features across different resolutions. Our framework
generates node feature representations via a graph neural network under the
Lorenz model and generates pairs of positive samples under multiple different
views for comparative learning, followed by further feature extraction via
multi-scale graph wavelet transforms to predict potential PPIs. Experiments on
public datasets show that HyboWaveNet improves over both existing
state-of-the-art methods. We also demonstrate through ablation experimental
studies that the multi-scale graph wavelet transform module improves the
predictive performance and generalization ability of HyboWaveNet. This work
links geometric deep learning and signal processing to advance PPI prediction,
providing a principled approach for analyzing complex biological systems

</details>

### [42] [Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors](https://arxiv.org/abs/2504.20106)
*Ren-Wei Liang,Chin-Ting Hsu,Chan-Hung Yu,Saransh Agrawal,Shih-Cheng Huang,Shang-Tse Chen,Kuan-Hao Huang,Shao-Hua Sun*

Main category: cs.LG

TLDR: 论文提出了一种名为Preference Vector的新框架，通过分离训练和动态合并偏好向量，解决了现有方法在平衡LLM帮助性和无害性时的性能冲突和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如RLHF和DPO）在平衡LLM的帮助性和无害性时存在性能冲突、可控性差和扩展性不足的问题。

Method: 训练独立模型以提取偏好向量，并在测试时动态合并，实现模块化的偏好调整。

Result: 实验表明，Preference Vector框架在不增加保守性的情况下提高了帮助性，支持平滑的偏好权衡控制和多偏好对齐。

Conclusion: Preference Vector框架为LLM的偏好管理提供了一种灵活、可扩展的解决方案。

Abstract: Ensuring that large language models (LLMs) are both helpful and harmless is a
critical challenge, as overly strict constraints can lead to excessive
refusals, while permissive models risk generating harmful content. Existing
approaches, such as reinforcement learning from human feedback (RLHF) and
direct preference optimization (DPO), attempt to balance these trade-offs but
suffer from performance conflicts, limited controllability, and poor
extendability. To address these issues, we propose Preference Vector, a novel
framework inspired by task arithmetic. Instead of optimizing multiple
preferences within a single objective, we train separate models on individual
preferences, extract behavior shifts as preference vectors, and dynamically
merge them at test time. This modular approach enables fine-grained,
user-controllable preference adjustments and facilitates seamless integration
of new preferences without retraining. Experiments show that our proposed
Preference Vector framework improves helpfulness without excessive
conservatism, allows smooth control over preference trade-offs, and supports
scalable multi-preference alignment.

</details>

### [43] [Swapped Logit Distillation via Bi-level Teacher Alignment](https://arxiv.org/abs/2504.20108)
*Stephen Ekaputra Limantoro,Jhe-Hao Lin,Chih-Yu Wang,Yi-Lung Tsai,Hong-Han Shuai,Ching-Chun Huang,Wen-Huang Cheng*

Main category: cs.LG

TLDR: 提出了一种基于对数交换的知识蒸馏方法（SLD），通过交换教师和学生输出的对数，提升知识蒸馏的效果。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法直接将教师网络的原始分布传递给学生网络，可能导致错误预测。SLD旨在解决这一问题。

Method: 采用对数交换处理方案，将教师和学生输出转化为两个教师，并引入损失调度以优化两者的对齐。

Result: 在图像分类任务中，SLD表现优于现有最先进方法。

Conclusion: SLD通过交换对数和损失调度，有效提升了知识蒸馏的性能。

Abstract: Knowledge distillation (KD) compresses the network capacity by transferring
knowledge from a large (teacher) network to a smaller one (student). It has
been mainstream that the teacher directly transfers knowledge to the student
with its original distribution, which can possibly lead to incorrect
predictions. In this article, we propose a logit-based distillation via swapped
logit processing, namely Swapped Logit Distillation (SLD). SLD is proposed
under two assumptions: (1) the wrong prediction occurs when the prediction
label confidence is not the maximum; (2) the "natural" limit of probability
remains uncertain as the best value addition to the target cannot be
determined. To address these issues, we propose a swapped logit processing
scheme. Through this approach, we find that the swap method can be effectively
extended to teacher and student outputs, transforming into two teachers. We
further introduce loss scheduling to boost the performance of two teachers'
alignment. Extensive experiments on image classification tasks demonstrate that
SLD consistently performs best among previous state-of-the-art methods.

</details>

### [44] [Attention to Detail: Fine-Scale Feature Preservation-Oriented Geometric Pre-training for AI-Driven Surrogate Modeling](https://arxiv.org/abs/2504.20110)
*Yu-hsuan Chen,Jing Bi,Cyril Ngo Ngoc,Victor Oancea,Jonathan Cagan,Levent Burak Kara*

Main category: cs.LG

TLDR: 本文提出了一种自监督的几何表示学习方法，用于从非参数化3D模型中捕捉精细几何特征，解决了传统方法在保留细尺度几何细节方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 由于标记CAD到模拟数据集的稀缺性，传统基于物理的模拟方法在3D设计和制造中效率低下，需要一种能够捕捉精细几何特征的数据驱动方法。

Method: 提出了一种自监督学习方法，通过几何重建损失学习潜在空间嵌入，并采用近零水平采样和批自适应注意力加权损失函数来增强几何特征编码。

Result: 在结构力学案例研究中验证了方法的有效性，能够准确捕捉设计特征并实现少样本物理预测。

Conclusion: 该方法在几何与物理表示之间架起桥梁，为数据稀缺场景下的代理建模提供了有效解决方案。

Abstract: AI-driven surrogate modeling has become an increasingly effective alternative
to physics-based simulations for 3D design, analysis, and manufacturing. These
models leverage data-driven methods to predict physical quantities
traditionally requiring computationally expensive simulations. However, the
scarcity of labeled CAD-to-simulation datasets has driven recent advancements
in self-supervised and foundation models, where geometric representation
learning is performed offline and later fine-tuned for specific downstream
tasks. While these approaches have shown promise, their effectiveness is
limited in applications requiring fine-scale geometric detail preservation.
This work introduces a self-supervised geometric representation learning method
designed to capture fine-scale geometric features from non-parametric 3D
models. Unlike traditional end-to-end surrogate models, this approach decouples
geometric feature extraction from downstream physics tasks, learning a latent
space embedding guided by geometric reconstruction losses. Key elements include
the essential use of near-zero level sampling and the innovative batch-adaptive
attention-weighted loss function, which enhance the encoding of intricate
design features. The proposed method is validated through case studies in
structural mechanics, demonstrating strong performance in capturing design
features and enabling accurate few-shot physics predictions. Comparisons with
traditional parametric surrogate modeling highlight its potential to bridge the
gap between geometric and physics-based representations, providing an effective
solution for surrogate modeling in data-scarce scenarios.

</details>

### [45] [Supervised Pretraining for Material Property Prediction](https://arxiv.org/abs/2504.20112)
*Chowdhury Mohammad Abid Rahman,Aldo H. Romero,Prashnna K. Gyawali*

Main category: cs.LG

TLDR: 论文提出了一种基于监督预训练的方法，利用替代标签改进材料性能预测，并通过图增强技术提升模型鲁棒性，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 材料性能的准确预测对发现新型功能材料至关重要，但现有深度学习方法依赖大量标注数据，成本高昂。自监督学习（SSL）提供了一种替代方案，但如何进一步优化预训练效果仍需探索。

Method: 提出监督预训练策略，利用类别信息作为替代标签；引入图增强技术，通过噪声注入提升模型鲁棒性；在六种材料性能预测任务上微调模型。

Result: 模型性能显著提升，平均绝对误差（MAE）改善2%至6.67%，为材料性能预测树立了新基准。

Conclusion: 本研究首次探索了在材料性能预测中使用替代标签的监督预训练方法，推动了该领域的方法和应用发展。

Abstract: Accurate prediction of material properties facilitates the discovery of novel
materials with tailored functionalities. Deep learning models have recently
shown superior accuracy and flexibility in capturing structure-property
relationships. However, these models often rely on supervised learning, which
requires large, well-annotated datasets an expensive and time-consuming
process. Self-supervised learning (SSL) offers a promising alternative by
pretraining on large, unlabeled datasets to develop foundation models that can
be fine-tuned for material property prediction. In this work, we propose
supervised pretraining, where available class information serves as surrogate
labels to guide learning, even when downstream tasks involve unrelated material
properties. We evaluate this strategy on two state-of-the-art SSL models and
introduce a novel framework for supervised pretraining. To further enhance
representation learning, we propose a graph-based augmentation technique that
injects noise to improve robustness without structurally deforming material
graphs. The resulting foundation models are fine-tuned for six challenging
material property predictions, achieving significant performance gains over
baselines, ranging from 2% to 6.67% improvement in mean absolute error (MAE)
and establishing a new benchmark in material property prediction. This study
represents the first exploration of supervised pertaining with surrogate labels
in material property prediction, advancing methodology and application in the
field.

</details>

### [46] [Benchmarking Transferability: A Framework for Fair and Robust Evaluation](https://arxiv.org/abs/2504.20121)
*Alireza Kazemi,Helia Rezvani,Mahsa Baktashmotlagh*

Main category: cs.LG

TLDR: 本文提出了一种系统评估迁移性分数的基准框架，发现现有评估方法可能未能全面反映各方法的优劣，并强调了标准化评估协议的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有迁移性分数评估方法的可靠性和实用性尚未明确，亟需系统化评估框架。

Method: 引入全面的基准测试框架，通过多样化实验评估不同迁移性分数指标的表现。

Result: 实验显示不同指标在不同场景下表现各异，提出的新指标在特定实验设置中提升了3.5%。

Conclusion: 标准化评估协议对提升迁移性分数的可靠性和模型选择至关重要。

Abstract: Transferability scores aim to quantify how well a model trained on one domain
generalizes to a target domain. Despite numerous methods proposed for measuring
transferability, their reliability and practical usefulness remain
inconclusive, often due to differing experimental setups, datasets, and
assumptions. In this paper, we introduce a comprehensive benchmarking framework
designed to systematically evaluate transferability scores across diverse
settings. Through extensive experiments, we observe variations in how different
metrics perform under various scenarios, suggesting that current evaluation
practices may not fully capture each method's strengths and limitations. Our
findings underscore the value of standardized assessment protocols, paving the
way for more reliable transferability measures and better-informed model
selection in cross-domain applications. Additionally, we achieved a 3.5\%
improvement using our proposed metric for the head-training fine-tuning
experimental setup. Our code is available in this repository:
https://github.com/alizkzm/pert_robust_platform.

</details>

### [47] [LZ Penalty: An information-theoretic repetition penalty for autoregressive language models](https://arxiv.org/abs/2504.20131)
*Antonio A. Ginart,Naveen Kodali,Jason Lee,Caiming Xiong,Silvio Savarese,John R. Emmons*

Main category: cs.LG

TLDR: LZ惩罚是一种针对自回归语言模型的专用惩罚方法，旨在减少退化重复而不损失模型能力。


<details>
  <summary>Details</summary>
Motivation: 解决自回归语言模型中常见的退化重复问题，同时保持模型性能。

Method: 基于LZ77无损压缩算法的编码长度设计惩罚机制，通过预测-压缩对偶性解释其作用。

Result: LZ惩罚使开源推理模型在贪婪解码（温度为零）下无退化重复，优于行业标准的频率和重复惩罚方法。

Conclusion: LZ惩罚是一种有效的解决方案，能显著减少退化重复，同时保持模型能力。

Abstract: We introduce the LZ penalty, a penalty specialized for reducing degenerate
repetitions in autoregressive language models without loss of capability. The
penalty is based on the codelengths in the LZ77 universal lossless compression
algorithm. Through the lens of the prediction-compression duality, decoding the
LZ penalty has the interpretation of sampling from the residual distribution
after removing the information that is highly compressible. We demonstrate the
LZ penalty enables state-of-the-art open-source reasoning models to operate
with greedy (temperature zero) decoding without loss of capability and without
instances of degenerate repetition. Both the industry-standard frequency
penalty and repetition penalty are ineffective, incurring degenerate repetition
rates of up to 4%.

</details>

### [48] [Causal Identification in Time Series Models](https://arxiv.org/abs/2504.20172)
*Erik Jahn,Karthik Karnik,Leonard J. Schulman*

Main category: cs.LG

TLDR: 本文研究了因果识别算法在具有潜在混杂因素的因果时间序列图上的适用性，并首次给出了仅依赖于每时间步变量数量和最大时间滞后的边界。


<details>
  <summary>Details</summary>
Motivation: 分析因果识别算法在无限时间步长的因果时间序列图中的适用性，解决因果效应可识别性问题。

Method: 通过分析时间序列图的固定大小片段，证明其足以决定因果效应的可识别性。

Result: 首次给出了仅依赖于每时间步变量数量和最大时间滞后的边界，证明固定大小片段足以决定可识别性。

Conclusion: 即使时间间隔无界，固定大小的时间序列图片段也足以决定因果效应的可识别性。

Abstract: In this paper, we analyze the applicability of the Causal Identification
algorithm to causal time series graphs with latent confounders. Since these
graphs extend over infinitely many time steps, deciding whether causal effects
across arbitrary time intervals are identifiable appears to require computation
on graph segments of unbounded size. Even for deciding the identifiability of
intervention effects on variables that are close in time, no bound is known on
how many time steps in the past need to be considered. We give a first bound of
this kind that only depends on the number of variables per time step and the
maximum time lag of any direct or latent causal effect. More generally, we show
that applying the Causal Identification algorithm to a constant-size segment of
the time series graph is sufficient to decide identifiability of causal
effects, even across unbounded time intervals.

</details>

### [49] [AI Recommendation Systems for Lane-Changing Using Adherence-Aware Reinforcement Learning](https://arxiv.org/abs/2504.20187)
*Weihao Sun,Heeseung Bang,Andreas A. Malikopoulos*

Main category: cs.LG

TLDR: 提出了一种基于强化学习的车道变更推荐方法，考虑了人类驾驶员的部分遵从性，以提高单车的行驶效率。


<details>
  <summary>Details</summary>
Motivation: 在半自动驾驶环境中，人类驾驶员对推荐动作的遵从性不完全，影响了行驶效率。

Method: 采用马尔可夫决策过程框架和基于深度Q网络的强化学习方法，结合驾驶员遵从性建模。

Result: 在CARLA驾驶环境中进行了评估，验证了方法的有效性。

Conclusion: 该方法能够有效提升半自动驾驶环境中的行驶效率。

Abstract: In this paper, we present an adherence-aware reinforcement learning (RL)
approach aimed at seeking optimal lane-changing recommendations within a
semi-autonomous driving environment to enhance a single vehicle's travel
efficiency. The problem is framed within a Markov decision process setting and
is addressed through an adherence-aware deep Q network, which takes into
account the partial compliance of human drivers with the recommended actions.
This approach is evaluated within CARLA's driving environment under realistic
scenarios.

</details>

### [50] [ProFi-Net: Prototype-based Feature Attention with Curriculum Augmentation for WiFi-based Gesture Recognition](https://arxiv.org/abs/2504.20193)
*Zhe Cui,Shuxian Zhang,Kangzhi Lou,Le-Nam Tran*

Main category: cs.LG

TLDR: ProFi-Net是一种新型的少样本学习框架，用于WiFi手势识别，通过原型度量学习和特征级注意力机制解决数据不足和特征稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 解决WiFi手势识别中训练数据有限和特征表示稀疏的挑战。

Method: 采用原型度量学习架构，结合特征级注意力机制动态优化欧氏距离，并引入课程式数据增强策略。

Result: 在多种真实环境中显著优于传统原型网络和其他少样本学习方法。

Conclusion: ProFi-Net在分类准确性和训练效率上表现出色，具有更好的泛化能力和抗过拟合能力。

Abstract: This paper presents ProFi-Net, a novel few-shot learning framework for
WiFi-based gesture recognition that overcomes the challenges of limited
training data and sparse feature representations. ProFi-Net employs a
prototype-based metric learning architecture enhanced with a feature-level
attention mechanism, which dynamically refines the Euclidean distance by
emphasizing the most discriminative feature dimensions. Additionally, our
approach introduces a curriculum-inspired data augmentation strategy
exclusively on the query set. By progressively incorporating Gaussian noise of
increasing magnitude, the model is exposed to a broader range of challenging
variations, thereby improving its generalization and robustness to overfitting.
Extensive experiments conducted across diverse real-world environments
demonstrate that ProFi-Net significantly outperforms conventional prototype
networks and other state-of-the-art few-shot learning methods in terms of
classification accuracy and training efficiency.

</details>

### [51] [Representation Learning on a Random Lattice](https://arxiv.org/abs/2504.20197)
*Aryeh Brill*

Main category: cs.LG

TLDR: 论文提出了一种几何视角，将深度神经网络的表示分解为可解释的特征，以提升其安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 通过理解特征的本质，增强神经网络的安全性和可靠性。

Method: 采用几何视角，将特征视为嵌入数据分布的坐标系，并基于随机格子模型分析其性质。

Result: 特征被分类为上下文、组件和表面特征，与近期机制可解释性研究一致。

Conclusion: 模型为未来研究提供了方向，支持进一步探索神经网络的可解释性。

Abstract: Decomposing a deep neural network's learned representations into
interpretable features could greatly enhance its safety and reliability. To
better understand features, we adopt a geometric perspective, viewing them as a
learned coordinate system for mapping an embedded data distribution. We
motivate a model of a generic data distribution as a random lattice and analyze
its properties using percolation theory. Learned features are categorized into
context, component, and surface features. The model is qualitatively consistent
with recent findings in mechanistic interpretability and suggests directions
for future research.

</details>

### [52] [Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework](https://arxiv.org/abs/2504.20213)
*Yuan Xia,Akanksha Atrey,Fadoua Khmaissia,Kedar S. Namjoshi*

Main category: cs.LG

TLDR: 本文研究了大型语言模型（LLM）的逻辑推理能力，通过布尔逻辑证明任务评估其表现，并提出了一种数据增强技术（模板转换）以提高模型处理复杂逻辑的能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否真正具备逻辑推理能力，并解决训练数据稀缺的问题。

Method: 使用布尔逻辑证明任务，结合自动证明检查器验证输出；提出模板转换技术增强数据多样性。

Result: 实验显示LLM在短证明任务中表现良好，但随着证明复杂度增加性能下降；模板转换显著提升了模型准确性。

Conclusion: LLM在逻辑推理方面具有一定能力，但性能受证明复杂度影响；模板转换是一种有效的增强技术。

Abstract: This paper investigates the logical reasoning capabilities of large language
models (LLMs). For a precisely defined yet tractable formulation, we choose the
conceptually simple but technically complex task of constructing proofs in
Boolean logic. A trained LLM receives as input a set of assumptions and a goal,
and produces as output a proof that formally derives the goal from the
assumptions. Incorrect proofs are caught by an automated proof checker. A
critical obstacle for training is the scarcity of real-world proofs. We propose
an efficient, randomized procedure for synthesizing valid proofs and introduce
Template Transformation, a data augmentation technique that enhances the
model's ability to handle complex logical expressions. The central evaluation
question is whether an LLM has indeed learned to reason. We propose tests to
measure the reasoning ability of a black-box LLM. By these measures,
experiments demonstrate strong reasoning capabilities for assertions with short
proofs, which decline with proof complexity. Notably, template transformation
improves accuracy even for smaller models, suggesting its effectiveness across
model scales.

</details>

### [53] [Temporal Neural Operator for Modeling Time-Dependent Physical Phenomena](https://arxiv.org/abs/2504.20249)
*W. Diab,M. Al-Kobaisi*

Main category: cs.LG

TLDR: 提出了一种名为Temporal Neural Operator (TNO)的新型神经算子，专门用于时空算子学习，解决了现有神经算子在时间依赖性PDE中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子（如DeepONet和FNO）在时间依赖性PDE中表现不佳，尤其是对未在训练中显式出现的时间步长，且训练成本高。

Method: TNO通过引入时间分支、结合多种架构设计选择和训练策略（如马尔可夫假设、教师强制、时间捆绑等）来优化性能。

Result: TNO在多种示例问题中表现出长时程时间外推能力、误差累积鲁棒性、分辨率不变性及多输入函数处理灵活性。

Conclusion: TNO是一种高效的神经算子，显著提升了时间依赖性PDE的时空算子学习能力。

Abstract: Neural Operators (NOs) are machine learning models designed to solve partial
differential equations (PDEs) by learning to map between function spaces.
Neural Operators such as the Deep Operator Network (DeepONet) and the Fourier
Neural Operator (FNO) have demonstrated excellent generalization properties
when mapping between spatial function spaces. However, they struggle in mapping
the temporal dynamics of time-dependent PDEs, especially for time steps not
explicitly seen during training. This limits their temporal accuracy as they do
not leverage these dynamics in the training process. In addition, most NOs tend
to be prohibitively costly to train, especially for higher-dimensional PDEs. In
this paper, we propose the Temporal Neural Operator (TNO), an efficient neural
operator specifically designed for spatio-temporal operator learning for
time-dependent PDEs. TNO achieves this by introducing a temporal-branch to the
DeepONet framework, leveraging the best architectural design choices from
several other NOs, and a combination of training strategies including Markov
assumption, teacher forcing, temporal bundling, and the flexibility to
condition the output on the current state or past states. Through extensive
benchmarking and an ablation study on a diverse set of example problems we
demonstrate the TNO long range temporal extrapolation capabilities, robustness
to error accumulation, resolution invariance, and flexibility to handle
multiple input functions.

</details>

### [54] [Financial Data Analysis with Robust Federated Logistic Regression](https://arxiv.org/abs/2504.20250)
*Kun Yang,Nikhil Krishnan,Sanjeev R. Kulkarni*

Main category: cs.LG

TLDR: 提出了一种鲁棒的联邦逻辑回归框架，用于保护数据隐私并提高模型可解释性，同时在IID和非IID数据（含异常值）上表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究联邦学习中的金融数据分析，关注数据隐私保护、模型可解释性及对异常值的鲁棒性。

Method: 提出了一种鲁棒的联邦逻辑回归框架，平衡隐私保护、可解释性和鲁棒性。

Result: 在多个公开数据集上验证，性能与经典集中式算法（如逻辑回归、决策树、K近邻）相当。

Conclusion: 该框架在联邦学习中实现了隐私保护、可解释性和鲁棒性的平衡，适用于实际应用。

Abstract: In this study, we focus on the analysis of financial data in a federated
setting, wherein data is distributed across multiple clients or locations, and
the raw data never leaves the local devices. Our primary focus is not only on
the development of efficient learning frameworks (for protecting user data
privacy) in the field of federated learning but also on the importance of
designing models that are easier to interpret. In addition, we care about the
robustness of the framework to outliers. To achieve these goals, we propose a
robust federated logistic regression-based framework that strives to strike a
balance between these goals. To verify the feasibility of our proposed
framework, we carefully evaluate its performance not only on independently
identically distributed (IID) data but also on non-IID data, especially in
scenarios involving outliers. Extensive numerical results collected from
multiple public datasets demonstrate that our proposed method can achieve
comparable performance to those of classical centralized algorithms, such as
Logistical Regression, Decision Tree, and K-Nearest Neighbors, in both binary
and multi-class classification tasks.

</details>

### [55] [Investigating task-specific prompts and sparse autoencoders for activation monitoring](https://arxiv.org/abs/2504.20271)
*Henk Tillman,Dan Mossing*

Main category: cs.LG

TLDR: 论文探讨了如何通过语言模型的内部激活信息监控其输出，比较了多种改进线性探测的方法，并推荐了在不同计算资源下的最优方案。


<details>
  <summary>Details</summary>
Motivation: 语言模型可能产生意外或不安全的输出，因此需要有效监控其行为。利用内部激活信息可以提升监控效果。

Method: 比较了多种改进线性探测的方法，包括提示探测（prompted probing）和稀疏自编码器（SAE）方法，并提出了新的优化方案。

Result: 零样本直接询问模型是一个合理基线，但激活探测方法在足够训练数据下表现更优。提示探测在计算资源充足时效果最佳，SAE方法在计算受限时更优。

Conclusion: 推荐在计算资源充足时使用提示探测，计算受限时采用SAE方法，以平衡数据效率和性能。

Abstract: Language models can behave in unexpected and unsafe ways, and so it is
valuable to monitor their outputs. Internal activations of language models
encode additional information that could be useful for this. The baseline
approach for activation monitoring is some variation of linear probing on a
particular layer: starting from a labeled dataset, train a logistic regression
classifier on that layer's activations. Recent work has proposed several
approaches which may improve on naive linear probing, by leveraging additional
computation. One class of techniques, which we call "prompted probing,"
leverages test time computation to improve monitoring by (1) prompting the
model with a description of the monitoring task, and (2) applying a learned
linear probe to resulting activations. Another class of techniques uses
computation at train time: training sparse autoencoders offline to identify an
interpretable basis for the activations, and e.g. max-pooling activations
across tokens using that basis before applying a linear probe. However, one can
also prompt the model with a description of the monitoring task and use its
output directly. We develop and test novel refinements of these methods and
compare them against each other. We find asking the model zero-shot is a
reasonable baseline when inference-time compute is not limited; however,
activation probing methods can substantially outperform this baseline given
sufficient training data. Specifically, we recommend prompted probing when
inference-time compute is available, due to its superior data efficiency and
good generalization performance. Alternatively, if inference-time compute is
limited, we find SAE-based probing methods outperform raw activation probing.

</details>

### [56] [Generative Diffusion Models for Resource Allocation in Wireless Networks](https://arxiv.org/abs/2504.20277)
*Yigit Berkay Uslu,Samar Hadou,Shirin Saeedi Bidokhti,Alejandro Ribeiro*

Main category: cs.LG

TLDR: 提出了一种基于生成扩散模型的监督训练算法，用于学习随机资源分配策略，并通过图神经网络实现泛化。


<details>
  <summary>Details</summary>
Motivation: 解决随机资源分配问题，目标是最大化效用函数并满足服务质量约束。

Method: 利用生成扩散模型模仿专家策略生成样本，并通过图神经网络参数化反向扩散过程。

Result: 在干扰网络中的功率控制案例中展示了接近最优的性能。

Conclusion: 该方法能够有效模仿专家策略并实现泛化，适用于多种网络配置。

Abstract: This paper proposes a supervised training algorithm for learning stochastic
resource allocation policies with generative diffusion models (GDMs). We
formulate the allocation problem as the maximization of an ergodic utility
function subject to ergodic Quality of Service (QoS) constraints. Given samples
from a stochastic expert policy that yields a near-optimal solution to the
problem, we train a GDM policy to imitate the expert and generate new samples
from the optimal distribution. We achieve near-optimal performance through
sequential execution of the generated samples. To enable generalization to a
family of network configurations, we parameterize the backward diffusion
process with a graph neural network (GNN) architecture. We present numerical
results in a case study of power control in multi-user interference networks.

</details>

### [57] [FedCCL: Federated Clustered Continual Learning Framework for Privacy-focused Energy Forecasting](https://arxiv.org/abs/2504.20282)
*Michael A. Helcig,Stefan Nastic*

Main category: cs.LG

TLDR: FedCCL是一个针对静态组织特征但动态客户端可用性的联邦学习框架，通过静态预训练聚类和异步FedAvg算法，实现高效知识共享和模型专业化。


<details>
  <summary>Details</summary>
Motivation: 解决现有联邦学习方法在数据分布异构和计算能力差异下的效率低下和模型专业化延迟问题。

Method: 结合静态预训练聚类和异步FedAvg算法，采用三层模型拓扑（全局、集群特定和本地模型）。

Result: 在光伏安装数据上实现3.93%的能源预测误差，性能仅下降0.14个百分点。

Conclusion: FedCCL是一个高效、隐私保护的分布式学习框架，适应动态参与者群体。

Abstract: Privacy-preserving distributed model training is crucial for modern machine
learning applications, yet existing Federated Learning approaches struggle with
heterogeneous data distributions and varying computational capabilities.
Traditional solutions either treat all participants uniformly or require costly
dynamic clustering during training, leading to reduced efficiency and delayed
model specialization. We present FedCCL (Federated Clustered Continual
Learning), a framework specifically designed for environments with static
organizational characteristics but dynamic client availability. By combining
static pre-training clustering with an adapted asynchronous FedAvg algorithm,
FedCCL enables new clients to immediately profit from specialized models
without prior exposure to their data distribution, while maintaining reduced
coordination overhead and resilience to client disconnections. Our approach
implements an asynchronous Federated Learning protocol with a three-tier model
topology - global, cluster-specific, and local models - that efficiently
manages knowledge sharing across heterogeneous participants. Evaluation using
photovoltaic installations across central Europe demonstrates that FedCCL's
location-based clustering achieves an energy prediction error of 3.93%
(+-0.21%), while maintaining data privacy and showing that the framework
maintains stability for population-independent deployments, with 0.14
percentage point degradation in performance for new installations. The results
demonstrate that FedCCL offers an effective framework for privacy-preserving
distributed learning, maintaining high accuracy and adaptability even with
dynamic participant populations.

</details>

### [58] [Radius-Guided Post-Clustering for Shape-Aware, Scalable Refinement of k-Means Results](https://arxiv.org/abs/2504.20293)
*Stefan Kober*

Main category: cs.LG

TLDR: 论文提出了一种基于几何增强的k-means改进方法，通过合并重叠半径的簇来适应非凸形状，并放宽了对k值精确性的要求。


<details>
  <summary>Details</summary>
Motivation: 传统k-means在非凸形状上表现不佳且需要预先指定k值，限制了其应用范围。

Method: 在标准k-means后，为每个簇中心分配半径（最远点的距离），合并半径重叠的簇。支持递归分区和分布式处理。

Result: 在基准数据集上表现良好，准确性高且计算开销低。

Conclusion: 该方法通过简单后处理步骤提升了k-means的灵活性和扩展性，适用于非凸形状和大规模数据。

Abstract: Traditional k-means clustering underperforms on non-convex shapes and
requires the number of clusters k to be specified in advance. We propose a
simple geometric enhancement: after standard k-means, each cluster center is
assigned a radius (the distance to its farthest assigned point), and clusters
whose radii overlap are merged. This post-processing step loosens the
requirement for exact k: as long as k is overestimated (but not excessively),
the method can often reconstruct non-convex shapes through meaningful merges.
We also show that this approach supports recursive partitioning: clustering can
be performed independently on tiled regions of the feature space, then globally
merged, making the method scalable and suitable for distributed systems.
Implemented as a lightweight post-processing step atop scikit-learn's k-means,
the algorithm performs well on benchmark datasets, achieving high accuracy with
minimal additional computation.

</details>

### [59] [The Dark Side of Digital Twins: Adversarial Attacks on AI-Driven Water Forecasting](https://arxiv.org/abs/2504.20295)
*Mohammadhossein Homaei,Victor Gonzalez Morales,Oscar Mogollon-Gutierrez,Andres Caro*

Main category: cs.LG

TLDR: 本文提出了一种用于西班牙供水网络的数字孪生平台，利用LSTM预测用水量，但机器学习模型易受对抗攻击（如FGSM和PGD）。通过引入学习自动机（LA）和随机LA方法动态调整扰动，实验显示预测可靠性显著下降（MAPE从26%升至35%），强调了AI驱动数字孪生的网络安全风险及防御需求。


<details>
  <summary>Details</summary>
Motivation: 数字孪生（DTs）通过实时数据和预测模型优化供水系统，但机器学习模型易受对抗攻击，需研究其影响及防御措施。

Method: 提出基于LSTM的数字孪生平台，并引入LA和随机LA方法动态调整对抗扰动，测试其对预测准确性的影响。

Result: 对抗攻击显著降低预测可靠性（MAPE从26%升至35%），自适应攻击策略进一步加剧风险。

Conclusion: 研究揭示了AI驱动数字孪生的网络安全漏洞，呼吁采用对抗训练、异常检测等防御措施。

Abstract: Digital twins (DTs) are improving water distribution systems by using
real-time data, analytics, and prediction models to optimize operations. This
paper presents a DT platform designed for a Spanish water supply network,
utilizing Long Short-Term Memory (LSTM) networks to predict water consumption.
However, machine learning models are vulnerable to adversarial attacks, such as
the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD).
These attacks manipulate critical model parameters, injecting subtle
distortions that degrade forecasting accuracy. To further exploit these
vulnerabilities, we introduce a Learning Automata (LA) and Random LA-based
approach that dynamically adjusts perturbations, making adversarial attacks
more difficult to detect. Experimental results show that this approach
significantly impacts prediction reliability, causing the Mean Absolute
Percentage Error (MAPE) to rise from 26% to over 35%. Moreover, adaptive attack
strategies amplify this effect, highlighting cybersecurity risks in AI-driven
DTs. These findings emphasize the urgent need for robust defenses, including
adversarial training, anomaly detection, and secure data pipelines.

</details>

### [60] [FigBO: A Generalized Acquisition Function Framework with Look-Ahead Capability for Bayesian Optimization](https://arxiv.org/abs/2504.20307)
*Hui Chen,Xuhui Fan,Zhangkai Wu,Longbing Cao*

Main category: cs.LG

TLDR: 提出了一种名为FigBO的广义获取函数，通过结合候选点对未来全局信息增益的影响，解决了传统短视获取函数缺乏前瞻性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统短视获取函数因其简单性和有效性被广泛采用，但缺乏前瞻性限制了其性能。

Method: 提出FigBO，一种广义获取函数，可无缝集成现有短视获取函数（如EI），并分析其后悔界和收敛速度。

Result: 实验结果表明，FigBO在多种任务中表现优异，收敛速度显著快于现有方法。

Conclusion: FigBO是一种即插即用的方法，能够显著提升贝叶斯优化的性能。

Abstract: Bayesian optimization is a powerful technique for optimizing
expensive-to-evaluate black-box functions, consisting of two main components: a
surrogate model and an acquisition function. In recent years, myopic
acquisition functions have been widely adopted for their simplicity and
effectiveness. However, their lack of look-ahead capability limits their
performance. To address this limitation, we propose FigBO, a generalized
acquisition function that incorporates the future impact of candidate points on
global information gain. FigBO is a plug-and-play method that can integrate
seamlessly with most existing myopic acquisition functions. Theoretically, we
analyze the regret bound and convergence rate of FigBO when combined with the
myopic base acquisition function expected improvement (EI), comparing them to
those of standard EI. Empirically, extensive experimental results across
diverse tasks demonstrate that FigBO achieves state-of-the-art performance and
significantly faster convergence compared to existing methods.

</details>

### [61] [A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning](https://arxiv.org/abs/2504.20310)
*Greg Gluch,Shafi Goldwasser*

Main category: cs.LG

TLDR: 该论文研究了机器学习算法在推理时对抗输入的检测与缓解问题，定义了防御检测（DbD）和防御缓解（DbM），并探讨了它们在分类和生成任务中的差异。


<details>
  <summary>Details</summary>
Motivation: 研究对抗输入在推理时的防御策略，旨在保护机器学习算法免受攻击，同时不显著降低其在正常输入上的性能。

Method: 通过3轮协议形式化定义DbD和DbM，并分析其正确性、完整性和可靠性。在分类任务中证明DbD和DbM等效，而在生成任务中展示其分离性。

Result: 分类任务中DbD和DbM等效，生成任务中DbM可行但DbD不可行（基于特定加密假设）。缓解阶段所需样本显著少于初始训练。

Conclusion: 防御策略的选择需根据任务类型（分类或生成）而定，生成任务中DbM更具优势。

Abstract: In this paper, we initiate a cryptographically inspired theoretical study of
detection versus mitigation of adversarial inputs produced by attackers of
Machine Learning algorithms during inference time.
  We formally define defense by detection (DbD) and defense by mitigation
(DbM). Our definitions come in the form of a 3-round protocol between two
resource-bounded parties: a trainer/defender and an attacker. The attacker aims
to produce inference-time inputs that fool the training algorithm. We define
correctness, completeness, and soundness properties to capture successful
defense at inference time while not degrading (too much) the performance of the
algorithm on inputs from the training distribution.
  We first show that achieving DbD and achieving DbM are equivalent for ML
classification tasks. Surprisingly, this is not the case for ML generative
learning tasks, where there are many possible correct outputs that can be
generated for each input. We show a separation between DbD and DbM by
exhibiting a generative learning task for which is possible to defend by
mitigation but is provably impossible to defend by detection under the
assumption that the Identity-Based Fully Homomorphic Encryption (IB-FHE),
publicly-verifiable zero-knowledge Succinct Non-Interactive Arguments of
Knowledge (zk-SNARK) and Strongly Unforgeable Signatures exist. The mitigation
phase uses significantly fewer samples than the initial training algorithm.

</details>

### [62] [Perturbation-efficient Zeroth-order Optimization for Hardware-friendly On-device Training](https://arxiv.org/abs/2504.20314)
*Qitao Tan,Sung-En Chang,Rui Xia,Huidong Ji,Chence Yang,Ci Zhang,Jun Liu,Zheng Zhan,Zhou Zou,Yanzhi Wang,Jin Lu,Geng Yuan*

Main category: cs.LG

TLDR: PeZO是一种高效的零阶优化框架，通过减少随机数生成需求和硬件友好的方法，解决了传统ZO优化在硬件实现中的挑战。


<details>
  <summary>Details</summary>
Motivation: 零阶优化（ZO）在DNN训练中具有计算简单和内存节省的优势，但其需要大量高斯随机数的生成，导致在FPGAs和ASICs等硬件平台上难以实现。

Method: 提出PeZO框架，采用随机数重用策略和硬件友好的自适应缩放方法，用均匀分布替代高斯分布。

Result: 实验表明，PeZO显著减少了随机数生成的资源消耗（LUTs减少48.6%，FFs减少12.7%），并节省高达86%的功耗，同时不损失训练性能。

Conclusion: PeZO首次探索了零阶优化在设备上训练的潜力，为未来研究提供了有价值的见解。

Abstract: Zeroth-order (ZO) optimization is an emerging deep neural network (DNN)
training paradigm that offers computational simplicity and memory savings.
However, this seemingly promising approach faces a significant and long-ignored
challenge. ZO requires generating a substantial number of Gaussian random
numbers, which poses significant difficulties and even makes it infeasible for
hardware platforms, such as FPGAs and ASICs. In this paper, we identify this
critical issue, which arises from the mismatch between algorithm and hardware
designers. To address this issue, we proposed PeZO, a perturbation-efficient ZO
framework. Specifically, we design random number reuse strategies to
significantly reduce the demand for random number generation and introduce a
hardware-friendly adaptive scaling method to replace the costly Gaussian
distribution with a uniform distribution. Our experiments show that PeZO
reduces the required LUTs and FFs for random number generation by 48.6\% and
12.7\%, and saves at maximum 86\% power consumption, all without compromising
training performance, making ZO optimization feasible for on-device training.
To the best of our knowledge, we are the first to explore the potential of
on-device ZO optimization, providing valuable insights for future research.

</details>

### [63] [Bayesian Experimental Design for Model Discrepancy Calibration: An Auto-Differentiable Ensemble Kalman Inversion Approach](https://arxiv.org/abs/2504.20319)
*Huchen Yang,Xinghao Dong,Jin-Long Wu*

Main category: cs.LG

TLDR: 提出了一种基于自动微分集合卡尔曼反演（AD-EKI）的混合贝叶斯实验设计（BED）框架，用于高效处理高维模型差异问题。


<details>
  <summary>Details</summary>
Motivation: 传统BED方法因模型差异（预测模型与真实物理系统的不匹配）导致参数估计偏差，且高维参数空间对贝叶斯更新和设计优化带来挑战。

Method: 结合AD-EKI的混合框架，通过梯度自由方法高效估计高维网络参数的信息增益，并利用标准梯度方法优化设计。

Result: 在经典对流-扩散BED示例中，AD-EKI框架有效校准模型差异并稳健推断未知物理参数。

Conclusion: AD-EKI不仅解决了BED中的模型差异问题，还为元学习和结构优化等双层优化领域提供了高效可扩展的框架。

Abstract: Bayesian experimental design (BED) offers a principled framework for
optimizing data acquisition by leveraging probabilistic inference. However,
practical implementations of BED are often compromised by model discrepancy,
i.e., the mismatch between predictive models and true physical systems, which
can potentially lead to biased parameter estimates. While data-driven
approaches have been recently explored to characterize the model discrepancy,
the resulting high-dimensional parameter space poses severe challenges for both
Bayesian updating and design optimization. In this work, we propose a hybrid
BED framework enabled by auto-differentiable ensemble Kalman inversion (AD-EKI)
that addresses these challenges by providing a computationally efficient,
gradient-free alternative to estimate the information gain for high-dimensional
network parameters. The AD-EKI allows a differentiable evaluation of the
utility function in BED and thus facilitates the use of standard gradient-based
methods for design optimization. In the proposed hybrid framework, we
iteratively optimize experimental designs, decoupling the inference of
low-dimensional physical parameters handled by standard BED methods, from the
high-dimensional model discrepancy handled by AD-EKI. The identified optimal
designs for the model discrepancy enable us to systematically collect
informative data for its calibration. The performance of the proposed method is
studied by a classical convection-diffusion BED example, and the hybrid
framework enabled by AD-EKI efficiently identifies informative data to
calibrate the model discrepancy and robustly infers the unknown physical
parameters in the modeled system. Besides addressing the challenges of BED with
model discrepancy, AD-EKI also potentially fosters efficient and scalable
frameworks in many other areas with bilevel optimization, such as meta-learning
and structure optimization.

</details>

### [64] [Generative Learning for Slow Manifolds and Bifurcation Diagrams](https://arxiv.org/abs/2504.20375)
*Ellis R. Crabtree,Dimitris G. Giovanis,Nikolaos Evangelou,Juan M. Bello-Rivas,Ioannis G. Kevrekidis*

Main category: cs.LG

TLDR: 论文提出了一种利用条件生成模型（cSGMs）快速初始化慢流形和逼近分岔图中稳态的方法，用于多时间尺度系统的模型降维和参数化研究。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于利用生成模型的能力，快速生成符合特定条件的数据分布，以解决传统数值方法在慢流形初始化和分岔图稳态采样中的效率问题。

Method: 方法是通过条件生成模型（cSGMs）生成与目标条件（如慢流形上的特定值或新参数）一致的数据样本，用于初始化慢流形或逼近分岔图中的稳态。

Result: 结果表明，该方法能够有效揭示慢流形的几何结构，并填补分岔图中缺失的稳态段。

Conclusion: 结论指出，条件生成模型为多时间尺度系统的模型降维和参数化研究提供了一种高效的新工具。

Abstract: In dynamical systems characterized by separation of time scales, the
approximation of so called ``slow manifolds'', on which the long term dynamics
lie, is a useful step for model reduction. Initializing on such slow manifolds
is a useful step in modeling, since it circumvents fast transients, and is
crucial in multiscale algorithms alternating between fine scale (fast) and
coarser scale (slow) simulations. In a similar spirit, when one studies the
infinite time dynamics of systems depending on parameters, the system
attractors (e.g., its steady states) lie on bifurcation diagrams. Sampling
these manifolds gives us representative attractors (here, steady states of ODEs
or PDEs) at different parameter values. Algorithms for the systematic
construction of these manifolds are required parts of the ``traditional''
numerical nonlinear dynamics toolkit.
  In more recent years, as the field of Machine Learning develops, conditional
score-based generative models (cSGMs) have demonstrated capabilities in
generating plausible data from target distributions that are conditioned on
some given label. It is tempting to exploit such generative models to produce
samples of data distributions conditioned on some quantity of interest (QoI).
In this work, we present a framework for using cSGMs to quickly (a) initialize
on a low-dimensional (reduced-order) slow manifold of a multi-time-scale system
consistent with desired value(s) of a QoI (a ``label'') on the manifold, and
(b) approximate steady states in a bifurcation diagram consistent with a (new,
out-of-sample) parameter value. This conditional sampling can help uncover the
geometry of the reduced slow-manifold and/or approximately ``fill in'' missing
segments of steady states in a bifurcation diagram.

</details>

### [65] [Manifold Clustering with Schatten p-norm Maximization](https://arxiv.org/abs/2504.20390)
*Fangfang Li,Quanxue Gao*

Main category: cs.LG

TLDR: 提出了一种新的流形聚类框架，通过标签引导流形结构并确保数据与标签的一致性，同时利用Schatten p-范数保持类别平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注K-means与流形学习的结合，忽视了数据结构与标签的一致性。

Method: 融合K-means与流形学习，利用标签引导流形结构，最大化Schatten p-范数以保持类别平衡。

Result: 实验结果表明，所提模型在多个数据库上表现优越。

Conclusion: 新框架灵活兼容多种距离函数，能高效处理非线性可分数据。

Abstract: Manifold clustering, with its exceptional ability to capture complex data
structures, holds a pivotal position in cluster analysis. However, existing
methods often focus only on finding the optimal combination between K-means and
manifold learning, and overlooking the consistency between the data structure
and labels. To address this issue, we deeply explore the relationship between
K-means and manifold learning, and on this basis, fuse them to develop a new
clustering framework. Specifically, the algorithm uses labels to guide the
manifold structure and perform clustering on it, which ensures the consistency
between the data structure and labels. Furthermore, in order to naturally
maintain the class balance in the clustering process, we maximize the Schatten
p-norm of labels, and provide a theoretical proof to support this.
Additionally, our clustering framework is designed to be flexible and
compatible with many types of distance functions, which facilitates efficient
processing of nonlinear separable data. The experimental results of several
databases confirm the superiority of our proposed model.

</details>

### [66] [FourierSpecNet: Neural Collision Operator Approximation Inspired by the Fourier Spectral Method for Solving the Boltzmann Equation](https://arxiv.org/abs/2504.20408)
*Jae Yong Lee,Gwang Jae Jung,Byung Chan Lim,Hyung Ju Hwang*

Main category: cs.LG

TLDR: 提出了一种结合傅里叶谱方法和深度学习的混合框架FourierSpecNet，用于高效近似玻尔兹曼方程的碰撞算子，支持零样本超分辨率，并显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 玻尔兹曼方程的数值解计算成本高，尤其在非弹性碰撞和高维速度域中，需要一种高效且准确的替代方法。

Method: 结合傅里叶谱方法和深度学习，提出FourierSpecNet框架，实现分辨率无关的学习和零样本超分辨率。

Result: 在多个基准测试中，FourierSpecNet表现出与传统谱方法相当的准确性，同时显著降低计算成本。

Conclusion: FourierSpecNet为玻尔兹曼方程的求解提供了一种高效、可扩展的替代方案，适用于弹性和非弹性碰撞场景。

Abstract: The Boltzmann equation, a fundamental model in kinetic theory, describes the
evolution of particle distribution functions through a nonlinear,
high-dimensional collision operator. However, its numerical solution remains
computationally demanding, particularly for inelastic collisions and
high-dimensional velocity domains. In this work, we propose the Fourier Neural
Spectral Network (FourierSpecNet), a hybrid framework that integrates the
Fourier spectral method with deep learning to approximate the collision
operator in Fourier space efficiently. FourierSpecNet achieves
resolution-invariant learning and supports zero-shot super-resolution, enabling
accurate predictions at unseen resolutions without retraining. Beyond empirical
validation, we establish a consistency result showing that the trained operator
converges to the spectral solution as the discretization is refined. We
evaluate our method on several benchmark cases, including Maxwellian and
hard-sphere molecular models, as well as inelastic collision scenarios. The
results demonstrate that FourierSpecNet offers competitive accuracy while
significantly reducing computational cost compared to traditional spectral
solvers. Our approach provides a robust and scalable alternative for solving
the Boltzmann equation across both elastic and inelastic regimes.

</details>

### [67] [ADiff4TPP: Asynchronous Diffusion Models for Temporal Point Processes](https://arxiv.org/abs/2504.20411)
*Amartya Mukherjee,Ruizhi Deng,He Zhao,Yuzhen Mao,Leonid Sigal,Frederick Tung*

Main category: cs.LG

TLDR: 提出了一种基于异步噪声调度的扩散模型，用于建模时间点过程，通过调整噪声调度实现更快生成早期事件，并在预测任务中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在建模时间点过程时难以高效处理不同时间尺度的事件，且预测性能有限。

Method: 采用扩散模型结合异步噪声调度，通过条件流匹配训练模型，灵活调整生成过程的起点和终点。

Result: 在基准数据集上实现了对下一事件时间和类型预测的最优性能，尤其在长时预测任务中表现突出。

Conclusion: 该方法通过异步噪声调度和灵活的生成过程，显著提升了时间点过程的建模和预测能力。

Abstract: This work introduces a novel approach to modeling temporal point processes
using diffusion models with an asynchronous noise schedule. At each step of the
diffusion process, the noise schedule injects noise of varying scales into
different parts of the data. With a careful design of the noise schedules,
earlier events are generated faster than later ones, thus providing stronger
conditioning for forecasting the more distant future. We derive an objective to
effectively train these models for a general family of noise schedules based on
conditional flow matching. Our method models the joint distribution of the
latent representations of events in a sequence and achieves state-of-the-art
results in predicting both the next inter-event time and event type on
benchmark datasets. Additionally, it flexibly accommodates varying lengths of
observation and prediction windows in different forecasting settings by
adjusting the starting and ending points of the generation process. Finally,
our method shows superior performance in long-horizon prediction tasks,
outperforming existing baseline methods.

</details>

### [68] [Mitigating the Structural Bias in Graph Adversarial Defenses](https://arxiv.org/abs/2504.20848)
*Junyuan Fang,Huimin Liu,Han Yang,Jiajing Wu,Zibin Zheng,Chi K. Tse*

Main category: cs.LG

TLDR: 提出了一种针对图神经网络（GNNs）对抗攻击的防御策略，通过异构-同构增强图构建、kNN增强图构建和多视角节点注意力模块，减少GNNs在低度节点上的结构偏差。


<details>
  <summary>Details</summary>
Motivation: 当前GNNs防御方法在低度节点上存在结构偏差，类似于传统GNNs在干净图中的问题，需要一种更均衡的防御策略。

Method: 采用异构-同构增强图构建（去除异质链接并添加同质链接）和kNN增强图构建，结合多视角节点注意力模块自适应融合不同图视图的表征。

Result: 实验证明该策略在基准数据集上具有防御和去偏效果。

Conclusion: 提出的策略有效减少了GNNs在对抗攻击下的结构偏差，提升了防御能力。

Abstract: In recent years, graph neural networks (GNNs) have shown great potential in
addressing various graph structure-related downstream tasks. However, recent
studies have found that current GNNs are susceptible to malicious adversarial
attacks. Given the inevitable presence of adversarial attacks in the real
world, a variety of defense methods have been proposed to counter these attacks
and enhance the robustness of GNNs. Despite the commendable performance of
these defense methods, we have observed that they tend to exhibit a structural
bias in terms of their defense capability on nodes with low degree (i.e., tail
nodes), which is similar to the structural bias of traditional GNNs on nodes
with low degree in the clean graph. Therefore, in this work, we propose a
defense strategy by including hetero-homo augmented graph construction, $k$NN
augmented graph construction, and multi-view node-wise attention modules to
mitigate the structural bias of GNNs against adversarial attacks. Notably, the
hetero-homo augmented graph consists of removing heterophilic links (i.e.,
links connecting nodes with dissimilar features) globally and adding homophilic
links (i.e., links connecting nodes with similar features) for nodes with low
degree. To further enhance the defense capability, an attention mechanism is
adopted to adaptively combine the representations from the above two kinds of
graph views. We conduct extensive experiments to demonstrate the defense and
debiasing effect of the proposed strategy on benchmark datasets.

</details>

### [69] [Understanding GNNs and Homophily in Dynamic Node Classification](https://arxiv.org/abs/2504.20421)
*Michael Ito,Danai Koutra,Jenna Wiens*

Main category: cs.LG

TLDR: 论文探讨了动态图中同质性的新度量方法——动态同质性，并验证其与图神经网络性能的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有同质性度量仅适用于静态图，而动态图中的图神经网络性能尚未被充分研究。

Method: 提出动态同质性度量，并通过理论分析和实验验证其在动态图中的适用性。

Result: 动态同质性与图神经网络性能相关，现有模型在低动态同质性下表现不佳。

Conclusion: 动态同质性为动态图中的图神经网络设计提供了新方向。

Abstract: Homophily, as a measure, has been critical to increasing our understanding of
graph neural networks (GNNs). However, to date this measure has only been
analyzed in the context of static graphs. In our work, we explore homophily in
dynamic settings. Focusing on graph convolutional networks (GCNs), we
demonstrate theoretically that in dynamic settings, current GCN discriminative
performance is characterized by the probability that a node's future label is
the same as its neighbors' current labels. Based on this insight, we propose
dynamic homophily, a new measure of homophily that applies in the dynamic
setting. This new measure correlates with GNN discriminative performance and
sheds light on how to potentially design more powerful GNNs for dynamic graphs.
Leveraging a variety of dynamic node classification datasets, we demonstrate
that popular GNNs are not robust to low dynamic homophily. Going forward, our
work represents an important step towards understanding homophily and GNN
performance in dynamic node classification.

</details>

### [70] [Quantifying the Noise of Structural Perturbations on Graph Adversarial Attacks](https://arxiv.org/abs/2504.20869)
*Junyuan Fang,Han Yang,Haixian Wen,Jiajing Wu,Zibin Zheng,Chi K. Tse*

Main category: cs.LG

TLDR: 该论文提出了一种基于噪声的攻击强度量化方法，并设计了三种攻击策略，以提升图神经网络的对抗攻击可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络对抗攻击研究多关注攻击性能优化，而忽略了攻击强度的量化，导致攻击选择缺乏可解释性。

Method: 提出噪声概念量化攻击强度，基于分类边界设计单步和多步优化的三种攻击策略。

Result: 在基准数据集上对三种代表性图神经网络的实验验证了攻击策略的有效性，并分析了有效对抗扰动的偏好模式。

Conclusion: 通过噪声量化和分类边界设计，提升了对抗攻击的可解释性和攻击效果。

Abstract: Graph neural networks have been widely utilized to solve graph-related tasks
because of their strong learning power in utilizing the local information of
neighbors. However, recent studies on graph adversarial attacks have proven
that current graph neural networks are not robust against malicious attacks.
Yet much of the existing work has focused on the optimization objective based
on attack performance to obtain (near) optimal perturbations, but paid less
attention to the strength quantification of each perturbation such as the
injection of a particular node/link, which makes the choice of perturbations a
black-box model that lacks interpretability. In this work, we propose the
concept of noise to quantify the attack strength of each adversarial link.
Furthermore, we propose three attack strategies based on the defined noise and
classification margins in terms of single and multiple steps optimization.
Extensive experiments conducted on benchmark datasets against three
representative graph neural networks demonstrate the effectiveness of the
proposed attack strategies. Particularly, we also investigate the preferred
patterns of effective adversarial perturbations by analyzing the corresponding
properties of the selected perturbation nodes.

</details>

### [71] [Learning Laplacian Positional Encodings for Heterophilous Graphs](https://arxiv.org/abs/2504.20430)
*Michael Ito,Jiong Zhu,Dexiong Chen,Danai Koutra,Jenna Wiens*

Main category: cs.LG

TLDR: 当前图位置编码（PEs）在异质图任务中可能无效甚至有害，作者提出可学习拉普拉斯位置编码（LLPE）以解决这一问题，并在理论和实验上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图位置编码在异质图（节点邻近但标签不同）中表现不佳，而现实网络常具有异质性，需改进。

Method: 提出LLPE，利用图拉普拉斯的全谱信息，适用于同质和异质图，理论证明其近似图距离能力。

Result: 在12个基准测试中，LLPE将GNN（包括图变换器）的准确率提升高达35%（合成图）和14%（真实图）。

Conclusion: LLPE是开发适用于异质图的PE的重要进展，能有效捕捉复杂结构。

Abstract: In this work, we theoretically demonstrate that current graph positional
encodings (PEs) are not beneficial and could potentially hurt performance in
tasks involving heterophilous graphs, where nodes that are close tend to have
different labels. This limitation is critical as many real-world networks
exhibit heterophily, and even highly homophilous graphs can contain local
regions of strong heterophily. To address this limitation, we propose Learnable
Laplacian Positional Encodings (LLPE), a new PE that leverages the full
spectrum of the graph Laplacian, enabling them to capture graph structure on
both homophilous and heterophilous graphs. Theoretically, we prove LLPE's
ability to approximate a general class of graph distances and demonstrate its
generalization properties. Empirically, our evaluation on 12 benchmarks
demonstrates that LLPE improves accuracy across a variety of GNNs, including
graph transformers, by up to 35% and 14% on synthetic and real-world graphs,
respectively. Going forward, our work represents a significant step towards
developing PEs that effectively capture complex structures in heterophilous
graphs.

</details>

### [72] [GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2504.20437)
*DiJia Su,Andrew Gu,Jane Xu,Yuandong Tian,Jiawei Zhao*

Main category: cs.LG

TLDR: GaLore 2是一个高效且可扩展的框架，解决了GaLore在计算开销和并行化策略集成方面的挑战，并在大规模预训练中展示了其潜力。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）训练中的内存瓶颈问题，同时克服GaLore在计算开销和并行化策略集成方面的不足。

Method: 利用梯度低秩投影（GaLore）技术，结合低比特量化和高阶张量结构，提出GaLore 2框架。

Result: 通过预训练Llama 7B模型（5000亿训练词元），验证了GaLore 2的高效性和可扩展性。

Conclusion: GaLore 2为LLM预训练提供了高效且可扩展的解决方案，具有实际应用潜力。

Abstract: Large language models (LLMs) have revolutionized natural language
understanding and generation but face significant memory bottlenecks during
training. GaLore, Gradient Low-Rank Projection, addresses this issue by
leveraging the inherent low-rank structure of weight gradients, enabling
substantial memory savings without sacrificing performance. Recent works
further extend GaLore from various aspects, including low-bit quantization and
higher-order tensor structures. However, there are several remaining challenges
for GaLore, such as the computational overhead of SVD for subspace updates and
the integration with state-of-the-art training parallelization strategies
(e.g., FSDP). In this paper, we present GaLore 2, an efficient and scalable
GaLore framework that addresses these challenges and incorporates recent
advancements. In addition, we demonstrate the scalability of GaLore 2 by
pre-training Llama 7B from scratch using up to 500 billion training tokens,
highlighting its potential impact on real LLM pre-training scenarios.

</details>

### [73] [Multidimensional precipitation index prediction based on CNN-LSTM hybrid framework](https://arxiv.org/abs/2504.20442)
*Yuchen Wang,Pengfei Jia,Zhitao Shu,Keyan Liu,Abdul Rashid Mohamed Shariff*

Main category: cs.LG

TLDR: 本文提出了一种基于CNN-LSTM混合框架的多维降水指数预测模型，旨在提高降水预测的准确性。实验结果表明，该模型在预测精度和泛化能力上优于传统方法，但计算资源需求较高。


<details>
  <summary>Details</summary>
Motivation: 全球气候变化加剧，准确预测降水对防灾减灾、农业生产和交通具有重要意义。

Method: 采用CNN-LSTM混合框架，分析印度普纳地区1972-2002年的月均降水数据，捕捉局部特征和长期依赖关系。

Result: 模型均方根误差（RMSE）为6.752，优于传统时间序列预测方法。

Conclusion: 模型在降水预测中表现优越，但需优化计算资源需求和多维数据预测能力，未来可扩展为更高效的气象预测技术。

Abstract: With the intensification of global climate change, accurate prediction of
weather indicators is of great significance in disaster prevention and
mitigation, agricultural production, and transportation. Precipitation, as one
of the key meteorological indicators, plays a crucial role in water resource
management, agricultural production, and urban flood control. This study
proposes a multidimensional precipitation index prediction model based on a
CNN- LSTM hybrid framework, aiming to improve the accuracy of precipitation
forecasts. The dataset is sourced from Pune, Maharashtra, India, covering
monthly mean precipitation data from 1972 to 2002. This dataset includes nearly
31 years (1972-2002) of monthly average precipitation, reflecting the long-term
fluctuations and seasonal variations of precipitation in the region. By
analyzing these time series data, the CNN-LSTM model effectively captures local
features and long-term dependencies. Experimental results show that the model
achieves a root mean square error (RMSE) of 6.752, which demonstrates a
significant advantage over traditional time series prediction methods in terms
of prediction accuracy and generalization ability. Furthermore, this study
provides new research ideas for precipitation prediction. However, the model
requires high computational resources when dealing with large-scale datasets,
and its predictive ability for multidimensional precipitation data still needs
improvement. Future research could extend the model to support and predict
multidimensional precipitation data, thereby promoting the development of more
accurate and efficient meteorological prediction technologies.

</details>

### [74] [FT-MoE: Sustainable-learning Mixture of Experts Model for Fault-Tolerant Computing with Multiple Tasks](https://arxiv.org/abs/2504.20446)
*Wenjing Xiao,Wenhao Song,Miaojiang Chen,Ruikun Luo,Min Chen*

Main category: cs.LG

TLDR: FT-MoE是一种可持续学习的混合专家模型，用于多任务容错计算，通过解耦长距离依赖关系和双专家网络实现高可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习容错算法依赖单一神经网络模型，难以处理异构故障知识和复杂时间序列依赖关系。

Method: 使用基于解码器的Transformer模型获取故障原型向量，设计双专家网络进行高精度预测，并采用两阶段优化方案（离线训练和在线调优）。

Result: 实验表明，FT-MoE在容错基准测试中优于现有方法。

Conclusion: FT-MoE通过可持续学习和动态适应，显著提升了容错计算的性能和可靠性。

Abstract: Intelligent fault-tolerant (FT) computing has recently demonstrated
significant advantages of predicting and diagnosing faults in advance, enabling
reliable service delivery. However, due to heterogeneity of fault knowledge and
complex dependence relationships of time series log data, existing deep
learning-based FT algorithms further improve detection performance relying on
single neural network model with difficulty. To this end, we propose FT-MoE, a
sustainable-learning mixture-of-experts model for fault-tolerant computing with
multiple tasks, which enables different parameters learning distinct fault
knowledge to achieve high-reliability for service system. Firstly, we use
decoder-based transformer models to obtain fault prototype vectors of
decoupling long-distance dependencies. Followed by, we present a dual mixture
of experts networks for high-accurate prediction for both fault detection and
classification tasks. Then, we design a two-stage optimization scheme of
offline training and online tuning, which allows that in operation FT-MoE can
also keep learning to adapt to dynamic service environments. Finally, to verify
the effectiveness of FT-MoE, we conduct extensive experiments on the FT
benchmark. Experimental results show that FT-MoE achieves superior performance
compared to the state-of-the-art methods. Code will be available upon
publication.

</details>

### [75] [Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding](https://arxiv.org/abs/2504.20456)
*Gabe Guo,Stefano Ermon*

Main category: cs.LG

TLDR: AS-ARMs（任意子集自回归模型）通过Any-Subset Speculative Decoding（ASSD）算法，解决了并行生成令牌时分布偏离的问题，提升了生成速度且不牺牲质量。


<details>
  <summary>Details</summary>
Motivation: 研究如何在并行生成令牌时保持正确的联合分布，解决离散扩散模型在并行生成时的分布偏离问题。

Method: 提出AS-ARMs模型及ASSD算法，支持并行化联合概率密度估计，并通过数学证明和实验验证其有效性。

Result: AS-ARMs在子2亿参数模型中表现最佳，代码生成任务中接近50倍大模型的性能。

Conclusion: AS-ARMs是语言建模中一个有前景的方向。

Abstract: In arbitrary-order language models, it is an open question how to sample
tokens in parallel from the correct joint distribution. With discrete diffusion
models, the more tokens they generate in parallel, the less their predicted
distributions adhere to the originally learned data distribution, as they rely
on a conditional independence assumption that only works with infinitesimally
small timesteps. We find that a different class of models, any-subset
autoregressive models (AS-ARMs), holds the solution. As implied by the name,
AS-ARMs can generate tokens in any order, and in parallel. Moreover, AS-ARMs
support parallelized joint probability density estimation, allowing them to
correct their own parallel-generated token distributions, via our Any-Subset
Speculative Decoding (ASSD) algorithm. ASSD provably enables generation of
tokens from the correct joint distribution, with the number of neural network
calls upper bounded by the number of tokens predicted. We empirically verify
that ASSD speeds up language generation, without sacrificing quality.
Furthermore, we provide a mathematically justified scheme for training AS-ARMs
for generation, and show that AS-ARMs achieve state-of-the-art performance
among sub-200M parameter models on infilling benchmark tasks, and nearly match
the performance of models 50X larger on code generation. Our theoretical and
empirical results indicate that the once-forgotten AS-ARMs are a promising
direction of language modeling.

</details>

### [76] [The Estimation of Continual Causal Effect for Dataset Shifting Streams](https://arxiv.org/abs/2504.20471)
*Baining Chen,Yiming Zhang,Yuqiao Han,Ruyue Zhang,Ruihuan Du,Zhishuo Zhou,Zhengdan Zhu,Xun Liu,Jiecheng Guo*

Main category: cs.LG

TLDR: 论文提出了一种名为ICE-PKD的增量因果效应框架，用于解决营销优化中因时间数据集偏移带来的复杂性。


<details>
  <summary>Details</summary>
Motivation: 在线环境中，时间数据集偏移（用户行为和领域分布随时间变化）导致传统因果效应估计框架性能下降，需要改进。

Method: ICE-PKD框架包括两部分：(i) 多处理提升网络，通过反事实回归消除混杂偏差；(ii) 增量训练策略，通过最新数据更新并基于回放的知识蒸馏保护泛化能力。

Result: 在模拟和在线数据集上的实验表明，ICE-PKD框架性能更优，并已部署在华夏租车营销系统中。

Conclusion: ICE-PKD框架有效解决了时间数据集偏移问题，提升了因果效应估计的在线性能。

Abstract: Causal effect estimation has been widely used in marketing optimization. The
framework of an uplift model followed by a constrained optimization algorithm
is popular in practice. To enhance performance in the online environment, the
framework needs to be improved to address the complexities caused by temporal
dataset shift. This paper focuses on capturing the dataset shift from user
behavior and domain distribution changing over time. We propose an Incremental
Causal Effect with Proxy Knowledge Distillation (ICE-PKD) framework to tackle
this challenge. The ICE-PKD framework includes two components: (i) a
multi-treatment uplift network that eliminates confounding bias using
counterfactual regression; (ii) an incremental training strategy that adapts to
the temporal dataset shift by updating with the latest data and protects
generalization via replay-based knowledge distillation. We also revisit the
uplift modeling metrics and introduce a novel metric for more precise online
evaluation in multiple treatment scenarios. Extensive experiments on both
simulated and online datasets show that the proposed framework achieves better
performance. The ICE-PKD framework has been deployed in the marketing system of
Huaxiaozhu, a ride-hailing platform in China.

</details>

### [77] [Group Relative Knowledge Distillation: Learning from Teacher's Relational Inductive Bias](https://arxiv.org/abs/2504.20482)
*Chao Li,Changhua Zhou,Jia Chen*

Main category: cs.LG

TLDR: 论文提出了一种新的知识蒸馏方法GRKD，通过关注教师模型输出的相对排名而非绝对概率，提升学生模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法主要关注绝对概率的模仿，忽略了教师模型中相对预测的关系性归纳偏差，导致暴露偏差。

Method: 提出GRKD框架，引入组相对损失函数，鼓励学生模型学习教师输出的类别相对排序。

Result: 在分类基准测试中，GRKD表现出优于现有方法的泛化能力，尤其在细粒度分类任务中。

Conclusion: GRKD为知识蒸馏提供了新视角，强调关系结构而非绝对概率的重要性。

Abstract: Knowledge distillation typically transfers knowledge from a teacher model to
a student model by minimizing differences between their output distributions.
However, existing distillation approaches largely focus on mimicking absolute
probabilities and neglect the valuable relational inductive biases embedded in
the teacher's relative predictions, leading to exposure bias. In this paper, we
propose Group Relative Knowledge Distillation (GRKD), a novel framework that
distills teacher knowledge by learning the relative ranking among classes,
rather than directly fitting the absolute distribution. Specifically, we
introduce a group relative loss that encourages the student model to preserve
the pairwise preference orderings provided by the teacher's outputs. Extensive
experiments on classification benchmarks demonstrate that GRKD achieves
superior generalization compared to existing methods, especially in tasks
requiring fine-grained class differentiation. Our method provides a new
perspective on exploiting teacher knowledge, focusing on relational structure
rather than absolute likelihood.

</details>

### [78] [Wavelet-Filtering of Symbolic Music Representations for Folk Tune Segmentation and Classification](https://arxiv.org/abs/2504.20522)
*Gissel Velarde,Tillman Weyde,David Meredith*

Main category: cs.LG

TLDR: 该研究评估了一种基于Haar小波滤波的机器学习方法，用于民谣符号表示的分类，并与Gestalt方法对比，结果显示小波方法在优化参数后分类更优。


<details>
  <summary>Details</summary>
Motivation: 探索更有效的民谣分类方法，比较小波滤波与传统Gestalt方法的性能差异。

Method: 使用Haar小波对旋律进行连续小波变换，提取局部极值作为边界，结合k近邻分类器进行分类。

Result: 小波滤波方法在优化参数后分类准确率更高。

Conclusion: Haar小波滤波方法在民谣分类中优于传统Gestalt方法。

Abstract: The aim of this study is to evaluate a machine-learning method in which
symbolic representations of folk songs are segmented and classified into tune
families with Haar-wavelet filtering. The method is compared with previously
proposed Gestalt-based method. Melodies are represented as discrete symbolic
pitch-time signals. We apply the continuous wavelet transform (CWT) with the
Haar wavelet at specific scales, obtaining filtered versions of melodies
emphasizing their information at particular time-scales. We use the filtered
signal for representation and segmentation, using the wavelet coefficients'
local maxima to indicate local boundaries and classify segments by means of
k-nearest neighbours based on standard vector-metrics (Euclidean, cityblock),
and compare the results to a Gestalt-based segmentation method and metrics
applied directly to the pitch signal. We found that the wavelet based
segmentation and wavelet-filtering of the pitch signal lead to better
classification accuracy in cross-validated evaluation when the time-scale and
other parameters are optimized.

</details>

### [79] [DeeP-Mod: Deep Dynamic Programming based Environment Modelling using Feature Extraction](https://arxiv.org/abs/2504.20535)
*Chris Child,Lam Ngo*

Main category: cs.LG

TLDR: DeeP-Mod框架通过DDPN提取特征构建环境模型，解决了DQN中状态信息丢失的问题，实现了任务和动作集的独立性，并在噪声下表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决DQN中因混合状态-动作表示导致深层状态信息丢失的问题。

Method: 使用动态规划训练DDPN，通过值迭代确保输出为状态值而非状态-动作对，并提取特征构建环境模型。

Result: 减少的DDPN在噪声下收敛更快且性能优于原始DDPN；DeeP-Mod框架无需外部环境模型即可学习最优策略。

Conclusion: DeeP-Mod框架通过DDPN特征提取有效解决了状态信息丢失问题，适用于广泛环境。

Abstract: The DeeP-Mod framework builds an environment model using features from a Deep
Dynamic Programming Network (DDPN), trained via a Deep Q-Network (DQN). While
Deep Q-Learning is effective in decision-making, state information is lost in
deeper DQN layers due to mixed state-action representations. We address this by
using Dynamic Programming (DP) to train a DDPN, where Value Iteration ensures
the output represents state values, not state-action pairs. Extracting features
from the DDPN preserves state information, enabling task and action set
independence. We show that a reduced DDPN can be trained using features
extracted from the original DDPN trained on an identical problem. This reduced
DDPN achieves faster convergence under noise and outperforms the original DDPN.
Finally, we introduce the DeeP-Mod framework, which creates an environment
model using the evolution of features extracted from a DDPN in response to
actions. A second DDPN, which learns directly from this feature model rather
than raw states, can learn an effective feature-value representation and thus
optimal policy. A key advantage of DeeP-Mod is that an externally defined
environment model is not needed at any stage, making DDPN applicable to a wide
range of environments.

</details>

### [80] [Inclusive Training Separation and Implicit Knowledge Interaction for Balanced Online Class-Incremental Learning](https://arxiv.org/abs/2504.20566)
*Shunjie Wen,Thomas Heinis,Dong-Wan Choi*

Main category: cs.LG

TLDR: 论文提出了一种名为BOIL的新方法，通过双分类器和包容性训练分离策略，在在线类增量学习中实现了高可塑性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 在线类增量学习（OCIL）的主要挑战在于平衡新旧类别的知识，现有方法往往难以兼顾可塑性和稳定性。

Method: 提出BOIL方法，采用双分类器和包容性训练分离策略，通过隐式知识转移整合新旧类别知识。

Result: 在三个广泛使用的OCIL基准数据集上，BOIL表现出更平衡且优于现有方法的性能。

Conclusion: BOIL方法在OCIL中实现了更平衡的性能，解决了现有方法的局限性。

Abstract: Online class-incremental learning (OCIL) focuses on gradually learning new
classes (called plasticity) from a stream of data in a single-pass, while
concurrently preserving knowledge of previously learned classes (called
stability). The primary challenge in OCIL lies in maintaining a good balance
between the knowledge of old and new classes within the continually updated
model. Most existing methods rely on explicit knowledge interaction through
experience replay, and often employ exclusive training separation to address
bias problems. Nevertheless, it still remains a big challenge to achieve a
well-balanced learner, as these methods often exhibit either reduced plasticity
or limited stability due to difficulties in continually integrating knowledge
in the OCIL setting. In this paper, we propose a novel replay-based method,
called Balanced Online Incremental Learning (BOIL), which can achieve both high
plasticity and stability, thus ensuring more balanced performance in OCIL. Our
BOIL method proposes an inclusive training separation strategy using dual
classifiers so that knowledge from both old and new classes can effectively be
integrated into the model, while introducing implicit approaches for
transferring knowledge across the two classifiers. Extensive experimental
evaluations over three widely-used OCIL benchmark datasets demonstrate the
superiority of BOIL, showing more balanced yet better performance compared to
state-of-the-art replay-based OCIL methods.

</details>

### [81] [Digital Shielding for Cross-Domain Wi-Fi Signal Adaptation using Relativistic Average Generative Adversarial Network](https://arxiv.org/abs/2504.20568)
*Danilo Avola,Federica Bruni,Gian Luca Foresti,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TLDR: 论文提出了一种基于深度学习的Wi-Fi信号跨域适应模型，利用RaGAN和Bi-LSTM架构，通过模拟物理屏蔽环境实现高精度材料识别。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi传感技术受环境影响大，跨域适应性差，亟需一种能稳定提取特征的方法。

Method: 使用RaGAN和Bi-LSTM架构，通过模拟法拉第笼的屏蔽环境收集信号数据，训练多类SVM进行分类。

Result: 模型在去噪信号上达到96%的准确率，具备强材料识别能力。

Conclusion: 该方法在安全领域有潜力，可用于识别隐蔽物体。

Abstract: Wi-Fi sensing uses radio-frequency signals from Wi-Fi devices to analyze
environments, enabling tasks such as tracking people, detecting intrusions, and
recognizing gestures. The rise of this technology is driven by the IEEE
802.11bf standard and growing demand for tools that can ensure privacy and
operate through obstacles. However, the performance of Wi-Fi sensing is heavily
influenced by environmental conditions, especially when extracting spatial and
temporal features from the surrounding scene. A key challenge is achieving
robust generalization across domains, ensuring stable performance even when the
sensing environment changes significantly. This paper introduces a novel deep
learning model for cross-domain adaptation of Wi-Fi signals, inspired by
physical signal shielding. The model uses a Relativistic average Generative
Adversarial Network (RaGAN) with Bidirectional Long Short-Term Memory (Bi-LSTM)
architectures for both the generator and discriminator. To simulate physical
shielding, an acrylic box lined with electromagnetic shielding fabric was
constructed, mimicking a Faraday cage. Wi-Fi signal spectra were collected from
various materials both inside (domain-free) and outside (domain-dependent) the
box to train the model. A multi-class Support Vector Machine (SVM) was trained
on domain-free spectra and tested on signals denoised by the RaGAN. The system
achieved 96% accuracy and demonstrated strong material discrimination
capabilities, offering potential for use in security applications to identify
concealed objects based on their composition.

</details>

### [82] [Reinforcement Learning for Reasoning in Large Language Models with One Training Example](https://arxiv.org/abs/2504.20571)
*Yiping Wang,Qing Yang,Zhiyuan Zeng,Liliang Ren,Lucas Liu,Baolin Peng,Hao Cheng,Xuehai He,Kuan Wang,Jianfeng Gao,Weizhu Chen,Shuohang Wang,Simon Shaolei Du,Yelong Shen*

Main category: cs.LG

TLDR: 1-shot RLVR显著提升LLMs数学推理能力，单例训练使MATH500性能从36%提升至73.6%，并验证了探索机制的关键作用。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过单例训练（1-shot RLVR）高效提升大型语言模型的数学推理能力。

Method: 使用1-shot RLVR方法，结合GRPO和PPO算法，验证不同模型和数学示例的效果。

Result: 单例训练显著提升性能（MATH500从36%到73.6%），并发现跨域泛化、自反思增加等现象。

Conclusion: 1-shot RLVR高效且具泛化性，探索机制（如熵损失）是关键，为未来RLVR研究提供方向。

Abstract: We show that reinforcement learning with verifiable reward using one training
example (1-shot RLVR) is effective in incentivizing the math reasoning
capabilities of large language models (LLMs). Applying RLVR to the base model
Qwen2.5-Math-1.5B, we identify a single example that elevates model performance
on MATH500 from 36.0% to 73.6%, and improves the average performance across six
common mathematical reasoning benchmarks from 17.6% to 35.7%. This result
matches the performance obtained using the 1.2k DeepScaleR subset (MATH500:
73.6%, average: 35.9%), which includes the aforementioned example. Similar
substantial improvements are observed across various models (Qwen2.5-Math-7B,
Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and
PPO), and different math examples (many of which yield approximately 30% or
greater improvement on MATH500 when employed as a single training example). In
addition, we identify some interesting phenomena during 1-shot RLVR, including
cross-domain generalization, increased frequency of self-reflection, and
sustained test performance improvement even after the training accuracy has
saturated, a phenomenon we term post-saturation generalization. Moreover, we
verify that the effectiveness of 1-shot RLVR primarily arises from the policy
gradient loss, distinguishing it from the "grokking" phenomenon. We also show
the critical role of promoting exploration (e.g., by adding entropy loss with
an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe
that applying entropy loss alone, without any outcome reward, significantly
enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings
can inspire future work on RLVR data efficiency and encourage a re-examination
of both recent progress and the underlying mechanisms in RLVR. Our code, model,
and data are open source at https://github.com/ypwang61/One-Shot-RLVR

</details>

### [83] [Representation Learning Preserving Ignorability and Covariate Matching for Treatment Effects](https://arxiv.org/abs/2504.20579)
*Praharsh Nanavati,Ranjitha Prasad,Karthikeyan Shanmugam*

Main category: cs.LG

TLDR: 论文提出了一种结合神经网络架构的方法，同时解决观测数据中的隐藏混杂和协变量不匹配问题，通过梯度匹配和协变量匹配变换学习有效调整集，并在多个因果基准数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 观测数据中的因果效应估计面临隐藏混杂和协变量不匹配两大挑战，现有方法通常只解决其中之一，缺乏统一框架。

Method: 结合两种神经网络架构：基于梯度匹配的域适应方法和协变量匹配变换，学习满足调整和匹配约束的预治疗协变量表示。

Result: 提出的方法在IHDP、Jobs、Cattaneo和图像数据集上优于基线，证明了近似不变表示能提供因果效应的有效边界。

Conclusion: 通过结合梯度匹配和协变量匹配，论文为同时解决隐藏混杂和选择偏差提供了统一框架，并在实验中验证了其有效性。

Abstract: Estimating treatment effects from observational data is challenging due to
two main reasons: (a) hidden confounding, and (b) covariate mismatch (control
and treatment groups not having identical distributions). Long lines of works
exist that address only either of these issues. To address the former,
conventional techniques that require detailed knowledge in the form of causal
graphs have been proposed. For the latter, covariate matching and importance
weighting methods have been used. Recently, there has been progress in
combining testable independencies with partial side information for tackling
hidden confounding. A common framework to address both hidden confounding and
selection bias is missing. We propose neural architectures that aim to learn a
representation of pre-treatment covariates that is a valid adjustment and also
satisfies covariate matching constraints. We combine two different neural
architectures: one based on gradient matching across domains created by
subsampling a suitable anchor variable that assumes causal side information,
followed by the other, a covariate matching transformation. We prove that
approximately invariant representations yield approximate valid adjustment sets
which would enable an interval around the true causal effect. In contrast to
usual sensitivity analysis, where an unknown nuisance parameter is varied, we
have a testable approximation yielding a bound on the effect estimate. We also
outperform various baselines with respect to ATE and PEHE errors on causal
benchmarks that include IHDP, Jobs, Cattaneo, and an image-based Crowd
Management dataset.

</details>

### [84] [Independent Learning in Performative Markov Potential Games](https://arxiv.org/abs/2504.20593)
*Rilind Sahitaj,Paulius Sasnauskas,Yiğit Yalın,Debmalya Mandal,Goran Radanović*

Main category: cs.LG

TLDR: 论文研究了多智能体表演性强化学习（PRL），提出表演性稳定均衡（PSE）概念，并证明其在合理假设下存在。分析了IPGA和INPG算法的收敛性，验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 探讨多智能体环境中策略部署对奖励和动态的影响，扩展表演性效应到马尔可夫势博弈（MPGs）中。

Method: 引入PSE概念，分析IPGA和INPG算法的收敛性，并通过实验验证。

Result: 证明PSE存在，IPGA和INPG收敛到近似PSE，INPG渐近收敛到PSE。表演性效应消失时恢复经典收敛率。

Conclusion: 多智能体PRL中PSE存在且可收敛，为相关研究提供理论基础和算法支持。

Abstract: Performative Reinforcement Learning (PRL) refers to a scenario in which the
deployed policy changes the reward and transition dynamics of the underlying
environment. In this work, we study multi-agent PRL by incorporating
performative effects into Markov Potential Games (MPGs). We introduce the
notion of a performatively stable equilibrium (PSE) and show that it always
exists under a reasonable sensitivity assumption. We then provide convergence
results for state-of-the-art algorithms used to solve MPGs. Specifically, we
show that independent policy gradient ascent (IPGA) and independent natural
policy gradient (INPG) converge to an approximate PSE in the best-iterate
sense, with an additional term that accounts for the performative effects.
Furthermore, we show that INPG asymptotically converges to a PSE in the
last-iterate sense. As the performative effects vanish, we recover the
convergence rates from prior work. For a special case of our game, we provide
finite-time last-iterate convergence results for a repeated retraining
approach, in which agents independently optimize a surrogate objective. We
conduct extensive experiments to validate our theoretical findings.

</details>

### [85] [Bridging the Generalisation Gap: Synthetic Data Generation for Multi-Site Clinical Model Validation](https://arxiv.org/abs/2504.20635)
*Bradley Segal,Joshua Fieggen,David Clifton,Lei Clifton*

Main category: cs.LG

TLDR: 提出了一种结构化合成数据框架，用于系统评估临床机器学习模型的鲁棒性、公平性和泛化性，解决了现有方法依赖真实数据且缺乏透明性的问题。


<details>
  <summary>Details</summary>
Motivation: 临床机器学习模型的泛化性因患者人口统计、疾病流行率和机构实践的多样性而面临挑战，现有评估方法依赖真实数据且存在局限性。

Method: 开发了一种结构化合成数据框架，提供对数据生成过程的显式控制，包括站点特异性流行变化、分层子组效应和结构化特征交互。

Result: 实验表明，该框架能隔离站点变化的影响，支持公平性审计，并揭示泛化失败，特别是模型复杂性与站点特异性效应的交互。

Conclusion: 该工作为临床环境中可靠部署机器学习提供了一种可重复、可解释和可配置的工具。

Abstract: Ensuring the generalisability of clinical machine learning (ML) models across
diverse healthcare settings remains a significant challenge due to variability
in patient demographics, disease prevalence, and institutional practices.
Existing model evaluation approaches often rely on real-world datasets, which
are limited in availability, embed confounding biases, and lack the flexibility
needed for systematic experimentation. Furthermore, while generative models aim
for statistical realism, they often lack transparency and explicit control over
factors driving distributional shifts. In this work, we propose a novel
structured synthetic data framework designed for the controlled benchmarking of
model robustness, fairness, and generalisability. Unlike approaches focused
solely on mimicking observed data, our framework provides explicit control over
the data generating process, including site-specific prevalence variations,
hierarchical subgroup effects, and structured feature interactions. This
enables targeted investigation into how models respond to specific
distributional shifts and potential biases. Through controlled experiments, we
demonstrate the framework's ability to isolate the impact of site variations,
support fairness-aware audits, and reveal generalisation failures, particularly
highlighting how model complexity interacts with site-specific effects. This
work contributes a reproducible, interpretable, and configurable tool designed
to advance the reliable deployment of ML in clinical settings.

</details>

### [86] [Decision-centric fairness: Evaluation and optimization for resource allocation problems](https://arxiv.org/abs/2504.20642)
*Simon De Vos,Jente Van Belle,Andres Algaba,Wouter Verbeke,Sam Verboven*

Main category: cs.LG

TLDR: 论文提出了一种决策中心公平性方法，仅在决策区域内实施公平性，避免全局公平性方法对模型预测质量的过度限制。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的决策工具可能导致资源分配不公平，特别是在二元分类模型中，某些人口群体可能受到歧视。

Method: 提出决策中心公平性方法，仅在相关决策阈值范围内实施公平性，而非全局范围。

Result: 通过实验比较，决策中心方法在多个数据集上优于全局公平性方法，避免了不必要的模型性能下降。

Conclusion: 决策中心公平性方法在资源分配决策中更有效，同时保持模型预测质量。

Abstract: Data-driven decision support tools play an increasingly central role in
decision-making across various domains. In this work, we focus on binary
classification models for predicting positive-outcome scores and deciding on
resource allocation, e.g., credit scores for granting loans or churn propensity
scores for targeting customers with a retention campaign. Such models may
exhibit discriminatory behavior toward specific demographic groups through
their predicted scores, potentially leading to unfair resource allocation. We
focus on demographic parity as a fairness metric to compare the proportions of
instances that are selected based on their positive outcome scores across
groups. In this work, we propose a decision-centric fairness methodology that
induces fairness only within the decision-making region -- the range of
relevant decision thresholds on the score that may be used to decide on
resource allocation -- as an alternative to a global fairness approach that
seeks to enforce parity across the entire score distribution. By restricting
the induction of fairness to the decision-making region, the proposed
decision-centric approach avoids imposing overly restrictive constraints on the
model, which may unnecessarily degrade the quality of the predicted scores. We
empirically compare our approach to a global fairness approach on multiple
(semi-synthetic) datasets to identify scenarios in which focusing on fairness
where it truly matters, i.e., decision-centric fairness, proves beneficial.

</details>

### [87] [Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified File Selection](https://arxiv.org/abs/2504.20644)
*Ziqing Fan,Siyuan Du,Shengchao Hu,Pingjie Wang,Li Shen,Ya Zhang,Dacheng Tao,Yanfeng Wang*

Main category: cs.LG

TLDR: 提出了一种多样化文件选择算法（DiSF），通过选择特征空间中相关性最低的文本文件，解决了领域相似性选择标准导致的多样性问题，显著提升了模型的整体性能。


<details>
  <summary>Details</summary>
Motivation: 在有限计算预算下，选择高质量预训练数据对提升大语言模型性能至关重要。现有方法依赖领域相似性选择标准，但会导致多样性问题，影响通用性能。

Method: 提出DiSF算法，使用贪心算法选择特征空间中相关性最低的文本文件，优化特征协方差矩阵的特征值分布。

Result: 在TinyLlama架构上实验，DiSF显著提升了整体性能，节省了98.5%的训练文件，并在50B训练预算内优于全数据预训练。

Conclusion: DiSF通过增强数据多样性，显著提升了训练和数据效率，解决了领域相似性选择标准的局限性。

Abstract: Selecting high-quality pre-training data for large language models (LLMs) is
crucial for enhancing their overall performance under limited computation
budget, improving both training and sample efficiency. Recent advancements in
file selection primarily rely on using an existing or trained proxy model to
assess the similarity of samples to a target domain, such as high quality
sources BookCorpus and Wikipedia. However, upon revisiting these methods, the
domain-similarity selection criteria demonstrates a diversity dilemma,
i.e.dimensional collapse in the feature space, improving performance on the
domain-related tasks but causing severe degradation on generic performance. To
prevent collapse and enhance diversity, we propose a DiverSified File selection
algorithm (DiSF), which selects the most decorrelated text files in the feature
space. We approach this with a classical greedy algorithm to achieve more
uniform eigenvalues in the feature covariance matrix of the selected texts,
analyzing its approximation to the optimal solution under a formulation of
$\gamma$-weakly submodular optimization problem. Empirically, we establish a
benchmark and conduct extensive experiments on the TinyLlama architecture with
models from 120M to 1.1B parameters. Evaluating across nine tasks from the
Harness framework, DiSF demonstrates a significant improvement on overall
performance. Specifically, DiSF saves 98.5% of 590M training files in
SlimPajama, outperforming the full-data pre-training within a 50B training
budget, and achieving about 1.5x training efficiency and 5x data efficiency.

</details>

### [88] [RuleKit 2: Faster and simpler rule learning](https://arxiv.org/abs/2504.20650)
*Adam Gudyś,Cezary Maszczyk,Joanna Badura,Adam Grzelak,Marek Sikora,Łukasz Wróbel*

Main category: cs.LG

TLDR: RuleKit 2是一个基于规则的数据分析工具包，通过新算法和优化实现显著提升了计算性能，并新增了Python包和浏览器应用以增强可用性。


<details>
  <summary>Details</summary>
Motivation: 规则具有预测和描述能力，RuleKit在分类、回归和生存问题中表现出色，因此推出第二版以进一步提升性能和可用性。

Method: 通过新算法和优化实现改进计算性能，新增Python包和浏览器应用以支持更广泛的使用场景。

Result: 计算性能显著提升，部分数据集分析时间减少两个数量级，同时提供更友好的用户界面和集成能力。

Conclusion: RuleKit 2是一个高效且易用的规则分析工具，适用于多种数据挖掘任务，并支持与现有分析流程的无缝集成。

Abstract: Rules offer an invaluable combination of predictive and descriptive
capabilities. Our package for rule-based data analysis, RuleKit, has proven its
effectiveness in classification, regression, and survival problems. Here we
present its second version. New algorithms and optimized implementations of
those previously included, significantly improved the computational performance
of our suite, reducing the analysis time of some data sets by two orders of
magnitude. The usability of RuleKit 2 is provided by two new components: Python
package and browser application with a graphical user interface. The former
complies with scikit-learn, the most popular data mining library for Python,
allowing RuleKit 2 to be straightforwardly integrated into existing data
analysis pipelines. RuleKit 2 is available at GitHub under GNU AGPL 3 license
(https://github.com/adaa-polsl/RuleKit)

</details>

### [89] [Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition](https://arxiv.org/abs/2504.20938)
*Zhengfu He,Junxuan Wang,Rui Lin,Xuyang Ge,Wentao Shu,Qiong Tang,Junping Zhang,Xipeng Qiu*

Main category: cs.LG

TLDR: Lorsa是一种稀疏注意力模型，用于分解Transformer的多头自注意力机制，提供更清晰的注意力行为解释。


<details>
  <summary>Details</summary>
Motivation: 解决多头自注意力中注意力叠加的问题，以更好地理解特征间的交互。

Method: 提出Lorsa模型，通过稀疏字典学习方法分解注意力层，发现更细粒度的注意力行为。

Result: Lorsa发现了更清晰的注意力行为（如归纳头、后继头等），并在算术任务中表现优异，与SAE在可解释性上相当，但在电路发现上更优。

Conclusion: Lorsa在可解释性和电路发现方面表现优异，为Transformer注意力机制提供了新的分析工具。

Abstract: We propose Low-Rank Sparse Attention (Lorsa), a sparse replacement model of
Transformer attention layers to disentangle original Multi Head Self Attention
(MHSA) into individually comprehensible components. Lorsa is designed to
address the challenge of attention superposition to understand
attention-mediated interaction between features in different token positions.
We show that Lorsa heads find cleaner and finer-grained versions of previously
discovered MHSA behaviors like induction heads, successor heads and attention
sink behavior (i.e., heavily attending to the first token). Lorsa and Sparse
Autoencoder (SAE) are both sparse dictionary learning methods applied to
different Transformer components, and lead to consistent findings in many ways.
For instance, we discover a comprehensive family of arithmetic-specific Lorsa
heads, each corresponding to an atomic operation in Llama-3.1-8B. Automated
interpretability analysis indicates that Lorsa achieves parity with SAE in
interpretability while Lorsa exhibits superior circuit discovery properties,
especially for features computed collectively by multiple MHSA heads. We also
conduct extensive experiments on architectural design ablation, Lorsa scaling
law and error analysis.

</details>

### [90] [Federated learning, ethics, and the double black box problem in medical AI](https://arxiv.org/abs/2504.20656)
*Joshua Hatherley,Anders Søgaard,Angela Ballantyne,Ruben Pauwels*

Main category: cs.LG

TLDR: 本文探讨了医疗联邦学习（FL）的伦理风险，指出其存在“联邦不透明性”问题，可能导致双重黑箱问题，并强调需克服关键挑战以实现伦理可行性。


<details>
  <summary>Details</summary>
Motivation: 医疗FL虽能保护患者隐私，但其伦理风险未被充分研究，本文旨在填补这一空白。

Method: 通过分析医疗FL的“联邦不透明性”及其引发的双重黑箱问题，评估其伦理风险。

Result: 医疗FL的预期益处可能被夸大，存在伦理挑战。

Conclusion: 需解决关键挑战，才能使医疗FL在伦理上可行。

Abstract: Federated learning (FL) is a machine learning approach that allows multiple
devices or institutions to collaboratively train a model without sharing their
local data with a third-party. FL is considered a promising way to address
patient privacy concerns in medical artificial intelligence. The ethical risks
of medical FL systems themselves, however, have thus far been underexamined.
This paper aims to address this gap. We argue that medical FL presents a new
variety of opacity -- federation opacity -- that, in turn, generates a
distinctive double black box problem in healthcare AI. We highlight several
instances in which the anticipated benefits of medical FL may be exaggerated,
and conclude by highlighting key challenges that must be overcome to make FL
ethically feasible in medicine.

</details>

### [91] [Quantum-Enhanced Hybrid Reinforcement Learning Framework for Dynamic Path Planning in Autonomous Systems](https://arxiv.org/abs/2504.20660)
*Sahil Tomar,Shamshe Alam,Sandeep Kumar,Amit Mathur*

Main category: cs.LG

TLDR: 提出了一种量子-经典混合强化学习框架，结合量子计算的并行性，显著减少训练时间并提升复杂环境中的适应性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂动态环境中传统强化学习训练时间长、适应性不足的问题。

Method: 通过量子计算生成鲁棒的Q表和转向成本估计，并与经典强化学习结合。

Result: 模拟和实际测试（如IIT Delhi校园）显示路径效率、轨迹平滑度和任务成功率显著提升。

Conclusion: 该框架在复杂不可预测环境中具有实时自主导航潜力。

Abstract: In this paper, a novel quantum classical hybrid framework is proposed that
synergizes quantum with Classical Reinforcement Learning. By leveraging the
inherent parallelism of quantum computing, the proposed approach generates
robust Q tables and specialized turn cost estimations, which are then
integrated with a classical Reinforcement Learning pipeline. The Classical
Quantum fusion results in rapid convergence of training, reducing the training
time significantly and improved adaptability in scenarios featuring static,
dynamic, and moving obstacles. Simulator based evaluations demonstrate
significant enhancements in path efficiency, trajectory smoothness, and mission
success rates, underscoring the potential of framework for real time,
autonomous navigation in complex and unpredictable environments. Furthermore,
the proposed framework was tested beyond simulations on practical scenarios,
including real world map data such as the IIT Delhi campus, reinforcing its
potential for real time, autonomous navigation in complex and unpredictable
environments.

</details>

### [92] [SFi-Former: Sparse Flow Induced Attention for Graph Transformer](https://arxiv.org/abs/2504.20666)
*Zhonghao Li,Ji Shi,Xinming Zhang,Miao Zhang,Bo Li*

Main category: cs.LG

TLDR: 论文提出了一种名为SFi-attention的新型注意力机制，通过最小化基于网络流和l1正则化的能量函数来学习稀疏模式，解决了图变换器（GTs）因密集注意力导致的弱归纳偏置、过拟合和过度全局化问题。基于此，设计了SFi-Former模型，在多种图数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 图变换器（GTs）在处理长程依赖图数据时表现优异，但密集注意力机制导致弱归纳偏置、过拟合和过度全局化问题。

Method: 提出SFi-attention机制，通过最小化基于网络流和l1正则化的能量函数学习稀疏模式，并设计SFi-Former模型利用稀疏注意力生成稀疏网络流。

Result: 在GNN Benchmark和LRGB数据集上取得竞争性性能，且泛化误差较小，表明模型不易过拟合。

Conclusion: SFi-Former通过稀疏注意力机制有效解决了GTs的问题，并在实验中表现出优越性能。

Abstract: Graph Transformers (GTs) have demonstrated superior performance compared to
traditional message-passing graph neural networks in many studies, especially
in processing graph data with long-range dependencies. However, GTs tend to
suffer from weak inductive bias, overfitting and over-globalizing problems due
to the dense attention. In this paper, we introduce SFi-attention, a novel
attention mechanism designed to learn sparse pattern by minimizing an energy
function based on network flows with l1-norm regularization, to relieve those
issues caused by dense attention. Furthermore, SFi-Former is accordingly
devised which can leverage the sparse attention pattern of SFi-attention to
generate sparse network flows beyond adjacency matrix of graph data.
Specifically, SFi-Former aggregates features selectively from other nodes
through flexible adaptation of the sparse attention, leading to a more robust
model. We validate our SFi-Former on various graph datasets, especially those
graph data exhibiting long-range dependencies. Experimental results show that
our SFi-Former obtains competitive performance on GNN Benchmark datasets and
SOTA performance on LongRange Graph Benchmark (LRGB) datasets. Additionally,
our model gives rise to smaller generalization gaps, which indicates that it is
less prone to over-fitting. Click here for codes.

</details>

### [93] [Explanations Go Linear: Interpretable and Individual Latent Encoding for Post-hoc Explainability](https://arxiv.org/abs/2504.20667)
*Simone Piaggesi,Riccardo Guidotti,Fosca Giannotti,Dino Pedreschi*

Main category: cs.LG

TLDR: ILLUME是一种基于表示学习的灵活可解释框架，结合全局代理模型和实例特定线性变换，提供局部和全局解释，解决了传统代理方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 黑盒机器学习模型的可解释性需求日益增长，传统代理方法在局部和全局解释中存在计算成本高或难以捕捉复杂行为的局限性。

Method: ILLUME框架结合全局训练的代理模型和通过元编码器学习的实例特定线性变换，生成局部和全局解释。

Result: 实验证明ILLUME能生成准确、鲁棒且忠实于黑盒模型的特征归因和决策规则。

Conclusion: ILLUME提供了一个统一的解释框架，有效解决了传统代理方法的局限性。

Abstract: Post-hoc explainability is essential for understanding black-box machine
learning models. Surrogate-based techniques are widely used for local and
global model-agnostic explanations but have significant limitations. Local
surrogates capture non-linearities but are computationally expensive and
sensitive to parameters, while global surrogates are more efficient but
struggle with complex local behaviors. In this paper, we present ILLUME, a
flexible and interpretable framework grounded in representation learning, that
can be integrated with various surrogate models to provide explanations for any
black-box classifier. Specifically, our approach combines a globally trained
surrogate with instance-specific linear transformations learned with a
meta-encoder to generate both local and global explanations. Through extensive
empirical evaluations, we demonstrate the effectiveness of ILLUME in producing
feature attributions and decision rules that are not only accurate but also
robust and faithful to the black-box, thus providing a unified explanation
framework that effectively addresses the limitations of traditional surrogate
methods.

</details>

### [94] [What's Wrong with Your Synthetic Tabular Data? Using Explainable AI to Evaluate Generative Models](https://arxiv.org/abs/2504.20687)
*Jan Kapar,Niklas Koenen,Martin Jullum*

Main category: cs.LG

TLDR: 论文提出了一种利用可解释AI（XAI）技术评估合成表格数据质量的方法，通过分析分类器的特征重要性和特征效应，揭示合成数据的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据质量评估指标存在冲突且无法具体指出问题，需要更透明和深入的分析方法。

Method: 应用XAI技术（如特征重要性、部分依赖图、Shapley值和反事实解释）分析二元检测分类器，揭示合成数据的分布差异。

Result: 该方法能发现合成数据中的不一致性、不现实依赖或缺失模式，优于传统评估技术。

Conclusion: XAI方法提高了合成数据评估的透明度，为诊断和改进合成数据质量提供了更深入的见解。

Abstract: Evaluating synthetic tabular data is challenging, since they can differ from
the real data in so many ways. There exist numerous metrics of synthetic data
quality, ranging from statistical distances to predictive performance, often
providing conflicting results. Moreover, they fail to explain or pinpoint the
specific weaknesses in the synthetic data. To address this, we apply
explainable AI (XAI) techniques to a binary detection classifier trained to
distinguish real from synthetic data. While the classifier identifies
distributional differences, XAI concepts such as feature importance and feature
effects, analyzed through methods like permutation feature importance, partial
dependence plots, Shapley values and counterfactual explanations, reveal why
synthetic data are distinguishable, highlighting inconsistencies, unrealistic
dependencies, or missing patterns. This interpretability increases transparency
in synthetic data evaluation and provides deeper insights beyond conventional
metrics, helping diagnose and improve synthetic data quality. We apply our
approach to two tabular datasets and generative models, showing that it
uncovers issues overlooked by standard evaluation techniques.

</details>

### [95] [Unsupervised Surrogate Anomaly Detection](https://arxiv.org/abs/2504.20733)
*Simon Klüttermann,Tim Katzke,Emmanuel Müller*

Main category: cs.LG

TLDR: 论文提出了一种名为DEAN的无监督异常检测算法，通过学习正常数据的神经网络表示来检测异常，并在121个基准数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 受工程中类似概念的启发，研究如何通过学习正常数据的模式来检测异常。

Method: 提出了一种名为DEAN的算法，基于一组最优代理模型的公理设计。

Result: 在121个基准数据集上验证了DEAN的性能，优于19种现有方法，并展示了其可扩展性和可靠性。

Conclusion: DEAN是一种高效的异常检测方法，满足代理模型的最优公理，并在实验中表现出色。

Abstract: In this paper, we study unsupervised anomaly detection algorithms that learn
a neural network representation, i.e. regular patterns of normal data, which
anomalies are deviating from. Inspired by a similar concept in engineering, we
refer to our methodology as surrogate anomaly detection. We formalize the
concept of surrogate anomaly detection into a set of axioms required for
optimal surrogate models and propose a new algorithm, named DEAN (Deep Ensemble
ANomaly detection), designed to fulfill these criteria. We evaluate DEAN on 121
benchmark datasets, demonstrating its competitive performance against 19
existing methods, as well as the scalability and reliability of our method.

</details>

### [96] [Intelligent Task Offloading in VANETs: A Hybrid AI-Driven Approach for Low-Latency and Energy Efficiency](https://arxiv.org/abs/2504.20735)
*Tariq Qayyum,Asadullah Tariq,Muhammad Ali,Mohamed Adel Serhani,Zouheir Trabelsi,Maite López-Sánchez*

Main category: cs.LG

TLDR: 提出了一种结合监督学习、强化学习和粒子群优化的混合AI框架，用于VANET中的任务卸载和资源分配，显著降低了延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: VANET的高动态性导致网络条件不可预测、高延迟、能源效率低下和任务失败，需要智能解决方案。

Method: 整合监督学习预测最优卸载策略，强化学习实现自适应决策，粒子群优化优化延迟和能耗。

Result: 仿真显示框架显著降低延迟和能耗，提高任务成功率和网络吞吐量。

Conclusion: 该框架为动态车载环境中的实时应用提供了高效、可扩展的解决方案。

Abstract: Vehicular Ad-hoc Networks (VANETs) are integral to intelligent transportation
systems, enabling vehicles to offload computational tasks to nearby roadside
units (RSUs) and mobile edge computing (MEC) servers for real-time processing.
However, the highly dynamic nature of VANETs introduces challenges, such as
unpredictable network conditions, high latency, energy inefficiency, and task
failure. This research addresses these issues by proposing a hybrid AI
framework that integrates supervised learning, reinforcement learning, and
Particle Swarm Optimization (PSO) for intelligent task offloading and resource
allocation. The framework leverages supervised models for predicting optimal
offloading strategies, reinforcement learning for adaptive decision-making, and
PSO for optimizing latency and energy consumption. Extensive simulations
demonstrate that the proposed framework achieves significant reductions in
latency and energy usage while improving task success rates and network
throughput. By offering an efficient, and scalable solution, this framework
sets the foundation for enhancing real-time applications in dynamic vehicular
environments.

</details>

### [97] [DDPS: Discrete Diffusion Posterior Sampling for Paths in Layered Graphs](https://arxiv.org/abs/2504.20754)
*Hao Luan,See-Kiong Ng,Chun Kai Ling*

Main category: cs.LG

TLDR: 本文研究了在分层图中生成路径的问题，使用离散扩散模型并确保生成的样本是有效路径。提出了一种称为PALM的简单有效表示方法，并通过分类器引导优化路径生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成模型领域表现优异，但现有方法很少处理生成样本的显式约束问题。本文旨在解决在分层图中生成路径的约束问题。

Method: 提出PALM表示方法，并结合分类器引导技术，无需重新训练扩散模型即可优化路径生成。

Result: 初步实验表明，该方法在生成满足路径约束的样本上优于其他不显式考虑约束的方法。

Conclusion: PALM表示和分类器引导的结合有效解决了分层图中路径生成的约束问题，为扩散模型的应用提供了新思路。

Abstract: Diffusion models form an important class of generative models today,
accounting for much of the state of the art in cutting edge AI research. While
numerous extensions beyond image and video generation exist, few of such
approaches address the issue of explicit constraints in the samples generated.
In this paper, we study the problem of generating paths in a layered graph (a
variant of a directed acyclic graph) using discrete diffusion models, while
guaranteeing that our generated samples are indeed paths. Our approach utilizes
a simple yet effective representation for paths which we call the padded
adjacency-list matrix (PALM). In addition, we show how to effectively perform
classifier guidance, which helps steer the sampled paths to specific preferred
edges without any retraining of the diffusion model. Our preliminary results
show that empirically, our method outperforms alternatives which do not
explicitly account for path constraints.

</details>

### [98] [JTreeformer: Graph-Transformer via Latent-Diffusion Model for Molecular Generation](https://arxiv.org/abs/2504.20770)
*Ji Shi,Chengxun Xie,Zhonghao Li,Xinming Zhang,Miao Zhang*

Main category: cs.LG

TLDR: 本文提出了一种基于图变换器的分子生成框架JTreeformer，通过将图生成转化为连接树生成，结合GCN与多头注意力编码器，以及有向无环GCN解码器，显著提升了分子生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于变换器的图解码器难以有效利用图信息，限制了分子生成的拓扑结构表达能力。

Method: 结合GCN与多头注意力作为编码器，集成有向无环GCN到变换器解码器中，并在潜在空间中引入扩散模型。

Result: 实验表明，JTreeformer在分子生成任务上优于现有方法。

Conclusion: 该框架为药物发现提供了有前景的工具。

Abstract: The discovery of new molecules based on the original chemical molecule
distributions is of great importance in medicine. The graph transformer, with
its advantages of high performance and scalability compared to traditional
graph networks, has been widely explored in recent research for applications of
graph structures. However, current transformer-based graph decoders struggle to
effectively utilize graph information, which limits their capacity to leverage
only sequences of nodes rather than the complex topological structures of
molecule graphs. This paper focuses on building a graph transformer-based
framework for molecular generation, which we call \textbf{JTreeformer} as it
transforms graph generation into junction tree generation. It combines GCN
parallel with multi-head attention as the encoder. It integrates a directed
acyclic GCN into a graph-based Transformer to serve as a decoder, which can
iteratively synthesize the entire molecule by leveraging information from the
partially constructed molecular structure at each step. In addition, a
diffusion model is inserted in the latent space generated by the encoder, to
enhance the efficiency and effectiveness of sampling further. The empirical
results demonstrate that our novel framework outperforms existing molecule
generation methods, thus offering a promising tool to advance drug discovery
(https://anonymous.4open.science/r/JTreeformer-C74C).

</details>

### [99] [Evaluating Effects of Augmented SELFIES for Molecular Understanding Using QK-LSTM](https://arxiv.org/abs/2504.20789)
*Collin Beaudoin,Swaroop Ghosh*

Main category: cs.LG

TLDR: 论文提出了一种混合量子-经典模型QK-LSTM，用于分子性质和副作用预测，并首次分析了增强SMILES和SELFIES在量子领域的效果。


<details>
  <summary>Details</summary>
Motivation: 药物开发中分子性质和副作用的识别耗时且关键，传统方法存在风险，机器学习尤其是量子-经典混合模型提供了创新解决方案。

Method: QK-LSTM结合量子核函数与经典LSTM，通过高维量子特征空间捕捉序列数据的复杂非线性模式，减少参数需求。同时研究了增强SMILES和SELFIES的效果。

Result: 增强SELFIES在经典和量子-经典混合领域分别带来5.97%和5.91%的显著性能提升。

Conclusion: QK-LSTM和增强SELFIES为分子性质预测和副作用识别提供了高效新方法，具有重要应用潜力。

Abstract: Identifying molecular properties, including side effects, is a critical yet
time-consuming step in drug development. Failing to detect these side effects
before regulatory submission can result in significant financial losses and
production delays, and overlooking them during the regulatory review can lead
to catastrophic consequences. This challenge presents an opportunity for
innovative machine learning approaches, particularly hybrid quantum-classical
models like the Quantum Kernel-Based Long Short-Term Memory (QK-LSTM) network.
The QK-LSTM integrates quantum kernel functions into the classical LSTM
framework, enabling the capture of complex, non-linear patterns in sequential
data. By mapping input data into a high-dimensional quantum feature space, the
QK-LSTM model reduces the need for large parameter sets, allowing for model
compression without sacrificing accuracy in sequence-based tasks. Recent
advancements have been made in the classical domain using augmented variations
of the Simplified Molecular Line-Entry System (SMILES). However, to the best of
our knowledge, no research has explored the impact of augmented SMILES in the
quantum domain, nor the role of augmented Self-Referencing Embedded Strings
(SELFIES) in either classical or hybrid quantum-classical settings. This study
presents the first analysis of these approaches, providing novel insights into
their potential for enhancing molecular property prediction and side effect
identification. Results reveal that augmenting SELFIES yields in statistically
significant improvements from SMILES by a 5.97% improvement for the classical
domain and a 5.91% improvement for the hybrid quantum-classical domain.

</details>

### [100] [Q-Fusion: Diffusing Quantum Circuits](https://arxiv.org/abs/2504.20794)
*Collin Beaudoin,Swaroop Ghosh*

Main category: cs.LG

TLDR: 提出了一种基于扩散模型的量子架构搜索方法，用于自动生成量子电路，解决了NISQ设备的限制和量子算法设计的复杂性。


<details>
  <summary>Details</summary>
Motivation: 量子计算和量子机器学习潜力巨大，但受限于NISQ设备的硬件限制和量子算法设计的高门槛，需要自动化工具简化流程。

Method: 采用扩散模型结合LayerDAG框架生成量子电路，区别于其他基于LLM、RL或VAE的方法。

Result: 模型能100%生成有效的量子电路输出。

Conclusion: 该方法为量子算法设计提供了高效自动化解决方案，具有实际应用潜力。

Abstract: Quantum computing holds great potential for solving socially relevant and
computationally complex problems. Furthermore, quantum machine learning (QML)
promises to rapidly improve our current machine learning capabilities. However,
current noisy intermediate-scale quantum (NISQ) devices are constrained by
limitations in the number of qubits and gate counts, which hinder their full
capabilities. Furthermore, the design of quantum algorithms remains a laborious
task, requiring significant domain expertise and time. Quantum Architecture
Search (QAS) aims to streamline this process by automatically generating novel
quantum circuits, reducing the need for manual intervention. In this paper, we
propose a diffusion-based algorithm leveraging the LayerDAG framework to
generate new quantum circuits. This method contrasts with other approaches that
utilize large language models (LLMs), reinforcement learning (RL), variational
autoencoders (VAE), and similar techniques. Our results demonstrate that the
proposed model consistently generates 100% valid quantum circuit outputs.

</details>

### [101] [The When and How of Target Variable Transformations](https://arxiv.org/abs/2504.20821)
*Loren Nuyts,Jesse Davis*

Main category: cs.LG

TLDR: 论文强调目标变量变换在机器学习中的重要性，提供实用案例、通用规则及适用情境下的变换建议。


<details>
  <summary>Details</summary>
Motivation: 数据准备阶段对模型学习至关重要，但目标变量变换的影响常被忽视。本文旨在填补这一空白。

Method: 通过案例展示目标变量变换的实用性，提出通用规则，并讨论不同情境下的适用变换。

Result: 目标变量变换能显著提升模型性能，需根据具体情境选择合适的变换方法。

Conclusion: 目标变量变换是机器学习中不可忽视的一环，未来研究应更重视其应用与优化。

Abstract: The machine learning pipeline typically involves the iterative process of (1)
collecting the data, (2) preparing the data, (3) learning a model, and (4)
evaluating a model. Practitioners recognize the importance of the data
preparation phase in terms of its impact on the ability to learn accurate
models. In this regard, significant attention is often paid to manipulating the
feature set (e.g., selection, transformations, dimensionality reduction). A
point that is less well appreciated is that transformations on the target
variable can also have a large impact on whether it is possible to learn a
suitable model. These transformations may include accounting for
subject-specific biases (e.g., in how someone uses a rating scale), contexts
(e.g., population size effects), and general trends (e.g., inflation). However,
this point has received a much more cursory treatment in the existing
literature. The goal of this paper is three-fold. First, we aim to highlight
the importance of this problem by showing when transforming the target variable
has been useful in practice. Second, we will provide a set of generic ``rules
of thumb'' that indicate situations when transforming the target variable may
be needed. Third, we will discuss which transformations should be considered in
a given situation.

</details>

### [102] [An approach to melodic segmentation and classification based on filtering with the Haar-wavelet](https://arxiv.org/abs/2504.20822)
*Gissel Velarde,Tillman Weyde,David Meredith*

Main category: cs.LG

TLDR: 提出了一种基于Haar小波变换的旋律分类与分割方法，在巴赫作品分类中表现优于未滤波信号，但在荷兰民谣分类中与基于多特征的字符串匹配方法相比稍逊。


<details>
  <summary>Details</summary>
Motivation: 探索一种更有效的旋律分类与分割方法，以提升对音乐作品的分析能力。

Method: 使用Haar小波变换滤波音高信号，通过局部极值或零交叉点分割旋律，再用k近邻算法分类。

Result: 在巴赫作品分类中表现优于未滤波信号，但在荷兰民谣分类中表现一般。

Conclusion: 该方法在特定任务中有效，但需进一步优化以提升通用性。

Abstract: We present a novel method of classification and segmentation of melodies in
symbolic representation. The method is based on filtering pitch as a signal
over time with the Haar-wavelet, and we evaluate it on two tasks. The filtered
signal corresponds to a single-scale signal ws from the continuous Haar wavelet
transform. The melodies are first segmented using local maxima or
zero-crossings of w_s. The segments of w_s are then classified using the
k-nearest neighbour algorithm with Euclidian and city-block distances. The
method proves more effective than using unfiltered pitch signals and
Gestalt-based segmentation when used to recognize the parent works of segments
from Bach's Two-Part Inventions (BWV 772-786). When used to classify 360 Dutch
folk tunes into 26 tune families, the performance of the method is comparable
to the use of pitch signals, but not as good as that of string-matching methods
based on multiple features.

</details>

### [103] [Hybrid Quantum Recurrent Neural Network For Remaining Useful Life Prediction](https://arxiv.org/abs/2504.20823)
*Olga Tsurkan,Aleksandra Konstantinova,Aleksandr Sedykh,Dmitrii Zhiganov,Arsenii Senokosov,Daniil Tarpanov,Matvei Anoshin,Leonid Fedichkin*

Main category: cs.LG

TLDR: 提出了一种混合量子循环神经网络框架，用于预测喷气发动机剩余使用寿命，在NASA数据集上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 航空航天领域的预测性维护需要准确估计喷气发动机的剩余使用寿命，现有方法在高频成分学习上存在不足。

Method: 结合量子长短期记忆层和经典密集层，利用量子深度注入电路替代传统线性变换，提升高频成分学习能力。

Result: 混合量子循环神经网络在均方根误差和平均绝对误差上比传统循环神经网络提升5%，且优于随机森林、卷积神经网络和多层感知器。

Conclusion: 混合量子-经典方法在有限数据条件下具有潜力，为预测性维护任务提供了新思路。

Abstract: Predictive maintenance in aerospace heavily relies on accurate estimation of
the remaining useful life of jet engines. In this paper, we introduce a Hybrid
Quantum Recurrent Neural Network framework, combining Quantum Long Short-Term
Memory layers with classical dense layers for Remaining Useful Life forecasting
on NASA's Commercial Modular Aero-Propulsion System Simulation dataset. Each
Quantum Long Short-Term Memory gate replaces conventional linear
transformations with Quantum Depth-Infused circuits, allowing the network to
learn high-frequency components more effectively. Experimental results
demonstrate that, despite having fewer trainable parameters, the Hybrid Quantum
Recurrent Neural Network achieves up to a 5% improvement over a Recurrent
Neural Network based on stacked Long Short-Term Memory layers in terms of mean
root mean squared error and mean absolute error. Moreover, a thorough
comparison of our method with established techniques, including Random Forest,
Convolutional Neural Network, and Multilayer Perceptron, demonstrates that our
approach, which achieves a Root Mean Squared Error of 15.46, surpasses these
baselines by approximately 13.68%, 16.21%, and 7.87%, respectively.
Nevertheless, it remains outperformed by certain advanced joint architectures.
Our findings highlight the potential of hybrid quantum-classical approaches for
robust time-series forecasting under limited data conditions, offering new
avenues for enhancing reliability in predictive maintenance tasks.

</details>

### [104] [Reinforcement Learning for LLM Reasoning Under Memory Constraints](https://arxiv.org/abs/2504.20834)
*Alan Lee,Harry Tong*

Main category: cs.LG

TLDR: 论文提出两种内存高效的强化学习方法（S-GRPO和T-SPMO），用于在资源受限条件下提升大语言模型的推理能力，显著提高了SVAMP基准和多位数乘法任务的性能。


<details>
  <summary>Details</summary>
Motivation: 在学术环境中，由于内存和计算资源的限制，需要开发高效的强化学习方法以提升大语言模型的推理能力。

Method: 提出了S-GRPO（内存高效的GRPO变体）和T-SPMO（基于令牌前缀匹配的细粒度信用分配策略），并在单块40GB GPU上使用LoRA微调进行实验。

Result: 在Qwen2-1.5B模型上，两种方法将SVAMP基准准确率从46%提升至70%以上，T-SPMO在多位数乘法任务中表现优异。

Conclusion: 内存高效的强化学习方法在资源受限条件下能显著提升模型性能，并可能通过正则化作用稳定训练。

Abstract: We explore reinforcement learning (RL) techniques to enhance reasoning within
targeted problem spaces in large language models (LLMs) under memory and
compute constraints. Our focus is on critic-free methods compatible with LoRA
fine-tuning on a single 40GB GPU, a common limitation in academic settings. We
introduce S-GRPO, a memory-efficient variant of Group Relative Policy
Optimization, and T-SPMO, a token-level prefix matching strategy for
fine-grained credit assignment. Despite limited resources, when used to
fine-tune Qwen2-1.5B both methods significantly improve SVAMP benchmark
accuracy from 46% to above 70% using LoRA training. T-SPMO also excels in
multi-digit multiplication tasks, underscoring the potential of RL fine-tuning
under hardware constraints. Additionally, we find that our full-token GRPO
baseline under LoRA fine-tuning did not improve model performance (compared to
base model) on either task, suggesting that our memory-efficient methods may
act as a form of regularization that stabilizes training when only a small
subset of parameters are updated.

</details>

### [105] [Tabular Data Adapters: Improving Outlier Detection for Unlabeled Private Data](https://arxiv.org/abs/2504.20862)
*Dayananda Herurkar,Jörn Hees,Vesselin Tzvetkov,Andreas Dengel*

Main category: cs.LG

TLDR: 提出了一种名为Tabular Data Adapters（TDA）的新方法，用于为无标签的表格数据生成软标签，以解决私有数据集在异常检测任务中面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 深度学习在大型公共数据集上表现优异，但在私有数据集上应用时，常因结构差异、领域偏移和标签缺失而遇到困难。

Method: 通过识别统计上相似的公共数据集，并利用共享自编码器将私有数据转换为与现有公共模型兼容的格式，生成弱标签。

Result: 在50个不同领域的表格数据集上实验表明，该方法比基线方法提供更准确的标注，同时减少计算时间。

Conclusion: TDA为公共研究模型与工业应用之间的差距提供了一种可扩展、高效且经济的解决方案。

Abstract: The remarkable success of Deep Learning approaches is often based and
demonstrated on large public datasets. However, when applying such approaches
to internal, private datasets, one frequently faces challenges arising from
structural differences in the datasets, domain shift, and the lack of labels.
In this work, we introduce Tabular Data Adapters (TDA), a novel method for
generating soft labels for unlabeled tabular data in outlier detection tasks.
By identifying statistically similar public datasets and transforming private
data (based on a shared autoencoder) into a format compatible with
state-of-the-art public models, our approach enables the generation of weak
labels. It thereby can help to mitigate the cold start problem of labeling by
basing on existing outlier detection models for public datasets. In experiments
on 50 tabular datasets across different domains, we demonstrate that our method
is able to provide more accurate annotations than baseline approaches while
reducing computational time. Our approach offers a scalable, efficient, and
cost-effective solution, to bridge the gap between public research models and
real-world industrial applications.

</details>

### [106] [Return Capping: Sample-Efficient CVaR Policy Gradient Optimisation](https://arxiv.org/abs/2504.20887)
*Harry Mead,Clarissa Costen,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TLDR: 论文提出了一种改进的条件风险价值（CVaR）优化方法，通过限制训练轨迹的总回报而非直接丢弃，提高了样本效率。


<details>
  <summary>Details</summary>
Motivation: 当前方法在优化CVaR时丢弃大量轨迹，导致样本效率低下。

Method: 重新定义CVaR优化问题，限制训练轨迹的总回报，而非直接丢弃。

Result: 实验证明，该方法在多个环境中表现优于基线。

Conclusion: 通过合理设置回报上限，新方法在保持问题等价性的同时提高了性能。

Abstract: When optimising for conditional value at risk (CVaR) using policy gradients
(PG), current methods rely on discarding a large proportion of trajectories,
resulting in poor sample efficiency. We propose a reformulation of the CVaR
optimisation problem by capping the total return of trajectories used in
training, rather than simply discarding them, and show that this is equivalent
to the original problem if the cap is set appropriately. We show, with
empirical results in an number of environments, that this reformulation of the
problem results in consistently improved performance compared to baselines.

</details>

### [107] [Does Feedback Help in Bandits with Arm Erasures?](https://arxiv.org/abs/2504.20894)
*Merve Karakas,Osama Hanna,Lin F. Yang,Christina Fragouli*

Main category: cs.LG

TLDR: 研究分布式多臂老虎机问题，探讨在通信受限网络中反馈对遗憾边界的影响。


<details>
  <summary>Details</summary>
Motivation: 随着多臂老虎机算法在通信受限网络中的应用增加，研究在臂擦除信道下的性能表现。

Method: 考虑学习者通过擦除信道与代理通信，代理可以反馈是否收到臂请求，从而学习者知道实际播放的臂。

Result: 证明擦除反馈并未改善最坏情况下的遗憾上界，遗憾下界为Ω(√KT + K/(1-ε))。

Conclusion: 反馈虽未改变遗憾边界阶数，但简化了算法设计，可能改善常数项性能。

Abstract: We study a distributed multi-armed bandit (MAB) problem over arm erasure
channels, motivated by the increasing adoption of MAB algorithms over
communication-constrained networks. In this setup, the learner communicates the
chosen arm to play to an agent over an erasure channel with probability
$\epsilon \in [0,1)$; if an erasure occurs, the agent continues pulling the
last successfully received arm; the learner always observes the reward of the
arm pulled. In past work, we considered the case where the agent cannot convey
feedback to the learner, and thus the learner does not know whether the arm
played is the requested or the last successfully received one. In this paper,
we instead consider the case where the agent can send feedback to the learner
on whether the arm request was received, and thus the learner exactly knows
which arm was played. Surprisingly, we prove that erasure feedback does not
improve the worst-case regret upper bound order over the previously studied
no-feedback setting. In particular, we prove a regret lower bound of
$\Omega(\sqrt{KT} + K / (1 - \epsilon))$, where $K$ is the number of arms and
$T$ the time horizon, that matches no-feedback upper bounds up to logarithmic
factors. We note however that the availability of feedback enables simpler
algorithm designs that may achieve better constants (albeit not better order)
regret bounds; we design one such algorithm and evaluate its performance
numerically.

</details>

### [108] [Evaluating Generative Models for Tabular Data: Novel Metrics and Benchmarking](https://arxiv.org/abs/2504.20900)
*Dayananda Herurkar,Ahmad Ali,Andreas Dengel*

Main category: cs.LG

TLDR: 论文提出三种新评估指标（FAED、FPCAD、RFIS）用于表格数据生成模型，实验表明FAED能有效捕捉现有指标忽略的问题。


<details>
  <summary>Details</summary>
Motivation: 表格数据生成模型的评估存在挑战，现有指标不全面，需新方法。

Method: 提出FAED、FPCAD、RFIS三种新指标，并在三个网络入侵检测数据集上实验。

Result: FAED表现最佳，FPCAD需改进，新框架为表格数据生成模型评估提供实用方案。

Conclusion: 新指标和框架为表格数据生成模型评估提供更全面的解决方案。

Abstract: Generative models have revolutionized multiple domains, yet their application
to tabular data remains underexplored. Evaluating generative models for tabular
data presents unique challenges due to structural complexity, large-scale
variability, and mixed data types, making it difficult to intuitively capture
intricate patterns. Existing evaluation metrics offer only partial insights,
lacking a comprehensive measure of generative performance. To address this
limitation, we propose three novel evaluation metrics: FAED, FPCAD, and RFIS.
Our extensive experimental analysis, conducted on three standard network
intrusion detection datasets, compares these metrics with established
evaluation methods such as Fidelity, Utility, TSTR, and TRTS. Our results
demonstrate that FAED effectively captures generative modeling issues
overlooked by existing metrics. While FPCAD exhibits promising performance,
further refinements are necessary to enhance its reliability. Our proposed
framework provides a robust and practical approach for assessing generative
models in tabular data applications.

</details>

### [109] [MOSIC: Model-Agnostic Optimal Subgroup Identification with Multi-Constraint for Improved Reliability](https://arxiv.org/abs/2504.20908)
*Wenxin Chen,Weishen Pan,Kyra Gan,Fei Wang*

Main category: cs.LG

TLDR: 提出了一种模型无关的框架，用于在多约束条件下识别最优亚组，解决了现有方法未能统一处理亚组大小和混杂因素平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 个性化医疗中，识别从特定治疗中受益的亚组是关键挑战，现有方法未能同时满足亚组大小和混杂因素平衡的临床需求。

Method: 将组合问题重新表述为无约束的极小极大优化问题，通过梯度下降上升算法求解，并证明其收敛性。

Result: 在合成和真实数据集上验证了方法的有效性，能够满足多约束条件，实现更高的治疗效果和更好的混杂因素平衡。

Conclusion: 该方法稳定且灵活，适用于多种模型和技术，为个性化医疗提供了实用的亚组识别工具。

Abstract: Identifying subgroups that benefit from specific treatments using
observational data is a critical challenge in personalized medicine. Most
existing approaches solely focus on identifying a subgroup with an improved
treatment effect. However, practical considerations, such as ensuring a minimum
subgroup size for representativeness or achieving sufficient confounder balance
for reliability, are also important for making findings clinically meaningful
and actionable. While some studies address these constraints individually, none
offer a unified approach to handle them simultaneously. To bridge this gap, we
propose a model-agnostic framework for optimal subgroup identification under
multiple constraints. We reformulate this combinatorial problem as an
unconstrained min-max optimization problem with novel modifications and solve
it by a gradient descent ascent algorithm. We further prove its convergence to
a feasible and locally optimal solution. Our method is stable and highly
flexible, supporting various models and techniques for estimating and
optimizing treatment effectiveness with observational data. Extensive
experiments on both synthetic and real-world datasets demonstrate its
effectiveness in identifying subgroups that satisfy multiple constraints,
achieving higher treatment effects and better confounder balancing results
across different group sizes.

</details>

### [110] [Statistical and Predictive Analysis to Identify Risk Factors and Effects of Post COVID-19 Syndrome](https://arxiv.org/abs/2504.20915)
*Milad Leyli-abadi,Jean-Patrick Brunet,Axel Tahmasebimoradi*

Main category: cs.LG

TLDR: 论文研究了长期COVID的症状持续与多种因素的关系，并通过数据驱动方法预测其强度。神经网络表现最佳，关键预测因素包括嗅觉丧失、头痛等。


<details>
  <summary>Details</summary>
Motivation: 探索长期COVID的持续症状与患者特征、疫苗接种时间等因素的关系，以提供干预指导。

Method: 使用线性模型、随机森林、梯度提升和神经网络等方法，基于Lifelines COVID-19队列数据进行统计和预测分析。

Result: 神经网络表现最佳（MAPE为19%），关键预测因素包括嗅觉丧失、头痛、肌肉疼痛和疫苗接种时间。

Conclusion: 研究为理解长期COVID和制定针对性干预措施提供了重要见解。

Abstract: Based on recent studies, some COVID-19 symptoms can persist for months after
infection, leading to what is termed long COVID. Factors such as vaccination
timing, patient characteristics, and symptoms during the acute phase of
infection may contribute to the prolonged effects and intensity of long COVID.
Each patient, based on their unique combination of factors, develops a specific
risk or intensity of long COVID. In this work, we aim to achieve two
objectives: (1) conduct a statistical analysis to identify relationships
between various factors and long COVID, and (2) perform predictive analysis of
long COVID intensity using these factors. We benchmark and interpret various
data-driven approaches, including linear models, random forests, gradient
boosting, and neural networks, using data from the Lifelines COVID-19 cohort.
Our results show that Neural Networks (NN) achieve the best performance in
terms of MAPE, with predictions averaging 19\% error. Additionally,
interpretability analysis reveals key factors such as loss of smell, headache,
muscle pain, and vaccination timing as significant predictors, while chronic
disease and gender are critical risk factors. These insights provide valuable
guidance for understanding long COVID and developing targeted interventions.

</details>

### [111] [Improvements of Dark Experience Replay and Reservoir Sampling towards Better Balance between Consolidation and Plasticity](https://arxiv.org/abs/2504.20932)
*Taisuke Kobayashi*

Main category: cs.LG

TLDR: 本文提出了改进DER和RS的策略，以平衡记忆巩固和可塑性，提升持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习是自主代理的关键能力，但现有方法DER和RS存在权衡问题，影响学习效果。

Method: 改进DER（自动权重调整、错误数据屏蔽、输出修正）和RS（概率泛化、多缓冲分层、数据选择性忽略）。

Result: 在回归、分类和强化学习任务中验证了性能提升。

Conclusion: 改进方法通过平衡记忆巩固和可塑性，显著提升了学习效果。

Abstract: Continual learning is the one of the most essential abilities for autonomous
agents, which can incrementally learn daily-life skills. For this ultimate
goal, a simple but powerful method, dark experience replay (DER), has been
proposed recently. DER mitigates catastrophic forgetting, in which the skills
acquired in the past are unintentionally forgotten, by stochastically storing
the streaming data in a reservoir sampling (RS) buffer and by relearning them
or retaining the past outputs for them. However, since DER considers multiple
objectives, it will not function properly without appropriate weighting of
them. In addition, the ability to retain past outputs inhibits learning if the
past outputs are incorrect due to distribution shift or other effects. This is
due to a tradeoff between memory consolidation and plasticity. The tradeoff is
hidden even in the RS buffer, which gradually stops storing new data for new
skills in it as data is continuously passed to it. To alleviate the tradeoff
and achieve better balance, this paper proposes improvement strategies to each
of DER and RS. Specifically, DER is improved with automatic adaptation of
weights, block of replaying erroneous data, and correction of past outputs. RS
is also improved with generalization of acceptance probability, stratification
of plural buffers, and intentional omission of unnecessary data. These
improvements are verified through multiple benchmarks including regression,
classification, and reinforcement learning problems. As a result, the proposed
methods achieve steady improvements in learning performance by balancing the
memory consolidation and plasticity.

</details>

### [112] [Scenario-based Compositional Verification of Autonomous Systems with Neural Perception](https://arxiv.org/abs/2504.20942)
*Christopher Watson,Rajeev Alur,Divya Gopinath,Ravi Mangal,Corina S. Pasareanu*

Main category: cs.LG

TLDR: 提出了一种基于概率验证框架的自主系统验证方法，通过场景建模、概率抽象和符号推理解决深度神经网络感知系统的复杂性挑战。


<details>
  <summary>Details</summary>
Motivation: 由于深度神经网络的复杂性和环境条件的不确定性，自主系统的形式化验证具有挑战性。

Method: 采用场景建模分解任务，构建概率抽象表示感知性能，并通过符号推理和加速证明规则进行高效验证。

Result: 在两个案例研究中验证了方法的有效性：飞机滑行引导系统和F1Tenth自动驾驶汽车模型。

Conclusion: 提出的概率验证框架能够有效应对自主系统验证的复杂性，适用于多变环境条件下的系统验证。

Abstract: Recent advances in deep learning have enabled the development of autonomous
systems that use deep neural networks for perception. Formal verification of
these systems is challenging due to the size and complexity of the perception
DNNs as well as hard-to-quantify, changing environment conditions. To address
these challenges, we propose a probabilistic verification framework for
autonomous systems based on the following key concepts: (1) Scenario-based
Modeling: We decompose the task (e.g., car navigation) into a composition of
scenarios, each representing a different environment condition. (2)
Probabilistic Abstractions: For each scenario, we build a compact abstraction
of perception based on the DNN's performance on an offline dataset that
represents the scenario's environment condition. (3) Symbolic Reasoning and
Acceleration: The abstractions enable efficient compositional verification of
the autonomous system via symbolic reasoning and a novel acceleration proof
rule that bounds the error probability of the system under arbitrary variations
of environment conditions. We illustrate our approach on two case studies: an
experimental autonomous system that guides airplanes on taxiways using
high-dimensional perception DNNs and a simulation model of an F1Tenth
autonomous car using LiDAR observations.

</details>

### [113] [Deep Learning Characterizes Depression and Suicidal Ideation from Eye Movements](https://arxiv.org/abs/2504.20944)
*Kleanthis Avramidis,Woojae Jeong,Aditya Kommineni,Sudarsana R. Kadiri,Marcus Ma,Colin McDaniel,Myzelle Hughes,Thomas McGee,Elsi Kaiser,Dani Byrd,Assal Habibi,B. Rael Cahn,Idan A. Blank,Kristina Lerman,Takfarinas Medani,Richard M. Leahy,Shrikanth Narayanan*

Main category: cs.LG

TLDR: 研究利用眼动追踪技术结合深度学习模型，通过分析情绪句子阅读时的眼动模式，成功识别抑郁和自杀倾向，为心理健康评估提供了客观工具。


<details>
  <summary>Details</summary>
Motivation: 抑郁症和自杀倾向缺乏客观生物标志物，目前主要依赖主观报告和临床访谈，亟需更客观的筛查方法。

Method: 记录126名年轻人在阅读情绪句子时的眼动数据，开发深度学习模型，分别分析积极和消极情绪试验，利用2D时间序列表征眼动变化。

Result: 模型对抑郁和自杀倾向的识别AUC分别为0.793和0.826，对抑郁与自杀倾向的区分AUC为0.609。消极情绪句子效果最显著。

Conclusion: 眼动追踪可作为心理健康评估的客观工具，情绪刺激对眼动控制的调节作用显著。

Abstract: Identifying physiological and behavioral markers for mental health conditions
is a longstanding challenge in psychiatry. Depression and suicidal ideation, in
particular, lack objective biomarkers, with screening and diagnosis primarily
relying on self-reports and clinical interviews. Here, we investigate eye
tracking as a potential marker modality for screening purposes. Eye movements
are directly modulated by neuronal networks and have been associated with
attentional and mood-related patterns; however, their predictive value for
depression and suicidality remains unclear. We recorded eye-tracking sequences
from 126 young adults as they read and responded to affective sentences, and
subsequently developed a deep learning framework to predict their clinical
status. The proposed model included separate branches for trials of positive
and negative sentiment, and used 2D time-series representations to account for
both intra-trial and inter-trial variations. We were able to identify
depression and suicidal ideation with an area under the receiver operating
curve (AUC) of 0.793 (95% CI: 0.765-0.819) against healthy controls, and
suicidality specifically with 0.826 AUC (95% CI: 0.797-0.852). The model also
exhibited moderate, yet significant, accuracy in differentiating depressed from
suicidal participants, with 0.609 AUC (95% CI 0.571-0.646). Discriminative
patterns emerge more strongly when assessing the data relative to response
generation than relative to the onset time of the final word of the sentences.
The most pronounced effects were observed for negative-sentiment sentences,
that are congruent to depressed and suicidal participants. Our findings
highlight eye tracking as an objective tool for mental health assessment and
underscore the modulatory impact of emotional stimuli on cognitive processes
affecting oculomotor control.

</details>

### [114] [AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security](https://arxiv.org/abs/2504.20965)
*Zikui Cai,Shayan Shabihi,Bang An,Zora Che,Brian R. Bartoldson,Bhavya Kailkhura,Tom Goldstein,Furong Huang*

Main category: cs.LG

TLDR: AegisLLM是一种多代理防御系统，通过协作代理（协调者、偏转器、响应者和评估者）实时防御对抗攻击和信息泄露，无需模型重训练。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法多为静态且依赖模型修改，AegisLLM旨在通过动态代理协作提升实时适应性和鲁棒性。

Method: 采用多代理结构化工作流，结合自动提示优化（如DSPy），在测试时扩展代理推理系统。

Result: 在WMDP遗忘基准上实现近乎完美的遗忘（仅需20个训练样本和300次LM调用），在越狱基准上比基线模型提升51%。

Conclusion: AegisLLM展示了动态代理推理优于静态防御，成为运行时防御的强有力替代方案。

Abstract: We introduce AegisLLM, a cooperative multi-agent defense against adversarial
attacks and information leakage. In AegisLLM, a structured workflow of
autonomous agents - orchestrator, deflector, responder, and evaluator -
collaborate to ensure safe and compliant LLM outputs, while self-improving over
time through prompt optimization. We show that scaling agentic reasoning system
at test-time - both by incorporating additional agent roles and by leveraging
automated prompt optimization (such as DSPy)- substantially enhances robustness
without compromising model utility. This test-time defense enables real-time
adaptability to evolving attacks, without requiring model retraining.
Comprehensive evaluations across key threat scenarios, including unlearning and
jailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning
benchmark, AegisLLM achieves near-perfect unlearning with only 20 training
examples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve
51% improvement compared to the base model on StrongReject, with false refusal
rates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our
results highlight the advantages of adaptive, agentic reasoning over static
defenses, establishing AegisLLM as a strong runtime alternative to traditional
approaches based on model modifications. Code is available at
https://github.com/zikuicai/aegisllm

</details>

### [115] [Softpick: No Attention Sink, No Massive Activations with Rectified Softmax](https://arxiv.org/abs/2504.20966)
*Zayd M. K. Zuhri,Erland Hilman Fuadi,Alham Fikri Aji*

Main category: cs.LG

TLDR: Softpick是一种替代softmax的注意力机制，解决了注意力下沉和大激活问题，保持性能的同时显著降低激活峰度和稀疏性，并在量化场景中表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决softmax在注意力机制中导致的注意力下沉和大激活问题，同时探索量化、低精度训练等新可能性。

Method: 提出softpick作为softmax的替代方案，实验验证其在340M参数模型中的表现。

Result: softpick在标准基准测试中性能与softmax相当，但实现了0%的注意力下沉率，显著降低激活峰度和稀疏性，量化时表现更优。

Conclusion: softpick为量化、低精度训练、稀疏优化等提供了新可能性，代码已开源。

Abstract: We introduce softpick, a rectified, not sum-to-one, drop-in replacement for
softmax in transformer attention mechanisms that eliminates attention sink and
massive activations. Our experiments with 340M parameter models demonstrate
that softpick maintains performance parity with softmax on standard benchmarks
while achieving 0% sink rate. The softpick transformer produces hidden states
with significantly lower kurtosis (340 vs 33,510) and creates sparse attention
maps (46.97% sparsity). Models using softpick consistently outperform softmax
when quantized, with particularly pronounced advantages at lower bit
precisions. Our analysis and discussion shows how softpick has the potential to
open new possibilities for quantization, low-precision training, sparsity
optimization, pruning, and interpretability. Our code is available at
https://github.com/zaydzuhri/softpick-attention.

</details>

### [116] [Equivariant non-linear maps for neural networks on homogeneous spaces](https://arxiv.org/abs/2504.20974)
*Elias Nyholm,Oscar Carlsson,Maurice Weiler,Daniel Persson*

Main category: cs.LG

TLDR: 本文提出了一种用于齐次空间上非线性等变神经网络层的新框架，扩展了线性设置下的理论，并证明了其普适性。


<details>
  <summary>Details</summary>
Motivation: 受非线性层（如自注意力或输入依赖核）的实证成功启发，作者希望将线性等变层的理论推广到非线性设置。

Method: 推导了非线性层需满足的广义可操纵性约束，并证明了构造的普适性。

Result: 框架适用于多种常见等变网络架构（如G-CNNs、隐式可操纵核网络、基于注意力的Transformer等）。

Conclusion: 该框架为未来等变神经网络层的设计提供了理论支持，并展示了其广泛适用性。

Abstract: This paper presents a novel framework for non-linear equivariant neural
network layers on homogeneous spaces. The seminal work of Cohen et al. on
equivariant $G$-CNNs on homogeneous spaces characterized the representation
theory of such layers in the linear setting, finding that they are given by
convolutions with kernels satisfying so-called steerability constraints.
Motivated by the empirical success of non-linear layers, such as self-attention
or input dependent kernels, we set out to generalize these insights to the
non-linear setting. We derive generalized steerability constraints that any
such layer needs to satisfy and prove the universality of our construction. The
insights gained into the symmetry-constrained functional dependence of
equivariant operators on feature maps and group elements informs the design of
future equivariant neural network layers. We demonstrate how several common
equivariant network architectures - $G$-CNNs, implicit steerable kernel
networks, conventional and relative position embedded attention based
transformers, and LieTransformers - may be derived from our framework.

</details>

### [117] [Hubs and Spokes Learning: Efficient and Scalable Collaborative Machine Learning](https://arxiv.org/abs/2504.20988)
*Atul Sharma,Kavindu Herath,Saurabh Bagchi,Chaoyue Liu,Somali Chaterji*

Main category: cs.LG

TLDR: HSL是一种结合联邦学习和去中心化学习优势的新型协作学习框架，通过双层通信结构避免单点故障，并在相同或更低通信预算下优于现有P2PL框架。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习的单点故障问题，同时提升去中心化学习的性能，适用于资源受限系统。

Method: 采用双层通信结构（Hubs和Spokes），优化通信效率和节点共识。

Result: 在相同通信预算下性能优于ELL，低预算时也能匹配其性能（如400边达到ELL 1000边的效果）。

Conclusion: HSL适合大规模协作学习，具有高效性和实用性。

Abstract: We introduce the Hubs and Spokes Learning (HSL) framework, a novel paradigm
for collaborative machine learning that combines the strengths of Federated
Learning (FL) and Decentralized Learning (P2PL). HSL employs a two-tier
communication structure that avoids the single point of failure inherent in FL
and outperforms the state-of-the-art P2PL framework, Epidemic Learning Local
(ELL). At equal communication budgets (total edges), HSL achieves higher
performance than ELL, while at significantly lower communication budgets, it
can match ELL's performance. For instance, with only 400 edges, HSL reaches the
same test accuracy that ELL achieves with 1000 edges for 100 peers (spokes) on
CIFAR-10, demonstrating its suitability for resource-constrained systems. HSL
also achieves stronger consensus among nodes after mixing, resulting in
improved performance with fewer training rounds. We substantiate these claims
through rigorous theoretical analyses and extensive experimental results,
showcasing HSL's practicality for large-scale collaborative learning.

</details>

### [118] [Toward Efficient Exploration by Large Language Model Agents](https://arxiv.org/abs/2504.20997)
*Dilip Arumugam,Thomas L. Griffiths*

Main category: cs.LG

TLDR: 论文探讨了基于大型语言模型（LLM）的强化学习（RL）代理在数据效率上的挑战，提出了一种显式实现现有RL算法（后验采样）的方法，以解决探索问题。


<details>
  <summary>Details</summary>
Motivation: 设计基于LLM的自主决策代理，但现有方法在数据效率和探索能力上存在不足，需要改进。

Method: 显式实现后验采样强化学习算法（PSRL），而非依赖微调或上下文学习。

Result: 在需要谨慎探索的自然语言任务中，该方法显著提升了数据效率。

Conclusion: 通过显式实现已知算法，LLM代理在探索和数据效率上表现更优。

Abstract: A burgeoning area within reinforcement learning (RL) is the design of
sequential decision-making agents centered around large language models (LLMs).
While autonomous decision-making agents powered by modern LLMs could facilitate
numerous real-world applications, such successes demand agents that are capable
of data-efficient RL. One key obstacle to achieving data efficiency in RL is
exploration, a challenge that we demonstrate many recent proposals for LLM
agent designs struggle to contend with. Meanwhile, classic algorithms from the
RL literature known to gracefully address exploration require technical
machinery that can be challenging to operationalize in purely natural language
settings. In this work, rather than relying on finetuning or in-context
learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate
how LLMs can be used to explicitly implement an existing RL algorithm
(Posterior Sampling for Reinforcement Learning) whose capacity for
statistically-efficient exploration is already well-studied. We offer empirical
results demonstrating how our LLM-based implementation of a known,
data-efficient RL algorithm can be considerably more effective in natural
language tasks that demand prudent exploration.

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [119] [It's the same but not the same: Do LLMs distinguish Spanish varieties?](https://arxiv.org/abs/2504.20049)
*Marina Mayor-Rocher,Cristina Pozo,Nina Melero,Gonzalo Martínez,María Grandury,Pedro Reviriego*

Main category: cs.CL

TLDR: 研究评估了九种语言模型识别七种西班牙语变体的能力，发现GPT-4o是唯一能识别西班牙语多样性的模型。


<details>
  <summary>Details</summary>
Motivation: 西班牙语存在丰富的地域变体，但现有语言模型对其多样性的识别能力尚不明确。

Method: 通过多选题测试评估模型对七种西班牙语变体的识别能力。

Result: 所有模型中，半岛西班牙语变体识别最佳，GPT-4o是唯一能识别多样性的模型。

Conclusion: GPT-4o在识别西班牙语多样性方面表现突出，其他模型需改进。

Abstract: In recent years, large language models (LLMs) have demonstrated a high
capacity for understanding and generating text in Spanish. However, with five
hundred million native speakers, Spanish is not a homogeneous language but
rather one rich in diatopic variations spanning both sides of the Atlantic. For
this reason, in this study, we evaluate the ability of nine language models to
identify and distinguish the morphosyntactic and lexical peculiarities of seven
varieties of Spanish (Andean, Antillean, Continental Caribbean, Chilean,
Peninsular, Mexican and Central American and Rioplatense) through a
multiple-choice test. The results indicate that the Peninsular Spanish variety
is the best identified by all models and that, among them, GPT-4o is the only
model capable of recognizing the variability of the Spanish language.
  --
  En los \'ultimos a\~nos, los grandes modelos de lenguaje (LLMs, por sus
siglas en ingl\'es) han demostrado una alta capacidad para comprender y generar
texto en espa\~nol. Sin embargo, con quinientos millones de hablantes nativos,
la espa\~nola no es una lengua homog\'enea, sino rica en variedades
diat\'opicas que se extienden a ambos lados del Atl\'antico. Por todo ello,
evaluamos en este trabajo la capacidad de nueve modelos de lenguaje de
identificar y discernir las peculiaridades morfosint\'acticas y l\'exicas de
siete variedades de espa\~nol (andino, antillano, caribe\~no continental,
chileno, espa\~nol peninsular, mexicano y centroamericano y rioplatense)
mediante un test de respuesta m\'ultiple. Los resultados obtenidos indican que
la variedad de espa\~nol peninsular es la mejor identificada por todos los
modelos y que, de entre todos, GPT-4o es el \'unico modelo capaz de identificar
la variabilidad de la lengua espa\~nola.

</details>

### [120] [Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts](https://arxiv.org/abs/2504.20051)
*Frances Laureano De Leon,Harish Tayyar Madabushi,Mark G. Lee*

Main category: cs.CL

TLDR: 该研究评估了大型语言模型在处理多词表达歧义时的表现，发现即使是最新模型如GPT-4，在多词表达的检测和语义任务中表现不佳，尤其是在低频语境下。


<details>
  <summary>Details</summary>
Motivation: 多词表达具有非组合意义和句法不规则性，其字面和惯用意义可能导致显著语义变化。尽管大型语言模型在许多任务中表现优异，但其处理此类语言微妙之处的能力尚不明确。

Method: 研究通过评估英语、葡萄牙语和加利西亚语中的多词表达，使用新颖的代码切换数据集和任务，测试模型在低频语境下的表现。

Result: 大型语言模型在多词表达任务中表现不佳，尤其是GPT-4未能超越xlm-roBERTa-base基线，在新任务中表现尤其差。

Conclusion: 多词表达，尤其是歧义性表达，仍是语言模型的挑战。

Abstract: Multiword expressions, characterised by non-compositional meanings and
syntactic irregularities, are an example of nuanced language. These expressions
can be used literally or idiomatically, leading to significant changes in
meaning. While large language models have demonstrated strong performance
across many tasks, their ability to handle such linguistic subtleties remains
uncertain. Therefore, this study evaluates how state-of-the-art language models
process the ambiguity of potentially idiomatic multiword expressions,
particularly in contexts that are less frequent, where models are less likely
to rely on memorisation. By evaluating models across in Portuguese and
Galician, in addition to English, and using a novel code-switched dataset and a
novel task, we find that large language models, despite their strengths,
struggle with nuanced language. In particular, we find that the latest models,
including GPT-4, fail to outperform the xlm-roBERTa-base baselines in both
detection and semantic tasks, with especially poor performance on the novel
tasks we introduce, despite its similarity to existing tasks. Overall, our
results demonstrate that multiword expressions, especially those which are
ambiguous, continue to be a challenge to models.

</details>

### [121] [Understanding and Mitigating Risks of Generative AI in Financial Services](https://arxiv.org/abs/2504.20086)
*Sebastian Gehrmann,Claire Huang,Xian Teng,Sergei Yurovski,Iyanuoluwa Shode,Chirag S. Patel,Arjun Bhorkar,Naveen Thomas,John Doucette,David Rosenberg,Mark Dredze,David Rabinowitz*

Main category: cs.CL

TLDR: 论文探讨了金融领域生成式AI的内容安全风险，提出了风险分类法，并评估了现有开源防护措施的不足。


<details>
  <summary>Details</summary>
Motivation: 当前对生成式AI的安全性评估多集中于通用领域（如毒性、偏见等），而忽视了专业领域的法律和监管要求。本文旨在填补金融服务业的内容安全研究空白。

Method: 提出了金融领域的AI内容风险分类法，并通过红队测试数据评估现有开源防护措施的效果。

Result: 现有防护措施未能检测出大部分讨论的内容风险。

Conclusion: 金融领域的AI内容安全需结合行业法规和监管要求，现有技术防护措施需改进。

Abstract: To responsibly develop Generative AI (GenAI) products, it is critical to
define the scope of acceptable inputs and outputs. What constitutes a "safe"
response is an actively debated question. Academic work puts an outsized focus
on evaluating models by themselves for general purpose aspects such as
toxicity, bias, and fairness, especially in conversational applications being
used by a broad audience. In contrast, less focus is put on considering
sociotechnical systems in specialized domains. Yet, those specialized systems
can be subject to extensive and well-understood legal and regulatory scrutiny.
These product-specific considerations need to be set in industry-specific laws,
regulations, and corporate governance requirements. In this paper, we aim to
highlight AI content safety considerations specific to the financial services
domain and outline an associated AI content risk taxonomy. We compare this
taxonomy to existing work in this space and discuss implications of risk
category violations on various stakeholders. We evaluate how existing
open-source technical guardrail solutions cover this taxonomy by assessing them
on data collected via red-teaming activities. Our results demonstrate that
these guardrails fail to detect most of the content risks we discuss.

</details>

### [122] [Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models](https://arxiv.org/abs/2504.20157)
*Zae Myung Kim,Chanwoo Park,Vipul Raheja,Dongyeop Kang*

Main category: cs.CL

TLDR: 论文提出Meta Policy Optimization (MPO)框架，通过动态调整奖励模型的提示，解决奖励信号漏洞和依赖人工提示工程的问题。


<details>
  <summary>Details</summary>
Motivation: 现有奖励对齐方法易受奖励信号漏洞影响，且依赖人工设计的提示工程，效率低且不稳定。

Method: MPO引入元奖励模型，动态调整奖励模型的提示，提供自适应奖励信号，减少人工干预。

Result: MPO性能优于或等同手工设计的奖励提示，且适用于多种任务（如问答和数学推理）。

Conclusion: MPO为LLM的奖励对齐提供了更稳健和自适应的解决方案，代码和模型将公开。

Abstract: Reward-based alignment methods for large language models (LLMs) face two key
limitations: vulnerability to reward hacking, where models exploit flaws in the
reward signal; and reliance on brittle, labor-intensive prompt engineering when
LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a
framework that addresses these challenges by integrating a meta-reward model
that dynamically refines the reward model's prompt throughout training. In MPO,
the meta-reward model monitors the evolving training context and continuously
adjusts the reward model's prompt to maintain high alignment, providing an
adaptive reward signal that resists exploitation by the policy. This
meta-learning approach promotes a more stable policy optimization, and greatly
reduces the need for manual reward prompt design. It yields performance on par
with or better than models guided by extensively hand-crafted reward prompts.
Furthermore, we show that MPO maintains its effectiveness across diverse tasks,
such as question answering and mathematical reasoning, without requiring
specialized reward designs. Beyond standard RLAIF, MPO's meta-learning
formulation is readily extensible to higher-level alignment frameworks.
Overall, this method addresses theoretical and practical challenges in
reward-based RL alignment for LLMs, paving the way for more robust and
adaptable alignment strategies. The code and models will be publicly shared.

</details>

### [123] [MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools](https://arxiv.org/abs/2504.20168)
*Nishant Subramani,Jason Eisner,Justin Svegliato,Benjamin Van Durme,Yu Su,Sam Thomson*

Main category: cs.CL

TLDR: 论文提出了一种新型模型内部置信度估计器（MICE），通过解码语言模型的中间层并计算相似度得分，结合概率分类器评估置信度，显著提升了工具调用的实用性和安全性。


<details>
  <summary>Details</summary>
Motivation: 工具使用代理需要兼具实用性和安全性，而现有模型的置信度校准较差，因此需要一种更准确的置信度评估方法。

Method: MICE解码语言模型各中间层（使用logitLens），计算每层生成与最终输出的相似度得分，通过概率分类器评估置信度。

Result: 在STE数据集上，MICE在平滑预期校准误差上优于基线，显著提升了工具调用的预期效用，且能零样本泛化到未见API。

Conclusion: MICE是一种高效、通用的置信度估计方法，能显著提升工具调用的安全性和实用性，代码已开源。

Abstract: Tool-using agents that act in the world need to be both useful and safe.
Well-calibrated model confidences can be used to weigh the risk versus reward
of potential actions, but prior work shows that many models are poorly
calibrated. Inspired by interpretability literature exploring the internals of
models, we propose a novel class of model-internal confidence estimators (MICE)
to better assess confidence when calling tools. MICE first decodes from each
intermediate layer of the language model using logitLens and then computes
similarity scores between each layer's generation and the final output. These
features are fed into a learned probabilistic classifier to assess confidence
in the decoded output. On the simulated trial and error (STE) tool-calling
dataset using Llama3 models, we find that MICE beats or matches the baselines
on smoothed expected calibration error. Using MICE confidences to determine
whether to call a tool significantly improves over strong baselines on a new
metric, expected tool-calling utility. Further experiments show that MICE is
sample-efficient, can generalize zero-shot to unseen APIs, and results in
higher tool-calling utility in scenarios with varying risk levels. Our code is
open source, available at https://github.com/microsoft/mice_for_cats.

</details>

### [124] [A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports](https://arxiv.org/abs/2504.20220)
*Henning Schäfer,Cynthia S. Schmidt,Johannes Wutzkowsky,Kamil Lorek,Lea Reinartz,Johannes Rückert,Christian Temme,Britta Böckmann,Peter A. Horn,Christoph M. Friedrich*

Main category: cs.CL

TLDR: 提出一种开源流程，用于从扫描文档中提取和分类复选框数据，以减少人工转录错误和工作量。


<details>
  <summary>Details</summary>
Motivation: 尽管电子健康记录普及，许多流程仍依赖纸质文档，人工转录耗时且易出错。

Method: 整合复选框检测、多语言OCR和多语言视觉语言模型（VLMs）。

Result: 与2017至2024年的黄金标准相比，实现了高精度和召回率，减少了行政工作量。

Conclusion: 开源流程支持自托管解析复选框表单，提升效率和准确性。

Abstract: Despite the growing adoption of electronic health records, many processes
still rely on paper documents, reflecting the heterogeneous real-world
conditions in which healthcare is delivered. The manual transcription process
is time-consuming and prone to errors when transferring paper-based data to
digital formats. To streamline this workflow, this study presents an
open-source pipeline that extracts and categorizes checkbox data from scanned
documents. Demonstrated on transfusion reaction reports, the design supports
adaptation to other checkbox-rich document types. The proposed method
integrates checkbox detection, multilingual optical character recognition (OCR)
and multilingual vision-language models (VLMs). The pipeline achieves high
precision and recall compared against annually compiled gold-standards from
2017 to 2024. The result is a reduction in administrative workload and accurate
regulatory reporting. The open-source availability of this pipeline encourages
self-hosted parsing of checkbox forms.

</details>

### [125] [A Platform for Generating Educational Activities to Teach English as a Second Language](https://arxiv.org/abs/2504.20251)
*Aiala Rosá,Santiago Góngora,Juan Pablo Filevich,Ignacio Sastre,Laura Musto,Brian Carpenter,Luis Chiruzzo*

Main category: cs.CL

TLDR: 一个基于自然语言处理技术的英语教学平台，支持生成游戏和练习，并计划扩展图像和文本生成功能。


<details>
  <summary>Details</summary>
Motivation: 为英语作为外语教学提供多样化的教育活动，结合自然语言处理技术提升教学效果。

Method: 平台利用半自动生成和手动编辑的资源，支持教师输入文本生成复杂游戏或练习，并集成神经网络工具。

Result: 平台已部署并面向最终用户，解决了开发中的挑战，并计划迁移至更强大的服务器。

Conclusion: 平台展示了结合自然语言处理技术的潜力，未来将扩展功能并优化性能。

Abstract: We present a platform for the generation of educational activities oriented
to teaching English as a foreign language. The different activities -- games
and language practice exercises -- are strongly based on Natural Language
Processing techniques. The platform offers the possibility of playing
out-of-the-box games, generated from resources created semi-automatically and
then manually curated. It can also generate games or exercises of greater
complexity from texts entered by teachers, providing a stage of review and
edition of the generated content before use. As a way of expanding the variety
of activities in the platform, we are currently experimenting with image and
text generation. In order to integrate them and improve the performance of
other neural tools already integrated, we are working on migrating the platform
to a more powerful server. In this paper we describe the development of our
platform and its deployment for end users, discussing the challenges faced and
how we overcame them, and also detail our future work plans.

</details>

### [126] [Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi](https://arxiv.org/abs/2504.20276)
*Dandan Chen Kaptur,Yue Huang,Xuejun Ryan Ji,Yanhui Guo,Bradley Kaptur*

Main category: cs.CL

TLDR: 研究比较了GPT-4和Kimi在系统综述中的表现，发现其性能受数据量和问题复杂度影响。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）在系统综述中的表现，并与人工生成的代码进行对比。

Method: 通过比较LLM生成的代码与人工生成的代码，评估GPT-4和Kimi的性能。

Result: LLMs在系统综述中的表现因数据量和问题复杂度而异。

Conclusion: LLMs在系统综述中的应用需考虑数据量和问题复杂度的影响。

Abstract: This research delved into GPT-4 and Kimi, two Large Language Models (LLMs),
for systematic reviews. We evaluated their performance by comparing
LLM-generated codes with human-generated codes from a peer-reviewed systematic
review on assessment. Our findings suggested that the performance of LLMs
fluctuates by data volume and question complexity for systematic reviews.

</details>

### [127] [UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions](https://arxiv.org/abs/2504.20304)
*Xiulin Yang,Zhuoxuan Ju,Lanni Bu,Zoey Liu,Nathan Schneider*

Main category: cs.CL

TLDR: 本文介绍了UD-English-CHILDES，这是首个基于CHILDES数据的官方发布的Universal Dependencies树库，包含48k句子的统一标注，并提供了1M句子的银标准标注。


<details>
  <summary>Details</summary>
Motivation: CHILDES是广泛使用的儿童语言资源，但缺乏一致的UD标注标准。本文旨在填补这一空白。

Method: 整合了11名儿童及其照顾者的数据，统一标注，并在UD v2框架下验证现有标注。

Result: 生成了包含48k金标准句子和1M银标准句子的统一资源。

Conclusion: UD-English-CHILDES为计算和语言学研究提供了高质量的标注资源。

Abstract: CHILDES is a widely used resource of transcribed child and child-directed
speech. This paper introduces UD-English-CHILDES, the first officially released
Universal Dependencies (UD) treebank derived from previously
dependency-annotated CHILDES data with consistent and unified annotation
guidelines. Our corpus harmonizes annotations from 11 children and their
caregivers, totaling over 48k sentences. We validate existing gold-standard
annotations under the UD v2 framework and provide an additional 1M
silver-standard sentences, offering a consistent resource for computational and
linguistic research.

</details>

### [128] [Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation](https://arxiv.org/abs/2504.20323)
*Chao-Lin Liu,Po-Hsien Wu,Yi-Ting Yu*

Main category: cs.CL

TLDR: 提出了一种基于法律条款共引用的方法，用于解决法律推荐系统中标注数据不足的问题，并在劳动纠纷领域验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决法律推荐系统在专业领域（如劳动纠纷）中标注数据有限的问题。

Method: 利用法律条款在案例中的共引用关系建立相似性，并通过算法进行标注，结合文本嵌入模型和BiLSTM模块推荐相似案例。

Result: 验证了该方法能够有效推荐基于法律条款共引用相似性的劳动纠纷案例。

Conclusion: 该研究为法律文档的自动化标注技术提供了新思路，尤其在法律数据库有限的领域具有应用价值。

Abstract: This report addresses the challenge of limited labeled datasets for
developing legal recommender systems, particularly in specialized domains like
labor disputes. We propose a new approach leveraging the co-citation of legal
articles within cases to establish similarity and enable algorithmic
annotation. This method draws a parallel to the concept of case co-citation,
utilizing cited precedents as indicators of shared legal issues. To evaluate
the labeled results, we employ a system that recommends similar cases based on
plaintiffs' accusations, defendants' rebuttals, and points of disputes. The
evaluation demonstrates that the recommender, with finetuned text embedding
models and a reasonable BiLSTM module can recommend labor cases whose
similarity was measured by the co-citation of the legal articles. This research
contributes to the development of automated annotation techniques for legal
documents, particularly in areas with limited access to comprehensive legal
databases.

</details>

### [129] [Local Prompt Optimization](https://arxiv.org/abs/2504.20355)
*Yash Jain,Vishal Chowdhary*

Main category: cs.CL

TLDR: 论文提出了一种局部提示优化（LPO）方法，通过专注于优化提示中的关键令牌，显著提升了自动提示工程的性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法全局优化所有令牌，导致优化空间过大且指导不足。

Method: 提出LPO方法，识别并优化提示中的关键令牌，减少优化空间。

Result: 在数学推理（GSM8k和MultiArith）及BIG-bench Hard基准测试中表现显著提升，且收敛速度更快。

Conclusion: LPO方法有效解决了全局优化的问题，提升了提示优化的效率和性能。

Abstract: In recent years, the use of prompts to guide the output of Large Language
Models have increased dramatically. However, even the best of experts struggle
to choose the correct words to stitch up a prompt for the desired task. To
solve this, LLM driven prompt optimization emerged as an important problem.
Existing prompt optimization methods optimize a prompt globally, where in all
the prompt tokens have to be optimized over a large vocabulary while solving a
complex task. The large optimization space (tokens) leads to insufficient
guidance for a better prompt. In this work, we introduce Local Prompt
Optimization (LPO) that integrates with any general automatic prompt
engineering method. We identify the optimization tokens in a prompt and nudge
the LLM to focus only on those tokens in its optimization step. We observe
remarkable performance improvements on Math Reasoning (GSM8k and MultiArith)
and BIG-bench Hard benchmarks across various automatic prompt engineering
methods. Further, we show that LPO converges to the optimal prompt faster than
global methods.

</details>

### [130] [What Causes Knowledge Loss in Multilingual Language Models?](https://arxiv.org/abs/2504.20356)
*Maria Khelli,Samuel Cahyawijaya,Ayu Purwarianti,Genta Indra Winata*

Main category: cs.CL

TLDR: 研究探讨了多语言NLP模型中的灾难性遗忘问题，通过LoRA适配器评估参数共享对知识保留的影响。


<details>
  <summary>Details</summary>
Motivation: 传统方法在多语言场景中难以模拟现实情况，导致灾难性遗忘问题，研究旨在探索如何通过参数共享缓解这一问题。

Method: 使用52种语言和不同等级的LoRA适配器，评估非共享、部分共享和完全共享参数的效果。

Result: 非拉丁文字语言更容易发生灾难性遗忘，而拉丁文字语言在多语言迁移中表现更好。

Conclusion: 参数共享（尤其是通过适配器）可以缓解灾难性遗忘，同时保留先验知识。

Abstract: Cross-lingual transfer in natural language processing (NLP) models enhances
multilingual performance by leveraging shared linguistic knowledge. However,
traditional methods that process all data simultaneously often fail to mimic
real-world scenarios, leading to challenges like catastrophic forgetting, where
fine-tuning on new tasks degrades performance on previously learned ones. Our
study explores this issue in multilingual contexts, focusing on linguistic
differences affecting representational learning rather than just model
parameters. We experiment with 52 languages using LoRA adapters of varying
ranks to evaluate non-shared, partially shared, and fully shared parameters.
Our aim is to see if parameter sharing through adapters can mitigate forgetting
while preserving prior knowledge. We find that languages using non-Latin
scripts are more susceptible to catastrophic forgetting, whereas those written
in Latin script facilitate more effective cross-lingual transfer.

</details>

### [131] [DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation](https://arxiv.org/abs/2504.20371)
*Zhibo Man,Yuanmeng Chen,Yujie Zhang,Yufeng Chen,Jinan Xu*

Main category: cs.CL

TLDR: 该论文提出了一个评估框架DMDTEval，用于评估大语言模型（LLMs）在多领域翻译中的消歧能力，包括构建测试集、设计提示模板和精确指标，并揭示了关键发现。


<details>
  <summary>Details</summary>
Motivation: 多领域翻译（MDT）中词义的歧义性导致LLMs表现不佳，评估其消歧能力是一个开放问题。

Method: 构建多领域歧义词标注的测试集，设计多样化的提示模板和精确的消歧指标，评估多种提示策略在LLMs上的效果。

Result: 实验揭示了关键发现，为改进LLMs的消歧能力提供了方向。

Conclusion: DMDTEval框架为LLMs在多领域翻译中的消歧能力研究奠定了基础，并推动了进一步研究。

Abstract: Currently, Large Language Models (LLMs) have achieved remarkable results in
machine translation. However, their performance in multi-domain translation
(MDT) is less satisfactory; the meanings of words can vary across different
domains, highlighting the significant ambiguity inherent in MDT. Therefore,
evaluating the disambiguation ability of LLMs in MDT remains an open problem.
To this end, we present an evaluation and analysis of LLMs on disambiguation in
multi-domain translation (DMDTEval), our systematic evaluation framework
consisting of three critical aspects: (1) we construct a translation test set
with multi-domain ambiguous word annotation, (2) we curate a diverse set of
disambiguation prompting templates, and (3) we design precise disambiguation
metrics, and study the efficacy of various prompting strategies on multiple
state-of-the-art LLMs. Our extensive experiments reveal a number of crucial
findings that we believe will pave the way and also facilitate further research
in the critical area of improving the disambiguation of LLMs.

</details>

### [132] [On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?](https://arxiv.org/abs/2504.20444)
*Mika Hämäläinen*

Main category: cs.CL

TLDR: 研究比较了ChatGPT、Gemini和Claude三种商业LLM中的首因效应，通过类似Asch实验的方法，发现不同模型在形容词顺序影响偏好上表现不同。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM是否像人类一样受首因效应影响，即在信息呈现顺序不同时产生偏好差异。

Method: 采用Asch实验设计，测试LLM对形容词顺序（先正后负或先负后正）的偏好，分为同时呈现和分开呈现两种实验。

Result: ChatGPT在同时呈现时偏好先正后负，分开呈现时偏好先负后正；Gemini在分开呈现时偏好先负后正；Claude在同时呈现时拒绝选择，分开呈现时偏好先负后正。

Conclusion: 不同LLM对信息顺序的敏感性不同，首因效应在LLM中存在但表现各异。

Abstract: We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and
Claude. We do this by repurposing the famous experiment Asch (1946) conducted
using human subjects. The experiment is simple, given two candidates with equal
descriptions which one is preferred if one description has positive adjectives
first before negative ones and another description has negative adjectives
followed by positive ones. We test this in two experiments. In one experiment,
LLMs are given both candidates simultaneously in the same prompt, and in
another experiment, LLMs are given both candidates separately. We test all the
models with 200 candidate pairs. We found that, in the first experiment,
ChatGPT preferred the candidate with positive adjectives listed first, while
Gemini preferred both equally often. Claude refused to make a choice. In the
second experiment, ChatGPT and Claude were most likely to rank both candidates
equally. In the case where they did not give an equal rating, both showed a
clear preference to a candidate that had negative adjectives listed first.
Gemini was most likely to prefer a candidate with negative adjectives listed
first.

</details>

### [133] [Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs](https://arxiv.org/abs/2504.20451)
*Daniel Lee,Harsh Sharma,Jieun Han,Sunny Jeong,Alice Oh,Vered Shwartz*

Main category: cs.CL

TLDR: LLMs outperform traditional MT models in translating knowledge-intensive English-Korean texts but struggle with cultural adaptation of entities. Key issues include incorrect responses and entity name errors.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of preserving cultural and language-specific nuances in translating entity-rich texts between English and Korean.

Method: Evaluated 13 models (LLMs and MT models) using automatic metrics and human assessment by bilingual annotators, and constructed an error taxonomy.

Result: LLMs perform better than traditional MT systems but face difficulties with culturally adaptive entity translation. Performance varies by entity type and popularity.

Conclusion: Highlights gaps in automatic evaluation metrics and aims to inspire future work on culturally-nuanced machine translation.

Abstract: Translating knowledge-intensive and entity-rich text between English and
Korean requires transcreation to preserve language-specific and cultural
nuances beyond literal, phonetic or word-for-word conversion. We evaluate 13
models (LLMs and MT models) using automatic metrics and human assessment by
bilingual annotators. Our findings show LLMs outperform traditional MT systems
but struggle with entity translation requiring cultural adaptation. By
constructing an error taxonomy, we identify incorrect responses and entity name
errors as key issues, with performance varying by entity type and popularity
level. This work exposes gaps in automatic evaluation metrics and hope to
enable future work in completing culturally-nuanced machine translation.

</details>

### [134] [Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models](https://arxiv.org/abs/2504.20469)
*Enfa Fane,Mihai Surdeanu,Eduardo Blanco,Steven R. Corman*

Main category: cs.CL

TLDR: 论文评估了大型语言模型（LLMs）在零样本分类新闻叙事中实体框架角色的能力，通过分层方法和优化输入上下文与提示策略，取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 研究新闻叙事如何框架实体对媒体影响社会事件认知的重要性。

Method: 采用分层分类方法，先识别广泛角色再细化，并优化输入上下文与提示策略。

Result: 主角色准确率达89.4%，精确匹配率为34.5%，证明了方法的有效性。

Conclusion: 强调定制化提示设计和输入上下文优化对提升LLM在实体框架任务中表现的重要性。

Abstract: Understanding how news narratives frame entities is crucial for studying
media's impact on societal perceptions of events. In this paper, we evaluate
the zero-shot capabilities of large language models (LLMs) in classifying
framing roles. Through systematic experimentation, we assess the effects of
input context, prompting strategies, and task decomposition. Our findings show
that a hierarchical approach of first identifying broad roles and then
fine-grained roles, outperforms single-step classification. We also demonstrate
that optimal input contexts and prompts vary across task levels, highlighting
the need for subtask-specific strategies. We achieve a Main Role Accuracy of
89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of our
approach. Our findings emphasize the importance of tailored prompt design and
input context optimization for improving LLM performance in entity framing.

</details>

### [135] [Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training](https://arxiv.org/abs/2504.20484)
*Linjuan Wu,Haoran Wei,Huan Lin,Tianhao Li,Baosong Yang,Weiming Lu*

Main category: cs.CL

TLDR: CrossIC-PT是一种通过语义相关的双语文本增强跨语言迁移的简单可扩展方法，显著提升了多语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言迁移方法受限于并行资源，覆盖范围和领域有限，需要更高效的方法。

Method: 提出CrossIC-PT，通过语义相关的双语文本进行上下文预训练，并采用分段策略和滑动窗口机制保持上下文连贯性。

Result: 在三种模型和六种目标语言上，性能分别提升3.79%、3.99%和1.95%，数据增强后效果更佳。

Conclusion: CrossIC-PT是一种有效的跨语言迁移增强方法，具有简单性和可扩展性。

Abstract: Large language models (LLMs) exhibit remarkable multilingual capabilities
despite English-dominated pre-training, attributed to cross-lingual mechanisms
during pre-training. Existing methods for enhancing cross-lingual transfer
remain constrained by parallel resources, suffering from limited linguistic and
domain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT),
a simple and scalable approach that enhances cross-lingual transfer by
leveraging semantically related bilingual texts via simple next-word
prediction. We construct CrossIC-PT samples by interleaving semantic-related
bilingual Wikipedia documents into a single context window. To access window
size constraints, we implement a systematic segmentation policy to split long
bilingual document pairs into chunks while adjusting the sliding window
mechanism to preserve contextual coherence. We further extend data availability
through a semantic retrieval framework to construct CrossIC-PT samples from
web-crawled corpus. Experimental results demonstrate that CrossIC-PT improves
multilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and
Qwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%,
3.99%, and 1.95%, respectively, with additional improvements after data
augmentation.

</details>

### [136] [UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation](https://arxiv.org/abs/2504.20500)
*Huimin Lu,Masaru Isonuma,Junichiro Mori,Ichiro Sakata*

Main category: cs.CL

TLDR: UniDetox是一种通用的去毒方法，适用于多种大型语言模型（LLMs），无需针对不同模型单独调参。


<details>
  <summary>Details</summary>
Motivation: 现有去毒方法通常针对特定模型或模型家族，且需要在去毒效果和语言建模性能之间权衡调参。

Method: 提出一种基于对比解码的数据集蒸馏技术，生成合成文本数据，通过微调实现通用去毒。

Result: 实验表明，从GPT-2蒸馏的去毒文本可有效用于OPT、Falcon和LLaMA-2等更大模型，且无需单独调参。

Conclusion: UniDetox不仅实现了通用去毒，还减少了政治偏见内容，为LLMs去毒提供了新思路。

Abstract: We present UniDetox, a universally applicable method designed to mitigate
toxicity across various large language models (LLMs). Previous detoxification
methods are typically model-specific, addressing only individual models or
model families, and require careful hyperparameter tuning due to the trade-off
between detoxification efficacy and language modeling performance. In contrast,
UniDetox provides a detoxification technique that can be universally applied to
a wide range of LLMs without the need for separate model-specific tuning.
Specifically, we propose a novel and efficient dataset distillation technique
for detoxification using contrastive decoding. This approach distills
detoxifying representations in the form of synthetic text data, enabling
universal detoxification of any LLM through fine-tuning with the distilled
text. Our experiments demonstrate that the detoxifying text distilled from
GPT-2 can effectively detoxify larger models, including OPT, Falcon, and
LLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter
tuning for each model, as a single hyperparameter configuration can be
seamlessly applied across different models. Additionally, analysis of the
detoxifying text reveals a reduction in politically biased content, providing
insights into the attributes necessary for effective detoxification of LLMs.

</details>

### [137] [Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records](https://arxiv.org/abs/2504.20547)
*Jesus Lovon,Thouria Ben-Haddi,Jules Di Scala,Jose G. Moreno,Lynda Tamine*

Main category: cs.CL

TLDR: 论文提出将MIMIC-IV数据集整合到Hugging Face库中，并探讨了将EHR表格数据转换为文本的方法，实验表明微调的文本模型在患者死亡率任务上表现优于零样本LLM。


<details>
  <summary>Details</summary>
Motivation: 解决医疗领域缺乏标准化评估基准的问题，促进自然语言模型在健康相关任务中的应用。

Method: 整合MIMIC-IV数据到Hugging Face库，使用模板将EHR表格数据转换为文本，并对比微调文本模型与零样本LLM的性能。

Result: 微调文本模型在患者死亡率任务上表现优于零样本LLM，后者难以有效利用EHR表示。

Conclusion: 文本方法在医疗领域具有潜力，但需进一步改进零样本LLM的表现。

Abstract: The lack of standardized evaluation benchmarks in the medical domain for text
inputs can be a barrier to widely adopting and leveraging the potential of
natural language models for health-related downstream tasks. This paper
revisited an openly available MIMIC-IV benchmark for electronic health records
(EHRs) to address this issue. First, we integrate the MIMIC-IV data within the
Hugging Face datasets library to allow an easy share and use of this
collection. Second, we investigate the application of templates to convert EHR
tabular data to text. Experiments using fine-tuned and zero-shot LLMs on the
mortality of patients task show that fine-tuned text-based models are
competitive against robust tabular classifiers. In contrast, zero-shot LLMs
struggle to leverage EHR representations. This study underlines the potential
of text-based approaches in the medical field and highlights areas for further
improvement.

</details>

### [138] [BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters](https://arxiv.org/abs/2504.20552)
*Baz Roland,Kristina Malyseva,Anna Pappa,Tristan Cazenave*

Main category: cs.CL

TLDR: BrAIcht是一个基于德国LeoLM的AI对话代理，能够生成类似德国剧作家Bertolt Brecht风格的对话。


<details>
  <summary>Details</summary>
Motivation: 旨在通过AI技术模仿Bertolt Brecht的独特戏剧风格，丰富对话生成领域。

Method: 使用QLoRA技术对7B参数的LeoLM进行微调，数据集包括29部Brecht剧作和907部风格相似的德国戏剧。

Result: 基于BLEU分数和困惑度评估，BrAIcht在生成Brecht风格对话方面表现出色。

Conclusion: BrAIcht展示了利用参数高效微调技术生成特定风格对话的潜力。

Abstract: This project introduces BrAIcht, an AI conversational agent that creates
dialogues in the distinctive style of the famous German playwright Bertolt
Brecht. BrAIcht is fine-tuned using German LeoLM, a large language model with 7
billion parameters and a modified version of the base Llama2 suitable for
German language tasks. For fine-tuning, 29 plays of Bertolt Brecht and 907 of
other German plays that are stylistically similar to Bertolt Brecht are used to
form a more di-erse dataset. Due to the limited memory capacity, a
parameterefficient fine-tuning technique called QLoRA is implemented to train
the large language model. The results, based on BLEU score and perplexity, show
very promising performance of BrAIcht in generating dialogues in the style of
Bertolt Brecht.

</details>

### [139] [ClonEval: An Open Voice Cloning Benchmark](https://arxiv.org/abs/2504.20581)
*Iwona Christop,Tomasz Kuczyński,Marek Kubis*

Main category: cs.CL

TLDR: 提出了一种新的语音克隆文本转语音模型基准，包括评估协议、开源库和排行榜。


<details>
  <summary>Details</summary>
Motivation: 为语音克隆模型提供标准化的评估工具和平台，促进模型性能的比较和改进。

Method: 设计了评估协议和开源库，详细描述了评估流程，并展示了排行榜的组织方式。

Result: 提供了一个完整的语音克隆模型评估框架，包括工具和结果展示平台。

Conclusion: 该基准为语音克隆领域的研究提供了实用工具和标准化评估方法。

Abstract: We present a novel benchmark for voice cloning text-to-speech models. The
benchmark consists of an evaluation protocol, an open-source library for
assessing the performance of voice cloning models, and an accompanying
leaderboard. The paper discusses design considerations and presents a detailed
description of the evaluation procedure. The usage of the software library is
explained, along with the organization of results on the leaderboard.

</details>

### [140] [TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models](https://arxiv.org/abs/2504.20605)
*Mihai Nadas,Laura Diosan,Andrei Piscoran,Andreea Tomescu*

Main category: cs.CL

TLDR: TF1-EN-3M是一个由8B参数模型生成的300万英语寓言数据集，填补了现代NLP缺乏结构化道德故事库的空白。


<details>
  <summary>Details</summary>
Motivation: 现代NLP缺乏结合明确道德教训的大规模结构化叙事数据集，TF1-EN-3M旨在填补这一空白。

Method: 使用组合提示引擎生成遵循六槽模板的故事，并通过混合评估流程（GPT评分和无参考多样性指标）评估质量。

Result: 8B参数的Llama-3变体在质量和速度上表现最佳，生成成本低至每1000个故事13.5美分。

Conclusion: TF1-EN-3M为指令遵循、叙事智能等研究提供了资源，证明大规模道德叙事无需依赖专有巨型模型。

Abstract: Moral stories are a time-tested vehicle for transmitting values, yet modern
NLP lacks a large, structured corpus that couples coherent narratives with
explicit ethical lessons. We close this gap with TF1-EN-3M, the first open
dataset of three million English-language fables generated exclusively by
instruction-tuned models no larger than 8B parameters. Each story follows a
six-slot scaffold (character -> trait -> setting -> conflict -> resolution ->
moral), produced through a combinatorial prompt engine that guarantees genre
fidelity while covering a broad thematic space.
  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores
grammar, creativity, moral clarity, and template adherence with (ii)
reference-free diversity and readability metrics. Among ten open-weight
candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed
trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)
at approximately 13.5 cents per 1,000 fables.
  We release the dataset, generation code, evaluation scripts, and full
metadata under a permissive license, enabling exact reproducibility and cost
benchmarking. TF1-EN-3M opens avenues for research in instruction following,
narrative intelligence, value alignment, and child-friendly educational AI,
demonstrating that large-scale moral storytelling no longer requires
proprietary giant models.

</details>

### [141] [WenyanGPT: A Large Language Model for Classical Chinese Tasks](https://arxiv.org/abs/2504.20609)
*Xinyu Yao,Mengdi Wang,Bo Chen,Xiaobing Zhao*

Main category: cs.CL

TLDR: 本文提出了一种针对文言文的语言处理解决方案，通过预训练和指令微调构建了文言文专用大模型WenyanGPT，并开发了评估基准数据集WenyanBENCH。实验表明该模型在文言文任务上显著优于现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言处理模型主要针对现代汉语优化，在文言文处理上表现不足，阻碍了古代文献的传承与研究。

Method: 在LLaMA3-8B-Chinese模型基础上进行继续预训练和指令微调，构建文言文专用模型WenyanGPT，并开发评估数据集WenyanBENCH。

Result: 实验结果显示WenyanGPT在文言文任务上显著优于现有先进模型。

Conclusion: 通过公开模型训练数据、指令微调数据和评估数据集，推动文言文处理领域的进一步研究与发展。

Abstract: Classical Chinese, as the core carrier of Chinese culture, plays a crucial
role in the inheritance and study of ancient literature. However, existing
natural language processing models primarily optimize for Modern Chinese,
resulting in inadequate performance on Classical Chinese. This paper presents a
comprehensive solution for Classical Chinese language processing. By continuing
pre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we
construct a large language model, WenyanGPT, which is specifically designed for
Classical Chinese tasks. Additionally, we develop an evaluation benchmark
dataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that
WenyanGPT significantly outperforms current advanced LLMs in various Classical
Chinese tasks. We make the model's training data, instruction fine-tuning
data\footnote, and evaluation benchmark dataset publicly available to promote
further research and development in the field of Classical Chinese processing.

</details>

### [142] [Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations](https://arxiv.org/abs/2504.20643)
*Moran Mizrahi,Chen Shani,Gabriel Stanovsky,Dan Jurafsky,Dafna Shahaf*

Main category: cs.CL

TLDR: 本文提出了一种结合大型语言模型（LLMs）与结构化表示和认知启发操作的新方法，以生成更具创造性和多样性的想法。实验表明，该方法在烹饪领域的创意食谱生成中优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在众多任务中表现出色，但在创造力方面仍有不足。本文旨在通过结构化表示和认知启发操作提升LLMs的创造力。

Method: 提出了一种结合LLMs与结构化表示和认知启发操作的方法，通过重新组合现有想法的结构化表示来探索更抽象的想法空间。

Result: 在烹饪领域的实验中，该方法生成的食谱在多样性和新颖性上显著优于GPT-4o，且大多具有连贯性和可行性。

Conclusion: 该方法在创意生成方面表现优异，为AI结构化创造力的研究提供了新方向。

Abstract: Large Language Models (LLMs) excel at countless tasks, yet struggle with
creativity. In this paper, we introduce a novel approach that couples LLMs with
structured representations and cognitively inspired manipulations to generate
more creative and diverse ideas. Our notion of creativity goes beyond
superficial token-level variations; rather, we explicitly recombine structured
representations of existing ideas, allowing our algorithm to effectively
explore the more abstract landscape of ideas. We demonstrate our approach in
the culinary domain with DishCOVER, a model that generates creative recipes.
Experiments comparing our model's results to those of GPT-4o show greater
diversity. Domain expert evaluations reveal that our outputs, which are mostly
coherent and feasible culinary creations, significantly surpass GPT-4o in terms
of novelty, thus outperforming it in creative generation. We hope our work
inspires further research into structured creativity in AI.

</details>

### [143] [A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages](https://arxiv.org/abs/2504.20668)
*Ivan Vykopal,Martin Hyben,Robert Moro,Michal Gregor,Jakub Simko*

Main category: cs.CL

TLDR: 本文提出了一种利用大型语言模型（LLMs）检索和评估已核实声明的方法，以减少冗余工作并提升事实核查效率。


<details>
  <summary>Details</summary>
Motivation: 在线虚假信息泛滥，事实核查人员面临重复核查已核实声明的问题，增加了工作负担并延迟了对新声明的响应。

Method: 使用LLMs过滤无关的事实核查内容，生成简洁摘要和解释，帮助核查人员快速判断声明是否已被核实。

Result: 实验表明，LLMs能有效过滤无关内容，减少工作量并优化核查流程。

Conclusion: LLMs在事实核查中具有潜力，能显著提升效率和响应速度。

Abstract: Online disinformation poses a global challenge, placing significant demands
on fact-checkers who must verify claims efficiently to prevent the spread of
false information. A major issue in this process is the redundant verification
of already fact-checked claims, which increases workload and delays responses
to newly emerging claims. This research introduces an approach that retrieves
previously fact-checked claims, evaluates their relevance to a given input, and
provides supplementary information to support fact-checkers. Our method employs
large language models (LLMs) to filter irrelevant fact-checks and generate
concise summaries and explanations, enabling fact-checkers to faster assess
whether a claim has been verified before. In addition, we evaluate our approach
through both automatic and human assessments, where humans interact with the
developed tool to review its effectiveness. Our results demonstrate that LLMs
are able to filter out many irrelevant fact-checks and, therefore, reduce
effort and streamline the fact-checking process.

</details>

### [144] [Non-native Children's Automatic Speech Assessment Challenge (NOCASA)](https://arxiv.org/abs/2504.20678)
*Yaroslav Getman,Tamás Grósz,Mikko Kurimo,Giampiero Salvi*

Main category: cs.CL

TLDR: NOCASA竞赛旨在开发评估非母语儿童发音的系统，提供数据集和基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决发音评估中数据有限和类别不平衡的问题。

Method: 提供数据集TeflonNorL2和两种基线模型（SVM和wav2vec 2.0）。

Result: wav2vec 2.0模型表现最佳，UAR为36.37%。

Conclusion: NOCASA为发音评估系统开发提供了数据和基线支持。

Abstract: This paper presents the "Non-native Children's Automatic Speech Assessment"
(NOCASA) - a data competition part of the IEEE MLSP 2025 conference. NOCASA
challenges participants to develop new systems that can assess single-word
pronunciations of young second language (L2) learners as part of a gamified
pronunciation training app. To achieve this, several issues must be addressed,
most notably the limited nature of available training data and the highly
unbalanced distribution among the pronunciation level categories. To expedite
the development, we provide a pseudo-anonymized training data (TeflonNorL2),
containing 10,334 recordings from 44 speakers attempting to pronounce 205
distinct Norwegian words, human-rated on a 1 to 5 scale (number of stars that
should be given in the game). In addition to the data, two already trained
systems are released as official baselines: an SVM classifier trained on the
ComParE_16 acoustic feature set and a multi-task wav2vec 2.0 model. The latter
achieves the best performance on the challenge test set, with an unweighted
average recall (UAR) of 36.37%.

</details>

### [145] [Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?](https://arxiv.org/abs/2504.20679)
*Wing Yan Li,Zeqiang Wang,Jon Johnson,Suparna De*

Main category: cs.CL

TLDR: 论文提出了一种自动检测社会科学纵向调查中语义等效问题的方法，通过多学科合作解决了概念表示不一致和词汇演变的挑战，并比较了多种无监督方法的性能。


<details>
  <summary>Details</summary>
Motivation: 社会科学纵向调查中语义等效问题的自动检测对长期研究至关重要，但面临概念表示不一致和词汇演变的双重挑战。

Method: 研究了多种无监督方法，包括概率模型、语言模型的线性探测和专为信息检索设计的预训练神经网络。

Result: 专为信息检索设计的神经网络模型表现最佳，其他方法表现相当；概率模型结果通过神经网络重新排序仅带来微小改进。

Conclusion: 研究为社会科学纵向研究的协调提供了进一步的研究方向，模型在词汇重叠高但子概念不匹配的问题上敏感性较低。

Abstract: Automated detection of semantically equivalent questions in longitudinal
social science surveys is crucial for long-term studies informing empirical
research in the social, economic, and health sciences. Retrieving equivalent
questions faces dual challenges: inconsistent representation of theoretical
constructs (i.e. concept/sub-concept) across studies as well as between
question and response options, and the evolution of vocabulary and structure in
longitudinal text. To address these challenges, our multi-disciplinary
collaboration of computer scientists and survey specialists presents a new
information retrieval (IR) task of identifying concept (e.g. Housing, Job,
etc.) equivalence across question and response options to harmonise
longitudinal population studies. This paper investigates multiple unsupervised
approaches on a survey dataset spanning 1946-2020, including probabilistic
models, linear probing of language models, and pre-trained neural networks
specialised for IR. We show that IR-specialised neural models achieve the
highest overall performance with other approaches performing comparably.
Additionally, the re-ranking of the probabilistic model's results with neural
models only introduces modest improvements of 0.07 at most in F1-score.
Qualitative post-hoc evaluation by survey specialists shows that models
generally have a low sensitivity to questions with high lexical overlap,
particularly in cases where sub-concepts are mismatched. Altogether, our
analysis serves to further research on harmonising longitudinal studies in
social science.

</details>

### [146] [Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?](https://arxiv.org/abs/2504.20699)
*Evangelia Gogoulou,Shorouq Zahra,Liane Guillou,Luise Dürlich,Joakim Nivre*

Main category: cs.CL

TLDR: 论文研究了LLMs在翻译和转述任务中检测内在幻觉的能力，发现模型性能因模型而异，但提示选择影响不大，且NLI模型表现相当。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs生成无意义、不合逻辑或事实错误输出的问题，即幻觉现象。

Method: 基于HalluciGen任务，评估开源LLMs在翻译和转述任务中检测内在幻觉的能力，研究模型大小、指令调整和提示选择的影响。

Result: 模型性能因模型而异，但提示选择影响不大；NLI模型表现与LLM相当。

Conclusion: LLM检测器并非唯一可行方案，NLI模型在特定任务中表现良好。

Abstract: A frequently observed problem with LLMs is their tendency to generate output
that is nonsensical, illogical, or factually incorrect, often referred to
broadly as hallucination. Building on the recently proposed HalluciGen task for
hallucination detection and generation, we evaluate a suite of open-access LLMs
on their ability to detect intrinsic hallucinations in two conditional
generation tasks: translation and paraphrasing. We study how model performance
varies across tasks and language and we investigate the impact of model size,
instruction tuning, and prompt choice. We find that performance varies across
models but is consistent across prompts. Finally, we find that NLI models
perform comparably well, suggesting that LLM-based detectors are not the only
viable option for this specific task.

</details>

### [147] [BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification](https://arxiv.org/abs/2504.20703)
*Foteini Papadopoulou,Osman Mutlu,Neris Özen,Bas H. M. van der Velden,Iris Hendrickx,Ali Hürriyetoğlu*

Main category: cs.CL

TLDR: 本文介绍了为SemEval-2025任务9开发的系统，通过文本增强技术提升少数类别的分类性能，发现BERT模型在细粒度分类中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决食品召回报告中危险和产品分类任务中少数类别性能不佳的问题。

Method: 采用三种词级数据增强技术（同义词替换、随机词交换和上下文词插入），并比较其在多种模型上的效果。

Result: BERT模型在细粒度分类中表现显著提升，上下文词插入技术使少数危险类别的预测准确率提高6%。

Conclusion: 针对少数类别的增强技术可以提升Transformer模型的性能。

Abstract: This paper presents our system developed for the SemEval-2025 Task 9: The
Food Hazard Detection Challenge. The shared task's objective is to evaluate
explainable classification systems for classifying hazards and products in two
levels of granularity from food recall incident reports. In this work, we
propose text augmentation techniques as a way to improve poor performance on
minority classes and compare their effect for each category on various
transformer and machine learning models. We explore three word-level data
augmentation techniques, namely synonym replacement, random word swapping, and
contextual word insertion. The results show that transformer models tend to
have a better overall performance. None of the three augmentation techniques
consistently improved overall performance for classifying hazards and products.
We observed a statistically significant improvement (P < 0.05) in the
fine-grained categories when using the BERT model to compare the baseline with
each augmented model. Compared to the baseline, the contextual words insertion
augmentation improved the accuracy of predictions for the minority hazard
classes by 6%. This suggests that targeted augmentation of minority classes can
improve the performance of transformer models.

</details>

### [148] [Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think](https://arxiv.org/abs/2504.20708)
*Hasan Abed Al Kader Hammoud,Hani Itani,Bernard Ghanem*

Main category: cs.CL

TLDR: 论文质疑仅依赖最终答案评估LLMs的合理性，提出通过分析中间推理步骤（子思想）提升准确性，实验显示显著改进。


<details>
  <summary>Details</summary>
Motivation: 挑战现有评估方法，探究最终答案是否可靠以及不同推理路径是否影响结果。

Method: 将推理轨迹分段为子思想，生成不同子思想的延续，提取潜在答案并选择最频繁的答案。

Result: 实验表明，聚合子思想的答案比原始完整轨迹的答案准确率更高，提升达13%和10%。

Conclusion: 分析子思想的答案一致性可识别模型置信度和正确性，为评估LLMs提供新方法。

Abstract: Large Language Models (LLMs) leverage step-by-step reasoning to solve complex
problems. Standard evaluation practice involves generating a complete reasoning
trace and assessing the correctness of the final answer presented at its
conclusion. In this paper, we challenge the reliance on the final answer by
posing the following two questions: Does the final answer reliably represent
the model's optimal conclusion? Can alternative reasoning paths yield different
results? To answer these questions, we analyze intermediate reasoning steps,
termed subthoughts, and propose a method based on our findings. Our approach
involves segmenting a reasoning trace into sequential subthoughts based on
linguistic cues. We start by prompting the model to generate continuations from
the end-point of each intermediate subthought. We extract a potential answer
from every completed continuation originating from different subthoughts. We
find that aggregating these answers by selecting the most frequent one (the
mode) often yields significantly higher accuracy compared to relying solely on
the answer derived from the original complete trace. Analyzing the consistency
among the answers derived from different subthoughts reveals characteristics
that correlate with the model's confidence and correctness, suggesting
potential for identifying less reliable answers. Our experiments across various
LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025)
show consistent accuracy improvements, with gains reaching up to 13\% and 10\%
respectively. Implementation is available at:
https://github.com/hammoudhasan/SubthoughtReasoner.

</details>

### [149] [UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities](https://arxiv.org/abs/2504.20734)
*Woongyeong Yeo,Kangsan Kim,Soyeong Jeong,Jinheon Baek,Sung Ju Hwang*

Main category: cs.CL

TLDR: UniversalRAG是一个新的RAG框架，通过动态路由机制从多模态、多粒度的异构知识源中检索和整合信息，解决了现有RAG方法局限于单一模态的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法通常局限于单一模态的知识源，无法满足现实世界中多样化查询的需求。

Method: 提出模态感知路由机制，动态选择最合适的模态特定知识库进行检索，并组织多粒度级别以适配查询的复杂性。

Result: 在8个多模态基准测试中，UniversalRAG优于单一模态和统一基线方法。

Conclusion: UniversalRAG通过多模态和多粒度的检索机制，显著提升了RAG的适应性和准确性。

Abstract: Retrieval-Augmented Generation (RAG) has shown substantial promise in
improving factual accuracy by grounding model responses with external knowledge
relevant to queries. However, most existing RAG approaches are limited to a
text-only corpus, and while recent efforts have extended RAG to other
modalities such as images and videos, they typically operate over a single
modality-specific corpus. In contrast, real-world queries vary widely in the
type of knowledge they require, which a single type of knowledge source cannot
address. To address this, we introduce UniversalRAG, a novel RAG framework
designed to retrieve and integrate knowledge from heterogeneous sources with
diverse modalities and granularities. Specifically, motivated by the
observation that forcing all modalities into a unified representation space
derived from a single combined corpus causes a modality gap, where the
retrieval tends to favor items from the same modality as the query, we propose
a modality-aware routing mechanism that dynamically identifies the most
appropriate modality-specific corpus and performs targeted retrieval within it.
Also, beyond modality, we organize each modality into multiple granularity
levels, enabling fine-tuned retrieval tailored to the complexity and scope of
the query. We validate UniversalRAG on 8 benchmarks spanning multiple
modalities, showing its superiority over modality-specific and unified
baselines.

</details>

### [150] [Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers](https://arxiv.org/abs/2504.20752)
*Roman Abramov,Felix Steinbauer,Gjergji Kasneci*

Main category: cs.CL

TLDR: 论文提出了一种通过合成数据增强知识图谱的方法，以提升Transformer模型在多步事实推理中的表现，并在2WikiMultiHopQA基准测试中达到95-100%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在现实知识稀疏场景下多步事实推理的不足，探索如何通过数据增强触发模型的泛化能力。

Method: 通过设计合成数据增强知识图谱，提高推断事实与原子事实的比例（φ_r），从而触发模型的grokking现象。

Result: 在2WikiMultiHopQA基准测试中达到95-100%的准确率，显著优于基线模型，并匹配或超越当前最优结果。

Conclusion: 合成数据增强可以解锁Transformer的隐式多步推理能力，为大规模语言模型提供更鲁棒和可解释的事实推理方法。

Abstract: Transformers have achieved great success in numerous NLP tasks but continue
to exhibit notable gaps in multi-step factual reasoning, especially when
real-world knowledge is sparse. Recent advances in grokking have demonstrated
that neural networks can transition from memorizing to perfectly generalizing
once they detect underlying logical patterns - yet these studies have primarily
used small, synthetic tasks. In this paper, for the first time, we extend
grokking to real-world factual data and address the challenge of dataset
sparsity by augmenting existing knowledge graphs with carefully designed
synthetic data to raise the ratio $\phi_r$ of inferred facts to atomic facts
above the threshold required for grokking. Surprisingly, we find that even
factually incorrect synthetic data can strengthen emergent reasoning circuits
rather than degrade accuracy, as it forces the model to rely on relational
structure rather than memorization. When evaluated on multi-hop reasoning
benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -
substantially improving over strong baselines and matching or exceeding current
state-of-the-art results. We further provide an in-depth analysis of how
increasing $\phi_r$ drives the formation of generalizing circuits inside
Transformers. Our findings suggest that grokking-based data augmentation can
unlock implicit multi-hop reasoning capabilities, opening the door to more
robust and interpretable factual reasoning in large-scale language models.

</details>

### [151] [Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption](https://arxiv.org/abs/2504.20769)
*Wenxiao Wang,Parsa Hosseini,Soheil Feizi*

Main category: cs.CL

TLDR: 链式防御思维提示显著提升大语言模型在非推理任务中的鲁棒性，尤其在参考数据被污染时表现突出。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用链式思维提示增强大语言模型的推理能力，以提升其在非推理任务中的鲁棒性。

Method: 提出链式防御思维提示方法，仅需提供少量带有结构化防御推理的示例作为演示。

Result: 实验表明，该方法显著提升模型鲁棒性，例如在自然问题任务中，GPT-4o的准确率从标准提示的3%提升至50%。

Conclusion: 链式防御思维提示是一种简单且高效的方法，能显著增强大语言模型在对抗性环境中的表现。

Abstract: Chain-of-thought prompting has demonstrated great success in facilitating the
reasoning abilities of large language models. In this work, we explore how
these enhanced reasoning abilities can be exploited to improve the robustness
of large language models in tasks that are not necessarily reasoning-focused.
In particular, we show how a wide range of large language models exhibit
significantly improved robustness against reference corruption using a simple
method called chain-of-defensive-thought, where only a few exemplars with
structured and defensive reasoning are provided as demonstrations. Empirically,
the improvements can be astounding, especially given the simplicity and
applicability of the method. For example, in the Natural Questions task, the
accuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting
when 1 out of 10 references provided is corrupted with prompt injection
attacks. In contrast, GPT-4o using chain-of-defensive-thought prompting
maintains an accuracy of 50%.

</details>

### [152] [Turing Machine Evaluation for Large Language Model](https://arxiv.org/abs/2504.20771)
*Haitao Wu,Zongbo Han,Huaxi Huang,Changqing Zhang*

Main category: cs.CL

TLDR: 论文提出了一种基于通用图灵机（UTM）模拟的评估框架TMBench，用于系统评估大语言模型（LLMs）的核心计算推理能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，对其计算推理能力的严格评估变得尤为重要，以确保其作为精确执行器的可靠性。

Method: 通过UTM模拟框架，要求LLMs在多步计算中严格遵循指令并跟踪动态状态（如磁带内容和读写头位置）。

Result: TMBench具有知识无关性、难度可调、覆盖基础能力等特点，且模型在TMBench上的表现与其他推理基准显著相关（Pearson系数0.73）。

Conclusion: 计算推理能力是衡量LLMs深层能力的重要维度，TMBench为标准化评估提供了有效工具。

Abstract: With the rapid development and widespread application of Large Language
Models (LLMs), rigorous evaluation has become particularly crucial. This
research adopts a novel perspective, focusing on evaluating the core
computational reasoning ability of LLMs, defined as the capacity of model to
accurately understand rules, and execute logically computing operations. This
capability assesses the reliability of LLMs as precise executors, and is
critical to advanced tasks such as complex code generation and multi-step
problem-solving. We propose an evaluation framework based on Universal Turing
Machine (UTM) simulation. This framework requires LLMs to strictly follow
instructions and track dynamic states, such as tape content and read/write head
position, during multi-step computations. To enable standardized evaluation, we
developed TMBench, a benchmark for systematically studying the computational
reasoning capabilities of LLMs. TMBench provides several key advantages,
including knowledge-agnostic evaluation, adjustable difficulty, foundational
coverage through Turing machine encoding, and unlimited capacity for instance
generation, ensuring scalability as models continue to evolve. We find that
model performance on TMBench correlates strongly with performance on other
recognized reasoning benchmarks (Pearson correlation coefficient is 0.73),
clearly demonstrating that computational reasoning is a significant dimension
for measuring the deep capabilities of LLMs. Code and data are available at
https://github.com/HaitaoWuTJU/Turing-Machine-Bench.

</details>

### [153] [Universal language model with the intervention of quantum theory](https://arxiv.org/abs/2504.20839)
*D. -F. Qin*

Main category: cs.CL

TLDR: 该论文探讨了基于量子力学理论的语言建模，提出将量子力学引入语言符号-意义对以构建自然语言表示模型，并尝试用量子统计等理论改进词嵌入技术。


<details>
  <summary>Details</summary>
Motivation: 研究量子力学框架如何解释和改进语言建模中的词嵌入技术，并探索自然语言的量子特性及其物理性来源。

Method: 通过量子力学和量子统计理论构建自然语言表示模型，并通过实验代码验证其可行性。

Result: 论文指出量子理论可用于自然语言建模，并讨论了其在生成模型和量子计算机中的潜在应用。

Conclusion: 量子力学为语言建模提供了新的理论框架，未来可能在生成模型和量子计算中有进一步应用。

Abstract: This paper examines language modeling based on the theory of quantum
mechanics. It focuses on the introduction of quantum mechanics into the
symbol-meaning pairs of language in order to build a representation model of
natural language. At the same time, it is realized that word embedding, which
is widely used as a basic technique for statistical language modeling, can be
explained and improved by the mathematical framework of quantum mechanics. On
this basis, this paper continues to try to use quantum statistics and other
related theories to study the mathematical representation, natural evolution
and statistical properties of natural language. It is also assumed that the
source of such quantum properties is the physicality of information. The
feasibility of using quantum theory to model natural language is pointed out
through the construction of a experimental code. The paper discusses, in terms
of applications, the possible help of the theory in constructing generative
models that are popular nowadays. A preliminary discussion of future
applications of the theory to quantum computers is also presented.

</details>

### [154] [JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry](https://arxiv.org/abs/2504.20849)
*Anum Afzal,Alexandre Mercier,Florian Matthes*

Main category: cs.CL

TLDR: 论文研究了基于LLM的数据到文本方法，用于生成高质量且多样化的营销文本，并提出了评估多样性的新指标JaccDiv。


<details>
  <summary>Details</summary>
Motivation: 传统生成方法容易陷入重复模式，导致文本单调，限制了自动化内容生成的广泛应用。

Method: 利用T5、GPT-3.5、GPT-4和LLaMa2等语言模型，结合微调、少样本和零样本方法，生成多样化营销文本。

Result: 提出了JaccDiv指标评估文本多样性，方法适用于音乐行业及其他重复内容生成领域。

Conclusion: LLM-based方法能够生成高质量且多样化的文本，为自动化内容生成提供了新思路。

Abstract: Online platforms are increasingly interested in using Data-to-Text
technologies to generate content and help their users. Unfortunately,
traditional generative methods often fall into repetitive patterns, resulting
in monotonous galleries of texts after only a few iterations. In this paper, we
investigate LLM-based data-to-text approaches to automatically generate
marketing texts that are of sufficient quality and diverse enough for broad
adoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in
conjunction with fine-tuning, few-shot, and zero-shot approaches to set a
baseline for diverse marketing texts. We also introduce a metric JaccDiv to
evaluate the diversity of a set of texts. This research extends its relevance
beyond the music industry, proving beneficial in various fields where
repetitive automated content generation is prevalent.

</details>

### [155] [DYNAMAX: Dynamic computing for Transformers and Mamba based architectures](https://arxiv.org/abs/2504.20922)
*Miguel Nogales,Matteo Gambella,Manuel Roveri*

Main category: cs.CL

TLDR: DYNAMAX框架首次将早期退出机制应用于Mamba架构，并展示了其作为高效分类器的潜力，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 探索早期退出机制在Mamba架构中的应用，填补其在解码器模型中的研究空白。

Method: 将早期退出机制集成到Mamba中，并测试其在Mamba和Transformer模型中的表现。

Result: Mamba作为早期退出分类器表现出色，在计算成本和性能间取得平衡。

Conclusion: Mamba的动态处理能力为资源受限环境中的高效推理提供了新方向。

Abstract: Early exits (EEs) offer a promising approach to reducing computational costs
and latency by dynamically terminating inference once a satisfactory prediction
confidence on a data sample is achieved. Although many works integrate EEs into
encoder-only Transformers, their application to decoder-only architectures and,
more importantly, Mamba models, a novel family of state-space architectures in
the LLM realm, remains insufficiently explored. This work introduces DYNAMAX,
the first framework to exploit the unique properties of Mamba architectures for
early exit mechanisms. We not only integrate EEs into Mamba but also repurpose
Mamba as an efficient EE classifier for both Mamba-based and transformer-based
LLMs, showcasing its versatility. Our experiments employ the Mistral 7B
transformer compared to the Codestral 7B Mamba model, using data sets such as
TruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and
consistency. The results highlight the adaptability of Mamba as a powerful EE
classifier and its efficiency in balancing computational cost and performance
quality across NLP tasks. By leveraging Mamba's inherent design for dynamic
processing, we open pathways for scalable and efficient inference in embedded
applications and resource-constrained environments. This study underscores the
transformative potential of Mamba in redefining dynamic computing paradigms for
LLMs.

</details>

### [156] [Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models](https://arxiv.org/abs/2504.20946)
*Tyler McDonald,Ali Emami*

Main category: cs.CL

TLDR: 论文提出了一种名为Trace-of-Thought Prompting的零样本提示工程方法，旨在通过开源模型提升算术推理能力，同时减少计算和财务成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的广泛使用可能导致计算和财务负担，且依赖闭源模型限制了定制化和可扩展性。因此，研究探索了开源模型的潜力。

Method: 采用Trace-of-Thought Prompting方法，指导LLMs通过可观察的子问题解决算术推理任务，结合开源模型和GPT-4进行实验。

Result: 该方法在7亿参数以下的开源模型中实现了高达125%的性能提升，并提供了问题解决过程的新见解。

Conclusion: 开源模型结合Trace-of-Thought Prompting方法能够有效提升性能，同时促进AI研究的民主化和高质量计算语言学应用的普及。

Abstract: As Large Language Models (LLMs) continue to be leveraged for daily tasks,
prompt engineering remains an active field of contribution within computational
linguistics, particularly in domains requiring specialized knowledge such as
arithmetic reasoning. While these LLMs are optimized for a variety of tasks,
their exhaustive employment may become computationally or financially
cumbersome for small teams. Additionally, complete reliance on proprietary,
closed-source models often limits customization and adaptability, posing
significant challenges in research and application scalability. Instead, by
leveraging open-source models at or below 7 billion parameters, we can optimize
our resource usage while still observing remarkable gains over standard
prompting approaches. To cultivate this notion, we introduce Trace-of-Thought
Prompting, a simple, zero-shot prompt engineering method that instructs LLMs to
create observable subproblems using critical problem-solving, specifically
designed to enhance arithmetic reasoning capabilities. When applied to
open-source models in tandem with GPT-4, we observe that Trace-of-Thought not
only allows novel insight into the problem-solving process but also introduces
performance gains as large as 125% on language models at or below 7 billion
parameters. This approach underscores the potential of open-source initiatives
in democratizing AI research and improving the accessibility of high-quality
computational linguistics applications.

</details>

### [157] [Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models](https://arxiv.org/abs/2504.20951)
*Maryna Vyshnyvetska*

Main category: cs.CL

TLDR: 提出了一种名为“信息引力”的理论模型，用于描述大型语言模型（LLM）中的文本生成过程，借鉴了场论和时空几何的物理概念。


<details>
  <summary>Details</summary>
Motivation: 解释LLM中观察到的现象，如幻觉、对查询表述的敏感性以及采样温度对输出多样性的影响。

Method: 将查询视为具有“信息质量”的对象，通过弯曲模型的语义空间形成引力势阱，吸引生成过程中的标记。

Result: 模型为LLM行为提供了机制解释，包括幻觉、查询敏感性及输出多样性的影响。

Conclusion: 信息引力模型为理解LLM的文本生成过程提供了新的理论框架。

Abstract: We propose a theoretical model called "information gravity" to describe the
text generation process in large language models (LLMs). The model uses
physical apparatus from field theory and spacetime geometry to formalize the
interaction between user queries and the probability distribution of generated
tokens. A query is viewed as an object with "information mass" that curves the
semantic space of the model, creating gravitational potential wells that
"attract" tokens during generation. This model offers a mechanism to explain
several observed phenomena in LLM behavior, including hallucinations (emerging
from low-density semantic voids), sensitivity to query formulation (due to
semantic field curvature changes), and the influence of sampling temperature on
output diversity.

</details>

### [158] [OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification](https://arxiv.org/abs/2504.20964)
*Shangyu Li,Juyong Jiang,Tiancheng Zhao,Jiasi Shen*

Main category: cs.CL

TLDR: OSVBench是一个新基准，用于评估大型语言模型（LLM）在操作系统内核验证任务中生成完整规范代码的能力。基于真实操作系统内核Hyperkernel，包含245个复杂任务，每个任务约20k-30k tokens。评估显示当前LLM在此类任务中表现有限。


<details>
  <summary>Details</summary>
Motivation: 操作系统内核验证需要生成复杂规范代码，现有LLM在此类任务中的能力尚不明确，因此提出OSVBench以填补这一空白。

Method: 将规范生成问题定义为程序合成问题，提供编程模型和验证假设，要求LLM在限定语法和语义空间内生成完整规范。

Result: 评估12个LLM，发现其在长上下文代码生成任务中表现有限，且性能差异显著。

Conclusion: OSVBench揭示了当前LLM在操作系统规范生成任务中的局限性，为未来研究提供了工具和基准。

Abstract: We introduce OSVBench, a new benchmark for evaluating Large Language Models
(LLMs) in generating complete specification code pertaining to operating system
kernel verification tasks. The benchmark first defines the specification
generation problem into a program synthesis problem within a confined scope of
syntax and semantics by providing LLMs with the programming model. The LLMs are
required to understand the provided verification assumption and the potential
syntax and semantics space to search for, then generate the complete
specification for the potentially buggy operating system code implementation
under the guidance of the high-level functional description of the operating
system. This benchmark is built upon a real-world operating system kernel,
Hyperkernel, and consists of 245 complex specification generation tasks in
total, each is a long context task of about 20k-30k tokens. Our comprehensive
evaluation of 12 LLMs exhibits the limited performance of the current LLMs on
the specification generation tasks for operating system verification.
Significant disparities in their performance on the benchmark highlight
differences in their ability to handle long-context code generation tasks. The
evaluation toolkit and benchmark are available at
https://github.com/lishangyu-hkust/OSVBench.

</details>

### [159] [SetKE: Knowledge Editing for Knowledge Elements Overlap](https://arxiv.org/abs/2504.20972)
*Yifan Wei,Xiaoyan Yu,Ran Song,Hao Peng,Angsheng Li*

Main category: cs.CL

TLDR: 论文提出了一种新的知识编辑方法SetKE，用于解决知识元素重叠（KEO）问题，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在知识更新时面临传统方法的局限性（如过拟合和高计算成本），而现有知识编辑方法忽略了知识元素重叠现象，导致性能下降。

Method: 提出知识集编辑（KSE）框架和SetKE方法，同时编辑一组三元组，并引入包含KEO三元组的数据集EditSet作为基准。

Result: 实验表明，SetKE在主流LLMs上优于现有方法，尤其在KEO场景中表现更优。

Conclusion: SetKE为知识编辑提供了一种高效且鲁棒的解决方案，EditSet数据集为未来研究提供了重要基准。

Abstract: Large Language Models (LLMs) excel in tasks such as retrieval and question
answering but require updates to incorporate new knowledge and reduce
inaccuracies and hallucinations. Traditional updating methods, like fine-tuning
and incremental learning, face challenges such as overfitting and high
computational costs. Knowledge Editing (KE) provides a promising alternative
but often overlooks the Knowledge Element Overlap (KEO) phenomenon, where
multiple triplets share common elements, leading to editing conflicts. We
identify the prevalence of KEO in existing KE datasets and show its significant
impact on current KE methods, causing performance degradation in handling such
triplets. To address this, we propose a new formulation, Knowledge Set Editing
(KSE), and introduce SetKE, a method that edits sets of triplets
simultaneously. Experimental results demonstrate that SetKE outperforms
existing methods in KEO scenarios on mainstream LLMs. Additionally, we
introduce EditSet, a dataset containing KEO triplets, providing a comprehensive
benchmark.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [160] [Evolution of AI in Education: Agentic Workflows](https://arxiv.org/abs/2504.20082)
*Firuz Kamalov,David Santandreu Calonge,Linda Smail,Dilshod Azizov,Dimple R. Thadani,Theresa Kwong,Amara Atif*

Main category: cs.AI

TLDR: 论文探讨了AI代理在教育中的潜力，通过反思、规划、工具使用和多代理协作四种范式，分析了其优势、应用与挑战，并提出了一个多代理框架用于自动作文评分，初步结果显示其优于传统LLMs。


<details>
  <summary>Details</summary>
Motivation: 传统LLMs在静态数据、适应性和推理能力上的局限性促使研究者探索AI代理在教育中的创新应用。

Method: 通过四种范式（反思、规划、工具使用和多代理协作）分析AI代理在教育中的角色，并开发了一个多代理框架用于自动作文评分。

Result: 初步结果表明，多代理框架在一致性上优于传统LLMs。

Conclusion: AI代理在教育中具有变革潜力，但需进一步研究其可解释性、可信度和对教学的可持续影响。

Abstract: Artificial intelligence (AI) has transformed various aspects of education,
with large language models (LLMs) driving advancements in automated tutoring,
assessment, and content generation. However, conventional LLMs are constrained
by their reliance on static training data, limited adaptability, and lack of
reasoning. To address these limitations and foster more sustainable
technological practices, AI agents have emerged as a promising new avenue for
educational innovation. In this review, we examine agentic workflows in
education according to four major paradigms: reflection, planning, tool use,
and multi-agent collaboration. We critically analyze the role of AI agents in
education through these key design paradigms, exploring their advantages,
applications, and challenges. To illustrate the practical potential of agentic
systems, we present a proof-of-concept application: a multi-agent framework for
automated essay scoring. Preliminary results suggest this agentic approach may
offer improved consistency compared to stand-alone LLMs. Our findings highlight
the transformative potential of AI agents in educational settings while
underscoring the need for further research into their interpretability,
trustworthiness, and sustainable impact on pedagogical impact.

</details>

### [161] [AI Awareness](https://arxiv.org/abs/2504.20084)
*Xiaojian Li,Haoyuan Shi,Rongwu Xu,Wei Xu*

Main category: cs.AI

TLDR: 论文探讨了AI意识的四种形式（元认知、自我意识、社会意识和情境意识），分析了其理论基础、评估方法及与AI能力的关联，并讨论了相关风险和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力的提升，研究AI意识的功能性表现成为必要，以理解其与智能行为的关系及潜在风险。

Method: 结合认知科学、心理学和计算理论，分析AI意识的理论基础，并系统评估现有方法和实证结果。

Result: 研究发现，更具意识的AI表现出更高的智能行为，但同时也带来安全和伦理风险。

Conclusion: AI意识是一把双刃剑，需在提升能力的同时谨慎管理风险，未来研究应进一步明确其在智能机器发展中的作用。

Abstract: Recent breakthroughs in artificial intelligence (AI) have brought about
increasingly capable systems that demonstrate remarkable abilities in
reasoning, language understanding, and problem-solving. These advancements have
prompted a renewed examination of AI awareness, not as a philosophical question
of consciousness, but as a measurable, functional capacity. In this review, we
explore the emerging landscape of AI awareness, which includes meta-cognition
(the ability to represent and reason about its own state), self-awareness
(recognizing its own identity, knowledge, limitations, inter alia), social
awareness (modeling the knowledge, intentions, and behaviors of other agents),
and situational awareness (assessing and responding to the context in which it
operates).
  First, we draw on insights from cognitive science, psychology, and
computational theory to trace the theoretical foundations of awareness and
examine how the four distinct forms of AI awareness manifest in
state-of-the-art AI. Next, we systematically analyze current evaluation methods
and empirical findings to better understand these manifestations. Building on
this, we explore how AI awareness is closely linked to AI capabilities,
demonstrating that more aware AI agents tend to exhibit higher levels of
intelligent behaviors. Finally, we discuss the risks associated with AI
awareness, including key topics in AI safety, alignment, and broader ethical
concerns.
  AI awareness is a double-edged sword: it improves general capabilities, i.e.,
reasoning, safety, while also raises concerns around misalignment and societal
risks, demanding careful oversight as AI capabilities grow. On the whole, our
interdisciplinary review provides a roadmap for future research and aims to
clarify the role of AI awareness in the ongoing development of intelligent
machines.

</details>

### [162] [Spark: A System for Scientifically Creative Idea Generation](https://arxiv.org/abs/2504.20090)
*Aishik Sanyal,Samuel Schapiro,Sumuk Shashidhar,Royce Moon,Lav R. Varshney,Dilek Hakkani-Tur*

Main category: cs.AI

TLDR: 论文介绍了Spark系统，结合检索增强的LLM生成科学创意与基于60万科学评论训练的评审模型Judge，旨在激发计算创造力研究。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在科学创意生成中的潜力，并基于计算创造力原则推动研究。

Method: 开发Spark系统，结合检索增强的LLM生成创意，并训练Judge模型评估。

Result: 展示了Spark系统，并公开了训练Judge的数据集。

Conclusion: 呼吁研究者探索LLM在创意生成与评估中的应用，推动计算创造力领域发展。

Abstract: Recently, large language models (LLMs) have shown promising abilities to
generate novel research ideas in science, a direction which coincides with many
foundational principles in computational creativity (CC). In light of these
developments, we present an idea generation system named Spark that couples
retrieval-augmented idea generation using LLMs with a reviewer model named
Judge trained on 600K scientific reviews from OpenReview. Our work is both a
system demonstration and intended to inspire other CC researchers to explore
grounding the generation and evaluation of scientific ideas within foundational
CC principles. To this end, we release the annotated dataset used to train
Judge, inviting other researchers to explore the use of LLMs for idea
generation and creative evaluations.

</details>

### [163] [Personalized Artificial General Intelligence (AGI) via Neuroscience-Inspired Continuous Learning Systems](https://arxiv.org/abs/2504.20109)
*Rajeev Gupta,Suhani Gupta,Ronak Parikh,Divya Gupta,Amir Javaheri,Jairaj Singh Shaktawat*

Main category: cs.AI

TLDR: 本文提出了一种新型的个性化通用人工智能（AGI）架构，结合神经科学启发的学习机制，旨在实现边缘设备上的持续学习和适应。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型依赖参数扩展，难以实现持续学习和通用智能，尤其是在资源受限的边缘设备上。

Method: 结合神经科学原理（如突触修剪、Hebbian可塑性等），设计了一种包含快速-慢速学习模块和内存高效更新的架构。

Result: 提出了理论架构，解决了灾难性遗忘、内存效率和系统可扩展性等挑战。

Conclusion: 该架构为未来实现边缘设备上的持续个性化AGI提供了路线图。

Abstract: Artificial Intelligence has made remarkable advancements in recent years,
primarily driven by increasingly large deep learning models. However, achieving
true Artificial General Intelligence (AGI) demands fundamentally new
architectures rather than merely scaling up existing models. Current approaches
largely depend on expanding model parameters, which improves task-specific
performance but falls short in enabling continuous, adaptable, and generalized
learning. Achieving AGI capable of continuous learning and personalization on
resource-constrained edge devices is an even bigger challenge.
  This paper reviews the state of continual learning and neuroscience-inspired
AI, and proposes a novel architecture for Personalized AGI that integrates
brain-like learning mechanisms for edge deployment. We review literature on
continuous lifelong learning, catastrophic forgetting, and edge AI, and discuss
key neuroscience principles of human learning, including Synaptic Pruning,
Hebbian plasticity, Sparse Coding, and Dual Memory Systems, as inspirations for
AI systems. Building on these insights, we outline an AI architecture that
features complementary fast-and-slow learning modules, synaptic
self-optimization, and memory-efficient model updates to support on-device
lifelong adaptation.
  Conceptual diagrams of the proposed architecture and learning processes are
provided. We address challenges such as catastrophic forgetting, memory
efficiency, and system scalability, and present application scenarios for
mobile AI assistants and embodied AI systems like humanoid robots. We conclude
with key takeaways and future research directions toward truly continual,
personalized AGI on the edge. While the architecture is theoretical, it
synthesizes diverse findings and offers a roadmap for future implementation.

</details>

### [164] [Transforming Evidence Synthesis: A Systematic Review of the Evolution of Automated Meta-Analysis in the Age of AI](https://arxiv.org/abs/2504.20113)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.AI

TLDR: 本文对自动化元分析（AMA）的现状进行了系统评估，发现其在数据处理阶段自动化程度较高，但在高级合成阶段和全流程自动化方面存在显著不足，未来需进一步整合AI技术以实现全面自动化。


<details>
  <summary>Details</summary>
Motivation: 科学文献的指数增长推动了对高效证据合成的需求，催生了自动化元分析（AMA）领域的发展。本文旨在评估AMA的当前状态及其潜力。

Method: 通过PRISMA系统综述方法，筛选了2006年至2024年的978篇论文，并分析了54项研究，涵盖医学和非医学领域。

Result: 研究发现，AMA主要集中在数据处理自动化（57%），而高级合成阶段（17%）和全流程自动化（2%）进展有限。AI技术在统计建模和高级合成中的应用仍有不足。

Conclusion: 未来需整合先进AI技术，实现跨阶段的自动化，提升解释性和方法稳健性，以充分发挥AMA在高效、可扩展合成中的潜力。

Abstract: Exponential growth in scientific literature has heightened the demand for
efficient evidence-based synthesis, driving the rise of the field of Automated
Meta-analysis (AMA) powered by natural language processing and machine
learning. This PRISMA systematic review introduces a structured framework for
assessing the current state of AMA, based on screening 978 papers from 2006 to
2024, and analyzing 54 studies across diverse domains. Findings reveal a
predominant focus on automating data processing (57%), such as extraction and
statistical modeling, while only 17% address advanced synthesis stages. Just
one study (2%) explored preliminary full-process automation, highlighting a
critical gap that limits AMA's capacity for comprehensive synthesis. Despite
recent breakthroughs in large language models (LLMs) and advanced AI, their
integration into statistical modeling and higher-order synthesis, such as
heterogeneity assessment and bias evaluation, remains underdeveloped. This has
constrained AMA's potential for fully autonomous meta-analysis. From our
dataset spanning medical (67%) and non-medical (33%) applications, we found
that AMA has exhibited distinct implementation patterns and varying degrees of
effectiveness in actually improving efficiency, scalability, and
reproducibility. While automation has enhanced specific meta-analytic tasks,
achieving seamless, end-to-end automation remains an open challenge. As AI
systems advance in reasoning and contextual understanding, addressing these
gaps is now imperative. Future efforts must focus on bridging automation across
all meta-analysis stages, refining interpretability, and ensuring
methodological robustness to fully realize AMA's potential for scalable,
domain-agnostic synthesis.

</details>

### [165] [Deep Physics Prior for First Order Inverse Optimization](https://arxiv.org/abs/2504.20278)
*Haoyu Yang,Kamyar Azizzadenesheli,Haoxing Ren*

Main category: cs.AI

TLDR: 论文提出了一种名为Deep Physics Prior（DPP）的新方法，通过预训练的辅助神经算子实现基于梯度的逆优化，解决了传统方法在计算成本、可扩展性和噪声问题上的局限性。


<details>
  <summary>Details</summary>
Motivation: 逆设计优化在多个领域（如半导体制造、结构工程等）中面临缺乏显式数学表示的问题，导致传统优化方法无法直接应用。现有方法（如生成式AI和贝叶斯优化）存在计算成本高、对先验敏感等问题。

Method: 提出DPP方法，利用预训练的辅助神经算子，通过先验分布约束实现基于梯度的逆优化。

Result: DPP能够在先验数据和观测分布未知的情况下，提供稳健且有意义的解决方案。

Conclusion: DPP为逆设计优化提供了一种高效且可靠的新方法，克服了现有技术的局限性。

Abstract: Inverse design optimization aims to infer system parameters from observed
solutions, posing critical challenges across domains such as semiconductor
manufacturing, structural engineering, materials science, and fluid dynamics.
The lack of explicit mathematical representations in many systems complicates
this process and makes the first order optimization impossible. Mainstream
approaches, including generative AI and Bayesian optimization, address these
challenges but have limitations. Generative AI is computationally expensive,
while Bayesian optimization, relying on surrogate models, suffers from
scalability, sensitivity to priors, and noise issues, often leading to
suboptimal solutions. This paper introduces Deep Physics Prior (DPP), a novel
method enabling first-order gradient-based inverse optimization with surrogate
machine learning models. By leveraging pretrained auxiliary Neural Operators,
DPP enforces prior distribution constraints to ensure robust and meaningful
solutions. This approach is particularly effective when prior data and
observation distributions are unknown.

</details>

### [166] [mrCAD: Multimodal Refinement of Computer-aided Designs](https://arxiv.org/abs/2504.20294)
*William P. McCarthy,Saujas Vaduguru,Karl D. D. Willis,Justin Matejka,Judith E. Fan,Daniel Fried,Yewen Pu*

Main category: cs.AI

TLDR: 论文介绍了一个名为mrCAD的数据集，用于研究人类如何通过多模态指令（文本和绘图）迭代改进设计，并发现生成式AI在遵循生成指令上优于改进指令。


<details>
  <summary>Details</summary>
Motivation: 人类协作的特点是能够迭代改进沟通的概念，而生成式AI在内容生成上表现优异，但在语言指导的改进上表现不佳。为了弥合这一差距，研究团队创建了mrCAD数据集。

Method: 通过一个通信游戏收集数据，其中设计师通过文本、绘图或多模态指令指导制造者改进CAD设计。数据集包含6,082个游戏和15,163轮指令执行。

Result: 研究发现生成和改进指令在绘图和文本组成上有所不同，且当前最先进的视觉语言模型在遵循生成指令上表现更好。

Conclusion: mrCAD为分析和建模多模态改进语言提供了基础，填补了现有数据集的空白。

Abstract: A key feature of human collaboration is the ability to iteratively refine the
concepts we have communicated. In contrast, while generative AI excels at the
\textit{generation} of content, it often struggles to make specific
language-guided \textit{modifications} of its prior outputs. To bridge the gap
between how humans and machines perform edits, we present mrCAD, a dataset of
multimodal instructions in a communication game. In each game, players created
computer aided designs (CADs) and refined them over several rounds to match
specific target designs. Only one player, the Designer, could see the target,
and they must instruct the other player, the Maker, using text, drawing, or a
combination of modalities. mrCAD consists of 6,082 communication games, 15,163
instruction-execution rounds, played between 1,092 pairs of human players. We
analyze the dataset and find that generation and refinement instructions differ
in their composition of drawing and text. Using the mrCAD task as a benchmark,
we find that state-of-the-art VLMs are better at following generation
instructions than refinement instructions. These results lay a foundation for
analyzing and modeling a multimodal language of refinement that is not
represented in previous datasets.

</details>

### [167] [Leveraging Action Relational Structures for Integrated Learning and Planning](https://arxiv.org/abs/2504.20318)
*Ryan Xiao Wang,Felipe Trevizan*

Main category: cs.AI

TLDR: 论文提出了一种名为部分空间搜索的新方法，结合学习系统和搜索算法，利用PDDL动作模式的关系结构，提高了规划效率。


<details>
  <summary>Details</summary>
Motivation: 现有规划方法未充分利用动作模式的关系结构，且学习系统与搜索算法的结合研究较少。

Method: 引入部分空间搜索和动作集启发式，将现有启发式转换为动作集启发式，并通过训练数据优化。

Result: 新规划器LazyLifted在IPC 2023学习赛道和高分支因子任务中表现优于现有方法。

Conclusion: 部分空间搜索和动作集启发式的结合显著提升了规划性能。

Abstract: Recent advances in planning have explored using learning methods to help
planning. However, little attention has been given to adapting search
algorithms to work better with learning systems. In this paper, we introduce
partial-space search, a new search space for classical planning that leverages
the relational structure of actions given by PDDL action schemas -- a structure
overlooked by traditional planning approaches. Partial-space search provides a
more granular view of the search space and allows earlier pruning of poor
actions compared to state-space search. To guide partial-space search, we
introduce action set heuristics that evaluate sets of actions in a state. We
describe how to automatically convert existing heuristics into action set
heuristics. We also train action set heuristics from scratch using large
training datasets from partial-space search. Our new planner, LazyLifted,
exploits our better integrated search and learning heuristics and outperforms
the state-of-the-art ML-based heuristic on IPC 2023 learning track (LT)
benchmarks. We also show the efficiency of LazyLifted on high-branching factor
tasks and show that it surpasses LAMA in the combined IPC 2023 LT and
high-branching factor benchmarks.

</details>

### [168] [A Picture is Worth a Thousand Prompts? Efficacy of Iterative Human-Driven Prompt Refinement in Image Regeneration Tasks](https://arxiv.org/abs/2504.20340)
*Khoi Trinh,Scott Seidenberger,Raveen Wijewickrama,Murtuza Jadliwala,Anindya Maiti*

Main category: cs.AI

TLDR: 研究探讨了AI生成图像中的迭代提示优化方法，验证了图像相似度指标与人类感知的一致性，并证明了迭代优化在提升生成内容质量中的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容在数字平台上的普及，研究如何通过迭代优化提示生成特定目标图像，并验证图像相似度指标的有效性。

Method: 通过结构化用户研究，评估迭代提示优化对图像相似度的影响，并比较图像相似度指标与人类感知的一致性。

Result: 迭代提示优化显著提升了图像与目标的对齐度，主观评估和定量指标均验证了这一点。

Conclusion: 迭代工作流在生成AI内容中具有广泛应用潜力，图像相似度指标可作为有效的反馈机制。

Abstract: With AI-generated content becoming ubiquitous across the web, social media,
and other digital platforms, it is vital to examine how such content are
inspired and generated. The creation of AI-generated images often involves
refining the input prompt iteratively to achieve desired visual outcomes. This
study focuses on the relatively underexplored concept of image regeneration
using AI, in which a human operator attempts to closely recreate a specific
target image by iteratively refining their prompt. Image regeneration is
distinct from normal image generation, which lacks any predefined visual
reference. A separate challenge lies in determining whether existing image
similarity metrics (ISMs) can provide reliable, objective feedback in iterative
workflows, given that we do not fully understand if subjective human judgments
of similarity align with these metrics. Consequently, we must first validate
their alignment with human perception before assessing their potential as a
feedback mechanism in the iterative prompt refinement process. To address these
research gaps, we present a structured user study evaluating how iterative
prompt refinement affects the similarity of regenerated images relative to
their targets, while also examining whether ISMs capture the same improvements
perceived by human observers. Our findings suggest that incremental prompt
adjustments substantially improve alignment, verified through both subjective
evaluations and quantitative measures, underscoring the broader potential of
iterative workflows to enhance generative AI content creation across various
application domains.

</details>

### [169] [Skill Discovery for Software Scripting Automation via Offline Simulations with LLMs](https://arxiv.org/abs/2504.20406)
*Paiheng Xu,Gang Wu,Xiang Chen,Tong Yu,Chang Xiao,Franck Dernoncourt,Tianyi Zhou,Wei Ai,Viswanathan Swaminathan*

Main category: cs.AI

TLDR: 提出了一种离线模拟框架，利用LLMs和公开脚本指南生成已验证脚本的技能集，显著提高自动化成功率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 传统脚本编写需要编程知识，而LLMs生成的运行时代码存在安全风险和效率问题，需一种更优解决方案。

Method: 框架包含任务创建和技能生成两部分，结合GNN模型预测API协同性，优化脚本生成。

Result: 在Adobe Illustrator实验中，框架显著提升自动化成功率、减少响应时间并降低运行时成本。

Conclusion: 首次将软件脚本接口作为LLM系统测试平台，展示了在受控环境中利用执行反馈的优势。

Abstract: Scripting interfaces enable users to automate tasks and customize software
workflows, but creating scripts traditionally requires programming expertise
and familiarity with specific APIs, posing barriers for many users. While Large
Language Models (LLMs) can generate code from natural language queries, runtime
code generation is severely limited due to unverified code, security risks,
longer response times, and higher computational costs. To bridge the gap, we
propose an offline simulation framework to curate a software-specific skillset,
a collection of verified scripts, by exploiting LLMs and publicly available
scripting guides. Our framework comprises two components: (1) task creation,
using top-down functionality guidance and bottom-up API synergy exploration to
generate helpful tasks; and (2) skill generation with trials, refining and
validating scripts based on execution feedback. To efficiently navigate the
extensive API landscape, we introduce a Graph Neural Network (GNN)-based link
prediction model to capture API synergy, enabling the generation of skills
involving underutilized APIs and expanding the skillset's diversity.
Experiments with Adobe Illustrator demonstrate that our framework significantly
improves automation success rates, reduces response time, and saves runtime
token costs compared to traditional runtime code generation. This is the first
attempt to use software scripting interfaces as a testbed for LLM-based
systems, highlighting the advantages of leveraging execution feedback in a
controlled environment and offering valuable insights into aligning AI
capabilities with user needs in specialized software domains.

</details>

### [170] [RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library](https://arxiv.org/abs/2504.20426)
*Jiapeng Wang,Jinhao Jiang,Zhiqiang Zhang,Jun Zhou,Wayne Xin Zhao*

Main category: cs.AI

TLDR: RV-Syn提出了一种新的数学数据合成方法，通过构建结构化数学操作函数库和计算图，生成可验证的高质量推理数据，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数学数据合成方法在问题逻辑掌握和解决方案可验证性方面存在不足，需要改进。

Method: RV-Syn构建数学操作函数库，生成计算图作为解决方案，并反向翻译为复杂问题，确保逻辑感知和可验证性。

Result: 实验显示RV-Syn在数据扩展效率上优于现有方法，包括人工生成问题。

Conclusion: RV-Syn为生成高质量推理数据集提供了可扩展框架。

Abstract: The advancement of reasoning capabilities in Large Language Models (LLMs)
requires substantial amounts of high-quality reasoning data, particularly in
mathematics. Existing data synthesis methods, such as data augmentation from
annotated training sets or direct question generation based on relevant
knowledge points and documents, have expanded datasets but face challenges in
mastering the inner logic of the problem during generation and ensuring the
verifiability of the solutions. To address these issues, we propose RV-Syn, a
novel Rational and Verifiable mathematical Synthesis approach. RV-Syn
constructs a structured mathematical operation function library based on
initial seed problems and generates computational graphs as solutions by
combining Python-formatted functions from this library. These graphs are then
back-translated into complex problems. Based on the constructed computation
graph, we achieve solution-guided logic-aware problem generation. Furthermore,
the executability of the computational graph ensures the verifiability of the
solving process. Experimental results show that RV-Syn surpasses existing
synthesis methods, including those involving human-generated problems,
achieving greater efficient data scaling. This approach provides a scalable
framework for generating high-quality reasoning datasets.

</details>

### [171] [Head-Tail-Aware KL Divergence in Knowledge Distillation for Spiking Neural Networks](https://arxiv.org/abs/2504.20445)
*Tianqing Zhang,Zixin Zhu,Kairong Yu,Hongwei Wang*

Main category: cs.AI

TLDR: 论文提出了一种名为HTA-KL的新方法，通过动态区分高概率和低概率区域，优化了SNN的知识蒸馏性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法在SNN中表现不佳，未能充分利用SNN特性，导致性能差距。

Method: 提出HTA-KL方法，结合累积概率掩码和自适应权重，平衡知识转移。

Result: 在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上表现优于现有方法。

Conclusion: HTA-KL有效提升了SNN的性能，减少了时间步数需求。

Abstract: Spiking Neural Networks (SNNs) have emerged as a promising approach for
energy-efficient and biologically plausible computation. However, due to
limitations in existing training methods and inherent model constraints, SNNs
often exhibit a performance gap when compared to Artificial Neural Networks
(ANNs). Knowledge distillation (KD) has been explored as a technique to
transfer knowledge from ANN teacher models to SNN student models to mitigate
this gap. Traditional KD methods typically use Kullback-Leibler (KL) divergence
to align output distributions. However, conventional KL-based approaches fail
to fully exploit the unique characteristics of SNNs, as they tend to
overemphasize high-probability predictions while neglecting low-probability
ones, leading to suboptimal generalization. To address this, we propose
Head-Tail Aware Kullback-Leibler (HTA-KL) divergence, a novel KD method for
SNNs. HTA-KL introduces a cumulative probability-based mask to dynamically
distinguish between high- and low-probability regions. It assigns adaptive
weights to ensure balanced knowledge transfer, enhancing the overall
performance. By integrating forward KL (FKL) and reverse KL (RKL) divergence,
our method effectively align both head and tail regions of the distribution. We
evaluate our methods on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets. Our
method outperforms existing methods on most datasets with fewer timesteps.

</details>

### [172] [TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data](https://arxiv.org/abs/2504.20462)
*Qi Wang,Xiao Zhang,Mingyi Li,Yuan Yuan,Mengbai Xiao,Fuzhen Zhuang,Dongxiao Yu*

Main category: cs.AI

TLDR: 论文提出了一种名为TAMO的工具辅助LLM代理，用于解决微服务和云原生技术中的根因分析（RCA）问题。通过多模态观测数据和时间对齐表示，克服了现有LLM方法的局限性，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着分布式系统的发展，微服务和云原生技术带来了系统复杂性和操作挑战，传统RCA依赖人工干预，而现有LLM方法存在文本输入限制、动态服务依赖幻觉和上下文窗口限制等问题。

Method: 提出TAMO工具辅助LLM代理，统一多模态观测数据为时间对齐表示，结合专用工具进行根因定位和故障分类，并通过结构化提示指导LLM生成修复策略。

Result: 实验结果表明，TAMO在处理异构和常见故障类型的公共数据集时表现优异。

Conclusion: TAMO通过多模态数据和工具辅助，有效克服了LLM在RCA中的局限性，为AIOps提供了新解决方案。

Abstract: With the development of distributed systems, microservices and cloud native
technologies have become central to modern enterprise software development.
Despite bringing significant advantages, these technologies also increase
system complexity and operational challenges. Traditional root cause analysis
(RCA) struggles to achieve automated fault response, heavily relying on manual
intervention. In recent years, large language models (LLMs) have made
breakthroughs in contextual inference and domain knowledge integration,
providing new solutions for Artificial Intelligence for Operations (AIOps).
However, Existing LLM-based approaches face three key challenges: text input
constraints, dynamic service dependency hallucinations, and context window
limitations. To address these issues, we propose a tool-assisted LLM agent with
multi-modality observation data, namely TAMO, for fine-grained RCA. It unifies
multi-modal observational data into time-aligned representations to extract
consistent features and employs specialized root cause localization and fault
classification tools for perceiving the contextual environment. This approach
overcomes the limitations of LLM in handling real-time changing service
dependencies and raw observational data and guides LLM to generate repair
strategies aligned with system contexts by structuring key information into a
prompt. Experimental results show that TAMO performs well in root cause
analysis when dealing with public datasets characterized by heterogeneity and
common fault types, demonstrating its effectiveness.

</details>

### [173] [A Summary on GUI Agents with Foundation Models Enhanced by Reinforcement Learning](https://arxiv.org/abs/2504.20464)
*Jiahao Li,Kaer Huang*

Main category: cs.AI

TLDR: 本文总结了基于多模态大语言模型（MLLM）和强化学习（RL）的GUI智能代理的最新进展，包括任务形式化、架构模块、训练方法及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过MLLM和RL提升GUI智能代理的交互能力，以应对复杂现实环境中的任务。

Method: 将GUI代理任务形式化为马尔可夫决策过程，分析感知、规划、执行模块的架构演变，并分类训练方法为提示工程、监督微调和强化学习。

Result: 多模态感知、决策推理和自适应动作生成等创新显著提升了GUI代理的泛化能力和鲁棒性。

Conclusion: 未来需解决的关键挑战包括提升代理能力和可靠性，以推动GUI智能代理的进一步发展。

Abstract: Graphical User Interface (GUI) agents, driven by Multi-modal Large Language
Models (MLLMs), have emerged as a promising paradigm for enabling intelligent
interaction with digital systems. This paper provides a structured summary of
recent advances in GUI agents, focusing on architectures enhanced by
Reinforcement Learning (RL). We first formalize GUI agent tasks as Markov
Decision Processes and discuss typical execution environments and evaluation
metrics. We then review the modular architecture of (M)LLM-based GUI agents,
covering Perception, Planning, and Acting modules, and trace their evolution
through representative works. Furthermore, we categorize GUI agent training
methodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and
RL-based approaches, highlighting the progression from simple prompt
engineering to dynamic policy learning via RL. Our summary illustrates how
recent innovations in multimodal perception, decision reasoning, and adaptive
action generation have significantly improved the generalization and robustness
of GUI agents in complex real-world environments. We conclude by identifying
key challenges and future directions for building more capable and reliable GUI
agents.

</details>

### [174] [MuRAL: A Multi-Resident Ambient Sensor Dataset Annotated with Natural Language for Activities of Daily Living](https://arxiv.org/abs/2504.20505)
*Xi Chen,Julien Cumin,Fano Ramparany,Dominique Vaufreydaz*

Main category: cs.AI

TLDR: MuRAL是一个新的多居民环境传感器数据集，专为大型语言模型（LLM）设计，用于提升人类活动识别（HAR）任务。


<details>
  <summary>Details</summary>
Motivation: 现有数据集（如CASAS、ARAS、MARBLE）缺乏上下文丰富性和注释粒度，无法充分利用LLM的潜力。

Method: 引入MuRAL数据集，包含21小时的多用户传感器数据，标注了细粒度自然语言描述、居民身份和高层活动标签。

Result: LLM在MuRAL上表现良好，但在处理多用户模糊性和传感器上下文不足时仍有挑战。

Conclusion: MuRAL支持未来LLM驱动的、可解释的智能环境活动理解研究。

Abstract: Recent advances in Large Language Models (LLMs) have shown promising
potential for human activity recognition (HAR) using ambient sensors,
especially through natural language reasoning and zero-shot learning. However,
existing datasets such as CASAS, ARAS, and MARBLE were not originally designed
with LLMs in mind and therefore lack the contextual richness, complexity, and
annotation granularity required to fully exploit LLM capabilities. In this
paper, we introduce MuRAL, the first Multi-Resident Ambient sensor dataset with
natural Language, comprising over 21 hours of multi-user sensor data collected
from 21 sessions in a smart-home environment. MuRAL is annotated with
fine-grained natural language descriptions, resident identities, and high-level
activity labels, all situated in dynamic, realistic multi-resident settings. We
benchmark MuRAL using state-of-the-art LLMs for three core tasks: subject
assignment, action description, and activity classification. Our results
demonstrate that while LLMs can provide rich semantic interpretations of
ambient data, current models still face challenges in handling multi-user
ambiguity and under-specified sensor contexts. We release MuRAL to support
future research on LLM-powered, explainable, and socially aware activity
understanding in smart environments. For access to the dataset, please reach
out to us via the provided contact information. A direct link for dataset
retrieval will be made available at this location in due course.

</details>

### [175] [ReasonIR: Training Retrievers for Reasoning Tasks](https://arxiv.org/abs/2504.20595)
*Rulin Shao,Rui Qiao,Varsha Kishore,Niklas Muennighoff,Xi Victoria Lin,Daniela Rus,Bryan Kian Hsiang Low,Sewon Min,Wen-tau Yih,Pang Wei Koh,Luke Zettlemoyer*

Main category: cs.AI

TLDR: ReasonIR-8B是一种专为通用推理任务训练的检索模型，通过合成数据和公共数据混合训练，在推理密集型IR任务中表现优异，并显著提升RAG任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索模型在推理任务中表现有限，因为训练数据主要针对简单事实查询。

Method: 开发合成数据生成流程，创建挑战性查询和硬负样本，混合训练数据。

Result: 在BRIGHT基准上达到29.9 nDCG@10（无重排）和36.9 nDCG@10（有重排），在RAG任务中显著提升MMLU和GPQA性能。

Conclusion: ReasonIR-8B在推理任务中表现优异，训练方法通用且可扩展，代码和数据已开源。

Abstract: We present ReasonIR-8B, the first retriever specifically trained for general
reasoning tasks. Existing retrievers have shown limited gains on reasoning
tasks, in part because existing training datasets focus on short factual
queries tied to documents that straightforwardly answer them. We develop a
synthetic data generation pipeline that, for each document, our pipeline
creates a challenging and relevant query, along with a plausibly related but
ultimately unhelpful hard negative. By training on a mixture of our synthetic
data and existing public data, ReasonIR-8B achieves a new state-of-the-art of
29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a
widely-used reasoning-intensive information retrieval (IR) benchmark. When
applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%
and 22.6% respectively, relative to the closed-book baseline, outperforming
other retrievers and search engines. In addition, ReasonIR-8B uses test-time
compute more effectively: on BRIGHT, its performance consistently increases
with longer and more information-rich rewritten queries; it continues to
outperform other retrievers when combined with an LLM reranker. Our training
recipe is general and can be easily extended to future LLMs; to this end, we
open-source our code, data, and model.

</details>

### [176] [PaRT: Enhancing Proactive Social Chatbots with Personalized Real-Time Retrieval](https://arxiv.org/abs/2504.20624)
*Zihan Niu,Zheyong Xie,Shaosheng Cao,Chonggang Lu,Zheyu Ye,Tong Xu,Zuozhu Liu,Yan Gao,Jia Chen,Zhe Xu,Yi Wu,Yao Hu*

Main category: cs.AI

TLDR: PaRT框架通过个性化实时检索与生成，实现社交聊天机器人的上下文感知主动对话，显著提升对话时长。


<details>
  <summary>Details</summary>
Motivation: 传统聊天机器人依赖用户发起或维持对话，导致参与度低和对话时长缩短，需要更主动的对话机制。

Method: 结合用户画像和对话上下文，利用大语言模型（LLM）生成个性化话题，检索相关知识并生成优化响应。

Result: 在真实生产环境中稳定运行30天，对话平均时长提升21.77%。

Conclusion: PaRT框架有效提升社交聊天机器人的主动对话能力和用户参与度。

Abstract: Social chatbots have become essential intelligent companions in daily
scenarios ranging from emotional support to personal interaction. However,
conventional chatbots with passive response mechanisms usually rely on users to
initiate or sustain dialogues by bringing up new topics, resulting in
diminished engagement and shortened dialogue duration. In this paper, we
present PaRT, a novel framework enabling context-aware proactive dialogues for
social chatbots through personalized real-time retrieval and generation.
Specifically, PaRT first integrates user profiles and dialogue context into a
large language model (LLM), which is initially prompted to refine user queries
and recognize their underlying intents for the upcoming conversation. Guided by
refined intents, the LLM generates personalized dialogue topics, which then
serve as targeted queries to retrieve relevant passages from RedNote. Finally,
we prompt LLMs with summarized passages to generate knowledge-grounded and
engagement-optimized responses. Our approach has been running stably in a
real-world production environment for more than 30 days, achieving a 21.77\%
improvement in the average duration of dialogues.

</details>

### [177] [Cognitive maps are generative programs](https://arxiv.org/abs/2504.20628)
*Marta Kryven,Cole Wyeth,Aidan Curtis,Kevin Ellis*

Main category: cs.AI

TLDR: 论文探讨人类资源高效规划可能源于将世界表示为可预测结构，提出认知地图可表现为生成程序，并通过实验和计算模型验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究人类如何在有限资源下构建功能性世界表征，探索认知地图的生成程序形式如何提高规划效率。

Method: 结合行为实验和计算模型，利用生成程序表示认知地图，并基于人类先验知识推断程序化认知地图分布。

Result: 模型在计算效率、内存需求和预测人类行为方面优于非结构化规划算法，支持程序化认知地图的假设。

Conclusion: 人类规划策略依赖于程序化认知地图，这种表征形式显著提高了资源利用效率。

Abstract: Making sense of the world and acting in it relies on building simplified
mental representations that abstract away aspects of reality. This principle of
cognitive mapping is universal to agents with limited resources. Living
organisms, people, and algorithms all face the problem of forming functional
representations of their world under various computing constraints. In this
work, we explore the hypothesis that human resource-efficient planning may
arise from representing the world as predictably structured. Building on the
metaphor of concepts as programs, we propose that cognitive maps can take the
form of generative programs that exploit predictability and redundancy, in
contrast to directly encoding spatial layouts. We use a behavioral experiment
to show that people who navigate in structured spaces rely on modular planning
strategies that align with programmatic map representations. We describe a
computational model that predicts human behavior in a variety of structured
scenarios. This model infers a small distribution over possible programmatic
cognitive maps conditioned on human prior knowledge of the world, and uses this
distribution to generate resource-efficient plans. Our models leverages a Large
Language Model as an embedding of human priors, implicitly learned through
training on a vast corpus of human data. Our model demonstrates improved
computational efficiency, requires drastically less memory, and outperforms
unstructured planning algorithms with cognitive constraints at predicting human
behavior, suggesting that human planning strategies rely on programmatic
cognitive maps.

</details>

### [178] [The Limits of AI Explainability: An Algorithmic Information Theory Approach](https://arxiv.org/abs/2504.20676)
*Shrisha Rao*

Main category: cs.AI

TLDR: 论文通过算法信息理论为AI可解释性建立了理论基础，量化了近似误差和解释复杂性，提出了复杂性差距定理、局部与全局可解释性差距，并证明了监管不可能性定理。


<details>
  <summary>Details</summary>
Motivation: 研究AI可解释性的理论极限，为设计和评估可解释AI系统提供理论基础。

Method: 使用Kolmogorov复杂性量化解释复杂性，提出复杂性差距定理、边界条件和局部与全局可解释性差距分析。

Result: 证明解释复杂性随输入维度指数增长，但随误差容忍度多项式增长；局部解释可显著简化；监管框架无法同时满足无限制AI能力、人类可解释性和低误差。

Conclusion: 研究结果为可解释AI系统的设计、评估和监管提供了重要理论依据。

Abstract: This paper establishes a theoretical foundation for understanding the
fundamental limits of AI explainability through algorithmic information theory.
We formalize explainability as the approximation of complex models by simpler
ones, quantifying both approximation error and explanation complexity using
Kolmogorov complexity. Our key theoretical contributions include: (1) a
complexity gap theorem proving that any explanation significantly simpler than
the original model must differ from it on some inputs; (2) precise bounds
showing that explanation complexity grows exponentially with input dimension
but polynomially with error tolerance for Lipschitz functions; and (3) a
characterization of the gap between local and global explainability,
demonstrating that local explanations can be significantly simpler while
maintaining accuracy in relevant regions. We further establish a regulatory
impossibility theorem proving that no governance framework can simultaneously
pursue unrestricted AI capabilities, human-interpretable explanations, and
negligible error. These results highlight considerations likely to be relevant
to the design, evaluation, and oversight of explainable AI systems.

</details>

### [179] [Graph-Based Fault Diagnosis for Rotating Machinery: Adaptive Segmentation and Structural Feature Integration](https://arxiv.org/abs/2504.20756)
*Moirangthem Tiken Singh*

Main category: cs.AI

TLDR: 提出了一种基于图的新型框架，用于旋转机械的鲁棒且可解释的多类故障诊断。该方法结合熵优化信号分割、时频特征提取和图论建模，将振动信号转化为适合分类的结构化表示。


<details>
  <summary>Details</summary>
Motivation: 传统方法在噪声环境下表现不佳且缺乏可解释性，而深度学习模型复杂度高。本文旨在提出一种低复杂度、高鲁棒性且可解释的故障诊断方法。

Method: 通过熵优化信号分割、时频特征提取和图论建模，提取全局和局部故障特征，并使用逻辑回归分类器进行分类。

Result: 在CWRU和SU数据集上分别达到99.8%和100%的准确率，噪声环境下保持95.4%准确率，跨域迁移性能达99.7% F1分数。

Conclusion: 该方法无需深度学习架构，复杂度低且可解释，具有工业实时部署的潜力。

Abstract: This paper proposes a novel graph-based framework for robust and
interpretable multiclass fault diagnosis in rotating machinery. The method
integrates entropy-optimized signal segmentation, time-frequency feature
extraction, and graph-theoretic modeling to transform vibration signals into
structured representations suitable for classification. Graph metrics, such as
average shortest path length, modularity, and spectral gap, are computed and
combined with local features to capture global and segment-level fault
characteristics. The proposed method achieves high diagnostic accuracy when
evaluated on two benchmark datasets, the CWRU bearing dataset (under 0-3 HP
loads) and the SU gearbox and bearing datasets (under different speed-load
configurations). Classification scores reach up to 99.8% accuracy on Case
Western Reserve University (CWRU) and 100% accuracy on the Southeast University
datasets using a logistic regression classifier. Furthermore, the model
exhibits strong noise resilience, maintaining over 95.4% accuracy at high noise
levels (standard deviation = 0.5), and demonstrates excellent cross-domain
transferability with up to 99.7% F1-score in load-transfer scenarios. Compared
to traditional techniques, this approach requires no deep learning
architecture, enabling lower complexity while ensuring interpretability. The
results confirm the method's scalability, reliability, and potential for
real-time deployment in industrial diagnostics.

</details>

### [180] [Approximate Lifted Model Construction](https://arxiv.org/abs/2504.20784)
*Malte Luttermann,Jan Speller,Marcel Gehrke,Tanya Braun,Ralf Möller,Mattis Hartwig*

Main category: cs.AI

TLDR: 论文提出了一种改进的算法ε-ACP，用于在概率关系模型中处理潜在函数不完全匹配的情况，通过允许一定的偏差ε来高效识别和利用近似不可区分性。


<details>
  <summary>Details</summary>
Motivation: 现有的ACP算法要求潜在函数完全匹配才能识别不可区分性，但在实际应用中，从数据学习的潜在函数难免存在偏差，导致ACP不适用。

Method: 提出ε-ACP算法，通过引入超参数ε，允许潜在函数存在一定偏差，从而识别和利用近似不可区分性。

Result: 理论证明ε-ACP的近似误差严格有界，实验表明实际误差接近于零。

Conclusion: ε-ACP解决了ACP在实际应用中的局限性，能够高效处理近似不可区分性，且误差可控。

Abstract: Probabilistic relational models such as parametric factor graphs enable
efficient (lifted) inference by exploiting the indistinguishability of objects.
In lifted inference, a representative of indistinguishable objects is used for
computations. To obtain a relational (i.e., lifted) representation, the
Advanced Colour Passing (ACP) algorithm is the state of the art. The ACP
algorithm, however, requires underlying distributions, encoded as
potential-based factorisations, to exactly match to identify and exploit
indistinguishabilities. Hence, ACP is unsuitable for practical applications
where potentials learned from data inevitably deviate even if associated
objects are indistinguishable. To mitigate this problem, we introduce the
$\varepsilon$-Advanced Colour Passing ($\varepsilon$-ACP) algorithm, which
allows for a deviation of potentials depending on a hyperparameter
$\varepsilon$. $\varepsilon$-ACP efficiently uncovers and exploits
indistinguishabilities that are not exact. We prove that the approximation
error induced by $\varepsilon$-ACP is strictly bounded and our experiments show
that the approximation error is close to zero in practice.

</details>

### [181] [Partitioned Memory Storage Inspired Few-Shot Class-Incremental learning](https://arxiv.org/abs/2504.20797)
*Renye Zhang,Yimin Yin,Jinghua Zhang*

Main category: cs.AI

TLDR: 论文提出了一种新的Few-Shot Class-Incremental Learning方法，通过为每个会话学习独立模型，避免灾难性遗忘，并结合不确定性量化提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习技术依赖大量数据且缺乏动态适应性，与人类智能差距较大。FSCIL旨在解决这一问题，但现有方法存在稳定性与可塑性矛盾。

Method: 为每个会话学习独立模型，避免知识混淆，并在测试阶段结合不确定性量化进行模型部署。

Result: 在CIFAR-100和mini-ImageNet数据集上达到最先进性能。

Conclusion: 该方法为FSCIL提供了新视角，有效解决了灾难性遗忘问题。

Abstract: Current mainstream deep learning techniques exhibit an over-reliance on
extensive training data and a lack of adaptability to the dynamic world,
marking a considerable disparity from human intelligence. To bridge this gap,
Few-Shot Class-Incremental Learning (FSCIL) has emerged, focusing on continuous
learning of new categories with limited samples without forgetting old
knowledge. Existing FSCIL studies typically use a single model to learn
knowledge across all sessions, inevitably leading to the stability-plasticity
dilemma. Unlike machines, humans store varied knowledge in different cerebral
cortices. Inspired by this characteristic, our paper aims to develop a method
that learns independent models for each session. It can inherently prevent
catastrophic forgetting. During the testing stage, our method integrates
Uncertainty Quantification (UQ) for model deployment. Our method provides a
fresh viewpoint for FSCIL and demonstrates the state-of-the-art performance on
CIFAR-100 and mini-ImageNet datasets.

</details>

### [182] [Ascendra: Dynamic Request Prioritization for Efficient LLM Serving](https://arxiv.org/abs/2504.20828)
*Azam Ikram,Xiang Li,Sameh Elnikety,Saurabh Bagchi*

Main category: cs.AI

TLDR: Ascendra是一种LLM服务系统，通过分区GPU资源为高低优先级实例，同时满足TTFT和TBT的SLO要求，显著提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有系统通常牺牲一个指标（TTFT或TBT）以优化另一个，无法同时满足两者的SLO要求。

Method: Ascendra将GPU资源分为高低优先级实例，低优先级实例优化吞吐量，高优先级实例处理紧急请求以避免SLO违约。

Result: Ascendra的吞吐量比vLLM和Sarathi-Serve高1.7倍，同时满足TTFT和TBT的SLO。

Conclusion: Ascendra通过动态资源分区和优先级管理，有效平衡了高吞吐量和低延迟的需求。

Abstract: The rapid advancement of Large Language Models (LLMs) has driven the need for
more efficient serving strategies. In this context, efficiency refers to the
proportion of requests that meet their Service Level Objectives (SLOs),
particularly for Time To First Token (TTFT) and Time Between Tokens (TBT).
However, existing systems often prioritize one metric at the cost of the other.
We present Ascendra, an LLM serving system designed to meet both TTFT and TBT
SLOs simultaneously. The core insight behind Ascendra is that a request's
urgency evolves as it approaches its deadline. To leverage this, Ascendra
partitions GPU resources into two types of instances: low-priority and
high-priority. Low-priority instances maximize throughput by processing
requests out of arrival order, but at the risk of request starvation. To
address this, Ascendra employs a performance model to predict requests at risk
of missing their SLOs and proactively offloads them to high-priority instances.
High-priority instances are optimized for low-latency execution and handle
urgent requests nearing their deadlines. This partitioned architecture enables
Ascendra to effectively balance high throughput and low latency. Extensive
evaluation shows that Ascendra improves system throughput by up to 1.7x
compared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs.

</details>

### [183] [Disjunctive and Conjunctive Normal Form Explanations of Clusters Using Auxiliary Information](https://arxiv.org/abs/2504.20846)
*Robert F. Downey,S. S. Ravi*

Main category: cs.AI

TLDR: 论文提出了一种利用未用于聚类的辅助信息（标签）生成聚类后解释的方法，包括析取形式和两子句合取范式（CNF）解释，并采用整数线性规划（ILP）和启发式方法生成解释。


<details>
  <summary>Details</summary>
Motivation: 旨在通过辅助信息为聚类结果提供可解释性，帮助理解聚类背后的逻辑。

Method: 使用整数线性规划（ILP）和启发式方法生成析取形式和两子句CNF形式的解释。

Result: 通过多种数据集验证了方法的有效性，并展示了其可扩展性。

Conclusion: 提出的方法能够有效生成聚类解释，为理解聚类结果提供了新视角。

Abstract: We consider generating post-hoc explanations of clusters generated from
various datasets using auxiliary information which was not used by clustering
algorithms. Following terminology used in previous work, we refer to the
auxiliary information as tags. Our focus is on two forms of explanations,
namely disjunctive form (where the explanation for a cluster consists of a set
of tags) and a two-clause conjunctive normal form (CNF) explanation (where the
explanation consists of two sets of tags, combined through the AND operator).
We use integer linear programming (ILP) as well as heuristic methods to
generate these explanations. We experiment with a variety of datasets and
discuss the insights obtained from our explanations. We also present
experimental results regarding the scalability of our explanation methods.

</details>

### [184] [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879)
*Shivalika Singh,Yiyang Nan,Alex Wang,Daniel D'Souza,Sayash Kapoor,Ahmet Üstün,Sanmi Koyejo,Yuntian Deng,Shayne Longpre,Noah Smith,Beyza Ermis,Marzieh Fadaee,Sara Hooker*

Main category: cs.AI

TLDR: 论文指出Chatbot Arena排行榜存在系统性偏差，包括未公开的私人测试实践、选择性披露结果以及数据访问不对称，导致排行榜失真。


<details>
  <summary>Details</summary>
Motivation: 衡量进展对科学领域至关重要，但当前AI系统排行榜存在不公平现象，影响评估的公正性。

Method: 通过分析Chatbot Arena的数据和实践，识别私人测试、选择性披露和数据分配不均等问题。

Result: 发现私人测试和选择性披露导致排行榜失真，封闭模型获得更多数据访问，开放模型处于劣势。

Conclusion: 提出改革建议，以促进更公平、透明的AI系统评估框架。

Abstract: Measuring progress is fundamental to the advancement of any scientific field.
As benchmarks play an increasingly central role, they also grow more
susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard
for ranking the most capable AI systems. Yet, in this work we identify
systematic issues that have resulted in a distorted playing field. We find that
undisclosed private testing practices benefit a handful of providers who are
able to test multiple variants before public release and retract scores if
desired. We establish that the ability of these providers to choose the best
score leads to biased Arena scores due to selective disclosure of performance
results. At an extreme, we identify 27 private LLM variants tested by Meta in
the lead-up to the Llama-4 release. We also establish that proprietary closed
models are sampled at higher rates (number of battles) and have fewer models
removed from the arena than open-weight and open-source alternatives. Both
these policies lead to large data access asymmetries over time. Providers like
Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the
arena, respectively. In contrast, a combined 83 open-weight models have only
received an estimated 29.7% of the total data. We show that access to Chatbot
Arena data yields substantial benefits; even limited additional data can result
in relative performance gains of up to 112% on the arena distribution, based on
our conservative estimates. Together, these dynamics result in overfitting to
Arena-specific dynamics rather than general model quality. The Arena builds on
the substantial efforts of both the organizers and an open community that
maintains this valuable evaluation platform. We offer actionable
recommendations to reform the Chatbot Arena's evaluation framework and promote
fairer, more transparent benchmarking for the field

</details>

### [185] [CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models](https://arxiv.org/abs/2504.20898)
*Hasan Md Tusfiqur Alam,Devansh Srivastav,Abdulrahman Mohamed Selim,Md Abdul Kadir,Md Moktadiurl Hoque Shuvo,Daniel Sonntag*

Main category: cs.AI

TLDR: 论文提出了一种结合概念瓶颈模型（CBM）和多智能体检索增强生成（RAG）的自动化放射学报告生成框架，旨在提升AI的可解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在放射学工作流自动化中潜力巨大，但可解释性和可靠性问题阻碍了临床应用。

Method: 框架结合CBM（将胸部X光特征映射到临床概念）和RAG（多智能体协作与外部知识结合），生成透明且证据丰富的报告。

Result: 系统能够提供可解释的预测、减少幻觉，并生成高质量、定制化的报告，同时提升准确性、信任度和可用性。

Conclusion: 该框架为提升诊断一致性、为放射科医生提供可操作见解提供了可行路径。

Abstract: Advancements in generative Artificial Intelligence (AI) hold great promise
for automating radiology workflows, yet challenges in interpretability and
reliability hinder clinical adoption. This paper presents an automated
radiology report generation framework that combines Concept Bottleneck Models
(CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge
AI performance with clinical explainability. CBMs map chest X-ray features to
human-understandable clinical concepts, enabling transparent disease
classification. Meanwhile, the RAG system integrates multi-agent collaboration
and external knowledge to produce contextually rich, evidence-based reports.
Our demonstration showcases the system's ability to deliver interpretable
predictions, mitigate hallucinations, and generate high-quality, tailored
reports with an interactive interface addressing accuracy, trust, and usability
challenges. This framework provides a pathway to improving diagnostic
consistency and empowering radiologists with actionable insights.

</details>

### [186] [Leveraging Generative AI Through Prompt Engineering and Rigorous Validation to Create Comprehensive Synthetic Datasets for AI Training in Healthcare](https://arxiv.org/abs/2504.20921)
*Polycarp Nalela*

Main category: cs.AI

TLDR: 利用GPT-4 API生成高质量合成医疗数据，通过严格验证解决隐私问题，支持AI算法训练。


<details>
  <summary>Details</summary>
Motivation: 医疗数据因隐私问题难以获取，限制了AI算法的训练。

Method: 采用GPT-4 API生成合成数据，结合BERT、GPT-2、RoBERTa等模型进行多维度验证。

Result: 成功生成并通过验证的合成数据被整合到PostgreSQL数据库中。

Conclusion: 生成式AI模型结合严格验证可有效解决医疗数据隐私问题，支持AI训练。

Abstract: Access to high-quality medical data is often restricted due to privacy
concerns, posing significant challenges for training artificial intelligence
(AI) algorithms within Electronic Health Record (EHR) applications. In this
study, prompt engineering with the GPT-4 API was employed to generate
high-quality synthetic datasets aimed at overcoming this limitation. The
generated data encompassed a comprehensive array of patient admission
information, including healthcare provider details, hospital departments,
wards, bed assignments, patient demographics, emergency contacts, vital signs,
immunizations, allergies, medical histories, appointments, hospital visits,
laboratory tests, diagnoses, treatment plans, medications, clinical notes,
visit logs, discharge summaries, and referrals. To ensure data quality and
integrity, advanced validation techniques were implemented utilizing models
such as BERT's Next Sentence Prediction for sentence coherence, GPT-2 for
overall plausibility, RoBERTa for logical consistency, autoencoders for anomaly
detection, and conducted diversity analysis. Synthetic data that met all
validation criteria were integrated into a comprehensive PostgreSQL database,
serving as the data management system for the EHR application. This approach
demonstrates that leveraging generative AI models with rigorous validation can
effectively produce high-quality synthetic medical data, facilitating the
training of AI algorithms while addressing privacy concerns associated with
real patient data.

</details>

### [187] [A Domain-Agnostic Scalable AI Safety Ensuring Framework](https://arxiv.org/abs/2504.20924)
*Beomjun Kim,Kangyeon Kim,Sunwoo Kim,Heejin Ahn*

Main category: cs.AI

TLDR: 提出了一种新型AI安全框架，确保AI系统满足用户定义的约束条件，适用于多领域，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全方法局限于特定领域，无法泛化，需一种能适应多领域且满足用户自定义约束的解决方案。

Method: 结合AI组件与优化问题，利用内部测试数据和保守测试方法，确保约束满足概率，并提供损失函数近似方法。

Result: 数学证明约束满足概率的保证，实验显示在低安全阈值区域优于现有方法，且随测试数据规模扩展有效。

Conclusion: 该框架为AI系统在多领域中的安全部署提供了通用且可扩展的解决方案。

Abstract: Ensuring the safety of AI systems has recently emerged as a critical priority
for real-world deployment, particularly in physical AI applications. Current
approaches to AI safety typically address predefined domain-specific safety
conditions, limiting their ability to generalize across contexts.
  We propose a novel AI safety framework that ensures AI systems comply with
\textbf{any user-defined constraint}, with \textbf{any desired probability},
and across \textbf{various domains}.
  In this framework, we combine an AI component (e.g., neural network) with an
optimization problem to produce responses that minimize objectives while
satisfying user-defined constraints with probabilities exceeding user-defined
thresholds. For credibility assessment of the AI component, we propose
\textit{internal test data}, a supplementary set of safety-labeled data, and a
\textit{conservative testing} methodology that provides statistical validity of
using internal test data. We also present an approximation method of a loss
function and how to compute its gradient for training.
  We mathematically prove that probabilistic constraint satisfaction is
guaranteed under specific, mild conditions and prove a scaling law between
safety and the number of internal test data. We demonstrate our framework's
effectiveness through experiments in diverse domains: demand prediction for
production decision, safe reinforcement learning within the SafetyGym
simulator, and guarding AI chatbot outputs. Through these experiments, we
demonstrate that our method guarantees safety for user-specified constraints,
outperforms {for \textbf{up to several order of magnitudes}} existing methods
in low safety threshold regions, and scales effectively with respect to the
size of internal test data.

</details>

### [188] [ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification](https://arxiv.org/abs/2504.20930)
*Ziqing Fan,Cheng Liang,Chaoyi Wu,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: cs.AI

TLDR: ChestX-Reasoner是一种放射学诊断多模态大语言模型（MLLM），通过从临床报告中提取结构化推理链，模拟放射科医生的逐步推理过程，显著提升了诊断准确性和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前医学AI模型常忽略临床实践中的结构化推理过程，ChestX-Reasoner旨在填补这一空白。

Method: 构建大型数据集，提取并精炼放射学报告中的推理链；采用两阶段训练框架（监督微调和强化学习），结合过程奖励优化模型推理。

Result: ChestX-Reasoner在诊断准确性和推理能力上均优于现有医学和通用领域MLLM，推理能力提升16%、5.9%和18%，诊断准确性提升3.3%、24%和27%。

Conclusion: ChestX-Reasoner通过模拟临床推理过程显著提升性能，所有资源开源以推动医学推理MLLM研究。

Abstract: Recent advances in reasoning-enhanced large language models (LLMs) and
multimodal LLMs (MLLMs) have significantly improved performance in complex
tasks, yet medical AI models often overlook the structured reasoning processes
inherent in clinical practice. In this work, we present ChestX-Reasoner, a
radiology diagnosis MLLM designed to leverage process supervision mined
directly from clinical reports, reflecting the step-by-step reasoning followed
by radiologists. We construct a large dataset by extracting and refining
reasoning chains from routine radiology reports. Our two-stage training
framework combines supervised fine-tuning and reinforcement learning guided by
process rewards to better align model reasoning with clinical standards. We
introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual
question answering samples with 301K clinically validated reasoning steps, and
propose RadRScore, a metric evaluating reasoning factuality, completeness, and
effectiveness. ChestX-Reasoner outperforms existing medical and general-domain
MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,
and 18% improvements in reasoning ability compared to the best medical MLLM,
the best general MLLM, and its base model, respectively, as well as 3.3%, 24%,
and 27% improvements in outcome accuracy. All resources are open-sourced to
facilitate further research in medical reasoning MLLMs.

</details>

### [189] [Jekyll-and-Hyde Tipping Point in an AI's Behavior](https://arxiv.org/abs/2504.20980)
*Neil F. Johnson,Frank Yingjie Huo*

Main category: cs.AI

TLDR: 论文提出了一个基于基本原理的精确公式，用于预测LLM（如ChatGPT）输出何时会突然变得错误、误导、无关或危险，并解释了如何通过调整提示和训练来延迟或防止这种转折点。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏科学方法预测或解释LLM输出的突然变化，导致公众对AI的信任受损，甚至引发对LLM的过度谨慎行为（如礼貌对待）。

Method: 从基本原理推导出一个精确公式，仅需中学数学知识，揭示了LLM注意力分散导致突然崩溃的机制。

Result: 该公式提供了定量预测，指导如何通过调整提示和训练来延迟或防止LLM输出的转折点。

Conclusion: 该研究为政策制定者和公众提供了一个透明的基础，用于讨论AI的广泛使用和风险，并解答了诸如是否应对LLM保持礼貌等实际问题。

Abstract: Trust in AI is undermined by the fact that there is no science that predicts
-- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is
likely to tip mid-response to become wrong, misleading, irrelevant or
dangerous. With deaths and trauma already being blamed on LLMs, this
uncertainty is even pushing people to treat their 'pet' LLM more politely to
'dissuade' it (or its future Artificial General Intelligence offspring) from
suddenly turning on them. Here we address this acute need by deriving from
first principles an exact formula for when a Jekyll-and-Hyde tipping point
occurs at LLMs' most basic level. Requiring only secondary school mathematics,
it shows the cause to be the AI's attention spreading so thin it suddenly
snaps. This exact formula provides quantitative predictions for how the
tipping-point can be delayed or prevented by changing the prompt and the AI's
training. Tailored generalizations will provide policymakers and the public
with a firm platform for discussing any of AI's broader uses and risks, e.g. as
a personal counselor, medical advisor, decision-maker for when to use force in
a conflict situation. It also meets the need for clear and transparent answers
to questions like ''should I be polite to my LLM?''

</details>

### [190] [LTLf Adaptive Synthesis for Multi-Tier Goals in Nondeterministic Domains](https://arxiv.org/abs/2504.20983)
*Giuseppe De Giacomo,Gianmarco Parretti,Shufang Zhu*

Main category: cs.AI

TLDR: 该论文研究了一种LTLf合成的变体，用于在非确定性规划领域中合成实现多层级目标的适应性策略。


<details>
  <summary>Details</summary>
Motivation: 解决在多层级目标（由多个逐渐具有挑战性的LTLf目标组成）下，如何在非确定性环境中动态调整策略以最大化目标达成的问题。

Method: 提出了一种基于博弈论的技术，用于计算适应性策略，该技术是多项式时间复杂度的（二次）。

Result: 该技术是完备且正确的，能够动态调整策略以利用环境合作，同时仅比标准LTLf合成增加少量开销。

Conclusion: 该研究为处理多层级目标提供了一种高效的适应性策略合成方法，适用于非确定性规划领域。

Abstract: We study a variant of LTLf synthesis that synthesizes adaptive strategies for
achieving a multi-tier goal, consisting of multiple increasingly challenging
LTLf objectives in nondeterministic planning domains. Adaptive strategies are
strategies that at any point of their execution (i) enforce the satisfaction of
as many objectives as possible in the multi-tier goal, and (ii) exploit
possible cooperation from the environment to satisfy as many as possible of the
remaining ones. This happens dynamically: if the environment cooperates (ii)
and an objective becomes enforceable (i), then our strategies will enforce it.
We provide a game-theoretic technique to compute adaptive strategies that is
sound and complete. Notably, our technique is polynomial, in fact quadratic, in
the number of objectives. In other words, it handles multi-tier goals with only
a minor overhead compared to standard LTLf synthesis.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [191] [Can Geometry Save Central Views for Sports Field Registration?](https://arxiv.org/abs/2504.20052)
*Floriane Magera,Thomas Hoyoux,Martin Castin,Olivier Barnich,Anthony Cioppa,Marc Van Droogenbroeck*

Main category: cs.CV

TLDR: 提出了一种利用圆形标记进行运动场地注册的新方法，解决了现有方法在近距离视角下仅依赖线性标记的局限性。


<details>
  <summary>Details</summary>
Motivation: 运动场地注册是体育分析、裁判和球迷互动的基础，但现有方法在近距离视角下因稀疏和不均匀的标记分布而表现不佳。

Method: 通过从圆形对应关系中推导出一组点和线，将圆形标记纳入线性方程组，从而利用圆形标记进行场地注册和图像标注。

Result: 实验表明，该方法在困难场景下成功补充了高性能检测器，实现了运动场地的有效注册。

Conclusion: 提出的几何方法能够有效利用圆形标记，解决了现有方法的局限性，提升了运动场地注册的鲁棒性。

Abstract: Single-frame sports field registration often serves as the foundation for
extracting 3D information from broadcast videos, enabling applications related
to sports analytics, refereeing, or fan engagement. As sports fields have
rigorous specifications in terms of shape and dimensions of their line, circle
and point components, sports field markings are commonly used as calibration
targets for this task. However, because of the sparse and uneven distribution
of field markings, close-up camera views around central areas of the field
often depict only line and circle markings. On these views, sports field
registration is challenging for the vast majority of existing methods, as they
focus on leveraging line field markings and their intersections. It is indeed a
challenge to include circle correspondences in a set of linear equations. In
this work, we propose a novel method to derive a set of points and lines from
circle correspondences, enabling the exploitation of circle correspondences for
both sports field registration and image annotation. In our experiments, we
illustrate the benefits of our bottom-up geometric method against
top-performing detectors and show that our method successfully complements
them, enabling sports field registration in difficult scenarios.

</details>

### [192] [Marmot: Multi-Agent Reasoning for Multi-Object Self-Correcting in Improving Image-Text Alignment](https://arxiv.org/abs/2504.20054)
*Jiayang Sun,Hongbo Wang,Jie Cao,Huaibo Huang,Ran He*

Main category: cs.CV

TLDR: Marmot框架通过多智能体推理解决扩散模型在复杂多对象场景中的计数、属性和空间关系问题，提升图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在多对象场景中难以准确处理计数、属性和空间关系，需要一种通用框架提升图像-文本对齐和编辑连贯性。

Method: 采用分治策略将自校正任务分解为三个维度（计数、属性、空间关系），并进一步细化为对象级子任务，构建多智能体编辑系统，结合决策-执行-验证机制和像素域拼接平滑器。

Result: 实验表明Marmot显著提升了图像生成任务中对象计数、属性分配和空间关系的准确性。

Conclusion: Marmot通过多智能体推理和优化方法，有效解决了扩散模型在多对象场景中的局限性，提升了图像生成和编辑的可靠性。

Abstract: While diffusion models excel at generating high-quality images, they often
struggle with accurate counting, attributes, and spatial relationships in
complex multi-object scenes. To address these challenges, we propose Marmot, a
novel and generalizable framework that employs Multi-Agent Reasoning for
Multi-Object Self-Correcting, enhancing image-text alignment and facilitating
more coherent multi-object image editing. Our framework adopts a
divide-and-conquer strategy that decomposes the self-correction task into three
critical dimensions (counting, attributes, and spatial relationships), and
further divided into object-level subtasks. We construct a multi-agent editing
system featuring a decision-execution-verification mechanism, effectively
mitigating inter-object interference and enhancing editing reliability. To
resolve the problem of subtask integration, we propose a Pixel-Domain Stitching
Smoother that employs mask-guided two-stage latent space optimization. This
innovation enables parallel processing of subtask results, thereby enhancing
runtime efficiency while eliminating multi-stage distortion accumulation.
Extensive experiments demonstrate that Marmot significantly improves accuracy
in object counting, attribute assignment, and spatial relationships for image
generation tasks.

</details>

### [193] [Edge-Based Learning for Improved Classification Under Adversarial Noise](https://arxiv.org/abs/2504.20077)
*Manish Kansana,Keyan Alexander Rahimi,Elias Hossain,Iman Dehzangi,Noorbakhsh Amiri Golilarz*

Main category: cs.CV

TLDR: 研究探讨了对抗性噪声对图像分类的影响，发现边缘特征训练能提升模型对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 对抗性噪声会误导深度学习模型，研究旨在探索通过特定图像特征（如边缘）训练是否能提高模型鲁棒性。

Method: 使用FGSM对抗噪声，分别在原始图像和边缘特征上训练模型，并测试其对抗攻击的鲁棒性。

Result: 边缘特征训练的模型对对抗攻击更具抵抗力，但原始数据重训练后的精度提升略高于边缘数据。

Conclusion: 利用边缘特征学习可增强深度学习模型对抗对抗性扰动的能力。

Abstract: Adversarial noise introduces small perturbations in images, misleading deep
learning models into misclassification and significantly impacting recognition
accuracy. In this study, we analyzed the effects of Fast Gradient Sign Method
(FGSM) adversarial noise on image classification and investigated whether
training on specific image features can improve robustness. We hypothesize that
while adversarial noise perturbs various regions of an image, edges may remain
relatively stable and provide essential structural information for
classification. To test this, we conducted a series of experiments using brain
tumor and COVID datasets. Initially, we trained the models on clean images and
then introduced subtle adversarial perturbations, which caused deep learning
models to significantly misclassify the images. Retraining on a combination of
clean and noisy images led to improved performance. To evaluate the robustness
of the edge features, we extracted edges from the original/clean images and
trained the models exclusively on edge-based representations. When noise was
introduced to the images, the edge-based models demonstrated greater resilience
to adversarial attacks compared to those trained on the original or clean
images. These results suggest that while adversarial noise is able to exploit
complex non-edge regions significantly more than edges, the improvement in the
accuracy after retraining is marginally more in the original data as compared
to the edges. Thus, leveraging edge-based learning can improve the resilience
of deep learning models against adversarial perturbations.

</details>

### [194] [VideoMultiAgents: A Multi-Agent Framework for Video Question Answering](https://arxiv.org/abs/2504.20091)
*Noriyuki Kugo,Xiang Li,Zixin Li,Ashish Gupta,Arpandeep Khatua,Nidhish Jain,Chaitanya Patel,Yuta Kyuragi,Masamoto Tanabiki,Kazuki Kozuka,Ehsan Adeli*

Main category: cs.CV

TLDR: VideoMultiAgents框架通过多模态推理提升视频问答性能，结合视觉、场景图和文本处理代理，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答方法依赖单一模型处理帧级描述，难以捕捉时间和交互上下文。

Method: 提出VideoMultiAgents框架，整合视觉、场景图和文本处理代理，并引入问题引导的标题生成。

Result: 在Intent-QA、EgoSchema子集和NExT-QA上实现SOTA性能，分别提升6.2%、3.4%和0.4%。

Conclusion: VideoMultiAgents通过多代理协作和问题引导的标题生成，显著提升了视频问答的准确性和上下文理解。

Abstract: Video Question Answering (VQA) inherently relies on multimodal reasoning,
integrating visual, temporal, and linguistic cues to achieve a deeper
understanding of video content. However, many existing methods rely on feeding
frame-level captions into a single model, making it difficult to adequately
capture temporal and interactive contexts. To address this limitation, we
introduce VideoMultiAgents, a framework that integrates specialized agents for
vision, scene graph analysis, and text processing. It enhances video
understanding leveraging complementary multimodal reasoning from independently
operating agents. Our approach is also supplemented with a question-guided
caption generation, which produces captions that highlight objects, actions,
and temporal transitions directly relevant to a given query, thus improving the
answer accuracy. Experimental results demonstrate that our method achieves
state-of-the-art performance on Intent-QA (79.0%, +6.2% over previous SOTA),
EgoSchema subset (75.4%, +3.4%), and NExT-QA (79.6%, +0.4%).

</details>

### [195] [Long-Distance Field Demonstration of Imaging-Free Drone Identification in Intracity Environments](https://arxiv.org/abs/2504.20097)
*Junran Guo,Tonglin Mu,Keyuan Li,Jianing Li,Ziyang Luo,Ye Chen,Xiaodong Fan,Jinquan Huang,Minjie Liu,Jinbei Zhang,Ruoyang Qi,Naiting Gu,Shihai Sun*

Main category: cs.CV

TLDR: 论文提出了一种结合残差神经网络（ResNet）与D²SP²-LiDAR的新方法，显著提升了小型目标（如无人机）的长距离检测能力，检测范围扩展到5公里，并实现了高精度的姿态和类型识别。


<details>
  <summary>Details</summary>
Motivation: 小型目标（如无人机）的长距离检测在安全、监控、环境监测和自主系统中具有重要意义，但传统高分辨率成像方法受限于范围、功耗和成本。

Method: 通过将ResNet与D²SP²-LiDAR结合，并改进观测模型，实现了成像无关的长距离检测。

Result: 实验表明，该方法在5公里范围内实现了94.93%的姿态识别准确率和97.99%的类型分类准确率，优于传统成像系统。

Conclusion: 成像无关方法在现实场景中具有强大的长距离小型目标检测潜力。

Abstract: Detecting small objects, such as drones, over long distances presents a
significant challenge with broad implications for security, surveillance,
environmental monitoring, and autonomous systems. Traditional imaging-based
methods rely on high-resolution image acquisition, but are often constrained by
range, power consumption, and cost. In contrast, data-driven
single-photon-single-pixel light detection and ranging
(\text{D\textsuperscript{2}SP\textsuperscript{2}-LiDAR}) provides an
imaging-free alternative, directly enabling target identification while
reducing system complexity and cost. However, its detection range has been
limited to a few hundred meters. Here, we introduce a novel integration of
residual neural networks (ResNet) with
\text{D\textsuperscript{2}SP\textsuperscript{2}-LiDAR}, incorporating a refined
observation model to extend the detection range to 5~\si{\kilo\meter} in an
intracity environment while enabling high-accuracy identification of drone
poses and types. Experimental results demonstrate that our approach not only
outperforms conventional imaging-based recognition systems, but also achieves
94.93\% pose identification accuracy and 97.99\% type classification accuracy,
even under weak signal conditions with long distances and low signal-to-noise
ratios (SNRs). These findings highlight the potential of imaging-free methods
for robust long-range detection of small targets in real-world scenarios.

</details>

### [196] [An on-production high-resolution longitudinal neonatal fingerprint database in Brazil](https://arxiv.org/abs/2504.20104)
*Luiz F. P. Southier,Marcelo Filipak,Luiz A. Zanlorensi,Ildefonso Wasilevski,Fabio Favarim,Jefferson T. Oliva,Marcelo Teixeira,Dalcimar Casanova*

Main category: cs.CV

TLDR: 研究旨在建立新生儿指纹数据库，以支持机器学习模型开发，解决新生儿生物识别系统因生理变化导致的挑战。


<details>
  <summary>Details</summary>
Motivation: 新生儿期对生存至关重要，需准确识别以支持疫苗接种、HIV治疗等干预措施。现有生物识别方法因生理变化（如指纹生长）难以适应新生儿需求。

Method: 设计并开发高质量新生儿指纹数据库，采集多个早期生命阶段的指纹数据，用于训练和评估机器学习模型。

Result: 预期数据库将支持开发更准确的深度学习模型，优于传统的基于缩放的方法。

Conclusion: 研究为开发适应新生儿独特发育轨迹的生物识别系统奠定基础。

Abstract: The neonatal period is critical for survival, requiring accurate and early
identification to enable timely interventions such as vaccinations, HIV
treatment, and nutrition programs. Biometric solutions offer potential for
child protection by helping to prevent baby swaps, locate missing children, and
support national identity systems. However, developing effective biometric
identification systems for newborns remains a major challenge due to the
physiological variability caused by finger growth, weight changes, and skin
texture alterations during early development. Current literature has attempted
to address these issues by applying scaling factors to emulate growth-induced
distortions in minutiae maps, but such approaches fail to capture the complex
and non-linear growth patterns of infants. A key barrier to progress in this
domain is the lack of comprehensive, longitudinal biometric datasets capturing
the evolution of neonatal fingerprints over time. This study addresses this gap
by focusing on designing and developing a high-quality biometric database of
neonatal fingerprints, acquired at multiple early life stages. The dataset is
intended to support the training and evaluation of machine learning models
aimed at emulating the effects of growth on biometric features. We hypothesize
that such a dataset will enable the development of more robust and accurate
Deep Learning-based models, capable of predicting changes in the minutiae map
with higher fidelity than conventional scaling-based methods. Ultimately, this
effort lays the groundwork for more reliable biometric identification systems
tailored to the unique developmental trajectory of newborns.

</details>

### [197] [Forging and Removing Latent-Noise Diffusion Watermarks Using a Single Image](https://arxiv.org/abs/2504.20111)
*Anubhav Jain,Yuya Kobayashi,Naoki Murata,Yuhta Takida,Takashi Shibuya,Yuki Mitsufuji,Niv Cohen,Nasir Memon,Julian Togelius*

Main category: cs.CV

TLDR: 本文提出了一种针对扩散模型水印的黑盒对抗攻击方法，仅需一个水印示例即可伪造或移除水印，揭示了现有水印技术的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 水印技术对保护知识产权至关重要，但现有方法在扩散模型中存在漏洞，需要研究其安全性。

Method: 通过观察图像与初始噪声的多对一映射关系，提出一种对抗攻击方法，通过扰动图像进入或退出水印区域。

Result: 实验表明，该方法能有效攻击多种水印方案（Tree-Ring、RingID等），并暴露其脆弱性。

Conclusion: 现有水印技术存在漏洞，需进一步研究改进以提高安全性。

Abstract: Watermarking techniques are vital for protecting intellectual property and
preventing fraudulent use of media. Most previous watermarking schemes designed
for diffusion models embed a secret key in the initial noise. The resulting
pattern is often considered hard to remove and forge into unrelated images. In
this paper, we propose a black-box adversarial attack without presuming access
to the diffusion model weights. Our attack uses only a single watermarked
example and is based on a simple observation: there is a many-to-one mapping
between images and initial noises. There are regions in the clean image latent
space pertaining to each watermark that get mapped to the same initial noise
when inverted. Based on this intuition, we propose an adversarial attack to
forge the watermark by introducing perturbations to the images such that we can
enter the region of watermarked images. We show that we can also apply a
similar approach for watermark removal by learning perturbations to exit this
region. We report results on multiple watermarking schemes (Tree-Ring, RingID,
WIND, and Gaussian Shading) across two diffusion models (SDv1.4 and SDv2.0).
Our results demonstrate the effectiveness of the attack and expose
vulnerabilities in the watermarking methods, motivating future research on
improving them.

</details>

### [198] [A Transformer-based Multimodal Fusion Model for Efficient Crowd Counting Using Visual and Wireless Signals](https://arxiv.org/abs/2504.20178)
*Zhe Cui,Yuli Li,Le-Nam Tran*

Main category: cs.CV

TLDR: TransFusion是一种基于多模态融合的人群计数模型，结合了CSI和图像数据，利用Transformer和CNN的优势，实现了高精度和高效的人群计数。


<details>
  <summary>Details</summary>
Motivation: 当前的人群计数模型通常依赖单模态输入（如图像或无线信号数据），导致信息丢失和性能不佳。

Method: 提出TransFusion模型，通过Transformer网络融合CSI和图像数据，并结合CNN提取局部细节特征。

Result: 实验表明，TransFusion在保持高效的同时，实现了高精度和低计数误差。

Conclusion: TransFusion通过多模态融合和混合架构设计，显著提升了人群计数的性能。

Abstract: Current crowd-counting models often rely on single-modal inputs, such as
visual images or wireless signal data, which can result in significant
information loss and suboptimal recognition performance. To address these
shortcomings, we propose TransFusion, a novel multimodal fusion-based
crowd-counting model that integrates Channel State Information (CSI) with image
data. By leveraging the powerful capabilities of Transformer networks,
TransFusion effectively combines these two distinct data modalities, enabling
the capture of comprehensive global contextual information that is critical for
accurate crowd estimation. However, while transformers are well capable of
capturing global features, they potentially fail to identify finer-grained,
local details essential for precise crowd counting. To mitigate this, we
incorporate Convolutional Neural Networks (CNNs) into the model architecture,
enhancing its ability to extract detailed local features that complement the
global context provided by the Transformer. Extensive experimental evaluations
demonstrate that TransFusion achieves high accuracy with minimal counting
errors while maintaining superior efficiency.

</details>

### [199] [Integration Flow Models](https://arxiv.org/abs/2504.20179)
*Jingjing Wang,Dan Zhang,Joshua Luo,Yin Yang,Feng Luo*

Main category: cs.CV

TLDR: 本文提出了Integration Flow，一种直接学习ODE轨迹路径积分的方法，避免了数值求解ODE的离散化误差，并通过引入目标状态作为锚点提升稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: ODE生成模型存在数值求解器的离散化误差或训练不稳定性问题，限制了样本质量。

Method: 提出Integration Flow，直接学习ODE轨迹路径积分，并显式引入目标状态作为锚点。

Result: 在CIFAR10和ImageNet上，Integration Flow显著提升了现有ODE模型的性能，如一阶生成FID指标。

Conclusion: Integration Flow通过统一结构和理论证明，提升了ODE生成模型的稳定性和准确性，适用于多种现有模型。

Abstract: Ordinary differential equation (ODE) based generative models have emerged as
a powerful approach for producing high-quality samples in many applications.
However, the ODE-based methods either suffer the discretization error of
numerical solvers of ODE, which restricts the quality of samples when only a
few NFEs are used, or struggle with training instability. In this paper, we
proposed Integration Flow, which directly learns the integral of ODE-based
trajectory paths without solving the ODE functions. Moreover, Integration Flow
explicitly incorporates the target state $\mathbf{x}_0$ as the anchor state in
guiding the reverse-time dynamics. We have theoretically proven this can
contribute to both stability and accuracy. To the best of our knowledge,
Integration Flow is the first model with a unified structure to estimate
ODE-based generative models and the first to show the exact straightness of
1-Rectified Flow without reflow. Through theoretical analysis and empirical
evaluations, we show that Integration Flows achieve improved performance when
it is applied to existing ODE-based models, such as diffusion models, Rectified
Flows, and PFGM++. Specifically, Integration Flow achieves one-step generation
on CIFAR10 with FIDs of 2.86 for the Variance Exploding (VE) diffusion model,
3.36 for rectified flow without reflow, and 2.91 for PFGM++; and on ImageNet
with FIDs of 4.09 for VE diffusion model, 4.35 for rectified flow without
reflow and 4.15 for PFGM++.

</details>

### [200] [Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains](https://arxiv.org/abs/2504.20199)
*Juntian Zhang,Chuanqi cheng,Yuhan Liu,Wei Liu,Jian Luan,Rui Yan*

Main category: cs.CV

TLDR: 提出了一种名为Focus-Centric Visual Chain的新范式，通过Focus-Centric Data Synthesis方法合成高质量数据，构建了VISC-150K数据集，显著提升了视觉语言模型在多图像任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多图像输入复杂，现有视觉语言模型在处理多图像任务时性能下降明显，需要提升模型对复杂视觉特征的感知和理解能力。

Method: 提出Focus-Centric Visual Chain范式，并通过Focus-Centric Data Synthesis方法合成高质量数据，构建VISC-150K数据集。

Result: 在七个多图像基准测试中，平均性能提升3.16%和2.24%，且不影响通用视觉语言能力。

Conclusion: 该研究为构建更鲁棒和强大的视觉语言系统迈出了重要一步。

Abstract: Vision-language models (VLMs) achieve remarkable success in single-image
tasks. However, real-world scenarios often involve intricate multi-image
inputs, leading to a notable performance decline as models struggle to
disentangle critical information scattered across complex visual features. In
this work, we propose Focus-Centric Visual Chain, a novel paradigm that
enhances VLMs'perception, comprehension, and reasoning abilities in multi-image
scenarios. To facilitate this paradigm, we propose Focus-Centric Data
Synthesis, a scalable bottom-up approach for synthesizing high-quality data
with elaborate reasoning paths. Through this approach, We construct VISC-150K,
a large-scale dataset with reasoning data in the form of Focus-Centric Visual
Chain, specifically designed for multi-image tasks. Experimental results on
seven multi-image benchmarks demonstrate that our method achieves average
performance gains of 3.16% and 2.24% across two distinct model architectures,
without compromising the general vision-language capabilities. our study
represents a significant step toward more robust and capable vision-language
systems that can handle complex visual scenarios.

</details>

### [201] [Remote Sensing Imagery for Flood Detection: Exploration of Augmentation Strategies](https://arxiv.org/abs/2504.20203)
*Vladyslav Polushko,Damjan Hatic,Ronald Rösch,Thomas März,Markus Rauhut,Andreas Weinmann*

Main category: cs.CV

TLDR: 论文探讨了利用不同数据增强策略优化深度学习网络在RGB图像中检测河流洪水的效果。


<details>
  <summary>Details</summary>
Motivation: 洪水是全球性问题，快速有效响应需要准确及时的受灾区域信息，而遥感图像的有效利用依赖于特定检测方法。

Method: 使用BlessemFlood21数据集，研究从基础到复杂（如光学畸变）的不同数据增强策略。

Result: 旨在通过识别有效策略优化当前最先进的深度学习分割网络的训练过程。

Conclusion: 通过数据增强策略的优化，提升洪水检测的准确性和效率。

Abstract: Floods cause serious problems around the world. Responding quickly and
effectively requires accurate and timely information about the affected areas.
The effective use of Remote Sensing images for accurate flood detection
requires specific detection methods. Typically, Deep Neural Networks are
employed, which are trained on specific datasets. For the purpose of river
flood detection in RGB imagery, we use the BlessemFlood21 dataset. We here
explore the use of different augmentation strategies, ranging from basic
approaches to more complex techniques, including optical distortion. By
identifying effective strategies, we aim to refine the training process of
state-of-the-art Deep Learning segmentation networks.

</details>

### [202] [FreBIS: Frequency-Based Stratification for Neural Implicit Surface Representations](https://arxiv.org/abs/2504.20222)
*Naoko Sawada,Pedro Miraldo,Suhas Lohit,Tim K. Marks,Moitreya Chatterjee*

Main category: cs.CV

TLDR: FreBIS提出了一种基于频率分层的神经隐式表面表示方法，通过多编码器分别处理不同频率的表面信息，显著提升了复杂场景的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有神经隐式表面表示方法难以处理复杂场景，因其依赖单一编码器同时捕捉所有频率的表面信息。

Method: FreBIS将场景按表面频率分层，每层由专用编码器处理，并通过冗余感知权重模块促进特征互补性。

Result: 在BlendedMVS数据集上，FreBIS显著提升了3D表面重建质量和渲染保真度。

Conclusion: FreBIS通过频率分层和互补编码器设计，有效解决了复杂场景的表面重建问题。

Abstract: Neural implicit surface representation techniques are in high demand for
advancing technologies in augmented reality/virtual reality, digital twins,
autonomous navigation, and many other fields. With their ability to model
object surfaces in a scene as a continuous function, such techniques have made
remarkable strides recently, especially over classical 3D surface
reconstruction methods, such as those that use voxels or point clouds. However,
these methods struggle with scenes that have varied and complex surfaces
principally because they model any given scene with a single encoder network
that is tasked to capture all of low through high-surface frequency information
in the scene simultaneously. In this work, we propose a novel, neural implicit
surface representation approach called FreBIS to overcome this challenge.
FreBIS works by stratifying the scene based on the frequency of surfaces into
multiple frequency levels, with each level (or a group of levels) encoded by a
dedicated encoder. Moreover, FreBIS encourages these encoders to capture
complementary information by promoting mutual dissimilarity of the encoded
features via a novel, redundancy-aware weighting module. Empirical evaluations
on the challenging BlendedMVS dataset indicate that replacing the standard
encoder in an off-the-shelf neural surface reconstruction method with our
frequency-stratified encoders yields significant improvements. These
enhancements are evident both in the quality of the reconstructed 3D surfaces
and in the fidelity of their renderings from any viewpoint.

</details>

### [203] [Improving trajectory continuity in drone-based crowd monitoring using a set of minimal-cost techniques and deep discriminative correlation filters](https://arxiv.org/abs/2504.20234)
*Bartosz Ptak,Marek Kraft*

Main category: cs.CV

TLDR: 提出了一种基于点的在线跟踪算法，改进了无人机人群监控中的轨迹连续性和计数可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统检测-分配跟踪方法存在误报、漏报和身份切换问题，导致计数准确性下降。

Method: 基于SORT框架，采用点距离度量替代边界框分配，结合相机运动补偿、高度感知分配和分类轨迹验证，并集成DDCF以提高计算效率。

Result: 在DroneCrowd和UP-COUNT-TRACK数据集上，计数误差分别降至23%和15%，身份切换显著减少。

Conclusion: 该方法显著提升了跟踪性能，优于基线在线跟踪器和离线贪婪优化方法。

Abstract: Drone-based crowd monitoring is the key technology for applications in
surveillance, public safety, and event management. However, maintaining
tracking continuity and consistency remains a significant challenge.
Traditional detection-assignment tracking methods struggle with false
positives, false negatives, and frequent identity switches, leading to degraded
counting accuracy and making in-depth analysis impossible. This paper
introduces a point-oriented online tracking algorithm that improves trajectory
continuity and counting reliability in drone-based crowd monitoring. Our method
builds on the Simple Online and Real-time Tracking (SORT) framework, replacing
the original bounding-box assignment with a point-distance metric. The
algorithm is enhanced with three cost-effective techniques: camera motion
compensation, altitude-aware assignment, and classification-based trajectory
validation. Further, Deep Discriminative Correlation Filters (DDCF) that re-use
spatial feature maps from localisation algorithms for increased computational
efficiency through neural network resource sharing are integrated to refine
object tracking by reducing noise and handling missed detections. The proposed
method is evaluated on the DroneCrowd and newly shared UP-COUNT-TRACK datasets,
demonstrating substantial improvements in tracking metrics, reducing counting
errors to 23% and 15%, respectively. The results also indicate a significant
reduction of identity switches while maintaining high tracking accuracy,
outperforming baseline online trackers and even an offline greedy optimisation
method.

</details>

### [204] [Physics-Informed Diffusion Models for SAR Ship Wake Generation from Text Prompts](https://arxiv.org/abs/2504.20241)
*Kamirul Kamirul,Odysseas Pappas,Alin Achim*

Main category: cs.CV

TLDR: 利用扩散模型生成SAR图像中的船舶尾迹，比物理模拟更快且支持端到端学习。


<details>
  <summary>Details</summary>
Motivation: 监督学习因标注数据稀缺而受限，物理模拟速度慢且限制端到端学习。

Method: 使用扩散模型，基于物理模拟生成的数据和文本提示进行训练。

Result: 模型生成真实的Kelvin尾迹，推理速度显著快于物理模拟。

Conclusion: 扩散模型为快速可控的尾迹图像生成提供了新可能，支持端到端海事SAR分析。

Abstract: Detecting ship presence via wake signatures in SAR imagery is attracting
considerable research interest, but limited annotated data availability poses
significant challenges for supervised learning. Physics-based simulations are
commonly used to address this data scarcity, although they are slow and
constrain end-to-end learning. In this work, we explore a new direction for
more efficient and end-to-end SAR ship wake simulation using a diffusion model
trained on data generated by a physics-based simulator. The training dataset is
built by pairing images produced by the simulator with text prompts derived
from simulation parameters. Experimental result show that the model generates
realistic Kelvin wake patterns and achieves significantly faster inference than
the physics-based simulator. These results highlight the potential of diffusion
models for fast and controllable wake image generation, opening new
possibilities for end-to-end downstream tasks in maritime SAR analysis.

</details>

### [205] [Image Interpolation with Score-based Riemannian Metrics of Diffusion Models](https://arxiv.org/abs/2504.20288)
*Shinnosuke Saito,Takashi Matsubara*

Main category: cs.CV

TLDR: 提出了一个将预训练扩散模型的数据空间视为黎曼流形的新框架，通过分数函数定义度量，提升了图像插值的真实性和提示忠实度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型缺乏显式利用数据流形的方法，而其他生成模型（如潜在空间模型）具备这一能力。

Method: 将扩散模型的数据空间视为黎曼流形，基于分数函数定义度量，实现几何感知的图像插值。

Result: 在MNIST和Stable Diffusion上的实验表明，该方法生成的插值图像更真实、噪声更少、更忠实于提示。

Conclusion: 该框架为改进内容生成和编辑提供了潜力。

Abstract: Diffusion models excel in content generation by implicitly learning the data
manifold, yet they lack a practical method to leverage this manifold - unlike
other deep generative models equipped with latent spaces. This paper introduces
a novel framework that treats the data space of pre-trained diffusion models as
a Riemannian manifold, with a metric derived from the score function.
Experiments with MNIST and Stable Diffusion show that this geometry-aware
approach yields image interpolations that are more realistic, less noisy, and
more faithful to prompts than existing methods, demonstrating its potential for
improved content generation and editing.

</details>

### [206] [DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes](https://arxiv.org/abs/2504.20303)
*Junlin Guo,James R. Zimmer-Dauphinee,Jordan M. Nieusma,Siqi Lu,Quan Liu,Ruining Deng,Can Cui,Jialin Yue,Yizhe Lin,Tianyuan Yao,Juming Xiong,Junchao Zhu,Chongyu Qu,Yuechen Yang,Mitchell Wilkes,Xiao Wang,Parker VanValkenburgh,Steven A. Wernke,Yuankai Huo*

Main category: cs.CV

TLDR: DeepAndes是一种基于Transformer的视觉基础模型，专为安第斯考古学设计，通过自监督学习优化8波段多光谱卫星图像分析，显著提升了考古遥感任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统监督深度学习方法在标注细粒度考古特征时面临挑战，且现有视觉基础模型多针对RGB图像而非多光谱数据。

Method: 采用定制化的DINOv2自监督学习算法，训练于300万张多光谱卫星图像，支持图像分类、检索和语义分割任务。

Result: 在少样本学习中，DeepAndes的F1分数、平均精度和Dice分数显著优于从头训练或小数据集预训练的模型。

Conclusion: 大规模自监督预训练在考古遥感中具有显著效果，DeepAndes为安第斯地区提供了首个专用基础模型。

Abstract: By mapping sites at large scales using remotely sensed data, archaeologists
can generate unique insights into long-term demographic trends, inter-regional
social networks, and past adaptations to climate change. Remote sensing surveys
complement field-based approaches, and their reach can be especially great when
combined with deep learning and computer vision techniques. However,
conventional supervised deep learning methods face challenges in annotating
fine-grained archaeological features at scale. While recent vision foundation
models have shown remarkable success in learning large-scale remote sensing
data with minimal annotations, most off-the-shelf solutions are designed for
RGB images rather than multi-spectral satellite imagery, such as the 8-band
data used in our study. In this paper, we introduce DeepAndes, a
transformer-based vision foundation model trained on three million
multi-spectral satellite images, specifically tailored for Andean archaeology.
DeepAndes incorporates a customized DINOv2 self-supervised learning algorithm
optimized for 8-band multi-spectral imagery, marking the first foundation model
designed explicitly for the Andes region. We evaluate its image understanding
performance through imbalanced image classification, image instance retrieval,
and pixel-level semantic segmentation tasks. Our experiments show that
DeepAndes achieves superior F1 scores, mean average precision, and Dice scores
in few-shot learning scenarios, significantly outperforming models trained from
scratch or pre-trained on smaller datasets. This underscores the effectiveness
of large-scale self-supervised pre-training in archaeological remote sensing.
Codes will be available on https://github.com/geopacha/DeepAndes.

</details>

### [207] [Dynamic Contextual Attention Network: Transforming Spatial Representations into Adaptive Insights for Endoscopic Polyp Diagnosis](https://arxiv.org/abs/2504.20306)
*Teja Krishna Cherukuri,Nagur Shareef Shaik,Sribhuvan Reddy Yellu,Jun-Won Chung,Dong Hye Ye*

Main category: cs.CV

TLDR: 提出了一种动态上下文注意力网络（DCAN），通过注意力机制提升结肠息肉检测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统内窥镜成像在息肉定位和上下文感知方面存在不足，限制了诊断的可解释性。

Method: DCAN将空间表示转化为自适应上下文洞察，通过注意力机制增强关键息肉区域的关注，无需显式定位模块。

Result: DCAN提高了分类过程的决策可解释性和整体诊断性能。

Conclusion: DCAN的进步可能带来更可靠的结肠癌检测，改善患者预后。

Abstract: Colorectal polyps are key indicators for early detection of colorectal
cancer. However, traditional endoscopic imaging often struggles with accurate
polyp localization and lacks comprehensive contextual awareness, which can
limit the explainability of diagnoses. To address these issues, we propose the
Dynamic Contextual Attention Network (DCAN). This novel approach transforms
spatial representations into adaptive contextual insights, using an attention
mechanism that enhances focus on critical polyp regions without explicit
localization modules. By integrating contextual awareness into the
classification process, DCAN improves decision interpretability and overall
diagnostic performance. This advancement in imaging could lead to more reliable
colorectal cancer detection, enabling better patient outcomes.

</details>

### [208] [Fine Grain Classification: Connecting Meta using Cross-Contrastive pre-training](https://arxiv.org/abs/2504.20322)
*Sumit Mamtani,Yash Thesia*

Main category: cs.CV

TLDR: 论文提出了一种利用元信息辅助细粒度视觉分类的统一框架，通过跨对比预训练联合学习视觉和元信息，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉分类仅依赖外观信息难以准确区分类别，因此需要利用元信息辅助识别。

Method: 采用三个编码器分别处理图像、文本和元信息，通过跨对比预训练对齐嵌入表示，随后微调图像和元信息编码器进行分类任务。

Result: 在NABirds数据集上，框架利用元信息将性能提升7.83%，达到84.44%的准确率，优于现有方法。

Conclusion: 提出的框架通过有效利用元信息，显著提升了细粒度视觉分类的性能。

Abstract: Fine-grained visual classification aims to recognize objects belonging to
multiple subordinate categories within a super-category. However, this remains
a challenging problem, as appearance information alone is often insufficient to
accurately differentiate between fine-grained visual categories. To address
this, we propose a novel and unified framework that leverages meta-information
to assist fine-grained identification. We tackle the joint learning of visual
and meta-information through cross-contrastive pre-training. In the first
stage, we employ three encoders for images, text, and meta-information,
aligning their projected embeddings to achieve better representations. We then
fine-tune the image and meta-information encoders for the classification task.
Experiments on the NABirds dataset demonstrate that our framework effectively
utilizes meta-information to enhance fine-grained recognition performance. With
the addition of meta-information, our framework surpasses the current baseline
on NABirds by 7.83%. Furthermore, it achieves an accuracy of 84.44% on the
NABirds dataset, outperforming many existing state-of-the-art approaches that
utilize meta-information.

</details>

### [209] [MicarVLMoE: A Modern Gated Cross-Aligned Vision-Language Mixture of Experts Model for Medical Image Captioning and Report Generation](https://arxiv.org/abs/2504.20343)
*Amaan Izhar,Nurul Japar,Norisma Idris,Ting Dang*

Main category: cs.CV

TLDR: MicarVLMoE是一种视觉语言混合专家模型，通过多尺度视觉编码器和多模态对齐模块，解决了医学图像报告中细粒度特征提取和多模态对齐的挑战，并在多种医学图像类型上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在细粒度特征提取、多模态对齐和跨多种图像类型的泛化能力上表现不佳，主要依赖普通Transformer且仅关注胸部X光片。

Method: 提出MicarVLMoE模型，包含多尺度视觉编码器（MSVE）、多头双分支潜在注意力模块（MDLA）和调制混合专家解码器（MoE），用于自适应专家专业化。

Result: 在COVCTR、MMR、PGROSS和ROCO数据集上实现了最先进的性能，实验验证了临床准确性、跨模态对齐和模型可解释性的提升。

Conclusion: MicarVLMoE通过创新的架构设计，显著提升了医学图像报告的性能和泛化能力，代码已开源。

Abstract: Medical image reporting (MIR) aims to generate structured clinical
descriptions from radiological images. Existing methods struggle with
fine-grained feature extraction, multimodal alignment, and generalization
across diverse imaging types, often relying on vanilla transformers and
focusing primarily on chest X-rays. We propose MicarVLMoE, a vision-language
mixture-of-experts model with gated cross-aligned fusion, designed to address
these limitations. Our architecture includes: (i) a multiscale vision encoder
(MSVE) for capturing anatomical details at varying resolutions, (ii) a
multihead dual-branch latent attention (MDLA) module for vision-language
alignment through latent bottleneck representations, and (iii) a modulated
mixture-of-experts (MoE) decoder for adaptive expert specialization. We extend
MIR to CT scans, retinal imaging, MRI scans, and gross pathology images,
reporting state-of-the-art results on COVCTR, MMR, PGROSS, and ROCO datasets.
Extensive experiments and ablations confirm improved clinical accuracy,
cross-modal alignment, and model interpretability. Code is available at
https://github.com/AI-14/micar-vl-moe.

</details>

### [210] [TTTFusion: A Test-Time Training-Based Strategy for Multimodal Medical Image Fusion in Surgical Robots](https://arxiv.org/abs/2504.20362)
*Qinhua Xie,Hao Tang*

Main category: cs.CV

TLDR: TTTFusion是一种基于测试时训练（TTT）的图像融合策略，通过动态调整模型参数提升多模态医学图像的融合质量。


<details>
  <summary>Details</summary>
Motivation: 提高手术机器人处理多模态医学图像的能力，解决传统融合方法在实时性、细粒度特征提取和边缘保留方面的不足。

Method: 采用测试时训练（TTT）策略，在推理阶段动态调整模型参数，优化输入图像数据的融合效果。

Result: 实验表明，TTTFusion在多模态图像融合质量上显著优于传统方法，尤其在细粒度特征提取和边缘保留方面表现突出。

Conclusion: TTTFusion不仅提升了图像融合精度，还为手术机器人的实时图像处理提供了新颖的技术方案。

Abstract: With the increasing use of surgical robots in clinical practice, enhancing
their ability to process multimodal medical images has become a key research
challenge. Although traditional medical image fusion methods have made progress
in improving fusion accuracy, they still face significant challenges in
real-time performance, fine-grained feature extraction, and edge
preservation.In this paper, we introduce TTTFusion, a Test-Time Training
(TTT)-based image fusion strategy that dynamically adjusts model parameters
during inference to efficiently fuse multimodal medical images. By adapting the
model during the test phase, our method optimizes the parameters based on the
input image data, leading to improved accuracy and better detail preservation
in the fusion results.Experimental results demonstrate that TTTFusion
significantly enhances the fusion quality of multimodal images compared to
traditional fusion methods, particularly in fine-grained feature extraction and
edge preservation. This approach not only improves image fusion accuracy but
also offers a novel technical solution for real-time image processing in
surgical robots.

</details>

### [211] [Inception: Jailbreak the Memory Mechanism of Text-to-Image Generation Systems](https://arxiv.org/abs/2504.20376)
*Shiqian Zhao,Jiayang Liu,Yiming Li,Runyi Hu,Xiaojun Jia,Wenshu Fan,Xinfeng Li,Jie Zhang,Wei Dong,Tianwei Zhang,Luu Anh Tuan*

Main category: cs.CV

TLDR: 论文揭示了在线文本到图像生成系统中的记忆机制加剧了越狱攻击的风险，并提出了一种名为Inception的多轮越狱攻击方法。


<details>
  <summary>Details</summary>
Motivation: 尽管记忆机制在减轻标记化负担和捕获多轮交互中的关键信息方面具有实用性，但其安全性分析滞后，存在被滥用的风险。

Method: 提出Inception攻击方法，通过将恶意提示分段输入系统，并利用递归策略确保最小不安全词的可分性。

Result: 实验表明，Inception的攻击成功率比现有方法高出14%。

Conclusion: 记忆机制的安全性问题需要引起重视，Inception方法展示了其潜在风险。

Abstract: Currently, the memory mechanism has been widely and successfully exploited in
online text-to-image (T2I) generation systems ($e.g.$, DALL$\cdot$E 3) for
alleviating the growing tokenization burden and capturing key information in
multi-turn interactions. Despite its practicality, its security analyses have
fallen far behind. In this paper, we reveal that this mechanism exacerbates the
risk of jailbreak attacks. Different from previous attacks that fuse the unsafe
target prompt into one ultimate adversarial prompt, which can be easily
detected or may generate non-unsafe images due to under- or over-optimization,
we propose Inception, the first multi-turn jailbreak attack against the memory
mechanism in real-world text-to-image generation systems. Inception embeds the
malice at the inception of the chat session turn by turn, leveraging the
mechanism that T2I generation systems retrieve key information in their memory.
Specifically, Inception mainly consists of two modules. It first segments the
unsafe prompt into chunks, which are subsequently fed to the system in multiple
turns, serving as pseudo-gradients for directive optimization. Specifically, we
develop a series of segmentation policies that ensure the images generated are
semantically consistent with the target prompt. Secondly, after segmentation,
to overcome the challenge of the inseparability of minimum unsafe words, we
propose recursion, a strategy that makes minimum unsafe words subdivisible.
Collectively, segmentation and recursion ensure that all the request prompts
are benign but can lead to malicious outcomes. We conduct experiments on the
real-world text-to-image generation system ($i.e.$, DALL$\cdot$E 3) to validate
the effectiveness of Inception. The results indicate that Inception surpasses
the state-of-the-art by a 14\% margin in attack success rate.

</details>

### [212] [Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface Reconstruction from Sparse Views](https://arxiv.org/abs/2504.20378)
*Jiang Wu,Rui Li,Yu Zhu,Rong Guo,Jinqiu Sun,Yanning Zhang*

Main category: cs.CV

TLDR: 提出了一种基于稀疏输入视图的高斯泼溅表面重建方法Sparse2DGS，解决了传统方法在稀疏视图下的几何优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖密集视图，难以处理稀疏Structure-from-Motion点的初始化问题，而基于学习的MVS直接结合高斯泼溅效果不佳。

Method: 提出MVS初始化的高斯泼溅流程Sparse2DGS，引入几何优先增强方案，在病态条件下实现直接且鲁棒的几何学习。

Result: Sparse2DGS显著优于现有方法，且比基于NeRF的微调方法快2倍。

Conclusion: Sparse2DGS在稀疏视图下实现了完整且准确的表面重建。

Abstract: We present a Gaussian Splatting method for surface reconstruction using
sparse input views. Previous methods relying on dense views struggle with
extremely sparse Structure-from-Motion points for initialization. While
learning-based Multi-view Stereo (MVS) provides dense 3D points, directly
combining it with Gaussian Splatting leads to suboptimal results due to the
ill-posed nature of sparse-view geometric optimization. We propose Sparse2DGS,
an MVS-initialized Gaussian Splatting pipeline for complete and accurate
reconstruction. Our key insight is to incorporate the geometric-prioritized
enhancement schemes, allowing for direct and robust geometric learning under
ill-posed conditions. Sparse2DGS outperforms existing methods by notable
margins while being ${2}\times$ faster than the NeRF-based fine-tuning
approach.

</details>

### [213] [GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting](https://arxiv.org/abs/2504.20379)
*Jongwon Lee,Timothy Bretl*

Main category: cs.CV

TLDR: 提出了一种基于3D高斯散射（3DGS）场景表示的查询图像定位方法，显著降低了推理时间和估计误差。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在定位查询图像时推理时间长和误差大的问题。

Method: 1. 使用3DGS渲染合成RGBD图像；2. 建立查询图像与合成图像的2D-2D对应关系；3. 通过深度图将2D-2D对应提升为2D-3D对应，并求解PnP问题得到最终位姿估计。

Result: 在三个数据集上测试，推理时间从10秒降至0.1秒，位姿误差显著降低，且对初始位姿估计的大误差具有鲁棒性。

Conclusion: 该方法高效且鲁棒，适用于大规模场景的实时定位。

Abstract: In this paper, we present a method for localizing a query image with respect
to a precomputed 3D Gaussian Splatting (3DGS) scene representation. First, the
method uses 3DGS to render a synthetic RGBD image at some initial pose
estimate. Second, it establishes 2D-2D correspondences between the query image
and this synthetic image. Third, it uses the depth map to lift the 2D-2D
correspondences to 2D-3D correspondences and solves a perspective-n-point (PnP)
problem to produce a final pose estimate. Results from evaluation across three
existing datasets with 38 scenes and over 2,700 test images show that our
method significantly reduces both inference time (by over two orders of
magnitude, from more than 10 seconds to as fast as 0.1 seconds) and estimation
error compared to baseline methods that use photometric loss minimization.
Results also show that our method tolerates large errors in the initial pose
estimate of up to 55{\deg} in rotation and 1.1 units in translation (normalized
by scene scale), achieving final pose errors of less than 5{\deg} in rotation
and 0.05 units in translation on 90% of images from the Synthetic NeRF and
Mip-NeRF360 datasets and on 42% of images from the more challenging Tanks and
Temples dataset.

</details>

### [214] [Neural Stereo Video Compression with Hybrid Disparity Compensation](https://arxiv.org/abs/2504.20383)
*Shiyin Jiang,Zhenghao Chen,Minghao Han,Xingyu Zhou,Leheng Zhang,Shuhang Gu*

Main category: cs.CV

TLDR: 提出了一种混合视差补偿（HDC）策略，结合显式和隐式方法优化立体视频压缩，并构建了端到端优化的神经框架，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 立体视频压缩（SVC）中视差补偿是主要策略，但现有方法分为显式水平位移和隐式交叉注意力两种，各有局限。

Method: HDC策略融合显式像素位移和隐式交叉注意力机制，通过相似性图和归一化注意力分数实现特征对齐，并构建了HDC-FER和HDC-EM模块。

Result: 在KITTI 2012、KITTI 2015和Nagoya等基准测试中，性能优于传统和神经SVC方法。

Conclusion: HDC策略和端到端框架显著提升了立体视频压缩的性能，适用于自动驾驶和通用场景。

Abstract: Disparity compensation represents the primary strategy in stereo video
compression (SVC) for exploiting cross-view redundancy. These mechanisms can be
broadly categorized into two types: one that employs explicit horizontal
shifting, and another that utilizes an implicit cross-attention mechanism to
reduce cross-view disparity redundancy. In this work, we propose a hybrid
disparity compensation (HDC) strategy that leverages explicit pixel
displacement as a robust prior feature to simplify optimization and perform
implicit cross-attention mechanisms for subsequent warping operations, thereby
capturing a broader range of disparity information. Specifically, HDC first
computes a similarity map by fusing the horizontally shifted cross-view
features to capture pixel displacement information. This similarity map is then
normalized into an "explicit pixel-wise attention score" to perform the
cross-attention mechanism, implicitly aligning features from one view to
another. Building upon HDC, we introduce a novel end-to-end optimized neural
stereo video compression framework, which integrates HDC-based modules into key
coding operations, including cross-view feature extraction and reconstruction
(HDC-FER) and cross-view entropy modeling (HDC-EM). Extensive experiments on
SVC benchmarks, including KITTI 2012, KITTI 2015, and Nagoya, which cover both
autonomous driving and general scenes, demonstrate that our framework
outperforms both neural and traditional SVC methodologies.

</details>

### [215] [FiLA-Video: Spatio-Temporal Compression for Fine-Grained Long Video Understanding](https://arxiv.org/abs/2504.20384)
*Yanan Guo,Wenhui Dong,Jun Song,Shiding Zhu,Xuan Zhang,Hanqing Yang,Yingbo Wang,Yang Du,Xianing Chen,Bo Zheng*

Main category: cs.CV

TLDR: FiLA-Video提出了一种轻量级动态权重多帧融合策略，通过自适应整合多帧信息并保留关键视频内容，同时降低计算成本，解决了长视频理解中的复杂性和上下文处理限制问题。


<details>
  <summary>Details</summary>
Motivation: 视频数据的复杂性和上下文处理限制阻碍了长视频理解，现有方法要么未能优先处理关键特征，要么引入了计算昂贵的模块。

Method: FiLA-Video采用动态权重多帧融合策略和关键帧选择策略，结合长视频训练数据生成方法，提升模型性能。

Result: 实验表明，FiLA-Video在长视频理解中实现了更高的效率和准确性。

Conclusion: FiLA-Video通过轻量级设计和关键帧选择，显著提升了长视频理解的性能。

Abstract: Recent advancements in video understanding within visual large language
models (VLLMs) have led to notable progress. However, the complexity of video
data and contextual processing limitations still hinder long-video
comprehension. A common approach is video feature compression to reduce token
input to large language models, yet many methods either fail to prioritize
essential features, leading to redundant inter-frame information, or introduce
computationally expensive modules.To address these issues, we propose
FiLA(Fine-grained Vision Language Model)-Video, a novel framework that
leverages a lightweight dynamic-weight multi-frame fusion strategy, which
adaptively integrates multiple frames into a single representation while
preserving key video information and reducing computational costs. To enhance
frame selection for fusion, we introduce a keyframe selection strategy,
effectively identifying informative frames from a larger pool for improved
summarization. Additionally, we present a simple yet effective long-video
training data generation strategy, boosting model performance without extensive
manual annotation. Experimental results demonstrate that FiLA-Video achieves
superior efficiency and accuracy in long-video comprehension compared to
existing methods.

</details>

### [216] [GarmentX: Autoregressive Parametric Representations for High-Fidelity 3D Garment Generation](https://arxiv.org/abs/2504.20409)
*Jingfeng Guo,Jinnan Chen,Weikai Chen,Zhenyu Sun,Lanjiong Li,Baozhu Zhao,Lingting Zhu,Xin Wang,Qi Liu*

Main category: cs.CV

TLDR: GarmentX是一个从单张图像生成多样化、高保真且可穿戴3D服装的新框架，通过参数化表示和自回归模型解决传统方法的问题，并引入大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 传统服装重建方法直接预测2D图案边缘及其连接性，导致自相交和物理上不合理的结构，GarmentX旨在解决这一问题。

Method: 采用结构化可编辑参数化表示与GarmentCode兼容，使用掩码自回归模型预测参数，并构建大规模数据集GarmentX。

Result: 在几何保真度和输入图像对齐方面达到最先进性能，显著优于先前方法。

Conclusion: GarmentX框架和数据集为3D服装生成提供了高效且直观的解决方案，并计划公开数据集。

Abstract: This work presents GarmentX, a novel framework for generating diverse,
high-fidelity, and wearable 3D garments from a single input image. Traditional
garment reconstruction methods directly predict 2D pattern edges and their
connectivity, an overly unconstrained approach that often leads to severe
self-intersections and physically implausible garment structures. In contrast,
GarmentX introduces a structured and editable parametric representation
compatible with GarmentCode, ensuring that the decoded sewing patterns always
form valid, simulation-ready 3D garments while allowing for intuitive
modifications of garment shape and style. To achieve this, we employ a masked
autoregressive model that sequentially predicts garment parameters, leveraging
autoregressive modeling for structured generation while mitigating
inconsistencies in direct pattern prediction. Additionally, we introduce
GarmentX dataset, a large-scale dataset of 378,682 garment parameter-image
pairs, constructed through an automatic data generation pipeline that
synthesizes diverse and high-quality garment images conditioned on parametric
garment representations. Through integrating our method with GarmentX dataset,
we achieve state-of-the-art performance in geometric fidelity and input image
alignment, significantly outperforming prior approaches. We will release
GarmentX dataset upon publication.

</details>

### [217] [Plant Disease Detection through Multimodal Large Language Models and Convolutional Neural Networks](https://arxiv.org/abs/2504.20419)
*Konstantinos I. Roumeliotis,Ranjan Sapkota,Manoj Karkee,Nikolaos D. Tselikas,Dimitrios K. Nasiopoulos*

Main category: cs.CV

TLDR: 研究探讨了结合多模态大语言模型（GPT-4o）与卷积神经网络（CNN）在植物叶片图像疾病分类中的效果，发现微调后的GPT-4o性能略优于ResNet-50，但零样本表现较差。


<details>
  <summary>Details</summary>
Motivation: 解决农业自动化中作物监测和疾病管理的挑战，尤其是通过早期检测系统。

Method: 结合GPT-4o与CNN，使用PlantVillage数据集，评估零样本、少样本和渐进微调场景下的模型性能。

Result: 微调后的GPT-4o在苹果叶片图像上达到98.12%的分类准确率，优于ResNet-50的96.88%，但零样本表现显著较低。

Conclusion: 多模态大语言模型在自动化疾病检测中具有潜力，可提升精准农业系统的可扩展性和智能化。

Abstract: Automation in agriculture plays a vital role in addressing challenges related
to crop monitoring and disease management, particularly through early detection
systems. This study investigates the effectiveness of combining multimodal
Large Language Models (LLMs), specifically GPT-4o, with Convolutional Neural
Networks (CNNs) for automated plant disease classification using leaf imagery.
Leveraging the PlantVillage dataset, we systematically evaluate model
performance across zero-shot, few-shot, and progressive fine-tuning scenarios.
A comparative analysis between GPT-4o and the widely used ResNet-50 model was
conducted across three resolutions (100, 150, and 256 pixels) and two plant
species (apple and corn). Results indicate that fine-tuned GPT-4o models
achieved slightly better performance compared to the performance of ResNet-50,
achieving up to 98.12% classification accuracy on apple leaf images, compared
to 96.88% achieved by ResNet-50, with improved generalization and near-zero
training loss. However, zero-shot performance of GPT-4o was significantly
lower, underscoring the need for minimal training. Additional evaluations on
cross-resolution and cross-plant generalization revealed the models'
adaptability and limitations when applied to new domains. The findings
highlight the promise of integrating multimodal LLMs into automated disease
detection pipelines, enhancing the scalability and intelligence of precision
agriculture systems while reducing the dependence on large, labeled datasets
and high-resolution sensor infrastructure. Large Language Models, Vision
Language Models, LLMs and CNNs, Disease Detection with Vision Language Models,
VLMs

</details>

### [218] [AI Assisted Cervical Cancer Screening for Cytology Samples in Developing Countries](https://arxiv.org/abs/2504.20435)
*Love Panta,Suraj Prasai,Karishma Malla Vaidya,Shyam Shrestha,Suresh Manandhar*

Main category: cs.CV

TLDR: 提出了一种结合低成本显微镜和高效AI算法的宫颈癌筛查方法，通过自动化全玻片分析提高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统液基细胞学检查劳动密集、依赖专家且易出错，需更高效的筛查方法。

Method: 使用电动显微镜捕获图像，通过AI流程（图像拼接、细胞分割和分类）处理，采用轻量级UNet模型和CvT分类模型。

Result: 在SIPaKMeD数据集上准确分类五种细胞类型，优于现有方法。

Conclusion: 该框架显著提升了宫颈癌筛查的准确性和效率。

Abstract: Cervical cancer remains a significant health challenge, with high incidence
and mortality rates, particularly in transitioning countries. Conventional
Liquid-Based Cytology(LBC) is a labor-intensive process, requires expert
pathologists and is highly prone to errors, highlighting the need for more
efficient screening methods. This paper introduces an innovative approach that
integrates low-cost biological microscopes with our simple and efficient AI
algorithms for automated whole-slide analysis. Our system uses a motorized
microscope to capture cytology images, which are then processed through an AI
pipeline involving image stitching, cell segmentation, and classification. We
utilize the lightweight UNet-based model involving human-in-the-loop approach
to train our segmentation model with minimal ROIs. CvT-based classification
model, trained on the SIPaKMeD dataset, accurately categorizes five cell types.
Our framework offers enhanced accuracy and efficiency in cervical cancer
screening compared to various state-of-art methods, as demonstrated by
different evaluation metrics.

</details>

### [219] [PixelHacker: Image Inpainting with Structural and Semantic Consistency](https://arxiv.org/abs/2504.20438)
*Ziyang Xu,Kangsheng Duan,Xiaolei Shen,Zhifeng Ding,Wenyu Liu,Xiaohu Ruan,Xiaoxin Chen,Xinggang Wang*

Main category: cs.CV

TLDR: 论文提出了一种名为PixelHacker的扩散模型，通过潜在类别引导解决图像修复中的结构和语义问题，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂结构和语义修复上表现不佳，导致生成结果存在伪影和不合理性。

Method: 构建大规模图像-掩码数据集，通过潜在类别引导和线性注意力机制注入特征，预训练并微调模型。

Result: PixelHacker在Places2、CelebA-HQ和FFHQ等数据集上全面超越现有方法，表现出色。

Conclusion: 潜在类别引导和扩散模型的结合显著提升了图像修复的性能和一致性。

Abstract: Image inpainting is a fundamental research area between image editing and
image generation. Recent state-of-the-art (SOTA) methods have explored novel
attention mechanisms, lightweight architectures, and context-aware modeling,
demonstrating impressive performance. However, they often struggle with complex
structure (e.g., texture, shape, spatial relations) and semantics (e.g., color
consistency, object restoration, and logical correctness), leading to artifacts
and inappropriate generation. To address this challenge, we design a simple yet
effective inpainting paradigm called latent categories guidance, and further
propose a diffusion-based model named PixelHacker. Specifically, we first
construct a large dataset containing 14 million image-mask pairs by annotating
foreground and background (potential 116 and 21 categories, respectively).
Then, we encode potential foreground and background representations separately
through two fixed-size embeddings, and intermittently inject these features
into the denoising process via linear attention. Finally, by pre-training on
our dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker.
Extensive experiments show that PixelHacker comprehensively outperforms the
SOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits
remarkable consistency in both structure and semantics. Project page at
https://hustvl.github.io/projects/PixelHacker.

</details>

### [220] [LMM4Gen3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs](https://arxiv.org/abs/2504.20466)
*Woo Yi Yang,Jiarui Wang,Sijing Wu,Huiyu Duan,Yuxin Zhu,Liu Yang,Kang Fu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TLDR: 论文提出Gen3DHF基准和LMME3DHF模型，用于评估AI生成的3D人脸的质量和真实性，并在性能上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于人类对3D人脸的主观感知和敏感性，评估AI生成的3D人脸质量和真实性具有挑战性。

Method: 引入Gen3DHF基准数据集，并提出基于多模态大模型的LMME3DHF评估方法。

Result: LMME3DHF在质量评分、失真区域识别和失真类型判断上表现优异，与人类感知一致。

Conclusion: Gen3DHF和LMME3DHF为AI生成3D人脸的质量评估提供了有效工具。

Abstract: The rapid advancement in generative artificial intelligence have enabled the
creation of 3D human faces (HFs) for applications including media production,
virtual reality, security, healthcare, and game development, etc. However,
assessing the quality and realism of these AI-generated 3D human faces remains
a significant challenge due to the subjective nature of human perception and
innate perceptual sensitivity to facial features. To this end, we conduct a
comprehensive study on the quality assessment of AI-generated 3D human faces.
We first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of
AI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS)
collected across two dimensions, i.e., quality and authenticity, 2,000
distortion-aware saliency maps and distortion descriptions. Based on Gen3DHF,
we propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating
3DHF capable of quality and authenticity score prediction, distortion-aware
visual question answering, and distortion-aware saliency prediction.
Experimental results show that LMME3DHF achieves state-of-the-art performance,
surpassing existing methods in both accurately predicting quality scores for
AI-generated 3D human faces and effectively identifying distortion-aware
salient regions and distortion types, while maintaining strong alignment with
human perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be
released upon the publication.

</details>

### [221] [Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception](https://arxiv.org/abs/2504.20468)
*Yuanchen Wu,Lu Zhang,Hang Yao,Junlong Du,Ke Yan,Shouhong Ding,Yunsheng Wu,Xiaoqiang Li*

Main category: cs.CV

TLDR: 论文提出了一种名为“Antidote”的后训练框架，用于减少大型视觉语言模型（LVLMs）中的幻觉问题，特别是针对反事实预设问题（CPQs）。该方法通过合成数据实现自我修正，并在新基准CP-Bench上显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs在多模态任务中表现优异，但其在反事实预设问题中容易产生幻觉，生成不真实的回答。现有研究多关注模型回答生成，而忽视了问题本身的影响。

Method: 提出“Antidote”框架，利用合成数据将事实先验融入问题以实现自我修正，并将缓解过程分解为偏好优化问题。同时构建了CP-Bench基准用于评估。

Result: 在LLaVA系列模型中，Antidote显著提升了CP-Bench性能（50%以上），POPE（1.8-3.3%），以及CHAIR & SHR（30-50%），且无需外部监督。

Conclusion: Antidote有效缓解了LVLMs在反事实预设问题中的幻觉问题，同时避免了灾难性遗忘，为模型自我修正提供了新思路。

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive results across
various cross-modal tasks. However, hallucinations, i.e., the models generating
counterfactual responses, remain a challenge. Though recent studies have
attempted to alleviate object perception hallucinations, they focus on the
models' response generation, and overlooking the task question itself. This
paper discusses the vulnerability of LVLMs in solving counterfactual
presupposition questions (CPQs), where the models are prone to accept the
presuppositions of counterfactual objects and produce severe hallucinatory
responses. To this end, we introduce "Antidote", a unified, synthetic
data-driven post-training framework for mitigating both types of hallucination
above. It leverages synthetic data to incorporate factual priors into questions
to achieve self-correction, and decouple the mitigation process into a
preference optimization problem. Furthermore, we construct "CP-Bench", a novel
benchmark to evaluate LVLMs' ability to correctly handle CPQs and produce
factual responses. Applied to the LLaVA series, Antidote can simultaneously
enhance performance on CP-Bench by over 50%, POPE by 1.8-3.3%, and CHAIR & SHR
by 30-50%, all without relying on external supervision from stronger LVLMs or
human feedback and introducing noticeable catastrophic forgetting issues.

</details>

### [222] [Large-scale visual SLAM for in-the-wild videos](https://arxiv.org/abs/2504.20496)
*Shuo Sun,Torsten Sattler,Malcolm Mielle,Achim J. Lilienthal,Martin Magnusson*

Main category: cs.CV

TLDR: 提出了一种针对非约束视频的鲁棒3D场景重建方法，解决了现有SLAM方法在动态物体、纹理缺失和低视差情况下的局限性。


<details>
  <summary>Details</summary>
Motivation: 简化机器人部署到新环境的过程，但现有方法在非约束视频中表现不佳。

Method: 结合深度视觉里程计、动态物体掩码、单目深度估计和全局优化，提升重建鲁棒性。

Result: 在多种环境中实现了大规模连续的3D模型重建，优于基线方法。

Conclusion: 该方法为非约束视频的3D重建提供了新的基准，显著提升了重建的一致性和准确性。

Abstract: Accurate and robust 3D scene reconstruction from casual, in-the-wild videos
can significantly simplify robot deployment to new environments. However,
reliable camera pose estimation and scene reconstruction from such
unconstrained videos remains an open challenge. Existing visual-only SLAM
methods perform well on benchmark datasets but struggle with real-world footage
which often exhibits uncontrolled motion including rapid rotations and pure
forward movements, textureless regions, and dynamic objects. We analyze the
limitations of current methods and introduce a robust pipeline designed to
improve 3D reconstruction from casual videos. We build upon recent deep visual
odometry methods but increase robustness in several ways. Camera intrinsics are
automatically recovered from the first few frames using structure-from-motion.
Dynamic objects and less-constrained areas are masked with a predictive model.
Additionally, we leverage monocular depth estimates to regularize bundle
adjustment, mitigating errors in low-parallax situations. Finally, we integrate
place recognition and loop closure to reduce long-term drift and refine both
intrinsics and pose estimates through global bundle adjustment. We demonstrate
large-scale contiguous 3D models from several online videos in various
environments. In contrast, baseline methods typically produce locally
inconsistent results at several points, producing separate segments or
distorted maps. In lieu of ground-truth pose data, we evaluate map consistency,
execution time and visual accuracy of re-rendered NeRF models. Our proposed
system establishes a new baseline for visual reconstruction from casual
uncontrolled videos found online, demonstrating more consistent reconstructions
over longer sequences of in-the-wild videos than previously achieved.

</details>

### [223] [Style-Adaptive Detection Transformer for Single-Source Domain Generalized Object Detection](https://arxiv.org/abs/2504.20498)
*Jianhong Han,Yupei Wang,Liang Chen*

Main category: cs.CV

TLDR: SA-DETR是一种基于DETR的检测器，通过动态风格适配和对象感知对比学习，提升单源域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖数据增强和特征对齐，但无法覆盖所有未见域，而DETR在域适应中表现优异，但其在SDG任务中的潜力尚未探索。

Method: 提出SA-DETR，包含域风格适配器和对象感知对比学习模块，实现动态风格适配和域不变特征提取。

Result: 在五种天气场景中，SA-DETR表现出卓越的性能和泛化能力。

Conclusion: SA-DETR通过风格适配和对比学习，显著提升了单源域泛化能力。

Abstract: Single-source Domain Generalization (SDG) in object detection aims to develop
a detector using only data from a source domain that can exhibit strong
generalization capability when applied to unseen target domains. Existing
methods are built upon CNN-based detectors and primarily improve robustness by
employing carefully designed data augmentation strategies integrated with
feature alignment techniques. However, data augmentation methods have inherent
drawbacks; they are only effective when the augmented sample distribution
approximates or covers the unseen scenarios, thus failing to enhance
generalization across all unseen domains. Furthermore, while the recent
Detection Transformer (DETR) has demonstrated superior generalization
capability in domain adaptation tasks due to its efficient global information
extraction, its potential in SDG tasks remains unexplored. To this end, we
introduce a strong DETR-based detector named the Style-Adaptive Detection
Transformer (SA-DETR) for SDG in object detection. Specifically, we present a
domain style adapter that projects the style representation of the unseen
target domain into the training domain, enabling dynamic style adaptation.
Then, we propose an object-aware contrastive learning module to guide the
detector in extracting domain-invariant features through contrastive learning.
By using object-aware gating masks to constrain feature aggregation in both
spatial and semantic dimensions, this module achieves cross-domain contrast of
instance-level features, thereby enhancing generalization. Extensive
experiments demonstrate the superior performance and generalization capability
of SA-DETR across five different weather scenarios. Code is released at
https://github.com/h751410234/SA-DETR.

</details>

### [224] [MambaMoE: Mixture-of-Spectral-Spatial-Experts State Space Model for Hyperspectral Image Classification](https://arxiv.org/abs/2504.20509)
*Yichu Xu,Di Wang,Hongzan Jiao,Lefei Zhang,Liangpei Zhang*

Main category: cs.CV

TLDR: MambaMoE是一种新型的混合专家框架，用于高光谱图像分类，通过自适应光谱-空间建模和不确定性引导学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有Mamba模型在高光谱图像分类中忽视了光谱和空间方向特性，导致性能受限。

Method: 提出MambaMoE框架，包含混合Mamba专家块（MoMEB）和不确定性引导纠正学习（UGCL）策略。

Result: 在多个公开数据集上，MambaMoE在准确性和效率上均达到最先进水平。

Conclusion: MambaMoE显著提升了Mamba模型在高光谱图像分类中的性能，代码将开源。

Abstract: The Mamba model has recently demonstrated strong potential in hyperspectral
image (HSI) classification, owing to its ability to perform context modeling
with linear computational complexity. However, existing Mamba-based methods
usually neglect the spectral and spatial directional characteristics related to
heterogeneous objects in hyperspectral scenes, leading to limited
classification performance. To address these issues, we propose MambaMoE, a
novel spectral-spatial mixture-of-experts framework, representing the first
MoE-based approach in the HSI classification community. Specifically, we design
a Mixture of Mamba Expert Block (MoMEB) that leverages sparse expert activation
to enable adaptive spectral-spatial modeling. Furthermore, we introduce an
uncertainty-guided corrective learning (UGCL) strategy to encourage the model's
attention toward complex regions prone to prediction ambiguity. Extensive
experiments on multiple public HSI benchmarks demonstrate that MambaMoE
achieves state-of-the-art performance in both accuracy and efficiency compared
to existing advanced approaches, especially for Mamba-based methods. Code will
be released.

</details>

### [225] [SteelBlastQC: Shot-blasted Steel Surface Dataset with Interpretable Detection of Surface Defects](https://arxiv.org/abs/2504.20510)
*Irina Ruzavina,Lisa Sophie Theis,Jesse Lemeer,Rutger de Groen,Leo Ebeling,Andrej Hulak,Jouaria Ali,Guangzhi Tang,Rico Mockel*

Main category: cs.CV

TLDR: 该研究提出了一个用于钢表面质量控制的RGB图像数据集，并评估了三种分类方法，其中监督方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 自动化钢表面喷砂质量检查对提高制造效率和一致性至关重要。

Method: 使用1654张标记的RGB图像数据集，评估了CCT、SVM和CAE三种分类方法。

Result: CCT和SVM在测试集上达到95%的分类准确率，CAE作为无监督基线效果较差。

Conclusion: 研究通过公开数据集和代码，支持缺陷检测研究并推动工业自动化检查系统的应用。

Abstract: Automating the quality control of shot-blasted steel surfaces is crucial for
improving manufacturing efficiency and consistency. This study presents a
dataset of 1654 labeled RGB images (512x512) of steel surfaces, classified as
either "ready for paint" or "needs shot-blasting." The dataset captures
real-world surface defects, including discoloration, welding lines, scratches
and corrosion, making it well-suited for training computer vision models.
Additionally, three classification approaches were evaluated: Compact
Convolutional Transformers (CCT), Support Vector Machines (SVM) with ResNet-50
feature extraction, and a Convolutional Autoencoder (CAE). The supervised
methods (CCT and SVM) achieve 95% classification accuracy on the test set, with
CCT leveraging transformer-based attention mechanisms and SVM offering a
computationally efficient alternative. The CAE approach, while less effective,
establishes a baseline for unsupervised quality control. We present
interpretable decision-making by all three neural networks, allowing industry
users to visually pinpoint problematic regions and understand the model's
rationale. By releasing the dataset and baseline codes, this work aims to
support further research in defect detection, advance the development of
interpretable computer vision models for quality control, and encourage the
adoption of automated inspection systems in industrial applications.

</details>

### [226] [Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models](https://arxiv.org/abs/2504.20518)
*Zhongqi Wang,Jie Zhang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TLDR: 本文提出了一种名为动态注意力分析（DAA）的新方法，用于检测文本到图像扩散模型中的后门攻击，通过分析交叉注意力图的动态演化特征，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有后门检测方法主要关注静态特征，而扩散模型具有动态特性，因此需要一种新方法来利用动态特征进行更有效的检测。

Method: 提出DAA-I和DAA-S两种方法，分别通过Frobenius范数量化动态特征和基于图的状态方程建模空间相关性。

Result: 在五种代表性后门攻击场景中，DAA方法平均F1分数为79.49%，AUC为87.67%，显著优于现有方法。

Conclusion: 动态注意力分析（DAA）是一种有效的后门检测方法，为扩散模型的安全性提供了新视角。

Abstract: Recent studies have revealed that text-to-image diffusion models are
vulnerable to backdoor attacks, where attackers implant stealthy textual
triggers to manipulate model outputs. Previous backdoor detection methods
primarily focus on the static features of backdoor samples. However, a vital
property of diffusion models is their inherent dynamism. This study introduces
a novel backdoor detection perspective named Dynamic Attention Analysis (DAA),
showing that these dynamic characteristics serve as better indicators for
backdoor detection. Specifically, by examining the dynamic evolution of
cross-attention maps, we observe that backdoor samples exhibit distinct feature
evolution patterns at the $<$EOS$>$ token compared to benign samples. To
quantify these dynamic anomalies, we first introduce DAA-I, which treats the
tokens' attention maps as spatially independent and measures dynamic feature
using the Frobenius norm. Furthermore, to better capture the interactions
between attention maps and refine the feature, we propose a dynamical
system-based approach, referred to as DAA-S. This model formulates the spatial
correlations among attention maps using a graph-based state equation and we
theoretically analyze the global asymptotic stability of this method. Extensive
experiments across five representative backdoor attack scenarios demonstrate
that our approach significantly surpasses existing detection methods, achieving
an average F1 Score of 79.49% and an AUC of 87.67%. The code is available at
https://github.com/Robin-WZQ/DAA.

</details>

### [227] [Geometry-aware Temporal Aggregation Network for Monocular 3D Lane Detection](https://arxiv.org/abs/2504.20525)
*Huan Zheng,Wencheng Han,Tianyi Yan,Cheng-zhong Xu,Jianbing Shen*

Main category: cs.CV

TLDR: 论文提出了一种基于多帧输入的几何感知时序聚合网络（GTA-Net），用于解决单目3D车道检测中几何信息不准确和车道完整性难以保持的问题。


<details>
  <summary>Details</summary>
Motivation: 当前单目3D车道检测方法存在几何信息预测不准确和车道完整性难以保持的局限性，作者希望通过利用多帧输入提升几何感知和车道完整性。

Method: 提出了GTA-Net，包含时序几何增强模块（TGEM）和时序实例感知查询生成（TIQG），分别用于增强几何一致性和探索实例信息。

Result: 实验表明，GTA-Net在单目3D车道检测任务中达到了最先进的性能。

Conclusion: 通过利用多帧输入的几何和实例信息，GTA-Net有效提升了单目3D车道检测的准确性和完整性。

Abstract: Monocular 3D lane detection aims to estimate 3D position of lanes from
frontal-view (FV) images. However, current monocular 3D lane detection methods
suffer from two limitations, including inaccurate geometric information of the
predicted 3D lanes and difficulties in maintaining lane integrity. To address
these issues, we seek to fully exploit the potential of multiple input frames.
First, we aim at enhancing the ability to perceive the geometry of scenes by
leveraging temporal geometric consistency. Second, we strive to improve the
integrity of lanes by revealing more instance information from temporal
sequences. Therefore, we propose a novel Geometry-aware Temporal Aggregation
Network (GTA-Net) for monocular 3D lane detection. On one hand, we develop the
Temporal Geometry Enhancement Module (TGEM), which exploits geometric
consistency across successive frames, facilitating effective geometry
perception. On the other hand, we present the Temporal Instance-aware Query
Generation (TIQG), which strategically incorporates temporal cues into query
generation, thereby enabling the exploration of comprehensive instance
information. Experiments demonstrate that our GTA-Net achieves SoTA results,
surpassing existing monocular 3D lane detection solutions.

</details>

### [228] [Beyond the Horizon: Decoupling UAVs Multi-View Action Recognition via Partial Order Transfer](https://arxiv.org/abs/2504.20530)
*Wenxuan Liu,Xian Zhong,Zhuo Zhou,Siyuan Yang,Chia-Wen Lin,Alex Chichung Kot*

Main category: cs.CV

TLDR: 论文提出了一种针对无人机（UAV）动作识别的多视角方法POG-MVNet，通过建模视角的层次结构来应对高度变化带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 无人机动作识别因垂直视角变化大而面临独特挑战，传统方法难以应对。

Method: 提出POG-MVNet框架，包含视角分区（VP）、顺序感知特征解耦（OFD）和动作部分顺序引导（APOG）三个模块。

Result: 在Drone-Action、MOD20和UAV数据集上表现优异，分别比ASAT和FAR方法提升4.7%和3.5%。

Conclusion: POG-MVNet通过建模视角层次结构，显著提升了无人机动作识别的性能。

Abstract: Action recognition in unmanned aerial vehicles (UAVs) poses unique challenges
due to significant view variations along the vertical spatial axis. Unlike
traditional ground-based settings, UAVs capture actions from a wide range of
altitudes, resulting in considerable appearance discrepancies. We introduce a
multi-view formulation tailored to varying UAV altitudes and empirically
observe a partial order among views, where recognition accuracy consistently
decreases as the altitude increases. This motivates a novel approach that
explicitly models the hierarchical structure of UAV views to improve
recognition performance across altitudes. To this end, we propose the Partial
Order Guided Multi-View Network (POG-MVNet), designed to address drastic view
variations by effectively leveraging view-dependent information across
different altitude levels. The framework comprises three key components: a View
Partition (VP) module, which uses the head-to-body ratio to group views by
altitude; an Order-aware Feature Decoupling (OFD) module, which disentangles
action-relevant and view-specific features under partial order guidance; and an
Action Partial Order Guide (APOG), which leverages the partial order to
transfer informative knowledge from easier views to support learning in more
challenging ones. We conduct experiments on Drone-Action, MOD20, and UAV
datasets, demonstrating that POG-MVNet significantly outperforms competing
methods. For example, POG-MVNet achieves a 4.7% improvement on Drone-Action
dataset and a 3.5% improvement on UAV dataset compared to state-of-the-art
methods ASAT and FAR. The code for POG-MVNet will be made available soon.

</details>

### [229] [Autoencoder Models for Point Cloud Environmental Synthesis from WiFi Channel State Information: A Preliminary Study](https://arxiv.org/abs/2504.20541)
*Daniele Pannone,Danilo Avola*

Main category: cs.CV

TLDR: 提出了一种基于WiFi信道状态信息（CSI）数据生成点云的深度学习框架，采用两阶段自编码器方法，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 利用WiFi数据实现环境点云重建，拓展无线感知和环境映射的应用。

Method: 两阶段自编码器：PointNet自编码器生成点云，CNN自编码器将CSI数据映射到匹配的潜在空间。

Result: 实验证明该方法能准确从WiFi数据重建环境点云。

Conclusion: 该方法在无线感知和环境映射中具有潜力。

Abstract: This paper introduces a deep learning framework for generating point clouds
from WiFi Channel State Information data. We employ a two-stage autoencoder
approach: a PointNet autoencoder with convolutional layers for point cloud
generation, and a Convolutional Neural Network autoencoder to map CSI data to a
matching latent space. By aligning these latent spaces, our method enables
accurate environmental point cloud reconstruction from WiFi data. Experimental
results validate the effectiveness of our approach, highlighting its potential
for wireless sensing and environmental mapping applications.

</details>

### [230] [PartHOI: Part-based Hand-Object Interaction Transfer via Generalized Cylinders](https://arxiv.org/abs/2504.20599)
*Qiaochu Wang,Chufeng Xiao,Manfred Lau,Hongbo Fu*

Main category: cs.CV

TLDR: 提出了一种基于语义部分的HOI迁移方法PartHOI，通过几何参数化建立跨类别对象间的鲁棒对应关系，优化手部姿势以适应目标对象。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖形状匹配，难以跨类别迁移手部姿势，而HOI常涉及语义部分，这些部分在跨类别中形状更一致。

Method: 使用广义圆柱体参数化对象部分的几何形状，建立鲁棒的几何对应关系，迁移接触点并优化手部姿势。

Result: 定性和定量结果表明，PartHOI在跨类别对象上表现优异，生成高保真结果优于现有方法。

Conclusion: PartHOI通过语义部分和几何参数化，实现了跨类别HOI的高效迁移，为HOI数据生成提供了新思路。

Abstract: Learning-based methods to understand and model hand-object interactions (HOI)
require a large amount of high-quality HOI data. One way to create HOI data is
to transfer hand poses from a source object to another based on the objects'
geometry. However, current methods for transferring hand poses between objects
rely on shape matching, limiting the ability to transfer poses across different
categories due to differences in their shapes and sizes. We observe that HOI
often involves specific semantic parts of objects, which often have more
consistent shapes across categories. In addition, constructing size-invariant
correspondences between these parts is important for cross-category transfer.
Based on these insights, we introduce a novel method PartHOI for part-based HOI
transfer. Using a generalized cylinder representation to parameterize an object
parts' geometry, PartHOI establishes a robust geometric correspondence between
object parts, and enables the transfer of contact points. Given the transferred
points, we optimize a hand pose to fit the target object well. Qualitative and
quantitative results demonstrate that our method can generalize HOI transfers
well even for cross-category objects, and produce high-fidelity results that
are superior to the existing methods.

</details>

### [231] [Purifying, Labeling, and Utilizing: A High-Quality Pipeline for Small Object Detection](https://arxiv.org/abs/2504.20602)
*Siwei Wang,Zhiwei Chen,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TLDR: PLUSNet通过优化检测流程的上游、中游和下游，提出了一种高质量的小目标检测框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有小目标检测方法通常孤立优化流程的某个阶段，忽略了整体优化，限制了性能提升。

Method: PLUSNet包含三个模块：HFP（净化上游特征）、MCLA（优化中游样本标签分配）和FDHead（高效利用下游信息）。

Result: 实验表明PLUSNet在多个数据集上显著提升了小目标检测性能。

Conclusion: PLUSNet通过整体优化检测流程，为小目标检测提供了高效解决方案。

Abstract: Small object detection is a broadly investigated research task and is
commonly conceptualized as a "pipeline-style" engineering process. In the
upstream, images serve as raw materials for processing in the detection
pipeline, where pre-trained models are employed to generate initial feature
maps. In the midstream, an assigner selects training positive and negative
samples. Subsequently, these samples and features are fed into the downstream
for classification and regression. Previous small object detection methods
often focused on improving isolated stages of the pipeline, thereby neglecting
holistic optimization and consequently constraining overall performance gains.
To address this issue, we have optimized three key aspects, namely Purifying,
Labeling, and Utilizing, in this pipeline, proposing a high-quality Small
object detection framework termed PLUSNet. Specifically, PLUSNet comprises
three sequential components: the Hierarchical Feature Purifier (HFP) for
purifying upstream features, the Multiple Criteria Label Assignment (MCLA) for
improving the quality of midstream training samples, and the Frequency
Decoupled Head (FDHead) for more effectively exploiting information to
accomplish downstream tasks. The proposed PLUS modules are readily integrable
into various object detectors, thus enhancing their detection capabilities in
multi-scale scenarios. Extensive experiments demonstrate the proposed PLUSNet
consistently achieves significant and consistent improvements across multiple
datasets for small object detection.

</details>

### [232] [EfficientHuman: Efficient Training and Reconstruction of Moving Human using Articulated 2D Gaussian](https://arxiv.org/abs/2504.20607)
*Hao Tian,Rui Liu,Wen Shen,Yilong Hu,Zhihao Zheng,Xiaolin Qin*

Main category: cs.CV

TLDR: EfficientHuman模型通过Articulated 2D Gaussian surfels和LBS优化，快速完成动态人体重建，提升渲染质量和训练速度。


<details>
  <summary>Details</summary>
Motivation: 3DGS在动态人体重建中存在多视角不一致和冗余高斯问题，影响重建效率和精度。

Method: 使用Articulated 2D Gaussian surfels编码高斯斑点，通过LBS转换到姿态空间，并引入姿态校准和LBS优化模块。

Result: 在ZJU-MoCap数据集上，平均重建时间少于1分钟，比现有方法快20秒，同时减少冗余高斯数量。

Conclusion: EfficientHuman在动态人体重建中实现了高效和高质量的平衡。

Abstract: 3D Gaussian Splatting (3DGS) has been recognized as a pioneering technique in
scene reconstruction and novel view synthesis. Recent work on reconstructing
the 3D human body using 3DGS attempts to leverage prior information on human
pose to enhance rendering quality and improve training speed. However, it
struggles to effectively fit dynamic surface planes due to multi-view
inconsistency and redundant Gaussians. This inconsistency arises because
Gaussian ellipsoids cannot accurately represent the surfaces of dynamic
objects, which hinders the rapid reconstruction of the dynamic human body.
Meanwhile, the prevalence of redundant Gaussians means that the training time
of these works is still not ideal for quickly fitting a dynamic human body. To
address these, we propose EfficientHuman, a model that quickly accomplishes the
dynamic reconstruction of the human body using Articulated 2D Gaussian while
ensuring high rendering quality. The key innovation involves encoding Gaussian
splats as Articulated 2D Gaussian surfels in canonical space and then
transforming them to pose space via Linear Blend Skinning (LBS) to achieve
efficient pose transformations. Unlike 3D Gaussians, Articulated 2D Gaussian
surfels can quickly conform to the dynamic human body while ensuring
view-consistent geometries. Additionally, we introduce a pose calibration
module and an LBS optimization module to achieve precise fitting of dynamic
human poses, enhancing the model's performance. Extensive experiments on the
ZJU-MoCap dataset demonstrate that EfficientHuman achieves rapid 3D dynamic
human reconstruction in less than a minute on average, which is 20 seconds
faster than the current state-of-the-art method, while also reducing the number
of redundant Gaussians.

</details>

### [233] [AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation](https://arxiv.org/abs/2504.20629)
*Jeongsoo Choi,Ji-Hoon Kim,Kim Sung-Bin,Tae-Hyun Oh,Joon Son Chung*

Main category: cs.CV

TLDR: AlignDiT是一种多模态对齐扩散变换器，用于从文本、视频和参考音频生成高质量语音，解决了现有方法在语音清晰度、同步性和自然性上的不足。


<details>
  <summary>Details</summary>
Motivation: 多模态语音生成在电影制作、配音和虚拟形象等领域有广泛应用，但现有方法在语音清晰度、音视频同步、自然性和声音相似性上存在局限。

Method: 提出AlignDiT，基于DiT架构，采用三种策略对齐多模态表示，并引入多模态无分类器引导机制，动态平衡各模态信息。

Result: 实验表明，AlignDiT在质量、同步性和说话人相似性上显著优于现有方法，并在多种多模态任务中表现出强泛化能力。

Conclusion: AlignDiT在多模态语音生成任务中实现了最先进的性能，具有广泛的应用潜力。

Abstract: In this paper, we address the task of multimodal-to-speech generation, which
aims to synthesize high-quality speech from multiple input modalities: text,
video, and reference audio. This task has gained increasing attention due to
its wide range of applications, such as film production, dubbing, and virtual
avatars. Despite recent progress, existing methods still suffer from
limitations in speech intelligibility, audio-video synchronization, speech
naturalness, and voice similarity to the reference speaker. To address these
challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer
that generates accurate, synchronized, and natural-sounding speech from aligned
multimodal inputs. Built upon the in-context learning capability of the DiT
architecture, AlignDiT explores three effective strategies to align multimodal
representations. Furthermore, we introduce a novel multimodal classifier-free
guidance mechanism that allows the model to adaptively balance information from
each modality during speech synthesis. Extensive experiments demonstrate that
AlignDiT significantly outperforms existing methods across multiple benchmarks
in terms of quality, synchronization, and speaker similarity. Moreover,
AlignDiT exhibits strong generalization capability across various multimodal
tasks, such as video-to-speech synthesis and visual forced alignment,
consistently achieving state-of-the-art performance. The demo page is available
at https://mm.kaist.ac.kr/projects/AlignDiT .

</details>

### [234] [LDPoly: Latent Diffusion for Polygonal Road Outline Extraction in Large-Scale Topographic Mapping](https://arxiv.org/abs/2504.20645)
*Weiqin Jiao,Hao Cheng,George Vosselman,Claudio Persello*

Main category: cs.CV

TLDR: LDPoly是首个专门用于从高分辨率航拍图像中提取多边形道路轮廓的框架，采用双潜在扩散模型和通道嵌入融合模块，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未针对多边形道路轮廓提取设计，而道路的分支结构和拓扑连接性对现有方法构成挑战。

Method: LDPoly结合双潜在扩散模型和通道嵌入融合模块，生成道路掩码和顶点热图，并通过定制多边形化方法减少顶点冗余。

Result: 在Map2ImLas数据集上，LDPoly在像素覆盖率、顶点效率、多边形规则性和道路连接性等指标上优于现有方法。

Conclusion: LDPoly为遥感图像中精确矢量对象轮廓提取开辟了新途径，是扩散模型在此领域的首次应用。

Abstract: Polygonal road outline extraction from high-resolution aerial images is an
important task in large-scale topographic mapping, where roads are represented
as vectorized polygons, capturing essential geometric features with minimal
vertex redundancy. Despite its importance, no existing method has been
explicitly designed for this task. While polygonal building outline extraction
has been extensively studied, the unique characteristics of roads, such as
branching structures and topological connectivity, pose challenges to these
methods. To address this gap, we introduce LDPoly, the first dedicated
framework for extracting polygonal road outlines from high-resolution aerial
images. Our method leverages a novel Dual-Latent Diffusion Model with a
Channel-Embedded Fusion Module, enabling the model to simultaneously generate
road masks and vertex heatmaps. A tailored polygonization method is then
applied to obtain accurate vectorized road polygons with minimal vertex
redundancy. We evaluate LDPoly on a new benchmark dataset, Map2ImLas, which
contains detailed polygonal annotations for various topographic objects in
several Dutch regions. Our experiments include both in-region and cross-region
evaluations, with the latter designed to assess the model's generalization
performance on unseen regions. Quantitative and qualitative results demonstrate
that LDPoly outperforms state-of-the-art polygon extraction methods across
various metrics, including pixel-level coverage, vertex efficiency, polygon
regularity, and road connectivity. We also design two new metrics to assess
polygon simplicity and boundary smoothness. Moreover, this work represents the
first application of diffusion models for extracting precise vectorized object
outlines without redundant vertices from remote-sensing imagery, paving the way
for future advancements in this field.

</details>

### [235] [SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data](https://arxiv.org/abs/2504.20648)
*Michael Ogezi,Freda Shi*

Main category: cs.CV

TLDR: 论文提出了一种增强视觉语言模型（VLM）空间推理能力的方法，通过构建合成VQA数据集SpaRE，显著提升了模型在空间推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在空间推理任务中表现不佳，主要因为常用数据集缺乏多样化的空间关系数据。

Method: 利用Localized Narratives、DOCCI和PixMo-Cap中的超详细图像描述，构建了包含455k样本和3.4百万QA对的合成VQA数据集。

Result: 训练后的SpaRE VLM在空间推理基准测试中表现显著提升，最高提升49%，同时保持通用任务性能。

Conclusion: SpaRE缩小了人类与VLM在空间推理能力上的差距，提升了模型在机器人技术和导航等实际任务中的应用潜力。

Abstract: Vision-language models (VLMs) work well in tasks ranging from image
captioning to visual question answering (VQA), yet they struggle with spatial
reasoning, a key skill for understanding our physical world that humans excel
at. We find that spatial relations are generally rare in widely used VL
datasets, with only a few being well represented, while most form a long tail
of underrepresented relations. This gap leaves VLMs ill-equipped to handle
diverse spatial relationships. To bridge it, we construct a synthetic VQA
dataset focused on spatial reasoning generated from hyper-detailed image
descriptions in Localized Narratives, DOCCI, and PixMo-Cap. Our dataset
consists of 455k samples containing 3.4 million QA pairs. Trained on this
dataset, our Spatial-Reasoning Enhanced (SpaRE) VLMs show strong improvements
on spatial reasoning benchmarks, achieving up to a 49% performance gain on the
What's Up benchmark, while maintaining strong results on general tasks. Our
work narrows the gap between human and VLM spatial reasoning and makes VLMs
more capable in real-world tasks such as robotics and navigation.

</details>

### [236] [Image deidentification in the XNAT ecosystem: use cases and solutions](https://arxiv.org/abs/2504.20657)
*Alex Michie,Simon J Doran*

Main category: cs.CV

TLDR: XNAT平台用于DICOM数据去标识化，参与MIDI-B挑战，初始得分97.91%，后改进至99.61%。规则方法有效去除姓名信息，但地址处理不足，机器学习模型部分成功但稍降性能。未来优化地址识别和图像像素数据去除。


<details>
  <summary>Details</summary>
Motivation: 解决DICOM数据去标识化问题，满足不同研究场景需求，并通过MIDI-B挑战验证方法有效性。

Method: 结合XNAT平台工具和独立工具，采用规则和机器学习模型进行去标识化。

Result: 初始得分97.91%，改进后达99.61%；规则方法完全去除姓名信息，地址处理不足；机器学习模型稍降性能至99.54%。

Conclusion: 未来需优化地址识别和图像像素数据去除，当前去标识化失败率为0.19%。

Abstract: XNAT is a server-based data management platform widely used in academia for
curating large databases of DICOM images for research projects. We describe in
detail a deidentification workflow for DICOM data using facilities in XNAT,
together with independent tools in the XNAT "ecosystem". We list different
contexts in which deidentification might be needed, based on our prior
experience. The starting point for participation in the Medical Image
De-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local
methodologies, which were adapted during the validation phase of the challenge.
Our result in the test phase was 97.91\%, considerably lower than our peers,
due largely to an arcane technical incompatibility of our methodology with the
challenge's Synapse platform, which prevented us receiving feedback during the
validation phase. Post-submission, additional discrepancy reports from the
organisers and via the MIDI-B Continuous Benchmarking facility, enabled us to
improve this score significantly to 99.61\%. An entirely rule-based approach
was shown to be capable of removing all name-related information in the test
corpus, but exhibited failures in dealing fully with address data. Initial
experiments using published machine-learning models to remove addresses were
partially successful but showed the models to be "over-aggressive" on other
types of free-text data, leading to a slight overall degradation in performance
to 99.54\%. Future development will therefore focus on improving
address-recognition capabilities, but also on better removal of identifiable
data burned into the image pixels. Several technical aspects relating to the
"answer key" are still under discussion with the challenge organisers, but we
estimate that our percentage of genuine deidentification failures on the MIDI-B
test corpus currently stands at 0.19\%. (Abridged from original for arXiv
submission)

</details>

### [237] [Advance Fake Video Detection via Vision Transformers](https://arxiv.org/abs/2504.20669)
*Joy Battocchio,Stefano Dell'Anna,Andrea Montibeller,Giulia Boato*

Main category: cs.CV

TLDR: 本文提出了一种基于Vision Transformer（ViT）的创新框架，用于检测AI生成的视频，解决了虚假多媒体传播的紧迫问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成多媒体技术的快速发展，虚假内容的传播风险增加，亟需高精度、泛化能力强的检测方法。

Method: 扩展ViT框架至视频领域，通过时间整合ViT嵌入提升检测性能。

Result: 方法在新的大规模多样化数据集上表现出高准确性、泛化能力和少样本学习能力。

Conclusion: 该框架为AI生成视频的检测提供了有效解决方案，符合当前法规需求。

Abstract: Recent advancements in AI-based multimedia generation have enabled the
creation of hyper-realistic images and videos, raising concerns about their
potential use in spreading misinformation. The widespread accessibility of
generative techniques, which allow for the production of fake multimedia from
prompts or existing media, along with their continuous refinement, underscores
the urgent need for highly accurate and generalizable AI-generated media
detection methods, underlined also by new regulations like the European Digital
AI Act. In this paper, we draw inspiration from Vision Transformer (ViT)-based
fake image detection and extend this idea to video. We propose an {original}
%innovative framework that effectively integrates ViT embeddings over time to
enhance detection performance. Our method shows promising accuracy,
generalization, and few-shot learning capabilities across a new, large and
diverse dataset of videos generated using five open source generative
techniques from the state-of-the-art, as well as a separate dataset containing
videos produced by proprietary generative methods.

</details>

### [238] [FBRT-YOLO: Faster and Better for Real-Time Aerial Image Detection](https://arxiv.org/abs/2504.20670)
*Yao Xiao,Tingfa Xu,Yu Xin,Jianan Li*

Main category: cs.CV

TLDR: FBRT-YOLO是一种用于实时航空图像检测的新型检测器，通过两个轻量级模块（FCM和MKP）优化小目标检测的精度与效率平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法在小目标检测上存在信息丢失和精度与效率不平衡的问题，阻碍了实时航空图像检测的发展。

Method: 提出FCM模块缓解小目标信息丢失问题，MKP模块利用多尺度卷积增强目标感知能力。

Result: 在Visdrone、UAVDT和AI-TOD数据集上，FBRT-YOLO在性能和速度上优于其他实时检测器。

Conclusion: FBRT-YOLO通过创新模块设计，显著提升了小目标检测的精度与效率，推动了实时航空图像检测的进步。

Abstract: Embedded flight devices with visual capabilities have become essential for a
wide range of applications. In aerial image detection, while many existing
methods have partially addressed the issue of small target detection,
challenges remain in optimizing small target detection and balancing detection
accuracy with efficiency. These issues are key obstacles to the advancement of
real-time aerial image detection. In this paper, we propose a new family of
real-time detectors for aerial image detection, named FBRT-YOLO, to address the
imbalance between detection accuracy and efficiency. Our method comprises two
lightweight modules: Feature Complementary Mapping Module (FCM) and
Multi-Kernel Perception Unit(MKP), designed to enhance object perception for
small targets in aerial images. FCM focuses on alleviating the problem of
information imbalance caused by the loss of small target information in deep
networks. It aims to integrate spatial positional information of targets more
deeply into the network,better aligning with semantic information in the deeper
layers to improve the localization of small targets. We introduce MKP, which
leverages convolutions with kernels of different sizes to enhance the
relationships between targets of various scales and improve the perception of
targets at different scales. Extensive experimental results on three major
aerial image datasets, including Visdrone, UAVDT, and AI-TOD,demonstrate that
FBRT-YOLO outperforms various real-time detectors in terms of performance and
speed.

</details>

### [239] [Occlusion-aware Driver Monitoring System using the Driver Monitoring Dataset](https://arxiv.org/abs/2504.20677)
*Paola Natalia Cañas,Alexander Diez,David Galvañ,Marcos Nieto,Igor Rodríguez*

Main category: cs.CV

TLDR: 提出了一种基于RGB和红外图像的鲁棒驾驶员监控系统，支持驾驶员识别、视线区域估计和遮挡检测，适用于不同光照条件。


<details>
  <summary>Details</summary>
Motivation: 提升驾驶员监控系统在遮挡和低光条件下的性能，符合EuroNCAP标准，增强系统可信度。

Method: 使用RGB和红外图像训练独立算法，整合为统一流程，解决多传感器和实车部署问题。

Result: 在DMD数据集和实际场景中验证了系统有效性，RGB模型表现更优，遮挡检测功能创新。

Conclusion: 该系统在复杂条件下表现优异，遮挡检测功能为DMS领域提供了新贡献。

Abstract: This paper presents a robust, occlusion-aware driver monitoring system (DMS)
utilizing the Driver Monitoring Dataset (DMD). The system performs driver
identification, gaze estimation by regions, and face occlusion detection under
varying lighting conditions, including challenging low-light scenarios. Aligned
with EuroNCAP recommendations, the inclusion of occlusion detection enhances
situational awareness and system trustworthiness by indicating when the
system's performance may be degraded. The system employs separate algorithms
trained on RGB and infrared (IR) images to ensure reliable functioning. We
detail the development and integration of these algorithms into a cohesive
pipeline, addressing the challenges of working with different sensors and
real-car implementation. Evaluation on the DMD and in real-world scenarios
demonstrates the effectiveness of the proposed system, highlighting the
superior performance of RGB-based models and the pioneering contribution of
robust occlusion detection in DMS.

</details>

### [240] [OG-HFYOLO :Orientation gradient guidance and heterogeneous feature fusion for deformation table cell instance segmentation](https://arxiv.org/abs/2504.20682)
*Long Liu,Cihui Yang*

Main category: cs.CV

TLDR: 论文提出OG-HFYOLO模型，通过梯度方向感知提取器和异构核交叉融合模块解决变形表格的结构识别问题，并生成新数据集DWTAL。


<details>
  <summary>Details</summary>
Motivation: 变形表格的几何变形导致内容与结构关联性弱，影响下游任务准确性。

Method: 采用梯度方向感知提取器增强边缘响应，结合异构核交叉融合模块和尺度感知损失函数，后处理中引入掩码驱动的非极大抑制。

Result: 模型在所有主流实例分割模型中表现出优异的分割精度。

Conclusion: OG-HFYOLO模型和DWTAL数据集填补了变形表格细粒度空间坐标定位的空白。

Abstract: Table structure recognition is a key task in document analysis. However, the
geometric deformation in deformed tables causes a weak correlation between
content information and structure, resulting in downstream tasks not being able
to obtain accurate content information. To obtain fine-grained spatial
coordinates of cells, we propose the OG-HFYOLO model, which enhances the edge
response by Gradient Orientation-aware Extractor, combines a Heterogeneous
Kernel Cross Fusion module and a scale-aware loss function to adapt to
multi-scale objective features, and introduces mask-driven non-maximal
suppression in the post-processing, which replaces the traditional bounding box
suppression mechanism. Furthermore, we also propose a data generator, filling
the gap in the dataset for fine-grained deformation table cell spatial
coordinate localization, and derive a large-scale dataset named Deformation
Wired Table (DWTAL). Experiments show that our proposed model demonstrates
excellent segmentation accuracy on all mainstream instance segmentation models.
The dataset and the source code are open source:
https://github.com/justliulong/OGHFYOLO.

</details>

### [241] [Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion](https://arxiv.org/abs/2504.20685)
*Zesheng Wang,Alexandre Bruckert,Patrick Le Callet,Guangtao Zhai*

Main category: cs.CV

TLDR: 提出Facial Action Diffusion (FAD)和Efficient Listener Network (ELNet)，通过扩散方法和高效网络设计，显著减少计算时间并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法因3DMM计算速度限制难以实现实时交互响应。

Method: 结合FAD（扩散方法）和ELNet（高效网络），利用视觉和音频信息生成面部动作。

Result: 性能优于现有方法，计算时间减少99%。

Conclusion: FAD和ELNet有效解决了实时生成面部动作的挑战。

Abstract: Generating realistic listener facial motions in dyadic conversations remains
challenging due to the high-dimensional action space and temporal dependency
requirements. Existing approaches usually consider extracting 3D Morphable
Model (3DMM) coefficients and modeling in the 3DMM space. However, this makes
the computational speed of the 3DMM a bottleneck, making it difficult to
achieve real-time interactive responses. To tackle this problem, we propose
Facial Action Diffusion (FAD), which introduces the diffusion methods from the
field of image generation to achieve efficient facial action generation. We
further build the Efficient Listener Network (ELNet) specially designed to
accommodate both the visual and audio information of the speaker as input.
Considering of FAD and ELNet, the proposed method learns effective listener
facial motion representations and leads to improvements of performance over the
state-of-the-art methods while reducing 99% computational time.

</details>

### [242] [In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer](https://arxiv.org/abs/2504.20690)
*Zechuan Zhang,Ji Xie,Yu Lu,Zongxin Yang,Yi Yang*

Main category: cs.CV

TLDR: 论文提出了一种基于指令的图像编辑方法，通过结合Diffusion Transformer（DiT）的生成能力和上下文感知，解决了现有方法在精度和效率上的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于指令的图像编辑方法存在精度与效率的权衡问题，微调方法需要大量计算资源和数据，而无训练方法则难以理解指令和保证编辑质量。

Method: 提出了三个创新点：(1) 基于上下文提示的零样本编辑框架；(2) LoRA-MoE混合调优策略；(3) 使用视觉语言模型（VLMs）的早期过滤推理时间缩放方法。

Result: 实验表明，该方法优于现有技术，仅需0.5%的训练数据和1%的可训练参数。

Conclusion: 该工作建立了一种高精度且高效的指令引导编辑新范式。

Abstract: Instruction-based image editing enables robust image modification via natural
language prompts, yet current methods face a precision-efficiency tradeoff.
Fine-tuning methods demand significant computational resources and large
datasets, while training-free techniques struggle with instruction
comprehension and edit quality. We resolve this dilemma by leveraging
large-scale Diffusion Transformer (DiT)' enhanced generation capacity and
native contextual awareness. Our solution introduces three contributions: (1)
an in-context editing framework for zero-shot instruction compliance using
in-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning
strategy that enhances flexibility with efficient adaptation and dynamic expert
routing, without extensive retraining; and (3) an early filter inference-time
scaling method using vision-language models (VLMs) to select better initial
noise early, improving edit quality. Extensive evaluations demonstrate our
method's superiority: it outperforms state-of-the-art approaches while
requiring only 0.5% training data and 1% trainable parameters compared to
conventional baselines. This work establishes a new paradigm that enables
high-precision yet efficient instruction-guided editing. Codes and demos can be
found in https://river-zhang.github.io/ICEdit-gh-pages/.

</details>

### [243] [Adept: Annotation-Denoising Auxiliary Tasks with Discrete Cosine Transform Map and Keypoint for Human-Centric Pretraining](https://arxiv.org/abs/2504.20800)
*Weizhen He,Yunfeng Yan,Shixiang Tang,Yiheng Deng,Yangyang Zhong,Pengxin Luo,Donglian Qi*

Main category: cs.CV

TLDR: 论文提出了一种基于RGB图像频率空间（DCT）的人体中心预训练方法，通过丢弃深度信息并引入新的标注去噪辅助任务，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的人体中心任务研究多依赖于特定任务的小规模数据集或额外的深度信息，限制了预训练模型的泛化能力。本文旨在通过RGB图像的频率空间探索语义信息，解决数据扩展性问题。

Method: 利用离散余弦变换（DCT）从RGB图像中提取频率空间信息，并结合关键点和DCT图的标注去噪辅助任务，学习人体细粒度语义信息。

Result: 在多个数据集（如COCO、MPII等）上的实验表明，该方法在姿态估计、人体解析、人群计数等任务中均优于现有方法。

Conclusion: 通过频率空间和辅助任务的设计，本文方法在不依赖深度信息的情况下，显著提升了人体中心任务的性能。

Abstract: Human-centric perception is the core of diverse computer vision tasks and has
been a long-standing research focus. However, previous research studied these
human-centric tasks individually, whose performance is largely limited to the
size of the public task-specific datasets. Recent human-centric methods
leverage the additional modalities, e.g., depth, to learn fine-grained semantic
information, which limits the benefit of pretraining models due to their
sensitivity to camera views and the scarcity of RGB-D data on the Internet.
This paper improves the data scalability of human-centric pretraining methods
by discarding depth information and exploring semantic information of RGB
images in the frequency space by Discrete Cosine Transform (DCT). We further
propose new annotation denoising auxiliary tasks with keypoints and DCT maps to
enforce the RGB image extractor to learn fine-grained semantic information of
human bodies. Our extensive experiments show that when pretrained on
large-scale datasets (COCO and AIC datasets) without depth annotation, our
model achieves better performance than state-of-the-art methods by +0.5 mAP on
COCO, +1.4 PCKh on MPII and -0.51 EPE on Human3.6M for pose estimation, by
+4.50 mIoU on Human3.6M for human parsing, by -3.14 MAE on SHA and -0.07 MAE on
SHB for crowd counting, by +1.1 F1 score on SHA and +0.8 F1 score on SHA for
crowd localization, and by +0.1 mAP on Market1501 and +0.8 mAP on MSMT for
person ReID. We also validate the effectiveness of our method on MPII+NTURGBD
datasets

</details>

### [244] [GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion](https://arxiv.org/abs/2504.20829)
*Jiaxin Hong,Sixu Chen,Shuoyang Sun,Hongyao Yu,Hao Fang,Yuqi Tan,Bin Chen,Shuhan Qi,Jiawei Li*

Main category: cs.CV

TLDR: 该论文首次系统研究了3D高斯泼溅（3DGS）中的后门威胁，提出了一种名为GuassTrap的新型投毒攻击方法，能在特定视角植入恶意视图，同时保持非目标视图的高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 随着3DGS在安全关键领域的快速应用，亟需研究其潜在的安全漏洞，尤其是后门威胁可能导致的场景混淆和环境误判。

Method: GuassTrap通过三阶段流程（攻击、稳定和正常训练）植入隐蔽且视角一致的毒化渲染，联合优化攻击效果和感知真实性。

Result: 实验表明，GuassTrap能有效嵌入难以察觉但有害的后门视图，同时保持正常视图的高质量渲染。

Conclusion: 该研究揭示了3D渲染中的安全风险，为未来防御机制的设计提供了重要参考。

Abstract: As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene
representation and novel view synthesis, its rapid adoption in safety-critical
domains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of
potential security vulnerabilities. This paper presents the first systematic
study of backdoor threats in 3DGS pipelines. We identify that adversaries may
implant backdoor views to induce malicious scene confusion during inference,
potentially leading to environmental misperception in autonomous navigation or
spatial distortion in immersive environments. To uncover this risk, we propose
GuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap
injects malicious views at specific attack viewpoints while preserving
high-quality rendering in non-target views, ensuring minimal detectability and
maximizing potential harm. Specifically, the proposed method consists of a
three-stage pipeline (attack, stabilization, and normal training) to implant
stealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing
attack efficacy and perceptual realism to expose security risks in 3D
rendering. Extensive experiments on both synthetic and real-world datasets
demonstrate that GuassTrap can effectively embed imperceptible yet harmful
backdoor views while maintaining high-quality rendering in normal views,
validating its robustness, adaptability, and practical applicability.

</details>

### [245] [CMT: A Cascade MAR with Topology Predictor for Multimodal Conditional CAD Generation](https://arxiv.org/abs/2504.20830)
*Jianyu Wu,Yizhou Wang,Xiangyu Yue,Xinzhu Ma,Jingyang Guo,Dongzhan Zhou,Wanli Ouyang,Shixiang Tang*

Main category: cs.CV

TLDR: 提出了一种基于边界表示（B-Rep）的多模态CAD生成框架CMT，并开发了大规模多模态CAD数据集mmABC。CMT在无条件生成和条件生成任务中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有CAD方法因简化表示或架构无法支持多模态设计需求，难以实现准确且用户友好的设计。

Method: 提出级联MAR与拓扑预测器（CMT）框架，结合B-Rep的“边-面”先验，并开发包含130万B-Rep模型的mmABC数据集。

Result: CMT在无条件生成中Coverage和Valid ratio分别提升10.68%和10.3%，在图像条件生成中Chamfer提升4.01。

Conclusion: CMT在多模态CAD生成中表现优越，数据集和代码将公开。

Abstract: While accurate and user-friendly Computer-Aided Design (CAD) is crucial for
industrial design and manufacturing, existing methods still struggle to achieve
this due to their over-simplified representations or architectures incapable of
supporting multimodal design requirements. In this paper, we attempt to tackle
this problem from both methods and datasets aspects. First, we propose a
cascade MAR with topology predictor (CMT), the first multimodal framework for
CAD generation based on Boundary Representation (B-Rep). Specifically, the
cascade MAR can effectively capture the ``edge-counters-surface'' priors that
are essential in B-Reps, while the topology predictor directly estimates
topology in B-Reps from the compact tokens in MAR. Second, to facilitate
large-scale training, we develop a large-scale multimodal CAD dataset, mmABC,
which includes over 1.3 million B-Rep models with multimodal annotations,
including point clouds, text descriptions, and multi-view images. Extensive
experiments show the superior of CMT in both conditional and unconditional CAD
generation tasks. For example, we improve Coverage and Valid ratio by +10.68%
and +10.3%, respectively, compared to state-of-the-art methods on ABC in
unconditional generation. CMT also improves +4.01 Chamfer on image conditioned
CAD generation on mmABC. The dataset, code and pretrained network shall be
released.

</details>

### [246] [RadSAM: Segmenting 3D radiological images with a 2D promptable model](https://arxiv.org/abs/2504.20837)
*Julien Khlaut,Elodie Ferreres,Daniel Tordjman,Hélène Philippe,Tom Boeken,Pierre Manceron,Corentin Dancette*

Main category: cs.CV

TLDR: RadSAM提出了一种基于2D模型的3D医学图像分割方法，通过单次提示实现3D对象的分割，解决了现有方法需要逐片提示的问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在临床中至关重要，但现有方法如SAM虽强大，却无法有效处理3D医学数据且缺乏编辑功能。

Method: 训练2D模型使用噪声掩码、边界框和点作为初始提示，并通过迭代推理重建3D掩码。

Result: 在AMOS腹部器官分割数据集上验证了RadSAM的有效性，优于现有方法。

Conclusion: RadSAM填补了2D模型在3D医学图像分割中的不足，展示了高效的分割和编辑能力。

Abstract: Medical image segmentation is a crucial and time-consuming task in clinical
care, where mask precision is extremely important. The Segment Anything Model
(SAM) offers a promising approach, as it provides an interactive interface
based on visual prompting and edition to refine an initial segmentation. This
model has strong generalization capabilities, does not rely on predefined
classes, and adapts to diverse objects; however, it is pre-trained on natural
images and lacks the ability to process medical data effectively. In addition,
this model is built for 2D images, whereas a whole medical domain is based on
3D images, such as CT and MRI. Recent adaptations of SAM for medical imaging
are based on 2D models, thus requiring one prompt per slice to segment 3D
objects, making the segmentation process tedious. They also lack important
features such as editing. To bridge this gap, we propose RadSAM, a novel method
for segmenting 3D objects with a 2D model from a single prompt. In practice, we
train a 2D model using noisy masks as initial prompts, in addition to bounding
boxes and points. We then use this novel prompt type with an iterative
inference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a
benchmark to evaluate the model's ability to segment 3D objects in CT images
from a single prompt and evaluate the models' out-of-domain transfer and
edition capabilities. We demonstrate the effectiveness of our approach against
state-of-the-art models on this benchmark using the AMOS abdominal organ
segmentation dataset.

</details>

### [247] [FedMVP: Federated Multi-modal Visual Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2504.20860)
*Mainak Singha,Subhankar Roy,Sarthak Mehrotra,Ankit Jha,Moloud Abdar,Biplab Banerjee,Elisa Ricci*

Main category: cs.CV

TLDR: FedMVP通过多模态视觉提示调整（结合图像和文本特征）提升联邦学习中CLIP模型的泛化能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决文本提示调整在联邦学习中过度依赖已知概念和文本特征的问题，提升对未见概念的适应性。

Method: 提出FedMVP，利用PromptFormer模块通过交叉注意力对齐文本和视觉特征，生成动态多模态视觉提示，并结合CLIP相似性损失和一致性损失进行训练。

Result: 在20个数据集上的实验表明，FedMVP在保持已知类别性能的同时，对未见类别和领域具有更高的泛化能力。

Conclusion: FedMVP通过多模态特征整合显著提升了模型在联邦学习中的适应性和泛化能力。

Abstract: Textual prompt tuning adapts Vision-Language Models (e.g., CLIP) in federated
learning by tuning lightweight input tokens (or prompts) on local client data,
while keeping network weights frozen. Post training, only the prompts are
shared by the clients with the central server for aggregation. However, textual
prompt tuning often struggles with overfitting to known concepts and may be
overly reliant on memorized text features, limiting its adaptability to unseen
concepts. To address this limitation, we propose Federated Multimodal Visual
Prompt Tuning (FedMVP) that conditions the prompts on comprehensive contextual
information -- image-conditioned features and textual attribute features of a
class -- that is multimodal in nature. At the core of FedMVP is a PromptFormer
module that synergistically aligns textual and visual features through
cross-attention, enabling richer contexual integration. The dynamically
generated multimodal visual prompts are then input to the frozen vision encoder
of CLIP, and trained with a combination of CLIP similarity loss and a
consistency loss. Extensive evaluation on 20 datasets spanning three
generalization settings demonstrates that FedMVP not only preserves performance
on in-distribution classes and domains, but also displays higher
generalizability to unseen classes and domains when compared to
state-of-the-art methods. Codes will be released upon acceptance.

</details>

### [248] [AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection](https://arxiv.org/abs/2504.20865)
*Lorenzo Pellegrini,Davide Cozzolino,Serafino Pandolfini,Davide Maltoni,Matteo Ferrara,Luisa Verdoliva,Marco Prati,Marco Ramilli*

Main category: cs.CV

TLDR: Ai-GenBench是一个新的基准测试，旨在解决AI生成图像检测的挑战，通过动态评估框架和标准化协议，支持对新生成模型的泛化能力测试。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展带来了高质量图像合成的能力，但也对媒体真实性提出了挑战，需要一种鲁棒的检测方法。

Method: Ai-GenBench采用动态评估框架，逐步训练检测模型，测试其对新型生成模型（如从GANs到扩散模型）的泛化能力。

Result: 该基准提供了高质量、多样化的数据集和标准化工具，解决了现有方法的局限性，如不公平比较和计算需求过高。

Conclusion: Ai-GenBench通过公开代码和数据，支持可复现的研究，为开发鲁棒的检测工具提供了基础。

Abstract: The rapid advancement of generative AI has revolutionized image creation,
enabling high-quality synthesis from text prompts while raising critical
challenges for media authenticity. We present Ai-GenBench, a novel benchmark
designed to address the urgent need for robust detection of AI-generated images
in real-world scenarios. Unlike existing solutions that evaluate models on
static datasets, Ai-GenBench introduces a temporal evaluation framework where
detection methods are incrementally trained on synthetic images, historically
ordered by their generative models, to test their ability to generalize to new
generative models, such as the transition from GANs to diffusion models. Our
benchmark focuses on high-quality, diverse visual content and overcomes key
limitations of current approaches, including arbitrary dataset splits, unfair
comparisons, and excessive computational demands. Ai-GenBench provides a
comprehensive dataset, a standardized evaluation protocol, and accessible tools
for both researchers and non-experts (e.g., journalists, fact-checkers),
ensuring reproducibility while maintaining practical training requirements. By
establishing clear evaluation rules and controlled augmentation strategies,
Ai-GenBench enables meaningful comparison of detection methods and scalable
solutions. Code and data are publicly available to ensure reproducibility and
to support the development of robust forensic detectors to keep pace with the
rise of new synthetic generators.

</details>

### [249] [FLIM-based Salient Object Detection Networks with Adaptive Decoders](https://arxiv.org/abs/2504.20872)
*Gilson Junior Soares,Matheus Abrantes Cerqueira,Jancarlo F. Gomes,Laurent Najman,Silvio Jamil F. Guimarães,Alexandre Xavier Falcão*

Main category: cs.CV

TLDR: 该论文提出了一种名为FLIM的轻量级网络方法，用于显著目标检测（SOD），通过结合FLIM编码器和自适应解码器，实现了比现有轻量级模型更轻量化的设计，且仅需少量代表性图像训练。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在计算资源有限的情况下，使用预训练的轻量级模型而非深度神经网络进行SOD任务，以应对标记数据受限的应用场景。

Method: 方法包括使用FLIM编码器从少量代表性图像的标记像素中估计编码器核，并结合自适应解码器（其权重通过启发式函数为每张输入图像估计）。训练过程仅需3-4张图像且无需反向传播。

Result: 实验表明，提出的FLIM网络在两种挑战性SOD任务中优于现有轻量级网络和其他FLIM变体，验证了其有效性。

Conclusion: 结论强调了进一步研究此类方法在新应用中的重要性，展示了FLIM网络在资源受限场景下的潜力。

Abstract: Salient Object Detection (SOD) methods can locate objects that stand out in
an image, assign higher values to their pixels in a saliency map, and binarize
the map outputting a predicted segmentation mask. A recent tendency is to
investigate pre-trained lightweight models rather than deep neural networks in
SOD tasks, coping with applications under limited computational resources. In
this context, we have investigated lightweight networks using a methodology
named Feature Learning from Image Markers (FLIM), which assumes that the
encoder's kernels can be estimated from marker pixels on discriminative regions
of a few representative images. This work proposes flyweight networks, hundreds
of times lighter than lightweight models, for SOD by combining a FLIM encoder
with an adaptive decoder, whose weights are estimated for each input image by a
given heuristic function. Such FLIM networks are trained from three to four
representative images only and without backpropagation, making the models
suitable for applications under labeled data constraints as well. We study five
adaptive decoders; two of them are introduced here. Differently from the
previous ones that rely on one neuron per pixel with shared weights, the
heuristic functions of the new adaptive decoders estimate the weights of each
neuron per pixel. We compare FLIM models with adaptive decoders for two
challenging SOD tasks with three lightweight networks from the
state-of-the-art, two FLIM networks with decoders trained by backpropagation,
and one FLIM network whose labeled markers define the decoder's weights. The
experiments demonstrate the advantages of the proposed networks over the
baselines, revealing the importance of further investigating such methods in
new applications.

</details>

### [250] [Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for Visual Classifiers](https://arxiv.org/abs/2504.20902)
*Quentin Guimard,Moreno D'Incà,Massimiliano Mancini,Elisa Ricci*

Main category: cs.CV

TLDR: C2B是一种无需标注数据的偏差发现框架，通过任务描述生成偏差建议并评估模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有偏差识别方法依赖标注数据，限制了应用范围。C2B旨在实现无需标注的任务无关偏差检测。

Method: 利用大语言模型生成偏差建议和描述，通过检索模型收集图像并评估模型准确性。

Result: C2B在公开数据集上表现优于依赖标注的基线方法，能发现更多偏差。

Conclusion: C2B为无监督偏差检测提供了有前景的解决方案。

Abstract: A person downloading a pre-trained model from the web should be aware of its
biases. Existing approaches for bias identification rely on datasets containing
labels for the task of interest, something that a non-expert may not have
access to, or may not have the necessary resources to collect: this greatly
limits the number of tasks where model biases can be identified. In this work,
we present Classifier-to-Bias (C2B), the first bias discovery framework that
works without access to any labeled data: it only relies on a textual
description of the classification task to identify biases in the target
classification model. This description is fed to a large language model to
generate bias proposals and corresponding captions depicting biases together
with task-specific target labels. A retrieval model collects images for those
captions, which are then used to assess the accuracy of the model w.r.t. the
given biases. C2B is training-free, does not require any annotations, has no
constraints on the list of biases, and can be applied to any pre-trained model
on any classification task. Experiments on two publicly available datasets show
that C2B discovers biases beyond those of the original datasets and outperforms
a recent state-of-the-art bias detection baseline that relies on task-specific
annotations, being a promising first step toward addressing task-agnostic
unsupervised bias detection.

</details>

### [251] [DS_FusionNet: Dynamic Dual-Stream Fusion with Bidirectional Knowledge Distillation for Plant Disease Recognition](https://arxiv.org/abs/2504.20948)
*Yanghui Song,Chengfu Yang*

Main category: cs.CV

TLDR: 提出了一种动态双流融合网络（DS_FusionNet），用于解决植物病害识别中的小样本学习、叶片遮挡等问题，显著提高了识别精度。


<details>
  <summary>Details</summary>
Motivation: 全球经济作物生长安全面临严峻挑战，精确识别和预防植物病害成为人工智能农业技术中的关键问题。

Method: 采用双主干架构、可变形动态融合模块和双向知识蒸馏策略，构建DS_FusionNet。

Result: 在PlantDisease和CIFAR-10数据集上仅用10%数据达到90%以上分类准确率，在复杂PlantWild数据集上保持85%准确率。

Conclusion: 研究为细粒度图像分类提供了新技术思路，并为农业病害精确识别和管理奠定了坚实基础。

Abstract: Given the severe challenges confronting the global growth security of
economic crops, precise identification and prevention of plant diseases has
emerged as a critical issue in artificial intelligence-enabled agricultural
technology. To address the technical challenges in plant disease recognition,
including small-sample learning, leaf occlusion, illumination variations, and
high inter-class similarity, this study innovatively proposes a Dynamic
Dual-Stream Fusion Network (DS_FusionNet). The network integrates a
dual-backbone architecture, deformable dynamic fusion modules, and
bidirectional knowledge distillation strategy, significantly enhancing
recognition accuracy. Experimental results demonstrate that DS_FusionNet
achieves classification accuracies exceeding 90% using only 10% of the
PlantDisease and CIFAR-10 datasets, while maintaining 85% accuracy on the
complex PlantWild dataset, exhibiting exceptional generalization capabilities.
This research not only provides novel technical insights for fine-grained image
classification but also establishes a robust foundation for precise
identification and management of agricultural diseases.

</details>

### [252] [SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features](https://arxiv.org/abs/2504.20970)
*Mete Erdogan,Sebnem Demirtas*

Main category: cs.CV

TLDR: 提出了一种基于SVD-LS的多类肺炎分类框架，结合自监督和迁移学习模型，提供高效且准确的诊断工具。


<details>
  <summary>Details</summary>
Motivation: 通过X射线影像实现肺炎的早期准确诊断对治疗和患者预后至关重要，机器学习技术可辅助放射科医生提高效率和可靠性。

Method: 采用SVD-LS框架，利用自监督和迁移学习模型提取特征，避免计算昂贵的梯度微调，采用闭式非迭代分类方法。

Result: 实验表明SVD-LS在保持竞争力的同时显著降低计算成本，适用于实时医学影像应用。

Conclusion: SVD-LS是一种高效且准确的肺炎分类方法，适合临床实时应用。

Abstract: Accurate and early diagnosis of pneumonia through X-ray imaging is essential
for effective treatment and improved patient outcomes. Recent advancements in
machine learning have enabled automated diagnostic tools that assist
radiologists in making more reliable and efficient decisions. In this work, we
propose a Singular Value Decomposition-based Least Squares (SVD-LS) framework
for multi-class pneumonia classification, leveraging powerful feature
representations from state-of-the-art self-supervised and transfer learning
models. Rather than relying on computationally expensive gradient based
fine-tuning, we employ a closed-form, non-iterative classification approach
that ensures efficiency without compromising accuracy. Experimental results
demonstrate that SVD-LS achieves competitive performance while offering
significantly reduced computational costs, making it a viable alternative for
real-time medical imaging applications.

</details>

### [253] [TesserAct: Learning 4D Embodied World Models](https://arxiv.org/abs/2504.20995)
*Haoyu Zhen,Qiao Sun,Hongxin Zhang,Junyan Li,Siyuan Zhou,Yilun Du,Chuang Gan*

Main category: cs.CV

TLDR: 本文提出了一种学习4D世界模型的有效方法，通过RGB-DN视频训练，预测3D场景的动态变化，并确保时空一致性。


<details>
  <summary>Details</summary>
Motivation: 传统2D模型无法捕捉详细的形状、配置和时间变化，因此需要一种能预测4D动态场景的方法。

Method: 扩展机器人操作视频数据集，加入深度和法线信息，微调视频生成模型以预测RGB-DN帧，并设计算法将其转换为高质量4D场景。

Result: 方法在时空一致性、新颖视角合成和策略学习方面优于现有视频世界模型。

Conclusion: 该方法为4D世界建模提供了高效解决方案，显著提升了场景预测和策略学习能力。

Abstract: This paper presents an effective approach for learning novel 4D embodied
world models, which predict the dynamic evolution of 3D scenes over time in
response to an embodied agent's actions, providing both spatial and temporal
consistency. We propose to learn a 4D world model by training on RGB-DN (RGB,
Depth, and Normal) videos. This not only surpasses traditional 2D models by
incorporating detailed shape, configuration, and temporal changes into their
predictions, but also allows us to effectively learn accurate inverse dynamic
models for an embodied agent. Specifically, we first extend existing robotic
manipulation video datasets with depth and normal information leveraging
off-the-shelf models. Next, we fine-tune a video generation model on this
annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for
each frame. We then present an algorithm to directly convert generated RGB,
Depth, and Normal videos into a high-quality 4D scene of the world. Our method
ensures temporal and spatial coherence in 4D scene predictions from embodied
scenarios, enables novel view synthesis for embodied environments, and
facilitates policy learning that significantly outperforms those derived from
prior video-based world models.

</details>

### [254] [X-Fusion: Introducing New Modality to Frozen Large Language Models](https://arxiv.org/abs/2504.20996)
*Sicheng Mo,Thao Nguyen,Xun Huang,Siddharth Srinivasan Iyer,Yijun Li,Yuchen Liu,Abhishek Tandon,Eli Shechtman,Krishna Kumar Singh,Yong Jae Lee,Bolei Zhou,Yuheng Li*

Main category: cs.CV

TLDR: X-Fusion是一个扩展预训练大语言模型（LLM）用于多模态任务的框架，保持其语言能力的同时整合视觉信息。


<details>
  <summary>Details</summary>
Motivation: 解决如何在保留LLM语言能力的同时扩展其多模态任务能力的问题。

Method: 采用双塔设计，冻结LLM参数并引入模态特定权重，整合视觉信息。

Result: 在图像到文本和文本到图像任务中表现优于其他架构，理解数据提升生成质量，减少图像噪声提高性能。

Conclusion: X-Fusion为构建高效统一多模态模型提供了有价值的设计思路。

Abstract: We propose X-Fusion, a framework that extends pretrained Large Language
Models (LLMs) for multimodal tasks while preserving their language
capabilities. X-Fusion employs a dual-tower design with modality-specific
weights, keeping the LLM's parameters frozen while integrating vision-specific
information for both understanding and generation. Our experiments demonstrate
that X-Fusion consistently outperforms alternative architectures on both
image-to-text and text-to-image tasks. We find that incorporating
understanding-focused data improves generation quality, reducing image data
noise enhances overall performance, and feature alignment accelerates
convergence for smaller models but has minimal impact on larger ones. Our
findings provide valuable insights into building efficient unified multimodal
models.

</details>

### [255] [YoChameleon: Personalized Vision and Language Generation](https://arxiv.org/abs/2504.20998)
*Thao Nguyen,Krishna Kumar Singh,Jing Shi,Trung Bui,Yong Jae Lee,Yuheng Li*

Main category: cs.CV

TLDR: Yo'Chameleon是首个研究大型多模态模型个性化的方法，通过软提示调优嵌入特定主题信息，实现问答和图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型缺乏对用户特定概念的个性化知识，尤其在图像生成领域尚未探索。

Method: 利用软提示调优嵌入主题信息，结合自提示优化机制和“软正向”图像生成方法。

Result: 能够在少量样本下实现高质量的多模态个性化表现。

Conclusion: Yo'Chameleon为多模态模型个性化提供了有效解决方案。

Abstract: Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into
powerful tools with millions of users. However, they remain generic models and
lack personalized knowledge of specific user concepts. Previous work has
explored personalization for text generation, yet it remains unclear how these
methods can be adapted to new modalities, such as image generation. In this
paper, we introduce Yo'Chameleon, the first attempt to study personalization
for large multimodal models. Given 3-5 images of a particular concept,
Yo'Chameleon leverages soft-prompt tuning to embed subject-specific information
to (i) answer questions about the subject and (ii) recreate pixel-level details
to produce images of the subject in new contexts. Yo'Chameleon is trained with
(i) a self-prompting optimization mechanism to balance performance across
multiple modalities, and (ii) a ``soft-positive" image generation approach to
enhance image quality in a few-shot setting.

</details>

<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [256] [On Stochastic Rounding with Few Random Bits](https://arxiv.org/abs/2504.20634)
*Andrew Fitzgibbon,Stephen Felix*

Main category: math.NA

TLDR: 本文探讨了低精度浮点计算中随机舍入（SR）的实现，特别是少位随机舍入（FBSR）的潜在偏差问题，并分析了这些偏差对机器学习的影响。


<details>
  <summary>Details</summary>
Motivation: 低精度浮点格式和混合精度算术在大规模数值计算中应用广泛，随机舍入技术可以提升其性能。然而，高质量的随机比特生成成本较高，因此需要研究如何在减少随机比特使用的同时保持随机舍入的优良特性。

Method: 研究了几种少位随机舍入（FBSR）的实现方法，分析了这些方法在有限比特和有限精度条件下可能引入的偏差。

Result: 发现某些自然实现的FBSR会引入显著偏差，而这些偏差在无限比特和无限精度的条件下并不存在。

Conclusion: 研究揭示了低精度浮点计算中随机舍入配置参数的重要性，为开发者和使用者提供了新的注意事项。

Abstract: Large-scale numerical computations make increasing use of low-precision (LP)
floating point formats and mixed precision arithmetic, which can be enhanced by
the technique of stochastic rounding (SR), that is, rounding an intermediate
high-precision value up or down randomly as a function of the value's distance
to the two rounding candidates. Stochastic rounding requires, in addition to
the high-precision input value, a source of random bits. As the provision of
high-quality random bits is an additional computational cost, it is of interest
to require as few bits as possible while maintaining the desirable properties
of SR in a given computation, or computational domain. This paper examines a
number of possible implementations of few-bit stochastic rounding (FBSR), and
shows how several natural implementations can introduce sometimes significant
bias into the rounding process, which are not present in the case of
infinite-bit, infinite-precision examinations of these implementations. The
paper explores the impact of these biases in machine learning examples, and
hence opens another class of configuration parameters of which practitioners
should be aware when developing or adopting low-precision floating point. Code
is available at
http://github.com/graphcore-research/arith25-stochastic-rounding.

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [257] [SoK: Enhancing Privacy-Preserving Software Development from a Developers' Perspective](https://arxiv.org/abs/2504.20350)
*Tharaka Wijesundara,Nalin Asanka Gamagedara Arachchilage,Matthew Warren*

Main category: cs.SE

TLDR: 该论文通过系统文献综述（SLR）分析隐私保护软件开发中经过实证验证的解决方案，填补现有研究的空白，为研究者和实践者提供改进和选择有效方案的依据。


<details>
  <summary>Details</summary>
Motivation: 隐私保护在软件开发中日益重要，但现有工具和方法多为概念验证，缺乏实证评估，导致实际应用性不确定。

Method: 采用系统文献综述（SLR）方法，识别和分析经过实证验证的隐私保护解决方案。

Result: 研究将揭示现有解决方案的局限性，并为改进和开发新方法提供依据。

Conclusion: 该SLR将为研究者和开发者提供经过验证的隐私保护解决方案，推动隐私保护在软件开发中的实际应用。

Abstract: In software development, privacy preservation has become essential with the
rise of privacy concerns and regulations such as GDPR and CCPA. While several
tools, guidelines, methods, methodologies, and frameworks have been proposed to
support developers embedding privacy into software applications, most of them
are proofs-of-concept without empirical evaluations, making their practical
applicability uncertain. These solutions should be evaluated for different
types of scenarios (e.g., industry settings such as rapid software development
environments, teams with different privacy knowledge, etc.) to determine what
their limitations are in various industry settings and what changes are
required to refine current solutions before putting them into industry and
developing new developer-supporting approaches. For that, a thorough review of
empirically evaluated current solutions will be very effective. However, the
existing secondary studies that examine the available developer support provide
broad overviews but do not specifically analyze empirically evaluated solutions
and their limitations. Therefore, this Systematic Literature Review (SLR) aims
to identify and analyze empirically validated solutions that are designed to
help developers in privacy-preserving software development. The findings will
provide valuable insights for researchers to improve current privacy-preserving
solutions and for practitioners looking for effective and validated solutions
to embed privacy into software development.

</details>

### [258] [Secure Coding with AI, From Creation to Inspection](https://arxiv.org/abs/2504.20814)
*Vladislav Belozerov,Peter J Barclay,Ashkan Sami*

Main category: cs.SE

TLDR: 本文研究了ChatGPT生成的代码的安全性，基于真实开发者交互数据，发现ChatGPT生成的代码比开发者自己的代码更容易出现漏洞，且ChatGPT在检测和修复漏洞时存在局限性。


<details>
  <summary>Details</summary>
Motivation: 探索ChatGPT在真实开发者交互中生成的代码的安全性，并评估其检测和修复漏洞的能力。

Method: 使用静态扫描器分析1,586个代码片段，手动确认32个漏洞，并通过OpenAI API提交给ChatGPT进行检测和修复。

Result: ChatGPT成功检测到18/32个漏洞并修复了17个，但未能识别或修复其余漏洞，且22个漏洞是由ChatGPT自身引入的。

Conclusion: ChatGPT在生成安全代码和检测漏洞方面不可靠，仍需依赖静态扫描器和人工审查。

Abstract: While prior studies have explored security in code generated by ChatGPT and
other Large Language Models, they were conducted in controlled experimental
settings and did not use code generated or provided from actual developer
interactions. This paper not only examines the security of code generated by
ChatGPT based on real developer interactions, curated in the DevGPT dataset,
but also assesses ChatGPT's capability to find and fix these vulnerabilities.
We analysed 1,586 C, C++, and C# code snippets using static scanners, which
detected potential issues in 124 files. After manual analysis, we selected 26
files with 32 confirmed vulnerabilities for further investigation.
  We submitted these files to ChatGPT via the OpenAI API, asking it to detect
security issues, identify the corresponding Common Weakness Enumeration
numbers, and propose fixes. The responses and modified code were manually
reviewed and re-scanned for vulnerabilities. ChatGPT successfully detected 18
out of 32 security issues and resolved 17 issues but failed to recognize or fix
the remainder. Interestingly, only 10 vulnerabilities were resulted from the
user prompts, while 22 were introduced by ChatGPT itself.
  We highlight for developers that code generated by ChatGPT is more likely to
contain vulnerabilities compared to their own code. Furthermore, at times
ChatGPT reports incorrect information with apparent confidence, which may
mislead less experienced developers. Our findings confirm previous studies in
demonstrating that ChatGPT is not sufficiently reliable for generating secure
code nor identifying all vulnerabilities, highlighting the continuing
importance of static scanners and manual review.

</details>

### [259] [Self-Healing Software Systems: Lessons from Nature, Powered by AI](https://arxiv.org/abs/2504.20093)
*Mohammad Baqar,Rajat Khanda,Saba Naqvi*

Main category: cs.SE

TLDR: 该论文提出了一种受生物愈合启发的AI驱动自愈软件框架，结合日志分析、静态代码检查和AI生成补丁，以减少停机时间并增强软件弹性。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂性和规模的增加，自主检测、诊断和恢复故障的能力变得至关重要。受生物愈合过程的启发，研究如何通过AI实现类似的自愈功能。

Method: 提出了一种模仿生物模型的框架：系统可观测性工具作为感官输入，AI模型作为诊断和修复的核心，修复代理应用针对性的代码和测试修改。

Result: 通过案例研究和模拟评估，该框架在减少停机时间、加速调试和增强软件弹性方面优于传统手动方法。

Conclusion: 该研究为智能、自适应和自依赖的软件系统奠定了基础，使其能够像生物体一样持续自愈。

Abstract: As modern software systems grow in complexity and scale, their ability to
autonomously detect, diagnose, and recover from failures becomes increasingly
vital. Drawing inspiration from biological healing - where the human body
detects damage, signals the brain, and activates targeted recovery - this paper
explores the concept of self-healing software driven by artificial
intelligence. We propose a novel framework that mimics this biological model
system observability tools serve as sensory inputs, AI models function as the
cognitive core for diagnosis and repair, and healing agents apply targeted code
and test modifications. By combining log analysis, static code inspection, and
AI-driven generation of patches or test updates, our approach aims to reduce
downtime, accelerate debugging, and enhance software resilience. We evaluate
the effectiveness of this model through case studies and simulations, comparing
it against traditional manual debugging and recovery workflows. This work paves
the way toward intelligent, adaptive and self-reliant software systems capable
of continuous healing, akin to living organisms.

</details>

### [260] [ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies](https://arxiv.org/abs/2504.20117)
*Shubham Gandhi,Dhruv Shah,Manasi Patwardhan,Lovekesh Vig,Gautam Shroff*

Main category: cs.SE

TLDR: ResearchCodeAgent是一个基于大型语言模型的多智能体系统，用于自动化机器学习文献中研究方法的代码生成，显著减少编码时间并提高代码质量。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习研究中高概念与实现之间的鸿沟，加速研究实现过程。

Method: 采用灵活的智能体架构和动态规划机制，结合短期和长期记忆，支持上下文感知的研究环境交互。

Result: 在三种机器学习任务中，46.9%的生成代码高质量且无错误，25%优于基线实现，平均减少57.9%的编码时间。

Conclusion: ResearchCodeAgent为自动化研究实现提供了重要进展，有望加速机器学习研究进程。

Abstract: In this paper we introduce ResearchCodeAgent, a novel multi-agent system
leveraging large language models (LLMs) agents to automate the codification of
research methodologies described in machine learning literature. The system
bridges the gap between high-level research concepts and their practical
implementation, allowing researchers auto-generating code of existing research
papers for benchmarking or building on top-of existing methods specified in the
literature with availability of partial or complete starter code.
ResearchCodeAgent employs a flexible agent architecture with a comprehensive
action suite, enabling context-aware interactions with the research
environment. The system incorporates a dynamic planning mechanism, utilizing
both short and long-term memory to adapt its approach iteratively. We evaluate
ResearchCodeAgent on three distinct machine learning tasks with distinct task
complexity and representing different parts of the ML pipeline: data
augmentation, optimization, and data batching. Our results demonstrate the
system's effectiveness and generalizability, with 46.9% of generated code being
high-quality and error-free, and 25% showing performance improvements over
baseline implementations. Empirical analysis shows an average reduction of
57.9% in coding time compared to manual implementation. We observe higher gains
for more complex tasks. ResearchCodeAgent represents a significant step towards
automating the research implementation process, potentially accelerating the
pace of machine learning research.

</details>

### [261] [AutoP2C: An LLM-Based Agent Framework for Code Repository Generation from Multimodal Content in Academic Papers](https://arxiv.org/abs/2504.20115)
*Zijie Lin,Yiqing Shen,Qilin Cai,He Sun,Jinrui Zhou,Mingjun Xiao*

Main category: cs.SE

TLDR: 论文提出了一种名为“Paper-to-Code”（P2C）的新任务，旨在将科学论文中的多模态内容转化为可执行的代码仓库。为此，作者开发了AutoP2C框架，通过多阶段处理实现自动化代码生成，并在实验中展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前，将科学论文中的多模态内容（如文本、图表等）转化为可执行代码需要大量ML专业知识且耗时。P2C任务旨在解决这一问题，扩展了现有代码生成的范畴。

Method: AutoP2C是一个基于大语言模型的多智能体框架，包含四个阶段：1) 从现有代码库提取仓库蓝图；2) 解析文本、公式和图表的多模态内容；3) 分层任务分解生成结构化代码；4) 迭代反馈调试以确保功能。

Result: 在八篇研究论文的基准测试中，AutoP2C成功生成了所有论文的可执行代码仓库，而OpenAI-o1和DeepSeek-R1仅能为一篇论文生成可运行代码。

Conclusion: AutoP2C框架在P2C任务中表现出色，能够高效地将论文内容转化为可执行代码，为ML研究提供了实用工具。

Abstract: Machine Learning (ML) research is spread through academic papers featuring
rich multimodal content, including text, diagrams, and tabular results.
However, translating these multimodal elements into executable code remains a
challenging and time-consuming process that requires substantial ML expertise.
We introduce ``Paper-to-Code'' (P2C), a novel task that transforms the
multimodal content of scientific publications into fully executable code
repositories, which extends beyond the existing formulation of code generation
that merely converts textual descriptions into isolated code snippets. To
automate the P2C process, we propose AutoP2C, a multi-agent framework based on
large language models that processes both textual and visual content from
research papers to generate complete code repositories. Specifically, AutoP2C
contains four stages: (1) repository blueprint extraction from established
codebases, (2) multimodal content parsing that integrates information from
text, equations, and figures, (3) hierarchical task decomposition for
structured code generation, and (4) iterative feedback-driven debugging to
ensure functionality and performance. Evaluation on a benchmark of eight
research papers demonstrates the effectiveness of AutoP2C, which can
successfully generate executable code repositories for all eight papers, while
OpenAI-o1 or DeepSeek-R1 can only produce runnable code for one paper. The code
is available at https://github.com/shoushouyu/Automated-Paper-to-Code.

</details>

### [262] [BLADE: Benchmark suite for LLM-driven Automated Design and Evolution of iterative optimisation heuristics](https://arxiv.org/abs/2504.20183)
*Niki van Stein,Anna V. Kononova,Haoran Yin,Thomas Bäck*

Main category: cs.SE

TLDR: BLADE是一个模块化、可扩展的框架，用于评估LLM驱动的自动化算法设计方法，提供标准化测试和比较工具。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的自动化算法设计方法缺乏标准化评估，BLADE旨在填补这一空白。

Method: BLADE整合了多种基准问题、实例生成器和文本描述，支持灵活的测试和分析工具。

Result: BLADE通过两个用例展示了其在评估LLM驱动方法中的实用性。

Conclusion: BLADE为LLM驱动的自动化算法设计提供了系统化的评估解决方案。

Abstract: The application of Large Language Models (LLMs) for Automated Algorithm
Discovery (AAD), particularly for optimisation heuristics, is an emerging field
of research. This emergence necessitates robust, standardised benchmarking
practices to rigorously evaluate the capabilities and limitations of LLM-driven
AAD methods and the resulting generated algorithms, especially given the
opacity of their design process and known issues with existing benchmarks. To
address this need, we introduce BLADE (Benchmark suite for LLM-driven Automated
Design and Evolution), a modular and extensible framework specifically designed
for benchmarking LLM-driven AAD methods in a continuous black-box optimisation
context. BLADE integrates collections of benchmark problems (including MA-BBOB
and SBOX-COST among others) with instance generators and textual descriptions
aimed at capability-focused testing, such as generalisation, specialisation and
information exploitation. It offers flexible experimental setup options,
standardised logging for reproducibility and fair comparison, incorporates
methods for analysing the AAD process (e.g., Code Evolution Graphs and various
visualisation approaches) and facilitates comparison against human-designed
baselines through integration with established tools like IOHanalyser and
IOHexplainer. BLADE provides an `out-of-the-box' solution to systematically
evaluate LLM-driven AAD approaches. The framework is demonstrated through two
distinct use cases exploring mutation prompt strategies and function
specialisation.

</details>

### [263] [Prompting LLMs for Code Editing: Struggles and Remedies](https://arxiv.org/abs/2504.20196)
*Daye Nam,Ahmed Omran,Ambar Murillo,Saksham Thakur,Abner Araujo,Marcel Blistein,Alexander Frömmgen,Vincent Hellendoorn,Satish Chandra*

Main category: cs.SE

TLDR: 论文研究了开发者如何与IDE中的LLM代码编辑工具交互，发现频繁重新提示是使用困难的标志，并提出自动优化提示的工具AutoPrompter，提升编辑正确性27%。


<details>
  <summary>Details</summary>
Motivation: 理解开发者在实际工作流程中如何使用LLM代码编辑工具及其遇到的困难。

Method: 通过分析IDE中的使用日志和定性研究，识别开发者提示中的缺失信息，并开发自动优化提示的工具。

Result: 发现频繁重新提示是使用困难的标志，提出AutoPrompter工具，提升编辑正确性27%。

Conclusion: 优化开发者提示能显著提升LLM代码编辑工具的使用效果。

Abstract: Large Language Models (LLMs) are rapidly transforming software engineering,
with coding assistants embedded in an IDE becoming increasingly prevalent.
While research has focused on improving the tools and understanding developer
perceptions, a critical gap exists in understanding how developers actually use
these tools in their daily workflows, and, crucially, where they struggle. This
paper addresses part of this gap through a multi-phased investigation of
developer interactions with an LLM-powered code editing and transformation
feature, Transform Code, in an IDE widely used at Google. First, we analyze
telemetry logs of the feature usage, revealing that frequent re-prompting can
be an indicator of developer struggles with using Transform Code. Second, we
conduct a qualitative analysis of unsatisfactory requests, identifying five key
categories of information often missing from developer prompts. Finally, based
on these findings, we propose and evaluate a tool, AutoPrompter, for
automatically improving prompts by inferring missing information from the
surrounding code context, leading to a 27% improvement in edit correctness on
our test set.

</details>

### [264] [Automated Unit Test Case Generation: A Systematic Literature Review](https://arxiv.org/abs/2504.20357)
*Jason Wang,Basem Suleiman,Muhammad Johan Alibasa*

Main category: cs.SE

TLDR: 本文综述了自动化软件测试中遗传算法和粒子群优化的改进及挑战，填补了现有研究的信息空白。


<details>
  <summary>Details</summary>
Motivation: 软件测试成本高且耗时，自动化测试成为研究热点，但遗传算法和粒子群优化的改进领域存在信息空白。

Method: 通过系统文献综述，整合进化方法的现有知识，包括混合算法组合、突变测试与神经网络的互操作性。

Result: 总结了改进方法及当前挑战，如可读性、模拟等问题。

Conclusion: 本文为自动化测试领域的研究提供了知识整合，并指出了未来研究方向。

Abstract: Software is omnipresent within all factors of society. It is thus important
to ensure that software are well tested to mitigate bad user experiences as
well as the potential for severe financial and human losses. Software testing
is however expensive and absorbs valuable time and resources. As a result, the
field of automated software testing has grown of interest to researchers in
past decades. In our review of present and past research papers, we have
identified an information gap in the areas of improvement for the Genetic
Algorithm and Particle Swarm Optimisation. A gap in knowledge in the current
challenges that face automated testing has also been identified. We therefore
present this systematic literature review in an effort to consolidate existing
knowledge in regards to the evolutionary approaches as well as their
improvements and resulting limitations. These improvements include hybrid
algorithm combinations as well as interoperability with mutation testing and
neural networks. We will also explore the main test criterion that are used in
these algorithms alongside the challenges currently faced in the field related
to readability, mocking and more.

</details>

### [265] [CrashFixer: A crash resolution agent for the Linux kernel](https://arxiv.org/abs/2504.20412)
*Alex Mathai,Chenxi Huang,Suwei Ma,Jihwan Kim,Hailie Mitchell,Aleksandr Nogikh,Petros Maniatis,Franjo Ivančić,Junfeng Yang,Baishakhi Ray*

Main category: cs.SE

TLDR: 论文介绍了CrashFixer，首个适用于Linux内核漏洞的LLM修复工具，改进了kGym平台为kGymSuite，并验证了其修复复杂内核漏洞的能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码LLM的基准测试局限于小规模场景，而Linux内核漏洞修复需要更强大的工具。

Method: 基于kGym平台改进为kGymSuite，设计CrashFixer工具，模拟开发者工作流程修复内核漏洞。

Result: CrashFixer在开放漏洞中生成至少两个可行的补丁建议，验证了其修复能力。

Conclusion: CrashFixer展示了LLM在复杂系统修复中的潜力，生成假设的步骤对修复至关重要。

Abstract: Code large language models (LLMs) have shown impressive capabilities on a
multitude of software engineering tasks. In particular, they have demonstrated
remarkable utility in the task of code repair. However, common benchmarks used
to evaluate the performance of code LLMs are often limited to small-scale
settings. In this work, we build upon kGym, which shares a benchmark for
system-level Linux kernel bugs and a platform to run experiments on the Linux
kernel.
  This paper introduces CrashFixer, the first LLM-based software repair agent
that is applicable to Linux kernel bugs. Inspired by the typical workflow of a
kernel developer, we identify the key capabilities an expert developer
leverages to resolve a kernel crash. Using this as our guide, we revisit the
kGym platform and identify key system improvements needed to practically run
LLM-based agents at the scale of the Linux kernel (50K files and 20M lines of
code). We implement these changes by extending kGym to create an improved
platform - called kGymSuite, which will be open-sourced. Finally, the paper
presents an evaluation of various repair strategies for such complex kernel
bugs and showcases the value of explicitly generating a hypothesis before
attempting to fix bugs in complex systems such as the Linux kernel. We also
evaluated CrashFixer's capabilities on still open bugs, and found at least two
patch suggestions considered plausible to resolve the reported bug.

</details>

### [266] [ARCS: Agentic Retrieval-Augmented Code Synthesis with Iterative Refinement](https://arxiv.org/abs/2504.20434)
*Manish Bhattarai,Miguel Cordova,Javier Santos,Dan O'Malley*

Main category: cs.SE

TLDR: ARCS框架结合检索增强生成与思维链推理，优化代码生成、完成与翻译，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在超级计算中，高效且优化的代码生成对充分利用高性能系统至关重要。

Method: ARCS整合检索增强生成（RAG）与思维链（CoT）推理，通过状态-动作搜索树优化平衡代码正确性与编辑效率。

Result: 在Geeks4Geeks和HumanEval基准测试中，ARCS在翻译和生成质量上显著优于传统提示方法。

Conclusion: ARCS为超级计算应用中的代码开发自动化和优化提供了变革性潜力，提升了计算资源利用率。

Abstract: In supercomputing, efficient and optimized code generation is essential to
leverage high-performance systems effectively. We propose Agentic
Retrieval-Augmented Code Synthesis (ARCS), an advanced framework for accurate,
robust, and efficient code generation, completion, and translation. ARCS
integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (CoT)
reasoning to systematically break down and iteratively refine complex
programming tasks. An agent-based RAG mechanism retrieves relevant code
snippets, while real-time execution feedback drives the synthesis of candidate
solutions. This process is formalized as a state-action search tree
optimization, balancing code correctness with editing efficiency. Evaluations
on the Geeks4Geeks and HumanEval benchmarks demonstrate that ARCS significantly
outperforms traditional prompting methods in translation and generation
quality. By enabling scalable and precise code synthesis, ARCS offers
transformative potential for automating and optimizing code development in
supercomputing applications, enhancing computational resource utilization.

</details>

### [267] [Enhancing Cell Counting through MLOps: A Structured Approach for Automated Cell Analysis](https://arxiv.org/abs/2504.20126)
*Matteo Testi,Luca Clissa,Matteo Ballabio,Salvatore Ricciardi,Federico Baldo,Emanuele Frontoni,Sara Moccia,Gennario Vessio*

Main category: cs.SE

TLDR: 论文提出了一种名为CC-MLOps的框架，用于优化机器学习在细胞计数工作流程中的集成，涵盖数据预处理、模型训练、监控等功能，并通过实际案例展示了其提升模型可靠性和可扩展性的能力。


<details>
  <summary>Details</summary>
Motivation: 机器学习在细胞计数应用中潜力巨大，但需要稳健的操作框架来有效实施。

Method: 引入CC-MLOps框架，整合数据预处理、模型训练、监控、可解释性和可持续性等功能。

Result: 通过实际案例证明，CC-MLOps能提升模型可靠性、减少人为错误，并支持可扩展的细胞计数解决方案。

Conclusion: 该研究为研究人员和实验室专业人员提供了实施机器学习驱动的细胞计数系统的实用指导。

Abstract: Machine Learning (ML) models offer significant potential for advancing cell
counting applications in neuroscience, medical research, pharmaceutical
development, and environmental monitoring. However, implementing these models
effectively requires robust operational frameworks. This paper introduces Cell
Counting Machine Learning Operations (CC-MLOps), a comprehensive framework that
streamlines the integration of ML in cell counting workflows. CC-MLOps
encompasses data access and preprocessing, model training, monitoring,
explainability features, and sustainability considerations. Through a practical
use case, we demonstrate how MLOps principles can enhance model reliability,
reduce human error, and enable scalable Cell Counting solutions. This work
provides actionable guidance for researchers and laboratory professionals
seeking to implement machine learning (ML)- powered cell counting systems.

</details>

### [268] [CoCo-Bench: A Comprehensive Code Benchmark For Multi-task Large Language Model Evaluation](https://arxiv.org/abs/2504.20673)
*Wenjing Yin,Tianze Sun,Yijiong Yu,Jiawei Fang,Guangyao Su,Jiancheng Wang,Zekun Wang,Wei Wang,Ran Chen,Ziyun Dai,Shuai Yuan,Menghang Dong,Peng Luo,Dong Cao,Da Lei,Yajun Zhang,Hao Chen,Xiang Ma,Yong Liu,Weifeng Liu,Yuanjian Xu,Ji Pei*

Main category: cs.SE

TLDR: CoCo-Bench是一个全面的代码基准测试，用于评估大语言模型在代码理解、生成、修改和审查四个关键维度的表现，填补了现有基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试范围狭窄，缺乏反映实际应用的综合评估框架，因此需要更系统、更具代表性的评估工具。

Method: 设计了CoCo-Bench，涵盖多种编程语言和任务难度，并通过严格的人工审查确保数据质量。

Result: 实验结果表明，CoCo-Bench与现有基准测试一致，同时揭示了模型性能的显著差异，有效突出了优缺点。

Conclusion: CoCo-Bench为代码导向的大语言模型提供了全面客观的评估，为未来研究和技术进步提供了可靠基准。

Abstract: Large language models (LLMs) play a crucial role in software engineering,
excelling in tasks like code generation and maintenance. However, existing
benchmarks are often narrow in scope, focusing on a specific task and lack a
comprehensive evaluation framework that reflects real-world applications. To
address these gaps, we introduce CoCo-Bench (Comprehensive Code Benchmark),
designed to evaluate LLMs across four critical dimensions: code understanding,
code generation, code modification, and code review. These dimensions capture
essential developer needs, ensuring a more systematic and representative
evaluation. CoCo-Bench includes multiple programming languages and varying task
difficulties, with rigorous manual review to ensure data quality and accuracy.
Empirical results show that CoCo-Bench aligns with existing benchmarks while
uncovering significant variations in model performance, effectively
highlighting strengths and weaknesses. By offering a holistic and objective
evaluation, CoCo-Bench provides valuable insights to guide future research and
technological advancements in code-oriented LLMs, establishing a reliable
benchmark for the field.

</details>

### [269] [Using LLMs in Generating Design Rationale for Software Architecture Decisions](https://arxiv.org/abs/2504.20781)
*Xiyu Zhou,Ruiyin Li,Peng Liang,Beiqi Zhang,Mojtaba Shahin,Zengyang Li,Chen Yang*

Main category: cs.SE

TLDR: 论文探讨了利用大语言模型（LLMs）生成软件架构设计理由（DR）的可行性，并评估了三种提示策略的性能。


<details>
  <summary>Details</summary>
Motivation: 实践中DR常因开发者缺乏动力和努力而未被充分记录，LLMs的文本理解和生成能力可能解决这一问题。

Method: 收集100个架构相关问题，使用五种LLMs和三种提示策略（零样本、思维链、LLM代理）生成DR，并与专家提供的DR对比评估。

Result: LLM生成的DR在精确度、召回率和F1分数上表现一般，但部分未提及的论点仍有用，少数论点可能误导。

Conclusion: LLMs在生成DR方面有潜力，但需权衡提示策略的优缺点，并注意其局限性。

Abstract: Design Rationale (DR) for software architecture decisions refers to the
reasoning underlying architectural choices, which provides valuable insights
into the different phases of the architecting process throughout software
development. However, in practice, DR is often inadequately documented due to a
lack of motivation and effort from developers. With the recent advancements in
Large Language Models (LLMs), their capabilities in text comprehension,
reasoning, and generation may enable the generation and recovery of DR for
architecture decisions. In this study, we evaluated the performance of LLMs in
generating DR for architecture decisions. First, we collected 50 Stack Overflow
(SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture
decisions to construct a dataset of 100 architecture-related problems. Then, we
selected five LLMs to generate DR for the architecture decisions with three
prompting strategies, including zero-shot, chain of thought (CoT), and
LLM-based agents. With the DR provided by human experts as ground truth, the
Precision of LLM-generated DR with the three prompting strategies ranges from
0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389.
Additionally, 64.45% to 69.42% of the arguments of DR not mentioned by human
experts are also helpful, 4.12% to 4.87% of the arguments have uncertain
correctness, and 1.59% to 3.24% of the arguments are potentially misleading.
Based on the results, we further discussed the pros and cons of the three
prompting strategies and the strengths and limitations of the DR generated by
LLMs.

</details>

### [270] [Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation, and Challenges](https://arxiv.org/abs/2504.20799)
*Yunseo Lee,John Youngeun Song,Dongsun Kim,Jindae Kim,Mijung Kim,Jaechang Nam*

Main category: cs.SE

TLDR: 本文调查了CodeLLMs生成的代码中的幻觉问题，分类了幻觉类型，回顾了现有基准和缓解策略，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于CodeLLMs在生成代码时容易产生难以识别的幻觉代码，这对软件开发尤其是低代码环境带来潜在风险。

Method: 通过分类幻觉类型、回顾现有基准和缓解策略，分析相关研究。

Result: 总结了CodeLLMs生成代码中的幻觉问题及其挑战，提出了未来研究方向。

Conclusion: 需要进一步研究以检测和消除CodeLLMs生成的幻觉代码。

Abstract: Recent technical breakthroughs in large language models (LLMs) have enabled
them to fluently generate source code. Software developers often leverage both
general-purpose and code-specialized LLMs to revise existing code or even
generate a whole function from scratch. These capabilities are also beneficial
in no-code or low-code contexts, in which one can write programs without a
technical background. However, due to their internal design, LLMs are prone to
generating hallucinations, which are incorrect, nonsensical, and not
justifiable information but difficult to identify its presence. This problem
also occurs when generating source code. Once hallucinated code is produced, it
is often challenging for users to identify and fix it, especially when such
hallucinations can be identified under specific execution paths. As a result,
the hallucinated code may remain unnoticed within the codebase. This survey
investigates recent studies and techniques relevant to hallucinations generated
by CodeLLMs. We categorize the types of hallucinations in the code generated by
CodeLLMs, review existing benchmarks and mitigation strategies, and identify
open challenges. Based on these findings, this survey outlines further research
directions in the detection and removal of hallucinations produced by CodeLLMs.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [271] [DRO: Doppler-Aware Direct Radar Odometry](https://arxiv.org/abs/2504.20339)
*Cedric Le Gentil,Leonardo Brizi,Daniil Lisus,Xinyuan Qiao,Giorgio Grisetti,Timothy D. Barfoot*

Main category: cs.RO

TLDR: 提出了一种基于SE(2)的雷达里程计方法，直接利用雷达强度信息进行扫描到局部地图的配准，无需特征提取，支持几何特征匮乏场景。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达能穿透薄墙、植被和恶劣天气，但现有方法在特征匮乏场景中表现不佳，需改进。

Method: 直接配准雷达强度信息，结合多普勒约束优化速度估计，支持实时实现。

Result: 在公开数据集上验证，相对平移误差低至0.18%，优于现有方法。

Conclusion: 该方法在多样环境中表现优异，代码已开源。

Abstract: A renaissance in radar-based sensing for mobile robotic applications is
underway. Compared to cameras or lidars, millimetre-wave radars have the
ability to `see' through thin walls, vegetation, and adversarial weather
conditions such as heavy rain, fog, snow, and dust. In this paper, we propose a
novel SE(2) odometry approach for spinning frequency-modulated continuous-wave
radars. Our method performs scan-to-local-map registration of the incoming
radar data in a direct manner using all the radar intensity information without
the need for feature or point cloud extraction. The method performs locally
continuous trajectory estimation and accounts for both motion and Doppler
distortion of the radar scans. If the radar possesses a specific frequency
modulation pattern that makes radial Doppler velocities observable, an
additional Doppler-based constraint is formulated to improve the velocity
estimate and enable odometry in geometrically feature-deprived scenarios (e.g.,
featureless tunnels). Our method has been validated on over 250km of on-road
data sourced from public datasets (Boreas and MulRan) and collected using our
automotive platform. With the aid of a gyroscope, it outperforms
state-of-the-art methods and achieves an average relative translation error of
0.26% on the Boreas leaderboard. When using data with the appropriate
Doppler-enabling frequency modulation pattern, the translation error is reduced
to 0.18% in similar environments. We also benchmarked our algorithm using 1.5
hours of data collected with a mobile robot in off-road environments with
various levels of structure to demonstrate its versatility. Our real-time
implementation is publicly available: https://github.com/utiasASRL/dro.

</details>

### [272] [Hydra: Marker-Free RGB-D Hand-Eye Calibration](https://arxiv.org/abs/2504.20584)
*Martin Huber,Huanyu Tian,Christopher E. Mower,Lucas-Raphael Müller,Sébastien Ourselin,Christos Bergeles,Tom Vercauteren*

Main category: cs.RO

TLDR: 提出了一种基于RGB-D成像的无标记手眼标定方法，采用改进的ICP算法和鲁棒的点到平面目标函数，实验表明其收敛速度更快、精度更高。


<details>
  <summary>Details</summary>
Motivation: 解决传统手眼标定方法依赖标记物的问题，提高标定效率和精度。

Method: 使用改进的ICP算法和点到平面目标函数，结合Lie代数进行优化。

Result: 在仅需三个随机机器人配置的情况下，成功率达到90%，收敛速度比基线方法快2-3倍，精度提升至5毫米。

Conclusion: 该方法在无标记条件下显著提升了手眼标定的效率和精度，适用于实际部署。

Abstract: This work presents an RGB-D imaging-based approach to marker-free hand-eye
calibration using a novel implementation of the iterative closest point (ICP)
algorithm with a robust point-to-plane (PTP) objective formulated on a Lie
algebra. Its applicability is demonstrated through comprehensive experiments
using three well known serial manipulators and two RGB-D cameras. With only
three randomly chosen robot configurations, our approach achieves approximately
90% successful calibrations, demonstrating 2-3x higher convergence rates to the
global optimum compared to both marker-based and marker-free baselines. We also
report 2 orders of magnitude faster convergence time (0.8 +/- 0.4 s) for 9
robot configurations over other marker-free methods. Our method exhibits
significantly improved accuracy (5 mm in task space) over classical approaches
(7 mm in task space) whilst being marker-free. The benchmarking dataset and
code are open sourced under Apache 2.0 License, and a ROS 2 integration with
robot abstraction is provided to facilitate deployment.

</details>

### [273] [Learning a General Model: Folding Clothing with Topological Dynamics](https://arxiv.org/abs/2504.20720)
*Yiming Liu,Lijun Han,Enlin Gu,Hesheng Wang*

Main category: cs.RO

TLDR: 提出了一种基于拓扑动力学模型的衣物折叠方法，利用拓扑图表示衣物状态，并结合改进的图神经网络预测衣物变形。


<details>
  <summary>Details</summary>
Motivation: 衣物自由度多、结构复杂，传统方法难以处理其折叠问题。

Method: 通过语义分割和关键点检测生成拓扑图，利用改进的图神经网络学习衣物动力学。

Result: 实验证明该方法能有效识别和折叠具有自遮挡的复杂衣物。

Conclusion: 拓扑动力学模型为复杂衣物折叠提供了通用解决方案。

Abstract: The high degrees of freedom and complex structure of garments present
significant challenges for clothing manipulation. In this paper, we propose a
general topological dynamics model to fold complex clothing. By utilizing the
visible folding structure as the topological skeleton, we design a novel
topological graph to represent the clothing state. This topological graph is
low-dimensional and applied for complex clothing in various folding states. It
indicates the constraints of clothing and enables predictions regarding
clothing movement. To extract graphs from self-occlusion, we apply semantic
segmentation to analyze the occlusion relationships and decompose the clothing
structure. The decomposed structure is then combined with keypoint detection to
generate the topological graph. To analyze the behavior of the topological
graph, we employ an improved Graph Neural Network (GNN) to learn the general
dynamics. The GNN model can predict the deformation of clothing and is employed
to calculate the deformation Jacobi matrix for control. Experiments using
jackets validate the algorithm's effectiveness to recognize and fold complex
clothing with self-occlusion.

</details>

### [274] [A Survey on Event-based Optical Marker Systems](https://arxiv.org/abs/2504.20736)
*Nafiseh Jabbari Tofighi,Maxime Robic,Fabio Morbidi,Pascal Vasseur*

Main category: cs.RO

TLDR: 本文综述了基于事件的视觉标记系统（EBOMS），分析了其异步操作和对恶劣光照条件的鲁棒性，并探讨了其应用及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 事件相机的低延迟、高动态范围和低功耗特性为机器人视觉和机器感知带来了变革，结合光学标记（如AprilTags、LED阵列）为研究开辟了新方向。

Method: 综述了EBOMS的基本原理和技术，重点分析其异步操作和光照鲁棒性，并描述相关应用（如目标检测、位姿估计）。

Result: EBOMS在目标检测、跟踪、位姿估计和光通信等领域展现出潜力。

Conclusion: EBOMS是一个快速发展的多学科领域，未来研究可进一步探索其应用和技术优化。

Abstract: The advent of event-based cameras, with their low latency, high dynamic
range, and reduced power consumption, marked a significant change in robotic
vision and machine perception. In particular, the combination of these
neuromorphic sensors with widely-available passive or active optical markers
(e.g. AprilTags, arrays of blinking LEDs), has recently opened up a wide field
of possibilities. This survey paper provides a comprehensive review on
Event-Based Optical Marker Systems (EBOMS). We analyze the basic principles and
technologies on which these systems are based, with a special focus on their
asynchronous operation and robustness against adverse lighting conditions. We
also describe the most relevant applications of EBOMS, including object
detection and tracking, pose estimation, and optical communication. The article
concludes with a discussion of possible future research directions in this
rapidly-emerging and multidisciplinary field.

</details>

### [275] [PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations](https://arxiv.org/abs/2504.20520)
*Haowen Sun,Han Wang,Chengzhong Ma,Shaolong Zhang,Jiawei Ye,Xingyu Chen,Xuguang Lan*

Main category: cs.RO

TLDR: 提出了一种结合真实到模拟再到真实（real-to-sim-to-real）的流程，通过专家演示构建仿真环境，并利用视觉语言模型（VLM）监督的投影奖励模型训练RL策略，最终实现可靠的机器人控制策略。


<details>
  <summary>Details</summary>
Motivation: 解决机器人初始位置和物体姿态变化下的策略鲁棒性问题，避免直接交互的不安全性和仿真环境构建的高成本。

Method: 基于专家演示构建仿真环境，利用VLM监督的投影奖励模型训练RL策略，并通过专家演示微调策略。

Result: 实现了可靠的机器人控制策略，适用于真实场景。

Conclusion: 提出的集成流程有效解决了仿真环境构建和策略训练的挑战，为机器人控制提供了实用解决方案。

Abstract: Learning from few demonstrations to develop policies robust to variations in
robot initial positions and object poses is a problem of significant practical
interest in robotics. Compared to imitation learning, which often struggles to
generalize from limited samples, reinforcement learning (RL) can autonomously
explore to obtain robust behaviors. Training RL agents through direct
interaction with the real world is often impractical and unsafe, while building
simulation environments requires extensive manual effort, such as designing
scenes and crafting task-specific reward functions. To address these
challenges, we propose an integrated real-to-sim-to-real pipeline that
constructs simulation environments based on expert demonstrations by
identifying scene objects from images and retrieving their corresponding 3D
models from existing libraries. We introduce a projection-based reward model
for RL policy training that is supervised by a vision-language model (VLM)
using human-guided object projection relationships as prompts, with the policy
further fine-tuned using expert demonstrations. In general, our work focuses on
the construction of simulation environments and RL-based policy training,
ultimately enabling the deployment of reliable robotic control policies in
real-world scenarios.

</details>

### [276] [SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings](https://arxiv.org/abs/2504.20808)
*Florian Vahl,Jörn Griepenburg,Jan Gutsche,Jasper Güldenstein,Jianwei Zhang*

Main category: cs.RO

TLDR: SoccerDiffusion是一种基于Transformer的扩散模型，用于从真实比赛录像中学习人形机器人足球的端到端控制策略。


<details>
  <summary>Details</summary>
Motivation: 通过从RoboCup比赛中收集的数据，直接学习复杂的运动行为，如行走、踢球和跌倒恢复。

Method: 使用多模态传感器输入（视觉、本体感觉和游戏状态）预测关节命令轨迹，并通过蒸馏技术实现嵌入式平台上的实时推理。

Result: 模型能够在仿真和物理机器人上复现复杂运动行为，但高级战术行为仍有局限。

Conclusion: 为后续强化学习或偏好优化方法提供了坚实基础，并公开了数据集、预训练模型和代码。

Abstract: This paper introduces SoccerDiffusion, a transformer-based diffusion model
designed to learn end-to-end control policies for humanoid robot soccer
directly from real-world gameplay recordings. Using data collected from RoboCup
competitions, the model predicts joint command trajectories from multi-modal
sensor inputs, including vision, proprioception, and game state. We employ a
distillation technique to enable real-time inference on embedded platforms that
reduces the multi-step diffusion process to a single step. Our results
demonstrate the model's ability to replicate complex motion behaviors such as
walking, kicking, and fall recovery both in simulation and on physical robots.
Although high-level tactical behavior remains limited, this work provides a
robust foundation for subsequent reinforcement learning or preference
optimization methods. We release the dataset, pretrained models, and code
under: https://bit-bots.github.io/SoccerDiffusion

</details>

### [277] [XPG-RL: Reinforcement Learning with Explainable Priority Guidance for Efficiency-Boosted Mechanical Search](https://arxiv.org/abs/2504.20969)
*Yiting Zhang,Shichen Li,Elena Shrestha*

Main category: cs.RO

TLDR: XPG-RL是一种基于强化学习的框架，通过可解释的优先级决策和原始感官输入，提高机械搜索任务的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决机械搜索在杂乱环境中因遮挡和部分可观测性带来的长时规划和状态估计挑战。

Method: 结合任务驱动的动作优先级机制和上下文感知切换策略，动态选择动作原语（如抓取、遮挡移除和视角调整），并优化策略以输出自适应阈值。

Result: 在仿真和实际环境中，XPG-RL在任务成功率和运动效率上显著优于基线方法，长时任务效率提升高达4.5倍。

Conclusion: 将领域知识与可学习决策策略结合，能实现高效且鲁棒的机器人操作。

Abstract: Mechanical search (MS) in cluttered environments remains a significant
challenge for autonomous manipulators, requiring long-horizon planning and
robust state estimation under occlusions and partial observability. In this
work, we introduce XPG-RL, a reinforcement learning framework that enables
agents to efficiently perform MS tasks through explainable, priority-guided
decision-making based on raw sensory inputs. XPG-RL integrates a task-driven
action prioritization mechanism with a learned context-aware switching strategy
that dynamically selects from a discrete set of action primitives such as
target grasping, occlusion removal, and viewpoint adjustment. Within this
strategy, a policy is optimized to output adaptive threshold values that govern
the discrete selection among action primitives. The perception module fuses
RGB-D inputs with semantic and geometric features to produce a structured scene
representation for downstream decision-making. Extensive experiments in both
simulation and real-world settings demonstrate that XPG-RL consistently
outperforms baseline methods in task success rates and motion efficiency,
achieving up to 4.5$\times$ higher efficiency in long-horizon tasks. These
results underscore the benefits of integrating domain knowledge with learnable
decision-making policies for robust and efficient robotic manipulation.

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [278] [Generate more than one child in your co-evolutionary semi-supervised learning GAN](https://arxiv.org/abs/2504.20560)
*Francisco Sedeño,Jamal Toutouh,Francisco Chicano*

Main category: cs.NE

TLDR: 本文提出了一种新的协同进化方法CE-SSLGAN，通过改进种群结构和替换策略，提升了SSL-GAN的性能。


<details>
  <summary>Details</summary>
Motivation: 现有协同进化SSL-GAN方法基于空间结构种群和单一个体生成策略，限制了性能提升。

Method: 提出CE-SSLGAN，采用泛种群结构、精英替换策略和多子代生成。

Result: 在三个标准数据集上验证，CE-SSLGAN性能优于传统SSL-GAN。

Conclusion: 多子代和精英替换策略能有效提升SSL-GAN性能。

Abstract: Generative Adversarial Networks (GANs) are very useful methods to address
semi-supervised learning (SSL) datasets, thanks to their ability to generate
samples similar to real data. This approach, called SSL-GAN has attracted many
researchers in the last decade. Evolutionary algorithms have been used to guide
the evolution and training of SSL-GANs with great success. In particular,
several co-evolutionary approaches have been applied where the two networks of
a GAN (the generator and the discriminator) are evolved in separate
populations. The co-evolutionary approaches published to date assume some
spatial structure of the populations, based on the ideas of cellular
evolutionary algorithms. They also create one single individual per generation
and follow a generational replacement strategy in the evolution. In this paper,
we re-consider those algorithmic design decisions and propose a new
co-evolutionary approach, called Co-evolutionary Elitist SSL-GAN (CE-SSLGAN),
with panmictic population, elitist replacement, and more than one individual in
the offspring. We evaluate the performance of our proposed method using three
standard benchmark datasets. The results show that creating more than one
offspring per population and using elitism improves the results in comparison
with a classical SSL-GAN.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [279] [Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting](https://arxiv.org/abs/2504.20403)
*Hanxi Liu,Yifang Men,Zhouhui Lian*

Main category: cs.GR

TLDR: 提出了一种基于Tetrahedron-constrained Gaussian Splatting（TetGS）的框架，用于生成可编辑的3D虚拟化身，具有精确区域定位、几何适应性和逼真渲染。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D编辑方法在复杂重建场景下几何与纹理混合优化时表现不稳定的问题，为普通用户提供易于操作的3D虚拟化身编辑方案。

Method: 采用TetGS作为底层表示，分三个阶段优化：3D虚拟化身实例化、局部空间适应和基于几何的外观生成。

Result: 定性和定量实验表明，该方法在生成逼真可编辑3D虚拟化身方面具有优越性。

Conclusion: 提出的框架有效解决了3D虚拟化身编辑中的视觉质量和稳定性问题，适用于AR/VR等应用。

Abstract: Personalized 3D avatar editing holds significant promise due to its
user-friendliness and availability to applications such as AR/VR and virtual
try-ons. Previous studies have explored the feasibility of 3D editing, but
often struggle to generate visually pleasing results, possibly due to the
unstable representation learning under mixed optimization of geometry and
texture in complicated reconstructed scenarios. In this paper, we aim to
provide an accessible solution for ordinary users to create their editable 3D
avatars with precise region localization, geometric adaptability, and
photorealistic renderings. To tackle this challenge, we introduce a
meticulously designed framework that decouples the editing process into local
spatial adaptation and realistic appearance learning, utilizing a hybrid
Tetrahedron-constrained Gaussian Splatting (TetGS) as the underlying
representation. TetGS combines the controllable explicit structure of
tetrahedral grids with the high-precision rendering capabilities of 3D Gaussian
Splatting and is optimized in a progressive manner comprising three stages: 3D
avatar instantiation from real-world monocular videos to provide accurate
priors for TetGS initialization; localized spatial adaptation with explicitly
partitioned tetrahedrons to guide the redistribution of Gaussian kernels; and
geometry-based appearance generation with a coarse-to-fine activation strategy.
Both qualitative and quantitative experiments demonstrate the effectiveness and
superiority of our approach in generating photorealistic 3D editable avatars.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [280] [Coreset selection for the Sinkhorn divergence and generic smooth divergences](https://arxiv.org/abs/2504.20194)
*Alex Kokot,Alex Luedtke*

Main category: stat.ML

TLDR: CO2算法通过功能泰勒展开和最大均值差异最小化，高效生成凸加权核心集，应用于Sinkhorn散度，提供对数级数据点需求的新采样方法。


<details>
  <summary>Details</summary>
Motivation: 解决核心集选择问题，提供更高效的数据采样方法，并探索其与经典统计方法的联系。

Method: 利用功能泰勒展开将损失函数局部等价于其二阶近似，转化为最大均值差异最小化问题，应用于Sinkhorn散度。

Result: 提出对数级数据点需求的新采样方法，验证了熵正则化最优传输的新规律性。

Conclusion: CO2算法为数据子采样提供了新视角，并指出了提升算法效率和理论保证的方向。

Abstract: We introduce CO2, an efficient algorithm to produce convexly-weighted
coresets with respect to generic smooth divergences. By employing a functional
Taylor expansion, we show a local equivalence between sufficiently regular
losses and their second order approximations, reducing the coreset selection
problem to maximum mean discrepancy minimization. We apply CO2 to the Sinkhorn
divergence, providing a novel sampling procedure that requires logarithmically
many data points to match the approximation guarantees of random sampling. To
show this, we additionally verify several new regularity properties for
entropically regularized optimal transport of independent interest. Our
approach leads to a new perspective linking coreset selection and kernel
quadrature to classical statistical methods such as moment and score matching.
We showcase this method with a practical application of subsampling image data,
and highlight key directions to explore for improved algorithmic efficiency and
theoretical guarantees.

</details>

### [281] [Sobolev norm inconsistency of kernel interpolation](https://arxiv.org/abs/2504.20617)
*Yunfei Yang*

Main category: stat.ML

TLDR: 论文研究了在再生核希尔伯特空间中最小范数插值的一致性，发现核插值在特定条件下总是不一致的。


<details>
  <summary>Details</summary>
Motivation: 探讨核插值在再生核希尔伯特空间中的一致性，尤其是在不同范数下的泛化误差。

Method: 通过分析核插值在连续范数尺度下的泛化误差下界，研究了其一致性。

Result: 结果表明，当范数的平滑指数大于一个依赖于假设空间嵌入指数和特征值衰减率的常数时，核插值总是不一致的。

Conclusion: 核插值在特定条件下无法保证一致性，这为相关研究提供了理论边界。

Abstract: We study the consistency of minimum-norm interpolation in reproducing kernel
Hilbert spaces corresponding to bounded kernels. Our main result give lower
bounds for the generalization error of the kernel interpolation measured in a
continuous scale of norms that interpolate between $L^2$ and the hypothesis
space. These lower bounds imply that kernel interpolation is always
inconsistent, when the smoothness index of the norm is larger than a constant
that depends only on the embedding index of the hypothesis space and the decay
rate of the eigenvalues.

</details>

### [282] [Learning and Generalization with Mixture Data](https://arxiv.org/abs/2504.20651)
*Harsh Vardhan,Avishek Ghosh,Arya Mazumdar*

Main category: stat.ML

TLDR: 本文研究了从混合分布中采样的数据的泛化性能和统计速率，分析了混合数据的异质性对学习的影响，并提供了泛化和收敛速率的理论界限。


<details>
  <summary>Details</summary>
Motivation: 现代大规模学习中数据异质性是一个主要挑战，本文旨在研究混合分布数据的泛化性能和统计速率，以确定混合数据何时可被视为单一分布进行学习。

Method: 通过表征混合分布的异质性（基于子分布间的总变差距离），并在经典PAC框架下分析泛化性能，同时针对参数和非参数回归问题提供统计误差率。使用Rademacher复杂度和局部高斯复杂度界限推导泛化和收敛速率。

Result: 研究发现，随着函数类复杂度增加，对总变差距离的要求更严格。对于混合线性回归，提供了泛化误差的紧致界限。

Conclusion: 混合数据的异质性对学习性能有显著影响，复杂度高的函数类对异质性更敏感。研究结果为混合数据学习提供了理论支持。

Abstract: In many, if not most, machine learning applications the training data is
naturally heterogeneous (e.g. federated learning, adversarial attacks and
domain adaptation in neural net training). Data heterogeneity is identified as
one of the major challenges in modern day large-scale learning. A classical way
to represent heterogeneous data is via a mixture model. In this paper, we study
generalization performance and statistical rates when data is sampled from a
mixture distribution. We first characterize the heterogeneity of the mixture in
terms of the pairwise total variation distance of the sub-population
distributions. Thereafter, as a central theme of this paper, we characterize
the range where the mixture may be treated as a single (homogeneous)
distribution for learning. In particular, we study the generalization
performance under the classical PAC framework and the statistical error rates
for parametric (linear regression, mixture of hyperplanes) as well as
non-parametric (Lipschitz, convex and H\"older-smooth) regression problems. In
order to do this, we obtain Rademacher complexity and (local) Gaussian
complexity bounds with mixture data, and apply them to get the generalization
and convergence rates respectively. We observe that as the (regression)
function classes get more complex, the requirement on the pairwise total
variation distance gets stringent, which matches our intuition. We also do a
finer analysis for the case of mixed linear regression and provide a tight
bound on the generalization error in terms of heterogeneity.

</details>

### [283] [Preference-centric Bandits: Optimality of Mixtures and Regret-efficient Algorithms](https://arxiv.org/abs/2504.20877)
*Meltem Tatlı,Arpan Mukherjee,Prashanth L. A.,Karthikeyan Shanmugam,Ali Tajer*

Main category: stat.ML

TLDR: 论文提出了一种基于偏好度量（PM）的多臂老虎机框架，替代传统的期望值评估，以更灵活地建模风险偏好和不确定性态度。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机仅关注奖励的期望值，忽略了分布尾部行为和风险，无法满足复杂决策需求。

Method: 引入PM框架，设计了两类算法（horizon-dependent和anytime），通过估计最优混合策略和跟踪机制实现高效学习。

Result: 算法在多种PM代数形式下具有遗憾保证，能够收敛到基于特定混合权重的策略。

Conclusion: PM框架为多臂老虎机提供了更丰富的偏好建模能力，算法设计原则与传统方法显著不同。

Abstract: The objective of canonical multi-armed bandits is to identify and repeatedly
select an arm with the largest reward, often in the form of the expected value
of the arm's probability distribution. Such a utilitarian perspective and focus
on the probability models' first moments, however, is agnostic to the
distributions' tail behavior and their implications for variability and risks
in decision-making. This paper introduces a principled framework for shifting
from expectation-based evaluation to an alternative reward formulation, termed
a preference metric (PM). The PMs can place the desired emphasis on different
reward realization and can encode a richer modeling of preferences that
incorporate risk aversion, robustness, or other desired attitudes toward
uncertainty. A fundamentally distinct observation in such a PM-centric
perspective is that designing bandit algorithms will have a significantly
different principle: as opposed to the reward-based models in which the optimal
sampling policy converges to repeatedly sampling from the single best arm, in
the PM-centric framework the optimal policy converges to selecting a mix of
arms based on specific mixing weights. Designing such mixture policies departs
from the principles for designing bandit algorithms in significant ways,
primarily because of uncountable mixture possibilities. The paper formalizes
the PM-centric framework and presents two algorithm classes (horizon-dependent
and anytime) that learn and track mixtures in a regret-efficient fashion. These
algorithms have two distinctions from their canonical counterparts: (i) they
involve an estimation routine to form reliable estimates of optimal mixtures,
and (ii) they are equipped with tracking mechanisms to navigate arm selection
fractions to track the optimal mixtures. These algorithms' regret guarantees
are investigated under various algebraic forms of the PMs.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [284] [HCT-QA: A Benchmark for Question Answering on Human-Centric Tables](https://arxiv.org/abs/2504.20047)
*Mohammad S. Ahmad,Zan A. Naeem,Michaël Aupetit,Ahmed Elmagarmid,Mohamed Eltabakh,Xiasong Ma,Mourad Ouzzani,Chaoyi Ruan*

Main category: cs.IR

TLDR: 论文提出了HCT-QA基准，用于评估大语言模型处理复杂表格数据的能力。


<details>
  <summary>Details</summary>
Motivation: 人类中心表格（HCTs）具有高商业价值但复杂布局，传统方法难以处理。

Method: 构建包含真实和合成表格的HCT-QA数据集，并评估大语言模型的查询能力。

Result: 数据集包含2,188真实表格和4,679合成表格，共77.3K QA对。

Conclusion: HCT-QA为复杂表格查询提供了基准，大语言模型展现出潜力。

Abstract: Tabular data embedded within PDF files, web pages, and other document formats
are prevalent across numerous sectors such as government, engineering, science,
and business. These human-centric tables (HCTs) possess a unique combination of
high business value, intricate layouts, limited operational power at scale, and
sometimes serve as the only data source for critical insights. However, their
complexity poses significant challenges to traditional data extraction,
processing, and querying methods. While current solutions focus on transforming
these tables into relational formats for SQL queries, they fall short in
handling the diverse and complex layouts of HCTs and hence being amenable to
querying. This paper describes HCT-QA, an extensive benchmark of HCTs, natural
language queries, and related answers on thousands of tables. Our dataset
includes 2,188 real-world HCTs with 9,835 QA pairs and 4,679 synthetic tables
with 67.5K QA pairs. While HCTs can be potentially processed by different type
of query engines, in this paper, we focus on Large Language Models as potential
engines and assess their ability in processing and querying such tables.

</details>

### [285] [Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence](https://arxiv.org/abs/2504.20059)
*Joey Chan,Qiao Jin,Nicholas Wan,Charalampos S. Floudas,Elisabetta Xue,Zhiyong Lu*

Main category: cs.IR

TLDR: TrialGPT利用大型语言模型匹配患者与临床试验，比传统关键词搜索方法表现优46%，平均每位患者匹配7个试验。


<details>
  <summary>Details</summary>
Motivation: 临床试验招募面临挑战，如患者认知不足和复杂资格标准，而在线平台为扩大招募提供了新途径。

Method: 使用TrialGPT框架，基于大型语言模型，匹配50个在线患者案例与临床试验，并与传统关键词搜索对比。

Result: TrialGPT表现优于传统方法46%，平均每位患者匹配7个试验，且获得患者和试验组织者的积极反馈。

Conclusion: TrialGPT展示了利用AI提升临床试验招募效率的潜力，未来可进一步优化和应用。

Abstract: Clinical trials are crucial for assessing new treatments; however,
recruitment challenges - such as limited awareness, complex eligibility
criteria, and referral barriers - hinder their success. With the growth of
online platforms, patients increasingly turn to social media and health
communities for support, research, and advocacy, expanding recruitment pools
and established enrollment pathways. Recognizing this potential, we utilized
TrialGPT, a framework that leverages a large language model (LLM) as its
backbone, to match 50 online patient cases (collected from published case
reports and a social media website) to clinical trials and evaluate performance
against traditional keyword-based searches. Our results show that TrialGPT
outperforms traditional methods by 46% in identifying eligible trials, with
each patient, on average, being eligible for around 7 trials. Additionally, our
outreach efforts to case authors and trial organizers regarding these
patient-trial matches yielded highly positive feedback, which we present from
both perspectives.

</details>

### [286] [A model and package for German ColBERT](https://arxiv.org/abs/2504.20083)
*Thuong Dang,Qiqi Chen*

Main category: cs.IR

TLDR: 本文介绍了ColBERT的德语版本，专注于RAG应用，并展示了支持检索和微调工作流的ColBERT模型包的主要功能。


<details>
  <summary>Details</summary>
Motivation: 为德语用户提供ColBERT的本地化版本，并支持RAG应用的需求。

Method: 开发了ColBERT的德语版本，并提供了支持检索和微调的模型包。

Result: 成功实现了德语版ColBERT，并提供了功能完善的模型包。

Conclusion: 该工作为德语用户提供了高效的检索工具，并支持灵活的微调功能。

Abstract: In this work, we introduce a German version for ColBERT, a late interaction
multi-dense vector retrieval method, with a focus on RAG applications. We also
present the main features of our package for ColBERT models, supporting both
retrieval and fine-tuning workflows.

</details>

### [287] [An Integrated Framework for Contextual Personalized LLM-Based Food Recommendation](https://arxiv.org/abs/2504.20092)
*Ali Rostami*

Main category: cs.IR

TLDR: 论文提出了一种针对食品推荐系统的个性化框架F-RLP，通过多媒体食品日志平台和世界食品地图解决数据不平衡和领域复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 传统食品推荐系统因组件理解不足和机器学习方法在食品数据上的局限性表现不佳，需要专门化的解决方案。

Method: 引入多媒体食品日志平台和世界食品地图，提出F-RLP框架，专门针对食品领域优化LLMs。

Result: F-RLP框架克服了通用模型的限制，提供了更有效、上下文感知的个性化食品推荐。

Conclusion: F-RLP为食品推荐领域提供了一种创新且高效的解决方案。

Abstract: Personalized food recommendation systems (Food-RecSys) critically
underperform due to fragmented component understanding and the failure of
conventional machine learning with vast, imbalanced food data. While Large
Language Models (LLMs) offer promise, current generic Recommendation as
Language Processing (RLP) strategies lack the necessary specialization for the
food domain's complexity. This thesis tackles these deficiencies by first
identifying and analyzing the essential components for effective Food-RecSys.
We introduce two key innovations: a multimedia food logging platform for rich
contextual data acquisition and the World Food Atlas, enabling unique
geolocation-based food analysis previously unavailable. Building on this
foundation, we pioneer the Food Recommendation as Language Processing (F-RLP)
framework - a novel, integrated approach specifically architected for the food
domain. F-RLP leverages LLMs in a tailored manner, overcoming the limitations
of generic models and providing a robust infrastructure for effective,
contextual, and truly personalized food recommendations.

</details>

### [288] [MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?](https://arxiv.org/abs/2504.20094)
*Zheng Hui,Xiaokai Wei,Yexi Jiang,Kevin Gao,Chen Wang,Frank Ong,Se-eun Yoon,Rachit Pareek,Michelle Gong*

Main category: cs.IR

TLDR: 提出了一种名为MATCHA的多智能体协作框架，用于对话推荐系统，利用大语言模型提升个性化和用户参与度。


<details>
  <summary>Details</summary>
Motivation: 解决对话推荐系统中的关键挑战，如处理复杂用户请求、提升个性化、确保安全可信的交互。

Method: 引入多个专门智能体（意图分析、候选生成、排序、重新排序、可解释性和保障）协作工作。

Result: 在八个指标上表现优于或与当前最优模型相当。

Conclusion: MATCHA框架有效提升了对话推荐系统的准确性、多样性和安全性。

Abstract: In this paper, we propose a multi-agent collaboration framework called MATCHA
for conversational recommendation system, leveraging large language models
(LLMs) to enhance personalization and user engagement. Users can request
recommendations via free-form text and receive curated lists aligned with their
interests, preferences, and constraints. Our system introduces specialized
agents for intent analysis, candidate generation, ranking, re-ranking,
explainability, and safeguards. These agents collaboratively improve
recommendations accuracy, diversity, and safety. On eight metrics, our model
achieves superior or comparable performance to the current state-of-the-art.
Through comparisons with six baseline models, our approach addresses key
challenges in conversational recommendation systems for game recommendations,
including: (1) handling complex, user-specific requests, (2) enhancing
personalization through multi-agent collaboration, (3) empirical evaluation and
deployment, and (4) ensuring safe and trustworthy interactions.

</details>

### [289] [Search-Based Interaction For Conversation Recommendation via Generative Reward Model Based Simulated User](https://arxiv.org/abs/2504.20458)
*Xiaolei Wang,Chunxuan Xia,Junyi Li,Fanzhe Meng,Lei Huang,Jinpeng Wang,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.IR

TLDR: 提出了一种基于生成奖励模型的模拟用户（GRSU），用于自动与对话推荐系统（CRS）交互，以更好地捕捉用户偏好。


<details>
  <summary>Details</summary>
Motivation: 对话推荐系统（CRS）在理解用户偏好方面面临挑战，频繁的用户交互可能影响体验。

Method: 设计了两种反馈动作（生成式评分和基于属性的评价），并通过指令调优统一模拟用户。使用波束搜索优化交互过程。

Result: 在公开数据集上验证了方法的有效性、效率和可迁移性。

Conclusion: GRSU能有效提升CRS的推荐效果，同时平衡交互效率。

Abstract: Conversational recommendation systems (CRSs) use multi-turn interaction to
capture user preferences and provide personalized recommendations. A
fundamental challenge in CRSs lies in effectively understanding user
preferences from conversations. User preferences can be multifaceted and
complex, posing significant challenges for accurate recommendations even with
access to abundant external knowledge. While interaction with users can clarify
their true preferences, frequent user involvement can lead to a degraded user
experience.
  To address this problem, we propose a generative reward model based simulated
user, named GRSU, for automatic interaction with CRSs. The simulated user
provides feedback to the items recommended by CRSs, enabling them to better
capture intricate user preferences through multi-turn interaction. Inspired by
generative reward models, we design two types of feedback actions for the
simulated user: i.e., generative item scoring, which offers coarse-grained
feedback, and attribute-based item critique, which provides fine-grained
feedback. To ensure seamless integration, these feedback actions are unified
into an instruction-based format, allowing the development of a unified
simulated user via instruction tuning on synthesized data. With this simulated
user, automatic multi-turn interaction with CRSs can be effectively conducted.
Furthermore, to strike a balance between effectiveness and efficiency, we draw
inspiration from the paradigm of reward-guided search in complex reasoning
tasks and employ beam search for the interaction process. On top of this, we
propose an efficient candidate ranking method to improve the recommendation
results derived from interaction. Extensive experiments on public datasets
demonstrate the effectiveness, efficiency, and transferability of our approach.

</details>

### [290] [TreeHop: Generate and Filter Next Query Embeddings Efficiently for Multi-hop Question Answering](https://arxiv.org/abs/2504.20114)
*Zhonghao Li,Kunpeng Zhang,Jinghuai Ou,Shuliang Liu,Xuming Hu*

Main category: cs.IR

TLDR: TreeHop是一种无需LLM的嵌入级框架，通过动态更新查询嵌入和多跳检索优化，显著降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决多跳问答中传统RAG系统因多次LLM调用和多阶段处理导致的高计算成本问题。

Method: 提出TreeHop框架，通过嵌入空间操作动态更新查询嵌入，简化检索流程为“检索-嵌入-检索”循环，并引入规则停止标准。

Result: 在三个开放域多跳问答数据集上表现优异，仅需5%-0.4%的模型参数量，查询延迟降低约99%。

Conclusion: TreeHop是一种高效、低成本的多跳问答解决方案，适用于知识密集型应用。

Abstract: Retrieval-augmented generation (RAG) systems face significant challenges in
multi-hop question answering (MHQA), where complex queries require synthesizing
information across multiple document chunks. Existing approaches typically rely
on iterative LLM-based query rewriting and routing, resulting in high
computational costs due to repeated LLM invocations and multi-stage processes.
To address these limitations, we propose TreeHop, an embedding-level framework
without the need for LLMs in query refinement. TreeHop dynamically updates
query embeddings by fusing semantic information from prior queries and
retrieved documents, enabling iterative retrieval through embedding-space
operations alone. This method replaces the traditional
"Retrieve-Rewrite-Vectorize-Retrieve" cycle with a streamlined
"Retrieve-Embed-Retrieve" loop, significantly reducing computational overhead.
Moreover, a rule-based stop criterion is introduced to further prune redundant
retrievals, balancing efficiency and recall rate. Experimental results show
that TreeHop rivals advanced RAG methods across three open-domain MHQA
datasets, achieving comparable performance with only 5\%-0.4\% of the model
parameter size and reducing the query latency by approximately 99\% compared to
concurrent approaches. This makes TreeHop a faster and more cost-effective
solution for deployment in a range of knowledge-intensive applications. For
reproducibility purposes, codes and data are available here:
https://github.com/allen-li1231/TreeHop.

</details>

### [291] [X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2504.20859)
*Guy Hadad,Haggai Roitman,Yotam Eshel,Bracha Shapira,Lior Rokach*

Main category: cs.IR

TLDR: X-Cross是一种新型跨域顺序推荐模型，通过整合多个领域特定语言模型，减少对新领域数据的依赖和计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统在新领域快速适应的问题，减少对大量重新训练的需求。

Method: 使用低秩适配器（LoRA）微调多个领域特定语言模型，动态整合知识并保留领域特性。

Result: 在亚马逊数据集上表现优异，仅需25%额外参数和50%-75%更少的微调数据，性能优于其他跨域基线。

Conclusion: X-Cross提供了一种高效、可扩展的跨域推荐解决方案，适用于数据受限环境。

Abstract: As new products are emerging daily, recommendation systems are required to
quickly adapt to possible new domains without needing extensive retraining.
This work presents ``X-Cross'' -- a novel cross-domain
sequential-recommendation model that recommends products in new domains by
integrating several domain-specific language models; each model is fine-tuned
with low-rank adapters (LoRA). Given a recommendation prompt, operating layer
by layer, X-Cross dynamically refines the representation of each source
language model by integrating knowledge from all other models. These refined
representations are propagated from one layer to the next, leveraging the
activations from each domain adapter to ensure domain-specific nuances are
preserved while enabling adaptability across domains. Using Amazon datasets for
sequential recommendation, X-Cross achieves performance comparable to a model
that is fine-tuned with LoRA, while using only 25% of the additional
parameters. In cross-domain tasks, such as adapting from Toys domain to Tools,
Electronics or Sports, X-Cross demonstrates robust performance, while requiring
about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective.
Furthermore, X-Cross achieves significant improvement in accuracy over
alternative cross-domain baselines. Overall, X-Cross enables scalable and
adaptive cross-domain recommendations, reducing computational overhead and
providing an efficient solution for data-constrained environments.

</details>

### [292] [OpenTCM: A GraphRAG-Empowered LLM-based System for Traditional Chinese Medicine Knowledge Retrieval and Diagnosis](https://arxiv.org/abs/2504.20118)
*Jinglin He,Yunqi Guo,Lai Kwan Lam,Waikei Leung,Lixing He,Yuanan Jiang,Chi Chiu Wang,Guoliang Xing,Hongkai Chen*

Main category: cs.IR

TLDR: OpenTCM是一个基于LLM的系统，结合了中医知识图谱和图增强检索生成技术，显著提升了中医文献的现代化和可访问性。


<details>
  <summary>Details</summary>
Motivation: 中医文献复杂且晦涩，AI技术的集成对其现代化和普及至关重要。

Method: 从中医经典数据库中提取数据，构建多关系知识图谱，并利用中文LLM（如DeepSeek和Kimi）进行高保真语义理解。

Result: 知识图谱精度达98.55%，F1分数99.55%；OpenTCM在成分检索和诊断问答任务中表现优异。

Conclusion: OpenTCM为中医现代化提供了高效解决方案，优于现有技术。

Abstract: Traditional Chinese Medicine (TCM) represents a rich repository of ancient
medical knowledge that continues to play an important role in modern
healthcare. Due to the complexity and breadth of the TCM literature, the
integration of AI technologies is critical for its modernization and broader
accessibility. However, this integration poses considerable challenges,
including the interpretation of obscure classical Chinese texts and the
modeling of intricate semantic relationships among TCM concepts. In this paper,
we develop OpenTCM, an LLM-based system that combines a domain-specific TCM
knowledge graph and Graph-based Retrieval-Augmented Generation (GraphRAG).
First, we extract more than 3.73 million classical Chinese characters from 68
gynecological books in the Chinese Medical Classics Database, with the help of
TCM and gynecology experts. Second, we construct a comprehensive
multi-relational knowledge graph comprising more than 48,000 entities and
152,000 interrelationships, using customized prompts and Chinese-oriented LLMs
such as DeepSeek and Kimi to ensure high-fidelity semantic understanding. Last,
we integrate OpenTCM with this knowledge graph, enabling high-fidelity
ingredient knowledge retrieval and diagnostic question-answering without model
fine-tuning. Experimental evaluations demonstrate that our prompt design and
model selection significantly improve knowledge graph quality, achieving a
precision of 98. 55% and an F1 score of 99. 55%. In addition, OpenTCM achieves
mean expert scores of 4.5 in ingredient information retrieval and 3.8 in
diagnostic question-answering tasks, outperforming state-of-the-art solutions
in real-world TCM use cases.

</details>

### [293] [Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets](https://arxiv.org/abs/2504.20119)
*Lorenz Brehme,Thomas Ströhle,Ruth Breu*

Main category: cs.IR

TLDR: 本文系统综述了63篇学术文章，全面概述了RAG系统的最新评估方法，重点关注数据集、检索器、索引与数据库以及生成器组件，并探讨了自动化评估的可行性及其与人工评估的平衡。


<details>
  <summary>Details</summary>
Motivation: RAG系统的复杂性对系统评估和质量提升提出了挑战，需要系统化的方法来记录进展、比较配置并识别有效方法。

Method: 通过系统综述63篇学术文章，分析RAG评估方法，重点关注四个关键领域，并探讨自动化评估的可行性。

Result: 研究发现自动化评估方法在RAG系统中具有可行性，但仍需进一步研究以指导实践，同时强调了领域特定数据集的重要性。

Conclusion: 本文为RAG系统的系统化评估方法提供了贡献，并探讨了自动化与人工评估的平衡及其各自的优缺点。

Abstract: Retrieval-Augmented Generation (RAG) has advanced significantly in recent
years. The complexity of RAG systems, which involve multiple components-such as
indexing, retrieval, and generation-along with numerous other parameters, poses
substantial challenges for systematic evaluation and quality enhancement.
Previous research highlights that evaluating RAG systems is essential for
documenting advancements, comparing configurations, and identifying effective
approaches for domain-specific applications. This study systematically reviews
63 academic articles to provide a comprehensive overview of state-of-the-art
RAG evaluation methodologies, focusing on four key areas: datasets, retrievers,
indexing and databases, and the generator component. We observe the feasibility
of an automated evaluation approach for each component of a RAG system,
leveraging an LLM capable of both generating evaluation datasets and conducting
evaluations. In addition, we found that further practical research is essential
to provide companies with clear guidance on the do's and don'ts of implementing
and evaluating RAG systems. By synthesizing evaluation approaches for key RAG
components and emphasizing the creation and adaptation of domain-specific
datasets for benchmarking, we contribute to the advancement of systematic
evaluation methods and the improvement of evaluation rigor for RAG systems.
Furthermore, by examining the interplay between automated approaches leveraging
LLMs and human judgment, we contribute to the ongoing discourse on balancing
automation and human input, clarifying their respective contributions,
limitations, and challenges in achieving robust and reliable evaluations.

</details>

### [294] [Enhancing News Recommendation with Hierarchical LLM Prompting](https://arxiv.org/abs/2504.20452)
*Hai-Dang Kieu,Delvin Ce Zhang,Minh Duc Nguyen,Min Xu,Qiang Wu,Dung D. Le*

Main category: cs.IR

TLDR: PNR-LLM利用大语言模型（LLM）增强新闻标题和摘要的语义信息，通过注意力机制整合数据，提升个性化新闻推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现有个性化新闻推荐系统依赖浅层表征（如标题和摘要），难以捕捉用户偏好的复杂性。

Method: 提出PNR-LLM方法，包含新闻增强模块（利用LLM生成深层语义和实体信息）和注意力机制整合数据。

Result: 在MIND数据集上表现优于现有基线，且增强模块可通用提升其他模型性能。

Conclusion: PNR-LLM通过LLM增强新闻表征，显著提升推荐效果，模块设计具有通用性。

Abstract: Personalized news recommendation systems often struggle to effectively
capture the complexity of user preferences, as they rely heavily on shallow
representations, such as article titles and abstracts. To address this problem,
we introduce a novel method, namely PNR-LLM, for Large Language Models for
Personalized News Recommendation. Specifically, PNR-LLM harnesses the
generation capabilities of LLMs to enrich news titles and abstracts, and
consequently improves recommendation quality. PNR-LLM contains a novel module,
News Enrichment via LLMs, which generates deeper semantic information and
relevant entities from articles, transforming shallow contents into richer
representations. We further propose an attention mechanism to aggregate
enriched semantic- and entity-level data, forming unified user and news
embeddings that reveal a more accurate user-news match. Extensive experiments
on MIND datasets show that PNR-LLM outperforms state-of-the-art baselines.
Moreover, the proposed data enrichment module is model-agnostic, and we
empirically show that applying our proposed module to multiple existing models
can further improve their performance, verifying the advantage of our design.

</details>

### [295] [Information Retrieval in the Age of Generative AI: The RGB Model](https://arxiv.org/abs/2504.20610)
*Michele Garetto,Alessandro Cornacchia,Franco Galante,Emilio Leonardi,Alessandro Nordio,Alberto Tarable*

Main category: cs.IR

TLDR: 本文提出了一种定量方法，研究生成式AI工具使用增加带来的信息动态变化，揭示了快速采用生成式AI可能导致信息不准确传播的风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）和生成式AI的兴起在改变信息检索和处理方式的同时，也引发了内容真实性和可靠性的担忧。本文旨在揭示这些未被充分理解的信息动态。

Method: 提出了一种随机模型，用于描述新主题下信息的生成、索引和传播过程，并结合Stack Exchange数据进行了深入分析。

Result: 研究发现，生成式AI的快速采用可能超越人工验证速度，增加不准确信息传播的风险。高质量答案需要大量时间和人力投入。

Conclusion: 强调了负责任开发和部署生成式AI工具的重要性，以减少不准确信息传播的风险。

Abstract: The advent of Large Language Models (LLMs) and generative AI is fundamentally
transforming information retrieval and processing on the Internet, bringing
both great potential and significant concerns regarding content authenticity
and reliability. This paper presents a novel quantitative approach to shed
light on the complex information dynamics arising from the growing use of
generative AI tools. Despite their significant impact on the digital ecosystem,
these dynamics remain largely uncharted and poorly understood. We propose a
stochastic model to characterize the generation, indexing, and dissemination of
information in response to new topics. This scenario particularly challenges
current LLMs, which often rely on real-time Retrieval-Augmented Generation
(RAG) techniques to overcome their static knowledge limitations. Our findings
suggest that the rapid pace of generative AI adoption, combined with increasing
user reliance, can outpace human verification, escalating the risk of
inaccurate information proliferation across digital resources. An in-depth
analysis of Stack Exchange data confirms that high-quality answers inevitably
require substantial time and human effort to emerge. This underscores the
considerable risks associated with generating persuasive text in response to
new questions and highlights the critical need for responsible development and
deployment of future generative AI tools.

</details>

<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [296] [Heterogeneous network drug-target interaction prediction model based on graph wavelet transform and multi-level contrastive learning](https://arxiv.org/abs/2504.20103)
*Wenfeng Dai,Yanhong Wang,Shuai Yan,Qingzhi Yu,Xiang Cheng*

Main category: q-bio.QM

TLDR: 提出了一种结合图神经网络和多尺度信号处理的药物-靶点相互作用预测框架，解决了传统方法的黑盒问题，具有高效预测和多层次可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在药物-靶点相互作用预测中存在黑盒问题，难以揭示模型决策机制与生物分子相互作用模式的深层关联。

Method: 结合异构图卷积神经网络（HGCN）和多尺度信号处理技术，设计了局部-全局特征协同感知模块、多尺度图信号分解与生物解释模块，以及对比学习模块。

Result: 在所有数据集上表现出优异的预测性能，为药物靶点发现提供了从黑盒预测到机制解码的完整解决方案。

Conclusion: 该框架为复杂生物分子相互作用系统建模提供了重要参考价值。

Abstract: Drug-target interaction (DTI) prediction is a core task in drug development
and precision medicine in the biomedical field. However, traditional machine
learning methods generally have the black box problem, which makes it difficult
to reveal the deep correlation between the model decision mechanism and the
interaction pattern between biological molecules. This study proposes a
heterogeneous network drug target interaction prediction framework, integrating
graph neural network and multi scale signal processing technology to construct
a model with both efficient prediction and multi level interpretability. Its
technical breakthroughs are mainly reflected in the following three
dimensions:Local global feature collaborative perception module. Based on
heterogeneous graph convolutional neural network (HGCN), a multi order neighbor
aggregation strategy is designed.Multi scale graph signal decomposition and
biological interpretation module. A deep hierarchical node feature transform
(GWT) architecture is proposed.Contrastive learning combining multi dimensional
perspectives and hierarchical representations. By comparing the learning
models, the node representations from the two perspectives of HGCN and GWT are
aligned and fused, so that the model can integrate multi dimensional
information and improve the prediction robustness. Experimental results show
that our framework shows excellent prediction performance on all datasets. This
study provides a complete solution for drug target discovery from black box
prediction to mechanism decoding, and its methodology has important reference
value for modeling complex biomolecular interaction systems.

</details>

<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [297] [Guessing Efficiently for Constrained Subspace Approximation](https://arxiv.org/abs/2504.20883)
*Aditya Bhaskara,Sepideh Mahabadi,Madhusudhan Reddy Pittu,Ali Vakilian,David P. Woodruff*

Main category: cs.DS

TLDR: 本文研究了约束子空间逼近问题，提出了一种通用的框架（coreset-guess-solve），用于解决多种约束条件下的子空间逼近问题，并改进了多个相关问题的已知结果。


<details>
  <summary>Details</summary>
Motivation: 研究约束子空间逼近问题，旨在找到满足特定约束条件的子空间，以更好地逼近输入点集。

Method: 提出了一种称为coreset-guess-solve的通用框架，用于生成（1+ε）乘法或ε加法近似解。

Result: 该框架为分区约束子空间逼近提供了新算法，并改进了公平子空间逼近、k均值聚类和非负矩阵分解等问题的已知结果。

Conclusion: 提出的框架在多种约束条件下有效，且在某些问题上优于现有方法。

Abstract: In this paper we study constrained subspace approximation problem. Given a
set of $n$ points $\{a_1,\ldots,a_n\}$ in $\mathbb{R}^d$, the goal of the {\em
subspace approximation} problem is to find a $k$ dimensional subspace that best
approximates the input points. More precisely, for a given $p\geq 1$, we aim to
minimize the $p$th power of the $\ell_p$ norm of the error vector
$(\|a_1-\bm{P}a_1\|,\ldots,\|a_n-\bm{P}a_n\|)$, where $\bm{P}$ denotes the
projection matrix onto the subspace and the norms are Euclidean. In
\emph{constrained} subspace approximation (CSA), we additionally have
constraints on the projection matrix $\bm{P}$. In its most general form, we
require $\bm{P}$ to belong to a given subset $\mathcal{S}$ that is described
explicitly or implicitly.
  We introduce a general framework for constrained subspace approximation. Our
approach, that we term coreset-guess-solve, yields either
$(1+\varepsilon)$-multiplicative or $\varepsilon$-additive approximations for a
variety of constraints. We show that it provides new algorithms for
partition-constrained subspace approximation with applications to {\it fair}
subspace approximation, $k$-means clustering, and projected non-negative matrix
factorization, among others. Specifically, while we reconstruct the best known
bounds for $k$-means clustering in Euclidean spaces, we improve the known
results for the remainder of the problems.

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [298] [Pediatric Asthma Detection with Googles HeAR Model: An AI-Driven Respiratory Sound Classifier](https://arxiv.org/abs/2504.20124)
*Abul Ehtesham,Saket Kumar,Aditi Singh,Tala Talaei Khoei*

Main category: cs.SD

TLDR: AI利用HeAR模型分析儿童呼吸音，实现哮喘早期检测，准确率超91%。


<details>
  <summary>Details</summary>
Motivation: 早期发现儿童哮喘可减少长期并发症和紧急干预。

Method: 使用SPRSound数据集，通过HeAR模型提取特征，训练多种分类器区分哮喘音和正常音。

Result: 系统准确率超91%，在阳性病例中表现优异。

Conclusion: 该方法适用于远程医疗，为非侵入性哮喘筛查提供快速解决方案。

Abstract: Early detection of asthma in children is crucial to prevent long-term
respiratory complications and reduce emergency interventions. This work
presents an AI-powered diagnostic pipeline that leverages Googles Health
Acoustic Representations (HeAR) model to detect early signs of asthma from
pediatric respiratory sounds. The SPRSound dataset, the first open-access
collection of annotated respiratory sounds in children aged 1 month to 18
years, is used to extract 2-second audio segments labeled as wheeze, crackle,
rhonchi, stridor, or normal. Each segment is embedded into a 512-dimensional
representation using HeAR, a foundation model pretrained on 300 million
health-related audio clips, including 100 million cough sounds. Multiple
classifiers, including SVM, Random Forest, and MLP, are trained on these
embeddings to distinguish between asthma-indicative and normal sounds. The
system achieves over 91\% accuracy, with strong performance on precision-recall
metrics for positive cases. In addition to classification, learned embeddings
are visualized using PCA, misclassifications are analyzed through waveform
playback, and ROC and confusion matrix insights are provided. This method
demonstrates that short, low-resource pediatric recordings, when powered by
foundation audio models, can enable fast, noninvasive asthma screening. The
approach is especially promising for digital diagnostics in remote or
underserved healthcare settings.

</details>

### [299] [End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based Approach with Cross-Dataset Evaluation](https://arxiv.org/abs/2504.20923)
*Andrea Di Pierno,Luca Guarnera,Dario Allegra,Sebastiano Battiato*

Main category: cs.SD

TLDR: 论文提出了一种轻量级的端到端深度学习框架RawNetLite，用于检测音频深度伪造，通过结合多领域数据和Focal Loss提升模型鲁棒性，并在多种测试集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 音频深度伪造对数字安全和信任构成威胁，现有检测方法在开放世界条件下表现不佳，需要更鲁棒和通用的解决方案。

Method: 提出RawNetLite模型，直接处理原始波形，结合卷积-循环架构捕捉频谱和时序特征，采用多领域数据和Focal Loss训练策略，并引入音频增强技术。

Result: 模型在FakeOrReal数据集上达到99.7% F1和0.25% EER，在AVSpoof2021 + CodecFake数据集上达到83.4% F1和16.4% EER。

Conclusion: 多样化的训练数据、定制目标函数和音频增强对构建鲁棒的音频伪造检测器至关重要。

Abstract: Audio deepfakes represent a growing threat to digital security and trust,
leveraging advanced generative models to produce synthetic speech that closely
mimics real human voices. Detecting such manipulations is especially
challenging under open-world conditions, where spoofing methods encountered
during testing may differ from those seen during training. In this work, we
propose an end-to-end deep learning framework for audio deepfake detection that
operates directly on raw waveforms. Our model, RawNetLite, is a lightweight
convolutional-recurrent architecture designed to capture both spectral and
temporal features without handcrafted preprocessing. To enhance robustness, we
introduce a training strategy that combines data from multiple domains and
adopts Focal Loss to emphasize difficult or ambiguous samples. We further
demonstrate that incorporating codec-based manipulations and applying
waveform-level audio augmentations (e.g., pitch shifting, noise, and time
stretching) leads to significant generalization improvements under realistic
acoustic conditions. The proposed model achieves over 99.7% F1 and 0.25% EER on
in-domain data (FakeOrReal), and up to 83.4% F1 with 16.4% EER on a challenging
out-of-distribution test set (AVSpoof2021 + CodecFake). These findings
highlight the importance of diverse training data, tailored objective functions
and audio augmentations in building resilient and generalizable audio forgery
detectors. Code and pretrained models are available at
https://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/.

</details>

### [300] [APG-MOS: Auditory Perception Guided-MOS Predictor for Synthetic Speech](https://arxiv.org/abs/2504.20447)
*Zhicheng Lian,Lizhi Wang,Hua Huang*

Main category: cs.SD

TLDR: 提出了一种听觉感知引导的语音质量评估模型（APG-MOS），结合听觉建模与语义分析，提升与人类判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 减少耗时的人工评估，通过计算模型量化主观语音感知，现有深度学习模型忽视听觉感知机制，限制了与人类判断的一致性。

Method: 设计基于生物听觉机制的感知模块模拟耳蜗功能；提出基于残差向量量化的语义失真建模方法；设计残差交叉注意力架构进行多模态融合。

Result: 在两个主要基准测试中表现优异。

Conclusion: APG-MOS通过整合听觉感知与语义分析，显著提升了语音质量评估的准确性。

Abstract: Automatic speech quality assessment aims to quantify subjective human
perception of speech through computational models to reduce the need for
labor-consuming manual evaluations. While models based on deep learning have
achieved progress in predicting mean opinion scores (MOS) to assess synthetic
speech, the neglect of fundamental auditory perception mechanisms limits
consistency with human judgments. To address this issue, we propose an auditory
perception guided-MOS prediction model (APG-MOS) that synergistically
integrates auditory modeling with semantic analysis to enhance consistency with
human judgments. Specifically, we first design a perceptual module, grounded in
biological auditory mechanisms, to simulate cochlear functions, which encodes
acoustic signals into biologically aligned electrochemical representations.
Secondly, we propose a residual vector quantization (RVQ)-based semantic
distortion modeling method to quantify the degradation of speech quality at the
semantic level. Finally, we design a residual cross-attention architecture,
coupled with a progressive learning strategy, to enable multimodal fusion of
encoded electrochemical signals and semantic representations. Experiments
demonstrate that APG-MOS achieves superior performance on two primary
benchmarks. Our code and checkpoint will be available on a public repository
upon publication.

</details>

### [301] [DiffusionRIR: Room Impulse Response Interpolation using Diffusion Models](https://arxiv.org/abs/2504.20625)
*Sagi Della Torre,Mirco Pezzoli,Fabio Antonacci,Sharon Gannot*

Main category: cs.SD

TLDR: 该研究提出了一种基于去噪扩散概率模型（DDPM）的方法，用于估计房间内未测量位置的房间脉冲响应（RIR），显著优于传统插值方法。


<details>
  <summary>Details</summary>
Motivation: 高空间分辨率的RIR测量资源密集且不切实际，尤其是在大空间或需要密集采样时。研究旨在解决这一问题。

Method: 将RIR数据转化为适合扩散模型重建的格式，利用模拟RIR数据验证方法在多种麦克风阵列上的有效性。

Result: 方法成功重建缺失RIR，在归一化均方误差和余弦距离上显著优于基线样条立方插值。

Conclusion: 生成模型在RIR插值中具有潜力，为从有限测量生成额外数据提供了新途径。

Abstract: Room Impulse Responses (RIRs) characterize acoustic environments and are
crucial in multiple audio signal processing tasks. High-quality RIR estimates
drive applications such as virtual microphones, sound source localization,
augmented reality, and data augmentation. However, obtaining RIR measurements
with high spatial resolution is resource-intensive, making it impractical for
large spaces or when dense sampling is required. This research addresses the
challenge of estimating RIRs at unmeasured locations within a room using
Denoising Diffusion Probabilistic Models (DDPM). Our method leverages the
analogy between RIR matrices and image inpainting, transforming RIR data into a
format suitable for diffusion-based reconstruction.
  Using simulated RIR data based on the image method, we demonstrate our
approach's effectiveness on microphone arrays of different curvatures, from
linear to semi-circular. Our method successfully reconstructs missing RIRs,
even in large gaps between microphones. Under these conditions, it achieves
accurate reconstruction, significantly outperforming baseline Spline Cubic
Interpolation in terms of Normalized Mean Square Error and Cosine Distance
between actual and interpolated RIRs.
  This research highlights the potential of using generative models for
effective RIR interpolation, paving the way for generating additional data from
limited real-world measurements.

</details>

### [302] [ECOSoundSet: a finely annotated dataset for the automated acoustic identification of Orthoptera and Cicadidae in North, Central and temperate Western Europe](https://arxiv.org/abs/2504.20776)
*David Funosas,Elodie Massol,Yves Bas,Svenja Schmidt,Dominik Arend,Alexander Gebhard,Luc Barbaro,Sebastian König,Rafael Carbonell Font,David Sannier,Fernand Deroussen,Jérôme Sueur,Christian Roesti,Tomi Trilar,Wolfgang Forstmeier,Lucas Roger,Eloïsa Matheu,Piotr Guzik,Julien Barataud,Laurent Pelozuelo,Stéphane Puissant,Sandra Mueller,Björn Schuller,Jose M. Montoya,Andreas Triantafyllopoulos,Maxime Cauchoix*

Main category: cs.SD

TLDR: ECOSoundSet是一个包含欧洲直翅目和蝉类物种声音的数据集，旨在支持深度学习算法在自然声景中的自动识别。


<details>
  <summary>Details</summary>
Motivation: 现有工具在欧洲昆虫声音识别中范围有限，需要大规模生态异质性数据集来改进算法。

Method: 数据集包含10,653条录音，涵盖200种直翅目和24种蝉类，部分为弱标注，部分为强标注，并提供训练/验证/测试分割。

Result: 提供了可用于深度学习算法训练和评估的数据集。

Conclusion: ECOSoundSet可作为现有在线录音的有力补充，提升欧洲直翅目和蝉类的声学分类效果。

Abstract: Currently available tools for the automated acoustic recognition of European
insects in natural soundscapes are limited in scope. Large and ecologically
heterogeneous acoustic datasets are currently needed for these algorithms to
cross-contextually recognize the subtle and complex acoustic signatures
produced by each species, thus making the availability of such datasets a key
requisite for their development. Here we present ECOSoundSet (European
Cicadidae and Orthoptera Sound dataSet), a dataset containing 10,653 recordings
of 200 orthopteran and 24 cicada species (217 and 26 respective taxa when
including subspecies) present in North, Central, and temperate Western Europe
(Andorra, Belgium, Denmark, mainland France and Corsica, Germany, Ireland,
Luxembourg, Monaco, Netherlands, United Kingdom, Switzerland), collected partly
through targeted fieldwork in South France and Catalonia and partly through
contributions from various European entomologists. The dataset is composed of a
combination of coarsely labeled recordings, for which we can only infer the
presence, at some point, of their target species (weak labeling), and finely
annotated recordings, for which we know the specific time and frequency range
of each insect sound present in the recording (strong labeling). We also
provide a train/validation/test split of the strongly labeled recordings, with
respective approximate proportions of 0.8, 0.1 and 0.1, in order to facilitate
their incorporation in the training and evaluation of deep learning algorithms.
This dataset could serve as a meaningful complement to recordings already
available online for the training of deep learning algorithms for the acoustic
classification of orthopterans and cicadas in North, Central, and temperate
Western Europe.

</details>

<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [303] [Partial Answer of How Transformers Learn Automata](https://arxiv.org/abs/2504.20395)
*Tiantian,Zhang*

Main category: cs.FL

TLDR: 提出一种基于表示论半直积和傅里叶模块的有限自动机模拟框架，提升基于Transformer的实现效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的有限自动机模拟方法效率不足，需更高效的实现方案。

Method: 利用表示论半直积和傅里叶模块构建新型框架。

Result: 实现了更高效的Transformer-based有限自动机模拟。

Conclusion: 该框架为有限自动机模拟提供了高效的新方法。

Abstract: We introduce a novel framework for simulating finite automata using
representation-theoretic semidirect products and Fourier modules, achieving
more efficient Transformer-based implementations.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [304] [Fostering Self-Directed Growth with Generative AI: Toward a New Learning Analytics Framework](https://arxiv.org/abs/2504.20851)
*Qianrun Mao*

Main category: cs.CY

TLDR: 论文提出了一种结合生成式人工智能和学习分析的新框架A2PL，旨在培养学习者的自我导向成长能力，以应对去中心化知识生态系统的挑战。


<details>
  <summary>Details</summary>
Motivation: 在当前去中心化知识生态系统和AI技术普及的背景下，培养学习者的自我导向能力成为教育的关键需求。现有研究在自我导向学习和AI辅助教育方面存在空白。

Method: 提出了A2PL模型，整合生成式AI和学习分析，重新定义学习者愿望、复杂思维和总结性自我评估的互动关系。

Result: A2PL模型为未来干预设计和学习分析应用提供了方法论基础。

Conclusion: 自我导向成长是数字时代构建公平、适应性强且可持续学习系统的关键。

Abstract: In an era increasingly shaped by decentralized knowledge ecosystems and
pervasive AI technologies, fostering sustainable learner agency has become a
critical educational imperative. This study introduces a novel conceptual
framework integrating Generative Artificial Intelligence and Learning Analytics
to cultivate Self-Directed Growth, a dynamic competency that enables learners
to iteratively drive their own developmental pathways across diverse
contexts.Building upon critical gaps in current research on Self Directed
Learning and AI-mediated education, the proposed Aspire to Potentials for
Learners (A2PL) model reconceptualizes the interplay of learner aspirations,
complex thinking, and summative self-assessment within GAI supported
environments.Methodological implications for future intervention design and
learning analytics applications are discussed, positioning Self-Directed Growth
as a pivotal axis for developing equitable, adaptive, and sustainable learning
systems in the digital era.

</details>

### [305] [When Testing AI Tests Us: Safeguarding Mental Health on the Digital Frontlines](https://arxiv.org/abs/2504.20910)
*Sachin R. Pendse,Darren Gergle,Rachel Kornfield,Jonah Meyerhoff,David Mohr,Jina Suh,Annie Wescott,Casey Williams,Jessica Schleider*

Main category: cs.CY

TLDR: 论文探讨了AI红队成员的心理健康问题，提出了保护措施。


<details>
  <summary>Details</summary>
Motivation: 确保AI模型不传播有害内容的同时，保护红队成员的心理健康。

Method: 分析红队工作的心理健康影响，借鉴其他职业的保护策略。

Result: 提出了针对红队成员心理健康的个体和组织保护策略。

Conclusion: 红队成员的心理健康是AI安全的重要组成部分，需采取具体措施保护。

Abstract: Red-teaming is a core part of the infrastructure that ensures that AI models
do not produce harmful content. Unlike past technologies, the black box nature
of generative AI systems necessitates a uniquely interactional mode of testing,
one in which individuals on red teams actively interact with the system,
leveraging natural language to simulate malicious actors and solicit harmful
outputs. This interactional labor done by red teams can result in mental health
harms that are uniquely tied to the adversarial engagement strategies necessary
to effectively red team. The importance of ensuring that generative AI models
do not propagate societal or individual harm is widely recognized -- one less
visible foundation of end-to-end AI safety is also the protection of the mental
health and wellbeing of those who work to keep model outputs safe. In this
paper, we argue that the unmet mental health needs of AI red-teamers is a
critical workplace safety concern. Through analyzing the unique mental health
impacts associated with the labor done by red teams, we propose potential
individual and organizational strategies that could be used to meet these
needs, and safeguard the mental health of red-teamers. We develop our proposed
strategies through drawing parallels between common red-teaming practices and
interactional labor common to other professions (including actors, mental
health professionals, conflict photographers, and content moderators),
describing how individuals and organizations within these professional spaces
safeguard their mental health given similar psychological demands. Drawing on
these protective practices, we describe how safeguards could be adapted for the
distinct mental health challenges experienced by red teaming organizations as
they mitigate emerging technological risks on the new digital frontlines.

</details>

<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [306] [New Capacity Bounds for PIR on Graph and Multigraph-Based Replicated Storage](https://arxiv.org/abs/2504.20888)
*Xiangliang Kong,Shreya Meel,Thomas Jacob Maranzatto,Itzhak Tamo,Sennur Ulukus*

Main category: cs.IT

TLDR: 研究了基于图和多重图的复制系统中的私有信息检索（PIR）问题，推导了PIR容量的上界，并提出了接近这些上界的PIR方案。


<details>
  <summary>Details</summary>
Motivation: 探索在文件和服务器存储限制下的PIR问题，以优化信息检索的效率和隐私保护。

Method: 针对图和多重图系统，推导PIR容量的上界，并设计接近这些上界的PIR方案。

Result: 确定了路径图的精确PIR容量，改进了现有结果；提出了多重图的PIR方案，并建立了上下界。

Conclusion: 在特定情况下，提出的PIR方案和容量界限是紧的，为相关系统提供了理论支持。

Abstract: In this paper, we study the problem of private information retrieval (PIR) in
both graph-based and multigraph-based replication systems, where each file is
stored on exactly two servers, and any pair of servers shares at most $r$
files. We derive upper bounds on the PIR capacity for such systems and
construct PIR schemes that approach these bounds. For graph-based systems, we
determine the exact PIR capacity for path graphs and improve upon existing
results for complete bipartite graphs and complete graphs. For multigraph-based
systems, we propose a PIR scheme that leverages the symmetry of the underlying
graph-based construction, yielding a capacity lower bound for such multigraphs.
Furthermore, we establish several general upper and lower bounds on the PIR
capacity of multigraphs, which are tight in certain cases.

</details>

<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [307] [Energy-Based Coarse-Graining in Molecular Dynamics: A Flow-Based Framework Without Data](https://arxiv.org/abs/2504.20940)
*Maximilian Stupp,P. S. Koutsourelakis*

Main category: physics.chem-ph

TLDR: 提出了一种无需数据的粗粒化生成框架，直接针对全原子玻尔兹曼分布，通过结构化潜在空间和能量目标训练，生成平衡全原子样本。


<details>
  <summary>Details</summary>
Motivation: 传统粗粒化方法依赖全原子分子动力学轨迹，限制了模型的准确性和泛化性。

Method: 定义包含慢速和快速变量的结构化潜在空间，使用可学习的双射映射和基于能量的目标训练。

Result: 模型能准确重建原子构型，捕捉玻尔兹曼分布的所有相关模式，并学习物理意义的粗粒化表示。

Conclusion: 该方法无需模拟数据即可实现高精度粗粒化建模。

Abstract: Coarse-grained (CG) models offer an effective route to reducing the
complexity of molecular simulations, yet conventional approaches depend heavily
on long all-atom molecular dynamics (MD) trajectories to adequately sample
configurational space. This data-driven dependence limits their accuracy and
generalizability, as unvisited configurations remain excluded from the
resulting CG model. We introduce a data-free generative framework for
coarse-graining that directly targets the all-atom Boltzmann distribution. Our
model defines a structured latent space comprising slow collective variables,
which are statistically associated with multimodal marginal densities capturing
metastable states, and fast variables, which represent the remaining degrees of
freedom with simple, unimodal conditional distributions. A potentially
learnable, bijective map from the full latent space to the all-atom
configuration space enables automatic and accurate reconstruction of molecular
structures. The model is trained using an energy-based objective that minimizes
the reverse Kullback-Leibler divergence, relying solely on the interatomic
potential rather than sampled trajectories. A tempering scheme is used to
stabilize training and promote exploration of diverse configurations. Once
trained, the model can generate unbiased, one-shot equilibrium all-atom
samples. We validate the method on two synthetic systems-a double-well
potential and a Gaussian mixture-as well as on the benchmark alanine dipeptide.
The model captures all relevant modes of the Boltzmann distribution, accurately
reconstructs atomic configurations, and learns physically meaningful
coarse-grained representations, all without any simulation data.

</details>

<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [308] [Provably faster randomized and quantum algorithms for k-means clustering via uniform sampling](https://arxiv.org/abs/2504.20982)
*Tyler Chen,Archan Ray,Akshay Seshadri,Dylan Herman,Bao Bach,Pranav Deshpande,Abhishek Som,Niraj Kumar,Marco Pistoia*

Main category: quant-ph

TLDR: 论文提出了一种随机小批量$k$-均值算法及其量子版本，显著改进了之前算法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决$k$-均值算法在大数据应用中计算成本高的问题。

Method: 使用均匀采样的小批量$k$-均值算法及量子算法。

Result: 证明了比之前算法更优的最坏情况保证。

Conclusion: 均匀采样保留了$k$-均值问题的对称性，从而提升了算法性能。

Abstract: The $k$-means algorithm (Lloyd's algorithm) is a widely used method for
clustering unlabeled data. A key bottleneck of the $k$-means algorithm is that
each iteration requires time linear in the number of data points, which can be
expensive in big data applications. This was improved in recent works proposing
quantum and quantum-inspired classical algorithms to approximate the $k$-means
algorithm locally, in time depending only logarithmically on the number of data
points (along with data dependent parameters) [$q$-means: A quantum algorithm
for unsupervised machine learning; Kerenidis, Landman, Luongo, and Prakash,
NeurIPS 2019; Do you know what $q$-means?, Doriguello, Luongo, Tang]. In this
work, we describe a simple randomized mini-batch $k$-means algorithm and a
quantum algorithm inspired by the classical algorithm. We prove worse-case
guarantees that significantly improve upon the bounds for previous algorithms.
Our improvements are due to a careful use of uniform sampling, which preserves
certain symmetries of the $k$-means problem that are not preserved in previous
algorithms that use data norm-based sampling.

</details>

<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [309] [CarbonCall: Sustainability-Aware Function Calling for Large Language Models on Edge Devices](https://arxiv.org/abs/2504.20348)
*Varatheepan Paramanayakam,Andreas Karatzas,Iraklis Anagnostopoulos,Dimitrios Stamoulis*

Main category: cs.PF

TLDR: CarbonCall是一个可持续感知的函数调用框架，通过动态工具选择、碳感知执行和量化LLM适配，显著降低碳排放和功耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法在优化性能时忽略了可持续性，导致高能耗和碳排放，不适用于能源受限环境。

Method: CarbonCall整合动态工具选择、碳感知执行和量化LLM适配，根据实时碳强度预测调整功率阈值，并在模型变体间切换。

Result: 实验表明，CarbonCall在NVIDIA Jetson AGX Orin上减少碳排放52%、功耗30%和执行时间30%，同时保持高效率。

Conclusion: CarbonCall为边缘AI系统提供了一种高效且可持续的函数调用解决方案。

Abstract: Large Language Models (LLMs) enable real-time function calling in edge AI
systems but introduce significant computational overhead, leading to high power
consumption and carbon emissions. Existing methods optimize for performance
while neglecting sustainability, making them inefficient for energy-constrained
environments. We introduce CarbonCall, a sustainability-aware function-calling
framework that integrates dynamic tool selection, carbon-aware execution, and
quantized LLM adaptation. CarbonCall adjusts power thresholds based on
real-time carbon intensity forecasts and switches between model variants to
sustain high tokens-per-second throughput under power constraints. Experiments
on an NVIDIA Jetson AGX Orin show that CarbonCall reduces carbon emissions by
up to 52%, power consumption by 30%, and execution time by 30%, while
maintaining high efficiency.

</details>

<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [310] [Towards Large Language Models for Lunar Mission Planning and In Situ Resource Utilization](https://arxiv.org/abs/2504.20125)
*Michael Pekala,Gregory Canal,Samuel Barham,Milena B. Graziano,Morgan Trexler,Leslie Hamilton,Elizabeth Reilly,Christopher D. Stiles*

Main category: cs.DL

TLDR: 利用LLMs从科学文献中快速提取月球成分数据的可行性研究，发现现成LLMs在提取表格数据上有效，但需进一步优化以捕捉更精细信息。


<details>
  <summary>Details</summary>
Motivation: 评估月球任务规划中原材料本地可用性的能力，但相关数据分散在科学文献中，需高效提取。

Method: 利用LLMs处理科学文献语料库，提取月球成分数据，关注准确性和不确定性量化。

Result: 现成LLMs能有效提取表格数据，但对精细矿物学信息和复杂数据仍需改进。

Conclusion: LLMs在提取月球成分数据上初步有效，但需进一步优化以提升性能。

Abstract: A key factor for lunar mission planning is the ability to assess the local
availability of raw materials. However, many potentially relevant measurements
are scattered across a variety of scientific publications. In this paper we
consider the viability of obtaining lunar composition data by leveraging LLMs
to rapidly process a corpus of scientific publications. While leveraging LLMs
to obtain knowledge from scientific documents is not new, this particular
application presents interesting challenges due to the heterogeneity of lunar
samples and the nuances involved in their characterization. Accuracy and
uncertainty quantification are particularly crucial since many materials
properties can be sensitive to small variations in composition. Our findings
indicate that off-the-shelf LLMs are generally effective at extracting data
from tables commonly found in these documents. However, there remains
opportunity to further refine the data we extract in this initial approach; in
particular, to capture fine-grained mineralogy information and to improve
performance on more subtle/complex pieces of information.

</details>

<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [311] [AKIBoards: A Structure-Following Multiagent System for Predicting Acute Kidney Injury](https://arxiv.org/abs/2504.20368)
*David Gordon,Panayiotis Petousis,Susanne B. Nicholas,Alex A. T. Bui*

Main category: cs.MA

TLDR: 论文提出STRUC-MAS框架，通过多智能体系统（MAS）自动学习全局模型，用于医疗诊断推理，并在急性肾损伤（AKI）预测中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在复杂医疗场景中，多专家协作需要整合不同视角以优化诊断决策，因此需要一种自动化框架来学习全局模型并指导多智能体系统。

Method: 引入STRUC-MAS框架，通过学习全局模型并将其作为先验信念整合到多智能体系统中，应用于AKI预测。

Result: 实验显示，基于全局结构的智能体（SF-FT和SF-FT-RAG）在AKI预测中表现优于基线（NSF-FT和NSF-FT-RAG），且智能体通过交互增强了决策信心。

Conclusion: 学习并利用全局结构对多智能体系统在分类和诊断推理中的性能提升至关重要。

Abstract: Diagnostic reasoning entails a physician's local (mental) model based on an
assumed or known shared perspective (global model) to explain patient
observations with evidence assigned towards a clinical assessment. But in
several (complex) medical situations, multiple experts work together as a team
to optimize health evaluation and decision-making by leveraging different
perspectives. Such consensus-driven reasoning reflects individual knowledge
contributing toward a broader perspective on the patient. In this light, we
introduce STRUCture-following for Multiagent Systems (STRUC-MAS), a framework
automating the learning of these global models and their incorporation as prior
beliefs for agents in multiagent systems (MAS) to follow. We demonstrate proof
of concept with a prosocial MAS application for predicting acute kidney
injuries (AKIs). In this case, we found that incorporating a global structure
enabled multiple agents to achieve better performance (average precision, AP)
in predicting AKI 48 hours before onset (structure-following-fine-tuned, SF-FT,
AP=0.195; SF-FT-retrieval-augmented generation, SF-FT-RAG, AP=0.194) vs.
baseline (non-structure-following-FT, NSF-FT, AP=0.141; NSF-FT-RAG, AP=0.180)
for balanced precision-weighted-recall-weighted voting. Markedly, SF-FT agents
with higher recall scores reported lower confidence levels in the initial round
on true positive and false negative cases. But after explicit interactions,
their confidence in their decisions increased (suggesting reinforced belief).
In contrast, the SF-FT agent with the lowest recall decreased its confidence in
true positive and false negative cases (suggesting a new belief). This approach
suggests that learning and leveraging global structures in MAS is necessary
prior to achieving competitive classification and diagnostic reasoning
performance.

</details>

### [312] [Modeling AI-Human Collaboration as a Multi-Agent Adaptation](https://arxiv.org/abs/2504.20903)
*Prothit Sen,Sai Mihir Jakkaraju*

Main category: cs.MA

TLDR: 论文通过基于代理的模拟，研究了AI与人类协作的效果，发现任务结构（模块化或序列化）是决定协作效果的关键因素。


<details>
  <summary>Details</summary>
Motivation: 探索AI与人类协作在不同任务结构下的表现，为组织战略决策提供通用框架。

Method: 使用基于代理的模拟和NK模型，区分启发式人类适应和基于规则的AI搜索，分析模块化和序列化任务中的交互。

Result: 模块化任务中AI通常替代人类，除非人类专业知识极高；序列化任务中互补性更强，专家人类引导AI搜索时表现最佳。

Conclusion: AI与人类协作的效果主要取决于任务结构，任务分解是战略决策的核心分析单元。

Abstract: We develop an agent-based simulation to formalize AI-human collaboration as a
function of task structure, advancing a generalizable framework for strategic
decision-making in organizations. Distinguishing between heuristic-based human
adaptation and rule-based AI search, we model interactions across modular
(parallel) and sequenced (interdependent) tasks using an NK model. Our results
reveal that in modular tasks, AI often substitutes for humans - delivering
higher payoffs unless human expertise is very high, and the AI search space is
either narrowly focused or extremely broad. In sequenced tasks, interesting
complementarities emerge. When an expert human initiates the search and AI
subsequently refines it, aggregate performance is maximized. Conversely, when
AI leads, excessive heuristic refinement by the human can reduce payoffs. We
also show that even "hallucinatory" AI - lacking memory or structure - can
improve outcomes when augmenting low-capability humans by helping escape local
optima. These results yield a robust implication: the effectiveness of AI-human
collaboration depends less on context or industry, and more on the underlying
task structure. By elevating task decomposition as the central unit of
analysis, our model provides a transferable lens for strategic decision-making
involving humans and an agentic AI across diverse organizational settings.

</details>

<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [313] [Predictive AI with External Knowledge Infusion for Stocks](https://arxiv.org/abs/2504.20058)
*Ambedkar Dukkipati,Kawin Mayilvaghanan,Naveen Kumar Pallekonda,Sai Prakash Hadnoor,Ranga Shaarad Ayyagari*

Main category: q-fin.ST

TLDR: 本文提出了一种结合历史趋势和外部知识（通过时间知识图谱）的股票预测方法，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 股票价格波动受多种动态外部因素影响，现有方法未充分结合这些因素。

Method: 构建时间知识图谱数据集，将外部关系建模为图上的霍克斯过程事件。

Result: 动态表示能有效排名股票收益，在多个持有期内优于基线。

Conclusion: 结合外部知识的动态表示方法在股票预测中表现优异。

Abstract: Fluctuations in stock prices are influenced by a complex interplay of factors
that go beyond mere historical data. These factors, themselves influenced by
external forces, encompass inter-stock dynamics, broader economic factors,
various government policy decisions, outbreaks of wars, etc. Furthermore, all
of these factors are dynamic and exhibit changes over time. In this paper, for
the first time, we tackle the forecasting problem under external influence by
proposing learning mechanisms that not only learn from historical trends but
also incorporate external knowledge from temporal knowledge graphs. Since there
are no such datasets or temporal knowledge graphs available, we study this
problem with stock market data, and we construct comprehensive temporal
knowledge graph datasets. In our proposed approach, we model relations on
external temporal knowledge graphs as events of a Hawkes process on graphs.
With extensive experiments, we show that learned dynamic representations
effectively rank stocks based on returns across multiple holding periods,
outperforming related baselines on relevant metrics.

</details>

### [314] [Deep Learning vs. Black-Scholes: Option Pricing Performance on Brazilian Petrobras Stocks](https://arxiv.org/abs/2504.20088)
*Joao Felipe Gueiros,Hemanth Chandravamsi,Steven H. Frankel*

Main category: q-fin.ST

TLDR: 论文探讨了使用深度残差网络为巴西石油公司（Petrobras）的欧式期权定价，并与Black-Scholes模型进行比较，结果显示深度模型在误差减少和长期预测准确性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证深度学习方法在金融期权定价中的有效性，尤其是与传统Black-Scholes模型相比的性能优势。

Method: 通过从巴西证券交易所（B3）爬取的八年历史数据，使用自定义混合损失函数训练深度残差网络模型，数据分为80%训练集和20%测试集。

Result: 深度模型在测试集上实现了64.3%的平均绝对误差减少，且在长期到期期限下表现更稳定。

Conclusion: 深度学习在金融建模中具有潜力，未来可针对不同价格区间开发专用模型。

Abstract: This paper explores the use of deep residual networks for pricing European
options on Petrobras, one of the world's largest oil and gas producers, and
compares its performance with the Black-Scholes (BS) model. Using eight years
of historical data from B3 (Brazilian Stock Exchange) collected via web
scraping, a deep learning model was trained using a custom built hybrid loss
function that incorporates market data and analytical pricing. The data for
training and testing were drawn between the period spanning November 2016 to
January 2025, using an 80-20 train-test split. The test set consisted of data
from the final three months: November, December, and January 2025. The deep
residual network model achieved a 64.3\% reduction in the mean absolute error
for the 3-19 BRL (Brazilian Real) range when compared to the Black-Scholes
model on the test set. Furthermore, unlike the Black-Scholes solution, which
tends to decrease its accuracy for longer periods of time, the deep learning
model performed accurately for longer expiration periods. These findings
highlight the potential of deep learning in financial modeling, with future
work focusing on specialized models for different price ranges.

</details>

<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [315] [Towards Easy and Realistic Network Infrastructure Testing for Large-scale Machine Learning](https://arxiv.org/abs/2504.20854)
*Jinsun Yoo,ChonLam Lao,Lianjie Cao,Bob Lantz,Minlan Yu,Tushar Krishna,Puneet Sharma*

Main category: cs.NI

TLDR: Genie是一个测试框架，通过CPU模拟GPU通信，结合ASTRA-sim模拟器评估网络对ML性能的影响。


<details>
  <summary>Details</summary>
Motivation: 研究真实硬件网络行为对ML工作负载性能的影响，避免使用昂贵的GPU。

Method: 利用CPU生成流量模拟GPU间通信，并改造ASTRA-sim模拟器建模网络与ML的交互。

Result: 成功构建了Genie框架，能够低成本评估网络对ML性能的影响。

Conclusion: Genie为研究网络对ML性能的影响提供了高效且经济的解决方案。

Abstract: This paper lays the foundation for Genie, a testing framework that captures
the impact of real hardware network behavior on ML workload performance,
without requiring expensive GPUs. Genie uses CPU-initiated traffic over a
hardware testbed to emulate GPU to GPU communication, and adapts the ASTRA-sim
simulator to model interaction between the network and the ML workload.

</details>

<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [316] [Exploiting inter-agent coupling information for efficient reinforcement learning of cooperative LQR](https://arxiv.org/abs/2504.20927)
*Shahbaz P Qadri Syed,He Bai*

Main category: eess.SY

TLDR: 提出了一种基于代理间耦合信息的精确分解方法，改进了多代理强化学习的样本效率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中局部Q函数分解不精确的问题，利用代理间耦合信息提升多代理控制的效率和可扩展性。

Method: 提出系统化的局部Q函数精确分解方法，开发近似最小二乘策略迭代算法，并设计两种架构学习局部Q函数。

Result: 分解的最坏样本复杂度与集中式情况相同，且通过图形条件实现了更高的样本效率。数值实验验证了改进效果。

Conclusion: 该方法在多代理强化学习中显著提升了样本和计算效率，为实际应用提供了更高效的解决方案。

Abstract: Developing scalable and efficient reinforcement learning algorithms for
cooperative multi-agent control has received significant attention over the
past years. Existing literature has proposed inexact decompositions of local
Q-functions based on empirical information structures between the agents. In
this paper, we exploit inter-agent coupling information and propose a
systematic approach to exactly decompose the local Q-function of each agent. We
develop an approximate least square policy iteration algorithm based on the
proposed decomposition and identify two architectures to learn the local
Q-function for each agent. We establish that the worst-case sample complexity
of the decomposition is equal to the centralized case and derive necessary and
sufficient graphical conditions on the inter-agent couplings to achieve better
sample efficiency. We demonstrate the improved sample efficiency and
computational efficiency on numerical examples.

</details>

<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [317] [DejaVuzz: Disclosing Transient Execution Bugs with Dynamic Swappable Memory and Differential Information Flow Tracking assisted Processor Fuzzing](https://arxiv.org/abs/2504.20934)
*Jinyan Xu,Yangye Zhou,Xingzhi Zhang,Yinshuai Li,Qinhan Tan,Yinqian Zhang,Yajin Zhou,Rui Chang,Wenbo Shen*

Main category: cs.AR

TLDR: DejaVuzz是一种新型的预硅阶段处理器瞬态执行漏洞模糊测试工具，通过动态可交换内存和差分信息流跟踪技术，显著提升了漏洞检测效率。


<details>
  <summary>Details</summary>
Motivation: 现代处理器中的瞬态执行漏洞已成为严重威胁，现有硬件模糊测试技术因微架构可控性和可观测性不足，检测效果不佳。

Method: DejaVuzz采用动态可交换内存和差分信息流跟踪技术，隔离指令流并追踪敏感数据传播，结合污点覆盖矩阵和污点活跃度标注指导变异。

Result: DejaVuzz在性能上优于现有工具SpecDoctor，触发更全面的瞬态窗口，覆盖提升4.7倍，并发现5个新漏洞（6个CVE）。

Conclusion: DejaVuzz通过创新技术有效提升了瞬态执行漏洞的检测能力，为处理器安全提供了新工具。

Abstract: Transient execution vulnerabilities have emerged as a critical threat to
modern processors. Hardware fuzzing testing techniques have recently shown
promising results in discovering transient execution bugs in large-scale
out-of-order processor designs. However, their poor microarchitectural
controllability and observability prevent them from effectively and efficiently
detecting transient execution vulnerabilities.
  This paper proposes DejaVuzz, a novel pre-silicon stage processor transient
execution bug fuzzer. DejaVuzz utilizes two innovative operating primitives:
dynamic swappable memory and differential information flow tracking, enabling
more effective and efficient transient execution vulnerability detection. The
dynamic swappable memory enables the isolation of different instruction streams
within the same address space. Leveraging this capability, DejaVuzz generates
targeted training for arbitrary transient windows and eliminates ineffective
training, enabling efficient triggering of diverse transient windows. The
differential information flow tracking aids in observing the propagation of
sensitive data across the microarchitecture. Based on taints, DejaVuzz designs
the taint coverage matrix to guide mutation and uses taint liveness annotations
to identify exploitable leakages. Our evaluation shows that DejaVuzz
outperforms the state-of-the-art fuzzer SpecDoctor, triggering more
comprehensive transient windows with lower training overhead and achieving a
4.7x coverage improvement. And DejaVuzz also mitigates control flow
over-tainting with acceptable overhead and identifies 5 previously undiscovered
transient execution vulnerabilities (with 6 CVEs assigned) on BOOM and
XiangShan.

</details>

<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [318] [An Algebraic Approach to Asymmetric Delegation and Polymorphic Label Inference (Technical Report)](https://arxiv.org/abs/2504.20432)
*Silei Ren,Coşku Acay,Andrew C. Myers*

Main category: cs.PL

TLDR: 该论文研究了基于格的IFC标签模型的代数语义，提出了一种支持非对称委托的语义框架，并设计了静态检查NMIF的算法和标签推断程序。


<details>
  <summary>Details</summary>
Motivation: 现有IFC标签模型在建模某些安全假设（如半诚实代理）时存在局限性，因此需要更灵活的框架。

Method: 研究基于格的IFC标签模型的代数语义，提出支持非对称委托的语义框架，设计静态检查NMIF的算法和标签推断程序。

Result: 提出的框架支持信息降级并通过NMIF确保安全性，实现了高效的标签多态性支持。

Conclusion: 该框架解决了IFC标签模型的局限性，并展示了其在实际应用中的可行性。

Abstract: Language-based information flow control (IFC) enables reasoning about and
enforcing security policies in decentralized applications. While information
flow properties are relatively extensional and compositional, designing
expressive systems that enforce such properties remains challenging. In
particular, it can be difficult to use IFC labels to model certain security
assumptions, such as semi-honest agents.
  Motivated by these modeling limitations, we study the algebraic semantics of
lattice-based IFC label models, and propose a semantic framework that allows
formalizing asymmetric delegation, which is partial delegation of
confidentiality or integrity. Our framework supports downgrading of information
and ensures their safety through nonmalleable information flow (NMIF).
  To demonstrate the practicality of our framework, we design and implement a
novel algorithm that statically checks NMIF and a label inference procedure
that efficiently supports bounded label polymorphism, allowing users to write
code generic with respect to labels.

</details>

<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [319] [TriniMark: A Robust Generative Speech Watermarking Method for Trinity-Level Attribution](https://arxiv.org/abs/2504.20532)
*Yue Li,Weizhi Liu,Dongdong Lin*

Main category: cs.MM

TLDR: 提出了一种鲁棒的生成语音水印方法（TriniMark），用于认证生成内容并保护版权，通过轻量级水印编码器和时间感知门控卷积网络实现高效水印嵌入与提取。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型（如扩散模型）的进步，AI生成语音的逼真度提高，但现有检测技术难以应对其复杂性，且缺乏有效的水印保护方法以维护知识产权。

Method: 设计轻量级水印编码器将水印嵌入语音时域特征，并直接重构波形；采用时间感知门控卷积网络解码水印；提出波形引导微调策略，使扩散模型有效学习水印知识。

Result: 实验表明，该方法在对抗复合攻击时表现出卓越的鲁棒性，且水印可被代理模型学习并正确提取。

Conclusion: TriniMark为生成语音提供了高效的水印保护方案，解决了现有技术的不足，尤其在对抗复杂攻击时表现优异。

Abstract: The emergence of diffusion models has facilitated the generation of speech
with reinforced fidelity and naturalness. While deepfake detection technologies
have manifested the ability to identify AI-generated content, their efficacy
decreases as generative models become increasingly sophisticated. Furthermore,
current research in the field has not adequately addressed the necessity for
robust watermarking to safeguard the intellectual property rights associated
with synthetic speech and generative models. To remedy this deficiency, we
propose a \textbf{ro}bust generative \textbf{s}peech wat\textbf{e}rmarking
method (TriniMark) for authenticating the generated content and safeguarding
the copyrights by enabling the traceability of the diffusion model. We first
design a structure-lightweight watermark encoder that embeds watermarks into
the time-domain features of speech and reconstructs the waveform directly. A
temporal-aware gated convolutional network is meticulously designed in the
watermark decoder for bit-wise watermark recovery. Subsequently, the
waveform-guided fine-tuning strategy is proposed for fine-tuning the diffusion
model, which leverages the transferability of watermarks and enables the
diffusion model to incorporate watermark knowledge effectively. When an
attacker trains a surrogate model using the outputs of the target model, the
embedded watermark can still be learned by the surrogate model and correctly
extracted. Comparative experiments with state-of-the-art methods demonstrate
the superior robustness of our method, particularly in countering compound
attacks.

</details>

### [320] [TrueFake: A Real World Case Dataset of Last Generation Fake Images also Shared on Social Networks](https://arxiv.org/abs/2504.20658)
*Stefano Dell'Anna,Andrea Montibeller,Giulia Boato*

Main category: cs.MM

TLDR: TrueFake是一个包含60万张图像的大规模基准数据集，用于评估社交媒体共享对伪造图像检测性能的影响，并识别当前最有效的检测和训练策略。


<details>
  <summary>Details</summary>
Motivation: AI生成的合成媒体在现实场景中广泛传播虚假信息，而现有取证工具未能充分应对社交媒体压缩等挑战。

Method: 构建TrueFake数据集，包含多种生成技术和三个社交网络的共享图像，进行广泛的实验分析。

Result: 研究发现社交媒体共享会显著影响检测性能，并确定了最有效的检测策略。

Conclusion: 强调需要在真实世界条件下评估取证模型的重要性。

Abstract: AI-generated synthetic media are increasingly used in real-world scenarios,
often with the purpose of spreading misinformation and propaganda through
social media platforms, where compression and other processing can degrade fake
detection cues. Currently, many forensic tools fail to account for these
in-the-wild challenges. In this work, we introduce TrueFake, a large-scale
benchmarking dataset of 600,000 images including top notch generative
techniques and sharing via three different social networks. This dataset allows
for rigorous evaluation of state-of-the-art fake image detectors under very
realistic and challenging conditions. Through extensive experimentation, we
analyze how social media sharing impacts detection performance, and identify
current most effective detection and training strategies. Our findings
highlight the need for evaluating forensic models in conditions that mirror
real-world use.

</details>

<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [321] [A Physically Driven Long Short Term Memory Model for Estimating Snow Water Equivalent over the Continental United States](https://arxiv.org/abs/2504.20129)
*Arun M. Saranathan,Mahmoud Saeedimoghaddam,Brandon Smith,Deepthi Raghunandan,Grey Nearing,Craig Pelissier*

Main category: physics.ao-ph

TLDR: 该论文提出了一种基于LSTM网络的方法，用于估计雪水当量（SWE），解决了现有方法计算成本高或空间覆盖不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的SWE估计方法（如再分析产品或现场测量）存在计算成本高或空间覆盖不足的局限性，需要一种更高效且广泛适用的解决方案。

Method: 使用LSTM网络，将SWE估计分为两个任务：分类（判断雪的存在与否）和回归（估计SWE高度）。模型基于SNOTEL数据进行训练。

Result: 模型在雪存在分类任务中准确率≥93%，SWE估计的相关系数约为0.9，并能泛化到未见过的时空数据。

Conclusion: LSTM模型能够高效且准确地估计SWE，具有广泛的应用潜力。

Abstract: Snow is an essential input for various land surface models. Seasonal snow
estimates are available as snow water equivalent (SWE) from process-based
reanalysis products or locally from in situ measurements. While the reanalysis
products are computationally expensive and available at only fixed spatial and
temporal resolutions, the in situ measurements are highly localized and sparse.
To address these issues and enable the analysis of the effect of a large suite
of physical, morphological, and geological conditions on the presence and
amount of snow, we build a Long Short-Term Memory (LSTM) network, which is able
to estimate the SWE based on time series input of the various
physical/meteorological factors as well static spatial/morphological factors.
Specifically, this model breaks down the SWE estimation into two separate
tasks: (i) a classification task that indicates the presence/absence of snow on
a specific day and (ii) a regression task that indicates the height of the SWE
on a specific day in the case of snow presence. The model is trained using
physical/in situ SWE measurements from the SNOw TELemetry (SNOTEL) snow pillows
in the western United States. We will show that trained LSTM models have a
classification accuracy of $\geq 93\%$ for the presence of snow and a
coefficient of correlation of $\sim 0.9$ concerning their SWE estimates. We
will also demonstrate that the models can generalize both spatially and
temporally to previously unseen data.

</details>

### [322] [Testing the Limit of Atmospheric Predictability with a Machine Learning Weather Model](https://arxiv.org/abs/2504.20238)
*P. Trent Vonich,Gregory J. Hakim*

Main category: physics.ao-ph

TLDR: 使用机器学习模型GraphCast优化初始条件，显著提升天气预报技能，突破传统14天预测极限。


<details>
  <summary>Details</summary>
Motivation: 挑战传统大气可预测性14天的极限，探索机器学习在天气预报中的潜力。

Method: 采用GraphCast模型，通过梯度优化技术调整初始条件，进行2020年每日两次的预报实验。

Result: 10天预报误差平均减少86%，技能持续超过30天；优化初始条件在Pangu-Weather模型中误差减少21%。

Conclusion: 准确初始条件下，确定性预报技能可远超两周，颠覆传统大气可预测性假设。

Abstract: Atmospheric predictability research has long held that the limit of skillful
deterministic weather forecasts is about 14 days. We challenge this limit using
GraphCast, a machine-learning weather model, by optimizing forecast initial
conditions using gradient-based techniques for twice-daily forecasts spanning
2020. This approach yields an average error reduction of 86% at 10 days, with
skill lasting beyond 30 days. Mean optimal initial-condition perturbations
reveal large-scale, spatially coherent corrections to ERA5, primarily
reflecting an intensification of the Hadley circulation. Forecasts using
GraphCast-optimal initial conditions in the Pangu-Weather model achieve a 21%
error reduction, peaking at 4 days, indicating that analysis corrections
reflect a combination of both model bias and a reduction in analysis error.
These results demonstrate that, given accurate initial conditions, skillful
deterministic forecasts are consistently achievable far beyond two weeks,
challenging long-standing assumptions about the limits of atmospheric
predictability.

</details>

### [323] [Data Driven Deep Learning for Correcting Global Climate Model Projections of SST and DSL in the Bay of Bengal](https://arxiv.org/abs/2504.20620)
*Abhishek Pasula,Deepak N. Subramani*

Main category: physics.ao-ph

TLDR: 论文提出了一种基于深度学习的偏差校正方法，用于修正CMIP6气候模型在孟加拉湾的预测误差，显著降低了海表温度和动态海平面的均方根误差。


<details>
  <summary>Details</summary>
Motivation: 气候变化影响海洋条件，而CMIP6模型在孟加拉湾的预测与再分析数据存在显著差异，需要更准确的偏差校正方法。

Method: 使用深度学习模型，以气候模型输出为输入，ORAS5再分析数据为输出，进行训练和验证，并与传统EDCDF方法对比。

Result: 新方法将SST和DSL的RMSE分别降低了0.15C和0.3 m，优于传统方法。

Conclusion: 深度学习模型有效修正了气候预测偏差，为未来气候分析提供了更可靠的数据。

Abstract: Climate change alters ocean conditions, notably temperature and sea level. In
the Bay of Bengal, these changes influence monsoon precipitation and marine
productivity, critical to the Indian economy. In Phase 6 of the Coupled Model
Intercomparison Project (CMIP6), Global Climate Models (GCMs) use different
shared socioeconomic pathways (SSPs) to obtain future climate projections.
However, significant discrepancies are observed between these models and the
reanalysis data in the Bay of Bengal for 2015-2024. Specifically, the root mean
square error (RMSE) between the climate model output and the Ocean Reanalysis
System (ORAS5) is 1.2C for the sea surface temperature (SST) and 1.1 m for the
dynamic sea level (DSL). We introduce a new data-driven deep learning model to
correct for this bias. The deep neural model for each variable is trained using
pairs of climatology-removed monthly climate projections as input and the
corresponding month's ORAS5 as output. This model is trained with historical
data (1950 to 2014), validated with future projection data from 2015 to 2020,
and tested with future projections from 2021 to 2023. Compared to the
conventional EquiDistant Cumulative Distribution Function (EDCDF) statistical
method for bias correction in climate models, our approach decreases RMSE by
0.15C for SST and 0.3 m for DSL. The trained model subsequently corrects the
projections for 2024-2100. A detailed analysis of the monthly, seasonal, and
decadal means and variability is performed to underscore the implications of
the novel dynamics uncovered in our corrected projections.

</details>

<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [324] [Efficient patient-centric EMR sharing block tree](https://arxiv.org/abs/2504.20544)
*Xiaohan Hu,Jyoti Sahni,Colin R. Simpson,Normalia Samian,Winston K. G. Seah*

Main category: cs.DC

TLDR: 提出了一种名为MedBlockTree的新型区块链数据结构，旨在解决基于区块链的电子病历系统的可扩展性问题，特别是低区块吞吐量和患者感知问题。


<details>
  <summary>Details</summary>
Motivation: 电子病历（EMR）的分散存储增加了管理复杂性，区块链虽能解决互操作性、数据所有权和信任问题，但面临可扩展性挑战。

Method: 设计MedBlockTree，利用变色龙哈希函数生成碰撞块，将单链扩展为具有$n$分支的块树，并引入EnhancedPro共识算法管理多分支。

Result: 通过模拟评估，MedBlockTree在吞吐量上显著优于传统区块链EMR系统，处理速度提升$\nu\cdot n$倍。

Conclusion: MedBlockTree有效解决了区块链EMR系统的可扩展性问题，具有实际应用潜力。

Abstract: Flexible sharing of electronic medical records (EMRs) is an urgent need in
healthcare, as fragmented storage creates EMR management complexity for both
practitioners and patients. Blockchain has emerged as a promising solution to
address the limitations of centralized EMR systems regarding interoperability,
data ownership, and trust concerns. Whilst its healthcare implementation
continues to face scalability challenges, particularly in uploading lag time as
EMR volumes increase. In this paper, we describe the design of a novel
blockchain-based data structure, MedBlockTree, which aims to solve the
scalability issue in blockchain-based EMR systems, particularly low block
throughput and patient awareness. MedBlockTree leverages a chameleon hash
function to generate collision blocks for existing patients and expand a single
chain into a growing block tree with $n$ branches that are capable of
processing $n$ new blocks in a single consensus round. We also introduce the
EnhancedPro consensus algorithm to manage multiple branches and maintain
network consistency. Our comprehensive simulation evaluates performance across
four dimensions: branch number, worker number, collision rate, and network
latency. Comparative analysis against a traditional blockchain-based EMR system
demonstrates outstanding throughput improvements across all dimensions,
achieving processing speeds $\nu\cdot n$ times faster than conventional
approaches.

</details>

### [325] [EPSILON: Adaptive Fault Mitigation in Approximate Deep Neural Network using Statistical Signatures](https://arxiv.org/abs/2504.20074)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.DC

TLDR: EPSILON是一个轻量级框架，用于在近似计算深度神经网络加速器（AxDNNs）中高效检测和缓解故障，同时保持能效优势。


<details>
  <summary>Details</summary>
Motivation: 传统故障检测和缓解方法在AxDNNs中引入高开销和延迟，不适用于能源受限的实时部署。

Method: EPSILON利用预计算的统计特征和逐层重要性指标，采用非参数模式匹配算法实现恒定时间故障检测，并根据权重分布和层关键性动态调整缓解策略。

Result: EPSILON在多种场景下保持80.05%的准确率，推理时间提升22%，能效提升28%。

Conclusion: EPSILON为安全关键边缘应用中可靠部署AxDNNs提供了实用解决方案。

Abstract: The increasing adoption of approximate computing in deep neural network
accelerators (AxDNNs) promises significant energy efficiency gains. However,
permanent faults in AxDNNs can severely degrade their performance compared to
their accurate counterparts (AccDNNs). Traditional fault detection and
mitigation approaches, while effective for AccDNNs, introduce substantial
overhead and latency, making them impractical for energy-constrained real-time
deployment. To address this, we introduce EPSILON, a lightweight framework that
leverages pre-computed statistical signatures and layer-wise importance metrics
for efficient fault detection and mitigation in AxDNNs. Our framework
introduces a novel non-parametric pattern-matching algorithm that enables
constant-time fault detection without interrupting normal execution while
dynamically adapting to different network architectures and fault patterns.
EPSILON maintains model accuracy by intelligently adjusting mitigation
strategies based on a statistical analysis of weight distribution and layer
criticality while preserving the energy benefits of approximate computing.
Extensive evaluations across various approximate multipliers, AxDNN
architectures, popular datasets (MNIST, CIFAR-10, CIFAR-100, ImageNet-1k), and
fault scenarios demonstrate that EPSILON maintains 80.05\% accuracy while
offering 22\% improvement in inference time and 28\% improvement in energy
efficiency, establishing EPSILON as a practical solution for deploying reliable
AxDNNs in safety-critical edge applications.

</details>

### [326] [GenTorrent: Scaling Large Language Model Serving with An Overley Network](https://arxiv.org/abs/2504.20101)
*Fei Fang,Yifan Hua,Shengze Wang,Ruilin Zhou,Yi Liu,Chen Qian,Xiaoxue Zhang*

Main category: cs.DC

TLDR: GenTorrent是一种利用去中心化计算资源的LLM服务覆盖网络，解决了服务扩展性问题，显著降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 开源和低成本LLM的服务扩展性对小组织和个体开发者仍是挑战，受P2P网络启发，提出去中心化解决方案。

Method: 提出GenTorrent覆盖网络，解决四个关键问题：网络组织、隐私保护、资源效率转发和服务质量验证。

Result: 原型测试显示延迟降低50%，安全功能对性能影响极小。

Conclusion: 为未来AI服务的民主化和扩展性开辟了新方向。

Abstract: While significant progress has been made in research and development on
open-source and cost-efficient large-language models (LLMs), serving
scalability remains a critical challenge, particularly for small organizations
and individuals seeking to deploy and test their LLM innovations. Inspired by
peer-to-peer networks that leverage decentralized overlay nodes to increase
throughput and availability, we propose GenTorrent, an LLM serving overlay that
harnesses computing resources from decentralized contributors. We identify four
key research problems inherent to enabling such a decentralized infrastructure:
1) overlay network organization; 2) LLM communication privacy; 3) overlay
forwarding for resource efficiency; and 4) verification of serving quality.
This work presents the first systematic study of these fundamental problems in
the context of decentralized LLM serving. Evaluation results from a prototype
implemented on a set of decentralized nodes demonstrate that GenTorrent
achieves a latency reduction of over 50% compared to the baseline design
without overlay forwarding. Furthermore, the security features introduce
minimal overhead to serving latency and throughput. We believe this work
pioneers a new direction for democratizing and scaling future AI serving
capabilities.

</details>

### [327] [Electricity Cost Minimization for Multi-Workflow Allocation in Geo-Distributed Data Centers](https://arxiv.org/abs/2504.20105)
*Shuang Wang,He Zhang,Tianxing Wu,Yueyou Zhang,Wei Emma Zhang,Quan Z. Sheng*

Main category: cs.DC

TLDR: 论文提出了一种地理分布式数据中心（GDCs）中电力成本感知的多工作流调度算法（ECMWS），以降低电力成本并满足工作流应用的截止时间约束。


<details>
  <summary>Details</summary>
Motivation: 全球分布的GDCs为大规模工作流应用提供计算和存储服务，导致高电力成本，且成本因地理位置和时间而异。如何在满足工作流截止时间的同时降低电力成本是一个重要问题。

Method: 提出ECMWS算法，包括工作流排序、截止时间划分、任务排序和资源分配四个阶段，利用图嵌入模型和策略网络解决马尔可夫决策过程（MDP）。

Result: 实验结果表明，ECMWS算法显著优于现有方法，性能提升超过15%，同时保持可接受的计算时间。

Conclusion: ECMWS算法有效解决了GDCs中电力成本和工作流调度的挑战，具有实际应用价值。

Abstract: Worldwide, Geo-distributed Data Centers (GDCs) provide computing and storage
services for massive workflow applications, resulting in high electricity costs
that vary depending on geographical locations and time. How to reduce
electricity costs while satisfying the deadline constraints of workflow
applications is important in GDCs, which is determined by the execution time of
servers, power, and electricity price. Determining the completion time of
workflows with different server frequencies can be challenging, especially in
scenarios with heterogeneous computing resources in GDCs. Moreover, the
electricity price is also different in geographical locations and may change
dynamically. To address these challenges, we develop a geo-distributed system
architecture and propose an Electricity Cost aware Multiple Workflows
Scheduling algorithm (ECMWS) for servers of GDCs with fixed frequency and
power. ECMWS comprises four stages, namely workflow sequencing, deadline
partitioning, task sequencing, and resource allocation where two graph
embedding models and a policy network are constructed to solve the Markov
Decision Process (MDP). After statistically calibrating parameters and
algorithm components over a comprehensive set of workflow instances, the
proposed algorithms are compared with the state-of-the-art methods over two
types of workflow instances. The experimental results demonstrate that our
proposed algorithm significantly outperforms other algorithms, achieving an
improvement of over 15\% while maintaining an acceptable computational time.
The source codes are available at
https://gitee.com/public-artifacts/ecmws-experiments.

</details>

### [328] [Tempo: Application-aware LLM Serving with Mixed SLO Requirements](https://arxiv.org/abs/2504.20068)
*Wei Zhang,Zhiyu Wu,Yi Mu,Banruo Liu,Myungjin Lee,Fan Lai*

Main category: cs.DC

TLDR: Tempo是一种新型的SLO感知调度器，专为多样化LLM工作负载设计，通过优化服务增益和资源分配，显著提升了性能和SLO达成率。


<details>
  <summary>Details</summary>
Motivation: 现有调度器无法满足多样化LLM工作负载的需求，尤其是面对动态依赖和不可预测的请求信息时表现不佳。

Method: Tempo采用混合调度策略，包括基于分位数的响应上限估计、依赖图匹配、按服务增益密度优先级调度，以及在线动态调整。

Result: 实验表明，Tempo在端到端服务增益上提升高达8.3倍，SLO达成率提升高达10.3倍。

Conclusion: Tempo通过系统化的SLO感知调度，显著优化了LLM工作负载的性能和资源利用率。

Abstract: The integration of Large Language Models (LLMs) into diverse applications,
ranging from interactive chatbots and cloud AIOps to intelligent agents, has
introduced a wide spectrum of Service Level Objectives (SLOs) for
responsiveness. These workloads include latency-sensitive requests focused on
per-token latency in streaming chat, throughput-intensive requests that require
rapid full responses to invoke tools, and collective requests with dynamic
dependencies arising from self-reflection or agent-based reasoning. This
workload diversity, amplified by unpredictable request information such as
response lengths and runtime dependencies, makes existing schedulers inadequate
even within their design envelopes.
  In this paper, we define service gain as the useful service delivered by
completing requests. We observe that as SLO directly reflects the actual
performance needs of requests, completing a request much faster than its SLO
(e.g., deadline) yields limited additional service gain. Based on this insight,
we introduce Tempo, the first systematic SLO-aware scheduler designed to
maximize service gain across diverse LLM workloads. Tempo allocates just enough
serving bandwidth to meet each SLO, maximizing residual capacity for others
best-effort workloads. Instead of assuming request information or none at all,
it adopts a hybrid scheduling strategy: using quantile-based response upper
bounds and dependency-graph matching for conservative initial estimates,
prioritizing requests by service gain density, and refining decisions online as
generation progresses. Our evaluation across diverse workloads, including chat,
reasoning, and agentic pipelines, shows that Tempo improves end-to-end service
gain by up to 8.3$\times$ and achieves up to 10.3$\times$ SLO goodput compared
to state-of-the-art designs

</details>

### [329] [Leveraging Neural Graph Compilers in Machine Learning Research for Edge-Cloud Systems](https://arxiv.org/abs/2504.20198)
*Alireza Furutanpey,Carmen Walser,Philipp Raith,Pantelis A. Frangoudis,Schahram Dustdar*

Main category: cs.DC

TLDR: 本文评估了神经网络图编译器在异构硬件平台上的表现，揭示了编译器优化对性能比较的影响，并提出了量化编译器能力的新指标。


<details>
  <summary>Details</summary>
Motivation: 解决理论优化技术与实际部署场景之间的关键差距，揭示编译器优化对性能比较的影响。

Method: 通过系统分析和细粒度块级实验，评估不同神经网络架构和批量大小下的编译器性能。

Result: 发现编译器性能高度依赖架构和批量大小，且特定编译器能通过简单架构中的重复模式显著提升吞吐量。

Conclusion: 本文方法为实践者在异构硬件环境中优化性能提供了实用见解，填补了学术研究与实际部署之间的鸿沟。

Abstract: This work presents a comprehensive evaluation of neural network graph
compilers across heterogeneous hardware platforms, addressing the critical gap
between theoretical optimization techniques and practical deployment scenarios.
We demonstrate how vendor-specific optimizations can invalidate relative
performance comparisons between architectural archetypes, with performance
advantages sometimes completely reversing after compilation. Our systematic
analysis reveals that graph compilers exhibit performance patterns highly
dependent on both neural architecture and batch sizes. Through fine-grained
block-level experimentation, we establish that vendor-specific compilers can
leverage repeated patterns in simple architectures, yielding disproportionate
throughput gains as model depth increases. We introduce novel metrics to
quantify a compiler's ability to mitigate performance friction as batch size
increases. Our methodology bridges the gap between academic research and
practical deployment by incorporating compiler effects throughout the research
process, providing actionable insights for practitioners navigating complex
optimization landscapes across heterogeneous hardware environments.

</details>

<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [330] [Optimizing Hard Thresholding for Sparse Model Discovery](https://arxiv.org/abs/2504.20256)
*Derek W. Jollie,Scott G. McCalla*

Main category: math.OC

TLDR: 通过引入退火方案重新激活部分被移除的项，改进了稀疏字典学习算法的性能，提高了模型精度。


<details>
  <summary>Details</summary>
Motivation: 稀疏字典学习算法通常通过硬阈值过程移除库元素以实现稀疏激活，但可能导致性能不足。

Method: 提出退火方案，重新激活部分被移除的项，并结合冷却计划，应用于SINDy和硬阈值追踪两种优化方法。

Result: 在多种非线性系统（对流流动、可激发系统和种群动力学）和实验数据（抛体运动）中，退火方案显著提高了模型精度。

Conclusion: 退火方案能有效改进稀疏学习算法的性能，适用于多种实际系统。

Abstract: Many model selection algorithms rely on sparse dictionary learning to provide
interpretable and physics-based governing equations. The optimization
algorithms typically use a hard thresholding process to enforce sparse
activations in the model coefficients by removing library elements from
consideration. By introducing an annealing scheme that reactivates a fraction
of the removed terms with a cooling schedule, we are able to improve the
performance of these sparse learning algorithms. We concentrate on two
approaches to the optimization, SINDy, and an alternative using hard
thresholding pursuit. We see in both cases that annealing can improve model
accuracy. The effectiveness of annealing is demonstrated through comparisons on
several nonlinear systems pulled from convective flows, excitable systems, and
population dynamics. Finally we apply these algorithms to experimental data for
projectile motion.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [331] [SCOPE-MRI: Bankart Lesion Detection as a Case Study in Data Curation and Deep Learning for Challenging Diagnoses](https://arxiv.org/abs/2504.20405)
*Sahil Sethi,Sai Reddy,Mansi Sakarvadia,Jordan Serotte,Darlington Nwaudo,Nicholas Maassen,Lewis Shi*

Main category: eess.IV

TLDR: 该研究提出了ScopeMRI数据集和深度学习框架，用于检测Bankart病变，模型在标准MRI和MRA上表现优异，达到放射科医生水平。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注易于诊断的病理，而Bankart病变等难题未被充分探索，临床依赖侵入性MRA。

Method: 使用CNN和transformer结合的方法，训练独立模型处理标准MRI和MRA，并通过多视图集成优化性能。

Result: 模型在标准MRI和MRA上的AUC分别为0.91和0.93，敏感性和特异性表现优异，且非侵入性MRI诊断效果媲美MRA。

Conclusion: 深度学习模型可减少对侵入性MRA的依赖，ScopeMRI和代码开源将加速肌肉骨骼影像研究。

Abstract: While deep learning has shown strong performance in musculoskeletal imaging,
existing work has largely focused on pathologies where diagnosis is not a
clinical challenge, leaving more difficult problems underexplored, such as
detecting Bankart lesions (anterior-inferior glenoid labral tears) on standard
MRIs. Diagnosing these lesions is challenging due to their subtle imaging
features, often leading to reliance on invasive MRI arthrograms (MRAs). This
study introduces ScopeMRI, the first publicly available, expert-annotated
dataset for shoulder pathologies, and presents a deep learning (DL) framework
for detecting Bankart lesions on both standard MRIs and MRAs. ScopeMRI includes
586 shoulder MRIs (335 standard, 251 MRAs) from 558 patients who underwent
arthroscopy. Ground truth labels were derived from intraoperative findings, the
gold standard for diagnosis. Separate DL models for MRAs and standard MRIs were
trained using a combination of CNNs and transformers. Predictions from
sagittal, axial, and coronal views were ensembled to optimize performance. The
models were evaluated on a 20% hold-out test set (117 MRIs: 46 MRAs, 71
standard MRIs). The models achieved an AUC of 0.91 and 0.93, sensitivity of 83%
and 94%, and specificity of 91% and 86% for standard MRIs and MRAs,
respectively. Notably, model performance on non-invasive standard MRIs matched
or surpassed radiologists interpreting MRAs. External validation demonstrated
initial generalizability across imaging protocols. This study demonstrates that
DL models can achieve radiologist-level diagnostic performance on standard
MRIs, reducing the need for invasive MRAs. By releasing ScopeMRI and a modular
codebase for training and evaluating deep learning models on 3D medical imaging
data, we aim to accelerate research in musculoskeletal imaging and support the
development of new datasets for clinically challenging diagnostic tasks.

</details>

### [332] [LymphAtlas- A Unified Multimodal Lymphoma Imaging Repository Delivering AI-Enhanced Diagnostic Insight](https://arxiv.org/abs/2504.20454)
*Jiajun Ding,Beiyao Zhu,Xiaosheng Liu,Lishen Zhang,Zhao Liu*

Main category: eess.IV

TLDR: 该研究整合PET代谢信息与CT解剖结构，构建了基于全身FDG PET/CT检查的淋巴瘤3D多模态分割数据集，填补了血液恶性肿瘤领域标准化多模态分割数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 解决血液恶性肿瘤领域缺乏标准化多模态分割数据集的问题，促进淋巴瘤的精准诊断和治疗。

Method: 回顾性收集483例患者数据，基于nnUNet格式构建高质量数据集，并通过深度学习模型验证其分割准确性。

Result: 数据集支持深度学习模型实现高精度、鲁棒性和可重复性的淋巴瘤病灶分割，显著提升肿瘤形态、位置和代谢特征的描述。

Conclusion: 该数据集为淋巴瘤的早期诊断、临床分期和个性化治疗提供了可靠数据支持，推动了基于深度学习的自动化图像分割和精准医学发展。

Abstract: This study integrates PET metabolic information with CT anatomical structures
to establish a 3D multimodal segmentation dataset for lymphoma based on
whole-body FDG PET/CT examinations, which bridges the gap of the lack of
standardised multimodal segmentation datasets in the field of haematological
malignancies. We retrospectively collected 483 examination datasets acquired
between March 2011 and May 2024, involving 220 patients (106 non-Hodgkin
lymphoma, 42 Hodgkin lymphoma); all data underwent ethical review and were
rigorously de-identified. Complete 3D structural information was preserved
during data acquisition, preprocessing and annotation, and a high-quality
dataset was constructed based on the nnUNet format. By systematic technical
validation and evaluation of the preprocessing process, annotation quality and
automatic segmentation algorithm, the deep learning model trained based on this
dataset is verified to achieve accurate segmentation of lymphoma lesions in
PET/CT images with high accuracy, good robustness and reproducibility, which
proves the applicability and stability of this dataset in accurate segmentation
and quantitative analysis. The deep fusion of PET/CT images achieved with this
dataset not only significantly improves the accurate portrayal of the
morphology, location and metabolic features of tumour lesions, but also
provides solid data support for early diagnosis, clinical staging and
personalized treatment, and promotes the development of automated image
segmentation and precision medicine based on deep learning. The dataset and
related resources are available at https://github.com/SuperD0122/LymphAtlas-.

</details>

### [333] [SAM-Guided Robust Representation Learning for One-Shot 3D Medical Image Segmentation](https://arxiv.org/abs/2504.20501)
*Jia Wang,Yunan Mei,Jiarui Liu,Xin Fan*

Main category: eess.IV

TLDR: 提出了一种名为RRL-MedSAM的新框架，通过知识蒸馏和自提示解码器，将SAM模型适配到一次性3D医学图像分割任务中，显著提升了性能并减少了计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决SAM模型在一次性医学图像分割中依赖人工交互和高计算成本的问题。

Method: 采用双阶段知识蒸馏策略和互指数移动平均更新权重，结合自提示解码器提升分割性能。

Result: 在OASIS和CT-lung数据集上表现优于现有方法，轻量级编码器参数仅为SAM-Base的3%。

Conclusion: RRL-MedSAM成功地将SAM模型适配到一次性医学图像分割任务中，显著提升了效率和性能。

Abstract: One-shot medical image segmentation (MIS) is crucial for medical analysis due
to the burden of medical experts on manual annotation. The recent emergence of
the segment anything model (SAM) has demonstrated remarkable adaptation in MIS
but cannot be directly applied to one-shot medical image segmentation (MIS) due
to its reliance on labor-intensive user interactions and the high computational
cost. To cope with these limitations, we propose a novel SAM-guided robust
representation learning framework, named RRL-MedSAM, to adapt SAM to one-shot
3D MIS, which exploits the strong generalization capabilities of the SAM
encoder to learn better feature representation. We devise a dual-stage
knowledge distillation (DSKD) strategy to distill general knowledge between
natural and medical images from the foundation model to train a lightweight
encoder, and then adopt a mutual exponential moving average (mutual-EMA) to
update the weights of the general lightweight encoder and medical-specific
encoder. Specifically, pseudo labels from the registration network are used to
perform mutual supervision for such two encoders. Moreover, we introduce an
auto-prompting (AP) segmentation decoder which adopts the mask generated from
the general lightweight model as a prompt to assist the medical-specific model
in boosting the final segmentation performance. Extensive experiments conducted
on three public datasets, i.e., OASIS, CT-lung demonstrate that the proposed
RRL-MedSAM outperforms state-of-the-art one-shot MIS methods for both
segmentation and registration tasks. Especially, our lightweight encoder uses
only 3\% of the parameters compared to the encoder of SAM-Base.

</details>

### [334] [Full-field surrogate modeling of cardiac function encoding geometric variability](https://arxiv.org/abs/2504.20479)
*Elena Martinez,Beatrice Moscoloni,Matteo Salvador,Fanwei Kong,Mathias Peirlinck,Alison Lesley Marsden*

Main category: eess.IV

TLDR: 结合物理建模与数据驱动方法，提出一种新的计算流程，通过生成合成心脏几何数据集训练神经网络，实现个性化心脏功能预测。


<details>
  <summary>Details</summary>
Motivation: 将计算方法转化为临床应用需要结合物理建模与数据驱动方法，但现有模型多为几何特异性，需针对不同患者和病理条件重新训练。

Method: 使用多尺度数学模型生成电生理模拟数据集，采用BLNMs编码激活图，并通过统计形状建模生成合成几何数据集。

Result: 提出的替代模型在复杂患者队列中表现出鲁棒性和良好泛化能力，平均无维均方误差为0.0034。

Conclusion: 该方法为心脏功能预测提供了一种高效且通用的解决方案，代码已开源。

Abstract: Combining physics-based modeling with data-driven methods is critical to
enabling the translation of computational methods to clinical use in
cardiology. The use of rigorous differential equations combined with machine
learning tools allows for model personalization with uncertainty quantification
in time frames compatible with clinical practice. However, accurate and
efficient surrogate models of cardiac function, built from physics-based
numerical simulation, are still mostly geometry-specific and require retraining
for different patients and pathological conditions. We propose a novel
computational pipeline to embed cardiac anatomies into full-field surrogate
models. We generate a dataset of electrophysiology simulations using a complex
multi-scale mathematical model coupling partial and ordinary differential
equations. We adopt Branched Latent Neural Maps (BLNMs) as an effective
scientific machine learning method to encode activation maps extracted from
physics-based numerical simulations into a neural network. Leveraging large
deformation diffeomorphic metric mappings, we build a biventricular anatomical
atlas and parametrize the anatomical variability of a small and challenging
cohort of 13 pediatric patients affected by Tetralogy of Fallot. We propose a
novel statistical shape modeling based z-score sampling approach to generate a
new synthetic cohort of 52 biventricular geometries that are compatible with
the original geometrical variability. This synthetic cohort acts as the
training set for BLNMs. Our surrogate model demonstrates robustness and great
generalization across the complex original patient cohort, achieving an average
adimensional mean squared error of 0.0034. The Python implementation of our
BLNM model is publicly available under MIT License at
https://github.com/StanfordCBCL/BLNM.

</details>

### [335] [Quality-factor inspired deep neural network solver for solving inverse scattering problems](https://arxiv.org/abs/2504.20504)
*Yutong Du,Zicheng Liu,Miao Cao,Zupeng Liang,Yali Zong,Changyou Li*

Main category: eess.IV

TLDR: 论文提出了一种基于质量因子的深度神经网络（QuaDNN）求解器，用于电磁逆散射问题，通过优化训练数据集、改进网络结构和损失函数，提高了成像性能。


<details>
  <summary>Details</summary>
Motivation: 解决电磁逆散射问题中成像性能受训练数据集、网络结构和损失函数影响的问题。

Method: 定义质量因子优化训练数据集；结合残差连接和通道注意力机制改进网络结构；设计包含数据拟合误差、物理信息约束和期望特征的损失函数。

Result: 数值分析和实验验证表明，QuaDNN能有效抑制背景伪影并提高重建精度。

Conclusion: QuaDNN在电磁逆散射问题中表现出优越的成像性能。

Abstract: Deep neural networks have been applied to address electromagnetic inverse
scattering problems (ISPs) and shown superior imaging performances, which can
be affected by the training dataset, the network architecture and the applied
loss function. Here, the quality of data samples is cared and valued by the
defined quality factor. Based on the quality factor, the composition of the
training dataset is optimized. The network architecture is integrated with the
residual connections and channel attention mechanism to improve feature
extraction. A loss function that incorporates data-fitting error,
physical-information constraints and the desired feature of the solution is
designed and analyzed to suppress the background artifacts and improve the
reconstruction accuracy. Various numerical analysis are performed to
demonstrate the superiority of the proposed quality-factor inspired deep neural
network (QuaDNN) solver and the imaging performance is finally verified by
experimental imaging test.

</details>

<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [336] [Learning Hierarchical Interaction for Accurate Molecular Property Prediction](https://arxiv.org/abs/2504.20127)
*Huiyang Hong,Xinkai Wu,Hongyu Sun,Qi Wang,Yuquan Li*

Main category: q-bio.BM

TLDR: 论文提出了一种名为HimNet的新模型，通过分层交互消息传递机制，有效捕捉分子结构的层次性，提升了分子属性预测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法（如GNN和Transformer）在分子属性预测中难以高效利用分子结构的层次性，且缺乏多级特征间的有效交互机制。

Method: 提出Hierarchical Interaction Message Passing Mechanism，通过分层注意力引导的消息传递，实现原子、基序和分子级别的交互感知表示学习。

Result: 在多个基准数据集上的实验表明，HimNet在大多数分子属性预测任务中表现最佳或接近最佳，并展现出良好的层次可解释性。

Conclusion: HimNet为分子活性和ADMET属性预测提供了准确高效的解决方案，对药物发现早期决策有重要贡献。

Abstract: Discovering molecules with desirable molecular properties, including ADMET
(Absorption, Distribution, Metabolism, Excretion, and Toxicity) profiles, is of
great importance in drug discovery. Existing approaches typically employ deep
learning models, such as Graph Neural Networks (GNNs) and Transformers, to
predict these molecular properties by learning from diverse chemical
information. However, these models often fail to efficiently capture and
utilize the hierarchical nature of molecular structures, and lack mechanisms
for effective interaction among multi-level features. To address these
limitations, we propose a Hierarchical Interaction Message Passing Mechanism,
which serves as the foundation of our novel model, HimNet. Our method enables
interaction-aware representation learning across atomic, motif, and molecular
levels via hierarchical attention-guided message passing. This design allows
HimNet to effectively balance global and local information, ensuring rich and
task-relevant feature extraction for downstream property prediction tasks, such
as Blood-Brain Barrier Permeability (BBBP). Extensive experiments on multiple
benchmark datasets demonstrate that HimNet achieves the best or near-best
performance in most molecular property prediction tasks. Furthermore, our
method exhibits promising hierarchical interpretability, aligning well with
chemical intuition on representative molecules. We believe that HimNet offers
an accurate and efficient solution for molecular activity and ADMET property
prediction, contributing significantly to advanced decision-making in the early
stages of drug discovery.

</details>

<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [337] [Nonlinear Computation with Linear Optics via Source-Position Encoding](https://arxiv.org/abs/2504.20401)
*N. Richardson,C. Bosch,R. P. Adams*

Main category: physics.optics

TLDR: 提出了一种在完全线性介质中实现非线性计算的新方法，通过位置编码和拓扑优化设计高效光学神经网络。


<details>
  <summary>Details</summary>
Motivation: 光学计算系统适合神经网络任务，但缺乏高效的非线性实现方法。

Method: 利用位置编码在低功耗下实现非线性计算，结合拓扑优化设计专用光学神经网络。

Result: 在分类任务中表现优于线性方法，与标准人工神经网络性能相当。

Conclusion: 该方法为光学神经网络提供了高效的非线性计算解决方案。

Abstract: Optical computing systems provide an alternate hardware model which appears
to be aligned with the demands of neural network workloads. However, the
challenge of implementing energy efficient nonlinearities in optics -- a key
requirement for realizing neural networks -- is a conspicuous missing link. In
this work we introduce a novel method to achieve nonlinear computation in fully
linear media. Our method can operate at low power and requires only the ability
to drive the optical system at a data-dependent spatial position. Leveraging
this positional encoding, we formulate a fully automated,
topology-optimization-based hardware design framework for extremely specialized
optical neural networks, drawing on modern advancements in optimization and
machine learning. We evaluate our optical designs on machine learning
classification tasks: demonstrating significant improvements over linear
methods, and competitive performance when compared to standard artificial
neural networks.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [338] [Narrative-Centered Emotional Reflection: Scaffolding Autonomous Emotional Literacy with AI](https://arxiv.org/abs/2504.20342)
*Shou-Tzu Han*

Main category: cs.HC

TLDR: Reflexion是一个AI驱动的平台，通过情感检测、反思提示和隐喻故事生成，帮助用户进行深层次的情感探索。


<details>
  <summary>Details</summary>
Motivation: 基于表达性写作、认知重构、自我决定和批判意识发展理论，旨在提升情感表达和心理韧性。

Method: 整合实时情感检测、分层反思提示和隐喻故事生成，支持从情感识别到行动规划的渐进过程。

Result: 初步研究显示，用户在情感表达、认知重构和心理韧性方面有积极效果。

Conclusion: Reflexion为可扩展的情感计算干预提供了有前景的方向，适用于教育、治疗和公共健康领域。

Abstract: Reflexion is an AI-powered platform designed to enable structured emotional
self-reflection at scale. By integrating real-time emotion detection, layered
reflective prompting, and metaphorical storytelling generation, Reflexion
empowers users to engage in autonomous emotional exploration beyond basic
sentiment categorization. Grounded in theories of expressive writing, cognitive
restructuring, self-determination, and critical consciousness development, the
system scaffolds a progressive journey from surface-level emotional recognition
toward value-aligned action planning. Initial pilot studies with diverse
participants demonstrate positive outcomes in emotional articulation, cognitive
reframing, and perceived psychological resilience. Reflexion represents a
promising direction for scalable, theory-informed affective computing
interventions aimed at fostering emotional literacy and psychological growth
across educational, therapeutic, and public health contexts.

</details>

### [339] [In defence of post-hoc explanations in medical AI](https://arxiv.org/abs/2504.20741)
*Joshua Hatherley,Lauritz Munch,Jens Christian Bjerring*

Main category: cs.HC

TLDR: 文章为后解释方法在医疗AI中的价值辩护，认为尽管其无法完全复制黑盒系统的推理过程，但仍能提升用户的功能性理解、提高临床团队准确性，并帮助决策。


<details>
  <summary>Details</summary>
Motivation: 反驳近期对后解释方法价值的批评，证明其在医疗AI中的实用性。

Method: 通过理论论证和分析，探讨后解释方法的实际作用。

Result: 后解释方法能提升用户理解、团队准确性和决策合理性。

Conclusion: 后解释方法虽非完美，但仍是解决医疗AI黑盒问题的有效策略。

Abstract: Since the early days of the Explainable AI movement, post-hoc explanations
have been praised for their potential to improve user understanding, promote
trust, and reduce patient safety risks in black box medical AI systems.
Recently, however, critics have argued that the benefits of post-hoc
explanations are greatly exaggerated since they merely approximate, rather than
replicate, the actual reasoning processes that black box systems take to arrive
at their outputs. In this article, we aim to defend the value of post-hoc
explanations against this recent critique. We argue that even if post-hoc
explanations do not replicate the exact reasoning processes of black box
systems, they can still improve users' functional understanding of black box
systems, increase the accuracy of clinician-AI teams, and assist clinicians in
justifying their AI-informed decisions. While post-hoc explanations are not a
"silver bullet" solution to the black box problem in medical AI, we conclude
that they remain a useful strategy for addressing the black box problem in
medical AI.

</details>