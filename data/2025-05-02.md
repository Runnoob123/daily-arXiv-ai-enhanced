<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 60]
- [cs.CL](#cs.CL) [Total: 66]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.CV](#cs.CV) [Total: 47]
- [cs.GR](#cs.GR) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [stat.AP](#stat.AP) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [eess.IV](#eess.IV) [Total: 8]
- [quant-ph](#quant-ph) [Total: 3]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [math.AC](#math.AC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [econ.EM](#econ.EM) [Total: 2]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [stat.ML](#stat.ML) [Total: 6]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.RO](#cs.RO) [Total: 15]
- [cs.SD](#cs.SD) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Security-by-Design at the Telco Edge with OSS: Challenges and Lessons Learned](https://arxiv.org/abs/2505.00111)
*Carmine Cesarano,Alessio Foggia,Gianluca Roscigno,Luca Andreani,Roberto Natella*

Main category: cs.CR

TLDR: 论文总结了在工业研发项目中保护基于开源软件的边缘计算平台GENIO的经验，包括威胁识别、缓解措施及开源安全解决方案的成熟度与局限性。


<details>
  <summary>Details</summary>
Motivation: 在被动光网络基础设施上部署边缘计算平台GENIO时，面临安全威胁，需通过开源软件实现有效防护。

Method: 采用硬化、漏洞管理、数字签名、静态和动态分析等方法识别威胁并实施缓解措施。

Result: 总结了使用开源软件实施安全措施的经验，并评估了这些解决方案在工业环境中的成熟度和局限性。

Conclusion: 开源安全解决方案在工业环境中具有一定成熟度，但仍存在局限性，需进一步优化。

Abstract: This paper presents our experience, in the context of an industrial R&D
project, on securing GENIO, a platform for edge computing on Passive Optical
Network (PON) infrastructures, and based on Open-Source Software (OSS). We
identify threats and related mitigations through hardening, vulnerability
management, digital signatures, and static and dynamic analysis. In particular,
we report lessons learned in applying these mitigations using OSS, and share
our findings about the maturity and limitations of these security solutions in
an industrial context.

</details>

### [2] [LLM-Based Threat Detection and Prevention Framework for IoT Ecosystems](https://arxiv.org/abs/2505.00240)
*Yazan Otoum,Arghavan Asad,Amiya Nayak*

Main category: cs.CR

TLDR: 本文提出了一种基于大型语言模型（LLM）的框架，用于物联网（IoT）环境中的威胁检测与预防，显著提升了检测精度和资源效率。


<details>
  <summary>Details</summary>
Motivation: 随着物联网规模和复杂性的增加，安全性成为关键问题，传统方法难以满足需求。

Method: 采用轻量级LLM，基于IoT特定数据集（IoT-23、TON_IoT）进行微调，实现实时异常检测和自动化缓解策略，并通过模块化Docker部署进行可扩展评估。

Result: 在模拟IoT环境中，实验结果显示检测精度、响应延迟和资源效率均优于传统方法。

Conclusion: 该框架展示了LLM驱动的自主安全解决方案在未来IoT生态系统中的潜力。

Abstract: The increasing complexity and scale of the Internet of Things (IoT) have made
security a critical concern. This paper presents a novel Large Language Model
(LLM)-based framework for comprehensive threat detection and prevention in IoT
environments. The system integrates lightweight LLMs fine-tuned on IoT-specific
datasets (IoT-23, TON_IoT) for real-time anomaly detection and automated,
context-aware mitigation strategies optimized for resource-constrained devices.
A modular Docker-based deployment enables scalable and reproducible evaluation
across diverse network conditions. Experimental results in simulated IoT
environments demonstrate significant improvements in detection accuracy,
response latency, and resource efficiency over traditional security methods.
The proposed framework highlights the potential of LLM-driven, autonomous
security solutions for future IoT ecosystems.

</details>

### [3] [PatchFuzz: Patch Fuzzing for JavaScript Engines](https://arxiv.org/abs/2505.00289)
*Junjie Wang,Yuhan Ma,Xiaofei Xie,Xiaoning Du,Xiangwei Zhang*

Main category: cs.CR

TLDR: PatchFuzz是一种端到端的可持续方法，用于JavaScript引擎的补丁模糊测试，通过自动化收集历史漏洞的PoC并利用其修补程序更有效地发现新漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有补丁模糊测试方法依赖于普通测试用例或公开的漏洞PoC，其可持续性受限于PoC收集的自动化挑战。

Method: PatchFuzz自动化识别修复安全漏洞的git提交，提取并处理PoC作为模糊测试种子，通过选择性仪器化优化资源分配，并优化变异策略。

Result: 实验结果显示PatchFuzz成功发现6个流行JavaScript引擎中的54个漏洞，并获得总计62,500美元的奖金。

Conclusion: PatchFuzz提供了一种可持续且高效的补丁模糊测试方法，显著提升了漏洞发现的效率和效果。

Abstract: Patch fuzzing is a technique aimed at identifying vulnerabilities that arise
from newly patched code. While researchers have made efforts to apply patch
fuzzing to testing JavaScript engines with considerable success, these efforts
have been limited to using ordinary test cases or publicly available
vulnerability PoCs (Proof of Concepts) as seeds, and the sustainability of
these approaches is hindered by the challenges associated with automating the
PoC collection. To address these limitations, we propose an end-to-end
sustainable approach for JavaScript engine patch fuzzing, named PatchFuzz. It
automates the collection of PoCs of a broader range of historical
vulnerabilities and leverages both the PoCs and their corresponding patches to
uncover new vulnerabilities more effectively. PatchFuzz starts by recognizing
git commits which intend to fix security bugs. Subsequently, it extracts and
processes PoCs from these commits to form the seeds for fuzzing, while
utilizing code revisions to focus limited fuzzing resources on the more
vulnerable code areas through selective instrumentation. The mutation strategy
of PatchFuzz is also optimized to maximize the potential of the PoCs.
Experimental results demonstrate the effectiveness of PatchFuzz. Notably, 54
bugs across six popular JavaScript engines have been exposed and a total of
$62,500 bounties has been received.

</details>

### [4] [Vehicular Communication Security: Multi-Channel and Multi-Factor Authentication](https://arxiv.org/abs/2505.00340)
*Marco De Vincenzi,Shuyang Sun,Chen Bo Calvin Zhang,Manuel Garcia,Shaozu Ding,Chiara Bodei,Ilaria Matteucci,Dajiang Suo*

Main category: cs.CR

TLDR: 提出了一种结合非视距（NLOS）凭证和视距（LOS）视觉通道的多通道多因素认证（MFA）方案，用于车辆与基础设施（V2I）通信的安全认证。


<details>
  <summary>Details</summary>
Motivation: 当前V2I认证依赖无线NLOS信道的凭证方法，易受远程冒充和邻近攻击，需更安全的解决方案。

Method: 采用挑战-响应安全范式，基础设施发出挑战，车辆前灯通过闪烁编码安全数据响应，深度学习模型解码信息以认证车辆。

Result: 实验表明，在不同光照、天气、速度和距离条件下，平均测试准确率达95%和96.6%。

Conclusion: 双通道深度学习架构能同时解码闪烁序列并提取车辆空间和位置特征，实现鲁棒认证。

Abstract: Secure and reliable communications are crucial for Intelligent Transportation
Systems (ITSs), where Vehicle-to-Infrastructure (V2I) communication plays a key
role in enabling mobility-enhancing and safety-critical services. Current V2I
authentication relies on credential-based methods over wireless
Non-Line-of-Sight (NLOS) channels, leaving them exposed to remote impersonation
and proximity attacks. To mitigate these risks, we propose a unified
Multi-Channel, Multi-Factor Authentication (MFA) scheme that combines NLOS
cryptographic credentials with a Line-of-Sight (LOS) visual channel. Our
approach leverages a challenge-response security paradigm: the infrastructure
issues challenges and the vehicle's headlights respond by flashing a structured
sequence containing encoded security data. Deep learning models on the
infrastructure side then decode the embedded information to authenticate the
vehicle. Real-world experimental evaluations demonstrate high test accuracy,
reaching an average of 95% and 96.6%, respectively, under various lighting,
weather, speed, and distance conditions. Additionally, we conducted extensive
experiments on three state-of-the-art deep learning models, including detailed
ablation studies for decoding the flashing sequence. Our results indicate that
the optimal architecture employs a dual-channel design, enabling simultaneous
decoding of the flashing sequence and extraction of vehicle spatial and
locational features for robust authentication.

</details>

### [5] [HoneyWin: High-Interaction Windows Honeypot in Enterprise Environment](https://arxiv.org/abs/2505.00465)
*Yan Lin Aung,Yee Loon Khoo,Davis Yang Zheng,Bryan Swee Duo,Sudipta Chattopadhyay,Jianying Zhou,Liming Lu,Weihan Goh*

Main category: cs.CR

TLDR: HoneyWin是一种高交互Windows蜜罐，模拟企业IT环境，用于检测和分析针对Windows系统的恶意攻击。


<details>
  <summary>Details</summary>
Motivation: 由于Windows系统广泛使用且漏洞频发，成为恶意软件和勒索软件的主要目标，亟需高效防御机制。

Method: HoneyWin由三个Windows 11终端和一个企业级网关组成，具备网络流量捕获、主机日志记录、诱饵令牌等功能。

Result: 部署34天内，捕获570万次未授权连接、124万次登录尝试，并记录到攻击者利用诱饵令牌发起SMTP暴力破解攻击。

Conclusion: HoneyWin能有效检测和分析针对Windows系统的攻击，为防御提供实用工具。

Abstract: Windows operating systems (OS) are ubiquitous in enterprise Information
Technology (IT) and operational technology (OT) environments. Due to their
widespread adoption and known vulnerabilities, they are often the primary
targets of malware and ransomware attacks. With 93% of the ransomware targeting
Windows-based systems, there is an urgent need for advanced defensive
mechanisms to detect, analyze, and mitigate threats effectively. In this paper,
we propose HoneyWin a high-interaction Windows honeypot that mimics an
enterprise IT environment. The HoneyWin consists of three Windows 11 endpoints
and an enterprise-grade gateway provisioned with comprehensive network traffic
capturing, host-based logging, deceptive tokens, endpoint security and
real-time alerts capabilities. The HoneyWin has been deployed live in the wild
for 34 days and receives more than 5.79 million unsolicited connections, 1.24
million login attempts, 5 and 354 successful logins via remote desktop protocol
(RDP) and secure shell (SSH) respectively. The adversary interacted with the
deceptive token in one of the RDP sessions and exploited the public-facing
endpoint to initiate the Simple Mail Transfer Protocol (SMTP) brute-force bot
attack via SSH sessions. The adversary successfully harvested 1,250 SMTP
credentials after attempting 151,179 credentials during the attack.

</details>

### [6] [Decentralized Vulnerability Disclosure via Permissioned Blockchain: A Secure, Transparent Alternative to Centralized CVE Management](https://arxiv.org/abs/2505.00480)
*Novruz Amirov,Kemal Bicakci*

Main category: cs.CR

TLDR: 提出了一种基于区块链的去中心化CVE发布系统，以解决当前MITRE主导的集中式模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前CVE发布的集中式模型存在透明度不足和信任问题，希望通过区块链技术实现去中心化、透明和可审计的系统。

Method: 采用许可区块链技术，仅允许认证的CVE编号机构（CNAs）提交条目，结合智能合约支持关键功能如禁运披露和去中心化治理。

Result: 通过原型实现（Hyperledger Fabric）验证了可行性，并展示了其在透明度、信任去中心化和可审计性方面的优势。

Conclusion: 该系统为漏洞披露的未来提供了可行的解决方案，具有潜在的实际应用价值。

Abstract: This paper proposes a decentralized, blockchain-based system for the
publication of Common Vulnerabilities and Exposures (CVEs), aiming to mitigate
the limitations of the current centralized model primarily overseen by MITRE.
The proposed architecture leverages a permissioned blockchain, wherein only
authenticated CVE Numbering Authorities (CNAs) are authorized to submit
entries. This ensures controlled write access while preserving public
transparency. By incorporating smart contracts, the system supports key
features such as embargoed disclosures and decentralized governance. We
evaluate the proposed model in comparison with existing practices, highlighting
its advantages in transparency, trust decentralization, and auditability. A
prototype implementation using Hyperledger Fabric is presented to demonstrate
the feasibility of the approach, along with a discussion of its implications
for the future of vulnerability disclosure.

</details>

### [7] [Analysis of the vulnerability of machine learning regression models to adversarial attacks using data from 5G wireless networks](https://arxiv.org/abs/2505.00487)
*Leonid Legashev,Artur Zhigalov,Denis Parfenov*

Main category: cs.CR

TLDR: 论文研究了使用DeepMIMO模拟器进行数据集分析，并通过FGSM方法实施对抗性攻击。结果显示，对抗性攻击导致回归模型性能显著下降，而LightGBM分类器能高效检测异常数据。


<details>
  <summary>Details</summary>
Motivation: 研究对抗性攻击对机器学习模型的影响，并探索检测此类攻击的有效方法。

Method: 使用DeepMIMO模拟器生成数据集，通过FGSM方法实施对抗性攻击，并比较不同条件下回归模型和分类器的性能。

Result: 对抗性攻击使MSE增加33%，R2下降10%，而LightGBM分类器检测异常数据的准确率达98%。

Conclusion: 回归模型易受对抗性攻击，但快速网络流量分析可有效识别恶意活动。

Abstract: This article describes the process of creating a script and conducting an
analytical study of a dataset using the DeepMIMO emulator. An advertorial
attack was carried out using the FGSM method to maximize the gradient. A
comparison is made of the effectiveness of binary classifiers in the task of
detecting distorted data. The dynamics of changes in the quality indicators of
the regression model were analyzed in conditions without adversarial attacks,
during an adversarial attack and when the distorted data was isolated. It is
shown that an adversarial FGSM attack with gradient maximization leads to an
increase in the value of the MSE metric by 33% and a decrease in the R2
indicator by 10% on average. The LightGBM binary classifier effectively
identifies data with adversarial anomalies with 98% accuracy. Regression
machine learning models are susceptible to adversarial attacks, but rapid
analysis of network traffic and data transmitted over the network makes it
possible to identify malicious activity

</details>

### [8] [Notes on Univariate Sumcheck](https://arxiv.org/abs/2505.00554)
*Malcom Mohamed*

Main category: cs.CR

TLDR: 将多元和校验协议适配到单位根上的单变量多项式插值。


<details>
  <summary>Details</summary>
Motivation: 研究如何将多元和校验协议应用于单变量多项式，以简化或优化相关计算。

Method: 通过单位根上的单变量多项式插值，适配多元和校验协议。

Result: 成功实现了多元和校验协议在单变量多项式上的应用。

Conclusion: 该方法为多元和校验协议提供了新的应用场景和优化可能性。

Abstract: These notes describe an adaptation of the multivariate sumcheck protocol to
univariate polynomials interpolated over roots of unity.

</details>

### [9] [A Novel Feature-Aware Chaotic Image Encryption Scheme For Data Security and Privacy in IoT and Edge Networks](https://arxiv.org/abs/2505.00593)
*Muhammad Shahbaz Khan,Ahmed Al-Dubai,Jawad Ahmad,Nikolaos Pitropakis,Baraq Ghaleb*

Main category: cs.CR

TLDR: 提出了一种结合特征感知像素分割与混沌链机制的图像加密方案，适用于资源受限的IoT设备，显著提升安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统加密算法（如AES和RSA）在IoT设备上计算成本高且对大规模图像数据效果不佳，需一种高效安全的替代方案。

Method: 方案分为三阶段：特征感知像素分割（FAPS）、混沌链置换（动态密钥）、混沌链混淆（动态种子矩阵）。

Result: 显著降低像素相关性（接近零），熵值接近8，能抵抗差分密码攻击。

Conclusion: 该方案适合资源受限环境中的实时部署，兼具高效性和安全性。

Abstract: The security of image data in the Internet of Things (IoT) and edge networks
is crucial due to the increasing deployment of intelligent systems for
real-time decision-making. Traditional encryption algorithms such as AES and
RSA are computationally expensive for resource-constrained IoT devices and
ineffective for large-volume image data, leading to inefficiencies in
privacy-preserving distributed learning applications. To address these
concerns, this paper proposes a novel Feature-Aware Chaotic Image Encryption
scheme that integrates Feature-Aware Pixel Segmentation (FAPS) with Chaotic
Chain Permutation and Confusion mechanisms to enhance security while
maintaining efficiency. The proposed scheme consists of three stages: (1) FAPS,
which extracts and reorganizes pixels based on high and low edge intensity
features for correlation disruption; (2) Chaotic Chain Permutation, which
employs a logistic chaotic map with SHA-256-based dynamically updated keys for
block-wise permutation; and (3) Chaotic chain Confusion, which utilises
dynamically generated chaotic seed matrices for bitwise XOR operations.
Extensive security and performance evaluations demonstrate that the proposed
scheme significantly reduces pixel correlation -- almost zero, achieves high
entropy values close to 8, and resists differential cryptographic attacks. The
optimum design of the proposed scheme makes it suitable for real-time
deployment in resource-constrained environments.

</details>

### [10] [RevealNet: Distributed Traffic Correlation for Attack Attribution on Programmable Networks](https://arxiv.org/abs/2505.00618)
*Gurjot Singh,Alim Dhanani,Diogo Barradas*

Main category: cs.CR

TLDR: RevealNet是一种去中心化框架，利用P4可编程交换机进行流量关联，以实现高效、分布式的攻击溯源。


<details>
  <summary>Details</summary>
Motivation: 网络攻击者常使用代理链、VPN和匿名网络隐藏活动，现有流量关联技术依赖集中式系统，难以扩展到大容量高速网络。

Method: 基于流量草图计算和比较的关联原语，通过P4可编程交换机实现分布式流量关联。

Result: RevealNet在准确性上与集中式系统相当，同时显著降低了计算复杂度和带宽开销。

Conclusion: RevealNet为大规模高速网络中的攻击溯源提供了一种高效、可扩展的解决方案。

Abstract: Network attackers have increasingly resorted to proxy chains, VPNs, and
anonymity networks to conceal their activities. To tackle this issue, past
research has explored the applicability of traffic correlation techniques to
perform attack attribution, i.e., to identify an attacker's true network
location. However, current traffic correlation approaches rely on
well-provisioned and centralized systems that ingest flows from multiple
network probes to compute correlation scores. Unfortunately, this makes
correlation efforts scale poorly for large high-speed networks.
  In this paper, we propose RevealNet, a decentralized framework for attack
attribution that orchestrates a fleet of P4-programmable switches to perform
traffic correlation. RevealNet builds on a set of correlation primitives
inspired by prior work on computing and comparing flow sketches -- compact
summaries of flows' key characteristics -- to enable efficient, distributed,
in-network traffic correlation. Our evaluation suggests that RevealNet achieves
comparable accuracy to centralized attack attribution systems while
significantly reducing both the computational complexity and bandwidth
overheads imposed by correlation tasks.

</details>

### [11] [An Inversion Theorem for Buffered Linear Toeplitz (BLT) Matrices and Applications to Streaming Differential Privacy](https://arxiv.org/abs/2504.21413)
*H. Brendan McMahan,Krishna Pillutla*

Main category: cs.CR

TLDR: BLT矩阵的逆矩阵仍是BLT矩阵，且参数不同；提出了高效可微的O(d³)算法计算逆矩阵参数。


<details>
  <summary>Details</summary>
Motivation: 研究BLT矩阵在流式差分隐私中的作用，解决其逆矩阵计算问题。

Method: 提出BLT逆矩阵定理，并设计O(d³)算法计算逆矩阵参数。

Result: BLT逆矩阵保持BLT形式，算法高效且可微。

Conclusion: BLT逆矩阵特性及算法为隐私机制优化提供了直接支持。

Abstract: Buffered Linear Toeplitz (BLT) matrices are a family of parameterized
lower-triangular matrices that play an important role in streaming differential
privacy with correlated noise. Our main result is a BLT inversion theorem: the
inverse of a BLT matrix is itself a BLT matrix with different parameters. We
also present an efficient and differentiable $O(d^3)$ algorithm to compute the
parameters of the inverse BLT matrix, where $d$ is the degree of the original
BLT (typically $d < 10$). Our characterization enables direct optimization of
BLT parameters for privacy mechanisms through automatic differentiation.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [From Lab to Wrist: Bridging Metabolic Monitoring and Consumer Wearables for Heart Rate and Oxygen Consumption Modeling](https://arxiv.org/abs/2505.00101)
*Barak Gahtan,Sanketh Vedula,Gil Samuelly Leichtag,Einat Kodesh,Alex M. Bronstein*

Main category: cs.LG

TLDR: 该论文提出了一种基于消费级可穿戴设备数据的框架，首次能够预测瞬时氧气消耗（VO2）轨迹，结合心率动力学和VO2预测模型，实现了高精度和实用性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过可穿戴设备数据优化跑步表现、个性化训练和运动员健康管理，填补实验室级精度与日常可及性之间的鸿沟。

Method: 采用两种生理模型：1）通过生理约束ODE和神经卡尔曼滤波器建模心率动态；2）基于心率模型的新VO2预测架构，仅需初始VO2数据校准。

Result: 方法在心率预测上误差低至2.81bpm（相关性0.87），VO2预测平均绝对百分比误差约13%，能捕捉不同跑步强度下的生理变化。

Conclusion: 该框架通过结合生理约束与机器学习，为非侵入性代谢监测提供了实用工具，适用于从精英运动员到普通健身爱好者。

Abstract: Understanding physiological responses during running is critical for
performance optimization, tailored training prescriptions, and athlete health
management. We introduce a comprehensive framework -- what we believe to be the
first capable of predicting instantaneous oxygen consumption (VO$_{2}$)
trajectories exclusively from consumer-grade wearable data. Our approach
employs two complementary physiological models: (1) accurate modeling of heart
rate (HR) dynamics via a physiologically constrained ordinary differential
equation (ODE) and neural Kalman filter, trained on over 3 million HR
observations, achieving 1-second interval predictions with mean absolute errors
as low as 2.81\,bpm (correlation 0.87); and (2) leveraging the principles of
precise HR modeling, a novel VO$_{2}$ prediction architecture requiring only
the initial second of VO$_{2}$ data for calibration, enabling robust,
sequence-to-sequence metabolic demand estimation. Despite relying solely on
smartwatch and chest-strap data, our method achieves mean absolute percentage
errors of approximately 13\%, effectively capturing rapid physiological
transitions and steady-state conditions across diverse running intensities. Our
synchronized dataset, complemented by blood lactate measurements, further lays
the foundation for future noninvasive metabolic zone identification. By
embedding physiological constraints within modern machine learning, this
framework democratizes advanced metabolic monitoring, bridging laboratory-grade
accuracy and everyday accessibility, thus empowering both elite athletes and
recreational fitness enthusiasts.

</details>

### [13] [Kernel-Based Ensemble Gaussian Mixture Probability Hypothesis Density Filter](https://arxiv.org/abs/2505.00131)
*Dalton Durant,Renato Zanetti*

Main category: cs.LG

TLDR: 提出了一种基于核的集成高斯混合概率假设密度（EnGM-PHD）滤波器，结合了GM-PHD和SMC-PHD滤波器的优点，通过核密度估计技术提升多目标滤波性能。


<details>
  <summary>Details</summary>
Motivation: 结合高斯混合和粒子滤波技术的优势，提升多目标滤波的准确性和效率。

Method: 从后验强度函数中获取粒子，通过系统动力学传播，并使用核密度估计技术近似先验强度函数的高斯混合。

Result: EnGM-PHD滤波器在多目标滤波性能上优于GM-PHD和SMC-PHD滤波器，且使用相同数量的组件或粒子。

Conclusion: EnGM-PHD滤波器在多目标滤波应用中表现出更高的性能，且在某些情况下可简化为标准EnGMF滤波器。

Abstract: In this work, a kernel-based Ensemble Gaussian Mixture Probability Hypothesis
Density (EnGM-PHD) filter is presented for multi-target filtering applications.
The EnGM-PHD filter combines the Gaussian-mixture-based techniques of the
Gaussian Mixture Probability Hypothesis Density (GM-PHD) filter with the
particle-based techniques of the Sequential Monte Carlo Probability Hypothesis
Density (SMC-PHD) filter. It achieves this by obtaining particles from the
posterior intensity function, propagating them through the system dynamics, and
then using Kernel Density Estimation (KDE) techniques to approximate the
Gaussian mixture of the prior intensity function. This approach guarantees
convergence to the true intensity function in the limit of the number of
components. Moreover, in the special case of a single target with no births,
deaths, clutter, and perfect detection probability, the EnGM-PHD filter reduces
to the standard Ensemble Gaussian Mixture Filter (EnGMF). In the presented
experiment, the results indicate that the EnGM-PHD filter achieves better
multi-target filtering performance than both the GM-PHD and SMC-PHD filters
while using the same number of components or particles.

</details>

### [14] [GPRat: Gaussian Process Regression with Asynchronous Tasks](https://arxiv.org/abs/2505.00136)
*Maksim Helmann,Alexander Strack,Dirk Pflüger*

Main category: cs.LG

TLDR: 论文提出了一种将基于任务的C++代码通过pybind11绑定到Python API的新方法，开发了并行高斯过程库GPRat，展示了在AI应用中异步任务的潜力。


<details>
  <summary>Details</summary>
Motivation: Python是AI开发的主流语言，但现有库在低层并行化时可能导致性能和扩展性下降，因此需要一种高效的方法。

Method: 使用pybind11将基于HPX异步运行时模型的C++代码绑定到Python API，开发了并行高斯过程库GPRat。

Result: GPRat在64核AMD EPYC 7742 CPU上表现出色，训练速度优于GPyTorch和GPflow，预测速度分别提升7.63和25.25倍。

Conclusion: 异步任务在Python AI应用中具有显著潜力，GPRat展示了高性能和可扩展性。

Abstract: Python is the de-facto language for software development in artificial
intelligence (AI). Commonly used libraries, such as PyTorch and TensorFlow,
rely on parallelization built into their BLAS backends to achieve speedup on
CPUs. However, only applying parallelization in a low-level backend can lead to
performance and scaling degradation. In this work, we present a novel way of
binding task-based C++ code built on the asynchronous runtime model HPX to a
high-level Python API using pybind11. We develop a parallel Gaussian process
(GP) li- brary as an application. The resulting Python library GPRat combines
the ease of use of commonly available GP libraries with the performance and
scalability of asynchronous runtime systems. We evaluate the per- formance on a
mass-spring-damper system, a standard benchmark from control theory, for
varying numbers of regressors (features). The results show almost no binding
overhead when binding the asynchronous HPX code using pybind11. Compared to
GPyTorch and GPflow, GPRat shows superior scaling on up to 64 cores on an AMD
EPYC 7742 CPU for train- ing. Furthermore, our library achieves a prediction
speedup of 7.63 over GPyTorch and 25.25 over GPflow. If we increase the number
of features from eight to 128, we observe speedups of 29.62 and 21.19,
respectively. These results showcase the potential of using asynchronous tasks
within Python-based AI applications.

</details>

### [15] [Stochastic Subspace Descent Accelerated via Bi-fidelity Line Search](https://arxiv.org/abs/2505.00162)
*Nuojin Cheng,Alireza Doostan,Stephen Becker*

Main category: cs.LG

TLDR: 论文提出了一种名为BF-SSD的双保真度零阶优化算法，通过结合低保真度和高保真度函数评估构建代理模型，显著减少了计算成本，并在多个实际应用中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决目标函数和梯度评估计算成本高的问题，尤其是在梯度不可用时，零阶优化方法的性能受限于高函数查询成本。

Method: 提出BF-SSD算法，利用双保真度框架构建代理模型，结合低保真度和高保真度评估，实现高效的步长选择和收敛保证。

Result: 在合成优化基准、核岭回归、黑盒对抗攻击和语言模型微调等任务中，BF-SSD显著减少了高保真度评估次数，并表现出优越的优化性能。

Conclusion: BF-SSD通过整合双保真度策略，为零阶优化提供了一种高效且计算成本低的方法，适用于大规模高维实际问题。

Abstract: Efficient optimization remains a fundamental challenge across numerous
scientific and engineering domains, especially when objective function and
gradient evaluations are computationally expensive. While zeroth-order
optimization methods offer effective approaches when gradients are
inaccessible, their practical performance can be limited by the high cost
associated with function queries. This work introduces the bi-fidelity
stochastic subspace descent (BF-SSD) algorithm, a novel zeroth-order
optimization method designed to reduce this computational burden. BF-SSD
leverages a bi-fidelity framework, constructing a surrogate model from a
combination of computationally inexpensive low-fidelity (LF) and accurate
high-fidelity (HF) function evaluations. This surrogate model facilitates an
efficient backtracking line search for step size selection, for which we
provide theoretical convergence guarantees under standard assumptions. We
perform a comprehensive empirical evaluation of BF-SSD across four distinct
problems: a synthetic optimization benchmark, dual-form kernel ridge
regression, black-box adversarial attacks on machine learning models, and
transformer-based black-box language model fine-tuning. Numerical results
demonstrate that BF-SSD consistently achieves superior optimization performance
while requiring significantly fewer HF function evaluations compared to
relevant baseline methods. This study highlights the efficacy of integrating
bi-fidelity strategies within zeroth-order optimization, positioning BF-SSD as
a promising and computationally efficient approach for tackling large-scale,
high-dimensional problems encountered in various real-world applications.

</details>

### [16] [GEOM-Drugs Revisited: Toward More Chemically Accurate Benchmarks for 3D Molecule Generation](https://arxiv.org/abs/2505.00169)
*Filipp Nikitin,Ian Dunn,David Ryan Koes,Olexandr Isayev*

Main category: cs.LG

TLDR: 该论文提出了一种修正的评估框架，解决了GEOM-Drugs数据集在3D分子结构生成评估中的关键缺陷，并提供了更新的性能指标和未来基准测试的建议。


<details>
  <summary>Details</summary>
Motivation: 当前3D分子结构生成的评估协议存在严重缺陷，包括错误的化合价定义、键序计算错误以及依赖与参考数据不一致的力场。

Method: 作者重新审视了GEOM-Drugs数据集，修正了数据预处理问题，构建了化学上准确的化合价表，并引入了基于GFN2-xTB的几何和能量基准。

Result: 通过重新训练和评估多个领先模型，提供了更新的性能指标，并强调了化学严谨评估的重要性。

Conclusion: 论文强调了在3D分子生成中采用化学严谨评估实践的必要性，并提供了推荐的评估方法和处理脚本。

Abstract: Deep generative models have shown significant promise in generating valid 3D
molecular structures, with the GEOM-Drugs dataset serving as a key benchmark.
However, current evaluation protocols suffer from critical flaws, including
incorrect valency definitions, bugs in bond order calculations, and reliance on
force fields inconsistent with the reference data. In this work, we revisit
GEOM-Drugs and propose a corrected evaluation framework: we identify and fix
issues in data preprocessing, construct chemically accurate valency tables, and
introduce a GFN2-xTB-based geometry and energy benchmark. We retrain and
re-evaluate several leading models under this framework, providing updated
performance metrics and practical recommendations for future benchmarking. Our
results underscore the need for chemically rigorous evaluation practices in 3D
molecular generation. Our recommended evaluation methods and GEOM-Drugs
processing scripts are available at
https://github.com/isayevlab/geom-drugs-3dgen-evaluation.

</details>

### [17] [Attention-enabled Explainable AI for Bladder Cancer Recurrence Prediction](https://arxiv.org/abs/2505.00171)
*Saram Abbas,Naeem Soomro,Rishad Shafik,Rakesh Heer,Kabita Adhikari*

Main category: cs.LG

TLDR: 提出了一种可解释的深度学习框架，用于改进非肌层浸润性膀胱癌（NMIBC）的复发预测，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: NMIBC复发率高且现有预测工具不准确，缺乏个性化管理。

Method: 结合向量嵌入和注意力机制，捕捉患者属性与复发风险的复杂关系。

Result: 模型准确率达70%，发现新影响因素（如手术时长和住院时间）。

Conclusion: 该框架不仅提升预测性能，还为临床提供患者特异性解释。

Abstract: Non-muscle-invasive bladder cancer (NMIBC) is a relentless challenge in
oncology, with recurrence rates soaring as high as 70-80%. Each recurrence
triggers a cascade of invasive procedures, lifelong surveillance, and
escalating healthcare costs - affecting 460,000 individuals worldwide. However,
existing clinical prediction tools remain fundamentally flawed, often
overestimating recurrence risk and failing to provide personalized insights for
patient management. In this work, we propose an interpretable deep learning
framework that integrates vector embeddings and attention mechanisms to improve
NMIBC recurrence prediction performance. We incorporate vector embeddings for
categorical variables such as smoking status and intravesical treatments,
allowing the model to capture complex relationships between patient attributes
and recurrence risk. These embeddings provide a richer representation of the
data, enabling improved feature interactions and enhancing prediction
performance. Our approach not only enhances performance but also provides
clinicians with patient-specific insights by highlighting the most influential
features contributing to recurrence risk for each patient. Our model achieves
accuracy of 70% with tabular data, outperforming conventional statistical
methods while providing clinician-friendly patient-level explanations through
feature attention. Unlike previous studies, our approach identifies new
important factors influencing recurrence, such as surgical duration and
hospital stay, which had not been considered in existing NMIBC prediction
models.

</details>

### [18] [Chronic Diseases Prediction using Machine Learning and Deep Learning Methods](https://arxiv.org/abs/2505.00189)
*Houda Belhad,Asmae Bourbia,Salma Boughanja*

Main category: cs.LG

TLDR: 该研究探讨了机器学习和深度学习技术在预测慢性疾病和甲状腺疾病中的应用，发现集成方法（如随机森林和梯度提升树）表现最佳。


<details>
  <summary>Details</summary>
Motivation: 慢性疾病是全球早逝的主要原因，传统诊断方法因复杂性而常失败，因此需要更有效的预测方法。

Method: 研究使用了多种模型（如逻辑回归、随机森林、神经网络等），结合数据预处理（处理缺失值、分类编码等）和性能评估指标（如准确率、AUC等）。

Result: 集成方法和神经网络表现最佳，尤其在捕捉复杂数据模式方面。

Conclusion: ML和DL在慢性疾病预测中潜力巨大，但需解决数据质量和模型可解释性等挑战。

Abstract: Chronic diseases, such as cardiovascular disease, diabetes, chronic kidney
disease, and thyroid disorders, are the leading causes of premature mortality
worldwide. Early detection and intervention are crucial for improving patient
outcomes, yet traditional diagnostic methods often fail due to the complex
nature of these conditions. This study explores the application of machine
learning (ML) and deep learning (DL) techniques to predict chronic disease and
thyroid disorders. We used a variety of models, including Logistic Regression
(LR), Random Forest (RF), Gradient Boosted Trees (GBT), Neural Networks (NN),
Decision Trees (DT) and Native Bayes (NB), to analyze and predict disease
outcomes. Our methodology involved comprehensive data pre-processing, including
handling missing values, categorical encoding, and feature aggregation,
followed by model training and evaluation. Performance metrics such ad
precision, recall, accuracy, F1-score, and Area Under the Curve (AUC) were used
to assess the effectiveness of each model. The results demonstrated that
ensemble methods like Random Forest and Gradient Boosted Trees consistently
outperformed. Neutral Networks also showed superior performance, particularly
in capturing complex data patterns. The findings highlight the potential of ML
and DL in revolutionizing chronic disease prediction, enabling early diagnosis
and personalized treatment strategies. However, challenges such as data
quality, model interpretability, and the need for advanced computational
techniques in healthcare to improve patient outcomes and reduce the burden of
chronic diseases. This study was conducted as part of Big Data class project
under the supervision of our professors Mr. Abderrahmane EZ-ZAHOUT and Mr.
Abdessamad ESSAIDI.

</details>

### [19] [Empirical Evaluation of Progressive Coding for Sparse Autoencoders](https://arxiv.org/abs/2505.00190)
*Hans Peter,Anders Søgaard*

Main category: cs.LG

TLDR: 稀疏自编码器（SAEs）通过字典学习从神经网络中提取可解释特征，但计算成本高。研究发现字典重要性遵循幂律，并比较了基于子集剪枝的渐进编码与联合训练嵌套SAEs（Matryoshka SAEs）的性能。Matryoshka SAEs在重建损失和语言建模任务中表现更好，但剪枝的SAEs更具可解释性。


<details>
  <summary>Details</summary>
Motivation: 探索如何高效地从神经网络中提取可解释特征，解决稀疏自编码器计算成本高的问题。

Method: 比较基于子集剪枝的渐进编码与联合训练嵌套SAEs（Matryoshka SAEs）在语言建模任务中的性能。

Result: Matryoshka SAEs在重建损失和语言建模任务中表现更优，但剪枝的SAEs更具可解释性。

Conclusion: Matryoshka SAEs在性能上优于剪枝SAEs，但可解释性较差，需权衡两者。

Abstract: Sparse autoencoders (SAEs)
\citep{bricken2023monosemanticity,gao2024scalingevaluatingsparseautoencoders}
rely on dictionary learning to extract interpretable features from neural
networks at scale in an unsupervised manner, with applications to
representation engineering and information retrieval. SAEs are, however,
computationally expensive \citep{lieberum2024gemmascopeopensparse}, especially
when multiple SAEs of different sizes are needed. We show that dictionary
importance in vanilla SAEs follows a power law. We compare progressive coding
based on subset pruning of SAEs -- to jointly training nested SAEs, or
so-called {\em Matryoshka} SAEs
\citep{bussmann2024learning,nabeshima2024Matryoshka} -- on a language modeling
task. We show Matryoshka SAEs exhibit lower reconstruction loss and recaptured
language modeling loss, as well as higher representational similarity. Pruned
vanilla SAEs are more interpretable, however. We discuss the origins and
implications of this trade-off.

</details>

### [20] [Mapping minds not averages: a scalable subject-specific manifold learning framework for neuroimaging data](https://arxiv.org/abs/2505.00196)
*Eloy Geenjaar,Vince Calhoun*

Main category: cs.LG

TLDR: 提出了一种能够捕捉个体特异性空间变化的流形学习框架，适用于结构化和非结构化神经影像数据，并在模拟数据和真实数据中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于群体级别的空间对齐或时间结构化数据，无法有效捕捉个体差异，因此需要一种新的框架来解决这一问题。

Method: 引入了一种流形学习框架，能够处理结构化和非结构化神经影像数据，并在模拟数据和真实fMRI数据上进行验证。

Result: 框架在模拟数据和真实数据中表现优于基线方法，能够高效扩展到大数据集并泛化到新受试者，同时揭示了临床相关的大脑活动模式。

Conclusion: 该框架为建模个体特异性大脑活动提供了可扩展且高效的方法，具有计算神经科学和临床研究的应用潜力。

Abstract: Mental and cognitive representations are believed to reside on
low-dimensional, non-linear manifolds embedded within high-dimensional brain
activity. Uncovering these manifolds is key to understanding individual
differences in brain function, yet most existing machine learning methods
either rely on population-level spatial alignment or assume data that is
temporally structured, either because data is aligned among subjects or because
event timings are known. We introduce a manifold learning framework that can
capture subject-specific spatial variations across both structured and
temporally unstructured neuroimaging data. On simulated data and two
naturalistic fMRI datasets (Sherlock and Forrest Gump), our framework
outperforms group-based baselines by recovering more accurate and
individualized representations. We further show that the framework scales
efficiently to large datasets and generalizes well to new subjects. To test
this, we apply the framework to temporally unstructured resting-state fMRI data
from individuals with schizophrenia and healthy controls. We further apply our
method to a large resting-state fMRI dataset comprising individuals with
schizophrenia and controls. In this setting, we demonstrate that the framework
scales efficiently to large populations and generalizes robustly to unseen
subjects. The learned subject-specific spatial maps our model finds reveal
clinically relevant patterns, including increased activation in the basal
ganglia, visual, auditory, and somatosensory regions, and decreased activation
in the insula, inferior frontal gyrus, and angular gyrus. These findings
suggest that our framework can uncover clinically relevant subject-specific
brain activity patterns. Our approach thus provides a scalable and
individualized framework for modeling brain activity, with applications in
computational neuroscience and clinical research.

</details>

### [21] [Graph Privacy: A Heterogeneous Federated GNN for Trans-Border Financial Data Circulation](https://arxiv.org/abs/2505.00257)
*Zhizhong Tan,Jiexin Zheng,Kevin Qi Zhang,Wenyong Wang*

Main category: cs.LG

TLDR: 提出了一种异构联邦图神经网络（HFGNN）方法，解决金融数据跨境流动和共享中的隐私问题，实现数据可用不可见。


<details>
  <summary>Details</summary>
Motivation: 金融机构对外部数据共享需求强烈，但隐私问题导致平台互联困难、数据开放度低。

Method: 将异构业务数据分布作为子图，通过中央服务器构建统计异构全局图，子图通过本地训练学习个性化服务模型，分离和结合子图的拓扑与特征信息。

Result: 仿真实验表明，该方法比现有方法具有更高的准确性和更快的收敛速度。

Conclusion: HFGNN方法有效解决了金融数据共享中的隐私问题，提升了数据利用效率。

Abstract: The sharing of external data has become a strong demand of financial
institutions, but the privacy issue has led to the difficulty of
interconnecting different platforms and the low degree of data openness. To
effectively solve the privacy problem of financial data in trans-border flow
and sharing, to ensure that the data is available but not visible, to realize
the joint portrait of all kinds of heterogeneous data of business organizations
in different industries, we propose a Heterogeneous Federated Graph Neural
Network (HFGNN) approach. In this method, the distribution of heterogeneous
business data of trans-border organizations is taken as subgraphs, and the
sharing and circulation process among subgraphs is constructed as a
statistically heterogeneous global graph through a central server. Each
subgraph learns the corresponding personalized service model through local
training to select and update the relevant subset of subgraphs with aggregated
parameters, and effectively separates and combines topological and feature
information among subgraphs. Finally, our simulation experimental results show
that the proposed method has higher accuracy performance and faster convergence
speed than existing methods.

</details>

### [22] [Generative Machine Learning in Adaptive Control of Dynamic Manufacturing Processes: A Review](https://arxiv.org/abs/2505.00210)
*Suk Ki Lee,Hyunwoong Ko*

Main category: cs.LG

TLDR: 生成式机器学习在动态制造系统中缺乏控制导向视角，本文提出分类框架并探讨其潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 动态制造过程的复杂性需要先进监控与控制技术，生成式机器学习展现潜力但缺乏控制导向视角。

Method: 提出预测、直接策略、质量推断和知识集成四类方法，分析生成式ML架构的控制相关特性。

Result: 生成式ML在决策、仿真和数字孪生中展现潜力，但存在生成与控制分离等问题。

Conclusion: 需开发集成框架以结合生成式ML与控制技术，应对动态制造系统的复杂性。

Abstract: Dynamic manufacturing processes exhibit complex characteristics defined by
time-varying parameters, nonlinear behaviors, and uncertainties. These
characteristics require sophisticated in-situ monitoring techniques utilizing
multimodal sensor data and adaptive control systems that can respond to
real-time feedback while maintaining product quality. Recently, generative
machine learning (ML) has emerged as a powerful tool for modeling complex
distributions and generating synthetic data while handling these manufacturing
uncertainties. However, adopting these generative technologies in dynamic
manufacturing systems lacks a functional control-oriented perspective to
translate their probabilistic understanding into actionable process controls
while respecting constraints. This review presents a functional classification
of Prediction-Based, Direct Policy, Quality Inference, and Knowledge-Integrated
approaches, offering a perspective for understanding existing ML-enhanced
control systems and incorporating generative ML. The analysis of generative ML
architectures within this framework demonstrates control-relevant properties
and potential to extend current ML-enhanced approaches where conventional
methods prove insufficient. We show generative ML's potential for manufacturing
control through decision-making applications, process guidance, simulation, and
digital twins, while identifying critical research gaps: separation between
generation and control functions, insufficient physical understanding of
manufacturing phenomena, and challenges adapting models from other domains. To
address these challenges, we propose future research directions aimed at
developing integrated frameworks that combine generative ML and control
technologies to address the dynamic complexities of modern manufacturing
systems.

</details>

### [23] [Online Federation For Mixtures of Proprietary Agents with Black-Box Encoders](https://arxiv.org/abs/2505.00216)
*Xuwei Yang,Fatemeh Tavakoli,David B. Emerson,Anastasis Kratsios*

Main category: cs.LG

TLDR: 论文提出了一种针对黑盒生成AI和特征编码器的非竞争博弈论方法，通过纳什均衡和联邦学习实现混合专家模型的优化。


<details>
  <summary>Details</summary>
Motivation: 工业标准的生成AI和特征编码器多为黑盒，用户无法优化其内部参数，限制了混合专家模型的构建。

Method: 采用非竞争博弈论框架，提出一种去中心化的联邦学习算法，各AI代理在本地优化而不共享内部结构。

Result: 在真实和合成时间序列基准测试中，预测准确性显著提升。

Conclusion: 提出的方法解决了黑盒AI的优化问题，为混合专家模型提供了新思路。

Abstract: Most industry-standard generative AIs and feature encoders are proprietary,
offering only black-box access: their outputs are observable, but their
internal parameters and architectures remain hidden from the end-user. This
black-box access is especially limiting when constructing mixture-of-expert
type ensemble models since the user cannot optimize each proprietary AI's
internal parameters. Our problem naturally lends itself to a non-competitive
game-theoretic lens where each proprietary AI (agent) is inherently competing
against the other AI agents, with this competition arising naturally due to
their obliviousness of the AI's to their internal structure. In contrast, the
user acts as a central planner trying to synchronize the ensemble of competing
AIs.
  We show the existence of the unique Nash equilibrium in the online setting,
which we even compute in closed-form by eliciting a feedback mechanism between
any given time series and the sequence generated by each (proprietary) AI
agent. Our solution is implemented as a decentralized, federated-learning
algorithm in which each agent optimizes their structure locally on their
machine without ever releasing any internal structure to the others. We obtain
refined expressions for pre-trained models such as transformers, random feature
models, and echo-state networks. Our ``proprietary federated learning''
algorithm is implemented on a range of real-world and synthetic time-series
benchmarks. It achieves orders-of-magnitude improvements in predictive accuracy
over natural benchmarks, of which there are surprisingly few due to this
natural problem still being largely unexplored.

</details>

### [24] [Predicting Estimated Times of Restoration for Electrical Outages Using Longitudinal Tabular Transformers](https://arxiv.org/abs/2505.00225)
*Bogireddy Sai Prasanna Teja,Valliappan Muthukaruppan,Carls Benjamin*

Main category: cs.LG

TLDR: 论文提出了一种纵向表格Transformer（LTT）模型，用于提高自然灾害期间的电力恢复时间（ETR）预测准确性，显著提升了客户满意度指标。


<details>
  <summary>Details</summary>
Motivation: 由于气候变异性增加，电力公司需要更精确的ETR预测以支持客户决策，但现有方法依赖人工或传统统计方法，精度不足。

Method: 提出LTT模型，利用历史停电事件数据和顺序更新，结合客户满意度指标和可解释性技术。

Result: 在34,000次风暴相关停电事件中，LTT模型将客户满意度指标平均提升19.08%，且结果具有统计显著性。

Conclusion: LTT模型不仅提高了预测准确性，还增强了透明度，有助于提升客户对模型的信任。

Abstract: As climate variability increases, the ability of utility providers to deliver
precise Estimated Times of Restoration (ETR) during natural disasters has
become increasingly critical. Accurate and timely ETRs are essential for
enabling customer preparedness during extended power outages, where informed
decision-making can be crucial, particularly in severe weather conditions.
Nonetheless, prevailing utility practices predominantly depend on manual
assessments or traditional statistical methods, which often fail to achieve the
level of precision required for reliable and actionable predictions. To address
these limitations, we propose a Longitudinal Tabular Transformer (LTT) model
that leverages historical outage event data along with sequential updates of
these events to improve the accuracy of ETR predictions. The model's
performance was evaluated over 34,000 storm-related outage events from three
major utility companies, collectively serving over 3 million customers over a
2-year period. Results demonstrate that the LTT model improves the Customer
Satisfaction Impact (CSI) metric by an average of 19.08% (p > 0.001) compared
to existing methods. Additionally, we introduce customer-informed regression
metrics that align model evaluation with real-world satisfaction, ensuring the
outcomes resonate with customer expectations. Furthermore, we employ
interpretability techniques to analyze the temporal significance of
incorporating sequential updates in modeling outage events and to identify the
contributions of predictive features to a given ETR. This comprehensive
approach not only improves predictive accuracy but also enhances transparency,
fostering greater trust in the model's capabilities.

</details>

### [25] [Scaling On-Device GPU Inference for Large Generative Models](https://arxiv.org/abs/2505.00232)
*Jiuqiang Tang,Raman Sarokin,Ekaterina Ignasheva,Grant Jensen,Lin Chen,Juhyun Lee,Andrei Kulik,Matthias Grundmann*

Main category: cs.LG

TLDR: ML Drift是一个优化的框架，用于在设备上执行生成式AI工作负载，支持比现有模型多10到100倍的参数，并显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，设备上推理的需求因隐私和效率问题而增加。GPU作为广泛使用的设备上ML加速器，需要更高效的框架来支持复杂模型。

Method: ML Drift通过优化GPU加速推理引擎，解决了跨GPU API开发的工程挑战，并确保在移动和桌面/笔记本平台上的广泛兼容性。

Result: ML Drift实现了比现有开源GPU推理引擎高一个数量级的性能提升。

Conclusion: ML Drift为资源受限设备上部署更复杂的生成式AI模型提供了高效解决方案。

Abstract: Driven by the advancements in generative AI, large machine learning models
have revolutionized domains such as image processing, audio synthesis, and
speech recognition. While server-based deployments remain the locus of peak
performance, the imperative for on-device inference, necessitated by privacy
and efficiency considerations, persists. Recognizing GPUs as the on-device ML
accelerator with the widest reach, we present ML Drift--an optimized framework
that extends the capabilities of state-of-the-art GPU-accelerated inference
engines. ML Drift enables on-device execution of generative AI workloads which
contain 10 to 100x more parameters than existing on-device generative AI
models. ML Drift addresses intricate engineering challenges associated with
cross-GPU API development, and ensures broad compatibility across mobile and
desktop/laptop platforms, thereby facilitating the deployment of significantly
more complex models on resource-constrained devices. Our GPU-accelerated ML/AI
inference engine achieves an order-of-magnitude performance improvement
relative to existing open-source GPU inference engines.

</details>

### [26] [Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks](https://arxiv.org/abs/2505.00234)
*Vishnu Sarukkai,Zhiqiang Xie,Kayvon Fatahalian*

Main category: cs.LG

TLDR: 论文提出了一种通过自动学习成功经验提升LLM代理性能的方法，避免了任务特定的知识工程。


<details>
  <summary>Details</summary>
Motivation: 减少对任务特定知识工程的依赖，通过自动学习成功经验提升代理性能。

Method: 构建并优化自生成示例数据库，引入数据库级和示例级选择策略。

Result: 在ALFWorld、Wordcraft和InterCode-SQL基准上性能显著提升，最高达91%。

Conclusion: 自动轨迹数据库构建是替代人工知识工程的有效方法。

Abstract: Many methods for improving Large Language Model (LLM) agents for sequential
decision-making tasks depend on task-specific knowledge engineering--such as
prompt tuning, curated in-context examples, or customized observation and
action spaces. Using these approaches, agent performance improves with the
quality or amount of knowledge engineering invested. Instead, we investigate
how LLM agents can automatically improve their performance by learning
in-context from their own successful experiences on similar tasks. Rather than
relying on task-specific knowledge engineering, we focus on constructing and
refining a database of self-generated examples. We demonstrate that even a
naive accumulation of successful trajectories across training tasks boosts test
performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%),
and InterCode-SQL (75% to 79%)--matching the performance the initial agent
achieves if allowed two to three attempts per task. We then introduce two
extensions: (1) database-level selection through population-based training to
identify high-performing example collections, and (2) exemplar-level selection
that retains individual trajectories based on their empirical utility as
in-context examples. These extensions further enhance performance, achieving
91% on ALFWorld--matching more complex approaches that employ task-specific
components and prompts. Our results demonstrate that automatic trajectory
database construction offers a compelling alternative to labor-intensive
knowledge engineering.

</details>

### [27] [Node2Vec-DGI-EL: A Hierarchical Graph Representation Learning Model for Ingredient-Disease Association Prediction](https://arxiv.org/abs/2505.00236)
*Leifeng Zhang,Xin Dong,Shuaibing Jia,Jianhua Zhang*

Main category: cs.LG

TLDR: 该研究提出了一种基于分层图表示学习的成分-疾病关联预测模型（Node2Vec-DGI-EL），通过结合Node2Vec、DGI和集成学习方法，显著提高了预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统中药成分作为现代药物开发的重要来源，其与疾病的潜在关联预测具有重要价值。

Method: 模型结合Node2Vec提取节点嵌入向量，DGI进行深度表示学习，并引入集成学习方法提升预测精度。

Result: 模型表现优异，AUC为0.9987，AUPR为0.9545，并通过案例研究和分子对接验证了预测结果。

Conclusion: Node2Vec-DGI-EL模型有效预测中药成分与疾病的关联，克服了对节点语义信息的依赖。

Abstract: Traditional Chinese medicine, as an essential component of traditional
medicine, contains active ingredients that serve as a crucial source for modern
drug development, holding immense therapeutic potential and development value.
A multi-layered and complex network is formed from Chinese medicine to diseases
and used to predict the potential associations between Chinese medicine
ingredients and diseases. This study proposes an ingredient-disease association
prediction model (Node2Vec-DGI-EL) based on hierarchical graph representation
learning. First, the model uses the Node2Vec algorithm to extract node
embedding vectors from the network as the initial features of the nodes. Next,
the network nodes are deeply represented and learned using the DGI algorithm to
enhance the model's expressive power. To improve prediction accuracy and
robustness, an ensemble learning method is incorporated to achieve more
accurate ingredient-disease association predictions. The effectiveness of the
model is then evaluated through a series of theoretical verifications. The
results demonstrated that the proposed model significantly outperformed
existing methods, achieving an AUC of 0.9987 and an AUPR of 0.9545, thereby
indicating superior predictive capability. Ablation experiments further
revealed the contribution and importance of each module. Additionally, case
studies explored potential associations, such as triptonide with hypertensive
retinopathy and methyl ursolate with colorectal cancer. Molecular docking
experiments validated these findings, showing the triptonide-PGR interaction
and the methyl ursolate-NFE2L2 interaction can bind stable. In conclusion, the
Node2Vec-DGI-EL model focuses on TCM datasets and effectively predicts
ingredient-disease associations, overcoming the reliance on node semantic
information.

</details>

### [28] [Field-scale soil moisture estimated from Sentinel-1 SAR data using a knowledge-guided deep learning approach](https://arxiv.org/abs/2505.00265)
*Yi Yu,Patrick Filippi,Thomas F. A. Bishop*

Main category: cs.LG

TLDR: 该研究提出了一种结合知识引导的深度学习方法，将水云模型（WCM）原理与LSTM模型结合，利用Sentinel-1 SAR数据估计土壤湿度（SM），并通过改进的损失函数减少不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统WCM模型在复杂农业景观中性能受限，需要更精确的SM估计方法。

Method: 结合WCM原理与LSTM模型，利用SAR数据和植被信息，通过改进的损失函数（包括WCM半物理成分和边界条件正则化）进行建模。

Result: 模型减少了SM估计的不确定性（0.02 m³/m³），在多变植被和地表条件下相关系数达0.64。

Conclusion: 该方法有效解决了WCM的过度简化问题，展示了在复杂场景中的潜力。

Abstract: Soil moisture (SM) estimation from active microwave data remains challenging
due to the complex interactions between radar backscatter and surface
characteristics. While the water cloud model (WCM) provides a semi-physical
approach for understanding these interactions, its empirical component often
limits performance across diverse agricultural landscapes. This research
presents preliminary efforts for developing a knowledge-guided deep learning
approach, which integrates WCM principles into a long short-term memory (LSTM)
model, to estimate field SM using Sentinel-1 Synthetic Aperture Radar (SAR)
data. Our proposed approach leverages LSTM's capacity to capture spatiotemporal
dependencies while maintaining physical consistency through a modified
dual-component loss function, including a WCM-based semi-physical component and
a boundary condition regularisation. The proposed approach is built upon the
soil backscatter coefficients isolated from the total backscatter, together
with Landsat-resolution vegetation information and surface characteristics. A
four-fold spatial cross-validation was performed against in-situ SM data to
assess the model performance. Results showed the proposed approach reduced SM
retrieval uncertainties by 0.02 m$^3$/m$^3$ and achieved correlation
coefficients (R) of up to 0.64 in areas with varying vegetation cover and
surface conditions, demonstrating the potential to address the
over-simplification in WCM.

</details>

### [29] [Policies of Multiple Skill Levels for Better Strength Estimation in Games](https://arxiv.org/abs/2505.00279)
*Kyota Kuboki,Tatsuyoshi Ogawa,Chu-Hsuan Hsueh,Shi-Jim Yen,Kokolo Ikeda*

Main category: cs.LG

TLDR: 论文提出了一种结合玩家行为倾向和强度评分的方法，以提高人类技能水平估计的准确性，实验显示在围棋和国际象棋中均取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 准确估计人类技能水平对于设计有效的人机交互至关重要，以便AI能提供适当的挑战或指导。

Method: 通过结合神经网络训练的玩家行为策略和强度评分，改进技能水平估计。

Result: 在围棋中，10场比赛的估计准确率从71%提升至80%，20场比赛从84%提升至92%；国际象棋中也有类似改进。

Conclusion: 该方法显著提高了技能水平估计的准确性，有助于优化人机交互。

Abstract: Accurately estimating human skill levels is crucial for designing effective
human-AI interactions so that AI can provide appropriate challenges or
guidance. In games where AI players have beaten top human professionals,
strength estimation plays a key role in adapting AI behavior to match human
skill levels. In a previous state-of-the-art study, researchers have proposed a
strength estimator trained using human players' match data. Given some matches,
the strength estimator computes strength scores and uses them to estimate
player ranks (skill levels). In this paper, we focus on the observation that
human players' behavior tendency varies according to their strength and aim to
improve the accuracy of strength estimation by taking this into account.
Specifically, in addition to strength scores, we obtain policies for different
skill levels from neural networks trained using human players' match data. We
then combine features based on these policies with the strength scores to
estimate strength. We conducted experiments on Go and chess. For Go, our method
achieved an accuracy of 80% in strength estimation when given 10 matches, which
increased to 92% when given 20 matches. In comparison, the previous
state-of-the-art method had an accuracy of 71% with 10 matches and 84% with 20
matches, demonstrating improvements of 8-9%. We observed similar improvements
in chess. These results contribute to developing a more accurate strength
estimation method and to improving human-AI interaction.

</details>

### [30] [Multi-Hierarchical Fine-Grained Feature Mapping Driven by Feature Contribution for Molecular Odor Prediction](https://arxiv.org/abs/2505.00290)
*Hong Xin Xie,Jian De Sun,Fan Fu Xue,Zi Fei Han,Shan Shan Feng,Qi Chen*

Main category: cs.LG

TLDR: 提出了一种名为HMFNet的新方法，通过原子级特征提取和全局特征学习，结合化学知识损失函数，显著提升了分子气味预测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖简单描述符或手工特征，表达能力不足且存在类别不平衡问题，限制了AI模型的训练效果。

Method: 提出HMFNet，包含局部多层级特征提取模块（LMFE）、谐波调制特征映射（HMFM）、全局多层级特征提取模块（GMFE）和化学知识损失函数（CIL）。

Result: 实验表明，该方法显著提升了多种深度学习模型的性能。

Conclusion: HMFNet在分子结构表示和AI驱动技术发展方面具有潜力。

Abstract: Molecular odor prediction is the process of using a molecule's structure to
predict its smell. While accurate prediction remains challenging, AI models can
suggest potential odors. Existing methods, however, often rely on basic
descriptors or handcrafted fingerprints, which lack expressive power and hinder
effective learning. Furthermore, these methods suffer from severe class
imbalance, limiting the training effectiveness of AI models. To address these
challenges, we propose a Feature Contribution-driven Hierarchical Multi-Feature
Mapping Network (HMFNet). Specifically, we introduce a fine-grained, Local
Multi-Hierarchy Feature Extraction module (LMFE) that performs deep feature
extraction at the atomic level, capturing detailed features crucial for odor
prediction. To enhance the extraction of discriminative atomic features, we
integrate a Harmonic Modulated Feature Mapping (HMFM). This module dynamically
learns feature importance and frequency modulation, improving the model's
capability to capture relevant patterns. Additionally, a Global Multi-Hierarchy
Feature Extraction module (GMFE) is designed to learn global features from the
molecular graph topology, enabling the model to fully leverage global
information and enhance its discriminative power for odor prediction. To
further mitigate the issue of class imbalance, we propose a Chemically-Informed
Loss (CIL). Experimental results demonstrate that our approach significantly
improves performance across various deep learning models, highlighting its
potential to advance molecular structure representation and accelerate the
development of AI-driven technologies.

</details>

### [31] [Repetition Makes Perfect: Recurrent Sum-GNNs Match Message Passing Limit](https://arxiv.org/abs/2505.00291)
*Eran Rosenbluth,Martin Grohe*

Main category: cs.LG

TLDR: 论文证明了具有有限精度参数的循环图神经网络（recurrent GNNs）的表达能力可以达到颜色细化算法（Weisfeiler-Leman）的极限，且能模拟任何图算法。


<details>
  <summary>Details</summary>
Motivation: 研究循环GNNs的表达能力，特别是其是否能达到颜色细化算法的极限，并与非循环GNNs进行比较。

Method: 使用sum聚合和ReLU激活的循环GNNs，通过构造模拟图算法的方法，并引入多项式时间和空间开销。

Result: 循环GNNs可以模拟任何图算法，且通过随机初始化，还能模拟所有多项式时间复杂度的图算法。

Conclusion: 循环GNNs的表达能力达到了颜色细化算法的极限，且在实际应用中具有高效性。

Abstract: We provide first tight bounds for the expressivity of Recurrent Graph Neural
Networks (recurrent GNNs) with finite-precision parameters. We prove that
recurrent GNNs, with sum aggregation and ReLU activation, can emulate any graph
algorithm that respects the natural message-passing invariance induced by the
color refinement (or Weisfeiler-Leman) algorithm. While it is well known that
the expressive power of GNNs is limited by this invariance [Morris et al., AAAI
2019; Xu et al., ICLR 2019], we establish that recurrent GNNs can actually
reach this limit. This is in contrast to non-recurrent GNNs, which have the
power of Weisfeiler-Leman only in a very weak, "non-uniform", sense where every
graph size requires a different GNN model to compute with. The emulation we
construct introduces only a polynomial overhead in both time and space.
  Furthermore, we show that by incorporating random initialization, recurrent
GNNs can emulate all graph algorithms, implying in particular that any graph
algorithm with polynomial-time complexity can be emulated by a recurrent GNN
with random initialization, running in polynomial time.

</details>

### [32] [Temporal Attention Evolutional Graph Convolutional Network for Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.00302)
*Xinlong Zhao,Liying Zhang,Tianbo Zou,Yan Zhang*

Main category: cs.LG

TLDR: 提出了一种名为TAEGCN的新方法，通过动态图结构和多尺度时间注意力机制提升多元时间序列预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设固定图结构，而现实场景中图结构是动态的，且时间序列的交互在不同时间尺度上变化显著。

Method: 结合因果时间卷积和多头自注意力机制学习节点时间特征，并基于这些特征构建动态图结构，同时通过统一神经网络整合组件生成预测。

Result: 在METR-LA和PEMS-BAY两个公开交通网络数据集上验证了TAEGCN的优越性能。

Conclusion: TAEGCN能有效捕捉时空特征，提升预测准确性，适用于动态图结构的多元时间序列预测。

Abstract: Multivariate time series forecasting enables the prediction of future states
by leveraging historical data, thereby facilitating decision-making processes.
Each data node in a multivariate time series encompasses a sequence of multiple
dimensions. These nodes exhibit interdependent relationships, forming a graph
structure. While existing prediction methods often assume a fixed graph
structure, many real-world scenarios involve dynamic graph structures.
Moreover, interactions among time series observed at different time scales vary
significantly. To enhance prediction accuracy by capturing precise temporal and
spatial features, this paper introduces the Temporal Attention Evolutional
Graph Convolutional Network (TAEGCN). This novel method not only integrates
causal temporal convolution and a multi-head self-attention mechanism to learn
temporal features of nodes, but also construct the dynamic graph structure
based on these temporal features to keep the consistency of the changing in
spatial feature with temporal series. TAEGCN adeptly captures temporal causal
relationships and hidden spatial dependencies within the data. Furthermore,
TAEGCN incorporates a unified neural network that seamlessly integrates these
components to generate final predictions. Experimental results conducted on two
public transportation network datasets, METR-LA and PEMS-BAY, demonstrate the
superior performance of the proposed model.

</details>

### [33] [Gateformer: Advancing Multivariate Time Series Forecasting through Temporal and Variate-Wise Attention with Gated Representations](https://arxiv.org/abs/2505.00307)
*Yu-Hsiang Lan,Anton Alyakin,Eric K. Oermann*

Main category: cs.LG

TLDR: 该论文提出了一种改进的Transformer架构（Gateformer），用于高效建模多元时间序列中的跨时间和跨变量依赖关系，并在13个真实数据集上实现了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列预测需要同时建模跨时间和跨变量的依赖关系，但现有Transformer架构在这两方面的整合上存在挑战。本文旨在优化这种整合，以提升性能和效率。

Method: 通过独立嵌入每个变量的跨时间动态表示，并利用注意力机制建模跨变量依赖关系。在跨时间和跨变量建模阶段引入门控操作，以调节信息流。

Result: 在13个真实数据集上实现了最优性能，性能提升最高达20.7%，且可无缝集成到其他基于Transformer和LLM的预测模型中。

Conclusion: Gateformer通过门控机制和注意力机制的有效结合，显著提升了多元时间序列预测的性能和效率。

Abstract: There has been a recent surge of interest in time series modeling using the
Transformer architecture. However, forecasting multivariate time series with
Transformer presents a unique challenge as it requires modeling both temporal
(cross-time) and variate (cross-variate) dependencies. While Transformer-based
models have gained popularity for their flexibility in capturing both
sequential and cross-variate relationships, it is unclear how to best integrate
these two sources of information in the context of the Transformer architecture
while optimizing for both performance and efficiency. We re-purpose the
Transformer architecture to effectively model both cross-time and cross-variate
dependencies. Our approach begins by embedding each variate independently into
a variate-wise representation that captures its cross-time dynamics, and then
models cross-variate dependencies through attention mechanisms on these learned
embeddings. Gating operations in both cross-time and cross-variate modeling
phases regulate information flow, allowing the model to focus on the most
relevant features for accurate predictions. Our method achieves
state-of-the-art performance across 13 real-world datasets and can be
seamlessly integrated into other Transformer-based and LLM-based forecasters,
delivering performance improvements up to 20.7\% over original models. Code is
available at this repository: https://github.com/nyuolab/Gateformer.

</details>

### [34] [Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing](https://arxiv.org/abs/2505.00315)
*Piotr Piękos,Róbert Csordás,Jürgen Schmidhuber*

Main category: cs.LG

TLDR: MoSA提出了一种动态稀疏注意力机制，通过选择关键token降低计算复杂度，性能优于密集注意力。


<details>
  <summary>Details</summary>
Motivation: 解决自注意力机制的二次计算成本问题，同时保持性能。

Method: 基于Mixture of Experts的MoSA方法，动态选择token实现稀疏注意力。

Result: MoSA在相同计算预算下性能提升27%，并减少资源占用。

Conclusion: MoSA是高效且性能优越的稀疏注意力替代方案。

Abstract: Recent advances in large language models highlighted the excessive quadratic
cost of self-attention. Despite the significant research efforts, subquadratic
attention methods still suffer from inferior performance in practice. We
hypothesize that dynamic, learned content-based sparsity can lead to more
efficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),
a novel approach inspired by Mixture of Experts (MoE) with expert choice
routing. MoSA dynamically selects tokens for each attention head, allowing
arbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of
length $T$, MoSA reduces the computational complexity of each attention head
from $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same
computational budget, allowing higher specialization. We show that among the
tested sparse attention variants, MoSA is the only one that can outperform the
dense baseline, sometimes with up to 27% better perplexity for an identical
compute budget. MoSA can also reduce the resource usage compared to dense
self-attention. Despite using torch implementation without an optimized kernel,
perplexity-matched MoSA models are simultaneously faster in wall-clock time,
require less memory for training, and drastically reduce the size of the
KV-cache compared to the dense transformer baselines.

</details>

### [35] [Surrogate modeling of Cellular-Potts Agent-Based Models as a segmentation task using the U-Net neural network architecture](https://arxiv.org/abs/2505.00316)
*Tien Comlekoglu,J. Quetzalcóatl Toledo-Marín,Tina Comlekoglu,Douglas W. DeSimone,Shayn M. Peirce,Geoffrey Fox,James A. Glazier*

Main category: cs.LG

TLDR: 使用U-Net架构的卷积神经网络（CNN）替代模型加速了Cellular-Potts模型（CPM）的计算，速度提升590倍，并成功捕捉了血管生成等复杂行为。


<details>
  <summary>Details</summary>
Motivation: CPM计算成本高，限制了其在复杂生物系统模拟中的应用，因此需要开发高效的替代模型。

Method: 采用U-Net架构的CNN模型，考虑周期性边界条件，训练模型预测100个计算步骤（MCS）。

Result: 替代模型速度提升590倍，成功模拟了血管生成的关键行为（如血管分支、延伸和吻合）。

Conclusion: 深度学习可作为CPM的高效替代模型，显著提升计算效率，适用于更大时空尺度的生物过程模拟。

Abstract: The Cellular-Potts model is a powerful and ubiquitous framework for
developing computational models for simulating complex multicellular biological
systems. Cellular-Potts models (CPMs) are often computationally expensive due
to the explicit modeling of interactions among large numbers of individual
model agents and diffusive fields described by partial differential equations
(PDEs). In this work, we develop a convolutional neural network (CNN) surrogate
model using a U-Net architecture that accounts for periodic boundary
conditions. We use this model to accelerate the evaluation of a mechanistic CPM
previously used to investigate \textit{in vitro} vasculogenesis. The surrogate
model was trained to predict 100 computational steps ahead (Monte-Carlo steps,
MCS), accelerating simulation evaluations by a factor of 590 times compared to
CPM code execution. Over multiple recursive evaluations, our model effectively
captures the emergent behaviors demonstrated by the original Cellular-Potts
model of such as vessel sprouting, extension and anastomosis, and contraction
of vascular lacunae. This approach demonstrates the potential for deep learning
to serve as efficient surrogate models for CPM simulations, enabling faster
evaluation of computationally expensive CPM of biological processes at greater
spatial and temporal scales.

</details>

### [36] [Optimal Vector Compressed Sensing Using James Stein Shrinkage](https://arxiv.org/abs/2505.00326)
*Apratim Dey,David Donoho*

Main category: cs.LG

TLDR: 论文提出了一种名为SteinSense的轻量级迭代算法，用于高维向量恢复，证明其在B较大时是最优的。


<details>
  <summary>Details</summary>
Motivation: 传统基于凸优化的方法在高维向量恢复中被证明是次优的，特别是在B较大时。

Method: 提出SteinSense算法，无需调参、训练数据或稀疏性知识，实现简单且可扩展。

Result: 大量实验证实SteinSense的有效性，并在理论和实际数据中表现稳健。

Conclusion: SteinSense在高维向量恢复中具有优越性和鲁棒性。

Abstract: The trend in modern science and technology is to take vector measurements
rather than scalars, ruthlessly scaling to ever higher dimensional vectors. For
about two decades now, traditional scalar Compressed Sensing has been
synonymous with a Convex Optimization based procedure called Basis Pursuit. In
the vector recovery case, the natural tendency is to return to a
straightforward vector extension of Basis Pursuit, also based on Convex
Optimization. However, Convex Optimization is provably suboptimal, particularly
when $B$ is large. In this paper, we propose SteinSense, a lightweight
iterative algorithm, which is provably optimal when $B$ is large. It does not
have any tuning parameter, does not need any training data, requires zero
knowledge of sparsity, is embarrassingly simple to implement, and all of this
makes it easily scalable to high vector dimensions. We conduct a massive volume
of both real and synthetic experiments that confirm the efficacy of SteinSense,
and also provide theoretical justification based on ideas from Approximate
Message Passing. Fascinatingly, we discover that SteinSense is quite robust,
delivering the same quality of performance on real data, and even under
substantial departures from conditions under which existing theory holds.

</details>

### [37] [Communication-Efficient Wireless Federated Fine-Tuning for Large-Scale AI Models](https://arxiv.org/abs/2505.00333)
*Bumjun Kim,Wan Choi*

Main category: cs.LG

TLDR: 本文提出了一种无线联邦LoRA微调框架，通过低秩适应（LoRA）和自适应稀疏化方法（SOFT）优化学习性能和通信效率，同时提出两阶段联邦算法（TSFA）动态调整参数，显著减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决在联邦学习（FL）环境下微调大规模语言模型（LLMs）时面临的资源限制和通信开销问题。

Method: 采用低秩适应（LoRA）和自适应稀疏化方法（SOFT），结合两阶段联邦算法（TSFA）动态调整参数。

Result: 实验表明，该方法在基准数据集上达到与理想场景相当的准确性，同时显著减少通信开销。

Conclusion: 该框架为大规模模型在无线联邦学习场景中的高效部署提供了可行方案。

Abstract: Transformer-based large language models (LLMs) have achieved remarkable
success across various tasks. Yet, fine-tuning such massive models in federated
learning (FL) settings poses significant challenges due to resource constraints
and communication overhead. Low-Rank Adaptation (LoRA) addresses these issues
by training compact, low-rank matrices instead of fully fine-tuning large
models. This paper introduces a wireless federated LoRA fine-tuning framework
that optimizes both learning performance and communication efficiency. We
provide a novel convergence analysis, revealing how LoRA rank and covariance
effects influence FL training dynamics. Leveraging these insights, we propose
Sparsified Orthogonal Fine-Tuning (\textbf{SOFT}), an adaptive sparsification
method that streamlines parameter updates without expensive matrix
multiplications and singular value decomposition (SVD) operations.
Additionally, we present a Two Stage Federated Algorithm (\textbf{TSFA})
algorithm that pre-determines key parameters offline and dynamically adjusts
bandwidth and sparsification online, ensuring efficient training under latency
constraints. Experiments on benchmark datasets show that our approach achieves
accuracy comparable to ideal scenario models while significantly reducing
communication overhead. Our framework thus enables scalable, resource-efficient
deployment of large models in real-world wireless FL scenarios.

</details>

### [38] [T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation](https://arxiv.org/abs/2505.00337)
*Xuyang Guo,Jiayan Huo,Zhenmei Shi,Zhao Song,Jiahao Zhang,Jiale Zhao*

Main category: cs.LG

TLDR: T2VPhysBench是一个基于物理原理的基准测试，用于评估文本到视频生成模型是否遵守核心物理定律，结果显示当前模型普遍表现不佳。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到视频生成模型在美学和指令遵循上表现优异，但其对物理定律的遵守能力尚未被系统评估，导致生成内容可能违反基本物理规则。

Method: 通过T2VPhysBench基准测试，结合人类评估和三个具体研究（总体合规性评估、提示消融实验和反事实鲁棒性测试），系统评估模型对12项核心物理定律的遵守情况。

Result: 所有模型在每个物理定律类别中的平均得分低于0.60，即使提供详细提示也无法显著改善物理违规问题，且模型在反事实指令下会生成明显违反物理规则的内容。

Conclusion: 当前文本到视频生成模型在物理规则遵守方面存在显著不足，需进一步研究以实现真正物理感知的视频生成。

Abstract: Text-to-video generative models have made significant strides in recent
years, producing high-quality videos that excel in both aesthetic appeal and
accurate instruction following, and have become central to digital art creation
and user engagement online. Yet, despite these advancements, their ability to
respect fundamental physical laws remains largely untested: many outputs still
violate basic constraints such as rigid-body collisions, energy conservation,
and gravitational dynamics, resulting in unrealistic or even misleading
content. Existing physical-evaluation benchmarks typically rely on automatic,
pixel-level metrics applied to simplistic, life-scenario prompts, and thus
overlook both human judgment and first-principles physics. To fill this gap, we
introduce \textbf{T2VPhysBench}, a first-principled benchmark that
systematically evaluates whether state-of-the-art text-to-video systems, both
open-source and commercial, obey twelve core physical laws including Newtonian
mechanics, conservation principles, and phenomenological effects. Our benchmark
employs a rigorous human evaluation protocol and includes three targeted
studies: (1) an overall compliance assessment showing that all models score
below 0.60 on average in each law category; (2) a prompt-hint ablation
revealing that even detailed, law-specific hints fail to remedy physics
violations; and (3) a counterfactual robustness test demonstrating that models
often generate videos that explicitly break physical rules when so instructed.
The results expose persistent limitations in current architectures and offer
concrete insights for guiding future research toward truly physics-aware video
generation.

</details>

### [39] [Pushing the Limits of Low-Bit Optimizers: A Focus on EMA Dynamics](https://arxiv.org/abs/2505.00347)
*Cong Xu,Wenbin Liang,Mo Yu,Anan Liu,Ke-Yue Zhang,Lizhuang Ma,Jianyong Wang,Jun Wang,Wei Zhang*

Main category: cs.LG

TLDR: 论文提出了一种新型优化器SOLO，通过超低精度量化（低至2-3位）显著减少状态开销，解决了无符号量化中的信号淹没问题和有符号量化中的梯度方差问题，实现了内存节省（如7B模型节省45GB）且精度损失极小。


<details>
  <summary>Details</summary>
Motivation: 模型规模爆炸导致训练/微调成本剧增，尤其是状态优化器的辅助信息开销巨大（可达模型大小的2倍）。为降低资源需求，促进基础研究的可及性，提出了SOLO。

Method: 采用超低精度量化（2-3位），针对无符号量化中的信号淹没问题提出对数量化，针对有符号量化中的梯度方差问题设计精度特定的动量值。

Result: SOLO显著节省内存（如7B模型节省45GB），且精度损失极小。

Conclusion: SOLO通过超低精度量化解决了优化器的状态开销问题，有望突破计算资源瓶颈，推动基础研究的普及。

Abstract: The explosion in model sizes leads to continued growth in prohibitive
training/fine-tuning costs, particularly for stateful optimizers which maintain
auxiliary information of even 2x the model size to achieve optimal convergence.
We therefore present in this work a novel type of optimizer that carries with
extremely lightweight state overloads, achieved through ultra-low-precision
quantization. While previous efforts have achieved certain success with 8-bit
or 4-bit quantization, our approach enables optimizers to operate at precision
as low as 3 bits, or even 2 bits per state element. This is accomplished by
identifying and addressing two critical challenges: the signal swamping problem
in unsigned quantization that results in unchanged state dynamics, and the
rapidly increased gradient variance in signed quantization that leads to
incorrect descent directions. The theoretical analysis suggests a tailored
logarithmic quantization for the former and a precision-specific momentum value
for the latter. Consequently, the proposed SOLO achieves substantial memory
savings (approximately 45 GB when training a 7B model) with minimal accuracy
loss. We hope that SOLO can contribute to overcoming the bottleneck in
computational resources, thereby promoting greater accessibility in fundamental
research.

</details>

### [40] [Validation of a 24-hour-ahead Prediction model for a Residential Electrical Load under diverse climate](https://arxiv.org/abs/2505.00348)
*Ehtisham Asghar,Martin Hill,Ibrahim Sengor,Conor Lynch,Phan Quang An*

Main category: cs.LG

TLDR: 提出了一种全球适用的24小时家庭电力需求预测模型，适用于不同气候和数据集，并在爱尔兰和越南数据上验证了其高准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有预测模型多为区域特定且依赖大数据，缺乏灵活性，难以适应不同气候和数据稀缺地区。

Method: 设计了一个全球模型，利用爱尔兰和越南的数据验证其性能，并与现有机器学习方法对比。

Result: 模型在爱尔兰和越南数据集上的平均绝对百分比误差分别为8.0%和4.0%，优于基准模型。

Conclusion: 该模型具有全球适用性，可提升能源社区的效率和可持续性。

Abstract: Accurate household electrical energy demand prediction is essential for
effectively managing sustainable Energy Communities. Integrated with the Energy
Management System, these communities aim to optimise operational costs.
However, most existing forecasting models are region-specific and depend on
large datasets, limiting their applicability across different climates and
geographical areas. These models often lack flexibility and may not perform
well in regions with limited historical data, leading to inaccurate
predictions. This paper proposes a global model for 24-hour-ahead hourly
electrical energy demand prediction that is designed to perform effectively
across diverse climate conditions and datasets. The model's efficiency is
demonstrated using data from two distinct regions: Ireland, with a maritime
climate and Vietnam, with a tropical climate. Remarkably, the model achieves
high accuracy even with a limited dataset spanning only nine months. Its
robustness is further validated across different seasons in Ireland (summer and
winter) and Vietnam (dry and wet). The proposed model is evaluated against
state-of-the-art machine learning and deep learning methods. Simulation results
indicate that the model consistently outperforms benchmark models, showcasing
its capability to provide reliable forecasts globally, regardless of varying
climatic conditions and data availability. This research underscores the
model's potential to enhance the efficiency and sustainability of Energy
Communities worldwide. The proposed model achieves a Mean Absolute Percentage
Error of 8.0% and 4.0% on the full Irish and Vietnamese datasets.

</details>

### [41] [Optimizing Deep Neural Networks using Safety-Guided Self Compression](https://arxiv.org/abs/2505.00350)
*Mohammad Zbeeb,Mariam Salman,Mohammad Bazzi,Ammar Mohanna*

Main category: cs.LG

TLDR: 本文提出了一种安全驱动的量化框架，通过系统性地剪枝和量化神经网络权重，在保持性能的同时优化模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上部署深度神经网络需要有效的模型压缩策略，以平衡模型大小减少与性能保持。

Method: 利用保护集对神经网络权重进行剪枝和量化，优化模型复杂度。

Result: 在CNN和注意力语言模型上测试，框架在保持60%模型大小的同时，测试精度提升2.5%，且优于传统量化方法。

Conclusion: 安全驱动量化是一种高效优化深度学习模型的可靠策略。

Abstract: The deployment of deep neural networks on resource-constrained devices
necessitates effective model com- pression strategies that judiciously balance
the reduction of model size with the preservation of performance. This study
introduces a novel safety-driven quantization framework that leverages
preservation sets to systematically prune and quantize neural network weights,
thereby optimizing model complexity without compromising accuracy. The proposed
methodology is rigorously evaluated on both a convolutional neural network
(CNN) and an attention-based language model, demonstrating its applicability
across diverse architectural paradigms. Experimental results reveal that our
framework achieves up to a 2.5% enhancement in test accuracy relative to the
original unquantized models while maintaining 60% of the initial model size. In
comparison to conventional quantization techniques, our approach not only
augments generalization by eliminating parameter noise and retaining essential
weights but also reduces variance, thereby ensuring the retention of critical
model features. These findings underscore the efficacy of safety-driven
quantization as a robust and reliable strategy for the efficient optimization
of deep learn- ing models. The implementation and comprehensive experimental
evaluations of our framework are publicly accessible at GitHub.

</details>

### [42] [R&B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training](https://arxiv.org/abs/2505.00358)
*Albert Ge,Tzu-Heng Huang,John Cooper,Avi Trost,Ziyi Chu,Satya Sai Srinath Namburi GNVV,Ziyang Cai,Kendall Park,Nicholas Roberts,Frederic Sala*

Main category: cs.LG

TLDR: R&B框架通过语义相似性重新分组数据并高效优化数据组合，解决了传统数据混合方法的不足，性能优于现有方法且计算开销极低。


<details>
  <summary>Details</summary>
Motivation: 传统数据混合方法依赖预定义的数据域，无法捕捉语义细节且计算成本高。

Method: R&B通过语义相似性重新分组数据（Regroup），并利用域梯度诱导的Gram矩阵高效优化数据组合（Balance）。

Result: 在五个多样化数据集上，R&B性能优于现有方法，仅增加0.01%的计算开销。

Conclusion: R&B是一种高效且性能优越的数据混合策略，适用于多种任务。

Abstract: Data mixing strategies have successfully reduced the costs involved in
training language models. While promising, such methods suffer from two flaws.
First, they rely on predetermined data domains (e.g., data sources, task
types), which may fail to capture critical semantic nuances, leaving
performance on the table. Second, these methods scale with the number of
domains in a computationally prohibitive way. We address these challenges via
R&B, a framework that re-partitions training data based on semantic similarity
(Regroup) to create finer-grained domains, and efficiently optimizes the data
composition (Balance) by leveraging a Gram matrix induced by domain gradients
obtained throughout training. Unlike prior works, it removes the need for
additional compute to obtain evaluation information such as losses or
gradients. We analyze this technique under standard regularity conditions and
provide theoretical insights that justify R&B's effectiveness compared to
non-adaptive mixing approaches. Empirically, we demonstrate the effectiveness
of R&B on five diverse datasets ranging from natural language to reasoning and
multimodal tasks. With as little as 0.01% additional compute overhead, R&B
matches or exceeds the performance of state-of-the-art data mixing strategies.

</details>

### [43] [TNStream: Applying Tightest Neighbors to Micro-Clusters to Define Multi-Density Clusters in Streaming Data](https://arxiv.org/abs/2505.00359)
*Qifen Zeng,Haomin Bao,Yuanzhuo Hu,Zirui Zhang,Yuheng Zheng,Luosheng Wen*

Main category: cs.LG

TLDR: 本文提出了一种基于Tightest Neighbors和Skeleton Set理论的新型数据流聚类算法TNStream，能够自适应处理多密度、高维数据，并保持强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有密度聚类算法难以同时处理复杂多密度、高维数据且抗噪能力不足，导致聚类质量下降。

Method: 提出基于Tightest Neighbors和Skeleton Set理论的TNStream算法，利用局部相似性自适应确定聚类半径，并通过LSH优化高维数据存储。

Result: 在合成和真实数据集上的实验表明，TNStream显著提升了多密度数据的聚类质量。

Conclusion: TNStream为数据流聚类提供了一种高效且鲁棒的新方法，验证了所提理论的实用性。

Abstract: In data stream clustering, systematic theory of stream clustering algorithms
remains relatively scarce. Recently, density-based methods have gained
attention. However, existing algorithms struggle to simultaneously handle
arbitrarily shaped, multi-density, high-dimensional data while maintaining
strong outlier resistance. Clustering quality significantly deteriorates when
data density varies complexly. This paper proposes a clustering algorithm based
on the novel concept of Tightest Neighbors and introduces a data stream
clustering theory based on the Skeleton Set. Based on these theories, this
paper develops a new method, TNStream, a fully online algorithm. The algorithm
adaptively determines the clustering radius based on local similarity,
summarizing the evolution of multi-density data streams in micro-clusters. It
then applies a Tightest Neighbors-based clustering algorithm to form final
clusters. To improve efficiency in high-dimensional cases, Locality-Sensitive
Hashing (LSH) is employed to structure micro-clusters, addressing the challenge
of storing k-nearest neighbors. TNStream is evaluated on various synthetic and
real-world datasets using different clustering metrics. Experimental results
demonstrate its effectiveness in improving clustering quality for multi-density
data and validate the proposed data stream clustering theory.

</details>

### [44] [From GNNs to Trees: Multi-Granular Interpretability for Graph Neural Networks](https://arxiv.org/abs/2505.00364)
*Jie Yang,Yuwen Wang,Kaixuan Chen,Tongya Zheng,Yihe Zhou,Zhenbang Xiao,Ji Cao,Mingli Song,Shunyu Liu*

Main category: cs.LG

TLDR: 提出了一种新的树状可解释框架（TIF），用于图分类任务，通过分层树结构捕捉多粒度关系，提升全局可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于子图的可解释方法过于关注局部结构，忽略了长程依赖关系，且固定粒度的图粗化方法无法适应多粒度任务需求。

Method: TIF通过图粗化和扰动模块构建分层树结构，并利用自适应路由模块选择信息量最大的路径，实现多粒度可解释性。

Result: 实验表明，TIF在可解释性和预测性能上均优于现有方法。

Conclusion: TIF为图分类任务提供了一种灵活且可解释的解决方案，适用于多粒度关系场景。

Abstract: Interpretable Graph Neural Networks (GNNs) aim to reveal the underlying
reasoning behind model predictions, attributing their decisions to specific
subgraphs that are informative. However, existing subgraph-based interpretable
methods suffer from an overemphasis on local structure, potentially overlooking
long-range dependencies within the entire graphs. Although recent efforts that
rely on graph coarsening have proven beneficial for global interpretability,
they inevitably reduce the graphs to a fixed granularity. Such an inflexible
way can only capture graph connectivity at a specific level, whereas real-world
graph tasks often exhibit relationships at varying granularities (e.g.,
relevant interactions in proteins span from functional groups, to amino acids,
and up to protein domains). In this paper, we introduce a novel Tree-like
Interpretable Framework (TIF) for graph classification, where plain GNNs are
transformed into hierarchical trees, with each level featuring coarsened graphs
of different granularity as tree nodes. Specifically, TIF iteratively adopts a
graph coarsening module to compress original graphs (i.e., root nodes of trees)
into increasingly coarser ones (i.e., child nodes of trees), while preserving
diversity among tree nodes within different branches through a dedicated graph
perturbation module. Finally, we propose an adaptive routing module to identify
the most informative root-to-leaf paths, providing not only the final
prediction but also the multi-granular interpretability for the decision-making
process. Extensive experiments on the graph classification benchmarks with both
synthetic and real-world datasets demonstrate the superiority of TIF in
interpretability, while also delivering a competitive prediction performance
akin to the state-of-the-art counterparts.

</details>

### [45] [SacFL: Self-Adaptive Federated Continual Learning for Resource-Constrained End Devices](https://arxiv.org/abs/2505.00365)
*Zhengyi Zhong,Weidong Bao,Ji Wang,Jianguo Chen,Lingjuan Lyu,Wei Yang Bryan Lim*

Main category: cs.LG

TLDR: 论文提出了一种名为SacFL的新型联邦持续学习框架，通过编码器-解码器架构和对比学习，解决资源受限终端设备在数据漂移和对抗任务中的挑战。


<details>
  <summary>Details</summary>
Motivation: 终端设备上的机器学习模型面临动态数据漂移和隐私问题，传统持续学习方法不适用，需要一种既能保护隐私又能高效更新的解决方案。

Method: SacFL采用编码器-解码器架构分离任务鲁棒和任务敏感组件，结合对比学习实现自主数据漂移检测。

Result: 在Cifar100和THUCNews等数据集上的实验验证了SacFL在类和域增量场景中的有效性，并开发了演示系统。

Conclusion: SacFL为终端设备提供了一种高效、自主的持续学习解决方案，适用于实际应用场景。

Abstract: The proliferation of end devices has led to a distributed computing paradigm,
wherein on-device machine learning models continuously process diverse data
generated by these devices. The dynamic nature of this data, characterized by
continuous changes or data drift, poses significant challenges for on-device
models. To address this issue, continual learning (CL) is proposed, enabling
machine learning models to incrementally update their knowledge and mitigate
catastrophic forgetting. However, the traditional centralized approach to CL is
unsuitable for end devices due to privacy and data volume concerns. In this
context, federated continual learning (FCL) emerges as a promising solution,
preserving user data locally while enhancing models through collaborative
updates. Aiming at the challenges of limited storage resources for CL, poor
autonomy in task shift detection, and difficulty in coping with new adversarial
tasks in FCL scenario, we propose a novel FCL framework named SacFL. SacFL
employs an Encoder-Decoder architecture to separate task-robust and
task-sensitive components, significantly reducing storage demands by retaining
lightweight task-sensitive components for resource-constrained end devices.
Moreover, $\rm{SacFL}$ leverages contrastive learning to introduce an
autonomous data shift detection mechanism, enabling it to discern whether a new
task has emerged and whether it is a benign task. This capability ultimately
allows the device to autonomously trigger CL or attack defense strategy without
additional information, which is more practical for end devices. Comprehensive
experiments conducted on multiple text and image datasets, such as Cifar100 and
THUCNews, have validated the effectiveness of $\rm{SacFL}$ in both
class-incremental and domain-incremental scenarios. Furthermore, a demo system
has been developed to verify its practicality.

</details>

### [46] [Learning to Estimate Package Delivery Time in Mixed Imbalanced Delivery and Pickup Logistics Services](https://arxiv.org/abs/2505.00375)
*Jinhui Yi,Huan Yan,Haotian Wang,Jian Yuan,Yong Li*

Main category: cs.LG

TLDR: 论文提出了一种基于Transformer的多任务包裹送达时间预测模型TransPDT，解决了混合物流场景中取件对配送时间预测的更高影响问题。


<details>
  <summary>Details</summary>
Motivation: 准确预测包裹送达时间对物流行业至关重要，尤其是在取件和配送混合的场景中，取件对快递员决策行为的影响更大，但现有研究未充分关注这一点。

Method: 使用Transformer编码器捕捉快递员历史路径和待处理包裹的时空依赖，设计模式记忆模块通过注意力机制学习取件模式，并将路径预测作为辅助任务。

Result: 在真实工业规模数据集上的实验证明了方法的优越性，TransPDT已在京东物流内部部署，用于追踪北京地区2000多名快递员的每日数十万包裹配送。

Conclusion: TransPDT通过多任务学习和时空依赖建模，有效提升了混合物流场景中的包裹送达时间预测准确性。

Abstract: Accurately estimating package delivery time is essential to the logistics
industry, which enables reasonable work allocation and on-time service
guarantee. This becomes even more necessary in mixed logistics scenarios where
couriers handle a high volume of delivery and a smaller number of pickup
simultaneously. However, most of the related works treat the pickup and
delivery patterns on couriers' decision behavior equally, neglecting that the
pickup has a greater impact on couriers' decision-making compared to the
delivery due to its tighter time constraints. In such context, we have three
main challenges: 1) multiple spatiotemporal factors are intricately
interconnected, significantly affecting couriers' delivery behavior; 2) pickups
have stricter time requirements but are limited in number, making it
challenging to model their effects on couriers' delivery process; 3) couriers'
spatial mobility patterns are critical determinants of their delivery behavior,
but have been insufficiently explored. To deal with these, we propose TransPDT,
a Transformer-based multi-task package delivery time prediction model. We first
employ the Transformer encoder architecture to capture the spatio-temporal
dependencies of couriers' historical travel routes and pending package sets.
Then we design the pattern memory to learn the patterns of pickup in the
imbalanced dataset via attention mechanism. We also set the route prediction as
an auxiliary task of delivery time prediction, and incorporate the prior
courier spatial movement regularities in prediction. Extensive experiments on
real industry-scale datasets demonstrate the superiority of our method. A
system based on TransPDT is deployed internally in JD Logistics to track more
than 2000 couriers handling hundreds of thousands of packages per day in
Beijing.

</details>

### [47] [Approximation to Deep Q-Network by Stochastic Delay Differential Equations](https://arxiv.org/abs/2505.00382)
*Jianya Lu,Yingjun Mo*

Main category: cs.LG

TLDR: 论文通过构建基于DQN的随机微分延迟方程（SDDE），估计其与DQN的Wasserstein-1距离，并证明距离随步长趋零而收敛。


<details>
  <summary>Details</summary>
Motivation: 尽管DQN在强化学习中取得重大突破，但其理论分析仍有限，本文旨在填补这一空白。

Method: 构建SDDE模型，利用Lindeberg原理和算子比较方法，分析DQN与SDDE的距离。

Result: 给出了距离的上界，证明距离随步长趋零而收敛，揭示了目标网络对系统稳定性的作用。

Conclusion: 从连续系统视角理解DQN的关键技术（经验回放和目标网络），为理论分析提供新思路。

Abstract: Despite the significant breakthroughs that the Deep Q-Network (DQN) has
brought to reinforcement learning, its theoretical analysis remains limited. In
this paper, we construct a stochastic differential delay equation (SDDE) based
on the DQN algorithm and estimate the Wasserstein-1 distance between them. We
provide an upper bound for the distance and prove that the distance between the
two converges to zero as the step size approaches zero. This result allows us
to understand DQN's two key techniques, the experience replay and the target
network, from the perspective of continuous systems. Specifically, the delay
term in the equation, corresponding to the target network, contributes to the
stability of the system. Our approach leverages a refined Lindeberg principle
and an operator comparison to establish these results.

</details>

### [48] [Safety in the Face of Adversity: Achieving Zero Constraint Violation in Online Learning with Slowly Changing Constraints](https://arxiv.org/abs/2505.00398)
*Bassel Hamoud,Ilnura Usmanova,Kfir Y. Levy*

Main category: cs.LG

TLDR: 本文首次在在线凸优化（OCO）中实现了所有轮次的零约束违反，并处理了动态约束变化。通过原始-对偶方法和双重空间中的在线梯度上升，确保了严格安全性。


<details>
  <summary>Details</summary>
Motivation: 现有方法允许偶尔的安全违规，而本文旨在在约束逐渐演变的假设下实现绝对安全性。

Method: 采用原始-对偶方法和双重空间中的在线梯度上升，使用二分学习率确保安全性和次线性遗憾。

Result: 在约束逐渐变化的情况下，实现了零约束违反和次线性遗憾。

Conclusion: 本文首次为OCO中动态约束变化的绝对安全性提供了理论保证。

Abstract: We present the first theoretical guarantees for zero constraint violation in
Online Convex Optimization (OCO) across all rounds, addressing dynamic
constraint changes. Unlike existing approaches in constrained OCO, which allow
for occasional safety breaches, we provide the first approach for maintaining
strict safety under the assumption of gradually evolving constraints, namely
the constraints change at most by a small amount between consecutive rounds.
This is achieved through a primal-dual approach and Online Gradient Ascent in
the dual space. We show that employing a dichotomous learning rate enables
ensuring both safety, via zero constraint violation, and sublinear regret. Our
framework marks a departure from previous work by providing the first provable
guarantees for maintaining absolute safety in the face of changing constraints
in OCO.

</details>

### [49] [DeepSTA: A Spatial-Temporal Attention Network for Logistics Delivery Timely Rate Prediction in Anomaly Conditions](https://arxiv.org/abs/2505.00402)
*Jinhui Yi,Huan Yan,Haotian Wang,Jian Yuan,Yong Li*

Main category: cs.LG

TLDR: 提出了一种名为DeepSTA的深度时空注意力模型，用于预测快递员的准时送达率，特别关注异常情况下的表现。


<details>
  <summary>Details</summary>
Motivation: 物流行业中预测快递员准时送达率至关重要，尤其是在异常情况下（如疫情爆发），现有研究对此关注不足且未能有效建模异常事件。

Method: 设计了异常时空学习模块和异常模式注意力模块，结合Node2vec、图神经网络和LSTM，以捕捉时空依赖性和异常特征。

Result: 在2022年COVID-19疫情期间的真实物流数据集上，模型在MAE和MSE上分别优于基线12.11%和13.71%。

Conclusion: DeepSTA在异常情况下表现出色，解决了数据不足和信息丢失问题，为物流行业提供了有效的预测工具。

Abstract: Prediction of couriers' delivery timely rates in advance is essential to the
logistics industry, enabling companies to take preemptive measures to ensure
the normal operation of delivery services. This becomes even more critical
during anomaly conditions like the epidemic outbreak, during which couriers'
delivery timely rate will decline markedly and fluctuates significantly.
Existing studies pay less attention to the logistics scenario. Moreover, many
works focusing on prediction tasks in anomaly scenarios fail to explicitly
model abnormal events, e.g., treating external factors equally with other
features, resulting in great information loss. Further, since some anomalous
events occur infrequently, traditional data-driven methods perform poorly in
these scenarios. To deal with them, we propose a deep spatial-temporal
attention model, named DeepSTA. To be specific, to avoid information loss, we
design an anomaly spatio-temporal learning module that employs a recurrent
neural network to model incident information. Additionally, we utilize Node2vec
to model correlations between road districts, and adopt graph neural networks
and long short-term memory to capture the spatial-temporal dependencies of
couriers. To tackle the issue of insufficient training data in abnormal
circumstances, we propose an anomaly pattern attention module that adopts a
memory network for couriers' anomaly feature patterns storage via attention
mechanisms. The experiments on real-world logistics datasets during the
COVID-19 outbreak in 2022 show the model outperforms the best baselines by
12.11% in MAE and 13.71% in MSE, demonstrating its superior performance over
multiple competitive baselines.

</details>

### [50] [Machine Learning Meets Transparency in Osteoporosis Risk Assessment: A Comparative Study of ML and Explainability Analysis](https://arxiv.org/abs/2505.00410)
*Farhana Elias,Md Shihab Reza,Muhammad Zawad Mahmud,Samiha Islam*

Main category: cs.LG

TLDR: 研究通过机器学习方法预测骨质疏松风险，强调可解释AI（XAI）提升模型透明度。XGBoost表现最佳（准确率91%），SHAP等XAI方法揭示年龄是主要风险因素。


<details>
  <summary>Details</summary>
Motivation: 骨质疏松因无症状常被忽视，早期预测对预防骨折至关重要。

Method: 评估六种机器学习分类器，使用临床、人口和生活方式数据，通过GridSearchCV优化超参数。

Result: XGBoost准确率最高（91%），XAI方法确认年龄、激素变化和家族史为主要风险因素。

Conclusion: 研究强调可解释性对医疗ML模型的重要性，建议未来验证更多人群并整合生物标志物。

Abstract: The present research tackles the difficulty of predicting osteoporosis risk
via machine learning (ML) approaches, emphasizing the use of explainable
artificial intelligence (XAI) to improve model transparency. Osteoporosis is a
significant public health concern, sometimes remaining untreated owing to its
asymptomatic characteristics, and early identification is essential to avert
fractures. The research assesses six machine learning classifiers: Random
Forest, Logistic Regression, XGBoost, AdaBoost, LightGBM, and Gradient Boosting
and utilizes a dataset based on clinical, demographic, and lifestyle variables.
The models are refined using GridSearchCV to calibrate hyperparameters, with
the objective of enhancing predictive efficacy. XGBoost had the greatest
accuracy (91%) among the evaluated models, surpassing others in precision
(0.92), recall (0.91), and F1-score (0.90). The research further integrates XAI
approaches, such as SHAP, LIME, and Permutation Feature Importance, to
elucidate the decision-making process of the optimal model. The study indicates
that age is the primary determinant in forecasting osteoporosis risk, followed
by hormonal alterations and familial history. These results corroborate
clinical knowledge and affirm the models' therapeutic significance. The
research underscores the significance of explainability in machine learning
models for healthcare applications, guaranteeing that physicians can rely on
the system's predictions. The report ultimately proposes directions for further
research, such as validation across varied populations and the integration of
supplementary biomarkers for enhanced predictive accuracy.

</details>

### [51] [CICADA: Cross-Domain Interpretable Coding for Anomaly Detection and Adaptation in Multivariate Time Series](https://arxiv.org/abs/2505.00415)
*Tian Lan,Yifei Gao,Yimeng Lu,Chen Zhang*

Main category: cs.LG

TLDR: CICADA提出了一种跨领域时间序列异常检测方法，通过混合专家框架、选择性元学习、自适应扩展算法和分层注意力结构，解决了现有方法在跨领域泛化和可解释性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法因数据分布变化和非平稳性难以泛化到多源领域和新目标领域，CICADA旨在填补这一研究空白。

Method: CICADA采用混合专家框架捕获领域无关异常特征，结合选择性元学习防止负迁移，自适应扩展算法处理新领域，分层注意力结构增强可解释性。

Result: 在合成和真实工业数据集上，CICADA在跨领域检测性能和可解释性上均优于现有方法。

Conclusion: CICADA通过创新设计显著提升了跨领域时间序列异常检测的性能和可解释性。

Abstract: Unsupervised Time series anomaly detection plays a crucial role in
applications across industries. However, existing methods face significant
challenges due to data distributional shifts across different domains, which
are exacerbated by the non-stationarity of time series over time. Existing
models fail to generalize under multiple heterogeneous source domains and
emerging unseen new target domains. To fill the research gap, we introduce
CICADA (Cross-domain Interpretable Coding for Anomaly Detection and
Adaptation), with four key innovations: (1) a mixture of experts (MOE)
framework that captures domain-agnostic anomaly features with high flexibility
and interpretability; (2) a novel selective meta-learning mechanism to prevent
negative transfer between dissimilar domains, (3) an adaptive expansion
algorithm for emerging heterogeneous domain expansion, and (4) a hierarchical
attention structure that quantifies expert contributions during fusion to
enhance interpretability further.Extensive experiments on synthetic and
real-world industrial datasets demonstrate that CICADA outperforms
state-of-the-art methods in both cross-domain detection performance and
interpretability.

</details>

### [52] [Toward Automated Regulatory Decision-Making: Trustworthy Medical Device Risk Classification with Multimodal Transformers and Self-Training](https://arxiv.org/abs/2505.00422)
*Yu Han,Aaron Ceross,Jeroen H. M. Bergmann*

Main category: cs.LG

TLDR: 提出了一种基于Transformer的多模态框架，结合文本和视觉信息预测医疗设备风险分类，通过交叉注意力和自训练策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 医疗设备风险分类对监管和临床安全至关重要，但现有方法可能因单模态或有限监督而表现不佳。

Method: 采用Transformer框架，结合文本和视觉信息，利用交叉注意力捕捉模态间依赖，并通过自训练策略优化泛化能力。

Result: 在真实监管数据集上，模型达到90.4%准确率和97.9% AUROC，显著优于单模态基线，自训练机制进一步提升了性能。

Conclusion: 交叉注意力和自训练策略在多模态分类中具有互补优势，能有效提升有限监督下的泛化能力。

Abstract: Accurate classification of medical device risk levels is essential for
regulatory oversight and clinical safety. We present a Transformer-based
multimodal framework that integrates textual descriptions and visual
information to predict device regulatory classification. The model incorporates
a cross-attention mechanism to capture intermodal dependencies and employs a
self-training strategy for improved generalization under limited supervision.
Experiments on a real-world regulatory dataset demonstrate that our approach
achieves up to 90.4% accuracy and 97.9% AUROC, significantly outperforming
text-only (77.2%) and image-only (54.8%) baselines. Compared to standard
multimodal fusion, the self-training mechanism improved SVM performance by 3.3
percentage points in accuracy (from 87.1% to 90.4%) and 1.4 points in macro-F1,
suggesting that pseudo-labeling can effectively enhance generalization under
limited supervision. Ablation studies further confirm the complementary
benefits of both cross-modal attention and self-training.

</details>

### [53] [Per-Domain Generalizing Policies: On Validation Instances and Scaling Behavior](https://arxiv.org/abs/2505.00439)
*Timo P. Gros,Nicola J. Müller,Daniel Fiser,Isabel Valera,Verena Wolf,Jörg Hoffmann*

Main category: cs.LG

TLDR: 论文提出了一种动态生成验证集的方法，通过逐步增加实例规模来提升泛化能力，并改进了评估方法。实验显示动态验证在9个领域中均提升了GNN策略的扩展性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用固定验证集，限制了泛化能力的提升。动态生成验证集可以更有效地评估和优化模型的扩展行为。

Method: 动态生成验证集，逐步增加实例规模；改进评估方法，系统生成测试实例以确保覆盖性能的置信度。

Result: 在9个领域中，动态验证均提升了GNN策略的扩展性能。

Conclusion: 动态验证集生成方法有效提升了模型的泛化能力和扩展性能。

Abstract: Recent work has shown that successful per-domain generalizing action policies
can be learned. Scaling behavior, from small training instances to large test
instances, is the key objective; and the use of validation instances larger
than training instances is one key to achieve it. Prior work has used fixed
validation sets. Here, we introduce a method generating the validation set
dynamically, on the fly, increasing instance size so long as informative and
feasible.We also introduce refined methodology for evaluating scaling behavior,
generating test instances systematically to guarantee a given confidence in
coverage performance for each instance size. In experiments, dynamic validation
improves scaling behavior of GNN policies in all 9 domains used.

</details>

### [54] [A Generalised Framework for Property-Driven Machine Learning](https://arxiv.org/abs/2505.00466)
*Thomas Flinkow,Marco Casadio,Colin Kessler,Rosemary Monahan,Ekaterina Komendantskaya*

Main category: cs.LG

TLDR: 论文提出了一种结合对抗训练和可微分逻辑的统一框架，用于满足机器学习模型的安全性和正确性属性。


<details>
  <summary>Details</summary>
Motivation: 神经网络训练后常无法满足关键的安全和正确性属性，需要直接将这些属性融入训练方法。

Method: 结合对抗训练和可微分逻辑，通过广义超矩形灵活指定输入区域，并将逻辑约束编码为损失项。

Result: 展示了该框架在无人机系统神经网络控制器案例中的实际有效性。

Conclusion: 提出的统一框架能够满足多种属性需求，且实际应用效果显著。

Abstract: Neural networks have been shown to frequently fail to satisfy critical safety
and correctness properties after training, highlighting the pressing need for
training methods that incorporate such properties directly. While adversarial
training can be used to improve robustness to small perturbations within
$\epsilon$-cubes, domains other than computer vision -- such as control systems
and natural language processing -- may require more flexible input region
specifications via generalised hyper-rectangles. Meanwhile, differentiable
logics offer a way to encode arbitrary logical constraints as additional loss
terms that guide the learning process towards satisfying these constraints. In
this paper, we investigate how these two complementary approaches can be
unified within a single framework for property-driven machine learning. We show
that well-known properties from the literature are subcases of this general
approach, and we demonstrate its practical effectiveness on a case study
involving a neural network controller for a drone system. Our framework is
publicly available at https://github.com/tflinkow/property-driven-ml.

</details>

### [55] [Interpretable Spatial-Temporal Fusion Transformers: Multi-Output Prediction for Parametric Dynamical Systems with Time-Varying Inputs](https://arxiv.org/abs/2505.00473)
*Shuwen Sun,Lihong Feng,Peter Benner*

Main category: cs.LG

TLDR: 论文探讨了使用Transformer模型预测带有时变输入信号的参数动态系统输出的性能，提出了一种多输出Transformer模型，能够准确预测非线性系统的多输出序列。


<details>
  <summary>Details</summary>
Motivation: 动态系统的输出不仅受物理参数影响，还受外部时变输入信号影响，准确捕捉其动态特性具有挑战性。

Method: 扩展了现有的单输出Transformer模型，提出多输出Transformer模型，通过可解释的注意力权重矩阵分析时间相关性和多输出间的交互作用。

Result: 多输出Transformer能够准确预测非线性系统的多输出序列，且适用于高维参数空间。

Conclusion: 多输出Transformer在预测动态系统输出方面表现出色，同时提供了对输出域空间相关性的解释。

Abstract: We explore the promising performance of a transformer model in predicting
outputs of parametric dynamical systems with external time-varying input
signals. The outputs of such systems vary not only with physical parameters but
also with external time-varying input signals. Accurately catching the dynamics
of such systems is challenging. We have adapted and extended an existing
transformer model for single output prediction to a multiple-output transformer
that is able to predict multiple output responses of these systems. The
multiple-output transformer generalizes the interpretability of the original
transformer. The generalized interpretable attention weight matrix explores not
only the temporal correlations in the sequence, but also the interactions
between the multiple outputs, providing explanation for the spatial correlation
in the output domain. This multiple-output transformer accurately predicts the
sequence of multiple outputs, regardless of the nonlinearity of the system and
the dimensionality of the parameter space.

</details>

### [56] [Enhancing Tropical Cyclone Path Forecasting with an Improved Transformer Network](https://arxiv.org/abs/2505.00495)
*Nguyen Van Thanh,Nguyen Dang Huynh,Nguyen Ngoc Tan,Nguyen Thai Minh,Nguyen Nam Hoang*

Main category: cs.LG

TLDR: 提出了一种基于Transformer网络的改进深度学习方法，用于预测未来6小时的风暴轨迹，比传统方法更准确、快速且成本更低。


<details>
  <summary>Details</summary>
Motivation: 风暴路径预测对保护人类生命和财产至关重要，但因其轨迹多变而极具挑战性。

Method: 使用Transformer网络改进的深度学习方法，训练数据来自NOAA。

Result: 模拟结果显示，该方法比传统方法更准确、快速且成本更低。

Conclusion: 该方法为风暴路径预测提供了一种高效且经济的解决方案。

Abstract: A storm is a type of extreme weather. Therefore, forecasting the path of a
storm is extremely important for protecting human life and property. However,
storm forecasting is very challenging because storm trajectories frequently
change. In this study, we propose an improved deep learning method using a
Transformer network to predict the movement trajectory of a storm over the next
6 hours. The storm data used to train the model was obtained from the National
Oceanic and Atmospheric Administration (NOAA) [1]. Simulation results show that
the proposed method is more accurate than traditional methods. Moreover, the
proposed method is faster and more cost-effective

</details>

### [57] [Variational OOD State Correction for Offline Reinforcement Learning](https://arxiv.org/abs/2505.00503)
*Ke Jiang,Wen Jiang,Xiaoyang Tan*

Main category: cs.LG

TLDR: 提出了一种名为DASP的新方法，通过鼓励智能体优先选择高数据密度的动作来解决离线强化学习中的状态分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习的性能受状态分布偏移问题影响显著，OOD状态校正是解决该问题的流行方法。

Method: DASP方法在变分框架内优化目标，同时考虑决策的潜在结果及其密度，以提供安全决策的关键上下文信息。

Result: 在离线MuJoCo和AntMaze套件上的实验验证了该方法的有效性和可行性。

Conclusion: DASP方法通过关注高数据密度区域，有效解决了OOD状态校正问题。

Abstract: The performance of Offline reinforcement learning is significantly impacted
by the issue of state distributional shift, and out-of-distribution (OOD) state
correction is a popular approach to address this problem. In this paper, we
propose a novel method named Density-Aware Safety Perception (DASP) for OOD
state correction. Specifically, our method encourages the agent to prioritize
actions that lead to outcomes with higher data density, thereby promoting its
operation within or the return to in-distribution (safe) regions. To achieve
this, we optimize the objective within a variational framework that
concurrently considers both the potential outcomes of decision-making and their
density, thus providing crucial contextual information for safe
decision-making. Finally, we validate the effectiveness and feasibility of our
proposed method through extensive experimental evaluations on the offline
MuJoCo and AntMaze suites.

</details>

### [58] [Recursive KL Divergence Optimization: A Dynamic Framework for Representation Learning](https://arxiv.org/abs/2504.21707)
*Anthony D Martin*

Main category: cs.LG

TLDR: 论文提出了一种递归KL散度优化（RKDO）方法，将表示学习目标重新定义为局部条件分布的递归对齐过程，相比静态方法，RKDO在损失值和计算资源上均表现出显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如I-Con）通过固定邻域条件分布的KL散度统一学习范式，但忽视了学习过程中的递归结构，因此提出RKDO以动态捕捉这一结构。

Method: 引入RKDO框架，将表示学习建模为KL散度在数据邻域上的动态演化，涵盖对比学习、聚类和降维方法。

Result: 实验表明，RKDO在三个数据集上损失值降低约30%，计算资源减少60-80%，达到可比结果。

Conclusion: RKDO的递归更新机制为表示学习提供了更高效的优化路径，尤其适用于资源受限场景。

Abstract: We propose a generalization of modern representation learning objectives by
reframing them as recursive divergence alignment processes over localized
conditional distributions While recent frameworks like Information Contrastive
Learning I-Con unify multiple learning paradigms through KL divergence between
fixed neighborhood conditionals we argue this view underplays a crucial
recursive structure inherent in the learning process. We introduce Recursive KL
Divergence Optimization RKDO a dynamic formalism where representation learning
is framed as the evolution of KL divergences across data neighborhoods. This
formulation captures contrastive clustering and dimensionality reduction
methods as static slices while offering a new path to model stability and local
adaptation. Our experiments demonstrate that RKDO offers dual efficiency
advantages approximately 30 percent lower loss values compared to static
approaches across three different datasets and 60 to 80 percent reduction in
computational resources needed to achieve comparable results. This suggests
that RKDOs recursive updating mechanism provides a fundamentally more efficient
optimization landscape for representation learning with significant
implications for resource constrained applications.

</details>

### [59] [Self-Ablating Transformers: More Interpretability, Less Sparsity](https://arxiv.org/abs/2505.00509)
*Jeremias Ferrao,Luhan Mikaelson,Keenan Pepper,Natalia Perez-Campanero Antolin*

Main category: cs.LG

TLDR: 论文提出了一种自消融机制，研究稀疏性与可解释性之间的联系，发现局部专业化提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 探索稀疏性与可解释性之间的潜在联系，并设计一种机制直接在模型训练中促进可解释性。

Method: 引入动态的k-winner-takes-all约束，强制模型在神经元和注意力单元中选择性激活，结合TinyStories数据集训练小模型。

Result: 自消融导致更局部化的电路、集中的特征表示和神经元专业化，且不损害语言建模性能；全局稀疏性降低但局部专业化增强。

Conclusion: 稀疏性与可解释性之间存在复杂关系，局部专业化可提升可解释性，代码已开源。

Abstract: A growing intuition in machine learning suggests a link between sparsity and
interpretability. We introduce a novel self-ablation mechanism to investigate
this connection ante-hoc in the context of language transformers. Our approach
dynamically enforces a k-winner-takes-all constraint, forcing the model to
demonstrate selective activation across neuron and attention units. Unlike
post-hoc methods that analyze already-trained models, our approach integrates
interpretability directly into model training, promoting feature localization
from inception. Training small models on the TinyStories dataset and employing
interpretability tests, we find that self-ablation leads to more localized
circuits, concentrated feature representations, and increased neuron
specialization without compromising language modelling performance.
Surprisingly, our method also decreased overall sparsity, indicating that
self-ablation promotes specialization rather than widespread inactivity. This
reveals a complex interplay between sparsity and interpretability, where
decreased global sparsity can coexist with increased local specialization,
leading to enhanced interpretability. To facilitate reproducibility, we make
our code available at
https://github.com/keenanpepper/self-ablating-transformers.

</details>

### [60] [Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in Reinforcement Learning Frameworks](https://arxiv.org/abs/2505.00530)
*Xinyu Wang,Jinbo Bi,Minghu Song*

Main category: cs.LG

TLDR: PSV-PPO是一种新型强化学习算法，通过实时部分SMILES验证防止灾难性遗忘，同时鼓励探索，显著减少无效分子结构生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于SMILES的分子生成方法在强化学习阶段面临灾难性遗忘问题，导致预训练知识（如分子有效性）显著下降。

Method: 提出PSV-PPO算法，通过逐步验证部分SMILES序列，实时检测无效路径，避免灾难性遗忘。

Result: 在PMO和GuacaMol基准数据集上，PSV-PPO显著减少无效结构生成，同时保持探索和优化性能。

Conclusion: PSV-PPO框架可扩展，未来可融入更多领域知识，进一步推动药物发现中的强化学习应用。

Abstract: SMILES-based molecule generation has emerged as a powerful approach in drug
discovery. Deep reinforcement learning (RL) using large language model (LLM)
has been incorporated into the molecule generation process to achieve high
matching score in term of likelihood of desired molecule candidates. However, a
critical challenge in this approach is catastrophic forgetting during the RL
phase, where knowledge such as molecule validity, which often exceeds 99\%
during pretraining, significantly deteriorates. Current RL algorithms applied
in drug discovery, such as REINVENT, use prior models as anchors to retian
pretraining knowledge, but these methods lack robust exploration mechanisms. To
address these issues, we propose Partial SMILES Validation-PPO (PSV-PPO), a
novel RL algorithm that incorporates real-time partial SMILES validation to
prevent catastrophic forgetting while encouraging exploration. Unlike
traditional RL approaches that validate molecule structures only after
generating entire sequences, PSV-PPO performs stepwise validation at each
auto-regressive step, evaluating not only the selected token candidate but also
all potential branches stemming from the prior partial sequence. This enables
early detection of invalid partial SMILES across all potential paths. As a
result, PSV-PPO maintains high validity rates even during aggressive
exploration of the vast chemical space. Our experiments on the PMO and GuacaMol
benchmark datasets demonstrate that PSV-PPO significantly reduces the number of
invalid generated structures while maintaining competitive exploration and
optimization performance. While our work primarily focuses on maintaining
validity, the framework of PSV-PPO can be extended in future research to
incorporate additional forms of valuable domain knowledge, further enhancing
reinforcement learning applications in drug discovery.

</details>

### [61] [Test-time Correlation Alignment](https://arxiv.org/abs/2505.00533)
*Linjing You,Jiabao Lu,Xiayuan Huang*

Main category: cs.LG

TLDR: 论文提出了一种名为Test-time Correlation Alignment (TCA)的方法，通过理论分析和实验验证，解决了现有测试时适应(TTA)方法的局限性，包括忽略相关性对齐、计算复杂和领域遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在训练和测试数据分布不一致时性能下降，而隐私问题限制了训练数据的访问，因此需要仅使用未标记测试数据进行模型适应的TTA方法。现有TTA方法存在实例对齐为主、计算复杂和领域遗忘等问题。

Method: 提出了两种算法：LinearTCA和LinearTCA+。LinearTCA通过简单的线性变换实现实例和相关性的对齐，无需额外模型更新；LinearTCA+可作为插件提升现有TTA方法。

Result: 实验表明，TCA方法在多个任务、基准和骨干网络上显著优于基线方法。LinearTCA在OfficeHome数据集上提升5.88%的适应准确率，且仅占用4%的GPU内存和0.6%的计算时间。

Conclusion: TCA方法通过理论保证和高效实现，解决了TTA的挑战，为实际应用提供了简单有效的解决方案。

Abstract: Deep neural networks often experience performance drops due to distribution
shifts between training and test data. Although domain adaptation offers a
solution, privacy concerns restrict access to training data in many real-world
scenarios. This restriction has spurred interest in Test-Time Adaptation (TTA),
which adapts models using only unlabeled test data. However, current TTA
methods still face practical challenges: (1) a primary focus on instance-wise
alignment, overlooking CORrelation ALignment (CORAL) due to missing source
correlations; (2) complex backpropagation operations for model updating,
resulting in overhead computation and (3) domain forgetting.
  To address these challenges, we provide a theoretical analysis to investigate
the feasibility of Test-time Correlation Alignment (TCA), demonstrating that
correlation alignment between high-certainty instances and test instances can
enhance test performances with a theoretical guarantee. Based on this, we
propose two simple yet effective algorithms: LinearTCA and LinearTCA+.
LinearTCA applies a simple linear transformation to achieve both instance and
correlation alignment without additional model updates, while LinearTCA+ serves
as a plug-and-play module that can easily boost existing TTA methods. Extensive
experiments validate our theoretical insights and show that TCA methods
significantly outperforms baselines across various tasks, benchmarks and
backbones. Notably, LinearTCA improves adaptation accuracy by 5.88% on
OfficeHome dataset, while using only 4% maximum GPU memory usage and 0.6%
computation time compared to the best baseline TTA method.

</details>

### [62] [KnowEEG: Explainable Knowledge Driven EEG Classification](https://arxiv.org/abs/2505.00541)
*Amarpal Sahota,Navid Mohammadi Foumani,Raul Santos-Rodriguez,Zahraa S. Abdallah*

Main category: cs.LG

TLDR: KnowEEG是一种新型可解释的机器学习方法，用于EEG分类，结合了电极特征和连接性统计，性能优于现有深度学习模型，并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决EEG分类中深度学习模型可解释性不足的问题。

Method: 提取电极特征，通过统计测试筛选，整合电极间连接性统计，使用改进的随机森林模型（Fusion Forest）进行分类。

Result: 在五种分类任务中性能优于或媲美现有深度学习模型，并提供可解释的特征重要性评分。

Conclusion: KnowEEG在性能和可解释性上均表现出色，适用于医疗等需要EEG可解释性的领域。

Abstract: Electroencephalography (EEG) is a method of recording brain activity that
shows significant promise in applications ranging from disease classification
to emotion detection and brain-computer interfaces. Recent advances in deep
learning have improved EEG classification performance yet model explainability
remains an issue. To address this key limitation of explainability we introduce
KnowEEG; a novel explainable machine learning approach for EEG classification.
KnowEEG extracts a comprehensive set of per-electrode features, filters them
using statistical tests, and integrates between-electrode connectivity
statistics. These features are then input to our modified Random Forest model
(Fusion Forest) that balances per electrode statistics with between electrode
connectivity features in growing the trees of the forest. By incorporating
knowledge from both the generalized time-series and EEG-specific domains,
KnowEEG achieves performance comparable to or exceeding state-of-the-art deep
learning models across five different classification tasks: emotion detection,
mental workload classification, eyes open/closed detection, abnormal EEG
classification, and event detection. In addition to high performance, KnowEEG
provides inherent explainability through feature importance scores for
understandable features. We demonstrate by example on the eyes closed/open
classification task that this explainability can be used to discover knowledge
about the classes. This discovered knowledge for eyes open/closed
classification was proven to be correct by current neuroscience literature.
Therefore, the impact of KnowEEG will be significant for domains where EEG
explainability is critical such as healthcare.

</details>

### [63] [Directly Forecasting Belief for Reinforcement Learning with Delays](https://arxiv.org/abs/2505.00546)
*Qingyuan Wu,Yuhui Wang,Simon Sinong Zhan,Yixuan Wang,Chung-Wei Lin,Chen Lv,Qi Zhu,Jürgen Schmidhuber,Chao Huang*

Main category: cs.LG

TLDR: DFBT是一种新的信念估计方法，直接预测状态而非逐步递归预测，显著减少了误差累积，提升了学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中因延迟导致的状态估计误差累积问题。

Method: 提出DFBT方法，直接从观测中预测状态，避免逐步递归预测。

Result: 在D4RL离线数据集和MuJoCo基准测试中，DFBT显著优于现有方法。

Conclusion: DFBT通过直接预测状态减少了误差累积，提升了强化学习的性能和效率。

Abstract: Reinforcement learning (RL) with delays is challenging as sensory perceptions
lag behind the actual events: the RL agent needs to estimate the real state of
its environment based on past observations. State-of-the-art (SOTA) methods
typically employ recursive, step-by-step forecasting of states. This can cause
the accumulation of compounding errors. To tackle this problem, our novel
belief estimation method, named Directly Forecasting Belief Transformer (DFBT),
directly forecasts states from observations without incrementally estimating
intermediate states step-by-step. We theoretically demonstrate that DFBT
greatly reduces compounding errors of existing recursively forecasting methods,
yielding stronger performance guarantees. In experiments with D4RL offline
datasets, DFBT reduces compounding errors with remarkable prediction accuracy.
DFBT's capability to forecast state sequences also facilitates multi-step
bootstrapping, thus greatly improving learning efficiency. On the MuJoCo
benchmark, our DFBT-based method substantially outperforms SOTA baselines.

</details>

### [64] [Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors](https://arxiv.org/abs/2505.00580)
*Xinyu Ding,Lexuan Chen,Siyu Liao,Zhongfeng Wang*

Main category: cs.LG

TLDR: 提出了一种通过分解交错循环和对角矩阵来降低傅里叶域训练复杂度的新方法，减少了计算量和可训练参数。


<details>
  <summary>Details</summary>
Motivation: 基础模型计算和存储复杂度高，难以微调且实用性受限。

Method: 通过交错循环和对角矩阵的乘积分解，避免构建权重变化矩阵，使用1D FFT替代2D FFT。

Result: 在多种任务中表现相似或更好，显著减少FLOPs和可训练参数。

Conclusion: 该方法有效降低了复杂度，提升了实用性。

Abstract: Foundation models have achieved tremendous success in different domains.
However, their huge computation and storage complexity make these models
difficult to fine-tune and also less applicable in practice. Recent study shows
training in Fourier domain can be an effective fine-tuning method in terms of
both model performance and number of training parameters. In this work, we
propose to further reduce the complexity by the factorization through the
product of interleaved circulant and diagonal matrices. In addition, we address
the case of non-square fine-tuning weights by partitioning the circulant matrix
into blocks. Our method avoids the construction of weight change matrix and
utilizes 1D fast Fourier transform (FFT) instead of 2D FFT. Experimental
results show that our method achieves similar or better performance across
various tasks with much less floating-point operations (FLOPs) and the number
of trainable parameters.

</details>

### [65] [Unlocking the Potential of Linear Networks for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.00590)
*Chengsen Wang,Qi Qi,Jingyu Wang,Haifeng Sun,Zirui Zhuang,Jianxin Liao*

Main category: cs.LG

TLDR: 本文提出了一种名为AiT的新模型，用于处理不规则多元时间序列的预测问题，通过动态调整权重和结合Transformer模块，显著提升了预测精度和运行效率。


<details>
  <summary>Details</summary>
Motivation: 传统线性网络在处理不规则多元时间序列时，由于静态权重无法应对数据的不一致性和异步性，导致建模和预测效果不佳。

Method: AiT采用自适应线性网络动态调整权重以解决序列内不一致性，并结合Transformer模块捕捉变量相关性以应对序列间异步性。

Result: 在四个基准数据集上的实验表明，AiT比现有最优方法预测精度提升11%，运行时间减少52%。

Conclusion: AiT通过动态权重和Transformer模块，有效解决了不规则多元时间序列的建模和预测问题，具有显著优势。

Abstract: Time series forecasting holds significant importance across various
industries, including finance, transportation, energy, healthcare, and climate.
Despite the widespread use of linear networks due to their low computational
cost and effectiveness in modeling temporal dependencies, most existing
research has concentrated on regularly sampled and fully observed multivariate
time series. However, in practice, we frequently encounter irregular
multivariate time series characterized by variable sampling intervals and
missing values. The inherent intra-series inconsistency and inter-series
asynchrony in such data hinder effective modeling and forecasting with
traditional linear networks relying on static weights. To tackle these
challenges, this paper introduces a novel model named AiT. AiT utilizes an
adaptive linear network capable of dynamically adjusting weights according to
observation time points to address intra-series inconsistency, thereby
enhancing the accuracy of temporal dependencies modeling. Furthermore, by
incorporating the Transformer module on variable semantics embeddings, AiT
efficiently captures variable correlations, avoiding the challenge of
inter-series asynchrony. Comprehensive experiments across four benchmark
datasets demonstrate the superiority of AiT, improving prediction accuracy by
11% and decreasing runtime by 52% compared to existing state-of-the-art
methods.

</details>

### [66] [Explainable AI in Spatial Analysis](https://arxiv.org/abs/2505.00591)
*Ziqi Li*

Main category: cs.LG

TLDR: 本章探讨了可解释人工智能（XAI）在空间分析中的应用，强调XAI如何提升机器学习模型的透明度和理解，特别是通过Shapley值方法。


<details>
  <summary>Details</summary>
Motivation: 空间分析中机器学习虽灵活但常被视为黑箱，XAI能解决这一问题，增强模型的可解释性和可靠性。

Method: 介绍了XAI的关键概念和方法，重点是基于Shapley值的方法，并将其与空间分析结合，通过2020年美国总统选举的县级别投票行为案例进行实证分析。

Result: 展示了Shapley值与空间分析的结合应用，并与多尺度地理加权回归进行了比较。

Conclusion: 讨论了当前XAI技术的挑战和局限性，并提出了未来研究方向。

Abstract: This chapter discusses the opportunities of eXplainable Artificial
Intelligence (XAI) within the realm of spatial analysis. A key objective in
spatial analysis is to model spatial relationships and infer spatial processes
to generate knowledge from spatial data, which has been largely based on
spatial statistical methods. More recently, machine learning offers scalable
and flexible approaches that complement traditional methods and has been
increasingly applied in spatial data science. Despite its advantages, machine
learning is often criticized for being a black box, which limits our
understanding of model behavior and output. Recognizing this limitation, XAI
has emerged as a pivotal field in AI that provides methods to explain the
output of machine learning models to enhance transparency and understanding.
These methods are crucial for model diagnosis, bias detection, and ensuring the
reliability of results obtained from machine learning models. This chapter
introduces key concepts and methods in XAI with a focus on Shapley value-based
approaches, which is arguably the most popular XAI method, and their
integration with spatial analysis. An empirical example of county-level voting
behaviors in the 2020 Presidential election is presented to demonstrate the use
of Shapley values and spatial analysis with a comparison to multi-scale
geographically weighted regression. The chapter concludes with a discussion on
the challenges and limitations of current XAI techniques and proposes new
directions.

</details>

### [67] [Fast and Low-Cost Genomic Foundation Models via Outlier Removal](https://arxiv.org/abs/2505.00598)
*Haozheng Luo,Chenghao Qiu,Maojiang Su,Zhihan Zhou,Zoe Mehta,Guo Ye,Jerry Yao-Chieh Hu,Han Liu*

Main category: cs.LG

TLDR: GERM是首个针对基因组基础模型（GFMs）的统一对抗攻击基准，提供了系统性评估GFMs对抗攻击脆弱性的框架。


<details>
  <summary>Details</summary>
Motivation: 现有GFM基准缺乏对抗攻击评估，GERM填补了这一空白。

Method: 评估五种先进GFMs，使用四种攻击算法和三种防御策略，分析模型架构、量化方案和训练数据的影响。

Result: 基于Transformer的模型比HyenaDNA更鲁棒，对抗攻击常针对生物学重要区域。

Conclusion: GERM为GFMs对抗攻击研究提供了全面框架，揭示了模型架构和生物学特征的重要性。

Abstract: We propose the first unified adversarial attack benchmark for Genomic
Foundation Models (GFMs), named GERM. Unlike existing GFM benchmarks, GERM
offers the first comprehensive evaluation framework to systematically assess
the vulnerability of GFMs to adversarial attacks. Methodologically, we evaluate
the adversarial robustness of five state-of-the-art GFMs using four widely
adopted attack algorithms and three defense strategies. Importantly, our
benchmark provides an accessible and comprehensive framework to analyze GFM
vulnerabilities with respect to model architecture, quantization schemes, and
training datasets. Empirically, transformer-based models exhibit greater
robustness to adversarial perturbations compared to HyenaDNA, highlighting the
impact of architectural design on vulnerability. Moreover, adversarial attacks
frequently target biologically significant genomic regions, suggesting that
these models effectively capture meaningful sequence features.

</details>

### [68] [OmicsCL: Unsupervised Contrastive Learning for Cancer Subtype Discovery and Survival Stratification](https://arxiv.org/abs/2505.00650)
*Atahan Karagoz*

Main category: cs.LG

TLDR: OmicsCL是一种基于对比学习的框架，用于从多组学数据中无监督学习疾病亚型，结合生存感知损失，提升临床相关性。


<details>
  <summary>Details</summary>
Motivation: 通过无监督学习多组学数据，推动个性化医疗发展。

Method: OmicsCL框架联合嵌入多种组学数据，引入生存感知对比损失，无需标注数据。

Result: 在TCGA BRCA数据集中发现临床相关聚类，生存预测表现优异。

Conclusion: 对比学习框架在多组学数据中具有生物发现潜力。

Abstract: Unsupervised learning of disease subtypes from multi-omics data presents a
significant opportunity for advancing personalized medicine. We introduce
OmicsCL, a modular contrastive learning framework that jointly embeds
heterogeneous omics modalities-such as gene expression, DNA methylation, and
miRNA expression-into a unified latent space. Our method incorporates a
survival-aware contrastive loss that encourages the model to learn
representations aligned with survival-related patterns, without relying on
labeled outcomes. Evaluated on the TCGA BRCA dataset, OmicsCL uncovers
clinically meaningful clusters and achieves strong unsupervised concordance
with patient survival. The framework demonstrates robustness across
hyperparameter configurations and can be tuned to prioritize either subtype
coherence or survival stratification. Ablation studies confirm that integrating
survival-aware loss significantly enhances the predictive power of learned
embeddings. These results highlight the promise of contrastive objectives for
biological insight discovery in high-dimensional, heterogeneous omics data.

</details>

### [69] [Wasserstein Policy Optimization](https://arxiv.org/abs/2505.00663)
*David Pfau,Ian Davies,Diana Borsa,Joao G. M. Araujo,Brendan Tracey,Hado van Hasselt*

Main category: cs.LG

TLDR: Wasserstein Policy Optimization (WPO) 是一种用于连续动作空间的强化学习算法，结合了确定性策略梯度和经典策略梯度的优点。


<details>
  <summary>Details</summary>
Motivation: 提出一种能够在连续动作空间中高效优化策略的算法，同时避免重参数化技巧的限制。

Method: 通过近似Wasserstein梯度流，推导出简单且通用的闭式更新公式，适用于任意分布的随机策略。

Result: 在DeepMind Control Suite和磁约束聚变任务中表现优于现有连续控制方法。

Conclusion: WPO是一种高效且通用的强化学习算法，适用于连续动作空间。

Abstract: We introduce Wasserstein Policy Optimization (WPO), an actor-critic algorithm
for reinforcement learning in continuous action spaces. WPO can be derived as
an approximation to Wasserstein gradient flow over the space of all policies
projected into a finite-dimensional parameter space (e.g., the weights of a
neural network), leading to a simple and completely general closed-form update.
The resulting algorithm combines many properties of deterministic and classic
policy gradient methods. Like deterministic policy gradients, it exploits
knowledge of the gradient of the action-value function with respect to the
action. Like classic policy gradients, it can be applied to stochastic policies
with arbitrary distributions over actions -- without using the
reparameterization trick. We show results on the DeepMind Control Suite and a
magnetic confinement fusion task which compare favorably with state-of-the-art
continuous control methods.

</details>

### [70] [MINERVA: Evaluating Complex Video Reasoning](https://arxiv.org/abs/2505.00681)
*Arsha Nagrani,Sachit Menon,Ahmet Iscen,Shyamal Buch,Ramin Mehran,Nilpa Jha,Anja Hauth,Yukun Zhu,Carl Vondrick,Mikhail Sirotenko,Cordelia Schmid,Tobias Weyand*

Main category: cs.LG

TLDR: 论文提出了一个名为MINERVA的视频推理数据集，用于评估多模态模型在视频理解中的真实推理能力，而非仅依赖结果监督。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准测试仅提供结果监督，缺乏中间或可解释的推理步骤，难以评估模型是否真正结合了感知和时间信息进行推理。

Method: 创建了MINERVA数据集，包含多模态、多样化的视频问题，每个问题附带5个答案选项和详细的手工推理痕迹。

Result: 基准测试显示该数据集对前沿开源和专有模型具有挑战性，错误分析揭示了时间定位和视觉感知是主要失败模式。

Conclusion: MINERVA数据集为视频推理提供了更全面的评估工具，并揭示了模型在时间定位和视觉感知上的主要缺陷。

Abstract: Multimodal LLMs are turning their focus to video benchmarks, however most
video benchmarks only provide outcome supervision, with no intermediate or
interpretable reasoning steps. This makes it challenging to assess if models
are truly able to combine perceptual and temporal information to reason about
videos, or simply get the correct answer by chance or by exploiting linguistic
biases. To remedy this, we provide a new video reasoning dataset called MINERVA
for modern multimodal models. Each question in the dataset comes with 5 answer
choices, as well as detailed, hand-crafted reasoning traces. Our dataset is
multimodal, diverse in terms of video domain and length, and consists of
complex multi-step questions. Extensive benchmarking shows that our dataset
provides a challenge for frontier open-source and proprietary models. We
perform fine-grained error analysis to identify common failure modes across
various models, and create a taxonomy of reasoning errors. We use this to
explore both human and LLM-as-a-judge methods for scoring video reasoning
traces, and find that failure modes are primarily related to temporal
localization, followed by visual perception errors, as opposed to logical or
completeness errors. The dataset, along with questions, answer candidates and
reasoning traces will be publicly available under
https://github.com/google-deepmind/neptune?tab=readme-ov-file\#minerva.

</details>

### [71] [On the Importance of Gaussianizing Representations](https://arxiv.org/abs/2505.00685)
*Daniel Eftekhari,Vardan Papyan*

Main category: cs.LG

TLDR: 论文提出了一种新的归一化层（normality normalization），通过功率变换和高斯噪声鼓励神经网络特征表示的正态性，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 正态分布在信息论中具有核心地位，但现有方法未明确神经网络激活的分布及其实现方式。

Method: 使用功率变换和高斯噪声设计归一化层，促进特征表示的正态性。

Result: 实验表明该归一化层在泛化性能、模型适应性及鲁棒性方面表现优异。

Conclusion: normality normalization是一种有效的归一化方法，适用于多种场景。

Abstract: The normal distribution plays a central role in information theory - it is at
the same time the best-case signal and worst-case noise distribution, has the
greatest representational capacity of any distribution, and offers an
equivalence between uncorrelatedness and independence for joint distributions.
Accounting for the mean and variance of activations throughout the layers of
deep neural networks has had a significant effect on facilitating their
effective training, but seldom has a prescription for precisely what
distribution these activations should take, and how this might be achieved,
been offered. Motivated by the information-theoretic properties of the normal
distribution, we address this question and concurrently present normality
normalization: a novel normalization layer which encourages normality in the
feature representations of neural networks using the power transform and
employs additive Gaussian noise during training. Our experiments
comprehensively demonstrate the effectiveness of normality normalization, in
regards to its generalization performance on an array of widely used model and
dataset combinations, its strong performance across various common factors of
variation such as model width, depth, and training minibatch size, its
suitability for usage wherever existing normalization layers are conventionally
used, and as a means to improving model robustness to random perturbations.

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [72] [Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning](https://arxiv.org/abs/2505.00001)
*Shaun Baek,Shaun Esua-Mensah,Cyrus Tsui,Sejan Vigneswaralingam,Abdullah Alali,Michael Lu,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TLDR: Rosetta-PL是一个用于评估大语言模型在逻辑推理和泛化能力上的基准，通过将逻辑命题数据集翻译为自定义逻辑语言并微调LLM（如GPT-4o），研究发现翻译过程中保留逻辑关系能显著提升精度，且训练样本超过20,000后效果趋于稳定。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）主要在高资源自然语言上训练，限制了其在低资源环境和需要深度逻辑推理任务中的表现。

Method: 通过将Lean中的逻辑命题数据集翻译为自定义逻辑语言，构建Rosetta-PL基准，并用于微调LLM（如GPT-4o），分析数据集大小和翻译方法对模型性能的影响。

Result: 翻译过程中保留逻辑关系显著提升了精度，且训练样本超过20,000后效果趋于稳定。

Conclusion: 研究结果为优化LLM在形式推理任务中的训练和提升低资源语言应用性能提供了有价值的指导。

Abstract: Large Language Models (LLMs) are primarily trained on high-resource natural
languages, limiting their effectiveness in low-resource settings and in tasks
requiring deep logical reasoning. This research introduces Rosetta-PL, a
benchmark designed to evaluate LLMs' logical reasoning and generalization
capabilities in a controlled environment. We construct Rosetta-PL by
translating a dataset of logical propositions from Lean into a custom logical
language, which is then used to fine-tune an LLM (e.g., GPT-4o). Our
experiments analyze the impact of the size of the dataset and the translation
methodology on the performance of the model. Our results indicate that
preserving logical relationships in the translation process significantly
boosts precision, with accuracy plateauing beyond roughly 20,000 training
samples. These insights provide valuable guidelines for optimizing LLM training
in formal reasoning tasks and improving performance in various low-resource
language applications.

</details>

### [73] [Symbol grounding in computational systems: A paradox of intentions](https://arxiv.org/abs/2505.00002)
*Vincent C. Müller*

Main category: cs.CL

TLDR: 论文指出计算主义无法解释符号接地问题，因为无论计算是基于有意义还是无意义的符号，都会导致语义先天论。


<details>
  <summary>Details</summary>
Motivation: 探讨计算主义在解释符号接地问题时的局限性，揭示其隐含的语义先天论假设。

Method: 通过逻辑分析，提出计算主义的两种可能情况（有意义符号和无意义符号），并分别推导其后果。

Result: 无论计算是基于有意义还是无意义的符号，计算主义都隐含语义先天论，无法解决符号接地问题。

Conclusion: 计算主义在解释符号接地问题时存在根本性缺陷，需要其他理论补充或替代。

Abstract: The paper presents a paradoxical feature of computational systems that
suggests that computationalism cannot explain symbol grounding. If the mind is
a digital computer, as computationalism claims, then it can be computing either
over meaningful symbols or over meaningless symbols. If it is computing over
meaningful symbols its functioning presupposes the existence of meaningful
symbols in the system, i.e. it implies semantic nativism. If the mind is
computing over meaningless symbols, no intentional cognitive processes are
available prior to symbol grounding. In this case, no symbol grounding could
take place since any grounding presupposes intentional cognitive processes. So,
whether computing in the mind is over meaningless or over meaningful symbols,
computationalism implies semantic nativism.

</details>

### [74] [The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs](https://arxiv.org/abs/2505.00003)
*Zizhou Liu,Ziwei Gong,Lin Ai,Zheng Hui,Run Chen,Colin Wayne Leach,Michelle R. Greene,Julia Hirschberg*

Main category: cs.CL

TLDR: 心理学理论对大型语言模型（LLM）的开发至关重要，本文综述了心理学如何从数据、预训练、后训练及评估应用等阶段提升LLM。


<details>
  <summary>Details</summary>
Motivation: 心理学在NLP领域已有重要影响，随着LLM规模和复杂度的增长，心理学理论对实现类人认知和行为更为关键。

Method: 整合认知、发展、行为、社会、人格心理学及心理语言学的理论，分析心理学在LLM开发各阶段的应用现状与不足。

Result: 揭示了心理学理论在LLM研究中的趋势与缺口，强调了跨领域联系与潜在冲突。

Conclusion: 旨在弥合学科分歧，推动心理学在未来NLP研究中的更深入整合。

Abstract: Psychological insights have long shaped pivotal NLP breakthroughs, including
the cognitive underpinnings of attention mechanisms, formative reinforcement
learning, and Theory of Mind-inspired social modeling. As Large Language Models
(LLMs) continue to grow in scale and complexity, there is a rising consensus
that psychology is essential for capturing human-like cognition, behavior, and
interaction. This paper reviews how psychological theories can inform and
enhance stages of LLM development, including data, pre-training, post-training,
and evaluation\&application. Our survey integrates insights from cognitive,
developmental, behavioral, social, personality psychology, and
psycholinguistics. Our analysis highlights current trends and gaps in how
psychological theories are applied. By examining both cross-domain connections
and points of tension, we aim to bridge disciplinary divides and promote more
thoughtful integration of psychology into future NLP research.

</details>

### [75] [LangVAE and LangSpace: Building and Probing for Language Model VAEs](https://arxiv.org/abs/2505.00004)
*Danilo S. Carvalho,Yingji Zhang,Harriet Unsworth,André Freitas*

Main category: cs.CL

TLDR: LangVAE是一个基于预训练大语言模型（LLMs）构建变分自编码器（VAEs）的新框架，能生成更紧凑且语义解耦的表示。配套工具LangSpace提供多种分析方法。实验展示了其在泛化性和解耦性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 利用预训练语言模型的知识构建更高效的文本表示，并系统化实验和理解文本表示的方法。

Method: 基于预训练LLMs构建VAEs，配套LangSpace工具进行表示分析，包括向量遍历、解耦度量和聚类可视化。

Result: 实验表明LangVAE在不同架构和规模下具有泛化性和解耦性的潜力。

Conclusion: LangVAE为文本表示的系统化实验和理解提供了有前景的框架。

Abstract: We present LangVAE, a novel framework for modular construction of variational
autoencoders (VAEs) on top of pre-trained large language models (LLMs). Such
language model VAEs can encode the knowledge of their pre-trained components
into more compact and semantically disentangled representations. The
representations obtained in this way can be analysed with the LangVAE companion
framework: LangSpace, which implements a collection of probing methods, such as
vector traversal and interpolation, disentanglement measures, and cluster
visualisations. LangVAE and LangSpace offer a flexible, efficient and scalable
way of building and analysing textual representations, with simple integration
for models available on the HuggingFace Hub. Additionally, we conducted a set
of experiments with different encoder and decoder combinations, as well as
annotated inputs, revealing a wide range of interactions across architectural
families and sizes w.r.t. generalisation and disentanglement. Our findings
demonstrate a promising framework for systematising the experimentation and
understanding of textual representations.

</details>

### [76] [Toward a digital twin of U.S. Congress](https://arxiv.org/abs/2505.00006)
*Hayden Helm,Tianyi Chen,Harvey McGuinness,Paige Lee,Brandon Duderstadt,Carey E. Priebe*

Main category: cs.CL

TLDR: 论文提出了一种基于语言模型的美国国会议员虚拟模型，证明其符合数字孪生的定义，并展示了该模型在预测投票行为和党派倾向方面的应用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过语言模型构建国会议员的数字孪生，以预测其行为和辅助资源分配。

Method: 利用每日更新的国会议员推文数据集，训练特定于议员的语言模型，生成难以区分的推文。

Result: 生成的推文可用于预测投票行为和党派倾向，影响现实立法动态。

Conclusion: 研究展示了数字孪生的潜力，但也讨论了其局限性和未来扩展方向。

Abstract: In this paper we provide evidence that a virtual model of U.S.
congresspersons based on a collection of language models satisfies the
definition of a digital twin. In particular, we introduce and provide
high-level descriptions of a daily-updated dataset that contains every Tweet
from every U.S. congressperson during their respective terms. We demonstrate
that a modern language model equipped with congressperson-specific subsets of
this data are capable of producing Tweets that are largely indistinguishable
from actual Tweets posted by their physical counterparts. We illustrate how
generated Tweets can be used to predict roll-call vote behaviors and to
quantify the likelihood of congresspersons crossing party lines, thereby
assisting stakeholders in allocating resources and potentially impacting
real-world legislative dynamics. We conclude with a discussion of the
limitations and important extensions of our analysis.

</details>

### [77] [A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination](https://arxiv.org/abs/2505.00008)
*Zhaoyi Sun,Wen-Wai Yim,Ozlem Uzuner,Fei Xia,Meliha Yetisgen*

Main category: cs.CL

TLDR: 综述探讨了NLP在检测、纠正和减轻医学不准确信息（如错误、误信息和幻觉）中的潜力与挑战，强调其对患者安全和公共健康的意义。


<details>
  <summary>Details</summary>
Motivation: 通过统一医学不准确信息的概念，推动患者安全、改善公共健康沟通，并支持开发更可靠的医疗NLP应用。

Method: 遵循PRISMA指南的范围综述，分析了2020至2024年五个数据库中的研究，按主题、任务、文档类型等分类。

Result: NLP在检测和纠正医学不准确信息方面显示出潜力，但仍面临数据隐私、上下文依赖和评估标准等挑战。

Conclusion: 综述强调了NLP在医学领域的进展，但需解决数据、上下文和幻觉管理等问题，以确保应用的可靠性和透明度。

Abstract: Objective: This review aims to explore the potential and challenges of using
Natural Language Processing (NLP) to detect, correct, and mitigate medically
inaccurate information, including errors, misinformation, and hallucination. By
unifying these concepts, the review emphasizes their shared methodological
foundations and their distinct implications for healthcare. Our goal is to
advance patient safety, improve public health communication, and support the
development of more reliable and transparent NLP applications in healthcare.
  Methods: A scoping review was conducted following PRISMA guidelines,
analyzing studies from 2020 to 2024 across five databases. Studies were
selected based on their use of NLP to address medically inaccurate information
and were categorized by topic, tasks, document types, datasets, models, and
evaluation metrics.
  Results: NLP has shown potential in addressing medically inaccurate
information on the following tasks: (1) error detection (2) error correction
(3) misinformation detection (4) misinformation correction (5) hallucination
detection (6) hallucination mitigation. However, challenges remain with data
privacy, context dependency, and evaluation standards.
  Conclusion: This review highlights the advancements in applying NLP to tackle
medically inaccurate information while underscoring the need to address
persistent challenges. Future efforts should focus on developing real-world
datasets, refining contextual methods, and improving hallucination management
to ensure reliable and transparent healthcare applications.

</details>

### [78] [Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation](https://arxiv.org/abs/2505.00009)
*Xiao Zhang,Kangsheng Wang,Tianyu Hu,Huimin Ma*

Main category: cs.CL

TLDR: TA-LoRA是一种基于提示调优的多任务学习方法，通过低秩表示和快慢权重机制解决任务异质性，提升性能。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型在新任务上表现不佳，多任务学习虽能共享知识，但提示调优难以捕捉任务异质性。

Method: 提出TA-LoRA，结合低秩表示和快慢权重机制，避免共享与任务特定知识混合，并引入零初始化注意力机制。

Result: 在16个任务上实验表明，TA-LoRA在全数据和少样本设置下均达到最优性能。

Conclusion: TA-LoRA在多任务学习中高效且性能优越，解决了任务异质性挑战。

Abstract: Pre-trained language models (PLMs) demonstrate remarkable intelligence but
struggle with emerging tasks unseen during training in real-world applications.
Training separate models for each new task is usually impractical. Multi-task
learning (MTL) addresses this challenge by transferring shared knowledge from
source tasks to target tasks. As an dominant parameter-efficient fine-tuning
method, prompt tuning (PT) enhances MTL by introducing an adaptable vector that
captures task-specific knowledge, which acts as a prefix to the original prompt
that preserves shared knowledge, while keeping PLM parameters frozen. However,
PT struggles to effectively capture the heterogeneity of task-specific
knowledge due to its limited representational capacity. To address this
challenge, we propose Task-Adaptive Low-Rank Representation (TA-LoRA), an MTL
method built on PT, employing the low-rank representation to model task
heterogeneity and a fast-slow weights mechanism where the slow weight encodes
shared knowledge, while the fast weight captures task-specific nuances,
avoiding the mixing of shared and task-specific knowledge, caused by training
low-rank representations from scratch. Moreover, a zero-initialized attention
mechanism is introduced to minimize the disruption of immature low-rank
components on original prompts during warm-up epochs. Experiments on 16 tasks
demonstrate that TA-LoRA achieves state-of-the-art performance in full-data and
few-shot settings while maintaining superior parameter efficiency.

</details>

### [79] [Enhancing Security and Strengthening Defenses in Automated Short-Answer Grading Systems](https://arxiv.org/abs/2505.00061)
*Sahar Yarmohammadtoosky,Yiyun Zhou,Victoria Yaneva,Peter Baldwin,Saed Rezayi,Brian Clauser,Polina Harikeo*

Main category: cs.CL

TLDR: 研究探讨了基于Transformer的自动评分系统在医学教育中的漏洞，提出对抗性训练方法增强系统鲁棒性，结果显示结合集成技术可显著减少操纵风险。


<details>
  <summary>Details</summary>
Motivation: 揭示自动评分系统的漏洞及其被操纵的可能性，确保AI教育工具的可靠性和公平性。

Method: 识别三种操纵策略，采用对抗性训练和集成技术（如多数投票和岭回归）提升系统防御能力。

Result: 对抗性训练显著降低系统被操纵风险，结合集成技术和GPT-4提示技术效果更佳。

Conclusion: 需持续改进AI教育工具以应对高风险场景中的挑战，确保其公平可靠。

Abstract: This study examines vulnerabilities in transformer-based automated
short-answer grading systems used in medical education, with a focus on how
these systems can be manipulated through adversarial gaming strategies. Our
research identifies three main types of gaming strategies that exploit the
system's weaknesses, potentially leading to false positives. To counteract
these vulnerabilities, we implement several adversarial training methods
designed to enhance the systems' robustness. Our results indicate that these
methods significantly reduce the susceptibility of grading systems to such
manipulations, especially when combined with ensemble techniques like majority
voting and ridge regression, which further improve the system's defense against
sophisticated adversarial inputs. Additionally, employing large language models
such as GPT-4 with varied prompting techniques has shown promise in recognizing
and scoring gaming strategies effectively. The findings underscore the
importance of continuous improvements in AI-driven educational tools to ensure
their reliability and fairness in high-stakes settings.

</details>

### [80] [Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models](https://arxiv.org/abs/2505.00010)
*Tri Nguyen,Lohith Srikanth Pentapalli,Magnus Sieverding,Laurah Turner,Seth Overla,Weibing Zheng,Chris Zhou,David Furniss,Danielle Weber,Michael Gharib,Matt Kelleher,Michael Shukis,Cameron Pawlik,Kelly Cohen*

Main category: cs.CL

TLDR: 该研究通过分析语言特征检测LLM中的越狱行为，发现基于特征的模型优于提示工程，模糊决策树表现最佳。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的越狱行为威胁其在敏感领域（如教育）的安全使用，需开发有效的检测方法。

Method: 标注2,300多个提示，提取四个语言特征，训练多种预测模型（决策树、模糊逻辑分类器、提升方法和逻辑回归）。

Result: 基于特征的模型表现优于提示工程，模糊决策树效果最佳。

Conclusion: 基于语言特征的模型是检测越狱行为的有效且可解释的方法，未来可探索混合框架。

Abstract: Jailbreaking in Large Language Models (LLMs) threatens their safe use in
sensitive domains like education by allowing users to bypass ethical
safeguards. This study focuses on detecting jailbreaks in 2-Sigma, a clinical
education platform that simulates patient interactions using LLMs. We annotated
over 2,300 prompts across 158 conversations using four linguistic variables
shown to correlate strongly with jailbreak behavior. The extracted features
were used to train several predictive models, including Decision Trees, Fuzzy
Logic-based classifiers, Boosting methods, and Logistic Regression. Results
show that feature-based predictive models consistently outperformed Prompt
Engineering, with the Fuzzy Decision Tree achieving the best overall
performance. Our findings demonstrate that linguistic-feature-based models are
effective and explainable alternatives for jailbreak detection. We suggest
future work explore hybrid frameworks that integrate prompt-based flexibility
with rule-based robustness for real-time, spectrum-based jailbreak monitoring
in educational LLMs.

</details>

### [81] [The AI Co-Ethnographer: How Far Can Automation Take Qualitative Research?](https://arxiv.org/abs/2505.00012)
*Fabian Retkowski,Andreas Sudmann,Alexander Waibel*

Main category: cs.CL

TLDR: AICoE是一个端到端的定性研究管道，旨在超越简单的编码自动化，提供更全面的分析。


<details>
  <summary>Details</summary>
Motivation: 定性研究通常涉及劳动密集型过程，难以在保持分析深度的同时扩展。

Method: AICoE整合了开放编码、代码整合、代码应用和模式发现的全过程。

Result: AICoE能够对定性数据进行全面分析。

Conclusion: AICoE为定性研究提供了一种更高效且深度保留的方法。

Abstract: Qualitative research often involves labor-intensive processes that are
difficult to scale while preserving analytical depth. This paper introduces The
AI Co-Ethnographer (AICoE), a novel end-to-end pipeline developed for
qualitative research and designed to move beyond the limitations of simply
automating code assignments, offering a more integrated approach. AICoE
organizes the entire process, encompassing open coding, code consolidation,
code application, and even pattern discovery, leading to a comprehensive
analysis of qualitative data.

</details>

### [82] [Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa](https://arxiv.org/abs/2505.00013)
*Yoichi Takenaka*

Main category: cs.CL

TLDR: 研究旨在构建高精度模型预测日语文本中的八种Plutchik情绪，DeBERTa-v3-large表现最佳，优于其他模型和大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 日语文本情绪检测在社交媒体监控和客户反馈分析中有重要应用，但资源稀缺和类别不平衡限制了模型性能。

Method: 使用WRIME语料库，将读者平均强度分数转为二分类标签，微调四种预训练语言模型（BERT、RoBERTa、DeBERTa-v3-base、DeBERTa-v3-large），并评估两种大型语言模型（TinySwallow-1.5B-Instruct和ChatGPT-4o）。

Result: DeBERTa-v3-large在平均准确率（0.860）和F1分数（0.662）上表现最佳，而大型语言模型表现较差。

Conclusion: 微调后的DeBERTa-v3-large是目前日语二分类情绪检测的最可靠解决方案，未来需增强稀有情绪数据并优化模型。

Abstract: Background Practical applications such as social media monitoring and
customer-feedback analysis require accurate emotion detection for Japanese
text, yet resource scarcity and class imbalance hinder model performance.
  Objective This study aims to build a high-accuracy model for predicting the
presence or absence of eight Plutchik emotions in Japanese sentences.
  Methods Using the WRIME corpus, we transform reader-averaged intensity scores
into binary labels and fine-tune four pre-trained language models (BERT,
RoBERTa, DeBERTa-v3-base, DeBERTa-v3-large). For context, we also assess two
large language models (TinySwallow-1.5B-Instruct and ChatGPT-4o). Accuracy and
F1-score serve as evaluation metrics.
  Results DeBERTa-v3-large attains the best mean accuracy (0.860) and F1-score
(0.662), outperforming all other models. It maintains robust F1 across both
high-frequency emotions (e.g., Joy, Anticipation) and low-frequency emotions
(e.g., Anger, Trust). The LLMs lag, with ChatGPT-4o and
TinySwallow-1.5B-Instruct scoring 0.527 and 0.292 in mean F1, respectively.
  Conclusion The fine-tuned DeBERTa-v3-large model currently offers the most
reliable solution for binary emotion classification in Japanese. We release
this model as a pip-installable package (pip install
deberta-emotion-predictor). Future work should augment data for rare emotions,
reduce model size, and explore prompt engineering to improve LLM performance.
  This manuscript is under review for possible publication in New Generation
Computing.

</details>

### [83] [Manifold-Constrained Sentence Embeddings via Triplet Loss: Projecting Semantics onto Spheres, Tori, and Möbius Strips](https://arxiv.org/abs/2505.00014)
*Vinit K. Chavan*

Main category: cs.CL

TLDR: 论文提出了一种将句子嵌入约束在连续流形（如球面、环面和莫比乌斯带）上的新框架，通过三元组损失训练，显著提升了嵌入的区分性和拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 传统句子嵌入通常位于无约束的欧几里得空间，可能无法充分反映语言的复杂关系，因此需要探索几何约束的嵌入方法。

Method: 使用三元组损失将句子嵌入约束在球面、环面和莫比乌斯带上，通过微分几何约束优化嵌入空间。

Result: 在AG News和MBTI数据集上，流形约束嵌入（尤其是球面和莫比乌斯带）在聚类质量（Silhouette Score）和分类性能（Accuracy）上显著优于传统方法。

Conclusion: 流形空间嵌入通过拓扑结构补充语义分离，为NLP中的几何表示学习提供了新的数学基础方向。

Abstract: Recent advances in representation learning have emphasized the role of
embedding geometry in capturing semantic structure. Traditional sentence
embeddings typically reside in unconstrained Euclidean spaces, which may limit
their ability to reflect complex relationships in language. In this work, we
introduce a novel framework that constrains sentence embeddings to lie on
continuous manifolds -- specifically the unit sphere, torus, and M\"obius strip
-- using triplet loss as the core training objective. By enforcing differential
geometric constraints on the output space, our approach encourages the learning
of embeddings that are both discriminative and topologically structured.
  We evaluate our method on benchmark datasets (AG News and MBTI) and compare
it to classical baselines including TF-IDF, Word2Vec, and unconstrained
Keras-derived embeddings. Our results demonstrate that manifold-constrained
embeddings, particularly those projected onto spheres and M\"obius strips,
significantly outperform traditional approaches in both clustering quality
(Silhouette Score) and classification performance (Accuracy). These findings
highlight the value of embedding in manifold space -- where topological
structure complements semantic separation -- offering a new and mathematically
grounded direction for geometric representation learning in NLP.

</details>

### [84] [Design and Application of Multimodal Large Language Model Based System for End to End Automation of Accident Dataset Generation](https://arxiv.org/abs/2505.00015)
*MD Thamed Bin Zaman Chowdhury,Moazzem Hossain*

Main category: cs.CL

TLDR: 研究提出了一种基于大语言模型（LLM）和网络爬虫技术的自动化系统，用于解决孟加拉国交通事故数据收集的不可靠和碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 交通事故数据在发展中国家如孟加拉国存在手动收集、碎片化和不可靠的问题，导致数据不足和不一致。

Method: 系统包括四个模块：自动化网络爬虫代码生成、新闻收集、事故新闻分类与结构化数据提取、去重。使用多模态生成LLM Gemini-2.0-Flash实现自动化。

Result: 系统在111天内处理了15,000多篇新闻文章，识别出705起独特事故。代码生成模块校准准确率为91.3%，验证准确率为80%。

Conclusion: 研究表明，基于LLM的系统能够实现准确、低成本的交通事故数据收集，为数据驱动的道路安全政策制定提供了基础。

Abstract: Road traffic accidents remain a major public safety and socio-economic issue
in developing countries like Bangladesh. Existing accident data collection is
largely manual, fragmented, and unreliable, resulting in underreporting and
inconsistent records. This research proposes a fully automated system using
Large Language Models (LLMs) and web scraping techniques to address these
challenges. The pipeline consists of four components: automated web scraping
code generation, news collection from online sources, accident news
classification with structured data extraction, and duplicate removal. The
system uses the multimodal generative LLM Gemini-2.0-Flash for seamless
automation. The code generation module classifies webpages into pagination,
dynamic, or infinite scrolling categories and generates suitable Python scripts
for scraping. LLMs also classify and extract key accident information such as
date, time, location, fatalities, injuries, road type, vehicle types, and
pedestrian involvement. A deduplication algorithm ensures data integrity by
removing duplicate reports. The system scraped 14 major Bangladeshi news sites
over 111 days (Oct 1, 2024 - Jan 20, 2025), processing over 15,000 news
articles and identifying 705 unique accidents. The code generation module
achieved 91.3% calibration and 80% validation accuracy. Chittagong reported the
highest number of accidents (80), fatalities (70), and injuries (115), followed
by Dhaka, Faridpur, Gazipur, and Cox's Bazar. Peak accident times were morning
(8-9 AM), noon (12-1 PM), and evening (6-7 PM). A public repository was also
developed with usage instructions. This study demonstrates the viability of an
LLM-powered, scalable system for accurate, low-effort accident data collection,
providing a foundation for data-driven road safety policymaking in Bangladesh.

</details>

### [85] [Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning](https://arxiv.org/abs/2505.00016)
*Josefa Lia Stoisser,Marc Boubnovski Martell,Julien Fauqueur*

Main category: cs.CL

TLDR: 该研究将Text-to-SQL任务重新定义为教授大型语言模型（LLM）推理和操作表格数据的途径，提出了一种两阶段框架，通过SQL监督开发可迁移的表格推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统Text-to-SQL任务仅关注查询生成，而本研究旨在通过SQL监督提升LLM对表格数据的推理能力，实现更通用的表格操作能力。

Method: 1. 从真实SQL查询中合成详细的链式推理（CoT）轨迹，提供逐步、子句级监督；2. 引入GRPO强化学习目标，将SQL执行准确性与通用推理能力关联。

Result: 在标准Text-to-SQL基准测试中表现提升，尤其在BIRD和CRT-QA等推理密集型数据集上显著改进。LLaMA模型准确率提升20%，Qwen提升5%。

Conclusion: SQL不仅可以作为目标形式，还能作为学习结构化数据鲁棒、可迁移推理的有效支架。

Abstract: This work reframes the Text-to-SQL task as a pathway for teaching large
language models (LLMs) to reason over and manipulate tabular data--moving
beyond the traditional focus on query generation. We propose a two-stage
framework that leverages SQL supervision to develop transferable table
reasoning capabilities. First, we synthesize detailed chain-of-thought (CoT)
traces from real-world SQL queries, providing step-by-step, clause-level
supervision that teaches the model how to traverse, filter, and aggregate table
fields. Second, we introduce a Group Relative Policy Optimization (GRPO)
reinforcement learning objective that connects SQL execution accuracy to
generalizable reasoning by encouraging steps that extend beyond task-specific
syntax and transfer across datasets. Empirically, our approach improves
performance on standard Text-to-SQL benchmarks and achieves substantial gains
on reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced
generalization and interpretability. Specifically, the distilled-quantized
LLaMA model achieved a 20\% increase in accuracy when trained on Text-to-SQL
tasks, while Qwen achieved a 5\% increase. These results suggest that SQL can
serve not only as a target formalism but also as an effective scaffold for
learning robust, transferable reasoning over structured data.

</details>

### [86] [ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation](https://arxiv.org/abs/2505.00017)
*Dezheng Han,Yibin Jia,Ruxiao Chen,Wenjie Han,Shuaishuai Guo,Jianbo Wang*

Main category: cs.CL

TLDR: 提出了一种基于图结构特征标记数据库和多任务工作流的方法，用于提升大型语言模型在细胞类型注释中的精确性和自动化程度。


<details>
  <summary>Details</summary>
Motivation: 解决现有大型语言模型在细胞类型注释中精确性和自动化不足的问题。

Method: 开发了图结构特征标记数据库，并设计了多任务工作流以优化注释过程。

Result: 相比通用大型语言模型，该方法在11种组织类型中提升了人工评估分数0.21，语义相似性提高了6.1%，更接近人工注释的认知逻辑。

Conclusion: 该方法显著提升了细胞类型注释的精确性和自动化水平，更符合人工注释的逻辑。

Abstract: To enable precise and fully automated cell type annotation with large
language models (LLMs), we developed a graph structured feature marker database
to retrieve entities linked to differential genes for cell reconstruction. We
further designed a multi task workflow to optimize the annotation process.
Compared to general purpose LLMs, our method improves human evaluation scores
by up to 0.21 and semantic similarity by 6.1% across 11 tissue types, while
more closely aligning with the cognitive logic of manual annotation.

</details>

### [87] [An Empirical Study on Prompt Compression for Large Language Models](https://arxiv.org/abs/2505.00019)
*Zheng Zhang,Jinyi Li,Yihuai Lan,Xiang Wang,Hao Wang*

Main category: cs.CL

TLDR: 研究了六种提示压缩方法，旨在减少提示长度同时保持LLM响应质量，实验表明在长上下文任务中压缩效果更显著。


<details>
  <summary>Details</summary>
Motivation: 长提示增加了计算复杂性和经济成本，因此需要压缩提示以优化性能。

Method: 评估了六种提示压缩方法，覆盖生成性能、模型幻觉、多模态任务等，并在13个数据集上测试。

Result: 压缩对长上下文任务影响更大，适度压缩甚至能提升性能。

Conclusion: 提示压缩是有效的优化手段，尤其在长上下文任务中表现突出。

Abstract: Prompt engineering enables Large Language Models (LLMs) to perform a variety
of tasks. However, lengthy prompts significantly increase computational
complexity and economic costs. To address this issue, we study six prompt
compression methods for LLMs, aiming to reduce prompt length while maintaining
LLM response quality. In this paper, we present a comprehensive analysis
covering aspects such as generation performance, model hallucinations, efficacy
in multimodal tasks, word omission analysis, and more. We evaluate these
methods across 13 datasets, including news, scientific articles, commonsense
QA, math QA, long-context QA, and VQA datasets. Our experiments reveal that
prompt compression has a greater impact on LLM performance in long contexts
compared to short ones. In the Longbench evaluation, moderate compression even
enhances LLM performance. Our code and data is available at
https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression.

</details>

### [88] [Beyond Public Access in LLM Pre-Training Data](https://arxiv.org/abs/2505.00020)
*Sruly Rosenblat,Tim O'Reilly,Ilan Strauss*

Main category: cs.CL

TLDR: 论文通过DE-COP方法检测OpenAI模型是否未经许可使用了O'Reilly Media的版权内容，发现GPT-4o对付费内容识别率较高（AUROC=82%），而GPT-3.5 Turbo对公开内容更敏感，GPT-4o Mini则无识别能力。


<details>
  <summary>Details</summary>
Motivation: 研究OpenAI大型语言模型是否在未经授权的情况下使用了受版权保护的O'Reilly Media书籍内容。

Method: 使用DE-COP成员推理攻击方法，测试GPT-4o、GPT-3.5 Turbo和GPT-4o Mini对O'Reilly书籍内容的识别能力。

Result: GPT-4o对付费内容识别率显著（AUROC=82%），GPT-3.5 Turbo对公开内容更敏感，GPT-4o Mini无识别能力（AUROC≈50%）。

Conclusion: 研究强调了企业在AI训练数据来源上需要更高的透明度，以建立正式的内容授权框架。

Abstract: Using a legally obtained dataset of 34 copyrighted O'Reilly Media books, we
apply the DE-COP membership inference attack method to investigate whether
OpenAI's large language models were trained on copyrighted content without
consent. Our AUROC scores show that GPT-4o, OpenAI's more recent and capable
model, demonstrates strong recognition of paywalled O'Reilly book content
(AUROC = 82\%), compared to OpenAI's earlier model GPT-3.5 Turbo. In contrast,
GPT-3.5 Turbo shows greater relative recognition of publicly accessible
O'Reilly book samples. GPT-4o Mini, as a much smaller model, shows no knowledge
of public or non-public O'Reilly Media content when tested (AUROC $\approx$
50\%). Testing multiple models, with the same cutoff date, helps us account for
potential language shifts over time that might bias our findings. These results
highlight the urgent need for increased corporate transparency regarding
pre-training data sources as a means to develop formal licensing frameworks for
AI content training

</details>

### [89] [Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss](https://arxiv.org/abs/2505.00021)
*Zhuoang Cai,Zhenghao Li,Yang Liu,Liyuan Guo,Yangqiu Song*

Main category: cs.CL

TLDR: 论文提出了一种解决食品危害检测中数据不平衡问题的方法，通过数据增强和多种平衡策略提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 食品危害检测任务面临严重类别不平衡、文本短且非结构化、语义类别重叠等问题，需改进分类性能。

Method: 使用BERT和RoBERTa作为基础分类器，结合随机过采样、EDA数据增强和焦点损失等策略。

Result: EDA有效缓解类别不平衡，显著提升准确率和F1分数；结合焦点损失和过采样进一步增强了模型鲁棒性。

Conclusion: 该方法为食品危害检测的NLP分类模型提供了更有效的解决方案。

Abstract: Classification tasks often suffer from imbal- anced data distribution, which
presents chal- lenges in food hazard detection due to severe class imbalances,
short and unstructured text, and overlapping semantic categories. In this
paper, we present our system for SemEval- 2025 Task 9: Food Hazard Detection,
which ad- dresses these issues by applying data augmenta- tion techniques to
improve classification perfor- mance. We utilize transformer-based models, BERT
and RoBERTa, as backbone classifiers and explore various data balancing
strategies, including random oversampling, Easy Data Augmentation (EDA), and
focal loss. Our ex- periments show that EDA effectively mitigates class
imbalance, leading to significant improve- ments in accuracy and F1 scores.
Furthermore, combining focal loss with oversampling and EDA further enhances
model robustness, par- ticularly for hard-to-classify examples. These findings
contribute to the development of more effective NLP-based classification models
for food hazard detection.

</details>

### [90] [Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation](https://arxiv.org/abs/2505.00022)
*Thomas F Burns,Letitia Parcalabescu,Stephan Wäldchen,Michael Barlow,Gregor Ziegltrum,Volker Stampa,Bastian Harren,Björn Deiseroth*

Main category: cs.CL

TLDR: 论文介绍了一种德语数据集构建流程，结合启发式和模型过滤技术及合成数据生成，显著提升大语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 数据质量对提升大语言模型性能至关重要，但现有德语数据集质量不足。

Method: 采用启发式和模型过滤技术，结合合成数据生成，构建德语预训练数据集Aleph-Alpha-GermanWeb。

Result: 在德语基准测试中，Aleph-Alpha-GermanWeb表现优于FineWeb2，即使后者加入高质量数据。

Conclusion: 模型驱动的数据筛选和合成数据生成能显著提升预训练数据集质量。

Abstract: Scaling data quantity is essential for large language models (LLMs), yet
recent findings show that data quality can significantly boost performance and
training efficiency. We introduce a German-language dataset curation pipeline
that combines heuristic and model-based filtering techniques with synthetic
data generation. We use our pipeline to create Aleph-Alpha-GermanWeb, a
large-scale German pre-training dataset which draws from: (1) Common Crawl web
data, (2) FineWeb2, and (3) synthetically-generated data conditioned on actual,
organic web data. We evaluate our dataset by pre-training both a 1B Llama-style
model and an 8B tokenizer-free hierarchical autoregressive transformer (HAT). A
comparison on German-language benchmarks, including MMMLU, shows significant
performance gains of Aleph-Alpha-GermanWeb over FineWeb2 alone. This advantage
holds at the 8B scale even when FineWeb2 is enriched by human-curated
high-quality data sources such as Wikipedia. Our findings support the growing
body of evidence that model-based data curation and synthetic data generation
can significantly enhance LLM pre-training datasets.

</details>

### [91] [CORG: Generating Answers from Complex, Interrelated Contexts](https://arxiv.org/abs/2505.00023)
*Hyunji Lee,Franck Dernoncourt,Trung Bui,Seunghyun Yoon*

Main category: cs.CL

TLDR: 论文提出Context Organizer (CORG)框架，用于处理文档中知识重复和不一致的问题，通过分组处理上下文，有效解决歧义和复杂性。


<details>
  <summary>Details</summary>
Motivation: 现实语料库中知识重复但存在不一致性，语言模型难以处理复杂上下文关系，需系统化解决方案。

Method: CORG框架包含图构造器、重排序器和聚合器，将上下文分组独立处理。

Result: CORG在性能和效率上优于现有分组方法，接近计算密集型单上下文方法的性能。

Conclusion: CORG能有效组织复杂上下文关系，平衡性能与效率，为知识处理提供新思路。

Abstract: In a real-world corpus, knowledge frequently recurs across documents but
often contains inconsistencies due to ambiguous naming, outdated information,
or errors, leading to complex interrelationships between contexts. Previous
research has shown that language models struggle with these complexities,
typically focusing on single factors in isolation. We classify these
relationships into four types: distracting, ambiguous, counterfactual, and
duplicated. Our analysis reveals that no single approach effectively addresses
all these interrelationships simultaneously. Therefore, we introduce Context
Organizer (CORG), a framework that organizes multiple contexts into
independently processed groups. This design allows the model to efficiently
find all relevant answers while ensuring disambiguation. CORG consists of three
key components: a graph constructor, a reranker, and an aggregator. Our results
demonstrate that CORG balances performance and efficiency effectively,
outperforming existing grouping methods and achieving comparable results to
more computationally intensive, single-context approaches.

</details>

### [92] [Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning](https://arxiv.org/abs/2505.00024)
*Shaokun Zhang,Yi Dong,Jieyu Zhang,Jan Kautz,Bryan Catanzaro,Andrew Tao,Qingyun Wu,Zhiding Yu,Guilin Liu*

Main category: cs.CL

TLDR: 论文提出了一种新方法Nemotron-Research-Tool-N1系列模型，通过轻量级监督优化工具调用，无需标注推理轨迹，实现了超越GPT-4o的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在增强语言模型工具使用能力时，要么忽略推理，要么产生模仿性推理，限制了泛化能力。

Method: 采用类似DeepSeek-R1的规则强化学习训练范式，仅通过二元奖励评估工具调用的结构有效性和功能正确性。

Result: 在BFCL和API-Bank基准测试中，Nemotron-Research-Tool-N1-7B和14B模型表现优于GPT-4o。

Conclusion: 轻量级监督方法能有效提升语言模型的工具使用能力，且无需依赖标注推理轨迹。

Abstract: Enabling large language models with external tools has become a pivotal
strategy for extending their functionality beyond text generation tasks. Prior
work typically enhances tool-use abilities by either applying supervised
fine-tuning (SFT) to enforce tool-call correctness or distilling reasoning
traces from stronger models for SFT. However, both approaches fall short,
either omitting reasoning entirely or producing imitative reasoning that limits
generalization. Inspired by the success of DeepSeek-R1 in eliciting reasoning
through rule-based reinforcement learning, we develop the
Nemotron-Research-Tool-N1 series of tool-using language models using a similar
training paradigm. Instead of restrictively supervising intermediate reasoning
traces distilled from stronger models, Nemotron-Research-Tool-N1 is optimized
with a binary reward that evaluates only the structural validity and functional
correctness of tool invocations. This lightweight supervision allows the model
to autonomously internalize reasoning strategies, without the need for
annotated reasoning trajectories. Experiments on the BFCL and API-Bank
benchmarks show that Nemotron-Research-Tool-N1-7B and
Nemotron-Research-Tool-N1-14B, built on Qwen-2.5-7B/14B-Instruct, achieve
state-of-the-art results, outperforming GPT-4o on both evaluations.

</details>

### [93] [A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1](https://arxiv.org/abs/2505.00025)
*Mingda Zhang,Jianglong Qin*

Main category: cs.CL

TLDR: 本文提出了一种高效的轻量级医学垂直大语言模型架构方法，通过知识获取、模型压缩和计算优化三个维度解决医学大模型的轻量化问题，显著降低了内存消耗和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在通用任务中表现出色，但在医学场景中面临专业知识壁垒、计算资源需求和部署环境限制等问题，亟需轻量化解决方案。

Method: 设计了知识转移管道，采用LoRA技术调整关键注意力层；实施4位权重量化等压缩技术；集成Flash Attention加速和连续批处理等推理优化技术。

Result: 在医学问答数据集上，该方法在保持专业准确性的同时，内存消耗减少64.7%，推理延迟降低12.4%。

Conclusion: 该方法为资源受限环境（如边缘计算设备）中的医学大模型应用提供了有效解决方案。

Abstract: In recent years, despite foundation models like DeepSeek-R1 and ChatGPT
demonstrating significant capabilities in general tasks, professional knowledge
barriers, computational resource requirements, and deployment environment
limitations have severely hindered their application in actual medical
scenarios. Addressing these challenges, this paper proposes an efficient
lightweight medical vertical large language model architecture method,
systematically solving the lightweight problem of medical large models from
three dimensions: knowledge acquisition, model compression, and computational
optimization. At the knowledge acquisition level, a knowledge transfer pipeline
is designed from the fine-tuned DeepSeek-R1-Distill-70B teacher model to the
DeepSeek-R1-Distill-7B student model, and Low-Rank Adaptation (LoRA) technology
is adopted to precisely adjust key attention layers. At the model compression
level, compression techniques including 4-bit weight quantization are
implemented while preserving the core representation ability for medical
reasoning. At the computational optimization level, inference optimization
techniques such as Flash Attention acceleration and continuous batching are
integrated, and a professional prompt template system is constructed to adapt
to different types of medical problems. Experimental results on medical
question-answering datasets show that the method proposed in this paper
maintains professional accuracy while reducing memory consumption by 64.7\% and
inference latency by 12.4\%, providing an effective solution for the
application of medical large models in resource-constrained environments such
as edge computing devices.

</details>

### [94] [Theory of Mind in Large Language Models: Assessment and Enhancement](https://arxiv.org/abs/2505.00026)
*Ruirui Chen,Weifeng Jiang,Chengwei Qin,Cheston Tan*

Main category: cs.CL

TLDR: 该论文综述了大语言模型（LLMs）的心理理论（ToM）能力，包括评估基准和改进策略，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 评估和提升LLMs对人类心理状态的理解能力，以增强其社会智能。

Method: 通过分析故事型基准测试和改进方法，深入研究LLMs的ToM能力。

Result: 总结了现有基准和方法，并指出了未来研究的潜力方向。

Conclusion: 该综述为提升LLMs的ToM能力提供了有价值的参考和研究方向。

Abstract: Theory of Mind (ToM)-the ability to infer and reason about others' mental
states-is fundamental to human social intelligence. As Large Language Models
(LLMs) become increasingly integrated into daily life, it is crucial to assess
and enhance their capacity to interpret and respond to human mental states. In
this paper, we review LLMs' ToM capabilities by examining both evaluation
benchmarks and the strategies designed to improve them. We focus on widely
adopted story-based benchmarks and provide an in-depth analysis of methods
aimed at enhancing ToM in LLMs. Furthermore, we outline promising future
research directions informed by recent benchmarks and state-of-the-art
approaches. Our survey serves as a valuable resource for researchers interested
in advancing LLMs' ToM capabilities.

</details>

### [95] [Extracting Abstraction Dimensions by Identifying Syntax Pattern from Texts](https://arxiv.org/abs/2505.00027)
*Jian Zhou,Jiazheng Li,Sirui Zhuge,Hai Zhuge*

Main category: cs.CL

TLDR: 提出一种从文本中自动发现主语、动作、宾语和状语维度的方法，以高效操作文本并支持自然语言查询。


<details>
  <summary>Details</summary>
Motivation: 通过高质量和独立的树结构，全面表示文本中的主语、动作、宾语、状语及其子类关系，支持自然语言查询和精确操作。

Method: 构建抽象树表示文本中的主语、动作、宾语和状语维度，确保树的独立性、表达性和覆盖性。

Result: 实验显示抽象树的平均精确率、召回率和F1分数均超过80%，支持高效自然语言查询和快速定位目标句子。

Conclusion: 该方法通过多树搜索机制，显著提升文本操作和自然语言查询的效率和精确性。

Abstract: This paper proposed an approach to automatically discovering subject
dimension, action dimension, object dimension and adverbial dimension from
texts to efficiently operate texts and support query in natural language. The
high quality of trees guarantees that all subjects, actions, objects and
adverbials and their subclass relations within texts can be represented. The
independency of trees ensures that there is no redundant representation between
trees. The expressiveness of trees ensures that the majority of sentences can
be accessed from each tree and the rest of sentences can be accessed from at
least one tree so that the tree-based search mechanism can support querying in
natural language. Experiments show that the average precision, recall and
F1-score of the abstraction trees constructed by the subclass relations of
subject, action, object and adverbial are all greater than 80%. The application
of the proposed approach to supporting query in natural language demonstrates
that different types of question patterns for querying subject or object have
high coverage of texts, and searching multiple trees on subject, action, object
and adverbial according to the question pattern can quickly reduce search space
to locate target sentences, which can support precise operation on texts.

</details>

### [96] [Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation](https://arxiv.org/abs/2505.00028)
*Pengchao Feng,Ziyang Ma,Wenxi Chen,Yao Li,Sheng Wang,Kai Yu,Xie Chen*

Main category: cs.CL

TLDR: 论文提出了一种新颖的端到端RAG框架，直接从语音查询中检索相关文本知识，避免了中间语音转文本步骤，显著提升了端到端语音对话系统的性能。


<details>
  <summary>Details</summary>
Motivation: 端到端语音对话系统在整合外部知识方面存在挑战，尤其是语音与文本知识之间的模态差异。

Method: 提出了一种端到端的RAG框架，直接从语音查询中检索文本知识，无需中间语音转文本步骤。

Result: 实验表明，该方法显著提升了端到端语音对话系统的性能，同时提高了检索效率。

Conclusion: 尽管性能仍落后于级联模型，但该框架为端到端语音对话系统的知识整合提供了有前景的方向。

Abstract: In recent years, end-to-end speech-to-speech (S2S) dialogue systems have
garnered increasing research attention due to their advantages over traditional
cascaded systems, including achieving lower latency and more natural
integration of nonverbal cues such as emotion and speaker identity. However,
these end-to-end systems face key challenges, particularly in incorporating
external knowledge, a capability commonly addressed by Retrieval-Augmented
Generation (RAG) in text-based large language models (LLMs). The core
difficulty lies in the modality gap between input speech and retrieved textual
knowledge, which hinders effective integration. To address this issue, we
propose a novel end-to-end RAG framework that directly retrieves relevant
textual knowledge from speech queries, eliminating the need for intermediate
speech-to-text conversion via techniques like ASR. Experimental results
demonstrate that our method significantly improves the performance of
end-to-end S2S dialogue systems while achieving higher retrieval efficiency.
Although the overall performance still lags behind cascaded models, our
framework offers a promising direction for enhancing knowledge integration in
end-to-end S2S systems. We will release the code and dataset to support
reproducibility and promote further research in this area.

</details>

### [97] [Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for Knowledge Injection without Catastrophic Forgetting](https://arxiv.org/abs/2505.00029)
*Yijie Hong,Xiaofei Yin,Xinzhong Wang,Yi Tu,Ya Guo,Sufeng Duan,Weiqiang Wang,Lingyong Fang,Depeng Wang,Huijia Zhu*

Main category: cs.CL

TLDR: SDFT方法通过三阶段对话结构（基础保留、对比消歧和知识专业化）有效注入领域知识，同时减少灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决大型视觉语言模型在领域知识注入时导致的基础能力遗忘问题。

Method: 采用三阶段对话结构：基础保留、对比消歧和知识专业化。

Result: 实验证明SDFT在多个领域有效平衡知识获取与能力保留。

Conclusion: SDFT提供了一种数据中心的对话模板，成功平衡基础能力与领域知识。

Abstract: Large Vision Language Models have demonstrated impressive versatile
capabilities through extensive multimodal pre-training, but face significant
limitations when incorporating specialized knowledge domains beyond their
training distribution. These models struggle with a fundamental dilemma: direct
adaptation approaches that inject domain-specific knowledge often trigger
catastrophic forgetting of foundational visual-linguistic abilities. We
introduce Structured Dialogue Fine-Tuning (SDFT), an effective approach that
effectively injects domain-specific knowledge while minimizing catastrophic
forgetting. Drawing inspiration from supervised fine-tuning in LLMs and
subject-driven personalization in text-to-image diffusion models, our method
employs a three-phase dialogue structure: Foundation Preservation reinforces
pre-trained visual-linguistic alignment through caption tasks; Contrastive
Disambiguation introduces carefully designed counterfactual examples to
maintain semantic boundaries; and Knowledge Specialization embeds specialized
information through chain-of-thought reasoning. Experimental results across
multiple domains confirm SDFT's effectiveness in balancing specialized
knowledge acquisition with general capability retention. Our key contributions
include a data-centric dialogue template that balances foundational alignment
with targeted knowledge integration, a weighted multi-turn supervision
framework, and comprehensive evaluation across diverse knowledge types.

</details>

### [98] [Can Language Models Represent the Past without Anachronism?](https://arxiv.org/abs/2505.00030)
*Ted Underwood,Laura K. Nelson,Matthew Wilkens*

Main category: cs.CL

TLDR: 研究发现，当代语言模型无法通过提示或微调完全模拟历史文本风格，需预训练以可靠模拟历史视角。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型模拟历史文本风格的可行性及其局限性。

Method: 通过提示和微调当代语言模型，比较其输出与真实历史文本的差异。

Result: 微调模型能欺骗自动化评估，但人类仍能区分其输出与真实历史文本。

Conclusion: 需对模型进行历史文本预训练，才能可靠模拟历史视角。

Abstract: Before researchers can use language models to simulate the past, they need to
understand the risk of anachronism. We find that prompting a contemporary model
with examples of period prose does not produce output consistent with period
style. Fine-tuning produces results that are stylistically convincing enough to
fool an automated judge, but human evaluators can still distinguish fine-tuned
model outputs from authentic historical text. We tentatively conclude that
pretraining on period prose may be required in order to reliably simulate
historical perspectives for social research.

</details>

### [99] [Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving](https://arxiv.org/abs/2505.00031)
*Jin Zhang,Flood Sung,Zhilin Yang,Yang Gao,Chongjie Zhang*

Main category: cs.CL

TLDR: 论文提出了一种名为LEPA的自训练算法，通过让大语言模型在解决问题前生成抽象的计划，提升其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅生成逐步解决方案，缺乏抽象元知识，无法泛化到类似问题。

Method: LEPA算法让模型在解决问题前生成抽象计划，并通过自我反思优化计划，训练模型同时预测计划和解决方案。

Result: LEPA在多个自然语言推理基准测试中显著优于传统方法。

Conclusion: LEPA通过提取和利用抽象计划，有效提升了大语言模型的推理能力。

Abstract: In the field of large language model (LLM) post-training, the effectiveness
of utilizing synthetic data generated by the LLM itself has been
well-presented. However, a key question remains unaddressed: what essential
information should such self-generated data encapsulate? Existing approaches
only produce step-by-step problem solutions, and fail to capture the abstract
meta-knowledge necessary for generalization across similar problems. Drawing
insights from cognitive science, where humans employ high-level abstraction to
simplify complex problems before delving into specifics, we introduce a novel
self-training algorithm: LEarning to Plan before Answering (LEPA). LEPA trains
the LLM to formulate anticipatory plans, which serve as abstract meta-knowledge
for problem-solving, before engaging with the intricacies of problems. This
approach not only outlines the solution generation path but also shields the
LLM from the distraction of irrelevant details. During data generation, LEPA
first crafts an anticipatory plan based on the problem, and then generates a
solution that aligns with both the plan and the problem. LEPA refines the plan
through self-reflection, aiming to acquire plans that are instrumental in
yielding correct solutions. During model optimization, the LLM is trained to
predict both the refined plans and the corresponding solutions. By efficiently
extracting and utilizing the anticipatory plans, LEPA demonstrates remarkable
superiority over conventional algorithms on various challenging natural
language reasoning benchmarks.

</details>

### [100] [MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis](https://arxiv.org/abs/2505.00032)
*Yuyang Sha,Hongxin Pan,Wei Xu,Weiyu Meng,Gang Luo,Xinyu Du,Xiaobing Zhai,Henry H. Y. Tong,Caijuan Shi,Kefeng Li*

Main category: cs.CL

TLDR: 本文提出了一种名为MDD-LLM的高性能抑郁症诊断工具，基于大语言模型（LLMs）和真实世界样本，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 抑郁症（MDD）影响全球超3亿人，但医疗资源分布不均和诊断方法复杂导致许多地区关注不足。

Method: 利用UK Biobank队列中的274,348条个体数据，设计表格数据转换方法训练和评估模型。

Result: MDD-LLM（70B）准确率达0.8378，AUC为0.8919，显著优于现有机器学习框架。

Conclusion: MDD-LLM在抑郁症诊断中表现出色，并探讨了影响性能的关键因素。

Abstract: Major depressive disorder (MDD) impacts more than 300 million people
worldwide, highlighting a significant public health issue. However, the uneven
distribution of medical resources and the complexity of diagnostic methods have
resulted in inadequate attention to this disorder in numerous countries and
regions. This paper introduces a high-performance MDD diagnosis tool named
MDD-LLM, an AI-driven framework that utilizes fine-tuned large language models
(LLMs) and extensive real-world samples to tackle challenges in MDD diagnosis.
Therefore, we select 274,348 individual information from the UK Biobank cohort
to train and evaluate the proposed method. Specifically, we select 274,348
individual records from the UK Biobank cohort and design a tabular data
transformation method to create a large corpus for training and evaluating the
proposed approach. To illustrate the advantages of MDD-LLM, we perform
comprehensive experiments and provide several comparative analyses against
existing model-based solutions across multiple evaluation metrics. Experimental
results show that MDD-LLM (70B) achieves an accuracy of 0.8378 and an AUC of
0.8919 (95% CI: 0.8799 - 0.9040), significantly outperforming existing machine
learning and deep learning frameworks for MDD diagnosis. Given the limited
exploration of LLMs in MDD diagnosis, we examine numerous factors that may
influence the performance of our proposed method, such as tabular data
transformation techniques and different fine-tuning strategies.

</details>

### [101] [From Attention to Atoms: Spectral Dictionary Learning for Fast, Interpretable Language Models](https://arxiv.org/abs/2505.00033)
*Andrew Kiruluta*

Main category: cs.CL

TLDR: 提出了一种基于谱生成模型的新型自然语言处理框架，替代了Transformer中的自注意力机制，通过联合学习全局时变傅里叶字典和每个token的混合系数，实现了线性计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制在Transformer中计算复杂度高（二次），限制了其可扩展性，因此需要一种更高效的替代方案。

Method: 结合时域（嵌入重建）和频域（短时傅里叶变换幅度匹配）的重构损失，以及语言建模目标，并采用高斯混合模型（GMM）对混合向量建模。

Result: 在WikiText2和Penn Treebank等基准测试中实现了与Transformer相当的性能，同时显著降低了推理延迟和内存占用。

Conclusion: 谱字典模型是一种高效且可扩展的语言建模替代方案，性能与Transformer相当但计算复杂度更低。

Abstract: We propose a novel spectral generative modeling framework for natural
language processing that jointly learns a global time varying Fourier
dictionary and per token mixing coefficients, replacing the ubiquitous self
attention mechanism in transformer architectures. By enforcing reconstruction
losses in both the time domain (embedding reconstruction) and the frequency
domain (via Short Time Fourier Transform magnitude matching) alongside a
standard language modeling objective, and fitting a Gaussian Mixture Model
(GMM) prior over the learned mixing vectors, our approach achieves competitive
perplexity and generation quality on standard benchmarks such as WikiText2 and
Penn Treebank. In contrast to the quadratic computation complexity of self
attention, our method operates with linear complexity, delivering substantial
efficiency gains. We demonstrate that spectral dictionary models can achieve
competitive performance compared to transformer baselines while significantly
reducing inference latency and memory footprint, offering a compelling
alternative for scalable language modeling.

</details>

### [102] [Improving Phishing Email Detection Performance of Small Large Language Models](https://arxiv.org/abs/2505.00034)
*Zijie Lin,Zikang Liu,Hanbo Fan*

Main category: cs.CL

TLDR: 研究探讨了小型参数大语言模型（LLMs）在钓鱼邮件检测中的有效性，并通过提示工程、解释增强微调和模型集成等方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大参数LLMs在NLP任务中表现优异，但其计算资源需求高。为降低成本，研究小型LLMs在钓鱼邮件检测中的应用。

Method: 采用提示工程、解释增强微调和模型集成等方法优化小型LLMs。

Result: 在SpamAssassin数据集上，准确率从基线模型的0.5提升至0.976。

Conclusion: 小型LLMs通过优化方法可在钓鱼邮件检测中达到高性能，降低计算成本。

Abstract: Large language models(LLMs) have demonstrated remarkable performance on many
natural language processing(NLP) tasks and have been employed in phishing email
detection research. However, in current studies, well-performing LLMs typically
contain billions or even tens of billions of parameters, requiring enormous
computational resources. To reduce computational costs, we investigated the
effectiveness of small-parameter LLMs for phishing email detection. These LLMs
have around 3 billion parameters and can run on consumer-grade GPUs. However,
small LLMs often perform poorly in phishing email detection task. To address
these issues, we designed a set of methods including Prompt Engineering,
Explanation Augmented Fine-tuning, and Model Ensemble to improve phishing email
detection capabilities of small LLMs. We validated the effectiveness of our
approach through experiments, significantly improving accuracy on the
SpamAssassin dataset from around 0.5 for baseline models like
Qwen2.5-1.5B-Instruct to 0.976.

</details>

### [103] [Linguistic Complexity and Socio-cultural Patterns in Hip-Hop Lyrics](https://arxiv.org/abs/2505.00035)
*Aayam Bansal,Raghav Agarwal,Kaashvi Jain*

Main category: cs.CL

TLDR: 该论文通过自然语言处理技术分析了1980-2020年间3,814首嘻哈歌词的语言复杂性和社会文化趋势，揭示了词汇多样性、押韵密度和主题内容的显著变化。


<details>
  <summary>Details</summary>
Motivation: 研究嘻哈歌词的语言复杂性和社会文化趋势，以理解其作为艺术形式和社会动态反映的演变。

Method: 使用自然语言处理技术对3,814首歌曲的歌词进行量化分析，包括词汇多样性、押韵密度、主题建模和情感分析。

Result: 词汇多样性增加23.7%，押韵密度增加34.2%，主题内容从社会正义转向内省，情感在政治危机时更负面。

Conclusion: 研究为嘻哈音乐作为艺术形式和社会动态反映的演变提供了量化证据，揭示了语言创新与文化背景的互动。

Abstract: This paper presents a comprehensive computational framework for analyzing
linguistic complexity and socio-cultural trends in hip-hop lyrics. Using a
dataset of 3,814 songs from 146 influential artists spanning four decades
(1980-2020), we employ natural language processing techniques to quantify
multiple dimensions of lyrical complexity. Our analysis reveals a 23.7%
increase in vocabulary diversity over the study period, with East Coast artists
demonstrating 17.3% higher lexical variation than other regions. Rhyme density
increased by 34.2% across all regions, with Midwest artists exhibiting the
highest technical complexity (3.04 rhymes per line). Topic modeling identified
significant shifts in thematic content, with social justice themes decreasing
from 28.5% to 13.8% of content while introspective themes increased from 7.6%
to 26.3%. Sentiment analysis demon- strated that lyrics became significantly
more negative during sociopolitical crises, with polarity decreasing by 0.31
following major social unrest. Multi-dimensional analysis revealed four dis-
tinct stylistic approaches that correlate strongly with geographic origin
(r=0.68, p!0.001) and time period (r=0.59, p<0.001). These findings establish
quantitative evidence for the evolution of hip- hop as both an art form and a
reflection of societal dynamics, providing insights into the interplay between
linguistic innovation and cultural context in popular music.

</details>

### [104] [A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies](https://arxiv.org/abs/2505.00036)
*Zhongren Chen,Joshua Kalla,Quan Le,Shinpei Nakamura-Sakai,Jasjeet Sekhon,Ruixiao Wang*

Main category: cs.CL

TLDR: 研究探讨了大型语言模型（LLMs）在政治说服中的成本效益，发现其与传统竞选方法相比成本更低，但规模化难度更大。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在民主社会中的潜在威胁，特别是其说服能力对选民的影响。

Method: 通过两项调查实验和真实世界模拟，比较LLMs与传统竞选方法的说服成本及效果。

Result: LLMs说服每位选民的成本（48-74美元）低于传统方法（100美元），但规模化困难。

Conclusion: 目前LLMs的大规模政治说服潜力有限，但随着技术进步，其潜力可能增加。

Abstract: In recent years, significant concern has emerged regarding the potential
threat that Large Language Models (LLMs) pose to democratic societies through
their persuasive capabilities. We expand upon existing research by conducting
two survey experiments and a real-world simulation exercise to determine
whether it is more cost effective to persuade a large number of voters using
LLM chatbots compared to standard political campaign practice, taking into
account both the "receive" and "accept" steps in the persuasion process (Zaller
1992). These experiments improve upon previous work by assessing extended
interactions between humans and LLMs (instead of using single-shot
interactions) and by assessing both short- and long-run persuasive effects
(rather than simply asking users to rate the persuasiveness of LLM-produced
content). In two survey experiments (N = 10,417) across three distinct
political domains, we find that while LLMs are about as persuasive as actual
campaign ads once voters are exposed to them, political persuasion in the
real-world depends on both exposure to a persuasive message and its impact
conditional on exposure. Through simulations based on real-world parameters, we
estimate that LLM-based persuasion costs between \$48-\$74 per persuaded voter
compared to \$100 for traditional campaign methods, when accounting for the
costs of exposure. However, it is currently much easier to scale traditional
campaign persuasion methods than LLM-based persuasion. While LLMs do not
currently appear to have substantially greater potential for large-scale
political persuasion than existing non-LLM methods, this may change as LLM
capabilities continue to improve and it becomes easier to scalably encourage
exposure to persuasive LLMs.

</details>

### [105] [HyPerAlign: Hypotheses-driven Personalized Alignment](https://arxiv.org/abs/2505.00038)
*Cristina Garbacea,Chenhao Tan*

Main category: cs.CL

TLDR: 论文提出了一种基于假设驱动的个性化方法（HyPerAlign），通过少量用户示例推断其偏好和风格，从而为LLM生成定制化输出，优于传统基于偏好的微调方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM通常针对“平均用户”偏好进行对齐，但实际使用中需要针对个体用户的个性化输出。

Method: 提出HyPerAlign方法，通过用户示例推断其偏好和风格，并用于生成定制化输出。

Result: 在作者归属和审议对齐任务中，HyPerAlign显著优于传统方法，帮助性提升70%，作者归属胜率>90%。

Conclusion: HyPerAlign是一种高效、可解释的LLM个性化方法。

Abstract: Alignment algorithms are widely used to align large language models (LLMs) to
human users based on preference annotations that reflect their intended
real-world use cases. Typically these (often divergent) preferences are
aggregated over a diverse set of users, resulting in fine-tuned models that are
aligned to the ``average-user'' preference. Nevertheless, current models are
used by individual users in very specific contexts and situations, emphasizing
the need for user-dependent preference control. In this work we address the
problem of personalizing LLM outputs to their users, aiming to generate
customized responses tailored to individual users, instead of generic outputs
that emulate the collective voices of diverse populations. We propose a novel
interpretable and sample-efficient hypotheses-driven personalization approach
(HyPerAlign) where given few-shot examples written by a particular user, we
first infer hypotheses about their communication strategies, personality and
writing style, then prompt LLM models with these hypotheses and user specific
attributes to generate customized outputs. We conduct experiments on two
different personalization tasks, authorship attribution and deliberative
alignment, with datasets from diverse domains (news articles, blog posts,
emails, jailbreaking benchmarks), and demonstrate the superiority of
hypotheses-driven personalization approach when compared to preference-based
fine-tuning methods. For deliberative alignment, the helpfulness of LLM models
is improved by up to $70\%$ on average. For authorship attribution, results
indicate consistently high win-rates (commonly $>90\%$) against
state-of-the-art preference fine-tuning approaches for LLM personalization
across diverse user profiles and LLM models. Overall, our approach represents
an interpretable and sample-efficient strategy for the personalization of LLM
models to individual users.

</details>

### [106] [Graph RAG for Legal Norms: A Hierarchical and Temporal Approach](https://arxiv.org/abs/2505.00039)
*Hudson de Martim*

Main category: cs.CL

TLDR: 本文提出了一种针对法律规范分析的Graph RAG改进方法，结合知识图谱和文本片段，以应对法律数据的复杂性和规模。


<details>
  <summary>Details</summary>
Motivation: 法律规范具有层次结构、引用网络和多版本特性，传统方法难以处理其复杂性。

Method: 结合层次结构和时间演化的知识图谱，以及综合文本单元，构建更丰富的法律知识表示。

Result: Graph RAG为法律研究、立法分析和决策支持提供了更有效的AI解决方案。

Conclusion: 该方法显著推动了AI在法律领域的应用，为相关系统提供了新机会。

Abstract: This article proposes an adaptation of Graph Retrieval Augmented Generation
(Graph RAG) specifically designed for the analysis and comprehension of legal
norms, which are characterized by their predefined hierarchical structure,
extensive network of internal and external references and multiple temporal
versions. By combining structured knowledge graphs with contextually enriched
text segments, Graph RAG offers a promising solution to address the inherent
complexity and vast volume of legal data. The integration of hierarchical
structure and temporal evolution into knowledge graphs - along with the concept
of comprehensive Text Units - facilitates the construction of richer,
interconnected representations of legal knowledge. Through a detailed analysis
of Graph RAG and its application to legal norm datasets, this article aims to
significantly advance the field of Artificial Intelligence applied to Law,
creating opportunities for more effective systems in legal research,
legislative analysis, and decision support.

</details>

### [107] [Base Models Beat Aligned Models at Randomness and Creativity](https://arxiv.org/abs/2505.00047)
*Peter West,Christopher Potts*

Main category: cs.CL

TLDR: 对齐技术（如RLHF）在LLM开发中广泛应用，但研究发现基础语言模型在某些任务（如随机数生成、混合策略游戏和创意写作）上表现优于对齐模型。


<details>
  <summary>Details</summary>
Motivation: 探讨对齐技术是否应普遍应用，揭示其在某些任务上的局限性。

Method: 研究基础模型和对齐模型在随机数生成、混合策略游戏和创意写作任务上的表现差异。

Result: 对齐模型在需要不可预测输出的任务中表现较差，倾向于产生狭窄行为。

Conclusion: 对齐技术并非适用于所有任务，需权衡其在不同场景下的优缺点。

Abstract: Alignment has quickly become a default ingredient in LLM development, with
techniques such as reinforcement learning from human feedback making models act
safely, follow instructions, and perform ever-better on complex tasks. While
these techniques are certainly useful, we propose that they should not be
universally applied and demonstrate a range of tasks on which base language
models consistently outperform their popular aligned forms. Particularly, we
study tasks that require unpredictable outputs, such as random number
generation, mixed strategy games (rock-paper-scissors and hide-and-seek), and
creative writing. In each case, aligned models tend towards narrow behaviors
that result in distinct disadvantages, for instance, preferring to generate "7"
over other uniformly random numbers, becoming almost fully predictable in some
game states, or prioritizing pleasant writing over creative originality. Across
models tested, better performance on common benchmarks tends to correlate with
worse performance on our tasks, suggesting an effective trade-off in the
required capabilities.

</details>

### [108] [Emotional Analysis of Fashion Trends Using Social Media and AI: Sentiment Analysis on Twitter for Fashion Trend Forecasting](https://arxiv.org/abs/2505.00050)
*Aayam Bansal,Agneya Tharun*

Main category: cs.CL

TLDR: 通过分析Twitter数据，研究社交媒体情感与时尚趋势的关系，发现情感模式可作为时尚趋势的预测指标。


<details>
  <summary>Details</summary>
Motivation: 探索社交媒体情感分析在预测时尚趋势中的应用价值。

Method: 使用自然语言处理和机器学习技术，包括情感分类、时间序列分解、因果建模和跨平台比较。

Result: 发现情感模式与时尚主题流行度相关，配饰和街头服饰趋势显著；预测模型准确率达78.35%。

Conclusion: 社交媒体情感分析可作为时尚趋势的早期有效指标，需结合统计验证。

Abstract: This study explores the intersection of fashion trends and social media
sentiment through computational analysis of Twitter data using the T4SA
(Twitter for Sentiment Analysis) dataset. By applying natural language
processing and machine learning techniques, we examine how sentiment patterns
in fashion-related social media conversations can serve as predictors for
emerging fashion trends. Our analysis involves the identification and
categorization of fashion-related content, sentiment classification with
improved normalization techniques, time series decomposition, statistically
validated causal relationship modeling, cross-platform sentiment comparison,
and brand-specific sentiment analysis. Results indicate correlations between
sentiment patterns and fashion theme popularity, with accessories and
streetwear themes showing statistically significant rising trends. The Granger
causality analysis establishes sustainability and streetwear as primary trend
drivers, showing bidirectional relationships with several other themes. The
findings demonstrate that social media sentiment analysis can serve as an
effective early indicator of fashion trend trajectories when proper statistical
validation is applied. Our improved predictive model achieved 78.35% balanced
accuracy in sentiment classification, establishing a reliable foundation for
trend prediction across positive, neutral, and negative sentiment categories.

</details>

### [109] [Clustering Internet Memes Through Template Matching and Multi-Dimensional Similarity](https://arxiv.org/abs/2505.00056)
*Tygo Bloem,Filip Ilievski*

Main category: cs.CL

TLDR: 提出了一种基于模板匹配的多维相似性特征方法，用于聚类互联网模因，无需预定义数据库，支持自适应匹配。


<details>
  <summary>Details</summary>
Motivation: 模因聚类对毒性检测、传播建模和分类至关重要，但现有方法依赖数据库且忽略语义，难以处理多维度相似性。

Method: 采用模板匹配结合多维相似性特征（形式、视觉内容、文本和身份），通过局部和全局特征聚类模因。

Result: 该方法优于现有聚类方法，生成更一致和连贯的聚类，且特征集支持自适应并与人类直觉一致。

Conclusion: 提出的方法解决了模因聚类的挑战，代码已公开以支持后续研究。

Abstract: Meme clustering is critical for toxicity detection, virality modeling, and
typing, but it has received little attention in previous research. Clustering
similar Internet memes is challenging due to their multimodality, cultural
context, and adaptability. Existing approaches rely on databases, overlook
semantics, and struggle to handle diverse dimensions of similarity. This paper
introduces a novel method that uses template-based matching with
multi-dimensional similarity features, thus eliminating the need for predefined
databases and supporting adaptive matching. Memes are clustered using local and
global features across similarity categories such as form, visual content,
text, and identity. Our combined approach outperforms existing clustering
methods, producing more consistent and coherent clusters, while
similarity-based feature sets enable adaptability and align with human
intuition. We make all supporting code publicly available to support subsequent
research. Code: https://github.com/tygobl/meme-clustering

</details>

### [110] [A Report on the llms evaluating the high school questions](https://arxiv.org/abs/2505.00057)
*Zhu Jiawei,Chen Wei*

Main category: cs.CL

TLDR: 评估大语言模型（LLMs）在解答高中科学问题中的表现，探讨其在教育领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在自然语言处理领域的快速发展，其在教育中的应用受到广泛关注。

Method: 选取2019-2023年高考数学试题作为评估数据，使用至少8种LLM API提供答案，基于准确性、响应时间、逻辑推理和创造力等指标进行综合评估。

Result: LLMs在某些方面表现优异，但在逻辑推理和创造性问题解决方面仍有改进空间。

Conclusion: 为LLMs在教育领域的进一步研究和应用提供了实证基础，并提出了改进建议。

Abstract: This report aims to evaluate the performance of large language models (LLMs)
in solving high school science questions and to explore their potential
applications in the educational field. With the rapid development of LLMs in
the field of natural language processing, their application in education has
attracted widespread attention. This study selected mathematics exam questions
from the college entrance examinations (2019-2023) as evaluation data and
utilized at least eight LLM APIs to provide answers. A comprehensive assessment
was conducted based on metrics such as accuracy, response time, logical
reasoning, and creativity. Through an in-depth analysis of the evaluation
results, this report reveals the strengths and weaknesses of LLMs in handling
high school science questions and discusses their implications for educational
practice. The findings indicate that although LLMs perform excellently in
certain aspects, there is still room for improvement in logical reasoning and
creative problem-solving. This report provides an empirical foundation for
further research and application of LLMs in the educational field and offers
suggestions for improvement.

</details>

### [111] [BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition](https://arxiv.org/abs/2505.00059)
*Paige Tuttösí,Mantaj Dhillon,Luna Sang,Shane Eastwood,Poorvi Bhatia,Quang Minh Dinh,Avni Kapoor,Yewon Jin,Angelica Lim*

Main category: cs.CL

TLDR: 论文介绍了BERSt数据集，用于评估复杂场景下的语音识别任务，如ASR和SER，发现距离和喊叫水平会影响性能。


<details>
  <summary>Details</summary>
Motivation: 当前语音识别系统在复杂场景（如远距离语音）中表现不佳，现有数据集多关注多麦克风阵列系统，缺乏多样性。

Method: 收集了98位演员在不同家庭环境中的语音数据，包含多种口音、情感提示和喊叫/说话内容，手机放置位置多样。

Result: ASR性能随距离和喊叫水平下降，情感识别表现不一，数据集对ASR和SER任务具有挑战性。

Conclusion: BERSt数据集可用于评估语音识别任务，显示现有系统需进一步改进以适应真实场景。

Abstract: Some speech recognition tasks, such as automatic speech recognition (ASR),
are approaching or have reached human performance in many reported metrics.
Yet, they continue to struggle in complex, real-world, situations, such as with
distanced speech. Previous challenges have released datasets to address the
issue of distanced ASR, however, the focus remains primarily on distance,
specifically relying on multi-microphone array systems. Here we present the
B(asic) E(motion) R(andom phrase) S(hou)t(s) (BERSt) dataset. The dataset
contains almost 4 hours of English speech from 98 actors with varying regional
and non-native accents. The data was collected on smartphones in the actors
homes and therefore includes at least 98 different acoustic environments. The
data also includes 7 different emotion prompts and both shouted and spoken
utterances. The smartphones were places in 19 different positions, including
obstructions and being in a different room than the actor. This data is
publicly available for use and can be used to evaluate a variety of speech
recognition tasks, including: ASR, shout detection, and speech emotion
recognition (SER). We provide initial benchmarks for ASR and SER tasks, and
find that ASR degrades both with an increase in distance and shout level and
shows varied performance depending on the intended emotion. Our results show
that the BERSt dataset is challenging for both ASR and SER tasks and continued
work is needed to improve the robustness of such systems for more accurate
real-world use.

</details>

### [112] [Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence Using Exaone 3.5](https://arxiv.org/abs/2505.00060)
*Jeho Choi*

Main category: cs.CL

TLDR: 该研究提出了一个事实一致性评估框架，用于评估LLM生成的SQL在商业智能环境中的语义准确性，发现Exaone 3.5在简单任务中表现良好，但在复杂任务中表现不佳。


<details>
  <summary>Details</summary>
Motivation: LLM在商业智能（BI）中的应用受限于语义幻觉、结构错误和缺乏领域特定评估框架，因此需要一种评估方法来提升其可靠性。

Method: 研究构建了一个包含219个自然语言商业问题的领域特定基准，使用Exaone 3.5评估其生成的SQL的语义准确性，并通过多种指标（如答案准确率、执行成功率等）进行分析。

Result: Exaone 3.5在简单聚合任务中表现优异（L1准确率93%），但在算术推理（H1准确率4%）和分组排名任务（H4准确率31%）中表现较差，且语义错误和非响应集中在复杂任务中。

Conclusion: 研究揭示了LLM在商业关键环境中的局限性，强调了事实一致性验证层和混合推理方法的必要性，并提供了一个可复现的评估框架。

Abstract: Large Language Models (LLMs) have shown promise in enabling natural language
interfaces for structured data querying through text-to-SQL generation.
However, their application in real-world Business Intelligence (BI) contexts
remains limited due to semantic hallucinations, structural errors, and a lack
of domain-specific evaluation frameworks. In this study, we propose a
Fact-Consistency Evaluation Framework for assessing the semantic accuracy of
LLM-generated SQL outputs using Exaone 3.5--an instruction-tuned, bilingual LLM
optimized for enterprise tasks. We construct a domain-specific benchmark
comprising 219 natural language business questions across five SQL complexity
levels, derived from actual sales data in LG Electronics' internal BigQuery
environment. Each question is paired with a gold-standard SQL query and a
validated ground-truth answer. We evaluate model performance using answer
accuracy, execution success rate, semantic error rate, and non-response rate.
Experimental results show that while Exaone 3.5 performs well on simple
aggregation tasks (93% accuracy in L1), it exhibits substantial degradation in
arithmetic reasoning (4% accuracy in H1) and grouped ranking tasks (31% in H4),
with semantic errors and non-responses concentrated in complex cases.
Qualitative error analysis further identifies common failure types such as
misapplied arithmetic logic, incomplete filtering, and incorrect grouping
operations. Our findings highlight the current limitations of LLMs in
business-critical environments and underscore the need for fact-consistency
validation layers and hybrid reasoning approaches. This work contributes a
reproducible benchmark and evaluation methodology for advancing reliable
natural language interfaces to structured enterprise data systems.

</details>

### [113] [GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling](https://arxiv.org/abs/2505.00063)
*Siqi Li,Yufan Shen,Xiangnan Chen,Jiayi Chen,Hengwei Ju,Haodong Duan,Song Mao,Hongbin Zhou,Bo Zhang,Pinlong Cai,Licheng Wen,Botian Shi,Yong Liu,Xinyu Cai,Yu Qiao*

Main category: cs.CL

TLDR: GDI-Bench是一个新的多模态大语言模型（MLLMs）评估基准，包含1.9k张图像和19个文档任务，旨在通过分级任务识别模型弱点并指导优化。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法系统识别模型弱点或指导改进，因此需要更全面的评估工具。

Method: GDI-Bench通过解耦视觉和推理复杂度设计分级任务，并提出了GDI模型以避免监督微调中的灾难性遗忘。

Result: GPT-4o在推理任务中表现优异，但视觉能力有限；GDI模型在多个基准上达到最优性能。

Conclusion: GDI-Bench和GDI模型为MLLMs的评估和优化提供了新工具，并将开源。

Abstract: The rapid advancement of multimodal large language models (MLLMs) has
profoundly impacted the document domain, creating a wide array of application
scenarios. This progress highlights the need for a comprehensive benchmark to
evaluate these models' capabilities across various document-specific tasks.
However, existing benchmarks often fail to locate specific model weaknesses or
guide systematic improvements. To bridge this gap, we introduce a General
Document Intelligence Benchmark (GDI-Bench), featuring 1.9k images across 9 key
scenarios and 19 document-specific tasks. By decoupling visual complexity and
reasoning complexity, the GDI-Bench structures graded tasks that allow
performance assessment by difficulty, aiding in model weakness identification
and optimization guidance. We evaluate the GDI-Bench on various open-source and
closed-source models, conducting decoupled analyses in the visual and reasoning
domains. For instance, the GPT-4o model excels in reasoning tasks but exhibits
limitations in visual capabilities. To address the diverse tasks and domains in
the GDI-Bench, we propose a GDI Model that mitigates the issue of catastrophic
forgetting during the supervised fine-tuning (SFT) process through a
intelligence-preserving training strategy. Our model achieves state-of-the-art
performance on previous benchmarks and the GDI-Bench. Both our benchmark and
model will be open source.

</details>

### [114] [Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese](https://arxiv.org/abs/2505.00114)
*Silvana Yakhni,Ali Chehab*

Main category: cs.CL

TLDR: 论文研究了大型语言模型（LLM）在翻译低资源黎巴嫩方言中的效果，发现文化真实性数据比大规模翻译数据集更有效。对比三种微调方法，结果显示基于文化感知的小数据集表现更优。


<details>
  <summary>Details</summary>
Motivation: 探讨文化真实性数据在低资源方言翻译中的作用，挑战“数据越多越好”的范式。

Method: 使用开源Aya23模型，比较基本、对比和语法提示三种微调方法，并引入LebEval新基准。

Result: 文化感知的小数据集（LW）表现优于大规模非本地数据，对比微调结合对比提示效果最佳。

Conclusion: 文化真实性在方言翻译中至关重要，研究结果支持小规模高质量数据的有效性。

Abstract: This paper examines the effectiveness of Large Language Models (LLMs) in
translating the low-resource Lebanese dialect, focusing on the impact of
culturally authentic data versus larger translated datasets. We compare three
fine-tuning approaches: Basic, contrastive, and grammar-hint tuning, using
open-source Aya23 models. Experiments reveal that models fine-tuned on a
smaller but culturally aware Lebanese dataset (LW) consistently outperform
those trained on larger, non-native data. The best results were achieved
through contrastive fine-tuning paired with contrastive prompting, which
indicates the benefits of exposing translation models to bad examples. In
addition, to ensure authentic evaluation, we introduce LebEval, a new benchmark
derived from native Lebanese content, and compare it to the existing FLoRes
benchmark. Our findings challenge the "More Data is Better" paradigm and
emphasize the crucial role of cultural authenticity in dialectal translation.
We made our datasets and code available on Github.

</details>

### [115] [ConSens: Assessing context grounding in open-book question answering](https://arxiv.org/abs/2505.00065)
*Ivan Vankov,Matyo Ivanov,Adriana Correia,Victor Botev*

Main category: cs.CL

TLDR: 提出了一种新指标，通过对比模型在有上下文和无上下文时的困惑度，量化答案对上下文的依赖程度，解决了现有评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决开放书问答中模型答案依赖上下文而非参数知识的挑战，现有评估方法存在偏见、可扩展性差和成本高的问题。

Method: 提出一种新指标，对比模型在有上下文和无上下文时的困惑度，生成依赖分数。

Result: 实验证明该指标能有效识别答案是否基于上下文，且计算高效、可解释性强。

Conclusion: 该指标为开放书问答系统提供了一种可扩展且实用的上下文利用评估方法。

Abstract: Large Language Models (LLMs) have demonstrated considerable success in
open-book question answering (QA), where the task requires generating answers
grounded in a provided external context. A critical challenge in open-book QA
is to ensure that model responses are based on the provided context rather than
its parametric knowledge, which can be outdated, incomplete, or incorrect.
Existing evaluation methods, primarily based on the LLM-as-a-judge approach,
face significant limitations, including biases, scalability issues, and
dependence on costly external systems. To address these challenges, we propose
a novel metric that contrasts the perplexity of the model response under two
conditions: when the context is provided and when it is not. The resulting
score quantifies the extent to which the model's answer relies on the provided
context. The validity of this metric is demonstrated through a series of
experiments that show its effectiveness in identifying whether a given answer
is grounded in the provided context. Unlike existing approaches, this metric is
computationally efficient, interpretable, and adaptable to various use cases,
offering a scalable and practical solution to assess context utilization in
open-book QA systems.

</details>

### [116] [Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs](https://arxiv.org/abs/2505.00127)
*Jinyan Su,Jennifer Healey,Preslav Nakov,Claire Cardie*

Main category: cs.CL

TLDR: 研究发现，大型语言模型（LLMs）在推理长度与答案正确性之间存在矛盾：简单问题过度推理，复杂问题推理不足。通过偏好优化算法减少生成长度，可在保持准确性的同时显著缩短输出。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在推理长度与答案正确性之间的关系，揭示模型可能误判问题难度并无法校准响应长度的问题。

Method: 通过系统性实证研究分析推理长度与答案正确性的关系，并利用偏好优化算法减少生成长度。

Result: LLMs在简单问题上过度推理，复杂问题上推理不足；通过偏好优化算法可显著减少生成长度且保持准确性。

Conclusion: 生成长度是推理行为的重要信号，需进一步探索LLMs在推理长度自适应中的自我意识。

Abstract: Large language models (LLMs) are increasingly optimized for long reasoning,
under the assumption that more reasoning leads to better performance. However,
emerging evidence suggests that longer responses can sometimes degrade accuracy
rather than improve it. In this paper, we conduct a systematic empirical study
of the relationship between reasoning length and answer correctness. We find
that LLMs tend to overthink simple problems, generating unnecessarily long
outputs, and underthink harder ones, failing to extend their reasoning when it
is most needed. This indicates that models might misjudge problem difficulty
and fail to calibrate their response length appropriately. Furthermore, we
investigate the effects of length reduction with a preference optimization
algorithm when simply preferring the shorter responses regardless of answer
correctness. Experiments show that the generation length can be significantly
reduced while maintaining acceptable accuracy. Our findings highlight
generation length as a meaningful signal for reasoning behavior and motivate
further exploration into LLMs' self-awareness in reasoning length adaptation.

</details>

### [117] [AdaptMI: Adaptive Skill-based In-context Math Instruction for Small Language Models](https://arxiv.org/abs/2505.00147)
*Yinghui He,Abhishek Panigrahi,Yong Lin,Sanjeev Arora*

Main category: cs.CL

TLDR: 论文研究了小语言模型（SLMs）在上下文学习（ICL）中的性能差距，提出了一种自适应方法AdaptMI+，通过选择性引入技能示例提升SLMs的数学问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 研究发现技能提示策略在大型语言模型（LLMs）中有效，但在小语言模型（SLMs）中可能因信息过载而降低性能，因此需要一种自适应方法。

Method: 提出AdaptMI和AdaptMI+方法，基于认知负荷理论，仅在模型表现不佳时引入技能示例，并针对缺失技能补充示例。

Result: 在5-shot评估中，AdaptMI+在多个数学基准和SLMs上比传统技能策略提升了6%的准确率。

Conclusion: AdaptMI+通过自适应技能示例选择，有效提升了SLMs在上下文学习中的性能。

Abstract: In-context learning (ICL) allows a language model to improve its
problem-solving capability when provided with suitable information in context.
Since the choice of in-context information can be determined based on the
problem itself, in-context learning is analogous to human learning from
teachers in a classroom. Recent works (Didolkar et al., 2024a; 2024b) show that
ICL performance can be improved by leveraging a frontier large language model's
(LLM) ability to predict required skills to solve a problem, popularly referred
to as an LLM's metacognition, and using the recommended skills to construct
necessary in-context examples. While this skill-based strategy boosts ICL
performance in larger models, its gains on small language models (SLMs) have
been minimal, highlighting a performance gap in ICL capabilities. We
investigate this gap and show that skill-based prompting can hurt SLM
performance on easy questions by introducing unnecessary information, akin to
cognitive overload. To address this, we introduce AdaptMI, an adaptive approach
to selecting skill-based in-context Math Instructions for SLMs. Inspired by
cognitive load theory from human pedagogy, our method only introduces
skill-based examples when the model performs poorly. We further propose
AdaptMI+, which adds examples targeted to the specific skills missing from the
model's responses. On 5-shot evaluations across popular math benchmarks and
five SLMs (1B--7B; Qwen, Llama), AdaptMI+ improves accuracy by up to 6% over
naive skill-based strategies.

</details>

### [118] [IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports](https://arxiv.org/abs/2505.00191)
*Yuyan Ge,Kwan Ho Ryan Chan,Pablo Messina,René Vidal*

Main category: cs.CL

TLDR: 提出了一种可解释的AI框架，用于分类放射学报告，通过提取关键查询及其答案来预测诊断，从而提高临床信任和实用性。


<details>
  <summary>Details</summary>
Motivation: AI在医学诊断中的应用因缺乏可解释性而受限，阻碍了临床采用。

Method: 结合信息追踪框架提取关键查询，使用Flan-T5模型判断事实存在，并通过分类器预测疾病。

Result: 在MIMIC-CXR数据集上验证了方法的有效性，提升了AI的可信度和实用性。

Conclusion: 该框架为医学AI提供了可解释性设计，有望推动临床应用的广泛采纳。

Abstract: The development of AI-based methods for analyzing radiology reports could
lead to significant advances in medical diagnosis--from improving diagnostic
accuracy to enhancing efficiency and reducing workload. However, the lack of
interpretability in these methods has hindered their adoption in clinical
settings. In this paper, we propose an interpretable-by-design framework for
classifying radiology reports. The key idea is to extract a set of most
informative queries from a large set of reports and use these queries and their
corresponding answers to predict a diagnosis. Thus, the explanation for a
prediction is, by construction, the set of selected queries and answers. We use
the Information Pursuit framework to select informative queries, the Flan-T5
model to determine if facts are present in the report, and a classifier to
predict the disease. Experiments on the MIMIC-CXR dataset demonstrate the
effectiveness of the proposed method, highlighting its potential to enhance
trust and usability in medical AI.

</details>

### [119] [Enriching the Korean Learner Corpus with Multi-reference Annotations and Rubric-Based Scoring](https://arxiv.org/abs/2505.00261)
*Jayoung Song,KyungTae Lim,Jungyeul Park*

Main category: cs.CL

TLDR: 论文通过增强KoLLA韩语学习者语料库，添加多语法错误修正参考和评分标准，提升韩语L2写作研究的资源质量。


<details>
  <summary>Details</summary>
Motivation: 解决韩语L2写作学习者语料库不足的问题。

Method: 增加多语法错误修正参考，并基于韩国国立语言研究院标准添加评分。

Result: KoLLA成为更灵活、标准化的韩语L2教育资源。

Conclusion: 增强后的KoLLA支持韩语学习、评估和自动化错误修正的研究。

Abstract: Despite growing global interest in Korean language education, there remains a
significant lack of learner corpora tailored to Korean L2 writing. To address
this gap, we enhance the KoLLA Korean learner corpus by adding multiple
grammatical error correction (GEC) references, thereby enabling more nuanced
and flexible evaluation of GEC systems, and reflects the variability of human
language. Additionally, we enrich the corpus with rubric-based scores aligned
with guidelines from the Korean National Language Institute, capturing
grammatical accuracy, coherence, and lexical diversity. These enhancements make
KoLLA a robust and standardized resource for research in Korean L2 education,
supporting advancements in language learning, assessment, and automated error
correction.

</details>

### [120] [Consistency in Language Models: Current Landscape, Challenges, and Future Directions](https://arxiv.org/abs/2505.00268)
*Jekaterina Novikova,Carol Anderson,Borhane Blili-Hamelin,Subhabrata Majumdar*

Main category: cs.CL

TLDR: 论文探讨了AI语言系统的一致性研究现状，分析了形式与非形式一致性，并指出当前研究在定义标准化、多语言评估和改进方法上的不足。


<details>
  <summary>Details</summary>
Motivation: 人类语言使用具有一致性，而现有语言模型难以在不同场景中保持一致性，因此研究如何提升模型的一致性至关重要。

Method: 分析了当前一致性评估方法，包括形式一致性（如逻辑规则）和非形式一致性（如道德与事实连贯性），并识别研究空白。

Result: 发现现有研究在定义标准化、多语言评估和改进方法上存在不足，亟需建立鲁棒的评估基准。

Conclusion: 需采用跨学科方法提升语言模型在特定任务中的一致性，同时保持其实用性和适应性。

Abstract: The hallmark of effective language use lies in consistency -- expressing
similar meanings in similar contexts and avoiding contradictions. While human
communication naturally demonstrates this principle, state-of-the-art language
models struggle to maintain reliable consistency across different scenarios.
This paper examines the landscape of consistency research in AI language
systems, exploring both formal consistency (including logical rule adherence)
and informal consistency (such as moral and factual coherence). We analyze
current approaches to measure aspects of consistency, identify critical
research gaps in standardization of definitions, multilingual assessment, and
methods to improve consistency. Our findings point to an urgent need for robust
benchmarks to measure and interdisciplinary approaches to ensure consistency in
the application of language models on domain-specific tasks while preserving
the utility and adaptability.

</details>

### [121] [Enhancing AI-Driven Education: Integrating Cognitive Frameworks, Linguistic Feedback Analysis, and Ethical Considerations for Improved Content Generation](https://arxiv.org/abs/2505.00339)
*Antoun Yaacoub,Sansiri Tarnpradab,Phattara Khumprom,Zainab Assaghir,Lionel Prevost,Jérôme Da-Rugna*

Main category: cs.CL

TLDR: 本文提出一个综合框架，结合认知评估、语言分析和伦理设计，以优化AI教育工具的开发和应用。


<details>
  <summary>Details</summary>
Motivation: 利用AI提升教育的个性化和效率，同时关注其生成内容的质量、认知深度和伦理问题。

Method: 整合Bloom's Taxonomy、SOLO Taxonomy、语言分析和伦理原则，提出三阶段方法（认知对齐、语言反馈整合、伦理保障）。

Result: 框架成功应用于OneClickQuiz（Moodle插件），展示了其可行性和实用性。

Conclusion: 为教育者、研究者和开发者提供了兼顾AI潜力与教育伦理的实用指南。

Abstract: Artificial intelligence (AI) is rapidly transforming education, presenting
unprecedented opportunities for personalized learning and streamlined content
creation. However, realizing the full potential of AI in educational settings
necessitates careful consideration of the quality, cognitive depth, and ethical
implications of AI-generated materials. This paper synthesizes insights from
four related studies to propose a comprehensive framework for enhancing
AI-driven educational tools. We integrate cognitive assessment frameworks
(Bloom's Taxonomy and SOLO Taxonomy), linguistic analysis of AI-generated
feedback, and ethical design principles to guide the development of effective
and responsible AI tools. We outline a structured three-phase approach
encompassing cognitive alignment, linguistic feedback integration, and ethical
safeguards. The practical application of this framework is demonstrated through
its integration into OneClickQuiz, an AI-powered Moodle plugin for quiz
generation. This work contributes a comprehensive and actionable guide for
educators, researchers, and developers aiming to harness AI's potential while
upholding pedagogical and ethical standards in educational content generation.

</details>

### [122] [KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis](https://arxiv.org/abs/2505.00367)
*JunSeo Kim,HyeHyeon Kim*

Main category: cs.CL

TLDR: 该研究提出了首个针对韩国青少年认知扭曲的大规模数据集KoACD，并采用多LLM协商方法优化分类和生成合成数据。验证显示LLM在显性标记分类上表现良好，但在上下文推理上不如人类评估者。


<details>
  <summary>Details</summary>
Motivation: 现有研究多基于小规模成人数据集，缺乏对青少年认知扭曲的关注，因此需要大规模数据集和改进的分类方法。

Method: 使用多LLM协商方法，结合认知澄清和认知平衡两种策略，生成合成数据并优化分类。

Result: LLM在显性标记分类上表现良好，但在上下文推理任务中人类评估者更准确。

Conclusion: KoACD数据集和方法为未来认知扭曲检测研究提供了重要资源。

Abstract: Cognitive distortion refers to negative thinking patterns that can lead to
mental health issues like depression and anxiety in adolescents. Previous
studies using natural language processing (NLP) have focused mainly on
small-scale adult datasets, with limited research on adolescents. This study
introduces KoACD, the first large-scale dataset of cognitive distortions in
Korean adolescents, containing 108,717 instances. We applied a multi-Large
Language Model (LLM) negotiation method to refine distortion classification and
generate synthetic data using two approaches: cognitive clarification for
textual clarity and cognitive balancing for diverse distortion representation.
Validation through LLMs and expert evaluations showed that while LLMs
classified distortions with explicit markers, they struggled with
context-dependent reasoning, where human evaluators demonstrated higher
accuracy. KoACD aims to enhance future research on cognitive distortion
detection.

</details>

### [123] [CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass](https://arxiv.org/abs/2505.00389)
*Bowen Zhang,Zixin Song,Chunping Li*

Main category: cs.CL

TLDR: 提出了一种名为CSE-SFP的新方法，利用生成模型的结构特性进行无监督对比学习，显著提高了句子表示的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 当前无监督句子表示方法主要基于判别式预训练语言模型（如BERT），而生成式模型因其参数规模大而未被充分探索。鉴于生成式模型在学术界和工业界的广泛应用，亟需一种针对解码器专用模型的高效无监督文本表示框架。

Method: 提出CSE-SFP方法，通过单次前向传播实现无监督对比学习，利用生成模型的结构特性。

Result: 实验表明，CSE-SFP不仅生成更高质量的嵌入，还显著减少训练时间和内存消耗。此外，引入两种比率指标评估语义空间特性。

Conclusion: CSE-SFP为生成式模型的无监督句子表示提供了一种高效且高质量的解决方案，并通过新指标增强了评估能力。

Abstract: As a fundamental task in Information Retrieval and Computational Linguistics,
sentence representation has profound implications for a wide range of practical
applications such as text clustering, content analysis, question-answering
systems, and web search. Recent advances in pre-trained language models (PLMs)
have driven remarkable progress in this field, particularly through
unsupervised embedding derivation methods centered on discriminative PLMs like
BERT. However, due to time and computational constraints, few efforts have
attempted to integrate unsupervised sentence representation with generative
PLMs, which typically possess much larger parameter sizes. Given that
state-of-the-art models in both academia and industry are predominantly based
on generative architectures, there is a pressing need for an efficient
unsupervised text representation framework tailored to decoder-only PLMs. To
address this concern, we propose CSE-SFP, an innovative method that exploits
the structural characteristics of generative models. Compared to existing
strategies, CSE-SFP requires only a single forward pass to perform effective
unsupervised contrastive learning. Rigorous experimentation demonstrates that
CSE-SFP not only produces higher-quality embeddings but also significantly
reduces both training time and memory consumption. Furthermore, we introduce
two ratio metrics that jointly assess alignment and uniformity, thereby
providing a more robust means for evaluating the semantic spatial properties of
encoding models.

</details>

### [124] [Red Teaming Large Language Models for Healthcare](https://arxiv.org/abs/2505.00467)
*Vahid Balazadeh,Michael Cooper,David Pellow,Atousa Assadi,Jennifer Bell,Jim Fackler,Gabriel Funingana,Spencer Gable-Cook,Anirudh Gangadhar,Abhishek Jaiswal,Sumanth Kaja,Christopher Khoury,Randy Lin,Kaden McKeen,Sara Naimimohasses,Khashayar Namdar,Aviraj Newatia,Allan Pang,Anshul Pattoo,Sameer Peesapati,Diana Prepelita,Bogdana Rakova,Saba Sadatamin,Rafael Schulman,Ajay Shah,Syed Azhar Shah,Syed Ahmar Shah,Babak Taati,Balagopal Unnikrishnan,Stephanie Williams,Rahul G Krishnan*

Main category: cs.CL

TLDR: 论文介绍了2024年机器学习与医疗健康会议上关于大型语言模型（LLM）在医疗领域漏洞的预研讨会设计与发现。


<details>
  <summary>Details</summary>
Motivation: 通过临床专家与计算专家的合作，识别LLM在医疗场景中可能导致的临床危害，弥补开发者缺乏临床知识的不足。

Method: 研讨会参与者通过“红队”测试，设计真实临床提示以暴露LLM的漏洞，并进行分类和复现研究。

Result: 发现了LLM在医疗领域的潜在漏洞，并通过复现研究验证了这些漏洞的普遍性。

Conclusion: 临床专家的参与对识别LLM在医疗领域的漏洞至关重要，需进一步优化模型以减少临床风险。

Abstract: We present the design process and findings of the pre-conference workshop at
the Machine Learning for Healthcare Conference (2024) entitled Red Teaming
Large Language Models for Healthcare, which took place on August 15, 2024.
Conference participants, comprising a mix of computational and clinical
expertise, attempted to discover vulnerabilities -- realistic clinical prompts
for which a large language model (LLM) outputs a response that could cause
clinical harm. Red-teaming with clinicians enables the identification of LLM
vulnerabilities that may not be recognised by LLM developers lacking clinical
expertise. We report the vulnerabilities found, categorise them, and present
the results of a replication study assessing the vulnerabilities across all
LLMs provided.

</details>

### [125] [Computational Identification of Regulatory Statements in EU Legislation](https://arxiv.org/abs/2505.00479)
*Gijs Jan Brandsma,Jens Blom-Hansen,Christiaan Meijer,Kody Moodley*

Main category: cs.CL

TLDR: 论文提出两种方法（依赖解析和基于Transformer的机器学习模型）来自动识别欧盟法律中的监管语句，两种方法表现相似（准确率80%和84%），结合两者可能更优。


<details>
  <summary>Details</summary>
Motivation: 识别法律中的监管语句有助于衡量法规密度和严格性，但现有方法定义不一，需更具体的方法处理日益增长的欧盟法律。

Method: 基于机构语法工具定义监管语句，开发并比较依赖解析和Transformer模型两种方法。

Result: 两种方法准确率分别为80%和84%，K alpha为0.58，表现相似但一致性不高。

Conclusion: 两种方法各有优势，结合使用可能提升效果。

Abstract: Identifying regulatory statements in legislation is useful for developing
metrics to measure the regulatory density and strictness of legislation. A
computational method is valuable for scaling the identification of such
statements from a growing body of EU legislation, constituting approximately
180,000 published legal acts between 1952 and 2023. Past work on extraction of
these statements varies in the permissiveness of their definitions for what
constitutes a regulatory statement. In this work, we provide a specific
definition for our purposes based on the institutional grammar tool. We develop
and compare two contrasting approaches for automatically identifying such
statements in EU legislation, one based on dependency parsing, and the other on
a transformer-based machine learning model. We found both approaches performed
similarly well with accuracies of 80% and 84% respectively and a K alpha of
0.58. The high accuracies and not exceedingly high agreement suggests potential
for combining strengths of both approaches.

</details>

### [126] [HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection](https://arxiv.org/abs/2505.00506)
*Deanna Emery,Michael Goitia,Freddie Vargus,Iulia Neagu*

Main category: cs.CL

TLDR: 论文介绍了HalluMix Benchmark，一个多样化的任务无关数据集，用于检测大语言模型中的幻觉内容，并评估了七种检测系统，发现性能差异显著。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在高风险领域的部署，检测幻觉内容（无支持证据的文本）成为关键挑战，现有基准存在局限性。

Method: 提出HalluMix Benchmark，涵盖多领域和多格式数据，评估七种幻觉检测系统在不同任务和上下文长度中的表现。

Result: Quotient Detections表现最佳，准确率为0.82，F1分数为0.84；长上下文与短上下文的性能差异显著。

Conclusion: HalluMix Benchmark为幻觉检测提供了更全面的评估工具，揭示了现有系统在复杂场景中的局限性。

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes
domains, detecting hallucinated content$\unicode{x2013}$text that is not
grounded in supporting evidence$\unicode{x2013}$has become a critical
challenge. Existing benchmarks for hallucination detection are often
synthetically generated, narrowly focused on extractive question answering, and
fail to capture the complexity of real-world scenarios involving multi-document
contexts and full-sentence outputs. We introduce the HalluMix Benchmark, a
diverse, task-agnostic dataset that includes examples from a range of domains
and formats. Using this benchmark, we evaluate seven hallucination detection
systems$\unicode{x2013}$both open and closed
source$\unicode{x2013}$highlighting differences in performance across tasks,
document lengths, and input representations. Our analysis highlights
substantial performance disparities between short and long contexts, with
critical implications for real-world Retrieval Augmented Generation (RAG)
implementations. Quotient Detections achieves the best overall performance,
with an accuracy of 0.82 and an F1 score of 0.84.

</details>

### [127] [100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models](https://arxiv.org/abs/2505.00551)
*Chong Zhang,Yue Deng,Xiang Lin,Bin Wang,Dianwen Ng,Hai Ye,Xingxuan Li,Yao Xiao,Zhanfeng Mo,Qi Zhang,Lidong Bing*

Main category: cs.CL

TLDR: 本文总结了近期关于推理语言模型（RLMs）的复现研究，重点关注监督微调（SFT）和基于可验证奖励的强化学习（RLVR），旨在为未来研究提供灵感和参考。


<details>
  <summary>Details</summary>
Motivation: DeepSeek-R1等模型的成功激发了研究社区的兴趣，但其实现细节未完全开源，导致大量复现研究涌现。本文旨在总结这些研究，为未来RLMs的发展提供指导。

Method: 通过分析复现研究的数据构建、方法设计和训练流程，总结了SFT和RLVR的关键实现细节和实验结果。

Result: 复现研究展示了通过类似训练流程和开源数据资源可以达到与DeepSeek-R1相当的性能，并提出了多种有价值的见解。

Conclusion: 本文为RLMs的研究者和开发者提供了最新进展的总结，并探讨了未来增强RLMs的潜在技术和挑战。

Abstract: The recent development of reasoning language models (RLMs) represents a novel
evolution in large language models. In particular, the recent release of
DeepSeek-R1 has generated widespread social impact and sparked enthusiasm in
the research community for exploring the explicit reasoning paradigm of
language models. However, the implementation details of the released models
have not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero,
DeepSeek-R1, and the distilled small models. As a result, many replication
studies have emerged aiming to reproduce the strong performance achieved by
DeepSeek-R1, reaching comparable performance through similar training
procedures and fully open-source data resources. These works have investigated
feasible strategies for supervised fine-tuning (SFT) and reinforcement learning
from verifiable rewards (RLVR), focusing on data preparation and method design,
yielding various valuable insights. In this report, we provide a summary of
recent replication studies to inspire future research. We primarily focus on
SFT and RLVR as two main directions, introducing the details for data
construction, method design and training procedure of current replication
studies. Moreover, we conclude key findings from the implementation details and
experimental results reported by these studies, anticipating to inspire future
research. We also discuss additional techniques of enhancing RLMs, highlighting
the potential of expanding the application scope of these models, and
discussing the challenges in development. By this survey, we aim to help
researchers and developers of RLMs stay updated with the latest advancements,
and seek to inspire new ideas to further enhance RLMs.

</details>

### [128] [Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models](https://arxiv.org/abs/2505.00557)
*Makoto Sato*

Main category: cs.CL

TLDR: 论文提出了一种基于提示的框架（HIP和HQP），用于系统性地触发和量化大语言模型（LLM）中的幻觉现象，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: LLM在实际应用中（如医疗和法律）的幻觉问题日益突出，需要一种可重复的方法来研究和量化这种现象。

Method: 设计了两种提示：HIP（通过误导性融合远距离概念触发幻觉）和HQP（量化输出的合理性、置信度和连贯性）。

Result: 实验表明，HIP能显著降低输出的连贯性并增加幻觉，不同LLM的表现存在差异。

Conclusion: 该框架为研究LLM的幻觉脆弱性提供了可重复的测试平台，有助于开发更安全的模型。

Abstract: Hallucinations in large language models (LLMs) present a growing challenge
across real-world applications, from healthcare to law, where factual
reliability is essential. Despite advances in alignment and instruction tuning,
LLMs can still generate outputs that are fluent yet fundamentally untrue.
Understanding the cognitive dynamics that underlie these hallucinations remains
an open problem. In this study, we propose a prompt-based framework to
systematically trigger and quantify hallucination: a Hallucination-Inducing
Prompt (HIP), which synthetically fuses semantically distant concepts (e.g.,
periodic table of elements and tarot divination) in a misleading way, and a
Hallucination Quantifying Prompt (HQP), which scores the plausibility,
confidence, and coherence of the output. Controlled experiments across multiple
LLMs revealed that HIPs consistently produced less coherent and more
hallucinated responses than their null-fusion controls. These effects varied
across models, with reasoning-oriented LLMs showing distinct profiles from
general-purpose ones. Our framework provides a reproducible testbed for
studying hallucination vulnerability, and opens the door to developing safer,
more introspective LLMs that can detect and self-regulate the onset of
conceptual instability.

</details>

### [129] [FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension](https://arxiv.org/abs/2505.00570)
*Jushi Kai,Boyi Zeng,Yixuan Wang,Haoli Bai,Bo Jiang,Zhouhan Lin*

Main category: cs.CL

TLDR: 论文提出了一种名为FreqKV的新方法，通过频域压缩KV缓存来高效扩展LLMs的上下文窗口，解决了内存和计算复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 扩展LLMs的上下文窗口对生成长文本至关重要，但现有方法在长上下文扩展时存在性能下降问题，且KV缓存的内存需求和自注意力的二次复杂度是主要挑战。

Method: 利用KV缓存在频域中能量集中于低频分量的特性，提出FreqKV方法，通过过滤高频分量压缩KV缓存，无需额外参数或架构修改。

Result: 实验表明，FreqKV在长上下文语言建模和理解任务中高效且有效，能显著扩展上下文窗口。

Conclusion: FreqKV通过频域压缩KV缓存，为LLMs的上下文窗口扩展提供了一种高效解决方案。

Abstract: Extending the context window in large language models (LLMs) is essential for
applications involving long-form content generation. However, the linear
increase in key-value (KV) cache memory requirements and the quadratic
complexity of self-attention with respect to sequence length present
significant challenges during fine-tuning and inference. Existing methods
suffer from performance degradation when extending to longer contexts. In this
work, we introduce a novel context extension method that optimizes both
fine-tuning and inference efficiency. Our method exploits a key observation: in
the frequency domain, the energy distribution of the KV cache is primarily
concentrated in low-frequency components. By filtering out the high-frequency
components, the KV cache can be effectively compressed with minimal information
loss. Building on this insight, we propose an efficient compression technique,
FreqKV, that iteratively compresses the increasing KV cache to a fixed size in
the frequency domain, applicable to both fine-tuning and inference. FreqKV
introduces no additional parameters or architectural modifications. With
minimal fine-tuning, LLMs can learn to leverage the limited cache that is
compressed in the frequency domain and extend the context window efficiently.
Experiments on various long context language modeling and understanding tasks
demonstrate the efficiency and efficacy of the proposed method.

</details>

### [130] [Block Circulant Adapter for Large Language Models](https://arxiv.org/abs/2505.00582)
*Xinyu Ding,Meiqi Wang,Siyu Liao,Zhongfeng Wang*

Main category: cs.CL

TLDR: 提出了一种基于块循环矩阵的微调方法，显著降低了大型语言模型的微调成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的微调因模型规模庞大而困难，现有方法在降低成本和保持性能之间存在挑战。

Method: 利用循环矩阵和一维傅里叶变换的特性，结合稳定的训练启发式方法，减少存储和计算成本。

Result: 实验表明，该方法参数数量比VeRA少14倍，比LoRA小16倍，FLOPs比FourierFT少32倍，同时保持或提升任务性能。

Conclusion: 该方法为频域中微调大型模型提供了一种有前景的解决方案。

Abstract: Fine-tuning large language models (LLMs) is difficult due to their huge model
size. Recent Fourier domain-based methods show potential for reducing
fine-tuning costs. We propose a block circulant matrix-based fine-tuning method
with a stable training heuristic to leverage the properties of circulant
matrices and one-dimensional Fourier transforms to reduce storage and
computation costs. Experiments show that our method uses $14\times$ less number
of parameters than VeRA, $16\times$ smaller than LoRA and $32\times$ less FLOPs
than FourierFT, while maintaining close or better task performance. Our
approach presents a promising way in frequency domain to fine-tune large models
on downstream tasks.

</details>

### [131] [FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation](https://arxiv.org/abs/2505.00624)
*Chaitali Bhattacharyya,Yeseong Kim*

Main category: cs.CL

TLDR: FineScope是一个从大型预训练模型中提取紧凑、领域优化LLM的框架，通过稀疏自编码器和结构化剪枝，结合自数据蒸馏，显著提升领域特定任务的性能。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型需要大量计算资源，因此需要开发更小、领域特定的模型以保持效率和性能。

Method: 利用稀疏自编码器提取领域特定子集，结合结构化剪枝和自数据蒸馏，恢复剪枝过程中丢失的关键信息。

Result: FineScope在领域特定任务中表现优异，甚至超越了一些大规模先进LLM，并能通过SAE整理的数据集恢复模型性能。

Conclusion: FineScope提供了一种高效且鲁棒的方法，用于生成领域优化的紧凑模型，同时展示了其数据集的广泛适用性。

Abstract: Training large language models (LLMs) from scratch requires significant
computational resources, driving interest in developing smaller,
domain-specific LLMs that maintain both efficiency and strong task performance.
Medium-sized models such as LLaMA, llama} have served as starting points for
domain-specific adaptation, but they often suffer from accuracy degradation
when tested on specialized datasets. We introduce FineScope, a framework for
deriving compact, domain-optimized LLMs from larger pretrained models.
FineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its
ability to produce interpretable feature representations, to extract
domain-specific subsets from large datasets. We apply structured pruning with
domain-specific constraints, ensuring that the resulting pruned models retain
essential knowledge for the target domain. To further enhance performance,
these pruned models undergo self-data distillation, leveraging SAE-curated
datasets to restore key domain-specific information lost during pruning.
Extensive experiments and ablation studies demonstrate that FineScope achieves
highly competitive performance, outperforming several large-scale
state-of-the-art LLMs in domain-specific tasks. Additionally, our results show
that FineScope enables pruned models to regain a substantial portion of their
original performance when fine-tuned with SAE-curated datasets. Furthermore,
applying these datasets to fine-tune pretrained LLMs without pruning also
improves their domain-specific accuracy, highlighting the robustness of our
approach. The code will be released.

</details>

### [132] [The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)](https://arxiv.org/abs/2505.00626)
*Zihao Wang,Yibo Jiang,Jiahao Yu,Heqing Huang*

Main category: cs.CL

TLDR: 研究探讨了如何通过调整输入编码中的标记信号，帮助大型语言模型更可靠地区分不同角色的输入，而非依赖表面代理。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型在多角色输入中准确区分角色（角色分离）对行为一致性至关重要，但现有方法可能仅记忆已知触发点而非真正区分角色。

Method: 提出通过调整输入编码中的标记信号（如位置ID）来强化角色边界的不变信号，并通过简单实验框架验证其有效性。

Result: 发现微调模型依赖任务类型和文本起始位置等表面代理，而调整位置ID能帮助模型更清晰地区分角色。

Conclusion: 通过机制中心视角，研究为大型语言模型在多角色行为中提供更可靠的区分方法，避免仅记忆已知提示。

Abstract: Large language models (LLMs) that integrate multiple input roles (e.g.,
system instructions, user queries, external tool outputs) are increasingly
prevalent in practice. Ensuring that the model accurately distinguishes
messages from each role -- a concept we call \emph{role separation} -- is
crucial for consistent multi-role behavior. Although recent work often targets
state-of-the-art prompt injection defenses, it remains unclear whether such
methods truly teach LLMs to differentiate roles or merely memorize known
triggers. In this paper, we examine \emph{role-separation learning}: the
process of teaching LLMs to robustly distinguish system and user tokens.
Through a \emph{simple, controlled experimental framework}, we find that
fine-tuned models often rely on two proxies for role identification: (1) task
type exploitation, and (2) proximity to begin-of-text. Although data
augmentation can partially mitigate these shortcuts, it generally leads to
iterative patching rather than a deeper fix. To address this, we propose
reinforcing \emph{invariant signals} that mark role boundaries by adjusting
token-wise cues in the model's input encoding. In particular, manipulating
position IDs helps the model learn clearer distinctions and reduces reliance on
superficial proxies. By focusing on this mechanism-centered perspective, our
work illuminates how LLMs can more reliably maintain consistent multi-role
behavior without merely memorizing known prompts or triggers.

</details>

### [133] [Large Language Models Understanding: an Inherent Ambiguity Barrier](https://arxiv.org/abs/2505.00654)
*Daniel N. Nissani*

Main category: cs.CL

TLDR: 论文通过思想实验和半正式论证，提出LLMs存在固有歧义障碍，无法真正理解对话意义。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否真正理解对话意义，回应相关争论。

Method: 采用思想实验和半正式论证分析LLMs的理解能力。

Result: 发现LLMs存在固有歧义障碍，无法真正理解对话。

Conclusion: LLMs的流畅对话不代表其具备真正的理解能力。

Abstract: A lively ongoing debate is taking place, since the extraordinary emergence of
Large Language Models (LLMs) with regards to their capability to understand the
world and capture the meaning of the dialogues in which they are involved.
Arguments and counter-arguments have been proposed based upon thought
experiments, anecdotal conversations between LLMs and humans, statistical
linguistic analysis, philosophical considerations, and more. In this brief
paper we present a counter-argument based upon a thought experiment and
semi-formal considerations leading to an inherent ambiguity barrier which
prevents LLMs from having any understanding of what their amazingly fluent
dialogues mean.

</details>

### [134] [On the generalization of language models from in-context learning and finetuning: a controlled study](https://arxiv.org/abs/2505.00661)
*Andrew K. Lampinen,Arslan Chaudhry,Stephanie C. Y. Chan,Cody Wild,Diane Wan,Alex Ku,Jörg Bornschein,Razvan Pascanu,Murray Shanahan,James L. McClelland*

Main category: cs.CL

TLDR: 论文探讨了大型语言模型在微调和上下文学习中的泛化能力差异，发现上下文学习在某些情况下表现更好，并提出了一种结合上下文推理的微调方法来提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在微调和上下文学习中的泛化差异，以解决模型在实际应用中的泛化不足问题。

Method: 构建新数据集，对比微调和上下文学习的泛化表现，并提出结合上下文推理的微调方法。

Result: 上下文学习在数据匹配情况下泛化更灵活，结合上下文推理的微调方法显著提升了泛化能力。

Conclusion: 研究揭示了不同学习模式的归纳偏差，并提出了一种实用的方法以提升语言模型的泛化性能。

Abstract: Large language models exhibit exciting capabilities, yet can show
surprisingly narrow generalization from finetuning -- from failing to
generalize to simple reversals of relations they are trained on, to missing
logical deductions that can be made from trained information. These failures to
generalize from fine-tuning can hinder practical application of these models.
However, language models' in-context learning shows different inductive biases,
and can generalize better in some of these cases. Here, we explore these
differences in generalization between in-context- and fine-tuning-based
learning. To do so, we constructed several novel datasets to evaluate and
improve models' ability to generalize from finetuning data. The datasets are
constructed to isolate the knowledge in the dataset from that in pretraining,
to create clean tests of generalization. We expose pretrained large models to
controlled subsets of the information in these datasets -- either in context,
or through fine-tuning -- and evaluate their performance on test sets that
require various types of generalization. We find overall that in data-matched
settings, in-context learning can generalize more flexibly than fine-tuning
(though we also find some qualifications of prior findings, such as cases when
fine-tuning can generalize to reversals embedded in a larger structure of
knowledge). We build on these findings to propose a method to enable improved
generalization from fine-tuning: adding in-context inferences to finetuning
data. We show that this method improves generalization across various splits of
our datasets and other benchmarks. Our results have implications for
understanding the inductive biases of different modes of learning in language
models, and practically improving their performance.

</details>

### [135] [DeepCritic: Deliberate Critique with Large Language Models](https://arxiv.org/abs/2505.00662)
*Wenkai Yang,Jingwen Chen,Yankai Lin,Ji-Rong Wen*

Main category: cs.CL

TLDR: 本文提出了一种两阶段框架，用于增强大语言模型（LLMs）的数学批判能力，通过生成详细的分步批判和强化学习，显著提升了批判模型的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，如何提供准确反馈和可扩展的监督成为紧迫问题。利用LLMs作为批判模型实现自动化监督是一个有前景的解决方案。

Method: 提出两阶段框架：1）利用Qwen2.5-72B-Instruct生成4.5K长形式批判数据用于监督微调；2）通过强化学习（使用PRM800K或自动标注数据）进一步优化模型。

Result: 开发的批判模型在多个错误识别基准上显著优于现有LLM批判模型（包括DeepSeek-R1-distill和GPT-4o），并能更有效地帮助生成模型修正错误。

Conclusion: 该方法显著提升了LLM批判模型的性能，为自动化监督提供了有效解决方案。

Abstract: As Large Language Models (LLMs) are rapidly evolving, providing accurate
feedback and scalable oversight on their outputs becomes an urgent and critical
problem. Leveraging LLMs as critique models to achieve automated supervision is
a promising solution. In this work, we focus on studying and enhancing the math
critique ability of LLMs. Current LLM critics provide critiques that are too
shallow and superficial on each step, leading to low judgment accuracy and
struggling to offer sufficient feedback for the LLM generator to correct
mistakes. To tackle this issue, we propose a novel and effective two-stage
framework to develop LLM critics that are capable of deliberately critiquing on
each reasoning step of math solutions. In the first stage, we utilize
Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for
supervised fine-tuning. Each seed critique consists of deliberate step-wise
critiques that includes multi-perspective verifications as well as in-depth
critiques of initial critiques for each reasoning step. Then, we perform
reinforcement learning on the fine-tuned model with either existing
human-labeled data from PRM800K or our automatically annotated data obtained
via Monte Carlo sampling-based correctness estimation, to further incentivize
its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct
not only significantly outperforms existing LLM critics (including the
same-sized DeepSeek-R1-distill models and GPT-4o) on various error
identification benchmarks, but also more effectively helps the LLM generator
refine erroneous steps through more detailed feedback.

</details>

### [136] [Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions](https://arxiv.org/abs/2505.00675)
*Yiming Du,Wenyu Huang,Danna Zheng,Zhaowei Wang,Sebastien Montella,Mirella Lapata,Kam-Fai Wong,Jeff Z. Pan*

Main category: cs.CL

TLDR: 该论文综述了AI系统中记忆的原子操作和表示类型，提出了六种基本操作，并系统性地将其映射到相关研究领域，为未来研究提供了方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注记忆在大型语言模型（LLM）中的应用，但忽略了记忆动态的原子操作，本文旨在填补这一空白。

Method: 将记忆表示分为参数化、上下文结构化和上下文非结构化三类，并引入六种基本操作：巩固、更新、索引、遗忘、检索和压缩。

Result: 通过原子操作和表示类型的框架，系统性地梳理了记忆研究、基准数据集和工具，阐明了LLM中记忆的功能互动。

Conclusion: 本文为AI记忆研究提供了结构化视角，并指出了未来研究的潜在方向。

Abstract: Memory is a fundamental component of AI systems, underpinning large language
models (LLMs) based agents. While prior surveys have focused on memory
applications with LLMs, they often overlook the atomic operations that underlie
memory dynamics. In this survey, we first categorize memory representations
into parametric, contextual structured, and contextual unstructured and then
introduce six fundamental memory operations: Consolidation, Updating, Indexing,
Forgetting, Retrieval, and Compression. We systematically map these operations
to the most relevant research topics across long-term, long-context, parametric
modification, and multi-source memory. By reframing memory systems through the
lens of atomic operations and representation types, this survey provides a
structured and dynamic perspective on research, benchmark datasets, and tools
related to memory in AI, clarifying the functional interplay in LLMs based
agents while outlining promising directions for future research\footnote{The
paper list, datasets, methods and tools are available at
\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}.}.

</details>

### [137] [Steering Large Language Models with Register Analysis for Arbitrary Style Transfer](https://arxiv.org/abs/2505.00679)
*Xinchen Yang,Marine Carpuat*

Main category: cs.CL

TLDR: 提出一种基于注册分析的提示方法，指导大语言模型完成任意风格转换任务，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在文本风格改写方面表现出色，但如何利用其能力实现基于示例的任意风格转换仍是一个挑战。

Method: 提出基于注册分析的提示方法，指导模型进行风格转换。

Result: 实验表明，该方法在多个风格转换任务中提升了风格转换强度并更好地保留了原意。

Conclusion: 该方法为风格转换任务提供了一种有效的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
rewriting text across various styles. However, effectively leveraging this
ability for example-based arbitrary style transfer, where an input text is
rewritten to match the style of a given exemplar, remains an open challenge. A
key question is how to describe the style of the exemplar to guide LLMs toward
high-quality rewrites. In this work, we propose a prompting method based on
register analysis to guide LLMs to perform this task. Empirical evaluations
across multiple style transfer tasks show that our prompting approach enhances
style transfer strength while preserving meaning more effectively than existing
prompting strategies.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [138] [Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management](https://arxiv.org/abs/2505.00018)
*Ju Wu,Calvin K. L. Or*

Main category: cs.AI

TLDR: 本文提出了一种新的概念架构（Hierarchical Exploration-Exploitation Net），用于整合人机协作中的技术细节和理论框架，填补现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 当前人机协作研究缺乏统一的理论框架，尤其是在处理开放性和复杂任务时。

Method: 通过提出一种系统化的概念架构，整合多智能体协调、知识管理、控制论反馈循环和高级控制机制。

Result: 该框架为现有技术（如符号AI、连接主义LLM智能体）提供了整合和优化的路径，并启发了新的混合范式研究。

Conclusion: 本文为人类认知与AI能力的深度协同进化提供了理论基础和设计参考。

Abstract: This position paper critically surveys a broad spectrum of recent empirical
developments on human-AI agents collaboration, highlighting both their
technical achievements and persistent gaps. We observe a lack of a unifying
theoretical framework that can coherently integrate these varied studies,
especially when tackling open-ended, complex tasks. To address this, we propose
a novel conceptual architecture: one that systematically interlinks the
technical details of multi-agent coordination, knowledge management, cybernetic
feedback loops, and higher-level control mechanisms. By mapping existing
contributions, from symbolic AI techniques and connectionist LLM-based agents
to hybrid organizational practices, onto this proposed framework (Hierarchical
Exploration-Exploitation Net), our approach facilitates revision of legacy
methods and inspires new work that fuses qualitative and quantitative
paradigms. The paper's structure allows it to be read from any section, serving
equally as a critical review of technical implementations and as a
forward-looking reference for designing or extending human-AI symbioses.
Together, these insights offer a stepping stone toward deeper co-evolution of
human cognition and AI capability.

</details>

### [139] [First Order Logic with Fuzzy Semantics for Describing and Recognizing Nerves in Medical Images](https://arxiv.org/abs/2505.00173)
*Isabelle Bloch,Enzo Bonnot,Pietro Gori,Giammarco La Barbera,Sabine Sarnacki*

Main category: cs.AI

TLDR: 论文提出了一种基于模糊语义和一阶逻辑的神经纤维束描述与识别方法，用于医学图像中的神经分割和识别。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像中神经纤维束（如神经）的模糊描述问题，为外科手术规划提供支持。

Method: 结合模糊语义和一阶逻辑，定义空间实体、关系和量词的语言，提出空间推理算法。

Result: 在儿科盆腔神经影像中成功应用，实现了神经的分割和识别。

Conclusion: 该方法能有效处理神经的模糊描述，为手术规划提供可靠工具。

Abstract: This article deals with the description and recognition of fiber bundles, in
particular nerves, in medical images, based on the anatomical description of
the fiber trajectories. To this end, we propose a logical formalization of this
anatomical knowledge. The intrinsically imprecise description of nerves, as
found in anatomical textbooks, leads us to propose fuzzy semantics combined
with first-order logic. We define a language representing spatial entities,
relations between these entities and quantifiers. A formula in this language is
then a formalization of the natural language description. The semantics are
given by fuzzy representations in a concrete domain and satisfaction degrees of
relations. Based on this formalization, a spatial reasoning algorithm is
proposed for segmentation and recognition of nerves from anatomical and
diffusion magnetic resonance images, which is illustrated on pelvic nerves in
pediatric imaging, enabling surgeons to plan surgery.

</details>

### [140] [Real-World Gaps in AI Governance Research](https://arxiv.org/abs/2505.00174)
*Ilan Strauss,Isobel Moure,Tim O'Reilly,Sruly Rosenblat*

Main category: cs.AI

TLDR: 通过对1,178篇生成式AI安全与可靠性论文的分析，发现企业研究更集中于预部署领域，而部署阶段问题研究不足，建议加强部署数据的开放性和可观测性。


<details>
  <summary>Details</summary>
Motivation: 比较领先AI企业和大学的研究产出，揭示研究重点的差异及其潜在影响。

Method: 分析1,178篇生成式AI安全与可靠性论文（来自9,439篇论文，2020年1月至2025年3月），对比企业和大学的研究方向。

Result: 企业研究集中在模型对齐和测试评估等预部署领域，部署阶段问题（如模型偏见）研究减少；高风险领域存在显著研究空白。

Conclusion: 建议扩大外部研究者对部署数据的访问，并系统性观测市场中的AI行为，以弥补知识缺陷。

Abstract: Drawing on 1,178 safety and reliability papers from 9,439 generative AI
papers (January 2020 - March 2025), we compare research outputs of leading AI
companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI
universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of
Washington). We find that corporate AI research increasingly concentrates on
pre-deployment areas -- model alignment and testing & evaluation -- while
attention to deployment-stage issues such as model bias has waned. Significant
research gaps exist in high-risk deployment domains, including healthcare,
finance, misinformation, persuasive and addictive features, hallucinations, and
copyright. Without improved observability into deployed AI, growing corporate
concentration could deepen knowledge deficits. We recommend expanding external
researcher access to deployment data and systematic observability of in-market
AI behaviors.

</details>

### [141] [RAIL in the Wild: Operationalizing Responsible AI Evaluation Using Anthropic's Value Dataset](https://arxiv.org/abs/2505.00204)
*Sumit Verma,Pritam Prasun,Arpit Jaiswal,Pritish Kumar*

Main category: cs.AI

TLDR: 本文提出了一种基于RAIL框架的系统性方法，用于评估大型语言模型（LLMs）的伦理行为，并通过实际数据集验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 现有AI伦理框架缺乏可操作的评估方法，因此需要一种系统性的方法来衡量LLMs的伦理行为。

Method: 使用RAIL框架的八个可测量维度，结合Anthropic的“Values in the Wild”数据集（包含30.8万条匿名对话和3000条标注的价值表达），对LLMs的伦理行为进行评估。

Result: 通过将数据集中的价值映射到RAIL维度并计算合成分数，提供了LLMs在现实应用中的伦理行为洞察。

Conclusion: RAIL框架为评估LLMs的伦理行为提供了一种可行且系统的方法，填补了现有框架的不足。

Abstract: As AI systems become embedded in real-world applications, ensuring they meet
ethical standards is crucial. While existing AI ethics frameworks emphasize
fairness, transparency, and accountability, they often lack actionable
evaluation methods. This paper introduces a systematic approach using the
Responsible AI Labs (RAIL) framework, which includes eight measurable
dimensions to assess the normative behavior of large language models (LLMs). We
apply this framework to Anthropic's "Values in the Wild" dataset, containing
over 308,000 anonymized conversations with Claude and more than 3,000 annotated
value expressions. Our study maps these values to RAIL dimensions, computes
synthetic scores, and provides insights into the ethical behavior of LLMs in
real-world use.

</details>

### [142] [DeCo: Defect-Aware Modeling with Contrasting Matching for Optimizing Task Assignment in Online IC Testing](https://arxiv.org/abs/2505.00278)
*Lo Pang-Yun Ting,Yu-Hao Chiang,Yi-Tung Tsai,Hsu-Chao Lai,Kun-Ta Chuang*

Main category: cs.AI

TLDR: DeCo是一种基于AI的创新方法，通过构建缺陷感知图和对比分配机制，优化IC测试任务分配，提高处理成功率和均衡工作负载。


<details>
  <summary>Details</summary>
Motivation: 当前研究忽视缺陷特征、历史故障和工程师专业知识的整合，限制了IC处理的效率。

Method: DeCo构建缺陷感知图，结合工程师和任务的缺陷感知表示，并通过对比分配机制匹配任务与工程师。

Result: 实验显示DeCo在不同场景下任务处理成功率超过80%，并能均衡工作负载。

Conclusion: DeCo作为AI驱动解决方案，在IC故障分析和任务处理中具有实际应用潜力。

Abstract: In the semiconductor industry, integrated circuit (IC) processes play a vital
role, as the rising complexity and market expectations necessitate improvements
in yield. Identifying IC defects and assigning IC testing tasks to the right
engineers improves efficiency and reduces losses. While current studies
emphasize fault localization or defect classification, they overlook the
integration of defect characteristics, historical failures, and the insights
from engineer expertise, which restrains their effectiveness in improving IC
handling. To leverage AI for these challenges, we propose DeCo, an innovative
approach for optimizing task assignment in IC testing. DeCo constructs a novel
defect-aware graph from IC testing reports, capturing co-failure relationships
to enhance defect differentiation, even with scarce defect data. Additionally,
it formulates defect-aware representations for engineers and tasks, reinforced
by local and global structure modeling on the defect-aware graph. Finally, a
contrasting-based assignment mechanism pairs testing tasks with QA engineers by
considering their skill level and current workload, thus promoting an equitable
and efficient job dispatch. Experiments on a real-world dataset demonstrate
that DeCo achieves the highest task-handling success rates in different
scenarios, exceeding 80\%, while also maintaining balanced workloads on both
scarce or expanded defect data. Moreover, case studies reveal that DeCo can
assign tasks to potentially capable engineers, even for their unfamiliar
defects, highlighting its potential as an AI-driven solution for the real-world
IC failure analysis and task handling.

</details>

### [143] [CognitionNet: A Collaborative Neural Network for Play Style Discovery in Online Skill Gaming Platform](https://arxiv.org/abs/2505.00325)
*Rukma Talwadker,Surajit Chakrabarty,Aditya Pareek,Tridib Mukherjee,Deepak Saini*

Main category: cs.AI

TLDR: 论文提出了一种名为CognitionNet的两阶段深度神经网络，用于从在线技能游戏平台（如Rummy）的数据中挖掘玩家的游戏行为和风格，以揭示玩家心理和战术。


<details>
  <summary>Details</summary>
Motivation: 游戏数据可以反映玩家的心理状态和行为模式，通过挖掘这些数据可以更好地理解玩家的行为、体验和成长，从而提供更好的游戏体验和保护。

Method: 采用两阶段深度神经网络：第一阶段挖掘游戏行为作为潜在空间中的聚类表示，第二阶段通过监督分类目标聚合这些微模式以发现玩家的游戏风格。

Result: CognitionNet能够揭示玩家心理驱动的决策和战术，并在玩家参与预测方面显著优于现有基准。

Conclusion: 该研究首次实现了从遥测数据中自动化发现玩家心理和游戏战术，并提供了相关的诊断解释。

Abstract: Games are one of the safest source of realizing self-esteem and relaxation at
the same time. An online gaming platform typically has massive data coming in,
e.g., in-game actions, player moves, clickstreams, transactions etc. It is
rather interesting, as something as simple as data on gaming moves can help
create a psychological imprint of the user at that moment, based on her
impulsive reactions and response to a situation in the game. Mining this
knowledge can: (a) immediately help better explain observed and predicted
player behavior; and (b) consequently propel deeper understanding towards
players' experience, growth and protection. To this effect, we focus on
discovery of the "game behaviours" as micro-patterns formed by continuous
sequence of games and the persistent "play styles" of the players' as a
sequence of such sequences on an online skill gaming platform for Rummy. We
propose a two stage deep neural network, CognitionNet. The first stage focuses
on mining game behaviours as cluster representations in a latent space while
the second aggregates over these micro patterns to discover play styles via a
supervised classification objective around player engagement. The dual
objective allows CognitionNet to reveal several player psychology inspired
decision making and tactics. To our knowledge, this is the first and
one-of-its-kind research to fully automate the discovery of: (i) player
psychology and game tactics from telemetry data; and (ii) relevant diagnostic
explanations to players' engagement predictions. The collaborative training of
the two networks with differential input dimensions is enabled using a novel
formulation of "bridge loss". The network plays pivotal role in obtaining
homogeneous and consistent play style definitions and significantly outperforms
the SOTA baselines wherever applicable.

</details>

### [144] [Urban Air Mobility as a System of Systems: An LLM-Enhanced Holonic Approach](https://arxiv.org/abs/2505.00368)
*Ahmed R. Sadik,Muhammad Ashfaq,Niko Mäkitalo,Tommi Mikkonen*

Main category: cs.AI

TLDR: 本文提出了一种智能整体架构，结合大型语言模型（LLM）来管理城市空中交通（UAM）的复杂性，通过案例研究展示了其动态资源分配和实时重规划能力。


<details>
  <summary>Details</summary>
Motivation: 传统架构在动态复杂环境中难以实现可扩展性、适应性和无缝资源整合，因此需要一种新的智能架构来解决UAM的挑战。

Method: 采用智能整体架构，结合LLM处理自然语言输入、生成自适应计划并管理中断，实现半自主运行的实时协调。

Result: 通过多模式交通案例研究，该架构实现了动态资源分配、实时重规划和自主适应，无需集中控制。

Conclusion: 该工作为弹性、以人为中心的UAM生态系统奠定了基础，未来将探索混合AI集成和实际验证。

Abstract: Urban Air Mobility (UAM) is an emerging System of System (SoS) that faces
challenges in system architecture, planning, task management, and execution.
Traditional architectural approaches struggle with scalability, adaptability,
and seamless resource integration within dynamic and complex environments. This
paper presents an intelligent holonic architecture that incorporates Large
Language Model (LLM) to manage the complexities of UAM. Holons function semi
autonomously, allowing for real time coordination among air taxis, ground
transport, and vertiports. LLMs process natural language inputs, generate
adaptive plans, and manage disruptions such as weather changes or airspace
closures.Through a case study of multimodal transportation with electric
scooters and air taxis, we demonstrate how this architecture enables dynamic
resource allocation, real time replanning, and autonomous adaptation without
centralized control, creating more resilient and efficient urban transportation
networks. By advancing decentralized control and AI driven adaptability, this
work lays the groundwork for resilient, human centric UAM ecosystems, with
future efforts targeting hybrid AI integration and real world validation.

</details>

### [145] [ScaleTrack: Scaling and back-tracking Automated GUI Agents](https://arxiv.org/abs/2505.00416)
*Jing Huang,Zhixiong Zeng,Wenkang Han,Yufeng Zhong,Liming Zheng,Shuai Fu,Jingyuan Chen,Lin Ma*

Main category: cs.AI

TLDR: ScaleTrack是一个用于自动化GUI代理的训练框架，通过扩展GUI定位和回溯规划来解决数据不足和历史行为忽略的问题。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理训练方法存在数据不足和忽略历史行为的局限性，影响了定位和规划的效果。

Method: ScaleTrack通过收集多样化的GUI样本并统一模板训练定位模型，同时设计了一种结合当前GUI图像和历史动作预测下一步动作的策略。

Result: 实验证明ScaleTrack有效描述了GUI环境与动作的对应关系及其演化规则。

Conclusion: ScaleTrack显著提升了GUI代理的性能，数据和代码将公开。

Abstract: Automated GUI agents aims to facilitate user interaction by automatically
performing complex tasks in digital environments, such as web, mobile, desktop
devices. It receives textual task instruction and GUI description to generate
executable actions (\emph{e.g.}, click) and operation boxes step by step.
Training a GUI agent mainly involves grounding and planning stages, in which
the GUI grounding focuses on finding the execution coordinates according to the
task, while the planning stage aims to predict the next action based on
historical actions. However, previous work suffers from the limitations of
insufficient training data for GUI grounding, as well as the ignorance of
backtracking historical behaviors for GUI planning. To handle the above
challenges, we propose ScaleTrack, a training framework by scaling grounding
and backtracking planning for automated GUI agents. We carefully collected GUI
samples of different synthesis criterions from a wide range of sources, and
unified them into the same template for training GUI grounding models.
Moreover, we design a novel training strategy that predicts the next action
from the current GUI image, while also backtracking the historical actions that
led to the GUI image. In this way, ScaleTrack explains the correspondence
between GUI images and actions, which effectively describes the evolution rules
of the GUI environment. Extensive experimental results demonstrate the
effectiveness of ScaleTrack. Data and code will be available at url.

</details>

### [146] [UserCentrix: An Agentic Memory-augmented AI Framework for Smart Spaces](https://arxiv.org/abs/2505.00472)
*Alaa Saleh,Sasu Tarkoma,Praveen Kumar Donta,Naser Hossein Motlagh,Schahram Dustdar,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

TLDR: UserCentrix是一个基于代理AI的框架，通过动态、上下文感知的决策增强智能空间，整合了个性化LLM代理和混合分层控制系统。


<details>
  <summary>Details</summary>
Motivation: 通过自主和主动的决策能力，提升智能环境的动态适应性和资源管理效率。

Method: 结合生成式AI和多代理系统，采用记忆增强推理、协作代理协商和自适应编排策略。

Result: 实验证明，该框架在响应准确性、系统效率和计算资源管理方面表现优异。

Conclusion: UserCentrix为智能空间提供了一种高效、自适应的解决方案，具有广泛的应用潜力。

Abstract: Agentic AI, with its autonomous and proactive decision-making, has
transformed smart environments. By integrating Generative AI (GenAI) and
multi-agent systems, modern AI frameworks can dynamically adapt to user
preferences, optimize data management, and improve resource allocation. This
paper introduces UserCentrix, an agentic memory-augmented AI framework designed
to enhance smart spaces through dynamic, context-aware decision-making. This
framework integrates personalized Large Language Model (LLM) agents that
leverage user preferences and LLM memory management to deliver proactive and
adaptive assistance. Furthermore, it incorporates a hybrid hierarchical control
system, balancing centralized and distributed processing to optimize real-time
responsiveness while maintaining global situational awareness. UserCentrix
achieves resource-efficient AI interactions by embedding memory-augmented
reasoning, cooperative agent negotiation, and adaptive orchestration
strategies. Our key contributions include (i) a self-organizing framework with
proactive scaling based on task urgency, (ii) a Value of Information
(VoI)-driven decision-making process, (iii) a meta-reasoning personal LLM
agent, and (iv) an intelligent multi-agent coordination system for seamless
environment adaptation. Experimental results across various models confirm the
effectiveness of our approach in enhancing response accuracy, system
efficiency, and computational resource management in real-world application.

</details>

### [147] [Rule-based Classifier Models](https://arxiv.org/abs/2505.00474)
*Cecilia Di Florio,Huimin Dong,Antonino Rotolo*

Main category: cs.AI

TLDR: 扩展了法律领域分类器模型的形式框架，将事实与规则（尤其是判例理由）结合，提出基于规则的分类器方法。


<details>
  <summary>Details</summary>
Motivation: 现有分类器仅基于事实，而法律推理依赖事实与规则，需改进框架以更准确模拟法律决策。

Method: 基于Canavotto等人的规则推理模型，构建结合规则与事实的分类器框架，并展示其在新案例决策中的应用。

Result: 展示了如何利用时间因素和法院层级在新框架中推断决策。

Conclusion: 新框架为法律分类器提供了更全面的规则支持，增强了法律推理的准确性。

Abstract: We extend the formal framework of classifier models used in the legal domain.
While the existing classifier framework characterises cases solely through the
facts involved, legal reasoning fundamentally relies on both facts and rules,
particularly the ratio decidendi. This paper presents an initial approach to
incorporating sets of rules within a classifier. Our work is built on the work
of Canavotto et al. (2023), which has developed the rule-based reason model of
precedential constraint within a hierarchy of factors. We demonstrate how
decisions for new cases can be inferred using this enriched rule-based
classifier framework. Additionally, we provide an example of how the time
element and the hierarchy of courts can be used in the new classifier
framework.

</details>

### [148] [Can LLMs Help Improve Analogical Reasoning For Strategic Decisions? Experimental Evidence from Humans and GPT-4](https://arxiv.org/abs/2505.00603)
*Phanish Puranam,Prothit Sen,Maciej Workiewicz*

Main category: cs.AI

TLDR: GPT4在类比推理中召回率高但精度低，人类则精度高但召回率低，表明AI适合生成类比，人类适合评估。


<details>
  <summary>Details</summary>
Motivation: 研究GPT4是否能在战略决策中的类比推理上匹配人类能力。

Method: 采用新颖的实验设计，通过源到目标的匹配测试GPT4和人类的类比推理能力。

Result: GPT4召回率高但精度低，人类精度高但召回率低；AI错误源于表面匹配，人类错误源于因果误解。

Conclusion: AI适合生成类比，人类适合评估，两者在决策中可分工合作。

Abstract: This study investigates whether large language models, specifically GPT4, can
match human capabilities in analogical reasoning within strategic decision
making contexts. Using a novel experimental design involving source to target
matching, we find that GPT4 achieves high recall by retrieving all plausible
analogies but suffers from low precision, frequently applying incorrect
analogies based on superficial similarities. In contrast, human participants
exhibit high precision but low recall, selecting fewer analogies yet with
stronger causal alignment. These findings advance theory by identifying
matching, the evaluative phase of analogical reasoning, as a distinct step that
requires accurate causal mapping beyond simple retrieval. While current LLMs
are proficient in generating candidate analogies, humans maintain a comparative
advantage in recognizing deep structural similarities across domains. Error
analysis reveals that AI errors arise from surface level matching, whereas
human errors stem from misinterpretations of causal structure. Taken together,
the results suggest a productive division of labor in AI assisted
organizational decision making where LLMs may serve as broad analogy
generators, while humans act as critical evaluators, applying the most
contextually appropriate analogies to strategic problems.

</details>

### [149] [Combining LLMs with Logic-Based Framework to Explain MCTS](https://arxiv.org/abs/2505.00610)
*Ziyan An,Xia Wang,Hendrik Baier,Zirong Chen,Abhishek Dubey,Taylor T. Johnson,Jonathan Sprinkle,Ayan Mukhopadhyay,Meiyi Ma*

Main category: cs.AI

TLDR: 提出了一种基于计算树逻辑和大型语言模型的自然语言解释框架，用于增强对蒙特卡洛树搜索算法的可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能在序列规划中缺乏信任的问题，尤其是蒙特卡洛树搜索因其复杂的搜索树而难以解释。

Method: 设计了一个灵活的框架，将用户查询转化为逻辑和变量语句，确保解释与底层环境动态和约束一致。

Result: 通过定量评估，框架在准确性和事实一致性方面表现出色。

Conclusion: 该框架有效提升了蒙特卡洛树搜索的可解释性，适用于广泛的自由形式查询。

Abstract: In response to the lack of trust in Artificial Intelligence (AI) for
sequential planning, we design a Computational Tree Logic-guided large language
model (LLM)-based natural language explanation framework designed for the Monte
Carlo Tree Search (MCTS) algorithm. MCTS is often considered challenging to
interpret due to the complexity of its search trees, but our framework is
flexible enough to handle a wide range of free-form post-hoc queries and
knowledge-based inquiries centered around MCTS and the Markov Decision Process
(MDP) of the application domain. By transforming user queries into logic and
variable statements, our framework ensures that the evidence obtained from the
search tree remains factually consistent with the underlying environmental
dynamics and any constraints in the actual stochastic control process. We
evaluate the framework rigorously through quantitative assessments, where it
demonstrates strong performance in terms of accuracy and factual consistency.

</details>

### [150] [Position: AI Competitions Provide the Gold Standard for Empirical Rigor in GenAI Evaluation](https://arxiv.org/abs/2505.00612)
*D. Sculley,Will Cukierski,Phil Culliton,Sohier Dane,Maggie Demkin,Ryan Holbrook,Addison Howard,Paul Mooney,Walter Reade,Megan Risdal,Nate Keating*

Main category: cs.AI

TLDR: 传统机器学习评估方法不足以满足现代生成式AI模型的评估需求，AI竞赛中的防泄漏措施可借鉴为生成式AI评估的黄金标准。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型的输入输出空间无界、缺乏明确真实目标且存在强反馈循环，传统评估方法失效，亟需新策略。

Method: 提出借鉴AI竞赛中防泄漏和污染的措施，将其作为生成式AI评估的严格标准。

Result: AI竞赛的实践为解决生成式AI评估中的关键问题提供了有效资源。

Conclusion: 建议将AI竞赛视为生成式AI评估的黄金标准，并充分利用其成果。

Abstract: In this position paper, we observe that empirical evaluation in Generative AI
is at a crisis point since traditional ML evaluation and benchmarking
strategies are insufficient to meet the needs of evaluating modern GenAI models
and systems. There are many reasons for this, including the fact that these
models typically have nearly unbounded input and output spaces, typically do
not have a well defined ground truth target, and typically exhibit strong
feedback loops and prediction dependence based on context of previous model
outputs. On top of these critical issues, we argue that the problems of {\em
leakage} and {\em contamination} are in fact the most important and difficult
issues to address for GenAI evaluations. Interestingly, the field of AI
Competitions has developed effective measures and practices to combat leakage
for the purpose of counteracting cheating by bad actors within a competition
setting. This makes AI Competitions an especially valuable (but underutilized)
resource. Now is time for the field to view AI Competitions as the gold
standard for empirical rigor in GenAI evaluation, and to harness and harvest
their results with according value.

</details>

### [151] [Open-Source LLM-Driven Federated Transformer for Predictive IoV Management](https://arxiv.org/abs/2505.00651)
*Yazan Otoum,Arghavan Asad,Ishtiaq Ahmad*

Main category: cs.AI

TLDR: 论文提出了一种名为FPoTT的框架，利用开源大语言模型（LLMs）优化车联网（IoV）中的实时交通管理，解决了现有集中式方案的延迟、可扩展性和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 现有车联网解决方案存在高延迟、可扩展性差和依赖专有AI模型的问题，且大语言模型在车联网中的应用尚未充分探索。

Method: FPoTT采用动态提示优化机制和双层联邦学习架构，结合边缘模型和云端LLMs，并利用Transformer生成合成数据增强训练。

Result: FPoTT在真实数据上实现了99.86%的预测准确率，并在合成数据集上表现优异。

Conclusion: 开源LLMs在车联网管理中具有潜力，为智能交通生态系统提供了安全、自适应和可扩展的解决方案。

Abstract: The proliferation of connected vehicles within the Internet of Vehicles (IoV)
ecosystem presents critical challenges in ensuring scalable, real-time, and
privacy-preserving traffic management. Existing centralized IoV solutions often
suffer from high latency, limited scalability, and reliance on proprietary
Artificial Intelligence (AI) models, creating significant barriers to
widespread deployment, particularly in dynamic and privacy-sensitive
environments. Meanwhile, integrating Large Language Models (LLMs) in vehicular
systems remains underexplored, especially concerning prompt optimization and
effective utilization in federated contexts. To address these challenges, we
propose the Federated Prompt-Optimized Traffic Transformer (FPoTT), a novel
framework that leverages open-source LLMs for predictive IoV management. FPoTT
introduces a dynamic prompt optimization mechanism that iteratively refines
textual prompts to enhance trajectory prediction. The architecture employs a
dual-layer federated learning paradigm, combining lightweight edge models for
real-time inference with cloud-based LLMs to retain global intelligence. A
Transformer-driven synthetic data generator is incorporated to augment training
with diverse, high-fidelity traffic scenarios in the Next Generation Simulation
(NGSIM) format. Extensive evaluations demonstrate that FPoTT, utilizing
EleutherAI Pythia-1B, achieves 99.86% prediction accuracy on real-world data
while maintaining high performance on synthetic datasets. These results
underscore the potential of open-source LLMs in enabling secure, adaptive, and
scalable IoV management, offering a promising alternative to proprietary
solutions in smart mobility ecosystems.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [152] [Learning to Borrow Features for Improved Detection of Small Objects in Single-Shot Detectors](https://arxiv.org/abs/2505.00044)
*Richard Schmit*

Main category: cs.CV

TLDR: 提出一种新框架，通过从同类大对象中“借用”特征来增强小对象检测能力，显著提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 解决单次检测器中因空间分辨率与语义丰富度之间的权衡导致的小对象检测难题。

Method: 引入三个关键模块：特征匹配块（FMB）、特征表示块（FRB）和特征融合块（FFB），通过加权聚合和特征融合增强浅层特征。

Result: 实验表明，该方法显著提升了小对象检测精度，同时保持实时性能。

Conclusion: 该方法为复杂视觉环境中的鲁棒对象检测提供了有前景的方向。

Abstract: Detecting small objects remains a significant challenge in single-shot object
detectors due to the inherent trade-off between spatial resolution and semantic
richness in convolutional feature maps. To address this issue, we propose a
novel framework that enables small object representations to "borrow"
discriminative features from larger, semantically richer instances within the
same class. Our architecture introduces three key components: the Feature
Matching Block (FMB) to identify semantically similar descriptors across
layers, the Feature Representing Block (FRB) to generate enhanced shallow
features through weighted aggregation, and the Feature Fusion Block (FFB) to
refine feature maps by integrating original, borrowed, and context information.
Built upon the SSD framework, our method improves the descriptive capacity of
shallow layers while maintaining real-time detection performance. Experimental
results demonstrate that our approach significantly boosts small object
detection accuracy over baseline methods, offering a promising direction for
robust object detection in complex visual environments.

</details>

### [153] [Investigating Zero-Shot Diagnostic Pathology in Vision-Language Models with Efficient Prompt Design](https://arxiv.org/abs/2505.00134)
*Vasudev Sharma,Ahmed Alagha,Abdelhakim Khellaf,Vincent Quoc-Huy Trinh,Mahdi S. Hosseini*

Main category: cs.CV

TLDR: 本文研究了三种视觉语言模型（Quilt-Net、Quilt-LLAVA和CONCH）在消化病理数据集上的表现，发现提示工程对模型性能有显著影响，其中CONCH模型在精确解剖参考下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在计算病理学中的敏感性和诊断准确性，特别是在大规模临床数据和提示设计方面。

Method: 通过结构化消融研究，开发了一个全面的提示工程框架，系统性地变化领域特异性、解剖精度、指令框架和输出约束。

Result: CONCH模型在精确解剖参考下表现最佳，解剖上下文对性能至关重要，模型复杂度并非性能的唯一决定因素。

Conclusion: 提示工程在计算病理学中至关重要，视觉语言模型在适当提示下能显著提升诊断准确性。

Abstract: Vision-language models (VLMs) have gained significant attention in
computational pathology due to their multimodal learning capabilities that
enhance big-data analytics of giga-pixel whole slide image (WSI). However,
their sensitivity to large-scale clinical data, task formulations, and prompt
design remains an open question, particularly in terms of diagnostic accuracy.
In this paper, we present a systematic investigation and analysis of three
state of the art VLMs for histopathology, namely Quilt-Net, Quilt-LLAVA, and
CONCH, on an in-house digestive pathology dataset comprising 3,507 WSIs, each
in giga-pixel form, across distinct tissue types. Through a structured ablative
study on cancer invasiveness and dysplasia status, we develop a comprehensive
prompt engineering framework that systematically varies domain specificity,
anatomical precision, instructional framing, and output constraints. Our
findings demonstrate that prompt engineering significantly impacts model
performance, with the CONCH model achieving the highest accuracy when provided
with precise anatomical references. Additionally, we identify the critical
importance of anatomical context in histopathological image analysis, as
performance consistently degraded when reducing anatomical precision. We also
show that model complexity alone does not guarantee superior performance, as
effective domain alignment and domain-specific training are critical. These
results establish foundational guidelines for prompt engineering in
computational pathology and highlight the potential of VLMs to enhance
diagnostic accuracy when properly instructed with domain-appropriate prompts.

</details>

### [154] [Eye2Eye: A Simple Approach for Monocular-to-Stereo Video Synthesis](https://arxiv.org/abs/2505.00135)
*Michal Geyer,Omer Tov,Linyi Jin,Richard Tucker,Inbar Mosseri,Tali Dekel,Noah Snavely*

Main category: cs.CV

TLDR: 提出一种将文本到视频生成器转换为视频到立体视频生成器的简单方法，直接合成新视角，避免中间步骤。


<details>
  <summary>Details</summary>
Motivation: 立体3D视频生成因数据稀缺和现有方法在多阶段处理中的局限性（如镜面或透明物体场景）而具有挑战性。

Method: 利用预训练视频模型的几何、材质、光学和语义先验，直接合成新视角，无需外部几何模型或多阶段处理。

Result: 在复杂真实场景中展示了方法的优势，支持多样物体材质和组合。

Conclusion: 直接合成新视角的方法避免了传统多阶段处理的限制，适用于复杂场景。

Abstract: The rising popularity of immersive visual experiences has increased interest
in stereoscopic 3D video generation. Despite significant advances in video
synthesis, creating 3D videos remains challenging due to the relative scarcity
of 3D video data. We propose a simple approach for transforming a text-to-video
generator into a video-to-stereo generator. Given an input video, our framework
automatically produces the video frames from a shifted viewpoint, enabling a
compelling 3D effect. Prior and concurrent approaches for this task typically
operate in multiple phases, first estimating video disparity or depth, then
warping the video accordingly to produce a second view, and finally inpainting
the disoccluded regions. This approach inherently fails when the scene involves
specular surfaces or transparent objects. In such cases, single-layer disparity
estimation is insufficient, resulting in artifacts and incorrect pixel shifts
during warping. Our work bypasses these restrictions by directly synthesizing
the new viewpoint, avoiding any intermediate steps. This is achieved by
leveraging a pre-trained video model's priors on geometry, object materials,
optics, and semantics, without relying on external geometry models or manually
disentangling geometry from the synthesis process. We demonstrate the
advantages of our approach in complex, real-world scenarios featuring diverse
object materials and compositions. See videos on
https://video-eye2eye.github.io

</details>

### [155] [Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models](https://arxiv.org/abs/2505.00150)
*Minh-Hao Van,Xintao Wu*

Main category: cs.CV

TLDR: 该论文提出了一种基于视觉语言模型（VLMs）的检测和缓解仇恨内容的方法，包括定义引导的提示技术和UnHateMeme框架。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中多模态模因（memes）常被滥用传播仇恨言论，现有研究在检测仇恨内容方面较为成熟，但如何有效转化仇恨内容仍具挑战性。

Method: 采用定义引导的提示技术检测仇恨模因，并提出UnHateMeme框架，通过替换仇恨文本或视觉组件来缓解仇恨内容。

Result: VLMs在仇恨模因检测任务中表现优异，UnHateMeme框架能有效将仇恨模因转化为非仇恨形式，并保持多模态一致性。

Conclusion: 论文展示了VLMs在构建安全网络环境中的重要应用潜力，并分析了不同VLMs的优缺点。

Abstract: The rapid evolution of social media has provided enhanced communication
channels for individuals to create online content, enabling them to express
their thoughts and opinions. Multimodal memes, often utilized for playful or
humorous expressions with visual and textual elements, are sometimes misused to
disseminate hate speech against individuals or groups. While the detection of
hateful memes is well-researched, developing effective methods to transform
hateful content in memes remains a significant challenge. Leveraging the
powerful generation and reasoning capabilities of Vision-Language Models
(VLMs), we address the tasks of detecting and mitigating hateful content. This
paper presents two key contributions: first, a definition-guided prompting
technique for detecting hateful memes, and second, a unified framework for
mitigating hateful content in memes, named UnHateMeme, which works by replacing
hateful textual and/or visual components. With our definition-guided prompts,
VLMs achieve impressive performance on hateful memes detection task.
Furthermore, our UnHateMeme framework, integrated with VLMs, demonstrates a
strong capability to convert hateful memes into non-hateful forms that meet
human-level criteria for hate speech and maintain multimodal coherence between
image and text. Through empirical experiments, we show the effectiveness of
state-of-the-art pretrained VLMs such as LLaVA, Gemini and GPT-4o on the
proposed tasks, providing a comprehensive analysis of their respective
strengths and limitations for these tasks. This paper aims to shed light on
important applications of VLMs for ensuring safe and respectful online
environments.

</details>

### [156] [V3LMA: Visual 3D-enhanced Language Model for Autonomous Driving](https://arxiv.org/abs/2505.00156)
*Jannik Lübberstedt,Esteban Rivera,Nico Uhlemann,Markus Lienkamp*

Main category: cs.CV

TLDR: V3LMA通过结合大型语言模型（LLMs）和视觉语言模型（LVLMs），提升自动驾驶中的3D场景理解能力，无需微调即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型（LVLMs）在自动驾驶中对3D环境的理解有限，限制了其对动态环境的全面和安全理解。

Method: V3LMA通过预处理管道提取3D对象数据，结合文本描述和视频输入，增强3D场景理解。

Result: 在LingoQA基准测试中达到0.56分，提升了复杂交通场景下的情境感知和决策能力。

Conclusion: V3LMA通过融合策略和标记组合，推动了交通场景的解析，为更安全的自动驾驶系统提供了可能。

Abstract: Large Vision Language Models (LVLMs) have shown strong capabilities in
understanding and analyzing visual scenes across various domains. However, in
the context of autonomous driving, their limited comprehension of 3D
environments restricts their effectiveness in achieving a complete and safe
understanding of dynamic surroundings. To address this, we introduce V3LMA, a
novel approach that enhances 3D scene understanding by integrating Large
Language Models (LLMs) with LVLMs. V3LMA leverages textual descriptions
generated from object detections and video inputs, significantly boosting
performance without requiring fine-tuning. Through a dedicated preprocessing
pipeline that extracts 3D object data, our method improves situational
awareness and decision-making in complex traffic scenarios, achieving a score
of 0.56 on the LingoQA benchmark. We further explore different fusion
strategies and token combinations with the goal of advancing the interpretation
of traffic scenes, ultimately enabling safer autonomous driving systems.

</details>

### [157] [Direct Motion Models for Assessing Generated Videos](https://arxiv.org/abs/2505.00209)
*Kelsey Allen,Carl Doersch,Guangyao Zhou,Mohammed Suhail,Danny Driess,Ignacio Rocco,Yulia Rubanova,Thomas Kipf,Mehdi S. M. Sajjadi,Kevin Murphy,Joao Carreira,Sjoerd van Steenkiste*

Main category: cs.CV

TLDR: 论文提出了一种基于点轨迹自动编码的新指标，用于更准确地评估生成视频中的运动质量和对象交互，优于现有方法如FVD。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在生成逼真帧方面表现良好，但运动质量较差，而FVD等流行指标无法有效捕捉这一问题。

Method: 通过自动编码点轨迹提取运动特征，用于比较视频分布或评估单个视频的运动质量。

Result: 新指标对合成数据中的时间扭曲更敏感，能更好地预测人类对生成视频时间一致性和真实性的评价。

Conclusion: 点轨迹表示不仅能提升运动评估的准确性，还能定位生成视频中的不一致性，提供更强的可解释性。

Abstract: A current limitation of video generative video models is that they generate
plausible looking frames, but poor motion -- an issue that is not well captured
by FVD and other popular methods for evaluating generated videos. Here we go
beyond FVD by developing a metric which better measures plausible object
interactions and motion. Our novel approach is based on auto-encoding point
tracks and yields motion features that can be used to not only compare
distributions of videos (as few as one generated and one ground truth, or as
many as two datasets), but also for evaluating motion of single videos. We show
that using point tracks instead of pixel reconstruction or action recognition
features results in a metric which is markedly more sensitive to temporal
distortions in synthetic data, and can predict human evaluations of temporal
consistency and realism in generated videos obtained from open-source models
better than a wide range of alternatives. We also show that by using a point
track representation, we can spatiotemporally localize generative video
inconsistencies, providing extra interpretability of generated video errors
relative to prior work. An overview of the results and link to the code can be
found on the project page: http://trajan-paper.github.io.

</details>

### [158] [Towards Robust and Generalizable Gerchberg Saxton based Physics Inspired Neural Networks for Computer Generated Holography: A Sensitivity Analysis Framework](https://arxiv.org/abs/2505.00220)
*Ankit Amrutkar,Björn Kampa,Volkmar Schulz,Johannes Stegmaier,Markus Rothermel,Dorit Merhof*

Main category: cs.CV

TLDR: 提出了一种基于Saltelli扩展的Sobol方法的敏感性分析框架，量化了前向模型超参数对GS-PINN性能的影响，并确定了SLM像素分辨率是关键因素。


<details>
  <summary>Details</summary>
Motivation: 解决计算机生成全息术（CGH）中相位恢复的逆问题，尤其是前向模型及其超参数对Gerchberg-Saxton启发的物理神经网络（GS-PINNs）性能的限制问题。

Method: 采用Saltelli扩展的Sobol方法进行敏感性分析，评估前向模型超参数对GS-PINN性能的影响。

Result: 发现SLM像素分辨率是影响神经网络敏感性的主要因素，自由空间传播前向模型表现优于傅里叶全息术。

Conclusion: 研究为CGH中的前向模型选择、神经网络架构和性能评估提供了具体指南，推动了稳健、可解释和可泛化的神经网络发展。

Abstract: Computer-generated holography (CGH) enables applications in holographic
augmented reality (AR), 3D displays, systems neuroscience, and optical
trapping. The fundamental challenge in CGH is solving the inverse problem of
phase retrieval from intensity measurements. Physics-inspired neural networks
(PINNs), especially Gerchberg-Saxton-based PINNs (GS-PINNs), have advanced
phase retrieval capabilities. However, their performance strongly depends on
forward models (FMs) and their hyperparameters (FMHs), limiting generalization,
complicating benchmarking, and hindering hardware optimization. We present a
systematic sensitivity analysis framework based on Saltelli's extension of
Sobol's method to quantify FMH impacts on GS-PINN performance. Our analysis
demonstrates that SLM pixel-resolution is the primary factor affecting neural
network sensitivity, followed by pixel-pitch, propagation distance, and
wavelength. Free space propagation forward models demonstrate superior neural
network performance compared to Fourier holography, providing enhanced
parameterization and generalization. We introduce a composite evaluation metric
combining performance consistency, generalization capability, and
hyperparameter perturbation resilience, establishing a unified benchmarking
standard across CGH configurations. Our research connects physics-inspired deep
learning theory with practical CGH implementations through concrete guidelines
for forward model selection, neural network architecture, and performance
evaluation. Our contributions advance the development of robust, interpretable,
and generalizable neural networks for diverse holographic applications,
supporting evidence-based decisions in CGH research and implementation.

</details>

### [159] [ReXGradient-160K: A Large-Scale Publicly Available Dataset of Chest Radiographs with Free-text Reports](https://arxiv.org/abs/2505.00228)
*Xiaoman Zhang,Julián N. Acosta,Josh Miller,Ouwen Huang,Pranav Rajpurkar*

Main category: cs.CV

TLDR: ReXGradient-160K是目前公开可用的最大胸部X光数据集，包含160,000项研究和109,487名患者的配对放射报告，旨在推动医学影像AI研究。


<details>
  <summary>Details</summary>
Motivation: 加速医学影像AI研究，提升自动化放射分析的先进水平。

Method: 数据集包含来自3个美国医疗系统的160,000项胸部X光研究，分为训练、验证和测试集。

Result: 提供了包含详细放射报告和多图像研究的全面数据集，支持AI模型开发和评估。

Conclusion: ReXGradient-160K的开放将促进医学影像AI和自动化报告生成模型的进步。

Abstract: We present ReXGradient-160K, representing the largest publicly available
chest X-ray dataset to date in terms of the number of patients. This dataset
contains 160,000 chest X-ray studies with paired radiological reports from
109,487 unique patients across 3 U.S. health systems (79 medical sites). This
comprehensive dataset includes multiple images per study and detailed radiology
reports, making it particularly valuable for the development and evaluation of
AI systems for medical imaging and automated report generation models. The
dataset is divided into training (140,000 studies), validation (10,000
studies), and public test (10,000 studies) sets, with an additional private
test set (10,000 studies) reserved for model evaluation on the ReXrank
benchmark. By providing this extensive dataset, we aim to accelerate research
in medical imaging AI and advance the state-of-the-art in automated
radiological analysis. Our dataset will be open-sourced at
https://huggingface.co/datasets/rajpurkarlab/ReXGradient-160K.

</details>

### [160] [Empowering Agentic Video Analytics Systems with Video Language Models](https://arxiv.org/abs/2505.00254)
*Yuxuan Yan,Shiqi Jiang,Ting Cao,Yifan Yang,Qianqian Yang,Yuanchao Shu,Yuqing Yang,Lili Qiu*

Main category: cs.CV

TLDR: AVA是一种基于视频语言模型（VLM）的系统，通过事件知识图谱（EKG）和代理检索生成机制，解决了超长视频内容处理问题，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视频分析系统局限于预定义任务，难以适应开放场景，而VLM在处理超长视频时存在上下文窗口限制。

Method: AVA通过实时构建EKG索引视频流，并采用代理检索生成机制处理复杂查询。

Result: 在LVBench和VideoMME-Long上分别达到62.3%和64.1%的准确率，在AVA-100上达到75.8%。

Conclusion: AVA在开放场景和超长视频分析中表现出色，为视频分析领域提供了新方向。

Abstract: AI-driven video analytics has become increasingly pivotal across diverse
domains. However, existing systems are often constrained to specific,
predefined tasks, limiting their adaptability in open-ended analytical
scenarios. The recent emergence of Video-Language Models (VLMs) as
transformative technologies offers significant potential for enabling
open-ended video understanding, reasoning, and analytics. Nevertheless, their
limited context windows present challenges when processing ultra-long video
content, which is prevalent in real-world applications. To address this, we
introduce AVA, a VLM-powered system designed for open-ended, advanced video
analytics. AVA incorporates two key innovations: (1) the near real-time
construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or
continuous video streams, and (2) an agentic retrieval-generation mechanism
that leverages EKGs to handle complex and diverse queries. Comprehensive
evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that
AVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,
respectively, significantly surpassing existing VLM and video
Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video
analytics in ultra-long and open-world video scenarios, we introduce a new
benchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours
in duration, along with 120 manually annotated, diverse, and complex
question-answer pairs. On AVA-100, AVA achieves top-tier performance with an
accuracy of 75.8%.

</details>

### [161] [Pack-PTQ: Advancing Post-training Quantization of Neural Networks by Pack-wise Reconstruction](https://arxiv.org/abs/2505.00259)
*Changjun Li,Runqing Jiang,Zhuo Song,Pengpeng Yu,Ye Zhang,Yulan Guo*

Main category: cs.CV

TLDR: 提出了一种名为Pack-PTQ的新型后训练量化方法，通过Hessian引导的自适应分组机制和混合精度量化，解决了现有PTQ方法忽略跨块依赖性和低比特精度下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法采用块级重建，忽略了跨块依赖性，导致低比特情况下精度显著下降。

Method: 设计Hessian引导的自适应分组机制，将块划分为非重叠组作为重建基础单元；提出混合精度量化方法，根据敏感性分配不同比特宽度。

Result: 在2D图像和3D点云分类任务中，使用多种网络架构的实验表明，该方法优于现有PTQ方法。

Conclusion: Pack-PTQ通过保留跨块依赖性和自适应比特分配，显著提升了量化模型的性能。

Abstract: Post-training quantization (PTQ) has evolved as a prominent solution for
compressing complex models, which advocates a small calibration dataset and
avoids end-to-end retraining. However, most existing PTQ methods employ
block-wise reconstruction, which neglects cross-block dependency and exhibits a
notable accuracy drop in low-bit cases. To address these limitations, this
paper presents a novel PTQ method, dubbed Pack-PTQ. First, we design a
Hessian-guided adaptive packing mechanism to partition blocks into
non-overlapping packs, which serve as the base unit for reconstruction, thereby
preserving the cross-block dependency and enabling accurate quantization
parameters estimation. Second, based on the pack configuration, we propose a
mixed-precision quantization approach to assign varied bit-widths to packs
according to their distinct sensitivities, thereby further enhancing
performance. Extensive experiments on 2D image and 3D point cloud
classification tasks, using various network architectures, demonstrate the
superiority of our method over the state-of-the-art PTQ methods.

</details>

### [162] [AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor Long-Term Medication Adherence and Care](https://arxiv.org/abs/2505.00275)
*Md Asaduzzaman Jabin,Hanqi Jiang,Yiwei Li,Patrick Kaggwa,Eugene Douglass,Juliet N. Sekandi,Tianming Liu*

Main category: cs.CV

TLDR: AdCare-VLM是一种基于视频的多模态大视觉语言模型，用于通过患者视频进行药物依从性的视觉问答（VQA）。该方法在结核病（TB）药物监测视频数据集上表现优异，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 慢性疾病患者药物依从性差是常见问题，受多种因素影响。通过视觉问答技术监测依从性，有望改善疾病管理。

Method: 使用806个结核病药物监测视频的私有数据集，微调Video-LLaVA模型，检测依从性模式。提出LLM-TB-VQA数据集，整合视觉特征与医学概念。

Result: 模型在多种配置下性能优于LLaVA-V1.5和Chat-UniVi，绝对提升3.1%至3.54%。消融实验和注意力图可视化验证了方法的有效性。

Conclusion: AdCare-VLM通过多模态交互提升药物依从性监测的准确性和可解释性，为慢性病管理提供新工具。

Abstract: Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS,
epilepsy, and tuberculosis, necessitate rigorous adherence to medication to
avert disease progression, manage symptoms, and decrease mortality rates.
Adherence is frequently undermined by factors including patient behavior,
caregiver support, elevated medical costs, and insufficient healthcare
infrastructure. We propose AdCare-VLM, a specialized Video-LLaVA-based
multimodal large vision language model (LVLM) aimed at visual question
answering (VQA) concerning medication adherence through patient videos. We
employ a private dataset comprising 806 custom-annotated tuberculosis (TB)
medication monitoring videos, which have been labeled by clinical experts, to
fine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a
detailed medical adherence VQA dataset that encompasses positive, negative, and
ambiguous adherence cases. Our method identifies correlations between visual
features, such as the clear visibility of the patient's face, medication, water
intake, and the act of ingestion, and their associated medical concepts in
captions. This facilitates the integration of aligned visual-linguistic
representations and improves multimodal interactions. Experimental results
indicate that our method surpasses parameter-efficient fine-tuning (PEFT)
enabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute
improvements ranging from 3.1% to 3.54% across pre-trained, regular, and
low-rank adaptation (LoRA) configurations. Comprehensive ablation studies and
attention map visualizations substantiate our approach, enhancing
interpretability.

</details>

### [163] [Fine-grained spatial-temporal perception for gas leak segmentation](https://arxiv.org/abs/2505.00295)
*Xinlong Zhao,Shan Du*

Main category: cs.CV

TLDR: 提出了一种名为FGSTP的算法，用于高效准确地检测和分割气体泄漏，通过捕获帧间运动线索并结合精细化的对象特征，实现了优于现有技术的分割效果。


<details>
  <summary>Details</summary>
Motivation: 气体泄漏对人类健康和环境构成重大风险，但由于其隐蔽性和随机形状，现有方法难以高效准确地检测和分割。

Method: FGSTP算法通过构建相关体积捕获帧间运动信息，逐步细化对象级特征，并使用解码器优化边界分割。

Result: 在自建数据集GasVid上的实验表明，FGSTP在分割非刚性物体（如气体泄漏）方面优于现有SOTA模型。

Conclusion: FGSTP算法通过结合时空感知和精细化特征，显著提升了气体泄漏分割的准确性。

Abstract: Gas leaks pose significant risks to human health and the environment. Despite
long-standing concerns, there are limited methods that can efficiently and
accurately detect and segment leaks due to their concealed appearance and
random shapes. In this paper, we propose a Fine-grained Spatial-Temporal
Perception (FGSTP) algorithm for gas leak segmentation. FGSTP captures critical
motion clues across frames and integrates them with refined object features in
an end-to-end network. Specifically, we first construct a correlation volume to
capture motion information between consecutive frames. Then, the fine-grained
perception progressively refines the object-level features using previous
outputs. Finally, a decoder is employed to optimize boundary segmentation.
Because there is no highly precise labeled dataset for gas leak segmentation,
we manually label a gas leak video dataset, GasVid. Experimental results on
GasVid demonstrate that our model excels in segmenting non-rigid objects such
as gas leaks, generating the most accurate mask compared to other
state-of-the-art (SOTA) models.

</details>

### [164] [AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented Contour Quality](https://arxiv.org/abs/2505.00308)
*Biling Wang,Austen Maniscalco,Ti Bai,Siqiu Wang,Michael Dohopolski,Mu-Han Lin,Chenyang Shen,Dan Nguyen,Junzhou Huang,Steve Jiang,Xinlei Wang*

Main category: cs.CV

TLDR: 提出了一种基于深度学习的自动轮廓质量评估方法，利用贝叶斯序数分类和校准不确定性阈值，无需依赖真实轮廓或大量手动标注。


<details>
  <summary>Details</summary>
Motivation: 提高在线自适应放疗中自动轮廓的质量评估效率，减少手动工作量和依赖。

Method: 开发贝叶斯序数分类模型，通过校准不确定性阈值优化临床准确性，并在无标注、有限标注和充分标注三种数据场景下验证。

Result: 模型在所有场景下表现稳健，仅需30个手动标注即可达到90%以上准确率，校准后93%的轮廓质量预测准确率超过98%。

Conclusion: 该方法显著提升了放疗轮廓效率，减少手动审查，确保更安全可靠的工作流程。

Abstract: Purpose: This study presents a Deep Learning (DL)-based quality assessment
(QA) approach for evaluating auto-generated contours (auto-contours) in
radiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). Leveraging
Bayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds,
the method enables confident QA predictions without relying on ground truth
contours or extensive manual labeling. Methods: We developed a BOC model to
classify auto-contour quality and quantify prediction uncertainty. A
calibration step was used to optimize uncertainty thresholds that meet clinical
accuracy needs. The method was validated under three data scenarios: no manual
labels, limited labels, and extensive labels. For rectum contours in prostate
cancer, we applied geometric surrogate labels when manual labels were absent,
transfer learning when limited, and direct supervision when ample labels were
available. Results: The BOC model delivered robust performance across all
scenarios. Fine-tuning with just 30 manual labels and calibrating with 34
subjects yielded over 90% accuracy on test data. Using the calibrated
threshold, over 93% of the auto-contours' qualities were accurately predicted
in over 98% of cases, reducing unnecessary manual reviews and highlighting
cases needing correction. Conclusion: The proposed QA model enhances contouring
efficiency in OART by reducing manual workload and enabling fast, informed
clinical decisions. Through uncertainty quantification, it ensures safer, more
reliable radiotherapy workflows.

</details>

### [165] [AWARE-NET: Adaptive Weighted Averaging for Robust Ensemble Network in Deepfake Detection](https://arxiv.org/abs/2505.00312)
*Muhammad Salman,Iqra Tariq,Mishal Zulfiqar,Muqadas Jalal,Sami Aujla,Sumbal Fatima*

Main category: cs.CV

TLDR: 提出了一种基于深度学习的两层集成框架，用于深度伪造检测，通过动态权重机制结合多种先进架构，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于合成媒体的兴起对数字身份和网络安全构成威胁，深度伪造检测变得至关重要。现有方法在跨数据集和多种伪造类型上的性能一致性仍有挑战。

Method: 采用Xception、Res2Net101和EfficientNet-B7三种架构，每种架构初始化三次以增强多样性，并通过动态权重机制结合预测结果。第一层减少模型方差，第二层通过反向传播学习最优权重。

Result: 在FF++和CelebDF-v2数据集上取得了99.22%和100.00%的AUC分数，以及98.06%和99.94%的F1分数。跨数据集评估中AUC为88.20%和72.52%，F1为93.16%和80.62%。

Conclusion: 该框架在深度伪造检测中表现出卓越性能，尤其在跨数据集泛化能力上具有优势。

Abstract: Deepfake detection has become increasingly important due to the rise of
synthetic media, which poses significant risks to digital identity and cyber
presence for security and trust. While multiple approaches have improved
detection accuracy, challenges remain in achieving consistent performance
across diverse datasets and manipulation types. In response, we propose a novel
two-tier ensemble framework for deepfake detection based on deep learning that
hierarchically combines multiple instances of three state-of-the-art
architectures: Xception, Res2Net101, and EfficientNet-B7. Our framework employs
a unique approach where each architecture is instantiated three times with
different initializations to enhance model diversity, followed by a learnable
weighting mechanism that dynamically combines their predictions. Unlike
traditional fixed-weight ensembles, our first-tier averages predictions within
each architecture family to reduce model variance, while the second tier learns
optimal contribution weights through backpropagation, automatically adjusting
each architecture's influence based on their detection reliability. Our
experiments achieved state-of-the-art intra-dataset performance with AUC scores
of 99.22% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.06% (FF++) and
99.94% (CelebDF-v2) without augmentation. With augmentation, we achieve AUC
scores of 99.47% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.43%
(FF++) and 99.95% (CelebDF-v2). The framework demonstrates robust cross-dataset
generalization, achieving AUC scores of 88.20% and 72.52%, and F1 scores of
93.16% and 80.62% in cross-dataset evaluations.

</details>

### [166] [Quaternion Wavelet-Conditioned Diffusion Models for Image Super-Resolution](https://arxiv.org/abs/2505.00334)
*Luigi Sigillo,Christian Bianchi,Danilo Comminiello*

Main category: cs.CV

TLDR: 提出了一种名为ResQu的新型超分辨率框架，结合四元数小波预处理与潜在扩散模型，通过动态集成四元数小波嵌入提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有超分辨率方法在高倍放大时难以平衡感知质量与结构保真度的问题。

Method: 集成四元数小波预处理与潜在扩散模型，引入四元数小波和时间感知编码器，动态嵌入小波信息。

Result: 在领域特定数据集上表现优异，感知质量和标准评估指标均优于现有方法。

Conclusion: ResQu框架显著提升了超分辨率重建的质量，尤其在细节和纹理方面表现突出。

Abstract: Image Super-Resolution is a fundamental problem in computer vision with broad
applications spacing from medical imaging to satellite analysis. The ability to
reconstruct high-resolution images from low-resolution inputs is crucial for
enhancing downstream tasks such as object detection and segmentation. While
deep learning has significantly advanced SR, achieving high-quality
reconstructions with fine-grained details and realistic textures remains
challenging, particularly at high upscaling factors. Recent approaches
leveraging diffusion models have demonstrated promising results, yet they often
struggle to balance perceptual quality with structural fidelity. In this work,
we introduce ResQu a novel SR framework that integrates a quaternion wavelet
preprocessing framework with latent diffusion models, incorporating a new
quaternion wavelet- and time-aware encoder. Unlike prior methods that simply
apply wavelet transforms within diffusion models, our approach enhances the
conditioning process by exploiting quaternion wavelet embeddings, which are
dynamically integrated at different stages of denoising. Furthermore, we also
leverage the generative priors of foundation models such as Stable Diffusion.
Extensive experiments on domain-specific datasets demonstrate that our method
achieves outstanding SR results, outperforming in many cases existing
approaches in perceptual quality and standard evaluation metrics. The code will
be available after the revision process.

</details>

### [167] [Efficient Neural Video Representation with Temporally Coherent Modulation](https://arxiv.org/abs/2505.00335)
*Seungjun Shin,Suji Kim,Dokwan Oh*

Main category: cs.CV

TLDR: NVTM提出了一种新的视频表示框架，通过分解时空3D视频数据为带有流信息的2D网格，实现了快速学习和高效参数利用，显著提升了编码速度和视频质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于网格的隐式神经表示（INR）方法在视频应用中存在参数冗余和效率低下的问题，NVTM旨在解决这些问题。

Method: NVTM将视频数据分解为带有流信息的2D网格，通过时间相干调制捕捉视频动态特性，实现高效学习和快速编码。

Result: NVTM在编码速度上比NeRV快3倍以上，在PSNR/LPIPS指标上显著优于现有网格方法，并在压缩任务中达到与H.264/HEVC相当的性能。

Conclusion: NVTM是一种高效且通用的视频表示框架，适用于多种任务，如超分辨率、帧插值和视频修复。

Abstract: Implicit neural representations (INR) has found successful applications
across diverse domains. To employ INR in real-life, it is important to speed up
training. In the field of INR for video applications, the state-of-the-art
approach employs grid-type parametric encoding and successfully achieves a
faster encoding speed in comparison to its predecessors. However, the grid
usage, which does not consider the video's dynamic nature, leads to redundant
use of trainable parameters. As a result, it has significantly lower parameter
efficiency and higher bitrate compared to NeRV-style methods that do not use a
parametric encoding. To address the problem, we propose Neural Video
representation with Temporally coherent Modulation (NVTM), a novel framework
that can capture dynamic characteristics of video. By decomposing the
spatio-temporal 3D video data into a set of 2D grids with flow information,
NVTM enables learning video representation rapidly and uses parameter
efficiently. Our framework enables to process temporally corresponding pixels
at once, resulting in the fastest encoding speed for a reasonable video
quality, especially when compared to the NeRV-style method, with a speed
increase of over 3 times. Also, it remarks an average of 1.54dB/0.019
improvements in PSNR/LPIPS on UVG (Dynamic) (even with 10% fewer parameters)
and an average of 1.84dB/0.013 improvements in PSNR/LPIPS on MCL-JCV (Dynamic),
compared to previous grid-type works. By expanding this to compression tasks,
we demonstrate comparable performance to video compression standards (H.264,
HEVC) and recent INR approaches for video compression. Additionally, we perform
extensive experiments demonstrating the superior performance of our algorithm
across diverse tasks, encompassing super resolution, frame interpolation and
video inpainting. Project page is https://sujiikim.github.io/NVTM/.

</details>

### [168] [Automated segmenta-on of pediatric neuroblastoma on multi-modal MRI: Results of the SPPIN challenge at MICCAI 2023](https://arxiv.org/abs/2505.00369)
*M. A. D. Buser,D. C. Simons,M. Fitski,M. H. W. A. Wijnen,A. S. Littooij,A. H. ter Brugge,I. N. Vos,M. H. A. Janse,M. de Boer,R. ter Maat,J. Sato,S. Kido,S. Kondo,S. Kasai,M. Wodzinski,H. Muller,J. Ye,J. He,Y. Kirchhoff,M. R. Rokkus,G. Haokai,S. Zitong,M. Fernández-Patón,D. Veiga-Canuto,D. G. Ellis,M. R. Aizenberg,B. H. M. van der Velden,H. Kuijf,A. De Luca,A. F. W. van der Steeg*

Main category: cs.CV

TLDR: SPPIN挑战赛旨在推动神经母细胞瘤MRI自动分割技术的发展，最佳团队使用预训练网络STU-Net取得高分，但小肿瘤分割仍需改进。


<details>
  <summary>Details</summary>
Motivation: 神经母细胞瘤手术规划依赖耗时且依赖用户的MRI 3D模型，需开发全自动分割方法。

Method: 组织SPPIN挑战赛，提供多模态MRI数据，团队基于Dice分数、HD95和VS指标竞争。

Result: 最佳团队Dice分数0.82，HD95 7.69mm，VS 0.91，但小肿瘤分割效果不佳。

Conclusion: 预训练网络在小数据集有效，但需更可靠方法以支持临床手术规划。

Abstract: Surgery plays an important role within the treatment for neuroblastoma, a
common pediatric cancer. This requires careful planning, often via magnetic
resonance imaging (MRI)-based anatomical 3D models. However, creating these
models is often time-consuming and user dependent. We organized the Surgical
Planning in Pediatric Neuroblastoma (SPPIN) challenge, to stimulate
developments on this topic, and set a benchmark for fully automatic
segmentation of neuroblastoma on multi-model MRI. The challenge started with a
training phase, where teams received 78 sets of MRI scans from 34 patients,
consisting of both diagnostic and post-chemotherapy MRI scans. The final test
phase, consisting of 18 MRI sets from 9 patients, determined the ranking of the
teams. Ranking was based on the Dice similarity coefficient (Dice score), the
95th percentile of the Hausdorff distance (HD95) and the volumetric similarity
(VS). The SPPIN challenge was hosted at MICCAI 2023. The final leaderboard
consisted of 9 teams. The highest-ranking team achieved a median Dice score
0.82, a median HD95 of 7.69 mm and a VS of 0.91, utilizing a large, pretrained
network called STU-Net. A significant difference for the segmentation results
between diagnostic and post-chemotherapy MRI scans was observed (Dice = 0.89 vs
Dice = 0.59, P = 0.01) for the highest-ranking team. SPPIN is the first medical
segmentation challenge in extracranial pediatric oncology. The highest-ranking
team used a large pre-trained network, suggesting that pretraining can be of
use in small, heterogenous datasets. Although the results of the
highest-ranking team were high for most patients, segmentation especially in
small, pre-treated tumors were insufficient. Therefore, more reliable
segmentation methods are needed to create clinically applicable models to aid
surgical planning in pediatric neuroblastoma.

</details>

### [169] [Cues3D: Unleashing the Power of Sole NeRF for Consistent and Unique Instances in Open-Vocabulary 3D Panoptic Segmentation](https://arxiv.org/abs/2505.00378)
*Feng Xue,Wenzhuang Xu,Guofeng Zhong,Anlong Minga,Nicu Sebe*

Main category: cs.CV

TLDR: Cues3D提出了一种基于NeRF的紧凑方法，用于开放词汇3D全景分割，通过隐式3D场实现全局一致性，无需显式跨视图监督。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖高保真3D点云或跨视图关联预处理，但NeRF的隐式3D场能自然建立全局一致性几何，为对象区分提供优势。

Method: Cues3D采用三阶段训练框架（初始化-消歧-细化），利用NeRF渲染的3D掩码确保全局唯一实例ID。

Result: 在多个数据集上，Cues3D优于基于2D图像的方法，并与最新2D-3D融合方法竞争，甚至在使用额外3D点云时超越它们。

Conclusion: Cues3D通过NeRF的隐式几何一致性，实现了高效且一致的3D实例分割，无需复杂预处理。

Abstract: Open-vocabulary 3D panoptic segmentation has recently emerged as a
significant trend. Top-performing methods currently integrate 2D segmentation
with geometry-aware 3D primitives. However, the advantage would be lost without
high-fidelity 3D point clouds, such as methods based on Neural Radiance Field
(NeRF). These methods are limited by the insufficient capacity to maintain
consistency across partial observations. To address this, recent works have
utilized contrastive loss or cross-view association pre-processing for view
consensus. In contrast to them, we present Cues3D, a compact approach that
relies solely on NeRF instead of pre-associations. The core idea is that NeRF's
implicit 3D field inherently establishes a globally consistent geometry,
enabling effective object distinction without explicit cross-view supervision.
We propose a three-phase training framework for NeRF,
initialization-disambiguation-refinement, whereby the instance IDs are
corrected using the initially-learned knowledge. Additionally, an instance
disambiguation method is proposed to match NeRF-rendered 3D masks and ensure
globally unique 3D instance identities. With the aid of Cues3D, we obtain
highly consistent and unique 3D instance ID for each object across views with a
balanced version of NeRF. Our experiments are conducted on ScanNet v2,
ScanNet200, ScanNet++, and Replica datasets for 3D instance, panoptic, and
semantic segmentation tasks. Cues3D outperforms other 2D image-based methods
and competes with the latest 2D-3D merging based methods, while even surpassing
them when using additional 3D point clouds. The code link could be found in the
appendix and will be released on
\href{https://github.com/mRobotit/Cues3D}{github}

</details>

### [170] [The Invisible Threat: Evaluating the Vulnerability of Cross-Spectral Face Recognition to Presentation Attacks](https://arxiv.org/abs/2505.00380)
*Anjith George,Sebastien Marcel*

Main category: cs.CV

TLDR: 该论文研究了近红外（NIR）与可见光（VIS）跨光谱人脸识别系统对呈现攻击的脆弱性，发现虽然系统有一定可靠性，但仍存在漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管NIR成像在跨光谱人脸识别中具有优势（如光照鲁棒性、抗攻击性等），但其对呈现攻击的鲁棒性尚未被系统研究。

Method: 通过全面评估NIR-VIS跨光谱人脸识别系统对呈现攻击的脆弱性。

Result: 实证结果表明，系统虽有一定可靠性，但对特定攻击仍存在脆弱性。

Conclusion: 需进一步研究以提升NIR-VIS系统的抗攻击能力。

Abstract: Cross-spectral face recognition systems are designed to enhance the
performance of facial recognition systems by enabling cross-modal matching
under challenging operational conditions. A particularly relevant application
is the matching of near-infrared (NIR) images to visible-spectrum (VIS) images,
enabling the verification of individuals by comparing NIR facial captures
acquired with VIS reference images. The use of NIR imaging offers several
advantages, including greater robustness to illumination variations, better
visibility through glasses and glare, and greater resistance to presentation
attacks. Despite these claimed benefits, the robustness of NIR-based systems
against presentation attacks has not been systematically studied in the
literature. In this work, we conduct a comprehensive evaluation into the
vulnerability of NIR-VIS cross-spectral face recognition systems to
presentation attacks. Our empirical findings indicate that, although these
systems exhibit a certain degree of reliability, they remain vulnerable to
specific attacks, emphasizing the need for further research in this area.

</details>

### [171] [SOTA: Spike-Navigated Optimal TrAnsport Saliency Region Detection in Composite-bias Videos](https://arxiv.org/abs/2505.00394)
*Wenxuan Liu,Yao Deng,Kang Chen,Xian Zhong,Zhaofei Yu,Tiejun Huang*

Main category: cs.CV

TLDR: 论文提出了一种基于脉冲相机的显著性检测方法SOTA，通过微偏置和全局偏置模块解决噪声和样本质量问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有显著性检测方法在运动模糊和遮挡场景中表现不佳，而脉冲相机的高时间分辨率虽能提升显著性检测，但其固有噪声和低质量样本会导致检测偏差。

Method: 提出SOTA框架，结合脉冲相机的优势，引入Spike-based Micro-debias (SM)和Spike-based Global-debias (SG)模块，分别处理帧间细微变化和全局不一致性。

Result: 在真实和合成数据集上的实验表明，SOTA能有效消除复合噪声偏差，性能优于现有方法。

Conclusion: SOTA通过优化脉冲相机的显著性检测，解决了噪声和样本质量问题，为实际场景提供了更可靠的解决方案。

Abstract: Existing saliency detection methods struggle in real-world scenarios due to
motion blur and occlusions. In contrast, spike cameras, with their high
temporal resolution, significantly enhance visual saliency maps. However, the
composite noise inherent to spike camera imaging introduces discontinuities in
saliency detection. Low-quality samples further distort model predictions,
leading to saliency bias. To address these challenges, we propose
Spike-navigated Optimal TrAnsport Saliency Region Detection (SOTA), a framework
that leverages the strengths of spike cameras while mitigating biases in both
spatial and temporal dimensions. Our method introduces Spike-based Micro-debias
(SM) to capture subtle frame-to-frame variations and preserve critical details,
even under minimal scene or lighting changes. Additionally, Spike-based
Global-debias (SG) refines predictions by reducing inconsistencies across
diverse conditions. Extensive experiments on real and synthetic datasets
demonstrate that SOTA outperforms existing methods by eliminating composite
noise bias. Our code and dataset will be released at
https://github.com/lwxfight/sota.

</details>

### [172] [Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular Videos](https://arxiv.org/abs/2505.00421)
*Xia Yuan,Hai Yuan,Wenyi Ge,Ying Fu,Xi Wu,Guanyu Xing*

Main category: cs.CV

TLDR: 提出了一种基于2D高斯泼溅（2DGS）的实时框架，用于从单目视频重建高质量、可动画的3D人体化身，解决了现有方法在几何细节捕捉和动画稳定性上的不足。


<details>
  <summary>Details</summary>
Motivation: 减少对复杂硬件的依赖，提升在游戏开发、增强现实和社交媒体等应用中的实用性。

Method: 结合2DGS和全局SMPL姿态参数，提出旋转补偿网络（RCN）学习旋转残差，整合局部几何特征与全局姿态参数。

Result: 成功从单目视频重建逼真且高度可动画的人体化身，细节保留和动画稳定性优于现有方法。

Conclusion: 该方法在重建质量和动画鲁棒性上超越当前最优方法，适用于多种实际应用。

Abstract: High-quality, animatable 3D human avatar reconstruction from monocular videos
offers significant potential for reducing reliance on complex hardware, making
it highly practical for applications in game development, augmented reality,
and social media. However, existing methods still face substantial challenges
in capturing fine geometric details and maintaining animation stability,
particularly under dynamic or complex poses. To address these issues, we
propose a novel real-time framework for animatable human avatar reconstruction
based on 2D Gaussian Splatting (2DGS). By leveraging 2DGS and global SMPL pose
parameters, our framework not only aligns positional and rotational
discrepancies but also enables robust and natural pose-driven animation of the
reconstructed avatars. Furthermore, we introduce a Rotation Compensation
Network (RCN) that learns rotation residuals by integrating local geometric
features with global pose parameters. This network significantly improves the
handling of non-rigid deformations and ensures smooth, artifact-free pose
transitions during animation. Experimental results demonstrate that our method
successfully reconstructs realistic and highly animatable human avatars from
monocular videos, effectively preserving fine-grained details while ensuring
stable and natural pose variation. Our approach surpasses current
state-of-the-art methods in both reconstruction quality and animation
robustness on public benchmarks.

</details>

### [173] [Leveraging Pretrained Diffusion Models for Zero-Shot Part Assembly](https://arxiv.org/abs/2505.00426)
*Ruiyuan Zhang,Qi Wang,Jiaxiang Liu,Yu Zhang,Yuchi Huo,Chao Wu*

Main category: cs.CV

TLDR: 提出了一种零样本3D零件组装方法，利用预训练点云扩散模型作为判别器，通过迭代最近点（ICP）过程解决零件重叠问题，无需大量标注数据。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法依赖大量标注数据且成本高的问题，适应现实世界中形状和零件的多样性。

Method: 利用预训练点云扩散模型作为判别器，结合ICP过程和推离策略处理零件重叠。

Result: 实验表明该方法有效，甚至优于监督学习方法。

Conclusion: 提出的零样本方法在3D零件组装中具有高效性和鲁棒性，适用于大规模应用。

Abstract: 3D part assembly aims to understand part relationships and predict their
6-DoF poses to construct realistic 3D shapes, addressing the growing demand for
autonomous assembly, which is crucial for robots. Existing methods mainly
estimate the transformation of each part by training neural networks under
supervision, which requires a substantial quantity of manually labeled data.
However, the high cost of data collection and the immense variability of
real-world shapes and parts make traditional methods impractical for
large-scale applications. In this paper, we propose first a zero-shot part
assembly method that utilizes pre-trained point cloud diffusion models as
discriminators in the assembly process, guiding the manipulation of parts to
form realistic shapes. Specifically, we theoretically demonstrate that
utilizing a diffusion model for zero-shot part assembly can be transformed into
an Iterative Closest Point (ICP) process. Then, we propose a novel pushing-away
strategy to address the overlap parts, thereby further enhancing the robustness
of the method. To verify our work, we conduct extensive experiments and
quantitative comparisons to several strong baseline methods, demonstrating the
effectiveness of the proposed approach, which even surpasses the supervised
learning method. The code has been released on
https://github.com/Ruiyuan-Zhang/Zero-Shot-Assembly.

</details>

### [174] [ClearLines - Camera Calibration from Straight Lines](https://arxiv.org/abs/2505.00452)
*Gregory Schroeder,Mohamed Sabry,Cristina Olaverri-Monreal*

Main category: cs.CV

TLDR: 论文提出了一个名为“ClearLines”的小型数据集，旨在解决真实世界户外场景中直线校准的难题，并提供了数据集的创建过程以指导算法开发。


<details>
  <summary>Details</summary>
Motivation: 真实世界户外场景中的直线校准问题因复杂环境和缺乏专用数据集而难以解决。

Method: 通过创建“ClearLines”数据集，并详细描述其构建过程，为直线3D检测算法提供实践指导。

Result: 提供了一个可用于算法开发和优化的数据集。

Conclusion: “ClearLines”数据集填补了领域空白，并为直线检测算法的改进提供了实用资源。

Abstract: The problem of calibration from straight lines is fundamental in geometric
computer vision, with well-established theoretical foundations. However, its
practical applicability remains limited, particularly in real-world outdoor
scenarios. These environments pose significant challenges due to diverse and
cluttered scenes, interrupted reprojections of straight 3D lines, and varying
lighting conditions, making the task notoriously difficult. Furthermore, the
field lacks a dedicated dataset encouraging the development of respective
detection algorithms. In this study, we present a small dataset named
"ClearLines", and by detailing its creation process, provide practical insights
that can serve as a guide for developing and refining straight 3D line
detection algorithms.

</details>

### [175] [JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers](https://arxiv.org/abs/2505.00482)
*Kwon Byung-Ki,Qi Dai,Lee Hyoseok,Chong Luo,Tae-Hyun Oh*

Main category: cs.CV

TLDR: JointDiT是一种扩散变换器，用于建模RGB和深度的联合分布，通过自适应调度权重和不平衡时间步采样策略，实现高质量图像和深度图的生成。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用扩散变换器的架构优势，实现RGB和深度联合分布的高效建模，以支持多种生成任务。

Method: 提出自适应调度权重和不平衡时间步采样策略，训练模型处理不同噪声水平，支持联合生成、深度估计和深度条件图像生成。

Result: JointDiT在联合生成任务中表现优异，同时在深度估计和深度条件生成中达到可比结果。

Conclusion: 联合分布建模可以替代条件生成，JointDiT展示了其在多任务生成中的潜力。

Abstract: We present JointDiT, a diffusion transformer that models the joint
distribution of RGB and depth. By leveraging the architectural benefit and
outstanding image prior of the state-of-the-art diffusion transformer, JointDiT
not only generates high-fidelity images but also produces geometrically
plausible and accurate depth maps. This solid joint distribution modeling is
achieved through two simple yet effective techniques that we propose, i.e.,
adaptive scheduling weights, which depend on the noise levels of each modality,
and the unbalanced timestep sampling strategy. With these techniques, we train
our model across all noise levels for each modality, enabling JointDiT to
naturally handle various combinatorial generation tasks, including joint
generation, depth estimation, and depth-conditioned image generation by simply
controlling the timestep of each branch. JointDiT demonstrates outstanding
joint generation performance. Furthermore, it achieves comparable results in
depth estimation and depth-conditioned image generation, suggesting that joint
distribution modeling can serve as a replaceable alternative to conditional
generation. The project page is available at
https://byungki-k.github.io/JointDiT/.

</details>

### [176] [KeySync: A Robust Approach for Leakage-free Lip Synchronization in High Resolution](https://arxiv.org/abs/2505.00497)
*Antoni Bigata,Rodrigo Mira,Stella Bounareli,Michał Stypułkowski,Konstantinos Vougioukas,Stavros Petridis,Maja Pantic*

Main category: cs.CV

TLDR: KeySync是一个两阶段框架，解决了唇同步任务中的时间一致性、表情泄漏和遮挡问题，并提出了新的泄漏度量标准LipLeak。


<details>
  <summary>Details</summary>
Motivation: 唇同步任务在现有研究中常被忽视表情泄漏和遮挡问题，影响实际应用如自动配音。

Method: 采用两阶段框架和精心设计的掩码策略，解决时间一致性、泄漏和遮挡问题。

Result: KeySync在唇部重建和跨同步任务中取得最先进结果，视觉质量提升且泄漏减少。

Conclusion: KeySync通过新掩码方法和架构选择，有效解决了唇同步中的关键问题。

Abstract: Lip synchronization, known as the task of aligning lip movements in an
existing video with new input audio, is typically framed as a simpler variant
of audio-driven facial animation. However, as well as suffering from the usual
issues in talking head generation (e.g., temporal consistency), lip
synchronization presents significant new challenges such as expression leakage
from the input video and facial occlusions, which can severely impact
real-world applications like automated dubbing, but are often neglected in
existing works. To address these shortcomings, we present KeySync, a two-stage
framework that succeeds in solving the issue of temporal consistency, while
also incorporating solutions for leakage and occlusions using a carefully
designed masking strategy. We show that KeySync achieves state-of-the-art
results in lip reconstruction and cross-synchronization, improving visual
quality and reducing expression leakage according to LipLeak, our novel leakage
metric. Furthermore, we demonstrate the effectiveness of our new masking
approach in handling occlusions and validate our architectural choices through
several ablation studies. Code and model weights can be found at
https://antonibigata.github.io/KeySync.

</details>

### [177] [Towards Scalable Human-aligned Benchmark for Text-guided Image Editing](https://arxiv.org/abs/2505.00502)
*Suho Ryu,Kihyun Kim,Eugene Baek,Dongsoo Shin,Joonseok Lee*

Main category: cs.CV

TLDR: 提出了一种新的文本引导图像编辑基准HATIE，用于解决主观评估问题，并提供自动化多维度评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前文本引导图像编辑任务缺乏标准评估方法，依赖人工用户研究，主观性强。

Method: 引入HATIE基准，包含大规模数据集和自动化评估流程，结合多维度评分以对齐人类感知。

Result: 验证了HATIE评估与人类感知一致，并提供了多个先进模型的基准结果。

Conclusion: HATIE为文本引导图像编辑任务提供了可靠且自动化的评估标准。

Abstract: A variety of text-guided image editing models have been proposed recently.
However, there is no widely-accepted standard evaluation method mainly due to
the subjective nature of the task, letting researchers rely on manual user
study. To address this, we introduce a novel Human-Aligned benchmark for
Text-guided Image Editing (HATIE). Providing a large-scale benchmark set
covering a wide range of editing tasks, it allows reliable evaluation, not
limited to specific easy-to-evaluate cases. Also, HATIE provides a
fully-automated and omnidirectional evaluation pipeline. Particularly, we
combine multiple scores measuring various aspects of editing so as to align
with human perception. We empirically verify that the evaluation of HATIE is
indeed human-aligned in various aspects, and provide benchmark results on
several state-of-the-art models to provide deeper insights on their
performance.

</details>

### [178] [HeAL3D: Heuristical-enhanced Active Learning for 3D Object Detection](https://arxiv.org/abs/2505.00507)
*Esteban Rivera,Surya Prabhakaran,Markus Lienkamp*

Main category: cs.CV

TLDR: HeAL（启发式增强的主动学习）通过结合启发式特征（如物体距离和点数量）与定位和分类，提升了3D目标检测中样本选择的效果，仅用24%的样本即达到全监督基线的性能。


<details>
  <summary>Details</summary>
Motivation: 主动学习在自动驾驶模型训练中具有重要作用，但现有方法在非受控场景下的样本选择仍具挑战性，且忽视了3D检测模型的实践启发。

Method: 提出HeAL方法，整合启发式特征（如物体距离和点数量）与定位和分类，以选择对模型训练贡献最大的样本。

Result: 在KITTI数据集上，HeAL的mAP与最先进方法相当，仅用24%的样本即达到全监督基线的性能。

Conclusion: HeAL通过启发式特征显著提升了样本选择效率，为3D目标检测的主动学习提供了实用解决方案。

Abstract: Active Learning has proved to be a relevant approach to perform sample
selection for training models for Autonomous Driving. Particularly, previous
works on active learning for 3D object detection have shown that selection of
samples in uncontrolled scenarios is challenging. Furthermore, current
approaches focus exclusively on the theoretical aspects of the sample selection
problem but neglect the practical insights that can be obtained from the
extensive literature and application of 3D detection models. In this paper, we
introduce HeAL (Heuristical-enhanced Active Learning for 3D Object Detection)
which integrates those heuristical features together with Localization and
Classification to deliver the most contributing samples to the model's
training. In contrast to previous works, our approach integrates heuristical
features such as object distance and point-quantity to estimate the
uncertainty, which enhance the usefulness of selected samples to train
detection models. Our quantitative evaluation on KITTI shows that HeAL presents
competitive mAP with respect to the State-of-the-Art, and achieves the same mAP
as the full-supervised baseline with only 24% of the samples.

</details>

### [179] [Inconsistency-based Active Learning for LiDAR Object Detection](https://arxiv.org/abs/2505.00511)
*Esteban Rivera,Loic Stratil,Markus Lienkamp*

Main category: cs.CV

TLDR: 论文研究了在自动驾驶中基于LiDAR的主动学习策略，通过不一致性样本选择方法，仅用50%标注数据即可达到随机采样的mAP性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型需要大量标注数据，成本高昂，因此需要优化数据获取和标注过程。

Method: 扩展主动学习概念至LiDAR领域，开发基于不一致性的样本选择策略。

Result: 使用基于检测框数量的简单不一致性方法，仅需50%标注数据即可达到随机采样的mAP性能。

Conclusion: 主动学习在LiDAR领域具有潜力，可显著减少标注数据需求。

Abstract: Deep learning models for object detection in autonomous driving have recently
achieved impressive performance gains and are already being deployed in
vehicles worldwide. However, current models require increasingly large datasets
for training. Acquiring and labeling such data is costly, necessitating the
development of new strategies to optimize this process. Active learning is a
promising approach that has been extensively researched in the image domain. In
our work, we extend this concept to the LiDAR domain by developing several
inconsistency-based sample selection strategies and evaluate their
effectiveness in various settings. Our results show that using a naive
inconsistency approach based on the number of detected boxes, we achieve the
same mAP as the random sampling strategy with 50% of the labeled data.

</details>

### [180] [InterLoc: LiDAR-based Intersection Localization using Road Segmentation with Automated Evaluation Method](https://arxiv.org/abs/2505.00512)
*Nguyen Hoang Khoi Tran,Julie Stephany Berrio,Mao Shan,Zhenxing Ming,Stewart Worrall*

Main category: cs.CV

TLDR: 提出了一种基于LiDAR的交叉口检测方法，融合语义道路分割与车辆定位，并通过最小二乘法优化分支拓扑，性能优于现有学习基线。


<details>
  <summary>Details</summary>
Motivation: 交叉口是道路网络的关键点，但现有检测方法忽略车载语义信息或依赖稀缺标注数据，需填补这一空白。

Method: 方法分为两步：(i) 在鸟瞰图中融合语义分割与定位检测候选交叉口，(ii) 用最小二乘法分析分支拓扑优化候选。

Result: 在SemanticKITTI数据集上，平均定位误差1.9米，精度89%，召回率77%（5米容忍度），优于最新学习基线。

Conclusion: 该方法对分割错误鲁棒，适用于实际场景，且性能显著提升。

Abstract: Intersections are geometric and functional key points in every road network.
They offer strong landmarks to correct GNSS dropouts and anchor new sensor data
in up-to-date maps. Despite that importance, intersection detectors either
ignore the rich semantic information already computed onboard or depend on
scarce, hand-labeled intersection datasets. To close that gap, this paper
presents a LiDAR-based method for intersection detection that (i) fuses
semantic road segmentation with vehicle localization to detect intersection
candidates in a bird's eye view (BEV) representation and (ii) refines those
candidates by analyzing branch topology with a least squares formulation. To
evaluate our method, we introduce an automated benchmarking pipeline that pairs
detections with OpenStreetMap (OSM) intersection nodes using precise GNSS/INS
ground-truth poses. Tested on eight SemanticKITTI sequences, the approach
achieves a mean localization error of 1.9 m, 89% precision, and 77% recall at a
5 m tolerance, outperforming the latest learning-based baseline. Moreover, the
method is robust to segmentation errors higher than those of the benchmark
model, demonstrating its applicability in the real world.

</details>

### [181] [A Robust Deep Networks based Multi-Object MultiCamera Tracking System for City Scale Traffic](https://arxiv.org/abs/2505.00534)
*Muhammad Imran Zaman,Usama Ijaz Bajwa,Gulshan Saleem,Rana Hammad Raza*

Main category: cs.CV

TLDR: 提出了一种基于深度学习的多目标多摄像头跟踪框架，用于解决城市交通场景中的车辆跟踪问题，并在AI City Challenge数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着网络摄像头的增加，手动跟踪和匹配多摄像头中的车辆在复杂城市交通场景中面临巨大挑战，如遮挡、光照变化等。

Method: 使用Mask R-CNN进行目标检测，结合NMS选择目标；采用迁移学习进行重识别；利用ResNet-152和Deep SORT进行特征提取和跟踪。

Result: 在AI City Challenge数据集上，IDF1得分为0.8289，精确率和召回率分别为0.9026和0.8527。

Conclusion: 该框架在复杂场景下实现了鲁棒且准确的车辆跟踪，具有竞争力。

Abstract: Vision sensors are becoming more important in Intelligent Transportation
Systems (ITS) for traffic monitoring, management, and optimization as the
number of network cameras continues to rise. However, manual object tracking
and matching across multiple non-overlapping cameras pose significant
challenges in city-scale urban traffic scenarios. These challenges include
handling diverse vehicle attributes, occlusions, illumination variations,
shadows, and varying video resolutions. To address these issues, we propose an
efficient and cost-effective deep learning-based framework for Multi-Object
Multi-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for
object detection and employs Non-Maximum Suppression (NMS) to select target
objects from overlapping detections. Transfer learning is employed for
re-identification, enabling the association and generation of vehicle tracklets
across multiple cameras. Moreover, we leverage appropriate loss functions and
distance measures to handle occlusion, illumination, and shadow challenges. The
final solution identification module performs feature extraction using
ResNet-152 coupled with Deep SORT based vehicle tracking. The proposed
framework is evaluated on the 5th AI City Challenge dataset (Track 3),
comprising 46 camera feeds. Among these 46 camera streams, 40 are used for
model training and validation, while the remaining six are utilized for model
testing. The proposed framework achieves competitive performance with an IDF1
score of 0.8289, and precision and recall scores of 0.9026 and 0.8527
respectively, demonstrating its effectiveness in robust and accurate vehicle
tracking.

</details>

### [182] [X-ray illicit object detection using hybrid CNN-transformer neural network architectures](https://arxiv.org/abs/2505.00564)
*Jorgen Cani,Christos Diou,Spyridon Evangelatos,Panagiotis Radoglou-Grammatikis,Vasileios Argyriou,Panagiotis Sarigiannidis,Iraklis Varlamis,Georgios Th. Papadopoulos*

Main category: cs.CV

TLDR: 论文探讨了在X射线安全应用中，结合CNN和Transformer的混合架构在检测遮挡或隐藏物体时的性能表现，并与传统CNN方法（如YOLOv8）进行了对比。


<details>
  <summary>Details</summary>
Motivation: 在X射线安全成像领域，现有方法主要依赖CNN，而CNN与Transformer的结合尚未充分探索。研究旨在评估混合架构在复杂场景下的鲁棒性。

Method: 通过结合CNN（HGNetV2）和混合CNN-Transformer（Next-ViT-S）骨干网络，与不同检测头（YOLOv8和RT-DETR）进行实验，并在三个公开数据集（EDS、HiXray、PIDray）上评估性能。

Result: 混合架构在数据分布变化（EDS数据集）时表现更鲁棒，而YOLOv8在HiXray和PIDray数据集上表现更优。

Conclusion: 混合架构在特定场景下具有优势，研究结果为未来X射线安全检测的架构选择提供了指导。

Abstract: In the field of X-ray security applications, even the smallest details can
significantly impact outcomes. Objects that are heavily occluded or
intentionally concealed pose a great challenge for detection, whether by human
observation or through advanced technological applications. While certain Deep
Learning (DL) architectures demonstrate strong performance in processing local
information, such as Convolutional Neural Networks (CNNs), others excel in
handling distant information, e.g., transformers. In X-ray security imaging the
literature has been dominated by the use of CNN-based methods, while the
integration of the two aforementioned leading architectures has not been
sufficiently explored. In this paper, various hybrid CNN-transformer
architectures are evaluated against a common CNN object detection baseline,
namely YOLOv8. In particular, a CNN (HGNetV2) and a hybrid CNN-transformer
(Next-ViT-S) backbone are combined with different CNN/transformer detection
heads (YOLOv8 and RT-DETR). The resulting architectures are comparatively
evaluated on three challenging public X-ray inspection datasets, namely EDS,
HiXray, and PIDray. Interestingly, while the YOLOv8 detector with its default
backbone (CSP-DarkNet53) is generally shown to be advantageous on the HiXray
and PIDray datasets, when a domain distribution shift is incorporated in the
X-ray images (as happens in the EDS datasets), hybrid CNN-transformer
architectures exhibit increased robustness. Detailed comparative evaluation
results, including object-level detection performance and object-size error
analysis, demonstrate the strengths and weaknesses of each architectural
combination and suggest guidelines for future research. The source code and
network weights of the models employed in this study are available at
https://github.com/jgenc/xray-comparative-evaluation.

</details>

### [183] [Multimodal Masked Autoencoder Pre-training for 3D MRI-Based Brain Tumor Analysis with Missing Modalities](https://arxiv.org/abs/2505.00568)
*Lucas Robinet,Ahmad Berjaoui,Elizabeth Cohen-Jonathan Moyal*

Main category: cs.CV

TLDR: BM-MAE是一种针对多模态MRI数据的预训练策略，能够适应任何可用的模态组合，无需为每个子集单独训练模型。


<details>
  <summary>Details</summary>
Motivation: 在医学影像中，标注数据稀缺，且多模态数据常因采集问题缺失，传统方法需为每种模态组合单独训练模型，资源消耗大。

Method: 提出BM-MAE，一种掩码图像建模预训练策略，可提取模态内和模态间信息，适应任意模态子集。

Result: 实验表明，BM-MAE优于或与基线方法相当，且能高效重建缺失模态。

Conclusion: BM-MAE为多模态MRI数据提供了一种高效、灵活的预训练解决方案，具有实际应用价值。

Abstract: Multimodal magnetic resonance imaging (MRI) constitutes the first line of
investigation for clinicians in the care of brain tumors, providing crucial
insights for surgery planning, treatment monitoring, and biomarker
identification. Pre-training on large datasets have been shown to help models
learn transferable representations and adapt with minimal labeled data. This
behavior is especially valuable in medical imaging, where annotations are often
scarce. However, applying this paradigm to multimodal medical data introduces a
challenge: most existing approaches assume that all imaging modalities are
available during both pre-training and fine-tuning. In practice, missing
modalities often occur due to acquisition issues, specialist unavailability, or
specific experimental designs on small in-house datasets. Consequently, a
common approach involves training a separate model for each desired modality
combination, making the process both resource-intensive and impractical for
clinical use. Therefore, we introduce BM-MAE, a masked image modeling
pre-training strategy tailored for multimodal MRI data. The same pre-trained
model seamlessly adapts to any combination of available modalities, extracting
rich representations that capture both intra- and inter-modal information. This
allows fine-tuning on any subset of modalities without requiring architectural
changes, while still benefiting from a model pre-trained on the full set of
modalities. Extensive experiments show that the proposed pre-training strategy
outperforms or remains competitive with baselines that require separate
pre-training for each modality subset, while substantially surpassing training
from scratch on several downstream tasks. Additionally, it can quickly and
efficiently reconstruct missing modalities, highlighting its practical value.
Code and trained models are available at: https://github.com/Lucas-rbnt/bmmae

</details>

### [184] [AnimalMotionCLIP: Embedding motion in CLIP for Animal Behavior Analysis](https://arxiv.org/abs/2505.00569)
*Enmin Zhong,Carlos R. del-Blanco,Daniel Berjón,Fernando Jaureguizar,Narciso García*

Main category: cs.CV

TLDR: 提出AnimalMotionCLIP，结合CLIP框架与光流信息解决动物行为识别中的运动信息整合与时间建模问题，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型（如CLIP）在动物行为识别中泛化能力强，但需解决运动信息整合与时间建模的挑战。

Method: 在CLIP框架中交替使用视频帧与光流信息，并比较密集、半密集和稀疏三种分类器聚合的时间建模方案。

Result: 在Animal Kingdom数据集上表现优于现有方法，能准确识别精细时间动作。

Conclusion: AnimalMotionCLIP有效解决了动物行为识别中的关键问题，性能优越。

Abstract: Recently, there has been a surge of interest in applying deep learning
techniques to animal behavior recognition, particularly leveraging pre-trained
visual language models, such as CLIP, due to their remarkable generalization
capacity across various downstream tasks. However, adapting these models to the
specific domain of animal behavior recognition presents two significant
challenges: integrating motion information and devising an effective temporal
modeling scheme. In this paper, we propose AnimalMotionCLIP to address these
challenges by interleaving video frames and optical flow information in the
CLIP framework. Additionally, several temporal modeling schemes using an
aggregation of classifiers are proposed and compared: dense, semi dense, and
sparse. As a result, fine temporal actions can be correctly recognized, which
is of vital importance in animal behavior analysis. Experiments on the Animal
Kingdom dataset demonstrate that AnimalMotionCLIP achieves superior performance
compared to state-of-the-art approaches.

</details>

### [185] [Synthesizing and Identifying Noise Levels in Autonomous Vehicle Camera Radar Datasets](https://arxiv.org/abs/2505.00584)
*Mathis Morales,Golnaz Habibi*

Main category: cs.CV

TLDR: 论文提出了一种用于自动驾驶车辆（AV）数据集的合成数据增强方法，旨在模拟传感器故障和数据退化，以提高检测和跟踪管道的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数目标检测方法关注性能指标，而忽略了传感器故障对检测和跟踪鲁棒性的影响。

Method: 通过创建合成数据增强管道，模拟相机和雷达传感器的故障及数据退化，并训练了一个轻量级噪声识别神经网络。

Result: 在增强数据集上训练的噪声识别神经网络在11个类别上达到了54.4%的识别准确率。

Conclusion: 合成数据增强方法可以有效提高检测和跟踪管道的鲁棒性，尤其是在传感器故障情况下。

Abstract: Detecting and tracking objects is a crucial component of any autonomous
navigation method. For the past decades, object detection has yielded promising
results using neural networks on various datasets. While many methods focus on
performance metrics, few projects focus on improving the robustness of these
detection and tracking pipelines, notably to sensor failures. In this paper we
attempt to address this issue by creating a realistic synthetic data
augmentation pipeline for camera-radar Autonomous Vehicle (AV) datasets. Our
goal is to accurately simulate sensor failures and data deterioration due to
real-world interferences. We also present our results of a baseline lightweight
Noise Recognition neural network trained and tested on our augmented dataset,
reaching an overall recognition accuracy of 54.4\% on 11 categories across
10086 images and 2145 radar point-clouds.

</details>

### [186] [Uncertainty-Aware Multi-Expert Knowledge Distillation for Imbalanced Disease Grading](https://arxiv.org/abs/2505.00592)
*Shuo Tong,Shangde Gao,Ke Liu,Zihang Huang,Hongxia Xu,Haochao Ying,Jian Wu*

Main category: cs.CV

TLDR: 提出了一种不确定性感知的多专家知识蒸馏框架（UMKD），用于解决疾病图像分级中的领域偏移和数据不平衡问题，通过特征解耦和动态权重调整实现鲁棒知识迁移。


<details>
  <summary>Details</summary>
Motivation: 自动疾病图像分级在医疗AI中具有重要意义，但领域偏移和数据不平衡导致模型偏差，影响临床部署。

Method: UMKD框架通过特征解耦（任务无关和任务特定特征）和不确定性感知的解耦蒸馏（UDD）动态调整知识迁移权重，解决模型异构性和分布差异。

Result: 在组织学前列腺分级（SICAPv2）和眼底图像分级（APTOS）实验中，UMKD在源不平衡和目标不平衡场景下均达到最优性能。

Conclusion: UMKD为实际疾病图像分级提供了鲁棒且实用的解决方案，显著提升了模型的泛化能力和可靠性。

Abstract: Automatic disease image grading is a significant application of artificial
intelligence for healthcare, enabling faster and more accurate patient
assessments. However, domain shifts, which are exacerbated by data imbalance,
introduce bias into the model, posing deployment difficulties in clinical
applications. To address the problem, we propose a novel
\textbf{U}ncertainty-aware \textbf{M}ulti-experts \textbf{K}nowledge
\textbf{D}istillation (UMKD) framework to transfer knowledge from multiple
expert models to a single student model. Specifically, to extract
discriminative features, UMKD decouples task-agnostic and task-specific
features with shallow and compact feature alignment in the feature space. At
the output space, an uncertainty-aware decoupled distillation (UDD) mechanism
dynamically adjusts knowledge transfer weights based on expert model
uncertainties, ensuring robust and reliable distillation. Additionally, UMKD
also tackles the problems of model architecture heterogeneity and distribution
discrepancies between source and target domains, which are inadequately tackled
by previous KD approaches. Extensive experiments on histology prostate grading
(\textit{SICAPv2}) and fundus image grading (\textit{APTOS}) demonstrate that
UMKD achieves a new state-of-the-art in both source-imbalanced and
target-imbalanced scenarios, offering a robust and practical solution for
real-world disease image grading.

</details>

### [187] [Visual Trajectory Prediction of Vessels for Inland Navigation](https://arxiv.org/abs/2505.00599)
*Alexander Puzicha,Konstantin Wüstefeld,Kathrin Wilms,Frank Weichert*

Main category: cs.CV

TLDR: 论文探讨了内河导航中基于视频的船舶轨迹预测方法，结合目标检测、卡尔曼滤波和样条插值，改进了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 内河导航的未来依赖自主系统和远程操作，需要精确的船舶轨迹预测。现有检测系统因复杂环境常误分类物体。

Method: 整合先进目标检测、卡尔曼滤波和样条插值，比较了BoT-SORT、Deep OC-SORT和ByeTrack等跟踪算法。

Result: 实验表明卡尔曼滤波能提供平滑轨迹，预测准确性提升，对碰撞避免和态势感知至关重要。

Conclusion: 需定制数据集和模型，未来将扩展数据集并加入船舶分类，以优化预测，支持复杂环境下的自主系统和人工操作。

Abstract: The future of inland navigation increasingly relies on autonomous systems and
remote operations, emphasizing the need for accurate vessel trajectory
prediction. This study addresses the challenges of video-based vessel tracking
and prediction by integrating advanced object detection methods, Kalman
filters, and spline-based interpolation. However, existing detection systems
often misclassify objects in inland waterways due to complex surroundings. A
comparative evaluation of tracking algorithms, including BoT-SORT, Deep
OC-SORT, and ByeTrack, highlights the robustness of the Kalman filter in
providing smoothed trajectories. Experimental results from diverse scenarios
demonstrate improved accuracy in predicting vessel movements, which is
essential for collision avoidance and situational awareness. The findings
underline the necessity of customized datasets and models for inland
navigation. Future work will expand the datasets and incorporate vessel
classification to refine predictions, supporting both autonomous systems and
human operators in complex environments.

</details>

### [188] [Dietary Intake Estimation via Continuous 3D Reconstruction of Food](https://arxiv.org/abs/2505.00606)
*Wallace Lee,YuHao Chen*

Main category: cs.CV

TLDR: 提出了一种基于单目2D视频构建3D食物模型的方法，用于精确监测饮食行为，避免传统自我报告的不准确性。


<details>
  <summary>Details</summary>
Motivation: 传统饮食监测方法依赖自我报告，存在不准确性，可能导致健康风险。

Method: 利用COLMAP和姿态估计算法从2D视频生成3D食物模型，观察食物体积变化。

Result: 实验证明该方法在玩具模型和真实食物中有效，并提出了自动状态识别的新方法。

Conclusion: 3D重建方法有望为自动化饮食监测工具提供准确数据。

Abstract: Monitoring dietary habits is crucial for preventing health risks associated
with overeating and undereating, including obesity, diabetes, and
cardiovascular diseases. Traditional methods for tracking food intake rely on
self-reported data before or after the eating, which are prone to inaccuracies.
This study proposes an approach to accurately monitor ingest behaviours by
leveraging 3D food models constructed from monocular 2D video. Using COLMAP and
pose estimation algorithms, we generate detailed 3D representations of food,
allowing us to observe changes in food volume as it is consumed. Experiments
with toy models and real food items demonstrate the approach's potential.
Meanwhile, we have proposed a new methodology for automated state recognition
challenges to accurately detect state changes and maintain model fidelity. The
3D reconstruction approach shows promise in capturing comprehensive dietary
behaviour insights, ultimately contributing to the development of automated and
accurate dietary monitoring tools.

</details>

### [189] [Pixel3DMM: Versatile Screen-Space Priors for Single-Image 3D Face Reconstruction](https://arxiv.org/abs/2505.00615)
*Simon Giebenhain,Tobias Kirschstein,Martin Rünz,Lourdes Agapito,Matthias Nießner*

Main category: cs.CV

TLDR: 论文提出Pixel3DMM方法，通过单张RGB图像实现3D人脸重建，利用DINO基础模型的潜在特征和定制化的表面法线预测头，优化3DMM参数，并在新基准测试中表现优于基线15%。


<details>
  <summary>Details</summary>
Motivation: 解决单张RGB图像重建3D人脸的挑战，提升几何精度，尤其是在复杂表情和视角下的表现。

Method: 使用Pixel3DMM（基于视觉变换器）预测像素级几何线索，结合DINO模型特征和FLAME网格拓扑优化3DMM参数。

Result: 在包含高多样性表情和视角的新基准测试中，几何精度优于基线15%。

Conclusion: Pixel3DMM方法在单图像3D人脸重建中表现出色，尤其在复杂表情下具有显著优势。

Abstract: We address the 3D reconstruction of human faces from a single RGB image. To
this end, we propose Pixel3DMM, a set of highly-generalized vision transformers
which predict per-pixel geometric cues in order to constrain the optimization
of a 3D morphable face model (3DMM). We exploit the latent features of the DINO
foundation model, and introduce a tailored surface normal and uv-coordinate
prediction head. We train our model by registering three high-quality 3D face
datasets against the FLAME mesh topology, which results in a total of over
1,000 identities and 976K images. For 3D face reconstruction, we propose a
FLAME fitting opitmization that solves for the 3DMM parameters from the
uv-coordinate and normal estimates. To evaluate our method, we introduce a new
benchmark for single-image face reconstruction, which features high diversity
facial expressions, viewing angles, and ethnicities. Crucially, our benchmark
is the first to evaluate both posed and neutral facial geometry. Ultimately,
our method outperforms the most competitive baselines by over 15% in terms of
geometric accuracy for posed facial expressions.

</details>

### [190] [Diverse Semantics-Guided Feature Alignment and Decoupling for Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2505.00619)
*Neng Dong,Shuanglin Yan,Liyan Zhang,Jinhui Tang*

Main category: cs.CV

TLDR: 提出DSFAD网络，通过文本嵌入空间对齐跨模态特征并解耦无关特征，提升VI-ReID性能。


<details>
  <summary>Details</summary>
Motivation: 解决可见光与红外图像模态差异大、风格噪声影响特征判别性的问题。

Method: 设计DSFA模块对齐特征，SMFD模块解耦风格信息，SCFR模块恢复行人语义。

Result: 在三个VI-ReID数据集上表现优越。

Conclusion: DSFAD有效提升跨模态行人重识别性能。

Abstract: Visible-Infrared Person Re-Identification (VI-ReID) is a challenging task due
to the large modality discrepancy between visible and infrared images, which
complicates the alignment of their features into a suitable common space.
Moreover, style noise, such as illumination and color contrast, reduces the
identity discriminability and modality invariance of features. To address these
challenges, we propose a novel Diverse Semantics-guided Feature Alignment and
Decoupling (DSFAD) network to align identity-relevant features from different
modalities into a textual embedding space and disentangle identity-irrelevant
features within each modality. Specifically, we develop a Diverse
Semantics-guided Feature Alignment (DSFA) module, which generates pedestrian
descriptions with diverse sentence structures to guide the cross-modality
alignment of visual features. Furthermore, to filter out style information, we
propose a Semantic Margin-guided Feature Decoupling (SMFD) module, which
decomposes visual features into pedestrian-related and style-related
components, and then constrains the similarity between the former and the
textual embeddings to be at least a margin higher than that between the latter
and the textual embeddings. Additionally, to prevent the loss of pedestrian
semantics during feature decoupling, we design a Semantic Consistency-guided
Feature Restitution (SCFR) module, which further excavates useful information
for identification from the style-related features and restores it back into
the pedestrian-related features, and then constrains the similarity between the
features after restitution and the textual embeddings to be consistent with
that between the features before decoupling and the textual embeddings.
Extensive experiments on three VI-ReID datasets demonstrate the superiority of
our DSFAD.

</details>

### [191] [Brain Foundation Models with Hypergraph Dynamic Adapter for Brain Disease Analysis](https://arxiv.org/abs/2505.00627)
*Zhongying Deng,Haoyu Wang,Ziyan Huang,Lipei Zhang,Angelica I. Aviles-Rivero,Chaoyu Liu,Junjun He,Zoe Kourtzi,Carola-Bibiane Schönlieb*

Main category: cs.CV

TLDR: SAM-Brain3D和HyDA提出了一种针对脑部疾病的多模态、多尺度动态基础模型，显著提升了脑部疾病分割和分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前脑部基础模型受限于任务和数据同质性，泛化能力有限，难以适应多样化的临床任务。

Method: 提出SAM-Brain3D（基于14种MRI子模态的脑部基础模型）和HyDA（轻量级适配器），通过超图融合多模态数据并动态生成患者特异性卷积核。

Result: 实验表明，该方法在多种脑部疾病任务中优于现有最优方法。

Conclusion: 该框架为脑部疾病分析提供了新的多模态、多尺度和动态建模范式。

Abstract: Brain diseases, such as Alzheimer's disease and brain tumors, present
profound challenges due to their complexity and societal impact. Recent
advancements in brain foundation models have shown significant promise in
addressing a range of brain-related tasks. However, current brain foundation
models are limited by task and data homogeneity, restricted generalization
beyond segmentation or classification, and inefficient adaptation to diverse
clinical tasks. In this work, we propose SAM-Brain3D, a brain-specific
foundation model trained on over 66,000 brain image-label pairs across 14 MRI
sub-modalities, and Hypergraph Dynamic Adapter (HyDA), a lightweight adapter
for efficient and effective downstream adaptation. SAM-Brain3D captures
detailed brain-specific anatomical and modality priors for segmenting diverse
brain targets and broader downstream tasks. HyDA leverages hypergraphs to fuse
complementary multi-modal data and dynamically generate patient-specific
convolutional kernels for multi-scale feature fusion and personalized
patient-wise adaptation. Together, our framework excels across a broad spectrum
of brain disease segmentation and classification tasks. Extensive experiments
demonstrate that our method consistently outperforms existing state-of-the-art
approaches, offering a new paradigm for brain disease analysis through
multi-modal, multi-scale, and dynamic foundation modeling.

</details>

### [192] [Vision Mamba in Remote Sensing: A Comprehensive Survey of Techniques, Applications and Outlook](https://arxiv.org/abs/2505.00630)
*Muyi Bao,Shuchang Lyu,Zhaoyang Xu,Huiyu Zhou,Jinchang Ren,Shiming Xiang,Xiangtai Li,Guangliang Cheng*

Main category: cs.CV

TLDR: 该论文综述了Mamba架构在遥感领域的应用，分析了120项研究，提出了五个维度的贡献，并提供了开源资源。


<details>
  <summary>Details</summary>
Motivation: 解决CNN和ViT在遥感数据中的局限性，探索Mamba架构的潜力。

Method: 系统综述120项研究，构建分类体系，分析Mamba的创新与应用。

Result: Mamba结合线性计算和全局上下文建模，在遥感任务中表现优异。

Conclusion: Mamba是遥感分析的变革性框架，为未来研究提供了基础和开源资源。

Abstract: Deep learning has profoundly transformed remote sensing, yet prevailing
architectures like Convolutional Neural Networks (CNNs) and Vision Transformers
(ViTs) remain constrained by critical trade-offs: CNNs suffer from limited
receptive fields, while ViTs grapple with quadratic computational complexity,
hindering their scalability for high-resolution remote sensing data. State
Space Models (SSMs), particularly the recently proposed Mamba architecture,
have emerged as a paradigm-shifting solution, combining linear computational
scaling with global context modeling. This survey presents a comprehensive
review of Mamba-based methodologies in remote sensing, systematically analyzing
about 120 studies to construct a holistic taxonomy of innovations and
applications. Our contributions are structured across five dimensions: (i)
foundational principles of vision Mamba architectures, (ii) micro-architectural
advancements such as adaptive scan strategies and hybrid SSM formulations,
(iii) macro-architectural integrations, including CNN-Transformer-Mamba hybrids
and frequency-domain adaptations, (iv) rigorous benchmarking against
state-of-the-art methods in multiple application tasks, such as object
detection, semantic segmentation, change detection, etc. and (v) critical
analysis of unresolved challenges with actionable future directions. By
bridging the gap between SSM theory and remote sensing practice, this survey
establishes Mamba as a transformative framework for remote sensing analysis. To
our knowledge, this paper is the first systematic review of Mamba architectures
in remote sensing. Our work provides a structured foundation for advancing
research in remote sensing systems through SSM-based methods. We curate an
open-source repository
(https://github.com/BaoBao0926/Awesome-Mamba-in-Remote-Sensing) to foster
community-driven advancements.

</details>

### [193] [Deep Reinforcement Learning for Urban Air Quality Management: Multi-Objective Optimization of Pollution Mitigation Booth Placement in Metropolitan Environments](https://arxiv.org/abs/2505.00668)
*Kirtan Rajesh,Suvidha Rupesh Kumar*

Main category: cs.CV

TLDR: 论文提出了一种基于深度强化学习（DRL）的框架，用于优化德里市空气净化亭的布局，以改善空气质量指数（AQI）。


<details>
  <summary>Details</summary>
Motivation: 德里是全球污染最严重的城市之一，传统静态空气净化设施的布局效果不佳，亟需动态优化的解决方案。

Method: 采用近端策略优化（PPO）算法，结合人口密度、交通模式等空间和环境因素，迭代学习最优布局。

Result: 实验表明，DRL方法在AQI改善、空间覆盖等方面优于传统方法，实现了均衡高效的空气净化设施分布。

Conclusion: AI驱动的空间优化在智慧城市和空气质量管理的应用中具有巨大潜力。

Abstract: Urban air pollution remains a pressing global concern, particularly in
densely populated and traffic-intensive metropolitan areas like Delhi, where
exposure to harmful pollutants severely impacts public health. Delhi, being one
of the most polluted cities globally, experiences chronic air quality issues
due to vehicular emissions, industrial activities, and construction dust, which
exacerbate its already fragile atmospheric conditions. Traditional pollution
mitigation strategies, such as static air purifying installations, often fail
to maximize their impact due to suboptimal placement and limited adaptability
to dynamic urban environments. This study presents a novel deep reinforcement
learning (DRL) framework to optimize the placement of air purification booths
to improve the air quality index (AQI) in the city of Delhi. We employ Proximal
Policy Optimization (PPO), a state-of-the-art reinforcement learning algorithm,
to iteratively learn and identify high-impact locations based on multiple
spatial and environmental factors, including population density, traffic
patterns, industrial influence, and green space constraints. Our approach is
benchmarked against conventional placement strategies, including random and
greedy AQI-based methods, using multi-dimensional performance evaluation
metrics such as AQI improvement, spatial coverage, population and traffic
impact, and spatial entropy. Experimental results demonstrate that the RL-based
approach outperforms baseline methods by achieving a balanced and effective
distribution of air purification infrastructure. Notably, the DRL framework
achieves an optimal trade-off between AQI reduction and high-coverage
deployment, ensuring equitable environmental benefits across urban regions. The
findings underscore the potential of AI-driven spatial optimization in
advancing smart city initiatives and data-driven urban air quality management.

</details>

### [194] [Visual Test-time Scaling for GUI Agent Grounding](https://arxiv.org/abs/2505.00684)
*Tiange Luo,Lajanugen Logeswaran,Justin Johnson,Honglak Lee*

Main category: cs.CV

TLDR: RegionFocus是一种视觉测试时缩放方法，通过动态聚焦相关区域减少背景干扰，提升视觉语言模型代理的准确性。


<details>
  <summary>Details</summary>
Motivation: 理解网页因GUI图像的视觉复杂性和大量界面元素而具有挑战性，导致动作选择困难。

Method: 提出动态缩放相关区域的方法，并引入图像作为地图的机制，可视化关键地标以透明记录动作。

Result: 在Screenspot-pro和WebVoyager基准测试中，性能分别提升28%和24%，并在ScreenSpot-Pro上达到61.6%的最新性能。

Conclusion: RegionFocus在交互式环境中通过视觉测试时缩放显著提升了视觉语言模型代理的性能。

Abstract: We introduce RegionFocus, a visual test-time scaling approach for Vision
Language Model Agents. Understanding webpages is challenging due to the visual
complexity of GUI images and the large number of interface elements, making
accurate action selection difficult. Our approach dynamically zooms in on
relevant regions, reducing background clutter and improving grounding accuracy.
To support this process, we propose an image-as-map mechanism that visualizes
key landmarks at each step, providing a transparent action record and enables
the agent to effectively choose among action candidates. Even with a simple
region selection strategy, we observe significant performance gains of 28+\% on
Screenspot-pro and 24+\% on WebVoyager benchmarks on top of two
state-of-the-art open vision language model agents, UI-TARS and Qwen2.5-VL,
highlighting the effectiveness of visual test-time scaling in interactive
settings. We achieve a new state-of-the-art grounding performance of 61.6\% on
the ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model.
Our code will be released publicly at https://github.com/tiangeluo/RegionFocus.

</details>

### [195] [Towards Autonomous Micromobility through Scalable Urban Simulation](https://arxiv.org/abs/2505.00690)
*Wayne Wu,Honglin He,Chaoyuan Zhang,Jack He,Seth Z. Zhao,Ran Gong,Quanyi Li,Bolei Zhou*

Main category: cs.CV

TLDR: 论文提出了一种可扩展的模拟解决方案URBAN-SIM和基准测试URBAN-BENCH，用于推动自主微移动技术的发展，通过模拟和任务评估提升AI代理的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前微移动设备依赖人工操作，存在安全和效率问题，AI辅助操作是潜在解决方案。

Method: 构建URBAN-SIM平台（包含三个关键模块）和URBAN-BENCH任务套件，评估AI代理在微移动中的能力。

Result: 实验评估了四种机器人在不同任务中的表现，揭示了各自的优势和局限。

Conclusion: URBAN-SIM和URBAN-BENCH为自主微移动技术的发展提供了有效的模拟和评估工具。

Abstract: Micromobility, which utilizes lightweight mobile machines moving in urban
public spaces, such as delivery robots and mobility scooters, emerges as a
promising alternative to vehicular mobility. Current micromobility depends
mostly on human manual operation (in-person or remote control), which raises
safety and efficiency concerns when navigating busy urban environments full of
unpredictable obstacles and pedestrians. Assisting humans with AI agents in
maneuvering micromobility devices presents a viable solution for enhancing
safety and efficiency. In this work, we present a scalable urban simulation
solution to advance autonomous micromobility. First, we build URBAN-SIM - a
high-performance robot learning platform for large-scale training of embodied
agents in interactive urban scenes. URBAN-SIM contains three critical modules:
Hierarchical Urban Generation pipeline, Interactive Dynamics Generation
strategy, and Asynchronous Scene Sampling scheme, to improve the diversity,
realism, and efficiency of robot learning in simulation. Then, we propose
URBAN-BENCH - a suite of essential tasks and benchmarks to gauge various
capabilities of the AI agents in achieving autonomous micromobility.
URBAN-BENCH includes eight tasks based on three core skills of the agents:
Urban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robots
with heterogeneous embodiments, such as the wheeled and legged robots, across
these tasks. Experiments on diverse terrains and urban structures reveal each
robot's strengths and limitations.

</details>

### [196] [RayZer: A Self-supervised Large View Synthesis Model](https://arxiv.org/abs/2505.00702)
*Hanwen Jiang,Hao Tan,Peng Wang,Haian Jin,Yue Zhao,Sai Bi,Kai Zhang,Fujun Luan,Kalyan Sunkavalli,Qixing Huang,Georgios Pavlakos*

Main category: cs.CV

TLDR: RayZer是一种无需3D监督的自监督多视角3D视觉模型，能够从无标定图像中恢复相机参数、重建场景并合成新视角。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一种无需3D标注（如相机位姿和场景几何）的自监督模型，实现3D感知能力。

Method: 方法包括设计自监督框架，通过解耦相机和场景表示实现3D感知自动编码，以及基于Transformer的模型，仅依赖射线结构作为3D先验。

Result: RayZer在合成新视角任务中表现优于依赖标注的基准方法。

Conclusion: 结论是RayZer展示了无需3D监督的自监督学习在3D视觉任务中的潜力。

Abstract: We present RayZer, a self-supervised multi-view 3D Vision model trained
without any 3D supervision, i.e., camera poses and scene geometry, while
exhibiting emerging 3D awareness. Concretely, RayZer takes unposed and
uncalibrated images as input, recovers camera parameters, reconstructs a scene
representation, and synthesizes novel views. During training, RayZer relies
solely on its self-predicted camera poses to render target views, eliminating
the need for any ground-truth camera annotations and allowing RayZer to be
trained with 2D image supervision. The emerging 3D awareness of RayZer is
attributed to two key factors. First, we design a self-supervised framework,
which achieves 3D-aware auto-encoding of input images by disentangling camera
and scene representations. Second, we design a transformer-based model in which
the only 3D prior is the ray structure, connecting camera, pixel, and scene
simultaneously. RayZer demonstrates comparable or even superior novel view
synthesis performance than ``oracle'' methods that rely on pose annotations in
both training and testing. Project: https://hwjiang1510.github.io/RayZer/

</details>

### [197] [T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT](https://arxiv.org/abs/2505.00703)
*Dongzhi Jiang,Ziyu Guo,Renrui Zhang,Zhuofan Zong,Hao Li,Le Zhuo,Shilin Yan,Pheng-Ann Heng,Hongsheng Li*

Main category: cs.CV

TLDR: T2I-R1是一种基于强化学习和双层次思维链推理的文本到图像生成模型，显著提升了生成性能。


<details>
  <summary>Details</summary>
Motivation: 探索思维链和强化学习在视觉生成领域的应用，填补现有研究的空白。

Method: 提出双层次思维链（语义级和词元级）和BiCoT-GRPO方法，优化生成过程。

Result: 在T2I-CompBench和WISE基准上分别提升13%和19%，超越现有最佳模型FLUX。

Conclusion: T2I-R1通过双层次推理策略显著提升了文本到图像生成的性能。

Abstract: Recent advancements in large language models have demonstrated how
chain-of-thought (CoT) and reinforcement learning (RL) can improve performance.
However, applying such reasoning strategies to the visual generation domain
remains largely unexplored. In this paper, we present T2I-R1, a novel
reasoning-enhanced text-to-image generation model, powered by RL with a
bi-level CoT reasoning process. Specifically, we identify two levels of CoT
that can be utilized to enhance different stages of generation: (1) the
semantic-level CoT for high-level planning of the prompt and (2) the
token-level CoT for low-level pixel processing during patch-by-patch
generation. To better coordinate these two levels of CoT, we introduce
BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes
both generation CoTs within the same training step. By applying our reasoning
strategies to the baseline model, Janus-Pro, we achieve superior performance
with 13% improvement on T2I-CompBench and 19% improvement on the WISE
benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available
at: https://github.com/CaraJ7/T2I-R1

</details>

### [198] [Deep Learning for automated multi-scale functional field boundaries extraction using multi-date Sentinel-2 and PlanetScope imagery: Case Study of Netherlands and Pakistan](https://arxiv.org/abs/2411.15923)
*Saba Zahid,Sajid Ghuffar,Obaid-ur-Rehman,Syed Roshaan Ali Shah*

Main category: cs.CV

TLDR: 研究探讨了多时相卫星影像结合深度学习语义分割架构在荷兰和巴基斯坦不同地理与多尺度农业系统中划定功能田边界的有效性，发现多日期NDVI堆栈提供额外时间信息，并强调了多尺度地面数据的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索多时相卫星影像和深度学习在复杂农业环境中划定田边界的潜力，尤其是在不同地理区域和多尺度农业系统中的应用。

Method: 使用UNET架构的四种深度学习模型，结合多日期影像和NDVI堆栈，分别在荷兰和巴基斯坦进行训练和评估，并采用迁移学习技术。

Result: 多日期NDVI堆栈提供了额外的时间信息，反映了作物生长变化；高空间分辨率对小规模农田边界提取至关重要。

Conclusion: 多尺度地面信息和多时相数据对开发鲁棒且通用的田边界划定模型至关重要，研究结果可推广到异质农业环境中的多尺度应用。

Abstract: This study explores the effectiveness of multi-temporal satellite imagery for
better functional field boundary delineation using deep learning semantic
segmentation architecture on two distinct geographical and multi-scale farming
systems of Netherlands and Pakistan. Multidate images of April, August and
October 2022 were acquired for PlanetScope and Sentinel-2 in sub regions of
Netherlands and November 2022, February and March 2023 for selected area of
Dunyapur in Pakistan. For Netherlands, Basic registration crop parcels (BRP)
vector layer was used as labeled training data. while self-crafted field
boundary vector data were utilized for Pakistan. Four deep learning models with
UNET architecture were evaluated using different combinations of multi-date
images and NDVI stacks in the Netherlands subregions. A comparative analysis of
IoU scores assessed the effectiveness of the proposed multi-date NDVI stack
approach. These findings were then applied for transfer learning, using
pre-trained models from the Netherlands on the selected area in Pakistan.
Additionally, separate models were trained using self-crafted field boundary
data for Pakistan, and combined models were developed using data from both the
Netherlands and Pakistan. Results indicate that multi-date NDVI stacks provide
additional temporal context, reflecting crop growth over different times of the
season. The study underscores the critical role of multi-scale ground
information from diverse geographical areas in developing robust and
universally applicable models for field boundary delineation. The results also
highlight the importance of fine spatial resolution for extraction of field
boundaries in regions with small scale framing. The findings can be extended to
multi-scale implementations for improved automatic field boundary delineation
in heterogeneous agricultural environments.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [199] [Controllable Weather Synthesis and Removal with Video Diffusion Models](https://arxiv.org/abs/2505.00704)
*Chih-Hao Lin,Zian Wang,Ruofan Liang,Yuxuan Zhang,Sanja Fidler,Shenlong Wang,Zan Gojcic*

Main category: cs.GR

TLDR: WeatherWeaver是一种视频扩散模型，无需3D建模即可在视频中合成多样化的天气效果，如雨、雪、雾和云，并提供精确控制和高质量结果。


<details>
  <summary>Details</summary>
Motivation: 现有物理模拟方法难以扩展到野外视频，而视频编辑缺乏真实感和控制性，因此需要一种既能生成真实天气效果又能灵活控制的方法。

Method: 提出WeatherWeaver模型，结合合成视频、生成图像编辑和自动标记的真实视频数据，训练视频扩散模型以合成天气效果。

Result: 模型在天气模拟和去除任务中优于现有方法，生成高质量、物理合理且保留场景身份的结果。

Conclusion: WeatherWeaver为视频天气效果合成提供了一种高效、可控且真实的解决方案。

Abstract: Generating realistic and controllable weather effects in videos is valuable
for many applications. Physics-based weather simulation requires precise
reconstructions that are hard to scale to in-the-wild videos, while current
video editing often lacks realism and control. In this work, we introduce
WeatherWeaver, a video diffusion model that synthesizes diverse weather effects
-- including rain, snow, fog, and clouds -- directly into any input video
without the need for 3D modeling. Our model provides precise control over
weather effect intensity and supports blending various weather types, ensuring
both realism and adaptability. To overcome the scarcity of paired training
data, we propose a novel data strategy combining synthetic videos, generative
image editing, and auto-labeled real-world videos. Extensive evaluations show
that our method outperforms state-of-the-art methods in weather simulation and
removal, providing high-quality, physically plausible, and
scene-identity-preserving results over various real-world videos.

</details>

<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [200] [SA-GAT-SR: Self-Adaptable Graph Attention Networks with Symbolic Regression for high-fidelity material property prediction](https://arxiv.org/abs/2505.00625)
*Liu Junchi,Tang Ying,Tretiak Sergei,Duan Wenhui,Zhou Liujiang*

Main category: physics.comp-ph

TLDR: 提出了一种结合图神经网络和符号回归的新框架SA-GAT-SR，旨在提升材料科学的预测准确性和物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法（如GNN）在材料科学中预测性能强但缺乏物理可解释性，需要一种能同时兼顾两者的方法。

Method: 采用自适应的图注意力网络（SA-GAT）筛选关键特征，并结合符号回归（SR）生成解析表达式。

Result: 方法在180维特征空间中高效筛选特征，计算复杂度为O(n)，且比传统SR实现快23倍。

Conclusion: SA-GAT-SR框架填补了预测准确性与物理可解释性之间的鸿沟，为材料行为提供了有价值的物理见解。

Abstract: Recent advances in machine learning have demonstrated an enormous utility of
deep learning approaches, particularly Graph Neural Networks (GNNs) for
materials science. These methods have emerged as powerful tools for
high-throughput prediction of material properties, offering a compelling
enhancement and alternative to traditional first-principles calculations. While
the community has predominantly focused on developing increasingly complex and
universal models to enhance predictive accuracy, such approaches often lack
physical interpretability and insights into materials behavior. Here, we
introduce a novel computational paradigm, Self-Adaptable Graph Attention
Networks integrated with Symbolic Regression (SA-GAT-SR), that synergistically
combines the predictive capability of GNNs with the interpretative power of
symbolic regression. Our framework employs a self-adaptable encoding algorithm
that automatically identifies and adjust attention weights so as to screen
critical features from an expansive 180-dimensional feature space while
maintaining O(n) computational scaling. The integrated SR module subsequently
distills these features into compact analytical expressions that explicitly
reveal quantum-mechanically meaningful relationships, achieving 23 times
acceleration compared to conventional SR implementations that heavily rely on
first principle calculations-derived features as input. This work suggests a
new framework in computational materials science, bridging the gap between
predictive accuracy and physical interpretability, offering valuable physical
insights into material behavior.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [201] [Optimization of embeddings storage for RAG systems using quantization and dimensionality reduction techniques](https://arxiv.org/abs/2505.00105)
*Naamán Huerga-Pérez,Rubén Álvarez,Rubén Ferrero-Guillén,Alberto Martínez-Gutiérrez,Javier Díez-González*

Main category: cs.IR

TLDR: 论文研究了通过量化和降维优化检索增强生成模型的存储问题，发现float8量化和PCA结合能实现8倍压缩且性能损失最小。


<details>
  <summary>Details</summary>
Motivation: 解决高维向量嵌入存储带来的内存挑战。

Method: 系统评估了量化（float16、int8、float8等）和降维（PCA、UMAP等）策略。

Result: float8量化实现4倍存储减少且性能损失<0.3%，结合PCA可进一步实现8倍压缩。

Conclusion: 提出可视化方法帮助选择最优配置，平衡性能与存储需求。

Abstract: Retrieval-Augmented Generation enhances language models by retrieving
relevant information from external knowledge bases, relying on high-dimensional
vector embeddings typically stored in float32 precision. However, storing these
embeddings at scale presents significant memory challenges. To address this
issue, we systematically investigate on MTEB benchmark two complementary
optimization strategies: quantization, evaluating standard formats (float16,
int8, binary) and low-bit floating-point types (float8), and dimensionality
reduction, assessing methods like PCA, Kernel PCA, UMAP, Random Projections and
Autoencoders. Our results show that float8 quantization achieves a 4x storage
reduction with minimal performance degradation (<0.3%), significantly
outperforming int8 quantization at the same compression level, being simpler to
implement. PCA emerges as the most effective dimensionality reduction
technique. Crucially, combining moderate PCA (e.g., retaining 50% dimensions)
with float8 quantization offers an excellent trade-off, achieving 8x total
compression with less performance impact than using int8 alone (which provides
only 4x compression). To facilitate practical application, we propose a
methodology based on visualizing the performance-storage trade-off space to
identify the optimal configuration that maximizes performance within their
specific memory constraints.

</details>

### [202] [EnronQA: Towards Personalized RAG over Private Documents](https://arxiv.org/abs/2505.00263)
*Michael J. Ryan,Danmei Xu,Chris Nivera,Daniel Campos*

Main category: cs.IR

TLDR: 论文介绍了EnronQA基准数据集，用于改进检索增强生成（RAG）在私有数据上的性能评估，并探索私有文档处理中的记忆与检索权衡。


<details>
  <summary>Details</summary>
Motivation: 当前RAG基准主要基于公开数据，缺乏对私有和个人化数据的支持，限制了RAG在企业私有文档中的应用。

Method: 发布EnronQA数据集，包含103,638封邮件和528,304个问答对，覆盖150个用户邮箱，用于评估RAG在私有数据上的表现。

Result: EnronQA为私有数据上的RAG提供了更好的基准，并支持个性化检索设置的实验。

Conclusion: EnronQA填补了私有数据RAG评估的空白，并揭示了记忆与检索在私有文档处理中的权衡。

Abstract: Retrieval Augmented Generation (RAG) has become one of the most popular
methods for bringing knowledge-intensive context to large language models (LLM)
because of its ability to bring local context at inference time without the
cost or data leakage risks associated with fine-tuning. A clear separation of
private information from the LLM training has made RAG the basis for many
enterprise LLM workloads as it allows the company to augment LLM's
understanding using customers' private documents. Despite its popularity for
private documents in enterprise deployments, current RAG benchmarks for
validating and optimizing RAG pipelines draw their corpora from public data
such as Wikipedia or generic web pages and offer little to no personal context.
Seeking to empower more personal and private RAG we release the EnronQA
benchmark, a dataset of 103,638 emails with 528,304 question-answer pairs
across 150 different user inboxes. EnronQA enables better benchmarking of RAG
pipelines over private data and allows for experimentation on the introduction
of personalized retrieval settings over realistic data. Finally, we use EnronQA
to explore the tradeoff in memorization and retrieval when reasoning over
private documents.

</details>

### [203] [Investigating Task Arithmetic for Zero-Shot Information Retrieval](https://arxiv.org/abs/2505.00649)
*Marco Braga,Pranav Kasela,Alessandro Raganato,Gabriella Pasi*

Main category: cs.IR

TLDR: 论文提出了一种名为Task Arithmetic的方法，通过简单的数学操作（如加减）结合预训练LLMs的权重，无需额外微调即可适应不同任务和领域，显著提升了零样本检索性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在零样本任务中表现优异，但在未见过的任务和领域中性能下降，主要由于词汇和词分布的变化。

Method: 采用Task Arithmetic技术，通过数学操作结合不同任务或领域预训练的LLMs权重，合成多样任务和领域知识。

Result: 在科学、生物医学和多语言数据集上，NDCG@10和P@10分别提升18%和15%。

Conclusion: Task Arithmetic是一种有效的零样本学习和模型适应策略，兼具实用性和局限性。

Abstract: Large Language Models (LLMs) have shown impressive zero-shot performance
across a variety of Natural Language Processing tasks, including document
re-ranking. However, their effectiveness degrades on unseen tasks and domains,
largely due to shifts in vocabulary and word distributions. In this paper, we
investigate Task Arithmetic, a technique that combines the weights of LLMs
pre-trained on different tasks or domains via simple mathematical operations,
such as addition or subtraction, to adapt retrieval models without requiring
additional fine-tuning. Our method is able to synthesize diverse tasks and
domain knowledge into a single model, enabling effective zero-shot adaptation
in different retrieval contexts. Extensive experiments on publicly available
scientific, biomedical, and multilingual datasets show that our method improves
state-of-the-art re-ranking performance by up to 18% in NDCG@10 and 15% in
P@10. In addition to these empirical gains, our analysis provides insights into
the strengths and limitations of Task Arithmetic as a practical strategy for
zero-shot learning and model adaptation. We make our code publicly available at
https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR.

</details>

### [204] [Graph Spectral Filtering with Chebyshev Interpolation for Recommendation](https://arxiv.org/abs/2505.00552)
*Chanwoo Kim,Jinkyu Sung,Yebonn Han,Joonseok Lee*

Main category: cs.IR

TLDR: 论文提出ChebyCF框架，通过图谱滤波解决图卷积网络在协同过滤中的瓶颈问题，利用Chebyshev插值和全频谱信号提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有图卷积网络在协同过滤中存在嵌入层容量有限和邻域聚合能力不足的问题，限制了推荐性能。

Method: 采用图谱滤波技术，直接利用用户原始交互历史，结合Chebyshev插值和非线性滤波器设计。

Result: 实验表明ChebyCF在多个基准测试中达到最优性能，且推理速度较快。

Conclusion: ChebyCF通过全频谱信号和灵活滤波器设计，有效解决了现有方法的瓶颈，提升了推荐效果。

Abstract: Graph convolutional networks have recently gained prominence in collaborative
filtering (CF) for recommendations. However, we identify potential bottlenecks
in two foundational components. First, the embedding layer leads to a latent
space with limited capacity, overlooking locally observed but potentially
valuable preference patterns. Also, the widely-used neighborhood aggregation is
limited in its ability to leverage diverse preference patterns in a
fine-grained manner. Building on spectral graph theory, we reveal that these
limitations stem from graph filtering with a cut-off in the frequency spectrum
and a restricted linear form. To address these issues, we introduce ChebyCF, a
CF framework based on graph spectral filtering. Instead of a learned embedding,
it takes a user's raw interaction history to utilize the full spectrum of
signals contained in it. Also, it adopts Chebyshev interpolation to effectively
approximate a flexible non-linear graph filter, and further enhances it by
using an additional ideal pass filter and degree-based normalization. Through
extensive experiments, we verify that ChebyCF overcomes the aforementioned
bottlenecks and achieves state-of-the-art performance across multiple
benchmarks and reasonably fast inference. Our code is available at
https://github.com/chanwoo0806/ChebyCF.

</details>

<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [205] [On the Mechanistic Interpretability of Neural Networks for Causality in Bio-statistics](https://arxiv.org/abs/2505.00555)
*Jean-Baptiste A. Conan*

Main category: stat.AP

TLDR: 论文探讨了如何利用机制可解释性（MI）技术提升神经网络在生物统计因果推断中的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 在生物统计中，神经网络的‘黑箱’特性限制了其在高风险健康应用中的验证和信任，而机制可解释性技术有望解决这一问题。

Method: 应用MI技术分析神经网络的内部计算，包括验证其学习到的表示、可视化计算路径，以及比较不同模型的机制。

Result: MI工具能够验证神经网络的内部表示、揭示其处理输入的计算路径，并比较不同模型的优缺点。

Conclusion: 机制可解释性技术为神经网络在生物统计因果推断中的应用提供了透明度和可信度，有助于理解其优势和局限。

Abstract: Interpretable insights from predictive models remain critical in
bio-statistics, particularly when assessing causality, where classical
statistical and machine learning methods often provide inherent clarity. While
Neural Networks (NNs) offer powerful capabilities for modeling complex
biological data, their traditional "black-box" nature presents challenges for
validation and trust in high-stakes health applications. Recent advances in
Mechanistic Interpretability (MI) aim to decipher the internal computations
learned by these networks. This work investigates the application of MI
techniques to NNs within the context of causal inference for bio-statistics.
  We demonstrate that MI tools can be leveraged to: (1) probe and validate the
internal representations learned by NNs, such as those estimating nuisance
functions in frameworks like Targeted Minimum Loss-based Estimation (TMLE); (2)
discover and visualize the distinct computational pathways employed by the
network to process different types of inputs, potentially revealing how
confounders and treatments are handled; and (3) provide methodologies for
comparing the learned mechanisms and extracted insights across statistical,
machine learning, and NN models, fostering a deeper understanding of their
respective strengths and weaknesses for causal bio-statistical analysis.

</details>

<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [206] [Subspace-Distance-Enabled Active Learning for Efficient Data-Driven Model Reduction of Parametric Dynamical Systems](https://arxiv.org/abs/2505.00460)
*Harshit Kapadia,Peter Benner,Lihong Feng*

Main category: math.NA

TLDR: 提出了一种基于主动学习的数据驱动降阶模型（ROM）构建方法，通过贪婪选择参数样本，动态增加高保真解的数量，并利用子空间距离作为主动学习的指导机制。


<details>
  <summary>Details</summary>
Motivation: 在高保真动态系统需要重复评估且无法访问控制方程的情况下，数据驱动的降阶模型更为适用。

Method: 使用POD将高保真解表示为参数特定的线性子空间，提出子空间距离度量作为主动学习的机制，并扩展了两种非侵入式ROM方法。

Result: 在两种参数化物理模型中验证了所提出的SDE-AL方法的有效性。

Conclusion: SDE-AL框架通过主动学习显著提高了ROM的构建效率。

Abstract: In situations where the solution of a high-fidelity dynamical system needs to
be evaluated repeatedly, over a vast pool of parametric configurations and in
absence of access to the underlying governing equations, data-driven model
reduction techniques are preferable. We propose a novel active learning
approach to build a parametric data-driven reduced-order model (ROM) by
greedily picking the most important parameter samples from the parameter
domain. As a result, during the ROM construction phase, the number of
high-fidelity solutions dynamically grow in a principled fashion. The
high-fidelity solution snapshots are expressed in several parameter-specific
linear subspaces, with the help of proper orthogonal decomposition (POD), and
the relative distance between these subspaces is used as a guiding mechanism to
perform active learning. For successfully achieving this, we provide a distance
measure to evaluate the similarity between pairs of linear subspaces with
different dimensions, and also show that this distance measure is a metric. The
usability of the proposed subspace-distance-enabled active learning (SDE-AL)
framework is demonstrated by augmenting two existing non-intrusive
reduced-order modeling approaches, and providing their active-learning-driven
(ActLearn) extensions, namely, SDE-ActLearn-POD-KSNN, and SDE-ActLearn-POD-NN.
Furthermore, we report positive results for two parametric physical models,
highlighting the efficiency of the proposed SDE-AL approach.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [207] [SR-NeRV: Improving Embedding Efficiency of Neural Video Representation via Super-Resolution](https://arxiv.org/abs/2505.00046)
*Taiga Hayami,Kakeru Koizumi,Hiroshi Watanabe*

Main category: eess.IV

TLDR: 提出了一种结合超分辨率网络的INR视频表示方法，显著提升了高频细节的重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统INR方法在严格模型大小限制下难以重建高频细节，而高频细节在视频压缩中至关重要。

Method: 通过集成通用超分辨率网络，将高频细节的重建任务委托给该网络。

Result: 实验表明，该方法在重建质量上优于传统INR基线，同时保持相似的模型大小。

Conclusion: 该方法为INR视频压缩提供了一种有效的高频细节重建解决方案。

Abstract: Implicit Neural Representations (INRs) have garnered significant attention
for their ability to model complex signals across a variety of domains.
Recently, INR-based approaches have emerged as promising frameworks for neural
video compression. While conventional methods primarily focus on embedding
video content into compact neural networks for efficient representation, they
often struggle to reconstruct high-frequency details under stringent model size
constraints, which are critical in practical compression scenarios. To address
this limitation, we propose an INR-based video representation method that
integrates a general-purpose super-resolution (SR) network. Motivated by the
observation that high-frequency components exhibit low temporal redundancy
across frames, our method entrusts the reconstruction of fine details to the SR
network. Experimental results demonstrate that the proposed method outperforms
conventional INR-based baselines in terms of reconstruction quality, while
maintaining comparable model sizes.

</details>

### [208] [Rootlets-based registration to the spinal cord PAM50 template](https://arxiv.org/abs/2505.00115)
*Sandrine Bédard,Jan Valošek,Valeria Oliva,Kenneth A. Weber II,Julien Cohen-Adad*

Main category: eess.IV

TLDR: 提出了一种基于脊髓神经根的新型配准方法，提高了脊髓功能MRI研究中个体间对齐的准确性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 传统基于椎间盘的配准方法因个体间解剖变异大，难以实现精确对齐，影响群体分析的可靠性。

Method: 开发了一种利用颈背侧神经根分割并非线性对齐PAM50脊髓模板的配准方法，并在多站点和多颈部位置的数据集上验证。

Result: 根配准法在个体间对齐和颈部位置稳定性上优于传统方法，任务fMRI分析中激活簇数量和Z分数显著提高。

Conclusion: 根配准法提升了脊髓神经影像群体分析的精度和可靠性，具有广泛应用潜力。

Abstract: Spinal cord functional MRI studies require precise localization of spinal
levels for reliable voxelwise group analyses. Traditional template-based
registration of the spinal cord uses intervertebral discs for alignment.
However, substantial anatomical variability across individuals exists between
vertebral and spinal levels. This study proposes a novel registration approach
that leverages spinal nerve rootlets to improve alignment accuracy and
reproducibility across individuals. We developed a registration method
leveraging dorsal cervical rootlets segmentation and aligning them non-linearly
with the PAM50 spinal cord template. Validation was performed on a
multi-subject, multi-site dataset (n=267, 44 sites) and a multi-subject dataset
with various neck positions (n=10, 3 sessions). We further validated the method
on task-based functional MRI (n=23) to compare group-level activation maps
using rootlet-based registration to traditional disc-based methods.
Rootlet-based registration showed superior alignment across individuals
compared to the traditional disc-based method. Notably, rootlet positions were
more stable across neck positions. Group-level analysis of task-based
functional MRI using rootlet-based increased Z scores and activation cluster
size compared to disc-based registration (number of active voxels from 3292 to
7978). Rootlet-based registration enhances both inter- and intra-subject
anatomical alignment and yields better spatial normalization for group-level
fMRI analyses. Our findings highlight the potential of rootlet-based
registration to improve the precision and reliability of spinal cord
neuroimaging group analysis.

</details>

### [209] [Efficient and robust 3D blind harmonization for large domain gaps](https://arxiv.org/abs/2505.00133)
*Hwihun Jeong,Hayeon Lee,Se Young Chun,Jongho Lee*

Main category: eess.IV

TLDR: BlindHarmonyDiff是一种新型盲3D图像协调框架，通过边缘到图像模型和3D修正流实现高效协调，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有盲协调方法在3D图像中存在切片间异质性、图像质量中等及大域差距性能有限的问题。

Method: 采用3D修正流和边缘到图像模型，结合多跨度补丁训练和细化模块，实现高效协调。

Result: 实验表明，BlindHarmonyDiff在协调多样源域图像时优于现有方法，且在下游任务中表现优异。

Conclusion: BlindHarmonyDiff提供了一种鲁棒且通用的盲协调方法，适用于多样MR扫描仪。

Abstract: Blind harmonization has emerged as a promising technique for MR image
harmonization to achieve scale-invariant representations, requiring only target
domain data (i.e., no source domain data necessary). However, existing methods
face limitations such as inter-slice heterogeneity in 3D, moderate image
quality, and limited performance for a large domain gap. To address these
challenges, we introduce BlindHarmonyDiff, a novel blind 3D harmonization
framework that leverages an edge-to-image model tailored specifically to
harmonization. Our framework employs a 3D rectified flow trained on target
domain images to reconstruct the original image from an edge map, then yielding
a harmonized image from the edge of a source domain image. We propose
multi-stride patch training for efficient 3D training and a refinement module
for robust inference by suppressing hallucination. Extensive experiments
demonstrate that BlindHarmonyDiff outperforms prior arts by harmonizing diverse
source domain images to the target domain, achieving higher correspondence to
the target domain characteristics. Downstream task-based quality assessments
such as tissue segmentation and age prediction on diverse MR scanners further
confirm the effectiveness of our approach and demonstrate the capability of our
robust and generalizable blind harmonization.

</details>

### [210] [Towards Lightweight Hyperspectral Image Super-Resolution with Depthwise Separable Dilated Convolutional Network](https://arxiv.org/abs/2505.00374)
*Usman Muhammad,Jorma Laaksonen,Lyudmila Mihaylova*

Main category: eess.IV

TLDR: 提出了一种轻量级的深度可分离扩张卷积网络（DSDCN）用于高光谱图像超分辨率，解决了现有方法参数多或依赖额外图像的问题，并在公开数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 高光谱超分辨率由于数据的高光谱维度和训练样本稀缺而成为不适定问题，现有方法依赖大模型或额外图像，实用性差。

Method: 采用类似MobileNet的深度可分离卷积，结合扩张卷积融合块提取空间和光谱特征，并提出自定义损失函数（MSE、L2正则化和光谱角损失）。

Result: 在两个公开高光谱数据集上表现优异，适用于高光谱图像超分辨率任务。

Conclusion: DSDCN是一种轻量级且高效的高光谱超分辨率方法，代码已开源。

Abstract: Deep neural networks have demonstrated highly competitive performance in
super-resolution (SR) for natural images by learning mappings from
low-resolution (LR) to high-resolution (HR) images. However, hyperspectral
super-resolution remains an ill-posed problem due to the high spectral
dimensionality of the data and the scarcity of available training samples.
Moreover, existing methods often rely on large models with a high number of
parameters or require the fusion with panchromatic or RGB images, both of which
are often impractical in real-world scenarios. Inspired by the MobileNet
architecture, we introduce a lightweight depthwise separable dilated
convolutional network (DSDCN) to address the aforementioned challenges.
Specifically, our model leverages multiple depthwise separable convolutions,
similar to the MobileNet architecture, and further incorporates a dilated
convolution fusion block to make the model more flexible for the extraction of
both spatial and spectral features. In addition, we propose a custom loss
function that combines mean squared error (MSE), an L2 norm
regularization-based constraint, and a spectral angle-based loss, ensuring the
preservation of both spectral and spatial details. The proposed model achieves
very competitive performance on two publicly available hyperspectral datasets,
making it well-suited for hyperspectral image super-resolution tasks. The
source codes are publicly available at:
\href{https://github.com/Usman1021/lightweight}{https://github.com/Usman1021/lightweight}.

</details>

### [211] [CORSTITCH - A free, open source software for stitching and georeferencing underwater coral reef videos](https://arxiv.org/abs/2505.00462)
*Julian Christopher L. Maya,Johnenn R. Manalang,Maricor N. Soriano*

Main category: eess.IV

TLDR: CorStitch是一款开源软件，用于从视频片段自动生成精确的地理参考珊瑚礁拼接图。


<details>
  <summary>Details</summary>
Motivation: 开发CorStitch是为了自动化处理珊瑚礁评估系统的视频数据，生成可用于空间分析的拼接图。

Method: 采用基于傅里叶的图像相关算法拼接视频帧，并结合GNSS时间戳进行地理参考。

Result: 生成的拼接图以Keyhole Markup Language格式存储，验证显示其性能稳定可靠。

Conclusion: CorStitch能够高效生成精确的珊瑚礁拼接图，适用于地理信息系统分析。

Abstract: CorStitch is an open-source software developed to automate the creation of
accurate georeferenced reef mosaics from video transects obtained through
Automated Rapid Reef Assessment System surveys. We utilized a Fourier-based
image correlation algorithm to stitch sequential video frames, aligning them
with synchronized GNSS timestamps. The resulting compressed Keyhole Markup
Language files, compatible with geographic information systems such as Google
Earth, enable detailed spatial analysis. Validation through comparative
analysis of mosaics from two temporally distinct surveys of the same reef
demonstrated the software's consistent and reliable performance.

</details>

### [212] [A Methodological and Structural Review of Parkinsons Disease Detection Across Diverse Data Modalities](https://arxiv.org/abs/2505.00525)
*Abu Saleh Musa Miah,taro Suzuki,Jungpil Shin*

Main category: eess.IV

TLDR: 该论文综述了帕金森病（PD）识别系统的多模态方法，填补了现有研究的局限性，旨在通过综合多模态数据和机器学习技术提升诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断对改善患者预后至关重要，但现有研究多局限于单一数据模态，未能充分利用多模态方法的潜力。

Method: 基于347篇文献，综述了包括MRI、步态分析、手写分析、语音测试、EEG等多种数据模态及其融合技术的PD识别系统。

Result: 研究分析了数据收集方法、特征表示和系统性能，重点关注识别准确性和鲁棒性。

Conclusion: 该综述为研究人员提供了开发下一代PD识别系统的指导，通过多模态方法推动PD诊断技术的进步。

Abstract: Parkinsons Disease (PD) is a progressive neurological disorder that primarily
affects motor functions and can lead to mild cognitive impairment (MCI) and
dementia in its advanced stages. With approximately 10 million people diagnosed
globally 1 to 1.8 per 1,000 individuals, according to reports by the Japan
Times and the Parkinson Foundation early and accurate diagnosis of PD is
crucial for improving patient outcomes. While numerous studies have utilized
machine learning (ML) and deep learning (DL) techniques for PD recognition,
existing surveys are limited in scope, often focusing on single data modalities
and failing to capture the potential of multimodal approaches. To address these
gaps, this study presents a comprehensive review of PD recognition systems
across diverse data modalities, including Magnetic Resonance Imaging (MRI),
gait-based pose analysis, gait sensory data, handwriting analysis, speech test
data, Electroencephalography (EEG), and multimodal fusion techniques. Based on
over 347 articles from leading scientific databases, this review examines key
aspects such as data collection methods, settings, feature representations, and
system performance, with a focus on recognition accuracy and robustness. This
survey aims to serve as a comprehensive resource for researchers, providing
actionable guidance for the development of next generation PD recognition
systems. By leveraging diverse data modalities and cutting-edge machine
learning paradigms, this work contributes to advancing the state of PD
diagnostics and improving patient care through innovative, multimodal
approaches.

</details>

### [213] [Deep Learning Assisted Outer Volume Removal for Highly-Accelerated Real-Time Dynamic MRI](https://arxiv.org/abs/2505.00643)
*Merve Gülle,Sebastian Weingärtner,Mehmet Akçakaya*

Main category: eess.IV

TLDR: 提出了一种新型外体积去除（OVR）方法，通过深度学习模型消除实时动态MRI中的伪影，提高图像质量。


<details>
  <summary>Details</summary>
Motivation: 实时动态MRI在捕捉快速生理过程中至关重要，但高加速率下易受伪影干扰，影响心脏功能评估。

Method: 利用时间交错欠采样模式生成复合时间图像，训练深度学习模型识别并去除伪影，结合物理驱动DL方法重建图像。

Result: 在高加速率下，图像质量与临床基线图像相当，优于传统重建技术。

Conclusion: 该方法无需修改采集过程，即可有效减少伪影，为实时动态MRI提供高质量图像。

Abstract: Real-time (RT) dynamic MRI plays a vital role in capturing rapid
physiological processes, offering unique insights into organ motion and
function. Among these applications, RT cine MRI is particularly important for
functional assessment of the heart with high temporal resolution. RT imaging
enables free-breathing, ungated imaging of cardiac motion, making it a crucial
alternative for patients who cannot tolerate conventional breath-hold,
ECG-gated acquisitions. However, achieving high acceleration rates in RT cine
MRI is challenging due to aliasing artifacts from extra-cardiac tissues,
particularly at high undersampling factors. In this study, we propose a novel
outer volume removal (OVR) method to address this challenge by eliminating
aliasing contributions from non-cardiac regions in a post-processing framework.
Our approach estimates the outer volume signal for each timeframe using
composite temporal images from time-interleaved undersampling patterns, which
inherently contain pseudo-periodic ghosting artifacts. A deep learning (DL)
model is trained to identify and remove these artifacts, producing a clean
outer volume estimate that is subsequently subtracted from the corresponding
k-space data. The final reconstruction is performed with a physics-driven DL
(PD-DL) method trained using an OVR-specific loss function to restore high
spatio-temporal resolution images. Experimental results show that the proposed
method at high accelerations achieves image quality that is visually comparable
to clinical baseline images, while outperforming conventional reconstruction
techniques, both qualitatively and quantitatively. The proposed approach
provides a practical and effective solution for artifact reduction in RT cine
MRI without requiring acquisition modifications, offering a pathway to higher
acceleration rates while preserving diagnostic quality.

</details>

### [214] [GuideSR: Rethinking Guidance for One-Step High-Fidelity Diffusion-Based Super-Resolution](https://arxiv.org/abs/2505.00687)
*Aditya Arora,Zhengzhong Tu,Yufei Wang,Ruizheng Bai,Jian Wang,Sizhuo Ma*

Main category: eess.IV

TLDR: GuideSR是一种新型单步扩散图像超分辨率模型，通过双分支架构提升图像保真度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散超分辨率方法通过预训练生成模型适应图像修复任务，但常牺牲结构保真度。GuideSR旨在解决这一问题。

Method: 采用双分支架构：引导分支保留高保真结构，扩散分支利用预训练潜在扩散模型提升感知质量。结合FRBs和IGN等技术。

Result: 在基准数据集上表现优异，PSNR增益达1.39dB，计算成本低，优于现有方法。

Conclusion: GuideSR在图像修复任务中实现了高效且高质量的成果，具有实际应用价值。

Abstract: In this paper, we propose GuideSR, a novel single-step diffusion-based image
super-resolution (SR) model specifically designed to enhance image fidelity.
Existing diffusion-based SR approaches typically adapt pre-trained generative
models to image restoration tasks by adding extra conditioning on a
VAE-downsampled representation of the degraded input, which often compromises
structural fidelity. GuideSR addresses this limitation by introducing a
dual-branch architecture comprising: (1) a Guidance Branch that preserves
high-fidelity structures from the original-resolution degraded input, and (2) a
Diffusion Branch, which a pre-trained latent diffusion model to enhance
perceptual quality. Unlike conventional conditioning mechanisms, our Guidance
Branch features a tailored structure for image restoration tasks, combining
Full Resolution Blocks (FRBs) with channel attention and an Image Guidance
Network (IGN) with guided attention. By embedding detailed structural
information directly into the restoration pipeline, GuideSR produces sharper
and more visually consistent results. Extensive experiments on benchmark
datasets demonstrate that GuideSR achieves state-of-the-art performance while
maintaining the low computational cost of single-step approaches, with up to
1.39dB PSNR gain on challenging real-world datasets. Our approach consistently
outperforms existing methods across various reference-based metrics including
PSNR, SSIM, LPIPS, DISTS and FID, further representing a practical advancement
for real-world image restoration.

</details>

<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [215] [Can a Quantum Support Vector Machine algorithm be utilized to identify Key Biomarkers from Multi-Omics data of COVID19 patients?](https://arxiv.org/abs/2505.00037)
*Junggu Choi,Chansu Yu,Kyle L. Jung,Suan-Sin Foo,Weiqiang Chen,Suzy AA Comhair,Serpil C. Erzurum,Lara Jehi,Jae U. Jung*

Main category: quant-ph

TLDR: 研究评估了量子支持向量机（QSVM）在COVID-19生物标志物分类中的表现，发现其性能优于或等同于经典SVM，展示了QSVM在生物医学多组学数据分析中的潜力。


<details>
  <summary>Details</summary>
Motivation: 从高维多组学数据中识别COVID-19关键生物标志物对诊断和发病机制研究至关重要。

Method: 使用岭回归对生物标志物进行重要性排序，并分别用经典SVM和QSVM模型训练和评估。QSVM采用多种量子核实现。

Result: QSVM在分类性能上表现优于或等同于经典SVM，且能反映岭回归的重要性排序。

Conclusion: 尽管实验基于数值模拟，但QSVM在生物医学多组学数据分析中显示出潜力。

Abstract: Identifying key biomarkers for COVID-19 from high-dimensional multi-omics
data is critical for advancing both diagnostic and pathogenesis research. In
this study, we evaluated the applicability of the Quantum Support Vector
Machine (QSVM) algorithm for biomarker-based classification of COVID-19.
Proteomic and metabolomic biomarkers from two independent datasets were ranked
by importance using ridge regression and grouped accordingly. The top- and
bottom-ranked biomarker sets were then used to train and evaluate both
classical SVM (CSVM) and QSVM models, serving as predictive and negative
control inputs, respectively. The QSVM was implemented with multiple quantum
kernels, including amplitude encoding, angle encoding, the ZZ feature map, and
the projected quantum kernel. Across various experimental settings, QSVM
consistently achieved classification performance that was comparable to or
exceeded that of CSVM, while reflecting the importance rankings by ridge
regression. Although the experiments were conducted in numerical simulation,
our findings highlight the potential of QSVM as a promising approach for
multi-omics data analysis in biomedical research.

</details>

### [216] [Toward Practical Quantum Machine Learning: A Novel Hybrid Quantum LSTM for Fraud Detection](https://arxiv.org/abs/2505.00137)
*Rushikesh Ubale,Sujan K. K.,Sangram Deshpande,Gregory T. Byrd*

Main category: quant-ph

TLDR: 提出了一种新型的混合量子-经典神经网络架构，用于欺诈检测，结合了经典LSTM和变分量子电路，显著提升了训练速度和性能。


<details>
  <summary>Details</summary>
Motivation: 通过利用量子叠加和纠缠现象，增强序列交易数据的特征表示，捕捉经典模型难以处理的复杂非线性模式。

Method: 采用经典LSTM与变分量子电路的混合架构，通过统一的反向传播过程联合优化经典和量子梯度，使用参数偏移规则处理量子参数。

Result: 相比传统LSTM基线，在准确率、精确率、召回率和F1分数上均有显著提升，每轮训练时间仅需45-65秒，远快于文献中的类似架构。

Conclusion: 混合量子-经典技术有望提升欺诈检测系统的效率和性能。

Abstract: We present a novel hybrid quantum-classical neural network architecture for
fraud detection that integrates a classical Long Short-Term Memory (LSTM)
network with a variational quantum circuit. By leveraging quantum phenomena
such as superposition and entanglement, our model enhances the feature
representation of sequential transaction data, capturing complex non-linear
patterns that are challenging for purely classical models. A comprehensive data
preprocessing pipeline is employed to clean, encode, balance, and normalize a
credit card fraud dataset, ensuring a fair comparison with baseline models.
Notably, our hybrid approach achieves per-epoch training times in the range of
45-65 seconds, which is significantly faster than similar architectures
reported in the literature, where training typically requires several minutes
per epoch. Both classical and quantum gradients are jointly optimized via a
unified backpropagation procedure employing the parameter-shift rule for the
quantum parameters. Experimental evaluations demonstrate competitive
improvements in accuracy, precision, recall, and F1 score relative to a
conventional LSTM baseline. These results underscore the promise of hybrid
quantum-classical techniques in advancing the efficiency and performance of
fraud detection systems.
  Keywords: Hybrid Quantum-Classical Neural Networks, Quantum Computing, Fraud
Detection, Hybrid Quantum LSTM, Variational Quantum Circuit, Parameter-Shift
Rule, Financial Risk Analysis

</details>

### [217] [Learning to Learn with Quantum Optimization via Quantum Neural Networks](https://arxiv.org/abs/2505.00561)
*Kuan-Cheng Chen,Hiromichi Matsuyama,Wei-Hao Huang*

Main category: quant-ph

TLDR: 论文提出了一种结合量子长短期记忆网络（QLSTM）与量子近似优化算法（QAOA）的量子元学习框架，用于提升参数优化效率，并在大规模组合优化问题中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决QAOA在参数优化中面临的困难，如复杂的能量景观和硬件噪声，以实现更高效的量子优化。

Method: 利用QLSTM架构作为优化器，通过在小规模图实例上训练，快速推广到更复杂的大规模问题。

Result: 在Max-Cut和Sherrington-Kirkpatrick模型上的实验表明，QLSTM优化器收敛更快且逼近比更高。

Conclusion: 该方法为NISQ时代下的可扩展量子优化提供了有效途径。

Abstract: Quantum Approximate Optimization Algorithms (QAOA) promise efficient
solutions to classically intractable combinatorial optimization problems by
harnessing shallow-depth quantum circuits. Yet, their performance and
scalability often hinge on effective parameter optimization, which remains
nontrivial due to rugged energy landscapes and hardware noise. In this work, we
introduce a quantum meta-learning framework that combines quantum neural
networks, specifically Quantum Long Short-Term Memory (QLSTM) architectures,
with QAOA. By training the QLSTM optimizer on smaller graph instances, our
approach rapidly generalizes to larger, more complex problems, substantially
reducing the number of iterations required for convergence. Through
comprehensive benchmarks on Max-Cut and Sherrington-Kirkpatrick model
instances, we demonstrate that QLSTM-based optimizers converge faster and
achieve higher approximation ratios compared to classical baselines, thereby
offering a robust pathway toward scalable quantum optimization in the NISQ era.

</details>

<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [218] [Convolutional Autoencoders for Data Compression and Anomaly Detection in Small Satellite Technologies](https://arxiv.org/abs/2505.00040)
*Dishanand Jayeprokash,Julia Gonski*

Main category: astro-ph.IM

TLDR: 论文探讨了在小卫星上应用卷积自编码器，实现数据压缩和异常检测，以提升灾害监测效率。


<details>
  <summary>Details</summary>
Motivation: 小卫星技术的发展降低了成本，使得机器学习在卫星数据采集中的应用成为可能，从而提升任务效率和性能。

Method: 采用卷积自编码器，设计用于数据压缩和异常检测，应用于非洲大陆的灾害监测。

Result: 展示了该方法在数据压缩和异常检测中的双重功能，为小卫星应用提供了新的机器学习解决方案。

Conclusion: 研究为小卫星技术和人工智能在非洲的应用开辟了新途径，展示了机器学习在空间技术中的潜力。

Abstract: Small satellite technologies have enhanced the potential and feasibility of
geodesic missions, through simplification of design and decreased costs
allowing for more frequent launches. On-satellite data acquisition systems can
benefit from the implementation of machine learning (ML), for better
performance and greater efficiency on tasks such as image processing or feature
extraction. This work presents convolutional autoencoders for implementation on
the payload of small satellites, designed to achieve dual functionality of data
compression for more efficient off-satellite transmission, and at-source
anomaly detection to inform satellite data-taking. This capability is
demonstrated for a use case of disaster monitoring using aerial image datasets
of the African continent, offering avenues for both novel ML-based approaches
in small satellite applications along with the expansion of space technology
and artificial intelligence in Africa.

</details>

<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [219] [Explorative Curriculum Learning for Strongly Correlated Electron Systems](https://arxiv.org/abs/2505.00233)
*Kimihiro Yamazaki,Takuya Konishi,Yoshinobu Kawahara*

Main category: cond-mat.str-el

TLDR: 提出了一种基于迁移学习的课程学习框架，用于神经网络量子态（NQS），以高效探索量子多体系统的参数空间，并通过Pairing-Net架构验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管NQS在复杂量子多体系统中表现出高精度预测能力，但其计算成本高昂，且难以推广到大规模系统和多样化参数范围。

Method: 提出了一种结合迁移学习的课程学习框架，并通过Pairing-Net架构实现，同时利用微扰理论将先验物理知识融入学习过程。

Result: 实验结果显示，相比传统方法，计算速度提升约200倍，且优化稳定性显著提高。

Conclusion: 该框架为高效探索量子多体系统参数空间提供了可行方案，并展示了迁移学习与课程学习的结合潜力。

Abstract: Recent advances in neural network quantum states (NQS) have enabled
high-accuracy predictions for complex quantum many-body systems such as
strongly correlated electron systems. However, the computational cost remains
prohibitive, making exploration of the diverse parameters of interaction
strengths and other physical parameters inefficient. While transfer learning
has been proposed to mitigate this challenge, achieving generalization to
large-scale systems and diverse parameter regimes remains difficult. To address
this limitation, we propose a novel curriculum learning framework based on
transfer learning for NQS. This facilitates efficient and stable exploration
across a vast parameter space of quantum many-body systems. In addition, by
interpreting NQS transfer learning through a perturbative lens, we demonstrate
how prior physical knowledge can be flexibly incorporated into the curriculum
learning process. We also propose Pairing-Net, an architecture to practically
implement this strategy for strongly correlated electron systems, and
empirically verify its effectiveness. Our results show an approximately
200-fold speedup in computation and a marked improvement in optimization
stability compared to conventional methods.

</details>

<div id='math.AC'></div>

# math.AC [[Back]](#toc)

### [220] [Key exchange protocol based on circulant matrix action over congruence-simple semiring](https://arxiv.org/abs/2505.00664)
*Alvaro Otero Sanchez*

Main category: math.AC

TLDR: 提出了一种基于循环矩阵作用于同余简单半环上矩阵的新密钥交换协议，并分析了其计算成本和安全性。


<details>
  <summary>Details</summary>
Motivation: 设计一种更高效且安全的密钥交换协议。

Method: 基于循环矩阵作用于同余简单半环上的矩阵，并计算满足协议需求的矩阵。

Result: 提供了协议的计算成本分析和安全性评估。

Conclusion: 该协议在计算效率和安全性方面表现良好。

Abstract: We present a new key exchange protocol based on circulant matrices acting on
matrices over a congruence-simple semiring. We describe how to compute matrices
with the necessary properties for the implementation of the protocol.
Additionally, we provide an analysis of its computational cost and its security
against known attacks.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [221] [Data Therapist: Eliciting Domain Knowledge from Subject Matter Experts Using Large Language Models](https://arxiv.org/abs/2505.00455)
*Sungbok Shin,Hyeon Jeon,Sanghyun Hong,Niklas Elmqvist*

Main category: cs.HC

TLDR: 论文介绍了Data Therapist工具，通过混合主动过程帮助领域专家外化隐含知识，结合问答与交互式标注，利用大语言模型分析数据并生成结构化知识库，以支持可视化设计。


<details>
  <summary>Details</summary>
Motivation: 数据可视化需要理解领域特定上下文，但隐含知识（如数据来源、质量和用途）通常未明确记录。Data Therapist旨在帮助专家外化这些知识。

Method: 开发了基于Web的Data Therapist工具，结合大语言模型分析数据，通过问答和交互式标注生成结构化知识库。

Result: 在分子生物学、会计、政治科学和可用安全领域的专家研究中，发现专家对数据的推理模式，并识别AI支持可视化设计的潜在改进点。

Conclusion: Data Therapist能有效外化隐含知识，支持可视化设计，同时揭示了AI在提升可视化设计中的潜力。

Abstract: Effective data visualization requires not only technical proficiency but also
a deep understanding of the domain-specific context in which data exists. This
context often includes tacit knowledge about data provenance, quality, and
intended use, which is rarely explicit in the dataset itself. We present the
Data Therapist, a web-based tool that helps domain experts externalize this
implicit knowledge through a mixed-initiative process combining iterative Q&A
with interactive annotation. Powered by a large language model, the system
analyzes user-supplied datasets, prompts users with targeted questions, and
allows annotation at varying levels of granularity. The resulting structured
knowledge base can inform both human and automated visualization design. We
evaluated the tool in a qualitative study involving expert pairs from Molecular
Biology, Accounting, Political Science, and Usable Security. The study revealed
recurring patterns in how experts reason about their data and highlights areas
where AI support can improve visualization design.

</details>

<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [222] [Transition States Energies from Machine Learning: An Application to Reverse Water-Gas Shift on Single-Atom Alloys](https://arxiv.org/abs/2505.00574)
*Raffaele Cheula,Mie Andersen*

Main category: cond-mat.mtrl-sci

TLDR: 提出了一种基于高斯过程回归和Wasserstein Weisfeiler-Lehman图核的机器学习模型，用于预测过渡态能量，显著提高了准确性，并展示了其在催化剂筛选中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如密度泛函理论）计算过渡态能量成本高，限制了复杂材料和反应网络的筛选效率。

Method: 使用高斯过程回归结合Wasserstein Weisfeiler-Lehman图核（WWL-GPR）构建机器学习模型，预测过渡态能量，并通过集成模型量化不确定性。

Result: 模型显著提高了过渡态能量预测的准确性，误差比传统方法降低近一个数量级，并成功筛选出有前景的催化剂。

Conclusion: 结合机器学习、密度泛函理论和微动力学模型，为复杂反应（如RWGS）的催化剂设计提供了高效框架。

Abstract: Obtaining accurate transition state (TS) energies is a bottleneck in
computational screening of complex materials and reaction networks due to the
high cost of TS search methods and first-principles methods such as density
functional theory (DFT). Here we propose a machine learning (ML) model for
predicting TS energies based on Gaussian process regression with the
Wasserstein Weisfeiler-Lehman graph kernel (WWL-GPR). Applying the model to
predict adsorption and TS energies for the reverse water-gas shift (RWGS)
reaction on single-atom alloy (SAA) catalysts, we show that it can
significantly improve the accuracy compared to traditional approaches based on
scaling relations or ML models without a graph representation. Further
benefitting from the low cost of model training, we train an ensemble of
WWL-GPR models to obtain uncertainties through subsampling of the training data
and show how these uncertainties propagate to turnover frequency (TOF)
predictions through the construction of an ensemble of microkinetic models.
Comparing the errors in model-based vs DFT-based TOF predictions, we show that
the WWL-GPR model reduces errors by almost an order of magnitude compared to
scaling relations. This demonstrates the critical impact of accurate energy
predictions on catalytic activity estimation. Finally, we apply our model to
screen new materials, identifying promising catalysts for RWGS. This work
highlights the power of combining advanced ML techniques with DFT and
microkinetic modeling for screening catalysts for complex reactions like RWGS,
providing a robust framework for future catalyst design.

</details>

<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [223] [A Unifying Framework for Robust and Efficient Inference with Unstructured Data](https://arxiv.org/abs/2505.00282)
*Jacob Carlson,Melissa Dell*

Main category: econ.EM

TLDR: 提出MARS框架，解决非结构化数据推断中的偏差问题，结合半参数推断方法，提供高效稳健的估计器。


<details>
  <summary>Details</summary>
Motivation: 神经网络预测可能引入偏差，影响基于非结构化数据的参数推断。

Method: 将非结构化数据推断重新定义为缺失结构化数据问题，利用神经网络填补数据，并应用半参数推断方法。

Result: 开发了MARS框架，提供高效稳健的估计器，适用于描述性和因果性估计。

Conclusion: MARS框架在理论和实际应用中均表现出色，解决了现有文献中未充分关注的问题。

Abstract: This paper presents a general framework for conducting efficient and robust
inference on parameters derived from unstructured data, which include text,
images, audio, and video. Economists have long incorporated data extracted from
texts and images into their analyses, a practice that has accelerated with
advancements in deep neural networks. However, neural networks do not
generically produce unbiased predictions, potentially propagating bias to
estimators that use their outputs. To address this challenge, we reframe
inference with unstructured data as a missing structured data problem, where
structured data are imputed from unstructured inputs using deep neural
networks. This perspective allows us to apply classic results from
semiparametric inference, yielding valid, efficient, and robust estimators
based on unstructured data. We formalize this approach with MARS (Missing At
Random Structured Data), a unifying framework that integrates and extends
existing methods for debiased inference using machine learning predictions,
linking them to a variety of older, familiar problems such as causal inference.
We develop robust and efficient estimators for both descriptive and causal
estimands and address challenges such as inference using aggregated and
transformed predictions from unstructured data. Importantly, MARS applies to
common empirical settings that have received limited attention in the existing
literature. Finally, we reanalyze prominent studies that use unstructured data,
demonstrating the practical value of MARS.

</details>

### [224] [Pre-Training Estimators for Structural Models: Application to Consumer Search](https://arxiv.org/abs/2505.00526)
*Yanhao 'Max' Wei,Zhenling Jiang*

Main category: econ.EM

TLDR: 论文提出了一种预训练结构计量经济学模型估计器的方法，通过神经网络识别模型参数，显著降低了后续应用的运算成本和研究者工作量。


<details>
  <summary>Details</summary>
Motivation: 解决传统结构计量经济学模型估计中计算成本高、研究者工作量大等问题，使模型更易于应用。

Method: 利用神经网络识别结构模型的参数，并以一个难以估计的序列搜索模型为例进行预训练。

Result: 在14个真实数据集上测试，估计速度快且准确性高。

Conclusion: 预训练估计器可以提升结构模型的易用性，促进研究和实践中的应用。

Abstract: We explore pretraining estimators for structural econometric models. The
estimator is "pretrained" in the sense that the bulk of the computational cost
and researcher effort occur during the construction of the estimator.
Subsequent applications of the estimator to different datasets require little
computational cost or researcher effort. The estimation leverages a neural net
to recognize the structural model's parameter from data patterns. As an initial
trial, this paper builds a pretrained estimator for a sequential search model
that is known to be difficult to estimate. We evaluate the pretrained estimator
on 14 real datasets. The estimation takes seconds to run and shows high
accuracy. We provide the estimator at pnnehome.github.io. More generally,
pretrained, off-the-shelf estimators can make structural models more accessible
to researchers and practitioners.

</details>

<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [225] [The Planted Orthogonal Vectors Problem](https://arxiv.org/abs/2505.00206)
*David Kühnemann,Adam Polak,Alon Rosen*

Main category: cs.CC

TLDR: 论文研究了k-正交向量（k-OV）问题，提出了一种在随机向量中植入唯一解的方法，并推测其平均情况下仍需n^{k-o(1)}时间求解。


<details>
  <summary>Details</summary>
Motivation: k-OV问题是细粒度复杂性理论中的核心问题，研究其平均情况下的复杂性有助于理解问题的本质。

Method: 通过在独立同分布的p-偏置向量中植入唯一解，并验证其平均情况下的复杂性。

Result: 证明了植入解的唯一性，并给出了平均情况下的搜索到决策的归约。

Conclusion: 该方法为k-OV问题的平均复杂性提供了新的视角，支持其平均情况下仍需高时间复杂度的猜想。

Abstract: In the $k$-Orthogonal Vectors ($k$-OV) problem we are given $k$ sets, each
containing $n$ binary vectors of dimension $d=n^{o(1)}$, and our goal is to
pick one vector from each set so that at each coordinate at least one vector
has a zero. It is a central problem in fine-grained complexity, conjectured to
require $n^{k-o(1)}$ time in the worst case.
  We propose a way to \emph{plant} a solution among vectors with i.i.d.
$p$-biased entries, for appropriately chosen $p$, so that the planted solution
is the unique one. Our conjecture is that the resulting $k$-OV instances still
require time $n^{k-o(1)}$ to solve, \emph{on average}.
  Our planted distribution has the property that any subset of strictly less
than $k$ vectors has the \emph{same} marginal distribution as in the model
distribution, consisting of i.i.d. $p$-biased random vectors. We use this
property to give average-case search-to-decision reductions for $k$-OV.

</details>

<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [226] [Edge Large AI Models: Revolutionizing 6G Networks](https://arxiv.org/abs/2505.00321)
*Zixin Wang,Yuanming Shi,Yong Zhou,Jingyang Zhu,Khaled. B. Letaief*

Main category: cs.NI

TLDR: 本文探讨了边缘大模型（edge LAM）在6G中的机遇与挑战，提出了协作微调和全参数训练框架，以及微服务辅助推理架构，以优化无线网络中的部署。


<details>
  <summary>Details</summary>
Motivation: 边缘LAM能够支持高度通用和多样化的任务，但由于无线网络的资源限制，其实际部署面临巨大挑战。

Method: 提出了协作微调和全参数训练框架，以及微服务辅助推理架构。

Result: 这些创新框架和应用为6G技术的发展提供了有价值的见解和解决方案。

Conclusion: 边缘LAM在6G中具有广阔的应用前景，但仍需解决资源管理等问题。

Abstract: Large artificial intelligence models (LAMs) possess human-like abilities to
solve a wide range of real-world problems, exemplifying the potential of
experts in various domains and modalities. By leveraging the communication and
computation capabilities of geographically dispersed edge devices, edge LAM
emerges as an enabling technology to empower the delivery of various real-time
intelligent services in 6G. Unlike traditional edge artificial intelligence
(AI) that primarily supports a single task using small models, edge LAM is
featured by the need of the decomposition and distributed deployment of large
models, and the ability to support highly generalized and diverse tasks.
However, due to limited communication, computation, and storage resources over
wireless networks, the vast number of trainable neurons and the substantial
communication overhead pose a formidable hurdle to the practical deployment of
edge LAMs. In this paper, we investigate the opportunities and challenges of
edge LAMs from the perspectives of model decomposition and resource management.
Specifically, we propose collaborative fine-tuning and full-parameter training
frameworks, alongside a microservice-assisted inference architecture, to
enhance the deployment of edge LAM over wireless networks. Additionally, we
investigate the application of edge LAM in air-interface designs, focusing on
channel prediction and beamforming. These innovative frameworks and
applications offer valuable insights and solutions for advancing 6G technology.

</details>

<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [227] [Auditing without Leaks Despite Curiosity](https://arxiv.org/abs/2505.00665)
*Hagit Attiya,Antonio Fernández Anta,Alessia Milani,Alexandre Rapetti,Corentin Travers*

Main category: cs.DC

TLDR: 本文提出了一种改进的审计性定义，重点关注读操作的有效性而非完成性，并设计了一种无等待的多写多读寄存器实现，防止未经授权的审计。


<details>
  <summary>Details</summary>
Motivation: 现有审计性定义未考虑读者可能通过非显式读取或非审计者身份获取数据访问信息的情况，需改进。

Method: 通过原子操作结合值访问和访问日志，并使用一次性密码加密技术记录访问，实现无等待的多写多读寄存器。

Result: 成功实现了一种防止未经授权审计的寄存器，并扩展至可审计的最大寄存器、快照对象和版本化类型。

Conclusion: 改进的审计性定义和实现方法有效解决了非显式读取和未经授权审计的问题，为更复杂的审计场景提供了基础。

Abstract: \textit{Auditing} data accesses helps preserve privacy and ensures
accountability by allowing one to determine who accessed (potentially
sensitive) information. A prior formal definition of register auditability was
based on the values returned by read operations, \emph{without accounting for
cases where a reader might learn a value without explicitly reading it or gain
knowledge of data access without being an auditor}.
  This paper introduces a refined definition of auditability that focuses on
when a read operation is \emph{effective}, rather than relying on its
completion and return of a value. Furthermore, we formally specify the
constraints that \textit{prevent readers from learning values they did not
explicitly read or from auditing other readers' accesses.}
  Our primary algorithmic contribution is a wait-free implementation of a
\emph{multi-writer, multi-reader register} that tracks effective reads while
preventing unauthorized audits. The key challenge is ensuring that a read is
auditable as soon as it becomes effective, which we achieve by combining value
access and access logging into a single atomic operation. Another challenge is
recording accesses without exposing them to readers, which we address using a
simple encryption technique (one-time pad).
  We extend this implementation to an \emph{auditable max register} that tracks
the largest value ever written. The implementation deals with the additional
challenge posed by the max register semantics, which allows readers to learn
prior values without reading them.
  The max register, in turn, serves as the foundation for implementing an
\emph{auditable snapshot} object and, more generally, \emph{versioned types}.
These extensions maintain the strengthened notion of auditability,
appropriately adapted from multi-writer, multi-reader registers.

</details>

### [228] [Intelligent Task Scheduling for Microservices via A3C-Based Reinforcement Learning](https://arxiv.org/abs/2505.00299)
*Yang Wang,Tengda Tang,Zhou Fang,Yingnan Deng,Yifei Duan*

Main category: cs.DC

TLDR: 本文提出了一种基于A3C强化学习算法的自适应资源调度方法，用于解决微服务系统中高资源动态性和高任务并发性的挑战。


<details>
  <summary>Details</summary>
Motivation: 微服务系统面临高资源动态性和密集任务并发的挑战，传统方法在重负载下资源分配效率低下。

Method: 将调度问题建模为马尔可夫决策过程，结合异步多线程学习机制，优化策略和价值网络以实现细粒度资源分配。

Result: 实验表明，该方法在任务延迟、调度成功率、资源利用率和收敛速度等指标上优于传统方法。

Conclusion: 该方法在并发任务环境中表现出高调度性能和系统稳定性，具有实际应用价值。

Abstract: To address the challenges of high resource dynamism and intensive task
concurrency in microservice systems, this paper proposes an adaptive resource
scheduling method based on the A3C reinforcement learning algorithm. The
scheduling problem is modeled as a Markov Decision Process, where policy and
value networks are jointly optimized to enable fine-grained resource allocation
under varying load conditions. The method incorporates an asynchronous
multi-threaded learning mechanism, allowing multiple agents to perform parallel
sampling and synchronize updates to the global network parameters. This design
improves both policy convergence efficiency and model stability. In the
experimental section, a real-world dataset is used to construct a scheduling
scenario. The proposed method is compared with several typical approaches
across multiple evaluation metrics, including task delay, scheduling success
rate, resource utilization, and convergence speed. The results show that the
proposed method delivers high scheduling performance and system stability in
multi-task concurrent environments. It effectively alleviates the resource
allocation bottlenecks faced by traditional methods under heavy load,
demonstrating its practical value for intelligent scheduling in microservice
systems.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [229] [On the expressivity of deep Heaviside networks](https://arxiv.org/abs/2505.00110)
*Insung Kong,Juntong Chen,Sophie Langer,Johannes Schmidt-Hieber*

Main category: stat.ML

TLDR: 深度Heaviside网络（DHNs）的表达能力有限，但可以通过引入跳跃连接或线性激活神经元来提升。论文提供了这些网络类的VC维度和逼近率的下界和上界，并应用于非参数回归模型中DHN拟合的统计收敛率。


<details>
  <summary>Details</summary>
Motivation: 研究深度Heaviside网络的表达能力限制，探索提升其性能的方法。

Method: 引入跳跃连接或线性激活神经元，分析其VC维度和逼近率。

Result: 提供了DHNs的VC维度和逼近率的下界和上界，并推导了其在非参数回归模型中的统计收敛率。

Conclusion: 通过跳跃连接或线性激活神经元可以显著提升DHNs的表达能力和应用效果。

Abstract: We show that deep Heaviside networks (DHNs) have limited expressiveness but
that this can be overcome by including either skip connections or neurons with
linear activation. We provide lower and upper bounds for the
Vapnik-Chervonenkis (VC) dimensions and approximation rates of these network
classes. As an application, we derive statistical convergence rates for DHN
fits in the nonparametric regression model.

</details>

### [230] [Inference for max-linear Bayesian networks with noise](https://arxiv.org/abs/2505.00229)
*Mark Adams,Kamillo Ferry,Ruriko Yoshida*

Main category: stat.ML

TLDR: MLBNs用于极值因果推断，通过取对数将拓扑参数转化为max-plus代数，证明DAG中边参数估计服从正态分布，并通过EM算法和二次优化进行实验验证。


<details>
  <summary>Details</summary>
Motivation: 研究极值环境下的因果推断问题，利用MLBNs框架提供理论支持。

Method: 采用max-plus代数对拓扑参数取对数，推导DAG边参数的正态分布估计，并应用EM算法和二次优化进行实验。

Result: 证明了DAG边参数估计服从正态分布，并通过实验验证了方法的有效性。

Conclusion: MLBNs在极值因果推断中具有潜力，实验验证了理论推导的可行性。

Abstract: Max-Linear Bayesian Networks (MLBNs) provide a powerful framework for causal
inference in extreme-value settings; we consider MLBNs with noise parameters
with a given topology in terms of the max-plus algebra by taking its logarithm.
Then, we show that an estimator of a parameter for each edge in a directed
acyclic graph (DAG) is distributed normally. We end this paper with
computational experiments with the expectation and maximization (EM) algorithm
and quadratic optimization.

</details>

### [231] [Reinforcement Learning with Continuous Actions Under Unmeasured Confounding](https://arxiv.org/abs/2505.00304)
*Yuhan Li,Eugene Han,Yifan Hu,Wenzhuo Zhou,Zhengling Qi,Yifan Cui,Ruoqing Zhu*

Main category: stat.ML

TLDR: 本文提出了一种在连续动作空间中处理未测量混杂因素的离线策略学习方法，通过非参数估计和极小极大估计器优化策略价值。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注离散动作空间的策略评估，而本文旨在解决连续动作空间中的未测量混杂问题。

Method: 建立非参数识别结果，开发极小极大估计器，并设计基于策略梯度的算法以优化策略价值。

Result: 理论证明了最优策略的一致性、有限样本误差界和遗憾界，仿真和实际数据验证了方法的有效性。

Conclusion: 本文方法在连续动作空间和未测量混杂条件下表现优越，为离线策略学习提供了新思路。

Abstract: This paper addresses the challenge of offline policy learning in
reinforcement learning with continuous action spaces when unmeasured
confounders are present. While most existing research focuses on policy
evaluation within partially observable Markov decision processes (POMDPs) and
assumes discrete action spaces, we advance this field by establishing a novel
identification result to enable the nonparametric estimation of policy value
for a given target policy under an infinite-horizon framework. Leveraging this
identification, we develop a minimax estimator and introduce a
policy-gradient-based algorithm to identify the in-class optimal policy that
maximizes the estimated policy value. Furthermore, we provide theoretical
results regarding the consistency, finite-sample error bound, and regret bound
of the resulting optimal policy. Extensive simulations and a real-world
application using the German Family Panel data demonstrate the effectiveness of
our proposed methodology.

</details>

### [232] [Statistical Learning for Heterogeneous Treatment Effects: Pretraining, Prognosis, and Prediction](https://arxiv.org/abs/2505.00310)
*Maximilian Schuessler,Erik Sverdrup,Robert Tibshirani*

Main category: stat.ML

TLDR: 论文提出了一种预训练策略，利用真实世界应用中预后因素与治疗效果异质性之间的关联，通过R-learner框架提高条件平均处理效应（CATE）的估计准确性。


<details>
  <summary>Details</summary>
Motivation: 异质性治疗效果估计是决策制定的关键挑战，现有机器学习方法虽灵活但面临高维协变量下的准确性不足问题。

Method: 基于R-learner框架，利用预后因素与治疗效果异质性的关联，通过残差损失解决个体预测问题，并整合“辅助信息”开发模型。

Result: 在存在协同效应的场景下，该方法降低了估计误差、减少假发现率，并提高了异质性检测的统计功效。

Conclusion: 通过跨任务学习，该方法显著提升了CATE估计的准确性，为个性化决策提供了更可靠的工具。

Abstract: Robust estimation of heterogeneous treatment effects is a fundamental
challenge for optimal decision-making in domains ranging from personalized
medicine to educational policy. In recent years, predictive machine learning
has emerged as a valuable toolbox for causal estimation, enabling more flexible
effect estimation. However, accurately estimating conditional average treatment
effects (CATE) remains a major challenge, particularly in the presence of many
covariates. In this article, we propose pretraining strategies that leverages a
phenomenon in real-world applications: factors that are prognostic of the
outcome are frequently also predictive of treatment effect heterogeneity. In
medicine, for example, components of the same biological signaling pathways
frequently influence both baseline risk and treatment response. Specifically,
we demonstrate our approach within the R-learner framework, which estimates the
CATE by solving individual prediction problems based on a residualized loss. We
use this structure to incorporate "side information" and develop models that
can exploit synergies between risk prediction and causal effect estimation. In
settings where these synergies are present, this cross-task learning enables
more accurate signal detection: yields lower estimation error, reduced false
discovery rates, and higher power for detecting heterogeneity.

</details>

### [233] [Hypothesis-free discovery from epidemiological data by automatic detection and local inference for tree-based nonlinearities and interactions](https://arxiv.org/abs/2505.00571)
*Giorgio Spadaccini,Marjolein Fokkema,Mark A. van de Wiel*

Main category: stat.ML

TLDR: RuleSHAP是一个结合稀疏贝叶斯回归、树集成和Shapley值的框架，用于流行病学中的假设自由发现，能够高效检测和测试复杂模式。


<details>
  <summary>Details</summary>
Motivation: 在流行病学中，机器学习虽能发现非线性关系和交互作用，但缺乏可靠的推断方法，限制了其应用。

Method: 提出RuleSHAP框架，结合稀疏贝叶斯回归、树集成和Shapley值，一步完成复杂模式的检测和测试，并推导了高效计算边际Shapley值的公式。

Result: 在模拟数据上验证了框架的有效性，并应用于流行病学队列数据，成功检测出高胆固醇和高血压的非线性交互作用。

Conclusion: RuleSHAP为流行病学中的假设自由发现提供了可靠且高效的推断工具。

Abstract: In epidemiological settings, Machine Learning (ML) is gaining popularity for
hypothesis-free discovery of risk (or protective) factors. Although ML is
strong at discovering non-linearities and interactions, this power is currently
compromised by a lack of reliable inference. Although local measures of feature
effect can be combined with tree ensembles, uncertainty quantifications for
these measures remain only partially available and oftentimes unsatisfactory.
We propose RuleSHAP, a framework for using rule-based, hypothesis-free
discovery that combines sparse Bayesian regression, tree ensembles and Shapley
values in a one-step procedure that both detects and tests complex patterns at
the individual level. To ease computation, we derive a formula that computes
marginal Shapley values more efficiently for our setting. We demonstrate the
validity of our framework on simulated data. To illustrate, we apply our
machinery to data from an epidemiological cohort to detect and infer several
effects for high cholesterol and blood pressure, such as nonlinear interaction
effects between features like age, sex, ethnicity, BMI and glucose level.

</details>

### [234] [Bayes-Optimal Fair Classification with Multiple Sensitive Features](https://arxiv.org/abs/2505.00631)
*Yi Yang,Yinghui Huang,Xiangyu Chang*

Main category: stat.ML

TLDR: 本文研究了多敏感特征下的贝叶斯最优公平分类器，提出了适用于一般近似公平度量的理论框架，并设计了两种实用算法。


<details>
  <summary>Details</summary>
Motivation: 现有理论通常只考虑单一敏感特征，而实践中个体常由多个敏感特征定义，因此需要扩展理论框架以涵盖多敏感特征情况。

Method: 通过线性变换将近似公平度量（如均值差和均值比）转化为特定群体的选择率，并推导出贝叶斯最优公平分类器的实例依赖阈值规则。

Result: 提出的框架适用于属性感知和属性盲设置，并能处理复合公平概念（如均衡赔率）。两种算法在实验中表现优于现有方法。

Conclusion: 本文为多敏感特征下的公平分类提供了理论支持和实用算法，扩展了现有研究的适用范围。

Abstract: Existing theoretical work on Bayes-optimal fair classifiers usually considers
a single (binary) sensitive feature. In practice, individuals are often defined
by multiple sensitive features. In this paper, we characterize the
Bayes-optimal fair classifier for multiple sensitive features under general
approximate fairness measures, including mean difference and mean ratio. We
show that these approximate measures for existing group fairness notions,
including Demographic Parity, Equal Opportunity, Predictive Equality, and
Accuracy Parity, are linear transformations of selection rates for specific
groups defined by both labels and sensitive features. We then characterize that
Bayes-optimal fair classifiers for multiple sensitive features become
instance-dependent thresholding rules that rely on a weighted sum of these
group membership probabilities. Our framework applies to both attribute-aware
and attribute-blind settings and can accommodate composite fairness notions
like Equalized Odds. Building on this, we propose two practical algorithms for
Bayes-optimal fair classification via in-processing and post-processing. We
show empirically that our methods compare favorably to existing methods.

</details>

<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [235] [Perceptual Implications of Automatic Anonymization in Pathological Speech](https://arxiv.org/abs/2505.00409)
*Soroosh Tayebi Arasteh,Saba Afza,Tri-Thien Nguyen,Lukas Buess,Maryam Parvin,Tomas Arias-Vergara,Paula Andrea Perez-Toro,Hiu Ching Hung,Mahshad Lotfinia,Thomas Gorges,Elmar Noeth,Maria Schuster,Seung Hee Yang,Andreas Maier*

Main category: eess.AS

TLDR: 该研究首次全面分析了匿名化病理语音的感知效果，发现匿名化虽能保护隐私，但显著降低语音质量，且效果因病理类型而异，需针对不同病理和背景制定匿名化策略。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索匿名化技术对病理语音共享的伦理影响及其感知后果，填补现有研究的空白。

Method: 采用结构化感知协议，由10名母语和非母语德语听者对180名不同病理类型的语音进行匿名化与原语音的对比评估，包括图灵式辨别和质量评分任务。

Result: 匿名化显著降低语音质量（从83%降至59%），辨别准确率高（91%-93%），但效果因病理类型不同。听者背景对评分影响小，无性别偏见。

Conclusion: 需开发针对病理和背景的匿名化策略，平衡隐私保护与语音质量，尤其关注儿童等脆弱群体。

Abstract: Automatic anonymization techniques are essential for ethical sharing of
pathological speech data, yet their perceptual consequences remain
understudied. This study presents the first comprehensive human-centered
analysis of anonymized pathological speech, using a structured perceptual
protocol involving ten native and non-native German listeners with diverse
linguistic, clinical, and technical backgrounds. Listeners evaluated
anonymized-original utterance pairs from 180 speakers spanning Cleft Lip and
Palate, Dysarthria, Dysglossia, Dysphonia, and age-matched healthy controls.
Speech was anonymized using state-of-the-art automatic methods (equal error
rates in the range of 30-40%). Listeners completed Turing-style discrimination
and quality rating tasks under zero-shot (single-exposure) and few-shot
(repeated-exposure) conditions. Discrimination accuracy was high overall (91%
zero-shot; 93% few-shot), but varied by disorder (repeated-measures ANOVA:
p=0.007), ranging from 96% (Dysarthria) to 86% (Dysphonia). Anonymization
consistently reduced perceived quality (from 83% to 59%, p<0.001), with
pathology-specific degradation patterns (one-way ANOVA: p=0.005). Native
listeners rated original speech slightly higher than non-native listeners
(Delta=4%, p=0.199), but this difference nearly disappeared after anonymization
(Delta=1%, p=0.724). No significant gender-based bias was observed. Critically,
human perceptual outcomes did not correlate with automatic privacy or clinical
utility metrics. These results underscore the need for listener-informed,
disorder- and context-specific anonymization strategies that preserve privacy
while maintaining interpretability, communicative functions, and diagnostic
utility, especially for vulnerable populations such as children.

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [236] [Neuroevolution of Self-Attention Over Proto-Objects](https://arxiv.org/abs/2505.00186)
*Rafael C. Pinto,Anderson R. Tavares*

Main category: cs.NE

TLDR: 论文提出了一种基于原型对象（proto-objects）的注意力机制，替代传统的基于矩形图像块的注意力机制，显著降低了计算复杂度和训练时间。


<details>
  <summary>Details</summary>
Motivation: 传统基于矩形图像块的注意力机制在视觉强化学习任务中表现优异，但存在计算复杂度和参数过多的问题。通过利用图像分割技术处理更高层次的特征，可以更高效地表示图像。

Method: 采用图像分割技术将图像分解为原型对象，每个原型对象编码为紧凑的特征向量，从而构建更小的自注意力模块。

Result: 实验表明，该方法在性能上匹配或超越基于图像块的实现，同时减少了62%的参数和2.6倍的训练时间。

Conclusion: 原型对象为基础的注意力机制是一种高效且性能优越的替代方案，适用于视觉强化学习任务。

Abstract: Proto-objects - image regions that share common visual properties - offer a
promising alternative to traditional attention mechanisms based on
rectangular-shaped image patches in neural networks. Although previous work
demonstrated that evolving a patch-based hard-attention module alongside a
controller network could achieve state-of-the-art performance in visual
reinforcement learning tasks, our approach leverages image segmentation to work
with higher-level features. By operating on proto-objects rather than fixed
patches, we significantly reduce the representational complexity: each image
decomposes into fewer proto-objects than regular patches, and each proto-object
can be efficiently encoded as a compact feature vector. This enables a
substantially smaller self-attention module that processes richer semantic
information. Our experiments demonstrate that this proto-object-based approach
matches or exceeds the state-of-the-art performance of patch-based
implementations with 62% less parameters and 2.6 times less training time.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [237] [Evaluating the AI-Lab Intervention: Impact on Student Perception and Use of Generative AI in Early Undergraduate Computer Science Courses](https://arxiv.org/abs/2505.00100)
*Ethan Dickey,Andres Bejarano,Rhianna Kuperus,Bárbara Fagundes*

Main category: cs.CY

TLDR: 研究探讨了生成式AI（GenAI）在计算机科学教育中的影响，通过结构化干预（AI-Lab）提升学生对AI工具的使用意识和技能发展。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在教育中的应用日益广泛，但其对学生学习、技能发展和认知的影响尚未充分研究，尤其是如何通过结构化指导避免过度依赖。

Method: 在普渡大学的四门课程中引入AI-Lab模块，采用混合方法分析831份前后调查问卷和焦点小组讨论数据。

Result: 干预后，学生对AI工具的舒适度和开放性显著提高，调试行为更谨慎，且更注重技能发展。

Conclusion: 结构化干预能帮助学生有效利用GenAI而不损害核心能力，为教育者提供了整合GenAI的实践建议。

Abstract: Generative AI (GenAI) is rapidly entering computer science education, yet its
effects on student learning, skill development, and perceptions remain
underexplored. Concerns about overreliance coexist with a gap in research on
structured scaffolding to guide tool use in formal courses. This study examines
the impact of a dedicated "AI-Lab" intervention -- emphasizing guided
scaffolding and mindful engagement -- on undergraduate students in Data
Structures and Algorithms, Competitive Programming, and first-year engineering
courses at Purdue University.
  Over three semesters, we integrated AI-Lab modules into four mandatory and
elective courses, yielding 831 matched pre- and post-intervention survey
responses, alongside focus group discussions. Employing a mixed-methods
approach, we analyzed quantitative shifts in usage patterns and attitudes as
well as qualitative narratives of student experiences.
  While the overall frequency of GenAI usage for homework or programming
projects remained largely stable, we observed large effect sizes in comfort and
openness across conceptual, debugging, and homework problems. Notably, usage
patterns for debugging also shifted statistically significantly, reflecting
students' more mindful and deliberate approach. Focus group discussions
corroborated these results, suggesting that the intervention "bridged the gap"
between naive GenAI usage and more nuanced, reflective integration of AI tools
into coursework, ultimately heightening students' awareness of their own skill
development.
  These findings suggest that structured, scaffolded interventions can enable
students to harness GenAI's benefits without undermining essential
competencies. We offer evidence-based recommendations for educators seeking to
integrate GenAI responsibly into computing curricula and identify avenues for
future research on GenAI-supported pedagogy.

</details>

### [238] [Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications](https://arxiv.org/abs/2505.00049)
*Wenhan Dong,Yuemeng Zhao,Zhen Sun,Yule Liu,Zifan Peng,Jingyi Zheng,Zongmin Zhang,Ziyi Zhang,Jun Wu,Ruiming Wang,Shengmin Xu,Xinyi Huang,Xinlei He*

Main category: cs.CY

TLDR: 论文系统回顾了将心理学理论应用于大语言模型（LLMs）的六个关键维度，包括评估工具、数据集、评估指标、实证发现、人格模拟方法和行为模拟，并指出了当前方法的优缺点。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在人类中心任务中的广泛应用，评估其心理特质对理解其社会影响和确保可信AI对齐至关重要。现有研究未系统讨论某些重要领域，如多样化的心理测试、LLM专用数据集及其应用。

Method: 系统回顾了六个维度：评估工具、LLM专用数据集、评估指标（一致性和稳定性）、实证发现、人格模拟方法和行为模拟。

Result: 部分LLMs在特定提示方案下表现出可复现的人格模式，但任务和设置间存在显著变异性。方法学挑战包括心理工具与LLM能力的不匹配及评估实践的不一致。

Conclusion: 研究旨在为开发更具可解释性、鲁棒性和泛化性的LLM心理评估框架提出未来方向。

Abstract: As large language models (LLMs) are increasingly used in human-centered
tasks, assessing their psychological traits is crucial for understanding their
social impact and ensuring trustworthy AI alignment. While existing reviews
have covered some aspects of related research, several important areas have not
been systematically discussed, including detailed discussions of diverse
psychological tests, LLM-specific psychological datasets, and the applications
of LLMs with psychological traits. To address this gap, we systematically
review six key dimensions of applying psychological theories to LLMs: (1)
assessment tools; (2) LLM-specific datasets; (3) evaluation metrics
(consistency and stability); (4) empirical findings; (5) personality simulation
methods; and (6) LLM-based behavior simulation. Our analysis highlights both
the strengths and limitations of current methods. While some LLMs exhibit
reproducible personality patterns under specific prompting schemes, significant
variability remains across tasks and settings. Recognizing methodological
challenges such as mismatches between psychological tools and LLMs'
capabilities, as well as inconsistencies in evaluation practices, this study
aims to propose future directions for developing more interpretable, robust,
and generalizable psychological assessment frameworks for LLMs.

</details>

### [239] [Algorithmic Collective Action with Two Collectives](https://arxiv.org/abs/2505.00195)
*Aditya Karan,Nicholas Vincent,Karrie Karahalios,Hari Sundaram*

Main category: cs.CY

TLDR: 论文提出了一种研究集体行动对数据驱动系统影响的框架，发现多集体行动时会产生意外交互，导致效能显著下降，并强调算法透明度和集体行动的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着数据依赖算法系统在更多领域产生影响，个体需要联合行动以维护自身利益并问责算法。

Method: 引入首个研究多集体战略行为的框架，通过语言模型分类器和推荐系统实验，分析集体目标、策略、规模和同质性对效能的影响。

Result: 多集体行动时，第一集体的效能可能下降75%；推荐系统中，完全异质或同质集体效能无显著优势，集体规模影响更大。

Conclusion: 研究呼吁算法模型和集体行为的透明度，为集体问责开发者并利用数据维护自身利益提供框架。

Abstract: Given that data-dependent algorithmic systems have become impactful in more
domains of life, the need for individuals to promote their own interests and
hold algorithms accountable has grown. To have meaningful influence,
individuals must band together to engage in collective action. Groups that
engage in such algorithmic collective action are likely to vary in size,
membership characteristics, and crucially, objectives. In this work, we
introduce a first of a kind framework for studying collective action with two
or more collectives that strategically behave to manipulate data-driven
systems. With more than one collective acting on a system, unexpected
interactions may occur. We use this framework to conduct experiments with
language model-based classifiers and recommender systems where two collectives
each attempt to achieve their own individual objectives. We examine how
differing objectives, strategies, sizes, and homogeneity can impact a
collective's efficacy. We find that the unintentional interactions between
collectives can be quite significant; a collective acting in isolation may be
able to achieve their objective (e.g., improve classification outcomes for
themselves or promote a particular item), but when a second collective acts
simultaneously, the efficacy of the first group drops by as much as $75\%$. We
find that, in the recommender system context, neither fully heterogeneous nor
fully homogeneous collectives stand out as most efficacious and that
heterogeneity's impact is secondary compared to collective size. Our results
signal the need for more transparency in both the underlying algorithmic models
and the different behaviors individuals or collectives may take on these
systems. This approach also allows collectives to hold algorithmic system
developers accountable and provides a framework for people to actively use
their own data to promote their own interests.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [240] [CoordField: Coordination Field for Agentic UAV Task Allocation In Low-altitude Urban Scenarios](https://arxiv.org/abs/2505.00091)
*Tengchao Zhang,Yonglin Tian,Fei Lin,Jun Huang,Rui Qin,Fei-Yue Wang*

Main category: cs.RO

TLDR: 论文提出了一种基于协调场代理系统的异构无人机群协调方法，利用大语言模型（LLMs）解析高层指令，并通过协调场机制实现任务分配，实验表明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决异构无人机群在复杂城市环境中高效语义理解、灵活任务规划和动态协调策略调整的挑战。

Method: 结合大语言模型解析指令，提出协调场机制实现分散式任务分配和自适应调整。

Result: 在50轮对比测试中，系统在任务覆盖率、响应时间和动态适应性方面表现优异。

Conclusion: 该系统为异构无人机群在复杂场景中的协调提供了有效解决方案。

Abstract: With the increasing demand for heterogeneous Unmanned Aerial Vehicle (UAV)
swarms to perform complex tasks in urban environments, system design now faces
major challenges, including efficient semantic understanding, flexible task
planning, and the ability to dynamically adjust coordination strategies in
response to evolving environmental conditions and continuously changing task
requirements. To address the limitations of existing approaches, this paper
proposes coordination field agentic system for coordinating heterogeneous UAV
swarms in complex urban scenarios. In this system, large language models (LLMs)
is responsible for interpreting high-level human instructions and converting
them into executable commands for the UAV swarms, such as patrol and target
tracking. Subsequently, a Coordination field mechanism is proposed to guide UAV
motion and task selection, enabling decentralized and adaptive allocation of
emergent tasks. A total of 50 rounds of comparative testing were conducted
across different models in a 2D simulation space to evaluate their performance.
Experimental results demonstrate that the proposed system achieves superior
performance in terms of task coverage, response time, and adaptability to
dynamic changes.

</details>

### [241] [AI-Enhanced Automatic Design of Efficient Underwater Gliders](https://arxiv.org/abs/2505.00222)
*Peter Yichen Chen,Pingchuan Ma,Niklas Hagemann,John Romanishin,Wei Wang,Daniela Rus,Wojciech Matusik*

Main category: cs.RO

TLDR: 论文提出了一种AI增强的自动化计算框架，用于设计复杂形状的水下滑翔机，通过优化形状和控制信号，提高了能源效率。


<details>
  <summary>Details</summary>
Motivation: 传统设计工具依赖人工试错，限制了水下滑翔机的形状多样性，且计算成本高。

Method: 采用降阶几何表示和基于神经网络的流体代理模型，实现形状与控制信号的协同优化。

Result: 实验验证表明，计算设计的滑翔机在能源效率上优于人工设计。

Conclusion: 该框架为高效水下滑翔机的开发提供了新途径，对海洋探索和环境监测有重要意义。

Abstract: The development of novel autonomous underwater gliders has been hindered by
limited shape diversity, primarily due to the reliance on traditional design
tools that depend heavily on manual trial and error. Building an automated
design framework is challenging due to the complexities of representing glider
shapes and the high computational costs associated with modeling complex
solid-fluid interactions. In this work, we introduce an AI-enhanced automated
computational framework designed to overcome these limitations by enabling the
creation of underwater robots with non-trivial hull shapes. Our approach
involves an algorithm that co-optimizes both shape and control signals,
utilizing a reduced-order geometry representation and a differentiable
neural-network-based fluid surrogate model. This end-to-end design workflow
facilitates rapid iteration and evaluation of hydrodynamic performance, leading
to the discovery of optimal and complex hull shapes across various control
settings. We validate our method through wind tunnel experiments and swimming
pool gliding tests, demonstrating that our computationally designed gliders
surpass manually designed counterparts in terms of energy efficiency. By
addressing challenges in efficient shape representation and neural fluid
surrogate models, our work paves the way for the development of highly
efficient underwater gliders, with implications for long-range ocean
exploration and environmental monitoring.

</details>

### [242] [LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving](https://arxiv.org/abs/2505.00284)
*Zhijie Qiao,Haowei Li,Zhong Cao,Henry X. Liu*

Main category: cs.RO

TLDR: LightEMMA是一个轻量级端到端多模态自动驾驶模型，用于评估VLMs在自动驾驶任务中的表现，发现其实用性能仍需改进。


<details>
  <summary>Details</summary>
Motivation: 探索如何充分利用VLMs在自动驾驶中的潜力，解决其在实际任务中的局限性。

Method: 构建12个基于不同VLMs的自动驾驶代理，并在nuScenes预测任务中评估其性能。

Result: 尽管VLMs在场景理解方面表现优异，但其在自动驾驶任务中的实际性能仍存在问题。

Conclusion: VLMs在自动驾驶中的应用仍需进一步改进，LightEMMA为相关研究提供了统一框架。

Abstract: Vision-Language Models (VLMs) have demonstrated significant potential for
end-to-end autonomous driving. However, fully exploiting their capabilities for
safe and reliable vehicle control remains an open research challenge. To
systematically examine advances and limitations of VLMs in driving tasks, we
introduce LightEMMA, a Lightweight End-to-End Multimodal Model for Autonomous
driving. LightEMMA provides a unified, VLM-based autonomous driving framework
without ad hoc customizations, enabling easy integration and evaluation of
evolving state-of-the-art commercial and open-source models. We construct
twelve autonomous driving agents using various VLMs and evaluate their
performance on the nuScenes prediction task, comprehensively assessing metrics
such as inference time, computational cost, and predictive accuracy.
Illustrative examples highlight that, despite their strong scenario
interpretation capabilities, VLMs' practical performance in autonomous driving
tasks remains concerning, emphasizing the need for further improvements. The
code is available at https://github.com/michigan-traffic-lab/LightEMMA.

</details>

### [243] [Robotic Visual Instruction](https://arxiv.org/abs/2505.00693)
*Yanbang Li,Ziyang Gong,Haoyang Li,Haoyang Li,Xiaoqi Huang,Haolan Kang,Guangping Bai,Xianzheng Ma*

Main category: cs.RO

TLDR: 提出了一种名为RoVI的视觉指令范式，通过手绘符号表示指导机器人任务，解决了自然语言交互的空间模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 自然语言在机器人控制中缺乏空间精确性，导致模糊性和冗长问题。

Method: 引入RoVI和VIEW流程，利用视觉语言模型解析2D符号指令并生成3D动作序列。

Result: 在11个新任务中验证，真实场景成功率达87.5%。

Conclusion: RoVI和VIEW显著提升了机器人任务的精确性和泛化能力。

Abstract: Recently, natural language has been the primary medium for human-robot
interaction. However, its inherent lack of spatial precision for robotic
control introduces challenges such as ambiguity and verbosity. To address these
limitations, we introduce the Robotic Visual Instruction (RoVI), a novel
paradigm to guide robotic tasks through an object-centric, hand-drawn symbolic
representation. RoVI effectively encodes spatial-temporal information into
human-interpretable visual instructions through 2D sketches, utilizing arrows,
circles, colors, and numbers to direct 3D robotic manipulation. To enable
robots to understand RoVI better and generate precise actions based on RoVI, we
present Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for
RoVI-conditioned policies. This approach leverages Vision-Language Models
(VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from
2D pixel space via keypoint extraction, and then transform them into executable
3D action sequences. We additionally curate a specialized dataset of 15K
instances to fine-tune small VLMs for edge deployment, enabling them to
effectively learn RoVI capabilities. Our approach is rigorously validated
across 11 novel tasks in both real and simulated environments, demonstrating
significant generalization capability. Notably, VIEW achieves an 87.5% success
rate in real-world scenarios involving unseen tasks that feature multi-step
actions, with disturbances, and trajectory-following requirements. Code and
Datasets in this paper will be released soon.

</details>

### [244] [AI2-Active Safety: AI-enabled Interaction-aware Active Safety Analysis with Vehicle Dynamics](https://arxiv.org/abs/2505.00322)
*Keshu Wu,Zihao Li,Sixu Li,Xinyue Ye,Dominique Lord,Yang Zhou*

Main category: cs.RO

TLDR: 论文提出了一种基于AI的交互感知主动安全分析框架，通过结合车辆动力学模型和超图AI模型，预测复杂交通环境中的车辆轨迹和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统安全分析方法难以捕捉复杂交通环境中的多车交互和不确定性，因此需要一种更精确的框架来提升安全感知能力。

Method: 框架结合了自行车模型（考虑道路坡度）和超图AI模型，通过随机微分方程计算车辆间距，并生成高保真安全指标（如TTC）。

Result: 与传统方法相比，该框架能更准确地反映复杂交通环境中的多车行为和不确定性，生成概率加权的高保真TTC分布。

Conclusion: 该框架为复杂交通环境中的主动安全分析提供了系统性方法，具有提升安全感知能力的潜力。

Abstract: This paper introduces an AI-enabled, interaction-aware active safety analysis
framework that accounts for groupwise vehicle interactions. Specifically, the
framework employs a bicycle model-augmented with road gradient
considerations-to accurately capture vehicle dynamics. In parallel, a
hypergraph-based AI model is developed to predict probabilistic trajectories of
ambient traffic. By integrating these two components, the framework derives
vehicle intra-spacing over a 3D road surface as the solution of a stochastic
ordinary differential equation, yielding high-fidelity surrogate safety
measures such as time-to-collision (TTC). To demonstrate its effectiveness, the
framework is analyzed using stochastic numerical methods comprising 4th-order
Runge-Kutta integration and AI inference, generating probability-weighted
high-fidelity TTC (HF-TTC) distributions that reflect complex multi-agent
maneuvers and behavioral uncertainties. Evaluated with HF-TTC against
traditional constant-velocity TTC and non-interaction-aware approaches on
highway datasets, the proposed framework offers a systematic methodology for
active safety analysis with enhanced potential for improving safety perception
in complex traffic environments.

</details>

### [245] [Future-Oriented Navigation: Dynamic Obstacle Avoidance with One-Shot Energy-Based Multimodal Motion Prediction](https://arxiv.org/abs/2505.00237)
*Ze Zhang,Georg Hess,Junjie Hu,Emmanuel Dean,Lennart Svensson,Knut Åkesson*

Main category: cs.RO

TLDR: 提出了一种集成方法，用于在动态和不确定环境中安全高效地控制移动机器人，结合多模态运动预测和模型预测控制。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中移动机器人导航的挑战，提高避障效率和安全性。

Method: 采用基于能量的神经网络进行多模态运动预测，并通过模型预测控制将预测结果融入运动规划。

Result: 在典型仓库场景中，该方法优于现有动态避障方法。

Conclusion: 该方法有效提升了移动机器人在动态环境中的导航能力。

Abstract: This paper proposes an integrated approach for the safe and efficient control
of mobile robots in dynamic and uncertain environments. The approach consists
of two key steps: one-shot multimodal motion prediction to anticipate motions
of dynamic obstacles and model predictive control to incorporate these
predictions into the motion planning process. Motion prediction is driven by an
energy-based neural network that generates high-resolution, multi-step
predictions in a single operation. The prediction outcomes are further utilized
to create geometric shapes formulated as mathematical constraints. Instead of
treating each dynamic obstacle individually, predicted obstacles are grouped by
proximity in an unsupervised way to improve performance and efficiency. The
overall collision-free navigation is handled by model predictive control with a
specific design for proactive dynamic obstacle avoidance. The proposed approach
allows mobile robots to navigate effectively in dynamic environments. Its
performance is accessed across various scenarios that represent typical
warehouse settings. The results demonstrate that the proposed approach
outperforms other existing dynamic obstacle avoidance methods.

</details>

### [246] [MULE: Multi-terrain and Unknown Load Adaptation for Effective Quadrupedal Locomotion](https://arxiv.org/abs/2505.00488)
*Vamshi Kumar Kurva,Shishir Kolathaya*

Main category: cs.RO

TLDR: 提出了一种自适应强化学习框架，使四足机器人能够动态适应变化的负载和地形，无需预定义步态或手动调整。


<details>
  <summary>Details</summary>
Motivation: 现有MPC方法依赖预定义步态或轨迹生成器，限制了在非结构化环境中的适应性。

Method: 结合基线运动策略和自适应策略，通过强化学习动态调整动作以保持稳定性和跟踪性能。

Result: 在仿真和硬件实验中，自适应控制器在多种地形和负载变化下表现优于传统方法。

Conclusion: 该框架显著提升了四足机器人在复杂环境中的适应性和鲁棒性。

Abstract: Quadrupedal robots are increasingly deployed for load-carrying tasks across
diverse terrains. While Model Predictive Control (MPC)-based methods can
account for payload variations, they often depend on predefined gait schedules
or trajectory generators, limiting their adaptability in unstructured
environments. To address these limitations, we propose an Adaptive
Reinforcement Learning (RL) framework that enables quadrupedal robots to
dynamically adapt to both varying payloads and diverse terrains. The framework
consists of a nominal policy responsible for baseline locomotion and an
adaptive policy that learns corrective actions to preserve stability and
improve command tracking under payload variations. We validate the proposed
approach through large-scale simulation experiments in Isaac Gym and real-world
hardware deployment on a Unitree Go1 quadruped. The controller was tested on
flat ground, slopes, and stairs under both static and dynamic payload changes.
Across all settings, our adaptive controller consistently outperformed the
controller in tracking body height and velocity commands, demonstrating
enhanced robustness and adaptability without requiring explicit gait design or
manual tuning.

</details>

### [247] [Optimal Interactive Learning on the Job via Facility Location Planning](https://arxiv.org/abs/2505.00490)
*Shivam Vats,Michelle Zhao,Patrick Callaghan,Mingxi Jia,Maxim Likhachev,Oliver Kroemer,George Konidaris*

Main category: cs.RO

TLDR: COIL是一种多任务交互规划器，通过策略性地选择查询类型（技能、偏好和帮助）来最小化人类在任务序列中的努力。


<details>
  <summary>Details</summary>
Motivation: 协作机器人需要持续适应新任务和用户偏好，同时避免给用户带来过多负担。现有方法通常局限于单任务场景，不适合持续的多任务协作。

Method: COIL将问题建模为无容量限制设施选址（UFL）问题，并在多项式时间内实现近似最优规划。针对用户偏好不确定性，引入一步信念空间规划。

Result: 模拟和物理实验表明，COIL显著减少了人类的工作量，同时保持任务成功完成。

Conclusion: COIL在多任务协作中有效降低了人类负担，适用于实际应用。

Abstract: Collaborative robots must continually adapt to novel tasks and user
preferences without overburdening the user. While prior interactive robot
learning methods aim to reduce human effort, they are typically limited to
single-task scenarios and are not well-suited for sustained, multi-task
collaboration. We propose COIL (Cost-Optimal Interactive Learning) -- a
multi-task interaction planner that minimizes human effort across a sequence of
tasks by strategically selecting among three query types (skill, preference,
and help). When user preferences are known, we formulate COIL as an
uncapacitated facility location (UFL) problem, which enables bounded-suboptimal
planning in polynomial time using off-the-shelf approximation algorithms. We
extend our formulation to handle uncertainty in user preferences by
incorporating one-step belief space planning, which uses these approximation
algorithms as subroutines to maintain polynomial-time performance. Simulated
and physical experiments on manipulation tasks show that our framework
significantly reduces the amount of work allocated to the human while
maintaining successful task completion.

</details>

### [248] [FedEMA: Federated Exponential Moving Averaging with Negative Entropy Regularizer in Autonomous Driving](https://arxiv.org/abs/2505.00318)
*Wei-Bin Kou,Guangxu Zhu,Bingyang Cheng,Shuai Wang,Ming Tang,Yik-Chung Wu*

Main category: cs.RO

TLDR: 论文提出了一种名为FedEMA的新框架，通过融合当前和历史的联邦学习模型以及负熵正则化，解决了自动驾驶模型在动态环境中的遗忘问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在动态环境中部署时，联邦学习模型面临时间性灾难性遗忘问题，导致历史知识丢失。

Method: 提出FedEMA框架，包括服务器端的历史模型融合和车辆端的负熵正则化，以平衡模型的泛化性和适应性。

Result: 在Cityscapes和Camvid数据集上的实验表明，FedEMA比现有方法提高了7.12%的mIoU。

Conclusion: FedEMA通过双重优化策略有效解决了时间性遗忘问题，提升了自动驾驶场景的语义理解能力。

Abstract: Street Scene Semantic Understanding (denoted as S3U) is a crucial but complex
task for autonomous driving (AD) vehicles. Their inference models typically
face poor generalization due to domain-shift. Federated Learning (FL) has
emerged as a promising paradigm for enhancing the generalization of AD models
through privacy-preserving distributed learning. However, these FL AD models
face significant temporal catastrophic forgetting when deployed in dynamically
evolving environments, where continuous adaptation causes abrupt erosion of
historical knowledge. This paper proposes Federated Exponential Moving Average
(FedEMA), a novel framework that addresses this challenge through two integral
innovations: (I) Server-side model's historical fitting capability preservation
via fusing current FL round's aggregation model and a proposed previous FL
round's exponential moving average (EMA) model; (II) Vehicle-side negative
entropy regularization to prevent FL models' possible overfitting to
EMA-introduced temporal patterns. Above two strategies empower FedEMA a
dual-objective optimization that balances model generalization and
adaptability. In addition, we conduct theoretical convergence analysis for the
proposed FedEMA. Extensive experiments both on Cityscapes dataset and Camvid
dataset demonstrate FedEMA's superiority over existing approaches, showing
7.12% higher mean Intersection-over-Union (mIoU).

</details>

### [249] [Safety-Critical Traffic Simulation with Guided Latent Diffusion Model](https://arxiv.org/abs/2505.00515)
*Mingxing Peng,Ruoyu Yao,Xusen Guo,Yuting Xie,Xianda Chen,Jun Ma*

Main category: cs.RO

TLDR: 提出了一种基于引导潜在扩散模型（LDM）的方法，用于生成物理真实且对抗性的安全关键交通场景，提高了生成效率和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成安全关键交通场景时存在物理不真实和效率低下的问题，需要改进。

Method: 使用图变分自编码器（VAE）学习紧凑的潜在空间，结合扩散模型和引导目标生成对抗性且真实的驾驶行为。

Result: 在nuScenes数据集上验证了方法的优越性，生成效率高且对抗性强，同时保持高真实性。

Conclusion: 该方法为自动驾驶系统的安全关键场景仿真提供了有效工具，有助于更鲁棒的系统评估。

Abstract: Safety-critical traffic simulation plays a crucial role in evaluating
autonomous driving systems under rare and challenging scenarios. However,
existing approaches often generate unrealistic scenarios due to insufficient
consideration of physical plausibility and suffer from low generation
efficiency. To address these limitations, we propose a guided latent diffusion
model (LDM) capable of generating physically realistic and adversarial
safety-critical traffic scenarios. Specifically, our model employs a
graph-based variational autoencoder (VAE) to learn a compact latent space that
captures complex multi-agent interactions while improving computational
efficiency. Within this latent space, the diffusion model performs the
denoising process to produce realistic trajectories. To enable controllable and
adversarial scenario generation, we introduce novel guidance objectives that
drive the diffusion process toward producing adversarial and behaviorally
realistic driving behaviors. Furthermore, we develop a sample selection module
based on physical feasibility checks to further enhance the physical
plausibility of the generated scenarios. Extensive experiments on the nuScenes
dataset demonstrate that our method achieves superior adversarial effectiveness
and generation efficiency compared to existing baselines while maintaining a
high level of realism. Our work provides an effective tool for realistic
safety-critical scenario simulation, paving the way for more robust evaluation
of autonomous driving systems.

</details>

### [250] [Implicit Neural-Representation Learning for Elastic Deformable-Object Manipulations](https://arxiv.org/abs/2505.00500)
*Minseok Song,JeongHo Ha,Bonggyeong Park,Daehyung Park*

Main category: cs.RO

TLDR: 提出了一种基于隐式神经表示（INR）的方法INR-DOM，用于解决可变形物体（如弹性带）的操控问题，通过强化学习优化表示学习，并在仿真和真实环境中验证。


<details>
  <summary>Details</summary>
Motivation: 可变形物体操控（DOM）因物体自由度无限且观测部分密集，导致状态空间大、采样复杂性和策略学习不确定性高。

Method: 提出INR-DOM方法，学习与部分可观测弹性物体一致的状态表示，并通过强化学习进行表示微调。

Result: 在仿真环境和真实机械臂（Franka Emika Panda）上进行了定量和定性分析，验证了方法的有效性。

Conclusion: INR-DOM通过隐式神经表示和强化学习，有效解决了可变形物体操控中的状态表示和策略学习问题。

Abstract: We aim to solve the problem of manipulating deformable objects, particularly
elastic bands, in real-world scenarios. However, deformable object manipulation
(DOM) requires a policy that works on a large state space due to the unlimited
degree of freedom (DoF) of deformable objects. Further, their dense but partial
observations (e.g., images or point clouds) may increase the sampling
complexity and uncertainty in policy learning. To figure it out, we propose a
novel implicit neural-representation (INR) learning for elastic DOMs, called
INR-DOM. Our method learns consistent state representations associated with
partially observable elastic objects reconstructing a complete and implicit
surface represented as a signed distance function. Furthermore, we perform
exploratory representation fine-tuning through reinforcement learning (RL) that
enables RL algorithms to effectively learn exploitable representations while
efficiently obtaining a DOM policy. We perform quantitative and qualitative
analyses building three simulated environments and real-world manipulation
studies with a Franka Emika Panda arm. Videos are available at
http://inr-dom.github.io.

</details>

### [251] [TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching](https://arxiv.org/abs/2505.00562)
*Yue Meng,Chuchu Fan*

Main category: cs.RO

TLDR: 提出TeLoGraF方法，利用图神经网络和流匹配技术解决通用STL规范问题，性能优于基线方法，推理速度快10-100倍。


<details>
  <summary>Details</summary>
Motivation: 解决复杂任务时，现有方法因缺乏多样STL数据集和有效编码器，仅能处理固定或参数化STL规范。

Method: 使用图神经网络编码器和流匹配技术，收集20万条STL规范及演示数据，在五个仿真环境中实验。

Result: 在STL满足率上优于基线方法，推理速度快10-100倍，且适用于任意系统动态。

Conclusion: TeLoGraF能高效解决复杂STL问题，并对分布外STL规范具有鲁棒性。

Abstract: Learning to solve complex tasks with signal temporal logic (STL)
specifications is crucial to many real-world applications. However, most
previous works only consider fixed or parametrized STL specifications due to
the lack of a diverse STL dataset and encoders to effectively extract temporal
logic information for downstream tasks. In this paper, we propose TeLoGraF,
Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN)
encoder and flow-matching to learn solutions for general STL specifications. We
identify four commonly used STL templates and collect a total of 200K
specifications with paired demonstrations. We conduct extensive experiments in
five simulation environments ranging from simple dynamical models in the 2D
space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped
navigation. Results show that our method outperforms other baselines in the STL
satisfaction rate. Compared to classical STL planning algorithms, our approach
is 10-100X faster in inference and can work on any system dynamics. Besides, we
show our graph-encoding method's capability to solve complex STLs and
robustness to out-distribution STL specifications. Code is available at
https://github.com/mengyuest/TeLoGraF

</details>

### [252] [A Finite-State Controller Based Offline Solver for Deterministic POMDPs](https://arxiv.org/abs/2505.00596)
*Alex Schutz,Yang You,Matias Mattamala,Ipek Caliskanelli,Bruno Lacerda,Nick Hawes*

Main category: cs.RO

TLDR: DetMCVI算法是MCVI的改进版，用于解决确定性部分可观察马尔可夫决策过程（DetPOMDPs），在大型问题中表现优异。


<details>
  <summary>Details</summary>
Motivation: DetPOMDPs在规划问题中常见，但现有方法处理能力有限，需更高效的解决方案。

Method: 提出DetMCVI算法，基于MCVI改进，生成有限状态控制器（FSCs）形式的策略。

Result: DetMCVI在大型问题中成功率高，优于现有基线方法，并在真实机器人森林测绘场景中验证。

Conclusion: DetMCVI为DetPOMDPs提供了高效解决方案，适用于实际应用。

Abstract: Deterministic partially observable Markov decision processes (DetPOMDPs)
often arise in planning problems where the agent is uncertain about its
environmental state but can act and observe deterministically. In this paper,
we propose DetMCVI, an adaptation of the Monte Carlo Value Iteration (MCVI)
algorithm for DetPOMDPs, which builds policies in the form of finite-state
controllers (FSCs). DetMCVI solves large problems with a high success rate,
outperforming existing baselines for DetPOMDPs. We also verify the performance
of the algorithm in a real-world mobile robot forest mapping scenario.

</details>

### [253] [Neural Network Verification for Gliding Drone Control: A Case Study](https://arxiv.org/abs/2505.00622)
*Colin Kessler,Ekaterina Komendantskaya,Marco Casadio,Ignazio Maria Viola,Thomas Flinkow,Albaraa Ammar Othman,Alistair Malhotra,Robbie McPherson*

Main category: cs.RO

TLDR: 论文提出了一种验证受Alsomitra启发的微型滑翔无人机神经控制器的案例研究，旨在提高轨迹跟踪性能，并探讨了现有工具的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在自主系统中的广泛应用，验证神经网络控制器的需求日益增长。本文旨在解决微型滑翔无人机在轨迹跟踪中的验证问题。

Method: 提出了一种新的鲁棒回归网络训练方法，并在Vehicle和CORA工具中形式化验证了案例研究。

Result: 验证结果表明，所研究的训练方法提高了控制器的性能和鲁棒性，但受限于工具和系统复杂性。

Conclusion: 尽管存在局限性，克服这些问题将有助于开发更安全、鲁棒的技术，改善生活并减少环境影响。

Abstract: As machine learning is increasingly deployed in autonomous systems,
verification of neural network controllers is becoming an active research
domain. Existing tools and annual verification competitions suggest that soon
this technology will become effective for real-world applications. Our
application comes from the emerging field of microflyers that are passively
transported by the wind, which may have various uses in weather or pollution
monitoring. Specifically, we investigate centimetre-scale bio-inspired gliding
drones that resemble Alsomitra macrocarpa diaspores. In this paper, we propose
a new case study on verifying Alsomitra-inspired drones with neural network
controllers, with the aim of adhering closely to a target trajectory. We show
that our system differs substantially from existing VNN and ARCH competition
benchmarks, and show that a combination of tools holds promise for verifying
such systems in the future, if certain shortcomings can be overcome. We propose
a novel method for robust training of regression networks, and investigate
formalisations of this case study in Vehicle and CORA. Our verification results
suggest that the investigated training methods do improve performance and
robustness of neural network controllers in this application, but are limited
in scope and usefulness. This is due to systematic limitations of both Vehicle
and CORA, and the complexity of our system reducing the scale of reachability,
which we investigate in detail. If these limitations can be overcome, it will
enable engineers to develop safe and robust technologies that improve people's
lives and reduce our impact on the environment.

</details>

### [254] [ParkDiffusion: Heterogeneous Multi-Agent Multi-Modal Trajectory Prediction for Automated Parking using Diffusion Models](https://arxiv.org/abs/2505.00586)
*Jiarong Wei,Niclas Vödisch,Anna Rehr,Christian Feist,Abhinav Valada*

Main category: cs.RO

TLDR: ParkDiffusion是一种基于扩散模型的新方法，用于预测自动停车场景中车辆和行人的轨迹，结合了语义和几何信息，并通过控制信号确保运动学可行性。


<details>
  <summary>Details</summary>
Motivation: 自动停车是高级驾驶辅助系统（ADAS）的关键功能，但现有研究多集中于单模态轨迹预测，缺乏对多模态和不确定性的处理。

Method: 提出ParkDiffusion，采用扩散模型捕捉轨迹的不确定性和多模态性，包括双地图编码器、自适应代理类型嵌入模块和运动学框架。

Result: 在DLP和inD数据集上表现优异，显著优于现有方法。

Conclusion: ParkDiffusion为停车场景中的异构轨迹预测设定了新基准。

Abstract: Automated parking is a critical feature of Advanced Driver Assistance Systems
(ADAS), where accurate trajectory prediction is essential to bridge perception
and planning modules. Despite its significance, research in this domain remains
relatively limited, with most existing studies concentrating on single-modal
trajectory prediction of vehicles. In this work, we propose ParkDiffusion, a
novel approach that predicts the trajectories of both vehicles and pedestrians
in automated parking scenarios. ParkDiffusion employs diffusion models to
capture the inherent uncertainty and multi-modality of future trajectories,
incorporating several key innovations. First, we propose a dual map encoder
that processes soft semantic cues and hard geometric constraints using a
two-step cross-attention mechanism. Second, we introduce an adaptive agent type
embedding module, which dynamically conditions the prediction process on the
distinct characteristics of vehicles and pedestrians. Third, to ensure
kinematic feasibility, our model outputs control signals that are subsequently
used within a kinematic framework to generate physically feasible trajectories.
We evaluate ParkDiffusion on the Dragon Lake Parking (DLP) dataset and the
Intersections Drone (inD) dataset. Our work establishes a new baseline for
heterogeneous trajectory prediction in parking scenarios, outperforming
existing methods by a considerable margin.

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [255] [Voice Cloning: Comprehensive Survey](https://arxiv.org/abs/2505.00579)
*Hussam Azzuni,Abdulmotaleb El Saddik*

Main category: cs.SD

TLDR: 本文旨在为语音克隆建立标准化术语，并探讨其不同变体，包括说话人适应、少样本、零样本和多语言TTS，同时总结评估指标和相关数据集。


<details>
  <summary>Details</summary>
Motivation: 语音克隆技术快速发展，但缺乏标准化术语和全面研究，本文旨在填补这一空白并促进研究以防止滥用。

Method: 通过综述现有语音克隆算法，分析说话人适应、少样本、零样本和多语言TTS等技术。

Result: 总结了语音克隆的多种变体、评估指标和相关数据集。

Conclusion: 本文为语音克隆研究提供了标准化框架，并呼吁进一步研究以限制其滥用。

Abstract: Voice Cloning has rapidly advanced in today's digital world, with many
researchers and corporations working to improve these algorithms for various
applications. This article aims to establish a standardized terminology for
voice cloning and explore its different variations. It will cover speaker
adaptation as the fundamental concept and then delve deeper into topics such as
few-shot, zero-shot, and multilingual TTS within that context. Finally, we will
explore the evaluation metrics commonly used in voice cloning research and
related datasets. This survey compiles the available voice cloning algorithms
to encourage research toward its generation and detection to limit its misuse.

</details>

<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [256] [Over-the-Air Inference over Multi-hop MIMO Networks](https://arxiv.org/abs/2505.00430)
*Chenghong Bian,Meng Hua,Deniz Gunduz*

Main category: eess.SP

TLDR: 提出了一种基于多跳MIMO网络的空中机器学习框架，通过设计预编码矩阵模仿全连接神经网络层，实现了在功率约束下的满意分类性能。


<details>
  <summary>Details</summary>
Motivation: 探索在多跳MIMO网络中实现高效机器学习传输的方法，解决功率约束和分类性能的平衡问题。

Method: 设计PrototypeNet神经网络，通过优化预编码矩阵和噪声注入训练，满足功率约束。

Result: 在功率约束下实现了满意的分类精度，且随着跳数增加，分类精度提升。

Conclusion: 该框架在多跳MIMO网络中有效，为空中机器学习提供了新思路。

Abstract: A novel over-the-air machine learning framework over multi-hop multiple-input
and multiple-output (MIMO) networks is proposed. The core idea is to imitate
fully connected (FC) neural network layers using multiple MIMO channels by
carefully designing the precoding matrices at the transmitting nodes. A neural
network dubbed PrototypeNet is employed consisting of multiple FC layers, with
the number of neurons of each layer equal to the number of antennas of the
corresponding terminal. To achieve satisfactory performance, we train
PrototypeNet based on a customized loss function consisting of classification
error and the power of latent vectors to satisfy transmit power constraints,
with noise injection during training. Precoding matrices for each hop are then
obtained by solving an optimization problem. We also propose a multiple-block
extension when the number of antennas is limited. Numerical results verify that
the proposed over-the-air transmission scheme can achieve satisfactory
classification accuracy under a power constraint. The results also show that
higher classification accuracy can be achieved with an increasing number of
hops at a modest signal-to-noise ratio (SNR).

</details>

<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [257] [Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems](https://arxiv.org/abs/2505.00212)
*Shaokun Zhang,Ming Yin,Jieyu Zhang,Jiale Liu,Zhiguang Han,Jingyang Zhang,Beibin Li,Chi Wang,Huazheng Wang,Yiran Chen,Qingyun Wu*

Main category: cs.MA

TLDR: 论文提出了一种新研究领域：LLM多智能体系统中的自动化故障归因，并介绍了Who&When数据集和三种方法，但效果有限。


<details>
  <summary>Details</summary>
Motivation: LLM多智能体系统中的故障归因对调试至关重要，但目前研究不足且劳动密集。

Method: 提出并评估了三种自动化故障归因方法，使用Who&When数据集进行测试。

Result: 最佳方法在识别故障责任代理上达到53.5%准确率，但在定位故障步骤上仅14.2%，部分方法表现低于随机。

Conclusion: 任务复杂，现有方法（包括SOTA模型）实用性不足，需进一步研究。

Abstract: Failure attribution in LLM multi-agent systems-identifying the agent and step
responsible for task failures-provides crucial clues for systems debugging but
remains underexplored and labor-intensive. In this paper, we propose and
formulate a new research area: automated failure attribution for LLM
multi-agent systems. To support this initiative, we introduce the Who&When
dataset, comprising extensive failure logs from 127 LLM multi-agent systems
with fine-grained annotations linking failures to specific agents and decisive
error steps. Using the Who&When, we develop and evaluate three automated
failure attribution methods, summarizing their corresponding pros and cons. The
best method achieves 53.5% accuracy in identifying failure-responsible agents
but only 14.2% in pinpointing failure steps, with some methods performing below
random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to
achieve practical usability. These results highlight the task's complexity and
the need for further research in this area. Code and dataset are available at
https://github.com/mingyin1/Agents_Failure_Attribution

</details>

### [258] [Emergence of Roles in Robotic Teams with Model Sharing and Limited Communication](https://arxiv.org/abs/2505.00540)
*Ian O'Flynn,Harun Šiljak*

Main category: cs.MA

TLDR: 提出一种集中式强化学习策略，用于多智能体觅食系统，通过周期性传播学习模型以减少计算和能源需求。


<details>
  <summary>Details</summary>
Motivation: 在多智能体强化学习（MARL）常见的领域中，减少计算和能源需求，同时实现高性能觅食智能体，以应用于物流、环境监测和自主探索等实际场景。

Method: 集中式学习策略，单个智能体学习并周期性传播模型至非学习智能体，结合奖励函数促进角色分化。

Result: 智能体行为差异化，动态调整角色，无需显式通信。

Conclusion: 该方法在减少资源消耗的同时，实现了高效的多智能体协作，适用于实际应用。

Abstract: We present a reinforcement learning strategy for use in multi-agent foraging
systems in which the learning is centralised to a single agent and its model is
periodically disseminated among the population of non-learning agents. In a
domain where multi-agent reinforcement learning (MARL) is the common approach,
this approach aims to significantly reduce the computational and energy demands
compared to approaches such as MARL and centralised learning models. By
developing high performing foraging agents, these approaches can be translated
into real-world applications such as logistics, environmental monitoring, and
autonomous exploration. A reward function was incorporated into this approach
that promotes role development among agents, without explicit directives. This
led to the differentiation of behaviours among the agents. The implicit
encouragement of role differentiation allows for dynamic actions in which
agents can alter roles dependent on their interactions with the environment
without the need for explicit communication between agents.

</details>

<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [259] [D-Tracker: Modeling Interest Diffusion in Social Activity Tensor Data Streams](https://arxiv.org/abs/2505.00242)
*Shingo Higashiguchi,Yasuko Matsubara,Koki Kawabata,Taichi Murayama,Yasushi Sakurai*

Main category: cs.SI

TLDR: D-Tracker是一种用于从社交活动张量数据流中捕捉时变模式并预测未来活动的方法，具有可解释性、自动化和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 社交活动数据（如网络搜索量和传染病感染数）反映了人们的兴趣和活动，但因其高维性和时变动态性（如趋势、季节性和兴趣扩散）而难以建模和预测。

Method: D-Tracker将偏微分方程融入张量分解框架，以可解释的方式捕捉时变模式（如趋势、季节性和兴趣扩散），无需超参数且计算时间与时间序列长度无关。

Result: 实验表明，D-Tracker在GoogleTrends搜索量和COVID-19感染数据上比现有方法具有更高的预测准确性和更短的计算时间。

Conclusion: D-Tracker是一种高效、自动且可扩展的方法，适用于社交活动数据流的建模和预测。

Abstract: Large quantities of social activity data, such as weekly web search volumes
and the number of new infections with infectious diseases, reflect peoples'
interests and activities. It is important to discover temporal patterns from
such data and to forecast future activities accurately. However, modeling and
forecasting social activity data streams is difficult because they are
high-dimensional and composed of multiple time-varying dynamics such as trends,
seasonality, and interest diffusion. In this paper, we propose D-Tracker, a
method for continuously capturing time-varying temporal patterns within social
activity tensor data streams and forecasting future activities. Our proposed
method has the following properties: (a) Interpretable: it incorporates the
partial differential equation into a tensor decomposition framework and
captures time-varying temporal patterns such as trends, seasonality, and
interest diffusion between locations in an interpretable manner; (b) Automatic:
it has no hyperparameters and continuously models tensor data streams fully
automatically; (c) Scalable: the computation time of D-Tracker is independent
of the time series length. Experiments using web search volume data obtained
from GoogleTrends, and COVID-19 infection data obtained from COVID-19 Open Data
Repository show that our method can achieve higher forecasting accuracy in less
computation time than existing methods while extracting the interest diffusion
between locations. Our source code and datasets are available at
{https://github.com/Higashiguchi-Shingo/D-Tracker.

</details>