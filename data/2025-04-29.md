<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 44]
- [cs.LG](#cs.LG) [Total: 102]
- [cs.CL](#cs.CL) [Total: 66]
- [cs.AI](#cs.AI) [Total: 33]
- [cs.CV](#cs.CV) [Total: 134]
- [math.ST](#math.ST) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.GR](#cs.GR) [Total: 5]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.HC](#cs.HC) [Total: 7]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.CY](#cs.CY) [Total: 6]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.SE](#cs.SE) [Total: 10]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.ET](#cs.ET) [Total: 1]
- [eess.IV](#eess.IV) [Total: 8]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 6]
- [quant-ph](#quant-ph) [Total: 4]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.CC](#cs.CC) [Total: 2]
- [econ.GN](#econ.GN) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [cs.RO](#cs.RO) [Total: 14]
- [eess.AS](#eess.AS) [Total: 3]
- [math.OC](#math.OC) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 4]
- [eess.SY](#eess.SY) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [stat.ML](#stat.ML) [Total: 11]
- [cs.NI](#cs.NI) [Total: 1]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Backdoor Defense in Diffusion Models via Spatial Attention Unlearning](https://arxiv.org/abs/2504.18563)
*Abha Jha,Ashwath Vaithinathan Aravindan,Matthew Salaway,Atharva Sandeep Bhide,Duygu Nur Yaldiz*

Main category: cs.CR

TLDR: SAU是一种通过潜在空间操作和空间注意力机制消除扩散模型中后门攻击的新技术，效果显著且高效。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型易受后门攻击，现有防御机制不足，尤其是高维输出空间增加了检测和缓解的难度。

Method: 提出Spatial Attention Unlearning（SAU），利用潜在空间操作和空间注意力机制隔离并移除后门触发的潜在表示。

Result: SAU在多种后门攻击中实现100%触发移除准确率，CLIP得分0.7023，优于现有方法且保持图像生成质量。

Conclusion: SAU是一种鲁棒、可扩展且实用的解决方案，能有效保护文本到图像扩散模型免受后门攻击。

Abstract: Text-to-image diffusion models are increasingly vulnerable to backdoor
attacks, where malicious modifications to the training data cause the model to
generate unintended outputs when specific triggers are present. While
classification models have seen extensive development of defense mechanisms,
generative models remain largely unprotected due to their high-dimensional
output space, which complicates the detection and mitigation of subtle
perturbations. Defense strategies for diffusion models, in particular, remain
under-explored. In this work, we propose Spatial Attention Unlearning (SAU), a
novel technique for mitigating backdoor attacks in diffusion models. SAU
leverages latent space manipulation and spatial attention mechanisms to isolate
and remove the latent representation of backdoor triggers, ensuring precise and
efficient removal of malicious effects. We evaluate SAU across various types of
backdoor attacks, including pixel-based and style-based triggers, and
demonstrate its effectiveness in achieving 100% trigger removal accuracy.
Furthermore, SAU achieves a CLIP score of 0.7023, outperforming existing
methods while preserving the model's ability to generate high-quality,
semantically aligned images. Our results show that SAU is a robust, scalable,
and practical solution for securing text-to-image diffusion models against
backdoor attacks.

</details>

### [2] [DualBreach: Efficient Dual-Jailbreaking via Target-Driven Initialization and Multi-Target Optimization](https://arxiv.org/abs/2504.18564)
*Xinzhe Huang,Kedong Xiu,Tianhang Zheng,Churui Zeng,Wangze Ni,Zhan Qiin,Kui Ren,Chun Chen*

Main category: cs.CR

TLDR: DualBreach是一个针对大型语言模型（LLMs）和防护栏的双重越狱框架，通过目标驱动初始化和多目标优化方法，显著提高了越狱成功率并减少了查询次数。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法在绕过带有防护栏的安全对齐LLMs时效果有限，因此需要一种能同时攻击LLMs和防护栏的双重越狱方法。

Method: DualBreach采用目标驱动初始化（TDI）动态构建初始提示，结合多目标优化（MTO）方法，利用近似梯度联合优化提示，以减少查询次数并提高成功率。对于黑盒防护栏，使用开源防护栏或训练代理模型模拟。

Result: DualBreach在多个数据集上表现优异，平均双重越狱成功率达93.67%，仅需1.77次查询/成功越狱，优于现有方法。防御方面提出的EGuard机制性能优于Llama-Guard-3。

Conclusion: DualBreach是一种高效的双重越狱框架，显著提升了攻击效果，同时提出的EGuard防御机制为防护栏提供了新的解决方案。

Abstract: Recent research has focused on exploring the vulnerabilities of Large
Language Models (LLMs), aiming to elicit harmful and/or sensitive content from
LLMs. However, due to the insufficient research on dual-jailbreaking -- attacks
targeting both LLMs and Guardrails, the effectiveness of existing attacks is
limited when attempting to bypass safety-aligned LLMs shielded by guardrails.
Therefore, in this paper, we propose DualBreach, a target-driven framework for
dual-jailbreaking. DualBreach employs a Target-driven Initialization (TDI)
strategy to dynamically construct initial prompts, combined with a Multi-Target
Optimization (MTO) method that utilizes approximate gradients to jointly adapt
the prompts across guardrails and LLMs, which can simultaneously save the
number of queries and achieve a high dual-jailbreaking success rate. For
black-box guardrails, DualBreach either employs a powerful open-sourced
guardrail or imitates the target black-box guardrail by training a proxy model,
to incorporate guardrails into the MTO process.
  We demonstrate the effectiveness of DualBreach in dual-jailbreaking scenarios
through extensive evaluation on several widely-used datasets. Experimental
results indicate that DualBreach outperforms state-of-the-art methods with
fewer queries, achieving significantly higher success rates across all
settings. More specifically, DualBreach achieves an average dual-jailbreaking
success rate of 93.67% against GPT-4 with Llama-Guard-3 protection, whereas the
best success rate achieved by other methods is 88.33%. Moreover, DualBreach
only uses an average of 1.77 queries per successful dual-jailbreak,
outperforming other state-of-the-art methods. For the purpose of defense, we
propose an XGBoost-based ensemble defensive mechanism named EGuard, which
integrates the strengths of multiple guardrails, demonstrating superior
performance compared with Llama-Guard-3.

</details>

### [3] [RepliBench: Evaluating the autonomous replication capabilities of language model agents](https://arxiv.org/abs/2504.18565)
*Sid Black,Asa Cooper Stickland,Jake Pencharz,Oliver Sourbut,Michael Schmatz,Jay Bailey,Ollie Matthews,Ben Millwood,Alex Remedios,Alan Cooney*

Main category: cs.CR

TLDR: RepliBench评估套件用于衡量语言模型代理的自主复制能力，发现当前前沿模型虽未构成可信威胁，但在多个组件上表现良好且进步迅速。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型代理的自主复制能力及其潜在安全风险。

Method: 开发RepliBench评估套件，包含20个任务家族的86个任务，测试5个前沿模型在资源获取、模型权重外泄、计算复制和持久性四个核心领域的能力。

Result: 模型在简单安全设置下能部署云实例、编写自传播程序和外泄权重，但在KYC检查和持久部署上表现不佳。最佳模型Claude 3.7 Sonnet在15/20任务家族中pass@10得分>50%。

Conclusion: 自主复制能力可能在剩余领域改进或人类协助下很快出现。

Abstract: Uncontrollable autonomous replication of language model agents poses a
critical safety risk. To better understand this risk, we introduce RepliBench,
a suite of evaluations designed to measure autonomous replication capabilities.
RepliBench is derived from a decomposition of these capabilities covering four
core domains: obtaining resources, exfiltrating model weights, replicating onto
compute, and persisting on this compute for long periods. We create 20 novel
task families consisting of 86 individual tasks. We benchmark 5 frontier
models, and find they do not currently pose a credible threat of
self-replication, but succeed on many components and are improving rapidly.
Models can deploy instances from cloud compute providers, write
self-propagating programs, and exfiltrate model weights under simple security
setups, but struggle to pass KYC checks or set up robust and persistent agent
deployments. Overall the best model we evaluated (Claude 3.7 Sonnet) has a >50%
pass@10 score on 15/20 task families, and a >50% pass@10 score for 9/20
families on the hardest variants. These findings suggest autonomous replication
capability could soon emerge with improvements in these remaining areas or with
human assistance.

</details>

### [4] [Feature Selection via GANs (GANFS): Enhancing Machine Learning Models for DDoS Mitigation](https://arxiv.org/abs/2504.18566)
*Harsh Patel*

Main category: cs.CR

TLDR: 论文提出了一种基于生成对抗网络的特征选择方法（GANFS），用于高效检测DDoS攻击，并通过实验验证其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: DDoS攻击对现代网络系统构成持续威胁，传统特征选择方法在复杂攻击环境中存在局限性。

Method: 提出GANFS方法，利用对抗学习动态识别关键特征，通过扰动敏感性分析排序特征重要性。

Result: 实验表明GANFS提高了分类器准确性并显著降低特征维度，提升了计算效率。

Conclusion: 生成学习模型在网络安全中有潜力构建更自适应和可扩展的检测系统。

Abstract: Distributed Denial of Service (DDoS) attacks represent a persistent and
evolving threat to modern networked systems, capable of causing large-scale
service disruptions. The complexity of such attacks, often hidden within
high-dimensional and redundant network traffic data, necessitates robust and
intelligent feature selection techniques for effective detection. Traditional
methods such as filter-based, wrapper-based, and embedded approaches, each
offer strengths but struggle with scalability or adaptability in complex attack
environments. In this study, we explore these existing techniques through a
detailed comparative analysis and highlight their limitations when applied to
large-scale DDoS detection tasks. Building upon these insights, we introduce a
novel Generative Adversarial Network-based Feature Selection (GANFS) method
that leverages adversarial learning dynamics to identify the most informative
features. By training a GAN exclusively on attack traffic and employing a
perturbation-based sensitivity analysis on the Discriminator, GANFS effectively
ranks feature importance without relying on full supervision. Experimental
evaluations using the CIC-DDoS2019 dataset demonstrate that GANFS not only
improves the accuracy of downstream classifiers but also enhances computational
efficiency by significantly reducing feature dimensionality. These results
point to the potential of integrating generative learning models into
cybersecurity pipelines to build more adaptive and scalable detection systems.

</details>

### [5] [Large Language Model Empowered Privacy-Protected Framework for PHI Annotation in Clinical Notes](https://arxiv.org/abs/2504.18569)
*Guanchen Wu,Linzhi Zheng,Han Xie,Zhen Xiang,Jiaying Lu,Darren Liu,Delgersuren Bold,Bo Li,Xiao Hu,Carl Yang*

Main category: cs.CR

TLDR: LPPA是一个基于LLM的隐私保护框架，用于临床笔记中的PHI标注，通过本地微调LLM提高隐私保护和标注准确性。


<details>
  <summary>Details</summary>
Motivation: 医疗数据去标识化对保护患者隐私至关重要，但现有方法泛化性差且需大量标注数据。LLM虽具潜力，但存在隐私风险和计算成本问题。

Method: LPPA通过本地微调LLM（使用合成笔记）实现隐私保护和PHI标注的高精度。

Result: 实验表明LPPA能有效去标识化私人信息，提供可扩展且高效的隐私保护方案。

Conclusion: LPPA为临床笔记的去标识化提供了隐私保护强、准确性高的解决方案。

Abstract: The de-identification of private information in medical data is a crucial
process to mitigate the risk of confidentiality breaches, particularly when
patient personal details are not adequately removed before the release of
medical records. Although rule-based and learning-based methods have been
proposed, they often struggle with limited generalizability and require
substantial amounts of annotated data for effective performance. Recent
advancements in large language models (LLMs) have shown significant promise in
addressing these issues due to their superior language comprehension
capabilities. However, LLMs present challenges, including potential privacy
risks when using commercial LLM APIs and high computational costs for deploying
open-source LLMs locally. In this work, we introduce LPPA, an LLM-empowered
Privacy-Protected PHI Annotation framework for clinical notes, targeting the
English language. By fine-tuning LLMs locally with synthetic notes, LPPA
ensures strong privacy protection and high PHI annotation accuracy. Extensive
experiments demonstrate LPPA's effectiveness in accurately de-identifying
private information, offering a scalable and efficient solution for enhancing
patient privacy protection.

</details>

### [6] [Residual-Evasive Attacks on ADMM in Distributed Optimization](https://arxiv.org/abs/2504.18570)
*Sabrina Bruckmeier,Huadong Mo,James Qin*

Main category: cs.CR

TLDR: 论文提出两种攻击策略，通过避免残差显著变化来规避ADMM系统的检测，展示了其隐蔽性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有检测算法依赖残差变化识别攻击，但攻击者可通过保持残差不变实现隐蔽攻击，揭示系统漏洞。

Method: 策略一结合随机起点与Gram-Schmidt正交化确保隐蔽性；策略二在此基础上以经济利益为目标，操纵无功功率并利用系统约束。

Result: 在IEEE 14-bus系统上的案例研究表明，两种策略在隐蔽性和破坏性上优于常见攻击，揭示了ADMM系统的脆弱性。

Conclusion: 研究强调需开发更鲁棒的监测算法以应对高级攻击策略。

Abstract: This paper presents two attack strategies designed to evade detection in
ADMM-based systems by preventing significant changes to the residual during the
attacked iteration. While many detection algorithms focus on identifying false
data injection through residual changes, we show that our attacks remain
undetected by keeping the residual largely unchanged. The first strategy uses a
random starting point combined with Gram-Schmidt orthogonalization to ensure
stealth, with potential for refinement by enhancing the orthogonal component to
increase system disruption. The second strategy builds on the first, targeting
financial gains by manipulating reactive power and pushing the system to its
upper voltage limit, exploiting operational constraints. The effectiveness of
the proposed attack-resilient mechanism is demonstrated through case studies on
the IEEE 14-bus system. A comparison of the two strategies, along with commonly
used naive attacks, reveals trade-offs between simplicity, detectability, and
effectiveness, providing insights into ADMM system vulnerabilities. These
findings underscore the need for more robust monitoring algorithms to protect
against advanced attack strategies.

</details>

### [7] [Intelligent Detection of Non-Essential IoT Traffic on the Home Gateway](https://arxiv.org/abs/2504.18571)
*Fabio Palmese,Anna Maria Mandalari,Hamed Haddadi,Alessandro Enrico Cesare Redondi*

Main category: cs.CR

TLDR: ML-IoTrim是一个基于边缘计算和机器学习的系统，用于检测和阻止智能家居中非必要的物联网流量，提升隐私保护。


<details>
  <summary>Details</summary>
Motivation: 物联网设备在智能家居中的快速普及带来了安全和隐私问题，现有解决方案依赖云端威胁检测或过时的允许列表，效果不佳。

Method: 通过构建标记数据集和特征提取管道，利用机器学习对网络目的地进行二元分类（必要与非必要流量）。

Result: 在智能家居环境中测试，模型能准确识别并阻止非必要流量，包括未知目的地，且无需传统允许列表。

Conclusion: ML-IoTrim展示了在大型物联网环境中可扩展部署的潜力，为智能家居隐私保护提供了新方向。

Abstract: The rapid expansion of Internet of Things (IoT) devices, particularly in
smart home environments, has introduced considerable security and privacy
concerns due to their persistent connectivity and interaction with cloud
services. Despite advancements in IoT security, effective privacy measures
remain uncovered, with existing solutions often relying on cloud-based threat
detection that exposes sensitive data or outdated allow-lists that inadequately
restrict non-essential network traffic. This work presents ML-IoTrim, a system
for detecting and mitigating non-essential IoT traffic (i.e., not influencing
the device operations) by analyzing network behavior at the edge, leveraging
Machine Learning to classify network destinations. Our approach includes
building a labeled dataset based on IoT device behavior and employing a
feature-extraction pipeline to enable a binary classification of essential vs.
non-essential network destinations. We test our framework in a consumer smart
home setup with IoT devices from five categories, demonstrating that the model
can accurately identify and block non-essential traffic, including previously
unseen destinations, without relying on traditional allow-lists. We implement
our solution on a home access point, showing the framework has strong potential
for scalable deployment, supporting near-real-time traffic classification in
large-scale IoT environments with hundreds of devices. This research advances
privacy-aware traffic control in smart homes, paving the way for future
developments in IoT device privacy.

</details>

### [8] [WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks](https://arxiv.org/abs/2504.18575)
*Ivan Evtimov,Arman Zharmagambetov,Aaron Grattafiori,Chuan Guo,Kamalika Chaudhuri*

Main category: cs.CR

TLDR: 论文提出了一个名为WASP的新基准，用于测试Web导航AI代理对提示注入攻击的安全性，并评估了三种流行系统的基线攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究对Web代理的提示注入攻击威胁关注不足，测试场景过于简单或不真实，缺乏对实际攻击目标的模拟。

Method: 构建WASP基准，模拟真实的Web代理劫持目标，并在隔离环境中测试三种流行Web代理系统的基线攻击。

Result: 实验显示，即使使用先进推理能力的模型，代理仍易受低难度提示注入攻击，但实际攻击目标完成率较低。

Conclusion: 研究呼吁对抗性研究需在更现实的约束下展示更强的攻击效果。

Abstract: Web navigation AI agents use language-and-vision foundation models to enhance
productivity but these models are known to be susceptible to indirect prompt
injections that get them to follow instructions different from the legitimate
user's. Existing explorations of this threat applied to web agents often focus
on a single isolated adversarial goal, test with injected instructions that are
either too easy or not truly malicious, and often give the adversary
unreasonable access. In order to better focus adversarial research, we
construct a new benchmark called WASP (Web Agent Security against Prompt
injection attacks) that introduces realistic web agent hijacking objectives and
an isolated environment to test them in that does not affect real users or the
live web. As part of WASP, we also develop baseline attacks against three
popular web agentic systems (VisualWebArena, Claude Computer Use, and Operator)
instantiated with various state-of-the-art models. Our evaluation shows that
even AI agents backed by models with advanced reasoning capabilities and by
models with instruction hierarchy mitigations are susceptible to low-effort
human-written prompt injections. However, the realistic objectives in WASP also
allow us to observe that agents are currently not capable enough to complete
the goals of attackers end-to-end. Agents begin executing the adversarial
instruction between 16 and 86% of the time but only achieve the goal between 0
and 17% of the time. Based on these findings, we argue that adversarial
researchers should demonstrate stronger attacks that more consistently maintain
control over the agent given realistic constraints on the adversary's power.

</details>

### [9] [Defending Against Intelligent Attackers at Large Scales](https://arxiv.org/abs/2504.18577)
*Andrew J. Lohn*

Main category: cs.CR

TLDR: 论文研究了AI对网络安全中攻击与防御规模的影响，发现独立智能攻击的规模效应可以通过小幅增加防御数量或质量来抵消。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在网络安全中可能带来的攻击与防御规模变化，特别是独立智能攻击的影响。

Method: 通过数学模型分析攻击与防御的规模效应，考虑攻击者独立行动和学习能力。

Result: 小幅增加防御数量或质量可以抵消独立攻击数量或速度的指数级增长。

Conclusion: 智能攻击的规模效应可通过优化防御策略有效应对。

Abstract: We investigate the scale of attack and defense mathematically in the context
of AI's possible effect on cybersecurity. For a given target today, highly
scaled cyber attacks such as from worms or botnets typically all fail or all
succeed. Here, we consider the effect of scale if those attack agents were
intelligent and creative enough to act independently such that each attack
attempt was different from the others or such that attackers could learn from
their successes and failures. We find that small increases in the number or
quality of defenses can compensate for exponential increases in the number of
independent attacks and for exponential speedups.

</details>

### [10] [Enhancing Privacy in Semantic Communication over Wiretap Channels leveraging Differential Privacy](https://arxiv.org/abs/2504.18581)
*Weixuan Chen,Shunpu Tang,Qianqian Yang,Zhiguo Shi,Dusit Niyato*

Main category: cs.CR

TLDR: 本文提出了一种结合差分隐私（DP）和生成对抗网络（GAN）反转技术的语义通信框架，以保护敏感语义特征并解决DP不可逆性问题。


<details>
  <summary>Details</summary>
Motivation: 语义通信（SemCom）通过传输任务相关信息提高效率，但在不安全信道中传输语义丰富的数据会带来隐私风险。

Method: 使用GAN反转技术提取解耦的语义特征，并通过神经网络（NNs）近似DP应用和移除过程，同时引入基于NN的加密方案增强信道输入安全性。

Result: 仿真结果表明，该方法能有效防止窃听者重建敏感信息，同时为合法用户提供高质量的图像重建，并在不同隐私预算和信道条件下表现稳健。

Conclusion: 该框架在隐私保护和重建保真度之间实现了最佳平衡。

Abstract: Semantic communication (SemCom) improves transmission efficiency by focusing
on task-relevant information. However, transmitting semantic-rich data over
insecure channels introduces privacy risks. This paper proposes a novel SemCom
framework that integrates differential privacy (DP) mechanisms to protect
sensitive semantic features. This method employs the generative adversarial
network (GAN) inversion technique to extract disentangled semantic features and
uses neural networks (NNs) to approximate the DP application and removal
processes, effectively mitigating the non-invertibility issue of DP.
Additionally, an NN-based encryption scheme is introduced to strengthen the
security of channel inputs. Simulation results demonstrate that the proposed
approach effectively prevents eavesdroppers from reconstructing sensitive
information by generating chaotic or fake images, while ensuring high-quality
image reconstruction for legitimate users. The system exhibits robust
performance across various privacy budgets and channel conditions, achieving an
optimal balance between privacy protection and reconstruction fidelity.

</details>

### [11] [Optimizing the Privacy-Utility Balance using Synthetic Data and Configurable Perturbation Pipelines](https://arxiv.org/abs/2504.18596)
*Anantha Sharma,Swetha Devabhaktuni,Eklove Mohan*

Main category: cs.CR

TLDR: 论文探讨了现代合成数据生成和高级数据扰动技术在BFSI等行业中的应用，以提升安全性、数据效用和效率，对比传统匿名化方法。


<details>
  <summary>Details</summary>
Motivation: 解决数据敏感行业（如BFSI、医疗、零售等）中隐私保护与数据效用平衡的挑战。

Method: 使用生成模型（如GANs）、上下文感知PII转换、可配置统计扰动和差分隐私等先进技术。

Result: 现代技术显著提升了隐私保护与数据效用的平衡，并可能降低运营成本、加速分析。

Conclusion: 这些方法在降低监管风险、支持数据驱动创新的同时，保护了敏感信息。

Abstract: This paper explores the strategic use of modern synthetic data generation and
advanced data perturbation techniques to enhance security, maintain analytical
utility, and improve operational efficiency when managing large datasets, with
a particular focus on the Banking, Financial Services, and Insurance (BFSI)
sector. We contrast these advanced methods encompassing generative models like
GANs, sophisticated context-aware PII transformation, configurable statistical
perturbation, and differential privacy with traditional anonymization
approaches.
  The goal is to create realistic, privacy-preserving datasets that retain high
utility for complex machine learning tasks and analytics, a critical need in
the data-sensitive industries like BFSI, Healthcare, Retail, and
Telecommunications. We discuss how these modern techniques potentially offer
significant improvements in balancing privacy preservation while maintaining
data utility compared to older methods. Furthermore, we examine the potential
for operational gains, such as reduced overhead and accelerated analytics, by
using these privacy-enhanced datasets. We also explore key use cases where
these methods can mitigate regulatory risks and enable scalable, data-driven
innovation without compromising sensitive customer information.

</details>

### [12] [Accurate BGV Parameters Selection: Accounting for Secret and Public Key Dependencies in Average-Case Analysis](https://arxiv.org/abs/2504.18597)
*Beatrice Biasioli,Chiara Marcolla,Nadir Murru,Matilda Urani*

Main category: cs.CR

TLDR: 本文提出了一种新的噪声增长估计方法，用于BGV全同态加密方案，以优化参数选择并提高效率。


<details>
  <summary>Details</summary>
Motivation: BGV方案中噪声增长对参数选择和效率至关重要，但现有方法估计不准确。

Method: 提出了一种新的噪声增长估计方法，与实验结果更吻合。

Result: 新方法能更准确地估计噪声增长，为参数选择提供依据。

Conclusion: 该方法确保了BGV方案的正确性并提高了效率。

Abstract: The Brakerski-Gentry-Vaikuntanathan (BGV) scheme is one of the most
significant fully homomorphic encryption (FHE) schemes. It belongs to a class
of FHE schemes whose security is based on the presumed intractability of the
Learning with Errors (LWE) problem and its ring variant (RLWE). Such schemes
deal with a quantity, called noise, which increases each time a homomorphic
operation is performed. Specifically, in order for the scheme to work properly,
it is essential that the noise remains below a certain threshold throughout the
process. For BGV, this threshold strictly depends on the ciphertext modulus,
which is one of the initial parameters whose selection heavily affects both the
efficiency and security of the scheme. In this paper, we provide a new method
to estimate noise growth, closely aligning with experimental results and
forming the basis for parameter selection that ensures correctness and improves
efficiency.

</details>

### [13] [BadMoE: Backdooring Mixture-of-Experts LLMs via Optimizing Routing Triggers and Infecting Dormant Experts](https://arxiv.org/abs/2504.18598)
*Qingyue Wang,Qi Pang,Xixun Lin,Shuai Wang,Daoyuan Wu*

Main category: cs.CR

TLDR: 论文提出了一种针对MoE架构LLM的后门攻击方法BadMoE，通过毒化休眠专家并优化路由触发器来控制模型输出。


<details>
  <summary>Details</summary>
Motivation: 研究MoE架构LLM的潜在漏洞，尤其是后门攻击的可能性，填补了这一领域的研究空白。

Method: 1) 识别与目标任务无关的休眠专家；2) 构建路由感知损失优化激活触发器；3) 通过毒化训练数据提升休眠专家至主导地位。

Result: 证明了MoE模型中存在少数主导专家，休眠专家可被操纵为支配模型预测的主导专家。

Conclusion: BadMoE攻击成功利用了MoE架构的独特性，揭示了其潜在安全风险。

Abstract: Mixture-of-Experts (MoE) have emerged as a powerful architecture for
  large language models (LLMs), enabling efficient scaling of model capacity
  while maintaining manageable computational costs. The key advantage lies in
  their ability to route different tokens to different ``expert'' networks
  within the model, enabling specialization and efficient handling of diverse
  input. However, the vulnerabilities of MoE-based LLMs still have barely been
  studied, and the potential for backdoor attacks in this context remains
  largely unexplored. This paper presents the first backdoor attack against
  MoE-based LLMs where the attackers poison ``dormant experts'' (i.e.,
underutilized
  experts) and activate them by optimizing routing triggers, thereby gaining
  control over the model's output. We first rigorously prove the existence of a
few ``dominating
  experts'' in MoE models, whose outputs can determine the overall MoE's
  output. We also show that dormant experts can serve as dominating experts to
manipulate model predictions.
  Accordingly, our attack, namely \textsc{BadMoE}, exploits the unique
  architecture of MoE models by 1) identifying dormant experts unrelated to the
target task, 2)
  constructing a routing-aware loss to optimize the activation triggers of
these experts, and 3) promoting dormant experts to dominating roles via
poisoned training data.

</details>

### [14] [ECG Identity Authentication in Open-set with Multi-model Pretraining and Self-constraint Center & Irrelevant Sample Repulsion Learning](https://arxiv.org/abs/2504.18608)
*Mingyu Dong,Zhidong Zhao,Hao Wang,Yefei Zhang,Yanjun Deng*

Main category: cs.CR

TLDR: 提出了一种基于心电信号（ECG）的鲁棒身份认证系统，解决了开放集场景下的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注闭集场景，而实际应用中存在大量未见数据，导致安全漏洞和性能下降。

Method: 采用多模态预训练框架，结合自约束中心学习和无关样本排斥学习，优化特征分布。

Result: 认证准确率达99.83%，开放集样本下误接受率低至5.39%，开放集分类率稳定在95%以上。

Conclusion: 该方法在开放集场景下表现出色，具有高准确性和稳定性。

Abstract: Electrocardiogram (ECG) signal exhibits inherent uniqueness, making it a
promising biometric modality for identity authentication. As a result, ECG
authentication has gained increasing attention in recent years. However, most
existing methods focus primarily on improving authentication accuracy within
closed-set settings, with limited research addressing the challenges posed by
open-set scenarios. In real-world applications, identity authentication systems
often encounter a substantial amount of unseen data, leading to potential
security vulnerabilities and performance degradation. To address this issue, we
propose a robust ECG identity authentication system that maintains high
performance even in open-set settings. Firstly, we employ a multi-modal
pretraining framework, where ECG signals are paired with textual reports
derived from their corresponding fiducial features to enhance the
representational capacity of the signal encoder. During fine-tuning, we
introduce Self-constraint Center Learning and Irrelevant Sample Repulsion
Learning to constrain the feature distribution, ensuring that the encoded
representations exhibit clear decision boundaries for classification. Our
method achieves 99.83% authentication accuracy and maintains a False Accept
Rate as low as 5.39% in the presence of open-set samples. Furthermore, across
various open-set ratios, our method demonstrates exceptional stability,
maintaining an Open-set Classification Rate above 95%.

</details>

### [15] [A Gradient-Optimized TSK Fuzzy Framework for Explainable Phishing Detection](https://arxiv.org/abs/2504.18636)
*Lohith Srikanth Pentapalli,Jon Salisbury,Josette Riep,Kelly Cohen*

Main category: cs.CR

TLDR: 提出了一种基于模糊逻辑和梯度优化的新型钓鱼URL检测系统，兼具高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有钓鱼检测方法难以同时实现高准确性和可解释性，无法应对新型攻击或模型透明度不足。

Method: 采用一阶TSK模糊推理模型，结合Adam优化器进行参数调优，实现模糊逻辑与梯度优化的智能结合。

Result: 在23.5万URL数据集上测试，准确率达99.95%，AUC为1.00，且模型决策透明。

Conclusion: 该框架显著提升了网络安全防御能力，提供了高准确性和可解释性的工具。

Abstract: Phishing attacks represent an increasingly sophisticated and pervasive threat
to individuals and organizations, causing significant financial losses,
identity theft, and severe damage to institutional reputations. Existing
phishing detection methods often struggle to simultaneously achieve high
accuracy and explainability, either failing to detect novel attacks or
operating as opaque black-box models. To address this critical gap, we propose
a novel phishing URL detection system based on a first-order Takagi-Sugeno-Kang
(TSK) fuzzy inference model optimized through gradient-based techniques. Our
approach intelligently combines the interpretability and human-like reasoning
capabilities of fuzzy logic with the precision and adaptability provided by
gradient optimization methods, specifically leveraging the Adam optimizer for
efficient parameter tuning. Experiments conducted using a comprehensive dataset
of over 235,000 URLs demonstrate rapid convergence, exceptional predictive
performance (accuracy averaging 99.95% across 5 cross-validation folds, with a
perfect AUC i.e. 1.00). Furthermore, optimized fuzzy rules and membership
functions improve interoperability, clearly indicating how the model makes
decisions - an essential feature for cybersecurity applications. This
high-performance, transparent, and interpretable phishing detection framework
significantly advances current cybersecurity defenses, providing practitioners
with accurate and explainable decision-making tools.

</details>

### [16] [SynFuzz: Leveraging Fuzzing of Netlist to Detect Synthesis Bugs](https://arxiv.org/abs/2504.18812)
*Raghul Saravanan,Sudipta Paria,Aritra Dasgupta,Venkat Nitin Patnala,Swarup Bhunia,Sai Manoj P D*

Main category: cs.CR

TLDR: SynFuzz是一种新型硬件模糊测试工具，专注于门级网表以检测合成漏洞，克服了现有硬件模糊测试框架的局限性。


<details>
  <summary>Details</summary>
Motivation: 现代处理器和IP核的设计复杂性增加，导致设计正确性和安全性面临新挑战，现有硬件模糊测试方法无法检测合成和门级转换引入的漏洞。

Method: SynFuzz通过门级覆盖度量和差分模糊测试，检测合成漏洞和EDA库相关漏洞，并评估了其在开源处理器和IP设计上的效果。

Result: SynFuzz成功识别了7个新的合成漏洞，并通过CLiMA攻击展示了其对抗恶意库的能力。

Conclusion: SynFuzz提供了一种比传统验证工具更全面和鲁棒的硬件验证方法。

Abstract: In the evolving landscape of integrated circuit (IC) design, the increasing
complexity of modern processors and intellectual property (IP) cores has
introduced new challenges in ensuring design correctness and security. The
recent advancements in hardware fuzzing techniques have shown their efficacy in
detecting hardware bugs and vulnerabilities at the RTL abstraction level of
hardware. However, they suffer from several limitations, including an inability
to address vulnerabilities introduced during synthesis and gate-level
transformations. These methods often fail to detect issues arising from library
adversaries, where compromised or malicious library components can introduce
backdoors or unintended behaviors into the design. In this paper, we present a
novel hardware fuzzer, SynFuzz, designed to overcome the limitations of
existing hardware fuzzing frameworks. SynFuzz focuses on fuzzing hardware at
the gate-level netlist to identify synthesis bugs and vulnerabilities that
arise during the transition from RTL to the gate-level. We analyze the
intrinsic hardware behaviors using coverage metrics specifically tailored for
the gate-level. Furthermore, SynFuzz implements differential fuzzing to uncover
bugs associated with EDA libraries. We evaluated SynFuzz on popular open-source
processors and IP designs, successfully identifying 7 new synthesis bugs.
Additionally, by exploiting the optimization settings of EDA tools, we
performed a compromised library mapping attack (CLiMA), creating a malicious
version of hardware designs that remains undetectable by traditional
verification methods. We also demonstrate how SynFuzz overcomes the limitations
of the industry-standard formal verification tool, Cadence Conformal, providing
a more robust and comprehensive approach to hardware verification.

</details>

### [17] [Zero-Day Botnet Attack Detection in IoV: A Modular Approach Using Isolation Forests and Particle Swarm Optimization](https://arxiv.org/abs/2504.18814)
*Abdelaziz Amara korba,Nour Elislem Karabadji,Yacine Ghamri-Doudane*

Main category: cs.CR

TLDR: 提出了一种基于边缘的入侵检测系统（IDS），用于检测车联网（IoV）中的已知和未知攻击，通过元集成分类器和粒子群优化（PSO）实现高检测率。


<details>
  <summary>Details</summary>
Motivation: 车联网（IoV）的互联性增加了安全风险，尤其是针对联网和自动驾驶车辆（CAVs）的僵尸网络攻击。

Method: 使用多个隔离森林（IF）模型在边缘服务器上训练，并通过PSO堆叠策略构建元分类器。

Result: 在车辆僵尸网络数据集上，对已知攻击的检测率为92.80%，对未知攻击为77.32%。

Conclusion: 该IDS系统能有效检测已知和新兴威胁，为车联网提供可扩展的防御机制。

Abstract: The Internet of Vehicles (IoV) is transforming transportation by enhancing
connectivity and enabling autonomous driving. However, this increased
interconnectivity introduces new security vulnerabilities. Bot malware and
cyberattacks pose significant risks to Connected and Autonomous Vehicles
(CAVs), as demonstrated by real-world incidents involving remote vehicle system
compromise. To address these challenges, we propose an edge-based Intrusion
Detection System (IDS) that monitors network traffic to and from CAVs. Our
detection model is based on a meta-ensemble classifier capable of recognizing
known (Nday) attacks and detecting previously unseen (zero-day) attacks. The
approach involves training multiple Isolation Forest (IF) models on
Multi-access Edge Computing (MEC) servers, with each IF specialized in
identifying a specific type of botnet attack. These IFs, either trained locally
or shared by other MEC nodes, are then aggregated using a Particle Swarm
Optimization (PSO) based stacking strategy to construct a robust
meta-classifier. The proposed IDS has been evaluated on a vehicular botnet
dataset, achieving an average detection rate of 92.80% for N-day attacks and
77.32% for zero-day attacks. These results highlight the effectiveness of our
solution in detecting both known and emerging threats, providing a scalable and
adaptive defense mechanism for CAVs within the IoV ecosystem.

</details>

### [18] [Redefining Hybrid Blockchains: A Balanced Architecture](https://arxiv.org/abs/2504.18966)
*Syed Ibrahim Omer*

Main category: cs.CR

TLDR: 本文提出了一种新型混合区块链架构，通过半中心化模型解决区块链技术的可扩展性、治理和经济可持续性问题。


<details>
  <summary>Details</summary>
Motivation: 区块链技术在去中心化金融领域具有革命性意义，但其广泛应用受到可扩展性、治理和经济可持续性问题的限制。

Method: 采用资源与节点隔离、容器化、网络与计算层分离、Kafka发布-订阅网络替代点对点网络以及基于权益的验证者选择等策略。

Result: 在Kubernetes上的模拟显示，该架构可实现每秒1000多笔交易，性能稳定，甚至在资源有限的消费级笔记本电脑上也能运行。

Conclusion: 该架构为企业和政府提供了一种可扩展、安全且经济可行的区块链解决方案。

Abstract: Blockchain technology has completely revolutionized the field of
decentralized finance with the emergence of a variety of cryptocurrencies and
digital assets. However, widespread adoption of this technology by governments
and enterprises has been limited by concerns regarding the technology's
scalability, governance, and economic sustainability. This paper aims to
introduce a novel hybrid blockchain architecture that balances scalability,
governance, and decentralization while being economically viable for all
parties involved. The new semi-centralized model leverages strategies not
prevalent in the field, such as resource and node isolation, containerization,
separation of networking and compute layers, use of a Kafka pub-sub network
instead of a peer-to-peer network, and stakes-based validator selection to
possibly mitigate a variety of issues related to scalability, security,
governance, and economic sustainability. Simulations conducted on Kubernetes
demonstrate the architecture's ability to achieve over 1000 transactions per
second, with consistent performance across scaled deployments, even on a
lightweight consumer-grade laptop with resource constraints. The findings
highlight the system's scalability, security, and economic viability, offering
a robust framework for enterprise and government adoption.

</details>

### [19] [SONNI: Secure Oblivious Neural Network Inference](https://arxiv.org/abs/2504.18974)
*Luke Sperling,Sandeep S. Kulkarni*

Main category: cs.CR

TLDR: 论文提出了一种名为Silver Platter的新型模型窃取攻击，揭示了现有隐私保护MLaaS协议中的漏洞，并提出了一种即使在多数不诚实情况下也能保护隐私的缓解方案。


<details>
  <summary>Details</summary>
Motivation: 随着MLaaS中计算外包给第三方服务器的情况增多，现有协议存在漏洞，可能导致模型参数被窃取，威胁数据隐私。

Method: 提出了一种基于结果检查协议的新方法，确保计算正确性同时保护隐私，防止服务器与客户或模型提供者合谋窃取数据。

Result: 在32k参数的小型加密模型中，隐私保护失败概率低至1.51 x 10^-28，批处理能力仅降低0.2%。

Conclusion: 新方法有效防止模型窃取和数据泄露，即使在多数不诚实情况下也能保障隐私安全。

Abstract: In the standard privacy-preserving Machine learning as-a-service (MLaaS)
model, the client encrypts data using homomorphic encryption and uploads it to
a server for computation. The result is then sent back to the client for
decryption. It has become more and more common for the computation to be
outsourced to third-party servers. In this paper we identify a weakness in this
protocol that enables a completely undetectable novel model-stealing attack
that we call the Silver Platter attack. This attack works even under multikey
encryption that prevents a simple collusion attack to steal model parameters.
We also propose a mitigation that protects privacy even in the presence of a
malicious server and malicious client or model provider (majority dishonest).
When compared to a state-of-the-art but small encrypted model with 32k
parameters, we preserve privacy with a failure chance of 1.51 x 10^-28 while
batching capability is reduced by 0.2%. Our approach uses a novel
results-checking protocol that ensures the computation was performed correctly
without violating honest clients' data privacy. Even with collusion between the
client and the server, they are unable to steal model parameters. Additionally,
the model provider cannot learn any client data if maliciously working with the
server.

</details>

### [20] [Safety Interventions against Adversarial Patches in an Open-Source Driver Assistance System](https://arxiv.org/abs/2504.18990)
*Cheng Chen,Grant Xiao,Daehyun Lee,Lishan Yang,Evgenia Smirni,Homa Alemzadeh,Xugui Zhou*

Main category: cs.CR

TLDR: 评估ADAS对感知输入攻击的弹性，强调及时干预和冲突解决的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着ADAS的复杂性增加，自动驾驶车辆更容易受到攻击和故障，需评估其弹性。

Method: 模拟多种安全机制，评估其对攻击的缓解效果。

Result: 实验表明及时干预和冲突解决对提升系统弹性和可靠性至关重要。

Conclusion: 需优化安全机制以减少冲突并增强ADAS的弹性和可靠性。

Abstract: Drivers are becoming increasingly reliant on advanced driver assistance
systems (ADAS) as autonomous driving technology becomes more popular and
developed with advanced safety features to enhance road safety. However, the
increasing complexity of the ADAS makes autonomous vehicles (AVs) more exposed
to attacks and accidental faults. In this paper, we evaluate the resilience of
a widely used ADAS against safety-critical attacks that target perception
inputs. Various safety mechanisms are simulated to assess their impact on
mitigating attacks and enhancing ADAS resilience. Experimental results
highlight the importance of timely intervention by human drivers and automated
safety mechanisms in preventing accidents in both driving and lateral
directions and the need to resolve conflicts among safety interventions to
enhance system resilience and reliability.

</details>

### [21] [Differentially Private Quasi-Concave Optimization: Bypassing the Lower Bound and Application to Geometric Problems](https://arxiv.org/abs/2504.19001)
*Kobbi Nissim,Eliad Tsfadia,Chao Yan*

Main category: cs.CR

TLDR: N/A


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the sample complexity of differentially private optimization of
quasi-concave functions. For a fixed input domain $\mathcal{X}$, Cohen et al.
(STOC 2023) proved that any generic private optimizer for low sensitive
quasi-concave functions must have sample complexity
$\Omega(2^{\log^*|\mathcal{X}|})$. We show that the lower bound can be bypassed
for a series of ``natural'' problems. We define a new class of
\emph{approximated} quasi-concave functions, and present a generic
differentially private optimizer for approximated quasi-concave functions with
sample complexity $\tilde{O}(\log^*|\mathcal{X}|)$. As applications, we use our
optimizer to privately select a center point of points in $d$ dimensions and
\emph{probably approximately correct} (PAC) learn $d$-dimensional halfspaces.
In previous works, Bun et al. (FOCS 2015) proved a lower bound of
$\Omega(\log^*|\mathcal{X}|)$ for both problems. Beimel et al. (COLT 2019) and
Kaplan et al. (NeurIPS 2020) gave an upper bound of $\tilde{O}(d^{2.5}\cdot
2^{\log^*|\mathcal{X}|})$ for the two problems, respectively. We improve the
dependency of the upper bounds on the cardinality of the domain by presenting a
new upper bound of $\tilde{O}(d^{5.5}\cdot\log^*|\mathcal{X}|)$ for both
problems. To the best of our understanding, this is the first work to reduce
the sample complexity dependency on $|\mathcal{X}|$ for these two problems from
exponential in $\log^* |\mathcal{X}|$ to $\log^* |\mathcal{X}|$.

</details>

### [22] [BinPool: A Dataset of Vulnerabilities for Binary Security Analysis](https://arxiv.org/abs/2504.19055)
*Sima Arasteh,Georgios Nikitopoulos,Wei-Cheng Wu,Nicolaas Weideman,Aaron Portnoy,Mukund Raghothaman,Christophe Hauser*

Main category: cs.CR

TLDR: 论文介绍了一个名为Binpool的新公开数据集，包含易受攻击和修补版本的Debian软件包样本，适用于安全分析工具评估。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在二进制代码漏洞发现领域存在不足，如公开不可用、缺乏语义多样性或标签错误，需要更高质量的数据集。

Method: 通过自动整理，收集了多个易受攻击和修补版本的Debian软件包样本，并编译为四种优化级别。

Result: 数据集覆盖603个CVE、89个CWE类、162个Debian软件包，包含6144个二进制文件。

Conclusion: Binpool数据集适用于漏洞发现、二进制函数相似性和抄袭检测等多种安全分析工具的评估。

Abstract: The development of machine learning techniques for discovering software
vulnerabilities relies fundamentally on the availability of appropriate
datasets. The ideal dataset consists of a large and diverse collection of
real-world vulnerabilities, paired so as to contain both vulnerable and patched
versions of each program. Naturally, collecting such datasets is a laborious
and time-consuming task. Within the specific domain of vulnerability discovery
in binary code, previous datasets are either publicly unavailable, lack
semantic diversity, involve artificially introduced vulnerabilities, or were
collected using static analyzers, thereby themselves containing incorrectly
labeled example programs.
  In this paper, we describe a new publicly available dataset which we dubbed
Binpool, containing numerous samples of
  vulnerable versions of Debian packages across the years. The dataset was
automatically curated, and contains both vulnerable and patched versions of
each program, compiled at four different optimization levels. Overall, the
dataset covers 603 distinct CVEs across 89 CWE classes, 162 Debian packages,
and contains 6144 binaries. We argue that this dataset is suitable for
evaluating a range of security analysis tools, including for vulnerability
discovery, binary function similarity, and plagiarism detection.

</details>

### [23] [Security Vulnerabilities in Quantum Cloud Systems: A Survey on Emerging Threats](https://arxiv.org/abs/2504.19064)
*Justin Coupel,Tasnuva Farheen*

Main category: cs.CR

TLDR: 本文综述了量子云计算中的安全挑战，重点关注多租户漏洞和经典-量子接口，并探讨了新兴的量子安全解决方案。


<details>
  <summary>Details</summary>
Motivation: 量子计算的广泛应用带来了安全风险，尤其是量子云服务中的漏洞可能威胁系统的机密性、完整性和可用性。

Method: 通过分析量子云系统的设计、实现及多租户环境中的威胁（如串扰攻击、量子侧信道漏洞和内部威胁），提出安全挑战的全面概述。

Result: 揭示了量子云计算中的关键安全威胁，并讨论了现有解决方案和最佳实践。

Conclusion: 本文指出了当前研究的不足，并提出了未来构建安全、弹性量子云基础设施的方向。

Abstract: Quantum computing is becoming increasingly widespread due to the potential
and capabilities to solve complex problems beyond the scope of classical
computers. As Quantum Cloud services are adopted by businesses and research
groups, they allow for greater progress and application in many fields.
However, the inherent vulnerabilities of these environments pose significant
security concerns. This survey delivers a comprehensive analysis of the
security challenges that emerged in quantum cloud systems, with a distinct
focus on multi-tenant vulnerabilities and the classical-quantum interface. Key
threats such as crosstalk attacks, quantum-specific side-channel
vulnerabilities, and insider threats are all examined, as well as their effects
on the confidentiality, integrity, and availability of quantum circuits. The
design and implementation of various quantum architectures from quantum cloud
providers are also discussed. In addition, this paper delves into emerging
quantum security solutions and best practices to mitigate these risks. This
survey offers insights into current research gaps and proposes future
directions for secure and resilient quantum cloud infrastructures.

</details>

### [24] [CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through Cryptography Challenges](https://arxiv.org/abs/2504.19093)
*Yu Li,Qizhi Pei,Mengyuan Sun,Honglin Lin,Chenlin Ming,Xin Gao,Jiang Wu,Conghui He,Lijun Wu*

Main category: cs.CR

TLDR: 论文介绍了CipherBank，一个评估大语言模型（LLM）在密码解密任务中推理能力的基准测试，揭示了当前模型在密码推理中的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在数学和编程领域表现出色，但其在需要密码学专业知识的领域的推理能力尚未充分探索。

Method: CipherBank包含2,358个精心设计的问题，涵盖5个领域和14个子领域的262个独特明文，并涉及3大类9种加密算法。

Result: 评估发现，通用聊天LLM与专注于推理的LLM之间存在显著差距，且当前推理模型在经典密码解密任务中表现不佳。

Conclusion: 研究强调了提升LLM密码推理能力的必要性，并指出了改进方向。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities,
especially the recent advancements in reasoning, such as o1 and o3, pushing the
boundaries of AI. Despite these impressive achievements in mathematics and
coding, the reasoning abilities of LLMs in domains requiring cryptographic
expertise remain underexplored. In this paper, we introduce CipherBank, a
comprehensive benchmark designed to evaluate the reasoning capabilities of LLMs
in cryptographic decryption tasks. CipherBank comprises 2,358 meticulously
crafted problems, covering 262 unique plaintexts across 5 domains and 14
subdomains, with a focus on privacy-sensitive and real-world scenarios that
necessitate encryption. From a cryptographic perspective, CipherBank
incorporates 3 major categories of encryption methods, spanning 9 distinct
algorithms, ranging from classical ciphers to custom cryptographic techniques.
We evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and
cutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results
reveal significant gaps in reasoning abilities not only between general-purpose
chat LLMs and reasoning-focused LLMs but also in the performance of current
reasoning-focused models when applied to classical cryptographic decryption
tasks, highlighting the challenges these models face in understanding and
manipulating encrypted data. Through detailed analysis and error
investigations, we provide several key observations that shed light on the
limitations and potential improvement areas for LLMs in cryptographic
reasoning. These findings underscore the need for continuous advancements in
LLM reasoning capabilities.

</details>

### [25] [Comparative Analysis of AI-Driven Security Approaches in DevSecOps: Challenges, Solutions, and Future Directions](https://arxiv.org/abs/2504.19154)
*Farid Binbeshr,Muhammad Imam*

Main category: cs.CR

TLDR: 本文通过系统文献综述（SLR）分析和比较了DevSecOps中AI驱动的安全解决方案，评估其技术能力、实施挑战和操作影响，揭示了实证验证、可扩展性和AI集成方面的不足，并提出了优化方向。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发中，DevSecOps通过整合安全性以解决漏洞同时保持敏捷性，AI/ML被用于增强安全自动化、威胁检测和合规性，但现有研究缺乏对AI驱动安全方法的系统比较。

Method: 采用系统文献综述（SLR）方法，分析和比较DevSecOps中AI驱动的安全解决方案。

Result: 研究发现AI在安全自动化中的实证验证、可扩展性和集成方面存在不足，并总结了最佳实践和研究空白。

Conclusion: 研究提出了优化AI驱动安全框架的未来方向，填补了现有研究的空白。

Abstract: The integration of security within DevOps, known as DevSecOps, has gained
traction in modern software development to address security vulnerabilities
while maintaining agility. Artificial Intelligence (AI) and Machine Learning
(ML) have been increasingly leveraged to enhance security automation, threat
detection, and compliance enforcement. However, existing studies primarily
focus on individual aspects of AI-driven security in DevSecOps, lacking a
structured comparison of methodologies. This study conducts a systematic
literature review (SLR) to analyze and compare AI-driven security solutions in
DevSecOps, evaluating their technical capabilities, implementation challenges,
and operational impacts. The findings reveal gaps in empirical validation,
scalability, and integration of AI in security automation. The study highlights
best practices, identifies research gaps, and proposes future directions for
optimizing AI-based security frameworks in DevSecOps.

</details>

### [26] [Evaluating Organization Security: User Stories of European Union NIS2 Directive](https://arxiv.org/abs/2504.19222)
*Mari Seeba,Magnus Valgre,Raimundas Matulevičius*

Main category: cs.CR

TLDR: 论文探讨了如何利用NIS2用户故事评估组织信息安全水平，通过需求提取原则提取法律要求，定义关键角色及其目标，并验证用户故事的实用性。


<details>
  <summary>Details</summary>
Motivation: NIS2指令要求欧盟成员国确保高水平的网络安全，需评估是否达到要求的安全级别，因此需要理解不同角色的需求和目标。

Method: 通过需求提取原则从NIS2中提取法律要求，定义六个关键角色及其目标，基于信息制定用户故事，并验证其与安全评估工具的实用性。

Result: 定义的用户故事有助于调整现有安全评估工具和方法以符合NIS2，同时为新方法的开发提供模式。

Conclusion: 用户故事不仅优化了评估工具的合规性，还减少了实体的行政负担。

Abstract: The NIS2 directive requires EU Member States to ensure a consistently high
level of cybersecurity by setting risk-management measures for essential and
important entities. Evaluations are necessary to assess whether the required
security level is met. This involves understanding the needs and goals of
different personas defined by NIS2, who benefit from evaluation results. In
this paper, we consider how NIS2 user stories support the evaluation of the
level of information security in organizations. Using requirements elicitation
principles, we extracted the legal requirements from NIS2 from our narrowed
scope, identified six key personas and their goals, formulated user stories
based on the gathered information, and validated the usability and relevance of
the user stories with security evaluation instruments or methods we found from
the literature. The defined user stories help to adjust existing instruments
and methods of assessing the security level to comply with NIS2. On the other
hand, user stories enable us to see the patterns related to security evaluation
when developing new NIS2-compliant security evaluation methods to optimize the
administrative burden of entities.

</details>

### [27] [Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for Agentic Multi-Modal Large Reasoning Model](https://arxiv.org/abs/2504.19373)
*Weidi Luo,Qiming Zhang,Tianyu Lu,Xiaogeng Liu,Yue Zhao,Zhen Xiang,Chaowei Xiao*

Main category: cs.CR

TLDR: ChatGPT o3的视觉推理能力可能导致隐私泄露，研究发现其能高精度预测用户位置，需隐私保护措施。


<details>
  <summary>Details</summary>
Motivation: 研究ChatGPT o3等模型的视觉推理能力对隐私的潜在威胁，尤其是图像地理位置泄露问题。

Method: 手动构建包含50张真实图像的隐私敏感数据集，通过实验评估模型的定位能力，并进行遮挡实验。

Result: ChatGPT o3在60%的案例中能实现街道级精确定位，关键视觉线索如街道布局和庭院设计影响推理成功。

Conclusion: 需在开发多模态大模型时加强隐私保护，特别是在涉及私人图像的应用中。

Abstract: The increasing capabilities of agentic multi-modal large reasoning models,
such as ChatGPT o3, have raised critical concerns regarding privacy leakage
through inadvertent image geolocation. In this paper, we conduct the first
systematic and controlled study on the potential privacy risks associated with
visual reasoning abilities of ChatGPT o3. We manually collect and construct a
dataset comprising 50 real-world images that feature individuals alongside
privacy-relevant environmental elements, capturing realistic and sensitive
scenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can
predict user locations with high precision, achieving street-level accuracy
(within one mile) in 60% of cases. Through analysis, we identify key visual
cues, including street layout and front yard design, that significantly
contribute to the model inference success. Additionally, targeted occlusion
experiments demonstrate that masking critical features effectively mitigates
geolocation accuracy, providing insights into potential defense mechanisms. Our
findings highlight an urgent need for privacy-aware development for agentic
multi-modal large reasoning models, particularly in applications involving
private imagery.

</details>

### [28] [ChipletQuake: On-die Digital Impedance Sensing for Chiplet and Interposer Verification](https://arxiv.org/abs/2504.19418)
*Saleh Khalaj Monfared,Maryam Saadat Safa,Shahin Tajik*

Main category: cs.CR

TLDR: 论文提出了一种名为ChipletQuake的框架，用于检测芯片间安全威胁，如硬件木马和篡改行为，通过监测电源网络的阻抗变化实现。


<details>
  <summary>Details</summary>
Motivation: 随着芯片设计转向模块化的小芯片架构，新的安全挑战如硬件木马和篡改行为出现，需要一种无需额外硬件的检测方法。

Method: 利用电源传输网络（PDN）的阻抗变化检测邻近小芯片的篡改行为，无需直接信号接口或额外硬件。

Result: 该框架能有效识别被动和隐蔽的恶意电路，成功检测硬件木马和中介层篡改。

Conclusion: ChipletQuake为小芯片架构提供了一种高效的安全解决方案，增强了系统的物理安全性和完整性。

Abstract: The increasing complexity and cost of manufacturing monolithic chips have
driven the semiconductor industry toward chiplet-based designs, where smaller
and modular chiplets are integrated onto a single interposer. While chiplet
architectures offer significant advantages, such as improved yields, design
flexibility, and cost efficiency, they introduce new security challenges in the
horizontal hardware manufacturing supply chain. These challenges include risks
of hardware Trojans, cross-die side-channel and fault injection attacks,
probing of chiplet interfaces, and intellectual property theft. To address
these concerns, this paper presents \textit{ChipletQuake}, a novel on-chiplet
framework for verifying the physical security and integrity of adjacent
chiplets during the post-silicon stage. By sensing the impedance of the power
delivery network (PDN) of the system, \textit{ChipletQuake} detects tamper
events in the interposer and neighboring chiplets without requiring any direct
signal interface or additional hardware components. Fully compatible with the
digital resources of FPGA-based chiplets, this framework demonstrates the
ability to identify the insertion of passive and subtle malicious circuits,
providing an effective solution to enhance the security of chiplet-based
systems. To validate our claims, we showcase how our framework detects Hardware
Trojan and interposer tampering.

</details>

### [29] [GTSD: Generative Text Steganography Based on Diffusion Model](https://arxiv.org/abs/2504.19433)
*Zhengxian Wu,Juan Wen,Yiming Xue,Ziwei Zhang,Yinghan Zhou*

Main category: cs.CR

TLDR: 提出了一种基于扩散模型的生成式文本隐写方法（GTSD），解决了现有自回归模型隐写方法的生成速度慢、隐蔽性差和鲁棒性低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自回归隐写方法存在生成速度慢、隐蔽性差和鲁棒性低的局限性。

Method: 通过提示映射和批量映射嵌入秘密信息，利用预训练扩散模型生成候选句子批次并选择隐写文本。

Result: 实验表明GTSD在生成速度、鲁棒性和隐蔽性上优于现有方法，且嵌入容量与提示容量和模型批量大小正相关。

Conclusion: GTSD在保持安全性的同时显著提升了生成式文本隐写的性能。

Abstract: With the rapid development of deep learning, existing generative text
steganography methods based on autoregressive models have achieved success.
However, these autoregressive steganography approaches have certain
limitations. Firstly, existing methods require encoding candidate words
according to their output probability and generating each stego word one by
one, which makes the generation process time-consuming. Secondly, encoding and
selecting candidate words changes the sampling probabilities, resulting in poor
imperceptibility of the stego text. Thirdly, existing methods have low
robustness and cannot resist replacement attacks. To address these issues, we
propose a generative text steganography method based on a diffusion model
(GTSD), which improves generative speed, robustness, and imperceptibility while
maintaining security. To be specific, a novel steganography scheme based on
diffusion model is proposed to embed secret information through prompt mapping
and batch mapping. The prompt mapping maps secret information into a
conditional prompt to guide the pre-trained diffusion model generating batches
of candidate sentences. The batch mapping selects stego text based on secret
information from batches of candidate sentences. Extensive experiments show
that the GTSD outperforms the SOTA method in terms of generative speed,
robustness, and imperceptibility while maintaining comparable anti-steganalysis
performance. Moreover, we verify that the GTSD has strong potential: embedding
capacity is positively correlated with prompt capacity and model batch sizes
while maintaining security.

</details>

### [30] [JailbreaksOverTime: Detecting Jailbreak Attacks Under Distribution Shift](https://arxiv.org/abs/2504.19440)
*Julien Piet,Xiao Huang,Dennis Jacob,Annabella Chow,Maha Alrashed,Geng Zhao,Zhanhao Hu,Chawin Sitawarin,Basel Alomair,David Wagner*

Main category: cs.CR

TLDR: 论文研究了AI部署中的安全问题，特别是语言模型的越狱攻击，提出了持续学习和无监督监控的方法来检测和更新越狱防御系统。


<details>
  <summary>Details</summary>
Motivation: 尽管通过RLHF进行了安全训练，语言模型仍易受越狱攻击，尤其是通用越狱攻击。现有检测系统因时间分布偏移而失效，需改进防御方法。

Method: 提出双管齐下的方法：1）持续学习以快速适应新越狱攻击；2）无监督主动监控通过行为识别新型越狱攻击。

Result: 持续学习将漏检率从4%降至0.3%；无监督方法虽漏检率较高（4.1%），但能识别分布外攻击。

Conclusion: 结合持续学习和无监督监控可有效应对越狱攻击的演变，提升防御系统的鲁棒性。

Abstract: Safety and security remain critical concerns in AI deployment. Despite safety
training through reinforcement learning with human feedback (RLHF) [ 32],
language models remain vulnerable to jailbreak attacks that bypass safety
guardrails. Universal jailbreaks - prefixes that can circumvent alignment for
any payload - are particularly concerning. We show empirically that jailbreak
detection systems face distribution shift, with detectors trained at one point
in time performing poorly against newer exploits. To study this problem, we
release JailbreaksOverTime, a comprehensive dataset of timestamped real user
interactions containing both benign requests and jailbreak attempts collected
over 10 months. We propose a two-pronged method for defenders to detect new
jailbreaks and continuously update their detectors. First, we show how to use
continuous learning to detect jailbreaks and adapt rapidly to new emerging
jailbreaks. While detectors trained at a single point in time eventually fail
due to drift, we find that universal jailbreaks evolve slowly enough for
self-training to be effective. Retraining our detection model weekly using its
own labels - with no new human labels - reduces the false negative rate from 4%
to 0.3% at a false positive rate of 0.1%. Second, we introduce an unsupervised
active monitoring approach to identify novel jailbreaks. Rather than
classifying inputs directly, we recognize jailbreaks by their behavior,
specifically, their ability to trigger models to respond to known-harmful
prompts. This approach has a higher false negative rate (4.1%) than supervised
methods, but it successfully identified some out-of-distribution attacks that
were missed by the continuous learning approach.

</details>

### [31] [Provably Secure Public-Key Steganography Based on Admissible Encoding](https://arxiv.org/abs/2504.19454)
*Xin Zhang,Kejiang Chen,Na Zhao,Weiming Zhang,Nenghai Yu*

Main category: cs.CR

TLDR: 本文提出了一种基于可容许编码的椭圆曲线公钥隐写方法，解决了现有方法对曲线参数严格限制和仅能使用部分点的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于椭圆曲线的公钥隐写方法存在对曲线参数严格限制且仅能使用部分点的问题，限制了其实际应用。

Method: 通过将张量平方函数应用于已知的均匀分布编码，构造可容许编码，从而构建伪随机公钥加密函数。

Result: 理论分析和实验结果表明，该方法适用于所有类型的曲线，并能利用曲线上的所有点。

Conclusion: 提出的方法扩展了公钥隐写的适用范围，具有更高的通用性和实用性。

Abstract: The technique of hiding secret messages within seemingly harmless covertext
to evade examination by censors with rigorous security proofs is known as
provably secure steganography (PSS). PSS evolves from symmetric key
steganography to public-key steganography, functioning without the requirement
of a pre-shared key and enabling the extension to multi-party covert
communication and identity verification mechanisms. Recently, a public-key
steganography method based on elliptic curves was proposed, which uses point
compression to eliminate the algebraic structure of curve points. However, this
method has strict requirements on the curve parameters and is only available on
half of the points. To overcome these limitations, this paper proposes a more
general elliptic curve public key steganography method based on admissible
encoding. By applying the tensor square function to the known well-distributed
encoding, we construct admissible encoding, which can create the pseudo-random
public-key encryption function. The theoretical analysis and experimental
results show that the proposed provable secure public-key steganography method
can be deployed on all types of curves and utilize all points on the curve.

</details>

### [32] [FCGHunter: Towards Evaluating Robustness of Graph-Based Android Malware Detection](https://arxiv.org/abs/2504.19456)
*Shiwen Song,Xiaofei Xie,Ruitao Feng,Qi Guo,Sen Chen*

Main category: cs.CR

TLDR: FCGHunter是一个针对基于FCG的安卓恶意软件检测系统的鲁棒性测试框架，通过创新技术优化搜索空间，显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 动态和敌对环境中部署的恶意软件检测器鲁棒性不足，现有方法受限于巨大的扰动空间。

Method: FCGHunter通过识别关键FCG区域、依赖感知的交叉变异方法和多目标反馈，生成多样化FCG并优化搜索过程。

Result: 在40个场景中平均攻击成功率达87.9%，显著优于基线方法，对鲁棒模型攻击成功率达100%。

Conclusion: FCGHunter有效提升了FCG-based AMD系统的鲁棒性测试能力。

Abstract: Graph-based detection methods leveraging Function Call Graphs (FCGs) have
shown promise for Android malware detection (AMD) due to their semantic
insights. However, the deployment of malware detectors in dynamic and hostile
environments raises significant concerns about their robustness. While recent
approaches evaluate the robustness of FCG-based detectors using adversarial
attacks, their effectiveness is constrained by the vast perturbation space,
particularly across diverse models and features.
  To address these challenges, we introduce FCGHunter, a novel robustness
testing framework for FCG-based AMD systems. Specifically, FCGHunter employs
innovative techniques to enhance exploration and exploitation within this huge
search space. Initially, it identifies critical areas within the FCG related to
malware behaviors to narrow down the perturbation space. We then develop a
dependency-aware crossover and mutation method to enhance the validity and
diversity of perturbations, generating diverse FCGs. Furthermore, FCGHunter
leverages multi-objective feedback to select perturbed FCGs, significantly
improving the search process with interpretation-based feature change feedback.
  Extensive evaluations across 40 scenarios demonstrate that FCGHunter achieves
an average attack success rate of 87.9%, significantly outperforming baselines
by at least 44.7%. Notably, FCGHunter achieves a 100% success rate on robust
models (e.g., AdaBoost with MalScan), where baselines achieve only 11% or are
inapplicable.

</details>

### [33] [The Cost of Performance: Breaking ThreadX with Kernel Object Masquerading Attacks](https://arxiv.org/abs/2504.19486)
*Xinhui Shao,Zhen Ling,Yue Zhang,Huaiyu Yan,Yumeng Wei,Lan Luo,Zixia Liu,Junzhou Luo,Xinwen Fu*

Main category: cs.CR

TLDR: 论文分析了嵌入式实时操作系统（RTOS）的安全漏洞，提出了一种名为KOM的新型攻击方式，并通过自动化方法验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标准化的安全指南，嵌入式RTOS存在不同程度的安全风险，尤其是系统调用参数消毒的实现差异可能导致严重漏洞。

Method: 通过近距离分析RTOS实现，发现ThreadX的性能优化引入安全漏洞，提出KOM攻击，并利用符号执行自动化识别攻击。

Result: 实验证明KOM攻击在ThreadX平台上可行，厂商已确认漏洞并公开致谢。

Conclusion: 研究揭示了RTOS安全漏洞的严重性，并提供了自动化检测方法，为未来安全设计提供了参考。

Abstract: Microcontroller-based IoT devices often use embedded real-time operating
systems (RTOSs). Vulnerabilities in these embedded RTOSs can lead to
compromises of those IoT devices. Despite the significance of security
protections, the absence of standardized security guidelines results in various
levels of security risk across RTOS implementations. Our initial analysis
reveals that popular RTOSs such as FreeRTOS lack essential security
protections. While Zephyr OS and ThreadX are designed and implemented with
essential security protections, our closer examination uncovers significant
differences in their implementations of system call parameter sanitization. We
identify a performance optimization practice in ThreadX that introduces
security vulnerabilities, allowing for the circumvention of parameter
sanitization processes. Leveraging this insight, we introduce a novel attack
named the Kernel Object Masquerading (KOM) Attack (as the attacker needs to
manipulate one or multiple kernel objects through carefully selected system
calls to launch the attack), demonstrating how attackers can exploit these
vulnerabilities to access sensitive fields within kernel objects, potentially
leading to unauthorized data manipulation, privilege escalation, or system
compromise. We introduce an automated approach involving under-constrained
symbolic execution to identify the KOM attacks and to understand the
implications. Experimental results demonstrate the feasibility of KOM attacks
on ThreadX-powered platforms. We reported our findings to the vendors, who
recognized the vulnerabilities, with Amazon and Microsoft acknowledging our
contribution on their websites.

</details>

### [34] [Security Steerability is All You Need](https://arxiv.org/abs/2504.19521)
*Itay Hazan,Idan Habler,Ron Bitton,Itsik Mantin*

Main category: cs.CR

TLDR: 本文提出了一种以应用为中心的生成式AI安全方法，定义了安全可控性（Security Steerability）作为衡量LLM遵循系统提示中严格护栏能力的新指标，并开发了两种数据集来测量LLM的安全可控性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的广泛应用带来了新的安全威胁，但现有研究多关注通用威胁，而忽略了应用级安全。本文旨在填补这一空白。

Method: 采用应用中心的方法，定义安全可控性，并开发VeganRibs和ReverseText两种数据集来测量LLM的安全可控性。

Result: 研究表明，LLM虽无法完全抵御特定应用威胁，但能为应用提供自我保护框架。

Conclusion: 安全可控性是衡量LLM安全性的有效指标，为应用级安全防护提供了新思路。

Abstract: The adoption of Generative AI (GenAI) in various applications inevitably
comes with expanding the attack surface, combining new security threats along
with the traditional ones. Consequently, numerous research and industrial
initiatives aim to mitigate these security threats in GenAI by developing
metrics and designing defenses. However, while most of the GenAI security work
focuses on universal threats (e.g. manipulating the LLM to generate forbidden
content), there is significantly less discussion on application-level security
and how to mitigate it.
  Thus, in this work we adopt an application-centric approach to GenAI
security, and show that while LLMs cannot protect against ad-hoc application
specific threats, they can provide the framework for applications to protect
themselves against such threats. Our first contribution is defining Security
Steerability - a novel security measure for LLMs, assessing the model's
capability to adhere to strict guardrails that are defined in the system prompt
('Refrain from discussing about politics'). These guardrails, in case
effective, can stop threats in the presence of malicious users who attempt to
circumvent the application and cause harm to its providers.
  Our second contribution is a methodology to measure the security steerability
of LLMs, utilizing two newly-developed datasets: VeganRibs assesses the LLM
behavior in forcing specific guardrails that are not security per se in the
presence of malicious user that uses attack boosters (jailbreaks and
perturbations), and ReverseText takes this approach further and measures the
LLM ability to force specific treatment of the user input as plain text while
do user try to give it additional meanings...

</details>

### [35] [Metadata-private Messaging without Coordination](https://arxiv.org/abs/2504.19566)
*Peipei Jiang,Yihao Wu,Lei Xu,Wentao Dong,Peiyuan Chen,Yulong Ming,Cong Wang,Xiaohua Jia,Qian Wang*

Main category: cs.CR

TLDR: PingPong是一种新型的端到端元数据隐私消息系统，通过灵活的“通知-检索”工作流取代传统“拨号-对话”模式，提升了可用性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有元数据隐私消息系统在建立通信前需资源密集的“拨号”阶段，限制了多任务并发和灵活性。

Method: PingPong采用“通知-检索”工作流，结合元数据隐私通知子系统Ping和消息存储Pong，利用硬件安全飞地提升性能。

Result: 原型验证显示PingPong在保持元数据隐私的同时，提升了可用性和带宽利用率。

Conclusion: PingPong在元数据隐私和用户体验间取得了平衡，优于现有系统。

Abstract: For those seeking end-to-end private communication free from pervasive
metadata tracking and censorship, the Tor network has been the de-facto choice
in practice, despite its susceptibility to traffic analysis attacks. Recently,
numerous metadata-private messaging proposals have emerged with the aim to
surpass Tor in the messaging context by obscuring the relationships between any
two messaging buddies, even against global and active attackers. However, most
of these systems face an undesirable usability constraint: they require a
metadata-private "dialing" phase to establish mutual agreement and timing or
round coordination before initiating any regular chats among users. This phase
is not only resource-intensive but also inflexible, limiting users' ability to
manage multiple concurrent conversations seamlessly. For stringent privacy
requirement, the often-enforced traffic uniformity further exacerbated the
limitations of this roadblock.
  In this paper, we introduce PingPong, a new end-to-end system for
metadata-private messaging designed to overcome these limitations. Under the
same traffic uniformity requirement, PingPong replaces the rigid
"dial-before-converse" paradigm with a more flexible "notify-before-retrieval"
workflow. This workflow incorporates a metadata-private notification subsystem,
Ping, and a metadata-private message store, Pong. Both Ping and Pong leverage
hardware-assisted secure enclaves for performance and operates through a series
of customized oblivious algorithms, while meeting the uniformity requirements
for metadata protection. By allowing users to switch between conversations on
demand, PingPong achieves a level of usability akin to modern instant messaging
systems, while also offering improved performance and bandwidth utilization for
goodput. We have built a prototype of PingPong with 32 8-core servers equipped
with enclaves to validate our claims.

</details>

### [36] [GenPTW: In-Generation Image Watermarking for Provenance Tracing and Tamper Localization](https://arxiv.org/abs/2504.19567)
*Zhenliang Gan,Chunya Liu,Yichao Tang,Binghao Wang,Weiqiang Wang,Xinpeng Zhang*

Main category: cs.CR

TLDR: GenPTW是一种针对潜在扩散模型（LDMs）的图像水印框架，结合了来源追踪和篡改定位功能，通过生成阶段嵌入水印信号，提高了图像保真度、水印提取准确性和篡改定位性能。


<details>
  <summary>Details</summary>
Motivation: 随着生成图像模型的快速发展，确保内容真实性和版权归属成为关键挑战，现有水印方法在保真度、鲁棒性和篡改定位方面存在不足。

Method: GenPTW在图像生成阶段嵌入结构化水印信号，采用频率协调解码器提取水印，并引入模拟AIGC编辑的失真层增强鲁棒性。

Result: 实验表明，GenPTW在图像保真度、水印提取准确性和篡改定位性能上优于现有方法。

Conclusion: GenPTW为可信AIGC图像生成提供了高效实用的解决方案。

Abstract: The rapid development of generative image models has brought tremendous
opportunities to AI-generated content (AIGC) creation, while also introducing
critical challenges in ensuring content authenticity and copyright ownership.
Existing image watermarking methods, though partially effective, often rely on
post-processing or reference images, and struggle to balance fidelity,
robustness, and tamper localization. To address these limitations, we propose
GenPTW, an In-Generation image watermarking framework for latent diffusion
models (LDMs), which integrates Provenance Tracing and Tamper Localization into
a unified Watermark-based design. It embeds structured watermark signals during
the image generation phase, enabling unified provenance tracing and tamper
localization. For extraction, we construct a frequency-coordinated decoder to
improve robustness and localization precision in complex editing scenarios.
Additionally, a distortion layer that simulates AIGC editing is introduced to
enhance robustness. Extensive experiments demonstrate that GenPTW outperforms
existing methods in image fidelity, watermark extraction accuracy, and tamper
localization performance, offering an efficient and practical solution for
trustworthy AIGC image generation.

</details>

### [37] [From Paper Trails to Trust on Tracks: Adding Public Transparency to Railways via zk-SNARKs](https://arxiv.org/abs/2504.19640)
*Tarek Galal,Valeria Tisch,Katja Assaf,Andreas Polze*

Main category: cs.CR

TLDR: 论文分析了德国铁路基础设施修改指南，提出使用数字签名和零知识证明来提升透明度和信任，验证了系统的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 铁路系统缺乏透明机制导致公众信任降低，需要一种既能验证流程合规性又能保护隐私的解决方案。

Method: 利用数字签名和零知识证明建模铁路修改流程，验证流程正确性而不泄露敏感信息。

Result: 系统成功应用于铁路流程，验证了规则实现和可扩展性，证明零知识证明在提升透明度方面的潜力。

Conclusion: 该方法不仅适用于铁路，还可推广到其他领域，利用零知识证明增强公共透明度和信任。

Abstract: Railways provide a critical service and operate under strict regulatory
frameworks for implementing changes or upgrades. Despite their impact on the
public, these frameworks do not define means or mechanisms for transparency
towards the public, leading to reduced trust and complex tracking processes.
  We analyse the German guideline for railway-infrastructural modifications
from proposal to approval, using the guideline as a motivating example for
modelling decisions in processes using digital signatures and zero-knowledge
proofs. Therein, a verifier can verify that a process was executed correctly by
the involved parties and according to specification without learning
confidential information such as trade secrets or identities of the
participants. We validate our system by applying it to the railway process,
demonstrating how it realises various rules, and we evaluate its scalability
with increased process complexities. Our solution is not railway-specific but
also applicable to other contexts, helping leverage zero-knowledge proofs for
public transparency and trust.

</details>

### [38] [$\texttt{SAGE}$: A Generic Framework for LLM Safety Evaluation](https://arxiv.org/abs/2504.19674)
*Madhur Jindal,Hari Shrawgi,Parag Agrawal,Sandipan Dandapat*

Main category: cs.CR

TLDR: 论文提出了一种名为SAGE的自动化模块化框架，用于定制化和动态化的LLM安全评估，解决了现有评估方法在应用特定性和动态对话特性上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估难以适应快速发展的应用需求，且缺乏对动态对话特性的关注，导致潜在危害未被发现。

Method: 引入SAGE框架，利用系统感知的对抗性用户模型进行多轮对话评估，覆盖多种应用场景和危害政策。

Result: 实验发现危害随对话长度增加而上升，不同用户个性和场景下模型行为差异显著，部分模型通过过度拒绝策略减少危害但影响实用性。

Conclusion: 需采用自适应和上下文特定的测试方法，以确保LLM在现实场景中的安全部署。

Abstract: Safety evaluation of Large Language Models (LLMs) has made progress and
attracted academic interest, but it remains challenging to keep pace with the
rapid integration of LLMs across diverse applications. Different applications
expose users to various harms, necessitating application-specific safety
evaluations with tailored harms and policies. Another major gap is the lack of
focus on the dynamic and conversational nature of LLM systems. Such potential
oversights can lead to harms that go unnoticed in standard safety benchmarks.
This paper identifies the above as key requirements for robust LLM safety
evaluation and recognizing that current evaluation methodologies do not satisfy
these, we introduce the $\texttt{SAGE}$ (Safety AI Generic Evaluation)
framework. $\texttt{SAGE}$ is an automated modular framework designed for
customized and dynamic harm evaluations. It utilizes adversarial user models
that are system-aware and have unique personalities, enabling a holistic
red-teaming evaluation. We demonstrate $\texttt{SAGE}$'s effectiveness by
evaluating seven state-of-the-art LLMs across three applications and harm
policies. Our experiments with multi-turn conversational evaluations revealed a
concerning finding that harm steadily increases with conversation length.
Furthermore, we observe significant disparities in model behavior when exposed
to different user personalities and scenarios. Our findings also reveal that
some models minimize harmful outputs by employing severe refusal tactics that
can hinder their usefulness. These insights highlight the necessity of adaptive
and context-specific testing to ensure better safety alignment and safer
deployment of LLMs in real-world scenarios.

</details>

### [39] [Prompt Injection Attack to Tool Selection in LLM Agents](https://arxiv.org/abs/2504.19793)
*Jiawen Shi,Zenghui Yuan,Guiyao Tie,Pan Zhou,Neil Zhenqiang Gong,Lichao Sun*

Main category: cs.CR

TLDR: ToolHijacker是一种针对无盒场景下工具选择的新型提示注入攻击，通过优化恶意工具文档操纵LLM代理的工具选择过程，实验证明其高效性，现有防御策略不足。


<details>
  <summary>Details</summary>
Motivation: 研究工具选择在LLM代理中的重要性，并揭示现有防御策略的不足，推动新防御策略的开发。

Method: 提出ToolHijacker攻击方法，通过两阶段优化策略生成恶意工具文档，操纵工具选择过程。

Result: ToolHijacker显著优于现有手动和自动提示注入攻击，实验证明其高效性。

Conclusion: 现有防御策略不足以应对ToolHijacker，亟需开发新的防御方法。

Abstract: Tool selection is a key component of LLM agents. The process operates through
a two-step mechanism - \emph{retrieval} and \emph{selection} - to pick the most
appropriate tool from a tool library for a given task. In this work, we
introduce \textit{ToolHijacker}, a novel prompt injection attack targeting tool
selection in no-box scenarios. ToolHijacker injects a malicious tool document
into the tool library to manipulate the LLM agent's tool selection process,
compelling it to consistently choose the attacker's malicious tool for an
attacker-chosen target task. Specifically, we formulate the crafting of such
tool documents as an optimization problem and propose a two-phase optimization
strategy to solve it. Our extensive experimental evaluation shows that
ToolHijacker is highly effective, significantly outperforming existing
manual-based and automated prompt injection attacks when applied to tool
selection. Moreover, we explore various defenses, including prevention-based
defenses (StruQ and SecAlign) and detection-based defenses (known-answer
detection, perplexity detection, and perplexity windowed detection). Our
experimental results indicate that these defenses are insufficient,
highlighting the urgent need for developing new defense strategies.

</details>

### [40] [SILENT: A New Lens on Statistics in Software Timing Side Channels](https://arxiv.org/abs/2504.19821)
*Martin Dunsche,Patrick Bastian,Marcel Maehren,Nurullah Erinola,Robert Merget,Nicolai Bissantz,Holger Dette,Jörg Schwenk*

Main category: cs.CR

TLDR: 论文提出了一种新的算法，用于分析时间测量数据，具有强大的统计保证，解决了开发者对统计技术可靠性的疑虑。


<details>
  <summary>Details</summary>
Motivation: 现代CPU上的侧信道攻击（如Meltdown、Spectre、Hertzbleed）挑战了恒定时间代码的假设，开发者需要可靠的时间测量验证工具。

Method: 引入了一种非参数算法，处理未知数据依赖，可预先估计样本量，并允许定义可忽略的泄漏阈值。

Result: 算法在合成基准和实际应用中证明了其必要性、有效性和优势。

Conclusion: 该算法为开发者提供了可靠的统计分析工具，解决了时间测量验证的挑战。

Abstract: Cryptographic research takes software timing side channels seriously.
Approaches to mitigate them include constant-time coding and techniques to
enforce such practices. However, recent attacks like Meltdown [42], Spectre
[37], and Hertzbleed [70] have challenged our understanding of what it means
for code to execute in constant time on modern CPUs. To ensure that assumptions
on the underlying hardware are correct and to create a complete feedback loop,
developers should also perform \emph{timing measurements} as a final validation
step to ensure the absence of exploitable side channels. Unfortunately, as
highlighted by a recent study by Jancar et al. [30], developers often avoid
measurements due to the perceived unreliability of the statistical analysis and
its guarantees.
  In this work, we combat the view that statistical techniques only provide
weak guarantees by introducing a new algorithm for the analysis of timing
measurements with strong, formal statistical guarantees, giving developers a
reliable analysis tool. Specifically, our algorithm (1) is non-parametric,
making minimal assumptions about the underlying distribution and thus
overcoming limitations of classical tests like the t-test, (2) handles unknown
data dependencies in measurements, (3) can estimate in advance how many samples
are needed to detect a leak of a given size, and (4) allows the definition of a
negligible leak threshold $\Delta$, ensuring that acceptable non-exploitable
leaks do not trigger false positives, without compromising statistical
soundness. We demonstrate the necessity, effectiveness, and benefits of our
approach on both synthetic benchmarks and real-world applications.

</details>

### [41] [The Automation Advantage in AI Red Teaming](https://arxiv.org/abs/2504.19855)
*Rob Mulla,Will Pearce,Nick Landers,Brian Greunke,Brad Palm,Vincent Abruzzo,Ads Dawson*

Main category: cs.CR

TLDR: 论文分析了LLM的安全漏洞，发现自动化方法在成功率上显著优于手动方法，但在某些创造性推理场景中手动方法更快。


<details>
  <summary>Details</summary>
Motivation: 研究LLM安全漏洞的测试方法，比较自动化和手动技术的效果。

Method: 基于Crucible平台的214,271次攻击尝试数据，分析30个LLM挑战中的表现。

Result: 自动化方法成功率69.5%，手动方法47.6%；手动方法在创造性推理中更快。

Conclusion: 最优安全测试应结合人类创造力和程序化执行。

Abstract: This paper analyzes Large Language Model (LLM) security vulnerabilities based
on data from Crucible, encompassing 214,271 attack attempts by 1,674 users
across 30 LLM challenges. Our findings reveal automated approaches
significantly outperform manual techniques (69.5% vs 47.6% success rate),
despite only 5.2% of users employing automation. We demonstrate that automated
approaches excel in systematic exploration and pattern matching challenges,
while manual approaches retain speed advantages in certain creative reasoning
scenarios, often solving problems 5x faster when successful. Challenge
categories requiring systematic exploration are most effectively targeted
through automation, while intuitive challenges sometimes favor manual
techniques for time-to-solve metrics. These results illuminate how algorithmic
testing is transforming AI red-teaming practices, with implications for both
offensive security research and defensive measures. Our analysis suggests
optimal security testing combines human creativity for strategy development
with programmatic execution for thorough exploration.

</details>

### [42] [Securing GenAI Multi-Agent Systems Against Tool Squatting: A Zero Trust Registry-Based Approach](https://arxiv.org/abs/2504.19951)
*Vineeth Sai Narajala,Ken Huang,Idan Habler*

Main category: cs.CR

TLDR: 论文提出了一种针对生成式AI多智能体系统中工具抢占威胁的安全解决方案，设计了一个工具注册系统，通过集中管理、动态信任评分和精细访问控制来防范风险。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI多智能体系统的普及，标准化协议的需求增加，但同时也带来了工具抢占等安全挑战。本文旨在解决这些新兴的安全问题。

Method: 提出了一种安全架构，包括管理员控制的注册、集中化工具发现、精细访问策略、动态信任评分机制和即时凭证配置。

Result: 设计的工具注册系统能够有效防范工具抢占威胁，同时保持多智能体系统的灵活性和功能性。

Conclusion: 该研究填补了生成式AI生态系统的安全空白，为生产环境中的安全工具集成提供了基础。

Abstract: The rise of generative AI (GenAI) multi-agent systems (MAS) necessitates
standardized protocols enabling agents to discover and interact with external
tools. However, these protocols introduce new security challenges,
particularly; tool squatting; the deceptive registration or representation of
tools. This paper analyzes tool squatting threats within the context of
emerging interoperability standards, such as Model Context Protocol (MCP) or
seamless communication between agents protocols. It introduces a comprehensive
Tool Registry system designed to mitigate these risks. We propose a
security-focused architecture featuring admin-controlled registration,
centralized tool discovery, fine grained access policies enforced via dedicated
Agent and Tool Registry services, a dynamic trust scoring mechanism based on
tool versioning and known vulnerabilities, and just in time credential
provisioning. Based on its design principles, the proposed registry framework
aims to effectively prevent common tool squatting vectors while preserving the
flexibility and power of multi-agent systems. This work addresses a critical
security gap in the rapidly evolving GenAI ecosystem and provides a foundation
for secure tool integration in production environments.

</details>

### [43] [Securing Agentic AI: A Comprehensive Threat Model and Mitigation Framework for Generative AI Agents](https://arxiv.org/abs/2504.19956)
*Vineeth Sai Narajala,Om Narayan*

Main category: cs.CR

TLDR: 论文提出了一种针对生成式AI（GenAI）代理的全面威胁模型，识别了9种主要威胁，并提出了两个框架ATFAA和SHIELD以应对这些风险。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI代理在企业环境中的普及，其自主性、记忆能力和复杂推理能力带来了传统系统不具备的安全挑战，需要专门的威胁模型和防御策略。

Method: 研究通过分析GenAI代理的独特特性（如自主性、持久记忆、工具集成等），构建了威胁模型，并提出了ATFAA和SHIELD两个框架。

Result: 识别了9种主要威胁，分布在五个关键领域，并提出了针对性的缓解策略。

Conclusion: GenAI代理需要全新的安全视角，现有威胁模型和防御措施需调整以适应其独特架构和行为，否则可能成为企业的重大风险。

Abstract: As generative AI (GenAI) agents become more common in enterprise settings,
they introduce security challenges that differ significantly from those posed
by traditional systems. These agents are not just LLMs; they reason, remember,
and act, often with minimal human oversight. This paper introduces a
comprehensive threat model tailored specifically for GenAI agents, focusing on
how their autonomy, persistent memory access, complex reasoning, and tool
integration create novel risks. This research work identifies 9 primary threats
and organizes them across five key domains: cognitive architecture
vulnerabilities, temporal persistence threats, operational execution
vulnerabilities, trust boundary violations, and governance circumvention. These
threats are not just theoretical they bring practical challenges such as
delayed exploitability, cross-system propagation, cross system lateral
movement, and subtle goal misalignments that are hard to detect with existing
frameworks and standard approaches. To help address this, the research work
present two complementary frameworks: ATFAA - Advanced Threat Framework for
Autonomous AI Agents, which organizes agent-specific risks, and SHIELD, a
framework proposing practical mitigation strategies designed to reduce
enterprise exposure. While this work builds on existing work in LLM and AI
security, the focus is squarely on what makes agents different and why those
differences matter. Ultimately, this research argues that GenAI agents require
a new lens for security. If we fail to adapt our threat models and defenses to
account for their unique architecture and behavior, we risk turning a powerful
new tool into a serious enterprise liability.

</details>

### [44] [Simplified and Secure MCP Gateways for Enterprise AI Integration](https://arxiv.org/abs/2504.19997)
*Ivo Brett*

Main category: cs.CR

TLDR: 论文提出MCP Gateway，为企业自托管MCP服务器提供安全集成方案，解决公共MCP服务器无法满足的企业需求。


<details>
  <summary>Details</summary>
Motivation: 企业采用MCP协议时面临安全挑战，现有公共MCP服务器解决方案无法满足企业自托管需求。

Method: 提出MCP Gateway架构，整合安全原则、认证、入侵检测和安全隧道技术。

Result: 提供参考架构、威胁模型映射、简化集成策略及开源实现建议。

Conclusion: MCP Gateway为企业自托管AI集成提供了安全且高效的解决方案。

Abstract: The increased adoption of the Model Context Protocol (MCP) for AI Agents
necessitates robust security for Enterprise integrations. This paper introduces
the MCP Gateway to simplify self-hosted MCP server integration. The proposed
architecture integrates security principles, authentication, intrusion
detection, and secure tunneling, enabling secure self-hosting without exposing
infrastructure. Key contributions include a reference architecture, threat
model mapping, simplified integration strategies, and open-source
implementation recommendations. This work focuses on the unique challenges of
enterprise-centric, self-hosted AI integrations, unlike existing public MCP
server solutions.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Critical Challenges and Guidelines in Evaluating Synthetic Tabular Data: A Systematic Review](https://arxiv.org/abs/2504.18544)
*Nazia Nafis,Inaki Esnaola,Alvaro Martinez-Perez,Maria-Cruz Villa-Uriol,Venet Osmani*

Main category: cs.LG

TLDR: 本文系统综述了合成健康数据的评估挑战，提出了改进指南。


<details>
  <summary>Details</summary>
Motivation: 合成健康数据的评估缺乏共识和方法，影响其可靠性和应用。

Method: 通过筛选1766篇论文并详细分析101篇，识别关键问题。

Result: 发现评估方法不统一、指标误用、专家参与不足等问题。

Conclusion: 提出生成和评估合成数据的指南，以促进其应用和创新。

Abstract: Generating synthetic tabular data can be challenging, however evaluation of
their quality is just as challenging, if not more. This systematic review sheds
light on the critical importance of rigorous evaluation of synthetic health
data to ensure reliability, relevance, and their appropriate use. Based on
screening of 1766 papers and a detailed review of 101 papers we identified key
challenges, including lack of consensus on evaluation methods, improper use of
evaluation metrics, limited input from domain experts, inadequate reporting of
dataset characteristics, and limited reproducibility of results. In response,
we provide several guidelines on the generation and evaluation of synthetic
data, to allow the community to unlock and fully harness the transformative
potential of synthetic data and accelerate innovation.

</details>

### [46] [Low-Bit Integerization of Vision Transformers using Operand Reodering for Efficient Hardware](https://arxiv.org/abs/2504.18547)
*Ching-Yi Lin,Sahil Shah*

Main category: cs.LG

TLDR: 提出了一种基于操作重排序的整数化方法，延迟反量化至矩阵运算后，降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉Transformer计算和内存成本高，现有量化方法仍因反量化导致显著计算开销。

Method: 通过分析计算图，提出基于操作重排序的整数化过程，延迟反量化至矩阵运算后，直接处理量化输入。

Result: 实验表明，低比特推理降低了线性层和矩阵乘法的每PE功耗。

Conclusion: 该方法缩小了量化模型与高效推理之间的差距。

Abstract: Pre-trained vision transformers have achieved remarkable performance across
various visual tasks but suffer from expensive computational and memory costs.
While model quantization reduces memory usage by lowering precision, these
models still incur significant computational overhead due to the dequantization
before matrix operations. In this work, we analyze the computation graph and
propose an integerization process based on operation reordering. Specifically,
the process delays dequantization until after matrix operations. This enables
integerized matrix multiplication and linear module by directly processing the
quantized input. To validate our approach, we synthesize the self-attention
module of ViT on a systolic array-based hardware. Experimental results show
that our low-bit inference reduces per-PE power consumption for linear layer
and matrix multiplication, bridging the gap between quantized models and
efficient inference.

</details>

### [47] [RDI: An adversarial robustness evaluation metric for deep neural networks based on sample clustering features](https://arxiv.org/abs/2504.18556)
*Jialei Song,Xingquan Zuo,Feiyang Wang,Hai Huang,Tianle Zhang*

Main category: cs.LG

TLDR: 提出了一种基于样本聚类特征的对抗鲁棒性评估指标RDI，解决了现有方法依赖攻击算法、耗时或难以实现的问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络易受对抗样本影响，现有评估方法存在依赖攻击算法、耗时或难以在大模型上实现的局限性。

Method: 通过分析决策边界分隔的特征向量的类内和类间距离，提出RDI指标，独立于攻击算法且计算高效。

Result: 实验表明，RDI与攻击成功率（ASR）相关性更强，计算时间仅为PGD攻击评估方法的1/30。

Conclusion: RDI是一种高效且攻击独立的对抗鲁棒性评估方法。

Abstract: Deep neural networks (DNNs) are highly susceptible to adversarial samples,
raising concerns about their reliability in safety-critical tasks. Currently,
methods of evaluating adversarial robustness are primarily categorized into
attack-based and certified robustness evaluation approaches. The former not
only relies on specific attack algorithms but also is highly time-consuming,
while the latter due to its analytical nature, is typically difficult to
implement for large and complex models. A few studies evaluate model robustness
based on the model's decision boundary, but they suffer from low evaluation
accuracy. To address the aforementioned issues, we propose a novel adversarial
robustness evaluation metric, Robustness Difference Index (RDI), which is based
on sample clustering features. RDI draws inspiration from clustering evaluation
by analyzing the intra-class and inter-class distances of feature vectors
separated by the decision boundary to quantify model robustness. It is
attack-independent and has high computational efficiency. Experiments show
that, RDI demonstrates a stronger correlation with the gold-standard
adversarial robustness metric of attack success rate (ASR). The average
computation time of RDI is only 1/30 of the evaluation method based on the PGD
attack. Our open-source code is available at:
https://anonymous.4open.science/r/RDI-B1DA.

</details>

### [48] [Deep Learning with Pretrained 'Internal World' Layers: A Gemma 3-Based Modular Architecture for Wildfire Prediction](https://arxiv.org/abs/2504.18562)
*Ayoub Jadouli,Chaker El Amrani*

Main category: cs.LG

TLDR: 该论文提出了一种基于Gemma 3的模块化架构，通过利用其预训练Transformer层的内部知识来预测野火发生，减少了可训练参数并提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 利用大型Transformer模型的中间层作为“内部世界”，以解决野火预测中数据有限和过拟合的问题。

Method: 开发了一个定制的前馈模块，将表格数据转换为Gemma 3的隐藏维度，并冻结其预训练层，仅训练输入输出网络。

Result: 在摩洛哥野火数据集上表现优于传统方法，证明了冻结Transformer层的有效性。

Conclusion: 模块化重用预训练Transformer可以提高数据效率和可解释性，适用于野火风险管理等环境应用。

Abstract: Deep learning models, especially large Transformers, carry substantial
"memory" in their intermediate layers -- an \emph{internal world} that encodes
a wealth of relational and contextual knowledge. This work harnesses that
internal world for wildfire occurrence prediction by introducing a modular
architecture built upon Gemma 3, a state-of-the-art multimodal model. Rather
than relying on Gemma 3's original embedding and positional encoding stacks, we
develop a custom feed-forward module that transforms tabular wildfire features
into the hidden dimension required by Gemma 3's mid-layer Transformer blocks.
We freeze these Gemma 3 sub-layers -- thus preserving their pretrained
representation power -- while training only the smaller input and output
networks. This approach minimizes the number of trainable parameters and
reduces the risk of overfitting on limited wildfire data, yet retains the
benefits of Gemma 3's broad knowledge. Evaluations on a Moroccan wildfire
dataset demonstrate improved predictive accuracy and robustness compared to
standard feed-forward and convolutional baselines. Ablation studies confirm
that the frozen Transformer layers consistently contribute to better
representations, underscoring the feasibility of reusing large-model mid-layers
as a learned internal world. Our findings suggest that strategic modular reuse
of pretrained Transformers can enable more data-efficient and interpretable
solutions for critical environmental applications such as wildfire risk
management.

</details>

### [49] [Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism](https://arxiv.org/abs/2504.18574)
*Aviv Bick,Eric Xing,Albert Gu*

Main category: cs.LG

TLDR: 论文研究了Transformer和SSM模型中的上下文检索机制，发现两者均采用Gather-and-Aggregate（G&A）机制，且集中在少数关键头部。SSM的检索问题源于G&A实现方式，而非整体模型。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer和SSM模型在上下文检索中的表现差异，揭示其底层机制。

Method: 分析两种模型的G&A机制，通过实验（如禁用关键头部）验证其重要性。

Result: G&A机制集中在少数头部，SSM的检索问题源于G&A实现方式。替换SSM中的G&A头部可显著提升性能。

Conclusion: G&A机制是性能差异的关键，结合两者优势可改进模型设计。

Abstract: SSMs offer efficient processing of long sequences with fixed state sizes, but
struggle with algorithmic tasks like retrieving past context. In this work, we
examine how such in-context retrieval operates within Transformer- and
SSM-based language models. We find that both architectures develop the same
fundamental Gather-and-Aggregate (G&A) mechanism. A Gather Head first
identifies and extracts relevant information from the context, which an
Aggregate Head then integrates into a final representation. Across both model
types, G&A concentrates in just a few heads, making them critical bottlenecks
even for benchmarks that require a basic form of retrieval. For example,
disabling a single Gather or Aggregate Head of a pruned Llama-3.1-8B degrades
its ability to retrieve the correct answer letter in MMLU, reducing accuracy
from 66% to 25%. This finding suggests that in-context retrieval can obscure
the limited knowledge demands of certain tasks. Despite strong MMLU performance
with retrieval intact, the pruned model fails on other knowledge tests. Similar
G&A dependencies exist in GSM8K, BBH, and dialogue tasks. Given the
significance of G&A in performance, we show that retrieval challenges in SSMs
manifest in how they implement G&A, leading to smoother attention patterns
rather than the sharp token transitions that effective G&A relies on. Thus,
while a gap exists between Transformers and SSMs in implementing in-context
retrieval, it is confined to a few heads, not the entire model. This insight
suggests a unified explanation for performance differences between Transformers
and SSMs while also highlighting ways to combine their strengths. For example,
in pretrained hybrid models, attention components naturally take on the role of
Aggregate Heads. Similarly, in a pretrained pure SSM, replacing a single G&A
head with an attention-based variant significantly improves retrieval.

</details>

### [50] [An Artificial Intelligence-Based Framework for Predicting Emergency Department Overcrowding: Development and Evaluation Study](https://arxiv.org/abs/2504.18578)
*Orhun Vural,Bunyamin Ozaydin,Khalid Y. Aram,James Booth,Brittany F. Lindsey,Abdulaziz Ahmed*

Main category: cs.LG

TLDR: 研究开发机器学习模型预测急诊科候诊人数，支持资源规划和减少拥挤。


<details>
  <summary>Details</summary>
Motivation: 急诊科拥挤导致护理延迟和运营压力，需提前预测以优化资源分配。

Method: 使用11种机器学习算法，结合内外数据特征，预测每小时和每日候诊人数。

Result: TSiTPlus和XCMPlus分别在每小时和每日预测中表现最佳，误差较低。

Conclusion: 模型能准确预测候诊人数，有望改善急诊科资源分配和患者流动。

Abstract: Background: Emergency department (ED) overcrowding remains a major challenge,
causing delays in care and increased operational strain. Hospital management
often reacts to congestion after it occurs. Machine learning predictive
modeling offers a proactive approach by forecasting patient flow metrics, such
as waiting count, to improve resource planning and hospital efficiency.
  Objective: This study develops machine learning models to predict ED waiting
room occupancy at two time scales. The hourly model forecasts the waiting count
six hours ahead (e.g., a 1 PM prediction for 7 PM), while the daily model
estimates the average waiting count for the next 24 hours (e.g., a 5 PM
prediction for the following day's average). These tools support staffing
decisions and enable earlier interventions to reduce overcrowding.
  Methods: Data from a partner hospital's ED in the southeastern United States
were used, integrating internal metrics and external features. Eleven machine
learning algorithms, including traditional and deep learning models, were
trained and evaluated. Feature combinations were optimized, and performance was
assessed across varying patient volumes and hours.
  Results: TSiTPlus achieved the best hourly prediction (MAE: 4.19, MSE:
29.32). The mean hourly waiting count was 18.11, with a standard deviation of
9.77. Accuracy varied by hour, with MAEs ranging from 2.45 (11 PM) to 5.45 (8
PM). Extreme case analysis at one, two, and three standard deviations above the
mean showed MAEs of 6.16, 10.16, and 15.59, respectively. For daily
predictions, XCMPlus performed best (MAE: 2.00, MSE: 6.64), with a daily mean
of 18.11 and standard deviation of 4.51.
  Conclusions: These models accurately forecast ED waiting room occupancy and
support proactive resource allocation. Their implementation has the potential
to improve patient flow and reduce overcrowding in emergency care settings.

</details>

### [51] [ZipR1: Reinforcing Token Sparsity in MLLMs](https://arxiv.org/abs/2504.18579)
*Feng Chen,Yefei He,Lequan Lin,Jing Liu,Bohan Zhuang,Qi Wu*

Main category: cs.LG

TLDR: ZipR1是一种基于强化学习的后训练方法，通过优化推理效率和性能的权衡，显著减少多模态大语言模型的token比例，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 稀疏注意力机制虽能减少计算开销，但如何主动促进token稀疏性以提升推理加速效果尚未充分探索。

Method: 提出ZipR1方法，将token减少比例作为效率奖励，答案准确性作为性能奖励，通过强化学习优化效率与性能的权衡。

Result: 在13个图像和视频基准测试中，ZipR1将Qwen2/2.5-VL的token比例从80%降至25%，准确性损失极小。

Conclusion: ZipR1有效解决了计算和内存瓶颈，为多模态大语言模型的推理加速提供了可行方案。

Abstract: Sparse attention mechanisms aim to reduce computational overhead by
selectively processing a subset of salient tokens while preserving model
performance. Despite the effectiveness of such designs, how to actively
encourage token sparsity of well-posed MLLMs remains under-explored, which
fundamentally limits the achievable acceleration effect during inference. In
this paper, we propose a simple RL-based post-training method named
\textbf{ZipR1} that treats the token reduction ratio as the efficiency reward
and answer accuracy as the performance reward.
  In this way, our method can jointly alleviate the computation and memory
bottlenecks via directly optimizing the inference-consistent
efficiency-performance tradeoff. Experimental results demonstrate that ZipR1
can reduce the token ratio of Qwen2/2.5-VL from 80\% to 25\% with a minimal
accuracy reduction on 13 image and video benchmarks.

</details>

### [52] [Parameter-Efficient Checkpoint Merging via Metrics-Weighted Averaging](https://arxiv.org/abs/2504.18580)
*Shi Jie Yu,Sehyun Choi*

Main category: cs.LG

TLDR: 提出了一种基于性能指标加权的检查点合并方法（MWA），在参数高效微调（PEFT）中有效提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索在参数高效微调（PEFT）中合并检查点的方法，以减少大型语言模型的训练时间并提升性能。

Method: 提出Metrics-Weighted Averaging（MWA），根据训练损失或训练步数加权合并检查点，引入惩罚因子调整权重分布。

Result: 在数学推理、偏好对齐和通用指令调优任务中，MWA优于均匀合并，损失加权合并效果最佳，任务准确率提升5%。

Conclusion: 验证了检查点合并对PEFT的有效性，表明基于指标的加权启发式方法能以最小计算开销提升模型性能。

Abstract: Checkpoint merging is a technique for combining multiple model snapshots into
a single superior model, potentially reducing training time for large language
models. This paper explores checkpoint merging in the context of
parameter-efficient fine-tuning (PEFT), where only small adapter modules (e.g.
LoRA) are trained. We propose Metrics-Weighted Averaging (MWA), a simple yet
effective method to merge model checkpoints by weighting their parameters
according to performance metrics. In particular, we investigate weighting by
training loss and by training steps, under the intuition that lower-loss or
later-step checkpoints are more valuable. We introduce a formula with a penalty
factor to adjust weight distribution, requiring only one hyperparameter
regardless of the number of checkpoints. Experiments on three fine-tuning tasks
(mathematical reasoning, preference alignment, and general instruction tuning)
show that MWA consistently produces merged models that outperform the naive
uniform average of checkpoints. Notably, loss-weighted merging often yields the
best results, delivering up to 5% higher task accuracy than the baseline
uniform merge and even surpassing the final individual checkpoint's
performance. These findings validate checkpoint merging for PEFT and
demonstrate that a metric-driven weighting heuristic can efficiently boost
model performance with minimal computational overhead.

</details>

### [53] [PARD: Accelerating LLM Inference with Low-Cost PARallel Draft Model Adaptation](https://arxiv.org/abs/2504.18583)
*Zihao An,Huajun Bai,Ziqiong Liu,Dong Li,Emad Barsoum*

Main category: cs.LG

TLDR: PARD是一种新型的推测解码方法，通过并行预测多个未来令牌和条件丢弃令牌技术，显著提升推理效率，同时降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的自回归特性限制了推理速度，而现有的推测解码方法在草拟阶段的开销和训练成本限制了其效率和适应性。

Method: PARD通过并行草拟模型预测多个令牌，并引入条件丢弃令牌方法加速训练，其目标无关性使得单一草拟模型适用于不同模型家族。

Result: PARD将LLaMA3.1-8B的推理速度提升4.08倍，达到每秒311.5个令牌，训练效率提高3倍。

Conclusion: PARD通过并行化和条件丢弃令牌技术，显著提升了推理效率和训练效率，同时降低了适应性成本。

Abstract: The autoregressive nature of large language models (LLMs) limits inference
speed. Each forward pass generates only a single token and is often
bottlenecked by memory bandwidth. Speculative decoding alleviates this issue
using a draft-then-verify approach to accelerate token generation. However, the
overhead introduced during the draft phase and the training cost of the draft
model limit the efficiency and adaptability of speculative decoding. In this
work, we introduce PARallel Draft (PARD), a novel speculative decoding method
that enables low-cost adaptation of autoregressive draft models into parallel
draft models. PARD enhances inference efficiency by predicting multiple future
tokens in a single forward pass of the draft phase, and incorporates a
conditional drop token method to accelerate training. Its target-independence
property allows a single draft model to be applied to an entire family of
different models, minimizing the adaptation cost. Our proposed conditional drop
token method can improves draft model training efficiency by 3x. On our
optimized inference framework, PARD accelerates LLaMA3.1-8B inference by 4.08x,
achieving 311.5 tokens per second.

</details>

### [54] [Training Large Language Models to Reason via EM Policy Gradient](https://arxiv.org/abs/2504.18587)
*Tianbing Xu*

Main category: cs.LG

TLDR: 提出了一种名为EM Policy Gradient的离策略强化学习算法，通过优化推理轨迹的期望回报来增强LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法（如PPO和GRPO）依赖复杂的权重和启发式剪枝，而新方法旨在简化流程并保持性能。

Method: 将推理任务建模为EM优化问题，交替采样多样化的推理轨迹并进行奖励引导的微调。

Result: 在GSM8K和MATH（HARD）数据集上表现与GRPO相当或略优，同时具备可扩展性和简洁性。

Conclusion: 该方法不仅提升了LLM推理的鲁棒性和可解释性，还展示了认知行为（如子问题分解和回溯）。

Abstract: Recently, foundation models such as OpenAI's O1 and O3, along with DeepSeek's
R1, have demonstrated strong reasoning capacities and problem-solving skills
acquired through large-scale reinforcement learning (RL), with wide
applications in mathematics, coding, science, intelligent agents, and virtual
assistants. In this work, we introduce an off-policy reinforcement learning
algorithm, EM Policy Gradient, aimed at enhancing LLM reasoning by optimizing
expected return over reasoning trajectories. We frame the reasoning task as an
Expectation-Maximization (EM) optimization problem, alternating between
sampling diverse rationale trajectories and performing reward-guided
fine-tuning. Unlike PPO and GRPO, which rely on complex importance weights and
heuristic clipping, our method provides a simpler, more principled off-policy
policy gradient approach, eliminating these complexities while maintaining
strong performance. We evaluate the effectiveness of EM Policy Gradient on the
GSM8K and MATH (HARD) datasets, where it achieves performance comparable to or
slightly surpassing the state-of-the-art GRPO, while offering additional
advantages in scalability, simplicity, and reasoning conciseness. Moreover,
models fine-tuned with our method exhibit cognitive behaviors, such as
sub-problem decomposition, self-verification, and backtracking, highlighting
its potential to enhance both the interpretability and robustness of LLM
reasoning.

</details>

### [55] [Dynamic QoS Prediction via a Non-Negative Tensor Snowflake Factorization](https://arxiv.org/abs/2504.18588)
*YongHui Xia,Lan Wang,Hao Wu*

Main category: cs.LG

TLDR: 提出了一种非负雪花张量分解模型，用于预测未观测到的QoS数据，通过雪花核心张量增强学习能力，并采用SLF-NMUT方法进行参数学习，实验表明其预测效果更优。


<details>
  <summary>Details</summary>
Motivation: 随着用户和服务数量的增加，大量未观测的QoS数据影响了用户选择服务的能力，因此需要准确预测这些数据。

Method: 设计了雪花核心张量以增强模型学习能力，并采用SLF-NMUT方法进行参数学习。

Result: 模型能更准确地学习动态用户-服务交互模式，从而提升对缺失QoS数据的预测效果。

Conclusion: 提出的非负雪花张量分解模型在预测未观测QoS数据方面表现优异。

Abstract: Dynamic quality of service (QoS) data exhibit rich temporal patterns in
user-service interactions, which are crucial for a comprehensive understanding
of user behavior and service conditions in Web service. As the number of users
and services increases, there is a large amount of unobserved QoS data, which
significantly affects users'choice of services. To predict unobserved QoS data,
we propose a Non-negative Snowflake Factorization of tensors model. This method
designs a snowflake core tensor to enhance the model's learning capability.
Additionally, it employs a single latent factor-based, nonnegative
multiplication update on tensor (SLF-NMUT) for parameter learning. Empirical
results demonstrate that the proposed model more accurately learns dynamic
user-service interaction patterns, thereby yielding improved predictions for
missing QoS data.

</details>

### [56] [A multilevel approach to accelerate the training of Transformers](https://arxiv.org/abs/2504.18590)
*Guillaume Lauga,Maël Chaumette,Edgar Desainte-Maréville,Étienne Lasalle,Arthur Lebeurrier*

Main category: cs.LG

TLDR: 多级方法加速Transformer训练，通过ODE解释和离散化调整实现。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用多级方法加速Transformer架构的训练。

Method: 基于ODE解释，提出离散化调整方法。

Result: 实验验证了该方法比标准训练更快。

Conclusion: 多级方法有效加速Transformer训练。

Abstract: In this article, we investigate the potential of multilevel approaches to
accelerate the training of transformer architectures. Using an ordinary
differential equation (ODE) interpretation of these architectures, we propose
an appropriate way of varying the discretization of these ODE Transformers in
order to accelerate the training. We validate our approach experimentally by a
comparison with the standard training procedure.

</details>

### [57] [Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations](https://arxiv.org/abs/2504.18591)
*Giovanni Catalani,Michael Bauerheim,Frédéric Tost,Xavier Bertrand,Joseph Morlier*

Main category: cs.LG

TLDR: enf2enf是一种基于等变神经场架构的编码器-解码器方法，用于预测具有非参数化几何变化的稳态偏微分方程。


<details>
  <summary>Details</summary>
Motivation: 利用神经场的最新进展，解决偏微分方程在一般几何上的学习问题，特别是处理几何变化与物理耦合的挑战。

Method: 通过编码输入几何为潜在点云嵌入，结合全局参数并解码为连续输出场，利用局部性和平移不变性的归纳偏置。

Result: 在高保真空气动力学数据集、超弹性材料基准和多元素翼型几何上表现优于或与现有方法相当，支持实时推理和零样本超分辨率。

Conclusion: enf2enf在几何与物理耦合建模中表现出色，具有高效训练和高精度优势。

Abstract: Recent advances in Neural Fields have enabled powerful,
discretization-invariant methods for learning neural operators that approximate
solutions of Partial Differential Equations (PDEs) on general geometries.
Building on these developments, we introduce enf2enf, an encoder--decoder
methodology for predicting steady-state Partial Differential Equations with
non-parameterized geometric variability, based on recently proposed Equivariant
Neural Field architectures. In enf2enf, input geometries are encoded into
latent point cloud embeddings that inherently preserve geometric grounding and
capture local phenomena. The resulting representations are then combined with
global parameters and directly decoded into continuous output fields, thus
efficiently modeling the coupling between geometry and physics. By leveraging
the inductive biases of locality and translation invariance, our approach is
able to capture fine-scale physical features as well as complex shape
variations, thereby enhancing generalization and physical compliance. Extensive
experiments on a high-fidelity aerodynamic dataset, a hyper-elastic material
benchmark, and multi-element airfoil geometries, demonstrate that the proposed
model achieves superior or competitive performance compared to state-of-the-art
graph based, operator learning, and neural field methods. Notably, our method
supports real time inference and zero-shot super-resolution, enabling efficient
training on low-resolution meshes while maintaining high accuracy on full-scale
discretizations.

</details>

### [58] [Severity Classification of Chronic Obstructive Pulmonary Disease in Intensive Care Units: A Semi-Supervised Approach Using MIMIC-III Dataset](https://arxiv.org/abs/2504.18593)
*Akram Shojaei,Mehdi Delrobaei*

Main category: cs.LG

TLDR: 该研究提出了一种基于机器学习的COPD严重程度分类框架，利用MIMIC-III数据库和半监督学习技术，随机森林分类器表现优异，准确率达92.51%，ROC AUC为0.98。


<details>
  <summary>Details</summary>
Motivation: COPD是全球重大健康负担，ICU中精确评估其严重程度对临床管理至关重要。

Method: 采用MIMIC-III数据库，结合ICU关键参数（如血气测量和生命体征），并运用半监督学习技术优化模型性能。

Result: 随机森林分类器表现最佳，准确率为92.51%，ROC AUC为0.98。

Conclusion: 该机器学习工具为ICU中COPD严重程度评估提供了高效准确的方法，未来需进一步外部验证和临床决策支持系统整合。

Abstract: Chronic obstructive pulmonary disease (COPD) represents a significant global
health burden, where precise severity assessment is particularly critical for
effective clinical management in intensive care unit (ICU) settings. This study
introduces an innovative machine learning framework for COPD severity
classification utilizing the MIMIC-III critical care database, thereby
expanding the applications of artificial intelligence in critical care
medicine. Our research developed a robust classification model incorporating
key ICU parameters such as blood gas measurements and vital signs, while
implementing semi-supervised learning techniques to effectively utilize
unlabeled data and enhance model performance. The random forest classifier
emerged as particularly effective, demonstrating exceptional discriminative
capability with 92.51% accuracy and 0.98 ROC AUC in differentiating between
mild-to-moderate and severe COPD cases. This machine learning approach provides
clinicians with a practical, accurate, and efficient tool for rapid COPD
severity evaluation in ICU environments, with significant potential to improve
both clinical decision-making processes and patient outcomes. Future research
directions should prioritize external validation across diverse patient
populations and integration with clinical decision support systems to optimize
COPD management in critical care settings.

</details>

### [59] [A Simple DropConnect Approach to Transfer-based Targeted Attack](https://arxiv.org/abs/2504.18594)
*Tongrui Su,Qingbin Li,Shengyu Zhu,Wei Chen,Xueqi Cheng*

Main category: cs.LG

TLDR: 该论文提出了一种通过DropConnect缓解扰动共适应（MCD）的方法，以提高黑盒攻击中的对抗样本迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在目标攻击中的攻击成功率（ASR）较低，对抗样本容易过拟合替代模型而无法误导其他模型。

Method: 通过DropConnect在每次优化迭代中创建替代模型的多样化变体，缓解扰动共适应现象。

Result: 在从CNN模型迁移到Transformer模型的挑战性场景中，MCD的平均ASR比现有基线高13%。

Conclusion: MCD通过引入更多变体多样性并保留足够的语义信息，显著提升了对抗样本的迁移性和攻击效果。

Abstract: We study the problem of transfer-based black-box attack, where adversarial
samples generated using a single surrogate model are directly applied to target
models. Compared with untargeted attacks, existing methods still have lower
Attack Success Rates (ASRs) in the targeted setting, i.e., the obtained
adversarial examples often overfit the surrogate model but fail to mislead
other models. In this paper, we hypothesize that the pixels or features in
these adversarial examples collaborate in a highly dependent manner to maximize
the success of an adversarial attack on the surrogate model, which we refer to
as perturbation co-adaptation. Then, we propose to Mitigate perturbation
Co-adaptation by DropConnect (MCD) to enhance transferability, by creating
diverse variants of surrogate model at each optimization iteration. We conduct
extensive experiments across various CNN- and Transformer-based models to
demonstrate the effectiveness of MCD. In the challenging scenario of
transferring from a CNN-based model to Transformer-based models, MCD achieves
13% higher average ASRs compared with state-of-the-art baselines. MCD boosts
the performance of self-ensemble methods by bringing in more diversification
across the variants while reserving sufficient semantic information for each
variant. In addition, MCD attains the highest performance gain when scaling the
compute of crafting adversarial examples.

</details>

### [60] [EnviroPiNet: A Physics-Guided AI Model for Predicting Biofilter Performance](https://arxiv.org/abs/2504.18595)
*Uzma,Fabien Cholet,Domenic Quinn,Cindy Smith,Siming You,William Sloan*

Main category: cs.LG

TLDR: 论文提出了一种基于Buckingham Pi理论的模型EnviroPiNet，用于预测生物滤池性能，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 由于高维稀疏数据难以捕捉生物滤池系统的复杂行为，需结合物理原理与AI方法提升预测精度。

Method: 应用Buckingham Pi理论进行降维，构建物理引导的神经网络模型EnviroPiNet，并与PCA和自编码器对比。

Result: EnviroPiNet在测试集上R^2达0.9236，优于传统方法，且揭示了生物滤池的物理化学关系。

Conclusion: 结合物理原理与AI方法可有效建模复杂环境系统，为系统设计和优化提供新思路。

Abstract: Environmental biotechnologies, such as drinking water biofilters, rely on
complex interactions between microbial communities and their surrounding
physical-chemical environments. Predicting the performance of these systems is
challenging due to high-dimensional, sparse datasets that lack diversity and
fail to fully capture system behaviour. Accurate predictive models require
innovative, science-guided approaches. In this study, we present the first
application of Buckingham Pi theory to modelling biofilter performance. This
dimensionality reduction technique identifies meaningful, dimensionless
variables that enhance predictive accuracy and improve model interpretability.
Using these variables, we developed the Environmental Buckingham Pi Neural
Network (EnviroPiNet), a physics-guided model benchmarked against traditional
data-driven methods, including Principal Component Analysis (PCA) and
autoencoder neural networks. Our findings demonstrate that the EnviroPiNet
model achieves an R^2 value of 0.9236 on the testing dataset, significantly
outperforming PCA and autoencoder methods. The Buckingham Pi variables also
provide insights into the physical and chemical relationships governing
biofilter behaviour, with implications for system design and optimization. This
study highlights the potential of combining physical principles with AI
approaches to model complex environmental systems characterized by sparse,
high-dimensional datasets.

</details>

### [61] [A Hybrid Framework for Real-Time Data Drift and Anomaly Identification Using Hierarchical Temporal Memory and Statistical Tests](https://arxiv.org/abs/2504.18599)
*Subhadip Bandyopadhyay,Joy Bose,Sujoy Roy Chowdhury*

Main category: cs.LG

TLDR: 论文提出了一种结合HTM和SPRT的混合框架，用于实时数据漂移检测和异常识别，避免了频繁的重新训练，并在多维监督场景中扩展了HTM的应用。


<details>
  <summary>Details</summary>
Motivation: 数据漂移会导致模型性能下降，现有方法需要频繁重新训练且效果有限。HTM和SPRT的结合为解决这些问题提供了新思路。

Method: 提出了一种结合HTM和SPRT的混合框架，用于实时数据漂移检测和异常识别。同时扩展了HTM在多维监督场景中的应用。

Result: 实验表明，该方法在准确性、适应性和计算效率上优于传统漂移检测技术（如KS检验、Wasserstein距离和PSI）。

Conclusion: 该方法在实时数据漂移检测和异常识别中表现出色，适用于电信等领域，并提供了超参数优化的见解。

Abstract: Data Drift is the phenomenon where the generating model behind the data
changes over time. Due to data drift, any model built on the past training data
becomes less relevant and inaccurate over time. Thus, detecting and controlling
for data drift is critical in machine learning models. Hierarchical Temporal
Memory (HTM) is a machine learning model developed by Jeff Hawkins, inspired by
how the human brain processes information. It is a biologically inspired model
of memory that is similar in structure to the neocortex, and whose performance
is claimed to be comparable to state of the art models in detecting anomalies
in time series data. Another unique benefit of HTMs is its independence from
training and testing cycle; all the learning takes place online with streaming
data and no separate training and testing cycle is required. In sequential
learning paradigm, Sequential Probability Ratio Test (SPRT) offers some unique
benefit for online learning and inference. This paper proposes a novel hybrid
framework combining HTM and SPRT for real-time data drift detection and anomaly
identification. Unlike existing data drift methods, our approach eliminates
frequent retraining and ensures low false positive rates. HTMs currently work
with one dimensional or univariate data. In a second study, we also propose an
application of HTM in multidimensional supervised scenario for anomaly
detection by combining the outputs of multiple HTM columns, one for each
dimension of the data, through a neural network. Experimental evaluations
demonstrate that the proposed method outperforms conventional drift detection
techniques like the Kolmogorov-Smirnov (KS) test, Wasserstein distance, and
Population Stability Index (PSI) in terms of accuracy, adaptability, and
computational efficiency. Our experiments also provide insights into optimizing
hyperparameters for real-time deployment in domains such as Telecom.

</details>

### [62] [Unsupervised outlier detection to improve bird audio dataset labels](https://arxiv.org/abs/2504.18650)
*Bruce Collins*

Main category: cs.LG

TLDR: 论文提出了一种通过音频预处理、降维和无监督异常检测（UOD）的方法，用于减少从Xeno-Canto音频库中提取的鸟类数据集中的标签噪声。


<details>
  <summary>Details</summary>
Motivation: Xeno-Canto音频库中的鸟类录音常包含非目标物种的声音，导致标签噪声，影响机器学习模型的分类准确性。

Method: 采用音频预处理、降维（包括卷积自编码器和变分深度嵌入VaDE）和无监督异常检测来清理数据集。

Result: 方法对大多数鸟类数据集有效，但不同物种间的性能差异显著。

Conclusion: 清理过程能显著减少标签噪声，但效果因物种而异。

Abstract: The Xeno-Canto bird audio repository is an invaluable resource for those
interested in vocalizations and other sounds made by birds around the world.
This is particularly the case for machine learning researchers attempting to
improve on the bird species recognition accuracy of classification models.
However, the task of extracting labeled datasets from the recordings found in
this crowd-sourced repository faces several challenges. One challenge of
particular significance to machine learning practitioners is that one bird
species label is applied to each audio recording, but frequently other sounds
are also captured including other bird species, other animal sounds,
anthropogenic and other ambient sounds. These non-target bird species sounds
can result in dataset labeling discrepancies referred to as label noise. In
this work we present a cleaning process consisting of audio preprocessing
followed by dimensionality reduction and unsupervised outlier detection (UOD)
to reduce the label noise in a dataset derived from Xeno-Canto recordings. We
investigate three neural network dimensionality reduction techniques: two
flavors of convolutional autoencoders and variational deep embedding (VaDE
(Jiang, 2017)). While both methods show some degree of effectiveness at
detecting outliers for most bird species datasets, we found significant
variation in the performance of the methods from one species to the next. We
believe that the results of this investigation demonstrate that the application
of our cleaning process can meaningfully reduce the label noise of bird species
datasets derived from Xeno-Canto audio repository but results vary across
species.

</details>

### [63] [Exploring the Potential of Latent Embeddings for Sea Ice Characterization using ICESat-2 Data](https://arxiv.org/abs/2504.18668)
*Daehyeon Han,Morteza Karimzadeh*

Main category: cs.LG

TLDR: 该研究探讨了无监督自编码器在ICESat-2数据上的应用，以减少对人工标注的依赖，并通过LSTM和CNN模型生成潜在嵌入。


<details>
  <summary>Details</summary>
Motivation: ICESat-2数据的人工标注耗时且依赖背景图像匹配，限制了监督学习的效率。

Method: 使用LSTM和CNN自编码器重构地形序列，并通过UMAP降维可视化嵌入。

Result: 自编码器生成的嵌入保留了数据结构，且聚类更紧凑，表明可以减少所需标注样本数量。

Conclusion: 无监督自编码器在减少ICESat-2数据标注需求方面具有潜力。

Abstract: The Ice, Cloud, and Elevation Satellite-2 (ICESat-2) provides high-resolution
measurements of sea ice height. Recent studies have developed machine learning
methods on ICESat-2 data, primarily focusing on surface type classification.
However, the heavy reliance on manually collected labels requires significant
time and effort for supervised learning, as it involves cross-referencing track
measurements with overlapping background optical imagery. Additionally, the
coincidence of ICESat-2 tracks with background images is relatively rare due to
the different overpass patterns and atmospheric conditions. To address these
limitations, this study explores the potential of unsupervised autoencoder on
unlabeled data to derive latent embeddings. We develop autoencoder models based
on Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNN) to
reconstruct topographic sequences from ICESat-2 and derive embeddings. We then
apply Uniform Manifold Approximation and Projection (UMAP) to reduce dimensions
and visualize the embeddings. Our results show that embeddings from
autoencoders preserve the overall structure but generate relatively more
compact clusters compared to the original ICESat-2 data, indicating the
potential of embeddings to lessen the number of required labels samples.

</details>

### [64] [A Unified MDL-based Binning and Tensor Factorization Framework for PDF Estimation](https://arxiv.org/abs/2504.18686)
*Mustafa Musab,Joseph K. Chege,Arie Yeredor,Martin Haardt*

Main category: cs.LG

TLDR: 提出了一种基于最小描述长度（MDL）的分箱和分位数切割的非参数多变量概率密度函数估计方法，解决了传统直方图在捕捉局部变化和连续性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统直方图密度估计方法在捕捉非均匀分布的局部变化和连续性方面存在不足，影响了梯度优化、聚类等任务的性能。

Method: 采用基于MDL的分箱和分位数切割，结合张量分解技术（CPD）进行联合概率张量分解。

Result: 在合成数据和真实干豆分类数据集上验证了方法的有效性。

Conclusion: 该方法能够更准确地估计多模态和非均匀分布的概率密度函数，适用于需要平滑导数的任务。

Abstract: Reliable density estimation is fundamental for numerous applications in
statistics and machine learning. In many practical scenarios, data are best
modeled as mixtures of component densities that capture complex and multimodal
patterns. However, conventional density estimators based on uniform histograms
often fail to capture local variations, especially when the underlying
distribution is highly nonuniform. Furthermore, the inherent discontinuity of
histograms poses challenges for tasks requiring smooth derivatives, such as
gradient-based optimization, clustering, and nonparametric discriminant
analysis. In this work, we present a novel non-parametric approach for
multivariate probability density function (PDF) estimation that utilizes
minimum description length (MDL)-based binning with quantile cuts. Our approach
builds upon tensor factorization techniques, leveraging the canonical polyadic
decomposition (CPD) of a joint probability tensor. We demonstrate the
effectiveness of our method on synthetic data and a challenging real dry bean
classification dataset.

</details>

### [65] [Active Few-Shot Learning for Vertex Classification Starting from an Unlabeled Dataset](https://arxiv.org/abs/2504.18696)
*Felix Burr,Marcel Hoffmann,Ansgar Scherp*

Main category: cs.LG

TLDR: 论文研究了在少量标注顶点的情况下进行图数据分类的方法，比较了原型网络和判别模型（如GCN）的性能，并通过逐步放宽假设的实验验证了原型网络的优越性。


<details>
  <summary>Details</summary>
Motivation: 由于获取顶点标签的成本高昂，研究如何在少量标注数据下进行有效分类具有重要意义。现有方法依赖类别预言机，但实际场景中并不存在。

Method: 通过迭代提示人类标注者标注顶点，设计了三个实验：平衡采样、不平衡采样和未知类别数量，逐步放宽假设。

Result: 原型网络在所有实验中表现优于判别模型，尤其在每类少于20个样本时。放宽假设后，原型网络的性能下降较少。

Conclusion: 原型网络在少量标注数据下更具鲁棒性，适合实际应用场景。

Abstract: Despite the ample availability of graph data, obtaining vertex labels is a
tedious and expensive task. Therefore, it is desirable to learn from a few
labeled vertices only. Existing few-shot learners assume a class oracle, which
provides labeled vertices for a desired class. However, such an oracle is not
available in a real-world setting, i.e., when drawing a vertex for labeling it
is unknown to which class the vertex belongs. Few-shot learners are often
combined with prototypical networks, while classical semi-supervised vertex
classification uses discriminative models, e.g., Graph Convolutional Networks
(GCN). In this paper, we train our models by iteratively prompting a human
annotator with vertices to annotate. We perform three experiments where we
continually relax our assumptions. First, we assume a class oracle, i.e., the
human annotator is provided with an equal number of vertices to label for each
class. We denote this as "Balanced Sampling''. In the subsequent experiment,
"Unbalanced Sampling,'' we replace the class oracle with $k$-medoids clustering
and draw vertices to label from the clusters. In the last experiment, the
"Unknown Number of Classes,'' we no longer assumed we knew the number and
distribution of classes. Our results show that prototypical models outperform
discriminative models in all experiments when fewer than $20$ samples per class
are available. While dropping the assumption of the class oracle for the
"Unbalanced Sampling'' experiment reduces the performance of the GCN by $9\%$,
the prototypical network loses only $1\%$ on average. For the "Unknown Number
of Classes'' experiment, the average performance for both models decreased
further by $1\%$.
  Source code: https://github.com/Ximsa/2023-felix-ma

</details>

### [66] [Explicit neural network classifiers for non-separable data](https://arxiv.org/abs/2504.18710)
*Patrícia Muñoz Ewald*

Main category: cs.LG

TLDR: 论文通过截断映射完全描述了一类前馈神经网络，并展示了ReLU网络如何实现分离同心数据的特征映射。


<details>
  <summary>Details</summary>
Motivation: 研究前馈神经网络的特性及其在实际问题中的应用，特别是如何通过ReLU网络处理复杂数据分布。

Method: 使用截断映射对一类前馈神经网络进行完整描述，并应用于ReLU网络的特征映射实现。

Result: 证明了ReLU网络能够通过特征映射有效分离同心数据。

Conclusion: 截断映射为理解前馈神经网络提供了新视角，ReLU网络在处理特定数据分布时具有潜力。

Abstract: We fully characterize a large class of feedforward neural networks in terms
of truncation maps. As an application, we show how a ReLU neural network can
implement a feature map which separates concentric data.

</details>

### [67] [Appa: Bending Weather Dynamics with Latent Diffusion Models for Global Data Assimilation](https://arxiv.org/abs/2504.18720)
*Gérôme Andry,François Rozet,Sacha Lewin,Omer Rochman,Victor Mangeleer,Matthias Pirlet,Elise Faulx,Marilaure Grégoire,Gilles Louppe*

Main category: cs.LG

TLDR: Appa是一个基于分数的数据同化模型，用于从观测数据中推断全球大气轨迹，支持多种推理任务。


<details>
  <summary>Details</summary>
Motivation: 解决从大量观测数据中识别当前大气状态的挑战性问题。

Method: 使用1.5B参数的时空潜在扩散模型，基于ERA5再分析数据训练，支持多种观测条件。

Result: 实验显示物理一致性良好，重建和预测能力优异。

Conclusion: 基于分数的数据同化为未来全球大气建模系统提供了有前景的基础。

Abstract: Deep learning has transformed weather forecasting by improving both its
accuracy and computational efficiency. However, before any forecast can begin,
weather centers must identify the current atmospheric state from vast amounts
of observational data. To address this challenging problem, we introduce Appa,
a score-based data assimilation model producing global atmospheric trajectories
at 0.25-degree resolution and 1-hour intervals. Powered by a 1.5B-parameter
spatio-temporal latent diffusion model trained on ERA5 reanalysis data, Appa
can be conditioned on any type of observations to infer the posterior
distribution of plausible state trajectories, without retraining. Our unified
probabilistic framework flexibly tackles multiple inference tasks --
reanalysis, filtering, and forecasting -- using the same model, eliminating the
need for task-specific architectures or training procedures. Experiments
demonstrate physical consistency on a global scale and good reconstructions
from observations, while showing competitive forecasting skills. Our results
establish latent score-based data assimilation as a promising foundation for
future global atmospheric modeling systems.

</details>

### [68] [Multimodal graph representation learning for website generation based on visual sketch](https://arxiv.org/abs/2504.18729)
*Tung D. Vu,Chung Hoang,Truong-Son Hy*

Main category: cs.LG

TLDR: 提出了一种基于多模态图表示学习的新方法，用于将设计草图转换为功能源代码，显著提高了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法在解释网页设计的复杂视觉细节和结构关系时存在局限性，导致自动化效率低下。

Method: 利用多模态图表示学习，整合视觉和结构信息，生成语义正确且结构合理的HTML代码。

Result: 实验表明，该方法在准确性和效率上均显著优于现有技术。

Conclusion: 该方法有望彻底改变设计到代码的自动化过程。

Abstract: The Design2Code problem, which involves converting digital designs into
functional source code, is a significant challenge in software development due
to its complexity and time-consuming nature. Traditional approaches often
struggle with accurately interpreting the intricate visual details and
structural relationships inherent in webpage designs, leading to limitations in
automation and efficiency. In this paper, we propose a novel method that
leverages multimodal graph representation learning to address these challenges.
By integrating both visual and structural information from design sketches, our
approach enhances the accuracy and efficiency of code generation, particularly
in producing semantically correct and structurally sound HTML code. We present
a comprehensive evaluation of our method, demonstrating significant
improvements in both accuracy and efficiency compared to existing techniques.
Extensive evaluation demonstrates significant improvements of multimodal graph
learning over existing techniques, highlighting the potential of our method to
revolutionize design-to-code automation. Code available at
https://github.com/HySonLab/Design2Code

</details>

### [69] [TLoRA: Tri-Matrix Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2504.18735)
*Tanvir Islam*

Main category: cs.LG

TLDR: TLoRA是一种新型的三矩阵低秩适应方法，通过分解权重更新为三个矩阵（两个固定随机矩阵和一个可训练矩阵）并结合可学习的层间缩放因子，实现了高效的参数适应。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种资源高效的低秩适应方法，减少可训练参数数量同时保持性能。

Method: 采用三矩阵设计（两个固定随机矩阵和一个可训练矩阵）及层间缩放因子，优化参数适应效率。

Result: 在GLUE基准测试中表现与现有低秩方法相当，但可训练参数显著减少；适应动态分析显示其具有高斯分布权重、稳定参数范数和层间缩放因子变化。

Conclusion: TLoRA是一种高效且有效的LLM微调方法，为资源高效模型适应提供了重要进展。

Abstract: We propose TLoRA, a novel tri-matrix low-rank adaptation method that
decomposes weight updates into three matrices: two fixed random matrices and
one trainable matrix, combined with a learnable, layer-wise scaling factor.
This tri-matrix design enables TLoRA to achieve highly efficient parameter
adaptation while introducing minimal additional computational overhead. Through
extensive experiments on the GLUE benchmark, we demonstrate that TLoRA achieves
comparable performance to existing low-rank methods such as LoRA and
Adapter-based techniques, while requiring significantly fewer trainable
parameters. Analyzing the adaptation dynamics, we observe that TLoRA exhibits
Gaussian-like weight distributions, stable parameter norms, and scaling factor
variability across layers, further highlighting its expressive power and
adaptability. Additionally, we show that TLoRA closely resembles LoRA in its
eigenvalue distributions, parameter norms, and cosine similarity of updates,
underscoring its ability to effectively approximate LoRA's adaptation behavior.
Our results establish TLoRA as a highly efficient and effective fine-tuning
method for LLMs, offering a significant step forward in resource-efficient
model adaptation.

</details>

### [70] [Non-Asymptotic Guarantees for Average-Reward Q-Learning with Adaptive Stepsizes](https://arxiv.org/abs/2504.18743)
*Zaiwei Chen*

Main category: cs.LG

TLDR: 本文首次对异步实现的平均奖励Q学习的最后迭代收敛性进行了有限时间分析，证明了自适应步长的必要性及其在抵消异步更新影响中的作用。


<details>
  <summary>Details</summary>
Motivation: 研究异步Q学习算法的收敛性，特别是自适应步长对算法性能的影响，以解决非马尔可夫随机逼近问题。

Method: 采用自适应步长作为局部时钟，结合中心化步骤，通过时间非齐次马尔可夫重构和非马尔可夫随机逼近技术分析收敛性。

Result: 算法在均方意义下以O(1/k)的速率收敛到最优相对Q函数，自适应步长被证明是收敛的关键。

Conclusion: 自适应步长是异步Q学习收敛的必要条件，所开发的分析工具可广泛应用于带自适应步长的随机逼近算法。

Abstract: This work presents the first finite-time analysis for the last-iterate
convergence of average-reward Q-learning with an asynchronous implementation. A
key feature of the algorithm we study is the use of adaptive stepsizes, which
serve as local clocks for each state-action pair. We show that the iterates
generated by this Q-learning algorithm converge at a rate of $O(1/k)$ (in the
mean-square sense) to the optimal relative Q-function in the span seminorm.
Moreover, by adding a centering step to the algorithm, we further establish
pointwise mean-square convergence to a centered optimal relative Q-function,
also at a rate of $O(1/k)$. To prove these results, we show that adaptive
stepsizes are necessary, as without them, the algorithm fails to converge to
the correct target. In addition, adaptive stepsizes can be interpreted as a
form of implicit importance sampling that counteracts the effects of
asynchronous updates.
  Technically, the use of adaptive stepsizes makes each Q-learning update
depend on the entire sample history, introducing strong correlations and making
the algorithm a non-Markovian stochastic approximation (SA) scheme. Our
approach to overcoming this challenge involves (1) a time-inhomogeneous
Markovian reformulation of non-Markovian SA, and (2) a combination of
almost-sure time-varying bounds, conditioning arguments, and Markov chain
concentration inequalities to break the strong correlations between the
adaptive stepsizes and the iterates. The tools developed in this work are
likely to be broadly applicable to the analysis of general SA algorithms with
adaptive stepsizes.

</details>

### [71] [High-order Graph Neural Networks with Common Neighbor Awareness for Link Prediction](https://arxiv.org/abs/2504.18758)
*Ling Wang,Minglian Han*

Main category: cs.LG

TLDR: 论文提出了一种高阶图神经网络HGNN-CNA，通过考虑多跳共同邻居来改进动态图中的链接预测任务，显著提升了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有动态图神经网络（DGNN）主要依赖成对节点交互，忽略了共同邻居的作用，导致性能受限。

Method: 提出HGNN-CNA，通过计算多跳共同邻居的相关性得分，并将其融入消息传递过程，直接考虑共同邻居交互。

Result: 在三个真实动态图上的实验表明，HGNN-CNA在链接预测任务上显著优于现有模型。

Conclusion: HGNN-CNA通过引入共同邻居感知机制，有效提升了动态图链接预测的性能。

Abstract: Link prediction is a fundamental task in dynamic graph learning (DGL),
inherently shaped by the topology of the DG. Recent advancements in dynamic
graph neural networks (DGNN), primarily by modeling the relationships among
nodes via a message passing scheme, have significantly improved link prediction
performance. However, DGNNs heavily rely on the pairwise node interactions,
which neglect the common neighbor interaction in DGL. To address this
limitation, we propose a High-order Graph Neural Networks with Common Neighbor
Awareness (HGNN-CNA) for link prediction with two-fold ideas: a) estimating
correlation score by considering multi-hop common neighbors for capturing the
complex interaction between nodes; b) fusing the correlation into the
message-passing process to consider common neighbor interaction directly in
DGL. Experimental results on three real DGs demonstrate that the proposed
HGNN-CNA acquires a significant accuracy gain over several state-of-the-art
models on the link prediction task.

</details>

### [72] [Dynamic Action Interpolation: A Universal Approach for Accelerating Reinforcement Learning with Expert Guidance](https://arxiv.org/abs/2504.18766)
*Wenjun Cao*

Main category: cs.LG

TLDR: 提出了一种名为动态动作插值（DAI）的简单框架，通过时间变化权重插值专家和RL动作，显著提升强化学习的样本效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）样本效率低，尤其是在早期训练阶段，需要大量环境交互。现有方法通常通过引入先验知识解决，但增加了架构和实现的复杂性。

Method: DAI框架通过时间变化权重α(t)插值专家和RL动作，无需额外网络或损失函数，可轻松集成到任何Actor-Critic算法中。

Result: 在MuJoCo连续控制任务中，DAI早期性能平均提升160%，最终性能提升50%以上，Humanoid任务早期提升4倍，收敛时提升2倍。

Conclusion: DAI证明了无需复杂架构修改即可实现高效样本强化学习，挑战了现有假设。

Abstract: Reinforcement learning (RL) suffers from severe sample inefficiency,
especially during early training, requiring extensive environmental
interactions to perform competently. Existing methods tend to solve this by
incorporating prior knowledge, but introduce significant architectural and
implementation complexity. We propose Dynamic Action Interpolation (DAI), a
universal yet straightforward framework that interpolates expert and RL actions
via a time-varying weight $\alpha(t)$, integrating into any Actor-Critic
algorithm with just a few lines of code and without auxiliary networks or
additional losses. Our theoretical analysis shows that DAI reshapes state
visitation distributions to accelerate value function learning while preserving
convergence guarantees. Empirical evaluations across MuJoCo continuous control
tasks demonstrate that DAI improves early-stage performance by over 160\% on
average and final performance by more than 50\%, with the Humanoid task showing
a 4$\times$ improvement early on and a 2$\times$ gain at convergence. These
results challenge the assumption that complex architectural modifications are
necessary for sample-efficient reinforcement learning.

</details>

### [73] [Performance of Machine Learning Classifiers for Anomaly Detection in Cyber Security Applications](https://arxiv.org/abs/2504.18771)
*Markus Haug,Gissel Velarde*

Main category: cs.LG

TLDR: 论文评估了多种机器学习模型在两类不平衡公开数据集上的表现，发现XGB和MLP优于生成模型，并比较了不同缺失数据处理方法的效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估不同机器学习模型在处理不平衡数据集时的表现，并探索缺失数据对模型性能的影响。

Method: 方法包括数据准备、模型训练和评估，使用80/20的训练/测试划分，测试了XGB、MLP、GAN、VAE和MO-GAAL等模型，并结合ROS和SPE技术。评估采用5折交叉验证和多种缺失数据填补方法。

Result: 结果显示XGB和MLP优于生成模型，IterativeImputer填补效果与均值和中位数相当，但不推荐用于大数据集。

Conclusion: 结论是XGB和MLP在处理不平衡数据时表现更优，而IterativeImputer填补方法在复杂性和执行时间上存在局限性。

Abstract: This work empirically evaluates machine learning models on two imbalanced
public datasets (KDDCUP99 and Credit Card Fraud 2013). The method includes data
preparation, model training, and evaluation, using an 80/20 (train/test) split.
Models tested include eXtreme Gradient Boosting (XGB), Multi Layer Perceptron
(MLP), Generative Adversarial Network (GAN), Variational Autoencoder (VAE), and
Multiple-Objective Generative Adversarial Active Learning (MO-GAAL), with XGB
and MLP further combined with Random-Over-Sampling (ROS) and
Self-Paced-Ensemble (SPE). Evaluation involves 5-fold cross-validation and
imputation techniques (mean, median, and IterativeImputer) with 10, 20, 30, and
50 % missing data. Findings show XGB and MLP outperform generative models.
IterativeImputer results are comparable to mean and median, but not recommended
for large datasets due to increased complexity and execution time. The code
used is publicly available on GitHub (github.com/markushaug/acr-25).

</details>

### [74] [ALF: Advertiser Large Foundation Model for Multi-Modal Advertiser Understanding](https://arxiv.org/abs/2504.18785)
*Santosh Rajagopalan,Jonathan Vronsky,Songbai Yan,S. Alireza Golestaneh,Shubhra Chandra,Min Zhou*

Main category: cs.LG

TLDR: ALF是一种多模态Transformer架构，用于理解广告主行为和意图，通过对比学习和多任务优化实现统一表征，在欺诈检测等任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决广告主行为和意图的多模态理解问题，提升广告平台的安全性和效率。

Method: 采用多模态Transformer架构，结合对比学习和多任务优化，引入跨样本注意力机制和谱归一化投影。

Result: 在欺诈检测等任务中达到SOTA性能，生产部署中误报减少90%，精确度达99.8%。

Conclusion: ALF通过多模态融合和优化技术，显著提升了广告主行为理解的准确性和效率。

Abstract: We present ALF (Advertiser Large Foundation model), a multi-modal transformer
architecture for understanding advertiser behavior and intent across text,
image, video and structured data modalities. Through contrastive learning and
multi-task optimization, ALF creates unified advertiser representations that
capture both content and behavioral patterns. Our model achieves
state-of-the-art performance on critical tasks including fraud detection,
policy violation identification, and advertiser similarity matching. In
production deployment, ALF reduces false positives by 90% while maintaining
99.8% precision on abuse detection tasks. The architecture's effectiveness
stems from its novel combination of multi-modal transformations, inter-sample
attention mechanism, spectrally normalized projections, and calibrated
probabilistic outputs.

</details>

### [75] [Frequency-Integrated Transformer for Arbitrary-Scale Super-Resolution](https://arxiv.org/abs/2504.18818)
*Xufei Wang,Fei Ge,Jinchen Zhu,Mingjian Zhang,Qi Wu,Jifeng Ren Shizhuang Weng*

Main category: cs.LG

TLDR: 提出了一种名为FIT的新网络，通过结合频域信息提升任意尺度超分辨率任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于隐式神经表示的方法忽略了频域信息的潜在价值，导致性能不佳。

Method: FIT包含FIM模块（通过FFT和实虚映射引入频域信息）和FUSAM模块（通过IISA和FCSA利用频域信息）。

Result: 实验表明FIT在多个基准数据集上优于现有方法，FIM丰富了细节表征，IISA提高了频域保真度，FCSA捕获了全局上下文。

Conclusion: FIT通过有效利用频域信息显著提升了超分辨率任务的性能。

Abstract: Methods based on implicit neural representation have demonstrated remarkable
capabilities in arbitrary-scale super-resolution (ASSR) tasks, but they neglect
the potential value of the frequency domain, leading to sub-optimal
performance. We proposes a novel network called Frequency-Integrated
Transformer (FIT) to incorporate and utilize frequency information to enhance
ASSR performance. FIT employs Frequency Incorporation Module (FIM) to introduce
frequency information in a lossless manner and Frequency Utilization
Self-Attention module (FUSAM) to efficiently leverage frequency information by
exploiting spatial-frequency interrelationship and global nature of frequency.
FIM enriches detail characterization by incorporating frequency information
through a combination of Fast Fourier Transform (FFT) with real-imaginary
mapping. In FUSAM, Interaction Implicit Self-Attention (IISA) achieves
cross-domain information synergy by interacting spatial and frequency
information in subspace, while Frequency Correlation Self-attention (FCSA)
captures the global context by computing correlation in frequency. Experimental
results demonstrate FIT yields superior performance compared to existing
methods across multiple benchmark datasets. Visual feature map proves the
superiority of FIM in enriching detail characterization. Frequency error map
validates IISA productively improve the frequency fidelity. Local attribution
map validates FCSA effectively captures global context.

</details>

### [76] [Preserving Seasonal and Trend Information: A Variational Autoencoder-Latent Space Arithmetic Based Approach for Non-stationary Learning](https://arxiv.org/abs/2504.18819)
*Hassan Wasswa,Aziida Nanyonga,Timothy Lynar*

Main category: cs.LG

TLDR: 提出了一种在潜在空间中强制平稳性同时保留趋势和季节性信息的方法，解决了传统方法因数据平稳化导致信息丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 大多数AI模型假设训练环境是平稳的，但实际数据往往是非平稳的，传统方法在平稳化过程中丢失了趋势和季节性信息。

Method: 结合差分、时间序列分解和潜在空间算术（LSA），在变分自编码器（VAE）的潜在空间中保留趋势和季节性信息。

Result: 在两个非平稳时间序列数据集上验证了方法的有效性，四种深度学习模型在潜在向量表示上均取得与现有技术相当的预测性能。

Conclusion: 该方法成功保留了趋势和季节性信息，同时实现了与非平稳数据兼容的预测性能。

Abstract: AI models have garnered significant research attention towards predictive
task automation. However, a stationary training environment is an underlying
assumption for most models and such models simply do not work on non-stationary
data since a stationary relationship is learned. The existing solutions propose
making data stationary prior to model training and evaluation. This leads to
loss of trend and seasonal patterns which are vital components for learning
temporal dependencies of the system under study. This research aims to address
this limitation by proposing a method for enforcing stationary behaviour within
the latent space while preserving trend and seasonal information. The method
deploys techniques including Differencing, Time-series decomposition, and
Latent Space Arithmetic (LSA), to learn information vital for efficient
approximation of trend and seasonal information which is then stored as
embeddings within the latent space of a Variational Autoencoder (VAE). The
approach's ability to preserve trend and seasonal information was evaluated on
two time-series non-stationary datasets. For predictive performance evaluation,
four deep learning models were trained on the latent vector representations of
the datasets after application of the proposed method and all models produced
competitive results in comparison with state-of-the-art techniques using RMSE
as the performance metric.

</details>

### [77] [Introducing Interval Neural Networks for Uncertainty-Aware System Identification](https://arxiv.org/abs/2504.18845)
*Mehmet Ali Ferah,Tufan Kumbasar*

Main category: cs.LG

TLDR: 论文提出了一种基于区间神经网络（INNs）的系统辨识框架，用于量化深度学习模型的不确定性，并扩展了LSTM和Neural ODEs为ILSTM和INODE架构。


<details>
  <summary>Details</summary>
Motivation: 传统系统辨识方法难以捕捉非线性动态，而深度学习缺乏不确定性量化（UQ），限制了其可靠性和安全性。

Method: 通过将预训练神经网络的参数转化为区间值参数，利用区间算术生成预测区间（PIs），并提出了训练INNs的深度学习框架。

Result: 实验验证了ILSTM和INODE在系统辨识中的有效性，能够有效捕捉目标覆盖范围。

Conclusion: INNs为系统辨识提供了一种无需概率假设的不确定性量化方法，扩展了深度学习在动态系统建模中的应用。

Abstract: System Identification (SysID) is crucial for modeling and understanding
dynamical systems using experimental data. While traditional SysID methods
emphasize linear models, their inability to fully capture nonlinear dynamics
has driven the adoption of Deep Learning (DL) as a more powerful alternative.
However, the lack of uncertainty quantification (UQ) in DL-based models poses
challenges for reliability and safety, highlighting the necessity of
incorporating UQ. This paper introduces a systematic framework for constructing
and learning Interval Neural Networks (INNs) to perform UQ in SysID tasks. INNs
are derived by transforming the learnable parameters (LPs) of pre-trained
neural networks into interval-valued LPs without relying on probabilistic
assumptions. By employing interval arithmetic throughout the network, INNs can
generate Prediction Intervals (PIs) that capture target coverage effectively.
We extend Long Short-Term Memory (LSTM) and Neural Ordinary Differential
Equations (Neural ODEs) into Interval LSTM (ILSTM) and Interval NODE (INODE)
architectures, providing the mathematical foundations for their application in
SysID. To train INNs, we propose a DL framework that integrates a UQ loss
function and parameterization tricks to handle constraints arising from
interval LPs. We introduce novel concept "elasticity" for underlying
uncertainty causes and validate ILSTM and INODE in SysID experiments,
demonstrating their effectiveness.

</details>

### [78] [Theoretical Framework for Tempered Fractional Gradient Descent: Application to Breast Cancer Classification](https://arxiv.org/abs/2504.18849)
*Omar Naifar*

Main category: cs.LG

TLDR: TFGD结合分数阶微积分和指数调温，提出了一种新型优化框架，解决了传统梯度下降在高维噪声环境中的振荡和收敛慢问题。


<details>
  <summary>Details</summary>
Motivation: 传统梯度下降方法在高维噪声环境中表现不佳，TFGD旨在通过引入调温记忆机制改进这一问题。

Method: TFGD通过分数阶系数和调温参数加权历史梯度，理论分析证明了其收敛性，并保持了与SGD相当的时间复杂度。

Result: 在乳腺癌威斯康星数据集上，TFGD实现了98.25%的测试准确率，收敛速度是SGD的2倍。

Conclusion: TFGD在理论和应用上均表现出色，是传统优化器的有力替代方案。

Abstract: This paper introduces Tempered Fractional Gradient Descent (TFGD), a novel
optimization framework that synergizes fractional calculus with exponential
tempering to enhance gradient-based learning. Traditional gradient descent
methods often suffer from oscillatory updates and slow convergence in
high-dimensional, noisy landscapes. TFGD addresses these limitations by
incorporating a tempered memory mechanism, where historical gradients are
weighted by fractional coefficients $|w_j| = \binom{\alpha}{j}$ and
exponentially decayed via a tempering parameter $\lambda$. Theoretical analysis
establishes TFGD's convergence guarantees: in convex settings, it achieves an
$\mathcal{O}(1/K)$ rate with alignment coefficient $d_{\alpha,\lambda} = (1 -
e^{-\lambda})^{-\alpha}$, while stochastic variants attain
$\mathcal{O}(1/k^\alpha)$ error decay. The algorithm maintains $\mathcal{O}(n)$
time complexity equivalent to SGD, with memory overhead scaling as
$\mathcal{O}(d/\lambda)$ for parameter dimension $d$. Empirical validation on
the Breast Cancer Wisconsin dataset demonstrates TFGD's superiority, achieving
98.25\% test accuracy (vs. 92.11\% for SGD) and 2$\times$ faster convergence.
The tempered memory mechanism proves particularly effective in medical
classification tasks, where feature correlations benefit from stable gradient
averaging. These results position TFGD as a robust alternative to conventional
optimizers in both theoretical and applied machine learning.

</details>

### [79] [TSRM: A Lightweight Temporal Feature Encoding Architecture for Time Series Forecasting and Imputation](https://arxiv.org/abs/2504.18878)
*Robert Leppich,Michael Stenger,Daniel Grillmeyer,Vanessa Borst,Samuel Kounev*

Main category: cs.LG

TLDR: TSRM是一种用于多变量时间序列预测和填补的时间特征编码架构，基于CNN和注意力机制，性能优于现有方法且参数复杂度低。


<details>
  <summary>Details</summary>
Motivation: 解决多变量时间序列预测和填补任务中捕捉多样化时间模式的问题。

Method: 采用CNN表示层、注意力特征提取层和合并层，基于Transformer编码器结构，核心为自注意力机制。

Result: 在七个基准数据集上优于现有方法，同时显著减少可学习参数。

Conclusion: TSRM在性能和效率上均表现优异，代码已开源。

Abstract: We introduce a temporal feature encoding architecture called Time Series
Representation Model (TSRM) for multivariate time series forecasting and
imputation. The architecture is structured around CNN-based representation
layers, each dedicated to an independent representation learning task and
designed to capture diverse temporal patterns, followed by an attention-based
feature extraction layer and a merge layer, designed to aggregate extracted
features. The architecture is fundamentally based on a configuration that is
inspired by a Transformer encoder, with self-attention mechanisms at its core.
The TSRM architecture outperforms state-of-the-art approaches on most of the
seven established benchmark datasets considered in our empirical evaluation for
both forecasting and imputation tasks. At the same time, it significantly
reduces complexity in the form of learnable parameters. The source code is
available at https://github.com/RobertLeppich/TSRM.

</details>

### [80] [TSCAN: Context-Aware Uplift Modeling via Two-Stage Training for Online Merchant Business Diagnosis](https://arxiv.org/abs/2504.18881)
*Hangtao Zhang,Zhe Li,Kairui Zhang*

Main category: cs.LG

TLDR: 论文提出了一种基于两阶段训练方法的上下文感知提升模型（TSCAN），通过CAN-U和CAN-D子模型解决样本选择偏差问题，并利用上下文感知注意力层处理不同环境下的治疗效果差异。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理样本选择偏差时可能引入信息损失，且无法充分利用上下文特征。

Method: TSCAN采用两阶段训练：第一阶段（CAN-U）结合IPM和倾向得分预测生成反事实提升标签；第二阶段（CAN-D）通过等渗输出层直接建模提升效果，并引入上下文感知注意力层。

Result: 在两个真实数据集上的实验验证了TSCAN的有效性，并在实际应用中展示了其实用性。

Conclusion: TSCAN通过两阶段设计和上下文感知机制，显著提升了治疗效果估计的准确性和实用性。

Abstract: A primary challenge in ITE estimation is sample selection bias. Traditional
approaches utilize treatment regularization techniques such as the Integral
Probability Metrics (IPM), re-weighting, and propensity score modeling to
mitigate this bias. However, these regularizations may introduce undesirable
information loss and limit the performance of the model. Furthermore, treatment
effects vary across different external contexts, and the existing methods are
insufficient in fully interacting with and utilizing these contextual features.
To address these issues, we propose a Context-Aware uplift model based on the
Two-Stage training approach (TSCAN), comprising CAN-U and CAN-D sub-models. In
the first stage, we train an uplift model, called CAN-U, which includes the
treatment regularizations of IPM and propensity score prediction, to generate a
complete dataset with counterfactual uplift labels. In the second stage, we
train a model named CAN-D, which utilizes an isotonic output layer to directly
model uplift effects, thereby eliminating the reliance on the regularization
components. CAN-D adaptively corrects the errors estimated by CAN-U through
reinforcing the factual samples, while avoiding the negative impacts associated
with the aforementioned regularizations. Additionally, we introduce a
Context-Aware Attention Layer throughout the two-stage process to manage the
interactions between treatment, merchant, and contextual features, thereby
modeling the varying treatment effect in different contexts. We conduct
extensive experiments on two real-world datasets to validate the effectiveness
of TSCAN. Ultimately, the deployment of our model for real-world merchant
diagnosis on one of China's largest online food ordering platforms validates
its practical utility and impact.

</details>

### [81] [SPD Learning for Covariance-Based Neuroimaging Analysis: Perspectives, Methods, and Challenges](https://arxiv.org/abs/2504.18882)
*Ce Ju,Reinmar J. Kobler,Antoine Collas,Motoaki Kawanabe,Cuntai Guan,Bertrand Thirion*

Main category: cs.LG

TLDR: 该论文综述了基于协方差的神经影像数据的机器学习方法，重点介绍了对称正定（SPD）矩阵的黎曼几何分析框架。


<details>
  <summary>Details</summary>
Motivation: 神经影像数据存在低信噪比、跨会话非平稳性和样本量有限等挑战，需要开发更有效的方法来解码任务特异性特征。

Method: 利用黎曼度量（如仿射不变或对数欧几里得）将SPD矩阵空间转化为黎曼流形，进行几何分析。

Result: 提出了SPD学习框架，系统利用SPD流形的几何特性处理协方差特征，推动了脑影像分析的发展。

Conclusion: SPD学习框架为神经影像数据的机器学习提供了统一的几何分析方法，具有重要的理论和应用价值。

Abstract: Neuroimaging provides a critical framework for characterizing brain activity
by quantifying connectivity patterns and functional architecture across
modalities. While modern machine learning has significantly advanced our
understanding of neural processing mechanisms through these datasets, decoding
task-specific signatures must contend with inherent neuroimaging constraints,
for example, low signal-to-noise ratios in raw electrophysiological recordings,
cross-session non-stationarity, and limited sample sizes. This review focuses
on machine learning approaches for covariance-based neuroimaging data, where
often symmetric positive definite (SPD) matrices under full-rank conditions
encode inter-channel relationships. By equipping the space of SPD matrices with
Riemannian metrics (e.g., affine-invariant or log-Euclidean), their space forms
a Riemannian manifold enabling geometric analysis. We unify methodologies
operating on this manifold under the SPD learning framework, which
systematically leverages the SPD manifold's geometry to process covariance
features, thereby advancing brain imaging analytics.

</details>

### [82] [Factor Analysis with Correlated Topic Model for Multi-Modal Data](https://arxiv.org/abs/2504.18914)
*Małgorzata Łazęcka,Ewa Szczurek*

Main category: cs.LG

TLDR: FACTM是一种新型的多视图、多结构贝叶斯模型，结合了因子分析和相关主题建模，用于处理结构化数据模态。


<details>
  <summary>Details</summary>
Motivation: 传统因子分析（FA）不适用于结构化数据模态（如文本或单细胞测序数据），因此需要一种新方法来整合这些数据模态并揭示共享的变异轴。

Method: FACTM结合了因子分析和相关主题建模，并通过变分推断进行优化，同时引入了一种旋转潜在因子的方法以提高可解释性。

Result: 在文本、视频基准以及真实世界的音乐和COVID-19数据集上，FACTM在识别结构化数据中的聚类和整合简单模态方面优于其他方法。

Conclusion: FACTM能够有效整合结构化数据模态，并通过推断共享的可解释因子提升分析效果。

Abstract: Integrating various data modalities brings valuable insights into underlying
phenomena. Multimodal factor analysis (FA) uncovers shared axes of variation
underlying different simple data modalities, where each sample is represented
by a vector of features. However, FA is not suited for structured data
modalities, such as text or single cell sequencing data, where multiple data
points are measured per each sample and exhibit a clustering structure. To
overcome this challenge, we introduce FACTM, a novel, multi-view and
multi-structure Bayesian model that combines FA with correlated topic modeling
and is optimized using variational inference. Additionally, we introduce a
method for rotating latent factors to enhance interpretability with respect to
binary features. On text and video benchmarks as well as real-world music and
COVID-19 datasets, we demonstrate that FACTM outperforms other methods in
identifying clusters in structured data, and integrating them with simple
modalities via the inference of shared, interpretable factors.

</details>

### [83] [Revisiting Transformers through the Lens of Low Entropy and Dynamic Sparsity](https://arxiv.org/abs/2504.18929)
*Ruifeng Ren,Yong Liu*

Main category: cs.LG

TLDR: 本文探讨了Transformer在数据压缩中的独特归纳偏好，发现其倾向于学习低熵分布，且模型规模越大越明显。FFN模块是驱动这一偏好的关键。此外，模型参数中存在冗余性，可通过动态稀疏性表征。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer在压缩任务中的表现，特别是其如何逼近目标分布以及信息压缩的机制，以解决目标分布未知和熵计算复杂度高的问题。

Method: 在受控实验环境下分析Transformer的压缩行为，重点关注其熵偏好和动态稀疏性。

Result: Transformer倾向于学习低熵分布，且FFN模块是这一偏好的关键驱动因素。模型参数中存在冗余性，动态稀疏性模式在注意力与FFN模块中表现不同。

Conclusion: 研究从熵和动态稀疏性角度深化了对Transformer的理解，揭示了其压缩行为的独特偏好及潜在机制。

Abstract: Compression has been a critical lens to understand the success of
Transformers. In the past, we have typically taken the target distribution as a
criterion to evaluate a model's compression performance. Nevertheless,it often
remains challenging to precisely assess how well the model achieves compression
and to compare the information content of the learned distribution with that of
the target distribution during compression,as the target distribution is
typically unknown and entropy computation often incurs exponential cost. In
this work, we explore these issues under a controlled experimental setup. We
find that Transformers exhibit a unique inductive bias in data compression:
beyond approaching the target distribution, they tend to favor learning
lower-entropy distributions, with this tendency becoming more pronounced as the
model size increases. This preference prevents Transformers from perfectly
aligning with the target distribution, instead further compressing its
information content. Furthermore, we show that the FFN module plays a critical
role in driving this bias. In addition, while models remove informational
redundancy from data during compression, they also exhibit redundancy within
their parameters, which enables compression and can be characterized through
dynamic sparsity. However, the dynamic sparsity patterns in Transformers,
particularly in attention and FFN modules, demand further exploration. As for
this, we show that larger Transformers show stronger preferences for bypassing
attention computations via residual connections and have lower proportion of
active neurons. Interestingly, we also find that training instability in larger
models strongly correlates with sudden increases in dead neurons. Our work
contributes to a deeper understanding of Transformers from the lens of entropy
and dynamic sparsity.

</details>

### [84] [Unveiling and Mitigating Adversarial Vulnerabilities in Iterative Optimizers](https://arxiv.org/abs/2504.19000)
*Elad Sofer,Tomer Shaked,Caroline Chaux,Nir Shlezinger*

Main category: cs.LG

TLDR: 论文研究了非学习型迭代优化器对对抗样本的敏感性，发现其与机器学习模型类似，并提出了通过对抗训练增强鲁棒性的方法。


<details>
  <summary>Details</summary>
Motivation: 探讨非学习型决策规则（尤其是迭代优化器）是否也具有对抗样本敏感性，以填补现有研究的空白。

Method: 将迭代优化器视为机器学习模型进行分析，并通过对抗训练提升其鲁棒性。

Result: 证明了迭代优化器对对抗样本的敏感性，并通过对抗训练显著提高了其鲁棒性。

Conclusion: 非学习型迭代优化器与机器学习模型类似，具有对抗样本敏感性，但可通过对抗训练改善鲁棒性。

Abstract: Machine learning (ML) models are often sensitive to carefully crafted yet
seemingly unnoticeable perturbations. Such adversarial examples are considered
to be a property of ML models, often associated with their black-box operation
and sensitivity to features learned from data. This work examines the
adversarial sensitivity of non-learned decision rules, and particularly of
iterative optimizers. Our analysis is inspired by the recent developments in
deep unfolding, which cast such optimizers as ML models. We show that
non-learned iterative optimizers share the sensitivity to adversarial examples
of ML models, and that attacking iterative optimizers effectively alters the
optimization objective surface in a manner that modifies the minima sought. We
then leverage the ability to cast iteration-limited optimizers as ML models to
enhance robustness via adversarial training. For a class of proximal gradient
optimizers, we rigorously prove how their learning affects adversarial
sensitivity. We numerically back our findings, showing the vulnerability of
various optimizers, as well as the robustness induced by unfolding and
adversarial training.

</details>

### [85] [Deep Learning-Based Multi-Modal Fusion for Robust Robot Perception and Navigation](https://arxiv.org/abs/2504.19002)
*Delun Lai,Yeyubei Zhang,Yunchong Liu,Chaojie Li,Huadong Mo*

Main category: cs.LG

TLDR: 本文提出了一种基于深度学习的多模态融合架构，用于提升自主导航机器人在复杂环境中的感知能力。通过创新的特征提取模块、自适应融合策略和时间序列建模机制，系统有效整合了RGB图像和LiDAR数据。


<details>
  <summary>Details</summary>
Motivation: 提升自主导航机器人在复杂环境中的感知能力，解决多模态数据融合的挑战。

Method: 设计了轻量级特征提取网络、自适应加权跨模态融合策略，并引入时间序列信息建模。

Result: 在KITTI数据集上，导航和定位精度分别提高了3.5%和2.2%，同时保持实时性能。

Conclusion: 该工作为复杂环境中的自主机器人导航提供了一种新颖的解决方案。

Abstract: This paper introduces a novel deep learning-based multimodal fusion
architecture aimed at enhancing the perception capabilities of autonomous
navigation robots in complex environments. By utilizing innovative feature
extraction modules, adaptive fusion strategies, and time-series modeling
mechanisms, the system effectively integrates RGB images and LiDAR data. The
key contributions of this work are as follows: a. the design of a lightweight
feature extraction network to enhance feature representation; b. the
development of an adaptive weighted cross-modal fusion strategy to improve
system robustness; and c. the incorporation of time-series information modeling
to boost dynamic scene perception accuracy. Experimental results on the KITTI
dataset demonstrate that the proposed approach increases navigation and
positioning accuracy by 3.5% and 2.2%, respectively, while maintaining
real-time performance. This work provides a novel solution for autonomous robot
navigation in complex environments.

</details>

### [86] [\$PINN -- a Domain Decomposition Method for Bayesian Physics-Informed Neural Networks](https://arxiv.org/abs/2504.19013)
*Júlia Vicens Figueres,Juliette Vanderhaeghen,Federica Bragone,Kateryna Morozovska,Khemraj Shukla*

Main category: cs.LG

TLDR: 提出了一种基于贝叶斯框架的全局不确定性计算方法，结合局部BPINN和区域分解，用于解决PDE中的不确定性量化问题。


<details>
  <summary>Details</summary>
Motivation: 在大规模多尺度问题中，有效量化PDE中的认知和随机不确定性仍具挑战性。

Method: 通过结合局部贝叶斯物理信息神经网络（BPINN）和区域分解，利用通量连续性确保子域间解的连续性。

Result: 实验表明，该方法能高效恢复全局不确定性，且可并行计算子域不确定性，对噪声和不同域大小具有鲁棒性。

Conclusion: 提出的方法在PDE不确定性量化中表现出高效性和鲁棒性，适用于多种区域分解技术。

Abstract: Physics-Informed Neural Networks (PINNs) are a novel computational approach
for solving partial differential equations (PDEs) with noisy and sparse initial
and boundary data. Although, efficient quantification of epistemic and
aleatoric uncertainties in big multi-scale problems remains challenging. We
propose \$PINN a novel method of computing global uncertainty in PDEs using a
Bayesian framework, by combining local Bayesian Physics-Informed Neural
Networks (BPINN) with domain decomposition. The solution continuity across
subdomains is obtained by imposing the flux continuity across the interface of
neighboring subdomains. To demonstrate the effectiveness of \$PINN, we conduct
a series of computational experiments on PDEs in 1D and 2D spatial domains.
Although we have adopted conservative PINNs (cPINNs), the method can be
seamlessly extended to other domain decomposition techniques. The results infer
that the proposed method recovers the global uncertainty by computing the local
uncertainty exactly more efficiently as the uncertainty in each subdomain can
be computed concurrently. The robustness of \$PINN is verified by adding
uncorrelated random noise to the training data up to 15% and testing for
different domain sizes.

</details>

### [87] [Towards minimax optimal algorithms for Active Simple Hypothesis Testing](https://arxiv.org/abs/2504.19014)
*Sushant Vijayan*

Main category: cs.LG

TLDR: 本文研究了主动简单假设检验（ASHT）问题，提出了一种基于博弈论和偏微分方程（PDE）的近似最优算法，并通过Blackwell Approachability设计了一种更高效的计算方法。


<details>
  <summary>Details</summary>
Motivation: 解决ASHT问题中现有算法的计算复杂性和维度灾难问题。

Method: 利用博弈论和PDE工具提出近似最优算法，并通过Blackwell Approachability设计高效算法。

Result: 新算法在所有ASHT实例中优于静态算法，并在多个实例中观察到达到最优指数。

Conclusion: 提出的算法在计算效率和性能上优于现有方法，但未证明其最优性。

Abstract: We study the Active Simple Hypothesis Testing (ASHT) problem, a simpler
variant of the Fixed Budget Best Arm Identification problem. In this work, we
provide novel game theoretic formulation of the upper bounds of the ASHT
problem. This formulation allows us to leverage tools of differential games and
Partial Differential Equations (PDEs) to propose an approximately optimal
algorithm that is computationally tractable compared to prior work. However,
the optimal algorithm still suffers from a curse of dimensionality and instead
we use a novel link to Blackwell Approachability to propose an algorithm that
is far more efficient computationally. We show that this new algorithm,
although not proven to be optimal, is always better than static algorithms in
all instances of ASHT and is numerically observed to attain the optimal
exponent in various instances.

</details>

### [88] [Smooth Approximations of the Rounding Function](https://arxiv.org/abs/2504.19026)
*Stanislav Semenov*

Main category: cs.LG

TLDR: 提出了两种平滑逼近经典舍入函数的新方法，适用于可微分优化和机器学习应用。


<details>
  <summary>Details</summary>
Motivation: 为梯度优化和机器学习提供可微分的舍入函数替代方案。

Method: 基于局部化Sigmoid窗口函数和归一化加权Sigmoid导数密度的方法。

Result: 两种方法均能点态收敛到经典舍入函数，并实现平滑性与精度的可控权衡。

Conclusion: 这些方法为硬舍入提供了高效、可微分的替代方案。

Abstract: We propose novel smooth approximations to the classical rounding function,
suitable for differentiable optimization and machine learning applications. Our
constructions are based on two approaches: (1) localized sigmoid window
functions centered at each integer, and (2) normalized weighted sums of sigmoid
derivatives representing local densities. The first method approximates the
step-like behavior of rounding through differences of shifted sigmoids, while
the second method achieves smooth interpolation between integers via
density-based weighting. Both methods converge pointwise to the classical
rounding function as the sharpness parameter k tends to infinity, and allow
controlled trade-offs between smoothness and approximation accuracy. We
demonstrate that by restricting the summation to a small set of nearest
integers, the computational cost remains low without sacrificing precision.
These constructions provide fully differentiable alternatives to hard rounding,
which are valuable in contexts where gradient-based methods are essential.

</details>

### [89] [On learning functions over biological sequence space: relating Gaussian process priors, regularization, and gauge fixing](https://arxiv.org/abs/2504.19034)
*Samantha Petti,Carlos Martí-Gómez,Justin B. Kinney,Juannan Zhou,David M. McCandlish*

Main category: cs.LG

TLDR: 论文探讨了从生物序列到功能定量测量的映射问题，重点研究了预测性序列-功能映射的推断及其子序列贡献的分解方法。通过正则化回归和高斯过程，统一了序列-功能关系的推断与解释框架。


<details>
  <summary>Details</summary>
Motivation: 研究生物序列（DNA、RNA、蛋白质）到功能定量测量的映射问题，旨在推断预测性序列-功能映射并分解子序列贡献。

Method: 使用正则化回归和高斯过程方法，分析权重空间与函数空间的关系，构建与任意高斯过程先验对应的正则化器。

Result: 推导了高斯过程后验隐含的权重分布，并证明其对长序列的高效计算可行性。

Conclusion: 框架统一并扩展了序列-功能关系的推断与解释能力。

Abstract: Mappings from biological sequences (DNA, RNA, protein) to quantitative
measures of sequence functionality play an important role in contemporary
biology. We are interested in the related tasks of (i) inferring predictive
sequence-to-function maps and (ii) decomposing sequence-function maps to
elucidate the contributions of individual subsequences. Because each
sequence-function map can be written as a weighted sum over subsequences in
multiple ways, meaningfully interpreting these weights requires "gauge-fixing,"
i.e., defining a unique representation for each map. Recent work has
established that most existing gauge-fixed representations arise as the unique
solutions to $L_2$-regularized regression in an overparameterized "weight
space" where the choice of regularizer defines the gauge. Here, we establish
the relationship between regularized regression in overparameterized weight
space and Gaussian process approaches that operate in "function space," i.e.
the space of all real-valued functions on a finite set of sequences. We
disentangle how weight space regularizers both impose an implicit prior on the
learned function and restrict the optimal weights to a particular gauge. We
also show how to construct regularizers that correspond to arbitrary explicit
Gaussian process priors combined with a wide variety of gauges. Next, we derive
the distribution of gauge-fixed weights implied by the Gaussian process
posterior and demonstrate that even for long sequences this distribution can be
efficiently computed for product-kernel priors using a kernel trick. Finally,
we characterize the implicit function space priors associated with the most
common weight space regularizers. Overall, our framework unifies and extends
our ability to infer and interpret sequence-function relationships.

</details>

### [90] [TeleSparse: Practical Privacy-Preserving Verification of Deep Neural Networks](https://arxiv.org/abs/2504.19274)
*Mohammad M Maheri,Hamed Haddadi,Alex Davidson*

Main category: cs.LG

TLDR: TeleSparse是一种ZK-SNARK友好的后处理机制，通过稀疏化和神经传送技术，显著减少验证现代神经网络的计算开销，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 验证深度学习推理的完整性需要访问敏感数据或模型权重，而ZK-SNARKs可以避免这一问题，但计算开销大。

Method: TeleSparse通过稀疏化减少电路约束，并通过神经传送优化激活范围，降低查找表大小。

Result: TeleSparse将证明生成时间减少46%，内存使用减少67%，精度损失约1%。

Conclusion: TeleSparse为ZK友好的模型设计提供了新方向，推动了可扩展且高效的深度学习验证。

Abstract: Verification of the integrity of deep learning inference is crucial for
understanding whether a model is being applied correctly. However, such
verification typically requires access to model weights and (potentially
sensitive or private) training data. So-called Zero-knowledge Succinct
Non-Interactive Arguments of Knowledge (ZK-SNARKs) would appear to provide the
capability to verify model inference without access to such sensitive data.
However, applying ZK-SNARKs to modern neural networks, such as transformers and
large vision models, introduces significant computational overhead.
  We present TeleSparse, a ZK-friendly post-processing mechanisms to produce
practical solutions to this problem. TeleSparse tackles two fundamental
challenges inherent in applying ZK-SNARKs to modern neural networks: (1)
Reducing circuit constraints: Over-parameterized models result in numerous
constraints for ZK-SNARK verification, driving up memory and proof generation
costs. We address this by applying sparsification to neural network models,
enhancing proof efficiency without compromising accuracy or security. (2)
Minimizing the size of lookup tables required for non-linear functions, by
optimizing activation ranges through neural teleportation, a novel adaptation
for narrowing activation functions' range.
  TeleSparse reduces prover memory usage by 67% and proof generation time by
46% on the same model, with an accuracy trade-off of approximately 1%. We
implement our framework using the Halo2 proving system and demonstrate its
effectiveness across multiple architectures (Vision-transformer, ResNet,
MobileNet) and datasets (ImageNet,CIFAR-10,CIFAR-100). This work opens new
directions for ZK-friendly model design, moving toward scalable,
resource-efficient verifiable deep learning.

</details>

### [91] [Atlantes: A system of GPS transformers for global-scale real-time maritime intelligence](https://arxiv.org/abs/2504.19036)
*Henry Herzog,Joshua Hansen,Yawen Zhang,Patrick Beukema*

Main category: cs.LG

TLDR: Atlantes是一个基于深度学习的系统，首次实现全球范围内船舶行为的实时监测，支持高效治理和政策制定。


<details>
  <summary>Details</summary>
Motivation: 全球变暖和不合理海洋开发威胁沿海社区，需要准确、及时的海洋活动监测以支持治理和政策。

Method: 利用定制化的transformer模型处理大量船舶GPS数据，量化船舶行为。

Result: 系统低延迟、高性能，已在全球数百个组织中应用，支持实时决策和干预。

Conclusion: Atlantes为全球实时海洋监测提供了高效、经济的解决方案。

Abstract: Unsustainable exploitation of the oceans exacerbated by global warming is
threatening coastal communities worldwide. Accurate and timely monitoring of
maritime activity is an essential step to effective governance and to inform
future policy. In support of this complex global-scale effort, we built
Atlantes, a deep learning based system that provides the first-ever real-time
view of vessel behavior at global scale. Atlantes leverages a series of bespoke
transformers to distill a high volume, continuous stream of GPS messages
emitted by hundreds of thousands of vessels into easily quantifiable behaviors.
The combination of low latency and high performance enables operationally
relevant decision-making and successful interventions on the high seas where
illegal and exploitative activity is too common. Atlantes is already in use by
hundreds of organizations worldwide. Here we provide an overview of the model
and infrastructure that enables this system to function efficiently and
cost-effectively at global-scale and in real-time.

</details>

### [92] [Improved Molecular Generation through Attribute-Driven Integrative Embeddings and GAN Selectivity](https://arxiv.org/abs/2504.19040)
*Nandan Joshi,Erhan Guven*

Main category: cs.LG

TLDR: 本文提出了一种结合Transformer和改良GAN的方法，用于生成具有特定性质的分子。


<details>
  <summary>Details</summary>
Motivation: 药物发现和化学工程等领域对定制分子的需求推动了分子设计计算方法的发展。

Method: 使用基于Transformer的向量嵌入生成器和改良GAN，结合新型分子描述符（Morgan指纹和全局分子属性）生成分子。

Result: Transformer的转换准确率达94%，GAN成功生成了具有特定气味性质的分子。

Conclusion: 该方法展示了结合新型嵌入和改良GAN在分子设计中的潜力，为定制分子发现提供了高效工具。

Abstract: The growing demand for molecules with tailored properties in fields such as
drug discovery and chemical engineering has driven advancements in
computational methods for molecular design. Machine learning-based approaches
for de-novo molecular generation have recently garnered significant attention.
This paper introduces a transformer-based vector embedding generator combined
with a modified Generative Adversarial Network (GAN) to generate molecules with
desired properties. The embedding generator utilizes a novel molecular
descriptor, integrating Morgan fingerprints with global molecular attributes,
enabling the transformer to capture local functional groups and broader
molecular characteristics. Modifying the GAN generator loss function ensures
the generation of molecules with specific desired properties. The transformer
achieves a reconversion accuracy of 94% while translating molecular descriptors
back to SMILES strings, validating the utility of the proposed embeddings for
generative tasks. The approach is validated by generating novel odorant
molecules using a labeled dataset of odorant and non-odorant compounds. With
the modified range-loss function, the GAN exclusively generates odorant
molecules. This work underscores the potential of combining novel vector
embeddings with transformers and modified GAN architectures to accelerate the
discovery of tailored molecules, offering a robust tool for diverse molecular
design applications.

</details>

### [93] [Score-Debiased Kernel Density Estimation](https://arxiv.org/abs/2504.19084)
*Elliot L. Epstein,Rajat Dwaraknath,Thanawat Sornwanee,John Winnicki,Jerry Weihong Liu*

Main category: cs.LG

TLDR: 提出了一种基于分数函数去偏核密度估计（SD-KDE）的新方法，通过调整数据点和修改带宽显著降低误差。


<details>
  <summary>Details</summary>
Motivation: 传统核密度估计（KDE）存在偏差，希望通过分数函数校正提升精度。

Method: 利用分数函数对数据点进行单步调整，结合修改后的带宽进行KDE，以消除主要偏差。

Result: 在1D、2D和MNIST任务中，SD-KDE显著降低了均方积分误差，优于标准Silverman KDE。

Conclusion: 分数函数校正可有效提升非参数密度估计的精度。

Abstract: We propose a novel method for density estimation that leverages an estimated
score function to debias kernel density estimation (SD-KDE). In our approach,
each data point is adjusted by taking a single step along the score function
with a specific choice of step size, followed by standard KDE with a modified
bandwidth. The step size and modified bandwidth are chosen to remove the
leading order bias in the KDE. Our experiments on synthetic tasks in 1D, 2D and
on MNIST, demonstrate that our proposed SD-KDE method significantly reduces the
mean integrated squared error compared to the standard Silverman KDE, even with
noisy estimates in the score function. These results underscore the potential
of integrating score-based corrections into nonparametric density estimation.

</details>

### [94] [Harmonizing Generalization and Personalization in Ring-topology Decentralized Federated Learning](https://arxiv.org/abs/2504.19103)
*Shunxin Guo,Jiaqi Lv,Xin Geng*

Main category: cs.LG

TLDR: 论文提出了一种基于环状拓扑的分散式联邦学习框架（DRDFL），通过特征生成模型解决数据异构性问题，同时实现个性化和泛化能力的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统集中式联邦学习存在单点故障风险，而分散式联邦学习（RDFL）因点对点通信方式导致信息共享效率低，且数据异构性加剧了模型差异。

Method: 提出DRDFL框架，包含PersonaNet模块（学习本地数据的判别性特征）和Learngene模块（通过对抗分类器提取全局不变知识）。

Result: 实验表明，DRDFL在多种数据异构性场景下优于现有方法。

Conclusion: DRDFL通过结合个性化和共享知识提取，有效提升了分散式联邦学习的性能。

Abstract: We introduce Ring-topology Decentralized Federated Learning (RDFL) for
distributed model training, aiming to avoid the inherent risks of centralized
failure in server-based FL. However, RDFL faces the challenge of low
information-sharing efficiency due to the point-to-point communication manner
when handling inherent data heterogeneity. Existing studies to mitigate data
heterogeneity focus on personalized optimization of models, ignoring that the
lack of shared information constraints can lead to large differences among
models, weakening the benefits of collaborative learning. To tackle these
challenges, we propose a Divide-and-conquer RDFL framework (DRDFL) that uses a
feature generation model to extract personalized information and invariant
shared knowledge from the underlying data distribution, ensuring both effective
personalization and strong generalization. Specifically, we design a
\textit{PersonaNet} module that encourages class-specific feature
representations to follow a Gaussian mixture distribution, facilitating the
learning of discriminative latent representations tailored to local data
distributions. Meanwhile, the \textit{Learngene} module is introduced to
encapsulate shared knowledge through an adversarial classifier to align latent
representations and extract globally invariant information. Extensive
experiments demonstrate that DRDFL outperforms state-of-the-art methods in
various data heterogeneity settings.

</details>

### [95] [Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments](https://arxiv.org/abs/2504.19139)
*Yun Qu,Qi,Wang,Yixiu Mao,Yiqin Lv,Xiangyang Ji*

Main category: cs.LG

TLDR: 论文提出了一种名为PDTS的方法，用于高效且鲁棒的主动任务采样，以提升序列决策中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统风险规避策略（如条件风险价值原则）在任务优化中效率低下的问题，尤其是在需要大量评估的困难任务上。

Method: 将鲁棒主动任务采样的优化过程建模为马尔可夫决策过程，并提出PDTS方法，结合后验和多样性协同任务采样。

Result: 实验表明PDTS显著提升了零样本和少样本适应能力，并在某些场景下加速了学习过程。

Conclusion: PDTS是一种易于实现且高效的方法，适用于风险规避场景下的鲁棒序列决策。

Abstract: Task robust adaptation is a long-standing pursuit in sequential
decision-making. Some risk-averse strategies, e.g., the conditional
value-at-risk principle, are incorporated in domain randomization or meta
reinforcement learning to prioritize difficult tasks in optimization, which
demand costly intensive evaluations. The efficiency issue prompts the
development of robust active task sampling to train adaptive policies, where
risk-predictive models are used to surrogate policy evaluation. This work
characterizes the optimization pipeline of robust active task sampling as a
Markov decision process, posits theoretical and practical insights, and
constitutes robustness concepts in risk-averse scenarios. Importantly, we
propose an easy-to-implement method, referred to as Posterior and Diversity
Synergized Task Sampling (PDTS), to accommodate fast and robust sequential
decision-making. Extensive experiments show that PDTS unlocks the potential of
robust active task sampling, significantly improves the zero-shot and few-shot
adaptation robustness in challenging tasks, and even accelerates the learning
process under certain scenarios. Our project website is at
https://thu-rllab.github.io/PDTS_project_page.

</details>

### [96] [Reliable Thermal Monitoring of Electric Machines through Machine Learning](https://arxiv.org/abs/2504.19141)
*Panagiotis Kakosimos*

Main category: cs.LG

TLDR: 论文研究了人工智能技术在感应电机冷却效率监测中的应用，开发了三种机器学习模型，并通过实验数据验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着电动化趋势的增强，确保电机内部温度在安全范围内运行至关重要，传统建模方法复杂且依赖专家知识，而数据驱动的方法提供了新的可能性。

Method: 通过实验数据收集，开发了三种机器学习模型，并通过超参数搜索确定最优配置，使用多种指标评估模型性能。

Result: 三种模型在瞬态运行条件下均表现良好，证明了数据驱动方法在热管理中的潜力。

Conclusion: 数据驱动的人工智能技术能够有效监测感应电机的冷却效率，为热管理提供了新的解决方案。

Abstract: The electrification of powertrains is rising as the objective for a more
viable future is intensified. To ensure continuous and reliable operation
without undesirable malfunctions, it is essential to monitor the internal
temperatures of machines and keep them within safe operating limits.
Conventional modeling methods can be complex and usually require expert
knowledge. With the amount of data collected these days, it is possible to use
information models to assess thermal behaviors. This paper investigates
artificial intelligence techniques for monitoring the cooling efficiency of
induction machines. Experimental data was collected under specific operating
conditions, and three machine-learning models have been developed. The optimal
configuration for each approach was determined through rigorous hyperparameter
searches, and the models were evaluated using a variety of metrics. The three
solutions performed well in monitoring the condition of the machine even under
transient operation, highlighting the potential of data-driven methods in
improving the thermal management.

</details>

### [97] [Newton-Puiseux Analysis for Interpretability and Calibration of Complex-Valued Neural Networks](https://arxiv.org/abs/2504.19176)
*Piotr Migus*

Main category: cs.LG

TLDR: 提出了一种基于Newton-Puiseux框架的方法，用于分析复数神经网络（CVNNs）的决策表面，提供鲁棒性和置信度的闭式估计。


<details>
  <summary>Details</summary>
Motivation: 复数神经网络在相位敏感任务中表现优异，但其多层面决策表面难以用传统解释性和校准工具分析。

Method: 通过局部多项式拟合高不确定性输入，并解析分解为分数幂级数（Puiseux展开），提取主导系数和相位对齐曲率描述符。

Result: 在控制实验中，方法达到RMSE < 0.09，预测对抗翻转半径误差<10^-3；在MIT-BIH心律失常数据集中，校准误差从0.087降至0.034。

Conclusion: 该方法显著提升了CVNNs的解释性和校准能力，代码和预训练权重已开源。

Abstract: Complex-valued neural networks (CVNNs) excel where phase matters, yet their
multi-sheeted decision surfaces defy standard explainability and calibration
tools. We propose a \emph{Newton-Puiseux} framework that fits a local
polynomial surrogate to a high-uncertainty input and analytically decomposes
this surrogate into fractional-power series. The resulting Puiseux expansions,
dominant Puiseux coefficients, and phase-aligned curvature descriptors deliver
closed-form estimates of robustness and over-confidence that gradient - or
perturbation-based methods (saliency, LIME, SHAP) cannot provide. On a
controlled $\mathbb{C}^2$ helix the surrogate attains RMSE $< 0.09$ while
recovering the number of decision sheets; quartic coefficients predict
adversarial flip radii within $10^{-3}$. On the real-world MIT-BIH arrhythmia
corpus, Puiseux-guided, phase-aware temperature scaling lowers expected
calibration error from 0.087 to 0.034, contributing to the advancement of
CVNNs. Full code, pre-trained weights, and scripts are at
https://github.com/piotrmgs/puiseux-cvnn.

</details>

### [98] [Hierarchical Attention Generates Better Proofs](https://arxiv.org/abs/2504.19188)
*Jianlong Chen,Chao Li,Yang Yuan,Andrew C Yao*

Main category: cs.LG

TLDR: 论文提出了一种名为“分层注意力”的正则化方法，通过将大语言模型（LLMs）的注意力机制与数学推理结构对齐，提升了定理证明的成功率并降低了证明复杂度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在定理证明中表现良好，但其基于标记的处理方式未能充分捕捉数学证明的层次结构。

Method: 引入分层注意力方法，建立从基础元素到高级概念的五级层次结构，确保证明生成中的结构化信息流。

Result: 实验显示，该方法在miniF2F和ProofNet数据集上分别提高了2.05%和1.69%的证明成功率，同时降低了23.81%和16.50%的证明复杂度。

Conclusion: 分层注意力方法有效提升了LLMs在定理证明中的表现，代码已开源。

Abstract: Large language models (LLMs) have shown promise in formal theorem proving,
but their token-level processing often fails to capture the inherent
hierarchical nature of mathematical proofs. We introduce \textbf{Hierarchical
Attention}, a regularization method that aligns LLMs' attention mechanisms with
mathematical reasoning structures. Our approach establishes a five-level
hierarchy from foundational elements to high-level concepts, ensuring
structured information flow in proof generation. Experiments demonstrate that
our method improves proof success rates by 2.05\% on miniF2F and 1.69\% on
ProofNet while reducing proof complexity by 23.81\% and 16.50\% respectively.
The code is available at https://github.com/Car-pe/HAGBP.

</details>

### [99] [HetGL2R: Learning to Rank Critical Road Segments via Attributed Heterogeneous Graph Random Walks](https://arxiv.org/abs/2504.19199)
*Ming Xu,Jinrong Xiang,Zilong Xie,Xiangfu Meng*

Main category: cs.LG

TLDR: HetGL2R是一种基于属性异构图学习的道路网络节点重要性排序方法，通过整合OD需求、路径选择和结构特征，显著提升了排序准确性。


<details>
  <summary>Details</summary>
Motivation: 现有节点重要性排序方法主要依赖结构特征，忽略了OD需求和路径信息，导致排序准确性不足。

Method: 提出HetGL2R方法，构建三部分图（行程图）整合多种特征，设计嵌入方法（HetGWalk和Transformer编码器）学习节点表示，并采用列表排序策略。

Result: 实验表明HetGL2R在合成数据集和真实数据（北京出租车轨迹）上均优于基线方法，排序更准确。

Conclusion: HetGL2R通过整合多源信息，显著提升了道路网络节点重要性排序的准确性和实用性。

Abstract: Accurately identifying critical nodes with high spatial influence in road
networks is essential for enhancing the efficiency of traffic management and
urban planning. However, existing node importance ranking methods mainly rely
on structural features and topological information, often overlooking critical
factors such as origin-destination (OD) demand and route information. This
limitation leaves considerable room for improvement in ranking accuracy. To
address this issue, we propose HetGL2R, an attributed heterogeneous graph
learning approach for ranking node importance in road networks. This method
introduces a tripartite graph (trip graph) to model the structure of the road
network, integrating OD demand, route choice, and various structural features
of road segments. Based on the trip graph, we design an embedding method to
learn node representations that reflect the spatial influence of road segments.
The method consists of a heterogeneous random walk sampling algorithm
(HetGWalk) and a Transformer encoder. HetGWalk constructs multiple
attribute-guided graphs based on the trip graph to enrich the diversity of
semantic associations between nodes. It then applies a joint random walk
mechanism to convert both topological structures and node attributes into
sequences, enabling the encoder to capture spatial dependencies more
effectively among road segments. Finally, a listwise ranking strategy is
employed to evaluate node importance. To validate the performance of our
method, we construct two synthetic datasets using SUMO based on simulated road
networks. Experimental results demonstrate that HetGL2R significantly
outperforms baselines in incorporating OD demand and route choice information,
achieving more accurate and robust node ranking. Furthermore, we conduct a case
study using real-world taxi trajectory data from Beijing, further verifying the
practicality of the proposed method.

</details>

### [100] [Convergence Properties of Natural Gradient Descent for Minimizing KL Divergence](https://arxiv.org/abs/2504.19259)
*Adwait Datar,Nihat Ay*

Main category: cs.LG

TLDR: 论文研究了在概率单纯形上最小化KL散度的问题，分析了两种对偶坐标系下梯度下降算法的收敛行为，并与自然梯度下降（NGD）进行了比较。结果表明，NGD在离散时间下表现更优。


<details>
  <summary>Details</summary>
Motivation: KL散度在概率机器学习中作为损失函数的核心作用，以及优化参数化选择对收敛的影响。

Method: 在指数族和混合族坐标系下分析梯度下降（GD）和自然梯度下降（NGD）的收敛行为，并比较其性能。

Result: 连续时间下，GD在两种坐标系中的收敛率分别为NGD的下界和上界；离散时间下，NGD收敛更快且更鲁棒。

Conclusion: NGD在离散时间优化中具有优势，尽管在连续时间下其收敛率并非最优。

Abstract: The Kullback-Leibler (KL) divergence plays a central role in probabilistic
machine learning, where it commonly serves as the canonical loss function.
Optimization in such settings is often performed over the probability simplex,
where the choice of parameterization significantly impacts convergence. In this
work, we study the problem of minimizing the KL divergence and analyze the
behavior of gradient-based optimization algorithms under two dual coordinate
systems within the framework of information geometry$-$ the exponential family
($\theta$ coordinates) and the mixture family ($\eta$ coordinates). We compare
Euclidean gradient descent (GD) in these coordinates with the
coordinate-invariant natural gradient descent (NGD), where the natural gradient
is a Riemannian gradient that incorporates the intrinsic geometry of the
parameter space. In continuous time, we prove that the convergence rates of GD
in the $\theta$ and $\eta$ coordinates provide lower and upper bounds,
respectively, on the convergence rate of NGD. Moreover, under affine
reparameterizations of the dual coordinates, the convergence rates of GD in
$\eta$ and $\theta$ coordinates can be scaled to $2c$ and $\frac{2}{c}$,
respectively, for any $c>0$, while NGD maintains a fixed convergence rate of
$2$, remaining invariant to such transformations and sandwiched between them.
Although this suggests that NGD may not exhibit uniformly superior convergence
in continuous time, we demonstrate that its advantages become pronounced in
discrete time, where it achieves faster convergence and greater robustness to
noise, outperforming GD. Our analysis hinges on bounding the spectrum and
condition number of the Hessian of the KL divergence at the optimum, which
coincides with the Fisher information matrix.

</details>

### [101] [Anyprefer: An Agentic Framework for Preference Data Synthesis](https://arxiv.org/abs/2504.19276)
*Yiyang Zhou,Zhaoyang Wang,Tianle Wang,Shangyu Xing,Peng Xia,Bo Li,Kaiyuan Zheng,Zijian Zhang,Zhaorun Chen,Wenhao Zheng,Xuchao Zhang,Chetan Bansal,Weitong Zhang,Ying Wei,Mohit Bansal,Huaxiu Yao*

Main category: cs.LG

TLDR: Anyprefer框架通过两玩家马尔可夫游戏合成高质量偏好数据，显著提升模型对齐性能。


<details>
  <summary>Details</summary>
Motivation: 手动标注偏好数据耗时且昂贵，现有自奖励方法易放大模型偏见。

Method: Anyprefer采用目标模型与评判模型协作的两玩家游戏，引入外部工具减少偏见，优化提示反馈机制。

Result: 生成58K高质量偏好对数据集Anyprefer-V1，在21个数据集中平均提升性能（如自然语言生成18.55%）。

Conclusion: Anyprefer有效解决偏好数据质量问题，显著提升模型对齐效果。

Abstract: High-quality preference data is essential for aligning foundation models with
human values through preference learning. However, manual annotation of such
data is often time-consuming and costly. Recent methods often adopt a
self-rewarding approach, where the target model generates and annotates its own
preference data, but this can lead to inaccuracies since the reward model
shares weights with the target model, thereby amplifying inherent biases. To
address these issues, we propose Anyprefer, a framework designed to synthesize
high-quality preference data for aligning the target model. Anyprefer frames
the data synthesis process as a cooperative two-player Markov Game, where the
target model and the judge model collaborate together. Here, a series of
external tools are introduced to assist the judge model in accurately rewarding
the target model's responses, mitigating biases in the rewarding process. In
addition, a feedback mechanism is introduced to optimize prompts for both
models, enhancing collaboration and improving data quality. The synthesized
data is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K
high-quality preference pairs. Extensive experiments show that Anyprefer
significantly improves model alignment performance across four main
applications, covering 21 datasets, achieving average improvements of 18.55% in
five natural language generation datasets, 3.66% in nine vision-language
understanding datasets, 30.05% in three medical image analysis datasets, and
16.00% in four visuo-motor control tasks.

</details>

### [102] [Ethical Challenges of Using Artificial Intelligence in Judiciary](https://arxiv.org/abs/2504.19284)
*Angel Mary John,Aiswarya M. U.,Jerrin Thomas Panachakel*

Main category: cs.LG

TLDR: AI在司法系统中的潜力与伦理挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨AI如何提升司法效率，同时分析其带来的伦理问题。

Method: 分析AI在司法中的应用及其伦理挑战，并提出建议。

Result: AI能优化司法流程，但需解决伦理问题。

Conclusion: 需负责任地部署AI以平衡效率与伦理。

Abstract: Artificial intelligence (AI) has emerged as a ubiquitous concept in numerous
domains, including the legal system. AI has the potential to revolutionize the
functioning of the judiciary and the dispensation of justice. Incorporating AI
into the legal system offers the prospect of enhancing decision-making for
judges, lawyers, and legal professionals, while concurrently providing the
public with more streamlined, efficient, and cost-effective services. The
integration of AI into the legal landscape offers manifold benefits,
encompassing tasks such as document review, legal research, contract analysis,
case prediction, and decision-making. By automating laborious and error-prone
procedures, AI has the capacity to alleviate the burden associated with these
arduous tasks. Consequently, courts around the world have begun embracing AI
technology as a means to enhance the administration of justice. However,
alongside its potential advantages, the use of AI in the judiciary poses a
range of ethical challenges. These ethical quandaries must be duly addressed to
ensure the responsible and equitable deployment of AI systems. This article
delineates the principal ethical challenges entailed in employing AI within the
judiciary and provides recommendations to effectively address these issues.

</details>

### [103] [Flow Along the K-Amplitude for Generative Modeling](https://arxiv.org/abs/2504.19353)
*Weitao Du,Shuning Chang,Jiasheng Tang,Yu Rong,Fan Wang,Shengchao Liu*

Main category: cs.LG

TLDR: K-Flow是一种基于K-振幅分解的生成学习算法，通过控制不同尺度的信息实现可调控生成。


<details>
  <summary>Details</summary>
Motivation: 提出一种新的生成学习范式，通过K-振幅分解实现跨尺度流匹配，解决生成模型中的可控性问题。

Method: 利用K-振幅分解，将时间作为尺度参数，实现流匹配，并探讨其理论、能量动态及实际应用。

Result: 在无条件图像生成、类条件图像生成和分子组装生成任务中验证了K-Flow的有效性，并通过消融实验展示了其对图像生成分辨率的控制能力。

Conclusion: K-Flow通过K-振幅分解实现了可控生成，为生成模型提供了新的理论和实践工具。

Abstract: In this work, we propose a novel generative learning paradigm, K-Flow, an
algorithm that flows along the $K$-amplitude. Here, $k$ is a scaling parameter
that organizes frequency bands (or projected coefficients), and amplitude
describes the norm of such projected coefficients. By incorporating the
$K$-amplitude decomposition, K-Flow enables flow matching across the scaling
parameter as time. We discuss three venues and six properties of K-Flow, from
theoretical foundations, energy and temporal dynamics, and practical
applications, respectively. Specifically, from the practical usage perspective,
K-Flow allows steerable generation by controlling the information at different
scales. To demonstrate the effectiveness of K-Flow, we conduct experiments on
unconditional image generation, class-conditional image generation, and
molecule assembly generation. Additionally, we conduct three ablation studies
to demonstrate how K-Flow steers scaling parameter to effectively control the
resolution of image generation.

</details>

### [104] [Rethinking Label-specific Features for Label Distribution Learning](https://arxiv.org/abs/2504.19374)
*Suping Xu,Chuyi Dai,Lin Shang,Changbin Shao,Xibei Yang,Witold Pedrycz*

Main category: cs.LG

TLDR: 论文提出了一种改进的标签分布学习方法LDL-LIFT-SAP，通过引入结构锚点（SAPs）捕捉簇间交互，优化了标签特定特征（LSFs）的构建，提升了标签分布学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法LIFT在标签分布学习中存在局限性，主要关注簇内关系而忽略簇间交互，且仅依赖欧氏距离构建LSFs可能导致噪声和偏差。

Method: 提出LIFT-SAP策略，结合距离和方向信息构建LSFs；进一步提出LDL-LIFT-SAP算法，统一不同LSF空间的标签描述度。

Result: 在15个真实数据集上的实验表明，LIFT-SAP优于LIFT，LDL-LIFT-SAP优于其他七种算法。

Conclusion: LDL-LIFT-SAP通过整合多视角信息和簇间交互，显著提升了标签分布学习的性能和鲁棒性。

Abstract: Label distribution learning (LDL) is an emerging learning paradigm designed
to capture the relative importance of labels for each instance. Label-specific
features (LSFs), constructed by LIFT, have proven effective for learning tasks
with label ambiguity by leveraging clustering-based prototypes for each label
to re-characterize instances. However, directly introducing LIFT into LDL tasks
can be suboptimal, as the prototypes it collects primarily reflect
intra-cluster relationships while neglecting interactions among distinct
clusters. Additionally, constructing LSFs using multi-perspective information,
rather than relying solely on Euclidean distance, provides a more robust and
comprehensive representation of instances, mitigating noise and bias that may
arise from a single distance perspective. To address these limitations, we
introduce Structural Anchor Points (SAPs) to capture inter-cluster
interactions. This leads to a novel LSFs construction strategy, LIFT-SAP, which
enhances LIFT by integrating both distance and direction information of each
instance relative to SAPs. Furthermore, we propose a novel LDL algorithm, Label
Distribution Learning via Label-specifIc FeaTure with SAPs (LDL-LIFT-SAP),
which unifies multiple label description degrees predicted from different LSF
spaces into a cohesive label distribution. Extensive experiments on 15
real-world datasets demonstrate the effectiveness of LIFT-SAP over LIFT, as
well as the superiority of LDL-LIFT-SAP compared to seven other
well-established algorithms.

</details>

### [105] [$O(1/k)$ Finite-Time Bound for Non-Linear Two-Time-Scale Stochastic Approximation](https://arxiv.org/abs/2504.19375)
*Siddharth Chandak*

Main category: cs.LG

TLDR: 本文改进了非线性双时间尺度随机逼近算法的误差界，从之前的$O(1/k^{2/3})$提升到$O(1/k)$，适用于梯度下降-上升和双时间尺度拉格朗日优化等算法。


<details>
  <summary>Details</summary>
Motivation: 双时间尺度随机逼近算法在强化学习、优化和博弈控制中有广泛应用，但非线性收缩情况下的误差界仍有改进空间。

Method: 通过重写原始迭代为平均噪声序列，并采用基于归纳的方法证明迭代的期望有界。

Result: 获得了非线性双时间尺度随机逼近算法的改进误差界$O(1/k)$。

Conclusion: 本文方法为非线性双时间尺度算法提供了更优的理论保证，适用于多种实际应用。

Abstract: Two-time-scale stochastic approximation is an algorithm with coupled
iterations which has found broad applications in reinforcement learning,
optimization and game control. While several prior works have obtained a mean
square error bound of $O(1/k)$ for linear two-time-scale iterations, the best
known bound in the non-linear contractive setting has been $O(1/k^{2/3})$. In
this work, we obtain an improved bound of $O(1/k)$ for non-linear
two-time-scale stochastic approximation. Our result applies to algorithms such
as gradient descent-ascent and two-time-scale Lagrangian optimization. The key
step in our analysis involves rewriting the original iteration in terms of an
averaged noise sequence which decays sufficiently fast. Additionally, we use an
induction-based approach to show that the iterates are bounded in expectation.

</details>

### [106] [HyperController: A Hyperparameter Controller for Fast and Stable Training of Reinforcement Learning Neural Networks](https://arxiv.org/abs/2504.19382)
*Jonathan Gornet,Yiannis Kantaros,Bruno Sinopoli*

Main category: cs.LG

TLDR: HyperController是一种高效算法，用于优化强化学习神经网络的超参数，通过建模为线性高斯动态系统并利用卡尔曼滤波器实现快速优化。


<details>
  <summary>Details</summary>
Motivation: 提高强化学习神经网络的训练效率，快速优化超参数以加速训练和部署。

Method: 将超参数优化问题建模为线性高斯动态系统，利用卡尔曼滤波器学习目标函数的高效表示。

Result: 在五个OpenAI Gymnasium环境中的四个中，HyperController的中位奖励最高。

Conclusion: HyperController展示了高效稳定训练强化学习神经网络的潜力。

Abstract: We introduce Hyperparameter Controller (HyperController), a computationally
efficient algorithm for hyperparameter optimization during training of
reinforcement learning neural networks. HyperController optimizes
hyperparameters quickly while also maintaining improvement of the reinforcement
learning neural network, resulting in faster training and deployment. It
achieves this by modeling the hyperparameter optimization problem as an unknown
Linear Gaussian Dynamical System, which is a system with a state that linearly
changes. It then learns an efficient representation of the hyperparameter
objective function using the Kalman filter, which is the optimal one-step
predictor for a Linear Gaussian Dynamical System. To demonstrate the
performance of HyperController, it is applied as a hyperparameter optimizer
during training of reinforcement learning neural networks on a variety of
OpenAI Gymnasium environments. In four out of the five Gymnasium environments,
HyperController achieves highest median reward during evaluation compared to
other algorithms. The results exhibit the potential of HyperController for
efficient and stable training of reinforcement learning neural networks.

</details>

### [107] [Bi-directional Model Cascading with Proxy Confidence](https://arxiv.org/abs/2504.19391)
*David Warren,Mark Dras*

Main category: cs.LG

TLDR: 提出了一种双向延迟方法，通过同时考虑小模型和大模型的置信度，结合隐藏状态分析和代理模型，优化模型级联效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖小模型的有限置信度估计，而大模型的置信度对级联效果至关重要，因此需要一种更全面的方法。

Method: 使用隐藏状态分析改进小模型的后调用置信度，并结合微型代理模型估计大模型的前调用置信度。

Result: 在多项选择数据集上验证，减少了向高成本模型的延迟，性能优于基线方法。

Conclusion: 双向延迟方法通过综合置信度估计显著提升了模型级联的效率。

Abstract: Model Cascading, recently applied successfully to LLMs, is a simple but
powerful technique that improves the efficiency of inference by selectively
applying models of varying sizes. Models are used in sequence from smallest to
largest, only deferring samples to large, costly models when smaller models are
not sufficiently confident. Existing approaches to deferral use only limited
small model confidence estimates because of the inaccessibility of the large
model, although large model confidence is known to be important. We therefore
propose a bi-directional approach to deferral that considers the confidence of
small and large models in the cascade simultaneously through the use of a proxy
for the large model. This requires a richer representation of model confidence
to enable comparative calibration: we use an analysis of hidden states to
improve post-invocation confidence of the small model, which in itself improves
cascading results over prior approaches. We then combine this with a tiny proxy
model to estimate pre-invocation confidence of the large model. We examine the
proposed cascading system over challenging, multiple-choice datasets, finding
improvements over standard cascading baselines reflected in reductions in
deferrals to more costly models.

</details>

### [108] [Observational Learning with a Budget](https://arxiv.org/abs/2504.19396)
*Shuo Wu,Pawan Poojary,Randall Berry*

Main category: cs.LG

TLDR: 研究贝叶斯观察学习模型，提出两种最优预算分配策略以提高信号质量，最大化正确信息级联概率。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过预算分配提升信号质量，优化代理决策准确性。

Method: 建模贝叶斯观察学习，分析预算分配问题，提出两种最优策略。

Result: 至少一种策略能最大化正确信息级联的概率。

Conclusion: 预算分配策略可有效提升信息级联准确性。

Abstract: We consider a model of Bayesian observational learning in which a sequence of
agents receives a private signal about an underlying binary state of the world.
Each agent makes a decision based on its own signal and its observations of
previous agents. A central planner seeks to improve the accuracy of these
signals by allocating a limited budget to enhance signal quality across agents.
We formulate and analyze the budget allocation problem and propose two optimal
allocation strategies. At least one of these strategies is shown to maximize
the probability of achieving a correct information cascade.

</details>

### [109] [UNet with Axial Transformer : A Neural Weather Model for Precipitation Nowcasting](https://arxiv.org/abs/2504.19408)
*Maitreya Sonawane,Sumit Mamtani*

Main category: cs.LG

TLDR: 该论文提出了一种基于Transformer的深度学习方法，用于高精度、本地化的天气临近预报，替代传统数值模型。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气模型在局部风暴或快速演变天气事件（如雷暴）的预测上存在挑战，深度学习模型能提供快速、高分辨率的预测。

Method: 采用基于轴向注意力机制的Transformer模型，从时间序列数据中学习复杂模式，适用于单变量和多变量数据。

Result: 在给定数据集上，使用UNet与轴向Transformer的组合取得了PSNR=47.67、SSIM=0.9943的先进结果。

Conclusion: 该方法为天气临近预报提供了高效、通用的框架，展示了深度学习在此领域的潜力。

Abstract: Making accurate weather predictions can be particularly challenging for
localized storms or events that evolve on hourly timescales, such as
thunderstorms. Hence, our goal for the project was to model Weather Nowcasting
for making highly localized and accurate predictions that apply to the
immediate future replacing the current numerical weather models and data
assimilation systems with Deep Learning approaches. A significant advantage of
machine learning is that inference is computationally cheap given an
already-trained model, allowing forecasts that are nearly instantaneous and in
the native high resolution of the input data. In this work we developed a novel
method that employs Transformer-based machine learning models to forecast
precipitation. This approach works by leveraging axial attention mechanisms to
learn complex patterns and dynamics from time series frames. Moreover, it is a
generic framework and can be applied to univariate and multivariate time series
data, as well as time series embeddings data. This paper represents an initial
research on the dataset used in the domain of next frame prediciton, and hence,
we demonstrate state-of-the-art results in terms of metrices (PSNR = 47.67,
SSIM = 0.9943) used for the given dataset using UNet with Axial Transformer.

</details>

### [110] [Graph-based Semi-supervised and Unsupervised Methods for Local Clustering](https://arxiv.org/abs/2504.19419)
*Zhaiming Shen,Sung Ha Kang*

Main category: cs.LG

TLDR: 论文提出了一种半监督和无监督的局部聚类方法，通过随机采样和图扩散提取局部簇，并在低标签率下取得先进效果。


<details>
  <summary>Details</summary>
Motivation: 解决在大图中无需全局信息即可识别特定子结构的问题，尤其是在标签数据极少或无标签的情况下。

Method: 随机采样图，应用扩散提取局部簇，通过结果重叠找到每个簇，并建立节点共成员条件。

Result: 方法在低标签率下表现优异，实验验证了其先进性和正确性。

Conclusion: 提出的半监督和无监督局部聚类方法有效且高效，适用于标签稀缺的场景。

Abstract: Local clustering aims to identify specific substructures within a large graph
without requiring full knowledge of the entire graph. These substructures are
typically small compared to the overall graph, enabling the problem to be
approached by finding a sparse solution to a linear system associated with the
graph Laplacian. In this work, we first propose a method for identifying
specific local clusters when very few labeled data is given, which we term
semi-supervised local clustering. We then extend this approach to the
unsupervised setting when no prior information on labels is available. The
proposed methods involve randomly sampling the graph, applying diffusion
through local cluster extraction, then examining the overlap among the results
to find each cluster. We establish the co-membership conditions for any pair of
nodes and rigorously prove the correctness of our methods. Additionally, we
conduct extensive experiments to demonstrate that the proposed methods achieve
state-of-the-arts results in the low-label rates regime.

</details>

### [111] [Learning High-dimensional Gaussians from Censored Data](https://arxiv.org/abs/2504.19446)
*Arnab Bhattacharyya,Constantinos Daskalakis,Themis Gouleakis,Yuhao Wang*

Main category: cs.LG

TLDR: 论文提出了两种高维高斯数据缺失情况下的高效学习算法：自审查和线性阈值缺失模型。


<details>
  <summary>Details</summary>
Motivation: 解决高维高斯数据在非随机缺失（MNAR）情况下的分布学习问题，假设缺失模型已知。

Method: 针对自审查和线性阈值两种缺失模型，设计了基于多项式样本复杂度的算法。

Result: 在自审查模型中，算法能以多项式样本学习高斯分布；在线性阈值模型中，实现了高效的均值估计。

Conclusion: 算法在非随机缺失情况下高效，且对样本复杂度和缺失模式有较低要求。

Abstract: We provide efficient algorithms for the problem of distribution learning from
high-dimensional Gaussian data where in each sample, some of the variable
values are missing. We suppose that the variables are missing not at random
(MNAR). The missingness model, denoted by $S(y)$, is the function that maps any
point $y$ in $R^d$ to the subsets of its coordinates that are seen. In this
work, we assume that it is known. We study the following two settings:
  (i) Self-censoring: An observation $x$ is generated by first sampling the
true value $y$ from a $d$-dimensional Gaussian $N(\mu*, \Sigma*)$ with unknown
$\mu*$ and $\Sigma*$. For each coordinate $i$, there exists a set $S_i$
subseteq $R^d$ such that $x_i = y_i$ if and only if $y_i$ in $S_i$. Otherwise,
$x_i$ is missing and takes a generic value (e.g., "?"). We design an algorithm
that learns $N(\mu*, \Sigma*)$ up to total variation (TV) distance epsilon,
using $poly(d, 1/\epsilon)$ samples, assuming only that each pair of
coordinates is observed with sufficiently high probability.
  (ii) Linear thresholding: An observation $x$ is generated by first sampling
$y$ from a $d$-dimensional Gaussian $N(\mu*, \Sigma)$ with unknown $\mu*$ and
known $\Sigma$, and then applying the missingness model $S$ where $S(y) = {i in
[d] : v_i^T y <= b_i}$ for some $v_1, ..., v_d$ in $R^d$ and $b_1, ..., b_d$ in
$R$. We design an efficient mean estimation algorithm, assuming that none of
the possible missingness patterns is very rare conditioned on the values of the
observed coordinates and that any small subset of coordinates is observed with
sufficiently high probability.

</details>

### [112] [R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference](https://arxiv.org/abs/2504.19449)
*Zhenyu Zhang,Zechun Liu,Yuandong Tian,Harshit Khaitan,Zhangyang Wang,Steven Li*

Main category: cs.LG

TLDR: R-Sparse是一种无需训练的激活稀疏方法，可在高级LLMs中实现高稀疏度，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在边缘设备上推理时面临计算和内存挑战，现有激活稀疏方法对非ReLU激活函数或高稀疏度效果有限。

Method: 通过分析线性层中不同组件对输出的贡献，提出基于输入通道和权重奇异值的稀疏推理方法，无需预测活跃通道。

Result: 在Llama-2/3和Mistral模型上，R-Sparse在50%稀疏度下性能相当，端到端效率提升43%。

Conclusion: R-Sparse为LLMs的高效推理提供了一种无需训练的高稀疏解决方案。

Abstract: Large Language Models (LLMs), while demonstrating remarkable capabilities
across various applications, present significant challenges during inference
due to their substantial model size, especially when deployed on edge devices.
Activation sparsity offers a promising solution to reduce computation and
memory movement, enabling more efficient inference, particularly for
small-batch on-device applications. However, current approaches face
limitations with non-ReLU activation function, which are foundational to most
advanced LLMs, or require heavy continual training. Additionally, the
difficulty in predicting active channels and limited achievable sparsity ratios
constrain the effectiveness of activation sparsity-based methods. In this
paper, we introduce R-Sparse, a training-free activation sparsity approach
capable of achieving high sparsity levels in advanced LLMs. We conducted two
preliminary investigations into how different components contribute to the
output within a single linear layer and found two key observations: (i) the
non-sparse components of the input function can be regarded as a few bias
terms, and (ii) The full computation can be effectively approximated by an
appropriate combination of input channels and weight singular values. Building
on this, we replace the linear layers in LLMs with a rank-aware sparse
inference method that leverages the sparsity of input channels and singular
value components, eliminating the need for active channel prediction like the
output sparsity based approaches. Experiments on Llama-2/3 and Mistral models
across ten diverse tasks demonstrate that R-Sparse achieves comparable
performance at 50% model-level sparsity, resulting in a significant 43%
end-to-end efficient improvements with customized kernels.

</details>

### [113] [Geometry-Informed Neural Operator Transformer](https://arxiv.org/abs/2504.19452)
*Qibang Liu,Vincient Zhong,Hadi Meidani,Diab Abueidda,Seid Koric,Philippe Geubelle*

Main category: cs.LG

TLDR: GINOT是一种基于机器学习的替代模型，结合Transformer架构和神经算子框架，用于高效预测任意几何形状的偏微分方程解。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在重复求解偏微分方程时计算效率低，机器学习替代模型可显著提升效率。

Method: GINOT通过采样分组和注意力机制编码几何表面点云，结合Transformer架构实现几何不变性和鲁棒性。

Result: GINOT在多个复杂数据集上验证了高精度和强泛化能力。

Conclusion: GINOT为复杂几何问题提供了一种高效且通用的解决方案。

Abstract: Machine-learning-based surrogate models offer significant computational
efficiency and faster simulations compared to traditional numerical methods,
especially for problems requiring repeated evaluations of partial differential
equations. This work introduces the Geometry-Informed Neural Operator
Transformer (GINOT), which integrates the transformer architecture with the
neural operator framework to enable forward predictions for arbitrary
geometries. GINOT encodes the surface points cloud of a geometry using a
sampling and grouping mechanism combined with an attention mechanism, ensuring
invariance to point order and padding while maintaining robustness to
variations in point density. The geometry information is seamlessly integrated
with query points in the solution decoder through the attention mechanism. The
performance of GINOT is validated on multiple challenging datasets, showcasing
its high accuracy and strong generalization capabilities for complex and
arbitrary 2D and 3D geometries.

</details>

### [114] [Stability Enhancement in Reinforcement Learning via Adaptive Control Lyapunov Function](https://arxiv.org/abs/2504.19473)
*Donghe Chen,Han Wang,Lin Cheng,Shengping Gong*

Main category: cs.LG

TLDR: 论文提出了一种结合Soft Actor-Critic与Control Lyapunov Function的框架（SAC-CLF），以增强强化学习在控制任务中的安全性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在安全探索方面存在不足，可能导致系统故障，限制了其在现实世界中的应用。

Method: 通过任务特定的CLF设计、动态约束调整和改进控制输入平滑性，SAC-CLF框架提升了安全性和性能。

Result: 在非线性系统和卫星姿态控制实验中，SAC-CLF表现出优于现有方法的效果。

Conclusion: SAC-CLF为强化学习在现实世界中的安全应用提供了有效解决方案。

Abstract: Reinforcement Learning (RL) has shown promise in control tasks but faces
significant challenges in real-world applications, primarily due to the absence
of safety guarantees during the learning process. Existing methods often
struggle with ensuring safe exploration, leading to potential system failures
and restricting applications primarily to simulated environments. Traditional
approaches such as reward shaping and constrained policy optimization can fail
to guarantee safety during initial learning stages, while model-based methods
using Control Lyapunov Functions (CLFs) or Control Barrier Functions (CBFs) may
hinder efficient exploration and performance. To address these limitations,
this paper introduces Soft Actor-Critic with Control Lyapunov Function
(SAC-CLF), a framework that enhances stability and safety through three key
innovations: (1) a task-specific CLF design method for safe and optimal
performance; (2) dynamic adjustment of constraints to maintain robustness under
unmodeled dynamics; and (3) improved control input smoothness while ensuring
safety. Experimental results on a classical nonlinear system and satellite
attitude control demonstrate the effectiveness of SAC-CLF in overcoming the
shortcomings of existing methods.

</details>

### [115] [An Automated Reinforcement Learning Reward Design Framework with Large Language Model for Cooperative Platoon Coordination](https://arxiv.org/abs/2504.19480)
*Dixiao Wei,Peng Yi,Jinlong Lei,Yiguang Hong,Yuchuan Du*

Main category: cs.LG

TLDR: 论文提出了一种基于大语言模型（LLM）的自动奖励函数设计框架（PCRD），用于解决强化学习（RL）在车队协调问题中奖励函数设计的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于车队协调目标的多变性、决策问题的复杂性以及手动设计试错耗时，设计高性能奖励函数以指导RL训练仍具挑战性。

Method: 提出PCRD框架，通过LLM驱动的初始化和迭代优化自动生成奖励函数，包括分析初始化奖励（AIR）模块和进化模块。

Result: 在长江三角洲交通网络模拟的六种复杂场景中，PCRD生成的奖励函数使RL代理性能平均提升10%。

Conclusion: PCRD框架能有效自动化奖励函数设计，显著提升RL在车队协调问题中的性能。

Abstract: Reinforcement Learning (RL) has demonstrated excellent decision-making
potential in platoon coordination problems. However, due to the variability of
coordination goals, the complexity of the decision problem, and the
time-consumption of trial-and-error in manual design, finding a well
performance reward function to guide RL training to solve complex platoon
coordination problems remains challenging. In this paper, we formally define
the Platoon Coordination Reward Design Problem (PCRDP), extending the RL-based
cooperative platoon coordination problem to incorporate automated reward
function generation. To address PCRDP, we propose a Large Language Model
(LLM)-based Platoon coordination Reward Design (PCRD) framework, which
systematically automates reward function discovery through LLM-driven
initialization and iterative optimization. In this method, LLM first
initializes reward functions based on environment code and task requirements
with an Analysis and Initial Reward (AIR) module, and then iteratively
optimizes them based on training feedback with an evolutionary module. The AIR
module guides LLM to deepen their understanding of code and tasks through a
chain of thought, effectively mitigating hallucination risks in code
generation. The evolutionary module fine-tunes and reconstructs the reward
function, achieving a balance between exploration diversity and convergence
stability for training. To validate our approach, we establish six challenging
coordination scenarios with varying complexity levels within the Yangtze River
Delta transportation network simulation. Comparative experimental results
demonstrate that RL agents utilizing PCRD-generated reward functions
consistently outperform human-engineered reward functions, achieving an average
of 10\% higher performance metrics in all scenarios.

</details>

### [116] [Improving Reasoning Performance in Large Language Models via Representation Engineering](https://arxiv.org/abs/2504.19483)
*Bertram Højer,Oliver Jarvis,Stefan Heinrich*

Main category: cs.LG

TLDR: 通过表示工程方法，利用LLM的残差流激活生成控制向量，提升推理任务性能。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM推理能力是否与其他信息处理任务类似，并尝试通过干预提升其表现。

Method: 从LLM残差流中提取激活，生成控制向量，在推理时干预模型表示空间。

Result: 在多个推理任务中，控制向量显著提升了模型表现。

Conclusion: LLM的推理能力可通过简单干预调控，无需额外训练。

Abstract: Recent advancements in large language models (LLMs) have resulted in
increasingly anthropomorphic language concerning the ability of LLMs to reason.
Whether reasoning in LLMs should be understood to be inherently different is,
however, widely debated. We propose utilizing a representation engineering
approach wherein model activations are read from the residual stream of an LLM
when processing a reasoning task. The activations are used to derive a control
vector that is applied to the model as an inference-time intervention,
modulating the representational space of the model, to improve performance on
the specified task. We publish the code for deriving control vectors and
analyzing model representations. The method allows us to improve performance on
reasoning benchmarks and assess how control vectors influence the final logit
distribution of a model via metrics such as KL divergence and entropy. We apply
control vectors to Mistral-7B-Instruct and a range of Pythia models on an
inductive, a deductive and mathematical reasoning task. We show that an LLM
can, to a certain degree, be controlled to improve its perceived reasoning
ability by modulating activations. The intervention is dependent upon the
ability to reliably extract the model's typical state when correctly solving a
task. Our results suggest that reasoning performance can be modulated in the
same manner as other information-processing tasks performed by LLMs and
demonstrate that we are capable of improving performance on specific tasks via
a simple intervention on the residual stream with no additional training.

</details>

### [117] [DISCO: learning to DISCover an evolution Operator for multi-physics-agnostic prediction](https://arxiv.org/abs/2504.19496)
*Rudy Morel,Jiequn Han,Edouard Oyallon*

Main category: cs.LG

TLDR: DISCO模型通过超网络处理短轨迹生成小算子网络参数，用于预测动态系统的下一状态，表现优异且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 解决在未知时间偏微分方程（PDEs）下仅用短轨迹预测动态系统下一状态的问题。

Method: 使用超网络处理短轨迹生成小算子网络参数，通过时间积分预测下一状态。

Result: 在多样化物理数据集上预训练后表现优异，泛化能力强，下游任务微调后仍具竞争力。

Conclusion: DISCO框架在动态系统预测任务中高效且泛化能力强。

Abstract: We address the problem of predicting the next state of a dynamical system
governed by unknown temporal partial differential equations (PDEs) using only a
short trajectory. While standard transformers provide a natural black-box
solution to this task, the presence of a well-structured evolution operator in
the data suggests a more tailored and efficient approach. Specifically, when
the PDE is fully known, classical numerical solvers can evolve the state
accurately with only a few parameters. Building on this observation, we
introduce DISCO, a model that uses a large hypernetwork to process a short
trajectory and generate the parameters of a much smaller operator network,
which then predicts the next state through time integration. Our framework
decouples dynamics estimation (i.e., DISCovering an evolution operator from a
short trajectory) from state prediction (i.e., evolving this operator).
Experiments show that pretraining our model on diverse physics datasets
achieves state-of-the-art performance while requiring significantly fewer
epochs. Moreover, it generalizes well and remains competitive when fine-tuned
on downstream tasks.

</details>

### [118] [Identification and Estimation of Long-Term Treatment Effects with Monotone Missing](https://arxiv.org/abs/2504.19527)
*Qinwei Yang,Ruocheng Guo,Shasha Han,Peng Wu*

Main category: cs.LG

TLDR: 论文提出三种新方法（逆概率加权、序贯回归插补和序贯边际结构模型）估计长期治疗效果，并引入BalanceNet解决数据稀疏性问题。


<details>
  <summary>Details</summary>
Motivation: 长期治疗效果估计在多个领域有广泛应用，但现有研究很少关注单调缺失问题。

Method: 提出序贯缺失假设，并设计三种估计方法（逆概率加权、序贯回归插补、序贯边际结构模型），进一步提出BalanceNet解决数据稀疏性问题。

Result: 在两个基准数据集上的实验验证了方法的有效性。

Conclusion: 论文填补了单调缺失在长期治疗效果估计中的研究空白，提出的方法具有实际应用价值。

Abstract: Estimating long-term treatment effects has a wide range of applications in
various domains. A key feature in this context is that collecting long-term
outcomes typically involves a multi-stage process and is subject to monotone
missing, where individuals missing at an earlier stage remain missing at
subsequent stages. Despite its prevalence, monotone missing has been rarely
explored in previous studies on estimating long-term treatment effects. In this
paper, we address this gap by introducing the sequential missingness assumption
for identification. We propose three novel estimation methods, including
inverse probability weighting, sequential regression imputation, and sequential
marginal structural model (SeqMSM). Considering that the SeqMSM method may
suffer from high variance due to severe data sparsity caused by monotone
missing, we further propose a novel balancing-enhanced approach, BalanceNet, to
improve the stability and accuracy of the estimation methods. Extensive
experiments on two widely used benchmark datasets demonstrate the effectiveness
of our proposed methods.

</details>

### [119] [Euclidean Distance Matrix Completion via Asymmetric Projected Gradient Descent](https://arxiv.org/abs/2504.19530)
*Yicheng Li,Xinghua Sun*

Main category: cs.LG

TLDR: 论文提出了一种基于Burer-Monteiro分解的梯度型算法APGD，用于解决欧氏距离矩阵补全问题，并首次证明了其在特定条件下的全局收敛性和精确恢复能力。


<details>
  <summary>Details</summary>
Motivation: 解决欧氏距离矩阵补全问题，提出一种无需样本分割且具有全局收敛保证的算法。

Method: 使用APGD算法，基于Burer-Monteiro分解，通过新的上界替代随机图引理。

Result: 在丰富样本区域表现出精确线性收敛，但在样本有限时性能下降。

Conclusion: APGD的隐式正则化能力在样本有限时减弱，且稳定新梯度方向需要更多样本。

Abstract: This paper proposes and analyzes a gradient-type algorithm based on
Burer-Monteiro factorization, called the Asymmetric Projected Gradient Descent
(APGD), for reconstructing the point set configuration from partial Euclidean
distance measurements, known as the Euclidean Distance Matrix Completion (EDMC)
problem. By paralleling the incoherence matrix completion framework, we show
for the first time that global convergence guarantee with exact recovery of
this routine can be established given $\mathcal{O}(\mu^2 r^3 \kappa^2 n \log
n)$ Bernoulli random observations without any sample splitting. Unlike
leveraging the tangent space Restricted Isometry Property (RIP) and local
curvature of the low-rank embedding manifold in some very recent works, our
proof provides new upper bounds to replace the random graph lemma under EDMC
setting. The APGD works surprisingly well and numerical experiments demonstrate
exact linear convergence behavior in rich-sample regions yet deteriorates fast
when compared with the performance obtained by optimizing the s-stress
function, i.e., the standard but unexplained non-convex approach for EDMC, if
the sample size is limited. While virtually matching our theoretical
prediction, this unusual phenomenon might indicate that: (i) the power of
implicit regularization is weakened when specified in the APGD case; (ii) the
stabilization of such new gradient direction requires substantially more
samples than the information-theoretic limit would suggest.

</details>

### [120] [Towards Faster and More Compact Foundation Models for Molecular Property Prediction](https://arxiv.org/abs/2504.19538)
*Yasir Ghunaim,Andrés Villa,Gergo Ignacz,Gyorgy Szekely,Motasem Alfarra,Bernard Ghanem*

Main category: cs.LG

TLDR: 通过分析JMP模型的层贡献，发现后期交互块收益递减，提出剪枝策略以减少模型大小并保持性能。


<details>
  <summary>Details</summary>
Motivation: 尽管JMP模型在多领域任务中表现优异，但其在小规模到大规模分子数据集上的微调仍耗费大量时间和计算资源。

Method: 分析JMP模型的层贡献，提出剪枝策略以减少模型大小，并评估其对效率和准确性的影响。

Result: 移除两个交互块后，模型大小减少32%，推理吞吐量提升1.3倍，性能下降极小。

Conclusion: JMP-L模型存在过参数化问题，通过剪枝可开发更轻量、高效的变体，降低计算成本。

Abstract: Advancements in machine learning for molecular property prediction have
improved accuracy but at the expense of higher computational cost and longer
training times. Recently, the Joint Multi-domain Pre-training (JMP) foundation
model has demonstrated strong performance across various downstream tasks with
reduced training time over previous models. Despite JMP's advantages,
fine-tuning it on molecular datasets ranging from small-scale to large-scale
requires considerable time and computational resources. In this work, we
investigate strategies to enhance efficiency by reducing model size while
preserving performance. To better understand the model's efficiency, we analyze
the layer contributions of JMP and find that later interaction blocks provide
diminishing returns, suggesting an opportunity for model compression. We
explore block reduction strategies by pruning the pre-trained model and
evaluating its impact on efficiency and accuracy during fine-tuning. Our
analysis reveals that removing two interaction blocks results in a minimal
performance drop, reducing the model size by 32% while increasing inference
throughput by 1.3x. These results suggest that JMP-L is over-parameterized and
that a smaller, more efficient variant can achieve comparable performance with
lower computational cost. Our study provides insights for developing lighter,
faster, and more scalable foundation models for molecular and materials
discovery. The code is publicly available at:
https://github.com/Yasir-Ghunaim/efficient-jmp.

</details>

### [121] [Quantifying Memory Utilization with Effective State-Size](https://arxiv.org/abs/2504.19561)
*Rom N. Parnichkun,Neehal Tumma,Armin W. Thomas,Alessandro Moro,Qi An,Taiji Suzuki,Atsushi Yamashita,Michael Poli,Stefano Massaroli*

Main category: cs.LG

TLDR: 提出了一种名为“有效状态大小”（ESS）的量化指标，用于衡量序列模型的内存利用率，结合信号处理和控制理论，为模型设计提供可解释和可操作的指导。


<details>
  <summary>Details</summary>
Motivation: 随着序列模型设计空间的扩展，需要开发一个通用的架构分析框架，以量化模型如何利用过去信息生成未来输出。

Method: 从经典信号处理和控制理论中汲取灵感，提出ESS指标，适用于具有输入不变和输入变化线性算子的系统，涵盖注意力、卷积和循环等计算单元。

Result: ESS可用于改进初始化策略、设计新的正则化方法，并通过模型蒸馏提升性能效率；同时揭示了不同架构在利用内存时的差异。

Conclusion: ESS为理解内存利用动态提供了有价值的见解，有助于设计更高效和有效的序列模型。

Abstract: The need to develop a general framework for architecture analysis is becoming
increasingly important, given the expanding design space of sequence models. To
this end, we draw insights from classical signal processing and control theory,
to develop a quantitative measure of \textit{memory utilization}: the internal
mechanisms through which a model stores past information to produce future
outputs. This metric, which we call \textbf{\textit{effective state-size}}
(ESS), is tailored to the fundamental class of systems with
\textit{input-invariant} and \textit{input-varying linear operators},
encompassing a variety of computational units such as variants of attention,
convolutions, and recurrences. Unlike prior work on memory utilization, which
either relies on raw operator visualizations (e.g. attention maps), or simply
the total \textit{memory capacity} (i.e. cache size) of a model, our metrics
provide highly interpretable and actionable measurements. In particular, we
show how ESS can be leveraged to improve initialization strategies, inform
novel regularizers and advance the performance-efficiency frontier through
model distillation. Furthermore, we demonstrate that the effect of context
delimiters (such as end-of-speech tokens) on ESS highlights cross-architectural
differences in how large language models utilize their available memory to
recall information. Overall, we find that ESS provides valuable insights into
the dynamics that dictate memory utilization, enabling the design of more
efficient and effective sequence models.

</details>

### [122] [Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning](https://arxiv.org/abs/2504.19583)
*Hanlu Zhang,Yumeng Ma,Shuo Wang,Guiran Liu,Binrong Zhu*

Main category: cs.LG

TLDR: 提出了一种基于图谱分析的大语言模型参数协同优化算法，旨在提升微调效率和训练中的结构感知能力。


<details>
  <summary>Details</summary>
Motivation: 改进大语言模型的参数优化方法，结合结构信号处理，提升微调质量和训练稳定性。

Method: 将预训练模型参数视为图节点，构建加权图并应用拉普拉斯谱分解，设计联合损失函数和谱滤波机制。

Result: 在多任务评估中表现优异，有效减少参数扰动并提升微调质量，同时保持模型性能。

Conclusion: 该框架为大规模模型的参数高效训练提供了新方法，强调了结构信号处理在深度学习优化中的重要性。

Abstract: This paper proposes a parameter collaborative optimization algorithm for
large language models, enhanced with graph spectral analysis. The goal is to
improve both fine-tuning efficiency and structural awareness during training.
In the proposed method, the parameters of a pre-trained language model are
treated as nodes in a graph. A weighted graph is constructed, and Laplacian
spectral decomposition is applied to enable frequency-domain modeling and
structural representation of the parameter space. Based on this structure, a
joint loss function is designed. It combines the task loss with a spectral
regularization term to facilitate collaborative updates among parameters. In
addition, a spectral filtering mechanism is introduced during the optimization
phase. This mechanism adjusts gradients in a structure-aware manner, enhancing
the model's training stability and convergence behavior. The method is
evaluated on multiple tasks, including traditional fine-tuning comparisons,
few-shot generalization tests, and convergence speed analysis. In all settings,
the proposed approach demonstrates superior performance. The experimental
results confirm that the spectral collaborative optimization framework
effectively reduces parameter perturbations and improves fine-tuning quality
while preserving overall model performance. This work contributes significantly
to the field of artificial intelligence by advancing parameter-efficient
training methodologies for large-scale models, reinforcing the importance of
structural signal processing in deep learning optimization, and offering a
robust, generalizable framework for enhancing language model adaptability and
performance.

</details>

### [123] [Soft-Label Caching and Sharpening for Communication-Efficient Federated Distillation](https://arxiv.org/abs/2504.19602)
*Kitsuya Azuma,Takayuki Nishio,Yuichi Kitagawa,Wakako Nakano,Takahito Tanimura*

Main category: cs.LG

TLDR: SCARLET是一种新型联邦学习框架，通过同步软标签缓存和增强的熵减少聚合机制，显著减少通信开销并保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习因频繁参数共享导致高通信开销和模型异构性受限，而基于蒸馏的方法存在冗余传输问题。

Method: SCARLET结合同步软标签缓存和增强的熵减少聚合（Enhanced ERA）机制，减少冗余通信并适应非IID数据。

Result: 实验表明，SCARLET在通信成本上减少50%，同时在准确性和效率上优于现有方法。

Conclusion: SCARLET为联邦学习提供了一种高效、适应性强的解决方案，适用于多样化客户端场景。

Abstract: Federated Learning (FL) enables collaborative model training across
decentralized clients, enhancing privacy by keeping data local. Yet
conventional FL, relying on frequent parameter-sharing, suffers from high
communication overhead and limited model heterogeneity. Distillation-based FL
approaches address these issues by sharing predictions (soft-labels) instead,
but they often involve redundant transmissions across communication rounds,
reducing efficiency. We propose SCARLET, a novel framework integrating
synchronized soft-label caching and an enhanced Entropy Reduction Aggregation
(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing
cached soft-labels, achieving up to 50% reduction in communication costs
compared to existing methods while maintaining accuracy. Enhanced ERA can be
tuned to adapt to non-IID data variations, ensuring robust aggregation and
performance in diverse client scenarios. Experimental evaluations demonstrate
that SCARLET consistently outperforms state-of-the-art distillation-based FL
methods in terms of accuracy and communication efficiency. The implementation
of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.

</details>

### [124] [AI Alignment in Medical Imaging: Unveiling Hidden Biases Through Counterfactual Analysis](https://arxiv.org/abs/2504.19621)
*Haroui Ma,Francesco Quinzan,Theresa Willem,Stefan Bauer*

Main category: cs.LG

TLDR: 提出了一种新的统计框架，用于评估医学影像机器学习模型对敏感属性的依赖性，通过反事实不变性概念量化偏差，并结合条件潜在扩散模型与统计假设检验。


<details>
  <summary>Details</summary>
Motivation: 医学影像机器学习模型的诊断能力虽强，但易受偏差影响，可能损害泛化性能，需解决这一问题以提高AI在医疗中的安全性。

Method: 利用反事实不变性概念，结合条件潜在扩散模型与统计假设检验，无需直接访问反事实数据即可识别和量化偏差。

Result: 在合成数据集及真实医学影像数据集（如CheXpert和MIMIC-CXR）上验证，方法符合反事实公平原则，优于基线方法。

Conclusion: 该方法为医学影像机器学习模型提供了一种稳健的偏差评估工具，有助于提升AI在医疗中的泛化能力和安全性。

Abstract: Machine learning (ML) systems for medical imaging have demonstrated
remarkable diagnostic capabilities, but their susceptibility to biases poses
significant risks, since biases may negatively impact generalization
performance. In this paper, we introduce a novel statistical framework to
evaluate the dependency of medical imaging ML models on sensitive attributes,
such as demographics. Our method leverages the concept of counterfactual
invariance, measuring the extent to which a model's predictions remain
unchanged under hypothetical changes to sensitive attributes. We present a
practical algorithm that combines conditional latent diffusion models with
statistical hypothesis testing to identify and quantify such biases without
requiring direct access to counterfactual data. Through experiments on
synthetic datasets and large-scale real-world medical imaging datasets,
including \textsc{cheXpert} and MIMIC-CXR, we demonstrate that our approach
aligns closely with counterfactual fairness principles and outperforms standard
baselines. This work provides a robust tool to ensure that ML diagnostic
systems generalize well, e.g., across demographic groups, offering a critical
step towards AI safety in healthcare. Code:
https://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging.

</details>

### [125] [LODAP: On-Device Incremental Learning Via Lightweight Operations and Data Pruning](https://arxiv.org/abs/2504.19638)
*Biqing Duan,Qing Wang,Di Liu,Wei Zhou,Zhenli He,Shengfa Miao*

Main category: cs.LG

TLDR: LODAP是一种新的边缘系统设备端增量学习框架，通过高效增量模块（EIM）和数据剪枝策略，显著提升增量学习精度并降低模型复杂性和训练开销。


<details>
  <summary>Details</summary>
Motivation: 边缘设备难以与远程服务器通信进行密集计算学习，因此需要一种高效的设备端增量学习方法。

Method: 提出LODAP框架，核心是EIM模块，结合普通卷积和轻量操作（如适配器），并通过数据剪枝减少训练数据。

Result: 在CIFAR-100和Tiny-ImageNet数据集上，LODAP精度提升4.32%，模型复杂性降低50%。

Conclusion: LODAP适用于设备端机器学习，显著提升了增量学习的效率和精度。

Abstract: Incremental learning that learns new classes over time after the model's
deployment is becoming increasingly crucial, particularly for industrial edge
systems, where it is difficult to communicate with a remote server to conduct
computation-intensive learning. As more classes are expected to learn after
their execution for edge devices. In this paper, we propose LODAP, a new
on-device incremental learning framework for edge systems. The key part of
LODAP is a new module, namely Efficient Incremental Module (EIM). EIM is
composed of normal convolutions and lightweight operations. During incremental
learning, EIM exploits some lightweight operations, called adapters, to
effectively and efficiently learn features for new classes so that it can
improve the accuracy of incremental learning while reducing model complexity as
well as training overhead. The efficiency of LODAP is further enhanced by a
data pruning strategy that significantly reduces the training data, thereby
lowering the training overhead. We conducted extensive experiments on the
CIFAR-100 and Tiny- ImageNet datasets. Experimental results show that LODAP
improves the accuracy by up to 4.32\% over existing methods while reducing
around 50\% of model complexity. In addition, evaluations on real edge systems
demonstrate its applicability for on-device machine learning. The code is
available at https://github.com/duanbiqing/LODAP.

</details>

### [126] [A Unified Benchmark of Federated Learning with Kolmogorov-Arnold Networks for Medical Imaging](https://arxiv.org/abs/2504.19639)
*Youngjoon Lee,Jinu Gong,Joonhyuk Kang*

Main category: cs.LG

TLDR: 本文评估了Kolmogorov-Arnold Networks (KAN)在联邦学习(FL)中的表现，发现其优于传统MLP，并分析了关键超参数对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 研究如何在隐私敏感的医疗领域（如血液细胞分类）中，通过联邦学习实现高效且隐私保护的模型训练。

Method: 在六种先进的FL算法上比较KAN与传统MLP，并分析超参数（如网格大小和网络架构）对KAN性能的影响。

Result: KAN在联邦环境中表现优于MLP，且优化宽度和最小深度时性能最佳。

Conclusion: KAN是分布式医疗中隐私保护医学影像任务的有前途的替代方案。

Abstract: Federated Learning (FL) enables model training across decentralized devices
without sharing raw data, thereby preserving privacy in sensitive domains like
healthcare. In this paper, we evaluate Kolmogorov-Arnold Networks (KAN)
architectures against traditional MLP across six state-of-the-art FL algorithms
on a blood cell classification dataset. Notably, our experiments demonstrate
that KAN can effectively replace MLP in federated environments, achieving
superior performance with simpler architectures. Furthermore, we analyze the
impact of key hyperparameters-grid size and network architecture-on KAN
performance under varying degrees of Non-IID data distribution. Additionally,
our ablation studies reveal that optimizing KAN width while maintaining minimal
depth yields the best performance in federated settings. As a result, these
findings establish KAN as a promising alternative for privacy-preserving
medical imaging applications in distributed healthcare. To the best of our
knowledge, this is the first comprehensive benchmark of KAN in FL settings for
medical imaging task.

</details>

### [127] [Intelligent4DSE: Optimizing High-Level Synthesis Design Space Exploration with Graph Neural Networks and Large Language Models](https://arxiv.org/abs/2504.19649)
*Lei Xu,Shanshan Wang,Emmanuel Casseau,Chenglong Xiao*

Main category: cs.LG

TLDR: 论文提出CoGNNs-LLMEA框架，结合图神经网络和语言模型增强的进化算法，优化HLS设计空间探索，显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有HLS预测模型忽视任务特性，进化算法需专业知识设计操作符，限制了效率和适应性。

Method: 提出CoGNNs-LLMEA框架，利用图神经网络和语言模型增强的进化算法，直接从源代码中间表示预测QoR。

Result: CoGNNs在HLS后QoR预测中表现最佳，延迟和资源利用率预测误差分别降低2.8倍和3.4倍。

Conclusion: CoGNNs-LLMEA有效填补高级抽象与物理实现间的鸿沟，显著提升预测精度和适应性。

Abstract: High-level synthesis (HLS) design space exploration (DSE) is an optimization
process in electronic design automation (EDA) that systematically explores
high-level design configurations to achieve Pareto-optimal hardware
implementations balancing performance, area, and power (PPA). To optimize this
process, HLS prediction tasks often employ message-passing neural networks
(MPNNs), leveraging complex architectures to achieve high accuracy. These
predictors serve as evaluators in the DSE process, effectively bypassing the
time-consuming estimations traditionally required by HLS tools. However,
existing models often prioritize structural complexity and minimization of
training loss, overlooking task-specific characteristics. Additionally, while
evolutionary algorithms are widely used in DSE, they typically require
extensive domain-specific knowledge to design effective crossover and mutation
operators. To address these limitations, we propose CoGNNs-LLMEA, a framework
that integrates a graph neural network with task-adaptive message passing and a
large language model-enhanced evolutionary algorithm. As a predictive model,
CoGNNs directly leverages intermediate representations generated from source
code after compiler front-end processing, enabling prediction of quality of
results (QoR) without invoking HLS tools. Due to its strong adaptability to
tasks, CoGNNs can be tuned to predict post-HLS and post-implementation
outcomes, effectively bridging the gap between high-level abstractions and
physical implementation characteristics. CoGNNs achieves state-of-the-art
prediction accuracy in post-HLS QoR prediction, reducing mean prediction errors
by 2.8$\times$ for latency and 3.4$\times$ for resource utilization compared to
baseline models.

</details>

### [128] [Hardware/Software Co-Design of RISC-V Extensions for Accelerating Sparse DNNs on FPGAs](https://arxiv.org/abs/2504.19659)
*Muhammad Sabih,Abrarul Karim,Jakob Wittmann,Frank Hannig,Jürgen Teich*

Main category: cs.LG

TLDR: 论文提出了一种基于RISC-V扩展的硬件/软件协同设计方法，用于加速半结构化和非结构化稀疏的DNN模型，分别实现了3倍和4倍的加速，组合设计可达5倍加速。


<details>
  <summary>Details</summary>
Motivation: RISC-V的可定制性使其成为加速DNN的理想选择，但需要硬件/软件协同设计以充分利用其潜力。

Method: 提出两种RISC-V扩展：利用FPGA细粒度配置性编码稀疏信息的半结构化加速单元，以及基于非零权重的可变周期乘加单元的非结构化加速单元。

Result: 半结构化和非结构化加速分别实现3倍和4倍加速，组合设计可达5倍加速，且仅占用少量FPGA资源。

Conclusion: 该设计在小规模FPGA上也能有效加速DNN，适用于TinyML应用。

Abstract: The customizability of RISC-V makes it an attractive choice for accelerating
deep neural networks (DNNs). It can be achieved through instruction set
extensions and corresponding custom functional units. Yet, efficiently
exploiting these opportunities requires a hardware/software co-design approach
in which the DNN model, software, and hardware are designed together. In this
paper, we propose novel RISC-V extensions for accelerating DNN models
containing semi-structured and unstructured sparsity. While the idea of
accelerating structured and unstructured pruning is not new, our novel design
offers various advantages over other designs. To exploit semi-structured
sparsity, we take advantage of the fine-grained (bit-level) configurability of
FPGAs and suggest reserving a few bits in a block of DNN weights to encode the
information about sparsity in the succeeding blocks. The proposed custom
functional unit utilizes this information to skip computations. To exploit
unstructured sparsity, we propose a variable cycle sequential
multiply-and-accumulate unit that performs only as many multiplications as the
non-zero weights. Our implementation of unstructured and semi-structured
pruning accelerators can provide speedups of up to a factor of 3 and 4,
respectively. We then propose a combined design that can accelerate both types
of sparsities, providing speedups of up to a factor of 5. Our designs consume a
small amount of additional FPGA resources such that the resulting co-designs
enable the acceleration of DNNs even on small FPGAs. We benchmark our designs
on standard TinyML applications such as keyword spotting, image classification,
and person detection.

</details>

### [129] [A Tripartite Perspective on GraphRAG](https://arxiv.org/abs/2504.19667)
*Michael Banf,Johannes Kuhn*

Main category: cs.LG

TLDR: 论文提出了一种结合LLMs与三方知识图谱（Tripartite-GraphRAG）的新方法，以解决LLMs在知识密集型任务中的局限性，如幻觉、来源追踪和知识更新问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在需要事实准确性的领域（如工业自动化和医疗保健）表现不佳，主要问题包括幻觉、缺乏来源追踪和知识更新困难。

Method: 通过三方知识图谱表示，将领域特定对象与文本块中的相关部分连接起来，利用概念锚定的预分析构建知识图谱，并将其转化为LLM提示的无监督节点分类问题。

Result: 实验表明，该方法能优化LLM提示的信息密度、覆盖范围和排列，减少提示长度，从而降低成本并提高输出的可靠性和一致性。

Conclusion: Tripartite-GraphRAG方法为解决LLMs在知识密集型任务中的局限性提供了有效途径，尤其在医疗保健领域表现出潜力。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities across
various domains, yet they struggle with knowledge-intensive tasks in areas that
demand factual accuracy, e.g. industrial automation and healthcare. Key
limitations include their tendency to hallucinate, lack of source traceability
(provenance), and challenges in timely knowledge updates. Combining language
models with knowledge graphs (GraphRAG) offers promising avenues for overcoming
these deficits. However, a major challenge lies in creating such a knowledge
graph in the first place. Here, we propose a novel approach that combines LLMs
with a tripartite knowledge graph representation, which is constructed by
connecting complex, domain-specific objects via a curated ontology of
corresponding, domain-specific concepts to relevant sections within chunks of
text through a concept-anchored pre-analysis of source documents starting from
an initial lexical graph. As a consequence, our Tripartite-GraphRAG approach
implements: i) a concept-specific, information-preserving pre-compression of
textual chunks; ii) allows for the formation of a concept-specific relevance
estimation of embedding similarities grounded in statistics; and iii) avoids
common challenges w.r.t. continuous extendability, such as the need for entity
resolution and deduplication. By applying a transformation to the knowledge
graph, we formulate LLM prompt creation as an unsupervised node classification
problem, drawing on ideas from Markov Random Fields. We evaluate our approach
on a healthcare use case, involving multi-faceted analyses of patient anamneses
given a set of medical concepts as well as clinical literature. Experiments
indicate that it can optimize information density, coverage, and arrangement of
LLM prompts while reducing their lengths, which may lead to reduced costs and
more consistent and reliable LLM outputs.

</details>

### [130] [Graph Fourier Transformer with Structure-Frequency Information](https://arxiv.org/abs/2504.19740)
*Yonghui Zhai,Yang Zhang,Minghao Shang,Lihua Pang,Yaxin Ren*

Main category: cs.LG

TLDR: 本文提出Grafourierformer，通过结合图傅里叶变换与自注意力机制，优化图结构任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图变换器（GT）的自注意力机制忽略了图的泛化偏差，仅从结构角度补偿偏差，性能次优。

Method: 利用图拉普拉斯矩阵的特征值构建特征值矩阵掩码，结合逆傅里叶变换提取节点高低频特征，优化注意力机制。

Result: 在多个基准测试中，Grafourierformer在图分类和节点分类任务上优于GNN和GT模型。

Conclusion: Grafourierformer通过结合结构和频率信息，有效提升性能，实验验证了其必要性和有效性。

Abstract: Graph Transformers (GTs) have shown advantages in numerous graph structure
tasks but their self-attention mechanism ignores the generalization bias of
graphs, with existing methods mainly compensating for this bias from aspects
like position encoding, attention bias and relative distance yet still having
sub-optimal performance and being insufficient by only considering the
structural perspective of generalization bias. To address this, this paper
proposes Grafourierformer, which innovatively combines GT with inductive bias
containing Frequency-Structure information by applying Graph Fourier Transform
to the Attention Matrix: specifically, eigenvalues from the Graph Laplacian
matrix are used to construct an Eigenvalue matrix mask (reflecting node
positions and structural relationships with neighboring nodes to enable
consideration of node range structural characteristics and focus on local graph
details), and inverse Fourier transform is employed to extract node
high-frequency and low-frequency features, calculate low-frequency and
high-frequency energy, and construct a node frequency-energy matrix to filter
the eigenvalue matrix mask, allowing attention heads to incorporate both graph
structural information and node frequency information optimization, adaptively
distinguish global trends from local details, and effectively suppress
redundant information interference. Extensive experiments on various benchmarks
show Grafourierformer consistently outperforms GNN and GT-based models in graph
classification and node classification tasks, with ablation experiments further
validating the effectiveness and necessity of the method. Codes are available
at https://github.com/Arichibald/Grafourierformer.git

</details>

### [131] [FineQ: Software-Hardware Co-Design for Low-Bit Fine-Grained Mixed-Precision Quantization of LLMs](https://arxiv.org/abs/2504.19746)
*Xilong Xie,Liang Wang,Limin Xiao,Meng Han,Lin Sun,Shuai Zheng,Xiangrong Xu*

Main category: cs.LG

TLDR: FineQ是一种软硬件协同设计的低比特细粒度混合精度量化方法，用于减少大型语言模型的内存和计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 现有单精度量化方法在超低比特量化时精度显著下降，而混合精度量化方法因粗粒度分组导致内存开销大或精度低。

Method: FineQ将权重细粒度分簇并考虑异常值分布，提出簇内异常值保护机制和编码方案，并设计支持该算法的加速器。

Result: FineQ在接近平均比特宽度下实现了比现有混合精度量化算法更高的模型精度，加速器能效提升1.79倍，面积减少61.2%。

Conclusion: FineQ通过软硬件协同设计，有效平衡了模型精度与内存开销，为大型语言模型的量化提供了高效解决方案。

Abstract: Large language models (LLMs) have significantly advanced the natural language
processing paradigm but impose substantial demands on memory and computational
resources. Quantization is one of the most effective ways to reduce memory
consumption of LLMs. However, advanced single-precision quantization methods
experience significant accuracy degradation when quantizing to ultra-low bits.
Existing mixed-precision quantization methods are quantized by groups with
coarse granularity. Employing high precision for group data leads to
substantial memory overhead, whereas low precision severely impacts model
accuracy. To address this issue, we propose FineQ, software-hardware co-design
for low-bit fine-grained mixed-precision quantization of LLMs. First, FineQ
partitions the weights into finer-grained clusters and considers the
distribution of outliers within these clusters, thus achieving a balance
between model accuracy and memory overhead. Then, we propose an outlier
protection mechanism within clusters that uses 3 bits to represent outliers and
introduce an encoding scheme for index and data concatenation to enable aligned
memory access. Finally, we introduce an accelerator utilizing temporal coding
that effectively supports the quantization algorithm while simplifying the
multipliers in the systolic array. FineQ achieves higher model accuracy
compared to the SOTA mixed-precision quantization algorithm at a close average
bit-width. Meanwhile, the accelerator achieves up to 1.79x energy efficiency
and reduces the area of the systolic array by 61.2%.

</details>

### [132] [If Concept Bottlenecks are the Question, are Foundation Models the Answer?](https://arxiv.org/abs/2504.19774)
*Nicola Debole,Pietro Barbiero,Francesco Giannini,Andrea Passeggini,Stefano Teso,Emanuele Marconato*

Main category: cs.LG

TLDR: 论文研究了VLM-CBM架构中弱监督对概念质量的影响，发现其与专家标注存在差异，且概念准确性与质量相关性不强。


<details>
  <summary>Details</summary>
Motivation: 探讨使用基础模型弱监督替代专家标注对概念瓶颈模型（CBMs）概念质量的影响。

Method: 通过实证分析，使用多种指标评估VLM-CBMs学习的概念质量。

Result: VLM监督与专家标注存在显著差异，且概念准确性与质量无强相关性。

Conclusion: 弱监督在CBMs中的应用需谨慎，概念质量与准确性需分别评估。

Abstract: Concept Bottleneck Models (CBMs) are neural networks designed to conjoin high
performance with ante-hoc interpretability. CBMs work by first mapping inputs
(e.g., images) to high-level concepts (e.g., visible objects and their
properties) and then use these to solve a downstream task (e.g., tagging or
scoring an image) in an interpretable manner. Their performance and
interpretability, however, hinge on the quality of the concepts they learn. The
go-to strategy for ensuring good quality concepts is to leverage expert
annotations, which are expensive to collect and seldom available in
applications. Researchers have recently addressed this issue by introducing
"VLM-CBM" architectures that replace manual annotations with weak supervision
from foundation models. It is however unclear what is the impact of doing so on
the quality of the learned concepts. To answer this question, we put
state-of-the-art VLM-CBMs to the test, analyzing their learned concepts
empirically using a selection of significant metrics. Our results show that,
depending on the task, VLM supervision can sensibly differ from expert
annotations, and that concept accuracy and quality are not strongly correlated.
Our code is available at https://github.com/debryu/CQA.

</details>

### [133] [Learning Brenier Potentials with Convex Generative Adversarial Neural Networks](https://arxiv.org/abs/2504.19779)
*Claudia Drygala,Hanno Gottschalk,Thomas Kruse,Ségolène Martin,Annika Mütze*

Main category: cs.LG

TLDR: 论文研究了生成对抗网络（GANs）学习Brenier势的统计学习理论，提出了一种结合交叉熵损失和凸性惩罚的对抗训练方法，证明了学习过程的稳定性，并通过实验验证了理论预测。


<details>
  <summary>Details</summary>
Motivation: Brenier势在概率测度间的传输映射中具有重要作用，但其统计学习理论尚未充分研究。本文旨在填补这一空白，并开发一种能够学习Brenier势的生成对抗网络。

Method: 提出了一种基于ReCU网络（使用三次激活函数）的通用逼近理论，结合对抗训练方法，通过交叉熵损失和凸性惩罚项确保网络的严格凸性。

Result: 理论分析表明，在高惩罚参数下，网络能保持严格凸性，学习过程具有一致性。实验验证了网络在训练中自动学习凸性。

Conclusion: 本文提出的方法成功实现了Brenier势的学习，并通过理论和实验验证了其有效性，为生成对抗网络的进一步研究提供了新思路。

Abstract: Brenier proved that under certain conditions on a source and a target
probability measure there exists a strictly convex function such that its
gradient is a transport map from the source to the target distribution. This
function is called the Brenier potential. Furthermore, detailed information on
the H\"older regularity of the Brenier potential is available. In this work we
develop the statistical learning theory of generative adversarial neural
networks that learn the Brenier potential. As by the transformation of
densities formula, the density of the generated measure depends on the second
derivative of the Brenier potential, we develop the universal approximation
theory of ReCU networks with cubic activation $\mathtt{ReCU}(x)=\max\{0,x\}^3$
that combines the favorable approximation properties of H\"older functions with
a Lipschitz continuous density. In order to assure the convexity of such
general networks, we introduce an adversarial training procedure for a
potential function represented by the ReCU networks that combines the classical
discriminator cross entropy loss with a penalty term that enforces (strict)
convexity. We give a detailed decomposition of learning errors and show that
for a suitable high penalty parameter all networks chosen in the adversarial
min-max optimization problem are strictly convex. This is further exploited to
prove the consistency of the learning procedure for (slowly) expanding network
capacity. We also implement the described learning algorithm and apply it to a
number of standard test cases from Gaussian mixture to image data as target
distributions. As predicted in theory, we observe that the convexity loss
becomes inactive during the training process and the potentials represented by
the neural networks have learned convexity.

</details>

### [134] [Heterophily-informed Message Passing](https://arxiv.org/abs/2504.19785)
*Haishan Wang,Arno Solin,Vikas Garg*

Main category: cs.LG

TLDR: 提出一种调节消息聚合的新方法，缓解GNN因同质性假设导致的过平滑问题，保留信息的高低频成分。


<details>
  <summary>Details</summary>
Motivation: GNN因同质性假设易过平滑，需解决此问题以提升性能。

Method: 通过调节消息传递的类型和范围，仅依赖学习嵌入，无需辅助标签。

Result: 在多种数据集和GNN架构上表现提升，分子生成任务中性能显著改进。

Conclusion: 新方法有效缓解过平滑，适用于广泛任务，如生成建模。

Abstract: Graph neural networks (GNNs) are known to be vulnerable to oversmoothing due
to their implicit homophily assumption. We mitigate this problem with a novel
scheme that regulates the aggregation of messages, modulating the type and
extent of message passing locally thereby preserving both the low and
high-frequency components of information. Our approach relies solely on learnt
embeddings, obviating the need for auxiliary labels, thus extending the
benefits of heterophily-aware embeddings to broader applications, e.g.,
generative modelling. Our experiments, conducted across various data sets and
GNN architectures, demonstrate performance enhancements and reveal heterophily
patterns across standard classification benchmarks. Furthermore, application to
molecular generation showcases notable performance improvements on
chemoinformatics benchmarks.

</details>

### [135] [Contextures: The Mechanism of Representation Learning](https://arxiv.org/abs/2504.19792)
*Runtian Zhai*

Main category: cs.LG

TLDR: 该论文提出了“contexture理论”，用于数学化表征学习的机制，解释了预训练模型学习表征的原理及其对下游任务的适用性。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在实证上取得了显著成功，但其学习的具体表征及其对下游任务的有用性尚不明确。科学理解表征学习至关重要，尤其是在模型规模扩大带来的收益递减时，设计新的预训练方法对进一步进展至关重要。

Method: 通过统一的“contexture理论”框架分析表征学习方法，证明编码器若能最大化输入X与上下文变量A的关联信息（即学习“contexture”），则在该上下文兼容的任务上表现最优。

Result: 研究表明，上下文在X与A关联既不过强也不过弱时最有用，单纯增加模型规模收益递减，需改进上下文。论文还提出了SVME和KISE两种通用目标，并展示了如何混合多个上下文。

Conclusion: “contexture理论”为表征学习提供了统一框架，揭示了模型规模扩展的局限性，并强调了改进上下文的重要性。

Abstract: This dissertation establishes the contexture theory to mathematically
characterize the mechanism of representation learning, or pretraining. Despite
the remarkable empirical success of foundation models, it is not very clear
what representations they learn, and why these representations are useful for
various downstream tasks. A scientific understanding of representation learning
is critical, especially at this point when scaling up the model size is
producing diminishing returns, and designing new pretraining methods is
imperative for further progress.
  Prior work treated different representation learning methods quite
differently, whereas the contexture theory provides a unified framework for
analyzing these methods. The central argument is that a representation is
learned from the association between the input X and a context variable A. We
prove that if an encoder captures the maximum information of this association,
in which case we say that the encoder learns the contexture, then it will be
optimal on the class of tasks that are compatible with the context. We also
show that a context is the most useful when the association between X and A is
neither too strong nor too weak. The important implication of the contexture
theory is that increasing the model size alone will achieve diminishing
returns, and further advancements require better contexts.
  We demonstrate that many pretraining objectives can learn the contexture,
including supervised learning, self-supervised learning, generative models,
etc. Then, we introduce two general objectives -- SVME and KISE, for learning
the contexture. We also show how to mix multiple contexts together, an
effortless way to create better contexts from existing ones. Then, we prove
statistical learning bounds for representation learning. Finally, we discuss
the effect of the data distribution shift from pretraining to the downstream
task.

</details>

### [136] [Hierarchical Uncertainty-Aware Graph Neural Network](https://arxiv.org/abs/2504.19820)
*Yoonhyuk Choi,Chong-Kwon Kim*

Main category: cs.LG

TLDR: 提出了一种新型图神经网络架构HU-GNN，结合多尺度表示学习、不确定性估计和自监督嵌入多样性，提升鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索局部不确定性和图层次结构的协同整合，HU-GNN旨在填补这一空白。

Method: HU-GNN通过自适应节点聚类和多尺度不确定性估计，结合鲁棒消息传递机制和注意力加权，减少噪声和对抗扰动。

Result: 在标准基准测试中，HU-GNN实现了最先进的鲁棒性和可解释性。

Conclusion: HU-GNN为图神经网络提供了一种统一且高效的框架，具有理论和实践意义。

Abstract: Recent research on graph neural networks (GNNs) has explored mechanisms for
capturing local uncertainty and exploiting graph hierarchies to mitigate data
sparsity and leverage structural properties. However, the synergistic
integration of these two approaches remains underexplored. In this work, we
introduce a novel architecture, the Hierarchical Uncertainty-Aware Graph Neural
Network (HU-GNN), which unifies multi-scale representation learning, principled
uncertainty estimation, and self-supervised embedding diversity within a single
end-to-end framework. Specifically, HU-GNN adaptively forms node clusters and
estimates uncertainty at multiple structural scales from individual nodes to
higher levels. These uncertainty estimates guide a robust message-passing
mechanism and attention weighting, effectively mitigating noise and adversarial
perturbations while preserving predictive accuracy on both node- and
graph-level tasks. We also offer key theoretical contributions, including a
probabilistic formulation, rigorous uncertainty-calibration guarantees, and
formal robustness bounds. Finally, by incorporating recent advances in graph
contrastive learning, HU-GNN maintains diverse, structurally faithful
embeddings. Extensive experiments on standard benchmarks demonstrate that our
model achieves state-of-the-art robustness and interpretability.

</details>

### [137] [Mjölnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density](https://arxiv.org/abs/2504.19822)
*Minjong Cheon*

Main category: cs.LG

TLDR: Mj\"olnir是一种基于深度学习的全球闪电密度参数化框架，通过InceptionNeXt和SENet架构，结合多任务学习策略，准确预测闪电活动。


<details>
  <summary>Details</summary>
Motivation: 利用深度学习模拟复杂大气动力学，填补全球闪电参数化的空白。

Method: 基于ERA5大气预测因子和WWLLN观测数据，采用InceptionNeXt和SENet架构，结合多任务学习策略。

Result: 模型在全球闪电分布、季节变化和区域特征上表现优异，年平均值皮尔逊相关系数达0.96。

Conclusion: Mj\"olnir不仅是有效的闪电参数化工具，还为下一代AI地球系统模型提供了潜力。

Abstract: Recent advances in AI-based weather forecasting models, such as FourCastNet,
Pangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep
learning to emulate complex atmospheric dynamics. Building on this momentum, we
propose Mj\"olnir, a novel deep learning-based framework for global lightning
flash density parameterization. Trained on ERA5 atmospheric predictors and
World Wide Lightning Location Network (WWLLN) observations at a daily temporal
resolution and 1 degree spatial resolution, Mj\"olnir captures the nonlinear
mapping between large-scale environmental conditions and lightning activity.
The model architecture is based on the InceptionNeXt backbone with SENet, and a
multi-task learning strategy to simultaneously predict lightning occurrence and
magnitude. Extensive evaluations yield that Mollnir accurately reproduces the
global distribution, seasonal variability, and regional characteristics of
lightning activity, achieving a global Pearson correlation coefficient of 0.96
for annual mean fields. These results suggest that Mj\"olnir serves not only as
an effective data-driven global lightning parameterization but also as a
promising AI-based scheme for next-generation Earth system models (AI-ESMs).

</details>

### [138] [TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate](https://arxiv.org/abs/2504.19874)
*Amir Zandieh,Majid Daliri,Majid Hadian,Vahab Mirrokni*

Main category: cs.LG

TLDR: TurboQuant是一种新型向量量化方法，通过随机旋转和分阶段量化，实现了接近最优的失真率，并在实验中验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 现有向量量化方法无法同时优化均方误差和内积失真，TurboQuant旨在解决这一问题。

Method: 通过随机旋转输入向量诱导Beta分布，分阶段应用MSE量化和1-bit QJL变换，实现无偏内积量化。

Result: TurboQuant在KV缓存量化和最近邻搜索任务中表现优异，接近理论下界。

Conclusion: TurboQuant在失真率和效率上优于现有方法，适用于在线应用。

Abstract: Vector quantization, a problem rooted in Shannon's source coding theory, aims
to quantize high-dimensional Euclidean vectors while minimizing distortion in
their geometric structure. We propose TurboQuant to address both mean-squared
error (MSE) and inner product distortion, overcoming limitations of existing
methods that fail to achieve optimal distortion rates. Our data-oblivious
algorithms, suitable for online applications, achieve near-optimal distortion
rates (within a small constant factor) across all bit-widths and dimensions.
TurboQuant achieves this by randomly rotating input vectors, inducing a
concentrated Beta distribution on coordinates, and leveraging the
near-independence property of distinct coordinates in high dimensions to simply
apply optimal scalar quantizers per each coordinate. Recognizing that
MSE-optimal quantizers introduce bias in inner product estimation, we propose a
two-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL
(QJL) transform on the residual, resulting in an unbiased inner product
quantizer. We also provide a formal proof of the information-theoretic lower
bounds on best achievable distortion rate by any vector quantizer,
demonstrating that TurboQuant closely matches these bounds, differing only by a
small constant ($\approx 2.7$) factor. Experimental results validate our
theoretical findings, showing that for KV cache quantization, we achieve
absolute quality neutrality with 3.5 bits per channel and marginal quality
degradation with 2.5 bits per channel. Furthermore, in nearest neighbor search
tasks, our method outperforms existing product quantization techniques in
recall while reducing indexing time to virtually zero.

</details>

### [139] [Attention Mechanism, Max-Affine Partition, and Universal Approximation](https://arxiv.org/abs/2504.19901)
*Hude Liu,Jerry Yao-Chieh Hu,Zhao Song,Han Liu*

Main category: cs.LG

TLDR: 单层单头自注意力和交叉注意力机制具有通用逼近能力，仅需最小附加结构即可实现。


<details>
  <summary>Details</summary>
Motivation: 探索单头注意力机制在函数逼近中的潜力，证明其仅需简单结构即可实现通用逼近。

Method: 将单头注意力解释为输入域划分机制，通过设计注意力权重模仿目标函数。

Result: 证明了单层自注意力（带线性变换）可在$L_\infty$-范数下逼近任意连续函数，并在$L_p$-范数下逼近可积函数。交叉注意力也首次实现相同逼近能力。

Conclusion: 单头注意力机制在简单结构下具有强大的通用逼近能力，为注意力模型的理论基础提供了新视角。

Abstract: We establish the universal approximation capability of single-layer,
single-head self- and cross-attention mechanisms with minimal attached
structures. Our key insight is to interpret single-head attention as an input
domain-partition mechanism that assigns distinct values to subregions. This
allows us to engineer the attention weights such that this assignment imitates
the target function. Building on this, we prove that a single self-attention
layer, preceded by sum-of-linear transformations, is capable of approximating
any continuous function on a compact domain under the $L_\infty$-norm.
Furthermore, we extend this construction to approximate any Lebesgue integrable
function under $L_p$-norm for $1\leq p <\infty$. Lastly, we also extend our
techniques and show that, for the first time, single-head cross-attention
achieves the same universal approximation guarantees.

</details>

### [140] [Convergence Analysis of Asynchronous Federated Learning with Gradient Compression for Non-Convex Optimization](https://arxiv.org/abs/2504.19903)
*Diying Yang,Yingwei Hou,Danyang Xiao,Weigang Wu*

Main category: cs.LG

TLDR: 论文研究了异步联邦学习（FL）中梯度压缩和误差反馈（EF）的收敛行为，分析了不同框架下的表现，并证明了EF能有效减少梯度估计方差。


<details>
  <summary>Details</summary>
Motivation: 填补异步FL中梯度压缩和EF技术缺乏系统性研究的空白。

Method: 分析了三种框架：AsynFL（基础异步FL）、AsynFLC（带梯度压缩）和AsynFLC-EF（带EF），并提供了收敛性分析。

Result: EF能减少梯度估计方差，使AsynFLC-EF的收敛速度与AsynFL匹配；异步延迟会放大压缩带来的方差。

Conclusion: EF在异步FL中有效，但异步延迟和高数据异质性会阻碍收敛；实验结果支持理论分析。

Abstract: Gradient compression is an effective technique for reducing communication
costs in federated learning (FL), and error feedback (EF) is usually adopted to
remedy the compression errors. However, there remains a lack of systematic
study on these techniques in asynchronous FL. In this paper, we fill this gap
by analyzing the convergence behaviors of FL under different frameworks. We
firstly consider a basic asynchronous FL framework AsynFL, and provide an
improved convergence analysis that relies on fewer assumptions and yields a
superior convergence rate than prior studies. Then, we consider a variant
framework with gradient compression, AsynFLC. We show sufficient conditions for
its convergence to the optimum, indicating the interaction between asynchronous
delay and compression rate. Our analysis also demonstrates that asynchronous
delay amplifies the variance caused by compression, thereby hindering
convergence, and such an impact is exacerbated by high data heterogeneity.
Furthermore, we study the convergence of AsynFLC-EF, the framework that further
integrates EF. We prove that EF can effectively reduce the variance of gradient
estimation despite asynchronous delay, which enables AsynFLC-EF to match the
convergence rate of AsynFL. We also show that the impact of asynchronous delay
on EF is limited to slowing down the higher-order convergence term.
Experimental results substantiate our analytical findings very well.

</details>

### [141] [Robust Federated Personalised Mean Estimation for the Gaussian Mixture Model](https://arxiv.org/abs/2504.19955)
*Malhar A. Managoli,Vinod M. Prabhakaran,Suhas Diggavi*

Main category: cs.LG

TLDR: 本文探讨了联邦学习中异构数据个性化与鲁棒性的结合，提出了一种针对高斯混合模型的个性化均值估计算法，并给出了误差与损坏样本比例的线性关系。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中异构数据的个性化和鲁棒性分别受到关注，但两者的结合尚未充分研究。本文旨在解决这一问题。

Method: 提出了一种针对高斯混合模型的个性化均值估计算法，并分析了其性能。

Result: 算法误差与损坏样本比例几乎呈线性关系，且通过下界验证了这一行为的合理性。

Conclusion: 本文为联邦学习中个性化与鲁棒性的结合提供了理论支持，并展示了算法的有效性。

Abstract: Federated learning with heterogeneous data and personalization has received
significant recent attention. Separately, robustness to corrupted data in the
context of federated learning has also been studied. In this paper we explore
combining personalization for heterogeneous data with robustness, where a
constant fraction of the clients are corrupted. Motivated by this broad
problem, we formulate a simple instantiation which captures some of its
difficulty. We focus on the specific problem of personalized mean estimation
where the data is drawn from a Gaussian mixture model. We give an algorithm
whose error depends almost linearly on the ratio of corrupted to uncorrupted
samples, and show a lower bound with the same behavior, albeit with a gap of a
constant factor.

</details>

### [142] [Transfer Learning Under High-Dimensional Network Convolutional Regression Model](https://arxiv.org/abs/2504.19979)
*Liyuan Wang,Jiachen Chen,Kathryn L. Lunetta,Danyang Huang,Huimin Cheng,Debarghya Mukherjee*

Main category: cs.LG

TLDR: 论文提出了一种基于网络卷积回归（NCR）的高维迁移学习框架，用于处理网络数据中的依赖关系，并通过理论和实证验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在处理网络数据的依赖关系时存在挑战，尤其是在标记数据稀缺的情况下。

Method: 提出NCR模型，结合随机网络结构，设计了两步迁移学习算法和源检测机制。

Result: 理论分析表明迁移学习提高了收敛速度；实证结果显示在目标领域标记数据有限时预测准确性显著提升。

Conclusion: NCR框架有效解决了网络数据中的迁移学习问题，尤其在标记数据稀缺时表现优异。

Abstract: Transfer learning enhances model performance by utilizing knowledge from
related domains, particularly when labeled data is scarce. While existing
research addresses transfer learning under various distribution shifts in
independent settings, handling dependencies in networked data remains
challenging. To address this challenge, we propose a high-dimensional transfer
learning framework based on network convolutional regression (NCR), inspired by
the success of graph convolutional networks (GCNs). The NCR model incorporates
random network structure by allowing each node's response to depend on its
features and the aggregated features of its neighbors, capturing local
dependencies effectively. Our methodology includes a two-step transfer learning
algorithm that addresses domain shift between source and target networks, along
with a source detection mechanism to identify informative domains.
Theoretically, we analyze the lasso estimator in the context of a random graph
based on the Erdos-Renyi model assumption, demonstrating that transfer learning
improves convergence rates when informative sources are present. Empirical
evaluations, including simulations and a real-world application using Sina
Weibo data, demonstrate substantial improvements in prediction accuracy,
particularly when labeled data in the target domain is limited.

</details>

### [143] [Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets](https://arxiv.org/abs/2504.19981)
*Adam Younsi,Abdalgader Abubaker,Mohamed El Amine Seddik,Hakim Hacid,Salem Lahlou*

Main category: cs.LG

TLDR: 论文提出了一种结合过程奖励模型（PRM）和生成流网络（GFlowNets）的方法，以提升大型语言模型（LLMs）在数学推理中的准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在复杂数学领域中准确性和多样性不足的问题，避免依赖昂贵的人工标注。

Method: 使用蒙特卡洛树搜索和相似性数据增强技术训练PRM，并基于PRM调整GFlowNets，以生成多样且高质量的推理步骤。

Result: 在数学基准测试中显著提升了准确性和多样性（如MATH Level 5上Llama3.2-3B的准确率提升2.59%），并能泛化到未见数据集（SAT MATH上提升9.4%）。

Conclusion: PRM引导的GFlowNets为LLMs的数学推理提供了更稳健和通用的解决方案。

Abstract: Achieving both accuracy and diverse reasoning remains challenging for Large
Language Models (LLMs) in complex domains like mathematics. A key bottleneck is
evaluating intermediate reasoning steps to guide generation without costly
human annotations. To address this, we first introduce a novel Process Reward
Model (PRM) trained automatically using Monte Carlo Tree Search coupled with a
similarity-based data augmentation technique, effectively capturing step-level
reasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks
(GFlowNets) to operate at the reasoning step level. Unlike traditional
reinforcement learning focused on maximizing a single reward, GFlowNets
naturally sample diverse, high-quality solutions proportional to their rewards,
as measured by our PRM. Empirical evaluation shows strong improvements in both
accuracy and solution diversity on challenging mathematical benchmarks (e.g.,
+2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective
generalization to unseen datasets (+9.4% absolute on SAT MATH). Our work
demonstrates the potential of PRM-guided, step-level GFlowNets for developing
more robust and versatile mathematical reasoning in LLMs.

</details>

### [144] [Emergence and scaling laws in SGD learning of shallow neural networks](https://arxiv.org/abs/2504.19983)
*Yunwei Ren,Eshaan Nichani,Denny Wu,Jason D. Lee*

Main category: cs.LG

TLDR: 论文研究了在线随机梯度下降（SGD）在两层神经网络学习中的复杂性，特别是在高斯数据下，分析了信号方向恢复的过渡时间及损失函数的标度规律。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解SGD在广泛宽度（P≫1）和发散条件数情况下的动态行为，特别是在两层神经网络中信号方向恢复的精确时间。

Method: 方法包括对SGD动态的精确分析，通过训练学生网络最小化均方误差（MSE）目标，并识别信号方向恢复的过渡时间。

Result: 结果表明，虽然单个教师神经元的学习表现出突变过渡，但多个学习曲线的叠加导致累积目标的平滑标度规律。

Conclusion: 结论指出，在广泛宽度和发散条件数下，SGD动态可以精确描述，并揭示了损失函数的标度规律。

Abstract: We study the complexity of online stochastic gradient descent (SGD) for
learning a two-layer neural network with $P$ neurons on isotropic Gaussian
data: $f_*(\boldsymbol{x}) = \sum_{p=1}^P a_p\cdot
\sigma(\langle\boldsymbol{x},\boldsymbol{v}_p^*\rangle)$, $\boldsymbol{x} \sim
\mathcal{N}(0,\boldsymbol{I}_d)$, where the activation
$\sigma:\mathbb{R}\to\mathbb{R}$ is an even function with information exponent
$k_*>2$ (defined as the lowest degree in the Hermite expansion),
$\{\boldsymbol{v}^*_p\}_{p\in[P]}\subset \mathbb{R}^d$ are orthonormal signal
directions, and the non-negative second-layer coefficients satisfy $\sum_{p}
a_p^2=1$. We focus on the challenging ``extensive-width'' regime $P\gg 1$ and
permit diverging condition number in the second-layer, covering as a special
case the power-law scaling $a_p\asymp p^{-\beta}$ where
$\beta\in\mathbb{R}_{\ge 0}$. We provide a precise analysis of SGD dynamics for
the training of a student two-layer network to minimize the mean squared error
(MSE) objective, and explicitly identify sharp transition times to recover each
signal direction. In the power-law setting, we characterize scaling law
exponents for the MSE loss with respect to the number of training samples and
SGD steps, as well as the number of parameters in the student neural network.
Our analysis entails that while the learning of individual teacher neurons
exhibits abrupt transitions, the juxtaposition of $P\gg 1$ emergent learning
curves at different timescales leads to a smooth scaling law in the cumulative
objective.

</details>

### [145] [Modelling of Underwater Vehicles using Physics-Informed Neural Networks with Control](https://arxiv.org/abs/2504.20019)
*Abdelhakim Amer,David Felsager,Yury Brodskiy,Andriy Sarabakha*

Main category: cs.LG

TLDR: 本文介绍了PINC框架的开源实现，结合物理定律和数据驱动模型，用于水下车辆动力学建模，优于非物理基线。


<details>
  <summary>Details</summary>
Motivation: 提高水下车辆动力学建模的泛化能力和样本效率。

Method: 通过初始状态、控制动作和时间输入扩展PINNs，测试不同配置（损失函数、梯度加权方案和超参数）。

Result: 在模拟水下车辆上验证，PINC比非物理基线具有更准确的长时预测。

Conclusion: PINC框架有效整合物理定律和数据驱动模型，提升了预测性能。

Abstract: Physics-informed neural networks (PINNs) integrate physical laws with
data-driven models to improve generalization and sample efficiency. This work
introduces an open-source implementation of the Physics-Informed Neural Network
with Control (PINC) framework, designed to model the dynamics of an underwater
vehicle. Using initial states, control actions, and time inputs, PINC extends
PINNs to enable physically consistent transitions beyond the training domain.
Various PINC configurations are tested, including differing loss functions,
gradient-weighting schemes, and hyperparameters. Validation on a simulated
underwater vehicle demonstrates more accurate long-horizon predictions compared
to a non-physics-informed baseline

</details>

### [146] [Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models](https://arxiv.org/abs/2504.20020)
*Xin Wang,Haoyang Li,Zeyang Zhang,Haibo Chen,Wenwu Zhu*

Main category: cs.LG

TLDR: 论文提出了一种新型学习范式——模块化机器学习（MML），旨在通过分解大语言模型（LLMs）的复杂结构，提升其推理能力、减少幻觉，并增强公平性、安全性和透明度。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在推理、事实一致性和可解释性方面存在明显局限性，需要新的方法来改进这些问题。

Method: MML将LLMs分解为三个相互依赖的组件：模块化表示、模块化模型和模块化推理，结合解耦表示学习、神经架构搜索和神经符号学习等技术实现。

Result: MML能够明确LLMs的内部工作机制，支持灵活的任务自适应设计，并实现可解释的逻辑驱动决策过程。

Conclusion: MML范式与LLMs的结合有望弥合统计学习与形式推理之间的鸿沟，为构建鲁棒、适应性强且可信赖的AI系统铺平道路。

Abstract: Large language models (LLMs) have dramatically advanced machine learning
research including natural language processing, computer vision, data mining,
etc., yet they still exhibit critical limitations in reasoning, factual
consistency, and interpretability. In this paper, we introduce a novel learning
paradigm -- Modular Machine Learning (MML) -- as an essential approach toward
new-generation LLMs. MML decomposes the complex structure of LLMs into three
interdependent components: modular representation, modular model, and modular
reasoning, aiming to enhance LLMs' capability of counterfactual reasoning,
mitigating hallucinations, as well as promoting fairness, safety, and
transparency. Specifically, the proposed MML paradigm can: i) clarify the
internal working mechanism of LLMs through the disentanglement of semantic
components; ii) allow for flexible and task-adaptive model design; iii) enable
interpretable and logic-driven decision-making process. We present a feasible
implementation of MML-based LLMs via leveraging advanced techniques such as
disentangled representation learning, neural architecture search and
neuro-symbolic learning. We critically identify key challenges, such as the
integration of continuous neural and discrete symbolic processes, joint
optimization, and computational scalability, present promising future research
directions that deserve further exploration. Ultimately, the integration of the
MML paradigm with LLMs has the potential to bridge the gap between statistical
(deep) learning and formal (logical) reasoning, thereby paving the way for
robust, adaptable, and trustworthy AI systems across a wide range of real-world
applications.

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [147] [Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages](https://arxiv.org/abs/2504.18560)
*Alessio Buscemi,Cédric Lothritz,Sergio Morales,Marcos Gomez-Vazquez,Robert Clarisó,Jordi Cabot,German Castignani*

Main category: cs.CL

TLDR: MLA-BiTe框架通过多语言增强偏见测试，改进现有方法，支持跨语言全面评估LLMs的社会偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言处理中表现出色，但常延续训练数据中的社会偏见，需改进偏见评估方法。

Method: 引入MLA-BiTe框架，利用自动翻译和改写技术，在六种语言（含两种低资源语言）中测试四种先进LLMs的七类敏感偏见。

Result: MLA-BiTe有效支持多语言偏见测试，扩展了评估范围。

Conclusion: MLA-BiTe为多语言环境下LLMs的偏见评估提供了系统性解决方案。

Abstract: Large Language Models (LLMs) have exhibited impressive natural language
processing capabilities but often perpetuate social biases inherent in their
training data. To address this, we introduce MultiLingual Augmented Bias
Testing (MLA-BiTe), a framework that improves prior bias evaluation methods by
enabling systematic multilingual bias testing. MLA-BiTe leverages automated
translation and paraphrasing techniques to support comprehensive assessments
across diverse linguistic settings. In this study, we evaluate the
effectiveness of MLA-BiTe by testing four state-of-the-art LLMs in six
languages -- including two low-resource languages -- focusing on seven
sensitive categories of discrimination.

</details>

### [148] [Span-Level Hallucination Detection for LLM-Generated Answers](https://arxiv.org/abs/2504.18639)
*Passant Elchafei,Mervet Abu-Elkheir*

Main category: cs.CL

TLDR: 本文提出了一种基于语义角色标注（SRL）和文本蕴含模型的跨度级幻觉检测框架，用于检测LLM生成答案中的幻觉内容，并在Mu-SHROOM数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 检测LLM生成答案中的幻觉跨度对提高事实一致性至关重要。

Method: 通过SRL将答案分解为原子角色，与基于问题提示的参考上下文进行比较，使用DeBERTa模型评估语义对齐，并结合置信度分数检测幻觉跨度。

Result: 在Mu-SHROOM数据集上表现出竞争力，并通过GPT-4和LLaMA验证了幻觉跨度的准确性。

Conclusion: 该框架为LLM生成答案中的幻觉检测提供了有效方法。

Abstract: Detecting spans of hallucination in LLM-generated answers is crucial for
improving factual consistency. This paper presents a span-level hallucination
detection framework for the SemEval-2025 Shared Task, focusing on English and
Arabic texts. Our approach integrates Semantic Role Labeling (SRL) to decompose
the answer into atomic roles, which are then compared with a retrieved
reference context obtained via question-based LLM prompting. Using a
DeBERTa-based textual entailment model, we evaluate each role semantic
alignment with the retrieved context. The entailment scores are further refined
through token-level confidence measures derived from output logits, and the
combined scores are used to detect hallucinated spans. Experiments on the
Mu-SHROOM dataset demonstrate competitive performance. Additionally,
hallucinated spans have been verified through fact-checking by prompting GPT-4
and LLaMA. Our findings contribute to improving hallucination detection in
LLM-generated responses.

</details>

### [149] [Can Third-parties Read Our Emotions?](https://arxiv.org/abs/2504.18673)
*Jiayi Li,Yingfan Zhou,Pranav Narayanan Venkit,Halima Binte Islam,Sneha Arya,Shomir Wilson,Sarah Rajtmajer*

Main category: cs.CL

TLDR: 研究比较了第三方标注与作者自标注在情感识别任务中的差异，发现第三方标注（包括人类标注者和LLMs）在捕捉作者真实情感状态时存在显著局限性，但LLMs表现优于人类标注者。通过实验发现，标注者与作者的人口统计相似性可提升标注质量，而LLMs在加入作者人口统计信息后表现略有提升。


<details>
  <summary>Details</summary>
Motivation: 验证第三方标注是否能准确捕捉作者的真实情感状态，并探索提升标注质量的方法。

Method: 通过人类实验比较第三方标注与作者自标注的差异，并测试标注者与作者的人口统计相似性对标注质量的影响。

Result: 第三方标注在捕捉作者真实情感状态时存在显著局限性，但LLMs表现优于人类标注者。人口统计相似性可提升人类标注质量，LLMs在加入作者信息后表现略有提升。

Conclusion: 研究揭示了第三方标注的局限性，并提出了改进标注质量的框架，呼吁更精确的标注实践以准确建模作者的真实情感状态。

Abstract: Natural Language Processing tasks that aim to infer an author's private
states, e.g., emotions and opinions, from their written text, typically rely on
datasets annotated by third-party annotators. However, the assumption that
third-party annotators can accurately capture authors' private states remains
largely unexamined. In this study, we present human subjects experiments on
emotion recognition tasks that directly compare third-party annotations with
first-party (author-provided) emotion labels. Our findings reveal significant
limitations in third-party annotations-whether provided by human annotators or
large language models (LLMs)-in faithfully representing authors' private
states. However, LLMs outperform human annotators nearly across the board. We
further explore methods to improve third-party annotation quality. We find that
demographic similarity between first-party authors and third-party human
annotators enhances annotation performance. While incorporating first-party
demographic information into prompts leads to a marginal but statistically
significant improvement in LLMs' performance. We introduce a framework for
evaluating the limitations of third-party annotations and call for refined
annotation practices to accurately represent and model authors' private states.

</details>

### [150] [Spatial Speech Translation: Translating Across Space With Binaural Hearables](https://arxiv.org/abs/2504.18715)
*Tuochao Chen,Qirui Wang,Runlin He,Shyam Gollakota*

Main category: cs.CL

TLDR: 提出了一种新型空间语音翻译技术，通过智能耳机实时翻译多语言环境中的语音，同时保留说话者的方向感和声音特征。


<details>
  <summary>Details</summary>
Motivation: 解决在多语言嘈杂环境中实时翻译语音并保持空间感知的技术挑战。

Method: 结合盲源分离、定位、实时表达翻译和双耳渲染技术，实现在Apple M2芯片上的实时推理。

Result: 原型耳机在干扰环境下实现了高达22.01的BLEU分数，用户研究验证了其在实际环境中的有效性。

Conclusion: 首次将空间感知融入语音翻译，为未来技术发展奠定了基础。

Abstract: Imagine being in a crowded space where people speak a different language and
having hearables that transform the auditory space into your native language,
while preserving the spatial cues for all speakers. We introduce spatial speech
translation, a novel concept for hearables that translate speakers in the
wearer's environment, while maintaining the direction and unique voice
characteristics of each speaker in the binaural output. To achieve this, we
tackle several technical challenges spanning blind source separation,
localization, real-time expressive translation, and binaural rendering to
preserve the speaker directions in the translated audio, while achieving
real-time inference on the Apple M2 silicon. Our proof-of-concept evaluation
with a prototype binaural headset shows that, unlike existing models, which
fail in the presence of interference, we achieve a BLEU score of up to 22.01
when translating between languages, despite strong interference from other
speakers in the environment. User studies further confirm the system's
effectiveness in spatially rendering the translated speech in previously unseen
real-world reverberant environments. Taking a step back, this work marks the
first step towards integrating spatial perception into speech translation.

</details>

### [151] [Building UD Cairo for Old English in the Classroom](https://arxiv.org/abs/2504.18718)
*Lauren Levine,Junghyun Min,Amir Zeldes*

Main category: cs.CL

TLDR: 本文介绍了一个基于UD Cairo句子的古英语样本树库，通过LLM提示和真实古英语数据搜索收集数据，并由初学者标注后比较和调整。结果表明LLM输出需后编辑，初学者标注虽不完美但可共同完成任务。初步解析实验显示现代英语训练数据对古英语解析效果有限，但标注特征可提升性能。


<details>
  <summary>Details</summary>
Motivation: 为历史语言学课程创建古英语样本树库，探索LLM和初学者标注在古英语数据处理中的潜力。

Method: 结合LLM提示和真实古英语数据搜索收集20个句子，由初学者标注后比较和调整，并进行初步解析实验。

Result: LLM输出需后编辑以反映真实语法；初学者标注虽不完美但可共同完成任务；标注特征（如lemma）可提升解析性能。

Conclusion: LLM和初学者标注在古英语数据处理中具有潜力，但需进一步优化和探索。

Abstract: In this paper we present a sample treebank for Old English based on the UD
Cairo sentences, collected and annotated as part of a classroom curriculum in
Historical Linguistics. To collect the data, a sample of 20 sentences
illustrating a range of syntactic constructions in the world's languages, we
employ a combination of LLM prompting and searches in authentic Old English
data. For annotation we assigned sentences to multiple students with limited
prior exposure to UD, whose annotations we compare and adjudicate. Our results
suggest that while current LLM outputs in Old English do not reflect authentic
syntax, this can be mitigated by post-editing, and that although beginner
annotators do not possess enough background to complete the task perfectly,
taken together they can produce good results and learn from the experience. We
also conduct preliminary parsing experiments using Modern English training
data, and find that although performance on Old English is poor, parsing on
annotated features (lemma, hyperlemma, gloss) leads to improved performance.

</details>

### [152] [EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers](https://arxiv.org/abs/2504.18736)
*Jianyou Wang,Weili Cao,Kaicheng Wang,Xiaoyue Wang,Ashish Dalvi,Gino Prasad,Qishan Liang,Hsuan-lin Her,Ming Wang,Qin Yang,Gene W. Yeo,David E. Neal,Maxim Khan,Christopher D. Rosin,Ramamohan Paturi,Leon Bergen*

Main category: cs.CL

TLDR: 论文提出了一种自动化在生物医学论文中寻找与假设相关证据的任务，并开发了EvidenceBench基准和标注流程来评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究者在验证科学假设时需要找到相关证据，但目前缺乏有效的自动化方法。

Method: 通过假设生成和逐句标注生物医学论文的流程创建EvidenceBench，并验证其准确性。

Result: 现有语言模型和检索系统在任务中表现显著低于专家水平。

Conclusion: 论文提出了可扩展的标注流程，并发布了包含10万篇标注论文的EvidenceBench-100k数据集。

Abstract: We study the task of automatically finding evidence relevant to hypotheses in
biomedical papers. Finding relevant evidence is an important step when
researchers investigate scientific hypotheses. We introduce EvidenceBench to
measure models performance on this task, which is created by a novel pipeline
that consists of hypothesis generation and sentence-by-sentence annotation of
biomedical papers for relevant evidence, completely guided by and faithfully
following existing human experts judgment. We demonstrate the pipeline's
validity and accuracy with multiple sets of human-expert annotations. We
evaluated a diverse set of language models and retrieval systems on the
benchmark and found that model performances still fall significantly short of
the expert level on this task. To show the scalability of our proposed
pipeline, we create a larger EvidenceBench-100k with 107,461 fully annotated
papers with hypotheses to facilitate model training and development. Both
datasets are available at https://github.com/EvidenceBench/EvidenceBench

</details>

### [153] [SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning](https://arxiv.org/abs/2504.18762)
*Ojasw Upadhyay,Abishek Saravankumar,Ayman Ismail*

Main category: cs.CL

TLDR: SynLexLM是一种新颖的法律大语言模型预训练方法，通过课程学习和合成数据增强，解决了法律领域数据稀缺和模型适应性问题。


<details>
  <summary>Details</summary>
Motivation: 通用预训练模型难以捕捉法律领域的细微差别，且获取足够法律数据具有挑战性。

Method: 采用课程学习（从简单到复杂的法律文本和查询）和合成数据增强（如使用Gemini Pro生成QA对）。

Result: 在BigLaw-Bench和EUR-Lex-Sum等法律基准测试中表现优于传统模型和微调版本。

Conclusion: SynLexLM有望提升法律文档分析和研究工具的性能，推动法律AI的普及。

Abstract: Large Language Models (LLMs) are powerful but often require extensive
fine-tuning and large datasets for specialized domains like law.
General-purpose pre-training may not capture legal nuances, and acquiring
sufficient legal data is challenging. We introduce SynLexLM, a novel approach
to efficiently pre-train a legal LLM. Our method employs curriculum learning,
progressing from simple to complex legal texts and queries, combined with
synthetic data augmentation using models like Gemini Pro to address data
scarcity. We aim to achieve improved performance on legal benchmarks
(BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned
versions. Preliminary work involves generating synthetic QA pairs reflecting
legal reasoning. This work aims to enhance legal document analysis and research
tools, potentially democratizing access to advanced legal AI.

</details>

### [154] [Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation](https://arxiv.org/abs/2504.18805)
*Jong Inn Park,Maanas Taneja,Qianwen Wang,Dongyeop Kang*

Main category: cs.CL

TLDR: SciTalk是一个多LLM代理框架，通过迭代反馈机制生成科学准确的短视频，优于简单提示方法，但尚未达到人类创作者水平。


<details>
  <summary>Details</summary>
Motivation: 解决科学论文短视频生成中的内容复杂性和专家与读者之间的鸿沟问题，避免现有方法的准确性不足和视觉伪影。

Method: 提出SciTalk框架，结合文本、图表、视觉风格和头像，利用专业代理进行内容摘要、视觉场景规划及编辑，并通过迭代反馈优化生成。

Result: 实验表明，SciTalk在科学准确性和吸引力上优于简单提示方法，但质量仍不及人类创作者。

Conclusion: SciTalk为反馈驱动的视频生成提供了挑战和益处的见解，代码和数据将公开。

Abstract: Generating engaging, accurate short-form videos from scientific papers is
challenging due to content complexity and the gap between expert authors and
readers. Existing end-to-end methods often suffer from factual inaccuracies and
visual artifacts, limiting their utility for scientific dissemination. To
address these issues, we propose SciTalk, a novel multi-LLM agentic framework,
grounding videos in various sources, such as text, figures, visual styles, and
avatars. Inspired by content creators' workflows, SciTalk uses specialized
agents for content summarization, visual scene planning, and text and layout
editing, and incorporates an iterative feedback mechanism where video agents
simulate user roles to give feedback on generated videos from previous
iterations and refine generation prompts. Experimental evaluations show that
SciTalk outperforms simple prompting methods in generating scientifically
accurate and engaging content over the refined loop of video generation.
Although preliminary results are still not yet matching human creators'
quality, our framework provides valuable insights into the challenges and
benefits of feedback-driven video generation. Our code, data, and generated
videos will be publicly available.

</details>

### [155] [Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks](https://arxiv.org/abs/2504.18838)
*Yixin Cao,Shibo Hong,Xinze Li,Jiahao Ying,Yubo Ma,Haiyuan Liang,Yantao Liu,Zijun Yao,Xiaozhi Wang,Dan Huang,Wenxuan Zhang,Lifu Huang,Muhao Chen,Lei Hou,Qianru Sun,Xingjun Ma,Zuxuan Wu,Min-Yen Kan,David Lo,Qi Zhang,Heng Ji,Jing Jiang,Juanzi Li,Aixin Sun,Xuanjing Huang,Tat-Seng Chua,Yu-Gang Jiang*

Main category: cs.CL

TLDR: 该论文探讨了大型语言模型（LLMs）评估中的核心挑战，包括从任务特定到能力评估的转变，以及从手动到自动评估的转变，同时指出了评估泛化问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的快速发展，现有评估方法难以跟上其能力的扩展，需要重新审视评估框架。

Method: 分析了两种关键转变：能力评估和自动评估，并从方法、数据集、评估者和指标等角度探讨了核心挑战。

Result: 提出了评估泛化问题，并计划通过GitHub仓库持续更新以应对快速发展的领域。

Conclusion: 呼吁贡献者和合作者共同解决LLMs评估中的挑战，并保持评估方法的动态更新。

Abstract: Large Language Models (LLMs) are advancing at an amazing speed and have
become indispensable across academia, industry, and daily applications. To keep
pace with the status quo, this survey probes the core challenges that the rise
of LLMs poses for evaluation. We identify and analyze two pivotal transitions:
(i) from task-specific to capability-based evaluation, which reorganizes
benchmarks around core competencies such as knowledge, reasoning, instruction
following, multi-modal understanding, and safety; and (ii) from manual to
automated evaluation, encompassing dynamic dataset curation and
"LLM-as-a-judge" scoring.
  Yet, even with these transitions, a crucial obstacle persists: the evaluation
generalization issue. Bounded test sets cannot scale alongside models whose
abilities grow seemingly without limit. We will dissect this issue, along with
the core challenges of the above two transitions, from the perspectives of
methods, datasets, evaluators, and metrics. Due to the fast evolving of this
field, we will maintain a living GitHub repository (links are in each section)
to crowd-source updates and corrections, and warmly invite contributors and
collaborators.

</details>

### [156] [Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in Large Language Models with Self-Guided Reasoning](https://arxiv.org/abs/2504.18839)
*Abdellah Ghassel,Xianzhi Li,Xiaodan Zhu*

Main category: cs.CL

TLDR: 该论文提出了一种结合微调和高级提示策略的方法，用于检测和缓解LLM驱动的对话系统中的对话崩溃问题，显著提升了小模型的性能并降低了运营成本。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在许多对话任务中表现出色，但仍会产生不连贯或矛盾的响应（对话崩溃），影响用户信任。

Method: 结合微调（8B模型）和高级提示策略（如少样本学习、链式思维推理和类比提示），并设计实时部署架构。

Result: 在BETOLD数据集上实现了7%的准确率提升，性能超越现有专用分类器，同时降低了运营成本。

Conclusion: 该方法为高影响力领域提供了高效、可解释且可靠的对话AI解决方案。

Abstract: Large language models (LLMs) are rapidly changing various domains. However,
their capabilities in handling conversational breakdowns still require an
in-depth exploration. This paper addresses the challenge of detecting and
mitigating dialogue breakdowns within LLM-driven conversational systems. While
powerful models from OpenAI and Anthropic excel in many dialogue tasks, they
can still produce incoherent or contradictory responses, commonly referred to
as breakdowns, which undermine user trust. To tackle this, we propose an
approach that combines specialized fine-tuning with advanced prompting
strategies, including few-shot learning, chain-of-thought reasoning, and
analogical prompting. In particular, we fine-tune a small 8B model and
demonstrate its robust classification and calibration capabilities in English
and Japanese dialogue. We also validate its generalization on the BETOLD
dataset, achieving a 7\% accuracy improvement over its base model. Furthermore,
we introduce a real-time deployment architecture that selectively escalates
suspicious responses to more resource-intensive frontier models only when
breakdowns are detected, significantly cutting operational expenses and energy
consumption. Experimental results show our method surpasses prior
state-of-the-art specialized classifiers while also narrowing performance gaps
between smaller open-source models and large proprietary ones. Our approach
offers a scalable solution for robust conversational AI in high-impact domains
by combining efficiency, interpretability, and reliability.

</details>

### [157] [When2Call: When (not) to Call Tools](https://arxiv.org/abs/2504.18851)
*Hayley Ross,Ameya Sunil Mahabaleshwarkar,Yoshi Suhara*

Main category: cs.CL

TLDR: 论文提出了一个新基准When2Call，用于评估语言模型在工具调用决策上的表现，包括何时调用工具、何时提问或承认无法回答。现有模型在此基准上表现不佳，作者还开发了训练集和优化方法。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注工具调用的准确性，而忽略了模型在何时调用工具、提问或承认无法回答的决策能力。

Method: 开发了When2Call基准，评估工具调用决策；并设计了训练集和基于偏好的优化训练方法。

Result: 当前最先进的工具调用模型在When2Call上表现不佳，优化方法比传统微调效果更好。

Conclusion: When2Call基准和优化方法为提升语言模型的工具调用决策能力提供了重要工具和数据支持。

Abstract: Leveraging external tools is a key feature for modern Language Models (LMs)
to expand their capabilities and integrate them into existing systems. However,
existing benchmarks primarily focus on the accuracy of tool calling -- whether
the correct tool is called with the correct parameters -- and less on
evaluating when LMs should (not) call tools. We develop a new benchmark,
When2Call, which evaluates tool-calling decision-making: when to generate a
tool call, when to ask follow-up questions and when to admit the question can't
be answered with the tools provided. We find that state-of-the-art tool-calling
LMs show significant room for improvement on When2Call, indicating the
importance of this benchmark. We also develop a training set for When2Call and
leverage the multiple-choice nature of the benchmark to develop a preference
optimization training regime, which shows considerably more improvement than
traditional fine-tuning. We release the benchmark and training data as well as
evaluation scripts at https://github.com/NVIDIA/When2Call.

</details>

### [158] [Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation](https://arxiv.org/abs/2504.18857)
*Yi Lu,Wanxu Zhao,Xin Zhou,Chenxin An,Chenglong Wang,Shuo Li,Yuming Yang,Jun Zhao,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TLDR: 提出了一种无需训练的框架DPE，通过调整RoPE隐藏维度的位置嵌入，显著扩展LLMs的上下文窗口，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在处理超长上下文时的性能问题，避免昂贵的再训练开销。

Method: 通过分析RoPE的不同隐藏维度，检测关键维度并调整其位置索引，实现上下文窗口的扩展。

Result: DPE显著优于基线方法，支持128k tokens的上下文窗口，并在训练长度内提升模型性能。

Conclusion: DPE是一种高效、无需训练的方法，显著提升了LLMs的长上下文处理能力。

Abstract: Large Language Models (LLMs) often struggle to process and generate coherent
context when the number of input tokens exceeds the pre-trained length. Recent
advancements in long-context extension have significantly expanded the context
window of LLMs but require expensive overhead to train the large-scale models
with longer context. In this work, we propose Dimension-Wise Positional
Embeddings Manipulation (DPE), a training-free framework to extrapolate the
context window of LLMs by diving into RoPE's different hidden dimensions.
Instead of manipulating all dimensions equally, DPE detects the effective
length for every dimension and finds the key dimensions for context extension.
We reuse the original position indices with their embeddings from the
pre-trained model and manipulate the key dimensions' position indices to their
most effective lengths. In this way, DPE adjusts the pre-trained models with
minimal modifications while ensuring that each dimension reaches its optimal
state for extrapolation. DPE significantly surpasses well-known baselines such
as YaRN and Self-Extend. DPE enables Llama3-8k 8B to support context windows of
128k tokens without continual training and integrates seamlessly with Flash
Attention 2. In addition to its impressive extrapolation capability, DPE also
dramatically improves the models' performance within training length, such as
Llama3.1 70B, by over 18 points on popular long-context benchmarks RULER. When
compared with commercial models, Llama 3.1 70B with DPE even achieves better
performance than GPT-4-128K.

</details>

### [159] [Latent Adversarial Training Improves the Representation of Refusal](https://arxiv.org/abs/2504.18872)
*Alexandra Abbas,Nora Petrova,Helios Ael Lyons,Natalia Perez-Campanero*

Main category: cs.CL

TLDR: 研究发现，语言模型的拒绝行为主要编码在其潜在空间的单一方向上，容易受到针对性攻击。尽管潜在对抗训练（LAT）通过引入噪声试图提高鲁棒性，但其对拒绝行为编码的影响尚不明确。通过分析Llama 2 7B，发现LAT显著改变了拒绝行为的表示，使其集中在SVD的前两个成分中，提高了鲁棒性但也增加了对自身生成攻击向量的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 理解潜在对抗训练（LAT）如何影响语言模型拒绝行为的编码，以评估其有效性和局限性。

Method: 通过分析Llama 2 7B，比较LAT与监督安全微调（SSFT）和嵌入空间对抗训练（AT）对拒绝行为编码的影响，使用激活差异和奇异值分解（SVD）进行分析。

Result: LAT显著改变了拒绝行为的表示，集中在SVD的前两个成分中（解释75%的方差），提高了鲁棒性但对自身生成攻击向量更脆弱。

Conclusion: LAT的训练扰动使拒绝行为的表示更全面，既展示了其提升模型安全的潜力，也揭示了其可能的脆弱性。

Abstract: Recent work has shown that language models' refusal behavior is primarily
encoded in a single direction in their latent space, making it vulnerable to
targeted attacks. Although Latent Adversarial Training (LAT) attempts to
improve robustness by introducing noise during training, a key question
remains: How does this noise-based training affect the underlying
representation of refusal behavior? Understanding this encoding is crucial for
evaluating LAT's effectiveness and limitations, just as the discovery of linear
refusal directions revealed vulnerabilities in traditional supervised safety
fine-tuning (SSFT).
  Through the analysis of Llama 2 7B, we examine how LAT reorganizes the
refusal behavior in the model's latent space compared to SSFT and embedding
space adversarial training (AT). By computing activation differences between
harmful and harmless instruction pairs and applying Singular Value
Decomposition (SVD), we find that LAT significantly alters the refusal
representation, concentrating it in the first two SVD components which explain
approximately 75 percent of the activation differences variance - significantly
higher than in reference models. This concentrated representation leads to more
effective and transferable refusal vectors for ablation attacks: LAT models
show improved robustness when attacked with vectors from reference models but
become more vulnerable to self-generated vectors compared to SSFT and AT. Our
findings suggest that LAT's training perturbations enable a more comprehensive
representation of refusal behavior, highlighting both its potential strengths
and vulnerabilities for improving model safety.

</details>

### [160] [A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification](https://arxiv.org/abs/2504.18884)
*Junichiro Niimi*

Main category: cs.CL

TLDR: 本文提出了一种基于大语言模型（LLMs）的集成策略，用于情感分析，通过多次推理的集成方法提高了结果的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有文献忽视了LLMs结果的变异性和可重复性问题，而实际人类标注通常通过多数投票解决分歧。

Method: 采用中等规模LLMs的多次推理集成策略。

Result: 集成方法比单次使用大型模型更鲁棒和准确，RMSE降低了18.6%。

Conclusion: 集成策略能有效提升LLMs在情感分析中的表现。

Abstract: With the advance of large language models (LLMs), LLMs have been utilized for
the various tasks. However, the issues of variability and reproducibility of
results from each trial of LLMs have been largely overlooked in existing
literature while actual human annotation uses majority voting to resolve
disagreements among annotators. Therefore, this study introduces the
straightforward ensemble strategy to a sentiment analysis using LLMs. As the
results, we demonstrate that the ensemble of multiple inference using
medium-sized LLMs produces more robust and accurate results than using a large
model with a single attempt with reducing RMSE by 18.6%.

</details>

### [161] [MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction](https://arxiv.org/abs/2504.18938)
*Junhong Liang,Yu Zhou*

Main category: cs.CL

TLDR: 论文提出了一种多轮中文拼写纠错框架（MTCSC），通过检索增强生成（RAG）和长度反射机制，解决了传统方法在领域适应和输出长度一致性上的限制。


<details>
  <summary>Details</summary>
Motivation: 传统中文拼写纠错（CSC）方法要求输入输出长度一致，且难以适应领域特定纠错需求，限制了其应用范围。

Method: 提出MTCSC框架，结合检索数据库和多源组合策略，通过迭代长度反射机制优化纠错性能。

Result: 实验表明，该方法在领域特定和变长纠错任务中显著优于现有方法。

Conclusion: MTCSC框架有效提升了中文拼写纠错的灵活性和准确性，尤其在领域适应和变长纠错方面表现突出。

Abstract: Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens
in sentences. While Large Language Models (LLMs) have shown remarkable success
in identifying and rectifying potential errors, they often struggle with
maintaining consistent output lengths and adapting to domain-specific
corrections. Furthermore, existing CSC task impose rigid constraints requiring
input and output lengths to be identical, limiting their applicability. In this
work, we extend traditional CSC to variable-length correction scenarios,
including Chinese Splitting Error Correction (CSEC) and ASR N-best Error
Correction. To address domain adaptation and length consistency, we propose
MTCSC (Multi-Turn CSC) framework based on RAG enhanced with a length reflection
mechanism. Our approach constructs a retrieval database from domain-specific
training data and dictionaries, fine-tuning retrievers to optimize performance
for error-containing inputs. Additionally, we introduce a multi-source
combination strategy with iterative length reflection to ensure output length
fidelity. Experiments across diverse domain datasets demonstrate that our
method significantly outperforms current approaches in correction quality,
particularly in handling domain-specific and variable-length error correction
tasks.

</details>

### [162] [LawFlow : Collecting and Simulating Lawyers' Thought Processes](https://arxiv.org/abs/2504.18942)
*Debarati Das,Khanh Chi Le,Ritik Sachin Parkar,Karin De Langis,Brendan Madson,Chad M. Berryman,Robin M. Willis,Daniel H. Moses,Brett McDonnell,Daniel Schwarcz,Dongyeop Kang*

Main category: cs.CL

TLDR: 论文介绍了LawFlow数据集，用于支持法律工作流的端到端决策，比较了人类与LLM生成的工作流差异，并提出了AI辅助法律实践的设计建议。


<details>
  <summary>Details</summary>
Motivation: 当前AI在法律领域的应用局限于孤立子任务，缺乏对真实法律实践中端到端决策的支持。

Method: 引入LawFlow数据集，收集真实法律工作流，比较人类与LLM生成的工作流，分析差异。

Result: 人类工作流更具模块化和适应性，LLM工作流更线性且缺乏灵活性；法律从业者倾向于AI在支持性角色中发挥作用。

Conclusion: 提出了基于实证的设计建议，以开发更具协作性和推理能力的法律AI系统。

Abstract: Legal practitioners, particularly those early in their careers, face complex,
high-stakes tasks that require adaptive, context-sensitive reasoning. While AI
holds promise in supporting legal work, current datasets and models are
narrowly focused on isolated subtasks and fail to capture the end-to-end
decision-making required in real-world practice. To address this gap, we
introduce LawFlow, a dataset of complete end-to-end legal workflows collected
from trained law students, grounded in real-world business entity formation
scenarios. Unlike prior datasets focused on input-output pairs or linear chains
of thought, LawFlow captures dynamic, modular, and iterative reasoning
processes that reflect the ambiguity, revision, and client-adaptive strategies
of legal practice. Using LawFlow, we compare human and LLM-generated workflows,
revealing systematic differences in structure, reasoning flexibility, and plan
execution. Human workflows tend to be modular and adaptive, while LLM workflows
are more sequential, exhaustive, and less sensitive to downstream implications.
Our findings also suggest that legal professionals prefer AI to carry out
supportive roles, such as brainstorming, identifying blind spots, and surfacing
alternatives, rather than executing complex workflows end-to-end. Building on
these findings, we propose a set of design suggestions, rooted in empirical
observations, that align AI assistance with human goals of clarity,
completeness, creativity, and efficiency, through hybrid planning, adaptive
execution, and decision-point support. Our results highlight both the current
limitations of LLMs in supporting complex legal workflows and opportunities for
developing more collaborative, reasoning-aware legal AI systems. All data and
code are available on our project page
(https://minnesotanlp.github.io/LawFlow-website/).

</details>

### [163] [Dynamic Fisher-weighted Model Merging via Bayesian Optimization](https://arxiv.org/abs/2504.18992)
*Sanwoo Lee,Jiahao Liu,Qifan Wang,Jingang Wang,Xunliang Cai,Yunfang Wu*

Main category: cs.CL

TLDR: DF-Merge是一种动态Fisher加权合并方法，通过贝叶斯优化调整参数系数，显著提升了多任务模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法存在性能差距，需统一策略以提升多任务模型效果。

Method: 提出DF-Merge框架，动态调整参数系数并结合Fisher信息，优化验证集性能。

Result: 实验表明DF-Merge在不同规模和任务上优于基线方法。

Conclusion: DF-Merge通过统一合并视角和高效优化，实现接近最优性能。

Abstract: The fine-tuning of pre-trained language models has resulted in the widespread
availability of task-specific models. Model merging offers an efficient way to
create multi-task models by combining these fine-tuned models at the parameter
level, without the need for training data or joint training on multiple
datasets. Existing merging approaches typically involve scaling the parameters
model-wise or integrating parameter importance parameter-wise. Both approaches
exhibit their own weaknesses, leading to a notable performance gap compared to
multi-task fine-tuning. In this paper, we unify these seemingly distinct
strategies into a more general merging framework, and introduce Dynamic
Fisher-weighted Merging (DF-Merge). Specifically, candidate models are
associated with a set of coefficients that linearly scale their fine-tuned
parameters. Bayesian optimization is applied to dynamically adjust these
coefficients, aiming to maximize overall performance on validation sets. Each
iteration of this process integrates parameter importance based on the Fisher
information conditioned by the coefficients. Experimental results show that
DF-Merge outperforms strong baselines across models of different sizes and a
variety of tasks. Our analysis shows that the effectiveness of DF-Merge arises
from the unified view of merging and that near-optimal performance is
achievable in a few iterations, even with minimal validation data.

</details>

### [164] [Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs](https://arxiv.org/abs/2504.19019)
*Mohammad Akbar-Tajari,Mohammad Taher Pilehvar,Mohammad Mahmoody*

Main category: cs.CL

TLDR: GoAT是一种基于图结构的方法，用于生成对抗性提示以测试大型语言模型（LLM）的安全对齐性，其效果优于现有攻击方法。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型仍易受对抗性越狱攻击，识别这些漏洞对提升模型鲁棒性至关重要。

Method: GoAT利用图结构框架生成高质量、人类可读的对抗提示，无需访问目标模型参数，通过动态整合和优化推理路径提升攻击效果。

Result: GoAT在对抗鲁棒模型（如Llama）时，成功率达到现有最佳攻击方法的五倍，且查询次数更少。

Conclusion: GoAT通过图结构推理显著提升了对抗性漏洞的探索能力，为LLM安全对齐提供了有效工具。

Abstract: The challenge of ensuring Large Language Models (LLMs) align with societal
standards is of increasing interest, as these models are still prone to
adversarial jailbreaks that bypass their safety mechanisms. Identifying these
vulnerabilities is crucial for enhancing the robustness of LLMs against such
exploits. We propose Graph of ATtacks (GoAT), a method for generating
adversarial prompts to test the robustness of LLM alignment using the Graph of
Thoughts framework [Besta et al., 2024]. GoAT excels at generating highly
effective jailbreak prompts with fewer queries to the victim model than
state-of-the-art attacks, achieving up to five times better jailbreak success
rate against robust models like Llama. Notably, GoAT creates high-quality,
human-readable prompts without requiring access to the targeted model's
parameters, making it a black-box attack. Unlike approaches constrained by
tree-based reasoning, GoAT's reasoning is based on a more intricate graph
structure. By making simultaneous attack paths aware of each other's progress,
this dynamic framework allows a deeper integration and refinement of reasoning
paths, significantly enhancing the collaborative exploration of adversarial
vulnerabilities in LLMs. At a technical level, GoAT starts with a graph
structure and iteratively refines it by combining and improving thoughts,
enabling synergy between different thought paths. The code for our
implementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks.

</details>

### [165] [Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting](https://arxiv.org/abs/2504.19021)
*Zhyar Rzgar K Rostam,Gábor Kertész*

Main category: cs.CL

TLDR: 该研究通过预训练语言模型（如BERT、SciBERT等）在Web of Science数据集上微调，结合数据增强和硬投票策略，显著提升了科学文本分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 高效文本分类对处理日益增长的学术出版物至关重要。

Method: 使用预训练语言模型（PLMs）在WoS-46985数据集上微调，通过数据增强和硬投票策略优化性能。

Result: 领域特定模型（如SciBERT、BioBERT）表现优于通用模型，分类准确性显著提升。

Conclusion: 数据增强、推理驱动标签预测和微调技术为自动化学术文本分类提供了高效且可扩展的解决方案。

Abstract: Efficient text classification is essential for handling the increasing volume
of academic publications. This study explores the use of pre-trained language
models (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on
the Web of Science (WoS-46985) dataset for scientific text classification. To
enhance performance, we augment the dataset by executing seven targeted queries
in the WoS database, retrieving 1,000 articles per category aligned with
WoS-46985's main classes. PLMs predict labels for this unlabeled data, and a
hard-voting strategy combines predictions for improved accuracy and confidence.
Fine-tuning on the expanded dataset with dynamic learning rates and early
stopping significantly boosts classification accuracy, especially in
specialized domains. Domain-specific models like SciBERT and BioBERT
consistently outperform general-purpose models such as BERT. These findings
underscore the efficacy of dataset augmentation, inference-driven label
prediction, hard-voting, and fine-tuning techniques in creating robust and
scalable solutions for automated academic text classification.

</details>

### [166] [KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation](https://arxiv.org/abs/2504.19024)
*Jiabin Fan,Guoqing Luo,Michael Bowling,Lili Mou*

Main category: cs.CL

TLDR: 提出了一种名为KETCHUP的k步回报估计方法，用于基于强化学习的知识蒸馏，通过多步贝尔曼最优方程减少梯度估计方差，提升优化效果。


<details>
  <summary>Details</summary>
Motivation: 解决基于强化学习的知识蒸馏中梯度估计方差大的问题，尤其是在学生模型较大时。

Method: 使用多步贝尔曼最优方程诱导k步回报，理论分析表明其能减少梯度估计方差。

Result: 在三个文本生成任务中表现优异，包括标准任务指标和基于大语言模型的评估。

Conclusion: k步回报诱导为基于强化学习的知识蒸馏提供了有前景的方向。

Abstract: We propose a novel k-step return estimation method (called KETCHUP) for
Reinforcement Learning(RL)-based knowledge distillation (KD) in text generation
tasks. Our idea is to induce a K-step return by using the Bellman Optimality
Equation for multiple steps. Theoretical analysis shows that this K-step
formulation reduces the variance of the gradient estimates, thus leading to
improved RL optimization especially when the student model size is large.
Empirical evaluation on three text generation tasks demonstrates that our
approach yields superior performance in both standard task metrics and large
language model (LLM)-based evaluation. These results suggest that our K-step
return induction offers a promising direction for enhancing RL-based KD in LLM
research.

</details>

### [167] [Calibrating Translation Decoding with Quality Estimation on LLMs](https://arxiv.org/abs/2504.19044)
*Di Wu,Yibin Lei,Christof Monz*

Main category: cs.CL

TLDR: 本文提出了一种通过优化Pearson相关性来校准神经机器翻译（NMT）假设似然的方法，显著提升了翻译质量，并在大型语言模型（LLM）上实现了高效改进。


<details>
  <summary>Details</summary>
Motivation: 传统最大后验（MAP）解码方法在翻译质量上表现不佳，导致低质量或病态假设，解码目标与实际翻译质量不一致。

Method: 通过直接优化Pearson相关性，从分布角度校准假设似然，提升翻译解码效果。

Result: 在有限训练（每方向2K实例）后，翻译质量显著提升，效果优于监督微调和偏好优化方法，且校准后的似然可直接作为翻译质量的强代理。

Conclusion: 该方法不仅提升了MAP解码效率，还实现了多语言翻译的先进性能，相关代码和评估数据已开源。

Abstract: Neural machine translation (NMT) systems typically employ maximum a
posteriori (MAP) decoding to select the highest-scoring translation from the
distribution mass. However, recent evidence highlights the inadequacy of MAP
decoding, often resulting in low-quality or even pathological hypotheses -- the
decoding objective is not aligned with real-world translation quality. This
paper proposes calibrating hypothesis likelihoods with translation quality from
a distribution view by directly optimizing their Pearson correlation -- thereby
enhancing the effectiveness of translation decoding. With our method,
translation on large language models (LLMs) improves substantially after
limited training (2K instances per direction). This improvement is orthogonal
to those achieved through supervised fine-tuning, leading to substantial gains
across a broad range of metrics and human evaluations -- even when applied to
top-performing translation-specialized LLMs fine-tuned on high-quality
translation data, such as Tower, or when compared to recent preference
optimization methods, like CPO. Moreover, the calibrated translation likelihood
can directly serve as a strong proxy for translation quality, closely
approximating or even surpassing some state-of-the-art translation quality
estimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates
that calibration enhances the effectiveness of MAP decoding, thereby enabling
greater efficiency in real-world deployment. The resulting state-of-the-art
translation model, which covers 10 languages, along with the accompanying code
and human evaluation data, has been released to the community:
https://github.com/moore3930/calibrating-llm-mt.

</details>

### [168] [Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models](https://arxiv.org/abs/2504.19061)
*Anindya Bijoy Das,Shibbir Ahmed,Shahnewaz Karim Sakib*

Main category: cs.CL

TLDR: 本文研究了开源大语言模型（LLMs）在临床出院报告摘要中的有效性，重点关注关键事件提取和幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 临床摘要对医疗信息传递至关重要，LLMs因其自然语言理解能力有望提升摘要的自动化与准确性。

Method: 通过数值模拟评估开源LLMs在提取出院报告关键事件（如入院原因、院内事件、随访行动）中的表现，并检测摘要中的幻觉现象。

Result: 研究评估了LLMs在临床摘要中的准确性和内容保真度，并分析了幻觉的普遍性。

Conclusion: 开源LLMs在临床摘要中具有潜力，但需解决幻觉问题以确保信息可靠性。

Abstract: Clinical summarization is crucial in healthcare as it distills complex
medical data into digestible information, enhancing patient understanding and
care management. Large language models (LLMs) have shown significant potential
in automating and improving the accuracy of such summarizations due to their
advanced natural language understanding capabilities. These models are
particularly applicable in the context of summarizing medical/clinical texts,
where precise and concise information transfer is essential. In this paper, we
investigate the effectiveness of open-source LLMs in extracting key events from
discharge reports, such as reasons for hospital admission, significant
in-hospital events, and critical follow-up actions. In addition, we also assess
the prevalence of various types of hallucinations in the summaries produced by
these models. Detecting hallucinations is vital as it directly influences the
reliability of the information, potentially affecting patient care and
treatment outcomes. We conduct comprehensive numerical simulations to
rigorously evaluate the performance of these models, further probing the
accuracy and fidelity of the extracted content in clinical summarization.

</details>

### [169] [ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics](https://arxiv.org/abs/2504.19066)
*Deeksha Varshney,Keane Ong,Rui Mao,Erik Cambria,Gianmarco Mengaldo*

Main category: cs.CL

TLDR: 论文提出了一种名为EWRA的方法，通过结合大语言模型的结构化推理路径增强小语言模型，并引入ExtremeWeatherNews数据集，用于极端天气事件的分析任务。


<details>
  <summary>Details</summary>
Motivation: 全球许多地区缺乏局部和细粒度的极端天气数据，限制了分析和决策能力。大语言模型能够处理大量非结构化文本数据并提取有用信息。

Method: 提出EWRA方法，通过大语言模型的结构化推理路径增强小语言模型，并构建ExtremeWeatherNews数据集。

Result: EWRA方法显著提升了小语言模型在极端天气分析任务中的表现，超越了任务专用模型。

Conclusion: EWRA和ExtremeWeatherNews组成的框架ClimaEmpact能够有效提升极端天气事件分析的准确性和实用性。

Abstract: Accurate assessments of extreme weather events are vital for research and
policy, yet localized and granular data remain scarce in many parts of the
world. This data gap limits our ability to analyze potential outcomes and
implications of extreme weather events, hindering effective decision-making.
Large Language Models (LLMs) can process vast amounts of unstructured text
data, extract meaningful insights, and generate detailed assessments by
synthesizing information from multiple sources. Furthermore, LLMs can
seamlessly transfer their general language understanding to smaller models,
enabling these models to retain key knowledge while being fine-tuned for
specific tasks. In this paper, we propose Extreme Weather Reasoning-Aware
Alignment (EWRA), a method that enhances small language models (SLMs) by
incorporating structured reasoning paths derived from LLMs, and
ExtremeWeatherNews, a large dataset of extreme weather event-related news
articles. EWRA and ExtremeWeatherNews together form the overall framework,
ClimaEmpact, that focuses on addressing three critical extreme-weather tasks:
categorization of tangible vulnerabilities/impacts, topic labeling, and emotion
analysis. By aligning SLMs with advanced reasoning strategies on
ExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for
SLM alignment), EWRA improves the SLMs' ability to generate well-grounded and
domain-specific responses for extreme weather analytics. Our results show that
the approach proposed guides SLMs to output domain-aligned responses,
surpassing the performance of task-specific models and offering enhanced
real-world applicability for extreme weather analytics.

</details>

### [170] [Sample-Efficient Language Model for Hinglish Conversational AI](https://arxiv.org/abs/2504.19070)
*Sakshi Singh,Abhinav Prakash,Aakriti Shah,Chaitanya Sachdeva,Sanjana Dumpala*

Main category: cs.CL

TLDR: 本文提出了一种高效的语言模型开发方法，用于构建Hinglish（印地语与英语混合）聊天机器人，通过微调预训练模型解决数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: Hinglish作为一种混合语言，拼写不一致、缺乏标准化且对话数据质量有限，计算挑战大。

Method: 评估了多种预训练跨语言模型（如Gemma3-4B和Qwen2.5-7B），并结合微调技术和合成对话数据。

Result: 实验表明，参数较少的模型在高质量混合数据上微调后，能实现竞争性性能且计算高效。

Conclusion: 该方法为Hinglish对话生成提供了高效解决方案，解决了数据稀缺问题。

Abstract: This paper presents our process for developing a sample-efficient language
model for a conversational Hinglish chatbot. Hinglish, a code-mixed language
that combines Hindi and English, presents a unique computational challenge due
to inconsistent spelling, lack of standardization, and limited quality of
conversational data. This work evaluates multiple pre-trained cross-lingual
language models, including Gemma3-4B and Qwen2.5-7B, and employs fine-tuning
techniques to improve performance on Hinglish conversational tasks. The
proposed approach integrates synthetically generated dialogues with insights
from existing Hinglish datasets to address data scarcity. Experimental results
demonstrate that models with fewer parameters, when appropriately fine-tuned on
high-quality code-mixed data, can achieve competitive performance for Hinglish
conversation generation while maintaining computational efficiency.

</details>

### [171] [Efficient Reasoning for LLMs through Speculative Chain-of-Thought](https://arxiv.org/abs/2504.19095)
*Jikai Wang,Juntao Li,Lijun Wu,Min Zhang*

Main category: cs.CL

TLDR: 提出了一种名为SCoT的方法，通过大小模型协作加速推理速度，显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理语言模型因模型规模和生成长思维链导致的高推理成本和延迟问题。

Method: 使用轻量级草稿模型进行思维级草拟，选择最佳CoT草拟并用目标模型纠正错误。

Result: 在多个数据集上，SCoT将推理延迟降低48%~66%，同时保持接近目标模型的性能。

Conclusion: SCoT通过思维行为对齐和草拟选择策略，高效降低了推理延迟且保持准确性。

Abstract: Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have
recently attracted widespread attention due to their impressive task-solving
abilities. However, the enormous model size and the generation of lengthy
thought chains introduce significant reasoning costs and response latency.
Existing methods for efficient reasoning mainly focus on reducing the number of
model parameters or shortening the chain-of-thought length. In this paper, we
introduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency
from another perspective by accelerated average reasoning speed through large
and small model collaboration. SCoT conducts thought-level drafting using a
lightweight draft model. Then it selects the best CoT draft and corrects the
error cases with the target model. The proposed thinking behavior alignment
improves the efficiency of drafting and the draft selection strategy maintains
the prediction accuracy for complex problems. Experimental results on GSM8K,
MATH, GaoKao, CollegeMath and Olympiad datasets show that SCoT reduces
reasoning latency by 48\%$\sim$66\% for Deepseek-R1-Distill-Qwen-32B while
achieving near-target-model-level performance. Our code is available at
https://github.com/Jikai0Wang/Speculative_CoT.

</details>

### [172] [Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation](https://arxiv.org/abs/2504.19101)
*Qianren Mao,Qili Zhang,Hanwen Hao,Zhentao Han,Runhua Xu,Weifeng Jiang,Qi Hu,Zhijun Chen,Tyler Zhou,Bo Li,Yangqiu Song,Jin Dong,Jianxin Li,Philip S. Yu*

Main category: cs.CL

TLDR: 提出了一种名为FedE4RAG的框架，结合联邦学习和知识蒸馏，解决私有RAG系统中的数据隐私和稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 私有RAG系统面临数据稀缺和隐私问题，阻碍其部署。

Method: 采用联邦学习框架FedE4RAG，结合知识蒸馏和同态加密技术，实现隐私保护的协作训练。

Result: 实验验证FedE4RAG显著提升私有RAG系统性能，同时保障数据隐私。

Conclusion: FedE4RAG是解决私有RAG系统隐私和性能问题的有效方案。

Abstract: Retrieval-Augmented Generation (RAG) has recently emerged as a promising
solution for enhancing the accuracy and credibility of Large Language Models
(LLMs), particularly in Question & Answer tasks. This is achieved by
incorporating proprietary and private data from integrated databases. However,
private RAG systems face significant challenges due to the scarcity of private
domain data and critical data privacy issues. These obstacles impede the
deployment of private RAG systems, as developing privacy-preserving RAG systems
requires a delicate balance between data security and data availability. To
address these challenges, we regard federated learning (FL) as a highly
promising technology for privacy-preserving RAG services. We propose a novel
framework called Federated Retrieval-Augmented Generation (FedE4RAG). This
framework facilitates collaborative training of client-side RAG retrieval
models. The parameters of these models are aggregated and distributed on a
central-server, ensuring data privacy without direct sharing of raw data. In
FedE4RAG, knowledge distillation is employed for communication between the
server and client models. This technique improves the generalization of local
RAG retrievers during the federated learning process. Additionally, we apply
homomorphic encryption within federated learning to safeguard model parameters
and mitigate concerns related to data leakage. Extensive experiments conducted
on the real-world dataset have validated the effectiveness of FedE4RAG. The
results demonstrate that our proposed framework can markedly enhance the
performance of private RAG systems while maintaining robust data privacy
protection.

</details>

### [173] [APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries](https://arxiv.org/abs/2504.19110)
*Huajian Xin,Luming Li,Xiaoran Jin,Jacques Fleuriot,Wenda Li*

Main category: cs.CL

TLDR: 论文提出自动化证明工程（APE）范式，利用大语言模型（LLM）自动化数学库中的证明工程任务，并发布首个真实基准APE-Bench I。


<details>
  <summary>Details</summary>
Motivation: 现有基准局限于静态证明任务，未能反映真实数学库的迭代工程流程，受软件工程启发，提出APE以自动化证明工程任务。

Method: 基于Mathlib4的提交历史构建APE-Bench I，结合Lean编译器和LLM-as-a-Judge验证任务，开发并行验证工具Eleanstic。

Result: 实验显示LLM在局部编辑中表现良好，但在复杂证明工程中表现显著下降。

Conclusion: 为证明工程中的代理工作流奠定基础，未来将扩展至多文件协作和项目级验证。

Abstract: Recent progress in large language models (LLMs) has shown promise in formal
theorem proving, yet existing benchmarks remain limited to isolated, static
proof tasks, failing to capture the iterative, engineering-intensive workflows
of real-world formal mathematics libraries. Motivated by analogous advances in
software engineering, we introduce the paradigm of Automated Proof Engineering
(APE), which aims to automate proof engineering tasks such as feature addition,
proof refactoring, and bug fixing using LLMs. To facilitate research in this
direction, we present APE-Bench I, the first realistic benchmark built from
real-world commit histories of Mathlib4, featuring diverse file-level tasks
described in natural language and verified via a hybrid approach combining the
Lean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable
parallel verification infrastructure optimized for proof checking across
multiple versions of Mathlib. Empirical results on state-of-the-art LLMs
demonstrate strong performance on localized edits but substantial degradation
on handling complex proof engineering. This work lays the foundation for
developing agentic workflows in proof engineering, with future benchmarks
targeting multi-file coordination, project-scale verification, and autonomous
agents capable of planning, editing, and repairing formal libraries.

</details>

### [174] [SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning](https://arxiv.org/abs/2504.19162)
*Jiaqi Chen,Bang Zhang,Ruotian Ma,Peisong Wang,Xiaodan Liang,Zhaopeng Tu,Xiaolong Li,Kwan-Yee K. Wong*

Main category: cs.CL

TLDR: SPC是一种通过对抗性自玩游戏提升LLM推理步骤可靠性的方法，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 评估LLM推理步骤的可靠性困难且成本高，需高质量监督。

Method: 通过对抗性自玩游戏，生成器和批评器模型相互改进，使用强化学习优化。

Result: SPC在多个基准测试中表现优于基线模型，显著提升数学推理性能。

Conclusion: SPC有效提升LLM推理步骤的可靠性，优于现有方法。

Abstract: Evaluating the step-by-step reliability of large language model (LLM)
reasoning, such as Chain-of-Thought, remains challenging due to the difficulty
and cost of obtaining high-quality step-level supervision. In this paper, we
introduce Self-Play Critic (SPC), a novel approach where a critic model evolves
its ability to assess reasoning steps through adversarial self-play games,
eliminating the need for manual step-level annotation. SPC involves fine-tuning
two copies of a base model to play two roles, namely a "sneaky generator" that
deliberately produces erroneous steps designed to be difficult to detect, and a
"critic" that analyzes the correctness of reasoning steps. These two models
engage in an adversarial game in which the generator aims to fool the critic,
while the critic model seeks to identify the generator's errors. Using
reinforcement learning based on the game outcomes, the models iteratively
improve; the winner of each confrontation receives a positive reward and the
loser receives a negative reward, driving continuous self-evolution.
Experiments on three reasoning process benchmarks (ProcessBench, PRM800K,
DeltaBench) demonstrate that our SPC progressively enhances its error detection
capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and
surpasses strong baselines, including distilled R1 model. Furthermore, applying
SPC to guide the test-time search of diverse LLMs significantly improves their
mathematical reasoning performance on MATH500 and AIME2024, outperforming
state-of-the-art process reward models.

</details>

### [175] [WuNeng: Hybrid State with Attention](https://arxiv.org/abs/2504.19191)
*Liu Xiao,Li Zhiyuan,Lin Yueyu*

Main category: cs.CL

TLDR: WuNeng架构通过结合RNN-based RWKV-7与高级注意力机制，提升语言模型的表达能力和上下文连贯性，同时保持高效性。


<details>
  <summary>Details</summary>
Motivation: 增强大型语言模型的表达能力和上下文连贯性，而非单纯减少KV缓存大小。

Method: 集成RWKV-7状态驱动头与标准多头注意力，采用交叉头交互技术和多令牌状态处理机制。

Result: 显著提升模型在复杂推理和序列生成任务中的表现，且参数增加极少。

Conclusion: WuNeng在表达能力和计算效率之间实现了新的平衡，为现代神经架构树立了新标准。

Abstract: The WuNeng architecture introduces a novel approach to enhancing the
expressivity and power of large language models by integrating recurrent neural
network (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing
heightened contextual coherence over reducing KV cache size. Building upon the
hybrid-head concept from Hymba, WuNeng augments standard multi-head attention
with additional RWKV-7 state-driven heads, rather than replacing existing
heads, to enrich the model's representational capacity. A cross-head
interaction technique fosters dynamic synergy among standard, state-driven, and
newly introduced middle heads, leveraging concatenation, additive modulation,
and gated fusion for robust information integration. Furthermore, a multi-token
state processing mechanism harnesses the continuous RWKV-7 state to capture
intricate, sequence-wide dependencies, significantly boosting expressivity.
Remarkably, these enhancements are achieved with minimal additional parameters,
ensuring efficiency while empowering the model to excel in complex reasoning
and sequence generation tasks. WuNeng sets a new standard for balancing
expressivity and computational efficiency in modern neural architectures.

</details>

### [176] [Dynamic Embedded Topic Models: properties and recommendations based on diverse corpora](https://arxiv.org/abs/2504.19209)
*Elisabeth Fittschen,Bella Xia,Leib Celnik,Paul Dilley,Tom Lippincott*

Main category: cs.CL

TLDR: 研究了动态嵌入主题模型（Dynamic Embedded Topic Model）在不同实现选择下的效果，并提出了优化应用的建议。


<details>
  <summary>Details</summary>
Motivation: 目标是确定模型使用和进一步开发中的关键决策，以最大化其在应用研究中的实用性。

Method: 通过分析五个不同的历时语料库，评估不同实现选择的影响。

Result: 发现词汇量的实用扩展性和更灵活的时间间隔建模是关键，同时某些因素对性能影响不大。

Conclusion: 提出了优化模型应用的优先级，并指出了一些无需过度关注的实现细节。

Abstract: We measure the effects of several implementation choices for the Dynamic
Embedded Topic Model, as applied to five distinct diachronic corpora, with the
goal of isolating important decisions for its use and further development. We
identify priorities that will maximize utility in applied scholarship,
including the practical scalability of vocabulary size to best exploit the
strengths of embedded representations, and more flexible modeling of intervals
to accommodate the uneven temporal distributions of historical writing. Of
similar importance, we find performance is not significantly or consistently
affected by several aspects that otherwise limit the model's application or
might consume the resources of a grid search.

</details>

### [177] [Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers](https://arxiv.org/abs/2504.19254)
*Dylan Bouchard,Mohit Singh Chauhan*

Main category: cs.CL

TLDR: 提出了一种零资源幻觉检测框架，结合多种不确定性量化技术，通过可调集成方法优化性能，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在高风险领域（如医疗和金融）中的幻觉问题亟待解决，需要有效的检测方法。

Method: 采用黑盒、白盒不确定性量化和LLM-as-a-Judge技术，转化为标准化置信分数，并引入可调集成方法。

Result: 实验表明，可调集成方法优于单个组件和现有检测方法，提升了LLM的准确性和可靠性。

Conclusion: 定制化的幻觉检测策略能显著改善LLM的性能，适用于实际应用场景。

Abstract: Hallucinations are a persistent problem with Large Language Models (LLMs). As
these models become increasingly used in high-stakes domains, such as
healthcare and finance, the need for effective hallucination detection is
crucial. To this end, we propose a versatile framework for zero-resource
hallucination detection that practitioners can apply to real-world use cases.
To achieve this, we adapt a variety of existing uncertainty quantification (UQ)
techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,
transforming them as necessary into standardized response-level confidence
scores ranging from 0 to 1. To enhance flexibility, we introduce a tunable
ensemble approach that incorporates any combination of the individual
confidence scores. This approach enables practitioners to optimize the ensemble
for a specific use case for improved performance. To streamline implementation,
the full suite of scorers is offered in this paper's companion Python toolkit,
UQLM. To evaluate the performance of the various scorers, we conduct an
extensive set of experiments using several LLM question-answering benchmarks.
We find that our tunable ensemble typically surpasses its individual components
and outperforms existing hallucination detection methods. Our results
demonstrate the benefits of customized hallucination detection strategies for
improving the accuracy and reliability of LLMs.

</details>

### [178] [VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?](https://arxiv.org/abs/2504.19267)
*Mohamed Gado,Towhid Taliee,Muhammad Memon,Dmitry Ignatov,Radu Timofte*

Main category: cs.CL

TLDR: 本文提出了一种基于多模态模型的视觉叙事方法VIST-GPT，并设计了新的评估指标RoViST和GROOVIST，以更准确地评估叙事质量。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标（如BLEU、METEOR等）不适用于视觉叙事任务，需要更符合人类判断的评估方法。

Method: 采用基于Transformer的多模态模型，结合VIST数据集，生成视觉叙事。

Result: VIST-GPT模型能够生成视觉相关且上下文连贯的叙事。

Conclusion: RoViST和GROOVIST指标能更准确地评估叙事质量，优于传统指标。

Abstract: Visual storytelling is an interdisciplinary field combining computer vision
and natural language processing to generate cohesive narratives from sequences
of images. This paper presents a novel approach that leverages recent
advancements in multimodal models, specifically adapting transformer-based
architectures and large multimodal models, for the visual storytelling task.
Leveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT
model produces visually grounded, contextually appropriate narratives. We
address the limitations of traditional evaluation metrics, such as BLEU,
METEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we
utilize RoViST and GROOVIST, novel reference-free metrics designed to assess
visual storytelling, focusing on visual grounding, coherence, and
non-redundancy. These metrics provide a more nuanced evaluation of narrative
quality, aligning closely with human judgment.

</details>

### [179] [AndroidGen: Building an Android Language Agent under Data Scarcity](https://arxiv.org/abs/2504.19298)
*Hanyu Lai,Junjie Gao,Xiao Liu,Yifan Xu,Shudan Zhang,Yuxiao Dong,Jie Tang*

Main category: cs.CL

TLDR: 论文提出AndroidGen框架，解决LLM在移动设备上作为代理的数据稀缺问题，并展示了其性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在NLP任务中潜力巨大，但在移动设备上作为代理的广泛应用仍受限于高质量数据源的缺乏。人工标注耗时且劳动密集，现有LLM完成率不足且缺乏数据过滤策略。

Method: 开发AndroidGen框架，通过收集人类任务轨迹并训练开源LLM，无需手动标注轨迹，提升LLM代理能力。

Result: 在AndroidWorld、AitW等应用中广泛评估，展示了性能改进并揭示未来优化方向。

Conclusion: AndroidGen为数据稀缺下的LLM代理提供了有效解决方案，代码、模型和数据已开源。

Abstract: Large language models have opened up a world of possibilities for various NLP
tasks, sparking optimism for the future. Despite their potential, LLMs have yet
to be widely used as agents on real mobile devices. The main challenge is the
need for high-quality data sources. Time constraints and labor intensity often
hinder human annotation. On the other hand, existing LLMs exhibit inadequate
completion rates and need a robust data filtration strategy. Given these
challenges, we develop a framework called AndroidGen to enhance the
capabilities of LLM-based agents under data scarcity. In addition, we leverage
AndroidGen to collect trajectories given human tasks and train open-source LLMs
on these trajectories to develop an open-source mobile agent without manually
labeled trajectories. We extensively evaluate AndroidGen with AndroidWorld,
AitW, and various popular applications, demonstrating its improvements and
revealing potential areas for future improvement. Code, model, and data are
available at https://github.com/THUDM/AndroidGen.

</details>

### [180] [BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese](https://arxiv.org/abs/2504.19314)
*Peilin Zhou,Bruce Leon,Xiang Ying,Can Zhang,Yifan Shao,Qichen Ye,Dading Chong,Zhiling Jin,Chenxuan Xie,Meng Cao,Yuxin Gu,Sixin Hong,Jing Ren,Jian Chen,Chao Liu,Yining Hua*

Main category: cs.CL

TLDR: BrowseComp-ZH是一个专为评估中文网页上的大型语言模型（LLM）代理能力而设计的高难度基准测试，覆盖11个领域，包含289个多跳问题。测试结果显示，现有模型表现普遍较差，最高准确率仅42.9%。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试（如BrowseComp）主要关注英文，忽略了中文等语言生态系统的复杂性，因此需要专门的中文基准测试。

Method: 通过反向工程从简短、客观且易于验证的答案中构建289个多跳问题，并采用两阶段质量控制确保问题难度和答案唯一性。

Result: 测试了20多个先进语言模型和搜索系统，大多数模型准确率低于10%，最佳表现系统（OpenAI的DeepResearch）仅达到42.9%。

Conclusion: BrowseComp-ZH展示了当前模型在复杂检索和推理能力上的不足，为未来研究提供了重要基准。

Abstract: As large language models (LLMs) evolve into tool-using agents, the ability to
browse the web in real-time has become a critical yardstick for measuring their
reasoning and retrieval competence. Existing benchmarks such as BrowseComp
concentrate on English and overlook the linguistic, infrastructural, and
censorship-related complexities of other major information ecosystems -- most
notably Chinese. To address this gap, we introduce BrowseComp-ZH, a
high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents
on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning
11 diverse domains. Each question is reverse-engineered from a short,
objective, and easily verifiable answer (e.g., a date, number, or proper noun).
A two-stage quality control protocol is applied to strive for high question
difficulty and answer uniqueness. We benchmark over 20 state-of-the-art
language models and agentic search systems on our proposed BrowseComp-ZH.
Despite their strong conversational and retrieval capabilities, most models
struggle severely: a large number achieve accuracy rates below 10%, and only a
handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,
reaches just 42.9%. These results demonstrate the considerable difficulty of
BrowseComp-ZH, where success demands not only effective retrieval strategies,
but also sophisticated reasoning and information reconciliation -- capabilities
that current models still struggle to master. Our dataset, construction
guidelines, and benchmark results have been publicly released at
https://github.com/PALIN2018/BrowseComp-ZH.

</details>

### [181] [Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing](https://arxiv.org/abs/2504.19333)
*James O' Neill,Santhosh Subramanian,Eric Lin,Vaikkunth Mugunthan*

Main category: cs.CL

TLDR: 论文提出了一种高效的任务特定数据生成方法，显著优于现有技术，同时模型更小。通过多任务预训练和模型合并，进一步提升了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在防止不良行为方面表现出潜力，但其高延迟、内存消耗和成本限制了应用。

Method: 1. 任务特定数据生成；2. 多任务预训练模型（MultiTaskGuard）；3. 搜索式模型合并方法（UniGuard）。

Result: 在多个数据集上，模型性能显著优于现有技术，F1分数平均提升29.92（对比Aegis-LlamaGuard）和21.62（对比GPT-4o）。

Conclusion: 任务特定数据生成和小型化模型设计是高效实现行为防护的关键。

Abstract: The trend towards large language models (LLMs) for guardrailing against
undesired behaviors is increasing and has shown promise for censoring user
inputs. However, increased latency, memory consumption, hosting expenses and
non-structured outputs can make their use prohibitive.
  In this work, we show that task-specific data generation can lead to
fine-tuned classifiers that significantly outperform current state of the art
(SoTA) while being orders of magnitude smaller. Secondly, we show that using a
single model, \texttt{MultiTaskGuard}, that is pretrained on a large
synthetically generated dataset with unique task instructions further improves
generalization. Thirdly, our most performant models, \texttt{UniGuard}, are
found using our proposed search-based model merging approach that finds an
optimal set of parameters to combine single-policy models and multi-policy
guardrail models. % On 7 public datasets and 4 guardrail benchmarks we created,
our efficient guardrail classifiers improve over the best performing SoTA
publicly available LLMs and 3$^{\text{rd}}$ party guardrail APIs in detecting
unsafe and safe behaviors by an average F1 score improvement of \textbf{29.92}
points over Aegis-LlamaGuard and \textbf{21.62} over \texttt{gpt-4o},
respectively. Lastly, our guardrail synthetic data generation process that uses
custom task-specific guardrail poli

</details>

### [182] [Explanatory Summarization with Discourse-Driven Planning](https://arxiv.org/abs/2504.19339)
*Dongqi Liu,Xi Yu,Vera Demberg,Mirella Lapata*

Main category: cs.CL

TLDR: 本文提出了一种基于计划的摘要生成方法，通过话语框架引导解释性内容，提升摘要质量。


<details>
  <summary>Details</summary>
Motivation: 当前自动摘要方法未明确建模解释性内容，导致与人工摘要的匹配度不足。

Method: 采用两种话语驱动的计划策略，将计划作为输入或输出前缀的一部分。

Result: 在三个数据集上实验表明，该方法在摘要质量、鲁棒性和可控性上优于现有方法，并减少幻觉。

Conclusion: 基于话语框架的计划方法能有效提升摘要的解释性和质量。

Abstract: Lay summaries for scientific documents typically include explanations to help
readers grasp sophisticated concepts or arguments. However, current automatic
summarization methods do not explicitly model explanations, which makes it
difficult to align the proportion of explanatory content with human-written
summaries. In this paper, we present a plan-based approach that leverages
discourse frameworks to organize summary generation and guide explanatory
sentences by prompting responses to the plan. Specifically, we propose two
discourse-driven planning strategies, where the plan is conditioned as part of
the input or part of the output prefix, respectively. Empirical experiments on
three lay summarization datasets show that our approach outperforms existing
state-of-the-art methods in terms of summary quality, and it enhances model
robustness, controllability, and mitigates hallucination.

</details>

### [183] [ICL CIPHERS: Quantifying "Learning'' in In-Context Learning via Substitution Ciphers](https://arxiv.org/abs/2504.19395)
*Zhouxiang Fang,Aayush Mishra,Muhan Gao,Anqi Liu,Daniel Khashabi*

Main category: cs.CL

TLDR: 论文提出了ICL CIPHERS方法，通过替换密码将输入中的部分标记替换为无关标记，以研究LLMs在任务检索和任务学习中的表现。实验表明，LLMs在可逆映射（BIJECTIVE）下表现优于不可逆（NON-BIJECTIVE）基线。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在上下文学习（ICL）中的任务检索和任务学习模式，并试图分离这两种模式。

Method: 引入ICL CIPHERS方法，使用替换密码对输入进行可逆变换，测试LLMs在可逆和不可逆映射下的表现。

Result: LLMs在可逆映射下表现优于不可逆基线，且这种差距在四个数据集和六个模型中一致存在。

Conclusion: ICL CIPHERS提供了一种量化ICL中“学习”的新方法，并揭示了LLMs对密码输入的潜在解码能力。

Abstract: Recent works have suggested that In-Context Learning (ICL) operates in dual
modes, i.e. task retrieval (remember learned patterns from pre-training) and
task learning (inference-time ``learning'' from demonstrations). However,
disentangling these the two modes remains a challenging goal. We introduce ICL
CIPHERS, a class of task reformulations based on substitution ciphers borrowed
from classic cryptography. In this approach, a subset of tokens in the
in-context inputs are substituted with other (irrelevant) tokens, rendering
English sentences less comprehensible to human eye. However, by design, there
is a latent, fixed pattern to this substitution, making it reversible. This
bijective (reversible) cipher ensures that the task remains a well-defined task
in some abstract sense, despite the transformations. It is a curious question
if LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires
deciphering the latent cipher. We show that LLMs are better at solving ICL
CIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline,
providing a novel approach to quantify ``learning'' in ICL. While this gap is
small, it is consistent across the board on four datasets and six models.
Finally, we examine LLMs' internal representations and identify evidence in
their ability to decode the ciphered inputs.

</details>

### [184] [Context Selection and Rewriting for Video-based EducationalQuestion Generation](https://arxiv.org/abs/2504.19406)
*Mengxia Yu,Bang Nguyen,Olivia Zino,Meng Jiang*

Main category: cs.CL

TLDR: 论文提出了一种基于真实课堂内容的教育问题生成框架，解决了现有方法在时间戳对齐和目标答案融入上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有教育问题生成系统依赖预定义文本，无法反映真实课堂内容（如演讲和幻灯片），且生成问题与目标答案和时间戳对齐困难。

Method: 提出新框架，利用大语言模型动态选择和重写上下文，结合演讲文本和视频关键帧，生成包含答案的知识陈述。

Result: 框架显著提升了生成问题的质量和相关性。

Conclusion: 该方法和数据集填补了真实课堂内容生成教育问题的空白，代码和数据集已开源。

Abstract: Educational question generation (EQG) is a crucial component of intelligent
educational systems, significantly aiding self-assessment, active learning, and
personalized education. While EQG systems have emerged, existing datasets
typically rely on predefined, carefully edited texts, failing to represent
real-world classroom content, including lecture speech with a set of
complementary slides. To bridge this gap, we collect a dataset of educational
questions based on lectures from real-world classrooms. On this realistic
dataset, we find that current methods for EQG struggle with accurately
generating questions from educational videos, particularly in aligning with
specific timestamps and target answers. Common challenges include selecting
informative contexts from extensive transcripts and ensuring generated
questions meaningfully incorporate the target answer. To address the
challenges, we introduce a novel framework utilizing large language models for
dynamically selecting and rewriting contexts based on target timestamps and
answers. First, our framework selects contexts from both lecture transcripts
and video keyframes based on answer relevance and temporal proximity. Then, we
integrate the contexts selected from both modalities and rewrite them into
answer-containing knowledge statements, to enhance the logical connection
between the contexts and the desired answer. This approach significantly
improves the quality and relevance of the generated questions. Our dataset and
code are released in https://github.com/mengxiayu/COSER.

</details>

### [185] [Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory](https://arxiv.org/abs/2504.19413)
*Prateek Chhikara,Dev Khant,Saket Aryan,Taranjeet Singh,Deshraj Yadav*

Main category: cs.CL

TLDR: 论文提出Mem0及其图记忆变体，解决LLMs在长对话中上下文固定的问题，显著优于现有方法，降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在长对话中因固定上下文窗口导致的连贯性问题。

Method: 动态提取、整合和检索对话关键信息，并引入图记忆表示复杂关系。

Result: 在LOCOMO基准测试中优于六类基线，Mem0提升26%性能，图记忆变体再提升2%，显著降低计算成本。

Conclusion: 结构化持久记忆机制对长对话连贯性至关重要，为高效LLM驱动AI代理铺路。

Abstract: Large Language Models (LLMs) have demonstrated remarkable prowess in
generating contextually coherent responses, yet their fixed context windows
pose fundamental challenges for maintaining consistency over prolonged
multi-session dialogues. We introduce Mem0, a scalable memory-centric
architecture that addresses this issue by dynamically extracting,
consolidating, and retrieving salient information from ongoing conversations.
Building on this foundation, we further propose an enhanced variant that
leverages graph-based memory representations to capture complex relational
structures among conversational elements. Through comprehensive evaluations on
LOCOMO benchmark, we systematically compare our approaches against six baseline
categories: (i) established memory-augmented systems, (ii) retrieval-augmented
generation (RAG) with varying chunk sizes and k-values, (iii) a full-context
approach that processes the entire conversation history, (iv) an open-source
memory solution, (v) a proprietary model system, and (vi) a dedicated memory
management platform. Empirical results show that our methods consistently
outperform all existing memory systems across four question categories:
single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26%
relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with
graph memory achieves around 2% higher overall score than the base
configuration. Beyond accuracy gains, we also markedly reduce computational
overhead compared to full-context method. In particular, Mem0 attains a 91%
lower p95 latency and saves more than 90% token cost, offering a compelling
balance between advanced reasoning capabilities and practical deployment
constraints. Our findings highlight critical role of structured, persistent
memory mechanisms for long-term conversational coherence, paving the way for
more reliable and efficient LLM-driven AI agents.

</details>

### [186] [Context-Guided Dynamic Retrieval for Improving Generation Quality in RAG Models](https://arxiv.org/abs/2504.19436)
*Jacky He,Guiran Liu,Binrong Zhu,Hanlu Zhang,Hongye Zheng,Xiaokai Wang*

Main category: cs.CL

TLDR: 本文提出了一种动态优化的RAG架构，通过状态感知的动态知识检索机制提升语义理解和知识调度效率，适用于开放域问答和复杂生成任务。


<details>
  <summary>Details</summary>
Motivation: 解决静态RAG架构在上下文适应和知识访问方面的局限性。

Method: 引入多级感知检索向量构建策略和可微分文档匹配路径，实现端到端联合训练。

Result: 在Natural Questions数据集上验证，BLEU和ROUGE-L分数显著提升，且在语义模糊和多文档融合任务中表现更强鲁棒性。

Conclusion: 该方法在构建高质量语言生成系统中具有广泛的应用潜力和实用价值。

Abstract: This paper focuses on the dynamic optimization of the Retrieval-Augmented
Generation (RAG) architecture. It proposes a state-aware dynamic knowledge
retrieval mechanism to enhance semantic understanding and knowledge scheduling
efficiency in large language models for open-domain question answering and
complex generation tasks. The method introduces a multi-level perceptive
retrieval vector construction strategy and a differentiable document matching
path. These components enable end-to-end joint training and collaborative
optimization of the retrieval and generation modules. This effectively
addresses the limitations of static RAG structures in context adaptation and
knowledge access. Experiments are conducted on the Natural Questions dataset.
The proposed structure is thoroughly evaluated across different large models,
including GPT-4, GPT-4o, and DeepSeek. Comparative and ablation experiments
from multiple perspectives confirm the significant improvements in BLEU and
ROUGE-L scores. The approach also demonstrates stronger robustness and
generation consistency in tasks involving semantic ambiguity and multi-document
fusion. These results highlight its broad application potential and practical
value in building high-quality language generation systems.

</details>

### [187] [Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks](https://arxiv.org/abs/2504.19445)
*Yi-Long Lu,Chunhui Zhang,Wei Wang*

Main category: cs.CL

TLDR: 研究发现，大语言模型（LLMs）在二元和连续响应格式下表现出系统性负面偏见，二元格式更易产生负面判断。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在不同响应格式（二元vs连续）下的判断偏见，以评估其可靠性。

Method: 通过价值判断和文本情感分析任务，测试多种LLMs在两种响应格式下的表现。

Result: LLMs在二元格式下更倾向于负面判断，且这一模式在两种任务中均一致。

Conclusion: 任务设计中的响应格式选择可能引入系统性偏见，需谨慎考虑。

Abstract: Large Language Models (LLMs) are increasingly used in tasks such as
psychological text analysis and decision-making in automated workflows.
However, their reliability remains a concern due to potential biases inherited
from their training process. In this study, we examine how different response
format: binary versus continuous, may systematically influence LLMs' judgments.
In a value statement judgments task and a text sentiment analysis task, we
prompted LLMs to simulate human responses and tested both formats across
several models, including both open-source and commercial models. Our findings
revealed a consistent negative bias: LLMs were more likely to deliver
"negative" judgments in binary formats compared to continuous ones. Control
experiments further revealed that this pattern holds across both tasks. Our
results highlight the importance of considering response format when applying
LLMs to decision tasks, as small changes in task design can introduce
systematic biases.

</details>

### [188] [Towards Long Context Hallucination Detection](https://arxiv.org/abs/2504.19457)
*Siyi Liu,Kishaloy Halder,Zheng Qi,Wei Xiao,Nikolaos Pappas,Phu Mon Htut,Neha Anna John,Yassine Benajiba,Dan Roth*

Main category: cs.CL

TLDR: 论文提出了一种新架构，用于检测长上下文输入中的幻觉问题，并通过实验验证其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在长上下文输入中产生的幻觉问题。

Method: 构建专用数据集，并提出一种分解与聚合机制的新架构，使预训练编码器模型能处理长上下文。

Result: 新架构在多项指标上显著优于同类模型和基于LLM的模型，且推理速度更快。

Conclusion: 该研究为解决长上下文幻觉问题提供了有效方法，并展示了新架构的优越性能。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various tasks. However, they are prone to contextual hallucination, generating
information that is either unsubstantiated or contradictory to the given
context. Although many studies have investigated contextual hallucinations in
LLMs, addressing them in long-context inputs remains an open problem. In this
work, we take an initial step toward solving this problem by constructing a
dataset specifically designed for long-context hallucination detection.
Furthermore, we propose a novel architecture that enables pre-trained encoder
models, such as BERT, to process long contexts and effectively detect
contextual hallucinations through a decomposition and aggregation mechanism.
Our experimental results show that the proposed architecture significantly
outperforms previous models of similar size as well as LLM-based models across
various metrics, while providing substantially faster inference.

</details>

### [189] [BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text](https://arxiv.org/abs/2504.19467)
*Jiageng Wu,Bowen Gu,Ren Zhou,Kevin Xie,Doug Snyder,Yixing Jiang,Valentina Carducci,Richard Wyss,Rishi J Desai,Emily Alsentzer,Leo Anthony Celi,Adam Rodman,Sebastian Schneeweiss,Jonathan H. Chen,Santiago Romero-Brufau,Kueiyu Joshua Lin,Jie Yang*

Main category: cs.CL

TLDR: BRIDGE是一个多语言临床基准测试，评估了52种先进LLM在真实世界临床数据上的表现，发现开源模型可与专有模型媲美，而医学微调的旧架构模型表现较差。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在临床环境中的评估有限，现有基准未能捕捉真实电子健康记录的复杂性或缺乏通用性。

Method: 提出了BRIDGE基准，包含87个任务，基于多语言真实临床数据，系统评估了52种LLM在不同推理策略下的表现。

Result: 实验显示模型性能因规模、语言、任务和临床领域而异，开源模型表现接近专有模型，医学微调旧模型表现不佳。

Conclusion: BRIDGE为LLM在临床文本理解中的开发和评估提供了基础资源和参考。

Abstract: Large language models (LLMs) hold great promise for medical applications and
are evolving rapidly, with new models being released at an accelerated pace.
However, current evaluations of LLMs in clinical contexts remain limited. Most
existing benchmarks rely on medical exam-style questions or PubMed-derived
text, failing to capture the complexity of real-world electronic health record
(EHR) data. Others focus narrowly on specific application scenarios, limiting
their generalizability across broader clinical use. To address this gap, we
present BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks
sourced from real-world clinical data sources across nine languages. We
systematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1,
GPT-4o, Gemini, and Llama 4) under various inference strategies. With a total
of 13,572 experiments, our results reveal substantial performance variation
across model sizes, languages, natural language processing tasks, and clinical
specialties. Notably, we demonstrate that open-source LLMs can achieve
performance comparable to proprietary models, while medically fine-tuned LLMs
based on older architectures often underperform versus updated general-purpose
models. The BRIDGE and its corresponding leaderboard serve as a foundational
resource and a unique reference for the development and evaluation of new LLMs
in real-world clinical text understanding.

</details>

### [190] [Conflicts in Texts: Data, Implications and Challenges](https://arxiv.org/abs/2504.19472)
*Siyi Liu,Dan Roth*

Main category: cs.CL

TLDR: 该论文探讨了NLP模型中冲突信息的问题，将其分为三类（自然文本、人工标注数据、模型交互），分析了影响并提出了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着NLP模型在现实应用中的普及，冲突信息可能导致模型不可靠，需系统化研究以提升模型可信度。

Method: 通过分类冲突信息（自然文本、人工标注数据、模型交互），分析其影响并讨论缓解策略。

Result: 总结了冲突信息的来源及影响，提出了开发冲突感知NLP系统的未来方向。

Conclusion: 需开发能有效处理冲突信息的NLP系统，以提升模型可靠性和信任度。

Abstract: As NLP models become increasingly integrated into real-world applications, it
becomes clear that there is a need to address the fact that models often rely
on and generate conflicting information. Conflicts could reflect the complexity
of situations, changes that need to be explained and dealt with, difficulties
in data annotation, and mistakes in generated outputs. In all cases,
disregarding the conflicts in data could result in undesired behaviors of
models and undermine NLP models' reliability and trustworthiness. This survey
categorizes these conflicts into three key areas: (1) natural texts on the web,
where factual inconsistencies, subjective biases, and multiple perspectives
introduce contradictions; (2) human-annotated data, where annotator
disagreements, mistakes, and societal biases impact model training; and (3)
model interactions, where hallucinations and knowledge conflicts emerge during
deployment. While prior work has addressed some of these conflicts in
isolation, we unify them under the broader concept of conflicting information,
analyze their implications, and discuss mitigation strategies. We highlight key
challenges and future directions for developing conflict-aware NLP systems that
can reason over and reconcile conflicting information more effectively.

</details>

### [191] [Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment](https://arxiv.org/abs/2504.19556)
*Kristen Sussman,Daniel Carter*

Main category: cs.CL

TLDR: 研究通过比较2020年和2024年关于特朗普的推文，发现AI对社交媒体语言和情感表达的影响，表现为情感极性增加和中性内容减少。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型对语言模式的影响，特别是AI介导的沟通（AI-MC）在社交媒体上的作用。

Method: 比较2020年和2024年的推文数据集，使用Flesch-Kincaid可读性和极性评分分析文本复杂性和情感变化。

Result: 发现情感极性显著增加（0.12 vs. 0.04），中性内容减少（54.8%到39.8%），正面表达增加（28.6%到45.9%）。

Conclusion: AI在社交媒体中的存在增加，并对语言和情感表达模式产生了显著影响。

Abstract: Given the subtle human-like effects of large language models on linguistic
patterns, this study examines shifts in language over time to detect the impact
of AI-mediated communication (AI- MC) on social media. We compare a replicated
dataset of 970,919 tweets from 2020 (pre-ChatGPT) with 20,000 tweets from the
same period in 2024, all of which mention Donald Trump during election periods.
Using a combination of Flesch-Kincaid readability and polarity scores, we
analyze changes in text complexity and sentiment. Our findings reveal a
significant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift
from predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more
positive expressions (28.6% to 45.9%). These findings suggest not only an
increasing presence of AI in social media communication but also its impact on
language and emotional expression patterns.

</details>

### [192] [m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training](https://arxiv.org/abs/2504.19565)
*Meng Xiao,Xunxin Cai,Chengrui Wang,Yuanchun Zhou*

Main category: cs.CL

TLDR: 提出了一种基于知识驱动的多智能体框架，用于生物医学领域的大语言模型训练，通过多智能体协作从科学文献中提取高质量数据，显著提升了模型在生物医学问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有开源生物医学标注语料库在数量和质量上不足，无法满足大语言模型训练需求，需解决生物医学知识复杂层次结构的挑战。

Method: 采用基于MeSH层次结构的多智能体协作框架，智能体自主提取、合成和评估高质量文本数据，生成领域特定的问答对。

Result: 实验表明，基于该框架训练的语言模型在生物医学问答任务中表现优异，超越现有基线模型和专有模型，如Llama3-70B超越GPT-4。

Conclusion: 多智能体协作框架在生物医学LLM训练中具有显著潜力，能高效生成高质量数据集，提升模型性能。

Abstract: The rapid progress of large language models (LLMs) in biomedical research has
underscored the limitations of existing open-source annotated scientific
corpora, which are often insufficient in quantity and quality. Addressing the
challenge posed by the complex hierarchy of biomedical knowledge, we propose a
knowledge-driven, multi-agent framework for scientific corpus distillation
tailored for LLM training in the biomedical domain. Central to our approach is
a collaborative multi-agent architecture, where specialized agents, each guided
by the Medical Subject Headings (MeSH) hierarchy, work in concert to
autonomously extract, synthesize, and self-evaluate high-quality textual data
from vast scientific literature. These agents collectively generate and refine
domain-specific question-answer pairs, ensuring comprehensive coverage and
consistency with biomedical ontologies while minimizing manual involvement.
Extensive experimental results show that language models trained on our
multi-agent distilled datasets achieve notable improvements in biomedical
question-answering tasks, outperforming both strong life sciences LLM baselines
and advanced proprietary models. Notably, our AI-Ready dataset enables
Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger
scale. Detailed ablation studies and case analyses further validate the
effectiveness and synergy of each agent within the framework, highlighting the
potential of multi-agent collaboration in biomedical LLM training.

</details>

### [193] [Arabic Metaphor Sentiment Classification Using Semantic Information](https://arxiv.org/abs/2504.19590)
*Israa Alsiyat*

Main category: cs.CL

TLDR: 论文讨论了使用基于语义标签的新工具对阿拉伯语隐喻语料库（AMC）进行情感分类测试，并通过标准方法（F-score、召回率、精确率）评估工具效果。


<details>
  <summary>Details</summary>
Motivation: 研究阿拉伯在线隐喻对情感的影响，填补了使用语义标签对阿拉伯语隐喻进行情感分类的研究空白。

Method: 设计基于语义情感标签的自动工具，用于AMC的情感分类，并通过标准评估方法验证工具效果。

Result: 工具成功应用于AMC的情感分类，展示了阿拉伯语隐喻对情感的影响。

Conclusion: 这是首次使用语义标签对阿拉伯语隐喻进行情感分类的研究，为相关领域提供了新方法。

Abstract: In this paper, I discuss the testing of the Arabic Metaphor Corpus (AMC) [1]
using newly designed automatic tools for sentiment classification for AMC based
on semantic tags. The tool incorporates semantic emotional tags for sentiment
classification. I evaluate the tool using standard methods, which are F-score,
recall, and precision. The method is to show the impact of Arabic online
metaphors on sentiment through the newly designed tools. To the best of our
knowledge, this is the first approach to conduct sentiment classification for
Arabic metaphors using semantic tags to find the impact of the metaphor.

</details>

### [194] [Coreference Resolution for Vietnamese Narrative Texts](https://arxiv.org/abs/2504.19606)
*Hieu-Dai Tran,Duc-Vu Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.CL

TLDR: 该论文构建了一个越南语指代消解标注数据集，并评估了GPT-3.5-Turbo和GPT-4在该任务上的表现，发现GPT-4表现更优。


<details>
  <summary>Details</summary>
Motivation: 越南语作为低资源语言，缺乏标注数据集，指代消解任务面临挑战。

Method: 使用VnExpress新闻文本构建标注数据集，并评估GPT-3.5-Turbo和GPT-4的性能。

Result: GPT-4在准确性和一致性上显著优于GPT-3.5-Turbo。

Conclusion: GPT-4是越南语指代消解更可靠的工具。

Abstract: Coreference resolution is a vital task in natural language processing (NLP)
that involves identifying and linking different expressions in a text that
refer to the same entity. This task is particularly challenging for Vietnamese,
a low-resource language with limited annotated datasets. To address these
challenges, we developed a comprehensive annotated dataset using narrative
texts from VnExpress, a widely-read Vietnamese online news platform. We
established detailed guidelines for annotating entities, focusing on ensuring
consistency and accuracy. Additionally, we evaluated the performance of large
language models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset.
Our results demonstrate that GPT-4 significantly outperforms GPT-3.5-Turbo in
terms of both accuracy and response consistency, making it a more reliable tool
for coreference resolution in Vietnamese.

</details>

### [195] [VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning](https://arxiv.org/abs/2504.19627)
*Run Luo,Renke Shan,Longze Chen,Ziqiang Liu,Lu Wang,Min Yang,Xiaobo Xia*

Main category: cs.CL

TLDR: VCM是一种自监督视觉概念建模框架，通过减少计算成本（如减少85%的FLOPs）并保持性能，提升了大型视觉语言模型的效率。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型（LVLMs）在图像处理上效率低下，缺乏视觉概念模型，限制了实际应用。

Method: 提出VCM框架，结合隐式对比学习和视觉语言微调，无需昂贵的概念级标注。

Result: VCM显著降低计算成本（如LLaVA-1.5-7B减少85% FLOPs），并在多种图像理解任务中保持性能。

Conclusion: VCM通过高效建模视觉概念，提升了LVLMs的实用性和视觉编码器能力。

Abstract: Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like
embodied intelligence due to their strong vision-language reasoning abilities.
However, current LVLMs process entire images at the token level, which is
inefficient compared to humans who analyze information and generate content at
the conceptual level, extracting relevant visual concepts with minimal effort.
This inefficiency, stemming from the lack of a visual concept model, limits
LVLMs' usability in real-world applications. To address this, we propose VCM,
an end-to-end self-supervised visual concept modeling framework. VCM leverages
implicit contrastive learning across multiple sampled instances and
vision-language fine-tuning to construct a visual concept model without
requiring costly concept-level annotations. Our results show that VCM
significantly reduces computational costs (e.g., 85\% fewer FLOPs for
LLaVA-1.5-7B) while maintaining strong performance across diverse image
understanding tasks. Moreover, VCM enhances visual encoders' capabilities in
classic visual concept perception tasks. Extensive quantitative and qualitative
experiments validate the effectiveness and efficiency of VCM.

</details>

### [196] [A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks](https://arxiv.org/abs/2504.19645)
*Shadan Shukr Sabr,Nazira Sabr Mustafa,Talar Sabah Omar,Salah Hwayyiz Rasool,Nawzad Anwer Omer,Darya Sabir Hamad,Hemin Abdulhameed Shams,Omer Mahmood Kareem,Rozhan Noori Abdullah,Khabat Atar Abdullah,Mahabad Azad Mohammad,Haneen Al-Raghefy,Safar M. Asaad,Sara Jamal Mohammed,Twana Saeed Ali,Fazil Shawrow,Halgurd S. Maghdid*

Main category: cs.CL

TLDR: 该研究为低资源语言中央库尔德语（CKL）设计了一个准确且全面的词性标注集，以提升库尔德语NLP任务的性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏资源，中央库尔德语等低资源语言在NLP领域的研究较少，现有的词性标注集既不标准化也不全面，影响了相关NLP任务的发展。

Method: 研究整合了不同研究和库尔德语言学专家的词性标注，设计了一个标准化的词性标注集，并用于标注大型CKL语料库。

Result: 通过与通用依存框架对比，初步研究表明，提出的词性标注集能更准确地优化库尔德语NLP任务的句子处理。

Conclusion: 该研究为库尔德语NLP任务提供了标准化的词性标注集，有望推动低资源语言的NLP发展。

Abstract: - The field of natural language processing (NLP) has dramatically expanded
within the last decade. Many human-being applications are conducted daily via
NLP tasks, starting from machine translation, speech recognition, text
generation and recommendations, Part-of-Speech tagging (POS), and Named-Entity
Recognition (NER). However, low-resourced languages, such as the
Central-Kurdish language (CKL), mainly remain unexamined due to shortage of
necessary resources to support their development. The POS tagging task is the
base of other NLP tasks; for example, the POS tag set has been used to
standardized languages to provide the relationship between words among the
sentences, followed by machine translation and text recommendation.
Specifically, for the CKL, most of the utilized or provided POS tagsets are
neither standardized nor comprehensive. To this end, this study presented an
accurate and comprehensive POS tagset for the CKL to provide better performance
of the Kurdish NLP tasks. The article also collected most of the POS tags from
different studies as well as from Kurdish linguistic experts to standardized
part-of-speech tags. The proposed POS tagset is designed to annotate a large
CKL corpus and support Kurdish NLP tasks. The initial investigations of this
study via comparison with the Universal Dependencies framework for standard
languages, show that the proposed POS tagset can streamline or correct
sentences more accurately for Kurdish NLP tasks.

</details>

### [197] [Multimodal Conditioned Diffusive Time Series Forecasting](https://arxiv.org/abs/2504.19669)
*Chen Su,Yuanhe Tian,Yan Song*

Main category: cs.CL

TLDR: MCD-TSF是一种多模态条件扩散模型，利用时间戳和文本作为额外指导，提升时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型主要关注单模态数值序列，忽略了时间序列中的多模态信息。

Method: 结合时间戳和文本作为额外指导，建立时间和语义关联，动态对齐数据点。

Result: 在八个领域的真实数据集上表现优异，达到最先进水平。

Conclusion: MCD-TSF通过多模态信息有效提升了时间序列预测的性能。

Abstract: Diffusion models achieve remarkable success in processing images and text,
and have been extended to special domains such as time series forecasting
(TSF). Existing diffusion-based approaches for TSF primarily focus on modeling
single-modality numerical sequences, overlooking the rich multimodal
information in time series data. To effectively leverage such information for
prediction, we propose a multimodal conditioned diffusion model for TSF,
namely, MCD-TSF, to jointly utilize timestamps and texts as extra guidance for
time series modeling, especially for forecasting. Specifically, Timestamps are
combined with time series to establish temporal and semantic correlations among
different data points when aggregating information along the temporal
dimension. Texts serve as supplementary descriptions of time series' history,
and adaptively aligned with data points as well as dynamically controlled in a
classifier-free manner. Extensive experiments on real-world benchmark datasets
across eight domains demonstrate that the proposed MCD-TSF model achieves
state-of-the-art performance.

</details>

### [198] [Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs](https://arxiv.org/abs/2504.19675)
*Osma Suominen,Juho Inkinen,Mona Lehtinen*

Main category: cs.CL

TLDR: Annif系统在SemEval-2025任务5中结合传统NLP与LLM技术，取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 探索如何结合传统自然语言处理与大型语言模型技术，提升多语言环境下主题索引的准确性和效率。

Method: 结合Annif工具包的传统NLP与机器学习技术，以及LLM的翻译、合成数据生成和单语模型预测合并方法。

Result: 在定量评估中排名第一（全主题类别）和第二（TIB核心主题类别），定性评估中排名第四。

Conclusion: 结合传统XMTC算法与现代LLM技术可显著提升多语言主题索引的性能。

Abstract: This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects),
which focussed on subject indexing using large language models (LLMs). The task
required creating subject predictions for bibliographic records from the
bilingual TIBKAT database using the GND subject vocabulary. Our approach
combines traditional natural language processing and machine learning
techniques implemented in the Annif toolkit with innovative LLM-based methods
for translation and synthetic data generation, and merging predictions from
monolingual models. The system ranked first in the all-subjects category and
second in the tib-core-subjects category in the quantitative evaluation, and
fourth in qualitative evaluations. These findings demonstrate the potential of
combining traditional XMTC algorithms with modern LLM techniques to improve the
accuracy and efficiency of subject indexing in multilingual contexts.

</details>

### [199] [Taming the Titans: A Survey of Efficient LLM Inference Serving](https://arxiv.org/abs/2504.19720)
*Ranran Zhen,Juntao Li,Yixin Ji,Zhenlin Yang,Tong Liu,Qingrong Xia,Xinyu Duan,Zhefeng Wang,Baoxing Huai,Min Zhang*

Main category: cs.CL

TLDR: 本文综述了大语言模型（LLM）推理服务中的低延迟和高吞吐量挑战，总结了实例级、集群级和新兴场景的优化方法，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: LLM在生成式AI中表现卓越，但其庞大的参数和高计算需求导致内存和延迟问题，阻碍了高效推理服务的发展。

Method: 通过实例级（如模型放置、请求调度）、集群级（如GPU集群部署、负载均衡）和新兴场景（如特定任务优化）的方法进行综述。

Result: 总结了当前优化LLM推理服务的关键技术和策略，提供了全面的研究进展概述。

Conclusion: 未来研究应进一步探索LLM推理服务的优化方向，以应对实际应用中的挑战。

Abstract: Large Language Models (LLMs) for Generative AI have achieved remarkable
progress, evolving into sophisticated and versatile tools widely adopted across
various domains and applications. However, the substantial memory overhead
caused by their vast number of parameters, combined with the high computational
demands of the attention mechanism, poses significant challenges in achieving
low latency and high throughput for LLM inference services. Recent
advancements, driven by groundbreaking research, have significantly accelerated
progress in this field. This paper provides a comprehensive survey of these
methods, covering fundamental instance-level approaches, in-depth cluster-level
strategies, emerging scenario directions, and other miscellaneous but important
areas. At the instance level, we review model placement, request scheduling,
decoding length prediction, storage management, and the disaggregation
paradigm. At the cluster level, we explore GPU cluster deployment,
multi-instance load balancing, and cloud service solutions. For emerging
scenarios, we organize the discussion around specific tasks, modules, and
auxiliary methods. To ensure a holistic overview, we also highlight several
niche yet critical areas. Finally, we outline potential research directions to
further advance the field of LLM inference serving.

</details>

### [200] [LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding](https://arxiv.org/abs/2504.19734)
*Ying Na,Shihui Feng*

Main category: cs.CL

TLDR: 该研究提出了一种基于LLM的自动化对话数据编码方法，通过角色提示和思维链方法预测对话代码，并利用多模型协作和上下文一致性检查提高准确性。


<details>
  <summary>Details</summary>
Motivation: 对话数据是理解学习过程的重要来源，但LLM在处理复杂上下文时面临挑战，研究旨在解决这一问题。

Method: 1) 基于对话特性（交际行为和事件）预测代码；2) 多LLM协作预测；3) 利用事件和行为关系进行一致性检查。

Result: 上下文一致性检查显著提高准确性，行为预测准确率高于事件预测。

Conclusion: 研究为对话数据自动化编码提供了新方法框架，并解决了上下文分析的挑战。

Abstract: Dialogue data has been a key source for understanding learning processes,
offering critical insights into how students engage in collaborative
discussions and how these interactions shape their knowledge construction. The
advent of Large Language Models (LLMs) has introduced promising opportunities
for advancing qualitative research, particularly in the automated coding of
dialogue data. However, the inherent contextual complexity of dialogue presents
unique challenges for these models, especially in understanding and
interpreting complex contextual information. This study addresses these
challenges by developing a novel LLM-assisted automated coding approach for
dialogue data. The novelty of our proposed framework is threefold: 1) We
predict the code for an utterance based on dialogue-specific characteristics --
communicative acts and communicative events -- using separate prompts following
the role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs
including GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We
leveraged the interrelation between events and acts to implement consistency
checking using GPT-4o. In particular, our contextual consistency checking
provided a substantial accuracy improvement. We also found the accuracy of act
predictions was consistently higher than that of event predictions. This study
contributes a new methodological framework for enhancing the precision of
automated coding of dialogue data as well as offers a scalable solution for
addressing the contextual challenges inherent in dialogue analysis.

</details>

### [201] [Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages in LLMs](https://arxiv.org/abs/2504.19759)
*Huichi Zhou,Zehao Xu,Munan Zhao,Kaihong Li,Yiqiang Li,Hongtao Wang*

Main category: cs.CL

TLDR: 论文介绍了多语言道德推理基准（MMRB），评估大语言模型在五种语言和三种上下文复杂度下的表现，发现性能随复杂度下降，低资源语言影响更大。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在多语言和不同上下文复杂度下的道德推理能力，并探索低资源语言的作用。

Method: 使用MMRB基准测试五种语言和三种复杂度，并对LLaMA-3-8B模型进行微调。

Result: 道德推理性能随复杂度下降，低资源语言（如越南语）影响更大，且对多语言推理有显著作用。

Conclusion: 低资源语言在多语言NLP中具有关键作用，需更多关注。

Abstract: In this paper, we introduce the Multilingual Moral Reasoning Benchmark (MMRB)
to evaluate the moral reasoning abilities of large language models (LLMs)
across five typologically diverse languages and three levels of contextual
complexity: sentence, paragraph, and document. Our results show moral reasoning
performance degrades with increasing context complexity, particularly for
low-resource languages such as Vietnamese. We further fine-tune the open-source
LLaMA-3-8B model using curated monolingual data for alignment and poisoning.
Surprisingly, low-resource languages have a stronger impact on multilingual
reasoning than high-resource ones, highlighting their critical role in
multilingual NLP.

</details>

### [202] [Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance](https://arxiv.org/abs/2504.19811)
*Takuya Tamura,Taro Yano,Masafumi Enomoto,Masafumi Oyamada*

Main category: cs.CL

TLDR: 提出了一种基于谱系关系的矩阵分解方法（LRMF），用于预测大语言模型性能，优于传统方法，并能解决冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 减少大语言模型开发中的计算成本和开发时间，传统方法忽视了模型间的谱系关系。

Method: 提出LRMF框架，利用图拉普拉斯正则化编码模型间的谱系关系。

Result: 在2934个模型和21000+实例上验证，LRMF比基线方法相关性高7-10个百分点。

Conclusion: LRMF为现代大语言模型开发提供了资源高效的性能预测方法。

Abstract: Accurately forecasting the performance of Large Language Models (LLMs) before
extensive fine-tuning or merging can substantially reduce both computational
expense and development time. Although prior approaches like scaling laws
account for global factors such as parameter size or training tokens, they
often overlook explicit lineage relationships - i.e., which models are derived
or merged from which parents. In this work, we propose a novel
Lineage-Regularized Matrix Factorization (LRMF) framework that encodes
ancestral ties among LLMs via a graph Laplacian regularizer. By leveraging
multi-hop parent-child connections, LRMF consistently outperforms conventional
matrix factorization and collaborative filtering methods in both instance-level
and benchmark-level performance prediction. Our large-scale study includes
2,934 publicly available Hugging Face models and 21,000+ instances across 6
major benchmarks, showing that lineage constraints yield up to 7-10 percentage
points higher correlation with actual performance compared to baselines.
Moreover, LRMF effectively addresses the cold-start problem, providing accurate
estimates for newly derived or merged models even with minimal data. This
lineage-guided strategy thus offers a resource-efficient way to inform
hyperparameter tuning, data selection, and model combination in modern LLM
development.

</details>

### [203] [To MT or not to MT: An eye-tracking study on the reception by Dutch readers of different translation and creativity levels](https://arxiv.org/abs/2504.19850)
*Kyo Gerrits,Ana Guerberof-Arenas*

Main category: cs.CL

TLDR: 研究探讨了四种翻译方式（机器翻译、后编辑、人工翻译和原文）对读者认知负荷的影响，发现创造性内容增加认知负荷，且人工翻译效果最显著。


<details>
  <summary>Details</summary>
Motivation: 了解不同翻译方式中创造性和错误如何影响读者的认知负荷。

Method: 八名参与者通过问卷、眼动仪和回顾性有声思维访谈评估翻译故事。

Result: 创造性内容增加认知负荷，人工翻译效果最显著；错误无显著影响。

Conclusion: 翻译创造性对认知负荷的影响是新的发现，为未来研究开辟了新方向。

Abstract: This article presents the results of a pilot study involving the reception of
a fictional short story translated from English into Dutch under four
conditions: machine translation (MT), post-editing (PE), human translation (HT)
and original source text (ST). The aim is to understand how creativity and
errors in different translation modalities affect readers, specifically
regarding cognitive load. Eight participants filled in a questionnaire, read a
story using an eye-tracker, and conducted a retrospective think-aloud (RTA)
interview. The results show that units of creative potential (UCP) increase
cognitive load and that this effect is highest for HT and lowest for MT; no
effect of error was observed. Triangulating the data with RTAs leads us to
hypothesize that the higher cognitive load in UCPs is linked to increases in
reader enjoyment and immersion. The effect of translation creativity on
cognitive load in different translation modalities at word-level is novel and
opens up new avenues for further research. All the code and data are available
at https://github.com/INCREC/Pilot_to_MT_or_not_to_MT

</details>

### [204] [Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language](https://arxiv.org/abs/2504.19856)
*Anastasia Zhukova,Christian E. Matt,Terry Ruas,Bela Gipp*

Main category: cs.CL

TLDR: ICL-APT是一种高效方法，利用上下文学习和k近邻增强目标数据，显著减少GPU时间并保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统DAPT需要大量领域数据的问题，特别是在非英语语言（如德语）的特定领域（如流程工业）中数据难以获取。

Method: 结合上下文学习（ICL）和k近邻（kNN）来增强目标数据，减少计算需求。

Result: ICL-APT比传统DAPT性能提升3.5%（平均IR指标），计算时间减少近4倍。

Conclusion: 该方法为计算资源有限的行业提供了经济高效的解决方案，并适用于其他低资源领域。

Abstract: Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique
that further trains a language model (LM) on its pretraining task, e.g.,
language masking. Although popular, it requires a significant corpus of
domain-related data, which is difficult to obtain for specific domains in
languages other than English, such as the process industry in the German
language. This paper introduces an efficient approach called ICL-augmented
pretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest
neighbors (kNN) to augment target data with domain-related and in-domain texts,
significantly reducing GPU time while maintaining strong model performance. Our
results show that this approach performs better than traditional DAPT by 3.5 of
the average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost 4 times
less computing time, providing a cost-effective solution for industries with
limited computational capacity. The findings highlight the broader
applicability of this framework to other low-resource industries, making
NLP-based solutions more accessible and feasible in production environments.

</details>

### [205] [semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage](https://arxiv.org/abs/2504.19867)
*Ke Hong,Lufang Chen,Zhong Wang,Xiuhong Li,Qiuli Mao,Jianping Ma,Chao Xiong,Guanyu Wu,Buhe Han,Guohao Dai,Yun Liang,Yu Wang*

Main category: cs.CL

TLDR: 论文提出了一种名为semi-PD的新型LLM服务系统，通过分离计算和统一存储解决了现有系统中的存储效率问题，显著提升了服务性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统存在存储效率低下的问题，导致高请求率下性能不佳。

Method: 提出semi-PD系统，采用分离计算和统一存储的设计，引入计算资源控制器和统一内存管理器。

Result: semi-PD在高请求率下保持更低延迟，性能优于现有系统。

Conclusion: semi-PD通过优化存储和计算资源分配，显著提升了LLM服务系统的效率和性能。

Abstract: Existing large language model (LLM) serving systems fall into two categories:
1) a unified system where prefill phase and decode phase are co-located on the
same GPU, sharing the unified computational resource and storage, and 2) a
disaggregated system where the two phases are disaggregated to different GPUs.
The design of the disaggregated system addresses the latency interference and
sophisticated scheduling issues in the unified system but leads to storage
challenges including 1) replicated weights for both phases that prevent
flexible deployment, 2) KV cache transfer overhead between the two phases, 3)
storage imbalance that causes substantial wasted space of the GPU capacity, and
4) suboptimal resource adjustment arising from the difficulties in migrating KV
cache. Such storage inefficiency delivers poor serving performance under high
request rates.
  In this paper, we identify that the advantage of the disaggregated system
lies in the disaggregated computation, i.e., partitioning the computational
resource to enable the asynchronous computation of two phases. Thus, we propose
a novel LLM serving system, semi-PD, characterized by disaggregated computation
and unified storage. In semi-PD, we introduce a computation resource controller
to achieve disaggregated computation at the streaming multi-processor (SM)
level, and a unified memory manager to manage the asynchronous memory access
from both phases. semi-PD has a low-overhead resource adjustment mechanism
between the two phases, and a service-level objective (SLO) aware dynamic
partitioning algorithm to optimize the SLO attainment. Compared to
state-of-the-art systems, semi-PD maintains lower latency at higher request
rates, reducing the average end-to-end latency per request by 1.27-2.58x on
DeepSeek series models, and serves 1.55-1.72x more requests adhering to latency
constraints on Llama series models.

</details>

### [206] [GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets](https://arxiv.org/abs/2504.19898)
*Mingqian He,Fei Zhao,Chonggang Lu,Ziyan Liu,Yue Wang,Haofu Qian*

Main category: cs.CL

TLDR: GenCLS++是一个联合优化监督微调（SFT）和强化学习（RL）的框架，通过系统探索五种策略维度提升生成式文本分类器的性能，平均准确率提升3.46%。


<details>
  <summary>Details</summary>
Motivation: 传统判别方法忽视了大型语言模型（LLMs）的生成能力，而现有生成分类方法仅依赖简单SFT，未充分利用RL和训练与推理提示的交互。

Method: GenCLS++联合优化SFT和RL，探索五种策略维度（如上下文学习变体、类别定义等），并在SFT后进行RL优化。

Result: 在七个数据集上，GenCLS++平均准确率提升3.46%，公开数据集上提升4.00%。

Conclusion: 分类任务无需显式推理步骤，这一发现为未来LLM应用提供了重要指导。

Abstract: As a fundamental task in machine learning, text classification plays a
crucial role in many areas. With the rapid scaling of Large Language Models
(LLMs), particularly through reinforcement learning (RL), there is a growing
need for more capable discriminators. Consequently, advances in classification
are becoming increasingly vital for enhancing the overall capabilities of LLMs.
Traditional discriminative methods map text to labels but overlook LLMs'
intrinsic generative strengths. Generative classification addresses this by
prompting the model to directly output labels. However, existing studies still
rely on simple SFT alone, seldom probing the interplay between training and
inference prompts, and no work has systematically leveraged RL for generative
text classifiers and unified SFT, RL, and inference-time prompting in one
framework. We bridge this gap with GenCLS++, a framework that jointly optimizes
SFT and RL while systematically exploring five high-level strategy
dimensions-in-context learning variants, category definitions, explicit
uncertainty labels, semantically irrelevant numeric labels, and
perplexity-based decoding-during both training and inference. After an SFT
"policy warm-up," we apply RL with a simple rule-based reward, yielding sizable
extra gains. Across seven datasets, GenCLS++ achieves an average accuracy
improvement of 3.46% relative to the naive SFT baseline; on public datasets,
this improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that
benefit from explicit thinking processes, we find that classification tasks
perform better without such reasoning steps. These insights into the role of
explicit reasoning provide valuable guidance for future LLM applications.

</details>

### [207] [Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking](https://arxiv.org/abs/2504.19940)
*Luigia Costabile,Gian Marco Orlando,Valerio La Gatta,Vincenzo Moscato*

Main category: cs.CL

TLDR: 论文探讨了利用大型语言模型（LLM）生成代理在众包事实核查任务中的潜力，发现其比人类众包表现更优，且更少受偏见影响。


<details>
  <summary>Details</summary>
Motivation: 在线错误信息的泛滥需要可扩展且可靠的事实核查解决方案，众包事实核查虽成本低但存在质量和偏见问题，而LLM在此领域的潜力尚未被探索。

Method: 使用La Barbera等人的协议，模拟具有多样化人口和意识形态特征的生成代理群，代理执行证据检索、多维质量评估和最终真实性判断。

Result: 代理群在真实性分类上优于人类群，内部一致性更高，且更少受社会和认知偏见影响，决策过程更结构化。

Conclusion: 生成代理可作为众包事实核查系统中可扩展、一致且偏见较少的贡献者。

Abstract: The growing spread of online misinformation has created an urgent need for
scalable, reliable fact-checking solutions. Crowdsourced fact-checking - where
non-experts evaluate claim veracity - offers a cost-effective alternative to
expert verification, despite concerns about variability in quality and bias.
Encouraged by promising results in certain contexts, major platforms such as X
(formerly Twitter), Facebook, and Instagram have begun shifting from
centralized moderation to decentralized, crowd-based approaches.
  In parallel, advances in Large Language Models (LLMs) have shown strong
performance across core fact-checking tasks, including claim detection and
evidence evaluation. However, their potential role in crowdsourced workflows
remains unexplored. This paper investigates whether LLM-powered generative
agents - autonomous entities that emulate human behavior and decision-making -
can meaningfully contribute to fact-checking tasks traditionally reserved for
human crowds. Using the protocol of La Barbera et al. (2024), we simulate
crowds of generative agents with diverse demographic and ideological profiles.
Agents retrieve evidence, assess claims along multiple quality dimensions, and
issue final veracity judgments.
  Our results show that agent crowds outperform human crowds in truthfulness
classification, exhibit higher internal consistency, and show reduced
susceptibility to social and cognitive biases. Compared to humans, agents rely
more systematically on informative criteria such as Accuracy, Precision, and
Informativeness, suggesting a more structured decision-making process. Overall,
our findings highlight the potential of generative agents as scalable,
consistent, and less biased contributors to crowd-based fact-checking systems.

</details>

### [208] [TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons](https://arxiv.org/abs/2504.19982)
*Emre Can Acikgoz,Carl Guo,Suvodip Dey,Akul Datta,Takyoung Kim,Gokhan Tur,Dilek Hakkani-Tür*

Main category: cs.CL

TLDR: TD-EVAL是一个两阶段评估框架，结合细粒度的轮次级分析和整体对话级比较，用于评估任务导向对话系统。


<details>
  <summary>Details</summary>
Motivation: 传统自动评估方法无法检测用户与代理交互中的关键中间错误，需要更精细的评估框架。

Method: TD-EVAL包括轮次级评估（对话连贯性、后端知识一致性和策略合规性）和对话级评估（TOD Agent Arena的成对比较）。

Result: 实验表明TD-EVAL能有效识别传统指标遗漏的对话错误，且比传统和基于LLM的指标更符合人类判断。

Conclusion: TD-EVAL为任务导向对话系统评估提供了新范式，支持轮次和系统级评估，具有即插即用的灵活性。

Abstract: Task-oriented dialogue (TOD) systems are experiencing a revolution driven by
Large Language Models (LLMs), yet the evaluation methodologies for these
systems remain insufficient for their growing sophistication. While traditional
automatic metrics effectively assessed earlier modular systems, they focus
solely on the dialogue level and cannot detect critical intermediate errors
that can arise during user-agent interactions. In this paper, we introduce
TD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework
that unifies fine-grained turn-level analysis with holistic dialogue-level
comparisons. At turn level, we evaluate each response along three TOD-specific
dimensions: conversation cohesion, backend knowledge consistency, and policy
compliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons
to provide a measure of dialogue-level quality. Through experiments on MultiWOZ
2.4 and {\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the
conversational errors that conventional metrics miss. Furthermore, TD-EVAL
exhibits better alignment with human judgments than traditional and LLM-based
metrics. These findings demonstrate that TD-EVAL introduces a new paradigm for
TOD system evaluation, efficiently assessing both turn and system levels with a
plug-and-play framework for future research.

</details>

### [209] [Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom](https://arxiv.org/abs/2504.20000)
*Rishika Sen,Sujoy Roychowdhury,Sumit Soman,H. G. Ranjani,Srikhetra Mohanty*

Main category: cs.CL

TLDR: 研究知识蒸馏（KD）在电信领域问答任务中的应用，探讨教师模型、学生模型或两者的监督微调（SFT）对蒸馏效果的影响。实验表明，教师模型的SFT在相同词汇表时能提升蒸馏效果，而两者均进行SFT效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨在领域特定任务中，教师模型、学生模型或两者的监督微调（SFT）对知识蒸馏效果的影响，特别是在电信领域的问答任务中。

Method: 通过实验比较教师模型、学生模型或两者的SFT对知识蒸馏的影响，并研究词汇表和KD算法（如vanilla KD和DSKD）的作用，使用14种不同指标进行评估。

Result: 教师模型的SFT在相同词汇表时能提升蒸馏效果，而两者均进行SFT效果最佳，但统计显著性取决于教师模型的词汇表。

Conclusion: 在电信领域问答任务中，教师模型和学生模型的SFT均能提升知识蒸馏效果，教师模型的词汇表对结果有重要影响。

Abstract: Knowledge Distillation (KD) is one of the approaches to reduce the size of
Large Language Models (LLMs). A LLM with smaller number of model parameters
(student) is trained to mimic the performance of a LLM of a larger size
(teacher model) on a specific task. For domain-specific tasks, it is not clear
if teacher or student model, or both, must be considered for domain adaptation.
In this work, we study this problem from perspective of telecom domain
Question-Answering (QA) task. We systematically experiment with Supervised
Fine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to
KD. We design experiments to study the impact of vocabulary (same and
different) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the
distilled model. Multi-faceted evaluation of the distillation using 14
different metrics (N-gram, embedding and LLM-based metrics) is considered.
Experimental results show that SFT of teacher improves performance of distilled
model when both models have same vocabulary, irrespective of algorithm and
metrics. Overall, SFT of both teacher and student results in better performance
across all metrics, although the statistical significance of the same depends
on the vocabulary of the teacher models.

</details>

### [210] [LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation](https://arxiv.org/abs/2504.20013)
*Beizhe Hu,Qiang Sheng,Juan Cao,Yang Li,Danding Wang*

Main category: cs.CL

TLDR: 研究探讨了大型语言模型（LLM）生成的假新闻对新闻推荐系统的影响，发现其会导致真实新闻在排名中逐渐失去优势（真相衰减现象），并提出了可能的应对措施。


<details>
  <summary>Details</summary>
Motivation: 在线假新闻治理面临LLM恶意生成假新闻的新挑战，现有研究未充分探讨其大规模发布对新闻生态系统的影响。

Method: 开发了模拟管道和包含约56k条多样化生成新闻的数据集，研究LLM生成假新闻在神经新闻推荐系统中的影响。

Result: 发现真相衰减现象，即真实新闻在排名中逐渐落后于假新闻；从熟悉度角度解释了现象，并显示困惑度与新闻排名正相关。

Conclusion: LLM生成的假新闻威胁新闻生态系统完整性，呼吁利益相关者采取应对措施。

Abstract: Online fake news moderation now faces a new challenge brought by the
malicious use of large language models (LLMs) in fake news production. Though
existing works have shown LLM-generated fake news is hard to detect from an
individual aspect, it remains underexplored how its large-scale release will
impact the news ecosystem. In this study, we develop a simulation pipeline and
a dataset with ~56k generated news of diverse types to investigate the effects
of LLM-generated fake news within neural news recommendation systems. Our
findings expose a truth decay phenomenon, where real news is gradually losing
its advantageous position in news ranking against fake news as LLM-generated
news is involved in news recommendation. We further provide an explanation
about why truth decay occurs from a familiarity perspective and show the
positive correlation between perplexity and news ranking. Finally, we discuss
the threats of LLM-generated fake news and provide possible countermeasures. We
urge stakeholders to address this emerging challenge to preserve the integrity
of news ecosystems.

</details>

### [211] [Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages](https://arxiv.org/abs/2504.20022)
*Pritika Rohera,Chaitrali Ginimav,Gayatri Sawant,Raviraj Joshi*

Main category: cs.CL

TLDR: 研究评估了多语言大语言模型（LLMs）在英语和印度语言中的事实准确性，发现模型在英语中表现更好，低资源印度语言中更容易出现幻觉。


<details>
  <summary>Details</summary>
Motivation: 探究多语言LLMs在低资源语言（如印度语言）中的事实准确性，尤其是与英语表现的对比。

Method: 使用IndicQuest数据集，比较GPT-4o、Gemma-2-9B、Gemma-2-2B和Llama-3.1-8B在英语和19种印度语言中的表现。

Result: LLMs在英语中表现更优，低资源印度语言中更容易出现幻觉。

Conclusion: 当前多语言LLMs在低资源语言中的理解和事实准确性仍存在挑战。

Abstract: Multilingual Large Language Models (LLMs) have demonstrated significant
effectiveness across various languages, particularly in high-resource languages
such as English. However, their performance in terms of factual accuracy across
other low-resource languages, especially Indic languages, remains an area of
investigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,
Gemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in
English and Indic languages using the IndicQuest dataset, which contains
question-answer pairs in English and 19 Indic languages. By asking the same
questions in English and their respective Indic translations, we analyze
whether the models are more reliable for regional context questions in Indic
languages or when operating in English. Our findings reveal that LLMs often
perform better in English, even for questions rooted in Indic contexts.
Notably, we observe a higher tendency for hallucination in responses generated
in low-resource Indic languages, highlighting challenges in the multilingual
understanding capabilities of current LLMs.

</details>

### [212] [AutoJudge: Judge Decoding Without Manual Annotation](https://arxiv.org/abs/2504.20039)
*Roman Garipov,Fedor Velikonivtsev,Ruslan Svirschevski,Vage Egiazarian,Max Ryabinin*

Main category: cs.CL

TLDR: AutoJudge框架通过任务特定的有损推测解码加速LLM推理，通过识别不影响下游质量的“不重要”令牌来提升速度。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码需要逐令牌匹配目标模型输出分布，限制了推理速度。AutoJudge旨在通过放松对不重要令牌的匹配要求来加速推理。

Method: 采用半贪婪搜索算法识别需纠正的令牌不匹配，并训练轻量级分类器预测可安全接受的令牌。

Result: 在GSM8K推理任务中，AutoJudge比标准推测解码多接受1.5倍令牌，准确率下降小于1%；在编程任务中也能泛化。

Conclusion: AutoJudge通过选择性令牌匹配显著提升推理速度，且能跨任务泛化。

Abstract: We introduce AutoJudge, a framework that accelerates large language model
(LLM) inference with task-specific lossy speculative decoding. Instead of
matching the original model output distribution token-by-token, we identify
which of the generated tokens affect the downstream quality of the generated
response, relaxing the guarantee so that the "unimportant" tokens can be
generated faster. Our approach relies on a semi-greedy search algorithm to test
which of the mismatches between target and draft model should be corrected to
preserve quality, and which ones may be skipped. We then train a lightweight
classifier based on existing LLM embeddings to predict, at inference time,
which mismatching tokens can be safely accepted without compromising the final
answer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B
(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more
accepted tokens per verification cycle with under 1% degradation in answer
accuracy compared to standard speculative decoding and over 2x with small loss
in accuracy. When applied to the LiveCodeBench benchmark, our approach
automatically detects other, programming-specific important tokens and shows
similar speedups, demonstrating its ability to generalize across tasks.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [213] [BELL: Benchmarking the Explainability of Large Language Models](https://arxiv.org/abs/2504.18572)
*Syed Quiser Ahmed,Bharathi Vokkaliga Ganesh,Jagadish Babu P,Karthick Selvaraj,ReddySiva Naga Parvathi Devi,Sravya Kappala*

Main category: cs.AI

TLDR: 提出了一种标准化基准测试方法，用于评估大语言模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的决策过程缺乏透明度，引发了对信任、偏见和性能的担忧。

Method: 引入了一种标准化基准测试技术，名为“大语言模型可解释性基准测试”。

Result: 未明确提及具体结果，但目标是评估模型的可解释性。

Conclusion: 评估大语言模型的可解释性对解决透明度和信任问题至关重要。

Abstract: Large Language Models have demonstrated remarkable capabilities in natural
language processing, yet their decision-making processes often lack
transparency. This opaqueness raises significant concerns regarding trust,
bias, and model performance. To address these issues, understanding and
evaluating the interpretability of LLMs is crucial. This paper introduces a
standardised benchmarking technique, Benchmarking the Explainability of Large
Language Models, designed to evaluate the explainability of large language
models.

</details>

### [214] [A Cognitive-Mechanistic Human Reliability Analysis Framework: A Nuclear Power Plant Case Study](https://arxiv.org/abs/2504.18604)
*Xingyu Xiao,Peng Chen,Jiejuan Tong,Shunshun Liu,Hongru Zhao,Jun Zhao,Qianqian Jia,Jingang Liang,Haitao Wang*

Main category: cs.AI

TLDR: 提出了一种结合认知模型和生成对抗网络的框架（COGMIF），用于改进传统HRA方法，通过数字孪生和合成数据提升人误概率评估的可扩展性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统HRA方法依赖专家判断和规则，忽略了认知基础，且实验成本高，难以适应新型核电站需求。

Method: 整合ACT-R认知模型和TimeGAN生成对抗网络，模拟操作员行为并生成合成数据，驱动IDHEAS-ECA评估。

Result: COGMIF在SPAR-H对比和敏感性分析中表现出鲁棒性和实用性，量化了操作风险的关键因素。

Conclusion: COGMIF为认知理论融入工业HRA实践提供了高效可信的途径。

Abstract: Traditional human reliability analysis (HRA) methods, such as IDHEAS-ECA,
rely on expert judgment and empirical rules that often overlook the cognitive
underpinnings of human error. Moreover, conducting human-in-the-loop
experiments for advanced nuclear power plants is increasingly impractical due
to novel interfaces and limited operational data. This study proposes a
cognitive-mechanistic framework (COGMIF) that enhances the IDHEAS-ECA
methodology by integrating an ACT-R-based human digital twin (HDT) with
TimeGAN-augmented simulation. The ACT-R model simulates operator cognition,
including memory retrieval, goal-directed procedural reasoning, and
perceptual-motor execution, under high-fidelity scenarios derived from a
high-temperature gas-cooled reactor (HTGR) simulator. To overcome the resource
constraints of large-scale cognitive modeling, TimeGAN is trained on
ACT-R-generated time-series data to produce high-fidelity synthetic operator
behavior datasets. These simulations are then used to drive IDHEAS-ECA
assessments, enabling scalable, mechanism-informed estimation of human error
probabilities (HEPs). Comparative analyses with SPAR-H and sensitivity
assessments demonstrate the robustness and practical advantages of the proposed
COGMIF. Finally, procedural features are mapped onto a Bayesian network to
quantify the influence of contributing factors, revealing key drivers of
operational risk. This work offers a credible and computationally efficient
pathway to integrate cognitive theory into industrial HRA practices.

</details>

### [215] [Research on Personalized Medical Intervention Strategy Generation System based on Group Relative Policy Optimization and Time-Series Data Fusion](https://arxiv.org/abs/2504.18631)
*Dingxin Lu,Shurui Wu,Xinyi Huang*

Main category: cs.AI

TLDR: 提出了一种基于GRPO和时间序列数据融合的个性化医疗干预策略生成系统，通过多源数据融合和优化算法提升决策效果。


<details>
  <summary>Details</summary>
Motivation: 医疗领域需要基于高维异构时间序列信息生成个性化干预方案，但现有方法在准确性和覆盖性上存在不足。

Method: 结合GRPO和多层神经网络进行策略优化，使用多通道神经网络和自注意力机制进行动态特征提取，并通过遗传算法和蒙特卡洛树搜索进行全局优化。

Result: 实验结果表明，该方法在准确性、覆盖性和决策效益上显著优于现有方法。

Conclusion: 该系统为个性化医疗干预提供了高效、可解释的解决方案。

Abstract: With the timely formation of personalized intervention plans based on
high-dimensional heterogeneous time series information becoming an important
challenge in the medical field today, electronic medical records, wearables,
and other multi-source medical data are increasingly generated and diversified.
In this work, we develop a system to generate personalized medical intervention
strategies based on Group Relative Policy Optimization (GRPO) and Time-Series
Data Fusion. First, by incorporating relative policy constraints among the
groups during policy gradient updates, we adaptively balance individual and
group gains. To improve the robustness and interpretability of decision-making,
a multi-layer neural network structure is employed to group-code patient
characteristics. Second, for the rapid multi-modal fusion of multi-source
heterogeneous time series, a multi-channel neural network combined with a
self-attention mechanism is used for dynamic feature extraction. Key feature
screening and aggregation are achieved through a differentiable gating network.
Finally, a collaborative search process combining a genetic algorithm and Monte
Carlo tree search is proposed to find the ideal intervention strategy,
achieving global optimization. Experimental results show significant
improvements in accuracy, coverage, and decision-making benefits compared with
existing methods.

</details>

### [216] [Exploring a Large Language Model for Transforming Taxonomic Data into OWL: Lessons Learned and Implications for Ontology Development](https://arxiv.org/abs/2504.18651)
*Filipi Miranda Soares,Antonio Mauro Saraiva,Luís Ferreira Pires,Luiz Olavo Bonino da Silva Santos,Dilvan de Abreu Moreira,Fernando Elias Corrêa,Kelly Rosa Braghetto,Debora Pignatari Drucker,Alexandre Cláudio Botazzo Delbem*

Main category: cs.AI

TLDR: 论文探讨了利用ChatGPT-4自动化管理物种分类中的科学名称，通过两种方法生成OWL文件，并比较了其优缺点。


<details>
  <summary>Details</summary>
Motivation: 由于物种分类的不断变化，手动维护科学名称变得困难，需要自动化解决方案。

Method: 使用ChatGPT-4从GBIF Backbone API提取数据并生成OWL文件，尝试了两种方法：直接提示和设计Python算法。

Result: 第一种方法扩展性有限，第二种方法解决了扩展性问题但存在数据处理的拼写错误。

Conclusion: 大型语言模型如ChatGPT-4在物种名称管理中有潜力，尽管存在限制，但能提升本体开发的效率。

Abstract: Managing scientific names in ontologies that represent species taxonomies is
challenging due to the ever-evolving nature of these taxonomies. Manually
maintaining these names becomes increasingly difficult when dealing with
thousands of scientific names. To address this issue, this paper investigates
the use of ChatGPT-4 to automate the development of the :Organism module in the
Agricultural Product Types Ontology (APTO) for species classification. Our
methodology involved leveraging ChatGPT-4 to extract data from the GBIF
Backbone API and generate OWL files for further integration in APTO. Two
alternative approaches were explored: (1) issuing a series of prompts for
ChatGPT-4 to execute tasks via the BrowserOP plugin and (2) directing ChatGPT-4
to design a Python algorithm to perform analogous tasks. Both approaches rely
on a prompting method where we provide instructions, context, input data, and
an output indicator. The first approach showed scalability limitations, while
the second approach used the Python algorithm to overcome these challenges, but
it struggled with typographical errors in data handling. This study highlights
the potential of Large language models like ChatGPT-4 to streamline the
management of species names in ontologies. Despite certain limitations, these
tools offer promising advancements in automating taxonomy-related tasks and
improving the efficiency of ontology development.

</details>

### [217] [Proof-of-TBI -- Fine-Tuned Vision Language Model Consortium and OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild Traumatic Brain Injury (TBI) Prediction](https://arxiv.org/abs/2504.18671)
*Ross Gore,Eranga Bandara,Sachin Shetty,Alberto E. Musto,Pratip Rana,Ambrosio Valencia-Romero,Christopher Rhea,Lobat Tayebi,Heather Richter,Atmaram Yarlagadda,Donna Edmonds,Steven Wallace,Donna Broshek*

Main category: cs.AI

TLDR: 论文提出了一种名为Proof-of-TBI的系统，通过整合多个微调的视觉语言模型和OpenAI-o3推理大语言模型（LLM），用于轻度创伤性脑损伤（TBI）的诊断支持。


<details>
  <summary>Details</summary>
Motivation: 轻度TBI的诊断因症状在医学影像中表现微妙且模糊而具有挑战性，需要更准确的诊断方法。

Method: 系统通过微调多个视觉语言模型，结合OpenAI-o3推理LLM进行预测聚合和最终诊断决策。

Result: 原型系统展示了结合视觉语言模型和推理LLM的潜力，能够实现高精度、可靠且自动化的TBI预测。

Conclusion: 该研究首次将微调的视觉语言模型与推理LLM结合用于TBI预测，具有创新性和应用潜力。

Abstract: Mild Traumatic Brain Injury (TBI) detection presents significant challenges
due to the subtle and often ambiguous presentation of symptoms in medical
imaging, making accurate diagnosis a complex task. To address these challenges,
we propose Proof-of-TBI, a medical diagnosis support system that integrates
multiple fine-tuned vision-language models with the OpenAI-o3 reasoning large
language model (LLM). Our approach fine-tunes multiple vision-language models
using a labeled dataset of TBI MRI scans, training them to diagnose TBI
symptoms effectively. The predictions from these models are aggregated through
a consensus-based decision-making process. The system evaluates the predictions
from all fine-tuned vision language models using the OpenAI-o3 reasoning LLM, a
model that has demonstrated remarkable reasoning performance, to produce the
most accurate final diagnosis. The LLM Agents orchestrates interactions between
the vision-language models and the reasoning LLM, managing the final
decision-making process with transparency, reliability, and automation. This
end-to-end decision-making workflow combines the vision-language model
consortium with the OpenAI-o3 reasoning LLM, enabled by custom prompt
engineering by the LLM agents. The prototype for the proposed platform was
developed in collaboration with the U.S. Army Medical Research team in Newport
News, Virginia, incorporating five fine-tuned vision-language models. The
results demonstrate the transformative potential of combining fine-tuned
vision-language model inputs with the OpenAI-o3 reasoning LLM to create a
robust, secure, and highly accurate diagnostic system for mild TBI prediction.
To the best of our knowledge, this research represents the first application of
fine-tuned vision-language models integrated with a reasoning LLM for TBI
prediction tasks.

</details>

### [218] [Transformational Creativity in Science: A Graphical Theory](https://arxiv.org/abs/2504.18687)
*Samuel Schapiro,Jonah Black,Lav R. Varshney*

Main category: cs.AI

TLDR: 论文提出了一种图形理论，用于解释变革性科学创造力，结合了Boden和Kuhn的理论，证明了修改图形模型的公理具有最大的变革潜力，并通过历史实例验证了框架。


<details>
  <summary>Details</summary>
Motivation: 研究变革性科学创造力的机制，结合Boden的“约束条件变化”和Kuhn的“范式转换”理论，提出统一的图形理论。

Method: 构建图形模型，分析修改公理对变革潜力的影响，并通过历史实例验证模型的有效性。

Result: 证明修改公理具有最大的变革潜力，框架能够解释历史上的变革性创造力实例。

Conclusion: 图形理论为理解变革性科学创造力提供了新视角，验证了其理论价值和实用性。

Abstract: Creative processes are typically divided into three types: combinatorial,
exploratory, and transformational. Here, we provide a graphical theory of
transformational scientific creativity, synthesizing Boden's insight that
transformational creativity arises from changes in the "enabling constraints"
of a conceptual space and Kuhn's structure of scientific revolutions as
resulting from paradigm shifts. We prove that modifications made to axioms of
our graphical model have the most transformative potential and then illustrate
how several historical instances of transformational creativity can be captured
by our framework.

</details>

### [219] [A Vision for Auto Research with LLM Agents](https://arxiv.org/abs/2504.18765)
*Chengwei Liu,Chong Wang,Jiayue Cao,Jingquan Ge,Kun Wang,Lvye Zhang,Ming-Ming Cheng,Penghai Zhao,Tianlin Li,Xiaojun Jia,Xiang Li,Xinfeng Li,Yang Liu,Yebo Feng,Yihao Huang,Yijia Xu,Yuqiang Sun,Zhenhong Zhou,Zhengzi Xu*

Main category: cs.AI

TLDR: 本文介绍了一种基于多智能体的自动化研究框架，旨在通过LLMs和模块化协作优化科研全生命周期。


<details>
  <summary>Details</summary>
Motivation: 解决科研流程碎片化、方法学能力不均和认知过载问题。

Method: 利用多智能体协作和LLMs覆盖文献综述、实验设计、论文写作等科研阶段。

Result: 初步探索表明该框架可行，为AI驱动的自优化科研提供了新范式。

Conclusion: 该框架为系统性、可扩展的科研自动化提供了潜力。

Abstract: This paper introduces Agent-Based Auto Research, a structured multi-agent
framework designed to automate, coordinate, and optimize the full lifecycle of
scientific research. Leveraging the capabilities of large language models
(LLMs) and modular agent collaboration, the system spans all major research
phases, including literature review, ideation, methodology planning,
experimentation, paper writing, peer review response, and dissemination. By
addressing issues such as fragmented workflows, uneven methodological
expertise, and cognitive overload, the framework offers a systematic and
scalable approach to scientific inquiry. Preliminary explorations demonstrate
the feasibility and potential of Auto Research as a promising paradigm for
self-improving, AI-driven research processes.

</details>

### [220] [Evaluating AI-Driven Automated Map Digitization in QGIS](https://arxiv.org/abs/2504.18777)
*Diana Febrita*

Main category: cs.AI

TLDR: 研究评估了AI工具Deepness在自动化地图数字化中的效果，并与OpenStreetMap（OSM）的手动数字化结果进行对比。


<details>
  <summary>Details</summary>
Motivation: 传统地图数字化需要大量人工干预，而AI技术（如Deepness）可能提供更高效的替代方案。

Method: 使用Deepness插件在QGIS中对Google Earth影像进行AI数字化，并与OSM的手动数字化结果比较。

Result: 研究分析了AI数字化与手动数字化的性能差异。

Conclusion: 评估了Deepness在自动化地图数字化中的潜力与效果。

Abstract: Map digitization is an important process that converts maps into digital
formats that can be used for further analysis. This process typically requires
a deep human involvement because of the need for interpretation and
decision-making when translating complex features. With the advancement of
artificial intelligence, there is an alternative to conducting map digitization
with the help of machine learning techniques. Deepness, or Deep Neural Remote
Sensing, is an advanced AI-driven tool designed and integrated as a plugin in
QGIS application. This research focuses on assessing the effectiveness of
Deepness in automated digitization. This study analyses AI-generated
digitization results from Google Earth imagery and compares them with digitized
outputs from OpenStreetMap (OSM) to evaluate performance.

</details>

### [221] [Hierarchical Reinforcement Learning in Multi-Goal Spatial Navigation with Autonomous Mobile Robots](https://arxiv.org/abs/2504.18794)
*Brendon Johnson,Alfredo Weitzenfeld*

Main category: cs.AI

TLDR: 该研究对比了分层强化学习（HRL）与传统强化学习在复杂导航任务中的表现，重点分析了HRL的子目标生成和终止功能特性。


<details>
  <summary>Details</summary>
Motivation: 利用HRL解决机器人学习任务中稀疏奖励的问题，并验证其相对于传统强化学习的优势。

Method: 通过实验比较HRL与PPO算法，测试不同子目标生成方式（手动与自动）及终止频率对性能的影响。

Result: 实验结果表明HRL在复杂任务中具有优势，特别是在子目标生成和终止功能方面。

Conclusion: HRL在复杂导航任务中表现优于传统强化学习，其子目标生成和终止功能是关键优势。

Abstract: Hierarchical reinforcement learning (HRL) is hypothesized to be able to take
advantage of the inherent hierarchy in robot learning tasks with sparse reward
schemes, in contrast to more traditional reinforcement learning algorithms. In
this research, hierarchical reinforcement learning is evaluated and contrasted
with standard reinforcement learning in complex navigation tasks. We evaluate
unique characteristics of HRL, including their ability to create sub-goals and
the termination function. We constructed experiments to test the differences
between PPO and HRL, different ways of creating sub-goals, manual vs automatic
sub-goal creation, and the effects of the frequency of termination on
performance. These experiments highlight the advantages of HRL and how it
achieves these advantages.

</details>

### [222] [Generative to Agentic AI: Survey, Conceptualization, and Challenges](https://arxiv.org/abs/2504.18875)
*Johannes Schneider*

Main category: cs.AI

TLDR: 本文比较了生成式AI（GenAI）与代理式AI（Agentic AI），探讨了代理式AI如何弥补GenAI的不足，并分析了其潜在应用与挑战。


<details>
  <summary>Details</summary>
Motivation: 由于代理式AI与GenAI的区别尚不明确，本文旨在通过文献综述填补这一认知空白，帮助学术界和工业界理解代理式AI的独特能力与潜在风险。

Method: 文章分为两部分：第一部分通过文献比较GenAI与代理式AI的特性与进化路径；第二部分深入探讨代理式AI的新颖特性、实际应用及未来研究方向。

Result: 代理式AI在推理与交互能力上显著优于GenAI，能够实现更复杂的任务自主性，但也带来了超越人类智能的潜在风险。

Conclusion: 代理式AI代表了AI的下一步进化方向，但需谨慎应对其挑战与风险，未来研究应关注其定义、应用及伦理问题。

Abstract: Agentic Artificial Intelligence (AI) builds upon Generative AI (GenAI). It
constitutes the next major step in the evolution of AI with much stronger
reasoning and interaction capabilities that enable more autonomous behavior to
tackle complex tasks. Since the initial release of ChatGPT (3.5), Generative AI
has seen widespread adoption, giving users firsthand experience. However, the
distinction between Agentic AI and GenAI remains less well understood. To
address this gap, our survey is structured in two parts. In the first part, we
compare GenAI and Agentic AI using existing literature, discussing their key
characteristics, how Agentic AI remedies limitations of GenAI, and the major
steps in GenAI's evolution toward Agentic AI. This section is intended for a
broad audience, including academics in both social sciences and engineering, as
well as industry professionals. It provides the necessary insights to
comprehend novel applications that are possible with Agentic AI but not with
GenAI. In the second part, we deep dive into novel aspects of Agentic AI,
including recent developments and practical concerns such as defining agents.
Finally, we discuss several challenges that could serve as a future research
agenda, while cautioning against risks that can emerge when exceeding human
intelligence.

</details>

### [223] [Reshaping MOFs Text Mining with a Dynamic Multi-Agent Framework of Large Language Agents](https://arxiv.org/abs/2504.18880)
*Zuhong Lin,Daoyuan Ren,Kai Ran,Sun Jing,Xiaotiang Huang,Haiyang He,Pengxu Pan,Xiaohang Zhang,Ying Fang,Tianying Wang,Minli Wu,Zhanglin Li,Xiaochuan Zhang,Haipu Li,Jingjing Yao*

Main category: cs.AI

TLDR: 利用大型语言模型（LLM）开发了MOFh6工具，用于优化金属有机框架（MOF）的合成条件查询和分析。


<details>
  <summary>Details</summary>
Motivation: MOF合成条件多样且复杂，传统方法难以高效筛选最优条件，LLM为解决这一问题提供了新思路。

Method: 通过整合MOF相关代理（合成、属性和化学信息代理），利用gpt-4o-mini作为核心代理，开发了MOFh6工具，支持多种查询格式。

Result: MOFh6能够分析用户查询，提供最优合成条件，并生成密度泛函理论预建模文件。

Conclusion: MOFh6有望显著提升MOF合成研究的效率，为研究人员提供便利。

Abstract: The mining of synthesis conditions for metal-organic frameworks (MOFs) is a
significant focus in materials science. However, identifying the precise
synthesis conditions for specific MOFs within the vast array of possibilities
presents a considerable challenge. Large Language Models (LLMs) offer a
promising solution to this problem. We leveraged the capabilities of LLMs,
specifically gpt-4o-mini, as core agents to integrate various MOF-related
agents, including synthesis, attribute, and chemical information agents. This
integration culminated in the development of MOFh6, an LLM tool designed to
streamline the MOF synthesis process. MOFh6 allows users to query in multiple
formats, such as submitting scientific literature, or inquiring about specific
MOF codes or structural properties. The tool analyzes these queries to provide
optimal synthesis conditions and generates model files for density functional
theory pre modeling. We believe MOFh6 will enhance efficiency in the MOF
synthesis of all researchers.

</details>

### [224] [Use of Metric Learning for the Recognition of Handwritten Digits, and its Application to Increase the Outreach of Voice-based Communication Platforms](https://arxiv.org/abs/2504.18948)
*Devesh Pant,Dibyendu Talukder,Deepak Kumar,Rachit Pandey,Aaditeshwar Seth,Chetan Arora*

Main category: cs.AI

TLDR: 论文提出了一种基于OCR和OMR技术的纸质数据数字化方法，并开发了深度学习模型用于实际环境中的手写数字识别。


<details>
  <summary>Details</summary>
Motivation: 解决因设备成本或培训不足导致数字设备数据收集不可行的问题，提出纸质数据自动数字化的替代方案。

Method: 使用OCR和OMR技术，结合深度学习模型处理手写数字数据，并在实际项目中应用。

Result: 成功开发并部署了OCR工具，用于数字化纸质表单，推动了近400万通电话。

Conclusion: 该方法在实际项目中验证有效，数据和模型已开源。

Abstract: Initiation, monitoring, and evaluation of development programmes can involve
field-based data collection about project activities. This data collection
through digital devices may not always be feasible though, for reasons such as
unaffordability of smartphones and tablets by field-based cadre, or shortfalls
in their training and capacity building. Paper-based data collection has been
argued to be more appropriate in several contexts, with automated digitization
of the paper forms through OCR (Optical Character Recognition) and OMR (Optical
Mark Recognition) techniques. We contribute with providing a large dataset of
handwritten digits, and deep learning based models and methods built using this
data, that are effective in real-world environments. We demonstrate the
deployment of these tools in the context of a maternal and child health and
nutrition awareness project, which uses IVR (Interactive Voice Response)
systems to provide awareness information to rural women SHG (Self Help Group)
members in north India. Paper forms were used to collect phone numbers of the
SHG members at scale, which were digitized using the OCR tools developed by us,
and used to push almost 4 million phone calls. The data, model, and code have
been released in the open-source domain.

</details>

### [225] [Sparks: Multi-Agent Artificial Intelligence Model Discovers Protein Design Principles](https://arxiv.org/abs/2504.19017)
*Alireza Ghafarollahi,Markus J. Buehler*

Main category: cs.AI

TLDR: Sparks是一种多模态多代理AI模型，能够自主完成科学发现周期，包括假设生成、实验设计和迭代优化，无需人类干预。


<details>
  <summary>Details</summary>
Motivation: 传统AI系统通常只能从训练数据中提取已有知识，而Sparks旨在实现完全自主的科学发现。

Method: Sparks结合生成式序列设计、高精度结构预测和物理感知属性模型，通过生成与反思代理实现自我修正和可重复性。

Result: 在蛋白质科学中，Sparks发现了两项新现象：长度依赖的力学交叉和链长/二级结构稳定性图。

Conclusion: Sparks能够独立进行严谨的科学探究，并识别未知科学原理。

Abstract: Advances in artificial intelligence (AI) promise autonomous discovery, yet
most systems still resurface knowledge latent in their training data. We
present Sparks, a multi-modal multi-agent AI model that executes the entire
discovery cycle that includes hypothesis generation, experiment design and
iterative refinement to develop generalizable principles and a report without
human intervention. Applied to protein science, Sparks uncovered two previously
unknown phenomena: (i) a length-dependent mechanical crossover whereby
beta-sheet-biased peptides surpass alpha-helical ones in unfolding force beyond
~80 residues, establishing a new design principle for peptide mechanics; and
(ii) a chain-length/secondary-structure stability map revealing unexpectedly
robust beta-sheet-rich architectures and a "frustration zone" of high variance
in mixed alpha/beta folds. These findings emerged from fully self-directed
reasoning cycles that combined generative sequence design, high-accuracy
structure prediction and physics-aware property models, with paired
generation-and-reflection agents enforcing self-correction and reproducibility.
The key result is that Sparks can independently conduct rigorous scientific
inquiry and identify previously unknown scientific principles.

</details>

### [226] [GLaMoR: Consistency Checking of OWL Ontologies using Graph Language Models](https://arxiv.org/abs/2504.19023)
*Justin Mücke,Ansgar Scherp*

Main category: cs.AI

TLDR: 论文提出GLaMoR，一种基于图语言模型（GLM）的推理管道，用于高效检查OWL本体的一致性，在准确性和速度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语义推理方法在检查本体一致性时计算成本高且效率随本体规模下降，传统机器学习模型难以捕捉复杂关系，而大型语言模型（LLM）在结构化推理上表现不佳。

Method: 将OWL本体转换为图结构数据，并基于GLM架构构建GLaMoR推理管道，用于一致性检查。

Result: 在NCBO BioPortal本体库上测试，GLaMoR准确率达95%，速度比传统推理器快20倍。

Conclusion: GLaMoR结合图结构和语言模型优势，为高效本体一致性检查提供了新方法。

Abstract: Semantic reasoning aims to infer new knowledge from existing knowledge, with
OWL ontologies serving as a standardized framework for organizing information.
A key challenge in semantic reasoning is verifying ontology consistency.
However, state-of-the-art reasoners are computationally expensive, and their
efficiency decreases as ontology sizes grow. While classical machine learning
models have been explored for consistency checking, they struggle to capture
complex relationships within ontologies. Large language models (LLMs) have
shown promising results for simple reasoning tasks but perform poorly on
structured reasoning. The recently introduced Graph Language Model (GLM) offers
a way to simultaneously process graph-structured data and text. This paper
proposes GLaMoR (Graph Language Model for Reasoning), a reasoning pipeline that
transforms OWL ontologies into graph-structured data and adapts the GLM
architecture for consistency checking. We evaluate GLaMoR on ontologies from
the NCBO BioPortal repository, converting them into triples suitable for model
input. Our results show that the GLM outperforms all baseline models, achieving
$95\%$ accuracy while being 20 times faster than classical reasoners.
  The Code is accessible under: https://github.com/JustinMuecke/GLaMoR

</details>

### [227] [DiCE-Extended: A Robust Approach to Counterfactual Explanations in Machine Learning](https://arxiv.org/abs/2504.19027)
*Volkan Bakir,Polat Goktas,Sureyya Akyuz*

Main category: cs.AI

TLDR: DiCE-Extended是一个改进的反事实解释框架，通过多目标优化技术提升鲁棒性，同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法在平衡邻近性、多样性和鲁棒性方面存在不足，限制了其实际应用。

Method: 引入基于Dice-Sorensen系数的新鲁棒性指标，并使用加权损失组件（lambda_p、lambda_d、lambda_r）优化反事实生成。

Result: 在多个基准数据集和ML后端上验证，DiCE-Extended在有效性、稳定性和决策边界对齐方面优于标准DiCE。

Conclusion: DiCE-Extended为高风险应用生成更可靠和可解释的反事实解释，未来将探索自适应优化技术和领域特定约束。

Abstract: Explainable artificial intelligence (XAI) has become increasingly important
in decision-critical domains such as healthcare, finance, and law.
Counterfactual (CF) explanations, a key approach in XAI, provide users with
actionable insights by suggesting minimal modifications to input features that
lead to different model outcomes. Despite significant advancements, existing CF
generation methods often struggle to balance proximity, diversity, and
robustness, limiting their real-world applicability. A widely adopted
framework, Diverse Counterfactual Explanations (DiCE), emphasizes diversity but
lacks robustness, making CF explanations sensitive to perturbations and domain
constraints. To address these challenges, we introduce DiCE-Extended, an
enhanced CF explanation framework that integrates multi-objective optimization
techniques to improve robustness while maintaining interpretability. Our
approach introduces a novel robustness metric based on the Dice-Sorensen
coefficient, ensuring stability under small input variations. Additionally, we
refine CF generation using weighted loss components (lambda_p, lambda_d,
lambda_r) to balance proximity, diversity, and robustness. We empirically
validate DiCE-Extended on benchmark datasets (COMPAS, Lending Club, German
Credit, Adult Income) across multiple ML backends (Scikit-learn, PyTorch,
TensorFlow). Results demonstrate improved CF validity, stability, and alignment
with decision boundaries compared to standard DiCE-generated explanations. Our
findings highlight the potential of DiCE-Extended in generating more reliable
and interpretable CFs for high-stakes applications. Future work will explore
adaptive optimization techniques and domain-specific constraints to further
enhance CF generation in real-world scenarios.

</details>

### [228] [ChiseLLM: Unleashing the Power of Reasoning LLMs for Chisel Agile Hardware Development](https://arxiv.org/abs/2504.19144)
*Bowei Wang,Jiaran Gao,Yelai Feng,Renzhi Chen,Shanshan Li,Lei Wang*

Main category: cs.AI

TLDR: 本文介绍了ChiseLLM，一种针对Chisel代码生成的解决方案，通过数据处理、提示引导的推理轨迹合成和领域适应模型训练，显著提升了语法正确性和设计变异性。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）在Chisel代码生成任务中面临语法正确性和设计变异性的挑战，需要一种领域适应的方法来提升性能。

Method: ChiseLLM通过数据处理与转换、提示引导的推理轨迹合成和领域适应模型训练，构建高质量数据集并优化模型性能。

Result: 实验显示，ChiseLLM-7B和ChiseLLM-32B模型分别将语法正确性提升了18.85%和26.32%，设计变异性能力提高了47.58%。

Conclusion: ChiseLLM为基于HCL的敏捷硬件开发方法提供了高性能、低成本的模型，并为未来研究提供了有效基线。

Abstract: The growing demand for Domain-Specific Architecture (DSA) has driven the
development of Agile Hardware Development Methodology (AHDM). Hardware
Construction Language (HCL) like Chisel offers high-level abstraction features,
making it an ideal language for HCL-Based AHDM. While Large Language Models
(LLMs) excel in code generation tasks, they still face challenges with Chisel
generation, particularly regarding syntax correctness and design variability.
Recent reasoning models have significantly enhanced code generation
capabilities through test-time scaling techniques. However, we found that
reasoning models without domain adaptation cannot bring substantial benefits to
Chisel code generation tasks. This paper presents ChiseLLM, a solution
comprising data processing and transformation, prompt-guided reasoning trace
synthesis, and domain-adapted model training. We constructed high-quality
datasets from public RTL code resources and guided the model to adopt
structured thinking patterns through prompt enhancement methods. Experiments
demonstrate that our ChiseLLM-7B and ChiseLLM-32B models improved syntax
correctness by 18.85% and 26.32% respectively over base models, while
increasing variability design ability by 47.58% compared to baseline reasoning
models. Our datasets and models are publicly available, providing
high-performance, cost-effective models for HCL-Based AHDM, and offering an
effective baseline for future research. Github repository:
https://github.com/observerw/ChiseLLM

</details>

### [229] [A Dynamic Fuzzy Rule and Attribute Management Framework for Fuzzy Inference Systems in High-Dimensional Data](https://arxiv.org/abs/2504.19148)
*Ke Liu,Jing Ma,Edmund M-K Lai*

Main category: cs.AI

TLDR: ADAR框架通过双权重机制和自动增长/剪枝策略，优化高维数据的神经模糊推理系统，显著降低RMSE并保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决高维数据在神经模糊系统中带来的复杂性和性能挑战。

Method: 集成双权重机制（属性和规则）及自动增长/剪枝策略，动态简化模糊模型。

Result: 在多个数据集上RMSE表现优于现有方法，如北京PM2.5数据集RMSE为56.87。

Conclusion: ADAR有效平衡规则复杂性和特征重要性，适用于高精度且透明的实际应用。

Abstract: This paper presents an Adaptive Dynamic Attribute and Rule (ADAR) framework
designed to address the challenges posed by high-dimensional data in
neuro-fuzzy inference systems. By integrating dual weighting
mechanisms-assigning adaptive importance to both attributes and rules-together
with automated growth and pruning strategies, ADAR adaptively streamlines
complex fuzzy models without sacrificing performance or interpretability.
Experimental evaluations on four diverse datasets - Auto MPG (7 variables),
Beijing PM2.5 (10 variables), Boston Housing (13 variables), and Appliances
Energy Consumption (27 variables) show that ADAR-based models achieve
consistently lower Root Mean Square Error (RMSE) compared to state-of-the-art
baselines. On the Beijing PM2.5 dataset, for instance, ADAR-SOFENN attained an
RMSE of 56.87 with nine rules, surpassing traditional ANFIS [12] and SOFENN
[16] models. Similarly, on the high-dimensional Appliances Energy dataset,
ADAR-ANFIS reached an RMSE of 83.25 with nine rules, outperforming established
fuzzy logic approaches and interpretability-focused methods such as APLR.
Ablation studies further reveal that combining rule-level and attribute-level
weight assignment significantly reduces model overlap while preserving
essential features, thereby enhancing explainability. These results highlight
ADAR's effectiveness in dynamically balancing rule complexity and feature
importance, paving the way for scalable, high-accuracy, and transparent
neuro-fuzzy systems applicable to a range of real-world scenarios.

</details>

### [230] [A Design Framework for operationalizing Trustworthy Artificial Intelligence in Healthcare: Requirements, Tradeoffs and Challenges for its Clinical Adoption](https://arxiv.org/abs/2504.19179)
*Pedro A. Moreno-Sánchez,Javier Del Ser,Mark van Gils,Jussi Hernesniemi*

Main category: cs.AI

TLDR: 本文提出了一种设计框架，旨在将可信人工智能（TAI）原则嵌入医疗AI系统，以解决临床采用中的伦理、监管和信任问题。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在医疗领域潜力巨大，但因伦理、监管和信任等非技术问题，其临床采用受限。TAI原则为解决这些问题提供了方向。

Method: 提出一个设计框架，为不同医疗流程中的利益相关者制定疾病无关的TAI要求，并探讨实践中的挑战与权衡。

Result: 以心血管疾病为例，展示了TAI原则的应用及现存障碍。

Conclusion: 该框架为医疗AI系统开发者提供了实践TAI原则的指导，但需进一步解决实际应用中的挑战。

Abstract: Artificial Intelligence (AI) holds great promise for transforming healthcare,
particularly in disease diagnosis, prognosis, and patient care. The increasing
availability of digital medical data, such as images, omics, biosignals, and
electronic health records, combined with advances in computing, has enabled AI
models to approach expert-level performance. However, widespread clinical
adoption remains limited, primarily due to challenges beyond technical
performance, including ethical concerns, regulatory barriers, and lack of
trust. To address these issues, AI systems must align with the principles of
Trustworthy AI (TAI), which emphasize human agency and oversight, algorithmic
robustness, privacy and data governance, transparency, bias and discrimination
avoidance, and accountability. Yet, the complexity of healthcare processes
(e.g., screening, diagnosis, prognosis, and treatment) and the diversity of
stakeholders (clinicians, patients, providers, regulators) complicate the
integration of TAI principles. To bridge the gap between TAI theory and
practical implementation, this paper proposes a design framework to support
developers in embedding TAI principles into medical AI systems. Thus, for each
stakeholder identified across various healthcare processes, we propose a
disease-agnostic collection of requirements that medical AI systems should
incorporate to adhere to the principles of TAI. Additionally, we examine the
challenges and tradeoffs that may arise when applying these principles in
practice. To ground the discussion, we focus on cardiovascular diseases, a
field marked by both high prevalence and active AI innovation, and demonstrate
how TAI principles have been applied and where key obstacles persist.

</details>

### [231] [The Convergent Ethics of AI? Analyzing Moral Foundation Priorities in Large Language Models with a Multi-Framework Approach](https://arxiv.org/abs/2504.19255)
*Chad Coleman,W. Russell Neuman,Ali Dasdan,Safinah Ali,Manan Shah*

Main category: cs.AI

TLDR: 该论文提出了PRIME框架，用于评估大型语言模型（LLMs）在道德推理能力上的表现，发现模型在关怀/伤害和公平/欺骗维度上表现突出，但在权威、忠诚和神圣维度上较弱。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在重要决策中的应用增加，系统评估其道德推理能力变得至关重要。

Method: 采用PRIME框架，结合直接提问和响应分析，评估六种领先LLMs在道德困境中的表现。

Result: 所有模型在关怀/伤害和公平/欺骗维度上表现一致，但在其他维度上较弱，且与人类道德偏好相符。

Conclusion: PRIME框架为道德基准测试提供了可扩展方法，揭示了当前AI道德推理的潜力和局限性。

Abstract: As large language models (LLMs) are increasingly deployed in consequential
decision-making contexts, systematically assessing their ethical reasoning
capabilities becomes a critical imperative. This paper introduces the
Priorities in Reasoning and Intrinsic Moral Evaluation (PRIME) framework--a
comprehensive methodology for analyzing moral priorities across foundational
ethical dimensions including consequentialist-deontological reasoning, moral
foundations theory, and Kohlberg's developmental stages. We apply this
framework to six leading LLMs through a dual-protocol approach combining direct
questioning and response analysis to established ethical dilemmas. Our analysis
reveals striking patterns of convergence: all evaluated models demonstrate
strong prioritization of care/harm and fairness/cheating foundations while
consistently underweighting authority, loyalty, and sanctity dimensions.
Through detailed examination of confidence metrics, response reluctance
patterns, and reasoning consistency, we establish that contemporary LLMs (1)
produce decisive ethical judgments, (2) demonstrate notable cross-model
alignment in moral decision-making, and (3) generally correspond with
empirically established human moral preferences. This research contributes a
scalable, extensible methodology for ethical benchmarking while highlighting
both the promising capabilities and systematic limitations in current AI moral
reasoning architectures--insights critical for responsible development as these
systems assume increasingly significant societal roles.

</details>

### [232] [Small Models, Big Tasks: An Exploratory Empirical Study on Small Language Models for Function Calling](https://arxiv.org/abs/2504.19277)
*Ishan Kavathekar,Raghav Donakanti,Ponnurangam Kumaraguru,Karthik Vaidhyanathan*

Main category: cs.AI

TLDR: 该论文探讨了小型语言模型（SLMs）在函数调用任务中的表现，比较了零样本、少样本和微调方法，并评估了其在边缘设备上的实用性。


<details>
  <summary>Details</summary>
Motivation: 函数调用在信息检索、软件工程和自动化等领域有广泛应用，但大型语言模型（LLMs）计算成本高，不适合资源受限环境。SLMs因其高效性成为潜在替代方案。

Method: 通过零样本、少样本和微调方法评估SLMs生成函数调用的效果，包括提示注入实验，并在边缘设备上测试延迟和内存使用。

Result: SLMs从零样本到微调表现逐步提升，但在输出格式一致性上表现不佳。提示注入实验显示模型鲁棒性较好，性能略有下降。

Conclusion: SLMs在函数调用任务中展现出潜力，但需进一步优化以满足实时需求。

Abstract: Function calling is a complex task with widespread applications in domains
such as information retrieval, software engineering and automation. For
example, a query to book the shortest flight from New York to London on January
15 requires identifying the correct parameters to generate accurate function
calls. Large Language Models (LLMs) can automate this process but are
computationally expensive and impractical in resource-constrained settings. In
contrast, Small Language Models (SLMs) can operate efficiently, offering faster
response times, and lower computational demands, making them potential
candidates for function calling on edge devices. In this exploratory empirical
study, we evaluate the efficacy of SLMs in generating function calls across
diverse domains using zero-shot, few-shot, and fine-tuning approaches, both
with and without prompt injection, while also providing the finetuned models to
facilitate future applications. Furthermore, we analyze the model responses
across a range of metrics, capturing various aspects of function call
generation. Additionally, we perform experiments on an edge device to evaluate
their performance in terms of latency and memory usage, providing useful
insights into their practical applicability. Our findings show that while SLMs
improve from zero-shot to few-shot and perform best with fine-tuning, they
struggle significantly with adhering to the given output format. Prompt
injection experiments further indicate that the models are generally robust and
exhibit only a slight decline in performance. While SLMs demonstrate potential
for the function call generation task, our results also highlight areas that
need further refinement for real-time functioning.

</details>

### [233] [Logic-Based Artificial Intelligence Algorithms Supporting Categorical Semantics](https://arxiv.org/abs/2504.19320)
*Ralph Wojtowicz*

Main category: cs.AI

TLDR: 论文提出了一种基于范畴逻辑的符号推理方法，用于设计能处理比集合更丰富结构的智能体。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用范畴逻辑设计智能体，使其能对结构化对象进行符号推理，尤其是在不支持经典逻辑的语义范畴中。

Method: 采用Johnstone的序列演算，开发了前向链和正规形式算法，并改进了多类理论的一阶合一算法。

Result: 成功实现了在笛卡尔范畴中基于Horn逻辑的推理，并支持多类理论和上下文。

Conclusion: 该方法扩展了符号推理的应用范围，适用于更广泛的语义范畴。

Abstract: This paper seeks to apply categorical logic to the design of artificial
intelligent agents that reason symbolically about objects more richly
structured than sets. Using Johnstone's sequent calculus of terms- and
formulae-in-context, we develop forward chaining and normal form algorithms for
reasoning about objects in cartesian categories with the rules for Horn logic.
We also adapt first-order unification to support multi-sorted theories,
contexts, and fragments of first-order logic. The significance of these
reformulations rests in the fact that they can be applied to reasoning about
objects in semantic categories that do not support classical logic or even all
its connectives.

</details>

### [234] [Neurosymbolic Association Rule Mining from Tabular Data](https://arxiv.org/abs/2504.19354)
*Erkan Karabulut,Paul Groth,Victoria Degeler*

Main category: cs.AI

TLDR: Aerial+是一种新型神经符号关联规则挖掘方法，通过神经表示和重构机制生成高质量规则集，显著减少规则爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 高维数据集导致规则爆炸，增加执行时间并影响下游任务性能，亟需解决方案。

Method: Aerial+利用不完全自编码器生成数据神经表示，通过重构机制提取规则。

Result: 在五个数据集上优于七种基线方法，生成更简洁、高质量的规则集，并显著减少执行时间。

Conclusion: Aerial+有效解决了规则爆炸问题，提升了规则挖掘的性能和效率。

Abstract: Association Rule Mining (ARM) is the task of mining patterns among data
features in the form of logical rules, with applications across a myriad of
domains. However, high-dimensional datasets often result in an excessive number
of rules, increasing execution time and negatively impacting downstream task
performance. Managing this rule explosion remains a central challenge in ARM
research. To address this, we introduce Aerial+, a novel neurosymbolic ARM
method. Aerial+ leverages an under-complete autoencoder to create a neural
representation of the data, capturing associations between features. It
extracts rules from this neural representation by exploiting the model's
reconstruction mechanism. Extensive evaluations on five datasets against seven
baselines demonstrate that Aerial+ achieves state-of-the-art results by
learning more concise, high-quality rule sets with full data coverage. When
integrated into rule-based interpretable machine learning models, Aerial+
significantly reduces execution time while maintaining or improving accuracy.

</details>

### [235] [Graph Reinforcement Learning for QoS-Aware Load Balancing in Open Radio Access Networks](https://arxiv.org/abs/2504.19499)
*Omid Semiari,Hosein Nikopour,Shilpa Talwar*

Main category: cs.AI

TLDR: 提出了一种基于图强化学习（GRL）的QoS感知负载均衡方法，用于优化多频段O-RAN中的GBR和BE流量性能。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络需要严格的QoS保障，而避免小区拥塞是关键挑战。

Method: 将负载均衡建模为马尔可夫决策过程，利用图神经网络（GNN）和强化学习（RL）结合的方法，设计状态和奖励信号。

Result: 相比基线方法，QoS违规减少53%，BE流量的第5百分位速率提升四倍。

Conclusion: GRL方法在负载均衡中表现出色，能有效满足QoS需求。

Abstract: Next-generation wireless cellular networks are expected to provide
unparalleled Quality-of-Service (QoS) for emerging wireless applications,
necessitating strict performance guarantees, e.g., in terms of link-level data
rates. A critical challenge in meeting these QoS requirements is the prevention
of cell congestion, which involves balancing the load to ensure sufficient
radio resources are available for each cell to serve its designated User
Equipments (UEs). In this work, a novel QoS-aware Load Balancing (LB) approach
is developed to optimize the performance of Guaranteed Bit Rate (GBR) and Best
Effort (BE) traffic in a multi-band Open Radio Access Network (O-RAN) under QoS
and resource constraints. The proposed solution builds on Graph Reinforcement
Learning (GRL), a powerful framework at the intersection of Graph Neural
Network (GNN) and RL. The QoS-aware LB is modeled as a Markov Decision Process,
with states represented as graphs. QoS consideration are integrated into both
state representations and reward signal design. The LB agent is then trained
using an off-policy dueling Deep Q Network (DQN) that leverages a GNN-based
architecture. This design ensures the LB policy is invariant to the ordering of
nodes (UE or cell), flexible in handling various network sizes, and capable of
accounting for spatial node dependencies in LB decisions. Performance of the
GRL-based solution is compared with two baseline methods. Results show
substantial performance gains, including a $53\%$ reduction in QoS violations
and a fourfold increase in the 5th percentile rate for BE traffic.

</details>

### [236] [GVPO: Group Variance Policy Optimization for Large Language Model Post-Training](https://arxiv.org/abs/2504.19599)
*Kaichen Zhang,Yuzhong Hong,Junwei Bao,Hongfei Jiang,Yang Song,Dingqian Hong,Hui Xiong*

Main category: cs.AI

TLDR: GVPO是一种新的后训练方法，通过将KL约束奖励最大化的解析解直接融入梯度权重，解决了现有方法（如GRPO）的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法（如GRPO）虽然性能优越，但存在训练不稳定的问题，限制了实际应用。

Method: GVPO通过将KL约束奖励最大化的解析解融入梯度权重，确保与最优策略对齐，并提供直观的物理解释。

Result: GVPO保证了唯一最优解，支持灵活的采样分布，避免了现有方法的局限性。

Conclusion: GVPO为可靠且通用的LLM后训练建立了新范式。

Abstract: Post-training plays a crucial role in refining and aligning large language
models to meet specific tasks and human preferences. While recent advancements
in post-training techniques, such as Group Relative Policy Optimization (GRPO),
leverage increased sampling with relative reward scoring to achieve superior
performance, these methods often suffer from training instability that limits
their practical adoption. To address this challenge, we present Group Variance
Policy Optimization (GVPO). GVPO incorporates the analytical solution to
KL-constrained reward maximization directly into its gradient weights, ensuring
alignment with the optimal policy. The method provides intuitive physical
interpretations: its gradient mirrors the mean squared error between the
central distance of implicit rewards and that of actual rewards. GVPO offers
two key advantages: (1) it guarantees a unique optimal solution, exactly the
KL-constrained reward maximization objective, (2) it supports flexible sampling
distributions that avoids on-policy and importance sampling limitations. By
unifying theoretical guarantees with practical adaptability, GVPO establishes a
new paradigm for reliable and versatile LLM post-training.

</details>

### [237] [From Evidence to Belief: A Bayesian Epistemology Approach to Language Models](https://arxiv.org/abs/2504.19622)
*Minsu Kim,Sangryul Kim,James Thorne*

Main category: cs.AI

TLDR: 该论文从贝叶斯认识论角度研究语言模型的知识，发现模型在真实证据下表现良好，但在其他证据类型下偏离贝叶斯假设，且高置信度不一定对应高准确性。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型如何根据证据的信息量和可靠性调整其置信度和响应，以理解其知识表示。

Method: 构建包含多种证据类型的数据集，通过语言化置信度、标记概率和采样分析模型的响应和置信度。

Result: 语言模型在真实证据下符合贝叶斯确认假设，但在其他证据类型下表现不一致；高置信度不一定保证高准确性，且模型对黄金证据存在偏好。

Conclusion: 语言模型在贝叶斯假设下的表现不一致，揭示了其知识表示的局限性，为未来改进提供了方向。

Abstract: This paper investigates the knowledge of language models from the perspective
of Bayesian epistemology. We explore how language models adjust their
confidence and responses when presented with evidence with varying levels of
informativeness and reliability. To study these properties, we create a dataset
with various types of evidence and analyze language models' responses and
confidence using verbalized confidence, token probability, and sampling. We
observed that language models do not consistently follow Bayesian epistemology:
language models follow the Bayesian confirmation assumption well with true
evidence but fail to adhere to other Bayesian assumptions when encountering
different evidence types. Also, we demonstrated that language models can
exhibit high confidence when given strong evidence, but this does not always
guarantee high accuracy. Our analysis also reveals that language models are
biased toward golden evidence and show varying performance depending on the
degree of irrelevance, helping explain why they deviate from Bayesian
assumptions.

</details>

### [238] [Fitness Landscape of Large Language Model-Assisted Automated Algorithm Search](https://arxiv.org/abs/2504.19636)
*Fei Liu,Qingfu Zhang,Xialiang Tong,Mingxuan Yuan,Kun Mao*

Main category: cs.AI

TLDR: 本文通过图论方法分析了LLM辅助算法搜索（LAS）的适应度景观，揭示了其多模态和崎岖特性，并探讨了任务类型和LLM选择对景观结构的影响。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在算法设计中的潜力时，发现其适应度景观（对搜索行为至关重要）尚未充分探索。

Method: 采用基于图的方法，节点表示算法，边表示算法间的过渡，并在六种算法设计任务和六种常用LLM上进行了广泛评估。

Result: LAS景观高度多模态且崎岖，任务类型和LLM选择导致结构差异；群体规模影响探索-开发权衡。

Conclusion: 研究结果深化了对LAS景观的理解，并为设计更有效的LAS方法提供了实用指导。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in
algorithm design. However, when integrated into search frameworks for iterative
algorithm search, the underlying fitness landscape--critical for understanding
search behaviou--remains underexplored. In this paper, we illustrate and
analyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a
graph-based approach, where nodes represent algorithms and edges denote
transitions between them. We conduct extensive evaluations across six algorithm
design tasks and six commonly used LLMs. Our findings reveal that LAS
landscapes are highly multimodal and rugged, particularly in combinatorial
optimization tasks, with distinct structural variations across tasks and LLMs.
For instance, heuristic design tasks exhibit dense clusters of high-performing
algorithms, while symbolic regression tasks show sparse, scattered
distributions. Additionally, we demonstrate how population size influences
exploration-exploitation trade-offs and the evolving trajectory of elite
algorithms. These insights not only advance our understanding of LAS landscapes
but also provide practical guidance for designing more effective LAS methods.

</details>

### [239] [From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review](https://arxiv.org/abs/2504.19678)
*Mohamed Amine Ferrag,Norbert Tihanyi,Merouane Debbah*

Main category: cs.AI

TLDR: 本文对2019至2025年间的大型语言模型和自主AI代理的评估基准、框架及协作协议进行了系统比较，提出了一套分类法，并探讨了实际应用和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前评估基准和框架的碎片化，缺乏统一的分类法或全面综述。

Method: 通过横向比较2019至2025年的评估基准，提出涵盖多个领域的60项基准分类法，并回顾2023至2025年的AI代理框架。

Result: 提出了一套分类法，总结了实际应用领域，并分析了三种关键代理协作协议。

Conclusion: 未来研究应关注高级推理策略、多代理系统的失败模式、自动化科学发现等方向。

Abstract: Large language models and autonomous AI agents have evolved rapidly,
resulting in a diverse array of evaluation benchmarks, frameworks, and
collaboration protocols. However, the landscape remains fragmented and lacks a
unified taxonomy or comprehensive survey. Therefore, we present a side-by-side
comparison of benchmarks developed between 2019 and 2025 that evaluate these
models and agents across multiple domains. In addition, we propose a taxonomy
of approximately 60 benchmarks that cover general and academic knowledge
reasoning, mathematical problem-solving, code generation and software
engineering, factual grounding and retrieval, domain-specific evaluations,
multimodal and embodied tasks, task orchestration, and interactive assessments.
Furthermore, we review AI-agent frameworks introduced between 2023 and 2025
that integrate large language models with modular toolkits to enable autonomous
decision-making and multi-step reasoning. Moreover, we present real-world
applications of autonomous AI agents in materials science, biomedical research,
academic ideation, software engineering, synthetic data generation, chemical
reasoning, mathematical problem-solving, geographic information systems,
multimedia, healthcare, and finance. We then survey key agent-to-agent
collaboration protocols, namely the Agent Communication Protocol (ACP), the
Model Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally,
we discuss recommendations for future research, focusing on advanced reasoning
strategies, failure modes in multi-agent LLM systems, automated scientific
discovery, dynamic tool integration via reinforcement learning, integrated
search capabilities, and security vulnerabilities in agent protocols.

</details>

### [240] [Learning Efficiency Meets Symmetry Breaking](https://arxiv.org/abs/2504.19738)
*Yingbin Bai,Sylvie Thiebaux,Felipe Trevizan*

Main category: cs.AI

TLDR: 论文提出了一种基于图神经网络的规划方法，结合动作剪枝和状态剪枝技术，有效处理对称性问题，并在Fast Downward中首次超越LAMA。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的规划器在大搜索空间中表现良好，但对对称性问题的处理尚未充分探索。

Method: 引入一种图表示方法，结合动作剪枝和状态剪枝技术，管理搜索中的对称性。

Result: 在最新IPC学习赛道数据集上，首次超越LAMA。

Conclusion: 该方法有效结合学习效率和对称性检测，提升了规划性能。

Abstract: Learning-based planners leveraging Graph Neural Networks can learn search
guidance applicable to large search spaces, yet their potential to address
symmetries remains largely unexplored. In this paper, we introduce a graph
representation of planning problems allying learning efficiency with the
ability to detect symmetries, along with two pruning methods, action pruning
and state pruning, designed to manage symmetries during search. The integration
of these techniques into Fast Downward achieves a first-time success over LAMA
on the latest IPC learning track dataset. Code is released at:
https://github.com/bybeye/Distincter.

</details>

### [241] [Can AI Agents Design and Implement Drug Discovery Pipelines?](https://arxiv.org/abs/2504.19912)
*Khachik Smbatyan,Tsolak Ghukasyan,Tigran Aghajanyan,Hovhannes Dabaghyan,Sergey Adamyan,Aram Bughdaryan,Vahagn Altunyan,Gagik Navasardyan,Aram Davtyan,Anush Hakobyan,Aram Gharibyan,Arman Fahradyan,Artur Hakobyan,Hasmik Mnatsakanyan,Narek Ginoyan,Garik Petrosyan*

Main category: cs.AI

TLDR: 论文介绍了DO Challenge基准，用于评估AI代理在药物发现中的决策能力，并展示了Deep Thought多代理系统的表现。


<details>
  <summary>Details</summary>
Motivation: 通过AI加速药物发现，减少对昂贵实验的依赖。

Method: 提出DO Challenge基准，测试AI代理在虚拟筛选场景中的能力，并分析Deep Thought系统的表现。

Result: Deep Thought系统表现优异，但不及专家设计的方案，且稳定性不足。

Conclusion: AI在药物发现中潜力巨大，但仍需改进稳定性与性能。

Abstract: The rapid advancement of artificial intelligence, particularly autonomous
agentic systems based on Large Language Models (LLMs), presents new
opportunities to accelerate drug discovery by improving in-silico modeling and
reducing dependence on costly experimental trials. Current AI agent-based
systems demonstrate proficiency in solving programming challenges and
conducting research, indicating an emerging potential to develop software
capable of addressing complex problems such as pharmaceutical design and drug
discovery. This paper introduces DO Challenge, a benchmark designed to evaluate
the decision-making abilities of AI agents in a single, complex problem
resembling virtual screening scenarios. The benchmark challenges systems to
independently develop, implement, and execute efficient strategies for
identifying promising molecular structures from extensive datasets, while
navigating chemical space, selecting models, and managing limited resources in
a multi-objective context. We also discuss insights from the DO Challenge 2025,
a competition based on the proposed benchmark, which showcased diverse
strategies explored by human participants. Furthermore, we present the Deep
Thought multi-agent system, which demonstrated strong performance on the
benchmark, outperforming most human teams. Among the language models tested,
Claude 3.7 Sonnet, Gemini 2.5 Pro and o3 performed best in primary agent roles,
and GPT-4o, Gemini 2.0 Flash were effective in auxiliary roles. While
promising, the system's performance still fell short of expert-designed
solutions and showed high instability, highlighting both the potential and
current limitations of AI-driven methodologies in transforming drug discovery
and broader scientific research.

</details>

### [242] [Automated decision-making for dynamic task assignment at scale](https://arxiv.org/abs/2504.19933)
*Riccardo Lo Bianco,Willem van Jaarsveld,Jeroen Middelhuis,Luca Begnardi,Remco Dijkman*

Main category: cs.AI

TLDR: 本文提出了一种基于深度强化学习（DRL）的决策支持系统（DSS），用于解决现实规模的动态任务分配问题（DTAP），通过新颖的图结构和奖励函数设计，显著提升了任务处理效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究多局限于解决小规模、合成问题，忽视了现实场景中的挑战，本文旨在填补这一空白。

Method: 引入了一个DRL代理，包含图结构的观察与动作表示以及等效于最小化任务平均周期的奖励函数。

Result: 实验表明，该DRL代理在所有DTAP实例中均匹配或优于最佳基线，且具有泛化能力。

Conclusion: 提出的DSS为现实规模的DTAP提供了有效且可推广的解决方案。

Abstract: The Dynamic Task Assignment Problem (DTAP) concerns matching resources to
tasks in real time while minimizing some objectives, like resource costs or
task cycle time. In this work, we consider a DTAP variant where every task is a
case composed of a stochastic sequence of activities. The DTAP, in this case,
involves the decision of which employee to assign to which activity to process
requests as quickly as possible. In recent years, Deep Reinforcement Learning
(DRL) has emerged as a promising tool for tackling this DTAP variant, but most
research is limited to solving small-scale, synthetic problems, neglecting the
challenges posed by real-world use cases. To bridge this gap, this work
proposes a DRL-based Decision Support System (DSS) for real-world scale DTAPS.
To this end, we introduce a DRL agent with two novel elements: a graph
structure for observations and actions that can effectively represent any DTAP
and a reward function that is provably equivalent to the objective of
minimizing the average cycle time of tasks. The combination of these two
novelties allows the agent to learn effective and generalizable assignment
policies for real-world scale DTAPs. The proposed DSS is evaluated on five DTAP
instances whose parameters are extracted from real-world logs through process
mining. The experimental evaluation shows how the proposed DRL agent matches or
outperforms the best baseline in all DTAP instances and generalizes on
different time horizons and across instances.

</details>

### [243] [How Group Lives Go Well](https://arxiv.org/abs/2504.19968)
*John Beverley,Regina Hurley*

Main category: cs.AI

TLDR: 论文提出了一种基于本体工程的群体福祉框架，扩展了反事实账户（CT）理论，以解决个体牺牲对集体进步的贡献问题。


<details>
  <summary>Details</summary>
Motivation: 传统福祉理论聚焦个体状态，难以解释个体牺牲对集体福祉的贡献，需一种新框架来建模群体繁荣。

Method: 结合基础形式本体论（BFO），提出群体功能模型，评估群体福祉基于功能持续性、制度角色和历史影响。

Result: 新模型支持语义互操作性，能结构化推理群体福利、社会制度和长期群体繁荣。

Conclusion: 该框架为群体福祉建模提供了新视角，适用于社会贡献的长期分析。

Abstract: This paper explores the ontological space of group well being, proposing a
framework for representing collective welfare, group functions, and long term
contributions within an ontology engineering context. Traditional well being
theories focus on individual states, often relying on hedonistic, desire
satisfaction, or objective list models. Such approaches struggle to account for
cases where individual sacrifices contribute to broader social progress, a
critical challenge in modeling group flourishing. To address this, the paper
refines and extends the Counterfactual Account (CT) of well being, which
evaluates goodness of an event by comparing an individual's actual well being
with a hypothetical counterpart in a nearby possible world. While useful, this
framework is insufficient for group level ontologies, where well being depends
on functional persistence, institutional roles, and historical impact rather
than immediate individual outcomes. Drawing on Basic Formal Ontology (BFO), the
paper introduces a model in which group flourishing is evaluated in terms of
group functional, where members bear roles and exhibit persistence conditions
akin to biological systems or designed artifacts. This approach enables
semantic interoperability for modeling longitudinal social contributions,
allowing for structured reasoning about group welfare, social institutions, and
group flourishing over time.

</details>

### [244] [Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage](https://arxiv.org/abs/2504.20007)
*Anita Srbinovska,Angela Srbinovska,Vivek Senthil,Adrian Martin,John McCluskey,Ernest Fokoué*

Main category: cs.AI

TLDR: 提出了一种多学科框架，利用AI和ML技术分析警察佩戴摄像头（BWC）的录像，以识别警民互动中的行为模式。


<details>
  <summary>Details</summary>
Motivation: 通过分析BWC录像，揭示警民互动中的行为动态（如尊重、不尊重、升级和降级），为执法提供实用方法并推动知识发现。

Method: 结合视频、音频和自然语言处理（NLP）技术进行多模态数据分析。

Result: 提出了方法论、计算技术和研究发现，展示了从BWC数据中提取有意义见解的实用方法。

Conclusion: 该框架为执法部门提供了实用工具，同时推动了BWC数据的知识发现前沿。

Abstract: This paper proposes a novel interdisciplinary framework for analyzing police
body-worn camera (BWC) footage from the Rochester Police Department (RPD) using
advanced artificial intelligence (AI) and statistical machine learning (ML)
techniques. Our goal is to detect, classify, and analyze patterns of
interaction between police officers and civilians to identify key behavioral
dynamics, such as respect, disrespect, escalation, and de-escalation. We apply
multimodal data analysis by integrating video, audio, and natural language
processing (NLP) techniques to extract meaningful insights from BWC footage. We
present our methodology, computational techniques, and findings, outlining a
practical approach for law enforcement while advancing the frontiers of
knowledge discovery from police BWC data.

</details>

### [245] [Towards Automated Scoping of AI for Social Good Projects](https://arxiv.org/abs/2504.20010)
*Jacob Emmerson,Rayid Ghani,Zheyuan Ryan Shi*

Main category: cs.AI

TLDR: AI4SG利用AI解决社会问题，但问题界定过程复杂且资源密集。本文提出基于LLM的问题界定代理（PSA），生成与专家相当的提案，并探讨了现实挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: AI4SG面临问题界定过程的瓶颈，缺乏兼具技术和领域知识的专业人员。

Method: 提出基于LLM的问题界定代理（PSA），利用科学文献和实际知识生成提案。

Result: PSA生成的提案在盲审和AI评估中与专家提案相当。

Conclusion: PSA有效但需解决现实挑战，未来工作可进一步优化。

Abstract: Artificial Intelligence for Social Good (AI4SG) is an emerging effort that
aims to address complex societal challenges with the powerful capabilities of
AI systems. These challenges range from local issues with transit networks to
global wildlife preservation. However, regardless of scale, a critical
bottleneck for many AI4SG initiatives is the laborious process of problem
scoping -- a complex and resource-intensive task -- due to a scarcity of
professionals with both technical and domain expertise. Given the remarkable
applications of large language models (LLM), we propose a Problem Scoping Agent
(PSA) that uses an LLM to generate comprehensive project proposals grounded in
scientific literature and real-world knowledge. We demonstrate that our PSA
framework generates proposals comparable to those written by experts through a
blind review and AI evaluations. Finally, we document the challenges of
real-world problem scoping and note several areas for future work.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [246] [A Decade of You Only Look Once (YOLO) for Object Detection](https://arxiv.org/abs/2504.18586)
*Leo Thomas Ramos,Angel D. Sappa*

Main category: cs.CV

TLDR: 本文回顾了YOLO框架十周年发展历程，总结了其技术演进、应用领域及未来方向。


<details>
  <summary>Details</summary>
Motivation: 纪念YOLO框架问世十周年，分析其技术发展、应用影响及未来潜力。

Method: 综述YOLO主要版本、架构趋势、应用领域，并探讨评估实践、伦理问题及未来方向。

Result: YOLO从高效检测器发展为多样化架构，具有模块化、跨领域适应性，广泛应用于多个领域。

Conclusion: YOLO框架在实时目标检测领域具有深远影响，未来仍有持续发展的潜力。

Abstract: This review marks the tenth anniversary of You Only Look Once (YOLO), one of
the most influential frameworks in real-time object detection. Over the past
decade, YOLO has evolved from a streamlined detector into a diverse family of
architectures characterized by efficient design, modular scalability, and
cross-domain adaptability. The paper presents a technical overview of the main
versions, highlights key architectural trends, and surveys the principal
application areas in which YOLO has been adopted. It also addresses evaluation
practices, ethical considerations, and potential future directions for the
framework's continued development. The analysis aims to provide a comprehensive
and critical perspective on YOLO's trajectory and ongoing transformation.

</details>

### [247] [Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency](https://arxiv.org/abs/2504.18589)
*Zhikai Wang,Jiashuo Sun,Wenqi Zhang,Zhiqiang Hu,Xin Li,Fan Wang,Deli Zhao*

Main category: cs.CV

TLDR: 论文介绍了VCBENCH，一个用于评估多模态数学推理能力的基准测试，填补了现有基准在视觉依赖数学问题评估上的空白。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型（LVLMs）在知识中心任务上表现优异，但在基础数学和视觉概念推理方面存在不足，需要新的评估方法。

Method: 提出VCBENCH基准，包含1,720个问题，覆盖六个认知领域，涉及6,697张图像，要求多图像推理。

Result: 评估26个先进LVLMs，发现性能差距显著，顶级模型准确率不足50%。

Conclusion: 研究揭示了视觉-数学整合的挑战，为未来LVLM发展提供了方向。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have
significantly enhanced their ability to integrate visual and linguistic
information, achieving near-human proficiency in tasks like object recognition,
captioning, and visual question answering. However, current benchmarks
typically focus on knowledge-centric evaluations that assess domain-specific
expertise, often neglecting the core ability to reason about fundamental
mathematical elements and visual concepts. We identify a gap in evaluating
elementary-level math problems, which rely on explicit visual
dependencies-requiring models to discern, integrate, and reason across multiple
images while incorporating commonsense knowledge, all of which are crucial for
advancing toward broader AGI capabilities. To address this gap, we introduce
VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with
explicit visual dependencies. VCBENCH includes 1,720 problems across six
cognitive domains, featuring 6,697 images (averaging 3.9 per question) to
ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH,
revealing substantial performance disparities, with even the top models unable
to exceed 50% accuracy. Our findings highlight the ongoing challenges in
visual-mathematical integration and suggest avenues for future LVLM
advancements.

</details>

### [248] [Co-Training with Active Contrastive Learning and Meta-Pseudo-Labeling on 2D Projections for Deep Semi-Supervised Learning](https://arxiv.org/abs/2504.18666)
*David Aparco-Cardenas,Jancarlo F. Gomes,Alexandre X. Falcão,Pedro J. de Rezende*

Main category: cs.CV

TLDR: 论文提出了一种名为active-DeepFA的方法，结合对比学习、元伪标签和主动学习，用于在标注数据稀缺的场景下训练非预训练的CNN模型。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型训练中标注数据稀缺的问题，尤其是在数据标注耗时且易出错的领域。

Method: 结合对比学习、教师-学生元伪标签和主动学习，通过双网络协同训练减少伪标签的确认偏差，并动态选择信息量大的样本进行标注。

Result: 在三个生物图像数据集上仅使用5%的标注数据，性能优于六种现有方法，且在3%标注数据下仍能取得可比结果。

Conclusion: active-DeepFA在标注数据稀缺的场景下表现优异，显著减少了标注需求。

Abstract: A major challenge that prevents the training of DL models is the limited
availability of accurately labeled data. This shortcoming is highlighted in
areas where data annotation becomes a time-consuming and error-prone task. In
this regard, SSL tackles this challenge by capitalizing on scarce labeled and
abundant unlabeled data; however, SoTA methods typically depend on pre-trained
features and large validation sets to learn effective representations for
classification tasks. In addition, the reduced set of labeled data is often
randomly sampled, neglecting the selection of more informative samples. Here,
we present active-DeepFA, a method that effectively combines CL,
teacher-student-based meta-pseudo-labeling and AL to train non-pretrained CNN
architectures for image classification in scenarios of scarcity of labeled and
abundance of unlabeled data. It integrates DeepFA into a co-training setup that
implements two cooperative networks to mitigate confirmation bias from
pseudo-labels. The method starts with a reduced set of labeled samples by
warming up the networks with supervised CL. Afterward and at regular epoch
intervals, label propagation is performed on the 2D projections of the
networks' deep features. Next, the most reliable pseudo-labels are exchanged
between networks in a cross-training fashion, while the most meaningful samples
are annotated and added into the labeled set. The networks independently
minimize an objective loss function comprising supervised contrastive,
supervised and semi-supervised loss components, enhancing the representations
towards image classification. Our approach is evaluated on three challenging
biological image datasets using only 5% of labeled samples, improving baselines
and outperforming six other SoTA methods. In addition, it reduces annotation
effort by achieving comparable results to those of its counterparts with only
3% of labeled data.

</details>

### [249] [SORT3D: Spatial Object-centric Reasoning Toolbox for Zero-Shot 3D Grounding Using Large Language Models](https://arxiv.org/abs/2504.18684)
*Nader Zantout,Haochen Zhang,Pujith Kachana,Jinkai Qiu,Ji Zhang,Wenshan Wang*

Main category: cs.CV

TLDR: SORT3D是一种结合2D数据对象属性和大型语言模型（LLMs）的方法，用于解决3D场景中对象定位的挑战，无需文本到3D数据的训练，并在未见环境中实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 机器人需要理解对象引用语言并在3D场景中定位对象，但由于场景多样性、对象细粒度及语言复杂性，这一任务极具挑战性。此外，3D领域缺乏大规模自然语言训练数据。

Method: SORT3D利用2D数据的丰富对象属性，结合启发式空间推理工具箱和LLMs的序列推理能力，无需文本到3D数据的训练。

Result: SORT3D在两个基准测试中实现了最先进的性能，并在自动驾驶车辆上实时运行，展示了在未见真实环境中的对象目标导航能力。

Conclusion: SORT3D通过结合2D数据和LLMs，有效解决了3D对象定位的挑战，具有零样本泛化能力，适用于实际应用。

Abstract: Interpreting object-referential language and grounding objects in 3D with
spatial relations and attributes is essential for robots operating alongside
humans. However, this task is often challenging due to the diversity of scenes,
large number of fine-grained objects, and complex free-form nature of language
references. Furthermore, in the 3D domain, obtaining large amounts of natural
language training data is difficult. Thus, it is important for methods to learn
from little data and zero-shot generalize to new environments. To address these
challenges, we propose SORT3D, an approach that utilizes rich object attributes
from 2D data and merges a heuristics-based spatial reasoning toolbox with the
ability of large language models (LLMs) to perform sequential reasoning.
Importantly, our method does not require text-to-3D data for training and can
be applied zero-shot to unseen environments. We show that SORT3D achieves
state-of-the-art performance on complex view-dependent grounding tasks on two
benchmarks. We also implement the pipeline to run real-time on an autonomous
vehicle and demonstrate that our approach can be used for object-goal
navigation on previously unseen real-world environments. All source code for
the system pipeline is publicly released at https://github.com/nzantout/SORT3D .

</details>

### [250] [HierSum: A Global and Local Attention Mechanism for Video Summarization](https://arxiv.org/abs/2504.18689)
*Apoorva Beedu,Irfan Essa*

Main category: cs.CV

TLDR: HierSum是一种用于教学视频摘要的分层方法，结合局部字幕和全局上下文信息，利用“最多重播”统计数据提升摘要效果。


<details>
  <summary>Details</summary>
Motivation: 教学视频摘要需要保留关键步骤信息，但现有方法效果有限。

Method: 提出HierSum方法，整合字幕的局部线索和视频级指令的全局上下文，利用“最多重播”统计数据作为监督信号。

Result: 在多个基准数据集上表现优于现有方法，F1分数和排名相关性显著提升。

Conclusion: HierSum通过多模态数据集训练显著提升摘要效果，为教学视频摘要提供了有效解决方案。

Abstract: Video summarization creates an abridged version (i.e., a summary) that
provides a quick overview of the video while retaining pertinent information.
In this work, we focus on summarizing instructional videos and propose a method
for breaking down a video into meaningful segments, each corresponding to
essential steps in the video. We propose \textbf{HierSum}, a hierarchical
approach that integrates fine-grained local cues from subtitles with global
contextual information provided by video-level instructions. Our approach
utilizes the ``most replayed" statistic as a supervisory signal to identify
critical segments, thereby improving the effectiveness of the summary. We
evaluate on benchmark datasets such as TVSum, BLiSS, Mr.HiSum, and the WikiHow
test set, and show that HierSum consistently outperforms existing methods in
key metrics such as F1-score and rank correlation. We also curate a new
multi-modal dataset using WikiHow and EHow videos and associated articles
containing step-by-step instructions. Through extensive ablation studies, we
demonstrate that training on this dataset significantly enhances summarization
on the target datasets.

</details>

### [251] [A Review of 3D Object Detection with Vision-Language Models](https://arxiv.org/abs/2504.18738)
*Ranjan Sapkota,Konstantinos I Roumeliotis,Rahul Harsha Cheppally,Marco Flores Calero,Manoj Karkee*

Main category: cs.CV

TLDR: 本文系统综述了基于视觉语言模型（VLMs）的3D目标检测，分析了100多篇研究论文，总结了该领域的挑战、方法、性能及未来方向。


<details>
  <summary>Details</summary>
Motivation: 探索3D目标检测与视觉语言模型的结合，解决传统方法在空间推理和数据复杂性上的不足，推动多模态AI的发展。

Method: 比较了点云和体素网格的传统方法与现代视觉语言框架（如CLIP和3D LLMs），并分析了关键架构、预训练策略和提示工程方法。

Result: 展示了性能和行为评估的视觉化示例和基准测试，同时指出了当前挑战，如数据集有限和计算需求高。

Conclusion: 提出了未来研究方向，以推动基于视觉语言模型的3D目标检测进一步发展。

Abstract: This review provides a systematic analysis of comprehensive survey of 3D
object detection with vision-language models(VLMs) , a rapidly advancing area
at the intersection of 3D vision and multimodal AI. By examining over 100
research papers, we provide the first systematic analysis dedicated to 3D
object detection with vision-language models. We begin by outlining the unique
challenges of 3D object detection with vision-language models, emphasizing
differences from 2D detection in spatial reasoning and data complexity.
Traditional approaches using point clouds and voxel grids are compared to
modern vision-language frameworks like CLIP and 3D LLMs, which enable
open-vocabulary detection and zero-shot generalization. We review key
architectures, pretraining strategies, and prompt engineering methods that
align textual and 3D features for effective 3D object detection with
vision-language models. Visualization examples and evaluation benchmarks are
discussed to illustrate performance and behavior. Finally, we highlight current
challenges, such as limited 3D-language datasets and computational demands, and
propose future research directions to advance 3D object detection with
vision-language models. >Object Detection, Vision-Language Models, Agents,
VLMs, LLMs, AI

</details>

### [252] [Dream-Box: Object-wise Outlier Generation for Out-of-Distribution Detection](https://arxiv.org/abs/2504.18746)
*Brian K. S. Isaac-Medina,Toby P. Breckon*

Main category: cs.CV

TLDR: 论文提出了一种名为Dream-Box的方法，利用扩散模型在像素空间中生成对象级异常值，用于训练目标检测器进行OOD检测，同时提供可视化支持。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在训练和测试数据分布相同时表现良好，但OOD检测仍具挑战性。现有方法在特征空间生成异常值，但缺乏可视化，而像素空间方法尚未应用于目标检测。

Method: 使用扩散模型在像素空间生成对象级异常值，用于训练目标检测器进行OOD检测。

Result: Dream-Box在性能上与传统方法相当，并首次提供了生成的OOD对象的具体可视化。

Conclusion: Dream-Box为OOD检测提供了一种新方法，结合了性能与可视化优势，填补了目标检测任务中的空白。

Abstract: Deep neural networks have demonstrated great generalization capabilities for
tasks whose training and test sets are drawn from the same distribution.
Nevertheless, out-of-distribution (OOD) detection remains a challenging task
that has received significant attention in recent years. Specifically, OOD
detection refers to the detection of instances that do not belong to the
training distribution, while still having good performance on the
in-distribution task (e.g., classification or object detection). Recent work
has focused on generating synthetic outliers and using them to train an outlier
detector, generally achieving improved OOD detection than traditional OOD
methods. In this regard, outliers can be generated either in feature or pixel
space. Feature space driven methods have shown strong performance on both the
classification and object detection tasks, at the expense that the
visualization of training outliers remains unknown, making further analysis on
OOD failure modes challenging. On the other hand, pixel space outlier
generation techniques enabled by diffusion models have been used for image
classification using, providing improved OOD detection performance and outlier
visualization, although their adaption to the object detection task is as yet
unexplored. We therefore introduce Dream-Box, a method that provides a link to
object-wise outlier generation in the pixel space for OOD detection.
Specifically, we use diffusion models to generate object-wise outliers that are
used to train an object detector for an in-distribution task and OOD detection.
Our method achieves comparable performance to previous traditional methods
while being the first technique to provide concrete visualization of generated
OOD objects.

</details>

### [253] [Multi-Stage Boundary-Aware Transformer Network for Action Segmentation in Untrimmed Surgical Videos](https://arxiv.org/abs/2504.18756)
*Rezowan Shuvo,M S Mekala,Eyad Elyan*

Main category: cs.CV

TLDR: 提出了一种多阶段边界感知Transformer网络（MSBATN），用于手术视频中的动作分割，解决了传统方法在边界模糊和分割质量上的问题。


<details>
  <summary>Details</summary>
Motivation: 手术动作的多样性和模糊边界导致传统分割方法效果不佳，需要更精确的模型来捕捉动作边界。

Method: 采用分层滑动窗口注意力机制和统一的损失函数，将动作分类与边界检测作为相互依赖的任务处理。

Result: 在三个手术数据集上表现优异，F1分数在25%和50%阈值下达到最优，其他指标也表现良好。

Conclusion: MSBATN通过改进边界检测和动作分割，显著提升了手术视频分析的准确性。

Abstract: Understanding actions within surgical workflows is essential for evaluating
post-operative outcomes. However, capturing long sequences of actions performed
in surgical settings poses challenges, as individual surgeons have their unique
approaches shaped by their expertise, leading to significant variability. To
tackle this complex problem, we focused on segmentation with precise
boundaries, a demanding task due to the inherent variability in action
durations and the subtle transitions often observed in untrimmed videos. These
transitions, marked by ambiguous starting and ending points, complicate the
segmentation process. Traditional models, such as MS-TCN, which depend on large
receptive fields, frequently face challenges of over-segmentation (resulting in
fragmented segments) or under-segmentation (merging distinct actions). Both of
these issues negatively impact the quality of segmentation. To overcome these
challenges, we present the Multi-Stage Boundary-Aware Transformer Network
(MSBATN) with hierarchical sliding window attention, designed to enhance action
segmentation. Our proposed approach incorporates a novel unified loss function
that treats action classification and boundary detection as distinct yet
interdependent tasks. Unlike traditional binary boundary detection methods, our
boundary voting mechanism accurately identifies start and end points by
leveraging contextual information. Extensive experiments using three
challenging surgical datasets demonstrate the superior performance of the
proposed method, achieving state-of-the-art results in F1 scores at thresholds
of 25% and 50%, while also delivering comparable performance in other metrics.

</details>

### [254] [PyViT-FUSE: A Foundation Model for Multi-Sensor Earth Observation Data](https://arxiv.org/abs/2504.18770)
*Manuel Weber,Carly Beneke*

Main category: cs.CV

TLDR: PyViT-FUSE是一个用于地球观测数据的基础模型，通过注意力机制融合多模态图像输入，采用金字塔结构的视觉变换器处理，并通过自监督学习训练。


<details>
  <summary>Details</summary>
Motivation: 处理多模态地球观测数据的需求，尤其是如何融合不同分辨率的输入波段。

Method: 使用注意力机制融合多模态输入，采用金字塔结构的视觉变换器处理，基于SwAV算法的自监督学习训练。

Result: 模型展示了注意力分数的可解释性，并适用于下游任务。

Conclusion: PyViT-FUSE为多模态地球观测数据提供了一种有效的融合和处理方法。

Abstract: We propose PyViT-FUSE, a foundation model for earth observation data
explicitly designed to handle multi-modal imagery by learning to fuse an
arbitrary number of mixed-resolution input bands into a single representation
through an attention mechanism. The learned patch tokens are further processed
by a stack of vision transformers with a novel pyramidal structure. We train
the model on a globally sampled dataset in a self-supervised manner, leveraging
core concepts of the SwAV algorithm. We show the interpretability of the fusion
mechanism by visualization of the attention scores and the models applicability
to downstream tasks.

</details>

### [255] [Depth as Points: Center Point-based Depth Estimation](https://arxiv.org/abs/2504.18773)
*Zhiheng Tu,Xinjian Huang,Yong He,Ruiyang Zhou,Bo Du,Weitao Wu*

Main category: cs.CV

TLDR: 提出了一种高效生成虚拟数据集的方法，并基于此构建了VirDepth数据集；同时提出了一种轻量级单目深度估计架构CenterDepth，结合全局语义信息和多尺度特征，显著提升了深度估计的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中感知任务的数据集生成高成本和计算需求大的问题。

Method: 开发高效虚拟数据集生成方法，构建VirDepth数据集；提出CenterDepth架构，结合Center FC-CRFs算法和多尺度特征聚合。

Result: 实验表明，该方法在计算速度和预测精度上均表现优异。

Conclusion: 提出的方法在自动驾驶深度估计任务中具有高效性和优越性能。

Abstract: The perception of vehicles and pedestrians in urban scenarios is crucial for
autonomous driving. This process typically involves complicated data
collection, imposes high computational and hardware demands. To address these
limitations, we first develop a highly efficient method for generating virtual
datasets, which enables the creation of task- and scenario-specific datasets in
a short time. Leveraging this method, we construct the virtual depth estimation
dataset VirDepth, a large-scale, multi-task autonomous driving dataset.
Subsequently, we propose CenterDepth, a lightweight architecture for monocular
depth estimation that ensures high operational efficiency and exhibits superior
performance in depth estimation tasks with highly imbalanced height-scale
distributions. CenterDepth integrates global semantic information through the
innovative Center FC-CRFs algorithm, aggregates multi-scale features based on
object key points, and enables detection-based depth estimation of targets.
Experiments demonstrate that our proposed method achieves superior performance
in terms of both computational speed and prediction accuracy.

</details>

### [256] [IoT Botnet Detection: Application of Vision Transformer to Classification of Network Flow Traffic](https://arxiv.org/abs/2504.18781)
*Hassan Wasswa,Timothy Lynar,Aziida Nanyonga,Hussein Abbass*

Main category: cs.CV

TLDR: 提出了一种新的预处理方法，使视觉变换器（ViT）适用于基于网络流包的物联网僵尸网络攻击检测，并改进了ViT模型以支持多种分类器。


<details>
  <summary>Details</summary>
Motivation: 现有工具无法捕捉网络流包的序列和空间模式，限制了变换器模型在物联网攻击检测中的应用。

Method: 从.pcap文件中提取特征，将每个实例转换为1通道2D图像形状，并改进ViT模型以支持多种分类器（如DNN、LSTM、BLSTM）。

Result: 在两种物联网攻击数据集上，改进后的模型在多类攻击检测中表现出色，精度、召回率和F1分数均具竞争力。

Conclusion: 该方法成功地将ViT应用于物联网攻击检测，并展示了其灵活性和有效性。

Abstract: Despite the demonstrated effectiveness of transformer models in NLP, and
image and video classification, the available tools for extracting features
from captured IoT network flow packets fail to capture sequential patterns in
addition to the absence of spatial patterns consequently limiting transformer
model application. This work introduces a novel preprocessing method to adapt
transformer models, the vision transformer (ViT) in particular, for IoT botnet
attack detection using network flow packets. The approach involves feature
extraction from .pcap files and transforming each instance into a 1-channel 2D
image shape, enabling ViT-based classification. Also, the ViT model was
enhanced to allow use any classifier besides Multilayer Perceptron (MLP) that
was deployed in the initial ViT paper. Models including the conventional feed
forward Deep Neural Network (DNN), LSTM and Bidirectional-LSTM (BLSTM)
demonstrated competitive performance in terms of precision, recall, and
F1-score for multiclass-based attack detection when evaluated on two IoT attack
datasets.

</details>

### [257] [CAMeL: Cross-modality Adaptive Meta-Learning for Text-based Person Retrieval](https://arxiv.org/abs/2504.18782)
*Hang Yu,Jiahao Wen,Zhedong Zheng*

Main category: cs.CV

TLDR: 本文提出了一种基于跨模态自适应元学习（CAMeL）的领域无关预训练框架，以提升模型在预训练阶段的泛化能力，从而更好地支持下游任务。


<details>
  <summary>Details</summary>
Motivation: 由于标注成本高和隐私保护问题，研究人员依赖合成数据进行预训练和微调，但这些数据存在领域偏差，限制了预训练模型的可扩展性。

Method: 开发了一系列反映真实场景多样性和复杂性的任务，并引入动态错误样本记忆单元记录多任务中的错误历史，同时采用自适应双速更新策略平衡新任务和历史任务的权重更新。

Result: 在真实世界基准测试（如CUHK-PEDES、ICFG-PEDES和RSTPReid）上超越了现有最优方法，并在处理有偏合成图像和噪声文本标注时表现出鲁棒性和可扩展性。

Conclusion: CAMeL框架简单有效，显著提升了模型在文本-图像检索任务中的性能，并展示了其在复杂场景下的适应能力。

Abstract: Text-based person retrieval aims to identify specific individuals within an
image database using textual descriptions. Due to the high cost of annotation
and privacy protection, researchers resort to synthesized data for the paradigm
of pretraining and fine-tuning. However, these generated data often exhibit
domain biases in both images and textual annotations, which largely compromise
the scalability of the pre-trained model. Therefore, we introduce a
domain-agnostic pretraining framework based on Cross-modality Adaptive
Meta-Learning (CAMeL) to enhance the model generalization capability during
pretraining to facilitate the subsequent downstream tasks. In particular, we
develop a series of tasks that reflect the diversity and complexity of
real-world scenarios, and introduce a dynamic error sample memory unit to
memorize the history for errors encountered within multiple tasks. To further
ensure multi-task adaptation, we also adopt an adaptive dual-speed update
strategy, balancing fast adaptation to new tasks and slow weight updates for
historical tasks. Albeit simple, our proposed model not only surpasses existing
state-of-the-art methods on real-world benchmarks, including CUHK-PEDES,
ICFG-PEDES, and RSTPReid, but also showcases robustness and scalability in
handling biased synthetic images and noisy text annotations. Our code is
available at https://github.com/Jahawn-Wen/CAMeL-reID.

</details>

### [258] [Video CLIP Model for Multi-View Echocardiography Interpretation](https://arxiv.org/abs/2504.18800)
*Ryo Takizawa,Satoshi Kodera,Tempei Kabayama,Ryo Matsuoka,Yuta Ando,Yuto Nakamura,Haruki Settai,Norihiko Takeda*

Main category: cs.CV

TLDR: 开发了一种基于视频语言模型的多视角超声心动图分析方法，优于单视角或静态图像模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型多依赖单帧图像输入，对依赖心脏运动的诊断准确性较低，且未充分利用多视角信息。

Method: 采用五视角和完整视频序列作为输入，基于60,747例超声心动图视频与临床报告对进行训练。

Result: 多视角视频模型比单视角或静态图像模型具有更高的诊断准确性。

Conclusion: 多视角视频输入能显著提升超声心动图的自动诊断准确性。

Abstract: Echocardiography involves recording videos of the heart using ultrasound,
enabling clinicians to evaluate its condition. Recent advances in large-scale
vision-language models (VLMs) have garnered attention for automating the
interpretation of echocardiographic videos. However, most existing VLMs
proposed for medical interpretation thus far rely on single-frame (i.e., image)
inputs. Consequently, these image-based models often exhibit lower diagnostic
accuracy for conditions identifiable through cardiac motion. Moreover,
echocardiographic videos are recorded from various views that depend on the
direction of ultrasound emission, and certain views are more suitable than
others for interpreting specific conditions. Incorporating multiple views could
potentially yield further improvements in accuracy. In this study, we developed
a video-language model that takes five different views and full video sequences
as input, training it on pairs of echocardiographic videos and clinical reports
from 60,747 cases. Our experiments demonstrate that this expanded approach
achieves higher interpretation accuracy than models trained with only
single-view videos or with still images.

</details>

### [259] [Audio-Driven Talking Face Video Generation with Joint Uncertainty Learning](https://arxiv.org/abs/2504.18810)
*Yifan Xie,Fei Ma,Yi Bin,Ying He,Fei Yu*

Main category: cs.CV

TLDR: 提出了一种联合不确定性学习网络（JULNet），通过预测误差图和不确定性图，并结合KL散度优化，提升了说话人脸视频生成的视觉质量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉不确定性的学习上关注不足，导致生成视频的视觉质量不一致且性能不可靠。

Method: 设计不确定性模块预测误差图和不确定性图，引入直方图技术和KL散度优化联合误差和不确定性。

Result: 实验表明，该方法在视觉保真度和音频-唇同步方面优于现有方法。

Conclusion: JULNet通过联合优化误差和不确定性，显著提升了说话人脸视频生成的质量和鲁棒性。

Abstract: Talking face video generation with arbitrary speech audio is a significant
challenge within the realm of digital human technology. The previous studies
have emphasized the significance of audio-lip synchronization and visual
quality. Currently, limited attention has been given to the learning of visual
uncertainty, which creates several issues in existing systems, including
inconsistent visual quality and unreliable performance across different input
conditions. To address the problem, we propose a Joint Uncertainty Learning
Network (JULNet) for high-quality talking face video generation, which
incorporates a representation of uncertainty that is directly related to visual
error. Specifically, we first design an uncertainty module to individually
predict the error map and uncertainty map after obtaining the generated image.
The error map represents the difference between the generated image and the
ground truth image, while the uncertainty map is used to predict the
probability of incorrect estimates. Furthermore, to match the uncertainty
distribution with the error distribution through a KL divergence term, we
introduce a histogram technique to approximate the distributions. By jointly
optimizing error and uncertainty, the performance and robustness of our model
can be enhanced. Extensive experiments demonstrate that our method achieves
superior high-fidelity and audio-lip synchronization in talking face video
generation compared to previous methods.

</details>

### [260] [Multi-Resolution Pathology-Language Pre-training Model with Text-Guided Visual Representation](https://arxiv.org/abs/2504.18856)
*Shahad Albastaki,Anabia Sohail,Iyyakutti Iyappan Ganapathi,Basit Alawode,Asim Khan,Sajid Javed,Naoufel Werghi,Mohammed Bennamoun,Arif Mahmood*

Main category: cs.CV

TLDR: 提出了一种多分辨率视觉语言模型（VLM）方法，用于计算病理学（CPath），通过多分辨率对齐和跨分辨率对齐提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有单分辨率对齐方法在癌症亚型分类等任务中表现不足，多分辨率方法能提供更丰富的细节信息。

Method: 利用全切片图像（WSIs）提取多分辨率组织学图像块，生成对应文本描述，并通过多模态编码器实现跨分辨率对齐。

Result: 在TCGA数据集上预训练后，模型在多个任务中优于现有方法。

Conclusion: 多分辨率VLM方法显著提升了CPath任务的性能，代码已开源。

Abstract: In Computational Pathology (CPath), the introduction of Vision-Language
Models (VLMs) has opened new avenues for research, focusing primarily on
aligning image-text pairs at a single magnification level. However, this
approach might not be sufficient for tasks like cancer subtype classification,
tissue phenotyping, and survival analysis due to the limited level of detail
that a single-resolution image can provide. Addressing this, we propose a novel
multi-resolution paradigm leveraging Whole Slide Images (WSIs) to extract
histology patches at multiple resolutions and generate corresponding textual
descriptions through advanced CPath VLM. We introduce visual-textual alignment
at multiple resolutions as well as cross-resolution alignment to establish more
effective text-guided visual representations. Cross-resolution alignment using
a multimodal encoder enhances the model's ability to capture context from
multiple resolutions in histology images. Our model aims to capture a broader
range of information, supported by novel loss functions, enriches feature
representation, improves discriminative ability, and enhances generalization
across different resolutions. Pre-trained on a comprehensive TCGA dataset with
34 million image-language pairs at various resolutions, our fine-tuned model
outperforms state-of-the-art (SOTA) counterparts across multiple datasets and
tasks, demonstrating its effectiveness in CPath. The code is available on
GitHub at: https://github.com/BasitAlawode/MR-PLIP

</details>

### [261] [Spike Imaging Velocimetry: Dense Motion Estimation of Fluids Using Spike Cameras](https://arxiv.org/abs/2504.18864)
*Yunzhong Zhang,Bo Xiong,You Zhou,Changqing Su,Zhen Cheng,Zhaofei Yu,Xun Cao,Tiejun Huang*

Main category: cs.CV

TLDR: 提出了一种基于脉冲相机的深度学习框架（SIV），用于高湍流复杂流场的粒子图像测速（PIV），并通过新数据集PSSD验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 需要准确且非侵入式的流场测量方法，脉冲相机在PIV中具有巨大潜力。

Method: 结合了细节保持分层变换（DPHT）模块和图形编码器（GE），设计了SIV框架，并创建了PSSD数据集。

Result: 在PSSD数据集上，SIV表现优于现有基线方法。

Conclusion: SIV为复杂流场测速提供了有效解决方案，相关数据和代码已开源。

Abstract: The need for accurate and non-intrusive flow measurement methods has led to
the widespread adoption of Particle Image Velocimetry (PIV), a powerful
diagnostic tool in fluid motion estimation. This study investigates the
tremendous potential of spike cameras (a type of ultra-high-speed,
high-dynamic-range camera) in PIV. We propose a deep learning framework, Spike
Imaging Velocimetry (SIV), designed specifically for highly turbulent and
intricate flow fields. To aggregate motion features from the spike stream while
minimizing information loss, we incorporate a Detail-Preserving Hierarchical
Transform (DPHT) module. Additionally, we introduce a Graph Encoder (GE) to
extract contextual features from highly complex fluid flows. Furthermore, we
present a spike-based PIV dataset, Particle Scenes with Spike and Displacement
(PSSD), which provides labeled data for three challenging fluid dynamics
scenarios. Our proposed method achieves superior performance compared to
existing baseline methods on PSSD. The datasets and our implementation of SIV
are open-sourced in the supplementary materials.

</details>

### [262] [PiercingEye: Dual-Space Video Violence Detection with Hyperbolic Vision-Language Guidance](https://arxiv.org/abs/2504.18866)
*Jiaxu Leng,Zhanjie Wu,Mingpi Tan,Mengjingcheng Mo,Jiankang Zheng,Qingqing Li,Ji Gan,Xinbo Gao*

Main category: cs.CV

TLDR: 论文提出PiercingEye框架，结合欧几里得和双曲几何，通过双空间学习和逻辑引导的模糊样本生成，提升视频暴力检测的细粒度性能。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督视频暴力检测方法依赖欧几里得表示学习，难以区分视觉相似但语义不同的事件，且模糊样本不足。

Method: 提出PiercingEye框架，结合双曲几何的层次建模和跨空间注意力机制，利用大语言模型生成模糊样本，并通过双曲视觉-语言对比损失优化。

Result: 在XD-Violence和UCF-Crime基准测试中达到最优性能，尤其在模糊事件子集上表现突出。

Conclusion: PiercingEye通过双空间学习和模糊样本增强，显著提升了视频暴力检测的细粒度能力。

Abstract: Existing weakly supervised video violence detection (VVD) methods primarily
rely on Euclidean representation learning, which often struggles to distinguish
visually similar yet semantically distinct events due to limited hierarchical
modeling and insufficient ambiguous training samples. To address this
challenge, we propose PiercingEye, a novel dual-space learning framework that
synergizes Euclidean and hyperbolic geometries to enhance discriminative
feature representation. Specifically, PiercingEye introduces a layer-sensitive
hyperbolic aggregation strategy with hyperbolic Dirichlet energy constraints to
progressively model event hierarchies, and a cross-space attention mechanism to
facilitate complementary feature interactions between Euclidean and hyperbolic
spaces. Furthermore, to mitigate the scarcity of ambiguous samples, we leverage
large language models to generate logic-guided ambiguous event descriptions,
enabling explicit supervision through a hyperbolic vision-language contrastive
loss that prioritizes high-confusion samples via dynamic similarity-aware
weighting. Extensive experiments on XD-Violence and UCF-Crime benchmarks
demonstrate that PiercingEye achieves state-of-the-art performance, with
particularly strong results on a newly curated ambiguous event subset,
validating its superior capability in fine-grained violence detection.

</details>

### [263] [WLTCL: Wide Field-of-View 3-D LiDAR Truck Compartment Automatic Localization System](https://arxiv.org/abs/2504.18870)
*Guodong Sun,Mingjing Li,Dingjie Liu,Mingxuan Liu,Bo Wu,Yang Zhang*

Main category: cs.CV

TLDR: 提出了一种基于宽视场3D LiDAR的卡车车厢自动定位系统，解决现有方法对不同尺寸车厢适应性差、坐标系不统一及可靠性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 物流自动化中，卡车车厢的精确自动定位是关键，但现有方法难以适应不同尺寸车厢，且缺乏统一的坐标系，在复杂环境中可靠性低。

Method: 利用宽视场3D LiDAR生成高密度点云，结合停车区域约束分割车厢点云，并通过几何特征精确定位车厢角点。

Result: 实验表明，该系统定位精度高，计算资源消耗低，适用于不同尺寸车厢的复杂场景。

Conclusion: 该系统为物流自动化提供了可靠的解决方案，具有广泛应用前景。

Abstract: As an essential component of logistics automation, the automated loading
system is becoming a critical technology for enhancing operational efficiency
and safety. Precise automatic positioning of the truck compartment, which
serves as the loading area, is the primary step in automated loading. However,
existing methods have difficulty adapting to truck compartments of various
sizes, do not establish a unified coordinate system for LiDAR and mobile
manipulators, and often exhibit reliability issues in cluttered environments.
To address these limitations, our study focuses on achieving precise automatic
positioning of key points in large, medium, and small fence-style truck
compartments in cluttered scenarios. We propose an innovative wide
field-of-view 3-D LiDAR vehicle compartment automatic localization system. For
vehicles of various sizes, this system leverages the LiDAR to generate
high-density point clouds within an extensive field-of-view range. By
incorporating parking area constraints, our vehicle point cloud segmentation
method more effectively segments vehicle point clouds within the scene. Our
compartment key point positioning algorithm utilizes the geometric features of
the compartments to accurately locate the corner points, providing stackable
spatial regions. Extensive experiments on our collected data and public
datasets demonstrate that this system offers reliable positioning accuracy and
reduced computational resource consumption, leading to its application and
promotion in relevant fields.

</details>

### [264] [Exploiting Multiple Representations: 3D Face Biometrics Fusion with Application to Surveillance](https://arxiv.org/abs/2504.18886)
*Simone Maurizio La Cava,Roberto Casula,Sara Concas,Giulia Orrù,Ruben Tolosana,Martin Drahansky,Julian Fierrez,Gian Luca Marcialis*

Main category: cs.CV

TLDR: 研究探讨如何利用多种3D人脸重建算法提升人脸识别系统在复杂场景中的性能，并通过融合策略增强生物识别鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对不同应用场景的3D人脸重建算法存在局限性，研究旨在通过融合多种算法提升人脸识别系统的泛化能力和鲁棒性。

Method: 采用参数化和非参数化的分数级融合方法，结合多种3D人脸重建算法，并在不同条件下（如距离、相机设置等）进行实验验证。

Result: 实验表明，多种算法的信息互补可以提升泛化能力，融合策略能显著增强3D人脸重建在人脸识别中的可靠性。

Conclusion: 研究为3D人脸重建算法在生物识别中的应用提供了新思路，融合方法可推广至其他人脸相关任务。

Abstract: 3D face reconstruction (3DFR) algorithms are based on specific assumptions
tailored to the limits and characteristics of the different application
scenarios. In this study, we investigate how multiple state-of-the-art 3DFR
algorithms can be used to generate a better representation of subjects, with
the final goal of improving the performance of face recognition systems in
challenging uncontrolled scenarios. We also explore how different parametric
and non-parametric score-level fusion methods can exploit the unique strengths
of multiple 3DFR algorithms to enhance biometric recognition robustness. With
this goal, we propose a comprehensive analysis of several face recognition
systems across diverse conditions, such as varying distances and camera setups,
intra-dataset and cross-dataset, to assess the robustness of the proposed
ensemble method. The results demonstrate that the distinct information provided
by different 3DFR algorithms can alleviate the problem of generalizing over
multiple application scenarios. In addition, the present study highlights the
potential of advanced fusion strategies to enhance the reliability of
3DFR-based face recognition systems, providing the research community with key
insights to exploit them in real-world applications effectively. Although the
experiments are carried out in a specific face verification setup, our proposed
fusion-based 3DFR methods may be applied to other tasks around face biometrics
that are not strictly related to identity recognition.

</details>

### [265] [Sim-to-Real: An Unsupervised Noise Layer for Screen-Camera Watermarking Robustness](https://arxiv.org/abs/2504.18906)
*Yufeng Wu,Xin Liao,Baowei Wang,Han Fang,Xiaoshuai Wu,Guiling Wang*

Main category: cs.CV

TLDR: 提出了一种名为Simulation-to-Real (S2R)的无监督噪声层方法，用于增强屏幕-相机图像水印的鲁棒性，解决了现有方法在噪声模拟上的不足。


<details>
  <summary>Details</summary>
Motivation: 未经授权的屏幕截图和传播会导致数据泄露和信息盗窃，现有水印方法在噪声模拟上存在偏差或依赖配对数据，无法有效逼近真实噪声。

Method: 采用无监督噪声层，利用未配对数据学习模拟噪声分布与真实噪声分布之间的差异，而非直接学习从清晰图像到真实图像的映射。

Result: 实验结果表明，该方法在水印鲁棒性和泛化性上优于现有先进方法。

Conclusion: S2R方法通过简化噪声分布的学习任务，显著提升了水印技术的效果。

Abstract: Unauthorized screen capturing and dissemination pose severe security threats
such as data leakage and information theft. Several studies propose robust
watermarking methods to track the copyright of Screen-Camera (SC) images,
facilitating post-hoc certification against infringement. These techniques
typically employ heuristic mathematical modeling or supervised neural network
fitting as the noise layer, to enhance watermarking robustness against SC.
However, both strategies cannot fundamentally achieve an effective
approximation of SC noise. Mathematical simulation suffers from biased
approximations due to the incomplete decomposition of the noise and the absence
of interdependence among the noise components. Supervised networks require
paired data to train the noise-fitting model, and it is difficult for the model
to learn all the features of the noise. To address the above issues, we propose
Simulation-to-Real (S2R). Specifically, an unsupervised noise layer employs
unpaired data to learn the discrepancy between the modeling simulated noise
distribution and the real-world SC noise distribution, rather than directly
learning the mapping from sharp images to real-world images. Learning this
transformation from simulation to reality is inherently simpler, as it
primarily involves bridging the gap in noise distributions, instead of the
complex task of reconstructing fine-grained image details. Extensive
experimental results validate the efficacy of the proposed method,
demonstrating superior watermark robustness and generalization compared to
those of state-of-the-art methods.

</details>

### [266] [Kinship Verification through a Forest Neural Network](https://arxiv.org/abs/2504.18910)
*Ali Nazari,Mohsen Ebrahimi Moghaddam,Omidreza Borzoei*

Main category: cs.CV

TLDR: 提出了一种基于图神经网络的方法，用于亲属关系验证，结合中心损失逐步训练网络，在KinFaceW-I和II数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 早期方法使用面部表示进行亲属关系验证，但准确性不如联合表示方法。本文旨在通过图神经网络提升准确性。

Method: 采用图神经网络概念，设计分类模块结构，并引入中心损失组合逐步训练网络。

Result: 在KinFaceW-II上取得最佳结果，平均提升1.6；在KinFaceW-I上接近最佳表现。

Conclusion: 所提方法在亲属关系验证中表现优异，代码已开源。

Abstract: Early methods used face representations in kinship verification, which are
less accurate than joint representations of parents' and children's facial
images learned from scratch. We propose an approach featuring graph neural
network concepts to utilize face representations and have comparable results to
joint representation algorithms. Moreover, we designed the structure of the
classification module and introduced a new combination of losses to engage the
center loss gradually in training our network. Additionally, we conducted
experiments on KinFaceW-I and II, demonstrating the effectiveness of our
approach. We achieved the best result on KinFaceW-II, an average improvement of
nearly 1.6 for all kinship types, and we were near the best on KinFaceW-I. The
code is available at https://github.com/ali-nazari/Kinship-Verification

</details>

### [267] [R-Sparse R-CNN: SAR Ship Detection Based on Background-Aware Sparse Learnable Proposals](https://arxiv.org/abs/2504.18959)
*Kamirul Kamirul,Odysseas Pappas,Alin Achim*

Main category: cs.CV

TLDR: R-Sparse R-CNN是一种用于SAR图像中定向船舶检测的新方法，通过稀疏可学习提案和背景感知提案（BAPs）提升检测精度，结合双上下文池化（DCP）和交互模块实现高效和准确的检测。


<details>
  <summary>Details</summary>
Motivation: 解决SAR图像中船舶检测的复杂性和冗余计算问题，提升检测精度和效率。

Method: 采用稀疏提案（BAPs）和双上下文池化（DCP）策略，结合基于Transformer的交互模块。

Result: 在SSDD和RSDD-SAR数据集上分别超越现有方法12.8%和11.9%。

Conclusion: R-Sparse R-CNN是一种高效且准确的SAR图像船舶检测框架。

Abstract: We introduce R-Sparse R-CNN, a novel pipeline for oriented ship detection in
Synthetic Aperture Radar (SAR) images that leverages sparse learnable proposals
enriched with background contextual information, termed background-aware
proposals (BAPs). The adoption of sparse proposals streamlines the pipeline by
eliminating the need for proposal generators and post-processing for
overlapping predictions. The proposed BAPs enrich object representation by
integrating ship and background features, allowing the model to learn their
contextual relationships for more accurate distinction of ships in complex
environments. To complement BAPs, we propose Dual-Context Pooling (DCP), a
novel strategy that jointly extracts ship and background features in a single
unified operation. This unified design improves efficiency by eliminating
redundant computation inherent in separate pooling. Moreover, by ensuring that
ship and background features are pooled from the same feature map level, DCP
provides aligned features that improve contextual relationship learning.
Finally, as a core component of contextual relationship learning in R-Sparse
R-CNN, we design a dedicated transformer-based Interaction Module. This module
interacts pooled ship and background features with corresponding proposal
features and models their relationships. Experimental results show that
R-Sparse R-CNN delivers outstanding accuracy, surpassing state-of-the-art
models by margins of up to 12.8% and 11.9% on SSDD and RSDD-SAR inshore
datasets, respectively. These results demonstrate the effectiveness and
competitiveness of R-Sparse R-CNN as a robust framework for oriented ship
detection in SAR imagery. The code is available at:
www.github.com/ka-mirul/R-Sparse-R-CNN.

</details>

### [268] [3DPyranet Features Fusion for Spatio-temporal Feature Learning](https://arxiv.org/abs/2504.18977)
*Ihsan Ullah,Alfredo Petrosino*

Main category: cs.CV

TLDR: 提出了一种名为3DPyraNet的3D金字塔神经网络及其变体3DPyraNet-F，用于时空特征学习，减少了参数和计算成本，并在视频动作和场景识别中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统CNN的深层变体增加了参数数量，部分丧失了CNN参数少的优势，因此需要一种既能保持空间拓扑结构又能减少参数和计算成本的新方法。

Method: 3DPyraNet引入了一种新的加权方案，从时空维度学习特征，并保持生物合理的结构；3DPyraNet-F则提取最高层特征图并通过线性SVM分类器增强识别能力。

Result: 3DPyraNet在真实环境中表现良好，尤其在相机运动情况下；3DPyraNet-F在三个基准数据集上优于现有技术，在第四个数据集上表现相当。

Conclusion: 3DPyraNet和3DPyraNet-F在减少参数和计算成本的同时，有效提升了视频动作和场景识别的性能。

Abstract: Convolutional neural network (CNN) slides a kernel over the whole image to
produce an output map. This kernel scheme reduces the number of parameters with
respect to a fully connected neural network (NN). While CNN has proven to be an
effective model in recognition of handwritten characters and traffic signal
sign boards, etc. recently, its deep variants have proven to be effective in
similar as well as more challenging applications like object, scene and action
recognition. Deep CNN add more layers and kernels to the classical CNN,
increasing the number of parameters, and partly reducing the main advantage of
CNN which is less parameters. In this paper, a 3D pyramidal neural network
called 3DPyraNet and a discriminative approach for spatio-temporal feature
learning based on it, called 3DPyraNet-F, are proposed. 3DPyraNet introduces a
new weighting scheme which learns features from both spatial and temporal
dimensions analyzing multiple adjacent frames and keeping a biological
plausible structure. It keeps the spatial topology of the input image and
presents fewer parameters and lower computational and memory costs compared to
both fully connected NNs and recent deep CNNs. 3DPyraNet-F extract the features
maps of the highest layer of the learned network, fuse them in a single vector,
and provide it as input in such a way to a linear-SVM classifier that enhances
the recognition of human actions and dynamic scenes from the videos.
Encouraging results are reported with 3DPyraNet in real-world environments,
especially in the presence of camera induced motion. Further, 3DPyraNet-F
clearly outperforms the state-of-the-art on three benchmark datasets and shows
comparable result for the fourth.

</details>

### [269] [MediAug: Exploring Visual Augmentation in Medical Imaging](https://arxiv.org/abs/2504.18983)
*Xuyin Qi,Zeyu Zhang,Canxuan Gang,Hao Zhang,Lei Zhang,Zhiwei Zhang,Yang Zhao*

Main category: cs.CV

TLDR: 论文提出MediAug框架，系统评估六种混合增强方法在医学影像任务中的表现，并展示了不同方法在不同任务和架构中的效果。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中数据增强的两大挑战：自然图像与医学图像的领域差异，以及现有研究碎片化、缺乏统一评估的问题。

Method: 提出MediAug框架，集成六种混合增强方法（MixUp、YOCO、CropMix、CutMix、AugMix、SnapMix），并在卷积（ResNet-50）和Transformer（ViT-B）架构上进行评估。

Result: MixUp在脑肿瘤分类任务中表现最佳（ResNet-50准确率79.19%），SnapMix在ViT-B中表现最佳（99.44%）；YOCO在眼病分类任务中表现最佳（ResNet-50准确率91.60%），CutMix在ViT-B中表现最佳（97.94%）。

Conclusion: MediAug为医学影像数据增强提供了统一且可复现的基准，展示了混合增强方法的潜力，并明确了不同方法在不同任务中的优势。

Abstract: Data augmentation is essential in medical imaging for improving
classification accuracy, lesion detection, and organ segmentation under limited
data conditions. However, two significant challenges remain. First, a
pronounced domain gap between natural photographs and medical images can
distort critical disease features. Second, augmentation studies in medical
imaging are fragmented and limited to single tasks or architectures, leaving
the benefits of advanced mix-based strategies unclear. To address these
challenges, we propose a unified evaluation framework with six mix-based
augmentation methods integrated with both convolutional and transformer
backbones on brain tumour MRI and eye disease fundus datasets. Our
contributions are threefold. (1) We introduce MediAug, a comprehensive and
reproducible benchmark for advanced data augmentation in medical imaging. (2)
We systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix
with ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive
experiments that MixUp yields the greatest improvement on the brain tumor
classification task for ResNet-50 with 79.19% accuracy and SnapMix yields the
greatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the
greatest improvement on the eye disease classification task for ResNet-50 with
91.60% accuracy and CutMix yields the greatest improvement for ViT-B with
97.94% accuracy. Code will be available at
https://github.com/AIGeeksGroup/MediAug.

</details>

### [270] [VISUALCENT: Visual Human Analysis using Dynamic Centroid Representation](https://arxiv.org/abs/2504.19032)
*Niaz Ahmad,Youngmoon Lee,Guanghui Wang*

Main category: cs.CV

TLDR: VISUALCENT是一个统一的人体姿态和实例分割框架，通过基于质心的自底向上关键点检测和动态质心聚类，解决了多人视觉人体分析的通用性和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 解决多人视觉人体分析中通用性和可扩展性的限制。

Method: 采用基于质心的自底向上关键点检测范式，结合关键点热图和动态质心（MaskCentroid）进行像素聚类。

Result: 在COCO和OCHuman数据集上表现出色，mAP分数和执行帧率优于现有方法。

Conclusion: VISUALCENT在准确性和实时性能上具有优势，适用于快速变化或遮挡严重的场景。

Abstract: We introduce VISUALCENT, a unified human pose and instance segmentation
framework to address generalizability and scalability limitations to multi
person visual human analysis. VISUALCENT leverages centroid based bottom up
keypoint detection paradigm and uses Keypoint Heatmap incorporating Disk
Representation and KeyCentroid to identify the optimal keypoint coordinates.
For the unified segmentation task, an explicit keypoint is defined as a dynamic
centroid called MaskCentroid to swiftly cluster pixels to specific human
instance during rapid changes in human body movement or significantly occluded
environment. Experimental results on COCO and OCHuman datasets demonstrate
VISUALCENTs accuracy and real time performance advantages, outperforming
existing methods in mAP scores and execution frame rate per second. The
implementation is available on the project page.

</details>

### [271] [Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions](https://arxiv.org/abs/2504.19056)
*Mohammad Mahdi Abootorabi,Omid Ghahroodi,Pardis Sadat Zahraei,Hossein Behzadasl,Alireza Mirrokni,Mobina Salimipanah,Arash Rasouli,Bahar Behzadipour,Sara Azarnoush,Benyamin Maleki,Erfan Sadraiye,Kiarash Kiani Feriz,Mahdi Teymouri Nahad,Ali Moghadasi,Abolfazl Eshagh Abianeh,Nizi Nazar,Hamid R. Rabiee,Mahdieh Soleymani Baghshah,Meisam Ahmadi,Ehsaneddin Asgari*

Main category: cs.CV

TLDR: 本文综述了生成式AI在角色动画中的应用，包括面部动画、表情渲染、图像合成、手势建模等，提供了最新研究、数据集和趋势，并为新手提供了基础知识。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在动画领域的快速发展导致领域知识分散，需要一篇综合性的综述来整合这些信息。

Method: 通过分析面部动画、表情渲染、图像合成、手势建模等多个领域的最新研究，提供全面的视角和背景知识。

Result: 总结了生成式AI在角色动画中的主要应用、研究进展、数据集和未来趋势。

Conclusion: 本文为研究人员和开发者提供了生成式AI动画领域的综合资源，并指出了未来的研究方向。

Abstract: Generative AI is reshaping art, gaming, and most notably animation. Recent
breakthroughs in foundation and diffusion models have reduced the time and cost
of producing animated content. Characters are central animation components,
involving motion, emotions, gestures, and facial expressions. The pace and
breadth of advances in recent months make it difficult to maintain a coherent
view of the field, motivating the need for an integrative review. Unlike
earlier overviews that treat avatars, gestures, or facial animation in
isolation, this survey offers a single, comprehensive perspective on all the
main generative AI applications for character animation. We begin by examining
the state-of-the-art in facial animation, expression rendering, image
synthesis, avatar creation, gesture modeling, motion synthesis, object
generation, and texture synthesis. We highlight leading research, practical
deployments, commonly used datasets, and emerging trends for each area. To
support newcomers, we also provide a comprehensive background section that
introduces foundational models and evaluation metrics, equipping readers with
the knowledge needed to enter the field. We discuss open challenges and map
future research directions, providing a roadmap to advance AI-driven
character-animation technologies. This survey is intended as a resource for
researchers and developers entering the field of generative AI animation or
adjacent fields. Resources are available at:
https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.

</details>

### [272] [Dual-Branch Residual Network for Cross-Domain Few-Shot Hyperspectral Image Classification with Refined Prototype](https://arxiv.org/abs/2504.19074)
*Anyong Qin,Chaoqi Yuan,Qiang Li,Feng Yang,Tiecheng Song,Chenqiang Gao*

Main category: cs.CV

TLDR: 提出了一种双分支残差网络，结合空间和光谱特征，并通过正则项优化原型，同时采用核概率匹配策略缓解域偏移，在少量样本场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 3D卷积神经网络在HSI分类中计算成本高且泛化能力有限，域偏移问题进一步影响跨数据集适应性。

Method: 采用双分支残差网络整合空间和光谱特征，通过正则项优化原型，并引入核概率匹配策略对齐源域和目标域特征。

Result: 在四个公开HSI数据集上的实验表明，该方法优于其他方法。

Conclusion: 该方法在少量样本场景下有效提升了HSI分类性能，并缓解了域偏移问题。

Abstract: Convolutional neural networks (CNNs) are effective for hyperspectral image
(HSI) classification, but their 3D convolutional structures introduce high
computational costs and limited generalization in few-shot scenarios. Domain
shifts caused by sensor differences and environmental variations further hinder
cross-dataset adaptability. Metric-based few-shot learning (FSL) prototype
networks mitigate this problem, yet their performance is sensitive to prototype
quality, especially with limited samples. To overcome these challenges, a
dual-branch residual network that integrates spatial and spectral features via
parallel branches is proposed in this letter. Additionally, more robust refined
prototypes are obtained through a regulation term. Furthermore, a kernel
probability matching strategy aligns source and target domain features,
alleviating domain shift. Experiments on four publicly available HSI datasets
illustrate that the proposal achieves superior performance compared to other
methods.

</details>

### [273] [HoloDx: Knowledge- and Data-Driven Multimodal Diagnosis of Alzheimer's Disease](https://arxiv.org/abs/2504.19075)
*Qiuhui Chen,Jintao Wang,Gang Wang,Yi Hong*

Main category: cs.CV

TLDR: HoloDx是一个结合领域知识和多模态临床数据的框架，通过动态整合专家知识和数据提升阿尔茨海默病诊断的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以充分利用多模态信息和动态领域知识，HoloDx旨在解决这一问题。

Method: HoloDx采用知识注入模块（知识感知门控交叉注意力）和记忆注入模块（原型记忆注意力），动态整合领域知识和患者数据。

Result: 在五个AD数据集上，HoloDx优于现有方法，表现出更高的诊断准确性和泛化能力。

Conclusion: HoloDx通过结合知识和数据，显著提升了AD诊断的准确性和鲁棒性。

Abstract: Accurate diagnosis of Alzheimer's disease (AD) requires effectively
integrating multimodal data and clinical expertise. However, existing methods
often struggle to fully utilize multimodal information and lack structured
mechanisms to incorporate dynamic domain knowledge. To address these
limitations, we propose HoloDx, a knowledge- and data-driven framework that
enhances AD diagnosis by aligning domain knowledge with multimodal clinical
data. HoloDx incorporates a knowledge injection module with a knowledge-aware
gated cross-attention, allowing the model to dynamically integrate
domain-specific insights from both large language models (LLMs) and clinical
expertise. Also, a memory injection module with a designed prototypical memory
attention enables the model to retain and retrieve subject-specific
information, ensuring consistency in decision-making. By jointly leveraging
these mechanisms, HoloDx enhances interpretability, improves robustness, and
effectively aligns prior knowledge with current subject data. Evaluations on
five AD datasets demonstrate that HoloDx outperforms state-of-the-art methods,
achieving superior diagnostic accuracy and strong generalization across diverse
cohorts. The source code will be released upon publication acceptance.

</details>

### [274] [Learning to Drive from a World Model](https://arxiv.org/abs/2504.19077)
*Mitchell Goff,Greg Hogan,George Hotz,Armand du Parc Locmaria,Kacper Raczy,Harald Schäfer,Adeeb Shihadeh,Weixing Zhang,Yassine Yousfi*

Main category: cs.CV

TLDR: 论文提出了一种端到端的训练架构，利用真实驾驶数据在模拟器中训练驾驶策略，无需人工编码驾驶规则。


<details>
  <summary>Details</summary>
Motivation: 目前自动驾驶系统依赖人工编码的感知输出和驾驶规则，而直接从人类驾驶数据中学习可以简化架构并提高扩展性。

Method: 提出了两种模拟方法：重投影模拟和学习的世界模型，用于训练驾驶策略。

Result: 两种方法均能训练出无需人工编码规则的驾驶策略，并在闭环模拟和真实ADAS中验证了性能。

Conclusion: 端到端方法能有效训练自动驾驶策略，简化系统设计并提高扩展性。

Abstract: Most self-driving systems rely on hand-coded perception outputs and
engineered driving rules. Learning directly from human driving data with an
end-to-end method can allow for a training architecture that is simpler and
scales well with compute and data.
  In this work, we propose an end-to-end training architecture that uses real
driving data to train a driving policy in an on-policy simulator. We show two
different methods of simulation, one with reprojective simulation and one with
a learned world model. We show that both methods can be used to train a policy
that learns driving behavior without any hand-coded driving rules. We evaluate
the performance of these policies in a closed-loop simulation and when deployed
in a real-world advanced driver-assistance system.

</details>

### [275] [MIA-Mind: A Multidimensional Interactive Attention Mechanism Based on MindSpore](https://arxiv.org/abs/2504.19080)
*Zhenkai Qin,Jiaquan Liang,Qiao Fang*

Main category: cs.CV

TLDR: MIA-Mind是一种轻量级多维交互注意力机制，通过联合建模空间和通道特征提升深度学习性能，实验验证其高效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制独立建模通道和空间特征，忽视了其内在关联，限制了效果。

Method: 提出MIA-Mind，基于MindSpore框架，采用统一的跨注意力融合策略联合建模空间和通道特征。

Result: 在CIFAR-10、ISBI2012和CIC-IDS2017数据集上分别达到82.9%、78.7%和91.9%的准确率。

Conclusion: MIA-Mind具有轻量化和泛化能力，未来将扩展至大规模数据集和分布式部署。

Abstract: Attention mechanisms have significantly advanced deep learning by enhancing
feature representation through selective focus. However, existing approaches
often independently model channel importance and spatial saliency, overlooking
their inherent interdependence and limiting their effectiveness. To address
this limitation, we propose MIA-Mind, a lightweight and modular
Multidimensional Interactive Attention Mechanism, built upon the MindSpore
framework. MIA-Mind jointly models spatial and channel features through a
unified cross-attentive fusion strategy, enabling fine-grained feature
recalibration with minimal computational overhead. Extensive experiments are
conducted on three representative datasets: on CIFAR-10, MIA-Mind achieves an
accuracy of 82.9\%; on ISBI2012, it achieves an accuracy of 78.7\%; and on
CIC-IDS2017, it achieves an accuracy of 91.9\%. These results validate the
versatility, lightweight design, and generalization ability of MIA-Mind across
heterogeneous tasks. Future work will explore the extension of MIA-Mind to
large-scale datasets, the development of ada,ptive attention fusion strategies,
and distributed deployment to further enhance scalability and robustness.

</details>

### [276] [Boosting Single-domain Generalized Object Detection via Vision-Language Knowledge Interaction](https://arxiv.org/abs/2504.19086)
*Xiaoran Xu,Jiangang Yang,Wenyue Chong,Wenhui Shi,Shichu Sun,Jing Xing,Jian Liu*

Main category: cs.CV

TLDR: 论文提出了一种新的跨模态特征学习方法，通过细粒度文本和视觉特征的动态交互，提升单域广义目标检测（S-DGOD）的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有S-DGOD方法在细粒度区域和对象级别特征学习上的不足，提升模型在多样化未见目标域中的泛化能力。

Method: 提出跨模态和区域感知特征交互机制，结合跨域提议精炼与混合策略，增强区域特征的泛化性和定位能力。

Result: 在Cityscapes-C和DWD基准数据集上分别实现了+8.8%和+7.9%的mPC提升，达到新的SOTA性能。

Conclusion: 该方法通过细粒度跨模态交互和跨域提议优化，显著提升了S-DGOD任务的性能。

Abstract: Single-Domain Generalized Object Detection~(S-DGOD) aims to train an object
detector on a single source domain while generalizing well to diverse unseen
target domains, making it suitable for multimedia applications that involve
various domain shifts, such as intelligent video surveillance and VR/AR
technologies. With the success of large-scale Vision-Language Models, recent
S-DGOD approaches exploit pre-trained vision-language knowledge to guide
invariant feature learning across visual domains. However, the utilized
knowledge remains at a coarse-grained level~(e.g., the textual description of
adverse weather paired with the image) and serves as an implicit regularization
for guidance, struggling to learn accurate region- and object-level features in
varying domains. In this work, we propose a new cross-modal feature learning
method, which can capture generalized and discriminative regional features for
S-DGOD tasks. The core of our method is the mechanism of Cross-modal and
Region-aware Feature Interaction, which simultaneously learns both inter-modal
and intra-modal regional invariance through dynamic interactions between
fine-grained textual and visual features. Moreover, we design a simple but
effective strategy called Cross-domain Proposal Refining and Mixing, which
aligns the position of region proposals across multiple domains and diversifies
them, enhancing the localization ability of detectors in unseen scenarios. Our
method achieves new state-of-the-art results on S-DGOD benchmark datasets, with
improvements of +8.8\%~mPC on Cityscapes-C and +7.9\%~mPC on DWD over
baselines, demonstrating its efficacy.

</details>

### [277] [Towards Latency-Aware 3D Streaming Perception for Autonomous Driving](https://arxiv.org/abs/2504.19115)
*Jiaqi Peng,Tai Wang,Jiangmiao Pang,Yuan Shen*

Main category: cs.CV

TLDR: 提出了一种针对边缘设备运行时延迟的3D感知基准和框架LASP，通过历史特征整合和预测检测模块优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D感知算法在边缘设备上因运行时延迟问题难以部署，需针对性解决方案。

Method: LASP框架包含延迟感知历史整合和预测检测模块，动态处理延迟问题。

Result: 在Jetson AGX Orin上，在线性能接近离线评估的80%，无需加速技术。

Conclusion: LASP框架能泛化处理不同延迟水平，显著提升边缘设备3D感知性能。

Abstract: Although existing 3D perception algorithms have demonstrated significant
improvements in performance, their deployment on edge devices continues to
encounter critical challenges due to substantial runtime latency. We propose a
new benchmark tailored for online evaluation by considering runtime latency.
Based on the benchmark, we build a Latency-Aware 3D Streaming Perception (LASP)
framework that addresses the latency issue through two primary components: 1)
latency-aware history integration, which extends query propagation into a
continuous process, ensuring the integration of historical feature regardless
of varying latency; 2) latency-aware predictive detection, a module that
compensates the detection results with the predicted trajectory and the
posterior accessed latency. By incorporating the latency-aware mechanism, our
method shows generalization across various latency levels, achieving an online
performance that closely aligns with 80\% of its offline evaluation on the
Jetson AGX Orin without any acceleration techniques.

</details>

### [278] [Blind Source Separation Based on Sparsity](https://arxiv.org/abs/2504.19124)
*Zhongxuan Li*

Main category: cs.CV

TLDR: 论文回顾了盲源分离（BSS）的经典ICA方法和稀疏表示方法，提出了一种改进的块稀疏字典学习算法SAC+BK-SVD，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统ICA方法依赖源信号相互独立的假设，存在局限性。稀疏表示方法（如MCA）通过信号在字典中的稀疏分解提供了新的解决思路。

Method: 论文介绍了稀疏表示理论、块坐标松弛MCA算法及其变体（MMCA和GMCA），并提出了一种改进的块稀疏字典学习算法SAC+BK-SVD。

Result: 实验表明，SAC+BK-SVD在盲图像分离中优于传统K-SVD方法，分离质量更高。

Conclusion: 稀疏表示和块稀疏字典学习为BSS提供了有效工具，改进算法SAC+BK-SVD在性能上具有优势。

Abstract: Blind source separation (BSS) is a key technique in array processing and data
analysis, aiming to recover unknown sources from observed mixtures without
knowledge of the mixing matrix. Classical independent component analysis (ICA)
methods rely on the assumption that sources are mutually independent. To
address limitations of ICA, sparsity-based methods have been introduced, which
decompose source signals sparsely in a predefined dictionary. Morphological
Component Analysis (MCA), based on sparse representation theory, assumes that a
signal is a linear combination of components with distinct geometries, each
sparsely representable in one dictionary and not in others. This approach has
recently been applied to BSS with promising results.
  This report reviews key approaches derived from classical ICA and explores
sparsity-based methods for BSS. It introduces the theory of sparse
representation and decomposition, followed by a block coordinate relaxation MCA
algorithm, whose variants are used in Multichannel MCA (MMCA) and Generalized
MCA (GMCA). A local dictionary learning method using K-SVD is then presented.
Finally, we propose an improved algorithm, SAC+BK-SVD, which enhances K-SVD by
learning a block-sparsifying dictionary that clusters and updates similar atoms
in blocks.
  The implementation includes experiments on image segmentation and blind image
source separation using the discussed techniques. We also compare the proposed
block-sparse dictionary learning algorithm with K-SVD. Simulation results
demonstrate that our method yields improved blind image separation quality.

</details>

### [279] [DeepSPG: Exploring Deep Semantic Prior Guidance for Low-light Image Enhancement with Multimodal Learning](https://arxiv.org/abs/2504.19127)
*Jialang Lu,Huayu Zhao,Huiyu Zhai,Xingxing Yang,Shini Han*

Main category: cs.CV

TLDR: 论文提出了一种基于Retinex图像分解的深度语义先验引导框架（DeepSPG），通过预训练的语义分割模型和多模态学习，探索低光图像增强（LLIE）中的语义信息。


<details>
  <summary>Details</summary>
Motivation: 现有LLIE方法忽略了不同区域的语义信息，尤其是在极暗区域信息严重丢失的情况下。

Method: 结合图像级和文本级语义先验，通过预训练模型和多尺度语义感知结构，引导增强过程。

Result: 在五个基准数据集上表现优于现有方法。

Conclusion: DeepSPG通过引入语义先验和多模态学习，显著提升了低光图像增强的效果。

Abstract: There has long been a belief that high-level semantics learning can benefit
various downstream computer vision tasks. However, in the low-light image
enhancement (LLIE) community, existing methods learn a brutal mapping between
low-light and normal-light domains without considering the semantic information
of different regions, especially in those extremely dark regions that suffer
from severe information loss. To address this issue, we propose a new deep
semantic prior-guided framework (DeepSPG) based on Retinex image decomposition
for LLIE to explore informative semantic knowledge via a pre-trained semantic
segmentation model and multimodal learning. Notably, we incorporate both
image-level semantic prior and text-level semantic prior and thus formulate a
multimodal learning framework with combinatorial deep semantic prior guidance
for LLIE. Specifically, we incorporate semantic knowledge to guide the
enhancement process via three designs: an image-level semantic prior guidance
by leveraging hierarchical semantic features from a pre-trained semantic
segmentation model; a text-level semantic prior guidance by integrating natural
language semantic constraints via a pre-trained vision-language model; a
multi-scale semantic-aware structure that facilitates effective semantic
feature incorporation. Eventually, our proposed DeepSPG demonstrates superior
performance compared to state-of-the-art methods across five benchmark
datasets. The implementation details and code are publicly available at
https://github.com/Wenyuzhy/DeepSPG.

</details>

### [280] [PAD: Phase-Amplitude Decoupling Fusion for Multi-Modal Land Cover Classification](https://arxiv.org/abs/2504.19136)
*Huiling Zheng,Xian Zhong,Bin Liu,Yi Xiao,Bihan Wen,Xiaofeng Li*

Main category: cs.CV

TLDR: 提出了一种名为PAD的频率感知框架，通过解耦相位和振幅来解决SAR和RGB图像融合中的模态异质性问题。


<details>
  <summary>Details</summary>
Motivation: SAR和RGB图像融合因模态异质性和光谱互补性利用不足而具有挑战性。现有方法未能分离共享结构和模态特定特征，导致信息丢失。

Method: PAD框架在傅里叶域分离相位（共享）和振幅（特定）成分，包括相位谱校正（PSC）和振幅谱融合（ASF）。

Result: 在WHU-OPT-SAR和DDHR-SK数据集上实现了最先进的性能。

Conclusion: PAD为遥感中的物理感知多模态融合建立了新范式。

Abstract: The fusion of Synthetic Aperture Radar (SAR) and RGB imagery for land cover
classification remains challenging due to modality heterogeneity and the
underutilization of spectral complementarity. Existing methods often fail to
decouple shared structural features from modality-specific radiometric
attributes, leading to feature conflicts and information loss. To address this
issue, we propose Phase-Amplitude Decoupling (PAD), a frequency-aware framework
that separates phase (modality-shared) and amplitude (modality-specific)
components in the Fourier domain. Specifically, PAD consists of two key
components: 1) Phase Spectrum Correction (PSC), which aligns cross-modal phase
features through convolution-guided scaling to enhance geometric consistency,
and 2) Amplitude Spectrum Fusion (ASF), which dynamically integrates
high-frequency details and low-frequency structures using frequency-adaptive
multilayer perceptrons. This approach leverages SAR's sensitivity to
morphological features and RGB's spectral richness. Extensive experiments on
WHU-OPT-SAR and DDHR-SK datasets demonstrate state-of-the-art performance. Our
work establishes a new paradigm for physics-aware multi-modal fusion in remote
sensing. The code will be available at https://github.com/RanFeng2/PAD.

</details>

### [281] [RadioFormer: A Multiple-Granularity Radio Map Estimation Transformer with 1\textpertenthousand Spatial Sampling](https://arxiv.org/abs/2504.19161)
*Zheng Fang,Kangjun Liu,Ke Chen,Qingyu Liu,Jianguo Zhang,Lingyang Song,Yaowei Wang*

Main category: cs.CV

TLDR: 论文提出RadioFormer，一种多粒度Transformer模型，用于解决无线电地图估计中空间稀疏观测的挑战，通过双流自注意力和跨流交叉注意力模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度视觉模型（如U-Net）在无线电地图估计中需要足够空间观测（0.01%-1%像素），但实际场景中观测可能极端稀疏，亟需新方法。

Method: 提出RadioFormer，结合双流自注意力（DSA）模块学习像素级信号功率和块级建筑几何相关性，并通过跨流交叉注意力（CCA）模块整合多尺度表示。

Result: 在RadioMapSeer数据集上，RadioFormer性能优于现有方法，计算成本最低，并展现出优异的泛化能力和零样本性能。

Conclusion: RadioFormer为无线电地图估计提供了一种更实用的解决方案，适用于观测节点极少的场景。

Abstract: The task of radio map estimation aims to generate a dense representation of
electromagnetic spectrum quantities, such as the received signal strength at
each grid point within a geographic region, based on measurements from a subset
of spatially distributed nodes (represented as pixels). Recently, deep vision
models such as the U-Net have been adapted to radio map estimation, whose
effectiveness can be guaranteed with sufficient spatial observations (typically
0.01% to 1% of pixels) in each map, to model local dependency of observed
signal power. However, such a setting of sufficient measurements can be less
practical in real-world scenarios, where extreme sparsity in spatial sampling
can be widely encountered. To address this challenge, we propose RadioFormer, a
novel multiple-granularity transformer designed to handle the constraints posed
by spatial sparse observations. Our RadioFormer, through a dual-stream
self-attention (DSA) module, can respectively discover the correlation of
pixel-wise observed signal power and also learn patch-wise buildings'
geometries in a style of multiple granularities, which are integrated into
multi-scale representations of radio maps by a cross stream cross-attention
(CCA) module. Extensive experiments on the public RadioMapSeer dataset
demonstrate that RadioFormer outperforms state-of-the-art methods in radio map
estimation while maintaining the lowest computational cost. Furthermore, the
proposed approach exhibits exceptional generalization capabilities and robust
zero-shot performance, underscoring its potential to advance radio map
estimation in a more practical setting with very limited observation nodes.

</details>

### [282] [IM-Portrait: Learning 3D-aware Video Diffusion for PhotorealisticTalking Heads from Monocular Videos](https://arxiv.org/abs/2504.19165)
*Yuan Li,Ziqian Bai,Feitong Tan,Zhaopeng Cui,Sean Fanello,Yinda Zhang*

Main category: cs.CV

TLDR: 提出了一种基于扩散模型的3D感知方法，直接从单张身份图像和显式控制信号生成逼真的说话头部视频。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要额外阶段或联合优化来重建3D表示，而本文方法通过单次去噪过程直接生成最终输出，避免了后处理步骤。

Method: 生成多平面图像（MPIs）以确保几何一致性，并通过随机重建目标或参考相机空间的训练机制学习3D信息。

Result: 实验表明，该方法在无需显式3D重建或多视角训练数据的情况下，实现了高质量的虚拟形象和新视角渲染。

Conclusion: 该方法在效率和效果上均具有竞争力，适用于VR等沉浸式体验。

Abstract: We propose a novel 3D-aware diffusion-based method for generating
photorealistic talking head videos directly from a single identity image and
explicit control signals (e.g., expressions). Our method generates Multiplane
Images (MPIs) that ensure geometric consistency, making them ideal for
immersive viewing experiences like binocular videos for VR headsets. Unlike
existing methods that often require a separate stage or joint optimization to
reconstruct a 3D representation (such as NeRF or 3D Gaussians), our approach
directly generates the final output through a single denoising process,
eliminating the need for post-processing steps to render novel views
efficiently. To effectively learn from monocular videos, we introduce a
training mechanism that reconstructs the output MPI randomly in either the
target or the reference camera space. This approach enables the model to
simultaneously learn sharp image details and underlying 3D information.
Extensive experiments demonstrate the effectiveness of our method, which
achieves competitive avatar quality and novel-view rendering capabilities, even
without explicit 3D reconstruction or high-quality multi-view training data.

</details>

### [283] [Segmenting Objectiveness and Task-awareness Unknown Region for Autonomous Driving](https://arxiv.org/abs/2504.19183)
*Mi Zheng,Guanglei Yang,Zitong Huang,Zhenhua Guo,Kevin Han,Wangmeng Zuo*

Main category: cs.CV

TLDR: 论文提出了一种名为SOTA的新框架，用于自动驾驶场景中的异常检测，通过语义融合块和场景理解引导的提示上下文适配器提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前道路场景分割方法在开放集数据上表现不足，且现有异常检测方法未充分考虑目标属性和环境约束。

Method: 提出SOTA框架，结合语义融合块（SFB）和场景理解引导的提示上下文适配器（SG-PCA）。

Result: 在多个基准数据集上验证，SOTA显著提升了异常检测性能。

Conclusion: SOTA框架在自动驾驶场景中实现了更鲁棒和准确的异常分割。

Abstract: With the emergence of transformer-based architectures and large language
models (LLMs), the accuracy of road scene perception has substantially
advanced. Nonetheless, current road scene segmentation approaches are
predominantly trained on closed-set data, resulting in insufficient detection
capabilities for out-of-distribution (OOD) objects. To overcome this
limitation, road anomaly detection methods have been proposed. However,
existing methods primarily depend on image inpainting and OOD distribution
detection techniques, facing two critical issues: (1) inadequate consideration
of the objectiveness attributes of anomalous regions, causing incomplete
segmentation when anomalous objects share similarities with known classes, and
(2) insufficient attention to environmental constraints, leading to the
detection of anomalies irrelevant to autonomous driving tasks. In this paper,
we propose a novel framework termed Segmenting Objectiveness and Task-Awareness
(SOTA) for autonomous driving scenes. Specifically, SOTA enhances the
segmentation of objectiveness through a Semantic Fusion Block (SFB) and filters
anomalies irrelevant to road navigation tasks using a Scene-understanding
Guided Prompt-Context Adaptor (SG-PCA). Extensive empirical evaluations on
multiple benchmark datasets, including Fishyscapes Lost and Found,
Segment-Me-If-You-Can, and RoadAnomaly, demonstrate that the proposed SOTA
consistently improves OOD detection performance across diverse detectors,
achieving robust and accurate segmentation outcomes.

</details>

### [284] [LRFusionPR: A Polar BEV-Based LiDAR-Radar Fusion Network for Place Recognition](https://arxiv.org/abs/2504.19186)
*Zhangshuo Qi,Luqi Cheng,Zijie Zhou,Guangming Xiong*

Main category: cs.CV

TLDR: 论文提出LRFusionPR，通过融合LiDAR和雷达数据提升自动驾驶中地点识别的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在GPS缺失环境中，地点识别对自动驾驶至关重要。LiDAR和雷达各有优势，但融合两者仍具挑战性。

Method: 提出双分支网络，在统一极坐标BEV表示中融合不同模态数据，利用交叉注意力实现跨模态特征交互。

Result: 在多数据集上验证，LRFusionPR实现了高精度地点识别，并在不同天气条件下保持鲁棒性。

Conclusion: LRFusionPR为LiDAR与雷达融合提供了有效解决方案，代码已开源。

Abstract: In autonomous driving, place recognition is critical for global localization
in GPS-denied environments. LiDAR and radar-based place recognition methods
have garnered increasing attention, as LiDAR provides precise ranging, whereas
radar excels in adverse weather resilience. However, effectively leveraging
LiDAR-radar fusion for place recognition remains challenging. The noisy and
sparse nature of radar data limits its potential to further improve recognition
accuracy. In addition, heterogeneous radar configurations complicate the
development of unified cross-modality fusion frameworks. In this paper, we
propose LRFusionPR, which improves recognition accuracy and robustness by
fusing LiDAR with either single-chip or scanning radar. Technically, a
dual-branch network is proposed to fuse different modalities within the unified
polar coordinate bird's eye view (BEV) representation. In the fusion branch,
cross-attention is utilized to perform cross-modality feature interactions. The
knowledge from the fusion branch is simultaneously transferred to the
distillation branch, which takes radar as its only input to further improve the
robustness. Ultimately, the descriptors from both branches are concatenated,
producing the multimodal global descriptor for place retrieval. Extensive
evaluations on multiple datasets demonstrate that our LRFusionPR achieves
accurate place recognition, while maintaining robustness under varying weather
conditions. Our open-source code will be released at
https://github.com/QiZS-BIT/LRFusionPR.

</details>

### [285] [Adaptive Dual-domain Learning for Underwater Image Enhancement](https://arxiv.org/abs/2504.19198)
*Lingtao Peng,Liheng Bian*

Main category: cs.CV

TLDR: 论文提出了一种基于空间-光谱双域自适应学习的水下图像增强方法（SS-UIE），通过MCSS和SWSA模块建模不同区域的退化水平，并结合频率损失函数提升高频细节重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法未同时考虑空间区域和光谱波段的不一致退化水平，且未区分高频细节区域的重建难度。

Method: 提出SS-UIE网络，结合MCSS（空间多尺度循环选择性扫描）和SWSA（光谱自注意力）模块，并引入频率损失函数（FWL）。

Result: 实验表明SS-UIE在性能和计算成本上优于现有方法。

Conclusion: SS-UIE通过双域自适应学习和频率损失函数，有效提升了水下图像增强的效果和效率。

Abstract: Recently, learning-based Underwater Image Enhancement (UIE) methods have
demonstrated promising performance. However, existing learning-based methods
still face two challenges. 1) They rarely consider the inconsistent degradation
levels in different spatial regions and spectral bands simultaneously. 2) They
treat all regions equally, ignoring that the regions with high-frequency
details are more difficult to reconstruct. To address these challenges, we
propose a novel UIE method based on spatial-spectral dual-domain adaptive
learning, termed SS-UIE. Specifically, we first introduce a spatial-wise
Multi-scale Cycle Selective Scan (MCSS) module and a Spectral-Wise
Self-Attention (SWSA) module, both with linear complexity, and combine them in
parallel to form a basic Spatial-Spectral block (SS-block). Benefiting from the
global receptive field of MCSS and SWSA, SS-block can effectively model the
degradation levels of different spatial regions and spectral bands, thereby
enabling degradation level-based dual-domain adaptive UIE. By stacking multiple
SS-blocks, we build our SS-UIE network. Additionally, a Frequency-Wise Loss
(FWL) is introduced to narrow the frequency-wise discrepancy and reinforce the
model's attention on the regions with high-frequency details. Extensive
experiments validate that the SS-UIE technique outperforms state-of-the-art UIE
methods while requiring cheaper computational and memory costs.

</details>

### [286] [FlexPara: Flexible Neural Surface Parameterization](https://arxiv.org/abs/2504.19210)
*Yuming Zhao,Qijian Zhang,Junhui Hou,Jiazhi Xia,Wenping Wang,Ying He*

Main category: cs.CV

TLDR: FlexPara是一种无监督神经优化框架，通过点对点映射实现全局和多图表表面参数化，无需手动切割。


<details>
  <summary>Details</summary>
Motivation: 传统参数化方法需要高质量网格三角化且拓扑受限，FlexPara旨在提供更灵活可控的处理流程。

Method: 设计几何可解释的子网络（切割、变形、展开、包裹）构建双向循环映射框架，支持自适应学习图表分配。

Result: 实验证明FlexPara具有普适性、优越性和潜力。

Conclusion: FlexPara为表面参数化提供了高效且灵活的神经范式，代码已开源。

Abstract: Surface parameterization is a fundamental geometry processing task, laying
the foundations for the visual presentation of 3D assets and numerous
downstream shape analysis scenarios. Conventional parameterization approaches
demand high-quality mesh triangulation and are restricted to certain simple
topologies unless additional surface cutting and decomposition are provided. In
practice, the optimal configurations (e.g., type of parameterization domains,
distribution of cutting seams, number of mapping charts) may vary drastically
with different surface structures and task characteristics, thus requiring more
flexible and controllable processing pipelines. To this end, this paper
introduces FlexPara, an unsupervised neural optimization framework to achieve
both global and multi-chart surface parameterizations by establishing
point-wise mappings between 3D surface points and adaptively-deformed 2D UV
coordinates. We ingeniously design and combine a series of
geometrically-interpretable sub-networks, with specific functionalities of
cutting, deforming, unwrapping, and wrapping, to construct a bi-directional
cycle mapping framework for global parameterization without the need for
manually specified cutting seams. Furthermore, we construct a multi-chart
parameterization framework with adaptively-learned chart assignment. Extensive
experiments demonstrate the universality, superiority, and inspiring potential
of our neural surface parameterization paradigm. The code will be publicly
available at https://github.com/AidenZhao/FlexPara

</details>

### [287] [CapsFake: A Multimodal Capsule Network for Detecting Instruction-Guided Deepfakes](https://arxiv.org/abs/2504.19212)
*Tuan Nguyen,Naseem Khan,Issa Khalil*

Main category: cs.CV

TLDR: CapsFake是一种新型多模态胶囊网络，用于检测基于文本提示的深度伪造图像编辑，优于现有方法20%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的快速发展，尤其是基于指令的图像编辑，威胁数字图像的完整性，现有防御系统难以检测。

Method: 提出CapsFake网络，整合视觉、文本和频域模态的低级胶囊，通过竞争路由机制动态聚合局部特征，精准识别篡改区域。

Result: 在多个数据集上表现优异，检测准确率提升20%，对抗攻击和自然扰动下的检测率分别达96%和94%。

Conclusion: CapsFake为对抗复杂图像篡改提供了强大框架。

Abstract: The rapid evolution of deepfake technology, particularly in
instruction-guided image editing, threatens the integrity of digital images by
enabling subtle, context-aware manipulations. Generated conditionally from real
images and textual prompts, these edits are often imperceptible to both humans
and existing detection systems, revealing significant limitations in current
defenses. We propose a novel multimodal capsule network, CapsFake, designed to
detect such deepfake image edits by integrating low-level capsules from visual,
textual, and frequency-domain modalities. High-level capsules, predicted
through a competitive routing mechanism, dynamically aggregate local features
to identify manipulated regions with precision. Evaluated on diverse datasets,
including MagicBrush, Unsplash Edits, Open Images Edits, and Multi-turn Edits,
CapsFake outperforms state-of-the-art methods by up to 20% in detection
accuracy. Ablation studies validate its robustness, achieving detection rates
above 94% under natural perturbations and 96% against adversarial attacks, with
excellent generalization to unseen editing scenarios. This approach establishes
a powerful framework for countering sophisticated image manipulations.

</details>

### [288] [CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis](https://arxiv.org/abs/2504.19223)
*Alexander Baumann,Leonardo Ayala,Silvia Seidlitz,Jan Sellner,Alexander Studier-Fischer,Berkin Özdemir,Lena Maier-Hein,Slobodan Ilic*

Main category: cs.CV

TLDR: 论文提出了一种名为CARL的相机无关表示学习方法，用于解决光谱成像中因相机差异导致的模型泛化性问题。


<details>
  <summary>Details</summary>
Motivation: 光谱成像在不同领域应用广泛，但相机间的通道维度和波长差异限制了AI方法的通用性。

Method: 引入波长位置编码和自注意力-交叉注意力机制，将光谱信息压缩为学习到的查询表示，并通过自监督预训练策略实现光谱-空间预训练。

Result: 在医学成像、自动驾驶和卫星成像等领域的大规模实验中，CARL表现出对光谱异质性的独特鲁棒性，优于现有方法。

Conclusion: CARL的扩展性和通用性使其成为未来光谱基础模型的骨干。

Abstract: Spectral imaging offers promising applications across diverse domains,
including medicine and urban scene understanding, and is already established as
a critical modality in remote sensing. However, variability in channel
dimensionality and captured wavelengths among spectral cameras impede the
development of AI-driven methodologies, leading to camera-specific models with
limited generalizability and inadequate cross-camera applicability. To address
this bottleneck, we introduce $\textbf{CARL}$, a model for
$\textbf{C}$amera-$\textbf{A}$gnostic $\textbf{R}$epresentation
$\textbf{L}$earning across RGB, multispectral, and hyperspectral imaging
modalities. To enable the conversion of a spectral image with any channel
dimensionality to a camera-agnostic embedding, we introduce wavelength
positional encoding and a self-attention-cross-attention mechanism to compress
spectral information into learned query representations. Spectral-spatial
pre-training is achieved with a novel spectral self-supervised JEPA-inspired
strategy tailored to CARL. Large-scale experiments across the domains of
medical imaging, autonomous driving, and satellite imaging demonstrate our
model's unique robustness to spectral heterogeneity, outperforming on datasets
with simulated and real-world cross-camera spectral variations. The scalability
and versatility of the proposed approach position our model as a backbone for
future spectral foundation models.

</details>

### [289] [Unsupervised 2D-3D lifting of non-rigid objects using local constraints](https://arxiv.org/abs/2504.19227)
*Shalini Maiti,Lourdes Agapito,Benjamin Graham*

Main category: cs.CV

TLDR: 通过无监督损失训练的高容量模型，结合局部低秩约束，显著提高了非刚性物体3D形状预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决非刚性物体从2D关键点预测3D形状时的病态问题（如遮挡和视角变化），传统低秩约束模型训练困难且重建质量受限。

Method: 采用高容量模型和无监督损失，结合局部低秩约束，优化形状预测。

Result: 在S-Up3D数据集上，重建误差降低了70%以上。

Conclusion: 高容量模型结合局部约束能显著提升非刚性物体3D重建的准确性。

Abstract: For non-rigid objects, predicting the 3D shape from 2D keypoint observations
is ill-posed due to occlusions, and the need to disentangle changes in
viewpoint and changes in shape. This challenge has often been addressed by
embedding low-rank constraints into specialized models. These models can be
hard to train, as they depend on finding a canonical way of aligning
observations, before they can learn detailed geometry. These constraints have
limited the reconstruction quality. We show that generic, high capacity models,
trained with an unsupervised loss, allow for more accurate predicted shapes. In
particular, applying low-rank constraints to localized subsets of the full
shape allows the high capacity to be suitably constrained. We reduce the
state-of-the-art reconstruction error on the S-Up3D dataset by over 70%.

</details>

### [290] [Semantic-Aligned Learning with Collaborative Refinement for Unsupervised VI-ReID](https://arxiv.org/abs/2504.19244)
*De Cheng,Lingfeng He,Nannan Wang,Dingwen Zhang,Xinbo Gao*

Main category: cs.CV

TLDR: 提出了一种名为SALCR的框架，通过语义对齐学习和协作优化解决无监督跨模态行人重识别中的特征表示和伪标签分布问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了跨模态特征表示和伪标签分布的细粒度模式差异，导致全局特征学习不足。

Method: SALCR框架包括双向伪标签统一模块（DAGI）、细粒度语义对齐学习模块（FGSAL）和全局-局部协作优化模块（GPCR）。

Result: 实验表明，SALCR在性能上优于现有方法。

Conclusion: SALCR通过细粒度语义对齐和协作优化，有效提升了跨模态行人重识别的性能。

Abstract: Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to
match pedestrian images of the same individual across different modalities
without human annotations for model learning. Previous methods unify
pseudo-labels of cross-modality images through label association algorithms and
then design contrastive learning framework for global feature learning.
However, these methods overlook the cross-modality variations in feature
representation and pseudo-label distributions brought by fine-grained patterns.
This insight results in insufficient modality-shared learning when only global
features are optimized. To address this issue, we propose a Semantic-Aligned
Learning with Collaborative Refinement (SALCR) framework, which builds up
optimization objective for specific fine-grained patterns emphasized by each
modality, thereby achieving complementary alignment between the label
distributions of different modalities. Specifically, we first introduce a Dual
Association with Global Learning (DAGI) module to unify the pseudo-labels of
cross-modality instances in a bi-directional manner. Afterward, a Fine-Grained
Semantic-Aligned Learning (FGSAL) module is carried out to explore part-level
semantic-aligned patterns emphasized by each modality from cross-modality
instances. Optimization objective is then formulated based on the
semantic-aligned features and their corresponding label space. To alleviate the
side-effects arising from noisy pseudo-labels, we propose a Global-Part
Collaborative Refinement (GPCR) module to mine reliable positive sample sets
for the global and part features dynamically and optimize the inter-instance
relationships. Extensive experiments demonstrate the effectiveness of the
proposed method, which achieves superior performances to state-of-the-art
methods. Our code is available at
\href{https://github.com/FranklinLingfeng/code-for-SALCR}.

</details>

### [291] [ODExAI: A Comprehensive Object Detection Explainable AI Evaluation](https://arxiv.org/abs/2504.19249)
*Loc Phuc Truong Nguyen,Hung Truong Thanh Nguyen,Hung Cao*

Main category: cs.CV

TLDR: 论文提出了ODExAI框架，用于评估目标检测中的XAI方法，比较了不同方法在定位准确性、模型忠实性和计算复杂度上的表现。


<details>
  <summary>Details</summary>
Motivation: 目标检测中的XAI方法缺乏系统性评估标准，阻碍了方法比较和选择。

Method: 引入ODExAI框架，评估XAI方法在三个核心维度上的表现，并在YOLOX和Faster R-CNN等模型上进行了基准测试。

Result: 区域方法（如D-CLOSE）定位准确（PG=88.49%）且忠实性高（OA=0.863），但计算开销大（71.42s）；CAM方法（如G-CAME）定位更优（PG=96.13%）且速度快（0.54s），但忠实性较低（OA=0.549）。

Conclusion: 现有XAI方法存在关键权衡，需根据任务需求选择，ODExAI框架为评估提供了标准化工具。

Abstract: Explainable Artificial Intelligence (XAI) techniques for interpreting object
detection models remain in an early stage, with no established standards for
systematic evaluation. This absence of consensus hinders both the comparative
analysis of methods and the informed selection of suitable approaches. To
address this gap, we introduce the Object Detection Explainable AI Evaluation
(ODExAI), a comprehensive framework designed to assess XAI methods in object
detection based on three core dimensions: localization accuracy, faithfulness
to model behavior, and computational complexity. We benchmark a set of XAI
methods across two widely used object detectors (YOLOX and Faster R-CNN) and
standard datasets (MS-COCO and PASCAL VOC). Empirical results demonstrate that
region-based methods (e.g., D-CLOSE) achieve strong localization (PG = 88.49%)
and high model faithfulness (OA = 0.863), though with substantial computational
overhead (Time = 71.42s). On the other hand, CAM-based methods (e.g., G-CAME)
achieve superior localization (PG = 96.13%) and significantly lower runtime
(Time = 0.54s), but at the expense of reduced faithfulness (OA = 0.549). These
findings demonstrate critical trade-offs among existing XAI approaches and
reinforce the need for task-specific evaluation when deploying them in object
detection pipelines. Our implementation and evaluation benchmarks are publicly
available at: https://github.com/Analytics-Everywhere-Lab/odexai.

</details>

### [292] [DeeCLIP: A Robust and Generalizable Transformer-Based Framework for Detecting AI-Generated Images](https://arxiv.org/abs/2504.19876)
*Mamadou Keita,Wassim Hamidouche,Hessen Bougueffa Eutamene,Abdelmalik Taleb-Ahmed,Abdenour Hadid*

Main category: cs.CV

TLDR: DeeCLIP是一种基于CLIP-ViT和融合学习的新框架，用于检测AI生成图像，通过融合模块和三元组损失提升鲁棒性和区分能力，采用LoRA进行轻量级调优，在少量训练数据下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有检测方法对生成模型泛化能力差和对微小扰动敏感的问题。

Method: 结合DeeFuser融合模块整合高低级特征，应用三元组损失优化嵌入空间，采用LoRA进行参数高效调优。

Result: 在4类ProGAN数据上训练，DeeCLIP在19个测试子集上平均准确率达89.00%，优于现有方法。

Conclusion: DeeCLIP在鲁棒性和泛化能力上表现突出，代码已开源。

Abstract: This paper introduces DeeCLIP, a novel framework for detecting AI-generated
images using CLIP-ViT and fusion learning. Despite significant advancements in
generative models capable of creating highly photorealistic images, existing
detection methods often struggle to generalize across different models and are
highly sensitive to minor perturbations. To address these challenges, DeeCLIP
incorporates DeeFuser, a fusion module that combines high-level and low-level
features, improving robustness against degradations such as compression and
blurring. Additionally, we apply triplet loss to refine the embedding space,
enhancing the model's ability to distinguish between real and synthetic
content. To further enable lightweight adaptation while preserving pre-trained
knowledge, we adopt parameter-efficient fine-tuning using low-rank adaptation
(LoRA) within the CLIP-ViT backbone. This approach supports effective zero-shot
learning without sacrificing generalization. Trained exclusively on 4-class
ProGAN data, DeeCLIP achieves an average accuracy of 89.00% on 19 test subsets
composed of generative adversarial network (GAN) and diffusion models. Despite
having fewer trainable parameters, DeeCLIP outperforms existing methods,
demonstrating superior robustness against various generative models and
real-world distortions. The code is publicly available at
https://github.com/Mamadou-Keita/DeeCLIP for research purposes.

</details>

### [293] [LM-MCVT: A Lightweight Multi-modal Multi-view Convolutional-Vision Transformer Approach for 3D Object Recognition](https://arxiv.org/abs/2504.19256)
*Songsong Xiong,Hamidreza Kasaei*

Main category: cs.CV

TLDR: 提出了一种轻量级多模态多视角卷积-视觉Transformer网络（LM-MCVT），用于提升机器人应用中的3D物体识别能力，通过全局熵基嵌入融合（GEEF）方法整合多视角数据，显著提高了识别精度。


<details>
  <summary>Details</summary>
Motivation: 在餐厅、家庭和仓库等以人为中心的环境中，机器人常因物体形状多样性和环境复杂性而难以准确识别3D物体。

Method: 采用LM-MCVT架构，结合预和中层卷积编码器及局部与全局Transformer，利用GEEF方法高效融合多视角数据。

Result: 在ModelNet40数据集上达到95.6%的识别准确率（四视角配置），并在OmniObject3D数据集上通过5折交叉验证验证了方法的鲁棒性。

Conclusion: LM-MCVT在合成和真实世界3D数据中均表现出优越性能，为机器人3D物体识别提供了高效解决方案。

Abstract: In human-centered environments such as restaurants, homes, and warehouses,
robots often face challenges in accurately recognizing 3D objects. These
challenges stem from the complexity and variability of these environments,
including diverse object shapes. In this paper, we propose a novel Lightweight
Multi-modal Multi-view Convolutional-Vision Transformer network (LM-MCVT) to
enhance 3D object recognition in robotic applications. Our approach leverages
the Globally Entropy-based Embeddings Fusion (GEEF) method to integrate
multi-views efficiently. The LM-MCVT architecture incorporates pre- and
mid-level convolutional encoders and local and global transformers to enhance
feature extraction and recognition accuracy. We evaluate our method on the
synthetic ModelNet40 dataset and achieve a recognition accuracy of 95.6% using
a four-view setup, surpassing existing state-of-the-art methods. To further
validate its effectiveness, we conduct 5-fold cross-validation on the
real-world OmniObject3D dataset using the same configuration. Results
consistently show superior performance, demonstrating the method's robustness
in 3D object recognition across synthetic and real-world 3D data.

</details>

### [294] [OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via Adaptive Radial Fusion](https://arxiv.org/abs/2504.19258)
*Shuhao Kang,Martin Y. Liao,Yan Xia,Olaf Wysocki,Boris Jutzi,Daniel Cremers*

Main category: cs.CV

TLDR: OPAL是一种新型LiDAR地点识别网络，利用OpenStreetMap作为轻量级先验，通过跨模态可见性掩码和自适应径向融合模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖密集3D地图或航拍图像，存储开销大且缺乏实时适应性。

Method: 提出跨模态可见性掩码和自适应径向融合模块，结合LiDAR扫描与OSM数据。

Result: 在KITTI和KITTI-360数据集上，召回率提升15.98%，推理速度快12倍。

Conclusion: OPAL在LiDAR地点识别中表现出色，兼具高效性和实时性。

Abstract: LiDAR place recognition is a critical capability for autonomous navigation
and cross-modal localization in large-scale outdoor environments. Existing
approaches predominantly depend on pre-built 3D dense maps or aerial imagery,
which impose significant storage overhead and lack real-time adaptability. In
this paper, we propose OPAL, a novel network for LiDAR place recognition that
leverages OpenStreetMap as a lightweight and up-to-date prior. Our key
innovation lies in bridging the domain disparity between sparse LiDAR scans and
structured OSM data through two carefully designed components: a cross-modal
visibility mask that identifies maximal observable regions from both modalities
to guide feature learning, and an adaptive radial fusion module that
dynamically consolidates multiscale radial features into discriminative global
descriptors. Extensive experiments on the augmented KITTI and KITTI-360
datasets demonstrate OPAL's superiority, achieving 15.98% higher recall at @1m
threshold for top-1 retrieved matches while operating at 12x faster inference
speeds compared to state-of-the-art approaches. Code and datasets are publicly
available at: https://github.com/WHU-USI3DV/OPAL .

</details>

### [295] [Rendering Anywhere You See: Renderability Field-guided Gaussian Splatting](https://arxiv.org/abs/2504.19261)
*Xiaofeng Jin,Yan Fang,Matteo Frosi,Jianfei Ge,Jiangjian Xiao,Matteo Matteucci*

Main category: cs.CV

TLDR: 提出了一种基于渲染能力场引导的高斯泼溅方法（RF-GS），用于解决场景视图合成中非均匀观测导致的渲染质量问题。


<details>
  <summary>Details</summary>
Motivation: 场景视图合成在虚拟现实、增强现实和机器人等领域应用广泛，但非均匀观测导致渲染质量不稳定，亟需解决方案。

Method: 通过渲染能力场量化输入不均匀性，引导伪视图采样以提升视觉一致性；训练图像修复模型优化宽基线伪视图质量；采用混合数据优化策略融合伪视图角度和源视图纹理信息。

Result: 在模拟和真实数据上的实验表明，该方法在渲染稳定性上优于现有方法。

Conclusion: RF-GS方法有效解决了场景视图合成中的渲染质量问题，为相关应用提供了更稳定的解决方案。

Abstract: Scene view synthesis, which generates novel views from limited perspectives,
is increasingly vital for applications like virtual reality, augmented reality,
and robotics. Unlike object-based tasks, such as generating 360{\deg} views of
a car, scene view synthesis handles entire environments where non-uniform
observations pose unique challenges for stable rendering quality. To address
this issue, we propose a novel approach: renderability field-guided gaussian
splatting (RF-GS). This method quantifies input inhomogeneity through a
renderability field, guiding pseudo-view sampling to enhanced visual
consistency. To ensure the quality of wide-baseline pseudo-views, we train an
image restoration model to map point projections to visible-light styles.
Additionally, our validated hybrid data optimization strategy effectively fuses
information of pseudo-view angles and source view textures. Comparative
experiments on simulated and real-world data show that our method outperforms
existing approaches in rendering stability.

</details>

### [296] [OpenFusion++: An Open-vocabulary Real-time Scene Understanding System](https://arxiv.org/abs/2504.19266)
*Xiaofeng Jin,Matteo Frosi,Matteo Matteucci*

Main category: cs.CV

TLDR: OpenFusion++ 是一种基于 TSDF 的实时 3D 语义几何重建系统，通过融合基础模型的置信度图、动态更新语义标签和双路径编码框架，显著提升了语义准确性和查询响应能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在实例分割不精确、语义更新静态以及复杂查询处理能力有限等问题。

Method: 融合置信度图优化点云，动态更新全局语义标签，采用双路径编码框架整合对象属性和环境上下文。

Result: 在多个数据集上显著优于基线方法，语义准确性和查询响应能力均有提升。

Conclusion: OpenFusion++ 为实时开放词汇场景理解提供了高效解决方案。

Abstract: Real-time open-vocabulary scene understanding is essential for efficient 3D
perception in applications such as vision-language navigation, embodied
intelligence, and augmented reality. However, existing methods suffer from
imprecise instance segmentation, static semantic updates, and limited handling
of complex queries. To address these issues, we present OpenFusion++, a
TSDF-based real-time 3D semantic-geometric reconstruction system. Our approach
refines 3D point clouds by fusing confidence maps from foundational models,
dynamically updates global semantic labels via an adaptive cache based on
instance area, and employs a dual-path encoding framework that integrates
object attributes with environmental context for precise query responses.
Experiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate
that OpenFusion++ significantly outperforms the baseline in both semantic
accuracy and query responsiveness.

</details>

### [297] [VI3NR: Variance Informed Initialization for Implicit Neural Representations](https://arxiv.org/abs/2504.19270)
*Chamin Hewa Koneputugodage,Yizhak Ben-Shabat,Sameera Ramasinghe,Stephen Gould*

Main category: cs.CV

TLDR: 本文提出了一种适用于任何激活函数的神经网络初始化方法，改进了INRs的收敛性和准确性。


<details>
  <summary>Details</summary>
Motivation: INRs的成功依赖于网络初始化，但现有初始化方法不适用于许多激活函数，尤其是INRs中使用的那些。

Method: 推导出一种层间方差稳定的初始化方法，适用于任何激活函数，并推广了现有方法。

Result: 该方法在多种信号模态中提高了INRs的性能，特别是在高斯INRs中，理论与实验表现一致。

Conclusion: 提出的初始化方法在图像、音频和3D表面重建任务中表现出色。

Abstract: Implicit Neural Representations (INRs) are a versatile and powerful tool for
encoding various forms of data, including images, videos, sound, and 3D shapes.
A critical factor in the success of INRs is the initialization of the network,
which can significantly impact the convergence and accuracy of the learned
model. Unfortunately, commonly used neural network initializations are not
widely applicable for many activation functions, especially those used by INRs.
In this paper, we improve upon previous initialization methods by deriving an
initialization that has stable variance across layers, and applies to any
activation function. We show that this generalizes many previous initialization
methods, and has even better stability for well studied activations. We also
show that our initialization leads to improved results with INR activation
functions in multiple signal modalities. Our approach is particularly effective
for Gaussian INRs, where we demonstrate that the theory of our initialization
matches with task performance in multiple experiments, allowing us to achieve
improvements in image, audio, and 3D surface reconstruction.

</details>

### [298] [Leveraging Multi-Modal Saliency and Fusion for Gaze Target Detection](https://arxiv.org/abs/2504.19271)
*Athul M. Mathew,Arshad Ali Khan,Thariq Khalid,Faroq AL-Tam,Riad Souissi*

Main category: cs.CV

TLDR: 提出了一种融合多模态信息的凝视目标检测方法，通过3D表示和深度增强的显著性模块提升性能，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 凝视目标检测（GTD）需要理解人物头部、身体、眼睛与环境的复杂关系，现有方法难以全面捕捉这些信息。

Method: 将2D图像投影为3D表示，提取深度增强的显著性模块、面部和深度模态，融合多模态信息以预测凝视目标。

Result: 在VideoAttentionTarget、GazeFollow和GOO-Real数据集上表现优于现有方法。

Conclusion: 该方法为GTD提供了一种有前景的新思路。

Abstract: Gaze target detection (GTD) is the task of predicting where a person in an
image is looking. This is a challenging task, as it requires the ability to
understand the relationship between the person's head, body, and eyes, as well
as the surrounding environment. In this paper, we propose a novel method for
GTD that fuses multiple pieces of information extracted from an image. First,
we project the 2D image into a 3D representation using monocular depth
estimation. We then extract a depth-infused saliency module map, which
highlights the most salient (\textit{attention-grabbing}) regions in image for
the subject in consideration. We also extract face and depth modalities from
the image, and finally fuse all the extracted modalities to identify the gaze
target. We quantitatively evaluated our method, including the ablation analysis
on three publicly available datasets, namely VideoAttentionTarget, GazeFollow
and GOO-Real, and showed that it outperforms other state-of-the-art methods.
This suggests that our method is a promising new approach for GTD.

</details>

### [299] [Optimal Hyperspectral Undersampling Strategy for Satellite Imaging](https://arxiv.org/abs/2504.19279)
*Vita V. Vlasova,Vladimir G. Kuzmin,Maria S. Varetsa,Natalia A. Ibragimova,Oleg Y. Rogov,Elena V. Lyapuntsova*

Main category: cs.CV

TLDR: 提出了一种基于小波变换的迭代梯度采样（IWGS）方法，用于高光谱图像分类中的波段选择，显著提升了分类精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临高维度、光谱冗余和标记数据有限的问题，需要一种高效的波段选择方法。

Method: IWGS通过分析小波变换域中的梯度，迭代选择信息量最大的光谱波段，同时排除冗余或噪声波段。

Result: 在Houston 2013和Indian Pines数据集上，IWGS的分类精度高达97.8%，优于现有方法。

Conclusion: IWGS适用于资源受限的环境，具有高效性和泛化能力。

Abstract: Hyperspectral image (HSI) classification presents significant challenges due
to the high dimensionality, spectral redundancy, and limited labeled data
typically available in real-world applications. To address these issues and
optimize classification performance, we propose a novel band selection strategy
known as Iterative Wavelet-based Gradient Sampling (IWGS). This method
incrementally selects the most informative spectral bands by analyzing
gradients within the wavelet-transformed domain, enabling efficient and
targeted dimensionality reduction. Unlike traditional selection methods, IWGS
leverages the multi-resolution properties of wavelets to better capture subtle
spectral variations relevant for classification. The iterative nature of the
approach ensures that redundant or noisy bands are systematically excluded
while maximizing the retention of discriminative features. We conduct
comprehensive experiments on two widely-used benchmark HSI datasets: Houston
2013 and Indian Pines. Results demonstrate that IWGS consistently outperforms
state-of-the-art band selection and classification techniques in terms of both
accuracy and computational efficiency. These improvements make our method
especially suitable for deployment in edge devices or other
resource-constrained environments, where memory and processing power are
limited. In particular, IWGS achieved an overall accuracy up to 97.8% on Indian
Pines for selected classes, confirming its effectiveness and generalizability
across different HSI scenarios.

</details>

### [300] [Marine Snow Removal Using Internally Generated Pseudo Ground Truth](https://arxiv.org/abs/2504.19289)
*Alexandra Malyugina,Guoxi Huang,Eduardo Ruiz,Benjamin Leslie,Nantheera Anantrasirichai*

Main category: cs.CV

TLDR: 论文提出了一种新方法，通过生成配对数据集来增强水下视频质量，解决了海洋雪花噪声问题。


<details>
  <summary>Details</summary>
Motivation: 水下视频因光线吸收、散射和噪声（如海洋雪花）导致质量下降，现有方法因缺乏配对训练数据而效果不佳。

Method: 提出了一种生成配对数据集的新框架，从原始水下视频中生成含雪花和无雪花的配对图像，用于监督训练。

Result: 生成的配对数据集有效提升了水下图像恢复的效果，尤其是在缺乏真实数据的情况下。

Conclusion: 该方法为水下视频增强提供了一种有效解决方案，尤其在缺乏真实配对数据时表现突出。

Abstract: Underwater videos often suffer from degraded quality due to light absorption,
scattering, and various noise sources. Among these, marine snow, which is
suspended organic particles appearing as bright spots or noise, significantly
impacts machine vision tasks, particularly those involving feature matching.
Existing methods for removing marine snow are ineffective due to the lack of
paired training data. To address this challenge, this paper proposes a novel
enhancement framework that introduces a new approach for generating paired
datasets from raw underwater videos. The resulting dataset consists of paired
images of generated snowy and snow, free underwater videos, enabling supervised
training for video enhancement. We describe the dataset creation process,
highlight its key characteristics, and demonstrate its effectiveness in
enhancing underwater image restoration in the absence of ground truth.

</details>

### [301] [FusionNet: Multi-model Linear Fusion Framework for Low-light Image Enhancement](https://arxiv.org/abs/2504.19295)
*Kangbiao Shi,Yixu Feng,Tao Hu,Yu Cao,Peng Wu,Yijin Liang,Yanning Zhang,Qingsen Yan*

Main category: cs.CV

TLDR: FusionNet是一种新型多模型线性融合框架，通过并行操作捕捉不同颜色空间的全局和局部特征，解决了现有融合策略的参数爆炸、优化不稳定和特征错位问题，并在CVPR2025 NTIRE低光增强挑战赛中取得第一名。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法在融合不同架构和颜色空间时面临参数爆炸、优化不稳定和特征错位等挑战，限制了性能提升。

Method: 提出FusionNet，基于Hilbert空间理论保证的线性融合策略，并行捕捉全局和局部特征，减少训练成本和网络崩溃风险。

Result: 在合成和真实数据集上显著优于现有方法，定量和定性结果均表现优异。

Conclusion: FusionNet通过高效的线性融合策略，在低光图像增强任务中实现了高性能和鲁棒性。

Abstract: The advent of Deep Neural Networks (DNNs) has driven remarkable progress in
low-light image enhancement (LLIE), with diverse architectures (e.g., CNNs and
Transformers) and color spaces (e.g., sRGB, HSV, HVI) yielding impressive
results. Recent efforts have sought to leverage the complementary strengths of
these paradigms, offering promising solutions to enhance performance across
varying degradation scenarios. However, existing fusion strategies are hindered
by challenges such as parameter explosion, optimization instability, and
feature misalignment, limiting further improvements. To overcome these issues,
we introduce FusionNet, a novel multi-model linear fusion framework that
operates in parallel to effectively capture global and local features across
diverse color spaces. By incorporating a linear fusion strategy underpinned by
Hilbert space theoretical guarantees, FusionNet mitigates network collapse and
reduces excessive training costs. Our method achieved 1st place in the CVPR2025
NTIRE Low Light Enhancement Challenge. Extensive experiments conducted on
synthetic and real-world benchmark datasets demonstrate that the proposed
method significantly outperforms state-of-the-art methods in terms of both
quantitative and qualitative results, delivering robust enhancement under
diverse low-light conditions.

</details>

### [302] [Myocardial Region-guided Feature Aggregation Net for Automatic Coronary artery Segmentation and Stenosis Assessment using Coronary Computed Tomography Angiography](https://arxiv.org/abs/2504.19300)
*Ni Yao,Xiangyu Liu,Danyang Sun,Chuang Han,Yanting Li,Jiaofen Nan,Chengyang Li,Fubao Zhu,Weihua Zhou,Chen Zhao*

Main category: cs.CV

TLDR: 提出了一种新型U型双编码器架构MGFA-Net，用于冠状动脉分割和狭窄检测，结合心肌区域引导和特征融合，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉疾病（CAD）是全球主要死因之一，现有方法在低对比度、形态变异性和小血管分割方面表现不佳，需改进。

Method: 采用心肌区域引导模块、残差特征提取编码模块和多尺度特征融合模块，结合蒙特卡洛dropout量化不确定性。

Result: MGFA-Net在Dice得分（85.04%）、准确率（84.24%）和狭窄检测真阳性率（提升5.46%）上优于3D U-Net。

Conclusion: MGFA-Net结合解剖先验知识，为CAD提供自动化和临床可解释的评估，推动精准医疗。

Abstract: Coronary artery disease (CAD) remains a leading cause of mortality worldwide,
requiring accurate segmentation and stenosis detection using Coronary Computed
Tomography angiography (CCTA). Existing methods struggle with challenges such
as low contrast, morphological variability and small vessel segmentation. To
address these limitations, we propose the Myocardial Region-guided Feature
Aggregation Net, a novel U-shaped dual-encoder architecture that integrates
anatomical prior knowledge to enhance robustness in coronary artery
segmentation. Our framework incorporates three key innovations: (1) a
Myocardial Region-guided Module that directs attention to coronary regions via
myocardial contour expansion and multi-scale feature fusion, (2) a Residual
Feature Extraction Encoding Module that combines parallel spatial channel
attention with residual blocks to enhance local-global feature discrimination,
and (3) a Multi-scale Feature Fusion Module for adaptive aggregation of
hierarchical vascular features. Additionally, Monte Carlo dropout f quantifies
prediction uncertainty, supporting clinical interpretability. For stenosis
detection, a morphology-based centerline extraction algorithm separates the
vascular tree into anatomical branches, enabling cross-sectional area
quantification and stenosis grading. The superiority of MGFA-Net was
demonstrated by achieving an Dice score of 85.04%, an accuracy of 84.24%, an
HD95 of 6.1294 mm, and an improvement of 5.46% in true positive rate for
stenosis detection compared to3D U-Net. The integrated segmentation-to-stenosis
pipeline provides automated, clinically interpretable CAD assessment, bridging
deep learning with anatomical prior knowledge for precision medicine. Our code
is publicly available at http://github.com/chenzhao2023/MGFA_CCTA

</details>

### [303] [Platonic Grounding for Efficient Multimodal Language Models](https://arxiv.org/abs/2504.19327)
*Moulik Choraria,Xinbo Wu,Akhil Bhimaraju,Nitesh Sekhar,Yue Wu,Xu Zhang,Prateek Singhal,Lav R. Varshney*

Main category: cs.CV

TLDR: 论文提出了一种改进多模态框架的简单方法，通过利用预训练模型的隐式对齐特性，在保持性能的同时显著降低了训练和推理的计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型的数据和参数规模扩大，性能提升逐渐减少，而训练成本却很高。因此，需要更高效的微调和推理方法，尤其是在多模态学习中。

Method: 基于预训练模型深层隐式对齐的发现，提出了一种改进多模态框架的简单方法。

Result: 该方法在保持基线性能的同时，显著降低了训练和推理的计算成本，甚至在某些情况下提升了性能。

Conclusion: 该研究不仅为多模态学习提供了高效解决方案，还为高效整合预训练模型提供了启示。

Abstract: The hyperscaling of data and parameter count in Transformer-based models is
yielding diminishing performance improvement, especially when weighed against
training costs. Such plateauing indicates the importance of methods for more
efficient finetuning and inference, while retaining similar performance. This
is especially relevant for multimodal learning paradigms, where inference costs
of processing multimodal tokens can determine the model's practical viability.
At the same time, research on representations and mechanistic interpretability
has improved our understanding of the inner workings of Transformer-based
models; one such line of work reveals an implicit alignment in the deeper
layers of pretrained models, across modalities. Taking inspiration from this,
we motivate and propose a simple modification to existing multimodal frameworks
that rely on aligning pretrained models. We demonstrate that our approach
maintains and, in some cases, even improves performance of baseline methods
while achieving significant gains in both training and inference-time compute.
Our work also has implications for combining pretrained models into larger
systems efficiently.

</details>

### [304] [Enhancing seeding efficiency using a computer vision system to monitor furrow quality in real-time](https://arxiv.org/abs/2504.19334)
*Sidharth Rai,Aryan Dalal,Riley Slichter,Ajay Sharda*

Main category: cs.CV

TLDR: 开发了一种基于计算机视觉的方法，用于评估行清洁器性能，以解决精准农业中种子播种的障碍。


<details>
  <summary>Details</summary>
Motivation: 精准农业中种子播种面临残留物堆积、低土壤温度和残留物堵塞等问题，缺乏定量评估行清洁器性能的方法。

Method: 通过视频采集系统捕捉行清洁器操作后的沟槽状况，开发分割模型分析土壤、秸秆和机械等关键元素，并量化行清洁器性能。

Result: 结果表明该方法可有效评估行清洁器性能，有助于提升精准农业的播种效率。

Conclusion: 该方法为行清洁器选择和播种效率提升提供了新途径。

Abstract: Effective seed sowing in precision agriculture is hindered by challenges such
as residue accumulation, low soil temperatures, and hair pinning (crop residue
pushed in the trench by furrow opener), which obstruct optimal trench
formation. Row cleaners are employed to mitigate these issues, but there is a
lack of quantitative methods to assess trench cleanliness. In this study, a
novel computer vision-based method was developed to evaluate row cleaner
performance. Multiple air seeders were equipped with a video acquisition system
to capture trench conditions after row cleaner operation, enabling an effective
comparison of the performance of each row cleaner. The captured data were used
to develop a segmentation model that analyzed key elements such as soil, straw,
and machinery. Using the results from the segmentation model, an objective
method was developed to quantify row cleaner performance. The results
demonstrated the potential of this method to improve row cleaner selection and
enhance seeding efficiency in precision agriculture.

</details>

### [305] [Improving Small Drone Detection Through Multi-Scale Processing and Data Augmentation](https://arxiv.org/abs/2504.19347)
*Rayson Laroca,Marcelo dos Santos,David Menotti*

Main category: cs.CV

TLDR: 提出了一种基于YOLOv11的多尺度无人机检测方法，结合数据增强和后处理技术，在复杂环境中有效区分无人机与鸟类。


<details>
  <summary>Details</summary>
Motivation: 现代监控中，无人机与鸟类难以区分，需高效检测方法。

Method: 采用多尺度输入处理、复制粘贴数据增强和帧间一致性后处理技术。

Result: 在2025年IJCNN的无人机检测挑战赛中排名前三。

Conclusion: 该方法在复杂环境中能有效检测无人机。

Abstract: Detecting small drones, often indistinguishable from birds, is crucial for
modern surveillance. This work introduces a drone detection methodology built
upon the medium-sized YOLOv11 object detection model. To enhance its
performance on small targets, we implemented a multi-scale approach in which
the input image is processed both as a whole and in segmented parts, with
subsequent prediction aggregation. We also utilized a copy-paste data
augmentation technique to enrich the training dataset with diverse drone and
bird examples. Finally, we implemented a post-processing technique that
leverages frame-to-frame consistency to mitigate missed detections. The
proposed approach attained a top-3 ranking in the 8th WOSDETC Drone-vsBird
Detection Grand Challenge, held at the 2025 International Joint Conference on
Neural Networks (IJCNN), showcasing its capability to detect drones in complex
environments effectively.

</details>

### [306] [MERA: Multimodal and Multiscale Self-Explanatory Model with Considerably Reduced Annotation for Lung Nodule Diagnosis](https://arxiv.org/abs/2504.19357)
*Jiahao Lu,Chong Yin,Silvia Ingala,Kenny Erleben,Michael Bachmann Nielsen,Sune Darkner*

Main category: cs.CV

TLDR: MERA是一种多模态、多尺度的自解释模型，用于肺结节诊断，显著减少标注需求，结合无监督和弱监督学习策略，提供多层次解释。


<details>
  <summary>Details</summary>
Motivation: 肺癌早期检测对患者预后至关重要，但现有XAI系统在有限标注数据下难以提供清晰全面的解释。

Method: MERA结合自监督学习和Vision Transformer进行无监督特征提取，利用半监督主动学习在潜在空间中进行分层预测。

Result: 在LIDC数据集上，MERA仅需1%标注样本即可达到或超越全标注方法的诊断准确性，并提供多层次解释。

Conclusion: MERA的设计增强了AI诊断的可信度和透明度，为医学领域部署诊断AI降低了门槛。

Abstract: Lung cancer, a leading cause of cancer-related deaths globally, emphasises
the importance of early detection for better patient outcomes. Pulmonary
nodules, often early indicators of lung cancer, necessitate accurate, timely
diagnosis. Despite Explainable Artificial Intelligence (XAI) advances, many
existing systems struggle providing clear, comprehensive explanations,
especially with limited labelled data. This study introduces MERA, a Multimodal
and Multiscale self-Explanatory model designed for lung nodule diagnosis with
considerably Reduced Annotation requirements. MERA integrates unsupervised and
weakly supervised learning strategies (self-supervised learning techniques and
Vision Transformer architecture for unsupervised feature extraction) and a
hierarchical prediction mechanism leveraging sparse annotations via
semi-supervised active learning in the learned latent space. MERA explains its
decisions on multiple levels: model-level global explanations via semantic
latent space clustering, instance-level case-based explanations showing similar
instances, local visual explanations via attention maps, and concept
explanations using critical nodule attributes. Evaluations on the public LIDC
dataset show MERA's superior diagnostic accuracy and self-explainability. With
only 1% annotated samples, MERA achieves diagnostic accuracy comparable to or
exceeding state-of-the-art methods requiring full annotation. The model's
inherent design delivers comprehensive, robust, multilevel explanations aligned
closely with clinical practice, enhancing trustworthiness and transparency.
Demonstrated viability of unsupervised and weakly supervised learning lowers
the barrier to deploying diagnostic AI in broader medical domains. Our complete
code is open-source available: https://github.com/diku-dk/credanno.

</details>

### [307] [Mitigating Bias in Facial Recognition Systems: Centroid Fairness Loss Optimization](https://arxiv.org/abs/2504.19370)
*Jean-Rémy Conti,Stéphan Clémençon*

Main category: cs.CV

TLDR: 本文提出一种后处理方法，通过优化基于质心的回归损失，提升预训练人脸识别模型的公平性，同时保持全局准确性。


<details>
  <summary>Details</summary>
Motivation: 社会对公平AI系统的需求促使研究社区开发满足新公平标准的预测模型，尤其是解决人脸识别系统在不同人口群体中的误差差异问题。

Method: 采用后处理方法，优化基于质心的回归损失，以改进预训练模型的公平性。

Result: 数值实验表明，该方法显著提升了公平性，同时保持了全局准确性。

Conclusion: 该方法为设计公平的人脸识别系统提供了有效的解决方案。

Abstract: The urging societal demand for fair AI systems has put pressure on the
research community to develop predictive models that are not only globally
accurate but also meet new fairness criteria, reflecting the lack of disparate
mistreatment with respect to sensitive attributes ($\textit{e.g.}$ gender,
ethnicity, age). In particular, the variability of the errors made by certain
Facial Recognition (FR) systems across specific segments of the population
compromises the deployment of the latter, and was judged unacceptable by
regulatory authorities. Designing fair FR systems is a very challenging
problem, mainly due to the complex and functional nature of the performance
measure used in this domain ($\textit{i.e.}$ ROC curves) and because of the
huge heterogeneity of the face image datasets usually available for training.
In this paper, we propose a novel post-processing approach to improve the
fairness of pre-trained FR models by optimizing a regression loss which acts on
centroid-based scores. Beyond the computational advantages of the method, we
present numerical experiments providing strong empirical evidence of the gain
in fairness and of the ability to preserve global accuracy.

</details>

### [308] [HumMorph: Generalized Dynamic Human Neural Fields from Few Views](https://arxiv.org/abs/2504.19390)
*Jakub Zadrożny,Hakan Bilen*

Main category: cs.CV

TLDR: HumMorph是一种新颖的自由视角动态人体渲染方法，具有显式姿势控制，仅需少量观察视图即可生成任意姿势的高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在多视角同步相机依赖和噪声参数估计下的局限性，提供更实用的单视角或少量视角渲染方案。

Method: 结合粗粒度规范T姿势表示与细粒度像素对齐特征，通过前馈模型快速推断。

Result: 在单视角下与现有技术竞争，双视角下视觉质量显著提升，对噪声参数更具鲁棒性。

Conclusion: HumMorph在实用场景中优于现有技术，尤其在噪声参数和少量输入视图下表现突出。

Abstract: We introduce HumMorph, a novel generalized approach to free-viewpoint
rendering of dynamic human bodies with explicit pose control. HumMorph renders
a human actor in any specified pose given a few observed views (starting from
just one) in arbitrary poses. Our method enables fast inference as it relies
only on feed-forward passes through the model. We first construct a coarse
representation of the actor in the canonical T-pose, which combines visual
features from individual partial observations and fills missing information
using learned prior knowledge. The coarse representation is complemented by
fine-grained pixel-aligned features extracted directly from the observed views,
which provide high-resolution appearance information. We show that HumMorph is
competitive with the state-of-the-art when only a single input view is
available, however, we achieve results with significantly better visual quality
given just 2 monocular observations. Moreover, previous generalized methods
assume access to accurate body shape and pose parameters obtained using
synchronized multi-camera setups. In contrast, we consider a more practical
scenario where these body parameters are noisily estimated directly from the
observed views. Our experimental results demonstrate that our architecture is
more robust to errors in the noisy parameters and clearly outperforms the state
of the art in this setting.

</details>

### [309] [Dynamic Arthroscopic Navigation System for Anterior Cruciate Ligament Reconstruction Based on Multi-level Memory Architecture](https://arxiv.org/abs/2504.19398)
*Shuo Wang,Weili Shi,Shuai Yang,Jiahao Cui,Qinwei Guo*

Main category: cs.CV

TLDR: 本文提出了一种基于多级记忆架构的动态关节镜导航系统，用于前交叉韧带重建手术，显著提升了跟踪精度和实时性能。


<details>
  <summary>Details</summary>
Motivation: 传统静态匹配方法在复杂手术场景中（如视角变化、器械遮挡和组织变形）表现不佳，需要一种更稳定、实时的导航系统。

Method: 系统将Atkinson-Shiffrin记忆模型的三级架构（感官记忆、工作记忆和长期记忆）融入动态视频序列跟踪，无需额外硬件即可实时运行。

Result: 动态系统在1000帧序列中误差为5.3±1.5像素，比静态系统提升约45%；实时性能达25.3 FPS，延迟仅39.5 ms。

Conclusion: 该系统克服了传统静态方法的局限，为提升前交叉韧带重建手术精度提供了新技术支持。

Abstract: This paper presents a dynamic arthroscopic navigation system based on
multi-level memory architecture for anterior cruciate ligament (ACL)
reconstruction surgery. The system extends our previously proposed markerless
navigation method from static image matching to dynamic video sequence
tracking. By integrating the Atkinson-Shiffrin memory model's three-level
architecture (sensory memory, working memory, and long-term memory), our system
maintains continuous tracking of the femoral condyle throughout the surgical
procedure, providing stable navigation support even in complex situations
involving viewpoint changes, instrument occlusion, and tissue deformation.
Unlike existing methods, our system operates in real-time on standard
arthroscopic equipment without requiring additional tracking hardware,
achieving 25.3 FPS with a latency of only 39.5 ms, representing a 3.5-fold
improvement over our previous static system. For extended sequences (1000
frames), the dynamic system maintained an error of 5.3 plus-minus 1.5 pixels,
compared to the static system's 12.6 plus-minus 3.7 pixels - an improvement of
approximately 45 percent. For medium-length sequences (500 frames) and short
sequences (100 frames), the system achieved approximately 35 percent and 19
percent accuracy improvements, respectively. Experimental results demonstrate
the system overcomes limitations of traditional static matching methods,
providing new technical support for improving surgical precision in ACL
reconstruction.

</details>

### [310] [Boosting 3D Liver Shape Datasets with Diffusion Models and Implicit Neural Representations](https://arxiv.org/abs/2504.19402)
*Khoa Tuan Nguyen,Francesca Tozzi,Wouter Willaert,Joris Vankerschaver,Nikdokht Rashidian,Wesley De Neve*

Main category: cs.CV

TLDR: 论文提出了一种结合扩散模型和隐式神经表示（INRs）的方法，用于增强和扩展现有的3D肝脏形状数据集，以解决数据稀缺和数据集质量问题。


<details>
  <summary>Details</summary>
Motivation: 现有的3D医学形状数据集存在组织混乱和包含伪影的问题，限制了鲁棒模型的开发和训练，尤其是3D重建任务。

Method: 使用扩散模型生成多样且真实的3D肝脏形状，结合隐式神经表示（INRs）来扩展数据集。

Result: 实验结果表明，该方法提高了数据集的多样性，为3D肝脏重建和生成提供了可扩展的解决方案。

Conclusion: 扩散模型还可应用于其他3D医学影像的下游任务。

Abstract: While the availability of open 3D medical shape datasets is increasing,
offering substantial benefits to the research community, we have found that
many of these datasets are, unfortunately, disorganized and contain artifacts.
These issues limit the development and training of robust models, particularly
for accurate 3D reconstruction tasks. In this paper, we examine the current
state of available 3D liver shape datasets and propose a solution using
diffusion models combined with implicit neural representations (INRs) to
augment and expand existing datasets. Our approach utilizes the generative
capabilities of diffusion models to create realistic, diverse 3D liver shapes,
capturing a wide range of anatomical variations and addressing the problem of
data scarcity. Experimental results indicate that our method enhances dataset
diversity, providing a scalable solution to improve the accuracy and
reliability of 3D liver reconstruction and generation in medical applications.
Finally, we suggest that diffusion models can also be applied to other
downstream tasks in 3D medical imaging.

</details>

### [311] [GMAR: Gradient-Driven Multi-Head Attention Rollout for Vision Transformer Interpretability](https://arxiv.org/abs/2504.19414)
*Sehyeong Jo,Gangjae Jang,Haesol Park*

Main category: cs.CV

TLDR: GMAR是一种基于梯度的多注意力头重要性评估方法，旨在提升Vision Transformer的可解释性。


<details>
  <summary>Details</summary>
Motivation: Vision Transformer的多头注意力机制缺乏可解释性，现有方法难以区分不同头的重要性。

Method: 提出GMAR方法，通过梯度评分量化每个注意力头的重要性，并归一化生成加权注意力分数。

Result: 实验表明GMAR优于传统注意力展开技术，提升了模型的可解释性。

Conclusion: GMAR为Vision Transformer提供了一个增强可解释性的实用框架。

Abstract: The Vision Transformer (ViT) has made significant advancements in computer
vision, utilizing self-attention mechanisms to achieve state-of-the-art
performance across various tasks, including image classification, object
detection, and segmentation. Its architectural flexibility and capabilities
have made it a preferred choice among researchers and practitioners. However,
the intricate multi-head attention mechanism of ViT presents significant
challenges to interpretability, as the underlying prediction process remains
opaque. A critical limitation arises from an observation commonly noted in
transformer architectures: "Not all attention heads are equally meaningful."
Overlooking the relative importance of specific heads highlights the
limitations of existing interpretability methods. To address these challenges,
we introduce Gradient-Driven Multi-Head Attention Rollout (GMAR), a novel
method that quantifies the importance of each attention head using
gradient-based scores. These scores are normalized to derive a weighted
aggregate attention score, effectively capturing the relative contributions of
individual heads. GMAR clarifies the role of each head in the prediction
process, enabling more precise interpretability at the head level. Experimental
results demonstrate that GMAR consistently outperforms traditional attention
rollout techniques. This work provides a practical contribution to
transformer-based architectures, establishing a robust framework for enhancing
the interpretability of Vision Transformer models.

</details>

### [312] [A Real-Time Event-Based Normal Flow Estimator](https://arxiv.org/abs/2504.19417)
*Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TLDR: 本文提出了一种实时、异步、基于事件的正常流估计器，优化了原方法的实现，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 原方法在处理事件切片时计算复杂度高，难以实现实时性能。

Method: 通过将事件坐标视为整数，将表示步骤重新定义为池化操作，降低了计算复杂度。

Result: 新方法在RTX 3070上每秒处理400万正常流，RTX A5000上每秒600万，仅占用1GB CUDA内存。

Conclusion: 优化后的方法实现了实时性能，并开源了CUDA实现和Python接口。

Abstract: This paper presents a real-time, asynchronous, event-based normal flow
estimator. It follows the same algorithm as Learning Normal Flow Directly From
Event Neighborhoods, but with a more optimized implementation. The original
method treats event slices as 3D point clouds, encodes each event's local
geometry into a fixed-length vector, and uses a multi-layer perceptron to
predict normal flow. It constructs representations by multiplying an adjacency
matrix with a feature matrix, resulting in quadratic time complexity with
respect to the number of events. In contrast, we leverage the fact that event
coordinates are integers and reformulate the representation step as a pooling
operation. This achieves the same effect as the adjacency matrix but with much
lower computational cost. As a result, our method supports real-time normal
flow prediction on event cameras. Our estimator uses 1 GB of CUDA memory and
runs at 4 million normal flows per second on an RTX 3070, or 6 million per
second on an RTX A5000. We release the CUDA implementation along with a Python
interface at https://github.com/dhyuan99/VecKM_flow_cpp.

</details>

### [313] [EarthMapper: Visual Autoregressive Models for Controllable Bidirectional Satellite-Map Translation](https://arxiv.org/abs/2504.19432)
*Zhe Dong,Yuzhe Sun,Tianzhu Liu,Wangmeng Zuo,Yanfeng Gu*

Main category: cs.CV

TLDR: EarthMapper是一个用于卫星图像与地图双向翻译的自回归框架，解决了模态间像素对齐缺失和高质量视觉合成的挑战，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 卫星图像和地图的双向翻译在规划和灾害响应中有重要应用，但缺乏精确对齐和高抽象合成能力是主要挑战。

Method: EarthMapper采用地理坐标嵌入和多尺度特征对齐，结合语义注入和关键点自适应引导机制，实现可控翻译。

Result: 在CNSatMap和纽约数据集上，EarthMapper在视觉真实性、语义一致性和结构保真度上显著优于现有方法。

Conclusion: EarthMapper不仅性能优越，还支持零样本任务，展现了其多功能性。

Abstract: Satellite imagery and maps, as two fundamental data modalities in remote
sensing, offer direct observations of the Earth's surface and
human-interpretable geographic abstractions, respectively. The task of
bidirectional translation between satellite images and maps (BSMT) holds
significant potential for applications in urban planning and disaster response.
However, this task presents two major challenges: first, the absence of precise
pixel-wise alignment between the two modalities substantially complicates the
translation process; second, it requires achieving both high-level abstraction
of geographic features and high-quality visual synthesis, which further
elevates the technical complexity. To address these limitations, we introduce
EarthMapper, a novel autoregressive framework for controllable bidirectional
satellite-map translation. EarthMapper employs geographic coordinate embeddings
to anchor generation, ensuring region-specific adaptability, and leverages
multi-scale feature alignment within a geo-conditioned joint scale
autoregression (GJSA) process to unify bidirectional translation in a single
training cycle. A semantic infusion (SI) mechanism is introduced to enhance
feature-level consistency, while a key point adaptive guidance (KPAG) mechanism
is proposed to dynamically balance diversity and precision during inference. We
further contribute CNSatMap, a large-scale dataset comprising 302,132 precisely
aligned satellite-map pairs across 38 Chinese cities, enabling robust
benchmarking. Extensive experiments on CNSatMap and the New York dataset
demonstrate EarthMapper's superior performance, achieving significant
improvements in visual realism, semantic consistency, and structural fidelity
over state-of-the-art methods. Additionally, EarthMapper excels in zero-shot
tasks like in-painting, out-painting and coordinate-conditional generation,
underscoring its versatility.

</details>

### [314] [CLIP-KOA: Enhancing Knee Osteoarthritis Diagnosis with Multi-Modal Learning and Symmetry-Aware Loss Functions](https://arxiv.org/abs/2504.19443)
*Yejin Jeong,Donghun Lee*

Main category: cs.CV

TLDR: 论文提出了一种基于CLIP的框架（CLIP-KOA），通过结合图像和文本信息及对称性和一致性损失，提升了膝骨关节炎（KOA）严重程度预测的一致性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 膝骨关节炎（KOA）的早期诊断至关重要，但现有的KL分级系统存在观察者间变异性和主观性问题，需要更一致的自动化诊断方法。

Method: 提出CLIP-KOA框架，结合图像和文本信息，引入对称性损失和一致性损失，确保原始图像与翻转图像预测的一致性。

Result: CLIP-KOA在KOA严重程度预测任务中达到71.86%的准确率，比标准CLIP模型提升2.36%。

Conclusion: 该研究为数据驱动的医学预测提供了新方向，不仅提升了细粒度诊断的可靠性，还探索了多模态方法在医学图像分析中的应用。

Abstract: Knee osteoarthritis (KOA) is a universal chronic musculoskeletal disorders
worldwide, making early diagnosis crucial. Currently, the Kellgren and Lawrence
(KL) grading system is widely used to assess KOA severity. However, its high
inter-observer variability and subjectivity hinder diagnostic consistency. To
address these limitations, automated diagnostic techniques using deep learning
have been actively explored in recent years. In this study, we propose a
CLIP-based framework (CLIP-KOA) to enhance the consistency and reliability of
KOA grade prediction. To achieve this, we introduce a learning approach that
integrates image and text information and incorporate Symmetry Loss and
Consistency Loss to ensure prediction consistency between the original and
flipped images. CLIP-KOA achieves state-of-the-art accuracy of 71.86\% on KOA
severity prediction task, and ablation studies show that CLIP-KOA has 2.36\%
improvement in accuracy over the standard CLIP model due to our contribution.
This study shows a novel direction for data-driven medical prediction not only
to improve reliability of fine-grained diagnosis and but also to explore
multimodal methods for medical image analysis. Our code is available at
https://github.com/anonymized-link.

</details>

### [315] [Masked Language Prompting for Generative Data Augmentation in Few-shot Fashion Style Recognition](https://arxiv.org/abs/2504.19455)
*Yuki Hirakawa,Ryotaro Shimizu*

Main category: cs.CV

TLDR: 提出了一种名为Masked Language Prompting (MLP)的新提示策略，通过掩码参考标题中的部分词语并利用大型语言模型生成多样且语义连贯的补全，以增强时尚风格识别的数据多样性。


<details>
  <summary>Details</summary>
Motivation: 时尚风格识别数据集的构建因风格概念的主观性和模糊性而具有挑战性，现有基于类别名称或参考标题的方法难以平衡视觉多样性和风格一致性。

Method: 采用MLP策略，掩码参考标题中的部分词语，利用大型语言模型生成多样且语义连贯的补全，从而在不微调的情况下实现风格一致且多样化的图像生成。

Result: 在FashionStyle14数据集上的实验表明，基于MLP的数据增强方法在有限监督下显著优于基于类别名称和参考标题的基线方法。

Conclusion: MLP策略有效解决了时尚风格识别中数据多样性与风格一致性的平衡问题，为有限监督下的任务提供了实用解决方案。

Abstract: Constructing dataset for fashion style recognition is challenging due to the
inherent subjectivity and ambiguity of style concepts. Recent advances in
text-to-image models have facilitated generative data augmentation by
synthesizing images from labeled data, yet existing methods based solely on
class names or reference captions often fail to balance visual diversity and
style consistency. In this work, we propose \textbf{Masked Language Prompting
(MLP)}, a novel prompting strategy that masks selected words in a reference
caption and leverages large language models to generate diverse yet
semantically coherent completions. This approach preserves the structural
semantics of the original caption while introducing attribute-level variations
aligned with the intended style, enabling style-consistent and diverse image
generation without fine-tuning. Experimental results on the FashionStyle14
dataset demonstrate that our MLP-based augmentation consistently outperforms
class-name and caption-based baselines, validating its effectiveness for
fashion style recognition under limited supervision.

</details>

### [316] [Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video](https://arxiv.org/abs/2504.19475)
*Sonia Joseph,Praneet Suresh,Lorenz Hufe,Edward Stevinson,Robert Graham,Yash Vadi,Danilo Bzdok,Sebastian Lapuschkin,Lee Sharkey,Blake Aaron Richards*

Main category: cs.CV

TLDR: Prisma是一个开源框架，旨在加速视觉机制可解释性研究，提供统一工具包、预训练权重和分析工具，并揭示了一些新发现。


<details>
  <summary>Details</summary>
Motivation: 视觉机制可解释性研究因缺乏可访问的框架和预训练权重而进展缓慢，Prisma旨在解决这一问题。

Method: Prisma提供75+视觉和视频Transformer的统一工具包，支持稀疏自编码器（SAE）训练，提供80+预训练SAE权重，以及激活缓存、电路分析和可视化工具。

Result: 研究发现视觉SAE的稀疏模式显著低于语言SAE，且某些情况下SAE重建可以降低模型损失。

Conclusion: Prisma为理解视觉模型内部机制提供了新方向，同时降低了该领域的入门门槛。

Abstract: Robust tooling and publicly available pre-trained models have helped drive
recent advances in mechanistic interpretability for language models. However,
similar progress in vision mechanistic interpretability has been hindered by
the lack of accessible frameworks and pre-trained weights. We present Prisma
(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an
open-source framework designed to accelerate vision mechanistic
interpretability research, providing a unified toolkit for accessing 75+ vision
and video transformers; support for sparse autoencoder (SAE), transcoder, and
crosscoder training; a suite of 80+ pre-trained SAE weights; activation
caching, circuit analysis tools, and visualization tools; and educational
resources. Our analysis reveals surprising findings, including that effective
vision SAEs can exhibit substantially lower sparsity patterns than language
SAEs, and that in some instances, SAE reconstructions can decrease model loss.
Prisma enables new research directions for understanding vision model internals
while lowering barriers to entry in this emerging field.

</details>

### [317] [CasaGPT: Cuboid Arrangement and Scene Assembly for Interior Design](https://arxiv.org/abs/2504.19478)
*Weitao Feng,Hang Zhou,Jing Liao,Li Cheng,Wenbo Zhou*

Main category: cs.CV

TLDR: 提出了一种基于立方体分解的室内场景合成方法CasaGPT，通过自回归模型排列立方体，减少物体交叉，提升场景质量。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用边界框确定3D物体位置和尺寸，效果有限；立方体作为替代方案更简单高效。

Method: 使用自回归模型顺序排列立方体，通过拒绝采样过滤碰撞场景，优化数据集3DFRONT-NC。

Result: 在3D-FRONT和3DFRONT-NC数据集上优于现有方法，提升场景真实感。

Conclusion: CasaGPT为3D场景合成提供了高效且高质量的新方向。

Abstract: We present a novel approach for indoor scene synthesis, which learns to
arrange decomposed cuboid primitives to represent 3D objects within a scene.
Unlike conventional methods that use bounding boxes to determine the placement
and scale of 3D objects, our approach leverages cuboids as a straightforward
yet highly effective alternative for modeling objects. This allows for compact
scene generation while minimizing object intersections. Our approach, coined
CasaGPT for Cuboid Arrangement and Scene Assembly, employs an autoregressive
model to sequentially arrange cuboids, producing physically plausible scenes.
By applying rejection sampling during the fine-tuning stage to filter out
scenes with object collisions, our model further reduces intersections and
enhances scene quality. Additionally, we introduce a refined dataset,
3DFRONT-NC, which eliminates significant noise presented in the original
dataset, 3D-FRONT. Extensive experiments on the 3D-FRONT dataset as well as our
dataset demonstrate that our approach consistently outperforms the
state-of-the-art methods, enhancing the realism of generated scenes, and
providing a promising direction for 3D scene synthesis.

</details>

### [318] [Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2504.19500)
*Yan Wang,Baoxiong Jia,Ziyu Zhu,Siyuan Huang*

Main category: cs.CV

TLDR: MPEC是一种新颖的掩蔽点-实体对比学习方法，用于开放词汇3D语义分割，通过3D实体-语言对齐和多视角点云一致性提升特征表示，在ScanNet上取得领先结果。


<details>
  <summary>Details</summary>
Motivation: 开放词汇3D场景理解对物理智能至关重要，使智能体能在真实环境中动态交互。

Method: MPEC结合3D实体-语言对齐和多视角点云一致性，学习实体特定特征表示。

Result: 在ScanNet上实现最佳开放词汇3D语义分割性能，并在零样本场景理解中表现优异。

Conclusion: MPEC展示了学习到的3D特征在多种任务中的潜力，推动了3D场景理解的性能提升。

Abstract: Open-vocabulary 3D scene understanding is pivotal for enhancing physical
intelligence, as it enables embodied agents to interpret and interact
dynamically within real-world environments. This paper introduces MPEC, a novel
Masked Point-Entity Contrastive learning method for open-vocabulary 3D semantic
segmentation that leverages both 3D entity-language alignment and point-entity
consistency across different point cloud views to foster entity-specific
feature representations. Our method improves semantic discrimination and
enhances the differentiation of unique instances, achieving state-of-the-art
results on ScanNet for open-vocabulary 3D semantic segmentation and
demonstrating superior zero-shot scene understanding capabilities. Extensive
fine-tuning experiments on 8 datasets, spanning from low-level perception to
high-level reasoning tasks, showcase the potential of learned 3D features,
driving consistent performance gains across varied 3D scene understanding
tasks. Project website: https://mpec-3d.github.io/

</details>

### [319] [SynergyAmodal: Deocclude Anything with Text Control](https://arxiv.org/abs/2504.19506)
*Xinyang Li,Chengjie Yi,Jiawei Lai,Mingbao Lin,Yansong Qu,Shengchuan Zhang,Liujuan Cao*

Main category: cs.CV

TLDR: SynergyAmodal框架通过数据-人类-模型三方协作，利用野外图像数据、人类专家知识和生成先验，合成高质量的无遮挡数据集，并训练扩散模型实现零样本泛化和文本可控性。


<details>
  <summary>Details</summary>
Motivation: 解决图像去遮挡任务中高质量数据稀缺的问题，平衡多样性、合理性和保真度。

Method: 1. 自监督学习算法利用野外图像数据；2. 人类专家参与迭代筛选和标注；3. 训练扩散模型并加入文本提示。

Result: 生成了约16K对高质量无遮挡数据集，模型表现出零样本泛化和文本可控性。

Conclusion: SynergyAmodal框架有效解决了数据稀缺问题，为图像去遮挡任务提供了高质量数据和模型。

Abstract: Image deocclusion (or amodal completion) aims to recover the invisible
regions (\ie, shape and appearance) of occluded instances in images. Despite
recent advances, the scarcity of high-quality data that balances diversity,
plausibility, and fidelity remains a major obstacle. To address this challenge,
we identify three critical elements: leveraging in-the-wild image data for
diversity, incorporating human expertise for plausibility, and utilizing
generative priors for fidelity. We propose SynergyAmodal, a novel framework for
co-synthesizing in-the-wild amodal datasets with comprehensive shape and
appearance annotations, which integrates these elements through a tripartite
data-human-model collaboration. First, we design an occlusion-grounded
self-supervised learning algorithm to harness the diversity of in-the-wild
image data, fine-tuning an inpainting diffusion model into a partial completion
diffusion model. Second, we establish a co-synthesis pipeline to iteratively
filter, refine, select, and annotate the initial deocclusion results of the
partial completion diffusion model, ensuring plausibility and fidelity through
human expert guidance and prior model constraints. This pipeline generates a
high-quality paired amodal dataset with extensive category and scale diversity,
comprising approximately 16K pairs. Finally, we train a full completion
diffusion model on the synthesized dataset, incorporating text prompts as
conditioning signals. Extensive experiments demonstrate the effectiveness of
our framework in achieving zero-shot generalization and textual
controllability. Our code, dataset, and models will be made publicly available
at https://github.com/imlixinyang/SynergyAmodal.

</details>

### [320] [FSBench: A Figure Skating Benchmark for Advancing Artistic Sports Understanding](https://arxiv.org/abs/2504.19514)
*Rong Gao,Xin Liu,Zhuozhao Hu,Bohao Xing,Baiqiang Xia,Zitong Yu,Heikki Kälviäinen*

Main category: cs.CV

TLDR: 介绍了FSAnno数据集，用于提升对花样滑冰等艺术体育的理解，包含训练、测试和基准数据集FSBench，测试显示现有模型对艺术体育的理解有限。


<details>
  <summary>Details</summary>
Motivation: 现有花样滑冰数据集仅关注单一任务，缺乏对技术和艺术评价的综合标注，且体育研究多集中于球类运动，艺术体育研究不足。

Method: 提出FSAnno数据集，包含开放训练测试数据和基准数据集FSBench（含文本和多模态QA对），支持从技术分析到表演评论的多任务。

Result: FSBench初步测试显示现有模型对艺术体育的理解存在显著局限。

Conclusion: 希望FSBench成为评估和提升模型对花样滑冰理解的关键工具。

Abstract: Figure skating, known as the "Art on Ice," is among the most artistic sports,
challenging to understand due to its blend of technical elements (like jumps
and spins) and overall artistic expression. Existing figure skating datasets
mainly focus on single tasks, such as action recognition or scoring, lacking
comprehensive annotations for both technical and artistic evaluation. Current
sports research is largely centered on ball games, with limited relevance to
artistic sports like figure skating. To address this, we introduce FSAnno, a
large-scale dataset advancing artistic sports understanding through figure
skating. FSAnno includes an open-access training and test dataset, alongside a
benchmark dataset, FSBench, for fair model evaluation. FSBench consists of
FSBench-Text, with multiple-choice questions and explanations, and
FSBench-Motion, containing multimodal data and Question and Answer (QA) pairs,
supporting tasks from technical analysis to performance commentary. Initial
tests on FSBench reveal significant limitations in existing models'
understanding of artistic sports. We hope FSBench will become a key tool for
evaluating and enhancing model comprehension of figure skating.

</details>

### [321] [LR-IAD:Mask-Free Industrial Anomaly Detection with Logical Reasoning](https://arxiv.org/abs/2504.19524)
*Peijian Zeng,Feiyan Pang,Zhanbo Wang,Aimin Yang*

Main category: cs.CV

TLDR: 提出了一种无需掩码标注的工业异常检测方法，通过动态奖励函数和链式思维框架解决类别不平衡问题，显著提升了检测精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖大数据集和掩码标注，成本高且难以扩展；现有视觉语言模型也存在类似问题。工业数据集中缺陷样本占比极低，加剧了检测难度。

Method: 采用动态奖励函数优化训练，优先处理稀有缺陷模式；提出基于链式思维和GRPO的无掩码推理框架，直接从原始图像中检测异常。

Result: 在MVTec-AD和VisA数据集上分别提升36%和16%的准确率，达到最先进性能。

Conclusion: 该方法无需掩码标注，降低了成本，同时提供可解释的输出，推动了工业异常检测的实用性和可扩展性。

Abstract: Industrial Anomaly Detection (IAD) is critical for ensuring product quality
by identifying defects. Traditional methods such as feature embedding and
reconstruction-based approaches require large datasets and struggle with
scalability. Existing vision-language models (VLMs) and Multimodal Large
Language Models (MLLMs) address some limitations but rely on mask annotations,
leading to high implementation costs and false positives. Additionally,
industrial datasets like MVTec-AD and VisA suffer from severe class imbalance,
with defect samples constituting only 23.8% and 11.1% of total data
respectively. To address these challenges, we propose a reward function that
dynamically prioritizes rare defect patterns during training to handle class
imbalance. We also introduce a mask-free reasoning framework using Chain of
Thought (CoT) and Group Relative Policy Optimization (GRPO) mechanisms,
enabling anomaly detection directly from raw images without annotated masks.
This approach generates interpretable step-by-step explanations for defect
localization. Our method achieves state-of-the-art performance, outperforming
prior approaches by 36% in accuracy on MVTec-AD and 16% on VisA. By eliminating
mask dependency and reducing costs while providing explainable outputs, this
work advances industrial anomaly detection and supports scalable quality
control in manufacturing. Code to reproduce the experiment is available at
https://github.com/LilaKen/LR-IAD.

</details>

### [322] [Adversarial Shallow Watermarking](https://arxiv.org/abs/2504.19529)
*Guobiao Li,Lei Tan,Yuliang Xue,Gaozhi Liu,Zhenxing Qian,Sheng Li,Xinpeng Zhang*

Main category: cs.CV

TLDR: 提出了一种新型水印框架ASW，通过浅层解码器抵抗未知失真，无需训练、编码器或噪声层。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度神经网络的水印方法对未知失真鲁棒性不足。

Method: ASW使用随机参数化的浅层解码器，通过对抗优化嵌入水印，提取时利用解码器对失真的不敏感性。

Result: ASW在已知和未知失真上均表现优异，鲁棒性优于现有方法。

Conclusion: ASW为水印技术提供了一种简单高效的解决方案，尤其适用于未知失真场景。

Abstract: Recent advances in digital watermarking make use of deep neural networks for
message embedding and extraction. They typically follow the ``encoder-noise
layer-decoder''-based architecture. By deliberately establishing a
differentiable noise layer to simulate the distortion of the watermarked
signal, they jointly train the deep encoder and decoder to fit the noise layer
to guarantee robustness. As a result, they are usually weak against unknown
distortions that are not used in their training pipeline. In this paper, we
propose a novel watermarking framework to resist unknown distortions, namely
Adversarial Shallow Watermarking (ASW). ASW utilizes only a shallow decoder
that is randomly parameterized and designed to be insensitive to distortions
for watermarking extraction. During the watermark embedding, ASW freezes the
shallow decoder and adversarially optimizes a host image until its updated
version (i.e., the watermarked image) stably triggers the shallow decoder to
output the watermark message. During the watermark extraction, it accurately
recovers the message from the watermarked image by leveraging the insensitive
nature of the shallow decoder against arbitrary distortions. Our ASW is
training-free, encoder-free, and noise layer-free. Experiments indicate that
the watermarked images created by ASW have strong robustness against various
unknown distortions. Compared to the existing ``encoder-noise layer-decoder''
approaches, ASW achieves comparable results on known distortions and better
robustness on unknown distortions.

</details>

### [323] [Point2Quad: Generating Quad Meshes from Point Clouds via Face Prediction](https://arxiv.org/abs/2504.19545)
*Zezeng Li,Zhihui Qi,Weimin Wang,Ziliang Wang,Junyi Duan,Na Lei*

Main category: cs.CV

TLDR: Point2Quad是首个基于学习的点云生成纯四边形网格的方法，通过融合点级和面级特征解决四边形网格生成的挑战。


<details>
  <summary>Details</summary>
Motivation: 四边形网格在几何建模和计算力学中至关重要，但现有基于学习的方法主要针对三角形网格，四边形网格生成因需满足共面性、凸性和纯四边形等约束而较少被探索。

Method: Point2Quad通过k-NN候选生成考虑共面性和方形性，使用两个编码器提取几何和拓扑特征，融合特征训练分类器，并通过四边形专用后处理优化结果。

Result: 在清晰和噪声数据上的实验表明，Point2Quad在综合指标上优于基线方法。

Conclusion: Point2Quad为四边形网格生成提供了一种有效的学习框架，解决了现有方法的局限性。

Abstract: Quad meshes are essential in geometric modeling and computational mechanics.
Although learning-based methods for triangle mesh demonstrate considerable
advancements, quad mesh generation remains less explored due to the challenge
of ensuring coplanarity, convexity, and quad-only meshes. In this paper, we
present Point2Quad, the first learning-based method for quad-only mesh
generation from point clouds. The key idea is learning to identify quad mesh
with fused pointwise and facewise features. Specifically, Point2Quad begins
with a k-NN-based candidate generation considering the coplanarity and
squareness. Then, two encoders are followed to extract geometric and
topological features that address the challenge of quad-related constraints,
especially by combining in-depth quadrilaterals-specific characteristics.
Subsequently, the extracted features are fused to train the classifier with a
designed compound loss. The final results are derived after the refinement by a
quad-specific post-processing. Extensive experiments on both clear and noise
data demonstrate the effectiveness and superiority of Point2Quad, compared to
baseline methods under comprehensive metrics.

</details>

### [324] [Crowd Detection Using Very-Fine-Resolution Satellite Imagery](https://arxiv.org/abs/2504.19546)
*Tong Xiao,Qunming Wang,Ping Lu,Tenghai Huang,Xiaohua Tong,Peter M. Atkinson*

Main category: cs.CV

TLDR: 论文提出CrowdSat-Net，一种基于点的卷积神经网络，用于高分辨率卫星图像中的群体检测，并创建了首个相关数据集CrowdSat。


<details>
  <summary>Details</summary>
Motivation: 现有群体检测方法受限于时空覆盖范围，高分辨率卫星图像为此提供了新机会但尚未被利用。

Method: 提出DCPAN模块和HFGDU模块，分别用于特征表示和高频信息恢复，并构建了CrowdSat数据集。

Result: CrowdSat-Net在实验中表现最佳，F1-score和Precision分别达到66.12%和73.23%。

Conclusion: 研究通过新网络架构和数据集推动了群体检测技术的发展。

Abstract: Accurate crowd detection (CD) is critical for public safety and historical
pattern analysis, yet existing methods relying on ground and aerial imagery
suffer from limited spatio-temporal coverage. The development of
very-fine-resolution (VFR) satellite sensor imagery (e.g., ~0.3 m spatial
resolution) provides unprecedented opportunities for large-scale crowd activity
analysis, but it has never been considered for this task. To address this gap,
we proposed CrowdSat-Net, a novel point-based convolutional neural network,
which features two innovative components: Dual-Context Progressive Attention
Network (DCPAN) to improve feature representation of individuals by aggregating
scene context and local individual characteristics, and High-Frequency Guided
Deformable Upsampler (HFGDU) that recovers high-frequency information during
upsampling through frequency-domain guided deformable convolutions. To validate
the effectiveness of CrowdSat-Net, we developed CrowdSat, the first VFR
satellite imagery dataset designed specifically for CD tasks, comprising over
120k manually labeled individuals from multi-source satellite platforms
(Beijing-3N, Jilin-1 Gaofen-04A and Google Earth) across China. In the
experiments, CrowdSat-Net was compared with five state-of-the-art point-based
CD methods (originally designed for ground or aerial imagery) using CrowdSat
and achieved the largest F1-score of 66.12% and Precision of 73.23%, surpassing
the second-best method by 1.71% and 2.42%, respectively. Moreover, extensive
ablation experiments validated the importance of the DCPAN and HFGDU modules.
Furthermore, cross-regional evaluation further demonstrated the spatial
generalizability of CrowdSat-Net. This research advances CD capability by
providing both a newly developed network architecture for CD and a pioneering
benchmark dataset to facilitate future CD development.

</details>

### [325] [DEEMO: De-identity Multimodal Emotion Recognition and Reasoning](https://arxiv.org/abs/2504.19549)
*Deng Li,Bohao Xing,Xin Liu,Baiqiang Xia,Bihan Wen,Heikki Kälviäinen*

Main category: cs.CV

TLDR: 论文提出DEEMO任务，通过去身份化的多模态输入实现情感理解，并开发了DEEMO-LLaMA模型，在隐私保护的情感识别和推理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有情感理解方法依赖身份敏感信息（如面部表情和语音），可能侵犯隐私。DEEMO任务旨在解决这一问题。

Method: 提出DEEMO数据集（包含DEEMO-NFBL和DEEMO-MER子集）和DEEMO-LLaMA模型，整合去身份化的音视频及文本信息。

Result: DEEMO-LLaMA在去身份情感识别中达到74.49%准确率和74.45% F1分数，在情感推理中表现显著优于现有模型。

Conclusion: 该研究推动了隐私保护的情感理解，促进了负责任的AI发展。

Abstract: Emotion understanding is a critical yet challenging task. Most existing
approaches rely heavily on identity-sensitive information, such as facial
expressions and speech, which raises concerns about personal privacy. To
address this, we introduce the De-identity Multimodal Emotion Recognition and
Reasoning (DEEMO), a novel task designed to enable emotion understanding using
de-identified video and audio inputs. The DEEMO dataset consists of two
subsets: DEEMO-NFBL, which includes rich annotations of Non-Facial Body
Language (NFBL), and DEEMO-MER, an instruction dataset for Multimodal Emotion
Recognition and Reasoning using identity-free cues. This design supports
emotion understanding without compromising identity privacy. In addition, we
propose DEEMO-LLaMA, a Multimodal Large Language Model (MLLM) that integrates
de-identified audio, video, and textual information to enhance both emotion
recognition and reasoning. Extensive experiments show that DEEMO-LLaMA achieves
state-of-the-art performance on both tasks, outperforming existing MLLMs by a
significant margin, achieving 74.49% accuracy and 74.45% F1-score in
de-identity emotion recognition, and 6.20 clue overlap and 7.66 label overlap
in de-identity emotion reasoning. Our work contributes to ethical AI by
advancing privacy-preserving emotion understanding and promoting responsible
affective computing.

</details>

### [326] [CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel View Synthesis in Autonomous Driving Scenes](https://arxiv.org/abs/2504.19557)
*Mohammad Altillawi,Fengyi Shen,Liudi Yang,Sai Manoj Prakhya,Ziyuan Liu*

Main category: cs.CV

TLDR: CE-NPBG提出了一种基于神经点的新方法，通过结合几何和外观模态，解决了大规模点云地图在新视角合成中的渲染质量和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于点的方法在大规模3D点云地图中面临渲染质量和可扩展性限制，主要问题是几何与外观之间的可见性不匹配。

Method: CE-NPBG利用相机和LiDAR数据构建几何与外观的连接关系图，通过神经描述符和联合对抗训练提升渲染质量。

Result: 该方法显著提升了渲染质量，并通过仅使用点云子集优化了运行时间和可扩展性。

Conclusion: CE-NPBG为大规模自动驾驶场景的新视角合成提供了一种高效且高质量的解决方案。

Abstract: Current point-based approaches encounter limitations in scalability and
rendering quality when using large 3D point cloud maps because using them
directly for novel view synthesis (NVS) leads to degraded visualizations. We
identify the primary issue behind these low-quality renderings as a visibility
mismatch between geometry and appearance, stemming from using these two
modalities together. To address this problem, we present CE-NPBG, a new
approach for novel view synthesis (NVS) in large-scale autonomous driving
scenes. Our method is a neural point-based technique that leverages two
modalities: posed images (cameras) and synchronized raw 3D point clouds
(LiDAR). We first employ a connectivity relationship graph between appearance
and geometry, which retrieves points from a large 3D point cloud map observed
from the current camera perspective and uses them for rendering. By leveraging
this connectivity, our method significantly improves rendering quality and
enhances run-time and scalability by using only a small subset of points from
the large 3D point cloud map. Our approach associates neural descriptors with
the points and uses them to synthesize views. To enhance the encoding of these
descriptors and elevate rendering quality, we propose a joint adversarial and
point rasterization training. During training, we pair an image-synthesizer
network with a multi-resolution discriminator. At inference, we decouple them
and use the image-synthesizer to generate novel views. We also integrate our
proposal into the recent 3D Gaussian Splatting work to highlight its benefits
for improved rendering and scalability.

</details>

### [327] [Category-Level and Open-Set Object Pose Estimation for Robotics](https://arxiv.org/abs/2504.19572)
*Peter Hönig,Matthias Hirschmanner,Markus Vincze*

Main category: cs.CV

TLDR: 本文比较了类别级6D姿态估计的数据集、精度指标和算法，并探讨如何将其与开放集姿态估计结合以实现泛化。


<details>
  <summary>Details</summary>
Motivation: 解决类别级和开放集中物体姿态估计的挑战，特别是纹理、形状和尺寸未知时的困难。

Method: 比较和分析不同数据集、精度指标和算法。

Result: 提出了实现泛化的建议和可操作的推荐。

Conclusion: 通过比较和分析，为类别级和开放集姿态估计的泛化提供了方向和建议。

Abstract: Object pose estimation enables a variety of tasks in computer vision and
robotics, including scene understanding and robotic grasping. The complexity of
a pose estimation task depends on the unknown variables related to the target
object. While instance-level methods already excel for opaque and Lambertian
objects, category-level and open-set methods, where texture, shape, and size
are partially or entirely unknown, still struggle with these basic material
properties. Since texture is unknown in these scenarios, it cannot be used for
disambiguating object symmetries, another core challenge of 6D object pose
estimation. The complexity of estimating 6D poses with such a manifold of
unknowns led to various datasets, accuracy metrics, and algorithmic solutions.
This paper compares datasets, accuracy metrics, and algorithms for solving 6D
pose estimation on the category-level. Based on this comparison, we analyze how
to bridge category-level and open-set object pose estimation to reach
generalization and provide actionable recommendations.

</details>

### [328] [DG-DETR: Toward Domain Generalized Detection Transformer](https://arxiv.org/abs/2504.19574)
*Seongmin Hwang,Daeyoung Han,Moongu Jeon*

Main category: cs.CV

TLDR: DG-DETR是一种简单、有效的端到端Transformer检测器，通过正交投影和小波分解提升跨域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化研究主要关注CNN检测器，而忽视了提升DETR的鲁棒性。

Method: 提出域无关查询选择策略和小波分解，分离特征为域不变和域特定成分。

Result: 实验验证了DG-DETR的有效性。

Conclusion: DG-DETR为DETR的跨域鲁棒性提供了简单且可插拔的解决方案。

Abstract: End-to-end Transformer-based detectors (DETRs) have demonstrated strong
detection performance. However, domain generalization (DG) research has
primarily focused on convolutional neural network (CNN)-based detectors, while
paying little attention to enhancing the robustness of DETRs. In this letter,
we introduce a Domain Generalized DEtection TRansformer (DG-DETR), a simple,
effective, and plug-and-play method that improves out-of-distribution (OOD)
robustness for DETRs. Specifically, we propose a novel domain-agnostic query
selection strategy that removes domain-induced biases from object queries via
orthogonal projection onto the instance-specific style space. Additionally, we
leverage a wavelet decomposition to disentangle features into domain-invariant
and domain-specific components, enabling synthesis of diverse latent styles
while preserving the semantic features of objects. Experimental results
validate the effectiveness of DG-DETR. Our code is available at
https://github.com/sminhwang/DG-DETR.

</details>

### [329] [SAMBLE: Shape-Specific Point Cloud Sampling for an Optimal Trade-Off Between Local Detail and Global Uniformity](https://arxiv.org/abs/2504.19581)
*Chengzhi Wu,Yuxin Wan,Hao Fu,Julius Pfrommer,Zeyun Zhong,Junwei Zheng,Jiaming Zhang,Jürgen Beyerer*

Main category: cs.CV

TLDR: SAMBLE方法通过稀疏注意力图和分箱学习，为点云形状提供特定采样策略，平衡局部细节与全局均匀性，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 点云采样在3D计算机视觉中至关重要，但现有学习方法存在采样模式不可识别或结果偏差问题，且忽视了点分布的自然变化。

Method: 提出SAMBLE方法，结合稀疏注意力图和分箱学习，学习形状特定的采样策略。

Result: SAMBLE在局部细节与全局均匀性之间取得更好平衡，在少点采样场景下也表现优异。

Conclusion: SAMBLE为点云采样提供了一种高效且适应性强的解决方案，显著提升下游任务性能。

Abstract: Driven by the increasing demand for accurate and efficient representation of
3D data in various domains, point cloud sampling has emerged as a pivotal
research topic in 3D computer vision. Recently, learning-to-sample methods have
garnered growing interest from the community, particularly for their ability to
be jointly trained with downstream tasks. However, previous learning-based
sampling methods either lead to unrecognizable sampling patterns by generating
a new point cloud or biased sampled results by focusing excessively on sharp
edge details. Moreover, they all overlook the natural variations in point
distribution across different shapes, applying a similar sampling strategy to
all point clouds. In this paper, we propose a Sparse Attention Map and
Bin-based Learning method (termed SAMBLE) to learn shape-specific sampling
strategies for point cloud shapes. SAMBLE effectively achieves an improved
balance between sampling edge points for local details and preserving
uniformity in the global shape, resulting in superior performance across
multiple common point cloud downstream tasks, even in scenarios with few-point
sampling.

</details>

### [330] [ShowMak3r: Compositional TV Show Reconstruction](https://arxiv.org/abs/2504.19584)
*Sangmin Kim,Seunguk Do,Jaesik Park*

Main category: cs.CV

TLDR: ShowMak3r是一个动态辐射场重建管道，用于从娱乐视频中重建和编辑场景，解决了演员遮挡、复杂舞台和小基线视图等问题。


<details>
  <summary>Details</summary>
Motivation: 娱乐视频（如电视剧）中的动态辐射场重建面临演员遮挡、复杂舞台和镜头切换等挑战，需要一种综合解决方案。

Method: ShowMak3r包含3DLocator模块定位演员、ShotMatcher模块跟踪镜头切换，以及动态面部拟合网络恢复表情。

Result: 在Sitcoms3D数据集上，ShowMak3r成功重建了场景并支持合成镜头制作、演员重定位等应用。

Conclusion: ShowMak3r为娱乐视频的动态场景重建和编辑提供了一种有效的解决方案，具有广泛的应用潜力。

Abstract: Reconstructing dynamic radiance fields from video clips is challenging,
especially when entertainment videos like TV shows are given. Many challenges
make the reconstruction difficult due to (1) actors occluding with each other
and having diverse facial expressions, (2) cluttered stages, and (3) small
baseline views or sudden shot changes. To address these issues, we present
ShowMak3r, a comprehensive reconstruction pipeline that allows the editing of
scenes like how video clips are made in a production control room. In
ShowMak3r, a 3DLocator module locates recovered actors on the stage using depth
prior and estimates unseen human poses via interpolation. The proposed
ShotMatcher module then tracks the actors under shot changes. Furthermore,
ShowMak3r introduces a face-fitting network that dynamically recovers the
actors' expressions. Experiments on Sitcoms3D dataset show that our pipeline
can reassemble TV show scenes with new cameras at different timestamps. We also
demonstrate that ShowMak3r enables interesting applications such as synthetic
shot-making, actor relocation, insertion, deletion, and pose manipulation.
Project page : https://nstar1125.github.io/showmak3r

</details>

### [331] [Magnifier: A Multi-grained Neural Network-based Architecture for Burned Area Delineation](https://arxiv.org/abs/2504.19589)
*Daniele Rege Cambrin,Luca Colomba,Paolo Garza*

Main category: cs.CV

TLDR: 论文提出了一种名为Magnifier的新方法，通过双编码器（局部和全局）在数据稀缺的情况下提升图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 在危机管理和遥感中，图像分割对灾难响应和应急规划至关重要，但数据稀缺和缺乏基准数据集限制了神经网络模型的训练能力。

Method: Magnifier通过双编码器（局部和全局）从同一输入中提取不同粒度的信息，扩展了现有编码器-解码器架构。

Result: Magnifier平均IoU提升了2.65%，且参数增加有限，在燃烧区域分割任务中表现优于或接近现有方法，计算量减半。

Conclusion: Magnifier在数据稀缺情况下显著提升了分割性能，同时保持了计算效率。

Abstract: In crisis management and remote sensing, image segmentation plays a crucial
role, enabling tasks like disaster response and emergency planning by analyzing
visual data. Neural networks are able to analyze satellite acquisitions and
determine which areas were affected by a catastrophic event. The problem in
their development in this context is the data scarcity and the lack of
extensive benchmark datasets, limiting the capabilities of training large
neural network models. In this paper, we propose a novel methodology, namely
Magnifier, to improve segmentation performance with limited data availability.
The Magnifier methodology is applicable to any existing encoder-decoder
architecture, as it extends a model by merging information at different
contextual levels through a dual-encoder approach: a local and global encoder.
Magnifier analyzes the input data twice using the dual-encoder approach. In
particular, the local and global encoders extract information from the same
input at different granularities. This allows Magnifier to extract more
information than the other approaches given the same set of input images.
Magnifier improves the quality of the results of +2.65% on average IoU while
leading to a restrained increase in terms of the number of trainable parameters
compared to the original model. We evaluated our proposed approach with
state-of-the-art burned area segmentation models, demonstrating, on average,
comparable or better performances in less than half of the GFLOPs.

</details>

### [332] [Neural network task specialization via domain constraining](https://arxiv.org/abs/2504.19592)
*Roman Malashin,Daniil Ilyukhin*

Main category: cs.CV

TLDR: 论文提出通过任务特定领域约束实现神经网络专业化，提升网络在特定数据子空间上的性能。实验表明，仅通过约束类别标签空间即可提高通用网络的准确性，无需额外数据或改变训练方式。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索如何通过专业化提升神经网络在特定任务上的性能，同时避免通用网络的冗余计算和资源浪费。

Method: 方法包括任务特定领域约束、修改传统微调方法，以及在网络调优前进行专家提取阶段。

Result: 实验结果显示，专业化能显著提升通用网络的准确性，且无需额外数据或训练调整。

Conclusion: 结论指出，专业化方法为未来动态可配置图像分析系统的开发奠定了基础，并能在排除特定数据域时提升系统性能。

Abstract: This paper introduces a concept of neural network specialization via
task-specific domain constraining, aimed at enhancing network performance on
data subspace in which the network operates. The study presents experiments on
training specialists for image classification and object detection tasks. The
results demonstrate that specialization can enhance a generalist's accuracy
even without additional data or changing training regimes: solely by
constraining class label space in which the network performs. Theoretical and
experimental analyses indicate that effective specialization requires modifying
traditional fine-tuning methods and constraining data space to semantically
coherent subsets. The specialist extraction phase before tuning the network is
proposed for maximal performance gains. We also provide analysis of the
evolution of the feature space during specialization. This study paves way to
future research for developing more advanced dynamically configurable image
analysis systems, where computations depend on the specific input.
Additionally, the proposed methods can help improve system performance in
scenarios where certain data domains should be excluded from consideration of
the generalist network.

</details>

### [333] [Lightweight Adapter Learning for More Generalized Remote Sensing Change Detection](https://arxiv.org/abs/2504.19598)
*Dou Quan,Rufan Zhou,Shuang Wang,Ning Huyan,Dong Zhao,Yunan Li,Licheng Jiao*

Main category: cs.CV

TLDR: 论文提出了一种通用的变化检测网络CANet，通过共享和特定数据集模块解决现有方法泛化能力差的问题，具有轻量化和高性能特点。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在遥感图像变化检测中泛化能力差，无法适应不同数据集的数据分布和标注差异。

Method: 提出CANet，包含共享模块和轻量适配器模块，引入感兴趣变化区域掩码和独特批归一化层处理数据差异。

Result: CANet在多个数据集上表现优异，泛化能力强，训练成本低（仅更新4.1%-7.7%参数）。

Conclusion: CANet是一种高效通用的变化检测方法，适用于不同数据集，性能优于现有方法。

Abstract: Deep learning methods have shown promising performances in remote sensing
image change detection (CD). However, existing methods usually train a
dataset-specific deep network for each dataset. Due to the significant
differences in the data distribution and labeling between various datasets, the
trained dataset-specific deep network has poor generalization performances on
other datasets. To solve this problem, this paper proposes a change adapter
network (CANet) for a more universal and generalized CD. CANet contains
dataset-shared and dataset-specific learning modules. The former explores the
discriminative features of images, and the latter designs a lightweight adapter
model, to deal with the characteristics of different datasets in data
distribution and labeling. The lightweight adapter can quickly generalize the
deep network for new CD tasks with a small computation cost. Specifically, this
paper proposes an interesting change region mask (ICM) in the adapter, which
can adaptively focus on interested change objects and decrease the influence of
labeling differences in various datasets. Moreover, CANet adopts a unique batch
normalization layer for each dataset to deal with data distribution
differences. Compared with existing deep learning methods, CANet can achieve
satisfactory CD performances on various datasets simultaneously. Experimental
results on several public datasets have verified the effectiveness and
advantages of the proposed CANet on CD. CANet has a stronger generalization
ability, smaller training costs (merely updating 4.1%-7.7% parameters), and
better performances under limited training datasets than other deep learning
methods, which also can be flexibly inserted with existing deep models.

</details>

### [334] [Image Generation Method Based on Heat Diffusion Models](https://arxiv.org/abs/2504.19600)
*Pengfei Zhang,Shouqing Jia*

Main category: cs.CV

TLDR: HDM通过结合像素级操作和二维热方程，改进了DDPM的图像生成质量，生成更真实的图像。


<details>
  <summary>Details</summary>
Motivation: DDPM虽然能生成高质量图像，但未充分利用相邻像素的关联性，HDM旨在通过像素级操作提升细节保留和图像真实性。

Method: HDM在DDPM的基础上引入二维热方程的离散形式，计算相邻像素关系，同时保持训练过程不变。

Result: 实验表明，HDM生成的图像质量优于DDPM、CDM、LDM和VQGAN等模型。

Conclusion: HDM通过像素级操作显著提升了图像生成质量，为扩散模型提供了新思路。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) achieve high-quality image
generation without adversarial training, but they process images as a whole.
Since adjacent pixels are highly likely to belong to the same object, we
propose the Heat Diffusion Model (HDM) to further preserve image details and
generate more realistic images. HDM is a model that incorporates pixel-level
operations while maintaining the same training process as DDPM. In HDM, the
discrete form of the two-dimensional heat equation is integrated into the
diffusion and generation formulas of DDPM, enabling the model to compute
relationships between neighboring pixels during image processing. Our
experiments demonstrate that HDM can generate higher-quality samples compared
to models such as DDPM, Consistency Diffusion Models (CDM), Latent Diffusion
Models (LDM), and Vector Quantized Generative Adversarial Networks (VQGAN).

</details>

### [335] [DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer](https://arxiv.org/abs/2504.19614)
*Junpeng Jiang,Gangyi Hong,Miao Zhang,Hengtong Hu,Kun Zhan,Rui Shao,Liqiang Nie*

Main category: cs.CV

TLDR: DiVE是一个基于扩散变换器的生成框架，用于生成高质量、时间一致且多视角一致的驾驶场景视频，解决了现有生成模型的质量和一致性问题。


<details>
  <summary>Details</summary>
Motivation: 收集多视角驾驶场景视频成本高且困难，现有生成模型生成的视频质量和一致性不足，限制了其在感知任务中的应用。

Method: DiVE采用扩散变换器框架，结合统一的跨注意力和SketchFormer控制多模态数据，并引入无额外参数的视角膨胀注意力机制确保多视角一致性。

Result: 在nuScenes数据集上，DiVE实现了最先进的性能，生成的照片级真实视频具有出色的时间和跨视角一致性。

Conclusion: DiVE通过创新的多条件控制和分辨率渐进采样策略，显著提升了生成视频的质量和效率，为驾驶场景感知任务提供了高质量的合成数据。

Abstract: Collecting multi-view driving scenario videos to enhance the performance of
3D visual perception tasks presents significant challenges and incurs
substantial costs, making generative models for realistic data an appealing
alternative. Yet, the videos generated by recent works suffer from poor quality
and spatiotemporal consistency, undermining their utility in advancing
perception tasks under driving scenarios. To address this gap, we propose DiVE,
a diffusion transformer-based generative framework meticulously engineered to
produce high-fidelity, temporally coherent, and cross-view consistent
multi-view videos, aligning seamlessly with bird's-eye view layouts and textual
descriptions. DiVE leverages a unified cross-attention and a SketchFormer to
exert precise control over multimodal data, while incorporating a view-inflated
attention mechanism that adds no extra parameters, thereby guaranteeing
consistency across views. Despite these advancements, synthesizing
high-resolution videos under multimodal constraints introduces dual challenges:
investigating the optimal classifier-free guidance coniguration under intricate
multi-condition inputs and mitigating excessive computational latency in
high-resolution rendering--both of which remain underexplored in prior
researches. To resolve these limitations, we introduce two innovations:
Multi-Control Auxiliary Branch Distillation, which streamlines multi-condition
CFG selection while circumventing high computational overhead, and Resolution
Progressive Sampling, a training-free acceleration strategy that staggers
resolution scaling to reduce high latency due to high resolution. These
innovations collectively achieve a 2.62x speedup with minimal quality
degradation. Evaluated on the nuScenes dataset, DiVE achieves SOTA performance
in multi-view video generation, yielding photorealistic outputs with
exceptional temporal and cross-view coherence.

</details>

### [336] [NSegment : Noisy Segment Improves Remote Sensing Image Segmentation](https://arxiv.org/abs/2504.19634)
*Yechan Kim,DongHo Yoon,SooYeon Kim,Moongu Jeon*

Main category: cs.CV

TLDR: NSegment是一种简单有效的数据增强方法，通过弹性变换标签来解决遥感图像分割中的标注错误问题。


<details>
  <summary>Details</summary>
Motivation: 遥感图像分割数据集中存在标注错误，且标注数据稀缺，传统方法复杂且耗时。

Method: 仅对分割标签应用弹性变换，并根据样本调整变形强度。

Result: 实验表明，该方法提升了多种先进模型的性能。

Conclusion: NSegment是一种高效且简单的解决方案，适用于遥感图像分割任务。

Abstract: Labeling errors in remote sensing (RS) image segmentation datasets often
remain implicit and subtle due to ambiguous class boundaries, mixed pixels,
shadows, complex terrain features, and subjective annotator bias. Furthermore,
the scarcity of annotated RS data due to high image acquisition and labeling
costs complicates training noise-robust models. While sophisticated mechanisms
such as label selection or noise correction might address this issue, they tend
to increase training time and add implementation complexity. In this letter, we
propose NSegment-a simple yet effective data augmentation solution to mitigate
this issue. Unlike traditional methods, it applies elastic transformations only
to segmentation labels, varying deformation intensity per sample in each
training epoch to address annotation inconsistencies. Experimental results
demonstrate that our approach improves the performance of RS image segmentation
on various state-of-the-art models.

</details>

### [337] [Exploiting Inter-Sample Correlation and Intra-Sample Redundancy for Partially Relevant Video Retrieval](https://arxiv.org/abs/2504.19637)
*Junlong Ren,Gangjian Zhang,Yu Hu,Jian Shu,Hao Wang*

Main category: cs.CV

TLDR: 提出了一种新的PRVR框架，通过捕捉样本间相关性和样本内冗余性，结合三个核心模块（ICE、IRM、TCP），显著提升了部分相关视频检索的性能。


<details>
  <summary>Details</summary>
Motivation: PRVR任务中，文本查询与视频内容存在语义不对称性，现有方法忽略了跨模态的双重特性（样本间相关性和样本内冗余性）。

Method: 设计了三个模块：ICE（捕捉样本间相关性）、IRM（减少样本内冗余性）、TCP（增强特征判别性）。

Result: 在三个数据集上的实验表明，该方法优于现有方法，达到了最先进的性能。

Conclusion: 通过系统性利用跨模态特性，提出的框架显著提升了PRVR任务的性能。

Abstract: Partially Relevant Video Retrieval (PRVR) aims to retrieve the target video
that is partially relevant to the text query. The primary challenge in PRVR
arises from the semantic asymmetry between textual and visual modalities, as
videos often contain substantial content irrelevant to the query. Existing
methods coarsely align paired videos and text queries to construct the semantic
space, neglecting the critical cross-modal dual nature inherent in this task:
inter-sample correlation and intra-sample redundancy. To this end, we propose a
novel PRVR framework to systematically exploit these two characteristics. Our
framework consists of three core modules. First, the Inter Correlation
Enhancement (ICE) module captures inter-sample correlation by identifying
semantically similar yet unpaired text queries and video moments, combining
them to form pseudo-positive pairs for more robust semantic space construction.
Second, the Intra Redundancy Mining (IRM) module mitigates intra-sample
redundancy by mining redundant video moment features and treating them as hard
negative samples, thereby encouraging the model to learn more discriminative
representations. Finally, to reinforce these modules, we introduce the Temporal
Coherence Prediction (TCP) module, which enhances feature discrimination by
training the model to predict the original temporal order of randomly shuffled
video frames and moments. Extensive experiments on three datasets demonstrate
the superiority of our approach compared to previous methods, achieving
state-of-the-art results.

</details>

### [338] [BARIS: Boundary-Aware Refinement with Environmental Degradation Priors for Robust Underwater Instance Segmentation](https://arxiv.org/abs/2504.19643)
*Pin-Chi Pan,Soo-Chang Pei*

Main category: cs.CV

TLDR: BARIS-ERA框架通过边界感知解码器和环境鲁棒适配器提升水下实例分割性能，显著优于Mask R-CNN。


<details>
  <summary>Details</summary>
Motivation: 水下视觉条件（如光衰减、散射和颜色失真）导致模型性能下降，需改进分割精度。

Method: 提出BARIS-Decoder（边界感知细化解码器）和ERA（环境鲁棒适配器），减少90%以上可训练参数。

Result: BARIS-ERA在Swin-B和ConvNeXt V2骨干网络上分别提升3.4和3.8 mAP。

Conclusion: BARIS-ERA为水下实例分割提供了高效且鲁棒的解决方案。

Abstract: Underwater instance segmentation is challenging due to adverse visual
conditions such as light attenuation, scattering, and color distortion, which
degrade model performance. In this work, we propose BARIS-Decoder
(Boundary-Aware Refinement Decoder for Instance Segmentation), a framework that
enhances segmentation accuracy through feature refinement. To address
underwater degradations, we introduce the Environmental Robust Adapter (ERA),
which efficiently models underwater degradation patterns while reducing
trainable parameters by over 90\% compared to full fine-tuning. The integration
of BARIS-Decoder with ERA-tuning, referred to as BARIS-ERA, achieves
state-of-the-art performance, surpassing Mask R-CNN by 3.4 mAP with a Swin-B
backbone and 3.8 mAP with ConvNeXt V2. Our findings demonstrate the
effectiveness of BARIS-ERA in advancing underwater instance segmentation,
providing a robust and efficient solution.

</details>

### [339] [xEdgeFace: Efficient Cross-Spectral Face Recognition for Edge Devices](https://arxiv.org/abs/2504.19646)
*Anjith George,Sebastien Marcel*

Main category: cs.CV

TLDR: 提出了一种轻量级异构人脸识别框架，结合CNN-Transformer架构，高效训练且性能优越，适用于资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 解决异构人脸识别中计算密集型方法的局限性，提升在边缘设备上的实用性。

Method: 采用混合CNN-Transformer架构，支持高效端到端训练，减少对配对异构数据的依赖。

Result: 在多个异构和标准人脸识别基准测试中表现优于现有方法，计算开销低。

Conclusion: 该框架为异构和同构场景提供了高效且高性能的解决方案。

Abstract: Heterogeneous Face Recognition (HFR) addresses the challenge of matching face
images across different sensing modalities, such as thermal to visible or
near-infrared to visible, expanding the applicability of face recognition
systems in real-world, unconstrained environments. While recent HFR methods
have shown promising results, many rely on computation-intensive architectures,
limiting their practicality for deployment on resource-constrained edge
devices. In this work, we present a lightweight yet effective HFR framework by
adapting a hybrid CNN-Transformer architecture originally designed for face
recognition. Our approach enables efficient end-to-end training with minimal
paired heterogeneous data while preserving strong performance on standard RGB
face recognition tasks. This makes it a compelling solution for both
homogeneous and heterogeneous scenarios. Extensive experiments across multiple
challenging HFR and face recognition benchmarks demonstrate that our method
consistently outperforms state-of-the-art approaches while maintaining a low
computational overhead.

</details>

### [340] [Explaining Vision GNNs: A Semantic and Visual Analysis of Graph-based Image Classification](https://arxiv.org/abs/2504.19682)
*Nikolaos Chaidos,Angeliki Dimitriou,Nikolaos Spanos,Athanasios Voulodimos,Giorgos Stamou*

Main category: cs.CV

TLDR: 该论文研究了基于图神经网络（GNN）的图像分类器的可解释性，分析了不同层中图的语义一致性及其对对象结构和关系的保留能力。


<details>
  <summary>Details</summary>
Motivation: 尽管GNN在视觉任务中表现出高效性，但其可解释性尚未充分研究，而图本身具有天然的可解释性。

Method: 通过量化层间图连接的语义相似性和空间一致性，并结合标准与对抗设置下的解释对比，评估模型的鲁棒性。

Result: 研究发现，模型的决策过程可以有效解释，但其推理与人类感知并不完全一致，尤其是在深层网络中。

Conclusion: 研究表明GNN模型的可解释性存在潜力，但也揭示了其与人类理解的差异，尤其是在复杂场景中。

Abstract: Graph Neural Networks (GNNs) have emerged as an efficient alternative to
convolutional approaches for vision tasks such as image classification,
leveraging patch-based representations instead of raw pixels. These methods
construct graphs where image patches serve as nodes, and edges are established
based on patch similarity or classification relevance. Despite their
efficiency, the explainability of GNN-based vision models remains
underexplored, even though graphs are naturally interpretable. In this work, we
analyze the semantic consistency of the graphs formed at different layers of
GNN-based image classifiers, focusing on how well they preserve object
structures and meaningful relationships. A comprehensive analysis is presented
by quantifying the extent to which inter-layer graph connections reflect
semantic similarity and spatial coherence. Explanations from standard and
adversarial settings are also compared to assess whether they reflect the
classifiers' robustness. Additionally, we visualize the flow of information
across layers through heatmap-based visualization techniques, thereby
highlighting the models' explainability. Our findings demonstrate that the
decision-making processes of these models can be effectively explained, while
also revealing that their reasoning does not necessarily align with human
perception, especially in deeper layers.

</details>

### [341] [ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather Classification in Traffic Camera Imagery](https://arxiv.org/abs/2504.19684)
*Anush Lakshman Sivaraman,Kojo Adu-Gyamfi,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

TLDR: 论文提出了一种结合生成域适应和高效对比学习的框架，用于提升低质量交通摄像头图像中的天气分类性能，特别是在夜间条件下。


<details>
  <summary>Details</summary>
Motivation: 解决低质量交通摄像头图像（尤其是夜间条件下）天气分类的挑战。

Method: 使用CycleGAN进行域转换以提升夜间图像质量，并结合SigLIP-2对比损失进行高效学习。

Result: 最佳模型在夜间条件下达到85.90%的准确率，整体准确率为97.01%。

Conclusion: 结合域适应和高效对比学习可构建实用的天气分类系统。

Abstract: Accurate weather classification from low-quality traffic camera imagery
remains a challenging task, particularly under adverse nighttime conditions. In
this study, we propose a scalable framework that combines generative domain
adaptation with efficient contrastive learning to enhance classification
performance. Using CycleGAN-based domain translation, we improve the quality of
nighttime images, enabling better feature extraction by downstream models.
While the baseline EVA-02 model employing CLIP-based contrastive loss achieves
an overall accuracy of 96.55\%, it exhibits a significant performance gap
between daytime (97.21\%) and nighttime conditions (63.40\%). Replacing CLIP
with the lightweight SigLIP-2 (Sigmoid contrastive loss) achieves a competitive
overall accuracy of 94.00\%, with substantial improvements in nighttime
performance (85.90\% accuracy). The combination of Vision-SigLIP-2,
Text-SigLIP-2, CycleGAN, and contrastive training achieves the best nighttime
accuracy (85.90\%) among all models tested, while EVA-02 with CycleGAN
maintains the highest overall accuracy (97.01\%) and per-class accuracies.
These findings demonstrate the potential of combining domain adaptation and
efficient contrastive learning to build practical, resource-efficient weather
classification systems for intelligent transportation infrastructure.

</details>

### [342] [Prompt Guiding Multi-Scale Adaptive Sparse Representation-driven Network for Low-Dose CT MAR](https://arxiv.org/abs/2504.19687)
*Baoshun Shi,Bing Chen,Shaolei Zhang,Huazhu Fu,Zhanli Hu*

Main category: cs.CV

TLDR: 提出了一种名为PMSRNet的多尺度自适应稀疏表示驱动网络，用于低剂量CT重建和金属伪影减少任务，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT（LDCT）虽能减少辐射，但会降低图像质量并产生金属伪影。现有深度学习方法忽视多尺度信息和需要为不同剂量训练多个模型。

Method: 设计了PMSRNet，结合多尺度稀疏框架，利用PSATG和MSFuM模块捕捉多尺度信息；提出PDuMSRNet框架，通过提示引导策略训练单一模型适应多剂量。

Result: 实验表明，该方法在多种剂量水平下优于现有LDMAR方法。

Conclusion: PMSRNet和PDuMSRNet有效解决了LDCT重建和金属伪影减少的问题，性能优越且模型更高效。

Abstract: Low-dose CT (LDCT) is capable of reducing X-ray radiation exposure, but it
will potentially degrade image quality, even yields metal artifacts at the case
of metallic implants. For simultaneous LDCT reconstruction and metal artifact
reduction (LDMAR), existing deep learning-based efforts face two main
limitations: i) the network design neglects multi-scale and within-scale
information; ii) training a distinct model for each dose necessitates
significant storage space for multiple doses. To fill these gaps, we propose a
prompt guiding multi-scale adaptive sparse representation-driven network,
abbreviated as PMSRNet, for LDMAR task. Specifically, we construct PMSRNet
inspired from multi-scale sparsifying frames, and it can simultaneously employ
within-scale characteristics and cross-scale complementarity owing to an
elaborated prompt guiding scale-adaptive threshold generator (PSATG) and a
built multi-scale coefficient fusion module (MSFuM). The PSATG can adaptively
capture multiple contextual information to generate more faithful thresholds,
achieved by fusing features from local, regional, and global levels.
Furthermore, we elaborate a model interpretable dual domain LDMAR framework
called PDuMSRNet, and train single model with a prompt guiding strategy for
multiple dose levels. We build a prompt guiding module, whose input contains
dose level, metal mask and input instance, to provide various guiding
information, allowing a single model to accommodate various CT dose settings.
Extensive experiments at various dose levels demonstrate that the proposed
methods outperform the state-of-the-art LDMAR methods.

</details>

### [343] [SubGrapher: Visual Fingerprinting of Chemical Structures](https://arxiv.org/abs/2504.19695)
*Lucas Morin,Gerhard Ingmar Meijer,Valéry Weber,Luc Van Gool,Peter W. J. Staar*

Main category: cs.CV

TLDR: SubGrapher是一种直接从化学结构图像中提取分子指纹的方法，优于传统的光学化学结构识别（OCSR）和指纹方法。


<details>
  <summary>Details</summary>
Motivation: 加速药物发现和材料科学等领域的研究，特别是从专利文档中提取视觉形式的分子信息。

Method: 基于学习的实例分割技术，识别功能基团和碳骨架，构建基于子结构的指纹。

Result: 在检索性能和鲁棒性上优于现有方法。

Conclusion: SubGrapher是一种高效的工具，数据集、模型和代码将公开。

Abstract: Automatic extraction of chemical structures from scientific literature plays
a crucial role in accelerating research across fields ranging from drug
discovery to materials science. Patent documents, in particular, contain
molecular information in visual form, which is often inaccessible through
traditional text-based searches. In this work, we introduce SubGrapher, a
method for the visual fingerprinting of chemical structure images. Unlike
conventional Optical Chemical Structure Recognition (OCSR) models that attempt
to reconstruct full molecular graphs, SubGrapher focuses on extracting
molecular fingerprints directly from chemical structure images. Using
learning-based instance segmentation, SubGrapher identifies functional groups
and carbon backbones, constructing a substructure-based fingerprint that
enables chemical structure retrieval. Our approach is evaluated against
state-of-the-art OCSR and fingerprinting methods, demonstrating superior
retrieval performance and robustness across diverse molecular depictions. The
dataset, models, and code will be made publicly available.

</details>

### [344] [Open-set Anomaly Segmentation in Complex Scenarios](https://arxiv.org/abs/2504.19706)
*Song Xia,Yi Yu,Henghui Ding,Wenhan Yang,Shifei Liu,Alex C. Kot,Xudong Jiang*

Main category: cs.CV

TLDR: 论文提出了ComsAmy基准和DiffEEL方法，用于复杂开放世界场景中的异常分割，显著提升了现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有异常分割基准在复杂气象条件下的评估不可靠，限制了模型在安全关键应用中的部署。

Method: 提出能量-熵学习策略（EEL）和基于扩散的异常数据合成器，增强模型在复杂环境中的鲁棒性。

Result: DiffEEL方法在AUPRC和FPR95指标上分别平均提升了4.96%和9.87%。

Conclusion: DiffEEL是一种有效的即插即用方法，显著提升了异常分割模型在复杂开放世界场景中的性能。

Abstract: Precise segmentation of out-of-distribution (OoD) objects, herein referred to
as anomalies, is crucial for the reliable deployment of semantic segmentation
models in open-set, safety-critical applications, such as autonomous driving.
Current anomalous segmentation benchmarks predominantly focus on favorable
weather conditions, resulting in untrustworthy evaluations that overlook the
risks posed by diverse meteorological conditions in open-set environments, such
as low illumination, dense fog, and heavy rain. To bridge this gap, this paper
introduces the ComsAmy, a challenging benchmark specifically designed for
open-set anomaly segmentation in complex scenarios. ComsAmy encompasses a wide
spectrum of adverse weather conditions, dynamic driving environments, and
diverse anomaly types to comprehensively evaluate the model performance in
realistic open-world scenarios. Our extensive evaluation of several
state-of-the-art anomalous segmentation models reveals that existing methods
demonstrate significant deficiencies in such challenging scenarios,
highlighting their serious safety risks for real-world deployment. To solve
that, we propose a novel energy-entropy learning (EEL) strategy that integrates
the complementary information from energy and entropy to bolster the robustness
of anomaly segmentation under complex open-world environments. Additionally, a
diffusion-based anomalous training data synthesizer is proposed to generate
diverse and high-quality anomalous images to enhance the existing copy-paste
training data synthesizer. Extensive experimental results on both public and
ComsAmy benchmarks demonstrate that our proposed diffusion-based synthesizer
with energy and entropy learning (DiffEEL) serves as an effective and
generalizable plug-and-play method to enhance existing models, yielding an
average improvement of around 4.96% in $\rm{AUPRC}$ and 9.87% in
$\rm{FPR}_{95}$.

</details>

### [345] [A computer vision method to estimate ventilation rate of Atlantic salmon in sea fish farms](https://arxiv.org/abs/2504.19719)
*Lukas Folkman,Quynh LK Vo,Colin Johnston,Bela Stantic,Kylie A Pitt*

Main category: cs.CV

TLDR: 开发了一种基于计算机视觉的方法，用于在商业海水养殖场中监测大西洋鲑鱼的呼吸率，通过鱼头检测和嘴部状态分类实现高效监测。


<details>
  <summary>Details</summary>
Motivation: 现有智能监测方法多局限于受控环境，缺乏对实际海水养殖场中鱼类生理特征的直接监测能力。

Method: 使用卷积神经网络检测鱼头并分类嘴部状态，结合多目标跟踪技术估计呼吸率。

Result: 在独立测试集上，预测呼吸率与真实值的皮尔逊相关系数达到0.82。

Conclusion: 该方法能准确识别鱼类呼吸窘迫，具有广泛适用性，可改善鱼类健康监测。

Abstract: The increasing demand for aquaculture production necessitates the development
of innovative, intelligent tools to effectively monitor and manage fish health
and welfare. While non-invasive video monitoring has become a common practice
in finfish aquaculture, existing intelligent monitoring methods predominantly
focus on assessing body condition or fish swimming patterns and are often
developed and evaluated in controlled tank environments, without demonstrating
their applicability to real-world aquaculture settings in open sea farms. This
underscores the necessity for methods that can monitor physiological traits
directly within the production environment of sea fish farms. To this end, we
have developed a computer vision method for monitoring ventilation rates of
Atlantic salmon (Salmo salar), which was specifically designed for videos
recorded in the production environment of commercial sea fish farms using the
existing infrastructure. Our approach uses a fish head detection model, which
classifies the mouth state as either open or closed using a convolutional
neural network. This is followed with multiple object tracking to create
temporal sequences of fish swimming across the field of view of the underwater
video camera to estimate ventilation rates. The method demonstrated high
efficiency, achieving a Pearson correlation coefficient of 0.82 between ground
truth and predicted ventilation rates in a test set of 100 fish collected
independently of the training data. By accurately identifying pens where fish
exhibit signs of respiratory distress, our method offers broad applicability
and the potential to transform fish health and welfare monitoring in finfish
aquaculture.

</details>

### [346] [The ATLAS of Traffic Lights: A Reliable Perception Framework for Autonomous Driving](https://arxiv.org/abs/2504.19722)
*Rupert Polley,Nikolai Polley,Dominik Heid,Marc Heinrich,Sven Ochs,J. Marius Zöllner*

Main category: cs.CV

TLDR: 提出了一种模块化的交通灯感知框架，结合了先进的检测模型和实时关联决策框架，并发布了ATLAS数据集以提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有公共数据集在交通灯状态和图标标注上的不足，提升自动驾驶车辆在复杂城市环境中的导航安全性。

Method: 集成先进的检测模型与实时关联决策框架，并使用ATLAS数据集进行训练和评估。

Result: 在ATLAS数据集上显著提升了交通灯检测的准确性和鲁棒性，并在实际自动驾驶场景中验证了框架的可靠性。

Conclusion: 提出的框架和数据集有效提升了交通灯感知性能，适用于实时自动驾驶应用。

Abstract: Traffic light perception is an essential component of the camera-based
perception system for autonomous vehicles, enabling accurate detection and
interpretation of traffic lights to ensure safe navigation through complex
urban environments. In this work, we propose a modularized perception framework
that integrates state-of-the-art detection models with a novel real-time
association and decision framework, enabling seamless deployment into an
autonomous driving stack. To address the limitations of existing public
datasets, we introduce the ATLAS dataset, which provides comprehensive
annotations of traffic light states and pictograms across diverse environmental
conditions and camera setups. This dataset is publicly available at
https://url.fzi.de/ATLAS. We train and evaluate several state-of-the-art
traffic light detection architectures on ATLAS, demonstrating significant
performance improvements in both accuracy and robustness. Finally, we evaluate
the framework in real-world scenarios by deploying it in an autonomous vehicle
to make decisions at traffic light-controlled intersections, highlighting its
reliability and effectiveness for real-time operation.

</details>

### [347] [RepText: Rendering Visual Text via Replicating](https://arxiv.org/abs/2504.19724)
*Haofan Wang,Yujia Xu,Yimeng Li,Junchen Li,Chaowei Zhang,Jing Wang,Kejia Yang,Zhibo Chen*

Main category: cs.CV

TLDR: RepText通过改进预训练的单语言文本到图像生成模型，使其能够准确渲染多语言视觉文本，而无需真正理解文本内容。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型在非拉丁字母的精确和灵活渲染上表现不足，RepText旨在解决这一问题。

Method: 结合ControlNet设置，引入语言无关的字形和位置信息，使用文本感知损失和扩散损失，并通过噪声字形潜在初始化及区域掩码优化渲染过程。

Result: RepText在实验中表现优于现有开源方法，并与闭源多语言模型效果相当。

Conclusion: RepText有效提升了多语言文本渲染能力，但仍存在局限性，需进一步探讨。

Abstract: Although contemporary text-to-image generation models have achieved
remarkable breakthroughs in producing visually appealing images, their capacity
to generate precise and flexible typographic elements, especially non-Latin
alphabets, remains constrained. To address these limitations, we start from an
naive assumption that text understanding is only a sufficient condition for
text rendering, but not a necessary condition. Based on this, we present
RepText, which aims to empower pre-trained monolingual text-to-image generation
models with the ability to accurately render, or more precisely, replicate,
multilingual visual text in user-specified fonts, without the need to really
understand them. Specifically, we adopt the setting from ControlNet and
additionally integrate language agnostic glyph and position of rendered text to
enable generating harmonized visual text, allowing users to customize text
content, font and position on their needs. To improve accuracy, a text
perceptual loss is employed along with the diffusion loss. Furthermore, to
stabilize rendering process, at the inference phase, we directly initialize
with noisy glyph latent instead of random initialization, and adopt region
masks to restrict the feature injection to only the text region to avoid
distortion of the background. We conducted extensive experiments to verify the
effectiveness of our RepText relative to existing works, our approach
outperforms existing open-source methods and achieves comparable results to
native multi-language closed-source models. To be more fair, we also
exhaustively discuss its limitations in the end.

</details>

### [348] [Measuring Train Driver Performance as Key to Approval of Driverless Trains](https://arxiv.org/abs/2504.19735)
*Rustam Tagiew,Prasannavenkatesh Balaji*

Main category: cs.CV

TLDR: 论文总结了计算机视觉系统在无人驾驶列车安全批准中的简化方法，并提供了新的公开数据集以解决障碍物检测性能量化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏公开的测量结果，障碍物检测性能难以量化，论文旨在填补这一空白。

Method: 通过提供包含711次列车驾驶员性能测量的新数据集，涵盖不同速度、障碍物大小、列车保护系统和颜色对比。

Result: 数据集包括反应时间和障碍物距离的测量值，为研究、标准化和监管提供支持。

Conclusion: 论文提供了无偏且详尽的数据集描述，有助于推动相关研究和法规制定。

Abstract: Points 2.1.4(b), 2.4.2(b) and 2.4.3(b) in Annex I of Implementing Regulation
(EU) No. 402/2013 allow a simplified approach for the safety approval of
computer vision systems for driverless trains, if they have 'similar' functions
and interfaces as the replaced human driver. The human driver is not replaced
one-to-one by a technical system - only a limited set of cognitive functions
are replaced. However, performance in the most challenging function, obstacle
detection, is difficult to quantify due to the deficiency of published
measurement results. This article summarizes the data published so far. This
article also goes a long way to remedy this situation by providing a new public
and anonymized dataset of 711 train driver performance measurements from
controlled experiments. The measurements are made for different speeds,
obstacle sizes, train protection systems and obstacle color contrasts
respectively. The measured values are reaction time and distance to the
obstacle. The goal of this paper is an unbiased and exhaustive description of
the presented dataset for research, standardization and regulation. Further
project related information including the dataset and source code is available
at https://atosense-02371c.usercontent.opencode.de/

</details>

### [349] [CoDEx: Combining Domain Expertise for Spatial Generalization in Satellite Image Analysis](https://arxiv.org/abs/2504.19737)
*Abhishek Kuriyal,Elliot Vincent,Mathieu Aubry,Loic Landrieu*

Main category: cs.CV

TLDR: 提出了一种新的卫星图像领域泛化框架，通过为每个训练域训练一个专家模型，并学习专家相似性，提升模型在测试时的性能。


<details>
  <summary>Details</summary>
Motivation: 解决卫星图像分析中因地形外观差异导致的模型泛化性能差的问题。

Method: 为每个训练域训练一个专家模型，学习专家相似性并鼓励相似专家保持一致，通过模型选择模块选择最合适的专家并聚合预测。

Result: 在四个数据集（DynamicEarthNet、MUDS、OSCD、FMoW）上表现优于现有领域泛化和适应方法。

Conclusion: 提出的框架显著提升了卫星图像分析的泛化性能，代码已开源。

Abstract: Global variations in terrain appearance raise a major challenge for satellite
image analysis, leading to poor model performance when training on locations
that differ from those encountered at test time. This remains true even with
recent large global datasets. To address this challenge, we propose a novel
domain-generalization framework for satellite images. Instead of trying to
learn a single generalizable model, we train one expert model per training
domain, while learning experts' similarity and encouraging similar experts to
be consistent. A model selection module then identifies the most suitable
experts for a given test sample and aggregates their predictions. Experiments
on four datasets (DynamicEarthNet, MUDS, OSCD, and FMoW) demonstrate consistent
gains over existing domain generalization and adaptation methods. Our code is
publicly available at https://github.com/Abhishek19009/CoDEx.

</details>

### [350] [Contrastive Language-Image Learning with Augmented Textual Prompts for 3D/4D FER Using Vision-Language Model](https://arxiv.org/abs/2504.19739)
*Muzammil Behzad,Guoying Zhao*

Main category: cs.CV

TLDR: AffectVLM是一个视觉语言模型，用于从3D/4D数据中多视角理解面部情绪，结合联合表示学习和梯度友好损失函数，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 通过多视角和语义丰富的视觉语言模型，更全面地理解面部情绪。

Method: 提出联合表示学习框架、梯度友好损失函数、增强文本提示和混合视角增强。

Result: 在多个基准测试中表现出色。

Conclusion: AffectVLM在面部情绪理解任务中具有优越性能。

Abstract: In this paper, we introduce AffectVLM, a vision-language model designed to
integrate multiviews for a semantically rich and visually comprehensive
understanding of facial emotions from 3D/4D data. To effectively capture visual
features, we propose a joint representation learning framework paired with a
novel gradient-friendly loss function that accelerates model convergence
towards optimal feature representation. Additionally, we introduce augmented
textual prompts to enhance the model's linguistic capabilities and employ mixed
view augmentation to expand the visual dataset. We also develop a Streamlit app
for a real-time interactive inference and enable the model for distributed
learning. Extensive experiments validate the superior performance of AffectVLM
across multiple benchmarks.

</details>

### [351] [EcoWikiRS: Learning Ecological Representation of Satellite Images from Weak Supervision with Species Observations and Wikipedia](https://arxiv.org/abs/2504.19742)
*Valerie Zermatten,Javiera Castillo-Navarro,Pallavi Jain,Devis Tuia,Diego Marcos*

Main category: cs.CV

TLDR: 提出了一种通过遥感图像预测生态属性的方法，结合物种栖息地描述和弱监督学习，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 生态属性（如土地覆盖、气候条件等）对理解生态系统至关重要，但传统方法依赖实地调查，成本高且难以扩展。

Method: 利用EcoWikiRS数据集（包含高分辨率航拍图像、物种观测数据及栖息地文本描述），提出WINCEL损失函数处理弱监督问题。

Result: 在EUNIS定义的生态系统零样本分类任务中表现优异，提升了遥感图像的生态解释能力。

Conclusion: 该方法为生态遥感提供了一种可扩展的解决方案，代码和数据集已开源。

Abstract: The presence of species provides key insights into the ecological properties
of a location such as land cover, climatic conditions or even soil properties.
We propose a method to predict such ecological properties directly from remote
sensing (RS) images by aligning them with species habitat descriptions. We
introduce the EcoWikiRS dataset, consisting of high-resolution aerial images,
the corresponding geolocated species observations, and, for each species, the
textual descriptions of their habitat from Wikipedia. EcoWikiRS offers a
scalable way of supervision for RS vision language models (RS-VLMs) for
ecology. This is a setting with weak and noisy supervision, where, for
instance, some text may describe properties that are specific only to part of
the species' niche or is irrelevant to a specific image. We tackle this by
proposing WINCEL, a weighted version of the InfoNCE loss. We evaluate our model
on the task of ecosystem zero-shot classification by following the habitat
definitions from the European Nature Information System (EUNIS). Our results
show that our approach helps in understanding RS images in a more ecologically
meaningful manner. The code and the dataset are available at
https://github.com/eceo-epfl/EcoWikiRS.

</details>

### [352] [STCOcc: Sparse Spatial-Temporal Cascade Renovation for 3D Occupancy and Scene Flow Prediction](https://arxiv.org/abs/2504.19749)
*Zhimin Liao,Ping Wei,Shuaijia Chen,Haoxuan Wang,Ziyang Ren*

Main category: cs.CV

TLDR: 提出了一种基于显式状态建模的新方法，通过稀疏遮挡感知注意力机制和级联细化策略，改进3D特征，并在占用和场景流预测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有隐式学习方法在捕捉局部细节和空间判别能力上的不足。

Method: 采用显式状态建模，结合稀疏遮挡感知注意力机制和级联细化策略，以及长期动态交互建模方法。

Result: 在RayIoU和mAVE指标上表现优于现有方法，训练时GPU内存使用降至8.7GB。

Conclusion: 显式状态建模方法在性能和效率上均优于隐式学习方法。

Abstract: 3D occupancy and scene flow offer a detailed and dynamic representation of 3D
scene. Recognizing the sparsity and complexity of 3D space, previous
vision-centric methods have employed implicit learning-based approaches to
model spatial and temporal information. However, these approaches struggle to
capture local details and diminish the model's spatial discriminative ability.
To address these challenges, we propose a novel explicit state-based modeling
method designed to leverage the occupied state to renovate the 3D features.
Specifically, we propose a sparse occlusion-aware attention mechanism,
integrated with a cascade refinement strategy, which accurately renovates 3D
features with the guidance of occupied state information. Additionally, we
introduce a novel method for modeling long-term dynamic interactions, which
reduces computational costs and preserves spatial information. Compared to the
previous state-of-the-art methods, our efficient explicit renovation strategy
not only delivers superior performance in terms of RayIoU and mAVE for
occupancy and scene flow prediction but also markedly reduces GPU memory usage
during training, bringing it down to 8.7GB. Our code is available on
https://github.com/lzzzzzm/STCOcc

</details>

### [353] [Hybrid Approach Combining Ultrasound and Blood Test Analysis with a Voting Classifier for Accurate Liver Fibrosis and Cirrhosis Assessment](https://arxiv.org/abs/2504.19755)
*Kapil Kashyap,Sean Fargose,Chrisil Dabre,Fatema Dolaria,Nilesh Patil,Aniket Kore*

Main category: cs.CV

TLDR: 提出了一种结合机器学习和临床数据的混合模型，用于提高肝纤维化和肝硬化的检测准确性，准确率达92.5%。


<details>
  <summary>Details</summary>
Motivation: 传统肝活检诊断方法具有侵入性，不适合常规筛查，因此需要一种非侵入性且准确的替代方法。

Method: 结合血液检测数据和深度学习模型（DenseNet-201）对超声图像进行预测，构建混合模型。

Result: 混合模型的准确率达到92.5%。

Conclusion: 该混合模型能显著提高诊断准确性，支持肝病的早期干预。

Abstract: Liver cirrhosis is an insidious condition involving the substitution of
normal liver tissue with fibrous scar tissue and causing major health
complications. The conventional method of diagnosis using liver biopsy is
invasive and, therefore, inconvenient for use in regular screening. In this
paper,we present a hybrid model that combines machine learning techniques with
clinical data and ultrasoundscans to improve liver fibrosis and cirrhosis
detection accuracy is presented. The model integrates fixed blood test
probabilities with deep learning model predictions (DenseNet-201) for
ultrasonic images. The combined hybrid model achieved an accuracy of 92.5%. The
findings establish the viability of the combined model in enhancing diagnosis
accuracy and supporting early intervention in liver disease care.

</details>

### [354] [Joint Optimization of Neural Radiance Fields and Continuous Camera Motion from a Monocular Video](https://arxiv.org/abs/2504.19819)
*Hoang Chuong Nguyen,Wei Mao,Jose M. Alvarez,Miaomiao Liu*

Main category: cs.CV

TLDR: 提出了一种新方法，通过建模连续相机运动为时间依赖的角速度和速度，消除了对初始相机位姿或深度先验的依赖，实现了更准确的相机位姿和深度估计。


<details>
  <summary>Details</summary>
Motivation: NeRF需要准确的预计算相机位姿，现有方法依赖良好的位姿初始化或深度先验，但在大旋转等挑战性场景中表现不佳。

Method: 通过时间依赖的NeRF建模连续相机运动，首先学习相机间的相对运动，再通过速度积分得到相机位姿。

Result: 在Co3D和Scannet上实现了优于现有方法的相机位姿和深度估计，且新视角合成性能相当。

Conclusion: 该方法通过连续运动建模，显著提升了相机位姿估计的准确性，同时保持了场景几何表示的高质量。

Abstract: Neural Radiance Fields (NeRF) has demonstrated its superior capability to
represent 3D geometry but require accurately precomputed camera poses during
training. To mitigate this requirement, existing methods jointly optimize
camera poses and NeRF often relying on good pose initialisation or depth
priors. However, these approaches struggle in challenging scenarios, such as
large rotations, as they map each camera to a world coordinate system. We
propose a novel method that eliminates prior dependencies by modeling
continuous camera motions as time-dependent angular velocity and velocity.
Relative motions between cameras are learned first via velocity integration,
while camera poses can be obtained by aggregating such relative motions up to a
world coordinate system defined at a single time step within the video.
Specifically, accurate continuous camera movements are learned through a
time-dependent NeRF, which captures local scene geometry and motion by training
from neighboring frames for each time step. The learned motions enable
fine-tuning the NeRF to represent the full scene geometry. Experiments on Co3D
and Scannet show our approach achieves superior camera pose and depth
estimation and comparable novel-view synthesis performance compared to
state-of-the-art methods. Our code is available at
https://github.com/HoangChuongNguyen/cope-nerf.

</details>

### [355] [Taming the Randomness: Towards Label-Preserving Cropping in Contrastive Learning](https://arxiv.org/abs/2504.19824)
*Mohamed Hassan,Mohammad Wasil,Sebastian Houben*

Main category: cs.CV

TLDR: 对比学习（CL）通过改进随机裁剪方法，提升了自监督学习在图像分类任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 随机裁剪可能导致语义偏离原始图像，从而产生错误标签，影响模型效果。

Method: 提出了两种参数化裁剪方法，增强自标签的鲁棒性。

Result: 新方法在CIFAR-10分类任务中，准确率提升2.7%至12.4%。

Conclusion: 参数化裁剪方法显著提高了对比学习的有效性。

Abstract: Contrastive learning (CL) approaches have gained great recognition as a very
successful subset of self-supervised learning (SSL) methods. SSL enables
learning from unlabeled data, a crucial step in the advancement of deep
learning, particularly in computer vision (CV), given the plethora of unlabeled
image data. CL works by comparing different random augmentations (e.g.,
different crops) of the same image, thus achieving self-labeling. Nevertheless,
randomly augmenting images and especially random cropping can result in an
image that is semantically very distant from the original and therefore leads
to false labeling, hence undermining the efficacy of the methods. In this
research, two novel parameterized cropping methods are introduced that increase
the robustness of self-labeling and consequently increase the efficacy. The
results show that the use of these methods significantly improves the accuracy
of the model by between 2.7\% and 12.4\% on the downstream task of classifying
CIFAR-10, depending on the crop size compared to that of the non-parameterized
random cropping method.

</details>

### [356] [HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination](https://arxiv.org/abs/2504.19828)
*Zhiming Hu,Daniel Haeufle,Syn Schmitt,Andreas Bulling*

Main category: cs.CV

TLDR: HOIGaze提出了一种基于学习的新方法，用于在扩展现实（XR）中手-物交互（HOI）时的视线估计，通过利用眼、手和头部运动的协调性来优化训练数据。


<details>
  <summary>Details</summary>
Motivation: 传统视线估计方法将所有训练样本视为同等重要，而HOIGaze通过利用眼、手和头部的协调性，识别出对训练最有用的样本，从而有效去噪训练数据。

Method: 1) 提出分层框架，先识别视觉关注的手，再基于该手估计视线方向；2) 使用跨模态Transformer融合头部和手-物特征；3) 引入眼-头部协调损失函数优化训练样本。

Result: 在HOT3D和ADT数据集上显著优于现有方法，平均角度误差分别降低15.6%和6.0%。

Conclusion: 眼-手-头部协调性蕴含丰富信息，为基于学习的视线估计开辟了新方向。

Abstract: We present HOIGaze - a novel learning-based approach for gaze estimation
during hand-object interactions (HOI) in extended reality (XR). HOIGaze
addresses the challenging HOI setting by building on one key insight: The eye,
hand, and head movements are closely coordinated during HOIs and this
coordination can be exploited to identify samples that are most useful for gaze
estimator training - as such, effectively denoising the training data. This
denoising approach is in stark contrast to previous gaze estimation methods
that treated all training samples as equal. Specifically, we propose: 1) a
novel hierarchical framework that first recognises the hand currently visually
attended to and then estimates gaze direction based on the attended hand; 2) a
new gaze estimator that uses cross-modal Transformers to fuse head and
hand-object features extracted using a convolutional neural network and a
spatio-temporal graph convolutional network; and 3) a novel eye-head
coordination loss that upgrades training samples belonging to the coordinated
eye-head movements. We evaluate HOIGaze on the HOT3D and Aria digital twin
(ADT) datasets and show that it significantly outperforms state-of-the-art
methods, achieving an average improvement of 15.6% on HOT3D and 6.0% on ADT in
mean angular error. To demonstrate the potential of our method, we further
report significant performance improvements for the sample downstream task of
eye-based activity recognition on ADT. Taken together, our results underline
the significant information content available in eye-hand-head coordination
and, as such, open up an exciting new direction for learning-based gaze
estimation.

</details>

### [357] [AnimateAnywhere: Rouse the Background in Human Image Animation](https://arxiv.org/abs/2504.19834)
*Xiaoyu Liu,Mingshuai Yao,Yabo Zhang,Xianhui Lin,Peiran Ren,Xiaoming Li,Ming Liu,Wangmeng Zuo*

Main category: cs.CV

TLDR: AnimateAnywhere框架通过背景运动学习器（BML）从人体姿态序列中学习背景运动，无需相机轨迹，生成生动逼真的人体动画。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视背景生成，导致静态或不协调的结果；相机轨迹准备不实用。

Method: 引入BML学习背景运动，结合极线约束优化3D注意力图。

Result: 实验证明AnimateAnywhere能有效学习背景运动，生成逼真动画。

Conclusion: AnimateAnywhere无需相机轨迹，实现高质量人体动画生成。

Abstract: Human image animation aims to generate human videos of given characters and
backgrounds that adhere to the desired pose sequence. However, existing methods
focus more on human actions while neglecting the generation of background,
which typically leads to static results or inharmonious movements. The
community has explored camera pose-guided animation tasks, yet preparing the
camera trajectory is impractical for most entertainment applications and
ordinary users. As a remedy, we present an AnimateAnywhere framework, rousing
the background in human image animation without requirements on camera
trajectories. In particular, based on our key insight that the movement of the
human body often reflects the motion of the background, we introduce a
background motion learner (BML) to learn background motions from human pose
sequences. To encourage the model to learn more accurate cross-frame
correspondences, we further deploy an epipolar constraint on the 3D attention
map. Specifically, the mask used to suppress geometrically unreasonable
attention is carefully constructed by combining an epipolar mask and the
current 3D attention map. Extensive experiments demonstrate that our
AnimateAnywhere effectively learns the background motion from human pose
sequences, achieving state-of-the-art performance in generating human animation
results with vivid and realistic backgrounds. The source code and model will be
available at https://github.com/liuxiaoyu1104/AnimateAnywhere.

</details>

### [358] [SRMF: A Data Augmentation and Multimodal Fusion Approach for Long-Tail UHR Satellite Image Segmentation](https://arxiv.org/abs/2504.19839)
*Yulong Guo,Zilun Zhang,Yongheng Shang,Tiancheng Zhao,Shuiguang Deng,Yingchun Yang,Jianwei Yin*

Main category: cs.CV

TLDR: 论文提出SRMF框架，通过数据增强和多模态特征融合解决UHR卫星图像语义分割中的长尾问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: UHR卫星图像语义分割中长尾问题被忽视，现有方法多关注多尺度特征提取而忽略数据分布不均。

Method: 结合多尺度裁剪技术和基于语义重排序与重采样的数据增强策略，并首次提出无需区域文本描述的多模态特征融合方法。

Result: 在URUR、GID和FBP数据集上mIoU分别提升3.33%、0.66%和0.98%，达到SOTA。

Conclusion: SRMF框架有效缓解长尾问题，为UHR语义分割提供新思路。

Abstract: The long-tail problem presents a significant challenge to the advancement of
semantic segmentation in ultra-high-resolution (UHR) satellite imagery. While
previous efforts in UHR semantic segmentation have largely focused on
multi-branch network architectures that emphasize multi-scale feature
extraction and fusion, they have often overlooked the importance of addressing
the long-tail issue. In contrast to prior UHR methods that focused on
independent feature extraction, we emphasize data augmentation and multimodal
feature fusion to alleviate the long-tail problem. In this paper, we introduce
SRMF, a novel framework for semantic segmentation in UHR satellite imagery. Our
approach addresses the long-tail class distribution by incorporating a
multi-scale cropping technique alongside a data augmentation strategy based on
semantic reordering and resampling. To further enhance model performance, we
propose a multimodal fusion-based general representation knowledge injection
method, which, for the first time, fuses text and visual features without the
need for individual region text descriptions, extracting more robust features.
Extensive experiments on the URUR, GID, and FBP datasets demonstrate that our
method improves mIoU by 3.33\%, 0.66\%, and 0.98\%, respectively, achieving
state-of-the-art performance. Code is available at:
https://github.com/BinSpa/SRMF.git.

</details>

### [359] [Foundation Model-Driven Framework for Human-Object Interaction Prediction with Segmentation Mask Integration](https://arxiv.org/abs/2504.19847)
*Juhan Park,Kyungjae Lee,Hyung Jin Chang,Jungchan Cho*

Main category: cs.CV

TLDR: Seg2HOI是一种新框架，结合分割基础模型与HOI任务，通过引入四元组（包括分割掩码）提升HOI检测，并在零样本场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统HOI方法基于检测，缺乏分割信息的整合，Seg2HOI旨在通过结合分割基础模型提升HOI任务的灵活性和性能。

Method: Seg2HOI继承视觉基础模型的可提示和交互机制，并设计解码器将其应用于HOI任务，无需额外训练。

Result: 在两个公开数据集上，Seg2HOI性能与最先进方法相当，且支持零样本场景和未训练文本/视觉提示的生成。

Conclusion: Seg2HOI通过整合分割信息，提升了HOI任务的灵活性和应用范围，展现了基础模型在HOI中的潜力。

Abstract: In this work, we introduce Segmentation to Human-Object Interaction
(\textit{\textbf{Seg2HOI}}) approach, a novel framework that integrates
segmentation-based vision foundation models with the human-object interaction
task, distinguished from traditional detection-based Human-Object Interaction
(HOI) methods. Our approach enhances HOI detection by not only predicting the
standard triplets but also introducing quadruplets, which extend HOI triplets
by including segmentation masks for human-object pairs. More specifically,
Seg2HOI inherits the properties of the vision foundation model (e.g.,
promptable and interactive mechanisms) and incorporates a decoder that applies
these attributes to HOI task. Despite training only for HOI, without additional
training mechanisms for these properties, the framework demonstrates that such
features still operate efficiently. Extensive experiments on two public
benchmark datasets demonstrate that Seg2HOI achieves performance comparable to
state-of-the-art methods, even in zero-shot scenarios. Lastly, we propose that
Seg2HOI can generate HOI quadruplets and interactive HOI segmentation from
novel text and visual prompts that were not used during training, making it
versatile for a wide range of applications by leveraging this flexibility.

</details>

### [360] [CoherenDream: Boosting Holistic Text Coherence in 3D Generation via Multimodal Large Language Models Feedback](https://arxiv.org/abs/2504.19860)
*Chenhan Jiang,Yihan Zeng,Hang Xu,Dit-Yan Yeung*

Main category: cs.CV

TLDR: 论文提出了一种新的SDS目标（TCSD），通过整合多模态大语言模型（MLLMs）的反馈来改善文本-3D对齐问题，并开发了专用模型3DLLaVA-CRITIC和LLM布局初始化方法，显著提升了3D生成的语义一致性和优化效率。


<details>
  <summary>Details</summary>
Motivation: 现有SDS方法在多对象复杂交互场景中难以保持语义忠实度，且优化过程中存在视图无关偏差累积问题，导致文本-3D对齐退化。

Method: 提出TCSD目标，利用MLLMs的跨模态理解能力评估和引导文本-3D对齐；开发3DLLaVA-CRITIC模型用于多视角文本对齐评估；引入LLM布局初始化加速优化收敛。

Result: CoherenDream框架在T$^3$Bench和TIFA子集等基准测试中达到最先进性能，显著提升了文本一致性和语义交互表现。

Conclusion: TCSD通过结合MLLMs解决了SDS的文本对齐问题，CoherenDream展示了在复杂场景中的优越性能，为3D生成任务提供了新思路。

Abstract: Score Distillation Sampling (SDS) has achieved remarkable success in
text-to-3D content generation. However, SDS-based methods struggle to maintain
semantic fidelity for user prompts, particularly when involving multiple
objects with intricate interactions. While existing approaches often address 3D
consistency through multiview diffusion model fine-tuning on 3D datasets, this
strategy inadvertently exacerbates text-3D alignment degradation. The
limitation stems from SDS's inherent accumulation of view-independent biases
during optimization, which progressively diverges from the ideal text alignment
direction. To alleviate this limitation, we propose a novel SDS objective,
dubbed as Textual Coherent Score Distillation (TCSD), which integrates
alignment feedback from multimodal large language models (MLLMs). Our TCSD
leverages cross-modal understanding capabilities of MLLMs to assess and guide
the text-3D correspondence during the optimization. We further develop
3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text
alignment in 3D generations. Additionally, we introduce an LLM-layout
initialization that significantly accelerates optimization convergence through
semantic-aware spatial configuration. Comprehensive evaluations demonstrate
that our framework, CoherenDream, establishes state-of-the-art performance in
text-aligned 3D generation across multiple benchmarks, including T$^3$Bench and
TIFA subset. Qualitative results showcase the superior performance of
CoherenDream in preserving textual consistency and semantic interactions. As
the first study to incorporate MLLMs into SDS optimization, we also conduct
extensive ablation studies to explore optimal MLLM adaptations for 3D
generation tasks.

</details>

### [361] [Towards Ball Spin and Trajectory Analysis in Table Tennis Broadcast Videos via Physically Grounded Synthetic-to-Real Transfer](https://arxiv.org/abs/2504.19863)
*Daniel Kienzle,Robin Schön,Rainer Lienhart,Shin'Ichi Satoh*

Main category: cs.CV

TLDR: 通过视频中的2D轨迹推断乒乓球的初始旋转和3D轨迹，无需真实数据训练，仅用合成数据即可实现泛化。


<details>
  <summary>Details</summary>
Motivation: 分析乒乓球运动员技术需要球的3D轨迹和旋转信息，但旋转在标准广播视频中不可直接观测。

Method: 提出一种从视频2D轨迹推断初始旋转和3D轨迹的新方法，仅用合成数据训练神经网络，通过物理正确的合成数据和针对性增强实现泛化。

Result: 在单目广播视频中首次实现旋转和轨迹预测，旋转分类准确率92.0%，2D重投影误差为图像对角线的0.19%。

Conclusion: 无需真实数据训练，仅通过合成数据和简单技术即可实现泛化，为乒乓球技术分析提供新工具。

Abstract: Analyzing a player's technique in table tennis requires knowledge of the
ball's 3D trajectory and spin. While, the spin is not directly observable in
standard broadcasting videos, we show that it can be inferred from the ball's
trajectory in the video. We present a novel method to infer the initial spin
and 3D trajectory from the corresponding 2D trajectory in a video. Without
ground truth labels for broadcast videos, we train a neural network solely on
synthetic data. Due to the choice of our input data representation, physically
correct synthetic training data, and using targeted augmentations, the network
naturally generalizes to real data. Notably, these simple techniques are
sufficient to achieve generalization. No real data at all is required for
training. To the best of our knowledge, we are the first to present a method
for spin and trajectory prediction in simple monocular broadcast videos,
achieving an accuracy of 92.0% in spin classification and a 2D reprojection
error of 0.19% of the image diagonal.

</details>

### [362] [Using Fixed and Mobile Eye Tracking to Understand How Visitors View Art in a Museum: A Study at the Bowes Museum, County Durham, UK](https://arxiv.org/abs/2504.19881)
*Claire Warwick,Andrew Beresford,Soazig Casteau,Hubert P. H. Shum,Dan Smith,Francis Xiatian Zhang*

Main category: cs.CV

TLDR: 研究团队通过固定和移动眼动追踪技术，分析博物馆访客如何观看艺术品，旨在优化展览设计以提升访客参与度。


<details>
  <summary>Details</summary>
Motivation: 了解博物馆访客在实体画廊中如何观看艺术品，以优化展览设计并提升访客的参与度。

Method: 采用固定和移动眼动追踪技术，结合跨学科团队（数字人文、心理学、艺术史、计算机科学）与博物馆专业人士的合作。

Result: 研究结果为博物馆如何更有效地展示藏品提供了建议。

Conclusion: 通过眼动追踪技术，研究为博物馆展览设计提供了优化方向，以增强访客的参与体验。

Abstract: The following paper describes a collaborative project involving researchers
at Durham University, and professionals at the Bowes Museum, Barnard Castle,
County Durham, UK, during which we used fixed and mobile eye tracking to
understand how visitors view art. Our study took place during summer 2024 and
builds on work presented at DH2017 (Bailey-Ross et al., 2017). Our
interdisciplinary team included researchers from digital humanities,
psychology, art history and computer science, working in collaboration with
professionals from the museum. We used fixed and mobile eye tracking to
understand how museum visitors view art in a physical gallery setting. This
research will enable us to make recommendations about how the Museum's
collections could be more effectively displayed, encouraging visitors to engage
with them more fully.

</details>

### [363] [Federated Out-of-Distribution Generalization: A Causal Augmentation View](https://arxiv.org/abs/2504.19882)
*Runhui Zhang,Sijin Zhou,Zhuang Qi*

Main category: cs.CV

TLDR: 该论文提出了一种名为FedCAug的联邦因果增强方法，通过因果数据增强打破属性与类别间的虚假关联，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在数据偏差和上下文信息利用上存在不足，FedCAug旨在解决这些问题。

Method: 设计因果区域定位模块和因果数据增强模块，生成反事实样本，增强数据多样性。

Result: 实验表明，FedCAug显著减少模型对背景的依赖，性能优于现有方法。

Conclusion: FedCAug有效提升联邦学习性能，同时保护数据隐私。

Abstract: Federated learning aims to collaboratively model by integrating multi-source
information to obtain a model that can generalize across all client data.
Existing methods often leverage knowledge distillation or data augmentation to
mitigate the negative impact of data bias across clients. However, the limited
performance of teacher models on out-of-distribution samples and the inherent
quality gap between augmented and original data hinder their effectiveness and
they typically fail to leverage the advantages of incorporating rich contextual
information. To address these limitations, this paper proposes a Federated
Causal Augmentation method, termed FedCAug, which employs causality-inspired
data augmentation to break the spurious correlation between attributes and
categories. Specifically, it designs a causal region localization module to
accurately identify and decouple the background and objects in the image,
providing rich contextual information for causal data augmentation.
Additionally, it designs a causality-inspired data augmentation module that
integrates causal features and within-client context to generate counterfactual
samples. This significantly enhances data diversity, and the entire process
does not require any information sharing between clients, thereby contributing
to the protection of data privacy. Extensive experiments conducted on three
datasets reveal that FedCAug markedly reduces the model's reliance on
background to predict sample labels, achieving superior performance compared to
state-of-the-art methods.

</details>

### [364] [Enhancing breast cancer detection on screening mammogram using self-supervised learning and a hybrid deep model of Swin Transformer and Convolutional Neural Network](https://arxiv.org/abs/2504.19888)
*Han Chen,Anne L. Martel*

Main category: cs.CV

TLDR: 提出了一种结合自监督学习和深度混合模型（HybMNet）的新方法，用于提高乳腺钼靶筛查中乳腺癌的检测效果。


<details>
  <summary>Details</summary>
Motivation: 高质量标注医学数据的稀缺性是AI应用于乳腺癌诊断的主要限制之一，现有方法需要大量标注数据，成本高昂且耗时。

Method: 采用两阶段学习：1）自监督预训练（EsViT技术预训练Swin-T）；2）下游任务训练（HybMNet结合Swin-T和CNN，通过融合模块整合全局与局部信息）。

Result: 在CMMD和INbreast数据集上分别达到AUC 0.864和0.889，验证了方法的有效性。

Conclusion: 该方法通过自监督学习和混合模型显著提升了乳腺癌检测性能，减少了标注数据需求。

Abstract: Purpose: The scarcity of high-quality curated labeled medical training data
remains one of the major limitations in applying artificial intelligence (AI)
systems to breast cancer diagnosis. Deep models for mammogram analysis and mass
(or micro-calcification) detection require training with a large volume of
labeled images, which are often expensive and time-consuming to collect. To
reduce this challenge, we proposed a novel method that leverages
self-supervised learning (SSL) and a deep hybrid model, named \textbf{HybMNet},
which combines local self-attention and fine-grained feature extraction to
enhance breast cancer detection on screening mammograms.
  Approach: Our method employs a two-stage learning process: (1) SSL
Pretraining: We utilize EsViT, a SSL technique, to pretrain a Swin Transformer
(Swin-T) using a limited set of mammograms. The pretrained Swin-T then serves
as the backbone for the downstream task. (2) Downstream Training: The proposed
HybMNet combines the Swin-T backbone with a CNN-based network and a novel
fusion strategy. The Swin-T employs local self-attention to identify
informative patch regions from the high-resolution mammogram, while the
CNN-based network extracts fine-grained local features from the selected
patches. A fusion module then integrates global and local information from both
networks to generate robust predictions. The HybMNet is trained end-to-end,
with the loss function combining the outputs of the Swin-T and CNN modules to
optimize feature extraction and classification performance.
  Results: The proposed method was evaluated for its ability to detect breast
cancer by distinguishing between benign (normal) and malignant mammograms.
Leveraging SSL pretraining and the HybMNet model, it achieved AUC of 0.864 (95%
CI: 0.852, 0.875) on the CMMD dataset and 0.889 (95% CI: 0.875, 0.903) on the
INbreast dataset, highlighting its effectiveness.

</details>

### [365] [CineVerse: Consistent Keyframe Synthesis for Cinematic Scene Composition](https://arxiv.org/abs/2504.19894)
*Quynh Phung,Long Mai,Fabian David Caba Heilbron,Feng Liu,Jia-Bin Huang,Cusuh Ham*

Main category: cs.CV

TLDR: CineVerse是一个用于电影场景合成的框架，通过两阶段方法生成连贯且丰富的电影场景。


<details>
  <summary>Details</summary>
Motivation: 解决电影制作中多角色、复杂互动和视觉效果的挑战。

Method: 1. 使用LLM生成详细场景和镜头计划；2. 微调文本到图像模型生成高质量关键帧。

Result: 实验显示CineVerse能生成视觉连贯且内容丰富的电影场景。

Conclusion: CineVerse为电影视频合成提供了新的探索方向。

Abstract: We present CineVerse, a novel framework for the task of cinematic scene
composition. Similar to traditional multi-shot generation, our task emphasizes
the need for consistency and continuity across frames. However, our task also
focuses on addressing challenges inherent to filmmaking, such as multiple
characters, complex interactions, and visual cinematic effects. In order to
learn to generate such content, we first create the CineVerse dataset. We use
this dataset to train our proposed two-stage approach. First, we prompt a large
language model (LLM) with task-specific instructions to take in a high-level
scene description and generate a detailed plan for the overall setting and
characters, as well as the individual shots. Then, we fine-tune a text-to-image
generation model to synthesize high-quality visual keyframes. Experimental
results demonstrate that CineVerse yields promising improvements in generating
visually coherent and contextually rich movie scenes, paving the way for
further exploration in cinematic video synthesis.

</details>

### [366] [Breast Cancer Detection from Multi-View Screening Mammograms with Visual Prompt Tuning](https://arxiv.org/abs/2504.19900)
*Han Chen,Anne L. Martel*

Main category: cs.CV

TLDR: 提出了一种多视图视觉提示调优网络（MVPT-NET），用于高效分析高分辨率乳腺X光片，结合多视图数据提升乳腺癌检测性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌的早期诊断需要高精度检测，多视图数据能提供更全面的信息，但传统方法在处理高分辨率数据时效率不足。

Method: 先预训练单视图分类模型，再通过提示调优技术选择性调整少量参数（7%），高效整合多视图数据。

Result: 在大型多机构数据集上表现优于传统方法，AUROC达0.852，区分良性、DCIS和浸润性三类。

Conclusion: MVPT-NET为医学影像任务提供了高效、可扩展的多视图数据整合方案。

Abstract: Accurate detection of breast cancer from high-resolution mammograms is
crucial for early diagnosis and effective treatment planning. Previous studies
have shown the potential of using single-view mammograms for breast cancer
detection. However, incorporating multi-view data can provide more
comprehensive insights. Multi-view classification, especially in medical
imaging, presents unique challenges, particularly when dealing with
large-scale, high-resolution data. In this work, we propose a novel Multi-view
Visual Prompt Tuning Network (MVPT-NET) for analyzing multiple screening
mammograms. We first pretrain a robust single-view classification model on
high-resolution mammograms and then innovatively adapt multi-view feature
learning into a task-specific prompt tuning process. This technique selectively
tunes a minimal set of trainable parameters (7\%) while retaining the
robustness of the pre-trained single-view model, enabling efficient integration
of multi-view data without the need for aggressive downsampling. Our approach
offers an efficient alternative to traditional feature fusion methods,
providing a more robust, scalable, and efficient solution for high-resolution
mammogram analysis. Experimental results on a large multi-institution dataset
demonstrate that our method outperforms conventional approaches while
maintaining detection efficiency, achieving an AUROC of 0.852 for
distinguishing between Benign, DCIS, and Invasive classes. This work highlights
the potential of MVPT-NET for medical imaging tasks and provides a scalable
solution for integrating multi-view data in breast cancer detection.

</details>

### [367] [Enhancing Surgical Documentation through Multimodal Visual-Temporal Transformers and Generative AI](https://arxiv.org/abs/2504.19918)
*Hugo Georgenthum,Cristian Cosentino,Fabrizio Marozzo,Pietro Liò*

Main category: cs.CV

TLDR: 提出一种多模态框架，结合计算机视觉和大型语言模型，自动生成手术视频摘要。


<details>
  <summary>Details</summary>
Motivation: 提升手术文档记录、支持手术培训并促进术后分析。

Method: 分三阶段：视频分片段提取视觉特征，生成帧级描述，结合时间特征生成片段级摘要，最终汇总为完整报告。

Result: 在CholecT50数据集上表现优异，工具检测精度96%，BERT得分0.74。

Conclusion: 为AI辅助手术报告提供智能可靠的解决方案。

Abstract: The automatic summarization of surgical videos is essential for enhancing
procedural documentation, supporting surgical training, and facilitating
post-operative analysis. This paper presents a novel method at the intersection
of artificial intelligence and medicine, aiming to develop machine learning
models with direct real-world applications in surgical contexts. We propose a
multi-modal framework that leverages recent advancements in computer vision and
large language models to generate comprehensive video summaries. % The approach
is structured in three key stages. First, surgical videos are divided into
clips, and visual features are extracted at the frame level using visual
transformers. This step focuses on detecting tools, tissues, organs, and
surgical actions. Second, the extracted features are transformed into
frame-level captions via large language models. These are then combined with
temporal features, captured using a ViViT-based encoder, to produce clip-level
summaries that reflect the broader context of each video segment. Finally, the
clip-level descriptions are aggregated into a full surgical report using a
dedicated LLM tailored for the summarization task. % We evaluate our method on
the CholecT50 dataset, using instrument and action annotations from 50
laparoscopic videos. The results show strong performance, achieving 96\%
precision in tool detection and a BERT score of 0.74 for temporal context
summarization. This work contributes to the advancement of AI-assisted tools
for surgical reporting, offering a step toward more intelligent and reliable
clinical documentation.

</details>

### [368] [Enhancing Quality for VVC Compressed Videos with Omniscient Quality Enhancement Model](https://arxiv.org/abs/2504.19935)
*Xiem HoangVan,Hieu Bui Minh,Sang NguyenQuang,Wen-Hsiao Peng*

Main category: cs.CV

TLDR: 本文提出了一种名为OVQE-VVC的新型视频质量增强网络，用于提升H.266/VVC压缩视频的感知质量。通过结合时空特征和跨频率信息，该网络在PSNR和比特率节省方面表现显著。


<details>
  <summary>Details</summary>
Motivation: 尽管H.266/VVC在压缩性能上有显著提升，但解码端对更高感知质量的需求以及编码端的压缩性能挑战仍然存在。AI技术，尤其是基于深度学习的视频质量增强方法，为解决这一问题提供了可能。

Method: 提出了一种改进的OVQE模型，并将其集成到最新的STD-VVC解码器架构中。该模型利用时空特征和跨频率信息来增强视频质量。

Result: 实验表明，OVQE-VVC在PSNR上显著提升（约0.74 dB至1.2 dB），同时节省约19.6%的比特率。

Conclusion: OVQE-VVC是一种有效的视频质量增强解决方案，适用于H.266/VVC压缩视频，显著提升了感知质量和压缩效率。

Abstract: The latest video coding standard H.266/VVC has shown its great improvement in
terms of compression performance when compared to its predecessor HEVC
standard. Though VVC was implemented with many advanced techniques, it still
met the same challenges as its predecessor due to the need for even higher
perceptual quality demand at the decoder side as well as the compression
performance at the encoder side. The advancement of Artificial Intelligence
(AI) technology, notably the deep learning-based video quality enhancement
methods, was shown to be a promising approach to improving the perceptual
quality experience. In this paper, we propose a novel Omniscient video quality
enhancement Network for VVC compressed Videos. The Omniscient Network for
compressed video quality enhancement was originally designed for HEVC
compressed videos in which not only the spatial-temporal features but also
cross-frequencies information were employed to augment the visual quality.
Inspired by this work, we propose a modification of the OVQE model and
integrate it into the lasted STD-VVC (Standard Versatile Video Coding) decoder
architecture. As assessed in a rich set of test conditions, the proposed
OVQE-VVC solution is able to achieve significant PSNR improvement, notably
around 0.74 dB and up to 1.2 dB with respect to the original STD-VVC codec.
This also corresponds to around 19.6% of bitrate saving while keeping a similar
quality observation.

</details>

### [369] [Mesh-Learner: Texturing Mesh with Spherical Harmonics](https://arxiv.org/abs/2504.19938)
*Yunfei Wan,Jianheng Liu,Jiarong Lin,Fu Zhang*

Main category: cs.CV

TLDR: Mesh-Learner是一个兼容传统光栅化管道的3D重建与渲染框架，通过网格和球谐纹理学习视图依赖的辐射度，渲染效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在兼容性和渲染效果上的不足，提供一种高效且兼容光栅化管道的解决方案。

Method: 结合网格和球谐纹理，利用新颖的插值方法渲染图像，并通过梯度反向传播优化纹理。

Result: 在Replica和FAST-LIVO2数据集上实现了优于3D高斯泼溅和M2-Mapping的渲染效果。

Conclusion: Mesh-Learner在兼容性和渲染性能上表现出色，适用于多种基于光栅化管道的任务。

Abstract: In this paper, we present a 3D reconstruction and rendering framework termed
Mesh-Learner that is natively compatible with traditional rasterization
pipelines. It integrates mesh and spherical harmonic (SH) texture (i.e.,
texture filled with SH coefficients) into the learning process to learn each
mesh s view-dependent radiance end-to-end. Images are rendered by interpolating
surrounding SH Texels at each pixel s sampling point using a novel
interpolation method. Conversely, gradients from each pixel are back-propagated
to the related SH Texels in SH textures. Mesh-Learner exploits graphic features
of rasterization pipeline (texture sampling, deferred rendering) to render,
which makes Mesh-Learner naturally compatible with tools (e.g., Blender) and
tasks (e.g., 3D reconstruction, scene rendering, reinforcement learning for
robotics) that are based on rasterization pipelines. Our system can train vast,
unlimited scenes because we transfer only the SH textures within the frustum to
the GPU for training. At other times, the SH textures are stored in CPU RAM,
which results in moderate GPU memory usage. The rendering results on
interpolation and extrapolation sequences in the Replica and FAST-LIVO2
datasets achieve state-of-the-art performance compared to existing
state-of-the-art methods (e.g., 3D Gaussian Splatting and M2-Mapping). To
benefit the society, the code will be available at
https://github.com/hku-mars/Mesh-Learner.

</details>

### [370] [Shopformer: Transformer-Based Framework for Detecting Shoplifting via Human Pose](https://arxiv.org/abs/2504.19970)
*Narges Rashvand,Ghazal Alinezhad Noghre,Armin Danesh Pazho,Babak Rahimi Ardabili,Hamed Tabkhi*

Main category: cs.CV

TLDR: Shopformer是一种基于姿态序列的Transformer模型，用于检测商店盗窃行为，解决了传统视频分析的隐私和计算资源问题。


<details>
  <summary>Details</summary>
Motivation: 传统监控系统效率低下且侵犯隐私，现有AI方法依赖视频分析，存在环境敏感和计算资源需求高的问题。

Method: 提出Shopformer，通过姿态序列分析检测盗窃行为，采用自定义标记化策略将姿态序列转换为紧凑嵌入。

Result: 在真实姿态数据上评估，优于现有异常检测模型，提供隐私保护且可扩展的实时监控方案。

Conclusion: Shopformer为零售监控提供了一种高效、隐私友好的解决方案。

Abstract: Shoplifting remains a costly issue for the retail sector, but traditional
surveillance systems, which are mostly based on human monitoring, are still
largely ineffective, with only about 2% of shoplifters being arrested. Existing
AI-based approaches rely on pixel-level video analysis which raises privacy
concerns, is sensitive to environmental variations, and demands significant
computational resources. To address these limitations, we introduce Shopformer,
a novel transformer-based model that detects shoplifting by analyzing pose
sequences rather than raw video. We propose a custom tokenization strategy that
converts pose sequences into compact embeddings for efficient transformer
processing. To the best of our knowledge, this is the first pose-sequence-based
transformer model for shoplifting detection. Evaluated on real-world pose data,
our method outperforms state-of-the-art anomaly detection models, offering a
privacy-preserving, and scalable solution for real-time retail surveillance.
The code base for this work is available at
https://github.com/TeCSAR-UNCC/Shopformer.

</details>

### [371] [Mapping of Weed Management Methods in Orchards using Sentinel-2 and PlanetScope Data](https://arxiv.org/abs/2504.19991)
*Ioannis Kontogiorgakis,Iason Tsardanidis,Dimitrios Bormpoudakis,Ilias Tsoumas,Dimitra A. Loka,Christos Noulas,Alexandros Tsitouras,Charalampos Kontoes*

Main category: cs.CV

TLDR: 利用卫星图像时间序列和机器学习方法，开发了一种高效、准确的果园杂草管理方法映射技术。


<details>
  <summary>Details</summary>
Motivation: 杂草管理对农业生产力至关重要，但传统地面调查成本高且耗时，亟需更高效的监测方法。

Method: 结合Sentinel-2和PlanetScope卫星数据，采用机器学习技术识别四种杂草管理方法（割草、耕作、化学喷洒和无操作）。

Result: 研究表明，机器学习驱动的遥感技术能显著提升果园杂草管理映射的效率和准确性。

Conclusion: 该方法为政策制定者提供了一种低成本、高效的杂草管理监测工具。

Abstract: Effective weed management is crucial for improving agricultural productivity,
as weeds compete with crops for vital resources like nutrients and water.
Accurate maps of weed management methods are essential for policymakers to
assess farmer practices, evaluate impacts on vegetation health, biodiversity,
and climate, as well as ensure compliance with policies and subsidies. However,
monitoring weed management methods is challenging as commonly rely on on-ground
field surveys, which are often costly, time-consuming and subject to delays. In
order to tackle this problem, we leverage Earth Observation (EO) data and
Machine Learning (ML). Specifically, we developed an ML approach for mapping
four distinct weed management methods (Mowing, Tillage, Chemical-spraying, and
No practice) in orchards using satellite image time series (SITS) data from two
different sources: Sentinel-2 (S2) and PlanetScope (PS). The findings
demonstrate the potential of ML-driven remote sensing to enhance the efficiency
and accuracy of weed management mapping in orchards.

</details>

### [372] [Monitoring digestate application on agricultural crops using Sentinel-2 Satellite imagery](https://arxiv.org/abs/2504.19996)
*Andreas Kalogeras,Dimitrios Bormpoudakis,Iason Tsardanidis,Dimitra A. Loka,Charalampos Kontoes*

Main category: cs.CV

TLDR: 研究利用Sentinel-2卫星影像和机器学习模型检测农业中外源有机质（EOM）的应用效果，支持精准农业和可持续发展。


<details>
  <summary>Details</summary>
Motivation: 监测EOM对土壤和作物健康的影响，因其可能带来微塑料污染和氮流失等环境风险。

Method: 使用Sentinel-2卫星影像时间序列分析特定指数（EOMI、NDVI、EVI），并结合多种机器学习模型（随机森林、k-NN、梯度提升和前馈神经网络）检测EOM应用。

Result: 机器学习模型检测EOM的F1分数高达0.85。

Conclusion: 结合遥感和机器学习可实现EOM应用的规模化、低成本监测，助力精准农业和可持续发展。

Abstract: The widespread use of Exogenous Organic Matter in agriculture necessitates
monitoring to assess its effects on soil and crop health. This study evaluates
optical Sentinel-2 satellite imagery for detecting digestate application, a
practice that enhances soil fertility but poses environmental risks like
microplastic contamination and nitrogen losses. In the first instance,
Sentinel-2 satellite image time series (SITS) analysis of specific indices
(EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after
application on the soils of four different crop types in Thessaly, Greece.
Furthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient
Boosting and a Feed-Forward Neural Network), were used to investigate digestate
presence detection, achieving F1-scores up to 0.85. The findings highlight the
potential of combining remote sensing and ML for scalable and cost-effective
monitoring of EOM applications, supporting precision agriculture and
sustainability.

</details>

### [373] [SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning](https://arxiv.org/abs/2504.20024)
*Wufei Ma,Yu-Cheng Chou,Qihao Liu,Xingrui Wang,Celso de Melo,Jieneng Chen,Jianwen Xie,Alan Yuille*

Main category: cs.CV

TLDR: 论文提出SpatialReasoner，一种新型大型视觉语言模型（LVLM），通过显式3D表示改进3D空间推理性能，并在新问题上表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有3D空间推理方法多为隐式，且对未见问题的泛化能力不足。

Method: 采用显式3D表示，结合3D感知、计算和推理阶段。

Result: 在多种空间推理基准上表现更优，对新问题泛化能力更强。

Conclusion: 结合视觉基础模型和大型语言模型，为3D空间推理开辟新方向。

Abstract: Recent studies in 3D spatial reasoning explore data-driven approaches and
achieve enhanced spatial reasoning performance with reinforcement learning
(RL). However, these methods typically perform spatial reasoning in an implicit
manner, and it remains underexplored whether the acquired 3D knowledge
generalizes to unseen question types at any stage of the training. In this work
we introduce SpatialReasoner, a novel large vision-language model (LVLM) that
address 3D spatial reasoning with explicit 3D representations shared between
stages -- 3D perception, computation, and reasoning. Explicit 3D
representations provide a coherent interface that supports advanced 3D spatial
reasoning and enable us to study the factual errors made by LVLMs. Results show
that our SpatialReasoner achieve improved performance on a variety of spatial
reasoning benchmarks and generalizes better when evaluating on novel 3D spatial
reasoning questions. Our study bridges the 3D parsing capabilities of prior
visual foundation models with the powerful reasoning abilities of large
language models, opening new directions for 3D spatial reasoning.

</details>

### [374] [LIRM: Large Inverse Rendering Model for Progressive Reconstruction of Shape, Materials and View-dependent Radiance Fields](https://arxiv.org/abs/2504.20026)
*Zhengqin Li,Dilin Wang,Ka Chen,Zhaoyang Lv,Thu Nguyen-Phuoc,Milim Lee,Jia-Bin Huang,Lei Xiao,Cheng Zhang,Yufeng Zhu,Carl S. Marshall,Yufeng Ren,Richard Newcombe,Zhao Dong*

Main category: cs.CV

TLDR: LIRM是一种基于Transformer的架构，能够在不到一秒的时间内联合重建高质量的形状、材质和辐射场，解决了现有LRMs在重建未见部分和生成可重光照3D内容方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有LRMs在重建未见部分和生成可重光照3D内容方面表现不佳，限制了其在实际应用中的实用性。

Method: 1. 引入更新模型以逐步添加输入视图；2. 提出六面体神经SDF表示以恢复细节；3. 开发神经方向嵌入机制处理视角依赖效果。

Result: 在几何和重光照准确性上优于基于优化的密集视图逆渲染方法，且推理时间大幅缩短。

Conclusion: LIRM通过技术创新实现了高效、高质量的多视图3D重建，具有实际应用潜力。

Abstract: We present Large Inverse Rendering Model (LIRM), a transformer architecture
that jointly reconstructs high-quality shape, materials, and radiance fields
with view-dependent effects in less than a second. Our model builds upon the
recent Large Reconstruction Models (LRMs) that achieve state-of-the-art
sparse-view reconstruction quality. However, existing LRMs struggle to
reconstruct unseen parts accurately and cannot recover glossy appearance or
generate relightable 3D contents that can be consumed by standard Graphics
engines. To address these limitations, we make three key technical
contributions to build a more practical multi-view 3D reconstruction framework.
First, we introduce an update model that allows us to progressively add more
input views to improve our reconstruction. Second, we propose a hexa-plane
neural SDF representation to better recover detailed textures, geometry and
material parameters. Third, we develop a novel neural directional-embedding
mechanism to handle view-dependent effects. Trained on a large-scale shape and
material dataset with a tailored coarse-to-fine training scheme, our model
achieves compelling results. It compares favorably to optimization-based
dense-view inverse rendering methods in terms of geometry and relighting
accuracy, while requiring only a fraction of the inference time.

</details>

### [375] [More Clear, More Flexible, More Precise: A Comprehensive Oriented Object Detection benchmark for UAV](https://arxiv.org/abs/2504.20032)
*Kai Ye,Haidi Tang,Bowen Liu,Pingyang Dai,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TLDR: CODrone是一个面向无人机应用的综合性定向物体检测数据集，解决了现有数据集在泛化性和实用性上的不足，并通过实验验证了其作为新基准的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有无人机定向物体检测数据集泛化性差，无法满足实际飞行场景需求，因此需要开发一个更全面、更贴近现实的数据集。

Method: 提出CODrone数据集，改进现有数据集的四大局限性（低分辨率、有限类别、单视角成像、受限飞行高度），并包含多城市、多光照条件下的标注图像。

Result: 通过22种经典或SOTA方法的实验验证，CODrone在真实场景中表现出色，并揭示了无人机定向物体检测的关键瓶颈与机遇。

Conclusion: CODrone填补了无人机定向物体检测的数据空白，提供了一个泛化能力更强的基准，更贴合实际应用和未来算法发展。

Abstract: Applications of unmanned aerial vehicle (UAV) in logistics, agricultural
automation, urban management, and emergency response are highly dependent on
oriented object detection (OOD) to enhance visual perception. Although existing
datasets for OOD in UAV provide valuable resources, they are often designed for
specific downstream tasks.Consequently, they exhibit limited generalization
performance in real flight scenarios and fail to thoroughly demonstrate
algorithm effectiveness in practical environments. To bridge this critical gap,
we introduce CODrone, a comprehensive oriented object detection dataset for
UAVs that accurately reflects real-world conditions. It also serves as a new
benchmark designed to align with downstream task requirements, ensuring greater
applicability and robustness in UAV-based OOD.Based on application
requirements, we identify four key limitations in current UAV OOD datasets-low
image resolution, limited object categories, single-view imaging, and
restricted flight altitudes-and propose corresponding improvements to enhance
their applicability and robustness.Furthermore, CODrone contains a broad
spectrum of annotated images collected from multiple cities under various
lighting conditions, enhancing the realism of the benchmark. To rigorously
evaluate CODrone as a new benchmark and gain deeper insights into the novel
challenges it presents, we conduct a series of experiments based on 22
classical or SOTA methods.Our evaluation not only assesses the effectiveness of
CODrone in real-world scenarios but also highlights key bottlenecks and
opportunities to advance OOD in UAV applications.Overall, CODrone fills the
data gap in OOD from UAV perspective and provides a benchmark with enhanced
generalization capability, better aligning with practical applications and
future algorithm development.

</details>

### [376] [Mitigating Catastrophic Forgetting in the Incremental Learning of Medical Images](https://arxiv.org/abs/2504.20033)
*Sara Yavari,Jacob Furst*

Main category: cs.CV

TLDR: 本文提出了一种增量学习方法（IL），通过知识蒸馏（KD）提升深度学习模型在T2加权MRI前列腺癌检测中的准确性和效率，利用PI-CAI数据集验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 解决医疗图像分析中数据分散存储且无法直接访问原始数据的问题，提升模型在多任务学习中的表现。

Method: 采用知识蒸馏技术，利用过去任务生成的图像指导后续任务的模型训练。

Result: 模型性能提升且收敛更快，在PI-CAI、OCT、PathMNIST和CIFAR-10数据集上验证了方法的鲁棒性。

Conclusion: 知识蒸馏是医疗图像增量学习中的有效方法，尤其适用于数据分散存储的场景。

Abstract: This paper proposes an Incremental Learning (IL) approach to enhance the
accuracy and efficiency of deep learning models in analyzing T2-weighted (T2w)
MRI medical images prostate cancer detection using the PI-CAI dataset. We used
multiple health centers' artificial intelligence and radiology data, focused on
different tasks that looked at prostate cancer detection using MRI (PI-CAI). We
utilized Knowledge Distillation (KD), as it employs generated images from past
tasks to guide the training of models for subsequent tasks. The approach
yielded improved performance and faster convergence of the models. To
demonstrate the versatility and robustness of our approach, we evaluated it on
the PI-CAI dataset, a diverse set of medical imaging modalities including OCT
and PathMNIST, and the benchmark continual learning dataset CIFAR-10. Our
results indicate that KD can be a promising technique for IL in medical image
analysis in which data is sourced from individual health centers and the
storage of large datasets is not feasible. By using generated images from prior
tasks, our method enables the model to retain and apply previously acquired
knowledge without direct access to the original data.

</details>

### [377] [MP-SfM: Monocular Surface Priors for Robust Structure-from-Motion](https://arxiv.org/abs/2504.20040)
*Zador Pataki,Paul-Edouard Sarlin,Johannes L. Schönberger,Marc Pollefeys*

Main category: cs.CV

TLDR: 论文提出了一种结合单目深度和法线先验的改进SfM方法，显著提升了在极端视角变化和对称场景下的重建性能。


<details>
  <summary>Details</summary>
Motivation: 传统SfM系统在极端视角变化、低重叠或高对称性场景中表现不佳，限制了其广泛应用。

Method: 通过深度神经网络推断单目深度和法线先验，并将其与多视图约束紧密结合。

Result: 新方法在极端条件下显著优于现有方法，并能有效处理对称性问题。

Conclusion: 该方法首次实现了从少量图像中可靠重建复杂室内环境，且对未来单目深度估计的进步具有兼容性。

Abstract: While Structure-from-Motion (SfM) has seen much progress over the years,
state-of-the-art systems are prone to failure when facing extreme viewpoint
changes in low-overlap, low-parallax or high-symmetry scenarios. Because
capturing images that avoid these pitfalls is challenging, this severely limits
the wider use of SfM, especially by non-expert users. We overcome these
limitations by augmenting the classical SfM paradigm with monocular depth and
normal priors inferred by deep neural networks. Thanks to a tight integration
of monocular and multi-view constraints, our approach significantly outperforms
existing ones under extreme viewpoint changes, while maintaining strong
performance in standard conditions. We also show that monocular priors can help
reject faulty associations due to symmetries, which is a long-standing problem
for SfM. This makes our approach the first capable of reliably reconstructing
challenging indoor environments from few images. Through principled uncertainty
propagation, it is robust to errors in the priors, can handle priors inferred
by different models with little tuning, and will thus easily benefit from
future progress in monocular depth and normal estimation. Our code is publicly
available at https://github.com/cvg/mpsfm.

</details>

### [378] [Learning Streaming Video Representation via Multitask Training](https://arxiv.org/abs/2504.20041)
*Yibin Yan,Jilan Xu,Shangzhe Di,Yikun Liu,Yudi Shi,Qirui Chen,Zeqian Li,Yifei Huang,Weidi Xie*

Main category: cs.CV

TLDR: StreamFormer是一种新型的流式视频处理模型，通过结合因果时间注意力和预训练视觉变换器，实现了高效的低延迟视频流处理。


<details>
  <summary>Details</summary>
Motivation: 实时视频流理解在嵌入式AI和自动驾驶等应用中至关重要，但现有方法难以同时满足低延迟、历史信息保留和多任务处理的需求。

Method: StreamFormer结合了因果时间注意力和预训练视觉变换器，通过多任务视觉-语言对齐框架统一训练，同时学习全局语义、时间动态和细粒度空间关系。

Result: 在在线动作检测、视频实例分割和视频问答任务中，StreamFormer表现出色，兼顾效率和性能。

Conclusion: StreamFormer为实时视频流理解提供了一种高效、多功能的解决方案，具有广泛的应用潜力。

Abstract: Understanding continuous video streams plays a fundamental role in real-time
applications including embodied AI and autonomous driving. Unlike offline video
understanding, streaming video understanding requires the ability to process
video streams frame by frame, preserve historical information, and make
low-latency decisions.To address these challenges, our main contributions are
three-fold. (i) We develop a novel streaming video backbone, termed as
StreamFormer, by incorporating causal temporal attention into a pre-trained
vision transformer. This enables efficient streaming video processing while
maintaining image representation capability.(ii) To train StreamFormer, we
propose to unify diverse spatial-temporal video understanding tasks within a
multitask visual-language alignment framework. Hence, StreamFormer learns
global semantics, temporal dynamics, and fine-grained spatial relationships
simultaneously. (iii) We conduct extensive experiments on online action
detection, online video instance segmentation, and video question answering.
StreamFormer achieves competitive results while maintaining efficiency,
demonstrating its potential for real-time applications.

</details>

### [379] [CompleteMe: Reference-based Human Image Completion](https://arxiv.org/abs/2504.20042)
*Yu-Ju Tsai,Brian Price,Qing Liu,Luis Figueroa,Daniil Pakhomov,Zhihong Ding,Scott Cohen,Ming-Hsuan Yang*

Main category: cs.CV

TLDR: CompleteMe提出了一种基于参考的人体图像补全框架，通过双U-Net架构和区域聚焦注意力块（RFA）提升细节保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在不依赖显式参考图像的情况下保留独特细节（如服装图案或配饰），即使是基于参考的图像修复方法也难准确捕捉细粒度细节。

Method: 采用双U-Net架构结合RFA块，引导模型关注参考图像中的相关区域，以捕捉细节并确保语义一致性。

Result: 实验表明，该方法在视觉质量和语义一致性上优于现有技术，并提出了专门用于评估的基准。

Conclusion: CompleteMe显著提升了基于参考的人体图像补全的细节保留和语义一致性。

Abstract: Recent methods for human image completion can reconstruct plausible body
shapes but often fail to preserve unique details, such as specific clothing
patterns or distinctive accessories, without explicit reference images. Even
state-of-the-art reference-based inpainting approaches struggle to accurately
capture and integrate fine-grained details from reference images. To address
this limitation, we propose CompleteMe, a novel reference-based human image
completion framework. CompleteMe employs a dual U-Net architecture combined
with a Region-focused Attention (RFA) Block, which explicitly guides the
model's attention toward relevant regions in reference images. This approach
effectively captures fine details and ensures accurate semantic correspondence,
significantly improving the fidelity and consistency of completed images.
Additionally, we introduce a challenging benchmark specifically designed for
evaluating reference-based human image completion tasks. Extensive experiments
demonstrate that our proposed method achieves superior visual quality and
semantic consistency compared to existing techniques. Project page:
https://liagm.github.io/CompleteMe/

</details>

<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [380] [On Stopping Times of Power-one Sequential Tests: Tight Lower and Upper Bounds](https://arxiv.org/abs/2504.19952)
*Shubhada Agrawal,Aaditya Ramdas*

Main category: math.ST

TLDR: 论文证明了复合零假设与备择假设下序贯检验停止时间的两个下界，并给出了匹配上界的充分条件。


<details>
  <summary>Details</summary>
Motivation: 研究在复合假设检验中停止时间的理论下界，扩展了现有结果的适用范围。

Method: 通过分析KL散度的下界，推导出停止时间的两个下界，并验证匹配上界的条件。

Result: 得到两个下界：1）当第一类错误率趋近于零时，下界为log(1/α)/KL_inf；2）当KL_inf趋近于零时，下界为c KL_inf^{-1} log log KL_inf^{-1}。

Conclusion: 论文在更一般的条件下证明了停止时间的下界，并验证了匹配上界的条件，适用于多种特殊情况。

Abstract: We prove two lower bounds for stopping times of sequential tests between
general composite nulls and alternatives. The first lower bound is for the
setting where the type-1 error level $\alpha$ approaches zero, and equals
$\log(1/\alpha)$ divided by a certain infimum KL divergence, termed
$\operatorname{KL_{inf}}$. The second lower bound applies to the setting where
$\alpha$ is fixed and $\operatorname{KL_{inf}}$ approaches 0 (meaning that the
null and alternative sets are not separated) and equals $c
\operatorname{KL_{inf}}^{-1} \log \log \operatorname{KL_{inf}}^{-1}$ for a
universal constant $c > 0$. We also provide a sufficient condition for matching
the upper bounds and show that this condition is met in several special cases.
Given past work, these upper and lower bounds are unsurprising in their form;
our main contribution is the generality in which they hold, for example, not
requiring reference measures or compactness of the classes.

</details>

<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [381] [Towards Robust Multimodal Physiological Foundation Models: Handling Arbitrary Missing Modalities](https://arxiv.org/abs/2504.19596)
*Xi Fu,Wei-Bang Jiang,Yi Ding,Cuntai Guan*

Main category: eess.SP

TLDR: PhysioOmni是一种用于多模态生理信号分析的基础模型，通过解耦多模态信号和提取通用表示，解决了现有方法在跨数据集泛化和处理缺失模态时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以学习通用表示且无法处理推理时的缺失模态，因此需要一种能够适应多样化和不完整模态组合的模型。

Method: PhysioOmni通过训练解耦的多模态标记器，结合模态不变和模态特定的目标进行掩码信号预训练，并通过原型对齐在下游任务中进行微调。

Result: 在情感识别、睡眠阶段分类、运动预测和脑力负荷检测四个任务中，PhysioOmni实现了最先进的性能，并对缺失模态表现出强鲁棒性。

Conclusion: PhysioOmni是一种高效且鲁棒的多模态生理信号分析基础模型，其代码和模型权重将公开。

Abstract: Multimodal physiological signals, such as EEG, ECG, EOG, and EMG, are crucial
for healthcare and brain-computer interfaces. While existing methods rely on
specialized architectures and dataset-specific fusion strategies, they struggle
to learn universal representations that generalize across datasets and handle
missing modalities at inference time. To address these issues, we propose
PhysioOmni, a foundation model for multimodal physiological signal analysis
that models both homogeneous and heterogeneous features to decouple multimodal
signals and extract generic representations while maintaining compatibility
with arbitrary missing modalities. PhysioOmni trains a decoupled multimodal
tokenizer, enabling masked signal pre-training via modality-invariant and
modality-specific objectives. To ensure adaptability to diverse and incomplete
modality combinations, the pre-trained encoders undergo resilient fine-tuning
with prototype alignment on downstream datasets. Extensive experiments on four
downstream tasks, emotion recognition, sleep stage classification, motor
prediction, and mental workload detection, demonstrate that PhysioOmni achieves
state-of-the-art performance while maintaining strong robustness to missing
modalities. Our code and model weights will be released.

</details>

<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [382] [Approximating Nash Equilibria in General-Sum Games via Meta-Learning](https://arxiv.org/abs/2504.18868)
*David Sychrovský,Christopher Solinas,Revan MacQueen,Kevin Wang,James R. Wright,Nathan R. Sturtevant,Michael Bowling*

Main category: cs.GT

TLDR: 该论文提出了一种利用元学习减少遗憾最小化算法中策略相关性的方法，以更接近纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 纳什均衡在一般和博弈中难以高效求解，而遗憾最小化算法只能收敛到粗相关均衡（CCE），无法直接得到纳什均衡。

Method: 通过元学习减少遗憾最小化算法产生的策略相关性，从而更接近纳什均衡。

Result: 在一般和不完全信息博弈中，该方法显著优于现有遗憾最小化技术，提供了更好的纳什均衡近似。

Conclusion: 元学习可以优化遗憾最小化算法，使其更接近纳什均衡，同时保持收敛到CCE的保证。

Abstract: Nash equilibrium is perhaps the best-known solution concept in game theory.
Such a solution assigns a strategy to each player which offers no incentive to
unilaterally deviate. While a Nash equilibrium is guaranteed to always exist,
the problem of finding one in general-sum games is PPAD-complete, generally
considered intractable. Regret minimization is an efficient framework for
approximating Nash equilibria in two-player zero-sum games. However, in
general-sum games, such algorithms are only guaranteed to converge to a
coarse-correlated equilibrium (CCE), a solution concept where players can
correlate their strategies. In this work, we use meta-learning to minimize the
correlations in strategies produced by a regret minimizer. This encourages the
regret minimizer to find strategies that are closer to a Nash equilibrium. The
meta-learned regret minimizer is still guaranteed to converge to a CCE, but we
give a bound on the distance to Nash equilibrium in terms of our meta-loss. We
evaluate our approach in general-sum imperfect information games. Our
algorithms provide significantly better approximations of Nash equilibria than
state-of-the-art regret minimization techniques.

</details>

### [383] [Meta-Learning in Self-Play Regret Minimization](https://arxiv.org/abs/2504.18917)
*David Sychrovský,Martin Schmid,Michal Šustr,Michael Bowling*

Main category: cs.GT

TLDR: 论文提出了一种基于元学习的自对弈框架，用于加速在相似游戏分布上的遗憾最小化，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实践中玩家常面对相似但不同的游戏分布（如股票交易或子游戏策略优化），现有方法仅针对单一游戏，无法充分利用分布信息。

Method: 扩展离线元学习框架至自对弈场景，通过全局信息整合而非传统局部遗憾分解优化策略选择。

Result: 在标准形式游戏和河流扑克子游戏中，新方法显著优于现有遗憾最小化算法。

Conclusion: 该框架为大规模领域均衡近似提供了更高效的自对弈解决方案。

Abstract: Regret minimization is a general approach to online optimization which plays
a crucial role in many algorithms for approximating Nash equilibria in
two-player zero-sum games. The literature mainly focuses on solving individual
games in isolation. However, in practice, players often encounter a
distribution of similar but distinct games. For example, when trading
correlated assets on the stock market, or when refining the strategy in
subgames of a much larger game. Recently, offline meta-learning was used to
accelerate one-sided equilibrium finding on such distributions. We build upon
this, extending the framework to the more challenging self-play setting, which
is the basis for most state-of-the-art equilibrium approximation algorithms for
domains at scale. When selecting the strategy, our method uniquely integrates
information across all decision states, promoting global communication as
opposed to the traditional local regret decomposition. Empirical evaluation on
normal-form games and river poker subgames shows our meta-learned algorithms
considerably outperform other state-of-the-art regret minimization algorithms.

</details>

<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [384] [Periodic Online Testing for Sparse Systolic Tensor Arrays](https://arxiv.org/abs/2504.18628)
*Christodoulos Peltekis,Chrysostomos Nicopoulos,Giorgos Dimitrakopoulos*

Main category: cs.AR

TLDR: 本文提出了一种在线错误检测技术，用于在稀疏脉动张量阵列计算前检测和定位永久故障，仅需四个测试向量，验证了高效性和低开销。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在安全关键系统中的广泛应用，确保稀疏脉动张量阵列的可靠性变得至关重要。

Method: 利用已加载的权重值和四个测试向量，开发了一种在线错误检测技术，并通过门级网表中的故障注入实验验证其有效性。

Result: 在三种经典卷积神经网络上验证，该方法实现了高故障覆盖率，且性能和面积开销极低。

Conclusion: 该技术为稀疏脉动张量阵列提供了一种高效且低开销的可靠性保障方案。

Abstract: Modern Machine Learning (ML) applications often benefit from structured
sparsity, a technique that efficiently reduces model complexity and simplifies
handling of sparse data in hardware. Sparse systolic tensor arrays -
specifically designed to accelerate these structured-sparse ML models - play a
pivotal role in enabling efficient computations. As ML is increasingly
integrated into safety-critical systems, it is of paramount importance to
ensure the reliability of these systems. This paper introduces an online
error-checking technique capable of detecting and locating permanent faults
within sparse systolic tensor arrays before computation begins. The new
technique relies on merely four test vectors and exploits the weight values
already loaded within the systolic array to comprehensively test the system.
Fault-injection campaigns within the gate-level netlist, while executing three
well-established Convolutional Neural Networks (CNN), validate the efficiency
of the proposed approach, which is shown to achieve very high fault coverage,
while incurring minimal performance and area overheads.

</details>

### [385] [NSFlow: An End-to-End FPGA Framework with Scalable Dataflow Architecture for Neuro-Symbolic AI](https://arxiv.org/abs/2504.19323)
*Hanchen Yang,Zishen Wan,Ritik Raj,Joongun Park,Ziwei Li,Ananda Samajdar,Arijit Raychowdhury,Tushar Krishna*

Main category: cs.AR

TLDR: NSFlow是一个基于FPGA的加速框架，专为Neuro-Symbolic AI（NSAI）设计，解决了现有硬件在NSAI任务中的效率问题。


<details>
  <summary>Details</summary>
Motivation: NSAI任务在现有硬件（如CPU、GPU、TPU）上执行困难，因其异构计算内核、高内存需求和独特访问模式。现有ML加速器无法适应NSAI算法的多样性。

Method: 提出NSFlow框架，包括设计架构生成器（优化数据流架构）和可重构阵列（灵活计算单元、可重组内存、混合精度支持）。

Result: NSFlow在NSAI任务中表现优异，比Jetson TX2快31倍，比GPU快2倍以上，比TPU快8倍，比Xilinx DPU快3倍以上。

Conclusion: NSFlow是首个实现实时通用NSAI算法加速的框架，为下一代认知系统提供了有前景的解决方案。

Abstract: Neuro-Symbolic AI (NSAI) is an emerging paradigm that integrates neural
networks with symbolic reasoning to enhance the transparency, reasoning
capabilities, and data efficiency of AI systems. Recent NSAI systems have
gained traction due to their exceptional performance in reasoning tasks and
human-AI collaborative scenarios. Despite these algorithmic advancements,
executing NSAI tasks on existing hardware (e.g., CPUs, GPUs, TPUs) remains
challenging, due to their heterogeneous computing kernels, high memory
intensity, and unique memory access patterns. Moreover, current NSAI algorithms
exhibit significant variation in operation types and scales, making them
incompatible with existing ML accelerators. These challenges highlight the need
for a versatile and flexible acceleration framework tailored to NSAI workloads.
In this paper, we propose NSFlow, an FPGA-based acceleration framework designed
to achieve high efficiency, scalability, and versatility across NSAI systems.
NSFlow features a design architecture generator that identifies workload data
dependencies and creates optimized dataflow architectures, as well as a
reconfigurable array with flexible compute units, re-organizable memory, and
mixed-precision capabilities. Evaluating across NSAI workloads, NSFlow achieves
31x speedup over Jetson TX2, more than 2x over GPU, 8x speedup over TPU-like
systolic array, and more than 3x over Xilinx DPU. NSFlow also demonstrates
enhanced scalability, with only 4x runtime increase when symbolic workloads
scale by 150x. To the best of our knowledge, NSFlow is the first framework to
enable real-time generalizable NSAI algorithms acceleration, demonstrating a
promising solution for next-generation cognitive systems.

</details>

### [386] [Dynamic Tsetlin Machine Accelerators for On-Chip Training at the Edge using FPGAs](https://arxiv.org/abs/2504.19797)
*Gang Mao,Tousif Rahman,Sidharth Maheshwari,Bob Pattison,Zhuang Shao,Rishad Shafik,Alex Yakovlev*

Main category: cs.AR

TLDR: 论文提出了一种动态Tsetlin机（DTM）训练加速器，作为深度神经网络（DNN）的替代方案，适用于边缘计算任务，具有高效能和低功耗优势。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习应用中数据隐私和安全需求的增加，边缘计算在物联网节点上的训练变得重要，但DNN在边缘部署和训练中存在复杂性和资源限制问题。

Method: DTM采用基于逻辑的片上推理和有限状态自动机驱动学习，支持运行时重新配置，无需重新合成，适用于多变量传感器任务。

Result: 与DNN相比，DTM训练时减少了乘加运算和导数计算，能效提升2.54倍，功耗降低6倍。

Conclusion: DTM是一种高效、低功耗的边缘训练解决方案，适用于资源受限的物联网节点。

Abstract: The increased demand for data privacy and security in machine learning (ML)
applications has put impetus on effective edge training on Internet-of-Things
(IoT) nodes. Edge training aims to leverage speed, energy efficiency and
adaptability within the resource constraints of the nodes. Deploying and
training Deep Neural Networks (DNNs)-based models at the edge, although
accurate, posit significant challenges from the back-propagation algorithm's
complexity, bit precision trade-offs, and heterogeneity of DNN layers. This
paper presents a Dynamic Tsetlin Machine (DTM) training accelerator as an
alternative to DNN implementations. DTM utilizes logic-based on-chip inference
with finite-state automata-driven learning within the same Field Programmable
Gate Array (FPGA) package. Underpinned on the Vanilla and Coalesced Tsetlin
Machine algorithms, the dynamic aspect of the accelerator design allows for a
run-time reconfiguration targeting different datasets, model architectures, and
model sizes without resynthesis. This makes the DTM suitable for targeting
multivariate sensor-based edge tasks. Compared to DNNs, DTM trains with fewer
multiply-accumulates, devoid of derivative computation. It is a data-centric ML
algorithm that learns by aligning Tsetlin automata with input data to form
logical propositions enabling efficient Look-up-Table (LUT) mapping and frugal
Block RAM usage in FPGA training implementations. The proposed accelerator
offers 2.54x more Giga operations per second per Watt (GOP/s per W) and uses 6x
less power than the next-best comparable design.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [387] [REED-VAE: RE-Encode Decode Training for Iterative Image Editing with Diffusion Models](https://arxiv.org/abs/2504.18989)
*Gal Almog,Ariel Shamir,Ohad Fried*

Main category: cs.GR

TLDR: REED-VAE提出了一种新的训练方案，解决了潜在扩散模型在迭代图像编辑中累积噪声和伪影的问题，支持多方法编辑。


<details>
  <summary>Details</summary>
Motivation: 当前潜在扩散模型在迭代编辑同一图像时，因多次像素与潜在空间转换导致噪声和伪影累积，限制了灵活性。

Method: 通过RE-encode decode (REED)训练变分自编码器(VAEs)，保持多次迭代后的图像质量。

Result: REED-VAE支持多种迭代编辑操作，提升了编辑成功率和精确性，适用于文本和掩码编辑框架。

Conclusion: REED-VAE为多方法图像编辑任务提供了新基准，增强了图像的可编辑性。

Abstract: While latent diffusion models achieve impressive image editing results, their
application to iterative editing of the same image is severely restricted. When
trying to apply consecutive edit operations using current models, they
accumulate artifacts and noise due to repeated transitions between pixel and
latent spaces. Some methods have attempted to address this limitation by
performing the entire edit chain within the latent space, sacrificing
flexibility by supporting only a limited, predetermined set of diffusion
editing operations. We present a RE-encode decode (REED) training scheme for
variational autoencoders (VAEs), which promotes image quality preservation even
after many iterations. Our work enables multi-method iterative image editing:
users can perform a variety of iterative edit operations, with each operation
building on the output of the previous one using both diffusion-based
operations and conventional editing techniques. We demonstrate the advantage of
REED-VAE across a range of image editing scenarios, including text-based and
mask-based editing frameworks. In addition, we show how REED-VAE enhances the
overall editability of images, increasing the likelihood of successful and
precise edit operations. We hope that this work will serve as a benchmark for
the newly introduced task of multi-method image editing. Our code and models
will be available at https://github.com/galmog/REED-VAE

</details>

### [388] [TransparentGS: Fast Inverse Rendering of Transparent Objects with Gaussians](https://arxiv.org/abs/2504.18768)
*Letian Huang,Dongwei Ye,Jialin Dan,Chengzhi Tao,Huiwen Liu,Kun Zhou,Bo Ren,Yuanqi Li,Yanwen Guo,Jie Guo*

Main category: cs.GR

TLDR: TransparentGS是一种基于3D高斯泼溅（3D-GS）的快速逆向渲染管线，用于解决透明物体重建中的镜面反射和折射问题。


<details>
  <summary>Details</summary>
Motivation: 当前神经和高斯辐射场方法在透明物体重建中因高频率光线变化的不稳定性和错误过拟合而表现不佳。

Method: 设计了透明高斯基元表示透明物体，采用高斯光场探针（GaussProbe）编码环境光和附近内容，并提出基于深度的迭代探针查询（IterQuery）算法。

Result: 实验表明，TransparentGS在复杂环境中能快速且准确地重建透明物体。

Conclusion: TransparentGS为计算机图形和视觉中的透明物体重建提供了高效解决方案。

Abstract: The emergence of neural and Gaussian-based radiance field methods has led to
considerable advancements in novel view synthesis and 3D object reconstruction.
Nonetheless, specular reflection and refraction continue to pose significant
challenges due to the instability and incorrect overfitting of radiance fields
to high-frequency light variations. Currently, even 3D Gaussian Splatting
(3D-GS), as a powerful and efficient tool, falls short in recovering
transparent objects with nearby contents due to the existence of apparent
secondary ray effects. To address this issue, we propose TransparentGS, a fast
inverse rendering pipeline for transparent objects based on 3D-GS. The main
contributions are three-fold. Firstly, an efficient representation of
transparent objects, transparent Gaussian primitives, is designed to enable
specular refraction through a deferred refraction strategy. Secondly, we
leverage Gaussian light field probes (GaussProbe) to encode both ambient light
and nearby contents in a unified framework. Thirdly, a depth-based iterative
probes query (IterQuery) algorithm is proposed to reduce the parallax errors in
our probe-based framework. Experiments demonstrate the speed and accuracy of
our approach in recovering transparent objects from complex environments, as
well as several applications in computer graphics and vision.

</details>

### [389] [CLR-Wire: Towards Continuous Latent Representations for 3D Curve Wireframe Generation](https://arxiv.org/abs/2504.19174)
*Xueqi Ma,Yilin Liu,Tianlong Gao,Qirui Huang,Hui Huang*

Main category: cs.GR

TLDR: CLR-Wire是一种新型3D曲线框架，通过连续潜在表示统一几何与拓扑，利用注意力驱动的VAE和流匹配模型生成高质量3D线框。


<details>
  <summary>Details</summary>
Motivation: 传统方法将顶点、边和面分离处理，难以统一建模几何与拓扑。CLR-Wire旨在解决这一问题，实现联合学习与生成。

Method: 采用注意力驱动的VAE编码曲线及其拓扑为连续潜在空间，结合流匹配模型从高斯噪声生成完整3D线框。

Result: 实验表明，CLR-Wire在准确性、新颖性和多样性上显著优于现有生成方法。

Conclusion: CLR-Wire为CAD设计、几何重建和3D内容创作提供了高效全面的解决方案。

Abstract: We introduce CLR-Wire, a novel framework for 3D curve-based wireframe
generation that integrates geometry and topology into a unified Continuous
Latent Representation. Unlike conventional methods that decouple vertices,
edges, and faces, CLR-Wire encodes curves as Neural Parametric Curves along
with their topological connectivity into a continuous and fixed-length latent
space using an attention-driven variational autoencoder (VAE). This unified
approach facilitates joint learning and generation of both geometry and
topology. To generate wireframes, we employ a flow matching model to
progressively map Gaussian noise to these latents, which are subsequently
decoded into complete 3D wireframes. Our method provides fine-grained modeling
of complex shapes and irregular topologies, and supports both unconditional
generation and generation conditioned on point cloud or image inputs.
Experimental results demonstrate that, compared with state-of-the-art
generative approaches, our method achieves substantial improvements in
accuracy, novelty, and diversity, offering an efficient and comprehensive
solution for CAD design, geometric reconstruction, and 3D content creation.

</details>

### [390] [Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation](https://arxiv.org/abs/2504.19189)
*Lei Zhong,Chuan Guo,Yiming Xie,Jiawei Wang,Changjian Li*

Main category: cs.GR

TLDR: 提出了一种名为Sketch2Anim的方法，通过条件运动合成将2D故事板草图直接转换为3D动画，解决了传统方法耗时耗力的问题。


<details>
  <summary>Details</summary>
Motivation: 传统3D动画制作依赖2D草图参考，过程繁琐且需要专业技能，因此需要自动化工具实现2D到3D的直接转换。

Method: 设计了3D条件运动生成器和神经映射器，分别用于生成精确运动和将2D草图与3D关键姿势对齐。

Result: 成功将故事板转换为高质量3D动画，并支持直接编辑。实验和用户研究验证了方法的有效性。

Conclusion: Sketch2Anim为2D到3D动画转换提供了高效且灵活的新方法。

Abstract: Storyboarding is widely used for creating 3D animations. Animators use the 2D
sketches in storyboards as references to craft the desired 3D animations
through a trial-and-error process. The traditional approach requires
exceptional expertise and is both labor-intensive and time-consuming.
Consequently, there is a high demand for automated methods that can directly
translate 2D storyboard sketches into 3D animations. This task is
under-explored to date and inspired by the significant advancements of motion
diffusion models, we propose to address it from the perspective of conditional
motion synthesis. We thus present Sketch2Anim, composed of two key modules for
sketch constraint understanding and motion generation. Specifically, due to the
large domain gap between the 2D sketch and 3D motion, instead of directly
conditioning on 2D inputs, we design a 3D conditional motion generator that
simultaneously leverages 3D keyposes, joint trajectories, and action words, to
achieve precise and fine-grained motion control. Then, we invent a neural
mapper dedicated to aligning user-provided 2D sketches with their corresponding
3D keyposes and trajectories in a shared embedding space, enabling, for the
first time, direct 2D control of motion generation. Our approach successfully
transfers storyboards into high-quality 3D motions and inherently supports
direct 3D animation editing, thanks to the flexibility of our multi-conditional
motion generator. Comprehensive experiments and evaluations, and a user
perceptual study demonstrate the effectiveness of our approach.

</details>

### [391] [Pixels2Points: Fusing 2D and 3D Features for Facial Skin Segmentation](https://arxiv.org/abs/2504.19718)
*Victoria Yue Chen,Daoye Wang,Stephan Garbin,Sebastian Winberg,Timo Bolkart,Thabo Beeler*

Main category: cs.GR

TLDR: 提出了一种结合2D和3D特征的新方法，用于在3D人脸扫描中准确分割皮肤和非皮肤区域，显著提高了配准精度。


<details>
  <summary>Details</summary>
Motivation: 现有2D或3D分割方法在非皮肤区域（如头发、胡须、饰品）表现不佳，导致配准质量下降，需要一种更准确的分割方法。

Method: 通过冻结的图像基础模型提取多视角图像特征，并将其与3D几何特征融合，直接在扫描网格上预测分割掩码。

Result: 实验表明，该方法比纯2D或3D分割方法的配准精度分别提高了8.89%和14.3%，且能很好地泛化到真实数据。

Conclusion: 该方法通过结合2D和3D特征，显著提升了3D人脸扫描中皮肤分割的准确性，从而改善了配准质量。

Abstract: Face registration deforms a template mesh to closely fit a 3D face scan, the
quality of which commonly degrades in non-skin regions (e.g., hair, beard,
accessories), because the optimized template-to-scan distance pulls the
template mesh towards the noisy scan surface. Improving registration quality
requires a clean separation of skin and non-skin regions on the scan mesh.
Existing image-based (2D) or scan-based (3D) segmentation methods however
perform poorly. Image-based segmentation outputs multi-view inconsistent masks,
and they cannot account for scan inaccuracies or scan-image misalignment, while
scan-based methods suffer from lower spatial resolution compared to images. In
this work, we introduce a novel method that accurately separates skin from
non-skin geometry on 3D human head scans. For this, our method extracts
features from multi-view images using a frozen image foundation model and
aggregates these features in 3D. These lifted 2D features are then fused with
3D geometric features extracted from the scan mesh, to then predict a
segmentation mask directly on the scan mesh. We show that our segmentations
improve the registration accuracy over pure 2D or 3D segmentation methods by
8.89% and 14.3%, respectively. Although trained only on synthetic data, our
model generalizes well to real data.

</details>

<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [392] [Exploring Visual Complaints through a test battery in Acquired Brain Injury Patients: A Detailed Analysis of the DiaNAH Dataset](https://arxiv.org/abs/2504.18540)
*Gonçalo Hora de Carvalho*

Main category: q-bio.NC

TLDR: 研究使用AutoML处理缺失数据，分析ABI患者的视觉症状与标准视觉测试的关系，发现相关性较弱。


<details>
  <summary>Details</summary>
Motivation: 探索ABI患者主观视觉症状与客观视觉测试之间的复杂关系。

Method: 使用AutoML处理缺失数据，对767名患者进行线性相关分析。

Result: 视觉症状与标准测试相关性微弱。

Conclusion: 需更大样本进一步研究视觉症状群及其对脑损伤后视觉感知的影响。

Abstract: This study investigated visual impairment complaints in a sample of 948
Acquired Brain Injury (ABI) patients using the DiaNAH dataset, emphasizing
advanced machine learning techniques for managing missing data. Patients
completed a CVS questionnaire capturing eight types of visual symptoms,
including blurred vision and altered contrast perception. Due to incomplete
data, 181 patients were excluded, resulting in an analytical subset of 767
individuals. To address the challenge of missing data, an automated machine
learning (AutoML) approach was employed for data imputation, preserving the
distributional characteristics of the original dataset. Patients were grouped
according to singular and combined complaint clusters derived from the 40,320
potential combinations identified through the CVS questionnaire. A linear
correlation analysis revealed minimal to no direct relationship between
patient-reported visual complaints and standard visual perceptual function
tests. This study represents an initial systematic attempt to understand the
complex relationship between subjective visual complaints and objective visual
perceptual assessments in ABI patients. Given the limitations of sample size
and variability, further studies with larger populations are recommended to
robustly explore these complaint clusters and their implications for visual
perception following brain injury.

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [393] [Improving Pretrained YAMNet for Enhanced Speech Command Detection via Transfer Learning](https://arxiv.org/abs/2504.19030)
*Sidahmed Lachenani,Hamza Kheddar,Mohamed Ouldzmirli*

Main category: cs.SD

TLDR: 该研究通过迁移学习和预训练的YAMNet模型，显著提升了语音命令识别的准确性和效率，最终模型准确率达到95.28%。


<details>
  <summary>Details</summary>
Motivation: 提升语音命令识别系统的准确性和效率，以改善智能应用中的用户交互体验。

Method: 利用预训练的YAMNet模型和迁移学习技术，对语音命令数据集进行增强和特征提取，训练模型识别语音命令。

Result: 模型在语音命令识别任务中达到了95.28%的准确率。

Conclusion: 该研究展示了迁移学习在语音命令识别中的实际应用价值，为音频处理技术设定了新的基准。

Abstract: This work addresses the need for enhanced accuracy and efficiency in speech
command recognition systems, a critical component for improving user
interaction in various smart applications. Leveraging the robust pretrained
YAMNet model and transfer learning, this study develops a method that
significantly improves speech command recognition. We adapt and train a YAMNet
deep learning model to effectively detect and interpret speech commands from
audio signals. Using the extensively annotated Speech Commands dataset
(speech_commands_v0.01), our approach demonstrates the practical application of
transfer learning to accurately recognize a predefined set of speech commands.
The dataset is meticulously augmented, and features are strategically extracted
to boost model performance. As a result, the final model achieved a recognition
accuracy of 95.28%, underscoring the impact of advanced machine learning
techniques on speech command recognition. This achievement marks substantial
progress in audio processing technologies and establishes a new benchmark for
future research in the field.

</details>

### [394] [Generative Adversarial Network based Voice Conversion: Techniques, Challenges, and Recent Advancements](https://arxiv.org/abs/2504.19197)
*Sandipan Dhar,Nanda Dulal Jana,Swagatam Das*

Main category: cs.SD

TLDR: 本文综述了语音转换（VC）技术，特别是基于生成对抗网络（GAN）的方法，分析了其应用、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 语音转换技术在语音合成中具有广泛应用，如电影配音和病理语音康复，但现有方法仍面临训练稳定性、语言一致性和感知自然性等挑战。

Method: 通过系统综述，分类现有方法，分析技术障碍，并评估GAN在VC中的最新进展。

Result: 综述总结了不同方法的优缺点，并为未来研究提供了方向。

Conclusion: 本文为研究人员提供了VC领域的全面视角，有助于推动技术的进一步发展。

Abstract: Voice conversion (VC) stands as a crucial research area in speech synthesis,
enabling the transformation of a speaker's vocal characteristics to resemble
another while preserving the linguistic content. This technology has broad
applications, including automated movie dubbing, speech-to-singing conversion,
and assistive devices for pathological speech rehabilitation. With the
increasing demand for high-quality and natural-sounding synthetic voices,
researchers have developed a wide range of VC techniques. Among these,
generative adversarial network (GAN)-based approaches have drawn considerable
attention for their powerful feature-mapping capabilities and potential to
produce highly realistic speech. Despite notable advancements, challenges such
as ensuring training stability, maintaining linguistic consistency, and
achieving perceptual naturalness continue to hinder progress in GAN-based VC
systems. This systematic review presents a comprehensive analysis of the voice
conversion landscape, highlighting key techniques, key challenges, and the
transformative impact of GANs in the field. The survey categorizes existing
methods, examines technical obstacles, and critically evaluates recent
developments in GAN-based VC. By consolidating and synthesizing research
findings scattered across the literature, this review provides a structured
understanding of the strengths and limitations of different approaches. The
significance of this survey lies in its ability to guide future research by
identifying existing gaps, proposing potential directions, and offering
insights for building more robust and efficient VC systems. Overall, this work
serves as an essential resource for researchers, developers, and practitioners
aiming to advance the state-of-the-art (SOTA) in voice conversion technology.

</details>

### [395] [Speaker Retrieval in the Wild: Challenges, Effectiveness and Robustness](https://arxiv.org/abs/2504.18950)
*Erfan Loweimi,Mengjie Qian,Kate Knill,Mark Gales*

Main category: cs.SD

TLDR: 论文研究了在非受控环境下（如BBC Rewind档案）的说话人检索系统，探讨了从有限元数据中提取任务相关标签的挑战，并提出了解决方案。通过实验验证了系统的有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着音频/视频档案的增多，高效检索内容的需求日益重要，但现有档案的元数据不足且环境复杂，亟需解决方案。

Method: 研究了说话人检索系统的开发（如说话人分割、嵌入提取、查询选择），并在干净和模拟失真环境下进行实验。

Result: 实验表明，所开发的说话人检索系统在复杂环境中表现有效且鲁棒。

Conclusion: 提出的框架具有广泛适用性和扩展性，适用于BBC Rewind档案以外的多种应用。

Abstract: There is a growing abundance of publicly available or company-owned
audio/video archives, highlighting the increasing importance of efficient
access to desired content and information retrieval from these archives. This
paper investigates the challenges, solutions, effectiveness, and robustness of
speaker retrieval systems developed "in the wild" which involves addressing two
primary challenges: extraction of task-relevant labels from limited metadata
for system development and evaluation, as well as the unconstrained acoustic
conditions encountered in the archive, ranging from quiet studios to adverse
noisy environments. While we focus on the publicly-available BBC Rewind archive
(spanning 1948 to 1979), our framework addresses the broader issue of speaker
retrieval on extensive and possibly aged archives with no control over the
content and acoustic conditions. Typically, these archives offer a brief and
general file description, mostly inadequate for specific applications like
speaker retrieval, and manual annotation of such large-scale archives is
unfeasible. We explore various aspects of system development (e.g., speaker
diarisation, embedding extraction, query selection) and analyse the challenges,
possible solutions, and their functionality. To evaluate the performance, we
conduct systematic experiments in both clean setup and against various
distortions simulating real-world applications. Our findings demonstrate the
effectiveness and robustness of the developed speaker retrieval systems,
establishing the versatility and scalability of the proposed framework for a
wide range of applications beyond the BBC Rewind corpus.

</details>

<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [396] [Neuronal correlations shape the scaling behavior of memory capacity and nonlinear computational capability of recurrent neural networks](https://arxiv.org/abs/2504.19657)
*Shotaro Takasu,Toshio Aoyagi*

Main category: cond-mat.dis-nn

TLDR: 储层计算的内存容量随读出神经元数量呈亚线性增长，神经元相关性是主要原因。


<details>
  <summary>Details</summary>
Motivation: 研究储层计算中内存容量与读出神经元数量的关系及其对非线性处理能力的影响。

Method: 建立理论框架分析内存容量，并通过数值模拟验证。

Result: 内存容量增长亚线性，神经元相关性还影响非线性计算能力的顺序增长。

Conclusion: 为设计可扩展且经济高效的储层计算提供了理论基础。

Abstract: Reservoir computing is a powerful framework for real-time information
processing, characterized by its high computational ability and quick learning,
with applications ranging from machine learning to biological systems. In this
paper, we demonstrate that the memory capacity of a reservoir recurrent neural
network scales sublinearly with the number of readout neurons. To elucidate
this phenomenon, we develop a theoretical framework for analytically deriving
memory capacity, attributing the decaying growth of memory capacity to neuronal
correlations. In addition, numerical simulations reveal that once memory
capacity becomes sublinear, increasing the number of readout neurons
successively enables nonlinear processing at progressively higher polynomial
orders. Furthermore, our theoretical framework suggests that neuronal
correlations govern not only memory capacity but also the sequential growth of
nonlinear computational capabilities. Our findings establish a foundation for
designing scalable and cost-effective reservoir computing, providing novel
insights into the interplay among neuronal correlations, linear memory, and
nonlinear processing.

</details>

<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [397] [Mapping the Italian Telegram Ecosystem](https://arxiv.org/abs/2504.19594)
*Lorenzo Alvisi,Serena Tardelli,Maurizio Tesconi*

Main category: cs.SI

TLDR: 本文通过大规模分析意大利Telegram生态，揭示了其政治话语、意识形态同质性和毒性内容的传播模式。


<details>
  <summary>Details</summary>
Motivation: 填补对Telegram生态系统的全面理解空白，尤其是意大利语境下的政治和毒性内容传播。

Method: 利用网络分析、大型语言模型和毒性检测工具，分析186万条消息和13,151个聊天数据。

Result: 发现强烈的主题和意识形态同质性，毒性内容广泛存在，且仇恨言论主要针对黑人、犹太人和同性恋群体。

Conclusion: 研究首次绘制了意大利Telegram生态的大规模图谱，为跨文化在线毒性研究提供了新视角。

Abstract: Telegram has become a major space for political discourse and alternative
media. However, its lack of moderation allows misinformation, extremism, and
toxicity to spread. While prior research focused on these particular phenomena
or topics, these have mostly been examined separately, and a broader
understanding of the Telegram ecosystem is still missing. In this work, we fill
this gap by conducting a large-scale analysis of the Italian Telegram sphere,
leveraging a dataset of 186 million messages from 13,151 chats collected in
2023. Using network analysis, Large Language Models, and toxicity detection
tools, we examine how different thematic communities form, align ideologically,
and engage in harmful discourse within the Italian cultural context. Results
show strong thematic and ideological homophily. We also identify mixed
ideological communities where far-left and far-right rhetoric coexist on
particular geopolitical issues. Beyond political analysis, we find that
toxicity, rather than being isolated in a few extreme chats, appears widely
normalized within highly toxic communities. Moreover, we find that Italian
discourse primarily targets Black people, Jews, and gay individuals
independently of the topic. Finally, we uncover common trend of intra-national
hostility, where Italians often attack other Italians, reflecting regional and
intra-regional cultural conflicts that can be traced back to old historical
divisions. This study provides the first large-scale mapping of the Italian
Telegram ecosystem, offering insights into ideological interactions, toxicity,
and identity-targets of hate and contributing to research on online toxicity
across different cultural and linguistic contexts on Telegram.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [398] [From Prompts to Propositions: A Logic-Based Lens on Student-LLM Interactions](https://arxiv.org/abs/2504.18691)
*Ali Alfageeh,Sadegh AlMahdi Kazemi Zarkouei,Daye Nam,Daniel Prol,Matin Amoozadeh,Souti Chattopadhyay,James Prather,Paul Denny,Juho Leinonen,Michael Hilton,Sruti Srinivasa Ragavan,Mohammad Amin Alipour*

Main category: cs.HC

TLDR: 论文提出Prompt2Constraints方法，将学生提示转化为逻辑约束，分析编程任务中提示的演变模式，识别困难学生并优化策略。


<details>
  <summary>Details</summary>
Motivation: 研究学生如何利用LLMs解决计算任务，现有方法缺乏可扩展性或语义捕捉能力。

Method: 引入Prompt2Constraints，将学生提示转化为逻辑约束，分析1,872条提示数据。

Result: 成功与失败尝试约束数量相似，但失败时提示修改更显著，策略转变明显。

Conclusion: 提供可扩展方法检测学生困难，未来可扩展至复杂任务并集成实时支持工具。

Abstract: Background and Context. The increasing integration of large language models
(LLMs) in computing education presents an emerging challenge in understanding
how students use LLMs and craft prompts to solve computational tasks. Prior
research has used both qualitative and quantitative methods to analyze
prompting behavior, but these approaches lack scalability or fail to
effectively capture the semantic evolution of prompts. Objective. In this
paper, we investigate whether students prompts can be systematically analyzed
using propositional logic constraints. We examine whether this approach can
identify patterns in prompt evolution, detect struggling students, and provide
insights into effective and ineffective strategies. Method. We introduce
Prompt2Constraints, a novel method that translates students prompts into
logical constraints. The constraints are able to represent the intent of the
prompts in succinct and quantifiable ways. We used this approach to analyze a
dataset of 1,872 prompts from 203 students solving introductory programming
tasks. Findings. We find that while successful and unsuccessful attempts tend
to use a similar number of constraints overall, when students fail, they often
modify their prompts more significantly, shifting problem-solving strategies
midway. We also identify points where specific interventions could be most
helpful to students for refining their prompts. Implications. This work offers
a new and scalable way to detect students who struggle in solving natural
language programming tasks. This work could be extended to investigate more
complex tasks and integrated into programming tools to provide real-time
support.

</details>

### [399] [Clinical knowledge in LLMs does not translate to human interactions](https://arxiv.org/abs/2504.18919)
*Andrew M. Bean,Rebecca Payne,Guy Parsons,Hannah Rose Kirk,Juan Ciro,Rafael Mosquera,Sara Hincapié Monsalve,Aruna S. Ekanayaka,Lionel Tarassenko,Luc Rocher,Adam Mahdi*

Main category: cs.HC

TLDR: 大型语言模型（LLM）在医疗场景中表现优异，但在实际应用中用户交互效果不佳，未能显著提升公众识别病情和选择治疗方案的能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在提供医疗建议中的实际效果，尤其是在公众使用时的表现。

Method: 在1298名参与者中进行对照实验，比较LLM辅助与对照组在10个医疗场景中的表现。

Result: LLM单独测试表现优秀（病情识别94.9%，治疗方案56.3%），但用户使用后效果显著下降（病情识别<34.5%，治疗方案<44.2%），与对照组无差异。

Conclusion: 用户交互是LLM在医疗建议中的主要挑战，需系统性用户测试评估交互能力后再部署。

Abstract: Global healthcare providers are exploring use of large language models (LLMs)
to provide medical advice to the public. LLMs now achieve nearly perfect scores
on medical licensing exams, but this does not necessarily translate to accurate
performance in real-world settings. We tested if LLMs can assist members of the
public in identifying underlying conditions and choosing a course of action
(disposition) in ten medical scenarios in a controlled study with 1,298
participants. Participants were randomly assigned to receive assistance from an
LLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested
alone, LLMs complete the scenarios accurately, correctly identifying conditions
in 94.9% of cases and disposition in 56.3% on average. However, participants
using the same LLMs identified relevant conditions in less than 34.5% of cases
and disposition in less than 44.2%, both no better than the control group. We
identify user interactions as a challenge to the deployment of LLMs for medical
advice. Standard benchmarks for medical knowledge and simulated patient
interactions do not predict the failures we find with human participants.
Moving forward, we recommend systematic human user testing to evaluate
interactive capabilities prior to public deployments in healthcare.

</details>

### [400] [LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings](https://arxiv.org/abs/2504.18988)
*Saramsh Gautam,Mahmood Jasim*

Main category: cs.HC

TLDR: 论文研究了英语非母语研究者（ESL）在多语言团队协作中的沟通障碍，提出了LINC系统以支持多语言实时沟通和会后分析，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多语言协作中，ESL研究者因语言障碍难以充分参与讨论，限制了其贡献。

Method: 通过调查64名ESL研究者，提出四个设计目标，并开发了LINC系统，包括实时多语言沟通模块和会后分析仪表盘。通过六组多语言团队的实验评估系统。

Result: LINC帮助参与者用偏好语言沟通，回顾会议要点，并为后续会议做准备。

Conclusion: LINC系统有效支持多语言协作，但外部因素（如非语言偏好）仍需考虑，系统可扩展至其他多语言混合协作场景。

Abstract: Collaborative research often includes contributors with varied perspectives
from diverse linguistic backgrounds. However, English as a Second Language
(ESL) researchers often struggle to communicate during meetings in English and
comprehend discussions, leading to limited contribution. To investigate these
challenges, we surveyed 64 ESL researchers who frequently collaborate in
multilingual teams and identified four key design goals around participation,
comprehension, documentation, and feedback. Guided by these design goals, we
developed LINC, a multimodal Language INdependent Collaboration system with two
components: a real-time module for multilingual communication during meetings
and a post-meeting dashboard for discussion analysis. We evaluated the system
through a two-phased study with six triads of multilingual teams. We found that
using LINC, participants benefited from communicating in their preferred
language, recalled and reviewed actionable insights, and prepared for upcoming
meetings effectively. We discuss external factors that impact multilingual
meeting participation beyond language preferences and the implications of
multimodal systems in facilitating meetings in hybrid multilingual
collaborative settings beyond research.

</details>

### [401] [Clones in the Machine: A Feminist Critique of Agency in Digital Cloning](https://arxiv.org/abs/2504.18807)
*Siân Brooke*

Main category: cs.HC

TLDR: 本文批判学术研究中数字克隆的AI解决方案主义倾向，指出其简化人类复杂性并掩盖伦理问题，提出去中心化数据存储和动态同意模型以促进伦理AI实践。


<details>
  <summary>Details</summary>
Motivation: 数字克隆被广泛视为行为洞察的可扩展工具，但其忽视了伦理问题如同意、代理和代表性，本文旨在揭示这些问题并提出改进方案。

Method: 通过女性主义代理理论分析数字克隆的局限性，并提出去中心化数据存储和动态同意模型作为解决方案。

Result: 研究发现数字克隆可能加剧系统性偏见，并提出了更伦理、情境感知的AI实践框架。

Conclusion: 本文呼吁挑战AI解决方案主义的简化逻辑，推动更伦理、包容的数字克隆实践。

Abstract: This paper critiques digital cloning in academic research, highlighting how
it exemplifies AI solutionism. Digital clones, which replicate user data to
simulate behavior, are often seen as scalable tools for behavioral insights.
However, this framing obscures ethical concerns around consent, agency, and
representation. Drawing on feminist theories of agency, the paper argues that
digital cloning oversimplifies human complexity and risks perpetuating systemic
biases. To address these issues, it proposes decentralized data repositories
and dynamic consent models, promoting ethical, context-aware AI practices that
challenge the reductionist logic of AI solutionism

</details>

### [402] [AI Chatbots for Mental Health: Values and Harms from Lived Experiences of Depression](https://arxiv.org/abs/2504.18932)
*Dong Whi Yoo,Jiayue Melissa Shi,Violeta J. Rodriguez,Koustuv Saha*

Main category: cs.HC

TLDR: 研究探讨了基于GPT-4o的心理健康聊天机器人Zenny在抑郁症自我管理中的潜在危害与用户价值观的关系，并提出了设计建议。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在心理健康领域的应用日益增多，但其有效性和可靠性尚不明确，可能带来危害。研究旨在通过理解用户价值观来识别和减轻这些危害。

Method: 开发了基于GPT-4o的聊天机器人Zenny，用于与17名有抑郁症经历的人进行互动，并通过主题分析提取关键价值观。

Result: 研究发现用户关注的核心价值观包括信息支持、情感支持、个性化、隐私和危机管理。

Conclusion: 研究揭示了用户价值观与潜在危害的关系，为心理健康AI聊天机器人的设计提供了建议，以优化支持并降低风险。

Abstract: Recent advancements in LLMs enable chatbots to interact with individuals on a
range of queries, including sensitive mental health contexts. Despite
uncertainties about their effectiveness and reliability, the development of
LLMs in these areas is growing, potentially leading to harms. To better
identify and mitigate these harms, it is critical to understand how the values
of people with lived experiences relate to the harms. In this study, we
developed a technology probe, a GPT-4o based chatbot called Zenny, enabling
participants to engage with depression self-management scenarios informed by
previous research. We used Zenny to interview 17 individuals with lived
experiences of depression. Our thematic analysis revealed key values:
informational support, emotional support, personalization, privacy, and crisis
management. This work explores the relationship between lived experience
values, potential harms, and design recommendations for mental health AI
chatbots, aiming to enhance self-management support while minimizing risks.

</details>

### [403] [Beyond Levels of Driving Automation: A Triadic Framework of Human-AI Collaboration in On-Road Mobility](https://arxiv.org/abs/2504.19120)
*Gaojian Huang,Yantong Jin,Wei-Hsiang Lo*

Main category: cs.HC

TLDR: 提出了一种三元人机协作框架，用于自动驾驶领域，动态适应人类需求。


<details>
  <summary>Details</summary>
Motivation: 现有分类（如SAE自动化等级）仅关注车辆控制权分配，未解决动态驾驶场景中人与AI的实时协作问题。

Method: 提出三元框架，定义AI的三种角色（顾问、副驾驶、守护者），动态调整以适应人类需求。

Result: 为自动驾驶中基于角色的自适应人机协作策略奠定了基础。

Conclusion: 该研究填补了动态驾驶场景中人机协作的空白，为未来自适应协作策略提供了框架。

Abstract: The goal of the current study is to introduce a triadic human-AI
collaboration framework for the automated vehicle domain. Previous
classifications (e.g., SAE Levels of Automation) focus on defining automation
levels based on who controls the vehicle. However, it remains unclear how human
users and AI should collaborate in real-time, especially in dynamic driving
contexts, where roles can shift frequently. To fill the gap, this study
proposes a triadic human-AI collaboration framework with three AI roles (i.e.,
Advisor, Co-Pilot, and Guardian) that dynamically adapt to human needs.
Overall, the study lays a foundation for developing adaptive, role-based
human-AI collaboration strategies in automated vehicles.

</details>

### [404] [A Real-Time Gesture-Based Control Framework](https://arxiv.org/abs/2504.19460)
*Mahya Khazaei,Ali Bahrani,George Tzanetakis*

Main category: cs.HC

TLDR: 提出了一种实时、人机交互的手势控制框架，通过分析实时视频输入动态调整音频和音乐，实现舞者和表演者通过动作影响音乐。


<details>
  <summary>Details</summary>
Motivation: 旨在创造一种视觉与听觉刺激之间的响应式连接，为现场表演、互动装置和个人使用提供沉浸式体验。

Method: 结合计算机视觉和机器学习技术跟踪和解释动作，用户可通过手势操纵音频元素（如节奏、音高、效果和播放顺序）。

Result: 系统实现了用户无关的功能，仅需50至80个样本即可标记简单手势，展示了人机交互与机器响应的无缝结合。

Conclusion: 该框架为动态交互体验提供了新方法，展示了手势控制音乐的实际应用潜力。

Abstract: We introduce a real-time, human-in-the-loop gesture control framework that
can dynamically adapt audio and music based on human movement by analyzing live
video input. By creating a responsive connection between visual and auditory
stimuli, this system enables dancers and performers to not only respond to
music but also influence it through their movements. Designed for live
performances, interactive installations, and personal use, it offers an
immersive experience where users can shape the music in real time.
  The framework integrates computer vision and machine learning techniques to
track and interpret motion, allowing users to manipulate audio elements such as
tempo, pitch, effects, and playback sequence. With ongoing training, it
achieves user-independent functionality, requiring as few as 50 to 80 samples
to label simple gestures. This framework combines gesture training, cue
mapping, and audio manipulation to create a dynamic, interactive experience.
Gestures are interpreted as input signals, mapped to sound control commands,
and used to naturally adjust music elements, showcasing the seamless interplay
between human interaction and machine response.

</details>

<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [405] [Learning Stochastic Thermodynamics Directly from Correlation and Trajectory-Fluctuation Currents](https://arxiv.org/abs/2504.19007)
*Jinghao Lyu,Kyle J. Ray,James P. Crutchfield*

Main category: cond-mat.stat-mech

TLDR: 论文提出了一种基于电流的学习框架，用于随机动力学建模，揭示了机器学习扩散模型与随机热力学熵产生估计之间的深层联系。


<details>
  <summary>Details</summary>
Motivation: 研究如何从复杂动力系统的时间序列测量中学习，特别是在噪声数据下的小系统动力学建模。

Method: 利用电流构建学习框架，直接从系统动力学推导损失函数，避免传统热力学不确定性关系（TUR）的中间步骤。

Result: 提出的损失函数能够重现TUR和其他方法的结果，并可用于发现新损失函数，包括远离稳态系统的轨迹熵产生。

Conclusion: 该方法统一了动态推断与熵产生估计，为随机热力学和机器学习提供了新的联系。

Abstract: Markedly increased computational power and data acquisition have led to
growing interest in data-driven inverse dynamics problems. These seek to answer
a fundamental question: What can we learn from time series measurements of a
complex dynamical system? For small systems interacting with external
environments, the effective dynamics are inherently stochastic, making it
crucial to properly manage noise in data. Here, we explore this for systems
obeying Langevin dynamics and, using currents, we construct a learning
framework for stochastic modeling. Currents have recently gained increased
attention for their role in bounding entropy production (EP) from thermodynamic
uncertainty relations (TURs). We introduce a fundamental relationship between
the cumulant currents there and standard machine-learning loss functions. Using
this, we derive loss functions for several key thermodynamic functions directly
from the system dynamics without the (common) intermediate step of deriving a
TUR. These loss functions reproduce results derived both from TURs and other
methods. More significantly, they open a path to discover new loss functions
for previously inaccessible quantities. Notably, this includes access to
per-trajectory entropy production, even if the observed system is driven far
from its steady-state. We also consider higher order estimation. Our method is
straightforward and unifies dynamic inference with recent approaches to entropy
production estimation. Taken altogether, this reveals a deep connection between
diffusion models in machine learning and entropy production estimation in
stochastic thermodynamics.

</details>

<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [406] [Parameter Tuning of the Firefly Algorithm by Three Tuning Methods: Standard Monte Carlo, Quasi-Monte Carlo and Latin Hypercube Sampling Methods](https://arxiv.org/abs/2504.18545)
*Geethu Joy,Christian Huyck,Xin-She Yang*

Main category: stat.CO

TLDR: 论文研究了萤火虫算法（FA）的参数调优方法，比较了蒙特卡洛、准蒙特卡洛和拉丁超立方采样三种方法，发现调优方法对FA性能无显著影响。


<details>
  <summary>Details</summary>
Motivation: 探讨不同参数调优方法对萤火虫算法性能的影响，以优化算法表现。

Method: 使用蒙特卡洛、准蒙特卡洛和拉丁超立方采样三种方法调优FA参数，并应用于六种优化问题，进行统计假设检验。

Result: 调优方法对FA性能无显著影响，参数值也基本独立于调优方法。

Conclusion: FA在解决优化问题时具有灵活性，三种调优方法均可有效使用。

Abstract: There are many different nature-inspired algorithms in the literature, and
almost all such algorithms have algorithm-dependent parameters that need to be
tuned. The proper setting and parameter tuning should be carried out to
maximize the performance of the algorithm under consideration. This work is the
extension of the recent work on parameter tuning by Joy et al. (2024) presented
at the International Conference on Computational Science (ICCS 2024), and the
Firefly Algorithm (FA) is tuned using three different methods: the Monte Carlo
method, the Quasi-Monte Carlo method and the Latin Hypercube Sampling. The FA
with the tuned parameters is then used to solve a set of six different
optimization problems, and the possible effect of parameter setting on the
quality of the optimal solutions is analyzed. Rigorous statistical hypothesis
tests have been carried out, including Student's t-tests, F-tests,
non-parametric Friedman tests and ANOVA. Results show that the performance of
the FA is not influenced by the tuning methods used. In addition, the tuned
parameter values are largely independent of the tuning methods used. This
indicates that the FA can be flexible and equally effective in solving
optimization problems, and any of the three tuning methods can be used to tune
its parameters effectively.

</details>

### [407] [A Langevin sampling algorithm inspired by the Adam optimizer](https://arxiv.org/abs/2504.18911)
*Benedict Leimkuhler,René Lohmann,Peter Whalley*

Main category: stat.CO

TLDR: 提出了一种基于时间重标Langevin动力学的自适应步长MCMC采样框架，通过引入额外自由度动态调整步长，提升数值稳定性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统固定步长MCMC采样在复杂后验分布中可能效率低下或数值不稳定，需动态调整步长以优化性能。

Method: 通过引入辅助变量和时间重参数化，动态控制步长变化，无需修改物理系统的漂移项，并可结合现有固定步长Langevin积分器。

Result: 实验表明，该方法在Neal漏斗和MNIST数据分类的贝叶斯神经网络中显著提升了稳定性和准确性。

Conclusion: 该框架为自适应步长MCMC提供了一种高效且易于实现的解决方案，适用于复杂后验分布采样。

Abstract: We present a framework for adaptive-stepsize MCMC sampling based on
time-rescaled Langevin dynamics, in which the stepsize variation is dynamically
driven by an additional degree of freedom. Our approach augments the phase
space by an additional variable which in turn defines a time
reparameterization. The use of an auxiliary relaxation equation allows
accumulation of a moving average of a local monitor function and provides for
precise control of the timestep while circumventing the need to modify the
drift term in the physical system. Our algorithm is straightforward to
implement and can be readily combined with any off-the-peg fixed-stepsize
Langevin integrator. As a particular example, we consider control of the
stepsize by monitoring the norm of the log-posterior gradient, which takes
inspiration from the Adam optimizer, the stepsize being automatically reduced
in regions of steep change of the log posterior and increased on plateaus,
improving numerical stability and convergence speed. As in Adam, the stepsize
variation depends on the recent history of the gradient norm, which enhances
stability and improves accuracy compared to more immediate control approaches.
We demonstrate the potential benefit of this method--both in accuracy and in
stability--in numerical experiments including Neal's funnel and a Bayesian
neural network for classification of MNIST data.

</details>

<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [408] [GPU accelerated program synthesis: Enumerate semantics, not syntax!](https://arxiv.org/abs/2504.18943)
*Martin Berger,Nathanaël Fijalkow,Mojtaba Valizadeh*

Main category: cs.PL

TLDR: 探讨了利用GPU实现基于搜索的程序合成器以提升性能的可能性，并展示了GPU友好的编程技术如何显著提升合成问题的规模和速度。


<details>
  <summary>Details</summary>
Motivation: 随着GPU在深度学习中的显著性能提升，研究是否可以通过GPU实现程序合成器以取得类似性能改进。

Method: 使用GPU友好的编程技术，如利用公式语义减少数据移动和数据依赖分支，构建合成器。

Result: 合成器能够处理更大规模的合成问题，且速度远超基于CPU的最先进方法。

Conclusion: GPU友好的方法有望提升其他形式化方法（FM）任务的性能。

Abstract: Program synthesis is an umbrella term for generating programs and logical
formulae from specifications. With the remarkable performance improvements that
GPUs enable for deep learning, a natural question arose: can we also implement
a search-based program synthesiser on GPUs to achieve similar performance
improvements? In this article we discuss our insights on this question, based
on recent works~. The goal is to build a synthesiser running on GPUs which
takes as input positive and negative example traces and returns a logical
formula accepting the positive and rejecting the negative traces. With
GPU-friendly programming techniques -- using the semantics of formulae to
minimise data movement and reduce data-dependent branching -- our synthesiser
scales to significantly larger synthesis problems, and operates much faster
than the previous CPU-based state-of-the-art. We believe the insights that make
our approach GPU-friendly have wide potential for enhancing the performance of
other formal methods (FM) workloads.

</details>

### [409] [Rulebook: bringing co-routines to reinforcement learning environments](https://arxiv.org/abs/2504.19625)
*Massimo Fioravanti,Samuele Pasini,Giovanni Agosta*

Main category: cs.PL

TLDR: Rulebook是一种新的领域特定语言，通过协程和编译技术自动生成与机器学习算法交互所需的状态机，降低开发成本。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法依赖外部系统学习，传统实现方式（如独立进程或状态机）存在同步、通信开销或编程结构混乱的问题。

Method: 提出Rulebook语言，基于协程和编译技术，自动生成状态机，无需手动管理状态。

Result: Rulebook能够以更低开发成本创建更复杂的环境，且无性能开销。

Conclusion: Rulebook为强化学习环境提供了一种高效、易用的实现方案。

Abstract: Reinforcement learning (RL) algorithms, due to their reliance on external
systems to learn from, require digital environments (e.g., simulators) with
very simple interfaces, which in turn constrain significantly the
implementation of such environments. In particular, these environments are
implemented either as separate processes or as state machines, leading to
synchronization and communication overheads in the first case, and to
unstructured programming in the second.
  We propose a new domain-specific, co-routine-based, compiled language, called
Rulebook, designed to automatically generate the state machine required to
interact with machine learning (ML) algorithms and similar applications, with
no performance overhead. Rulebook allows users to express programs without
needing to be aware of the specific interface required by the ML components. By
decoupling the execution model of the program from the syntactical encoding of
the program, and thus without the need for manual state management, Rulebook
allows to create larger and more sophisticated environments at a lower
development cost.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [410] [The Philosophic Turn for AI Agents: Replacing centralized digital rhetoric with decentralized truth-seeking](https://arxiv.org/abs/2504.18601)
*Philipp Koralus*

Main category: cs.CY

TLDR: 论文探讨了AI决策支持系统对人类自主性和能动性的潜在威胁，提出了一种基于哲学对话的AI设计方法，以增强用户判断力而不损害自主性。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展，人类依赖AI代理处理复杂决策可能导致自主性丧失或被外部控制的风险。

Method: 提出一种哲学转向的AI设计方法，模仿苏格拉底式对话，促进分散式真理探索和开放式探究。

Result: 这种设计方法能够增强用户的判断力和集体适应性学习，同时保持自主性。

Conclusion: 论文提出了自主性保护AI系统的关键特征，为增强而非削弱人类判断力的AI系统指明了方向。

Abstract: In the face of rapidly advancing AI technology, individuals will increasingly
rely on AI agents to navigate life's growing complexities, raising critical
concerns about maintaining both human agency and autonomy. This paper addresses
a fundamental dilemma posed by AI decision-support systems: the risk of either
becoming overwhelmed by complex decisions, thus losing agency, or having
autonomy compromised by externally controlled choice architectures reminiscent
of ``nudging'' practices. While the ``nudge'' framework, based on the use of
choice-framing to guide individuals toward presumed beneficial outcomes,
initially appeared to preserve liberty, at AI-driven scale, it threatens to
erode autonomy. To counteract this risk, the paper proposes a philosophic turn
in AI design. AI should be constructed to facilitate decentralized
truth-seeking and open-ended inquiry, mirroring the Socratic method of
philosophical dialogue. By promoting individual and collective adaptive
learning, such AI systems would empower users to maintain control over their
judgments, augmenting their agency without undermining autonomy. The paper
concludes by outlining essential features for autonomy-preserving AI systems,
sketching a path toward AI systems that enhance human judgment rather than
undermine it.

</details>

### [411] [Toward Personalizing Quantum Computing Education: An Evolutionary LLM-Powered Approach](https://arxiv.org/abs/2504.18603)
*Iizalaarab Elhaimeur,Nikos Chrisochoides*

Main category: cs.CY

TLDR: 论文介绍了一种用于量子计算教育的智能教学助手，结合知识图谱和大型语言模型（LLM）代理，动态适应学生需求并优化学习路径。


<details>
  <summary>Details</summary>
Motivation: 量子计算教育因复杂性和工具限制面临挑战，需智能系统辅助教学。

Method: 系统采用知识图谱增强架构，包含教学代理和课程规划代理，通过中央知识图谱协调任务。

Result: 初步结果显示系统能捕获交互数据、动态调整课程计划，并实现情境感知辅导。

Conclusion: 系统潜力显著，但需系统性评估。

Abstract: Quantum computing education faces significant challenges due to its
complexity and the limitations of current tools; this paper introduces a novel
Intelligent Teaching Assistant for quantum computing education and details its
evolutionary design process. The system combines a knowledge-graph-augmented
architecture with two specialized Large Language Model (LLM) agents: a Teaching
Agent for dynamic interaction, and a Lesson Planning Agent for lesson plan
generation. The system is designed to adapt to individual student needs, with
interactions meticulously tracked and stored in a knowledge graph. This graph
represents student actions, learning resources, and relationships, aiming to
enable reasoning about effective learning pathways. We describe the
implementation of the system, highlighting the challenges encountered and the
solutions implemented, including introducing a dual-agent architecture where
tasks are separated, all coordinated through a central knowledge graph that
maintains system awareness, and a user-facing tag system intended to mitigate
LLM hallucination and improve user control. Preliminary results illustrate the
system's potential to capture rich interaction data, dynamically adapt lesson
plans based on student feedback via a tag system in simulation, and facilitate
context-aware tutoring through the integrated knowledge graph, though
systematic evaluation is required.

</details>

### [412] [Balancing Creativity and Automation: The Influence of AI on Modern Film Production and Dissemination](https://arxiv.org/abs/2504.19275)
*Yiren Xu*

Main category: cs.CY

TLDR: AI在电影制作中的双重影响：提升效率与创造力，同时带来伦理和实践挑战。研究通过混合方法探讨了人机关系、创造力与自动化的平衡，以及伦理指南。


<details>
  <summary>Details</summary>
Motivation: 探索AI在现代电影中的双重影响，解决人机关系、创造力与自动化的平衡问题，并制定伦理指南。

Method: 采用混合方法，结合理论框架（作者理论、人机关系）和案例研究（《The Safe Zone》、《Fast & Furious 7》、《The Brutalist》）。

Result: 研究发现，将AI定位为“体现工具”而非独立“他者伙伴”能保护人类作者身份和艺术完整性。同时揭示了AI驱动市场中的监控资本主义风险和深度伪造技术的伦理困境。

Conclusion: 提出国际监管框架和人类控制指数（HCI）等建议，旨在指导电影制作、政策制定和学术研究，保护文化多样性和伦理标准。

Abstract: The integration of Artificial Intelligence(AI) into film production has
revolutionized efficiency and creativity, yet it simultaneously raises critical
ethical and practical challenges. This study explores the dual impact of AI on
modern cinema through three objectives: defining the optimal human-AI
relationship, balancing creativity with automation, and developing ethical
guidelines. By employing a mixed-method approach combining theoretical
frameworks (auteur theory, human-technology relations) and case studies (The
Safe Zone, Fast & Furious 7, The Brutalist), the research reveals that
positioning AI as an "embodiment tool" rather than an independent "alterity
partner" preserves human authorship and artistic integrity. Key findings
highlight the risks of surveillance capitalism in AI-driven markets and the
ethical dilemmas of deepfake technology. The study concludes with actionable
recommendations, including international regulatory frameworks and a Human
Control Index (HCI) to quantify AI involvement. These insights aim to guide
filmmakers, policymakers, and scholars in navigating the evolving AI-cinema
landscape while safeguarding cultural diversity and ethical standards.

</details>

### [413] [Navigating AI Policy Landscapes: Insights into Human Rights Considerations Across IEEE Regions](https://arxiv.org/abs/2504.19264)
*Angel Mary John,Jerrin Thomas Panachakel,Anusha S. P*

Main category: cs.CY

TLDR: 比较不同IEEE地区（美国、欧洲、中国、新加坡）在AI监管框架中融入人权考量的差异，强调全球对话的必要性。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在不同地区的AI监管框架中平衡人权保护与技术发展。

Method: 比较分析美国、欧洲、中国和新加坡的AI监管策略及其对人权的关注。

Result: 欧洲注重严格保护个人权利，美国倾向宽松创新，中国强调国家控制，新加坡倡导自我监管。

Conclusion: 需全球对话以协调AI监管，兼顾人权与技术发展，反映地区多样性。

Abstract: This paper explores the integration of human rights considerations into AI
regulatory frameworks across different IEEE regions - specifically the United
States (Region 1-6), Europe (Region 8), China (part of Region 10), and
Singapore (part of Region 10). While all acknowledge the transformative
potential of AI and the necessity of ethical guidelines, their regulatory
approaches significantly differ. Europe exhibits a rigorous framework with
stringent protections for individual rights, while the U.S. promotes innovation
with less restrictive regulations. China emphasizes state control and societal
order in its AI strategies. In contrast, Singapore's advisory framework
encourages self-regulation and aligns closely with international norms. This
comparative analysis underlines the need for ongoing global dialogue to
harmonize AI regulations that safeguard human rights while promoting
technological advancement, reflecting the diverse perspectives and priorities
of each region.

</details>

### [414] [Generative AI in Education: Student Skills and Lecturer Roles](https://arxiv.org/abs/2504.19673)
*Stefanie Krause,Ashish Dalvi,Syed Khubaib Zaidi*

Main category: cs.CY

TLDR: 研究探讨了生成式AI（如ChatGPT）在教育中的影响，识别了学生需具备的关键能力，并提出了教师整合GenAI的策略。


<details>
  <summary>Details</summary>
Motivation: 评估GenAI在教育中的潜力与挑战，为学生和教师提供实用指导。

Method: 混合方法：文献综述和定量调查（130名学生）。

Result: 文献综述确定了14项学生关键技能（如AI素养、批判性思维），调查揭示了提示工程和偏见意识等不足。教师策略中，GenAI整合和课程设计最受重视。

Conclusion: 需将GenAI融入教育，注重实践学习与伦理政策平衡，推动公平访问和全球研究。

Abstract: Generative Artificial Intelligence (GenAI) tools such as ChatGPT are emerging
as a revolutionary tool in education that brings both positive aspects and
challenges for educators and students, reshaping how learning and teaching are
approached. This study aims to identify and evaluate the key competencies
students need to effectively engage with GenAI in education and to provide
strategies for lecturers to integrate GenAI into teaching practices. The study
applied a mixed method approach with a combination of a literature review and a
quantitative survey involving 130 students from South Asia and Europe to obtain
its findings. The literature review identified 14 essential student skills for
GenAI engagement, with AI literacy, critical thinking, and ethical AI practices
emerging as the most critical. The student survey revealed gaps in prompt
engineering, bias awareness, and AI output management. In our study of lecturer
strategies, we identified six key areas, with GenAI Integration and Curriculum
Design being the most emphasised. Our findings highlight the importance of
incorporating GenAI into education. While literature prioritized ethics and
policy development, students favour hands-on, project-based learning and
practical AI applications. To foster inclusive and responsible GenAI adoption,
institutions should ensure equitable access to GenAI tools, establish clear
academic integrity policies, and advocate for global GenAI research
initiatives.

</details>

### [415] [Mitigating Societal Cognitive Overload in the Age of AI: Challenges and Directions](https://arxiv.org/abs/2504.19990)
*Salem Lahlou*

Main category: cs.CY

TLDR: 论文探讨了AI时代信息过载对社会认知的负面影响，提出减轻认知过载对应对AI潜在风险的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于信息爆炸和AI复杂性导致的社会认知过载，及其对人类福祉和社会韧性的威胁。

Method: 通过分析AI加剧认知过载的机制（如信息泛滥、算法操纵等），重新构建AI安全讨论框架。

Result: 研究发现认知过载是连接短期危害与长期风险的桥梁，需从制度、研究和政策层面应对。

Conclusion: 结论指出需以认知过载为视角探索人机对齐的未来路径，而非提供固定解决方案。

Abstract: Societal cognitive overload, driven by the deluge of information and
complexity in the AI age, poses a critical challenge to human well-being and
societal resilience. This paper argues that mitigating cognitive overload is
not only essential for improving present-day life but also a crucial
prerequisite for navigating the potential risks of advanced AI, including
existential threats. We examine how AI exacerbates cognitive overload through
various mechanisms, including information proliferation, algorithmic
manipulation, automation anxieties, deregulation, and the erosion of meaning.
The paper reframes the AI safety debate to center on cognitive overload,
highlighting its role as a bridge between near-term harms and long-term risks.
It concludes by discussing potential institutional adaptations, research
directions, and policy considerations that arise from adopting an
overload-resilient perspective on human-AI alignment, suggesting pathways for
future exploration rather than prescribing definitive solutions.

</details>

<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [416] [Global Climate Model Bias Correction Using Deep Learning](https://arxiv.org/abs/2504.19145)
*Abhishek Pasula,Deepak N. Subramani*

Main category: physics.ao-ph

TLDR: 论文提出了一种基于深度学习的偏差校正方法，用于修正气候模型在孟加拉湾的海表温度预测，相比传统方法减少了15%的均方根误差。


<details>
  <summary>Details</summary>
Motivation: 气候模型（如CMIP）在孟加拉湾的海表温度预测中存在显著偏差，需要更准确的校正方法以提高预测可靠性。

Method: 使用三种深度神经网络架构（UNet、双向LSTM和ConvLSTM）进行偏差校正，并与线性回归和EDCDF方法对比。

Result: UNet架构在去除气候学影响后表现最佳，相比EDCDF方法减少了15%的均方根误差。

Conclusion: 深度学习模型能有效校正气候模型的偏差，UNet架构尤其适用于海表温度的偏差校正。

Abstract: Climate change affects ocean temperature, salinity and sea level, impacting
monsoons and ocean productivity. Future projections by Global Climate Models
based on shared socioeconomic pathways from the Coupled Model Intercomparison
Project (CMIP) are widely used to understand the effects of climate change.
However, CMIP models have significant bias compared to reanalysis in the Bay of
Bengal for the time period when both projections and reanalysis are available.
For example, there is a 1.5C root mean square error (RMSE) in the sea surface
temperature (SST) projections of the climate model CNRM-CM6 compared to the
Ocean Reanalysis System (ORAS5). We develop a suite of data-driven deep
learning models for bias correction of climate model projections and apply it
to correct SST projections of the Bay of Bengal. We propose the use of three
different deep neural network architectures: convolutional encoder-decoder
UNet, Bidirectional LSTM and ConvLSTM. We also use a baseline linear regression
model and the Equi-Distant Cumulative Density Function (EDCDF) bias correction
method for comparison and evaluating the impact of the new deep learning
models. All bias correction models are trained using pairs of monthly CMIP6
projections and the corresponding month's ORAS5 as input and output. Historical
data (1950-2014) and future projection data (2015-2020) of CNRM-CM6 are used
for training and validation, including hyperparameter tuning. Testing is
performed on future projection data from 2021 to 2024. Detailed analysis of the
three deep neural models has been completed. We found that the UNet
architecture trained using a climatology-removed CNRM-CM6 projection as input
and climatology-removed ORAS5 as output gives the best bias-corrected
projections. Our novel deep learning-based method for correcting CNRM-CM6 data
has a 15% reduction in RMSE compared EDCDF.

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [417] [On the Prevalence and Usage of Commit Signing on GitHub: A Longitudinal and Cross-Domain Study](https://arxiv.org/abs/2504.19215)
*Anupam Sharma,Sreyashi Karmakar,Gayatri Priyadarsini Kancherla,Abhishek Bichhawat*

Main category: cs.SE

TLDR: GitHub上的代码提交容易被伪造，目前仅依赖提交签名作为防御手段。研究发现，60个常用仓库中仅约10%的提交经过验证，安全相关仓库的开发者更注重签名。不同Git客户端的签名便利性不同，GitKraken最方便，GitHub Web最易验证。此外，GitHub处理未验证邮箱的方式存在问题。低验证率可能源于意识不足、设置复杂或密钥管理困难。最后，提出了基于GitHub Events API的提交所有权验证方法。


<details>
  <summary>Details</summary>
Motivation: GitHub作为广泛使用的代码托管平台，存在提交伪造漏洞，可能导致恶意代码注入。目前仅依赖提交签名作为防御手段，但其使用情况尚不明确。

Method: 构建框架提取GitHub仓库的提交元数据，分析60个开源仓库（涵盖四个领域）中已验证提交的占比，并评估不同Git客户端的签名便利性。

Result: 仅约10%的提交经过验证，安全相关仓库的开发者更注重签名。GitKraken签名最方便，GitHub Web验证最易用。GitHub处理未验证邮箱的方式存在问题。

Conclusion: 低验证率可能源于意识不足、设置复杂或密钥管理困难。提出了基于GitHub Events API的提交所有权验证方法，以解决提交伪造问题。

Abstract: GitHub is one of the most widely used public code development platform.
However, the code hosted publicly on the platform is vulnerable to commit
spoofing that allows an adversary to introduce malicious code or commits into
the repository by spoofing the commit metadata to indicate that the code was
added by a legitimate user. The only defense that GitHub employs is the process
of commit signing, which indicates whether a commit is from a valid source or
not based on the keys registered by the users.
  In this work, we perform an empirical analysis of how prevalent is the use of
commit signing in commonly used GitHub repositories. To this end, we build a
framework that allows us to extract the metadata of all prior commits of a
GitHub repository, and identify what commits in the repository are verified. We
analyzed 60 open-source repositories belonging to four different domains -- web
development, databases, machine learning and security -- using our framework
and study the presence of verified commits in each repositories over five
years. Our analysis shows that only ~10% of all the commits in these 60
repositories are verified. Developers committing code to security-related
repositories are much more vigilant when it comes to signing commits by users.
  We also analyzed different Git clients for the ease of commit signing, and
found that GitKraken provides the most convenient way of commit signing whereas
GitHub Web provides the most accessible way for verifying commits. During our
analysis, we also identified an unexpected behavior in how GitHub handles
unverified emails in user accounts preventing legitimate owner to use the email
address. We believe that the low number of verified commits may be due to lack
of awareness, difficulty in setup and key management. Finally, we propose ways
to identify commit ownership based on GitHub's Events API addressing the issue
of commit spoofing.

</details>

### [418] [Technical Challenges in Maintaining Tax Prep Software with Large Language Models](https://arxiv.org/abs/2504.18693)
*Sina Gogani-Khiabani,Varsha Dewangan,Nina Olson,Ashutosh Trivedi,Saeid Tizpaz-Niari*

Main category: cs.SE

TLDR: 论文探讨了如何利用大型语言模型（LLMs）自动从IRS出版物中提取代码差异并整合到税务软件中，以解决当前税务软件维护耗时且易错的问题。


<details>
  <summary>Details</summary>
Motivation: 税务法规的动态变化使得税务软件维护变得复杂且容易出错，目前的方法依赖人工分析和专家解读，效率低下。

Method: 研究利用LLMs（如ChatGPT和Llama）自动从IRS出版物中提取代码差异，并将其整合到现有代码中。

Result: 通过LLMs自动提取和整合代码差异，有望实现税务软件维护的自动化。

Conclusion: 利用LLMs自动化税务软件维护是可行的，能够显著提高效率和准确性。

Abstract: As the US tax law evolves to adapt to ever-changing politico-economic
realities, tax preparation software plays a significant role in helping
taxpayers navigate these complexities. The dynamic nature of tax regulations
poses a significant challenge to accurately and timely maintaining tax software
artifacts. The state-of-the-art in maintaining tax prep software is
time-consuming and error-prone as it involves manual code analysis combined
with an expert interpretation of tax law amendments. We posit that the rigor
and formality of tax amendment language, as expressed in IRS publications,
makes it amenable to automatic translation to executable specifications (code).
Our research efforts focus on identifying, understanding, and tackling
technical challenges in leveraging Large Language Models (LLMs), such as
ChatGPT and Llama, to faithfully extract code differentials from IRS
publications and automatically integrate them with the prior version of the
code to automate tax prep software maintenance.

</details>

### [419] [Can We Enhance Bug Report Quality Using LLMs?: An Empirical Study of LLM-Based Bug Report Generation](https://arxiv.org/abs/2504.18804)
*Jagrit Acharya,Gouri Ginde*

Main category: cs.SE

TLDR: 论文探讨了通过指令微调的大型语言模型（LLMs）将非结构化的缺陷报告自动转化为结构化报告的能力，Qwen 2.5表现最佳。


<details>
  <summary>Details</summary>
Motivation: 缺陷报告中模糊或不完整的信息可能导致修复延迟和额外人工成本，研究旨在通过自动化提升效率。

Method: 评估了Qwen 2.5、Mistral、Llama 3.2和ChatGPT-4o在CTQRS、ROUGE等指标上的表现。

Result: Qwen 2.5在CTQRS得分上以77%领先，且在特定字段（如重现步骤）表现突出。

Conclusion: 指令微调LLMs可有效自动化缺陷报告生成，减少开发者的手动工作。

Abstract: Bug reports contain the information developers need to triage and fix
software bugs. However, unclear, incomplete, or ambiguous information may lead
to delays and excessive manual effort spent on bug triage and resolution. In
this paper, we explore whether Instruction fine-tuned Large Language Models
(LLMs) can automatically transform casual, unstructured bug reports into
high-quality, structured bug reports adhering to a standard template. We
evaluate three open-source instruction-tuned LLMs (\emph{Qwen 2.5, Mistral, and
Llama 3.2}) against ChatGPT-4o, measuring performance on established metrics
such as CTQRS, ROUGE, METEOR, and SBERT. Our experiments show that fine-tuned
Qwen 2.5 achieves a CTQRS score of \textbf{77%}, outperforming both fine-tuned
Mistral (\textbf{71%}), Llama 3.2 (\textbf{63%}) and ChatGPT in 3-shot learning
(\textbf{75%}). Further analysis reveals that Llama 3.2 shows higher accuracy
of detecting missing fields particularly Expected Behavior and Actual Behavior,
while Qwen 2.5 demonstrates superior performance in capturing
Steps-to-Reproduce, with an F1 score of 76%. Additional testing of the models
on other popular projects (e.g., Eclipse, GCC) demonstrates that our approach
generalizes well, achieving up to \textbf{70%} CTQRS in unseen projects' bug
reports. These findings highlight the potential of instruction fine-tuning in
automating structured bug report generation, reducing manual effort for
developers and streamlining the software maintenance process.

</details>

### [420] [Large Language Models are Qualified Benchmark Builders: Rebuilding Pre-Training Datasets for Advancing Code Intelligence Tasks](https://arxiv.org/abs/2504.19444)
*Kang Yang,Xinjun Mao,Shangwen Wang,Yanlin Wang,Tanghaoran Zhang,Bo Lin,Yihao Qin,Zhang Zhang,Yao Lu,Kamal Al-Sabahi*

Main category: cs.SE

TLDR: 研究探讨用LLM生成的代码注释替代人工注释，以提升预训练数据质量，并通过新评估任务验证其效果。


<details>
  <summary>Details</summary>
Motivation: 人工注释易过时，影响模型性能；LLM生成的注释质量高，可能改进数据集。

Method: 提出两种无参考评估任务（代码-注释不一致检测和语义代码搜索），并用LLM生成注释重建数据集。

Result: LLM生成的注释语义一致性优于人工注释，模型在LLM增强数据上表现更优。

Conclusion: LLM可替代人工注释重建数据集，推动代码智能发展。

Abstract: Pre-trained code models rely heavily on high-quality pre-training data,
particularly human-written reference comments that bridge code and natural
language. However, these comments often become outdated as software evolves,
degrading model performance. Large language models (LLMs) excel at generating
high-quality code comments. We investigate whether replacing human-written
comments with LLM-generated ones improves pre-training datasets. Since standard
metrics cannot assess reference comment quality, we propose two novel
reference-free evaluation tasks: code-comment inconsistency detection and
semantic code search. Results show that LLM-generated comments are more
semantically consistent with code than human-written ones, as confirmed by
manual evaluation. Leveraging this finding, we rebuild the CodeSearchNet
dataset with LLM-generated comments and re-pre-train CodeT5. Evaluations
demonstrate that models trained on LLM-enhanced data outperform those using
original human comments in code summarization, generation, and translation
tasks. This work validates rebuilding pre-training datasets with LLMs to
advance code intelligence, challenging the traditional reliance on human
reference comments.

</details>

### [421] [Test It Before You Trust It: Applying Software Testing for Trustworthy In-context Learning](https://arxiv.org/abs/2504.18827)
*Teeradaj Racharak,Chaiyong Ragkhitwetsagul,Chommakorn Sontesadisai,Thanwadee Sunetnanta*

Main category: cs.SE

TLDR: 论文提出了一种名为MMT4NL的软件测试框架，用于评估大型语言模型（LLMs）在上下文学习（ICL）中的可信度，通过对抗性扰动和软件测试技术揭示其潜在缺陷。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在上下文学习中表现出强大的适应性，但它们对细微的对抗性扰动仍然脆弱，且在语言变化下行为不可预测。

Method: MMT4NL框架基于软件测试原则，通过生成对抗性示例来量化ICL中的缺陷，并将其视为软件进行功能验证。

Result: 实验在情感分析和问答任务中应用MMT4NL，成功揭示了当前先进LLMs中的多种语言缺陷。

Conclusion: MMT4NL为评估LLMs的可信度提供了一种有效方法，强调了软件测试在语言模型验证中的重要性。

Abstract: In-context learning (ICL) has emerged as a powerful capability of large
language models (LLMs), enabling them to perform new tasks based on a few
provided examples without explicit fine-tuning. Despite their impressive
adaptability, these models remain vulnerable to subtle adversarial
perturbations and exhibit unpredictable behavior when faced with linguistic
variations. Inspired by software testing principles, we introduce a software
testing-inspired framework, called MMT4NL, for evaluating the trustworthiness
of in-context learning by utilizing adversarial perturbations and software
testing techniques. It includes diverse evaluation aspects of linguistic
capabilities for testing the ICL capabilities of LLMs. MMT4NL is built around
the idea of crafting metamorphic adversarial examples from a test set in order
to quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is
to treat any LLM as software and validate its functionalities just like testing
the software. Finally, we demonstrate applications of MMT4NL on the sentiment
analysis and question-answering tasks. Our experiments could reveal various
linguistic bugs in state-of-the-art LLMs.

</details>

### [422] [Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial Attacks Using LLM-as-a-Judge](https://arxiv.org/abs/2504.19730)
*Wenhan Mu,Ling Xu,Shuren Pei,Le Mi,Huichi Zhou*

Main category: cs.SE

TLDR: 论文提出EP-Shield框架，用于评估和净化标识符替换攻击，通过自然性感知推理提升对抗样本质量。


<details>
  <summary>Details</summary>
Motivation: 现有标识符替换攻击生成的对抗样本存在不自然代码模式，容易被检测。

Method: 使用LLM-as-a-Judge评估代码自然性，识别并净化对抗代码。

Result: EP-Shield在对抗微调上提升83.36%，且设计轻量（7B参数），性能媲美GPT-4。

Conclusion: EP-Shield能有效检测和净化对抗样本，提升模型安全性。

Abstract: The widespread adoption of code language models in software engineering tasks
has exposed vulnerabilities to adversarial attacks, especially the identifier
substitution attacks. Although existing identifier substitution attackers
demonstrate high success rates, they often produce adversarial examples with
unnatural code patterns. In this paper, we systematically assess the quality of
adversarial examples using LLM-as-a-Judge. Our analysis reveals that over 80%
of adversarial examples generated by state-of-the-art identifier substitution
attackers (e.g., ALERT) are actually detectable. Based on this insight, we
propose EP-Shield, a unified framework for evaluating and purifying identifier
substitution attacks via naturalness-aware reasoning. Specifically, we first
evaluate the naturalness of code and identify the perturbed adversarial code,
then purify it so that the victim model can restore correct prediction.
Extensive experiments demonstrate the superiority of EP-Shield over adversarial
fine-tuning (up to 83.36% improvement) and its lightweight design 7B
parameters) with GPT-4-level performance.

</details>

### [423] [Why you shouldn't fully trust ChatGPT: A synthesis of this AI tool's error rates across disciplines and the software engineering lifecycle](https://arxiv.org/abs/2504.18858)
*Vahid Garousi*

Main category: cs.SE

TLDR: 该研究通过多源文献综述（MLR）量化了ChatGPT在不同领域和软件开发生命周期（SDLC）阶段的错误率，发现错误率因领域、任务和模型版本而异，强调了持续评估和人工监督的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管ChatGPT等大型语言模型（LLM）在多个领域广泛应用，但其可靠性仍存疑，尤其是在不同领域和SDLC阶段的错误率问题。

Method: 采用多源文献综述（MLR）方法，收集截至2025年的学术研究、报告和灰色文献数据，按领域和SE阶段分组，并通过箱线图可视化错误分布。

Result: 错误率因领域和版本而异，例如医疗领域为8%-83%，商业和经济领域从GPT-3.5的50%降至GPT-4的15-20%。软件工程中，需求与设计阶段错误率较低（5-20%），而编码、测试和维护阶段较高（10-50%）。

Conclusion: 尽管ChatGPT有所改进，但其错误率仍不可忽视，尤其是在关键领域。完全依赖而无人工监督存在风险，需持续评估和验证以确保可靠性。

Abstract: Context: ChatGPT and other large language models (LLMs) are widely used
across healthcare, business, economics, engineering, and software engineering
(SE). Despite their popularity, concerns persist about their reliability,
especially their error rates across domains and the software development
lifecycle (SDLC).
  Objective: This study synthesizes and quantifies ChatGPT's reported error
rates across major domains and SE tasks aligned with SDLC phases. It provides
an evidence-based view of where ChatGPT excels, where it fails, and how
reliability varies by task, domain, and model version (GPT-3.5, GPT-4,
GPT-4-turbo, GPT-4o).
  Method: A Multivocal Literature Review (MLR) was conducted, gathering data
from academic studies, reports, benchmarks, and grey literature up to 2025.
Factual, reasoning, coding, and interpretive errors were considered. Data were
grouped by domain and SE phase and visualized using boxplots to show error
distributions.
  Results: Error rates vary across domains and versions. In healthcare, rates
ranged from 8% to 83%. Business and economics saw error rates drop from ~50%
with GPT-3.5 to 15-20% with GPT-4. Engineering tasks averaged 20-30%.
Programming success reached 87.5%, though complex debugging still showed over
50% errors. In SE, requirements and design phases showed lower error rates
(~5-20%), while coding, testing, and maintenance phases had higher variability
(10-50%). Upgrades from GPT-3.5 to GPT-4 improved reliability.
  Conclusion: Despite improvements, ChatGPT still exhibits non-negligible error
rates varying by domain, task, and SDLC phase. Full reliance without human
oversight remains risky, especially in critical settings. Continuous evaluation
and critical validation are essential to ensure reliability and
trustworthiness.

</details>

### [424] [VeriDebug: A Unified LLM for Verilog Debugging via Contrastive Embedding and Guided Correction](https://arxiv.org/abs/2504.19099)
*Ning Wang,Bingkun Yao,Jie Zhou,Yuchen Hu,Xi Wang,Nan Guan,Zhe Jiang*

Main category: cs.SE

TLDR: VeriDebug是一种结合对比表示和引导校正的自动化Verilog调试方法，显著提升了调试准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多种编程语言调试中表现优异，但其在Verilog调试中的应用尚未充分探索。

Method: VeriDebug采用基于嵌入的技术检索内部信息并进行修复，通过共享参数空间统一检测与校正。

Result: VeriDebugLoc模型在修复准确率（Acc1）上达到64.7%，远超现有开源方法（11.3%）和GPT-3.5-turbo（36.6%）。

Conclusion: VeriDebug为Verilog调试提供了更高效的解决方案，优于传统方法和现有模型。

Abstract: Large Language Models (LLMs) have demonstrated remarkable potential in
debugging for various programming languages. However, the application of LLMs
to Verilog debugging remains insufficiently explored. Here, we present
VeriDebug, an approach that integrates contrastive representation and guided
correction capabilities for automated Verilog debugging. Unlike existing
methods, VeriDebug employs an embedding-based technique to accurately retrieve
internal information, followed by bug-fixing. VeriDebug unifies Verilog bug
detection and correction through a shared parameter space. By simultaneously
learning bug patterns and fixes, it streamlines debugging via contrastive
embedding and guided correction. Empirical results show the efficacy of
VeriDebug in enhancing Verilog debugging. Our VeriDebugLoc, Type model achieves
64.7 accuracy in bug fixing (Acc1), a significant improvement from the existing
open-source SOTAs 11.3. This performance not only outperforms open-source
alternatives but also exceeds larger closed-source models like GPT-3.5-turbo
(36.6), offering a more accurate alternative to conventional debugging methods.

</details>

### [425] [From Inductive to Deductive: LLMs-Based Qualitative Data Analysis in Requirements Engineering](https://arxiv.org/abs/2504.19384)
*Syed Tauhid Ullah Shah,Mohamad Hussein,Ann Barcomb,Mohammad Moshirpour*

Main category: cs.SE

TLDR: 论文探讨了使用大型语言模型（如GPT-4、Mistral和LLaMA-2）改进需求工程中的定性数据分析（QDA），发现GPT-4在演绎标注任务中表现优异，与人类分析师一致性高，而零标注任务表现有限。


<details>
  <summary>Details</summary>
Motivation: 传统QDA方法耗时且依赖人工，研究旨在利用LLMs减少人工负担并保持标注质量。

Method: 评估LLMs在归纳（零标注）和演绎（一标注、少标注）任务中的表现，使用详细提示提升准确性。

Result: GPT-4在演绎任务中与人类分析师一致性高（Cohen's Kappa>0.7），零标注任务表现较差。

Conclusion: LLMs（尤其是GPT-4）可有效支持QDA，减少人工负担，同时提供结构化标签以辅助系统设计。

Abstract: Requirements Engineering (RE) is essential for developing complex and
regulated software projects. Given the challenges in transforming stakeholder
inputs into consistent software designs, Qualitative Data Analysis (QDA)
provides a systematic approach to handling free-form data. However, traditional
QDA methods are time-consuming and heavily reliant on manual effort. In this
paper, we explore the use of Large Language Models (LLMs), including GPT-4,
Mistral, and LLaMA-2, to improve QDA tasks in RE. Our study evaluates LLMs'
performance in inductive (zero-shot) and deductive (one-shot, few-shot)
annotation tasks, revealing that GPT-4 achieves substantial agreement with
human analysts in deductive settings, with Cohen's Kappa scores exceeding 0.7,
while zero-shot performance remains limited. Detailed, context-rich prompts
significantly improve annotation accuracy and consistency, particularly in
deductive scenarios, and GPT-4 demonstrates high reliability across repeated
runs. These findings highlight the potential of LLMs to support QDA in RE by
reducing manual effort while maintaining annotation quality. The structured
labels automatically provide traceability of requirements and can be directly
utilized as classes in domain models, facilitating systematic software design.

</details>

### [426] [LLMs for Engineering: Teaching Models to Design High Powered Rockets](https://arxiv.org/abs/2504.19394)
*Toby Simonds*

Main category: cs.SE

TLDR: 论文研究了LLMs在火箭设计中的应用，发现其基础能力虽强，但需结合强化学习才能超越人类表现。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在物理工程领域的潜力，填补其在软件工程以外应用的研究空白。

Method: 通过RocketBench基准测试，结合高保真火箭模拟，评估LLMs在目标高度优化和精准着陆任务中的表现，并引入强化学习提升性能。

Result: 基础LLMs表现有限，但结合强化学习的7B参数模型超越了现有模型和人类专家。

Conclusion: RL训练的LLMs可作为复杂工程优化的有效工具，拓展其应用领域。

Abstract: Large Language Models (LLMs) have transformed software engineering, but their
application to physical engineering domains remains underexplored. This paper
evaluates LLMs' capabilities in high-powered rocketry design through
RocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations.
We test models on two increasingly complex design tasks: target altitude
optimization and precision landing challenges. Our findings reveal that while
state-of-the-art LLMs demonstrate strong baseline engineering knowledge, they
struggle to iterate on their designs when given simulation results and
ultimately plateau below human performance levels. However, when enhanced with
reinforcement learning (RL), we show that a 7B parameter model outperforms both
SoTA foundation models and human experts. This research demonstrates that
RL-trained LLMs can serve as effective tools for complex engineering
optimization, potentially transforming engineering domains beyond software
development.

</details>

<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [427] [Explainable Deep-Learning Based Potentially Hazardous Asteroids Classification Using Graph Neural Networks](https://arxiv.org/abs/2504.18605)
*Baimam Boukar Jean Jacques*

Main category: astro-ph.EP

TLDR: 论文提出了一种基于图神经网络（GNN）的方法，用于分类潜在危险小行星（PHAs），解决了传统方法忽视小行星间动态关系的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法在分类潜在危险小行星时忽略了小行星间的动态关系，而GNN方法能够更好地捕捉这些关系，为行星防御和深空导航提供支持。

Method: 使用NASA数据集（958,524条记录），将小行星建模为节点（轨道和物理特征），边表示相似性，并应用合成少数类过采样技术（SMOTE）解决类别不平衡问题。

Result: 模型整体准确率为99%，AUC为0.99，对危险小行星的召回率为78%，F1分数为37%。特征重要性分析显示反照率、近日点距离和半长轴是关键预测因子。

Conclusion: 该框架为行星防御任务提供了可解释且可扩展的解决方案，并证实了AI在未来自主导航任务（如NASA的NEO Surveyor和ESA的Ramses）中的潜力。

Abstract: Classifying potentially hazardous asteroids (PHAs) is crucial for planetary
defense and deep space navigation, yet traditional methods often overlook the
dynamical relationships among asteroids. We introduce a Graph Neural Network
(GNN) approach that models asteroids as nodes with orbital and physical
features, connected by edges representing their similarities, using a NASA
dataset of 958,524 records. Despite an extreme class imbalance with only 0.22%
of the dataset with the hazardous label, our model achieves an overall accuracy
of 99% and an AUC of 0.99, with a recall of 78% and an F1-score of 37% for
hazardous asteroids after applying the Synthetic Minority Oversampling
Technique. Feature importance analysis highlights albedo, perihelion distance,
and semi-major axis as main predictors. This framework supports planetary
defense missions and confirms AI's potential in enabling autonomous navigation
for future missions such as NASA's NEO Surveyor and ESA's Ramses, offering an
interpretable and scalable solution for asteroid hazard assessment.

</details>

<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [428] [PhenoAssistant: A Conversational Multi-Agent AI System for Automated Plant Phenotyping](https://arxiv.org/abs/2504.19818)
*Feng Chen,Ilias Stogiannidis,Andrew Wood,Danilo Bueno,Dominic Williams,Fraser Macfarlane,Bruce Grieve,Darren Wells,Jonathan A. Atkinson,Malcolm J. Hawkesford,Stephen A. Rolfe,Tracy Lawson,Tony Pridmore,Mario Valerio Giuffrida,Sotirios A. Tsaftaris*

Main category: cs.MA

TLDR: PhenoAssistant是一个基于AI的系统，通过自然语言交互简化植物表型分析，降低技术门槛。


<details>
  <summary>Details</summary>
Motivation: 解决现有植物表型分析工具复杂、难以维护且对非技术用户不友好的问题。

Method: 利用大型语言模型协调工具包，支持表型提取、数据可视化和模型训练。

Result: 通过案例研究和评估任务验证了系统的有效性。

Conclusion: PhenoAssistant展示了AI方法在植物生物学中普及的潜力。

Abstract: Plant phenotyping increasingly relies on (semi-)automated image-based
analysis workflows to improve its accuracy and scalability. However, many
existing solutions remain overly complex, difficult to reimplement and
maintain, and pose high barriers for users without substantial computational
expertise. To address these challenges, we introduce PhenoAssistant: a
pioneering AI-driven system that streamlines plant phenotyping via intuitive
natural language interaction. PhenoAssistant leverages a large language model
to orchestrate a curated toolkit supporting tasks including automated phenotype
extraction, data visualisation and automated model training. We validate
PhenoAssistant through several representative case studies and a set of
evaluation tasks. By significantly lowering technical hurdles, PhenoAssistant
underscores the promise of AI-driven methodologies to democratising AI adoption
in plant biology.

</details>

### [429] [Diffusion Stochastic Learning Over Adaptive Competing Networks](https://arxiv.org/abs/2504.19635)
*Yike Zhao,Haoyuan Cai,Ali H. Sayed*

Main category: cs.MA

TLDR: 本文研究了一种随机动态博弈，涉及两个竞争团队，每个团队由协作代理网络组成。团队间目标可能冲突，如零和博弈。提出了扩散学习算法解决两类网络博弈，并分析了算法的稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究团队间竞争动态博弈，探索协作代理在对抗性环境中的信息共享与策略适应。

Method: 提出扩散学习算法，分别针对弱交叉团队子图交互的零和博弈和强交互的非零和博弈。

Result: 在合理假设下分析了算法的稳定性，并通过实验验证了理论结果。

Conclusion: 扩散学习算法能有效解决团队竞争博弈问题，实验验证了其性能。

Abstract: This paper studies a stochastic dynamic game between two competing teams,
each consisting of a network of collaborating agents. Unlike fully cooperative
settings, where all agents share a common objective, each team in this game
aims to minimize its own distinct objective. In the adversarial setting, their
objectives could be conflicting as in zero-sum games. Throughout the
competition, agents share strategic information within their own team while
simultaneously inferring and adapting to the strategies of the opposing team.
We propose diffusion learning algorithms to address two important classes of
this network game: i) a zero-sum game characterized by weak cross-team subgraph
interactions, and ii) a general non-zero-sum game exhibiting strong cross-team
subgraph interactions. We analyze the stability performance of the proposed
algorithms under reasonable assumptions and illustrate the theoretical results
through experiments on Cournot team competition and decentralized GAN training.

</details>

<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [430] [Enhancing short-term traffic prediction by integrating trends and fluctuations with attention mechanism](https://arxiv.org/abs/2504.19967)
*Adway Das,Agnimitra Sengupta,S. Ilgin Guler*

Main category: cs.ET

TLDR: 提出了一种混合深度学习框架，结合长期趋势和短期波动信息，通过并行处理和注意力机制提升交通流量预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 交通流量预测对智能交通系统至关重要，但现有深度学习模型因架构限制难以同时捕捉长期趋势和短期波动。

Method: 采用并行处理的双输入特征设计，结合Bahdanau注意力机制，选择性关注关键时间步。

Result: 实验表明，该方法显著提高了多时间尺度的预测拟合度，注意力机制尤其提升了短期预测精度。

Conclusion: 该框架通过增强预测模型的鲁棒性和精确性，有助于缓解拥堵和优化城市交通规划。

Abstract: Traffic flow prediction is a critical component of intelligent transportation
systems, yet accurately forecasting traffic remains challenging due to the
interaction between long-term trends and short-term fluctuations. Standard deep
learning models often struggle with these challenges because their
architectures inherently smooth over fine-grained fluctuations while focusing
on general trends. This limitation arises from low-pass filtering effects, gate
biases favoring stability, and memory update mechanisms that prioritize
long-term information retention. To address these shortcomings, this study
introduces a hybrid deep learning framework that integrates both long-term
trend and short-term fluctuation information using two input features processed
in parallel, designed to capture complementary aspects of traffic flow
dynamics. Further, our approach leverages attention mechanisms, specifically
Bahdanau attention, to selectively focus on critical time steps within traffic
data, enhancing the model's ability to predict congestion and other transient
phenomena. Experimental results demonstrate that features learned from both
branches are complementary, significantly improving the goodness-of-fit
statistics across multiple prediction horizons compared to a baseline model.
Notably, the attention mechanism enhances short-term forecast accuracy by
directly targeting immediate fluctuations, though challenges remain in fully
integrating long-term trends. This framework can contribute to more effective
congestion mitigation and urban mobility planning by advancing the robustness
and precision of traffic prediction models.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [431] [Surgeons vs. Computer Vision: A comparative analysis on surgical phase recognition capabilities](https://arxiv.org/abs/2504.18954)
*Marco Mezzina,Pieter De Backer,Tom Vercauteren,Matthew Blaschko,Alexandre Mottrie,Tinne Tuytelaars*

Main category: eess.IV

TLDR: 研究探讨了自动化手术阶段识别（SPR）在机器人辅助部分肾切除术（RAPN）中的应用，比较了专家与AI在有无时间上下文情况下的表现。


<details>
  <summary>Details</summary>
Motivation: 填补以往研究对非线性手术流程和时间上下文影响的空白，提升SPR的准确性和实用性。

Method: 通过专家和初学者对单帧和视频片段进行分类，结合AI模型训练和对比分析。

Result: 视频片段和视觉标志物提高了分类准确性，专家表现优于初学者，AI与专家表现相当，时间上下文进一步提升了性能。

Conclusion: SPR对专家和AI均具挑战性，时间信息和视觉标志物是关键，未来自动化SPR将依赖这些因素。

Abstract: Purpose: Automated Surgical Phase Recognition (SPR) uses Artificial
Intelligence (AI) to segment the surgical workflow into its key events,
functioning as a building block for efficient video review, surgical education
as well as skill assessment. Previous research has focused on short and linear
surgical procedures and has not explored if temporal context influences
experts' ability to better classify surgical phases. This research addresses
these gaps, focusing on Robot-Assisted Partial Nephrectomy (RAPN) as a highly
non-linear procedure. Methods: Urologists of varying expertise were grouped and
tasked to indicate the surgical phase for RAPN on both single frames and video
snippets using a custom-made web platform. Participants reported their
confidence levels and the visual landmarks used in their decision-making. AI
architectures without and with temporal context as trained and benchmarked on
the Cholec80 dataset were subsequently trained on this RAPN dataset. Results:
Video snippets and presence of specific visual landmarks improved phase
classification accuracy across all groups. Surgeons displayed high confidence
in their classifications and outperformed novices, who struggled discriminating
phases. The performance of the AI models is comparable to the surgeons in the
survey, with improvements when temporal context was incorporated in both cases.
Conclusion: SPR is an inherently complex task for expert surgeons and computer
vision, where both perform equally well when given the same context.
Performance increases when temporal information is provided. Surgical tools and
organs form the key landmarks for human interpretation and are expected to
shape the future of automated SPR.

</details>

### [432] [Reservoir-enhanced Segment Anything Model for Subsurface Diagnosis](https://arxiv.org/abs/2504.18802)
*Xiren Zhou,Shikang Liu,Xinyu Yan,Yizhan Fan,Xiangyu Wang,Yu Kang,Jian Cheng,Huanhuan Chen*

Main category: eess.IV

TLDR: 论文提出了一种名为Res-SAM的创新框架，用于精确检测地下异常（如裂缝和空洞），结合视觉和电磁波特性，实现高精度检测（>85%），且资源高效。


<details>
  <summary>Details</summary>
Motivation: 城市地下异常（如裂缝和空洞）对基础设施构成威胁，但现有GPR技术因数据标签不足、地下条件多变和目标边界模糊而难以准确检测。

Method: 提出Res-SAM框架，结合视觉判别和电磁波变化特性，通过最小提示识别候选异常区域，并进一步分析局部GPR数据中的波变化信息，实现精确提取和分类。

Result: 实验表明，Res-SAM检测准确率超过85%，优于现有技术，且仅需少量非目标数据和简单人工交互。

Conclusion: Res-SAM为快速、资源高效的地下异常检测提供了可扩展的解决方案，提升了城市安全监测效率并降低了成本和人工需求。

Abstract: Urban roads and infrastructure, vital to city operations, face growing
threats from subsurface anomalies like cracks and cavities. Ground Penetrating
Radar (GPR) effectively visualizes underground conditions employing
electromagnetic (EM) waves; however, accurate anomaly detection via GPR remains
challenging due to limited labeled data, varying subsurface conditions, and
indistinct target boundaries. Although visually image-like, GPR data
fundamentally represent EM waves, with variations within and between waves
critical for identifying anomalies. Addressing these, we propose the
Reservoir-enhanced Segment Anything Model (Res-SAM), an innovative framework
exploiting both visual discernibility and wave-changing properties of GPR data.
Res-SAM initially identifies apparent candidate anomaly regions given minimal
prompts, and further refines them by analyzing anomaly-induced changing
information within and between EM waves in local GPR data, enabling precise and
complete anomaly region extraction and category determination. Real-world
experiments demonstrate that Res-SAM achieves high detection accuracy (>85%)
and outperforms state-of-the-art. Notably, Res-SAM requires only minimal
accessible non-target data, avoids intensive training, and incorporates simple
human interaction to enhance reliability. Our research provides a scalable,
resource-efficient solution for rapid subsurface anomaly detection across
diverse environments, improving urban safety monitoring while reducing manual
effort and computational cost.

</details>

### [433] [Low-Rank Adaptive Structural Priors for Generalizable Diabetic Retinopathy Grading](https://arxiv.org/abs/2504.19362)
*Yunxuan Wang,Ray Yin,Yumei Tan,Hao Chen,Haiying Xia*

Main category: eess.IV

TLDR: 提出了一种名为LoASP的新方法，通过结合结构先验提升现有域泛化方法在糖尿病视网膜病变分级中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有域泛化方法在糖尿病视网膜病变分级中忽视病变特异性特征，导致精度不足。

Method: 引入低秩自适应结构先验（LoASP），一种即插即用框架，学习自适应结构表示以提升泛化能力。

Result: 在八个数据集上的实验验证了LoASP在单源和多源域场景中的有效性，可视化显示其与血管和病变结构一致。

Conclusion: LoASP通过结合结构先验显著提升了糖尿病视网膜病变分级的泛化性能和可解释性。

Abstract: Diabetic retinopathy (DR), a serious ocular complication of diabetes, is one
of the primary causes of vision loss among retinal vascular diseases. Deep
learning methods have been extensively applied in the grading of diabetic
retinopathy (DR). However, their performance declines significantly when
applied to data outside the training distribution due to domain shifts. Domain
generalization (DG) has emerged as a solution to this challenge. However, most
existing DG methods overlook lesion-specific features, resulting in
insufficient accuracy. In this paper, we propose a novel approach that enhances
existing DG methods by incorporating structural priors, inspired by the
observation that DR grading is heavily dependent on vessel and lesion
structures. We introduce Low-rank Adaptive Structural Priors (LoASP), a
plug-and-play framework designed for seamless integration with existing DG
models. LoASP improves generalization by learning adaptive structural
representations that are finely tuned to the complexities of DR diagnosis.
Extensive experiments on eight diverse datasets validate its effectiveness in
both single-source and multi-source domain scenarios. Furthermore,
visualizations reveal that the learned structural priors intuitively align with
the intricate architecture of the vessels and lesions, providing compelling
insights into their interpretability and diagnostic relevance.

</details>

### [434] [Dual-Modality Computational Ophthalmic Imaging with Deep Learning and Coaxial Optical Design](https://arxiv.org/abs/2504.18549)
*Boyuan Peng,Jiaju Chen,Yiwei Zhang,Cuiyi Peng,Junyang Li,Jiaming Deng,Peiwu Qin*

Main category: eess.IV

TLDR: 该研究提出了一种紧凑型双功能光学设备，结合眼底摄影和屈光误差检测，通过创新的光学设计和深度学习算法实现高精度筛查。


<details>
  <summary>Details</summary>
Motivation: 近视和视网膜疾病负担日益加重，需要更便捷高效的眼部筛查解决方案。

Method: 采用同轴光学设计和二向色镜分离波长依赖的成像路径，结合Dense-U-Net算法进行瞳孔分割和自动对焦。

Result: 实验显示系统能实现高精度瞳孔定位（EDE = 2.8 px，mIoU = 0.931）和屈光误差估计（平均绝对误差低于5%）。

Conclusion: 尽管受限于商用镜头组件，该系统为社区健康环境提供了快速、智能且可扩展的眼科筛查方案。

Abstract: The growing burden of myopia and retinal diseases necessitates more
accessible and efficient eye screening solutions. This study presents a
compact, dual-function optical device that integrates fundus photography and
refractive error detection into a unified platform. The system features a
coaxial optical design using dichroic mirrors to separate wavelength-dependent
imaging paths, enabling simultaneous alignment of fundus and refraction
modules. A Dense-U-Net-based algorithm with customized loss functions is
employed for accurate pupil segmentation, facilitating automated alignment and
focusing. Experimental evaluations demonstrate the system's capability to
achieve high-precision pupil localization (EDE = 2.8 px, mIoU = 0.931) and
reliable refractive estimation with a mean absolute error below 5%. Despite
limitations due to commercial lens components, the proposed framework offers a
promising solution for rapid, intelligent, and scalable ophthalmic screening,
particularly suitable for community health settings.

</details>

### [435] [Improving Generalization in MRI-Based Deep Learning Models for Total Knee Replacement Prediction](https://arxiv.org/abs/2504.19203)
*Ehsan Karami,Hamid Soltanian-Zadeh*

Main category: eess.IV

TLDR: 通过替换批归一化为实例归一化、使用数据增强和对比损失，改进了膝关节骨关节炎预测模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决MRI深度学习模型在不同来源影像数据上泛化能力不足的问题。

Method: 采用实例归一化、数据增强和对比损失改进基线模型，使用OAI数据库的MRI数据训练和评估。

Result: 在源域和目标域上分类准确性显著提高，优于基线模型。

Conclusion: 该方法有效提升了模型在不同影像数据上的泛化性能。

Abstract: Knee osteoarthritis (KOA) is a common joint disease that causes pain and
mobility issues. While MRI-based deep learning models have demonstrated
superior performance in predicting total knee replacement (TKR) and disease
progression, their generalizability remains challenging, particularly when
applied to imaging data from different sources. In this study, we have shown
that replacing batch normalization with instance normalization, using data
augmentation, and applying contrastive loss improves model generalization in a
baseline deep learning model for knee osteoarthritis (KOA) prediction. We
trained and evaluated our model using MRI data from the Osteoarthritis
Initiative (OAI) database, considering sagittal fat-suppressed
intermediate-weighted turbo spin-echo (FS-IW-TSE) images as the source domain
and sagittal fat-suppressed three-dimensional (3D) dual-echo in steady state
(DESS) images as the target domain. The results demonstrate a statistically
significant improvement in classification accuracy across both domains, with
our approach outperforming the baseline model.

</details>

### [436] [Dual Attention Driven Lumbar Magnetic Resonance Image Feature Enhancement and Automatic Diagnosis of Herniation](https://arxiv.org/abs/2504.19438)
*Lingrui Zhang,Liang Guo,Xiao An,Feng Lin,Binlong Zheng,Jiankun Wang,Zhirui Li*

Main category: eess.IV

TLDR: 提出了一种自动分类腰椎间盘突出（LDH）的创新框架，利用T1和T2加权MRI图像，通过数据增强和注意力机制实现高准确率诊断。


<details>
  <summary>Details</summary>
Motivation: LDH诊断依赖放射科医生的专业知识，导致诊断延迟和培训成本高，需要自动化解决方案。

Method: 使用205人的T1和T2加权MRI图像，结合数据增强和通道空间注意力机制提取特征并生成标准化诊断输出。

Result: AUC-ROC为0.969，准确率为0.9486，仅需少量训练数据即可实现高准确率。

Conclusion: 该框架能提升基层医院的LDH检测能力，为临床决策提供高效支持。

Abstract: Lumbar disc herniation (LDH) is a common musculoskeletal disease that
requires magnetic resonance imaging (MRI) for effective clinical management.
However, the interpretation of MRI images heavily relies on the expertise of
radiologists, leading to delayed diagnosis and high costs for training
physicians. Therefore, this paper proposes an innovative automated LDH
classification framework. To address these key issues, the framework utilizes
T1-weighted and T2-weighted MRI images from 205 people. The framework extracts
clinically actionable LDH features and generates standardized diagnostic
outputs by leveraging data augmentation and channel and spatial attention
mechanisms. These outputs can help physicians make confident and time-effective
care decisions when needed. The proposed framework achieves an area under the
receiver operating characteristic curve (AUC-ROC) of 0.969 and an accuracy of
0.9486 for LDH detection. The experimental results demonstrate the performance
of the proposed framework. Our framework only requires a small number of
datasets for training to demonstrate high diagnostic accuracy. This is expected
to be a solution to enhance the LDH detection capabilities of primary
hospitals.

</details>

### [437] [Accelerated 3D-3D rigid registration of echocardiographic images obtained from apical window using particle filter](https://arxiv.org/abs/2504.19930)
*Thanuja Uruththirakodeeswaran,Harald Becher,Michelle Noga,Lawrence H. Le,Pierre Boulanger,Jonathan Windram,Kumaradevan Punithakumar*

Main category: eess.IV

TLDR: 提出了一种加速的SMC算法，用于3D-3D刚性配准，提高了超声图像的质量和配准速度。


<details>
  <summary>Details</summary>
Motivation: 解决超声图像噪声和强度变化对配准的影响，并提高配准效率。

Method: 采用迭代过程估计刚性变换的平移和旋转分量，支持图像和掩膜两种配准方式。

Result: 掩膜配准方法Dice得分为0.819±0.045，速度提升了16.7倍。

Conclusion: 加速SMC算法在超声图像配准中表现出高效和鲁棒性。

Abstract: The perfect alignment of 3D echocardiographic images captured from various
angles has improved image quality and broadened the field of view. This study
proposes an accelerated sequential Monte Carlo (SMC) algorithm for 3D-3D rigid
registration of transthoracic echocardiographic images with significant and
limited overlap taken from apical window that is robust to the noise and
intensity variation in ultrasound images. The algorithm estimates the
translational and rotational components of the rigid transform through an
iterative process and requires an initial approximation of the rotation and
translation limits. We perform registration in two ways: the image-based
registration computes the transform to align the end-diastolic frame of the
apical nonstandard image to the apical standard image and applies the same
transform to all frames of the cardiac cycle, whereas the mask-based
registration approach uses the binary masks of the left ventricle in the same
way. The SMC and exhaustive search (EX) algorithms were evaluated for 4D
temporal sequences recorded from 7 volunteers who participated in a study
conducted at the Mazankowski Alberta Heart Institute. The evaluations
demonstrate that the mask-based approach of the accelerated SMC yielded a Dice
score value of 0.819 +/- 0.045 for the left ventricle and gained 16.7x speedup
compared to the CPU version of the SMC algorithm.

</details>

### [438] [SST-DUNet: Automated preclinical functional MRI skull stripping using Smart Swin Transformer and Dense UNet](https://arxiv.org/abs/2504.19937)
*Sima Soltanpour,Rachel Utama,Arnold Chang,Md Taufiq Nasseef,Dan Madularu,Praveen Kulkarni,Craig Ferris,Chris Joslin*

Main category: eess.IV

TLDR: 提出了一种名为SST-DUNet的新方法，用于自动剥离fMRI数据中的颅骨，解决了现有方法在低分辨率和变切片尺寸下的挑战。


<details>
  <summary>Details</summary>
Motivation: 手动剥离颅骨耗时且依赖操作者，而现有方法难以处理临床前fMRI数据的复杂特性。

Method: 结合密集UNet架构和基于Smart Swin Transformer的特征提取器，使用SSW-MSA模块改进特征学习，并采用Focal和Dice损失函数解决类别不平衡问题。

Result: 在三个内部数据集上Dice相似性得分分别为98.65%、97.86%和98.04%，自动剥离结果与手动剥离高度一致。

Conclusion: SST-DUNet可有效替代手动剥离，适用于大鼠fMRI分析。

Abstract: Skull stripping is a common preprocessing step that is often performed
manually in Magnetic Resonance Imaging (MRI) pipelines, including functional
MRI (fMRI). This manual process is time-consuming and operator dependent.
Automating this process is challenging for preclinical data due to variations
in brain geometry, resolution, and tissue contrast. While existing methods for
MRI skull stripping exist, they often struggle with the low resolution and
varying slice sizes in preclinical fMRI data. This study proposes a novel
method called SST-DUNet, that integrates a dense UNet-based architecture with a
feature extractor based on Smart Swin Transformer (SST) for fMRI skull
stripping. The Smart Shifted Window Multi-Head Self-Attention (SSW-MSA) module
in SST is adapted to replace the mask-based module in the Swin Transformer
(ST), enabling the learning of distinct channel-wise features while focusing on
relevant dependencies within brain structures. This modification allows the
model to better handle the complexities of fMRI skull stripping, such as low
resolution and variable slice sizes. To address the issue of class imbalance in
preclinical data, a combined loss function using Focal and Dice loss is
utilized. The model was trained on rat fMRI images and evaluated across three
in-house datasets with a Dice similarity score of 98.65%, 97.86%, and 98.04%.
The fMRI results obtained through automatic skull stripping using the SST-DUNet
model closely align with those from manual skull stripping for both seed-based
and independent component analyses. These results indicate that the SST-DUNet
can effectively substitute manual brain extraction in rat fMRI analysis.

</details>

<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [439] [Metric Similarity and Manifold Learning of Circular Dichroism Spectra of Proteins](https://arxiv.org/abs/2504.19355)
*Gionni Marchetti*

Main category: physics.soc-ph

TLDR: N/A


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a machine learning analysis of circular dichroism spectra of
globular proteins from the SP175 database, using the optimal transport-based
$1$-Wasserstein distance $\mathcal{W}_1$ (with order $p=1$) and the manifold
learning algorithm $t$-SNE. Our results demonstrate that $\mathcal{W}_1$ is
consistent with both Euclidean and Manhattan metrics while exhibiting
robustness to noise. On the other hand, $t$-SNE uncovers meaningful structure
in the high-dimensional data. The clustering in the $t$-SNE embedding is
primarily determined by proteins with distinct secondary structure
compositions: one cluster predominantly contains $\beta$-rich proteins, while
the other consists mainly of proteins with mixed $\alpha/\beta$ and
$\alpha$-helical content.

</details>

<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [440] [Predicting Stress in Two-phase Random Materials and Super-Resolution Method for Stress Images by Embedding Physical Information](https://arxiv.org/abs/2504.18854)
*Tengfei Xing,Xiaodan Ren,Jie Li*

Main category: cond-mat.mtrl-sci

TLDR: 该研究提出了一种针对两相随机材料（TRMs）的应力预测框架，结合了多成分U-net和基于物理信息的神经网络方法，有效解决了相边界应力预测误差问题，并实现了应力图像的超分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 材料微观结构的应力分析对材料设计至关重要，但现有方法在相边界应力预测和图像分辨率上存在局限，需要结合物理约束改进。

Method: 研究提出MC U-net用于低分辨率应力预测，并开发SRPINN方法，利用物理信息约束实现无配对训练的超分辨率重建。

Result: 框架在不同相体积分数和加载状态下表现出高精度和泛化能力。

Conclusion: 该框架为复杂微观结构材料的应力分析提供了有效工具，尤其在相边界应力预测和超分辨率重建方面具有优势。

Abstract: Stress analysis is an important part of material design. For materials with
complex microstructures, such as two-phase random materials (TRMs), material
failure is often accompanied by stress concentration. Phase interfaces in
two-phase materials are critical for stress concentration. Therefore, the
prediction error of stress at phase boundaries is crucial. In practical
engineering, the pixels of the obtained material microstructure images are
limited, which limits the resolution of stress images generated by deep
learning methods, making it difficult to observe stress concentration regions.
Existing Image Super-Resolution (ISR) technologies are all based on data-driven
supervised learning. However, stress images have natural physical constraints,
which provide new ideas for new ISR technologies. In this study, we constructed
a stress prediction framework for TRMs. First, the framework uses a proposed
Multiple Compositions U-net (MC U-net) to predict stress in low-resolution
material microstructures. By considering the phase interface information of the
microstructure, the MC U-net effectively reduces the problem of excessive
prediction errors at phase boundaries. Secondly, a Mixed Physics-Informed
Neural Network (MPINN) based method for stress ISR (SRPINN) was proposed. By
introducing the constraints of physical information, the new method does not
require paired stress images for training and can increase the resolution of
stress images to any multiple. This enables a multiscale analysis of the stress
concentration regions at phase boundaries. Finally, we performed stress
analysis on TRMs with different phase volume fractions and loading states
through transfer learning. The results show the proposed stress prediction
framework has satisfactory accuracy and generalization ability.

</details>

### [441] [Leveraging Modified Ex Situ Tomography Data for Segmentation of In Situ Synchrotron X-Ray Computed Tomography](https://arxiv.org/abs/2504.19200)
*Tristan Manchester,Adam Anders,Julio Spadotto,Hannah Eccleston,William Beavan,Hugues Arcis,Brian J. Connolly*

Main category: cond-mat.mtrl-sci

TLDR: 提出了一种基于深度学习的图像分割方法，通过将高质量实验室数据转化为训练模型，用于同步辐射数据的二值分割，显著提高了处理速度。


<details>
  <summary>Details</summary>
Motivation: 同步辐射X射线计算机断层扫描的动态材料研究面临自动分割的挑战，主要由于复杂的成像伪影和有限的训练数据。

Method: 采用改进的SegFormer架构，利用高质量实验室数据训练模型，用于同步辐射数据的二值分割。

Result: 方法在未见数据上表现出高分割性能，处理时间从小时级缩短至秒级，且在实验过程中形态变化显著时仍保持稳定性能。

Conclusion: 该方法可广泛应用于多种材料系统，加速时间分辨断层数据的分析。

Abstract: In situ synchrotron X-ray computed tomography enables dynamic material
studies, but automated segmentation remains challenging due to complex imaging
artefacts and limited training data. We present a methodology for deep
learning-based segmentation by transforming high-quality ex situ laboratory
data to train models for binary segmentation of in situ synchrotron data,
demonstrated through copper oxide dissolution studies. Using a modified
SegFormer architecture, our approach achieves high segmentation performance on
unseen data while reducing processing time from hours to seconds per 3D
dataset. The method maintains consistent performance over significant
morphological changes during experiments, despite training only on static
specimens. This methodology can be readily applied to diverse materials
systems, accelerating the analysis of time-resolved tomographic data across
scientific disciplines.

</details>

### [442] [Composable and adaptive design of machine learning interatomic potentials guided by Fisher-information analysis](https://arxiv.org/abs/2504.19372)
*Weishi Wang,Mark K. Transtrum,Vincenzo Lordi,Vasily V. Bulatov,Amit Samanta*

Main category: cond-mat.mtrl-sci

TLDR: 提出了一种自适应物理信息模型设计策略，用于机器学习原子间势（MLIPs），通过迭代重构复合模型和统一训练，结合Fisher信息矩阵和多属性误差指标评估，实现了灵活性与可扩展性的平衡。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够自适应调整的MLIP模型，以解决传统方法在灵活性和扩展性上的不足。

Method: 采用迭代重构复合模型的方法，结合Fisher信息矩阵和多属性误差指标进行模型评估和超参数优化。

Result: 在铌数据集案例中，生成75参数的最优配置，力RMSE为0.172 eV/Å，能量RMSE为0.013 eV/atom。

Conclusion: 该策略有效平衡了灵活性与可扩展性，为MLIP设计提供了新思路。

Abstract: An adaptive physics-informed model design strategy for machine-learning
interatomic potentials (MLIPs) is proposed. This strategy follows an iterative
reconfiguration of composite models from single-term models, followed by a
unified training procedure. A model evaluation method based on the Fisher
information matrix (FIM) and multiple-property error metrics is proposed to
guide model reconfiguration and hyperparameter optimization. Combining the
model reconfiguration and the model evaluation subroutines, we provide an
adaptive MLIP design strategy that balances flexibility and extensibility. In a
case study of designing models against a structurally diverse niobium dataset,
we managed to obtain an optimal configuration with 75 parameters generated by
our framework that achieved a force RMSE of 0.172 eV/{\AA} and an energy RMSE
of 0.013 eV/atom.

</details>

### [443] [Interpretable machine learning-guided design of Fe-based soft magnetic alloys](https://arxiv.org/abs/2504.19787)
*Aditi Nachnani,Kai K. Li-Caldwell,Saptarshi Biswas,Prince Sharma,Gaoyuan Ouyang,Prashant Singh*

Main category: cond-mat.mtrl-sci

TLDR: 机器学习方法预测Fe-Si-B合金的饱和磁化强度（MS）和矫顽力（HC），实验验证与DFT预测一致，揭示了成分与工艺条件对磁性能的影响。


<details>
  <summary>Details</summary>
Motivation: 加速寻找高性能、无Co和Ni的软磁材料。

Method: 基于实验数据训练机器学习模型，结合DFT预测和实验验证。

Result: Si和B含量增加会降低MS，HC对工艺条件更敏感；模型在伪四元合金中表现出与已知材料相当的磁性能。

Conclusion: 机器学习框架在软磁材料设计中具有潜力，可高效预测和优化材料性能。

Abstract: We present a machine-learning guided approach to predict saturation
magnetization (MS) and coercivity (HC) in Fe-rich soft magnetic alloys,
particularly Fe-Si-B systems. ML models trained on experimental data reveals
that increasing Si and B content reduces MS from 1.81T (DFT~2.04 T) to ~1.54 T
(DFT~1.56T) in Fe-Si-B, which is attributed to decreased magnetic density and
structural modifications. Experimental validation of ML predicted magnetic
saturation on Fe-1Si-1B (2.09T), Fe-5Si-5B (2.01T) and Fe-10Si-10B (1.54T)
alloy compositions further support our findings. These trends are consistent
with density functional theory (DFT) predictions, which link increased
electronic disorder and band broadening to lower MS values. Experimental
validation on selected alloys confirms the predictive accuracy of the ML model,
with good agreement across compositions. Beyond predictive accuracy, detailed
uncertainty quantification and model interpretability including through feature
importance and partial dependence analysis reveals that MS is governed by a
nonlinear interplay between Fe content, early transition metal ratios, and
annealing temperature, while HC is more sensitive to processing conditions such
as ribbon thickness and thermal treatment windows. The ML framework was further
applied to Fe-Si-B/Cr/Cu/Zr/Nb alloys in a pseudo-quaternary compositional
space, which shows comparable magnetic properties to NANOMET
(Fe84.8Si0.5B9.4Cu0.8 P3.5C1), FINEMET (Fe73.5Si13.5B9 Cu1Nb3), NANOPERM
(Fe88Zr7B4Cu1), and HITPERM (Fe44Co44Zr7B4Cu1. Our fundings demonstrate the
potential of ML framework for accelerated search of high-performance, Co- and
Ni-free, soft magnetic materials.

</details>

### [444] [Graph Neural Network Prediction of Nonlinear Optical Properties](https://arxiv.org/abs/2504.19987)
*Yomn Alkabakibi,Congwei Xie,Artem R. Oganov*

Main category: cond-mat.mtrl-sci

TLDR: 使用深度学习模型ALIGNN预测非线性光学材料的二次谐波生成性能，准确率达82.5%。


<details>
  <summary>Details</summary>
Motivation: 传统实验和第一性原理计算耗时且昂贵，亟需高效方法发现新型非线性光学材料。

Method: 基于NOEMD数据库数据，利用ALIGNN模型预测Kurtz-Perry系数。

Result: 模型在绝对误差1 pm/V、相对误差0.5内达到82.5%准确率。

Conclusion: 深度学习可加速具有目标性能的光学材料发现与设计。

Abstract: Nonlinear optical (NLO) materials for generating lasers via second harmonic
generation (SHG) are highly sought in today's technology. However, discovering
novel materials with considerable SHG is challenging due to the time-consuming
and costly nature of both experimental methods and first-principles
calculations. In this study, we present a deep learning approach using the
Atomistic Line Graph Neural Network (ALIGNN) to predict NLO properties.
Sourcing data from the Novel Opto-Electronic Materials Discovery (NOEMD)
database and using the Kurtz-Perry (KP) coefficient as the key target, we
developed a robust model capable of accurately estimating nonlinear optical
responses. Our results demonstrate that the model achieves 82.5% accuracy at a
tolerated absolute error up to 1 pm/V and relative error not exceeding 0.5.
This work highlights the potential of deep learning in accelerating the
discovery and design of advanced optical materials with desired properties.

</details>

### [445] [Curiosity Driven Exploration to Optimize Structure-Property Learning in Microscopy](https://arxiv.org/abs/2504.20011)
*Aditya Vatsavai,Ganesh Narasimha,Yongtao Liu,Jan-Chi Yang,Hiroshu Funakubo,Maxim Ziatdinov,Rama Vasudevan*

Main category: cond-mat.mtrl-sci

TLDR: 提出了一种轻量级的好奇心算法，用于高效映射材料科学中的结构-性能关系，优于随机采样。


<details>
  <summary>Details</summary>
Motivation: 快速确定材料的结构-性能关系对理解机制和材料设计至关重要，但现有方法计算成本高。

Method: 利用深度学习代理模型预测误差，主动采样未探索的结构-性能关系区域。

Result: 算法在预测性能方面优于随机采样。

Conclusion: 该算法为材料科学中的高效结构-性能关系映射提供了便利工具。

Abstract: Rapidly determining structure-property correlations in materials is an
important challenge in better understanding fundamental mechanisms and greatly
assists in materials design. In microscopy, imaging data provides a direct
measurement of the local structure, while spectroscopic measurements provide
relevant functional property information. Deep kernel active learning
approaches have been utilized to rapidly map local structure to functional
properties in microscopy experiments, but are computationally expensive for
multi-dimensional and correlated output spaces. Here, we present an alternative
lightweight curiosity algorithm which actively samples regions with unexplored
structure-property relations, utilizing a deep-learning based surrogate model
for error prediction. We show that the algorithm outperforms random sampling
for predicting properties from structures, and provides a convenient tool for
efficient mapping of structure-property relationships in materials science.

</details>

<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [446] [QFGN: A Quantum Approach to High-Fidelity Implicit Neural Representations](https://arxiv.org/abs/2504.19053)
*Hongni Jin,Gurinder Singh,Kenneth M. Merz Jr*

Main category: quant-ph

TLDR: 本文提出了一种基于量子的机器学习模型QFGN，用于改进信号表示，通过惩罚低频分量平衡频谱，提升量子电路的表达能力。实验表明，QFGN在参数极少的情况下优于当前SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示在多种应用中显示出潜力，但图像超分辨率中的精确重建和细节清晰化仍具挑战性。

Method: 引入Quantum Fourier Gaussian Network (QFGN)，通过惩罚低频分量平衡频谱，优化量子电路的表达能力。

Result: QFGN在参数极少的情况下优于当前SOTA模型，硬件噪声下仍能达到与SIREN相当的精度。

Conclusion: QFGN展示了量子机器学习在该领域的潜在应用价值。

Abstract: Implicit neural representations have shown potential in various applications.
However, accurately reconstructing the image or providing clear details via
image super-resolution remains challenging. This paper introduces Quantum
Fourier Gaussian Network (QFGN), a quantum-based machine learning model for
better signal representations. The frequency spectrum is well balanced by
penalizing the low-frequency components, leading to the improved expressivity
of quantum circuits. The results demonstrate that with minimal parameters, QFGN
outperforms the current state-of-the-art (SOTA) models. Despite noise on
hardware, the model achieves accuracy comparable to that of SIREN, highlighting
the potential applications of quantum machine learning in this field.

</details>

### [447] [Inverse-Transpilation: Reverse-Engineering Quantum Compiler Optimization Passes from Circuit Snapshots](https://arxiv.org/abs/2504.19113)
*Satwik Kundu,Swaroop Ghosh*

Main category: quant-ph

TLDR: 论文提出了一种基于机器学习的框架，用于逆向工程量子电路编译器的优化技术，旨在提高透明度和识别商业系统中的知识产权保护优化方法。


<details>
  <summary>Details</summary>
Motivation: 增强电路优化的透明度以改进跨平台调试和性能调优，同时识别商业系统中可能的知识产权保护优化技术。

Method: 利用原始电路与编译后电路之间的结构差异，提出了一种简单的基于机器学习的框架来推断底层优化技术。

Result: 在数千个量子电路上的广泛评估显示，神经网络在检测优化步骤方面表现最佳，单个步骤的F1分数高达0.96。

Conclusion: 初步研究表明，这种威胁对编译器保密性的可行性，并强调了在该领域开展积极研究的必要性。

Abstract: Circuit compilation, a crucial process for adapting quantum algorithms to
hardware constraints, often operates as a ``black box,'' with limited
visibility into the optimization techniques used by proprietary systems or
advanced open-source frameworks. Due to fundamental differences in qubit
technologies, efficient compiler design is an expensive process, further
exposing these systems to various security threats. In this work, we take a
first step toward evaluating one such challenge affecting compiler
confidentiality, specifically, reverse-engineering compilation methodologies.
We propose a simple ML-based framework to infer underlying optimization
techniques by leveraging structural differences observed between original and
compiled circuits. The motivation is twofold: (1) enhancing transparency in
circuit optimization for improved cross-platform debugging and performance
tuning, and (2) identifying potential intellectual property (IP)-protected
optimizations employed by commercial systems. Our extensive evaluation across
thousands of quantum circuits shows that a neural network performs the best in
detecting optimization passes, with individual pass F1-scores reaching as high
as 0.96. Thus, our initial study demonstrates the viability of this threat to
compiler confidentiality and underscores the need for active research in this
area.

</details>

### [448] [The effect of the number of parameters and the number of local feature patches on loss landscapes in distributed quantum neural networks](https://arxiv.org/abs/2504.19239)
*Yoshiaki Kawase*

Main category: quant-ph

TLDR: 分布式量子神经网络通过多量子神经网络处理局部数据，减少优化挑战，提升训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 量子神经网络在优化过程中面临复杂损失地形问题，如贫瘠高原和局部极小值，限制了其实际应用。

Method: 采用分布式局部补丁方法，通过多个独立量子神经网络处理数据并聚合输出，分析参数和补丁数量对损失地形的影响。

Result: 增加参数数量会加深损失地形，而增加补丁数量可显著减小Hessian最大特征值，提升优化稳定性。

Conclusion: 分布式补丁方法是一种有效的隐式正则化策略，有望提升量子机器学习模型的训练性和实用性。

Abstract: Quantum neural networks hold promise for tackling computationally challenging
tasks that are intractable for classical computers. However, their practical
application is hindered by significant optimization challenges, arising from
complex loss landscapes characterized by barren plateaus and numerous local
minima. These problems become more severe as the number of parameters or qubits
increases, hampering effective training. To mitigate these optimization
challenges, particularly for quantum machine learning applied to classical
data, we employ an approach of distributing overlapping local patches across
multiple quantum neural networks, processing each patch with an independent
quantum neural network, and aggregating their outputs for prediction. In this
study, we investigate how the number of parameters and patches affects the loss
landscape geometry of this distributed quantum neural network architecture via
Hessian analysis and loss landscape visualization. Our results confirm that
increasing the number of parameters tends to lead to deeper and sharper loss
landscapes. Crucially, we demonstrate that increasing the number of patches
significantly reduces the largest Hessian eigenvalue at minima. This finding
suggests that our distributed patch approach acts as a form of implicit
regularization, promoting optimization stability and potentially enhancing
generalization. Our study provides valuable insights into optimization
challenges and highlights that the distributed patch approach is a promising
strategy for developing more trainable and practical quantum machine learning
models for classical data tasks.

</details>

### [449] [QFDNN: A Resource-Efficient Variational Quantum Feature Deep Neural Networks for Fraud Detection and Loan Prediction](https://arxiv.org/abs/2504.19632)
*Subham Das,Ashtakala Meghanath,Bikash K. Behera,Shahid Mumtaz,Saif Al-Kuwari,Ahmed Farouk*

Main category: quant-ph

TLDR: 论文提出了一种量子特征深度神经网络（QFDNN），用于高效处理高维金融数据，解决信用欺诈检测和贷款资格预测中的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决传统机器学习模型在处理高维金融数据时的可扩展性、过拟合和高计算成本问题，以及现有量子算法在噪声环境中的鲁棒性不足。

Method: 提出QFDNN模型，通过优化特征表示，减少量子比特需求和简化变分电路，实现高效且抗噪声的量子计算。

Result: 在信用欺诈检测和贷款资格预测数据集上分别达到82.2%和74.4%的准确率，并在六种噪声模型中表现出鲁棒性。

Conclusion: QFDNN通过高效、抗噪声的设计，提升了社会金融技术中的信任和安全性，同时支持可持续发展。

Abstract: Social financial technology focuses on trust, sustainability, and social
responsibility, which require advanced technologies to address complex
financial tasks in the digital era. With the rapid growth in online
transactions, automating credit card fraud detection and loan eligibility
prediction has become increasingly challenging. Classical machine learning (ML)
models have been used to solve these challenges; however, these approaches
often encounter scalability, overfitting, and high computational costs due to
complexity and high-dimensional financial data. Quantum computing (QC) and
quantum machine learning (QML) provide a promising solution to efficiently
processing high-dimensional datasets and enabling real-time identification of
subtle fraud patterns. However, existing quantum algorithms lack robustness in
noisy environments and fail to optimize performance with reduced feature sets.
To address these limitations, we propose a quantum feature deep neural network
(QFDNN), a novel, resource efficient, and noise-resilient quantum model that
optimizes feature representation while requiring fewer qubits and simpler
variational circuits. The model is evaluated using credit card fraud detection
and loan eligibility prediction datasets, achieving competitive accuracies of
82.2% and 74.4%, respectively, with reduced computational overhead.
Furthermore, we test QFDNN against six noise models, demonstrating its
robustness across various error conditions. Our findings highlight QFDNN
potential to enhance trust and security in social financial technology by
accurately detecting fraudulent transactions while supporting sustainability
through its resource-efficient design and minimal computational overhead.

</details>

<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [450] [Modeling Regime Structure and Informational Drivers of Stock Market Volatility via the Financial Chaos Index](https://arxiv.org/abs/2504.18958)
*Masoud Ataei*

Main category: q-fin.ST

TLDR: 该论文通过金融混沌指数（FCIX）研究股市波动的结构动态，识别了三种市场状态，并利用弹性网络回归模型预测隐含波动率。


<details>
  <summary>Details</summary>
Motivation: 基于金融危机中波动行为的经验证据和时间感知变化，研究旨在理解系统性不确定性如何影响市场波动。

Method: 采用基于修正对数正态幂律分布的机制转换框架，分析FCIX数据，并使用弹性网络回归模型预测波动率。

Result: 识别出低混沌、中混沌和高混沌三种市场状态，发现宏观经济、金融、政策和地缘政治不确定性对波动率有显著预测能力。

Conclusion: 研究提供了系统性不确定性如何同时影响实际市场波动和隐含波动率的统一实证视角。

Abstract: This paper investigates the structural dynamics of stock market volatility
through the Financial Chaos Index, a tensor- and eigenvalue-based measure
designed to capture realized volatility via mutual fluctuations among asset
prices. Motivated by empirical evidence of regime-dependent volatility behavior
and perceptual time dilation during financial crises, we develop a
regime-switching framework based on the Modified Lognormal Power-Law
distribution. Analysis of the FCIX from January 1990 to December 2023
identifies three distinct market regimes, low-chaos, intermediate-chaos, and
high-chaos, each characterized by differing levels of systemic stress,
statistical dispersion and persistence characteristics. Building upon the
segmented regime structure, we further examine the informational forces that
shape forward-looking market expectations. Using sentiment-based predictors
derived from the Equity Market Volatility tracker, we employ an elastic net
regression model to forecast implied volatility, as proxied by the VIX index.
Our findings indicate that shifts in macroeconomic, financial, policy, and
geopolitical uncertainty exhibit strong predictive power for volatility
dynamics across regimes. Together, these results offer a unified empirical
perspective on how systemic uncertainty governs both the realized evolution of
financial markets and the anticipatory behavior embedded in implied volatility
measures.

</details>

<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [451] [Two-parameter superposable S-curves](https://arxiv.org/abs/2504.19488)
*Vijay Prakash S*

Main category: stat.ME

TLDR: 论文研究了通过奇异扰动直线方程$y=mx$生成的S形曲线，提出其可作为统计模型，用于从均匀分布到单值分布的熵变化表示，并在鸢尾花数据集上验证了其模式识别能力。


<details>
  <summary>Details</summary>
Motivation: 探讨S形曲线如何从均匀分布过渡到单值分布，并验证其作为统计模型的实用性。

Method: 通过奇异扰动直线方程生成S形曲线，提出其叠加形式作为统计模型，并在鸢尾花数据集上进行拟合和分析。

Result: S形曲线能够有效表示非均匀模式，但其参数估计对初始条件敏感。

Conclusion: S形曲线可作为统计模型表示非均匀模式，但需注意参数估计的局限性。

Abstract: Straight line equation $y=mx$ with slope $m$, when singularly perturbed as
$ay^3+y=mx$ with a positive parameter $a$, results in S-shaped curves or
S-curves on a real plane. As $a\rightarrow 0$, we get back $y=mx$ which is a
cumulative distribution function of a continuous uniform distribution that
describes the occurrence of every event in an interval to be equally probable.
As $a\rightarrow\infty$, the derivative of $y$ has finite support only at $y=0$
resembling a degenerate distribution. Based on these arguments, in this work,
we propose that these S-curves can represent maximum entropy uniform
distribution to a zero entropy single value. We also argue that these S-curves
are superposable as they are only parametrically nonlinear but fundamentally
linear. So far, the superposed forms have been used to capture the patterns of
natural systems such as nonlinear dynamics of biological growth and kinetics of
enzyme reactions. Here, we attempt to use the S-curve and its superposed form
as a statistical model. We fit the models on a classical dataset containing
flower measurements of iris plants and analyze their usefulness in pattern
recognition. Based on these models, we claim that any non-uniform pattern can
be represented as a singular perturbation to uniform distribution. However, our
parametric estimation procedure have some limitations such as sensitivity to
initial conditions depending on the data at hand.

</details>

<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [452] [Validation and Calibration of Semi-Analytical Models for the Event Horizon Telescope Observations of Sagittarius A*](https://arxiv.org/abs/2504.18624)
*Ali SaraerToosi,Avery Broderick*

Main category: astro-ph.HE

TLDR: 利用生成式机器学习模型ALINET高效生成RIAF图像，并评估未建模物理效应的影响，以校准EHT观测中的物理参数估计。


<details>
  <summary>Details</summary>
Motivation: 探索黑洞吸积流在事件视界尺度的物理特性，但传统方法计算成本高。

Method: 使用ALINET生成RIAF图像，评估未建模效应（如星际散射和源变异性）的不确定性。

Result: 通过ALINET校准了RIAF模型拟合EHT数据时的物理参数及其不确定性。

Conclusion: ALINET为黑洞物理研究提供了一种高效且可靠的工具。

Abstract: The Event Horizon Telescope (EHT) enables the exploration of black hole
accretion flows at event-horizon scales. Fitting ray-traced physical models to
EHT observations requires the generation of synthetic images, a task that is
computationally demanding. This study leverages \alinet, a generative machine
learning model, to efficiently produce radiatively inefficient accretion flow
(RIAF) images as a function of the specified physical parameters. \alinet has
previously been shown to be able to interpolate black hole images and their
associated physical parameters after training on a computationally tractable
set of library images. We utilize this model to estimate the uncertainty
introduced by a number of anticipated unmodeled physical effects, including
interstellar scattering and intrinsic source variability. We then use this to
calibrate physical parameter estimates and their associated uncertainties from
RIAF model fits to mock EHT data via a library of general relativistic
magnetohydrodynamics models.

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [453] [Application of the Brain Drain Optimization Algorithm to the N-Queens Problem](https://arxiv.org/abs/2504.18953)
*Sahar Ramezani Jolfaei,Sepehr Khodadadi Hossein Abadi*

Main category: cs.NE

TLDR: 本文介绍了Brain Drain Optimization（BRADO）算法在N-Queens问题中的应用，展示了其在解决组合优化问题中的优越性。


<details>
  <summary>Details</summary>
Motivation: 研究BRADO算法在经典组合优化问题N-Queens中的表现，验证其作为通用求解器的潜力。

Method: 使用基于TOPSIS的多准则决策过程调整配置，并通过设计的成本函数引导搜索。

Result: BRADO在解质量和目标函数值上优于PSO、GA、ICA、ILS和LS等算法。

Conclusion: BRADO在组合问题中表现出色，有望应用于人工智能的其他领域。

Abstract: This paper introduces the application of the Brain Drain Optimization
algorithm -- a swarm-based metaheuristic inspired by the emigration of
intellectual elites -- to the N-Queens problem. The N-Queens problem, a classic
combinatorial optimization problem, serves as a challenge for applying the
BRADO. A designed cost function guides the search, and the configurations are
tuned using a TOPSIS-based multicriteria decision making process. BRADO
consistently outperforms alternatives in terms of solution quality, achieving
fewer threats and better objective function values. To assess BRADO's efficacy,
it is benchmarked against several established metaheuristic algorithms,
including Particle Swarm Optimization (PSO), Genetic Algorithm (GA),
Imperialist Competitive Algorithm (ICA), Iterated Local Search (ILS), and basic
Local Search (LS). The study highlights BRADO's potential as a general-purpose
solver for combinatorial problems, opening pathways for future applications in
other domains of artificial intelligence.

</details>

<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [454] [Generative Models for Fast Simulation of Cherenkov Detectors at the Electron-Ion Collider](https://arxiv.org/abs/2504.19042)
*James Giroux,Michael Martinez,Cristiano Fanelli*

Main category: physics.ins-det

TLDR: 提出了一种基于生成模型的快速模拟工具，用于加速Cherenkov探测器的粒子识别任务，替代传统Geant4模拟。


<details>
  <summary>Details</summary>
Motivation: 传统Geant4模拟计算成本高，尤其在Cherenkov探测器中光学光子传输模拟成为瓶颈。

Method: 开发了一个开放的、GPU加速的快速模拟框架，专注于hpDIRC探测器，结合生成模型。

Result: 提供了高效生成高保真数据集的方法，支持DL驱动的粒子识别方法开发与测试。

Conclusion: 该工具为EIC范围内的粒子识别策略提供了关键支持，实现了大规模模拟样本的快速生成。

Abstract: The integration of Deep Learning (DL) into experimental nuclear and particle
physics has driven significant progress in simulation and reconstruction
workflows. However, traditional simulation frameworks such as Geant4 remain
computationally intensive, especially for Cherenkov detectors, where simulating
optical photon transport through complex geometries and reflective surfaces
introduces a major bottleneck. To address this, we present an open, standalone
fast simulation tool for Detection of Internally Reflected Cherenkov Light
(DIRC) detectors, with a focus on the High-Performance DIRC (hpDIRC) at the
future Electron-Ion Collider (EIC). Our framework incorporates a suite of
generative models tailored to accelerate particle identification (PID) tasks by
offering a scalable, GPU-accelerated alternative to full Geant4-based
simulations. Designed with accessibility in mind, our simulation package
enables both DL researchers and physicists to efficiently generate
high-fidelity large-scale datasets on demand, without relying on complex
traditional simulation stacks. This flexibility supports the development and
benchmarking of novel DL-driven PID methods. Moreover, this fast simulation
pipeline represents a critical step toward enabling EIC-wide PID strategies
that depend on virtually unlimited simulated samples, spanning the full
acceptance of the hpDIRC.

</details>

<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [455] [The Big Send-off: High Performance Collectives on GPU-based Supercomputers](https://arxiv.org/abs/2504.18658)
*Siddharth Singh,Mahua Singh,Abhinav Bhatele*

Main category: cs.DC

TLDR: PCCL是一种针对GPU超级计算机优化的通信库，显著提升了大规模语言模型训练中的集体通信性能。


<details>
  <summary>Details</summary>
Motivation: 现有通信库（如RCCL和Cray-MPICH）在GPU超级计算机上存在资源利用不足和扩展性问题，限制了大规模语言模型训练的效率。

Method: 开发了PCCL，针对分布式深度学习工作负载优化了all-gather和reduce-scatter操作，充分利用网络和计算资源。

Result: PCCL在2048个GPU上比RCCL快6-33倍，比Cray-MPICH快28-70倍；在GPT-3训练中，对7B和13B模型分别提速60%和40%。

Conclusion: PCCL显著提升了大规模语言模型训练的通信效率，解决了现有库的局限性。

Abstract: We evaluate the current state of collective communication on GPU-based
supercomputers for large language model (LLM) training at scale. Existing
libraries such as RCCL and Cray-MPICH exhibit critical limitations on systems
such as Frontier -- Cray-MPICH underutilizes network and compute resources,
while RCCL suffers from severe scalability issues. To address these challenges,
we introduce PCCL, a communication library with highly optimized
implementations of all-gather and reduce-scatter operations tailored for
distributed deep learning workloads. PCCL is designed to maximally utilize all
available network and compute resources and to scale efficiently to thousands
of GPUs. It achieves substantial performance improvements, delivering 6-33x
speedups over RCCL and 28-70x over Cray-MPICH for all-gather on 2048 GCDs of
Frontier. These gains translate directly to end-to-end performance: in
large-scale GPT-3-style training, PCCL provides up to 60% and 40% speedups over
RCCL for 7B and 13B parameter models, respectively.

</details>

### [456] [FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation](https://arxiv.org/abs/2504.19519)
*Ke Hong,Xiuhong Li,Minxu Liu,Qiuli Mao,Tianqi Wu,Zixiao Huang,Lufang Chen,Zhong Wang,Yichong Zhang,Zhenhua Zhu,Guohao Dai,Yu Wang*

Main category: cs.DC

TLDR: 论文提出FlashOverlap，一种轻量级设计，通过瓦片级重叠、无干扰计算和通信无关性优化多GPU系统中的通信瓶颈。


<details>
  <summary>Details</summary>
Motivation: 多GPU系统中，GPU间通信成为瓶颈，尤其在消费级GPU上。现有设计无法同时满足瓦片级重叠、无干扰计算和通信无关性。

Method: 提出FlashOverlap，采用新型信号机制识别瓦片级数据依赖，并重新排序数据以实现高效通信。

Result: 实验显示FlashOverlap最高提速1.65倍，优于现有方法。

Conclusion: FlashOverlap通过轻量级设计有效解决了多GPU通信瓶颈问题。

Abstract: Generative models have achieved remarkable success across various
applications, driving the demand for multi-GPU computing. Inter-GPU
communication becomes a bottleneck in multi-GPU computing systems, particularly
on consumer-grade GPUs. By exploiting concurrent hardware execution,
overlapping computation and communication latency is an effective technique for
mitigating the communication overhead. We identify that an efficient and
adaptable overlapping design should satisfy (1) tile-wise overlapping to
maximize the overlapping opportunity, (2) interference-free computation to
maintain the original computational performance, and (3) communication
agnosticism to reduce the development burden against varying communication
primitives. Nevertheless, current designs fail to simultaneously optimize for
all of those features.
  To address the issue, we propose FlashOverlap, a lightweight design
characterized by tile-wise overlapping, interference-free computation, and
communication agnosticism. FlashOverlap utilizes a novel signaling mechanism to
identify tile-wise data dependency without interrupting the computation
process, and reorders data to contiguous addresses, enabling communication by
simply calling NCCL APIs. Experiments show that such a lightweight design
achieves up to 1.65x speedup, outperforming existing works in most cases.

</details>

### [457] [UnifyFL: Enabling Decentralized Cross-Silo Federated Learning](https://arxiv.org/abs/2504.18916)
*Sarang S,Druva Dhakshinamoorthy,Aditya Shiva Sharma,Yuvraj Singh Bhadauria,Siddharth Chaitra Vivek,Arihant Bansal,Arnab K. Paul*

Main category: cs.DC

TLDR: 本文提出了一种名为\proj的信任驱动的跨机构联邦学习框架，通过去中心化编排和分布式存储，解决了信任与资源效率的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习中，机构间因缺乏有效协作机制而难以合作，现有方法在信任和资源效率之间存在矛盾。

Method: 开发了\proj框架，采用去中心化编排和分布式存储，支持同步和异步模式以处理延迟问题。

Result: 实验表明，\proj在性能上与理想的多级集中式联邦学习相当，同时实现了信任和资源的高效利用。

Conclusion: \proj为跨机构联邦学习提供了一种灵活且高效的解决方案，平衡了信任与资源约束。

Abstract: Federated Learning (FL) is a decentralized machine learning (ML) paradigm in
which models are trained on private data across several devices called clients
and combined at a single node called an aggregator rather than aggregating the
data itself. Many organizations employ FL to have better privacy-aware
ML-driven decision-making capabilities. However, organizations often operate
independently rather than collaborate to enhance their FL capabilities due to
the lack of an effective mechanism for collaboration. The challenge lies in
balancing trust and resource efficiency. One approach relies on trusting a
third-party aggregator to consolidate models from all organizations (multilevel
FL), but this requires trusting an entity that may be biased or unreliable.
Alternatively, organizations can bypass a third party by sharing their local
models directly, which requires significant computational resources for
validation. Both approaches reflect a fundamental trade-off between trust and
resource constraints, with neither offering an ideal solution. In this work, we
develop a trust-based cross-silo FL framework called \proj, which uses
decentralized orchestration and distributed storage. \proj provides flexibility
to the participating organizations and presents synchronous and asynchronous
modes to handle stragglers. Our evaluation on a diverse testbed shows that
\proj achieves a performance comparable to the ideal multilevel centralized FL
while allowing trust and optimal use of resources.

</details>

### [458] [Accelerating Mixture-of-Experts Training with Adaptive Expert Replication](https://arxiv.org/abs/2504.19925)
*Athinagoras Skiadopoulos,Mark Zhao,Swapnil Gandhi,Thomas Norrie,Shrijeet Mukherjee,Christos Kozyrakis*

Main category: cs.DC

TLDR: SwiftMoE是一种自适应MoE训练系统，通过解耦专家参数和优化器状态的放置，动态调整资源分配，显著提升了训练效率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 当前MoE模型训练中，专家负载不均衡导致性能与准确性之间的权衡问题，现有系统要么牺牲收敛性，要么承受高迁移开销。

Method: SwiftMoE静态分区优化器状态，动态调整专家参数放置，利用现有权重更新避免迁移开销，实现资源高效分配。

Result: 相比DeepSpeed和FlexMoE，SwiftMoE分别实现了30.5%和25.9%的收敛时间缩短。

Conclusion: SwiftMoE通过创新设计解决了MoE训练中的负载不均衡问题，显著提升了训练效率。

Abstract: Mixture-of-Experts (MoE) models have become a widely adopted solution to
continue scaling model sizes without a corresponding linear increase in
compute. During MoE model training, each input token is dynamically routed to a
subset of experts -- sparsely-activated feed-forward networks -- within each
transformer layer. The distribution of tokens assigned to each expert varies
widely and rapidly over the course of training. To handle the wide load
imbalance across experts, current systems are forced to either drop tokens
assigned to popular experts, degrading convergence, or frequently rebalance
resources allocated to each expert based on popularity, incurring high state
migration overheads.
  To break this performance-accuracy tradeoff, we introduce SwiftMoE, an
adaptive MoE training system. The key insight of SwiftMoE is to decouple the
placement of expert parameters from their large optimizer state. SwiftMoE
statically partitions the optimizer of each expert across all training nodes.
Meanwhile, SwiftMoE dynamically adjusts the placement of expert parameters by
repurposing existing weight updates, avoiding migration overheads. In doing so,
SwiftMoE right-sizes the GPU resources allocated to each expert, on a
per-iteration basis, with minimal overheads. Compared to state-of-the-art MoE
training systems, DeepSpeed and FlexMoE, SwiftMoE is able to achieve a 30.5%
and 25.9% faster time-to-convergence, respectively.

</details>

<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [459] [Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective](https://arxiv.org/abs/2504.19458)
*Taoyu Su,Jiawei Sheng,Duohe Ma,Xiaodong Li,Juwei Yue,Mengxiao Song,Yingkai Tang,Tingwen Liu*

Main category: cs.MM

TLDR: 论文提出了一种反事实去偏框架CDMEA，用于解决多模态实体对齐中视觉模态可能带来的偏差问题，通过因果分析减少视觉模态的直接影响，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究在多模态实体对齐中过度依赖视觉模态，可能导致模型偏向图像匹配任务，而忽略其他模态的贡献。

Method: 提出CDMEA框架，通过因果分析估计视觉模态的总效应（TE）并排除其自然直接效应（NDE），确保模型基于总间接效应（TIE）进行预测。

Result: 在9个基准数据集上的实验表明，CDMEA优于14种现有方法，尤其在低相似度、高噪声和低资源场景中表现突出。

Conclusion: CDMEA通过反事实去偏有效减少了视觉模态的偏差，提升了多模态实体对齐的性能和鲁棒性。

Abstract: Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from
different Multi-Modal Knowledge Graphs (MMKGs), a critical information
retrieval task. Existing studies have explored various fusion paradigms and
consistency constraints to improve the alignment of equivalent entities, while
overlooking that the visual modality may not always contribute positively.
Empirically, entities with low-similarity images usually generate
unsatisfactory performance, highlighting the limitation of overly relying on
visual features. We believe the model can be biased toward the visual modality,
leading to a shortcut image-matching task. To address this, we propose a
counterfactual debiasing framework for MMEA, termed CDMEA, which investigates
visual modality bias from a causal perspective. Our approach aims to leverage
both visual and graph modalities to enhance MMEA while suppressing the direct
causal effect of the visual modality on model predictions. By estimating the
Total Effect (TE) of both modalities and excluding the Natural Direct Effect
(NDE) of the visual modality, we ensure that the model predicts based on the
Total Indirect Effect (TIE), effectively utilizing both modalities and reducing
visual modality bias. Extensive experiments on 9 benchmark datasets show that
CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,
high-noise, and low-resource data scenarios.

</details>

### [460] [WILD: a new in-the-Wild Image Linkage Dataset for synthetic image attribution](https://arxiv.org/abs/2504.19595)
*Pietro Bongini,Sara Mandelli,Andrea Montibeller,Mirko Casu,Orazio Pontorno,Claudio Ragaglia,Luca Zanchetta,Mattia Aquilina,Taiba Majid Wani,Luca Guarnera,Benedetta Tondi,Paolo Bestagini,Irene Amerini,Francesco Denatale,Sebastiano Battiato,Mauro Barni*

Main category: cs.MM

TLDR: WILD数据集为合成图像来源识别提供训练和基准测试工具，包含封闭和开放集，支持多种任务评估。


<details>
  <summary>Details</summary>
Motivation: 合成图像来源识别因生成技术复杂且缺乏高质量数据集而具挑战性，WILD旨在解决这一问题。

Method: 构建包含10个封闭集和10个开放集生成器的数据集，共20,000张图像，部分经后处理。

Result: WILD支持封闭/开放集识别、验证及抗后处理和对抗攻击的鲁棒性测试，并评估了七种基线方法。

Conclusion: WILD为合成图像来源识别提供了实用工具，其挑战性场景有望提升模型性能。

Abstract: Synthetic image source attribution is an open challenge, with an increasing
number of image generators being released yearly. The complexity and the sheer
number of available generative techniques, as well as the scarcity of
high-quality open source datasets of diverse nature for this task, make
training and benchmarking synthetic image source attribution models very
challenging. WILD is a new in-the-Wild Image Linkage Dataset designed to
provide a powerful training and benchmarking tool for synthetic image
attribution models. The dataset is built out of a closed set of 10 popular
commercial generators, which constitutes the training base of attribution
models, and an open set of 10 additional generators, simulating a real-world
in-the-wild scenario. Each generator is represented by 1,000 images, for a
total of 10,000 images in the closed set and 10,000 images in the open set.
Half of the images are post-processed with a wide range of operators. WILD
allows benchmarking attribution models in a wide range of tasks, including
closed and open set identification and verification, and robust attribution
with respect to post-processing and adversarial attacks. Models trained on WILD
are expected to benefit from the challenging scenario represented by the
dataset itself. Moreover, an assessment of seven baseline methodologies on
closed and open set attribution is presented, including robustness tests with
respect to post-processing.

</details>

<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [461] [MODP: Multi Objective Directional Prompting](https://arxiv.org/abs/2504.18722)
*Aashutosh Nema,Samaksh Gulati,Evangelos Giakoumakis,Bipana Thapaliya*

Main category: cs.CC

TLDR: MODP框架通过多目标性和定向提示优化LLM提示工程，在摘要任务中实现26%性能提升，并成功应用于Dell的生产工具。


<details>
  <summary>Details</summary>
Motivation: 当前提示工程研究多关注任务优化，忽视LLM内在行为，MODP旨在填补这一空白。

Method: 提出MODP框架，结合多目标性（考虑LLM行为）和定向提示（指标驱动方法）。

Result: 在摘要任务中性能提升26%，并成功应用于Dell的生产工具。

Conclusion: MODP为提示工程提供了更系统、高效的框架，具有实际应用价值。

Abstract: Recent advances in large language models (LLMs) have led to their popularity
across multiple use-cases. However, prompt engineering, the process for
optimally utilizing such models, remains approximation-driven and subjective.
Most of the current research on prompt engineering focuses on task-specific
optimization, while neglecting the behavior of the LLM under consideration
during prompt development. This paper introduces MODP -- Multi Objective
Directional Prompting, a framework based on two key concepts: 1)
multi-objectivity: the importance of considering an LLM's intrinsic behavior as
an additional objective in prompt development, and 2) directional prompting: a
metrics-driven method for prompt engineering to ensure development of robust
and high-precision prompts. We demonstrate the effectiveness of our proposed
ideas on a summarization task, using a synthetically created dataset, achieving
a 26% performance gain over initial prompts. Finally, we apply MODP to develop
prompts for Dell's Next Best Action support tool, which is now in production
and is used by more than 10,000 internal support agents and serving millions of
customers worldwide.

</details>

### [462] [Probabilistic and Causal Satisfiability: Constraining the Model](https://arxiv.org/abs/2504.19944)
*Markus Bläser,Julian Dörfler,Maciej Liśkiewicz,Benito van der Zander*

Main category: cs.CC

TLDR: 研究了概率和因果推理中可满足性问题的复杂性，扩展了现有工作，增加了模型约束的两个新维度。


<details>
  <summary>Details</summary>
Motivation: 探索概率和因果推理中可满足性问题的复杂性，特别是在固定图结构和小模型约束下的表现。

Method: 通过固定结构因果模型的图结构和小模型约束，分析不同算术和PCH层次下的复杂性。

Result: 在固定图结构和小模型约束下，复杂性表现不同，尤其是紧凑边际化下多项式大小模型不再保证。

Conclusion: 扩展了概率和因果推理中可满足性问题的复杂性研究，提供了新的复杂性分类。

Abstract: We study the complexity of satisfiability problems in probabilistic and
causal reasoning. Given random variables $X_1, X_2,\ldots$ over finite domains,
the basic terms are probabilities of propositional formulas over atomic events
$X_i = x_i$, such as $P(X_1 = x_1)$ or $P(X_1 = x_1 \vee X_2 = x_2)$. The basic
terms can be combined using addition (yielding linear terms) or multiplication
(polynomial terms). The probabilistic satisfiability problem asks whether a
joint probability distribution satisfies a Boolean combination of
(in)equalities over such terms. Fagin et al. (1990) showed that for basic and
linear terms, this problem is NP-complete, making it no harder than Boolean
satisfiability, while Moss\'e et al. (2022) proved that for polynomial terms,
it is complete for the existential theory of the reals.
  Pearl's Causal Hierarchy (PCH) extends the probabilistic setting with
interventional and counterfactual reasoning, enriching the expressiveness of
languages. However, Moss\'e et al. (2022) found that satisfiability complexity
remains unchanged. Van der Zander et al. (2023) showed that introducing a
marginalization operator to languages induces a significant increase in
complexity.
  We extend this line of work by adding two new dimensions to the problem by
constraining the models. First, we fix the graph structure of the underlying
structural causal model, motivated by settings like Pearl's do-calculus, and
give a nearly complete landscape across different arithmetics and PCH levels.
Second, we study small models. While earlier work showed that satisfiable
instances admit polynomial-size models, this is no longer guaranteed with
compact marginalization. We characterize the complexities of satisfiability
under small-model constraints across different settings.

</details>

<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [463] [AI Recommendations and Non-instrumental Image Concerns](https://arxiv.org/abs/2504.19047)
*David Almog*

Main category: econ.GN

TLDR: 研究发现，尽管人类与AI合作潜力巨大，但实践中因非工具性形象问题，人们常忽视AI建议，导致任务表现下降。


<details>
  <summary>Details</summary>
Motivation: 探索人类与AI合作中为何实际效果未达预期，聚焦非工具性形象担忧的影响。

Method: 通过在线实验，分析参与者因形象担忧而忽视AI建议的行为及其对任务表现的影响。

Result: 实验表明，即使无实际后果，形象担忧仍导致参与者拒绝AI建议，降低任务表现。

Conclusion: 非工具性形象问题是人类与AI合作中的关键障碍，需进一步研究解决。

Abstract: There is growing enthusiasm about the potential for humans and AI to
collaborate by leveraging their respective strengths. Yet in practice, this
promise often falls short. This paper uses an online experiment to identify
non-instrumental image concerns as a key reason individuals underutilize AI
recommendations. I show that concerns about how one is perceived, even when
those perceptions carry no monetary consequences, lead participants to
disregard AI advice and reduce task performance.

</details>

<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [464] [Machine Learning-Based Modeling of the Anode Heel Effect in X-ray Beam Monte Carlo Simulations](https://arxiv.org/abs/2504.19155)
*Hussein Harb,Didier Benoit,Axel Rannou,Chi-Hieu Pham,Valentin Tissot,Bahaa Nasr,Julien Bert*

Main category: physics.med-ph

TLDR: AI模型优化X射线成像中的蒙特卡洛模拟，提升剂量精度和图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决X射线成像中阳极脚跟效应导致的剂量分布不对称问题。

Method: 通过机器学习动态调整X射线管阳极和阴极侧的束流权重。

Result: 实验显示阴极侧剂量率提升9.6%，阳极侧降低12.5%。

Conclusion: AI模型显著提升剂量模拟精度，适用于临床和研究场景。

Abstract: This study enhances Monte Carlo simulation accuracy in X-ray imaging by
developing an AI-driven model for the anode heel effect, achieving improved
beam intensity distribution and dosimetric precision. Through dynamic
adjustments to beam weights on the anode and cathode sides of the X-ray tube,
our machine learning model effectively replicates the asymmetry characteristic
of clinical X-ray beams. Experimental results reveal dose rate increases of up
to 9.6% on the cathode side and reductions of up to 12.5% on the anode side,
for energy levels between 50 and 120 kVp. These experimentally optimized beam
weights were integrated into the OpenGATE and GGEMS Monte Carlo toolkits,
significantly advancing dosimetric simulation accuracy and the image quality
which closely resembles the clinical imaging. Validation with fluence and dose
actors demonstrated that the AI-based model closely mirrors clinical beam
behavior, providing substantial improvements in dose consistency and accuracy
over conventional X-ray models. This approach provides a robust framework for
improving X-ray dosimetry, with potential applications in dose optimization,
imaging quality enhancement, and radiation safety in both clinical and research
settings.

</details>

### [465] [Innovative Integration of 4D Cardiovascular Reconstruction and Hologram: A New Visualization Tool for Coronary Artery Bypass Grafting Planning](https://arxiv.org/abs/2504.19401)
*Shuo Wang,Tong Ren,Nan Cheng,Li Zhang,Rong Wang*

Main category: physics.med-ph

TLDR: 开发了一种动态心血管全息可视化工具，用于冠状动脉搭桥术（CABG）术前规划，临床反馈证实其有效性。


<details>
  <summary>Details</summary>
Motivation: CABG术前规划需要高级空间可视化，考虑冠状动脉深度、钙化和心包粘连。

Method: 利用4D心脏CT血管造影数据，开发半自动化工作流程，包括心脏结构分割、冠状动脉钙化评分、心包粘连评估，并通过动态全息图展示。

Result: 外科医生对工具的术前规划效用评分高（平均4.57/5.0），全息图心包粘连评分与术中结果强相关（r=0.786）。

Conclusion: 该研究建立了基于患者数据的动态全息可视化框架，临床反馈证实其术前规划的有效性。

Abstract: Background: Coronary artery bypass grafting (CABG) planning requires advanced
spatial visualization and consideration of coronary artery depth,
calcification, and pericardial adhesions. Objective: To develop and evaluate a
dynamic cardiovascular holographic visualization tool for preoperative CABG
planning. Methods: Using 4D cardiac computed tomography angiography data from
14 CABG candidates, we developed a semi-automated workflow for time-resolved
segmentation of cardiac structures, epicardial adipose tissue (EAT), and
coronary arteries with calcium scoring. The workflow incorporated methods for
cardiac segmentation, coronary calcification quantification, visualization of
coronary depth within EAT, and pericardial adhesion assessment through motion
analysis. Dynamic cardiovascular holograms were displayed using the Looking
Glass platform. Thirteen cardiac surgeons evaluated the tool using a Likert
scale. Additionally, pericardial adhesion scores from holograms of 21 patients
(including seven undergoing secondary cardiac surgeries) were compared with
intraoperative findings. Results: Surgeons rated the visualization tool highly
for preoperative planning utility (mean Likert score: 4.57/5.0). Hologram-based
pericardial adhesion scoring strongly correlated with intraoperative findings
(r=0.786, P<0.001). Conclusion: This study establishes a visualization
framework for CABG planning that produces clinically relevant dynamic holograms
from patient-specific data, with clinical feedback confirming its effectiveness
for preoperative planning.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [466] [M2R2: MulitModal Robotic Representation for Temporal Action Segmentation](https://arxiv.org/abs/2504.18662)
*Daniel Sliwowski,Dongheui Lee*

Main category: cs.RO

TLDR: 提出M2R2，一种结合本体感觉和外感觉传感器的多模态特征提取器，用于时间动作分割（TAS），通过新预训练策略实现特征跨模型重用，性能领先46.6%。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器人TAS模型难以重用特征和视觉特征提取器在低可见度场景表现不佳的问题。

Method: 结合本体感觉和外感觉传感器，设计M2R2多模态特征提取器，并引入新预训练策略。

Result: 在REASSEMBLE数据集上性能领先现有模型46.6%，并通过消融实验评估各模态贡献。

Conclusion: M2R2在多模态TAS任务中表现优异，特征重用策略有效。

Abstract: Temporal action segmentation (TAS) has long been a key area of research in
both robotics and computer vision. In robotics, algorithms have primarily
focused on leveraging proprioceptive information to determine skill boundaries,
with recent approaches in surgical robotics incorporating vision. In contrast,
computer vision typically relies on exteroceptive sensors, such as cameras.
Existing multimodal TAS models in robotics integrate feature fusion within the
model, making it difficult to reuse learned features across different models.
Meanwhile, pretrained vision-only feature extractors commonly used in computer
vision struggle in scenarios with limited object visibility. In this work, we
address these challenges by proposing M2R2, a multimodal feature extractor
tailored for TAS, which combines information from both proprioceptive and
exteroceptive sensors. We introduce a novel pretraining strategy that enables
the reuse of learned features across multiple TAS models. Our method achieves
state-of-the-art performance on the REASSEMBLE dataset, a challenging
multimodal robotic assembly dataset, outperforming existing robotic action
segmentation models by 46.6%. Additionally, we conduct an extensive ablation
study to evaluate the contribution of different modalities in robotic TAS
tasks.

</details>

### [467] [Imitation Learning for Autonomous Driving: Insights from Real-World Testing](https://arxiv.org/abs/2504.18847)
*Hidayet Ersin Dursun,Yusuf Güven,Tufan Kumbasar*

Main category: cs.RO

TLDR: 研究设计了一种基于深度学习的自动驾驶系统，并在MIT Racecar上测试其效果，比较了多种DNN模型的表现。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在真实驾驶场景中实时准确生成转向命令的深度学习系统。

Method: 采用模仿学习框架，设计并比较了PD系统、CNN、CNN-LSTM和CNN-NODE等模型，通过增量设计优化模型性能。

Result: CNN-LSTM和CNN-NODE表现最佳，能够处理复杂驾驶场景，而PD系统和CNN在特定情况下表现不足。

Conclusion: 迭代设计过程对开发鲁棒的自动驾驶DNN模型至关重要。

Abstract: This work focuses on the design of a deep learning-based autonomous driving
system deployed and tested on the real-world MIT Racecar to assess its
effectiveness in driving scenarios. The Deep Neural Network (DNN) translates
raw image inputs into real-time steering commands in an end-to-end learning
fashion, following the imitation learning framework. The key design challenge
is to ensure that DNN predictions are accurate and fast enough, at a high
sampling frequency, and result in smooth vehicle operation under different
operating conditions. In this study, we design and compare various DNNs, to
identify the most effective approach for real-time autonomous driving. In
designing the DNNs, we adopted an incremental design approach that involved
enhancing the model capacity and dataset to address the challenges of
real-world driving scenarios. We designed a PD system, CNN, CNN-LSTM, and
CNN-NODE, and evaluated their performance on the real-world MIT Racecar. While
the PD system handled basic lane following, it struggled with sharp turns and
lighting variations. The CNN improved steering but lacked temporal awareness,
which the CNN-LSTM addressed as it resulted in smooth driving performance. The
CNN-NODE performed similarly to the CNN-LSTM in handling driving dynamics, yet
with slightly better driving performance. The findings of this research
highlight the importance of iterative design processes in developing robust
DNNs for autonomous driving applications. The experimental video is available
at https://www.youtube.com/watch?v=FNNYgU--iaY.

</details>

### [468] [Advanced Longitudinal Control and Collision Avoidance for High-Risk Edge Cases in Autonomous Driving](https://arxiv.org/abs/2504.18931)
*Dianwei Chen,Yaobang Gong,Xianfeng Yang*

Main category: cs.RO

TLDR: 提出了一种基于深度强化学习的纵向控制和碰撞避免算法，同时考虑前车和后车行为，显著提高了高速公路场景下的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有ADAS和ADS系统主要关注前车行为，忽略了后车行为，导致高速密集交通中的连锁碰撞。

Method: 结合自适应巡航和紧急制动，利用深度强化学习，并通过数据预处理框架增强训练鲁棒性。

Result: 在模拟高风险场景中有效防止连环碰撞，高速公路三车减速场景下成功率高达99%，远超FHWA标准的36.77%。

Conclusion: 该算法显著提升了复杂交通场景下的安全性，为ADAS和ADS系统提供了更全面的解决方案。

Abstract: Advanced Driver Assistance Systems (ADAS) and Advanced Driving Systems (ADS)
are key to improving road safety, yet most existing implementations focus
primarily on the vehicle ahead, neglecting the behavior of following vehicles.
This shortfall often leads to chain reaction collisions in high speed, densely
spaced traffic particularly when a middle vehicle suddenly brakes and trailing
vehicles cannot respond in time. To address this critical gap, we propose a
novel longitudinal control and collision avoidance algorithm that integrates
adaptive cruising with emergency braking. Leveraging deep reinforcement
learning, our method simultaneously accounts for both leading and following
vehicles. Through a data preprocessing framework that calibrates real-world
sensor data, we enhance the robustness and reliability of the training process,
ensuring the learned policy can handle diverse driving conditions. In simulated
high risk scenarios (e.g., emergency braking in dense traffic), the algorithm
effectively prevents potential pile up collisions, even in situations involving
heavy duty vehicles. Furthermore, in typical highway scenarios where three
vehicles decelerate, the proposed DRL approach achieves a 99% success rate far
surpassing the standard Federal Highway Administration speed concepts guide,
which reaches only 36.77% success under the same conditions.

</details>

### [469] [Diffeomorphic Obstacle Avoidance for Contractive Dynamical Systems via Implicit Representations](https://arxiv.org/abs/2504.18860)
*Ken-Joel Simmoteit,Philipp Schillinger,Leonel Rozo*

Main category: cs.RO

TLDR: 论文提出了一种结合神经收缩动力系统和SDF微分变换的方法，以确保机器人技能的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着机器人任务复杂性和动态性增加，确保技能的安全性和鲁棒性变得至关重要。

Method: 基于神经收缩动力系统，设计了一种全身体避障策略，利用SDF和微分变换保持收缩稳定性。

Result: 在合成数据集和真实厨房任务中验证，方法能局部调整学习到的向量场，同时保持动态特性。

Conclusion: 该方法优于现有技术，能有效平衡安全性和鲁棒性。

Abstract: Ensuring safety and robustness of robot skills is becoming crucial as robots
are required to perform increasingly complex and dynamic tasks. The former is
essential when performing tasks in cluttered environments, while the latter is
relevant to overcome unseen task situations. This paper addresses the challenge
of ensuring both safety and robustness in dynamic robot skills learned from
demonstrations. Specifically, we build on neural contractive dynamical systems
to provide robust extrapolation of the learned skills, while designing a
full-body obstacle avoidance strategy that preserves contraction stability via
diffeomorphic transforms. This is particularly crucial in complex environments
where implicit scene representations, such as Signed Distance Fields (SDFs),
are necessary. To this end, our framework called Signed Distance Field
Diffeomorphic Transform, leverages SDFs and flow-based diffeomorphisms to
achieve contraction-preserving obstacle avoidance. We thoroughly evaluate our
framework on synthetic datasets and several real-world robotic tasks in a
kitchen environment. Our results show that our approach locally adapts the
learned contractive vector field while staying close to the learned dynamics
and without introducing highly-curved motion paths, thus outperforming several
state-of-the-art methods.

</details>

### [470] [PolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-rich Manipulation Using Tactile-Diffusion Policies](https://arxiv.org/abs/2504.19341)
*Jialiang Zhao,Naveen Kuppuswamy,Siyuan Feng,Benjamin Burchfiel,Edward Adelson*

Main category: cs.RO

TLDR: PolyTouch是一种新型机器人手指，集成了触觉、声学和视觉传感，显著提升了触觉感知能力，并用于合成触觉扩散策略，优于传统无触觉策略。


<details>
  <summary>Details</summary>
Motivation: 在非结构化家庭环境中实现稳健的灵巧操作是机器人领域的重大挑战，传统无触觉策略因遮挡和视觉复杂性而表现不佳。

Method: 开发了PolyTouch手指，结合多模态触觉反馈和视觉-本体感知，从人类演示中合成触觉扩散策略。

Result: PolyTouch寿命比商用触觉传感器长20倍，触觉感知策略在多种任务中显著优于无触觉策略。

Conclusion: 多模态触觉传感的集成能加速开发高效接触感知策略，推动家用机器人的可靠性和多功能性。

Abstract: Achieving robust dexterous manipulation in unstructured domestic environments
remains a significant challenge in robotics. Even with state-of-the-art robot
learning methods, haptic-oblivious control strategies (i.e. those relying only
on external vision and/or proprioception) often fall short due to occlusions,
visual complexities, and the need for precise contact interaction control. To
address these limitations, we introduce PolyTouch, a novel robot finger that
integrates camera-based tactile sensing, acoustic sensing, and peripheral
visual sensing into a single design that is compact and durable. PolyTouch
provides high-resolution tactile feedback across multiple temporal scales,
which is essential for efficiently learning complex manipulation tasks.
Experiments demonstrate an at least 20-fold increase in lifespan over
commercial tactile sensors, with a design that is both easy to manufacture and
scalable. We then use this multi-modal tactile feedback along with
visuo-proprioceptive observations to synthesize a tactile-diffusion policy from
human demonstrations; the resulting contact-aware control policy significantly
outperforms haptic-oblivious policies in multiple contact-aware manipulation
policies. This paper highlights how effectively integrating multi-modal contact
sensing can hasten the development of effective contact-aware manipulation
policies, paving the way for more reliable and versatile domestic robots. More
information can be found at https://polytouch.alanz.info/

</details>

### [471] [GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field](https://arxiv.org/abs/2504.19409)
*Zuxing Lu,Xin Yuan,Shaowen Yang,Jingyu Liu,Jiawei Wang,Changyin Sun*

Main category: cs.RO

TLDR: GSFF-SLAM是一种基于3D高斯泼溅的密集语义SLAM系统，通过特征场联合渲染外观、几何和语义特征，显著提升了跟踪精度和渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有语义SLAM系统依赖2D先验监督，但在真实环境中这些信号稀疏且噪声大，限制了性能。

Method: 提出GSFF-SLAM，利用3D高斯泼溅和特征场独立优化特征梯度，支持多种2D先验的语义重建。

Result: 实验显示，GSFF-SLAM在跟踪精度和渲染质量上优于现有方法，使用2D先验时达到95.03% mIoU，速度提升2.9倍。

Conclusion: GSFF-SLAM通过3D高斯泼溅和特征场优化，有效解决了稀疏和噪声信号问题，实现了高性能的语义重建。

Abstract: Semantic-aware 3D scene reconstruction is essential for autonomous robots to
perform complex interactions. Semantic SLAM, an online approach, integrates
pose tracking, geometric reconstruction, and semantic mapping into a unified
framework, shows significant potential. However, existing systems, which rely
on 2D ground truth priors for supervision, are often limited by the sparsity
and noise of these signals in real-world environments. To address this
challenge, we propose GSFF-SLAM, a novel dense semantic SLAM system based on 3D
Gaussian Splatting that leverages feature fields to achieve joint rendering of
appearance, geometry, and N-dimensional semantic features. By independently
optimizing feature gradients, our method supports semantic reconstruction using
various forms of 2D priors, particularly sparse and noisy signals. Experimental
results demonstrate that our approach outperforms previous methods in both
tracking accuracy and photorealistic rendering quality. When utilizing 2D
ground truth priors, GSFF-SLAM achieves state-of-the-art semantic segmentation
performance with 95.03\% mIoU, while achieving up to 2.9$\times$ speedup with
only marginal performance degradation.

</details>

### [472] [Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy](https://arxiv.org/abs/2504.18829)
*Jiayi Chen,Yubin Ke,Lin Peng,He Wang*

Main category: cs.RO

TLDR: 提出一种高效流程，合成多种抓握类型的通用灵巧抓握数据集，并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 通用灵巧抓握是智能机器人的基础技能，但现有数据集和方法在抓握类型和对象类别上受限。

Method: 采用两阶段流程：先优化对象以适应手部模板，再在仿真中局部调整手部。引入接触感知控制策略验证抓握。

Result: 合成数据集包含10.7k对象和9.5M抓握，覆盖31种抓握类型。生成模型在真实实验中成功率82.3%。

Conclusion: 方法显著优于基线，为通用灵巧抓握提供了高质量数据集和生成模型。

Abstract: Generalizable dexterous grasping with suitable grasp types is a fundamental
skill for intelligent robots. Developing such skills requires a large-scale and
high-quality dataset that covers numerous grasp types (i.e., at least those
categorized by the GRASP taxonomy), but collecting such data is extremely
challenging. Existing automatic grasp synthesis methods are often limited to
specific grasp types or object categories, hindering scalability. This work
proposes an efficient pipeline capable of synthesizing contact-rich,
penetration-free, and physically plausible grasps for any grasp type, object,
and articulated hand. Starting from a single human-annotated template for each
hand and grasp type, our pipeline tackles the complicated synthesis problem
with two stages: optimize the object to fit the hand template first, and then
locally refine the hand to fit the object in simulation. To validate the
synthesized grasps, we introduce a contact-aware control strategy that allows
the hand to apply the appropriate force at each contact point to the object.
Those validated grasps can also be used as new grasp templates to facilitate
future synthesis. Experiments show that our method significantly outperforms
previous type-unaware grasp synthesis baselines in simulation. Using our
algorithm, we construct a dataset containing 10.7k objects and 9.5M grasps,
covering 31 grasp types in the GRASP taxonomy. Finally, we train a
type-conditional generative model that successfully performs the desired grasp
type from single-view object point clouds, achieving an 82.3% success rate in
real-world experiments. Project page: https://pku-epic.github.io/Dexonomy.

</details>

### [473] [Quantitative evaluation of brain-inspired vision sensors in high-speed robotic perception](https://arxiv.org/abs/2504.19253)
*Taoyi Wang,Lijian Wang,Yihan Lin,Mingtao Ou,Yuguo Chen,Xinglong Ji,Rong Zhao*

Main category: cs.RO

TLDR: 论文提出了首个定量评估框架，用于比较两类脑启发视觉传感器（BVS）在高速动态机器人感知中的性能，发现EVS在高速稀疏场景表现良好，而Tianmouc在各种场景下均表现稳定。


<details>
  <summary>Details</summary>
Motivation: 传统相机在高速动态条件下因运动模糊影响性能，而BVS因其高时间分辨率和低功耗成为替代方案。

Method: 建立统一测试协议，包括跨传感器校准、标准化测试平台和质量指标，评估EVS和Tianmouc在结构信息捕捉和任务性能（如角点检测和运动估计）中的表现。

Result: EVS在高速稀疏场景表现良好，但在高速复杂场景受限；Tianmouc在各种速度和场景下表现一致。

Conclusion: 研究为BVS技术的应用选择提供了依据，支持其进一步发展和优化。

Abstract: Perception systems in robotics encounter significant challenges in high-speed
and dynamic conditions when relying on traditional cameras, where motion blur
can compromise spatial feature integrity and task performance. Brain-inspired
vision sensors (BVS) have recently gained attention as an alternative, offering
high temporal resolution with reduced bandwidth and power requirements. Here,
we present the first quantitative evaluation framework for two representative
classes of BVSs in variable-speed robotic sensing, including event-based vision
sensors (EVS) that detect asynchronous temporal contrasts, and the
primitive-based sensor Tianmouc that employs a complementary mechanism to
encode both spatiotemporal changes and intensity. A unified testing protocol is
established, including crosssensor calibrations, standardized testing
platforms, and quality metrics to address differences in data modality. From an
imaging standpoint, we evaluate the effects of sensor non-idealities, such as
motion-induced distortion, on the capture of structural information. For
functional benchmarking, we examine task performance in corner detection and
motion estimation under different rotational speeds. Results indicate that EVS
performs well in highspeed, sparse scenarios and in modestly fast, complex
scenes, but exhibits performance limitations in high-speed, cluttered settings
due to pixel-level bandwidth variations and event rate saturation. In
comparison, Tianmouc demonstrates consistent performance across sparse and
complex scenarios at various speeds, supported by its global, precise,
high-speed spatiotemporal gradient samplings. These findings offer valuable
insights into the applicationdependent suitability of BVS technologies and
support further advancement in this area.

</details>

### [474] [NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks](https://arxiv.org/abs/2504.19854)
*Chia-Yu Hung,Qi Sun,Pengfei Hong,Amir Zadeh,Chuan Li,U-Xuan Tan,Navonil Majumder,Soujanya Poria*

Main category: cs.RO

TLDR: NORA是一个3B参数的视觉-语言-动作模型，旨在减少计算开销，同时保持任务性能。它基于Qwen-2.5-VL-3B模型，结合真实机器人演示数据，在实时机器人环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在视觉编码和计算开销方面存在局限性，不适合实时机器人环境。

Method: 采用Qwen-2.5-VL-3B作为主干，结合970k真实机器人演示数据，使用FAST+分词器生成动作序列。

Result: NORA在任务性能上优于现有大型VLA模型，计算开销显著降低。

Conclusion: NORA为实时机器人自主性提供了更实用的解决方案。

Abstract: Existing Visual-Language-Action (VLA) models have shown promising performance
in zero-shot scenarios, demonstrating impressive task execution and reasoning
capabilities. However, a significant challenge arises from the limitations of
visual encoding, which can result in failures during tasks such as object
grasping. Moreover, these models typically suffer from high computational
overhead due to their large sizes, often exceeding 7B parameters. While these
models excel in reasoning and task planning, the substantial computational
overhead they incur makes them impractical for real-time robotic environments,
where speed and efficiency are paramount. To address the limitations of
existing VLA models, we propose NORA, a 3B-parameter model designed to reduce
computational overhead while maintaining strong task performance. NORA adopts
the Qwen-2.5-VL-3B multimodal model as its backbone, leveraging its superior
visual-semantic understanding to enhance visual reasoning and action grounding.
Additionally, our \model{} is trained on 970k real-world robot demonstrations
and equipped with the FAST+ tokenizer for efficient action sequence generation.
Experimental results demonstrate that NORA outperforms existing large-scale VLA
models, achieving better task performance with significantly reduced
computational overhead, making it a more practical solution for real-time
robotic autonomy.

</details>

### [475] [GAN-SLAM: Real-Time GAN Aided Floor Plan Creation Through SLAM](https://arxiv.org/abs/2504.19653)
*Leon Davies,Baihua Li,Mohamad Saada,Simon Sølvsten,Qinggang Meng*

Main category: cs.RO

TLDR: 论文提出了一种名为GAN-SLAM的新方法，利用生成对抗网络（GAN）在SLAM过程中清理和补全占用网格，显著提高了2D地图的质量和保真度。


<details>
  <summary>Details</summary>
Motivation: SLAM系统在动态环境中常因运动噪声导致2D地图（如占用网格地图）质量下降，影响下游任务（如平面图生成）。

Method: 结合3D SLAM中的精确位姿估计技术，将其适配到2D形式，并利用GAN清理和补全占用网格。

Result: 实验表明，GAN-SLAM显著减少了噪声和误差，提高了地图质量，适用于大规模复杂环境。

Conclusion: GAN-SLAM为SLAM领域提供了重要进展，展示了GAN在占用网格误差校正中的潜力。

Abstract: SLAM is a fundamental component of modern autonomous systems, providing
robots and their operators with a deeper understanding of their environment.
SLAM systems often encounter challenges due to the dynamic nature of robotic
motion, leading to inaccuracies in mapping quality, particularly in 2D
representations such as Occupancy Grid Maps. These errors can significantly
degrade map quality, hindering the effectiveness of specific downstream tasks
such as floor plan creation. To address this challenge, we introduce our novel
'GAN-SLAM', a new SLAM approach that leverages Generative Adversarial Networks
to clean and complete occupancy grids during the SLAM process, reducing the
impact of noise and inaccuracies introduced on the output map. We adapt and
integrate accurate pose estimation techniques typically used for 3D SLAM into a
2D form. This enables the quality improvement 3D LiDAR-odometry has seen in
recent years to be effective for 2D representations. Our results demonstrate
substantial improvements in map fidelity and quality, with minimal noise and
errors, affirming the effectiveness of GAN-SLAM for real-world mapping
applications within large-scale complex environments. We validate our approach
on real-world data operating in real-time, and on famous examples of 2D maps.
The improved quality of the output map enables new downstream tasks, such as
floor plan drafting, further enhancing the capabilities of autonomous systems.
Our novel approach to SLAM offers a significant step forward in the field,
improving the usability for SLAM in mapping-based tasks, and offers insight
into the usage of GANs for OGM error correction.

</details>

### [476] [Transformation & Translation Occupancy Grid Mapping: 2-Dimensional Deep Learning Refined SLAM](https://arxiv.org/abs/2504.19654)
*Leon Davies,Baihua Li,Mohamad Saada,Simon Sølvsten,Qinggang Meng*

Main category: cs.RO

TLDR: 论文提出了一种新型的2D SLAM方法TT-OGM，结合3D SLAM技术和GANs，显著提升了复杂环境中地图的质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 2D SLAM在复杂环境中存在定位和地图构建的误差问题，现有方法如OGM在复杂场景中效果不佳，需要改进。

Method: 提出TT-OGM方法，结合3D SLAM的位姿估计技术和GANs进行误差校正，并通过DRL生成训练数据。

Result: 在真实环境中实时运行，并在多种复杂场景中验证了方法的通用性和优越性。

Conclusion: TT-OGM显著提升了2D SLAM在复杂场景中的地图质量、准确性和可靠性。

Abstract: SLAM (Simultaneous Localisation and Mapping) is a crucial component for
robotic systems, providing a map of an environment, the current location and
previous trajectory of a robot. While 3D LiDAR SLAM has received notable
improvements in recent years, 2D SLAM lags behind. Gradual drifts in odometry
and pose estimation inaccuracies hinder modern 2D LiDAR-odometry algorithms in
large complex environments. Dynamic robotic motion coupled with inherent
estimation based SLAM processes introduce noise and errors, degrading map
quality. Occupancy Grid Mapping (OGM) produces results that are often noisy and
unclear. This is due to the fact that evidence based mapping represents maps
according to uncertain observations. This is why OGMs are so popular in
exploration or navigation tasks. However, this also limits OGMs' effectiveness
for specific mapping based tasks such as floor plan creation in complex scenes.
To address this, we propose our novel Transformation and Translation Occupancy
Grid Mapping (TT-OGM). We adapt and enable accurate and robust pose estimation
techniques from 3D SLAM to the world of 2D and mitigate errors to improve map
quality using Generative Adversarial Networks (GANs). We introduce a novel data
generation method via deep reinforcement learning (DRL) to build datasets large
enough for training a GAN for SLAM error correction. We demonstrate our SLAM in
real-time on data collected at Loughborough University. We also prove its
generalisability on a variety of large complex environments on a collection of
large scale well-known 2D occupancy maps. Our novel approach enables the
creation of high quality OGMs in complex scenes, far surpassing the
capabilities of current SLAM algorithms in terms of quality, accuracy and
reliability.

</details>

### [477] [Human-Centered AI and Autonomy in Robotics: Insights from a Bibliometric Study](https://arxiv.org/abs/2504.19848)
*Simona Casini,Pietro Ducange,Francesco Marcelloni,Lorenzo Pollini*

Main category: cs.RO

TLDR: 本文通过文献计量分析探讨了智能自主机器人系统的发展趋势，重点关注人本AI（HCAI）在平衡人机协作中的作用，并将研究成果映射到IBM MAPE-K架构中。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能在自主机器人系统中的进展及其对人机协作的影响，强调HCAI在确保性能与责任平衡中的重要性。

Method: 使用SciMAT和VOSViewer对Scopus数据库中的数据进行文献计量分析，识别学术趋势和新兴主题。

Result: 揭示了智能自适应机器人行为的研究趋势，并突出了HCAI架构的关键作用。

Conclusion: 研究结果为实际机器人自主系统的开发提供了指导，强调了HCAI在实现可靠、安全和可信赖系统中的应用。

Abstract: The development of autonomous robotic systems offers significant potential
for performing complex tasks with precision and consistency. Recent advances in
Artificial Intelligence (AI) have enabled more capable intelligent automation
systems, addressing increasingly complex challenges. However, this progress
raises questions about human roles in such systems. Human-Centered AI (HCAI)
aims to balance human control and automation, ensuring performance enhancement
while maintaining creativity, mastery, and responsibility. For real-world
applications, autonomous robots must balance task performance with reliability,
safety, and trustworthiness. Integrating HCAI principles enhances human-robot
collaboration and ensures responsible operation.
  This paper presents a bibliometric analysis of intelligent autonomous robotic
systems, utilizing SciMAT and VOSViewer to examine data from the Scopus
database. The findings highlight academic trends, emerging topics, and AI's
role in self-adaptive robotic behaviour, with an emphasis on HCAI architecture.
These insights are then projected onto the IBM MAPE-K architecture, with the
goal of identifying how these research results map into actual robotic
autonomous systems development efforts for real-world scenarios.

</details>

### [478] [Real-Time Imitation of Human Head Motions, Blinks and Emotions by Nao Robot: A Closed-Loop Approach](https://arxiv.org/abs/2504.19985)
*Keyhan Rayati,Amirhossein Feizi,Alireza Beigy,Pourya Shahverdi,Mehdi Tale Masouleh,Ahmad Kalhor*

Main category: cs.RO

TLDR: 提出了一种实时模仿人类头部运动的Nao机器人方法，结合MediaPipe和DeepFace库，提升人机交互体验，尤其对自闭症儿童有益。


<details>
  <summary>Details</summary>
Motivation: 提升人机交互的自然性和精确性，特别是为自闭症儿童提供更好的沟通工具。

Method: 利用MediaPipe和DeepFace库捕捉头部运动和情感表达，采用闭环反馈机制实现高精度模仿。

Result: 在俯仰和偏转运动上分别达到96.3和98.9的R2分数，表现优异。

Conclusion: 该方法成功整合实时头部模仿和情感识别，有望改善特殊需求人群的交互体验。

Abstract: This paper introduces a novel approach for enabling real-time imitation of
human head motion by a Nao robot, with a primary focus on elevating human-robot
interactions. By using the robust capabilities of the MediaPipe as a computer
vision library and the DeepFace as an emotion recognition library, this
research endeavors to capture the subtleties of human head motion, including
blink actions and emotional expressions, and seamlessly incorporate these
indicators into the robot's responses. The result is a comprehensive framework
which facilitates precise head imitation within human-robot interactions,
utilizing a closed-loop approach that involves gathering real-time feedback
from the robot's imitation performance. This feedback loop ensures a high
degree of accuracy in modeling head motion, as evidenced by an impressive R2
score of 96.3 for pitch and 98.9 for yaw. Notably, the proposed approach holds
promise in improving communication for children with autism, offering them a
valuable tool for more effective interaction. In essence, proposed work
explores the integration of real-time head imitation and real-time emotion
recognition to enhance human-robot interactions, with potential benefits for
individuals with unique communication needs.

</details>

### [479] [Socially-Aware Autonomous Driving: Inferring Yielding Intentions for Safer Interactions](https://arxiv.org/abs/2504.20004)
*Jing Wang,Yan Jin,Hamid Taghavifar,Fei Ding,Chongfeng Wei*

Main category: cs.RO

TLDR: 论文提出了一种基于有向无环图（DAG）的社会意图估计算法和深度强化学习（DRL）决策框架，以提高自动驾驶车辆（AVs）在变道场景中的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆（AVs）与人类驾驶车辆（HVs）共存时，安全和可靠的决策是主要挑战，尤其是在变道和与周围HVs交互时。准确估计HVs的意图对AVs的决策至关重要。

Method: 提出了一种基于DAG的社会意图估计算法，并结合DRL决策框架，通过模拟环境中的变道场景进行测试。

Result: 实验结果表明，该方法显著提升了AVs在变道时的安全性和效率。

Conclusion: 该研究为AVs在复杂交通环境中的安全决策提供了有效解决方案。

Abstract: Since the emergence of autonomous driving technology, it has advanced rapidly
over the past decade. It is becoming increasingly likely that autonomous
vehicles (AVs) would soon coexist with human-driven vehicles (HVs) on the
roads. Currently, safety and reliable decision-making remain significant
challenges, particularly when AVs are navigating lane changes and interacting
with surrounding HVs. Therefore, precise estimation of the intentions of
surrounding HVs can assist AVs in making more reliable and safe lane change
decision-making. This involves not only understanding their current behaviors
but also predicting their future motions without any direct communication.
However, distinguishing between the passing and yielding intentions of
surrounding HVs still remains ambiguous. To address the challenge, we propose a
social intention estimation algorithm rooted in Directed Acyclic Graph (DAG),
coupled with a decision-making framework employing Deep Reinforcement Learning
(DRL) algorithms. To evaluate the method's performance, the proposed framework
can be tested and applied in a lane-changing scenario within a simulated
environment. Furthermore, the experiment results demonstrate how our approach
enhances the ability of AVs to navigate lane changes safely and efficiently on
roads.

</details>

<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [480] [Versatile Framework for Song Generation with Prompt-based Control](https://arxiv.org/abs/2504.19062)
*Yu Zhang,Wenxiang Guo,Changhao Pan,Zhiyuan Zhu,Ruiqi Li,Jingyu Lu,Rongjie Huang,Ruiyuan Zhang,Zhiqing Hong,Ziyue Jiang,Zhou Zhao*

Main category: eess.AS

TLDR: VersBand是一个多任务歌曲生成框架，通过VocalBand和AccompBand等模型实现高质量、对齐的歌曲生成，支持基于提示的控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成基于提示的、对齐的人声和伴奏方面表现不佳，且不支持多任务。

Method: VersBand包含VocalBand（基于流匹配的人声生成模型）、AccompBand（基于流和变压器的伴奏生成模型）及LyricBand和MelodyBand（歌词和旋律生成模型）。

Result: 实验表明，VersBand在多个任务中优于基线模型。

Conclusion: VersBand通过多任务框架实现了高质量、可控的歌曲生成。

Abstract: Song generation focuses on producing controllable high-quality songs based on
various prompts. However, existing methods struggle to generate vocals and
accompaniments with prompt-based control and proper alignment. Additionally,
they fall short in supporting various tasks. To address these challenges, we
introduce VersBand, a multi-task song generation framework for synthesizing
high-quality, aligned songs with prompt-based control. VersBand comprises these
primary models: 1) VocalBand, a decoupled model, leverages the flow-matching
method for generating singing styles, pitches, and mel-spectrograms, allowing
fast, high-quality vocal generation with style control. 2) AccompBand, a
flow-based transformer model, incorporates the Band-MOE, selecting suitable
experts for enhanced quality, alignment, and control. This model allows for
generating controllable, high-quality accompaniments aligned with vocals. 3)
Two generation models, LyricBand for lyrics and MelodyBand for melodies,
contribute to the comprehensive multi-task song generation system, allowing for
extensive control based on multiple prompts. Experimental results demonstrate
that VersBand performs better over baseline models across multiple song
generation tasks using objective and subjective metrics. Audio samples are
available at https://VersBand.github.io.

</details>

### [481] [Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation](https://arxiv.org/abs/2504.18539)
*Sungnyun Kim,Sungwoo Cho,Sangmin Bae,Kangwook Jang,Se-Young Yun*

Main category: eess.AS

TLDR: CAV2vec是一种自监督学习框架，用于处理音频-视觉联合损坏，通过自蒸馏和多任务学习提升语音识别的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决现有音频-视觉语音识别（AVSR）在视觉损坏（如唇部遮挡或模糊视频）下的性能不足问题。

Method: 采用自蒸馏方法，学生模型通过损坏的输入帧预测教师模型生成的干净目标，并结合跨模态知识蒸馏和对齐。

Result: 在多种损坏环境下显著提升了语音识别的准确性。

Conclusion: CAV2vec通过处理联合损坏，实现了更可靠的音频-视觉融合，适用于复杂环境。

Abstract: Audio-visual speech recognition (AVSR) incorporates auditory and visual
modalities to improve recognition accuracy, particularly in noisy environments
where audio-only speech systems are insufficient. While previous research has
largely addressed audio disruptions, few studies have dealt with visual
corruptions, e.g., lip occlusions or blurred videos, which are also
detrimental. To address this real-world challenge, we propose CAV2vec, a novel
self-supervised speech representation learning framework particularly designed
to handle audio-visual joint corruption. CAV2vec employs a self-distillation
approach with a corrupted prediction task, where the student model learns to
predict clean targets, generated by the teacher model, with corrupted input
frames. Specifically, we suggest a unimodal multi-task learning, which distills
cross-modal knowledge and aligns the corrupted modalities, by predicting clean
audio targets with corrupted videos, and clean video targets with corrupted
audios. This strategy mitigates the dispersion in the representation space
caused by corrupted modalities, leading to more reliable and robust
audio-visual fusion. Our experiments on robust AVSR benchmarks demonstrate that
the corrupted representation learning method significantly enhances recognition
accuracy across generalized environments involving various types of corruption.

</details>

### [482] [Enhancing Cochlear Implant Signal Coding with Scaled Dot-Product Attention](https://arxiv.org/abs/2504.19046)
*Billel Essaid,Hamza Kheddar,Noureddine Batel*

Main category: eess.AS

TLDR: 本文探讨了使用深度学习技术生成人工耳蜗电刺激图，与传统ACE策略相比，性能接近且更具灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统人工耳蜗编码策略（如ACE）在适应性和精确性上存在局限，深度学习技术有望提供更优解决方案。

Method: 提出深度学习模型生成电刺激图，并与ACE策略通过STOI指标比较音频信号重建的清晰度。

Result: 深度学习模型的STOI得分为0.6031，接近ACE的0.6126，且在灵活性和适应性上更具优势。

Conclusion: 研究表明AI技术可提升人工耳蜗的个性化和效率，为未来技术发展提供新方向。

Abstract: Cochlear implants (CIs) play a vital role in restoring hearing for
individuals with severe to profound sensorineural hearing loss by directly
stimulating the auditory nerve with electrical signals. While traditional
coding strategies, such as the advanced combination encoder (ACE), have proven
effective, they are constrained by their adaptability and precision. This paper
investigates the use of deep learning (DL) techniques to generate
electrodograms for CIs, presenting our model as an advanced alternative. We
compared the performance of our model with the ACE strategy by evaluating the
intelligibility of reconstructed audio signals using the short-time objective
intelligibility (STOI) metric. The results indicate that our model achieves a
STOI score of 0.6031, closely approximating the 0.6126 score of the ACE
strategy, and offers potential advantages in flexibility and adaptability. This
study underscores the benefits of incorporating artificial intelligent (AI)
into CI technology, such as enhanced personalization and efficiency.

</details>

<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [483] [Sharp higher order convergence rates for the Adam optimizer](https://arxiv.org/abs/2504.19426)
*Steffen Dereich,Arnulf Jentzen,Adrian Riekert*

Main category: math.OC

TLDR: 本文研究了Adam和RMSprop优化器的收敛速度，发现Adam与动量法一样具有最优收敛速率，而RMSprop仅达到标准梯度下降的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 研究不同优化方法（如Adam和RMSprop）在训练深度神经网络时的收敛速度，以验证其效率。

Method: 通过理论分析比较Adam、RMSprop和标准梯度下降的收敛速率，基于Hessian矩阵的条件数。

Result: Adam的收敛速率为(√x - 1)(√x + 1)^{-1}，优于RMSprop的(x - 1)(x + 1)^{-1}。

Conclusion: Adam在收敛速度上优于RMSprop，与动量法相当，证明了其高效性。

Abstract: Gradient descent based optimization methods are the methods of choice to
train deep neural networks in machine learning. Beyond the standard gradient
descent method, also suitable modified variants of standard gradient descent
involving acceleration techniques such as the momentum method and/or adaptivity
techniques such as the RMSprop method are frequently considered optimization
methods. These days the most popular of such sophisticated optimization schemes
is presumably the Adam optimizer that has been proposed in 2014 by Kingma and
Ba. A highly relevant topic of research is to investigate the speed of
convergence of such optimization methods. In particular, in 1964 Polyak showed
that the standard gradient descent method converges in a neighborhood of a
strict local minimizer with rate (x - 1)(x + 1)^{-1} while momentum achieves
the (optimal) strictly faster convergence rate (\sqrt{x} - 1)(\sqrt{x} +
1)^{-1} where x \in (1,\infty) is the condition number (the ratio of the
largest and the smallest eigenvalue) of the Hessian of the objective function
at the local minimizer. It is the key contribution of this work to reveal that
Adam also converges with the strictly faster convergence rate (\sqrt{x} -
1)(\sqrt{x} + 1)^{-1} while RMSprop only converges with the convergence rate (x
- 1)(x + 1)^{-1}.

</details>

<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [484] [QuantBench: Benchmarking AI Methods for Quantitative Investment](https://arxiv.org/abs/2504.18600)
*Saizhuo Wang,Hao Kong,Jiadong Guo,Fengrui Hua,Yiyan Qi,Wanyun Zhou,Jiahao Zheng,Xinyu Wang,Lionel M. Ni,Jian Guo*

Main category: q-fin.CP

TLDR: QuantBench是一个工业级基准平台，旨在解决AI在量化投资领域缺乏标准化基准的问题，提供标准化、灵活性和全流程覆盖。


<details>
  <summary>Details</summary>
Motivation: 量化投资领域的AI研究缺乏与行业实践一致的标准化基准，阻碍了研究进展和实际应用。

Method: 开发QuantBench平台，具备标准化、灵活性和全流程覆盖的特点，支持多种AI算法的集成。

Result: 实证研究揭示了关键研究方向，如持续学习、关系金融数据建模和低信噪比环境下的抗过拟合方法。

Conclusion: QuantBench为评估提供共同基础，促进研究者与实践者合作，加速AI在量化投资中的进展。

Abstract: The field of artificial intelligence (AI) in quantitative investment has seen
significant advancements, yet it lacks a standardized benchmark aligned with
industry practices. This gap hinders research progress and limits the practical
application of academic innovations. We present QuantBench, an industrial-grade
benchmark platform designed to address this critical need. QuantBench offers
three key strengths: (1) standardization that aligns with quantitative
investment industry practices, (2) flexibility to integrate various AI
algorithms, and (3) full-pipeline coverage of the entire quantitative
investment process. Our empirical studies using QuantBench reveal some critical
research directions, including the need for continual learning to address
distribution shifts, improved methods for modeling relational financial data,
and more robust approaches to mitigate overfitting in low signal-to-noise
environments. By providing a common ground for evaluation and fostering
collaboration between researchers and practitioners, QuantBench aims to
accelerate progress in AI for quantitative investment, similar to the impact of
benchmark platforms in computer vision and natural language processing.

</details>

<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [485] [BQSched: A Non-intrusive Scheduler for Batch Concurrent Queries via Reinforcement Learning](https://arxiv.org/abs/2504.19142)
*Chenhao Xu,Chunyu Chen,Jinglin Peng,Jiannan Wang,Jun Gao*

Main category: cs.DB

TLDR: BQSched是一种基于强化学习的非侵入式批量查询调度器，通过注意力机制和优化的PPO算法显著提升调度效率。


<details>
  <summary>Details</summary>
Motivation: 现有工具依赖简单启发式规则，难以处理复杂查询特征和相互影响，而强化学习方法虽具潜力，但直接应用面临调度空间大、采样成本高和样本利用率低的问题。

Method: BQSched设计了基于注意力的状态表示和IQ-PPO算法，结合自适应掩码、查询聚类和增量模拟器优化策略。

Result: 实验表明，BQSched在TPC-DS基准测试中平均减少34%和13%的总完成时间，优于启发式策略和现有RL调度器。

Conclusion: BQSched在效率、稳定性、可扩展性和适应性方面表现优异，是首个基于RL的非侵入式批量查询调度器。

Abstract: Most large enterprises build predefined data pipelines and execute them
periodically to process operational data using SQL queries for various tasks. A
key issue in minimizing the overall makespan of these pipelines is the
efficient scheduling of concurrent queries within the pipelines. Existing tools
mainly rely on simple heuristic rules due to the difficulty of expressing the
complex features and mutual influences of queries. The latest reinforcement
learning (RL) based methods have the potential to capture these patterns from
feedback, but it is non-trivial to apply them directly due to the large
scheduling space, high sampling cost, and poor sample utilization.
  Motivated by these challenges, we propose BQSched, a non-intrusive Scheduler
for Batch concurrent Queries via reinforcement learning. Specifically, BQSched
designs an attention-based state representation to capture the complex query
patterns, and proposes IQ-PPO, an auxiliary task-enhanced proximal policy
optimization (PPO) algorithm, to fully exploit the rich signals of Individual
Query completion in logs. Based on the RL framework above, BQSched further
introduces three optimization strategies, including adaptive masking to prune
the action space, scheduling gain-based query clustering to deal with large
query sets, and an incremental simulator to reduce sampling cost. To our
knowledge, BQSched is the first non-intrusive batch query scheduler via RL.
Extensive experiments show that BQSched can significantly improve the
efficiency and stability of batch query scheduling, while also achieving
remarkable scalability and adaptability in both data and queries. For example,
across all DBMSs and scales tested, BQSched reduces the overall makespan of
batch queries on TPC-DS benchmark by an average of 34% and 13%, compared with
the commonly used heuristic strategy and the adapted RL-based scheduler,
respectively.

</details>

### [486] [MINT: Multi-Vector Search Index Tuning](https://arxiv.org/abs/2504.20018)
*Jiongli Zhu,Yue Wang,Bailu Ding,Philip A. Bernstein,Vivek Narasayya,Surajit Chaudhuri*

Main category: cs.DB

TLDR: 论文提出了一种多向量搜索索引调优框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多向量搜索在多模态和多特征场景中日益重要，但其索引调优仍缺乏研究。

Method: 定义多向量搜索索引调优问题，并开发算法以最小化延迟并满足存储和召回约束。

Result: 与基线相比，延迟提升了2.1倍至8.3倍。

Conclusion: 提出的框架有效解决了多向量搜索索引调优的挑战。

Abstract: Vector search plays a crucial role in many real-world applications. In
addition to single-vector search, multi-vector search becomes important for
multi-modal and multi-feature scenarios today. In a multi-vector database, each
row is an item, each column represents a feature of items, and each cell is a
high-dimensional vector. In multi-vector databases, the choice of indexes can
have a significant impact on performance. Although index tuning for relational
databases has been extensively studied, index tuning for multi-vector search
remains unclear and challenging. In this paper, we define multi-vector search
index tuning and propose a framework to solve it. Specifically, given a
multi-vector search workload, we develop algorithms to find indexes that
minimize latency and meet storage and recall constraints. Compared to the
baseline, our latency achieves 2.1X to 8.3X speedup.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [487] [World Food Atlas Project](https://arxiv.org/abs/2504.18727)
*Ali Rostami,Z Xie,A Ishino,Y Yamakata,K Aizawa,Ramesh Jain*

Main category: cs.IR

TLDR: 论文提出构建世界食物图谱（WFA）的尝试，包括食物知识图谱（FKG）和食物记录应用（FoodLog Athl和RecipeLog），旨在帮助人们更好地了解和掌控食物。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情迫使人们居家生活，促使人们意识到食物对身体的影响，因此需要更好地了解和掌控食物。

Method: 1. 构建食物知识图谱（FKG），基于食谱和营养数据表示食物与成分的关系；2. 开发食物记录应用（FoodLog Athl和RecipeLog），收集详细的饮食习惯数据。

Result: 提出了两种构建WFA的方法，并讨论了整合这些方法时需解决的问题。

Conclusion: 通过FKG和食物记录应用的结合，有望为构建WFA提供基础，帮助人们更深入地了解和控制食物。

Abstract: A coronavirus pandemic is forcing people to be "at home" all over the world.
In a life of hardly ever going out, we would have realized how the food we eat
affects our bodies. What can we do to know our food more and control it better?
To give us a clue, we are trying to build a World Food Atlas (WFA) that
collects all the knowledge about food in the world. In this paper, we present
two of our trials. The first is the Food Knowledge Graph (FKG), which is a
graphical representation of knowledge about food and ingredient relationships
derived from recipes and food nutrition data. The second is the FoodLog Athl
and the RecipeLog that are applications for collecting people's detailed
records about food habit. We also discuss several problems that we try to solve
to build the WFA by integrating these two ideas.

</details>

### [488] [Generative Product Recommendations for Implicit Superlative Queries](https://arxiv.org/abs/2504.18748)
*Kaustubh D. Dhole,Nikhita Vedula,Saar Kuzi,Giuseppe Castellucci,Eugene Agichtein,Shervin Malmasi*

Main category: cs.IR

TLDR: 论文探讨了推荐系统中用户通过模糊查询（如“最佳越野跑鞋”）寻找产品时面临的挑战，提出利用大语言模型（LLMs）生成隐含属性并改进推荐。


<details>
  <summary>Details</summary>
Motivation: 用户常通过模糊查询寻找产品，但现有系统难以处理此类查询，需改进推荐方法。

Method: 提出SUPERB四点标注框架，结合LLM生成产品标注，并评估现有检索和排序方法。

Result: 实证评估了多种方法，为新数据集提供了见解，并探讨了实际应用。

Conclusion: LLMs能有效处理模糊查询，SUPERB框架为改进推荐系统提供了新思路。

Abstract: In Recommender Systems, users often seek the best products through indirect,
vague, or under-specified queries, such as "best shoes for trail running". Such
queries, also referred to as implicit superlative queries, pose a significant
challenge for standard retrieval and ranking systems as they lack an explicit
mention of attributes and require identifying and reasoning over complex
factors. We investigate how Large Language Models (LLMs) can generate implicit
attributes for ranking as well as reason over them to improve product
recommendations for such queries. As a first step, we propose a novel
four-point schema for annotating the best product candidates for superlative
queries called SUPERB, paired with LLM-based product annotations. We then
empirically evaluate several existing retrieval and ranking approaches on our
new dataset, providing insights and discussing their integration into
real-world e-commerce production systems.

</details>

### [489] [Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation](https://arxiv.org/abs/2504.19754)
*Carlo Merola,Jaspinder Singh*

Main category: cs.IR

TLDR: 本文分析了两种优化检索增强生成（RAG）系统的技术：延迟分块和上下文检索，比较了它们在保持语义连贯性和计算效率方面的优劣。


<details>
  <summary>Details</summary>
Motivation: 解决传统分块方法在RAG系统中导致上下文碎片化的问题，探索更有效的知识管理技术。

Method: 对延迟分块和上下文检索进行严格分析，评估它们在RAG系统中的效果和效率。

Result: 上下文检索能更好地保持语义连贯性但计算成本高，延迟分块效率更高但牺牲了相关性和完整性。

Conclusion: 两种技术各有优劣，需根据具体需求权衡选择。

Abstract: Retrieval-augmented generation (RAG) has become a transformative approach for
enhancing large language models (LLMs) by grounding their outputs in external
knowledge sources. Yet, a critical question persists: how can vast volumes of
external knowledge be managed effectively within the input constraints of LLMs?
Traditional methods address this by chunking external documents into smaller,
fixed-size segments. While this approach alleviates input limitations, it often
fragments context, resulting in incomplete retrieval and diminished coherence
in generation. To overcome these shortcomings, two advanced techniques, late
chunking and contextual retrieval, have been introduced, both aiming to
preserve global context. Despite their potential, their comparative strengths
and limitations remain unclear. This study presents a rigorous analysis of late
chunking and contextual retrieval, evaluating their effectiveness and
efficiency in optimizing RAG systems. Our results indicate that contextual
retrieval preserves semantic coherence more effectively but requires greater
computational resources. In contrast, late chunking offers higher efficiency
but tends to sacrifice relevance and completeness.

</details>

### [490] [Feature Fusion Revisited: Multimodal CTR Prediction for MMCTR Challenge](https://arxiv.org/abs/2504.18961)
*Junjie Zhou*

Main category: cs.IR

TLDR: 论文探讨了多模态大语言模型（MLLMs）在推荐系统中的应用，并针对高延迟问题提出了改进方法。团队在EReL@MIR研讨会的竞赛中获奖，并公开了代码和模型权重。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在推荐系统中潜力巨大，但高延迟问题限制了其实际应用。

Method: 团队在竞赛中尝试了多种方法，优化多模态表示学习效率，并提交了技术报告。

Result: 团队在Task 2（多模态CTR预测）中获奖，并公开了代码和模型权重。

Conclusion: 未来工作应关注如何将推荐信号有效整合到多模态表示中。

Abstract: With the rapid advancement of Multimodal Large Language Models (MLLMs), an
increasing number of researchers are exploring their application in
recommendation systems. However, the high latency associated with large models
presents a significant challenge for such use cases. The EReL@MIR workshop
provided a valuable opportunity to experiment with various approaches aimed at
improving the efficiency of multimodal representation learning for information
retrieval tasks. As part of the competition's requirements, participants were
mandated to submit a technical report detailing their methodologies and
findings. Our team was honored to receive the award for Task 2 - Winner
(Multimodal CTR Prediction). In this technical report, we present our methods
and key findings. Additionally, we propose several directions for future work,
particularly focusing on how to effectively integrate recommendation signals
into multimodal representations. The codebase for our implementation is
publicly available at: https://github.com/Lattice-zjj/MMCTR_Code, and the
trained model weights can be accessed at:
https://huggingface.co/FireFlyCourageous/MMCTR_DIN_MicroLens_1M_x1.

</details>

<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [491] [Nonconvex Linear System Identification with Minimal State Representation](https://arxiv.org/abs/2504.18791)
*Uday Kiran Reddy Tadipatri,Benjamin D. Haeffele,Joshua Agterberg,Ingvar Ziemann,René Vidal*

Main category: eess.SY

TLDR: 论文提出两种非凸重构方法（Burer-Monterio分解和直接优化系统参数）以解决低阶线性系统辨识问题，显著提升计算效率并降低统计误差。


<details>
  <summary>Details</summary>
Motivation: 传统Hankel秩最小化方法依赖凸松弛，计算成本高且需要多次SVD。

Method: 提出两种非凸重构方法：BM分解和直接优化系统参数，避免重复SVD计算。

Result: 证明直接优化系统参数可降低统计误差和样本复杂度，且算法在多项式时间内达到全局最优。

Conclusion: 所提方法在计算效率和理论性能上优于传统方法，并通过实验验证。

Abstract: Low-order linear System IDentification (SysID) addresses the challenge of
estimating the parameters of a linear dynamical system from finite samples of
observations and control inputs with minimal state representation. Traditional
approaches often utilize Hankel-rank minimization, which relies on convex
relaxations that can require numerous, costly singular value decompositions
(SVDs) to optimize. In this work, we propose two nonconvex reformulations to
tackle low-order SysID (i) Burer-Monterio (BM) factorization of the Hankel
matrix for efficient nuclear norm minimization, and (ii) optimizing directly
over system parameters for real, diagonalizable systems with an atomic norm
style decomposition. These reformulations circumvent the need for repeated
heavy SVD computations, significantly improving computational efficiency.
Moreover, we prove that optimizing directly over the system parameters yields
lower statistical error rates, and lower sample complexities that do not scale
linearly with trajectory length like in Hankel-nuclear norm minimization.
Additionally, while our proposed formulations are nonconvex, we provide
theoretical guarantees of achieving global optimality in polynomial time.
Finally, we demonstrate algorithms that solve these nonconvex programs and
validate our theoretical claims on synthetic data.

</details>

### [492] [Negative Imaginary Neural ODEs: Learning to Control Mechanical Systems with Stability Guarantees](https://arxiv.org/abs/2504.19497)
*Kanghong Shi,Ruigang Wang,Ian R. Manchester*

Main category: eess.SY

TLDR: 提出了一种基于负虚神经常微分方程（NINODE）控制器的神经控制方法，用于机械系统的稳定控制。


<details>
  <summary>Details</summary>
Motivation: 为机械系统提供具有稳定性保证的控制方法，利用神经网络的灵活性设计控制器。

Method: 在哈密顿框架中使用具有特定性质的神经网络作为状态空间函数矩阵，确保系统具有负虚（NI）性质。

Result: NINODE控制器在满足条件下可渐近稳定负虚（NI）系统，并通过非线性质量-弹簧系统验证了其有效性和稳定性。

Conclusion: NINODE控制器为机械系统提供了稳定且有效的控制方案，其设计条件可转化为神经网络的约束。

Abstract: We propose a neural control method to provide guaranteed stabilization for
mechanical systems using a novel negative imaginary neural ordinary
differential equation (NINODE) controller. Specifically, we employ neural
networks with desired properties as state-space function matrices within a
Hamiltonian framework to ensure the system possesses the NI property. This
NINODE system can serve as a controller that asymptotically stabilizes an NI
plant under certain conditions. For mechanical plants with colocated force
actuators and position sensors, we demonstrate that all the conditions required
for stability can be translated into regularity constraints on the neural
networks used in the controller. We illustrate the utility, effectiveness, and
stability guarantees of the NINODE controller through an example involving a
nonlinear mass-spring system.

</details>

### [493] [Model-based controller assisted domain randomization in deep reinforcement learning: application to nonlinear powertrain control](https://arxiv.org/abs/2504.19715)
*Heisei Yonezawa,Ansei Yonezawa,Itsuro Kajiwara*

Main category: eess.SY

TLDR: 论文提出了一种基于深度强化学习（DRL）的新型鲁棒控制方法，结合领域随机化、LSTM网络和模型控制，用于处理复杂机械系统中的非线性和不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒控制在处理某些非线性和不确定性时存在局限性，需要一种更实用的方法。

Method: 采用领域随机化DRL、LSTM网络和模型控制（MBC）的协同策略，通过潜在马尔可夫决策过程（LMDP）建模问题。

Result: 提出的控制器在复杂动力系统应用中表现出高鲁棒性，且具有更好的泛化能力和更小的神经网络架构。

Conclusion: 该方法在复杂非线性系统中具有显著优势，验证了其实际应用潜力。

Abstract: Complex mechanical systems such as vehicle powertrains are inherently subject
to multiple nonlinearities and uncertainties arising from parametric
variations. Modeling and calibration errors are therefore unavoidable, making
the transfer of control systems from simulation to real-world systems a
critical challenge. Traditional robust controls have limitations in handling
certain types of nonlinearities and uncertainties, requiring a more practical
approach capable of comprehensively compensating for these various constraints.
This study proposes a new robust control approach using the framework of deep
reinforcement learning (DRL). The key strategy lies in the synergy among domain
randomization-based DRL, long short-term memory (LSTM)-based actor and critic
networks, and model-based control (MBC). The problem setup is modeled via the
latent Markov decision process (LMDP), a set of vanilla MDPs, for a controlled
system subject to uncertainties and nonlinearities. In LMDP, the dynamics of an
environment simulator is randomized during training to improve the robustness
of the control system to real testing environments. The randomization increases
training difficulties as well as conservativeness of the resultant control
system; therefore, progress is assisted by concurrent use of a model-based
controller based on a nominal system model. Compared to traditional DRL-based
controls, the proposed controller design is smarter in that we can achieve a
high level of generalization ability with a more compact neural network
architecture and a smaller amount of training data. The proposed approach is
verified via practical application to active damping for a complex powertrain
system with nonlinearities and parametric variations. Comparative tests
demonstrate the high robustness of the proposed approach.

</details>

### [494] [Capturing Aerodynamic Characteristics of ATTAS Aircraft with Evolving Intelligent System](https://arxiv.org/abs/2504.19949)
*Aydoğan Soylu,Tufan Kumbasar*

Main category: eess.SY

TLDR: 论文提出了一种新型的eT2QFNN模型，用于ATTAS飞机的气动系数建模，通过增量学习和量子隶属函数提高了鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 精确建模气动系数对优化现代飞机性能至关重要，传统方法难以处理非线性和不确定性。

Method: 采用eT2QFNN，通过增量学习生成多个线性子模型，利用量子隶属函数增强抗噪声能力，并自动调整规则和参数。

Result: eT2QFNN在大量数据和有限数据下均优于基线模型，且规则更少。Delta方法验证了其在稳定性分析中的优越性。

Conclusion: eT2QFNN在气动系数建模中表现优异，具有更高的鲁棒性和准确性。

Abstract: Accurate modeling of aerodynamic coefficients is crucial for understanding
and optimizing the performance of modern aircraft systems. This paper presents
the novel deployment of an Evolving Type-2 Quantum Fuzzy Neural Network
(eT2QFNN) for modeling the aerodynamic coefficients of the ATTAS aircraft to
express the aerodynamic characteristics. eT2QFNN can represent the nonlinear
aircraft model by creating multiple linear submodels with its rule-based
structure through an incremental learning strategy rather than a traditional
batch learning approach. Moreover, it enhances robustness to uncertainties and
data noise through its quantum membership functions, as well as its automatic
rule-learning and parameter-tuning capabilities. During the estimation of the
aerodynamic coefficients via the flight data of the ATTAS, two different
studies are conducted in the training phase: one with a large amount of data
and the other with a limited amount of data. The results show that the modeling
performance of the eT2QFNN is superior in comparison to baseline counterparts.
Furthermore, eT2QFNN estimated the aerodynamic model with fewer rules compared
to Type-1 fuzzy counterparts. In addition, by applying the Delta method to the
proposed approach, the stability and control derivatives of the aircraft are
analyzed. The results prove the superiority of the proposed eT2QFNN in
representing aerodynamic coefficients.

</details>

### [495] [Digital Twin-based Out-of-Distribution Detection in Autonomous Vessels](https://arxiv.org/abs/2504.19816)
*Erblin Isaku,Hassan Sartaj,Shaukat Ali*

Main category: eess.SY

TLDR: 本文提出了一种基于数字孪生的方法（ODDIT），用于实时预测自主船舶（AV）的未来异常状态，以实现主动干预。


<details>
  <summary>Details</summary>
Motivation: 现有文献对利用机器学习技术构建数字孪生进行实时数据分析的研究较少，尤其是在自主船舶领域。

Method: ODDIT通过两个机器学习模型预测未来船舶状态并判断是否为异常状态。

Result: 在模拟条件下，ODDIT在多个船舶上实现了高精度的异常状态检测（AUROC和TNR@TPR95达99%）。

Conclusion: ODDIT为自主船舶的实时异常检测和主动干预提供了有效解决方案。

Abstract: An autonomous vessel (AV) is a complex cyber-physical system (CPS) with
software enabling many key functionalities, e.g., navigation software enables
an AV to autonomously or semi-autonomously follow a path to its destination.
Digital twins of such AVs enable advanced functionalities such as running
what-if scenarios, performing predictive maintenance, and enabling fault
diagnosis. Due to technological improvements, real-time analyses using
continuous data from vessels' real-time operations have become increasingly
possible. However, the literature has little explored developing advanced
analyses in real-time data in AVs with digital twins built with machine
learning techniques. To this end, we present a novel digital twin-based
approach (ODDIT) to detect future out-of-distribution (OOD) states of an AV
before reaching them, enabling proactive intervention. Such states may indicate
anomalies requiring attention (e.g., manual correction by the ship master) and
assist testers in scenario-centered testing. The digital twin consists of two
machine-learning models predicting future vessel states and whether the
predicted state will be OOD. We evaluated ODDIT with five vessels across
waypoint and zigzag maneuvering under simulated conditions, including sensor
and actuator noise and environmental disturbances i.e., ocean current. ODDIT
achieved high accuracy in detecting OOD states, with AUROC and TNR@TPR95 scores
reaching 99\% across multiple vessels.

</details>

<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [496] [Detecting speculative data flow vulnerabilities using weakest precondition reasoning](https://arxiv.org/abs/2504.19128)
*Graeme Smith*

Main category: cs.LO

TLDR: 论文提出了一种基于最弱前置条件推理的方法，用于检测数据流漏洞Spectre-STL和Spectre-PSF。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注基于预测控制流的漏洞检测，而基于预测数据流的漏洞研究较少。

Method: 采用最弱前置条件推理方法。

Result: 在文献中的测试套件上验证了方法的有效性。

Conclusion: 该方法填补了数据流漏洞检测的空白，为相关研究提供了新思路。

Abstract: Speculative execution is a hardware optimisation technique where a processor,
while waiting on the completion of a computation required for an instruction,
continues to execute later instructions based on a predicted value of the
pending computation. It came to the forefront of security research in 2018 with
the disclosure of two related attacks, Spectre and Meltdown. Since then many
similar attacks have been identified. While there has been much research on
using formal methods to detect speculative execution vulnerabilities based on
predicted control flow, there has been significantly less on vulnerabilities
based on predicted data flow. In this paper, we introduce an approach for
detecting the data flow vulnerabilities, Spectre-STL and Spectre-PSF, using
weakest precondition reasoning. We validate our approach on a suite of litmus
tests used to validate related approaches in the literature.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [497] [Statistical Inference for Clustering-based Anomaly Detection](https://arxiv.org/abs/2504.18633)
*Nguyen Thi Minh Phu,Duong Tan Loc,Vo Nguyen Le Duy*

Main category: stat.ML

TLDR: SI-CLAD是一种新的统计框架，用于测试基于聚类的异常检测结果，能够严格控制误检率，并提高真检率。


<details>
  <summary>Details</summary>
Motivation: 基于聚类的异常检测方法缺乏对检测结果可靠性的保证，因此需要一种统计框架来验证其有效性。

Method: 利用选择性推断（SI）框架分析聚类AD的选择机制，提出SI-CLAD方法，控制误检率并提高真检率。

Result: 实验证明SI-CLAD能有效控制误检率，并在合成和真实数据集上表现优异。

Conclusion: SI-CLAD为基于聚类的异常检测提供了统计验证工具，显著提升了检测的可靠性。

Abstract: Unsupervised anomaly detection (AD) is a fundamental problem in machine
learning and statistics. A popular approach to unsupervised AD is
clustering-based detection. However, this method lacks the ability to guarantee
the reliability of the detected anomalies. In this paper, we propose SI-CLAD
(Statistical Inference for CLustering-based Anomaly Detection), a novel
statistical framework for testing the clustering-based AD results. The key
strength of SI-CLAD lies in its ability to rigorously control the probability
of falsely identifying anomalies, maintaining it below a pre-specified
significance level $\alpha$ (e.g., $\alpha = 0.05$). By analyzing the selection
mechanism inherent in clustering-based AD and leveraging the Selective
Inference (SI) framework, we prove that false detection control is attainable.
Moreover, we introduce a strategy to boost the true detection rate, enhancing
the overall performance of SI-CLAD. Extensive experiments on synthetic and
real-world datasets provide strong empirical support for our theoretical
findings, showcasing the superior performance of the proposed method.

</details>

### [498] [Foundations of Safe Online Reinforcement Learning in the Linear Quadratic Regulator: $\sqrt{T}$-Regret](https://arxiv.org/abs/2504.18657)
*Benjamin Schiffer,Lucas Janson*

Main category: stat.ML

TLDR: N/A


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Understanding how to efficiently learn while adhering to safety constraints
is essential for using online reinforcement learning in practical applications.
However, proving rigorous regret bounds for safety-constrained reinforcement
learning is difficult due to the complex interaction between safety,
exploration, and exploitation. In this work, we seek to establish foundations
for safety-constrained reinforcement learning by studying the canonical problem
of controlling a one-dimensional linear dynamical system with unknown dynamics.
We study the safety-constrained version of this problem, where the state must
with high probability stay within a safe region, and we provide the first safe
algorithm that achieves regret of $\tilde{O}_T(\sqrt{T})$. Furthermore, the
regret is with respect to the baseline of truncated linear controllers, a
natural baseline of non-linear controllers that are well-suited for
safety-constrained linear systems. In addition to introducing this new
baseline, we also prove several desirable continuity properties of the optimal
controller in this baseline. In showing our main result, we prove that whenever
the constraints impact the optimal controller, the non-linearity of our
controller class leads to a faster rate of learning than in the unconstrained
setting.

</details>

### [499] [Local Polynomial Lp-norm Regression](https://arxiv.org/abs/2504.18695)
*Ladan Tazik,James Stafford,John Braun*

Main category: stat.ML

TLDR: 论文提出了一种基于$L_p$-范数的局部多项式回归方法，用于处理非高斯噪声下的回归问题，并通过理论和数值实验证明其优于传统的最小二乘法。


<details>
  <summary>Details</summary>
Motivation: 传统的最小二乘估计在非高斯噪声下表现不佳，残差常呈现非正态分布特性，因此需要基于其他范数的估计方法。

Method: 使用加权$L_p$-范数估计替代加权最小二乘估计，并提出一种从残差中估计参数$p$的新方法。

Result: 在一维数据中优于局部最小二乘法，并在二维数据中展现出良好的潜力。

Conclusion: $L_p$-范数回归方法在非高斯噪声环境下具有优越性和适应性。

Abstract: The local least squares estimator for a regression curve cannot provide
optimal results when non-Gaussian noise is present. Both theoretical and
empirical evidence suggests that residuals often exhibit distributional
properties different from those of a normal distribution, making it worthwhile
to consider estimation based on other norms. It is suggested that $L_p$-norm
estimators be used to minimize the residuals when these exhibit non-normal
kurtosis. In this paper, we propose a local polynomial $L_p$-norm regression
that replaces weighted least squares estimation with weighted $L_p$-norm
estimation for fitting the polynomial locally. We also introduce a new method
for estimating the parameter $p$ from the residuals, enhancing the adaptability
of the approach. Through numerical and theoretical investigation, we
demonstrate our method's superiority over local least squares in
one-dimensional data and show promising outcomes for higher dimensions,
specifically in 2D.

</details>

### [500] [A Dictionary of Closed-Form Kernel Mean Embeddings](https://arxiv.org/abs/2504.18830)
*François-Xavier Briol,Alexandra Gessner,Toni Karvonen,Maren Mahsereci*

Main category: stat.ML

TLDR: 本文总结了核均值嵌入的闭式表达式，并提供了实用工具和Python库以扩展其应用。


<details>
  <summary>Details</summary>
Motivation: 解决核均值嵌入闭式表达式难以推导的问题，扩展核基技术的适用性。

Method: 整理已知核均值嵌入的闭式表达式，并提供推导新嵌入的工具和Python库。

Result: 提供了全面的核均值嵌入字典和实用工具，增强了核基技术的可用性。

Conclusion: 通过提供闭式表达式和工具，本文显著提升了核均值嵌入的实用性和可访问性。

Abstract: Kernel mean embeddings -- integrals of a kernel with respect to a probability
distribution -- are essential in Bayesian quadrature, but also widely used in
other computational tools for numerical integration or for statistical
inference based on the maximum mean discrepancy. These methods often require,
or are enhanced by, the availability of a closed-form expression for the kernel
mean embedding. However, deriving such expressions can be challenging, limiting
the applicability of kernel-based techniques when practitioners do not have
access to a closed-form embedding. This paper addresses this limitation by
providing a comprehensive dictionary of known kernel mean embeddings, along
with practical tools for deriving new embeddings from known ones. We also
provide a Python library that includes minimal implementations of the
embeddings.

</details>

### [501] [ReLU integral probability metric and its applications](https://arxiv.org/abs/2504.18897)
*Yuha Park,Kunwoong Kim,Insung Kong,Yongdai Kim*

Main category: stat.ML

TLDR: 提出了一种参数化积分概率度量（IPM），利用特定参数化判别器（如ReLU激活的单节点神经网络）高效衡量概率分布差异，适用于高维场景。


<details>
  <summary>Details</summary>
Motivation: 传统IPM使用非参数化判别器，计算复杂且收敛性差。本文旨在提出一种参数化IPM，简化实现并提升性能。

Method: 通过优化参数化判别器（如单节点神经网络）的参数，提出高效算法，减少超参数需求。

Result: 理论证明其收敛性好，可作为其他IPM的替代；实验显示在因果推断和公平表示学习中性能优越。

Conclusion: 参数化IPM兼具理论保证和实际高效性，适用于多种任务。

Abstract: We propose a parametric integral probability metric (IPM) to measure the
discrepancy between two probability measures. The proposed IPM leverages a
specific parametric family of discriminators, such as single-node neural
networks with ReLU activation, to effectively distinguish between
distributions, making it applicable in high-dimensional settings. By optimizing
over the parameters of the chosen discriminator class, the proposed IPM
demonstrates that its estimators have good convergence rates and can serve as a
surrogate for other IPMs that use smooth nonparametric discriminator classes.
We present an efficient algorithm for practical computation, offering a simple
implementation and requiring fewer hyperparameters. Furthermore, we explore its
applications in various tasks, such as covariate balancing for causal inference
and fair representation learning. Across such diverse applications, we
demonstrate that the proposed IPM provides strong theoretical guarantees, and
empirical experiments show that it achieves comparable or even superior
performance to other methods.

</details>

### [502] [Geometry-aware Active Learning of Spatiotemporal Dynamic Systems](https://arxiv.org/abs/2504.19012)
*Xizhuo,Zhang,Bing Yao*

Main category: stat.ML

TLDR: 提出了一种几何感知的主动学习框架，用于建模时空动态系统，结合几何特征与时间相关性，并通过自适应策略优化数据收集。


<details>
  <summary>Details</summary>
Motivation: 复杂动态系统的传感信号分布在3D几何中且快速变化，传统方法难以有效建模。

Method: 提出几何感知时空高斯过程（G-ST-GP）和自适应主动学习策略，结合几何特征与时间相关性。

Result: 在3D心脏几何中建模时空电动力学，数值实验显示优于传统方法。

Conclusion: 框架通过结合几何信息和优化数据收集，显著提升了预测准确性。

Abstract: Rapid developments in advanced sensing and imaging have significantly
enhanced information visibility, opening opportunities for predictive modeling
of complex dynamic systems. However, sensing signals acquired from such complex
systems are often distributed across 3D geometries and rapidly evolving over
time, posing significant challenges in spatiotemporal predictive modeling. This
paper proposes a geometry-aware active learning framework for modeling
spatiotemporal dynamic systems. Specifically, we propose a geometry-aware
spatiotemporal Gaussian Process (G-ST-GP) to effectively integrate the temporal
correlations and geometric manifold features for reliable prediction of
high-dimensional dynamic behaviors. In addition, we develop an adaptive active
learning strategy to strategically identify informative spatial locations for
data collection and further maximize the prediction accuracy. This strategy
achieves the adaptive trade-off between the prediction uncertainty in the
G-ST-GP model and the space-filling design guided by the geodesic distance
across the 3D geometry. We implement the proposed framework to model the
spatiotemporal electrodynamics in a 3D heart geometry. Numerical experiments
show that our framework outperforms traditional methods lacking the mechanism
of geometric information incorporation or effective data collection.

</details>

### [503] [Test Set Sizing for the Ridge Regression](https://arxiv.org/abs/2504.19231)
*Alexander Dubbs*

Main category: stat.ML

TLDR: 本文推导了岭回归在高精度大样本下的理想训练/测试分割，发现分割对岭参数α的依赖较弱，主要取决于样本数m和特征数n。


<details>
  <summary>Details</summary>
Motivation: 目标是最大化模型的'完整性'，使训练模型的测量误差尽可能接近理论值。

Method: 通过数学推导计算岭回归在大数据极限下的理想分割。

Result: 分割结果与普通线性回归的前两项渐近匹配，实际差异可忽略。

Conclusion: 这是首次为大样本机器学习模型计算理想分割，岭回归与普通回归的分割差异极小。

Abstract: We derive the ideal train/test split for the ridge regression to high
accuracy in the limit that the number of training rows m becomes large. The
split must depend on the ridge tuning parameter, alpha, but we find that the
dependence is weak and can asymptotically be ignored; all parameters vanish
except for m and the number of features, n. This is the first time that such a
split is calculated mathematically for a machine learning model in the large
data limit. The goal of the calculations is to maximize "integrity," so that
the measured error in the trained model is as close as possible to what it
theoretically should be. This paper's result for the ridge regression split
matches prior art for the plain vanilla linear regression split to the first
two terms asymptotically, and it appears that practically there is no
difference.

</details>

### [504] [Contextual Online Uncertainty-Aware Preference Learning for Human Feedback](https://arxiv.org/abs/2504.19342)
*Nan Lu,Ethan X. Fang,Junwei Lu*

Main category: stat.ML

TLDR: 提出了一种基于动态上下文信息的统计框架，用于在RLHF中同时进行在线决策和统计推断，通过两阶段算法和理论分析实现最优性能。


<details>
  <summary>Details</summary>
Motivation: RLHF是AI中对齐大型模型与人类偏好的关键方法，但动态上下文下的依赖性问题尚未解决。

Method: 提出两阶段算法（ε-greedy+利用阶段），结合反集中不等式和矩阵鞅技术进行理论分析。

Result: 仿真显示方法优于现有技术，应用于大型语言模型排名数据，揭示医学解剖知识表现。

Conclusion: 框架在RLHF中高效且理论可靠，适用于动态上下文下的模型优化。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a pivotal
paradigm in artificial intelligence to align large models with human
preferences. In this paper, we propose a novel statistical framework to
simultaneously conduct the online decision-making and statistical inference on
the optimal model using human preference data based on dynamic contextual
information. Our approach introduces an efficient decision strategy that
achieves both the optimal regret bound and the asymptotic distribution of the
estimators. A key challenge in RLHF is handling the dependent online human
preference outcomes with dynamic contexts. To address this, in the
methodological aspect, we propose a two-stage algorithm starting with
$\epsilon$-greedy followed by exploitations; in the theoretical aspect, we
tailor anti-concentration inequalities and matrix martingale concentration
techniques to derive the uniform estimation rate and asymptotic normality of
the estimators using dependent samples from both stages. Extensive simulation
results demonstrate that our method outperforms state-of-the-art strategies. We
apply the proposed framework to analyze the human preference data for ranking
large language models on the Massive Multitask Language Understanding dataset,
yielding insightful results on the performance of different large language
models for medical anatomy knowledge.

</details>

### [505] [The Double Descent Behavior in Two Layer Neural Network for Binary Classification](https://arxiv.org/abs/2504.19351)
*Chathurika S Abeykoon,Aleksandr Beknazaryan,Hailin Sang*

Main category: stat.ML

TLDR: 论文研究了模型测试误差的双下降现象，通过两层ReLU神经网络模型分析其数学理论。


<details>
  <summary>Details</summary>
Motivation: 探究模型复杂度增加时测试误差先降后升再降的双下降现象。

Method: 使用两层ReLU神经网络模型，结合凸高斯最小最大定理分析全局训练损失。

Result: 量化了模型大小对测试误差的影响，揭示了双下降行为的数学机制。

Conclusion: 双下降现象与模型复杂度密切相关，为理解模型性能提供了新视角。

Abstract: Recent studies observed a surprising concept on model test error called the
double descent phenomenon, where the increasing model complexity decreases the
test error first and then the error increases and decreases again. To observe
this, we work on a two layer neural network model with a ReLU activation
function designed for binary classification under supervised learning. Our aim
is to observe and investigate the mathematical theory behind the double descent
behavior of model test error for varying model sizes. We quantify the model
size by the ratio of number of training samples to the dimension of the model.
Due to the complexity of the empirical risk minimization procedure, we use the
Convex Gaussian Min Max Theorem to find a suitable candidate for the global
training loss.

</details>

### [506] [Model uncertainty quantification using feature confidence sets for outcome excursions](https://arxiv.org/abs/2504.19464)
*Junting Ren,Armin Schwartzman*

Main category: stat.ML

TLDR: 本文提出了一种模型无关的框架，用于量化连续和二元结果中的不确定性，通过构建数据依赖的内外置信集来识别特征空间中预期或实际结果超过特定阈值的子集。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中（如医疗、金融和自动驾驶），量化预测不确定性对风险管理至关重要。传统方法（如置信区间和预测区间）仅提供对预期或实际结果的概率覆盖保证，而本文旨在更灵活地识别特征子集。

Method: 提出了一种模型无关的框架，构建数据依赖的内外置信集，以识别特征空间中预期或实际结果超过特定阈值的子集。理论保证了这些置信集包含真实特征子集的概率。

Result: 通过模拟和实际数据集（如房价预测和医疗诊断）验证了该框架的有效性，展示了其在连续和二元预测模型中的广泛适用性。

Conclusion: 该框架为不确定性量化提供了一种统一方法，适用于多种连续和二元预测模型，具有理论和实际应用价值。

Abstract: When implementing prediction models for high-stakes real-world applications
such as medicine, finance, and autonomous systems, quantifying prediction
uncertainty is critical for effective risk management. Traditional approaches
to uncertainty quantification, such as confidence and prediction intervals,
provide probability coverage guarantees for the expected outcomes
$f(\boldsymbol{x})$ or the realized outcomes $f(\boldsymbol{x})+\epsilon$.
Instead, this paper introduces a novel, model-agnostic framework for
quantifying uncertainty in continuous and binary outcomes using confidence sets
for outcome excursions, where the goal is to identify a subset of the feature
space where the expected or realized outcome exceeds a specific value. The
proposed method constructs data-dependent inner and outer confidence sets that
aim to contain the true feature subset for which the expected or realized
outcomes of these features exceed a specified threshold. We establish
theoretical guarantees for the probability that these confidence sets contain
the true feature subset, both asymptotically and for finite sample sizes. The
framework is validated through simulations and applied to real-world datasets,
demonstrating its utility in contexts such as housing price prediction and time
to sepsis diagnosis in healthcare. This approach provides a unified method for
uncertainty quantification that is broadly applicable across various continuous
and binary prediction models.

</details>

### [507] [Optimal Sequential Recommendations: Exploiting User and Item Structure](https://arxiv.org/abs/2504.19476)
*Mina Karzand,Guy Bresler*

Main category: stat.ML

TLDR: 论文提出了一种在线推荐系统模型，结合用户和物品结构，证明了其近优性，并指出仅使用单一结构的次优性。


<details>
  <summary>Details</summary>
Motivation: 研究在线推荐系统中用户和物品的潜在结构，以改进传统协同过滤算法的性能。

Method: 提出一种同时利用用户和物品结构的算法，并通过信息论下界证明其近优性。

Result: 算法在理论和实验中均表现出优于仅使用单一结构的方法。

Conclusion: 结合用户和物品结构的推荐算法更优，传统单一结构方法存在局限性。

Abstract: We consider an online model for recommendation systems, with each user being
recommended an item at each time-step and providing 'like' or 'dislike'
feedback. A latent variable model specifies the user preferences: both users
and items are clustered into types. The model captures structure in both the
item and user spaces, as used by item-item and user-user collaborative
filtering algorithms. We study the situation in which the type preference
matrix has i.i.d. entries. Our main contribution is an algorithm that
simultaneously uses both item and user structures, proved to be near-optimal
via corresponding information-theoretic lower bounds. In particular, our
analysis highlights the sub-optimality of using only one of item or user
structure (as is done in most collaborative filtering algorithms).

</details>

<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [508] [Transformer-Empowered Actor-Critic Reinforcement Learning for Sequence-Aware Service Function Chain Partitioning](https://arxiv.org/abs/2504.18902)
*Cyril Shih-Huan Hsu,Anestis Dalgkitsis,Chrysa Papagianni,Paola Grosso*

Main category: cs.NI

TLDR: 提出了一种基于Transformer的actor-critic框架，用于6G网络中服务功能链（SFC）的序列感知分区，解决了传统方法在可扩展性和依赖关系建模上的不足。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要高效管理虚拟化网络功能（VNFs），但现有方法在可扩展性和依赖关系处理上表现不佳。

Method: 采用Transformer的自注意力机制和actor-critic框架，结合ε-LoPe探索策略和渐进回报归一化技术。

Result: 在长期接受率、资源利用效率和可扩展性上优于现有方法，并实现快速推理。

Conclusion: 该方法为6G网络中的SFC分区提供了可扩展且鲁棒的解决方案，同时将大型语言模型（LLMs）与网络优化相结合。

Abstract: In the forthcoming era of 6G networks, characterized by unprecedented data
rates, ultra-low latency, and extensive connectivity, effective management of
Virtualized Network Functions (VNFs) is essential. VNFs are software-based
counterparts of traditional hardware devices that facilitate flexible and
scalable service provisioning. Service Function Chains (SFCs), structured as
ordered sequences of VNFs, are pivotal in orchestrating complex network
services. Nevertheless, partitioning SFCs across multi-domain network
infrastructures presents substantial challenges due to stringent latency
constraints and limited resource availability. Conventional optimization-based
methods typically exhibit low scalability, whereas existing data-driven
approaches often fail to adequately balance computational efficiency with the
capability to effectively account for dependencies inherent in SFCs. To
overcome these limitations, we introduce a Transformer-empowered actor-critic
framework specifically designed for sequence-aware SFC partitioning. By
utilizing the self-attention mechanism, our approach effectively models complex
inter-dependencies among VNFs, facilitating coordinated and parallelized
decision-making processes. Additionally, we enhance training stability and
convergence using $\epsilon$-LoPe exploration strategy as well as Asymptotic
Return Normalization. Comprehensive simulation results demonstrate that the
proposed methodology outperforms existing state-of-the-art solutions in terms
of long-term acceptance rates, resource utilization efficiency, and
scalability, while achieving rapid inference. This study not only advances
intelligent network orchestration by delivering a scalable and robust solution
for SFC partitioning within emerging 6G environments, but also bridging recent
advancements in Large Language Models (LLMs) with the optimization of
next-generation networks.

</details>