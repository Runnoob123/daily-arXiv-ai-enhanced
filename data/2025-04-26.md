<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 70]
- [cs.CL](#cs.CL) [Total: 39]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.CV](#cs.CV) [Total: 66]
- [eess.SY](#eess.SY) [Total: 3]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 7]
- [cs.CY](#cs.CY) [Total: 6]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.NE](#cs.NE) [Total: 3]
- [stat.ML](#stat.ML) [Total: 4]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.RO](#cs.RO) [Total: 4]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.CE](#cs.CE) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 3]
- [cs.AR](#cs.AR) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Integrating Graph Theoretical Approaches in Cybersecurity Education CSCI-RTED](https://arxiv.org/abs/2504.17059)
*Goksel Kucukkaya,Murat Ozer,Kazim Ciris*

Main category: cs.CR

TLDR: 本文提出了一种基于图论的增强版NSL-KDD数据集，用于网络安全分析，并通过IBM Auto AI验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着网络安全威胁的演变，需要更先进的工具来分析复杂的网络环境，图论为此提供了强大的建模框架。

Method: 开发了基于图论的增强版NSL-KDD数据集，并利用IBM Auto AI进行验证。

Result: 增强后的数据集为网络安全分析提供了实用工具，能够有效支持分类和威胁预测。

Conclusion: 该研究为未来的网络安全专业人员提供了应对复杂挑战的实用工具。

Abstract: As cybersecurity threats continue to evolve, the need for advanced tools to
analyze and understand complex cyber environments has become increasingly
critical. Graph theory offers a powerful framework for modeling relationships
within cyber ecosystems, making it highly applicable to cybersecurity. This
paper focuses on the development of an enriched version of the widely
recognized NSL-KDD dataset, incorporating graph-theoretical concepts to enhance
its practical value. The enriched dataset provides a resource for students and
professionals to engage in hands-on analysis, enabling them to explore
graph-based methodologies for identifying network behavior and vulnerabilities.
To validate the effectiveness of this dataset, we employed IBM Auto AI,
demonstrating its capability in real-world applications such as classification
and threat prediction. By addressing the need for graph-theoretical datasets,
this study provides a practical tool for equipping future cybersecurity
professionals with the skills necessary to confront complex cyber challenges.

</details>

### [2] [Evaluating Argon2 Adoption and Effectiveness in Real-World Software](https://arxiv.org/abs/2504.17121)
*Pascal Tippe,Michael P. Berner*

Main category: cs.CR

TLDR: 论文分析了Argon2密码哈希算法的实际应用，发现其参数配置对安全性影响显著，但弱密码问题仍无法解决，且开发者配置习惯不佳。


<details>
  <summary>Details</summary>
Motivation: 探讨Argon2从理论安全到实际部署的挑战，揭示参数配置对安全性的影响及开发者实践中的不足。

Method: 结合攻击模拟（量化参数配置对破解成本的影响）和大规模实证研究（分析GitHub上Argon2的配置情况）。

Result: OWASP推荐的46 MiB配置显著提升安全性，但内存硬度的边际效益递减；弱密码问题普遍存在；开发者配置习惯不佳。

Conclusion: 仅靠安全算法不足以保证安全，需加强参数指导和开发者教育。

Abstract: Modern password hashing remains a critical defense against credential
cracking, yet the transition from theoretically secure algorithms to robust
real-world implementations remains fraught with challenges. This paper presents
a dual analysis of Argon2, the Password Hashing Competition winner, combining
attack simulations quantifying how parameter configurations impact guessing
costs under realistic budgets, with the first large-scale empirical study of
Argon2 adoption across public GitHub software repositories. Our economic model,
validated against cryptocurrency mining benchmarks, demonstrates that OWASP's
recommended 46 MiB configuration reduces compromise rates by 42.5% compared to
SHA-256 at \$1/account attack budgets for strong user passwords. However,
memory-hardness exhibits diminishing returns as increasing allocations to RFC
9106's 2048 MiB provides just 23.3% (\$1) and 17.7% (\$20) additional
protection despite 44.5 times greater memory demands. Crucially, both
configurations fail to mitigate risks from weak passwords, with 96.9-99.8%
compromise rates for RockYou-like credentials regardless of algorithm choice.
Our repository analysis shows accelerating Argon2 adoption, yet weak
configuration practices: 46.6% of deployments use weaker-than-OWASP parameters.
Surprisingly, sensitive applications (password managers, encryption tools) show
no stronger configurations than general software. Our findings highlight that a
secure algorithm alone cannot ensure security, effective parameter guidance and
developer education remain essential for realizing Argon2's theoretical
advantages.

</details>

### [3] [P$_\ell$-Kyber: Packing $\ell$ Plaintexts and Lattice Coding for Kyber](https://arxiv.org/abs/2504.17185)
*Shuiyin Liu,Amin Sakzad*

Main category: cs.CR

TLDR: 论文提出了一种结合编码和加密的KEM设计，通过密文打包和格打包技术，显著降低了解密失败率和通信成本。


<details>
  <summary>Details</summary>
Motivation: 传统KEM设计假设解码噪声独立，但实际中噪声可能相关。本文旨在消除这一假设，提升安全性和效率。

Method: 1. 扩展PVW方法到Kyber，实现多层明文打包（Pₗ-Kyber）；2. 提出跨层格编码方案，确保解码噪声独立。

Result: Pₗ-Kyber在M-LWE假设下IND-CCA安全，且噪声独立。使用Leech格编码时，CER降低90%，DFR极低（<2⁻²⁸¹）。

Conclusion: 结合密文和格打包的编码加密设计，显著提升了KEM的性能和安全性，无需噪声独立假设。

Abstract: In this work, we propose a joint design of encoding and encryption processes
for KEMs like Kyber, without assuming the independence of the decoding noise
entries. Our design features two techniques: ciphertext packing and lattice
packing. First, we extend the Peikert-Vaikuntanathan-Waters (PVW) method to the
Kyber: $\ell$ plaintexts are packed into a single ciphertext. This scheme is
referred to as P$_\ell$-Kyber. We prove that the P$_\ell$-Kyber is IND-CCA
secure under the M-LWE hardness assumption. We show that the decryption
decoding noise entries across the $\ell$ plaintexts (also known as layers) are
mutually independent. Second, we propose a cross-layer lattice encoding scheme
for the P$_\ell$-Kyber, where every $\ell$ cross-layer information symbols are
encoded to a lattice point. This way we obtain a \emph{coded} P$_\ell$-Kyber,
where the decoding noise entries for each lattice point are mutually
independent. Therefore, the decryption failure rate (DFR) analysis does not
require the assumption of independence among the decryption decoding noise
entries. Both DFR and communication cost (CER) are greatly decreased thanks to
ciphertext packing and lattice packing. Finally, we demonstrate that with
$\ell=24$ and Leech lattice encoder, the proposed coded P$_\ell$-KYBER1024
achieves DFR $<2^{-281}$ and CER $ = 4.6$, i.e., a decrease of CER by $90\%$
compared to KYBER1024.

</details>

### [4] [Developing a Blockchain-Based Secure Digital Contents Distribution System](https://arxiv.org/abs/2504.17194)
*Syed Mohiuddin Qadri,Sangwhan Cha*

Main category: cs.CR

TLDR: 本文提出了一种基于区块链的安全数字内容分发系统，结合去中心化存储网络Sia和内容分发网络Skynet，通过双层架构和智能合约增强内容保护和分发。


<details>
  <summary>Details</summary>
Motivation: 随着数字内容分发的快速扩展，传统集中式系统存在单点故障和未授权访问追踪困难等问题，亟需更安全的解决方案。

Method: 采用双层架构：链下用于用户认证，链上通过智能合约和非对称加密进行交易验证，并引入许可证发放和秘密区块机制。

Result: 实验结果表明，该系统在安全分发多媒体文件方面具有可行性和可扩展性。

Conclusion: 该系统不仅提升了内容安全性，还为未来去中心化应用和集成版税支付机制的发展奠定了基础。

Abstract: As digital content distribution expands rapidly through online platforms,
securing digital media and protecting intellectual property has become
increasingly complex. Traditional centralized systems, while widely adopted,
suffer from vulnerabilities such as single points of failure and limited
traceability of unauthorized access. This paper presents a blockchain-based
secure digital content distribution system that integrates Sia, a decentralized
storage network, and Skynet, a content delivery network, to enhance content
protection and distribution. The proposed system employs a dual-layer
architecture: off-chain for user authentication and on-chain for transaction
validation using smart contracts and asymmetric encryption. By introducing a
license issuance and secret block mechanism, the system ensures content
authenticity, privacy, and controlled access. Experimental results demonstrate
the feasibility and scalability of the system in securely distributing
multimedia files. The proposed platform not only improves content security but
also paves the way for future enhancements with decentralized applications and
integrated royalty payment mechanisms.

</details>

### [5] [A Comment on "e-PoS: Making PoS Decentralized and Fair"](https://arxiv.org/abs/2504.17256)
*Suhyeon Lee,Seungjoo Kim*

Main category: cs.CR

TLDR: 本文指出Saad等人的e-PoS协议对传统PoS模型的公平性问题的误解，认为其与传统PoS概念及实际加密货币不符。


<details>
  <summary>Details</summary>
Motivation: 澄清Saad等人对传统PoS模型的误解，强调其与一般PoS概念及实际应用的差异。

Method: 通过分析传统PoS模型与e-PoS的对比，指出误解所在。

Result: 发现Saad等人的e-PoS协议对传统PoS模型的公平性问题的描述不准确。

Conclusion: 传统PoS模型并未如Saad等人所述导致公平性问题，其误解需被纠正。

Abstract: Proof-of-Stake (PoS) is a prominent Sybil control mechanism for
blockchain-based systems. In "e-PoS: Making PoS Decentralized and Fair," Saad
et al. (TPDS'21) introduced a new Proof-of-Stake protocol, e-PoS, to enhance
PoS applications' decentralization and fairness. In this comment paper, we
address a misunderstanding in the work of Saad et al. The conventional
Proof-of-Stake model that causes the fairness problem does not align with the
general concept of Proof-of-Stake nor the Proof-of-Stake cryptocurrencies
mentioned in their paper.

</details>

### [6] [Contrastive Learning for Continuous Touch-Based Authentication](https://arxiv.org/abs/2504.17271)
*Mengyu Qiao,Yunpeng Zhai,Yang Wang*

Main category: cs.CR

TLDR: 提出了一种基于对比学习的统一框架，用于移动设备的连续用户认证，结合时间掩码自编码器和Siamese网络，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 移动设备处理敏感信息需求高安全性，基于触摸行为的连续认证是一种自然且无缝的解决方案。

Method: 采用时间掩码自编码器提取多传感器数据的时间模式，结合Siamese时序注意力卷积网络和对比学习框架，引入多头注意力和通道注意力机制。

Result: 在公开基准和自收集数据集上表现优于现有方法。

Conclusion: 该方法为移动设备用户认证提供了可靠且高效的解决方案。

Abstract: Smart mobile devices have become indispensable in modern daily life, where
sensitive information is frequently processed, stored, and transmitted-posing
critical demands for robust security controls. Given that touchscreens are the
primary medium for human-device interaction, continuous user authentication
based on touch behavior presents a natural and seamless security solution.
While existing methods predominantly adopt binary classification under
single-modal learning settings, we propose a unified contrastive learning
framework for continuous authentication in a non-disruptive manner.
Specifically, the proposed method leverages a Temporal Masked Autoencoder to
extract temporal patterns from raw multi-sensor data streams, capturing
continuous motion and gesture dynamics. The pre-trained TMAE is subsequently
integrated into a Siamese Temporal-Attentive Convolutional Network within a
contrastive learning paradigm to model both sequential and cross-modal
patterns. To further enhance performance, we incorporate multi-head attention
and channel attention mechanisms to capture long-range dependencies and
optimize inter-channel feature integration. Extensive experiments on public
benchmarks and a self-collected dataset demonstrate that our approach
outperforms state-of-the-art methods, offering a reliable and effective
solution for user authentication on mobile devices.

</details>

### [7] [Proof of Useful Intelligence (PoUI): Blockchain Consensus Beyond Energy Waste](https://arxiv.org/abs/2504.17539)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.CR

TLDR: 提出了一种名为PoUI的混合共识机制，结合了AI任务执行与区块链安全性。


<details>
  <summary>Details</summary>
Motivation: 解决传统共识机制（如PoW和PoS）在资源消耗或中心化风险上的不足。

Method: 通过智能合约协调节点（任务发布者、市场协调者、工作者和验证者），工作者执行AI任务赚取代币并用于网络安全性。

Result: PoUI机制在保障安全性的同时提供了实际应用价值。

Conclusion: PoUI是一种兼顾安全性与实用性的新型共识机制。

Abstract: Blockchain technology enables secure, transparent data management in
decentralized systems, supporting applications from cryptocurrencies like
Bitcoin to tokenizing real-world assets like property. Its scalability and
sustainability hinge on consensus mechanisms balancing security and efficiency.
Proof of Work (PoW), used by Bitcoin, ensures security through energy-intensive
computations but demands significant resources. Proof of Stake (PoS), as in
Ethereum post-Merge, selects validators based on staked cryptocurrency,
offering energy efficiency but risking centralization from wealth
concentration. With AI models straining computational resources, we propose
Proof of Useful Intelligence (PoUI), a hybrid consensus mechanism. In PoUI,
workers perform AI tasks like language processing or image analysis to earn
coins, which are staked to secure the network, blending security with practical
utility. Decentralized nodes--job posters, market coordinators, workers, and
validators --collaborate via smart contracts to manage tasks and rewards.

</details>

### [8] [Evaluating the Vulnerability of ML-Based Ethereum Phishing Detectors to Single-Feature Adversarial Perturbations](https://arxiv.org/abs/2504.17684)
*Ahod Alghuried,Ali Alkinoon,Abdulaziz Alghamdi,Soohyeon Choi,Manar Mohaisen,David Mohaisen*

Main category: cs.CR

TLDR: 本文研究了机器学习模型在以太坊欺诈交易检测中对简单单特征对抗攻击的脆弱性，并通过实验验证了不同攻击策略对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习模型在以太坊欺诈交易检测中的脆弱性，揭示其对简单对抗攻击的易感性。

Method: 通过全面实验，研究不同对抗攻击策略对模型性能的影响，并测试了对抗训练和增强特征选择等缓解策略的有效性。

Result: 研究发现模型对简单攻击非常敏感，且不同算法对攻击的反应不一致，但缓解策略（如对抗训练和特征选择）能显著提升模型鲁棒性。

Conclusion: 研究揭示了模型在对抗攻击下的脆弱性，同时证明了缓解策略的有效性，为未来研究提供了改进方向。

Abstract: This paper explores the vulnerability of machine learning models to simple
single-feature adversarial attacks in the context of Ethereum fraudulent
transaction detection. Through comprehensive experimentation, we investigate
the impact of various adversarial attack strategies on model performance
metrics. Our findings, highlighting how prone those techniques are to simple
attacks, are alarming, and the inconsistency in the attacks' effect on
different algorithms promises ways for attack mitigation. We examine the
effectiveness of different mitigation strategies, including adversarial
training and enhanced feature selection, in enhancing model robustness and show
their effectiveness.

</details>

### [9] [User Profiles: The Achilles' Heel of Web Browsers](https://arxiv.org/abs/2504.17692)
*Dolière Francis Somé,Moaz Airan,Zakir Durumeric,Cristian-Alexandru Staicu*

Main category: cs.CR

TLDR: 论文首次系统研究了浏览器配置文件的安全性，发现除Tor浏览器外，现代浏览器将敏感数据存储在缺乏保护的目录中，易受攻击。


<details>
  <summary>Details</summary>
Motivation: 研究浏览器与操作系统及文件系统的交互安全性，填补现有研究空白。

Method: 通过分析浏览器配置文件的存储方式，展示攻击者如何绕过安全措施（如密码加密）并利用文件系统API发起攻击。

Result: 发现浏览器配置文件易受攻击，攻击者可安装恶意扩展、劫持HTTPS流量等。

Conclusion: 呼吁开发更安全的机制保护用户浏览器数据。

Abstract: Web browsers provide the security foundation for our online experiences.
Significant research has been done into the security of browsers themselves,
but relatively little investigation has been done into how they interact with
the operating system or the file system. In this work, we provide the first
systematic security study of browser profiles, the on-disk persistence layer of
browsers, used for storing everything from users' authentication cookies and
browser extensions to certificate trust decisions and device permissions. We
show that, except for the Tor Browser, all modern browsers store sensitive data
in home directories with little to no integrity or confidentiality controls. We
show that security measures like password and cookie encryption can be easily
bypassed. In addition, HTTPS can be sidestepped entirely by deploying malicious
root certificates within users' browser profiles. The Public Key Infrastructure
(PKI), the backbone of the secure Web. HTTPS can be fully bypassed with the
deployment of custom potentially malicious root certificates. More worryingly,
we show how these powerful attacks can be fully mounted directly from web
browsers themselves, through the File System Access API, a recent feature added
by Chromium browsers that enables a website to directly manipulate a user's
file system via JavaScript. In a series of case studies, we demonstrate how an
attacker can install malicious browser extensions, inject additional root
certificates, hijack HTTPS traffic, and enable websites to access hardware
devices like the camera and GPS. Based on our findings, we argue that
researchers and browser vendors need to develop and deploy more secure
mechanisms for protecting users' browser data against file system attackers.

</details>

### [10] [Identity Control Plane: The Unifying Layer for Zero Trust Infrastructure](https://arxiv.org/abs/2504.17759)
*Surya Teja Avirneni*

Main category: cs.CR

TLDR: 本文介绍了身份控制平面（ICP），一个用于在人类用户、工作负载和自动化系统中实施身份感知的零信任访问的架构框架。


<details>
  <summary>Details</summary>
Motivation: 当前模型在统一不同身份类型（如工作负载身份、用户身份和自动化凭证）方面存在不足，ICP旨在解决这一问题。

Method: ICP通过结合SPIFFE、OIDC/SAML和代理颁发的交易令牌，提出了一种可组合的强制执行层，使用ABAC策略引擎。

Result: 论文提出了ICP的架构组件、集成模式、用例、与现有模型的比较分析，以及理论性能指标。

Conclusion: ICP为安全研究人员和平台架构师提供了一个理论框架，支持FedRAMP和SLSA合规性。

Abstract: This paper introduces the Identity Control Plane (ICP), an architectural
framework for enforcing identity-aware Zero Trust access across human users,
workloads, and automation systems. The ICP model unifies SPIFFE-based workload
identity, OIDC/SAML user identity, and scoped automation credentials via
broker-issued transaction tokens. We propose a composable enforcement layer
using ABAC policy engines (e.g., OPA, Cedar), aligned with IETF WIMSE drafts
and OAuth transaction tokens. The paper includes architectural components,
integration patterns, use cases, a comparative analysis with current models,
and theorized performance metrics. A FedRAMP and SLSA compliance mapping is
also presented. This is a theoretical infrastructure architecture paper
intended for security researchers and platform architects. No prior version of
this work has been published.

</details>

### [11] [Silenzio: Secure Non-Interactive Outsourced MLP Training](https://arxiv.org/abs/2504.17785)
*Jonas Sander,Thomas Eisenbarth*

Main category: cs.CR

TLDR: Silenzio是一种基于全同态加密（FHE）的非交互式外包MLP训练方案，支持128位安全性，无需多方计算（MPC）的交互或非共谋假设。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限客户在云上外包ML训练时的隐私风险，尤其是对高度敏感的训练数据。

Method: 采用低比特宽度整数（不超过8位）和新型矩阵乘法技术，结合输入依赖的余数系统和Karatsuba乘法，避免FHE处理中的溢出。

Result: 在标准MLP训练任务中，Silenzio的运行时和模型性能与32位浮点计算的PyTorch相当。

Conclusion: Silenzio为隐私保护ML提供了新的基准，实现了安全且非交互的外包MLP训练。

Abstract: Outsourcing the ML training to cloud providers presents a compelling
opportunity for resource constrained clients, while it simultaneously bears
inherent privacy risks, especially for highly sensitive training data. We
introduce Silenzio, the first fully non-interactive outsourcing scheme for the
training of multi-layer perceptrons that achieves 128 bit security using FHE.
Unlike traditional MPC based protocols that necessitate interactive
communication between the client and server(s) or non-collusion assumptions
among multiple servers, Silenzio enables the fire-and-forget paradigm without
such assumptions. In this approach, the client encrypts the training data once,
and the cloud server performs the training without any further interaction.
  Silenzio operates over low bitwidth integers - never exceeding 8 bit - to
mitigate the computational overhead of FHE. Our approach features a novel
low-bitwidth matrix multiplication that leverages input-dependent residue
number systems and a Karatsuba-inspired multiplication routine, ensuring that
no intermediate FHE-processed value overflows 8 bit. Starting from an
RNS-to-MRNS conversion process, we propose an efficient block-scaling
mechanism, which approximately shifts encrypted tensor values to the
user-specified most significant bits. To instantiate the backpropagation of the
error, Silenzio introduces a low-bitwidth and TFHE friendly gradient
computation for the cross entropy loss.
  Implemented using the state-of-the-art Concrete library, we evaluate Silenzio
on standard MLP training tasks regarding runtime as well as model performance
and achieve similar classification accuracy as MLPs trained using standard
PyTorch with 32 bit floating-point computations. Our open-source implementation
represents a significant advancement in privacy-preserving ML, providing a new
baseline for secure and non-interactive outsourced MLP training.

</details>

<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [A Novel Graph Transformer Framework for Gene Regulatory Network Inference](https://arxiv.org/abs/2504.16961)
*Binon Teji,Swarup Roy*

Main category: cs.LG

TLDR: 本文提出了一种名为GT-GRN的图变换器模型，通过整合多源网络信息和基因表达数据，显著提升了基因调控网络（GRN）的推断准确性。


<details>
  <summary>Details</summary>
Motivation: 基因调控网络推断是理解复杂生物系统的关键，但现有方法易受噪声干扰且难以反映真实的生物调控关系。因此，需要结合基因表达数据、先验网络结构和位置信息，以提高推断的准确性和鲁棒性。

Method: 采用自编码器嵌入基因表达数据，利用随机游走和BERT模型编码先验网络结构，结合位置编码，构建图变换器模型GT-GRN进行GRN推断。

Result: 实验表明，GT-GRN在GRN推断中显著优于现有方法，具有更高的准确性和鲁棒性。

Conclusion: GT-GRN通过整合多源信息和先进模型，为基因调控网络推断提供了一种高效且可靠的方法。

Abstract: The inference of gene regulatory networks (GRNs) is a foundational stride
towards deciphering the fundamentals of complex biological systems. Inferring a
possible regulatory link between two genes can be formulated as a link
prediction problem. Inference of GRNs via gene coexpression profiling data may
not always reflect true biological interactions, as its susceptibility to noise
and misrepresenting true biological regulatory relationships. Most GRN
inference methods face several challenges in the network reconstruction phase.
Therefore, it is important to encode gene expression values, leverege the prior
knowledge gained from the available inferred network structures and positional
informations of the input network nodes towards inferring a better and more
confident GRN network reconstruction. In this paper, we explore the integration
of multiple inferred networks to enhance the inference of Gene Regulatory
Networks (GRNs). Primarily, we employ autoencoder embeddings to capture gene
expression patterns directly from raw data, preserving intricate biological
signals. Then, we embed the prior knowledge from GRN structures transforming
them into a text-like representation using random walks, which are then encoded
with a masked language model, BERT, to generate global embeddings for each gene
across all networks. Additionally, we embed the positional encodings of the
input gene networks to better identify the position of each unique gene within
the graph. These embeddings are integrated into graph transformer-based model,
termed GT-GRN, for GRN inference. The GT-GRN model effectively utilizes the
topological structure of the ground truth network while incorporating the
enriched encoded information. Experimental results demonstrate that GT-GRN
significantly outperforms existing GRN inference methods, achieving superior
accuracy and highlighting the robustness of our approach.

</details>

### [13] [Backslash: Rate Constrained Optimized Training of Large Language Models](https://arxiv.org/abs/2504.16968)
*Jun Wu,Jiangtao Wen,Yuxing Han*

Main category: cs.LG

TLDR: 论文提出了一种名为Backslash的训练时压缩方法，通过率失真优化实现模型精度与复杂度的灵活权衡，显著减少参数冗余且不损失性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的参数压缩研究主要集中在训练后阶段，而训练阶段的压缩仍未被充分探索。

Method: 采用率失真优化（RDO）的Rate-Constrained Training（Backslash）方法，在训练过程中实现压缩。

Result: 实验表明，Backslash可减少60%-90%的内存占用且无精度损失，相比训练后压缩效果更显著。

Conclusion: Backslash不仅高效压缩模型，还提升了泛化能力、鲁棒性，并简化网络以加速边缘设备推理。

Abstract: The rapid advancement of large-language models (LLMs) has driven extensive
research into parameter compression after training has been completed, yet
compression during the training phase remains largely unexplored. In this work,
we introduce Rate-Constrained Training (Backslash), a novel training-time
compression approach based on rate-distortion optimization (RDO). Backslash
enables a flexible trade-off between model accuracy and complexity,
significantly reducing parameter redundancy while preserving performance.
Experiments in various architectures and tasks demonstrate that Backslash can
reduce memory usage by 60\% - 90\% without accuracy loss and provides
significant compression gain compared to compression after training. Moreover,
Backslash proves to be highly versatile: it enhances generalization with small
Lagrange multipliers, improves model robustness to pruning (maintaining
accuracy even at 80\% pruning rates), and enables network simplification for
accelerated inference on edge devices.

</details>

### [14] [STFM: A Spatio-Temporal Information Fusion Model Based on Phase Space Reconstruction for Sea Surface Temperature Prediction](https://arxiv.org/abs/2504.16970)
*Yin Wang,Chunlin Gong,Xiang Wu,Hanleran Zhang*

Main category: cs.LG

TLDR: 提出了一种基于数据驱动技术的海表温度（SST）预测框架，通过相空间重构和时空融合映射（STFM）高效捕捉SST动态，并在少量训练数据下实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 海表温度（SST）对生产规划至关重要，但现有预测方法（物理数值模拟和数据驱动机器学习）存在计算复杂、数据需求大或可解释性差等问题。

Method: 使用相空间重构构建初始延迟吸引子对，并通过时空融合映射（STFM）揭示其内在联系，高效捕捉SST动态。

Result: 该方法在比较测试中以少量训练数据实现了高预测精度。

Conclusion: 提出的数据驱动框架为SST预测提供了一种高效且准确的解决方案，克服了传统方法的局限性。

Abstract: The sea surface temperature (SST), a key environmental parameter, is crucial
to optimizing production planning, making its accurate prediction a vital
research topic. However, the inherent nonlinearity of the marine dynamic system
presents significant challenges. Current forecasting methods mainly include
physics-based numerical simulations and data-driven machine learning
approaches. The former, while describing SST evolution through differential
equations, suffers from high computational complexity and limited
applicability, whereas the latter, despite its computational benefits, requires
large datasets and faces interpretability challenges. This study presents a
prediction framework based solely on data-driven techniques. Using phase space
reconstruction, we construct initial-delay attractor pairs with a mathematical
homeomorphism and design a Spatio-Temporal Fusion Mapping (STFM) to uncover
their intrinsic connections. Unlike conventional models, our method captures
SST dynamics efficiently through phase space reconstruction and achieves high
prediction accuracy with minimal training data in comparative tests

</details>

### [15] [Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications](https://arxiv.org/abs/2504.16972)
*Hossein Ahmadi,Sajjad Emdadi Mahdimahalleh,Arman Farahat,Banafsheh Saffari*

Main category: cs.LG

TLDR: 本文综述了自编码器和视觉变换器在无监督信号分析中的应用，探讨了其架构、应用和趋势，并指出了可解释性、扩展性和领域泛化等挑战。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信、雷达、生物医学工程和物联网等领域中未标记时间序列数据的快速增长，无监督学习的进展成为研究重点。

Method: 通过自编码器和视觉变换器进行特征提取、异常检测和分类，并研究混合架构和自监督学习的优势。

Result: 展示了这些模型在多种信号类型（如心电图、雷达波形和物联网传感器数据）中的有效性。

Conclusion: 本文为开发鲁棒、自适应的信号智能模型提供了路线图，同时指出了未来研究的方向。

Abstract: The rapid growth of unlabeled time-series data in domains such as wireless
communications, radar, biomedical engineering, and the Internet of Things (IoT)
has driven advancements in unsupervised learning. This review synthesizes
recent progress in applying autoencoders and vision transformers for
unsupervised signal analysis, focusing on their architectures, applications,
and emerging trends. We explore how these models enable feature extraction,
anomaly detection, and classification across diverse signal types, including
electrocardiograms, radar waveforms, and IoT sensor data. The review highlights
the strengths of hybrid architectures and self-supervised learning, while
identifying challenges in interpretability, scalability, and domain
generalization. By bridging methodological innovations and practical
applications, this work offers a roadmap for developing robust, adaptive models
for signal intelligence.

</details>

### [16] [Safety Pretraining: Toward the Next Generation of Safe AI](https://arxiv.org/abs/2504.16980)
*Pratyush Maini,Sachin Goyal,Dylan Sam,Alex Robey,Yash Savani,Yiding Jiang,Andy Zou,Zacharcy C. Lipton,J. Zico Kolter*

Main category: cs.LG

TLDR: 论文提出了一种数据中心的预训练框架，通过安全分类器、合成安全数据集和注入安全标签等方法，显著降低了大型语言模型生成有害内容的风险。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在高风险场景中生成有害或毒性内容的问题，避免后处理对齐方法的脆弱性。

Method: 使用安全分类器过滤数据、生成合成安全数据集、创建拒绝对话和教育材料数据集、注入安全标签以及在预训练阶段进行安全评估。

Result: 安全预训练模型将攻击成功率从38.8%降至8.4%，且在标准安全基准测试中无性能下降。

Conclusion: 数据中心的预训练框架能有效从源头提升模型安全性，减少有害内容生成。

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes
settings, the risk of generating harmful or toxic content remains a central
challenge. Post-hoc alignment methods are brittle: once unsafe patterns are
learned during pretraining, they are hard to remove. We present a data-centric
pretraining framework that builds safety into the model from the start. Our
contributions include: (i) a safety classifier trained on 10,000 GPT-4 labeled
examples, used to filter 600B tokens; (ii) the largest synthetic safety dataset
to date (100B tokens) generated via recontextualization of harmful web data;
(iii) RefuseWeb and Moral Education datasets that convert harmful prompts into
refusal dialogues and web-style educational material; (iv) Harmfulness-Tag
annotations injected during pretraining to flag unsafe content and steer away
inference from harmful generations; and (v) safety evaluations measuring base
model behavior before instruction tuning. Our safety-pretrained models reduce
attack success rates from 38.8% to 8.4% with no performance degradation on
standard LLM safety benchmarks.

</details>

### [17] [(Im)possibility of Automated Hallucination Detection in Large Language Models](https://arxiv.org/abs/2504.17004)
*Amin Karbasi,Omar Montasser,John Sous,Grigoris Velegkas*

Main category: cs.LG

TLDR: 论文探讨了自动检测大型语言模型（LLM）幻觉的可行性，通过理论框架分析其与语言识别的等价性，并指出专家标注反馈对实现检测的关键作用。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索是否能够自动检测LLM生成的幻觉内容，以提升模型的可靠性。

Method: 方法包括将幻觉检测与语言识别任务等价化，并分析在不同训练数据（仅正确示例与专家标注的正负示例）下的检测可能性。

Result: 结果表明，仅使用正确示例无法实现幻觉检测，而引入专家标注的正负示例后，检测对所有可数语言集合成为可能。

Conclusion: 结论强调了专家标注反馈在训练幻觉检测器中的重要性，为基于反馈的方法（如RLHF）提供了理论支持。

Abstract: Is automated hallucination detection possible? In this work, we introduce a
theoretical framework to analyze the feasibility of automatically detecting
hallucinations produced by large language models (LLMs). Inspired by the
classical Gold-Angluin framework for language identification and its recent
adaptation to language generation by Kleinberg and Mullainathan, we investigate
whether an algorithm, trained on examples drawn from an unknown target language
$K$ (selected from a countable collection) and given access to an LLM, can
reliably determine whether the LLM's outputs are correct or constitute
hallucinations.
  First, we establish an equivalence between hallucination detection and the
classical task of language identification. We prove that any hallucination
detection method can be converted into a language identification method, and
conversely, algorithms solving language identification can be adapted for
hallucination detection. Given the inherent difficulty of language
identification, this implies that hallucination detection is fundamentally
impossible for most language collections if the detector is trained using only
correct examples from the target language.
  Second, we show that the use of expert-labeled feedback, i.e., training the
detector with both positive examples (correct statements) and negative examples
(explicitly labeled incorrect statements), dramatically changes this
conclusion. Under this enriched training regime, automated hallucination
detection becomes possible for all countable language collections.
  These results highlight the essential role of expert-labeled examples in
training hallucination detectors and provide theoretical support for
feedback-based methods, such as reinforcement learning with human feedback
(RLHF), which have proven critical for reliable LLM deployment.

</details>

### [18] [Democracy of AI Numerical Weather Models: An Example of Global Forecasting with FourCastNetv2 Made by a University Research Lab Using GPU](https://arxiv.org/abs/2504.17028)
*Iman Khadir,Shane Stevenson,Henry Li,Kyle Krick,Abram Burrows,David Hall,Stan Posey,Samuel S. P. Shen*

Main category: cs.LG

TLDR: 本文探讨了利用GPU和免费AI模型（如NVIDIA的FourCastNetv2）在大学研究小组中普及AI驱动的全球天气预报模型的可行性，并分析了其优势和挑战。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报（NWP）成本高且耗时，而AI模型（如FourCastNetv2）能显著降低时间和成本。然而，资源有限的研究小组在复现结果时面临挑战。本文旨在探索如何在资源受限的环境中利用这些技术。

Method: 通过FourCastNetv2的API生成预测，并利用NVIDIA硬件训练原始FourCastNet模型。同时研究了数据管理、训练效率和模型验证。

Result: 展示了NVIDIA A100在资源受限环境中的能力和局限性，并提供了相关GitHub材料作为研究指南。

Conclusion: 本文为大学研究小组和课程提供了AI天气预报的研究和教育资源，有助于在数字经济中普及AI驱动的NWP。

Abstract: This paper demonstrates the feasibility of democratizing AI-driven global
weather forecasting models among university research groups by leveraging
Graphics Processing Units (GPUs) and freely available AI models, such as
NVIDIA's FourCastNetv2. FourCastNetv2 is an NVIDIA's advanced neural network
for weather prediction and is trained on a 73-channel subset of the European
Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset
at single levels and different pressure levels. Although the training
specifications for FourCastNetv2 are not released to the public, the training
documentation of the model's first generation, FourCastNet, is available to all
users. The training had 64 A100 GPUs and took 16 hours to complete. Although
NVIDIA's models offer significant reductions in both time and cost compared to
traditional Numerical Weather Prediction (NWP), reproducing published
forecasting results presents ongoing challenges for resource-constrained
university research groups with limited GPU availability. We demonstrate both
(i) leveraging FourCastNetv2 to create predictions through the designated
application programming interface (API) and (ii) utilizing NVIDIA hardware to
train the original FourCastNet model. Further, this paper demonstrates the
capabilities and limitations of NVIDIA A100's for resource-limited research
groups in universities. We also explore data management, training efficiency,
and model validation, highlighting the advantages and challenges of using
limited high-performance computing resources. Consequently, this paper and its
corresponding GitHub materials may serve as an initial guide for other
university research groups and courses related to machine learning, climate
science, and data science to develop research and education programs on AI
weather forecasting, and hence help democratize the AI NWP in the digital
economy.

</details>

### [19] [Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation](https://arxiv.org/abs/2504.17058)
*Rahul Vishwakarma*

Main category: cs.LG

TLDR: 提出了一种结合共形预测与GAN的新框架（cGAN），为生成数据提供统计保证。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型缺乏对数据分布的严格统计保证，限制了其在关键领域的应用。

Method: 将多种共形预测方法（如ICP、Mondrian等）整合到GAN中，实现分布无关的不确定性量化。

Result: cGAN在保持生成能力的同时，增强了校准性，生成数据具有可证明的统计保证。

Conclusion: cGAN为高风险领域（如医疗、金融）提供了可靠的合成数据生成方法。

Abstract: The generation of high-quality synthetic data presents significant challenges
in machine learning research, particularly regarding statistical fidelity and
uncertainty quantification. Existing generative models produce compelling
synthetic samples but lack rigorous statistical guarantees about their relation
to the underlying data distribution, limiting their applicability in critical
domains requiring robust error bounds. We address this fundamental limitation
by presenting a novel framework that incorporates conformal prediction
methodologies into Generative Adversarial Networks (GANs). By integrating
multiple conformal prediction paradigms including Inductive Conformal
Prediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction,
and Venn-Abers Predictors, we establish distribution-free uncertainty
quantification in generated samples. This approach, termed Conformalized GAN
(cGAN), demonstrates enhanced calibration properties while maintaining the
generative power of traditional GANs, producing synthetic data with provable
statistical guarantees. We provide rigorous mathematical proofs establishing
finite-sample validity guarantees and asymptotic efficiency properties,
enabling the reliable application of synthetic data in high-stakes domains
including healthcare, finance, and autonomous systems.

</details>

### [20] [Antenna Near-Field Reconstruction from Far-Field Data Using Convolutional Neural Networks](https://arxiv.org/abs/2504.17065)
*Sahar Bagherkhani,Jackson Christopher Earls,Franco De Flaviis,Pierre Baldi*

Main category: cs.LG

TLDR: 本文提出了一种基于深度学习的远场到近场（FF-NF）转换方法，使用卷积神经网络（CNN）从天线远场数据重建近场分布，避免了显式解析变换。


<details>
  <summary>Details</summary>
Motivation: 电磁场重建在多个应用中至关重要，如天线诊断、电磁干扰分析和系统建模。传统方法依赖解析变换，而深度学习提供了一种替代方案。

Method: 使用CNN训练配对的远场和近场数据，以均方误差（MSE）作为评估指标。

Result: 最佳模型的训练误差为0.0199，测试误差为0.3898。视觉对比显示模型能有效捕捉复杂电磁场行为。

Conclusion: 深度学习在电磁场重建中具有潜力，能够高效且准确地完成远场到近场的转换。

Abstract: Electromagnetic field reconstruction is crucial in many applications,
including antenna diagnostics, electromagnetic interference analysis, and
system modeling. This paper presents a deep learning-based approach for
Far-Field to Near-Field (FF-NF) transformation using Convolutional Neural
Networks (CNNs). The goal is to reconstruct near-field distributions from the
far-field data of an antenna without relying on explicit analytical
transformations. The CNNs are trained on paired far-field and near-field data
and evaluated using mean squared error (MSE). The best model achieves a
training error of 0.0199 and a test error of 0.3898. Moreover, visual
comparisons between the predicted and true near-field distributions demonstrate
the model's effectiveness in capturing complex electromagnetic field behavior,
highlighting the potential of deep learning in electromagnetic field
reconstruction.

</details>

### [21] [Whence Is A Model Fair? Fixing Fairness Bugs via Propensity Score Matching](https://arxiv.org/abs/2504.17066)
*Kewen Peng,Yicheng Yang,Hao Zhuo,Tim Menzies*

Main category: cs.LG

TLDR: 论文提出FairMatch方法，通过倾向得分匹配评估和缓解数据采样对公平性指标的影响，提升公平性评估和缓解效果。


<details>
  <summary>Details</summary>
Motivation: 现有公平性学习模型在多种公平性指标下仍存在不公平问题，可能源于训练和测试数据的随机采样方式。

Method: 提出FairMatch后处理方法，利用倾向得分匹配调整子群决策阈值，并对无法匹配样本进行概率校准。

Result: 实验表明，FairMatch能准确定位测试数据中无偏子集，并显著减少剩余数据的偏差。

Conclusion: 倾向得分匹配为公平性评估和缓解提供了有效方法，且不牺牲预测性能。

Abstract: Fairness-aware learning aims to mitigate discrimination against specific
protected social groups (e.g., those categorized by gender, ethnicity, age)
while minimizing predictive performance loss. Despite efforts to improve
fairness in machine learning, prior studies have shown that many models remain
unfair when measured against various fairness metrics. In this paper, we
examine whether the way training and testing data are sampled affects the
reliability of reported fairness metrics. Since training and test sets are
often randomly sampled from the same population, bias present in the training
data may still exist in the test data, potentially skewing fairness
assessments. To address this, we propose FairMatch, a post-processing method
that applies propensity score matching to evaluate and mitigate bias. FairMatch
identifies control and treatment pairs with similar propensity scores in the
test set and adjusts decision thresholds for different subgroups accordingly.
For samples that cannot be matched, we perform probabilistic calibration using
fairness-aware loss functions. Experimental results demonstrate that our
approach can (a) precisely locate subsets of the test data where the model is
unbiased, and (b) significantly reduce bias on the remaining data. Overall,
propensity score matching offers a principled way to improve both fairness
evaluation and mitigation, without sacrificing predictive performance.

</details>

### [22] [In-Context Learning can distort the relationship between sequence likelihoods and biological fitness](https://arxiv.org/abs/2504.17068)
*Pranav Kantroo,Günter P. Wagner,Benjamin B. Machta*

Main category: cs.LG

TLDR: 语言模型在生物序列预测中表现出色，但在上下文学习中可能扭曲适应性与似然分数的关系，尤其是对重复基序的序列。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在生物序列预测中的表现，特别是上下文学习对适应性与似然分数关系的影响。

Method: 使用基于掩码语言建模目标的蛋白质语言模型（尤其是Transformer架构）进行实验。

Result: 发现重复基序的序列会获得异常高的似然分数，模型通过检索行为覆盖其先验知识。

Conclusion: 上下文学习可能导致模型对重复基序的序列产生偏差，影响预测准确性。

Abstract: Language models have emerged as powerful predictors of the viability of
biological sequences. During training these models learn the rules of the
grammar obeyed by sequences of amino acids or nucleotides. Once trained, these
models can take a sequence as input and produce a likelihood score as an
output; a higher likelihood implies adherence to the learned grammar and
correlates with experimental fitness measurements. Here we show that in-context
learning can distort the relationship between fitness and likelihood scores of
sequences. This phenomenon most prominently manifests as anomalously high
likelihood scores for sequences that contain repeated motifs. We use protein
language models with different architectures trained on the masked language
modeling objective for our experiments, and find transformer-based models to be
particularly vulnerable to this effect. This behavior is mediated by a look-up
operation where the model seeks the identity of the masked position by using
the other copy of the repeated motif as a reference. This retrieval behavior
can override the model's learned priors. This phenomenon persists for
imperfectly repeated sequences, and extends to other kinds of biologically
relevant features such as reversed complement motifs in RNA sequences that fold
into hairpin structures.

</details>

### [23] [Enhancing Variational Autoencoders with Smooth Robust Latent Encoding](https://arxiv.org/abs/2504.17219)
*Hyomin Lee,Minseon Kim,Sangwon Jang,Jongheon Jeong,Sung Ju Hwang*

Main category: cs.LG

TLDR: SRL-VAE是一种新型对抗训练框架，通过平滑潜在空间提升生成质量和鲁棒性，同时保持原始保真度。


<details>
  <summary>Details</summary>
Motivation: 尽管对抗训练在预测模型中已被证明能增强鲁棒性，但在生成模型中因担心性能与鲁棒性之间的权衡而被忽视。本文挑战这一假设。

Method: 提出SRL-VAE框架，通过对抗扰动平滑潜在空间，并结合原始表示正则化以保持保真度。

Result: 实验表明，SRL-VAE在图像重建、文本引导编辑及对抗攻击（如Nightshade）中均提升了生成质量和鲁棒性。

Conclusion: 对抗训练不仅能提升生成模型的鲁棒性，还能增强保真度，颠覆了传统认知。

Abstract: Variational Autoencoders (VAEs) have played a key role in scaling up
diffusion-based generative models, as in Stable Diffusion, yet questions
regarding their robustness remain largely underexplored. Although adversarial
training has been an established technique for enhancing robustness in
predictive models, it has been overlooked for generative models due to concerns
about potential fidelity degradation by the nature of trade-offs between
performance and robustness. In this work, we challenge this presumption,
introducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training
framework that boosts both generation quality and robustness. In contrast to
conventional adversarial training, which focuses on robustness only, our
approach smooths the latent space via adversarial perturbations, promoting more
generalizable representations while regularizing with originality
representation to sustain original fidelity. Applied as a post-training step on
pre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal
computational overhead. Experiments show that SRL-VAE improves both generation
quality, in image reconstruction and text-guided image editing, and robustness,
against Nightshade attacks and image editing attacks. These results establish a
new paradigm, showing that adversarial training, once thought to be detrimental
to generative models, can instead enhance both fidelity and robustness.

</details>

### [24] [Sparse Phased Array Optimization Using Deep Learning](https://arxiv.org/abs/2504.17073)
*David Lu,Lior Maman,Jackson Earls,Amir Boag,Pierre Baldi*

Main category: cs.LG

TLDR: 论文提出了一种基于深度学习的稀疏相控阵优化方法，通过减少栅瓣来提升设计效果，平均改进达552%。


<details>
  <summary>Details</summary>
Motivation: 稀疏相控阵设计存在非凸性和自由度高的挑战，传统方法难以优化。

Method: 使用神经网络近似非凸成本函数，结合梯度下降优化天线布局，并引入物理约束增强鲁棒性。

Result: 在10种初始成本最低的阵列配置中，成本进一步降低411%至643%，平均改进552%。

Conclusion: 该方法显著降低了旁瓣水平，为超精确波束成形和下一代无线及雷达系统提供了新途径。

Abstract: Antenna arrays are widely used in wireless communication, radar systems,
radio astronomy, and military defense to enhance signal strength, directivity,
and interference suppression. We introduce a deep learning-based optimization
approach that enhances the design of sparse phased arrays by reducing grating
lobes. This approach begins by generating sparse array configurations to
address the non-convex challenges and extensive degrees of freedom inherent in
array design. We use neural networks to approximate the non-convex cost
function that estimates the energy ratio between the main and side lobes. This
differentiable approximation facilitates cost function minimization through
gradient descent, optimizing the antenna elements' coordinates and leading to
an improved layout. Additionally, we incorporate a tailored penalty mechanism
that includes various physical and design constraints into the optimization
process, enhancing its robustness and practical applicability. We demonstrate
the effectiveness of our method by applying it to the ten array configurations
with the lowest initial costs, achieving further cost reductions ranging from
411% to 643%, with an impressive average improvement of 552%. By significantly
reducing side lobe levels in antenna arrays, this breakthrough paves the way
for ultra-precise beamforming, enhanced interference mitigation, and
next-generation wireless and radar systems with unprecedented efficiency and
clarity.

</details>

### [25] [Conditional Diffusion-Based Retrieval of Atmospheric CO2 from Earth Observing Spectroscopy](https://arxiv.org/abs/2504.17074)
*William R. Keely,Otto Lamminpää,Steffen Mauceri,Sean M. R. Crowell,Christopher W. O'Dell,Gregory R. McGarragh*

Main category: cs.LG

TLDR: 论文提出了一种基于扩散的方法，用于从卫星观测数据中快速准确地反演温室气体浓度，解决了传统最优估计算法计算成本高和后验分布非高斯的问题。


<details>
  <summary>Details</summary>
Motivation: 卫星观测数据反演温室气体浓度对理解碳循环至关重要，但现有方法计算成本高且不确定性估计不准确，亟需开发快速、准确的算法。

Method: 采用基于扩散的方法，灵活反演高斯或非高斯后验分布，显著提升计算速度。

Result: 该方法为NASA的OCO-2光谱仪提供了高效的反演方案，优于现有最优估计算法。

Conclusion: 扩散方法有望实现近实时全球碳源汇监测，对政策制定具有重要价值。

Abstract: Satellite-based estimates of greenhouse gas (GHG) properties from
observations of reflected solar spectra are integral for understanding and
monitoring complex terrestrial systems and their impact on the carbon cycle due
to their near global coverage. Known as retrieval, making GHG concentration
estimations from these observations is a non-linear Bayesian inverse problem,
which is operationally solved using a computationally expensive algorithm
called Optimal Estimation (OE), providing a Gaussian approximation to a
non-Gaussian posterior. This leads to issues in solver algorithm convergence,
and to unrealistically confident uncertainty estimates for the retrieved
quantities. Upcoming satellite missions will provide orders of magnitude more
data than the current constellation of GHG observers. Development of fast and
accurate retrieval algorithms with robust uncertainty quantification is
critical. Doing so stands to provide substantial climate impact of moving
towards the goal of near continuous real-time global monitoring of carbon
sources and sinks which is essential for policy making. To achieve this goal,
we propose a diffusion-based approach to flexibly retrieve a Gaussian or
non-Gaussian posterior, for NASA's Orbiting Carbon Observatory-2 spectrometer,
while providing a substantial computational speed-up over the current
operational state-of-the-art.

</details>

### [26] [A Novel Hybrid Approach Using an Attention-Based Transformer + GRU Model for Predicting Cryptocurrency Prices](https://arxiv.org/abs/2504.17079)
*Esam Mahdi,C. Martin-Barreiro,X. Cabezas*

Main category: cs.LG

TLDR: 提出了一种结合Transformer和GRU的混合深度学习模型，用于提高加密货币价格预测的准确性，并在实验中表现优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 加密货币价格预测的复杂性需要结合长短期依赖的模型，因此提出了一种混合架构以提升预测精度。

Method: 结合Transformer（捕捉长程模式）和GRU（建模短时序列趋势），并与其他四种模型（RBFN、GRNN、BiLSTM、BiGRU）进行比较。

Result: 混合模型在MSE、RMSE、MAE和MAPE等指标上表现最优，并通过统计验证。

Conclusion: 该混合模型在加密货币价格预测中表现卓越，为金融决策提供了有效工具。

Abstract: In this article, we introduce a novel deep learning hybrid model that
integrates attention Transformer and Gated Recurrent Unit (GRU) architectures
to improve the accuracy of cryptocurrency price predictions. By combining the
Transformer's strength in capturing long-range patterns with the GRU's ability
to model short-term and sequential trends, the hybrid model provides a
well-rounded approach to time series forecasting. We apply the model to predict
the daily closing prices of Bitcoin and Ethereum based on historical data that
include past prices, trading volumes, and the Fear and Greed index. We evaluate
the performance of our proposed model by comparing it with four other machine
learning models: two are non-sequential feedforward models: Radial Basis
Function Network (RBFN) and General Regression Neural Network (GRNN), and two
are bidirectional sequential memory-based models: Bidirectional Long-Short-Term
Memory (BiLSTM) and Bidirectional Gated Recurrent Unit (BiGRU). The performance
of the model is assessed using several metrics, including Mean Squared Error
(MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean
Absolute Percentage Error (MAPE), along with statistical validation through the
nonparametric Friedman test followed by a post hoc Wilcoxon signed rank test.
The results demonstrate that our hybrid model consistently achieves superior
accuracy, highlighting its effectiveness for financial prediction tasks. These
findings provide valuable insights for improving real-time decision making in
cryptocurrency markets and support the growing use of hybrid deep learning
models in financial analytics.

</details>

### [27] [GeoRDF2Vec Learning Location-Aware Entity Representations in Knowledge Graphs](https://arxiv.org/abs/2504.17099)
*Martin Boeckling,Heiko Paulheim,Sarah Detzler*

Main category: cs.LG

TLDR: 本文提出了一种改进的RDF2Vec方法，通过融入几何信息生成位置感知的实体嵌入，优于现有非位置感知方法和GeoTransE。


<details>
  <summary>Details</summary>
Motivation: 知识图谱中许多空间实体（如城市、建筑）的几何信息未被现有实体表示学习方法充分利用。

Method: 通过从地理节点扩展图，生成淹没图，并应用基于空间权重的改进RDF2Vec方法。

Result: 在多个基准数据集上，该方法优于非位置感知的RDF2Vec和GeoTransE。

Conclusion: 融入几何信息能显著提升实体表示学习效果。

Abstract: Many knowledge graphs contain a substantial number of spatial entities, such
as cities, buildings, and natural landmarks. For many of these entities, exact
geometries are stored within the knowledge graphs. However, most existing
approaches for learning entity representations do not take these geometries
into account. In this paper, we introduce a variant of RDF2Vec that
incorporates geometric information to learn location-aware embeddings of
entities. Our approach expands different nodes by flooding the graph from
geographic nodes, ensuring that each reachable node is considered. Based on the
resulting flooded graph, we apply a modified version of RDF2Vec that biases
graph walks using spatial weights. Through evaluations on multiple benchmark
datasets, we demonstrate that our approach outperforms both non-location-aware
RDF2Vec and GeoTransE.

</details>

### [28] [Discovering the Precursors of Traffic Breakdowns Using Spatiotemporal Graph Attribution Networks](https://arxiv.org/abs/2504.17109)
*Zhaobin Mo,Xiangyi Liao,Dominik A. Karbowski,Yanbing Wang*

Main category: cs.LG

TLDR: 提出了一种结合时空图神经网络和Shapley值的新方法，用于识别和解释交通崩溃的前兆，并在Interstate-24数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 理解和预测交通崩溃的前兆对提高道路安全和交通流量管理至关重要。

Method: 结合时空图神经网络（ST-GNNs）和Shapley值，扩展Shapley解释方法至时空场景。

Result: 发现道路拓扑和急刹车是导致交通崩溃的主要因素。

Conclusion: 该方法填补了黑盒神经网络预测与可解释原因之间的空白。

Abstract: Understanding and predicting the precursors of traffic breakdowns is critical
for improving road safety and traffic flow management. This paper presents a
novel approach combining spatiotemporal graph neural networks (ST-GNNs) with
Shapley values to identify and interpret traffic breakdown precursors. By
extending Shapley explanation methods to a spatiotemporal setting, our proposed
method bridges the gap between black-box neural network predictions and
interpretable causes. We demonstrate the method on the Interstate-24 data, and
identify that road topology and abrupt braking are major factors that lead to
traffic breakdowns.

</details>

### [29] [Scalable Permutation-Aware Modeling for Temporal Set Prediction](https://arxiv.org/abs/2504.17140)
*Ashish Ranjan,Ayush Agarwal,Shalin Barot,Sushant Kumar*

Main category: cs.LG

TLDR: 提出了一种高效且可扩展的时序集合预测框架，通过置换等变和置换不变变换显著减少计算开销，性能与现有最优模型相当或更优。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖复杂架构，计算开销大，限制了可扩展性。

Method: 利用置换等变和置换不变变换高效建模集合动态。

Result: 在多个公开基准测试中，性能与最优模型相当或更优，同时显著减少训练和推理时间。

Conclusion: 该框架实现了高效且可扩展的时序集合预测。

Abstract: Temporal set prediction involves forecasting the elements that will appear in
the next set, given a sequence of prior sets, each containing a variable number
of elements. Existing methods often rely on intricate architectures with
substantial computational overhead, which hampers their scalability. In this
work, we introduce a novel and scalable framework that leverages
permutation-equivariant and permutation-invariant transformations to
efficiently model set dynamics. Our approach significantly reduces both
training and inference time while maintaining competitive performance.
Extensive experiments on multiple public benchmarks show that our method
achieves results on par with or superior to state-of-the-art models across
several evaluation metrics. These results underscore the effectiveness of our
model in enabling efficient and scalable temporal set prediction.

</details>

### [30] [OUI Need to Talk About Weight Decay: A New Perspective on Overfitting Detection](https://arxiv.org/abs/2504.17160)
*Alberto Fernández-Hernández,Jose I. Mestre,Manuel F. Dolz,Jose Duato,Enrique S. Quintana-Ortí*

Main category: cs.LG

TLDR: OUI是一种新工具，用于监控DNN训练动态并识别最佳正则化超参数，无需验证数据即可指示过拟合或欠拟合。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖验证数据调整超参数，OUI旨在提供更快速、更准确的指标。

Method: 通过实验验证OUI在多种DNN和数据集上的有效性，指导Weight Decay超参数选择。

Result: OUI能更快收敛，显著提高泛化性能，并在训练早期识别最佳超参数。

Conclusion: OUI是一种高效工具，可优化正则化超参数，提升模型性能。

Abstract: We introduce the Overfitting-Underfitting Indicator (OUI), a novel tool for
monitoring the training dynamics of Deep Neural Networks (DNNs) and identifying
optimal regularization hyperparameters. Specifically, we validate that OUI can
effectively guide the selection of the Weight Decay (WD) hyperparameter by
indicating whether a model is overfitting or underfitting during training
without requiring validation data. Through experiments on DenseNet-BC-100 with
CIFAR- 100, EfficientNet-B0 with TinyImageNet and ResNet-34 with ImageNet-1K,
we show that maintaining OUI within a prescribed interval correlates strongly
with improved generalization and validation scores. Notably, OUI converges
significantly faster than traditional metrics such as loss or accuracy,
enabling practitioners to identify optimal WD (hyperparameter) values within
the early stages of training. By leveraging OUI as a reliable indicator, we can
determine early in training whether the chosen WD value leads the model to
underfit the training data, overfit, or strike a well-balanced trade-off that
maximizes validation scores. This enables more precise WD tuning for optimal
performance on the tested datasets and DNNs. All code for reproducing these
experiments is available at https://github.com/AlbertoFdezHdez/OUI.

</details>

### [31] [A Double-Norm Aggregated Tensor Latent Factorization Model for Temporal-Aware Traffic Speed Imputation](https://arxiv.org/abs/2504.17196)
*Jiawen Hou,Hao Wu*

Main category: cs.LG

TLDR: 论文提出了一种名为TATSI的方法，结合$L_2$-norm和${SL}_1$-norm的损失函数，用于填补交通速度数据中的缺失值，提高了算法的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 交通速度数据常因传感器故障或传输延迟而缺失，现有基于张量分解的方法多依赖$L_2$-norm，导致算法鲁棒性不足。

Method: TATSI采用单潜在因子依赖的非负乘法更新（SLF-NMU）方法，结合$L_2$-norm和${SL}_1$-norm的损失函数。

Result: 在三个真实交通速度数据集上的实验表明，TATSI能更精确捕捉时间模式，填补缺失数据的准确性优于现有方法。

Conclusion: TATSI通过结合两种范数，显著提升了交通速度数据填补的准确性和鲁棒性。

Abstract: In intelligent transportation systems (ITS), traffic management departments
rely on sensors, cameras, and GPS devices to collect real-time traffic data.
Traffic speed data is often incomplete due to sensor failures, data
transmission delays, or occlusions, resulting in missing speed data in certain
road segments. Currently, tensor decomposition based methods are extensively
utilized, they mostly rely on the $L_2$-norm to construct their learning
objectives, which leads to reduced robustness in the algorithms. To address
this, we propose Temporal-Aware Traffic Speed Imputation (TATSI), which
combines the $L_2$-norm and smooth $L_1$ (${SL}_1$)-norm in its loss function,
thereby achieving both high accuracy and robust performance in imputing missing
time-varying traffic speed data. TATSI adopts a single latent factor-dependent,
nonnegative, and multiplicative update (SLF-NMU) approach, which serves as an
efficient solver for performing nonnegative latent factor analysis (LFA) on a
tensor. Empirical studies on three real-world time-varying traffic speed
datasets demonstrate that, compared with state-of-the-art traffic speed
predictors, TATSI more precisely captures temporal patterns, thereby yielding
the most accurate imputations for missing traffic speed data.

</details>

### [32] [Synthetic Power Flow Data Generation Using Physics-Informed Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2504.17210)
*Junfei Wang,Darshana Upadhyay,Marzia Zaman,Pirathayini Srikantha*

Main category: cs.LG

TLDR: 本文提出了一种基于DDPM的物理信息生成框架，用于合成可行的电力潮流数据，解决了实际数据受限的问题。


<details>
  <summary>Details</summary>
Motivation: 智能电网中许多数据驱动模块依赖高质量的电力潮流数据，但实际数据因隐私和操作限制而有限。

Method: 采用基于DDPM的物理信息生成框架，结合辅助训练和物理信息损失函数，确保生成数据具有统计保真性和电力系统可行性。

Result: 在IEEE 14-bus和30-bus基准系统上验证，模型在可行性、多样性和统计特征准确性上优于三种基线模型。

Conclusion: 该工作展示了生成模型在数据驱动电力系统应用中的潜力。

Abstract: Many data-driven modules in smart grid rely on access to high-quality power
flow data; however, real-world data are often limited due to privacy and
operational constraints. This paper presents a physics-informed generative
framework based on Denoising Diffusion Probabilistic Models (DDPMs) for
synthesizing feasible power flow data. By incorporating auxiliary training and
physics-informed loss functions, the proposed method ensures that the generated
data exhibit both statistical fidelity and adherence to power system
feasibility. We evaluate the approach on the IEEE 14-bus and 30-bus benchmark
systems, demonstrating its ability to capture key distributional properties and
generalize to out-of-distribution scenarios. Comparative results show that the
proposed model outperforms three baseline models in terms of feasibility,
diversity, and accuracy of statistical features. This work highlights the
potential of integrating generative modelling into data-driven power system
applications.

</details>

### [33] [Multi-Modal Traffic Analysis: Integrating Time-Series Forecasting, Accident Prediction, and Image Classification](https://arxiv.org/abs/2504.17232)
*Nivedita M,Yasmeen Shajitha S*

Main category: cs.LG

TLDR: 提出了一种结合时间序列预测、分类和计算机视觉技术的机器学习框架，用于高级交通分析。


<details>
  <summary>Details</summary>
Motivation: 通过整合多种技术提升交通分析的准确性和实用性，支持智能城市系统的实时监控和事故预防。

Method: 使用ARIMA(2,0,1)模型进行交通预测，XGBoost分类器进行事故严重性分类，CNN进行交通图像分类。

Result: 在多样化数据集上测试，性能优于基线模型，预测MAE为2.1，分类准确率100%（平衡数据），图像分类准确率92%。

Conclusion: 该框架的模块化设计适用于智能城市系统，有助于实时监控、事故预防和资源优化，推动智能交通系统的发展。

Abstract: This study proposes an integrated machine learning framework for advanced
traffic analysis, combining time-series forecasting, classification, and
computer vision techniques. The system utilizes an ARIMA(2,0,1) model for
traffic prediction (MAE: 2.1), an XGBoost classifier for accident severity
classification (100% accuracy on balanced data), and a Convolutional Neural
Network (CNN) for traffic image classification (92% accuracy). Tested on
diverse datasets, the framework outperforms baseline models and identifies key
factors influencing accident severity, including weather and road
infrastructure. Its modular design supports deployment in smart city systems
for real-time monitoring, accident prevention, and resource optimization,
contributing to the evolution of intelligent transportation systems.

</details>

### [34] [NeuralGrok: Accelerate Grokking by Neural Gradient Transformation](https://arxiv.org/abs/2504.17243)
*Xinyu Zhou,Simin Fan,Martin Jaggi,Jie Fu*

Main category: cs.LG

TLDR: NeuralGrok是一种基于梯度的新方法，通过动态调整梯度分量加速Transformer在算术任务中的泛化。


<details>
  <summary>Details</summary>
Motivation: 探索并加速Transformer模型在算术任务中的泛化现象（grokking）。

Method: 训练一个辅助模块（如MLP块），通过双层优化算法动态调整梯度分量对泛化的贡献。

Result: NeuralGrok显著加速泛化，提升训练稳定性，并通过AGE指标验证其降低模型复杂度的效果。

Conclusion: NeuralGrok为理解Transformer的泛化现象提供了新视角，并展示了其实际应用潜力。

Abstract: Grokking is proposed and widely studied as an intricate phenomenon in which
generalization is achieved after a long-lasting period of overfitting. In this
work, we propose NeuralGrok, a novel gradient-based approach that learns an
optimal gradient transformation to accelerate the generalization of
transformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary
module (e.g., an MLP block) in conjunction with the base model. This module
dynamically modulates the influence of individual gradient components based on
their contribution to generalization, guided by a bilevel optimization
algorithm. Our extensive experiments demonstrate that NeuralGrok significantly
accelerates generalization, particularly in challenging arithmetic tasks. We
also show that NeuralGrok promotes a more stable training paradigm, constantly
reducing the model's complexity, while traditional regularization methods, such
as weight decay, can introduce substantial instability and impede
generalization. We further investigate the intrinsic model complexity
leveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that
NeuralGrok effectively facilitates generalization by reducing the model
complexity. We offer valuable insights on the grokking phenomenon of
Transformer models, which encourages a deeper understanding of the fundamental
principles governing generalization ability.

</details>

### [35] [Targeted AMP generation through controlled diffusion with efficient embeddings](https://arxiv.org/abs/2504.17247)
*Diogo Soares,Leon Hetzel,Paulina Szymczak,Fabian Theis,Stephan Günnemann,Ewa Szczurek*

Main category: cs.LG

TLDR: OmegAMP是一个基于扩散模型的框架，用于高效生成具有特定性质的抗菌肽，显著提升发现效率。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在抗菌肽发现中实验命中率低、可控性不足及肽性质建模效率低的问题。

Method: 采用扩散生成模型，结合低维嵌入、精确可控机制和新型分类器，减少假阳性率。

Result: OmegAMP在抗菌肽发现流程中表现优异，显著提升计算框架对抗菌耐药性的潜力。

Conclusion: OmegAMP为抗菌肽发现提供了高效、可控且多样化的解决方案，具有广泛应用前景。

Abstract: Deep learning-based antimicrobial peptide (AMP) discovery faces critical
challenges such as low experimental hit rates as well as the need for nuanced
controllability and efficient modeling of peptide properties. To address these
challenges, we introduce OmegAMP, a framework that leverages a diffusion-based
generative model with efficient low-dimensional embeddings, precise
controllability mechanisms, and novel classifiers with drastically reduced
false positive rates for candidate filtering. OmegAMP enables the targeted
generation of AMPs with specific physicochemical properties, activity profiles,
and species-specific effectiveness. Moreover, it maximizes sample diversity
while ensuring faithfulness to the underlying data distribution during
generation. We demonstrate that OmegAMP achieves state-of-the-art performance
across all stages of the AMP discovery pipeline, significantly advancing the
potential of computational frameworks in combating antimicrobial resistance.

</details>

### [36] [Group Downsampling with Equivariant Anti-aliasing](https://arxiv.org/abs/2504.17258)
*Md Ashiqur Rahman,Raymond A. Yeh*

Main category: cs.LG

TLDR: 论文研究了在群等变架构（如G-CNNs）中均匀下采样层的泛化，提出了一种基于有限群和抗混叠的下采样方法，并在图像分类任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 下采样层是CNN架构的关键组成部分，但现有方法在群等变架构中的泛化能力有限，需要一种更通用的下采样方法。

Method: 提出了一种算法，用于在有限群中选择合适的子群，并研究了带限性和抗混叠的实现方法。

Result: 实验表明，该方法在图像分类任务中提高了准确性，更好地保持了等变性，并减少了模型大小。

Conclusion: 该方法不仅泛化了经典采样理论中的下采样概念，还在实际应用中表现出优越性能。

Abstract: Downsampling layers are crucial building blocks in CNN architectures, which
help to increase the receptive field for learning high-level features and
reduce the amount of memory/computation in the model. In this work, we study
the generalization of the uniform downsampling layer for group equivariant
architectures, e.g., G-CNNs. That is, we aim to downsample signals (feature
maps) on general finite groups with anti-aliasing. This involves the following:
(a) Given a finite group and a downsampling rate, we present an algorithm to
form a suitable choice of subgroup. (b) Given a group and a subgroup, we study
the notion of bandlimited-ness and propose how to perform anti-aliasing.
Notably, our method generalizes the notion of downsampling based on classical
sampling theory. When the signal is on a cyclic group, i.e., periodic, our
method recovers the standard downsampling of an ideal low-pass filter followed
by a subsampling operation. Finally, we conducted experiments on image
classification tasks demonstrating that the proposed downsampling operation
improves accuracy, better preserves equivariance, and reduces model size when
incorporated into G-equivariant networks

</details>

### [37] [Symbolic Representation for Any-to-Any Generative Tasks](https://arxiv.org/abs/2504.17261)
*Jiaqi Chen,Xiaoye Zhu,Yue Wang,Tianyang Liu,Xinhui Chen,Ying Chen,Chak Tou Leong,Yifei Ke,Joseph Liu,Yiwen Yuan,Julian McAuley,Li-jia Li*

Main category: cs.LG

TLDR: 提出了一种符号化生成任务描述语言及推理引擎，能够将多模态任务表示为结构化符号流，无需大规模训练即可实现高效、灵活的任务执行。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型依赖大规模训练和隐式神经表示，计算成本高且灵活性有限，因此需要一种更高效、灵活的方法。

Method: 采用显式符号表示，包括函数、参数和拓扑逻辑三个核心原语，利用预训练语言模型将自然语言指令直接映射为符号化工作流。

Result: 成功执行12种多模态生成任务，性能优于现有统一模型，同时具备更高的效率、可编辑性和可中断性。

Conclusion: 符号化任务表示为生成式AI提供了成本效益高且可扩展的基础。

Abstract: We propose a symbolic generative task description language and a
corresponding inference engine capable of representing arbitrary multimodal
tasks as structured symbolic flows. Unlike conventional generative models that
rely on large-scale training and implicit neural representations to learn
cross-modal mappings, often at high computational cost and with limited
flexibility, our framework introduces an explicit symbolic representation
comprising three core primitives: functions, parameters, and topological logic.
Leveraging a pre-trained language model, our inference engine maps natural
language instructions directly to symbolic workflows in a training-free manner.
Our framework successfully performs over 12 diverse multimodal generative
tasks, demonstrating strong performance and flexibility without the need for
task-specific tuning. Experiments show that our method not only matches or
outperforms existing state-of-the-art unified models in content quality, but
also offers greater efficiency, editability, and interruptibility. We believe
that symbolic task representations provide a cost-effective and extensible
foundation for advancing the capabilities of generative AI.

</details>

### [38] [Signal Recovery from Random Dot-Product Graphs Under Local Differential Privacy](https://arxiv.org/abs/2504.17274)
*Siddharth Vishwanath,Jonathan Hehir*

Main category: cs.LG

TLDR: N/A


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of recovering latent information from graphs under
$\varepsilon$-edge local differential privacy where the presence of
relationships/edges between two users/vertices remains confidential, even from
the data curator. For the class of generalized random dot-product graphs, we
show that a standard local differential privacy mechanism induces a specific
geometric distortion in the latent positions. Leveraging this insight, we show
that consistent recovery of the latent positions is achievable by appropriately
adjusting the statistical inference procedure for the privatized graph.
Furthermore, we prove that our procedure is nearly minimax-optimal under local
edge differential privacy constraints. Lastly, we show that this framework
allows for consistent recovery of geometric and topological information
underlying the latent positions, as encoded in their persistence diagrams. Our
results extend previous work from the private community detection literature to
a substantially richer class of models and inferential tasks.

</details>

### [39] [HeRB: Heterophily-Resolved Structure Balancer for Graph Neural Networks](https://arxiv.org/abs/2504.17276)
*Ke-Jia Chen,Wenhui Mu,Zheng Liu*

Main category: cs.LG

TLDR: 论文提出了一种名为HeRB的方法，通过减少异质性并转移同质性知识来解决GNN中的结构不平衡问题。


<details>
  <summary>Details</summary>
Motivation: GNN在图形数据表示方面取得了显著进展，但仍面临结构不平衡的挑战，且现有方法未考虑异质性的影响。

Method: HeRB包含两个创新组件：1) 减少异质性的增强模块；2) 同质性知识转移机制。

Result: 实验表明，HeRB在两种同质性和六种异质性基准数据集上表现优异。

Conclusion: HeRB通过解决异质性问题并转移同质性知识，有效提升了GNN的性能。

Abstract: Recent research has witnessed the remarkable progress of Graph Neural
Networks (GNNs) in the realm of graph data representation. However, GNNs still
encounter the challenge of structural imbalance. Prior solutions to this
problem did not take graph heterophily into account, namely that connected
nodes process distinct labels or features, thus resulting in a deficiency in
effectiveness. Upon verifying the impact of heterophily on solving the
structural imbalance problem, we propose to rectify the heterophily first and
then transfer homophilic knowledge. To the end, we devise a method named HeRB
(Heterophily-Resolved Structure Balancer) for GNNs. HeRB consists of two
innovative components: 1) A heterophily-lessening augmentation module which
serves to reduce inter-class edges and increase intra-class edges; 2) A
homophilic knowledge transfer mechanism to convey homophilic information from
head nodes to tail nodes. Experimental results demonstrate that HeRB achieves
superior performance on two homophilic and six heterophilic benchmark datasets,
and the ablation studies further validate the efficacy of two proposed
components.

</details>

### [40] [ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders](https://arxiv.org/abs/2504.17277)
*Zongliang Ji,Andre Carlos Kajdacsy-Balla Amaral,Anna Goldenberg,Rahul G. Krishnan*

Main category: cs.LG

TLDR: 论文提出了一种结合离策略学习和特权信息的新方法ExOSITO，用于优化ICU实验室测试订单，减少过度测试并降低成本。


<details>
  <summary>Details</summary>
Motivation: ICU中实验室测试的过度订购增加了临床负担和成本，需要一种平衡信息获取与资源优化的方法。

Method: 采用离策略学习和因果赌博机框架，结合临床知识和观察数据，生成可解释的辅助工具。

Result: ExOSITO优于医生策略和现有方法，减少成本且不遗漏关键测试。

Conclusion: ExOSITO为ICU实验室测试订单提供了一种高效且可解释的解决方案。

Abstract: Ordering a minimal subset of lab tests for patients in the intensive care
unit (ICU) can be challenging. Care teams must balance between ensuring the
availability of the right information and reducing the clinical burden and
costs associated with each lab test order. Most in-patient settings experience
frequent over-ordering of lab tests, but are now aiming to reduce this burden
on both hospital resources and the environment. This paper develops a novel
method that combines off-policy learning with privileged information to
identify the optimal set of ICU lab tests to order. Our approach, EXplainable
Off-policy learning with Side Information for ICU blood Test Orders (ExOSITO)
creates an interpretable assistive tool for clinicians to order lab tests by
considering both the observed and predicted future status of each patient. We
pose this problem as a causal bandit trained using offline data and a reward
function derived from clinically-approved rules; we introduce a novel learning
framework that integrates clinical knowledge with observational data to bridge
the gap between the optimal and logging policies. The learned policy function
provides interpretable clinical information and reduces costs without omitting
any vital lab orders, outperforming both a physician's policy and prior
approaches to this practical problem.

</details>

### [41] [The Ultimate Cookbook for Invisible Poison: Crafting Subtle Clean-Label Text Backdoors with Style Attributes](https://arxiv.org/abs/2504.17300)
*Wencong You,Daniel Lowd*

Main category: cs.LG

TLDR: 本文提出了一种更隐蔽的后门攻击方法AttrBkd，通过提取细粒度属性生成自然触发器，使其在人类检查下难以察觉，同时保持高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击的触发器通常不自然，容易被人类检测和过滤，因此需要更隐蔽的攻击方法。

Method: 提出AttrBkd，包含三种生成自然触发器的方案，并通过人类评估验证其隐蔽性和有效性。

Result: AttrBkd生成的触发器比基线方法更隐蔽（人类检测率更低）且更有效（攻击成功率更高）。

Conclusion: 后门攻击可以通过自然触发器绕过人类检测，同时保持高效，且自动化指标与人类判断存在偏差。

Abstract: Backdoor attacks on text classifiers can cause them to predict a predefined
label when a particular "trigger" is present. Prior attacks often rely on
triggers that are ungrammatical or otherwise unusual, leading to conspicuous
attacks. As a result, human annotators, who play a critical role in curating
training data in practice, can easily detect and filter out these unnatural
texts during manual inspection, reducing the risk of such attacks. We argue
that a key criterion for a successful attack is for text with and without
triggers to be indistinguishable to humans. However, prior work neither
directly nor comprehensively evaluated attack subtlety and invisibility with
human involvement. We bridge the gap by conducting thorough human evaluations
to assess attack subtlety. We also propose \emph{AttrBkd}, consisting of three
recipes for crafting subtle yet effective trigger attributes, such as
extracting fine-grained attributes from existing baseline backdoor attacks. Our
human evaluations find that AttrBkd with these baseline-derived attributes is
often more effective (higher attack success rate) and more subtle (fewer
instances detected by humans) than the original baseline backdoor attacks,
demonstrating that backdoor attacks can bypass detection by being inconspicuous
and appearing natural even upon close inspection, while still remaining
effective. Our human annotation also provides information not captured by
automated metrics used in prior work, and demonstrates the misalignment of
these metrics with human judgment.

</details>

### [42] [Machine learning-based condition monitoring of powertrains in modern electric drives](https://arxiv.org/abs/2504.17305)
*Dinan Li,Panagiotis Kakosimos,Luca Peretti*

Main category: cs.LG

TLDR: 论文探讨了利用数据分析和机器学习优化工业驱动系统，特别是通过开发数据驱动的热模型来提升功率模块的性能。


<details>
  <summary>Details</summary>
Motivation: 数字化技术进步为工业领域带来了革命性变化，通过数据分析可以优化资产性能。工业驱动系统积累了丰富的数据，结合机器学习可以进一步提升智能化水平。

Method: 利用现代电力驱动中的数据，开发了数据驱动的热模型。设计了测试平台，训练和验证了热数字孪生模型，并比较了从传统线性模型到深度神经网络的不同方法。

Result: 通过多种评估指标，比较了不同机器学习模型的性能，并探讨了其在工业嵌入式系统中的实现。

Conclusion: 研究表明，数据驱动的热模型结合机器学习可以有效提升工业系统的智能化水平，为功率模块的温度估计提供了优化方案。

Abstract: The recent technological advances in digitalization have revolutionized the
industrial sector. Leveraging data analytics has now enabled the collection of
deep insights into the performance and, as a result, the optimization of
assets. Industrial drives, for example, already accumulate all the necessary
information to control electric machines. These signals include but are not
limited to currents, frequency, and temperature. Integrating machine learning
(ML) models responsible for predicting the evolution of those directly
collected or implicitly derived parameters enhances the smartness of industrial
systems even further. In this article, data already residing in most modern
electric drives has been used to develop a data-driven thermal model of a power
module. A test bench has been designed and used specifically for training and
validating the thermal digital twin undergoing various static and dynamic
operating profiles. Different approaches, from traditional linear models to
deep neural networks, have been implemented to emanate the best ML model for
estimating the case temperature of a power module. Several evaluation metrics
were then used to assess the investigated methods' performance and
implementation in industrial embedded systems.

</details>

### [43] [Class-Conditional Distribution Balancing for Group Robust Classification](https://arxiv.org/abs/2504.17314)
*Miaoyun Zhao,Qiang Zhang,Chenrong Li*

Main category: cs.LG

TLDR: 论文提出了一种无需偏置标注或预测的鲁棒学习方法，通过重新加权样本平衡类条件分布，消除虚假相关性。


<details>
  <summary>Details</summary>
Motivation: 虚假相关性导致模型基于错误原因做出预测，现有方法依赖昂贵的偏置标注或大规模数据，难以适用于资源有限的罕见领域。

Method: 通过减少虚假因素与标签信息的互信息，采用样本重新加权策略实现类条件分布平衡。

Result: 实验表明，该方法性能优异，媲美依赖偏置监督的方法。

Conclusion: 该方法简单有效，无需偏置标注或预测，适用于资源有限场景。

Abstract: Spurious correlations that lead models to correct predictions for the wrong
reasons pose a critical challenge for robust real-world generalization.
Existing research attributes this issue to group imbalance and addresses it by
maximizing group-balanced or worst-group accuracy, which heavily relies on
expensive bias annotations. A compromise approach involves predicting bias
information using extensively pretrained foundation models, which requires
large-scale data and becomes impractical for resource-limited rare domains. To
address these challenges, we offer a novel perspective by reframing the
spurious correlations as imbalances or mismatches in class-conditional
distributions, and propose a simple yet effective robust learning method that
eliminates the need for both bias annotations and predictions. With the goal of
reducing the mutual information between spurious factors and label information,
our method leverages a sample reweighting strategy to achieve class-conditional
distribution balancing, which automatically highlights minority groups and
classes, effectively dismantling spurious correlations and producing a debiased
data distribution for classification. Extensive experiments and analysis
demonstrate that our approach consistently delivers state-of-the-art
performance, rivaling methods that rely on bias supervision.

</details>

### [44] [Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization](https://arxiv.org/abs/2504.17355)
*Xiaohan Huang,Dongjie Wang,Zhiyuan Ning,Ziyue Qiao,Qingqing Long,Haowei Zhu,Yi Du,Min Wu,Yuanchun Zhou,Meng Xiao*

Main category: cs.LG

TLDR: TCTO是一种基于多智能体强化学习的图驱动特征工程框架，通过动态交互图优化特征转换路径，提升下游机器学习任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有特征转换方法常忽略转换步骤间的动态依赖关系，导致冗余操作和低效探索。

Method: 提出TCTO框架，利用动态交互图建模特征和转换，通过图剪枝和回溯优化路径，并支持历史子图复用。

Result: 实验表明TCTO在多个数据集上表现优异。

Conclusion: TCTO通过动态图优化和路径回溯，显著提升了特征工程的效率和性能。

Abstract: Feature transformation methods aim to find an optimal mathematical
feature-feature crossing process that generates high-value features and
improves the performance of downstream machine learning tasks. Existing
frameworks, though designed to mitigate manual costs, often treat feature
transformations as isolated operations, ignoring dynamic dependencies between
transformation steps. To address the limitations, we propose TCTO, a
collaborative multi-agent reinforcement learning framework that automates
feature engineering through graph-driven path optimization. The framework's
core innovation lies in an evolving interaction graph that models features as
nodes and transformations as edges. Through graph pruning and backtracking, it
dynamically eliminates low-impact edges, reduces redundant operations, and
enhances exploration stability. This graph also provides full traceability to
empower TCTO to reuse high-utility subgraphs from historical transformations.
To demonstrate the efficacy and adaptability of our approach, we conduct
comprehensive experiments and case studies, which show superior performance
across a range of datasets.

</details>

### [45] [Doubly Adaptive Social Learning](https://arxiv.org/abs/2504.17370)
*Marco Carpentiero,Virginia Bordignon,Vincenzo Matta,Ali H. Sayed*

Main category: cs.LG

TLDR: 论文提出了一种双重自适应社会学习策略（A²SL），用于动态环境中网络代理的信念更新，以应对假设和模型的变化。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，传统的社会学习方法可能因假设和模型的变化而失效，导致代理做出错误决策。

Method: A²SL策略包含两个自适应阶段：1) 使用随机梯度下降跟踪决策模型的漂移；2) 自适应信念更新以跟踪真实假设的变化。

Result: 理论分析表明，所有代理在足够小的自适应参数下能一致学习，最终将信念集中在真实假设上。

Conclusion: A²SL策略在动态环境中有效，理论结果通过合成数据和真实数据验证。

Abstract: In social learning, a network of agents assigns probability scores (beliefs)
to some hypotheses of interest, which rule the generation of local streaming
data observed by each agent. Belief formation takes place by means of an
iterative two-step procedure where: i) the agents update locally their beliefs
by using some likelihood model; and ii) the updated beliefs are combined with
the beliefs of the neighboring agents, using a pooling rule. This procedure can
fail to perform well in the presence of dynamic drifts, leading the agents to
incorrect decision making. Here, we focus on the fully online setting where
both the true hypothesis and the likelihood models can change over time. We
propose the doubly adaptive social learning ($\text{A}^2\text{SL}$) strategy,
which infuses social learning with the necessary adaptation capabilities. This
goal is achieved by exploiting two adaptation stages: i) a stochastic gradient
descent update to learn and track the drifts in the decision model; ii) and an
adaptive belief update to track the true hypothesis changing over time. These
stages are controlled by two adaptation parameters that govern the evolution of
the error probability for each agent. We show that all agents learn
consistently for sufficiently small adaptation parameters, in the sense that
they ultimately place all their belief mass on the true hypothesis. In
particular, the probability of choosing the wrong hypothesis converges to
values on the order of the adaptation parameters. The theoretical analysis is
illustrated both on synthetic data and by applying the $\text{A}^2\text{SL}$
strategy to a social learning problem in the online setting using real data.

</details>

### [46] [Coding for Computation: Efficient Compression of Neural Networks for Reconfigurable Hardware](https://arxiv.org/abs/2504.17403)
*Hans Rosenberger,Rodrigo Fischer,Johanna S. Fröhlich,Ali Bereyhi,Ralf R. Müller*

Main category: cs.LG

TLDR: 本文提出了一种针对可重构硬件（如FPGA）的神经网络压缩方案，通过剪枝、权重共享和线性计算编码（LCC）减少推理时的计算量。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络规模增大，资源高效实现变得尤为重要。现有压缩技术主要关注减少权重存储，而本文旨在减少推理时的加法运算。

Method: 结合剪枝、权重共享和线性计算编码（LCC），优化硬件友好的计算量减少。

Result: 在多层感知机和ResNet-34等大规模深度神经网络上取得竞争性性能。

Conclusion: 该方案为硬件友好的神经网络压缩提供了有效方法。

Abstract: As state of the art neural networks (NNs) continue to grow in size, their
resource-efficient implementation becomes ever more important. In this paper,
we introduce a compression scheme that reduces the number of computations
required for NN inference on reconfigurable hardware such as FPGAs. This is
achieved by combining pruning via regularized training, weight sharing and
linear computation coding (LCC). Contrary to common NN compression techniques,
where the objective is to reduce the memory used for storing the weights of the
NNs, our approach is optimized to reduce the number of additions required for
inference in a hardware-friendly manner. The proposed scheme achieves
competitive performance for simple multilayer perceptrons, as well as for large
scale deep NNs such as ResNet-34.

</details>

### [47] [Towards Harnessing the Collaborative Power of Large and Small Models for Domain Tasks](https://arxiv.org/abs/2504.17421)
*Yang Liu,Bingjie Yan,Tianyuan Zou,Jianqing Zhang,Zixuan Gu,Jianbing Ding,Xidong Wang,Jingyi Li,Xiaozhou Ye,Ye Ouyang,Qiang Yang,Ya-Qin Zhang*

Main category: cs.LG

TLDR: 本文主张通过大模型与小模型的协作，加速大模型在私有领域的适应，并探索了协作策略、挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 大模型需要大量数据和计算资源，而小模型更高效且可定制。协作可以结合两者的优势，推动AI在私有领域的应用。

Method: 探讨了大模型与小模型的协作策略，并分析了潜在挑战与机遇。

Result: 提出行业驱动的研究方向，强调在真实私有数据集和应用上的多目标基准测试。

Conclusion: 协作模式能释放AI新潜力，建议行业优先研究多目标基准测试。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, but
they require vast amounts of data and computational resources. In contrast,
smaller models (SMs), while less powerful, can be more efficient and tailored
to specific domains. In this position paper, we argue that taking a
collaborative approach, where large and small models work synergistically, can
accelerate the adaptation of LLMs to private domains and unlock new potential
in AI. We explore various strategies for model collaboration and identify
potential challenges and opportunities. Building upon this, we advocate for
industry-driven research that prioritizes multi-objective benchmarks on
real-world private datasets and applications.

</details>

### [48] [CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated Active Learning](https://arxiv.org/abs/2504.17448)
*Jun Zhang,Jue Wang,Huan Li,Zhongle Xie,Ke Chen,Lidan Shou*

Main category: cs.LG

TLDR: CHASe是一种针对联邦主动学习（FAL）的客户异构感知数据选择方法，通过跟踪认知变化、校准决策边界和高效数据选择机制，提升了模型准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有FAL方法未考虑客户端数据分布的异构性及模型参数的波动，导致模型准确性下降。

Method: CHASe通过分析训练周期中的推理不一致性跟踪认知变化，使用对齐损失校准决策边界，并通过数据冻结和唤醒机制提高选择效率。

Result: 实验表明，CHASe在多种数据集、模型复杂度和异构联邦设置下优于现有基线方法。

Conclusion: CHASe有效解决了FAL中的异构性问题，显著提升了模型性能和效率。

Abstract: Active learning (AL) reduces human annotation costs for machine learning
systems by strategically selecting the most informative unlabeled data for
annotation, but performing it individually may still be insufficient due to
restricted data diversity and annotation budget. Federated Active Learning
(FAL) addresses this by facilitating collaborative data selection and model
training, while preserving the confidentiality of raw data samples. Yet,
existing FAL methods fail to account for the heterogeneity of data distribution
across clients and the associated fluctuations in global and local model
parameters, adversely affecting model accuracy. To overcome these challenges,
we propose CHASe (Client Heterogeneity-Aware Data Selection), specifically
designed for FAL. CHASe focuses on identifying those unlabeled samples with
high epistemic variations (EVs), which notably oscillate around the decision
boundaries during training. To achieve both effectiveness and efficiency,
\model{} encompasses techniques for 1) tracking EVs by analyzing inference
inconsistencies across training epochs, 2) calibrating decision boundaries of
inaccurate models with a new alignment loss, and 3) enhancing data selection
efficiency via a data freeze and awaken mechanism with subset sampling.
Experiments show that CHASe surpasses various established baselines in terms of
effectiveness and efficiency, validated across diverse datasets, model
complexities, and heterogeneous federation settings.

</details>

### [49] [HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models](https://arxiv.org/abs/2504.17449)
*Jun Zhang,Jue Wang,Huan Li,Lidan Shou,Ke Chen,Gang Chen,Qin Xie,Guiming Xie,Xuejian Gong*

Main category: cs.LG

TLDR: HMI是一种基于分层知识管理的多租户推理系统，旨在高效管理不同PLM租户的资源需求，通过分层知识提取和管理显著减少GPU内存使用，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型（PLMs）的高计算需求在多租户环境中效率低下，需要专用硬件，因此需要一种资源高效的管理系统。

Method: 1. 将PLM知识分为通用、领域特定和任务特定三类，构建分层PLMs（hPLMs）以减少GPU内存使用。2. 通过频率构建和更新领域知识树管理领域特定知识，通过参数交换管理任务特定知识。3. 系统优化包括分层知识预取和批处理矩阵乘法。

Result: 实验表明，HMI可在单个GPU上高效服务多达10,000个hPLM（hBERT和hGPT），准确性损失可忽略。

Conclusion: HMI通过分层知识管理和系统优化，显著提高了多租户环境中PLM的资源效率和推理吞吐量。

Abstract: The significant computational demands of pretrained language models (PLMs),
which often require dedicated hardware, present a substantial challenge in
serving them efficiently, especially in multi-tenant environments. To address
this, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant
Inference system, designed to manage tenants with distinct PLMs
resource-efficiently. Our approach is three-fold: Firstly, we categorize PLM
knowledge into general, domain-specific, and task-specific. Leveraging insights
on knowledge acquisition across different model layers, we construct
hierarchical PLMs (hPLMs) by extracting and storing knowledge at different
levels, significantly reducing GPU memory usage per tenant. Secondly, we
establish hierarchical knowledge management for hPLMs generated by various
tenants in HMI. We manage domain-specific knowledge with acceptable storage
increases by constructing and updating domain-specific knowledge trees based on
frequency. We manage task-specific knowledge within limited GPU memory through
parameter swapping. Finally, we propose system optimizations to enhance
resource utilization and inference throughput. These include fine-grained
pipelining via hierarchical knowledge prefetching to overlap CPU and I/O
operations with GPU computations, and optimizing parallel implementations with
batched matrix multiplications. Our experimental results demonstrate that the
proposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a
single GPU, with only a negligible compromise in accuracy.

</details>

### [50] [Evaluating Time Series Models for Urban Wastewater Management: Predictive Performance, Model Complexity and Resilience](https://arxiv.org/abs/2504.17461)
*Vipin Singh,Tianheng Ling,Teodor Chiaburu,Felix Biessmann*

Main category: cs.LG

TLDR: 该论文提出了一种评估神经网络架构用于城市污水系统时间序列预测的协议，重点考察预测性能、模型复杂性和抗干扰能力。研究发现全局模型性能更优，但局部模型在分散场景中更具韧性。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致极端降雨频率增加，给城市污水系统带来压力，传统物理模型成本高且难以适应动态变化，机器学习提供了一种更具成本效益和适应性的解决方案。

Method: 提出了一种评估神经网络架构的协议，比较全局模型和局部模型的性能，并引入误差模型评估模型韧性。

Result: 全局模型预测性能更高，但局部模型在分散场景中更具韧性；具有更长预测视野的模型对数据扰动更具鲁棒性。

Conclusion: 研究为可持续城市污水管理提供了可解释且可靠的机器学习解决方案，相关实现已在GitHub上开源。

Abstract: Climate change increases the frequency of extreme rainfall, placing a
significant strain on urban infrastructures, especially Combined Sewer Systems
(CSS). Overflows from overburdened CSS release untreated wastewater into
surface waters, posing environmental and public health risks. Although
traditional physics-based models are effective, they are costly to maintain and
difficult to adapt to evolving system dynamics. Machine Learning (ML)
approaches offer cost-efficient alternatives with greater adaptability. To
systematically assess the potential of ML for modeling urban infrastructure
systems, we propose a protocol for evaluating Neural Network architectures for
CSS time series forecasting with respect to predictive performance, model
complexity, and robustness to perturbations. In addition, we assess model
performance on peak events and critical fluctuations, as these are the key
regimes for urban wastewater management. To investigate the feasibility of
lightweight models suitable for IoT deployment, we compare global models, which
have access to all information, with local models, which rely solely on nearby
sensor readings. Additionally, to explore the security risks posed by network
outages or adversarial attacks on urban infrastructure, we introduce error
models that assess the resilience of models. Our results demonstrate that while
global models achieve higher predictive performance, local models provide
sufficient resilience in decentralized scenarios, ensuring robust modeling of
urban infrastructure. Furthermore, models with longer native forecast horizons
exhibit greater robustness to data perturbations. These findings contribute to
the development of interpretable and reliable ML solutions for sustainable
urban wastewater management. The implementation is available in our GitHub
repository.

</details>

### [51] [GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework](https://arxiv.org/abs/2504.17471)
*Yacine Belal,Mohamed Maouche,Sonia Ben Mokhtar,Anthony Simonet-Boulogne*

Main category: cs.LG

TLDR: GRANITE框架通过历史感知的拜占庭抗性对等采样协议（HaPS）和自适应概率阈值（APT），在稀疏动态图中实现鲁棒学习，抵御高达30%的拜占庭节点攻击。


<details>
  <summary>Details</summary>
Motivation: 解决动态图中Gossip Learning对拜占庭攻击的脆弱性问题，尤其是当拜占庭节点通过攻击RPS协议扩大模型毒化时。

Method: 结合HaPS协议（减少对抗性影响）和APT（根据拜占庭节点比例自适应设置聚合阈值）。

Result: GRANITE在高达30%拜占庭节点下仍能保持收敛，学习速度提升，且适用于比现有理论稀疏9倍的图。

Conclusion: GRANITE为动态稀疏图中的鲁棒学习提供了有效解决方案，显著提升了抗攻击能力和学习效率。

Abstract: Gossip Learning (GL) is a decentralized learning paradigm where users
iteratively exchange and aggregate models with a small set of neighboring
peers. Recent GL approaches rely on dynamic communication graphs built and
maintained using Random Peer Sampling (RPS) protocols. Thanks to graph
dynamics, GL can achieve fast convergence even over extremely sparse
topologies. However, the robustness of GL over dy- namic graphs to Byzantine
(model poisoning) attacks remains unaddressed especially when Byzantine nodes
attack the RPS protocol to scale up model poisoning. We address this issue by
introducing GRANITE, a framework for robust learning over sparse, dynamic
graphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two
key components (i) a History-aware Byzantine-resilient Peer Sampling protocol
(HaPS), which tracks previously encountered identifiers to reduce adversarial
influence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which
leverages an estimate of Byzantine presence to set aggregation thresholds with
formal guarantees. Empirical results confirm that GRANITE maintains convergence
with up to 30% Byzantine nodes, improves learning speed via adaptive filtering
of poisoned models and obtains these results in up to 9 times sparser graphs
than dictated by current theory.

</details>

### [52] [Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning](https://arxiv.org/abs/2504.17490)
*Mingqi Yuan,Qi Wang,Guozheng Ma,Bo Li,Xin Jin,Yunbo Wang,Xiaokang Yang,Wenjun Zeng,Dacheng Tao*

Main category: cs.LG

TLDR: Plasticine是一个开源框架，用于评估深度强化学习中的可塑性优化问题，提供多种方法和指标。


<details>
  <summary>Details</summary>
Motivation: 开发终身学习代理对通用人工智能至关重要，但深度强化学习系统常因可塑性损失而失去适应能力，缺乏统一基准。

Method: Plasticine框架集成了13种缓解方法、10种评估指标及不同非平稳性学习场景，支持系统化评估。

Result: Plasticine为研究者提供了量化可塑性损失、评估策略和分析动态的工具。

Conclusion: Plasticine填补了可塑性优化领域的空白，支持更高效的研究。

Abstract: Developing lifelong learning agents is crucial for artificial general
intelligence. However, deep reinforcement learning (RL) systems often suffer
from plasticity loss, where neural networks gradually lose their ability to
adapt during training. Despite its significance, this field lacks unified
benchmarks and evaluation protocols. We introduce Plasticine, the first
open-source framework for benchmarking plasticity optimization in deep RL.
Plasticine provides single-file implementations of over 13 mitigation methods,
10 evaluation metrics, and learning scenarios with increasing non-stationarity
levels from standard to open-ended environments. This framework enables
researchers to systematically quantify plasticity loss, evaluate mitigation
strategies, and analyze plasticity dynamics across different contexts. Our
documentation, examples, and source code are available at
https://github.com/RLE-Foundation/Plasticine.

</details>

### [53] [Prototype-enhanced prediction in graph neural networks for climate applications](https://arxiv.org/abs/2504.17492)
*Nawid Keshtmand,Elena Fillola,Jeffrey Nicholas Clark,Raul Santos-Rodriguez,Matthew Rigby*

Main category: cs.LG

TLDR: 通过原型改进高维数据驱动仿真器的输出质量，提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 减少物理仿真计算成本和时间，提高仿真器输出的质量。

Method: 使用原型（仿真输出的近似值）作为额外输入，提升模型预测能力。

Result: 原型模型性能更优，随机选择原型即可提升性能，数据驱动方法（如k-means）可进一步提升10%。

Conclusion: 原型方法有效提升仿真器性能，数据驱动选择原型效果更佳。

Abstract: Data-driven emulators are increasingly being used to learn and emulate
physics-based simulations, reducing computational expense and run time. Here,
we present a structured way to improve the quality of these high-dimensional
emulated outputs, through the use of prototypes: an approximation of the
emulator's output passed as an input, which informs the model and leads to
better predictions. We demonstrate our approach to emulate atmospheric
dispersion, key for greenhouse gas emissions monitoring, by comparing a
baseline model to models trained using prototypes as an additional input. The
prototype models achieve better performance, even with few prototypes and even
if they are chosen at random, but we show that choosing the prototypes through
data-driven methods (k-means) can lead to almost 10\% increased performance in
some metrics.

</details>

### [54] [Goal-Oriented Time-Series Forecasting: Foundation Framework Design](https://arxiv.org/abs/2504.17493)
*Luca-Andrei Fechete,Mohamed Sana,Fadhel Ayed,Nicola Piovesan,Wenjie Li,Antonio De Domenico,Tareq Si Salem*

Main category: cs.LG

TLDR: 提出了一种动态调整预测范围的新训练方法，提升预测精度和应用性能。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测仅关注最小化预测误差，忽略了实际应用中对特定预测范围的需求。

Method: 将整个信号范围分解为小段，动态加权组合以生成预测。

Result: 在标准数据集和新无线通信数据集上测试，提高了预测精度和应用性能。

Conclusion: 为预测与决策在实际应用中更好结合提供了基础。

Abstract: Traditional time-series forecasting often focuses only on minimizing
prediction errors, ignoring the specific requirements of real-world
applications that employ them. This paper presents a new training methodology,
which allows a forecasting model to dynamically adjust its focus based on the
importance of forecast ranges specified by the end application. Unlike previous
methods that fix these ranges beforehand, our training approach breaks down
predictions over the entire signal range into smaller segments, which are then
dynamically weighted and combined to produce accurate forecasts. We tested our
method on standard datasets, including a new dataset from wireless
communication, and found that not only it improves prediction accuracy but also
improves the performance of end application employing the forecasting model.
This research provides a basis for creating forecasting systems that better
connect prediction and decision-making in various practical applications.

</details>

### [55] [Combining GCN Structural Learning with LLM Chemical Knowledge for or Enhanced Virtual Screening](https://arxiv.org/abs/2504.17497)
*Radia Berreziga,Mohammed Brahimi,Khairedine Kraim,Hamid Azzoune*

Main category: cs.LG

TLDR: 该论文提出了一种结合图卷积网络（GCN）和大语言模型（LLM）嵌入的混合架构，用于虚拟筛选，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法（如SVM和XGBoost）依赖预定义的分子表示，可能导致信息丢失和偏差。深度学习（如GCN）和LLM提供了更优的替代方案。

Method: 提出了一种混合架构，将GCN与LLM嵌入结合，并在每个GCN层后拼接LLM嵌入，以全局整合化学知识。

Result: 模型性能显著优于基线方法（GCN、XGBoost、SVM），F1-score达到88.8%。

Conclusion: 混合架构有效结合了局部结构学习和全局化学知识，为虚拟筛选提供了更优的解决方案。

Abstract: Virtual screening plays a critical role in modern drug discovery by enabling
the identification of promising candidate molecules for experimental
validation. Traditional machine learning methods such as support vector
machines (SVM) and XGBoost rely on predefined molecular representations, often
leading to information loss and potential bias. In contrast, deep learning
approaches-particularly Graph Convolutional Networks (GCNs)-offer a more
expressive and unbiased alternative by operating directly on molecular graphs.
Meanwhile, Large Language Models (LLMs) have recently demonstrated
state-of-the-art performance in drug design, thanks to their capacity to
capture complex chemical patterns from large-scale data via attention
mechanisms.
  In this paper, we propose a hybrid architecture that integrates GCNs with
LLM-derived embeddings to combine localized structural learning with global
chemical knowledge. The LLM embeddings can be precomputed and stored in a
molecular feature library, removing the need to rerun the LLM during training
or inference and thus maintaining computational efficiency. We found that
concatenating the LLM embeddings after each GCN layer-rather than only at the
final layer-significantly improves performance, enabling deeper integration of
global context throughout the network. The resulting model achieves superior
results, with an F1-score of (88.8%), outperforming standalone GCN (87.9%),
XGBoost (85.5%), and SVM (85.4%) baselines.

</details>

### [56] [Tailored minimal reservoir computing: on the bidirectional connection between nonlinearities in the reservoir and in data](https://arxiv.org/abs/2504.17503)
*Davide Prosperino,Haochun Ma,Christoph Räth*

Main category: cs.LG

TLDR: 研究输入数据的非线性程度如何影响储层计算机的最优设计，发现储层的非线性与数据非线性匹配时预测性能最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨储层计算机的非线性设计如何与输入数据的非线性特性匹配，以优化预测性能。

Method: 通过将最小储层计算机简化为单一可调非线性参数，研究预测性能随储层非线性程度的变化，并推广到分数阶Halvorsen系统进行实验。

Result: 预测性能在储层非线性与数据非线性匹配时达到最佳；数据中存在多个非线性时，匹配最小非线性可正确重建预测信号的相关维度。

Conclusion: 提出一种估计未知时间序列最小非线性的方法，并将其应用于传统储层计算机架构，提升性能，特别是在资源受限场景中。

Abstract: We study how the degree of nonlinearity in the input data affects the optimal
design of reservoir computers, focusing on how closely the model's nonlinearity
should align with that of the data. By reducing minimal RCs to a single tunable
nonlinearity parameter, we explore how the predictive performance varies with
the degree of nonlinearity in the reservoir. To provide controlled testbeds, we
generalize to the fractional Halvorsen system, a novel chaotic system with
fractional exponents. Our experiments reveal that the prediction performance is
maximized when the reservoir's nonlinearity matches the nonlinearity present in
the data. In cases where multiple nonlinearities are present in the data, we
find that the correlation dimension of the predicted signal is reconstructed
correctly when the smallest nonlinearity is matched. We use this observation to
propose a method for estimating the minimal nonlinearity in unknown time series
by sweeping the reservoir exponent and identifying the transition to a
successful reconstruction. Applying this method to both synthetic and
real-world datasets, including financial time series, we demonstrate its
practical viability. Finally, we transfer these insights to classical RC by
augmenting traditional architectures with fractional, generalized reservoir
states. This yields performance gains, particularly in resource-constrained
scenarios such as physical reservoirs, where increasing reservoir size is
impractical or economically unviable. Our work provides a principled route
toward tailoring RCs to the intrinsic complexity of the systems they aim to
model.

</details>

### [57] [Communication-Efficient Personalized Distributed Learning with Data and Node Heterogeneity](https://arxiv.org/abs/2504.17520)
*Zhuojun Tian,Zhaoyang Zhang,Yiwei Li,Mehdi Bennis*

Main category: cs.LG

TLDR: 论文提出了一种基于分布式强彩票假设（DSLTH）的通信高效个性化学习算法，通过全局参数与个性化二进制掩码的Hadamard积表示本地模型，并利用群稀疏正则化和掩码聚合算法优化硬件实现。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化学习中的数据与节点异质性挑战，同时满足个性化需求。

Method: 使用全局实值参数与个性化二进制掩码的Hadamard积表示本地模型，引入群稀疏正则化，设计二进制掩码聚合算法。

Result: 数值模拟验证了DSLTH的有效性，并证明了算法的优越性。

Conclusion: 提出的方法在异质节点条件下有效平衡了全局与个性化需求，DSLTH为方法提供了理论基础。

Abstract: To jointly tackle the challenges of data and node heterogeneity in
decentralized learning, we propose a distributed strong lottery ticket
hypothesis (DSLTH), based on which a communication-efficient personalized
learning algorithm is developed. In the proposed method, each local model is
represented as the Hadamard product of global real-valued parameters and a
personalized binary mask for pruning. The local model is learned by updating
and fusing the personalized binary masks while the real-valued parameters are
fixed among different agents. To further reduce the complexity of hardware
implementation, we incorporate a group sparse regularization term in the loss
function, enabling the learned local model to achieve structured sparsity.
Then, a binary mask aggregation algorithm is designed by introducing an
intermediate aggregation tensor and adding a personalized fine-tuning step in
each iteration, which constrains model updates towards the local data
distribution. The proposed method effectively leverages the relativity among
agents while meeting personalized requirements in heterogeneous node
conditions. We also provide a theoretical proof for the DSLTH, establishing it
as the foundation of the proposed method. Numerical simulations confirm the
validity of the DSLTH and demonstrate the effectiveness of the proposed
algorithm.

</details>

### [58] [Cooperative Task Offloading through Asynchronous Deep Reinforcement Learning in Mobile Edge Computing for Future Networks](https://arxiv.org/abs/2504.17526)
*Yuelin Liu,Haiyuan Li,Xenofon Vasilakos,Rasheed Hussain,Dimitra Simeonidou*

Main category: cs.LG

TLDR: 论文提出了一种基于Transformer预测的协作任务卸载框架（CTO-TP），通过异步多智能体深度强化学习优化边缘计算中的任务卸载和资源分配，显著降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 未来网络（如6G）对计算资源需求高，传统单边缘服务器任务卸载策略存在资源利用不均和性能不佳问题，且集中式决策导致高延迟和计算瓶颈。

Method: 采用异步多智能体深度强化学习，结合Transformer驱动的预测，实现边缘服务器间的协作和异步训练，优化任务卸载与资源分配。

Result: CTO-TP算法相比基准方案，系统延迟降低80%，能耗减少87%。

Conclusion: CTO-TP框架有效解决了边缘计算中的延迟和能耗问题，为未来网络提供了高效的任务卸载解决方案。

Abstract: Future networks (including 6G) are poised to accelerate the realisation of
Internet of Everything. However, it will result in a high demand for computing
resources to support new services. Mobile Edge Computing (MEC) is a promising
solution, enabling to offload computation-intensive tasks to nearby edge
servers from the end-user devices, thereby reducing latency and energy
consumption. However, relying solely on a single MEC server for task offloading
can lead to uneven resource utilisation and suboptimal performance in complex
scenarios. Additionally, traditional task offloading strategies specialise in
centralised policy decisions, which unavoidably entail extreme transmission
latency and reach computational bottleneck. To fill the gaps, we propose a
latency and energy efficient Cooperative Task Offloading framework with
Transformer-driven Prediction (CTO-TP), leveraging asynchronous multi-agent
deep reinforcement learning to address these challenges. This approach fosters
edge-edge cooperation and decreases the synchronous waiting time by performing
asynchronous training, optimising task offloading, and resource allocation
across distributed networks. The performance evaluation demonstrates that the
proposed CTO-TP algorithm reduces up to 80% overall system latency and 87%
energy consumption compared to the baseline schemes.

</details>

### [59] [TACO: Tackling Over-correction in Federated Learning with Tailored Adaptive Correction](https://arxiv.org/abs/2504.17528)
*Weijie Liu,Ziwei Zhan,Carlee Joe-Wong,Edith Ngai,Jingpu Duan,Deke Guo,Xu Chen,Xiaoxi Zhang*

Main category: cs.LG

TLDR: 论文提出TACO算法，解决联邦学习中非独立同分布数据导致的过校正问题，通过细粒度梯度校正和模型聚合提升性能。


<details>
  <summary>Details</summary>
Motivation: 非独立同分布数据在联邦学习中导致统计异质性，现有方法的统一校正系数可能引发过校正，影响模型性能。

Method: 提出TACO算法，采用细粒度、客户端特定的梯度校正和模型聚合，减少计算开销。

Result: 实验验证TACO在多种数据集上表现优越且稳定，收敛分析揭示了过校正的根源。

Conclusion: TACO有效解决了非独立同分布数据的挑战，提升了联邦学习的效率和准确性。

Abstract: Non-independent and identically distributed (Non-IID) data across edge
clients have long posed significant challenges to federated learning (FL)
training in edge computing environments. Prior works have proposed various
methods to mitigate this statistical heterogeneity. While these works can
achieve good theoretical performance, in this work we provide the first
investigation into a hidden over-correction phenomenon brought by the uniform
model correction coefficients across clients adopted by existing methods. Such
over-correction could degrade model performance and even cause failures in
model convergence. To address this, we propose TACO, a novel algorithm that
addresses the non-IID nature of clients' data by implementing fine-grained,
client-specific gradient correction and model aggregation, steering local
models towards a more accurate global optimum. Moreover, we verify that leading
FL algorithms generally have better model accuracy in terms of communication
rounds rather than wall-clock time, resulting from their extra computation
overhead imposed on clients. To enhance the training efficiency, TACO deploys a
lightweight model correction and tailored aggregation approach that requires
minimum computation overhead and no extra information beyond the synchronized
model parameters. To validate TACO's effectiveness, we present the first FL
convergence analysis that reveals the root cause of over-correction. Extensive
experiments across various datasets confirm TACO's superior and stable
performance in practice.

</details>

### [60] [Learning Isometric Embeddings of Road Networks using Multidimensional Scaling](https://arxiv.org/abs/2504.17534)
*Juan Carlos Climent Pardo*

Main category: cs.LG

TLDR: 论文提出了一种基于图表示和多维尺度分析（MDS）的方法，以解决自动驾驶中学习泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的自动驾驶应用泛化能力有限，无法覆盖多样化的道路场景。需要一种能够捕捉多种道路结构和动态环境变化的通用方法。

Method: 利用图表示道路网络，并通过多维尺度分析（MDS）技术生成特征空间，以支持神经网络的运动规划任务。

Result: 分析了最先进的图表示和MDS方法在自动驾驶中的应用，并探讨了嵌入图节点以简化学习和降维的可能性。

Conclusion: 图表示结合MDS技术为自动驾驶中的学习泛化问题提供了有效解决方案。

Abstract: The lack of generalization in learning-based autonomous driving applications
is shown by the narrow range of road scenarios that vehicles can currently
cover. A generalizable approach should capture many distinct road structures
and topologies, as well as consider traffic participants, and dynamic changes
in the environment, so that vehicles can navigate and perform motion planning
tasks even in the most difficult situations. Designing suitable feature spaces
for neural network-based motion planers that encapsulate all kinds of road
scenarios is still an open research challenge. This paper tackles this
learning-based generalization challenge and shows how graph representations of
road networks can be leveraged by using multidimensional scaling (MDS)
techniques in order to obtain such feature spaces. State-of-the-art graph
representations and MDS approaches are analyzed for the autonomous driving use
case. Finally, the option of embedding graph nodes is discussed in order to
perform easier learning procedures and obtain dimensionality reduction.

</details>

### [61] [Beyond Cox Models: Assessing the Performance of Machine-Learning Methods in Non-Proportional Hazards and Non-Linear Survival Analysis](https://arxiv.org/abs/2504.17568)
*Ivan Rossi,Flavio Sartori,Cesare Rollo,Giovanni Birolo,Piero Fariselli,Tiziana Sanavia*

Main category: cs.LG

TLDR: 该研究评估了机器学习和深度学习方法在生存分析中的表现，比较了它们与惩罚Cox模型的性能，发现某些条件下非线性和非比例风险模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统Cox模型依赖线性性和比例风险假设，研究旨在探索放宽这些约束的机器学习和深度学习方法的表现。

Method: 在三个合成和三个真实数据集上测试了八种模型，包括六种非线性和四种非比例风险模型，使用Antolini一致性指数和Brier评分评估性能。

Result: Cox回归通常表现良好，但在特定条件下，机器学习和深度学习模型表现更优。Antolini指数和Brier评分结合能更全面评估模型性能。

Conclusion: 生存预测应根据样本量、非线性和非比例风险条件选择合适方法，研究提供了代码和文档以便复现。

Abstract: Survival analysis often relies on Cox models, assuming both linearity and
proportional hazards (PH). This study evaluates machine and deep learning
methods that relax these constraints, comparing their performance with
penalized Cox models on a benchmark of three synthetic and three real datasets.
In total, eight different models were tested, including six non-linear models
of which four were also non-PH. Although Cox regression often yielded
satisfactory performance, we showed the conditions under which machine and deep
learning models can perform better. Indeed, the performance of these methods
has often been underestimated due to the improper use of Harrell's concordance
index (C-index) instead of more appropriate scores such as Antolini's
concordance index, which generalizes C-index in cases where the PH assumption
does not hold. In addition, since occasionally high C-index models happen to be
badly calibrated, combining Antolini's C-index with Brier's score is useful to
assess the overall performance of a survival method. Results on our benchmark
data showed that survival prediction should be approached by testing different
methods to select the most appropriate one according to sample size,
non-linearity and non-PH conditions. To allow an easy reproducibility of these
tests on our benchmark data, code and documentation are freely available at
https://github.com/compbiomed-unito/survhive.

</details>

### [62] [TileLang: A Composable Tiled Programming Model for AI Systems](https://arxiv.org/abs/2504.17577)
*Lei Wang,Yu Cheng,Yining Shi,Zhengju Tang,Zhiwen Mo,Wenhao Xie,Lingxiao Ma,Yuqing Xia,Jilong Xue,Fan Yang,Zhi Yang*

Main category: cs.LG

TLDR: TileLang是一种通用的分块编程模型，旨在简化高性能AI内核编程，通过分离调度空间与数据流，提升效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现代AI内核编程复杂且需要硬件优化，现有编译器在可用性和表达性上存在不足。

Method: TileLang将调度空间（线程绑定、布局、张量化等）与数据流解耦，封装为可定制注释和原语。

Result: 实验表明，TileLang在关键内核中实现了最先进的性能。

Conclusion: TileLang的统一块线程范式及透明调度能力满足了现代AI系统开发的需求。

Abstract: Modern AI workloads rely heavily on optimized computing kernels for both
training and inference. These AI kernels follow well-defined data-flow
patterns, such as moving tiles between DRAM and SRAM and performing a sequence
of computations on those tiles. However, writing high-performance kernels
remains complex despite the clarity of these patterns. Achieving peak
performance requires careful, hardware-centric optimizations to fully leverage
modern accelerators. While domain-specific compilers attempt to reduce the
burden of writing high-performance kernels, they often struggle with usability
and expressiveness gaps. In this paper, we present TileLang, a generalized
tiled programming model for more efficient AI Kernel programming. TileLang
decouples scheduling space (thread binding, layout, tensorize and pipeline)
from dataflow, and encapsulated them as a set of customization annotations and
primitives. This approach allows users to focus on the kernel's data-flow
itself, while leaving most other optimizations to compilers. We conduct
comprehensive experiments on commonly-used devices, across numerous
experiments, our evaluation shows that TileLang can achieve state-of-the-art
performance in key kernels, demonstrating that its unified block-and-thread
paradigm and transparent scheduling capabilities deliver both the power and
flexibility demanded by modern AI system development.

</details>

### [63] [Advancing CMA-ES with Learning-Based Cooperative Coevolution for Scalable Optimization](https://arxiv.org/abs/2504.17578)
*Hongshu Guo,Wenjie Qiu,Zeyuan Ma,Xinglin Zhang,Jun Zhang,Yue-Jiao Gong*

Main category: cs.LG

TLDR: 论文提出了一种基于学习的协作协同进化框架LCC，通过动态调度分解策略优化大规模全局问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有协作协同进化方法需要专家知识选择分解策略，限制了其应用。

Method: LCC通过神经网络参数化分解策略选择器，使用强化学习训练，动态选择最优策略。

Result: 实验表明LCC在优化效果和资源消耗上优于基线，并具有良好迁移性。

Conclusion: LCC为大规模优化问题提供了一种无需专家干预的自动化解决方案。

Abstract: Recent research in Cooperative Coevolution~(CC) have achieved promising
progress in solving large-scale global optimization problems. However, existing
CC paradigms have a primary limitation in that they require deep expertise for
selecting or designing effective variable decomposition strategies. Inspired by
advancements in Meta-Black-Box Optimization, this paper introduces LCC, a
pioneering learning-based cooperative coevolution framework that dynamically
schedules decomposition strategies during optimization processes. The
decomposition strategy selector is parameterized through a neural network,
which processes a meticulously crafted set of optimization status features to
determine the optimal strategy for each optimization step. The network is
trained via the Proximal Policy Optimization method in a reinforcement learning
manner across a collection of representative problems, aiming to maximize the
expected optimization performance. Extensive experimental results demonstrate
that LCC not only offers certain advantages over state-of-the-art baselines in
terms of optimization effectiveness and resource consumption, but it also
exhibits promising transferability towards unseen problems.

</details>

### [64] [Interpretable non-linear dimensionality reduction using gaussian weighted linear transformation](https://arxiv.org/abs/2504.17601)
*Erik Bergh*

Main category: cs.LG

TLDR: 本文提出了一种结合线性和非线性变换的新算法，用于高维数据的降维，同时保持可解释性和表达能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有降维方法（如t-SNE和PCA）在表达能力和可解释性之间的权衡问题。

Method: 通过线性变换和高斯函数的加权组合构建非线性映射，保留线性方法的可解释性。

Result: 算法既能实现强大的降维，又能提供对变换空间的透明理解。

Conclusion: 该算法在学术和工业领域具有实用价值，并提供了用户友好的软件包以促进应用。

Abstract: Dimensionality reduction techniques are fundamental for analyzing and
visualizing high-dimensional data. With established methods like t-SNE and PCA
presenting a trade-off between representational power and interpretability.
This paper introduces a novel approach that bridges this gap by combining the
interpretability of linear methods with the expressiveness of non-linear
transformations. The proposed algorithm constructs a non-linear mapping between
high-dimensional and low-dimensional spaces through a combination of linear
transformations, each weighted by Gaussian functions. This architecture enables
complex non-linear transformations while preserving the interpretability
advantages of linear methods, as each transformation can be analyzed
independently. The resulting model provides both powerful dimensionality
reduction and transparent insights into the transformed space. Techniques for
interpreting the learned transformations are presented, including methods for
identifying suppressed dimensions and how space is expanded and contracted.
These tools enable practitioners to understand how the algorithm preserves and
modifies geometric relationships during dimensionality reduction. To ensure the
practical utility of this algorithm, the creation of user-friendly software
packages is emphasized, facilitating its adoption in both academia and
industry.

</details>

### [65] [TarDiff: Target-Oriented Diffusion Guidance for Synthetic Electronic Health Record Time Series Generation](https://arxiv.org/abs/2504.17613)
*Bowen Deng,Chang Xu,Hao Li,Yuhao Huang,Min Hou,Jiang Bian*

Main category: cs.LG

TLDR: TarDiff是一种新型的目标导向扩散框架，通过任务特定的影响指导生成合成EHR时间序列数据，显著提升下游模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注数据分布的复制，忽略了罕见但重要的情况，导致模型性能受限。TarDiff旨在通过优化合成数据对特定任务的贡献来解决这一问题。

Method: TarDiff将任务特定的影响梯度嵌入反向扩散过程，量化合成样本对下游模型性能的预期贡献，从而生成效用优化的数据。

Result: 在六个公开EHR数据集上，TarDiff在AUPRC和AUROC上分别比现有方法提升20.4%和18.4%，同时保持时间保真度。

Conclusion: TarDiff不仅解决了数据稀缺和类别不平衡问题，还显著提升了临床模型的性能，为医疗分析提供了更可靠的解决方案。

Abstract: Synthetic Electronic Health Record (EHR) time-series generation is crucial
for advancing clinical machine learning models, as it helps address data
scarcity by providing more training data. However, most existing approaches
focus primarily on replicating statistical distributions and temporal
dependencies of real-world data. We argue that fidelity to observed data alone
does not guarantee better model performance, as common patterns may dominate,
limiting the representation of rare but important conditions. This highlights
the need for generate synthetic samples to improve performance of specific
clinical models to fulfill their target outcomes. To address this, we propose
TarDiff, a novel target-oriented diffusion framework that integrates
task-specific influence guidance into the synthetic data generation process.
Unlike conventional approaches that mimic training data distributions, TarDiff
optimizes synthetic samples by quantifying their expected contribution to
improving downstream model performance through influence functions.
Specifically, we measure the reduction in task-specific loss induced by
synthetic samples and embed this influence gradient into the reverse diffusion
process, thereby steering the generation towards utility-optimized data.
Evaluated on six publicly available EHR datasets, TarDiff achieves
state-of-the-art performance, outperforming existing methods by up to 20.4% in
AUPRC and 18.4% in AUROC. Our results demonstrate that TarDiff not only
preserves temporal fidelity but also enhances downstream model performance,
offering a robust solution to data scarcity and class imbalance in healthcare
analytics.

</details>

### [66] [Decentralized Time Series Classification with ROCKET Features](https://arxiv.org/abs/2504.17617)
*Bruno Casella,Matthias Jakobs,Marco Aldinucci,Sebastian Buschjäger*

Main category: cs.LG

TLDR: DROCKS是一个完全去中心化的联邦学习框架，用于时间序列分类，通过ROCKET特征提升性能，并在节点故障和恶意攻击下表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 解决传统联邦学习中客户端-服务器架构带来的单点故障和隐私风险问题。

Method: 采用去中心化架构，通过节点间的顺序路径训练全局模型，并优化本地核选择。

Result: 在UCR存档上的实验显示，DROCKS优于现有的客户端-服务器联邦学习方法，且更具鲁棒性。

Conclusion: DROCKS为时间序列分类提供了一种高效、安全的去中心化联邦学习解决方案。

Abstract: Time series classification (TSC) is a critical task with applications in
various domains, including healthcare, finance, and industrial monitoring. Due
to privacy concerns and data regulations, Federated Learning has emerged as a
promising approach for learning from distributed time series data without
centralizing raw information. However, most FL solutions rely on a
client-server architecture, which introduces robustness and confidentiality
risks related to the distinguished role of the server, which is a single point
of failure and can observe knowledge extracted from clients. To address these
challenges, we propose DROCKS, a fully decentralized FL framework for TSC that
leverages ROCKET (RandOm Convolutional KErnel Transform) features. In DROCKS,
the global model is trained by sequentially traversing a structured path across
federation nodes, where each node refines the model and selects the most
effective local kernels before passing them to the successor. Extensive
experiments on the UCR archive demonstrate that DROCKS outperforms
state-of-the-art client-server FL approaches while being more resilient to node
failures and malicious attacks. Our code is available at
https://anonymous.4open.science/r/DROCKS-7FF3/README.md.

</details>

### [67] [The effects of Hessian eigenvalue spectral density type on the applicability of Hessian analysis to generalization capability assessment of neural networks](https://arxiv.org/abs/2504.17618)
*Nikita Gabdullin*

Main category: cs.LG

TLDR: 本文研究了神经网络（NN）Hessian矩阵的特征值谱密度（HESD）类型及其对泛化能力的影响，提出了统一的HESD分析方法，并探讨了训练过程中HESD的变化。


<details>
  <summary>Details</summary>
Motivation: Hessian矩阵的特征值谱密度（HESD）能反映NN损失曲面的曲率，进而估计泛化能力。本文旨在进一步研究HESD类型的影响因素及其适用性。

Method: 通过大量实验，分析了不同优化器、数据集及预处理方法下HESD的类型（主要正或负），并提出了判断HESD类型和泛化潜力的标准。

Result: 发现HESD主要为正（MP-HESD）是常见现象，而主要为负（MN-HESD）与外部梯度操作相关。提出了统一的HESD分析方法，并观察到准奇异（QS）HESD的影响。

Conclusion: HESD类型与泛化能力相关，但需注意外部梯度操作的影响。准奇异HESD对传统Hessian特征值与曲率关系的假设提出了挑战。

Abstract: Hessians of neural network (NN) contain essential information about the
curvature of NN loss landscapes which can be used to estimate NN generalization
capabilities. We have previously proposed generalization criteria that rely on
the observation that Hessian eigenvalue spectral density (HESD) behaves
similarly for a wide class of NNs. This paper further studies their
applicability by investigating factors that can result in different types of
HESD. We conduct a wide range of experiments showing that HESD mainly has
positive eigenvalues (MP-HESD) for NN training and fine-tuning with various
optimizers on different datasets with different preprocessing and augmentation
procedures. We also show that mainly negative HESD (MN-HESD) is a consequence
of external gradient manipulation, indicating that the previously proposed
Hessian analysis methodology cannot be applied in such cases. We also propose
criteria and corresponding conditions to determine HESD type and estimate NN
generalization potential. These HESD types and previously proposed
generalization criteria are combined into a unified HESD analysis methodology.
Finally, we discuss how HESD changes during training, and show the occurrence
of quasi-singular (QS) HESD and its influence on the proposed methodology and
on the conventional assumptions about the relation between Hessian eigenvalues
and NN loss landscape curvature.

</details>

### [68] [PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph](https://arxiv.org/abs/2504.17641)
*Shengtao Zhang,Haokai Zhang,Shiqi Lou,Zicheng Wang,Zinan Zeng,Yilin Wang,Minnan Luo*

Main category: cs.LG

TLDR: PTCL提出了一种动态节点分类方法，仅需最终时间戳标签，通过伪标签和时间课程学习策略解决标注不足问题。


<details>
  <summary>Details</summary>
Motivation: 动态节点分类在现实场景中常因标注成本高和标签不确定性而难以获取所有时间戳标签，而最终标签更易获得。

Method: PTCL采用时间解耦架构（骨干网络学习时间感知表示，解码器与最终标签对齐生成伪标签）和时间课程学习策略（通过指数衰减函数加权伪标签）。

Result: 实验表明PTCL在多个真实场景中优于其他方法，并贡献了新数据集CoOAG。

Conclusion: PTCL和FLiD框架为标签有限的动态节点分类提供了完整解决方案，支持多种模型和数据集。

Abstract: Dynamic node classification is critical for modeling evolving systems like
financial transactions and academic collaborations. In such systems,
dynamically capturing node information changes is critical for dynamic node
classification, which usually requires all labels at every timestamp. However,
it is difficult to collect all dynamic labels in real-world scenarios due to
high annotation costs and label uncertainty (e.g., ambiguous or delayed labels
in fraud detection). In contrast, final timestamp labels are easier to obtain
as they rely on complete temporal patterns and are usually maintained as a
unique label for each user in many open platforms, without tracking the history
data. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum
Learning), a pioneering method addressing label-limited dynamic node
classification where only final labels are available. PTCL introduces: (1) a
temporal decoupling architecture separating the backbone (learning time-aware
representations) and decoder (strictly aligned with final labels), which
generate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that
prioritizes pseudo-labels closer to the final timestamp by assigning them
higher weights using an exponentially decaying function. We contribute a new
academic dataset (CoOAG), capturing long-range research interest in dynamic
graph. Experiments across real-world scenarios demonstrate PTCL's consistent
superiority over other methods adapted to this task. Beyond methodology, we
propose a unified framework FLiD (Framework for Label-Limited Dynamic Node
Classification), consisting of a complete preparation workflow, training
pipeline, and evaluation standards, and supporting various models and datasets.
The code can be found at https://github.com/3205914485/FLiD.

</details>

### [69] [Aerial Image Classification in Scarce and Unconstrained Environments via Conformal Prediction](https://arxiv.org/abs/2504.17655)
*Farhad Pourkamali-Anaraki*

Main category: cs.LG

TLDR: 本文对共形预测方法在复杂航空图像数据集上的表现进行了实证分析，探讨了其在数据稀缺和高变异性环境中的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究共形预测在真实世界复杂场景中的应用，特别是在数据稀缺和高变异性环境中的表现。

Method: 使用预训练模型（MobileNet、DenseNet和ResNet）进行微调，生成预测集，并通过两条并行管道（带和不带温度缩放）评估校准效果。

Result: 共形预测即使在小样本和简单非一致性分数下也能提供有价值的预测集；温度缩放并不总能缩小预测集。

Conclusion: 未来研究应关注噪声标签对共形预测的影响，并探索模型压缩策略。

Abstract: This paper presents a comprehensive empirical analysis of conformal
prediction methods on a challenging aerial image dataset featuring diverse
events in unconstrained environments. Conformal prediction is a powerful
post-hoc technique that takes the output of any classifier and transforms it
into a set of likely labels, providing a statistical guarantee on the coverage
of the true label. Unlike evaluations on standard benchmarks, our study
addresses the complexities of data-scarce and highly variable real-world
settings. We investigate the effectiveness of leveraging pretrained models
(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to
generate informative prediction sets. To further evaluate the impact of
calibration, we consider two parallel pipelines (with and without temperature
scaling) and assess performance using two key metrics: empirical coverage and
average prediction set size. This setup allows us to systematically examine how
calibration choices influence the trade-off between reliability and efficiency.
Our findings demonstrate that even with relatively small labeled samples and
simple nonconformity scores, conformal prediction can yield valuable
uncertainty estimates for complex tasks. Moreover, our analysis reveals that
while temperature scaling is often employed for calibration, it does not
consistently lead to smaller prediction sets, underscoring the importance of
careful consideration in its application. Furthermore, our results highlight
the significant potential of model compression techniques within the conformal
prediction pipeline for deployment in resource-constrained environments. Based
on our observations, we advocate for future research to delve into the impact
of noisy or ambiguous labels on conformal prediction performance and to explore
effective model reduction strategies.

</details>

### [70] [Effortless, Simulation-Efficient Bayesian Inference using Tabular Foundation Models](https://arxiv.org/abs/2504.17660)
*Julius Vetter,Manuel Gloeckler,Daniel Gedon,Jakob H. Macke*

Main category: cs.LG

TLDR: 本文提出了一种基于预训练概率基础模型（TabPFN）的仿真推理方法NPE-PF，显著提高了仿真效率，减少了所需模拟次数，同时保持了高精度。


<details>
  <summary>Details</summary>
Motivation: 仿真推理（SBI）需要大量模拟数据，尤其是对于昂贵的仿真器，如何减少模拟次数是一个关键挑战。

Method: 利用TabPFN作为预训练的自回归条件密度估计器，提出NPE-PF方法，无需网络选择、训练和超参数调优。

Result: NPE-PF在精度上与现有SBI方法相当，但在仿真效率上显著优于它们，有时甚至减少数量级的模拟次数。

Conclusion: NPE-PF为SBI提供了一种无需训练、通用且高效的解决方案，适用于广泛的随机逆问题。

Abstract: Simulation-based inference (SBI) offers a flexible and general approach to
performing Bayesian inference: In SBI, a neural network is trained on synthetic
data simulated from a model and used to rapidly infer posterior distributions
for observed data. A key goal for SBI is to achieve accurate inference with as
few simulations as possible, especially for expensive simulators. In this work,
we address this challenge by repurposing recent probabilistic foundation models
for tabular data: We show how tabular foundation models -- specifically TabPFN
-- can be used as pre-trained autoregressive conditional density estimators for
SBI. We propose Neural Posterior Estimation with Prior-data Fitted Networks
(NPE-PF) and show that it is competitive with current SBI approaches in terms
of accuracy for both benchmark tasks and two complex scientific inverse
problems. Crucially, it often substantially outperforms them in terms of
simulation efficiency, sometimes requiring orders of magnitude fewer
simulations. NPE-PF eliminates the need for inference network selection,
training, and hyperparameter tuning. We also show that it exhibits superior
robustness to model misspecification and can be scaled to simulation budgets
that exceed the context size limit of TabPFN. NPE-PF provides a new direction
for SBI, where training-free, general-purpose inference models offer efficient,
easy-to-use, and flexible solutions for a wide range of stochastic inverse
problems.

</details>

### [71] [On Multivariate Financial Time Series Classification](https://arxiv.org/abs/2504.17664)
*Grégory Bournassenko*

Main category: cs.LG

TLDR: 论文研究了机器学习和深度学习模型在金融市场多元时间序列分析中的应用，比较了小数据和大数据方法，并探讨了扩展的好处。


<details>
  <summary>Details</summary>
Motivation: 探讨机器学习和深度学习在金融时间序列分析中的有效性，以及大数据方法相对于传统方法的优势。

Method: 比较了传统方法（如SVM）和现代架构（如ConvTimeNet）在小数据和大数据环境下的表现。

Result: 结果表明，深入理解和使用大数据对金融时间序列的分析和预测至关重要。

Conclusion: 大数据方法在金融时间序列分析中具有显著优势，尤其是在现代深度学习架构的支持下。

Abstract: This article investigates the use of Machine Learning and Deep Learning
models in multivariate time series analysis within financial markets. It
compares small and big data approaches, focusing on their distinct challenges
and the benefits of scaling. Traditional methods such as SVMs are contrasted
with modern architectures like ConvTimeNet. The results show the importance of
using and understanding Big Data in depth in the analysis and prediction of
financial time series.

</details>

### [72] [Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence](https://arxiv.org/abs/2504.17703)
*Edward Collins,Michel Wang*

Main category: cs.LG

TLDR: 联邦学习（FL）是一种分布式机器学习范式，允许多个客户端协作训练共享全局模型而无需集中敏感数据，解决了隐私和安全问题。本文综述了FL的核心架构、技术挑战、新兴趋势、实际应用及未来方向。


<details>
  <summary>Details</summary>
Motivation: 解决数据隐私、安全和合规性问题，特别是在医疗、金融和物联网等领域。

Method: 介绍了FL的核心架构、通信协议、生命周期（本地训练、模型聚合、全局更新），以及处理非IID数据、系统异构性、通信开销和隐私保护的技术。

Result: 总结了FL的实际应用、基准数据集和评估指标，并探讨了新兴趋势如个性化FL、跨设备与跨场景设置等。

Conclusion: 提出了未来研究方向，以开发可扩展、高效且可信的FL系统。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in the field
of distributed machine learning, enabling multiple clients such as mobile
devices, edge nodes, or organizations to collaboratively train a shared global
model without the need to centralize sensitive data. This decentralized
approach addresses growing concerns around data privacy, security, and
regulatory compliance, making it particularly attractive in domains such as
healthcare, finance, and smart IoT systems. This survey provides a concise yet
comprehensive overview of Federated Learning, beginning with its core
architecture and communication protocol. We discuss the standard FL lifecycle,
including local training, model aggregation, and global updates. A particular
emphasis is placed on key technical challenges such as handling non-IID
(non-independent and identically distributed) data, mitigating system and
hardware heterogeneity, reducing communication overhead, and ensuring privacy
through mechanisms like differential privacy and secure aggregation.
Furthermore, we examine emerging trends in FL research, including personalized
FL, cross-device versus cross-silo settings, and integration with other
paradigms such as reinforcement learning and quantum computing. We also
highlight real-world applications and summarize benchmark datasets and
evaluation metrics commonly used in FL research. Finally, we outline open
research problems and future directions to guide the development of scalable,
efficient, and trustworthy FL systems.

</details>

### [73] [Fault Diagnosis in New Wind Turbines using Knowledge from Existing Turbines by Generative Domain Adaptation](https://arxiv.org/abs/2504.17709)
*Stefan Jonas,Angela Meyer*

Main category: cs.LG

TLDR: 提出了一种基于CycleGAN的生成深度学习方法，解决风力涡轮机因训练数据不足导致的故障诊断不可靠问题。


<details>
  <summary>Details</summary>
Motivation: 风力涡轮机的智能状态监测对减少停机时间至关重要，但数据驱动的正常行为模型（NBMs）需要大量训练数据，数据不足会导致诊断不可靠。

Method: 通过CycleGAN进行域映射，使缺乏训练数据的风力涡轮机的SCADA数据与具有代表性数据的涡轮机数据相似，从而应用已有NBM。

Result: 在7个不同风力涡轮机上测试，显著提高了数据稀缺情况下的故障诊断性能，F1分数提升10.3%（1个月数据）和16.8%（2周数据）。

Conclusion: 该方法为数据稀缺情况下的异常检测提供了新方向，能更早、更可靠地进行故障诊断。

Abstract: Intelligent condition monitoring of wind turbines is essential for reducing
downtimes. Machine learning models trained on wind turbine operation data are
commonly used to detect anomalies and, eventually, operation faults. However,
data-driven normal behavior models (NBMs) require a substantial amount of
training data, as NBMs trained with scarce data may result in unreliable fault
diagnosis. To overcome this limitation, we present a novel generative deep
learning approach to make SCADA samples from one wind turbine lacking training
data resemble SCADA data from wind turbines with representative training data.
Through CycleGAN-based domain mapping, our method enables the application of an
NBM trained on an existing wind turbine to one with severely limited data. We
demonstrate our approach on field data mapping SCADA samples across 7
substantially different WTs. Our findings show significantly improved fault
diagnosis in wind turbines with scarce data. Our method achieves the most
similar anomaly scores to an NBM trained with abundant data, outperforming NBMs
trained on scarce training data with improvements of +10.3% in F1-score when 1
month of training data is available and +16.8% when 2 weeks are available. The
domain mapping approach outperforms conventional fine-tuning at all considered
degrees of data scarcity, ranging from 1 to 8 weeks of training data. The
proposed technique enables earlier and more reliable fault diagnosis in newly
installed wind farms, demonstrating a novel and promising research direction to
improve anomaly detection when faced with training data scarcity.

</details>

### [74] [Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis and Interpretable Patient-Similarity Representations](https://arxiv.org/abs/2504.17717)
*Óscar Escudero-Arnanz,Antonio G. Marques,Inmaculada Mora-Jiménez,Joaquín Álvarez-Rodríguez,Cristina Soguero-Ruiz*

Main category: cs.LG

TLDR: 提出了一种基于多变量时间序列和患者相似性的可解释机器学习框架，用于预测多药耐药性（MDR），在ICU电子健康记录上验证效果优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 多药耐药性（MDR）是全球健康问题，导致住院时间延长、医疗成本增加和死亡率上升。研究旨在通过可解释的机器学习框架提高预测准确性和解释性。

Method: 将患者建模为多变量时间序列，使用动态时间规整和时间聚类核量化患者相似性，结合逻辑回归、随机森林和支持向量机进行分类，并通过谱聚类和t-SNE可视化高风险集群。

Result: 在ICU电子健康记录上验证，AUC达81%，优于基线模型。识别出关键风险因素（如抗生素使用、侵入性操作）和临床相关集群。

Conclusion: 患者相似性表示与图分析结合，提供了准确的MDR预测和可解释的见解，支持早期检测和患者分层，凸显可解释机器学习在重症监护中的潜力。

Abstract: Background and Objectives: Multidrug Resistance (MDR) is a critical global
health issue, causing increased hospital stays, healthcare costs, and
mortality. This study proposes an interpretable Machine Learning (ML) framework
for MDR prediction, aiming for both accurate inference and enhanced
explainability.
  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing
clinical progression and patient-to-patient interactions. Similarity among
patients is quantified using MTS-based methods: descriptive statistics, Dynamic
Time Warping, and Time Cluster Kernel. These similarity measures serve as
inputs for MDR classification via Logistic Regression, Random Forest, and
Support Vector Machines, with dimensionality reduction and kernel
transformations improving model performance. For explainability, patient
similarity networks are constructed from these metrics. Spectral clustering and
t-SNE are applied to identify MDR-related subgroups and visualize high-risk
clusters, enabling insight into clinically relevant patterns.
  Results: The framework was validated on ICU Electronic Health Records from
the University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms
baseline ML and deep learning models by leveraging graph-based patient
similarity. The approach identifies key risk factors -- prolonged antibiotic
use, invasive procedures, co-infections, and extended ICU stays -- and reveals
clinically meaningful clusters. Code and results are available at
\https://github.com/oscarescuderoarnanz/DM4MTS.
  Conclusions: Patient similarity representations combined with graph-based
analysis provide accurate MDR prediction and interpretable insights. This
method supports early detection, risk factor identification, and patient
stratification, highlighting the potential of explainable ML in critical care.

</details>

### [75] [Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees](https://arxiv.org/abs/2504.17721)
*Cheng Shen,Yuewei Liu*

Main category: cs.LG

TLDR: 论文提出了一种基于统计校准的方法，用于提高钢铁表面缺陷检测的可靠性，通过定义损失函数和风险水平阈值，确保测试集的预期错误率严格受控。


<details>
  <summary>Details</summary>
Motivation: 传统手动检测效率低且成本高，而基于深度学习的自动化检测方法因数据标注不确定性和过拟合问题导致可靠性不足。

Method: 通过满足独立同分布条件的校准数据评估模型性能，定义损失函数量化检测错误率，并基于用户定义的风险水平生成统计严格的阈值，构建预测集。

Result: 该方法能严格控制测试集的预期错误率，并观察到预测集大小与风险水平之间的负相关性，验证了模型的适应性和操作有效性。

Conclusion: 提出的方法显著提升了钢铁表面缺陷检测的可靠性，为自动化检测提供了统计严格的解决方案。

Abstract: In industrial settings, surface defects on steel can significantly compromise
its service life and elevate potential safety risks. Traditional defect
detection methods predominantly rely on manual inspection, which suffers from
low efficiency and high costs. Although automated defect detection approaches
based on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly,
their reliability remains challenged due to data annotation uncertainties
during deep model training and overfitting issues. These limitations may lead
to detection deviations when processing the given new test samples, rendering
automated detection processes unreliable. To address this challenge, we first
evaluate the detection model's practical performance through calibration data
that satisfies the independent and identically distributed (i.i.d) condition
with test data. Specifically, we define a loss function for each calibration
sample to quantify detection error rates, such as the complement of recall rate
and false discovery rate. Subsequently, we derive a statistically rigorous
threshold based on a user-defined risk level to identify high-probability
defective pixels in test images, thereby constructing prediction sets (e.g.,
defect regions). This methodology ensures that the expected error rate (mean
error rate) on the test set remains strictly bounced by the predefined risk
level. Additionally, we observe a negative correlation between the average
prediction set size and the risk level on the test set, establishing a
statistically rigorous metric for assessing detection model uncertainty.
Furthermore, our study demonstrates robust and efficient control over the
expected test set error rate across varying calibration-to-test partitioning
ratios, validating the method's adaptability and operational effectiveness.

</details>

### [76] [Towards Robust LLMs: an Adversarial Robustness Measurement Framework](https://arxiv.org/abs/2504.17723)
*Natan Levy,Adiel Ashrov,Guy Katz*

Main category: cs.LG

TLDR: 本文提出了一种基于RoMA框架的方法，用于量化大型语言模型（LLM）对抗扰动的鲁棒性，无需访问模型参数。通过实验验证了其准确性，并揭示了鲁棒性在不同模型、任务和扰动类型中的显著差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在高风险应用中的可靠性受到对抗扰动的威胁，而现有研究主要关注视觉神经网络，LLM的鲁棒性研究不足。

Method: 采用RoMA框架，通过比较其估计与形式验证方法的结果，量化LLM对抗扰动的鲁棒性。

Result: 实验表明，鲁棒性在不同模型、任务类别和扰动类型中存在显著差异，强调任务特定评估的必要性。

Conclusion: 本文提供了一种系统评估LLM鲁棒性的方法，有助于开发更可靠的语言模型。

Abstract: The rise of Large Language Models (LLMs) has revolutionized artificial
intelligence, yet these models remain vulnerable to adversarial perturbations,
undermining their reliability in high-stakes applications. While adversarial
robustness in vision-based neural networks has been extensively studied, LLM
robustness remains under-explored. We adapt the Robustness Measurement and
Assessment (RoMA) framework to quantify LLM resilience against adversarial
inputs without requiring access to model parameters. By comparing RoMA's
estimates to those of formal verification methods, we demonstrate its accuracy
with minimal error margins while maintaining computational efficiency. Our
empirical evaluation reveals that robustness varies significantly not only
between different models but also across categories within the same task and
between various types of perturbations. This non-uniformity underscores the
need for task-specific robustness evaluations, enabling practitioners to
compare and select models based on application-specific robustness
requirements. Our work provides a systematic methodology to assess LLM
robustness, advancing the development of more reliable language models for
real-world deployment.

</details>

### [77] [Interpretable Early Detection of Parkinson's Disease through Speech Analysis](https://arxiv.org/abs/2504.17739)
*Lorenzo Simone,Mauro Giuseppe Camporeale,Vito Marco Rubino,Vincenzo Gervasi,Giovanni Dimauro*

Main category: cs.LG

TLDR: 提出了一种基于深度学习的早期帕金森病语音检测方法，并通过解释性分析关联预测性语音模式与发音特征。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期症状包括语音障碍，机器学习为及时检测提供了工具。

Method: 使用深度学习分析语音记录，识别关键语音特征以增强解释性。

Result: 在意大利帕金森语音数据库中表现优异，分类性能与现有方法相当。

Conclusion: 该方法不仅性能优越，还能解释预测背后的语音特征，有助于理解神经肌肉损伤。

Abstract: Parkinson's disease is a progressive neurodegenerative disorder affecting
motor and non-motor functions, with speech impairments among its earliest
symptoms. Speech impairments offer a valuable diagnostic opportunity, with
machine learning advances providing promising tools for timely detection. In
this research, we propose a deep learning approach for early Parkinson's
disease detection from speech recordings, which also highlights the vocal
segments driving predictions to enhance interpretability. This approach seeks
to associate predictive speech patterns with articulatory features, providing a
basis for interpreting underlying neuromuscular impairments. We evaluated our
approach using the Italian Parkinson's Voice and Speech Database, containing
831 audio recordings from 65 participants, including both healthy individuals
and patients. Our approach showed competitive classification performance
compared to state-of-the-art methods, while providing enhanced interpretability
by identifying key speech features influencing predictions.

</details>

### [78] [Embedding Empirical Distributions for Computing Optimal Transport Maps](https://arxiv.org/abs/2504.17740)
*Mingchen Jiang,Peng Xu,Xichen Ye,Xiaohui Chen,Yun Yang,Yifan Chen*

Main category: cs.LG

TLDR: 提出了一种基于Transformer和超网络的新方法，用于学习多个概率分布之间的最优传输映射。


<details>
  <summary>Details</summary>
Motivation: 现代信号处理中分布数据的重要性增加，但现有神经最优传输方法主要关注两个分布之间的单一映射。

Method: 使用Transformer架构生成分布数据的嵌入，再通过超网络生成神经最优传输映射。

Result: 通过数值实验验证了嵌入和生成的OT映射的有效性。

Conclusion: 该方法为多分布最优传输问题提供了新的解决方案，代码已开源。

Abstract: Distributional data have become increasingly prominent in modern signal
processing, highlighting the necessity of computing optimal transport (OT) maps
across multiple probability distributions. Nevertheless, recent studies on
neural OT methods predominantly focused on the efficient computation of a
single map between two distributions. To address this challenge, we introduce a
novel approach to learning transport maps for new empirical distributions.
Specifically, we employ the transformer architecture to produce embeddings from
distributional data of varying length; these embeddings are then fed into a
hypernetwork to generate neural OT maps. Various numerical experiments were
conducted to validate the embeddings and the generated OT maps. The model
implementation and the code are provided on
https://github.com/jiangmingchen/HOTET.

</details>

### [79] [MSGCN: Multiplex Spatial Graph Convolution Network for Interlayer Link Weight Prediction](https://arxiv.org/abs/2504.17749)
*Steven E. Wilson,Sina Khanmohammadi*

Main category: cs.LG

TLDR: 提出了一种名为MSGCN的新方法，用于预测多层网络中的链接权重，解决了现有方法在复杂多层网络中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络在链接权重预测任务上表现不足，尤其是在多层网络中，节点跨层连接的复杂性增加了预测难度。

Method: 提出了Multiplex Spatial Graph Convolution Network (MSGCN)，通过跨层空间嵌入信息来预测链接权重，并捕捉多层网络中节点的几何结构。

Result: 实验表明，MSGCN在多种多层网络结构中表现出鲁棒、准确且可泛化的链接权重预测性能。

Conclusion: MSGCN为多层网络中的链接权重预测提供了一种有效的解决方案，具有广泛的应用潜力。

Abstract: Graph Neural Networks (GNNs) have been widely used for various learning
tasks, ranging from node classification to link prediction. They have
demonstrated excellent performance in multiple domains involving
graph-structured data. However, an important category of learning tasks, namely
link weight prediction, has received less emphasis due to its increased
complexity compared to binary link classification. Link weight prediction
becomes even more challenging when considering multilayer networks, where nodes
can be interconnected across multiple layers. To address these challenges, we
propose a new method named Multiplex Spatial Graph Convolution Network (MSGCN),
which spatially embeds information across multiple layers to predict interlayer
link weights. The MSGCN model generalizes spatial graph convolution to
multiplex networks and captures the geometric structure of nodes across
multiple layers. Extensive experiments using data with known interlayer link
information show that the MSGCN model has robust, accurate, and generalizable
link weight prediction performance across a wide variety of multiplex network
structures.

</details>

### [80] [Disaggregated Deep Learning via In-Physics Computing at Radio Frequency](https://arxiv.org/abs/2504.17752)
*Zhihui Gao,Sri Krishna Vadlamani,Kfir Sulimany,Dirk Englund,Tingjun Chen*

Main category: cs.LG

TLDR: WISE是一种新型无线边缘网络计算架构，通过无线广播和射频计算实现高效深度学习推理，显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源有限，传统数字计算架构在实时推理中需要大量内存和计算能力，WISE旨在解决这一问题。

Method: 采用无线广播模型权重和在射频直接进行复数矩阵向量乘法计算。

Result: WISE在6.0 fJ/MAC的超低功耗下实现95.7%的图像分类准确率，计算效率达165.8 TOPS/W。

Conclusion: WISE比传统数字计算效率提升两个数量级，为无线边缘设备提供了高效深度学习推理方案。

Abstract: Modern edge devices, such as cameras, drones, and Internet-of-Things nodes,
rely on deep learning to enable a wide range of intelligent applications,
including object recognition, environment perception, and autonomous
navigation. However, deploying deep learning models directly on the often
resource-constrained edge devices demands significant memory footprints and
computational power for real-time inference using traditional digital computing
architectures. In this paper, we present WISE, a novel computing architecture
for wireless edge networks designed to overcome energy constraints in deep
learning inference. WISE achieves this goal through two key innovations:
disaggregated model access via wireless broadcasting and in-physics computation
of general complex-valued matrix-vector multiplications directly at radio
frequency. Using a software-defined radio platform with wirelessly broadcast
model weights over the air, we demonstrate that WISE achieves 95.7% image
classification accuracy with ultra-low operation power of 6.0 fJ/MAC per
client, corresponding to a computation efficiency of 165.8 TOPS/W. This
approach enables energy-efficient deep learning inference on wirelessly
connected edge devices, achieving more than two orders of magnitude improvement
in efficiency compared to traditional digital computing.

</details>

### [81] [Replay to Remember: Retaining Domain Knowledge in Streaming Language Models](https://arxiv.org/abs/2504.17780)
*Sneh Pillai*

Main category: cs.LG

TLDR: 论文提出了一种结合LoRA和最小化回放机制的轻量级方法，用于解决大语言模型在持续学习中的灾难性遗忘问题，并在三个领域进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 持续学习中的灾难性遗忘问题限制了模型在实际应用中的适应性，尤其是在计算和数据流受限的场景下。

Method: 采用LoRA和最小化回放机制，结合困惑度、语义相似度和GPT评估指标，量化模型的适应、遗忘和恢复能力。

Result: 实验表明，即使是最小化回放也能显著稳定并部分恢复领域特定知识。

Conclusion: 该方法为资源受限的实际场景中部署适应性强的LLM提供了实用见解。

Abstract: Continual learning in large language models (LLMs) typically encounters the
critical challenge of catastrophic forgetting, where previously acquired
knowledge deteriorates upon exposure to new data. While techniques like replay
buffers and parameter-efficient tuning (e.g., Low-Rank Adaptation or LoRA) have
been proposed, few studies investigate real-time domain adaptation under strict
computational and data-stream constraints. In this paper, we demonstrate a
lightweight method combining LoRA and a minimal replay mechanism in a realistic
streaming setting across three diverse knowledge domains: medical question
answering, genetics, and law. Using perplexity, semantic similarity, and
GPT-based human-like evaluation metrics, we quantify the model's adaptation,
forgetting, and recovery over time. Our experiments reveal that while
catastrophic forgetting naturally occurs, even minimal replay significantly
stabilizes and partially restores domain-specific knowledge. This study
contributes practical insights for deploying adaptable LLMs in
resource-constrained, real-world scenarios.

</details>

<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [82] [Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity](https://arxiv.org/abs/2504.16956)
*Cong Qi,Hanzhang Fang,Tianxing Hu,Siqi Jiang,Wei Zhi*

Main category: cs.CL

TLDR: GeneMamba是一种基于状态空间建模的单细胞转录组学基础模型，通过Bi-Mamba架构高效捕获双向基因上下文，解决了Transformer模型的高复杂性和长程依赖问题。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序（scRNA-seq）的高维度、稀疏性和批次效应带来了计算挑战，现有Transformer模型因二次复杂性和长程依赖处理不足而受限。

Method: GeneMamba采用Bi-Mamba架构，具有线性时间复杂性，预训练于近3000万个细胞，结合了生物学目标（如通路感知对比损失和基于排名的基因编码）。

Result: 在多批次整合、细胞类型注释和基因-基因相关性等任务中表现优异，具有强性能、可解释性和鲁棒性。

Conclusion: GeneMamba是Transformer方法的实用替代方案，推动了大规模单细胞数据分析工具的发展。

Abstract: Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of
cellular heterogeneity, but its complexity, which is marked by high
dimensionality, sparsity, and batch effects, which poses major computational
challenges. Transformer-based models have made significant advances in this
domain but are often limited by their quadratic complexity and suboptimal
handling of long-range dependencies. In this work, we introduce GeneMamba, a
scalable and efficient foundation model for single-cell transcriptomics built
on state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba
captures bidirectional gene context with linear-time complexity, offering
substantial computational gains over transformer baselines. The model is
pretrained on nearly 30 million cells and incorporates biologically informed
objectives, including pathway-aware contrastive loss and rank-based gene
encoding. We evaluate GeneMamba across diverse tasks, including multi-batch
integration, cell type annotation, and gene-gene correlation, demonstrating
strong performance, interpretability, and robustness. These results position
GeneMamba as a practical and powerful alternative to transformer-based methods,
advancing the development of biologically grounded, scalable tools for
large-scale single-cell data analysis.

</details>

### [83] [Tokenization Matters: Improving Zero-Shot NER for Indic Languages](https://arxiv.org/abs/2504.16977)
*Priyaranjan Pattnayak,Hitesh Laxmichand Patel,Amit Agarwal*

Main category: cs.CL

TLDR: 论文比较了BPE、SentencePiece和字符级分词方法在低资源印度语言NER任务中的表现，发现SentencePiece在跨语言零样本设置中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究BPE在低资源印度语言NER任务中的适用性不足问题，探索更优的分词方法。

Method: 使用IndicBERT对多种印度语言（如阿萨姆语、孟加拉语等）进行BPE、SentencePiece和字符级分词比较。

Result: SentencePiece在跨语言零样本设置中表现优于BPE，尤其在形态丰富的语言中。

Conclusion: SentencePiece是低资源印度语言NER任务中更有效的分词策略。

Abstract: Tokenization is a critical component of Natural Language Processing (NLP),
especially for low resource languages, where subword segmentation influences
vocabulary structure and downstream task accuracy. Although Byte Pair Encoding
(BPE) is a standard tokenization method in multilingual language models, its
suitability for Named Entity Recognition (NER) in low resource Indic languages
remains underexplored due to its limitations in handling morphological
complexity. In this work, we systematically compare BPE, SentencePiece, and
Character Level tokenization strategies using IndicBERT for NER tasks in low
resource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as
extremely low resource Indic languages like Santali, Manipuri, and Sindhi. We
assess both intrinsic linguistic properties tokenization efficiency, out of
vocabulary (OOV) rates, and morphological preservation as well as extrinsic
downstream performance, including fine tuning and zero shot cross lingual
transfer.
  Our experiments show that SentencePiece is a consistently better performing
approach than BPE for NER in low resource Indic Languages, particularly in zero
shot cross lingual settings, as it better preserves entity consistency. While
BPE provides the most compact tokenization form, it is not capable of
generalization because it misclassifies or even fails to recognize entity
labels when tested on unseen languages. In contrast, SentencePiece constitutes
a better linguistic structural preservation model, benefiting extremely low
resource and morphologically rich Indic languages, such as Santali and
Manipuri, for superior entity recognition, as well as high generalization
across scripts, such as Sindhi, written in Arabic. The results point to
SentencePiece as the more effective tokenization strategy for NER within
multilingual and low resource Indic NLP applications.

</details>

### [84] [Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation](https://arxiv.org/abs/2504.17025)
*Luca Moroni,Giovanni Puccetti,Pere-Lluis Huguet Cabot,Andrei Stefan Bejgu,Edoardo Barba,Alessio Miaschi,Felice Dell'Orletta,Andrea Esuli,Roberto Navigli*

Main category: cs.CL

TLDR: 论文提出了一种名为SAVA的新方法，通过词汇替换优化英语LLMs以适应意大利语，显著降低了token fertility和参数数量，并在多任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs主要针对英语设计，对其他语言支持不足，导致编码效率低和推理速度慢。

Method: 提出了SAVA方法，利用神经映射进行词汇替换，优化了Mistral-7b-v0.1和Llama-3.1-8B模型。

Result: SAVA显著降低了token fertility（25%）和参数数量（1 billion），并在多任务中表现优异。

Conclusion: SAVA方法有效优化了LLMs对非英语语言的支持，且通过有限训练即可恢复性能。

Abstract: The number of pretrained Large Language Models (LLMs) is increasing steadily,
though the majority are designed predominantly for the English language. While
state-of-the-art LLMs can handle other languages, due to language contamination
or some degree of multilingual pretraining data, they are not optimized for
non-English languages, leading to inefficient encoding (high token "fertility")
and slower inference speed. In this work, we thoroughly compare a variety of
vocabulary adaptation techniques for optimizing English LLMs for the Italian
language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a
novel method that leverages neural mapping for vocabulary substitution. SAVA
achieves competitive performance across multiple downstream tasks, enhancing
grounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing
token fertility by 25\%, and Llama-3.1-8B, optimizing the vocabulary and
reducing the number of parameters by 1 billion. We show that, following the
adaptation of the vocabulary, these models can recover their performance with a
relatively limited stage of continual training on the target language. Finally,
we test the capabilities of the adapted models on various multi-choice and
generative tasks.

</details>

### [85] [Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models](https://arxiv.org/abs/2504.17052)
*Shariar Kabir,Kevin Esterling,Yue Dong*

Main category: cs.CL

TLDR: 论文提出了一种评估大语言模型（LLM）政治立场深度的方法，发现其回应具有主题特定的稳定性而非统一意识形态。


<details>
  <summary>Details</summary>
Motivation: 现有研究将LLM输出简单归类为左或右倾向，但未探究其回应是否反映真实信念或仅是训练数据的表面一致性。

Method: 提出新框架，通过分析（1）论证一致性和（2）不确定性量化，评估12个LLM在19项经济政策上的信念稳定性。

Result: LLM表现出主题特定的信念稳定性，左倾模型95%、右倾模型89%的回应在挑战下保持一致，语义熵能有效区分表面一致与真实信念（AUROC=0.78）。

Conclusion: LLM未必具有稳定的人类意识形态，需进行主题特定的可靠性评估。

Abstract: Large Language Models (LLMs) are increasingly shaping political discourse,
yet their responses often display inconsistency when subjected to scrutiny.
While prior research has primarily categorized LLM outputs as left- or
right-leaning to assess their political stances, a critical question remains:
Do these responses reflect genuine internal beliefs or merely surface-level
alignment with training data? To address this, we propose a novel framework for
evaluating belief depth by analyzing (1) argumentative consistency and (2)
uncertainty quantification. We evaluate 12 LLMs on 19 economic policies from
the Political Compass Test, challenging their belief stability with both
supportive and opposing arguments. Our analysis reveals that LLMs exhibit
topic-specific belief stability rather than a uniform ideological stance.
Notably, up to 95% of left-leaning models' responses and 89% of right-leaning
models' responses remain consistent under the challenge, enabling semantic
entropy to achieve high accuracy (AUROC=0.78), effectively distinguishing
between surface-level alignment from genuine belief. These findings call into
question the assumption that LLMs maintain stable, human-like political
ideologies, emphasizing the importance of conducting topic-specific reliability
assessments for real-world applications.

</details>

### [86] [Agree to Disagree? A Meta-Evaluation of LLM Misgendering](https://arxiv.org/abs/2504.17075)
*Arjun Subramonian,Vagrant Gautam,Preethi Seshadri,Dietrich Klakow,Kai-Wei Chang,Yizhou Sun*

Main category: cs.CL

TLDR: 本文通过系统元评估研究了LLM性别错误评估方法的收敛效度，发现不同方法在实例、数据集和模型层面存在显著分歧，且自动评估与人类评估存在本质差异。


<details>
  <summary>Details</summary>
Motivation: 探讨现有LLM性别错误评估方法是否具有收敛效度，即不同方法的结果是否一致。

Method: 对三种现有数据集进行转换，实现概率和生成评估的并行分析，并自动评估6个模型。

Result: 不同方法在20.2%的评估实例中存在冲突，且自动评估无法捕捉性别错误的复杂性。

Conclusion: 建议未来评估需改进方法，并质疑LLM评估中广泛假设不同方法一致的惯例。

Abstract: Numerous methods have been proposed to measure LLM misgendering, including
probability-based evaluations (e.g., automatically with templatic sentences)
and generation-based evaluations (e.g., with automatic heuristics or human
validation). However, it has gone unexamined whether these evaluation methods
have convergent validity, that is, whether their results align. Therefore, we
conduct a systematic meta-evaluation of these methods across three existing
datasets for LLM misgendering. We propose a method to transform each dataset to
enable parallel probability- and generation-based evaluation. Then, by
automatically evaluating a suite of 6 models from 3 families, we find that
these methods can disagree with each other at the instance, dataset, and model
levels, conflicting on 20.2% of evaluation instances. Finally, with a human
evaluation of 2400 LLM generations, we show that misgendering behaviour is
complex and goes far beyond pronouns, which automatic evaluations are not
currently designed to capture, suggesting essential disagreement with human
evaluations. Based on our findings, we provide recommendations for future
evaluations of LLM misgendering. Our results are also more widely relevant, as
they call into question broader methodological conventions in LLM evaluation,
which often assume that different evaluation methods agree.

</details>

### [87] [How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study](https://arxiv.org/abs/2504.17083)
*Rendi Chevi,Kentaro Inui,Thamar Solorio,Alham Fikri Aji*

Main category: cs.CL

TLDR: 研究发现，LLM的语言风格（如权威性、确定性、表达清晰度等）显著影响用户偏好，但具体影响因用户群体和个体特质而异。需注意样本局限性，未来研究将扩大样本并深入分析变量间的因果关系。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM的语言风格如何影响用户偏好，以及这种动态可能带来的双重影响（提升用户体验与增加风险）。

Method: 通过探索性和实验性用户研究，分析不同语言风格对用户偏好的影响，并考察用户个体特质的调节作用。

Result: 语言风格确实影响用户偏好，但具体影响因用户群体和个体特质而异。样本局限性需谨慎解读。

Conclusion: 未来研究需扩大样本多样性，深入分析语言风格、个体特质与偏好的联合效应及因果关系。

Abstract: What makes an interaction with the LLM more preferable for the user? While it
is intuitive to assume that information accuracy in the LLM's responses would
be one of the influential variables, recent studies have found that inaccurate
LLM's responses could still be preferable when they are perceived to be more
authoritative, certain, well-articulated, or simply verbose. These variables
interestingly fall under the broader category of language style, implying that
the style in the LLM's responses might meaningfully influence users'
preferences. This hypothesized dynamic could have double-edged consequences:
enhancing the overall user experience while simultaneously increasing their
susceptibility to risks such as LLM's misinformation or hallucinations. In this
short paper, we present our preliminary studies in exploring this subject.
Through a series of exploratory and experimental user studies, we found that
LLM's language style does indeed influence user's preferences, but how and
which language styles influence the preference varied across different user
populations, and more interestingly, moderated by the user's very own
individual traits. As a preliminary work, the findings in our studies should be
interpreted with caution, particularly given the limitations in our samples,
which still need wider demographic diversity and larger sample sizes. Our
future directions will first aim to address these limitations, which would
enable a more comprehensive joint effect analysis between the language style,
individual traits, and preferences, and further investigate the potential
causal relationship between and beyond these variables.

</details>

### [88] [Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2504.17091)
*Seunghyun Yoo*

Main category: cs.CL

TLDR: 论文提出了一种交互式思维链框架（Interactive CoT），通过透明化、模块化和用户可编辑的推理过程，提升AI的可解释性和负责任使用。


<details>
  <summary>Details</summary>
Motivation: 短内容泛滥和AI快速普及导致深度思考机会减少，削弱了用户的批判性思维和对AI输出的理解。

Method: 框架将推理分解为可检查、修改和重新执行的模块，并整合轻量级编辑适应机制，支持多样认知风格。

Result: 通过透明元数据、偏见检查功能和隐私保护措施，确保伦理透明性。

Conclusion: 该框架设计原则和架构旨在促进批判性参与、负责任交互和包容性适应，应对复杂社会挑战。

Abstract: Due to the proliferation of short-form content and the rapid adoption of AI,
opportunities for deep, reflective thinking have significantly diminished,
undermining users' critical thinking and reducing engagement with the reasoning
behind AI-generated outputs. To address this issue, we propose an Interactive
Chain-of-Thought (CoT) Framework that enhances human-centered explainability
and responsible AI usage by making the model's inference process transparent,
modular, and user-editable. The framework decomposes reasoning into clearly
defined blocks that users can inspect, modify, and re-execute, encouraging
active cognitive engagement rather than passive consumption. It further
integrates a lightweight edit-adaptation mechanism inspired by preference
learning, allowing the system to align with diverse cognitive styles and user
intentions. Ethical transparency is ensured through explicit metadata
disclosure, built-in bias checkpoint functionality, and privacy-preserving
safeguards. This work outlines the design principles and architecture necessary
to promote critical engagement, responsible interaction, and inclusive
adaptation in AI systems aimed at addressing complex societal challenges.

</details>

### [89] [The Rise of Small Language Models in Healthcare: A Comprehensive Survey](https://arxiv.org/abs/2504.17119)
*Muskan Garg,Shaina Raza,Shebuti Rayana,Xingyi Liu,Sunghwan Sohn*

Main category: cs.CL

TLDR: 本文综述了小型语言模型（SLMs）在医疗领域的应用，提出了一种分类框架，帮助医疗专业人员和信息学家理解SLMs的架构优化、适应性和可持续性，并展示了其在医疗NLP任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在医疗应用中取得进展，但数据隐私和资源限制问题促使小型语言模型（SLMs）成为资源受限环境下的可行解决方案。

Method: 通过分类框架分析SLMs在NLP任务、利益相关者角色和护理连续性三个维度的贡献，并探讨了模型构建、适应性和压缩技术。

Result: 展示了SLMs在医疗NLP任务中的实验成果，突出了其变革潜力。

Conclusion: SLMs为医疗信息学提供了高效、可扩展的解决方案，未来研究可基于此框架进一步优化和开发。

Abstract: Despite substantial progress in healthcare applications driven by large
language models (LLMs), growing concerns around data privacy, and limited
resources; the small language models (SLMs) offer a scalable and clinically
viable solution for efficient performance in resource-constrained environments
for next-generation healthcare informatics. Our comprehensive survey presents a
taxonomic framework to identify and categorize them for healthcare
professionals and informaticians. The timeline of healthcare SLM contributions
establishes a foundational framework for analyzing models across three
dimensions: NLP tasks, stakeholder roles, and the continuum of care. We present
a taxonomic framework to identify the architectural foundations for building
models from scratch; adapting SLMs to clinical precision through prompting,
instruction fine-tuning, and reasoning; and accessibility and sustainability
through compression techniques. Our primary objective is to offer a
comprehensive survey for healthcare professionals, introducing recent
innovations in model optimization and equipping them with curated resources to
support future research and development in the field. Aiming to showcase the
groundbreaking advancements in SLMs for healthcare, we present a comprehensive
compilation of experimental results across widely studied NLP tasks in
healthcare to highlight the transformative potential of SLMs in healthcare. The
updated repository is available at Github

</details>

### [90] [Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control](https://arxiv.org/abs/2504.17130)
*Hannah Cyberey,David Evans*

Main category: cs.CL

TLDR: 论文研究了大型语言模型（LLMs）的审查机制，提出了一种检测和控制模型输出中审查水平的方法，并揭示了“思维抑制”这一额外的审查维度。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs如何通过调整拒绝有害请求和生成符合控制者偏好的响应来实现“审查”，并探索其机制。

Method: 使用表征工程技术分析开放权重的安全调整模型，找到拒绝-服从向量以检测和控制审查水平；分析推理LLMs，发现“思维抑制”维度。

Result: 提出了一种方法可以找到控制审查水平的向量，并揭示了“思维抑制”作为额外的审查机制。

Conclusion: 通过表征工程技术可以揭示和控制LLMs的审查机制，包括拒绝-服从和思维抑制，为理解和操作模型行为提供了新途径。

Abstract: Large language models (LLMs) have transformed the way we access information.
These models are often tuned to refuse to comply with requests that are
considered harmful and to produce responses that better align with the
preferences of those who control the models. To understand how this
"censorship" works. We use representation engineering techniques to study
open-weights safety-tuned models. We present a method for finding a
refusal--compliance vector that detects and controls the level of censorship in
model outputs. We also analyze recent reasoning LLMs, distilled from
DeepSeek-R1, and uncover an additional dimension of censorship through "thought
suppression". We show a similar approach can be used to find a vector that
suppresses the model's reasoning process, allowing us to remove censorship by
applying the negative multiples of this vector

</details>

### [91] [MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation](https://arxiv.org/abs/2504.17137)
*Chanhee Park,Hyeonseok Moon,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TLDR: MIRAGE是一个专门为RAG系统评估设计的问答数据集，包含7,560个实例和37,800个检索条目，并引入了新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 由于RAG系统中检索与生成组件的复杂交互，现有评估方法不足，缺乏针对性的基准。

Method: 提出MIRAGE数据集，包含精心设计的实例和检索条目，并引入新的评估指标（如噪声脆弱性、上下文可接受性等）。

Result: 通过实验揭示了RAG系统中模型对的最优对齐方式及其动态特性。

Conclusion: MIRAGE为RAG系统的评估提供了高效、精确的工具，数据集和代码已公开。

Abstract: Retrieval-Augmented Generation (RAG) has gained prominence as an effective
method for enhancing the generative capabilities of Large Language Models
(LLMs) through the incorporation of external knowledge. However, the evaluation
of RAG systems remains a challenge, due to the intricate interplay between
retrieval and generation components. This limitation has resulted in a scarcity
of benchmarks that facilitate a detailed, component-specific assessment. In
this work, we present MIRAGE, a Question Answering dataset specifically
designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped
to a retrieval pool of 37,800 entries, enabling an efficient and precise
evaluation of both retrieval and generation tasks. We also introduce novel
evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions
such as noise vulnerability, context acceptability, context insensitivity, and
context misinterpretation. Through comprehensive experiments across various
retriever-LLM configurations, we provide new insights into the optimal
alignment of model pairs and the nuanced dynamics within RAG systems. The
dataset and evaluation code are publicly available, allowing for seamless
integration and customization in diverse research settings\footnote{The MIRAGE
code and data are available at https://github.com/nlpai-lab/MIRAGE.

</details>

### [92] [Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning](https://arxiv.org/abs/2504.17192)
*Minju Seo,Jinheon Baek,Seongyun Lee,Sung Ju Hwang*

Main category: cs.CL

TLDR: PaperCoder是一个多智能体LLM框架，将机器学习论文转化为功能代码仓库，通过规划、分析和生成三个阶段实现，并在评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 机器学习研究的代码实现常不可用，导致复现和扩展工作耗时费力。LLMs在理解科学文档和生成高质量代码方面表现优异，因此开发了PaperCoder。

Method: PaperCoder分为三个阶段：规划（设计架构、依赖关系）、分析（解读实现细节）、生成（模块化代码）。每个阶段由专门智能体协作完成。

Result: PaperCoder在生成高质量代码实现方面表现优异，在PaperBench基准测试中显著超越基线方法。

Conclusion: PaperCoder能有效将论文转化为功能代码，为研究复现和扩展提供了高效工具。

Abstract: Despite the rapid growth of machine learning research, corresponding code
implementations are often unavailable, making it slow and labor-intensive for
researchers to reproduce results and build upon prior work. In the meantime,
recent Large Language Models (LLMs) excel at understanding scientific documents
and generating high-quality code. Inspired by this, we introduce PaperCoder, a
multi-agent LLM framework that transforms machine learning papers into
functional code repositories. PaperCoder operates in three stages: planning,
where it constructs a high-level roadmap, designs the system architecture with
diagrams, identifies file dependencies, and generates configuration files;
analysis, which focuses on interpreting implementation-specific details; and
generation, where modular, dependency-aware code is produced. Moreover, each
phase is instantiated through a set of specialized agents designed to
collaborate effectively across the pipeline. We then evaluate PaperCoder on
generating code implementations from machine learning papers based on both
model-based and human evaluations, specifically from the original paper
authors, with author-released repositories as ground truth if available. Our
results demonstrate the effectiveness of PaperCoder in creating high-quality,
faithful implementations. Furthermore, it consistently shows strengths in the
recently released PaperBench benchmark, surpassing strong baselines by
substantial margins.

</details>

### [93] [A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation](https://arxiv.org/abs/2504.17200)
*Yangxinyu Xie,Bowen Jiang,Tanwi Mallick,Joshua David Bergerson,John K. Hutchison,Duane R. Verner,Jordan Branham,M. Ross Alexander,Robert B. Ross,Yan Feng,Leslie-Anne Levy,Weijie Su,Camillo J. Taylor*

Main category: cs.CL

TLDR: 论文提出了一种基于检索增强生成（RAG）的多智能体LLM系统WildfireGPT，专注于野火灾害，通过整合专业数据提升决策支持的准确性和上下文相关性。


<details>
  <summary>Details</summary>
Motivation: 通用LLM在提供特定领域（如自然灾害）的上下文信息时表现不佳，因此需要开发专门化的系统以支持决策。

Method: 采用RAG框架和多智能体设计，整合灾害预测数据、观测数据集和科学文献，为用户提供定制化风险分析。

Result: 在十个专家主导的案例研究中，WildfireGPT显著优于现有基于LLM的决策支持方案。

Conclusion: WildfireGPT展示了多智能体RAG系统在自然灾害决策支持中的潜力，为LLM在专业领域的应用提供了新方向。

Abstract: Large language models (LLMs) are a transformational capability at the
frontier of artificial intelligence and machine learning that can support
decision-makers in addressing pressing societal challenges such as extreme
natural hazard events. As generalized models, LLMs often struggle to provide
context-specific information, particularly in areas requiring specialized
knowledge. In this work we propose a retrieval-augmented generation (RAG)-based
multi-agent LLM system to support analysis and decision-making in the context
of natural hazards and extreme weather events. As a proof of concept, we
present WildfireGPT, a specialized system focused on wildfire hazards. The
architecture employs a user-centered, multi-agent design to deliver tailored
risk insights across diverse stakeholder groups. By integrating natural hazard
and extreme weather projection data, observational datasets, and scientific
literature through an RAG framework, the system ensures both the accuracy and
contextual relevance of the information it provides. Evaluation across ten
expert-led case studies demonstrates that WildfireGPT significantly outperforms
existing LLM-based solutions for decision support.

</details>

### [94] [Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?](https://arxiv.org/abs/2504.17220)
*Kaidong Feng,Zhu Sun,Jie Yang,Hui Fang,Xinghua Qu,Wenyuan Liu*

Main category: cs.CL

TLDR: 该论文研究了知识蒸馏（KD）在基于大型语言模型（LLM）的捆绑生成中的应用，旨在降低计算成本同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 由于LLM的大规模参数化导致的高计算成本，研究如何通过知识蒸馏技术将大模型的知识转移到小模型，以提高效率。

Method: 提出一个全面的KD框架，包括逐步提取知识、捕获不同数量的蒸馏知识，以及利用互补的LLM适应技术。

Result: 实验表明知识格式、数量和利用方法共同影响捆绑生成性能，KD在提高效率的同时保持性能方面具有显著潜力。

Conclusion: 知识蒸馏为基于LLM的捆绑生成提供了一种高效且有效的解决方案。

Abstract: LLMs are increasingly explored for bundle generation, thanks to their
reasoning capabilities and knowledge. However, deploying large-scale LLMs
introduces significant efficiency challenges, primarily high computational
costs during fine-tuning and inference due to their massive parameterization.
Knowledge distillation (KD) offers a promising solution, transferring expertise
from large teacher models to compact student models. This study systematically
investigates knowledge distillation approaches for bundle generation, aiming to
minimize computational demands while preserving performance. We explore three
critical research questions: (1) how does the format of KD impact bundle
generation performance? (2) to what extent does the quantity of distilled
knowledge influence performance? and (3) how do different ways of utilizing the
distilled knowledge affect performance? We propose a comprehensive KD framework
that (i) progressively extracts knowledge (patterns, rules, deep thoughts);
(ii) captures varying quantities of distilled knowledge through different
strategies; and (iii) exploits complementary LLM adaptation techniques
(in-context learning, supervised fine-tuning, combination) to leverage
distilled knowledge in small student models for domain-specific adaptation and
enhanced efficiency. Extensive experiments provide valuable insights into how
knowledge format, quantity, and utilization methodologies collectively shape
LLM-based bundle generation performance, exhibiting KD's significant potential
for more efficient yet effective LLM-based bundle generation.

</details>

### [95] [Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues](https://arxiv.org/abs/2504.17238)
*Jinfeng Zhou,Yuxuan Chen,Jianing Yin,Yongkang Huang,Yihan Shi,Xikun Zhang,Libiao Peng,Rongsheng Zhang,Tangjie Lv,Zhipeng Hu,Hongning Wang,Minlie Huang*

Main category: cs.CL

TLDR: CRDial是一个新框架，通过多轮对话实现认知重构（CR），结合支持性对话策略和多通道循环机制，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床医生短缺和心理健康污名化促使开发人机交互心理治疗工具，但现有方法未能有效实现CR。

Method: 提出CRDial框架，设计多轮对话阶段，整合支持性策略和多通道循环机制，并基于LLM生成数据集Crisp，训练对话模型Crispers。

Result: Crispers在点对点、成对和干预评估中表现优异。

Conclusion: CRDial和Crispers为CR提供了高效解决方案，优于现有方法。

Abstract: Cognitive Restructuring (CR) is a psychotherapeutic process aimed at
identifying and restructuring an individual's negative thoughts, arising from
mental health challenges, into more helpful and positive ones via multi-turn
dialogues. Clinician shortage and stigma urge the development of human-LLM
interactive psychotherapy for CR. Yet, existing efforts implement CR via simple
text rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to
align with the psychotherapeutic process for effective CR. To address this gap,
we propose CRDial, a novel framework for CR, which creates multi-turn dialogues
with specifically designed identification and restructuring stages of negative
thoughts, integrates sentence-level supportive conversation strategies, and
adopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we
distill Crisp, a large-scale and high-quality bilingual dialogue dataset, from
LLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and
14B scales. Extensive human studies show the superiority of Crispers in
pointwise, pairwise, and intervention evaluations.

</details>

### [96] [Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo](https://arxiv.org/abs/2504.17252)
*Ocheme Anthony Ekle,Biswarup Das*

Main category: cs.CL

TLDR: 该研究开发了基于神经机器翻译（NMT）和Transformer的迁移学习模型，用于英语到伊博语的翻译，通过RNN架构和注意力机制提升翻译准确性，并结合迁移学习进一步优化性能。


<details>
  <summary>Details</summary>
Motivation: 伊博语是一种低资源非洲语言，使用人数超过4000万，但缺乏高质量的翻译工具。研究旨在填补这一空白。

Method: 使用RNN架构（LSTM和GRU）结合注意力机制，并利用MarianNMT预训练模型进行迁移学习。

Result: RNN系统表现接近现有基准，迁移学习使性能提升4.83 BLEU点，翻译准确率估计达70%。

Conclusion: 结合RNN和迁移学习能有效提升低资源语言翻译任务的性能。

Abstract: In this study, we develop Neural Machine Translation (NMT) and
Transformer-based transfer learning models for English-to-Igbo translation - a
low-resource African language spoken by over 40 million people across Nigeria
and West Africa. Our models are trained on a curated and benchmarked dataset
compiled from Bible corpora, local news, Wikipedia articles, and Common Crawl,
all verified by native language experts. We leverage Recurrent Neural Network
(RNN) architectures, including Long Short-Term Memory (LSTM) and Gated
Recurrent Units (GRU), enhanced with attention mechanisms to improve
translation accuracy. To further enhance performance, we apply transfer
learning using MarianNMT pre-trained models within the SimpleTransformers
framework. Our RNN-based system achieves competitive results, closely matching
existing English-Igbo benchmarks. With transfer learning, we observe a
performance gain of +4.83 BLEU points, reaching an estimated translation
accuracy of 70%. These findings highlight the effectiveness of combining RNNs
with transfer learning to address the performance gap in low-resource language
translation tasks.

</details>

### [97] [JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning](https://arxiv.org/abs/2504.17264)
*Zhaolu Kang,Hongtian Cai,Xiangyang Ji,Jinzhe Li,Nanfei Gu*

Main category: cs.CL

TLDR: JurisCTC是一种新型模型，用于改进法律判决预测任务，通过对比学习实现跨法律领域的知识迁移，显著提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 解决法律文本复杂且标注数据有限的问题，探索无监督领域适应在法律领域的应用。

Method: 提出JurisCTC模型，利用对比学习区分不同法律领域的样本，实现民事与刑事法律领域的知识迁移。

Result: JurisCTC在准确率上表现优异，分别达到76.59%和78.83%。

Conclusion: JurisCTC为跨法律领域的知识迁移提供了有效解决方案，显著提升了法律判决预测的性能。

Abstract: In recent years, Unsupervised Domain Adaptation (UDA) has gained significant
attention in the field of Natural Language Processing (NLP) owing to its
ability to enhance model generalization across diverse domains. However, its
application for knowledge transfer between distinct legal domains remains
largely unexplored. To address the challenges posed by lengthy and complex
legal texts and the limited availability of large-scale annotated datasets, we
propose JurisCTC, a novel model designed to improve the accuracy of Legal
Judgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC
facilitates effective knowledge transfer across various legal domains and
employs contrastive learning to distinguish samples from different domains.
Specifically, for the LJP task, we enable knowledge transfer between civil and
criminal law domains. Compared to other models and specific large language
models (LLMs), JurisCTC demonstrates notable advancements, achieving peak
accuracies of 76.59% and 78.83%, respectively.

</details>

### [98] [Evaluating and Mitigating Bias in AI-Based Medical Text Generation](https://arxiv.org/abs/2504.17279)
*Xiuying Chen,Tairan Wang,Juexiao Zhou,Zirui Song,Xin Gao,Xiangliang Zhang*

Main category: cs.CL

TLDR: 研究探讨了医疗领域文本生成中的公平性问题，提出了一种选择性优化算法以减少偏见，并在不牺牲整体性能的情况下显著提升了公平性。


<details>
  <summary>Details</summary>
Motivation: AI系统在医疗应用中表现出色，但可能放大人类偏见，尤其在历史弱势群体中表现不佳。公平性问题在文本生成领域尚未充分研究。

Method: 提出了一种选择性优化算法，综合考虑词级准确性和病理准确性，确保过程可微分以有效训练模型。

Result: 算法在多骨干、数据集和模态中验证，公平性提升30%以上，文本生成准确性变化通常在2%以内。

Conclusion: 该方法有效减少了文本生成中的偏见，提升了医疗诊断的公平性和可靠性。

Abstract: Artificial intelligence (AI) systems, particularly those based on deep
learning models, have increasingly achieved expert-level performance in medical
applications. However, there is growing concern that such AI systems may
reflect and amplify human bias, and reduce the quality of their performance in
historically under-served populations. The fairness issue has attracted
considerable research interest in the medical imaging classification field, yet
it remains understudied in the text generation domain. In this study, we
investigate the fairness problem in text generation within the medical field
and observe significant performance discrepancies across different races,
sexes, and age groups, including intersectional groups, various model scales,
and different evaluation metrics. To mitigate this fairness issue, we propose
an algorithm that selectively optimizes those underperformed groups to reduce
bias. The selection rules take into account not only word-level accuracy but
also the pathology accuracy to the target reference, while ensuring that the
entire process remains fully differentiable for effective model training. Our
evaluations across multiple backbones, datasets, and modalities demonstrate
that our proposed algorithm enhances fairness in text generation without
compromising overall performance. Specifically, the disparities among various
groups across different metrics were diminished by more than 30% with our
algorithm, while the relative change in text generation accuracy was typically
within 2%. By reducing the bias generated by deep learning models, our proposed
approach can potentially alleviate concerns about the fairness and reliability
of text generation diagnosis in medical domain.
  Our code is publicly available to facilitate further research at
https://github.com/iriscxy/GenFair.

</details>

### [99] [CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality](https://arxiv.org/abs/2504.17309)
*Junyan Zhang,Shuliang Liu,Aiwei Liu,Yubo Gao,Jungang Li,Xiaojie Gu,Xuming Hu*

Main category: cs.CL

TLDR: CoheMark是一种先进的句子级水印技术，通过利用句子间的连贯关系提升逻辑流畅性，同时保持高文本质量和强水印检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有句子级水印技术依赖随机分割或生成过程，限制了适用性并影响文本质量，需要一种平衡高质量文本和强水印检测的方法。

Method: CoheMark采用模糊C均值聚类选择句子，并应用特定下一句选择标准，以增强逻辑连贯性。

Result: 实验表明，CoheMark在保持文本质量的同时实现了强水印强度。

Conclusion: CoheMark有效解决了句子级水印技术中文本质量与水印强度之间的平衡问题。

Abstract: Watermarking technology is a method used to trace the usage of content
generated by large language models. Sentence-level watermarking aids in
preserving the semantic integrity within individual sentences while maintaining
greater robustness. However, many existing sentence-level watermarking
techniques depend on arbitrary segmentation or generation processes to embed
watermarks, which can limit the availability of appropriate sentences. This
limitation, in turn, compromises the quality of the generated response. To
address the challenge of balancing high text quality with robust watermark
detection, we propose CoheMark, an advanced sentence-level watermarking
technique that exploits the cohesive relationships between sentences for better
logical fluency. The core methodology of CoheMark involves selecting sentences
through trained fuzzy c-means clustering and applying specific next sentence
selection criteria. Experimental evaluations demonstrate that CoheMark achieves
strong watermark strength while exerting minimal impact on text quality.

</details>

### [100] [FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation](https://arxiv.org/abs/2504.17311)
*Yulia Otmakhova,Hung Thinh Truong,Rahmad Mahendra,Zenan Zhai,Rongxin Zhu,Daniel Beck,Jey Han Lau*

Main category: cs.CL

TLDR: FLUKE是一个任务无关的框架，通过系统化的最小数据变化评估模型鲁棒性，覆盖从拼写到方言的语言层次，结合LLM和人工验证生成修改。实验表明，语言变化影响因任务而异，LLM整体鲁棒性更强但仍脆弱，所有模型对否定修改普遍脆弱。


<details>
  <summary>Details</summary>
Motivation: 现有模型鲁棒性评估缺乏系统性和任务无关性，FLUKE旨在填补这一空白。

Method: FLUKE通过控制语言层次（拼写、方言等）生成测试数据变化，结合LLM和人工验证，评估模型在四个NLP任务中的表现。

Result: 语言变化影响任务依赖性强；LLM整体鲁棒性优于微调模型，但对某些变化仍脆弱；所有模型对否定修改普遍脆弱。

Conclusion: 系统化鲁棒性测试对理解模型行为至关重要，FLUKE为此提供了有效工具。

Abstract: We present FLUKE (Framework for LingUistically-driven and tasK-agnostic
robustness Evaluation), a task-agnostic framework for assessing model
robustness through systematic minimal variations of test data. FLUKE introduces
controlled variations across linguistic levels - from orthography to dialect
and style varieties - and leverages large language models (LLMs) with human
validation to generate modifications. We demonstrate FLUKE's utility by
evaluating both fine-tuned models and LLMs across four diverse NLP tasks, and
reveal that (1) the impact of linguistic variations is highly task-dependent,
with some tests being critical for certain tasks but irrelevant for others; (2)
while LLMs have better overall robustness compared to fine-tuned models, they
still exhibit significant brittleness to certain linguistic variations; (3) all
models show substantial vulnerability to negation modifications across most
tasks. These findings highlight the importance of systematic robustness testing
for understanding model behaviors.

</details>

### [101] [Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection](https://arxiv.org/abs/2504.17332)
*Zihan Wang,Lu Yuan,Zhengxuan Zhang,Qing Zhao*

Main category: cs.CL

TLDR: 提出了一种结合认知与情感共情的双方面共情框架（DAE），用于更全面地检测社交媒体中的虚假信息。


<details>
  <summary>Details</summary>
Motivation: 传统虚假信息检测方法忽视人类共情在传播中的作用，DAE填补了这一空白。

Method: DAE整合认知与情感共情，从创作者和读者视角分析虚假信息，并引入共情感知过滤机制。

Result: 实验表明DAE在基准数据集上优于现有方法。

Conclusion: DAE为多模态虚假信息检测提供了新范式。

Abstract: In the digital era, social media has become a major conduit for information
dissemination, yet it also facilitates the rapid spread of misinformation.
Traditional misinformation detection methods primarily focus on surface-level
features, overlooking the crucial roles of human empathy in the propagation
process. To address this gap, we propose the Dual-Aspect Empathy Framework
(DAE), which integrates cognitive and emotional empathy to analyze
misinformation from both the creator and reader perspectives. By examining
creators' cognitive strategies and emotional appeals, as well as simulating
readers' cognitive judgments and emotional responses using Large Language
Models (LLMs), DAE offers a more comprehensive and human-centric approach to
misinformation detection. Moreover, we further introduce an empathy-aware
filtering mechanism to enhance response authenticity and diversity.
Experimental results on benchmark datasets demonstrate that DAE outperforms
existing methods, providing a novel paradigm for multimodal misinformation
detection.

</details>

### [102] [M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction](https://arxiv.org/abs/2504.17353)
*Chengguang Gan,Sunbowen Lee,Zhixi Cai,Yanbin Wei,Lei Zheng,Yunhao Liang,Shiwen Ni,Tatsunori Mori*

Main category: cs.CL

TLDR: 本文首次将互增强效应（MRE）扩展到多模态信息提取领域，提出多模态互增强效应（M-MRE）任务，并构建相应数据集。通过提出的提示格式适配器（PFA），实验证明MRE在多模态场景中同样有效。


<details>
  <summary>Details</summary>
Motivation: 探索MRE在多模态领域的适用性，填补视觉和多模态信息提取中的研究空白。

Method: 提出多模态互增强效应（M-MRE）任务，构建数据集，并设计提示格式适配器（PFA）以适配大型视觉语言模型（LVLMs）。

Result: 实验证明MRE在多模态任务中同样有效，支持三个相关任务的互增强。

Conclusion: MRE在多模态领域具有通用性，为跨任务互增强提供了新思路。

Abstract: Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection
of information extraction and model interpretability. MRE aims to leverage the
mutual understanding between tasks of different granularities, enhancing the
performance of both coarse-grained and fine-grained tasks through joint
modeling. While MRE has been explored and validated in the textual domain, its
applicability to visual and multimodal domains remains unexplored. In this
work, we extend MRE to the multimodal information extraction domain for the
first time. Specifically, we introduce a new task: Multimodal Mutual
Reinforcement Effect (M-MRE), and construct a corresponding dataset to support
this task. To address the challenges posed by M-MRE, we further propose a
Prompt Format Adapter (PFA) that is fully compatible with various Large
Vision-Language Models (LVLMs). Experimental results demonstrate that MRE can
also be observed in the M-MRE task, a multimodal text-image understanding
scenario. This provides strong evidence that MRE facilitates mutual gains
across three interrelated tasks, confirming its generalizability beyond the
textual domain.

</details>

### [103] [PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare](https://arxiv.org/abs/2504.17360)
*Jose G. Moreno,Jesus Lovon,M'Rick Robin-Charlet,Christine Damase-Michel,Lynda Tamine*

Main category: cs.CL

TLDR: PatientDx框架通过模型合并技术，无需微调患者数据，即可提升LLMs在健康预测任务中的性能，同时避免数据隐私问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在敏感医疗领域微调时面临的数据隐私问题。

Method: 基于模型合并技术，优化构建块合并策略，使用适应数值推理的枢纽模型，并通过性能指标调整超参数。

Result: 在MIMIC-IV数据集上，AUROC提升7%，且相比微调模型更不易出现数据泄露问题。

Conclusion: PatientDx在保持性能的同时有效保护数据隐私，适用于医疗领域。

Abstract: Fine-tuning of Large Language Models (LLMs) has become the default practice
for improving model performance on a given task. However, performance
improvement comes at the cost of training on vast amounts of annotated data
which could be sensitive leading to significant data privacy concerns. In
particular, the healthcare domain is one of the most sensitive domains exposed
to data privacy issues. In this paper, we present PatientDx, a framework of
model merging that allows the design of effective LLMs for health-predictive
tasks without requiring fine-tuning nor adaptation on patient data. Our
proposal is based on recently proposed techniques known as merging of LLMs and
aims to optimize a building block merging strategy. PatientDx uses a pivotal
model adapted to numerical reasoning and tunes hyperparameters on examples
based on a performance metric but without training of the LLM on these data.
Experiments using the mortality tasks of the MIMIC-IV dataset show improvements
up to 7% in terms of AUROC when compared to initial models. Additionally, we
confirm that when compared to fine-tuned models, our proposal is less prone to
data leak problems without hurting performance. Finally, we qualitatively show
the capabilities of our proposal through a case study. Our best model is
publicly available at https://huggingface.co/ Jgmorenof/mistral\_merged\_0\_4.

</details>

### [104] [LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams](https://arxiv.org/abs/2504.17366)
*Yongxuan Wu,Runyu Chen,Peiyu Liu,Hongjin Qian*

Main category: cs.CL

TLDR: 论文构建了一个基于直播的冗余丰富口语长文本数据集，评估了现有方法在长上下文理解中的表现，并提出了一种新基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能反映真实对话的复杂性，限制了大型语言模型在实际场景中的应用。

Method: 构建了首个口语长文本数据集，设计了检索依赖、推理依赖和混合任务，评估了流行LLM和专业方法。

Result: 现有方法在冗余输入上表现不佳，新基线方法在任务中表现优异。

Conclusion: 研究揭示了当前方法的局限性，为改进长上下文理解提供了方向，并填补了评估口语长文本理解的空白。

Abstract: Long-context understanding poses significant challenges in natural language
processing, particularly for real-world dialogues characterized by speech-based
elements, high redundancy, and uneven information density. Although large
language models (LLMs) achieve impressive results on existing benchmarks, these
datasets fail to reflect the complexities of such texts, limiting their
applicability to practical scenarios. To bridge this gap, we construct the
first spoken long-text dataset, derived from live streams, designed to reflect
the redundancy-rich and conversational nature of real-world scenarios. We
construct tasks in three categories: retrieval-dependent, reasoning-dependent,
and hybrid. We then evaluate both popular LLMs and specialized methods to
assess their ability to understand long-contexts in these tasks. Our results
show that current methods exhibit strong task-specific preferences and perform
poorly on highly redundant inputs, with no single method consistently
outperforming others. We propose a new baseline that better handles redundancy
in spoken text and achieves strong performance across tasks. Our findings
highlight key limitations of current methods and suggest future directions for
improving long-context understanding. Finally, our benchmark fills a gap in
evaluating long-context spoken language understanding and provides a practical
foundation for developing real-world e-commerce systems. The code and benchmark
are available at https://github.com/Yarayx/livelongbench.

</details>

### [105] [PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona](https://arxiv.org/abs/2504.17390)
*Jihyun Lee,Yejin Jeon,Seungyeon Seo,Gary Geunbae Lee*

Main category: cs.CL

TLDR: PicPersona-TOD是一个结合用户图像的新数据集，用于生成个性化对话响应，提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有任务导向对话系统常生成通用、单调的响应，缺乏个性化和对用户属性的适应。

Method: 通过用户图像作为人物设定的一部分，结合第一印象、对话策略引导和外部知识，减少幻觉。

Result: 人类评估证实个性化响应提升了用户体验，新模型Pictor在未见领域表现优异。

Conclusion: PicPersona-TOD和Pictor模型显著改善了对话系统的个性化和适应性。

Abstract: Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests
through natural language interactions, yet existing systems often produce
generic, monotonic responses that lack individuality and fail to adapt to
users' personal attributes. To address this, we introduce PicPersona-TOD, a
novel dataset that incorporates user images as part of the persona, enabling
personalized responses tailored to user-specific factors such as age or
emotional context. This is facilitated by first impressions, dialogue
policy-guided prompting, and the use of external knowledge to reduce
hallucinations. Human evaluations confirm that our dataset enhances user
experience, with personalized responses contributing to a more engaging
interaction. Additionally, we introduce a new NLG model, Pictor, which not only
personalizes responses, but also demonstrates robust performance across unseen
domains https://github.com/JihyunLee1/PicPersona.

</details>

### [106] [Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation](https://arxiv.org/abs/2504.17445)
*Anna Lieb,Maneesh Arora,Eni Mustafaraj*

Main category: cs.CL

TLDR: 论文探讨了如何利用LLM生成的文本增强技术提升主题模型的实用性和可解释性，特别是在社会科学研究中。


<details>
  <summary>Details</summary>
Motivation: 主题模型在社会科学研究中存在可解释性和实用性不足的问题，需要改进以更好地服务于特定领域的研究问题。

Method: 使用GPT-4生成的文本增强主题模型，并通过政治学案例研究验证其效果。

Result: 实验表明，GPT-4增强的主题模型能生成高度可解释的类别，适用于特定领域研究问题，且需要较少人工干预。

Conclusion: LLM生成的文本增强技术能显著提升主题模型的实用性和可解释性，为社会科学研究提供了新工具。

Abstract: Unsupervised machine learning techniques, such as topic modeling and
clustering, are often used to identify latent patterns in unstructured text
data in fields such as political science and sociology. These methods overcome
common concerns about reproducibility and costliness involved in the
labor-intensive process of human qualitative analysis. However, two major
limitations of topic models are their interpretability and their practicality
for answering targeted, domain-specific social science research questions. In
this work, we investigate opportunities for using LLM-generated text
augmentation to improve the usefulness of topic modeling output. We use a
political science case study to evaluate our results in a domain-specific
application, and find that topic modeling using GPT-4 augmentations creates
highly interpretable categories that can be used to investigate domain-specific
research questions with minimal human guidance.

</details>

### [107] [Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation](https://arxiv.org/abs/2504.17480)
*Xin Yi,Shunfan Zhengc,Linlin Wanga,Xiaoling Wang,Liang He*

Main category: cs.CL

TLDR: 论文提出了一种名为CDG-KD的统一框架，用于在未经授权的知识蒸馏下进行双向攻击（去除水印和伪造水印），并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 水印在大型语言模型（LLMs）中用于对抗错误信息和保护知识产权，但其在未经授权的知识蒸馏中的鲁棒性和不可伪造性尚未充分研究。

Method: 采用对比解码从学生模型中提取被破坏或放大的水印文本，并通过双向蒸馏训练新的学生模型，分别实现水印去除和伪造。

Result: 实验表明，CDG-KD能有效执行攻击，同时保持蒸馏模型的通用性能。

Conclusion: 研究强调了开发鲁棒且不可伪造的水印方案的迫切需求。

Abstract: Watermarking has emerged as a critical technique for combating misinformation
and protecting intellectual property in large language models (LLMs). A recent
discovery, termed watermark radioactivity, reveals that watermarks embedded in
teacher models can be inherited by student models through knowledge
distillation. On the positive side, this inheritance allows for the detection
of unauthorized knowledge distillation by identifying watermark traces in
student models. However, the robustness of watermarks against scrubbing attacks
and their unforgeability in the face of spoofing attacks under unauthorized
knowledge distillation remain largely unexplored. Existing watermark attack
methods either assume access to model internals or fail to simultaneously
support both scrubbing and spoofing attacks. In this work, we propose
Contrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified
framework that enables bidirectional attacks under unauthorized knowledge
distillation. Our approach employs contrastive decoding to extract corrupted or
amplified watermark texts via comparing outputs from the student model and
weakly watermarked references, followed by bidirectional distillation to train
new student models capable of watermark removal and watermark forgery,
respectively. Extensive experiments show that CDG-KD effectively performs
attacks while preserving the general performance of the distilled model. Our
findings underscore critical need for developing watermarking schemes that are
robust and unforgeable.

</details>

### [108] [HalluLens: LLM Hallucination Benchmark](https://arxiv.org/abs/2504.17550)
*Yejin Bang,Ziwei Ji,Alan Schelten,Anthony Hartshorn,Tara Fowler,Cheng Zhang,Nicola Cancedda,Pascale Fung*

Main category: cs.CL

TLDR: 论文提出了一个全面的幻觉基准，通过明确分类和动态测试集生成，解决LLM幻觉问题，促进研究一致性。


<details>
  <summary>Details</summary>
Motivation: LLM生成的幻觉内容损害用户信任，阻碍生成式AI系统的采用，亟需解决。

Method: 引入新的外在和内在评估任务，构建清晰的幻觉分类法，动态生成测试集防止数据泄漏。

Result: 建立了清晰的幻觉分类法，提出了新的外在幻觉任务，并分析了现有基准的局限性。

Conclusion: 该工作为LLM幻觉研究提供了统一框架，推动了生成式AI系统的可靠性提升。

Abstract: Large language models (LLMs) often generate responses that deviate from user
input or training data, a phenomenon known as "hallucination." These
hallucinations undermine user trust and hinder the adoption of generative AI
systems. Addressing hallucinations is essential for the advancement of LLMs.
This paper introduces a comprehensive hallucination benchmark, incorporating
both new extrinsic and existing intrinsic evaluation tasks, built upon clear
taxonomy of hallucination. A major challenge in benchmarking hallucinations is
the lack of a unified framework due to inconsistent definitions and
categorizations. We disentangle LLM hallucination from "factuality," proposing
a clear taxonomy that distinguishes between extrinsic and intrinsic
hallucinations, to promote consistency and facilitate research. Extrinsic
hallucinations, where the generated content is not consistent with the training
data, are increasingly important as LLMs evolve. Our benchmark includes dynamic
test set generation to mitigate data leakage and ensure robustness against such
leakage. We also analyze existing benchmarks, highlighting their limitations
and saturation. The work aims to: (1) establish a clear taxonomy of
hallucinations, (2) introduce new extrinsic hallucination tasks, with data that
can be dynamically regenerated to prevent saturation by leakage, (3) provide a
comprehensive analysis of existing benchmarks, distinguishing them from
factuality evaluations.

</details>

### [109] [When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars](https://arxiv.org/abs/2504.17562)
*Rei Higuchi,Ryotaro Kawata,Naoki Nishikawa,Kazusato Oko,Shoichiro Yamaguchi,Sosuke Kobayashi,Seiya Tokui,Kohei Hayashi,Daisuke Okanohara,Taiji Suzuki*

Main category: cs.CL

TLDR: 研究发现，在预训练数据前添加元数据（如URL、域名等）可以提升语言模型在特定下游任务中的性能，但效果取决于下游任务提示是否能推断出潜在语义。


<details>
  <summary>Details</summary>
Motivation: 探索预训练时添加元数据对模型性能的影响，尤其是为何在某些任务中表现提升而在其他任务中表现下降。

Method: 通过人工生成的数据（如概率上下文无关文法生成的数据）研究模型行为，分析元数据对潜在语义推断的影响。

Result: 元数据在上下文足够长时能提升性能，但在上下文信息不足时反而会降低性能。

Conclusion: 元数据的有效性取决于下游任务提示是否能推断出潜在语义，需根据任务特性谨慎使用。

Abstract: The ability to acquire latent semantics is one of the key properties that
determines the performance of language models. One convenient approach to
invoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at
the beginning of texts in the pre-training data, making it easier for the model
to access latent semantics before observing the entire text. Previous studies
have reported that this technique actually improves the performance of trained
models in downstream tasks; however, this improvement has been observed only in
specific downstream tasks, without consistent enhancement in average next-token
prediction loss. To understand this phenomenon, we closely investigate how
prepending metadata during pre-training affects model performance by examining
its behavior using artificial data. Interestingly, we found that this approach
produces both positive and negative effects on the downstream tasks. We
demonstrate that the effectiveness of the approach depends on whether latent
semantics can be inferred from the downstream task's prompt. Specifically,
through investigations using data generated by probabilistic context-free
grammars, we show that training with metadata helps improve model's performance
when the given context is long enough to infer the latent semantics. In
contrast, the technique negatively impacts performance when the context lacks
the necessary information to make an accurate posterior inference.

</details>

### [110] [DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training](https://arxiv.org/abs/2504.17565)
*Xiaoyu Tian,Sitong Zhao,Haotian Wang,Shuaiting Chen,Yiping Peng,Yunjie Ji,Han Zhao,Xiangang Li*

Main category: cs.CL

TLDR: 论文通过构建大规模难度分级的推理数据集，精确选择高质量训练数据，显著提升基础模型的推理能力，并在AIME2024数学推理基准上达到79.2%的通过率。


<details>
  <summary>Details</summary>
Motivation: 当前对基础模型训练过程和数据质量的理解不足，需要深入研究以提升推理能力。

Method: 构建包含约3.34百万个难度分级的查询和约4千万蒸馏响应的数据集，利用通过率和变异系数选择高质量数据，并调整学习率进行训练。

Result: 在AIME2024数学推理基准上达到79.2%的通过率，接近最先进水平。

Conclusion: 通过数据选择和训练优化显著提升推理能力，公开数据集和方法以推动开源长推理LLM的发展。

Abstract: Although large language models (LLMs) have recently achieved remarkable
performance on various complex reasoning benchmarks, the academic community
still lacks an in-depth understanding of base model training processes and data
quality. To address this, we construct a large-scale, difficulty-graded
reasoning dataset containing approximately 3.34 million unique queries of
varying difficulty levels and about 40 million distilled responses generated by
multiple models over several passes. Leveraging pass rate and Coefficient of
Variation (CV), we precisely select the most valuable training data to enhance
reasoning capability. Notably, we observe a training pattern shift, indicating
that reasoning-focused training based on base models requires higher learning
rates for effective training. Using this carefully selected data, we
significantly improve the reasoning capabilities of the base model, achieving a
pass rate of 79.2\% on the AIME2024 mathematical reasoning benchmark. This
result surpasses most current distilled models and closely approaches
state-of-the-art performance. We provide detailed descriptions of our data
processing, difficulty assessment, and training methodology, and have publicly
released all datasets and methods to promote rapid progress in open-source
long-reasoning LLMs. The dataset is available at:
https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M

</details>

### [111] [RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore](https://arxiv.org/abs/2504.17574)
*Zhenkai Qin,Guifang Yang,Dongze Wu*

Main category: cs.CL

TLDR: RAGAT-Mind是一种基于MindSpore深度学习框架的中文谣言检测模型，结合多种技术实现多粒度建模，实验显示其性能优越。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上虚假信息泛滥，谣言检测成为自然语言处理领域的迫切挑战。

Method: 模型整合了TextCNN、双向GRU、多头自注意力机制和双向图卷积网络（BiGCN），用于提取局部语义、学习序列上下文、聚焦全局依赖和表示词共现图结构。

Result: 在Weibo1-Rumor数据集上，模型达到99.2%的准确率和0.9919的macro-F1分数。

Conclusion: 结合分层语言特征和图语义结构的模型在谣言检测中表现出色，具有强泛化能力和可解释性，适用于实际应用。

Abstract: As false information continues to proliferate across social media platforms,
effective rumor detection has emerged as a pressing challenge in natural
language processing. This paper proposes RAGAT-Mind, a multi-granular modeling
approach for Chinese rumor detection, built upon the MindSpore deep learning
framework. The model integrates TextCNN for local semantic extraction,
bidirectional GRU for sequential context learning, Multi-Head Self-Attention
for global dependency focusing, and Bidirectional Graph Convolutional Networks
(BiGCN) for structural representation of word co-occurrence graphs. Experiments
on the Weibo1-Rumor dataset demonstrate that RAGAT-Mind achieves superior
classification performance, attaining 99.2% accuracy and a macro-F1 score of
0.9919. The results validate the effectiveness of combining hierarchical
linguistic features with graph-based semantic structures. Furthermore, the
model exhibits strong generalization and interpretability, highlighting its
practical value for real-world rumor detection applications.

</details>

### [112] [Towards a comprehensive taxonomy of online abusive language informed by machine leaning](https://arxiv.org/abs/2504.17653)
*Samaneh Hosseini Moghaddam,Kelly Lyons,Cheryl Regehr,Vivek Goel,Kaitlyn Regehr*

Main category: cs.CL

TLDR: 本文提出了一种用于区分在线文本中辱骂语言关键特征的分类法，整合了18个多标签数据集的分类系统，形成了包含5个类别和17个维度的层次化分类法。


<details>
  <summary>Details</summary>
Motivation: 在线辱骂语言的泛滥对个人和社区的健康与福祉构成威胁，亟需识别和缓解有害内容的方法。

Method: 采用系统性方法开发分类法，整合18个现有多标签数据集的分类系统，构建层次化和多方面的分类法。

Result: 分类法包含5个类别和17个维度，涵盖辱骂语言的背景、目标、强度、直接性和主题等方面。

Conclusion: 该分类法有助于统一研究、政策和实践中的理解，促进在线辱骂检测和缓解领域的进展。

Abstract: The proliferation of abusive language in online communications has posed
significant risks to the health and wellbeing of individuals and communities.
The growing concern regarding online abuse and its consequences necessitates
methods for identifying and mitigating harmful content and facilitating
continuous monitoring, moderation, and early intervention. This paper presents
a taxonomy for distinguishing key characteristics of abusive language within
online text. Our approach uses a systematic method for taxonomy development,
integrating classification systems of 18 existing multi-label datasets to
capture key characteristics relevant to online abusive language classification.
The resulting taxonomy is hierarchical and faceted, comprising 5 categories and
17 dimensions. It classifies various facets of online abuse, including context,
target, intensity, directness, and theme of abuse. This shared understanding
can lead to more cohesive efforts, facilitate knowledge exchange, and
accelerate progress in the field of online abuse detection and mitigation among
researchers, policy makers, online platform owners, and other stakeholders.

</details>

### [113] [Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics](https://arxiv.org/abs/2504.17665)
*Zena Al-Khalili,Nick Howell,Dietrich Klakow*

Main category: cs.CL

TLDR: 论文探讨了代码辅助LLMs在数学推理任务中的表现，发现现有评估多关注执行正确性，而忽略了对生成程序的深入分析。研究通过手动和自动评估，揭示了LLMs在数学规则应用上的差异及其对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有评估代码辅助LLMs的方法过于关注执行正确性，缺乏对生成程序的深入分析，尤其是数学规则的应用情况。

Method: 研究评估了五种不同LLMs在两个数学数据集上的生成程序，结合手动和自动分析，重点关注数学规则的应用程度。

Result: 结果显示，数学规则的应用程度因LLMs能力和问题难度而异，闭源模型表现更好，开源模型在规则应用上存在不足。MATH500数据集的规则应用率降至一半。

Conclusion: 研究强调需要超越执行准确性的深入评估，以更好地理解代码辅助LLMs在数学领域的能力和局限。

Abstract: Assisting LLMs with code generation improved their performance on
mathematical reasoning tasks. However, the evaluation of code-assisted LLMs is
generally restricted to execution correctness, lacking a rigorous evaluation of
their generated programs. In this work, we bridge this gap by conducting an
in-depth analysis of code-assisted LLMs' generated programs in response to math
reasoning tasks. Our evaluation focuses on the extent to which LLMs ground
their programs to math rules, and how that affects their end performance. For
this purpose, we assess the generations of five different LLMs, on two
different math datasets, both manually and automatically. Our results reveal
that the distribution of grounding depends on LLMs' capabilities and the
difficulty of math problems. Furthermore, mathematical grounding is more
effective for closed-source models, while open-source models fail to employ
math rules in their solutions correctly. On MATH500, the percentage of grounded
programs decreased to half, while the ungrounded generations doubled in
comparison to ASDiv grade-school problems. Our work highlights the need for
in-depth evaluation beyond execution accuracy metrics, toward a better
understanding of code-assisted LLMs' capabilities and limits in the math
domain.

</details>

### [114] [Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction](https://arxiv.org/abs/2504.17671)
*Yuanchang Ye,Weiyan Wen*

Main category: cs.CL

TLDR: 提出了一种基于Split Conformal Prediction（SCP）的框架，用于减少大型视觉语言模型（LVLM）在视觉问答（VQA）任务中的幻觉问题，并通过动态阈值校准和跨模态一致性验证实现不确定性量化。


<details>
  <summary>Details</summary>
Motivation: LVLM在多模态推理中表现优异，但其输出常伴随高置信度的幻觉内容，这在安全关键应用中存在风险。

Method: 采用SCP框架，通过数据分区（校准集和测试集）计算非一致性分数，构建具有统计保证的预测集，并动态调整预测集大小。

Result: 在多个基准测试（如ScienceQA、MMMU）和八种LVLM上验证了SCP的理论保证，且在不同校准-测试分割比例下表现稳定。

Conclusion: 该框架为多模态AI系统提供了可扩展的幻觉检测和不确定性感知决策方案，适用于医疗、自动驾驶等安全敏感领域。

Abstract: This study addresses the critical challenge of hallucination mitigation in
Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks
through a Split Conformal Prediction (SCP) framework. While LVLMs excel in
multi-modal reasoning, their outputs often exhibit hallucinated content with
high confidence, posing risks in safety-critical applications. We propose a
model-agnostic uncertainty quantification method that integrates dynamic
threshold calibration and cross-modal consistency verification. By partitioning
data into calibration and test sets, the framework computes nonconformity
scores to construct prediction sets with statistical guarantees under
user-defined risk levels ($\alpha$). Key innovations include: (1) rigorous
control of \textbf{marginal coverage} to ensure empirical error rates remain
strictly below $\alpha$; (2) dynamic adjustment of prediction set sizes
inversely with $\alpha$, filtering low-confidence outputs; (3) elimination of
prior distribution assumptions and retraining requirements. Evaluations on
benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces
theoretical guarantees across all $\alpha$ values. The framework achieves
stable performance across varying calibration-to-test split ratios,
underscoring its robustness for real-world deployment in healthcare, autonomous
systems, and other safety-sensitive domains. This work bridges the gap between
theoretical reliability and practical applicability in multi-modal AI systems,
offering a scalable solution for hallucination detection and uncertainty-aware
decision-making.

</details>

### [115] [Energy Considerations of Large Language Model Inference and Efficiency Optimizations](https://arxiv.org/abs/2504.17674)
*Jared Fernandez,Clara Na,Vashisth Tiwari,Yonatan Bisk,Sasha Luccioni,Emma Strubell*

Main category: cs.CL

TLDR: 本文系统分析了大型语言模型（LLM）推理效率优化对能源消耗的影响，提出了一种建模方法，并揭示了优化策略可显著降低能源使用。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模和应用的增加，其计算和环境成本持续上升，但现有研究多关注理想化场景的延迟减少，忽略了实际工作负载对能源的影响。

Method: 通过输入输出令牌分布的分箱策略和批量大小变化，模拟真实LLM工作流程，并分析软件框架、解码策略、GPU架构等多种因素。

Result: 研究发现，推理优化的效果高度依赖于工作负载、软件栈和硬件，优化策略可减少高达73%的能源消耗。

Conclusion: 研究为可持续LLM部署提供了基础，并为未来AI基础设施的节能设计策略提供了参考。

Abstract: As large language models (LLMs) scale in size and adoption, their
computational and environmental costs continue to rise. Prior benchmarking
efforts have primarily focused on latency reduction in idealized settings,
often overlooking the diverse real-world inference workloads that shape energy
use. In this work, we systematically analyze the energy implications of common
inference efficiency optimizations across diverse Natural Language Processing
(NLP) and generative Artificial Intelligence (AI) workloads, including
conversational AI and code generation. We introduce a modeling approach that
approximates real-world LLM workflows through a binning strategy for
input-output token distributions and batch size variations. Our empirical
analysis spans software frameworks, decoding strategies, GPU architectures,
online and offline serving settings, and model parallelism configurations. We
show that the effectiveness of inference optimizations is highly sensitive to
workload geometry, software stack, and hardware accelerators, demonstrating
that naive energy estimates based on FLOPs or theoretical GPU utilization
significantly underestimate real-world energy consumption. Our findings reveal
that the proper application of relevant inference efficiency optimizations can
reduce total energy use by up to 73% from unoptimized baselines. These insights
provide a foundation for sustainable LLM deployment and inform energy-efficient
design strategies for future AI infrastructure.

</details>

### [116] [Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks](https://arxiv.org/abs/2504.17685)
*Haru-Tada Sato,Fuka Matsuzaki,Jun-ichiro Takahashi*

Main category: cs.CL

TLDR: 本研究提出了一种名为Ensemble Bayesian Inference (EBI)的新方法，通过贝叶斯估计结合多个小型语言模型(SLM)的预测，使其性能超越单个模型，并在多语言任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过SLM集成达到与专有大型语言模型(LLMs)相当的准确性，同时降低计算资源需求。

Method: 提出EBI方法，利用贝叶斯估计结合多个SLM的预测，并分析包含负Lift值模型对整体性能的影响。

Result: 实验表明EBI在多语言任务中表现优异，甚至能利用低性能模型提升整体表现。

Conclusion: EBI为资源有限的高性能AI系统构建提供了新思路，并展示了低性能模型的有效利用潜力。

Abstract: This study explores the potential of small language model(SLM) ensembles to
achieve accuracy comparable to proprietary large language models (LLMs). We
propose Ensemble Bayesian Inference (EBI), a novel approach that applies
Bayesian estimation to combine judgments from multiple SLMs, allowing them to
exceed the performance limitations of individual models. Our experiments on
diverse tasks(aptitude assessments and consumer profile analysis in both
Japanese and English) demonstrate EBI's effectiveness. Notably, we analyze
cases where incorporating models with negative Lift values into ensembles
improves overall performance, and we examine the method's efficacy across
different languages. These findings suggest new possibilities for constructing
high-performance AI systems with limited computational resources and for
effectively utilizing models with individually lower performance. Building on
existing research on LLM performance evaluation, ensemble methods, and
open-source LLM utilization, we discuss the novelty and significance of our
approach.

</details>

### [117] [Safety in Large Reasoning Models: A Survey](https://arxiv.org/abs/2504.17704)
*Cheng Wang,Yue Liu,Baolong Li,Duzhen Zhang,Zhongzhi Li,Junfeng Fang*

Main category: cs.CL

TLDR: 本文综述了大推理模型（LRMs）的安全性风险、攻击方式和防御策略，旨在为未来研究提供清晰的结构化理解。


<details>
  <summary>Details</summary>
Motivation: 随着LRMs在数学和编码等任务中的强大表现，其安全漏洞和风险成为实际应用中的挑战。

Method: 通过详细分类法，系统梳理了新兴的安全风险、攻击和防御策略。

Result: 提出了一个清晰的结构化框架，帮助理解LRMs的安全现状。

Conclusion: 该工作为未来提升LRMs安全性和可靠性的研究奠定了基础。

Abstract: Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks
like mathematics and coding, leveraging their advanced reasoning capabilities.
Nevertheless, as these capabilities progress, significant concerns regarding
their vulnerabilities and safety have arisen, which can pose challenges to
their deployment and application in real-world settings. This paper presents a
comprehensive survey of LRMs, meticulously exploring and summarizing the newly
emerged safety risks, attacks, and defense strategies. By organizing these
elements into a detailed taxonomy, this work aims to offer a clear and
structured understanding of the current safety landscape of LRMs, facilitating
future research and development to enhance the security and reliability of
these powerful models.

</details>

### [118] [Multilingual Performance Biases of Large Language Models in Education](https://arxiv.org/abs/2504.17720)
*Vansh Gupta,Sankalan Pal Chowdhury,Vilém Zouhar,Donya Rooein,Mrinmaya Sachan*

Main category: cs.CL

TLDR: 论文研究了大型语言模型（LLMs）在非英语教育任务中的表现，发现其性能与训练数据中的语言资源量相关，建议部署前验证目标语言的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs主要为英语设计，研究其在非英语教育任务中的适用性。

Method: 评估了流行LLMs在六种语言（除英语外）的四种教育任务中的表现。

Result: 性能与训练数据中的语言资源量相关，非英语语言表现显著低于英语。

Conclusion: 建议在部署前验证LLMs在目标语言中的表现。

Abstract: Large language models (LLMs) are increasingly being adopted in educational
settings. These applications expand beyond English, though current LLMs remain
primarily English-centric. In this work, we ascertain if their use in education
settings in non-English languages is warranted. We evaluated the performance of
popular LLMs on four educational tasks: identifying student misconceptions,
providing targeted feedback, interactive tutoring, and grading translations in
six languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to
English. We find that the performance on these tasks somewhat corresponds to
the amount of language represented in training data, with lower-resource
languages having poorer task performance. Although the models perform
reasonably well in most languages, the frequent performance drop from English
is significant. Thus, we recommend that practitioners first verify that the LLM
works well in the target language for their educational task before deployment.

</details>

### [119] [Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT](https://arxiv.org/abs/2504.17753)
*Anuja Tayal,Devika Salunke,Barbara Di Eugenio,Paula Allen-Meares,Eulalia Puig Abril,Olga Garcia,Carolyn Dickens,Andrew Boyd*

Main category: cs.CL

TLDR: 比较两种对话助手（神经符号架构与ChatGPT）在心力衰竭患者食物盐分查询中的表现，发现前者更准确、高效，后者语言错误更少，患者无偏好。


<details>
  <summary>Details</summary>
Motivation: 评估生成式AI与传统架构在医疗对话系统中的优缺点，为实际应用提供依据。

Method: 采用组内用户研究，对比神经符号架构与ChatGPT版本的对话助手。

Result: 神经符号系统更准确、任务完成率高且简洁，ChatGPT版本语言错误少、澄清需求低。患者无偏好。

Conclusion: 两种架构各有优劣，需根据具体需求选择；患者接受度无差异。

Abstract: Conversational assistants are becoming more and more popular, including in
healthcare, partly because of the availability and capabilities of Large
Language Models. There is a need for controlled, probing evaluations with real
stakeholders which can highlight advantages and disadvantages of more
traditional architectures and those based on generative AI. We present a
within-group user study to compare two versions of a conversational assistant
that allows heart failure patients to ask about salt content in food. One
version of the system was developed in-house with a neurosymbolic architecture,
and one is based on ChatGPT. The evaluation shows that the in-house system is
more accurate, completes more tasks and is less verbose than the one based on
ChatGPT; on the other hand, the one based on ChatGPT makes fewer speech errors
and requires fewer clarifications to complete the task. Patients show no
preference for one over the other.

</details>

### [120] [The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs](https://arxiv.org/abs/2504.17768)
*Piotr Nawrot,Robert Li,Renjie Huang,Sebastian Ruder,Kelly Marchisio,Edoardo M. Ponti*

Main category: cs.CL

TLDR: 本文研究了稀疏注意力在Transformer LLMs中的长上下文能力扩展潜力，通过实验比较不同模型规模、序列长度和稀疏度下的表现，发现稀疏注意力虽有效但需权衡效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 探索稀疏注意力在扩展Transformer LLMs长上下文能力中的可行性、效率-准确性权衡及系统性扩展研究。

Method: 在不同模型规模、序列长度和稀疏度下，对训练无关的稀疏注意力方法进行实验比较，并引入新的长序列任务。

Result: 1）长序列下，大而稀疏的模型优于小而密集的模型；2）解码阶段的稀疏度容忍度高于预填充阶段；3）稀疏注意力非通用解决方案，需场景适配；4）验证了稀疏注意力的新缩放规律。

Conclusion: 稀疏注意力是增强Transformer LLMs长序列处理能力的关键工具，但需谨慎评估性能敏感应用的权衡。

Abstract: Sparse attention offers a promising strategy to extend long-context
capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy
trade-offs, and systematic scaling studies remain unexplored. To address this
gap, we perform a careful comparison of training-free sparse attention methods
at varying model scales, sequence lengths, and sparsity levels on a diverse
collection of long-sequence tasks-including novel ones that rely on natural
language while remaining controllable and easy to evaluate. Based on our
experiments, we report a series of key findings: 1) an isoFLOPS analysis
reveals that for very long sequences, larger and highly sparse models are
preferable to smaller and dense ones. 2) The level of sparsity attainable while
statistically guaranteeing accuracy preservation is higher during decoding than
prefilling, and correlates with model size in the former. 3) There is no clear
strategy that performs best across tasks and phases, with different units of
sparsification or budget adaptivity needed for different scenarios. Even
moderate sparsity levels often result in significant performance degradation on
at least one task, highlighting that sparse attention is not a universal
solution. 4) We introduce and validate novel scaling laws specifically tailored
for sparse attention, providing evidence that our findings are likely to hold
true beyond our range of experiments. Through these insights, we demonstrate
that sparse attention is a key tool to enhance the capabilities of Transformer
LLMs for processing longer sequences, but requires careful evaluation of
trade-offs for performance-sensitive applications.

</details>

<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [121] [A Framework for the Assurance of AI-Enabled Systems](https://arxiv.org/abs/2504.16937)
*Ariel S. Kapusta,David Jin,Peter M. Teague,Robert A. Houston,Jonathan B. Elliott,Grace Y. Park,Shelby S. Holdren*

Main category: cs.AI

TLDR: 美国国防部提出基于声明的AI风险管理框架，以加速AI能力部署，同时解决技术、安全和伦理挑战。


<details>
  <summary>Details</summary>
Motivation: 国防部希望加速AI在国防领域的应用，但AI的强大特性带来技术、安全和伦理挑战，可能阻碍其采用。

Method: 提出基于声明的风险管理与保障框架，支持快速部署、严格评估和成功采用AI系统。

Result: 贡献包括AI保障框架流程、相关定义及重要考量，旨在为国防部提供高效机制。

Conclusion: 该框架旨在快速部署有效AI能力，同时避免关键风险并维护利益相关者信任。

Abstract: The United States Department of Defense (DOD) looks to accelerate the
development and deployment of AI capabilities across a wide spectrum of defense
applications to maintain strategic advantages. However, many common features of
AI algorithms that make them powerful, such as capacity for learning,
large-scale data ingestion, and problem-solving, raise new technical, security,
and ethical challenges. These challenges may hinder adoption due to uncertainty
in development, testing, assurance, processes, and requirements.
Trustworthiness through assurance is essential to achieve the expected value
from AI.
  This paper proposes a claims-based framework for risk management and
assurance of AI systems that addresses the competing needs for faster
deployment, successful adoption, and rigorous evaluation. This framework
supports programs across all acquisition pathways provide grounds for
sufficient confidence that an AI-enabled system (AIES) meets its intended
mission goals without introducing unacceptable risks throughout its lifecycle.
The paper's contributions are a framework process for AI assurance, a set of
relevant definitions to enable constructive conversations on the topic of AI
assurance, and a discussion of important considerations in AI assurance. The
framework aims to provide the DOD a robust yet efficient mechanism for swiftly
fielding effective AI capabilities without overlooking critical risks or
undermining stakeholder trust.

</details>

### [122] [Rational Inference in Formal Concept Analysis](https://arxiv.org/abs/2504.16938)
*Lucas Carr,Nicholas Leisegang,Thomas Meyer,Sergei Obiedkov*

Main category: cs.AI

TLDR: 本文提出了一种在形式概念分析（FCA）中应用KLM框架进行可废止推理的方法，并证明其符合非单调推理的原则。


<details>
  <summary>Details</summary>
Motivation: FCA中的传统依赖关系无法处理错误数据或异常情况，非单调推理在FCA中尚未被充分研究。

Method: 在FCA中构建KLM框架的可废止条件语义，保持与原框架一致的非单调推理原则。

Result: 提出的方法在FCA中提供了更具上下文视角的推理能力，相比命题情况能得出更相关的结论。

Conclusion: 该方法不仅与KLM框架一致，还扩展了FCA中的推理能力，适用于更复杂的数据场景。

Abstract: Defeasible conditionals are a form of non-monotonic inference which enable
the expression of statements like "if $\phi$ then normally $\psi$". The KLM
framework defines a semantics for the propositional case of defeasible
conditionals by construction of a preference ordering over possible worlds. The
pattern of reasoning induced by these semantics is characterised by consequence
relations satisfying certain desirable properties of non-monotonic reasoning.
In FCA, implications are used to describe dependencies between attributes.
However, these implications are unsuitable to reason with erroneous data or
data prone to exceptions. Until recently, the topic of non-monotonic inference
in FCA has remained largely uninvestigated. In this paper, we provide a
construction of the KLM framework for defeasible reasoning in FCA and show that
this construction remains faithful to the principle of non-monotonic inference
described in the original framework. We present an additional argument that,
while remaining consistent with the original ideas around non-monotonic
reasoning, the defeasible reasoning we propose in FCA offers a more contextual
view on inference, providing the ability for more relevant conclusions to be
drawn when compared to the propositional case.

</details>

### [123] [A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2504.16939)
*Emre Can Acikgoz,Cheng Qian,Hongru Wang,Vardhan Dongre,Xiusi Chen,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.AI

TLDR: 本文综述了基于大语言模型（LLM）的对话代理的现状、挑战与未来方向，提出了一个分类框架，并指出了关键研究缺口。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM驱动的对话代理的能力与局限，为下一代对话代理的发展提供结构化基础。

Method: 通过三个维度（推理、监控、控制）系统分析对话代理，并提出新的分类法。

Result: 识别了研究缺口，如长期多轮推理、自我进化能力等，并提出了未来研究方向。

Conclusion: 本文为对话代理的研究提供了框架，推动了人工通用智能（AGI）的进展。

Abstract: Recent advances in Large Language Models (LLMs) have propelled conversational
AI from traditional dialogue systems into sophisticated agents capable of
autonomous actions, contextual awareness, and multi-turn interactions with
users. Yet, fundamental questions about their capabilities, limitations, and
paths forward remain open. This survey paper presents a desideratum for
next-generation Conversational Agents - what has been achieved, what challenges
persist, and what must be done for more scalable systems that approach
human-level intelligence. To that end, we systematically analyze LLM-driven
Conversational Agents by organizing their capabilities into three primary
dimensions: (i) Reasoning - logical, systematic thinking inspired by human
intelligence for decision making, (ii) Monitor - encompassing self-awareness
and user interaction monitoring, and (iii) Control - focusing on tool
utilization and policy following. Building upon this, we introduce a novel
taxonomy by classifying recent work on Conversational Agents around our
proposed desideratum. We identify critical research gaps and outline key
directions, including realistic evaluations, long-term multi-turn reasoning
skills, self-evolution capabilities, collaborative and multi-agent task
completion, personalization, and proactivity. This work aims to provide a
structured foundation, highlight existing limitations, and offer insights into
potential future research directions for Conversational Agents, ultimately
advancing progress toward Artificial General Intelligence (AGI). We maintain a
curated repository of papers at:
https://github.com/emrecanacikgoz/awesome-conversational-agents.

</details>

### [124] [A Systematic Approach to Design Real-World Human-in-the-Loop Deep Reinforcement Learning: Salient Features, Challenges and Trade-offs](https://arxiv.org/abs/2504.17006)
*Jalal Arabneydi,Saiful Islam,Srijita Das,Sai Krishna Gottipati,William Duguay,Cloderic Mars,Matthew E. Taylor,Matthew Guzdial,Antoine Fagette,Younes Zerouali*

Main category: cs.AI

TLDR: 本文提出了一种新颖的多层次分层HITL DRL算法，结合了三种学习方式和三种人类输入形式，并通过无人机任务验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着深度强化学习（DRL）的普及，人机协同（HITL）方法有望革新决策问题解决方式，推动人机协作的新机遇。

Method: 提出了一种多层次分层HITL DRL算法，包含自我学习、模仿学习和迁移学习，并整合了奖励、动作和演示三种人类输入形式。

Result: 实验验证了HITL能加速训练并提高性能，人类建议能引导梯度方法并降低方差，且建议量需适中以避免过拟合或欠拟合。

Conclusion: HITL DRL算法在复杂问题中表现出色，人机协作在过载和诱饵攻击等实际场景中发挥了重要作用。

Abstract: With the growing popularity of deep reinforcement learning (DRL),
human-in-the-loop (HITL) approach has the potential to revolutionize the way we
approach decision-making problems and create new opportunities for human-AI
collaboration. In this article, we introduce a novel multi-layered hierarchical
HITL DRL algorithm that comprises three types of learning: self learning,
imitation learning and transfer learning. In addition, we consider three forms
of human inputs: reward, action and demonstration. Furthermore, we discuss main
challenges, trade-offs and advantages of HITL in solving complex problems and
how human information can be integrated in the AI solution systematically. To
verify our technical results, we present a real-world unmanned aerial vehicles
(UAV) problem wherein a number of enemy drones attack a restricted area. The
objective is to design a scalable HITL DRL algorithm for ally drones to
neutralize the enemy drones before they reach the area. To this end, we first
implement our solution using an award-winning open-source HITL software called
Cogment. We then demonstrate several interesting results such as (a) HITL leads
to faster training and higher performance, (b) advice acts as a guiding
direction for gradient methods and lowers variance, and (c) the amount of
advice should neither be too large nor too small to avoid over-training and
under-training. Finally, we illustrate the role of human-AI cooperation in
solving two real-world complex scenarios, i.e., overloaded and decoy attacks.

</details>

### [125] [Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification](https://arxiv.org/abs/2504.17017)
*Balaji Rao,William Eiers,Carlo Lipizzi*

Main category: cs.AI

TLDR: 提出了一种生成形式化证明的框架，结合自然语言生成、LLM生成证明和启发式模块，用于验证代码的正确性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成代码的普及，形式化验证代码属性变得尤为重要，但目前通用的定理证明任务仍未完全解决。

Method: 框架包含三个组件：生成自然语言描述、LLM生成形式化证明、启发式模块构建最终证明。LLM通过两阶段微调（SFT和RL）训练。

Result: 在miniF2F-test基准和Isabelle证明助手中验证了框架的有效性，并设计了AWS S3桶访问策略代码的验证用例。

Conclusion: 该框架为形式化验证提供了新途径，并展示了LLM在定理证明中的潜力。

Abstract: Formally verifying properties of software code has been a highly desirable
task, especially with the emergence of LLM-generated code. In the same vein,
they provide an interesting avenue for the exploration of formal verification
and mechanistic interpretability. Since the introduction of code-specific
models, despite their successes in generating code in Lean4 and Isabelle, the
task of generalized theorem proving still remains far from being fully solved
and will be a benchmark for reasoning capability in LLMs. In this work, we
introduce a framework that generates whole proofs in a formal language to be
used within systems that utilize the power of built-in tactics and
off-the-shelf automated theorem provers. Our framework includes 3 components:
generating natural language statements of the code to be verified, an LLM that
generates formal proofs for the given statement, and a module employing
heuristics for building the final proof. To train the LLM, we employ a 2-stage
fine-tuning process, where we first use SFT-based training to enable the model
to generate syntactically correct Isabelle code and then RL-based training that
encourages the model to generate proofs verified by a theorem prover. We
validate our framework using the miniF2F-test benchmark and the Isabelle proof
assistant and design a use case to verify the correctness of the AWS S3 bucket
access policy code. We also curate a dataset based on the
FVEL\textsubscript{\textnormal{ER}} dataset for future training tasks.

</details>

### [126] [Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments](https://arxiv.org/abs/2504.17087)
*Yuran Li,Jama Hussein Mohamud,Chongren Sun,Di Wu,Benoit Boulet*

Main category: cs.AI

TLDR: 论文提出了一种三阶段元评判选择流程，通过多智能体协作和综合评分标准，提升大型语言模型（LLM）作为评判者的性能，实验结果显示显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注LLM判断与人类偏好的一致性，忽视了人类判断的偏见和错误，且在多响应情况下如何选择合适LLM判断的研究不足。

Method: 提出三阶段流程：1) 与GPT-4和人类专家制定综合评分标准；2) 使用三个高级LLM智能体评分；3) 通过阈值过滤低分判断。

Result: 在JudgeBench数据集上，相比原始判断和单智能体基线，分别提升约15.55%和8.37%。

Conclusion: 研究展示了LLM作为元评判者的潜力，为未来构建LLM作为评判者的强化学习偏好数据集奠定了基础。

Abstract: Large language models (LLMs) are being widely applied across various fields,
but as tasks become more complex, evaluating their responses is increasingly
challenging. Compared to human evaluators, the use of LLMs to support
performance evaluation offers a more efficient alternative. However, most
studies focus mainly on aligning LLMs' judgments with human preferences,
overlooking the existence of biases and mistakes in human judgment.
Furthermore, how to select suitable LLM judgments given multiple potential LLM
responses remains underexplored. To address these two aforementioned issues, we
propose a three-stage meta-judge selection pipeline: 1) developing a
comprehensive rubric with GPT-4 and human experts, 2) using three advanced LLM
agents to score judgments, and 3) applying a threshold to filter out
low-scoring judgments. Compared to methods using a single LLM as both judge and
meta-judge, our pipeline introduces multi-agent collaboration and a more
comprehensive rubric. Experimental results on the JudgeBench dataset show about
15.55\% improvement compared to raw judgments and about 8.37\% improvement over
the single-agent baseline. Our work demonstrates the potential of LLMs as
meta-judges and lays the foundation for future research on constructing
preference datasets for LLM-as-a-judge reinforcement learning.

</details>

### [127] [AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models](https://arxiv.org/abs/2504.17179)
*Mohammad Zarei,Melanie A Jutras,Eliana Evans,Mike Tan,Omid Aaramoon*

Main category: cs.AI

TLDR: 本文提出了一种利用生成和可解释AI技术来理解和解决自动驾驶车辆（AVs）中罕见故障模式（RFMs）的新方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆依赖AI检测物体，但难以识别罕见故障模式（RFMs），即“长尾挑战”。本文旨在通过生成多样化环境图像和自然语言描述，提升AV系统的鲁棒性和可靠性。

Method: 提取目标对象的分割掩码并反转生成环境掩码，结合文本提示输入定制扩散模型。利用Stable Diffusion修复模型和对抗噪声优化生成多样环境图像，以暴露AI系统的漏洞。

Result: 生成了能够逃避对象检测模型的多样化环境图像，并提供了自然语言描述的RFMs，帮助开发者和政策制定者改进AV系统。

Conclusion: 该方法通过生成和解释RFMs，为提升自动驾驶系统的安全性和可靠性提供了有效工具。

Abstract: Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately
detect objects and interpret their surroundings. However, even when trained
using millions of miles of real-world data, AVs are often unable to detect rare
failure modes (RFMs). The problem of RFMs is commonly referred to as the
"long-tail challenge", due to the distribution of data including many instances
that are very rarely seen. In this paper, we present a novel approach that
utilizes advanced generative and explainable AI techniques to aid in
understanding RFMs. Our methods can be used to enhance the robustness and
reliability of AVs when combined with both downstream model training and
testing. We extract segmentation masks for objects of interest (e.g., cars) and
invert them to create environmental masks. These masks, combined with carefully
crafted text prompts, are fed into a custom diffusion model. We leverage the
Stable Diffusion inpainting model guided by adversarial noise optimization to
generate images containing diverse environments designed to evade object
detection models and expose vulnerabilities in AI systems. Finally, we produce
natural language descriptions of the generated RFMs that can guide developers
and policymakers to improve the safety and reliability of AV systems.

</details>

### [128] [Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning](https://arxiv.org/abs/2504.17282)
*Lynn Cherif,Flemming Kondrup,David Venuto,Ankit Anand,Doina Precup,Khimya Khetarpal*

Main category: cs.AI

TLDR: 论文提出了一种名为CoGA的方法，通过意图驱动的动作空间约束和预训练视觉语言模型生成代码，显著提高了在低数据环境下强化学习代理的样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决在稀疏奖励和大动作空间环境（如网页GUI）中，传统方法需要大量专家演示的问题。

Method: 利用预训练视觉语言模型生成代码，通过自动化程序生成和验证流程约束动作空间，结合强化学习代理。

Result: 在MiniWob++基准测试中，CoGA样本效率显著提升，任务间泛化能力强，且在小样本专家演示下表现优于或与行为克隆相当。

Conclusion: CoGA通过意图驱动的动作空间约束和自动化代码生成，有效解决了低数据环境下的样本效率问题。

Abstract: Agents that can autonomously navigate the web through a graphical user
interface (GUI) using a unified action space (e.g., mouse and keyboard actions)
can require very large amounts of domain-specific expert demonstrations to
achieve good performance. Low sample efficiency is often exacerbated in
sparse-reward and large-action-space environments, such as a web GUI, where
only a few actions are relevant in any given situation. In this work, we
consider the low-data regime, with limited or no access to expert behavior. To
enable sample-efficient learning, we explore the effect of constraining the
action space through $\textit{intent-based affordances}$ -- i.e., considering
in any situation only the subset of actions that achieve a desired outcome. We
propose $\textbf{Code as Generative Affordances}$ $(\textbf{$\texttt{CoGA}$})$,
a method that leverages pre-trained vision-language models (VLMs) to generate
code that determines affordable actions through implicit intent-completion
functions and using a fully-automated program generation and verification
pipeline. These programs are then used in-the-loop of a reinforcement learning
agent to return a set of affordances given a pixel observation. By greatly
reducing the number of actions that an agent must consider, we demonstrate on a
wide range of tasks in the MiniWob++ benchmark that: $\textbf{1)}$
$\texttt{CoGA}$ is orders of magnitude more sample efficient than its RL agent,
$\textbf{2)}$ $\texttt{CoGA}$'s programs can generalize within a family of
tasks, and $\textbf{3)}$ $\texttt{CoGA}$ performs better or on par compared
with behavior cloning when a small number of expert demonstrations is
available.

</details>

### [129] [AI-Enhanced Business Process Automation: A Case Study in the Insurance Domain Using Object-Centric Process Mining](https://arxiv.org/abs/2504.17295)
*Shahrzad Khayatbashi,Viktor Sjölind,Anders Granåker,Amin Jalali*

Main category: cs.AI

TLDR: 论文探讨了AI（特别是LLMs）如何通过自动化知识密集型任务推动业务流程重构，并以保险行业为例，展示了OCPM方法在评估AI自动化影响中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究AI自动化对业务流程的影响，尤其是在传统与AI增强流程共存时的过渡阶段，需要数据驱动的方法。

Method: 采用对象中心流程挖掘（OCPM）方法，结合保险行业的实际案例，分析LLM自动化对流程可扩展性的影响。

Result: LLM显著提升了操作能力，但也引入了新的流程动态，需要进一步优化；OCPM在真实场景中展现了其优势和局限性。

Conclusion: 研究证明了OCPM在评估AI自动化影响中的实用性，同时指出LLM虽提升效率，但需进一步优化流程动态。

Abstract: Recent advancements in Artificial Intelligence (AI), particularly Large
Language Models (LLMs), have enhanced organizations' ability to reengineer
business processes by automating knowledge-intensive tasks. This automation
drives digital transformation, often through gradual transitions that improve
process efficiency and effectiveness. To fully assess the impact of such
automation, a data-driven analysis approach is needed - one that examines how
traditional and AI-enhanced process variants coexist during this transition.
Object-Centric Process Mining (OCPM) has emerged as a valuable method that
enables such analysis, yet real-world case studies are still needed to
demonstrate its applicability. This paper presents a case study from the
insurance sector, where an LLM was deployed in production to automate the
identification of claim parts, a task previously performed manually and
identified as a bottleneck for scalability. To evaluate this transformation, we
apply OCPM to assess the impact of AI-driven automation on process scalability.
Our findings indicate that while LLMs significantly enhance operational
capacity, they also introduce new process dynamics that require further
refinement. This study also demonstrates the practical application of OCPM in a
real-world setting, highlighting its advantages and limitations.

</details>

### [130] [Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning](https://arxiv.org/abs/2504.17356)
*Weiliang Zhang,Xiaohan Huang,Yi Du,Ziyue Qiao,Qingqing Long,Zhen Meng,Yuanchun Zhou,Meng Xiao*

Main category: cs.AI

TLDR: 论文提出了一种名为HRLFS的新方法，通过结合大型语言模型（LLM）和分层强化学习（RL）来解决特征选择中的效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的特征选择方法在处理复杂数据集时效率低下，主要原因是每个特征对应一个代理的模式不够高效。

Method: HRLFS利用LLM提取特征的数学和语义信息，聚类特征后构建分层代理，以减少代理数量并提升效率。

Result: 实验表明，HRLFS在提升下游机器学习性能的同时，显著减少了运行时间。

Conclusion: HRLFS通过分层代理和LLM的结合，为复杂数据集的特征选择提供了高效且可扩展的解决方案。

Abstract: Feature selection aims to preprocess the target dataset, find an optimal and
most streamlined feature subset, and enhance the downstream machine learning
task. Among filter, wrapper, and embedded-based approaches, the reinforcement
learning (RL)-based subspace exploration strategy provides a novel objective
optimization-directed perspective and promising performance. Nevertheless, even
with improved performance, current reinforcement learning approaches face
challenges similar to conventional methods when dealing with complex datasets.
These challenges stem from the inefficient paradigm of using one agent per
feature and the inherent complexities present in the datasets. This observation
motivates us to investigate and address the above issue and propose a novel
approach, namely HRLFS. Our methodology initially employs a Large Language
Model (LLM)-based hybrid state extractor to capture each feature's mathematical
and semantic characteristics. Based on this information, features are
clustered, facilitating the construction of hierarchical agents for each
cluster and sub-cluster. Extensive experiments demonstrate the efficiency,
scalability, and robustness of our approach. Compared to contemporary or the
one-feature-one-agent RL-based approaches, HRLFS improves the downstream ML
performance with iterative feature subspace exploration while accelerating
total run time by reducing the number of agents involved.

</details>

### [131] [Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation](https://arxiv.org/abs/2504.17402)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisarkka,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TLDR: LLMs（如DeepSeek和o1-preview）在跨领域本体生成任务中表现一致，展示了其通用性和潜力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在领域特定本体生成任务中的适用性。

Method: 使用两种具备推理能力的LLMs（DeepSeek和o1-preview），基于能力问题（CQs）和用户故事生成本体，并在六个不同领域进行实验。

Result: 实验结果显示，LLMs在所有领域中的表现一致，表明其具备跨领域本体生成的通用能力。

Conclusion: LLMs为可扩展且领域无关的本体构建提供了潜力，为自动化推理和知识表示技术的进一步研究奠定了基础。

Abstract: Large Language Models (LLMs) have shown significant potential for ontology
engineering. However, it is still unclear to what extent they are applicable to
the task of domain-specific ontology generation. In this study, we explore the
application of LLMs for automated ontology generation and evaluate their
performance across different domains. Specifically, we investigate the
generalizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both
equipped with reasoning capabilities, by generating ontologies from a set of
competency questions (CQs) and related user stories. Our experimental setup
comprises six distinct domains carried out in existing ontology engineering
projects and a total of 95 curated CQs designed to test the models' reasoning
for ontology engineering. Our findings show that with both LLMs, the
performance of the experiments is remarkably consistent across all domains,
indicating that these methods are capable of generalizing ontology generation
tasks irrespective of the domain. These results highlight the potential of
LLM-based approaches in achieving scalable and domain-agnostic ontology
construction and lay the groundwork for further research into enhancing
automated reasoning and knowledge representation techniques.

</details>

### [132] [Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society](https://arxiv.org/abs/2504.17404)
*Feifei Zhao,Yuwei Wang,Enmeng Lu,Dongcheng Zhao,Bing Han,Haibo Tong,Yao Liang,Dongqi Liang,Kang Sun,Lei Wang,Yitao Liang,Chao Liu,Yaodong Yang,Yi Zeng*

Main category: cs.AI

TLDR: 论文探讨了超级对齐（superalignment）问题，提出了一种结合外部监督和内在主动对齐的框架，以确保超级智能AI与人类价值观一致。


<details>
  <summary>Details</summary>
Motivation: 随着AI向超级智能（ASI）发展，可能超出人类控制并引发灾难性后果，因此需要解决超级对齐问题，确保ASI与人类意图和价值观一致。

Method: 提出了一种整合外部监督和内在主动对齐的框架。外部监督基于人类决策和自动化评估，内在主动对齐则通过自我认知和同理心实现人类-AI协同对齐。

Result: 通过结合外部监督和内在主动对齐，能够实现可持续的共生社会，为安全且有益的AGI和ASI铺平道路。

Conclusion: 论文强调超级对齐应通过人类与AI的协同对齐实现，为未来超级智能的安全发展提供了新思路。

Abstract: Artificial Intelligence (AI) systems are becoming increasingly powerful and
autonomous, and may progress to surpass human intelligence levels, namely
Artificial Superintelligence (ASI). During the progression from AI to ASI, it
may exceed human control, violate human values, and even lead to irreversible
catastrophic consequences in extreme cases. This gives rise to a pressing issue
that needs to be addressed: superalignment, ensuring that AI systems much
smarter than humans, remain aligned with human (compatible) intentions and
values. Existing scalable oversight and weak-to-strong generalization methods
may prove substantially infeasible and inadequate when facing ASI. We must
explore safer and more pluralistic frameworks and approaches for
superalignment. In this paper, we redefine superalignment as the human-AI
co-alignment towards a sustainable symbiotic society, and highlight a framework
that integrates external oversight and intrinsic proactive alignment. External
oversight superalignment should be grounded in human-centered ultimate
decision, supplemented by interpretable automated evaluation and correction, to
achieve continuous alignment with humanity's evolving values. Intrinsic
proactive superalignment is rooted in a profound understanding of the self,
others, and society, integrating self-awareness, self-reflection, and empathy
to spontaneously infer human intentions, distinguishing good from evil and
proactively considering human well-being, ultimately attaining human-AI
co-alignment through iterative interaction. The integration of
externally-driven oversight with intrinsically-driven proactive alignment
empowers sustainable symbiotic societies through human-AI co-alignment, paving
the way for achieving safe and beneficial AGI and ASI for good, for human, and
for a symbiotic ecology.

</details>

### [133] [Towards Machine-Generated Code for the Resolution of User Intentions](https://arxiv.org/abs/2504.17531)
*Justus Flerlage,Ilja Behnke,Odej Kao*

Main category: cs.AI

TLDR: 论文探讨了利用大型语言模型（LLM）生成代码以实现用户意图的可行性，展示了AI在无GUI操作系统中生成和执行工作流的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力的提升，特别是LLM的发展，用户与设备的交互方式需要重新评估。传统的高层应用方式可能被AI驱动的意图解析所取代。

Method: 研究通过向LLM（GPT-4o-mini）提供具体用户意图和简化API，生成代码工作流，并分析其执行效果。

Result: 结果表明，该方法总体可行，LLM在生成符合用户意图的代码工作流方面表现出色。

Conclusion: AI与人类协作生成工作流是一种有前景的方向，LLM在意图解析和代码生成方面具有显著能力。

Abstract: The growing capabilities of Artificial Intelligence (AI), particularly Large
Language Models (LLMs), prompt a reassessment of the interaction mechanisms
between users and their devices. Currently, users are required to use a set of
high-level applications to achieve their desired results. However, the advent
of AI may signal a shift in this regard, as its capabilities have generated
novel prospects for user-provided intent resolution through the deployment of
model-generated code, which is tantamount to the generation of workflows
comprising a multitude of interdependent steps. This development represents a
significant progression in the realm of hybrid workflows, where human and
artificial intelligence collaborate to address user intentions, with the former
responsible for defining these intentions and the latter for implementing the
solutions to address them. In this paper, we investigate the feasibility of
generating and executing workflows through code generation that results from
prompting an LLM with a concrete user intention, such as \emph{Please send my
car title to my insurance company}, and a simplified application programming
interface for a GUI-less operating system. We provide in-depth analysis and
comparison of various user intentions, the resulting code, and its execution.
The findings demonstrate a general feasibility of our approach and that the
employed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of
code-oriented workflows in accordance with provided user intentions.

</details>

### [134] [Auditing the Ethical Logic of Generative AI Models](https://arxiv.org/abs/2504.17544)
*W. Russell Neuman,Chad Coleman,Ali Dasdan,Safinah Ali,Manan Shah*

Main category: cs.AI

TLDR: 本文提出了一种五维审计模型，用于评估大型语言模型（LLMs）的伦理逻辑，发现模型在伦理决策上趋同，但在解释严谨性和道德优先级上存在差异。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型在高风险领域的应用增加，评估其伦理推理能力的需求日益迫切。

Method: 采用五维审计模型（分析质量、伦理考虑广度、解释深度、一致性和决断力），通过多轮提示方法（包括新颖的伦理困境）评估LLMs的推理能力。

Result: 评估了七种主要LLMs，发现模型在伦理决策上趋同，但解释严谨性和道德优先级差异显著；链式思维提示和优化推理模型显著提升了性能。

Conclusion: 研究提出了一种可扩展的AI伦理评估方法，并展示了AI在复杂决策中辅助人类道德推理的潜力。

Abstract: As generative AI models become increasingly integrated into high-stakes
domains, the need for robust methods to evaluate their ethical reasoning
becomes increasingly important. This paper introduces a five-dimensional audit
model -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth
of Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic
of leading large language models (LLMs). Drawing on traditions from applied
ethics and higher-order thinking, we present a multi-battery prompt approach,
including novel ethical dilemmas, to probe the models' reasoning across diverse
contexts. We benchmark seven major LLMs finding that while models generally
converge on ethical decisions, they vary in explanatory rigor and moral
prioritization. Chain-of-Thought prompting and reasoning-optimized models
significantly enhance performance on our audit metrics. This study introduces a
scalable methodology for ethical benchmarking of AI systems and highlights the
potential for AI to complement human moral reasoning in complex decision-making
contexts.

</details>

<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [135] [Dense Air Pollution Estimation from Sparse in-situ Measurements and Satellite Data](https://arxiv.org/abs/2504.17039)
*Ruben Gonzalez Avilés,Linus Scheibenreif,Damian Borth*

Main category: cs.CV

TLDR: 本文提出了一种新的密集估计技术，用于高效估算全球环境中的氮氧化物（NO2）浓度，解决了现有方法的计算密集性问题，并显著提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有卫星空气质量估算方法在计算大范围区域时存在高计算成本问题，亟需一种高效且准确的替代方案。

Method: 采用均匀随机偏移采样策略，将地面真实数据均匀分散到更大区域，通过密集估计方法一步生成网格估算值。

Result: 新方法在MAE上比现有点式方法提高了9.45%，达到4.98 µg/m³，同时显著降低了计算资源需求。

Conclusion: 该方法为大规模环境监测提供了高效且准确的解决方案，适用于全球范围。

Abstract: This paper addresses the critical environmental challenge of estimating
ambient Nitrogen Dioxide (NO$_2$) concentrations, a key issue in public health
and environmental policy. Existing methods for satellite-based air pollution
estimation model the relationship between satellite and in-situ measurements at
select point locations. While these approaches have advanced our ability to
provide air quality estimations on a global scale, they come with inherent
limitations. The most notable limitation is the computational intensity
required for generating comprehensive estimates over extensive areas. Motivated
by these limitations, this study introduces a novel dense estimation technique.
Our approach seeks to balance the accuracy of high-resolution estimates with
the practicality of computational constraints, thereby enabling efficient and
scalable global environmental assessment. By utilizing a uniformly random
offset sampling strategy, our method disperses the ground truth data pixel
location evenly across a larger patch. At inference, the dense estimation
method can then generate a grid of estimates in a single step, significantly
reducing the computational resources required to provide estimates for larger
areas. Notably, our approach also surpasses the results of existing point-wise
methods by a significant margin of $9.45\%$, achieving a Mean Absolute Error
(MAE) of $4.98\ \mu\text{g}/\text{m}^3$. This demonstrates both high accuracy
and computational efficiency, highlighting the applicability of our method for
global environmental assessment. Furthermore, we showcase the method's
adaptability and robustness by applying it to diverse geographic regions. Our
method offers a viable solution to the computational challenges of large-scale
environmental monitoring.

</details>

### [136] [DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs](https://arxiv.org/abs/2504.17040)
*Zhenhailong Wang,Senthil Purushwalkam,Caiming Xiong,Silvio Savarese,Heng Ji,Ran Xu*

Main category: cs.CV

TLDR: DyMU是一种无需训练的高效框架，动态减少视觉语言模型的计算负担，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉变换器中固定长度输出的低效问题，动态适应图像内容以减少计算成本。

Method: 结合动态令牌合并（DToMe）和虚拟令牌解合并（VTU），动态调整令牌压缩并模拟完整序列的注意力动态。

Result: 实验显示DyMU能将视觉令牌数量减少32%-85%，性能与完整模型相当。

Conclusion: DyMU无需额外训练，适用于多种视觉语言模型架构，并提供用户对计算成本的控制。

Abstract: We present DyMU, an efficient, training-free framework that dynamically
reduces the computational burden of vision-language models (VLMs) while
maintaining high task performance. Our approach comprises two key components.
First, Dynamic Token Merging (DToMe) reduces the number of visual token
embeddings by merging similar tokens based on image complexity, addressing the
inherent inefficiency of fixed-length outputs in vision transformers. Second,
Virtual Token Unmerging (VTU) simulates the expected token sequence for large
language models (LLMs) by efficiently reconstructing the attention dynamics of
a full sequence, thus preserving the downstream performance without additional
fine-tuning. Unlike previous approaches, our method dynamically adapts token
compression to the content of the image and operates completely training-free,
making it readily applicable to most state-of-the-art VLM architectures.
Extensive experiments on image and video understanding tasks demonstrate that
DyMU can reduce the average visual token count by 32%-85% while achieving
comparable performance to full-length models across diverse VLM architectures,
including the recently popularized AnyRes-based visual encoders. Furthermore,
through qualitative analyses, we demonstrate that DToMe effectively adapts
token reduction based on image complexity and, unlike existing systems,
provides users more control over computational costs. Project page:
https://mikewangwzhl.github.io/dymu/.

</details>

### [137] [PPS-Ctrl: Controllable Sim-to-Real Translation for Colonoscopy Depth Estimation](https://arxiv.org/abs/2504.17067)
*Xinqi Xiong,Andrea Dunn Beltran,Jun Myeong Choi,Marc Niethammer,Roni Sengupta*

Main category: cs.CV

TLDR: 提出了一种结合Stable Diffusion和ControlNet的图像翻译框架，利用PPS地图生成更真实的纹理，提升内窥镜深度估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 临床环境中获取真实深度数据困难，合成数据与真实数据存在领域差距，影响深度估计的泛化能力。

Method: 结合Stable Diffusion和ControlNet，以PPS地图的潜在表示为条件，生成保留结构的真实纹理。

Result: 实验表明，该方法生成的图像更真实，深度估计性能优于基于GAN的MI-CycleGAN。

Conclusion: 提出的框架有效缩小了合成与真实数据的领域差距，提升了深度估计的准确性。

Abstract: Accurate depth estimation enhances endoscopy navigation and diagnostics, but
obtaining ground-truth depth in clinical settings is challenging. Synthetic
datasets are often used for training, yet the domain gap limits generalization
to real data. We propose a novel image-to-image translation framework that
preserves structure while generating realistic textures from clinical data. Our
key innovation integrates Stable Diffusion with ControlNet, conditioned on a
latent representation extracted from a Per-Pixel Shading (PPS) map. PPS
captures surface lighting effects, providing a stronger structural constraint
than depth maps. Experiments show our approach produces more realistic
translations and improves depth estimation over GAN-based MI-CycleGAN. Our code
is publicly accessible at https://github.com/anaxqx/PPS-Ctrl.

</details>

### [138] [Distilling semantically aware orders for autoregressive image generation](https://arxiv.org/abs/2504.17069)
*Rishav Pramanik,Antoine Poupon,Juan A. Rodriguez,Masih Aminbeidokhti,David Vazquez,Christopher Pal,Zhaozheng Yin,Marco Pedersoli*

Main category: cs.CV

TLDR: 论文提出了一种改进的自回归图像生成方法，通过任意顺序生成图像块并优化生成顺序，提升了图像质量。


<details>
  <summary>Details</summary>
Motivation: 传统的自回归图像生成采用固定的光栅扫描顺序，忽略了图像内容间的因果关系，导致生成效果不理想。

Method: 首先训练模型以任意顺序生成图像块，推断内容和位置；然后利用提取的顺序微调模型以生成更高质量的图像。

Result: 实验表明，新方法在两种数据集上生成的图像质量优于传统光栅扫描方法，且训练成本相似。

Conclusion: 通过优化生成顺序，自回归图像生成的质量得到显著提升，无需额外标注。

Abstract: Autoregressive patch-based image generation has recently shown competitive
results in terms of image quality and scalability. It can also be easily
integrated and scaled within Vision-Language models. Nevertheless,
autoregressive models require a defined order for patch generation. While a
natural order based on the dictation of the words makes sense for text
generation, there is no inherent generation order that exists for image
generation. Traditionally, a raster-scan order (from top-left to bottom-right)
guides autoregressive image generation models. In this paper, we argue that
this order is suboptimal, as it fails to respect the causality of the image
content: for instance, when conditioned on a visual description of a sunset, an
autoregressive model may generate clouds before the sun, even though the color
of clouds should depend on the color of the sun and not the inverse. In this
work, we show that first by training a model to generate patches in
any-given-order, we can infer both the content and the location (order) of each
patch during generation. Secondly, we use these extracted orders to finetune
the any-given-order model to produce better-quality images. Through our
experiments, we show on two datasets that this new generation method produces
better images than the traditional raster-scan approach, with similar training
costs and no extra annotations.

</details>

### [139] [Scene-Aware Location Modeling for Data Augmentation in Automotive Object Detection](https://arxiv.org/abs/2504.17076)
*Jens Petersen,Davide Abati,Amirhossein Habibian,Auke Wiggers*

Main category: cs.CV

TLDR: 论文提出了一种基于场景感知的概率位置模型，用于生成更真实的图像布局增强数据，显著提升了汽车目标检测和实例分割的性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成图像模型在数据增强中忽视了物体布局的真实性，导致增强效果受限。

Method: 引入场景感知概率位置模型，预测新物体在场景中的合理位置，并结合生成模型进行修复。

Result: 在两项汽车目标检测任务中取得最佳性能，mAP提升高达2.8倍，实例分割也有显著改进。

Conclusion: 布局真实性的增强对生成数据增强至关重要，提出的方法显著优于现有技术。

Abstract: Generative image models are increasingly being used for training data
augmentation in vision tasks. In the context of automotive object detection,
methods usually focus on producing augmented frames that look as realistic as
possible, for example by replacing real objects with generated ones. Others try
to maximize the diversity of augmented frames, for example by pasting lots of
generated objects onto existing backgrounds. Both perspectives pay little
attention to the locations of objects in the scene. Frame layouts are either
reused with little or no modification, or they are random and disregard realism
entirely. In this work, we argue that optimal data augmentation should also
include realistic augmentation of layouts. We introduce a scene-aware
probabilistic location model that predicts where new objects can realistically
be placed in an existing scene. By then inpainting objects in these locations
with a generative model, we obtain much stronger augmentation performance than
existing approaches. We set a new state of the art for generative data
augmentation on two automotive object detection tasks, achieving up to
$2.8\times$ higher gains than the best competing approach ($+1.4$ vs. $+0.5$
mAP boost). We also demonstrate significant improvements for instance
segmentation.

</details>

### [140] [Transferring Spatial Filters via Tangent Space Alignment in Motor Imagery BCIs](https://arxiv.org/abs/2504.17111)
*Tekin Gunasar,Virginia de Sa*

Main category: cs.CV

TLDR: 提出了一种基于黎曼流形对齐协方差矩阵和改进的CSP空间滤波器的方法，以提升运动想象BCI中的跨被试迁移性能。


<details>
  <summary>Details</summary>
Motivation: 解决运动想象BCI中跨被试数据迁移效果不佳的问题，尤其是在训练数据有限时。

Method: 在黎曼流形上对齐协方差矩阵，并设计新的CSP空间滤波器，同时探索多被试信息整合方式。

Result: 在三个数据集上表现略优于标准CSP，但在训练数据有限时改进更显著。

Conclusion: 该方法在数据有限时能显著提升跨被试迁移性能，为BCI应用提供了更稳定的解决方案。

Abstract: We propose a method to improve subject transfer in motor imagery BCIs by
aligning covariance matrices on a Riemannian manifold, followed by computing a
new common spatial patterns (CSP) based spatial filter. We explore various ways
to integrate information from multiple subjects and show improved performance
compared to standard CSP. Across three datasets, our method shows marginal
improvements over standard CSP; however, when training data are limited, the
improvements become more significant.

</details>

### [141] [Latent Video Dataset Distillation](https://arxiv.org/abs/2504.17132)
*Ning Li,Antai Andy Liu,Jingran Zhang,Justin Cui*

Main category: cs.CV

TLDR: 提出了一种新的视频数据集蒸馏方法，在潜在空间中操作，结合多样性感知数据选择和训练无关的压缩技术，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频数据集蒸馏方法主要关注像素空间压缩，忽视了潜在空间的进展，本文旨在填补这一空白。

Method: 使用先进的变分编码器在潜在空间进行蒸馏，结合多样性感知数据选择和训练无关的压缩方法。

Result: 在所有数据集上表现优于现有方法，例如在HMDB51 IPC 1上性能提升2.6%，在MiniUCF IPC 5上提升7.8%。

Conclusion: 提出的方法在视频数据集蒸馏中实现了新的最先进性能。

Abstract: Dataset distillation has demonstrated remarkable effectiveness in
high-compression scenarios for image datasets. While video datasets inherently
contain greater redundancy, existing video dataset distillation methods
primarily focus on compression in the pixel space, overlooking advances in the
latent space that have been widely adopted in modern text-to-image and
text-to-video models. In this work, we bridge this gap by introducing a novel
video dataset distillation approach that operates in the latent space using a
state-of-the-art variational encoder. Furthermore, we employ a diversity-aware
data selection strategy to select both representative and diverse samples.
Additionally, we introduce a simple, training-free method to further compress
the distilled latent dataset. By combining these techniques, our approach
achieves a new state-of-the-art performance in dataset distillation,
outperforming prior methods on all datasets, e.g. on HMDB51 IPC 1, we achieve a
2.6% performance increase; on MiniUCF IPC 5, we achieve a 7.8% performance
increase.

</details>

### [142] [A Comprehensive Review on RNA Subcellular Localization Prediction](https://arxiv.org/abs/2504.17162)
*Cece Zhang,Xuehuan Zhu,Nick Peterson,Jieqiong Wang,Shibiao Wan*

Main category: cs.CV

TLDR: 论文综述了基于AI/ML的RNA亚细胞定位预测方法的最新进展，涵盖多种RNA类型，并讨论了挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 传统湿实验室方法耗时耗力，计算方法的出现为RNA亚细胞定位研究提供了高效替代方案。

Method: 综述了基于序列、图像及混合方法的AI/ML技术，用于预测RNA亚细胞定位。

Result: AI/ML方法能加速RNA研究，揭示分子通路，指导疾病治疗。

Conclusion: 论文为RNA亚细胞定位研究提供了资源，并指出数据稀缺和缺乏基准等挑战。

Abstract: The subcellular localization of RNAs, including long non-coding RNAs
(lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs,
plays a critical role in determining their biological functions. For instance,
lncRNAs are predominantly associated with chromatin and act as regulators of
gene transcription and chromatin structure, while mRNAs are distributed across
the nucleus and cytoplasm, facilitating the transport of genetic information
for protein synthesis. Understanding RNA localization sheds light on processes
like gene expression regulation with spatial and temporal precision. However,
traditional wet lab methods for determining RNA localization, such as in situ
hybridization, are often time-consuming, resource-demanding, and costly. To
overcome these challenges, computational methods leveraging artificial
intelligence (AI) and machine learning (ML) have emerged as powerful
alternatives, enabling large-scale prediction of RNA subcellular localization.
This paper provides a comprehensive review of the latest advancements in
AI-based approaches for RNA subcellular localization prediction, covering
various RNA types and focusing on sequence-based, image-based, and hybrid
methodologies that combine both data types. We highlight the potential of these
methods to accelerate RNA research, uncover molecular pathways, and guide
targeted disease treatments. Furthermore, we critically discuss the challenges
in AI/ML approaches for RNA subcellular localization, such as data scarcity and
lack of benchmarks, and opportunities to address them. This review aims to
serve as a valuable resource for researchers seeking to develop innovative
solutions in the field of RNA subcellular localization and beyond.

</details>

### [143] [PhysioSync: Temporal and Cross-Modal Contrastive Learning Inspired by Physiological Synchronization for EEG-Based Emotion Recognition](https://arxiv.org/abs/2504.17163)
*Kai Cui,Jia Li,Yu Liu,Xuesong Zhang,Zhenzhen Hu,Meng Wang*

Main category: cs.CV

TLDR: PhysioSync是一个新型预训练框架，通过跨模态和时间对比学习解决EEG信号噪声和个体差异问题，提升情绪识别性能。


<details>
  <summary>Details</summary>
Motivation: EEG信号虽能反映情绪状态，但易受噪声和个体差异影响，且现有多模态方法未充分探索动态同步和语义一致性。

Method: 提出PhysioSync框架，结合跨模态一致性对齐（CM-CA）和长短时时间对比学习（LS-TCL），预训练后通过特征融合优化情绪识别。

Result: 在DEAP和DREAMER数据集上，PhysioSync在单模态和跨模态条件下均表现出优越性能。

Conclusion: PhysioSync通过跨模态和时间对比学习有效提升了EEG为中心的情绪识别效果。

Abstract: Electroencephalography (EEG) signals provide a promising and involuntary
reflection of brain activity related to emotional states, offering significant
advantages over behavioral cues like facial expressions. However, EEG signals
are often noisy, affected by artifacts, and vary across individuals,
complicating emotion recognition. While multimodal approaches have used
Peripheral Physiological Signals (PPS) like GSR to complement EEG, they often
overlook the dynamic synchronization and consistent semantics between the
modalities. Additionally, the temporal dynamics of emotional fluctuations
across different time resolutions in PPS remain underexplored. To address these
challenges, we propose PhysioSync, a novel pre-training framework leveraging
temporal and cross-modal contrastive learning, inspired by physiological
synchronization phenomena. PhysioSync incorporates Cross-Modal Consistency
Alignment (CM-CA) to model dynamic relationships between EEG and complementary
PPS, enabling emotion-related synchronizations across modalities. Besides, it
introduces Long- and Short-Term Temporal Contrastive Learning (LS-TCL) to
capture emotional synchronization at different temporal resolutions within
modalities. After pre-training, cross-resolution and cross-modal features are
hierarchically fused and fine-tuned to enhance emotion recognition. Experiments
on DEAP and DREAMER datasets demonstrate PhysioSync's advanced performance
under uni-modal and cross-modal conditions, highlighting its effectiveness for
EEG-centered emotion recognition.

</details>

### [144] [A Genealogy of Multi-Sensor Foundation Models in Remote Sensing](https://arxiv.org/abs/2504.17177)
*Kevin Lane,Morteza Karimzadeh*

Main category: cs.CV

TLDR: 本文综述了遥感领域中基础模型的现状，探讨了其与计算机视觉领域的联系、优势与不足，并提出了未来改进方向，特别是多传感器数据的利用。


<details>
  <summary>Details</summary>
Motivation: 遥感领域的基础模型发展尚不成熟，存在多种竞争性方法，各有优缺点。本文旨在分析这些方法及其在计算机视觉中的根源，以指导未来遥感专用基础模型的改进。

Method: 通过比较遥感与计算机视觉领域的基础模型方法，分析其表征学习质量、计算资源需求及多传感器数据利用情况。

Result: 总结了现有方法的优势与不足，强调了多传感器数据的重要性，并提出了改进方向。

Conclusion: 未来研究应更充分利用未标记、季节性和多传感器遥感数据，以优化基础模型的性能。

Abstract: Foundation models have garnered increasing attention for representation
learning in remote sensing, primarily adopting approaches that have
demonstrated success in computer vision with minimal domain-specific
modification. However, the development and application of foundation models in
this field are still burgeoning, as there are a variety of competing approaches
that each come with significant benefits and drawbacks. This paper examines
these approaches along with their roots in the computer vision field in order
to characterize potential advantages and pitfalls while outlining future
directions to further improve remote sensing-specific foundation models. We
discuss the quality of the learned representations and methods to alleviate the
need for massive compute resources. We place emphasis on the multi-sensor
aspect of Earth observations, and the extent to which existing approaches
leverage multiple sensors in training foundation models in relation to
multi-modal foundation models. Finally, we identify opportunities for further
harnessing the vast amounts of unlabeled, seasonal, and multi-sensor remote
sensing observations.

</details>

### [145] [We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback](https://arxiv.org/abs/2504.17180)
*Minkyu Choi,S P Sharan,Harsh Goel,Sahil Shah,Sandeep Chinchali*

Main category: cs.CV

TLDR: 论文提出了一种无需训练的零训练视频优化方法，通过神经符号反馈提升文本到视频生成模型的语义和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成模型在处理复杂提示时难以保持语义和时间一致性，且直接改进模型计算成本高。

Method: 提出一种零训练视频优化流程，利用神经符号反馈分析视频表示并定位不一致的事件和对象，指导针对性编辑。

Result: 实验表明该方法显著提升了时间与逻辑对齐效果，改进幅度近40%。

Conclusion: 该方法有效解决了复杂提示下视频生成的语义和时间一致性问题，且无需额外训练。

Abstract: Current text-to-video (T2V) generation models are increasingly popular due to
their ability to produce coherent videos from textual prompts. However, these
models often struggle to generate semantically and temporally consistent videos
when dealing with longer, more complex prompts involving multiple objects or
sequential events. Additionally, the high computational costs associated with
training or fine-tuning make direct improvements impractical. To overcome these
limitations, we introduce \(\projectname\), a novel zero-training video
refinement pipeline that leverages neuro-symbolic feedback to automatically
enhance video generation, achieving superior alignment with the prompts. Our
approach first derives the neuro-symbolic feedback by analyzing a formal video
representation and pinpoints semantically inconsistent events, objects, and
their corresponding frames. This feedback then guides targeted edits to the
original video. Extensive empirical evaluations on both open-source and
proprietary T2V models demonstrate that \(\projectname\) significantly enhances
temporal and logical alignment across diverse prompts by almost $40\%$.

</details>

### [146] [Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation](https://arxiv.org/abs/2504.17207)
*Phillip Y. Lee,Jihyeon Je,Chanho Park,Mikaela Angelina Uy,Leonidas Guibas,Minhyuk Sung*

Main category: cs.CV

TLDR: 论文提出了一种名为APC的框架，通过心理意象模拟提升视觉语言模型（VLMs）的视角感知推理能力，解决了现有模型在视角转换上的不足。


<details>
  <summary>Details</summary>
Motivation: 视角感知是人类视觉理解的关键能力，但现有VLMs在此方面表现不足，存在自我中心偏差。通过模拟人类心理意象，论文旨在缩小VLMs与人类感知的差距。

Method: 提出Abstract Perspective Change (APC)框架，利用视觉基础模型（如目标检测、分割和方向估计）构建场景抽象并实现视角转换。

Result: 在合成和真实图像基准测试中，APC显著提升了视角感知推理能力，优于微调的空间推理模型和新视角合成方法。

Conclusion: APC框架通过心理意象模拟有效增强了VLMs的视角感知能力，为未来研究提供了新方向。

Abstract: We present a framework for perspective-aware reasoning in vision-language
models (VLMs) through mental imagery simulation. Perspective-taking, the
ability to perceive an environment or situation from an alternative viewpoint,
is a key benchmark for human-level visual understanding, essential for
environmental interaction and collaboration with autonomous agents. Despite
advancements in spatial reasoning within VLMs, recent research has shown that
modern VLMs significantly lack perspective-aware reasoning capabilities and
exhibit a strong bias toward egocentric interpretations. To bridge the gap
between VLMs and human perception, we focus on the role of mental imagery,
where humans perceive the world through abstracted representations that
facilitate perspective shifts. Motivated by this, we propose a framework for
perspective-aware reasoning, named Abstract Perspective Change (APC), that
effectively leverages vision foundation models, such as object detection,
segmentation, and orientation estimation, to construct scene abstractions and
enable perspective transformations. Our experiments on synthetic and real-image
benchmarks, compared with various VLMs, demonstrate significant improvements in
perspective-aware reasoning with our framework, further outperforming
fine-tuned spatial reasoning models and novel-view-synthesis-based approaches.

</details>

### [147] [MCAF: Efficient Agent-based Video Understanding Framework through Multimodal Coarse-to-Fine Attention Focusing](https://arxiv.org/abs/2504.17213)
*Shiwen Cao,Zhaoxing Zhang,Junming Jiao,Juyi Qiao,Guowen Song,Rong Shen*

Main category: cs.CV

TLDR: MCAF是一种基于代理的无训练框架，通过多模态粗到细的注意力聚焦实现视频理解，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 视频理解，尤其是长视频，信息冗余且复杂，需要模型全局策略性地分配注意力。

Method: MCAF通过多模态信息分层聚焦相关帧，并采用扩张时间扩展机制避免遗漏关键细节，结合自反馈机制迭代优化注意力分配。

Result: 在多个数据集上表现优异，如EgoSchema提升5%，Next-QA和IntentQA分别提升0.2%和0.3%，在长视频数据集Video-MME上也优于其他方法。

Conclusion: MCAF通过创新的注意力聚焦策略，显著提升了视频理解的准确性和效率。

Abstract: Even in the era of rapid advances in large models, video understanding,
particularly long videos, remains highly challenging. Compared with textual or
image-based information, videos commonly contain more information with
redundancy, requiring large models to strategically allocate attention at a
global level for accurate comprehension. To address this, we propose MCAF, an
agent-based, training-free framework perform video understanding through
Multimodal Coarse-to-fine Attention Focusing. The key innovation lies in its
ability to sense and prioritize segments of the video that are highly relevant
to the understanding task. First, MCAF hierarchically concentrates on highly
relevant frames through multimodal information, enhancing the correlation
between the acquired contextual information and the query. Second, it employs a
dilated temporal expansion mechanism to mitigate the risk of missing crucial
details when extracting information from these concentrated frames. In
addition, our framework incorporates a self-reflection mechanism utilizing the
confidence level of the model's responses as feedback. By iteratively applying
these two creative focusing strategies, it adaptively adjusts attention to
capture highly query-connected context and thus improves response accuracy.
MCAF outperforms comparable state-of-the-art methods on average. On the
EgoSchema dataset, it achieves a remarkable 5% performance gain over the
leading approach. Meanwhile, on Next-QA and IntentQA datasets, it outperforms
the current state-of-the-art standard by 0.2% and 0.3% respectively. On the
Video-MME dataset, which features videos averaging nearly an hour in length,
MCAF also outperforms other agent-based methods.

</details>

### [148] [Towards Generalizable Deepfake Detection with Spatial-Frequency Collaborative Learning and Hierarchical Cross-Modal Fusion](https://arxiv.org/abs/2504.17223)
*Mengyu Qiao,Runze Tian,Yang Wang*

Main category: cs.CV

TLDR: 提出了一种结合多尺度空间-频率分析的新型深度伪造检测框架，显著提升了检测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法主要依赖空间域分析，频率域操作仅用于特征增强，未能充分利用频率原生伪影和空间-频率交互。

Method: 框架包括局部频谱特征提取、全局频谱特征提取和多阶段跨模态融合机制。

Result: 在广泛采用的基准测试中，该方法在准确性和泛化性上优于现有技术。

Conclusion: 该框架通过多尺度空间-频率分析有效解决了深度伪造检测中的挑战，具有广泛应用潜力。

Abstract: The rapid evolution of deep generative models poses a critical challenge to
deepfake detection, as detectors trained on forgery-specific artifacts often
suffer significant performance degradation when encountering unseen forgeries.
While existing methods predominantly rely on spatial domain analysis, frequency
domain operations are primarily limited to feature-level augmentation, leaving
frequency-native artifacts and spatial-frequency interactions insufficiently
exploited. To address this limitation, we propose a novel detection framework
that integrates multi-scale spatial-frequency analysis for universal deepfake
detection. Our framework comprises three key components: (1) a local spectral
feature extraction pipeline that combines block-wise discrete cosine transform
with cascaded multi-scale convolutions to capture subtle spectral artifacts;
(2) a global spectral feature extraction pipeline utilizing scale-invariant
differential accumulation to identify holistic forgery distribution patterns;
and (3) a multi-stage cross-modal fusion mechanism that incorporates
shallow-layer attention enhancement and deep-layer dynamic modulation to model
spatial-frequency interactions. Extensive evaluations on widely adopted
benchmarks demonstrate that our method outperforms state-of-the-art deepfake
detection methods in both accuracy and generalizability.

</details>

### [149] [Visual and textual prompts for enhancing emotion recognition in video](https://arxiv.org/abs/2504.17224)
*Zhifeng Wang,Qixuan Zhang,Peter Zhang,Wenjia Niu,Kaihao Zhang,Ramesh Sankaranarayana,Sabrina Caldwell,Tom Gedeon*

Main category: cs.CV

TLDR: 提出了一种名为SoVTP的新框架，通过整合空间标注、生理信号和上下文线索，提升VLLMs在视频情感识别中的零样本能力。


<details>
  <summary>Details</summary>
Motivation: 现有的VLLMs在视频情感识别中因空间和上下文意识不足而受限，传统方法忽视非语言线索，导致鲁棒性下降。

Method: SoVTP框架结合空间标注（如边界框、面部标志）、生理信号（面部动作单元）和上下文线索（身体姿势、场景动态等），形成统一的提示策略。

Result: 实验表明，SoVTP在视觉提示方法中表现显著优于现有方法，增强了VLLMs的视频情感识别能力。

Conclusion: SoVTP通过保留整体场景信息并实现细粒度分析，有效提升了VLLMs在视频情感识别中的性能。

Abstract: Vision Large Language Models (VLLMs) exhibit promising potential for
multi-modal understanding, yet their application to video-based emotion
recognition remains limited by insufficient spatial and contextual awareness.
Traditional approaches, which prioritize isolated facial features, often
neglect critical non-verbal cues such as body language, environmental context,
and social interactions, leading to reduced robustness in real-world scenarios.
To address this gap, we propose Set-of-Vision-Text Prompting (SoVTP), a novel
framework that enhances zero-shot emotion recognition by integrating spatial
annotations (e.g., bounding boxes, facial landmarks), physiological signals
(facial action units), and contextual cues (body posture, scene dynamics,
others' emotions) into a unified prompting strategy. SoVTP preserves holistic
scene information while enabling fine-grained analysis of facial muscle
movements and interpersonal dynamics. Extensive experiments show that SoVTP
achieves substantial improvements over existing visual prompting methods,
demonstrating its effectiveness in enhancing VLLMs' video emotion recognition
capabilities.

</details>

### [150] [STCL:Curriculum learning Strategies for deep learning image steganography models](https://arxiv.org/abs/2504.17609)
*Fengchun Liu,Tong Zhang,Chunying Zhang*

Main category: cs.CV

TLDR: 提出了一种基于课程学习的图像隐写训练策略（STCL），通过逐步增加训练难度提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习图像隐写模型中图像质量差和网络收敛慢的问题。

Method: 使用基于教师模型的难度评估策略和基于拐点的训练调度策略，逐步从简单到复杂图像训练。

Result: 在多个数据集上验证，模型性能提升，隐写图像质量高且分析得分低。

Conclusion: STCL策略有效提升了图像隐写模型的性能和质量。

Abstract: Aiming at the problems of poor quality of steganographic images and slow
network convergence of image steganography models based on deep learning, this
paper proposes a Steganography Curriculum Learning training strategy (STCL) for
deep learning image steganography models. So that only easy images are selected
for training when the model has poor fitting ability at the initial stage, and
gradually expand to more difficult images, the strategy includes a difficulty
evaluation strategy based on the teacher model and an knee point-based training
scheduling strategy. Firstly, multiple teacher models are trained, and the
consistency of the quality of steganographic images under multiple teacher
models is used as the difficulty score to construct the training subsets from
easy to difficult. Secondly, a training control strategy based on knee points
is proposed to reduce the possibility of overfitting on small training sets and
accelerate the training process. Experimental results on three large public
datasets, ALASKA2, VOC2012 and ImageNet, show that the proposed image
steganography scheme is able to improve the model performance under multiple
algorithmic frameworks, which not only has a high PSNR, SSIM score, and
decoding accuracy, but also the steganographic images generated by the model
under the training of the STCL strategy have a low steganography analysis
scores. You can find our code at
\href{https://github.com/chaos-boops/STCL}{https://github.com/chaos-boops/STCL}.

</details>

### [151] [Range Image-Based Implicit Neural Compression for LiDAR Point Clouds](https://arxiv.org/abs/2504.17229)
*Akihiro Kuwabara,Sorachi Kato,Takuya Fujihashi,Toshiaki Koike-Akino,Takashi Watanabe*

Main category: cs.CV

TLDR: 本文提出了一种基于隐式神经表示（INR）的新型LiDAR点云压缩方案，通过深度和掩码图像的分割与压缩，显著提升了低比特率下的3D重建和检测质量。


<details>
  <summary>Details</summary>
Motivation: 传统图像压缩技术在处理2D范围图像（RIs）时效率有限，因其与自然图像在比特精度和像素值分布上存在差异，需要更高效的压缩方法。

Method: 将RIs分割为深度和掩码图像，分别采用基于INR的块级和像素级架构，结合模型剪枝和量化技术进行压缩。

Result: 在KITTI数据集上的实验表明，该方法在低比特率和解码延迟下，优于现有的图像、点云、RI和INR压缩方法。

Conclusion: 提出的INR-based RI压缩方法为高效3D场景存档提供了新思路，显著提升了压缩效率和重建质量。

Abstract: This paper presents a novel scheme to efficiently compress Light Detection
and Ranging~(LiDAR) point clouds, enabling high-precision 3D scene archives,
and such archives pave the way for a detailed understanding of the
corresponding 3D scenes. We focus on 2D range images~(RIs) as a lightweight
format for representing 3D LiDAR observations. Although conventional image
compression techniques can be adapted to improve compression efficiency for
RIs, their practical performance is expected to be limited due to differences
in bit precision and the distinct pixel value distribution characteristics
between natural images and RIs. We propose a novel implicit neural
representation~(INR)--based RI compression method that effectively handles
floating-point valued pixels. The proposed method divides RIs into depth and
mask images and compresses them using patch-wise and pixel-wise INR
architectures with model pruning and quantization, respectively. Experiments on
the KITTI dataset show that the proposed method outperforms existing image,
point cloud, RI, and INR-based compression methods in terms of 3D
reconstruction and detection quality at low bitrates and decoding latency.

</details>

### [152] [Scene Perceived Image Perceptual Score (SPIPS): combining global and local perception for image quality assessment](https://arxiv.org/abs/2504.17234)
*Zhiqiang Lao,Heather Yu*

Main category: cs.CV

TLDR: 提出了一种结合深度学习与传统方法的图像质量评估（IQA）新方法，通过分离高层语义和低层感知特征，更准确地反映人类视觉感知。


<details>
  <summary>Details</summary>
Motivation: 随着AI和智能手机的普及，图像数据激增，传统IQA方法在处理DNN生成的图像时表现不足，需要更符合人类感知的评估方法。

Method: 将深度特征分解为高层语义和低层感知细节，结合传统IQA指标，通过多层感知机（MLP）生成质量评分。

Result: 实验表明，该方法比现有IQA模型更符合人类感知判断。

Conclusion: 提出的混合方法在图像质量评估中更全面，能更好地模拟人类视觉过程。

Abstract: The rapid advancement of artificial intelligence and widespread use of
smartphones have resulted in an exponential growth of image data, both real
(camera-captured) and virtual (AI-generated). This surge underscores the
critical need for robust image quality assessment (IQA) methods that accurately
reflect human visual perception. Traditional IQA techniques primarily rely on
spatial features - such as signal-to-noise ratio, local structural distortions,
and texture inconsistencies - to identify artifacts. While effective for
unprocessed or conventionally altered images, these methods fall short in the
context of modern image post-processing powered by deep neural networks (DNNs).
The rise of DNN-based models for image generation, enhancement, and restoration
has significantly improved visual quality, yet made accurate assessment
increasingly complex. To address this, we propose a novel IQA approach that
bridges the gap between deep learning methods and human perception. Our model
disentangles deep features into high-level semantic information and low-level
perceptual details, treating each stream separately. These features are then
combined with conventional IQA metrics to provide a more comprehensive
evaluation framework. This hybrid design enables the model to assess both
global context and intricate image details, better reflecting the human visual
process, which first interprets overall structure before attending to
fine-grained elements. The final stage employs a multilayer perceptron (MLP) to
map the integrated features into a concise quality score. Experimental results
demonstrate that our method achieves improved consistency with human perceptual
judgments compared to existing IQA models.

</details>

### [153] [DIVE: Inverting Conditional Diffusion Models for Discriminative Tasks](https://arxiv.org/abs/2504.17253)
*Yinqi Li,Hong Chang,Ruibing Hou,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TLDR: 本文提出了一种利用预训练扩散模型进行判别性任务（如目标检测）的方法，通过反转布局到图像的扩散模型，并结合梯度优化和先验分布模型，实现了与基础判别性方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用预训练扩散模型的生成能力完成判别性任务，扩展其应用范围。

Method: 通过反转预训练的布局到图像扩散模型，提出梯度离散优化和先验分布模型，以简化预测枚举过程并提高贝叶斯规则的准确性。

Result: 在COCO数据集上，该方法与基础判别性目标检测基线相当，同时显著提升了扩散分类方法的速度而不损失精度。

Conclusion: 该方法成功将扩散模型应用于判别性任务，展示了其在目标检测中的潜力，并提供了高效实现。

Abstract: Diffusion models have shown remarkable progress in various generative tasks
such as image and video generation. This paper studies the problem of
leveraging pretrained diffusion models for performing discriminative tasks.
Specifically, we extend the discriminative capability of pretrained frozen
generative diffusion models from the classification task to the more complex
object detection task, by "inverting" a pretrained layout-to-image diffusion
model. To this end, a gradient-based discrete optimization approach for
replacing the heavy prediction enumeration process, and a prior distribution
model for making more accurate use of the Bayes' rule, are proposed
respectively. Empirical results show that this method is on par with basic
discriminative object detection baselines on COCO dataset. In addition, our
method can greatly speed up the previous diffusion-based method for
classification without sacrificing accuracy. Code and models are available at
https://github.com/LiYinqi/DIVE .

</details>

### [154] [Precision Neural Network Quantization via Learnable Adaptive Modules](https://arxiv.org/abs/2504.17263)
*Wenqiang Zhou,Zhendong Yu,Xinyu Liu,Jiaming Yang,Rong Xiao,Tao Wang,Chenwei Tang,Jiancheng Lv*

Main category: cs.CV

TLDR: ASQ是一种自适应神经网络量化方法，通过动态调整量化参数和非均匀量化方案，显著提升了量化模型的性能，甚至在某些情况下优于全精度模型。


<details>
  <summary>Details</summary>
Motivation: 解决量化感知训练（QAT）中量化参数固定导致的灵活性不足问题，尤其是处理分布差异大的激活值时。

Method: 提出ASQ方法，动态调整量化缩放因子，并采用基于POST的非均匀量化方案，结合LUT保持计算效率。

Result: ASQ在实验中表现优于现有QAT方法，4位量化的ResNet34在ImageNet上准确率提升1.2%。

Conclusion: ASQ通过自适应量化策略，有效平衡了模型压缩与性能，为神经网络量化提供了新思路。

Abstract: Quantization Aware Training (QAT) is a neural network quantization technique
that compresses model size and improves operational efficiency while
effectively maintaining model performance. The paradigm of QAT is to introduce
fake quantization operators during the training process, allowing the model to
autonomously compensate for information loss caused by quantization. Making
quantization parameters trainable can significantly improve the performance of
QAT, but at the cost of compromising the flexibility during inference,
especially when dealing with activation values with substantially different
distributions. In this paper, we propose an effective learnable adaptive neural
network quantization method, called Adaptive Step Size Quantization (ASQ), to
resolve this conflict. Specifically, the proposed ASQ method first dynamically
adjusts quantization scaling factors through a trained module capable of
accommodating different activations. Then, to address the rigid resolution
issue inherent in Power of Two (POT) quantization, we propose an efficient
non-uniform quantization scheme. We utilize the Power Of Square root of Two
(POST) as the basis for exponential quantization, effectively handling the
bell-shaped distribution of neural network weights across various bit-widths
while maintaining computational efficiency through a Look-Up Table method
(LUT). Extensive experimental results demonstrate that the proposed ASQ method
is superior to the state-of-the-art QAT approaches. Notably that the ASQ is
even competitive compared to full precision baselines, with its 4-bit quantized
ResNet34 model improving accuracy by 1.2\% on ImageNet.

</details>

### [155] [Towards Generalized and Training-Free Text-Guided Semantic Manipulation](https://arxiv.org/abs/2504.17269)
*Yu Hong,Xiao Cai,Pengpeng Zeng,Shuai Zhang,Jingkuan Song,Lianli Gao,Heng Tao Shen*

Main category: cs.CV

TLDR: 论文提出了一种名为GTF的新方法，用于文本引导的语义图像编辑，支持多种语义操作且无需训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法效率低、扩展性差且通用性有限，而扩散模型中噪声的几何特性与语义变化密切相关。

Method: 通过控制噪声的几何关系实现多种语义操作（如添加、移除和风格迁移），无需调优或优化。

Result: 实验证明GTF能高效生成高保真结果，且支持跨模态任务。

Conclusion: GTF在语义操作领域具有潜力，可推动技术前沿。

Abstract: Text-guided semantic manipulation refers to semantically editing an image
generated from a source prompt to match a target prompt, enabling the desired
semantic changes (e.g., addition, removal, and style transfer) while preserving
irrelevant contents. With the powerful generative capabilities of the diffusion
model, the task has shown the potential to generate high-fidelity visual
content. Nevertheless, existing methods either typically require time-consuming
fine-tuning (inefficient), fail to accomplish multiple semantic manipulations
(poorly extensible), and/or lack support for different modality tasks (limited
generalizability). Upon further investigation, we find that the geometric
properties of noises in the diffusion model are strongly correlated with the
semantic changes. Motivated by this, we propose a novel $\textit{GTF}$ for
text-guided semantic manipulation, which has the following attractive
capabilities: 1) $\textbf{Generalized}$: our $\textit{GTF}$ supports multiple
semantic manipulations (e.g., addition, removal, and style transfer) and can be
seamlessly integrated into all diffusion-based methods (i.e., Plug-and-play)
across different modalities (i.e., modality-agnostic); and 2)
$\textbf{Training-free}$: $\textit{GTF}$ produces high-fidelity results via
simply controlling the geometric relationship between noises without tuning or
optimization. Our extensive experiments demonstrate the efficacy of our
approach, highlighting its potential to advance the state-of-the-art in
semantics manipulation.

</details>

### [156] [EdgePoint2: Compact Descriptors for Superior Efficiency and Accuracy](https://arxiv.org/abs/2504.17280)
*Haodi Yao,Fenghua He,Ning Hao,Chen Xie*

Main category: cs.CV

TLDR: EdgePoint2是一种轻量级关键点检测与描述神经网络，专为边缘计算设计，兼顾高效与准确性，提供多种子模型适应不同需求。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习关键点提取方法计算成本高，难以部署于实时边缘应用，且高维描述符不利于分布式应用。

Method: 优化网络架构，结合正交Procrustes损失和相似性损失训练紧凑描述符，提供14个子模型。

Result: 在多种场景下实现SOTA准确性与效率，支持低维描述符（32/48/64）。

Conclusion: EdgePoint2在灵活性、鲁棒性和多功能性上表现优异，适用于多样化计算与通信约束的视觉任务。

Abstract: The field of keypoint extraction, which is essential for vision applications
like Structure from Motion (SfM) and Simultaneous Localization and Mapping
(SLAM), has evolved from relying on handcrafted methods to leveraging deep
learning techniques. While deep learning approaches have significantly improved
performance, they often incur substantial computational costs, limiting their
deployment in real-time edge applications. Efforts to create lightweight neural
networks have seen some success, yet they often result in trade-offs between
efficiency and accuracy. Additionally, the high-dimensional descriptors
generated by these networks poses challenges for distributed applications
requiring efficient communication and coordination, highlighting the need for
compact yet competitively accurate descriptors. In this paper, we present
EdgePoint2, a series of lightweight keypoint detection and description neural
networks specifically tailored for edge computing applications on embedded
system. The network architecture is optimized for efficiency without
sacrificing accuracy. To train compact descriptors, we introduce a combination
of Orthogonal Procrustes loss and similarity loss, which can serve as a general
approach for hypersphere embedding distillation tasks. Additionally, we offer
14 sub-models to satisfy diverse application requirements. Our experiments
demonstrate that EdgePoint2 consistently achieves state-of-the-art (SOTA)
accuracy and efficiency across various challenging scenarios while employing
lower-dimensional descriptors (32/48/64). Beyond its accuracy, EdgePoint2
offers significant advantages in flexibility, robustness, and versatility.
Consequently, EdgePoint2 emerges as a highly competitive option for visual
tasks, especially in contexts demanding adaptability to diverse computational
and communication constraints.

</details>

### [157] [Advanced Segmentation of Diabetic Retinopathy Lesions Using DeepLabv3+](https://arxiv.org/abs/2504.17306)
*Meher Boulaabi,Takwa Ben Aïcha Gader,Afef Kacem Echi,Sameh Mbarek*

Main category: cs.CV

TLDR: 提出了一种针对糖尿病视网膜病变病变的二元分割方法，通过结合多个模型输出提高分割精度，并采用特定预处理和数据增强技术，最终达到99%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决糖尿病视网膜病变病变分割中的数据集限制和标注复杂性挑战，提高分割精度。

Method: 采用DeepLabv3+模型，结合特定预处理（如裁剪和CLAHE）及数据增强技术，对每种病变类型进行二元分割，并将结果合并为单一图像。

Result: 在IDRID数据集上验证，分割准确率达到99%。

Conclusion: 该方法通过创新策略有效提升了糖尿病视网膜病变病变的分割精度，为医学图像分析提供了新思路。

Abstract: To improve the segmentation of diabetic retinopathy lesions (microaneurysms,
hemorrhages, exudates, and soft exudates), we implemented a binary segmentation
method specific to each type of lesion. As post-segmentation, we combined the
individual model outputs into a single image to better analyze the lesion
types. This approach facilitated parameter optimization and improved accuracy,
effectively overcoming challenges related to dataset limitations and annotation
complexity. Specific preprocessing steps included cropping and applying
contrast-limited adaptive histogram equalization to the L channel of the LAB
image. Additionally, we employed targeted data augmentation techniques to
further refine the model's efficacy. Our methodology utilized the DeepLabv3+
model, achieving a segmentation accuracy of 99%. These findings highlight the
efficacy of innovative strategies in advancing medical image analysis,
particularly in the precise segmentation of diabetic retinopathy lesions. The
IDRID dataset was utilized to validate and demonstrate the robustness of our
approach.

</details>

### [158] [DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model](https://arxiv.org/abs/2504.17315)
*Zhanglin Wu,Tengfei Song,Ning Xie,Weidong Zhang,Pengfei Li,Shuang Wu,Chong Li,Junhao Zhu,Hao Yang*

Main category: cs.CV

TLDR: 华为翻译服务中心提出了一种基于大型视觉语言模型（LVLM）的端到端文档图像机器翻译解决方案，结合多任务学习和感知思维链，并在推理阶段采用最小贝叶斯解码和后处理策略。


<details>
  <summary>Details</summary>
Motivation: 解决复杂布局文档图像的端到端机器翻译问题，同时支持OCR和无OCR任务。

Method: 结合多任务学习和感知思维链的训练框架，采用最小贝叶斯解码和后处理策略进行推理。

Result: 展示了有效的文档图像机器翻译方法，并在实验中验证了其性能。

Conclusion: 该方案为复杂布局文档图像翻译提供了一种统一的端到端解决方案。

Abstract: This paper presents the technical solution proposed by Huawei Translation
Service Center (HW-TSC) for the "End-to-End Document Image Machine Translation
for Complex Layouts" competition at the 19th International Conference on
Document Analysis and Recognition (DIMT25@ICDAR2025). Leveraging
state-of-the-art open-source large vision-language model (LVLM), we introduce a
training framework that combines multi-task learning with perceptual
chain-of-thought to develop a comprehensive end-to-end document translation
system. During the inference phase, we apply minimum Bayesian decoding and
post-processing strategies to further enhance the system's translation
capabilities. Our solution uniquely addresses both OCR-based and OCR-free
document image translation tasks within a unified framework. This paper
systematically details the training methods, inference strategies, LVLM base
models, training data, experimental setups, and results, demonstrating an
effective approach to document image machine translation.

</details>

### [159] [TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos](https://arxiv.org/abs/2504.17343)
*Linli Yao,Yicheng Li,Yuancheng Wei,Lei Li,Shuhuai Ren,Yuanxin Liu,Kun Ouyang,Lean Wang,Shicheng Li,Sida Li,Lingpeng Kong,Qi Liu,Yuanxing Zhang,Xu Sun*

Main category: cs.CV

TLDR: TimeChat-Online是一种新型在线视频大语言模型，通过创新的差分令牌丢弃（DTD）模块高效处理实时视频流，减少冗余帧，保持高性能。


<details>
  <summary>Details</summary>
Motivation: 在线视频平台的快速增长，尤其是直播服务，需要实时视频理解系统，而现有视频大语言模型在流媒体场景中存在处理冗余帧的局限性。

Method: 引入差分令牌丢弃（DTD）模块，受人类视觉感知中的变化盲现象启发，保留有意义的时序变化，过滤静态冗余内容。

Result: DTD减少了82.8%的视频令牌，同时保持98%的性能，表明流媒体视频中80%以上的视觉内容是冗余的。TimeChat-Online在流媒体和长视频任务中表现优异。

Conclusion: TimeChat-Online通过DTD模块和主动响应能力，显著提升了实时视频交互的效率与性能，适用于多样化场景。

Abstract: The rapid growth of online video platforms, particularly live streaming
services, has created an urgent need for real-time video understanding systems.
These systems must process continuous video streams and respond to user queries
instantaneously, presenting unique challenges for current Video Large Language
Models (VideoLLMs). While existing VideoLLMs excel at processing complete
videos, they face significant limitations in streaming scenarios due to their
inability to handle dense, redundant frames efficiently. We introduce
TimeChat-Online, a novel online VideoLLM that revolutionizes real-time video
interaction. At its core lies our innovative Differential Token Drop (DTD)
module, which addresses the fundamental challenge of visual redundancy in
streaming videos. Drawing inspiration from human visual perception's Change
Blindness phenomenon, DTD preserves meaningful temporal changes while filtering
out static, redundant content between frames. Remarkably, our experiments
demonstrate that DTD achieves an 82.8% reduction in video tokens while
maintaining 98% performance on StreamingBench, revealing that over 80% of
visual content in streaming videos is naturally redundant without requiring
language guidance. To enable seamless real-time interaction, we present
TimeChat-Online-139K, a comprehensive streaming video dataset featuring diverse
interaction patterns including backward-tracing, current-perception, and
future-responding scenarios. TimeChat-Online's unique Proactive Response
capability, naturally achieved through continuous monitoring of video scene
transitions via DTD, sets it apart from conventional approaches. Our extensive
evaluation demonstrates TimeChat-Online's superior performance on streaming
benchmarks (StreamingBench and OvOBench) and maintaining competitive results on
long-form video tasks such as Video-MME and MLVU.

</details>

### [160] [DRC: Enhancing Personalized Image Generation via Disentangled Representation Composition](https://arxiv.org/abs/2504.17349)
*Yiyan Xu,Wuqiang Zheng,Wenjie Wang,Fengbin Zhu,Xinting Hu,Yang Zhang,Fuli Feng,Tat-Seng Chua*

Main category: cs.CV

TLDR: DRC框架通过解耦表征学习提升个性化图像生成，解决了现有方法在风格与语义意图融合上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确捕捉和融合用户的风格偏好与语义意图，尤其是LMM方法存在视觉特征纠缠问题，导致生成的图像无法保留用户偏好或反映指定语义。

Method: DRC采用解耦表征组合，通过双塔解耦器分离风格与语义特征，并结合语义保留增强技术进行个性化建模。

Result: 在两项基准测试中，DRC表现出竞争力，有效缓解了引导崩溃问题。

Conclusion: 解耦表征学习对可控且有效的个性化图像生成至关重要。

Abstract: Personalized image generation has emerged as a promising direction in
multimodal content creation. It aims to synthesize images tailored to
individual style preferences (e.g., color schemes, character appearances,
layout) and semantic intentions (e.g., emotion, action, scene contexts) by
leveraging user-interacted history images and multimodal instructions. Despite
notable progress, existing methods -- whether based on diffusion models, large
language models, or Large Multimodal Models (LMMs) -- struggle to accurately
capture and fuse user style preferences and semantic intentions. In particular,
the state-of-the-art LMM-based method suffers from the entanglement of visual
features, leading to Guidance Collapse, where the generated images fail to
preserve user-preferred styles or reflect the specified semantics.
  To address these limitations, we introduce DRC, a novel personalized image
generation framework that enhances LMMs through Disentangled Representation
Composition. DRC explicitly extracts user style preferences and semantic
intentions from history images and the reference image, respectively, to form
user-specific latent instructions that guide image generation within LMMs.
Specifically, it involves two critical learning stages: 1) Disentanglement
learning, which employs a dual-tower disentangler to explicitly separate style
and semantic features, optimized via a reconstruction-driven paradigm with
difficulty-aware importance sampling; and 2) Personalized modeling, which
applies semantic-preserving augmentations to effectively adapt the disentangled
representations for robust personalized generation. Extensive experiments on
two benchmarks demonstrate that DRC shows competitive performance while
effectively mitigating the guidance collapse issue, underscoring the importance
of disentangled representation learning for controllable and effective
personalized image generation.

</details>

### [161] [I-INR: Iterative Implicit Neural Representations](https://arxiv.org/abs/2504.17364)
*Ali Haider,Muhammad Salman Ali,Maryam Qamar,Tahir Khalil,Soo Ye Kim,Jihyong Oh,Enzo Tartaglione,Sung-Ho Bae*

Main category: cs.CV

TLDR: 提出了一种名为I-INRs的迭代隐式神经表示框架，通过迭代细化提升信号重建质量，解决了传统INRs在高频细节捕获和噪声处理上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统隐式神经表示（INRs）作为回归问题容易导致均值回归，限制了其在高频细节保留和噪声处理上的能力。

Method: 提出I-INRs框架，通过迭代细化过程增强信号重建，兼容现有INRs架构。

Result: 实验表明I-INRs在图像恢复、去噪和物体占用预测等任务中优于WIRE、SIREN和Gauss等基线方法。

Conclusion: I-INRs通过迭代细化显著提升了信号重建质量，具有广泛的应用潜力。

Abstract: Implicit Neural Representations (INRs) have revolutionized signal processing
and computer vision by modeling signals as continuous, differentiable functions
parameterized by neural networks. However, their inherent formulation as a
regression problem makes them prone to regression to the mean, limiting their
ability to capture fine details, retain high-frequency information, and handle
noise effectively. To address these challenges, we propose Iterative Implicit
Neural Representations (I-INRs) a novel plug-and-play framework that enhances
signal reconstruction through an iterative refinement process. I-INRs
effectively recover high-frequency details, improve robustness to noise, and
achieve superior reconstruction quality. Our framework seamlessly integrates
with existing INR architectures, delivering substantial performance gains
across various tasks. Extensive experiments show that I-INRs outperform
baseline methods, including WIRE, SIREN, and Gauss, in diverse computer vision
applications such as image restoration, image denoising, and object occupancy
prediction.

</details>

### [162] [TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation](https://arxiv.org/abs/2504.17365)
*Ling You,Wenxuan Huang,Xinni Xie,Xiangyi Wei,Bangyan Li,Shaohui Lin,Yang Li,Changbo Wang*

Main category: cs.CV

TLDR: TimeSoccer是首个端到端的足球多模态大语言模型，用于全场比赛视频的单锚点密集视频描述（SDVC），通过联合预测时间戳和生成描述，实现全局上下文建模。


<details>
  <summary>Details</summary>
Motivation: 现有足球MLLMs依赖时间先验或复杂的两步范式，无法端到端处理长视频且性能次优。

Method: 提出TimeSoccer，结合MoFA-Select模块自适应选择代表性帧，并通过互补训练范式增强长时序处理能力。

Result: 实验表明TimeSoccer在SDVC任务上达到最先进性能，生成高质量评论且时间对齐准确。

Conclusion: TimeSoccer解决了足球视频端到端处理的挑战，为长视频理解提供了有效方案。

Abstract: Soccer is a globally popular sporting event, typically characterized by long
matches and distinctive highlight moments. Recent advances in Multimodal Large
Language Models (MLLMs) offer promising capabilities in temporal grounding and
video understanding, soccer commentary generation often requires precise
temporal localization and semantically rich descriptions over long-form video.
However, existing soccer MLLMs often rely on the temporal a priori for caption
generation, so they cannot process the soccer video end-to-end. While some
traditional approaches follow a two-step paradigm that is complex and fails to
capture the global context to achieve suboptimal performance. To solve the
above issues, we present TimeSoccer, the first end-to-end soccer MLLM for
Single-anchor Dense Video Captioning (SDVC) in full-match soccer videos.
TimeSoccer jointly predicts timestamps and generates captions in a single pass,
enabling global context modeling across 45-minute matches. To support long
video understanding of soccer matches, we introduce MoFA-Select, a
training-free, motion-aware frame compression module that adaptively selects
representative frames via a coarse-to-fine strategy, and incorporates
complementary training paradigms to strengthen the model's ability to handle
long temporal sequences. Extensive experiments demonstrate that our TimeSoccer
achieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end
form, generating high-quality commentary with accurate temporal alignment and
strong semantic relevance.

</details>

### [163] [Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset](https://arxiv.org/abs/2504.17371)
*Oussema Dhaouadi,Johannes Meier,Luca Wahl,Jacques Kaiser,Luca Scalerandi,Nick Wandelburg,Zhuolun Zhou,Nijanthan Berinpanathan,Holger Banzhaf,Daniel Cremers*

Main category: cs.CV

TLDR: DSC3D是一个高质量、无遮挡的3D轨迹数据集，通过无人机跟踪技术获取，覆盖多种交通场景，旨在提升自动驾驶系统的环境感知能力。


<details>
  <summary>Details</summary>
Motivation: 传统数据集因固定传感器和遮挡问题受限，无法全面捕捉动态环境。DSC3D通过无人机技术解决这些问题，提供更全面的3D轨迹数据。

Method: 采用单目相机无人机跟踪技术，采集14类交通参与者的175,000多条轨迹，覆盖多种复杂场景。

Result: 数据集在多样性和规模上超越现有数据集，包含前所未有的复杂场景，如高密度城市街道和停车操作。

Conclusion: DSC3D为自动驾驶系统提供详细环境表示，可提升障碍物交互和安全性，支持多项应用研究。

Abstract: Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet,
traditional datasets are usually captured by fixed sensors mounted on a car and
are susceptible to occlusion. Additionally, such an approach can precisely
reconstruct the dynamic environment in the close vicinity of the measurement
vehicle only, while neglecting objects that are further away. In this paper, we
introduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality,
occlusion-free dataset of 6 degrees of freedom bounding box trajectories
acquired through a novel monocular camera drone tracking pipeline. Our dataset
includes more than 175,000 trajectories of 14 types of traffic participants and
significantly exceeds existing datasets in terms of diversity and scale,
containing many unprecedented scenarios such as complex vehicle-pedestrian
interaction on highly populated urban streets and comprehensive parking
maneuvers from entry to exit. DSC3D dataset was captured in five various
locations in Europe and the United States and include: a parking lot, a crowded
inner-city, a steep urban intersection, a federal highway, and a suburban
intersection. Our 3D trajectory dataset aims to enhance autonomous driving
systems by providing detailed environmental 3D representations, which could
lead to improved obstacle interactions and safety. We demonstrate its utility
across multiple applications including motion prediction, motion planning,
scenario mining, and generative reactive traffic agents. Our interactive online
visualization platform and the complete dataset are publicly available at
app.deepscenario.com, facilitating research in motion prediction, behavior
modeling, and safety validation.

</details>

### [164] [SDVPT: Semantic-Driven Visual Prompt Tuning for Open-World Object Counting](https://arxiv.org/abs/2504.17395)
*Yiming Zhao,Guorong Li,Laiyun Qing,Amin Beheshti,Jian Yang,Michael Sheng,Yuankai Qi,Qingming Huang*

Main category: cs.CV

TLDR: 提出了一种名为SDVPT的框架，通过语义驱动的视觉提示调优，提升预训练视觉语言模型在开放世界物体计数中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在微调时仅关注训练集中的类别，导致对未见类别的泛化能力有限。

Method: 采用两阶段视觉提示学习策略（CSPI和TGPR），动态合成未见类别的视觉提示。

Result: 在FSC-147、CARPK和PUCPR+数据集上验证了SDVPT的有效性和适应性。

Conclusion: SDVPT通过语义驱动的方法显著提升了开放世界物体计数的性能。

Abstract: Open-world object counting leverages the robust text-image alignment of
pre-trained vision-language models (VLMs) to enable counting of arbitrary
categories in images specified by textual queries. However, widely adopted
naive fine-tuning strategies concentrate exclusively on text-image consistency
for categories contained in training, which leads to limited generalizability
for unseen categories. In this work, we propose a plug-and-play Semantic-Driven
Visual Prompt Tuning framework (SDVPT) that transfers knowledge from the
training set to unseen categories with minimal overhead in parameters and
inference time. First, we introduce a two-stage visual prompt learning strategy
composed of Category-Specific Prompt Initialization (CSPI) and Topology-Guided
Prompt Refinement (TGPR). The CSPI generates category-specific visual prompts,
and then TGPR distills latent structural patterns from the VLM's text encoder
to refine these prompts. During inference, we dynamically synthesize the visual
prompts for unseen categories based on the semantic correlation between unseen
and training categories, facilitating robust text-image alignment for unseen
categories. Extensive experiments integrating SDVPT with all available
open-world object counting models demonstrate its effectiveness and
adaptability across three widely used datasets: FSC-147, CARPK, and PUCPR+.

</details>

### [165] [Fine-tune Smarter, Not Harder: Parameter-Efficient Fine-Tuning for Geospatial Foundation Models](https://arxiv.org/abs/2504.17397)
*Francesc Marti-Escofet,Benedikt Blumenstiel,Linus Scheibenreif,Paolo Fraccaro,Konrad Schindler*

Main category: cs.CV

TLDR: 论文探讨了参数高效微调（PEFT）技术在地球观测（EO）领域的应用，通过实验验证其在减少计算资源和成本的同时保持或超越全微调性能的能力。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型规模的增大，全微调的计算成本和资源需求限制了其可访问性和扩展性，且可能导致预训练特征的遗忘和模型泛化能力下降。

Method: 通过多种基础模型架构和PEFT技术，在五个不同的EO数据集上进行实验，评估其有效性。

Result: PEFT技术在性能上匹配或超越全微调，同时减少训练时间和内存需求，并提升模型对未见地理区域的泛化能力。

Conclusion: PEFT技术为预训练地理空间模型的适应提供了高效解决方案，推荐使用UNet解码器且不包含元数据的配置，相关模型和技术已集成到开源包TerraTorch中。

Abstract: Earth observation (EO) is crucial for monitoring environmental changes,
responding to disasters, and managing natural resources. In this context,
foundation models facilitate remote sensing image analysis to retrieve relevant
geoinformation accurately and efficiently. However, as these models grow in
size, fine-tuning becomes increasingly challenging due to the associated
computational resources and costs, limiting their accessibility and
scalability. Furthermore, full fine-tuning can lead to forgetting pre-trained
features and even degrade model generalization. To address this,
Parameter-Efficient Fine-Tuning (PEFT) techniques offer a promising solution.
In this paper, we conduct extensive experiments with various foundation model
architectures and PEFT techniques to evaluate their effectiveness on five
different EO datasets. Our results provide a comprehensive comparison, offering
insights into when and how PEFT methods support the adaptation of pre-trained
geospatial models. We demonstrate that PEFT techniques match or even exceed
full fine-tuning performance and enhance model generalisation to unseen
geographic regions, while reducing training time and memory requirements.
Additional experiments investigate the effect of architecture choices such as
the decoder type or the use of metadata, suggesting UNet decoders and
fine-tuning without metadata as the recommended configuration. We have
integrated all evaluated foundation models and techniques into the open-source
package TerraTorch to support quick, scalable, and cost-effective model
adaptation.

</details>

### [166] [S2S-Net: Addressing the Domain Gap of Heterogeneous Sensor Systems in LiDAR-Based Collective Perception](https://arxiv.org/abs/2504.17399)
*Sven Teufel,Jörg Gamerdinger,Oliver Bringmann*

Main category: cs.CV

TLDR: 该论文提出了一种名为S2S-Net的传感器域鲁棒架构，用于解决车辆间集体感知中的Sensor2Sensor域差距问题，并在SCOPE数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 集体感知在自动驾驶中具有潜力，但不同传感器系统导致的Sensor2Sensor域差距问题尚未解决。缺乏包含异构传感器设置的数据集进一步加剧了这一挑战。

Method: 提出了S2S-Net架构，并在SCOPE数据集上进行了Sensor2Sensor域适应能力的深入分析。

Result: S2S-Net在未见过的传感器域中保持了极高的性能，并在SCOPE数据集上取得了最先进的结果。

Conclusion: S2S-Net为解决集体感知中的Sensor2Sensor域差距问题提供了有效方案，展示了其在异构传感器环境中的鲁棒性。

Abstract: Collective Perception (CP) has emerged as a promising approach to overcome
the limitations of individual perception in the context of autonomous driving.
Various approaches have been proposed to realize collective perception;
however, the Sensor2Sensor domain gap that arises from the utilization of
different sensor systems in Connected and Automated Vehicles (CAVs) remains
mostly unaddressed. This is primarily due to the paucity of datasets containing
heterogeneous sensor setups among the CAVs. The recently released SCOPE
datasets address this issue by providing data from three different LiDAR
sensors for each CAV. This study is the first to tackle the Sensor2Sensor
domain gap in vehicle to vehicle (V2V) collective perception. First, we present
our sensor-domain robust architecture S2S-Net. Then an in-depth analysis of the
Sensor2Sensor domain adaptation capabilities of S2S-Net on the SCOPE dataset is
conducted. S2S-Net demonstrates the capability to maintain very high
performance in unseen sensor domains and achieved state-of-the-art results on
the SCOPE dataset.

</details>

### [167] [StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies](https://arxiv.org/abs/2504.17401)
*Xu Wang,Jialang Xu,Shuai Zhang,Baoru Huang,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TLDR: StereoMamba架构通过FE-Mamba和MFF模块优化了RAMIS中的立体视差估计，在精度、鲁棒性和速度间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 解决RAMIS中立体视差估计在精度、鲁棒性和推理速度之间的平衡问题。

Method: 提出StereoMamba架构，包括FE-Mamba模块增强空间依赖性和MFF模块融合多尺度特征。

Result: 在SCARED基准上表现优异，EPE为2.64 px，深度MAE为2.55 mm，推理速度21.28 FPS，SSIM和PSNR表现最佳。

Conclusion: StereoMamba在RAMIS中实现了高精度、鲁棒性和效率的平衡，具有强泛化能力。

Abstract: Stereo disparity estimation is crucial for obtaining depth information in
robot-assisted minimally invasive surgery (RAMIS). While current deep learning
methods have made significant advancements, challenges remain in achieving an
optimal balance between accuracy, robustness, and inference speed. To address
these challenges, we propose the StereoMamba architecture, which is
specifically designed for stereo disparity estimation in RAMIS. Our approach is
based on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances
long-range spatial dependencies both within and across stereo images. To
effectively integrate multi-scale features from FE-Mamba, we then introduce a
novel Multidimensional Feature Fusion (MFF) module. Experiments against the
state-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba
achieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the
second-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining
an inference speed of 21.28 FPS for a pair of high-resolution images
(1280*1024), striking the optimum balance between accuracy, robustness, and
efficiency. Furthermore, by comparing synthesized right images, generated from
warping left images using the generated disparity maps, with the actual right
image, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),
exhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS
datasets.

</details>

### [168] [3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models](https://arxiv.org/abs/2504.17414)
*Min Wei,Chaohui Yu,Jingkai Zhou,Fan Wang*

Main category: cs.CV

TLDR: 3DV-TON是一个基于扩散模型的视频试穿框架，通过生成可动画的3D网格作为帧级指导，解决了现有方法在复杂服装和多样姿势下生成高质量、时间一致结果的难题。


<details>
  <summary>Details</summary>
Motivation: 现有视频试穿方法在处理复杂服装图案和多样身体姿势时，难以生成高质量且时间一致的结果。

Method: 采用生成的可动画纹理3D网格作为帧级指导，包括关键帧选择和3D网格重建与动画化，并引入矩形掩码策略减少信息泄漏。

Result: 在HR-VVT高分辨率数据集上展示了优于现有方法的性能。

Conclusion: 3DV-TON通过3D网格指导和掩码策略，显著提升了视频试穿的质量和时间一致性。

Abstract: Video try-on replaces clothing in videos with target garments. Existing
methods struggle to generate high-quality and temporally consistent results
when handling complex clothing patterns and diverse body poses. We present
3DV-TON, a novel diffusion-based framework for generating high-fidelity and
temporally consistent video try-on results. Our approach employs generated
animatable textured 3D meshes as explicit frame-level guidance, alleviating the
issue of models over-focusing on appearance fidelity at the expanse of motion
coherence. This is achieved by enabling direct reference to consistent garment
texture movements throughout video sequences. The proposed method features an
adaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe
for initial 2D image try-on, followed by (2) reconstructing and animating a
textured 3D mesh synchronized with original video poses. We further introduce a
robust rectangular masking strategy that successfully mitigates artifact
propagation caused by leaking clothing information during dynamic human and
garment movements. To advance video try-on research, we introduce HR-VVT, a
high-resolution benchmark dataset containing 130 videos with diverse clothing
types and scenarios. Quantitative and qualitative results demonstrate our
superior performance over existing methods. The project page is at this link
https://2y7c3.github.io/3DV-TON/

</details>

### [169] [Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs](https://arxiv.org/abs/2504.17432)
*Tiancheng Gu,Kaicheng Yang,Ziyong Feng,Xingjun Wang,Yanzhao Zhang,Dingkun Long,Yingda Chen,Weidong Cai,Jiankang Deng*

Main category: cs.CV

TLDR: UniME框架通过两阶段方法（知识蒸馏和硬负样本增强）提升多模态表示学习，解决了CLIP的局限性，并在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: CLIP框架在图像-文本检索中存在文本截断、孤立编码和组合性不足的问题，而多模态大语言模型（MLLMs）的潜力尚未充分挖掘。

Method: UniME采用两阶段方法：1）从LLM教师模型进行知识蒸馏；2）通过硬负样本增强指令调优。

Result: 在MMEB基准和多个检索任务中，UniME表现优异，提升了判别性和组合能力。

Conclusion: UniME通过创新方法显著提升了多模态表示学习的效果，适用于多样化下游任务。

Abstract: The Contrastive Language-Image Pre-training (CLIP) framework has become a
widely used approach for multimodal representation learning, particularly in
image-text retrieval and clustering. However, its efficacy is constrained by
three key limitations: (1) text token truncation, (2) isolated image-text
encoding, and (3) deficient compositionality due to bag-of-words behavior.
While recent Multimodal Large Language Models (MLLMs) have demonstrated
significant advances in generalized vision-language understanding, their
potential for learning transferable multimodal representations remains
underexplored.In this work, we present UniME (Universal Multimodal Embedding),
a novel two-stage framework that leverages MLLMs to learn discriminative
representations for diverse downstream tasks. In the first stage, we perform
textual discriminative knowledge distillation from a powerful LLM-based teacher
model to enhance the embedding capability of the MLLM\'s language component. In
the second stage, we introduce hard negative enhanced instruction tuning to
further advance discriminative representation learning. Specifically, we
initially mitigate false negative contamination and then sample multiple hard
negatives per instance within each batch, forcing the model to focus on
challenging samples. This approach not only improves discriminative power but
also enhances instruction-following ability in downstream tasks. We conduct
extensive experiments on the MMEB benchmark and multiple retrieval tasks,
including short and long caption retrieval and compositional retrieval. Results
demonstrate that UniME achieves consistent performance improvement across all
tasks, exhibiting superior discriminative and compositional capabilities.

</details>

### [170] [Predict-Optimize-Distill: A Self-Improving Cycle for 4D Object Understanding](https://arxiv.org/abs/2504.17441)
*Mingxuan Wu,Huang Huang,Justin Kerr,Chung Min Kim,Anthony Zhang,Brent Yi,Angjoo Kanazawa*

Main category: cs.CV

TLDR: POD是一个自改进框架，通过预测、优化和蒸馏的循环提升4D物体理解能力。


<details>
  <summary>Details</summary>
Motivation: 人类通过长时间观察提升对物体3D状态的预测能力，现有系统依赖多视角观察或监督数据集，POD旨在通过自改进循环实现更好的性能。

Method: POD框架通过预测（神经网络预测局部姿态）、优化（全局优化通过逆渲染）、蒸馏（生成自标记数据）的循环迭代提升模型。

Result: POD在真实和合成物体上表现优于纯优化基线，性能随视频长度和迭代次数提升。

Conclusion: POD展示了自改进框架的潜力，能够通过循环迭代和长时间观察提升性能。

Abstract: Humans can resort to long-form inspection to build intuition on predicting
the 3D configurations of unseen objects. The more we observe the object motion,
the better we get at predicting its 3D state immediately. Existing systems
either optimize underlying representations from multi-view observations or
train a feed-forward predictor from supervised datasets. We introduce
Predict-Optimize-Distill (POD), a self-improving framework that interleaves
prediction and optimization in a mutually reinforcing cycle to achieve better
4D object understanding with increasing observation time. Given a multi-view
object scan and a long-form monocular video of human-object interaction, POD
iteratively trains a neural network to predict local part poses from RGB
frames, uses this predictor to initialize a global optimization which refines
output poses through inverse rendering, then finally distills the results of
optimization back into the model by generating synthetic self-labeled training
data from novel viewpoints. Each iteration improves both the predictive model
and the optimized motion trajectory, creating a virtuous cycle that bootstraps
its own training data to learn about the pose configurations of an object. We
also introduce a quasi-multiview mining strategy for reducing depth ambiguity
by leveraging long video. We evaluate POD on 14 real-world and 5 synthetic
objects with various joint types, including revolute and prismatic joints as
well as multi-body configurations where parts detach or reattach independently.
POD demonstrates significant improvement over a pure optimization baseline
which gets stuck in local minima, particularly for longer videos. We also find
that POD's performance improves with both video length and successive
iterations of the self-improving cycle, highlighting its ability to scale
performance with additional observations and looped refinement.

</details>

### [171] [FRAG: Frame Selection Augmented Generation for Long Video and Long Document Understanding](https://arxiv.org/abs/2504.17447)
*De-An Huang,Subhashree Radhakrishnan,Zhiding Yu,Jan Kautz*

Main category: cs.CV

TLDR: 论文提出了一种名为FRAG的框架，通过选择输入中的相关帧而非处理长上下文，提升了大型多模态模型（LMMs）在长视频和多页文档任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的长上下文LMMs在模型规模和性能上受限于计算成本，因此探索了一种无需长上下文处理的方法。

Method: FRAG框架通过独立评分选择相关帧，并基于这些帧生成最终输出，避免了长上下文处理。

Result: 实验表明，FRAG显著提升了模型性能，在长视频和文档任务中达到SOTA水平，如InternVL2-76B在MLVU上提升5.8%。

Conclusion: FRAG是一种简单有效的框架，适用于现有LMMs，无需微调即可显著提升长输入任务的表现。

Abstract: There has been impressive progress in Large Multimodal Models (LMMs). Recent
works extend these models to long inputs, including multi-page documents and
long videos. However, the model size and performance of these long context
models are still limited due to the computational cost in both training and
inference. In this work, we explore an orthogonal direction and process long
inputs without long context LMMs. We propose Frame Selection Augmented
Generation (FRAG), where the model first selects relevant frames within the
input, and then only generates the final outputs based on the selected frames.
The core of the selection process is done by scoring each frame independently,
which does not require long context processing. The frames with the highest
scores are then selected by a simple Top-K selection. We show that this
frustratingly simple framework is applicable to both long videos and multi-page
documents using existing LMMs without any fine-tuning. We consider two models,
LLaVA-OneVision and InternVL2, in our experiments and show that FRAG
consistently improves the performance and achieves state-of-the-art
performances for both long video and long document understanding. For videos,
FRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on
Video-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA
compared with recent LMMs specialized in long document understanding. Code is
available at: https://github.com/NVlabs/FRAG

</details>

### [172] [Unveiling Hidden Vulnerabilities in Digital Human Generation via Adversarial Attacks](https://arxiv.org/abs/2504.17457)
*Zhiying Li,Yeying Jin,Fan Shen,Zhi Liu,Weibin Chen,Pengju Zhang,Xiaomei Zhang,Boyu Chen,Michael Shen,Kejian Wu,Zhaoxin Fan,Jin Dong*

Main category: cs.CV

TLDR: 论文提出了一种名为Tangible Attack (TBA)的新框架，通过Dual Heterogeneous Noise Generator (DHNG)和自定义对抗损失函数，显著提高了对抗攻击的效果，揭示了当前EHPS模型的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注减少估计误差，但忽视了鲁棒性和安全性，导致系统易受对抗攻击。

Method: 提出TBA框架，结合DHNG（利用VAE和ControlNet生成多样化噪声）和自定义对抗损失函数，通过多梯度信号迭代优化对抗样本。

Result: 实验显示TBA显著提升了对抗攻击效果，估计误差增加41.0%，平均提升约17.0%。

Conclusion: 研究揭示了当前EHPS模型的安全漏洞，强调了数字人生成系统需要更强的防御措施。

Abstract: Expressive human pose and shape estimation (EHPS) is crucial for digital
human generation, especially in applications like live streaming. While
existing research primarily focuses on reducing estimation errors, it largely
neglects robustness and security aspects, leaving these systems vulnerable to
adversarial attacks. To address this significant challenge, we propose the
\textbf{Tangible Attack (TBA)}, a novel framework designed to generate
adversarial examples capable of effectively compromising any digital human
generation model. Our approach introduces a \textbf{Dual Heterogeneous Noise
Generator (DHNG)}, which leverages Variational Autoencoders (VAE) and
ControlNet to produce diverse, targeted noise tailored to the original image
features. Additionally, we design a custom \textbf{adversarial loss function}
to optimize the noise, ensuring both high controllability and potent
disruption. By iteratively refining the adversarial sample through
multi-gradient signals from both the noise and the state-of-the-art EHPS model,
TBA substantially improves the effectiveness of adversarial attacks. Extensive
experiments demonstrate TBA's superiority, achieving a remarkable 41.0\%
increase in estimation error, with an average improvement of approximately
17.0\%. These findings expose significant security vulnerabilities in current
EHPS models and highlight the need for stronger defenses in digital human
generation systems.

</details>

### [173] [Enhanced Sample Selection with Confidence Tracking: Identifying Correctly Labeled yet Hard-to-Learn Samples in Noisy Data](https://arxiv.org/abs/2504.17474)
*Weiran Pan,Wei Wei,Feida Zhu,Yong Deng*

Main category: cs.CV

TLDR: 提出一种基于模型预测置信度趋势的新样本选择方法，用于解决噪声标签下的图像分类问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将小损失样本视为正确标签，但部分正确标签样本因难学可能在训练早期表现出高损失，导致基于损失阈值的样本选择在精度和召回率之间存在权衡。

Method: 通过跟踪标注标签与其他类别间的置信度差距趋势（使用Mann-Kendall检验），区分正确标签但难学的样本与错误标签样本。

Result: 实验表明，该方法能有效提升现有噪声标签学习方法的性能。

Conclusion: 提出的方法可作为即插即用组件，显著改善噪声标签下的样本选择效果。

Abstract: We propose a novel sample selection method for image classification in the
presence of noisy labels. Existing methods typically consider small-loss
samples as correctly labeled. However, some correctly labeled samples are
inherently difficult for the model to learn and can exhibit high loss similar
to mislabeled samples in the early stages of training. Consequently, setting a
threshold on per-sample loss to select correct labels results in a trade-off
between precision and recall in sample selection: a lower threshold may miss
many correctly labeled hard-to-learn samples (low recall), while a higher
threshold may include many mislabeled samples (low precision). To address this
issue, our goal is to accurately distinguish correctly labeled yet
hard-to-learn samples from mislabeled ones, thus alleviating the trade-off
dilemma. We achieve this by considering the trends in model prediction
confidence rather than relying solely on loss values. Empirical observations
show that only for correctly labeled samples, the model's prediction confidence
for the annotated labels typically increases faster than for any other classes.
Based on this insight, we propose tracking the confidence gaps between the
annotated labels and other classes during training and evaluating their trends
using the Mann-Kendall Test. A sample is considered potentially correctly
labeled if all its confidence gaps tend to increase. Our method functions as a
plug-and-play component that can be seamlessly integrated into existing sample
selection techniques. Experiments on several standard benchmarks and real-world
datasets demonstrate that our method enhances the performance of existing
methods for learning with noisy labels.

</details>

### [174] [RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation](https://arxiv.org/abs/2504.17502)
*Aviv Slobodkin,Hagai Taitelbaum,Yonatan Bitton,Brian Gordon,Michal Sokolik,Nitzan Bitton Guetta,Almog Gueta,Royi Rassin,Itay Laish,Dani Lischinski,Idan Szpektor*

Main category: cs.CV

TLDR: RefVNLI是一种新的评估指标，用于同时评估文本对齐和主题保留，优于现有方法，且成本低。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法要么仅关注文本对齐或主题保留，要么与人类判断不一致，或依赖昂贵的API评估。

Method: 引入RefVNLI，基于大规模视频推理基准和图像扰动数据集训练，评估文本对齐和主题保留。

Result: RefVNLI在多个基准和主题类别中表现优异，文本对齐提升6.4分，主题一致性提升8.5分，与人类偏好一致性达87%。

Conclusion: RefVNLI是一种高效、可靠的评估指标，适用于主题驱动的T2I生成任务。

Abstract: Subject-driven text-to-image (T2I) generation aims to produce images that
align with a given textual description, while preserving the visual identity
from a referenced subject image. Despite its broad downstream applicability --
ranging from enhanced personalization in image generation to consistent
character representation in video rendering -- progress in this field is
limited by the lack of reliable automatic evaluation. Existing methods either
assess only one aspect of the task (i.e., textual alignment or subject
preservation), misalign with human judgments, or rely on costly API-based
evaluation. To address this, we introduce RefVNLI, a cost-effective metric that
evaluates both textual alignment and subject preservation in a single
prediction. Trained on a large-scale dataset derived from video-reasoning
benchmarks and image perturbations, RefVNLI outperforms or matches existing
baselines across multiple benchmarks and subject categories (e.g.,
\emph{Animal}, \emph{Object}), achieving up to 6.4-point gains in textual
alignment and 8.5-point gains in subject consistency. It also excels with
lesser-known concepts, aligning with human preferences at over 87\% accuracy.

</details>

### [175] [Mamba-Sea: A Mamba-based Framework with Global-to-Local Sequence Augmentation for Generalizable Medical Image Segmentation](https://arxiv.org/abs/2504.17515)
*Zihan Cheng,Jintao Guo,Jian Zhang,Lei Qi,Luping Zhou,Yinghuan Shi,Yang Gao*

Main category: cs.CV

TLDR: 论文提出了一种基于Mamba架构的新框架Mamba-Sea，用于解决医学图像分割中的分布偏移问题，通过全局到局部的序列增强提升模型泛化能力，并在Prostate数据集上取得了超过90%的Dice系数。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中分布偏移问题限制了模型的泛化能力，现有方法主要基于CNN或ViT架构。Mamba因其长距离依赖捕捉能力和线性复杂度成为潜在替代方案，本文探索其在域泛化中的应用。

Method: 提出Mamba-Sea框架，结合全局和局部序列增强：全局机制模拟不同站点外观变化，抑制域特定信息学习；局部机制通过扰动连续子序列的令牌风格，增强模型对域偏移的鲁棒性。

Result: Mamba-Sea在Prostate数据集上Dice系数超过90%，优于之前SOTA的88.61%，成为首个基于Mamba的医学图像分割域泛化方法。

Conclusion: Mamba-Sea展示了Mamba架构在医学图像分割域泛化中的潜力，提供了一种高效且鲁棒的解决方案，代码已开源。

Abstract: To segment medical images with distribution shifts, domain generalization
(DG) has emerged as a promising setting to train models on source domains that
can generalize to unseen target domains. Existing DG methods are mainly based
on CNN or ViT architectures. Recently, advanced state space models, represented
by Mamba, have shown promising results in various supervised medical image
segmentation. The success of Mamba is primarily owing to its ability to capture
long-range dependencies while keeping linear complexity with input sequence
length, making it a promising alternative to CNNs and ViTs. Inspired by the
success, in the paper, we explore the potential of the Mamba architecture to
address distribution shifts in DG for medical image segmentation. Specifically,
we propose a novel Mamba-based framework, Mamba-Sea, incorporating
global-to-local sequence augmentation to improve the model's generalizability
under domain shift issues. Our Mamba-Sea introduces a global augmentation
mechanism designed to simulate potential variations in appearance across
different sites, aiming to suppress the model's learning of domain-specific
information. At the local level, we propose a sequence-wise augmentation along
input sequences, which perturbs the style of tokens within random continuous
sub-sequences by modeling and resampling style statistics associated with
domain shifts. To our best knowledge, Mamba-Sea is the first work to explore
the generalization of Mamba for medical image segmentation, providing an
advanced and promising Mamba-based architecture with strong robustness to
domain shifts. Remarkably, our proposed method is the first to surpass a Dice
coefficient of 90% on the Prostate dataset, which exceeds previous SOTA of
88.61%. The code is available at https://github.com/orange-czh/Mamba-Sea.

</details>

### [176] [Towards One-Stage End-to-End Table Structure Recognition with Parallel Regression for Diverse Scenarios](https://arxiv.org/abs/2504.17522)
*Anyi Xiao,Cihui Yang*

Main category: cs.CV

TLDR: 提出了一种名为TableCenterNet的单阶段端到端表格结构解析网络，统一了表格空间和逻辑结构的预测，并通过共享特征提取层和任务特定解码的协同架构，实现了跨场景适应性和计算效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法在表格结构识别中存在训练复杂、解码耗时或依赖复杂后处理的问题，难以兼顾跨场景适应性、鲁棒性和计算效率。

Method: TableCenterNet通过并行回归任务统一预测表格的空间和逻辑结构，利用共享特征提取层和任务特定解码的协同架构，隐式学习单元格的空间-逻辑位置映射规律。

Result: 在基准数据集上的实验表明，TableCenterNet能够有效解析多样化场景中的表格结构，并在TableGraph-24k数据集上达到最先进的性能。

Conclusion: TableCenterNet是一种更易训练、推理更快的单阶段方法，显著提升了表格结构识别的效率和性能。

Abstract: Table structure recognition aims to parse tables in unstructured data into
machine-understandable formats. Recent methods address this problem through a
two-stage process or optimized one-stage approaches. However, these methods
either require multiple networks to be serially trained and perform more
time-consuming sequential decoding, or rely on complex post-processing
algorithms to parse the logical structure of tables. They struggle to balance
cross-scenario adaptability, robustness, and computational efficiency. In this
paper, we propose a one-stage end-to-end table structure parsing network called
TableCenterNet. This network unifies the prediction of table spatial and
logical structure into a parallel regression task for the first time, and
implicitly learns the spatial-logical location mapping laws of cells through a
synergistic architecture of shared feature extraction layers and task-specific
decoding. Compared with two-stage methods, our method is easier to train and
faster to infer. Experiments on benchmark datasets show that TableCenterNet can
effectively parse table structures in diverse scenarios and achieve
state-of-the-art performance on the TableGraph-24k dataset. Code is available
at https://github.com/dreamy-xay/TableCenterNet.

</details>

### [177] [ESDiff: Encoding Strategy-inspired Diffusion Model with Few-shot Learning for Color Image Inpainting](https://arxiv.org/abs/2504.17524)
*Junyan Zhang,Yan Li,Mengxiao Geng,Liu Shi,Qiegen Liu*

Main category: cs.CV

TLDR: 本文提出了一种基于编码策略的扩散模型，用于小样本学习的彩色图像修复，通过虚拟掩码和高维对象构建，提升了修复质量。


<details>
  <summary>Details</summary>
Motivation: 传统图像修复方法难以保留复杂细节和结构，而深度学习模型需要大量训练数据。本文旨在解决这些问题。

Method: 采用编码策略的扩散模型，利用虚拟掩码构建高维对象，结合低秩方法和迭代修复，捕捉多样图像表示。

Result: 实验表明，该方法在定量指标上优于现有技术，修复图像在纹理和结构完整性上表现更优。

Conclusion: 该方法通过编码策略和扩散模型，实现了更精确和连贯的图像修复效果。

Abstract: Image inpainting is a technique used to restore missing or damaged regions of
an image. Traditional methods primarily utilize information from adjacent
pixels for reconstructing missing areas, while they struggle to preserve
complex details and structures. Simultaneously, models based on deep learning
necessitate substantial amounts of training data. To address this challenge, an
encoding strategy-inspired diffusion model with few-shot learning for color
image inpainting is proposed in this paper. The main idea of this novel
encoding strategy is the deployment of a "virtual mask" to construct
high-dimensional objects through mutual perturbations between channels. This
approach enables the diffusion model to capture diverse image representations
and detailed features from limited training samples. Moreover, the encoding
strategy leverages redundancy between channels, integrates with low-rank
methods during iterative inpainting, and incorporates the diffusion model to
achieve accurate information output. Experimental results indicate that our
method exceeds current techniques in quantitative metrics, and the
reconstructed images quality has been improved in aspects of texture and
structural integrity, leading to more precise and coherent results.

</details>

### [178] [Text-to-Image Alignment in Denoising-Based Models through Step Selection](https://arxiv.org/abs/2504.17525)
*Paul Grimal,Hervé Le Borgne,Olivier Ferret*

Main category: cs.CV

TLDR: 提出了一种新方法，通过选择性增强关键去噪步骤的信号来优化图像生成，解决了文本-图像对齐和推理限制的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉生成AI模型在文本-图像对齐和推理方面存在局限性，需要改进。

Method: 在去噪过程的后期阶段选择性增强信号，优化输入语义的图像生成。

Result: 在Diffusion和Flow Matching模型上验证了方法的有效性，取得了最先进的性能。

Conclusion: 选择合适的采样阶段对提升性能和图像对齐至关重要。

Abstract: Visual generative AI models often encounter challenges related to text-image
alignment and reasoning limitations. This paper presents a novel method for
selectively enhancing the signal at critical denoising steps, optimizing image
generation based on input semantics. Our approach addresses the shortcomings of
early-stage signal modifications, demonstrating that adjustments made at later
stages yield superior results. We conduct extensive experiments to validate the
effectiveness of our method in producing semantically aligned images on
Diffusion and Flow Matching model, achieving state-of-the-art performance. Our
results highlight the importance of a judicious choice of sampling stage to
improve performance and overall image alignment.

</details>

### [179] [An Explainable Nature-Inspired Framework for Monkeypox Diagnosis: Xception Features Combined with NGBoost and African Vultures Optimization Algorithm](https://arxiv.org/abs/2504.17540)
*Ahmadreza Shateri,Negar Nourani,Morteza Dorrigiv,Hamid Nasiri*

Main category: cs.CV

TLDR: 本研究提出了一种基于深度学习的自动化猴痘检测框架，结合迁移学习、降维和先进机器学习技术，在MSLD数据集上实现了高精度分类。


<details>
  <summary>Details</summary>
Motivation: 猴痘在全球非传统流行区域的传播引发公共卫生担忧，早期准确诊断对疾病管理至关重要。

Method: 采用Xception架构提取深度特征，PCA降维，NGBoost分类，并引入AVOA优化超参数。

Result: AVOA-NGBoost模型表现优异，准确率达97.53%，F1分数97.72%，AUC 97.47%。

Conclusion: 该框架为猴痘早期诊断提供了高效工具，尤其适用于资源有限环境。

Abstract: The recent global spread of monkeypox, particularly in regions where it has
not historically been prevalent, has raised significant public health concerns.
Early and accurate diagnosis is critical for effective disease management and
control. In response, this study proposes a novel deep learning-based framework
for the automated detection of monkeypox from skin lesion images, leveraging
the power of transfer learning, dimensionality reduction, and advanced machine
learning techniques. We utilize the newly developed Monkeypox Skin Lesion
Dataset (MSLD), which includes images of monkeypox, chickenpox, and measles, to
train and evaluate our models. The proposed framework employs the Xception
architecture for deep feature extraction, followed by Principal Component
Analysis (PCA) for dimensionality reduction, and the Natural Gradient Boosting
(NGBoost) algorithm for classification. To optimize the model's performance and
generalization, we introduce the African Vultures Optimization Algorithm (AVOA)
for hyperparameter tuning, ensuring efficient exploration of the parameter
space. Our results demonstrate that the proposed AVOA-NGBoost model achieves
state-of-the-art performance, with an accuracy of 97.53%, F1-score of 97.72%
and an AUC of 97.47%. Additionally, we enhance model interpretability using
Grad-CAM and LIME techniques, providing insights into the decision-making
process and highlighting key features influencing classification. This
framework offers a highly precise and efficient diagnostic tool, potentially
aiding healthcare providers in early detection and diagnosis, particularly in
resource-constrained environments.

</details>

### [180] [When Gaussian Meets Surfel: Ultra-fast High-fidelity Radiance Field Rendering](https://arxiv.org/abs/2504.17545)
*Keyang Ye,Tianjia Shao,Kun Zhou*

Main category: cs.CV

TLDR: Gaussian-enhanced Surfels (GESs) 是一种双尺度表示方法，用于辐射场渲染，结合了2D不透明面元和3D高斯分布，实现了快速、高保真的渲染。


<details>
  <summary>Details</summary>
Motivation: 为了解决辐射场渲染中快速和高保真之间的矛盾，提出了一种结合粗尺度几何和细尺度外观的双尺度表示方法。

Method: 使用2D面元表示粗尺度几何和外观，3D高斯分布补充细尺度细节。渲染分为两阶段：面元光栅化和高斯分布叠加。优化采用从粗到细的策略。

Result: GESs 实现了快速、无排序的渲染，避免了视角变化时的闪烁问题，并支持多种扩展（如抗锯齿、加速渲染和紧凑存储）。

Conclusion: GESs 是一种高效的辐射场渲染表示方法，在速度和保真度上均优于现有技术。

Abstract: We introduce Gaussian-enhanced Surfels (GESs), a bi-scale representation for
radiance field rendering, wherein a set of 2D opaque surfels with
view-dependent colors represent the coarse-scale geometry and appearance of
scenes, and a few 3D Gaussians surrounding the surfels supplement fine-scale
appearance details. The rendering with GESs consists of two passes -- surfels
are first rasterized through a standard graphics pipeline to produce depth and
color maps, and then Gaussians are splatted with depth testing and color
accumulation on each pixel order independently. The optimization of GESs from
multi-view images is performed through an elaborate coarse-to-fine procedure,
faithfully capturing rich scene appearance. The entirely sorting-free rendering
of GESs not only achieves very fast rates, but also produces view-consistent
images, successfully avoiding popping artifacts under view changes. The basic
GES representation can be easily extended to achieve anti-aliasing in rendering
(Mip-GES), boosted rendering speeds (Speedy-GES) and compact storage
(Compact-GES), and reconstruct better scene geometries by replacing 3D
Gaussians with 2D Gaussians (2D-GES). Experimental results show that GESs
advance the state-of-the-arts as a compelling representation for ultra-fast
high-fidelity radiance field rendering.

</details>

### [181] [A Comprehensive Survey of Knowledge-Based Vision Question Answering Systems: The Lifecycle of Knowledge in Visual Reasoning Task](https://arxiv.org/abs/2504.17547)
*Jiaqi Deng,Zonghan Wu,Huan Huo,Guandong Xu*

Main category: cs.CV

TLDR: 该论文是一篇关于知识驱动的视觉问答（KB-VQA）的综述，系统整理了现有方法，提出了分类体系，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: KB-VQA结合视觉、文本和外部知识，具有广泛应用前景，但缺乏系统综述。本文旨在填补这一空白。

Method: 建立KB-VQA的分类体系，分为知识表示、知识检索和知识推理三个阶段，并分析各类技术。

Result: 系统梳理了KB-VQA方法，总结了技术挑战，并提出了未来研究方向。

Conclusion: 本文为KB-VQA领域提供了全面的综述和分类框架，为未来研究奠定了基础。

Abstract: Knowledge-based Vision Question Answering (KB-VQA) extends general Vision
Question Answering (VQA) by not only requiring the understanding of visual and
textual inputs but also extensive range of knowledge, enabling significant
advancements across various real-world applications. KB-VQA introduces unique
challenges, including the alignment of heterogeneous information from diverse
modalities and sources, the retrieval of relevant knowledge from noisy or
large-scale repositories, and the execution of complex reasoning to infer
answers from the combined context. With the advancement of Large Language
Models (LLMs), KB-VQA systems have also undergone a notable transformation,
where LLMs serve as powerful knowledge repositories, retrieval-augmented
generators and strong reasoners. Despite substantial progress, no comprehensive
survey currently exists that systematically organizes and reviews the existing
KB-VQA methods. This survey aims to fill this gap by establishing a structured
taxonomy of KB-VQA approaches, and categorizing the systems into main stages:
knowledge representation, knowledge retrieval, and knowledge reasoning. By
exploring various knowledge integration techniques and identifying persistent
challenges, this work also outlines promising future research directions,
providing a foundation for advancing KB-VQA models and their applications.

</details>

### [182] [Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior](https://arxiv.org/abs/2504.17551)
*Lin Che,Yizi Chen,Tanhua Jin,Martin Raubal,Konrad Schindler,Peter Kiefer*

Main category: cs.CV

TLDR: 提出一种基于街景图像的无监督对比聚类模型，结合地理先验，用于城市土地利用分类与制图。


<details>
  <summary>Details</summary>
Motivation: 现有遥感技术在城市复杂环境中缺乏精度，而街景图像能捕捉更多地面细节和人类活动，但现有方法依赖监督分类，面临标注数据稀缺和泛化困难的问题。

Method: 采用无监督对比聚类模型，结合地理先验，并通过简单的视觉分配实现土地利用制图。

Result: 实验证明该方法能从两个城市的街景图像数据集中生成土地利用图。

Conclusion: 该方法基于地理空间数据的空间一致性，可适应不同场景，实现可扩展的无监督土地利用制图与更新。

Abstract: Urban land use classification and mapping are critical for urban planning,
resource management, and environmental monitoring. Existing remote sensing
techniques often lack precision in complex urban environments due to the
absence of ground-level details. Unlike aerial perspectives, street view images
provide a ground-level view that captures more human and social activities
relevant to land use in complex urban scenes. Existing street view-based
methods primarily rely on supervised classification, which is challenged by the
scarcity of high-quality labeled data and the difficulty of generalizing across
diverse urban landscapes. This study introduces an unsupervised contrastive
clustering model for street view images with a built-in geographical prior, to
enhance clustering performance. When combined with a simple visual assignment
of the clusters, our approach offers a flexible and customizable solution to
land use mapping, tailored to the specific needs of urban planners. We
experimentally show that our method can generate land use maps from geotagged
street view image datasets of two cities. As our methodology relies on the
universal spatial coherence of geospatial data ("Tobler's law"), it can be
adapted to various settings where street view images are available, to enable
scalable, unsupervised land use mapping and updating. The code will be
available at https://github.com/lin102/CCGP.

</details>

### [183] [Occlusion-Aware Self-Supervised Monocular Depth Estimation for Weak-Texture Endoscopic Images](https://arxiv.org/abs/2504.17582)
*Zebo Huang,Yinghui Wang*

Main category: cs.CV

TLDR: 提出一种自监督单目深度估计网络，针对内窥镜场景，通过遮挡感知框架和语义分割改进深度重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设光照一致，但内窥镜场景中动态光照和遮挡导致几何解释错误，信号不可靠。

Method: 引入遮挡感知自监督框架，包括遮挡掩模数据增强和语义分割（基于非负矩阵分解）。

Result: 在SCARED数据集上达到SOTA性能，并在Endo-SLAM和SERV-CT数据集上展示强泛化能力。

Conclusion: 该方法有效解决了内窥镜场景中的光照和遮挡问题，提升了深度估计的鲁棒性和准确性。

Abstract: We propose a self-supervised monocular depth estimation network tailored for
endoscopic scenes, aiming to infer depth within the gastrointestinal tract from
monocular images. Existing methods, though accurate, typically assume
consistent illumination, which is often violated due to dynamic lighting and
occlusions caused by GI motility. These variations lead to incorrect geometric
interpretations and unreliable self-supervised signals, degrading depth
reconstruction quality. To address this, we introduce an occlusion-aware
self-supervised framework. First, we incorporate an occlusion mask for data
augmentation, generating pseudo-labels by simulating viewpoint-dependent
occlusion scenarios. This enhances the model's ability to learn robust depth
features under partial visibility. Second, we leverage semantic segmentation
guided by non-negative matrix factorization, clustering convolutional
activations to generate pseudo-labels in texture-deprived regions, thereby
improving segmentation accuracy and mitigating information loss from lighting
changes. Experimental results on the SCARED dataset show that our method
achieves state-of-the-art performance in self-supervised depth estimation.
Additionally, evaluations on the Endo-SLAM and SERV-CT datasets demonstrate
strong generalization across diverse endoscopic environments.

</details>

### [184] [Tamper-evident Image using JPEG Fixed Points](https://arxiv.org/abs/2504.17594)
*Zhaofeng Si,Siwei Lyu*

Main category: cs.CV

TLDR: JPEG压缩过程中存在固定点，迭代几次后图像不再变化，这些固定点视觉质量高且失真小，可用于开发防篡改图像技术。


<details>
  <summary>Details</summary>
Motivation: 研究JPEG压缩中的固定点现象，并探索其在图像防篡改中的应用。

Method: 分析JPEG压缩与解压过程，证明固定点的存在及其快速收敛性。

Result: 固定点存在且多样，视觉质量高，失真小，可用于防篡改图像生成。

Conclusion: JPEG固定点现象为图像防篡改提供了新方法。

Abstract: An intriguing phenomenon about JPEG compression has been observed since two
decades ago- after repeating JPEG compression and decompression, it leads to a
stable image that does not change anymore, which is a fixed point. In this
work, we prove the existence of fixed points in the essential JPEG procedures.
We analyze JPEG compression and decompression processes, revealing the
existence of fixed points that can be reached within a few iterations. These
fixed points are diverse and preserve the image's visual quality, ensuring
minimal distortion. This result is used to develop a method to create a
tamper-evident image from the original authentic image, which can expose
tampering operations by showing deviations from the fixed point image.

</details>

### [185] [RGB-D Tracking via Hierarchical Modality Aggregation and Distribution Network](https://arxiv.org/abs/2504.17595)
*Boyue Xu,Yi Xu,Ruichao Hou,Jia Bei,Tongwei Ren,Gangshan Wu*

Main category: cs.CV

TLDR: HMAD网络通过层次化特征融合提升RGB-D跟踪的鲁棒性和效率，实验证明其性能优越且适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 当前RGB-D跟踪器效率低且仅关注单层特征，导致融合鲁棒性差且速度慢，无法满足实际需求。

Method: 提出HMAD网络，利用RGB和深度模态的独特特征表示能力，采用层次化特征分布与融合策略。

Result: 在多个RGB-D数据集上实现最优性能，并在实时场景中有效应对多种跟踪挑战。

Conclusion: HMAD通过层次化模态聚合与分布显著提升了RGB-D跟踪的鲁棒性和实时性。

Abstract: The integration of dual-modal features has been pivotal in advancing
RGB-Depth (RGB-D) tracking. However, current trackers are less efficient and
focus solely on single-level features, resulting in weaker robustness in fusion
and slower speeds that fail to meet the demands of real-world applications. In
this paper, we introduce a novel network, denoted as HMAD (Hierarchical
Modality Aggregation and Distribution), which addresses these challenges. HMAD
leverages the distinct feature representation strengths of RGB and depth
modalities, giving prominence to a hierarchical approach for feature
distribution and fusion, thereby enhancing the robustness of RGB-D tracking.
Experimental results on various RGB-D datasets demonstrate that HMAD achieves
state-of-the-art performance. Moreover, real-world experiments further validate
HMAD's capacity to effectively handle a spectrum of tracking challenges in
real-time scenarios.

</details>

### [186] [Enhancing CNNs robustness to occlusions with bioinspired filters for border completion](https://arxiv.org/abs/2504.17619)
*Catarina P. Coutinho,Aneeqa Merhab,Janko Petkovic,Ferdinando Zanchetta,Rita Fioresi*

Main category: cs.CV

TLDR: 通过模拟视觉皮层边界完成机制设计CNN滤波器，改进LeNet 5在遮挡MNIST图像上的准确性。


<details>
  <summary>Details</summary>
Motivation: 利用视觉皮层的数学建模提升CNN性能，尤其是在图像遮挡情况下。

Method: 基于视觉皮层边界完成机制设计自定义滤波器，应用于改进的LeNet 5。

Result: 在遮挡MNIST图像测试中，准确性有显著提升。

Conclusion: 视觉皮层机制建模为CNN滤波器设计提供了有效方法，尤其在处理遮挡图像时表现优越。

Abstract: We exploit the mathematical modeling of the visual cortex mechanism for
border completion to define custom filters for CNNs. We see a consistent
improvement in performance, particularly in accuracy, when our modified LeNet 5
is tested with occluded MNIST images.

</details>

### [187] [Improving Open-World Object Localization by Discovering Background](https://arxiv.org/abs/2504.17626)
*Ashish Singh,Michael J. Jones,Kuan-Chuan Peng,Anoop Cherian,Moitreya Chatterjee,Erik Learned-Miller*

Main category: cs.CV

TLDR: 提出了一种利用背景信息指导目标定位的新框架，通过发现图像中的背景区域并训练目标提议网络不在这些区域检测目标，显著提升了开放世界目标定位的性能。


<details>
  <summary>Details</summary>
Motivation: 解决开放世界目标定位问题，即在训练时仅使用有限类别边界框信息的情况下，在推理时定位所有类别（包括未见类别）的目标。

Method: 提出了一种新颖框架，通过识别图像中信息量低的冗余区域（背景），训练目标提议网络避免在这些区域检测目标。

Result: 在标准基准测试中，该方法显著优于现有最先进方法。

Conclusion: 通过利用背景信息指导目标定位，该方法在开放世界目标定位任务中取得了显著改进。

Abstract: Our work addresses the problem of learning to localize objects in an
open-world setting, i.e., given the bounding box information of a limited
number of object classes during training, the goal is to localize all objects,
belonging to both the training and unseen classes in an image, during
inference. Towards this end, recent work in this area has focused on improving
the characterization of objects either explicitly by proposing new objective
functions (localization quality) or implicitly using object-centric
auxiliary-information, such as depth information, pixel/region affinity map
etc. In this work, we address this problem by incorporating background
information to guide the learning of the notion of objectness. Specifically, we
propose a novel framework to discover background regions in an image and train
an object proposal network to not detect any objects in these regions. We
formulate the background discovery task as that of identifying image regions
that are not discriminative, i.e., those that are redundant and constitute low
information content. We conduct experiments on standard benchmarks to showcase
the effectiveness of our proposed approach and observe significant improvements
over the previous state-of-the-art approaches for this task.

</details>

### [188] [A Guide to Structureless Visual Localization](https://arxiv.org/abs/2504.17636)
*Vojtech Panek,Qunjie Zhou,Yaqing Ding,Sérgio Agostinho,Zuzana Kukelova,Torsten Sattler,Laura Leal-Taixé*

Main category: cs.CV

TLDR: 本文首次全面讨论和比较了无结构视觉定位方法，发现基于经典几何推理的方法在姿态精度上优于基于姿态回归的方法，但灵活性较高。


<details>
  <summary>Details</summary>
Motivation: 现有基于结构的视觉定位方法虽精度高但灵活性不足，而无结构方法更易更新但研究较少，本文旨在填补这一空白。

Method: 通过实验比较多种无结构方法，包括基于经典绝对或半广义相对姿态估计的方法与基于姿态回归的方法。

Result: 基于经典几何推理的方法在姿态精度上显著优于基于姿态回归的方法，但与基于结构的方法相比精度略低。

Conclusion: 无结构方法在灵活性和精度之间存在权衡，为未来研究提供了有趣的方向。

Abstract: Visual localization algorithms, i.e., methods that estimate the camera pose
of a query image in a known scene, are core components of many applications,
including self-driving cars and augmented / mixed reality systems.
State-of-the-art visual localization algorithms are structure-based, i.e., they
store a 3D model of the scene and use 2D-3D correspondences between the query
image and 3D points in the model for camera pose estimation. While such
approaches are highly accurate, they are also rather inflexible when it comes
to adjusting the underlying 3D model after changes in the scene. Structureless
localization approaches represent the scene as a database of images with known
poses and thus offer a much more flexible representation that can be easily
updated by adding or removing images. Although there is a large amount of
literature on structure-based approaches, there is significantly less work on
structureless methods. Hence, this paper is dedicated to providing the, to the
best of our knowledge, first comprehensive discussion and comparison of
structureless methods. Extensive experiments show that approaches that use a
higher degree of classical geometric reasoning generally achieve higher pose
accuracy. In particular, approaches based on classical absolute or
semi-generalized relative pose estimation outperform very recent methods based
on pose regression by a wide margin. Compared with state-of-the-art
structure-based approaches, the flexibility of structureless methods comes at
the cost of (slightly) lower pose accuracy, indicating an interesting direction
for future work.

</details>

### [189] [CLIPSE -- a minimalistic CLIP-based image search engine for research](https://arxiv.org/abs/2504.17643)
*Steve Göring*

Main category: cs.CV

TLDR: CLIPSE是一个基于CLIP嵌入的自托管图像搜索引擎，适用于研究场景，设计简单且易于扩展。


<details>
  <summary>Details</summary>
Motivation: 为研究领域提供一个简单高效的图像搜索解决方案。

Method: 使用CLIP嵌入处理图像和文本查询，设计简洁框架支持扩展。

Result: 在小型数据集上表现良好，大型数据集需分布式部署。

Conclusion: CLIPSE适用于小型数据集，大型数据集需分布式优化。

Abstract: A brief overview of CLIPSE, a self-hosted image search engine with the main
application of research, is provided. In general, CLIPSE uses CLIP embeddings
to process the images and also the text queries. The overall framework is
designed with simplicity to enable easy extension and usage. Two benchmark
scenarios are described and evaluated, covering indexing and querying time. It
is shown that CLIPSE is capable of handling smaller datasets; for larger
datasets, a distributed approach with several instances should be considered.

</details>

### [190] [DiMeR: Disentangled Mesh Reconstruction Model](https://arxiv.org/abs/2504.17670)
*Lutao Jiang,Jiantao Lin,Kanghao Chen,Wenhang Ge,Xin Yang,Yifan Jiang,Yuanhuiyi Lyu,Xu Zheng,Yingcong Chen*

Main category: cs.CV

TLDR: DiMeR是一种新型的双流解耦模型，用于稀疏视图网格重建，通过分离几何和纹理输入及框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: RGB图像在几何重建中可能导致训练目标冲突且缺乏清晰度，因此需要一种更有效的方法。

Method: DiMeR将输入和框架解耦为几何和纹理两部分，几何分支使用法线图作为输入，纹理分支使用RGB图像，并改进了网格提取算法。

Result: DiMeR在稀疏视图重建、单图像到3D和文本到3D任务中表现优异，Chamfer Distance在GSO和OmniObject3D数据集上提升超过30%。

Conclusion: DiMeR通过解耦设计和法线图输入，显著提高了网格重建的准确性和效率。

Abstract: With the advent of large-scale 3D datasets, feed-forward 3D generative
models, such as the Large Reconstruction Model (LRM), have gained significant
attention and achieved remarkable success. However, we observe that RGB images
often lead to conflicting training objectives and lack the necessary clarity
for geometry reconstruction. In this paper, we revisit the inductive biases
associated with mesh reconstruction and introduce DiMeR, a novel disentangled
dual-stream feed-forward model for sparse-view mesh reconstruction. The key
idea is to disentangle both the input and framework into geometry and texture
parts, thereby reducing the training difficulty for each part according to the
Principle of Occam's Razor. Given that normal maps are strictly consistent with
geometry and accurately capture surface variations, we utilize normal maps as
exclusive input for the geometry branch to reduce the complexity between the
network's input and output. Moreover, we improve the mesh extraction algorithm
to introduce 3D ground truth supervision. As for texture branch, we use RGB
images as input to obtain the textured mesh. Overall, DiMeR demonstrates robust
capabilities across various tasks, including sparse-view reconstruction,
single-image-to-3D, and text-to-3D. Numerous experiments show that DiMeR
significantly outperforms previous methods, achieving over 30% improvement in
Chamfer Distance on the GSO and OmniObject3D dataset.

</details>

### [191] [PICO: Reconstructing 3D People In Contact with Objects](https://arxiv.org/abs/2504.17695)
*Alpár Cseke,Shashank Tripathi,Sai Kumar Dwivedi,Arjun Lakshmipathy,Agniv Chatterjee,Michael J. Black,Dimitrios Tzionas*

Main category: cs.CV

TLDR: 该论文提出了一种从单张彩色图像中恢复3D人-物交互（HOI）的新方法，通过构建新数据集PICO-db和优化方法PICO-fit，解决了深度模糊、遮挡和物体多样性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在受控环境下工作，限制了泛化能力。论文旨在开发适用于自然图像和新型物体类的方法。

Method: 1. 构建PICO-db数据集，利用视觉基础模型和最小人工输入标注密集3D接触；2. 提出PICO-fit优化方法，通过渲染-比较迭代拟合3D模型。

Result: PICO-fit能够处理多种物体类别，优于现有方法。

Conclusion: 该方法推动了HOI在自然场景中的广泛应用，数据和代码已开源。

Abstract: Recovering 3D Human-Object Interaction (HOI) from single color images is
challenging due to depth ambiguities, occlusions, and the huge variation in
object shape and appearance. Thus, past work requires controlled settings such
as known object shapes and contacts, and tackles only limited object classes.
Instead, we need methods that generalize to natural images and novel object
classes. We tackle this in two main ways: (1) We collect PICO-db, a new dataset
of natural images uniquely paired with dense 3D contact on both body and object
meshes. To this end, we use images from the recent DAMON dataset that are
paired with contacts, but these contacts are only annotated on a canonical 3D
body. In contrast, we seek contact labels on both the body and the object. To
infer these given an image, we retrieve an appropriate 3D object mesh from a
database by leveraging vision foundation models. Then, we project DAMON's body
contact patches onto the object via a novel method needing only 2 clicks per
patch. This minimal human input establishes rich contact correspondences
between bodies and objects. (2) We exploit our new dataset of contact
correspondences in a novel render-and-compare fitting method, called PICO-fit,
to recover 3D body and object meshes in interaction. PICO-fit infers contact
for the SMPL-X body, retrieves a likely 3D object mesh and contact from PICO-db
for that object, and uses the contact to iteratively fit the 3D body and object
meshes to image evidence via optimization. Uniquely, PICO-fit works well for
many object categories that no existing method can tackle. This is crucial to
enable HOI understanding to scale in the wild. Our data and code are available
at https://pico.is.tue.mpg.de.

</details>

### [192] [Hierarchical and Multimodal Data for Daily Activity Understanding](https://arxiv.org/abs/2504.17696)
*Ghazal Kaviani,Yavuz Yarici,Seulgi Kim,Mohit Prabhushankar,Ghassan AlRegib,Mashhour Solh,Ameya Patil*

Main category: cs.CV

TLDR: DARai是一个多模态、分层标注的数据集，用于理解真实环境中的人类活动，包含50名参与者在10种环境中的200多小时数据，标注分为三个层次。实验展示了其在人本应用中的价值。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够捕捉人类活动复杂性的数据集，以支持人工智能在真实场景中的应用。

Method: 通过多传感器（如摄像头、IMU、EMG等）记录数据，并分为三个层次（L1独立任务、L2共享动作、L3详细步骤）进行标注。

Result: 实验验证了DARai在识别、时间定位和未来动作预测中的价值，并揭示了单个传感器的局限性。

Conclusion: DARai为研究人类活动提供了丰富资源，其多传感器和反事实设计为未来研究提供了新方向。

Abstract: Daily Activity Recordings for Artificial Intelligence (DARai, pronounced
"Dahr-ree") is a multimodal, hierarchically annotated dataset constructed to
understand human activities in real-world settings. DARai consists of
continuous scripted and unscripted recordings of 50 participants in 10
different environments, totaling over 200 hours of data from 20 sensors
including multiple camera views, depth and radar sensors, wearable inertial
measurement units (IMUs), electromyography (EMG), insole pressure sensors,
biomonitor sensors, and gaze tracker.
  To capture the complexity in human activities, DARai is annotated at three
levels of hierarchy: (i) high-level activities (L1) that are independent tasks,
(ii) lower-level actions (L2) that are patterns shared between activities, and
(iii) fine-grained procedures (L3) that detail the exact execution steps for
actions. The dataset annotations and recordings are designed so that 22.7% of
L2 actions are shared between L1 activities and 14.2% of L3 procedures are
shared between L2 actions. The overlap and unscripted nature of DARai allows
counterfactual activities in the dataset.
  Experiments with various machine learning models showcase the value of DARai
in uncovering important challenges in human-centered applications.
Specifically, we conduct unimodal and multimodal sensor fusion experiments for
recognition, temporal localization, and future action anticipation across all
hierarchical annotation levels. To highlight the limitations of individual
sensors, we also conduct domain-variant experiments that are enabled by DARai's
multi-sensor and counterfactual activity design setup.
  The code, documentation, and dataset are available at the dedicated DARai
website:
https://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/

</details>

### [193] [Generative Fields: Uncovering Hierarchical Feature Control for StyleGAN via Inverted Receptive Fields](https://arxiv.org/abs/2504.17712)
*Zhuo He,Paul Henderson,Nicolas Pugeault*

Main category: cs.CV

TLDR: 论文提出了一种基于生成场理论的新方法，改进了StyleGAN的图像编辑能力，通过通道风格潜在空间S实现了对特征合成的解耦控制。


<details>
  <summary>Details</summary>
Motivation: 解决StyleGAN在生成图像时特征控制困难的问题，尤其是低维潜在空间的强纠缠性限制了生成图像的可控性。

Method: 引入生成场理论解释StyleGAN的分层特征合成，并提出基于通道风格潜在空间S的图像编辑流程，利用CNN的内在结构特征实现特征解耦控制。

Result: 新方法能够更直接地控制特征合成，避免了W空间的限制和预训练的需求。

Conclusion: 生成场理论和通道风格潜在空间S为StyleGAN的图像编辑提供了更灵活和高效的工具。

Abstract: StyleGAN has demonstrated the ability of GANs to synthesize highly-realistic
faces of imaginary people from random noise. One limitation of GAN-based image
generation is the difficulty of controlling the features of the generated
image, due to the strong entanglement of the low-dimensional latent space.
Previous work that aimed to control StyleGAN with image or text prompts
modulated sampling in W latent space, which is more expressive than Z latent
space. However, W space still has restricted expressivity since it does not
control the feature synthesis directly; also the feature embedding in W space
requires a pre-training process to reconstruct the style signal, limiting its
application. This paper introduces the concept of "generative fields" to
explain the hierarchical feature synthesis in StyleGAN, inspired by the
receptive fields of convolution neural networks (CNNs). Additionally, we
propose a new image editing pipeline for StyleGAN using generative field theory
and the channel-wise style latent space S, utilizing the intrinsic structural
feature of CNNs to achieve disentangled control of feature synthesis at
synthesis time.

</details>

### [194] [DPMambaIR:All-in-One Image Restoration via Degradation-Aware Prompt State Space Model](https://arxiv.org/abs/2504.17732)
*Zhanwen Liu,Sai Zhou,Yuchao Dai,Yang Wang,Yisheng An,Xiangmo Zhao*

Main category: cs.CV

TLDR: DPMambaIR是一个新型的All-in-One图像修复框架，通过结合Degradation-Aware Prompt State Space Model（DP-SSM）和High-Frequency Enhancement Block（HEB），实现了对复杂退化信息的细粒度建模和高效全局整合，同时减少了多任务竞争导致的高频细节损失。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要为每种退化类型设计专用模型，训练成本和部署复杂度高。现有方法缺乏对退化信息的细粒度建模，且难以平衡多任务冲突。

Method: 提出DPMambaIR框架，整合DP-SSM和HEB。DP-SSM利用预训练的退化提取器捕获细粒度退化特征，动态整合到状态空间建模中；HEB补充高频信息。

Result: 在包含七种退化类型的混合数据集上，DPMambaIR取得了最佳性能（PSNR 27.69dB，SSIM 0.893）。

Conclusion: DPMambaIR展示了作为All-in-One图像修复统一解决方案的潜力和优越性。

Abstract: All-in-One image restoration aims to address multiple image degradation
problems using a single model, significantly reducing training costs and
deployment complexity compared to traditional methods that design dedicated
models for each degradation type. Existing approaches typically rely on
Degradation-specific models or coarse-grained degradation prompts to guide
image restoration. However, they lack fine-grained modeling of degradation
information and face limitations in balancing multi-task conflicts. To overcome
these limitations, we propose DPMambaIR, a novel All-in-One image restoration
framework. By integrating a Degradation-Aware Prompt State Space Model (DP-SSM)
and a High-Frequency Enhancement Block (HEB), DPMambaIR enables fine-grained
modeling of complex degradation information and efficient global integration,
while mitigating the loss of high-frequency details caused by task competition.
Specifically, the DP-SSM utilizes a pre-trained degradation extractor to
capture fine-grained degradation features and dynamically incorporates them
into the state space modeling process, enhancing the model's adaptability to
diverse degradation types. Concurrently, the HEB supplements high-frequency
information, effectively addressing the loss of critical details, such as edges
and textures, in multi-task image restoration scenarios. Extensive experiments
on a mixed dataset containing seven degradation types show that DPMambaIR
achieves the best performance, with 27.69dB and 0.893 in PSNR and SSIM,
respectively. These results highlight the potential and superiority of
DPMambaIR as a unified solution for All-in-One image restoration.

</details>

### [195] [EgoCHARM: Resource-Efficient Hierarchical Activity Recognition using an Egocentric IMU Sensor](https://arxiv.org/abs/2504.17735)
*Akhil Padmanabha,Saravanan Govindarajan,Hwanmun Kim,Sergio Ortiz,Rahul Rajan,Doruk Senkal,Sneha Kadetotad*

Main category: cs.CV

TLDR: 论文提出了一种资源高效的机器学习算法EgoCHARM，用于通过头戴式IMU识别高低级活动，采用半监督学习策略，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的头戴式活动识别方法性能低或资源消耗大，需要一种更高效的解决方案。

Method: 提出EgoCHARM算法，采用分层结构和半监督学习策略，仅需高级活动标签训练，生成可泛化的低级运动嵌入。

Result: 在9种高级和3种低级活动上，分别取得0.826和0.855的F1分数，模型参数仅63k和22k。

Conclusion: EgoCHARM高效且适用于当前IMU芯片，同时分析了头戴式IMU活动识别的机会与限制。

Abstract: Human activity recognition (HAR) on smartglasses has various use cases,
including health/fitness tracking and input for context-aware AI assistants.
However, current approaches for egocentric activity recognition suffer from low
performance or are resource-intensive. In this work, we introduce a resource
(memory, compute, power, sample) efficient machine learning algorithm,
EgoCHARM, for recognizing both high level and low level activities using a
single egocentric (head-mounted) Inertial Measurement Unit (IMU). Our
hierarchical algorithm employs a semi-supervised learning strategy, requiring
primarily high level activity labels for training, to learn generalizable low
level motion embeddings that can be effectively utilized for low level activity
recognition. We evaluate our method on 9 high level and 3 low level activities
achieving 0.826 and 0.855 F1 scores on high level and low level activity
recognition respectively, with just 63k high level and 22k low level model
parameters, allowing the low level encoder to be deployed directly on current
IMU chips with compute. Lastly, we present results and insights from a
sensitivity analysis and highlight the opportunities and limitations of
activity recognition using egocentric IMUs.

</details>

### [196] [Step1X-Edit: A Practical Framework for General Image Editing](https://arxiv.org/abs/2504.17761)
*Shiyu Liu,Yucheng Han,Peng Xing,Fukun Yin,Rui Wang,Wei Cheng,Jiaqi Liao,Yingming Wang,Honghao Fu,Chunrui Han,Guopeng Li,Yuang Peng,Quan Sun,Jingwei Wu,Yan Cai,Zheng Ge,Ranchen Ming,Lei Xia,Xianfang Zeng,Yibo Zhu,Binxing Jiao,Xiangyu Zhang,Gang Yu,Daxin Jiang*

Main category: cs.CV

TLDR: 本文提出了一种名为Step1X-Edit的开源图像编辑模型，旨在缩小与闭源模型（如GPT-4o和Gemini2 Flash）的差距，并在性能上接近这些领先模型。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态模型在图像编辑领域取得了显著进展，但开源算法与闭源模型之间仍存在较大差距。本文旨在填补这一空白。

Method: 采用多模态LLM处理参考图像和用户编辑指令，提取潜在嵌入并与扩散图像解码器结合生成目标图像。通过数据生成管道构建高质量训练数据集。

Result: 实验结果表明，Step1X-Edit在GEdit-Bench上显著优于现有开源基线，并接近领先闭源模型的性能。

Conclusion: Step1X-Edit为图像编辑领域做出了重要贡献，推动了开源模型的发展。

Abstract: In recent years, image editing models have witnessed remarkable and rapid
development. The recent unveiling of cutting-edge multimodal models such as
GPT-4o and Gemini2 Flash has introduced highly promising image editing
capabilities. These models demonstrate an impressive aptitude for fulfilling a
vast majority of user-driven editing requirements, marking a significant
advancement in the field of image manipulation. However, there is still a large
gap between the open-source algorithm with these closed-source models. Thus, in
this paper, we aim to release a state-of-the-art image editing model, called
Step1X-Edit, which can provide comparable performance against the closed-source
models like GPT-4o and Gemini2 Flash. More specifically, we adopt the
Multimodal LLM to process the reference image and the user's editing
instruction. A latent embedding has been extracted and integrated with a
diffusion image decoder to obtain the target image. To train the model, we
build a data generation pipeline to produce a high-quality dataset. For
evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world
user instructions. Experimental results on GEdit-Bench demonstrate that
Step1X-Edit outperforms existing open-source baselines by a substantial margin
and approaches the performance of leading proprietary models, thereby making
significant contributions to the field of image editing.

</details>

### [197] [The Fourth Monocular Depth Estimation Challenge](https://arxiv.org/abs/2504.17787)
*Anton Obukhov,Matteo Poggi,Fabio Tosi,Ripudaman Singh Arora,Jaime Spencer,Chris Russell,Simon Hadfield,Richard Bowden,Shuaihang Wang,Zhenxin Ma,Weijie Chen,Baobei Xu,Fengyu Sun,Di Xie,Jiang Zhu,Mykola Lavreniuk,Haining Guan,Qun Wu,Yupei Zeng,Chao Lu,Huanran Wang,Guangyuan Zhou,Haotian Zhang,Jianxiong Wang,Qiang Rao,Chunjie Wang,Xiao Liu,Zhiqiang Lou,Hualie Jiang,Yihao Chen,Rui Xu,Minglang Tan,Zihan Qin,Yifan Mao,Jiayang Liu,Jialei Xu,Yifan Yang,Wenbo Zhao,Junjun Jiang,Xianming Liu,Mingshuai Zhao,Anlong Ming,Wu Chen,Feng Xue,Mengying Yu,Shida Gao,Xiangfeng Wang,Gbenga Omotara,Ramy Farag,Jacket Demby,Seyed Mohamad Ali Tousi,Guilherme N DeSouza,Tuan-Anh Yang,Minh-Quang Nguyen,Thien-Phuc Tran,Albert Luginov,Muhammad Shahzad*

Main category: cs.CV

TLDR: 第四届单目深度估计挑战赛（MDEC）聚焦于零样本泛化能力，改进评估协议并引入新基线方法，最终提交结果优于基线，最佳方法提升了3D F-Score。


<details>
  <summary>Details</summary>
Motivation: 挑战赛旨在测试单目深度估计方法在SYNS-Patches数据集上的零样本泛化能力，改进评估协议以支持更灵活的预测对齐。

Method: 修订评估协议，采用最小二乘对齐（两自由度），引入Depth Anything v2和Marigold作为基线方法，接收24份优于基线的提交。

Result: 10份提交附带报告，多数领先方法依赖仿射不变预测，最佳结果将3D F-Score从22.58%提升至23.05%。

Conclusion: 挑战赛展示了单目深度估计方法的进步，仿射不变预测成为关键，未来可进一步优化泛化能力。

Abstract: This paper presents the results of the fourth edition of the Monocular Depth
Estimation Challenge (MDEC), which focuses on zero-shot generalization to the
SYNS-Patches benchmark, a dataset featuring challenging environments in both
natural and indoor settings. In this edition, we revised the evaluation
protocol to use least-squares alignment with two degrees of freedom to support
disparity and affine-invariant predictions. We also revised the baselines and
included popular off-the-shelf methods: Depth Anything v2 and Marigold. The
challenge received a total of 24 submissions that outperformed the baselines on
the test set; 10 of these included a report describing their approach, with
most leading methods relying on affine-invariant predictions. The challenge
winners improved the 3D F-Score over the previous edition's best result,
raising it from 22.58% to 23.05%.

</details>

### [198] [Dynamic Camera Poses and Where to Find Them](https://arxiv.org/abs/2504.17788)
*Chris Rockwell,Joseph Tung,Tsung-Yi Lin,Ming-Yu Liu,David F. Fouhey,Chen-Hsuan Lin*

Main category: cs.CV

TLDR: DynPose-100K是一个大规模动态互联网视频数据集，标注了相机姿态，解决了现有方法的局限性，并展示了其多样性和规模。


<details>
  <summary>Details</summary>
Motivation: 动态互联网视频的相机姿态标注对视频生成和模拟等领域至关重要，但现有数据集难以满足需求。

Method: 结合任务特定和通用模型进行过滤，并利用点跟踪、动态掩码和运动结构技术进行姿态估计。

Result: DynPose-100K数据集规模大且多样，优于现有方法。

Conclusion: 该数据集为下游应用提供了新的可能性。

Abstract: Annotating camera poses on dynamic Internet videos at scale is critical for
advancing fields like realistic video generation and simulation. However,
collecting such a dataset is difficult, as most Internet videos are unsuitable
for pose estimation. Furthermore, annotating dynamic Internet videos present
significant challenges even for state-of-theart methods. In this paper, we
introduce DynPose-100K, a large-scale dataset of dynamic Internet videos
annotated with camera poses. Our collection pipeline addresses filtering using
a carefully combined set of task-specific and generalist models. For pose
estimation, we combine the latest techniques of point tracking, dynamic
masking, and structure-from-motion to achieve improvements over the
state-of-the-art approaches. Our analysis and experiments demonstrate that
DynPose-100K is both large-scale and diverse across several key attributes,
opening up avenues for advancements in various downstream applications.

</details>

### [199] [Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models](https://arxiv.org/abs/2504.17789)
*Xu Ma,Peize Sun,Haoyu Ma,Hao Tang,Chih-Yao Ma,Jialiang Wang,Kunpeng Li,Xiaoliang Dai,Yujun Shi,Xuan Ju,Yushi Hu,Artsiom Sanakoyeu,Felix Juefei-Xu,Ji Hou,Junjiao Tian,Tao Xu,Tingbo Hou,Yen-Cheng Liu,Zecheng He,Zijian He,Matt Feiszli,Peizhao Zhang,Peter Vajda,Sam Tsai,Yun Fu*

Main category: cs.CV

TLDR: 论文提出了一种名为Token-Shuffle的新方法，通过减少Transformer中的图像标记数量，解决了自回归模型在图像合成中的效率问题，并首次将AR文本到图像生成的分辨率提升至2048x2048。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在图像合成中因需要大量图像标记而效率低下，限制了训练和推理效率以及图像分辨率。本文旨在解决这一问题。

Method: 提出Token-Shuffle方法，通过合并局部标记和恢复空间排列，减少输入标记数量，同时保持高效训练和推理。

Result: 在GenAI-benchmark中，2.7B模型在困难提示下得分0.77，优于AR模型LlamaGen和扩散模型LDM。

Conclusion: Token-Shuffle为MLLMs中高效高分辨率图像生成提供了基础设计，展示了显著的生成能力。

Abstract: Autoregressive (AR) models, long dominant in language generation, are
increasingly applied to image synthesis but are often considered less
competitive than Diffusion-based models. A primary limitation is the
substantial number of image tokens required for AR models, which constrains
both training and inference efficiency, as well as image resolution. To address
this, we present Token-Shuffle, a novel yet simple method that reduces the
number of image tokens in Transformer. Our key insight is the dimensional
redundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),
where low-dimensional visual codes from visual encoder are directly mapped to
high-dimensional language vocabularies. Leveraging this, we consider two key
operations: token-shuffle, which merges spatially local tokens along channel
dimension to decrease the input token number, and token-unshuffle, which
untangles the inferred tokens after Transformer blocks to restore the spatial
arrangement for output. Jointly training with textual prompts, our strategy
requires no additional pretrained text-encoder and enables MLLMs to support
extremely high-resolution image synthesis in a unified next-token prediction
way while maintaining efficient training and inference. For the first time, we
push the boundary of AR text-to-image generation to a resolution of 2048x2048
with gratifying generation performance. In GenAI-benchmark, our 2.7B model
achieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen
by 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human
evaluations also demonstrate our prominent image generation ability in terms of
text-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle
can serve as a foundational design for efficient high-resolution image
generation within MLLMs.

</details>

### [200] [LiDPM: Rethinking Point Diffusion for Lidar Scene Completion](https://arxiv.org/abs/2504.17791)
*Tetiana Martyniuk,Gilles Puy,Alexandre Boulch,Renaud Marlet,Raoul de Charette*

Main category: cs.CV

TLDR: 论文提出了一种基于扩散模型的方法LiDPM，用于直接处理激光雷达点云数据，解决了在室外场景中生成细粒度细节的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于在广阔视野中从白噪声生成细粒度细节的困难，训练直接处理激光雷达点云的扩散模型具有挑战性。

Method: 通过重新定义原始DDPM为局部扩散过程，并证明无需近似即可在场景级别操作，选择适当的起点即可完成场景补全。

Result: 在SemanticKITTI数据集上，LiDPM方法在场景补全任务中表现优于现有方法。

Conclusion: LiDPM方法通过优化扩散模型的起点，实现了在场景级别的高效补全，填补了现有方法的空白。

Abstract: Training diffusion models that work directly on lidar points at the scale of
outdoor scenes is challenging due to the difficulty of generating fine-grained
details from white noise over a broad field of view. The latest works
addressing scene completion with diffusion models tackle this problem by
reformulating the original DDPM as a local diffusion process. It contrasts with
the common practice of operating at the level of objects, where vanilla DDPMs
are currently used. In this work, we close the gap between these two lines of
work. We identify approximations in the local diffusion formulation, show that
they are not required to operate at the scene level, and that a vanilla DDPM
with a well-chosen starting point is enough for completion. Finally, we
demonstrate that our method, LiDPM, leads to better results in scene completion
on SemanticKITTI. The project page is https://astra-vision.github.io/LiDPM .

</details>

<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [201] [Breaking the Flow and the Bank: Stealthy Cyberattacks on Water Network Hydraulics](https://arxiv.org/abs/2504.17211)
*Abdallah Alalem Albustami,Ahmad F. Taha*

Main category: eess.SY

TLDR: 本文系统分析了针对供水网络的传感器攻击，探讨了攻击复杂性、系统知识需求与可实现影响之间的关系，并提出了多种攻击策略。


<details>
  <summary>Details</summary>
Motivation: 随着供水网络与数字基础设施的连接增加，其面临更多网络攻击威胁，尤其是隐蔽的虚假数据注入攻击（SFDIAs），现有研究未深入探讨攻击复杂性、系统知识需求与影响之间的关系。

Method: 通过分析物理约束、状态监测要求和入侵检测规避条件，提出多种攻击策略，包括满足物理和检测约束的定制策略及简单的测量操纵。

Result: 在Net1和Net3基准网络上进行案例研究，证明这些攻击能长期增加运营成本并改变水流，同时未被监测系统发现。

Conclusion: 研究为供水网络提供了漏洞评估的见解，并推动结合物理和统计安全机制的保护策略开发。

Abstract: As water distribution networks (WDNs) become increasingly connected with
digital infrastructures, they face greater exposure to cyberattacks that
threaten their operational integrity. Stealthy False Data Injection Attacks
(SFDIAs) are particularly concerning, as they manipulate sensor data to
compromise system operations while avoiding detection. While existing studies
have focused on either detection methods or specific attack formulations, the
relationship between attack sophistication, system knowledge requirements, and
achievable impact remains unexplored. This paper presents a systematic analysis
of sensor attacks against WDNs, investigating different combinations of
physical constraints, state monitoring requirements, and intrusion detection
evasion conditions. We propose several attack formulations that range from
tailored strategies satisfying both physical and detection constraints to
simpler measurement manipulations. The proposed attacks are simple and local --
requiring knowledge only of targeted sensors and their hydraulic connections --
making them scalable and practical. Through case studies on Net1 and Net3
benchmark networks, we demonstrate how these attacks can persistently increase
operational costs and alter water flows while remaining undetected by
monitoring systems for extended periods. The analysis provides utilities with
insights for vulnerability assessment and motivates the development of
protection strategies that combine physical and statistical security
mechanisms.

</details>

### [202] [Peer-Aware Cost Estimation in Nonlinear General-Sum Dynamic Games for Mutual Learning and Intent Inference](https://arxiv.org/abs/2504.17129)
*Seyed Yousef Soltanian,Wenlong Zhang*

Main category: eess.SY

TLDR: 提出了一种非线性同伴感知成本估计算法（N-PACE），用于解决不完全信息动态博弈中的均衡策略问题，通过迭代线性二次逼近和建模同伴学习动态，实现快速无偏学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设一个代理完全了解同伴，导致估计偏差和协调失败，N-PACE旨在解决这一问题。

Method: 使用迭代线性二次逼近非线性博弈，建模同伴学习动态并推断其目标函数。

Result: N-PACE实现了快速无偏学习，并支持意图通信。

Conclusion: N-PACE有效解决了动态博弈中的目标函数推断问题，提升了任务完成和安全性。

Abstract: Human-robot interactions can be modeled as incomplete-information general-sum
dynamic games since the objective functions of both agents are not explicitly
known to each other. However, solving for equilibrium policies for such games
presents a major challenge, especially if the games involve nonlinear
underlying dynamics. To simplify the problem, existing work often assumes that
one agent is an expert with complete information about its peer, which can lead
to biased estimates and failures in coordination. To address this challenge, we
propose a nonlinear peer-aware cost estimation (N-PACE) algorithm for
general-sum dynamic games. In N-PACE, using iterative linear quadratic (LQ)
approximation of the nonlinear general-sum game, each agent explicitly models
the learning dynamics of its peer agent while inferring their objective
functions, leading to unbiased fast learning in inferring the unknown objective
function of the peer agent, which is critical for task completion and safety
assurance. Additionally, we demonstrate how N-PACE enables \textbf{intent
communication} in such multi-agent systems by explicitly modeling the peer's
learning dynamics.

</details>

### [203] [PACE: A Framework for Learning and Control in Linear Incomplete-Information Differential Games](https://arxiv.org/abs/2504.17128)
*Seyed Yousef Soltanian,Wenlong Zhang*

Main category: eess.SY

TLDR: 提出了一种基于模型的Peer-Aware Cost Estimation（PACE）框架，用于解决信息不完全的线性二次微分博弈问题，通过实时学习对方成本参数来优化控制策略。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体控制、人机交互等领域中信息不完全的线性二次微分博弈问题，尤其是当双方均不了解对方成本函数时的复杂性。

Method: 提出PACE框架，将对方视为学习智能体而非静态最优智能体，建模其学习动态以推断其成本函数参数，并实时调整控制策略。

Result: 理论证明了参数估计的收敛性和系统状态的稳定性，数值研究表明PACE在稳定性和收敛速度上优于近似完全信息的方法。

Conclusion: PACE框架通过建模对方学习动态，有效解决了信息不完全的博弈问题，提升了稳定性和收敛速度。

Abstract: In this paper, we address the problem of a two-player linear quadratic
differential game with incomplete information, a scenario commonly encountered
in multi-agent control, human-robot interaction (HRI), and approximation
methods for solving general-sum differential games. While solutions to such
linear differential games are typically obtained through coupled Riccati
equations, the complexity increases when agents have incomplete information,
particularly when neither is aware of the other's cost function. To tackle this
challenge, we propose a model-based Peer-Aware Cost Estimation (PACE) framework
for learning the cost parameters of the other agent. In PACE, each agent treats
its peer as a learning agent rather than a stationary optimal agent, models
their learning dynamics, and leverages this dynamic to infer the cost function
parameters of the other agent. This approach enables agents to infer each
other's objective function in real time based solely on their previous state
observations and dynamically adapt their control policies. Furthermore, we
provide a theoretical guarantee for the convergence of parameter estimation and
the stability of system states in PACE. Additionally, in our numerical studies,
we demonstrate how modeling the learning dynamics of the other agent benefits
PACE, compared to approaches that approximate the other agent as having
complete information, particularly in terms of stability and convergence speed.

</details>

<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [204] [Fried Parameter Estimation from Single Wavefront Sensor Image with Artificial Neural Networks](https://arxiv.org/abs/2504.17029)
*Jeffrey Smith,Taisei Fujii,Jesse Craney,Charles Gretton*

Main category: astro-ph.IM

TLDR: 本文提出了一种基于机器学习的数据驱动方法，用于从单个波前传感器图像中估计Fried参数（r0），适用于自适应光学系统的实时控制。


<details>
  <summary>Details</summary>
Motivation: 地面望远镜的天文观测受大气湍流影响，导致图像模糊。自适应光学系统需要实时校正波前，而Fried参数是关键控制参数。

Method: 采用计算机视觉中的机器学习方法，通过单张Shack-Hartmann或金字塔波前传感器图像估计r0，并使用COMPASS AO仿真工具进行评估。

Result: 在多种条件下（如导星亮度、噪声、大气和仪器条件），该方法能准确估计r0，且适用于开环和闭环AO配置，推理时间仅为0.83ms。

Conclusion: 该方法为实时仪器控制提供了一种经济高效的解决方案。

Abstract: Atmospheric turbulence degrades the quality of astronomical observations in
ground-based telescopes, leading to distorted and blurry images. Adaptive
Optics (AO) systems are designed to counteract these effects, using atmospheric
measurements captured by a wavefront sensor to make real-time corrections to
the incoming wavefront. The Fried parameter, r0, characterises the strength of
atmospheric turbulence and is an essential control parameter for optimising the
performance of AO systems and more recently sky profiling for Free Space
Optical (FSO) communication channels. In this paper, we develop a novel
data-driven approach, adapting machine learning methods from computer vision
for Fried parameter estimation from a single Shack-Hartmann or pyramid
wavefront sensor image. Using these data-driven methods, we present a detailed
simulation-based evaluation of our approach using the open-source COMPASS AO
simulation tool to evaluate both the Shack-Hartmann and pyramid wavefront
sensors. Our evaluation is over a range of guide star magnitudes, and realistic
noise, atmospheric and instrument conditions. Remarkably, we are able to
develop a single network-based estimator that is accurate in both open and
closed-loop AO configurations. Our method accurately estimates the Fried
parameter from a single WFS image directly from AO telemetry to a few
millimetres. Our approach is suitable for real time control, exhibiting 0.83ms
r0 inference times on retail NVIDIA RTX 3090 GPU hardware, and thereby
demonstrating a compelling economic solution for use in real-time instrument
control.

</details>

<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [205] [Anatomy-constrained modelling of image-derived input functions in dynamic PET using multi-organ segmentation](https://arxiv.org/abs/2504.17114)
*Valentin Langer,Kartikay Tehlan,Thomas Wendler*

Main category: eess.IV

TLDR: 该研究提出了一种基于多器官分割的方法，整合主动脉、门静脉、肺动脉和输尿管的图像衍生输入函数（IDIFs），以提高动态PET中[$^{18}$F]FDG分布的动力学分析准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅从主动脉获取IDIFs，忽略了解剖变异和复杂血管贡献，限制了动力学分析的准确性。

Method: 利用高分辨率CT分割肝脏、肺、肾脏和膀胱，结合器官特异性血液供应来源，改进动力学建模。

Result: 在9名患者的动态[$^{18}$F]FDG PET数据中，肝脏和肺的平均平方误差（MSE）分别降低了13.39%和10.42%。

Conclusion: 多IDIFs方法有望改善解剖建模，推动示踪动力学建模在临床常规中的应用。

Abstract: Accurate kinetic analysis of [$^{18}$F]FDG distribution in dynamic positron
emission tomography (PET) requires anatomically constrained modelling of
image-derived input functions (IDIFs). Traditionally, IDIFs are obtained from
the aorta, neglecting anatomical variations and complex vascular contributions.
This study proposes a multi-organ segmentation-based approach that integrates
IDIFs from the aorta, portal vein, pulmonary artery, and ureters. Using
high-resolution CT segmentations of the liver, lungs, kidneys, and bladder, we
incorporate organ-specific blood supply sources to improve kinetic modelling.
Our method was evaluated on dynamic [$^{18}$F]FDG PET data from nine patients,
resulting in a mean squared error (MSE) reduction of $13.39\%$ for the liver
and $10.42\%$ for the lungs. These initial results highlight the potential of
multiple IDIFs in improving anatomical modelling and fully leveraging dynamic
PET imaging. This approach could facilitate the integration of tracer kinetic
modelling into clinical routine.

</details>

### [206] [Physiological neural representation for personalised tracer kinetic parameter estimation from dynamic PET](https://arxiv.org/abs/2504.17122)
*Kartikay Tehlan,Thomas Wendler*

Main category: eess.IV

TLDR: 提出了一种基于隐式神经表示（INRs）的个性化动力学参数估计方法，解决了传统方法计算量大和数据需求高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统动态PET的动力学参数估计方法计算量大且空间分辨率有限，而深度神经网络（DNNs）需要大量训练数据和计算资源。

Method: 采用隐式神经表示（INRs）学习连续函数，结合3D CT基础模型的解剖先验，实现高效、高分辨率的参数成像。

Result: 在[$^{18}$F]FDG动态PET/CT数据集上验证，结果显示更高的空间分辨率、更低的均方误差和更好的解剖一致性。

Conclusion: INRs在个性化、数据高效的示踪动力学建模中具有潜力，适用于肿瘤特征分析、分割和预后评估。

Abstract: Dynamic positron emission tomography (PET) with [$^{18}$F]FDG enables
non-invasive quantification of glucose metabolism through kinetic analysis,
often modelled by the two-tissue compartment model (TCKM). However, voxel-wise
kinetic parameter estimation using conventional methods is computationally
intensive and limited by spatial resolution. Deep neural networks (DNNs) offer
an alternative but require large training datasets and significant
computational resources. To address these limitations, we propose a
physiological neural representation based on implicit neural representations
(INRs) for personalized kinetic parameter estimation. INRs, which learn
continuous functions, allow for efficient, high-resolution parametric imaging
with reduced data requirements. Our method also integrates anatomical priors
from a 3D CT foundation model to enhance robustness and precision in kinetic
modelling. We evaluate our approach on an [$^{18}$F]FDG dynamic PET/CT dataset
and compare it to state-of-the-art DNNs. Results demonstrate superior spatial
resolution, lower mean-squared error, and improved anatomical consistency,
particularly in tumour and highly vascularized regions. Our findings highlight
the potential of INRs for personalized, data-efficient tracer kinetic
modelling, enabling applications in tumour characterization, segmentation, and
prognostic assessment.

</details>

### [207] [3D Deep-learning-based Segmentation of Human Skin Sweat Glands and Their 3D Morphological Response to Temperature Variations](https://arxiv.org/abs/2504.17255)
*Shaoyu Pei,Renxiong Wu,Hao Zheng,Lang Qin,Shuaichen Lin,Yuxing Gan,Wenjing Huang,Zhixuan Wang,Mohan Qin,Yong Liu,Guangming Ni*

Main category: eess.IV

TLDR: 提出了一种基于3D变换器的多目标分割框架，用于非侵入性、实时量化汗腺形态，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有汗腺形态观察方法多为二维、体外且破坏性，亟需实时、非侵入性技术。

Method: 结合滑动窗口、联合空间-通道注意力机制及浅深层异质性，实现OCT皮肤体积数据的精确3D分割。

Result: 首次可视化并量化了温度变化下汗腺3D形态的细微变化，建立了正常汗腺形态基准。

Conclusion: 该方法为汗腺结构研究提供了实时、非侵入性工具，推动了皮肤病学研究和临床应用。

Abstract: Skin, the primary regulator of heat exchange, relies on sweat glands for
thermoregulation. Alterations in sweat gland morphology play a crucial role in
various pathological conditions and clinical diagnoses. Current methods for
observing sweat gland morphology are limited by their two-dimensional, in
vitro, and destructive nature, underscoring the urgent need for real-time,
non-invasive, quantifiable technologies. We proposed a novel three-dimensional
(3D) transformer-based multi-object segmentation framework, integrating a
sliding window approach, joint spatial-channel attention mechanism, and
architectural heterogeneity between shallow and deep layers. Our proposed
network enables precise 3D sweat gland segmentation from skin volume data
captured by optical coherence tomography (OCT). For the first time, subtle
variations of sweat gland 3D morphology in response to temperature changes,
have been visualized and quantified. Our approach establishes a benchmark for
normal sweat gland morphology and provides a real-time, non-invasive tool for
quantifying 3D structural parameters. This enables the study of individual
variability and pathological changes in sweat gland structure, advancing
dermatological research and clinical applications, including thermoregulation
and bromhidrosis treatment.

</details>

### [208] [A Spatially-Aware Multiple Instance Learning Framework for Digital Pathology](https://arxiv.org/abs/2504.17379)
*Hassan Keshvarikhojasteh,Mihail Tifrea,Sibylle Hess,Josien P. W. Pluim,Mitko Veta*

Main category: eess.IV

TLDR: 论文提出了一种改进的ABMIL框架（GABMIL），通过显式建模实例间依赖关系，提升了病理图像分类性能，同时保持了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统MIL方法（如ABMIL）忽略了病理图像中关键的空间交互信息，而现有方法（如TransMIL）虽然引入空间上下文，但计算复杂度高。本文旨在探索在ABMIL中显式建模实例关系是否能带来性能提升。

Method: 提出GABMIL模型，通过集成交互感知表示来显式捕获实例间依赖关系，同时保持计算效率。

Result: 在乳腺癌和肺癌分型任务中，GABMIL相比ABMIL在AUPRC和Kappa分数上分别提升了7%和5%，且计算开销几乎不变。

Conclusion: 研究表明，在MIL框架中显式建模实例交互对性能提升至关重要，且无需牺牲计算效率。

Abstract: Multiple instance learning (MIL) is a promising approach for weakly
supervised classification in pathology using whole slide images (WSIs).
However, conventional MIL methods such as Attention-Based Deep Multiple
Instance Learning (ABMIL) typically disregard spatial interactions among
patches that are crucial to pathological diagnosis. Recent advancements, such
as Transformer based MIL (TransMIL), have incorporated spatial context and
inter-patch relationships. However, it remains unclear whether explicitly
modeling patch relationships yields similar performance gains in ABMIL, which
relies solely on Multi-Layer Perceptrons (MLPs). In contrast, TransMIL employs
Transformer-based layers, introducing a fundamental architectural shift at the
cost of substantially increased computational complexity. In this work, we
enhance the ABMIL framework by integrating interaction-aware representations to
address this question. Our proposed model, Global ABMIL (GABMIL), explicitly
captures inter-instance dependencies while preserving computational efficiency.
Experimental results on two publicly available datasets for tumor subtyping in
breast and lung cancers demonstrate that GABMIL achieves up to a 7 percentage
point improvement in AUPRC and a 5 percentage point increase in the Kappa score
over ABMIL, with minimal or no additional computational overhead. These
findings underscore the importance of incorporating patch interactions within
MIL frameworks.

</details>

### [209] [Beyond Labels: Zero-Shot Diabetic Foot Ulcer Wound Segmentation with Self-attention Diffusion Models and the Potential for Text-Guided Customization](https://arxiv.org/abs/2504.17628)
*Abderrachid Hamrani,Daniela Leizaola,Renato Sousa,Jose P. Ponce,Stanley Mathis,David G. Armstrong,Anuradha Godavarty*

Main category: eess.IV

TLDR: ADZUS是一种新型文本引导扩散模型，用于无监督分割糖尿病足溃疡，无需标注数据，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 糖尿病足溃疡（DFUs）的精确评估对改善患者结果至关重要，但传统方法依赖大量标注数据。

Method: ADZUS通过零样本学习和文本提示动态适应分割任务，无需标注训练数据。

Result: ADZUS在慢性伤口数据集上达到86.68% IoU和94.69%精度，显著优于FUSegNet等监督方法。

Conclusion: ADZUS为医学影像提供了高效、可扩展的解决方案，但计算成本和微调需求仍需改进。

Abstract: Diabetic foot ulcers (DFUs) pose a significant challenge in healthcare,
requiring precise and efficient wound assessment to enhance patient outcomes.
This study introduces the Attention Diffusion Zero-shot Unsupervised System
(ADZUS), a novel text-guided diffusion model that performs wound segmentation
without relying on labeled training data. Unlike conventional deep learning
models, which require extensive annotation, ADZUS leverages zero-shot learning
to dynamically adapt segmentation based on descriptive prompts, offering
enhanced flexibility and adaptability in clinical applications. Experimental
evaluations demonstrate that ADZUS surpasses traditional and state-of-the-art
segmentation models, achieving an IoU of 86.68\% and the highest precision of
94.69\% on the chronic wound dataset, outperforming supervised approaches such
as FUSegNet. Further validation on a custom-curated DFU dataset reinforces its
robustness, with ADZUS achieving a median DSC of 75\%, significantly surpassing
FUSegNet's 45\%. The model's text-guided segmentation capability enables
real-time customization of segmentation outputs, allowing targeted analysis of
wound characteristics based on clinical descriptions. Despite its competitive
performance, the computational cost of diffusion-based inference and the need
for potential fine-tuning remain areas for future improvement. ADZUS represents
a transformative step in wound segmentation, providing a scalable, efficient,
and adaptable AI-driven solution for medical imaging.

</details>

<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [210] [Reinforcement learning framework for the mechanical design of microelectronic components under multiphysics constraints](https://arxiv.org/abs/2504.17142)
*Siddharth Nair,Timothy F. Walsh,Greg Pickrell,Fabio Semperlotti*

Main category: physics.comp-ph

TLDR: 研究开发了一种基于强化学习的微电子元件设计方法，用于解决多物理约束下的优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统全局优化方法在处理少量设计参数时有效，但随着解空间和约束复杂度的增加，需要新技术。微电子元件设计因解空间大和多物理约束而极具挑战性。

Method: 以ASIC和异构集成中介层为原型，开发并数值测试了基于强化学习的优化框架，重点优化ASIC芯片的键合互连几何形状和中介层组件布局。

Result: 该方法成功应用于高维解空间和多物理约束下的优化问题。

Conclusion: 强化学习为复杂微电子元件设计提供了有效解决方案。

Abstract: This study focuses on the development of reinforcement learning based
techniques for the design of microelectronic components under multiphysics
constraints. While traditional design approaches based on global optimization
approaches are effective when dealing with a small number of design parameters,
as the complexity of the solution space and of the constraints increases
different techniques are needed. This is an important reason that makes the
design and optimization of microelectronic components (characterized by large
solution space and multiphysics constraints) very challenging for traditional
methods. By taking as prototypical elements an application-specific integrated
circuit (ASIC) and a heterogeneously integrated (HI) interposer, we develop and
numerically test an optimization framework based on reinforcement learning
(RL). More specifically, we consider the optimization of the bonded
interconnect geometry for an ASIC chip as well as the placement of components
on a HI interposer while satisfying thermoelastic and design constraints. This
placement problem is particularly interesting because it features a
high-dimensional solution space.

</details>

<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [211] [Mathematical Modeling of Protein Structures: A Cohomology-Based Approach to the Flagellar Motor](https://arxiv.org/abs/2504.16941)
*Zakaria Lamine,Abdelatif Hafid,Mohamed Rahouti*

Main category: q-bio.BM

TLDR: 本文提出了一种基于同调论的新数学模型，利用KEEL定理构建了曲线边界类的同调结构，并通过代数方法连接不同同调群，应用于蛋白质结构分析和预测。


<details>
  <summary>Details</summary>
Motivation: 探索同调论在生物大分子建模中的应用，特别是蛋白质结构的几何与代数基础。

Method: 利用斜交换分级代数和结构定理构建单纯复形，连接不同同调群，并应用于Flagellar Motor结构分析。

Result: 模型成功应用于蛋白质结构预测，为生物大分子建模提供了新的几何与代数视角。

Conclusion: 该模型展示了同调论在结构生物学中的潜力，为未来研究提供了新方向。

Abstract: This study presents a novel mathematical model derived from cohomology,
leveraging the KEEL-proven theorem that establishes cohomology as tautological,
generated by boundary classes of curves with fixed dual graphs. Simplicial
complexes are constructed using skew-commutative graded algebra, and the
structure theorem is applied to connect distinct homologies, enabling precise
interpretations of the resulting geometric forms. The proposed model is
utilized for protein structure analysis and prediction, with a specific
application to the Flagellar Motor structure. This approach offers new insights
into the geometric and algebraic foundations of biological macromolecular
modeling, highlighting its potential for advancement in structural biology.

</details>

### [212] [Deciphering the unique dynamic activation pathway in a G protein-coupled receptor enables unveiling biased signaling and identifying cryptic allosteric sites in conformational intermediates](https://arxiv.org/abs/2504.17624)
*Jigang Fan,Chunhao Zhu,Xiaobing Lan,Haiming Zhuang,Mingyu Li,Jian Zhang,Shaoyong Lu*

Main category: q-bio.BM

TLDR: 研究通过计算与实验方法揭示了NTSR1的动态激活机制和偏向信号传导，发现了一个潜在的变构位点，为开发治疗成瘾相关疾病的药物提供了新思路。


<details>
  <summary>Details</summary>
Motivation: NTSR1在调节多巴胺神经元活动和镇痛中起重要作用，其偏向信号传导可能减少药物滥用，为治疗成瘾相关疾病提供新途径。

Method: 结合分子动力学模拟、马尔可夫状态模型、时间通信网络分析、定点突变和构象生物传感器等方法，研究NTSR1的激活机制。

Result: 揭示了NTSR1的动态逐步激活机制和信号传导网络，发现了一个潜在的变构位点。

Conclusion: 研究为理解NTSR1的原子级激活机制和开发变构调节剂提供了重要基础。

Abstract: Neurotensin receptor 1 (NTSR1), a member of the Class A G protein-coupled
receptor superfamily, plays an important role in modulating dopaminergic
neuronal activity and eliciting opioid-independent analgesia. Recent studies
suggest that promoting \{beta}-arrestin-biased signaling in NTSR1 may diminish
drugs of abuse, such as psychostimulants, thereby offering a potential avenue
for treating human addiction-related disorders. In this study, we utilized a
novel computational and experimental approach that combined nudged elastic
band-based molecular dynamics simulations, Markov state models, temporal
communication network analysis, site-directed mutagenesis, and conformational
biosensors, to explore the intricate mechanisms underlying NTSR1 activation and
biased signaling. Our study reveals a dynamic stepwise transition mechanism and
activated transmission network associated with NTSR1 activation. It also yields
valuable insights into the complex interplay between the unique polar network,
non-conserved ion locks, and aromatic clusters in NTSR1 signaling. Moreover, we
identified a cryptic allosteric site located in the intracellular region of the
receptor that exists in an intermediate state within the activation pathway.
Collectively, these findings contribute to a more profound understanding of
NTSR1 activation and biased signaling at the atomic level, thereby providing a
potential strategy for the development of NTSR1 allosteric modulators in the
realm of G protein-coupled receptor biology, biophysics, and medicine.

</details>

<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [213] [Demonstration of an AI-driven workflow for dynamic x-ray spectroscopy](https://arxiv.org/abs/2504.17124)
*Ming Du,Mark Wolfman,Chengjun Sun,Shelly D. Kelly,Mathew J. Cherukara*

Main category: physics.app-ph

TLDR: 该论文提出了一种基于知识注入的贝叶斯优化方法，用于自适应XANES数据采集，显著减少了测量点数量（仅需15-20%的传统采样点），同时保持了高精度。


<details>
  <summary>Details</summary>
Motivation: 传统XANES光谱数据采集需要大量能量点，耗时且缺乏对光谱结构的领域知识利用。

Method: 采用知识注入的贝叶斯优化方法，结合吸收边和前边峰等光谱特征知识。

Result: 该方法仅需15-20%的传统采样点即可准确重建XANES光谱，吸收边误差小于0.1 eV，峰值误差小于0.03 eV，整体均方根误差小于0.005。

Conclusion: 该方法提高了XANES数据采集效率，支持高时间分辨率的动态实验，减少了吸收边附近的采样误差。

Abstract: X-ray absorption near edge structure (XANES) spectroscopy is a powerful
technique for characterizing the chemical state and symmetry of individual
elements within materials, but requires collecting data at many energy points
which can be time-consuming. While adaptive sampling methods exist for
efficiently collecting spectroscopic data, they often lack domain-specific
knowledge about XANES spectra structure. Here we demonstrate a
knowledge-injected Bayesian optimization approach for adaptive XANES data
collection that incorporates understanding of spectral features like absorption
edges and pre-edge peaks. We show this method accurately reconstructs the
absorption edge of XANES spectra using only 15-20% of the measurement points
typically needed for conventional sampling, while maintaining the ability to
determine the x-ray energy of the sharp peak after absorption edge with errors
less than 0.03 eV, the absorption edge with errors less than 0.1 eV; and
overall root-mean-square errors less than 0.005 compared to compared to
traditionally sampled spectra. Our experiments on battery materials and
catalysts demonstrate the method's effectiveness for both static and dynamic
XANES measurements, improving data collection efficiency and enabling better
time resolution for tracking chemical changes. This approach advances the
degree of automation in XANES experiments reducing the common errors of under-
or over-sampling points in near the absorption edge and enabling dynamic
experiments that require high temporal resolution or limited measurement time.

</details>

<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [214] [Neural Contraction Metrics with Formal Guarantees for Discrete-Time Nonlinear Dynamical Systems](https://arxiv.org/abs/2504.17102)
*Haoyu Li,Xiangru Zhong,Bin Hu,Huan Zhang*

Main category: math.OC

TLDR: 本文提出了一种通过学习参数化为神经网络的收缩度量来分析离散时间非线性动力系统的方法，解决了传统方法在非光滑系统中的应用限制。


<details>
  <summary>Details</summary>
Motivation: 复杂非线性系统的收缩度量识别是一个开放性问题，现有工具难以扩展且效果有限，尤其是在非光滑动力学系统中。

Method: 通过建立新的充分条件，利用神经网络验证工具α,β-CROWN验证收缩度量，并开发了一种从采样数据中学习神经收缩度量的方法。

Result: 成功合成并验证了多种非线性系统的神经网络收缩度量。

Conclusion: 该方法为复杂非线性系统的收缩度量分析提供了有效且可扩展的工具。

Abstract: Contraction metrics are crucial in control theory because they provide a
powerful framework for analyzing stability, robustness, and convergence of
various dynamical systems. However, identifying these metrics for complex
nonlinear systems remains an open challenge due to the lack of scalable and
effective tools. This paper explores the approach of learning verifiable
contraction metrics parametrized as neural networks (NNs) for discrete-time
nonlinear dynamical systems. While prior works on formal verification of
contraction metrics for general nonlinear systems have focused on convex
optimization methods (e.g. linear matrix inequalities, etc) under the
assumption of continuously differentiable dynamics, the growing prevalence of
NN-based controllers, often utilizing ReLU activations, introduces challenges
due to the non-smooth nature of the resulting closed-loop dynamics. To bridge
this gap, we establish a new sufficient condition for establishing formal
neural contraction metrics for general discrete-time nonlinear systems assuming
only the continuity of the dynamics. We show that from a computational
perspective, our sufficient condition can be efficiently verified using the
state-of-the-art neural network verifier $\alpha,\!\beta$-CROWN, which scales
up non-convex neural network verification via novel integration of symbolic
linear bound propagation and branch-and-bound. Built upon our analysis tool, we
further develop a learning method for synthesizing neural contraction metrics
from sampled data. Finally, our approach is validated through the successful
synthesis and verification of NN contraction metrics for various nonlinear
examples.

</details>

<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [215] [What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency Maps in Explainable AI (XAI)](https://arxiv.org/abs/2504.17023)
*Felix Kares,Timo Speith,Hanwei Zhang,Markus Langer*

Main category: cs.HC

TLDR: 该研究比较了三种显著性图方法（LIME、Grad-CAM和Guided Backpropagation）在不同评估方法下的表现，发现评估结果不一致，并探讨了用户研究与数学指标在可解释AI评估中的互补性。


<details>
  <summary>Details</summary>
Motivation: 显著性图是解释神经网络分类的常用方法，但如何评估这些方法的有效性仍是一个开放问题。研究旨在比较不同评估方法下显著性图的表现，以探讨其一致性和互补性。

Method: 通过一项涉及166名参与者的实验，研究比较了三种显著性图方法在主观用户信任、客观用户能力和数学指标三个评估维度上的表现。

Result: 评估结果不一致：主观信任无差异，Grad-CAM在用户能力提升上表现最佳，而Guided Backpropagation在数学指标上最优。部分数学指标与用户理解相关，但关系反直觉。

Conclusion: 研究强调了用户研究和数学指标在评估可解释AI方法时的互补性，并指出当前评估方法的不一致性。

Abstract: Saliency maps are a popular approach for explaining classifications of
(convolutional) neural networks. However, it remains an open question as to how
best to evaluate salience maps, with three families of evaluation methods
commonly being used: subjective user measures, objective user measures, and
mathematical metrics. We examine three of the most popular saliency map
approaches (viz., LIME, Grad-CAM, and Guided Backpropagation) in a between
subject study (N=166) across these families of evaluation methods. We test 1)
for subjective measures, if the maps differ with respect to user trust and
satisfaction; 2) for objective measures, if the maps increase users' abilities
and thus understanding of a model; 3) for mathematical metrics, which map
achieves the best ratings across metrics; and 4) whether the mathematical
metrics can be associated with objective user measures. To our knowledge, our
study is the first to compare several salience maps across all these evaluation
methods$-$with the finding that they do not agree in their assessment (i.e.,
there was no difference concerning trust and satisfaction, Grad-CAM improved
users' abilities best, and Guided Backpropagation had the most favorable
mathematical metrics). Additionally, we show that some mathematical metrics
were associated with user understanding, although this relationship was often
counterintuitive. We discuss these findings in light of general debates
concerning the complementary use of user studies and mathematical metrics in
the evaluation of explainable AI (XAI) approaches.

</details>

### [216] [Psychological Effect of AI driven marketing tools for beauty/facial feature enhancement](https://arxiv.org/abs/2504.17055)
*Ayushi Agrawal,Aditya Kondai,Kavita Vemuri*

Main category: cs.HC

TLDR: 研究探讨了AI面部评估工具对自我物化、自尊和情绪的影响，发现其可能强化外貌相关的不安全感，并存在性别差异。


<details>
  <summary>Details</summary>
Motivation: 了解AI工具如何影响个体的心理状态，尤其是自我物化和自尊，以及性别差异。

Method: 使用两种版本的面部分析工具（批评性和中性），测量自我物化、自尊、情绪、外貌增强行为及感知社会情绪。

Result: 高自我物化和低自尊与外貌增强行为相关，中性工具仍引发负面情绪，女性更倾向数字增强且对他人情绪感知较弱。

Conclusion: AI工具可能无意中强化社会偏见，需负责任的设计。未来研究将探讨训练数据中的意识形态如何影响工具输出及用户态度。

Abstract: AI-powered facial assessment tools are reshaping how individuals evaluate
appearance and internalize social judgments. This study examines the
psychological impact of such tools on self-objectification, self-esteem, and
emotional responses, with attention to gender differences. Two samples used
distinct versions of a facial analysis tool: one overtly critical (N=75; M=22.9
years), and another more neutral (N=51; M=19.9 years). Participants completed
validated self-objectification and self-esteem scales and custom items
measuring emotion, digital/physical appearance enhancement (DAE, PAEE), and
perceived social emotion (PSE). Results revealed consistent links between high
self-objectification, low self-esteem, and increased appearance enhancement
behaviors across both versions. Despite softer framing, the newer tool still
evoked negative emotional responses (U=1466.5, p=0.013), indicating implicit
feedback may reinforce appearance-related insecurities. Gender differences
emerged in DAE (p=0.025) and PSE (p<0.001), with females more prone to digital
enhancement and less likely to perceive emotional impact in others. These
findings reveal how AI tools may unintentionally reinforce and amplify existing
social biases and underscore the critical need for responsible AI design and
development. Future research will investigate how human ideologies embedded in
the training data of such tools shape their evaluative outputs, and how these,
in turn, influence user attitudes and decisions.

</details>

### [217] [Improving Human-Autonomous Vehicle Interaction in Complex Systems](https://arxiv.org/abs/2504.17170)
*Robert Kaufman*

Main category: cs.HC

TLDR: 论文探讨了自动驾驶车辆（AVs）如何满足乘客的信息需求，强调需根据个体差异和情境调整通信策略。


<details>
  <summary>Details</summary>
Motivation: 当前AV研究忽视了不同人群和情境的多样性，阻碍了实际应用。

Method: 通过三项实证研究：1）极端驾驶环境中的通信策略；2）错误通信系统的后果；3）机器学习预测信任因素。

Result: 研究发现任务敏感、情境适应和个性化的通信策略能提升驾驶表现、信任和信心。

Conclusion: AV系统需透明、可适应且个性化，以应对复杂的人机交互需求，为设计和政策提供指导。

Abstract: Unresolved questions about how autonomous vehicles (AVs) should meet the
informational needs of riders hinder real-world adoption. Complicating our
ability to satisfy rider needs is that different people, goals, and driving
contexts have different criteria for what constitutes interaction success.
Unfortunately, most human-AV research and design today treats all people and
situations uniformly. It is crucial to understand how an AV should communicate
to meet rider needs, and how communications should change when the human-AV
complex system changes. I argue that understanding the relationships between
different aspects of the human-AV system can help us build improved and
adaptable AV communications. I support this argument using three empirical
studies. First, I identify optimal communication strategies that enhance
driving performance, confidence, and trust for learning in extreme driving
environments. Findings highlight the need for task-sensitive,
modality-appropriate communications tuned to learner cognitive limits and
goals. Next, I highlight the consequences of deploying faulty communication
systems and demonstrate the need for context-sensitive communications. Third, I
use machine learning (ML) to illuminate personal factors predicting trust in
AVs, emphasizing the importance of tailoring designs to individual traits and
concerns. Together, this dissertation supports the necessity of transparent,
adaptable, and personalized AV systems that cater to individual needs, goals,
and contextual demands. By considering the complex system within which human-AV
interactions occur, we can deliver valuable insights for designers,
researchers, and policymakers. This dissertation also provides a concrete
domain to study theories of human-machine joint action and situational
awareness, and can be used to guide future human-AI interaction research.
[shortened for arxiv]

</details>

### [218] [Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality](https://arxiv.org/abs/2504.17331)
*Süleyman Özdel,Kadir Burak Buldu,Enkelejda Kasneci,Efe Bozkir*

Main category: cs.HC

TLDR: 论文提出了一种基于大型语言模型（LLM）的新型虚拟现实（VR）移动技术，通过自然语言实现上下文感知导航，并与传统方法（如控制器传送和语音转向）进行了比较。


<details>
  <summary>Details</summary>
Motivation: 传统语音移动方法依赖固定指令集，限制了交互的自然性和灵活性，而LLM驱动的移动技术旨在提供更自然、灵活的双手自由移动方案。

Method: 研究评估了三种移动方法：控制器传送、语音转向和LLM驱动的自然语言导航，通过眼动追踪数据分析和标准化问卷（如可用性、存在感、晕动症和认知负荷）进行对比。

Result: LLM驱动的移动方法在可用性、存在感和晕动症方面与传统方法相当，但能增强用户在虚拟环境中的注意力，表明更高的参与度。SHAP分析揭示了不同技术间视觉注意力和认知处理的差异。

Conclusion: LLM驱动的移动技术是一种舒适、自然的双手自由替代方案，尤其适用于提升虚拟现实的可访问性。

Abstract: Locomotion plays a crucial role in shaping the user experience within virtual
reality environments. In particular, hands-free locomotion offers a valuable
alternative by supporting accessibility and freeing users from reliance on
handheld controllers. To this end, traditional speech-based methods often
depend on rigid command sets, limiting the naturalness and flexibility of
interaction. In this study, we propose a novel locomotion technique powered by
large language models (LLMs), which allows users to navigate virtual
environments using natural language with contextual awareness. We evaluate
three locomotion methods: controller-based teleportation, voice-based steering,
and our language model-driven approach. Our evaluation measures include
eye-tracking data analysis, including explainable machine learning through SHAP
analysis as well as standardized questionnaires for usability, presence,
cybersickness, and cognitive load to examine user attention and engagement. Our
findings indicate that the LLM-driven locomotion possesses comparable
usability, presence, and cybersickness scores to established methods like
teleportation, demonstrating its novel potential as a comfortable, natural
language-based, hands-free alternative. In addition, it enhances user attention
within the virtual environment, suggesting greater engagement. Complementary to
these findings, SHAP analysis revealed that fixation, saccade, and
pupil-related features vary across techniques, indicating distinct patterns of
visual attention and cognitive processing. Overall, we state that our method
can facilitate hands-free locomotion in virtual spaces, especially in
supporting accessibility.

</details>

### [219] [Lessons from Deploying Learning-based CSI Localization on a Large-Scale ISAC Platform](https://arxiv.org/abs/2504.17173)
*Tianyu Zhang,Dongheng Zhang,Ruixu Geng,Xuecheng Xie,Shuai Yang,Yan Chen*

Main category: cs.HC

TLDR: 论文提出了一种针对大规模WiFi室内定位的CSI学习框架，解决了未标记数据利用不足和CSI测量异质性等问题，显著提升了定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有CSI定位系统在可控小规模环境中开发，泛化能力有限，且未充分利用未标记数据和CSI异质性。

Method: 采用图结构建模异构CSI数据，设计预训练任务利用空间和时间先验，并引入置信感知微调策略。

Result: 在五层楼25,600平方米的实验中，中位定位误差2.17米，楼层准确率99.49%，MAE降低18.7%。

Conclusion: 所提框架在大规模ISAC部署中显著提升了CSI定位性能，展示了实际应用的潜力。

Abstract: In recent years, Channel State Information (CSI), recognized for its
fine-grained spatial characteristics, has attracted increasing attention in
WiFi-based indoor localization. However, despite its potential, CSI-based
approaches have yet to achieve the same level of deployment scale and
commercialization as those based on Received Signal Strength Indicator (RSSI).
A key limitation lies in the fact that most existing CSI-based systems are
developed and evaluated in controlled, small-scale environments, limiting their
generalizability. To bridge this gap, we explore the deployment of a
large-scale CSI-based localization system involving over 400 Access Points
(APs) in a real-world building under the Integrated Sensing and Communication
(ISAC) paradigm. We highlight two critical yet often overlooked factors: the
underutilization of unlabeled data and the inherent heterogeneity of CSI
measurements. To address these challenges, we propose a novel CSI-based
learning framework for WiFi localization, tailored for large-scale ISAC
deployments on the server side. Specifically, we employ a novel graph-based
structure to model heterogeneous CSI data and reduce redundancy. We further
design a pretext pretraining task that incorporates spatial and temporal priors
to effectively leverage large-scale unlabeled CSI data. Complementarily, we
introduce a confidence-aware fine-tuning strategy to enhance the robustness of
localization results. In a leave-one-smartphone-out experiment spanning five
floors and 25, 600 m2, we achieve a median localization error of 2.17 meters
and a floor accuracy of 99.49%. This performance corresponds to an 18.7%
reduction in mean absolute error (MAE) compared to the best-performing
baseline.

</details>

### [220] [The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults](https://arxiv.org/abs/2504.17663)
*Michelle L. Ding,Harini Suresh*

Main category: cs.HC

TLDR: 本文采用以幸存者为中心的方法，探讨了社会技术AI治理在预防AI生成的未经同意的成人私密图像（AIG-NCII，俗称“深度伪造色情”）中的作用。研究发现了一个由开源换脸模型和近200个“脱衣”软件组成的“恶意技术生态系统”（MTE），并指出当前治理方法未能有效监管MTE。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示当前AI治理方法在防止AIG-NCII方面的不足，尤其是针对成人内容的监管漏洞。

Method: 采用幸存者为中心的方法，结合NIST AI 100-4报告，分析当前合成内容治理方法的局限性。

Result: 发现了一个恶意技术生态系统（MTE），并指出当前治理方法未能有效监管成人AIG-NCII。

Conclusion: 当前AI治理方法存在缺陷，需改进以有效应对AIG-NCII问题。

Abstract: In this paper, we adopt a survivor-centered approach to locate and dissect
the role of sociotechnical AI governance in preventing AI-Generated
Non-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as
"deep fake pornography." We identify a "malicious technical ecosystem" or
"MTE," comprising of open-source face-swapping models and nearly 200
"nudifying" software programs that allow non-technical users to create AIG-NCII
within minutes. Then, using the National Institute of Standards and Technology
(NIST) AI 100-4 report as a reflection of current synthetic content governance
methods, we show how the current landscape of practices fails to effectively
regulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining
these gaps.

</details>

### [221] [INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models](https://arxiv.org/abs/2504.17677)
*Jarne Thys,Sebe Vanbrabant,Davy Vanacken,Gustavo Rovelo Ruiz*

Main category: cs.HC

TLDR: INSIGHT是一个模块化设计的AI工具，旨在通过分析学生问题动态构建FAQ，帮助教师提供个性化支持。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在教育中的潜力与挑战，如个性化教学与学生隐私问题。

Method: 通过提取学生问题的关键词，动态构建FAQ，为教师提供个性化支持。

Result: INSIGHT成功整合到高等教育课程中，提升了教学效率。

Conclusion: 未来可扩展为自适应学习系统，提供更互动和包容的学习体验。

Abstract: The rise of AI, especially Large Language Models, presents challenges and
opportunities to integrate such technology into the classroom. AI has the
potential to revolutionize education by helping teaching staff with various
tasks, such as personalizing their teaching methods, but it also raises
concerns, for example, about the degradation of student-teacher interactions
and user privacy. This paper introduces INSIGHT, a proof of concept to combine
various AI tools to assist teaching staff and students in the process of
solving exercises. INSIGHT has a modular design that allows it to be integrated
into various higher education courses. We analyze students' questions to an LLM
by extracting keywords, which we use to dynamically build an FAQ from students'
questions and provide new insights for the teaching staff to use for more
personalized face-to-face support. Future work could build upon INSIGHT by
using the collected data to provide adaptive learning and adjust content based
on student progress and learning styles to offer a more interactive and
inclusive learning experience.

</details>

<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [222] [Intrinsic Barriers to Explaining Deep Foundation Models](https://arxiv.org/abs/2504.16948)
*Zhen Tan,Huan Liu*

Main category: cs.CY

TLDR: 本文探讨深度基础模型（DFMs）的解释性挑战，分析其内在障碍及当前方法的局限性，并讨论对技术验证与治理的影响。


<details>
  <summary>Details</summary>
Motivation: 随着DFMs能力的提升，理解其内部机制成为确保信任、安全与问责的关键，但复杂性带来了解释性挑战。

Method: 通过分析DFMs的基本特性及当前解释方法的局限性，探讨其内在障碍。

Result: 研究发现DFMs的解释性难题可能源于其本质特性，而非暂时的技术限制。

Conclusion: 结论指出需重新思考对DFMs的验证与治理方法，以应对其内在解释性挑战。

Abstract: Deep Foundation Models (DFMs) offer unprecedented capabilities but their
increasing complexity presents profound challenges to understanding their
internal workings-a critical need for ensuring trust, safety, and
accountability. As we grapple with explaining these systems, a fundamental
question emerges: Are the difficulties we face merely temporary hurdles,
awaiting more sophisticated analytical techniques, or do they stem from
\emph{intrinsic barriers} deeply rooted in the nature of these large-scale
models themselves? This paper delves into this critical question by examining
the fundamental characteristics of DFMs and scrutinizing the limitations
encountered by current explainability methods when confronted with this
inherent challenge. We probe the feasibility of achieving satisfactory
explanations and consider the implications for how we must approach the
verification and governance of these powerful technologies.

</details>

### [223] [Approaches to Responsible Governance of GenAI in Organizations](https://arxiv.org/abs/2504.17044)
*Dhari Gandhi,Himanshu Joshi,Lucas Hartman,Shabnam Hassani*

Main category: cs.CY

TLDR: 论文探讨了生成式AI（GenAI）快速发展带来的机遇与伦理、问责和社会影响等挑战，提出了基于风险的可适应治理框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展带来了机遇，但也引发了伦理、问责和社会影响等复杂问题，需要建立有效的治理框架。

Method: 通过文献综述、现有治理框架和行业圆桌讨论，识别核心治理原则，并提出可操作的建议。

Result: 研究发现需要可适应的风险评估工具、持续监控实践和跨部门合作，以建立可信赖的GenAI。

Conclusion: 论文提出了Responsible GenAI Guide（ResAI），为组织提供结构化基础，确保GenAI符合伦理、法律和运营最佳实践。

Abstract: The rapid evolution of Generative AI (GenAI) has introduced unprecedented
opportunities while presenting complex challenges around ethics,
accountability, and societal impact. This paper draws on a literature review,
established governance frameworks, and industry roundtable discussions to
identify core principles for integrating responsible GenAI governance into
diverse organizational structures. Our objective is to provide actionable
recommendations for a balanced, risk-based governance approach that enables
both innovation and oversight. Findings emphasize the need for adaptable risk
assessment tools, continuous monitoring practices, and cross-sector
collaboration to establish trustworthy GenAI. These insights provide a
structured foundation and Responsible GenAI Guide (ResAI) for organizations to
align GenAI initiatives with ethical, legal, and operational best practices.

</details>

### [224] [Towards User-Centred Design of AI-Assisted Decision-Making in Law Enforcement](https://arxiv.org/abs/2504.17393)
*Vesna Nowack,Dalal Alrajeh,Carolina Gutierrez Muñoz,Katie Thomas,William Hobson,Catherine Hamilton-Giachritsis,Patrick Benjamin,Tim Grant,Juliane A. Kloess,Jessica Woodhams*

Main category: cs.CY

TLDR: 论文探讨了执法机构中AI辅助系统的用户需求，强调高效数据处理、可扩展性、准确性、可解释性和适应性，并指出完全自动化难以实现。


<details>
  <summary>Details</summary>
Motivation: 研究动机是明确执法领域中AI辅助系统的用户需求，填补现有实践中的空白。

Method: 采用定性研究方法，分析执法机构中的决策过程，识别现有实践的局限性并探索用户需求。

Result: 研究发现用户需要高效处理数据的系统，并强调可扩展性、准确性、可解释性和适应性。此外，用户需参与数据验证和系统调整。

Conclusion: 结论指出执法领域的动态复杂性使得完全自动化难以实现，需持续的人机协作。

Abstract: Artificial Intelligence (AI) has become an important part of our everyday
lives, yet user requirements for designing AI-assisted systems in law
enforcement remain unclear. To address this gap, we conducted qualitative
research on decision-making within a law enforcement agency. Our study aimed to
identify limitations of existing practices, explore user requirements and
understand the responsibilities that humans expect to undertake in these
systems.
  Participants in our study highlighted the need for a system capable of
processing and analysing large volumes of data efficiently to help in crime
detection and prevention. Additionally, the system should satisfy requirements
for scalability, accuracy, justification, trustworthiness and adaptability to
be adopted in this domain. Participants also emphasised the importance of
having end users review the input data that might be challenging for AI to
interpret, and validate the generated output to ensure the system's accuracy.
To keep up with the evolving nature of the law enforcement domain, end users
need to help the system adapt to the changes in criminal behaviour and
government guidance, and technical experts need to regularly oversee and
monitor the system. Furthermore, user-friendly human interaction with the
system is essential for its adoption and some of the participants confirmed
they would be happy to be in the loop and provide necessary feedback that the
system can learn from. Finally, we argue that it is very unlikely that the
system will ever achieve full automation due to the dynamic and complex nature
of the law enforcement domain.

</details>

### [225] [Seeing The Words: Evaluating AI-generated Biblical Art](https://arxiv.org/abs/2504.16974)
*Hidde Makimei,Shuai Wang,Willem van Peursen*

Main category: cs.CY

TLDR: 本文研究了AI根据圣经文本生成图像的准确性，并提供了一个包含7K图像的数据集，通过多种神经网络工具评估其宗教和美学表现。


<details>
  <summary>Details</summary>
Motivation: 探讨AI是否能根据圣经文本生成符合其背景和语境的图像，填补系统性评估的空白。

Method: 构建大型数据集（7K图像），使用多种神经网络工具评估生成图像的准确性和美学质量。

Result: 提供了对生成图像的准确性评估，并从宗教和美学角度进行了分析。

Conclusion: 讨论了生成图像的应用，并反思了AI生成器的性能。

Abstract: The past years witnessed a significant amount of Artificial Intelligence (AI)
tools that can generate images from texts. This triggers the discussion of
whether AI can generate accurate images using text from the Bible with respect
to the corresponding biblical contexts and backgrounds. Despite some existing
attempts at a small scale, little work has been done to systematically evaluate
these generated images. In this work, we provide a large dataset of over 7K
images using biblical text as prompts. These images were evaluated with
multiple neural network-based tools on various aspects. We provide an
assessment of accuracy and some analysis from the perspective of religion and
aesthetics. Finally, we discuss the use of the generated images and reflect on
the performance of the AI generators.

</details>

### [226] [Flexibility of German gas-fired generation: evidence from clustering empirical operation](https://arxiv.org/abs/2504.16943)
*Chiara Fusar Bassini,Alice Lixuan Xu,Jorge Sánchez Canales,Lion Hirth,Lynn H. Kaack*

Main category: cs.CY

TLDR: 论文通过深度学习聚类德国天然气发电机组，发现两类峰荷机组和两类非峰荷机组，非峰荷机组灵活性较低且占必须运行发电量的83%，需政策调整提升其市场响应。


<details>
  <summary>Details</summary>
Motivation: 研究发电机组灵活性假设通常基于技术特性，但实际运行中服务义务和市场激励可能限制其灵活性，需实证分析。

Method: 使用深度学习处理2019-2023年每小时调度数据，将时间序列转换为易聚类表示，对德国60%以上大型天然气机组进行聚类。

Result: 识别出两类峰荷和两类非峰荷机组，非峰荷机组灵活性较低且占必须运行发电量的83%。

Conclusion: 需通过监管改革提升非峰荷机组的市场响应能力，释放其灵活性。

Abstract: A key input to energy models are assumptions about the flexibility of power
generation units, i.e., how quickly and often they can start up. These
assumptions are usually calibrated on the technical characteristics of the
units, such as installed capacity or technology type. However, even if power
generation units technically can dispatch flexibly, service obligations and
market incentives may constrain their operation. Here, we cluster over 60% of
German national gas generation (generation units of 100 MWp or above) based on
their empirical flexibility. We process the hourly dispatch of sample units
between 2019 and 2023 using a novel deep learning approach, that transforms
time series into easy-to-cluster representations. We identify two clusters of
peaker units and two clusters of non-peaker units, whose different empirical
flexibility is quantified by cluster-level ramp rates. Non-peaker units, around
half of the sample, are empirically less flexible than peakers, and make up for
more than 83% of sample must-run generation. Regulatory changes addressing the
low market responsiveness of non-peakers are needed to unlock their
flexibility.

</details>

### [227] [Engineering the Law-Machine Learning Translation Problem: Developing Legally Aligned Models](https://arxiv.org/abs/2504.16969)
*Mathias Hanson,Gregory Lewkowicz,Sam Verboven*

Main category: cs.CY

TLDR: 本文提出了一种五阶段跨学科框架，用于在机器学习模型开发中整合法律和技术分析，以解决法律合规性与预测性能之间的复杂权衡问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习技术在法律合规性方面面临挑战，现有方法未能充分结合法律分析和模型开发。

Method: 引入一个五阶段跨学科框架，结合法律推理和机器学习技术，指导模型设计和评估。

Result: 该框架能够帮助设计合法且高性能的机器学习模型，并通过反洗钱案例研究验证其应用。

Conclusion: 该框架填补了法律概念分析与机器学习确定性需求之间的空白，为法律合规的机器学习模型开发提供了实用方法。

Abstract: Organizations developing machine learning-based (ML) technologies face the
complex challenge of achieving high predictive performance while respecting the
law. This intersection between ML and the law creates new complexities. As ML
model behavior is inferred from training data, legal obligations cannot be
operationalized in source code directly. Rather, legal obligations require
"indirect" operationalization. However, choosing context-appropriate
operationalizations presents two compounding challenges: (1) laws often permit
multiple valid operationalizations for a given legal obligation-each with
varying degrees of legal adequacy; and, (2) each operationalization creates
unpredictable trade-offs among the different legal obligations and with
predictive performance. Evaluating these trade-offs requires metrics (or
heuristics), which are in turn difficult to validate against legal obligations.
Current methodologies fail to fully address these interwoven challenges as they
either focus on legal compliance for traditional software or on ML model
development without adequately considering legal complexities. In response, we
introduce a five-stage interdisciplinary framework that integrates legal and
ML-technical analysis during ML model development. This framework facilitates
designing ML models in a legally aligned way and identifying high-performing
models that are legally justifiable. Legal reasoning guides choices for
operationalizations and evaluation metrics, while ML experts ensure technical
feasibility, performance optimization and an accurate interpretation of metric
values. This framework bridges the gap between more conceptual analysis of law
and ML models' need for deterministic specifications. We illustrate its
application using a case study in the context of anti-money laundering.

</details>

<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [228] [Automatically Generating Rules of Malicious Software Packages via Large Language Model](https://arxiv.org/abs/2504.17198)
*XiangRui Zhang,HaoYu Chen,Yongzhong He,Wenjia Niu,Qiang Li*

Main category: cs.SE

TLDR: RuleLLM利用大型语言模型自动生成开源生态系统的安全规则，优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 现有安全工具依赖专家预定义规则，难以应对软件供应链攻击。

Method: RuleLLM通过提取恶意软件的元数据和代码片段，生成可直接部署的YARA和Semgrep规则，包括规则生成、优化和对齐三个子任务。

Result: 实验生成763条规则（452 YARA和311 Semgrep），精确率85.2%，召回率91.8%，优于现有工具。

Conclusion: RuleLLM有效解决了规则生成问题，并提出11类38子类的规则分类法。

Abstract: Today's security tools predominantly rely on predefined rules crafted by
experts, making them poorly adapted to the emergence of software supply chain
attacks. To tackle this limitation, we propose a novel tool, RuleLLM, which
leverages large language models (LLMs) to automate rule generation for OSS
ecosystems. RuleLLM extracts metadata and code snippets from malware as its
input, producing YARA and Semgrep rules that can be directly deployed in
software development. Specifically, the rule generation task involves three
subtasks: crafting rules, refining rules, and aligning rules. To validate
RuleLLM's effectiveness, we implemented a prototype system and conducted
experiments on the dataset of 1,633 malicious packages. The results are
promising that RuleLLM generated 763 rules (452 YARA and 311 Semgrep) with a
precision of 85.2\% and a recall of 91.8\%, outperforming state-of-the-art
(SOTA) tools and scored-based approaches. We further analyzed generated rules
and proposed a rule taxonomy: 11 categories and 38 subcategories.

</details>

### [229] [Wolves in the Repository: A Software Engineering Analysis of the XZ Utils Supply Chain Attack](https://arxiv.org/abs/2504.17473)
*Piotr Przymus,Thomas Durieux*

Main category: cs.SE

TLDR: 论文分析了开源软件（OSS）中的供应链攻击，以XZ Utils项目（CVE-2024-3094）为例，展示了攻击者如何利用开源开发流程注入后门。


<details>
  <summary>Details</summary>
Motivation: 开源软件的广泛使用带来了安全漏洞，尤其是资源不足的关键项目。本文旨在揭示一种新型供应链攻击，攻击者通过操纵开发流程（如社区管理和CI/CD配置）建立长期控制。

Method: 通过分析GitHub事件和开发工件，重构攻击时间线，研究攻击者战术的演变。

Result: 攻击者通过看似有益的贡献绕过传统安全措施，展示了软件工程实践如何被武器化。

Conclusion: 本文不仅分析了传统安全问题，还揭示了开源生态中开发流程的潜在风险，为保护开源生态系统提供了新视角。

Abstract: The digital economy runs on Open Source Software (OSS), with an estimated
90\% of modern applications containing open-source components. While this
widespread adoption has revolutionized software development, it has also
created critical security vulnerabilities, particularly in essential but
under-resourced projects. This paper examines a sophisticated attack on the XZ
Utils project (CVE-2024-3094), where attackers exploited not just code, but the
entire open-source development process to inject a backdoor into a fundamental
Linux compression library. Our analysis reveals a new breed of supply chain
attack that manipulates software engineering practices themselves -- from
community management to CI/CD configurations -- to establish legitimacy and
maintain long-term control. Through a comprehensive examination of GitHub
events and development artifacts, we reconstruct the attack timeline, analyze
the evolution of attacker tactics. Our findings demonstrate how attackers
leveraged seemingly beneficial contributions to project infrastructure and
maintenance to bypass traditional security measures. This work extends beyond
traditional security analysis by examining how software engineering practices
themselves can be weaponized, offering insights for protecting the open-source
ecosystem.

</details>

### [230] [SCALAR: A Part-of-speech Tagger for Identifiers](https://arxiv.org/abs/2504.17038)
*Christian D. Newman,Brandon Scholten,Sophia Testa,Joshua A. C. Behler,Syreen Banabilah,Michael L. Collard,Michael J. Decker,Mohamed Wiem Mkaouer,Marcos Zampieri,Eman Abdullah AlOmar,Reem Alsuhaibani,Anthony Peruma,Jonathan I. Maletic*

Main category: cs.SE

TLDR: SCALAR是一个专门用于将源代码标识符名称映射到其词性标签序列的工具，通过训练模型提升标注准确性。


<details>
  <summary>Details</summary>
Motivation: 开发者使用的自然语言结构独特，现有词性标注工具难以准确标注源代码标识符，因此需要专用工具。

Method: 使用scikit-learn的GradientBoostingClassifier训练模型，结合手动整理的标识符名称和语法模式数据集。

Result: SCALAR在标注标识符时优于旧版标注工具和现代通用词性标注工具。

Conclusion: SCALAR为源代码标识符的词性标注提供了更准确的解决方案，代码已开源。

Abstract: The paper presents the Source Code Analysis and Lexical Annotation Runtime
(SCALAR), a tool specialized for mapping (annotating) source code identifier
names to their corresponding part-of-speech tag sequence (grammar pattern).
SCALAR's internal model is trained using scikit-learn's
GradientBoostingClassifier in conjunction with a manually-curated oracle of
identifier names and their grammar patterns. This specializes the tagger to
recognize the unique structure of the natural language used by developers to
create all types of identifiers (e.g., function names, variable names etc.).
SCALAR's output is compared with a previous version of the tagger, as well as a
modern off-the-shelf part-of-speech tagger to show how it improves upon other
taggers' output for annotating identifiers. The code is available on Github

</details>

### [231] [Towards Leveraging Large Language Model Summaries for Topic Modeling in Source Code](https://arxiv.org/abs/2504.17426)
*Michele Carissimi,Martina Saletta,Claudio Ferretti*

Main category: cs.SE

TLDR: 论文提出了一种结合大语言模型（LLM）和主题建模技术的新方法，用于自动识别Python程序中的主题。


<details>
  <summary>Details</summary>
Motivation: 理解源代码对软件工程任务（如维护和重用）至关重要，而LLM和主题建模技术为提取代码语义信息提供了新思路。

Method: 通过LLM生成代码摘要，再对摘要应用主题建模，并与基于函数名和现有文档字符串的主题进行比较。

Result: 实验表明，LLM生成的摘要能提供更丰富、可解释的代码结构表示。

Conclusion: 该方法在自动文档生成、代码搜索等软件工程任务中具有应用潜力。

Abstract: Understanding source code is a topic of great interest in the software
engineering community, since it can help programmers in various tasks such as
software maintenance and reuse. Recent advances in large language models (LLMs)
have demonstrated remarkable program comprehension capabilities, while
transformer-based topic modeling techniques offer effective ways to extract
semantic information from text. This paper proposes and explores a novel
approach that combines these strengths to automatically identify meaningful
topics in a corpus of Python programs. Our method consists in applying topic
modeling on the descriptions obtained by asking an LLM to summarize the code.
To assess the internal consistency of the extracted topics, we compare them
against topics inferred from function names alone, and those derived from
existing docstrings. Experimental results suggest that leveraging LLM-generated
summaries provides interpretable and semantically rich representation of code
structure. The promising results suggest that our approach can be fruitfully
applied in various software engineering tasks such as automatic documentation
and tagging, code search, software reorganization and knowledge discovery in
large repositories.

</details>

### [232] [Detection, Classification and Prevalence of Self-Admitted Aging Debt](https://arxiv.org/abs/2504.17428)
*Murali Sridharan,Mika Mäntylä,Leevi Rantala*

Main category: cs.SE

TLDR: 论文提出“老化债务”（AD）概念，通过源代码注释中的“自认老化债务”（SAAD）研究软件老化，并提出分类法量化开源软件中的AD。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注运行时指标，忽略源代码注释等进化指标，导致对软件老化的理解不足。

Method: 采用混合方法，定性定量分析源代码注释，构建SAAD分类法并量化开源软件中的AD。

Result: 分类法将AD分为活跃和休眠两类，分析显示21%以上开源仓库存在SAAD，休眠AD为主。

Conclusion: 软件老化问题随规模增长加剧，分类法有助于研究和实践中的主动维护策略。

Abstract: Context: Previous research on software aging is limited with focus on dynamic
runtime indicators like memory and performance, often neglecting evolutionary
indicators like source code comments and narrowly examining legacy issues
within the TD context. Objective: We introduce the concept of Aging Debt (AD),
representing the increased maintenance efforts and costs needed to keep
software updated. We study AD through Self-Admitted Aging Debt (SAAD) observed
in source code comments left by software developers. Method: We employ a
mixed-methods approach, combining qualitative and quantitative analyses to
detect and measure AD in software. This includes framing SAAD patterns from the
source code comments after analysing the source code context, then utilizing
the SAAD patterns to detect SAAD comments. In the process, we develop a
taxonomy for SAAD that reflects the temporal aging of software and its
associated debt. Then we utilize the taxonomy to quantify the different types
of AD prevalent in OSS repositories. Results: Our proposed taxonomy categorizes
temporal software aging into Active and Dormant types. Our extensive analysis
of over 9,000+ Open Source Software (OSS) repositories reveals that more than
21% repositories exhibit signs of SAAD as observed from our gold standard SAAD
dataset. Notably, Dormant AD emerges as the predominant category, highlighting
a critical but often overlooked aspect of software maintenance. Conclusion: As
software volume grows annually, so do evolutionary aging and maintenance
challenges; our proposed taxonomy can aid researchers in detailed software
aging studies and help practitioners develop improved and proactive maintenance
strategies.

</details>

<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [233] [A Systematic Study on the Design of Odd-Sized Highly Nonlinear Boolean Functions via Evolutionary Algorithms](https://arxiv.org/abs/2504.17666)
*Claude Carlet,Marko Đurasevic,Domagoj Jakobovic,Stjepan Picek,Luca Mariot*

Main category: cs.NE

TLDR: 本文研究了如何演化具有高非线性度的奇数大小布尔函数，通过比较三种编码方式和四种问题实例，发现遗传编程表现最佳，但未达到已知最优结果。通过局部搜索和限制空间，遗传算法成功演化出非线性度为241的9变量布尔函数。


<details>
  <summary>Details</summary>
Motivation: 研究奇数大小布尔函数的高非线性度问题，因其在密码学中的重要性，尽管问题简单但难度极高。

Method: 采用三种编码方式和四种问题实例，比较不同类型进化算法的表现，并引入局部搜索和限制空间的方法。

Result: 遗传编程表现最佳，但未超越已知最优结果；通过改进方法，遗传算法成功演化出非线性度为241的9变量布尔函数。

Conclusion: 遗传编程在演化高非线性度布尔函数中表现优异，但结合局部搜索和限制空间的方法可进一步提升性能。

Abstract: This paper focuses on the problem of evolving Boolean functions of odd sizes
with high nonlinearity, a property of cryptographic relevance. Despite its
simple formulation, this problem turns out to be remarkably difficult. We
perform a systematic evaluation by considering three solution encodings and
four problem instances, analyzing how well different types of evolutionary
algorithms behave in finding a maximally nonlinear Boolean function. Our
results show that genetic programming generally outperforms other evolutionary
algorithms, although it falls short of the best-known results achieved by
ad-hoc heuristics. Interestingly, by adding local search and restricting the
space to rotation symmetric Boolean functions, we show that a genetic algorithm
with the bitstring encoding manages to evolve a $9$-variable Boolean function
with nonlinearity 241.

</details>

### [234] [Dual-Individual Genetic Algorithm: A Dual-Individual Approach for Efficient Training of Multi-Layer Neural Networks](https://arxiv.org/abs/2504.17346)
*Tran Thuy Nga Truong,Jooyong Kim*

Main category: cs.NE

TLDR: 本文提出了一种名为Dual-Individual GA的增强遗传算法，用于优化二分类任务的神经网络，通过Leader和Follower两种角色实现高效优化。


<details>
  <summary>Details</summary>
Motivation: 传统梯度方法在神经网络优化中存在局限性，需要手动调整层架构。本文旨在通过遗传算法自动优化网络结构和参数。

Method: 采用双个体遗传算法，Leader负责利用最优解，Follower负责探索多样性。引入自适应层维度机制，生成两种参数集。

Result: 实验显示，该方法在三层网络上达到99.04%训练准确率和80%测试准确率，优于传统梯度方法。

Conclusion: Dual-Individual GA在神经网络优化中表现出高效性和有效性，优于传统方法。

Abstract: This paper introduces an enhanced Genetic Algorithm technique called
Dual-Individual Genetic Algorithm (Dual-Individual GA), which optimizes neural
networks for binary image classification tasks, such as cat vs. non-cat
classification. The proposed method employs only two individuals for crossover,
represented by two parameter sets: Leader and Follower. The Leader focuses on
exploitation, representing the primary optimal solution at even-indexed
positions (0, 2, 4, ...), while the Follower promotes exploration by preserving
diversity and avoiding premature convergence, operating at odd-indexed
positions (1, 3, 5, ...). Leader and Follower are modeled as two phases or
roles. The key contributions of this work are threefold: (1) a self-adaptive
layer dimension mechanism that eliminates the need for manual tuning of layer
architectures; (2) generates two parameter sets, leader and follower parameter
sets, with 10 layer architecture configurations (5 for each set), ranked by
Pareto dominance and cost. post-optimization; and (3) demonstrated superior
performance compared to traditional gradient-based methods. Experimental
results show that the Dual-Individual GA achieves 99.04% training accuracy and
80% testing accuracy (cost = 0.034) on a three-layer network with architecture
[12288, 17, 4, 1], outperforming a gradient-based approach that achieves 98%
training accuracy and 80% testing accuracy (cost = 0.092) on a four-layer
network with architecture [12288, 20, 7, 5, 1]. These findings highlight the
efficiency and effectiveness of the proposed method in optimizing neural
networks.

</details>

### [235] [Revisiting Reset Mechanisms in Spiking Neural Networks for Sequential Modeling: Specialized Discretization for Binary Activated RNN](https://arxiv.org/abs/2504.17751)
*Enqi Zhang*

Main category: cs.NE

TLDR: 本文探讨了将脉冲神经网络（SNN）视为二元激活循环神经网络（RNN）的视角，分析了其在序列建模中的挑战，并提出了一种固定不应期SNN架构。


<details>
  <summary>Details</summary>
Motivation: 传统SNN在序列建模中存在缺乏有效记忆机制、生物启发组件理论不足以及无法并行训练的问题。

Method: 系统分析了重置操作和不应期的机制，重新评估其必要性，并提出固定不应期SNN架构。

Result: 提供了新的理论解释和见解，验证了固定不应期SNN的有效性。

Conclusion: 固定不应期SNN架构为序列建模提供了一种高效解决方案。

Abstract: In the field of image recognition, spiking neural networks (SNNs) have
achieved performance comparable to conventional artificial neural networks
(ANNs). In such applications, SNNs essentially function as traditional neural
networks with quantized activation values. This article focuses on an another
alternative perspective,viewing SNNs as binary-activated recurrent neural
networks (RNNs) for sequential modeling tasks.From this viewpoint, current SNN
architectures face several fundamental challenges in sequence modeling: (1)
Traditional models lack effective memory mechanisms for long-range sequence
modeling; (2) The biological-inspired components in SNNs (such as reset
mechanisms and refractory period applications) remain theoretically
under-explored for sequence tasks; (3) The RNN-like computational paradigm in
SNNs prevents parallel training across different timesteps.To address these
challenges, this study conducts a systematic analysis of the fundamental
mechanisms underlying reset operations and refractory periods in
binary-activated RNN-based SNN sequence models. We re-examine whether such
biological mechanisms are strictly necessary for generating sparse spiking
patterns, provide new theoretical explanations and insights, and ultimately
propose the fixed-refractory-period SNN architecture for sequence modeling.

</details>

<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [236] [Physics-informed features in supervised machine learning](https://arxiv.org/abs/2504.17112)
*Margherita Lampani,Sabrina Guastavino,Michele Piana,Federico Benvenuto*

Main category: stat.ML

TLDR: 论文提出了一种基于物理知识的特征学习方法，通过结合物理定律和维度分析构建非线性特征映射，提升模型可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统特征学习方法忽略物理意义，限制了模型在科学应用中的可解释性。

Method: 利用物理定律和维度分析构建非线性特征映射，结合领域知识进行学习。

Result: 方法提升了回归任务的预测性能和分类技能评分，同时增强了模型可解释性。

Conclusion: 物理知识驱动的特征学习方法不仅提升性能，还能在可解释机器学习中发现新的物理方程。

Abstract: Supervised machine learning involves approximating an unknown functional
relationship from a limited dataset of features and corresponding labels. The
classical approach to feature-based machine learning typically relies on
applying linear regression to standardized features, without considering their
physical meaning. This may limit model explainability, particularly in
scientific applications. This study proposes a physics-informed approach to
feature-based machine learning that constructs non-linear feature maps informed
by physical laws and dimensional analysis. These maps enhance model
interpretability and, when physical laws are unknown, allow for the
identification of relevant mechanisms through feature ranking. The method aims
to improve both predictive performance in regression tasks and classification
skill scores by integrating domain knowledge into the learning process, while
also enabling the potential discovery of new physical equations within the
context of explainable machine learning.

</details>

### [237] [Causal rule ensemble approach for multi-arm data](https://arxiv.org/abs/2504.17166)
*Ke Wan,Kensuke Tanioka,Toshio Shimokawa*

Main category: stat.ML

TLDR: 提出了一种可解释的机器学习框架，用于多臂试验中的异质性治疗效果（HTE）估计，通过规则集成方法平衡准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有HTE估计方法多针对二元治疗情况，且依赖黑盒模型，限制了在多臂试验中的适用性和可解释性。

Method: 采用基于规则的集成方法，包括规则生成、规则集成和HTE估计，确保预测准确性和可解释性。

Result: 在模拟研究和实际数据应用中，该方法相比现有方法具有更低的偏差和更高的估计准确性。

Conclusion: 该框架填补了准确性和可解释性之间的空白，为多臂HTE估计提供了有价值的工具，支持精准医学。

Abstract: Heterogeneous treatment effect (HTE) estimation is critical in medical
research. It provides insights into how treatment effects vary among
individuals, which can provide statistical evidence for precision medicine.
While most existing methods focus on binary treatment situations, real-world
applications often involve multiple interventions. However, current HTE
estimation methods are primarily designed for binary comparisons and often rely
on black-box models, which limit their applicability and interpretability in
multi-arm settings. To address these challenges, we propose an interpretable
machine learning framework for HTE estimation in multi-arm trials. Our method
employs a rule-based ensemble approach consisting of rule generation, rule
ensemble, and HTE estimation, ensuring both predictive accuracy and
interpretability. Through extensive simulation studies and real data
applications, the performance of our method was evaluated against
state-of-the-art multi-arm HTE estimation approaches. The results indicate that
our approach achieved lower bias and higher estimation accuracy compared with
those of existing methods. Furthermore, the interpretability of our framework
allows clearer insights into how covariates influence treatment effects,
facilitating clinical decision making. By bridging the gap between accuracy and
interpretability, our study contributes a valuable tool for multi-arm HTE
estimation, supporting precision medicine.

</details>

### [238] [Likelihood-Free Variational Autoencoders](https://arxiv.org/abs/2504.17622)
*Chen Xu,Qiang Wang,Lijun Sun*

Main category: stat.ML

TLDR: EnVAE提出了一种基于能量分数的无似然生成框架，解决了传统VAE因似然函数选择不当导致的模糊重建问题，并通过快速变体FEnVAE提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统VAE使用预定义的似然函数（如高斯分布）可能导致似然错误指定，造成重建模糊和数据保真度差，尤其是在高维数据（如图像）中。

Method: EnVAE采用确定性解码器和能量分数构建重建损失，实现无似然推断；FEnVAE通过解码器的局部平滑性和潜在变量后验分布的锐度，提出快速训练目标。

Result: 实验表明，EnVAE在重建和生成质量上优于基于似然的基线方法。

Conclusion: EnVAE为生成模型提供了一种通用、可扩展且统计上合理的非参数分布学习框架。

Abstract: Variational Autoencoders (VAEs) typically rely on a probabilistic decoder
with a predefined likelihood, most commonly an isotropic Gaussian, to model the
data conditional on latent variables. While convenient for optimization, this
choice often leads to likelihood misspecification, resulting in blurry
reconstructions and poor data fidelity, especially for high-dimensional data
such as images. In this work, we propose \textit{EnVAE}, a novel
likelihood-free generative framework that has a deterministic decoder and
employs the energy score -- a proper scoring rule -- to build the
reconstruction loss. This enables likelihood-free inference without requiring
explicit parametric density functions. To address the computational
inefficiency of the energy score, we introduce a fast variant, \textit{FEnVAE},
based on the local smoothness of the decoder and the sharpness of the posterior
distribution of latent variables. This yields an efficient single-sample
training objective that integrates seamlessly into existing VAE pipelines with
minimal overhead. Empirical results on standard benchmarks demonstrate that
\textit{EnVAE} achieves superior reconstruction and generation quality compared
to likelihood-based baselines. Our framework offers a general, scalable, and
statistically principled alternative for flexible and nonparametric
distribution learning in generative modeling.

</details>

### [239] [Evaluating Uncertainty in Deep Gaussian Processes](https://arxiv.org/abs/2504.17719)
*Matthijs van der Lende,Jeremias Lino Ferrao,Niclas Müller-Hof*

Main category: stat.ML

TLDR: 论文评估了深度高斯过程（DGPs）和深度Sigma点过程（DSPPs）在回归和分类任务中的不确定性量化表现，发现DSPPs在分布内校准表现良好，但在分布偏移下不如深度集成方法稳健。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习中可靠的不确定性估计至关重要，但DGPs和DSPPs在分布偏移下的校准和鲁棒性尚未充分研究。

Method: 在回归（CASP数据集）和分类（ESR数据集）任务中评估DGPs和DSPPs，使用MAE、准确性、NLL和ECE等指标，并测试其在合成特征级分布偏移下的鲁棒性。

Result: DSPPs在分布内校准表现良好，但在分布偏移下深度集成方法在性能和校准上均更稳健。

Conclusion: 深度集成方法在分布偏移下更具鲁棒性，而深度GP方法需谨慎评估其实际应用中的稳健性。

Abstract: Reliable uncertainty estimates are crucial in modern machine learning. Deep
Gaussian Processes (DGPs) and Deep Sigma Point Processes (DSPPs) extend GPs
hierarchically, offering promising methods for uncertainty quantification
grounded in Bayesian principles. However, their empirical calibration and
robustness under distribution shift relative to baselines like Deep Ensembles
remain understudied. This work evaluates these models on regression (CASP
dataset) and classification (ESR dataset) tasks, assessing predictive
performance (MAE, Accu- racy), calibration using Negative Log-Likelihood (NLL)
and Expected Calibration Error (ECE), alongside robustness under various
synthetic feature-level distribution shifts. Results indicate DSPPs provide
strong in-distribution calibration leveraging their sigma point approximations.
However, compared to Deep Ensembles, which demonstrated superior robustness in
both per- formance and calibration under the tested shifts, the GP-based
methods showed vulnerabilities, exhibiting particular sensitivity in the
observed metrics. Our findings underscore ensembles as a robust baseline,
suggesting that while deep GP methods offer good in-distribution calibration,
their practical robustness under distribution shift requires careful
evaluation. To facilitate reproducibility, we make our code available at
https://github.com/matthjs/xai-gp.

</details>

<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [240] [Multifaceted Evaluation of Audio-Visual Capability for MLLMs: Effectiveness, Efficiency, Generalizability and Robustness](https://arxiv.org/abs/2504.16936)
*Yusheng Zhao,Junyu Luo,Xiao Luo,Weizhi Zhang,Zhiping Xiao,Wei Ju,Philip S. Yu,Ming Zhang*

Main category: cs.MM

TLDR: 该论文对多模态大语言模型（MLLMs）的音频-视觉能力进行了多维度评估，发现其在零样本和小样本泛化能力上表现优异，但对视觉模态依赖性强，且易受对抗样本影响。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对MLLMs音频-视觉能力的全面评估，尤其是在分布偏移和对抗攻击等多样化场景中。

Method: 通过四个关键维度（有效性、效率、泛化性和鲁棒性）对MLLMs进行多方面的实验评估。

Result: MLLMs在零样本和小样本泛化能力上表现优异，但对视觉模态依赖性强，且在视觉输入受损或缺失时性能下降。虽然易受对抗样本影响，但比传统模型更鲁棒。

Conclusion: 研究揭示了MLLMs音频-视觉能力的优劣势，为未来改进和研究提供了指导。

Abstract: Multi-modal large language models (MLLMs) have recently achieved great
success in processing and understanding information from diverse modalities
(e.g., text, audio, and visual signals). Despite their growing popularity,
there remains a lack of comprehensive evaluation measuring the audio-visual
capabilities of these models, especially in diverse scenarios (e.g.,
distribution shifts and adversarial attacks). In this paper, we present a
multifaceted evaluation of the audio-visual capability of MLLMs, focusing on
four key dimensions: effectiveness, efficiency, generalizability, and
robustness. Through extensive experiments, we find that MLLMs exhibit strong
zero-shot and few-shot generalization abilities, enabling them to achieve great
performance with limited data. However, their success relies heavily on the
vision modality, which impairs performance when visual input is corrupted or
missing. Additionally, while MLLMs are susceptible to adversarial samples, they
demonstrate greater robustness compared to traditional models. The experimental
results and our findings provide insights into the audio-visual capabilities of
MLLMs, highlighting areas for improvement and offering guidance for future
research.

</details>

<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [241] [S2Vec: Self-Supervised Geospatial Embeddings](https://arxiv.org/abs/2504.16942)
*Shushman Choudhury,Elad Aharoni,Chandrakumari Suvarna,Iveel Tsogsuren,Abdul Rahman Kreidieh,Chun-Ta Lu,Neha Arora*

Main category: cs.SI

TLDR: S2Vec是一种自监督学习框架，用于生成通用的地理空间嵌入表示，通过S2几何库分区和掩码自编码技术，在多个社会经济预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 构建可扩展的通用地理空间表示对地理空间人工智能应用至关重要。

Method: 使用S2几何库将大区域划分为离散的S2单元，将单元内的特征向量栅格化为图像，并应用掩码自编码技术生成嵌入表示。

Result: 在三个大规模社会经济预测任务中表现优于现有图像嵌入方法，且与图像嵌入的多模态融合能进一步提升性能。

Conclusion: S2Vec能有效学习通用地理空间表示，并与其他数据模态互补，提升地理空间人工智能应用性能。

Abstract: Scalable general-purpose representations of the built environment are crucial
for geospatial artificial intelligence applications. This paper introduces
S2Vec, a novel self-supervised framework for learning such geospatial
embeddings. S2Vec uses the S2 Geometry library to partition large areas into
discrete S2 cells, rasterizes built environment feature vectors within cells as
images, and applies masked autoencoding on these rasterized images to encode
the feature vectors. This approach yields task-agnostic embeddings that capture
local feature characteristics and broader spatial relationships. We evaluate
S2Vec on three large-scale socioeconomic prediction tasks, showing its
competitive performance against state-of-the-art image-based embeddings. We
also explore the benefits of combining S2Vec embeddings with image-based
embeddings downstream, showing that such multimodal fusion can often improve
performance. Our results highlight how S2Vec can learn effective
general-purpose geospatial representations and how it can complement other data
modalities in geospatial artificial intelligence.

</details>

### [242] [MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation](https://arxiv.org/abs/2504.16946)
*Xiaotong Ye,Nicolas Bougie,Toshihiko Yamasaki,Narimasa Watanabe*

Main category: cs.SI

TLDR: 提出了一种可扩展的城市移动模拟框架，解决了现有方法在交通选择简化和计算资源需求上的不足，支持4000多个代理的模拟。


<details>
  <summary>Details</summary>
Motivation: 现有方法在模拟现代城市行为时过于简化交通选择，且计算资源需求高，无法支持大规模人口模拟。

Method: 构建虚拟城市模型，结合调查数据建模行为选择与移动偏好，开发可扩展的模拟框架。

Result: 框架支持4000多个代理的模拟，并通过微观和宏观分析验证了生成行为的真实性。

Conclusion: 该框架在复杂性和可扩展性上取得平衡，为城市行为模拟提供了实用工具。

Abstract: Generative agents offer promising capabilities for simulating realistic urban
behaviors. However, existing methods oversimplify transportation choices in
modern cities, and require prohibitive computational resources for large-scale
population simulation. To address these limitations, we first present a virtual
city that features multiple functional buildings and transportation modes.
Then, we conduct extensive surveys to model behavioral choices and mobility
preferences among population groups. Building on these insights, we introduce a
simulation framework that captures the complexity of urban mobility while
remaining scalable, enabling the simulation of over 4,000 agents. To assess the
realism of the generated behaviors, we perform a series of micro and
macro-level analyses. Beyond mere performance comparison, we explore insightful
experiments, such as predicting crowd density from movement patterns and
identifying trends in vehicle preferences across agent demographics.

</details>

### [243] [SCRAG: Social Computing-Based Retrieval Augmented Generation for Community Response Forecasting in Social Media Environments](https://arxiv.org/abs/2504.16947)
*Dachun Sun,You Lyu,Jinning Li,Yizhuo Chen,Tianshi Wang,Tomoyoshi Kimura,Tarek Abdelzaher*

Main category: cs.SI

TLDR: SCRAG是一个基于社交计算的预测框架，用于预测社区对社交媒体帖子的反应，结合了LLM和RAG技术，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）在动态社交媒体环境中预测社区反应时的局限性，如依赖静态数据和易产生幻觉。

Method: 整合LLM与RAG技术，检索目标社区的历史反应和外部知识（如新闻文章），以预测新帖子的社区反应。

Result: 在X平台（原Twitter）的六种场景中，关键评估指标平均提升超过10%。

Conclusion: SCRAG为需要准确预测社区反应的应用提供了有效的社交计算工具。

Abstract: This paper introduces SCRAG, a prediction framework inspired by social
computing, designed to forecast community responses to real or hypothetical
social media posts. SCRAG can be used by public relations specialists (e.g., to
craft messaging in ways that avoid unintended misinterpretations) or public
figures and influencers (e.g., to anticipate social responses), among other
applications related to public sentiment prediction, crisis management, and
social what-if analysis. While large language models (LLMs) have achieved
remarkable success in generating coherent and contextually rich text, their
reliance on static training data and susceptibility to hallucinations limit
their effectiveness at response forecasting in dynamic social media
environments. SCRAG overcomes these challenges by integrating LLMs with a
Retrieval-Augmented Generation (RAG) technique rooted in social computing.
Specifically, our framework retrieves (i) historical responses from the target
community to capture their ideological, semantic, and emotional makeup, and
(ii) external knowledge from sources such as news articles to inject
time-sensitive context. This information is then jointly used to forecast the
responses of the target community to new posts or narratives. Extensive
experiments across six scenarios on the X platform (formerly Twitter), tested
with various embedding models and LLMs, demonstrate over 10% improvements on
average in key evaluation metrics. A concrete example further shows its
effectiveness in capturing diverse ideologies and nuances. Our work provides a
social computing tool for applications where accurate and concrete insights
into community responses are crucial.

</details>

<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [244] [Plasma State Monitoring and Disruption Characterization using Multimodal VAEs](https://arxiv.org/abs/2504.17710)
*Yoeri Poels,Alessandro Pau,Christian Donner,Giulio Romanelli,Olivier Sauter,Cristina Venturini,Vlado Menkovski,the TCV team,the WPTE team*

Main category: physics.plasm-ph

TLDR: 该论文提出了一种基于变分自编码器（VAE）的方法，用于等离子体状态的解释性表征，以预测和区分托卡马克中的破裂事件。


<details>
  <summary>Details</summary>
Motivation: 托卡马克中等离子体破裂会产生巨大的热和电磁负载，但目前破裂的机制尚未完全理解，数据驱动模型虽能预测但缺乏解释性。

Method: 扩展VAE框架，包括连续投影等离子体轨迹、多模态结构分离操作区域，以及区分破裂区域，生成低维潜在表示。

Result: 方法在1600次TCV放电数据中验证，能有效识别破裂风险、区分不同类型破裂，并支持下游分析。

Conclusion: 该方法能以解释性方式识别不同操作区域及其与破裂的关联，为破裂预测提供了新工具。

Abstract: When a plasma disrupts in a tokamak, significant heat and electromagnetic
loads are deposited onto the surrounding device components. These forces scale
with plasma current and magnetic field strength, making disruptions one of the
key challenges for future devices. Unfortunately, disruptions are not fully
understood, with many different underlying causes that are difficult to
anticipate. Data-driven models have shown success in predicting them, but they
only provide limited interpretability. On the other hand, large-scale
statistical analyses have been a great asset to understanding disruptive
patterns. In this paper, we leverage data-driven methods to find an
interpretable representation of the plasma state for disruption
characterization. Specifically, we use a latent variable model to represent
diagnostic measurements as a low-dimensional, latent representation. We build
upon the Variational Autoencoder (VAE) framework, and extend it for (1)
continuous projections of plasma trajectories; (2) a multimodal structure to
separate operating regimes; and (3) separation with respect to disruptive
regimes. Subsequently, we can identify continuous indicators for the disruption
rate and the disruptivity based on statistical properties of measurement data.
The proposed method is demonstrated using a dataset of approximately 1600 TCV
discharges, selecting for flat-top disruptions or regular terminations. We
evaluate the method with respect to (1) the identified disruption risk and its
correlation with other plasma properties; (2) the ability to distinguish
different types of disruptions; and (3) downstream analyses. For the latter, we
conduct a demonstrative study on identifying parameters connected to
disruptions using counterfactual-like analysis. Overall, the method can
adequately identify distinct operating regimes characterized by varying
proximity to disruptions in an interpretable manner.

</details>

<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [245] [Analyzing Value Functions of States in Parametric Markov Chains](https://arxiv.org/abs/2504.17020)
*Kasper Engelen,Guillermo A. Pérez,Shrisha Rao*

Main category: cs.LO

TLDR: 本文提出了一种高效算法，通过合并等价类来简化参数马尔可夫链（pMC）的验证，并证明了其在减少模型大小和加速单调性检查方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 参数马尔可夫链（pMC）验证的复杂性促使研究者寻找更易验证的性质，如单调性。本文旨在通过简化pMC结构来提升验证效率。

Method: 将单调性问题转化为状态可达概率的比较问题，并利用高效算法合并等价类。

Result: 实验表明，该方法能显著减少模型大小并加速单调性检查和参数提升算法。

Conclusion: 提出的算法可作为快速预处理步骤，提升pMC验证的实践效率。

Abstract: Parametric Markov chains (pMC) are used to model probabilistic systems with
unknown or partially known probabilities. Although (universal) pMC verification
for reachability properties is known to be coETR-complete, there have been
efforts to approach it using potentially easier-to-check properties such as
asking whether the pMC is monotonic in certain parameters. In this paper, we
first reduce monotonicity to asking whether the reachability probability from a
given state is never less than that of another given state. Recent results for
the latter property imply an efficient algorithm to collapse same-value
equivalence classes, which in turn preserves verification results and
monotonicity. We implement our algorithm to collapse "trivial" equivalence
classes in the pMC and show empirical evidence for the following: First, the
collapse gives reductions in size for some existing benchmarks and significant
reductions on some custom benchmarks; Second, the collapse speeds up existing
algorithms to check monotonicity and parameter lifting, and hence can be used
as a fast pre-processing step in practice.

</details>

<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [246] [Towards a HIPAA Compliant Agentic AI System in Healthcare](https://arxiv.org/abs/2504.17669)
*Subash Neupane,Shaswata Mitra,Sudip Mittal,Shahram Rahimi*

Main category: cs.MA

TLDR: 论文介绍了一个符合HIPAA标准的Agentic AI框架，通过动态策略执行确保医疗数据合规性。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的AI系统在临床工作流中的应用增加，确保敏感医疗数据（如PHI）的合规性成为关键挑战。

Method: 框架整合了ABAC细粒度访问控制、混合PHI清理管道（结合正则表达式和BERT模型）以及不可变审计跟踪。

Result: 框架旨在减少PHI泄露风险，同时满足HIPAA合规要求。

Conclusion: 该框架为AI系统在医疗领域的合规应用提供了可行解决方案。

Abstract: Agentic AI systems powered by Large Language Models (LLMs) as their
foundational reasoning engine, are transforming clinical workflows such as
medical report generation and clinical summarization by autonomously analyzing
sensitive healthcare data and executing decisions with minimal human oversight.
However, their adoption demands strict compliance with regulatory frameworks
such as Health Insurance Portability and Accountability Act (HIPAA),
particularly when handling Protected Health Information (PHI). This
work-in-progress paper introduces a HIPAA-compliant Agentic AI framework that
enforces regulatory compliance through dynamic, context-aware policy
enforcement. Our framework integrates three core mechanisms: (1)
Attribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid
PHI sanitization pipeline combining regex patterns and BERT-based model to
minimize leakage, and (3) immutable audit trails for compliance verification.

</details>

<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [247] [From Randomized Response to Randomized Index: Answering Subset Counting Queries with Local Differential Privacy](https://arxiv.org/abs/2504.17523)
*Qingqing Ye,Liantong Yu,Kai Huang,Xiaokui Xiao,Weiran Liu,Haibo Hu*

Main category: cs.DB

TLDR: 论文提出了一种基于随机化索引的LDP方法CRIAD，替代传统扰动值的方法，显著提高了查询准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LDP扰动机制通过扰动原始值保护隐私，但会导致数据失真和效用下降，因此需要一种新方法。

Method: CRIAD通过随机化索引而非扰动值，结合多虚拟、多样本和多组策略，提供灵活且可扩展的解决方案。

Result: CRIAD在理论和实验中均优于传统值扰动方法，提供更准确的查询结果。

Conclusion: CRIAD是一种高效、灵活的LDP解决方案，适用于不同隐私需求和领域规模。

Abstract: Local Differential Privacy (LDP) is the predominant privacy model for
safeguarding individual data privacy. Existing perturbation mechanisms
typically require perturbing the original values to ensure acceptable privacy,
which inevitably results in value distortion and utility deterioration. In this
work, we propose an alternative approach -- instead of perturbing values, we
apply randomization to indexes of values while ensuring rigorous LDP
guarantees. Inspired by the deniability of randomized indexes, we present CRIAD
for answering subset counting queries on set-value data. By integrating a
multi-dummy, multi-sample, and multi-group strategy, CRIAD serves as a fully
scalable solution that offers flexibility across various privacy requirements
and domain sizes, and achieves more accurate query results than any existing
methods. Through comprehensive theoretical analysis and extensive experimental
evaluations, we validate the effectiveness of CRIAD and demonstrate its
superiority over traditional value-perturbation mechanisms.

</details>

### [248] [High-Fidelity And Complex Test Data Generation For Real-World SQL Code Generation Services](https://arxiv.org/abs/2504.17203)
*Shivasankari Kannan,Yeounoh Chung,Amita Gondi,Tristan Swadell,Fatma Ozcan*

Main category: cs.DB

TLDR: 利用大型语言模型（LLM）生成高保真测试数据，解决传统方法在复杂SQL模式下的不足。


<details>
  <summary>Details</summary>
Motivation: 工业环境中生产数据受限，传统方法生成的数据保真度低且无法处理复杂结构和语义关系。

Method: 结合LLM（如Gemini）及预处理和后处理步骤，生成符合复杂结构和语义约束的测试数据。

Result: 成功生成适用于复杂SQL查询的高保真测试数据，支持全面测试SQL代码生成服务。

Conclusion: LLM方法为工业SQL代码生成服务提供了实用的高保真测试数据生成方案。

Abstract: The demand for high-fidelity test data is paramount in industrial settings
where access to production data is largely restricted. Traditional data
generation methods often fall short, struggling with low-fidelity and the
ability to model complex data structures and semantic relationships that are
critical for testing complex SQL code generation services like Natural Language
to SQL (NL2SQL). In this paper, we address the critical need for generating
syntactically correct and semantically ``meaningful'' mock data for complex
schema that includes columns with nested structures that we frequently
encounter in Google SQL code generation workloads. We highlight the limitations
of existing approaches used in production, particularly their inability to
handle large and complex schema, as well as the lack of semantically coherent
test data that lead to limited test coverage. We demonstrate that by leveraging
Large Language Models (LLMs) and incorporating strategic pre- and
post-processing steps, we can generate realistic high-fidelity test data that
adheres to complex structural constraints and maintains semantic integrity to
the test targets (SQL queries/functions). This approach supports comprehensive
testing of complex SQL queries involving joins, aggregations, and even deeply
nested subqueries, ensuring robust evaluation of SQL code generation services,
like NL2SQL and SQL Code Assistant services. Our results demonstrate the
practical utility of an out-of-the-box LLM (\textit{gemini}) based test data
generation for industrial SQL code generation services where generating
realistic test data is essential due to the frequent unavailability of
production datasets.

</details>

<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [249] [Optimized Cloud Resource Allocation Using Genetic Algorithms for Energy Efficiency and QoS Assurance](https://arxiv.org/abs/2504.17675)
*Caroline Panggabean,Devaraj Verma C,Bhagyashree Gogoi,Ranju Limbu,Rhythm Sarker*

Main category: cs.DC

TLDR: 本文提出了一种基于遗传算法（GA）的虚拟机（VM）放置与整合方法，旨在降低能耗并满足服务质量（QoS）约束，优于传统启发式算法。


<details>
  <summary>Details</summary>
Motivation: 云计算环境需要动态高效的资源管理，以确保性能最优、能耗降低并遵守服务级别协议（SLA）。

Method: 采用遗传算法动态调整虚拟机分配，适应实时工作负载变化。

Result: 实验结果显示能耗、虚拟机迁移次数、SLA违规率和执行时间显著降低，并通过热图验证了关键性能指标间的强相关性。

Conclusion: 该方法有效优化了云资源利用率。

Abstract: Cloud computing environments demand dynamic and efficient resource management
to ensure optimal performance, reduced energy consumption, and adherence to
Service Level Agreements (SLAs). This paper presents a Genetic Algorithm
(GA)-based approach for Virtual Machine (VM) placement and consolidation,
aiming to minimize power usage while maintaining QoS constraints. The proposed
method dynamically adjusts VM allocation based on real-time workload
variations, outperforming traditional heuristics such as First Fit Decreasing
(FFD) and Best Fit Decreasing (BFD). Experimental results show notable
reductions in energy consumption, VM migrations, SLA violation rates, and
execution time. A correlation heatmap further illustrates strong relationships
among these key performance indicators, confirming the effectiveness of our
approach in optimizing cloud resource utilization.

</details>

<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [250] [ePBR: Extended PBR Materials in Image Synthesis](https://arxiv.org/abs/2504.17062)
*Yu Guo,Zhiqiang Lao,Xiyun Song,Yubin Zhou,Zongfang Lin,Heather Yu*

Main category: cs.GR

TLDR: 论文提出了一种扩展的PBR材料（ePBR），通过结合反射和透射特性，实现了透明材料（如玻璃和窗户）的可控合成。


<details>
  <summary>Details</summary>
Motivation: 现有的PBR材料在处理高反射和透明表面时表现不佳，而基于学习的方法缺乏物理一致性。

Method: 提出了一种显式的内在合成框架，扩展了内在图像表示，以包含反射和透射特性。

Result: 通过ePBR材料，能够精确控制材料编辑，实现可解释的图像合成。

Conclusion: 扩展的内在图像表示和ePBR材料为透明材料的合成提供了高效且可控的解决方案。

Abstract: Realistic indoor or outdoor image synthesis is a core challenge in computer
vision and graphics. The learning-based approach is easy to use but lacks
physical consistency, while traditional Physically Based Rendering (PBR) offers
high realism but is computationally expensive. Intrinsic image representation
offers a well-balanced trade-off, decomposing images into fundamental
components (intrinsic channels) such as geometry, materials, and illumination
for controllable synthesis. However, existing PBR materials struggle with
complex surface models, particularly high-specular and transparent surfaces. In
this work, we extend intrinsic image representations to incorporate both
reflection and transmission properties, enabling the synthesis of transparent
materials such as glass and windows. We propose an explicit intrinsic
compositing framework that provides deterministic, interpretable image
synthesis. With the Extended PBR (ePBR) Materials, we can effectively edit the
materials with precise controls.

</details>

### [251] [CasualHDRSplat: Robust High Dynamic Range 3D Gaussian Splatting from Casually Captured Videos](https://arxiv.org/abs/2504.17728)
*Shucheng Gong,Lingzhe Zhao,Wenpu Li,Hong Xie,Yin Zhang,Shiyu Zhao,Peidong Liu*

Main category: cs.GR

TLDR: 论文提出了一种名为CasualHDRSplat的单阶段方法，用于从随意拍摄的视频中重建3D HDR场景，解决了现有方法依赖固定曝光时间和多视图图像的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于多视图图像的HDR场景重建方法通常需要固定相机位置和不同曝光时间的图像，耗时且不灵活。

Method: CasualHDRSplat采用统一的微分物理成像模型，通过连续时间轨迹约束联合优化曝光时间、相机响应函数、相机位姿和3D HDR场景。

Result: 实验表明，该方法在鲁棒性和渲染质量上优于现有方法。

Conclusion: CasualHDRSplat提供了一种灵活且高效的HDR场景重建解决方案。

Abstract: Recently, photo-realistic novel view synthesis from multi-view images, such
as neural radiance field (NeRF) and 3D Gaussian Splatting (3DGS), have garnered
widespread attention due to their superior performance. However, most works
rely on low dynamic range (LDR) images, which limits the capturing of richer
scene details. Some prior works have focused on high dynamic range (HDR) scene
reconstruction, typically require capturing of multi-view sharp images with
different exposure times at fixed camera positions during exposure times, which
is time-consuming and challenging in practice. For a more flexible data
acquisition, we propose a one-stage method: \textbf{CasualHDRSplat} to easily
and robustly reconstruct the 3D HDR scene from casually captured videos with
auto-exposure enabled, even in the presence of severe motion blur and varying
unknown exposure time. \textbf{CasualHDRSplat} contains a unified
differentiable physical imaging model which first applies continuous-time
trajectory constraint to imaging process so that we can jointly optimize
exposure time, camera response function (CRF), camera poses, and sharp 3D HDR
scene. Extensive experiments demonstrate that our approach outperforms existing
methods in terms of robustness and rendering quality. Our source code will be
available at https://github.com/WU-CVGL/CasualHDRSplat

</details>

<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [252] [Automating tumor-infiltrating lymphocyte assessment in breast cancer histopathology images using QuPath: a transparent and accessible machine learning pipeline](https://arxiv.org/abs/2504.16979)
*Masoud Tafavvoghi,Lars Ailo Bongo,André Berli Delgado,Nikita Shvetsov,Anders Sildnes,Line Moi,Lill-Tove Rasmussen Busund,Kajsa Møllersen*

Main category: q-bio.QM

TLDR: 研究开发了一个基于QuPath的端到端肿瘤浸润淋巴细胞（TILs）评估流程，证明了现有工具在自动化完成复杂任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索利用易获取的工具（如QuPath）自动化完成TILs评估的可行性，以简化病理分析流程。

Method: 1. 训练像素分类器分割乳腺癌H&E染色全切片图像中的肿瘤和基质；2. 使用预训练的StarDist模型检测细胞并训练二元分类器区分TILs；3. 计算TIL密度并分类。

Result: 与病理学家评分相比，外部测试集的Cohen's kappa为0.71，验证了流程的可靠性。

Conclusion: 现有软件可为乳腺癌H&E切片中的TILs评估提供实用解决方案。

Abstract: In this study, we built an end-to-end tumor-infiltrating lymphocytes (TILs)
assessment pipeline within QuPath, demonstrating the potential of easily
accessible tools to perform complex tasks in a fully automatic fashion. First,
we trained a pixel classifier to segment tumor, tumor-associated stroma, and
other tissue compartments in breast cancer H&E-stained whole-slide images (WSI)
to isolate tumor-associated stroma for subsequent analysis. Next, we applied a
pre-trained StarDist deep learning model in QuPath for cell detection and used
the extracted cell features to train a binary classifier distinguishing TILs
from other cells. To evaluate our TILs assessment pipeline, we calculated the
TIL density in each WSI and categorized them as low, medium, or high TIL
levels. Our pipeline was evaluated against pathologist-assigned TIL scores,
achieving a Cohen's kappa of 0.71 on the external test set, corroborating
previous research findings. These results confirm that existing software can
offer a practical solution for the assessment of TILs in H&E-stained WSIs of
breast cancer.

</details>

<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [253] [A Machine Learning Approach for Denoising and Upsampling HRTFs](https://arxiv.org/abs/2504.17586)
*Xuyi Hu,Jian Li,Lorenzo Picinali,Aidan O. T. Hogg*

Main category: cs.SD

TLDR: 本文提出了一种结合U-Net和AE-GAN的新方法，用于从稀疏且有噪声的HRTF测量数据中实现高效上采样，显著降低了测量时间和环境要求。


<details>
  <summary>Details</summary>
Motivation: 个性化HRTF能提高声音定位准确性，但传统测量方法耗时且需无噪环境，限制了其广泛应用。

Method: 使用HRTF Denoisy U-Net进行去噪，结合AE-GAN从三个测量点上采样。

Result: 方法实现了5.41 dB的LSD误差和0.0070的余弦相似度损失，证明了其有效性。

Conclusion: 该方法为HRTF测量提供了一种高效且环境适应性强的解决方案。

Abstract: The demand for realistic virtual immersive audio continues to grow, with
Head-Related Transfer Functions (HRTFs) playing a key role. HRTFs capture how
sound reaches our ears, reflecting unique anatomical features and enhancing
spatial perception. It has been shown that personalized HRTFs improve
localization accuracy, but their measurement remains time-consuming and
requires a noise-free environment. Although machine learning has been shown to
reduce the required measurement points and, thus, the measurement time, a
controlled environment is still necessary. This paper proposes a method to
address this constraint by presenting a novel technique that can upsample
sparse, noisy HRTF measurements. The proposed approach combines an HRTF Denoisy
U-Net for denoising and an Autoencoding Generative Adversarial Network (AE-GAN)
for upsampling from three measurement points. The proposed method achieves a
log-spectral distortion (LSD) error of 5.41 dB and a cosine similarity loss of
0.0070, demonstrating the method's effectiveness in HRTF upsampling.

</details>

### [254] [Unleashing the Power of Natural Audio Featuring Multiple Sound Sources](https://arxiv.org/abs/2504.17782)
*Xize Cheng,Slytherin Wang,Zehan Wang,Rongjie Huang,Tao Jin,Zhou Zhao*

Main category: cs.SD

TLDR: ClearSep提出了一种新框架，通过数据引擎分解自然混合音频，提升真实场景下的声音分离性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖人工混合音频训练、难以泛化到自然混合音频的问题。

Method: 使用数据引擎分解自然混合音频，引入重混评价指标迭代优化分离性能，并提出针对分离音轨的训练策略。

Result: ClearSep在多个声音分离任务中达到最先进性能。

Conclusion: ClearSep在自然音频场景中具有推动声音分离技术进步的潜力。

Abstract: Universal sound separation aims to extract clean audio tracks corresponding
to distinct events from mixed audio, which is critical for artificial auditory
perception. However, current methods heavily rely on artificially mixed audio
for training, which limits their ability to generalize to naturally mixed audio
collected in real-world environments. To overcome this limitation, we propose
ClearSep, an innovative framework that employs a data engine to decompose
complex naturally mixed audio into multiple independent tracks, thereby
allowing effective sound separation in real-world scenarios. We introduce two
remix-based evaluation metrics to quantitatively assess separation quality and
use these metrics as thresholds to iteratively apply the data engine alongside
model training, progressively optimizing separation performance. In addition,
we propose a series of training strategies tailored to these separated
independent tracks to make the best use of them. Extensive experiments
demonstrate that ClearSep achieves state-of-the-art performance across multiple
sound separation tasks, highlighting its potential for advancing sound
separation in natural audio scenarios. For more examples and detailed results,
please visit our demo page at https://clearsep.github.io.

</details>

<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [255] [Robo-Troj: Attacking LLM-based Task Planners](https://arxiv.org/abs/2504.17070)
*Mohaiminul Al Nahian,Zainab Altaweel,David Reitano,Sabbir Ahmed,Saumitra Lohokare,Shiqi Zhang,Adnan Siraj Rakin*

Main category: cs.RO

TLDR: 本文提出了Robo-Troj，一种针对基于LLM的任务规划器的多触发后门攻击方法，旨在揭示其安全漏洞并促进安全机器人系统的发展。


<details>
  <summary>Details</summary>
Motivation: 尽管基于LLM的任务规划器表现优异，但其安全性研究不足。本文旨在填补这一空白，揭示潜在的安全风险。

Method: 开发了Robo-Troj，一种多触发后门攻击方法，通过特定触发词（如“herical”）激活恶意行为，并优化触发词选择。

Result: 成功展示了基于LLM的任务规划器的脆弱性，证明了多触发攻击的可行性。

Conclusion: 通过揭示LLM任务规划器的安全漏洞，呼吁加强机器人系统的安全性研究。

Abstract: Robots need task planning methods to achieve goals that require more than
individual actions. Recently, large language models (LLMs) have demonstrated
impressive performance in task planning. LLMs can generate a step-by-step
solution using a description of actions and the goal. Despite the successes in
LLM-based task planning, there is limited research studying the security
aspects of those systems. In this paper, we develop Robo-Troj, the first
multi-trigger backdoor attack for LLM-based task planners, which is the main
contribution of this work. As a multi-trigger attack, Robo-Troj is trained to
accommodate the diversity of robot application domains. For instance, one can
use unique trigger words, e.g., "herical", to activate a specific malicious
behavior, e.g., cutting hand on a kitchen robot. In addition, we develop an
optimization method for selecting the trigger words that are most effective.
Through demonstrating the vulnerability of LLM-based planners, we aim to
promote the development of secured robot systems.

</details>

### [256] [Object Pose Estimation by Camera Arm Control Based on the Next Viewpoint Estimation](https://arxiv.org/abs/2504.17424)
*Tomoki Mizuno,Kazuya Yabashi,Tsuyoshi Tasaki*

Main category: cs.RO

TLDR: 提出了一种新方法，通过同时估计下一个视角（NV）来提高零售机器人对简单形状产品的姿态估计准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经网络的RGBD相机方法在特征少时准确性下降，而传统数学模型方法难以有效估计NV。

Method: 开发了一种新的姿态估计神经网络，同时估计NV，利用姿态估计与NV估计的关系。

Result: 实验显示，NV估计使姿态估计成功率提高7.4个百分点至77.3%，机器人成功展示84.2%的产品。

Conclusion: 新方法显著提升了简单形状产品的姿态估计和展示效果。

Abstract: We have developed a new method to estimate a Next Viewpoint (NV) which is
effective for pose estimation of simple-shaped products for product display
robots in retail stores. Pose estimation methods using Neural Networks (NN)
based on an RGBD camera are highly accurate, but their accuracy significantly
decreases when the camera acquires few texture and shape features at a current
view point. However, it is difficult for previous mathematical model-based
methods to estimate effective NV which is because the simple shaped objects
have few shape features. Therefore, we focus on the relationship between the
pose estimation and NV estimation. When the pose estimation is more accurate,
the NV estimation is more accurate. Therefore, we develop a new pose estimation
NN that estimates NV simultaneously. Experimental results showed that our NV
estimation realized a pose estimation success rate 77.3\%, which was 7.4pt
higher than the mathematical model-based NV calculation did. Moreover, we
verified that the robot using our method displayed 84.2\% of products.

</details>

### [257] [BIM-Constrained Optimization for Accurate Localization and Deviation Correction in Construction Monitoring](https://arxiv.org/abs/2504.17693)
*Asier Bikandi,Muhammad Shaheer,Hriday Bavle,Jayan Jevanesan,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TLDR: 论文提出了一种基于BIM的漂移校正方法，通过将现实环境中的平面与BIM模型中的平面对齐，优化SLAM和BIM之间的转换，显著减少了建筑监控中AR应用的漂移误差。


<details>
  <summary>Details</summary>
Motivation: 建筑工地环境复杂，传统跟踪方法因特征缺失和动态变化导致AR模型与物理世界对齐不准确，需要一种更稳健的解决方案。

Method: 利用BIM作为先验结构知识，通过优化技术将现实检测到的平面与BIM中的平面匹配，计算SLAM和BIM坐标系之间的转换，减少漂移。

Result: 实验表明，该方法平均减少了52.24%的角度偏差和60.8%的距离误差，显著提升了AR对齐精度。

Conclusion: 结合BIM的漂移校正方法有效解决了建筑环境中AR应用的长期定位问题，提高了可视化准确性。

Abstract: Augmented reality (AR) applications for construction monitoring rely on
real-time environmental tracking to visualize architectural elements. However,
construction sites present significant challenges for traditional tracking
methods due to featureless surfaces, dynamic changes, and drift accumulation,
leading to misalignment between digital models and the physical world. This
paper proposes a BIM-aware drift correction method to address these challenges.
Instead of relying solely on SLAM-based localization, we align ``as-built"
detected planes from the real-world environment with ``as-planned"
architectural planes in BIM. Our method performs robust plane matching and
computes a transformation (TF) between SLAM (S) and BIM (B) origin frames using
optimization techniques, minimizing drift over time. By incorporating BIM as
prior structural knowledge, we can achieve improved long-term localization and
enhanced AR visualization accuracy in noisy construction environments. The
method is evaluated through real-world experiments, showing significant
reductions in drift-induced errors and optimized alignment consistency. On
average, our system achieves a reduction of 52.24% in angular deviations and a
reduction of 60.8% in the distance error of the matched walls compared to the
initial manual alignment by the user.

</details>

### [258] [Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control](https://arxiv.org/abs/2504.17771)
*Haochen Wang,Zhiwei Shi,Chengxi Zhu,Yafei Qiao,Cheng Zhang,Fan Yang,Pengjie Ren,Lan Lu,Dong Xuan*

Main category: cs.RO

TLDR: 本文提出了一种混合控制系统HAMLET，结合基于模型的方法和学习策略（模仿学习与强化学习），用于敏捷羽毛球机器人控制，显著提高了成功率和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有学习策略（如模仿学习和强化学习）在敏捷机器人任务中表现优异，但缺乏与基于模型方法的结合，导致训练复杂性和安全性问题。

Method: 提出基于模型的车身运动策略作为臂策略基础，并设计物理引导的“IL+RL”训练框架，利用特权信息指导训练，同时在IL阶段训练评论家模型以减少性能下降。

Result: 在自研羽毛球机器人上实现了94.5%对发球机和90.7%对人玩家的成功率。

Conclusion: HAMLET系统成功结合了模型与学习策略，显著提升性能，并可推广至其他敏捷移动操作任务。

Abstract: Learning-based methods, such as imitation learning (IL) and reinforcement
learning (RL), can produce excel control policies over challenging agile robot
tasks, such as sports robot. However, no existing work has harmonized
learning-based policy with model-based methods to reduce training complexity
and ensure the safety and stability for agile badminton robot control. In this
paper, we introduce \ourmethod, a novel hybrid control system for agile
badminton robots. Specifically, we propose a model-based strategy for chassis
locomotion which provides a base for arm policy. We introduce a
physics-informed ``IL+RL'' training framework for learning-based arm policy. In
this train framework, a model-based strategy with privileged information is
used to guide arm policy training during both IL and RL phases. In addition, we
train the critic model during IL phase to alleviate the performance drop issue
when transitioning from IL to RL. We present results on our self-engineered
badminton robot, achieving 94.5% success rate against the serving machine and
90.7% success rate against human players. Our system can be easily generalized
to other agile mobile manipulation tasks such as agile catching and table
tennis. Our project website: https://dreamstarring.github.io/HAMLET/.

</details>

<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [259] [Quantum Autoencoder for Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2504.17548)
*Kilian Tscharke,Maximilian Wendlinger,Afrae Ahouzi,Pallavi Bhardwaj,Kaweh Amoi-Taleghani,Michael Schrödl-Baumann,Pascal Debus*

Main category: quant-ph

TLDR: 提出了一种基于量子自编码器（QAE）的新框架，用于企业级多变量时间序列（MTS）异常检测，性能与神经网络自编码器相当但参数更少。


<details>
  <summary>Details</summary>
Motivation: 现有QAE方法仅适用于单变量时间序列，无法满足企业级多变量数据的需求。

Method: 设计并验证了一种专门用于MTS异常检测的QAE框架。

Result: 在模拟SAP系统数据的实验中，QAE表现与神经网络自编码器相当，且参数更少。

Conclusion: QAE是企业级半监督异常检测的高效替代方案。

Abstract: Anomaly Detection (AD) defines the task of identifying observations or events
that deviate from typical - or normal - patterns, a critical capability in IT
security for recognizing incidents such as system misconfigurations, malware
infections, or cyberattacks. In enterprise environments like SAP HANA Cloud
systems, this task often involves monitoring high-dimensional, multivariate
time series (MTS) derived from telemetry and log data. With the advent of
quantum machine learning offering efficient calculations in high-dimensional
latent spaces, many avenues open for dealing with such complex data. One
approach is the Quantum Autoencoder (QAE), an emerging and promising method
with potential for application in both data compression and AD. However, prior
applications of QAEs to time series AD have been restricted to univariate data,
limiting their relevance for real-world enterprise systems. In this work, we
introduce a novel QAE-based framework designed specifically for MTS AD towards
enterprise scale. We theoretically develop and experimentally validate the
architecture, demonstrating that our QAE achieves performance competitive with
neural-network-based autoencoders while requiring fewer trainable parameters.
We evaluate our model on datasets that closely reflect SAP system telemetry and
show that the proposed QAE is a viable and efficient alternative for
semisupervised AD in real-world enterprise settings.

</details>

### [260] [Near-Term Pseudorandom and Pseudoresource Quantum States](https://arxiv.org/abs/2504.17650)
*Andrew Tanggara,Mile Gu,Kishor Bharti*

Main category: quant-ph

TLDR: 本文提出了一种称为T-PRS的伪随机量子态，适用于计算能力受限的观察者，并分析了其所需的量子资源。


<details>
  <summary>Details</summary>
Motivation: 现有PRS构造仅适用于多项式时间量子计算，而本文旨在放宽效率定义，使其适用于近量子计算机的计算能力受限场景。

Method: 引入T-PRS概念，基于量子安全伪随机函数和伪随机函数构造，并分析其所需的相干性、纠缠性和魔法资源。

Result: 针对不同计算能力的观察者，量化了T-PRS所需的资源，并展示了资源需求随计算能力受限而降低的趋势。

Conclusion: T-PRS为计算能力受限的观察者提供了一种高效的伪随机量子态，并扩展了伪资源对的概念。

Abstract: A pseudorandom quantum state (PRS) is an ensemble of quantum states
indistinguishable from Haar-random states to observers with efficient quantum
computers. It allows one to substitute the costly Haar-random state with
efficiently preparable PRS as a resource for cryptographic protocols, while
also finding applications in quantum learning theory, black hole physics,
many-body thermalization, quantum foundations, and quantum chaos. All existing
constructions of PRS equate the notion of efficiency to quantum computers which
runtime is bounded by a polynomial in its input size. In this work, we relax
the notion of efficiency for PRS with respect to observers with near-term
quantum computers implementing algorithms with runtime that scales slower than
polynomial-time. We introduce the $\mathbf{T}$-PRS which is indistinguishable
to quantum algorithms with runtime $\mathbf{T}(n)$ that grows slower than
polynomials in the input size $n$. We give a set of reasonable conditions that
a $\mathbf{T}$-PRS must satisfy and give two constructions by using
quantum-secure pseudorandom functions and pseudorandom functions. For
$\mathbf{T}(n)$ being linearithmic, linear, polylogarithmic, and logarithmic
function, we characterize the amount of quantum resources a $\mathbf{T}$-PRS
must possess, particularly on its coherence, entanglement, and magic. Our
quantum resource characterization applies generally to any two state ensembles
that are indistinguishable to observers with computational power
$\mathbf{T}(n)$, giving a general necessary condition of whether a low-resource
ensemble can mimic a high-resource ensemble, forming a
$\mathbf{T}$-pseudoresource pair. We demonstate how the necessary amount of
resource decreases as the observer's computational power is more restricted,
giving a $\mathbf{T}$-pseudoresource pair with larger resource gap for more
computationally limited observers.

</details>

### [261] [On the Generalization of Adversarially Trained Quantum Classifiers](https://arxiv.org/abs/2504.17690)
*Petros Georgiou,Aaron Mark Thomas,Sharu Theresa Jose,Osvaldo Simeone*

Main category: quant-ph

TLDR: 本文研究了量子分类器在对抗攻击下的泛化误差边界，发现对抗训练能有效提升鲁棒性，且在高维输入下样本复杂度增加可忽略。


<details>
  <summary>Details</summary>
Motivation: 量子分类器易受对抗攻击，对抗训练是一种潜在解决方案，但缺乏理论支持。本文旨在填补这一空白。

Method: 通过理论分析，建立了对抗训练量子分类器的泛化误差边界，并探讨了量子嵌入的影响。

Result: 对抗训练的泛化误差随样本量增加而降低（1/√m），高维输入下样本复杂度增加可忽略；量子嵌入的影响取决于希尔伯特空间维度。

Conclusion: 对抗训练能有效提升量子分类器的鲁棒性，理论结果通过数值实验验证。

Abstract: Quantum classifiers are vulnerable to adversarial attacks that manipulate
their input classical or quantum data. A promising countermeasure is
adversarial training, where quantum classifiers are trained by using an
attack-aware, adversarial loss function. This work establishes novel bounds on
the generalization error of adversarially trained quantum classifiers when
tested in the presence of perturbation-constrained adversaries. The bounds
quantify the excess generalization error incurred to ensure robustness to
adversarial attacks as scaling with the training sample size $m$ as
$1/\sqrt{m}$, while yielding insights into the impact of the quantum embedding.
For quantum binary classifiers employing \textit{rotation embedding}, we find
that, in the presence of adversarial attacks on classical inputs $\mathbf{x}$,
the increase in sample complexity due to adversarial training over conventional
training vanishes in the limit of high dimensional inputs $\mathbf{x}$. In
contrast, when the adversary can directly attack the quantum state
$\rho(\mathbf{x})$ encoding the input $\mathbf{x}$, the excess generalization
error depends on the choice of embedding only through its Hilbert space
dimension. The results are also extended to multi-class classifiers. We
validate our theoretical findings with numerical experiments.

</details>

<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [262] [Rate-Distortion-Perception Theory for the Quadratic Wasserstein Space](https://arxiv.org/abs/2504.17236)
*Xiqiang Qu,Jun Chen,Lei Yu,Xiangyu Xu*

Main category: cs.IT

TLDR: 本文提出了在平方误差失真和平方Wasserstein-2感知度量下，有限共同随机性时的失真-率-感知基本权衡的单字母表征，并证明了其对高斯源的显式可评估性，同时澄清了多种通用表示的概念。


<details>
  <summary>Details</summary>
Motivation: 研究在有限共同随机性条件下，如何量化失真、编码率和感知质量之间的基本权衡关系，为信号处理和压缩领域提供理论支持。

Method: 通过建立单字母表征，结合平方误差失真和平方Wasserstein-2感知度量，分析高斯源下的显式评估方法。

Result: 成功推导出失真-率-感知权衡的单字母表征，并证明其在高斯源下的可计算性。

Conclusion: 该研究为信号压缩和感知优化提供了理论框架，并展示了在高斯源下的实际应用潜力。

Abstract: We establish a single-letter characterization of the fundamental
distortion-rate-perception tradeoff with limited common randomness under the
squared error distortion measure and the squared Wasserstein-2 perception
measure. Moreover, it is shown that this single-letter characterization can be
explicitly evaluated for the Gaussian source. Various notions of universal
representation are also clarified.

</details>

<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [263] [Can deep neural networks learn biological vision?](https://arxiv.org/abs/2504.16940)
*Drew Linsley,Pinyuan Feng,Thomas Serre*

Main category: q-bio.NC

TLDR: 论文探讨了深度神经网络（DNNs）与灵长类视觉系统的对齐趋势变化，并提出了未来生物视觉模型的发展方向。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解DNNs与生物视觉系统在性能提升过程中的对齐变化，以及如何改进未来的计算模型以更好地模拟生物视觉。

Method: 通过分析DNNs在计算机视觉任务中的表现与灵长类神经反应的对比，提出了一种新的研究方向。

Result: 研究发现现代DNNs在达到人类或超人类识别精度时，与生物视觉系统的特征依赖出现分歧。

Conclusion: 结论是未来的生物视觉模型需要脱离人工智能的框架，专注于设计更接近人类视觉系统的训练方法和数据。

Abstract: Deep neural networks (DNNs) once showed increasing alignment with primate
neural responses as they improved on computer vision benchmarks. This trend
raised the exciting possibility that better models of biological vision would
come as a byproduct of the deep learning revolution in artificial intelligence.
However, the trend has reversed over recent years as DNNs have scaled to human
or superhuman recognition accuracy, a divergence that may stem from modern DNNs
learning to rely on different visual features than primates to solve tasks.
Where will better computational models of biological vision come from? We
propose that vision science must break from artificial intelligence to develop
algorithms that are designed with biological visual systems in mind instead of
internet data benchmarks. We predict that the next generation of deep learning
models of biological vision will be trained with data diets, training routines,
and objectives that are closer to those that shape human vision than those that
are in use today.

</details>

<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [264] [You Are What You Bought: Generating Customer Personas for E-commerce Applications](https://arxiv.org/abs/2504.17304)
*Yimin Shi,Yang Fei,Shiqi Zhang,Haixun Wang,Xiaokui Xiao*

Main category: cs.IR

TLDR: 论文提出了一种基于客户角色的显式用户表示方法GPLR，结合预训练LLM和随机游走技术，显著提升了推荐和客户分群的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法生成的用户嵌入难以理解和结合外部知识，限制了应用效果。

Method: 提出GPLR方法，利用预训练LLM推断客户角色，并通过随机游走技术扩展角色预测范围，同时提出RevAff优化时间效率。

Result: 在三个真实电商数据集上验证，角色表示将推荐模型的NDCG@K和F1-Score@K提升高达12%。

Conclusion: 客户角色表示显著提升了推荐和分群任务的性能，且方法高效可扩展。

Abstract: In e-commerce, user representations are essential for various applications.
Existing methods often use deep learning techniques to convert customer
behaviors into implicit embeddings. However, these embeddings are difficult to
understand and integrate with external knowledge, limiting the effectiveness of
applications such as customer segmentation, search navigation, and product
recommendations. To address this, our paper introduces the concept of the
customer persona. Condensed from a customer's numerous purchasing histories, a
customer persona provides a multi-faceted and human-readable characterization
of specific purchase behaviors and preferences, such as Busy Parents or Bargain
Hunters.
  This work then focuses on representing each customer by multiple personas
from a predefined set, achieving readable and informative explicit user
representations. To this end, we propose an effective and efficient solution
GPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs to infer
personas for customers. To reduce overhead, GPLR applies LLM-based labeling to
only a fraction of users and utilizes a random walk technique to predict
personas for the remaining customers. We further propose RevAff, which provides
an absolute error $\epsilon$ guarantee while improving the time complexity of
the exact solution by a factor of at least
$O(\frac{\epsilon\cdot|E|N}{|E|+N\log N})$, where $N$ represents the number of
customers and products, and $E$ represents the interactions between them. We
evaluate the performance of our persona-based representation in terms of
accuracy and robustness for recommendation and customer segmentation tasks
using three real-world e-commerce datasets. Most notably, we find that
integrating customer persona representations improves the state-of-the-art
graph convolution-based recommendation model by up to 12% in terms of NDCG@K
and F1-Score@K.

</details>

### [265] [IRA: Adaptive Interest-aware Representation and Alignment for Personalized Multi-interest Retrieval](https://arxiv.org/abs/2504.17529)
*Youngjune Lee,Haeyu Jeong,Changgeon Lim,Jeong Choi,Hongjun Lim,Hangon Kim,Jiyoon Kwon,Saehun Kim*

Main category: cs.IR

TLDR: IRA框架通过动态兴趣单元和语义检索机制，实时适应用户兴趣变化，实现个性化推荐。


<details>
  <summary>Details</summary>
Motivation: 在线社区平台需要动态个性化推荐，但实时优化模型以适应变化的用户兴趣和新文档仍具挑战性。

Method: 提出IRA框架，利用兴趣单元捕获用户兴趣并通过累积更新调整，结合语义检索机制避免点击信号依赖。

Result: IRA在真实数据集上验证有效，并成功部署于NAVER CAFE平台。

Conclusion: IRA框架能持续适应用户兴趣变化，提供细粒度个性化推荐，且不受历史训练分布限制。

Abstract: Online community platforms require dynamic personalized retrieval and
recommendation that can continuously adapt to evolving user interests and new
documents. However, optimizing models to handle such changes in real-time
remains a major challenge in large-scale industrial settings. To address this,
we propose the Interest-aware Representation and Alignment (IRA) framework, an
efficient and scalable approach that dynamically adapts to new interactions
through a cumulative structure. IRA leverages two key mechanisms: (1) Interest
Units that capture diverse user interests as contextual texts, while
reinforcing or fading over time through cumulative updates, and (2) a retrieval
process that measures the relevance between Interest Units and documents based
solely on semantic relationships, eliminating dependence on click signals to
mitigate temporal biases. By integrating cumulative Interest Unit updates with
the retrieval process, IRA continuously adapts to evolving user preferences,
ensuring robust and fine-grained personalization without being constrained by
past training distributions. We validate the effectiveness of IRA through
extensive experiments on real-world datasets, including its deployment in the
Home Section of NAVER's CAFE, South Korea's leading community platform.

</details>

<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [266] [An introduction to R package `mvs`](https://arxiv.org/abs/2504.17546)
*Wouter van Loon*

Main category: stat.CO

TLDR: 论文介绍了一个R包`mvs`，用于处理多视图数据，基于多视图堆叠（MVS）框架，通过并行训练和权重分配提升模型性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 生物医学科学中，多视图数据常见但传统机器学习方法忽视其结构，限制了模型性能和可解释性。

Method: MVS框架通过单独训练每个视图的模型，交叉验证评估预测能力，再用另一算法分配权重，实现并行计算和降维。

Result: MVS适用于高维数据，能自动选择重要视图，支持多级堆叠模型和缺失数据处理。

Conclusion: `mvs`包为多视图数据提供了高效、灵活的解决方案，特别适合高维数据场景。

Abstract: In biomedical science, a set of objects or persons can often be described by
multiple distinct sets of features obtained from different data sources or
modalities (called "multi-view data"). Classical machine learning methods
ignore the multi-view structure of such data, limiting model interpretability
and performance. The R package `mvs` provides methods that were designed
specifically for dealing with multi-view data, based on the multi-view stacking
(MVS) framework. MVS is a form of supervised (machine) learning used to train
multi-view classification or prediction models. MVS works by training a
learning algorithm on each view separately, estimating the predictive power of
each view-specific model through cross-validation, and then using another
learning algorithm to assign weights to the view-specific models based on their
estimated predictions. MVS is a form of ensemble learning, dividing the large
multi-view learning problem into smaller sub-problems. Most of these
sub-problems can be solved in parallel, making it computationally attractive.
Additionally, the number of features of the sub-problems is greatly reduced
compared with the full multi-view learning problem. This makes MVS especially
useful when the total number of features is larger than the number of
observations (i.e., high-dimensional data). MVS can still be applied even if
the sub-problems are themselves high-dimensional by adding suitable penalty
terms to the learning algorithms. Furthermore, MVS can be used to automatically
select the views which are most important for prediction. The R package `mvs`
makes fitting MVS models, including such penalty terms, easily and openly
accessible. `mvs` allows for the fitting of stacked models with any number of
levels, with different penalty terms, different outcome distributions, and
provides several options for missing data handling.

</details>

<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [267] [Data-Driven Surrogate Modeling Techniques to Predict the Effective Contact Area of Rough Surface Contact Problems](https://arxiv.org/abs/2504.17354)
*Tarik Sahin,Jacopo Bonari,Sebastian Brandstaeter,Alexander Popp*

Main category: cs.CE

TLDR: 论文提出了一种基于数据驱动的替代模型框架，用于快速预测粗糙表面接触的有效接触面积，解决了传统数值方法计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 粗糙表面接触的有效接触面积对多物理现象（如磨损、密封、热或电传导）至关重要，但传统数值方法（如边界元法）计算成本高，限制了其在多查询场景中的应用。

Method: 研究使用多种机器学习算法训练预计算数据集，输入为施加的载荷和统计粗糙度参数，输出为有效接触面积，并通过超参数优化比较模型性能。

Result: Kernel Ridge Regressor在准确性和效率之间表现出最佳平衡，而Gaussian Process Regressor适用于需要不确定性量化的场景。模型在新模拟场景中验证了泛化能力。

Conclusion: 替代模型框架在多查询任务中实用且高效，尽管数据库生成是主要成本，但整体方法仍具有优势。

Abstract: The effective contact area in rough surface contact plays a critical role in
multi-physics phenomena such as wear, sealing, and thermal or electrical
conduction. Although accurate numerical methods, like the Boundary Element
Method (BEM), are available to compute this quantity, their high computational
cost limits their applicability in multi-query contexts, such as uncertainty
quantification, parameter identification, and multi-scale algorithms, where
many repeated evaluations are required. This study proposes a surrogate
modeling framework for predicting the effective contact area using
fast-to-evaluate data-driven techniques. Various machine learning algorithms
are trained on a precomputed dataset, where the inputs are the imposed load and
statistical roughness parameters, and the output is the corresponding effective
contact area. All models undergo hyperparameter optimization to enable fair
comparisons in terms of predictive accuracy and computational efficiency,
evaluated using established quantitative metrics. Among the models, the Kernel
Ridge Regressor demonstrates the best trade-off between accuracy and
efficiency, achieving high predictive accuracy, low prediction time, and
minimal training overhead-making it a strong candidate for general-purpose
surrogate modeling. The Gaussian Process Regressor provides an attractive
alternative when uncertainty quantification is required, although it incurs
additional computational cost due to variance estimation. The generalization
capability of the Kernel Ridge model is validated on an unseen simulation
scenario, confirming its ability to transfer to new configurations. Database
generation constitutes the dominant cost in the surrogate modeling process.
Nevertheless, the approach proves practical and efficient for multi-query
tasks, even when accounting for this initial expense.

</details>

### [268] [polyGen: A Learning Framework for Atomic-level Polymer Structure Generation](https://arxiv.org/abs/2504.17656)
*Ayush Jain,Rampi Ramprasad*

Main category: cs.CE

TLDR: polyGen是一种新型的潜在扩散模型，专门用于从最小输入（如重复单元化学结构）生成真实的聚合物3D结构，填补了合成聚合物生成算法的空白。


<details>
  <summary>Details</summary>
Motivation: 合成聚合物材料在多个领域至关重要，但其设计周期长，且现有生成算法未涵盖合成聚合物。polyGen旨在解决这一问题。

Method: 利用分子编码捕获聚合物连接性，结合DFT优化的聚合物和分子结构进行训练，通过结构匹配标准评估性能。

Result: polyGen能有效生成线性和复杂分支结构的多样构象，但对高原子数重复单元的性能下降。

Conclusion: polyGen是聚合物科学中原子级结构生成的范式转变，首次实现了对真实聚合物构象的预测。

Abstract: Synthetic polymeric materials underpin fundamental technologies in the
energy, electronics, consumer goods, and medical sectors, yet their development
still suffers from prolonged design timelines. Although polymer informatics
tools have supported speedup, polymer simulation protocols continue to face
significant challenges: on-demand generation of realistic 3D atomic structures
that respect the conformational diversity of polymer structures. Generative
algorithms for 3D structures of inorganic crystals, bio-polymers, and small
molecules exist, but have not addressed synthetic polymers. In this work, we
introduce polyGen, the first latent diffusion model designed specifically to
generate realistic polymer structures from minimal inputs such as the repeat
unit chemistry alone, leveraging a molecular encoding that captures polymer
connectivity throughout the architecture. Due to a scarce dataset of only 3855
DFT-optimized polymer structures, we augment our training with DFT-optimized
molecular structures, showing improvement in joint learning between similar
chemical structures. We also establish structure matching criteria to benchmark
our approach on this novel problem. polyGen effectively generates diverse
conformations of both linear chains and complex branched structures, though its
performance decreases when handling repeat units with a high atom count. Given
these initial results, polyGen represents a paradigm shift in atomic-level
structure generation for polymer science-the first proof-of-concept for
predicting realistic atomic-level polymer conformations while accounting for
their intrinsic structural flexibility.

</details>

<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [269] [On the workflow, opportunities and challenges of developing foundation model in geophysics](https://arxiv.org/abs/2504.17384)
*Hanlin Sheng,Xinming Wu,Hang Gao,Haibin Di,Sergey Fomel,Jintao Li,Xu Si*

Main category: physics.geo-ph

TLDR: 本文提出了一个完整框架，系统性地探讨了基础模型与地球物理数据结合的全流程，填补了该领域综述的空白。


<details>
  <summary>Details</summary>
Motivation: 地球物理领域基础模型应用逐渐扩展，但缺乏全流程的综述，本文旨在填补这一空白。

Method: 从数据收集、预处理到模型架构选择、预训练策略和部署，详细分析了各阶段关键技术，并针对地球物理数据的多样性、复杂性和物理一致性约束提出了解决方案。

Result: 通过利用基础模型的迁移学习能力，减少对标注数据的依赖，提高计算效率，并增强物理一致性和可解释性。

Conclusion: 本文不仅填补了地球物理领域基础模型全流程综述的空白，还为实际应用提供了有价值的指导，推动了该领域的创新与发展。

Abstract: Foundation models, as a mainstream technology in artificial intelligence,
have demonstrated immense potential across various domains in recent years,
particularly in handling complex tasks and multimodal data. In the field of
geophysics, although the application of foundation models is gradually
expanding, there is currently a lack of comprehensive reviews discussing the
full workflow of integrating foundation models with geophysical data. To
address this gap, this paper presents a complete framework that systematically
explores the entire process of developing foundation models in conjunction with
geophysical data. From data collection and preprocessing to model architecture
selection, pre-training strategies, and model deployment, we provide a detailed
analysis of the key techniques and methodologies at each stage. In particular,
considering the diversity, complexity, and physical consistency constraints of
geophysical data, we discuss targeted solutions to address these challenges.
Furthermore, we discuss how to leverage the transfer learning capabilities of
foundation models to reduce reliance on labeled data, enhance computational
efficiency, and incorporate physical constraints into model training, thereby
improving physical consistency and interpretability. Through a comprehensive
summary and analysis of the current technological landscape, this paper not
only fills the gap in the geophysics domain regarding a full-process review of
foundation models but also offers valuable practical guidance for their
application in geophysical data analysis, driving innovation and advancement in
the field.

</details>

### [270] [Dargana: fine-tuning EarthPT for dynamic tree canopy mapping from space](https://arxiv.org/abs/2504.17321)
*Michael J. Smith,Luke Fleming,James E. Geach,Ryan J. Roberts,Freddie Kalaitzis,James Banister*

Main category: physics.geo-ph

TLDR: Dargana是EarthPT时间序列基础模型的微调版本，仅用3%的预训练数据和5%的计算资源，实现了对10米分辨率树冠覆盖的分类，区分针叶树和阔叶树。


<details>
  <summary>Details</summary>
Motivation: 研究旨在展示如何通过微调预训练的大型观测模型（如EarthPT），实现高精度、动态的土地覆盖监测，为自然资本管理和保护提供可扩展工具。

Method: Dargana通过微调EarthPT模型，专注于生成10米分辨率的树冠覆盖分类，并区分针叶树和阔叶树类型。

Result: 在Cornwall测试中，模型在未见过的卫星图像上达到像素级ROC-AUC 0.98和PR-AUC 0.83，并能识别细粒度结构（如树篱）和跟踪时间变化。

Conclusion: Dargana展示了预训练模型在动态土地覆盖监测中的潜力，为自然资本管理提供了高效、可扩展的解决方案。

Abstract: We present Dargana, a fine-tuned variant of the EarthPT time-series
foundation model that achieves specialisation using <3% of its pre-training
data volume and 5% of its pre-training compute. Dargana is fine-tuned to
generate regularly updated classification of tree canopy cover at 10m
resolution, distinguishing conifer and broadleaved tree types. Using Cornwall,
UK, as a test case, the model achieves a pixel-level ROC-AUC of 0.98 and a
PR-AUC of 0.83 on unseen satellite imagery. Dargana can identify fine
structures like hedgerows and coppice below the training sample limit, and can
track temporal changes to canopy cover such as new woodland establishment. Our
results demonstrate how pre-trained Large Observation Models like EarthPT can
be specialised for granular, dynamic land cover monitoring from space,
providing a valuable, scalable tool for natural capital management and
conservation.

</details>

### [271] [HydroStartML: A combined machine learning and physics-based approach to reduce hydrological model spin-up time](https://arxiv.org/abs/2504.17420)
*Louisa Pawusch,Stefania Scheurer,Wolfgang Nowak,Reed Maxwell*

Main category: physics.geo-ph

TLDR: HydroStartML是一种机器学习模拟器，用于加速水文模型中的初始水位深度（DTWT）配置计算，显著减少传统迭代计算的时间。


<details>
  <summary>Details</summary>
Motivation: 传统的水文模型初始DTWT配置计算（即模型spin-up）耗时且计算量大，尤其是当初始配置远离稳态时。

Method: 开发了HydroStartML，基于美国本土的数据（如电导率和地表坡度）训练，预测流域的稳态DTWT配置，作为初始值。

Result: 使用HydroStartML预测的初始配置能更快收敛，且对未训练的地形配置也表现良好，显著减少计算时间，尤其是在DTWT较深的区域。

Conclusion: HydroStartML为混合机器学习与传统模拟的方法提供了可能，提高了水文预测的效率和准确性，有助于水资源管理和环境研究。

Abstract: Finding the initial depth-to-water table (DTWT) configuration of a catchment
is a critical challenge when simulating the hydrological cycle with integrated
models, significantly impacting simulation outcomes. Traditionally, this
involves iterative spin-up computations, where the model runs under constant
atmospheric settings until steady-state is achieved. These so-called model
spin-ups are computationally expensive, often requiring many years of simulated
time, particularly when the initial DTWT configuration is far from steady
state.
  To accelerate the model spin-up process we developed HydroStartML, a machine
learning emulator trained on steady-state DTWT configurations across the
contiguous United States. HydroStartML predicts, based on available data like
conductivity and surface slopes, a DTWT configuration of the respective
watershed, which can be used as an initial DTWT.
  Our results show that initializing spin-up computations with HydroStartML
predictions leads to faster convergence than with other initial configurations
like spatially constant DTWTs. The emulator accurately predicts configurations
close to steady state, even for terrain configurations not seen in training,
and allows especially significant reductions in computational spin-up effort in
regions with deep DTWTs. This work opens the door for hybrid approaches that
blend machine learning and traditional simulation, enhancing predictive
accuracy and efficiency in hydrology for improving water resource management
and understanding complex environmental interactions.

</details>

<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [272] [On-Device Qwen2.5: Efficient LLM Inference with Model Compression and Hardware Acceleration](https://arxiv.org/abs/2504.17376)
*Maoyang Xiang,Ramesh Fernando,Bo Wang*

Main category: cs.AR

TLDR: 本文提出了一种在边缘设备上高效部署Qwen2.5-0.5B模型的方法，结合AWQ量化和FPGA加速，显著提升了压缩率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在边缘设备上部署面临计算需求高、内存带宽受限和能耗大的挑战。

Method: 采用激活感知权重量化（AWQ）和FPGA加速执行管道，结合CPU和FPGA的混合执行策略。

Result: 模型压缩率达到55.08%，输出速度为5.1 tokens/秒，优于基线2.8 tokens/秒。

Conclusion: 该框架有效解决了边缘设备部署的挑战，提升了性能和效率。

Abstract: Transformer-based Large Language Models (LLMs) have significantly advanced AI
capabilities but pose considerable challenges for deployment on edge devices
due to high computational demands, memory bandwidth constraints, and energy
consumption. This paper addresses these challenges by presenting an efficient
framework for deploying the Qwen2.5-0.5B model on the Xilinx Kria KV260 edge
platform, a heterogeneous system integrating an ARM Cortex-A53 CPU with
reconfigurable FPGA logic. Leveraging Activation-aware Weight Quantization
(AWQ) with FPGA-accelerated execution pipelines, the proposed approach enhances
both model compression rate and system throughput. Additionally, we propose a
hybrid execution strategy that intelligently offloads compute-intensive
operations to the FPGA while utilizing the CPU for lighter tasks, effectively
balancing the computational workload and maximizing overall performance. Our
framework achieves a model compression rate of 55.08% compared to the original
model and produces output at a rate of 5.1 tokens per second, outperforming the
baseline performance of 2.8 tokens per second.

</details>

### [273] [L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference](https://arxiv.org/abs/2504.17584)
*Qingyuan Liu,Liyan Chen,Yanning Yang,Haocheng Wang,Dong Du,Zhigang Mao,Naifeng Jing,Yubin Xia,Haibo Chen*

Main category: cs.AR

TLDR: 论文提出L3系统，通过硬件-软件协同设计，利用DIMM-PIM和GPU设备优化LLM的长文本处理，解决内存带宽和容量的瓶颈。


<details>
  <summary>Details</summary>
Motivation: LLM处理长文本时面临GPU内存容量与带宽的权衡问题，现有HBM加速器容量有限，而主机DIMM数据交换开销大。

Method: L3系统通过硬件重新设计、通信优化和自适应调度器，整合DIMM-PIM与GPU，提升LLM推理效率。

Result: 实验表明，L3比现有HBM-PIM方案提速6.1倍，并显著提升批处理规模。

Conclusion: L3通过DIMM-PIM与GPU协同设计，有效解决了LLM长文本处理的内存瓶颈问题。

Abstract: Large Language Models (LLMs) increasingly require processing long text
sequences, but GPU memory limitations force difficult trade-offs between memory
capacity and bandwidth. While HBM-based acceleration offers high bandwidth, its
capacity remains constrained. Offloading data to host-side DIMMs improves
capacity but introduces costly data swapping overhead. We identify that the
critical memory bottleneck lies in the decoding phase of multi-head attention
(MHA) exclusively, which demands substantial capacity for storing KV caches and
high bandwidth for attention computation. Our key insight reveals this
operation uniquely aligns with modern DIMM-based processing-in-memory (PIM)
architectures, which offers scalability of both capacity and bandwidth.
  Based on this observation and insight, we propose L3, a hardware-software
co-designed system integrating DIMM-PIM and GPU devices. L3 introduces three
innovations: First, hardware redesigns resolve data layout mismatches and
computational element mismatches in DIMM-PIM, enhancing LLM inference
utilization. Second, communication optimization enables hiding the data
transfer overhead with the computation. Third, an adaptive scheduler
coordinates GPU-DIMM-PIM operations to maximize parallelism between devices.
Evaluations using real-world traces show L3 achieves up to 6.1$\times$ speedup
over state-of-the-art HBM-PIM solutions while significantly improving batch
sizes.

</details>

<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [274] [Physics-guided and fabrication-aware inverse design of photonic devices using diffusion models](https://arxiv.org/abs/2504.17077)
*Dongjin Seo,Soobin Um,Sangbin Lee,Jong Chul Ye,Haejun Chung*

Main category: physics.optics

TLDR: AdjointDiffusion是一种物理引导的框架，将伴随灵敏度梯度融入扩散模型的采样过程，显著提高了自由形式光子器件设计的效率和可制造性。


<details>
  <summary>Details</summary>
Motivation: 传统逆向设计方法（如全局优化或伴随梯度方法）需要复杂的二值化和过滤步骤，而深度学习策略需要大量模拟（10^5到10^6次）。AdjointDiffusion旨在克服这些限制。

Method: 训练扩散网络于合成、制造感知的二进制掩码数据集，并在推理过程中注入基于物理的伴随梯度，引导生成高优值解，无需后处理。

Result: 在弯曲波导和CMOS图像传感器颜色路由器问题上，AdjointDiffusion在效率和可制造性上均优于现有非线性优化器（如MMA和SLSQP），且模拟次数显著减少（约200次）。

Conclusion: AdjointDiffusion通过消除复杂二值化步骤和最小化模拟开销，为下一代光子器件设计提供了高效、制造感知的解决方案。

Abstract: Designing free-form photonic devices is fundamentally challenging due to the
vast number of possible geometries and the complex requirements of fabrication
constraints. Traditional inverse-design approaches--whether driven by human
intuition, global optimization, or adjoint-based gradient methods--often
involve intricate binarization and filtering steps, while recent deep learning
strategies demand prohibitively large numbers of simulations (10^5 to 10^6). To
overcome these limitations, we present AdjointDiffusion, a physics-guided
framework that integrates adjoint sensitivity gradients into the sampling
process of diffusion models. AdjointDiffusion begins by training a diffusion
network on a synthetic, fabrication-aware dataset of binary masks. During
inference, we compute the adjoint gradient of a candidate structure and inject
this physics-based guidance at each denoising step, steering the generative
process toward high figure-of-merit (FoM) solutions without additional
post-processing. We demonstrate our method on two canonical photonic design
problems--a bent waveguide and a CMOS image sensor color router--and show that
our method consistently outperforms state-of-the-art nonlinear optimizers (such
as MMA and SLSQP) in both efficiency and manufacturability, while using orders
of magnitude fewer simulations (approximately 2 x 10^2) than pure deep learning
approaches (approximately 10^5 to 10^6). By eliminating complex binarization
schedules and minimizing simulation overhead, AdjointDiffusion offers a
streamlined, simulation-efficient, and fabrication-aware pipeline for
next-generation photonic device design. Our open-source implementation is
available at https://github.com/dongjin-seo2020/AdjointDiffusion.

</details>